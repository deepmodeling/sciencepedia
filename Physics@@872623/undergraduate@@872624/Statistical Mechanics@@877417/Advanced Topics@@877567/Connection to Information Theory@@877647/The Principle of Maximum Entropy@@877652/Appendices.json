{"hands_on_practices": [{"introduction": "This first exercise provides a foundational application of the Principle of Maximum Entropy to a simple, discrete physical system. By calculating the probabilities for a system to be in one of three energy states, given a fixed average energy, you will practice the core technique of using Lagrange multipliers to find the least-biased distribution. This problem builds a solid base for understanding how the celebrated Boltzmann distribution arises from maximizing entropy. [@problem_id:2006969]", "problem": "A physical system can be found in one of three distinct, non-degenerate energy states with energies given by $E_1 = 0$, $E_2 = \\epsilon$, and $E_3 = 2\\epsilon$, where $\\epsilon$ is a positive energy constant. The ensemble average energy of the system is constrained to be $\\langle E \\rangle = \\frac{4}{5}\\epsilon$. The probability distribution $\\{p_1, p_2, p_3\\}$ that describes the system is the one that maximizes the Gibbs entropy, subject to the given constraints. Using this principle of maximum entropy, determine the exact analytical expression for the probability, $p_1$, of finding the system in the ground state (energy $E_1=0$).", "solution": "We maximize the Gibbs entropy $S=-k_{B}\\sum_{i=1}^{3} p_i\\ln p_i$ subject to the constraints $\\sum_{i=1}^{3} p_i=1$ and $\\sum_{i=1}^{3} p_i E_i=\\frac{4}{5}\\epsilon$, with $E_1=0$, $E_2=\\epsilon$, $E_3=2\\epsilon$. Introduce Lagrange multipliers $\\alpha$ and $\\gamma$ and consider\n$$\n\\mathcal{L}=-k_{B}\\sum_{i=1}^{3} p_i\\ln p_i-\\alpha\\left(\\sum_{i=1}^{3} p_i-1\\right)-\\gamma\\left(\\sum_{i=1}^{3} p_i E_i-\\frac{4}{5}\\epsilon\\right).\n$$\nStationarity with respect to $p_i$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_i}=-k_B\\left(\\ln p_i+1\\right)-\\alpha-\\gamma E_i=0 \\quad \\Rightarrow \\quad \\ln p_i=-1-\\frac{\\alpha}{k_B}-\\frac{\\gamma}{k_B}E_i.\n$$\nHence $p_i=A\\,\\exp(-\\beta E_i)$ with $A=\\exp\\!\\left(-1-\\frac{\\alpha}{k_B}\\right)$ and $\\beta=\\frac{\\gamma}{k_B}$. Normalization fixes $A$ via the partition function $Z=\\sum_{i=1}^{3}\\exp(-\\beta E_i)$, so\n$$\np_i=\\frac{\\exp(-\\beta E_i)}{Z}, \\quad Z=1+\\exp(-\\beta \\epsilon)+\\exp(-2\\beta \\epsilon).\n$$\nDefine $x=\\exp(-\\beta \\epsilon)>0$. Then\n$$\np_1=\\frac{1}{1+x+x^{2}}, \\quad p_2=\\frac{x}{1+x+x^{2}}, \\quad p_3=\\frac{x^{2}}{1+x+x^{2}}.\n$$\nThe energy constraint $\\langle E\\rangle=\\frac{4}{5}\\epsilon$ becomes\n$$\n\\frac{\\epsilon x+2\\epsilon x^{2}}{1+x+x^{2}}=\\frac{4}{5}\\epsilon \\quad \\Rightarrow \\quad \\frac{x+2x^{2}}{1+x+x^{2}}=\\frac{4}{5}.\n$$\nClearing denominators yields\n$$\n5(x+2x^{2})=4(1+x+x^{2}) \\quad \\Rightarrow \\quad 6x^{2}+x-4=0.\n$$\nSolving the quadratic equation gives\n$$\nx=\\frac{-1\\pm\\sqrt{1+96}}{12}=\\frac{-1\\pm\\sqrt{97}}{12}.\n$$\nSince $x > 0$, we select $x=\\frac{-1+\\sqrt{97}}{12}$. Therefore\n$$\np_1=\\frac{1}{1+x+x^{2}}=\\frac{1}{1+\\frac{\\sqrt{97}-1}{12}+\\left(\\frac{\\sqrt{97}-1}{12}\\right)^{2}}.\n$$\nCompute the denominator explicitly:\n$$\n1+\\frac{\\sqrt{97}-1}{12}+\\left(\\frac{\\sqrt{97}-1}{12}\\right)^{2}\n=1+\\frac{\\sqrt{97}-1}{12}+\\frac{97-2\\sqrt{97}+1}{144}\n=\\frac{115+5\\sqrt{97}}{72}.\n$$\nThus\n$$\np_1=\\frac{72}{115+5\\sqrt{97}}.\n$$\nThis is the exact analytical expression for the ground-state probability under the maximum-entropy distribution with the given mean energy.", "answer": "$$\\boxed{\\frac{72}{115+5\\sqrt{97}}}$$", "id": "2006969"}, {"introduction": "Moving from discrete states to continuous variables, this problem explores the probability distribution governing data from a stationary stochastic process. You will apply the maximum entropy principle to determine the joint probability density for two correlated data points, given their variance and covariance. This exercise is crucial as it reveals why the Gaussian (or Normal) distribution is so ubiquitous in science and engineering when our knowledge is limited to the mean and second moments. [@problem_id:2006959]", "problem": "A stationary stochastic process $X(t)$ is a sequence of random variables where the statistical properties do not change over time. Consider two data points, $x_1$ and $x_2$, sampled from such a process. Due to stationarity, we can assume the process has zero mean, such that $\\langle x_1 \\rangle = \\langle x_2 \\rangle = 0$. From empirical measurements, we have determined the following properties: the variance for each data point is $\\langle x_1^2 \\rangle = \\langle x_2^2 \\rangle = \\sigma^2$, and the covariance between them is $\\langle x_1 x_2 \\rangle = C$. The constants $\\sigma^2$ and $C$ are known, with $\\sigma^2 > 0$ and $\\sigma^4 > C^2$.\n\nUsing the principle of maximum entropy, determine the joint probability density function $p(x_1, x_2)$ that is most consistent with this limited information. The principle of maximum entropy states that, subject to a set of constraints, the most unbiased probability distribution is the one that maximizes the information entropy.\n\nExpress your answer as a single, closed-form analytic expression in terms of $x_1, x_2, \\sigma$, and $C$.", "solution": "We are given two real-valued random variables $x_{1}$ and $x_{2}$ sampled from a stationary stochastic process with constraints\n$$\\langle x_{1} \\rangle = 0, \\quad \\langle x_{2} \\rangle = 0, \\quad \\langle x_{1}^{2} \\rangle = \\sigma^{2}, \\quad \\langle x_{2}^{2} \\rangle = \\sigma^{2}, \\quad \\langle x_{1} x_{2} \\rangle = C,$$\nwith $\\sigma^{2} > 0$ and $\\sigma^{4} > C^{2}$.\n\nBy the principle of maximum entropy for continuous variables, the density that maximizes the differential entropy\n$$H[p] = - \\int_{\\mathbb{R}^{2}} p(x_{1},x_{2}) \\ln p(x_{1},x_{2}) \\, dx_{1} dx_{2}$$\nsubject to the constraints of normalization and the above moment constraints has the exponential family form obtained via Lagrange multipliers. Introducing multipliers for normalization, means, variances, and covariance, the variational problem yields an extremal density of the form\n$$p(x_{1},x_{2}) = Z^{-1} \\exp\\!\\left( - a x_{1}^{2} - b x_{2}^{2} - 2 d x_{1} x_{2} - u x_{1} - v x_{2} \\right),$$\nwhere $a$, $b$, $d$, $u$, and $v$ are constants determined by the constraints, and $Z$ is the normalization constant. Writing this in matrix form as\n$$p(\\mathbf{x}) = Z^{-1} \\exp\\!\\left( - \\mathbf{x}^{T} K \\mathbf{x} - \\mathbf{h}^{T} \\mathbf{x} \\right), \\quad \\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}, \\quad K = \\begin{pmatrix} a & d \\\\ d & b \\end{pmatrix}, \\quad \\mathbf{h} = \\begin{pmatrix} u \\\\ v \\end{pmatrix},$$\nwe complete the square:\n$$- \\mathbf{x}^{T} K \\mathbf{x} - \\mathbf{h}^{T} \\mathbf{x} = - (\\mathbf{x} - \\boldsymbol{\\mu})^{T} K (\\mathbf{x} - \\boldsymbol{\\mu}) + \\boldsymbol{\\mu}^{T} K \\boldsymbol{\\mu}, \\quad \\boldsymbol{\\mu} = - \\tfrac{1}{2} K^{-1} \\mathbf{h}.$$\nHence the mean is $\\langle \\mathbf{x} \\rangle = \\boldsymbol{\\mu}$. Enforcing $\\langle x_{1} \\rangle = 0$ and $\\langle x_{2} \\rangle = 0$ requires $\\boldsymbol{\\mu} = \\mathbf{0}$, which implies $\\mathbf{h} = \\mathbf{0}$. Therefore the maximum entropy density simplifies to a centered Gaussian,\n$$p(\\mathbf{x}) = Z^{-1} \\exp\\!\\left( - \\mathbf{x}^{T} K \\mathbf{x} \\right).$$\nFor such a density, the covariance matrix is related to $K$ by\n$$\\langle \\mathbf{x} \\mathbf{x}^{T} \\rangle = \\tfrac{1}{2} K^{-1},$$\nso to match the prescribed covariance matrix $\\Sigma$ we must set $K = \\tfrac{1}{2} \\Sigma^{-1}$. It is then standard to write the centered multivariate normal in the canonical form\n$$p(\\mathbf{x}) = \\frac{1}{(2 \\pi)^{n/2} \\sqrt{\\det \\Sigma}} \\exp\\!\\left( - \\tfrac{1}{2} \\mathbf{x}^{T} \\Sigma^{-1} \\mathbf{x} \\right), \\quad n=2.$$\nWith the given second moments, the covariance matrix is\n$$\\Sigma = \\begin{pmatrix} \\sigma^{2} & C \\\\ C & \\sigma^{2} \\end{pmatrix}, \\quad \\det \\Sigma = \\sigma^{4} - C^{2} > 0,$$\nwhich is positive definite by the assumption $\\sigma^{4} > C^{2}$. The inverse is\n$$\\Sigma^{-1} = \\frac{1}{\\sigma^{4} - C^{2}} \\begin{pmatrix} \\sigma^{2} & -C \\\\ -C & \\sigma^{2} \\end{pmatrix}.$$\nThus the maximum-entropy joint density is the zero-mean bivariate normal\n$$p(x_{1},x_{2}) = \\frac{1}{2 \\pi \\sqrt{\\sigma^{4} - C^{2}}} \\exp\\!\\left( - \\frac{1}{2(\\sigma^{4} - C^{2})} \\left[ \\sigma^{2} \\left( x_{1}^{2} + x_{2}^{2} \\right) - 2 C x_{1} x_{2} \\right] \\right),$$\nwhich uniquely satisfies the given constraints and maximizes the entropy subject to them.", "answer": "$$\\boxed{\\frac{1}{2 \\pi \\sqrt{\\sigma^{4} - C^{2}}}\\,\\exp\\!\\left(-\\frac{\\sigma^{2}\\!\\left(x_{1}^{2}+x_{2}^{2}\\right)-2 C x_{1} x_{2}}{2\\left(\\sigma^{4}-C^{2}\\right)}\\right)}$$", "id": "2006959"}, {"introduction": "This final practice problem delves into a more advanced and abstract application of maximum entropy, touching upon the field of random matrix theory, which has deep connections to quantum mechanics and number theory. You are tasked with finding the joint probability distribution for the eigenvalues of an ensemble of matrices, constrained by macroscopic averages like the trace. This exercise demonstrates the power and versatility of the principle in modeling complex systems where detailed microscopic information is unavailable. [@problem_id:2006940]", "problem": "Consider an ensemble of $2 \\times 2$ real symmetric matrices, $M$. Such matrices are often used in physics to model simplified Hamiltonians of complex quantum systems where the detailed interactions are unknown. The statistical properties of the ensemble can be described by the joint probability distribution of the eigenvalues, $p(\\lambda_1, \\lambda_2)$, of these matrices. For a real symmetric matrix, its eigenvalues are always real.\n\nSuppose that the only information known about this ensemble is that the average of the trace of the matrices is a constant $c_1$, and the average of the trace of the square of the matrices is another constant $c_2$. Specifically, these constraints are:\n1. $\\langle \\text{Tr}(M) \\rangle = c_1$\n2. $\\langle \\text{Tr}(M^2) \\rangle = c_2$\n\nAssume the eigenvalues $\\lambda_1$ and $\\lambda_2$ can take any real value. Invoke the principle of maximum entropy to determine the most unbiased joint probability distribution $p(\\lambda_1, \\lambda_2)$ consistent with these constraints. The principle of maximum entropy states that the probability distribution which best represents the current state of knowledge is the one with the largest entropy, subject to the known constraints. Find the analytical expression for $p(\\lambda_1, \\lambda_2)$ in terms of $\\lambda_1$, $\\lambda_2$, $c_1$, and $c_2$. It is given that the constants $c_1$ and $c_2$ satisfy the condition $2c_2 > c_1^2$.", "solution": "We maximize the differential entropy of the joint density $p(\\lambda_1,\\lambda_2)$ on $\\mathbb{R}^{2}$ subject to the constraints\n$$\n\\int_{\\mathbb{R}^{2}} p(\\lambda_1,\\lambda_2)\\,d\\lambda_1\\,d\\lambda_2=1,\\quad \\int_{\\mathbb{R}^{2}}(\\lambda_1+\\lambda_2)\\,p\\,d\\lambda_1\\,d\\lambda_2=c_1,\\quad \\int_{\\mathbb{R}^{2}}(\\lambda_1^2+\\lambda_2^2)\\,p\\,d\\lambda_1\\,d\\lambda_2=c_2.\n$$\nThe entropy is $S[p]=-\\int p\\ln p$. Introducing Lagrange multipliers $\\alpha$, $\\mu$, and $\\nu$ for the three constraints, we set up the variational functional\n$$\n\\mathcal{L}[p]=-\\int p\\ln p\\,d\\lambda_1\\,d\\lambda_2-\\alpha\\left(\\int p-1\\right)-\\mu\\left(\\int(\\lambda_1+\\lambda_2)\\,p-c_1\\right)-\\nu\\left(\\int(\\lambda_1^2+\\lambda_2^2)\\,p-c_2\\right).\n$$\nStationarity under variations $\\delta p$ yields\n$$\n-\\left(\\ln p+1\\right)-\\alpha-\\mu(\\lambda_1+\\lambda_2)-\\nu(\\lambda_1^2+\\lambda_2^2)=0,\n$$\nso the maximizing density has the exponential-family form\n$$\np(\\lambda_1,\\lambda_2)=\\frac{1}{Z(\\mu,\\nu)}\\exp\\!\\left(-\\mu(\\lambda_1+\\lambda_2)-\\nu(\\lambda_1^2+\\lambda_2^2)\\right),\n$$\nwith partition function\n$$\nZ(\\mu,\\nu)=\\int_{\\mathbb{R}^{2}}\\exp\\!\\left(-\\mu(\\lambda_1+\\lambda_2)-\\nu(\\lambda_1^2+\\lambda_2^2)\\right)\\,d\\lambda_1\\,d\\lambda_2.\n$$\nThe integral factorizes, and completing the square gives, for $\\nu > 0$,\n$$\n\\int_{-\\infty}^{\\infty}\\exp\\!\\left(-\\nu \\lambda^{2}-\\mu \\lambda\\right)\\,d\\lambda=\\sqrt{\\frac{\\pi}{\\nu}}\\exp\\!\\left(\\frac{\\mu^{2}}{4\\nu}\\right),\n$$\nhence\n$$\nZ(\\mu,\\nu)=\\left(\\sqrt{\\frac{\\pi}{\\nu}}\\exp\\!\\left(\\frac{\\mu^{2}}{4nu}\\right)\\right)^{2}=\\frac{\\pi}{\\nu}\\exp\\!\\left(\\frac{\\mu^{2}}{2\\nu}\\right).\n$$\nEquivalently, the density factorizes into identical one-dimensional Gaussians:\n$$\np(\\lambda_1,\\lambda_2)=\\left(\\sqrt{\\frac{\\nu}{\\pi}}\\exp\\!\\left(-\\nu \\lambda_1^2-\\mu \\lambda_1\\right)\\right)\\left(\\sqrt{\\frac{\\nu}{\\pi}}\\exp\\!\\left(-\\nu \\lambda_2^2-\\mu \\lambda_2\\right)\\right).\n$$\nThe constraints determine $\\mu$ and $\\nu$. Using the standard relations $E[\\lambda_1+\\lambda_2]=-\\,\\partial_\\mu\\ln Z$ and $E[\\lambda_1^2+\\lambda_2^2]=-\\,\\partial_\\nu\\ln Z$, together with\n$$\n\\ln Z(\\mu,\\nu)=\\ln \\pi-\\ln \\nu+\\frac{\\mu^{2}}{2\\nu},\n$$\nwe obtain\n$$\nE[\\lambda_1+\\lambda_2]=-\\,\\frac{\\partial \\ln Z}{\\partial \\mu}=-\\frac{\\mu}{\\nu}=c_1\\quad\\Rightarrow\\quad \\mu=-\\nu\\,c_1,\n$$\nand\n$$\nE[\\lambda_1^2+\\lambda_2^2]=-\\,\\frac{\\partial \\ln Z}{\\partial \\nu}=\\frac{1}{\\nu}+\\frac{\\mu^{2}}{2\\nu^{2}}=c_2.\n$$\nSubstituting $\\mu=-\\nu c_1$ into the second equation gives\n$$\nc_2=\\frac{1}{\\nu}+\\frac{c_1^2}{2}\\quad\\Rightarrow\\quad \\nu=\\frac{1}{c_2-\\frac{c_1^2}{2}}.\n$$\nThe given condition $2c_2>c_1^2$ ensures $\\nu>0$ and hence normalizability. Inserting $\\mu=-\\nu c_1$ into the exponent and completing the square yields the more transparent form\n$$\np(\\lambda_1,\\lambda_2)=\\frac{\\nu}{\\pi}\\exp\\!\\left(-\\nu\\left[(\\lambda_1-\\tfrac{c_1}{2})^{2}+(\\lambda_2-\\tfrac{c_1}{2})^{2}\\right]\\right),\n$$\nand substituting $\\nu=\\left(c_2-\\frac{c_1^2}{2}\\right)^{-1}$ finally gives\n$$\np(\\lambda_1,\\lambda_2)=\\frac{1}{\\pi\\left(c_2-\\frac{c_1^2}{2}\\right)}\\exp\\!\\left(-\\frac{\\left(\\lambda_1-\\frac{c_1}{2}\\right)^{2}+\\left(\\lambda_2-\\frac{c_1}{2}\\right)^{2}}{c_2-\\frac{c_1^2}{2}}\\right).\n$$\nThis is the maximum-entropy joint density on $\\mathbb{R}^{2}$ consistent with the given constraints, symmetric under exchange of $\\lambda_1$ and $\\lambda_2$, with independent and identically distributed Gaussian marginals of mean $\\frac{c_1}{2}$ and variance $\\frac{1}{2}\\left(c_2-\\frac{c_1^2}{2}\\right)$.", "answer": "$$\\boxed{\\frac{1}{\\pi\\left(c_2-\\frac{c_1^2}{2}\\right)}\\,\\exp\\!\\left(-\\frac{\\left(\\lambda_1-\\frac{c_1}{2}\\right)^{2}+\\left(\\lambda_2-\\frac{c_1}{2}\\right)^{2}}{c_2-\\frac{c_1^2}{2}}\\right)}$$", "id": "2006940"}]}