{"hands_on_practices": [{"introduction": "The first step in mastering Gibbs entropy is to become comfortable with its fundamental formula. This exercise provides a direct application of the Gibbs entropy equation, $S = -k_B \\sum_i p_i \\ln(p_i)$, to a concrete physical system with a known probability distribution. By calculating the entropy of a quantum system with three distinct energy levels, you will gain a tangible sense of how entropy quantifies the statistical uncertainty or \"mixedness\" of a state. [@problem_id:1967970]", "problem": "A researcher is investigating the properties of a single Nitrogen-Vacancy (NV) center in a diamond crystal, a point defect that can be treated as a quantum system. For the purposes of a simplified model, the NV center is considered to have three accessible electronic energy states. Through optical pumping and subsequent measurement, it is determined that the system is in a non-equilibrium steady state where the probabilities of occupying these three states are $p_1 = 0.6$, $p_2 = 0.3$, and $p_3 = 0.1$. Calculate the dimensionless quantity $S/k_B$, where $S$ is the Gibbs entropy of this single NV center system and $k_B$ is the Boltzmann constant. Round your final answer to three significant figures.", "solution": "The Gibbs entropy for a discrete system with probabilities $\\{p_{i}\\}$ is given by\n$$\nS=-k_{B}\\sum_{i}p_{i}\\ln(p_{i}).\n$$\nTherefore, the dimensionless entropy is\n$$\n\\frac{S}{k_{B}}=-\\sum_{i}p_{i}\\ln(p_{i}).\n$$\nSubstituting the given probabilities $p_{1}=0.6$, $p_{2}=0.3$, and $p_{3}=0.1$,\n$$\n\\frac{S}{k_{B}}=-(0.6\\ln 0.6+0.3\\ln 0.3+0.1\\ln 0.1).\n$$\nUsing $\\ln 0.6\\approx -0.5108256238$, $\\ln 0.3\\approx -1.2039728043$, and $\\ln 0.1\\approx -2.3025850930$, compute each term:\n$$\n0.6\\ln 0.6\\approx -0.3064953743,\\quad 0.3\\ln 0.3\\approx -0.3611918413,\\quad 0.1\\ln 0.1\\approx -0.2302585093.\n$$\nSumming,\n$$\n0.6\\ln 0.6+0.3\\ln 0.3+0.1\\ln 0.1\\approx -0.8979457249.\n$$\nApplying the overall negative sign,\n$$\n\\frac{S}{k_{B}}\\approx 0.8979457249.\n$$\nRounding to three significant figures gives\n$$\n\\frac{S}{k_{B}}\\approx 0.898.\n$$", "answer": "$$\\boxed{0.898}$$", "id": "1967970"}, {"introduction": "Now that we can calculate entropy for a given state, a natural and profound question arises: what is the maximum possible entropy a system can have? This practice explores the fundamental Principle of Maximum Entropy, which is a cornerstone of statistical physics and information theory. By determining the entropy-maximizing probability distribution for a system with a fixed number of states, you will discover the crucial link between maximum entropy and maximum uncertainty, where all outcomes are equally likely. [@problem_id:1968005]", "problem": "A single organic molecule is being investigated for its potential use in a futuristic nanoscale memory device. Theoretical models suggest that the molecule can exist in exactly five distinct and accessible conformational quantum states. Let the probability of finding the molecule in the $i$-th state (where $i=1, 2, 3, 4, 5$) be denoted by $p_i$. The set of probabilities $\\{p_i\\}$ is normalized, such that $\\sum_{i=1}^5 p_i = 1$.\n\nThe statistical state of the molecule is described by the Gibbs entropy, given by the formula $S = -k_B \\sum_{i=1}^5 p_i \\ln(p_i)$, where $k_B$ is the Boltzmann constant.\n\nDetermine the maximum possible value for the Gibbs entropy, $S_{max}$, of this single-molecule system. Express your answer as an analytic expression in terms of the Boltzmann constant, $k_B$.", "solution": "We seek to maximize the Gibbs entropy $S=-k_{B}\\sum_{i=1}^{5} p_{i}\\ln(p_{i})$ subject to the normalization constraint $\\sum_{i=1}^{5} p_{i}=1$ and $p_{i}\\geq 0$. Introduce a Lagrange multiplier $\\lambda$ for the constraint and consider the Lagrangian\n$$\n\\mathcal{L}=-k_{B}\\sum_{i=1}^{5} p_{i}\\ln(p_{i})+\\lambda\\left(\\sum_{i=1}^{5} p_{i}-1\\right).\n$$\nSetting the partial derivatives with respect to each $p_{i}$ to zero gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{i}}=-k_{B}\\left(\\ln(p_{i})+1\\right)+\\lambda=0,\n$$\nwhich implies\n$$\n\\ln(p_{i})=\\frac{\\lambda}{k_{B}}-1 \\quad \\Rightarrow \\quad p_{i}=\\exp\\!\\left(\\frac{\\lambda}{k_{B}}-1\\right).\n$$\nThus all $p_{i}$ are equal. Using the normalization constraint,\n$$\n\\sum_{i=1}^{5} p_{i}=5\\,p=1 \\quad \\Rightarrow \\quad p=\\frac{1}{5}.\n$$\nSubstituting $p_{i}=\\frac{1}{5}$ into the entropy,\n$$\nS_{\\max}=-k_{B}\\sum_{i=1}^{5} \\frac{1}{5}\\ln\\!\\left(\\frac{1}{5}\\right)=-k_{B}\\ln\\!\\left(\\frac{1}{5}\\right)=k_{B}\\ln(5).\n$$\nTo confirm this is a maximum, note the concavity of $-k_{B} p\\ln p$ in $p>0$ since\n$$\n\\frac{\\partial^{2}}{\\partial p_{i}^{2}}\\left(-k_{B} p_{i}\\ln p_{i}\\right)=-\\frac{k_{B}}{p_{i}}<0,\n$$\nso the stationary point under a linear constraint is the global maximum.", "answer": "$$\\boxed{k_{B}\\ln(5)}$$", "id": "1968005"}, {"introduction": "Entropy is not just a static property but is central to how systems evolve over time, a concept captured by the Second Law of Thermodynamics. This final practice applies our understanding to a dynamic process, calculating the change in entropy as a system relaxes from an ordered, low-entropy state to a disordered, high-entropy one. By modeling a memory cell that \"forgets\" its information, you will see how an increase in statistical entropy corresponds to a tangible loss of order and information. [@problem_id:1968021]", "problem": "Consider a simplified model for a memory cell based on the spin of a single electron. The electron's spin can be in one of two states: \"spin up\" or \"spin down\". Initially, the cell is in a well-ordered, \"written\" state where the probability of finding the electron's spin \"up\" is $p_{\\text{up, initial}} = 0.90$ and the probability of finding it \"spin down\" is $p_{\\text{down, initial}} = 0.10$. Over time, due to thermal interactions with its environment, the cell relaxes into a \"forgotten\" state where both spin configurations are equally likely, meaning the new probabilities are $p_{\\text{up, final}} = 0.50$ and $p_{\\text{down, final}} = 0.50$.\n\nUsing the Boltzmann constant $k_B = 1.38 \\times 10^{-23}$ J/K, calculate the change in the statistical entropy associated with the electron's spin as it transitions from the initial \"written\" state to the final \"forgotten\" state. Express your answer in Joules per Kelvin (J/K), rounded to three significant figures.", "solution": "The problem asks for the change in the statistical entropy, $\\Delta S$, of a two-state system as its state probabilities change. The statistical entropy, also known as the Gibbs entropy, for a system with a discrete set of states is given by the formula:\n$$S = -k_B \\sum_{i} p_i \\ln(p_i)$$\nwhere $k_B$ is the Boltzmann constant, and $p_i$ is the probability of the system being in state $i$. The sum is over all possible states. In our case, the states are \"spin up\" and \"spin down\".\n\nThe change in entropy is the difference between the final entropy and the initial entropy:\n$$\\Delta S = S_{\\text{final}} - S_{\\text{initial}}$$\n\nFirst, we calculate the initial entropy, $S_{\\text{initial}}$, using the initial probabilities $p_{\\text{up, initial}} = 0.90$ and $p_{\\text{down, initial}} = 0.10$.\n$$S_{\\text{initial}} = -k_B \\left( p_{\\text{up, initial}} \\ln(p_{\\text{up, initial}}) + p_{\\text{down, initial}} \\ln(p_{\\text{down, initial}}) \\right)$$\n$$S_{\\text{initial}} = -k_B \\left( 0.90 \\ln(0.90) + 0.10 \\ln(0.10) \\right)$$\n\nNext, we calculate the final entropy, $S_{\\text{final}}$, using the final probabilities $p_{\\text{up, final}} = 0.50$ and $p_{\\text{down, final}} = 0.50$.\n$$S_{\\text{final}} = -k_B \\left( p_{\\text{up, final}} \\ln(p_{\\text{up, final}}) + p_{\\text{down, final}} \\ln(p_{\\text{down, final}}) \\right)$$\n$$S_{\\text{final}} = -k_B \\left( 0.50 \\ln(0.50) + 0.50 \\ln(0.50) \\right)$$\n$$S_{\\text{final}} = -k_B \\left( 2 \\times 0.50 \\ln(0.50) \\right) = -k_B \\ln(0.50)$$\nUsing the property of logarithms $\\ln(1/x) = -\\ln(x)$, we can write:\n$$S_{\\text{final}} = -k_B \\ln\\left(\\frac{1}{2}\\right) = -k_B (-\\ln(2)) = k_B \\ln(2)$$\n\nNow, we can find the change in entropy $\\Delta S$:\n$$\\Delta S = S_{\\text{final}} - S_{\\text{initial}}$$\n$$\\Delta S = k_B \\ln(2) - \\left[ -k_B \\left( 0.90 \\ln(0.90) + 0.10 \\ln(0.10) \\right) \\right]$$\n$$\\Delta S = k_B \\left[ \\ln(2) + 0.90 \\ln(0.90) + 0.10 \\ln(0.10) \\right]$$\n\nNow we substitute the numerical values for the logarithms:\n$\\ln(2) \\approx 0.693147$\n$\\ln(0.90) \\approx -0.105361$\n$\\ln(0.10) \\approx -2.302585$\n\nSubstituting these into the expression for $\\Delta S$:\n$$\\Delta S \\approx k_B \\left[ 0.693147 + 0.90(-0.105361) + 0.10(-2.302585) \\right]$$\n$$\\Delta S \\approx k_B \\left[ 0.693147 - 0.094825 - 0.230259 \\right]$$\n$$\\Delta S \\approx k_B \\left[ 0.368063 \\right]$$\n\nFinally, we substitute the value of the Boltzmann constant, $k_B = 1.38 \\times 10^{-23}$ J/K.\n$$\\Delta S \\approx (1.38 \\times 10^{-23} \\text{ J/K}) \\times 0.368063$$\n$$\\Delta S \\approx 5.0792694 \\times 10^{-24} \\text{ J/K}$$\n\nThe problem requires the answer to be rounded to three significant figures.\n$$\\Delta S \\approx 5.08 \\times 10^{-24} \\text{ J/K}$$", "answer": "$$\\boxed{5.08 \\times 10^{-24}}$$", "id": "1968021"}]}