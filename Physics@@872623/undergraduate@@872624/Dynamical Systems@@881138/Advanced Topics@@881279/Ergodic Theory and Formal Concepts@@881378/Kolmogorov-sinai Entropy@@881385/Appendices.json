{"hands_on_practices": [{"introduction": "The concept of Kolmogorov-Sinai (KS) entropy is a direct generalization of Shannon's entropy from information theory. This first exercise provides a foundational link between these two ideas by examining a simple stochastic process. By comparing a predictable system (a biased coin) with a completely unpredictable one (a fair coin), you will develop a core intuition for how KS entropy quantifies the rate of information generation and uncertainty in a system [@problem_id:1688717].", "problem": "In the study of dynamical systems, the Kolmogorov-Sinai (KS) entropy is a fundamental concept that quantifies the rate of information generation, or the level of unpredictability, of a system over time. For a simple stochastic process consisting of a sequence of independent and identically distributed (i.i.d.) trials, the KS entropy, $h_{KS}$, measured in bits per trial, is given by the Shannon entropy formula:\n$$h_{KS} = - \\sum_{i=1}^{N} p_i \\log_2(p_i)$$\nwhere $N$ is the number of possible outcomes for each trial and $p_i$ is the probability of the $i$-th outcome.\n\nConsider two such processes, each with two possible outcomes, which we can label 'heads' (H) and 'tails' (T).\n\n1.  A **fair process**, where the probabilities of the two outcomes are equal: $p_H = 0.5$ and $p_T = 0.5$.\n2.  A **biased process**, where the probability of 'heads' is $p'_H = 0.9$ and the probability of 'tails' is $p'_T = 0.1$.\n\nCalculate the numerical ratio of the KS entropy of the biased process to the KS entropy of the fair process. Round your final answer to three significant figures.", "solution": "We are given the Kolmogorov-Sinai (KS) entropy for an i.i.d. process in bits per trial as the Shannon entropy\n$$\nh_{KS} = - \\sum_{i=1}^{N} p_{i} \\log_{2}(p_{i}).\n$$\nFor the fair process with $p_{H} = \\frac{1}{2}$ and $p_{T} = \\frac{1}{2}$, the entropy is\n$$\nh_{\\text{fair}} = -\\left(\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2} + \\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\right) = -\\log_{2}\\tfrac{1}{2} = 1,\n$$\nsince $\\log_{2}\\tfrac{1}{2} = -1$.\n\nFor the biased process with $p'_{H} = 0.9$ and $p'_{T} = 0.1$, the entropy is\n$$\nh_{\\text{biased}} = -\\left(0.9\\,\\log_{2}(0.9) + 0.1\\,\\log_{2}(0.1)\\right).\n$$\nUsing the change-of-base identity $\\log_{2}(x) = \\frac{\\ln(x)}{\\ln(2)}$ to evaluate numerically,\n$$\n\\log_{2}(0.9) \\approx -0.152003093,\\qquad \\log_{2}(0.1) \\approx -3.321928095,\n$$\nso\n$$\nh_{\\text{biased}} \\approx -\\left(0.9 \\cdot (-0.152003093) + 0.1 \\cdot (-3.321928095)\\right) \\approx 0.468995594.\n$$\nThe required ratio is\n$$\n\\frac{h_{\\text{biased}}}{h_{\\text{fair}}} = \\frac{0.468995594}{1} \\approx 0.468995594.\n$$\nRounding to three significant figures gives $0.469$.", "answer": "$$\\boxed{0.469}$$", "id": "1688717"}, {"introduction": "While it is intuitive that random processes generate information, one of the most profound ideas in chaos theory is that purely deterministic systems can also be unpredictable. This practice explores a deterministic map that models the mixing of a fluid by uniformly stretching and compressing space. You will apply Pesin's identity, a key theorem that directly connects the geometric action of the map—measured by its positive Lyapunov exponents—to its KS entropy, revealing how chaos generates complexity [@problem_id:1688770].", "problem": "Consider a simplified two-dimensional model for the mixing of a fluid within a square container. The state of any fluid particle is represented by a point in the unit square, $[0, 1] \\times [0, 1]$. The dynamics are observed at discrete time intervals. After one time step, an infinitesimally small rectangular region of the fluid, initially with sides aligned with the coordinate axes, is transformed into a new rectangular region. This transformation uniformly stretches the region's dimension along the x-axis by a factor of $\\sigma = 3$ and compresses its dimension along the y-axis by a factor of $1/\\sigma = 1/3$. This stretching and compression behavior is the same at every point in the unit square and for every time step. The system is area-preserving.\n\nFor such a system, the rate of information generation due to its chaotic behavior is quantified by the Kolmogorov-Sinai (KS) entropy per iteration. Calculate the exact value of the KS entropy, $h_{KS}$, for this dynamical system. Express your answer as an analytic expression.", "solution": "We consider a smooth, invertible, area-preserving map on the unit square that uniformly stretches by a factor $\\sigma$ along the $x$-axis and compresses by a factor $1/\\sigma$ along the $y$-axis at each iteration. The local linearization (Jacobian) of the map at any point is\n$$\nJ=\\begin{pmatrix}\n\\sigma  0\\\\\n0  \\frac{1}{\\sigma}\n\\end{pmatrix},\n$$\nwhich satisfies $\\det J=\\sigma\\cdot \\frac{1}{\\sigma}=1$, confirming area preservation.\n\nThe Lyapunov exponents are given by the logarithms of the moduli of the eigenvalues (equivalently, singular values) of $J$. Since the map is uniform in space and time, these exponents are constant and equal to\n$$\n\\lambda_{1}=\\ln \\sigma,\\qquad \\lambda_{2}=-\\ln \\sigma.\n$$\nBy Pesin's identity for smooth invertible maps preserving an SRB (in this case Lebesgue) measure, the Kolmogorov-Sinai entropy equals the sum of the positive Lyapunov exponents:\n$$\nh_{KS}=\\sum_{\\lambda_{i}0}\\lambda_{i}=\\ln \\sigma.\n$$\nSubstituting the given $\\sigma=3$, we obtain\n$$\nh_{KS}=\\ln 3.\n$$\nThis is the KS entropy per iteration.", "answer": "$$\\boxed{\\ln 3}$$", "id": "1688770"}, {"introduction": "Many complex dynamical systems can be understood by simplifying their behavior into sequences of symbols, a technique known as symbolic dynamics. This exercise models a physical system with a simple constraint: the symbol '1' cannot be followed by another '1'. The KS entropy in this context, also known as the topological entropy, measures the exponential growth rate of the number of allowed sequences. This problem demonstrates a powerful combinatorial method for calculating entropy by analyzing the \"language\" of the system [@problem_id:1688749].", "problem": "Consider a simplified model for a one-dimensional optical data storage system. Information is encoded as sequences of bits, where a bit can be in one of two states, labeled `0` and `1`. Due to thermal interactions between adjacent storage sites, a state of `1` cannot be followed immediately by another state of `1`. Consequently, any valid sequence of bits recorded on the medium must not contain the subsequence `11`.\n\nThe information capacity of such a system is related to its Kolmogorov-Sinai (KS) entropy, a concept from dynamical systems theory. For this type of symbolic system, the KS entropy, denoted by $h$, can be calculated as the asymptotic growth rate of the number of allowed finite sequences. Specifically, it is given by the limit:\n$$h = \\lim_{n \\to \\infty} \\frac{1}{n} \\ln(N_n)$$\nwhere $N_n$ is the total number of distinct, valid binary sequences of length $n$ that adhere to the given constraint.\n\nYour task is to calculate the exact value of the KS entropy $h$ for this data storage model. Present your answer as a closed-form analytic expression.", "solution": "Let $N_{n}$ denote the number of valid binary sequences of length $n$ with no occurrence of the subsequence $11$. Partition valid sequences of length $n$ by their initial symbols:\n- If the first symbol is $0$, the remaining $n-1$ symbols form any valid sequence of length $n-1$, which contributes $N_{n-1}$ possibilities.\n- If the first symbol is $1$, the second symbol must be $0$, and the remaining $n-2$ symbols form any valid sequence of length $n-2$, which contributes $N_{n-2}$ possibilities.\n\nTherefore,\n$$\nN_{n}=N_{n-1}+N_{n-2}.\n$$\nThe initial conditions are $N_{0}=1$ (empty sequence) and $N_{1}=2$ (the sequences $0$ and $1$).\n\nSolve the linear recurrence using the characteristic equation\n$$\nr^{2}-r-1=0,\n$$\nwhose roots are\n$$\n\\phi=\\frac{1+\\sqrt{5}}{2},\\quad \\psi=\\frac{1-\\sqrt{5}}{2}.\n$$\nThe general form is\n$$\nN_{n}=C_{1}\\phi^{n}+C_{2}\\psi^{n},\n$$\nwith constants $C_{1},C_{2}$ determined by the initial conditions. For the asymptotic growth rate, it suffices to note that $|\\psi|1$ and $\\phi1$, so\n$$\nN_{n}=C_{1}\\phi^{n}\\left(1+\\frac{C_{2}}{C_{1}}\\left(\\frac{\\psi}{\\phi}\\right)^{n}\\right).\n$$\nTaking logarithms and dividing by $n$ gives\n$$\n\\frac{1}{n}\\ln(N_{n})=\\frac{1}{n}\\ln C_{1}+\\ln \\phi+\\frac{1}{n}\\ln\\left(1+\\frac{C_{2}}{C_{1}}\\left(\\frac{\\psi}{\\phi}\\right)^{n}\\right).\n$$\nAs $n\\to\\infty$, the first and third terms tend to $0$ because $C_{1}$ is a constant and $\\left|\\frac{\\psi}{\\phi}\\right|1$. Hence,\n$$\n\\lim_{n\\to\\infty}\\frac{1}{n}\\ln(N_{n})=\\ln \\phi=\\ln\\!\\left(\\frac{1+\\sqrt{5}}{2}\\right).\n$$\nBy the definition given, this limit equals the KS entropy $h$ of the system.", "answer": "$$\\boxed{\\ln\\!\\left(\\frac{1+\\sqrt{5}}{2}\\right)}$$", "id": "1688749"}]}