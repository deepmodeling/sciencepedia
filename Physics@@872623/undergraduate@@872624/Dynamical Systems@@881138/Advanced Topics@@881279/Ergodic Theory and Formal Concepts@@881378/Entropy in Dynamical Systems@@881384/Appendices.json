{"hands_on_practices": [{"introduction": "Topological entropy quantifies the complexity of a dynamical system by measuring the exponential growth rate of distinguishable orbits. This first exercise provides a foundational understanding by guiding you through a direct calculation based on its most fundamental definition: counting the number of allowed sequences in a simple symbolic system. This hands-on counting approach builds a strong intuition for what entropy represents in the context of discrete states and transition rules [@problem_id:1674438].", "problem": "Consider an idealized one-dimensional discrete-state system. The system is modeled as an infinite chain of sites, indexed by the integers $\\mathbb{Z}$. At each site $i$, the system can be in one of three possible states, which we label $\\{A, B, C\\}$. A configuration of the entire system is a bi-infinite sequence $s = (\\dots, s_{-1}, s_0, s_1, s_2, \\dots)$, where each $s_i \\in \\{A, B, C\\}$.\n\nA fundamental interaction rule restricts the possible configurations: no two adjacent sites can be in the same state. This means that for any valid configuration $s$, the condition $s_i \\neq s_{i+1}$ must hold for all integers $i$. The set of all such valid configurations constitutes the phase space $X$ of the system.\n\nThe dynamics of the system are governed by the left shift map, $T: X \\to X$, defined by $(T(s))_i = s_{i+1}$. This map shifts the entire configuration one site to the left at each time step.\n\nThe complexity of this dynamical system can be quantified by its topological entropy. For a symbolic system of this nature, the topological entropy, $h(T)$, is a measure of the exponential growth rate of the number of distinguishable finite-length state sequences. It is given by the formula:\n$$h(T) = \\lim_{n \\to \\infty} \\frac{1}{n} \\ln N_n$$\nwhere $N_n$ denotes the number of distinct allowed sequences of length $n$. An allowed sequence of length $n$, also known as a word, is a finite sequence $(w_1, w_2, \\dots, w_n)$ with $w_k \\in \\{A, B, C\\}$ that satisfies the adjacent-site constraint, i.e., $w_k \\neq w_{k+1}$ for $1 \\le k < n$.\n\nCalculate the topological entropy $h(T)$ for this system. Present your answer as a closed-form analytic expression.", "solution": "We count allowed words directly. An allowed word of length $n$ is any sequence $(w_{1},\\dots,w_{n})$ with $w_{k} \\in \\{A,B,C\\}$ and $w_{k} \\neq w_{k+1}$ for $1 \\leq k < n$. The first symbol has $3$ choices. Given any valid prefix of length $n-1$, the last symbol determines that the next symbol can be any of the remaining $2$ symbols. Therefore, the number $N_{n}$ of allowed words of length $n$ satisfies the recurrence\n$$\nN_{1}=3,\\qquad N_{n}=2\\,N_{n-1}\\quad\\text{for }n\\geq 2,\n$$\nwhich solves to\n$$\nN_{n}=3\\cdot 2^{\\,n-1}.\n$$\nUsing the defining limit for topological entropy,\n$$\nh(T)=\\lim_{n\\to \\infty}\\frac{1}{n}\\ln N_{n}=\\lim_{n\\to \\infty}\\frac{1}{n}\\left(\\ln 3+(n-1)\\ln 2\\right)=\\ln 2.\n$$\nEquivalently, viewing the system as a subshift of finite type with adjacency matrix\n$$\nA=\\begin{pmatrix}\n0 & 1 & 1\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\n\\end{pmatrix}=J-I,\n$$\nwhere $J$ is the all-ones matrix, the eigenvalues of $A$ are $2,-1,-1$, so the spectral radius is $2$, and the general formula for subshifts of finite type gives $h(T)=\\ln \\rho(A)=\\ln 2$, in agreement with the direct count.", "answer": "$$\\boxed{\\ln 2}$$", "id": "1674438"}, {"introduction": "While symbolic systems offer a clear way to enumerate configurations, how do we measure complexity for continuous systems, like a point moving on an interval? This problem explores topological entropy for a piecewise linear map, a common model in chaos theory. By identifying how many segments of the domain are stretched over the entire space, you'll use a powerful geometric shortcut to calculate entropy, linking the abstract concept to the tangible actions of stretching and folding [@problem_id:1674436].", "problem": "In the study of one-dimensional dynamical systems, topological entropy is a non-negative real number that serves as a measure of the complexity of the system. For a certain class of piecewise-defined maps $T: [0, 1] \\to [0, 1]$, the topological entropy, denoted $h_{top}(T)$, can be determined by a straightforward method. Specifically, if the domain $[0,1]$ can be partitioned into a finite number of subintervals on which the map acts in a simple way, the entropy is given by $h_{top}(T) = \\ln(k)$. Here, $k$ is the total count of those subintervals from the partition whose image under the map $T$ is the entire interval $[0, 1]$.\n\nConsider the following map $T$, which is defined on the interval $[0, 1]$ and maps it to itself:\n$$ T(x) = \\begin{cases} 3x & \\text{if } 0 \\le x \\le 1/3 \\\\ 1/2 & \\text{if } 1/3 < x < 2/3 \\\\ 3x-2 & \\text{if } 2/3 \\le x \\le 1 \\end{cases} $$\nCalculate the topological entropy $h_{top}(T)$ for this map. Present your answer as a single closed-form analytic expression.", "solution": "The problem asks for the topological entropy $h_{top}(T)$ of a given piecewise map $T: [0, 1] \\to [0, 1]$. We are provided with a specific formula for this class of maps: $h_{top}(T) = \\ln(k)$, where $k$ is the number of subintervals in the domain partition whose image under $T$ is the entire interval $[0, 1]$.\n\nThe map is defined as:\n$$ T(x) = \\begin{cases} 3x & \\text{if } 0 \\le x \\le 1/3 \\\\ 1/2 & \\text{if } 1/3 < x < 2/3 \\\\ 3x-2 & \\text{if } 2/3 \\le x \\le 1 \\end{cases} $$\n\nThe domain $[0, 1]$ is naturally partitioned by this definition into three subintervals:\n1.  $I_1 = [0, 1/3]$\n2.  $I_2 = (1/3, 2/3)$\n3.  $I_3 = [2/3, 1]$\n\nWe need to find the image of each of these subintervals under the map $T$ to determine the value of $k$.\n\nFor the first subinterval, $I_1 = [0, 1/3]$, the map is $T(x) = 3x$. This is a linear function. To find the image of the interval, we can evaluate the function at the endpoints of the interval.\nAt $x = 0$, $T(0) = 3(0) = 0$.\nAt $x = 1/3$, $T(1/3) = 3(1/3) = 1$.\nSince $T(x)$ is continuous and increasing on this interval, the image of $I_1$ is the interval $[T(0), T(1/3)] = [0, 1]$.\nSo, the image of the first subinterval is the entire interval $[0, 1]$. This subinterval contributes to the count $k$.\n\nFor the second subinterval, $I_2 = (1/3, 2/3)$, the map is the constant function $T(x) = 1/2$. The image of this entire subinterval is the single point $\\{1/2\\}$. This is not the interval $[0, 1]$, so this subinterval does not contribute to the count $k$.\n\nFor the third subinterval, $I_3 = [2/3, 1]$, the map is $T(x) = 3x - 2$. This is also a linear function. We evaluate the function at the endpoints of the interval.\nAt $x = 2/3$, $T(2/3) = 3(2/3) - 2 = 2 - 2 = 0$.\nAt $x = 1$, $T(1) = 3(1) - 2 = 1$.\nSince $T(x)$ is continuous and increasing on this interval, the image of $I_3$ is the interval $[T(2/3), T(1)] = [0, 1]$.\nSo, the image of the third subinterval is also the entire interval $[0, 1]$. This subinterval also contributes to the count $k$.\n\nNow we count the number of subintervals, $k$, whose image is $[0, 1]$. We found that the subintervals $I_1 = [0, 1/3]$ and $I_3 = [2/3, 1]$ both map onto $[0, 1]$. Therefore, the value of $k$ is 2.\n\nUsing the formula provided in the problem statement, the topological entropy is:\n$h_{top}(T) = \\ln(k) = \\ln(2)$.", "answer": "$$\\boxed{\\ln(2)}$$", "id": "1674436"}, {"introduction": "Topological entropy treats all allowed behaviors as equally important, but what if some outcomes are more likely than others? Metric entropy addresses this by incorporating probabilities, a concept deeply connected to information theory. This practical problem [@problem_id:1674450] frames metric entropy in the language of data compression, calculating the \"inefficiency\" of a suboptimal code, which highlights the connection between the abstract entropy of a system and the tangible cost of encoding information about it.", "problem": "A deep-space probe is performing spectral analysis on the atmosphere of a newly discovered exoplanet. The probe's primary sensor detects a continuous stream of particles corresponding to one of three atmospheric gases, which mission scientists have named Azurine (A), Beryl-Hydride (B), and Carbyl (C).\n\nFor the initial design of the data compression and transmission system, engineers made a simplifying assumption that each gas would be detected with equal probability. This assumption formed the basis of an optimal encoding scheme intended to minimize the average number of bits transmitted per detection.\n\nHowever, after a year of data collection, analysis reveals a different reality. The long-term empirical probabilities of detecting each gas are found to be $p_A = 0.900$, $p_B = 0.050$, and $p_C = 0.050$. The probe continues to use the original encoding scheme based on the outdated equal-probability model. This mismatch results in an inefficient use of the communication channel.\n\nYour task is to quantify this inefficiency. Calculate the average number of \"wasted\" bits per symbol transmitted. This value is defined as the difference between the average number of bits per symbol required by the original, suboptimal encoding scheme (when applied to the actual data stream) and the theoretical minimum average number of bits per symbol that would be required by a new encoding scheme perfectly optimized for the empirical probabilities.\n\nProvide your answer in units of bits per symbol, rounded to three significant figures. All calculations should use the base-2 logarithm.", "solution": "Let the true symbol distribution be $p = (p_{A},p_{B},p_{C}) = (0.900,0.050,0.050)$ and the model used to design the original code be the uniform $q = (q_{A},q_{B},q_{C}) = \\left(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right)$.\n\nAn optimal code for the model $q$ assigns code lengths $l_{i} = -\\log_{2}(q_{i})$, so the average length under the true distribution $p$ (the cross-entropy) is\n$$\nL_{\\text{old}} = -\\sum_{i} p_{i}\\log_{2}(q_{i}) \\, .\n$$\nFor $q_{i} = \\frac{1}{3}$, this simplifies to\n$$\nL_{\\text{old}} = -\\sum_{i} p_{i}\\log_{2}\\!\\left(\\tfrac{1}{3}\\right) = \\left(\\sum_{i} p_{i}\\right)\\log_{2}(3) = \\log_{2}(3) \\, .\n$$\n\nThe theoretical minimum average length achievable with a code optimized for $p$ is the entropy\n$$\nH(p) = -\\sum_{i} p_{i}\\log_{2}(p_{i}) \\, .\n$$\nTherefore, the wasted bits per symbol are\n$$\nW = L_{\\text{old}} - H(p) = \\log_{2}(3) + \\sum_{i} p_{i}\\log_{2}(p_{i})\n= \\sum_{i} p_{i}\\log_{2}\\!\\left(\\frac{p_{i}}{q_{i}}\\right) \\, .\n$$\nNow compute $H(p)$:\n$$\nH(p) = -\\left[0.900\\,\\log_{2}(0.900) + 0.050\\,\\log_{2}(0.050) + 0.050\\,\\log_{2}(0.050)\\right] \\, .\n$$\nUsing base-2 logarithms,\n$$\n\\log_{2}(0.900) \\approx -0.152003093,\\quad \\log_{2}(0.050) = -4.321928095 \\, ,\n$$\nso\n$$\nH(p) \\approx -\\left[0.900(-0.152003093) + 0.050(-4.321928095) + 0.050(-4.321928095)\\right]\n= 0.568995594 \\text{ bits/symbol} \\, .\n$$\nThus,\n$$\nW = \\log_{2}(3) - H(p) \\approx 1.584962501 - 0.568995594 = 1.015966907 \\text{ bits/symbol} \\, .\n$$\nRounded to three significant figures, the wasted bits per symbol are $1.02$.", "answer": "$$\\boxed{1.02}$$", "id": "1674450"}]}