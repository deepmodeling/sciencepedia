{"hands_on_practices": [{"introduction": "The Picard-Lindelöf theorem hinges on a crucial property known as Lipschitz continuity. This exercise provides direct practice in verifying this condition, which is a cornerstone for guaranteeing the existence and uniqueness of solutions to Ordinary Differential Equations (ODEs) [@problem_id:1675257]. By analyzing the derivative of the function $f(t,y)$, you will learn a powerful technique to confirm if a dynamical system behaves predictably.", "problem": "In the context of the existence and uniqueness of solutions for ordinary differential equations, we often analyze the properties of the function on the right-hand side of the equation $y' = f(t, y)$. Consider the specific function $f(y) = \\arctan(y)$, which is independent of the variable $t$. Which of the following statements about this function is correct?\n\nA. The function $f(y) = \\arctan(y)$ is not globally Lipschitz continuous because its derivative is not constant.\n\nB. The function $f(y) = \\arctan(y)$ is globally Lipschitz continuous because its derivative has a global maximum.\n\nC. The function $f(y) = \\arctan(y)$ is not globally Lipschitz continuous because the function itself is unbounded.\n\nD. The function $f(y) = \\arctan(y)$ is globally Lipschitz continuous because the function itself is bounded.", "solution": "We recall the definition: a function $f:\\mathbb{R}\\to\\mathbb{R}$ is globally Lipschitz continuous if there exists $L \\geq 0$ such that for all $y_{1},y_{2}\\in\\mathbb{R}$,\n$$\n|f(y_{1})-f(y_{2})| \\leq L\\,|y_{1}-y_{2}|.\n$$\nIf $f$ is differentiable on $\\mathbb{R}$ and its derivative is bounded, then by the Mean Value Theorem there exists $\\xi$ between $y_{1}$ and $y_{2}$ such that\n$$\nf(y_{1})-f(y_{2}) = f'(\\xi)\\,(y_{1}-y_{2}),\n$$\nhence\n$$\n|f(y_{1})-f(y_{2})| \\leq \\sup_{y\\in\\mathbb{R}}|f'(y)| \\, |y_{1}-y_{2}|.\n$$\nTherefore, boundedness of $f'$ implies global Lipschitz continuity with Lipschitz constant $L=\\sup_{y\\in\\mathbb{R}}|f'(y)|$.\n\nFor $f(y)=\\arctan(y)$, we compute\n$$\nf'(y)=\\frac{1}{1+y^{2}}.\n$$\nSince $0\\frac{1}{1+y^{2}}\\leq 1$ for all $y\\in\\mathbb{R}$ and the global maximum is attained at $y=0$, we have\n$$\n\\sup_{y\\in\\mathbb{R}}|f'(y)| = 1.\n$$\nThus, $f$ is globally Lipschitz with $L=1$.\n\nAssessing the options:\n- A is false: a nonconstant derivative does not preclude global Lipschitz continuity; what matters is boundedness of the derivative, not constancy.\n- B is true: $f'$ has a global maximum (equal to $1$), hence $f$ is globally Lipschitz.\n- C is false: $\\arctan(y)$ is bounded, not unbounded.\n- D is false as a justification: although $\\arctan(y)$ is bounded (between $-\\frac{\\pi}{2}$ and $\\frac{\\pi}{2}$), mere boundedness of a function does not imply global Lipschitz continuity; the correct reason is the bounded derivative.", "answer": "$$\\boxed{B}$$", "id": "1675257"}, {"introduction": "While the Lipschitz condition guarantees uniqueness, what happens when it is not met? This problem explores a classic and insightful initial value problem, $y'(t) = |y(t)|^{\\alpha}$ with $y(0)=0$, where the uniqueness of the solution depends critically on the parameter $\\alpha$ [@problem_id:1675254]. By determining the threshold for $\\alpha$, you will gain a deeper appreciation for why the Lipschitz condition is so fundamental to the theory of ODEs.", "problem": "Consider the Initial Value Problem (IVP) defined by the ordinary differential equation $y'(t) = |y(t)|^{\\alpha}$ with the initial condition $y(0) = 0$. In this equation, $\\alpha$ represents a positive real constant.\n\nWe are given that the trivial solution, $y(t) = 0$ for all $t$, is always a valid solution for this IVP. The central question concerns the uniqueness of this solution: under what conditions on $\\alpha$ can we be certain that no other, non-trivial solutions exist?\n\nBased on the standard existence and uniqueness theorems for first-order ODEs, identify the range of $\\alpha$ for which the solution to this IVP is guaranteed to be unique within some neighborhood of $t=0$.\n\nA. $\\alpha  1$\n\nB. $\\alpha \\ge 1$\n\nC. $\\alpha  1$\n\nD. $\\alpha \\le 1$\n\nE. For all positive $\\alpha$", "solution": "We consider the IVP $y'(t)=|y(t)|^{\\alpha}$ with $y(0)=0$, where $\\alpha0$. Define $f(y)=|y|^{\\alpha}$. Existence of a solution near $t=0$ follows from continuity of $f$. For uniqueness via the Picard–Lindelöf theorem, $f$ must be locally Lipschitz in $y$ near $y=0$.\n\nFirst, we determine when $f$ is locally Lipschitz at $0$.\n- If $\\alpha=1$, then $f(y)=|y|$ and the inequality\n$$\n\\big||y|-|z|\\big|\\le |y-z|\n$$\nholds for all $y,z\\in\\mathbb{R}$. Hence $f$ is globally Lipschitz with constant $1$.\n- If $\\alpha1$, fix $r0$ and suppose $|y|\\le r$, $|z|\\le r$. By the mean value theorem applied to $g(s)=s^{\\alpha}$ on $[0,r]$, there exists $\\xi$ between $|y|$ and $|z|$ such that\n$$\n\\big||y|^{\\alpha}-|z|^{\\alpha}\\big|=\\alpha\\,\\xi^{\\alpha-1}\\,\\big||y|-|z|\\big|\\le \\alpha r^{\\alpha-1}\\,|y-z|.\n$$\nThus $f$ is locally Lipschitz near $0$.\n- If $0\\alpha1$, $f$ is not locally Lipschitz at $0$. Indeed, taking $z=0$,\n$$\n\\frac{\\big||y|^{\\alpha}-0\\big|}{|y-0|}=|y|^{\\alpha-1}\\to \\infty \\quad \\text{as } y\\to 0,\n$$\nso no Lipschitz constant exists in any neighborhood of $0$.\n\nTherefore, by the Picard–Lindelöf uniqueness theorem, the IVP has a unique solution near $t=0$ for $\\alpha\\ge 1$.\n\nTo see that uniqueness fails for $0\\alpha1$, one can use either Osgood’s criterion or construct explicit nontrivial solutions. Osgood’s condition for $y'=g(y)$ with $g(0)=0$ states that uniqueness at $y=0$ holds if and only if\n$$\n\\int_{0^{+}} \\frac{ds}{g(s)}=\\infty.\n$$\nHere $g(s)=s^{\\alpha}$, and\n$$\n\\int_{0^{+}} \\frac{ds}{s^{\\alpha}}\n$$\ndiverges if and only if $\\alpha\\ge 1$; it is finite when $0\\alpha1$, implying possible nonuniqueness in that range. Explicitly, for $0\\alpha1$ and any $T\\ge 0$, define\n$$\ny_{T}(t)=\n\\begin{cases}\n0,  t\\le T,\\\\\n\\left((1-\\alpha)(t-T)\\right)^{\\frac{1}{1-\\alpha}},  t\\ge T.\n\\end{cases}\n$$\nOne checks that $y_{T}$ is $C^{1}$, satisfies $y_{T}(0)=0$, and satisfies $y' = y^{\\alpha}$ for all $t$, providing infinitely many solutions and confirming nonuniqueness.\n\nHence, the solution is guaranteed to be unique near $t=0$ precisely for $\\alpha\\ge 1$.", "answer": "$$\\boxed{B}$$", "id": "1675254"}, {"introduction": "The proof of the existence and uniqueness theorem is constructive, meaning it provides an explicit method for building the solution. This powerful algorithm is known as Picard's method of successive approximations, which generates a sequence of functions that converge to the true solution [@problem_id:1675298]. In this exercise, you will apply this iterative process firsthand, gaining a tangible understanding of how a solution is systematically constructed from an initial guess and the differential equation itself.", "problem": "In a simplified model for the dynamics of a driven, damped system, a quantity of interest $y(t)$ evolves according to the first-order Ordinary Differential Equation (ODE):\n$$ \\frac{dy}{dt} = \\sin(t) - k y(t) $$\nwhere $k$ is a positive real constant representing a damping factor and the $\\sin(t)$ term represents a periodic driving force. The system is known to be at its zero state initially, meaning $y(0) = 0$.\n\nPicard's method of successive approximations provides a way to construct a sequence of functions, $y_n(t)$, that converge to the true solution of this Initial Value Problem (IVP). The process begins with an initial approximation $y_0(t)$ based on the initial condition. Each subsequent approximation is generated by integrating the governing ODE with the previous approximation substituted into it.\n\nYour task is to determine the third Picard approximation, $y_3(t)$, which is the third function generated by the iterative process after the initial guess $y_0(t)$. Express your final answer as an analytic function of $t$ and $k$.", "solution": "We apply Picard’s successive approximations to the IVP $y^{\\prime}(t)=\\sin(t)-k y(t)$ with $y(0)=0$. The iterative scheme is\n$$\ny_{n+1}(t)=y(0)+\\int_{0}^{t}\\left[\\sin(s)-k\\,y_{n}(s)\\right]\\,ds,\n$$\nstarting from $y_{0}(t)=0$.\n\nFirst approximation:\n$$\ny_{1}(t)=\\int_{0}^{t}\\sin(s)\\,ds=1-\\cos(t).\n$$\n\nSecond approximation:\n$$\ny_{2}(t)=\\int_{0}^{t}\\left[\\sin(s)-k\\,y_{1}(s)\\right]ds\n=\\int_{0}^{t}\\sin(s)\\,ds-k\\int_{0}^{t}\\left[1-\\cos(s)\\right]ds,\n$$\n$$\ny_{2}(t)=\\left[1-\\cos(t)\\right]-k\\left[t-\\sin(t)\\right]=1-\\cos(t)-k t+k\\sin(t).\n$$\n\nThird approximation:\n$$\ny_{3}(t)=\\int_{0}^{t}\\left[\\sin(s)-k\\,y_{2}(s)\\right]ds\n=\\int_{0}^{t}\\sin(s)\\,ds-k\\int_{0}^{t}\\left[1-\\cos(s)-k s+k\\sin(s)\\right]ds.\n$$\nCompute the integrals term by term:\n$$\n\\int_{0}^{t}\\sin(s)\\,ds=1-\\cos(t),\\quad \\int_{0}^{t}1\\,ds=t,\\quad \\int_{0}^{t}-\\cos(s)\\,ds=-\\sin(t),\n$$\n$$\n\\int_{0}^{t}-k s\\,ds=-\\frac{k}{2}t^{2},\\quad \\int_{0}^{t}k\\sin(s)\\,ds=k\\left[1-\\cos(t)\\right].\n$$\nTherefore,\n$$\n\\int_{0}^{t}y_{2}(s)\\,ds=t-\\sin(t)-\\frac{k}{2}t^{2}+k\\left[1-\\cos(t)\\right],\n$$\nand\n$$\ny_{3}(t)=\\left[1-\\cos(t)\\right]-k\\left[t-\\sin(t)-\\frac{k}{2}t^{2}+k\\left(1-\\cos(t)\\right)\\right].\n$$\nSimplifying,\n$$\ny_{3}(t)=\\left(1-k^{2}\\right)+\\left(k^{2}-1\\right)\\cos(t)+k\\sin(t)-k t+\\frac{k^{2}}{2}t^{2}\n= k\\sin(t)+\\left(k^{2}-1\\right)\\left[\\cos(t)-1\\right]-k t+\\frac{k^{2}}{2}t^{2}.\n$$\nThis is the third Picard approximation.", "answer": "$$\\boxed{k\\sin(t)+\\left(k^{2}-1\\right)\\left(\\cos(t)-1\\right)-k t+\\frac{k^{2}}{2}t^{2}}$$", "id": "1675298"}]}