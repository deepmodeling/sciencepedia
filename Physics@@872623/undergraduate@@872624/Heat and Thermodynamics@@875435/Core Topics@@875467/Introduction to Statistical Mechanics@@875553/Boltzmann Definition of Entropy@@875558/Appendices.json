{"hands_on_practices": [{"introduction": "The Boltzmann equation, $S = k_B \\ln W$, provides a powerful link between the microscopic details of a system (the number of microstates, $W$) and a macroscopic property (entropy, $S$). We begin with a direct and intuitive application: calculating the configurational entropy based on the number of ways to arrange a set of objects. This exercise helps build a concrete understanding of what a \"microstate\" represents in a simple, countable context, drawing an analogy between molecular arrangements and the letters in a word. [@problem_id:1844385]", "problem": "In a simplified biophysical model, the information content of a molecule can be related to its configurational entropy. Consider a short, single strand of a synthetic nucleic acid molecule composed of nucleotide bases. The sequence of bases for this specific molecule is found to be `GATTACCA`, where G, A, T, and C represent the four standard types of bases.\n\nThe configurational entropy of this molecule is determined by the number of distinct ways these eight bases can be arranged. We assume that each unique permutation of the bases represents a distinct, equally probable microstate of the system.\n\nUsing Boltzmann's definition of entropy, calculate the total configurational entropy of this single molecule. Use the Boltzmann constant $k_B = 1.381 \\times 10^{-23}$ J/K. Express your answer in joules per kelvin (J/K), rounded to three significant figures.", "solution": "We are asked for the configurational entropy using Boltzmann's definition. The relevant physical principle is Boltzmann's entropy formula:\n$$\nS = k_B \\ln W,\n$$\nwhere $W$ is the number of equally probable microstates.\n\nThe microstates here are all distinct permutations of the multiset of bases in the sequence GATTACCA. Counting the multiplicities, we have $n_{A} = 3$, $n_{T} = 2$, $n_{C} = 2$, $n_{G} = 1$, with total $n = 8$. The number of distinct permutations is\n$$\nW = \\frac{8!}{3! \\, 2! \\, 2! \\, 1!} = \\frac{40320}{6 \\cdot 2 \\cdot 2} = 1680.\n$$\nTherefore,\n$$\nS = k_B \\ln(1680).\n$$\nUsing the provided $k_B = 1.381 \\times 10^{-23}$ J/K and evaluating $\\ln(1680) \\approx 7.426549072$, we obtain\n$$\nS \\approx \\left(1.381 \\times 10^{-23}\\right) \\times 7.426549072 = 1.0256064269 \\times 10^{-22}.\n$$\nRounding to three significant figures gives\n$$\nS \\approx 1.03 \\times 10^{-22}.\n$$\nThe unit is joules per kelvin as implied by $k_B$.", "answer": "$$\\boxed{1.03 \\times 10^{-22}}$$", "id": "1844385"}, {"introduction": "Moving from simple arrangements to a physical system, this practice explores how a single macroscopic state, such as a fixed total energy, can arise from many different microscopic configurations. You will determine the entropy of a macrostate by explicitly counting the number of ways to distribute energy quanta among a set of distinguishable particles. This problem is fundamental to understanding why certain macroscopic states are more probable than othersâ€”it is simply because there are more ways to achieve them. [@problem_id:1844406]", "problem": "Consider a simplified toy model for the distribution of energy in a physical system. The system consists of $N=4$ distinguishable, non-interacting particles. Each particle can occupy one of six discrete, non-degenerate energy levels, given by $\\epsilon_j = j \\cdot \\epsilon_0$ for $j \\in \\{1, 2, 3, 4, 5, 6\\}$, where $\\epsilon_0$ is a fundamental energy constant.\n\nA specific configuration of energy levels for all particles is called a microstate. For example, if particle 1 is in level $\\epsilon_1$, particle 2 in level $\\epsilon_3$, particle 3 in level $\\epsilon_1$, and particle 4 in level $\\epsilon_5$, this constitutes one possible microstate. A macrostate of the system is defined by its total energy, $E_{\\text{total}}$, which is the sum of the energies of the individual particles.\n\nThe statistical entropy, $S$, of a given macrostate is defined by the Boltzmann formula $S = k_B \\ln(W)$, where $W$ is the total number of distinct microstates that correspond to that macrostate, and $k_B$ is the Boltzmann constant.\n\nCalculate the statistical entropy for the macrostate of this system corresponding to a total energy of $E_{\\text{total}} = 10 \\epsilon_0$.\n\nUse the value for the Boltzmann constant ($k_B$) as $1.381 \\times 10^{-23}$ J/K. Express your final answer in units of J/K, rounded to four significant figures.", "solution": "We have $N=4$ distinguishable, non-interacting particles, each occupying a discrete, non-degenerate level $\\epsilon_{j}=j\\epsilon_{0}$ with $j \\in \\{1,2,3,4,5,6\\}$. A microstate corresponds to an ordered $4$-tuple $(j_{1},j_{2},j_{3},j_{4})$, and the macrostate with total energy $E_{\\text{total}}=10\\epsilon_{0}$ corresponds to all solutions of\n$$\nj_{1}+j_{2}+j_{3}+j_{4}=10,\\quad j_{i}\\in\\{1,2,3,4,5,6\\}.\n$$\nThus, the number of microstates $W$ is the number of compositions of $10$ into $4$ positive integers each at most $6$. Define $y_{i}=j_{i}-1$, so $y_{i}\\geq 0$, $y_{i}\\leq 5$, and\n$$\ny_{1}+y_{2}+y_{3}+y_{4}=6.\n$$\nWithout the upper bounds, the number of nonnegative solutions is\n$$\n\\binom{6+4-1}{4-1}=\\binom{9}{3}=84.\n$$\nWe must exclude solutions with any $y_{i}\\geq 6$. Since the sum is $6$, the only way to have $y_{i}\\geq 6$ is $y_{i}=6$ and all others $0$, and at most one variable can violate the bound. There are $4$ such solutions (one for each choice of $i$). By inclusion-exclusion,\n$$\nW=84-4=80.\n$$\nThe Boltzmann entropy is given by the Boltzmann formula\n$$\nS=k_B\\ln(W).\n$$\nSubstituting $W=80$ and the given $k_B=1.381\\times 10^{-23}\\ \\text{J/K}$,\n$$\nS=(1.381\\times 10^{-23})\\ln(80).\n$$\nEvaluating $\\ln(80)$ and rounding the final result to four significant figures yields\n$$\nS\\approx 6.052\\times 10^{-23}.\n$$\nUnits are J/K as required.", "answer": "$$\\boxed{6.052 \\times 10^{-23}}$$", "id": "1844406"}, {"introduction": "For macroscopic systems containing a vast number of particles, counting every single microstate becomes impractical. Instead, we use powerful mathematical tools like Stirling's approximation to find the entropy in the thermodynamic limit. This advanced practice challenges you to derive a continuous function for entropy, $S(E)$, for a classic two-level system, demonstrating how the statistical definition of entropy gives rise to the familiar functions of thermodynamics. [@problem_id:1844411]", "problem": "Consider a simplified model for a novel form of information storage, which consists of a crystal containing $N$ identical, distinguishable molecules. Each molecule can exist in one of two distinct conformational states: a ground state with energy $0$ and an excited state with energy $\\epsilon > 0$. The total internal energy of the entire system of $N$ molecules is fixed at a value $E$.\n\nAssuming the number of molecules $N$ and the number of molecules in the excited state are both large enough for Stirling's approximation ($\\ln(n!) \\approx n \\ln n - n$) to be valid, derive an expression for the statistical entropy $S$ of the system as a function of $E$, $N$, $\\epsilon$, and the Boltzmann constant $k_B$.", "solution": "Let $m$ denote the number of molecules in the excited state. The fixed-energy constraint implies the relation\n$$\nE = m \\epsilon \\quad \\Longrightarrow \\quad m = \\frac{E}{\\epsilon}, \\quad 0 \\leq m \\leq N,\n$$\nwith the understanding that $m$ is an integer in the exact combinatorial count and that the Stirling approximation is valid for large $N$ and large $m$.\n\nBecause the molecules are distinguishable and each molecule can be either in the ground state or the excited state, the number of microstates consistent with $m$ excitations is the binomial coefficient\n$$\nW(E,N) = \\binom{N}{m} = \\frac{N!}{m! \\left(N-m\\right)!}.\n$$\nThe statistical entropy in the microcanonical ensemble is\n$$\nS(E,N) = k_B \\ln W(E,N).\n$$\nUsing Stirling's approximation $\\ln(n!) \\approx n \\ln n - n$ for large $n$, we obtain\n$$\n\\ln W \\approx \\left(N \\ln N - N\\right) - \\left(m \\ln m - m\\right) - \\left[\\left(N-m\\right) \\ln\\left(N-m\\right) - \\left(N-m\\right)\\right]\n$$\n$$\n= N \\ln N - m \\ln m - \\left(N-m\\right) \\ln\\left(N-m\\right),\n$$\nsince the linear terms cancel. Substituting $m = E/\\epsilon$ gives\n$$\nS(E,N) \\approx k_B \\left[ N \\ln N - \\frac{E}{\\epsilon} \\ln\\!\\left(\\frac{E}{\\epsilon}\\right) - \\left( N - \\frac{E}{\\epsilon} \\right) \\ln\\!\\left( N - \\frac{E}{\\epsilon} \\right) \\right].\n$$\nEquivalently, writing $x \\equiv \\frac{E}{N \\epsilon}$ with $0 \\le x \\le 1$, this simplifies to the binary-entropy form\n$$\nS(E,N) \\approx - k_B N \\left[ x \\ln x + \\left(1-x\\right) \\ln\\!\\left(1-x\\right) \\right],\n$$\nwhich is explicitly a function of $E$, $N$, $\\epsilon$, and $k_B$ via $x = \\frac{E}{N \\epsilon}$ and has dimensionless logarithm arguments as required.", "answer": "$$\\boxed{-k_{B} N \\left[ \\frac{E}{N \\epsilon} \\ln\\!\\left( \\frac{E}{N \\epsilon} \\right) + \\left( 1 - \\frac{E}{N \\epsilon} \\right) \\ln\\!\\left( 1 - \\frac{E}{N \\epsilon} \\right) \\right]}$$", "id": "1844411"}]}