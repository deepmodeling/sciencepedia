{"hands_on_practices": [{"introduction": "To build an intuition for entropy, it is instructive to begin with a system that is entirely predictable. This exercise explores a simple contracting map, where all initial points converge to a single fixed point. By calculating the Kolmogorov-Sinai (KS) entropy for this system [@problem_id:1688745], you will see firsthand that deterministic, non-chaotic systems generate no new information over time, resulting in an entropy of zero. This provides a crucial baseline for understanding the positive entropy found in more complex, chaotic systems.", "problem": "Consider a simple model for a signal attenuator in a discrete-time control system. The state of the signal at any given time, $x$, is represented by a real number in the interval $X = [-1, 1]$. The system evolves according to the linear map $T: X \\to X$ defined by $T(x) = \\frac{x}{3}$. The Kolmogorov-Sinai (KS) entropy, denoted $h_{KS}(T)$, provides a measure of the system's complexity and rate of information generation over time. For this specific attenuator system, calculate the value of the KS entropy $h_{KS}(T)$.", "solution": "The Kolmogorov-Sinai (KS) entropy of a dynamical system quantifies the rate at which the system produces information. For differentiable dynamical systems, the KS entropy is related to the Lyapunov exponents of the system through Pesin's Identity. For a one-dimensional map $T$ on an interval $X$, this identity simplifies. If the system admits an absolutely continuous invariant measure $\\mu$, the KS entropy is given by the integral of the positive part of the local Lyapunov exponent over the phase space:\n$$h_{KS}(T) = \\int_{X} \\max(0, \\lambda(x)) d\\mu(x)$$\nwhere $\\lambda(x)$ is the local Lyapunov exponent at point $x$.\n\nFirst, we need to find the local Lyapunov exponent for the given map, $T(x) = \\frac{x}{3}$. The local Lyapunov exponent is defined in terms of the derivative of the map, $\\lambda(x) = \\ln|T'(x)|$.\n\nLet's calculate the derivative of $T(x)$:\n$$T'(x) = \\frac{d}{dx} \\left(\\frac{x}{3}\\right) = \\frac{1}{3}$$\n\nThe derivative is a constant, $T'(x) = 1/3$, for all $x$ in the interval $[-1, 1]$.\nTherefore, the local Lyapunov exponent is also constant for all $x$:\n$$\\lambda(x) = \\ln\\left|\\frac{1}{3}\\right| = \\ln\\left(\\frac{1}{3}\\right) = -\\ln(3)$$\n\nNow we apply this to Pesin's Identity. We need to evaluate $\\max(0, \\lambda(x))$:\n$$\\max(0, \\lambda(x)) = \\max(0, -\\ln(3))$$\nSince $\\ln(3)  \\ln(1) = 0$, the value $-\\ln(3)$ is negative.\nThus,\n$$\\max(0, -\\ln(3)) = 0$$\n\nThe integrand in the formula for KS entropy is therefore 0 for all $x \\in X$.\nSubstituting this into the integral expression for the KS entropy:\n$$h_{KS}(T) = \\int_{-1}^{1} 0 \\ d\\mu(x) = 0$$\n\nThe KS entropy for this system is 0.\n\nThis result can also be understood intuitively. The map $T(x) = x/3$ is a contracting map because $|T'(x)| = 1/3  1$. Under repeated application of this map, any initial point $x_0 \\in [-1, 1]$ will have its trajectory converge to the fixed point at $x=0$. For example, $x_n = T^n(x_0) = x_0/3^n$, which approaches 0 as $n \\to \\infty$. A small interval of initial conditions will shrink in length with each iteration. Since all trajectories converge to a single, predictable point, the system does not exhibit any chaotic behavior or unpredictability. The KS entropy measures the rate of generation of new information (or unpredictability), so for a completely predictable, non-chaotic system like this contracting map, the KS entropy must be zero.", "answer": "$$\\boxed{0}$$", "id": "1688745"}, {"introduction": "After establishing a baseline with a zero-entropy system, we now tackle a classic example of a one-dimensional chaotic map. This practice [@problem_id:871213] applies Pesin's identity to the skew tent map, a system known for its complex dynamics. You will compute the Kolmogorov-Sinai entropy by integrating the logarithm of the map's derivative, directly connecting the geometric features of the map (its slopes) to its capacity for generating unpredictable behavior.", "problem": "Consider the one-dimensional skew tent map $T_a: [0, 1] \\to [0, 1]$, defined by the parameter $a \\in (0, 1)$. The map is given by the piecewise linear function:\n$$\nT_a(x) = \\begin{cases}\n\\frac{x}{a}  \\text{if } 0 \\le x \\le a \\\\\n\\frac{1-x}{1-a}  \\text{if } a  x \\le 1\n\\end{cases}\n$$\nThis map is a classic example of a chaotic dynamical system. For any value of the parameter $a$, the map possesses an ergodic invariant measure that is absolutely continuous with respect to the Lebesgue measure. In fact, for the skew tent map, the Lebesgue measure itself (with density $\\rho(x) = 1$ for $x \\in [0, 1]$) is an invariant measure.\n\nThe Kolmogorov-Sinai (KS) entropy, $h_{KS}$, quantifies the rate of information production of a dynamical system. For a one-dimensional map $T(x)$ with an invariant probability density $\\rho(x)$, the KS entropy can be calculated using Pesin's identity:\n$$\nh_{KS}(T) = \\int_0^1 \\ln|T'(x)| \\rho(x) \\, dx\n$$\nwhere $T'(x)$ is the derivative of the map.\n\nYour task is to compute the Kolmogorov-Sinai entropy $h_{KS}(T_a)$ for the skew tent map with respect to the invariant Lebesgue measure. Express your answer as a closed-form analytic expression in terms of the parameter $a$.", "solution": "We use Pesin’s identity for a one‐dimensional map with invariant density $\\rho(x)=1$:\n$$\nh_{KS}(T_a)=\\int_0^1 \\ln|T_a'(x)| \\rho(x) \\, dx.\n$$\nOn the two linear branches,\n\n$$\nT_a'(x)=\\begin{cases}\n\\frac{1}{a},0\\le x\\le a,\\\\\n-\\frac{1}{1-a},ax\\le1.\n\\end{cases}\n$$\n\nHence\n$$\nh_{KS}(T_a) = \\int_0^a \\ln\\left(\\frac{1}{a}\\right) \\,dx + \\int_a^1 \\ln\\left(\\frac{1}{1-a}\\right) \\,dx = a \\ln\\left(\\frac{1}{a}\\right) + (1-a) \\ln\\left(\\frac{1}{1-a}\\right).\n$$\nUsing $\\ln(1/u)=-\\ln u$ gives\n$$\nh_{KS}(T_a) = -a\\ln a - (1-a)\\ln(1-a).\n$$", "answer": "$$\\boxed{-a\\ln a - (1-a)\\ln(1-a)}$$", "id": "871213"}, {"introduction": "Beyond maps on continuous intervals, dynamical systems can be described through sequences of discrete symbols, a framework known as symbolic dynamics. This exercise introduces the \"golden mean subshift,\" a system defined by a simple forbidden sequence, and demonstrates how to calculate its complexity using the transfer matrix method [@problem_id:871313]. This problem illuminates the deep connection between topological entropy, which counts the exponential growth of allowed symbolic sequences, and the KS entropy of a special invariant measure that maximizes this complexity.", "problem": "Consider a symbolic dynamical system defined on the space of bi-infinite sequences of symbols from the alphabet $\\mathcal{A} = \\{0, 1\\}$. The dynamics are governed by a discrete-time shift map $T$, where $(T(x))_i = x_{i+1}$ for a sequence $x = (\\dots, x_{-1}, x_0, x_1, \\dots)$.\n\nWe restrict the space of all possible sequences to a specific subset called a subshift of finite type (SFT). This subset, denoted $\\Sigma_A$, consists of all sequences where only certain transitions between symbols are allowed. The allowed transitions are encoded in a transition matrix $A$, where $A_{ij}=1$ if the transition from symbol $i$ to symbol $j$ is allowed, and $A_{ij}=0$ otherwise. A sequence $x \\in \\Sigma_A$ if and only if $A_{x_i, x_{i+1}} = 1$ for all $i \\in \\mathbb{Z}$.\n\nThe \"golden mean subshift\" is a specific SFT on the alphabet $\\mathcal{A} = \\{0, 1\\}$ defined by the rule that the block of symbols '11' is forbidden.\n\nFor a dynamical system $(X, T)$, the topological entropy, $h_{top}(T)$, measures the exponential growth rate of the number of distinguishable orbits. For an SFT with transition matrix $A$, it is given by $h_{top}(T) = \\ln(\\lambda_{max})$, where $\\lambda_{max}$ is the Perron-Frobenius eigenvalue (the largest positive real eigenvalue) of $A$.\n\nThe Kolmogorov-Sinai (KS) entropy, $h_{\\mu}(T)$, quantifies the average rate of information production for a specific $T$-invariant measure $\\mu$. The variational principle connects these two entropies by the relation $h_{top}(T) = \\sup_{\\mu} h_{\\mu}(T)$, where the supremum is taken over all $T$-invariant probability measures. A measure $\\mu_{MME}$ for which this supremum is attained, i.e., $h_{\\mu_{MME}}(T) = h_{top}(T)$, is called a measure of maximal entropy (MME).\n\nYour task is to compute the Kolmogorov-Sinai (KS) entropy for the measure of maximal entropy associated with the golden mean subshift.", "solution": "1. The transition matrix for the golden mean subshift on $\\{0,1\\}$ (forbidding “11”) is\n$$\nA=\\begin{pmatrix}11\\\\10\\end{pmatrix}.\n$$\n2. Its Perron–Frobenius eigenvalue $\\lambda_{\\max}$ satisfies the characteristic equation\n$$\n\\det(A-\\lambda I)\n=\\begin{vmatrix}1-\\lambda1\\\\1-\\lambda\\end{vmatrix}\n=(1-\\lambda)(-\\lambda)-1\n=\\lambda^2-\\lambda-1=0.\n$$\nSolving gives\n$$\n\\lambda_{\\max}=\\frac{1+\\sqrt{5}}{2},\n$$\nthe golden ratio.\n3. The topological entropy of the SFT is\n$$\nh_{\\text{top}}(T)=\\ln(\\lambda_{\\max})\n=\\ln\\left(\\frac{1+\\sqrt{5}}{2}\\right).\n$$\nBy the variational principle, the KS entropy of the measure of maximal entropy equals $h_{\\text{top}}(T)$.", "answer": "$$\\boxed{\\ln\\frac{1+\\sqrt{5}}{2}}$$", "id": "871313"}]}