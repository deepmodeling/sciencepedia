{"hands_on_practices": [{"introduction": "This problem gets to the heart of the quantum covering lemma, a cornerstone result enabled by operator concentration inequalities. You will determine the number of random rank-one projectors required to form an approximate identity operator on a fixed subspace. This calculation serves as a foundational building block for many important theorems in quantum information theory, from decoupling to the existence of certain quantum codes. [@problem_id:159951]", "problem": "In quantum information theory, a common task is to approximate an operator on a particular subspace using a sum of random operators. This problem explores the number of random projectors needed to approximate the identity on a given subspace, a result often known as the covering lemma.\n\nLet $\\mathcal{H} = \\mathbb{C}^d$ be a $d$-dimensional Hilbert space. Let $S \\subset \\mathcal{H}$ be a fixed $k$-dimensional subspace, with $\\Pi_S$ being the orthogonal projector onto $S$. We generate a set of $N$ random quantum states $|\\psi_1\\rangle, \\dots, |\\psi_N\\rangle$, each chosen independently and uniformly from the unit sphere in $\\mathcal{H}$ (i.e., according to the Haar measure). From these states, we form the rank-one projectors $P_i = |\\psi_i\\rangle\\langle\\psi_i|$.\n\nWe are interested in the operator $Z = d \\cdot \\Pi_S \\left( \\sum_{i=1}^N P_i \\right) \\Pi_S$, which acts on the subspace $S$. The goal is to find the minimum number of projectors $N$ required to ensure that $Z$ is close to $N \\Pi_S$ with high probability. Specifically, we want to find the minimum integer $N$ such that\n$$ (1-\\epsilon) N \\Pi_S \\le Z \\le (1+\\epsilon) N \\Pi_S $$\nholds with a probability of at least $1-\\delta$, for given $\\epsilon \\in (0,1)$ and $\\delta \\in (0,1)$. The inequality $A \\le B$ for Hermitian operators signifies that $B-A$ is a positive semidefinite operator.\n\nTo solve this, you must use the following version of the Operator Chernoff Bound. For a sum of independent random positive semidefinite $k \\times k$ matrices $Y = \\sum_{i=1}^N Y_i$, where $\\|Y_i\\|_{\\infty} \\le R$ and $\\mu_{min} I \\le \\sum_{i=1}^N \\mathbb{E}[Y_i] \\le \\mu_{max} I$:\n1. $\\Pr\\left[ \\lambda_{\\min}(Y)  (1-\\epsilon) \\mu_{min} \\right] \\le k \\exp\\left( -\\frac{\\epsilon^2 \\mu_{min}}{2R} \\right)$\n2. $\\Pr\\left[ \\lambda_{\\max}(Y)  (1+\\epsilon) \\mu_{max} \\right] \\le k \\exp\\left( -\\frac{\\epsilon^2 \\mu_{max}}{3R} \\right)$\n\nDetermine the minimum integer $N$ that guarantees the desired condition. Express your answer as a closed-form expression in terms of $d, k, \\epsilon$, and $\\delta$.", "solution": "We set \n$$Y_i = d\\,\\Pi_S P_i \\Pi_S = d\\,\\Pi_S|\\psi_i\\rangle\\langle\\psi_i|\\Pi_S\\,,\\quad Y=\\sum_{i=1}^N Y_i\\,. $$ \n1. Norm bound: since $\\|\\Pi_S|\\psi_i\\rangle\\|\\le1$, \n$$\\|Y_i\\|_\\infty = d\\,\\|\\Pi_S|\\psi_i\\rangle\\|^2\\le d\\equiv R\\,. $$ \n2. Expectation: using $\\mathbb{E}[|\\psi_i\\rangle\\langle\\psi_i|]=I/d$, \n$$E[Y_i]=d\\,\\Pi_S\\frac{I}{d}\\Pi_S=\\Pi_S\\,,\\quad \\sum_{i=1}^N \\mathbb{E}[Y_i]=N\\Pi_S\\,. $$ \nHence $\\mu_{\\min}=\\mu_{\\max}=N$.  \n3. Apply Operator Chernoff: for $k$‐dimensional $S$ \n$$\\Pr[\\lambda_{\\min}(Y)(1-\\epsilon)N]\\le k\\,e^{-\\frac{\\epsilon^2N}{2d}},\\quad \n\\Pr[\\lambda_{\\max}(Y)(1+\\epsilon)N]\\le k\\,e^{-\\frac{\\epsilon^2N}{3d}}\\,. $$ \n4. By the union bound we demand each tail term $\\le\\delta/2$. The stronger condition is \n$$k\\exp\\Bigl(-\\frac{\\epsilon^2N}{3d}\\Bigr)\\le\\frac\\delta2 \n\\;\\Longrightarrow\\; N\\ge\\frac{3d}{\\epsilon^2}\\ln\\frac{2k}{\\delta}\\,. $$ \nThus the smallest integer $N$ is the ceiling of this bound.", "answer": "$$\\boxed{\\left\\lceil\\frac{3d}{\\epsilon^2}\\ln\\!\\bigl(\\tfrac{2k}{\\delta}\\bigr)\\right\\rceil}$$", "id": "159951"}, {"introduction": "Building on the basic covering argument, this exercise extends the concept to the ubiquitous setting of bipartite quantum systems. You will explore how projectors on one subsystem can be used to approximate the identity on the entire composite space, a key component in proofs of the decoupling theorem. This practice demonstrates how to adapt operator concentration bounds to tensor product structures, a vital skill for analyzing quantum communication and computation protocols. [@problem_id:159913]", "problem": "In quantum information theory, a common task is to approximate the identity operator on a Hilbert space using a sum of random projectors. This is a key component in proofs of the covering lemma, which is used to establish the existence of certain quantum codes and communication protocols. This problem explores the number of projectors required to achieve a good approximation, using the operator Chernoff bound.\n\nConsider a bipartite quantum system with Hilbert space $H = H_A \\otimes H_B$, where the dimensions of the subsystems are $\\dim(H_A) = d_A$ and $\\dim(H_B) = d_B$. Let $\\{|\\psi_i\\rangle\\}_{i=1}^N$ be a set of $N$ pure states chosen independently and uniformly at random from the unit sphere in $H_A$ (i.e., according to the Haar measure). Let $P_i = |\\psi_i\\rangle\\langle\\psi_i|$ be the projector onto the state $|\\psi_i\\rangle$.\n\nWe can extend these projectors to act on the total Hilbert space $H$ as $X_i = P_i \\otimes I_B$, where $I_B$ is the identity operator on $H_B$. We form the operator sum $M_N = \\sum_{i=1}^N X_i$. The expectation of a single random projector on $H_A$ is $\\mathbb{E}[P_i] = I_A/d_A$.\n\nWe want to find how large $N$ must be to ensure that the normalized sum $\\frac{d_A}{N}M_N$ is close to the identity operator $I_{AB}$ on $H$. Specifically, we want the following condition to hold with high probability:\n$$ \\left\\| \\frac{d_A}{N} M_N - I_{AB} \\right\\|_{\\infty} \\le \\epsilon $$\nwhere $\\|\\cdot\\|_{\\infty}$ is the operator norm (largest singular value), and $\\epsilon \\in (0, 1)$ is a small positive tolerance.\n\nTo solve this, you are given the following version of the matrix Chernoff bound. For a sum $Z = \\sum_{i=1}^N Y_i$ of $N$ independent and identically distributed self-adjoint random matrices $Y_i$ on $\\mathbb{C}^d$ satisfying $0 \\le Y_i \\le R \\cdot I$ for some real number $R$, let $\\mathbb{E}[Z]$ be the expectation of the sum. For any $\\delta \\in (0,1)$, the probability of relative deviation from the mean is bounded by:\n$$ \\mathbb{P}\\left( \\|Z - \\mathbb{E}[Z]\\|_{\\infty}  \\delta \\lambda_{\\max}(\\mathbb{E}[Z]) \\right) \\le 2d \\exp\\left( -\\frac{\\delta^2 \\lambda_{\\min}(\\mathbb{E}[Z])}{2R} \\right) $$\n\nTreating $N$ as a continuous variable, determine the minimum number of projectors required such that the probability of violating the desired accuracy condition is at most $\\eta$, where $\\eta \\in (0,1)$ is the failure probability. That is, find the minimum $N$ such that:\n$$ \\mathbb{P}\\left( \\left\\| \\frac{d_A}{N} M_N - I_{AB} \\right\\|_{\\infty}  \\epsilon \\right) \\le \\eta $$\n\nExpress your answer in terms of $d_A$, $d_B$, $\\epsilon$, and $\\eta$.", "solution": "We aim to find the minimum $N$ such that:\n\n$$\n\\mathbb{P}\\left( \\left\\| \\frac{d_A}{N} M_N - I_{AB} \\right\\|_{\\infty}  \\epsilon \\right) \\le \\eta,\n$$\n\nwhere $M_N = \\sum_{i=1}^N X_i$ and $X_i = P_i \\otimes I_B$.\n\nDefine $Z = M_N$, so:\n\n$$\n\\mathbb{E}[Z] = \\mathbb{E}\\left[ \\sum_{i=1}^N X_i \\right] = \\sum_{i=1}^N \\mathbb{E}[X_i] = N \\cdot \\mathbb{E}[X_1].\n$$\n\nGiven $\\mathbb{E}[P_i] = I_A / d_A$, it follows that:\n\n$$\n\\mathbb{E}[X_i] = \\mathbb{E}[P_i] \\otimes I_B = \\frac{I_A}{d_A} \\otimes I_B = \\frac{I_{AB}}{d_A},\n$$\n\nso:\n\n$$\n\\mathbb{E}[Z] = N \\cdot \\frac{I_{AB}}{d_A}.\n$$\n\nThe normalized sum is:\n\n$$\n\\frac{d_A}{N} Z = \\frac{d_A}{N} M_N,\n$$\n\nand its expectation is:\n\n$$\n\\mathbb{E}\\left[ \\frac{d_A}{N} Z \\right] = \\frac{d_A}{N} \\cdot \\mathbb{E}[Z] = \\frac{d_A}{N} \\cdot N \\cdot \\frac{I_{AB}}{d_A} = I_{AB}.\n$$\n\nThe deviation can be rewritten as:\n\n$$\n\\left\\| \\frac{d_A}{N} Z - I_{AB} \\right\\|_{\\infty} = \\left\\| \\frac{d_A}{N} (Z - \\mathbb{E}[Z]) \\right\\|_{\\infty} = \\frac{d_A}{N} \\| Z - \\mathbb{E}[Z] \\|_{\\infty}.\n$$\n\nThus, the event $\\left\\| \\frac{d_A}{N} Z - I_{AB} \\right\\|_{\\infty}  \\epsilon$ is equivalent to:\n\n$$\n\\frac{d_A}{N} \\| Z - \\mathbb{E}[Z] \\|_{\\infty}  \\epsilon \\implies \\| Z - \\mathbb{E}[Z] \\|_{\\infty}  \\epsilon \\frac{N}{d_A}.\n$$\n\n\nApply the matrix Chernoff bound to $Z = \\sum_{i=1}^N X_i$. Each $X_i$ is self-adjoint, and since $P_i$ is a rank-1 projector on $H_A$, $X_i = P_i \\otimes I_B$ satisfies $0 \\le X_i \\le I_{AB}$ (eigenvalues are 0 or 1). Thus, $R = 1$. The operators act on $H = H_A \\otimes H_B$, which has dimension $d = d_A d_B$.\n\nThe Chernoff bound states:\n\n$$\n\\mathbb{P}\\left( \\| Z - \\mathbb{E}[Z] \\|_{\\infty}  \\delta \\lambda_{\\max}(\\mathbb{E}[Z]) \\right) \\le 2d \\exp\\left( -\\frac{\\delta^2 \\lambda_{\\min}(\\mathbb{E}[Z])}{2R} \\right).\n$$\n\nHere, $\\mathbb{E}[Z] = \\frac{N}{d_A} I_{AB}$, so $\\lambda_{\\max}(\\mathbb{E}[Z]) = \\lambda_{\\min}(\\mathbb{E}[Z]) = N / d_A$. Set the deviation event as:\n\n$$\n\\| Z - \\mathbb{E}[Z] \\|_{\\infty}  \\epsilon \\frac{N}{d_A} = \\delta \\cdot \\frac{N}{d_A},\n$$\n\nwhich implies $\\delta = \\epsilon$. Substituting $R = 1$, $\\lambda_{\\min}(\\mathbb{E}[Z]) = N / d_A$, and $d = d_A d_B$:\n\n$$\n\\mathbb{P}\\left( \\| Z - \\mathbb{E}[Z] \\|_{\\infty}  \\epsilon \\frac{N}{d_A} \\right) \\le 2 d_A d_B \\exp\\left( -\\frac{\\epsilon^2 (N / d_A)}{2 \\cdot 1} \\right) = 2 d_A d_B \\exp\\left( -\\frac{\\epsilon^2 N}{2 d_A} \\right).\n$$\n\nThis probability bounds the event of interest. Set it less than or equal to $\\eta$:\n\n$$\n2 d_A d_B \\exp\\left( -\\frac{\\epsilon^2 N}{2 d_A} \\right) \\le \\eta.\n$$\n\nSolve for $N$:\n\n$$\n\\exp\\left( -\\frac{\\epsilon^2 N}{2 d_A} \\right) \\le \\frac{\\eta}{2 d_A d_B}.\n$$\n\nTake the natural logarithm:\n\n$$\n-\\frac{\\epsilon^2 N}{2 d_A} \\le \\ln\\left( \\frac{\\eta}{2 d_A d_B} \\right).\n$$\n\nMultiply both sides by $-1$ (reversing the inequality):\n\n$$\n\\frac{\\epsilon^2 N}{2 d_A} \\ge -\\ln\\left( \\frac{\\eta}{2 d_A d_B} \\right) = \\ln\\left( \\frac{2 d_A d_B}{\\eta} \\right).\n$$\n\nThus:\n\n$$\nN \\ge \\frac{2 d_A}{\\epsilon^2} \\ln\\left( \\frac{2 d_A d_B}{\\eta} \\right).\n$$\n\nTreating $N$ as continuous, the minimum $N$ is:\n\n$$\nN = \\frac{2 d_A}{\\epsilon^2} \\ln\\left( \\frac{2 d_A d_B}{\\eta} \\right).\n$$", "answer": "$$ \\boxed{ \\dfrac{ 2 d_{A} }{ \\epsilon^{2} } \\ln \\left( \\dfrac{ 2 d_{A} d_{B} }{ \\eta } \\right) } $$", "id": "159913"}, {"introduction": "While the power of operator-valued concentration inequalities is evident, one might wonder when they are strictly necessary compared to their simpler scalar counterparts. This problem provides a direct comparison between the scalar Hoeffding inequality and the matrix Bernstein inequality for estimating a specific matrix element. By finding the regime where the matrix bound becomes superior, you will gain a deeper appreciation for the additional power conferred by considering the full operator structure. [@problem_id:159989]", "problem": "This problem explores the relationship between scalar and matrix concentration inequalities, which are fundamental tools in quantum information theory, particularly in the proof of results like the quantum covering lemma.\n\n**Background:**\n\n1.  **Scalar Hoeffding's Inequality:** Let $Y_1, \\dots, Y_N$ be $N$ independent and identically distributed (i.i.d.) real random variables with common expectation $E[Y_k] = \\mu$. If $Y_k \\in [a, b]$ almost surely, then for any $\\epsilon  0$, the probability of the sample mean deviating from the true mean is bounded by:\n    $$\n    P\\left(\\frac{1}{N}\\sum_{k=1}^N Y_k - \\mu \\ge \\epsilon\\right) \\le \\exp\\left(-\\frac{2N\\epsilon^2}{(b-a)^2}\\right)\n    $$\n\n2.  **Matrix Bernstein Inequality (a Chernoff-type bound):** Let $X_1, \\dots, X_N$ be $N$ i.i.d. random self-adjoint matrices of dimension $d \\times d$. Let $A = E[X_k]$. Define the centered random matrices $Z_k = X_k - A$. Assume there exists a constant $R$ such that the operator norm $\\|Z_k\\| \\le R$ almost surely. Let $\\sigma^2 = \\|E[Z_k^2]\\|$ be the norm of the matrix variance. Then, for any $\\epsilon  0$, the probability that the largest eigenvalue of the deviation of the sample mean from its expectation exceeds $\\epsilon$ is bounded by:\n    $$\n    P\\left(\\lambda_{\\max}\\left(\\frac{1}{N}\\sum_{k=1}^N X_k - A\\right) \\ge \\epsilon\\right) \\le d \\cdot \\exp\\left(-\\frac{N\\epsilon^2/2}{\\sigma^2 + R\\epsilon/3}\\right)\n    $$\n\n**Problem Statement:**\n\nConsider a sequence of $N$ i.i.d. $2 \\times 2$ random matrices, $X_1, \\dots, X_N$, drawn from a distribution where $X_k$ can take one of two forms:\n$$\nX_k = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\quad \\text{with probability } p\n$$\n$$\nX_k = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\quad \\text{with probability } 1-p\n$$\nwhere $p$ is a parameter satisfying $1/2  p  1$.\n\nWe are interested in the deviation of the top-left entry of the sample mean, $\\hat{A}_N = \\frac{1}{N}\\sum_{k=1}^N X_k$. Let this entry be denoted by $(\\hat{A}_N)_{11}$.\n\nYour task is to compare the upper bounds on the deviation probability $P((\\hat{A}_N)_{11} - E[(\\hat{A}_N)_{11}] \\ge \\epsilon)$ obtained from the two inequalities above. Specifically, determine the minimum number of samples $N_{crit}$ such that for all $N  N_{crit}$, the Matrix Bernstein bound is strictly tighter than the scalar Hoeffding bound for the given event.\n\nAssume $\\epsilon  0$ is small enough such that the exponential rate of the Matrix Bernstein bound is larger than that of the Hoeffding bound. Express your answer for $N_{crit}$ as a closed-form analytical expression in terms of $p$ and $\\epsilon$.", "solution": "1. Scalar Hoeffding bound for $Y_k=(X_k)_{11}\\in\\{0,1\\}$ with $E[Y_k]=p$, $a=0$, $b=1$:\n$$ P\\bigl(\\tfrac1N\\sum Y_k - p \\ge \\epsilon\\bigr) \\le \\exp\\bigl(-2N\\epsilon^2\\bigr). $$\n2. Matrix Bernstein bound for $X_k$ ($2\\times2$ diagonal):\n   – Mean \n   $$ A=E[X_k]=\\begin{pmatrix}p0\\\\01-p\\end{pmatrix}. $$\n   – Centered $Z_k=X_k-A$ has $\\|Z_k\\|=\\max(p,1-p)=p$ (since $p1/2$), so $R=p$.\n   – Variance parameter\n   $$ \\sigma^2=\\bigl\\|E[Z_k^2]\\bigr\\| =\\bigl\\|p(1-p)I_2\\bigr\\|=p(1-p). $$\n   – Dimension $d=2$. Hence\n$$ P\\bigl(\\lambda_{\\max}(\\tfrac1N\\sum X_k -A)\\ge\\epsilon\\bigr) \\le 2\\exp\\!\\Bigl(-\\frac{N\\epsilon^2/2}{\\sigma^2+R\\epsilon/3}\\Bigr) =2\\exp\\!\\Bigl(-\\frac{N\\epsilon^2}{2\\bigl(p(1-p)+\\tfrac{p\\epsilon}{3}\\bigr)}\\Bigr). $$\n3. Since $(\\hat A_N)_{11}-p\\le\\lambda_{\\max}(\\hat A_N-A)$, the matrix bound also controls our event. We require\n$$ 2\\exp\\!\\Bigl(-\\frac{N\\epsilon^2}{2(p(1-p)+\\tfrac{p\\epsilon}{3})}\\Bigr) \\exp(-2N\\epsilon^2). $$\nTaking logarithms,\n$$ \\ln2-\\frac{N\\epsilon^2}{2(p(1-p)+\\tfrac{p\\epsilon}{3})}-2N\\epsilon^2 $$\n$$ \\implies\\ln2 N\\epsilon^2\\Bigl(\\frac1{2(p(1-p)+\\tfrac{p\\epsilon}{3})}-2\\Bigr). $$\nDefine \n$$ \\alpha=\\frac1{2\\bigl(p(1-p)+\\tfrac{p\\epsilon}{3}\\bigr)}-20 $$ \n(assumed by “exponential rate larger” condition). Hence\n$$ N\\frac{\\ln2}{\\epsilon^2\\,\\alpha} =\\frac{\\ln2}{\\epsilon^2\\!\\Bigl(\\tfrac1{2(p(1-p)+\\tfrac{p\\epsilon}{3})}-2\\Bigr)}. $$\nAlgebraically combine:\n$$ \\frac1{2(p(1-p)+\\tfrac{p\\epsilon}{3})}-2 =\\frac{1-4\\bigl(p(1-p)+\\tfrac{p\\epsilon}{3}\\bigr)} {2\\bigl(p(1-p)+\\tfrac{p\\epsilon}{3}\\bigr)}, $$\nso\n$$ N_{crit} =\\frac{2\\bigl(p(1-p)+\\tfrac{p\\epsilon}{3}\\bigr)\\ln2} {\\epsilon^2\\bigl(1-4p(1-p)-\\tfrac{4p\\epsilon}{3}\\bigr)} =\\frac{2p\\bigl(1-p+\\tfrac{\\epsilon}{3}\\bigr)\\ln2} {\\epsilon^2\\bigl(1-4p\\bigl(1-p+\\tfrac{\\epsilon}{3}\\bigr)\\bigr)}. $$", "answer": "$$\\boxed{\\frac{2\\,p\\Bigl(1-p+\\tfrac{\\epsilon}{3}\\Bigr)\\,\\ln2}{\\epsilon^2\\Bigl(1 - 4\\,p\\bigl(1-p+\\tfrac{\\epsilon}{3}\\bigr)\\Bigr)}}$$", "id": "159989"}]}