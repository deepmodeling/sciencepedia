{"hands_on_practices": [{"introduction": "The Asymptotic Equipartition Property (AEP) is a cornerstone of information theory, stating that for long sequences from a random source, almost all outcomes are 'typical.' This practice provides a hands-on entry point to this idea by focusing on the weak typical set, where typicality is defined by the sequence's sample entropy. By directly calculating the size of this set for a binary source, you will bridge the abstract definition with a concrete combinatorial count, developing intuition for why data compression is possible [@problem_id:56674].", "problem": "An independent and identically distributed (i.i.d.) discrete memoryless source is characterized by a random variable $X$ over a finite alphabet $\\mathcal{X}$, with a probability mass function $p(x) = \\text{Pr}(X=x)$. A sequence of $n$ symbols emitted by this source is denoted by $x^n = (x_1, x_2, \\ldots, x_n)$, and its probability is given by $p(x^n) = \\prod_{i=1}^n p(x_i)$.\n\nThe Shannon entropy of the source, using the logarithm of base 2, is defined as:\n$$ H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) $$\n\nThe *weak typical set* $A_\\epsilon^{(n)}$ for a given sequence length $n$ and a tolerance parameter $\\epsilon  0$ is the set of all sequences $x^n$ whose sample entropy is close to the true entropy of the source. Formally, it is defined as:\n$$ A_\\epsilon^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\le \\epsilon \\right\\} $$\n\nConsider a binary memoryless source with alphabet $\\mathcal{X} = \\{0, 1\\}$ and probabilities $p(1) = p$ and $p(0) = 1-p$.\nFor a specific instance of this source with probability $p=1/3$, sequence length $n=9$, and tolerance $\\epsilon=1/6$, calculate the total number of sequences in the weak typical set, i.e., find the size of the set $|A_\\epsilon^{(n)}|$.", "solution": "The weak typical set $A_\\epsilon^{(n)}$ is defined as:\n\n$$\nA_\\epsilon^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\le \\epsilon \\right\\}\n$$\n\nFor a binary source with $p(1) = p = \\frac{1}{3}$ and $p(0) = 1 - p = \\frac{2}{3}$, the probability of a sequence $x^n$ with $k$ ones and $n - k$ zeros is:\n\n$$\np(x^n) = p^k (1 - p)^{n - k} = \\left(\\frac{1}{3}\\right)^k \\left(\\frac{2}{3}\\right)^{n - k}\n$$\n\nThus:\n\n$$\n\\log_2 p(x^n) = k \\log_2 \\left(\\frac{1}{3}\\right) + (n - k) \\log_2 \\left(\\frac{2}{3}\\right) = -k \\log_2 3 + (n - k) (1 - \\log_2 3)\n$$\n\nSimplifying:\n\n$$\n\\log_2 p(x^n) = -k \\log_2 3 + n - k - n \\log_2 3 + k \\log_2 3 = n(1 - \\log_2 3) - k\n$$\n\nSo:\n\n$$\n-\\frac{1}{n} \\log_2 p(x^n) = -\\frac{1}{n} [n(1 - \\log_2 3) - k] = -1 + \\log_2 3 + \\frac{k}{n}\n$$\n\nThe entropy $H(X)$ is:\n\n$$\nH(X) = -p \\log_2 p - (1 - p) \\log_2 (1 - p) = -\\frac{1}{3} \\log_2 \\frac{1}{3} - \\frac{2}{3} \\log_2 \\frac{2}{3} = \\frac{1}{3} \\log_2 3 + \\frac{2}{3} \\log_2 \\frac{3}{2}\n$$\n\n\n$$\nH(X) = \\frac{1}{3} \\log_2 3 + \\frac{2}{3} (\\log_2 3 - \\log_2 2) = \\frac{1}{3} \\log_2 3 + \\frac{2}{3} \\log_2 3 - \\frac{2}{3} = \\log_2 3 - \\frac{2}{3}\n$$\n\nThe typicality condition becomes:\n\n$$\n\\left| \\left( -1 + \\log_2 3 + \\frac{k}{n} \\right) - \\left( \\log_2 3 - \\frac{2}{3} \\right) \\right| \\le \\epsilon\n$$\n\nSimplifying the expression inside the absolute value:\n\n$$\n-1 + \\log_2 3 + \\frac{k}{n} - \\log_2 3 + \\frac{2}{3} = \\frac{k}{n} - \\frac{1}{3}\n$$\n\nThus:\n\n$$\n\\left| \\frac{k}{n} - \\frac{1}{3} \\right| \\le \\epsilon\n$$\n\nWith $n = 9$ and $\\epsilon = \\frac{1}{6}$:\n\n$$\n\\left| \\frac{k}{9} - \\frac{1}{3} \\right| \\le \\frac{1}{6}\n$$\n\nSolving for $k$:\n\n$$\n-\\frac{1}{6} \\le \\frac{k}{9} - \\frac{1}{3} \\le \\frac{1}{6}\n$$\n\n\n$$\n\\frac{1}{3} - \\frac{1}{6} \\le \\frac{k}{9} \\le \\frac{1}{3} + \\frac{1}{6}\n$$\n\n\n$$\n\\frac{1}{6} \\le \\frac{k}{9} \\le \\frac{1}{2}\n$$\n\n\n$$\n\\frac{9}{6} \\le k \\le \\frac{9}{2} \\implies 1.5 \\le k \\le 4.5\n$$\n\nSince $k$ must be an integer, $k = 2, 3, 4$.\n\nThe size of $A_\\epsilon^{(n)}$ is the number of sequences with $k$ ones, which is the sum of binomial coefficients:\n\n$$\n|A_\\epsilon^{(n)}| = \\sum_{k=2}^{4} \\binom{9}{k}\n$$\n\nCalculating:\n\n$$\n\\binom{9}{2} = \\frac{9 \\times 8}{2} = 36, \\quad \\binom{9}{3} = \\frac{9 \\times 8 \\times 7}{6} = 84, \\quad \\binom{9}{4} = \\frac{9 \\times 8 \\times 7 \\times 6}{24} = 126\n$$\n\n\n$$\n|A_\\epsilon^{(n)}| = 36 + 84 + 126 = 246\n$$", "answer": "$$ \\boxed{246} $$", "id": "56674"}, {"introduction": "While weak typicality considers the overall probability of a sequence, strong typicality imposes a stricter condition on the empirical frequency of each symbol. This distinction is crucial, as the strong typical set has more robust properties, yet for i.i.d. sources, both sets encompass almost all the probability mass for large $n$. This exercise illuminates the relationship between these two sets by tasking you with finding and analyzing the rare sequences that are weakly typical but fail to be strongly typical, thereby clarifying the specific constraints that define each set [@problem_id:56805].", "problem": "Consider an information source that produces a sequence of symbols $x_1, x_2, \\ldots, x_n$ which are independent and identically distributed (i.i.d.) according to a probability mass function $p(x)$ over a finite alphabet $\\mathcal{X}$.\n\nThe **weakly typical set** $A_\\epsilon^{(n)}$ with respect to a tolerance $\\epsilon  0$ is the set of all sequences $x^n = (x_1, \\ldots, x_n)$ whose sample entropy is close to the Shannon entropy $H(X)$ of the source:\n$$A_\\epsilon^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\le \\epsilon \\right\\}$$\nwhere $p(x^n) = \\prod_{i=1}^n p(x_i)$ and $H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$.\n\nThe **strongly typical set** $T_\\delta^{(n)}$ with respect to a tolerance $\\delta  0$ is the set of all sequences $x^n$ for which the empirical frequency of each symbol is close to its true probability. Let $N(x|x^n)$ be the number of occurrences of symbol $x$ in the sequence $x^n$. Then:\n$$T_\\delta^{(n)} = \\left\\{ x^n \\in \\mathcal{X}^n : \\left| \\frac{N(x|x^n)}{n} - p(x) \\right| \\le \\delta \\text{ for all } x \\in \\mathcal{X}, \\text{ and } N(x|x^n)=0 \\text{ if } p(x)=0 \\right\\}$$\n\nNow, consider a specific ternary source with alphabet $\\mathcal{X} = \\{0, 1, 2\\}$ and probabilities $p(0) = p_0$, $p(1) = p_1$, and $p(2) = p_2$. For the particular case where $p_0=1/2$, $p_1=1/4$, $p_2=1/4$, and for a sequence length of $n=4$ with tolerances $\\epsilon = 3/10$ and $\\delta = 3/10$, calculate the total probability of all sequences that are weakly typical but not strongly typical. That is, compute $P(A_\\epsilon^{(n)} \\setminus T_\\delta^{(n)})$.", "solution": "The problem involves a ternary source with alphabet $\\mathcal{X} = \\{0, 1, 2\\}$ and probabilities $p(0) = \\frac{1}{2}$, $p(1) = \\frac{1}{4}$, $p(2) = \\frac{1}{4}$. The sequence length is $n = 4$, and tolerances are $\\epsilon = \\frac{3}{10}$ and $\\delta = \\frac{3}{10}$. The goal is to compute $P(A_\\epsilon^{(4)} \\setminus T_\\delta^{(4)})$, the probability of sequences that are weakly typical but not strongly typical.\n\nFirst, the Shannon entropy $H(X)$ is calculated:\n\n$$\nH(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) = -\\left[ \\frac{1}{2} \\log_2 \\frac{1}{2} + \\frac{1}{4} \\log_2 \\frac{1}{4} + \\frac{1}{4} \\log_2 \\frac{1}{4} \\right].\n$$\n\nUsing $\\log_2 \\frac{1}{2} = -1$ and $\\log_2 \\frac{1}{4} = -2$:\n\n$$\nH(X) = -\\left[ \\frac{1}{2} \\cdot (-1) + \\frac{1}{4} \\cdot (-2) + \\frac{1}{4} \\cdot (-2) \\right] = -\\left[ -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right] = -\\left[ -\\frac{3}{2} \\right] = \\frac{3}{2}.\n$$\n\n\nFor a sequence $x^4 = (x_1, x_2, x_3, x_4)$, let $n_0$, $n_1$, $n_2$ be the counts of symbols $0$, $1$, $2$ respectively, with $n_0 + n_1 + n_2 = 4$. The probability is:\n\n$$\np(x^4) = \\prod_{i=1}^4 p(x_i) = \\left( \\frac{1}{2} \\right)^{n_0} \\left( \\frac{1}{4} \\right)^{n_1} \\left( \\frac{1}{4} \\right)^{n_2}.\n$$\n\nThe sample entropy is:\n\n$$\n-\\frac{1}{4} \\log_2 p(x^4) = -\\frac{1}{4} \\log_2 \\left[ \\left( \\frac{1}{2} \\right)^{n_0} \\left( \\frac{1}{4} \\right)^{n_1 + n_2} \\right].\n$$\n\nSince $\\log_2 \\frac{1}{2} = -1$ and $\\log_2 \\frac{1}{4} = -2$:\n\n$$\n-\\frac{1}{4} \\log_2 p(x^4) = -\\frac{1}{4} \\left[ n_0 \\cdot (-1) + (n_1 + n_2) \\cdot (-2) \\right] = \\frac{1}{4} (n_0 + 2n_1 + 2n_2).\n$$\n\nSubstituting $n_1 + n_2 = 4 - n_0$:\n\n$$\n\\frac{1}{4} (n_0 + 2(4 - n_0)) = \\frac{1}{4} (n_0 + 8 - 2n_0) = \\frac{1}{4} (8 - n_0) = 2 - \\frac{n_0}{4}.\n$$\n\n\nThe weakly typical set condition is:\n\n$$\n\\left| 2 - \\frac{n_0}{4} - \\frac{3}{2} \\right| \\leq \\frac{3}{10} \\implies \\left| \\frac{1}{2} - \\frac{n_0}{4} \\right| \\leq \\frac{3}{10}.\n$$\n\nThis simplifies to:\n\n$$\n-\\frac{3}{10} \\leq \\frac{1}{2} - \\frac{n_0}{4} \\leq \\frac{3}{10}.\n$$\n\nSolving the inequalities:\n\n$$\n-\\frac{3}{10} - \\frac{1}{2} \\leq -\\frac{n_0}{4} \\leq \\frac{3}{10} - \\frac{1}{2} \\implies -\\frac{8}{10} \\leq -\\frac{n_0}{4} \\leq -\\frac{2}{10} \\implies \\frac{2}{10} \\leq \\frac{n_0}{4} \\leq \\frac{8}{10}.\n$$\n\nMultiplying by 4:\n\n$$\n0.8 \\leq n_0 \\leq 3.2.\n$$\n\nSince $n_0$ is an integer, $n_0 \\in \\{1, 2, 3\\}$. Thus, weakly typical sequences have $n_0 = 1, 2,$ or $3$.\n\nFor the strongly typical set, the empirical frequencies must satisfy $\\left| \\frac{N(x|x^4)}{4} - p(x) \\right| \\leq \\frac{3}{10}$ for each $x$:\n- For $x=0$: $\\left| \\frac{n_0}{4} - \\frac{1}{2} \\right| \\leq \\frac{3}{10}$, which gives $0.8 \\leq n_0 \\leq 3.2$, so $n_0 \\in \\{1, 2, 3\\}$ (same as weak typicality).\n- For $x=1$: $\\left| \\frac{n_1}{4} - \\frac{1}{4} \\right| \\leq \\frac{3}{10} \\implies |n_1 - 1| \\leq 1.2 \\implies -0.2 \\leq n_1 \\leq 2.2$. Since $n_1$ is a nonnegative integer, $n_1 \\in \\{0, 1, 2\\}$.\n- For $x=2$: similarly, $\\left| \\frac{n_2}{4} - \\frac{1}{4} \\right| \\leq \\frac{3}{10} \\implies n_2 \\in \\{0, 1, 2\\}$.\n\nAdditionally, $n_0 + n_1 + n_2 = 4$. A sequence is not strongly typical if $n_1 \\notin \\{0, 1, 2\\}$ or $n_2 \\notin \\{0, 1, 2\\}$, but since $n_1, n_2 \\geq 0$, the only violations occur when $n_1  2$ or $n_2  2$. Given the constraints:\n- If $n_0 = 1$, then $n_1 + n_2 = 3$. Possible pairs $(n_1, n_2)$: $(0,3)$, $(1,2)$, $(2,1)$, $(3,0)$. The pairs violating strong typicality are $(0,3)$ (since $n_2=3  2$) and $(3,0)$ (since $n_1=3  2$).\n- If $n_0 = 2$, then $n_1 + n_2 = 2$. Possible pairs: $(0,2)$, $(1,1)$, $(2,0)$. All satisfy $n_1 \\leq 2$, $n_2 \\leq 2$.\n- If $n_0 = 3$, then $n_1 + n_2 = 1$. Possible pairs: $(0,1)$, $(1,0)$. All satisfy $n_1 \\leq 2$, $n_2 \\leq 2$.\n\nThus, only sequences with $n_0=1$ and $(n_1, n_2) = (0,3)$ or $(3,0)$ are weakly typical but not strongly typical.\n\nThe probability for a sequence with counts $(n_0, n_1, n_2)$ is:\n\n$$\np(x^4) = \\left( \\frac{1}{2} \\right)^{n_0} \\left( \\frac{1}{4} \\right)^{n_1} \\left( \\frac{1}{4} \\right)^{n_2}.\n$$\n\nThe number of such sequences is the multinomial coefficient:\n\n$$\n\\binom{4}{n_0, n_1, n_2} = \\frac{4!}{n_0! \\, n_1! \\, n_2!}.\n$$\n\n\nFor $(n_0, n_1, n_2) = (1, 0, 3)$:\n\n$$\n\\text{Number of sequences} = \\frac{4!}{1! \\, 0! \\, 3!} = \\frac{24}{1 \\cdot 1 \\cdot 6} = 4,\n$$\n\n\n$$\n\\text{Probability per sequence} = \\left( \\frac{1}{2} \\right)^1 \\left( \\frac{1}{4} \\right)^0 \\left( \\frac{1}{4} \\right)^3 = \\frac{1}{2} \\cdot 1 \\cdot \\frac{1}{64} = \\frac{1}{128},\n$$\n\n\n$$\n\\text{Total probability} = 4 \\cdot \\frac{1}{128} = \\frac{4}{128} = \\frac{1}{32}.\n$$\n\n\nFor $(n_0, n_1, n_2) = (1, 3, 0)$:\n\n$$\n\\text{Number of sequences} = \\frac{4!}{1! \\, 3! \\, 0!} = \\frac{24}{1 \\cdot 6 \\cdot 1} = 4,\n$$\n\n\n$$\n\\text{Probability per sequence} = \\left( \\frac{1}{2} \\right)^1 \\left( \\frac{1}{4} \\right)^3 \\left( \\frac{1}{4} \\right)^0 = \\frac{1}{2} \\cdot \\frac{1}{64} \\cdot 1 = \\frac{1}{128},\n$$\n\n\n$$\n\\text{Total probability} = 4 \\cdot \\frac{1}{128} = \\frac{4}{128} = \\frac{1}{32}.\n$$\n\n\nThe total probability for sequences that are weakly typical but not strongly typical is the sum:\n\n$$\n\\frac{1}{32} + \\frac{1}{32} = \\frac{2}{32} = \\frac{1}{16}.\n$$", "answer": "$$ \\boxed{\\dfrac{1}{16}} $$", "id": "56805"}, {"introduction": "The concept of a typical set is not just about counting sequences; it's also about understanding their geometric arrangement in the vast space of all possible sequences. This knowledge is fundamental to coding theory, where we want to place codewords far apart. This practice explores the internal structure of the strong typical set by calculating the average Hamming distance between two of its distinct members, revealing how 'spread out' these sequences are and providing a quantitative measure of the set's volume and distribution [@problem_id:56639].", "problem": "A memoryless binary source produces a sequence of bits $X_1, X_2, \\dots, X_n$, where each bit is independently drawn from a distribution with $P(X_i=1) = p$ and $P(X_i=0) = 1-p$.\n\nIn information theory, the **strong typical set** $A_{\\epsilon}^{(n)}(p)$ is defined for a given sequence length $n$, probability $p$, and a small tolerance $\\epsilon  0$. It is the set of all binary sequences $x^n = (x_1, \\dots, x_n)$ of length $n$ whose empirical probability of the symbol '1' is close to $p$. Specifically, let $N_1(x^n)$ be the number of ones in the sequence $x^n$. Then the strong typical set is given by:\n$$A_{\\epsilon}^{(n)}(p) = \\left\\{ x^n \\in \\{0,1\\}^n \\;\\middle|\\; \\left| \\frac{N_1(x^n)}{n} - p \\right| \\le \\epsilon \\right\\}$$\nThis set is fundamental to understanding data compression and the asymptotic equipartition property (AEP).\n\nConsider a specific case where the probability is rational, given by $p = k/n$ for integers $n$ and $k$ satisfying $n  2$ and $0  k  n$. Furthermore, the tolerance $\\epsilon$ is chosen to be a real number such that $0  \\epsilon  1/n$.\n\nYour task is to calculate the exact average Hamming distance, $\\langle d_H \\rangle$, between two *distinct* sequences drawn uniformly at random from the strong typical set $A_{\\epsilon}^{(n)}(k/n)$ under these specific conditions. Express your answer as a closed-form analytical expression in terms of $n$ and $k$.", "solution": "The strong typical set $A_{\\epsilon}^{(n)}(k/n)$ consists of all binary sequences of length $n$ with exactly $k$ ones, since $\\epsilon  1/n$ and $p = k/n$ imply that $N_1(x^n) = k$ for all sequences in the set. The size of this set is $|S| = \\binom{n}{k}$.\n\nThe average Hamming distance $\\langle d_H \\rangle$ between two distinct sequences drawn uniformly at random from $S$ is defined as:\n\n$$\n\\langle d_H \\rangle = \\frac{1}{|S|(|S|-1)} \\sum_{\\substack{x, y \\in S \\\\ x \\neq y}} d_H(x,y),\n$$\n\nwhere $d_H(x,y)$ is the Hamming distance, the number of positions where $x$ and $y$ differ.\n\nBy linearity of expectation and symmetry, the expectation can be expressed per position. For each position $i$, define the indicator variable $Z_i = \\mathbf{1}\\{X_i \\neq Y_i\\}$. Then:\n\n$$\nd_H(X,Y) = \\sum_{i=1}^n Z_i, \\quad \\text{so} \\quad \\langle d_H \\rangle = \\sum_{i=1}^n \\mathbb{E}[Z_i \\mid X \\neq Y].\n$$\n\nBy symmetry, $\\mathbb{E}[Z_i \\mid X \\neq Y]$ is the same for all $i$, so:\n\n$$\n\\langle d_H \\rangle = n \\cdot \\mathbb{E}[Z_1 \\mid X \\neq Y].\n$$\n\nNow, $\\mathbb{E}[Z_1 \\mid X \\neq Y] = \\mathbb{P}(X_1 \\neq Y_1 \\mid X \\neq Y)$. To compute this, partition $S$ into:\n- $A = \\{ x \\in S : x_1 = 1 \\}$, with $|A| = \\binom{n-1}{k-1}$ (sequences with first bit 1 and $k-1$ ones in the remaining $n-1$ bits),\n- $B = \\{ x \\in S : x_1 = 0 \\}$, with $|B| = \\binom{n-1}{k}$ (sequences with first bit 0 and $k$ ones in the remaining $n-1$ bits).\n\nThe event $X_1 \\neq Y_1$ occurs in two disjoint cases:\n1. $X \\in A$, $Y \\in B$: $|A| \\cdot |B|$ ordered pairs,\n2. $X \\in B$, $Y \\in A$: $|B| \\cdot |A|$ ordered pairs.\nThus, the number of ordered pairs $(X,Y)$ with $X_1 \\neq Y_1$ and $X \\neq Y$ is $2 |A| |B|$ (since $A$ and $B$ are disjoint, $X \\neq Y$ automatically).\n\nThe total number of ordered pairs with $X \\neq Y$ is $|S|(|S|-1)$. Therefore:\n\n$$\n\\mathbb{P}(X_1 \\neq Y_1 \\mid X \\neq Y) = \\frac{2 |A| |B|}{|S|(|S|-1)}.\n$$\n\nSubstitute $|A| = \\binom{n-1}{k-1}$, $|B| = \\binom{n-1}{k}$, and $|S| = \\binom{n}{k}$:\n\n$$\n\\mathbb{P}(X_1 \\neq Y_1 \\mid X \\neq Y) = \\frac{2 \\binom{n-1}{k-1} \\binom{n-1}{k}}{\\binom{n}{k} \\left( \\binom{n}{k} - 1 \\right)}.\n$$\n\nUsing the identities:\n\n$$\n\\binom{n-1}{k-1} = \\frac{k}{n} \\binom{n}{k}, \\quad \\binom{n-1}{k} = \\frac{n-k}{n} \\binom{n}{k},\n$$\n\nit follows that:\n\n$$\n|A| |B| = \\binom{n-1}{k-1} \\binom{n-1}{k} = \\frac{k (n-k)}{n^2} \\binom{n}{k}^2.\n$$\n\nThus:\n\n$$\n\\mathbb{P}(X_1 \\neq Y_1 \\mid X \\neq Y) = \\frac{2 \\cdot \\frac{k (n-k)}{n^2} \\binom{n}{k}^2}{\\binom{n}{k} \\left( \\binom{n}{k} - 1 \\right)} = \\frac{2k(n-k) \\binom{n}{k}}{n^2 \\left( \\binom{n}{k} - 1 \\right)}.\n$$\n\nNow, the average Hamming distance is:\n\n$$\n\\langle d_H \\rangle = n \\cdot \\frac{2k(n-k) \\binom{n}{k}}{n^2 \\left( \\binom{n}{k} - 1 \\right)} = \\frac{2k(n-k) \\binom{n}{k}}{n \\left( \\binom{n}{k} - 1 \\right)}.\n$$\n\nThis is the closed-form expression in terms of $n$ and $k$.", "answer": "$$ \\boxed{ \\dfrac{ 2k(n-k) \\binom{n}{k} }{ n \\left( \\binom{n}{k} - 1 \\right) } } $$", "id": "56639"}]}