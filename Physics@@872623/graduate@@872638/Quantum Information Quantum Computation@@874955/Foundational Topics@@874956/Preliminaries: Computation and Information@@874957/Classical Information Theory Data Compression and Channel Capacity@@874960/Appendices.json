{"hands_on_practices": [{"introduction": "Practical data compression often involves algorithms that adapt to the statistics of the input data. The Lempel-Ziv-Welch (LZW) algorithm provides a beautiful example of this principle, building a dictionary of recurring strings on the fly. This exercise walks you through the LZW compression process for a specific string, offering a hands-on understanding of how dictionary-based codes are generated and how their output bit-rate adapts as the dictionary grows [@problem_id:53455].", "problem": "The Lempel-Ziv-Welch (LZW) algorithm is a universal lossless data compression algorithm. Its operation relies on building a dictionary of strings encountered during compression.\n\nThe LZW algorithm can be summarized as follows:\n1.  Initialize a dictionary with a set of single-character strings.\n2.  Set `W` (the working string) to the first character of the input stream.\n3.  Loop over the input stream:\n    a. Read the next character `K`.\n    b. If the string `W + K` exists in the dictionary, set `W = W + K`.\n    c. If `W + K` is not in the dictionary, then:\n        i. Output the dictionary code corresponding to `W`.\n        ii. Add the new string `W + K` to the dictionary with a new code.\n        iii. Set `W = K`.\n4.  After the input stream is exhausted, output the code for the final working string `W`.\n\nConsider a specific implementation of LZW with the following specifications:\n*   The initial dictionary contains 256 entries for the 8-bit ASCII characters. The codes for these characters are their corresponding ASCII values (0-255). The next available code for new dictionary entries is 256.\n*   The number of bits used to represent an output code is determined by the dictionary size at the moment of output. If the dictionary contains $N$ entries, the output code is represented using $\\lceil \\log_2(N) \\rceil$ bits.\n*   The input character string to be compressed is `BANANA_BANDANA`.\n\nYour task is to calculate the total number of bits in the final compressed output sequence.", "solution": "The input string is \"BANANA_BANDANA\". The LZW compression process is simulated step by step to determine the output codes and the dictionary size at the time of each output. The dictionary starts with 256 entries (0 to 255 for ASCII characters). The next available code is 256.\n\n**Step-by-step simulation:**\n\n1. Initialize dictionary with ASCII 0-255. Set $ W = $ first character 'B'.\n2. Read next character 'A': \"BA\" not in dictionary.\n   - Output code for 'B' (66).\n   - Add \"BA\" to dictionary with code 256.\n   - Set $ W = $ 'A'.\n   - Dictionary size at output: 256 → bits = $ \\lceil \\log_2(256) \\rceil = 8 $.\n3. Read next character 'N': \"AN\" not in dictionary.\n   - Output code for 'A' (65).\n   - Add \"AN\" to dictionary with code 257.\n   - Set $ W = $ 'N'.\n   - Dictionary size at output: 257 → bits = $ \\lceil \\log_2(257) \\rceil = 9 $ (since $ 256  257  512 $).\n4. Read next character 'A': \"NA\" not in dictionary.\n   - Output code for 'N' (78).\n   - Add \"NA\" to dictionary with code 258.\n   - Set $ W = $ 'A'.\n   - Dictionary size at output: 258 → bits = 9.\n5. Read next character 'N': \"A\" + \"N\" = \"AN\" is in dictionary (code 257). Set $ W = $ \"AN\".\n6. Read next character 'A': \"AN\" + \"A\" = \"ANA\" not in dictionary.\n   - Output code for \"AN\" (257).\n   - Add \"ANA\" to dictionary with code 259.\n   - Set $ W = $ 'A'.\n   - Dictionary size at output: 259 → bits = 9.\n7. Read next character '_' (underscore): \"A\" + \"_\" = \"A_\" not in dictionary.\n   - Output code for 'A' (65).\n   - Add \"A_\" to dictionary with code 260.\n   - Set $ W = $ '_'.\n   - Dictionary size at output: 260 → bits = 9.\n8. Read next character 'B': \"_\" + \"B\" = \"_B\" not in dictionary.\n   - Output code for '_' (95, ASCII value).\n   - Add \"_B\" to dictionary with code 261.\n   - Set $ W = $ 'B'.\n   - Dictionary size at output: 261 → bits = 9.\n9. Read next character 'A': \"B\" + \"A\" = \"BA\" is in dictionary (code 256). Set $ W = $ \"BA\".\n10. Read next character 'N': \"BA\" + \"N\" = \"BAN\" not in dictionary.\n    - Output code for \"BA\" (256).\n    - Add \"BAN\" to dictionary with code 262.\n    - Set $ W = $ 'N'.\n    - Dictionary size at output: 262 → bits = 9.\n11. Read next character 'D': \"N\" + \"D\" = \"ND\" not in dictionary.\n    - Output code for 'N' (78).\n    - Add \"ND\" to dictionary with code 263.\n    - Set $ W = $ 'D'.\n    - Dictionary size at output: 263 → bits = 9.\n12. Read next character 'A': \"D\" + \"A\" = \"DA\" not in dictionary.\n    - Output code for 'D' (68).\n    - Add \"DA\" to dictionary with code 264.\n    - Set $ W = $ 'A'.\n    - Dictionary size at output: 264 → bits = 9.\n13. Read next character 'N': \"A\" + \"N\" = \"AN\" is in dictionary (code 257). Set $ W = $ \"AN\".\n14. Read next character 'A': \"AN\" + \"A\" = \"ANA\" is in dictionary (code 259). Set $ W = $ \"ANA\". End of input.\n    - Output code for \"ANA\" (259).\n    - Dictionary size at output: 265 (since last addition was code 264, and no new string added here) → bits = $ \\lceil \\log_2(265) \\rceil = 9 $.\n\n**Output codes and bit counts:**\n- Output 1: 66 → 8 bits\n- Output 2: 65 → 9 bits\n- Output 3: 78 → 9 bits\n- Output 4: 257 → 9 bits\n- Output 5: 65 → 9 bits\n- Output 6: 95 → 9 bits\n- Output 7: 256 → 9 bits\n- Output 8: 78 → 9 bits\n- Output 9: 68 → 9 bits\n- Output 10: 259 → 9 bits\n\n**Total bits calculation:**\n- Output 1: 8 bits\n- Outputs 2 to 10: 9 outputs × 9 bits each = 81 bits\n- Total bits = 8 + 81 = 89", "answer": "\\boxed{89}", "id": "53455"}, {"introduction": "While many source coding methods assign variable-length codes to fixed-length source symbols, Tunstall coding offers a clever alternative by parsing the source stream into variable-length words and mapping them to a fixed-length code. This exercise challenges you to construct a Tunstall code for a skewed binary source, providing insight into optimal source parsing. By calculating the resulting compression rate, you will see how this method leverages source statistics to achieve efficiency [@problem_id:53423].", "problem": "A memoryless binary source emits symbols from the alphabet $\\mathcal{A} = \\{0, 1\\}$. The probability of emitting a '0' is $P(0) = p$, and the probability of emitting a '1' is $P(1) = 1-p$. We assume that the source is skewed, such that $p > 1/2$.\n\nFor this source, we wish to design a source code using the Tunstall coding algorithm to produce a dictionary of $D=8$ variable-length source words. These words will then be mapped to fixed-length binary codewords.\n\nThe Tunstall algorithm constructs the dictionary by building a tree. It starts with the source symbols as the initial leaves. Then, it iteratively finds the leaf node with the highest probability, converts it into an internal node, and adds new leaves corresponding to appending each possible source symbol. This process is repeated until the total number of leaves in the tree reaches the desired dictionary size $D$. The set of all leaves at the end of this process forms the dictionary.\n\nTo simplify the construction process, you are given that the probability $p$ is sufficiently large to satisfy the condition $p^6 > 1-p$. This ensures that at each step of the construction, the leaf node corresponding to the longest sequence of only '0's is always the most probable among all leaves.\n\nThe compression rate of this code is defined as $R = L / \\bar{N}$, where $L$ is the length of the fixed-length binary codewords for the dictionary words, and $\\bar{N}$ is the average length of the source words in the dictionary.\n\nCalculate the compression rate $R$ for this specific Tunstall code. Express your answer as a function of $p$.", "solution": "The Tunstall code for the binary source with $P(0) = p > \\frac{1}{2}$ and $P(1) = 1-p$, with dictionary size $D=8$, is constructed by expanding the leaf with the longest sequence of '0's at each step, as $p^6 > 1-p$ ensures this leaf always has the highest probability. The dictionary words and their probabilities are:\n\n- \"0000000\": $p^7$\n- \"0000001\": $p^6(1-p)$\n- \"000001\": $p^5(1-p)$\n- \"00001\": $p^4(1-p)$\n- \"0001\": $p^3(1-p)$\n- \"001\": $p^2(1-p)$\n- \"01\": $p(1-p)$\n- \"1\": $1-p$\n\nThe fixed-length binary codewords have length $L = \\log_2(8) = 3$ bits. The average length $\\bar{N}$ of the source words is:\n\n\n$$\n\\bar{N} = 7 \\cdot p^7 + 7 \\cdot p^6(1-p) + 6 \\cdot p^5(1-p) + 5 \\cdot p^4(1-p) + 4 \\cdot p^3(1-p) + 3 \\cdot p^2(1-p) + 2 \\cdot p(1-p) + 1 \\cdot (1-p)\n$$\n\n\nFactor out $(1-p)$ in the last seven terms:\n\n\n$$\n\\bar{N} = 7p^7 + (1-p)\\left(7p^6 + 6p^5 + 5p^4 + 4p^3 + 3p^2 + 2p + 1\\right)\n$$\n\n\nExpand the second term:\n\n\n$$\n(1-p)\\left(7p^6 + 6p^5 + 5p^4 + 4p^3 + 3p^2 + 2p + 1\\right) = 7p^6 + 6p^5 + 5p^4 + 4p^3 + 3p^2 + 2p + 1 - 7p^7 - 6p^6 - 5p^5 - 4p^4 - 3p^3 - 2p^2 - p\n$$\n\n\nCombine like terms:\n\n\n$$\n= -7p^7 + (7p^6 - 6p^6) + (6p^5 - 5p^5) + (5p^4 - 4p^4) + (4p^3 - 3p^3) + (3p^2 - 2p^2) + (2p - p) + 1 = -7p^7 + p^6 + p^5 + p^4 + p^3 + p^2 + p + 1\n$$\n\n\nSubstitute back:\n\n\n$$\n\\bar{N} = 7p^7 + (-7p^7 + p^6 + p^5 + p^4 + p^3 + p^2 + p + 1) = p^6 + p^5 + p^4 + p^3 + p^2 + p + 1\n$$\n\n\nThis is a geometric series:\n\n\n$$\n\\bar{N} = \\sum_{k=0}^{6} p^k = \\frac{1 - p^7}{1 - p}\n$$\n\n\nThe compression rate $R$ is:\n\n\n$$\nR = \\frac{L}{\\bar{N}} = \\frac{3}{\\frac{1 - p^7}{1 - p}} = 3 \\cdot \\frac{1 - p}{1 - p^7}\n$$\n\n\nThus, the compression rate is:\n\n\n$$\nR = \\frac{3(1 - p)}{1 - p^7}\n$$", "answer": "$$ \\boxed{\\dfrac{3(1-p)}{1-p^{7}}} $$", "id": "53423"}, {"introduction": "Moving beyond lossless methods, rate-distortion theory provides the fundamental limits for lossy data compression, quantifying the ultimate trade-off between compression rate and fidelity. This practice explores this concept for a bivariate Gaussian source, where data components are correlated. By calculating the rate-distortion function $R(D)$, you will apply the 'reverse water-filling' principle to optimally allocate distortion across different signal dimensions, a powerful technique central to compressing correlated data [@problem_id:53350].", "problem": "Consider a memoryless bivariate Gaussian source, where each sample is an independent draw of a two-dimensional random vector $X = (X_1, X_2)^T$. The vector $X$ follows a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$ with zero mean and a covariance matrix given by\n$$\n\\Sigma = \\sigma^2 \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\nwhere $\\sigma^2 > 0$ is the variance of each component and $\\rho$ is the correlation coefficient, satisfying $1/2  \\rho  1$.\n\nWe wish to compress this source and reconstruct it as $\\hat{X} = (\\hat{X}_1, \\hat{X}_2)^T$. The quality of the reconstruction is measured by the total mean-squared error (MSE) distortion, defined as $d(X, \\hat{X}) = E[\\|X - \\hat{X}\\|^2] = E[(X_1 - \\hat{X}_1)^2 + (X_2 - \\hat{X}_2)^2]$.\n\nThe rate-distortion function, $R(D)$, for this source specifies the minimum achievable rate (in bits per 2D vector sample) for a given maximum allowed average distortion $D$.\n\nCalculate the value of the rate-distortion function $R(D)$ for a total average distortion of exactly $D = \\sigma^2$.", "solution": "The rate-distortion function for a multivariate Gaussian source with covariance matrix $\\Sigma$ under mean-squared error distortion is found by reverse water-filling on the eigenvalues of $\\Sigma$. The covariance matrix is:\n$$\n\\Sigma = \\sigma^2 \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda_1$ and $\\lambda_2$ of $\\Sigma$ are obtained from the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} \\sigma^2 - \\lambda  \\sigma^2 \\rho \\\\ \\sigma^2 \\rho  \\sigma^2 - \\lambda \\end{pmatrix} = (\\sigma^2 - \\lambda)^2 - (\\sigma^2 \\rho)^2 = 0\n$$\nSolving:\n$$\n(\\sigma^2 - \\lambda - \\sigma^2 \\rho)(\\sigma^2 - \\lambda + \\sigma^2 \\rho) = 0\n$$\nThus:\n$$\n\\lambda_1 = \\sigma^2 (1 + \\rho), \\quad \\lambda_2 = \\sigma^2 (1 - \\rho)\n$$\nGiven $1/2  \\rho  1$, it follows that $\\lambda_1  \\lambda_2  0$.\n\nThe rate-distortion function is:\n$$\nR(D) = \\min_{\\substack{D_1, D_2 \\\\ D_1 + D_2 \\leq D}} \\sum_{i=1}^2 \\frac{1}{2} \\log_2 \\left( \\frac{\\lambda_i}{D_i} \\right)\n$$\nsubject to $0 \\leq D_i \\leq \\lambda_i$ for $i=1,2$. The minimization is achieved by reverse water-filling, setting $D_i = \\min\\{\\lambda_i, \\theta\\}$ for a water-level $\\theta$ chosen such that $D_1 + D_2 = D$.\n\nFor $D = \\sigma^2$, consider the cases for $\\theta$:\n\n- If $\\theta \\geq \\lambda_1$, then $D_1 = \\lambda_1$ and $D_2 = \\lambda_2$, so $D_1 + D_2 = \\lambda_1 + \\lambda_2 = 2\\sigma^2  \\sigma^2$, which is too large.\n- If $\\lambda_2 \\leq \\theta  \\lambda_1$, then $D_1 = \\theta$ and $D_2 = \\lambda_2$. Set $D_1 + D_2 = D$:\n  $$\n  \\theta + \\sigma^2 (1 - \\rho) = \\sigma^2\n  $$\n  Solving for $\\theta$:\n  $$\n  \\theta = \\sigma^2 \\rho\n  $$\n  Check the condition $\\lambda_2 \\leq \\theta  \\lambda_1$:\n  $$\n  \\sigma^2 (1 - \\rho) \\leq \\sigma^2 \\rho  \\sigma^2 (1 + \\rho)\n  $$\n  Since $\\rho  1/2$, $1 - \\rho  \\rho$ holds, and $\\rho  1 + \\rho$ is always true. Thus, the condition is satisfied.\n- If $\\theta  \\lambda_2$, then $D_1 = \\theta$ and $D_2 = \\theta$, so $2\\theta = \\sigma^2$ gives $\\theta = \\sigma^2 / 2$. But $\\theta  \\lambda_2$ implies $\\sigma^2 / 2  \\sigma^2 (1 - \\rho)$, or $\\rho  1/2$, which contradicts $\\rho  1/2$. Thus, this case is invalid.\n\nTherefore, the distortion allocation is $D_1 = \\sigma^2 \\rho$ and $D_2 = \\sigma^2 (1 - \\rho)$. The rate-distortion function is:\n$$\nR(D) = \\frac{1}{2} \\log_2 \\left( \\frac{\\lambda_1}{D_1} \\right) + \\frac{1}{2} \\log_2 \\left( \\frac{\\lambda_2}{D_2} \\right)\n$$\nSubstituting the values:\n$$\nR(D) = \\frac{1}{2} \\log_2 \\left( \\frac{\\sigma^2 (1 + \\rho)}{\\sigma^2 \\rho} \\right) + \\frac{1}{2} \\log_2 \\left( \\frac{\\sigma^2 (1 - \\rho)}{\\sigma^2 (1 - \\rho)} \\right)\n$$\nSimplifying:\n$$\nR(D) = \\frac{1}{2} \\log_2 \\left( \\frac{1 + \\rho}{\\rho} \\right) + \\frac{1}{2} \\log_2 (1) = \\frac{1}{2} \\log_2 \\left( \\frac{1 + \\rho}{\\rho} \\right)\n$$\nsince $\\log_2(1) = 0$.", "answer": "$$ \\boxed{\\dfrac{1}{2} \\log_{2} \\left( \\dfrac{1 + \\rho}{\\rho} \\right)} $$", "id": "53350"}]}