{"hands_on_practices": [{"introduction": "Before exploring its deeper properties, it is essential to master the fundamental calculation of Shannon entropy. This first practice provides a concrete scenario with a simple discrete random variable—a biased die—allowing you to directly apply the definition $H(X) = -\\sum_i p_i \\ln p_i$ and build your computational fluency. Understanding how to quantify the uncertainty of a system with a non-uniform probability distribution is the first step toward appreciating the broader implications of entropy. [@problem_id:1991819]", "problem": "A manufacturer is developing a specialized six-sided die for a game of chance. Due to an intentional weighting imbalance, the die is constructed such that the probability of it landing on the face showing '6' is exactly three times the probability of it landing on any one of the other five faces. The probabilities of landing on faces '1' through '5' are all equal to each other.\n\nCalculate the dimensionless Shannon entropy for the outcome of a single roll of this biased die. For your calculation, use the natural logarithm (base $e$).\n\nProvide your answer as a single real number, rounded to four significant figures.", "solution": "Let the probabilities for faces $1$ through $5$ be $p$ each, and for face $6$ be $3p$. Normalization gives $5p+3p=1$, hence $p=\\frac{1}{8}$ and the distribution is $\\left(\\frac{1}{8},\\frac{1}{8},\\frac{1}{8},\\frac{1}{8},\\frac{1}{8},\\frac{3}{8}\\right)$.\n\nThe Shannon entropy (in nats) is defined as\n$$\nH=-\\sum_{i=1}^{6} p_{i}\\ln p_{i}.\n$$\nSubstituting the probabilities,\n$$\nH=-\\left[5\\cdot \\frac{1}{8}\\ln\\!\\left(\\frac{1}{8}\\right)+\\frac{3}{8}\\ln\\!\\left(\\frac{3}{8}\\right)\\right].\n$$\nUsing $\\ln\\!\\left(\\frac{1}{8}\\right)=-\\ln 8$ and $\\ln\\!\\left(\\frac{3}{8}\\right)=\\ln 3-\\ln 8$,\n$$\nH=\\frac{5}{8}\\ln 8-\\frac{3}{8}\\left(\\ln 3-\\ln 8\\right)=\\ln 8-\\frac{3}{8}\\ln 3.\n$$\n\nFor a numerical value using the natural logarithm,\n$$\n\\ln 8=3\\ln 2 \\approx 2.0794415416798357,\\quad \\ln 3 \\approx 1.0986122886681098,\n$$\nso\n$$\nH \\approx 2.0794415416798357-\\frac{3}{8}\\cdot 1.0986122886681098 \\approx 1.6674619334292945.\n$$\nRounded to four significant figures, $H\\approx 1.667$.", "answer": "$$\\boxed{1.667}$$", "id": "1991819"}, {"introduction": "Moving beyond the entropy of a single distribution, we can use information-theoretic measures to compare two distributions. This exercise introduces the Kullback-Leibler (KL) divergence, or relative entropy, a fundamental measure of the dissimilarity between a true data distribution and a model distribution. By finding the parameter of a geometric distribution that minimizes the KL divergence from a given empirical distribution, you will gain hands-on experience with how these concepts are applied in statistical inference and model fitting. [@problem_id:132035]", "problem": "Let $p_\\theta$ be a geometric distribution on the set of positive integers $k \\in \\{1, 2, 3, \\dots\\}$, with probability mass function (PMF) given by:\n$$p_k(\\theta) = P(K=k) = (1-\\theta)^{k-1}\\theta$$\nwhere $\\theta \\in (0, 1)$ is the success probability.\n\nLet $q$ be an empirical probability distribution defined on the finite set of outcomes $\\{1, 2, 3\\}$, with the following probabilities:\n$$ q_1 = \\frac{1}{2}, \\quad q_2 = \\frac{1}{3}, \\quad q_3 = \\frac{1}{6} $$\nand $q_k = 0$ for $k > 3$.\n\nThe Kullback-Leibler (KL) divergence from the empirical distribution $q$ to the model distribution $p_\\theta$ is given by\n$$ D_{KL}(q || p_\\theta) = \\sum_{k=1}^{\\infty} q_k \\ln\\left(\\frac{q_k}{p_k(\\theta)}\\right) $$\nwhere $\\ln$ denotes the natural logarithm. Since $q_k=0$ for $k>3$, the sum is effectively over $k \\in \\{1, 2, 3\\}$.\n\nFind the value of the parameter $\\theta$ that minimizes $D_{KL}(q || p_\\theta)$.", "solution": "The Kullback-Leibler divergence is given by:\n\n$$\nD_{KL}(q || p_\\theta) = \\sum_{k=1}^{\\infty} q_k \\ln\\left(\\frac{q_k}{p_k(\\theta)}\\right)\n$$\n\nSince $q_k = 0$ for $k > 3$, the sum reduces to:\n\n$$\nD_{KL}(q || p_\\theta) = q_1 \\ln\\left(\\frac{q_1}{p_1(\\theta)}\\right) + q_2 \\ln\\left(\\frac{q_2}{p_2(\\theta)}\\right) + q_3 \\ln\\left(\\frac{q_3}{p_3(\\theta)}\\right)\n$$\n\nSubstitute the empirical probabilities:\n\n$$\nq_1 = \\frac{1}{2}, \\quad q_2 = \\frac{1}{3}, \\quad q_3 = \\frac{1}{6}\n$$\n\nand the geometric distribution PMF:\n\n$$\np_1(\\theta) = \\theta, \\quad p_2(\\theta) = (1-\\theta)\\theta, \\quad p_3(\\theta) = (1-\\theta)^2\\theta\n$$\n\nThus:\n\n$$\nD_{KL} = \\frac{1}{2} \\ln\\left(\\frac{\\frac{1}{2}}{\\theta}\\right) + \\frac{1}{3} \\ln\\left(\\frac{\\frac{1}{3}}{(1-\\theta)\\theta}\\right) + \\frac{1}{6} \\ln\\left(\\frac{\\frac{1}{6}}{(1-\\theta)^2\\theta}\\right)\n$$\n\nSimplify the logarithms:\n\n$$\nD_{KL} = \\frac{1}{2} \\left[ \\ln\\left(\\frac{1}{2}\\right) - \\ln(\\theta) \\right] + \\frac{1}{3} \\left[ \\ln\\left(\\frac{1}{3}\\right) - \\ln((1-\\theta)\\theta) \\right] + \\frac{1}{6} \\left[ \\ln\\left(\\frac{1}{6}\\right) - \\ln((1-\\theta)^2\\theta) \\right]\n$$\n\nExpand:\n\n$$\nD_{KL} = \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) - \\frac{1}{2} \\ln(\\theta) + \\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right) - \\frac{1}{3} \\left[ \\ln(1-\\theta) + \\ln(\\theta) \\right] + \\frac{1}{6} \\ln\\left(\\frac{1}{6}\\right) - \\frac{1}{6} \\left[ 2\\ln(1-\\theta) + \\ln(\\theta) \\right]\n$$\n\nGroup the terms involving $\\theta$:\n\n$$\nD_{KL} = \\text{constant} - \\left( \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} \\right) \\ln(\\theta) - \\left( \\frac{1}{3} + \\frac{2}{6} \\right) \\ln(1-\\theta)\n$$\n\nSimplify coefficients:\n\n$$\n\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1\n$$\n\n\n$$\n\\frac{1}{3} + \\frac{2}{6} = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}\n$$\n\nThus:\n\n$$\nD_{KL} = \\text{constant} - \\ln(\\theta) - \\frac{2}{3} \\ln(1-\\theta)\n$$\n\nMinimizing $D_{KL}$ is equivalent to maximizing:\n\n$$\nM(\\theta) = \\ln(\\theta) + \\frac{2}{3} \\ln(1-\\theta)\n$$\n\nTake the derivative with respect to $\\theta$:\n\n$$\n\\frac{dM}{d\\theta} = \\frac{1}{\\theta} + \\frac{2}{3} \\cdot \\frac{1}{1-\\theta} \\cdot (-1) = \\frac{1}{\\theta} - \\frac{2}{3(1-\\theta)}\n$$\n\nSet the derivative to zero:\n\n$$\n\\frac{1}{\\theta} - \\frac{2}{3(1-\\theta)} = 0\n$$\n\n\n$$\n\\frac{1}{\\theta} = \\frac{2}{3(1-\\theta)}\n$$\n\nCross-multiply:\n\n$$\n3(1-\\theta) = 2\\theta\n$$\n\n\n$$\n3 - 3\\theta = 2\\theta\n$$\n\n\n$$\n3 = 5\\theta\n$$\n\n\n$$\n\\theta = \\frac{3}{5}\n$$\n\nTo confirm it is a minimum, check the second derivative:\n\n$$\n\\frac{d^2M}{d\\theta^2} = -\\frac{1}{\\theta^2} - \\frac{2}{3} \\cdot \\frac{1}{(1-\\theta)^2} \\cdot (-1) \\cdot (-1) = -\\frac{1}{\\theta^2} - \\frac{2}{3(1-\\theta)^2}\n$$\n\nSince $\\theta \\in (0,1)$, both terms are negative, so $\\frac{d^2M}{d\\theta^2} < 0$, confirming a maximum for $M(\\theta)$, hence a minimum for $D_{KL}$.\n\nThus, the value of $\\theta$ that minimizes $D_{KL}(q || p_\\theta)$ is $\\frac{3}{5}$.", "answer": "$$ \\boxed{\\dfrac{3}{5}} $$", "id": "132035"}, {"introduction": "Information theory provides not just measures of uncertainty and distance, but also a way to understand the inherent geometry of statistical models. This final practice introduces the concept of a statistical manifold, where the Fisher information metric, which is intimately related to the KL divergence, defines a notion of distance. By calculating the total arc length of the manifold of Bernoulli distributions, you will uncover a surprising and elegant connection between statistics, information, and geometry. [@problem_id:132036]", "problem": "In the field of information geometry, a parametric family of probability distributions can be viewed as a Riemannian manifold, where the metric tensor is given by the Fisher information metric. This metric quantifies the \"statistical distance\" between nearby distributions.\n\nConsider the family of Bernoulli distributions, which is fundamental in probability theory and quantum computation (as it describes a qubit measurement in a fixed basis). A Bernoulli random variable $X$ takes the value $k=1$ with probability $p$ and the value $k=0$ with probability $1-p$. The parameter $p$ can take any value in the open interval $(0, 1)$. The probability mass function is given by:\n$$ P(k; p) = p^k (1-p)^{1-k}, \\quad k \\in \\{0, 1\\} $$\nThe set of all Bernoulli distributions forms a one-dimensional statistical manifold parameterized by the coordinate $p$.\n\nFor a one-dimensional manifold parameterized by a single parameter $p$, the Fisher information metric has a single component, $g_{pp}(p)$, which is defined as:\n$$ g_{pp}(p) = \\sum_{k} P(k; p) \\left( \\frac{\\partial \\ln P(k; p)}{\\partial p} \\right)^2 $$\nwhere the sum is over all possible outcomes $k$ of the random variable.\n\nThe total arc length $L$ of this one-dimensional manifold, which represents the total statistical distance between the deterministic outcomes $p=0$ and $p=1$, is given by the integral of the arc length element $ds = \\sqrt{g_{pp}(p)} \\, dp$ over the entire range of the parameter $p$:\n$$ L = \\int_{0}^{1} \\sqrt{g_{pp}(p)} \\, dp $$\n\nCalculate the total arc length $L$ of the statistical manifold of Bernoulli distributions.", "solution": "1. The Fisher information metric for a one-dimensional parameter $p$ is\n$$\ng_{pp}(p)\n=\\sum_{k=0}^{1}P(k;p)\\Bigl(\\frac{\\partial}{\\partial p}\\ln P(k;p)\\Bigr)^2.\n$$\nFor the Bernoulli distribution,\n$$\nP(1;p)=p,\\quad P(0;p)=1-p,\n\\quad\n\\frac{\\partial}{\\partial p}\\ln P(1;p)=\\frac1p,\n\\quad\n\\frac{\\partial}{\\partial p}\\ln P(0;p)=-\\frac1{1-p}.\n$$\n\n2. Compute $g_{pp}(p)$:\n$$\ng_{pp}(p)\n=p\\Bigl(\\frac1p\\Bigr)^2+(1-p)\\Bigl(-\\frac1{1-p}\\Bigr)^2\n=\\frac1p+\\frac1{1-p}\n=\\frac{1}{p(1-p)}.\n$$\n\n3. The total arc length is\n$$\nL=\\int_{0}^{1}\\sqrt{g_{pp}(p)}\\,dp\n=\\int_{0}^{1}\\frac{1}{\\sqrt{p(1-p)}}\\,dp.\n$$\nRecognize the Beta integral:\n$$\n\\int_{0}^{1}p^{-1/2}(1-p)^{-1/2}\\,dp\n=B\\bigl(\\tfrac12,\\tfrac12\\bigr)\n=\\frac{\\Gamma(\\frac12)\\,\\Gamma(\\frac12)}{\\Gamma(1)}\n=\\pi.\n$$\n\n4. Therefore, the total statistical distance is\n$$\nL=\\pi.\n$$", "answer": "$$\\boxed{\\pi}$$", "id": "132036"}]}