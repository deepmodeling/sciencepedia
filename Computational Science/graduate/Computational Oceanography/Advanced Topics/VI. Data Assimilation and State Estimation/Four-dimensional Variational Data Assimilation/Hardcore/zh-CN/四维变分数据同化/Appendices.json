{
    "hands_on_practices": [
        {
            "introduction": "四维变分资料同化（4D-Var）的核心是求解一个大型非线性优化问题，其关键在于精确计算代价函数的梯度。由于梯度是通过伴随模式计算的，这一过程复杂且容易出错。本实践将指导您执行一个严格的梯度检验（gradient check），通过将伴随方法计算的梯度与有限差分近似进行比较，来验证梯度计算的正确性，这是开发任何可靠的变分同化系统的基石。",
            "id": "3793675",
            "problem": "您必须为计算海洋学中的四维变分资料同化 (4D-Var) 的代价函数实现一个严格的梯度检验。从 4D-Var 代价函数的定义和一个线性、一维、周期性的示踪物模型开始。您将比较沿随机方向的有限差分方向导数与基于伴随的梯度，并使用明确的容差来决定是否接受。\n\n在无量纲单位下，建立以下纯数学和数值自洽的问题。考虑一个具有 $n$ 个网格点且网格间距为 $\\Delta x = 1/n$ 的一维周期性区域。设 $\\Delta t$ 为时间步长，$K$ 为时间步数，$c$ 为恒定平流速度，$\\nu$ 为恒定扩散系数。线性正向模型通过以下方式更新示踪物状态 $x_k \\in \\mathbb{R}^n$：\n$$\nx_{k+1} = M x_k,\n$$\n其中模型矩阵 $M \\in \\mathbb{R}^{n \\times n}$ 由具有周期性边界条件的平流-扩散方程的显式欧拉离散化定义：\n$$\nM = I + \\Delta t \\left( A_{\\mathrm{adv}} + A_{\\mathrm{diff}} \\right),\n$$\n其中\n$$\nA_{\\mathrm{adv}} = -\\frac{c}{\\Delta x}\\left(I - S_{-}\\right), \\quad A_{\\mathrm{diff}} = \\frac{\\nu}{\\Delta x^2}\\left(S_{+} + S_{-} - 2I\\right).\n$$\n此处 $I$ 是 $\\mathbb{R}^{n \\times n}$ 中的单位矩阵，$S_{-}$ 是循环移位矩阵使得 $(S_{-} x)_i = x_{i-1}$，$S_{+}$ 是循环移位矩阵使得 $(S_{+} x)_i = x_{i+1}$，其中下标对 $n$ 取模。\n\n在每个时间 $k \\in \\{1,\\dots,K\\}$ 的观测由一个线性观测算子 $H \\in \\mathbb{R}^{m \\times n}$（它从状态中选择 $m$ 个网格点的值）和观测向量 $y_k \\in \\mathbb{R}^m$ 定义。背景（先验）状态为 $x_b \\in \\mathbb{R}^n$。假设背景误差和观测误差的协方差矩阵为对角矩阵：$B = \\sigma_b^2 I$ 和 $R = \\sigma_r^2 I$，因此 $B^{-1} = \\sigma_b^{-2} I$ 和 $R^{-1} = \\sigma_r^{-2} I$。\n\n四维变分 (4D-Var) 代价函数为\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right),\n$$\n其中 $x_k = M^k x_0$。\n\n您的任务是通过以下步骤对 $J$ 关于 $x_0$ 进行梯度检验：\n- 从第一性原理推导 $J$ 关于 $x_0$ 的基于伴随的梯度。\n- 抽取单位欧几里得范数 $\\|v\\|_2 = 1$ 的随机方向 $v \\in \\mathbb{R}^n$。\n- 使用中心差分公式计算有限差分方向导数：\n$$\nD_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}.\n$$\n- 计算基于伴随的方向导数：\n$$\nD_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v.\n$$\n- 对于每个方向，评估绝对误差 $E_{\\mathrm{abs}} = |D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|$ 和相对误差\n$$\nE_{\\mathrm{rel}} = \\frac{|D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|}{\\max\\left(1, |D_{\\mathrm{FD}}(v)|, |D_{\\mathrm{ADJ}}(v)|\\right)}.\n$$\n- 如果 $E_{\\mathrm{rel}} \\le \\text{tol}_{\\mathrm{rel}}$ 或 $E_{\\mathrm{abs}} \\le \\text{tol}_{\\mathrm{abs}}$，则接受沿该方向的梯度检验。\n\n为了科学真实性，按如下方式构造 $y_k$ 和 $x_b$：\n- 生成一个具有独立标准正态分布项的“真实”初始条件 $x_0^{\\mathrm{true}}$，用相同的 $M$ 进行传播以获得 $x_k^{\\mathrm{true}}$，并设置 $y_k = H x_k^{\\mathrm{true}} + \\sigma_r \\eta_k$，其中 $\\eta_k$ 为独立的标准正态分布。\n- 设置 $x_b = x_0^{\\mathrm{true}} + \\sigma_b \\xi$，其中 $\\xi$ 为独立的标准正态分布。\n- 在 $x_0 = x_b$ 处评估梯度检验。\n\n全程使用无量纲单位。不涉及角度。不涉及百分比。\n\n实现一个程序，对于下面列出的参数集，使用独立的随机方向执行上述梯度检验，并为每个参数集返回一个布尔值：如果该集合中所有测试方向都被接受，则为 true，否则为 false。最终输出必须是单行，包含所有参数集的结果，格式为方括号内的逗号分隔列表。\n\n测试套件（每行是一个参数集；每个集合使用独立的随机种子以确保可复现性）：\n- 案例 1：$n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, 方向数量 $=5$。\n- 案例 2：$n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-10}$, $\\text{tol}_{\\mathrm{rel}}=10^{-3}$, $\\text{tol}_{\\mathrm{abs}}=10^{-8}$, 方向数量 $=5$。\n- 案例 3：$n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=8$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, 方向数量 $=5$。\n- 案例 4：$n=40$, $K=10$, $\\Delta t=5\\times 10^{-3}$, $c=5\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=40$, $\\sigma_b=2\\times 10^{-1}$, $\\sigma_r=5\\times 10^{-2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, 方向数量 $=5$。\n- 案例 5：$n=24$, $K=5$, $\\Delta t=10^{-2}$, $c=3\\times 10^{-1}$, $\\nu=4\\times 10^{-3}$, $m=12$, $\\sigma_b=10^{-2}$, $\\sigma_r=2\\times 10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, 方向数量 $=5$。\n\n最终输出格式：\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是案例 $i$ 的布尔值）。",
            "solution": "用户提供的问题被评估为 **有效** 的。这是一个在计算数据同化领域内适定、有科学依据的问题，没有歧义、矛盾和事实错误。任务是为一个 4D-Var 数据同化系统实现一个标准数值程序，即梯度检验。我们将进行完整求解。\n\n我们的目标是推导 4D-Var 代价函数 $J(x_0)$ 关于初始状态 $x_0$ 的解析梯度。然后，将使用伴随方法计算的这个解析梯度与有限差分近似进行比较，以验证其正确性。\n\n代价函数由下式给出：\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)\n$$\n其中时间步 $k$ 的状态 $x_k$ 通过线性正向模型 $x_k = M^k x_0$ 与初始状态 $x_0$ 相关。代价函数由两项组成：背景项 $J_b(x_0)$，用于衡量与先验估计 $x_b$ 的距离；以及观测项 $J_o(x_0)$，用于衡量在一个时间窗内与观测 $y_k$ 的失配。\n\n$$\nJ_b(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n$$\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=1}^K \\left(H M^k x_0 - y_k\\right)^\\top R^{-1} \\left(H M^k x_0 - y_k\\right)\n$$\n\n梯度 $\\nabla J(x_0)$ 是这两项梯度的和：$\\nabla J(x_0) = \\nabla J_b(x_0) + \\nabla J_o(x_0)$。\n\n1.  **背景项的梯度**\n    背景项 $J_b(x_0)$ 是一个标准的二次型。其关于 $x_0$ 的梯度计算起来简单直接：\n    $$\n    \\nabla J_b(x_0) = B^{-1}(x_0 - x_b)\n    $$\n    给定 $B = \\sigma_b^2 I$，其逆为 $B^{-1} = \\sigma_b^{-2} I$，因此该项简化为 $\\sigma_b^{-2}(x_0 - x_b)$。\n\n2.  **观测项的梯度（伴随方法）**\n    为了求 $\\nabla J_o(x_0)$，我们使用拉格朗日乘子法，由此产生伴随模型。我们将模型方程 $x_{k+1} = M x_k$（对于 $k=0, \\dots, K-1$）视为约束。拉格朗日量 $\\mathcal{L}$ 为：\n    $$\n    \\mathcal{L}(\\{x_k\\}_{k=0}^K, \\{\\lambda_k\\}_{k=1}^K) = J(\\{x_k\\}) + \\sum_{k=0}^{K-1} \\lambda_{k+1}^\\top (M x_k - x_{k+1})\n    $$\n    在这里，代价函数用完整状态轨迹 $\\{x_k\\}$ 表示。向量 $\\lambda_k$ 是拉格朗日乘子，也称为伴随变量。在最优点，拉格朗日量对其所有变量的梯度为零。$J(x_0)$ 的梯度等价于受约束代价函数的全导数 $\\frac{dJ}{dx_0}$。我们可以在施加条件 $\\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0$（对于 $k=1, \\dots, K$）后，通过计算 $\\frac{\\partial \\mathcal{L}}{\\partial x_0}$ 来求得。\n\n    对 $\\mathcal{L}$ 关于状态 $x_k$（其中 $k \\in \\{1, \\dots, K\\}$）求偏导数，得出：\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = \\frac{\\partial J}{\\partial x_k} + M^\\top \\lambda_{k+1} - \\lambda_k = H^\\top R^{-1} (H x_k - y_k) + M^\\top \\lambda_{k+1} - \\lambda_k\n    $$\n    令其为零可得到伴随方程。对于 $k=K$，求和中不存在 $\\lambda_{K+1}$ 项，因此我们定义 $\\lambda_{K+1}=0$。关于 $x_K$ 的导数为：\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_K} = H^\\top R^{-1} (H x_K - y_K) - \\lambda_K = 0 \\implies \\lambda_K = H^\\top R^{-1} (H x_K - y_K)\n    $$\n    对于 $k \\in \\{1, \\dots, K-1\\}$，我们有：\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0 \\implies \\lambda_k = M^\\top \\lambda_{k+1} + H^\\top R^{-1} (H x_k - y_k)\n    $$\n    这些方程定义了伴随变量的反向递推关系，从 $\\lambda_K$ 开始，随时间向后传播到 $\\lambda_1$。注意，由于 $R = \\sigma_r^2 I$，项 $H^\\top R^{-1} (H x_k - y_k)$ 简化为 $\\sigma_r^{-2} H^\\top (H x_k - y_k)$。\n\n    最后，通过对 $\\mathcal{L}$ 关于 $x_0$ 求导，得到代价函数关于初始状态 $x_0$ 的梯度：\n    $$\n    \\nabla J(x_0) = \\frac{d\\mathcal{L}}{dx_0} = \\frac{\\partial\\mathcal{L}}{\\partial x_0} = \\frac{\\partial J_b}{\\partial x_0} + M^\\top \\lambda_1 = B^{-1}(x_0 - x_b) + M^\\top \\lambda_1\n    $$\n\n3.  **梯度计算的算法总结**\n    计算 $\\nabla J(x_0)$ 的过程如下：\n    a.  **正向运行**：从 $x_0$ 开始，对 $k = 0, \\dots, K-1$ 积分正向模型 $x_{k+1} = M x_k$，并存储整个状态轨迹 $\\{x_k\\}_{k=1}^K$。\n    b.  **反向（伴随）运行**：\n        i.  在最终时间 $K$ 初始化伴随变量：$\\lambda_K = \\sigma_r^{-2} H^\\top (H x_K - y_K)$。\n        ii. 从 $k=K-1$ 向下到 $1$ 随时间反向积分伴随模型：$\\lambda_k = M^\\top \\lambda_{k+1} + \\sigma_r^{-2} H^\\top (H x_k - y_k)$。\n    c.  **梯度组装**：使用反向运行的结果计算最终梯度：$\\nabla J(x_0) = \\sigma_b^{-2}(x_0 - x_b) + M^\\top \\lambda_1$。\n\n4.  **梯度检验过程**\n    梯度检验通过将解析梯度沿随机方向 $v$ 的投影与方向导数的有限差分近似进行比较来验证解析梯度。\n    -   基于伴随的方向导数是 $D_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v$。\n    -   使用中心差分格式的有限差分近似是 $D_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}$。\n    该实现将为多个随机单位向量 $v$ 计算这两个量，计算绝对误差 $E_{\\mathrm{abs}}$ 和相对误差 $E_{\\mathrm{rel}}$，如果误差在指定的容差 $\\text{tol}_{\\mathrm{abs}}$ 和 $\\text{tol}_{\\mathrm{rel}}$ 范围内，则接受该梯度。对于一个参数集的总体检验只有在所有测试的随机方向都被接受时才通过。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient check for all test cases and print the results.\n    \"\"\"\n\n    def run_gradient_check(n, K, dt, c, nu, m, sigma_b, sigma_r, epsilon, tol_rel, tol_abs, num_directions, seed):\n        \"\"\"\n        Performs a gradient check for a 4D-Var cost function for a single parameter set.\n        \"\"\"\n        # 1. Setup\n        np.random.seed(seed)\n        dx = 1.0 / n\n\n        # 2. Build matrices M, and define H operation\n        I = np.eye(n)\n        # (S- @ x)_i = x_{i-1}. Matrix equivalent is np.roll(I, 1, axis=0)\n        S_minus = np.roll(I, 1, axis=0)\n        # (S+ @ x)_i = x_{i+1}. Matrix equivalent is np.roll(I, -1, axis=0)\n        S_plus = np.roll(I, -1, axis=0)\n        \n        A_adv = -(c / dx) * (I - S_minus)\n        A_diff = (nu / dx**2) * (S_plus + S_minus - 2 * I)\n        M = I + dt * (A_adv + A_diff)\n        M_T = M.T\n        \n        obs_indices = np.linspace(0, n - 1, num=m, dtype=int)\n\n        # 3. Generate data (truth, obs, background)\n        x0_true = np.random.randn(n)\n        \n        y_obs_list = []\n        x_k_true = np.copy(x0_true)\n        for _ in range(K):\n            x_k_true = M @ x_k_true\n            obs_noise = np.random.randn(m)\n            y_k = x_k_true[obs_indices] + sigma_r * obs_noise\n            y_obs_list.append(y_k)\n            \n        bg_noise = np.random.randn(n)\n        x_b = x0_true + sigma_b * bg_noise\n        \n        # 4. Define cost and gradient functions\n        inv_sigma_b2 = 1.0 / sigma_b**2\n        inv_sigma_r2 = 1.0 / sigma_r**2\n\n        def calculate_J(x0):\n            j_b = 0.5 * inv_sigma_b2 * np.sum((x0 - x_b)**2)\n            \n            j_o = 0.0\n            x_k = np.copy(x0)\n            for k in range(K):\n                x_k = M @ x_k\n                innov = x_k[obs_indices] - y_obs_list[k]\n                j_o += 0.5 * inv_sigma_r2 * np.sum(innov**2)\n                \n            return j_b + j_o\n\n        def calculate_grad_J(x0):\n            # Forward run: store trajectory\n            x_traj = [x0]\n            x_k = np.copy(x0)\n            for _ in range(K):\n                x_k = M @ x_k\n                x_traj.append(x_k)\n\n            # Backward run: compute adjoint variable\n            # Initialize lambda_K\n            x_K = x_traj[K]\n            y_K = y_obs_list[K-1] # y_obs_list is 0-indexed for k=1..K\n            innov_K = x_K[obs_indices] - y_K\n            forcing_term_K = np.zeros(n)\n            forcing_term_K[obs_indices] = inv_sigma_r2 * innov_K\n            lambda_next = forcing_term_K # This is lambda_K\n\n            # Loop for k from K-1 down to 1\n            for k in range(K - 1, 0, -1):\n                x_k = x_traj[k]\n                y_k = y_obs_list[k-1]\n                \n                innov_k = x_k[obs_indices] - y_k\n                forcing_term_k = np.zeros(n)\n                forcing_term_k[obs_indices] = inv_sigma_r2 * innov_k\n                \n                lambda_current = M_T @ lambda_next + forcing_term_k\n                lambda_next = lambda_current\n\n            # After loop, lambda_next is lambda_1\n            lambda_1 = lambda_next\n            \n            grad_b = inv_sigma_b2 * (x0 - x_b)\n            grad_o = M_T @ lambda_1\n            \n            return grad_b + grad_o\n\n        # 5. Perform the gradient check at x0 = x_b\n        x0_eval = x_b\n        grad_adj = calculate_grad_J(x0_eval)\n        \n        for i in range(num_directions):\n            v = np.random.randn(n)\n            v /= np.linalg.norm(v)\n\n            J_plus = calculate_J(x0_eval + epsilon * v)\n            J_minus = calculate_J(x0_eval - epsilon * v)\n\n            D_fd = (J_plus - J_minus) / (2.0 * epsilon)\n            D_adj = grad_adj.T @ v\n\n            E_abs = np.abs(D_fd - D_adj)\n            denom = max(1.0, np.abs(D_fd), np.abs(D_adj))\n            E_rel = E_abs / denom\n\n            if not (E_rel = tol_rel or E_abs = tol_abs):\n                return False # Failed test for this direction\n        \n        return True # All directions passed\n\n    test_cases = [\n        # n, K, dt, c, nu, m, sigma_b, sigma_r, epsilon, tol_rel, tol_abs, num_directions\n        (32, 6, 1e-2, 2e-1, 5e-3, 16, 5e-1, 1e-1, 1e-6, 1e-7, 1e-10, 5),\n        (32, 6, 1e-2, 2e-1, 5e-3, 16, 5e-1, 1e-1, 1e-10, 1e-3, 1e-8, 5),\n        (32, 6, 1e-2, 2e-1, 5e-3, 8, 5e-1, 1e2, 1e-6, 1e-7, 1e-10, 5),\n        (40, 10, 5e-3, 5e-1, 5e-3, 40, 2e-1, 5e-2, 1e-6, 1e-7, 1e-10, 5),\n        (24, 5, 1e-2, 3e-1, 4e-3, 12, 1e-2, 2e-1, 1e-6, 1e-7, 1e-10, 5)\n    ]\n\n    results = []\n    for i, case_params in enumerate(test_cases):\n        # Use an independent random seed for each test case for reproducibility\n        result = run_gradient_check(*case_params, seed=i)\n        results.append(result)\n\n    # Format output as a lowercase boolean list: [true,false,...]\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在验证了代价函数梯度的准确性之后，下一步便是利用该梯度来求解最优的初始状态。由于4D-Var代价函数可能具有复杂的非线性特征，简单的梯度下降法可能效率低下或不稳定。本实践将介绍一种更稳健的优化算法——信赖域方法（trust-region method），通过构建代价函数的二次模型并在此模型的可信范围内求解最优步长，从而高效地逼近最优解。",
            "id": "3793624",
            "problem": "考虑定义在状态向量 $\\mathbf{x} \\in \\mathbb{R}^n$ 上的线性四维变分资料同化代价函数\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\tfrac{1}{2}\\sum_{t=1}^{T}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right),\n$$\n其中 $\\mathbf{x}_b \\in \\mathbb{R}^n$ 是背景场状态，$\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ 是背景场误差协方差矩阵，$\\mathbf{M}_t \\in \\mathbb{R}^{n \\times n}$ 是时间索引 $t$ 处的线性模式传播算子，$\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$ 是线性观测算子，$\\mathbf{y}_t \\in \\mathbb{R}^m$ 是时间索引 $t$ 处的观测向量，以及 $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ 是观测误差协方差矩阵。对于给定的 $\\mathbf{x}_k$，定义二次模型\n$$\nm(\\mathbf{p}) = J(\\mathbf{x}_k) + \\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p},\n$$\n其中 $\\mathbf{g} = \\nabla J(\\mathbf{x}_k)$ 且 $\\mathbf{H}_{\\text{approx}}$ 是 $J$ 在 $\\mathbf{x}_k$ 处 Hessian 矩阵的一个正定近似。信赖域子问题旨在寻求\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^n} \\quad \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p} + \\mathbf{g}^\\top \\mathbf{p} \\quad \\text{subject to} \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta,\n$$\n对于给定的信赖域半径 $\\Delta  0$。预测下降量定义为\n$$\n\\text{pred} = m(\\mathbf{0}) - m(\\mathbf{p}) = -\\left(\\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p}\\right),\n$$\n而实际下降量为\n$$\n\\text{ared} = J(\\mathbf{x}_k) - J(\\mathbf{x}_k + \\mathbf{p}).\n$$\n监控收敛性的比率 $\\rho$ 是\n$$\n\\rho = \\frac{\\text{ared}}{\\text{pred}},\n$$\n并约定如果 $\\text{pred}$ 在数值上为零（如下文所述），则报告的 $\\rho$ 必须为 $0.0$。\n\n从以上定义以及 $\\mathbf{M}_t$ 和 $\\mathbf{H}$ 的线性性质出发，可以得出 $J$ 在任意 $\\mathbf{x}$ 处的梯度和精确 Hessian 矩阵为\n$$\n\\mathbf{g}(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right),\n\\quad\n\\mathbf{H}_{\\text{true}} = \\mathbf{B}^{-1} + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\mathbf{H}\\mathbf{M}_t.\n$$\n在本任务中，将 Hessian 近似设置为\n$$\n\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I},\n$$\n对于给定的标量 $\\alpha \\ge 0$，其中 $\\mathbf{I}$ 是单位矩阵，以模拟高斯-牛顿阻尼或正则化。信赖域步长 $\\mathbf{p}$ 必须通过求解一阶最优性条件来计算\n$$\n(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}, \\quad \\lambda \\ge 0, \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta, \\quad \\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0,\n$$\n使用一个基于对 $\\lambda$ 进行标量求根和线性求解的稳健算法，确保对于正定 $\\mathbf{H}_{\\text{approx}}$ 的数值稳定性。然后评估 $\\text{pred}$ 和 $\\text{ared}$ 并计算 $\\rho$。如果对于容差 $\\epsilon = 10^{-12}$，有 $|\\text{pred}|  \\epsilon$，则报告 $\\rho = 0.0$。\n\n实现一个程序，对以下测试套件执行此过程。所有矩阵和向量都在 $\\mathbb{R}$ 中指定，所有数字都应视为无量纲。对 $\\|\\cdot\\|_2$ 使用欧几里得范数。\n\n所有测试用例的通用基本定义：\n- 维度 $n = 3$，观测维度 $m = 2$，以及时间步数 $T = 2$。\n- 背景场状态 $\\mathbf{x}_b = [\\,0.5,\\,-0.2,\\,0.1\\,]^\\top$。\n- 背景场协方差 $\\mathbf{B} = \\operatorname{diag}([\\,0.4^2,\\,0.3^2,\\,0.5^2\\,])$，因此 $\\mathbf{B}^{-1} = \\operatorname{diag}([\\,6.25,\\,11.\\overline{1},\\,4.0\\,])$。\n- 观测协方差 $\\mathbf{R} = \\operatorname{diag}([\\,0.2^2,\\,0.3^2\\,])$，因此 $\\mathbf{R}^{-1} = \\operatorname{diag}([\\,25.0,\\,11.\\overline{1}\\,])$。\n- 观测算子\n$$\n\\mathbf{H} = \\begin{bmatrix}\n1.0  0.0  0.0 \\\\\n0.0  1.0  0.3\n\\end{bmatrix}.\n$$\n- 线性模式传播算子\n$$\n\\mathbf{M}_1 = \\begin{bmatrix}\n1.0  0.1  0.0 \\\\\n0.0  0.9  0.2 \\\\\n0.0  0.0  1.0\n\\end{bmatrix},\n\\quad\n\\mathbf{M}_2 = \\begin{bmatrix}\n0.9  0.0  0.1 \\\\\n0.1  1.0  0.0 \\\\\n0.0  0.2  0.95\n\\end{bmatrix}.\n$$\n- 一般情况下的名义观测值：\n$$\n\\mathbf{y}_1 = \\begin{bmatrix} 0.7 \\\\ -0.15 \\end{bmatrix}, \\quad\n\\mathbf{y}_2 = \\begin{bmatrix} 0.6 \\\\ -0.05 \\end{bmatrix}.\n$$\n\n定义四个测试用例如下：\n- 用例 1（理想路径）：$\\mathbf{x}_k = [\\,0.6,\\,-0.25,\\,0.0\\,]^\\top$，信赖域半径 $\\Delta = 1.5$，阻尼 $\\alpha = 0.0$，观测值 $\\mathbf{y}_1$ 和 $\\mathbf{y}_2$ 如上指定。\n- 用例 2（梯度近似为零）：$\\mathbf{x}_k = \\mathbf{x}_b$，信赖域半径 $\\Delta = 1.0$，阻尼 $\\alpha = 0.0$，且观测值选择为与背景场一致，即对 $t \\in \\{1,2\\}$ 有 $\\mathbf{y}_t = \\mathbf{H}\\mathbf{M}_t\\mathbf{x}_b$。\n- 用例 3（带阻尼的紧信赖域）：$\\mathbf{x}_k = [\\,0.0,\\,0.0,\\,0.0\\,]^\\top$，信赖域半径 $\\Delta = 0.1$，阻尼 $\\alpha = 0.5$，观测值 $\\mathbf{y}_1$ 和 $\\mathbf{y}_2$ 如上指定。\n- 用例 4（强阻尼）：$\\mathbf{x}_k = [\\,-0.3,\\,0.4,\\,-0.1\\,]^\\top$，信赖域半径 $\\Delta = 0.8$，阻尼 $\\alpha = 2.0$，观测值 $\\mathbf{y}_1$ 和 $\\mathbf{y}_2$ 如上指定。\n\n程序要求：\n- 使用上述定义计算 $\\mathbf{g}$ 和 $\\mathbf{H}_{\\text{true}}$。\n- 构造 $\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I}$。\n- 通过求解 $(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}$（其中 $\\lambda \\ge 0$ 使得 $\\|\\mathbf{p}\\|_2 \\le \\Delta$ 和 $\\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0$）来计算信赖域步长 $\\mathbf{p}$，需通过对 $\\lambda$ 进行稳健的标量搜索。\n- 评估 $\\text{pred}$ 和 $\\text{ared}$ 并计算 $\\rho = \\text{ared}/\\text{pred}$，并约定如果对于容差 $\\epsilon = 10^{-12}$ 有 $|\\text{pred}|  \\epsilon$，则报告 $\\rho = 0.0$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，即 $[\\,\\rho_1,\\rho_2,\\rho_3,\\rho_4\\,]$，其中 $\\rho_i$ 对应于用例 $i$。\n\n所有计算都是纯数学和无量纲的；不需要物理单位。不使用角度。不得使用百分比；任何比率都必须按规定报告为十进制浮点数。",
            "solution": "用户提供了一个在四维变分资料同化（4D-Var）领域的明确定义的计算问题，该领域是计算海洋学和数值天气预报的一个子领域。该问题在科学上是合理的，数学上是一致的，并且提供了所有必要的数据和定义。这是一个有效的问题。\n\n任务是为四个不同的测试用例计算比率 $\\rho = \\text{ared} / \\text{pred}$。这个比率是信赖域优化方法中的一个标准度量，用于评估目标函数的二次模型的质量。该过程涉及几个步骤：定义代价函数及其导数，求解一个信赖域子问题以找到最优步长，然后评估代价函数的实际下降量和预测下降量。\n\n### 步骤 1：代价函数及其导数的定义\n\n4D-Var 代价函数 $J(\\mathbf{x})$ 是状态向量 $\\mathbf{x} \\in \\mathbb{R}^n$ 的二次函数：\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\tfrac{1}{2}\\sum_{t=1}^{T}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)\n$$\n所有矩阵和向量都在问题陈述中定义。鉴于 $J(\\mathbf{x})$ 是二次的，其梯度 $\\mathbf{g}(\\mathbf{x}) = \\nabla J(\\mathbf{x})$ 是 $\\mathbf{x}$ 的线性函数，其 Hessian 矩阵 $\\mathbf{H}_{\\text{true}} = \\nabla^2 J(\\mathbf{x})$ 是一个常数矩阵。所提供的表达式是正确的：\n\n在一点 $\\mathbf{x}$ 处的梯度是：\n$$\n\\mathbf{g}(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)\n$$\n\n对于所有 $\\mathbf{x}$，精确 Hessian 矩阵是恒定的：\n$$\n\\mathbf{H}_{\\text{true}} = \\mathbf{B}^{-1} + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\mathbf{H}\\mathbf{M}_t\n$$\n\n对于每个测试用例，我们首先在给定的迭代点 $\\mathbf{x}_k$ 处计算梯度 $\\mathbf{g} = \\mathbf{g}(\\mathbf{x}_k)$。然后，Hessian 近似矩阵形成为 $\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I}$，其中 $\\mathbf{H}_{\\text{true}}$ 可以预先计算，因为它与 $\\mathbf{x}_k$ 和观测值 $\\mathbf{y}_t$ 无关。\n\n### 步骤 2：求解信赖域子问题\n\n任务的核心是求解关于步长 $\\mathbf{p}$ 的信赖域子问题：\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^n} \\quad \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p} + \\mathbf{g}^\\top \\mathbf{p} \\quad \\text{subject to} \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta\n$$\n解 $\\mathbf{p}$ 必须满足 Karush-Kuhn-Tucker (KKT) 条件：\n$$\n(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}, \\quad \\lambda \\ge 0, \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta, \\quad \\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0\n$$\n由于 $\\mathbf{B}$ 和 $\\mathbf{R}$ 是协方差矩阵，它们是对称正定的，其逆矩阵也是如此。这保证了 $\\mathbf{H}_{\\text{true}}$ 是正定的。在 $\\alpha \\ge 0$ 的情况下，$\\mathbf{H}_{\\text{approx}}$ 也是正定的。这简化了求解过程，因为我们不需要考虑所谓的信赖域子问题的“困难情况”。\n\n求解算法如下：\n1.  **检查平凡解**：如果梯度范数 $\\|\\mathbf{g}\\|_2$ 接近于零，则当前点 $\\mathbf{x}_k$ 已经是最优的。步长为 $\\mathbf{p} = \\mathbf{0}$。这适用于用例 2。\n2.  **内部解**：首先，尝试通过设置 $\\lambda=0$ 来找到无约束最小化子。这给出了牛顿步 $\\mathbf{p}_N = -\\mathbf{H}_{\\text{approx}}^{-1}\\mathbf{g}$。如果 $\\|\\mathbf{p}_N\\|_2 \\le \\Delta$，那么 $\\mathbf{p}=\\mathbf{p}_N$ 就是解，且信赖域约束是非活动的。\n3.  **边界解**：如果 $\\|\\mathbf{p}_N\\|_2  \\Delta$，解必须位于信赖域的边界上，即 $\\|\\mathbf{p}\\|_2 = \\Delta$。这需要找到一个 $\\lambda  0$ 来满足久期方程 $\\|\\mathbf{p}(\\lambda)\\|_2 = \\Delta$，其中 $\\mathbf{p}(\\lambda) = -(\\mathbf{H}_{\\text{approx}} + \\lambda\\mathbf{I})^{-1}\\mathbf{g}$。函数 $\\phi(\\lambda) = \\|\\mathbf{p}(\\lambda)\\|_2 - \\Delta$ 对于 $\\lambda \\ge 0$ 是一个单调递减函数。我们可以使用数值求根算法（例如 SciPy 中的 Brent-Dekker 方法 `brentq`）来找到 $\\phi(\\lambda) = 0$ 的唯一根 $\\lambda^*  0$。对 $\\lambda^*$ 的搜索在区间 $[a, b]$ 上进行，其中 $\\phi(a)0$ 且 $\\phi(b)0$。我们知道 $\\phi(0)0$，并且可以为 $b$ 找到一个足够大的值。\n\n### 步骤 3：$\\rho$ 的计算\n\n一旦确定了步长 $\\mathbf{p}$，就可以计算预测下降量和实际下降量。\n\n预测下降量基于 $J$ 的二次模型：\n$$\n\\text{pred} = -\\left(\\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p}\\right)\n$$\n\n实际下降量基于真实的代价函数 $J$：\n$$\n\\text{ared} = J(\\mathbf{x}_k) - J(\\mathbf{x}_k + \\mathbf{p})\n$$\n\n最后，计算比率 $\\rho$：\n$$\n\\rho = \\frac{\\text{ared}}{\\text{pred}}\n$$\n根据问题规范，如果 $|\\text{pred}|  10^{-12}$，我们设置 $\\rho = 0.0$ 以避免除以一个接近零的数。这与用例 2 相关，其中 $\\mathbf{g}=\\mathbf{0}$ 导致 $\\mathbf{p}=\\mathbf{0}$ 和 $\\text{pred}=0$。\n\n对四个测试用例中的每一个都实施了这整个过程，从而产生四个 $\\rho$ 值。该实现使用 `numpy` 进行线性代数运算，并使用 `scipy.optimize.brentq` 进行标量求根。所有计算均使用双精度浮点算术执行。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the 4D-Var trust-region problem for the four specified test cases.\n    \"\"\"\n\n    # 1. Define constants and common matrices\n    n, m, T = 3, 2, 2\n    xb = np.array([0.5, -0.2, 0.1])\n    Binv = np.diag([6.25, 100.0 / 9.0, 4.0])\n    Rinv = np.diag([25.0, 100.0 / 9.0])\n    I = np.eye(n)\n\n    H_op = np.array([[1.0, 0.0, 0.0],\n                     [0.0, 1.0, 0.3]])\n\n    M1 = np.array([[1.0, 0.1, 0.0],\n                   [0.0, 0.9, 0.2],\n                   [0.0, 0.0, 1.0]])\n\n    M2 = np.array([[0.9, 0.0, 0.1],\n                   [0.1, 1.0, 0.0],\n                   [0.0, 0.2, 0.95]])\n\n    # 2. Pre-compute constant matrices\n    C1 = H_op @ M1\n    C2 = H_op @ M2\n    C1T = C1.T\n    C2T = C2.T\n\n    # H_true = B_inv + C1^T R_inv C1 + C2^T R_inv C2\n    H_true = Binv + C1T @ Rinv @ C1 + C2T @ Rinv @ C2\n\n    # 3. Define helper functions that capture the constants from the outer scope\n    def J_cost(x, y1, y2):\n        term_b = 0.5 * (x - xb).T @ Binv @ (x - xb)\n        term_o1 = 0.5 * (C1 @ x - y1).T @ Rinv @ (C1 @ x - y1)\n        term_o2 = 0.5 * (C2 @ x - y2).T @ Rinv @ (C2 @ x - y2)\n        return term_b + term_o1 + term_o2\n\n    def gradient(x, y1, y2):\n        term_b = Binv @ (x - xb)\n        term_o1 = C1T @ Rinv @ (C1 @ x - y1)\n        term_o2 = C2T @ Rinv @ (C2 @ x - y2)\n        return term_b + term_o1 + term_o2\n\n    def solve_trust_region_subproblem(H_approx, g, delta):\n        g_norm = np.linalg.norm(g)\n        if g_norm  1e-15:\n            return np.zeros_like(g)\n\n        # Try interior solution (lambda = 0)\n        try:\n            p0 = np.linalg.solve(H_approx, -g)\n            if np.linalg.norm(p0) = delta:\n                return p0\n        except np.linalg.LinAlgError:\n            # H_approx is guaranteed to be PD in this problem, but this is good practice.\n            pass\n\n        # Boundary solution: find lambda > 0 such that ||p(lambda)||_2 = delta\n        def phi(lmbda):\n            mat = H_approx + lmbda * I\n            try:\n                p_lambda = np.linalg.solve(mat, -g)\n                return np.linalg.norm(p_lambda) - delta\n            except np.linalg.LinAlgError:\n                # Should not happen for lambda >= 0 as H_approx is PD\n                return 1e9\n\n        lambda_lower = 0.0\n        # Establish an upper bound for the root finding search\n        lambda_upper = g_norm / delta if delta > 0 else g_norm\n        if lambda_upper == 0: lambda_upper = 1.0 # Avoid zero upper bound\n        \n        while phi(lambda_upper) > 0:\n            lambda_upper *= 2\n\n        lambda_sol = brentq(phi, lambda_lower, lambda_upper)\n        p = np.linalg.solve(H_approx + lambda_sol * I, -g)\n        return p\n\n    # 4. Define and process test cases\n    y1_nom = np.array([0.7, -0.15])\n    y2_nom = np.array([0.6, -0.05])\n    \n    y1_case2 = C1 @ xb\n    y2_case2 = C2 @ xb\n\n    test_cases = [\n        {'xk': np.array([0.6, -0.25, 0.0]), 'delta': 1.5, 'alpha': 0.0, 'y1': y1_nom, 'y2': y2_nom},\n        {'xk': xb, 'delta': 1.0, 'alpha': 0.0, 'y1': y1_case2, 'y2': y2_case2},\n        {'xk': np.array([0.0, 0.0, 0.0]), 'delta': 0.1, 'alpha': 0.5, 'y1': y1_nom, 'y2': y2_nom},\n        {'xk': np.array([-0.3, 0.4, -0.1]), 'delta': 0.8, 'alpha': 2.0, 'y1': y1_nom, 'y2': y2_nom},\n    ]\n\n    results = []\n    pred_tolerance = 1e-12\n\n    for case in test_cases:\n        xk, delta, alpha = case['xk'], case['delta'], case['alpha']\n        y1, y2 = case['y1'], case['y2']\n\n        H_approx = H_true + alpha * I\n        g = gradient(xk, y1, y2)\n        \n        p = solve_trust_region_subproblem(H_approx, g, delta)\n        \n        pred = -(g.T @ p + 0.5 * p.T @ H_approx @ p)\n        \n        if abs(pred)  pred_tolerance:\n            rho = 0.0\n        else:\n            ared = J_cost(xk, y1, y2) - J_cost(xk + p, y1, y2)\n            rho = ared / pred\n            \n        results.append(rho)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个成功的资料同化系统不仅需要给出最优分析，还需要其统计假设（如背景误差协方差 $B$ 和观测误差协方差 $R$）与实际情况相符。本实践将引导您使用新息（观测减预报）和分析增量来构建卡方（$\\chi^2$）统计量。这些诊断工具能够量化评估同化系统与预设统计误差之间的一致性，是调整和验证同化系统性能的关键环节。",
            "id": "3793697",
            "problem": "在一个简化的、线性的计算海洋学设置中，考虑一个包含三个等间距时间点 $t_{0}$、$t_{1}$ 和 $t_{2}$ 的四维变分数据同化（4D-Var）窗口。在每个时间点，观测两个标量海洋量：海面高度异常和现场温度。假设如下：\n\n- 每个时间点的观测算子都是单位矩阵，$H_{t} = I$，将模式状态直接映射到观测值。\n- 每个时间点的观测误差是零均值高斯分布，并且在各分量和时间上相互独立，对于所有 $t \\in \\{t_{0}, t_{1}, t_{2}\\}$，观测误差协方差为 $R_{t} = \\mathrm{diag}(0.25, 1)$。\n- 三个时间点的观测新息（观测值减去模式等效值）为\n$$\nd_{t_{0}} = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}, \\quad\nd_{t_{1}} = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix}, \\quad\nd_{t_{2}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\n- 初始时间的背景（先验）状态在初始条件的三维控制空间中具有零均值高斯误差，其协方差为 $B = \\mathrm{diag}(1, 4, 0.25)$。初始时间对应的分析增量为\n$$\n\\delta x_{0} = x_{0} - x_{b} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{pmatrix}.\n$$\n- 观测误差在时间上相互独立，且与背景误差相互独立。\n\n从高斯误差假设和 4D-Var 成本函数分量的基本定义出发，构建窗口内基于新息和基于背景的卡方统计量，并确定它们各自的自由度。然后将组合归一化卡方诊断量定义为\n$$\nc_{\\mathrm{tot}} = \\frac{S_{o} + S_{b}}{m + n},\n$$\n其中，$S_{o}$ 是整个窗口内基于新息的总卡方统计量，$S_{b}$ 是初始时间基于背景的卡方统计量，$m$ 是整个窗口内独立观测分量的总数，$n$ 是背景控制向量的维度。根据给定数据计算 $c_{\\mathrm{tot}}$。将答案四舍五入至四位有效数字，并以无单位的纯数字形式表示。",
            "solution": "问题陈述已经过验证，被认为是有效的。它在四维变分数据同化（4D-Var）框架内具有科学依据，是适定的、客观的且内部一致的。所有计算所需诊断量的必要数据和定义均已提供。\n\n目标是计算组合归一化卡方诊断量 $c_{\\mathrm{tot}}$，其定义如下：\n$$\nc_{\\mathrm{tot}} = \\frac{S_{o} + S_{b}}{m + n}\n$$\n其中 $S_{o}$ 是基于新息的总卡方统计量，$S_{b}$ 是基于背景的卡方统计量，$m$ 是观测总数，$n$ 是控制向量的维度。我们将分别计算这四个分量。\n\n首先，我们确定自由度 $n$ 和 $m$。\n背景控制向量的维度 $n$ 对应于初始条件控制空间的大小。给定的背景误差协方差矩阵 $B$ 是一个 $3 \\times 3$ 的对角矩阵，分析增量 $\\delta x_0$ 是一个 3 维向量。因此，背景的自由度为 $n=3$。\n\n观测分量的总数 $m$ 是每个时间步的标量观测数乘以时间步数。在三个时间步（$t_{0}$、$t_{1}$、$t_{2}$）中的每一个，都观测了两个标量。因此，观测总数，也即基于新息的统计量的自由度，为 $m = 2 \\times 3 = 6$。\n\n接下来，我们构建并计算基于背景的卡方统计量 $S_{b}$。该统计量与背景（或先验）状态对 4D-Var 成本函数的贡献相关，在分析解处进行求值。其定义为：\n$$\nS_{b} = (x_{0} - x_{b})^{T} B^{-1} (x_{0} - x_{b}) = \\delta x_{0}^{T} B^{-1} \\delta x_{0}\n$$\n我们已知初始时间的分析增量：\n$$\n\\delta x_{0} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{pmatrix}\n$$\n以及背景误差协方差矩阵：\n$$\nB = \\mathrm{diag}(1, 4, 0.25)\n$$\n该对角矩阵的逆矩阵为：\n$$\nB^{-1} = \\mathrm{diag}(1^{-1}, 4^{-1}, 0.25^{-1}) = \\mathrm{diag}(1, 0.25, 4) = \\begin{pmatrix} 1  0  0 \\\\ 0  0.25  0 \\\\ 0  0  4 \\end{pmatrix}\n$$\n现在我们计算 $S_{b}$：\n$$\nS_{b} = \\begin{pmatrix} 1  -2  0.5 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  0.25  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{pmatrix}\n$$\n$$\nS_{b} = 1^{2} \\times 1 + (-2)^{2} \\times 0.25 + (0.5)^{2} \\times 4\n$$\n$$\nS_{b} = 1 \\times 1 + 4 \\times 0.25 + 0.25 \\times 4 = 1 + 1 + 1 = 3\n$$\n\n接下来，我们构建并计算基于新息的总卡方统计量 $S_{o}$。该统计量量化了由观测误差协方差加权的观测值与背景模式预报之间的失配。它通过对每个观测时间的卡方值求和来计算：\n$$\nS_{o} = \\sum_{t \\in \\{t_0, t_1, t_2\\}} S_{o,t} = \\sum_{t \\in \\{t_0, t_1, t_2\\}} d_{t}^{T} R_{t}^{-1} d_{t}\n$$\n观测误差协方差矩阵 $R_{t}$ 对所有时间步都相同：\n$$\nR_{t} = \\mathrm{diag}(0.25, 1)\n$$\n其逆矩阵为：\n$$\nR_{t}^{-1} = \\mathrm{diag}(0.25^{-1}, 1^{-1}) = \\mathrm{diag}(4, 1) = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n$$\n我们使用给定的新息向量 $d_t$ 计算每个时间步的贡献：\n\n对于 $t=t_{0}$：$d_{t_{0}} = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}$\n$$\nS_{o,t_{0}} = d_{t_{0}}^{T} R_{t_{0}}^{-1} d_{t_{0}} = (0.5)^{2} \\times 4 + 1^{2} \\times 1 = 0.25 \\times 4 + 1 = 1 + 1 = 2\n$$\n\n对于 $t=t_{1}$：$d_{t_{1}} = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix}$\n$$\nS_{o,t_{1}} = d_{t_{1}}^{T} R_{t_{1}}^{-1} d_{t_{1}} = (-0.5)^{2} \\times 4 + (-2)^{2} \\times 1 = 0.25 \\times 4 + 4 = 1 + 4 = 5\n$$\n\n对于 $t=t_{2}$：$d_{t_{2}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n$$\nS_{o,t_{2}} = d_{t_{2}}^{T} R_{t_{2}}^{-1} d_{t_{2}} = 0^{2} \\times 4 + 1^{2} \\times 1 = 0 + 1 = 1\n$$\n基于新息的总卡方统计量是这些值的总和：\n$$\nS_{o} = S_{o,t_{0}} + S_{o,t_{1}} + S_{o,t_{2}} = 2 + 5 + 1 = 8\n$$\n\n最后，我们计算组合归一化卡方诊断量 $c_{\\mathrm{tot}}$：\n$$\nc_{\\mathrm{tot}} = \\frac{S_{o} + S_{b}}{m + n} = \\frac{8 + 3}{6 + 3} = \\frac{11}{9}\n$$\n对该分数进行数值计算：\n$$\nc_{\\mathrm{tot}} = 1.2222...\n$$\n四舍五入到四位有效数字，我们得到 $1.222$。",
            "answer": "$$\n\\boxed{1.222}\n$$"
        }
    ]
}