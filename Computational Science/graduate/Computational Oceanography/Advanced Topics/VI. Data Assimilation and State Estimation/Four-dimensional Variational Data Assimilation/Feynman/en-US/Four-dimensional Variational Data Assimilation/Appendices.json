{
    "hands_on_practices": [
        {
            "introduction": "Before any optimization can be performed in variational data assimilation, we must have confidence in our calculation of the cost function's gradient. The adjoint method provides an efficient pathway to this gradient, but its implementation can be complex and prone to error. This practice introduces the \"gradient check,\" a fundamental verification technique that compares the analytically derived adjoint gradient against a numerical approximation from finite differences (). Mastering this check is a critical first step in developing any variational data assimilation system, ensuring the core of the optimization algorithm is built on a correct foundation.",
            "id": "3793675",
            "problem": "You must implement a rigorous gradient check for the cost function of Four-Dimensional Variational Data Assimilation (4D-Var) in computational oceanography. Begin from the definition of the 4D-Var cost function and a linear, one-dimensional periodic tracer model. You will compare finite-difference directional derivatives against adjoint-based gradients along random directions and decide acceptance using clear tolerances.\n\nSet up the following purely mathematical and numerically self-consistent problem in non-dimensional units. Consider a one-dimensional periodic domain with $n$ grid points and grid spacing $\\Delta x = 1/n$. Let $\\Delta t$ be the time step, $K$ the number of time steps, $c$ a constant advection velocity, and $\\nu$ a constant diffusion coefficient. The linear forward model updates the tracer state $x_k \\in \\mathbb{R}^n$ via\n$$\nx_{k+1} = M x_k,\n$$\nwhere the model matrix $M \\in \\mathbb{R}^{n \\times n}$ is defined by the explicit Euler discretization of advection-diffusion with periodic boundary conditions:\n$$\nM = I + \\Delta t \\left( A_{\\mathrm{adv}} + A_{\\mathrm{diff}} \\right),\n$$\nwith\n$$\nA_{\\mathrm{adv}} = -\\frac{c}{\\Delta x}\\left(I - S_{-}\\right), \\quad A_{\\mathrm{diff}} = \\frac{\\nu}{\\Delta x^2}\\left(S_{+} + S_{-} - 2I\\right).\n$$\nHere $I$ is the identity matrix in $\\mathbb{R}^{n \\times n}$, $S_{-}$ is the circulant shift matrix such that $(S_{-} x)_i = x_{i-1}$, and $S_{+}$ is the circulant shift matrix such that $(S_{+} x)_i = x_{i+1}$, with indices taken modulo $n$.\n\nObservations at each time $k \\in \\{1,\\dots,K\\}$ are defined by a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ that selects $m$ grid-point values from the state, and observation vectors $y_k \\in \\mathbb{R}^m$. The background (prior) state is $x_b \\in \\mathbb{R}^n$. Assume diagonal covariance matrices for background and observation errors: $B = \\sigma_b^2 I$ and $R = \\sigma_r^2 I$, so that $B^{-1} = \\sigma_b^{-2} I$ and $R^{-1} = \\sigma_r^{-2} I$.\n\nThe four-dimensional variational (4D-Var) cost function is\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right),\n$$\nwith $x_k = M^k x_0$.\n\nYour task is to perform a gradient check of $J$ with respect to $x_0$ by:\n- Computing the adjoint-based gradient of $J$ with respect to $x_0$ derived from first principles.\n- Drawing random directions $v \\in \\mathbb{R}^n$ with unit Euclidean norm $\\|v\\|_2 = 1$.\n- Computing the finite-difference directional derivative using the central difference formula:\n$$\nD_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}.\n$$\n- Computing the adjoint-based directional derivative:\n$$\nD_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v.\n$$\n- For each direction, evaluating the absolute error $E_{\\mathrm{abs}} = |D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|$ and the relative error\n$$\nE_{\\mathrm{rel}} = \\frac{|D_{\\mathrm{FD}}(v) - D_{\\mathrm{ADJ}}(v)|}{\\max\\left(1, |D_{\\mathrm{FD}}(v)|, |D_{\\mathrm{ADJ}}(v)|\\right)}.\n$$\n- Accepting the gradient check along a direction if either $E_{\\mathrm{rel}} \\le \\text{tol}_{\\mathrm{rel}}$ or $E_{\\mathrm{abs}} \\le \\text{tol}_{\\mathrm{abs}}$.\n\nConstruct $y_k$ and $x_b$ as follows for scientific realism:\n- Generate a \"truth\" initial condition $x_0^{\\mathrm{true}}$ with independent standard normal entries, propagate with the same $M$ to obtain $x_k^{\\mathrm{true}}$, and set $y_k = H x_k^{\\mathrm{true}} + \\sigma_r \\eta_k$ with independent standard normal $\\eta_k$.\n- Set $x_b = x_0^{\\mathrm{true}} + \\sigma_b \\xi$ with independent standard normal $\\xi$.\n- Evaluate the gradient check at $x_0 = x_b$.\n\nUse dimensionless units throughout. Angles are not involved. No percentages are involved.\n\nImplement a program that, for the parameter sets listed below, performs the above gradient check using independent random directions and returns, for each parameter set, a single boolean that is true if all tested directions in that set are accepted and false otherwise. The final output must be a single line containing the results across all parameter sets as a comma-separated list enclosed in square brackets.\n\nTest suite (each line is one parameter set; use an independent random seed per set to ensure reproducibility):\n- Case $1$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $2$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=16$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{-1}$, $\\varepsilon=10^{-10}$, $\\text{tol}_{\\mathrm{rel}}=10^{-3}$, $\\text{tol}_{\\mathrm{abs}}=10^{-8}$, number of directions $=5$.\n- Case $3$: $n=32$, $K=6$, $\\Delta t=10^{-2}$, $c=2\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=8$, $\\sigma_b=5\\times 10^{-1}$, $\\sigma_r=10^{2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $4$: $n=40$, $K=10$, $\\Delta t=5\\times 10^{-3}$, $c=5\\times 10^{-1}$, $\\nu=5\\times 10^{-3}$, $m=40$, $\\sigma_b=2\\times 10^{-1}$, $\\sigma_r=5\\times 10^{-2}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n- Case $5$: $n=24$, $K=5$, $\\Delta t=10^{-2}$, $c=3\\times 10^{-1}$, $\\nu=4\\times 10^{-3}$, $m=12$, $\\sigma_b=10^{-2}$, $\\sigma_r=2\\times 10^{-1}$, $\\varepsilon=10^{-6}$, $\\text{tol}_{\\mathrm{rel}}=10^{-7}$, $\\text{tol}_{\\mathrm{abs}}=10^{-10}$, number of directions $=5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$ where each $r_i$ is a boolean for Case $i$).",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of computational data assimilation that is free of ambiguity, contradiction, and factual error. The task is to implement a standard numerical procedure, a gradient check, for a 4D-Var data assimilation system. We will proceed with a full solution.\n\nOur objective is to derive the analytical gradient of the 4D-Var cost function $J(x_0)$ with respect to the initial state $x_0$. This analytical gradient, computed using the adjoint method, will then be compared against a finite-difference approximation to verify its correctness.\n\nThe cost function is given by:\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n+ \\frac{1}{2} \\sum_{k=1}^K \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)\n$$\nwhere the state $x_k$ at time step $k$ is related to the initial state $x_0$ by the linear forward model $x_k = M^k x_0$. The cost function consists of two terms: a background term $J_b(x_0)$ that measures the distance to a prior estimate $x_b$, and an observation term $J_o(x_0)$ that measures the misfit to observations $y_k$ over a time window.\n\n$$\nJ_b(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b)\n$$\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=1}^K \\left(H M^k x_0 - y_k\\right)^\\top R^{-1} \\left(H M^k x_0 - y_k\\right)\n$$\n\nThe gradient $\\nabla J(x_0)$ is the sum of the gradients of these two terms: $\\nabla J(x_0) = \\nabla J_b(x_0) + \\nabla J_o(x_0)$.\n\n1.  **Gradient of the Background Term**\n    The background term $J_b(x_0)$ is a standard quadratic form. Its gradient with respect to $x_0$ is straightforward to compute:\n    $$\n    \\nabla J_b(x_0) = B^{-1}(x_0 - x_b)\n    $$\n    Given $B = \\sigma_b^2 I$, the inverse is $B^{-1} = \\sigma_b^{-2} I$, so this term simplifies to $\\sigma_b^{-2}(x_0 - x_b)$.\n\n2.  **Gradient of the Observation Term (Adjoint Method)**\n    To find $\\nabla J_o(x_0)$, we use the method of Lagrange multipliers, which gives rise to the adjoint model. We treat the model equations $x_{k+1} = M x_k$ for $k=0, \\dots, K-1$ as constraints. The Lagrangian $\\mathcal{L}$ is:\n    $$\n    \\mathcal{L}(\\{x_k\\}_{k=0}^K, \\{\\lambda_k\\}_{k=1}^K) = J(\\{x_k\\}) + \\sum_{k=0}^{K-1} \\lambda_{k+1}^\\top (M x_k - x_{k+1})\n    $$\n    Here, the cost function is expressed in terms of the full state trajectory $\\{x_k\\}$. The vectors $\\lambda_k$ are the Lagrange multipliers, also known as the adjoint variables. At an optimal point, the gradient of the Lagrangian with respect to all its variables is zero. The gradient of $J(x_0)$ is equivalent to the total derivative of the constrained cost function, $\\frac{dJ}{dx_0}$. We can find this by evaluating $\\frac{\\partial \\mathcal{L}}{\\partial x_0}$ after imposing the conditions $\\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0$ for $k=1, \\dots, K$.\n\n    Taking the partial derivative of $\\mathcal{L}$ with respect to a state $x_k$ for $k \\in \\{1, \\dots, K\\}$ yields:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = \\frac{\\partial J}{\\partial x_k} + M^\\top \\lambda_{k+1} - \\lambda_k = H^\\top R^{-1} (H x_k - y_k) + M^\\top \\lambda_{k+1} - \\lambda_k\n    $$\n    Setting this to zero gives the adjoint equations. For $k=K$, the term $\\lambda_{K+1}$ is not present in the sum, so we define $\\lambda_{K+1}=0$. The derivative with respect to $x_K$ is:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_K} = H^\\top R^{-1} (H x_K - y_K) - \\lambda_K = 0 \\implies \\lambda_K = H^\\top R^{-1} (H x_K - y_K)\n    $$\n    For $k \\in \\{1, \\dots, K-1\\}$, we have:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_k} = 0 \\implies \\lambda_k = M^\\top \\lambda_{k+1} + H^\\top R^{-1} (H x_k - y_k)\n    $$\n    These equations define a backward recurrence for the adjoint variables, starting from $\\lambda_K$ and propagating backward in time to $\\lambda_1$. Note that since $R = \\sigma_r^2 I$, the term $H^\\top R^{-1} (H x_k - y_k)$ simplifies to $\\sigma_r^{-2} H^\\top (H x_k - y_k)$.\n\n    Finally, the gradient of the cost function with respect to the initial state $x_0$ is obtained by differentiating $\\mathcal{L}$ with respect to $x_0$:\n    $$\n    \\nabla J(x_0) = \\frac{d\\mathcal{L}}{dx_0} = \\frac{\\partial\\mathcal{L}}{\\partial x_0} = \\frac{\\partial J_b}{\\partial x_0} + M^\\top \\lambda_1 = B^{-1}(x_0 - x_b) + M^\\top \\lambda_1\n    $$\n\n3.  **Algorithmic Summary for Gradient Calculation**\n    The procedure to calculate $\\nabla J(x_0)$ is as follows:\n    a.  **Forward Run**: Starting with $x_0$, integrate the forward model $x_{k+1} = M x_k$ for $k = 0, \\dots, K-1$ and store the entire state trajectory $\\{x_k\\}_{k=1}^K$.\n    b.  **Backward (Adjoint) Run**:\n        i.  Initialize the adjoint variable at the final time $K$: $\\lambda_K = \\sigma_r^{-2} H^\\top (H x_K - y_K)$.\n        ii. Integrate the adjoint model backward in time from $k=K-1$ down to $1$: $\\lambda_k = M^\\top \\lambda_{k+1} + \\sigma_r^{-2} H^\\top (H x_k - y_k)$.\n    c.  **Gradient Assembly**: Compute the final gradient using the result from the backward run: $\\nabla J(x_0) = \\sigma_b^{-2}(x_0 - x_b) + M^\\top \\lambda_1$.\n\n4.  **Gradient Check Procedure**\n    The gradient check validates the analytical gradient by comparing its projection along a random direction $v$ with a finite-difference approximation of the directional derivative.\n    -   The adjoint-based directional derivative is $D_{\\mathrm{ADJ}}(v) = \\nabla J(x_0)^\\top v$.\n    -   The finite-difference approximation using a central difference scheme is $D_{\\mathrm{FD}}(v) = \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}$.\n    The implementation will compute both quantities for several random unit vectors $v$, calculate the absolute error $E_{\\mathrm{abs}}$ and relative error $E_{\\mathrm{rel}}$, and accept the gradient if the errors are within the specified tolerances $\\text{tol}_{\\mathrm{abs}}$ and $\\text{tol}_{\\mathrm{rel}}$. The overall check for a parameter set passes only if all tested random directions are accepted.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient check for all test cases and print the results.\n    \"\"\"\n\n    def run_gradient_check(n, K, dt, c, nu, m, sigma_b, sigma_r, epsilon, tol_rel, tol_abs, num_directions, seed):\n        \"\"\"\n        Performs a gradient check for a 4D-Var cost function for a single parameter set.\n        \"\"\"\n        # 1. Setup\n        np.random.seed(seed)\n        dx = 1.0 / n\n\n        # 2. Build matrices M, and define H operation\n        I = np.eye(n)\n        # (S- @ x)_i = x_{i-1}. Matrix equivalent is np.roll(I, 1, axis=0)\n        S_minus = np.roll(I, 1, axis=0)\n        # (S+ @ x)_i = x_{i+1}. Matrix equivalent is np.roll(I, -1, axis=0)\n        S_plus = np.roll(I, -1, axis=0)\n        \n        A_adv = -(c / dx) * (I - S_minus)\n        A_diff = (nu / dx**2) * (S_plus + S_minus - 2 * I)\n        M = I + dt * (A_adv + A_diff)\n        M_T = M.T\n        \n        obs_indices = np.linspace(0, n - 1, num=m, dtype=int)\n\n        # 3. Generate data (truth, obs, background)\n        x0_true = np.random.randn(n)\n        \n        y_obs_list = []\n        x_k_true = np.copy(x0_true)\n        for _ in range(K):\n            x_k_true = M @ x_k_true\n            obs_noise = np.random.randn(m)\n            y_k = x_k_true[obs_indices] + sigma_r * obs_noise\n            y_obs_list.append(y_k)\n            \n        bg_noise = np.random.randn(n)\n        x_b = x0_true + sigma_b * bg_noise\n        \n        # 4. Define cost and gradient functions\n        inv_sigma_b2 = 1.0 / sigma_b**2\n        inv_sigma_r2 = 1.0 / sigma_r**2\n\n        def calculate_J(x0):\n            j_b = 0.5 * inv_sigma_b2 * np.sum((x0 - x_b)**2)\n            \n            j_o = 0.0\n            x_k = np.copy(x0)\n            for k in range(K):\n                x_k = M @ x_k\n                innov = x_k[obs_indices] - y_obs_list[k]\n                j_o += 0.5 * inv_sigma_r2 * np.sum(innov**2)\n                \n            return j_b + j_o\n\n        def calculate_grad_J(x0):\n            # Forward run: store trajectory\n            x_traj = [x0]\n            x_k = np.copy(x0)\n            for _ in range(K):\n                x_k = M @ x_k\n                x_traj.append(x_k)\n\n            # Backward run: compute adjoint variable\n            # Initialize lambda_K\n            x_K = x_traj[K]\n            y_K = y_obs_list[K-1] # y_obs_list is 0-indexed for k=1..K\n            innov_K = x_K[obs_indices] - y_K\n            forcing_term_K = np.zeros(n)\n            forcing_term_K[obs_indices] = inv_sigma_r2 * innov_K\n            lambda_next = forcing_term_K # This is lambda_K\n\n            # Loop for k from K-1 down to 1\n            for k in range(K - 1, 0, -1):\n                x_k = x_traj[k]\n                y_k = y_obs_list[k-1]\n                \n                innov_k = x_k[obs_indices] - y_k\n                forcing_term_k = np.zeros(n)\n                forcing_term_k[obs_indices] = inv_sigma_r2 * innov_k\n                \n                lambda_current = M_T @ lambda_next + forcing_term_k\n                lambda_next = lambda_current\n\n            # After loop, lambda_next is lambda_1\n            lambda_1 = lambda_next\n            \n            grad_b = inv_sigma_b2 * (x0 - x_b)\n            grad_o = M_T @ lambda_1\n            \n            return grad_b + grad_o\n\n        # 5. Perform the gradient check at x0 = x_b\n        x0_eval = x_b\n        grad_adj = calculate_grad_J(x0_eval)\n        \n        for i in range(num_directions):\n            v = np.random.randn(n)\n            v /= np.linalg.norm(v)\n\n            J_plus = calculate_J(x0_eval + epsilon * v)\n            J_minus = calculate_J(x0_eval - epsilon * v)\n\n            D_fd = (J_plus - J_minus) / (2.0 * epsilon)\n            D_adj = grad_adj.T @ v\n\n            E_abs = np.abs(D_fd - D_adj)\n            denom = max(1.0, np.abs(D_fd), np.abs(D_adj))\n            E_rel = E_abs / denom\n\n            if not (E_rel <= tol_rel or E_abs <= tol_abs):\n                return False # Failed test for this direction\n        \n        return True # All directions passed\n\n    test_cases = [\n        # n, K, dt, c, nu, m, sigma_b, sigma_r, epsilon, tol_rel, tol_abs, num_directions\n        (32, 6, 1e-2, 2e-1, 5e-3, 16, 5e-1, 1e-1, 1e-6, 1e-7, 1e-10, 5),\n        (32, 6, 1e-2, 2e-1, 5e-3, 16, 5e-1, 1e-1, 1e-10, 1e-3, 1e-8, 5),\n        (32, 6, 1e-2, 2e-1, 5e-3, 8, 5e-1, 1e2, 1e-6, 1e-7, 1e-10, 5),\n        (40, 10, 5e-3, 5e-1, 5e-3, 40, 2e-1, 5e-2, 1e-6, 1e-7, 1e-10, 5),\n        (24, 5, 1e-2, 3e-1, 4e-3, 12, 1e-2, 2e-1, 1e-6, 1e-7, 1e-10, 5)\n    ]\n\n    results = []\n    for i, case_params in enumerate(test_cases):\n        # Use an independent random seed for each test case for reproducibility\n        result = run_gradient_check(*case_params, seed=i)\n        results.append(result)\n\n    # Format output as a lowercase boolean list: [true,false,...]\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a verified gradient at our disposal, the next challenge is to efficiently find the minimum of the high-dimensional 4D-Var cost function. While simple gradient descent methods exist, more robust algorithms are often required for the ill-conditioned problems typical in oceanography. This exercise explores a trust-region method, an advanced optimization strategy that builds a local quadratic model of the cost function and solves for an optimal step within a \"trusted\" radius (). This practice provides insight into the numerical machinery that underpins operational data assimilation systems and develops skills in implementing and diagnosing sophisticated optimizers.",
            "id": "3793624",
            "problem": "Consider the linear four-dimensional variational data assimilation cost function defined on a state vector $\\mathbf{x} \\in \\mathbb{R}^n$ by\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\tfrac{1}{2}\\sum_{t=1}^{T}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right),\n$$\nwhere $\\mathbf{x}_b \\in \\mathbb{R}^n$ is the background state, $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ is the background error covariance matrix, $\\mathbf{M}_t \\in \\mathbb{R}^{n \\times n}$ is the linear model propagator at time index $t$, $\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$ is the linear observation operator, $\\mathbf{y}_t \\in \\mathbb{R}^m$ is the observation vector at time index $t$, and $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ is the observation error covariance matrix. For given $\\mathbf{x}_k$, define the quadratic model\n$$\nm(\\mathbf{p}) = J(\\mathbf{x}_k) + \\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p},\n$$\nwhere $\\mathbf{g} = \\nabla J(\\mathbf{x}_k)$ and $\\mathbf{H}_{\\text{approx}}$ is a positive definite approximation to the Hessian of $J$ at $\\mathbf{x}_k$. The trust-region subproblem seeks\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^n} \\quad \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p} + \\mathbf{g}^\\top \\mathbf{p} \\quad \\text{subject to} \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta,\n$$\nfor a given trust-region radius $\\Delta > 0$. The predicted reduction is defined as\n$$\n\\text{pred} = m(\\mathbf{0}) - m(\\mathbf{p}) = -\\left(\\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p}\\right),\n$$\nand the actual reduction is\n$$\n\\text{ared} = J(\\mathbf{x}_k) - J(\\mathbf{x}_k + \\mathbf{p}).\n$$\nThe ratio $\\rho$ monitoring convergence is \n$$\n\\rho = \\frac{\\text{ared}}{\\text{pred}},\n$$\nwith the convention that if $\\text{pred}$ is numerically zero (specified below), the reported $\\rho$ must be $0.0$.\n\nStarting from the definitions above and the linearity of $\\mathbf{M}_t$ and $\\mathbf{H}$, it follows that the gradient and the exact Hessian for $J$ at any $\\mathbf{x}$ are\n$$\n\\mathbf{g}(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right),\n\\quad\n\\mathbf{H}_{\\text{true}} = \\mathbf{B}^{-1} + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\mathbf{H}\\mathbf{M}_t.\n$$\nIn this task, set the Hessian approximation to \n$$\n\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I},\n$$\nfor a given scalar $\\alpha \\ge 0$, where $\\mathbf{I}$ is the identity matrix, to emulate Gauss-Newton damping or regularization. The trust-region step $\\mathbf{p}$ must be computed by solving the first-order optimality conditions\n$$\n(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}, \\quad \\lambda \\ge 0, \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta, \\quad \\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0,\n$$\nusing a robust algorithm based on scalar root-finding for $\\lambda$ and linear solves, ensuring numerical stability for positive definite $\\mathbf{H}_{\\text{approx}}$. Then evaluate $\\text{pred}$ and $\\text{ared}$ and compute $\\rho$. If $\\text{pred} < \\epsilon$ for tolerance $\\epsilon = 10^{-12}$, report $\\rho = 0.0$.\n\nImplement a program that carries out this procedure for the following test suite. All matrices and vectors are specified in $\\mathbb{R}$ and all numbers are to be treated as dimensionless. Use the Euclidean norm for $\\|\\cdot\\|_2$.\n\nBase definitions common to all test cases:\n- Dimension $n = 3$, observation dimension $m = 2$, and number of time steps $T = 2$.\n- Background state $\\mathbf{x}_b = [\\,0.5,\\,-0.2,\\,0.1\\,]^\\top$.\n- Background covariance $\\mathbf{B} = \\operatorname{diag}([\\,0.4^2,\\,0.3^2,\\,0.5^2\\,])$ so that $\\mathbf{B}^{-1} = \\operatorname{diag}([\\,6.25,\\,11.\\overline{1},\\,4.0\\,])$.\n- Observation covariance $\\mathbf{R} = \\operatorname{diag}([\\,0.2^2,\\,0.3^2\\,])$ so that $\\mathbf{R}^{-1} = \\operatorname{diag}([\\,25.0,\\,11.\\overline{1}\\,])$.\n- Observation operator\n$$\n\\mathbf{H} = \\begin{bmatrix}\n1.0 & 0.0 & 0.0 \\\\\n0.0 & 1.0 & 0.3\n\\end{bmatrix}.\n$$\n- Linear model propagators\n$$\n\\mathbf{M}_1 = \\begin{bmatrix}\n1.0 & 0.1 & 0.0 \\\\\n0.0 & 0.9 & 0.2 \\\\\n0.0 & 0.0 & 1.0\n\\end{bmatrix},\n\\quad\n\\mathbf{M}_2 = \\begin{bmatrix}\n0.9 & 0.0 & 0.1 \\\\\n0.1 & 1.0 & 0.0 \\\\\n0.0 & 0.2 & 0.95\n\\end{bmatrix}.\n$$\n- Nominal observations for general cases:\n$$\n\\mathbf{y}_1 = \\begin{bmatrix} 0.7 \\\\ -0.15 \\end{bmatrix}, \\quad\n\\mathbf{y}_2 = \\begin{bmatrix} 0.6 \\\\ -0.05 \\end{bmatrix}.\n$$\n\nDefine the four test cases as follows:\n- Case $1$ (happy path): $\\mathbf{x}_k = [\\,0.6,\\,-0.25,\\,0.0\\,]^\\top$, trust-region radius $\\Delta = 1.5$, damping $\\alpha = 0.0$, observations $\\mathbf{y}_1$ and $\\mathbf{y}_2$ as specified above.\n- Case $2$ (gradient nearly zero): $\\mathbf{x}_k = \\mathbf{x}_b$, trust-region radius $\\Delta = 1.0$, damping $\\alpha = 0.0$, and observations chosen consistent with the background, i.e., $\\mathbf{y}_t = \\mathbf{H}\\mathbf{M}_t\\mathbf{x}_b$ for $t \\in \\{1,2\\}$.\n- Case $3$ (tight trust region with damping): $\\mathbf{x}_k = [\\,0.0,\\,0.0,\\,0.0\\,]^\\top$, trust-region radius $\\Delta = 0.1$, damping $\\alpha = 0.5$, observations $\\mathbf{y}_1$ and $\\mathbf{y}_2$ as specified above.\n- Case $4$ (strong damping): $\\mathbf{x}_k = [\\,-0.3,\\,0.4,\\,-0.1\\,]^\\top$, trust-region radius $\\Delta = 0.8$, damping $\\alpha = 2.0$, observations $\\mathbf{y}_1$ and $\\mathbf{y}_2$ as specified above.\n\nProgram requirements:\n- Compute $\\mathbf{g}$ and $\\mathbf{H}_{\\text{true}}$ using the definitions above.\n- Form $\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I}$.\n- Compute the trust-region step $\\mathbf{p}$ by solving $(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}$ with $\\lambda \\ge 0$ such that $\\|\\mathbf{p}\\|_2 \\le \\Delta$ and $\\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0$ via a robust scalar search for $\\lambda$.\n- Evaluate $\\text{pred}$ and $\\text{ared}$ and compute $\\rho = \\text{ared}/\\text{pred}$, with the convention that if $\\text{pred} < \\epsilon$ for $\\epsilon = 10^{-12}$, report $\\rho = 0.0$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[\\,\\rho_1,\\rho_2,\\rho_3,\\rho_4\\,]$, where $\\rho_i$ corresponds to Case $i$.\n\nAll computations are purely mathematical and dimensionless; no physical units are required. Angles are not used. Percentages must not be used; any ratios must be reported as decimal floating-point numbers as specified.",
            "solution": "The user has provided a well-defined computational problem in the domain of four-dimensional variational data assimilation (4D-Var), a subfield of computational oceanography and numerical weather prediction. The problem is scientifically sound, mathematically consistent, and all necessary data and definitions are provided. It is a valid problem.\n\nThe task is to compute the ratio $\\rho = \\text{ared} / \\text{pred}$ for four different test cases. This ratio is a standard metric in trust-region optimization methods, used to assess the quality of a quadratic model of an objective function. The procedure involves several steps: defining the cost function and its derivatives, solving a trust-region subproblem to find an optimal step, and then evaluating the actual and predicted reduction in the cost function.\n\n### Step 1: Definition of the Cost Function and its Derivatives\n\nThe 4D-Var cost function $J(\\mathbf{x})$ is a quadratic function of the state vector $\\mathbf{x} \\in \\mathbb{R}^n$:\n$$\nJ(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\tfrac{1}{2}\\sum_{t=1}^{T}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)\n$$\nAll matrices and vectors are defined in the problem statement. Given that $J(\\mathbf{x})$ is quadratic, its gradient $\\mathbf{g}(\\mathbf{x}) = \\nabla J(\\mathbf{x})$ is a linear function of $\\mathbf{x}$, and its Hessian $\\mathbf{H}_{\\text{true}} = \\nabla^2 J(\\mathbf{x})$ is a constant matrix. The expressions provided are correct:\n\nThe gradient at a point $\\mathbf{x}$ is:\n$$\n\\mathbf{g}(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\left(\\mathbf{H}\\mathbf{M}_t\\mathbf{x} - \\mathbf{y}_t\\right)\n$$\n\nThe exact Hessian is constant for all $\\mathbf{x}$:\n$$\n\\mathbf{H}_{\\text{true}} = \\mathbf{B}^{-1} + \\sum_{t=1}^{T}\\mathbf{M}_t^\\top \\mathbf{H}^\\top \\mathbf{R}^{-1}\\mathbf{H}\\mathbf{M}_t\n$$\n\nFor each test case, we first compute the gradient $\\mathbf{g} = \\mathbf{g}(\\mathbf{x}_k)$ at the given iterate $\\mathbf{x}_k$. The Hessian approximation is then formed as $\\mathbf{H}_{\\text{approx}} = \\mathbf{H}_{\\text{true}} + \\alpha \\mathbf{I}$, where $\\mathbf{H}_{\\text{true}}$ can be pre-computed as it is independent of $\\mathbf{x}_k$ and the observations $\\mathbf{y}_t$.\n\n### Step 2: Solving the Trust-Region Subproblem\n\nThe core of the task is to solve the trust-region subproblem for the step $\\mathbf{p}$:\n$$\n\\min_{\\mathbf{p} \\in \\mathbb{R}^n} \\quad \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p} + \\mathbf{g}^\\top \\mathbf{p} \\quad \\text{subject to} \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta\n$$\nThe solution $\\mathbf{p}$ must satisfy the Karush-Kuhn-Tucker (KKT) conditions:\n$$\n(\\mathbf{H}_{\\text{approx}} + \\lambda \\mathbf{I})\\mathbf{p} = -\\mathbf{g}, \\quad \\lambda \\ge 0, \\quad \\|\\mathbf{p}\\|_2 \\le \\Delta, \\quad \\lambda(\\|\\mathbf{p}\\|_2 - \\Delta) = 0\n$$\nSince $\\mathbf{B}$ and $\\mathbf{R}$ are covariance matrices, they are symmetric positive definite, and so are their inverses. This guarantees that $\\mathbf{H}_{\\text{true}}$ is positive definite. With $\\alpha \\ge 0$, $\\mathbf{H}_{\\text{approx}}$ is also positive definite. This simplifies the solution procedure, as we do not need to consider the so-called \"hard case\" of the trust-region subproblem.\n\nThe solution algorithm is as follows:\n1.  **Check for trivial solution**: If the gradient norm $\\|\\mathbf{g}\\|_2$ is close to zero, the current point $\\mathbf{x}_k$ is already optimal. The step is $\\mathbf{p} = \\mathbf{0}$. This applies to Case 2.\n2.  **Interior Solution**: First, attempt to find the unconstrained minimizer by setting $\\lambda=0$. This gives the Newton step $\\mathbf{p}_N = -\\mathbf{H}_{\\text{approx}}^{-1}\\mathbf{g}$. If $\\|\\mathbf{p}_N\\|_2 \\le \\Delta$, then $\\mathbf{p}=\\mathbf{p}_N$ is the solution, and the trust-region constraint is inactive.\n3.  **Boundary Solution**: If $\\|\\mathbf{p}_N\\|_2 > \\Delta$, the solution must lie on the boundary of the trust region, i.e., $\\|\\mathbf{p}\\|_2 = \\Delta$. This requires finding a $\\lambda > 0$ that satisfies the secular equation $\\|\\mathbf{p}(\\lambda)\\|_2 = \\Delta$, where $\\mathbf{p}(\\lambda) = -(\\mathbf{H}_{\\text{approx}} + \\lambda\\mathbf{I})^{-1}\\mathbf{g}$. The function $\\phi(\\lambda) = \\|\\mathbf{p}(\\lambda)\\|_2 - \\Delta$ is a monotonically decreasing function for $\\lambda \\ge 0$. We can find the unique root $\\lambda^* > 0$ of $\\phi(\\lambda) = 0$ using a numerical root-finding algorithm, such as the Brent-Dekker method (`brentq` in SciPy). The search for $\\lambda^*$ is performed on an interval $[a, b]$ where $\\phi(a)>0$ and $\\phi(b)<0$. We know $\\phi(0)>0$, and a sufficiently large value can be found for $b$.\n\n### Step 3: Calculation of $\\rho$\n\nOnce the step $\\mathbf{p}$ is determined, the predicted and actual reductions are calculated.\n\nThe predicted reduction is based on the quadratic model of $J$:\n$$\n\\text{pred} = -\\left(\\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{H}_{\\text{approx}} \\mathbf{p}\\right)\n$$\n\nThe actual reduction is based on the true cost function $J$:\n$$\n\\text{ared} = J(\\mathbf{x}_k) - J(\\mathbf{x}_k + \\mathbf{p})\n$$\n\nFinally, the ratio $\\rho$ is computed:\n$$\n\\rho = \\frac{\\text{ared}}{\\text{pred}}\n$$\nAs per the problem specification, if $|\\text{pred}| < 10^{-12}$, we set $\\rho = 0.0$ to avoid division by a near-zero number. This is relevant for Case 2, where $\\mathbf{g}=\\mathbf{0}$ leads to $\\mathbf{p}=\\mathbf{0}$ and $\\text{pred}=0$.\n\nThis entire procedure is implemented for each of the four test cases, yielding four values for $\\rho$. The implementation uses `numpy` for linear algebra operations and `scipy.optimize.brentq` for the scalar root-finding. All calculations are performed using double-precision floating-point arithmetic.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the 4D-Var trust-region problem for the four specified test cases.\n    \"\"\"\n\n    # 1. Define constants and common matrices\n    n, m, T = 3, 2, 2\n    xb = np.array([0.5, -0.2, 0.1])\n    Binv = np.diag([6.25, 100.0 / 9.0, 4.0])\n    Rinv = np.diag([25.0, 100.0 / 9.0])\n    I = np.eye(n)\n\n    H_op = np.array([[1.0, 0.0, 0.0],\n                     [0.0, 1.0, 0.3]])\n\n    M1 = np.array([[1.0, 0.1, 0.0],\n                   [0.0, 0.9, 0.2],\n                   [0.0, 0.0, 1.0]])\n\n    M2 = np.array([[0.9, 0.0, 0.1],\n                   [0.1, 1.0, 0.0],\n                   [0.0, 0.2, 0.95]])\n\n    # 2. Pre-compute constant matrices\n    C1 = H_op @ M1\n    C2 = H_op @ M2\n    C1T = C1.T\n    C2T = C2.T\n\n    # H_true = B_inv + C1^T R_inv C1 + C2^T R_inv C2\n    H_true = Binv + C1T @ Rinv @ C1 + C2T @ Rinv @ C2\n\n    # 3. Define helper functions that capture the constants from the outer scope\n    def J_cost(x, y1, y2):\n        term_b = 0.5 * (x - xb).T @ Binv @ (x - xb)\n        term_o1 = 0.5 * (C1 @ x - y1).T @ Rinv @ (C1 @ x - y1)\n        term_o2 = 0.5 * (C2 @ x - y2).T @ Rinv @ (C2 @ x - y2)\n        return term_b + term_o1 + term_o2\n\n    def gradient(x, y1, y2):\n        term_b = Binv @ (x - xb)\n        term_o1 = C1T @ Rinv @ (C1 @ x - y1)\n        term_o2 = C2T @ Rinv @ (C2 @ x - y2)\n        return term_b + term_o1 + term_o2\n\n    def solve_trust_region_subproblem(H_approx, g, delta):\n        g_norm = np.linalg.norm(g)\n        if g_norm < 1e-15:\n            return np.zeros_like(g)\n\n        # Try interior solution (lambda = 0)\n        try:\n            p0 = np.linalg.solve(H_approx, -g)\n            if np.linalg.norm(p0) <= delta:\n                return p0\n        except np.linalg.LinAlgError:\n            # H_approx is guaranteed to be PD in this problem, but this is good practice.\n            pass\n\n        # Boundary solution: find lambda > 0 such that ||p(lambda)||_2 = delta\n        def phi(lmbda):\n            mat = H_approx + lmbda * I\n            try:\n                p_lambda = np.linalg.solve(mat, -g)\n                return np.linalg.norm(p_lambda) - delta\n            except np.linalg.LinAlgError:\n                # Should not happen for lambda >= 0 as H_approx is PD\n                return 1e9\n\n        lambda_lower = 0.0\n        # Establish an upper bound for the root finding search\n        lambda_upper = g_norm / delta if delta > 0 else g_norm\n        if lambda_upper == 0: lambda_upper = 1.0 # Avoid zero upper bound\n        \n        while phi(lambda_upper) > 0:\n            lambda_upper *= 2\n\n        lambda_sol = brentq(phi, lambda_lower, lambda_upper)\n        p = np.linalg.solve(H_approx + lambda_sol * I, -g)\n        return p\n\n    # 4. Define and process test cases\n    y1_nom = np.array([0.7, -0.15])\n    y2_nom = np.array([0.6, -0.05])\n    \n    y1_case2 = C1 @ xb\n    y2_case2 = C2 @ xb\n\n    test_cases = [\n        {'xk': np.array([0.6, -0.25, 0.0]), 'delta': 1.5, 'alpha': 0.0, 'y1': y1_nom, 'y2': y2_nom},\n        {'xk': xb, 'delta': 1.0, 'alpha': 0.0, 'y1': y1_case2, 'y2': y2_case2},\n        {'xk': np.array([0.0, 0.0, 0.0]), 'delta': 0.1, 'alpha': 0.5, 'y1': y1_nom, 'y2': y2_nom},\n        {'xk': np.array([-0.3, 0.4, -0.1]), 'delta': 0.8, 'alpha': 2.0, 'y1': y1_nom, 'y2': y2_nom},\n    ]\n\n    results = []\n    pred_tolerance = 1e-12\n\n    for case in test_cases:\n        xk, delta, alpha = case['xk'], case['delta'], case['alpha']\n        y1, y2 = case['y1'], case['y2']\n\n        H_approx = H_true + alpha * I\n        g = gradient(xk, y1, y2)\n        \n        p = solve_trust_region_subproblem(H_approx, g, delta)\n        \n        pred = -(g.T @ p + 0.5 * p.T @ H_approx @ p)\n        \n        if pred < pred_tolerance:\n            rho = 0.0\n        else:\n            ared = J_cost(xk, y1, y2) - J_cost(xk + p, y1, y2)\n            rho = ared / pred\n            \n        results.append(rho)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After the minimization process converges to an analysis, a crucial final step is to evaluate its quality. We must assess whether the resulting analysis and the observations are statistically compatible with our prior assumptions about their errors. This practice demonstrates the use of chi-square ($\\chi^2$) diagnostics, which quantify the misfit between the analysis, background, and observations, normalized by their assumed error covariances (). By computing these statistics, you learn a powerful method to diagnose the performance of the assimilation system and detect potential mis-specifications in the crucial background ($B$) and observation ($R$) error covariance matrices.",
            "id": "3793697",
            "problem": "Consider a four-dimensional variational data assimilation (4D-Var) window in a simplified, linearized computational oceanography setting over three equally spaced times $t_{0}$, $t_{1}$, and $t_{2}$. At each time, two scalar oceanic quantities are observed: sea surface height anomaly and in situ temperature. Assume the following:\n\n- The observation operator at each time is the identity, $H_{t} = I$, mapping the model state directly to observed quantities.\n- Observation errors at each time are zero-mean Gaussian and independent across components and times, with observation error covariance $R_{t} = \\mathrm{diag}(0.25, 1)$ for all $t \\in \\{t_{0}, t_{1}, t_{2}\\}$.\n- The observed innovations (observations minus the model equivalents) at the three times are\n$$\nd_{t_{0}} = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}, \\quad\nd_{t_{1}} = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix}, \\quad\nd_{t_{2}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\n- The background (prior) state at the initial time has a zero-mean Gaussian error with covariance $B = \\mathrm{diag}(1, 4, 0.25)$ in a three-dimensional control space for the initial condition. The corresponding analysis increment at the initial time is\n$$\n\\delta x_{0} = x_{0} - x_{b} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{pmatrix}.\n$$\n- Observation errors are independent between times and of the background errors.\n\nStarting from the Gaussian error assumptions and fundamental definitions of the 4D-Var cost function components, construct the innovation-based and background-based chi-square statistics over the window and identify their respective degrees of freedom. Then define the combined normalized chi-square diagnostic as\n$$\nc_{\\mathrm{tot}} = \\frac{S_{o} + S_{b}}{m + n},\n$$\nwhere $S_{o}$ is the total innovation-based chi-square statistic across the window, $S_{b}$ is the background-based chi-square statistic at the initial time, $m$ is the total number of independent observed components across the window, and $n$ is the dimension of the background control vector. Compute $c_{\\mathrm{tot}}$ for the given data. Round your answer to four significant figures and express it as a pure number with no units.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded within the framework of four-dimensional variational data assimilation (4D-Var), well-posed, objective, and internally consistent. All necessary data and definitions are provided to compute the required diagnostic quantity.\n\nThe objective is to compute the combined normalized chi-square diagnostic, $c_{\\mathrm{tot}}$, defined as:\n$$\nc_{\\mathrm{tot}} = \\frac{S_{o} + S_{b}}{m + n}\n$$\nwhere $S_{o}$ is the total innovation-based chi-square statistic, $S_{b}$ is the background-based chi-square statistic, $m$ is the total number of observations, and $n$ is the dimension of the control vector. We will compute each of these four components.\n\nFirst, we identify the degrees of freedom, $n$ and $m$.\nThe dimension of the background control vector, $n$, corresponds to the size of the initial condition's control space. The background error covariance matrix $B$ is given as a $3 \\times 3$ diagonal matrix, and the analysis increment $\\delta x_0$ is a $3$-dimensional vector. Thus, the number of degrees of freedom for the background is $n=3$.\n\nThe total number of observed components, $m$, is the number of scalar observations at each time step multiplied by the number of time steps. At each of the three time steps ($t_{0}$, $t_{1}$, $t_{2}$), two scalar quantities are observed. Therefore, the total number of observations, which represents the degrees of freedom for the innovation-based statistic, is $m = 2 \\times 3 = 6$.\n\nNext, we construct and compute the background-based chi-square statistic, $S_{b}$. This statistic is associated with the background (or prior) state's contribution to the 4D-Var cost function, evaluated at the analysis solution. It is defined as:\n$$\nS_{b} = (x_{0} - x_{b})^{T} B^{-1} (x_{0} - x_{b}) = \\delta x_{0}^{T} B^{-1} \\delta x_{0}\n$$\nWe are given the analysis increment at the initial time:\n$$\n\\delta x_{0} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{pmatrix}\n$$\nAnd the background error covariance matrix:\n$$\nB = \\mathrm{diag}(1, 4, 0.25)\n$$\nThe inverse of this diagonal matrix is:\n$$\nB^{-1} = \\mathrm{diag}(1^{-1}, 4^{-1}, 0.25^{-1}) = \\mathrm{diag}(1, 0.25, 4) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0.25 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix}\n$$\nNow we compute $S_{b}$:\n$$\nS_{b} = \\begin{pmatrix} 1 & -2 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0.25 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{pmatrix}\n$$\n$$\nS_{b} = 1^{2} \\times 1 + (-2)^{2} \\times 0.25 + (0.5)^{2} \\times 4\n$$\n$$\nS_{b} = 1 \\times 1 + 4 \\times 0.25 + 0.25 \\times 4 = 1 + 1 + 1 = 3\n$$\n\nNext, we construct and compute the total innovation-based chi-square statistic, $S_{o}$. This statistic quantifies the misfit between the observations and the background model forecast, weighted by the observation error covariance. It is calculated by summing the chi-square values from each observation time:\n$$\nS_{o} = \\sum_{t \\in \\{t_0, t_1, t_2\\}} S_{o,t} = \\sum_{t \\in \\{t_0, t_1, t_2\\}} d_{t}^{T} R_{t}^{-1} d_{t}\n$$\nThe observation error covariance matrix $R_{t}$ is the same for all time steps:\n$$\nR_{t} = \\mathrm{diag}(0.25, 1)\n$$\nIts inverse is:\n$$\nR_{t}^{-1} = \\mathrm{diag}(0.25^{-1}, 1^{-1}) = \\mathrm{diag}(4, 1) = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nWe compute the contribution from each time step using the given innovation vectors $d_t$:\n\nFor $t=t_{0}$: $d_{t_{0}} = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}$\n$$\nS_{o,t_{0}} = d_{t_{0}}^{T} R_{t_{0}}^{-1} d_{t_{0}} = (0.5)^{2} \\times 4 + 1^{2} \\times 1 = 0.25 \\times 4 + 1 = 1 + 1 = 2\n$$\n\nFor $t=t_{1}$: $d_{t_{1}} = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix}$\n$$\nS_{o,t_{1}} = d_{t_{1}}^{T} R_{t_{1}}^{-1} d_{t_{1}} = (-0.5)^{2} \\times 4 + (-2)^{2} \\times 1 = 0.25 \\times 4 + 4 = 1 + 4 = 5\n$$\n\nFor $t=t_{2}$: $d_{t_{2}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n$$\nS_{o,t_{2}} = d_{t_{2}}^{T} R_{t_{2}}^{-1} d_{t_{2}} = 0^{2} \\times 4 + 1^{2} \\times 1 = 0 + 1 = 1\n$$\nThe total innovation-based chi-square statistic is the sum of these values:\n$$\nS_{o} = S_{o,t_{0}} + S_{o,t_{1}} + S_{o,t_{2}} = 2 + 5 + 1 = 8\n$$\n\nFinally, we compute the combined normalized chi-square diagnostic $c_{\\mathrm{tot}}$:\n$$\nc_{\\mathrm{tot}} = \\frac{S_{o} + S_{b}}{m + n} = \\frac{8 + 3}{6 + 3} = \\frac{11}{9}\n$$\nEvaluating this fraction numerically:\n$$\nc_{\\mathrm{tot}} = 1.2222...\n$$\nRounding to four significant figures, we get $1.222$.",
            "answer": "$$\n\\boxed{1.222}\n$$"
        }
    ]
}