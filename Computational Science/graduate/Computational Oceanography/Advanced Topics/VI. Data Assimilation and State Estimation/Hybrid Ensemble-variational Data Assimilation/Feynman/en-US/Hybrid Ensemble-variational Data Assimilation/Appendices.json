{
    "hands_on_practices": [
        {
            "introduction": "To truly master hybrid data assimilation, we must begin with its statistical bedrock: Bayesian inference. This first exercise strips the methodology down to its essential core, removing the complexity of high-dimensional models to focus on the fundamental logic. By working through a simple one-dimensional example, you will see precisely how prior knowledge (the background forecast) and new evidence (an observation) are mathematically combined to produce an updated, more accurate posterior state (the analysis) .",
            "id": "3795131",
            "problem": "Consider a $1$-dimensional ocean surface temperature analysis at a single grid point using Hybrid Ensemble-Variational (EnVar) Data Assimilation, where the hybrid background covariance is represented by an effective scalar variance. Assume a linear observation model and Gaussian errors, consistent with the linear-Gaussian limit in which the hybrid method reduces to a Bayesian update with an effective background covariance. Specifically, let the true state $x$ (temperature in degrees Celsius) have a Gaussian prior with mean $x_b$ and variance $B$, and let the observation operator be $H$ so that the observation $y$ satisfies $y = H x + \\varepsilon$, where the observation error $\\varepsilon$ is Gaussian with zero mean and variance $R$. The prior and likelihood are thus consistent with the foundational assumptions of linear Gaussian Bayesian estimation.\n\nFor a single direct thermometer observation with $H = 1$, use the following physically plausible parameters: $x_b = 10$ (degrees Celsius), $B = 4$ (degrees Celsius squared), $y = 12$ (degrees Celsius), and $R = 1$ (degrees Celsius squared). Starting from the definitions of Gaussian prior and likelihood and the application of Bayes' theorem, derive the posterior distribution for $x$ given $y$ in the linear-Gaussian setting, and then compute the posterior mean $x_a$ (analysis temperature) and posterior variance $P_a$ (analysis uncertainty).\n\nExpress the posterior mean $x_a$ in degrees Celsius and the posterior variance $P_a$ in degrees Celsius squared. Your final numerical values should be exact (no rounding). Report your final answer as the ordered pair $\\left(x_a, P_a\\right)$.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, and objective. The problem provides a self-contained, consistent set of parameters and asks for a standard derivation in Bayesian statistics, which is a cornerstone of data assimilation.\n\nThe problem asks for the posterior probability distribution $p(x|y)$ of a state variable $x$ (temperature) given a prior distribution and an observation $y$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nThe prior distribution of the state $x$ is given as a Gaussian distribution with mean $x_b$ and variance $B$:\n$$\np(x) = \\mathcal{N}(x; x_b, B) = \\frac{1}{\\sqrt{2\\pi B}} \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\nThe observation model is $y = Hx + \\varepsilon$, where the observation error $\\varepsilon$ is drawn from a Gaussian distribution with mean $0$ and variance $R$. This implies that the likelihood function, which is the probability of observing $y$ given the state $x$, is also a Gaussian distribution with mean $Hx$ and variance $R$:\n$$\np(y|x) = \\mathcal{N}(y; Hx, R) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right)\n$$\nSubstituting these into Bayes' theorem:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(y-Hx)^2}{R}\\right) \\exp\\left(-\\frac{1}{2} \\frac{(x-x_b)^2}{B}\\right)\n$$\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B} \\right]\\right)\n$$\nThe term in the square brackets is the variational cost function, often denoted as $J(x)$. The posterior distribution is a Gaussian, as the product of two Gaussians is an unnormalized Gaussian. To find its parameters (mean $x_a$ and variance $P_a$), we need to rearrange the exponent into the canonical quadratic form $-\\frac{1}{2} \\frac{(x-x_a)^2}{P_a}$.\nLet's analyze the exponent term $J(x)$:\n$$\nJ(x) = \\frac{(y-Hx)^2}{R} + \\frac{(x-x_b)^2}{B}\n$$\nWe expand the squared terms:\n$$\nJ(x) = \\frac{y^2 - 2yHx + H^2x^2}{R} + \\frac{x^2 - 2xx_b + x_b^2}{B}\n$$\nNow, we collect terms with respect to powers of $x$:\n$$\nJ(x) = x^2 \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right) - 2x \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right) + \\left(\\frac{y^2}{R} + \\frac{x_b^2}{B}\\right)\n$$\nTo complete the square for $x$, we identify the form $\\frac{1}{P_a}(x-x_a)^2 = \\frac{1}{P_a}(x^2 - 2xx_a + x_a^2)$.\nComparing the coefficient of $x^2$, we find the inverse of the posterior variance $P_a$:\n$$\n\\frac{1}{P_a} = \\frac{H^2}{R} + \\frac{1}{B}\n$$\nComparing the coefficient of the $-2x$ term, we have:\n$$\n\\frac{x_a}{P_a} = \\frac{Hy}{R} + \\frac{x_b}{B}\n$$\nFrom this, we can solve for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nSubstituting the expression for $P_a^{-1}$:\n$$\nx_a = \\left(\\frac{H^2}{R} + \\frac{1}{B}\\right)^{-1} \\left(\\frac{Hy}{R} + \\frac{x_b}{B}\\right)\n$$\nThe problem specifies a direct observation, so the observation operator $H$ is the identity, $H=1$. The formulas simplify to:\n$$\n\\frac{1}{P_a} = \\frac{1}{R} + \\frac{1}{B} = \\frac{B+R}{BR} \\implies P_a = \\frac{BR}{B+R}\n$$\nAnd for the posterior mean $x_a$:\n$$\nx_a = P_a \\left(\\frac{y}{R} + \\frac{x_b}{B}\\right) = \\frac{BR}{B+R} \\left(\\frac{yB + x_bR}{BR}\\right) = \\frac{yB + x_bR}{B+R}\n$$\nThis expression for $x_a$ can be interpreted as a weighted average of the background mean $x_b$ and the observation $y$:\n$$\nx_a = \\left(\\frac{R}{B+R}\\right)x_b + \\left(\\frac{B}{B+R}\\right)y\n$$\nThe weight for the observation is $K = \\frac{B}{B+R}$, known as the Kalman gain in this scalar case.\n\nNow, we substitute the given numerical values:\n- Background mean: $x_b = 10$\n- Background variance: $B = 4$\n- Observation: $y = 12$\n- Observation error variance: $R = 1$\n\nWe first calculate the posterior variance $P_a$:\n$$\nP_a = \\frac{BR}{B+R} = \\frac{4 \\times 1}{4+1} = \\frac{4}{5}\n$$\nNext, we calculate the posterior mean $x_a$:\n$$\nx_a = \\frac{yB + x_bR}{B+R} = \\frac{(12)(4) + (10)(1)}{4+1} = \\frac{48 + 10}{5} = \\frac{58}{5}\n$$\nThus, the posterior distribution is Gaussian with mean $x_a = \\frac{58}{5}$ and variance $P_a = \\frac{4}{5}$.\nThe analysis temperature is $x_a = 11.6$ degrees Celsius, and the analysis uncertainty (variance) is $P_a = 0.8$ degrees Celsius squared. The problem asks for the ordered pair $(x_a, P_a)$.\n\nThe final answer is the ordered pair $\\left(x_a, P_a\\right) = \\left(\\frac{58}{5}, \\frac{4}{5}\\right)$.",
            "answer": "$$\n\\boxed{\n\\left(\\frac{58}{5}, \\frac{4}{5}\\right)\n}\n$$"
        },
        {
            "introduction": "A key power of hybrid-EnVar methods lies in their ability to intelligently update multiple, related physical variables, even when only one is directly observed. This practice moves from a single variable to a coupled temperature-salinity system, a classic challenge in oceanography. You will use a hybrid background error covariance matrix to calculate how an observation of temperature creates a \"balanced\" and physically consistent adjustment in the unobserved salinity field, demonstrating the crucial role of cross-variable error correlations .",
            "id": "3795180",
            "problem": "In a single grid cell of a primitive-equation ocean model, consider the two-component state vector composed of in-situ temperature and practical salinity, denoted by $x = \\begin{pmatrix} T \\\\ S \\end{pmatrix}$. A Hybrid Ensemble-Variational (EnVar) Data Assimilation system is used to form a background error covariance by linearly combining an ensemble-estimated covariance and a static climatological covariance. Observations provide only the local temperature. The hybrid combination weight is $\\,\\beta\\,$ with $\\,0  \\beta  1\\,$.\n\nYou are provided with the following second-moment statistics at this grid cell:\n- Ensemble-estimated second moments: $\\operatorname{var}_{e}(T) = 0.6$, $\\operatorname{var}_{e}(S) = 0.25$, and $\\operatorname{cov}_{e}(S,T) = -0.09$.\n- Static climatological second moments: $\\operatorname{var}_{s}(T) = 0.4$, $\\operatorname{var}_{s}(S) = 0.3$, and $\\operatorname{cov}_{s}(S,T) = -0.06$.\n\nThe hybrid weight is $\\beta = 0.65$. The temperature observation operator is $H = \\begin{pmatrix} 1  0 \\end{pmatrix}$, and the observation error variance is $R = 0.05$. The observed-minus-background temperature innovation at this cell is $d = +0.3$.\n\nAssume linear-Gaussian dynamics and measurement error, and use multivariate linear regression consistent with Hybrid EnVar to compute the balanced salinity analysis increment, $\\delta S$, produced at this grid cell by assimilating the temperature observation. Express your final answer in practical salinity units (psu) and round your result to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of oceanographic data assimilation, well-posed with a complete and consistent set of parameters, and stated objectively. We can therefore proceed to a solution.\n\nThe goal is to compute the salinity analysis increment, $\\delta S$, resulting from the assimilation of a temperature observation. The state vector is $x = \\begin{pmatrix} T \\\\ S \\end{pmatrix}$, where $T$ is the in-situ temperature and $S$ is the practical salinity. The analysis increment vector, $\\delta x = \\begin{pmatrix} \\delta T \\\\ \\delta S \\end{pmatrix}$, is given by the standard linear-Gaussian update equation:\n$$ \\delta x = K d $$\nwhere $d$ is the innovation (observed-minus-background) and $K$ is the Kalman gain matrix.\n\nThe Kalman gain $K$ is defined as:\n$$ K = B H^T (H B H^T + R)^{-1} $$\nHere, $B$ is the background error covariance matrix, $H$ is the observation operator, and $R$ is the observation error variance.\n\nFirst, we must construct the hybrid background error covariance matrix $B$. It is a linear combination of the static climatological covariance $B_s$ and the ensemble-estimated covariance $B_e$. The standard formulation, which we adopt here, is:\n$$ B = (1-\\beta) B_s + \\beta B_e $$\nwhere $\\beta$ is the weight assigned to the ensemble covariance.\n\nThe problem provides the necessary second-moment statistics to construct $B_s$ and $B_e$.\nThe state vector is ordered as $\\begin{pmatrix} T \\\\ S \\end{pmatrix}$. The covariance matrices are of the form $\\begin{pmatrix} \\operatorname{var}(T)  \\operatorname{cov}(T,S) \\\\ \\operatorname{cov}(S,T)  \\operatorname{var}(S) \\end{pmatrix}$.\n\nFrom the givens:\nThe ensemble-estimated covariance matrix $B_e$ is:\n$$ B_e = \\begin{pmatrix} \\operatorname{var}_{e}(T)  \\operatorname{cov}_{e}(T,S) \\\\ \\operatorname{cov}_{e}(S,T)  \\operatorname{var}_{e}(S) \\end{pmatrix} = \\begin{pmatrix} 0.6  -0.09 \\\\ -0.09  0.25 \\end{pmatrix} $$\nThe static climatological covariance matrix $B_s$ is:\n$$ B_s = \\begin{pmatrix} \\operatorname{var}_{s}(T)  \\operatorname{cov}_{s}(T,S) \\\\ \\operatorname{cov}_{s}(S,T)  \\operatorname{var}_{s}(S) \\end{pmatrix} = \\begin{pmatrix} 0.4  -0.06 \\\\ -0.06  0.3 \\end{pmatrix} $$\n\nThe hybrid weight is given as $\\beta = 0.65$, so $1-\\beta = 1 - 0.65 = 0.35$.\nNow we compute the hybrid covariance matrix $B$:\n$$ B = 0.35 \\begin{pmatrix} 0.4  -0.06 \\\\ -0.06  0.3 \\end{pmatrix} + 0.65 \\begin{pmatrix} 0.6  -0.09 \\\\ -0.09  0.25 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0.35 \\times 0.4  0.35 \\times (-0.06) \\\\ 0.35 \\times (-0.06)  0.35 \\times 0.3 \\end{pmatrix} + \\begin{pmatrix} 0.65 \\times 0.6  0.65 \\times (-0.09) \\\\ 0.65 \\times (-0.09)  0.65 \\times 0.25 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0.14  -0.021 \\\\ -0.021  0.105 \\end{pmatrix} + \\begin{pmatrix} 0.39  -0.0585 \\\\ -0.0585  0.1625 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0.14+0.39  -0.021-0.0585 \\\\ -0.021-0.0585  0.105+0.1625 \\end{pmatrix} = \\begin{pmatrix} 0.53  -0.0795 \\\\ -0.0795  0.2675 \\end{pmatrix} $$\n\nNext, we calculate the terms in the Kalman gain formula. The observation operator is $H = \\begin{pmatrix} 1  0 \\end{pmatrix}$, and its transpose is $H^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The observation error variance is $R = 0.05$.\n\nCalculate $B H^T$:\n$$ B H^T = \\begin{pmatrix} 0.53  -0.0795 \\\\ -0.0795  0.2675 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.53 \\\\ -0.0795 \\end{pmatrix} $$\n\nCalculate $H B H^T$:\n$$ H B H^T = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 0.53 \\\\ -0.0795 \\end{pmatrix} = 0.53 $$\nThis term represents the background error variance projected into the observation space.\n\nNow, calculate the denominator of the Kalman gain, which is a scalar in this case:\n$$ H B H^T + R = 0.53 + 0.05 = 0.58 $$\n\nWe can now assemble the Kalman gain matrix $K$:\n$$ K = B H^T (H B H^T + R)^{-1} = \\begin{pmatrix} 0.53 \\\\ -0.0795 \\end{pmatrix} (0.58)^{-1} = \\begin{pmatrix} \\frac{0.53}{0.58} \\\\ \\frac{-0.0795}{0.58} \\end{pmatrix} $$\n\nFinally, we compute the analysis increment vector $\\delta x$ using the given innovation $d = 0.3$:\n$$ \\delta x = \\begin{pmatrix} \\delta T \\\\ \\delta S \\end{pmatrix} = K d = \\begin{pmatrix} \\frac{0.53}{0.58} \\\\ \\frac{-0.0795}{0.58} \\end{pmatrix} (0.3) $$\n\nThe problem asks for the salinity analysis increment, $\\delta S$, which is the second component of the vector $\\delta x$:\n$$ \\delta S = \\frac{-0.0795}{0.58} \\times 0.3 $$\n$$ \\delta S = \\frac{-0.02385}{0.58} \\approx -0.0411206896... $$\n\nRounding the result to four significant figures, we get:\n$$ \\delta S \\approx -0.04112 $$\nThis value is in practical salinity units (psu), consistent with the problem context. The negative sign is physically consistent with the negative temperature-salinity covariance; a positive temperature innovation (warmer observation) leads to a negative salinity increment (fresher analysis).",
            "answer": "$$\\boxed{-0.04112}$$"
        },
        {
            "introduction": "This final, capstone exercise synthesizes all the core components into a simplified but complete hybrid four-dimensional ensemble-variational (4D-EnVar) system. You will tackle an analysis problem that spans a time window, managing both the ensemble-derived and static components of the background error model. By setting up and solving the problem for the optimal control vector, you will gain direct, practical experience with the mathematical machinery that underpins modern operational weather and ocean forecasting systems .",
            "id": "3795123",
            "problem": "Consider a two-time-step four-dimensional ensemble-variational data assimilation (4D-EnVar) problem for a linear, time-evolving ocean state. The objective is to compute the minimum-variance analysis increment at the initial time from innovations at two times by combining an ensemble-derived subspace with a static background component. The fundamental basis is a linear Gaussian Bayesian estimation framework with the following elements.\n\nLet the model state increment at the initial time be written as a hybrid decomposition\n$\n\\delta x_{t_0} = \\sqrt{\\lambda}\\,A_{t_0}\\,w + \\sqrt{1-\\lambda}\\,L\\,v,\n$\nwhere $A_{t_0}$ is the ensemble anomaly matrix at time $t_0$, $L$ is a square-root of a static background covariance basis, $w$ and $v$ are coefficient vectors, and $\\lambda \\in [0,1]$ is a blending coefficient that determines the relative weight of ensemble and static components. The state increment that is sensed at time $t_k$ for $k \\in \\{0,1\\}$ is\n$\n\\delta x_{t_k} = \\sqrt{\\lambda}\\,A_{t_k}\\,w + \\sqrt{1-\\lambda}\\,S_{t_k}\\,v,\n$\nwhere $A_{t_k}$ are the model-propagated ensemble anomaly matrices and $S_{t_k}$ are the model-propagated static bases. The observation operators are linear, denoted $H_{t_k}$, and the observation innovations are $y_{t_k} = \\text{observation}_{t_k} - H_{t_k}\\,x^{\\text{background}}_{t_k}$. The observation errors are zero-mean Gaussian with covariances $R_{t_k}$ that are symmetric positive-definite.\n\nUnder these assumptions, the minimum-variance analysis in the hybrid ensemble-variational setting solves the following unconstrained quadratic minimization over the coefficient vectors $w$ and $v$:\n$$\n\\min_{w,v}\\; J(w,v) = \\sum_{k=0}^{1}\\left(H_{t_k}\\,\\delta x_{t_k} - y_{t_k}\\right)^{\\top} R_{t_k}^{-1} \\left(H_{t_k}\\,\\delta x_{t_k} - y_{t_k}\\right) + w^{\\top}w + v^{\\top}v.\n$$\nYou must compute the analysis increment at the initial time,\n$$\n\\delta x_{t_0}^{\\text{a}} = \\sqrt{\\lambda}\\,A_{t_0}\\,w^{\\ast} + \\sqrt{1-\\lambda}\\,L\\,v^{\\ast},\n$$\nwhere $(w^{\\ast},v^{\\ast})$ minimize $J(w,v)$.\n\nAll quantities are nondimensional in this exercise. Angles are not involved. The program must solve this problem by constructing and solving the equivalent stacked, weighted least-squares system implied by the cost function, without assuming any special structure beyond symmetry and positive-definiteness of $R_{t_k}$. Use numerically stable linear algebra with an explicit computation of $R_{t_k}^{-1/2}$ via an eigendecomposition or a Cholesky factorization. Do not hard-code any closed-form inversion formulas for the normal equations.\n\nTest Suite. Use the following three independent test cases. In each case, compute and return only the initial-time analysis increment $\\delta x_{t_0}^{\\text{a}}$.\n\nTest case $1$ (two-dimensional state, informative observations, moderate hybrid weight):\n- $A_{t_0} = \\begin{bmatrix} 0.8  -0.2 \\\\ 0.1  0.5 \\end{bmatrix}$,\n$A_{t_1} = \\begin{bmatrix} 0.6  -0.15 \\\\ 0.2  0.4 \\end{bmatrix}$.\n- $H_{t_0} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n$R_{t_0} = \\begin{bmatrix} 0.04  0.0 \\\\ 0.0  0.09 \\end{bmatrix}$,\n$y_{t_0} = \\begin{bmatrix} 0.5 \\\\ -0.1 \\end{bmatrix}$.\n- $H_{t_1} = \\begin{bmatrix} 1.0  1.0 \\end{bmatrix}$,\n$R_{t_1} = \\begin{bmatrix} 0.01 \\end{bmatrix}$,\n$y_{t_1} = \\begin{bmatrix} 0.2 \\end{bmatrix}$.\n- $L = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n$S_{t_0} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n$S_{t_1} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n$\\lambda = 0.7$.\n\nTest case $2$ (three-dimensional state, weak information at the second time due to large variance, pure ensemble):\n- $A_{t_0} = \\begin{bmatrix} 0.5  -0.1 \\\\ 0.2  0.3 \\\\ -0.3  0.4 \\end{bmatrix}$,\n$A_{t_1} = \\begin{bmatrix} 0.45  -0.08 \\\\ 0.25  0.28 \\\\ -0.25  0.35 \\end{bmatrix}$.\n- $H_{t_0} = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\end{bmatrix}$,\n$R_{t_0} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.25 \\end{bmatrix}$,\n$y_{t_0} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}$.\n- $H_{t_1} = \\begin{bmatrix} 0.0  0.0  1.0 \\end{bmatrix}$,\n$R_{t_1} = \\begin{bmatrix} 100.0 \\end{bmatrix}$,\n$y_{t_1} = \\begin{bmatrix} 0.05 \\end{bmatrix}$.\n- $L = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}$,\n$S_{t_0} = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}$,\n$S_{t_1} = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix}$,\n$\\lambda = 1.0$.\n\nTest case $3$ (two-dimensional state, rank-deficient ensemble subspace, strong static contribution):\n- $A_{t_0} = \\begin{bmatrix} 1.0  2.0 \\\\ 0.5  1.0 \\end{bmatrix}$,\n$A_{t_1} = \\begin{bmatrix} 0.9  1.8 \\\\ 0.45  0.9 \\end{bmatrix}$.\n- $H_{t_0} = \\begin{bmatrix} 1.0  -1.0 \\end{bmatrix}$,\n$R_{t_0} = \\begin{bmatrix} 0.04 \\end{bmatrix}$,\n$y_{t_0} = \\begin{bmatrix} 0.3 \\end{bmatrix}$.\n- $H_{t_1} = \\begin{bmatrix} 0.5  0.5 \\end{bmatrix}$,\n$R_{t_1} = \\begin{bmatrix} 0.04 \\end{bmatrix}$,\n$y_{t_1} = \\begin{bmatrix} 0.2 \\end{bmatrix}$.\n- $L = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n$S_{t_0} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n$S_{t_1} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n$\\lambda = 0.2$.\n\nFinal output format. Your program should produce a single line of output containing the three analysis increments $\\delta x_{t_0}^{\\text{a}}$ as a comma-separated list of lists, with each component rounded to six decimal places, enclosed in square brackets, for example, `[[a_11,a_12],[a_21,a_22,a_23],[a_31,a_32]]`, where each $a_{i,j}$ is a floating-point number rounded to six decimal places. No other text should be printed.",
            "solution": "The user has provided a well-defined four-dimensional ensemble-variational (4D-EnVar) data assimilation problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Hybrid State Increment Model:**\n    - At initial time $t_0$: $\\delta x_{t_0} = \\sqrt{\\lambda}\\,A_{t_0}\\,w + \\sqrt{1-\\lambda}\\,L\\,v$\n    - At time $t_k$: $\\delta x_{t_k} = \\sqrt{\\lambda}\\,A_{t_k}\\,w + \\sqrt{1-\\lambda}\\,S_{t_k}\\,v$, for $k \\in \\{0, 1\\}$.\n- **Control Variables:** $w$ (ensemble coefficients) and $v$ (static coefficients).\n- **Parameters:**\n    - $\\lambda$: Blending coefficient, $\\lambda \\in [0,1]$.\n    - $A_{t_k}$: Ensemble anomaly matrices.\n    - $L$: Square-root of a static background covariance basis.\n    - $S_{t_k}$: Model-propagated static bases.\n- **Observation Model:**\n    - $H_{t_k}$: Linear observation operators.\n    - $y_{t_k}$: Observation innovations.\n    - $R_{t_k}$: Observation error covariance matrices (symmetric positive-definite).\n- **Objective Function:** Minimize the quadratic cost function $J(w,v)$ over $w$ and $v$:\n$$\nJ(w,v) = \\sum_{k=0}^{1}\\left(H_{t_k}\\,\\delta x_{t_k} - y_{t_k}\\right)^{\\top} R_{t_k}^{-1} \\left(H_{t_k}\\,\\delta x_{t_k} - y_{t_k}\\right) + w^{\\top}w + v^{\\top}v.\n$$\n- **Target Quantity:** The analysis increment at the initial time, $\\delta x_{t_0}^{\\text{a}} = \\sqrt{\\lambda}\\,A_{t_0}\\,w^{\\ast} + \\sqrt{1-\\lambda}\\,L\\,v^{\\ast}$, where $(w^{\\ast}, v^{\\ast})$ are the optimal control vectors that minimize $J(w,v)$.\n- **Test Data:** Three distinct test cases with all necessary matrices and parameters are provided.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding:** The problem is a standard formulation of hybrid 4D-EnVar, a method widely used in operational weather and ocean forecasting. The formulation is based on linear-Gaussian Bayesian estimation, which is a fundamental principle in data assimilation. The problem is scientifically sound.\n- **Well-Posedness:** The cost function $J(w,v)$ is quadratic and convex. The regularization terms $w^{\\top}w + v^{\\top}v$ ensure the Hessian of $J$ is positive definite, which guarantees that a unique minimum $(w^{\\ast}, v^{\\ast})$ exists. The problem is therefore well-posed.\n- **Objectivity:** The problem is stated in precise mathematical terms and provides objective numerical data for the test cases. There are no subjective or ambiguous statements.\n- **Completeness and Consistency:** All required data and definitions are provided for each test case. The constraints and definitions are consistent with one another. For example, the dimensions of the matrices and vectors in each test case are compatible for the required matrix operations. The given $R_{t_k}$ matrices are symmetric and positive-definite as required.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is scientifically sound, well-posed, and all necessary information is provided. I will proceed with the solution.\n\n### Algorithmic Design and Solution\n\nThe core of the problem is to find the control vectors $(w, v)$ that minimize the cost function $J(w,v)$. This is an unconstrained quadratic optimization problem, which is equivalent to solving a linear system. The problem specifies solving this by formulating a weighted least-squares system.\n\nFirst, we rewrite the cost function. The term $\\left(H_{t_k}\\,\\delta x_{t_k} - y_{t_k}\\right)^{\\top} R_{t_k}^{-1} \\left(H_{t_k}\\,\\delta x_{t_k} - y_{t_k}\\right)$ can be expressed as a squared $L_2$-norm with a weighting matrix. Since $R_{t_k}$ is symmetric positive-definite, its inverse $R_{t_k}^{-1}$ is also symmetric positive-definite and has a unique symmetric positive-definite square root, denoted $R_{t_k}^{-1/2}$. This allows us to write the term as:\n$$\n\\left\\| R_{t_k}^{-1/2} \\left(H_{t_k}\\,\\delta x_{t_k} - y_{t_k}\\right) \\right\\|_2^2.\n$$\nThe problem requires an explicit computation of $R_{t_k}^{-1/2}$. This can be achieved via an eigendecomposition. If $R_{t_k} = Q D Q^{\\top}$, where $Q$ is the matrix of eigenvectors and $D$ is the diagonal matrix of eigenvalues, then $R_{t_k}^{-1/2} = Q D^{-1/2} Q^{\\top}$.\n\nSubstituting the expression for $\\delta x_{t_k}$, the term inside the norm becomes:\n$$\nR_{t_k}^{-1/2} \\left( H_{t_k} \\left( \\sqrt{\\lambda}\\,A_{t_k}\\,w + \\sqrt{1-\\lambda}\\,S_{t_k}\\,v \\right) - y_{t_k} \\right).\n$$\nThis expression is linear in the components of $w$ and $v$. We can combine the control variables into a single vector $z = \\begin{bmatrix} w \\\\ v \\end{bmatrix}$. The expression can then be written as a linear system:\n$$\nR_{t_k}^{-1/2} \\left( \\begin{bmatrix} \\sqrt{\\lambda}\\,H_{t_k}A_{t_k}  \\sqrt{1-\\lambda}\\,H_{t_k}S_{t_k} \\end{bmatrix} z - y_{t_k} \\right).\n$$\nLet's define the observation-space projection matrix for time $t_k$ as $G_k = \\begin{bmatrix} \\sqrt{\\lambda}\\,H_{t_k}A_{t_k}  \\sqrt{1-\\lambda}\\,H_{t_k}S_{t_k} \\end{bmatrix}$. The cost function becomes:\n$$\nJ(z) = \\sum_{k=0}^{1} \\left\\| R_{t_k}^{-1/2} G_k z - R_{t_k}^{-1/2} y_{t_k} \\right\\|_2^2 + \\|z\\|_2^2.\n$$\nThis is a regularized linear least-squares problem. We can formulate it as a standard (unregularized) least-squares problem by stacking the matrices and vectors. Let's define the augmented system matrix $\\mathcal{A}$ and augmented data vector $\\mathcal{B}$:\n$$\n\\mathcal{A} = \\begin{bmatrix}\nR_{t_0}^{-1/2} G_0 \\\\\nR_{t_1}^{-1/2} G_1 \\\\\nI\n\\end{bmatrix}, \\quad\n\\mathcal{B} = \\begin{bmatrix}\nR_{t_0}^{-1/2} y_{t_0} \\\\\nR_{t_1}^{-1/2} y_{t_1} \\\\\n0\n\\end{bmatrix}.\n$$\nHere, $I$ is the identity matrix with dimensions matching the size of $z$, and $0$ is a zero vector of the same size. The minimization problem is now to find $z^{\\ast}$ that solves:\n$$\n\\min_z \\|\\mathcal{A}z - \\mathcal{B}\\|_2^2.\n$$\nThis standard linear least-squares problem can be solved robustly using numerical linear algebra libraries, for instance, via QR decomposition or SVD, as implemented in `numpy.linalg.lstsq`. This approach avoids the direct formation and inversion of the Hessian matrix $(\\mathcal{A}^{\\top}\\mathcal{A})$, which can be ill-conditioned.\n\nOnce the optimal control vector $z^{\\ast} = \\begin{bmatrix} w^{\\ast} \\\\ v^{\\ast} \\end{bmatrix}$ is found, the components $w^{\\ast}$ and $v^{\\ast}$ are extracted. The final analysis increment at the initial time $t_0$ is computed using its definition:\n$$\n\\delta x_{t_0}^{\\text{a}} = \\sqrt{\\lambda}\\,A_{t_0}\\,w^{\\ast} + \\sqrt{1-\\lambda}\\,L\\,v^{\\ast}.\n$$\nThis procedure is applied to each test case to obtain the required analysis increments.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 4D-EnVar problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        {\n            'A_t0': np.array([[0.8, -0.2], [0.1, 0.5]]),\n            'A_t1': np.array([[0.6, -0.15], [0.2, 0.4]]),\n            'H_t0': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'R_t0': np.array([[0.04, 0.0], [0.0, 0.09]]),\n            'y_t0': np.array([0.5, -0.1]),\n            'H_t1': np.array([[1.0, 1.0]]),\n            'R_t1': np.array([[0.01]]),\n            'y_t1': np.array([0.2]),\n            'L': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'S_t0': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'S_t1': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'lambda_val': 0.7,\n        },\n        # Test case 2\n        {\n            'A_t0': np.array([[0.5, -0.1], [0.2, 0.3], [-0.3, 0.4]]),\n            'A_t1': np.array([[0.45, -0.08], [0.25, 0.28], [-0.25, 0.35]]),\n            'H_t0': np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            'R_t0': np.array([[1.0, 0.0], [0.0, 0.25]]),\n            'y_t0': np.array([0.1, -0.2]),\n            'H_t1': np.array([[0.0, 0.0, 1.0]]),\n            'R_t1': np.array([[100.0]]),\n            'y_t1': np.array([0.05]),\n            'L': np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            'S_t0': np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            'S_t1': np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            'lambda_val': 1.0,\n        },\n        # Test case 3\n        {\n            'A_t0': np.array([[1.0, 2.0], [0.5, 1.0]]),\n            'A_t1': np.array([[0.9, 1.8], [0.45, 0.9]]),\n            'H_t0': np.array([[1.0, -1.0]]),\n            'R_t0': np.array([[0.04]]),\n            'y_t0': np.array([0.3]),\n            'H_t1': np.array([[0.5, 0.5]]),\n            'R_t1': np.array([[0.04]]),\n            'y_t1': np.array([0.2]),\n            'L': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'S_t0': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'S_t1': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'lambda_val': 0.2,\n        },\n    ]\n\n    results = [compute_analysis_increment(case) for case in test_cases]\n    \n    # Format the output to match the required format: [[a,b,...],[c,d,...],...]\n    # without extra spaces.\n    rounded_results = [np.round(res, 6).tolist() for res in results]\n    output_str = str(rounded_results).replace(\" \", \"\")\n    print(output_str)\n\ndef compute_r_inv_sqrt(R):\n    \"\"\"\n    Computes R^{-1/2} for a symmetric positive-definite matrix R via eigendecomposition.\n    \"\"\"\n    if R.ndim == 0:\n        return np.array([[1.0 / np.sqrt(R)]])\n    eigvals, eigvecs = np.linalg.eigh(R)\n    if np.any(eigvals = 0):\n        raise ValueError(\"Matrix R must be positive-definite.\")\n    d_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n    return eigvecs @ d_inv_sqrt @ eigvecs.T\n\ndef compute_analysis_increment(case):\n    \"\"\"\n    Computes the analysis increment for a single test case.\n    \"\"\"\n    A_t0, A_t1 = case['A_t0'], case['A_t1']\n    H_t0, H_t1 = case['H_t0'], case['H_t1']\n    R_t0, R_t1 = case['R_t0'], case['R_t1']\n    y_t0, y_t1 = case['y_t0'], case['y_t1']\n    L, S_t0, S_t1 = case['L'], case['S_t0'], case['S_t1']\n    lambda_val = case['lambda_val']\n\n    sqrt_lambda = np.sqrt(lambda_val)\n    sqrt_1_minus_lambda = np.sqrt(1.0 - lambda_val)\n    \n    n_ens = A_t0.shape[1]\n    n_static = L.shape[1]\n    n_control = n_ens + n_static\n\n    # Compute weighting matrices R^{-1/2}\n    R0_inv_sqrt = compute_r_inv_sqrt(R_t0)\n    R1_inv_sqrt = compute_r_inv_sqrt(R_t1)\n\n    # Construct the matrix G_0 and G_1 blocks\n    G0 = np.hstack([sqrt_lambda * H_t0 @ A_t0, sqrt_1_minus_lambda * H_t0 @ S_t0])\n    G1 = np.hstack([sqrt_lambda * H_t1 @ A_t1, sqrt_1_minus_lambda * H_t1 @ S_t1])\n\n    # Apply weighting\n    G0_w = R0_inv_sqrt @ G0\n    G1_w = R1_inv_sqrt @ G1\n    \n    # Stack the G matrices to form the main part of the least-squares system matrix\n    G_stacked = np.vstack([G0_w, G1_w])\n    \n    # Construct the identity part for regularization\n    I_reg = np.identity(n_control)\n\n    # Form the full least-squares system matrix A_ls\n    A_ls = np.vstack([G_stacked, I_reg])\n\n    # Construct the weighted innovation vector for times t0 and t1\n    y0_w = (R0_inv_sqrt @ y_t0.reshape(-1, 1)).flatten()\n    y1_w = (R1_inv_sqrt @ y_t1.reshape(-1, 1)).flatten()\n    \n    # Stack the weighted innovations\n    y_stacked = np.concatenate([y0_w, y1_w])\n    \n    # Construct the zero vector for the regularization part\n    zeros_reg = np.zeros(n_control)\n\n    # Form the full least-squares right-hand side vector b_ls\n    b_ls = np.concatenate([y_stacked, zeros_reg])\n\n    # Solve the least-squares problem for the control vector z = [w, v]^T\n    z_star, _, _, _ = np.linalg.lstsq(A_ls, b_ls, rcond=None)\n    \n    # Extract optimal coefficients\n    w_star = z_star[:n_ens]\n    v_star = z_star[n_ens:]\n\n    # Compute the final analysis increment at time t0\n    dx_t0_a = sqrt_lambda * (A_t0 @ w_star) + sqrt_1_minus_lambda * (L @ v_star)\n    \n    return dx_t0_a\n\nsolve()\n\n```"
        }
    ]
}