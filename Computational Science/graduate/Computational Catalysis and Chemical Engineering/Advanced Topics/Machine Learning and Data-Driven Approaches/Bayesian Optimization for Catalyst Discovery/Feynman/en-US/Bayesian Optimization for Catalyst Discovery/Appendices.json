{
    "hands_on_practices": [
        {
            "introduction": "At the heart of Bayesian optimization lies the surrogate model, typically a Gaussian Process ($GP$), which provides probabilistic predictions of catalyst performance. This exercise provides a hands-on calculation of the $GP$ posterior mean and variance, fundamental quantities that inform the entire optimization loop. By working through this problem , you will gain a concrete understanding of how a $GP$ updates its beliefs based on observed data and a noise model.",
            "id": "3869839",
            "problem": "In Bayesian optimization for catalyst discovery, a Gaussian process (GP) surrogate model is used to represent the unknown mapping from catalyst descriptors to a measurable performance metric, such as normalized log turnover frequency. The Gaussian process (GP) prior assumes that the latent function values at the training points and a test point have a joint multivariate normal distribution governed by a positive semidefinite kernel. Observations are modeled as the latent function corrupted by independent additive Gaussian noise whose variance is controlled by a noise hyperparameter. Consider two catalysts with descriptor vectors collected in the set $X$ and a new test catalyst descriptor $x_{\\ast}$. The training outputs are measured normalized log turnover frequencies $y \\in \\mathbb{R}^{2}$, modeled as $y = f(X) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2} I)$, where $\\sigma_{n}^{2}$ is the observation noise variance (the noise hyperparameter) and $I$ is the identity matrix of appropriate dimension. The kernel values evaluated at these points are given as the $2 \\times 2$ matrix $K(X,X)$, the $2 \\times 1$ vector $k(X,x_{\\ast})$, and the scalar $k(x_{\\ast},x_{\\ast})$:\n$$\nK(X,X) = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad k(X,x_{\\ast}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad k(x_{\\ast},x_{\\ast}) = 2.\n$$\nAssume the observed outputs are\n$$\ny = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nStarting from the definition of a joint multivariate normal distribution for the latent function values and the observation model $y = f(X) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2} I)$, derive the Gaussian process posterior distribution at the test point $x_{\\ast}$, and compute the posterior mean $\\mu_{\\ast}$ and posterior variance $\\sigma_{\\ast}^{2}$ explicitly in terms of the noise hyperparameter $\\sigma_{n}^{2}$. Show how the noise hyperparameter affects the uncertainty by expressing $\\sigma_{\\ast}^{2}$ as a function of $\\sigma_{n}^{2}$. Report your final answer as a row matrix with two entries $(\\mu_{\\ast}, \\sigma_{\\ast}^{2})$. No rounding is required. Treat all quantities as dimensionless.",
            "solution": "The problem is well-posed, scientifically grounded, objective, and contains all necessary information for a unique solution. The provided kernel matrix $K(X,X)$ is symmetric and positive definite, with eigenvalues of $1$ and $3$, confirming its validity as a covariance matrix. The problem is therefore valid, and we may proceed with the derivation.\n\nThe core of Gaussian Process (GP) regression is the assumption that any finite collection of function values has a joint multivariate normal distribution. We begin by considering the latent (unobserved) function values at the training points, $f(X)$, and the test point, $f(x_{\\ast})$. For notational simplicity, let $f_{\\ast} = f(x_{\\ast})$. Assuming a zero-mean GP prior, the joint distribution is:\n$$\n\\begin{pmatrix} f(X) \\\\ f_{\\ast} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X,X) & k(X,x_{\\ast}) \\\\ k(X,x_{\\ast})^T & k(x_{\\ast},x_{\\ast}) \\end{pmatrix} \\right)\n$$\nwhere $\\mathcal{N}$ denotes the multivariate normal distribution, $\\mathbf{0}$ is a zero vector of appropriate dimension, and the block matrix is the covariance matrix defined by the kernel function $k$.\n\nThe problem states that the observations $y$ are corrupted by independent and identically distributed Gaussian noise, $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2}I)$, where $I$ is the identity matrix. The observation model is $y = f(X) + \\epsilon$. This implies that the vector of observations $y$ is also a Gaussian random variable. The distribution of $y$ is found by summing the random variables $f(X)$ and $\\epsilon$. Since both are zero-mean, the mean of $y$ is also zero. The covariance of $y$ is the sum of the covariances: $\\text{Cov}(y) = \\text{Cov}(f(X)) + \\text{Cov}(\\epsilon) = K(X,X) + \\sigma_{n}^{2}I$. The noise $\\epsilon$ is independent of the latent function values, so the covariance between $y$ and $f_{\\ast}$ is the same as the covariance between $f(X)$ and $f_{\\ast}$, which is $k(X,x_{\\ast})$.\n\nTherefore, the joint distribution of the observed training outputs $y$ and the latent function value at the test point $f_{\\ast}$ is:\n$$\n\\begin{pmatrix} y \\\\ f_{\\ast} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X,X) + \\sigma_{n}^{2}I & k(X,x_{\\ast}) \\\\ k(X,x_{\\ast})^T & k(x_{\\ast},x_{\\ast}) \\end{pmatrix} \\right)\n$$\nThe goal is to find the posterior distribution of $f_{\\ast}$ given the observed data $y$, which is $p(f_{\\ast} | y, X, x_{\\ast})$. For a partitioned multivariate normal distribution $\\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mu_{1} \\\\ \\mu_{2} \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix} \\right)$, the conditional distribution $p(z_{2} | z_{1})$ is also Gaussian with mean and covariance given by:\n$$\n\\mu_{2|1} = \\mu_{2} + \\Sigma_{21} \\Sigma_{11}^{-1} (z_{1} - \\mu_{1})\n$$\n$$\n\\Sigma_{22|1} = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\n$$\nBy mapping our variables ($z_{1} \\to y$, $z_{2} \\to f_{\\ast}$) and parameters ($\\mu_{1} \\to \\mathbf{0}$, $\\mu_{2} \\to 0$, $\\Sigma_{11} \\to K(X,X)+\\sigma_n^2 I$, $\\Sigma_{12} \\to k(X,x_{\\ast})$, $\\Sigma_{21} \\to k(X,x_{\\ast})^T$, $\\Sigma_{22} \\to k(x_{\\ast},x_{\\ast})$), we obtain the posterior mean $\\mu_{\\ast}$ and posterior variance $\\sigma_{\\ast}^{2}$ for $f_{\\ast}$:\n$$\n\\mu_{\\ast} = k(X,x_{\\ast})^T (K(X,X) + \\sigma_{n}^{2}I)^{-1} y\n$$\n$$\n\\sigma_{\\ast}^{2} = k(x_{\\ast},x_{\\ast}) - k(X,x_{\\ast})^T (K(X,X) + \\sigma_{n}^{2}I)^{-1} k(X,x_{\\ast})\n$$\nWe now substitute the given values into these expressions. The inputs are:\n$$\nK(X,X) = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad k(X,x_{\\ast}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad k(x_{\\ast},x_{\\ast}) = 2, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nFirst, we compute the matrix to be inverted, let us call it $K_{y} = K(X,X) + \\sigma_{n}^{2}I$:\n$$\nK_{y} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\sigma_{n}^{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2+\\sigma_{n}^{2} & 1 \\\\ 1 & 2+\\sigma_{n}^{2} \\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(K_{y}) = (2+\\sigma_{n}^{2})^{2} - 1^{2} = 4 + 4\\sigma_{n}^{2} + (\\sigma_{n}^{2})^{2} - 1 = (\\sigma_{n}^{2})^{2} + 4\\sigma_{n}^{2} + 3$. This can be factored as $(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)$.\nThe inverse is:\n$$\nK_{y}^{-1} = \\frac{1}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} \\begin{pmatrix} 2+\\sigma_{n}^{2} & -1 \\\\ -1 & 2+\\sigma_{n}^{2} \\end{pmatrix}\n$$\nNow we compute the posterior mean $\\mu_{\\ast}$:\n$$\n\\mu_{\\ast} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\frac{1}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} \\begin{pmatrix} 2+\\sigma_{n}^{2} & -1 \\\\ -1 & 2+\\sigma_{n}^{2} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nFirst, multiply the inverse matrix by the vector $y$:\n$$\n\\begin{pmatrix} 2+\\sigma_{n}^{2} & -1 \\\\ -1 & 2+\\sigma_{n}^{2} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3(2+\\sigma_{n}^{2}) - 1 \\\\ -3 + (2+\\sigma_{n}^{2}) \\end{pmatrix} = \\begin{pmatrix} 6+3\\sigma_{n}^{2}-1 \\\\ -1+\\sigma_{n}^{2} \\end{pmatrix} = \\begin{pmatrix} 5+3\\sigma_{n}^{2} \\\\ -1+\\sigma_{n}^{2} \\end{pmatrix}\n$$\nThen, pre-multiply by $k(X,x_{\\ast})^T$:\n$$\n\\mu_{\\ast} = \\frac{1}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 5+3\\sigma_{n}^{2} \\\\ -1+\\sigma_{n}^{2} \\end{pmatrix} = \\frac{(5+3\\sigma_{n}^{2}) + (-1+\\sigma_{n}^{2})}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} = \\frac{4+4\\sigma_{n}^{2}}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)}\n$$\nSimplifying this expression yields:\n$$\n\\mu_{\\ast} = \\frac{4(\\sigma_{n}^{2}+1)}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} = \\frac{4}{\\sigma_{n}^{2}+3}\n$$\nNext, we compute the posterior variance $\\sigma_{\\ast}^{2}$:\n$$\n\\sigma_{\\ast}^{2} = k(x_{\\ast},x_{\\ast}) - k(X,x_{\\ast})^T K_{y}^{-1} k(X,x_{\\ast}) = 2 - \\begin{pmatrix} 1 & 1 \\end{pmatrix} K_{y}^{-1} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nLet's compute the quadratic form term. We first evaluate $K_{y}^{-1} k(X,x_{\\ast})$:\n$$\nK_{y}^{-1} k(X,x_{\\ast}) = \\frac{1}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} \\begin{pmatrix} 2+\\sigma_{n}^{2} & -1 \\\\ -1 & 2+\\sigma_{n}^{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} \\begin{pmatrix} 1+\\sigma_{n}^{2} \\\\ 1+\\sigma_{n}^{2} \\end{pmatrix}\n$$\nThen, pre-multiply by $k(X,x_{\\ast})^T$:\n$$\nk(X,x_{\\ast})^T K_{y}^{-1} k(X,x_{\\ast}) = \\frac{1}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1+\\sigma_{n}^{2} \\\\ 1+\\sigma_{n}^{2} \\end{pmatrix} = \\frac{2(1+\\sigma_{n}^{2})}{(\\sigma_{n}^{2}+1)(\\sigma_{n}^{2}+3)} = \\frac{2}{\\sigma_{n}^{2}+3}\n$$\nFinally, we substitute this back into the expression for $\\sigma_{\\ast}^{2}$:\n$$\n\\sigma_{\\ast}^{2} = 2 - \\frac{2}{\\sigma_{n}^{2}+3} = \\frac{2(\\sigma_{n}^{2}+3) - 2}{\\sigma_{n}^{2}+3} = \\frac{2\\sigma_{n}^{2}+6-2}{\\sigma_{n}^{2}+3} = \\frac{2\\sigma_{n}^{2}+4}{\\sigma_{n}^{2}+3} = \\frac{2(\\sigma_{n}^{2}+2)}{\\sigma_{n}^{2}+3}\n$$\nThe posterior variance $\\sigma_{\\ast}^{2}$ explicitly shows its dependence on the noise hyperparameter $\\sigma_{n}^{2}$. As $\\sigma_{n}^{2} \\to \\infty$, the observation noise dominates, making the training data uninformative. In this limit, $\\mu_{\\ast} \\to 0$ (the prior mean) and $\\sigma_{\\ast}^{2} \\to 2$ (the prior variance $k(x_{\\ast},x_{\\ast})$). Conversely, as $\\sigma_{n}^{2} \\to 0$, the model becomes highly confident in the noise-free training data, yielding $\\mu_{\\ast} \\to 4/3$ and $\\sigma_{\\ast}^{2} \\to 4/3$.\n\nThe final results for the posterior mean and variance are thus:\n$\\mu_{\\ast} = \\frac{4}{\\sigma_{n}^{2}+3}$\n$\\sigma_{\\ast}^{2} = \\frac{2(\\sigma_{n}^{2}+2)}{\\sigma_{n}^{2}+3}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{\\sigma_{n}^{2} + 3} & \\frac{2(\\sigma_{n}^{2} + 2)}{\\sigma_{n}^{2} + 3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A predictive model is only as good as the strategy used to select the next experiment. This practice introduces the Expected Improvement ($EI$) acquisition function, a powerful tool for balancing exploration and exploitation in the search for better catalysts. By deriving and calculating $EI$ for several candidates , you will learn how Bayesian optimization intelligently decides where to sample next to maximize the probability of finding a superior material.",
            "id": "3869785",
            "problem": "A research team is using Bayesian optimization to accelerate the discovery of a heterogeneous catalyst that maximizes the turnover frequency per active site (units of $\\mathrm{s}^{-1}$) under fixed reaction conditions. A Gaussian Process (GP) surrogate has been trained on existing data. For three not-yet-tested candidate compositions, the GP posterior predictive distribution for the activity $Y$ is modeled as a Normal distribution with mean $\\mu$ and standard deviation $\\sigma$: $Y \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. The best activity observed so far is $y^{\\star} = 1.75$ $\\mathrm{s}^{-1}$, and the three candidates have the following predictive parameters:\n- Candidate $1$: $\\mu_{1} = 1.80$ $\\mathrm{s}^{-1}$, $\\sigma_{1} = 0.20$ $\\mathrm{s}^{-1}$.\n- Candidate $2$: $\\mu_{2} = 1.60$ $\\mathrm{s}^{-1}$, $\\sigma_{2} = 0.60$ $\\mathrm{s}^{-1}$.\n- Candidate $3$: $\\mu_{3} = 2.00$ $\\mathrm{s}^{-1}$, $\\sigma_{3} = 0.05$ $\\mathrm{s}^{-1}$.\n\nDefine the improvement random variable for a maximization objective as $I = \\max\\{0, Y - y^{\\star}\\}$. Using only the definition of mathematical expectation, the properties of the Normal distribution, and standard calculus, carry out the following:\n\n1. Starting from the definition of $I$ and $Y \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, derive an analytic expression for the expected improvement $\\mathbb{E}[I]$ in terms of $\\mu$, $\\sigma$, and $y^{\\star}$.\n2. Evaluate the expected improvement numerically for each candidate using the given $(\\mu,\\sigma)$ values. Express each expected improvement in $\\mathrm{s}^{-1}$ and round each to four significant figures.\n3. Rank the three candidates in descending order of expected improvement and identify which single candidate should be selected next.\n\nReport as your final answer only the index of the candidate with the highest expected improvement (i.e., $1$, $2$, or $3$). No other information should be included in the final answer.",
            "solution": "The problem has been validated and is scientifically grounded, well-posed, and objective. It is a standard application of Bayesian optimization principles in computational science. We shall now proceed with the solution.\n\nThe problem requires a three-part solution: first, to derive the analytical expression for the Expected Improvement ($\\mathbb{E}[I]$); second, to calculate its value for three candidate catalysts; and third, to rank the candidates and select the next one to test.\n\n### Part 1: Derivation of the Expected Improvement Expression\n\nThe improvement random variable $I$ is defined for a maximization problem as $I = \\max\\{0, Y - y^{\\star}\\}$, where $Y$ is the predicted activity and $y^{\\star}$ is the best activity observed so far. The activity $Y$ for a candidate is modeled by a Gaussian Process posterior, which is a Normal distribution, $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with a probability density function (PDF) given by:\n$$f_Y(y) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$$\nThe expected improvement, $\\mathbb{E}[I]$, is the expectation of the random variable $I$. By the definition of mathematical expectation for a continuous random variable, we have:\n$$\\mathbb{E}[I] = \\int_{-\\infty}^{\\infty} I(y) f_Y(y) \\, dy = \\int_{-\\infty}^{\\infty} \\max\\{0, y - y^{\\star}\\} f_Y(y) \\, dy$$\nThe term $\\max\\{0, y - y^{\\star}\\}$ is non-zero only when $y > y^{\\star}$. Therefore, the integral's lower limit can be changed from $-\\infty$ to $y^{\\star}$, and the integrand becomes $y - y^{\\star}$:\n$$\\mathbb{E}[I] = \\int_{y^{\\star}}^{\\infty} (y - y^{\\star}) f_Y(y) \\, dy$$\nWe can split this integral into two parts:\n$$\\mathbb{E}[I] = \\int_{y^{\\star}}^{\\infty} y f_Y(y) \\, dy - y^{\\star} \\int_{y^{\\star}}^{\\infty} f_Y(y) \\, dy$$\nTo evaluate these integrals, we perform a change of variables to the standard normal distribution. Let $z = \\frac{y - \\mu}{\\sigma}$. This implies $y = \\mu + \\sigma z$ and $dy = \\sigma dz$. The PDF of the standard normal distribution is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$. The original PDF can be written as $f_Y(y)dy = \\frac{1}{\\sigma} \\phi\\left(\\frac{y-\\mu}{\\sigma}\\right) dy = \\phi(z) dz$. The lower integration limit for $y$ is $y^{\\star}$, which corresponds to a standardized value $z_{std} = \\frac{y^{\\star} - \\mu}{\\sigma}$. The upper limit $y \\to \\infty$ corresponds to $z \\to \\infty$.\n\nSubstituting these into the split integral expression:\n$$\\mathbb{E}[I] = \\int_{z_{std}}^{\\infty} (\\mu + \\sigma z) \\phi(z) \\, dz - y^{\\star} \\int_{z_{std}}^{\\infty} \\phi(z) \\, dz$$\nCombining terms:\n$$\\mathbb{E}[I] = \\int_{z_{std}}^{\\infty} (\\mu - y^{\\star} + \\sigma z) \\phi(z) \\, dz$$\n$$\\mathbb{E}[I] = (\\mu - y^{\\star}) \\int_{z_{std}}^{\\infty} \\phi(z) \\, dz + \\sigma \\int_{z_{std}}^{\\infty} z \\phi(z) \\, dz$$\nLet's evaluate each term. The first integral is the probability that a standard normal variable $Z$ is greater than $z_{std}$, which is $P(Z > z_{std})$. This can be expressed using the standard normal cumulative distribution function (CDF), $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) \\, dt$, as $1 - \\Phi(z_{std})$.\nThe second integral is:\n$$\\int_{z_{std}}^{\\infty} z \\phi(z) \\, dz = \\int_{z_{std}}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\, dz$$\nThis integral can be solved by recognizing that the derivative of $\\exp(-z^2/2)$ with respect to $z$ is $-z \\exp(-z^2/2)$. Thus, the antiderivative of $z \\exp(-z^2/2)$ is $-\\exp(-z^2/2)$.\n$$\\int_{z_{std}}^{\\infty} z \\phi(z) \\, dz = \\frac{1}{\\sqrt{2\\pi}} \\left[-\\exp\\left(-\\frac{z^2}{2}\\right)\\right]_{z_{std}}^{\\infty} = -\\frac{1}{\\sqrt{2\\pi}} \\left( \\lim_{z \\to \\infty} \\exp\\left(-\\frac{z^2}{2}\\right) - \\exp\\left(-\\frac{z_{std}^2}{2}\\right) \\right)$$\n$$= -\\frac{1}{\\sqrt{2\\pi}} \\left(0 - \\exp\\left(-\\frac{z_{std}^2}{2}\\right)\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z_{std}^2}{2}\\right) = \\phi(z_{std})$$\nSubstituting these results back, we get the analytical expression for the expected improvement:\n$$\\mathbb{E}[I] = (\\mu - y^{\\star})(1 - \\Phi(z_{std})) + \\sigma \\phi(z_{std})$$\nwhere $z_{std} = \\frac{y^{\\star} - \\mu}{\\sigma}$. This is the final derived expression.\n\n### Part 2: Numerical Evaluation for Each Candidate\n\nWe are given $y^{\\star} = 1.75$ $\\mathrm{s}^{-1}$. We use the derived formula to calculate $\\mathbb{E}[I]$ for each of the three candidates. We will require numerical values for the standard normal PDF $\\phi(z)$ and CDF $\\Phi(z)$.\n\n**Candidate 1:** $\\mu_{1} = 1.80$ $\\mathrm{s}^{-1}$, $\\sigma_{1} = 0.20$ $\\mathrm{s}^{-1}$.\nFirst, we calculate the standardized value $z_{std,1}$:\n$$z_{std,1} = \\frac{y^{\\star} - \\mu_1}{\\sigma_1} = \\frac{1.75 - 1.80}{0.20} = -0.25$$\nNow we find the values of $\\phi(-0.25)$ and $\\Phi(-0.25)$.\nUsing standard tables or a calculator: $\\phi(-0.25) = \\phi(0.25) \\approx 0.38667$ and $\\Phi(-0.25) = 1 - \\Phi(0.25) \\approx 1 - 0.59871 = 0.40129$.\nSubstitute these values into the expression for $\\mathbb{E}[I_1]$:\n$$\\mathbb{E}[I_1] = (\\mu_1 - y^{\\star})(1 - \\Phi(z_{std,1})) + \\sigma_1 \\phi(z_{std,1})$$\n$$\\mathbb{E}[I_1] = (1.80 - 1.75)(1 - \\Phi(-0.25)) + (0.20)\\phi(-0.25)$$\n$$\\mathbb{E}[I_1] = (0.05)(\\Phi(0.25)) + (0.20)\\phi(0.25)$$\n$$\\mathbb{E}[I_1] \\approx (0.05)(0.59871) + (0.20)(0.38667) \\approx 0.0299355 + 0.077334 \\approx 0.1072695$$\nRounding to four significant figures, $\\mathbb{E}[I_1] \\approx 0.1073$ $\\mathrm{s}^{-1}$.\n\n**Candidate 2:** $\\mu_{2} = 1.60$ $\\mathrm{s}^{-1}$, $\\sigma_{2} = 0.60$ $\\mathrm{s}^{-1}$.\nStandardized value $z_{std,2}$:\n$$z_{std,2} = \\frac{y^{\\star} - \\mu_2}{\\sigma_2} = \\frac{1.75 - 1.60}{0.60} = \\frac{0.15}{0.60} = 0.25$$\nWe need $\\phi(0.25) \\approx 0.38667$ and $\\Phi(0.25) \\approx 0.59871$.\nSubstitute into the expression for $\\mathbb{E}[I_2]$:\n$$\\mathbb{E}[I_2] = (\\mu_2 - y^{\\star})(1 - \\Phi(z_{std,2})) + \\sigma_2 \\phi(z_{std,2})$$\n$$\\mathbb{E}[I_2] = (1.60 - 1.75)(1 - \\Phi(0.25)) + (0.60)\\phi(0.25)$$\n$$\\mathbb{E}[I_2] \\approx (-0.15)(1 - 0.59871) + (0.60)(0.38667)$$\n$$\\mathbb{E}[I_2] \\approx (-0.15)(0.40129) + 0.232002 \\approx -0.0601935 + 0.232002 \\approx 0.1718085$$\nRounding to four significant figures, $\\mathbb{E}[I_2] \\approx 0.1718$ $\\mathrm{s}^{-1}$.\n\n**Candidate 3:** $\\mu_{3} = 2.00$ $\\mathrm{s}^{-1}$, $\\sigma_{3} = 0.05$ $\\mathrm{s}^{-1}$.\nStandardized value $z_{std,3}$:\n$$z_{std,3} = \\frac{y^{\\star} - \\mu_3}{\\sigma_3} = \\frac{1.75 - 2.00}{0.05} = \\frac{-0.25}{0.05} = -5$$\nWe need $\\phi(-5)$ and $\\Phi(-5)$. For large negative $z$, these values become very small.\n$\\phi(-5) = \\phi(5) \\approx 1.4867 \\times 10^{-6}$.\n$\\Phi(-5) = 1 - \\Phi(5) \\approx 2.8665 \\times 10^{-7}$.\nSubstitute into the expression for $\\mathbb{E}[I_3]$:\n$$\\mathbb{E}[I_3] = (\\mu_3 - y^{\\star})(1 - \\Phi(z_{std,3})) + \\sigma_3 \\phi(z_{std,3})$$\n$$\\mathbb{E}[I_3] = (2.00 - 1.75)(1 - \\Phi(-5)) + (0.05)\\phi(-5)$$\n$$\\mathbb{E}[I_3] = (0.25)(\\Phi(5)) + (0.05)\\phi(5)$$\n$$\\mathbb{E}[I_3] \\approx (0.25)(1 - 2.8665 \\times 10^{-7}) + (0.05)(1.4867 \\times 10^{-6})$$\n$$\\mathbb{E}[I_3] \\approx (0.25 - 7.166 \\times 10^{-8}) + (7.4335 \\times 10^{-8}) \\approx 0.25000000267$$\nRounding to four significant figures, $\\mathbb{E}[I_3] \\approx 0.2500$ $\\mathrm{s}^{-1}$.\n\n### Part 3: Ranking and Selection\n\nThe calculated expected improvements are:\n-   $\\mathbb{E}[I_1] \\approx 0.1073$ $\\mathrm{s}^{-1}$\n-   $\\mathbb{E}[I_2] \\approx 0.1718$ $\\mathrm{s}^{-1}$\n-   $\\mathbb{E}[I_3] \\approx 0.2500$ $\\mathrm{s}^{-1}$\n\nRanking these values in descending order:\n1.  Candidate 3: $\\mathbb{E}[I_3] \\approx 0.2500$ $\\mathrm{s}^{-1}$\n2.  Candidate 2: $\\mathbb{E}[I_2] \\approx 0.1718$ $\\mathrm{s}^{-1}$\n3.  Candidate 1: $\\mathbb{E}[I_1] \\approx 0.1073$ $\\mathrm{s}^{-1}$\n\nThe strategy in Bayesian optimization using the Expected Improvement acquisition function is to select the candidate with the maximum $\\mathbb{E}[I]$ value for the next experiment. In this case, Candidate 3 has the highest expected improvement. Therefore, it is the single candidate that should be selected next.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Real-world catalyst design is often a constrained optimization problem, where we must maximize performance while satisfying crucial constraints like thermal stability or cost. This exercise extends the concept of Expected Improvement to handle such scenarios, teaching you how to combine a model for the objective with a model for the constraint. By computing a constrained acquisition value , you will see how to guide the optimization search towards regions that are not only high-performing but also feasible.",
            "id": "3869775",
            "problem": "A catalyst discovery campaign uses Gaussian Process (GP) regression to model the normalized catalytic activity and a GP classifier to model a binary feasibility constraint capturing thermal stability under reaction conditions. At a candidate point $x_{\\mathrm{c}}$ in compositionâ€“processing space, the GP regression posterior for the activity $f(x)$ is Gaussian with mean $\\mu_{f} = 1.05$ and standard deviation $\\sigma_{f} = 0.20$, and the current best observed activity is $f^{\\star} = 0.95$. The GP classifier employs a probit link for the feasibility label, with the latent variable $h(x)$ at $x_{\\mathrm{c}}$ distributed as a Gaussian with mean $\\mu_{h} = 0.70$ and variance $\\sigma_{h}^{2} = 0.36$.\n\nStarting from the definitions of the standard normal density and cumulative distribution functions, the Expected Improvement (EI) for a Gaussian predictive model, and the probit likelihood in GP classification, do the following:\n\n- Derive the probability of feasibility $\\mathrm{PF}(x_{\\mathrm{c}})$ as the expectation of the probit link under the Gaussian latent predictive distribution at $x_{\\mathrm{c}}$.\n- Derive the Expected Improvement $\\mathrm{EI}(x_{\\mathrm{c}})$ for the activity model at $x_{\\mathrm{c}}$.\n- Under the common independence assumption between the objective and constraint posteriors at $x_{\\mathrm{c}}$, combine the two to form the constrained acquisition value $a_{\\mathrm{c}}(x_{\\mathrm{c}}) = \\mathrm{PF}(x_{\\mathrm{c}})\\,\\mathrm{EI}(x_{\\mathrm{c}})$ used for acquisition ranking.\n\nCompute $a_{\\mathrm{c}}(x_{\\mathrm{c}})$ using the provided numerical values. Round your final constrained acquisition value to four significant figures. Express the final answer as a dimensionless quantity.",
            "solution": "The user-provided problem is evaluated as valid. It is scientifically grounded in the domain of Bayesian optimization for materials science, well-posed with a complete and consistent set of givens, and expressed in objective, formal language. The problem requires the derivation and calculation of standard quantities in this field, and all necessary information is provided.\n\nThe problem asks for the computation of the constrained acquisition value $a_{\\mathrm{c}}(x_{\\mathrm{c}})$ at a candidate point $x_{\\mathrm{c}}$, defined as $a_{\\mathrm{c}}(x_{\\mathrm{c}}) = \\mathrm{PF}(x_{\\mathrm{c}})\\,\\mathrm{EI}(x_{\\mathrm{c}})$. This requires the derivation and calculation of two components: the probability of feasibility, $\\mathrm{PF}(x_{\\mathrm{c}})$, and the expected improvement, $\\mathrm{EI}(x_{\\mathrm{c}})$.\n\n**1. Derivation and Calculation of Probability of Feasibility, $\\mathrm{PF}(x_{\\mathrm{c}})$**\n\nThe thermal stability is modeled by a binary feasibility label. The problem states that the GP classifier uses a probit link function. For a latent variable $h(x)$, the probability of the point $x$ being feasible (label $y=1$) is given by $P(y=1|h(x)) = \\Phi(h(x))$, where $\\Phi(\\cdot)$ is the standard normal cumulative distribution function (CDF).\n\nThe probability of feasibility at $x_{\\mathrm{c}}$, $\\mathrm{PF}(x_{\\mathrm{c}})$, is the expectation of this likelihood, averaged over the posterior predictive distribution of the latent variable $h(x_{\\mathrm{c}})$. The posterior for $h(x_{\\mathrm{c}})$ is given as a Gaussian distribution, $h(x_{\\mathrm{c}}) \\sim \\mathcal{N}(h; \\mu_{h}, \\sigma_{h}^{2})$.\n\n$$\n\\mathrm{PF}(x_{\\mathrm{c}}) = \\mathbb{E}_{h \\sim \\mathcal{N}(\\mu_h, \\sigma_h^2)}[\\Phi(h)] = \\int_{-\\infty}^{\\infty} \\Phi(h) \\, \\mathcal{N}(h; \\mu_{h}, \\sigma_{h}^{2}) \\, dh\n$$\n\nTo solve this integral, we can represent $\\Phi(h)$ as a probability statement. Let $z \\sim \\mathcal{N}(0, 1)$ be a standard normal variable, independent of $h$. Then $\\Phi(h) = P(z \\le h) = P(z - h \\le 0)$. The integral thus becomes the probability $P(z-h \\le 0)$ where the probability is taken over the joint distribution of $z$ and $h$.\n\nThe difference of two independent Gaussian variables is also a Gaussian variable. Let $d = h - z$.\nThe mean of $d$ is $\\mathbb{E}[d] = \\mathbb{E}[h] - \\mathbb{E}[z] = \\mu_{h} - 0 = \\mu_{h}$.\nThe variance of $d$ is $\\mathrm{Var}[d] = \\mathrm{Var}[h] + \\mathrm{Var}[-z] = \\mathrm{Var}[h] + (-1)^2 \\mathrm{Var}[z] = \\sigma_{h}^{2} + 1$.\nThus, $d \\sim \\mathcal{N}(\\mu_{h}, 1 + \\sigma_{h}^{2})$.\n\nThe probability of feasibility is $P(h \\ge z) = P(h-z \\ge 0) = P(d \\ge 0)$. For a Gaussian variable $d$, this is:\n$$\nP(d \\ge 0) = 1 - P(d < 0) = 1 - \\Phi\\left(\\frac{0 - \\mu_{h}}{\\sqrt{1+\\sigma_{h}^{2}}}\\right) = 1 - \\Phi\\left(-\\frac{\\mu_{h}}{\\sqrt{1+\\sigma_{h}^{2}}}\\right)\n$$\nUsing the identity $\\Phi(-x) = 1 - \\Phi(x)$, we arrive at the final expression:\n$$\n\\mathrm{PF}(x_{\\mathrm{c}}) = \\Phi\\left(\\frac{\\mu_{h}}{\\sqrt{1+\\sigma_{h}^{2}}}\\right)\n$$\n\nNow, we substitute the given numerical values: $\\mu_{h} = 0.70$ and $\\sigma_{h}^{2} = 0.36$.\nThe argument of $\\Phi$ is:\n$$\n\\frac{0.70}{\\sqrt{1+0.36}} = \\frac{0.70}{\\sqrt{1.36}} \\approx \\frac{0.70}{1.166190} \\approx 0.5999285\n$$\nSo, $\\mathrm{PF}(x_{\\mathrm{c}}) = \\Phi(0.5999285)$. Using a standard normal CDF calculator, this is:\n$$\n\\mathrm{PF}(x_{\\mathrm{c}}) \\approx 0.725722\n$$\n\n**2. Derivation and Calculation of Expected Improvement, $\\mathrm{EI}(x_{\\mathrm{c}})$**\n\nThe improvement function, $I(f)$, is defined as the improvement over the current best observed activity, $f^{\\star}$, given by $I(f) = \\max(0, f - f^{\\star})$.\nThe Expected Improvement, $\\mathrm{EI}(x_{\\mathrm{c}})$, is the expectation of $I(f)$ under the GP posterior predictive distribution for the activity $f(x_{\\mathrm{c}})$, which is given as a Gaussian: $f(x_{\\mathrm{c}}) \\sim \\mathcal{N}(f; \\mu_{f}, \\sigma_{f}^{2})$.\n\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) = \\mathbb{E}_{f \\sim \\mathcal{N}(\\mu_f, \\sigma_f^2)}[\\max(0, f - f^{\\star})] = \\int_{-\\infty}^{\\infty} \\max(0, f - f^{\\star}) \\, p(f) \\, df\n$$\nSince the improvement is non-zero only when $f > f^{\\star}$, the integral becomes:\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) = \\int_{f^{\\star}}^{\\infty} (f - f^{\\star}) \\, \\frac{1}{\\sigma_f\\sqrt{2\\pi}} \\exp\\left(-\\frac{(f-\\mu_f)^2}{2\\sigma_f^2}\\right) df\n$$\nLet's perform a change of variables to $z = \\frac{f - \\mu_f}{\\sigma_f}$. Then $f = \\mu_f + z\\sigma_f$ and $df = \\sigma_f dz$. The lower limit of integration becomes $\\frac{f^{\\star}-\\mu_f}{\\sigma_f}$.\nThe integral transforms to:\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) = \\int_{\\frac{f^{\\star}-\\mu_f}{\\sigma_f}}^{\\infty} (\\mu_f + z\\sigma_f - f^{\\star}) \\, \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz\n$$\nLet's define a standardized improvement $\\gamma = \\frac{\\mu_f - f^{\\star}}{\\sigma_f}$. The lower limit of integration is $-\\gamma$.\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) = \\int_{-\\gamma}^{\\infty} [(\\mu_f - f^{\\star}) + z\\sigma_f] \\, \\phi(z) \\, dz\n$$\nwhere $\\phi(z)$ is the standard normal probability density function (PDF). Splitting the integral:\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) = (\\mu_f - f^{\\star}) \\int_{-\\gamma}^{\\infty} \\phi(z) \\, dz + \\sigma_f \\int_{-\\gamma}^{\\infty} z \\, \\phi(z) \\, dz\n$$\nThe first integral is $1 - \\Phi(-\\gamma) = \\Phi(\\gamma)$.\nThe second integral is $\\int_{-\\gamma}^{\\infty} z \\, \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right) dz$. This evaluates to $\\left[-\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)\\right]_{-\\gamma}^{\\infty} = 0 - \\left(-\\phi(-\\gamma)\\right) = \\phi(-\\gamma) = \\phi(\\gamma)$, since $\\phi(z)$ is an even function.\nCombining the terms, we get the standard formula for EI:\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) = (\\mu_f - f^{\\star})\\Phi(\\gamma) + \\sigma_f\\phi(\\gamma) \\quad \\text{where} \\quad \\gamma = \\frac{\\mu_f - f^{\\star}}{\\sigma_f}\n$$\nNow, substitute the given numerical values: $\\mu_{f} = 1.05$, $\\sigma_{f} = 0.20$, and $f^{\\star} = 0.95$.\n$$\n\\gamma = \\frac{1.05 - 0.95}{0.20} = \\frac{0.10}{0.20} = 0.5\n$$\nWe need to calculate $\\Phi(0.5)$ and $\\phi(0.5)$.\n$$\n\\Phi(0.5) \\approx 0.691462\n$$\n$$\n\\phi(0.5) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{0.5^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-0.125) \\approx 0.352065\n$$\nSubstituting these into the EI formula:\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) = (1.05 - 0.95)\\Phi(0.5) + 0.20 \\cdot \\phi(0.5)\n$$\n$$\n\\mathrm{EI}(x_{\\mathrm{c}}) \\approx (0.10)(0.691462) + (0.20)(0.352065) \\approx 0.0691462 + 0.0704130 \\approx 0.139559\n$$\n\n**3. Calculation of the Constrained Acquisition Value, $a_{\\mathrm{c}}(x_{\\mathrm{c}})$**\n\nThe constrained acquisition value is the product of the probability of feasibility and the expected improvement, under the assumption that the posteriors for activity and feasibility are independent.\n$$\na_{\\mathrm{c}}(x_{\\mathrm{c}}) = \\mathrm{PF}(x_{\\mathrm{c}}) \\times \\mathrm{EI}(x_{\\mathrm{c}})\n$$\nUsing the calculated values:\n$$\na_{\\mathrm{c}}(x_{\\mathrm{c}}) \\approx 0.725722 \\times 0.139559 \\approx 0.1012895\n$$\nThe problem requires rounding the final value to four significant figures.\n$$\na_{\\mathrm{c}}(x_{\\mathrm{c}}) \\approx 0.1013\n$$\nThis quantity is dimensionless as it is a product of a probability ($\\mathrm{PF}$) and a quantity with the same units as the objective function ($\\mathrm{EI}$), and the objective function (normalized activity) is itself dimensionless.",
            "answer": "$$\n\\boxed{0.1013}\n$$"
        }
    ]
}