{
    "hands_on_practices": [
        {
            "introduction": "When solving large linear systems with iterative methods like GMRES, we rarely obtain the exact solution. Instead, we generate an approximation and measure its quality using the residual. This practice explores backward error analysis, a powerful concept that reframes the question from 'how close is our answer to the true answer?' to 'our answer is the exact solution to what nearby problem?' . By deriving and computing the minimal perturbation to the system operator, you will gain a deeper, more rigorous understanding of what the residual norm truly represents in the context of solution stability and accuracy.",
            "id": "4000459",
            "problem": "A magnetohydrodynamics (MHD) equilibrium calculation for a tokamak edge region leads, after spatial discretization of the linearized Grad–Shafranov operator, to a nonsymmetric linear system $A x = b$ that is solved using the Generalized Minimal Residual method (GMRES) with a right preconditioner $M$ chosen as an incomplete lower–upper factorization (ILU). At GMRES iteration $k$, the algorithm has produced an approximate solution $x_{k}$ and a residual $r_{k} = b - A x_{k}$. In backward error analysis, one interprets the computed $x_{k}$ as the exact solution to a nearby problem, typically by perturbing the operator to $(A + \\Delta A) x_{k} = b$.\n\nExplain the concept of backward error for linear systems in the context of this right-preconditioned Krylov subspace method, starting from the definitions of the residual $r_{k}$, vector $2$-norm, and matrix $2$-norm. Then, by deriving from first principles and without invoking prepackaged formulas, compute the minimal perturbation $\\Delta A$ (with respect to the matrix $2$-norm) that satisfies $(A + \\Delta A) x_{k} = b$ for the following data:\n- The iterate $x_{k} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$,\n- The residual $r_{k} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$,\n- The operator $A = \\begin{pmatrix} 10 & -2 & 0 \\\\ -2 & 5 & 1 \\\\ 0 & 1 & 3 \\end{pmatrix}$, and the right-hand side $b$ defined consistently by $b = A x_{k} + r_{k}$.\n\nProvide the explicit matrix $\\Delta A$ in closed form so that $(A + \\Delta A) x_{k} = b$ holds. No rounding is required. Express your final answer as a single explicit matrix.",
            "solution": "The problem asks for an explanation of backward error in the context of a right-preconditioned Krylov subspace method and for the computation of the minimal perturbation matrix $\\Delta A$ for a given linear system.\n\nFirst, we address the concept of backward error for an approximate solution to a linear system. Let the linear system be given by\n$$A x = b$$\nwhere $A$ is an $n \\times n$ matrix, and $x$ and $b$ are $n$-dimensional column vectors. An iterative method, such as the Generalized Minimal Residual method (GMRES), generates a sequence of approximate solutions $\\{x_k\\}$. For a given approximate solution $x_k$, the corresponding residual is defined as\n$$r_k = b - A x_k$$\nIf $x_k$ is the exact solution, then $r_k = 0$. Otherwise, $r_k$ is a non-zero vector that measures how well $x_k$ satisfies the equation.\n\nBackward error analysis interprets the inexact solution $x_k$ not as an approximation to the true solution $x$, but as the exact solution to a perturbed problem. We seek a perturbation $\\Delta A$ to the matrix $A$ such that $x_k$ is the exact solution of the new system:\n$$(A + \\Delta A) x_k = b$$\nExpanding this equation yields $A x_k + \\Delta A x_k = b$. Rearranging and using the definition of the residual, we obtain the fundamental condition that the perturbation matrix $\\Delta A$ must satisfy:\n$$\\Delta A x_k = b - A x_k = r_k$$\nThis is a system of linear equations for the entries of the matrix $\\Delta A$. In general, for $n > 1$, this system is underdetermined, and there are infinitely many matrices $\\Delta A$ that satisfy this condition. To obtain a unique and meaningful answer, we seek the \"smallest\" such $\\Delta A$. The size of the matrix is measured using a matrix norm. The problem specifies the matrix $2$-norm, also known as the spectral norm, which is defined as:\n$$\\|\\Delta A\\|_2 = \\sup_{z \\neq 0} \\frac{\\|\\Delta A z\\|_2}{\\|z\\|_2}$$\nwhere $\\| \\cdot \\|_2$ in the fraction denotes the Euclidean vector $2$-norm. The problem is thus to find the matrix $\\Delta A$ that solves the minimization problem:\n$$\\min_{\\Delta A} \\|\\Delta A\\|_2 \\quad \\text{subject to} \\quad \\Delta A x_k = r_k$$\n\nThe context of a right-preconditioned GMRES informs how the approximate solution $x_k$ is generated. The original system $Ax=b$ is transformed into an equivalent system by introducing a preconditioner $M$. For right preconditioning, we solve\n$$(A M^{-1}) y = b$$\nand then recover the solution $x$ via $x = M^{-1} y$. GMRES, when applied to this preconditioned system, finds an approximate solution $y_k$ that minimizes the $2$-norm of the residual $\\|b - (AM^{-1})y_k\\|_2$ over a certain Krylov subspace. The corresponding approximate solution for the original problem is $x_k = M^{-1} y_k$. The residual for the original system at this iterate is $r_k = b - A x_k = b - A(M^{-1} y_k) = b - (AM^{-1})y_k$. This is precisely the residual vector whose norm is minimized by the GMRES algorithm. Thus, right-preconditioned GMRES is designed to produce iterates $x_k$ with small residuals $\\|r_k\\|_2$, which, as we will see, corresponds to a small backward error.\n\nNow, we derive the formula for the minimal perturbation $\\Delta A$. For any matrix $\\Delta A$ that satisfies the condition $\\Delta A x_k = r_k$, its $2$-norm must satisfy the following inequality:\n$$\\|\\Delta A\\|_2 = \\sup_{z \\neq 0} \\frac{\\|\\Delta A z\\|_2}{\\|z\\|_2} \\ge \\frac{\\|\\Delta A x_k\\|_2}{\\|x_k\\|_2} = \\frac{\\|r_k\\|_2}{\\|x_k\\|_2}$$\nThis establishes a lower bound on the $2$-norm of any valid perturbation. A minimal perturbation would be one whose norm equals this lower bound.\n\nLet us construct a matrix $\\Delta A$ that achieves this bound. Consider the rank-one matrix formed by the outer product of $r_k$ and $x_k$:\n$$\\Delta A = \\frac{r_k x_k^T}{x_k^T x_k}$$\nNote that $x_k^T x_k = \\|x_k\\|_2^2$. First, we verify that this matrix satisfies the constraint:\n$$\\Delta A x_k = \\left(\\frac{r_k x_k^T}{\\|x_k\\|_2^2}\\right) x_k = \\frac{r_k (x_k^T x_k)}{\\|x_k\\|_2^2} = r_k$$\nThe condition is satisfied. Next, we compute its $2$-norm. For a rank-one matrix of the form $u v^T$, its $2$-norm is $\\|u v^T\\|_2 = \\|u\\|_2 \\|v\\|_2$. For our proposed $\\Delta A$, we can identify $u=r_k$ and $v = \\frac{x_k}{\\|x_k\\|_2^2}$. The norm is:\n$$\\|\\Delta A\\|_2 = \\left\\|r_k \\left(\\frac{x_k^T}{\\|x_k\\|_2^2}\\right)\\right\\|_2 = \\|r_k\\|_2 \\left\\|\\frac{x_k}{\\|x_k\\|_2^2}\\right\\|_2 = \\|r_k\\|_2 \\frac{\\|x_k\\|_2}{\\|x_k\\|_2^2} = \\frac{\\|r_k\\|_2}{\\|x_k\\|_2}$$\nSince the norm of this specific matrix $\\Delta A$ equals the derived lower bound, it is indeed the solution to the minimization problem. It is the minimal perturbation with respect to the matrix $2$-norm.\n\nWe now apply this formula to the data provided in the problem.\nThe given vectors are:\n$$x_{k} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad r_{k} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$$\nFirst, we compute the squared $2$-norm of $x_k$:\n$$\\|x_k\\|_2^2 = x_k^T x_k = (2)^2 + (-1)^2 + (3)^2 = 4 + 1 + 9 = 14$$\nNext, we compute the outer product $r_k x_k^T$:\n$$r_k x_k^T = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & 3 \\end{pmatrix} = \\begin{pmatrix} (1)(2) & (1)(-1) & (1)(3) \\\\ (-2)(2) & (-2)(-1) & (-2)(3) \\\\ (0)(2) & (0)(-1) & (0)(3) \\end{pmatrix} = \\begin{pmatrix} 2 & -1 & 3 \\\\ -4 & 2 & -6 \\\\ 0 & 0 & 0 \\end{pmatrix}$$\nThe minimal perturbation matrix $\\Delta A$ is this outer product divided by $\\|x_k\\|_2^2$:\n$$\\Delta A = \\frac{r_k x_k^T}{\\|x_k\\|_2^2} = \\frac{1}{14} \\begin{pmatrix} 2 & -1 & 3 \\\\ -4 & 2 & -6 \\\\ 0 & 0 & 0 \\end{pmatrix}$$\nFinally, we write the explicit matrix by distributing the scalar factor:\n$$\\Delta A = \\begin{pmatrix} \\frac{2}{14} & -\\frac{1}{14} & \\frac{3}{14} \\\\ -\\frac{4}{14} & \\frac{2}{14} & -\\frac{6}{14} \\\\ \\frac{0}{14} & \\frac{0}{14} & \\frac{0}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{7} & -\\frac{1}{14} & \\frac{3}{14} \\\\ -\\frac{2}{7} & \\frac{1}{7} & -\\frac{3}{7} \\\\ 0 & 0 & 0 \\end{pmatrix}$$\nThis is the required minimal perturbation matrix $\\Delta A$ such that $(A+\\Delta A)x_k=b$ is satisfied.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{7} & -\\frac{1}{14} & \\frac{3}{14} \\\\ -\\frac{2}{7} & \\frac{1}{7} & -\\frac{3}{7} \\\\ 0 & 0 & 0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "The GMRES algorithm is a cornerstone for solving non-symmetric linear systems, but its memory and computational costs grow with each iteration. The restarted version, GMRES($m$), offers a practical compromise by limiting the size of the Krylov subspace, but this requires choosing the restart parameter $m$ judiciously. This exercise guides you through the process of selecting an optimal $m$ by balancing theoretical convergence guarantees, derived from the spectral properties of the preconditioned operator, against the finite memory resources of a computational node . This is a critical skill for tackling large-scale simulations in fusion science.",
            "id": "4000471",
            "problem": "A resistive magnetohydrodynamics (MHD) implicit time-advancement for a tokamak edge simulation leads, at each Newton step, to a large non-symmetric linear system $A x = b$. A right preconditioning operator $M^{-1}$ based on an Incomplete Lower-Upper factorization (ILU) is applied, so the Krylov method sees the operator $M^{-1}A$. The Generalized Minimal Residual (GMRES) method with restart, denoted $\\mathrm{GMRES}(m)$, is used. The preconditioned operator $M^{-1}A$ is diagonalizable with real spectrum tightly clustered by physics-based preconditioning: all eigenvalues lie in the real interval $[\\alpha, \\beta]$ with $\\alpha = 0.85$ and $\\beta = 1.15$. Assume double-precision real arithmetic.\n\nThe discrete system has $n = 2.0 \\times 10^{7}$ unknowns. The total node memory is $64$ gibibytes (GiB). Of this, $20$ GiB is used to store the preconditioner $M$, $8$ GiB to store the sparse matrix $A$, and $4$ GiB reserved for operating system and other non-iterative overhead. The remainder is available for the $\\mathrm{GMRES}(m)$ workspace. The $\\mathrm{GMRES}(m)$ implementation stores $(m+1)$ Arnoldi basis vectors of length $n$, two additional work vectors of length $n$ for preconditioning and residual updates, and the upper Hessenberg matrix of size $(m+1) \\times m$ in double precision.\n\nYou must choose the restart length $m$ to meet a target relative residual reduction of $10^{-8}$ in a single restart cycle, while respecting the memory constraint. Use the following foundational facts:\n\n- The $\\mathrm{GMRES}(m)$ residual after $m$ steps can be bounded in terms of a degree-$m$ polynomial $p$ with $p(0) = 1$ evaluated on the spectrum of $M^{-1}A$, namely $\\|r_{m}\\| \\leq \\min_{p \\in \\mathcal{P}_{m},\\, p(0) = 1} \\max_{\\lambda \\in \\Lambda(M^{-1}A)} |p(\\lambda)| \\, \\|r_{0}\\|$.\n- For real spectra contained in an interval $[\\alpha, \\beta]$ not containing $0$, the minimizing polynomial is obtained by mapping the Chebyshev polynomial of the first kind $T_{m}$ from $[-1,1]$ to $[\\alpha, \\beta]$, yielding the bound $\\max_{\\lambda \\in [\\alpha,\\beta]} |p(\\lambda)| \\leq \\left| T_{m}\\!\\left(\\frac{-c}{d}\\right) \\right|^{-1}$, where $c = (\\alpha + \\beta)/2$ and $d = (\\beta - \\alpha)/2$.\n- For $|x| \\geq 1$, $T_{m}(x) = \\cosh\\!\\left(m \\, \\operatorname{arccosh}(x)\\right)$ and $\\operatorname{arccosh}(x) = \\ln\\!\\big(x + \\sqrt{x^{2} - 1}\\big)$.\n\nAssume the initial residual is normalized so that the target reduction factor is achieved when $\\max_{\\lambda \\in [\\alpha,\\beta]} |p(\\lambda)| \\leq 10^{-8}$ in one restart cycle. Compute the optimal integer restart length $m$ that meets the target in one cycle and does not exceed the available memory. The final answer must be the integer value of $m$. Do not round; provide the exact integer that satisfies the constraints.",
            "solution": "The problem requires determining the optimal integer restart length, denoted by $m$, for the $\\mathrm{GMRES}(m)$ algorithm. The value of $m$ must satisfy two constraints: a convergence criterion and a memory capacity limit. The optimal value is the smallest integer $m$ that satisfies the convergence criterion while also respecting the memory constraint, as this minimizes the computational work and storage for the restart cycle.\n\nFirst, we analyze the convergence constraint. The problem states that the preconditioned operator $M^{-1}A$ has a real spectrum contained entirely within the interval $[\\alpha, \\beta]$, where $\\alpha = 0.85$ and $\\beta = 1.15$. The target is to achieve a relative residual reduction of $10^{-8}$ in a single restart cycle of $m$ steps. This is formulated as the condition on the convergence factor:\n$$\n\\max_{\\lambda \\in [\\alpha,\\beta]} |p(\\lambda)| \\leq 10^{-8}\n$$\nwhere $p(\\lambda)$ is a polynomial of degree $m$ with $p(0)=1$. The problem provides the standard bound for this case, which involves the Chebyshev polynomial of the first kind, $T_m$. The bound on the residual reduction factor is given by:\n$$\n\\rho_m = \\left| T_{m}\\!\\left(\\frac{-c}{d}\\right) \\right|^{-1}\n$$\nwhere $c = (\\alpha + \\beta)/2$ and $d = (\\beta - \\alpha)/2$. We need to find the smallest integer $m$ for which $\\rho_m \\le 10^{-8}$.\n\nLet's compute $c$ and $d$:\n$$\nc = \\frac{0.85 + 1.15}{2} = \\frac{2.0}{2} = 1.0\n$$\n$$\nd = \\frac{1.15 - 0.85}{2} = \\frac{0.30}{2} = 0.15\n$$\nThe argument of the Chebyshev polynomial is $\\frac{-c}{d} = \\frac{-1.0}{0.15} = -\\frac{20}{3}$.\nThe condition $\\rho_m \\le 10^{-8}$ becomes:\n$$\n\\left| T_{m}\\!\\left(-\\frac{20}{3}\\right) \\right|^{-1} \\le 10^{-8} \\implies \\left| T_{m}\\!\\left(-\\frac{20}{3}\\right) \\right| \\ge 10^{8}\n$$\nSince $T_m(-x) = (-1)^m T_m(x)$, we have $|T_m(-x)| = |T_m(x)|$. The inequality is equivalent to:\n$$\nT_{m}\\!\\left(\\frac{20}{3}\\right) \\ge 10^{8}\n$$\nThe argument $x_0 = 20/3$ is greater than $1$, so we can use the identity $T_m(x) = \\cosh(m \\cdot \\operatorname{arccosh}(x))$:\n$$\n\\cosh\\left(m \\cdot \\operatorname{arccosh}\\left(\\frac{20}{3}\\right)\\right) \\ge 10^{8}\n$$\nApplying the inverse hyperbolic cosine function $\\operatorname{arccosh}$ to both sides, which is monotonically increasing for arguments $\\ge 1$, we get:\n$$\nm \\cdot \\operatorname{arccosh}\\left(\\frac{20}{3}\\right) \\ge \\operatorname{arccosh}(10^8)\n$$\nSolving for $m$:\n$$\nm \\ge \\frac{\\operatorname{arccosh}(10^8)}{\\operatorname{arccosh}(20/3)}\n$$\nWe use the logarithmic formula $\\operatorname{arccosh}(x) = \\ln(x + \\sqrt{x^2-1})$ to evaluate the terms:\n$$\n\\operatorname{arccosh}\\left(\\frac{20}{3}\\right) = \\ln\\left(\\frac{20}{3} + \\sqrt{\\left(\\frac{20}{3}\\right)^2 - 1}\\right) = \\ln\\left(\\frac{20}{3} + \\sqrt{\\frac{400}{9} - \\frac{9}{9}}\\right) = \\ln\\left(\\frac{20 + \\sqrt{391}}{3}\\right)\n$$\n$$\n\\operatorname{arccosh}(10^8) = \\ln(10^8 + \\sqrt{(10^8)^2 - 1}) = \\ln(10^8 + \\sqrt{10^{16}-1})\n$$\nNumerically, $\\operatorname{arccosh}(10^8) \\approx 19.1138$ and $\\operatorname{arccosh}(20/3) \\approx 2.5846$.\n$$\nm \\ge \\frac{19.1138...}{2.5846...} \\approx 7.3953...\n$$\nSince $m$ must be an integer, the smallest value that satisfies the convergence criterion is $m_{conv} = 8$.\n\nNext, we analyze the memory constraint.\nTotal node memory is $64$ gibibytes (GiB), where $1\\,\\mathrm{GiB} = 2^{30}$ bytes.\nTotal memory $= 64 \\times 2^{30}$ bytes.\nMemory used for $M$ is 20 GiB, for $A$ is 8 GiB, and for the OS is 4 GiB.\nTotal fixed memory usage $= (20 + 8 + 4)\\,\\mathrm{GiB} = 32\\,\\mathrm{GiB}$.\nAvailable memory for the $\\mathrm{GMRES}(m)$ workspace is $(64 - 32)\\,\\mathrm{GiB} = 32\\,\\mathrm{GiB} = 32 \\times 2^{30}$ bytes.\n\nThe memory for the $\\mathrm{GMRES}(m)$ workspace is used for:\n1. $(m+1)$ Arnoldi basis vectors of length $n$.\n2. two additional work vectors of length $n$.\n3. an upper Hessenberg matrix of size $(m+1) \\times m$.\nAll data is in double precision, which uses $8$ bytes per number. The system size is $n = 2.0 \\times 10^7$.\nTotal number of vectors is $(m+1) + 2 = m+3$.\nMemory for vectors is $(m+3) \\times n \\times 8$ bytes.\nMemory for the Hessenberg matrix is $(m+1) \\times m \\times 8$ bytes.\nTotal workspace memory is $\\left[ (m+3)n + m(m+1) \\right] \\times 8$ bytes.\nThis must be less than or equal to the available memory:\n$$\n8 \\left[ (m+3)n + m(m+1) \\right] \\le 32 \\times 2^{30}\n$$\n$$\n(m+3)n + m(m+1) \\le 4 \\times 2^{30} = 4,294,967,296\n$$\nSubstituting $n = 2 \\times 10^7$:\n$$\n(m+3)(2 \\times 10^7) + m^2 + m \\le 4,294,967,296\n$$\n$$\nm^2 + (20,000,001) m + 60,000,000 \\le 4,294,967,296\n$$\n$$\nm^2 + 20,000,001 m - 4,234,967,296 \\le 0\n$$\nThe term $m^2$ is negligible compared to the linear term in $m$. We can approximate the maximum value of $m$ by ignoring the quadratic term:\n$$\nm \\approx \\frac{4,234,967,296}{20,000,001} \\approx 211.75\n$$\nThis suggests the maximum integer value for $m$ is $211$. We verify this by testing $m=211$ and $m=212$ in the inequality.\nFor $m=211$:\n$$\n(211+3)(2 \\times 10^7) + 211(211+1) = 214 \\times (2 \\times 10^7) + 44,732 = 4,280,000,000 + 44,732 = 4,280,044,732\n$$\nSince $4,280,044,732 \\le 4,294,967,296$, $m=211$ is a valid choice.\nFor $m=212$:\n$$\n(212+3)(2 \\times 10^7) + 212(212+1) = 215 \\times (2 \\times 10^7) + 45,156 = 4,300,000,000 + 45,156 = 4,300,045,156\n$$\nSince $4,300,045,156 > 4,294,967,296$, $m=212$ is not a valid choice.\nThus, the maximum restart length allowed by the memory constraint is $m_{mem} = 211$.\n\nFinally, we must choose the optimal $m$. The convergence requires $m \\ge 8$, and memory allows $m \\le 211$. Any integer $m$ in the range $[8, 211]$ is a valid choice. The \"optimal\" choice is the one that minimizes the computational cost. The work per restart cycle, particularly the orthogonalization cost, increases with $m$ (quadratically, in fact). Therefore, the smallest value of $m$ that guarantees convergence is the most efficient choice.\nThe optimal restart length is $m = m_{conv} = 8$. This value satisfies the memory constraint, as $8 \\le 211$.",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "Many critical problems in fusion science, such as those in incompressible MHD, lead to coupled multi-field equations that take on a block saddle-point structure. Directly inverting these block operators is often computationally prohibitive, making the design of an effective preconditioner essential. This practice delves into the art of physics-based preconditioning, where you will derive a block factorization, identify the crucial Schur complement, and use physical reasoning about dominant effects to construct a simplified, invertible approximation . Mastering this technique allows you to transform an intractable problem into one that can be solved efficiently with Krylov methods.",
            "id": "4000472",
            "problem": "In incompressible resistive Magnetohydrodynamics (MHD), consider a linearized, semi-implicit time discretization over a time step of size $\\Delta t$ with constant density $\\rho$ and kinematic viscosity $\\nu$. The discrete velocity-pressure subproblem that enforces incompressibility assumes the saddle-point form with a $2\\times 2$ block operator\n$$\n\\mathcal{K} \\equiv \\begin{pmatrix} A & B^{\\top} \\\\ B & -C \\end{pmatrix},\n$$\nwhere $A$ denotes the discrete momentum block, $B$ is the discrete divergence operator, and $C$ is a symmetric positive semidefinite contribution arising from stabilization or coupling effects. For incompressible resistive MHD in the limit of strong enforcement of incompressibility, you may take $C = 0$ for the purposes of this problem.\n\nStarting from the block operator $\\mathcal{K}$:\n1) Derive the exact block factorization of $\\mathcal{K}$ and identify the exact Schur complement in terms of $A$, $B$, and $C$.\n2) Using physically justified asymptotics for resistive MHD timestepping at Alfvénic scales where the time step is small relative to viscous diffusion time scales, propose the approximations $\\widehat{A}$ and $\\widehat{S}$ that would be used to build a block lower-triangular preconditioner. Justify these approximations using only fundamental modeling considerations of term dominance and operator scaling in the semi-implicit discretization.\n\nNext, consider a single homogeneous Fourier mode on a periodic domain with wavenumber magnitude $k$. In this mode-by-mode analysis, take the symbols of the operators as follows: replace $A$ by the scalar\n$$\na \\equiv \\frac{\\rho}{\\Delta t} + \\nu k^{2},\n$$\nreplace $B$ and $B^{\\top}$ by the real scalar\n$$\nb \\equiv k,\n$$\nand set $C=0$. Adopt the mass-dominated approximations\n$$\n\\widehat{A} \\equiv a_{m} \\equiv \\frac{\\rho}{\\Delta t}, \\qquad \\widehat{S} \\equiv \\frac{b^{2}}{a_{m}}.\n$$\nConstruct the block lower-triangular preconditioner\n$$\n\\widehat{\\mathcal{M}} \\equiv \\begin{pmatrix} \\widehat{A} & 0 \\\\ B & -\\widehat{S} \\end{pmatrix},\n$$\nand derive a closed-form expression for the spectral radius of the preconditioned operator $\\widehat{\\mathcal{M}}^{-1} \\mathcal{K}$ for the single-mode scalar reduction described above.\n\nFinally, evaluate your expression for the parameter values $\\rho = 1$, $\\nu = 1$, $\\Delta t = 1$, and $k = 1$. Express your final spectral radius as a single exact analytic expression. Do not include units and do not round. The final answer must be a single expression.",
            "solution": "First, we derive the exact block factorization of the operator $\\mathcal{K}$ and identify the Schur complement. The given operator is\n$$\n\\mathcal{K} \\equiv \\begin{pmatrix} A & B^{\\top} \\\\ B & -C \\end{pmatrix}\n$$\nAssuming the block $A$ is invertible, we can perform block Gaussian elimination. We seek a factorization of the form $\\mathcal{K} = L U$, where $L$ is a block lower-triangular matrix with identity blocks on the diagonal and $U$ is a block upper-triangular matrix.\n$$\n\\mathcal{K} = \\begin{pmatrix} I & 0 \\\\ B A^{-1} & I \\end{pmatrix} \\begin{pmatrix} A & B^{\\top} \\\\ 0 & S \\end{pmatrix}\n$$\nMultiplying the factors on the right-hand side gives\n$$\n\\begin{pmatrix} I & 0 \\\\ B A^{-1} & I \\end{pmatrix} \\begin{pmatrix} A & B^{\\top} \\\\ 0 & S \\end{pmatrix} = \\begin{pmatrix} A & B^{\\top} \\\\ (B A^{-1})A & (B A^{-1})B^{\\top} + S \\end{pmatrix} = \\begin{pmatrix} A & B^{\\top} \\\\ B & B A^{-1} B^{\\top} + S \\end{pmatrix}\n$$\nComparing this to the original operator $\\mathcal{K}$, we must have $-C = B A^{-1} B^{\\top} + S$.\nThis leads to the expression for the exact Schur complement of $A$ in $\\mathcal{K}$:\n$$\nS = -C - B A^{-1} B^{\\top}\n$$\nFor the specific case where $C=0$, the Schur complement simplifies to $S = -B A^{-1} B^{\\top}$.\n\nSecond, we provide a physical justification for the approximations $\\widehat{A}$ and $\\widehat{S}$. The discrete momentum block $A$ represents the operator $\\frac{\\rho}{\\Delta t}I - \\nu \\nabla^2$ (among other possible terms not specified). Its symbol in Fourier space for a mode with wavenumber $k$ is $a = \\frac{\\rho}{\\Delta t} + \\nu k^2$. The problem states that the time step $\\Delta t$ is small relative to viscous diffusion time scales. The viscous diffusion time scale is $\\tau_{\\nu} \\sim (\\nu k^2)^{-1}$. The condition $\\Delta t \\ll \\tau_{\\nu}$ implies $\\Delta t \\ll (\\nu k^2)^{-1}$, which can be rewritten as $\\nu k^2 \\Delta t \\ll 1$.\nThe ratio of the magnitudes of the viscous term to the inertial term in the symbol $a$ is:\n$$\n\\frac{\\nu k^2}{\\rho / \\Delta t} = \\frac{\\nu k^2 \\Delta t}{\\rho}\n$$\nGiven that $\\nu k^2 \\Delta t \\ll 1$ and $\\rho$ is of order unity in typical non-dimensionalizations, this ratio is much less than $1$. Therefore, the inertial term dominates: $\\frac{\\rho}{\\Delta t} \\gg \\nu k^2$. This justifies approximating the operator $A$ by its mass-dominated part:\n$$\nA \\approx \\widehat{A} \\equiv \\frac{\\rho}{\\Delta t} I\n$$\nThe ideal preconditioner's lower-right block, $-\\widehat{S}$, should approximate the true Schur complement $S = -B A^{-1} B^{\\top}$ (since $C=0$). This implies that $\\widehat{S}$ should approximate $-S = B A^{-1} B^{\\top}$. A common and effective strategy is to form the approximate Schur complement by substituting the approximation for $A$ into this expression:\n$$\n\\widehat{S} \\approx B \\widehat{A}^{-1} B^{\\top} = B \\left(\\frac{\\rho}{\\Delta t} I\\right)^{-1} B^{\\top} = \\frac{\\Delta t}{\\rho} B B^{\\top}\n$$\nThis corresponds precisely to the scalar definition $\\widehat{S} \\equiv b^2/a_m$ provided in the problem, since for the scalar case $b^2/a_m = k^2 / (\\rho/\\Delta t) = \\frac{k^2 \\Delta t}{\\rho}$.\n\nThird, we derive the spectral radius of the preconditioned operator $\\widehat{\\mathcal{M}}^{-1}\\mathcal{K}$ for the scalar case.\nThe operators become scalars or $2 \\times 2$ matrices with scalar entries:\n$$\n\\mathcal{K} = \\begin{pmatrix} a & b \\\\ b & 0 \\end{pmatrix}, \\quad \\text{with } a = \\frac{\\rho}{\\Delta t} + \\nu k^{2} \\text{ and } b = k.\n$$\nThe preconditioner is:\n$$\n\\widehat{\\mathcal{M}} = \\begin{pmatrix} \\widehat{a} & 0 \\\\ b & -\\widehat{s} \\end{pmatrix}, \\quad \\text{with } \\widehat{a} = \\frac{\\rho}{\\Delta t} \\text{ and } \\widehat{s} = \\frac{b^2}{\\widehat{a}} = \\frac{k^2 \\Delta t}{\\rho}.\n$$\nThe eigenvalues $\\lambda$ of the preconditioned operator $\\widehat{\\mathcal{M}}^{-1}\\mathcal{K}$ are found by solving the generalized eigenvalue problem $\\mathcal{K}v = \\lambda \\widehat{\\mathcal{M}}v$, which is equivalent to finding the roots of $\\det(\\mathcal{K} - \\lambda \\widehat{\\mathcal{M}}) = 0$.\n$$\n\\mathcal{K} - \\lambda \\widehat{\\mathcal{M}} = \\begin{pmatrix} a & b \\\\ b & 0 \\end{pmatrix} - \\lambda \\begin{pmatrix} \\widehat{a} & 0 \\\\ b & -\\widehat{s} \\end{pmatrix} = \\begin{pmatrix} a - \\lambda\\widehat{a} & b \\\\ b - \\lambda b & \\lambda\\widehat{s} \\end{pmatrix} = \\begin{pmatrix} a - \\lambda\\widehat{a} & b \\\\ b(1-\\lambda) & \\lambda\\widehat{s} \\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(\\mathcal{K} - \\lambda \\widehat{\\mathcal{M}}) = (a - \\lambda\\widehat{a})(\\lambda\\widehat{s}) - b(b(1-\\lambda)) = a\\lambda\\widehat{s} - \\lambda^2\\widehat{a}\\widehat{s} - b^2 + \\lambda b^2 = 0\n$$\nRearranging this into a quadratic equation for $\\lambda$:\n$$\n(\\widehat{a}\\widehat{s})\\lambda^2 - (a\\widehat{s} + b^2)\\lambda + b^2 = 0\n$$\nWe compute the coefficients using their definitions:\n- Coefficient of $\\lambda^2$: $\\widehat{a}\\widehat{s} = \\left(\\frac{\\rho}{\\Delta t}\\right) \\left(\\frac{k^2 \\Delta t}{\\rho}\\right) = k^2$.\n- Coefficient of $\\lambda$: $-(a\\widehat{s} + b^2) = -\\left(\\left(\\frac{\\rho}{\\Delta t} + \\nu k^2\\right)\\left(\\frac{k^2 \\Delta t}{\\rho}\\right) + k^2\\right) = -\\left(\\frac{\\rho k^2\\Delta t}{\\Delta t \\rho} + \\frac{\\nu k^4 \\Delta t}{\\rho} + k^2\\right) = -\\left(k^2 + \\frac{\\nu k^4 \\Delta t}{\\rho} + k^2\\right) = -\\left(2k^2 + \\frac{\\nu k^4 \\Delta t}{\\rho}\\right)$.\n- Constant term: $b^2 = k^2$.\n\nThe characteristic equation becomes:\n$$\nk^2 \\lambda^2 - \\left(2k^2 + \\frac{\\nu k^4 \\Delta t}{\\rho}\\right)\\lambda + k^2 = 0\n$$\nAssuming $k \\neq 0$, we can divide by $k^2$:\n$$\n\\lambda^2 - \\left(2 + \\frac{\\nu k^2 \\Delta t}{\\rho}\\right)\\lambda + 1 = 0\n$$\nLet the dimensionless parameter $x = \\frac{\\nu k^2 \\Delta t}{\\rho}$. The equation is $\\lambda^2 - (2+x)\\lambda + 1 = 0$. Using the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{(2+x) \\pm \\sqrt{(2+x)^2 - 4}}{2} = \\frac{(2+x) \\pm \\sqrt{4+4x+x^2 - 4}}{2} = \\frac{(2+x) \\pm \\sqrt{x(4+x)}}{2}\n$$\nSince all parameters $\\rho, \\nu, \\Delta t, k^2$ are non-negative, $x \\ge 0$. This ensures the eigenvalues are real and positive. The spectral radius is the maximum of the absolute values of the eigenvalues, which is the larger of the two roots:\n$$\n\\rho(\\widehat{\\mathcal{M}}^{-1}\\mathcal{K}) = \\lambda_{\\max} = \\frac{1}{2} \\left(2 + \\frac{\\nu k^2 \\Delta t}{\\rho} + \\sqrt{\\frac{\\nu k^2 \\Delta t}{\\rho} \\left(4 + \\frac{\\nu k^2 \\Delta t}{\\rho}\\right)}\\right)\n$$\nFinally, we evaluate this expression for the given parameters $\\rho = 1$, $\\nu = 1$, $\\Delta t = 1$, and $k = 1$.\nThe parameter $x$ is:\n$$\nx = \\frac{\\nu k^2 \\Delta t}{\\rho} = \\frac{(1)(1)^2(1)}{1} = 1\n$$\nSubstituting $x=1$ into the expression for the spectral radius:\n$$\n\\rho(\\widehat{\\mathcal{M}}^{-1}\\mathcal{K}) = \\frac{(2+1) + \\sqrt{1(4+1)}}{2} = \\frac{3 + \\sqrt{5}}{2}\n$$\nThis is the final exact analytic expression for the spectral radius.",
            "answer": "$$\n\\boxed{\\frac{3 + \\sqrt{5}}{2}}\n$$"
        }
    ]
}