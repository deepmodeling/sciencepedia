{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of parallel computing is to solve problems faster. To rigorously quantify this, we use metrics like speedup and parallel efficiency. This first exercise focuses on strong scaling, where a fixed-size problem is run on an increasing number of processors, a common scenario when seeking to reduce time-to-solution. By analyzing a hypothetical but realistic set of timing data from a gyrokinetic solver, you will learn to dissect the total wall-clock time into computation, communication, and load imbalance, and calculate the true parallel efficiency, a fundamental skill in performance analysis and code optimization .",
            "id": "4025625",
            "problem": "A gyrokinetic turbulence solver used in computational fusion science and engineering is strong-scaled from $P=128$ to $P=256$ processing elements for a fixed problem size. For each processor count, the per-timestep wall-clock time is decomposed into three measured components: pure computation time, Message Passing Interface (MPI) communication time, and load-imbalance-induced waiting time (measured as the excess at global synchronization relative to the mean rank progress). The measurements are:\n- At $P=128$: computation $0.54$, communication $0.18$, imbalance $0.03$ (all in seconds per timestep).\n- At $P=256$: computation $0.29$, communication $0.22$, imbalance $0.05$ (all in seconds per timestep).\n\nAssume the problem size is fixed, and that the total per-timestep wall-clock time at each $P$ is the sum of the three reported components. Define the achieved speedup when going from $P=128$ to $P=256$ as the ratio of the $P=128$ total per-timestep time to the $P=256$ total per-timestep time for the same workload. Define the parallel efficiency for this doubling as the ratio of the achieved speedup to the ideal speedup for doubling the processor count at fixed problem size.\n\nCompute the parallel efficiency for the doubling from $P=128$ to $P=256$, explicitly accounting for communication overhead and load imbalance through the provided measurements. Express your answer as a dimensionless decimal and round to four significant figures.",
            "solution": "The problem is first validated against the established criteria.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Initial number of processing elements: $P_1 = 128$.\n- Final number of processing elements: $P_2 = 256$.\n- The problem size is fixed (strong scaling).\n- Time measurements at $P_1 = 128$:\n    - Computation time per timestep: $T_{\\text{comp},1} = 0.54$ s.\n    - Communication time per timestep: $T_{\\text{comm},1} = 0.18$ s.\n    - Load-imbalance time per timestep: $T_{\\text{imb},1} = 0.03$ s.\n- Time measurements at $P_2 = 256$:\n    - Computation time per timestep: $T_{\\text{comp},2} = 0.29$ s.\n    - Communication time per timestep: $T_{\\text{comm},2} = 0.22$ s.\n    - Load-imbalance time per timestep: $T_{\\text{imb},2} = 0.05$ s.\n- Definition of total time: The total per-timestep wall-clock time, $T_{\\text{total}}$, is the sum of the three components: $T_{\\text{total}} = T_{\\text{comp}} + T_{\\text{comm}} + T_{\\text{imb}}$.\n- Definition of achieved speedup: The achieved speedup, $S_{\\text{achieved}}$, is the ratio of the total time at $P_1$ to the total time at $P_2$, i.e., $S_{\\text{achieved}} = T_{\\text{total},1} / T_{\\text{total},2}$.\n- Definition of parallel efficiency: The parallel efficiency, $\\eta$, for this doubling is the ratio of the achieved speedup to the ideal speedup.\n- The final answer is to be a dimensionless decimal rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it describes a standard strong-scaling performance analysis common in high-performance computing and computational science. The decomposition of wall-clock time into computation, communication, and load imbalance is a valid and widely used methodology. The numerical values are realistic for a scaling experiment: computation time decreases (approximately halving, as expected), while communication and imbalance overheads increase with the processor count. The problem is well-posed, providing all necessary definitions and data for a unique solution. The language is objective and precise. The problem is self-contained, consistent, and does not violate any fundamental principles.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be derived.\n\n**Solution Derivation**\n\nThe objective is to compute the parallel efficiency, $\\eta$, for the strong scaling of a gyrokinetic turbulence solver from $P_1 = 128$ to $P_2 = 256$ processing elements.\n\nFirst, we calculate the total per-timestep wall-clock time for each processor count. According to the problem definition, the total time is the sum of the computation, communication, and imbalance components.\n\nFor the initial configuration with $P_1 = 128$ processors, the total time, $T_{\\text{total},1}$, is:\n$$T_{\\text{total},1} = T_{\\text{comp},1} + T_{\\text{comm},1} + T_{\\text{imb},1}$$\nSubstituting the given values:\n$$T_{\\text{total},1} = 0.54 \\, \\text{s} + 0.18 \\, \\text{s} + 0.03 \\, \\text{s} = 0.75 \\, \\text{s}$$\n\nFor the final configuration with $P_2 = 256$ processors, the total time, $T_{\\text{total},2}$, is:\n$$T_{\\text{total},2} = T_{\\text{comp},2} + T_{\\text{comm},2} + T_{\\text{imb},2}$$\nSubstituting the given values:\n$$T_{\\text{total},2} = 0.29 \\, \\text{s} + 0.22 \\, \\text{s} + 0.05 \\, \\text{s} = 0.56 \\, \\text{s}$$\n\nNext, we calculate the achieved speedup, $S_{\\text{achieved}}$. This is defined as the ratio of the total time on the smaller processor count to the total time on the larger processor count for a fixed problem size.\n$$S_{\\text{achieved}} = \\frac{T_{\\text{total},1}}{T_{\\text{total},2}}$$\nUsing our calculated total times:\n$$S_{\\text{achieved}} = \\frac{0.75}{0.56}$$\n\nThe ideal speedup, $S_{\\text{ideal}}$, for a strong-scaling problem is the ratio of the final processor count to the initial processor count.\n$$S_{\\text{ideal}} = \\frac{P_2}{P_1}$$\nSubstituting the given processor counts:\n$$S_{\\text{ideal}} = \\frac{256}{128} = 2$$\n\nFinally, the parallel efficiency, $\\eta$, is defined as the ratio of the achieved speedup to the ideal speedup.\n$$\\eta = \\frac{S_{\\text{achieved}}}{S_{\\text{ideal}}}$$\nSubstituting the expressions for $S_{\\text{achieved}}$ and $S_{\\text{ideal}}$:\n$$\\eta = \\frac{\\left(\\frac{T_{\\text{total},1}}{T_{\\text{total},2}}\\right)}{S_{\\text{ideal}}} = \\frac{\\left(\\frac{0.75}{0.56}\\right)}{2}$$\nNow, we compute the numerical value:\n$$\\eta = \\frac{1.3392857...}{2} = 0.669642857...$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\eta \\approx 0.6696$$\nThis value represents the efficiency of the parallelization, where an efficiency of $1$ would indicate perfect scaling. The result, $\\eta \\approx 0.6696$, signifies that the code achieved approximately $67\\%$ of the ideal speedup, with the remaining performance loss attributable to the increase in communication and load imbalance overheads as the number of processors was doubled.",
            "answer": "$$\\boxed{0.6696}$$"
        },
        {
            "introduction": "Large-scale plasma simulations are made possible by domain decomposition, where the global problem is partitioned across thousands of processors. This partitioning, however, creates artificial boundaries, and physical models like the charge continuity equation ($\\nabla \\cdot \\mathbf{J} = -\\partial \\rho / \\partial t$) require data from adjacent subdomains to compute spatial derivatives or fluxes correctly. This exercise tasks you with designing the fundamental communication pattern for this scenario: the ghost cell (or halo) exchange, which is the backbone of virtually all parallel finite-difference and finite-volume codes. You will implement a neighborhood communication scheme and analyze its memory footprint, gaining hands-on experience with the practical mechanics of distributed-memory parallelism .",
            "id": "4025608",
            "problem": "You are tasked with designing a Message Passing Interface (MPI) neighborhood collectives scheme for exchanging current densities with only geometrical neighbors in a three-dimensional, structured, finite-volume plasma simulation. The goal is to derive a principled algorithm and compute its memory footprint. You will implement and simulate the scheme in software, assuming a Cartesian domain decomposition with face-adjacent neighbors only. You must not rely on actual MPI libraries; instead, construct a correct neighborhood-exchange analog that adheres to the geometric constraints of a three-dimensional topology. All answers must be expressed in bytes, and your program must produce the results in the specified output format.\n\nBase scientific context: In continuum electrodynamics, the conservation of charge requires that the current density vector be consistently exchanged across cell faces so that discrete fluxes are balanced between subdomains. Starting from the charge continuity equation,\n$$\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot \\mathbf{J} = 0,$$\nwhere $\\rho$ is the charge density and $\\mathbf{J}$ is the current density, a finite-volume discretization introduces face fluxes that couple neighboring cells across subdomain boundaries. Under a Cartesian decomposition, the exchange of the current density $\\mathbf{J} = (J_x, J_y, J_z)$ across interfaces is limited to geometrical face neighbors, consistent with the locality of fluxes. In parallel computing with domain decomposition, ghost cells of thickness $\\delta$ layers are used so that each subdomain can compute face-centered fluxes using data exchanged from adjacent subdomains.\n\nYou must build a neighborhood collectives scheme that uses only geometrical face neighbors along the $x$, $y$, and $z$ axes. Periodic boundary conditions may wrap neighbor indices; nonperiodic boundaries truncate neighbor exchanges.\n\nCore definitions and assumptions to use:\n- The global mesh has dimensions $N_x \\times N_y \\times N_z$ cells.\n- The process grid is $P_x \\times P_y \\times P_z$ processes, using a Cartesian decomposition with equal block sizes. Assume $N_x$, $N_y$, $N_z$ are divisible by $P_x$, $P_y$, $P_z$, respectively.\n- Each process holds a local block of size $n_x \\times n_y \\times n_z$, where $n_x = N_x / P_x$, $n_y = N_y / P_y$, and $n_z = N_z / P_z$.\n- The ghost layer thickness is $\\delta$ (in number of cell layers). If $\\delta$ exceeds a local dimension $n_d$ along any axis $d \\in \\{x,y,z\\}$, use $\\min(\\delta, n_d)$ for exchange thickness along that axis.\n- Only face neighbors are exchanged ($\\pm x$, $\\pm y$, $\\pm z$). No edge or corner exchanges are performed.\n- The current density vector has $c$ components (use $c = 3$), with each component stored as a floating-point number of $b$ bytes (use $b = 8$ for double precision).\n- For any face with local area $A$ and thickness $t$, the message size in bytes is $c \\cdot b \\cdot A \\cdot t$.\n- For each process, the total send buffer size is the sum of face messages to existing neighbors; the total receive buffer size is the sum of face messages incoming from its neighbors (identical to the outgoing for uniform blocks but must be computed consistently). The per-process memory footprint is the sum of the send and receive buffer sizes in bytes.\n- Periodicity is specified per axis. If periodic along axis $d$, neighbors wrap around; otherwise, boundary processes without neighbors along $d$ do not exchange data along that face.\n\nAlgorithmic tasks to implement:\n- Construct the Cartesian process topology with rank coordinates $(i_x,i_y,i_z)$ and face-adjacent neighbor mapping considering periodicity.\n- Compute local block sizes $n_x$, $n_y$, $n_z$ and per-direction exchange thicknesses $t_x = \\min(\\delta, n_x)$, $t_y = \\min(\\delta, n_y)$, $t_z = \\min(\\delta, n_z)$.\n- Compute per-face areas $A_{x} = n_y n_z$, $A_{y} = n_x n_z$, and $A_{z} = n_x n_y$.\n- Derive per-process send and receive memory footprints by summing $c \\cdot b \\cdot A_d \\cdot t_d$ over existing face neighbors $d \\in \\{x,y,z\\}$ and directions $\\pm$.\n- Implement a neighborhood-exchange analog by:\n  - Allocating per-process interior arrays of shape $(n_x, n_y, n_z, c)$ and ghost-augmented arrays of shape $(n_x + 2\\delta, n_y + 2\\delta, n_z + 2\\delta, c)$.\n  - Filling interiors with a deterministic function of $(i,j,k)$, component index, and process rank to enable validation.\n  - Copying face boundary slices from neighbor interiors into the corresponding ghost layers with thicknesses $t_x$, $t_y$, and $t_z$.\n  - Validating that, for all existing neighbors, each process’s ghost face equals the neighbor’s interior boundary slice. Ignore non-existing faces in nonperiodic directions.\n\nValidation and outputs:\n- For each test case, compute:\n  $1)$ the maximum per-process memory footprint in bytes across all processes,\n  $2)$ the total memory footprint in bytes summed across all processes,\n  $3)$ a boolean indicating whether the neighborhood exchange produced correct ghost layers for all geometrical neighbor faces that exist.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and flattened as:\n  $[$max\\_bytes\\_case\\_1, total\\_bytes\\_case\\_1, validity\\_case\\_1, max\\_bytes\\_case\\_2, total\\_bytes\\_case\\_2, validity\\_case\\_2, \\dots$]$.\n\nTest suite:\n- Case $1$ (happy path): $N_x = 64$, $N_y = 48$, $N_z = 32$; $P_x = 4$, $P_y = 3$, $P_z = 2$; $\\delta = 2$; periodicity $(\\text{True}, \\text{True}, \\text{False})$; $c = 3$; $b = 8$ bytes.\n- Case $2$ (boundary condition with zero ghost): $N_x = 64$, $N_y = 48$, $N_z = 32$; $P_x = 4$, $P_y = 3$, $P_z = 2$; $\\delta = 0$; periodicity $(\\text{False}, \\text{False}, \\text{False})$; $c = 3$; $b = 8$ bytes.\n- Case $3$ (edge case with large ghost relative to block size): $N_x = 16$, $N_y = 16$, $N_z = 8$; $P_x = 2$, $P_y = 2$, $P_z = 2$; $\\delta = 3$; periodicity $(\\text{True}, \\text{True}, \\text{True})$; $c = 3$; $b = 8$ bytes.\n- Case $4$ (anisotropic blocks and mixed periodicity): $N_x = 90$, $N_y = 30$, $N_z = 24$; $P_x = 3$, $P_y = 3$, $P_z = 2$; $\\delta = 1$; periodicity $(\\text{False}, \\text{True}, \\text{True})$; $c = 3$; $b = 8$ bytes.\n\nAll memory results must be in bytes. The boolean validity must be either $\\text{True}$ or $\\text{False}$.",
            "solution": "The problem requires the design and validation of a neighborhood communication scheme for a three-dimensional plasma simulation on a parallel computer. The solution is predicated on the principles of domain decomposition, ghost cell exchange, and Cartesian process topologies, which are fundamental to scalable scientific computing.\n\n**1. Theoretical Framework: Domain Decomposition and Process Topology**\n\nThe global computational domain of size $N_x \\times N_y \\times N_z$ cells is decomposed into a Cartesian grid of $P_x \\times P_y \\times P_z$ processes. Each process is assigned a unique integer rank $r \\in [0, P_{total}-1]$, where $P_{total} = P_x P_y P_z$. This rank can be mapped to a unique 3D coordinate $(i_x, i_y, i_z)$ within the process grid, where $i_x \\in [0, P_x-1]$, $i_y \\in [0, P_y-1]$, and $i_z \\in [0, P_z-1]$. The mapping from rank $r$ to coordinates is given by:\n$$i_x = \\lfloor r / (P_y P_z) \\rfloor$$\n$$i_y = \\lfloor (r \\pmod{P_y P_z}) / P_z \\rfloor$$\n$$i_z = r \\pmod{P_z}$$\nThe inverse mapping, from coordinates to rank, is $r = i_x (P_y P_z) + i_y P_z + i_z$. Each process manages a local subdomain (block) of size $n_x \\times n_y \\times n_z$, where $n_d = N_d / P_d$ for each axis $d \\in \\{x, y, z\\}$.\n\nCommunication is restricted to face-adjacent neighbors. For a process at $(i_x, i_y, i_z)$, its neighbor in a direction, e.g., $+x$, is located at coordinate $(i_x+1, i_y, i_z)$. The existence of this neighbor depends on the boundary conditions. For a non-periodic boundary on axis $x$, a neighbor exists only if $0 \\le i_x+1 < P_x$. For a periodic boundary, the coordinate wraps around: $i'_x = (i_x+1) \\pmod{P_x}$. This logic applies to all $6$ directions ($\\pm x, \\pm y, \\pm z$).\n\n**2. Ghost Cells and Data Exchange**\n\nTo compute physical quantities like fluxes at the boundary of a subdomain, a process needs data from its neighbors. This is achieved by surrounding the local data block with layers of \"ghost cells\" (also called halo cells). These ghost cells are populated with data from the corresponding interior boundary regions of the neighboring processes. The thickness of the ghost layer is given by $\\delta$.\n\nThe amount of data to be exchanged along an axis $d$ is determined by the effective exchange thickness, $t_d$, defined as $t_d = \\min(\\delta, n_d)$. This accounts for cases where the ghost layer thickness $\\delta$ is larger than the local domain size $n_d$, in which case the entire local domain along that axis is exchanged.\n\nThe data being exchanged is the current density vector $\\mathbf{J}$, which has $c=3$ components, each stored as a floating-point number of $b=8$ bytes. The message size required to update a ghost region for a specific face is the product of the number of components ($c$), the size of each component ($b$), the area of the face ($A_d$), and the exchange thickness ($t_d$). For example, the message size for a face perpendicular to the $x$-axis is:\n$$M_x = c \\cdot b \\cdot A_x \\cdot t_x = c \\cdot b \\cdot (n_y n_z) \\cdot t_x$$\nSimilar expressions hold for $M_y = c \\cdot b \\cdot (n_x n_z) \\cdot t_y$ and $M_z = c \\cdot b \\cdot (n_x n_y) \\cdot t_z$.\n\n**3. Memory Footprint Calculation**\n\nThe per-process memory footprint for communication is defined as the sum of the sizes of its send and receive buffers. The total send buffer size for a process is the sum of the message sizes for all its existing neighbors. The total receive buffer size is defined similarly. For a process $r$ at coordinates $(i_x, i_y, i_z)$, we can calculate its footprint by summing the contributions from each of the $6$ potential neighbor directions:\n$$M_{proc}(r) = S_{send}(r) + S_{recv}(r)$$\nwhere $S_{send}(r)$ and $S_{recv}(r)$ are calculated as follows:\nInitialize $S_{send}(r) = 0$ and $S_{recv}(r) = 0$.\nFor each direction $d \\in \\{x,y,z\\}$ and sign $\\sigma \\in \\{-,+\\}$:\nIf a neighbor exists in the $\\sigma d$ direction:\n$$S_{send}(r) \\leftarrow S_{send}(r) + M_d$$\n$$S_{recv}(r) \\leftarrow S_{recv}(r) + M_d$$\nSince block sizes are uniform, the message size sent to a neighbor equals the size of the message received from it. The maximum per-process footprint is $\\max_{r} M_{proc}(r)$, and the total footprint is $\\sum_{r} M_{proc}(r)$.\n\n**4. Algorithmic Simulation and Validation**\n\nThe neighborhood exchange is simulated without using an actual MPI library. The process involves the following steps:\n1.  **Initialization**: For each process, an interior data array of shape $(n_x, n_y, n_z, c)$ is allocated. This array is filled with unique values using a deterministic function of the cell indices $(i, j, k)$, the vector component index, and the process rank. This ensures that any misplaced data can be detected. A larger ghost-augmented array of shape $(n_x + 2\\delta, n_y + 2\\delta, n_z + 2\\delta, c)$ is also allocated, and the interior data is copied into its center.\n2.  **Exchange Simulation**: The algorithm iterates through each process and each of its existing neighbors. For each neighbor pair, it identifies the boundary slice of data from the neighbor's interior array and copies it into the corresponding ghost cell region of the current process's ghost-augmented array. For example, to fill the ghost region on the $+x$ face of process $r$, data is copied from the $-x$ boundary face of its $+x$ neighbor. The slice to be copied has a thickness of $t_x$, a height of $n_y$, and a depth of $n_z$. This is repeated for all $6$ directions for all processes.\n3.  **Validation**: After the exchange, the correctness of the data transfer is verified. For each process and each existing neighbor, the data in a given ghost region is compared against the data in the corresponding boundary region of the neighbor's interior array. If all data in all ghost regions across all processes match their intended source slices, the exchange is deemed valid (`True`). If any mismatch is found, the exchange is invalid (`False`). For the case where $\\delta=0$, the ghost layers and exchange thickness are zero, resulting in a trivial but valid exchange with zero memory footprint.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that iterates through test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        {'N': (64, 48, 32), 'P': (4, 3, 2), 'delta': 2, 'periodicity': (True, True, False), 'c': 3, 'b': 8},\n        # Case 2 (boundary condition with zero ghost)\n        {'N': (64, 48, 32), 'P': (4, 3, 2), 'delta': 0, 'periodicity': (False, False, False), 'c': 3, 'b': 8},\n        # Case 3 (edge case with large ghost relative to block size)\n        {'N': (16, 16, 8), 'P': (2, 2, 2), 'delta': 3, 'periodicity': (True, True, True), 'c': 3, 'b': 8},\n        # Case 4 (anisotropic blocks and mixed periodicity)\n        {'N': (90, 30, 24), 'P': (3, 3, 2), 'delta': 1, 'periodicity': (False, True, True), 'c': 3, 'b': 8},\n    ]\n\n    results = []\n    for params in test_cases:\n        max_fp, total_fp, is_valid = simulate_case(**params)\n        results.extend([max_fp, total_fp, is_valid])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_case(N, P, delta, periodicity, c, b):\n    \"\"\"\n    Simulates the neighborhood collective scheme for a single test case.\n    \"\"\"\n    Nx, Ny, Nz = N\n    Px, Py, Pz = P\n    P_total = Px * Py * Pz\n\n    # Local block sizes\n    nx, ny, nz = Nx // Px, Ny // Py, Nz // Pz\n\n    # Per-direction exchange thicknesses\n    tx = min(delta, nx)\n    ty = min(delta, ny)\n    tz = min(delta, nz)\n\n    # Per-face areas\n    Ax = ny * nz\n    Ay = nx * nz\n    Az = nx * ny\n\n    # Message sizes\n    msg_size_x = c * b * Ax * tx\n    msg_size_y = c * b * Ay * ty\n    msg_size_z = c * b * Az * tz\n    msg_sizes = [msg_size_x, msg_size_y, msg_size_z]\n\n    # Process topology and neighbor finding\n    procs = []\n    for r in range(P_total):\n        # Rank to coordinates\n        ix = r // (Py * Pz)\n        iy = (r % (Py * Pz)) // Pz\n        iz = r % Pz\n        \n        proc_info = {'rank': r, 'coord': (ix, iy, iz), 'neighbors': {}}\n        \n        coords = [ix, iy, iz]\n        proc_dims = [Px, Py, Pz]\n\n        # Find neighbors for each of the 6 directions\n        for axis in range(3): # 0:x, 1:y, 2:z\n            for sign in [-1, 1]: # -1: negative, +1: positive\n                neighbor_coord = list(coords)\n                neighbor_coord[axis] += sign\n                \n                is_periodic = periodicity[axis]\n                dim_size = proc_dims[axis]\n                \n                neighbor_rank = None\n                if is_periodic:\n                    neighbor_coord[axis] %= dim_size\n                    # Coord to rank\n                    nix, niy, niz = neighbor_coord\n                    neighbor_rank = nix * Py * Pz + niy * Pz + niz\n                elif 0 <= neighbor_coord[axis] < dim_size:\n                    # Coord to rank\n                    nix, niy, niz = neighbor_coord\n                    neighbor_rank = nix * Py * Pz + niy * Pz + niz\n\n                proc_info['neighbors'][(axis, sign)] = neighbor_rank\n        procs.append(proc_info)\n\n    # Memory footprint calculation\n    footprints = []\n    for proc in procs:\n        send_buffer = 0\n        recv_buffer = 0\n        for (axis, sign), neighbor_rank in proc['neighbors'].items():\n            if neighbor_rank is not None:\n                msg_size = msg_sizes[axis]\n                send_buffer += msg_size\n                recv_buffer += msg_size\n        footprints.append(send_buffer + recv_buffer)\n\n    max_footprint = max(footprints) if footprints else 0\n    total_footprint = sum(footprints) if footprints else 0\n\n    # If delta is 0, exchange is trivial and valid, memory is 0.\n    if delta == 0:\n        return 0, 0, True\n\n    # --- Simulation of data exchange ---\n    \n    # 1. Initialize data for all processes\n    sim_data = {}\n    for r in range(P_total):\n        # Deterministic filling function\n        def fill_func(i, j, k, comp, rank):\n            return rank * 10000000 + i * 100000 + j * 1000 + k * 100 + comp\n\n        interior = np.fromfunction(np.vectorize(fill_func), (nx, ny, nz, c), dtype=np.float64, rank=r)\n        \n        ghosted_shape = (nx + 2 * delta, ny + 2 * delta, nz + 2 * delta, c)\n        ghosted = np.full(ghosted_shape, -1.0, dtype=np.float64) # Fill with a sentinel value\n        \n        # Copy interior data to the center of the ghosted array\n        ghosted[delta:delta+nx, delta:delta+ny, delta:delta+nz, :] = interior\n        \n        sim_data[r] = {'interior': interior, 'ghosted': ghosted}\n\n    # 2. Perform the exchange\n    for r in range(P_total):\n        proc = procs[r]\n        for (axis, sign), neighbor_rank in proc['neighbors'].items():\n            if neighbor_rank is not None:\n                neighbor_interior = sim_data[neighbor_rank]['interior']\n                local_ghosted = sim_data[r]['ghosted']\n                \n                # Get source slice from neighbor's interior\n                if axis == 0: # x-axis\n                    src = neighbor_interior[nx-tx:, :, :, :] if sign == -1 else neighbor_interior[:tx, :, :, :]\n                elif axis == 1: # y-axis\n                    src = neighbor_interior[:, ny-ty:, :, :] if sign == -1 else neighbor_interior[:, :ty, :, :]\n                else: # z-axis\n                    src = neighbor_interior[:, :, nz-tz:, :] if sign == -1 else neighbor_interior[:, :, :tz, :]\n                \n                # Get destination view in local ghosted array\n                if axis == 0: # x-axis\n                    dest_view = local_ghosted[delta-tx:delta, delta:delta+ny, delta:delta+nz, :] if sign == -1 \\\n                        else local_ghosted[delta+nx:delta+nx+tx, delta:delta+ny, delta:delta+nz, :]\n                elif axis == 1: # y-axis\n                    dest_view = local_ghosted[delta:delta+nx, delta-ty:delta, delta:delta+nz, :] if sign == -1 \\\n                        else local_ghosted[delta:delta+nx, delta+ny:delta+ny+ty, delta:delta+nz, :]\n                else: # z-axis\n                    dest_view = local_ghosted[delta:delta+nx, delta:delta+ny, delta-tz:delta, :] if sign == -1 \\\n                        else local_ghosted[delta:delta+nx, delta:delta+ny, delta+nz:delta+nz+tz, :]\n\n                dest_view[:] = src\n\n    # 3. Validation\n    is_valid = True\n    for r in range(P_total):\n        proc = procs[r]\n        for (axis, sign), neighbor_rank in proc['neighbors'].items():\n            if neighbor_rank is not None:\n                neighbor_interior = sim_data[neighbor_rank]['interior']\n                local_ghosted = sim_data[r]['ghosted']\n\n                # Re-calculate source slice from neighbor's interior\n                if axis == 0: # x-axis\n                    src_slice = neighbor_interior[nx-tx:, :, :, :] if sign == -1 else neighbor_interior[:tx, :, :, :]\n                elif axis == 1: # y-axis\n                    src_slice = neighbor_interior[:, ny-ty:, :, :] if sign == -1 else neighbor_interior[:, :ty, :, :]\n                else: # z-axis\n                    src_slice = neighbor_interior[:, :, nz-tz:, :] if sign == -1 else neighbor_interior[:, :, :tz, :]\n\n                # Re-calculate destination ghost slice from local ghosted array\n                if axis == 0: # x-axis\n                    ghost_slice = local_ghosted[delta-tx:delta, delta:delta+ny, delta:delta+nz, :] if sign == -1 \\\n                        else local_ghosted[delta+nx:delta+nx+tx, delta:delta+ny, delta:delta+nz, :]\n                elif axis == 1: # y-axis\n                    ghost_slice = local_ghosted[delta:delta+nx, delta-ty:delta, delta:delta+nz, :] if sign == -1 \\\n                        else local_ghosted[delta:delta+nx, delta+ny:delta+ny+ty, delta:delta+nz, :]\n                else: # z-axis\n                    ghost_slice = local_ghosted[delta:delta+nx, delta:delta+ny, delta-tz:delta, :] if sign == -1 \\\n                        else local_ghosted[delta:delta+nx, delta:delta+ny, delta+nz:delta+nz+tz, :]\n                \n                if not np.array_equal(src_slice, ghost_slice):\n                    is_valid = False\n                    break\n        if not is_valid:\n            break\n            \n    return int(max_footprint), int(total_footprint), is_valid\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "As simulation algorithms grow in complexity, their parallel execution involves more than just simple, monolithic communication steps. A single time step in a Particle-In-Cell (PIC) code, for instance, is a workflow of distinct operations: particle pushing, deposition, and field solving, each with its own dependencies. This exercise introduces a more advanced and flexible paradigm: task-based parallelism, where the algorithm is represented as a Directed Acyclic Graph (DAG). By modeling a PIC time step as a DAG, you will learn to identify the critical path, which determines the theoretical minimum execution time by enabling the overlap of independent computations and communications . This approach is central to performance optimization on modern, highly parallel architectures.",
            "id": "4025617",
            "problem": "Consider a single time-step of a Particle-In-Cell (PIC) algorithm for magnetized plasma in a domain-decomposed configuration used in computational fusion science and engineering. The PIC algorithm is structured by well-tested physics and numerical facts: Maxwell's equations couple the electric field $\\mathbf{E}$ and the magnetic field $\\mathbf{B}$ to charge density $\\rho$ and current density $\\mathbf{J}$ through the relations\n$$\n\\nabla \\cdot \\mathbf{E} = \\frac{\\rho}{\\varepsilon_0}, \\quad \\nabla \\times \\mathbf{B} - \\mu_0 \\varepsilon_0 \\frac{\\partial \\mathbf{E}}{\\partial t} = \\mu_0 \\mathbf{J},\n$$\nand the particle equations of motion obey\n$$\nm \\frac{d\\mathbf{v}}{dt} = q\\left(\\mathbf{E} + \\mathbf{v} \\times \\mathbf{B}\\right), \\quad \\frac{d\\mathbf{x}}{dt} = \\mathbf{v}.\n$$\nA standard PIC time step comprises particle-to-grid deposition, field solve on the grid, and particle push, where the field solve requires the deposited charge density $\\rho$ and potentially neighbor contributions via halo or ghost regions created by domain decomposition. In a parallel setting, these stages are organized as a Directed Acyclic Graph (DAG), whose nodes represent computational tasks and whose edges encode data dependencies emerging from the physical discretization and domain coupling. By overlapping deposition and local field solves across tiles (subdomains), one can reduce the end-to-end time for the time step. The goal is to construct such a DAG-based schedule and evaluate its critical path length.\n\nAssume a one-dimensional chain of $T$ tiles, indexed by $i \\in \\{0, 1, \\dots, T-1\\}$. For each tile $i$, define three categories of tasks:\n- Deposition task $D_i$, which maps particles in tile $i$ to grid-based charge density $\\rho_i$ and produces boundary contributions for neighbor exchanges.\n- Halo reduction task $H_{i,i+1}$, defined for each adjacent pair $(i,i+1)$, which merges and reduces boundary contributions between tiles $i$ and $i+1$; this task depends on both $D_i$ and $D_{i+1}$.\n- Local field solve task $S_i$, which computes the local electromagnetic field in tile $i$ using $\\rho_i$ and, when required, neighbor halo-reduced contributions; this task depends on $D_i$ and on all halo reductions adjacent to tile $i$.\n\nAll tasks are assumed to run on a runtime with effectively unlimited processors such that data dependencies are the only constraints; the schedule is therefore characterized by the earliest start policy induced by the DAG. The weight $w(v)$ of each node $v$ in the DAG is its duration in seconds. The earliest finish time $EF(v)$ is defined recursively by\n$$\nEF(v) = w(v) + \\max_{u \\in \\text{pred}(v)} EF(u),\n$$\nwith $EF(v) = w(v)$ if $v$ has no predecessors. The critical path length of the time step is the maximum earliest finish time among all local field solve tasks,\n$$\n\\text{CP} = \\max_{i} EF(S_i),\n$$\nwhich gives the minimum time to finish all local field solves, assuming ideal overlap.\n\nYour task is to write a complete, runnable program that:\n- Constructs the DAG from given tile counts and task durations.\n- Implements the earliest-start schedule on the DAG.\n- Computes and returns the critical path length $\\text{CP}$ in seconds for each test case.\n\nThe data dependencies must follow the physics-motivated structure described above. Specifically:\n- $D_i \\rightarrow H_{i,i+1}$ and $D_{i+1} \\rightarrow H_{i,i+1}$ for each adjacent pair $(i,i+1)$ if halos are required.\n- $D_i \\rightarrow S_i$ always.\n- $H_{i,i+1} \\rightarrow S_i$ and $H_{i,i+1} \\rightarrow S_{i+1}$ if halos are required.\n\nIf halos are not required, the local field solve $S_i$ depends only on $D_i$.\n\nThe program must use the following test suite. For each case, compute $\\text{CP}$ in seconds and round each result to $6$ decimal places:\n\n- Case $1$ (happy path with nontrivial overlap and nonzero halos): $T = 4$, deposit durations $\\left[d_0,d_1,d_2,d_3\\right] = \\left[0.06, 0.04, 0.05, 0.02\\right]~\\mathrm{s}$, halo durations on edges $\\left[h_{0,1}, h_{1,2}, h_{2,3}\\right] = \\left[0.01, 0.015, 0.01\\right]~\\mathrm{s}$, local field solve durations $\\left[s_0,s_1,s_2,s_3\\right] = \\left[0.08, 0.07, 0.09, 0.05\\right]~\\mathrm{s}$, halos required.\n- Case $2$ (boundary condition with a single tile): $T = 1$, deposit durations $\\left[d_0\\right] = \\left[0.12\\right]~\\mathrm{s}$, halo durations empty, local field solve durations $\\left[s_0\\right] = \\left[0.20\\right]~\\mathrm{s}$, halos required (no halo tasks exist because there are no neighbors).\n- Case $3$ (edge case with zero-width halos effectively disabled): $T = 3$, deposit durations $\\left[d_0,d_1,d_2\\right] = \\left[0.03, 0.09, 0.04\\right]~\\mathrm{s}$, halo durations $\\left[h_{0,1}, h_{1,2}\\right] = \\left[0.00, 0.00\\right]~\\mathrm{s}$, local field solve durations $\\left[s_0,s_1,s_2\\right] = \\left[0.05, 0.06, 0.05\\right]~\\mathrm{s}$, halos not required.\n- Case $4$ (strong imbalance where one tile dominates deposition, with nonzero halos): $T = 5$, deposit durations $\\left[d_0,d_1,d_2,d_3,d_4\\right] = \\left[0.02, 0.02, 0.15, 0.03, 0.02\\right]~\\mathrm{s}$, halo durations $\\left[h_{0,1}, h_{1,2}, h_{2,3}, h_{3,4}\\right] = \\left[0.02, 0.02, 0.02, 0.02\\right]~\\mathrm{s}$, local field solve durations $\\left[s_0,s_1,s_2,s_3,s_4\\right] = \\left[0.06, 0.06, 0.06, 0.06, 0.06\\right]~\\mathrm{s}$, halos required.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$), where each $\\text{result}_k$ is the computed critical path length in seconds for the corresponding case, rounded to $6$ decimal places.",
            "solution": "The problem statement has been analyzed and is determined to be **valid**. It is scientifically grounded in the principles of plasma physics and high-performance computing, specifically utilizing the Particle-In-Cell (PIC) method and Directed Acyclic Graph (DAG) scheduling. The problem is well-posed, with all necessary data, definitions, and dependencies provided to compute a unique, meaningful solution for each test case. The language is objective and the structure is logically sound.\n\nThe core of the problem is to determine the critical path length, $\\text{CP}$, of a task dependency graph that models one time step of a parallel PIC simulation. The critical path represents the minimum possible execution time for the set of tasks, assuming an idealized parallel environment with unlimited processors where task scheduling is constrained only by data dependencies. The total time is thus determined by the longest path through the DAG.\n\nWe are given three types of tasks for a one-dimensional chain of $T$ tiles, indexed by $i \\in \\{0, 1, \\dots, T-1\\}$:\n- Deposition task $D_i$ with duration $w(D_i) = d_i$.\n- Halo reduction task $H_{i,i+1}$ with duration $w(H_{i,i+1}) = h_{i,i+1}$.\n- Local field solve task $S_i$ with duration $w(S_i) = s_i$.\n\nThe calculation proceeds by determining the Earliest Finish time, $EF(v)$, for each task (node) $v$ in the DAG. The $EF(v)$ is defined by the recurrence relation:\n$$\nEF(v) = w(v) + \\max_{u \\in \\text{pred}(v)} EF(u)\n$$\nwhere $\\text{pred}(v)$ is the set of immediate predecessors of $v$. If a task $v$ has no predecessors, its earliest start time is $0$, so $EF(v) = w(v)$. The critical path length of the entire time step is the maximum earliest finish time among all local field solve tasks, $\\text{CP} = \\max_{i} EF(S_i)$.\n\nWe will now derive the expressions for the earliest finish times for each task type based on the specified dependencies.\n\n**1. Earliest Finish Time of Deposition Tasks ($EF(D_i)$)**\nThe deposition tasks $D_i$ are the initial tasks in the DAG for each tile. They have no predecessors. Therefore, their earliest finish time is simply their own duration.\n$$\nEF(D_i) = w(D_i) = d_i\n$$\nThis applies for all $i \\in \\{0, 1, \\dots, T-1\\}$.\n\n**2. Earliest Finish Time of Halo Reduction Tasks ($EF(H_{i,i+1})$)**\nThis calculation is relevant only if halos are required and $T > 1$. The halo reduction task $H_{i,i+1}$ for an adjacent pair of tiles $(i, i+1)$ depends on the completion of both deposition tasks $D_i$ and $D_{i+1}$. Thus, its earliest start time is $\\max(EF(D_i), EF(D_{i+1}))$.\n$$\nEF(H_{i,i+1}) = w(H_{i,i+1}) + \\max(EF(D_i), EF(D_{i+1})) = h_{i,i+1} + \\max(d_i, d_{i+1})\n$$\nThis applies for each adjacent pair, i.e., for $i \\in \\{0, 1, \\dots, T-2\\}$.\n\n**3. Earliest Finish Time of Local Field Solve Tasks ($EF(S_i)$)**\nThe dependencies for the local field solve task $S_i$ vary based on whether halos are required and the tile's position in the chain.\n\n**Case A: Halos are not required.**\nIn this scenario, $S_i$ depends only on the local deposition task $D_i$.\n$$\nEF(S_i) = w(S_i) + EF(D_i) = s_i + d_i\n$$\n\n**Case B: Halos are required.**\nThe dependencies are more complex.\n- For a single tile system ($T=1$), there are no adjacent tiles, so no halo tasks ($H$) exist. $S_0$ depends only on $D_0$, just as in Case A.\n$$EF(S_0) = s_0 + d_0 \\quad (\\text{for } T=1)$$\n\n- For a multi-tile system ($T > 1$), we must distinguish between boundary and interior tiles.\n    - **Left Boundary Tile ($i=0$):** $S_0$ depends on $D_0$ and the halo reduction $H_{0,1}$.\n      $$\n      EF(S_0) = w(S_0) + \\max(EF(D_0), EF(H_{0,1})) = s_0 + \\max(d_0, h_{0,1} + \\max(d_0, d_1))\n      $$\n    - **Interior Tiles ($i \\in \\{1, \\dots, T-2\\}$):** An interior solve $S_i$ depends on its local deposition $D_i$ and the halo reductions from both sides, $H_{i-1,i}$ and $H_{i,i+1}$.\n      $$\n      EF(S_i) = w(S_i) + \\max(EF(D_i), EF(H_{i-1,i}), EF(H_{i,i+1}))\n      $$\n      Substituting the expressions for the predecessor finish times:\n      $$\n      EF(S_i) = s_i + \\max(d_i, h_{i-1, i} + \\max(d_{i-1}, d_i), h_{i, i+1} + \\max(d_i, d_{i+1}))\n      $$\n    - **Right Boundary Tile ($i=T-1$):** $S_{T-1}$ depends on $D_{T-1}$ and the halo reduction $H_{T-2,T-1}$.\n      $$\n      EF(S_{T-1}) = w(S_{T-1}) + \\max(EF(D_{T-1}), EF(H_{T-2,T-1})) = s_{T-1} + \\max(d_{T-1}, h_{T-2,T-1} + \\max(d_{T-2}, d_{T-1}))\n      $$\n\n**4. Critical Path Length ($\\text{CP}$)**\nAfter computing $EF(S_i)$ for all tiles $i \\in \\{0, \\dots, T-1\\}$ using the appropriate formulas above, the critical path length is the maximum of these values.\n$$\n\\text{CP} = \\max_{i=0}^{T-1} EF(S_i)\n$$\n\n**Example Calculation: Case 1**\nGiven: $T = 4$, $d = [0.06, 0.04, 0.05, 0.02]$, $h = [0.01, 0.015, 0.01]$, $s = [0.08, 0.07, 0.09, 0.05]$, halos required.\n\n- **$EF(D_i)$:**\n  $EF(D_0) = 0.06$, $EF(D_1) = 0.04$, $EF(D_2) = 0.05$, $EF(D_3) = 0.02$.\n\n- **$EF(H_{i,i+1})$:**\n  $EF(H_{0,1}) = 0.01 + \\max(0.06, 0.04) = 0.07$.\n  $EF(H_{1,2}) = 0.015 + \\max(0.04, 0.05) = 0.065$.\n  $EF(H_{2,3}) = 0.01 + \\max(0.05, 0.02) = 0.06$.\n\n- **$EF(S_i)$:**\n  $EF(S_0) = s_0 + \\max(EF(D_0), EF(H_{0,1})) = 0.08 + \\max(0.06, 0.07) = 0.15$.\n  $EF(S_1) = s_1 + \\max(EF(D_1), EF(H_{0,1}), EF(H_{1,2})) = 0.07 + \\max(0.04, 0.07, 0.065) = 0.14$.\n  $EF(S_2) = s_2 + \\max(EF(D_2), EF(H_{1,2}), EF(H_{2,3})) = 0.09 + \\max(0.05, 0.065, 0.06) = 0.155$.\n  $EF(S_3) = s_3 + \\max(EF(D_3), EF(H_{2,3})) = 0.05 + \\max(0.02, 0.06) = 0.11$.\n\n- **$\\text{CP}$:**\n  $\\text{CP} = \\max(0.15, 0.14, 0.155, 0.11) = 0.155$.\n\nThis systematic application of the DAG scheduling rules allows for the calculation of the critical path for any given configuration. The provided Python program implements this logic for all specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_cp(T, d, h, s, halos_required):\n    \"\"\"\n    Calculates the critical path length for a single PIC time-step DAG.\n\n    Args:\n        T (int): The number of tiles.\n        d (list): Durations of deposition tasks [d_0, d_1, ...].\n        h (list): Durations of halo reduction tasks [h_{0,1}, h_{1,2}, ...].\n        s (list): Durations of local field solve tasks [s_0, s_1, ...].\n        halos_required (bool): Flag indicating if halo dependencies exist.\n\n    Returns:\n        float: The calculated critical path length (CP).\n    \"\"\"\n    d = np.array(d)\n    h = np.array(h)\n    s = np.array(s)\n\n    # The earliest finish times of deposition tasks are their durations.\n    ef_d = d\n\n    # Case where local solves only depend on local deposition.\n    if not halos_required:\n        ef_s = s + ef_d\n        return np.max(ef_s)\n\n    # Case with a single tile. No halo tasks exist.\n    if T == 1:\n        ef_s_0 = s[0] + ef_d[0]\n        return ef_s_0\n\n    # General case with halos required and T > 1.\n\n    # 1. Calculate earliest finish times for halo reduction tasks.\n    # H_{i,i+1} depends on D_i and D_{i+1}.\n    ef_h = np.zeros(T - 1)\n    for i in range(T - 1):\n        # Earliest start time for H_{i,i+1} is max(EF(D_i), EF(D_{i+1}))\n        start_h = np.max(ef_d[i:i+2])\n        ef_h[i] = h[i] + start_h\n\n    # 2. Calculate earliest finish times for local solve tasks.\n    ef_s = np.zeros(T)\n\n    # Tile 0 (left boundary): S_0 depends on D_0 and H_{0,1}.\n    start_s0 = np.max([ef_d[0], ef_h[0]])\n    ef_s[0] = s[0] + start_s0\n\n    # Interior tiles (if any): S_i depends on D_i, H_{i-1,i}, and H_{i,i+1}.\n    for i in range(1, T - 1):\n        start_si = np.max([ef_d[i], ef_h[i-1], ef_h[i]])\n        ef_s[i] = s[i] + start_si\n\n    # Tile T-1 (right boundary): S_{T-1} depends on D_{T-1} and H_{T-2,T-1}.\n    start_s_last = np.max([ef_d[T-1], ef_h[T-2]])\n    ef_s[T-1] = s[T-1] + start_s_last\n\n    # 3. The critical path is the maximum finish time of any solve task.\n    cp = np.max(ef_s)\n    return cp\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes the critical path for each, and prints results.\n    \"\"\"\n    test_cases = [\n        # Case 1: T=4, halos required\n        {'T': 4, 'd': [0.06, 0.04, 0.05, 0.02], 'h': [0.01, 0.015, 0.01], 's': [0.08, 0.07, 0.09, 0.05], 'halos_required': True},\n        # Case 2: T=1, halos required (no neighbors)\n        {'T': 1, 'd': [0.12], 'h': [], 's': [0.20], 'halos_required': True},\n        # Case 3: T=3, halos not required\n        {'T': 3, 'd': [0.03, 0.09, 0.04], 'h': [0.00, 0.00], 's': [0.05, 0.06, 0.05], 'halos_required': False},\n        # Case 4: T=5, halos required, strong imbalance\n        {'T': 5, 'd': [0.02, 0.02, 0.15, 0.03, 0.02], 'h': [0.02, 0.02, 0.02, 0.02], 's': [0.06, 0.06, 0.06, 0.06, 0.06], 'halos_required': True},\n    ]\n\n    results = []\n    for case in test_cases:\n        cp = calculate_cp(case['T'], case['d'], case['h'], case['s'], case['halos_required'])\n        # Round to 6 decimal places as specified\n        results.append(f\"{cp:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}