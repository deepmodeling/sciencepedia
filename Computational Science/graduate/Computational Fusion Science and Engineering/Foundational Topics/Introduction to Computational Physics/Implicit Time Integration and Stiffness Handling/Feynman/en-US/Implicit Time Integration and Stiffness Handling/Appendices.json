{
    "hands_on_practices": [
        {
            "introduction": "The power of implicit methods in handling stiffness comes at a price: each time step requires solving an algebraic system for the future state. This practice takes you through this fundamental procedure using the Backward Euler method on a stiff model equation . By setting up the residual and applying a Newton-Raphson iteration, you will gain hands-on experience with the core computational kernel of implicit solvers.",
            "id": "3992852",
            "problem": "In a lumped zero-dimensional model of resistive relaxation in a fusion plasma energy balance, a linearized perturbation amplitude $y(t)$ obeys the scalar Ordinary Differential Equation (ODE) $y'(t) = f(t,y) = -\\lambda y(t) + S(t)$, where $\\lambda$ represents a strong collisional damping rate and $S(t)$ is an external drive. Consider the case $\\lambda = 10^{6}$ and $S(t) = \\sin(t)$, which is representative of a stiff equation often encountered when discretizing Magnetohydrodynamics (MHD) in implicit time integration. To handle stiffness, use the Backward Euler method with constant time step $\\Delta t = 10^{-2}$.\n\nStarting from the definition of the derivative and the implicit discretization principle underlying Backward Euler, derive the algebraic update equation for $y_{n+1}$ at time $t_{n+1} = t_n + \\Delta t$ in terms of $y_n$ and $t_{n+1}$. Then, formulate the nonlinear residual $R(y)$ whose root is $y_{n+1}$ and compute its Jacobian. Perform one iteration of the Newton–Raphson method starting from the initial guess $y^{(0)} = y_n$ and simplify the resulting expression using $\\Delta t = 10^{-2}$. \n\nExpress your final answer as a single closed-form analytic expression for the one-step Newton iterate $y^{(1)}$ in terms of $y_n$ and $t_{n+1}$. No numerical rounding is required, and do not include units in your final expression.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and all necessary information is provided.\n\n### Step 1: Extract Givens\n- **Governing ODE**: $y'(t) = f(t,y) = -\\lambda y(t) + S(t)$\n- **Damping Rate**: $\\lambda = 10^{6}$\n- **External Drive**: $S(t) = \\sin(t)$\n- **Numerical Method**: Backward Euler\n- **Time Step**: $\\Delta t = 10^{-2}$\n- **Task**: Derive the update equation for $y_{n+1}$, formulate the residual $R(y)$ and its Jacobian $J(y)$, and perform one Newton-Raphson iteration with initial guess $y^{(0)}=y_n$ to find $y^{(1)}$. The final expression for $y^{(1)}$ should be in terms of $y_n$ and $t_{n+1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard stiff ordinary differential equation which is a simplified but representative model in plasma physics. The use of Backward Euler for stiff problems and Newton-Raphson for solving the resulting implicit equation are standard, well-established techniques in numerical analysis and computational science. The problem is well-posed, with all necessary parameters ($\\lambda$, $\\Delta t$) and conditions (initial guess $y^{(0)}=y_n$) specified. The terminology is precise and the objectives are clear. There are no contradictions, ambiguities, or factual errors.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be derived.\n\n### Derivation\nThe problem requires us to solve the stiff ordinary differential equation (ODE) $y'(t) = -\\lambda y(t) + \\sin(t)$ using the implicit Backward Euler method.\n\nThe general form of the Backward Euler method for an ODE $y'(t) = f(t, y(t))$ is given by the discretization:\n$$ \\frac{y_{n+1} - y_n}{\\Delta t} = f(t_{n+1}, y_{n+1}) $$\nwhere $y_n \\approx y(t_n)$, $y_{n+1} \\approx y(t_{n+1})$, and $t_{n+1} = t_n + \\Delta t$.\n\nFor the given ODE, the function on the right-hand side is $f(t, y) = -\\lambda y + \\sin(t)$. Applying the Backward Euler scheme, we get:\n$$ \\frac{y_{n+1} - y_n}{\\Delta t} = -\\lambda y_{n+1} + \\sin(t_{n+1}) $$\nThis is an implicit algebraic equation for the unknown $y_{n+1}$.\n\nThe problem requires us to solve this equation using the Newton-Raphson method. First, we formulate the residual function, $R(y)$, whose root is $y_{n+1}$. We rearrange the equation into the form $R(y_{n+1}) = 0$. Let $y$ be the variable we are solving for, which will be equal to $y_{n+1}$ at the root.\n$$ R(y) = y - y_n - \\Delta t(-\\lambda y + \\sin(t_{n+1})) $$\nExpanding this expression gives the residual:\n$$ R(y) = y - y_n + \\lambda \\Delta t y - \\Delta t \\sin(t_{n+1}) $$\n$$ R(y) = (1 + \\lambda \\Delta t)y - y_n - \\Delta t \\sin(t_{n+1}) $$\n\nNext, we compute the Jacobian of the residual, $J(y)$, which for a scalar function is its derivative with respect to $y$:\n$$ J(y) = \\frac{dR}{dy} $$\n$$ J(y) = \\frac{d}{dy} \\left[ (1 + \\lambda \\Delta t)y - y_n - \\Delta t \\sin(t_{n+1}) \\right] $$\nSince $y_n$ and $t_{n+1}$ are constants in the context of solving for $y$ at the $(n+1)$-th step, the derivative is:\n$$ J(y) = 1 + \\lambda \\Delta t $$\nNote that because the original ODE is linear in $y$, the residual $R(y)$ is linear in $y$, and its Jacobian is a constant, independent of $y$.\n\nThe Newton-Raphson update formula for finding the root of $R(y)$ is:\n$$ y^{(k+1)} = y^{(k)} - [J(y^{(k)})]^{-1} R(y^{(k)}) $$\nFor our scalar problem, this simplifies to:\n$$ y^{(k+1)} = y^{(k)} - \\frac{R(y^{(k)})}{J(y^{(k)})} $$\nWe are asked to perform one iteration starting with the initial guess $y^{(0)} = y_n$. The first iterate, $y^{(1)}$, is:\n$$ y^{(1)} = y^{(0)} - \\frac{R(y^{(0)})}{J(y^{(0)})} = y_n - \\frac{R(y_n)}{J(y_n)} $$\nWe evaluate the residual $R(y)$ at $y=y_n$:\n$$ R(y_n) = (1 + \\lambda \\Delta t)y_n - y_n - \\Delta t \\sin(t_{n+1}) = \\lambda \\Delta t y_n - \\Delta t \\sin(t_{n+1}) $$\nThe Jacobian is constant: $J(y_n) = 1 + \\lambda \\Delta t$.\nSubstituting these expressions into the formula for $y^{(1)}$:\n$$ y^{(1)} = y_n - \\frac{\\lambda \\Delta t y_n - \\Delta t \\sin(t_{n+1})}{1 + \\lambda \\Delta t} $$\nTo simplify, we find a common denominator:\n$$ y^{(1)} = \\frac{y_n (1 + \\lambda \\Delta t) - (\\lambda \\Delta t y_n - \\Delta t \\sin(t_{n+1}))}{1 + \\lambda \\Delta t} $$\n$$ y^{(1)} = \\frac{y_n + \\lambda \\Delta t y_n - \\lambda \\Delta t y_n + \\Delta t \\sin(t_{n+1})}{1 + \\lambda \\Delta t} $$\nThe terms involving $\\lambda \\Delta t y_n$ cancel out, yielding:\n$$ y^{(1)} = \\frac{y_n + \\Delta t \\sin(t_{n+1})}{1 + \\lambda \\Delta t} $$\nBecause the residual function $R(y)$ is linear, the Newton-Raphson method converges in a single iteration. Therefore, $y^{(1)}$ is the exact solution for $y_{n+1}$ in the Backward Euler step.\n\nFinally, we substitute the given numerical values: $\\lambda = 10^6$ and $\\Delta t = 10^{-2}$.\n$$ \\lambda \\Delta t = 10^6 \\times 10^{-2} = 10^4 $$\nThe denominator becomes $1 + \\lambda \\Delta t = 1 + 10^4 = 10001$.\nThe expression for $y^{(1)}$ is then:\n$$ y^{(1)} = \\frac{y_n + 10^{-2} \\sin(t_{n+1})}{10001} $$\nThis is the final closed-form analytic expression for the one-step Newton iterate $y^{(1)}$ in terms of $y_n$ and $t_{n+1}$.",
            "answer": "$$\n\\boxed{\\frac{y_n + 10^{-2} \\sin(t_{n+1})}{10001}}\n$$"
        },
        {
            "introduction": "The computational cost of the solve required in the previous practice is justified by the exceptional stability of implicit methods. This exercise explores this trade-off by analyzing the Crank-Nicolson method, a second-order scheme widely used in physics simulations . You will derive its amplification factor to understand how it handles stiff dynamics, revealing both its strengths (A-stability) and its critical weaknesses (spurious oscillations).",
            "id": "3992877",
            "problem": "Consider parallel electron heat conduction in a high-temperature magnetized fusion plasma along a single magnetic field line, modeled as one-dimensional diffusion on a periodic domain of length $L$. The electron temperature $T(x,t)$ obeys the linear diffusion equation $ \\partial T / \\partial t = \\chi \\, \\partial^{2} T / \\partial x^{2} $, where $\\chi  0$ is the (constant) parallel thermal diffusivity. For a single Fourier mode $T(x,t) = \\hat{T}(t) \\exp(i k x)$ with wavenumber $k = 2 \\pi m / L$ (integer $m$), the spatial operator reduces to an eigenvalue problem, giving the ordinary differential equation $ d \\hat{T} / d t = \\lambda \\, \\hat{T} $ with $\\lambda = - \\chi k^{2}$.\n\nAn implicit trapezoidal time integration, also known as the Crank–Nicolson (CN) method, updates the mode amplitude $\\hat{T}^{n}$ at time $t^{n}$ to $\\hat{T}^{n+1}$ at time $t^{n+1} = t^{n} + \\Delta t$ by averaging the right-hand side between $t^{n}$ and $t^{n+1}$. Define the non-dimensional step $z = \\lambda \\, \\Delta t$. Derive the CN amplification factor $G(z)$ for this linear mode, where $G(z) = \\hat{T}^{n+1} / \\hat{T}^{n}$, starting from the fundamental definition of the trapezoidal rule for linear ordinary differential equations. Then, verify analytically that the CN method exhibits zero numerical dissipation for purely imaginary $z$ in the sense that $|G(i \\beta)| = 1$ for any real $\\beta$, and explain the implications this has for stiffness handling of diffusive dynamics in fusion-relevant simulations where high-wavenumber modes make $|z|$ large and negative.\n\nExpress the final result as the closed-form expression for $G(z)$. No numerical evaluation is required. No rounding is required. Do not include any units in your final expression.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a complete solution. It describes a standard analysis of the Crank-Nicolson numerical method applied to a linear diffusion equation, a fundamental topic in computational science. We shall proceed with the derivation and analysis as requested.\n\nThe analysis is divided into three parts: first, the derivation of the amplification factor $G(z)$; second, the verification of its magnitude for a purely imaginary argument; and third, an explanation of the implications for simulating stiff systems.\n\n**Part 1: Derivation of the Amplification Factor $G(z)$**\n\nWe begin with the ordinary differential equation (ODE) governing the time evolution of a single Fourier mode amplitude, $\\hat{T}(t)$:\n$$\n\\frac{d \\hat{T}}{d t} = \\lambda \\hat{T}\n$$\nwhere $\\lambda = -\\chi k^2$ is the eigenvalue associated with the spatial mode of wavenumber $k$.\n\nThe implicit trapezoidal rule, or Crank-Nicolson (CN) method, discretizes this ODE in time. The value at the new time step, $\\hat{T}^{n+1}$, is related to the value at the old time step, $\\hat{T}^{n}$, by approximating the time derivative at the midpoint of the interval $[t^n, t^{n+1}]$ and evaluating the right-hand side as the average of its values at $t^n$ and $t^{n+1}$. For a general ODE $\\frac{dy}{dt} = f(y)$, the scheme is:\n$$\n\\frac{y^{n+1} - y^{n}}{\\Delta t} = \\frac{1}{2} \\left[ f(y^{n}) + f(y^{n+1}) \\right]\n$$\nIn our specific case, $y = \\hat{T}$ and the function is linear, $f(\\hat{T}) = \\lambda \\hat{T}$. Substituting this into the general form gives:\n$$\n\\frac{\\hat{T}^{n+1} - \\hat{T}^{n}}{\\Delta t} = \\frac{1}{2} \\left[ \\lambda \\hat{T}^{n} + \\lambda \\hat{T}^{n+1} \\right]\n$$\nThis equation is implicit in $\\hat{T}^{n+1}$, so we must rearrange it to solve for $\\hat{T}^{n+1}$.\n$$\n\\hat{T}^{n+1} - \\hat{T}^{n} = \\frac{\\lambda \\Delta t}{2} \\left( \\hat{T}^{n} + \\hat{T}^{n+1} \\right)\n$$\nWe group terms involving $\\hat{T}^{n+1}$ on the left-hand side and terms involving $\\hat{T}^{n}$ on the right-hand side:\n$$\n\\hat{T}^{n+1} - \\frac{\\lambda \\Delta t}{2} \\hat{T}^{n+1} = \\hat{T}^{n} + \\frac{\\lambda \\Delta t}{2} \\hat{T}^{n}\n$$\nFactoring out $\\hat{T}^{n+1}$ and $\\hat{T}^{n}$:\n$$\n\\hat{T}^{n+1} \\left( 1 - \\frac{\\lambda \\Delta t}{2} \\right) = \\hat{T}^{n} \\left( 1 + \\frac{\\lambda \\Delta t}{2} \\right)\n$$\nThe amplification factor $G$ is defined as the ratio $G = \\hat{T}^{n+1} / \\hat{T}^{n}$. Solving for this ratio yields:\n$$\nG = \\frac{1 + \\frac{\\lambda \\Delta t}{2}}{1 - \\frac{\\lambda \\Delta t}{2}}\n$$\nThe problem defines the non-dimensional parameter $z = \\lambda \\Delta t$. Substituting this definition, we obtain the final expression for the amplification factor:\n$$\nG(z) = \\frac{1 + \\frac{z}{2}}{1 - \\frac{z}{2}}\n$$\n\n**Part 2: Verification of $|G(i \\beta)| = 1$**\n\nWe are asked to verify that for a purely imaginary argument $z = i \\beta$, where $\\beta$ is any real number, the magnitude of the amplification factor is exactly unity. This property is characteristic of a scheme that conserves energy for purely oscillatory (non-dissipative) systems. Let's substitute $z=i\\beta$ into the expression for $G(z)$:\n$$\nG(i\\beta) = \\frac{1 + \\frac{i\\beta}{2}}{1 - \\frac{i\\beta}{2}}\n$$\nThe magnitude of a ratio of complex numbers is the ratio of their magnitudes. The magnitude of a complex number $a + ib$ is $|a+ib| = \\sqrt{a^2 + b^2}$.\n$$\n|G(i\\beta)| = \\frac{\\left| 1 + i\\frac{\\beta}{2} \\right|}{\\left| 1 - i\\frac{\\beta}{2} \\right|}\n$$\nThe magnitude of the numerator is:\n$$\n\\left| 1 + i\\frac{\\beta}{2} \\right| = \\sqrt{1^2 + \\left(\\frac{\\beta}{2}\\right)^2} = \\sqrt{1 + \\frac{\\beta^2}{4}}\n$$\nThe magnitude of the denominator is:\n$$\n\\left| 1 - i\\frac{\\beta}{2} \\right| = \\sqrt{1^2 + \\left(-\\frac{\\beta}{2}\\right)^2} = \\sqrt{1 + \\frac{\\beta^2}{4}}\n$$\nSince the numerator and the denominator have the same magnitude, their ratio is $1$:\n$$\n|G(i\\beta)| = \\frac{\\sqrt{1 + \\frac{\\beta^2}{4}}}{\\sqrt{1 + \\frac{\\beta^2}{4}}} = 1\n$$\nThis confirms analytically that the Crank-Nicolson method is perfectly non-dissipative for any system where the eigenvalues $\\lambda$ are purely imaginary.\n\n**Part 3: Implications for Stiffness Handling**\n\nStiffness in the context of the given diffusion problem arises from the wide separation of decay timescales associated with different Fourier modes. The decay time for a mode with wavenumber $k$ is $\\tau_k = 1/|\\lambda| = 1/(\\chi k^2)$. High-wavenumber (short wavelength) modes have very short decay times (fast dynamics), while low-wavenumber (long wavelength) modes have long decay times (slow dynamics). A numerical method must be able to handle this disparity.\n\nFor the diffusion problem, the eigenvalue is $\\lambda = -\\chi k^2$, which is real and non-positive. Therefore, the non-dimensional step is $z = \\lambda \\Delta t = -(\\chi k^2) \\Delta t \\le 0$. A numerical scheme is stable if $|G(z)| \\le 1$ for all such $z$. For the CN method with real $z \\le 0$:\n$$\n|G(z)| = \\left| \\frac{1 + z/2}{1 - z/2} \\right|\n$$\nSince $z \\le 0$, the denominator $1 - z/2$ is always $\\ge 1$. The numerator $1+z/2$ can be positive or negative. However, it is straightforward to show that $|1+z/2| \\le |1-z/2|$ for all $z \\le 0$, which means $|G(z)| \\le 1$. The scheme is unconditionally stable for diffusion, also known as A-stable. This is a primary advantage for stiff problems, as it allows the time step $\\Delta t$ to be chosen based on the accuracy requirements of the slow, physically relevant modes, rather than being restricted by the stability limit of the fastest, often unresolved modes.\n\nHowever, the property $|G(i\\beta)|=1$ has a critical consequence for stiff diffusive dynamics. This property implies that the CN method maps the stability boundary of the continuous system (the imaginary axis, $\\text{Re}(z)=0$) exactly onto the stability boundary of the discrete system (the unit circle, $|G|=1$). For a very stiff diffusive mode, the wavenumber $k$ is large, making $|\\lambda|$ very large. If a large time step $\\Delta t$ is used, the parameter $z = -\\chi k^2 \\Delta t$ becomes a negative number with very large magnitude, i.e., $z \\to -\\infty$.\n\nLet's examine the behavior of $G(z)$ in this limit:\n$$\n\\lim_{z \\to -\\infty} G(z) = \\lim_{z \\to -\\infty} \\frac{1 + z/2}{1 - z/2} = \\lim_{z \\to -\\infty} \\frac{z(1/z + 1/2)}{z(1/z - 1/2)} = \\frac{1/2}{-1/2} = -1\n$$\nThis means that for very stiff components, the numerical solution does not decay; instead, it flips its sign at each time step: $\\hat{T}^{n+1} \\approx -\\hat{T}^n$. While this behavior is stable (the magnitude does not grow), it fails to damp the high-frequency modes as they should be physically. The energy in these stiff modes persists as unphysical, high-frequency oscillations (often called \"ringing\"), which can corrupt the accuracy of the overall solution. The ideal behavior for stiff dissipative components is for $|G(z)| \\to 0$ as $z \\to -\\infty$ (a property known as L-stability), which would rapidly damp them out.\n\nIn summary, the property $|G(i\\beta)|=1$, which makes the CN method excellent for non-dissipative wave phenomena, is directly responsible for its poor damping of stiff diffusive modes. The unconditional stability allows for large time steps, but this comes at the cost of introducing spurious numerical oscillations from the undamped stiff components. This trade-off is a central consideration in choosing an implicit integrator for multiscale fusion plasma simulations.",
            "answer": "$$\n\\boxed{\\frac{1 + \\frac{z}{2}}{1 - \\frac{z}{2}}}\n$$"
        },
        {
            "introduction": "After analyzing a method's theoretical properties, we must verify that our code behaves as expected, a crucial step known as code verification. This practice guides you through this process for the Crank-Nicolson method using a manufactured solution . By comparing the numerical results against the exact solution at different time step sizes, you will compute the observed order of convergence and gain confidence in your implementation.",
            "id": "3992885",
            "problem": "You are asked to analyze and verify the temporal convergence of a second-order implicit time integration method on a stiff linear ordinary differential equation (ODE) representative of collisional relaxation in computational fusion science and engineering. Consider the scalar initial value problem\n$$\n\\frac{du}{dt} = \\lambda\\, u(t), \\quad u(0) = u_0,\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is constant and negative, modeling a dissipative sink term. The manufactured exact solution is\n$$\nu_{\\text{exact}}(t) = u_0\\, e^{\\lambda t}.\n$$\nUse the Crank–Nicolson (trapezoidal) method, which is a second-order implicit method, applied to the semi-discrete system with time step $\\Delta t$, producing the update\n$$\n\\frac{u^{n+1} - u^n}{\\Delta t} = \\frac{\\lambda}{2}\\left(u^{n+1} + u^n\\right),\n$$\nfor $n = 0,1,\\dots,N-1$, where $N$ is the number of steps such that $N \\Delta t = T$.\n\nTasks:\n- Starting from the fundamental definitions of local truncation error and Taylor expansion of smooth functions, derive the expected temporal convergence order $p$ for this second-order implicit method when applied to the manufactured solution $u_{\\text{exact}}(t)$ above. Your derivation must begin from the base facts that the local truncation error scales as a power of $\\Delta t$ for smooth $u(t)$ and that the global error is determined by stability and accumulation of local truncation error.\n- Implement the numerical integration using the Crank–Nicolson update to advance $u^n$ from $t^n$ to $t^{n+1}$ over $N$ steps to reach the final time $T$. Compute the absolute error at $t = T$ for two different time steps $\\Delta t_1$ and $\\Delta t_2$ for each test case:\n$$\nE(T,\\Delta t) = \\left| u_{\\text{num}}(T;\\Delta t) - u_{\\text{exact}}(T) \\right|.\n$$\n- Compute the observed temporal convergence rate using the two-step error ratio formula\n$$\nr = \\frac{\\log\\left(E(T,\\Delta t_1) / E(T,\\Delta t_2)\\right)}{\\log\\left(\\Delta t_1 / \\Delta t_2\\right)}.\n$$\nAssume all solutions and derivatives are smooth in time. Express all outputs as floating-point numbers without physical units.\n\nYour program must produce a single line of output containing the observed convergence rates for the specified test suite as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$).\n\nTest Suite:\nUse the following test cases, each defined by the tuple $(\\lambda, u_0, T, \\Delta t_1, \\Delta t_2)$:\n- Case $1$ (general accuracy verification): $(\\lambda, u_0, T, \\Delta t_1, \\Delta t_2) = (-1, 1, 1, 0.1, 0.05)$.\n- Case $2$ (moderately stiff): $(\\lambda, u_0, T, \\Delta t_1, \\Delta t_2) = (-1000, 1, 0.1, 0.001, 0.0005)$.\n- Case $3$ (very stiff, small final time): $(\\lambda, u_0, T, \\Delta t_1, \\Delta t_2) = (-1000000, 1, 0.001, 0.00001, 0.000005)$.\n\nDesign for coverage:\n- Case $1$ is a happy-path scenario with non-stiff dynamics.\n- Case $2$ verifies behavior under stiffness with sufficiently small $\\Delta t$ to avoid error pollution from non-$L$-stable damping.\n- Case $3$ stresses stiffness further while keeping $N$ finite and computationally feasible.\n\nAnswer specification:\n- For each case, compute $E(T,\\Delta t_1)$ and $E(T,\\Delta t_2)$ and then compute $r$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3]$.",
            "solution": "We consider the scalar initial value problem\n$$\n\\frac{du}{dt} = \\lambda\\, u(t), \\quad u(0) = u_0,\n$$\nwith manufactured exact solution\n$$\nu_{\\text{exact}}(t) = u_0\\, e^{\\lambda t}.\n$$\nThis model represents a stiff collisional relaxation term in computational fusion science and engineering when $\\lambda$ is large and negative. We analyze the Crank–Nicolson (trapezoidal) method, a second-order implicit method, defined by\n$$\n\\frac{u^{n+1} - u^n}{\\Delta t} = \\frac{\\lambda}{2}\\left(u^{n+1} + u^n\\right),\n$$\nfor $n = 0,1,\\dots,N-1$ with $N \\Delta t = T$.\n\nPrinciple-based derivation of expected convergence order:\n- Begin from the definition of local truncation error. Let $u(t)$ be the exact solution, assumed smooth. Substitute $u(t^{n+1})$ and $u(t^n)$ into the method:\n$$\n\\frac{u(t^{n+1}) - u(t^n)}{\\Delta t} - \\frac{\\lambda}{2}\\left(u(t^{n+1}) + u(t^n)\\right) = \\tau^{n+1},\n$$\nwhere $\\tau^{n+1}$ is the local truncation error at step $n+1$.\n- Use Taylor expansion about $t^n$:\n$$\nu(t^{n+1}) = u(t^n) + \\Delta t\\, u'(t^n) + \\frac{\\Delta t^2}{2}\\, u''(t^n) + \\frac{\\Delta t^3}{6}\\, u^{(3)}(t^n) + \\mathcal{O}(\\Delta t^4).\n$$\n- Substitute $u'(t) = \\lambda u(t)$, $u''(t) = \\lambda^2 u(t)$, and $u^{(3)}(t) = \\lambda^3 u(t)$ to obtain\n$$\n\\frac{u(t^{n+1}) - u(t^n)}{\\Delta t} = u'(t^n) + \\frac{\\Delta t}{2} u''(t^n) + \\frac{\\Delta t^2}{6} u^{(3)}(t^n) + \\mathcal{O}(\\Delta t^3),\n$$\nand\n$$\n\\frac{\\lambda}{2}\\left(u(t^{n+1}) + u(t^n)\\right) = \\lambda u(t^n) + \\frac{\\Delta t}{2} \\lambda u'(t^n) + \\frac{\\Delta t^2}{4} \\lambda u''(t^n) + \\mathcal{O}(\\Delta t^3).\n$$\n- Noting $u'(t^n) = \\lambda u(t^n)$ and $u''(t^n) = \\lambda^2 u(t^n)$, we observe the first-order terms cancel, and the defect is dominated by the $\\Delta t^2$ term:\n$$\n\\tau^{n+1} = \\left(\\frac{\\Delta t^2}{6} - \\frac{\\Delta t^2}{4}\\right) \\lambda^3 u(t^n) + \\mathcal{O}(\\Delta t^3) = -\\frac{\\Delta t^2}{12}\\lambda^3 u(t^n) + \\mathcal{O}(\\Delta t^3).\n$$\nThis demonstrates that the local truncation error, $\\tau^{n+1}$, is of order $\\mathcal{O}(\\Delta t^2)$.\n- Under zero-stability and appropriate stability for stiff problems (the trapezoidal rule is known to be $A$-stable, meaning its linear stability region covers the entire left-half complex plane), the global error after $N$ steps over a fixed final time $T = N \\Delta t$ scales as\n$$\nE(T, \\Delta t) = \\left|u_{\\text{num}}(T;\\Delta t) - u_{\\text{exact}}(T)\\right| = C(T,\\lambda,u_0)\\, \\Delta t^2 + \\mathcal{O}(\\Delta t^3),\n$$\nfor a bounded constant $C(T,\\lambda,u_0)$ depending on the solution and problem data. Therefore, the expected temporal convergence order is\n$$\np = 2.\n$$\n\nClosed-form update and stability:\n- Rearranging the Crank–Nicolson update yields\n$$\nu^{n+1}(1 - \\frac{\\lambda \\Delta t}{2}) = u^n(1 + \\frac{\\lambda \\Delta t}{2}),\n$$\nso the amplification factor per time step is\n$$\nR(z) = \\frac{1 + \\frac{z}{2}}{1 - \\frac{z}{2}}, \\quad z = \\lambda \\Delta t.\n$$\nThus,\n$$\nu_{\\text{num}}(T;\\Delta t) = u^N = u_0\\, R(\\lambda \\Delta t)^N, \\quad N = \\frac{T}{\\Delta t}.\n$$\n- For $\\lambda  0$, $R(z)$ satisfies $\\lvert R(z) \\rvert \\le 1$ for $\\text{Re}(z) \\le 0$, which ensures $A$-stability. However, as $z \\to -\\infty$, $R(z) \\to -1$, demonstrating the method is not $L$-stable (the damping does not vanish for extremely stiff modes), which can slow error decay for very stiff cases but does not destroy stability.\n\nObserved rate computation:\n- For two time steps $\\Delta t_1$ and $\\Delta t_2$ with $\\Delta t_2  \\Delta t_1$, compute\n$$\nE_1 = E(T,\\Delta t_1), \\quad E_2 = E(T,\\Delta t_2),\n$$\nand use\n$$\nr = \\frac{\\log(E_1/E_2)}{\\log(\\Delta t_1/\\Delta t_2)}.\n$$\nAs $\\Delta t \\to 0$, one expects $r \\to 2$.\n\nAlgorithmic plan:\n- For each test case $(\\lambda, u_0, T, \\Delta t_1, \\Delta t_2)$:\n  - Compute $N_1 = T/\\Delta t_1$ and $N_2 = T/\\Delta t_2$ (integers by construction).\n  - Compute $R_1 = (1 + \\lambda \\Delta t_1/2)/(1 - \\lambda \\Delta t_1/2)$ and $R_2 = (1 + \\lambda \\Delta t_2/2)/(1 - \\lambda \\Delta t_2/2)$.\n  - Compute $u_{\\text{num},1}(T) = u_0\\, R_1^{N_1}$ and $u_{\\text{num},2}(T) = u_0\\, R_2^{N_2}$.\n  - Compute $u_{\\text{exact}}(T) = u_0\\, e^{\\lambda T}$.\n  - Compute $E_1 = \\lvert u_{\\text{num},1}(T) - u_{\\text{exact}}(T)\\rvert$ and $E_2 = \\lvert u_{\\text{num},2}(T) - u_{\\text{exact}}(T)\\rvert$.\n  - Compute $r = \\log(E_1/E_2)/\\log(\\Delta t_1/\\Delta t_2)$.\n- Output the list $[r_1,r_2,r_3]$ on a single line.\n\nCoverage rationale:\n- Case $1$ confirms the method’s nominal second-order accuracy in a non-stiff regime.\n- Case $2$ exercises stiffness with time steps small enough to observe second-order behavior without significant non-$L$-stable artifacts.\n- Case $3$ increases stiffness further; although the method remains stable, error damping can be slower, yet the observed order should remain close to $2$ provided the steps are sufficiently small relative to the dynamics.\n\nNo physical units are required, and all angles or percentages are not applicable. All outputs are floating-point numbers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef crank_nicolson_final(u0: float, lam: float, T: float, dt: float) - float:\n    \"\"\"\n    Compute the final value u(T) using the Crank–Nicolson method for u' = lam * u.\n    Uses the closed-form amplification factor per step to avoid iteration overhead.\n    \"\"\"\n    # Number of steps; assume T is exactly divisible by dt in provided test cases\n    N = int(round(T / dt))\n    # Guard against numerical mismatch\n    if not np.isclose(N * dt, T, rtol=0, atol=1e-15):\n        raise ValueError(\"T must be an integer multiple of dt.\")\n    # Amplification factor R = (1 + lam * dt / 2) / (1 - lam * dt / 2)\n    R = (1.0 + lam * dt / 2.0) / (1.0 - lam * dt / 2.0)\n    # Final value\n    uT = u0 * (R ** N)\n    return uT\n\ndef exact_solution(u0: float, lam: float, T: float) - float:\n    return u0 * np.exp(lam * T)\n\ndef observed_rate(u0: float, lam: float, T: float, dt1: float, dt2: float) - float:\n    \"\"\"\n    Compute the observed convergence rate r from two time steps dt1 and dt2.\n    \"\"\"\n    u_num1 = crank_nicolson_final(u0, lam, T, dt1)\n    u_num2 = crank_nicolson_final(u0, lam, T, dt2)\n    u_ex = exact_solution(u0, lam, T)\n    E1 = abs(u_num1 - u_ex)\n    E2 = abs(u_num2 - u_ex)\n    # Avoid division by zero or log of non-positive by enforcing minimal positive floor\n    eps = 1e-300\n    E1 = max(E1, eps)\n    E2 = max(E2, eps)\n    r = np.log(E1 / E2) / np.log(dt1 / dt2)\n    return float(r)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (lambda, u0, T, dt1, dt2)\n    test_cases = [\n        (-1.0, 1.0, 1.0, 0.1, 0.05),           # Case 1: non-stiff, happy path\n        (-1000.0, 1.0, 0.1, 0.001, 0.0005),    # Case 2: moderately stiff\n        (-1_000_000.0, 1.0, 0.001, 1e-5, 5e-6) # Case 3: very stiff, small T\n    ]\n\n    results = []\n    for lam, u0, T, dt1, dt2 in test_cases:\n        r = observed_rate(u0, lam, T, dt1, dt2)\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}