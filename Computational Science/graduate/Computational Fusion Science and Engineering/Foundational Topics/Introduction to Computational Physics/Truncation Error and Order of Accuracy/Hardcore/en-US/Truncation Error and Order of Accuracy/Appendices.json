{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of analyzing numerical methods is the ability to derive the truncation error from first principles. This practice will guide you through the process using Taylor series expansions, a fundamental tool in computational science. By analyzing a fourth-order accurate stencil, common in high-fidelity simulations, you will gain a concrete understanding of how the specific weights in a finite difference formula are chosen to cancel lower-order error terms and achieve a desired level of accuracy .",
            "id": "4149911",
            "problem": "Consider the one-dimensional ($1\\mathrm{D}$) linearized acoustic system for small-amplitude waves in a homogeneous, inviscid medium with constant density $\\rho_{0}$ and sound speed $c$,\n$$\np_{t}(x,t)+\\rho_{0}c^{2}u_{x}(x,t)=0,\\qquad u_{t}(x,t)+\\frac{1}{\\rho_{0}}p_{x}(x,t)=0,\n$$\nwhere $p(x,t)$ is the acoustic pressure and $u(x,t)$ is the particle velocity. In explicit finite difference time integration schemes such as the Finite Difference Time Domain (FDTD) method, spatial derivatives like $p_{x}(x,t)$ are approximated on a uniform grid $x_{i}=x_{0}+ih$ with grid spacing $h0$. At a fixed time $t$ (suppressed for notational simplicity), define the fourth-order centered finite difference approximation for $p_{x}(x)$ at grid point $x_{i}$ by\n$$\nD_{x}^{(4)}p_{i}:=\\frac{-p(x_{i}+2h)+8\\,p(x_{i}+h)-8\\,p(x_{i}-h)+p(x_{i}-2h)}{12\\,h}.\n$$\nAssume $p(x)$ is sufficiently smooth (at least five times continuously differentiable in a neighborhood of $x_{i}$). Starting from Taylor’s theorem and the smoothness of $p(x)$, derive the leading truncation error term of the discrete derivative $D_{x}^{(4)}p_{i}$ relative to the exact derivative $p_{x}(x_{i})$, and demonstrate that the truncation error is $O(h^{4})$ with a nonzero constant prefactor multiplying $h^{4}$. Express your final answer as a single closed-form analytic expression for the leading term of the truncation error $\\tau_{i}:=D_{x}^{(4)}p_{i}-p_{x}(x_{i})$, in terms of $h$ and the appropriate derivative of $p$ evaluated at $x_{i}$. No numerical rounding is required. The final answer must be a single analytic expression with no units.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It consists of a standard derivation in numerical analysis applied to a common scenario in computational physics.\n\nThe objective is to derive the leading term of the truncation error $\\tau_i$ for the fourth-order centered finite difference approximation of the first derivative of a function $p(x)$ at grid point $x_i$. The truncation error is defined as the difference between the discrete approximation and the exact continuous derivative:\n$$\n\\tau_{i} := D_{x}^{(4)}p_{i} - \\frac{dp}{dx}(x_{i})\n$$\nwhere the discrete operator is given by:\n$$\nD_{x}^{(4)}p_{i} = \\frac{-p(x_{i}+2h)+8p(x_{i}+h)-8p(x_{i}-h)+p(x_{i}-2h)}{12h}\n$$\nThe problem states that $p(x)$ is at least five times continuously differentiable, which justifies the use of Taylor series expansions around the point $x_i$. We expand the terms $p(x_i \\pm h)$ and $p(x_i \\pm 2h)$ that appear in the numerator of the operator. Let $p^{(k)}(x_i)$ denote the $k$-th derivative of $p(x)$ with respect to $x$, evaluated at $x=x_i$.\n\nThe Taylor series expansion for $p(x_i + \\delta)$ around $x_i$ is:\n$$\np(x_i + \\delta) = p(x_i) + \\delta p^{(1)}(x_i) + \\frac{\\delta^2}{2!} p^{(2)}(x_i) + \\frac{\\delta^3}{3!} p^{(3)}(x_i) + \\frac{\\delta^4}{4!} p^{(4)}(x_i) + \\frac{\\delta^5}{5!} p^{(5)}(x_i) + O(\\delta^6)\n$$\nWe apply this expansion for $\\delta = h, -h, 2h, -2h$:\nFor $\\delta = h$:\n$$\np(x_{i}+h) = p(x_i) + h p^{(1)}(x_i) + \\frac{h^2}{2} p^{(2)}(x_i) + \\frac{h^3}{6} p^{(3)}(x_i) + \\frac{h^4}{24} p^{(4)}(x_i) + \\frac{h^5}{120} p^{(5)}(x_i) + O(h^6)\n$$\nFor $\\delta = -h$:\n$$\np(x_{i}-h) = p(x_i) - h p^{(1)}(x_i) + \\frac{h^2}{2} p^{(2)}(x_i) - \\frac{h^3}{6} p^{(3)}(x_i) + \\frac{h^4}{24} p^{(4)}(x_i) - \\frac{h^5}{120} p^{(5)}(x_i) + O(h^6)\n$$\nFor $\\delta = 2h$:\n$$\np(x_{i}+2h) = p(x_i) + 2h p^{(1)}(x_i) + \\frac{(2h)^2}{2} p^{(2)}(x_i) + \\frac{(2h)^3}{6} p^{(3)}(x_i) + \\frac{(2h)^4}{24} p^{(4)}(x_i) + \\frac{(2h)^5}{120} p^{(5)}(x_i) + O(h^6)\n$$\n$$\np(x_{i}+2h) = p(x_i) + 2h p^{(1)}(x_i) + 2h^2 p^{(2)}(x_i) + \\frac{4h^3}{3} p^{(3)}(x_i) + \\frac{2h^4}{3} p^{(4)}(x_i) + \\frac{4h^5}{15} p^{(5)}(x_i) + O(h^6)\n$$\nFor $\\delta = -2h$:\n$$\np(x_{i}-2h) = p(x_i) - 2h p^{(1)}(x_i) + \\frac{(-2h)^2}{2} p^{(2)}(x_i) + \\frac{(-2h)^3}{6} p^{(3)}(x_i) + \\frac{(-2h)^4}{24} p^{(4)}(x_i) + \\frac{(-2h)^5}{120} p^{(5)}(x_i) + O(h^6)\n$$\n$$\np(x_{i}-2h) = p(x_i) - 2h p^{(1)}(x_i) + 2h^2 p^{(2)}(x_i) - \\frac{4h^3}{3} p^{(3)}(x_i) + \\frac{2h^4}{3} p^{(4)}(x_i) - \\frac{4h^5}{15} p^{(5)}(x_i) + O(h^6)\n$$\nNow, we substitute these expansions into the numerator of $D_{x}^{(4)}p_{i}$:\n$$\n\\text{Numerator} = -p(x_{i}+2h)+8p(x_{i}+h)-8p(x_{i}-h)+p(x_{i}-2h)\n$$\nWe collect coefficients for each derivative of $p$ at $x_i$:\n-   Coefficient of $p(x_i)$: $-1 + 8 - 8 + 1 = 0$.\n-   Coefficient of $p^{(1)}(x_i)$: $h(-2 + 8 \\cdot 1 - 8 \\cdot (-1) + 1 \\cdot (-2)) = h(-2 + 8 + 8 - 2) = 12h$.\n-   Coefficient of $p^{(2)}(x_i)$: $h^2(-2 + 8 \\cdot \\frac{1}{2} - 8 \\cdot \\frac{1}{2} + 1 \\cdot 2) = h^2(-2 + 4 - 4 + 2) = 0$.\n-   Coefficient of $p^{(3)}(x_i)$: $h^3(-\\frac{4}{3} + 8 \\cdot \\frac{1}{6} - 8 \\cdot (-\\frac{1}{6}) + 1 \\cdot (-\\frac{4}{3})) = h^3(-\\frac{4}{3} + \\frac{4}{3} + \\frac{4}{3} - \\frac{4}{3}) = 0$.\n-   Coefficient of $p^{(4)}(x_i)$: $h^4(-\\frac{2}{3} + 8 \\cdot \\frac{1}{24} - 8 \\cdot \\frac{1}{24} + 1 \\cdot \\frac{2}{3}) = h^4(-\\frac{2}{3} + \\frac{1}{3} - \\frac{1}{3} + \\frac{2}{3}) = 0$.\n-   Coefficient of $p^{(5)}(x_i)$: $h^5(-\\frac{4}{15} + 8 \\cdot \\frac{1}{120} - 8 \\cdot (-\\frac{1}{120}) + 1 \\cdot (-\\frac{4}{15})) = h^5(-\\frac{4}{15} + \\frac{8}{120} + \\frac{8}{120} - \\frac{4}{15}) = h^5(-\\frac{8}{15} + \\frac{16}{120}) = h^5(-\\frac{64}{120} + \\frac{16}{120}) = -\\frac{48}{120}h^5 = -\\frac{2}{5}h^5$.\n\nThe odd-powered error terms (second, fourth derivatives) for a centered difference scheme for a first derivative must be zero, as confirmed. The next non-zero error term after the first derivative involves $p^{(3)}(x_i)$, but the specific weights of this fourth-order scheme are constructed to cancel this term as well. The next term, from $p^{(5)}(x_i)$, is the leading error term.\nThe numerator can thus be expressed as:\n$$\n\\text{Numerator} = 12h p^{(1)}(x_i) - \\frac{2}{5}h^5 p^{(5)}(x_i) + O(h^7)\n$$\nThe $O(h^7)$ term arises from the seventh derivative of $p(x)$, since the coefficient for the sixth derivative term also cancels out for this symmetric stencil.\n\nNow we can write the expression for the full discrete operator:\n$$\nD_{x}^{(4)}p_{i} = \\frac{12h p^{(1)}(x_i) - \\frac{2}{5}h^5 p^{(5)}(x_i) + O(h^7)}{12h}\n$$\n$$\nD_{x}^{(4)}p_{i} = p^{(1)}(x_i) - \\frac{2}{5 \\cdot 12} h^4 p^{(5)}(x_i) + O(h^6)\n$$\n$$\nD_{x}^{(4)}p_{i} = p^{(1)}(x_i) - \\frac{1}{30} h^4 p^{(5)}(x_i) + O(h^6)\n$$\nThe truncation error $\\tau_i$ is found by subtracting the exact derivative $p^{(1)}(x_i)$ (which is $p_x(x_i)$ in the problem's notation):\n$$\n\\tau_i = D_{x}^{(4)}p_{i} - p^{(1)}(x_i) = \\left( p^{(1)}(x_i) - \\frac{1}{30} h^4 p^{(5)}(x_i) + O(h^6) \\right) - p^{(1)}(x_i)\n$$\n$$\n\\tau_i = - \\frac{1}{30} h^4 p^{(5)}(x_i) + O(h^6)\n$$\nThis demonstrates that the truncation error is of order $h^4$, i.e., $\\tau_i = O(h^4)$, as the leading error term is proportional to $h^4$. The prefactor $-\\frac{1}{30}p^{(5)}(x_i)$ is non-zero for any function $p(x)$ for which the fifth derivative is non-zero, confirming the assertion.\n\nThe leading term of the truncation error is therefore the first non-zero term in the expansion, which is $-\\frac{1}{30}h^4 p^{(5)}(x_i)$.",
            "answer": "$$\n\\boxed{-\\frac{h^{4}}{30} p^{(5)}(x_i)}\n$$"
        },
        {
            "introduction": "The formal order of accuracy, derived via Taylor series, relies on the assumption that the function being differentiated is sufficiently smooth. This thought-provoking exercise challenges that assumption by applying a standard central difference scheme to a function with a discontinuous third derivative. By directly calculating the error, you will discover that the *effective* order of accuracy can be lower than the formal order, a crucial lesson in the practical application of numerical methods where solutions may have limited smoothness .",
            "id": "2421857",
            "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ be defined by $f(x)=|x|^{3}$. Consider a uniform grid with spacing $h0$ and the three-point central finite difference approximation for the second derivative of $f$ at $x=0$ given by\n$$\nD_{h}^{(2)}f(0)=\\frac{f(h)-2f(0)+f(-h)}{h^{2}}.\n$$\nDefine the effective order of accuracy $p$ of this approximation at $x=0$ as the largest real number $p$ such that the pointwise error $E(h)=\\left|D_{h}^{(2)}f(0)-f''(0)\\right|$ satisfies $E(h)=\\mathcal{O}(h^{p})$ as $h\\to 0^{+}$. It is known that $f''(0)$ exists but $f'''(0)$ is undefined.\n\nDetermine the exact value of $p$. Provide your answer as a single number with no units. No rounding is required.",
            "solution": "The problem statement presented for analysis is first subjected to a rigorous validation process.\n\nThe givens are as follows:\n- The function is $f(x) = |x|^3$ for $x \\in \\mathbb{R}$.\n- The finite difference approximation for the second derivative at $x=0$ is $D_{h}^{(2)}f(0) = \\frac{f(h) - 2f(0) + f(-h)}{h^2}$, for a grid spacing $h0$.\n- The pointwise error is $E(h) = |D_{h}^{(2)}f(0) - f''(0)|$.\n- The effective order of accuracy, $p$, is defined as the largest real number such that $E(h) = \\mathcal{O}(h^p)$ as $h \\to 0^+$.\n- It is stated that $f''(0)$ exists and $f'''(0)$ is undefined.\n\nAn initial verification of the stated properties of the function $f(x)=|x|^3$ is required.\nFor $x \\neq 0$, the derivatives are straightforward. For $x0$, $f(x)=x^3$, which gives $f'(x)=3x^2$ and $f''(x)=6x$. For $x0$, $f(x)=-x^3$, which gives $f'(x)=-3x^2$ and $f''(x)=-6x$. In compact form for $x \\neq 0$, $f'(x)=3x|x|$ and $f''(x)=6|x|$.\n\nWe must check the derivatives at $x=0$ from the limit definitions.\nThe first derivative is:\n$$\nf'(0) = \\lim_{\\Delta x \\to 0} \\frac{f(0+\\Delta x) - f(0)}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\frac{|\\Delta x|^3 - 0}{\\Delta x} = \\lim_{\\Delta x \\to 0} (\\Delta x)^2 \\frac{|\\Delta x|}{\\Delta x} = 0\n$$\nThe second derivative is:\n$$\nf''(0) = \\lim_{\\Delta x \\to 0} \\frac{f'(\\Delta x) - f'(0)}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\frac{3(\\Delta x)|\\Delta x| - 0}{\\Delta x} = \\lim_{\\Delta x \\to 0} 3|\\Delta x| = 0\n$$\nSo, $f''(0)$ exists and is equal to $0$.\nThe third derivative is:\n$$\nf'''(0) = \\lim_{\\Delta x \\to 0} \\frac{f''(\\Delta x) - f''(0)}{\\Delta x} = \\lim_{\\Delta x \\to 0} \\frac{6|\\Delta x| - 0}{\\Delta x} = 6 \\lim_{\\Delta x \\to 0} \\frac{|\\Delta x|}{\\Delta x}\n$$\nThe right-hand limit as $\\Delta x \\to 0^+$ is $6$, while the left-hand limit as $\\Delta x \\to 0^-$ is $-6$. Since the limits are not equal, $f'''(0)$ is undefined.\nThe premises of the problem are thus verified as correct and self-consistent. The problem is well-posed and scientifically grounded in the field of numerical analysis. We may proceed with the solution.\n\nThe task is to determine the effective order of accuracy $p$. We begin by computing the finite difference approximation $D_{h}^{(2)}f(0)$ directly.\nThe formula is:\n$$\nD_{h}^{(2)}f(0) = \\frac{f(h) - 2f(0) + f(-h)}{h^2}\n$$\nWe evaluate the function $f(x) = |x|^3$ at the required points. Since $h0$:\n- $f(h) = |h|^3 = h^3$\n- $f(0) = |0|^3 = 0$\n- $f(-h) = |-h|^3 = h^3$\n\nSubstituting these values into the formula:\n$$\nD_{h}^{(2)}f(0) = \\frac{h^3 - 2(0) + h^3}{h^2} = \\frac{2h^3}{h^2} = 2h\n$$\nNext, we evaluate the pointwise error $E(h) = |D_{h}^{(2)}f(0) - f''(0)|$. We have already established that $f''(0)=0$.\nTherefore, the error is:\n$$\nE(h) = |2h - 0| = 2h\n$$\nwhere we use the fact that $h0$.\n\nThe standard Taylor series analysis for a sufficiently smooth function $g \\in C^4(\\mathbb{R})$ would predict that the truncation error for this central difference scheme is of order $\\mathcal{O}(h^2)$, since\n$$\n\\frac{g(x+h) - 2g(x) + g(x-h)}{h^2} - g''(x) = \\frac{g^{(4)}(\\xi)}{12}h^2\n$$\nThis result is contingent on the cancellation of the third-order derivative terms, which requires the existence and continuity of $g'''(x)$. In the present problem, $f'''(0)$ does not exist. This lack of smoothness invalidates the standard analysis and leads to a degradation of the order of accuracy. Our direct calculation confirms this.\n\nWe must find the largest real number $p$ such that $E(h) = \\mathcal{O}(h^p)$ as $h \\to 0^+$. This is equivalent to finding the largest $p$ for which the following limit is bounded:\n$$\n\\limsup_{h \\to 0^+} \\frac{E(h)}{h^p}  \\infty\n$$\nSubstituting our expression for $E(h)$:\n$$\n\\limsup_{h \\to 0^+} \\frac{2h}{h^p} = \\limsup_{h \\to 0^+} 2h^{1-p}\n$$\nFor this limit to be finite, the exponent of $h$ must be non-negative. That is, $1-p \\ge 0$, which implies $p \\le 1$.\nThe largest real number $p$ satisfying this condition is $p=1$.\nFor $p=1$, the limit is $\\lim_{h \\to 0^+} 2h^{1-1} = \\lim_{h \\to 0^+} 2 = 2$. This is a finite, non-zero constant, confirming that $E(h)$ is indeed $\\mathcal{O}(h^1)$.\nFor any $p  1$, say $p = 1 + \\epsilon$ with $\\epsilon  0$, the limit becomes $\\lim_{h \\to 0^+} 2h^{-\\epsilon} = \\infty$. This shows that $E(h)$ is not $\\mathcal{O}(h^p)$ for any $p1$.\n\nThus, the largest real number $p$ for which $E(h) = \\mathcal{O}(h^p)$ is exactly $1$.\nThe effective order of accuracy is $p=1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Moving from theory to practice, this exercise introduces the Method of Manufactured Solutions (MMS), a rigorous and widely used technique for verifying the correctness and accuracy of numerical code. You will perform the complete MMS workflow: first, by analytically deriving a source term for a chosen \"manufactured\" solution to a physics-based PDE, and second, by writing a program to numerically measure the code's convergence rate. This capstone practice demonstrates how the theoretical concept of truncation error is used to create a definitive pass/fail test for a scientific computing application .",
            "id": "4059470",
            "problem": "Consider the steady-state anisotropic heat conduction relevant for magnetized fusion plasmas, where the heat flux obeys Fourier’s law with an anisotropic conductivity tensor. Begin from the fundamental base: conservation of energy and Fourier’s law. Specifically, the local energy balance in steady state is given by the divergence of heat flux plus sources equal to zero. For a scalar temperature field $u(x,y)$, and a constant, symmetric, positive-definite conductivity tensor $\\mathbf{K}$, the governing partial differential equation (PDE) is\n$$\n-\\nabla \\cdot \\left( \\mathbf{K} \\nabla u \\right) = S(x,y),\n$$\nwhere $S(x,y)$ is a volumetric heat source. In a strongly magnetized plasma, the conductivity tensor is highly anisotropic with respect to the unit magnetic field direction $\\mathbf{b}$: heat conduction is much larger along $\\mathbf{b}$ than across it. An established model for the conductivity tensor is\n$$\n\\mathbf{K} = \\kappa_{\\perp}\\mathbf{I} + \\left(\\kappa_{\\parallel} - \\kappa_{\\perp}\\right) \\mathbf{b}\\mathbf{b}^{\\top},\n$$\nwhere $\\kappa_{\\parallel}$ and $\\kappa_{\\perp}$ are constant parallel and perpendicular conductivities (dimensionless here), $\\mathbf{I}$ is the identity matrix, and $\\mathbf{b} = \\left(\\cos\\theta,\\sin\\theta\\right)$ with angle $\\theta$ measured in radians. This leads to a constant tensor with entries\n$$\nK_{xx} = \\kappa_{\\perp} + \\left(\\kappa_{\\parallel} - \\kappa_{\\perp}\\right)\\cos^{2}\\theta, \\quad\nK_{yy} = \\kappa_{\\perp} + \\left(\\kappa_{\\parallel} - \\kappa_{\\perp}\\right)\\sin^{2}\\theta, \\quad\nK_{xy} = \\left(\\kappa_{\\parallel} - \\kappa_{\\perp}\\right)\\cos\\theta \\sin\\theta.\n$$\n\nMethod of Manufactured Solutions (MMS) is a principled approach to verify numerical methods. Design a manufactured solution and derive the corresponding source term, then use grid refinement to estimate the observed order of accuracy of a finite difference discretization of the operator $\\nabla \\cdot (\\mathbf{K}\\nabla u)$.\n\nThe manufactured solution to be used is\n$$\nu(x,y) = \\sin(\\pi x)\\sin(\\pi y),\n$$\ndefined on the square domain $[0,1]\\times[0,1]$ with homogeneous Dirichlet boundary conditions implied by the manufactured solution. All quantities are dimensionless. Angle $\\theta$ must be treated in radians.\n\nYour tasks:\n- Derive the source term $S(x,y)$ such that the above $u(x,y)$ exactly satisfies the PDE $-\\nabla \\cdot (\\mathbf{K}\\nabla u)=S(x,y)$ for constant $\\mathbf{K}$ defined by the given $\\kappa_{\\parallel}$, $\\kappa_{\\perp}$, and $\\theta$.\n- Specify a second-order accurate finite difference method (FDM) on a uniform grid of $N\\times N$ nodes with grid spacing $h = 1/(N-1)$ to approximate $\\nabla \\cdot (\\mathbf{K}\\nabla u)$ at interior points. Use standard central differences for the second derivatives:\n  $$\n  u_{xx}\\big|_{i,j} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^{2}}, \\quad\n  u_{yy}\\big|_{i,j} \\approx \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^{2}},\n  $$\n  and the mixed derivative\n  $$\n  u_{xy}\\big|_{i,j} \\approx \\frac{u_{i+1,j+1} - u_{i+1,j-1} - u_{i-1,j+1} + u_{i-1,j-1}}{4h^{2}}.\n  $$\n  Combine these to approximate the differential operator as\n  $$\n  \\left(\\nabla \\cdot (\\mathbf{K}\\nabla u)\\right)_{i,j} \\approx K_{xx} \\, u_{xx}\\big|_{i,j} + 2K_{xy} \\, u_{xy}\\big|_{i,j} + K_{yy} \\, u_{yy}\\big|_{i,j}.\n  $$\n- Define the pointwise discrete residual (the discrete truncation error applied to the exact manufactured solution) at interior nodes as\n  $$\n  r_{i,j} = -\\left(\\nabla \\cdot (\\mathbf{K}\\nabla u)\\right)_{i,j}^{\\text{FDM}} - S(x_{i},y_{j}),\n  $$\n  where $\\left(\\cdot\\right)^{\\text{FDM}}$ denotes the finite difference approximation at grid indices $(i,j)$, and $S$ is evaluated at the same grid locations.\n- Measure the discrete root-mean-square (RMS) truncation error over interior nodes,\n  $$ \n  E(h) = \\sqrt{\\frac{1}{M}\\sum_{(i,j)\\in\\Omega_{\\text{int}}} r_{i,j}^{2}},\n  $$\n  where $\\Omega_{\\text{int}}$ denotes the set of interior grid indices and $M$ is the number of interior points.\n- Estimate the observed order of accuracy $p$ via a grid refinement (convergence) study by fitting a line to $(\\log h, \\log E(h))$ and taking the slope $p$ so that $E(h)\\approx C h^{p}$, for a constant $C$ independent of $h$.\n\nTest suite:\n- Use the following parameter sets and grid sizes:\n  - Case $1$: $\\kappa_{\\parallel} = 1.0$, $\\kappa_{\\perp} = 1.0$, $\\theta = 1.1$, $N \\in \\{17,33,65,129\\}$.\n  - Case $2$: $\\kappa_{\\parallel} = 100.0$, $\\kappa_{\\perp} = 1.0$, $\\theta = \\pi/4$, $N \\in \\{17,33,65,129\\}$.\n  - Case $3$: $\\kappa_{\\parallel} = 50.0$, $\\kappa_{\\perp} = 0.5$, $\\theta = 0.0$, $N \\in \\{17,33,65,129\\}$.\n- Angles must be treated in radians. All quantities are dimensionless. Return each observed order $p$ as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases: $[p_{\\text{case1}},p_{\\text{case2}},p_{\\text{case3}}]$.",
            "solution": "The problem is valid. It is a well-posed, scientifically-grounded exercise in the Method of Manufactured Solutions (MMS) for verifying the order of accuracy of a finite difference scheme applied to an anisotropic diffusion equation. All necessary components are provided and are internally consistent.\n\nThe solution process involves two main parts: first, the analytical derivation of the source term $S(x,y)$ corresponding to the manufactured solution, and second, the numerical computation of the order of accuracy through grid refinement.\n\n### Part 1: Analytical Derivation of the Source Term\n\nThe governing partial differential equation (PDE) is given as:\n$$\n-\\nabla \\cdot (\\mathbf{K} \\nabla u) = S(x,y)\n$$\nwhere $\\mathbf{K}$ is a constant, symmetric, positive-definite conductivity tensor. The operator on the left-hand side can be expanded. Given a scalar field $u(x,y)$, the gradient is $\\nabla u = \\left(\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y}\\right)^{\\top}$. The heat flux is $\\mathbf{q} = -\\mathbf{K}\\nabla u$. The PDE expresses conservation of energy $\\nabla \\cdot \\mathbf{q} = S(x,y)$.\n\nFor a constant tensor $\\mathbf{K}$ with components $K_{xx}$, $K_{yy}$, and $K_{xy} = K_{yx}$, the term $\\nabla \\cdot (\\mathbf{K} \\nabla u)$ expands to:\n$$\n\\nabla \\cdot \\begin{pmatrix} K_{xx} \\frac{\\partial u}{\\partial x} + K_{xy} \\frac{\\partial u}{\\partial y} \\\\ K_{xy} \\frac{\\partial u}{\\partial x} + K_{yy} \\frac{\\partial u}{\\partial y} \\end{pmatrix} = \\frac{\\partial}{\\partial x}\\left(K_{xx} \\frac{\\partial u}{\\partial x} + K_{xy} \\frac{\\partial u}{\\partial y}\\right) + \\frac{\\partial}{\\partial y}\\left(K_{xy} \\frac{\\partial u}{\\partial x} + K_{yy} \\frac{\\partial u}{\\partial y}\\right)\n$$\nSince the components of $\\mathbf{K}$ are constant, they can be factored out of the derivatives:\n$$\n\\nabla \\cdot (\\mathbf{K} \\nabla u) = K_{xx} \\frac{\\partial^2 u}{\\partial x^2} + K_{xy} \\frac{\\partial^2 u}{\\partial y \\partial x} + K_{xy} \\frac{\\partial^2 u}{\\partial x \\partial y} + K_{yy} \\frac{\\partial^2 u}{\\partial y^2}\n$$\nAssuming sufficient smoothness of $u(x,y)$ (which is true for the given manufactured solution), Clairaut's theorem states that the order of differentiation does not matter, so $\\frac{\\partial^2 u}{\\partial y \\partial x} = \\frac{\\partial^2 u}{\\partial x \\partial y}$. This simplifies the expression to:\n$$\n\\nabla \\cdot (\\mathbf{K} \\nabla u) = K_{xx} u_{xx} + 2K_{xy} u_{xy} + K_{yy} u_{yy}\n$$\nThe source term $S(x,y)$ is therefore $S(x,y) = -(K_{xx} u_{xx} + 2K_{xy} u_{xy} + K_{yy} u_{yy})$.\n\nThe manufactured solution is $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$. We compute its second partial derivatives:\n$$\nu_{x} = \\pi\\cos(\\pi x)\\sin(\\pi y) \\quad \\implies \\quad u_{xx} = -\\pi^2\\sin(\\pi x)\\sin(\\pi y)\n$$\n$$\nu_{y} = \\pi\\sin(\\pi x)\\cos(\\pi y) \\quad \\implies \\quad u_{yy} = -\\pi^2\\sin(\\pi x)\\sin(\\pi y)\n$$\n$$\nu_{xy} = \\frac{\\partial}{\\partial y}(\\pi\\cos(\\pi x)\\sin(\\pi y)) = \\pi^2\\cos(\\pi x)\\cos(\\pi y)\n$$\nSubstituting these derivatives into the expression for $S(x,y)$:\n$$\nS(x,y) = -\\left( K_{xx} (-\\pi^2\\sin(\\pi x)\\sin(\\pi y)) + 2K_{xy} (\\pi^2\\cos(\\pi x)\\cos(\\pi y)) + K_{yy} (-\\pi^2\\sin(\\pi x)\\sin(\\pi y)) \\right)\n$$\nFactoring out common terms yields the analytical source term:\n$$\nS(x,y) = \\pi^2(K_{xx} + K_{yy})\\sin(\\pi x)\\sin(\\pi y) - 2\\pi^2 K_{xy}\\cos(\\pi x)\\cos(\\pi y)\n$$\nThis is the source term that must be implemented in the verification code.\n\n### Part 2: Numerical Verification Procedure\n\nThe goal is to estimate the order of accuracy, $p$, of the provided finite difference method (FDM). The method assumes that the error $E$ scales with the grid spacing $h$ as $E(h) \\approx C h^p$ for some constant $C$. Taking the logarithm gives $\\log(E) \\approx \\log(C) + p\\log(h)$, which is a linear relationship between $\\log(E)$ and $\\log(h)$ with slope $p$.\n\nThe procedure is as follows:\n1.  For each test case and for each specified grid size $N$, calculate the grid spacing $h = 1/(N-1)$.\n2.  Construct a uniform $N \\times N$ grid over the domain $[0,1]\\times[0,1]$.\n3.  Evaluate the exact manufactured solution $u(x_i, y_j) = \\sin(\\pi x_i)\\sin(\\pi y_j)$ at all grid nodes.\n4.  Calculate the pointwise discrete residual (local truncation error) at each interior grid node $(i,j)$, where $i,j \\in \\{1, \\dots, N-2\\}$. The residual $r_{i,j}$ is defined as:\n    $$\n    r_{i,j} = -\\left(\\nabla \\cdot (\\mathbf{K}\\nabla u)\\right)_{i,j}^{\\text{FDM}} - S(x_{i},y_{j})\n    $$\n    The FDM operator is given by:\n    $$\n    \\left(\\nabla \\cdot (\\mathbf{K}\\nabla u)\\right)_{i,j}^{\\text{FDM}} = K_{xx} \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^{2}} + 2K_{xy} \\frac{u_{i+1,j+1} - u_{i+1,j-1} - u_{i-1,j+1} + u_{i-1,j-1}}{4h^{2}} + K_{yy} \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^{2}}\n    $$\n    In this expression, $u_{k,l}$ refers to the value of the exact manufactured solution evaluated at the grid node $(x_k, y_l)$. The source term $S(x_i, y_j)$ is the analytical expression derived above, evaluated at the interior node $(x_i, y_j)$.\n5.  Compute the discrete root-mean-square (RMS) error over the $M = (N-2)^2$ interior nodes:\n    $$\n    E(h) = \\sqrt{\\frac{1}{M}\\sum_{j=1}^{N-2}\\sum_{i=1}^{N-2} r_{i,j}^{2}}\n    $$\n6.  This process yields a set of pairs $(h, E(h))$ for each grid refinement level.\n7.  The observed order of accuracy $p$ is then determined by performing a linear least-squares fit on the points $(\\log(h), \\log(E(h)))$ and taking the slope of the resulting line.\n\nSince the FDM stencils for $u_{xx}$, $u_{yy}$, and $u_{xy}$ are all standard second-order accurate approximations, and the manufactured solution is smooth ($C^\\infty$), the overall scheme is expected to be second-order accurate. Therefore, the computed value for $p$ should be close to $2$ for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Method of Manufactured Solutions (MMS) analysis to find the\n    observed order of accuracy for a finite difference scheme.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'k_parallel': 1.0, 'k_perp': 1.0, 'theta': 1.1, 'N_list': [17, 33, 65, 129]},\n        {'k_parallel': 100.0, 'k_perp': 1.0, 'theta': np.pi / 4, 'N_list': [17, 33, 65, 129]},\n        {'k_parallel': 50.0, 'k_perp': 0.5, 'theta': 0.0, 'N_list': [17, 33, 65, 129]},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        k_parallel = case['k_parallel']\n        k_perp = case['k_perp']\n        theta = case['theta']\n        N_list = case['N_list']\n\n        h_values = []\n        error_rms_values = []\n\n        # Calculate constant conductivity tensor components\n        cos_t = np.cos(theta)\n        sin_t = np.sin(theta)\n        k_diff = k_parallel - k_perp\n        K_xx = k_perp + k_diff * cos_t**2\n        K_yy = k_perp + k_diff * sin_t**2\n        K_xy = k_diff * cos_t * sin_t\n\n        for N in N_list:\n            h = 1.0 / (N - 1)\n            h_values.append(h)\n\n            # Create grid\n            x = np.linspace(0, 1, N)\n            y = np.linspace(0, 1, N)\n            x_grid, y_grid = np.meshgrid(x, y)\n\n            # Evaluate exact solution on the full grid\n            u_exact = np.sin(np.pi * x_grid) * np.sin(np.pi * y_grid)\n\n            # Evaluate analytical source term on the interior grid\n            x_int = x_grid[1:-1, 1:-1]\n            y_int = y_grid[1:-1, 1:-1]\n            \n            S_analytical = (np.pi**2 * (K_xx + K_yy) * np.sin(np.pi * x_int) * np.sin(np.pi * y_int) -\n                            2 * np.pi**2 * K_xy * np.cos(np.pi * x_int) * np.cos(np.pi * y_int))\n\n            # Approximate derivatives on the interior grid using FDM\n            # u_int is the solution on the interior grid, used for convenience\n            u_int = u_exact[1:-1, 1:-1]\n            \n            # u_xx\n            u_xx_fdm = (u_exact[1:-1, 2:] - 2 * u_int + u_exact[1:-1, :-2]) / h**2\n            \n            # u_yy\n            u_yy_fdm = (u_exact[2:, 1:-1] - 2 * u_int + u_exact[:-2, 1:-1]) / h**2\n            \n            # u_xy\n            u_xy_fdm = (u_exact[2:, 2:] - u_exact[2:, :-2] - u_exact[:-2, 2:] + u_exact[:-2, :-2]) / (4 * h**2)\n\n            # Compute the FDM approximation of the full operator L(u)\n            L_h_u = K_xx * u_xx_fdm + 2 * K_xy * u_xy_fdm + K_yy * u_yy_fdm\n            \n            # Compute the pointwise residual (truncation error)\n            # r = -L_h(u) - S\n            residual = -L_h_u - S_analytical\n            \n            # Compute the RMS of the residual\n            error_rms = np.sqrt(np.mean(residual**2))\n            error_rms_values.append(error_rms)\n\n        # Calculate the order of accuracy 'p' using a linear fit\n        # E = C*h^p  =>  log(E) = log(C) + p*log(h)\n        log_h = np.log(np.array(h_values))\n        log_E = np.log(np.array(error_rms_values))\n        \n        # np.polyfit returns [slope, intercept] for degree 1\n        p, _ = np.polyfit(log_h, log_E, 1)\n        results.append(p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}