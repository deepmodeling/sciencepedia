{
    "hands_on_practices": [
        {
            "introduction": "The first step in designing effective preconditioners is to understand the ideal outcome. This exercise reveals the elegant spectral properties of a system preconditioned with an ideal block triangular matrix, where sub-solves are assumed to be exact. By computing the eigenvalues of this perfectly preconditioned system, you will see why this structure is the theoretical goal for achieving rapid and robust convergence in iterative solvers .",
            "id": "3954740",
            "problem": "Consider a linearized, semi-implicit resistive Magnetohydrodynamics (MHD) time step leading to a two-field, saddle-point algebraic system for a discretized velocity field and magnetic field with two degrees of freedom each. In this simplified setting, the coupled system takes the canonical saddle-point form $$\\mathcal{M} = \\begin{pmatrix} A & B^{\\top} \\\\ B & -C \\end{pmatrix},$$ where $A \\in \\mathbb{R}^{2 \\times 2}$ models the inertial-viscous response in the momentum equation, $C \\in \\mathbb{R}^{2 \\times 2}$ models resistive-diffusive effects in the induction equation, and $B \\in \\mathbb{R}^{2 \\times 2}$ captures the bilinear coupling induced by the Lorentz force and electromotive terms. Assume $A$ is symmetric positive definite and $C$ is symmetric positive definite. The ideal lower block triangular preconditioner is defined as $$\\mathcal{P} = \\begin{pmatrix} A & 0 \\\\ B & S \\end{pmatrix},$$ where $S$ is the exact Schur complement $S = C + B A^{-1} B^{\\top}$. This ideal preconditioner is representative of exact subsolves for the momentum and induction subsystems and exact coupling resolution in resistive MHD.\n\nConstruct and analyze the following concrete $4 \\times 4$ block example with\n$$A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 & -1 \\\\ 2 & 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix}.$$\nForm the saddle-point operator $\\mathcal{M}$ and the ideal lower block triangular preconditioner $\\mathcal{P}$ using the exact Schur complement $S = C + B A^{-1} B^{\\top}$. Compute the eigenvalues of the preconditioned operator $\\mathcal{P}^{-1} \\mathcal{M}$ exactly. Express your final answer as a single row matrix containing the four eigenvalues in any order. No rounding is required. Provide your answer as real-valued numbers without units.",
            "solution": "The problem requires the computation of the eigenvalues of the preconditioned operator $\\mathcal{P}^{-1} \\mathcal{M}$, where the saddle-point matrix $\\mathcal{M}$ and the ideal lower block triangular preconditioner $\\mathcal{P}$ are defined for a specific $4 \\times 4$ system.\n\nFirst, let us perform a formal validation of the problem statement.\n\n**Step 1: Extract Givens**\n- The saddle-point matrix is given by the block structure\n$$ \\mathcal{M} = \\begin{pmatrix} A & B^{\\top} \\\\ B & -C \\end{pmatrix} $$\n- The ideal lower block triangular preconditioner is\n$$ \\mathcal{P} = \\begin{pmatrix} A & 0 \\\\ B & S \\end{pmatrix} $$\n- The exact Schur complement $S$ is defined as\n$$ S = C + B A^{-1} B^{\\top} $$\n- The concrete matrices are:\n$$ A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 1 & -1 \\\\ 2 & 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\n- It is stated that $A$ is symmetric positive definite (SPD) and $C$ is symmetric positive definite (SPD).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in the field of numerical linear algebra, specifically the preconditioning of saddle-point systems. This is a canonical problem arising from the discretization of partial differential equations, such as the resistive MHD equations mentioned. The forms of $\\mathcal{M}$ and $\\mathcal{P}$ are standard.\n- **Well-Posedness**: The matrices $A$ and $C$ are explicitly given. We can verify their properties. For matrix $A$, the eigenvalues are $2$ and $3$, which are positive, confirming it is SPD and thus invertible. For matrix $C$, the leading principal minors are $\\det(4) = 4 > 0$ and $\\det(C) = (4)(2) - (1)(1) = 7 > 0$. Thus, $C$ is also SPD. Since $A$ is SPD, $A^{-1}$ is SPD. The matrix $B A^{-1} B^{\\top}$ is symmetric positive semi-definite. The Schur complement $S = C + B A^{-1} B^{\\top}$ is the sum of an SPD matrix and a symmetric positive semi-definite matrix, which results in an SPD matrix. Therefore, $S$ is invertible. The preconditioner $\\mathcal{P}$ is a block lower triangular matrix with invertible diagonal blocks ($A$ and $S$), which means $\\mathcal{P}$ is invertible. The problem of finding the eigenvalues of $\\mathcal{P}^{-1}\\mathcal{M}$ is therefore well-posed.\n- **Objectivity**: The problem is stated in precise mathematical terms, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with the solution.\n\nThe task is to find the eigenvalues of the matrix $K = \\mathcal{P}^{-1} \\mathcal{M}$. Instead of explicitly constructing the $4 \\times 4$ numerical matrices and then computing the inverse and product, which is computationally intensive and prone to arithmetic error, we can analyze the block structure of $K$ symbolically.\n\nThe preconditioned system is defined by the equation $\\mathcal{P}K = \\mathcal{M}$. We can write this in block form:\n$$ \\begin{pmatrix} A & 0 \\\\ B & S \\end{pmatrix} K = \\begin{pmatrix} A & B^{\\top} \\\\ B & -C \\end{pmatrix} $$\nLet $K$ be represented by the $2 \\times 2$ block matrix $K = \\begin{pmatrix} K_{11} & K_{12} \\\\ K_{21} & K_{22} \\end{pmatrix}$, where each $K_{ij}$ is a $2 \\times 2$ matrix. The equation becomes:\n$$ \\begin{pmatrix} A & 0 \\\\ B & S \\end{pmatrix} \\begin{pmatrix} K_{11} & K_{12} \\\\ K_{21} & K_{22} \\end{pmatrix} = \\begin{pmatrix} A K_{11} & A K_{12} \\\\ B K_{11} + S K_{21} & B K_{12} + S K_{22} \\end{pmatrix} = \\begin{pmatrix} A & B^{\\top} \\\\ B & -C \\end{pmatrix} $$\nBy equating the corresponding blocks, we get a system of four matrix equations:\n1. $A K_{11} = A$\n2. $A K_{12} = B^{\\top}$\n3. $B K_{11} + S K_{21} = B$\n4. $B K_{12} + S K_{22} = -C$\n\nWe solve this system for the blocks $K_{ij}$:\n1. From $A K_{11} = A$, since $A$ is invertible, we can multiply by $A^{-1}$ on the left to get $K_{11} = I$, where $I$ is the $2 \\times 2$ identity matrix.\n2. From $A K_{12} = B^{\\top}$, multiplying by $A^{-1}$ on the left gives $K_{12} = A^{-1} B^{\\top}$.\n3. Substituting $K_{11} = I$ into the third equation, we get $B I + S K_{21} = B$, which simplifies to $S K_{21} = 0$. As established during validation, the Schur complement $S = C + B A^{-1} B^{\\top}$ is SPD and thus invertible. Multiplying by $S^{-1}$ on the left yields $K_{21} = 0$, where $0$ is the $2 \\times 2$ zero matrix.\n4. Substituting $K_{12} = A^{-1} B^{\\top}$ into the fourth equation gives $B (A^{-1} B^{\\top}) + S K_{22} = -C$. From the definition of the Schur complement, $S = C + B A^{-1} B^{\\top}$, we can express $B A^{-1} B^{\\top}$ as $S - C$. Substituting this into the equation gives $(S - C) + S K_{22} = -C$. This simplifies to $S + S K_{22} = 0$, or $S(I + K_{22}) = 0$. Since $S$ is invertible, we must have $I + K_{22} = 0$, which means $K_{22} = -I$.\n\nCombining these results, the preconditioned matrix $K = \\mathcal{P}^{-1} \\mathcal{M}$ has the block structure:\n$$ K = \\begin{pmatrix} I & A^{-1} B^{\\top} \\\\ 0 & -I \\end{pmatrix} $$\nThis is a block upper triangular matrix. The eigenvalues of a block triangular matrix are the eigenvalues of its diagonal blocks. The diagonal blocks are the $2 \\times 2$ identity matrix $I$ and the $2 \\times 2$ negative identity matrix $-I$.\n- The eigenvalues of $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ are $\\lambda_1 = 1$ and $\\lambda_2 = 1$.\n- The eigenvalues of $-I = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}$ are $\\lambda_3 = -1$ and $\\lambda_4 = -1$.\n\nTherefore, the four eigenvalues of the preconditioned operator $\\mathcal{P}^{-1} \\mathcal{M}$ are $\\{1, 1, -1, -1\\}$. This result is general for any system with the given symbolic structure, as long as the matrices $A$ and $C$ satisfy the positive definite property. The specific numerical values provided in the problem serve as a concrete instance that satisfies these general conditions.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 1 & 1 & -1 & -1 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Ideal preconditioners are a useful theoretical construct, but real-world problems often present significant challenges. This practice uses a simple scalar block example to demonstrate a fundamental difficulty that arises when the $(1,1)$ block $A$ becomes ill-conditioned or singular. By analyzing this singular limit, you will discover how this ill-conditioning dramatically alters the Schur complement and causes simplistic preconditioner approximations to fail, underscoring the necessity of robustly approximating the full Schur complement operator .",
            "id": "3954742",
            "problem": "In magnetohydrodynamic equilibrium and stability computations for magnetic confinement fusion, mixed formulations that simultaneously impose equilibrium equations and constraints (such as divergence-free conditions) give rise to saddle-point linear systems. Consider the linear system obtained after a single spatial degree-of-freedom reduction that preserves the saddle-point structure,\n$$\n\\begin{pmatrix}\nA & B^{\\top} \\\\\nB & -C\n\\end{pmatrix}\n\\begin{pmatrix}\nu \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf \\\\\ng\n\\end{pmatrix},\n$$\nwhere $A$ is symmetric positive definite, $C$ is symmetric positive semidefinite, and $B$ represents the discrete constraint coupling. For the purposes of analysis, focus on the scalar block case with $A=a>0$, $B=b$, $C=c\\ge 0$, and right-hand side $f$ and $g$ arbitrary. Using only block elimination as a fundamental tool, eliminate $u$ to obtain the reduced equation for the Lagrange multiplier $\\lambda$ and identify the reduced operator (the Schur complement) $S$ as an explicit function of $a$, $b$, and $c$.\n\nThen, using this reduced operator, analyze the limit $a\\to 0^{+}$ with fixed $b\\neq 0$ and $c\\ge 0$ by arguing from first principles of block elimination and spectral equivalence what this implies for the quality of a block preconditioner that replaces the reduced operator by $c$ alone (i.e., neglects the coupling contribution arising from $B A^{-1} B^{\\top}$). Explain the mechanism by which this limit affects the numerical behavior, in particular the scaling of the reduced equation and the implications for preconditioned iterations in practice.\n\nProvide your final reported answer as a single closed-form symbolic expression for the reduced operator $S$ in terms of $a$, $b$, and $c$. No numerical evaluation is required, and no units are necessary. Do not include any qualitative analysis in the final answer box; all discussion should appear in your solution text.",
            "solution": "The problem statement is entirely valid. It poses a well-defined question in the field of numerical linear algebra, specifically concerning the analysis of saddle-point systems that are ubiquitous in scientific computing, including in the context of computational fusion science. The problem is self-contained, mathematically consistent, and scientifically grounded. We may proceed with the solution.\n\nThe problem starts with a generic saddle-point linear system:\n$$\n\\begin{pmatrix}\nA & B^{\\top} \\\\\nB & -C\n\\end{pmatrix}\n\\begin{pmatrix}\nu \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf \\\\\ng\n\\end{pmatrix}\n$$\nThis matrix equation represents a system of two coupled linear equations:\n$$\n(1) \\quad Au + B^{\\top}\\lambda = f \\\\\n(2) \\quad Bu - C\\lambda = g\n$$\nThe variables are the vectors $u$ and $\\lambda$. The matrices $A$, $B$, and $C$ and the vectors $f$ and $g$ are given. We are given that $A$ is symmetric positive definite ($A=A^{\\top}, A>0$), which guarantees that its inverse, $A^{-1}$, exists. The matrix $C$ is symmetric positive semidefinite ($C=C^{\\top}, C \\ge 0$).\n\nThe core task is to use block elimination to derive a reduced equation for the Lagrange multiplier variable $\\lambda$. To do this, we first solve equation $(1)$ for $u$:\n$$\nAu = f - B^{\\top}\\lambda\n$$\nMultiplying from the left by $A^{-1}$ yields an expression for $u$ in terms of $\\lambda$:\n$$\nu = A^{-1}(f - B^{\\top}\\lambda)\n$$\nNext, we substitute this expression for $u$ into equation $(2)$:\n$$\nB \\left( A^{-1}(f - B^{\\top}\\lambda) \\right) - C\\lambda = g\n$$\nWe distribute the matrix $B$ across the term in the parenthesis:\n$$\nB A^{-1}f - B A^{-1}B^{\\top}\\lambda - C\\lambda = g\n$$\nNow, we rearrange the equation to isolate the terms involving $\\lambda$ on one side. It is conventional to place them on the left-hand side:\n$$\n- B A^{-1}B^{\\top}\\lambda - C\\lambda = g - B A^{-1}f\n$$\nFactoring out $-\\lambda$ on the left-hand side gives:\n$$\n- (C + B A^{-1}B^{\\top})\\lambda = g - B A^{-1}f\n$$\nFinally, multiplying by $-1$ gives the reduced equation for $\\lambda$:\n$$\n(C + B A^{-1}B^{\\top})\\lambda = B A^{-1}f - g\n$$\nThis equation is of the form $S\\lambda = \\tilde{g}$, where the operator $S = C + B A^{-1}B^{\\top}$ is known as the Schur complement of the $(1,1)$ block $A$ in the original system matrix (up to a sign, depending on convention; here, we define it as the operator acting on $\\lambda$ in the reduced system).\n\nFor the specific scalar block case requested for analysis, we have $A=a>0$, $B=b$, and $C=c\\ge 0$. Since $B$ is a scalar, $B^{\\top}=b$. The inverse of $A$ is $A^{-1}=a^{-1}=\\frac{1}{a}$. Substituting these scalar quantities into the general expression for the reduced operator $S$ yields:\n$$\nS = c + \\frac{b^2}{a}\n$$\nThis expression is the reduced operator for the scalar block case.\n\nThe second part of the problem asks for an analysis of the limit $a\\to 0^{+}$ while $b\\neq 0$ and $c\\ge 0$ are held fixed. In this limit, the Schur complement becomes:\n$$\n\\lim_{a\\to 0^{+}} S = \\lim_{a\\to 0^{+}} \\left( c + \\frac{b^2}{a} \\right)\n$$\nSince $b\\neq 0$, $b^2 > 0$. As $a$ approaches $0$ from the positive side, the term $\\frac{b^2}{a}$ grows without bound. Therefore, $S \\to +\\infty$.\n\nThis has profound implications for numerical methods, particularly for preconditioning. We are asked to consider a preconditioner that approximates the Schur complement $S$ by only the block $C$ (or $c$ in the scalar case). Let us denote this preconditioner by $P_C = C$. For the scalar case, $P_c = c$. The effectiveness of a preconditioner is often assessed by analyzing the spectrum (or, in the scalar case, the value) of the preconditioned operator, $P_c^{-1}S$. For an iterative method like the preconditioned conjugate gradient method to converge rapidly, the eigenvalues of the preconditioned operator should be clustered around $1$.\n\nLet's assume $c>0$, so that $P_c$ is invertible. The preconditioned operator is:\n$$\nP_c^{-1}S = c^{-1} \\left( c + \\frac{b^2}{a} \\right) = 1 + \\frac{b^2}{ac}\n$$\nAs we take the limit $a\\to 0^{+}$, this value diverges:\n$$\n\\lim_{a\\to 0^{+}} \\left( 1 + \\frac{b^2}{ac} \\right) \\to +\\infty\n$$\nThis shows that the preconditioned operator is not bounded. In the more general matrix case, this would manifest as the largest eigenvalues of $P_C^{-1}S$ approaching infinity. The condition number of the preconditioned system, $\\kappa(P_C^{-1}S)$, would therefore diverge, indicating a catastrophic failure of the preconditioner. The number of iterations required by a Krylov subspace method would grow unboundedly as $a\\to 0^{+}$.\n\nThe mechanism for this failure is clear from the structure of $S = C + B A^{-1} B^{\\top}$. The limit $a\\to 0^{+}$ (or in the matrix case, $A$ becoming singular or highly ill-conditioned) is a singular limit. In this limit, the term $B A^{-1} B^{\\top}$ dominates the operator $S$. The preconditioner $P_C = C$ entirely neglects this dominant contribution. Physically, the term involving $A$ often represents a regularization, and its disappearance ($a \\to 0^{+}$) signifies the strengthening of the constraint represented by $B$. The reduced system becomes ill-conditioned because the operator $S$ becomes very large. Any preconditioner that aims to be effective in this limit *must* provide a good approximation to the entire Schur complement, including the $B A^{-1} B^{\\top}$ term. Simply using $C$ is inadequate and leads to a complete breakdown of the iterative solution process in practice. This highlights a fundamental principle in preconditioning saddle-point systems: the preconditioner for the Schur complement must respect the coupling between the different blocks of the original matrix, especially in singular or near-singular limits.",
            "answer": "$$\n\\boxed{c + \\frac{b^2}{a}}\n$$"
        },
        {
            "introduction": "Building on the challenges of ill-conditioning, this final practice delves into a common and difficult scenario in fluid and plasma dynamics: advection-dominated flow. You will construct a minimal example where the non-symmetry from strong advection causes a standard Least-Squares Commutator (LSC) preconditioner to lose its effectiveness. The exercise then guides you through deriving and analyzing an Augmented Lagrangian (AL) stabilization, a powerful technique for restoring robustness in these challenging physical regimes .",
            "id": "3954711",
            "problem": "Consider a reduced incompressible Resistive Magnetohydrodynamics (MHD) linearized edge-flow model in a tokamak scrape-off layer, discretized to yield a saddle-point system with block form\n$$\n\\begin{pmatrix}\nA & B^{\\top} \\\\\nB & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{u} \\\\\np\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{f} \\\\\ng\n\\end{pmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ approximates the momentum operator (diffusion plus advection), $B \\in \\mathbb{R}^{m \\times n}$ approximates the discrete divergence, $\\mathbf{u}$ is the velocity vector, and $p$ is the pressure. The Schur complement governing the pressure block is $S = B A^{-1} B^{\\top}$. In advection-dominated regimes with spatially varying drift, block preconditioners based on the Least-Squares Commutator (LSC) may lose robustness.\n\nConstruct the following minimal example tailored to strong, variable advection: let $n = 2$ and $m = 1$, with\n$$\nA = \\begin{pmatrix}\n\\varepsilon & \\alpha \\\\\n0 & \\varepsilon\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n1 & -1\n\\end{pmatrix},\n$$\nwhere $\\varepsilon > 0$ represents diffusion and $\\alpha > 0$ represents a locally strong, directed advection along the toroidal direction with shear (so that the discrete operator is nonsymmetric). The velocity mass matrix for LSC is the identity $M = I$.\n\nStarting from the standard definitions of the saddle-point Schur complement and the block preconditioning principles:\n- Derive the exact scalar Schur complement $S = B A^{-1} B^{\\top}$ for the above $A$ and $B$.\n- Using the Least-Squares Commutator (LSC) approximation $S_{\\mathrm{LSC}} = B M^{-1} B^{\\top}$, form the single-eigenvalue preconditioned operator $S S_{\\mathrm{LSC}}^{-1}$ and determine its asymptotic behavior as $\\alpha \\to \\infty$ for fixed $\\varepsilon > 0$.\n- Propose an Augmented Lagrangian (AL) correction by replacing $A$ with $A_{\\tau} = A + \\tau B^{\\top} B$, where $\\tau > 0$ is a stabilization parameter. For the block-diagonal AL preconditioner that uses $(1/\\tau) I$ on the pressure block, compute the single-eigenvalue of the preconditioned Schur operator,\n$$\n\\lambda_{\\mathrm{AL}}(\\varepsilon,\\alpha,\\tau) = \\tau \\, B A_{\\tau}^{-1} B^{\\top},\n$$\nand then simplify the result for the choice $\\tau = \\alpha$ that is proportional to the local advection strength.\n\nExpress your final answer as a single closed-form expression for $\\lambda_{\\mathrm{AL}}(\\varepsilon,\\alpha,\\tau)$ at $\\tau = \\alpha$. No units are required. If you introduce any additional symbols, define them clearly. Round nothing; provide the exact analytic expression.",
            "solution": "The problem is set in the context of numerical linear algebra for computational fluid and plasma dynamics, specifically addressing the preconditioning of saddle-point systems arising from discretized Magnetohydrodynamics (MHD) equations. The concepts used, such as Schur complements, Least-Squares Commutator (LSC) preconditioners, and Augmented Lagrangian (AL) methods, are standard in this field. The problem provides a well-defined minimal example to explore the behavior of these preconditioners in an advection-dominated regime. All definitions, parameters, and matrices are clearly stated and mathematically consistent. The problem is scientifically grounded, objective, and well-posed. Therefore, the problem is valid and a solution can be constructed.\n\nThe task is to perform three related calculations.\n\nFirst, we derive the exact scalar Schur complement $S = B A^{-1} B^{\\top}$. The given matrices are:\n$$\nA = \\begin{pmatrix}\n\\varepsilon & \\alpha \\\\\n0 & \\varepsilon\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\n$$\nwith parameters $\\varepsilon > 0$ and $\\alpha > 0$. We begin by finding the inverse of $A$. The determinant of $A$ is $\\det(A) = \\varepsilon \\cdot \\varepsilon - \\alpha \\cdot 0 = \\varepsilon^2$. The inverse is:\n$$\nA^{-1} = \\frac{1}{\\varepsilon^2} \\begin{pmatrix}\n\\varepsilon & -\\alpha \\\\\n0 & \\varepsilon\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{1}{\\varepsilon} & -\\frac{\\alpha}{\\varepsilon^2} \\\\\n0 & \\frac{1}{\\varepsilon}\n\\end{pmatrix}\n$$\nNow we compute the Schur complement $S$. The transpose of $B$ is $B^{\\top} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n$$\nS = B A^{-1} B^{\\top} = \\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{\\varepsilon} & -\\frac{\\alpha}{\\varepsilon^2} \\\\\n0 & \\frac{1}{\\varepsilon}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}\n$$\nWe first evaluate the product $B A^{-1}$:\n$$\nB A^{-1} = \\begin{pmatrix}\n1 \\cdot \\frac{1}{\\varepsilon} - 1 \\cdot 0 & 1 \\cdot \\left(-\\frac{\\alpha}{\\varepsilon^2}\\right) - 1 \\cdot \\frac{1}{\\varepsilon}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{1}{\\varepsilon} & -\\frac{\\alpha+\\varepsilon}{\\varepsilon^2}\n\\end{pmatrix}\n$$\nThen, we complete the product with $B^{\\top}$:\n$$\nS = \\begin{pmatrix}\n\\frac{1}{\\varepsilon} & -\\frac{\\alpha+\\varepsilon}{\\varepsilon^2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix} = \\frac{1}{\\varepsilon} - \\left(-\\frac{\\alpha+\\varepsilon}{\\varepsilon^2}\\right) = \\frac{\\varepsilon}{\\varepsilon^2} + \\frac{\\alpha+\\varepsilon}{\\varepsilon^2} = \\frac{2\\varepsilon+\\alpha}{\\varepsilon^2}\n$$\n\nSecond, we analyze the LSC preconditioned operator $S S_{\\mathrm{LSC}}^{-1}$. The LSC approximation is $S_{\\mathrm{LSC}} = B M^{-1} B^{\\top}$. With the mass matrix $M$ being the identity matrix $I$, $M^{-1} = I$.\n$$\nS_{\\mathrm{LSC}} = B I B^{\\top} = B B^{\\top} = \\begin{pmatrix}\n1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix} = 1^2 + (-1)^2 = 2\n$$\nThe preconditioned operator is a scalar (the single eigenvalue of the $1 \\times 1$ preconditioned system) given by the ratio $S/S_{\\mathrm{LSC}}$:\n$$\n\\lambda_{\\mathrm{LSC}} = S S_{\\mathrm{LSC}}^{-1} = \\frac{(2\\varepsilon+\\alpha)/\\varepsilon^2}{2} = \\frac{2\\varepsilon+\\alpha}{2\\varepsilon^2}\n$$\nThe asymptotic behavior as $\\alpha \\to \\infty$ for fixed $\\varepsilon > 0$ is:\n$$\n\\lim_{\\alpha \\to \\infty} \\lambda_{\\mathrm{LSC}} = \\lim_{\\alpha \\to \\infty} \\left( \\frac{2\\varepsilon}{2\\varepsilon^2} + \\frac{\\alpha}{2\\varepsilon^2} \\right) = \\lim_{\\alpha \\to \\infty} \\left( \\frac{1}{\\varepsilon} + \\frac{\\alpha}{2\\varepsilon^2} \\right) \\to \\infty\n$$\nThe eigenvalue grows linearly with $\\alpha$, indicating a lack of robustness of the LSC preconditioner for large advection.\n\nThird, we compute the eigenvalue $\\lambda_{\\mathrm{AL}}(\\varepsilon,\\alpha,\\tau) = \\tau \\, B A_{\\tau}^{-1} B^{\\top}$ for the Augmented Lagrangian formulation and then specialize to $\\tau=\\alpha$. The matrix $A$ is replaced by $A_{\\tau} = A + \\tau B^{\\top} B$.\n$$\nB^{\\top}B = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\n$$\n$$\nA_{\\tau} = \\begin{pmatrix} \\varepsilon & \\alpha \\\\ 0 & \\varepsilon \\end{pmatrix} + \\tau \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} \\varepsilon+\\tau & \\alpha-\\tau \\\\ -\\tau & \\varepsilon+\\tau \\end{pmatrix}\n$$\nThe determinant of $A_{\\tau}$ is $\\det(A_{\\tau}) = (\\varepsilon+\\tau)^2 - (\\alpha-\\tau)(-\\tau) = \\varepsilon^2 + 2\\varepsilon\\tau + \\tau^2 + \\tau\\alpha - \\tau^2 = \\varepsilon^2 + 2\\varepsilon\\tau + \\tau\\alpha$. The inverse $A_{\\tau}^{-1}$ is:\n$$\nA_{\\tau}^{-1} = \\frac{1}{\\varepsilon^2 + 2\\varepsilon\\tau + \\tau\\alpha} \\begin{pmatrix} \\varepsilon+\\tau & \\tau-\\alpha \\\\ \\tau & \\varepsilon+\\tau \\end{pmatrix}\n$$\nWe then compute the term $B A_{\\tau}^{-1} B^{\\top}$:\n$$\nB A_{\\tau}^{-1} = \\frac{1}{\\det(A_{\\tau})} \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} \\varepsilon+\\tau & \\tau-\\alpha \\\\ \\tau & \\varepsilon+\\tau \\end{pmatrix} = \\frac{1}{\\det(A_{\\tau})} \\begin{pmatrix} \\varepsilon+\\tau-\\tau & \\tau-\\alpha-(\\varepsilon+\\tau) \\end{pmatrix} = \\frac{1}{\\det(A_{\\tau})} \\begin{pmatrix} \\varepsilon & -\\alpha-\\varepsilon \\end{pmatrix}\n$$\n$$\nB A_{\\tau}^{-1} B^{\\top} = \\frac{1}{\\det(A_{\\tau})} \\begin{pmatrix} \\varepsilon & -(\\alpha+\\varepsilon) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{\\varepsilon - (-(\\alpha+\\varepsilon))}{\\det(A_{\\tau})} = \\frac{2\\varepsilon+\\alpha}{\\varepsilon^2 + 2\\varepsilon\\tau + \\tau\\alpha}\n$$\nThe eigenvalue is $\\lambda_{\\mathrm{AL}}(\\varepsilon,\\alpha,\\tau) = \\tau \\, B A_{\\tau}^{-1} B^{\\top}$:\n$$\n\\lambda_{\\mathrm{AL}}(\\varepsilon,\\alpha,\\tau) = \\frac{\\tau(2\\varepsilon+\\alpha)}{\\varepsilon^2 + 2\\varepsilon\\tau + \\tau\\alpha}\n$$\nFinally, we substitute the choice $\\tau = \\alpha$:\n$$\n\\lambda_{\\mathrm{AL}}(\\varepsilon,\\alpha,\\alpha) = \\frac{\\alpha(2\\varepsilon+\\alpha)}{\\varepsilon^2 + 2\\varepsilon\\alpha + \\alpha(\\alpha)} = \\frac{\\alpha(2\\varepsilon+\\alpha)}{\\varepsilon^2 + 2\\varepsilon\\alpha + \\alpha^2}\n$$\nThe denominator is the expansion of $(\\varepsilon+\\alpha)^2$. The final simplified expression is:\n$$\n\\lambda_{\\mathrm{AL}}(\\varepsilon,\\alpha,\\alpha) = \\frac{\\alpha(2\\varepsilon+\\alpha)}{(\\varepsilon+\\alpha)^2}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\alpha(2\\varepsilon+\\alpha)}{(\\varepsilon+\\alpha)^2}}\n$$"
        }
    ]
}