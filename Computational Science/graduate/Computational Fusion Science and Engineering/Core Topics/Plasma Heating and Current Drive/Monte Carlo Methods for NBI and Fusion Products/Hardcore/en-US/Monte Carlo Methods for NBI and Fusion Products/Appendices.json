{
    "hands_on_practices": [
        {
            "introduction": "A fundamental question in any Monte Carlo simulation is determining the number of particle histories required to achieve a desired statistical precision. This exercise  provides a first-principles approach to this problem by modeling neutral beam deposition as a Poisson process. You will derive the relationship between the deposition probability, the desired relative error, and the necessary number of simulation particles, gaining crucial insight into how simulation cost scales with the physics of the problem.",
            "id": "4015715",
            "problem": "A neutral beam injection (NBI) system delivers deuterium neutrals with injected power $P$ and particle kinetic energy $E_0$. The injected particle rate is defined by the quotient $\\dot{N} = P / E_0$ using consistent units. Consider a Monte Carlo (MC) estimation of the energy deposition fraction by treating ionization events along a straight beam path through a uniform plasma as a Poisson process in space with rate per unit length $k = n_e \\sigma_i$, where $n_e$ is the electron number density and $\\sigma_i$ is an effective ionization cross-section. If the neutral beam traverses a chord of length $L$ in the plasma, then a deposited history is recorded when at least one ionizing collision occurs within the chord, and otherwise it is counted as shine-through. Assume each history deposits its full initial energy $E_0$ if it ionizes within the plasma and deposits zero otherwise. Angles, when needed, must be expressed in radians.\n\nStarting from the basic definitions above and from the standard model of spatial Poisson processes for collisions in a uniform medium, derive how to compute the required number of statistically independent MC histories to resolve a specified relative standard error in the estimator of the mean deposition fraction. Your program must:\n\n- Convert $E_0 = 80\\,\\mathrm{keV}$ to joules using $1\\,\\mathrm{eV} = 1.602176634\\times 10^{-19}\\,\\mathrm{J}$, and compute the injected particle rate for $P = 5\\,\\mathrm{MW}$. Express the injected particle rate in $\\mathrm{s}^{-1}$ as a floating-point number.\n- For a given set of plasma conditions $\\{n_e, \\sigma_i, L\\}$, model the deposition-per-history as a Bernoulli random variable with success probability equal to the probability that at least one collision occurs inside the chord. Use this to determine the minimum number of histories needed to ensure that the relative standard error of the Monte Carlo estimator of the deposition fraction is no larger than $r = 0.01$.\n- Design a MC source that samples history energies from a normal distribution centered at $E_0$ with a standard deviation of $2\\,\\mathrm{keV}$, and injection angles from a normal distribution centered at $0$ with a standard deviation of $0.02\\,\\mathrm{rad}$, suitable for a narrow beam. Although the source sampling must be implemented, the final numerical outputs required are the particle rate and the required number of histories for each test case.\n\nUse the following test suite of three plasma conditions to compute the required number of histories, with all cross-sections in square meters, densities in $\\mathrm{m}^{-3}$, and lengths in meters:\n\n- Test case A (moderate deposition): $n_e = 5.0\\times 10^{19}$, $\\sigma_i = 3.0\\times 10^{-20}$, $L = 1.0$.\n- Test case B (low deposition): $n_e = 1.0\\times 10^{18}$, $\\sigma_i = 3.0\\times 10^{-20}$, $L = 1.0$.\n- Test case C (high deposition): $n_e = 2.0\\times 10^{20}$, $\\sigma_i = 3.0\\times 10^{-20}$, $L = 1.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets of the form $[\\dot{N}, N_A, N_B, N_C]$, where $\\dot{N}$ is a floating-point number in $\\mathrm{s}^{-1}$ and $N_A, N_B, N_C$ are integers. No other output should be printed.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It is based on standard principles of particle transport physics and Monte Carlo statistical analysis. All necessary data and definitions for a unique solution are provided, and there are no internal contradictions or scientifically implausible conditions.\n\nThe solution is developed in three parts: first, the calculation of the injected particle rate; second, the derivation of the formula for the required number of Monte Carlo histories; and third, the application of this formula to the specified test cases.\n\n**Part 1: Injected Particle Rate Calculation**\n\nThe problem provides the injected beam power $P = 5\\,\\mathrm{MW}$ and the initial kinetic energy of each deuterium neutral $E_0 = 80\\,\\mathrm{keV}$. The injected particle rate $\\dot{N}$ is defined as the ratio of power to energy per particle, $\\dot{N} = P / E_0$. To ensure consistent units, we convert both quantities to SI units.\n\nThe power is given in megawatts, which we convert to watts (joules per second):\n$$P = 5\\,\\mathrm{MW} = 5 \\times 10^6 \\,\\mathrm{W} = 5 \\times 10^6 \\,\\mathrm{J/s}$$\n\nThe particle energy is given in kilo-electron-volts ($keV$), which we convert to joules using the provided conversion factor $1\\,\\mathrm{eV} = 1.602176634 \\times 10^{-19}\\,\\mathrm{J}$.\n$$E_0 = 80\\,\\mathrm{keV} = 80 \\times 10^3\\,\\mathrm{eV}$$\n$$E_0 = (80 \\times 10^3\\,\\mathrm{eV}) \\times (1.602176634 \\times 10^{-19}\\,\\mathrm{J/eV}) = 1.2817413072 \\times 10^{-14}\\,\\mathrm{J}$$\n\nNow, we can compute the injected particle rate $\\dot{N}$:\n$$\\dot{N} = \\frac{P}{E_0} = \\frac{5 \\times 10^6 \\,\\mathrm{J/s}}{1.2817413072 \\times 10^{-14}\\,\\mathrm{J}} \\approx 3.9010006 \\times 10^{20} \\,\\mathrm{s}^{-1}$$\nThis is the number of neutral particles injected into the system per second.\n\n**Part 2: Derivation for Required Number of Monte Carlo Histories**\n\nThe problem models the ionization of a neutral particle along its path as a spatial Poisson process. The rate of this process per unit length is $k = n_e \\sigma_i$, where $n_e$ is the electron density and $\\sigma_i$ is the effective ionization cross-section. For a particle traversing a uniform plasma over a chord of length $L$, the number of expected collisions is $\\lambda = kL$.\n\nAccording to the Poisson distribution, the probability of observing exactly $m$ collisions over this length is given by:\n$$P(m) = \\frac{\\lambda^m e^{-\\lambda}}{m!}$$\n\nA particle history is considered \"deposited\" if at least one collision occurs ($m \\ge 1$) within the length $L$. The probability of this event, which we denote as $p$, is complementary to the probability of zero collisions ($m=0$):\n$$p = P(m \\ge 1) = 1 - P(m=0)$$\n$$P(m=0) = \\frac{\\lambda^0 e^{-\\lambda}}{0!} = e^{-\\lambda} = e^{-kL}$$\nTherefore, the deposition probability is:\n$$p = 1 - e^{-kL}$$\n\nThe Monte Carlo simulation treats each particle history as an independent Bernoulli trial. Let the random variable $X_i$ represent the outcome of the $i$-th history: $X_i = 1$ for deposition (with probability $p$) and $X_i = 0$ for shine-through (with probability $1-p$). The problem defines the deposition fraction estimator based on this binary outcome.\n\nThe true mean (expected value) of the deposition fraction is $\\mu = E[X_i] = 1 \\cdot p + 0 \\cdot (1-p) = p$.\nThe variance of this Bernoulli random variable is $\\sigma^2 = \\text{Var}(X_i) = p(1-p)$.\n\nFor $N$ independent Monte Carlo histories, the estimator for the mean deposition fraction is $\\hat{\\mu}_N = \\frac{1}{N} \\sum_{i=1}^N X_i$.\nThe variance of this estimator is:\n$$\\text{Var}(\\hat{\\mu}_N) = \\text{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N X_i\\right) = \\frac{1}{N^2}\\sum_{i=1}^N \\text{Var}(X_i) = \\frac{N\\sigma^2}{N^2} = \\frac{\\sigma^2}{N} = \\frac{p(1-p)}{N}$$\nThe standard error of the estimator, $SE(\\hat{\\mu}_N)$, is the square root of its variance:\n$$SE(\\hat{\\mu}_N) = \\sqrt{\\frac{p(1-p)}{N}}$$\n\nThe problem requires the *relative* standard error, $r$, to be no larger than a specified value $r_{max} = 0.01$. The relative standard error is defined as the standard error divided by the true mean:\n$$r = \\frac{SE(\\hat{\\mu}_N)}{\\mu} = \\frac{\\sqrt{p(1-p)/N}}{p} = \\frac{1}{\\sqrt{N}}\\sqrt{\\frac{p(1-p)}{p^2}} = \\frac{1}{\\sqrt{N}}\\sqrt{\\frac{1-p}{p}}$$\n\nWe set the condition $r \\le r_{max}$:\n$$\\frac{1}{\\sqrt{N}}\\sqrt{\\frac{1-p}{p}} \\le r_{max}$$\nSolving for $N$, we square both sides and rearrange:\n$$\\frac{1}{N}\\frac{1-p}{p} \\le r_{max}^2$$\n$$N \\ge \\frac{1-p}{p \\cdot r_{max}^2}$$\nSince $N$ must be an integer, the minimum number of histories required is the smallest integer satisfying this inequality, which is found by taking the ceiling of the right-hand side:\n$$N_{min} = \\left\\lceil \\frac{1-p}{p \\cdot r_{max}^2} \\right\\rceil$$\nSubstituting $p = 1 - e^{-kL}$ and $1-p = e^{-kL}$, we arrive at the final formula:\n$$N_{min} = \\left\\lceil \\frac{e^{-kL}}{(1 - e^{-kL}) r_{max}^2} \\right\\rceil$$\n\n**Part 3: Application to Test Cases**\n\nWe now apply this formula to the three test cases using $r_{max} = 0.01$ and $L = 1.0\\,\\mathrm{m}$.\n\n**Test Case A (moderate deposition):**\n$n_e = 5.0 \\times 10^{19}\\,\\mathrm{m}^{-3}$, $\\sigma_i = 3.0 \\times 10^{-20}\\,\\mathrm{m}^2$.\n$k_A = n_e \\sigma_i = (5.0 \\times 10^{19}) \\times (3.0 \\times 10^{-20}) = 1.5\\,\\mathrm{m}^{-1}$.\nArgument $\\lambda_A = k_A L = 1.5$.\n$N_A = \\left\\lceil \\frac{e^{-1.5}}{(1 - e^{-1.5}) (0.01)^2} \\right\\rceil = \\left\\lceil \\frac{0.223130}{0.776870 \\times 10^{-4}} \\right\\rceil = \\lceil 2872.23 \\rceil = 2873$.\n\n**Test Case B (low deposition):**\n$n_e = 1.0 \\times 10^{18}\\,\\mathrm{m}^{-3}$, $\\sigma_i = 3.0 \\times 10^{-20}\\,\\mathrm{m}^2$.\n$k_B = n_e \\sigma_i = (1.0 \\times 10^{18}) \\times (3.0 \\times 10^{-20}) = 0.03\\,\\mathrm{m}^{-1}$.\nArgument $\\lambda_B = k_B L = 0.03$.\n$N_B = \\left\\lceil \\frac{e^{-0.03}}{(1 - e^{-0.03}) (0.01)^2} \\right\\rceil = \\left\\lceil \\frac{0.9704455}{(1 - 0.9704455) \\times 10^{-4}} \\right\\rceil = \\lceil 328390.13 \\rceil = 328391$.\n\n**Test Case C (high deposition):**\n$n_e = 2.0 \\times 10^{20}\\,\\mathrm{m}^{-3}$, $\\sigma_i = 3.0 \\times 10^{-20}\\,\\mathrm{m}^2$.\n$k_C = n_e \\sigma_i = (2.0 \\times 10^{20}) \\times (3.0 \\times 10^{-20}) = 6.0\\,\\mathrm{m}^{-1}$.\nArgument $\\lambda_C = k_C L = 6.0$.\n$N_C = \\left\\lceil \\frac{e^{-6.0}}{(1 - e^{-6.0}) (0.01)^2} \\right\\rceil = \\left\\lceil \\frac{0.00247875}{(1 - 0.00247875) \\times 10^{-4}} \\right\\rceil = \\lceil 24.85 \\rceil = 25$.\n\nThe results are: $\\dot{N} \\approx 3.9010006 \\times 10^{20}\\,\\mathrm{s}^{-1}$, $N_A = 2873$, $N_B = 328391$, and $N_C = 25$.\n\nThe problem also requires the implementation of a Monte Carlo source sampling routine for particle energy and injection angle from specified normal distributions, which will be included in the final program code.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Although scipy is listed as available, it is not required for this problem.\n\ndef mc_source(E0_mean_keV, E0_std_keV, angle_mean_rad, angle_std_rad):\n    \"\"\"\n    Designs and samples from a Monte Carlo source for NBI particles.\n    \n    This function implements the source sampling as required by the problem\n    statement, generating a particle's initial energy and injection angle\n    from normal distributions. This function is included to fulfill the problem\n    requirements but is not directly used to compute the final numerical outputs.\n\n    Args:\n        E0_mean_keV (float): Mean injection energy in keV.\n        E0_std_keV (float): Standard deviation of injection energy in keV.\n        angle_mean_rad (float): Mean injection angle in radians.\n        angle_std_rad (float): Standard deviation of injection angle in radians.\n\n    Returns:\n        tuple[float, float]: A tuple containing the sampled energy (keV) and angle (rad).\n    \"\"\"\n    # Sample energy from a normal distribution\n    energy_keV = np.random.normal(loc=E0_mean_keV, scale=E0_std_keV)\n    # Sample angle from a normal distribution\n    angle_rad = np.random.normal(loc=angle_mean_rad, scale=angle_std_rad)\n    return energy_keV, angle_rad\n\ndef solve():\n    \"\"\"\n    Calculates the NBI particle rate and the required number of Monte Carlo\n    histories for different plasma conditions.\n    \"\"\"\n    \n    # --- Part 1: Injected Particle Rate Calculation ---\n    \n    # Constants and Givens\n    P_MW = 5.0  # Injected power in MW\n    E0_keV = 80.0  # Particle kinetic energy in keV\n    EV_TO_JOULE = 1.602176634e-19  # Conversion factor for eV to Joules\n\n    # Convert to SI units\n    P_watts = P_MW * 1e6  # Power in Watts (J/s)\n    E0_joules = E0_keV * 1e3 * EV_TO_JOULE  # Energy in Joules\n\n    # Calculate injected particle rate\n    n_dot = P_watts / E0_joules\n\n    # --- Part 2: Required Number of Histories Calculation ---\n\n    # Test suite of plasma conditions\n    test_cases = [\n        # Case A: moderate deposition\n        {'name': 'A', 'n_e': 5.0e19, 'sigma_i': 3.0e-20, 'L': 1.0},\n        # Case B: low deposition\n        {'name': 'B', 'n_e': 1.0e18, 'sigma_i': 3.0e-20, 'L': 1.0},\n        # Case C: high deposition\n        {'name': 'C', 'n_e': 2.0e20, 'sigma_i': 3.0e-20, 'L': 1.0},\n    ]\n\n    r_max = 0.01  # Maximum relative standard error\n\n    def calculate_min_histories(n_e, sigma_i, L, r):\n        \"\"\"\n        Computes the minimum number of MC histories for a given relative error.\n        \n        Args:\n            n_e (float): Electron number density (m^-3).\n            sigma_i (float): Effective ionization cross-section (m^2).\n            L (float): Chord length in plasma (m).\n            r (float): Target relative standard error.\n\n        Returns:\n            int: The minimum number of histories required.\n        \"\"\"\n        # Rate per unit length (inverse of mean free path)\n        k = n_e * sigma_i\n        \n        # Expected number of collisions over length L\n        lambda_val = k * L\n        \n        # Deposition probability p = 1 - exp(-lambda)\n        # Shine-through probability 1-p = exp(-lambda)\n        p = 1.0 - np.exp(-lambda_val)\n        \n        # If p is very close to 0 or 1, the number of histories can be huge or small.\n        # Handle the edge case of p=0 to avoid division by zero, though unlikely here.\n        if p == 0:\n            return np.iinfo(np.int64).max # Effectively infinite\n\n        # From r = sqrt((1-p)/p) / sqrt(N), we get N >= (1-p) / (p * r^2)\n        # 1-p = exp(-lambda_val)\n        num_histories_float = np.exp(-lambda_val) / (p * r**2)\n        \n        # The number of histories must be an integer, so take the ceiling.\n        return int(np.ceil(num_histories_float))\n\n    # Calculate required histories for each test case\n    results_N = []\n    for case in test_cases:\n        N_min = calculate_min_histories(case['n_e'], case['sigma_i'], case['L'], r_max)\n        results_N.append(N_min)\n        \n    N_A, N_B, N_C = results_N\n\n    # --- Part 3: Example call to the source sampler (not used in output) ---\n    # This call demonstrates the function's use as required by the problem.\n    E0_mean_keV_src = 80.0\n    E0_std_keV_src = 2.0\n    angle_mean_rad_src = 0.0\n    angle_std_rad_src = 0.02\n    _ = mc_source(E0_mean_keV_src, E0_std_keV_src, angle_mean_rad_src, angle_std_rad_src)\n\n    # Final print statement in the exact required format\n    # Output: [particle_rate (float), N_A (int), N_B (int), N_C (int)]\n    output_list = [n_dot, N_A, N_B, N_C]\n    print(f\"[{','.join(map(str, output_list))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The fidelity of a particle transport simulation depends critically on the accurate representation of physical data, such as interaction cross-sections, which often vary over many orders of magnitude. This practice  explores the practical challenge of interpolating tabulated cross-section data in a numerically stable and physically-motivated manner. By comparing a naive linear interpolation with a more robust log-log scheme, you will quantify the interpolation error and appreciate why the choice of numerical method is not merely a detail, but a cornerstone of a reliable simulation.",
            "id": "4015705",
            "problem": "Neutral Beam Injection (NBI) and fusion-product transport Monte Carlo algorithms advance charged and neutral particles through a plasma while stochastically sampling interaction events controlled by the microscopic cross section $\\sigma(E)$, where $E$ denotes kinetic energy. The probability density for interaction per unit path length is proportional to the product $n \\sigma(E)$, so the numerical fidelity of event sampling hinges on the accuracy and stability of interpolated $\\sigma(E)$ values between tabulated energies. In high-temperature plasma applications, tabulated ionization cross section $\\sigma_{\\mathrm{i}}(E)$ and charge exchange cross section $\\sigma_{\\mathrm{cx}}(E)$ often span multiple decades in $E$ and $\\sigma$, which demands an interpolation strategy that preserves positivity, avoids catastrophic cancellation over large dynamic ranges, and captures near power-law behavior typical of cross sections.\n\nStarting from the fundamental definition of cross section as the rate coefficient per unit relative flux, and the Monte Carlo sampling rule that the free path length distribution is exponential in the macroscopic cross section $n \\sigma(E)$, design and implement a numerically stable interpolation scheme for tabulated cross sections that respects these constraints. Specifically, implement a log-log interpolation scheme that interpolates linearly in $\\log(E)$ and $\\log(\\sigma)$, and quantify the error introduced by simple linear interpolation in $E$ and $\\sigma$ across decades in $E$.\n\nUse a set of scientifically plausible analytic cross section models to generate tabulated data, and then compare interpolants to the known “ground truth” models to quantify interpolation errors. For each test case, construct tabulated energies at $E \\in \\{10^3,10^4,10^5\\}$ electronvolts (eV) and generate the corresponding tabulated cross sections in square meters ($\\mathrm{m}^2$) by evaluating the specified ground truth model. Evaluate both interpolants over a dense grid of $E$ values that is logarithmically spaced across the entire range $[10^3,10^5]$ eV. Compute the maximum relative error and the root-mean-square (RMS) relative error for each interpolation method with respect to the ground truth model. Relative errors must be reported as decimal fractions (unitless).\n\nThe interpolation methods to compare are:\n- Linear interpolation in the original $(E,\\sigma)$ coordinates.\n- Linear interpolation in $(\\log(E),\\log(\\sigma))$ coordinates (log-log interpolation), which maps back to $\\sigma(E)$ by exponentiation.\n\nWhen computing logarithms, restrict attention to strictly positive $\\sigma(E)$; in all provided test cases, $\\sigma(E)$ is strictly positive over the stated range.\n\nExpress energy in electronvolts (eV) and cross section in square meters ($\\mathrm{m}^2$). The error metrics are dimensionless.\n\nTest suite:\n- Case $1$ (charge exchange-like power law): $\\sigma_{\\mathrm{cx}}(E) = A E^{p}$ with $p = -0.5$, amplitude $A$ chosen such that $\\sigma_{\\mathrm{cx}}(10^3\\,\\mathrm{eV}) = 10^{-19}\\,\\mathrm{m}^2$; evaluate over $E \\in [10^3,10^5]$ eV.\n- Case $2$ (ionization-like with a peak): $\\sigma_{\\mathrm{i}}(E) = A E^{0.7} \\exp(-E/E_0)$ with $E_0 = 3\\times 10^4$ eV and $A$ chosen such that $\\sigma_{\\mathrm{i}}(E_0) = 5\\times 10^{-21}\\,\\mathrm{m}^2$; evaluate over $E \\in [10^3,10^5]$ eV.\n- Case $3$ (plateau): $\\sigma(E) = C$ constant with $C = 10^{-20}\\,\\mathrm{m}^2$; evaluate over $E \\in [10^3,10^5]$ eV.\n- Case $4$ (steep power law): $\\sigma(E) = A E^{p}$ with $p = -2.0$, amplitude $A$ chosen such that $\\sigma(10^3\\,\\mathrm{eV}) = 10^{-19}\\,\\mathrm{m}^2$; evaluate over $E \\in [10^3,10^5]$ eV.\n\nEvaluation grid:\n- Use $N = 2001$ evaluation energies logarithmically spaced from $10^3$ eV to $10^5$ eV.\n\nError metrics:\n- Maximum relative error: $$\\max_{E}\\left|\\dfrac{\\sigma_{\\mathrm{interp}}(E) - \\sigma_{\\mathrm{true}}(E)}{\\sigma_{\\mathrm{true}}(E)}\\right|$$\n- RMS relative error: $$\\sqrt{\\dfrac{1}{N}\\sum_{j=1}^{N}\\left(\\dfrac{\\sigma_{\\mathrm{interp}}(E_j) - \\sigma_{\\mathrm{true}}(E_j)}{\\sigma_{\\mathrm{true}}(E_j)}\\right)^2}$$\n\nYour program must implement the following for each test case:\n- Generate the tabulated cross sections at the specified tabulated energies.\n- Construct both interpolants (linear-in-linear and linear-in-log-log).\n- Evaluate the interpolants and compute the error metrics over the evaluation grid.\n- Produce the final output as a single line containing a list of results for the four test cases. Each result must be a list of four floats: $[\\text{max\\_lin}, \\text{rms\\_lin}, \\text{max\\_loglog}, \\text{rms\\_loglog}]$, where $\\text{max\\_lin}$ and $\\text{rms\\_lin}$ are the maximum and RMS relative errors for the linear interpolant, and $\\text{max\\_loglog}$ and $\\text{rms\\_loglog}$ are the same for the log-log interpolant. The program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[[a,b,c,d],[\\dots],\\dots]$).\n\nNo user input is required. The program must be self-contained and deterministic.",
            "solution": "The problem requires the design and comparative analysis of two numerical interpolation schemes for atomic cross section data, $\\sigma(E)$, where $E$ denotes the kinetic energy of a projectile particle. This is a foundational task in computational physics, specifically for Monte Carlo simulations used to model particle transport in fusion plasmas, such as for Neutral Beam Injection (NBI) heating and for tracking high-energy fusion products. The probability of an interaction event over a given path length is directly proportional to the macroscopic cross section, $\\Sigma(E) = n \\sigma(E)$, where $n$ is the target particle density. The numerical fidelity of sampling these stochastic events therefore hinges on the ability to accurately and stably evaluate $\\sigma(E)$ at arbitrary energies between tabulated data points.\n\nCross section data often span several orders of magnitude in both energy $E$ and cross section $\\sigma$. A robust interpolation scheme must preserve positivity (since $\\sigma(E) > 0$), avoid catastrophic cancellation, and ideally capture the underlying physical trends, which are often approximated by power laws. The problem specifies a comparison between two methods for interpolating between a pair of tabulated points, $(E_1, \\sigma_1)$ and $(E_2, \\sigma_2)$.\n\nThe first method is standard linear interpolation in the original $(E, \\sigma)$ coordinate system. For an energy $E$ such that $E_1 \\le E \\le E_2$, the interpolated cross section, $\\sigma_{\\mathrm{lin}}(E)$, is given by the formula for a straight line connecting the two points:\n$$\n\\sigma_{\\mathrm{lin}}(E) = \\sigma_1 + (E - E_1) \\frac{\\sigma_2 - \\sigma_1}{E_2 - E_1}\n$$\nThis method is simple and computationally fast, but it does not inherently guarantee positivity if $\\sigma$ values are close to zero and data is noisy, and it can poorly represent the functional form of many cross sections.\n\nThe second method, log-log interpolation, is motivated by the common observation that many cross sections follow, at least approximately, a power-law relationship, $\\sigma(E) \\propto E^p$. Taking the natural logarithm transforms this into a linear relationship: $\\log(\\sigma) = p \\log(E) + \\text{constant}$. This suggests that performing linear interpolation in $(\\log E, \\log \\sigma)$ space is a more physically-grounded approach. Let $u = \\log(E)$ and $v = \\log(\\sigma)$. Linear interpolation in this transformed space gives:\n$$\nv(u) = v_1 + (u - u_1) \\frac{v_2 - v_1}{u_2 - u_1}\n$$\nSubstituting the original variables back, we obtain the expression for the logarithm of the interpolated cross section, $\\sigma_{\\mathrm{loglog}}(E)$:\n$$\n\\log(\\sigma_{\\mathrm{loglog}}(E)) = \\log(\\sigma_1) + (\\log(E) - \\log(E_1)) \\frac{\\log(\\sigma_2) - \\log(\\sigma_1)}{\\log(E_2) - \\log(E_1)}\n$$\nThe cross section itself is then found by exponentiation, $\\sigma_{\\mathrm{loglog}}(E) = \\exp(\\log(\\sigma_{\\mathrm{loglog}}(E)))$. This operation inherently guarantees that $\\sigma_{\\mathrm{loglog}}(E) > 0$. This formula can be elegantly rewritten as a local power law, which highlights its structure:\n$$\n\\sigma_{\\mathrm{loglog}}(E) = \\sigma_1 \\left( \\frac{E}{E_1} \\right)^m, \\quad \\text{where the exponent } m = \\frac{\\log(\\sigma_2 / \\sigma_1)}{\\log(E_2 / E_1)}\n$$\nThis form is exact for any function that is a pure power law.\n\nThe implementation will follow a precise numerical experiment. For each of four test cases, a \"ground truth\" analytic cross section model is used to generate a sparse table of cross sections at three specified energies: $E \\in \\{10^3, 10^4, 10^5\\}$ electronvolts (eV). Both interpolation schemes are constructed from this $3$-point table. The accuracy of each interpolant is then assessed by comparing its values against the ground truth model on a dense evaluation grid of $N=2001$ points, logarithmically spaced over the full energy range $[10^3, 10^5]\\,$eV.\n\nThe quantitative comparison relies on two standard error metrics, calculated on the evaluation grid $\\{E_j\\}_{j=1}^N$:\n$1$. The maximum relative error, which measures the worst-case deviation:\n$$\n\\text{Max Rel Err} = \\max_{j} \\left| \\frac{\\sigma_{\\mathrm{interp}}(E_j) - \\sigma_{\\mathrm{true}}(E_j)}{\\sigma_{\\mathrm{true}}(E_j)} \\right|\n$$\n$2$. The root-mean-square (RMS) relative error, which provides a measure of the average error:\n$$\n\\text{RMS Rel Err} = \\sqrt{\\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{\\sigma_{\\mathrm{interp}}(E_j) - \\sigma_{\\mathrm{true}}(E_j)}{\\sigma_{\\mathrm{true}}(E_j)} \\right)^2}\n$$\nThe Python implementation will compute these four error values (maximum and RMS for both linear and log-log methods) for each of the four test cases. The cases are chosen to test the behavior for power laws (where log-log is expected to be exact), a constant (where both are exact), and a more complex shape resembling a realistic ionization cross section. The results will demonstrate the superior stability and accuracy of the physically-motivated log-log interpolation scheme for this class of problems.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares linear and log-log interpolation for atomic cross sections.\n    \"\"\"\n    # Define tabulated energies and the evaluation grid as per the problem.\n    E_tab = np.array([1e3, 1e4, 1e5])\n    N_eval = 2001\n    E_eval = np.logspace(np.log10(E_tab[0]), np.log10(E_tab[-1]), N_eval)\n\n    # --- Define Ground Truth Models for Each Test Case ---\n\n    # Case 1: Charge exchange-like power law\n    # sigma(E) = A * E^p, with p = -0.5\n    # A is set such that sigma(1e3 eV) = 1e-19 m^2\n    A_case1 = 1e-19 / (1e3 ** -0.5)\n    def sigma_case1(E):\n        return A_case1 * E**(-0.5)\n\n    # Case 2: Ionization-like with a peak\n    # sigma(E) = A * E^0.7 * exp(-E/E0), with E0 = 3e4 eV\n    # A is set such that sigma(E0) = 5e-21 m^2\n    E0_case2 = 3e4\n    A_case2 = (5e-21 * np.e) / (E0_case2**0.7)\n    def sigma_case2(E):\n        return A_case2 * E**0.7 * np.exp(-E / E0_case2)\n\n    # Case 3: Plateau (constant)\n    # sigma(E) = 1e-20 m^2\n    def sigma_case3(E):\n        return np.full_like(E, 1e-20, dtype=float)\n\n    # Case 4: Steep power law\n    # sigma(E) = A * E^p, with p = -2.0\n    # A is set such that sigma(1e3 eV) = 1e-19 m^2\n    A_case4 = 1e-19 / (1e3 ** -2.0)\n    def sigma_case4(E):\n        return A_case4 * E**(-2.0)\n\n    test_cases = [\n        {\"name\": \"Power Law (p=-0.5)\", \"func\": sigma_case1},\n        {\"name\": \"Ionization-like\", \"func\": sigma_case2},\n        {\"name\": \"Plateau\", \"func\": sigma_case3},\n        {\"name\": \"Power Law (p=-2.0)\", \"func\": sigma_case4},\n    ]\n    \n    all_results = []\n\n    for case in test_cases:\n        sigma_true_func = case[\"func\"]\n        \n        # 1. Generate tabulated cross sections at specified energies.\n        sigma_tab = sigma_true_func(E_tab)\n        \n        # 2. Evaluate ground truth model on the dense evaluation grid.\n        sigma_true_eval = sigma_true_func(E_eval)\n\n        # 3. Construct and evaluate the linear interpolant.\n        # np.interp performs linear interpolation in (E, sigma) space.\n        sigma_lin_eval = np.interp(E_eval, E_tab, sigma_tab)\n        \n        # 4. Construct and evaluate the log-log interpolant.\n        # This is equivalent to linear interpolation in (log E, log sigma) space.\n        log_E_tab = np.log(E_tab)\n        log_sigma_tab = np.log(sigma_tab)\n        log_E_eval = np.log(E_eval)\n        \n        # Check for non-positive sigma_tab, which would invalidate log.\n        # Problem statement guarantees this will not happen for the given cases.\n        if np.any(sigma_tab <= 0):\n            # This branch should not be taken in this problem.\n            # Handle error appropriately in a general-purpose code.\n            # For this problem, we can just produce NaN errors.\n            sigma_loglog_eval = np.full_like(E_eval, np.nan)\n        else:\n            log_sigma_loglog_eval = np.interp(log_E_eval, log_E_tab, log_sigma_tab)\n            sigma_loglog_eval = np.exp(log_sigma_loglog_eval)\n\n        # 5. Calculate relative errors for both methods.\n        # The denominator sigma_true_eval is strictly positive for all cases.\n        rel_err_lin = (sigma_lin_eval - sigma_true_eval) / sigma_true_eval\n        rel_err_loglog = (sigma_loglog_eval - sigma_true_eval) / sigma_true_eval\n        \n        # 6. Compute error metrics.\n        max_lin = np.max(np.abs(rel_err_lin))\n        rms_lin = np.sqrt(np.mean(rel_err_lin**2))\n        \n        max_loglog = np.max(np.abs(rel_err_loglog))\n        rms_loglog = np.sqrt(np.mean(rel_err_loglog**2))\n        \n        all_results.append([max_lin, rms_lin, max_loglog, rms_loglog])\n\n    # 7. Format the final output string as required.\n    result_str_parts = []\n    for res in all_results:\n        result_str_parts.append(f\"[{','.join(f'{x:.10g}' for x in res)}]\")\n    final_output = f\"[{','.join(result_str_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "In realistic fusion plasmas, material properties and thus interaction cross-sections vary significantly in space, making the task of sampling a particle's free path complex and computationally expensive. This practice  introduces the elegant and powerful null-collision method, a variance reduction technique that circumvents this difficulty. You will derive and implement an optimal \"majorant\" cross-section, discovering how to minimize the computational overhead of the method and quantifying the efficiency gains offered by a more adaptive approach.",
            "id": "4015707",
            "problem": "Consider straight-line null-collision (also known as Woodcock) tracking for Neutral Beam Injection (NBI, Neutral Beam Injection) neutrals or fusion products moving through a magnetically confined plasma, modeled along a single characteristic path parametrized by a normalized toroidal flux coordinate $s \\in [0,1]$. The true macroscopic total cross-section profile is a known nonnegative function $s \\mapsto \\Sigma_t(s)$. The null-collision method introduces a majorant cross-section $s \\mapsto \\hat{\\Sigma}_t(s)$ such that $\\hat{\\Sigma}_t(s) \\ge \\Sigma_t(s)$ for all $s \\in [0,1]$. In a piecewise-constant implementation tied to flux surfaces, we partition $[0,1]$ into $K$ equal-width regions $[s_{i-1}, s_i]$ with $s_i = i/K$, and restrict $\\hat{\\Sigma}_t$ to be constant on each region. In a uniform global implementation, $\\hat{\\Sigma}_t$ is a single constant over $[0,1]$.\n\nFundamental base and modeling assumptions:\n- By the construction of the Woodcock tracking method, the expected number of collision attempts per differential path length $ds$ equals $\\hat{\\Sigma}_t(s)\\,ds$, and the expected number of true collisions per $ds$ equals $\\Sigma_t(s)\\,ds$, for any admissible majorant $s \\mapsto \\hat{\\Sigma}_t(s)$ that bounds the true cross-section.\n- The linearity of expectation and integrals applies.\n- The particle traverses the entire interval $s \\in [0,1]$ monotonically once, so the path length in the normalized coordinate is $1$ (dimensionless). All reported quantities in this problem are dimensionless numbers.\n\nTask:\n1) Starting from the above base, derive from first principles the piecewise-constant majorant function $s \\mapsto \\hat{\\Sigma}_t(s)$ on the partition $\\{[s_{i-1}, s_i]\\}_{i=1}^K$ that minimizes the expected number of null collisions over $s \\in [0,1]$ subject to the pointwise constraints $\\hat{\\Sigma}_t(s) \\ge \\Sigma_t(s)$ and the piecewise-constant restriction. Then, for the uniform global majorant, define the corresponding admissible constant and the induced expected number of null collisions. Explain why these constructions are optimal for their respective admissible classes.\n\n2) Implement a deterministic numerical algorithm to compute, for each specified test case:\n- the expected number of null collisions using the optimal piecewise-constant majorant over flux-surface regions,\n- the expected number of null collisions using the optimal uniform global majorant,\n- and the fractional cost reduction defined as the decimal fraction $(E_{\\text{uniform}} - E_{\\text{piecewise}})/E_{\\text{uniform}}$.\nUse a uniform quadrature in $s$ with at least $N \\ge 20000$ points to evaluate integrals, and a uniform grid with at least $M \\ge 1000$ points per region to approximate essential suprema within each region. Round each reported float to $6$ decimal places.\n\n3) Design your program to solve the following test suite. For each case, $K$ denotes the number of equal-width flux-surface regions and $\\Sigma_t(s)$ is specified:\n- Case A (polynomial): $K = 4$, and\n$$\n\\Sigma_t(s) = \\Sigma_0 \\bigl(1 + \\alpha s + \\beta s^2\\bigr), \\quad \\Sigma_0 = 2.0,\\ \\alpha = 1.5,\\ \\beta = 0.5.\n$$\n- Case B (core-peaked Gaussian): $K = 5$, and\n$$\n\\Sigma_t(s) = \\Sigma_0 \\bigl(1 + c\\, e^{-(s/\\sigma)^2}\\bigr), \\quad \\Sigma_0 = 1.0,\\ c = 3.0,\\ \\sigma = 0.3.\n$$\n- Case C (edge-peaked power law): $K = 3$, and\n$$\n\\Sigma_t(s) = \\Sigma_0 \\bigl(1 + c\\, s^p\\bigr), \\quad \\Sigma_0 = 0.5,\\ c = 5.0,\\ p = 4.\n$$\n- Case D (boundary check, single region): $K = 1$, and\n$$\n\\Sigma_t(s) = \\Sigma_0 \\bigl(1 + \\alpha s\\bigr), \\quad \\Sigma_0 = 1.2,\\ \\alpha = 2.0.\n$$\n\nOutput format:\n- For each test case, output a list of three floats $[E_{\\text{piecewise}}, E_{\\text{uniform}}, \\text{reduction}]$, where $E_{\\text{piecewise}}$ is the expected number of null collisions using the optimal piecewise-constant majorant, $E_{\\text{uniform}}$ is that using the optimal uniform global majorant, and $\\text{reduction}$ is the fractional cost reduction $(E_{\\text{uniform}} - E_{\\text{piecewise}})/E_{\\text{uniform}}$, all rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for Cases A–D as a comma-separated list of these lists, in order, enclosed in square brackets. For example, the printed line must look like\n$[ [x_A,y_A,z_A],[x_B,y_B,z_B],[x_C,y_C,z_C],[x_D,y_D,z_D] ]$\nwith each $x_\\cdot$, $y_\\cdot$, $z_\\cdot$ rounded to $6$ decimal places.\n- All outputs are dimensionless. No angles are involved.\n\nYour submission must be a complete, runnable program that computes these quantities exactly as specified and prints the single-line output in the exact format described.",
            "solution": "The problem requires the derivation and computation of quantities related to the null-collision (Woodcock) tracking method in computational transport theory. The core task is to find optimal majorant cross-sections $\\hat{\\Sigma}_t(s)$ for a given true cross-section $\\Sigma_t(s)$ and compute the resulting expected number of null collisions for two classes of majorants: piecewise-constant and uniform-global.\n\n**1. First Principles Derivation**\n\nLet $s \\in [0,1]$ be the normalized path length. The true total macroscopic cross-section is $\\Sigma_t(s)$, and the majorant cross-section is $\\hat{\\Sigma}_t(s)$, with the constraint $\\hat{\\Sigma}_t(s) \\ge \\Sigma_t(s)$ for all $s \\in [0,1]$.\n\nBased on the problem's assumptions:\n- The expected number of collision attempts per differential path length $ds$ is $dE_{\\text{attempts}} = \\hat{\\Sigma}_t(s) ds$.\n- The expected number of true collisions per differential path length $ds$ is $dE_{\\text{true}} = \\Sigma_t(s) ds$.\n\nThe number of null collisions is the number of attempts minus the number of true collisions. By linearity of expectation, the expected number of null collisions per differential path length $ds$ is:\n$$ dE_{\\text{null}} = dE_{\\text{attempts}} - dE_{\\text{true}} = (\\hat{\\Sigma}_t(s) - \\Sigma_t(s)) ds $$\n\nThe total expected number of null collisions, $E_{\\text{null}}$, over the entire path is the integral of this differential quantity over the domain $s \\in [0,1]$:\n$$ E_{\\text{null}}[\\hat{\\Sigma}_t] = \\int_0^1 (\\hat{\\Sigma}_t(s) - \\Sigma_t(s)) ds $$\nThis can be separated into two parts:\n$$ E_{\\text{null}}[\\hat{\\Sigma}_t] = \\int_0^1 \\hat{\\Sigma}_t(s) ds - \\int_0^1 \\Sigma_t(s) ds $$\nLet $C(\\hat{\\Sigma}_t) = \\int_0^1 \\hat{\\Sigma}_t(s) ds$ be the total expected number of collision attempts, which is the primary measure of computational cost in a Monte Carlo simulation. Let $C_{\\text{true}} = \\int_0^1 \\Sigma_t(s) ds$ be the total expected number of true collisions. For a given problem, $\\Sigma_t(s)$ is fixed, so $C_{\\text{true}}$ is a constant. Therefore, minimizing the expected number of null collisions $E_{\\text{null}}$ is equivalent to minimizing the computational cost $C(\\hat{\\Sigma}_t)$.\n\n**2. Optimal Piecewise-Constant Majorant**\n\nThe domain $[0,1]$ is partitioned into $K$ equal-width regions $R_i = [s_{i-1}, s_i]$ where $s_i = i/K$ for $i=1, \\dots, K$. The majorant $\\hat{\\Sigma}_t(s)$ is restricted to be a constant value, say $c_i$, for all $s$ within each region $R_i$.\nThe majorant constraint $\\hat{\\Sigma}_t(s) \\ge \\Sigma_t(s)$ implies that for each region $R_i$, the constant $c_i$ must satisfy $c_i \\ge \\Sigma_t(s)$ for all $s \\in R_i$. This is equivalent to requiring $c_i \\ge \\sup_{s \\in R_i} \\Sigma_t(s)$.\n\nThe cost to be minimized is $C(\\hat{\\Sigma}_t) = \\int_0^1 \\hat{\\Sigma}_t(s) ds = \\sum_{i=1}^K \\int_{s_{i-1}}^{s_i} c_i ds$. Since the regions have equal width $\\Delta s = 1/K$, this becomes:\n$$ C(\\hat{\\Sigma}_t) = \\sum_{i=1}^K c_i \\cdot \\frac{1}{K} = \\frac{1}{K} \\sum_{i=1}^K c_i $$\nTo minimize this sum, we must choose each $c_i$ to be as small as possible, subject to its constraint. The minimal admissible value for each $c_i$ is $c_i^{\\text{opt}} = \\sup_{s \\in R_i} \\Sigma_t(s)$. This choice is optimal because any smaller value would violate the majorant condition, and any larger value would increase the cost.\n\nThe optimal piecewise-constant majorant is $\\hat{\\Sigma}_t^{\\text{piecewise}}(s) = \\sum_{i=1}^K (\\sup_{s' \\in R_i} \\Sigma_t(s')) \\cdot \\mathbb{I}_{R_i}(s)$, where $\\mathbb{I}_{R_i}$ is the indicator function for region $R_i$. The corresponding minimum cost is:\n$$ C_{\\text{piecewise}} = \\frac{1}{K} \\sum_{i=1}^K \\sup_{s \\in [(i-1)/K, i/K]} \\Sigma_t(s) $$\nThe expected number of null collisions for this optimal case, which the problem asks for, is:\n$$ E_{\\text{piecewise}} = C_{\\text{piecewise}} - C_{\\text{true}} = \\frac{1}{K} \\sum_{i=1}^K \\sup_{s \\in [(i-1)/K, i/K]} \\Sigma_t(s) - \\int_0^1 \\Sigma_t(s) ds $$\n\n**3. Optimal Uniform Global Majorant**\n\nHere, the majorant is restricted to be a single constant $C$ over the entire interval $[0,1]$, i.e., $\\hat{\\Sigma}_t(s) = C$. The majorant constraint requires $C \\ge \\Sigma_t(s)$ for all $s \\in [0,1]$. This is equivalent to $C \\ge \\sup_{s \\in [0,1]} \\Sigma_t(s)$.\n\nThe cost to be minimized is $C(\\hat{\\Sigma}_t) = \\int_0^1 C ds = C$. To minimize $C$ subject to the constraint, we must choose the smallest possible value:\n$$ C^{\\text{opt}} = \\sup_{s \\in [0,1]} \\Sigma_t(s) $$\nThe optimal uniform global majorant is $\\hat{\\Sigma}_t^{\\text{uniform}}(s) = \\sup_{s' \\in [0,1]} \\Sigma_t(s')$. The corresponding minimum cost for this class of majorants is:\n$$ C_{\\text{uniform}} = \\sup_{s \\in [0,1]} \\Sigma_t(s) $$\nThe expected number of null collisions is:\n$$ E_{\\text{uniform}} = C_{\\text{uniform}} - C_{\\text{true}} = \\sup_{s \\in [0,1]} \\Sigma_t(s) - \\int_0^1 \\Sigma_t(s) ds $$\n\n**4. Fractional Cost Reduction**\n\nThe problem defines this as $(E_{\\text{uniform}} - E_{\\text{piecewise}})/E_{\\text{uniform}}$. Substituting the derived expressions:\n$$ \\text{reduction} = \\frac{(C_{\\text{uniform}} - C_{\\text{true}}) - (C_{\\text{piecewise}} - C_{\\text{true}})}{C_{\\text{uniform}} - C_{\\text{true}}} = \\frac{C_{\\text{uniform}} - C_{\\text{piecewise}}}{E_{\\text{uniform}}} $$\nSince the piecewise majorant is a more refined bound, we have $\\hat{\\Sigma}_t^{\\text{uniform}}(s) \\ge \\hat{\\Sigma}_t^{\\text{piecewise}}(s)$ for all $s$, which implies $C_{\\text{uniform}} \\ge C_{\\text{piecewise}}$ and $E_{\\text{uniform}} \\ge E_{\\text{piecewise}}$, so the reduction is non-negative.\n\n**5. Numerical Algorithm**\n\nThe three quantities to be computed are $E_{\\text{piecewise}}$, $E_{\\text{uniform}}$, and the fractional reduction. This requires the numerical evaluation of $C_{\\text{true}}$, $C_{\\text{piecewise}}$, and $C_{\\text{uniform}}$.\n\n- **Integral Evaluation ($C_{\\text{true}}$)**: The integral $\\int_0^1 \\Sigma_t(s) ds$ is approximated using a uniform quadrature rule (midpoint rule) with $N=20000$ points. We create a grid $s_j = (j+0.5)/N$ for $j=0, \\dots, N-1$ and approximate the integral as $\\frac{1}{N} \\sum_{j=0}^{N-1} \\Sigma_t(s_j)$.\n\n- **Supremum Approximation ($C_{\\text{piecewise}}, C_{\\text{uniform}}$)**: The supremum of $\\Sigma_t(s)$ over an interval $[a,b]$ is approximated by finding the maximum value of the function on a fine uniform grid of $M=1000$ points within that interval.\n    - For $C_{\\text{piecewise}}$, this is done for each of the $K$ regions $[(i-1)/K, i/K]$.\n    - For $C_{\\text{uniform}}$, this is done over the global interval $[0,1]$. A grid of $K \\times M$ points is used for consistency.\n\nThe final values are calculated from these numerical approximations and rounded to $6$ decimal places as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final output.\n    \"\"\"\n    \n    # Numerical parameters as per the problem statement\n    N_INTEGRATION_POINTS = 20000\n    M_SUPREMUM_POINTS_PER_REGION = 1000\n\n    # Define the cross-section functions for each case\n    sigma_funcs = {\n        'A': lambda s, p: p['Sigma0'] * (1 + p['alpha'] * s + p['beta'] * s**2),\n        'B': lambda s, p: p['Sigma0'] * (1 + p['c'] * np.exp(-(s / p['sigma'])**2)),\n        'C': lambda s, p: p['Sigma0'] * (1 + p['c'] * s**p['p']),\n        'D': lambda s, p: p['Sigma0'] * (1 + p['alpha'] * s),\n    }\n    \n    # Define the parameter sets for each test case\n    test_cases = [\n        ('A', {'K': 4, 'Sigma0': 2.0, 'alpha': 1.5, 'beta': 0.5}),\n        ('B', {'K': 5, 'Sigma0': 1.0, 'c': 3.0, 'sigma': 0.3}),\n        ('C', {'K': 3, 'Sigma0': 0.5, 'c': 5.0, 'p': 4.0}),\n        ('D', {'K': 1, 'Sigma0': 1.2, 'alpha': 2.0}),\n    ]\n    \n    all_results = []\n\n    for case_id, params in test_cases:\n        K = params['K']\n        sigma_t = lambda s: sigma_funcs[case_id](s, params)\n\n        # 1. Compute C_true = integral of Sigma_t(s) from 0 to 1\n        s_integral_grid = (np.arange(N_INTEGRATION_POINTS) + 0.5) / N_INTEGRATION_POINTS\n        sigma_vals_integral = sigma_t(s_integral_grid)\n        C_true = np.mean(sigma_vals_integral)\n\n        # 2. Compute C_uniform = global supremum of Sigma_t(s) on [0,1]\n        # Use a grid of K*M points for global supremum for consistency\n        num_global_sup_points = K * M_SUPREMUM_POINTS_PER_REGION\n        s_global_sup_grid = np.linspace(0.0, 1.0, num_global_sup_points)\n        sigma_vals_global_sup = sigma_t(s_global_sup_grid)\n        C_uniform = np.max(sigma_vals_global_sup)\n        \n        # 3. Compute C_piecewise\n        sum_of_suprema = 0.0\n        for i in range(K):\n            s_start = i / K\n            s_end = (i + 1) / K\n            s_local_sup_grid = np.linspace(s_start, s_end, M_SUPREMUM_POINTS_PER_REGION)\n            sigma_vals_local_sup = sigma_t(s_local_sup_grid)\n            supremum_in_region = np.max(sigma_vals_local_sup)\n            sum_of_suprema += supremum_in_region\n        C_piecewise = sum_of_suprema / K\n\n        # 4. Calculate the final quantities\n        E_piecewise = C_piecewise - C_true\n        E_uniform = C_uniform - C_true\n        \n        # Avoid division by zero if E_uniform is zero or numerically very small\n        if E_uniform < 1e-12:\n            reduction = 0.0\n        else:\n            reduction = (E_uniform - E_piecewise) / E_uniform\n            \n        all_results.append([E_piecewise, E_uniform, reduction])\n\n    # Format the output as specified\n    string_results = []\n    for res in all_results:\n        s_res = [f\"{v:.6f}\" for v in res]\n        string_results.append(f\"[{','.join(s_res)}]\")\n    \n    final_output = f\"[{','.join(string_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}