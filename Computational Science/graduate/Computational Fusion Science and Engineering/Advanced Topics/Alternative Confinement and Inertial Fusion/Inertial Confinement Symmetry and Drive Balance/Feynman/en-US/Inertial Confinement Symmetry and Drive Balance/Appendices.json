{
    "hands_on_practices": [
        {
            "introduction": "Understanding how drive nonuniformities translate into capsule shape distortions is fundamental to Inertial Confinement Fusion (ICF). This exercise guides you through a first-principles derivation, connecting a simple, low-mode pressure asymmetry to the final shape of an imploding shell . By applying basic kinematics and dynamics, you will develop a clear analytical model that reveals the core mechanism of asymmetry growth and its dependence on the implosion's convergence ratio.",
            "id": "3995575",
            "problem": "A spherical thin-shell capsule in an Inertial Confinement Fusion (ICF) implosion experiences an ablation-pressure drive that is nearly uniform but contains a controlled low-mode nonuniformity. Assume the shell can be modeled as a thin layer with angle-independent areal mass density $\\Sigma(t)$ during the acceleration phase, and that the local normal acceleration is given by Newton’s second law per unit area. Let the ablation pressure be\n$$\np_{a}(\\theta,t) = p_{0}(t)\\left[1 + \\epsilon\\,P_{2}(\\cos\\theta)\\right],\n$$\nwhere $P_{2}(\\cos\\theta)$ is the second-order Legendre polynomial, $P_{2}(\\mu) = \\frac{1}{2}\\left(3\\mu^{2}-1\\right)$, and $\\epsilon$ is a small, time-independent mode amplitude. A $\\pm 1\\%$ variation in the polar direction is represented by taking $\\epsilon = 0.01$, so that at the poles ($\\theta=0$) the pressure is increased by $+1\\%$ relative to the mean.\n\nThe shell begins at rest with outer radius $R_{i}$ at $t=0$ and is accelerated inward by $p_{a}(\\theta,t)$ until the end of the acceleration phase at time $t_{f}$ when the mean outer radius is $R_{f}$. Assume that the interior pressure remains spherically symmetric during this phase so that it does not contribute to any angular variation in the acceleration, and that the motion is purely radial at each polar angle.\n\n1. Using only fundamental principles (Newton’s second law per unit area and the given form of $p_{a}(\\theta,t)$), derive the angular dependence of the shell acceleration $a_{s}(\\theta,t)$ and its fractional variation relative to the mean.\n\n2. Denote the mean radius by $R_{0}(t)$ and the final radius field by\n$$\nR_{f}(\\theta) = R_{f}\\left[1 + a_{2}\\,P_{2}(\\cos\\theta)\\right],\n$$\nwhich defines the time-integrated shape coefficient $a_{2}$ at the end of the acceleration phase. Starting from the linearized kinematics of the shell and the relation between acceleration and displacement, derive a closed-form expression for $a_{2}$ in terms of $\\epsilon$ and the convergence $C \\equiv R_{i}/R_{f}$, assuming the initial conditions $R(\\theta,0)=R_{i}$ and $\\dot{R}(\\theta,0)=0$ are angle-independent.\n\n3. Evaluate the result for $R_{i} = 1.0~\\mathrm{mm}$ and $R_{f} = 0.2~\\mathrm{mm}$, with $\\epsilon = 0.01$. Express the final answer for $a_{2}$ as a dimensionless number. No rounding instruction is necessary for your final answer.",
            "solution": "The problem requires an analysis of the growth of a low-mode shape perturbation on an imploding spherical shell in Inertial Confinement Fusion (ICF), driven by a non-uniform ablation pressure. The analysis will be performed in three parts.\n\n### Part 1: Angular Dependence of Shell Acceleration\n\nWe begin with Newton's second law applied to a unit area of the shell. The net pressure accelerating the shell inward is the difference between the external ablation pressure $p_{a}(\\theta, t)$ and the internal pressure $p_{int}(t)$. The law states that the net force per unit area equals the areal mass density $\\Sigma(t)$ times the local normal acceleration $a_{s}(\\theta, t)$. We define acceleration to be positive for inward motion.\n\n$$\n\\Sigma(t) a_{s}(\\theta, t) = p_{a}(\\theta, t) - p_{int}(t)\n$$\n\nThe problem provides the form of the ablation pressure:\n$$\np_{a}(\\theta,t) = p_{0}(t)\\left[1 + \\epsilon\\,P_{2}(\\cos\\theta)\\right]\n$$\nThe internal pressure $p_{int}(t)$ is given as spherically symmetric. Substituting the pressure expression into Newton's second law gives:\n$$\na_{s}(\\theta, t) = \\frac{p_{0}(t)\\left[1 + \\epsilon\\,P_{2}(\\cos\\theta)\\right] - p_{int}(t)}{\\Sigma(t)}\n$$\n\nWe can separate this into a mean (spherically symmetric) component and a perturbed component. The mean acceleration, $a_{s,0}(t)$, is the spherical average of $a_{s}(\\theta, t)$. Since the spherical average of the Legendre polynomial $P_{l}(\\cos\\theta)$ is zero for $l  0$, the mean acceleration is obtained by setting the perturbed term to zero:\n$$\na_{s,0}(t) = \\frac{p_{0}(t) - p_{int}(t)}{\\Sigma(t)}\n$$\n\nThe full angularly dependent acceleration can now be expressed as:\n$$\na_{s}(\\theta, t) = \\frac{p_{0}(t) - p_{int}(t)}{\\Sigma(t)} + \\frac{p_{0}(t)\\epsilon\\,P_{2}(\\cos\\theta)}{\\Sigma(t)}\n$$\n$$\na_{s}(\\theta, t) = a_{s,0}(t) + \\frac{p_{0}(t)}{\\Sigma(t)}\\epsilon\\,P_{2}(\\cos\\theta)\n$$\n\nThe fractional variation of the acceleration relative to the mean is defined as $\\frac{a_{s}(\\theta, t) - a_{s,0}(t)}{a_{s,0}(t)}$.\n$$\n\\frac{\\Delta a_{s}}{a_{s,0}} = \\frac{a_{s}(\\theta, t) - a_{s,0}(t)}{a_{s,0}(t)} = \\frac{\\frac{p_{0}(t)}{\\Sigma(t)}\\epsilon\\,P_{2}(\\cos\\theta)}{a_{s,0}(t)}\n$$\nSubstituting the expression for $a_{s,0}(t)$:\n$$\n\\frac{\\Delta a_{s}}{a_{s,0}} = \\frac{\\frac{p_{0}(t)}{\\Sigma(t)}\\epsilon\\,P_{2}(\\cos\\theta)}{\\frac{p_{0}(t) - p_{int}(t)}{\\Sigma(t)}} = \\left(\\frac{p_{0}(t)}{p_{0}(t) - p_{int}(t)}\\right) \\epsilon\\,P_{2}(\\cos\\theta)\n$$\nThis is the angular dependence of the fractional variation in acceleration.\n\n### Part 2: Derivation of the Shape Coefficient $a_{2}$\n\nThe position of the shell at a given polar angle $\\theta$ and time $t$ is denoted by $R(\\theta, t)$. The motion is purely radial, so the acceleration is $a_s(\\theta, t) = -\\ddot{R}(\\theta, t)$, where the double dot denotes the second time derivative.\n\nWe linearize the problem by decomposing the radius into a mean component $R_0(t)$ and a small perturbation $\\delta R(\\theta, t)$:\n$$\nR(\\theta, t) = R_0(t) + \\delta R(\\theta, t)\n$$\nThe perturbation is driven by the $P_2$ mode in the pressure, so we assume its shape also follows a $P_2$ dependence:\n$$\n\\delta R(\\theta, t) = \\delta R_2(t) P_2(\\cos \\theta)\n$$\nThe equation of motion is $\\ddot{R}(\\theta, t) = -a_s(\\theta, t)$. Applying the decomposition:\n$$\n\\ddot{R}_0(t) + \\delta\\ddot{R}_2(t)P_2(\\cos \\theta) = -a_{s,0}(t) - \\frac{p_{0}(t)}{\\Sigma(t)}\\epsilon\\,P_{2}(\\cos\\theta)\n$$\nEquating the coefficients of the basis functions (1 and $P_2(\\cos \\theta)$) gives two equations:\n1. Mean motion: $\\ddot{R}_0(t) = -a_{s,0}(t) = -\\frac{p_0(t) - p_{int}(t)}{\\Sigma(t)}$\n2. Perturbed motion: $\\delta\\ddot{R}_2(t) = -\\frac{p_0(t)\\epsilon}{\\Sigma(t)}$\n\nTo obtain a closed-form solution for $a_2$ in terms of only the convergence $C$ and $\\epsilon$, we must eliminate the dependence on the unspecified time-dependent functions $p_0(t)$, $p_{int}(t)$, and $\\Sigma(t)$. We note that during the acceleration phase of an ICF implosion, the ablation pressure is typically much larger than the internal fill pressure, i.e., $p_0(t) \\gg p_{int}(t)$. Under this standard and physically motivated approximation, the equation for the mean motion simplifies to $\\ddot{R}_0(t) \\approx -p_0(t)/\\Sigma(t)$.\n\nSubstituting this approximation into the equation for the perturbation:\n$$\n\\delta\\ddot{R}_2(t) \\approx \\left(\\ddot{R}_0(t)\\right) \\epsilon\n$$\nThis provides a direct relationship between the perturbed and mean kinematics, independent of the specific drive history. We now integrate this equation twice with respect to time from $t=0$ to a general time $t$. The initial conditions are that the shell starts from rest with a perfect spherical shape:\n- $R(\\theta, 0) = R_i \\implies R_0(0) = R_i$ and $\\delta R(\\theta, 0) = 0$, so $\\delta R_2(0) = 0$.\n- $\\dot{R}(\\theta, 0) = 0 \\implies \\dot{R}_0(0) = 0$ and $\\delta\\dot{R}(\\theta, 0) = 0$, so $\\delta\\dot{R}_2(0) = 0$.\n\nIntegrating $\\delta\\ddot{R}_2(t) = \\epsilon \\ddot{R}_0(t)$ once:\n$$\n\\int_0^t \\delta\\ddot{R}_2(\\tau)d\\tau = \\epsilon \\int_0^t \\ddot{R}_0(\\tau)d\\tau\n$$\n$$\n\\delta\\dot{R}_2(t) - \\delta\\dot{R}_2(0) = \\epsilon \\left( \\dot{R}_0(t) - \\dot{R}_0(0) \\right)\n$$\nApplying the initial conditions $\\delta\\dot{R}_2(0) = 0$ and $\\dot{R}_0(0) = 0$:\n$$\n\\delta\\dot{R}_2(t) = \\epsilon \\dot{R}_0(t)\n$$\nIntegrating a second time:\n$$\n\\int_0^t \\delta\\dot{R}_2(\\tau)d\\tau = \\epsilon \\int_0^t \\dot{R}_0(\\tau)d\\tau\n$$\n$$\n\\delta R_2(t) - \\delta R_2(0) = \\epsilon \\left( R_0(t) - R_0(0) \\right)\n$$\nApplying the initial conditions $\\delta R_2(0) = 0$ and $R_0(0) = R_i$:\n$$\n\\delta R_2(t) = \\epsilon \\left( R_0(t) - R_i \\right)\n$$\nThis relation holds for the entire acceleration phase. We evaluate it at the final time $t_f$. At this time, the mean radius is $R_0(t_f) = R_f$. The perturbation amplitude is given by the problem definition of the final shape: $R_f(\\theta) = R_f [1 + a_2 P_2(\\cos\\theta)]$. Comparing this with $R(t_f, \\theta) = R_0(t_f) + \\delta R_2(t_f)P_2(\\cos \\theta)$, we identify $\\delta R_2(t_f) = R_f a_2$.\n\nSubstituting these into our integrated kinematic relation at $t=t_f$:\n$$\nR_f a_2 = \\epsilon (R_f - R_i)\n$$\nSolving for $a_2$:\n$$\na_2 = \\epsilon \\frac{R_f - R_i}{R_f} = \\epsilon \\left(1 - \\frac{R_i}{R_f}\\right)\n$$\nUsing the definition of the convergence ratio $C = R_i/R_f$:\n$$\na_2 = \\epsilon(1-C)\n$$\n\n### Part 3: Evaluation for Given Parameters\n\nThe problem provides the following values:\n- Initial radius: $R_i = 1.0~\\mathrm{mm}$\n- Final mean radius: $R_f = 0.2~\\mathrm{mm}$\n- Pressure mode amplitude: $\\epsilon = 0.01$\n\nFirst, we calculate the convergence ratio $C$:\n$$\nC = \\frac{R_i}{R_f} = \\frac{1.0}{0.2} = 5\n$$\nNow, we use the derived formula for $a_2$:\n$$\na_2 = \\epsilon(1-C) = 0.01 \\times (1 - 5) = 0.01 \\times (-4)\n$$\n$$\na_2 = -0.04\n$$\nThe value of $a_2$ is a dimensionless number representing the amplitude of the final shell shape distortion in units of the final radius. The negative sign indicates that for a pressure that is highest at the pole ($\\epsilon  0$), the shell moves inward fastest at the pole, resulting in a shape that is flattened (oblate) relative to the mean sphere, i.e., the radius at the pole is smaller than the mean radius.",
            "answer": "$$\\boxed{-0.04}$$"
        },
        {
            "introduction": "While analytical models provide crucial insight, real-world ICF design relies on complex numerical simulations. This practice bridges the gap by introducing computational sensitivity analysis, an essential tool for understanding and optimizing hohlraum performance . You will implement a model to compute how the drive asymmetry metric $|a_2|$ responds to changes in laser cone powers and pointings, and verify your results using standard numerical techniques, mirroring the workflow used to tune designs in large-scale radiation-hydrodynamics codes.",
            "id": "3995554",
            "problem": "Consider a simplified, axisymmetric numerical model of a cylindrical hohlraum driving a spherical capsule in Inertial Confinement Fusion (ICF). The capsule drive nonuniformity is characterized by the degree-$2$ Legendre coefficient $a_2$ of the incident radiation intensity over the capsule surface. The axisymmetric intensity $I(\\theta)$ is modeled as the superposition of contributions from $N$ laser cone groups, indexed by $i \\in \\{1,\\dots,N\\}$, each characterized by a cone power $P_i$, a pointing polar angle $\\theta_i$ (in radians), and a width parameter $s_i$ (dimensionless), and mapped to the capsule through a kernel defined on $\\mu = \\cos\\theta \\in [-1,1]$. The following definitions and assumptions apply.\n\nFundamental base:\n- The intensity over the sphere is expanded in Legendre polynomials $P_\\ell(\\mu)$, using the orthogonality relation\n$$\\int_{-1}^{1} P_\\ell(\\mu) P_m(\\mu)\\, d\\mu = \\frac{2}{2\\ell+1} \\delta_{\\ell m},$$\nand the coefficient for order $\\ell$ is given by\n$$a_\\ell = \\frac{2\\ell+1}{2}\\int_{-1}^{1} I(\\mu) P_\\ell(\\mu)\\, d\\mu,$$\nwhere $I(\\mu)$ is the axisymmetric intensity as a function of $\\mu$.\n\nModel specification:\n- The axisymmetric intensity is a sum of cone-group kernels,\n$$I(\\mu) = \\sum_{i=1}^{N} P_i\\, K_i(\\mu;\\mu_i,s_i),$$\nwhere $\\mu_i = \\cos\\theta_i$ and $K_i(\\mu;\\mu_i,s_i)$ is a normalized Gaussian in $\\mu$,\n$$K_i(\\mu;\\mu_i,s_i) = \\frac{\\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)}{\\displaystyle S_i(\\mu_i)}, \\quad S_i(\\mu_i) = \\int_{-1}^{1} \\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)\\, d\\mu.$$\n- The coefficient of interest is\n$$a_2 = \\frac{5}{2}\\int_{-1}^{1} I(\\mu) P_2(\\mu)\\, d\\mu,$$\nwith $P_2(\\mu) = \\frac{1}{2}(3\\mu^2 - 1)$.\n- The objective for sensitivity analysis is the gradient of the absolute value $|a_2|$ with respect to the vector of cone powers $\\mathbf{P} = (P_1,\\dots,P_N)$ and the vector of cone pointing angles $\\boldsymbol{\\theta} = (\\theta_1,\\dots,\\theta_N)$.\n\nAnalytical gradient target:\n- For $a_2 \\neq 0$, the chain rule implies\n$$\\nabla_{\\mathbf{P}} |a_2| = \\operatorname{sign}(a_2)\\, \\nabla_{\\mathbf{P}} a_2,\\qquad \\nabla_{\\boldsymbol{\\theta}} |a_2| = \\operatorname{sign}(a_2)\\, \\nabla_{\\boldsymbol{\\theta}} a_2,$$\nwith $\\operatorname{sign}(a_2) = +1$ if $a_2  0$ and $\\operatorname{sign}(a_2) = -1$ if $a_2  0$; if $a_2 = 0$, take $\\operatorname{sign}(a_2) = 0$.\n- Using differentiation under the integral sign,\n$$\\frac{\\partial a_2}{\\partial P_i} = \\frac{5}{2}\\int_{-1}^{1} K_i(\\mu;\\mu_i,s_i)\\, P_2(\\mu)\\, d\\mu,$$\nand\n$$\\frac{\\partial a_2}{\\partial \\theta_i} = \\frac{5}{2}\\, P_i \\int_{-1}^{1} \\frac{\\partial K_i(\\mu;\\mu_i,s_i)}{\\partial \\theta_i}\\, P_2(\\mu)\\, d\\mu.$$\n- With $\\mu_i = \\cos\\theta_i$, the chain rule gives $\\frac{\\partial}{\\partial \\theta_i} = \\frac{\\partial}{\\partial \\mu_i}\\frac{d\\mu_i}{d\\theta_i} = -\\sin\\theta_i\\, \\frac{\\partial}{\\partial \\mu_i}$, and\n$$\\frac{\\partial K_i}{\\partial \\mu_i} = \\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)\\left[-\\frac{S_i'(\\mu_i)}{S_i(\\mu_i)^2} + \\frac{\\mu-\\mu_i}{s_i^2\\, S_i(\\mu_i)}\\right],$$\nwith\n$$S_i'(\\mu_i) = \\frac{\\partial S_i}{\\partial \\mu_i} = \\int_{-1}^{1} \\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)\\, \\frac{\\mu-\\mu_i}{s_i^2}\\, d\\mu.$$\n\nNumerical requirements:\n- All integrals over $\\mu \\in [-1,1]$ must be evaluated numerically with sufficiently accurate quadrature. Angles must be handled and reported in radians. Powers $P_i$ and widths $s_i$ are dimensionless.\n- Verify the analytical gradients against central finite differences for each parameter in the test suite. Use a finite-difference step of $\\varepsilon_P = 10^{-8}$ for powers and $\\varepsilon_\\theta = 10^{-6}$ for angles. For angles at or near the boundaries $0$ or $\\pi$, use one-sided differences to remain within the domain $[0,\\pi]$.\n- A gradient component is considered verified if the absolute error is less than $10^{-6}$ or the relative error is less than $10^{-5}$.\n\nTest suite:\n- Use $N=3$ cones in each case. For each case, provide $P$, $\\theta$ (in radians), and $s$ as specified:\n    1. Case $1$: $P = [1.0, 0.8, 0.6]$, $\\theta = [0.6, 1.0, 1.3]$, $s = [0.20, 0.18, 0.15]$.\n    2. Case $2$: $P = [0.0, 1.0, 0.7]$, $\\theta = [0.5, 1.2, 1.4]$, $s = [0.10, 0.20, 0.20]$.\n    3. Case $3$: $P = [0.9, 0.9, 0.9]$, $\\theta = [0.05, 1.6, 3.09]$, $s = [0.10, 0.10, 0.10]$.\n    4. Case $4$: $P = [1.1, 0.7, 0.5]$, $\\theta = [0.7, 1.4, 2.4]$, $s = [0.25, 0.20, 0.15]$.\n\nYour task:\n- Implement a program that computes, for each test case, the analytical gradients $\\nabla_{\\mathbf{P}} |a_2|$ and $\\nabla_{\\boldsymbol{\\theta}} |a_2|$ using the formulas above, verifies them against finite-difference approximations, and returns, for each case, a list containing:\n    - the list of gradient components with respect to powers $[\\,\\partial |a_2|/\\partial P_1, \\partial |a_2|/\\partial P_2, \\partial |a_2|/\\partial P_3\\,]$,\n    - the list of gradient components with respect to angles $[\\,\\partial |a_2|/\\partial \\theta_1, \\partial |a_2|/\\partial \\theta_2, \\partial |a_2|/\\partial \\theta_3\\,]$,\n    - a boolean indicating whether all power-gradient components are verified within tolerance,\n    - a boolean indicating whether all angle-gradient components are verified within tolerance.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is formatted as: \n$$[[gP_1,gP_2,gP_3],[g\\theta_1,g\\theta_2,g\\theta_3],\\text{boolP},\\text{bool}\\theta],$$ \nfor example, \n$$[[0.1,0.2,0.3],[0.01,0.02,0.03],True,True].$$\n\nAngle unit requirement: All angles are in radians.\n\nOutput format requirement: A single line containing a list of the four case results, each formatted as above, with no extra text.",
            "solution": "The user has provided a computational problem in the field of Inertial Confinement Fusion (ICF) and requests a solution that computes and verifies the sensitivity of a drive asymmetry metric to laser parameters.\n\n### Problem Validation\n\nFirst, the problem statement is subjected to a rigorous validation process.\n\n**Step 1: Extracted Givens**\n- **Objective:** Compute the gradients $\\nabla_{\\mathbf{P}} |a_2|$ and $\\nabla_{\\boldsymbol{\\theta}} |a_2|$ for a drive asymmetry coefficient $a_2$.\n- **Legendre Expansion:** The coefficient $a_\\ell$ of an axisymmetric intensity profile $I(\\mu)$ is $a_\\ell = \\frac{2\\ell+1}{2}\\int_{-1}^{1} I(\\mu) P_\\ell(\\mu)\\, d\\mu$, where $\\mu = \\cos\\theta$.\n- **Model for Intensity:** $I(\\mu) = \\sum_{i=1}^{N} P_i\\, K_i(\\mu;\\mu_i,s_i)$, where $P_i$ is cone power, $\\mu_i = \\cos\\theta_i$ for cone angle $\\theta_i$, and $s_i$ is a width parameter.\n- **Kernel Function:** The kernel $K_i$ is a normalized Gaussian in $\\mu$, $K_i(\\mu;\\mu_i,s_i) = \\frac{\\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)}{S_i(\\mu_i)}$, with normalization $S_i(\\mu_i) = \\int_{-1}^{1} \\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)\\, d\\mu$.\n- **Coefficient of Interest:** $a_2 = \\frac{5}{2}\\int_{-1}^{1} I(\\mu) P_2(\\mu)\\, d\\mu$, using $P_2(\\mu) = \\frac{1}{2}(3\\mu^2 - 1)$.\n- **Analytical Gradient Formulas:**\n    - $\\nabla |a_2| = \\operatorname{sign}(a_2)\\, \\nabla a_2$ (for $a_2 \\neq 0$).\n    - $\\frac{\\partial a_2}{\\partial P_i} = \\frac{5}{2}\\int_{-1}^{1} K_i(\\mu;\\mu_i,s_i)\\, P_2(\\mu)\\, d\\mu$.\n    - $\\frac{\\partial a_2}{\\partial \\theta_i} = \\frac{5}{2}\\, P_i \\int_{-1}^{1} \\frac{\\partial K_i}{\\partial \\theta_i}\\, P_2(\\mu)\\, d\\mu = \\frac{5}{2}\\, P_i (-\\sin\\theta_i) \\int_{-1}^{1} \\frac{\\partial K_i}{\\partial \\mu_i}\\, P_2(\\mu)\\, d\\mu$.\n    - $\\frac{\\partial K_i}{\\partial \\mu_i} = \\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)\\left[-\\frac{S_i'(\\mu_i)}{S_i(\\mu_i)^2} + \\frac{\\mu-\\mu_i}{s_i^2\\, S_i(\\mu_i)}\\right]$.\n    - $S_i'(\\mu_i) = \\int_{-1}^{1} \\exp\\!\\left(-\\frac{(\\mu-\\mu_i)^2}{2 s_i^2}\\right)\\, \\frac{\\mu-\\mu_i}{s_i^2}\\, d\\mu$.\n- **Numerical Verification:**\n    - Method: Central finite differences. One-sided differences for angles near $0$ or $\\pi$.\n    - Step sizes: $\\varepsilon_P = 10^{-8}$ for powers, $\\varepsilon_\\theta = 10^{-6}$ for angles.\n    - Tolerance: Absolute error $ 10^{-6}$ or relative error $ 10^{-5}$.\n- **Test Suite:** Four cases are provided with $N=3$ cones, each specifying vectors for $P$, $\\theta$, and $s$.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientific Grounding:** The problem is well-grounded in the physics and computational methods of ICF. Using Legendre polynomials for symmetry analysis and phenomenological Gaussian kernels for laser deposition are standard practices. All mathematical formulations are correct.\n- **Well-Posedness:** The problem is fully specified. It provides all necessary equations, parameters, and a clear, unambiguous objective. A unique solution is derivable from the given information.\n- **Objectivity:** The problem is expressed in precise mathematical and computational terms, free from any subjectivity.\n- **Conclusion:** The problem statement is free of scientific flaws, contradictions, or ambiguities. It is a valid, well-posed computational physics problem.\n\n**Step 3: Verdict and Action**\n- **Verdict:** The problem is **valid**.\n- **Action:** Proceed with providing a reasoned solution.\n\n### Principled Solution Design\n\nThe task is to implement the provided analytical formulas for the gradients of the drive asymmetry metric $|a_2|$ and verify the results against a finite-difference approximation. The solution is structured logically to handle the calculation for each test case. An object-oriented approach is adopted, encapsulating the logic for a single test case within a class to manage complexity and state.\n\n**1. Numerical Integration**\nAll required calculations involve definite integrals over the domain $\\mu \\in [-1, 1]$. These are one-dimensional, well-behaved integrals that can be evaluated to high precision using numerical quadrature. The `scipy.integrate.quad` function, which implements a library of adaptive quadrature routines, is selected for this purpose due to its accuracy and robustness.\n\n**2. Calculation of Asymmetry Coefficient $a_2$**\nThe core quantity is the asymmetry coefficient $a_2$. A function is designed to compute its value for a given set of parameters $(\\mathbf{P}, \\boldsymbol{\\theta}, \\mathbf{s})$.\n$$a_2(\\mathbf{P}, \\boldsymbol{\\theta}, \\mathbf{s}) = \\frac{5}{2} \\sum_{i=1}^{N} P_i \\int_{-1}^{1} K_i(\\mu; \\mu_i, s_i) P_2(\\mu) d\\mu$$\nwhere $\\mu_i = \\cos \\theta_i$. The implementation of this formula involves an outer loop over the $N$ cones. For each cone $i$, the normalization factor $S_i(\\mu_i)$ is first computed via numerical integration. Then, the contribution of cone $i$ to $a_2$ is calculated by numerically integrating its kernel $K_i$ multiplied by the Legendre polynomial $P_2(\\mu)$. These contributions are summed and scaled by $5/2$. To optimize performance, especially for the numerous evaluations required by finite differences, the results of these calculations are memoized (cached).\n\n**3. Calculation of Analytical Gradients**\nThe gradients $\\nabla_{\\mathbf{P}} |a_2|$ and $\\nabla_{\\boldsymbol{\\theta}} |a_2|$ are computed using the provided analytical expressions. This requires first computing the nominal value of $a_2$ to determine its sign, $\\operatorname{sign}(a_2)$.\n\n- **Gradient with respect to Power, $\\partial |a_2| / \\partial P_i$**:\nThe calculation follows the formula $\\frac{\\partial |a_2|}{\\partial P_i} = \\operatorname{sign}(a_2) \\frac{\\partial a_2}{\\partial P_i}$.\n$$\\frac{\\partial a_2}{\\partial P_i} = \\frac{5}{2} \\int_{-1}^{1} K_i(\\mu; \\mu_i, s_i) P_2(\\mu) d\\mu$$\nThis integral is identical to the one computed for the $i$-th cone's contribution to $a_2/P_i$, and its value is reused.\n\n- **Gradient with respect to Angle, $\\partial |a_2| / \\partial \\theta_i$**:\nThis calculation is more involved. Using the chain rule, we have $\\frac{\\partial |a_2|}{\\partial \\theta_i} = \\operatorname{sign}(a_2) \\frac{\\partial a_2}{\\partial \\theta_i} = \\operatorname{sign}(a_2) (-\\sin\\theta_i) \\frac{\\partial a_2}{\\partial \\mu_i}$. The term $\\frac{\\partial a_2}{\\partial \\mu_i}$ requires evaluating an integral involving the derivative of the kernel, $\\frac{\\partial K_i}{\\partial \\mu_i}$.\n$$\\frac{\\partial a_2}{\\partial \\mu_i} = \\frac{5}{2} P_i \\int_{-1}^{1} \\frac{\\partial K_i(\\mu; \\mu_i, s_i)}{\\partial \\mu_i} P_2(\\mu) d\\mu$$\nImplementing this requires first computing the derivative of the normalization factor, $S'_i(\\mu_i)$, via numerical integration. Then, the full integrand, containing terms with $S_i$, $S'_i$, and the exponential function, is constructed and numerically integrated. The final gradient component is assembled by applying the chain rule factor $(-\\sin\\theta_i)$ and the sign factor $\\operatorname{sign}(a_2)$.\n\n**4. Verification with Finite Differences**\nThe analytical gradients are verified against numerical approximations.\n\n- **Finite Difference Method:** The gradient of the absolute value, $\\nabla|f(x)|$, is approximated. For a generic parameter $x_i$, the central difference formula is:\n$$\\frac{\\partial |a_2|}{\\partial x_i} \\approx \\frac{|a_2(x_i + \\varepsilon)| - |a_2(x_i - \\varepsilon)|}{2\\varepsilon}$$\n- **Boundary Conditions for Angles:** For angles $\\theta_i$ near the domain boundaries $[0, \\pi]$, one-sided differences are used to ensure the perturbed angle remains valid. If $\\theta_i  \\varepsilon_\\theta$, a forward difference is used. If $\\theta_i  \\pi - \\varepsilon_\\theta$, a backward difference is used. For all other cases, the more accurate central difference is employed. All test cases provided fall within the range where central differences can be safely applied.\n- **Tolerance Check:** A verification component is deemed successful if the absolute error between the analytical and numerical gradient is less than $10^{-6}$, or if the relative error is less than $10^{-5}$. The relative error check is guarded against division by zero for cases where the analytical gradient is close to zero.\n\n**5. Program Structure**\nThe final program defines a `SymmetryCalculator` class that encapsulates all the logic for a single test case. Its methods compute $a_2$, the analytical gradients, and perform the finite-difference verification. A main `solve` function iterates through the list of test cases, instantiates the calculator for each, runs the analysis, and formats the output string as specified. This modular design ensures clarity, correctness, and reusability of the code.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom functools import lru_cache\n\ndef format_result(gP, gTh, boolP, boolTh):\n    \"\"\"Formats the result for a single test case into the required string format.\"\"\"\n    p_str = f\"[{gP[0]},{gP[1]},{gP[2]}]\"\n    th_str = f\"[{gTh[0]},{gTh[1]},{gTh[2]}]\"\n    # Python's bool __str__ method produces 'True'/'False' which is correct.\n    return f\"[{p_str},{th_str},{boolP},{boolTh}]\"\n\nclass SymmetryCalculator:\n    \"\"\"\n    Encapsulates the calculation and verification of drive asymmetry gradients\n    for a given set of ICF cone parameters.\n    \"\"\"\n    def __init__(self, P, theta, s):\n        self.P_nominal = np.array(P, dtype=float)\n        self.theta_nominal = np.array(theta, dtype=float)\n        self.s = np.array(s, dtype=float)\n        self.N = len(self.P_nominal)\n        \n        # Finite difference steps\n        self.eps_P = 1e-8\n        self.eps_theta = 1e-6\n        \n        # Verification tolerances\n        self.abs_err_tol = 1e-6\n        self.rel_err_tol = 1e-5\n\n    @staticmethod\n    def _P2(mu):\n        \"\"\"Evaluates the Legendre polynomial P_2(mu).\"\"\"\n        return 0.5 * (3 * mu**2 - 1)\n\n    @lru_cache(maxsize=None)\n    def _get_S(self, mu_i, s_i):\n        \"\"\"Computes the normalization factor S_i for the Gaussian kernel.\"\"\"\n        integrand = lambda mu: np.exp(-(mu - mu_i)**2 / (2 * s_i**2))\n        return quad(integrand, -1, 1)[0]\n\n    @lru_cache(maxsize=None)\n    def _get_S_prime(self, mu_i, s_i):\n        \"\"\"Computes the derivative of the normalization factor, S'_i.\"\"\"\n        integrand = lambda mu: ((mu - mu_i) / s_i**2) * np.exp(-(mu - mu_i)**2 / (2 * s_i**2))\n        return quad(integrand, -1, 1)[0]\n    \n    @lru_cache(maxsize=None)\n    def get_a2(self, P_tuple, theta_tuple):\n        \"\"\"Computes the asymmetry coefficient a2 for a given P and theta.\"\"\"\n        P = np.array(P_tuple)\n        theta = np.array(theta_tuple)\n        mu_vals = np.cos(theta)\n        \n        total_a2 = 0.0\n        for i in range(self.N):\n            if P[i] == 0:\n                continue\n            \n            mu_i, s_i = mu_vals[i], self.s[i]\n            S_i = self._get_S(mu_i, s_i)\n            if S_i == 0: continue\n            \n            integrand = lambda mu: (np.exp(-(mu - mu_i)**2 / (2 * s_i**2)) / S_i) * self._P2(mu)\n            integral_val, _ = quad(integrand, -1, 1)\n            total_a2 += P[i] * integral_val\n            \n        return (5.0 / 2.0) * total_a2\n\n    def calculate_analytical_gradients(self):\n        \"\"\"Calculates the analytical gradients of |a2| w.r.t. P and theta.\"\"\"\n        a2_nominal = self.get_a2(tuple(self.P_nominal), tuple(self.theta_nominal))\n        sign_a2 = np.sign(a2_nominal)\n        \n        mu_nominal = np.cos(self.theta_nominal)\n        \n        grad_P = np.zeros(self.N)\n        for i in range(self.N):\n            mu_i, s_i = mu_nominal[i], self.s[i]\n            S_i = self._get_S(mu_i, s_i)\n            if S_i == 0: continue\n            \n            integrand = lambda mu: (np.exp(-(mu - mu_i)**2 / (2 * s_i**2)) / S_i) * self._P2(mu)\n            integral_val, _ = quad(integrand, -1, 1)\n            da2_dPi = (5.0 / 2.0) * integral_val\n            grad_P[i] = sign_a2 * da2_dPi\n\n        grad_theta = np.zeros(self.N)\n        for i in range(self.N):\n            P_i, theta_i, s_i = self.P_nominal[i], self.theta_nominal[i], self.s[i]\n            if P_i == 0.0:\n                grad_theta[i] = 0.0\n                continue\n                \n            mu_i = mu_nominal[i]\n            S_i = self._get_S(mu_i, s_i)\n            if S_i == 0: continue\n            \n            S_prime_i = self._get_S_prime(mu_i, s_i)\n            \n            def dKi_dmu_i(mu, mu_i_local, s_i_local, S_i_local, S_prime_i_local):\n                exp_term = np.exp(-(mu - mu_i_local)**2 / (2 * s_i_local**2))\n                term1 = (mu - mu_i_local) / (s_i_local**2 * S_i_local)\n                term2 = -S_prime_i_local / (S_i_local**2)\n                return exp_term * (term1 + term2)\n\n            integrand = lambda mu: dKi_dmu_i(mu, mu_i, s_i, S_i, S_prime_i) * self._P2(mu)\n            \n            integral_val, _ = quad(integrand, -1, 1)\n            \n            da2_d_mui = (5.0 / 2.0) * P_i * integral_val\n            da2_d_thetai = da2_d_mui * (-np.sin(theta_i))\n            grad_theta[i] = sign_a2 * da2_d_thetai\n            \n        return list(grad_P), list(grad_theta)\n\n    def _is_verified(self, an_val, fd_val):\n        \"\"\"Checks if a gradient component is verified within tolerance.\"\"\"\n        abs_err = np.abs(an_val - fd_val)\n        if abs_err  self.abs_err_tol:\n            return True\n        if np.abs(an_val)  1e-9:\n            rel_err = abs_err / np.abs(an_val)\n            if rel_err  self.rel_err_tol:\n                return True\n        return False\n\n    def verify_gradients(self, grad_P_an, grad_theta_an):\n        \"\"\"Verifies analytical gradients against finite differences.\"\"\"\n        all_P_verified = True\n        for i in range(self.N):\n            P_plus = self.P_nominal.copy()\n            P_plus[i] += self.eps_P\n            P_minus = self.P_nominal.copy()\n            P_minus[i] -= self.eps_P\n            \n            a2_plus = self.get_a2(tuple(P_plus), tuple(self.theta_nominal))\n            a2_minus = self.get_a2(tuple(P_minus), tuple(self.theta_nominal))\n            \n            fd_grad = (np.abs(a2_plus) - np.abs(a2_minus)) / (2 * self.eps_P)\n            \n            if not self._is_verified(grad_P_an[i], fd_grad):\n                all_P_verified = False\n\n        all_theta_verified = True\n        a2_nominal_val = self.get_a2(tuple(self.P_nominal), tuple(self.theta_nominal))\n        \n        for i in range(self.N):\n            theta_i = self.theta_nominal[i]\n            fd_grad = 0.0\n            \n            if self.eps_theta = theta_i = np.pi - self.eps_theta:\n                theta_plus = self.theta_nominal.copy()\n                theta_plus[i] += self.eps_theta\n                theta_minus = self.theta_nominal.copy()\n                theta_minus[i] -= self.eps_theta\n                a2_plus = self.get_a2(tuple(self.P_nominal), tuple(theta_plus))\n                a2_minus = self.get_a2(tuple(self.P_nominal), tuple(theta_minus))\n                fd_grad = (np.abs(a2_plus) - np.abs(a2_minus)) / (2 * self.eps_theta)\n            elif theta_i  self.eps_theta:\n                theta_plus = self.theta_nominal.copy()\n                theta_plus[i] += self.eps_theta\n                a2_plus = self.get_a2(tuple(self.P_nominal), tuple(theta_plus))\n                fd_grad = (np.abs(a2_plus) - np.abs(a2_nominal_val)) / self.eps_theta\n            else:\n                theta_minus = self.theta_nominal.copy()\n                theta_minus[i] -= self.eps_theta\n                a2_minus = self.get_a2(tuple(self.P_nominal), tuple(theta_minus))\n                fd_grad = (np.abs(a2_nominal_val) - np.abs(a2_minus)) / self.eps_theta\n            \n            if not self._is_verified(grad_theta_an[i], fd_grad):\n                all_theta_verified = False\n\n        return all_P_verified, all_theta_verified\n\n    def run(self):\n        \"\"\"Executes the full calculation and verification for the case.\"\"\"\n        grad_P, grad_theta = self.calculate_analytical_gradients()\n        verified_P, verified_theta = self.verify_gradients(grad_P, grad_theta)\n        return grad_P, grad_theta, verified_P, verified_theta\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the final result.\"\"\"\n    test_cases = [\n        # (P, theta, s)\n        ([1.0, 0.8, 0.6], [0.6, 1.0, 1.3], [0.20, 0.18, 0.15]),\n        ([0.0, 1.0, 0.7], [0.5, 1.2, 1.4], [0.10, 0.20, 0.20]),\n        ([0.9, 0.9, 0.9], [0.05, 1.6, 3.09], [0.10, 0.10, 0.10]),\n        ([1.1, 0.7, 0.5], [0.7, 1.4, 2.4], [0.25, 0.20, 0.15]),\n    ]\n    \n    results_str_list = []\n    for P, theta, s in test_cases:\n        calculator = SymmetryCalculator(P, theta, s)\n        gP, gTh, vP, vTh = calculator.run()\n        results_str_list.append(format_result(gP, gTh, vP, vTh))\n        \n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal in symmetry tuning is not just to analyze imbalances, but to proactively correct them. This advanced practice elevates the task from analysis to design synthesis, framing symmetry control as a constrained optimization problem . You will develop an algorithm to determine the optimal time-dependent laser power ratio, $R(t)$, that minimizes drive asymmetry throughout the implosion, demonstrating how computational physics is used to engineer robust solutions for achieving high-performance fusion.",
            "id": "3995583",
            "problem": "Given a discretized, one-dimensional time history indexed by $k \\in \\{0,1,\\dots,N-1\\}$, consider a simplified linear response model of inertial confinement fusion hohlraum symmetry in which the Legendre mode-$2$ drive imbalance amplitude $a_2(t)$ depends linearly on the inner and outer cone powers after Cross-Beam Energy Transfer (CBET). Assume the following physically motivated first-order model:\n\n1. CBET transfers a fraction $T(t)$ of the outer-cone incident power to the inner cone. Under energy conservation in the hohlraum, the effective inner and outer powers are\n$$\nP_{\\text{in}}^{\\text{eff}}(t) = P_{\\text{in}}(t) + T(t)\\,P_{\\text{out}}(t), \\qquad\nP_{\\text{out}}^{\\text{eff}}(t) = \\bigl(1 - T(t)\\bigr)\\,P_{\\text{out}}(t),\n$$\nwith $P_{\\text{in}}(t) = R(t)\\,P_{\\text{out}}(t)$, where $R(t)$ is the inner-to-outer power ratio we seek to determine.\n\n2. The mode-$2$ amplitude is modeled by\n$$\na_2(t) = H_2(t)\\,\\bigl[\\,G_{\\text{out}}(t)\\,P_{\\text{out}}^{\\text{eff}}(t) - G_{\\text{in}}(t)\\,P_{\\text{in}}^{\\text{eff}}(t)\\,\\bigr],\n$$\nwhere $H_2(t)$ is a time-dependent geometric transfer coefficient linking the drive asymmetry to the Legendre mode-$2$ response, and $G_{\\text{in}}(t), G_{\\text{out}}(t)$ are geometric coupling weights that capture cone-specific sensitivity of the hohlraum radiation field to inner and outer cones, respectively. This linear superposition is the first-order small-signal approximation consistent with energy conservation and linearized view-factor coupling.\n\n3. For computational tractability and without loss of generality, you may set $P_{\\text{out}}(t) \\equiv 1$.\n\nDefine the cost functional that penalizes symmetry imbalance and enforces time smoothness of $R(t)$:\n$$\nJ[R] = \\sum_{k=0}^{N-1} w_k\\,\\bigl(a_2(t_k)\\bigr)^2 \\;+\\; \\alpha\\,\\sum_{k=0}^{N-2}\\bigl(R(t_{k+1}) - R(t_k)\\bigr)^2,\n$$\nwhere $w_k$ are nonnegative weights and $\\alpha \\ge 0$ is a smoothing coefficient. Impose pointwise bounds $R_{\\min} \\le R(t_k) \\le R_{\\max}$.\n\nYour tasks are:\n- Derive, from the above definitions, a discrete-time quadratic program for $R(t_k)$ that minimizes $J[R]$.\n- Implement an algorithm that computes the optimal bounded ratio $R^\\star(t_k)$ by solving the unconstrained normal equations and then applying projected gradient iterations with step-size chosen from a valid upper bound on the spectral radius of the system matrix, ensuring convergence for the chosen step-size.\n- Analyze sensitivity to errors in the CBET transfer fraction $T(t)$ by computing finite-difference sensitivity metrics based on a small perturbation $\\delta T(t)$.\n\nYou must implement the following test suite, with all quantities expressed in dimensionless units:\n\n- Test case A (moderate constant CBET, sinusoidal $H_2$):\n    - $N = 64$,\n    - $T(t_k) = 0.25$,\n    - $H_2(t_k) = 0.8 + 0.2\\sin\\!\\bigl(2\\pi k/N\\bigr)$,\n    - $G_{\\text{in}}(t_k) = 1.0$, $G_{\\text{out}}(t_k) = 1.0$,\n    - $w_k = 1.0$ for all $k$,\n    - $\\alpha = 0.01$,\n    - $R_{\\min} = 0.6$, $R_{\\max} = 1.4$,\n    - $\\delta T(t_k) = 0.02\\cos\\!\\bigl(4\\pi k/N\\bigr)$.\n\n- Test case B (no CBET, constant $H_2$):\n    - $N = 64$,\n    - $T(t_k) = 0.0$,\n    - $H_2(t_k) = 1.0$,\n    - $G_{\\text{in}}(t_k) = 1.0$, $G_{\\text{out}}(t_k) = 1.0$,\n    - $w_k = 1.0$ for all $k$,\n    - $\\alpha = 0.01$,\n    - $R_{\\min} = 0.6$, $R_{\\max} = 1.4$,\n    - $\\delta T(t_k) = 0.02\\sin\\!\\bigl(2\\pi k/N\\bigr)$.\n\n- Test case C (ramped CBET, piecewise $H_2$, unequal geometry):\n    - $N = 64$,\n    - $T(t_k) = 0.1 + 0.4\\,\\frac{k}{N-1}$,\n    - $H_2(t_k) = \\begin{cases}1.0  k  N/2,\\\\ 0.6  k \\ge N/2,\\end{cases}$\n    - $G_{\\text{in}}(t_k) = 1.2$, $G_{\\text{out}}(t_k) = 0.8$,\n    - $w_k = 1.0$ for all $k$,\n    - $\\alpha = 0.02$,\n    - $R_{\\min} = 0.5$, $R_{\\max} = 1.8$,\n    - $\\delta T(t_k) = 0.02 + 0.01\\sin\\!\\bigl(2\\pi k/N\\bigr)$.\n\n- Test case D (small $H_2$, strong smoothing):\n    - $N = 64$,\n    - $T(t_k) = 0.3$,\n    - $H_2(t_k) = 0.05 + 0.05\\sin\\!\\bigl(2\\pi k/N\\bigr)$,\n    - $G_{\\text{in}}(t_k) = 1.0$, $G_{\\text{out}}(t_k) = 1.0$,\n    - $w_k = 1.0$ for all $k$,\n    - $\\alpha = 0.2$,\n    - $R_{\\min} = 0.7$, $R_{\\max} = 1.3$,\n    - $\\delta T(t_k) = 0.02\\sin\\!\\bigl(6\\pi k/N\\bigr)$.\n\nFor each test case, compute:\n- The minimized cost with nominal $T(t)$, denoted $J^\\star$.\n- The minimized cost with perturbed transfer $T(t) + \\delta T(t)$, denoted $J^\\star_{\\text{pert}}$.\n- The finite-difference sensitivity metric\n$$\nS = \\frac{J^\\star_{\\text{pert}} - J^\\star}{\\left\\|\\delta T\\right\\|_2},\n$$\nwhere $\\left\\|\\delta T\\right\\|_2 = \\Bigl(\\sum_{k=0}^{N-1} \\bigl(\\delta T(t_k)\\bigr)^2\\Bigr)^{1/2}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,\\dots]$). The sequence must concatenate, in order for test cases A, B, C, D, the triple of floats $\\bigl(J^\\star, J^\\star_{\\text{pert}}, S\\bigr)$ for each case. No other output is allowed.",
            "solution": "The problem requires the determination of an optimal inner-to-outer cone power ratio, $R(t)$, to maintain hohlraum drive symmetry in inertial confinement fusion. This is formulated as a constrained optimization problem, which we will solve using a projected gradient descent method. The analysis involves three main stages: deriving the quadratic program, designing the numerical solver, and performing a sensitivity analysis.\n\n### 1. Derivation of the Quadratic Program\n\nThe core of the problem is to minimize the cost functional $J[R]$ subject to box constraints on the control variable $R(t)$. First, we must express the cost functional $J[R]$ as a quadratic form in terms of the discretized power ratio vector $\\mathbf{R} = [R_0, R_1, \\dots, R_{N-1}]^T$, where $R_k = R(t_k)$.\n\nThe mode-$2$ drive imbalance amplitude $a_2(t_k)$ is given by:\n$$\na_2(t_k) = H_2(t_k)\\,\\bigl[\\,G_{\\text{out}}(t_k)\\,P_{\\text{out}}^{\\text{eff}}(t_k) - G_{\\text{in}}(t_k)\\,P_{\\text{in}}^{\\text{eff}}(t_k)\\,\\bigr]\n$$\nUsing the relations $P_{\\text{in}}(t) = R(t)P_{\\text{out}}(t)$, the definition of effective powers including Cross-Beam Energy Transfer (CBET), and the normalization $P_{\\text{out}}(t) \\equiv 1$, we have:\n$$\nP_{\\text{in}}^{\\text{eff}}(t_k) = R(t_k) + T(t_k) = R_k + T_k\n$$\n$$\nP_{\\text{out}}^{\\text{eff}}(t_k) = 1 - T(t_k)\n$$\nSubstituting these into the expression for $a_2(t_k)$:\n$$\na_2(t_k) = H_{2,k}\\,[G_{\\text{out},k}(1 - T_k) - G_{\\text{in},k}(R_k + T_k)]\n$$\nThis confirms that $a_2(t_k)$ is a linear function of $R_k$. Let's group terms involving $R_k$ and those that are independent of it:\n$$\na_2(t_k) = \\underbrace{(-H_{2,k} G_{\\text{in},k})}_{f_k} R_k + \\underbrace{H_{2,k}\\,[G_{\\text{out},k}(1 - T_k) - G_{\\text{in},k}T_k]}_{g_k}\n$$\nSo, we can write $a_{2,k} = f_k R_k + g_k$. In vector notation, where $\\mathbf{a}_2$, $\\mathbf{R}$, and $\\mathbf{g}$ are column vectors of length $N$, and $\\mathbf{F}$ is a diagonal matrix with $F_{kk} = f_k$, this relation becomes:\n$$\n\\mathbf{a}_2 = \\mathbf{F}\\mathbf{R} + \\mathbf{g}\n$$\nThe cost functional $J[R]$ consists of two parts: a symmetry imbalance penalty and a temporal smoothness regularizer.\n$$\nJ[R] = \\sum_{k=0}^{N-1} w_k\\,\\bigl(a_2(t_k)\\bigr)^2 \\;+\\; \\alpha\\,\\sum_{k=0}^{N-2}\\bigl(R_{k+1} - R_k\\bigr)^2\n$$\nThe first term can be written in matrix form as $\\mathbf{a}_2^T \\mathbf{W} \\mathbf{a}_2$, where $\\mathbf{W}$ is a diagonal matrix with entries $W_{kk} = w_k$:\n$$\n\\sum_{k=0}^{N-1} w_k a_{2,k}^2 = (\\mathbf{F}\\mathbf{R} + \\mathbf{g})^T \\mathbf{W} (\\mathbf{F}\\mathbf{R} + \\mathbf{g}) = \\mathbf{R}^T\\mathbf{F}^T\\mathbf{W}\\mathbf{F}\\mathbf{R} + 2\\mathbf{g}^T\\mathbf{W}\\mathbf{F}\\mathbf{R} + \\mathbf{g}^T\\mathbf{W}\\mathbf{g}\n$$\nThe second term, the Tikhonov regularization term, can be expressed as $\\alpha \\mathbf{R}^T \\mathbf{D} \\mathbf{R}$, where $\\mathbf{D}$ is the $N \\times N$ matrix representing the second-order finite difference operator (1D discrete Laplacian for these boundary conditions):\n$$\n\\mathbf{D} = \n\\begin{pmatrix}\n1  -1  0  \\dots  0 \\\\\n-1  2  -1  \\dots  0 \\\\\n0  -1  2  \\ddots  \\vdots \\\\\n\\vdots  \\vdots  \\ddots  2  -1 \\\\\n0  0  \\dots  -1  1\n\\end{pmatrix}\n$$\nCombining these terms, the cost functional becomes:\n$$\nJ(\\mathbf{R}) = \\mathbf{R}^T (\\mathbf{F}^T\\mathbf{W}\\mathbf{F} + \\alpha\\mathbf{D})\\mathbf{R} + 2(\\mathbf{F}^T\\mathbf{W}\\mathbf{g})^T \\mathbf{R} + \\mathbf{g}^T\\mathbf{W}\\mathbf{g}\n$$\nThis is a standard quadratic form. To simplify, let us define the system matrix $\\mathbf{A}$ and vector $\\mathbf{b}$:\n$$\n\\mathbf{A} = \\mathbf{F}^T\\mathbf{W}\\mathbf{F} + \\alpha\\mathbf{D} \\qquad \\text{and} \\qquad \\mathbf{b} = -\\mathbf{F}^T\\mathbf{W}\\mathbf{g}\n$$\nThe matrix $\\mathbf{A}$ is symmetric and positive definite since $\\mathbf{F}^T\\mathbf{W}\\mathbf{F}$ is diagonal with non-negative entries (positive if any $f_k \\neq 0$) and $\\mathbf{D}$ is positive semi-definite, with $\\alpha  0$ ensuring definiteness. The cost functional, ignoring the constant term $\\mathbf{g}^T\\mathbf{W}\\mathbf{g}$ which does not affect the minimizer, is:\n$$\nJ(\\mathbf{R}) = \\mathbf{R}^T \\mathbf{A} \\mathbf{R} - 2\\mathbf{b}^T \\mathbf{R}\n$$\nThe unconstrained minimum of this functional is found by setting its gradient with respect to $\\mathbf{R}$ to zero:\n$$\n\\nabla_{\\mathbf{R}} J = 2\\mathbf{A}\\mathbf{R} - 2\\mathbf{b} = \\mathbf{0} \\implies \\mathbf{A}\\mathbf{R} = \\mathbf{b}\n$$\nThe solution to this system of linear equations, $\\mathbf{R}_{\\text{uncon}} = \\mathbf{A}^{-1}\\mathbf{b}$, gives the unconstrained optimal power ratio.\n\n### 2. Algorithmic Solution: Projected Gradient Descent\n\nThe problem requires finding the minimum of $J(\\mathbf{R})$ subject to the box constraints $R_{\\min} \\le R_k \\le R_{\\max}$ for all $k$. This constrained optimization problem can be effectively solved using the projected gradient descent method. The algorithm iteratively updates the solution by taking a step in the negative gradient direction and then projecting the result back onto the feasible set (the box defined by the constraints).\n\nThe iterative update rule is:\n$$\n\\mathbf{R}^{(m+1)} = \\mathcal{P}_{[\\mathbf{R}_{\\min}, \\mathbf{R}_{\\max}]}(\\mathbf{R}^{(m)} - \\eta \\nabla J(\\mathbf{R}^{(m)}))\n$$\nwhere $m$ is the iteration index, $\\eta$ is the step size, and $\\mathcal{P}$ is the projection operator. For box constraints, the projection is a simple element-wise clipping operation:\n$$\n(\\mathcal{P}_{[\\mathbf{R}_{\\min}, \\mathbf{R}_{\\max}]}(\\mathbf{v}))_k = \\max(R_{\\min}, \\min(R_{\\max}, v_k))\n$$\nThe gradient is $\\nabla J(\\mathbf{R}) = 2(\\mathbf{A}\\mathbf{R} - \\mathbf{b})$. The algorithm proceeds as follows:\n1.  **Initialization**: Compute the unconstrained solution $\\mathbf{R}_{\\text{uncon}} = \\mathbf{A}^{-1}\\mathbf{b}$ to serve as the initial guess, $\\mathbf{R}^{(0)} = \\mathbf{R}_{\\text{uncon}}$.\n2.  **Step-Size Selection**: For the gradient descent to converge, the step size $\\eta$ must satisfy $0  \\eta  1/\\lambda_{\\max}(\\mathbf{A})$, where $\\lambda_{\\max}(\\mathbf{A})$ is the largest eigenvalue (spectral radius) of $\\mathbf{A}$. We can use an upper bound on $\\lambda_{\\max}(\\mathbf{A})$ to determine a safe step size. The Gershgorin circle theorem provides a convenient bound. For the symmetric matrix $\\mathbf{A}$, its largest eigenvalue is bounded by:\n    $$\n    \\lambda_{\\max}(\\mathbf{A}) \\le \\max_i \\left( A_{ii} + \\sum_{j \\neq i} |A_{ij}| \\right)\n    $$\n    Since $\\mathbf{A} = \\text{diag}(w_k f_k^2) + \\alpha\\mathbf{D}$, we have $A_{ii} = w_i f_i^2 + \\alpha D_{ii}$ and $A_{i,j \\neq i}$ are the non-zero off-diagonal elements of $\\alpha\\mathbf{D}$, which are $-\\alpha$. The maximum row sum of absolute off-diagonal values is $2\\alpha$. The diagonal entries of $\\mathbf{D}$ are $1$ or $2$. Thus, a simple upper bound is:\n    $$\n    \\rho_{\\text{upper}} = \\max_k(w_k f_k^2) + 4\\alpha\n    $$\n    We choose a step size $\\eta = 0.5 / \\rho_{\\text{upper}}$ to ensure robust convergence.\n3.  **Iteration**: Update the solution $\\mathbf{R}$ for a fixed number of iterations:\n    $$\n    \\mathbf{R}^{(m+1)} = \\mathcal{P}_{[\\mathbf{R}_{\\min}, \\mathbf{R}_{\\max}]}(\\mathbf{R}^{(m)} - 2\\eta(\\mathbf{A}\\mathbf{R}^{(m)} - \\mathbf{b}))\n    $$\n    After a sufficient number of iterations, $\\mathbf{R}^{(m)}$ converges to the optimal constrained solution $\\mathbf{R}^\\star$.\n\n### 3. Sensitivity Analysis\n\nThe sensitivity analysis quantifies how the minimized cost $J^\\star$ changes in response to a small perturbation $\\delta T(t)$ in the CBET transfer fraction. The procedure is as follows:\n1.  **Nominal Solution**: For a given test case, construct the matrices $\\mathbf{A}$ and vector $\\mathbf{b}$ using the nominal $T(t_k)$. Solve for the optimal $\\mathbf{R}^\\star$ using the projected gradient descent method. Compute the minimized cost $J^\\star = J[\\mathbf{R}^\\star]$ using the original cost functional definition.\n2.  **Perturbed Solution**: Create a perturbed transfer fraction $T'(t_k) = T(t_k) + \\delta T(t_k)$. This perturbation only affects the vector $\\mathbf{g}$, which in turn modifies the vector $\\mathbf{b}$. The system matrix $\\mathbf{A}$ remains unchanged. Let the new vector be $\\mathbf{b}'$. Solve the new problem $\\mathbf{A}\\mathbf{R} = \\mathbf{b}'$ with the same constraints to obtain the perturbed optimal solution $\\mathbf{R}^\\star_{\\text{pert}}$.\n3.  **Perturbed Cost**: Compute the cost $J^\\star_{\\text{pert}} = J'[\\mathbf{R}^\\star_{\\text{pert}}]$, where $J'$ is the cost functional evaluated with the perturbed inputs ($T'$ and consequently $g'$).\n4.  **Sensitivity Metric**: Calculate the finite-difference sensitivity:\n    $$\n    S = \\frac{J^\\star_{\\text{pert}} - J^\\star}{\\left\\|\\delta T\\right\\|_2}\n    $$\n    where $\\left\\|\\delta T\\right\\|_2$ is the Euclidean norm of the perturbation vector $\\delta \\mathbf{T}$.\n\nThis entire procedure is applied to each test case provided in the problem statement to generate the required numerical results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # Define test cases as per the problem statement.\n    test_cases = [\n        # Test case A\n        {\n            \"N\": 64,\n            \"T_func\": lambda k, N: 0.25,\n            \"H2_func\": lambda k, N: 0.8 + 0.2 * np.sin(2 * np.pi * k / N),\n            \"Gin_func\": lambda k, N: 1.0,\n            \"Gout_func\": lambda k, N: 1.0,\n            \"w_func\": lambda k, N: 1.0,\n            \"alpha\": 0.01,\n            \"Rmin\": 0.6,\n            \"Rmax\": 1.4,\n            \"deltaT_func\": lambda k, N: 0.02 * np.cos(4 * np.pi * k / N),\n        },\n        # Test case B\n        {\n            \"N\": 64,\n            \"T_func\": lambda k, N: 0.0,\n            \"H2_func\": lambda k, N: 1.0,\n            \"Gin_func\": lambda k, N: 1.0,\n            \"Gout_func\": lambda k, N: 1.0,\n            \"w_func\": lambda k, N: 1.0,\n            \"alpha\": 0.01,\n            \"Rmin\": 0.6,\n            \"Rmax\": 1.4,\n            \"deltaT_func\": lambda k, N: 0.02 * np.sin(2 * np.pi * k / N),\n        },\n        # Test case C\n        {\n            \"N\": 64,\n            \"T_func\": lambda k, N: 0.1 + 0.4 * k / (N - 1),\n            \"H2_func\": lambda k, N: 1.0 if k  N / 2 else 0.6,\n            \"Gin_func\": lambda k, N: 1.2,\n            \"Gout_func\": lambda k, N: 0.8,\n            \"w_func\": lambda k, N: 1.0,\n            \"alpha\": 0.02,\n            \"Rmin\": 0.5,\n            \"Rmax\": 1.8,\n            \"deltaT_func\": lambda k, N: 0.02 + 0.01 * np.sin(2 * np.pi * k / N),\n        },\n        # Test case D\n        {\n            \"N\": 64,\n            \"T_func\": lambda k, N: 0.3,\n            \"H2_func\": lambda k, N: 0.05 + 0.05 * np.sin(2 * np.pi * k / N),\n            \"Gin_func\": lambda k, N: 1.0,\n            \"Gout_func\": lambda k, N: 1.0,\n            \"w_func\": lambda k, N: 1.0,\n            \"alpha\": 0.2,\n            \"Rmin\": 0.7,\n            \"Rmax\": 1.3,\n            \"deltaT_func\": lambda k, N: 0.02 * np.sin(6 * np.pi * k / N),\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        J_star, J_star_pert, S = run_case(case)\n        all_results.extend([J_star, J_star_pert, S])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef calculate_cost(R, T, H2, Gin, Gout, w, alpha):\n    \"\"\"Calculates the cost functional J[R].\"\"\"\n    N = len(R)\n    # Calculate a2(t_k)\n    a2 = H2 * (Gout * (1 - T) - Gin * (R + T))\n    \n    # Imbalance term\n    imbalance_cost = np.sum(w * (a2**2))\n    \n    # Smoothness term\n    diff_R = R[1:] - R[:-1]\n    smoothness_cost = alpha * np.sum(diff_R**2)\n    \n    return imbalance_cost + smoothness_cost\n\n\ndef optimize_R(params):\n    \"\"\"\n    Solves for the optimal R* using projected gradient descent.\n    \"\"\"\n    N = params['N']\n    k = np.arange(N)\n    T = np.array([params['T_func'](i, N) for i in k])\n    H2 = np.array([params['H2_func'](i, N) for i in k])\n    Gin = np.array([params['Gin_func'](i, N) for i in k])\n    Gout = np.array([params['Gout_func'](i, N) for i in k])\n    w = np.array([params['w_func'](i, N) for i in k])\n    alpha = params['alpha']\n    Rmin = params['Rmin']\n    Rmax = params['Rmax']\n    \n    # 1. Construct matrices for the QP: A*R = b\n    # f_k = -H_{2,k} * G_{in,k}\n    f = -H2 * Gin\n    # g_k = H_{2,k} * [G_{out,k}(1-T_k) - G_{in,k}T_k]\n    g = H2 * (Gout * (1 - T) - Gin * T)\n    \n    # Matrix A = F^T*W*F + alpha*D, where F is diag(f) and W is diag(w)\n    # F^T*W*F is diagonal with entries w_k * f_k^2\n    A_diag = w * f**2\n    \n    # Matrix D for smoothness term\n    D = np.diag(2 * np.ones(N)) - np.diag(np.ones(N - 1), k=1) - np.diag(np.ones(N - 1), k=-1)\n    D[0, 0] = 1\n    D[N - 1, N - 1] = 1\n    \n    A = np.diag(A_diag) + alpha * D\n    \n    # Vector b = -F^T*W*g\n    b = -f * w * g\n    \n    # 2. Solve for optimal R using projected gradient descent\n    # Initial guess: unconstrained solution\n    try:\n        R_uncon = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        R_uncon = np.linalg.lstsq(A, b, rcond=None)[0]\n    \n    R = R_uncon.copy()\n\n    # Step size selection\n    rho_upper = np.max(A_diag) + 4 * alpha\n    # The gradient is 2*(A*R - b), so the effective step size in the standard\n    # formulation R_{k+1} = R_k - eta' * grad(f(R)) is eta' = eta.\n    # The update is R_{k+1} = R_k - eta * 2*(A*R-b)\n    # Convergence requires |1 - 2*eta*lambda_i|  1 for all eigenvalues lambda_i of A.\n    # This means 0  2*eta*lambda_max  2, so eta  1/lambda_max.\n    eta = 0.5 / rho_upper \n\n    num_iterations = 20000 \n    for _ in range(num_iterations):\n        grad = 2 * (A @ R - b)\n        R_new = R - eta * grad\n        R = np.clip(R_new, Rmin, Rmax)\n        # Convergence check could be added, but fixed iterations are sufficient here.\n\n    # 3. Calculate final cost\n    final_cost = calculate_cost(R, T, H2, Gin, Gout, w, alpha)\n    \n    return R, final_cost\n\n\ndef run_case(case_params):\n    \"\"\"\n    Runs a single test case for both nominal and perturbed T.\n    \"\"\"\n    N = case_params['N']\n    k = np.arange(N)\n    \n    # Nominal case\n    params_nom = case_params.copy()\n    _, J_star = optimize_R(params_nom)\n\n    # Perturbed case\n    params_pert = case_params.copy()\n    deltaT = np.array([params_pert['deltaT_func'](i, N) for i in k])\n    T_nom_func = params_pert['T_func']\n    params_pert['T_func'] = lambda i, n: T_nom_func(i, n) + deltaT[i] # Create new T function\n    \n    R_pert, J_star_pert = optimize_R(params_pert)\n\n    # Sensitivity metric\n    norm_delta_T = np.linalg.norm(deltaT)\n    if norm_delta_T == 0:\n        S = 0.0\n    else:\n        S = (J_star_pert - J_star) / norm_delta_T\n\n    return J_star, J_star_pert, S\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}