{
    "hands_on_practices": [
        {
            "introduction": "Building a trustworthy predictive model begins with a rigorous validation strategy. In time-series analysis, especially with experimental data from tokamaks, a naive cross-validation approach can lead to \"temporal data leakage,\" creating an illusion of high performance that vanishes in real-world use. This practice explores how correlations within a single plasma shot, combined with random data splitting, violate the independence assumption between training and test sets. By analyzing this common pitfall, you will learn to design a robust shot-level cross-validation scheme that provides a true estimate of your model's generalization capability, a cornerstone of reliable machine learning engineering .",
            "id": "4003862",
            "problem": "Consider a dataset collected from a Tokamak plasma experiment, consisting of $S$ discharges (shots), indexed by $i \\in \\{1,\\dots,S\\}$. Each shot $i$ provides a multivariate time series $\\{x^{(i)}(t)\\}_{t=0}^{T^{(i)}}$ of diagnostic signals, sampled at interval $\\Delta t$, where $x^{(i)}(t) \\in \\mathbb{R}^d$ are features computed causally from the window $[t-\\tau, t]$ for some fixed window length $\\tau$. For disrupted shots, let $t_d^{(i)}$ denote the disruption time. The disruption early warning task is to predict whether a time index $t$ lies in the pre-disruption interval $[t_d^{(i)} - L, t_d^{(i)})$ for some lead time $L  0$, using features $x^{(i)}(t)$ only. The correct label function is therefore $y^{(i)}(t) = \\mathbb{1}\\{t \\in [t_d^{(i)} - L, t_d^{(i)})\\}$ for disrupted shots and $y^{(i)}(t) = 0$ for non-disrupted shots.\n\nSuppose that due to logging latency and clock drift, a fixed per-shot time misalignment $\\delta^{(i)}$ is introduced when constructing labels, so that the dataset uses $\\tilde{y}^{(i)}(t) = \\mathbb{1}\\{t + \\delta^{(i)} \\in [t_d^{(i)} - L, t_d^{(i)})\\}$ with $\\delta^{(i)}$ on the order of tens of milliseconds and $|\\delta^{(i)}| \\ll T^{(i)}$. The features remain causally computed from $x^{(i)}(t)$ without using future samples.\n\nYou are tasked with performing $K$-fold Cross-Validation (CV) to estimate generalization. Two CV schemes are considered: window-level random $K$-fold CV that partitions individual time windows across folds irrespective of shot identity, and shot-level Group $K$-fold that assigns entire shots to folds. Assume the within-shot autocorrelation of the feature process is high for small lags, i.e., the autocorrelation function $\\rho^{(i)}(\\Delta) = \\mathbb{E}[\\langle x^{(i)}(t), x^{(i)}(t+\\Delta)\\rangle]/(\\|x^{(i)}(t)\\|_2 \\|x^{(i)}(t+\\Delta)\\|_2)$ satisfies $\\rho^{(i)}(\\Delta) \\approx 1$ for $|\\Delta| \\leq |\\delta^{(i)}|$.\n\nFrom first principles, use the definitions of causality, independence between training and test folds, and the effects of autocorrelation to reason about temporal leakage in CV under misaligned labels. Then, design a shot-level splitting scheme that eliminates leakage while maximizing training data diversity across operating conditions. Operating conditions are summarized per shot by scalar descriptors such as plasma current $I_p^{(i)}$, edge safety factor $q_{95}^{(i)}$, and normalized beta $\\beta_N^{(i)}$. Let the number of positive windows per disrupted shot be approximately $n_+^{(i)} \\approx L/\\Delta t$, and define per-shot class weight $w^{(i)}$ as $w^{(i)} = n_+^{(i)}$ for disrupted shots and $w^{(i)} = c \\cdot n_-^{(i)}$ for non-disrupted shots for some constant $c  0$, where $n_-^{(i)}$ counts negative windows.\n\nWhich of the following options correctly identify how misaligned labels introduce temporal leakage in CV and specify a valid shot-level splitting design that eliminates leakage while maximizing training diversity?\n\nA. In window-level random $K$-fold CV, misaligned labels $\\tilde{y}^{(i)}(t)$ imply that for a test window at time $t$ in shot $i$, there exist training windows at times $t' \\approx t + \\delta^{(i)}$ from the same shot with the same label. Because $\\rho^{(i)}(\\delta^{(i)}) \\approx 1$, $x^{(i)}(t)$ and $x^{(i)}(t')$ are nearly identical, so the training set contains near-duplicates of the test features with matched labels, violating independence and causing temporal leakage.\n\nB. A leakage-eliminating and diversity-maximizing scheme is Group $K$-fold by shot, with per-fold stratification to balance disrupted versus non-disrupted shots via weights $w^{(i)}$, and to match the empirical distributions of $\\{I_p^{(i)}, q_{95}^{(i)}, \\beta_N^{(i)}\\}$ across folds by minimizing a divergence between per-fold histograms. This ensures that all windows from a shot reside in a single fold, removes within-shot temporal overlap across train and test, enforces correct label alignment $y^{(i)}(t)$, and maximizes training diversity by balancing operating conditions.\n\nC. Randomly assigning windows to folds while ensuring each fold has the same number of windows suffices to eliminate leakage, because the number of samples per fold is the primary determinant of independence. Diversity is also maximized by equal counts alone.\n\nD. Computing features using a forward-looking window $[t, t+\\tau]$ while keeping the misaligned labels $\\tilde{y}^{(i)}(t)$ improves predictive performance without leakage, because the label shift $\\delta^{(i)}$ offsets the use of future information, making the task well-posed.\n\nSelect all correct options.",
            "solution": "The user wants me to act as a STEM professor to validate and solve a problem related to cross-validation in fusion plasma disruption prediction.\n\n### Problem Validation\n\nFirst, I must meticulously validate the problem statement.\n\n**Step 1: Extract Givens**\n\n*   **Dataset:** $S$ Tokamak discharges (shots), indexed by $i \\in \\{1,\\dots,S\\}$.\n*   **Time Series Data:** For each shot $i$, a multivariate time series $\\{x^{(i)}(t)\\}_{t=0}^{T^{(i)}}$ with sampling interval $\\Delta t$.\n*   **Features:** $x^{(i)}(t) \\in \\mathbb{R}^d$ are computed causally from the data window $[t-\\tau, t]$.\n*   **Disruption Time:** For disrupted shots, $t_d^{(i)}$ is the disruption time.\n*   **Lead Time:** A fixed lead time $L > 0$ defines the warning window.\n*   **Correct Labels:** For disrupted shots, $y^{(i)}(t) = \\mathbb{1}\\{t \\in [t_d^{(i)} - L, t_d^{(i)})\\}$; for non-disrupted shots, $y^{(i)}(t) = 0$.\n*   **Misaligned Labels:** The dataset uses labels $\\tilde{y}^{(i)}(t) = \\mathbb{1}\\{t + \\delta^{(i)} \\in [t_d^{(i)} - L, t_d^{(i)})\\}$ due to a fixed per-shot time misalignment $\\delta^{(i)}$.\n*   **Misalignment Magnitude:** $\\delta^{(i)}$ is on the order of tens of milliseconds and $|\\delta^{(i)}| \\ll T^{(i)}$.\n*   **Cross-Validation (CV) Schemes:**\n    1.  Window-level random $K$-fold CV.\n    2.  Shot-level Group $K$-fold CV.\n*   **Autocorrelation Assumption:** The within-shot feature autocorrelation function $\\rho^{(i)}(\\Delta) = \\mathbb{E}[\\langle x^{(i)}(t), x^{(i)}(t+\\Delta)\\rangle]/(\\|x^{(i)}(t)\\|_2 \\|x^{(i)}(t+\\Delta)\\|_2)$ satisfies $\\rho^{(i)}(\\Delta) \\approx 1$ for $|\\Delta| \\leq |\\delta^{(i)}|$.\n*   **Shot Descriptors:** Scalar per-shot operating conditions are given by plasma current $I_p^{(i)}$, edge safety factor $q_{95}^{(i)}$, and normalized beta $\\beta_N^{(i)}$.\n*   **Window Counts and Weights:** Positive windows per disrupted shot $n_+^{(i)} \\approx L/\\Delta t$. Per-shot class weight $w^{(i)}$ is $n_+^{(i)}$ for disrupted shots and $c \\cdot n_-^{(i)}$ for non-disrupted shots ($c > 0$, $n_-^{(i)}$ is count of negative windows).\n*   **Task:** Analyze temporal leakage in CV due to misaligned labels and design a leakage-free, diversity-maximizing shot-level splitting scheme.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem is firmly rooted in computational fusion science and machine learning, which is a major research area. Disruption prediction, Tokamak diagnostics, operating parameters ($I_p, q_{95}, \\beta_N$), time-series analysis, cross-validation, and temporal data leakage are all standard, well-established concepts in this domain. The scenario of time-base misalignment ($\\delta^{(i)}$) is a realistic data quality issue in experimental physics.\n*   **Well-Posed:** The problem provides a clear and complete setup. All terms are defined, the context is specified, and the question asks for a principled analysis of a specific phenomenon (leakage) and the design of a methodology to counteract it. A unique and meaningful analysis is possible based on the provided information.\n*   **Objective:** The problem is stated in precise, formal language, using mathematical notation and established technical terminology. It is free of subjective claims or ambiguity.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, and directly relevant to the field of machine learning for computational fusion science. I will now proceed with the solution derivation.\n\n### Solution Derivation\n\nThe problem has two parts: first, to analyze the temporal leakage introduced by a specific cross-validation scheme under the given data conditions, and second, to design a robust cross-validation scheme.\n\n**Part 1: Analysis of Temporal Leakage in Window-Level CV**\n\nThe fundamental principle of cross-validation is to estimate the generalization error of a model on unseen data. This requires that the training and test sets be independent. In the context of time series data from a single process (a plasma shot), samples at nearby time points are not independent; they are temporally correlated.\n\n1.  **CV Scheme:** The first scheme considered is window-level random $K$-fold CV. This scheme pools all time windows $(x^{(i)}(t), \\tilde{y}^{(i)}(t))$ from all shots $i \\in \\{1, \\dots, S\\}$ and randomly partitions them into $K$ folds. This means that for any given shot $i$, some of its time windows can be in the training set while others are in the test set.\n\n2.  **Autocorrelation:** The problem states that the feature autocorrelation $\\rho^{(i)}(\\Delta)$ is high for small lags $\\Delta$. Specifically, for a lag equal to the misalignment time, $\\rho^{(i)}(|\\delta^{(i)}|) \\approx 1$. This implies that the feature vectors $x^{(i)}(t)$ and $x^{(i)}(t+\\Delta)$ are nearly identical for small $\\Delta$.\n\n3.  **Violation of Independence (Leakage):** Let's consider a test sample $(x^{(i)}(t_{test}), \\tilde{y}^{(i)}(t_{test}))$ from shot $i$. Due to the random window-level splitting, it is highly probable that a sample from the same shot at a nearby time, say $(x^{(i)}(t_{train}), \\tilde{y}^{(i)}(t_{train}))$ where $t_{train} \\approx t_{test}$, is in the training set. Because of the high autocorrelation, the test features $x^{(i)}(t_{test})$ are nearly identical to the training features $x^{(i)}(t_{train})$. If their labels are also the same, the model can essentially \"memorize\" the test sample during training. This gives an artificially optimistic and invalid estimate of the model's performance on truly new data. This phenomenon is known as temporal leakage.\n\n4.  **Role of Misaligned Labels:** The label misalignment $\\delta^{(i)}$ does not cause the leakage, but it provides a specific temporal scale at which to analyze it, as highlighted in option A. The true cause is the combination of autocorrelation and splitting a single time series (a shot) across train/test sets. The misaligned label is $\\tilde{y}^{(i)}(t) = \\mathbb{1}\\{t + \\delta^{(i)} \\in [t_d^{(i)} - L, t_d^{(i)}) \\}$.\n\n**Part 2: Design of a Robust CV Scheme**\n\nA robust CV scheme must address the two primary challenges outlined:\n\n1.  **Eliminating Leakage:** To prevent the temporal leakage described above, the independence between training and test sets must be enforced at the level of the independent experimental units, which are the plasma shots. Therefore, all data (all time windows) from a single shot $i$ must belong to the same fold. This scheme is known as Group $K$-fold CV, where each shot $i$ is a group.\n\n2.  **Maximizing Training Diversity:** Tokamak plasmas are operated under varying conditions, summarized by parameters like $I_p^{(i)}, q_{95}^{(i)}, \\beta_N^{(i)}$. A simple random assignment of shots to folds could result in folds that are not representative of the overall dataset. For instance, one fold might contain a disproportionate number of high-current shots. A model trained on the other $K-1$ folds would then be tested on a regime it has not been sufficiently exposed to, leading to a poor and high-variance estimate of generalization error. To maximize diversity and obtain a stable estimate, the folds must be stratified. This means partitioning the shots such that the distribution of key characteristics is as similar as possible across all $K$ folds. The characteristics to stratify by are:\n    *   The shot outcome (disrupted vs. non-disrupted), potentially using the provided weights $w^{(i)}$ to balance the total \"importance\" of each class across folds.\n    *   The operating condition parameters $\\{I_p^{(i)}, q_{95}^{(i)}, \\beta_N^{(i)}\\}$. This is a multi-dimensional stratification problem. A common and effective method is to bin the shots based on these parameters and ensure each bin is distributed evenly across the folds, or more generally, to use an optimization algorithm that minimizes a divergence measure (like Wasserstein distance or differences in histogram moments) between the empirical distributions of these parameters in each fold.\n\nThis two-part strategy—Group $K$-fold by shot, plus stratification by shot-level properties—constitutes a state-of-the-art approach for this problem.\n\n### Option-by-Option Analysis\n\n**A. In window-level random $K$-fold CV, misaligned labels $\\tilde{y}^{(i)}(t)$ imply that for a test window at time $t$ in shot $i$, there exist training windows at times $t' \\approx t + \\delta^{(i)}$ from the same shot with the same label. Because $\\rho^{(i)}(\\delta^{(i)}) \\approx 1$, $x^{(i)}(t)$ and $x^{(i)}(t')$ are nearly identical, so the training set contains near-duplicates of the test features with matched labels, violating independence and causing temporal leakage.**\n\nThis option correctly identifies the mechanism of temporal leakage.\n1.  It correctly faults window-level random $K$-fold CV for splitting data from the same shot.\n2.  It correctly uses the given autocorrelation property, $\\rho^{(i)}(\\delta^{(i)}) \\approx 1$, to state that features separated by a lag of $\\delta^{(i)}$ are nearly identical.\n3.  It claims the labels for windows at $t$ and $t' = t+\\delta^{(i)}$ are the same. Let's check: $\\tilde{y}^{(i)}(t) = \\mathbb{1}\\{t+\\delta^{(i)} \\in [t_d^{(i)}-L, t_d^{(i)})\\}$ and $\\tilde{y}^{(i)}(t+\\delta^{(i)}) = \\mathbb{1}\\{t+2\\delta^{(i)} \\in [t_d^{(i)}-L, t_d^{(i)}) \\}$. Since the lead time $L$ is typically hundreds of milliseconds and the misalignment $\\delta^{(i)}$ is tens of milliseconds, $L \\gg |\\delta^{(i)}|$. Thus, for most time points $t$ where the label is $1$, a point $\\delta^{(i)}$ later will also have a label of $1$. The same holds for points where the label is $0$. The labels will only differ near the two boundaries of the warning window. Therefore, the claim of \"matched labels\" is approximately true for the vast majority of samples.\n4.  The conclusion that this constitutes a violation of independence and causes leakage is correct. The model is tested on data that is a near-duplicate of data it has already seen in training.\n**Verdict: Correct.**\n\n**B. A leakage-eliminating and diversity-maximizing scheme is Group $K$-fold by shot, with per-fold stratification to balance disrupted versus non-disrupted shots via weights $w^{(i)}$, and to match the empirical distributions of $\\{I_p^{(i)}, q_{95}^{(i)}, \\beta_N^{(i)}\\}$ across folds by minimizing a divergence between per-fold histograms. This ensures that all windows from a shot reside in a single fold, removes within-shot temporal overlap across train and test, enforces correct label alignment $y^{(i)}(t)$, and maximizes training diversity by balancing operating conditions.**\n\nThis option describes the robust CV scheme derived above.\n1.  `Group K-fold by shot` is the correct method to eliminate temporal leakage.\n2.  `per-fold stratification` to balance shot types and match distributions of operating parameters ($\\{I_p^{(i)}, q_{95}^{(i)}, \\beta_N^{(i)}\\}$) is the correct method to maximize diversity and ensure representative folds.\n3.  The statement that this `removes within-shot temporal overlap across train and test` is the direct, correct consequence of grouping by shot.\n4.  The clause `enforces correct label alignment y^{(i)}(t)` is imprecise. The splitting scheme itself only partitions data; it does not modify it. However, grouping by shot is a necessary prerequisite to correctly handle the per-shot misalignment $\\delta^{(i)}$ during the model training phase for each fold. Without shot-level grouping, it would be impossible to apply the correct per-shot time shift to the labels. In this sense, the described scheme *enables* the enforcement of correct alignment. Given that all other parts of this option describe the ideal methodology with high precision, this imprecision is a minor flaw in wording rather than a fundamental conceptual error. The overall scheme described is the scientifically correct one.\n**Verdict: Correct.**\n\n**C. Randomly assigning windows to folds while ensuring each fold has the same number of windows suffices to eliminate leakage, because the number of samples per fold is the primary determinant of independence. Diversity is also maximized by equal counts alone.**\n\nThis option is fundamentally incorrect.\n1.  `Randomly assigning windows to folds` is the definition of the flawed method that *causes* temporal leakage; it does not eliminate it.\n2.  The claim that `the number of samples per fold is the primary determinant of independence` is false. The source and correlation structure of the samples determine independence, not their count.\n3.  The claim that `Diversity is also maximized by equal counts alone` is false. Folds with equal numbers of windows can have vastly different distributions of operating conditions, leading to poor diversity.\n**Verdict: Incorrect.**\n\n**D. Computing features using a forward-looking window $[t, t+\\tau]$ while keeping the misaligned labels $\\tilde{y}^{(i)}(t)$ improves predictive performance without leakage, because the label shift $\\delta^{(i)}$ offsets the use of future information, making the task well-posed.**\n\nThis option proposes a non-causal modeling approach.\n1.  The task is to provide an *early warning*, which requires a causal model that uses only past and present information. Computing features from a forward-looking window $[t, t+\\tau]$ uses future information, violating causality.\n2.  The label is $\\tilde{y}^{(i)}(t) = y^{(i)}(t+\\delta^{(i)})$, which pertains to an event at time $t+\\delta^{(i)}$. Using features from $[t, t+\\tau]$ to predict this means the model is being asked to predict an event at $t+\\delta^{(i)}$ using information from a window that may contain or even succeed $t+\\delta^{(i)}$ (if $\\tau \\ge \\delta^{(i)}$). This is not prediction; it is interpolation or description.\n3.  The reasoning that the label shift `offsets` the use of future data is baseless. Using future data to predict a future event is a classic causality violation that leads to unrealistically inflated performance metrics and a model that is useless in practice.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "Many machine learning classifiers produce scores that, while good for ranking, do not represent true probabilities. For a disruption mitigation system where decisions depend on the likelihood of an event, having well-calibrated probabilities is a necessity. This exercise introduces probability calibration and the non-parametric method of isotonic regression, which corrects a model's output to better align with observed event frequencies. Through implementing the Pool-Adjacent-Violators Algorithm (PAVA), you will gain a practical tool for post-processing model outputs, ensuring they are reliable enough for quantitative risk assessment and decision-making .",
            "id": "4003844",
            "problem": "Consider a binary disruption warning model in magnetic confinement fusion that outputs a real-valued score for each pulse, intended to represent the probability of a disruption. Let the model output be denoted by the random variable $S \\in [0,1]$ and the ground truth disruption indicator be $Y \\in \\{0,1\\}$, where $Y=1$ denotes that a disruption occurred. The following concepts are fundamental.\n\n1. Probability calibration: A model is perfectly calibrated if for all $s \\in [0,1]$, it satisfies the conditional identity $\\mathbb{P}(Y=1 \\mid S=s) = s$. Calibration concerns the absolute probabilistic correctness of the outputs. Equivalently, for any set $A \\subseteq [0,1]$, one requires $\\mathbb{E}[Y \\mid S \\in A] \\approx \\mathbb{E}[S \\mid S \\in A]$, and perfect calibration achieves equality for any measurable $A$. Empirically, calibration is assessed by comparing observed frequencies of $Y=1$ with predicted scores $S$ at comparable $S$ values.\n\n2. Discrimination: Discrimination addresses relative ranking performance, not absolute probabilities. A model discriminates well if, for two pulses with different outcomes, it tends to assign the higher score to the disrupted pulse. This is often quantified by metrics such as the Area Under the Receiver Operating Characteristic (AUROC), defined as $\\mathrm{AUROC} = \\mathbb{P}(S^{+}  S^{-})$ where $S^{+}$ is the score for a disrupted pulse and $S^{-}$ is the score for a non-disrupted pulse. Discrimination does not imply calibration, and calibration does not imply discrimination; they address distinct properties.\n\nTo recalibrate a model, one can fit a nondecreasing function $g:[0,1]\\to[0,1]$ such that the calibrated output $g(S)$ better approximates $\\mathbb{P}(Y=1 \\mid S)$. A principled nonparametric approach is isotonic regression, which solves the weighted least squares problem\n$$\n\\min_{f \\in \\mathcal{F}} \\sum_{i=1}^{n} w_i \\left(f(s_i) - y_i\\right)^2,\\quad \\text{subject to } f \\text{ nondecreasing on } [0,1],\n$$\nwhere $(s_i,y_i)$ are validation pairs, $w_i0$ are sample weights (e.g., counts for tied scores), and $\\mathcal{F}$ is the set of all nondecreasing functions. The isotonic regression solution is a piecewise constant nondecreasing function that can be constructed by the Pool-Adjacent-Violators Algorithm (PAVA), which merges adjacent score bins whenever the nondecreasing constraint is violated, replacing their fitted values by the weighted average.\n\nAlgorithmic specification for this problem:\n- Group the validation data by identical scores to obtain unique sorted abscissae $\\{x_k\\}_{k=1}^{m}$, weights $\\{W_k\\}_{k=1}^{m}$ (the counts within each group), and group means $\\{\\bar{y}_k\\}_{k=1}^{m}$ where $\\bar{y}_k = \\frac{1}{W_k}\\sum_{i: s_i = x_k} y_i$.\n- Apply the Pool-Adjacent-Violators Algorithm to $(\\bar{y}_k,W_k)$ in ascending order of $x_k$ to produce blocks. Each block $B$ is a contiguous index set with a single fitted value $v_B$ equal to the weighted mean across the block and total weight $W_B=\\sum_{k\\in B} W_k$. Let $r_B$ denote the maximum $x_k$ inside block $B$ (the right edge of the block along the score axis). The sequence $\\{v_B\\}$ is nondecreasing and the sequence $\\{r_B\\}$ is strictly increasing.\n- Define the calibrated step function $g$ by right-continuous steps: for any $t \\in \\mathbb{R}$, find the smallest block $B$ such that $t \\le r_B$, and set $g(t) = v_B$. If $t  \\min_k x_k$, use the first block’s value; if $t  \\max_k x_k$, use the last block’s value. All outputs must lie in $[0,1]$.\n\nYou must implement isotonic regression using the Pool-Adjacent-Violators Algorithm from first principles, without external calibration libraries. Then, for each test case below, fit $g$ on the provided validation set and compute the recalibrated probabilities $g(t_j)$ for the specified test scores.\n\nTest suite:\n- Case A (general monotonic): Validation $(s_i,y_i)$ with $s_i = [0.02, 0.10, 0.18, 0.26, 0.34, 0.42, 0.50, 0.58, 0.66, 0.74, 0.82, 0.90]$ and $y_i = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]$. Test scores $t = [0.15, 0.45, 0.85]$.\n- Case B (constant scores boundary): Validation $s_i = [0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40]$, $y_i = [0,1,0,1,0,1,0,1,0,1]$. Test scores $t = [0.40, 0.10, 0.90]$.\n- Case C (anti-monotonic edge): Validation $s_i = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95]$, $y_i = [1,1,1,1,0,0,0,0,0,0]$. Test scores $t = [0.20, 0.60, 0.80]$.\n- Case D (ties, partial violations, out-of-range): Validation $s_i = [0.10, 0.10, 0.20, 0.20, 0.20, 0.40, 0.40, 0.60, 0.70, 0.90]$, $y_i = [0,1,0,0,1,0,1,1,1,1]$. Test scores $t = [0.05, 0.15, 0.30, 0.50, 0.80, 1.00]$.\n\nAll probabilities must be reported as decimals in $[0,1]$. Your program must output a single line with the recalibrated probabilities for each case, formatted as a comma-separated list of lists, enclosed in square brackets, with each probability rounded to six decimal places. For example, a valid output format is $[[0.123456,0.654321],[0.500000,0.500000]]$ for two cases. In this problem, produce four inner lists corresponding to the four cases in the test suite, preserving order Case A through Case D.",
            "solution": "The user-provided problem has been analyzed and is determined to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information to derive a unique solution. The problem requires the implementation of a standard statistical algorithm, isotonic regression via the Pool-Adjacent-Violators Algorithm (PAVA), to a series of well-defined test cases.\n\nThe overarching goal is to recalibrate the output scores $S$ of a binary classification model to better represent true probabilities. This is achieved by finding a non-decreasing function $g: [0,1] \\to [0,1]$ such that for a new score $S$, the recalibrated score $g(S)$ is a more accurate estimate of the conditional probability $\\mathbb{P}(Y=1 \\mid S)$. Isotonic regression provides a non-parametric method to find the best-fitting non-decreasing function in a weighted least-squares sense.\n\nThe solution proceeds in three main stages for each test case: data preprocessing, application of the Pool-Adjacent-Violators Algorithm (PAVA), and computation of calibrated probabilities for test scores using the resulting step function.\n\n### Stage 1: Data Preprocessing\n\nThe raw validation data consists of pairs $(s_i, y_i)$, where $s_i$ is the model score and $y_i \\in \\{0,1\\}$ is the ground-truth outcome. The PAVA algorithm operates on a reduced dataset of unique scores. We first group the data by identical scores.\n\nLet the set of unique scores, sorted in ascending order, be $\\{x_k\\}_{k=1}^{m}$. For each unique score $x_k$, we compute two quantities:\n1.  The weight $W_k$, which is the number of times the score $x_k$ appears in the validation set. Mathematically, $W_k = \\sum_{i=1}^{n} \\mathbf{1}_{s_i=x_k}$, where $\\mathbf{1}$ is the indicator function.\n2.  The group mean $\\bar{y}_k$, which is the observed frequency of disruptions ($Y=1$) for all pulses with score $x_k$. This is the empirical estimate of $\\mathbb{P}(Y=1 \\mid S=x_k)$. Mathematically, $\\bar{y}_k = \\frac{1}{W_k} \\sum_{i: s_i = x_k} y_i$.\n\nThe PAVA is then applied to the sequence of pairs $(\\bar{y}_k, W_k)$ ordered by $x_k$.\n\n### Stage 2: The Pool-Adjacent-Violators Algorithm (PAVA)\n\nPAVA is an iterative algorithm that finds the non-decreasing sequence $\\{\\hat{y}_k\\}_{k=1}^{m}$ that is closest to $\\{\\bar{y}_k\\}_{k=1}^{m}$ in the weighted least-squares sense. It operates by identifying and resolving \"violations\" of the non-decreasing constraint, where $\\bar{y}_k  \\bar{y}_{k-1}$.\n\nThe algorithm can be efficiently implemented by maintaining a set of active \"blocks.\" A block is a contiguous group of the initial points that are assigned a single, common fitted value.\n\n1.  **Initialization:** Start with an empty set of active blocks.\n2.  **Iteration:** Process the points $(\\bar{y}_k, W_k)$ for $k=1, \\dots, m$. For each point $k$, create a new block containing only this point. Its initial value is $\\bar{y}_k$ and its weight is $W_k$.\n3.  **Violation Check and Pooling:** After adding the new block, compare its value with the value of the block immediately preceding it. If the new block's value is smaller, a monotonicity violation has occurred.\n    -   Merge (pool) the new block with the preceding block.\n    -   The value of the newly merged block is the weighted average of the values of its constituent points: $v_{\\text{new}} = \\frac{\\sum_{j \\in \\text{merged block}} \\bar{y}_j W_j}{\\sum_{j \\in \\text{merged block}} W_j}$.\n    -   The weight of the merged block is the sum of the constituent weights.\n4.  **Cascading Merge:** This merged block is now the new last block. Repeat the violation check with the block that now precedes it. Continue pooling backwards until the non-decreasing constraint is satisfied or the beginning of the sequence is reached.\n5.  **Termination:** After all $m$ points are processed, the sequence of block values will be non-decreasing. This sequence of values, expanded back to the original length $m$, forms the calibrated isotonic solution $\\{\\hat{y}_k\\}_{k=1}^m$.\n\n### Stage 3: Constructing and Applying the Calibrated Function $g(t)$\n\nThe output of PAVA is a piecewise-constant, non-decreasing sequence $\\{\\hat{y}_k\\}_{k=1}^{m}$, where $\\hat{y}_k$ is the calibrated probability corresponding to the unique score $x_k$. This defines a step function $g(t)$.\n\nThe problem specifies a right-continuous definition for $g(t)$. The PAVA process partitions the set of unique scores $\\{x_k\\}$ into a set of disjoint, contiguous blocks $\\{B_j\\}_{j=1}^{J}$. Each block $B_j$ has a single fitted value $v_{B_j}$ and a rightmost score $r_{B_j} = \\max_{k \\in B_j} x_k$. The sequences $\\{v_{B_j}\\}$ and $\\{r_{B_j}\\}$ are strictly increasing.\n\nFor any test score $t$, the calibrated probability $g(t)$ is determined as follows:\n1.  Find the smallest block index $j$ such that $t \\le r_{B_j}$. The calibrated value is $g(t) = v_{B_j}$.\n2.  **Extrapolation:**\n    -   If $t$ is less than the minimum score in the validation set ($t  \\min_k x_k$), use the value of the first block, $g(t) = v_{B_1}$.\n    -   If $t$ is greater than the maximum score ($t  \\max_k x_k$), use the value of the last block, $g(t) = v_{B_J}$.\n\nThis procedure is applied to each test case.\n\n- **Case A (general monotonic):** The initial means $\\bar{y}_k$ are already non-decreasing ($[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]$). PAVA makes no changes. The resulting PAVA blocks are defined by the two unique values, 0 and 1. The first block has value 0 and right edge 0.34. The second block has value 1 and right edge 0.90. For test scores $t=[0.15, 0.45, 0.85]$, the calibrated values are $[0.0, 1.0, 1.0]$.\n- **Case B (constant scores boundary):** There is only one unique score $x_1=0.40$ with weight $W_1=10$ and mean $\\bar{y}_1=5/10=0.5$. The PAVA solution is a single block with value $0.5$. Thus, $g(t) = 0.5$ for all $t$. The calibrated values are $[0.5, 0.5, 0.5]$.\n- **Case C (anti-monotonic edge):** The initial means $\\bar{y}_k$ are non-increasing ($[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]$). PAVA will pool all points into a single block. The value of this block is the global weighted average, which is $\\frac{4 \\times 1 + 6 \\times 0}{10} = 0.4$. Thus, $g(t) = 0.4$ for all $t$. The calibrated values are $[0.4, 0.4, 0.4]$.\n- **Case D (ties, partial violations, out-of-range):**\n    - Preprocessing yields unique scores $x_k = [0.1, 0.2, 0.4, 0.6, 0.7, 0.9]$, weights $W_k = [2, 3, 2, 1, 1, 1]$, and means $\\bar{y}_k = [0.5, 1/3, 0.5, 1, 1, 1]$.\n    - PAVA identifies a violation between $\\bar{y}_1=0.5$ and $\\bar{y}_2 \\approx 0.333$. They are pooled into a new block covering scores $0.1$ and $0.2$, with value $(0.5 \\times 2 + 1/3 \\times 3)/(2+3) = 2/5 = 0.4$.\n    - The sequence of block values becomes $[0.4, 0.5, 1, 1, 1]$, which is non-decreasing.\n    - This defines three final blocks: Block 1 (value 0.4, right edge 0.2), Block 2 (value 0.5, right edge 0.4), and Block 3 (value 1.0, right edge 0.9).\n    - For test scores $t = [0.05, 0.15, 0.30, 0.50, 0.80, 1.00]$, the calibrated values are found using the right-continuous rule and extrapolation: $[0.4, 0.4, 0.5, 1.0, 1.0, 1.0]$.",
            "answer": "[[0.000000,1.000000,1.000000],[0.500000,0.500000,0.500000],[0.400000,0.400000,0.400000],[0.400000,0.400000,0.500000,1.000000,1.000000,1.000000]]"
        },
        {
            "introduction": "A disruption prediction model's output is just a number; the crucial next step is to use it to make an optimal decision. In fusion energy, the cost of a missed disruption (a false negative, $C_{\\mathrm{FN}}$) is vastly different from the cost of an unnecessary mitigation action (a false positive, $C_{\\mathrm{FP}}$). This problem applies Bayesian decision theory to derive the optimal decision threshold that minimizes the expected cost, given these asymmetric costs. This practice will demonstrate how to translate the abstract output of a calibrated classifier into a concrete, cost-minimizing action plan, a critical link between prediction and control in any high-stakes engineering system .",
            "id": "4003916",
            "problem": "In a tokamak disruption prediction and control system, a machine learning classifier outputs a calibrated posterior probability $q(\\mathbf{x}) = P(y=1 \\mid \\mathbf{x})$ that the plasma will disrupt within a short horizon $\\Delta t$, where $y=1$ denotes an imminent disruption and $y=0$ denotes no imminent disruption. The control decision is a binary action: trigger mitigation ($a=1$) or do not trigger mitigation ($a=0$). A false positive (FP) occurs when $a=1$ and $y=0$, incurring cost $C_{\\mathrm{FP}}$, and a false negative (FN) occurs when $a=0$ and $y=1$, incurring cost $C_{\\mathrm{FN}}$. Assume the classifier probabilities are well-calibrated under the data distribution with prior disruption rate $p = P(y=1)$.\n\nStarting from risk minimization principles and the definition of conditional expected loss (Bayes risk) for each $\\mathbf{x}$, derive the optimal threshold $\\tau^{\\star}$ such that the mitigation action is triggered if and only if $q(\\mathbf{x}) \\ge \\tau^{\\star}$ under asymmetric costs $C_{\\mathrm{FP}}$ and $C_{\\mathrm{FN}}$. Then, compute $\\tau^{\\star}$ numerically for a system with prior $p = 0.15$, $C_{\\mathrm{FP}} = 1.7$ (normalized cost units), and $C_{\\mathrm{FN}} = 9.3$ (normalized cost units), assuming the probabilities $q(\\mathbf{x})$ are calibrated posteriors with respect to the same prior $p$. Express the threshold as a decimal with no units and round your answer to four significant figures.",
            "solution": "The posed problem is valid. It is a well-defined application of Bayesian decision theory to a binary classification task with asymmetric costs, which is a standard and scientifically grounded problem in machine learning and its applications. The context of tokamak disruption prediction is a realistic and important area of research in computational fusion science. All necessary parameters are provided, and the objective is clearly stated.\n\nWe begin by establishing the framework for minimizing the expected loss, also known as the Bayes risk. The decision to trigger mitigation ($a=1$) or not ($a=0$) is made for each specific plasma state, represented by the feature vector $\\mathbf{x}$. The optimal decision rule chooses the action that minimizes the conditional expected loss, given the observation $\\mathbf{x}$.\n\nThe classifier provides the posterior probability of an imminent disruption, $q(\\mathbf{x}) = P(y=1 \\mid \\mathbf{x})$, where $y=1$ signifies a disruption and $y=0$ signifies stable operation. The probability of no disruption is therefore $P(y=0 \\mid \\mathbf{x}) = 1 - q(\\mathbf{x})$.\n\nThe problem defines a cost structure for incorrect decisions. Let $L(a, y)$ be the loss incurred for taking action $a$ when the true outcome is $y$. Based on the problem statement:\n\\begin{itemize}\n    \\item Cost of a False Positive (FP): Action is $a=1$ (mitigate), but the true state is $y=0$ (no disruption). The loss is $L(1, 0) = C_{\\mathrm{FP}}$.\n    \\item Cost of a False Negative (FN): Action is $a=0$ (do not mitigate), but the true state is $y=1$ (disruption). The loss is $L(0, 1) = C_{\\mathrm{FN}}$.\n\\end{itemize}\nThe costs for correct decisions are implicitly zero, which is the standard convention for focusing on the loss due to errors.\n\\begin{itemize}\n    \\item Cost of a True Positive (TP): Action is $a=1$, true state is $y=1$. The loss is $L(1, 1) = 0$.\n    \\item Cost of a True Negative (TN): Action is $a=0$, true state is $y=0$. The loss is $L(0, 0) = 0$.\n\\end{itemize}\n\nThe conditional expected loss, or risk, $R(a \\mid \\mathbf{x})$, for each action $a$ is the sum of the possible losses for that action, weighted by their conditional probabilities.\n\nFor the action $a=1$ (trigger mitigation), the conditional expected loss is:\n$$R(a=1 \\mid \\mathbf{x}) = L(1, 1)P(y=1 \\mid \\mathbf{x}) + L(1, 0)P(y=0 \\mid \\mathbf{x})$$\nSubstituting the known values and probabilities:\n$$R(a=1 \\mid \\mathbf{x}) = (0) \\cdot q(\\mathbf{x}) + C_{\\mathrm{FP}} \\cdot (1 - q(\\mathbf{x})) = C_{\\mathrm{FP}}(1 - q(\\mathbf{x}))$$\n\nFor the action $a=0$ (do not trigger mitigation), the conditional expected loss is:\n$$R(a=0 \\mid \\mathbf{x}) = L(0, 1)P(y=1 \\mid \\mathbf{x}) + L(0, 0)P(y=0 \\mid \\mathbf{x})$$\nSubstituting the known values and probabilities:\n$$R(a=0 \\mid \\mathbf{x}) = C_{\\mathrm{FN}} \\cdot q(\\mathbf{x}) + (0) \\cdot (1 - q(\\mathbf{x})) = C_{\\mathrm{FN}}q(\\mathbf{x})$$\n\nThe Bayes-optimal decision rule is to select the action with the minimum conditional expected loss. We should trigger mitigation ($a=1$) if the risk of doing so is less than or equal to the risk of not doing so. The problem specifies the decision rule as triggering for $q(\\mathbf{x}) \\ge \\tau^{\\star}$, which implies assigning the boundary case (equal risk) to action $a=1$. Therefore, we trigger mitigation if:\n$$R(a=1 \\mid \\mathbf{x}) \\le R(a=0 \\mid \\mathbf{x})$$\nSubstituting the expressions for the risks:\n$$C_{\\mathrm{FP}}(1 - q(\\mathbf{x})) \\le C_{\\mathrm{FN}}q(\\mathbf{x})$$\nWe now solve this inequality for $q(\\mathbf{x})$ to find the condition under which mitigation is the optimal action.\n$$C_{\\mathrm{FP}} - C_{\\mathrm{FP}}q(\\mathbf{x}) \\le C_{\\mathrm{FN}}q(\\mathbf{x})$$\n$$C_{\\mathrm{FP}} \\le C_{\\mathrm{FN}}q(\\mathbf{x}) + C_{\\mathrm{FP}}q(\\mathbf{x})$$\n$$C_{\\mathrm{FP}} \\le (C_{\\mathrm{FP}} + C_{\\mathrm{FN}})q(\\mathbf{x})$$\nAssuming $C_{\\mathrm{FP}} + C_{\\mathrm{FN}}  0$, which is physically required as costs cannot be negative, we can divide without changing the inequality's direction:\n$$q(\\mathbf{x}) \\ge \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FP}} + C_{\\mathrm{FN}}}$$\nThis inequality defines the decision rule. The mitigation action $a=1$ is chosen if and only if the classifier's output probability $q(\\mathbf{x})$ exceeds a certain threshold. By comparing this to the desired decision rule form, $q(\\mathbf{x}) \\ge \\tau^{\\star}$, we can directly identify the optimal threshold $\\tau^{\\star}$.\n$$\\tau^{\\star} = \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FP}} + C_{\\mathrm{FN}}}$$\nIt is noteworthy that this optimal threshold depends only on the costs of misclassification and is independent of the prior probability of disruption, $p=P(y=1)$. The prior is relevant for training and calibrating the classifier, such that $q(\\mathbf{x})$ is a correct posterior, but it does not enter the threshold calculation itself.\n\nNow, we compute the numerical value for $\\tau^{\\star}$ using the provided costs: $C_{\\mathrm{FP}} = 1.7$ and $C_{\\mathrm{FN}} = 9.3$.\n$$\\tau^{\\star} = \\frac{1.7}{1.7 + 9.3}$$\n$$\\tau^{\\star} = \\frac{1.7}{11.0}$$\n$$\\tau^{\\star} = 0.154545\\overline{45}...$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\\tau^{\\star} \\approx 0.1545$$\nThis is the optimal threshold for triggering the mitigation system. The action is $a=1$ if $q(\\mathbf{x}) \\ge 0.1545$ and $a=0$ otherwise.",
            "answer": "$$\n\\boxed{0.1545}\n$$"
        }
    ]
}