{
    "hands_on_practices": [
        {
            "introduction": "The foundation of Bayesian inference is the combination of prior knowledge with evidence from data to form a posterior distribution. A critical, and sometimes subtle, question is whether this resulting posterior is 'proper,' meaning it represents a valid probability distribution. This exercise explores a common scenario in experimental physics where a non-informative, improper prior is used for an instrument's calibration constant, demonstrating how observed data can provide the necessary information to yield a well-defined and useful posterior.",
            "id": "3954058",
            "problem": "In a magnetic confinement fusion experiment, a single line-of-sight channel of a neutron counting diagnostic integrates events over a fixed dwell time. A first-principles neutron transport model predicts an expected uncalibrated count $\\,\\mu_{0} \\in (0,\\infty)\\,$ for that dwell time, but the channel’s unknown absolute gain factor $\\,c \\in (0,\\infty)\\,$ must be calibrated against data. Conditional on $\\,c\\,$, the observed count $\\,y \\in \\{1,2,3,\\dots\\}\\,$ is modeled as Poisson with mean $\\,\\theta = c\\,\\mu_{0}\\,$, so that $\\,y \\mid c \\sim \\mathrm{Poisson}(c\\,\\mu_{0})\\,$. In the absence of prior calibration information, suppose the gain has the scale-invariant improper prior $\\,p(c) \\propto 1/c\\,$ on $\\,c>0\\,$.\n\nStarting from Bayes’ theorem and the definition of the Poisson likelihood, and using only standard integral identities as needed, derive whether the posterior $\\,p(c \\mid y, \\mu_{0})\\,$ is proper for the stated $\\,y\\,$, and compute its fully normalized analytical form. Provide the final answer as a single closed-form expression for the posterior density $\\,p(c \\mid y, \\mu_{0})\\,$ as a function of $\\,c\\,$, $\\,y\\,$, and $\\,\\mu_{0}\\,$. Do not use any named distributions in the derivation. Your final answer must be the explicit normalized expression for $\\,p(c \\mid y, \\mu_{0})\\,$.",
            "solution": "The user wants me to solve the problem by finding the posterior probability distribution of the gain factor.\n\n### Step 1: Extract Givens\n-   The uncalibrated expected count from a model is $\\mu_{0} \\in (0, \\infty)$.\n-   The unknown absolute gain factor is $c \\in (0, \\infty)$.\n-   The observed count is $y \\in \\{1, 2, 3, \\dots\\}$.\n-   The likelihood model is a Poisson distribution for the observed count $y$ conditional on the gain $c$, with mean $\\theta = c\\mu_{0}$. The notation is $y \\mid c \\sim \\mathrm{Poisson}(c\\mu_{0})$.\n-   The prior distribution for the gain factor $c$ is the scale-invariant improper prior $p(c) \\propto 1/c$ for $c > 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically and mathematically sound. The use of a Poisson likelihood for counting experiments is a standard and fundamental practice in physics. The choice of a scale-invariant prior, also known as a Jeffreys prior for a scale parameter, is a conventional and well-justified approach in Bayesian inference when there is no specific prior information about the magnitude of the parameter. The problem is well-posed; it provides all necessary components (likelihood, prior, and data form) to derive the posterior. Critically, the domain for the observed count $y$ is specified as the set of positive integers, $\\{1, 2, 3, \\dots\\}$, which is essential for ensuring the resulting posterior is a proper distribution. The problem is a standard exercise in Bayesian calibration and is directly relevant to the specified topic and field. It is not ambiguous, contradictory, or trivial.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Derivation\nWe begin with Bayes’ theorem, which relates the posterior probability density, $p(c \\mid y, \\mu_{0})$, to the likelihood, $p(y \\mid c, \\mu_{0})$, and the prior, $p(c)$. The posterior is proportional to the product of the likelihood and the prior:\n$$\np(c \\mid y, \\mu_{0}) \\propto p(y \\mid c, \\mu_{0}) p(c)\n$$\nThe full posterior is given by:\n$$\np(c \\mid y, \\mu_{0}) = \\frac{p(y \\mid c, \\mu_{0}) p(c)}{\\int_{0}^{\\infty} p(y \\mid c, \\mu_{0}) p(c) \\, dc}\n$$\nwhere the denominator is the marginal likelihood, or evidence, which serves as the normalization constant.\n\nThe problem provides the explicit forms for the likelihood and the prior.\n\nThe likelihood is given by the Poisson probability mass function. For an observed count $y$ and a mean of $\\theta = c\\mu_{0}$, the likelihood is:\n$$\np(y \\mid c, \\mu_{0}) = \\frac{(c\\mu_{0})^{y} \\exp(-c\\mu_{0})}{y!}\n$$\nThe domain for the gain factor is $c > 0$, and the observed count $y$ is a positive integer, $y \\in \\{1, 2, 3, \\dots\\}$.\n\nThe prior for the gain factor $c$ is given as an improper, scale-invariant prior:\n$$\np(c) \\propto \\frac{1}{c}\n$$\nWe can write this as $p(c) = K/c$ for some proportionality constant $K$.\n\nNow, we combine the likelihood and the prior to find the unnormalized posterior density. We can ignore any factors that do not depend on $c$, as they will be absorbed into a new proportionality constant.\n$$\np(c \\mid y, \\mu_{0}) \\propto \\left( \\frac{(c\\mu_{0})^{y} \\exp(-c\\mu_{0})}{y!} \\right) \\left( \\frac{1}{c} \\right)\n$$\nWe collect terms involving $c$:\n$$\np(c \\mid y, \\mu_{0}) \\propto c^{y} \\exp(-c\\mu_{0}) \\frac{1}{c} = c^{y-1} \\exp(-c\\mu_{0})\n$$\nThe terms $\\mu_{0}^{y}$ and $1/y!$ have been absorbed into the proportionality constant. This expression is the kernel of the posterior distribution.\n\nTo determine if the posterior is proper and to find its normalized form, we must evaluate the integral of this kernel over the domain of $c$, which is from $0$ to $\\infty$. Let this integral be $Z$:\n$$\nZ = \\int_{0}^{\\infty} c^{y-1} \\exp(-c\\mu_{0}) \\, dc\n$$\nThis integral must be finite and non-zero for the posterior to be proper. We perform a change of variables to evaluate it. Let $u = c\\mu_{0}$. Then $c = u/\\mu_{0}$ and $dc = du/\\mu_{0}$. The limits of integration for $u$ are also $0$ to $\\infty$, since $\\mu_{0} > 0$.\nSubstituting these into the integral:\n$$\nZ = \\int_{0}^{\\infty} \\left(\\frac{u}{\\mu_{0}}\\right)^{y-1} \\exp(-u) \\left(\\frac{1}{\\mu_{0}}\\right) du\n$$\nWe can factor out the terms that do not depend on $u$:\n$$\nZ = \\left(\\frac{1}{\\mu_{0}}\\right)^{y-1} \\left(\\frac{1}{\\mu_{0}}\\right) \\int_{0}^{\\infty} u^{y-1} \\exp(-u) \\, du = \\frac{1}{\\mu_{0}^{y}} \\int_{0}^{\\infty} u^{y-1} \\exp(-u) \\, du\n$$\nThe integral portion is the definition of the Gamma function, $\\Gamma(z) = \\int_{0}^{\\infty} t^{z-1} \\exp(-t) \\, dt$. In our case, the argument to the Gamma function is $z=y$.\n$$\n\\int_{0}^{\\infty} u^{y-1} \\exp(-u) \\, du = \\Gamma(y)\n$$\nThe normalization constant is therefore:\n$$\nZ = \\frac{\\Gamma(y)}{\\mu_{0}^{y}}\n$$\nThe Gamma function $\\Gamma(y)$ is finite for any $y > 0$. The problem states that $y \\in \\{1, 2, 3, \\dots\\}$, so $y$ is a positive integer. For positive integers, $\\Gamma(y) = (y-1)!$, which is finite. Since $\\mu_{0} > 0$ and $y \\ge 1$, the normalization constant $Z$ is finite and non-zero. Thus, the posterior distribution $p(c \\mid y, \\mu_{0})$ is proper.\n\nThe fully normalized posterior probability density is the kernel of the distribution divided by this normalization constant $Z$:\n$$\np(c \\mid y, \\mu_{0}) = \\frac{c^{y-1} \\exp(-c\\mu_{0_})}{Z} = \\frac{c^{y-1} \\exp(-c\\mu_{0})}{\\frac{\\Gamma(y)}{\\mu_{0}^{y}}}\n$$\nRearranging the terms yields the final expression for the posterior density:\n$$\np(c \\mid y, \\mu_{0}) = \\frac{\\mu_{0}^{y}}{\\Gamma(y)} c^{y-1} \\exp(-c\\mu_{0})\n$$\nThis is the analytical form of the posterior probability density for the gain factor $c$, defined for $c>0$.",
            "answer": "$$\n\\boxed{\\frac{\\mu_{0}^{y}}{\\Gamma(y)} c^{y-1} \\exp(-c\\mu_{0})}\n$$"
        },
        {
            "introduction": "Physical models often contain parameters that are subject to constraints, such as diffusion coefficients, which must be positive. This practice tackles the common challenge of incorporating such constraints into a Bayesian calibration workflow. You will learn a standard and powerful technique: reparameterizing the model from a constrained space to an unconstrained one, which simplifies computation and allows the use of standard optimization or sampling algorithms. This derivation emphasizes the formal application of the change-of-variables rule and the critical role of the Jacobian determinant in correctly transforming probability densities.",
            "id": "3954020",
            "problem": "In radial heat transport modeling for a magnetically confined plasma in a tokamak, consider a discretized diffusion representation with $d$ positive diffusion coefficients $\\theta \\in \\mathbb{R}_{+}^{d}$ at $d$ radial grid points. Under fixed-profile conditions and a local linearization of the transport response, suppose $m$ diagnostic measurements of heat flux $y \\in \\mathbb{R}^{m}$ are related to the diffusion coefficients by a linear model with additive Gaussian noise,\n$$\ny = A \\theta + \\varepsilon,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times d}$ is a known sensitivity matrix derived from the discretized flux-gradient relationship and $\\varepsilon \\sim \\mathcal{N}(0,\\Sigma)$ with a known, symmetric positive definite covariance matrix $\\Sigma \\in \\mathbb{R}^{m \\times m}$. Assume independent Gamma priors on the diffusion coefficients,\n$$\np(\\theta) = \\prod_{j=1}^{d} \\left[ \\frac{r_{j}^{k_{j}}}{\\Gamma(k_{j})} \\, \\theta_{j}^{\\,k_{j}-1} \\exp(-r_{j} \\theta_{j}) \\right], \\quad \\text{with} \\quad k_{j} > 0, \\; r_{j} > 0,\n$$\nto encode positivity and prior regularization commonly used in transport coefficient inference. To incorporate positivity constraints in the computational optimization, reparameterize $\\theta$ by an unconstrained vector $\\phi \\in \\mathbb{R}^{d}$ via $\\theta_{j} = \\exp(\\phi_{j})$ for $j=1,\\dots,d$.\n\nStarting from Bayes' theorem and the change-of-variables formula for probability densities, derive the unnormalized posterior density kernel in the transformed variable $\\phi$, explicitly accounting for the Jacobian of the transformation from $\\theta$ to $\\phi$. Your derivation must begin from the following foundational bases:\n- Bayes' theorem, stating $p(\\theta \\mid y) \\propto p(y \\mid \\theta)\\, p(\\theta)$.\n- The change-of-variables formula, stating for an invertible, differentiable map $T$ with $\\theta = T(\\phi)$, the transformed density satisfies $p(\\phi \\mid y) = p(\\theta \\mid y) \\, \\left| \\det \\left( \\frac{\\partial \\theta}{\\partial \\phi} \\right) \\right|$ evaluated at $\\theta = T(\\phi)$.\n- The Gaussian likelihood for the linear model with covariance $\\Sigma$, $p(y \\mid \\theta) \\propto \\exp\\!\\left(-\\tfrac{1}{2} (y - A \\theta)^{\\top} \\Sigma^{-1} (y - A \\theta)\\right)$.\n\nProvide your final result as a single closed-form analytic expression for the unnormalized posterior density kernel in $\\phi$, simplified as far as possible. No numerical evaluation is required. The final answer must be a single analytic expression and must not be written as an equation or an inequality.",
            "solution": "The objective is to derive the unnormalized posterior density kernel for the transformed parameters $\\phi \\in \\mathbb{R}^{d}$, where the original parameters $\\theta \\in \\mathbb{R}_{+}^{d}$ are defined by the component-wise transformation $\\theta_j = \\exp(\\phi_j)$ for $j=1, \\dots, d$.\n\nThe derivation begins with Bayes' theorem, which states that the posterior probability density of the parameters $\\theta$ given the data $y$ is proportional to the likelihood of the data given the parameters multiplied by the prior probability density of the parameters:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\n$$\nThe problem specifies the forms for the likelihood and the prior. The likelihood $p(y \\mid \\theta)$ arises from a linear model with additive Gaussian noise, $y = A \\theta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\Sigma)$. The kernel of this likelihood is given as:\n$$\np(y \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2} (y - A \\theta)^{\\top} \\Sigma^{-1} (y - A \\theta)\\right)\n$$\nThe prior on $\\theta$ is a product of independent Gamma distributions:\n$$\np(\\theta) = \\prod_{j=1}^{d} \\text{Gamma}(\\theta_j \\mid k_j, r_j) = \\prod_{j=1}^{d} \\left[ \\frac{r_{j}^{k_{j}}}{\\Gamma(k_{j})} \\, \\theta_{j}^{\\,k_{j}-1} \\exp(-r_{j} \\theta_{j}) \\right]\n$$\nFor the unnormalized posterior, we can drop the constant normalization factors, so the prior kernel is:\n$$\np(\\theta) \\propto \\prod_{j=1}^{d} \\theta_{j}^{\\,k_{j}-1} \\exp(-r_{j} \\theta_{j})\n$$\nCombining the likelihood and prior kernels, the unnormalized posterior for $\\theta$ is:\n$$\np(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2} (y - A \\theta)^{\\top} \\Sigma^{-1} (y - A \\theta)\\right) \\left( \\prod_{j=1}^{d} \\theta_{j}^{\\,k_{j}-1} \\exp(-r_{j} \\theta_{j}) \\right)\n$$\nTo find the posterior density for the transformed parameters $\\phi$, we apply the change-of-variables formula. Given an invertible, differentiable transformation $\\theta = T(\\phi)$, the new density is:\n$$\np(\\phi \\mid y) = p(\\theta \\mid y) \\Big|_{\\theta=T(\\phi)} \\cdot \\left| \\det \\left( \\frac{\\partial \\theta}{\\partial \\phi} \\right) \\right|\n$$\nHere, the transformation is $T(\\phi)$ defined by components $\\theta_j = \\exp(\\phi_j)$. First, we compute the Jacobian matrix $J = \\frac{\\partial \\theta}{\\partial \\phi}$, whose entries are $J_{ij} = \\frac{\\partial \\theta_i}{\\partial \\phi_j}$.\nSince $\\theta_i = \\exp(\\phi_i)$, its partial derivative with respect to $\\phi_j$ is:\n$$\n\\frac{\\partial \\theta_i}{\\partial \\phi_j} = \\begin{cases} \\exp(\\phi_i) & \\text{if } i=j \\\\ 0 & \\text{if } i \\neq j \\end{cases}\n$$\nThis means the Jacobian matrix is a diagonal matrix:\n$$\nJ = \\begin{pmatrix} \\exp(\\phi_1) & 0 & \\cdots & 0 \\\\ 0 & \\exp(\\phi_2) & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\exp(\\phi_d) \\end{pmatrix}\n$$\nThe determinant of a diagonal matrix is the product of its diagonal entries:\n$$\n\\det(J) = \\prod_{j=1}^{d} \\exp(\\phi_j) = \\exp\\left(\\sum_{j=1}^{d} \\phi_j\\right)\n$$\nSince $\\exp(\\phi_j) > 0$ for all real $\\phi_j$, the absolute value is redundant: $|\\det(J)| = \\det(J)$.\n\nNext, we substitute the transformation $\\theta_j = \\exp(\\phi_j)$ into the expression for $p(\\theta \\mid y)$. Let $\\exp(\\phi)$ denote the vector with components $\\exp(\\phi_j)$. The likelihood term becomes:\n$$\np(y \\mid \\theta) \\Big|_{\\theta=\\exp(\\phi)} \\propto \\exp\\left(-\\frac{1}{2} (y - A \\exp(\\phi))^{\\top} \\Sigma^{-1} (y - A \\exp(\\phi))\\right)\n$$\nThe prior term becomes:\n$$\np(\\theta) \\Big|_{\\theta=\\exp(\\phi)} \\propto \\prod_{j=1}^{d} (\\exp(\\phi_j))^{k_j-1} \\exp(-r_j \\exp(\\phi_j))\n$$\nThis can be simplified by combining the terms inside the product into a single exponential:\n$$\n\\prod_{j=1}^{d} \\exp((k_j-1)\\phi_j - r_j \\exp(\\phi_j)) = \\exp\\left(\\sum_{j=1}^{d} ((k_j-1)\\phi_j - r_j \\exp(\\phi_j))\\right)\n$$\nNow, we assemble the unnormalized posterior for $\\phi$ by multiplying the transformed likelihood, the transformed prior, and the Jacobian determinant:\n$$\np(\\phi \\mid y) \\propto \\left[ \\exp\\left(-\\frac{1}{2} (y - A \\exp(\\phi))^{\\top} \\Sigma^{-1} (y - A \\exp(\\phi))\\right) \\right] \\cdot \\left[ \\exp\\left(\\sum_{j=1}^{d} [(k_j-1)\\phi_j - r_j \\exp(\\phi_j)]\\right) \\right] \\cdot \\left[ \\exp\\left(\\sum_{j=1}^{d} \\phi_j\\right) \\right]\n$$\nSince all terms are exponentials, we can combine them by summing their arguments:\n$$\np(\\phi \\mid y) \\propto \\exp\\left( -\\frac{1}{2} (y - A \\exp(\\phi))^{\\top} \\Sigma^{-1} (y - A \\exp(\\phi)) + \\sum_{j=1}^{d} [(k_j-1)\\phi_j - r_j \\exp(\\phi_j)] + \\sum_{j=1}^{d} \\phi_j \\right)\n$$\nWe simplify the sum in the exponent. The terms involving $\\phi_j$ are:\n$$\n\\sum_{j=1}^{d} (k_j-1)\\phi_j + \\sum_{j=1}^{d} \\phi_j = \\sum_{j=1}^{d} (k_j-1+1)\\phi_j = \\sum_{j=1}^{d} k_j\\phi_j\n$$\nSubstituting this back into the exponent gives:\n$$\n-\\frac{1}{2} (y - A \\exp(\\phi))^{\\top} \\Sigma^{-1} (y - A \\exp(\\phi)) + \\sum_{j=1}^{d} k_j \\phi_j - \\sum_{j=1}^{d} r_j \\exp(\\phi_j)\n$$\nWe can combine the last two sums for a more compact form. The final unnormalized posterior density kernel for $\\phi$ is therefore:\n$$\np(\\phi \\mid y) \\propto \\exp\\left( -\\frac{1}{2} (y - A \\exp(\\phi))^{\\top} \\Sigma^{-1} (y - A \\exp(\\phi)) + \\sum_{j=1}^{d} (k_j \\phi_j - r_j \\exp(\\phi_j)) \\right)\n$$\nThis expression is the required closed-form analytic result, where $\\exp(\\phi)$ is understood as the vector formed by applying the exponential function element-wise to the components of the vector $\\phi$.",
            "answer": "$$\n\\boxed{\\exp\\left( -\\frac{1}{2} (y - A \\exp(\\phi))^{\\top} \\Sigma^{-1} (y - A \\exp(\\phi)) + \\sum_{j=1}^{d} (k_j \\phi_j - r_j \\exp(\\phi_j)) \\right)}\n$$"
        },
        {
            "introduction": "A key outcome of Bayesian calibration is not just a single best-fit parameter set, but a full characterization of parameter uncertainty. A primary goal is then to understand how this parameter uncertainty propagates to predictions or other derived quantities of interest. This exercise demonstrates the delta method, a versatile and widely used technique for this purpose, showing how a local Gaussian approximation of the posterior can be used to estimate the variance of a complex model output, such as the energy confinement time in a tokamak.",
            "id": "3954057",
            "problem": "In a Bayesian calibration of a zero-dimensional core confinement model for a tokamak, let the parameter vector be $\\theta = (\\chi_{i}, \\chi_{e}, S)^{\\top}$, where $\\chi_{i}$ and $\\chi_{e}$ are the ion and electron effective thermal diffusivities (in $\\mathrm{m}^{2}\\,\\mathrm{s}^{-1}$), and $S$ is the steady-state volumetric auxiliary heating rate (in $\\mathrm{MW}\\,\\mathrm{m}^{-3}$). Suppose diagnostic data have been assimilated to obtain a Maximum A Posteriori (MAP) estimate $\\hat{\\theta}$ and a local Gaussian approximation to the posterior $p(\\theta \\mid \\text{data})$ with covariance matrix $\\Sigma$.\n\nConsider the derived observable defined as the logarithm of the predicted energy confinement time,\n$$\ng(\\theta) \\equiv \\ln \\tau_{E}(\\theta), \\quad \\text{where} \\quad \\tau_{E}(\\theta) = c \\, (\\chi_{i}\\,\\chi_{e})^{-1/2}\\, S^{1/3},\n$$\nwith $c>0$ a known constant determined by device geometry and global power balance. Assume $g(\\theta)$ is sufficiently smooth and that the posterior near $\\hat{\\theta}$ is well-approximated by a multivariate normal distribution with mean $\\hat{\\theta}$ and covariance $\\Sigma$. The MAP estimate and covariance are\n$$\n\\hat{\\theta} = \\begin{pmatrix} 1.1 \\\\ 0.7 \\\\ 4.5 \\end{pmatrix}, \\quad\n\\Sigma = \\begin{pmatrix}\n0.04 & 0.015 & -0.012 \\\\\n0.015 & 0.0225 & -0.009 \\\\\n-0.012 & -0.009 & 0.09\n\\end{pmatrix},\n$$\nwhere the entries of $\\Sigma$ carry the appropriate units consistent with $\\theta$.\n\nStarting from the basic form of Bayes’ theorem $p(\\theta \\mid \\text{data}) \\propto p(\\text{data} \\mid \\theta)\\,p(\\theta)$ and using the local Gaussian (Laplace) approximation of the posterior at the MAP together with a first-order multivariate Taylor expansion of $g(\\theta)$ around $\\hat{\\theta}$, derive the first-order approximation to the variance of $g(\\theta)$ in terms of $\\nabla g(\\hat{\\theta})$ and $\\Sigma$. Then, evaluate this approximation numerically at the provided $\\hat{\\theta}$ and $\\Sigma$.\n\nRound your final numeric answer to four significant figures. Because $g(\\theta)$ is a logarithm, the variance is dimensionless; express your final answer without units.",
            "solution": "The objective is to derive a first-order approximation for the variance of a derived quantity, $g(\\theta)$, and then evaluate it numerically. The derivation begins from the approximation of the posterior probability distribution $p(\\theta \\mid \\text{data})$ as a multivariate normal distribution, which is the result of the Laplace approximation applied to the posterior defined by Bayes' theorem. The problem provides the mean $\\hat{\\theta}$ and covariance $\\Sigma$ of this distribution.\n\nThe core of the derivation relies on the propagation of uncertainty for a function of random variables. We are given the function $g(\\theta)$ and are told to use a first-order multivariate Taylor expansion of $g(\\theta)$ around the mean of the distribution, which is the Maximum A Posteriori (MAP) estimate $\\hat{\\theta}$.\n\nThe first-order Taylor expansion of $g(\\theta)$ around $\\hat{\\theta}$ is given by:\n$$\ng(\\theta) \\approx g(\\hat{\\theta}) + \\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta})\n$$\nwhere $\\nabla g(\\hat{\\theta})$ is the gradient of the function $g$ evaluated at the point $\\hat{\\theta}$.\n\nThe variance of a scalar function $f(X)$ of a random vector $X$ is defined as $\\text{Var}(f(X)) = E[(f(X) - E[f(X)])^2]$. We apply this definition to our approximation of $g(\\theta)$.\n\nFirst, let's find the expected value of $g(\\theta)$. Under the Gaussian approximation for the posterior, the random vector $\\theta$ follows a distribution with mean $E[\\theta] = \\hat{\\theta}$.\n$$\nE[g(\\theta)] \\approx E\\left[ g(\\hat{\\theta}) + \\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta}) \\right]\n$$\nBy the linearity of expectation, we have:\n$$\nE[g(\\theta)] \\approx g(\\hat{\\theta}) + \\nabla g(\\hat{\\theta})^{\\top} E[\\theta - \\hat{\\theta}]\n$$\nSince $E[\\theta] = \\hat{\\theta}$, the term $E[\\theta - \\hat{\\theta}] = E[\\theta] - \\hat{\\theta} = \\hat{\\theta} - \\hat{\\theta} = 0$. Therefore, to first order, the expected value of $g(\\theta)$ is simply its value at the mean:\n$$\nE[g(\\theta)] \\approx g(\\hat{\\theta})\n$$\nNow, we can compute the variance:\n$$\n\\text{Var}(g(\\theta)) = E\\left[ (g(\\theta) - E[g(\\theta)])^2 \\right] \\approx E\\left[ \\left( g(\\hat{\\theta}) + \\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta}) - g(\\hat{\\theta}) \\right)^2 \\right]\n$$\n$$\n\\text{Var}(g(\\theta)) \\approx E\\left[ \\left( \\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta}) \\right)^2 \\right]\n$$\nThe term $\\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta})$ is a scalar. The square of a scalar $z$ can be written as $z z^{\\top}$. Thus, we can write the expression inside the expectation as:\n$$\n\\left( \\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta}) \\right)^2 = \\left( (\\theta - \\hat{\\theta})^{\\top} \\nabla g(\\hat{\\theta}) \\right) \\left( \\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta}) \\right)\n$$\nRe-arranging the terms inside the expectation:\n$$\n\\text{Var}(g(\\theta)) \\approx E\\left[ \\nabla g(\\hat{\\theta})^{\\top} (\\theta - \\hat{\\theta}) (\\theta - \\hat{\\theta})^{\\top} \\nabla g(\\hat{\\theta}) \\right]\n$$\nSince $\\nabla g(\\hat{\\theta})$ is a constant vector (not a random variable), we can move it outside the expectation operator:\n$$\n\\text{Var}(g(\\theta)) \\approx \\nabla g(\\hat{\\theta})^{\\top} E\\left[ (\\theta - \\hat{\\theta}) (\\theta - \\hat{\\theta})^{\\top} \\right] \\nabla g(\\hat{\\theta})\n$$\nThe term $E\\left[ (\\theta - \\hat{\\theta}) (\\theta - \\hat{\\theta})^{\\top} \\right]$ is the definition of the covariance matrix of the random vector $\\theta$, which is given as $\\Sigma$.\nSubstituting this in, we arrive at the first-order approximation for the variance of $g(\\theta)$:\n$$\n\\text{Var}(g(\\theta)) \\approx \\nabla g(\\hat{\\theta})^{\\top} \\Sigma \\nabla g(\\hat{\\theta})\n$$\nThis is the required formula for the variance of the derived quantity $g(\\theta)$.\n\nNext, we evaluate this expression numerically. The parameter vector is $\\theta = (\\chi_{i}, \\chi_{e}, S)^{\\top}$. The function $g(\\theta)$ is:\n$$\ng(\\theta) = \\ln \\left( c \\, (\\chi_{i}\\,\\chi_{e})^{-1/2}\\, S^{1/3} \\right) = \\ln(c) - \\frac{1}{2}\\ln(\\chi_i \\chi_e) + \\frac{1}{3}\\ln(S)\n$$\n$$\ng(\\theta) = \\ln(c) - \\frac{1}{2}\\ln(\\chi_i) - \\frac{1}{2}\\ln(\\chi_e) + \\frac{1}{3}\\ln(S)\n$$\nWe compute the gradient of $g(\\theta)$ with respect to the components of $\\theta$:\n$$\n\\frac{\\partial g}{\\partial \\chi_{i}} = -\\frac{1}{2\\chi_{i}}, \\quad \\frac{\\partial g}{\\partial \\chi_{e}} = -\\frac{1}{2\\chi_{e}}, \\quad \\frac{\\partial g}{\\partial S} = \\frac{1}{3S}\n$$\nThe gradient vector is:\n$$\n\\nabla g(\\theta) = \\begin{pmatrix} -\\frac{1}{2\\chi_{i}} \\\\ -\\frac{1}{2\\chi_{e}} \\\\ \\frac{1}{3S} \\end{pmatrix}\n$$\nWe evaluate this gradient at the MAP estimate $\\hat{\\theta} = \\begin{pmatrix} 1.1 \\\\ 0.7 \\\\ 4.5 \\end{pmatrix}$:\n$$\n\\nabla g(\\hat{\\theta}) = \\begin{pmatrix} -\\frac{1}{2(1.1)} \\\\ -\\frac{1}{2(0.7)} \\\\ \\frac{1}{3(4.5)} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2.2} \\\\ -\\frac{1}{1.4} \\\\ \\frac{1}{13.5} \\end{pmatrix}\n$$\nLet's denote this gradient vector as $J = \\nabla g(\\hat{\\theta})$. The variance is computed as $J^{\\top} \\Sigma J$.\nThe given covariance matrix is:\n$$\n\\Sigma = \\begin{pmatrix}\n0.04 & 0.015 & -0.012 \\\\\n0.015 & 0.0225 & -0.009 \\\\\n-0.012 & -0.009 & 0.09\n\\end{pmatrix}\n$$\nThe variance calculation is:\n$$\n\\text{Var}(g(\\theta)) \\approx\n\\begin{pmatrix} -\\frac{1}{2.2} & -\\frac{1}{1.4} & \\frac{1}{13.5} \\end{pmatrix}\n\\begin{pmatrix}\n0.04 & 0.015 & -0.012 \\\\\n0.015 & 0.0225 & -0.009 \\\\\n-0.012 & -0.009 & 0.09\n\\end{pmatrix}\n\\begin{pmatrix} -\\frac{1}{2.2} \\\\ -\\frac{1}{1.4} \\\\ \\frac{1}{13.5} \\end{pmatrix}\n$$\nLet $J_1 = -\\frac{1}{2.2}$, $J_2 = -\\frac{1}{1.4}$, and $J_3 = \\frac{1}{13.5}$.\nThe variance is $\\text{Var}(g(\\theta)) \\approx \\sum_{i,j=1}^{3} J_i \\Sigma_{ij} J_j$. We expand this sum:\n$$\n\\text{Var}(g(\\theta)) \\approx J_1^2 \\Sigma_{11} + J_2^2 \\Sigma_{22} + J_3^2 \\Sigma_{33} + 2J_1 J_2 \\Sigma_{12} + 2J_1 J_3 \\Sigma_{13} + 2J_2 J_3 \\Sigma_{23}\n$$\nSubstituting the numerical values:\n$$\n\\text{Var}(g(\\theta)) \\approx \\left(-\\frac{1}{2.2}\\right)^2(0.04) + \\left(-\\frac{1}{1.4}\\right)^2(0.0225) + \\left(\\frac{1}{13.5}\\right)^2(0.09) + 2\\left(-\\frac{1}{2.2}\\right)\\left(-\\frac{1}{1.4}\\right)(0.015) + 2\\left(-\\frac{1}{2.2}\\right)\\left(\\frac{1}{13.5}\\right)(-0.012) + 2\\left(-\\frac{1}{1.4}\\right)\\left(\\frac{1}{13.5}\\right)(-0.009)\n$$\nCalculating each term:\n\\begin{itemize}\n    \\item $J_1^2 \\Sigma_{11} \\approx (0.20661157)(0.04) = 0.00826446$\n    \\item $J_2^2 \\Sigma_{22} \\approx (0.51020408)(0.0225) = 0.01147959$\n    \\item $J_3^2 \\Sigma_{33} \\approx (0.00548697)(0.09) = 0.00049383$\n    \\item $2J_1 J_2 \\Sigma_{12} \\approx 2(0.32467532)(0.015) = 0.00974026$\n    \\item $2J_1 J_3 \\Sigma_{13} \\approx 2(-0.03367003)(-0.012) = 0.00080808$\n    \\item $2J_2 J_3 \\Sigma_{23} \\approx 2(-0.05291005)(-0.009) = 0.00095238$\n\\end{itemize}\nSumming these values:\n$$\n\\text{Var}(g(\\theta)) \\approx 0.00826446 + 0.01147959 + 0.00049383 + 0.00974026 + 0.00080808 + 0.00095238\n$$\n$$\n\\text{Var}(g(\\theta)) \\approx 0.0317386\n$$\nThe problem asks to round the final answer to four significant figures.\n$$\n\\text{Var}(g(\\theta)) \\approx 0.03174\n$$\nSince $g(\\theta)$ is the logarithm of a physical quantity, it is dimensionless. Its variance is therefore also dimensionless.",
            "answer": "$$\\boxed{0.03174}$$"
        }
    ]
}