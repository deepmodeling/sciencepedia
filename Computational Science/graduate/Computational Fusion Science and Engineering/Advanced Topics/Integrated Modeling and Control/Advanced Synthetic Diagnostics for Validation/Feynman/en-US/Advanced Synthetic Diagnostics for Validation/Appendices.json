{
    "hands_on_practices": [
        {
            "introduction": "The first step in validating a complex plasma model is to test its most fundamental predictions against reliable measurements. This exercise provides hands-on practice in creating a basic synthetic diagnostic by translating a core physics model—the Grad–Shafranov equilibrium—into predicted signals for magnetic flux loops. By calculating the Root Mean Square Error (RMSE) between your synthetic data and hypothetical measurements, you will perform a foundational validation check, a critical skill for assessing the consistency of any equilibrium reconstruction .",
            "id": "3948442",
            "problem": "Consider an axisymmetric tokamak equilibrium described by the Grad–Shafranov equation, where the poloidal magnetic flux function $\\psi(R,Z)$ encodes the equilibrium magnetic field in cylindrical coordinates $(R,\\phi,Z)$, with $R$ the major radius and $Z$ the vertical coordinate. Under axisymmetry, define $\\psi(R,Z)$ such that the poloidal magnetic field $\\mathbf{B}_{\\mathrm{pol}}$ satisfies $\\mathbf{B}_{\\mathrm{pol}} = \\frac{1}{R} \\nabla \\psi(R,Z) \\times \\hat{\\boldsymbol{\\phi}}$, and the poloidal magnetic flux linking a toroidal loop at $(R,Z)$ equals $2\\pi \\psi(R,Z)$. A Magnetic Flux Loop (MFL) is a toroidally oriented conductor that measures the poloidal magnetic flux linking it; in equilibrium (steady-state), the static signal is proportional to $\\psi(R,Z)$, up to a constant that can be fixed by calibration. We will treat all signals as normalized poloidal flux per radian, i.e., $\\psi$ has units of Weber per radian (Wb/rad).\n\nTo compute synthetic diagnostic signals for validation, assume a near-axis truncated second-order model for the poloidal flux,\n$$\n\\psi(R,Z) = \\psi_0 + \\frac{a}{2}\\left(R^2 - R_0^2\\right) + \\frac{b}{2} Z^2,\n$$\nwhere $\\psi_0$, $R_0$, $a$, and $b$ are equilibrium parameters. This model is consistent with a local Taylor expansion of axisymmetric equilibria that satisfy the Grad–Shafranov balance for smoothly varying pressure and toroidal field functions near the magnetic axis.\n\nYour task: For each test case, compute synthetic Magnetic Flux Loop signals by evaluating $\\psi(R_i,Z_i)$ at the loop coordinates $(R_i,Z_i)$, compare them to measured values $m_i$, and report the Root Mean Square Error (RMSE) defined by\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} \\left[\\psi(R_i,Z_i) - m_i\\right]^2},\n$$\nwhere $N$ is the number of loops in the test case. All quantities are scalars, and RMSE must be expressed in Weber per radian (Wb/rad). Use the International System of Units (SI), with $R$ and $Z$ in meters (m) and $\\psi$ and $m_i$ in Weber per radian (Wb/rad). No angles appear, so no angle unit is required.\n\nTest suite specification:\n- Test Case $1$ (general consistency, five loops):\n  - Equilibrium parameters: $\\psi_0 = 0.0$ (Wb/rad), $R_0 = 1.7$ (m), $a = 0.45$ (Wb/rad/m$^2$), $b = 0.6$ (Wb/rad/m$^2$).\n  - Loop coordinates $(R_i,Z_i)$ in meters: $\\left[(1.7,0.0),(1.85,0.2),(1.5,-0.3),(1.95,-0.1),(1.6,0.35)\\right]$.\n  - Measured values $m_i$ in Wb/rad: $\\left[-0.0008, 0.1330125, -0.1175, 0.2092125, -0.0385\\right]$.\n- Test Case $2$ (includes magnetic axis loop, four loops):\n  - Equilibrium parameters: $\\psi_0 = 0.02$ (Wb/rad), $R_0 = 1.8$ (m), $a = 0.40$ (Wb/rad/m$^2$), $b = 0.55$ (Wb/rad/m$^2$).\n  - Loop coordinates $(R_i,Z_i)$ in meters: $\\left[(1.8,0.0),(1.85,0.0),(1.75,0.1),(1.9,-0.2)\\right]$.\n  - Measured values $m_i$ in Wb/rad: $\\left[0.02, 0.0563, -0.01265, 0.1047\\right]$.\n- Test Case $3$ (systematic inconsistency, five loops):\n  - Equilibrium parameters: $\\psi_0 = -0.01$ (Wb/rad), $R_0 = 1.6$ (m), $a = 0.50$ (Wb/rad/m$^2$), $b = 0.65$ (Wb/rad/m$^2$).\n  - Loop coordinates $(R_i,Z_i)$ in meters: $\\left[(1.6,0.0),(1.7,0.2),(1.5,-0.25),(1.8,0.35),(1.55,-0.4)\\right]$.\n  - Measured values $m_i$ in Wb/rad: $\\left[0.0055, 0.0998, -0.0519875, 0.2137125, 0.017625\\right]$.\n\nYour program should produce a single line of output containing the RMSE values for the three test cases as a comma-separated list enclosed in square brackets (e.g., $\\left[\\mathrm{RMSE}_1,\\mathrm{RMSE}_2,\\mathrm{RMSE}_3\\right]$). The outputs must be numeric floats in Weber per radian (Wb/rad).",
            "solution": "The problem presented is a well-defined computational task in the domain of computational fusion science, specifically concerning the validation of an equilibrium model against synthetic diagnostic data. A thorough analysis of the problem statement confirms its validity.\n\n1.  **Scientific Grounding**: The problem is based on fundamental principles of magnetohydrodynamics (MHD) in toroidal confinement systems (tokamaks). The Grad-Shafranov equation, the concept of poloidal magnetic flux $\\psi(R,Z)$, and the use of a local second-order Taylor expansion near the magnetic axis are all standard and scientifically sound concepts in plasma physics. The use of Magnetic Flux Loops as a diagnostic and the Root Mean Square Error (RMSE) as a validation metric are standard practices in experimental and computational science. All units are consistent within the International System of Units (SI).\n\n2.  **Well-Posedness**: The problem is mathematically unambiguous. It provides a closed-form analytical model for the poloidal flux, a clear definition of the error metric to be computed, and a complete set of input data for three distinct test cases. For each case, the parameters of the model, the coordinates of the measurement points, and the corresponding measured values are all explicitly given. This ensures that a unique and stable numerical solution exists for each case.\n\n3.  **Objectivity**: The problem is stated in precise, objective language, free of ambiguity or subjective claims. The task is a direct application of mathematical formulas to numerical data.\n\nThe problem is therefore deemed valid. We proceed with the solution.\n\nThe core of the problem is to quantify the discrepancy between a model prediction and a set of measurements. The model for the poloidal magnetic flux $\\psi$ at a given cylindrical coordinate point $(R, Z)$ is given by a truncated second-order expansion:\n$$\n\\psi(R,Z) = \\psi_0 + \\frac{a}{2}\\left(R^2 - R_0^2\\right) + \\frac{b}{2} Z^2\n$$\nHere, $\\psi_0$ is the flux at the magnetic axis, which is located at $(R_0, 0)$. The parameters $a$ and $b$ are related to the curvature of the flux surfaces and are determined by the plasma pressure and current profiles near the axis.\n\nThe objective is to compute the Root Mean Square Error (RMSE) between the synthetic flux values calculated from this model, $\\psi(R_i,Z_i)$, and a set of $N$ measured values, $m_i$, at corresponding locations $(R_i,Z_i)$. The RMSE is defined as:\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} \\left[\\psi(R_i,Z_i) - m_i\\right]^2}\n$$\nThis metric provides a measure of the average magnitude of the error between the model and the data, in the same units as the measured quantity (Weber per radian).\n\nThe computational procedure is as follows:\n1.  For each test case, the set of equilibrium parameters $(\\psi_0, R_0, a, b)$, the set of $N$ loop coordinates $\\{(R_i,Z_i)\\}_{i=1}^N$, and the set of $N$ measured values $\\{m_i\\}_{i=1}^N$ are provided.\n2.  For each loop $i$ from $1$ to $N$, the synthetic flux $\\psi(R_i, Z_i)$ is calculated by substituting the loop's coordinates $(R_i, Z_i)$ and the equilibrium parameters into the model equation.\n3.  The squared error for each loop, $(\\psi(R_i,Z_i) - m_i)^2$, is computed.\n4.  The Mean Squared Error (MSE) is calculated by averaging these squared errors: $\\mathrm{MSE} = \\frac{1}{N}\\sum_{i=1}^{N} (\\psi(R_i,Z_i) - m_i)^2$.\n5.  The RMSE is obtained by taking the square root of the MSE: $\\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}}$.\n\nThis procedure is applied independently to each of the three test cases specified in the problem statement. The resulting three RMSE values are then reported. The implementation will utilize vector operations for efficiency, where the synthetic flux values for all loops in a given test case are computed simultaneously.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Root Mean Square Error (RMSE) between synthetic magnetic flux loop\n    signals and measured values for three test cases in a tokamak.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple containing:\n    # 1. A tuple of equilibrium parameters: (psi_0, R_0, a, b)\n    # 2. A list of loop coordinates: [(R_i, Z_i), ...]\n    # 3. A list of measured values: [m_i, ...]\n    test_cases = [\n        # Test Case 1: general consistency, five loops\n        (\n            (0.0, 1.7, 0.45, 0.6),  # (psi0, R0, a, b)\n            [(1.7, 0.0), (1.85, 0.2), (1.5, -0.3), (1.95, -0.1), (1.6, 0.35)],  # (R_i, Z_i)\n            [-0.0008, 0.1330125, -0.1175, 0.2092125, -0.0385]  # m_i\n        ),\n        # Test Case 2: includes magnetic axis loop, four loops\n        (\n            (0.02, 1.8, 0.40, 0.55),  # (psi0, R0, a, b)\n            [(1.8, 0.0), (1.85, 0.0), (1.75, 0.1), (1.9, -0.2)],  # (R_i, Z_i)\n            [0.02, 0.0563, -0.01265, 0.1047]  # m_i\n        ),\n        # Test Case 3: systematic inconsistency, five loops\n        (\n            (-0.01, 1.6, 0.50, 0.65),  # (psi0, R0, a, b)\n            [(1.6, 0.0), (1.7, 0.2), (1.5, -0.25), (1.8, 0.35), (1.55, -0.4)],  # (R_i, Z_i)\n            [0.0055, 0.0998, -0.0519875, 0.2137125, 0.017625]  # m_i\n        )\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        params, coords, measurements = case\n        psi0, R0, a, b = params\n        \n        # Convert lists to NumPy arrays for efficient, vectorized calculations.\n        coords_arr = np.array(coords)\n        measurements_arr = np.array(measurements)\n        \n        # Extract R and Z coordinates into separate vectors.\n        R = coords_arr[:, 0]\n        Z = coords_arr[:, 1]\n        \n        # Calculate synthetic flux values for all points simultaneously.\n        # psi(R,Z) = psi0 + (a/2)*(R^2 - R0^2) + (b/2)*Z^2\n        psi_synthetic = psi0 + (a / 2.0) * (R**2 - R0**2) + (b / 2.0) * Z**2\n        \n        # Calculate the Root Mean Square Error (RMSE).\n        # RMSE = sqrt(mean((psi_synthetic - measurements)^2))\n        rmse = np.sqrt(np.mean((psi_synthetic - measurements_arr)**2))\n        \n        results.append(rmse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful diagnostic is not just about measuring a signal; it's about designing a system that can unambiguously resolve the physical quantity of interest. This practice delves into the crucial concept of identifiability by exploring how the geometric arrangement of chordal measurements affects our ability to reconstruct an emissivity profile. You will construct the forward operator matrix $\\mathbf{H}$ and analyze its rank and condition number, revealing how the measurement setup itself dictates the stability and feasibility of the inverse problem, long before any data is even analyzed .",
            "id": "3948452",
            "problem": "You are tasked with building a synthetic diagnostic forward operator for axisymmetric chord-integrated measurements of radiative emissivity in a toroidal magnetic confinement device. The aim is to examine how discretization of a continuous forward operator $H$ into a matrix $\\mathbf{H}$ affects identifiability by computing the matrix rank and the $2$-norm condition number under various chordal measurement setups. Your program must implement the geometry and compute the requested properties for a specified test suite, and then output the results in a single line.\n\nFundamental base and physical model:\n- Consider a circular poloidal cross-section of radius $R$ (in $\\mathrm{m}$) and an axisymmetric, nonnegative emissivity field $\\epsilon(r)$ that depends only on the radial coordinate $r$, with $0 \\le r \\le R$.\n- Measurements are obtained by straight lines-of-sight (chords) intersecting the cross-section. Each chord is defined by its impact parameter $b$ (in $\\mathrm{m}$), which is the minimum distance between the chord and the center of the cross-section. Assume the chords are sufficiently long to fully traverse the cross-section.\n- Under the axisymmetry assumption, each chord-integrated signal is a line integral of $\\epsilon(r)$ along the chord path through the plasma. If the emissivity is piecewise constant over concentric annuli, the measurement becomes a linear combination of annulus contributions proportional to the path lengths intersected by the chord within each annulus.\n\nDiscretization:\n- Discretize $[0,R]$ into $N$ concentric annuli (zones) with uniform boundaries $r_0, r_1, \\dots, r_N$, where $r_0 = 0$ and $r_n = R \\cdot n / N$ for $n = 1,2,\\dots,N$.\n- Let $f_n$ denote the constant emissivity (in arbitrary units) in the $n$-th annulus $(r_{n-1}, r_n]$. For a chord with impact parameter $b$, its measurement $s(b)$ is discretized as $s(b) = \\sum_{n=1}^{N} H_{n}(b) f_n$, where $H_n(b)$ is the path length (in $\\mathrm{m}$) of the chord within annulus $n$.\n- For $M$ chords with impact parameters $b_m$ for $m=1,\\dots,M$, this yields a linear system $\\mathbf{s} = \\mathbf{H} \\mathbf{f}$, where $\\mathbf{H} \\in \\mathbb{R}^{M \\times N}$.\n\nGeometric determination of $\\mathbf{H}$:\n- For each chord with impact parameter $b$, the intersection length with a circle of radius $a$ is $2\\sqrt{a^2 - b^2}$ if $b < a$ and $0$ otherwise. The path length within the annulus $(r_{n-1}, r_n]$ is the difference between the intersection lengths with $r_n$ and $r_{n-1}$. If an instrument aperture limits the observable radius to $R_{\\mathrm{ap}} \\le R$, then the effective outer boundary is $\\min(r_n, R_{\\mathrm{ap}})$ and the effective inner boundary is $\\min(r_{n-1}, R_{\\mathrm{ap}})$.\n\nIdentifiability metrics:\n- The matrix rank of $\\mathbf{H}$ indicates the dimension of the subspace of emissivity profiles identifiable from the chordal measurements. If $\\mathrm{rank}(\\mathbf{H}) = N$, then $\\mathbf{f}$ is identifiable in the linear sense; if $\\mathrm{rank}(\\mathbf{H}) < N$, then some components are unobservable.\n- The $2$-norm condition number $\\kappa_2(\\mathbf{H})$ equals the ratio of the largest singular value to the smallest singular value. A large $\\kappa_2(\\mathbf{H})$ implies ill-conditioning and high sensitivity of inferred $\\mathbf{f}$ to measurement noise. If the smallest singular value is $0$, then $\\kappa_2(\\mathbf{H})$ is infinite.\n\nTasks:\n1. Construct $\\mathbf{H}$ using the above geometric rules for each test case.\n2. Compute $\\mathrm{rank}(\\mathbf{H})$ and $\\kappa_2(\\mathbf{H})$ (use the matrix $2$-norm condition number).\n3. Output the results in the specified format.\n\nTest suite:\n- All test cases use $R = 1.0$ $\\mathrm{m}$ and uniform annuli.\n- Case $1$ (baseline, square system): $N = 8$, $M = 8$, impact parameters $b_m = R \\cdot [0.000, 0.125, 0.250, 0.375, 0.500, 0.625, 0.750, 0.875]$, no aperture limit.\n- Case $2$ (overdetermined): $N = 8$, $M = 16$, $b_m$ linearly spaced in $[0, 0.95 R]$, no aperture limit.\n- Case $3$ (underdetermined): $N = 8$, $M = 5$, impact parameters $b_m = R \\cdot [0.000, 0.200, 0.400, 0.600, 0.800]$, no aperture limit.\n- Case $4$ (clustered chords, near-degenerate sampling): $N = 8$, $M = 8$, impact parameters $b_m = R \\cdot [0.680, 0.690, 0.700, 0.710, 0.720, 0.730, 0.740, 0.750]$, no aperture limit.\n- Case $5$ (limited aperture): $N = 8$, $M = 8$, same $b_m$ as Case $1$, with $R_{\\mathrm{ap}} = 0.700 R$.\n\nOutput specification:\n- For each case, compute the integer rank and the float $2$-norm condition number rounded to $6$ decimal places. Collect all results in order as a single list with alternating rank and condition number values, i.e., $[\\mathrm{rank}_1, \\kappa_2(\\mathbf{H}_1), \\mathrm{rank}_2, \\kappa_2(\\mathbf{H}_2), \\dots]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,c_1,r_2,c_2,\\dots]$). No additional text should be printed.\n\nAngle unit:\n- No angles are used; chords are characterized solely by their impact parameters.\n\nUnits:\n- Geometry is defined in $\\mathrm{m}$, but the output quantities $\\mathrm{rank}(\\mathbf{H})$ and $\\kappa_2(\\mathbf{H})$ are dimensionless. No physical units are required for the output.\n\nConstraints:\n- Implement using standard linear algebra tools appropriate for graduate-level computation, without invoking any shortcut formulas beyond the fundamental base. Ensure numerical robustness when handling square roots and singular values.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded, well-posed, and provides a complete and consistent set of requirements for a computational task in the field of fusion plasma diagnostics. The problem asks for the construction and analysis of a forward operator, which is a standard procedure in synthetic diagnostics for comparing computational models with experimental measurements.\n\nThe core of the problem is to design a synthetic diagnostic that models chord-integrated emissivity measurements in a toroidal plasma with an assumed axisymmetric cross-section. This is mathematically equivalent to the Abel transform problem. We are tasked with discretizing this continuous operator and evaluating the properties of the resulting matrix representation, $\\mathbf{H}$, for several measurement configurations. The key metrics for evaluation are the matrix rank, $\\mathrm{rank}(\\mathbf{H})$, and the $2$-norm condition number, $\\kappa_2(\\mathbf{H})$.\n\nFirst, we formalize the model. The continuous emissivity profile, $\\epsilon(r)$, is assumed to depend only on the minor radius $r$, where $0 \\le r \\le R$. We discretize this profile into $N$ concentric annuli of uniform width. The annuli are defined by radii $r_n = R \\cdot n / N$ for $n=0, 1, \\dots, N$. Within each annulus $(r_{n-1}, r_n]$, the emissivity is assumed to be a constant value, $f_n$. This constitutes a piecewise constant approximation of the true profile $\\epsilon(r)$.\n\nA measurement is taken along a straight line-of-sight, or chord, defined by its impact parameter $b$, which is its minimum distance to the center of the circular cross-section. The measured signal $s(b)$ is the line integral of the emissivity along this chord. In our discretized model, this integral becomes a sum:\n$$\ns(b) = \\sum_{n=1}^{N} H_{n}(b) f_n\n$$\nwhere $H_{n}(b)$ is the path length of the chord with impact parameter $b$ through the $n$-th annulus. For a set of $M$ measurements with impact parameters $b_1, \\dots, b_M$, we obtain a linear system of equations:\n$$\n\\mathbf{s} = \\mathbf{H} \\mathbf{f}\n$$\nHere, $\\mathbf{s} \\in \\mathbb{R}^M$ is the vector of measurements, $\\mathbf{f} \\in \\mathbb{R}^N$ is the vector of unknown annulus emissivities, and $\\mathbf{H} \\in \\mathbb{R}^{M \\times N}$ is the forward operator matrix we must construct. The element $H_{mn}$ of this matrix is the path length for the $m$-th chord ($b_m$) through the $n$-th annulus.\n\nThe geometric calculation of the matrix elements $H_{mn}$ is central. The total path length of a chord with impact parameter $b$ through a circle of radius $a$ (where $b < a$) is given by elementary geometry as $L(a, b) = 2\\sqrt{a^2 - b^2}$. If $b \\ge a$, the chord does not intersect the circle, and the path length is $0$. The path length through an annulus $(r_{n-1}, r_n]$ is the difference between the path lengths through the circles defined by its outer and inner radii:\n$$\nH_{mn} = L(r_n, b_m) - L(r_{n-1}, b_m)\n$$\nThis formula holds provided we define $L(a, b) = 0$ for $b \\ge a$.\n\nThe problem also introduces an optional instrument aperture, $R_{\\mathrm{ap}}$, which limits the observable region to $r \\le R_{\\mathrm{ap}}$. To account for this, we must use effective radii. The path length through the $n$-th annulus is calculated using radii that are clipped by the aperture: $r'_n = \\min(r_n, R_{\\mathrm{ap}})$ and $r'_{n-1} = \\min(r_{n-1}, R_{\\mathrm{ap}})$. The formula for the matrix element becomes:\n$$\nH_{mn} = L(r'_n, b_m) - L(r'_{n-1}, b_m)\n$$\nDuring numerical implementation, care must be taken when $b_m$ is very close to an effective radius $r'$, as floating-point inaccuracies could make the term $(r')^2 - b_m^2$ slightly negative. We must ensure the argument of the square root is non-negative.\n\nOnce the matrix $\\mathbf{H}$ is constructed for a given test case, we analyze its properties.\n$1$. The rank of $\\mathbf{H}$, $\\mathrm{rank}(\\mathbf{H})$, is computed. The rank gives the number of linearly independent rows (or columns), which corresponds to the dimension of the data space spanned by the measurements. If $\\mathrm{rank}(\\mathbf{H}) = N$ (the number of unknowns), the emissivity profile $\\mathbf{f}$ is uniquely identifiable from the measurements $\\mathbf{s}$, assuming a square or overdetermined system. If $\\mathrm{rank}(\\mathbf{H}) < N$, the system is underdetermined or rank-deficient, and a unique solution for $\\mathbf{f}$ does not exist.\n$2$. The $2$-norm condition number, $\\kappa_2(\\mathbf{H})$, is computed. It is the ratio of the largest to the smallest singular value of $\\mathbf{H}$, $\\kappa_2(\\mathbf{H}) = \\sigma_{\\max} / \\sigma_{\\min}$. A large condition number indicates an ill-conditioned problem, where small errors or noise in the measurement vector $\\mathbf{s}$ can lead to large errors in the inferred solution $\\mathbf{f}$. If $\\mathbf{H}$ is singular or rank-deficient (i.e., $\\sigma_{\\min} = 0$), the condition number is infinite.\n\nThe implementation will proceed by iterating through the five specified test cases. For each case, we define the parameters ($N, M, R, \\{b_m\\}, R_{\\mathrm{ap}}$), construct the matrix $\\mathbf{H}$ element by element using the geometric formula, and then use standard numerical linear algebra routines to compute its rank and condition number. The results will be collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes a forward operator matrix for synthetic chord-integrated\n    emissivity measurements in a toroidal plasma cross-section.\n    \"\"\"\n\n    def get_h_matrix(R, N, M, b_params, R_ap=None):\n        \"\"\"\n        Constructs the forward operator matrix H.\n\n        Args:\n            R (float): Radius of the circular cross-section.\n            N (int): Number of concentric annuli (emissivity zones).\n            M (int): Number of measurement chords.\n            b_params (np.ndarray): Array of impact parameters for the chords.\n            R_ap (float, optional): Aperture radius limit. Defaults to None.\n\n        Returns:\n            np.ndarray: The M x N forward operator matrix H.\n        \"\"\"\n        # Define annulus radii\n        r = R * np.arange(N + 1) / N\n\n        # Initialize the H matrix\n        H = np.zeros((M, N))\n\n        # Helper function for chord length through a circle of radius 'a'\n        def chord_length(a, b):\n            # Numerically robust calculation of 2 * sqrt(a^2 - b^2)\n            if b >= a:\n                return 0.0\n            return 2.0 * np.sqrt(a**2 - b**2)\n\n        # Populate the H matrix\n        for m in range(M):\n            b = b_params[m]\n            for n in range(N):\n                r_inner = r[n]\n                r_outer = r[n + 1]\n\n                # Apply aperture limit if specified\n                r_eff_inner = r_inner\n                r_eff_outer = r_outer\n                if R_ap is not None:\n                    r_eff_inner = min(r_inner, R_ap)\n                    r_eff_outer = min(r_outer, R_ap)\n\n                # Path length is the difference of lengths through the outer and inner circles\n                # of the (effective) annulus.\n                len_outer = chord_length(r_eff_outer, b)\n                len_inner = chord_length(r_eff_inner, b)\n                H[m, n] = len_outer - len_inner\n        \n        return H\n\n    # Common parameters\n    R_base = 1.0\n\n    # Test suite definition\n    test_cases = [\n        # Case 1: baseline, square system\n        {'N': 8, 'M': 8, 'b_params': R_base * np.array([0.000, 0.125, 0.250, 0.375, 0.500, 0.625, 0.750, 0.875]), 'R_ap': None},\n        # Case 2: overdetermined\n        {'N': 8, 'M': 16, 'b_params': np.linspace(0, 0.95 * R_base, 16), 'R_ap': None},\n        # Case 3: underdetermined\n        {'N': 8, 'M': 5, 'b_params': R_base * np.array([0.000, 0.200, 0.400, 0.600, 0.800]), 'R_ap': None},\n        # Case 4: clustered chords, near-degenerate sampling\n        {'N': 8, 'M': 8, 'b_params': R_base * np.array([0.680, 0.690, 0.700, 0.710, 0.720, 0.730, 0.740, 0.750]), 'R_ap': None},\n        # Case 5: limited aperture\n        {'N': 8, 'M': 8, 'b_params': R_base * np.array([0.000, 0.125, 0.250, 0.375, 0.500, 0.625, 0.750, 0.875]), 'R_ap': 0.700 * R_base},\n    ]\n\n    results = []\n    for case in test_cases:\n        H = get_h_matrix(R_base, case['N'], case['M'], case['b_params'], case['R_ap'])\n\n        # Compute rank and condition number\n        rank = np.linalg.matrix_rank(H)\n        cond_num = np.linalg.cond(H)\n\n        results.append(rank)\n        \n        # Format condition number. np.inf will be handled by str() conversion later\n        if np.isinf(cond_num):\n            results.append(cond_num)\n        else:\n            results.append(round(cond_num, 6))\n\n    # Format the final output string as specified\n    # map(str, ...) correctly handles integers, floats, and np.inf\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Ultimately, the goal of validation is often to decide between competing physical theories. This exercise moves beyond simple error metrics to the sophisticated and powerful framework of Bayesian model selection. You will learn to compute and interpret Bayes factors, which provide a rigorous, quantitative measure of how strongly the evidence supports one model over another. By working directly with log-evidences—the cornerstone of this technique—you will practice the statistical reasoning used at the forefront of scientific inference to make robust conclusions from complex data .",
            "id": "3948507",
            "problem": "You are provided with pairs of sets of log-evidences computed from independent synthetic diagnostics designed to validate competing forward models in computational fusion science and engineering. In this setting, each model predicts diagnostic observables, and the model evidence is defined by Bayes theorem. The goal is to compute evidence ratios and interpret the strength of support using log-evidence differences.\n\nFundamental base: Bayes theorem establishes that for a hypothesis $H$, the posterior is $p(H \\mid D) = \\dfrac{p(D \\mid H) p(H)}{p(D)}$, where $p(D \\mid H)$ is the likelihood and $p(H)$ is the prior. The evidence $Z(H) = p(D)$ for hypothesis $H$ is\n$$\nZ(H) = \\int p(D \\mid \\theta, H) \\, \\pi(\\theta \\mid H) \\, d\\theta,\n$$\nwhere $\\pi(\\theta \\mid H)$ is the prior density over parameters $\\theta$. For multiple diagnostics $D_1, D_2, \\ldots, D_N$ that are conditionally independent given $H$, the joint likelihood factorizes as $p(D_1, \\ldots, D_N \\mid H) = \\prod_{i=1}^{N} p(D_i \\mid H)$, and the joint evidence factorizes accordingly. Therefore, the total log-evidence is a sum of per-diagnostic log-evidences:\n$$\n\\ln Z_{\\text{total}}(H) = \\sum_{i=1}^{N} \\ln Z_i(H).\n$$\n\nThe Bayes factor (BF) comparing hypotheses $H_1$ and $H_2$ is $K_{12} = \\dfrac{Z(H_1)}{Z(H_2)}$. Using the logarithmic representation for numerical stability, define $\\Delta = \\ln Z(H_1) - \\ln Z(H_2)$, which implies $K_{12} = \\exp(\\Delta)$.\n\nInterpretation: Use the Jeffreys-style interpretation on the magnitude of $\\Delta$ in natural logarithms. Define a categorical support code $c$ based on $|\\Delta|$ as follows:\n- If $0 \\leq |\\Delta| < 1$, then $c = 0$ (barely worth a mention).\n- If $1 \\leq |\\Delta| < 2.5$, then $c = 1$ (weak evidence).\n- If $2.5 \\leq |\\Delta| < 5$, then $c = 2$ (moderate evidence).\n- If $|\\Delta| \\geq 5$, then $c = 3$ (strong evidence).\n\nDefine a favored model index $m$ based on the sign of $\\Delta$ with a tie tolerance $\\epsilon = 10^{-9}$: if $|\\Delta| < \\epsilon$, set $m = 0$ (tie); if $\\Delta > 0$, set $m = 1$ (favor $H_1$); if $\\Delta < 0$, set $m = 2$ (favor $H_2$).\n\nYour task is to implement a program that, for each test case, computes the total log-evidence for $H_1$ and $H_2$, then computes $\\Delta$, the Bayes factor $K_{12}$, the support code $c$, and the favored model index $m$.\n\nTest suite (each test case provides per-diagnostic log-evidences for $H_1$ and $H_2$):\n\n- Test case A: $H_1$: [$-12.3$, $5.1$, $-0.8$]; $H_2$: [$-13.0$, $4.0$, $-0.5$].\n- Test case B: $H_1$: [$1.0$]; $H_2$: [$0.0$].\n- Test case C: $H_1$: [$-15.0$, $-2.0$]; $H_2$: [$-10.0$, $-1.0$].\n- Test case D: $H_1$: [$100.000000000001$]; $H_2$: [$100.0$].\n- Test case E: $H_1$: [$-700.0$, $300.0$, $10.0$]; $H_2$: [$-705.0$, $300.0$, $12.5$].\n- Test case F: $H_1$: [$-100.0$]; $H_2$: [$-110.0$].\n- Test case G: $H_1$: [$0.0$, $0.0$]; $H_2$: [$0.0$, $0.0$].\n\nFinal output format: Your program should produce a single line of output containing a comma-separated list of results enclosed in square brackets (e.g., [$r_A$, $r_B$, $r_C$, $\\ldots$]), where each result $r$ for a test case is itself a list of the form [$K_{12}$, $\\Delta$, $c$, $m$]. The values $K_{12}$ and $\\Delta$ must be floats, and $c$ and $m$ must be integers. No units are required since all quantities are dimensionless.",
            "solution": "The problem is well-posed, scientifically grounded in the principles of Bayesian inference, and provides all necessary information for a unique solution. We may therefore proceed with the derivation and implementation.\n\nThe core task is to compare two competing physical models, denoted as hypotheses $H_1$ and $H_2$, based on data from a set of $N$ independent synthetic diagnostics. The comparison is performed using the Bayesian evidence, $Z(H)$, for each model.\n\n**Step 1: Calculation of Total Log-Evidence**\n\nThe problem states that for a set of $N$ conditionally independent diagnostics, the total log-evidence for a given hypothesis $H$ is the sum of the log-evidences from each individual diagnostic. Let the per-diagnostic log-evidences for hypothesis $H_k$ be given by the set $\\{\\ln Z_{i}(H_k)\\}_{i=1}^N$. The total log-evidence, $\\ln Z_{\\text{total}}(H_k)$, is thus:\n$$\n\\ln Z_{\\text{total}}(H_k) = \\sum_{i=1}^{N} \\ln Z_i(H_k)\n$$\nFor each test case, we are provided with two lists of floating-point numbers, representing the per-diagnostic log-evidences for $H_1$ and $H_2$. We must first sum the values in each list to obtain $\\ln Z_{\\text{total}}(H_1)$ and $\\ln Z_{\\text{total}}(H_2)$.\n\n**Step 2: Calculation of the Log-Evidence Difference, $\\Delta$**\n\nThe primary quantity for comparing the two models is the difference in their total log-evidences. This quantity, denoted by $\\Delta$, is defined as:\n$$\n\\Delta = \\ln Z_{\\text{total}}(H_1) - \\ln Z_{\\text{total}}(H_2)\n$$\nA positive value of $\\Delta$ indicates that the evidence favors model $H_1$ over $H_2$, while a negative value indicates the opposite. A value of $\\Delta$ near zero suggests that the evidence does not distinguish between the models.\n\n**Step 3: Calculation of the Bayes Factor, $K_{12}$**\n\nThe Bayes factor, $K_{12}$, is the ratio of the evidences of the two models:\n$$\nK_{12} = \\frac{Z_{\\text{total}}(H_1)}{Z_{\\text{total}}(H_2)}\n$$\nUsing the properties of logarithms, the Bayes factor can be calculated directly from $\\Delta$:\n$$\n\\ln(K_{12}) = \\ln\\left(\\frac{Z_{\\text{total}}(H_1)}{Z_{\\text{total}}(H_2)}\\right) = \\ln Z_{\\text{total}}(H_1) - \\ln Z_{\\text{total}}(H_2) = \\Delta\n$$\nTherefore, the Bayes factor is obtained by exponentiating $\\Delta$:\n$$\nK_{12} = \\exp(\\Delta)\n$$\nThis value represents how many times more probable the observed data are under model $H_1$ compared to model $H_2$.\n\n**Step 4: Categorical Interpretation of Evidence Strength, $c$**\n\nThe strength of the evidence is categorized using a Jeffreys-style scale applied to the absolute value of the log-evidence difference, $|\\Delta|$. The categorical support code, $c$, is an integer determined by the following thresholds:\n- If $0 \\leq |\\Delta| < 1$, the support is considered negligible (\"barely worth a mention\"), and we assign $c = 0$.\n- If $1 \\leq |\\Delta| < 2.5$, the support is \"weak\", and we assign $c = 1$.\n- If $2.5 \\leq |\\Delta| < 5$, the support is \"moderate\", and we assign $c = 2$.\n- If $|\\Delta| \\geq 5$, the support is \"strong\", and we assign $c = 3$.\n\n**Step 5: Determination of the Favored Model, $m$**\n\nThe favored model index, $m$, is determined by the sign of $\\Delta$. A numerical tolerance, $\\epsilon = 10^{-9}$, is introduced to handle cases where the models are indistinguishable within machine precision.\n- If $|\\Delta| < \\epsilon$, the models are considered to be in a \"tie\", and we set $m = 0$.\n- If $\\Delta > 0$ (and $|\\Delta| \\geq \\epsilon$), the evidence favors model $H_1$, and we set $m = 1$.\n- If $\\Delta < 0$ (and $|\\Delta| \\geq \\epsilon$), the evidence favors model $H_2$, and we set $m = 2$.\n\nBy executing these five steps for each provided test case, we can compute the required tuple $[K_{12}, \\Delta, c, m]$.\n\nThe overall algorithm for a single test case is as follows:\n1.  Receive two lists of per-diagnostic log-evidences, $L_1$ for $H_1$ and $L_2$ for $H_2$.\n2.  Compute the total log-evidence for each model: $\\ln Z_1 = \\sum L_1$ and $\\ln Z_2 = \\sum L_2$.\n3.  Compute the difference: $\\Delta = \\ln Z_1 - \\ln Z_2$.\n4.  Compute the Bayes factor: $K_{12} = \\exp(\\Delta)$.\n5.  Determine the support code $c$ based on $|\\Delta|$.\n6.  Determine the favored model index $m$ based on the sign of $\\Delta$ and the tolerance $\\epsilon$.\n7.  Store the results as a list of four elements: $[K_{12}, \\Delta, c, m]$.\nThis procedure will be systematically applied to all test cases to generate the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and interprets Bayes factors from log-evidence values for\n    competing scientific models.\n    \"\"\"\n    # Test suite with per-diagnostic log-evidences for H1 and H2.\n    test_cases = [\n        # (log_evidence_H1, log_evidence_H2)\n        ([-12.3, 5.1, -0.8], [-13.0, 4.0, -0.5]),  # Test case A\n        ([1.0], [0.0]),                            # Test case B\n        ([-15.0, -2.0], [-10.0, -1.0]),            # Test case C\n        ([100.000000000001], [100.0]),             # Test case D\n        ([-700.0, 300.0, 10.0], [-705.0, 300.0, 12.5]), # Test case E\n        ([-100.0], [-110.0]),                      # Test case F\n        ([0.0, 0.0], [0.0, 0.0]),                  # Test case G\n    ]\n\n    results = []\n    epsilon = 1e-9\n\n    for h1_logs, h2_logs in test_cases:\n        # Step 1: Calculate total log-evidence for each model\n        ln_z1 = np.sum(h1_logs)\n        ln_z2 = np.sum(h2_logs)\n\n        # Step 2: Calculate the log-evidence difference (Delta)\n        delta = ln_z1 - ln_z2\n\n        # Step 3: Calculate the Bayes factor (K12)\n        k12 = np.exp(delta)\n\n        # Step 4: Determine the categorical support code (c)\n        abs_delta = abs(delta)\n        c = 0 # Default for 0 = |Delta|  1\n        if abs_delta >= 5.0:\n            c = 3 # Strong evidence\n        elif abs_delta >= 2.5:\n            c = 2 # Moderate evidence\n        elif abs_delta >= 1.0:\n            c = 1 # Weak evidence\n\n        # Step 5: Determine the favored model index (m)\n        m = 0 # Default for a tie\n        if abs_delta >= epsilon:\n            if delta > 0:\n                m = 1 # Favor H1\n            else: # delta  0\n                m = 2 # Favor H2\n\n        # Store the result tuple for this test case\n        results.append([k12, delta, c, m])\n\n    # Final print statement in the exact required format.\n    # The output is a list of lists, formatted as a string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}