{
    "hands_on_practices": [
        {
            "introduction": "To appreciate why in situ data reduction is not just an optimization but a necessity, we must first understand the scale of the data challenge in modern simulations. This first practice invites you to perform a fundamental calculation to determine how quickly a high-performance burst buffer can be saturated by raw simulation output . By grounding the abstract concept of \"big data\" in a concrete storage limit, you will gain a direct appreciation for the I/O bottleneck that motivates the entire in situ paradigm.",
            "id": "3994977",
            "problem": "A gyrokinetic Particle-In-Cell (PIC) simulation of magnetized plasma microturbulence in a tokamak is executed on a leadership-class High-Performance Computing (HPC) system. The workflow uses a node-local Non-Volatile Memory (NVM) burst buffer to stage data before it is flushed to the parallel file system. Consider a configuration without any in situ reduction or compression, such that each timestep produces a raw output of size $D = 20$ GB, and the total burst buffer capacity available to the job is $C = 200$ GB. Assume that data from successive timesteps accumulates in the burst buffer and that the flush to the parallel file system is disabled during a diagnostic window, so the only constraint on storage is that the cumulative buffered data must not exceed the burst buffer capacity.\n\nUsing only the fundamental definition that storage occupancy after $n$ timesteps is the sum of $n$ independent writes of size $D$, determine the maximum integer number of timesteps that can be buffered without exceeding capacity. Express your result as a dimensionless count of timesteps. Then, briefly explain the architectural implications for designing in situ data analysis and reduction workflows in computational fusion science and engineering.\n\nReport the final answer as a single integer (no units).",
            "solution": "The problem statement will first be validated for scientific soundness, self-consistency, and clarity before a solution is attempted.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   Data size per timestep: $D = 20$ GB\n-   Total burst buffer capacity: $C = 200$ GB\n-   System behavior: Data from successive timesteps accumulates in the burst buffer.\n-   Constraint: The cumulative buffered data must not exceed the burst buffer capacity. The flush to the parallel file system is disabled.\n-   Modeling assumption: Storage occupancy after $n$ timesteps is the sum of $n$ independent writes of size $D$.\n-   Objective 1: Determine the maximum integer number of timesteps, $n$, that can be buffered.\n-   Objective 2: Briefly explain the architectural implications for designing in situ data analysis and reduction workflows in computational fusion science and engineering.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem is scientifically grounded. It describes a realistic scenario in modern computational science, specifically within the domain of plasma physics simulations on High-Performance Computing (HPC) systems. The use of burst buffers, the specified data volumes, and the context of gyrokinetic simulations are all standard and factually correct concepts in the field.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary quantitative data ($C$ and $D$) and a clear, unambiguous relationship between the variables needed to find a unique integer solution for the number of timesteps, $n$.\n-   **Objectivity**: The problem is stated objectively, using precise technical language without subjective or opinion-based claims.\n-   **Flaw Assessment**: The problem does not violate any fundamental principles, is not missing information, contains no contradictions, and uses realistic parameters. The calculation required is elementary, but it serves as a foundation for a conceptual question directly relevant to the stated topic of *in situ data analysis and reduction workflows*. Therefore, it is not considered trivial or pseudo-profound in this context.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be provided.\n\n**Problem Solution**\n\nLet $n$ be the integer number of timesteps.\nLet $D$ be the raw output data size produced per timestep.\nLet $C$ be the total capacity of the Non-Volatile Memory (NVM) burst buffer.\n\nThe values given in the problem are:\n-   $D = 20$ GB\n-   $C = 200$ GB\n\nThe problem states that the total storage occupancy is the sum of the data from each timestep. For $n$ timesteps, the total accumulated data, which we shall denote as $S(n)$, is given by:\n$$\nS(n) = \\sum_{i=1}^{n} D = n \\times D\n$$\n\nThe primary constraint is that the total accumulated data must not exceed the burst buffer capacity $C$. This can be expressed as an inequality:\n$$\nS(n) \\le C\n$$\nSubstituting the expression for $S(n)$, we have:\n$$\nn \\times D \\le C\n$$\n\nTo find the maximum number of timesteps, $n_{\\text{max}}$, we must find the largest integer $n$ that satisfies this inequality. We can rearrange the inequality to solve for $n$:\n$$\nn \\le \\frac{C}{D}\n$$\n\nNow, we substitute the given numerical values for $C$ and $D$:\n$$\nn \\le \\frac{200 \\text{ GB}}{20 \\text{ GB}}\n$$\n$$\nn \\le 10\n$$\n\nSince $n$ must be an integer, the maximum number of timesteps that can be buffered without exceeding the capacity is $n_{\\text{max}} = 10$.\n\n**Architectural Implications for In Situ Workflows**\n\nThis simple calculation has profound architectural implications for the design of data analysis and reduction workflows in large-scale computational fusion science.\n\n1.  **The Data Velocity Bottleneck**: The result that a high-performance, and typically expensive, burst buffer with a capacity of $200$ GB can be saturated after only $10$ timesteps of a simulation highlights the extreme \"data velocity\" problem. Writing raw data from every timestep to any storage tier is unsustainable. If the simulation continues, the execution would be forced to stall until the buffer is flushed to a slower, parallel file system, creating a severe I/O (Input/Output) bottleneck that negates the performance gains of the HPC system.\n\n2.  **Mandate for In Situ Processing**: The physical impossibility of storing all raw data compels a paradigm shift from traditional post-processing to *in situ* processing. Data analysis, visualization, and reduction must be performed \"in transit,\" while the data resides in high-bandwidth memory or fast local storage like the NVM burst buffer. Instead of staging the full $20$ GB per timestep, an in situ workflow would generate a reduced data product (e.g., statistical moments, spectral coefficients, compressed or thresholded data) that is orders of magnitude smaller, thus allowing the simulation to run for thousands or millions of timesteps without stalling on I/O.\n\n3.  **Co-Design of Simulation and Analysis**: This constraint forces a tight integration, or \"co-design,\" of the simulation code with data analysis and reduction libraries. The workflow is no longer a simple sequence of `simulate` then `analyze`. Instead, analysis routines are coupled directly into the main simulation loop. This requires careful management of computational resources, as the cycles used for in situ analysis are cycles not used for advancing the simulation's physics. The analysis algorithms must be highly efficient to minimize their performance impact on the simulation.\n\n4.  **Information Loss and Scientific Discovery**: In situ reduction inherently involves a trade-off. By deciding *a priori* what analysis to perform and what data to discard, scientists risk losing unexpected or subtle features in the raw data that could lead to new discoveries. Therefore, a critical component of designing these workflows is developing intelligent reduction techniques, such as adaptive sampling or feature-based analysis, that can identify and preserve scientifically salient information while still achieving substantial data reduction factors. This places a premium on deep domain expertise during the workflow design phase.",
            "answer": "$$\\boxed{10}$$"
        },
        {
            "introduction": "Once the necessity of data reduction is clear, the next step is to quantify its impact on system performance. This exercise provides a hands-on opportunity to calculate the direct benefits of applying a simple compression algorithm before writing data to the filesystem . By translating a given compression ratio into reduced data size and I/O time, you will develop a quantitative understanding of how in situ reduction alleviates performance bottlenecks.",
            "id": "3994993",
            "problem": "A high-fidelity turbulence-resolving gyrokinetic simulation in computational fusion science produces an in situ scalar diagnostic field at each output checkpoint. The field is discretized on a uniform grid of $512^3$ points, and each sample is stored as a single-precision floating-point value, which occupies $4$ bytes. An on-node in situ compressor achieves a compression ratio of $10$ on this field prior to writing to a parallel filesystem with a sustained input/output (I/O) bandwidth of $100$ gigabytes per second. For unit consistency, define $1$ gigabyte (GB) as $10^9$ bytes.\n\nUsing only the foundational definitions that (i) data size equals the product of the number of samples and bytes per sample, (ii) compression ratio equals uncompressed size divided by compressed size, and (iii) I/O time equals data size divided by sustained bandwidth, determine the following three quantities:\n1. The uncompressed data size in gigabytes.\n2. The compressed data size in gigabytes.\n3. The reduced I/O time to write the compressed dataset in seconds on the stated filesystem.\n\nExpress the two sizes in GB and the time in seconds. Round each of the three quantities to four significant figures. Your final answer must be provided as a single row matrix containing the three rounded values, in the order listed above, without units.",
            "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Grid discretization: $512^3$ points\n- Bytes per sample: $4$ bytes (single-precision float)\n- Compression ratio, $C_r$: $10$\n- Sustained I/O bandwidth, $R_{IO}$: $100$ gigabytes per second\n- Definition of gigabyte (GB): $1 \\text{ GB} = 10^9 \\text{ bytes}$\n- Definition of data size: (number of samples) $\\times$ (bytes per sample)\n- Definition of compression ratio: (uncompressed size) / (compressed size)\n- Definition of I/O time: (data size) / (sustained bandwidth)\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is grounded in fundamental concepts of data storage, compression, and transfer rates, which are central to high-performance computing and computational science. The values provided ($512^3$ grid, $4$-byte floats, $10:1$ compression, $100$ GB/s bandwidth) are realistic for contemporary large-scale scientific simulations. The explicit definition of a gigabyte as $10^9$ bytes (as opposed to $2^{30}$ bytes) is a mark of precision.\n- **Well-Posed:** All necessary data and definitions are provided. The three quantities to be determined are clearly specified, and a unique solution can be derived from the givens.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased language.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is self-contained, scientifically sound, and well-posed. A formal solution will now be derived.\n\n### Solution Derivation\nLet $N$ be the total number of samples (grid points), $B_s$ be the bytes per sample, $S_u$ be the uncompressed data size, $S_c$ be the compressed data size, $C_r$ be the compression ratio, $R_{IO}$ be the I/O bandwidth, and $T_{IO}$ be the I/O time.\n\nThe given values are:\n- $N = 512^3$\n- $B_s = 4$ bytes\n- $C_r = 10$\n- $R_{IO} = 100 \\text{ GB/s}$\n- The conversion factor from bytes to gigabytes is $1 \\text{ GB} = 10^9 \\text{ bytes}$.\n\nThe three quantities to be determined are:\n1. $S_u$ in GB.\n2. $S_c$ in GB.\n3. $T_{IO}$ for the compressed data in seconds.\n\n**1. Uncompressed Data Size ($S_u$)**\n\nAccording to the provided definition, the uncompressed data size in bytes is the product of the number of samples and the bytes per sample.\n$$S_{u, \\text{bytes}} = N \\times B_s$$\nSubstituting the given values:\n$$S_{u, \\text{bytes}} = 512^3 \\times 4$$\nSince $512 = 2^9$, we have:\n$$S_{u, \\text{bytes}} = (2^9)^3 \\times 2^2 = 2^{27} \\times 2^2 = 2^{29} \\text{ bytes}$$\n$$S_{u, \\text{bytes}} = 536,870,912 \\text{ bytes}$$\nTo convert this size to gigabytes (GB), we divide by $10^9$.\n$$S_u = \\frac{S_{u, \\text{bytes}}}{10^9} = \\frac{536,870,912}{10^9} \\text{ GB}$$\n$$S_u = 0.536870912 \\text{ GB}$$\nRounding to four significant figures as required:\n$$S_u \\approx 0.5369 \\text{ GB}$$\n\n**2. Compressed Data Size ($S_c$)**\n\nThe compression ratio is defined as the uncompressed size divided by the compressed size.\n$$C_r = \\frac{S_u}{S_c}$$\nRearranging for the compressed size, $S_c$:\n$$S_c = \\frac{S_u}{C_r}$$\nUsing the unrounded value of $S_u$ for accuracy and the given $C_r$:\n$$S_c = \\frac{0.536870912 \\text{ GB}}{10}$$\n$$S_c = 0.0536870912 \\text{ GB}$$\nRounding to four significant figures:\n$$S_c \\approx 0.05369 \\text{ GB}$$\n\n**3. Reduced I/O Time ($T_{IO}$)**\n\nThe I/O time is defined as the data size to be written divided by the sustained I/O bandwidth. The data being written is the compressed dataset, so its size is $S_c$.\n$$T_{IO} = \\frac{S_c}{R_{IO}}$$\nThe units are consistent ($S_c$ in GB and $R_{IO}$ in GB/s), so the resulting time will be in seconds.\nUsing the unrounded value of $S_c$ for accuracy:\n$$T_{IO} = \\frac{0.0536870912 \\text{ GB}}{100 \\text{ GB/s}}$$\n$$T_{IO} = 0.000536870912 \\text{ s}$$\nRounding to four significant figures:\n$$T_{IO} \\approx 0.0005369 \\text{ s}$$\n\nThe three requested quantities, rounded to four significant figures, are $0.5369$, $0.05369$, and $0.0005369$. These will be presented in a row matrix as specified.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5369 & 0.05369 & 0.0005369\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While generic compression is effective, many scientific applications benefit from reduction techniques that are aware of the underlying structure in the data. This practice explores the use of Singular Value Decomposition (SVD), a powerful matrix factorization method, to achieve intelligent, physics-aware compression . You will learn how the reconstruction error is directly linked to the discarded singular values and determine the optimal compression level to meet a specific fidelity requirement, a crucial skill in co-designing simulation and analysis.",
            "id": "3995012",
            "problem": "A magnetically confined fusion experiment produces a matrix of snapshots $X \\in \\mathbb{R}^{m \\times n}$, where $m=400$ diagnostics channels and $n=1000$ time slices. To support in situ data analysis and reduction, the pipeline compresses each $X$ by keeping only the leading rank-$k$ factors from the Singular Value Decomposition (SVD; Singular Value Decomposition). The fidelity requirement stipulates that the relative squared reconstruction error must satisfy $\\lVert X - X_{k} \\rVert_{F}^{2} / \\lVert X \\rVert_{F}^{2} \\leq \\delta$, with $\\delta = 0.005$, to ensure that dominant magnetohydrodynamic mode energy is preserved in the compressed representation.\n\nAssume that $X$ has numerical rank $r=12$ with singular values (in descending order) given by $\\sigma_{1} = 10$, $\\sigma_{2} = 5$, $\\sigma_{3} = 3$, $\\sigma_{4} = 2$, $\\sigma_{5} = 1.5$, $\\sigma_{6} = 1.2$, $\\sigma_{7} = 1.0$, $\\sigma_{8} = 0.8$, $\\sigma_{9} = 0.6$, $\\sigma_{10} = 0.5$, $\\sigma_{11} = 0.4$, and $\\sigma_{12} = 0.3$.\n\nStarting only from the definitions of SVD and the Frobenius norm, derive how the squared Frobenius reconstruction error of the best rank-$k$ approximation depends on the singular values that are discarded. Then, using the provided singular values and the fidelity requirement, determine the minimal integer $k$ that satisfies $\\lVert X - X_{k} \\rVert_{F}^{2} / \\lVert X \\rVert_{F}^{2} \\leq \\delta$. Express the final answer as the minimal integer $k$.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, objective, and provides all necessary information to derive a unique solution.\n\nThe problem requires a two-part solution: first, a derivation of the relationship between the squared Frobenius reconstruction error and the singular values, and second, the application of this result to find the minimal rank $k$ for a given fidelity requirement.\n\n**Part 1: Derivation of the Reconstruction Error**\n\nLet $X$ be a real matrix of size $m \\times n$. The Singular Value Decomposition (SVD) of $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U$ is an $m \\times m$ orthogonal matrix ($U^T U = I_m$), $V$ is an $n \\times n$ orthogonal matrix ($V^T V = I_n$), and $\\Sigma$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers $\\sigma_i$ on the diagonal, called the singular values. The singular values are ordered in a descending manner, $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_p \\geq 0$, where $p = \\min(m, n)$. If the rank of $X$ is $r$, then there are exactly $r$ non-zero singular values. The SVD can also be written as an outer product expansion:\n$$X = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\nThe Frobenius norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as:\n$$\\lVert A \\rVert_{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2}$$\nThe squared Frobenius norm can be expressed using the trace of the matrix:\n$$\\lVert A \\rVert_{F}^2 = \\text{tr}(A^T A)$$\nApplying this to the matrix $X$:\n$$\\lVert X \\rVert_{F}^2 = \\text{tr}(X^T X)$$\nSubstituting the SVD of $X$ into this expression:\n$$\\lVert X \\rVert_{F}^2 = \\text{tr}((U \\Sigma V^T)^T (U \\Sigma V^T)) = \\text{tr}(V \\Sigma^T U^T U \\Sigma V^T)$$\nSince $U$ is orthogonal, $U^T U = I_m$. Using the cyclic property of the trace, $\\text{tr}(ABC) = \\text{tr}(CAB)$:\n$$\\lVert X \\rVert_{F}^2 = \\text{tr}(V \\Sigma^T \\Sigma V^T) = \\text{tr}(\\Sigma^T \\Sigma V^T V)$$\nSince $V$ is orthogonal, $V^T V = I_n$.\n$$\\lVert X \\rVert_{F}^2 = \\text{tr}(\\Sigma^T \\Sigma)$$\nThe matrix $\\Sigma^T \\Sigma$ is an $n \\times n$ diagonal matrix with the first $r$ diagonal entries being $\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_r^2$, and the rest being zero. The trace is the sum of the diagonal elements. Therefore, the squared Frobenius norm of $X$ is the sum of its squared singular values:\n$$\\lVert X \\rVert_{F}^2 = \\sum_{i=1}^r \\sigma_i^2$$\n\nAccording to the Eckart-Young-Mirsky theorem, the best rank-$k$ approximation to $X$ in the Frobenius norm, denoted as $X_k$, is obtained by truncating the SVD sum at the $k$-th term:\n$$X_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$$\nThe reconstruction error is the difference between the original matrix and its approximation, $X - X_k$. Using the outer product expansions:\n$$X - X_k = \\sum_{i=1}^r \\sigma_i u_i v_i^T - \\sum_{i=1}^k \\sigma_i u_i v_i^T = \\sum_{i=k+1}^r \\sigma_i u_i v_i^T$$\nThis error matrix itself has an SVD form. Its singular values are the discarded singular values of $X$, namely $\\sigma_{k+1}, \\sigma_{k+2}, \\dots, \\sigma_r$. Applying the same relationship between the Frobenius norm and singular values to the error matrix $X - X_k$, we find its squared Frobenius norm:\n$$\\lVert X - X_k \\rVert_{F}^2 = \\sum_{i=k+1}^r \\sigma_i^2$$\nThis result shows that the squared Frobenius reconstruction error of the best rank-$k$ approximation is precisely the sum of the squares of the discarded singular values.\n\n**Part 2: Finding the Minimal Rank $k$**\n\nThe fidelity requirement is given by:\n$$\\frac{\\lVert X - X_k \\rVert_{F}^2}{\\lVert X \\rVert_{F}^2} \\leq \\delta$$\nUsing the expressions derived above, this inequality becomes:\n$$\\frac{\\sum_{i=k+1}^r \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2} \\leq \\delta$$\nThe problem states that the numerical rank is $r=12$ and the fidelity threshold is $\\delta = 0.005$. The singular values are provided.\n\nFirst, we compute the total squared Frobenius norm, $\\lVert X \\rVert_{F}^2 = \\sum_{i=1}^{12} \\sigma_i^2$:\n$$\n\\begin{aligned}\n\\lVert X \\rVert_{F}^2 &= (10)^2 + (5)^2 + (3)^2 + (2)^2 + (1.5)^2 + (1.2)^2 + (1.0)^2 + (0.8)^2 + (0.6)^2 + (0.5)^2 + (0.4)^2 + (0.3)^2 \\\\\n&= 100 + 25 + 9 + 4 + 2.25 + 1.44 + 1 + 0.64 + 0.36 + 0.25 + 0.16 + 0.09 \\\\\n&= 144.19\n\\end{aligned}\n$$\nNext, we calculate the maximum allowed squared reconstruction error:\n$$\\lVert X - X_k \\rVert_{F}^2 \\leq \\delta \\cdot \\lVert X \\rVert_{F}^2 = 0.005 \\times 144.19 = 0.72095$$\nWe must find the minimal integer $k$ such that the sum of the squares of the discarded singular values, $\\sum_{i=k+1}^{12} \\sigma_i^2$, is less than or equal to this threshold.\nLet $E_k = \\sum_{i=k+1}^{12} \\sigma_i^2$. We will test values of $k$ starting from $r-1=11$ downwards.\nFor $k=11$:\n$E_{11} = \\sigma_{12}^2 = (0.3)^2 = 0.09$. Since $0.09 \\leq 0.72095$, the condition is met.\n\nFor $k=10$:\n$E_{10} = \\sigma_{11}^2 + \\sigma_{12}^2 = (0.4)^2 + 0.09 = 0.16 + 0.09 = 0.25$. Since $0.25 \\leq 0.72095$, the condition is met.\n\nFor $k=9$:\n$E_{9} = \\sigma_{10}^2 + E_{10} = (0.5)^2 + 0.25 = 0.25 + 0.25 = 0.50$. Since $0.50 \\leq 0.72095$, the condition is met.\n\nFor $k=8$:\n$E_{8} = \\sigma_{9}^2 + E_{9} = (0.6)^2 + 0.50 = 0.36 + 0.50 = 0.86$. Since $0.86 > 0.72095$, the condition is not met.\n\nThe condition fails for $k=8$ but holds for $k=9$. Therefore, the minimal integer value of $k$ that satisfies the fidelity requirement is $9$.\n\nAlternatively, we can express the fidelity requirement as retaining a certain fraction of the \"energy\":\n$\\sum_{i=1}^k \\sigma_i^2 \\geq (1-\\delta) \\sum_{i=1}^{12} \\sigma_i^2$.\nWith $1 - \\delta = 0.995$, the required retained energy is $0.995 \\times 144.19 = 143.46905$.\nLet's compute the cumulative sum of squared singular values, $S_k = \\sum_{i=1}^k \\sigma_i^2$:\n$S_8 = \\sum_{i=1}^8 \\sigma_i^2 = \\lVert X \\rVert_F^2 - E_8 = 144.19 - 0.86 = 143.33$.\nSince $S_8 = 143.33 < 143.46905$, $k=8$ is not sufficient.\n$S_9 = S_8 + \\sigma_9^2 = 143.33 + (0.6)^2 = 143.33 + 0.36 = 143.69$.\nSince $S_9 = 143.69 \\geq 143.46905$, $k=9$ is the minimal rank that satisfies the criterion.\nBoth methods confirm the result.",
            "answer": "$$\\boxed{9}$$"
        }
    ]
}