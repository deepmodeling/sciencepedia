{
    "hands_on_practices": [
        {
            "introduction": "To truly understand PDE classification, we begin with the foundational mathematics. This first exercise guides you through the derivation of characteristic curves for a general second-order linear PDE, starting directly from the definition of the principal symbol. This practice is crucial as it reveals how the algebraic discriminant, $\\Delta = b^2 - ac$, directly corresponds to the geometric existence of real characteristic directions, which forms the core of the hyperbolic, parabolic, and elliptic classification. ",
            "id": "3992010",
            "problem": "In computational fusion science and engineering, two-dimensional reduced models in the poloidal–radial plane often involve second-order linear operators whose leading-order behavior determines wave propagation, diffusion, or mixed behavior. Consider the principal part of a linearized operator acting on a scalar field $u(x,y)$, written locally as\n$$\na(x,y)\\,u_{xx} + 2\\,b(x,y)\\,u_{xy} + c(x,y)\\,u_{yy} \\, ,\n$$\nwhere $a(x,y)$, $b(x,y)$, and $c(x,y)$ are smooth functions determined by the underlying physics, such as the symmetry-reduced metric coefficients of flux-surface coordinates or an anisotropic transport tensor aligned with the magnetic field in Magnetohydrodynamics (MHD). The principal symbol is\n$$\nP(\\boldsymbol{\\xi}) \\equiv a\\,\\xi_x^{2} + 2\\,b\\,\\xi_x\\,\\xi_y + c\\,\\xi_y^{2} \\, .\n$$\nCharacteristic curves are defined by the vanishing of the principal symbol on the normal covector to the curve. Let a characteristic curve be described locally as the level set $F(x,y)=0$, and denote its slope by $m \\equiv \\frac{dy}{dx}$.\n\nStarting from the definition of the principal symbol and the characteristic condition $P(\\nabla F)=0$, derive the quadratic equation governing the slope $m$ of characteristic curves and solve it in closed form to obtain the two characteristic directions $m_{+}(x,y)$ and $m_{-}(x,y)$. Then relate the number of real solutions to the classification of the partial differential equation (hyperbolic, parabolic, or elliptic) in terms of the discriminant. Express your final answer as the ordered pair $\\big(m_{+},\\,m_{-}\\big)$ as analytic expressions in $a$, $b$, and $c$.",
            "solution": "The operator’s leading-order behavior is captured by its principal symbol,\n$$\nP(\\boldsymbol{\\xi}) = a\\,\\xi_x^{2} + 2\\,b\\,\\xi_x\\,\\xi_y + c\\,\\xi_y^{2} \\, ,\n$$\nwhich is a quadratic form in the covector $\\boldsymbol{\\xi} = (\\xi_x,\\xi_y)$. A curve $F(x,y)=0$ has normal covector $\\nabla F = (F_x,F_y)$. By the definition of characteristic curves for second-order linear partial differential equations, the principal symbol vanishes on the normal covector to a characteristic:\n$$\nP(\\nabla F) \\;=\\; a\\,F_x^{2} + 2\\,b\\,F_x\\,F_y + c\\,F_y^{2} \\;=\\; 0 \\, .\n$$\nTo connect this to the slope $m \\equiv \\frac{dy}{dx}$ of the curve $F(x,y)=0$, we use the total derivative along the curve:\n$$\n\\frac{d}{dx} F(x,y(x)) \\;=\\; F_x + F_y\\,\\frac{dy}{dx} \\;=\\; 0 \\;\\;\\Rightarrow\\;\\; m \\equiv \\frac{dy}{dx} \\;=\\; -\\,\\frac{F_x}{F_y} \\, ,\n$$\nprovided $F_y \\neq 0$ (the complementary case is analogous and yields vertical characteristics when $F_y=0$). Substitute $F_x = -\\,m\\,F_y$ into $P(\\nabla F)=0$ and divide by $F_y^{2}$ (which is nonzero along a regular curve):\n$$\na\\,\\left(\\frac{F_x}{F_y}\\right)^{2} + 2\\,b\\,\\left(\\frac{F_x}{F_y}\\right) + c \\;=\\; 0\n\\;\\;\\Rightarrow\\;\\;\na\\,(-m)^{2} + 2\\,b\\,(-m) + c \\;=\\; 0 \\, .\n$$\nSimplifying, we obtain the quadratic equation for the slope $m$:\n$$\na\\,m^{2} - 2\\,b\\,m + c \\;=\\; 0 \\, .\n$$\nSolving this quadratic for $m$ yields the two characteristic directions,\n$$\nm_{\\pm} \\;=\\; \\frac{2\\,b \\,\\pm\\, \\sqrt{(2\\,b)^{2} - 4\\,a\\,c}}{2\\,a}\n\\;=\\;\n\\frac{b \\,\\pm\\, \\sqrt{\\,b^{2} - a\\,c\\,}}{a} \\, ,\n$$\nassuming $a \\neq 0$ at the point of interest. The discriminant controlling the nature of the roots is\n$$\n\\Delta \\;\\equiv\\; b^{2} - a\\,c \\, .\n$$\nThe classification follows directly from the number and nature of the real roots $m_{\\pm}$:\n- If $\\Delta > 0$, there are two distinct real roots $m_{+} \\neq m_{-}$, and the operator is hyperbolic at the point.\n- If $\\Delta = 0$, there is a repeated real root $m_{+} = m_{-}$, and the operator is parabolic at the point.\n- If $\\Delta  0$, there are no real roots (the slopes are complex), and the operator is elliptic at the point.\n\nIn fusion-relevant contexts, an anisotropic but positive-definite transport tensor aligned with the magnetic field typically gives $a>0$, $c>0$, and $a\\,c - b^{2} > 0$, hence $\\Delta  0$ and elliptic behavior (diffusion-dominated). In contrast, linearized wave operators associated with Magnetohydrodynamics (MHD) modes can yield $\\Delta > 0$, corresponding to hyperbolic behavior and real characteristic directions for wave propagation.\n\nThus, the requested analytic expressions for the characteristic slopes are\n$$\nm_{+} \\;=\\; \\frac{b + \\sqrt{\\,b^{2} - a\\,c\\,}}{a}\n\\quad\\text{and}\\quad\nm_{-} \\;=\\; \\frac{b - \\sqrt{\\,b^{2} - a\\,c\\,}}{a} \\, .\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\dfrac{b + \\sqrt{b^{2} - a\\,c}}{a}  \\dfrac{b - \\sqrt{b^{2} - a\\,c}}{a}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Physical systems in fusion science often exhibit complex behaviors that are not captured by a single PDE type. This next practice explores this reality through the lens of the canonical Tricomi equation, a famous mixed-type PDE that models physical transitions like those from subsonic to supersonic flow. By applying the principles of characteristic analysis, you will determine how the equation's type changes from elliptic to hyperbolic across a critical boundary, providing a concrete example of how local classification uncovers rich and spatially varying physics. ",
            "id": "3992020",
            "problem": "In computational fusion science and engineering, mixed-type partial differential equations arise in reduced models of transonic magnetohydrodynamic (MHD) equilibria and wave propagation near critical surfaces. Consider the canonical Tricomi equation\n$$\nu_{xx} + x\\,u_{yy} = 0,\n$$\nwhich captures a change of type across the line $x=0$. Starting only from the foundational notions that (i) classification of a linear second-order partial differential equation in two variables is determined by the principal symbol and its sign properties, and (ii) characteristic curves are defined geometrically as those along which the principal symbol vanishes when evaluated on conormals to the curve, do the following:\n\n- Determine, from first principles, the type (elliptic, parabolic, hyperbolic) of the equation in the regions $x>0$, $x=0$, and $x0$, and explain in physical terms why a change of type is expected in transonic or critical-surface regimes in magnetized plasmas.\n- Derive the ordinary differential equation for characteristic curves in the $(x,y)$-plane implied by the geometric definition of characteristics in terms of the principal symbol.\n- Integrate this ordinary differential equation to obtain explicit characteristic invariants in the regions $x>0$ and $x0$, and identify what happens at $x=0$.\n\nProvide your final answer as the two characteristic invariants written as a single row matrix, with each entry given piecewise on $x0$, $x=0$, and $x>0$. No numerical rounding is required. Do not include any units. The final answer must be analytic expressions only, with no explanatory text inside the final answer box.",
            "solution": "The classification of the Tricomi equation, $u_{xx} + x\\,u_{yy} = 0$, is determined by the discriminant of its principal part, $\\Delta = B^2 - 4AC$. With coefficients $A=1$, $B=0$, and $C=x$, the discriminant is $\\Delta = -4x$. The type of the equation therefore depends on the sign of $x$:\n*   **Elliptic ($x > 0$)**: $\\Delta  0$. The principal symbol $k_x^2 + xk_y^2$ is positive definite. This regime is analogous to subsonic flow where information propagates globally.\n*   **Parabolic ($x = 0$)**: $\\Delta = 0$. The equation degenerates, representing a critical surface like a sonic transition.\n*   **Hyperbolic ($x  0$)**: $\\Delta > 0$. The principal symbol is indefinite, and information propagates along specific paths (characteristics), analogous to supersonic flow.\n\nCharacteristic curves are defined by the condition that the principal symbol vanishes for a vector normal to the curve. This yields an ordinary differential equation (ODE) for the characteristic slopes $\\frac{dy}{dx}$:\n$$\n\\left(\\frac{dy}{dx}\\right)^2 = -x \\quad \\implies \\quad \\frac{dy}{dx} = \\pm \\sqrt{-x}\n$$\nWe integrate this ODE to find the characteristic invariants (constants of integration, $C$).\n\n*   **In the hyperbolic region ($x  0$)**: $\\sqrt{-x}$ is real. Integration gives two families of real characteristic curves:\n$$\n\\int dy = \\pm \\int \\sqrt{-x} \\, dx \\quad \\implies \\quad y = \\mp \\frac{2}{3}(-x)^{3/2} + C\n$$\nThe invariants are $\\zeta_{1,2}(x,y) = y \\pm \\frac{2}{3}(-x)^{3/2}$.\n\n*   **In the elliptic region ($x > 0$)**: $\\sqrt{-x}$ is imaginary. There are no real characteristic curves. Formal integration gives complex invariants:\n$$\n\\int dy = \\pm \\int i\\sqrt{x} \\, dx \\quad \\implies \\quad y = \\pm i \\frac{2}{3}x^{3/2} + C\n$$\nThe invariants are $\\zeta_{1,2}(x,y) = y \\mp i\\frac{2}{3}x^{3/2}$.\n\n*   **On the parabolic line ($x = 0$)**: The ODE becomes $\\frac{dy}{dx} = 0$, which gives $y = C$. Both families of characteristics collapse into a single family of horizontal lines.\n\nCombining these results gives the piecewise definitions of the two characteristic invariants:\n$$ \\zeta_1(x,y) = \\begin{cases} y + \\frac{2}{3}(-x)^{3/2}  \\text{if } x  0 \\\\ y  \\text{if } x = 0 \\\\ y - i \\frac{2}{3}x^{3/2}  \\text{if } x > 0 \\end{cases} \\qquad \\zeta_2(x,y) = \\begin{cases} y - \\frac{2}{3}(-x)^{3/2}  \\text{if } x  0 \\\\ y  \\text{if } x = 0 \\\\ y + i \\frac{2}{3}x^{3/2}  \\text{if } x > 0 \\end{cases} $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\begin{cases} y + \\frac{2}{3}(-x)^{3/2}  x  0 \\\\ y  x=0 \\\\ y - i \\frac{2}{3}x^{3/2}  x  0 \\end{cases}\n\n\\begin{cases} y - \\frac{2}{3}(-x)^{3/2}  x  0 \\\\ y  x=0 \\\\ y + i \\frac{2}{3}x^{3/2}  x  0 \\end{cases}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Ultimately, the classification of a PDE has profound consequences for its numerical solution. This final hands-on practice bridges the gap between analytical theory and computational science by tasking you with constructing the discrete linear operators for canonical elliptic and hyperbolic problems. By analyzing the spectral properties of these matrices, you will establish a direct link between the PDE's type and the algebraic structure of the resulting system, which is the key to selecting an efficient iterative solver like the Conjugate Gradient (CG) or Generalized Minimal Residual (GMRES) method. ",
            "id": "3992007",
            "problem": "You are asked to construct and analyze discrete linear operators that reflect the spectral signatures of elliptic and hyperbolic steady-state Partial Differential Equations (PDEs) relevant to computational fusion science and engineering. Starting from first principles, use the definitions of PDE classification via the principal part and the mathematical properties of standard finite difference semi-discretizations to examine how eigenvalue distributions inform iterative solver choice. Your program must be a single, complete, runnable script that, for each test case provided, builds the matrix, computes spectral diagnostics, forms canonical iteration matrices, and outputs quantitative indicators.\n\nBegin from the following fundamental base:\n- For a second-order linear PDE in two dimensions with constant coefficients, the principal part can be written as $$\\mathcal{L} u = a_{11} \\frac{\\partial^2 u}{\\partial x^2} + 2 a_{12} \\frac{\\partial^2 u}{\\partial x \\partial y} + a_{22} \\frac{\\partial^2 u}{\\partial y^2},$$ with the classification determined by the definiteness of the symmetric matrix $$\\mathbf{A} = \\begin{pmatrix} a_{11}  a_{12} \\\\ a_{12}  a_{22} \\end{pmatrix}.$$ Elliptic type corresponds to $\\mathbf{A}$ positive definite, which for the isotropic diffusion operator reduces to coefficients $a_{11} = k_x$, $a_{22} = k_y$, $a_{12} = 0$ with $k_x > 0$ and $k_y > 0$.\n- For a first-order linear advection PDE in one dimension, the principal part is $$\\mathcal{H} u = a \\frac{\\partial u}{\\partial x},$$ which is hyperbolic and, under periodic boundary conditions with central differences, yields a discrete spatial operator that is skew-symmetric, indicating purely imaginary eigenvalues in the semi-discrete spectrum. To avoid singularity in the steady operator, a small real diagonal shift can be added to represent damping.\n- For a linear system $$\\mathbf{A} \\mathbf{u} = \\mathbf{b},$$ the Jacobi iteration matrix is $$\\mathbf{M}_{J} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A},$$ where $\\mathbf{D}$ is the diagonal of $\\mathbf{A}$. The Gauss–Seidel iteration matrix can be written as $$\\mathbf{M}_{GS} = -(\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{U},$$ where $\\mathbf{A} = \\mathbf{D} + \\mathbf{L} + \\mathbf{U}$ with $\\mathbf{L}$ strictly lower triangular and $\\mathbf{U}$ strictly upper triangular. Convergence of a stationary iteration is characterized by the spectral radius $$\\rho(\\mathbf{M})  1.$$\n- For symmetric positive definite (SPD) matrices, the Conjugate Gradient method is an optimal Krylov solver. For general nonsymmetric matrices, the Generalized Minimal Residual method (GMRES) is the canonical choice.\n\nTasks to implement for each test case:\n1. Construct the discrete operator matrix $\\mathbf{A}$ as prescribed by the case definition.\n2. Compute the full spectrum $\\{\\lambda_i\\}$ of $\\mathbf{A}$ and the fraction of eigenvalues with nonzero imaginary part (use a tolerance of $10^{-10}$).\n3. Determine whether $\\mathbf{A}$ is symmetric, and whether it is symmetric positive definite (SPD) by symmetry and by checking all eigenvalues are strictly positive (use a positivity tolerance of $10^{-12}$).\n4. Form the Jacobi and Gauss–Seidel iteration matrices $\\mathbf{M}_J$ and $\\mathbf{M}_{GS}$, and compute their spectral radii $\\rho(\\mathbf{M}_J)$ and $\\rho(\\mathbf{M}_{GS})$.\n5. Recommend a Krylov solver based on classification: output $0$ for Conjugate Gradient if SPD, else $1$ for Generalized Minimal Residual.\n\nDiscretizations to use:\n- Elliptic operator (anisotropic diffusion): consider the steady operator $$\\mathcal{L} u = -\\frac{\\partial}{\\partial x}\\left(k_x \\frac{\\partial u}{\\partial x}\\right) - \\frac{\\partial}{\\partial y}\\left(k_y \\frac{\\partial u}{\\partial y}\\right),$$ on a rectangular domain with homogeneous Dirichlet boundary conditions. Use a uniform grid with spacings $h_x$ and $h_y$, and approximate with the standard five-point stencil. For interior node $(i,j)$, the discrete operator has diagonal entry $$a_{ii} = \\frac{2 k_x}{h_x^2} + \\frac{2 k_y}{h_y^2},$$ and nearest-neighbor couplings $$-\\frac{k_x}{h_x^2}$$ in the $\\pm x$ directions and $$-\\frac{k_y}{h_y^2}$$ in the $\\pm y$ directions.\n- Hyperbolic operator (steady advection with periodic boundary conditions): consider $$\\mathcal{H} u = a \\frac{\\partial u}{\\partial x},$$ discretized with central differences on a uniform periodic grid of spacing $h$. The circulant matrix has off-diagonals $$+\\frac{a}{2h}$$ at the $+1$ shift and $$-\\frac{a}{2h}$$ at the $-1$ shift, with wrap-around. Add a small diagonal damping $$\\epsilon \\mathbf{I}$$ to avoid singularity.\n\nTest suite:\n- Case 1 (elliptic, isotropic): two-dimensional grid with $n_x = 4$, $n_y = 4$, $h_x = h_y = 1$, $k_x = 1$, $k_y = 1$.\n- Case 2 (hyperbolic, periodic central difference): one-dimensional periodic grid with $N = 31$, $h = 1/N$, advection speed $a = 1$, diagonal damping $\\epsilon = 10^{-2}$.\n- Case 3 (elliptic, strongly anisotropic): two-dimensional grid with $n_x = 3$, $n_y = 5$, $h_x = h_y = 1$, $k_x = 100$, $k_y = 1$.\n\nRequired outputs for each case as a list in the order: \n- a boolean indicating symmetry of $\\mathbf{A}$,\n- a boolean indicating SPD classification of $\\mathbf{A}$,\n- a float giving the fraction of eigenvalues with nonzero imaginary part,\n- a float for $\\rho(\\mathbf{M}_J)$,\n- a float for $\\rho(\\mathbf{M}_{GS})$,\n- an integer solver recommendation ($0$ for Conjugate Gradient, $1$ for Generalized Minimal Residual).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list described above for a test case. For example: $$[\\text{case1\\_list},\\text{case2\\_list},\\text{case3\\_list}]$$\n- No physical units are required; all quantities are dimensionless. Angles do not appear. Percentages should be expressed as decimal fractions.\n\nYour implementation must be fully self-contained and must not require any external input; use only the libraries specified in the execution environment.",
            "solution": "The problem requires a thorough analysis of the spectral properties of discrete linear operators derived from canonical elliptic and hyperbolic Partial Differential Equations (PDEs). The objective is to connect the theoretical classification of the PDE to the practical performance of iterative linear solvers by constructing the operators, analyzing their spectra, and evaluating the convergence characteristics of standard stationary iterative methods. This analysis will be conducted for three specific test cases.\n\nThe methodological approach for each case is as follows:\n1.  **Matrix Construction ($\\mathbf{A}$):** The discrete operator matrix $\\mathbf{A}$ is constructed based on the specified PDE, domain, boundary conditions, and finite difference scheme.\n2.  **Spectral Analysis:** The eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{A}$ are computed. These are used to determine:\n    - **Symmetry:** The matrix $\\mathbf{A}$ is tested for symmetry, i.e., $\\mathbf{A} = \\mathbf{A}^T$.\n    - **Spectrum Location:** The fraction of eigenvalues with a non-negligible imaginary part ($|\\text{Im}(\\lambda_i)| > 10^{-10}$) is calculated.\n    - **Positive Definiteness (SPD):** A matrix is classified as Symmetric Positive Definite (SPD) if it is symmetric and all its eigenvalues are strictly positive ($\\lambda_i > 10^{-12}$).\n3.  **Iterative Method Analysis:** The Jacobi ($\\mathbf{M}_J$) and Gauss-Seidel ($\\mathbf{M}_{GS}$) iteration matrices are formed. Their spectral radii, $\\rho(\\mathbf{M}_J)$ and $\\rho(\\mathbf{M}_{GS})$, are computed to assess their theoretical convergence rates. An iterative method is convergent if and only if the spectral radius of its iteration matrix is less than $1$.\n    - Jacobi Matrix: $\\mathbf{M}_{J} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}$\n    - Gauss-Seidel Matrix: $\\mathbf{M}_{GS} = -(\\mathbf{D} + \\mathbf{L})^{-1} \\mathbf{U}$, where $\\mathbf{A} = \\mathbf{D} + \\mathbf{L} + \\mathbf{U}$ is the decomposition of $\\mathbf{A}$ into its diagonal, strictly lower triangular, and strictly upper triangular parts, respectively.\n4.  **Krylov Solver Recommendation:** Based on the properties of $\\mathbf{A}$, a suitable Krylov subspace method is recommended. The Conjugate Gradient (CG) method is the optimal choice for SPD systems ($0$), while the Generalized Minimal Residual (GMRES) method is a robust choice for general, non-symmetric systems ($1$).\n\nThe analysis is performed on each test case as detailed below.\n\n**Case 1: Isotropic Elliptic Operator**\nThis case involves the operator $\\mathcal{L} u = -\\nabla \\cdot (k \\nabla u)$ with isotropic conductivity ($k_x = k_y = 1$) on a $4 \\times 4$ grid of interior points ($n_x=4$, $n_y=4$) with unit grid spacing ($h_x=h_y=1$). Homogeneous Dirichlet boundary conditions are applied. The problem is discretized using a five-point finite difference stencil. The total number of unknowns is $N = n_x \\times n_y = 16$. Using a lexicographical ordering of the grid points, the resulting $16 \\times 16$ matrix $\\mathbf{A}$ is a block tridiagonal matrix.\n-   The diagonal entries are $a_{k,k} = \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} = \\frac{2 \\cdot 1}{1^2} + \\frac{2 \\cdot 1}{1^2} = 4$.\n-   Off-diagonal entries corresponding to neighbors in the x- and y-directions are $-\\frac{k_x}{h_x^2} = -1$ and $-\\frac{k_y}{h_y^2} = -1$, respectively.\nThis discrete operator is a finite-dimensional representation of the negative Laplacian, which is a self-adjoint (symmetric) and positive definite operator. Consequently, the matrix $\\mathbf{A}$ is expected to be symmetric with real, positive eigenvalues. This qualifies it for the CG solver. The Jacobi and Gauss-Seidel methods are expected to converge, with $\\rho(\\mathbf{M}_{GS}) \\approx \\rho(\\mathbf{M}_J)^2$ as the matrix has 'Property A' and is consistently ordered.\n\n**Case 2: Hyperbolic Operator**\nThis case examines the steady-state advection equation $\\mathcal{H} u = a \\frac{\\partial u}{\\partial x}$ on a 1D periodic domain with $N=31$ grid points. The parameters are $a=1$, grid spacing $h=1/N$, and a small artificial damping $\\epsilon = 10^{-2}$. The operator is discretized using a central difference scheme, leading to a circulant matrix.\n- The diagonal entries are equal to the damping parameter, $\\epsilon = 10^{-2}$.\n- The entry on the first super-diagonal is $\\frac{a}{2h} = \\frac{1}{2(1/31)} = 15.5$.\n- The entry on the first sub-diagonal is $-\\frac{a}{2h} = -15.5$.\n- Periodicity introduces wrap-around entries: $A_{0,N-1} = -15.5$ and $A_{N-1,0} = 15.5$.\nThe pure central difference operator (without damping) is skew-symmetric ($\\mathbf{A}_{adv}^T = -\\mathbf{A}_{adv}$), resulting in purely imaginary eigenvalues. The addition of the diagonal term $\\epsilon\\mathbf{I}$ shifts the entire spectrum by $\\epsilon$ along the real axis. The resulting matrix $\\mathbf{A} = \\epsilon\\mathbf{I} + \\mathbf{A}_{adv}$ is not symmetric. Its eigenvalues will be complex (of the form $\\epsilon + i\\beta$), with the exception of one eigenvalue which will be purely real. The matrix is not SPD, thus requiring a general-purpose solver like GMRES. For stationary methods, the Jacobi iteration matrix is $\\mathbf{M}_J = \\mathbf{I} - \\mathbf{D}^{-1}\\mathbf{A} = \\mathbf{I} - (\\epsilon\\mathbf{I})^{-1}(\\epsilon\\mathbf{I} + \\mathbf{A}_{adv}) = -\\frac{1}{\\epsilon}\\mathbf{A}_{adv}$. Its spectral radius is $\\rho(\\mathbf{M}_J) = \\frac{1}{\\epsilon} \\rho(\\mathbf{A}_{adv}) = \\frac{1}{\\epsilon} \\frac{a}{h} \\sin(\\frac{\\pi(N-1)}{N}) \\approx \\frac{a}{\\epsilon h} = \\frac{1}{0.01 \\cdot (1/31)} = 3100$. This value is much greater than $1$, indicating rapid divergence.\n\n**Case 3: Strongly Anisotropic Elliptic Operator**\nThis case is similar to the first, but with strong anisotropy in the diffusion coefficients: $k_x = 100$ and $k_y = 1$. The grid of interior points is $n_x=3, n_y=5$, giving $N=15$ unknowns.\n-   The diagonal entries are $a_{k,k} = \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} = \\frac{2 \\cdot 100}{1^2} + \\frac{2 \\cdot 1}{1^2} = 202$.\n-   Off-diagonal entries are $-\\frac{k_x}{h_x^2} = -100$ (x-direction) and $-\\frac{k_y}{h_y^2} = -1$ (y-direction).\nLike Case 1, the underlying operator is self-adjoint and positive definite, so the matrix $\\mathbf{A}$ will be SPD with real, positive eigenvalues, making CG the appropriate solver. The strong anisotropy, however, typically leads to a more poorly conditioned matrix compared to the isotropic case. This is reflected in a spectral radius for Jacobi and Gauss-Seidel iterations that is closer to $1$, implying slower convergence for these stationary methods. For this consistently ordered matrix, we again expect $\\rho(\\mathbf{M}_{GS}) \\approx \\rho(\\mathbf{M}_J)^2$.\n\nThese theoretical expectations will be confirmed by the numerical implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef analyze_case(case_params):\n    \"\"\"\n    Constructs and analyzes the matrix for a given test case.\n    \"\"\"\n    case_type = case_params['type']\n    \n    if case_type == 'elliptic':\n        nx, ny = case_params['nx'], case_params['ny']\n        hx, hy = case_params['hx'], case_params['hy']\n        kx, ky = case_params['kx'], case_params['ky']\n        \n        N = nx * ny\n        A = np.zeros((N, N))\n        \n        val_diag = (2 * kx / hx**2) + (2 * ky / hy**2)\n        val_x = -kx / hx**2\n        val_y = -ky / hy**2\n        \n        for j in range(ny):\n            for i in range(nx):\n                k = i + j * nx\n                \n                # Diagonal entry\n                A[k, k] = val_diag\n                \n                # Off-diagonal entries (x-direction)\n                if i > 0:\n                    A[k, k - 1] = val_x\n                if i  nx - 1:\n                    A[k, k + 1] = val_x\n                \n                # Off-diagonal entries (y-direction)\n                if j > 0:\n                    A[k, k - nx] = val_y\n                if j  ny - 1:\n                    A[k, k + nx] = val_y\n\n    elif case_type == 'hyperbolic':\n        N = case_params['N']\n        h = case_params['h']\n        a = case_params['a']\n        eps = case_params['eps']\n        \n        A = np.zeros((N, N))\n        val_adv = a / (2 * h)\n        \n        for i in range(N):\n            A[i, i] = eps\n            A[i, (i + 1) % N] = val_adv\n            A[i, (i - 1 + N) % N] = -val_adv\n            \n    else:\n        raise ValueError(\"Unknown case type\")\n\n    # 1. Compute spectrum and related properties\n    eigvals = np.linalg.eigvals(A)\n    \n    # 2. Check symmetry\n    is_symmetric = np.allclose(A, A.T, atol=1e-12)\n    \n    # 3. Check for nonzero imaginary part\n    frac_imag = np.sum(np.abs(np.imag(eigvals)) > 1e-10) / len(eigvals)\n    \n    # 4. Check for SPD\n    # For a real matrix to be SPD, it must be symmetric and have all positive eigenvalues.\n    # If symmetric, eigenvalues are real.\n    is_spd = False\n    if is_symmetric:\n        if np.all(np.real(eigvals) > 1e-12):\n            is_spd = True\n\n    # 5. Form Jacobi and Gauss-Seidel matrices and compute spectral radii\n    D = np.diag(np.diag(A))\n    L = np.tril(A, k=-1)\n    U = np.triu(A, k=1)\n    \n    # Check if D is invertible (no zero diagonal elements)\n    if np.any(np.abs(np.diag(D))  1e-15):\n        rho_J = np.inf # Or handle as an error\n    else:\n        D_inv = np.linalg.inv(D)\n        Mj = np.identity(N) - D_inv @ A\n        eigvals_J = np.linalg.eigvals(Mj)\n        rho_J = np.max(np.abs(eigvals_J))\n\n    D_plus_L = D + L\n    if np.linalg.det(D_plus_L) == 0:\n         rho_GS = np.inf # Or handle as an error\n    else:\n        D_plus_L_inv = np.linalg.inv(D_plus_L)\n        Mgs = -D_plus_L_inv @ U\n        eigvals_GS = np.linalg.eigvals(Mgs)\n        rho_GS = np.max(np.abs(eigvals_GS))\n        \n    # 6. Recommend Krylov solver\n    solver_rec = 0 if is_spd else 1\n\n    return [is_symmetric, is_spd, float(frac_imag), float(rho_J), float(rho_GS), solver_rec]\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'type': 'elliptic', \n            'nx': 4, 'ny': 4, 'hx': 1.0, 'hy': 1.0, \n            'kx': 1.0, 'ky': 1.0\n        },\n        {\n            'type': 'hyperbolic',\n            'N': 31, 'h': 1.0/31.0, 'a': 1.0, 'eps': 1e-2\n        },\n        {\n            'type': 'elliptic',\n            'nx': 3, 'ny': 5, 'hx': 1.0, 'hy': 1.0, \n            'kx': 100.0, 'ky': 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = analyze_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of each inner list is generated automatically by map(str, ...).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}