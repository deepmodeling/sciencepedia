## 引言
流体动力学的控制方程，如[纳维-斯托克斯方程](@entry_id:142275)，其解析与数值求解一直是科学与工程计算中的核心挑战。近年来，[深度学习](@entry_id:142022)的兴起为这一经典领域带来了新的范式，其中，物理启发神经网络（PINN）尤为引人注目。它通过将物理定律直接编码到神经网络的[损失函数](@entry_id:634569)中，实现了一种新颖的、融[合数](@entry_id:263553)据与物理第一性原理的求解框架。然而，从理论概念到成功应用于复杂的流体问题，存在着一个显著的知识鸿沟：如何将抽象的[偏微分](@entry_id:194612)方程转化为可优化的目标？如何设计网络并有效训练，以确保解的物理一致性和准确性？本文旨在系统性地解答这些问题，为读者提供一份关于PINN在流体动力学中应用的深入指南。

本文的结构旨在引导读者逐步掌握PINN的核心思想与前沿应用。在**第一章“原理与机制”**中，我们将深入剖析PINN的基石，从如何将物理方程转化为损失函数，到[自动微分](@entry_id:144512)的计算核心，再到处理边界条件和提升训练稳定性的实用技巧。接着，**第二章“应用与交叉学科联系”**将展示PINN的强大能力，通过一系列案例探讨其在[复杂流体](@entry_id:198415)建模、多物理场耦合、[湍流](@entry_id:151300)与激[波模拟](@entry_id:176523)，以及解决逆问题等前沿领域的应用。最后，在**第三章“动手实践”**中，我们通过几个精心设计的练习，帮助读者将理论知识转化为解决实际问题的能力，加深对关键概念的理解。通过这三个章节，读者将建立起一个关于PINN在[流体动力](@entry_id:750449)学中从理论到实践的完整认知体系。

## 原理与机制

本章旨在深入阐述物理启发神经网络（PINN）在[流体动力](@entry_id:750449)学应用中的核心科学原理和关键计算机制。我们将从构建PINN的基础——将控制方程转化为可优化的[损失函数](@entry_id:634569)——出发，系统地探讨实现这一过程所需的技术细节，包括[网络架构](@entry_id:268981)的选择、[自动微分](@entry_id:144512)的运用、训练过程中的挑战及其对策。最后，我们将介绍一系列高级公式，以应对标准PINN在处理复杂流动问题时可能遇到的稳定性与准确性瓶颈。

### 从物理到[损失函数](@entry_id:634569)：强形式PINN

物理启发神经网络的核心思想是将流[体力](@entry_id:174230)学的基本定律，即[偏微分](@entry_id:194612)方程（PDEs），直接编码到神经网络的训练过程中。对于一个不可压缩[牛顿流体](@entry_id:263796)，其运动由[纳维-斯托克斯](@entry_id:276387)（Navier-Stokes）方程描述。这些方程源于[质量守恒](@entry_id:204015)和动量守恒两大基本物理原理。

对于密度 $\rho$ 和[运动粘度](@entry_id:275614) $\nu$ 恒定的流体，质量守恒简化为不可压缩性约束，即速度场 $\mathbf{u}$ 的散度为零：
$$
\nabla \cdot \mathbf{u} = 0
$$
[动量守恒](@entry_id:149964)方程，考虑了对流、压力梯度、粘性扩散和外力 $\mathbf{f}$ 的影响，其形式如下：
$$
\frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u} \cdot \nabla) \mathbf{u} = -\frac{1}{\rho}\nabla p + \nu \nabla^2 \mathbf{u} + \mathbf{f}
$$
这个方程组构成了所谓的 **[原始变量](@entry_id:753733)（primitive variable）** 公式，其基本未知量是流体的速度场 $\mathbf{u}(\mathbf{x}, t)$ 和压[力场](@entry_id:147325) $p(\mathbf{x}, t)$。因此，一个旨在求解这些方程的PINN，其最直接和通用的设计是构建一个神经网络，接收时空坐标 $(\mathbf{x}, t)$ 作为输入，并输出相应的速度和压力 $(\mathbf{u}, p)$。其他诸如[流函数](@entry_id:1132499)或涡量等变量的表示方法，虽然在特定维度（如二维）或特定问题中可能有效，但并不具备普遍性，而 $(\mathbf{u}, p)$ 的组合为任意维度 $d \ge 2$ 的问题提供了坚实的基础。

PINN的训练过程是通过最小化一个复合损失函数 $L(\boldsymbol{\theta})$ 来优化网络参数 $\boldsymbol{\theta}$。该[损失函数](@entry_id:634569)的核心部分是对控制方程 **残差（residual）** 的惩罚。我们将上述PDEs移项，得到 **强形式（strong-form）** 的残差表达式：
- 动量残差： $\mathbf{R}_{\text{mom}} = \frac{\partial \mathbf{u}}{\partial t} + (\mathbf{u} \cdot \nabla) \mathbf{u} + \frac{1}{\rho}\nabla p - \nu \nabla^2 \mathbf{u} - \mathbf{f}$
- 连续性（不可压缩）残差： $R_{\text{cont}} = \nabla \cdot \mathbf{u}$

一个理想的精确解将使这些残差在整个求解域内处处为零。PINN通过在求解域内部随机或结构化地选择一组 **配点（collocation points）**，并在这些点上计算由神经网络近似解 $(\mathbf{u}_{NN}, p_{NN})$ 产生的残差，然后最小化这些残差的均方误差（Mean Squared Error, MSE），从而“强迫”网络学习满足物理定律的函数。因此，物理损失项通常定义为：
$$
L_{\text{PDE}} = \frac{1}{N_c} \sum_{i=1}^{N_c} \left( \|\mathbf{R}_{\text{mom}}(\mathbf{x}_i, t_i)\|^2 + \lambda_c |R_{\text{cont}}(\mathbf{x}_i, t_i)|^2 \right)
$$
其中 $\{(\mathbf{x}_i, t_i)\}$ 是 $N_c$ 个内部配点，$\lambda_c$ 是一个用于平衡不同残差项的权重。这个过程的关键在于，所有出现在残差表达式中的[微分算子](@entry_id:140145)（如 $\nabla$, $\nabla^2$, $\frac{\partial}{\partial t}$）都必须作用于神经网络的输出。这引出了PINN的核心计算机制：自动微分。

### PINN的引擎：[自动微分](@entry_id:144512)与[网络架构](@entry_id:268981)

为了在配点上计算PDE残差，PINN必须能够精确计算其输出（如速度 $\mathbf{u}_{NN}$ 和压力 $p_{NN}$）关于其输入（时空坐标 $\mathbf{x}, t$）的任意阶导数。这一功能由 **自动微分（Automatic Differentiation, AD）** 实现。AD通过系统地应用链式法则，能够计算出复杂函数（如神经网络）的精确导数，其精度仅受限于机器[浮点数](@entry_id:173316)的精度。

然而，AD的有效性取决于神经网络本身的[可微性](@entry_id:140863)。一个神经网络的[可微性](@entry_id:140863)由其 **激活函数（activation function）** 的光滑度决定。[纳维-斯托克斯方程](@entry_id:142275)的动量残差中包含了粘性项 $\nu \nabla^2 \mathbf{u}$，这是一个二阶[微分算子](@entry_id:140145)（拉普拉斯算子）。为了使该项在经典意义上（逐点）有定义，神经网络对空间坐标的输出 $\mathbf{u}_{NN}$ 必须至少是二阶连续可微的（$C^2$）。

- **光滑激活函数**：诸如[双曲正切函数](@entry_id:634307)（$\tanh$）、[Sigmoid函数](@entry_id:137244)或正弦函数（$\sin$）等都是无穷次可微的（$C^\infty$）。使用这些激活函数的全连接网络，其本身也是一个 $C^\infty$ 函数，因此能够通过AD可靠地计算出任意阶导数，从而精确评估[纳维-斯托克斯方程](@entry_id:142275)的残差。例如，一个典型的架构是使用 $\tanh$ 作为所有隐藏层的[激活函数](@entry_id:141784)，并在线性输出层输出 $(\mathbf{u}, p)$。
- **非光滑[激活函数](@entry_id:141784)**：与之相对，[修正线性单元](@entry_id:636721)（ReLU），定义为 $\text{ReLU}(z) = \max(0, z)$，虽然应用广泛，但它在 $z=0$ 处不可微。其一阶导数是分段常数（亥维赛德[阶跃函数](@entry_id:159192)），而二阶导数在分布意义上是狄拉克$\delta$函数。因此，基于ReLU的网络是 $C^0$ 连续但非 $C^1$ 的，无法为包含二阶导数的PDE（如粘性项）提供有意义的逐点残差。

深入探究AD的计算机制，主要有两种模式：前向模式（forward-mode）和反向模式（reverse-mode）。
- **前向模式AD** 计算[雅可比-向量积](@entry_id:162748)（Jacobian-Vector Products, JVPs），即 $J(\mathbf{x})\mathbf{s}$，其中 $J$ 是函数关于输入的[雅可比矩阵](@entry_id:178326)，$\mathbf{s}$ 是一个“种子”向量。其计算成本约等于一次函数前向传播的成本。
- **反向模式AD**（即众所周知的[反向传播算法](@entry_id:198231)）计算向量-雅可比积（Vector-Jacobian Products, VJPs），即 $\mathbf{w}^\top J(\mathbf{x})$，其中 $\mathbf{w}$ 是一个种子向量。其计算成本也约等于一次前向传播的成本，但需要存储[计算图](@entry_id:636350)中的中间变量（激活值）。

对于流体力学中的常见算子，选择合适的AD模式可以显著提升计算效率。
- **对流项 $(\mathbf{u} \cdot \nabla)\mathbf{u}$**：该项可以写成[雅可比矩阵](@entry_id:178326)与速度向量自身的乘积 $J_{\mathbf{u}}(\mathbf{x})\mathbf{u}(\mathbf{x})$。利用前向模式AD，只需设置种子向量为速度向量 $\mathbf{s} = \mathbf{u}(\mathbf{x})$，通过一次JVP计算即可高效获得整个对流项，而无需显式构建整个[雅可比矩阵](@entry_id:178326)。
- **粘性项 $\nabla^2 \mathbf{u}$**：其分量 $(\nabla^2 \mathbf{u})_i = \text{Tr}(H_i(\mathbf{x}))$ 是速度分量 $u_i$ 的[海森矩阵](@entry_id:139140) $H_i$ 的迹。计算完整的[海森矩阵](@entry_id:139140)成本高昂。一种高效的替代方法是利用Hessian-向量积（HVPs）。通过 $d$ 次HVP计算（每次以一个[标准基向量](@entry_id:152417) $\mathbf{e}_j$ 为种子），可以逐个提取[海森矩阵](@entry_id:139140)的对角[线元](@entry_id:196833)素并求和，从而得到[拉普拉斯算子](@entry_id:146319)的值。这通常需要 $d$ [次梯度](@entry_id:142710)量级的计算。

理解这些计算细节对于设计和调试高效的PINN模型至关重要。

### 训练的艺术：平衡[损失函数](@entry_id:634569)

PINN的训练本质上是一个[多目标优化](@entry_id:637420)问题，需要同时最小化PDE残差、边界条件残差和数据残差等多个分量。这些分量的量级可能存在巨大差异，如果不加以处理，[梯度下降](@entry_id:145942)过程可能会被某个量级最大的损失项主导，而忽略其他同样重要的物理约束，导致训练失败或收敛缓慢。因此，对[损失函数](@entry_id:634569)的各项进行恰当的 **加权（weighting）** 是训练成功的关键。

一个根本性的策略是首先对问题进行 **无量纲化（nondimensionalization）**。通过引入特征长度 $L$、[特征速度](@entry_id:165394) $U$ 和特征压力 $\rho U^2$，我们可以将有量纲的变量 $(\mathbf{x}, t, \mathbf{u}, p)$ 转换为无量纲变量 $(\hat{\mathbf{x}}, \hat{t}, \hat{\mathbf{u}}, \hat{p})$。
$$
\mathbf{x} = L\hat{\mathbf{x}}, \quad t = \frac{L}{U}\hat{t}, \quad \mathbf{u} = U\hat{\mathbf{u}}, \quad p = \rho U^2 \hat{p}
$$
将这些关系代入[纳维-斯托克斯方程](@entry_id:142275)，经过整理，得到无量纲形式的[动量方程](@entry_id:197225)：
$$
\frac{\partial \hat{\mathbf{u}}}{\partial \hat{t}} + (\hat{\mathbf{u}}\cdot\hat{\nabla})\hat{\mathbf{u}} = -\hat{\nabla}\hat{p} + \frac{1}{\text{Re}}\hat{\nabla}^2 \hat{\mathbf{u}}
$$
其中出现了一个唯一的[无量纲参数](@entry_id:169335)——**雷诺数（Reynolds number）** $\text{Re} = \frac{UL}{\nu}$，它代表了惯性力与[粘性力](@entry_id:263294)的比值。

无量纲化使得方程中的所有项都具有相似的量级（假设无量纲变量及其导数均为 $\mathcal{O}(1)$），但雷诺数的存在引入了新的挑战。在[高雷诺数流](@entry_id:1126089)动中（$\text{Re} \gg 1$），粘性项的系数 $\frac{1}{\text{Re}}$ 会变得非常小。在PINN的损失函数中，这意味着粘性项对总梯度的贡献可能会消失，导致网络无法正确学习粘性效应。

为了解决这个问题，需要对损失项进行动态或静态的加权。一个有原则的加权方法是，在训练初期，调整权重使得每个损失项对总梯度的贡献处于同一数量级。一种实用的策略是：
1.  首先对所有残差进行[无量纲化](@entry_id:136704)，使其成为量纲为一的量。
2.  在训练开始的几个迭代步中，经验性地计算每个无量纲残差项的均方值 $\langle \|r^*\|^2 \rangle$。
3.  设置每个损失项的权重 $\lambda_i$ 与其对应的经验均方残差成反比，即 $\lambda_i \propto 1 / \langle \|r_i^*\|^2 \rangle$。

通过这种方式，初始量级较大的残差项（表明约束违反得更严重）会被赋予较小的权重，而量级较小的残差项则被赋予较大的权重，从而平衡它们在优化初期的“话语权”。对于[高雷诺数流](@entry_id:1126089)动中的粘性项，其系数 $\frac{1}{\text{Re}}$ 使得其初始残差可能很小。根据上述平衡原则，可以引入一个与雷诺数相关的权重。例如，为了使粘性项的贡献与其他 $\mathcal{O}(1)$ 的项（如对流项）相匹配，其权重应与 $\text{Re}$ 成正比。

### 边界与约束的处理

除了满足域内的控制方程，流体问题的解还必须满足边界条件。在PINN框架中，施加边界条件同样有两种主流方法：软约束和硬约束。

#### 边界条件的施加：硬约束与软约束

**软约束（Weak Enforcement）**，也称为[罚函数法](@entry_id:636090)，是最通用和直接的方法。它将边界条件的残差作为一个额外的损失项加入到总[损失函数](@entry_id:634569)中。例如，对于狄利克雷（Dirichlet）边界条件 $\mathbf{u} = \mathbf{u}_b$，其损失项可以定义为：
$$
L_{\text{BC}} = \frac{1}{N_b} \sum_{i=1}^{N_b} \|\mathbf{u}_{NN}(\mathbf{x}_i, t_i) - \mathbf{u}_b(\mathbf{x}_i, t_i)\|^2
$$
其中 $\{(\mathbf{x}_i, t_i)\}$ 是边界上的 $N_b$ 个采样点。这种方法的优点在于其通用性——它可以应用于任何类型的边界条件（包括复杂的诺伊曼（Neumann）或应力边界条件）和任何复杂的几何形状。其主要缺点是引入了额外的超参数（边界损失的权重），这些权重需要仔细调整以平衡PDE损失和BC损失。

**硬约束（Hard Enforcement）**，也称作构造法（ansatz method），通过特殊设计网络输出来精确地、自动地满足边界条件。例如，对于在边界 $\partial \Omega_D$ 上满足 $\mathbf{u} = \mathbf{u}_b$ 的[狄利克雷条件](@entry_id:137096)，可以构造如下形式的试探解：
$$
\tilde{\mathbf{u}}(\mathbf{x}, t) = \mathbf{u}_b(\mathbf{x}, t) + \mathcal{B}(\mathbf{x}) \mathcal{N}_\theta(\mathbf{x}, t)
$$
其中 $\mathcal{N}_\theta$ 是一个标准的神经网络，而 $\mathcal{B}(\mathbf{x})$ 是一个已知的“[包络函数](@entry_id:749028)”，其性质是在边界 $\partial \Omega_D$ 上为零（例如，可以使用到边界的光滑距离函数来构造）。这样，无论网络 $\mathcal{N}_\theta$ 的输出是什么，$\tilde{\mathbf{u}}$ 总能精确满足边界条件。这种方法的优点是无需调整边界损失权重，且减少了[可行解](@entry_id:634783)的搜索空间。然而，它的缺点也很明显：
1.  **几何限制**：对于有尖角或复杂拓扑的区域，构造一个足够光滑的[包络函数](@entry_id:749028) $\mathcal{B}(\mathbf{x})$ 非常困难。$\mathcal{B}(\mathbf{x})$ 的不光滑性会传递给导数，污染PDE残差的计算。
2.  **边界条件类型限制**：硬约束主要适用于[狄利克雷条件](@entry_id:137096)。对于依赖于解的梯度（如压力 $p$ 或[应力张量](@entry_id:148973) $\boldsymbol{\tau}$）的应力边界条件，构造一个先验满足该条件的ansatz几乎是不可能的。
3.  **噪声数据**：如果边界数据 $\mathbf{u}_b$ 本身含有噪声，硬约束会强迫模型精确拟合这些噪声，导致[过拟合](@entry_id:139093)和非物理的梯度。

因此，在实践中，对于具有简单几何和精确狄利克雷条件的问题，硬约束是首选。而在处理复杂几何、复杂边界条件或含噪声数据时，更具鲁棒性的软约[束方法](@entry_id:636307)则更为可取。

#### 梯度病理与训练稳定性

在使用软约束时，训练稳定性成为一个重要议题。PINN的复合[损失函数](@entry_id:634569) $L = L_{\text{PDE}} + L_{\text{BC}}$ 可能会导致 **梯度病理（gradient pathologies）**。特别地，当[边界点](@entry_id:176493)在批次中采样过多，或者边界损失项的权重设置不当，来自边界损失的梯度 $\nabla_{\boldsymbol{\theta}}L_{\text{BC}}$ 的量级可能远大于来自内部PDE损失的梯度 $\nabla_{\boldsymbol{\theta}}L_{\text{PDE}}$。这种梯度不平衡会导致优化器“只见边界，不见内部”，使得参数更新方向被边界信息主导，导致训练不稳定，甚至损失激增。

一个有效的应对策略是 **[梯度裁剪](@entry_id:634808)（gradient clipping）**。相比于对总梯度进行全局裁剪，**分项裁剪（per-term clipping）** 是一种更精细的控制方法。它分别对每个损失项的梯度设置一个范数上限 $\tau_i$：
$$
(\nabla_{\boldsymbol{\theta}}L_i)_{\text{clip}} = (\nabla_{\boldsymbol{\theta}}L_i) \cdot \min\left(1, \frac{\tau_i}{\|\nabla_{\boldsymbol{\theta}}L_i\|}\right)
$$
阈值 $\tau_i$ 的选择应基于对梯度[统计分布](@entry_id:182030)的观察。理想的阈值应高于该项梯度的典型值（例如，95百分位数），以避免影响正常的学习信号，但又能有效截断偶尔出现的极端梯度尖峰。例如，如果经验观察到边界梯度的均值约为 $18$，而内部梯度的均值约为 $2.5$，为了防止边界梯度主导（例如，控制其影响不超过内部梯度的10倍），可以设置边界裁剪阈值 $\tau_{\text{BC}} \leq 10 \times 2.5 = 25$，同时为了不影响正常梯度，可设 $\tau_{\text{BC}} > 15$（[中位数](@entry_id:264877)）。一个合理的选择可能是 $\tau_{\text{BC}} = 25$ 和 $\tau_{\text{PDE}} = 5$（高于内部梯度的95百分位数4.2），这既能控制不平衡，又能保护有效的学习信号。

### 高级公式与稳定性

标准的强形式PINN虽然概念简单，但在处理某些具有挑战性的流体问题时会遇到困难。为此，研究者们从经典的有限元方法（FEM）等数值方法中汲取灵感，发展了一系列高级公式以提升PINN的稳定性和准确性。

#### 弱形式PINN：放宽[可微性](@entry_id:140863)要求

强形式PINN要求网络至少是 $C^2$ 可微的，这限制了[网络架构](@entry_id:268981)的选择。一个替代方案是采用 **弱形式（weak-formulation）** 或[变分形式](@entry_id:166033)的PINN。弱形式是通过将PDE乘以一个 **测试函数（test function）** 并在求解域上积分得到的。通过分部积分，可以将[高阶导数](@entry_id:140882)转移到[测试函数](@entry_id:166589)上，从而降低对解函数（即神经网络）的[光滑性](@entry_id:634843)要求。

例如，对于粘性项，其弱形式积分为：
$$
-\int_{\Omega} (\mu \nabla^2 \mathbf{u}) \cdot \mathbf{v} \, d\Omega = \int_{\Omega} \mu \nabla \mathbf{u} : \nabla \mathbf{v} \, d\Omega - \int_{\partial\Omega} \mu (\nabla \mathbf{u} \cdot \mathbf{n}) \cdot \mathbf{v} \, dS
$$
其中 $\mathbf{v}$ 是测试函数。可以看到，积分后的表达式中只包含 $\mathbf{u}$ 的一阶导数。这意味着，神经网络只需要是 $H^1$ 函数（即函数本身及其[一阶导数](@entry_id:749425)是平方可积的），这是一个比 $C^2$ 弱得多的条件。此外，[弱形式](@entry_id:142897)还有两个显著优点：
1.  **自然边界条件**：[分部积分](@entry_id:136350)产生的边界积分项，如上式的 $\int_{\partial\Omega} \dots dS$，可以自然地用于施加诺伊曼（应力）边界条件，而无需将其作为额外的[罚函数](@entry_id:638029)项。
2.  **全局守恒**：通过选择特定的测试函数（如[常数函数](@entry_id:152060)），[弱形式](@entry_id:142897)可以更容易地强制执行全局守恒定律（如总质量守恒），而强形式PINN仅在离散的配点上满足方程，可能导致积分意义上的守恒律被违反。

[弱形式](@entry_id:142897)PINN的缺点在于需要计算积分，这通常通过[高斯求积](@entry_id:146011)或[蒙特卡洛积分](@entry_id:141042)实现，引入了额外的计算成本和近似误差。

#### 压力-速度耦合与稳定性

在求解[不可压缩纳维-斯托克斯](@entry_id:750595)方程时，一个经典的数值挑战来自于压力-速度的耦合，这在代数上表现为一个[鞍点问题](@entry_id:174221)。在有限元方法中，如果对速度和压力的[插值函数](@entry_id:262791)空间选择不当（例如，采用等阶插值），就会违反一个称为 **LBB（Ladyzhenskaya–Babuška–Brezzi）** 的稳定性条件，导致压力的[伪振荡](@entry_id:152404)。

在PINN中，当使用具有同等[表达能力](@entry_id:149863)（例如，相同的层数和神经元数）的神经网络来近似速度和压力时，也可能出现类似的LBB不稳定性问题。为了解决这个问题，可以借鉴FEM中的 **压力稳定化[Petrov-Galerkin](@entry_id:174072)（PSPG）** 方法。其核心思想是在[连续性方程](@entry_id:195013)的残差中引入一个与[动量方程](@entry_id:197225)[残差相关](@entry_id:754268)的稳定项。在PINN的强形式框架下，这可以转化为一个稳定化的连续性残差：
$$
r_c^{\text{stab}} := \nabla \cdot \mathbf{u} - \nabla \cdot (\tau \mathbf{R}_m)
$$
其中 $\mathbf{R}_m$ 是[动量方程](@entry_id:197225)的残差，$\tau > 0$ 是一个稳定化参数，它的大小通常与局部网格尺度和流体粘性有关。在PINN的损失函数中加入对 $\|r_c^{\text{stab}}\|^2$ 的惩罚，可以在不破坏[原始方程](@entry_id:1130162)一致性的前提下，增强压力和速度场之间的耦合，从而抑制压力的非物理振荡。

#### 谱偏差与多尺度问题

标准的基于梯度优化的全连接神经网络存在 **谱偏差（spectral bias）**，即它们倾向于首先学习[目标函数](@entry_id:267263)的低频分量，而对高频分量的学习则非常缓慢。这对于流体力学问题是一个巨大的挑战，因为许多流动（如[湍流](@entry_id:151300)或高雷诺数下的边界层）都包含丰富的[多尺度结构](@entry_id:752336)和高频特征。

例如，在[平板边界层](@entry_id:749449)问题中，壁面法向的速度梯度非常剧烈，其变化尺度与边界层厚度 $\delta = \mathcal{O}(\text{Re}^{-1/2})$ 相当，对应于法向的高波数（频率）分量 $k_y = \mathcal{O}(\text{Re}^{1/2})$。一个标准的PINN会很难捕捉到这种高频变化。

为了缓解谱偏差，一种有效的方法是采用 **傅里叶特征映射（Fourier Feature Mapping）**。该方法通过一个固定的[非线性映射](@entry_id:272931) $\gamma(\mathbf{x})$ 将原始的低维输入坐标 $\mathbf{x}$ 提升到一个高维的[特征空间](@entry_id:638014)，这个空间包含了输入坐标不同频率的正弦和余弦变换：
$$
\gamma(\mathbf{x}) = [\mathbf{x}, \cos(2\pi B \mathbf{x}), \sin(2\pi B \mathbf{x})]
$$
其中 $B$ 是一个包含了多个频率尺度的矩阵。通过向网络提供这些高频特征作为输入，可以使其更容易地拟合高频函数。对于具有各向异性特征的流动（如边界层），傅里叶特征的设计也应是 **各向异性（anisotropic）** 的：在梯度平缓的方向（如流向）使用较低的频率，而在梯度剧烈的方向（如法向）使用较高的频率，以[匹配问题](@entry_id:275163)的物理尺度。这种针对性的特征工程是解决PINN在多尺度[流体仿真](@entry_id:138114)中应用瓶颈的关键技术之一。