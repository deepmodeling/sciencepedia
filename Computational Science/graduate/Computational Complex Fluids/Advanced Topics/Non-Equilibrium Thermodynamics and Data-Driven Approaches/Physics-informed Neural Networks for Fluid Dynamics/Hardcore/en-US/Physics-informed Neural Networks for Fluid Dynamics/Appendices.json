{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in training Physics-Informed Neural Networks is properly balancing the different terms in the loss function, such as the momentum and mass conservation residuals. This exercise  guides you through a foundational technique using dimensional analysis to derive a static scaling factor. By ensuring the different physical constraints contribute commensurately to the loss, you can establish a more stable and effective training baseline from the very beginning.",
            "id": "4099871",
            "problem": "Consider a two-dimensional, incompressible, Newtonian fluid with density $\\rho$ and dynamic viscosity $\\mu$, evolving on a bounded spatial domain $\\Omega \\subset \\mathbb{R}^{2}$ over a time interval $[0,T]$. The primary fields are velocity $\\mathbf{u}(\\mathbf{x},t)$ and pressure $p(\\mathbf{x},t)$. The governing equations are the incompressible Navier–Stokes equations,\n$$\n\\rho\\left(\\frac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u}\\cdot\\nabla \\mathbf{u}\\right) = -\\nabla p + \\mu \\nabla^{2}\\mathbf{u} + \\mathbf{f}, \\qquad \\nabla\\cdot\\mathbf{u} = 0,\n$$\nwhere $\\mathbf{f}$ is a known body force density. A Physics-Informed Neural Network (PINN) is trained to approximate $(\\mathbf{u},p)$ by minimizing a physics loss that consists of two contributions: a momentum residual penalty and a mass-conservation (divergence) penalty. Define the pointwise momentum residual $\\mathbf{r}_{m}(\\mathbf{x},t)$ and continuity residual $r_{c}(\\mathbf{x},t)$ by\n$$\n\\mathbf{r}_{m} := \\rho\\left(\\frac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u}\\cdot\\nabla \\mathbf{u}\\right) + \\nabla p - \\mu \\nabla^{2}\\mathbf{u} - \\mathbf{f}, \\qquad r_{c} := \\nabla\\cdot\\mathbf{u}.\n$$\nThe physics loss takes the form\n$$\n\\mathcal{L}_{\\text{phys}} = \\lambda_{m}\\,\\mathbb{E}\\!\\left[\\,\\|\\mathbf{r}_{m}\\|^{2}\\,\\right] + \\lambda_{c}\\,\\mathbb{E}\\!\\left[\\,|r_{c}|^{2}\\,\\right],\n$$\nwhere $\\mathbb{E}[\\cdot]$ denotes an average over collocation points in $\\Omega\\times[0,T]$, and $\\lambda_{m}>0$ and $\\lambda_{c}>0$ are weights. The divergence penalty is thereby constructed as the squared $L^{2}$-norm of $r_{c}$ over the training distribution.\n\nTo avoid over-suppressing velocity dynamics during training, one seeks a scaling of the divergence penalty relative to the momentum residual that balances their contributions at the level of characteristic magnitudes. Let $U$ and $L$ be characteristic velocity and length scales for the flow, respectively. Using only the fundamental equations above and standard dimensional analysis, derive a closed-form expression for the relative scaling\n$$\ns_{\\mathrm{rel}} \\equiv \\frac{\\lambda_{c}}{\\lambda_{m}}\n$$\nthat equalizes the characteristic magnitudes of the two squared residual terms, evaluated on fields with characteristic scales $U$ and $L$. Express your final answer for $s_{\\mathrm{rel}}$ in terms of $\\rho$ and $U$, using the International System of Units (SI). Provide the final expression only; do not substitute numerical values.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is based on the established principles of fluid dynamics, dimensional analysis, and the contemporary research area of physics-informed machine learning. The request is to derive a scaling factor using standard physical reasoning, which is a valid and formalizable task. No flaws are identified.\n\nThe objective is to determine the relative scaling factor $s_{\\mathrm{rel}} \\equiv \\frac{\\lambda_{c}}{\\lambda_{m}}$ such that the two weighted penalty terms in the physics loss function, $\\lambda_{m}\\,\\mathbb{E}\\!\\left[\\,\\|\\mathbf{r}_{m}\\|^{2}\\,\\right]$ and $\\lambda_{c}\\,\\mathbb{E}\\!\\left[\\,|r_{c}|^{2}\\,\\right]$, have the same characteristic magnitude. This condition can be expressed as:\n$$\n\\lambda_{m} [\\|\\mathbf{r}_{m}\\|^{2}] \\sim \\lambda_{c} [|r_{c}|^{2}]\n$$\nwhere $[\\cdot]$ denotes the characteristic magnitude of a quantity. The problem is thus to find the scaling that balances the contributions of the momentum and continuity residuals based on the characteristic scales of the flow.\n\nRearranging the equivalence, we obtain an expression for the desired ratio:\n$$\ns_{\\mathrm{rel}} = \\frac{\\lambda_{c}}{\\lambda_{m}} \\sim \\frac{[\\|\\mathbf{r}_{m}\\|^{2}]}{[|r_{c}|^{2}]} = \\frac{[\\mathbf{r}_{m}]^{2}}{[r_{c}]^{2}}\n$$\nTo evaluate this ratio, we must perform a dimensional analysis of the pointwise residuals $\\mathbf{r}_{m}$ and $r_{c}$ using the provided characteristic velocity scale $U$ and length scale $L$.\n\nFirst, we analyze the continuity residual, $r_{c}$. It is defined as the divergence of the velocity field:\n$$\nr_{c} := \\nabla\\cdot\\mathbf{u}\n$$\nThe gradient operator $\\nabla$ involves spatial derivatives, so its characteristic scale is the inverse of the characteristic length scale, $[ \\nabla ] \\sim L^{-1}$. The velocity field $\\mathbf{u}$ has a characteristic magnitude of $U$. Therefore, the characteristic magnitude of the continuity residual is:\n$$\n[r_{c}] \\sim [ \\nabla ] \\cdot [\\mathbf{u}] \\sim \\frac{1}{L} \\cdot U = \\frac{U}{L}\n$$\n\nNext, we analyze the momentum residual, $\\mathbf{r}_{m}$, defined as:\n$$\n\\mathbf{r}_{m} := \\rho\\left(\\frac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u}\\cdot\\nabla \\mathbf{u}\\right) + \\nabla p - \\mu \\nabla^{2}\\mathbf{u} - \\mathbf{f}\n$$\nThe magnitude of $\\mathbf{r}_{m}$ is determined by the magnitudes of the terms in the momentum equation. In a well-posed physical problem, all terms must have the same physical units (force per unit volume). The characteristic magnitude of the residual is set by the dominant physical effects. For many fluid flows, particularly at moderate to high Reynolds numbers, the inertial terms are dominant or are used as the reference scale. Let us analyze the inertial terms: the local acceleration term $\\rho \\frac{\\partial \\mathbf{u}}{\\partial t}$ and the convective (advective) acceleration term $\\rho(\\mathbf{u}\\cdot\\nabla \\mathbf{u})$.\n\nThe characteristic time scale, $t_{char}$, can be related to the length and velocity scales. The advective time scale, which is the time it takes for a fluid particle to travel a characteristic distance $L$ at a characteristic velocity $U$, is $t_{char} \\sim L/U$.\n\nUsing this, the magnitude of the local acceleration term is:\n$$\n\\left[\\rho \\frac{\\partial \\mathbf{u}}{\\partial t}\\right] \\sim \\frac{[\\rho][\\mathbf{u}]}{[t]} \\sim \\frac{\\rho U}{L/U} = \\rho \\frac{U^{2}}{L}\n$$\nThe magnitude of the convective acceleration term is:\n$$\n[\\rho(\\mathbf{u}\\cdot\\nabla \\mathbf{u})] \\sim [\\rho][\\mathbf{u}][\\nabla][\\mathbf{u}] \\sim \\rho \\cdot U \\cdot \\frac{1}{L} \\cdot U = \\rho \\frac{U^{2}}{L}\n$$\nBoth inertial terms have the same characteristic magnitude, $\\rho U^{2}/L$. This magnitude represents the inertial force per unit volume. The pressure gradient term, $[\\nabla p]$, and the viscous term, $[\\mu \\nabla^2 \\mathbf{u}]$, are expected to scale in a way that balances the inertial terms. Specifically, the dynamic pressure scales as $p \\sim \\rho U^2$, leading to $[\\nabla p] \\sim \\rho U^2/L$. Thus, it is appropriate to set the characteristic magnitude of the entire momentum residual to that of the inertial terms:\n$$\n[\\mathbf{r}_{m}] \\sim \\rho \\frac{U^{2}}{L}\n$$\n\nNow we can substitute the characteristic magnitudes of $[r_{c}]$ and $[\\mathbf{r}_{m}]$ into the expression for $s_{\\mathrm{rel}}$:\n$$\ns_{\\mathrm{rel}} \\sim \\frac{[\\mathbf{r}_{m}]^{2}}{[r_{c}]^{2}} \\sim \\frac{\\left(\\rho \\frac{U^{2}}{L}\\right)^{2}}{\\left(\\frac{U}{L}\\right)^{2}}\n$$\nSimplifying this expression yields:\n$$\ns_{\\mathrm{rel}} \\sim \\frac{\\rho^{2} \\frac{U^{4}}{L^{2}}}{\\frac{U^{2}}{L^{2}}} = \\rho^{2} U^{2}\n$$\nThe derived scaling factor $s_{\\mathrm{rel}}$ is expressed solely in terms of the fluid density $\\rho$ and the characteristic velocity $U$, as required. A dimensional check confirms consistency. The units of $\\lambda_c/\\lambda_m$ must be the units of $\\|\\mathbf{r}_m\\|^2 / |r_c|^2$. In SI units, this is $(\\text{kg} \\cdot \\text{m}^{-2} \\cdot \\text{s}^{-2})^2 / (\\text{s}^{-1})^2 = \\text{kg}^2 \\cdot \\text{m}^{-4} \\cdot \\text{s}^{-2}$. The units of $\\rho^2 U^2$ are $(\\text{kg} \\cdot \\text{m}^{-3})^2 \\cdot (\\text{m} \\cdot \\text{s}^{-1})^2 = (\\text{kg}^2 \\cdot \\text{m}^{-6}) \\cdot (\\text{m}^2 \\cdot \\text{s}^{-2}) = \\text{kg}^2 \\cdot \\text{m}^{-4} \\cdot \\text{s}^{-2}$. The units match, confirming the correctness of the derivation.",
            "answer": "$$\\boxed{\\rho^{2} U^{2}}$$"
        },
        {
            "introduction": "The success of a PINN is not just in its loss function, but also in its very architecture; specifically, the choice of activation function is critical. The viscous term in the Navier-Stokes equations, involving a second-order Laplacian $\\nabla^2 \\mathbf{u}$, requires the network's output to be sufficiently smooth. This practice  challenges you to analyze why popular activations like ReLU fail for this task and to identify smoother alternatives that ensure all physical terms in the governing equations are mathematically well-defined.",
            "id": "4099921",
            "problem": "A Physics-Informed Neural Network (PINN) with parameters $\\theta$ is trained to approximate the velocity field $\\mathbf{u}_\\theta(\\mathbf{x},t)$ and pressure $p_\\theta(\\mathbf{x},t)$ for an incompressible Newtonian fluid in a domain $\\Omega \\subset \\mathbb{R}^2$. The governing equations are the incompressible Navier–Stokes equations, which state the balance of momentum and mass conservation: $$\\rho\\left(\\partial_t \\mathbf{u} + (\\mathbf{u}\\cdot \\nabla)\\mathbf{u}\\right) = -\\nabla p + \\mu \\nabla^2 \\mathbf{u} + \\mathbf{f}, \\quad \\nabla \\cdot \\mathbf{u} = 0,$$ where $\\rho$ is density, $\\mu$ is dynamic viscosity, and $\\mathbf{f}$ is body force. In the PINN loss, the residual of the viscous term requires evaluating $\\nabla^2 \\mathbf{u}_\\theta$ pointwise by Automatic Differentiation (AD). The network uses Rectified Linear Unit (ReLU) activations at all hidden layers. During training, the AD-computed second derivatives of $\\mathbf{u}_\\theta$ are observed to be numerically zero at almost all collocation points, and unstable near apparent “kink” sets, leading to a misleadingly small viscous residual and poor convergence.\n\nFrom first principles of differentiability and the definition of classical derivatives, analyze why AD-computed second derivatives of a ReLU network can be ill-defined for the pointwise Laplacian residual required by the viscous term, and identify an activation function replacement that ensures the residuals involving $\\nabla^2 \\mathbf{u}_\\theta$ are mathematically meaningful in the strong, pointwise sense sought by PINNs.\n\nSelect the option that most accurately explains this phenomenon and proposes a valid remedy:\n\nA. AD in modern frameworks computes second derivatives of ReLU activations as distributions that include singular measures at kink locations, so $\\nabla^2 \\mathbf{u}_\\theta$ is accurately captured almost everywhere; therefore, retaining ReLU does not impair viscous residual evaluation.\n\nB. Because ReLU networks produce piecewise-linear outputs, any composition of ReLU activations is twice differentiable everywhere, so the viscous term $\\mu \\nabla^2 \\mathbf{u}_\\theta$ vanishes identically and this speeds up training without harming accuracy.\n\nC. The classical second derivative of the ReLU function $r(x)=\\max(0,x)$ is $0$ for $x\\neq 0$ and is undefined at $x=0$, and compositions of ReLU yield outputs that are piecewise linear with kinks on sets where pre-activations cross $0$. AD in standard PINN implementations returns $0$ almost everywhere and ignores singularities, making pointwise $\\nabla^2 \\mathbf{u}_\\theta$ ill-defined for the viscous residual. A remedy is to replace ReLU with a twice continuously differentiable activation such as $\\tanh(x)$ or $\\mathrm{softplus}_\\beta(x)=\\beta^{-1}\\ln\\!\\big(1+e^{\\beta x}\\big)$, which are $C^\\infty$ and ensure well-defined second derivatives.\n\nD. One can retain ReLU and instead compute $\\nabla^2 \\mathbf{u}_\\theta$ by finite-difference stencils on a grid of collocation points; this produces accurate second derivatives and maintains end-to-end AD for PINN training.\n\nE. Switching to leaky ReLU $r_\\alpha(x)=\\max(\\alpha x,x)$ with $\\alpha\\in(0,1)$ fixes the problem because the function is differentiable everywhere and has nonzero curvature; thus pointwise second derivatives required by the viscous residual are well-defined.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Model:** A Physics-Informed Neural Network (PINN) with parameters $\\theta$.\n- **Approximated Quantities:** Velocity field $\\mathbf{u}_\\theta(\\mathbf{x},t)$ and pressure $p_\\theta(\\mathbf{x},t)$.\n- **Physical System:** An incompressible Newtonian fluid in a domain $\\Omega \\subset \\mathbb{R}^2$.\n- **Governing Equations (Incompressible Navier–Stokes):**\n  - Momentum: $\\rho\\left(\\partial_t \\mathbf{u} + (\\mathbf{u}\\cdot \\nabla)\\mathbf{u}\\right) = -\\nabla p + \\mu \\nabla^2 \\mathbf{u} + \\mathbf{f}$\n  - Mass Conservation: $\\nabla \\cdot \\mathbf{u} = 0$\n- **Parameters:** $\\rho$ (density), $\\mu$ (dynamic viscosity), $\\mathbf{f}$ (body force).\n- **PINN Implementation Detail:** The residual of the viscous term, which involves $\\nabla^2 \\mathbf{u}_\\theta$, is evaluated pointwise using Automatic Differentiation (AD).\n- **Network Architecture:** Rectified Linear Unit (ReLU) activations are used in all hidden layers.\n- **Observed Phenomenon:** During training, the AD-computed second derivatives of $\\mathbf{u}_\\theta$ are numerically zero at almost all collocation points. They are unstable near \"kink\" sets.\n- **Consequence:** The viscous residual is misleadingly small, leading to poor convergence.\n- **Question:** Analyze why AD-computed second derivatives of a ReLU network are ill-defined for the pointwise Laplacian residual and identify a suitable replacement activation function to ensure the residuals are mathematically meaningful in a strong, pointwise sense.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the established criteria.\n\n- **Scientifically Grounded:** The problem is firmly grounded in the fields of computational fluid dynamics and scientific machine learning. The Navier-Stokes equations are fundamental to fluid mechanics. Physics-Informed Neural Networks (PINNs) are a well-established and researched method for solving differential equations. The core of the question, concerning the differentiability of neural network architectures and its implication for computing PDE residuals, is a critical and non-trivial aspect of implementing PINNs for second-order PDEs. The phenomenon described is real and frequently encountered by practitioners. The problem is scientifically sound.\n- **Well-Posed:** The problem provides a clear context, a specific technical setup (PINN with ReLU for Navier-Stokes), a precise observation (vanishing second derivatives), and asks for a causal explanation and a valid remedy based on mathematical first principles. The question is unambiguous and admits a specific, derivable answer.\n- **Objective:** The language is technical, precise, and free of subjective or ambiguous terminology. All concepts—ReLU, AD, Laplacian, PINN—have rigorous mathematical or computational definitions.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, contradictoriness, or ambiguity. It presents a standard, yet important, technical challenge in the application of PINNs.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Principle-Based Derivation and Option Analysis\n\nThe core of the problem lies in the differentiability of the neural network's output function, $\\mathbf{u}_\\theta(\\mathbf{x}, t)$, which is determined by the properties of its activation functions. The viscous term in the Navier-Stokes equations, $\\mu \\nabla^2 \\mathbf{u}$, requires the computation of second-order spatial derivatives of the velocity field. In a PINN, this is accomplished by applying Automatic Differentiation (AD) to the network's output, $\\mathbf{u}_\\theta$.\n\nLet's analyze the Rectified Linear Unit (ReLU) activation function, defined as $r(x) = \\max(0, x)$.\n1.  **The function itself:** $r(x)$ is continuous everywhere. It is linear for $x > 0$ and for $x < 0$. The point $x=0$ is a \"kink\".\n2.  **First Derivative:** The classical derivative of $r(x)$ is the Heaviside step function:\n    $$ r'(x) = \\frac{d}{dx} \\max(0, x) = \\begin{cases} 0 & \\text{if } x < 0 \\\\ 1 & \\text{if } x > 0 \\end{cases} $$\n    The derivative is undefined at $x=0$. A neural network composed of affine transformations and ReLU activations, $\\mathbf{u}_\\theta(\\mathbf{x}, t)$, is a continuous, piecewise-linear function. Its first partial derivatives (e.g., $\\partial_{x_i} u_{\\theta,j}$) are therefore piecewise-constant functions, with jump discontinuities at the \"kinks\".\n3.  **Second Derivative:** The classical second derivative of $r(x)$ is:\n    $$ r''(x) = \\frac{d^2}{dx^2} \\max(0, x) = 0 \\quad \\text{for all } x \\neq 0 $$\n    At $x=0$, the classical second derivative is undefined. In the theory of distributions, the second derivative is the Dirac delta distribution, $r''(x) = \\delta(x)$. However, standard AD frameworks used in deep learning libraries (like TensorFlow or PyTorch) are designed to compute classical derivatives of the composition of elementary functions in the computational graph. They do not operate on distributions. When AD computes the second derivative, it differentiates the piecewise-constant first derivative, which results in a value of $0$ almost everywhere. At the points of discontinuity ($x=0$ for the pre-activations), the derivative is undefined, and AD will typically propagate a $0$ or `NaN`, but not a symbolic representation of a singularity.\n\nConsequently, for a network $\\mathbf{u}_\\theta$ constructed with ReLU activations, the AD-computed Laplacian $\\nabla^2 \\mathbf{u}_\\theta = \\sum_i \\partial^2_{x_i x_i} \\mathbf{u}_\\theta$ will be zero at any collocation point that does not lie exactly on the measure-zero set of kinks. This makes the viscous residual term $\\mu \\nabla^2 \\mathbf{u}_\\theta$ in the PINN loss function vanish, not because the physics is satisfied, but as a mathematical artifact of the network's structure. The network is thus not penalized for errors in the viscous term and fails to learn the correct solution, especially in flows where viscosity is significant.\n\nTo remedy this, the activation function must be smooth enough to have well-defined, non-zero, continuous second derivatives. An activation function $\\phi(x)$ must be at least in the class $C^2$ (twice continuously differentiable) for $\\nabla^2 \\mathbf{u}_\\theta$ to be well-defined and continuous. Better yet, using an infinitely differentiable ($C^\\infty$) activation function is a robust choice.\n- **Candidate 1: Hyperbolic Tangent, $\\tanh(x)$:** This function is $C^\\infty$. Its second derivative is $d^2/dx^2 \\tanh(x) = -2\\tanh(x)\\text{sech}^2(x)$, which is continuous and non-zero.\n- **Candidate 2: Softplus, $\\mathrm{softplus}_\\beta(x) = \\beta^{-1}\\ln(1+e^{\\beta x})$:** This function is also $C^\\infty$. It is a smooth approximation of ReLU. Its first derivative is the sigmoid function, and its second derivative is continuous and non-zero, allowing the network to represent non-zero Laplacians.\n\nTherefore, the problem is correctly diagnosed as the non-existence of a classical second derivative for ReLU, and the solution is to replace it with a $C^2$ or smoother activation like $\\tanh(x)$ or $\\mathrm{softplus}(x)$.\n\n### Evaluation of Options\n\n**A. AD in modern frameworks computes second derivatives of ReLU activations as distributions that include singular measures at kink locations, so $\\nabla^2 \\mathbf{u}_\\theta$ is accurately captured almost everywhere; therefore, retaining ReLU does not impair viscous residual evaluation.**\nThis statement is fundamentally incorrect. Standard AD packages in frameworks like TensorFlow and PyTorch do not compute distributional derivatives. They compute classical derivatives via the chain rule. They do not represent or handle singular measures like the Dirac delta function. The AD-computed second derivative is $0$ almost everywhere, which is precisely why the viscous residual evaluation is impaired.\n**Verdict: Incorrect.**\n\n**B. Because ReLU networks produce piecewise-linear outputs, any composition of ReLU activations is twice differentiable everywhere, so the viscous term $\\mu \\nabla^2 \\mathbf{u}_\\theta$ vanishes identically and this speeds up training without harming accuracy.**\nThis statement is incorrect on multiple counts. A piecewise-linear function is not even once differentiable everywhere, let alone twice. The fact that the viscous term artificially vanishes is the *source* of the problem and it severely *harms* accuracy by preventing the network from learning the correct physics of diffusion.\n**Verdict: Incorrect.**\n\n**C. The classical second derivative of the ReLU function $r(x)=\\max(0,x)$ is $0$ for $x\\neq 0$ and is undefined at $x=0$, and compositions of ReLU yield outputs that are piecewise linear with kinks on sets where pre-activations cross $0$. AD in standard PINN implementations returns $0$ almost everywhere and ignores singularities, making pointwise $\\nabla^2 \\mathbf{u}_\\theta$ ill-defined for the viscous residual. A remedy is to replace ReLU with a twice continuously differentiable activation such as $\\tanh(x)$ or $\\mathrm{softplus}_\\beta(x)=\\beta^{-1}\\ln\\!\\big(1+e^{\\beta x}\\big)$, which are $C^\\infty$ and ensure well-defined second derivatives.**\nThis option accurately describes the mathematical situation. It correctly identifies that the classical second derivative of ReLU is $0$ almost everywhere, explains why this leads to an ill-defined or zero viscous residual when using AD, and proposes the standard, correct remedy of using $C^2$-or-smoother activations like $\\tanh(x)$ or $\\mathrm{softplus}(x)$ which are $C^\\infty$. Every part of this statement is consistent with the first-principles analysis.\n**Verdict: Correct.**\n\n**D. One can retain ReLU and instead compute $\\nabla^2 \\mathbf{u}_\\theta$ by finite-difference stencils on a grid of collocation points; this produces accurate second derivatives and maintains end-to-end AD for PINN training.**\nThis option proposes a method that is both suboptimal and mischaracterized. Using finite differences (FD) introduces truncation error, moving away from the \"exact derivative\" principle of AD. For a non-smooth (piecewise-linear) function, FD approximations of second derivatives are notoriously unreliable. Furthermore, introducing an FD operator into the loss computation breaks the purely AD-based differentiation chain from the loss to the network parameters $\\theta$, thus it does not \"maintain end-to-end AD\" in the typical sense of a PINN residual. This approach does not fix the underlying problem that the network function itself is not smooth enough to represent a solution to a second-order PDE in a strong sense.\n**Verdict: Incorrect.**\n\n**E. Switching to leaky ReLU $r_\\alpha(x)=\\max(\\alpha x,x)$ with $\\alpha\\in(0,1)$ fixes the problem because the function is differentiable everywhere and has nonzero curvature; thus pointwise second derivatives required by the viscous residual are well-defined.**\nThis statement is mathematically false. The leaky ReLU function $r_\\alpha(x)$ is still piecewise-linear and has a kink at $x=0$. It is not differentiable at $x=0$. Its second derivative is also $0$ for all $x \\neq 0$ and is undefined at $x=0$. Therefore, it suffers from the exact same problem as the standard ReLU function concerning the computation of second derivatives for the viscous term. It does not solve the problem.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Training a PINN efficiently requires focusing computational resources where they are needed most. This advanced practice  explores residual-based adaptive refinement, a powerful strategy for concentrating collocation points in regions of high error, such as near boundary layers or shocks. You will formulate a complete adaptive sampling scheme, learning how to use importance weighting to correct for the non-uniform sampling and maintain a statistically unbiased loss estimator.",
            "id": "4100006",
            "problem": "Consider an incompressible Newtonian fluid in a bounded spatial domain $\\Omega \\subset \\mathbb{R}^d$ ($d \\in \\{2,3\\}$) over a time interval $[0,T]$. The primary fields are the velocity $\\mathbf{u}(\\mathbf{x},t)$ and the pressure $p(\\mathbf{x},t)$. The governing equations are the incompressible Navier–Stokes momentum balance and the continuity constraint, which (for density $\\rho>0$, dynamic viscosity $\\mu>0$, and body force $\\mathbf{f}$) read\n$$\n\\rho\\left(\\partial_t \\mathbf{u} + (\\mathbf{u}\\cdot\\nabla)\\mathbf{u}\\right) + \\nabla p - \\mu \\nabla^2 \\mathbf{u} - \\mathbf{f} = \\mathbf{0},\\quad \\nabla\\cdot\\mathbf{u}=\\;0.\n$$\nA Physics-Informed Neural Network (PINN) with parameters $\\boldsymbol{\\theta}$ approximates $(\\mathbf{u},p)$ and is trained by minimizing a continuous objective that penalizes the squared residuals of the momentum and continuity equations in the space-time domain, together with boundary and initial condition violations. Let the interior residuals be\n$$\nr_{\\text{mom}}(\\mathbf{x},t;\\boldsymbol{\\theta}) \\equiv \\rho\\left(\\partial_t \\mathbf{u}_\\boldsymbol{\\theta} + (\\mathbf{u}_\\boldsymbol{\\theta}\\cdot\\nabla)\\mathbf{u}_\\boldsymbol{\\theta}\\right) + \\nabla p_\\boldsymbol{\\theta} - \\mu \\nabla^2 \\mathbf{u}_\\boldsymbol{\\theta} - \\mathbf{f},\\quad r_{\\text{cont}}(\\mathbf{x},t;\\boldsymbol{\\theta}) \\equiv \\nabla\\cdot\\mathbf{u}_\\boldsymbol{\\theta},\n$$\nand denote by $\\|\\cdot\\|$ the Euclidean norm. The interior contribution to the training objective is the domain-time integral of a positive combination of $\\|r_{\\text{mom}}\\|^2$ and $|r_{\\text{cont}}|^2$. In practice, this integral is approximated by a Monte Carlo (MC) sum over a set of interior collocation points sampled from a distribution on $\\Omega\\times[0,T]$.\n\nYou are asked to design a residual-based adaptive refinement strategy that, during training, adds collocation points in space-time regions where the scalar indicator $s(\\mathbf{x},t;\\boldsymbol{\\theta})\\equiv \\|r_{\\text{mom}}(\\mathbf{x},t;\\boldsymbol{\\theta})\\| + |r_{\\text{cont}}(\\mathbf{x},t;\\boldsymbol{\\theta})|$ exceeds a user-specified threshold $\\tau>0$. The strategy must satisfy two requirements: \n($i$) it should update the sampling distribution over interior points in a way that concentrates new samples in high-residual regions but still allows exploration of the entire domain, and \n($ii$) it should update the loss evaluation so that the estimator remains consistent with the original continuous objective defined under a baseline distribution (for example, uniform over $\\Omega\\times[0,T]$), avoiding bias introduced by nonuniform adaptive sampling. You may assume separate handling for boundary and initial condition points.\n\nWhich option most correctly formulates such a strategy, starting from the above governing equations and residual definitions, and explicitly states how both the loss and the sampling distribution are updated under residual-thresholded adaptive refinement?\n\nA. After each training epoch $k$, compute $s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)$ on a candidate set in $\\Omega\\times[0,T]$ and define the exceedance region $\\mathcal{A}_k=\\{(\\mathbf{x},t): s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)>\\tau\\}$. Update the interior sampling distribution by a mixture\n$$\nq_{k+1}(\\mathbf{x},t) = (1-\\beta)\\,p(\\mathbf{x},t) + \\beta\\,g_k(\\mathbf{x},t),\\quad \\beta\\in(0,1),\n$$\nwhere $p$ is the baseline (for example, uniform) distribution over $\\Omega\\times[0,T]$, and $g_k$ is supported on $\\mathcal{A}_k$ and proportional to $s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)^\\gamma$ with $\\gamma\\geq 1$, i.e., $g_k(\\mathbf{x},t)=\\frac{s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)^\\gamma\\,\\mathbf{1}_{\\mathcal{A}_k}(\\mathbf{x},t)}{Z_k}$ for normalizer $Z_k>0$. Evaluate the interior loss by importance weighting to preserve the original objective under $p$:\n$$\n\\mathcal{L}_{\\text{int}}(\\boldsymbol{\\theta}_{k+1}) \\approx \\frac{1}{N}\\sum_{i=1}^{N}\\frac{p(\\mathbf{x}_i,t_i)}{q_{k+1}(\\mathbf{x}_i,t_i)}\\left(\\lambda_{\\text{mom},k}\\,\\|r_{\\text{mom}}(\\mathbf{x}_i,t_i;\\boldsymbol{\\theta}_{k+1})\\|^2+\\lambda_{\\text{cont},k}\\,|r_{\\text{cont}}(\\mathbf{x}_i,t_i;\\boldsymbol{\\theta}_{k+1})|^2\\right),\n$$\nwith $(\\mathbf{x}_i,t_i)\\sim q_{k+1}$, and where $\\lambda_{\\text{mom},k},\\lambda_{\\text{cont},k}>0$ are updated (for example, via running normalization of residual scales) to balance the contributions of momentum and continuity terms. Continue to sample a fraction $(1-\\beta)$ from $p$ to maintain exploration.\n\nB. After each training epoch, identify the set $\\mathcal{B}_k=\\{(\\mathbf{x},t): s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)<\\tau\\}$ and add new collocation points only from $\\mathcal{B}_k$ to stabilize training in already accurate regions. Update the sampling distribution by removing mass from $\\{s>\\tau\\}$ so that $q_{k+1}\\propto \\mathbf{1}_{\\mathcal{B}_k}$, and continue to evaluate the interior loss by the unweighted mean of $\\|r_{\\text{mom}}\\|^2$ and $|r_{\\text{cont}}|^2$ under $q_{k+1}$.\n\nC. At each epoch, set the sampling distribution $q_{k+1}(\\mathbf{x},t)\\propto \\exp\\left(s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)\\right)$ over all of $\\Omega\\times[0,T]$ and draw all interior collocation points from $q_{k+1}$, with no baseline component. Clip the residuals above $\\tau$ by replacing $s$ with $\\min\\{s,\\tau\\}$ in the loss to avoid domination, and evaluate the interior loss as the simple average of $\\|r_{\\text{mom}}\\|^2$ and $|r_{\\text{cont}}|^2$ under $q_{k+1}$, without importance weights.\n\nD. After each epoch, deterministically select new interior points at the $M$ locations of maximum $s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)$ for some fixed $M\\in\\mathbb{N}$, and keep the original uniform sampling distribution for all other points. Update the loss by multiplying the momentum and continuity residuals at those $M$ selected points by $s(\\mathbf{x},t;\\boldsymbol{\\theta}_k)$ in order to emphasize high-residual regions, while keeping the rest unweighted. No change to the overall sampling distribution is needed since uniform sampling is retained.",
            "solution": "The problem asks for a residual-based adaptive sampling strategy that satisfies two requirements: (i) it must concentrate samples in high-residual regions while maintaining exploration of the entire domain, and (ii) it must use a loss estimator that remains consistent with the original objective defined under a baseline distribution (e.g., uniform).\n\nLet's analyze these requirements.\n1.  **Requirement (i): Exploration and Exploitation.** A robust strategy for balancing exploration (sampling the whole domain) and exploitation (sampling high-error regions) is to use a mixture distribution for sampling. The new sampling distribution, $q(\\mathbf{x},t)$, should be a weighted sum of a baseline distribution $p(\\mathbf{x},t)$ (like uniform) and a targeted distribution $g(\\mathbf{x},t)$ that is concentrated in regions where the residual indicator $s(\\mathbf{x},t;\\boldsymbol{\\theta})$ is high. A form like $q = (1-\\beta)p + \\beta g$ explicitly allocates a fraction of samples to exploration and the rest to exploitation, satisfying this requirement.\n\n2.  **Requirement (ii): Consistent Loss Estimator.** The original continuous loss objective is an integral over the domain with respect to the baseline measure, $\\mathcal{L} = \\int L(\\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}$, where $\\mathbf{z}=(\\mathbf{x},t)$. When we sample from a different distribution $q(\\mathbf{z})$, the standard Monte Carlo average $\\frac{1}{N}\\sum L(\\mathbf{z}_i)$ becomes a biased estimator of $\\mathcal{L}$. To correct this bias and obtain a consistent estimator, one must use **importance sampling**. The loss for each sample point must be weighted by the ratio of the probability densities of the baseline and sampling distributions, $w_i = p(\\mathbf{z}_i)/q(\\mathbf{z}_i)$. The correct loss estimator is then $\\hat{\\mathcal{L}} \\approx \\frac{1}{N}\\sum_{i=1}^{N} w_i L(\\mathbf{z}_i)$.\n\nNow we evaluate the options based on these two principles.\n\n*   **Option A:** This option proposes a mixture distribution $q_{k+1} = (1-\\beta)p + \\beta g_k$, which correctly implements the exploration-exploitation balance (Requirement i). It also explicitly uses importance weighting in the loss function, $\\frac{p(\\mathbf{x}_i,t_i)}{q_{k+1}(\\mathbf{x}_i,t_i)}$, which provides a consistent estimator (Requirement ii). This option correctly addresses both core requirements of the problem.\n\n*   **Option B:** This option proposes sampling from low-residual regions, which is the opposite of the desired behavior. It also uses an unweighted mean for the loss, which creates a biased estimator. It fails both requirements.\n\n*   **Option C:** This option proposes a sampling distribution $q_{k+1} \\propto \\exp(s)$, which focuses on high residuals but lacks an explicit exploration component. More importantly, it uses a simple average for the loss without importance weights, violating Requirement (ii).\n\n*   **Option D:** This option proposes a deterministic selection of high-residual points and an ad-hoc re-weighting of the loss by the residual value itself. This is not a statistically sound importance sampling scheme and does not guarantee a consistent loss estimator, failing Requirement (ii). The claim that the sampling distribution is unchanged is also incorrect.\n\nTherefore, Option A is the only one that presents a complete and statistically correct adaptive sampling strategy.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}