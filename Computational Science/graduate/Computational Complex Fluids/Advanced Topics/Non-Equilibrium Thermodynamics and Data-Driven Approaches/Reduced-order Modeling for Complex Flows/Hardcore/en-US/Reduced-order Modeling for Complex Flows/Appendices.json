{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of creating any reduced-order model (ROM) is deciding its size, which involves a trade-off between accuracy and computational cost. Proper Orthogonal Decomposition (POD) provides a systematic way to do this by generating a basis where each mode's importance is quantified by its associated energy. This exercise provides fundamental, hands-on practice in computing the energy spectrum from flow data and using it to select the minimal model rank $r$ that captures a desired percentage of the system's dynamics .",
            "id": "4101511",
            "problem": "Consider a set of vector-valued flow snapshots arranged as columns in a matrix $X \\in \\mathbb{R}^{n \\times m}$, where each column $x_j \\in \\mathbb{R}^n$ represents the discrete state of a complex fluid flow at time index $j \\in \\{1,\\dots,m\\}$. Assume the standard Euclidean inner product over $\\mathbb{R}^n$ and define the energy of a snapshot $x_j$ as $\\|x_j\\|_2^2$. Let the mean-centered snapshot matrix be $\\tilde{X} = X - \\bar{x}\\mathbf{1}^\\top$, where $\\bar{x} = \\frac{1}{m} X \\mathbf{1}$ and $\\mathbf{1} \\in \\mathbb{R}^m$ is the vector of ones. Perform a Singular Value Decomposition (SVD) of $\\tilde{X}$ as $\\tilde{X} = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times r^\\star}$ has orthonormal columns, $\\Sigma \\in \\mathbb{R}^{r^\\star \\times r^\\star}$ is diagonal with nonnegative entries $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{r^\\star} \\ge 0$, $V \\in \\mathbb{R}^{m \\times r^\\star}$ has orthonormal columns, and $r^\\star = \\operatorname{rank}(\\tilde{X})$. Define the Proper Orthogonal Decomposition (POD) energy captured by the first $r$ modes as the cumulative fraction\n$$\nE(r) = \\frac{\\sum_{k=1}^{r} \\sigma_k^2}{\\sum_{k=1}^{m} \\sigma_k^2},\n$$\nwith the convention that $\\sigma_k = 0$ for $k > r^\\star$ and the denominator equals $\\|\\tilde{X}\\|_F^2 = \\sum_{k=1}^{r^\\star} \\sigma_k^2$. For a given tolerance $\\epsilon \\in [0,1]$, define\n$$\nr_{\\min}(\\epsilon) = \\min\\{r \\in \\{0,1,\\dots,m\\} \\mid E(r) \\ge \\epsilon\\},\n$$\nwith the convention $E(0) = 0$.\n\nYour task is to write a program that, for each specified test case, computes $E(r_q)$ for a provided query $r_q$ and determines $r_{\\min}(\\epsilon)$ for a provided tolerance $\\epsilon$, using the mean-centered snapshot matrix $\\tilde{X}$ constructed from the given $X$.\n\nUse the following test suite. In all cases, indices are $i \\in \\{1,\\dots,n\\}$ and $j \\in \\{1,\\dots,m\\}$, trigonometric functions act on radians, and all quantities are dimensionless.\n\n- Test case $1$ (general multi-frequency flow):\n  - $n = 64$, $m = 32$.\n  - $X_{ij} = \\sin\\!\\left(\\frac{2\\pi i}{n}\\right)\\cos\\!\\left(\\frac{2\\pi j}{m}\\right) + 0.4\\,\\sin\\!\\left(\\frac{4\\pi i}{n}\\right)\\sin\\!\\left(\\frac{3\\pi j}{m}\\right)$.\n  - $r_q = 5$.\n  - $\\epsilon = 0.95$.\n\n- Test case $2$ (near-rank-two structure, high tolerance boundary):\n  - $n = 50$, $m = 50$.\n  - $X_{ij} = \\frac{i}{n} + \\frac{j}{m}$.\n  - $r_q = 2$.\n  - $\\epsilon = 0.999$.\n\n- Test case $3$ (controlled modal superposition):\n  - $n = 60$, $m = 45$.\n  - Define $u_p(i) = \\sin\\!\\left(\\frac{2\\pi p i}{n}\\right)$ and $v_p(j) = \\cos\\!\\left(\\frac{2\\pi p j}{m}\\right)$ for $p \\in \\{1,2,3\\}$, with weights $a_1 = 2.0$, $a_2 = 1.0$, $a_3 = 0.5$. Construct $X = \\sum_{p=1}^{3} a_p\\, u_p\\, v_p^\\top$.\n  - $r_q = 2$.\n  - $\\epsilon = 0.90$.\n\n- Test case $4$ (rank-one structure, small tolerance and query edge):\n  - $n = 40$, $m = 20$.\n  - $X_{ij} = 0.01\\,\\sin\\!\\left(\\frac{2\\pi i}{n}\\right)\\cos\\!\\left(\\frac{2\\pi j}{m}\\right)$.\n  - $r_q = 0$.\n  - $\\epsilon = 0.01$.\n\nFinal output format: Your program should produce a single line containing a Python-style list of pairs, one per test case, where each pair is $[E(r_q), r_{\\min}(\\epsilon)]$. The list must be ordered in the same order as the test cases. For example, the output must look like `[ [ratio1,r1],[ratio2,r2],[ratio3,r3],[ratio4,r4] ]`, where each `ratioK` is a float in $[0,1]$ and each `rK$` is an integer in $\\{0,1,\\dots,m\\}$.",
            "solution": "The user-provided problem statement is subjected to a rigorous validation process before a solution is attempted.\n\n### Step 1: Extracted Givens\n- **Snapshot Matrix**: $X \\in \\mathbb{R}^{n \\times m}$, where columns $x_j \\in \\mathbb{R}^n$ are flow states for $j \\in \\{1,\\dots,m\\}$.\n- **Energy Definition**: Energy of a snapshot $x_j$ is $\\|x_j\\|_2^2$.\n- **Mean-Centered Matrix**: $\\tilde{X} = X - \\bar{x}\\mathbf{1}^\\top$, where $\\bar{x} = \\frac{1}{m} X \\mathbf{1}$ is the mean of the columns of $X$.\n- **Singular Value Decomposition (SVD)**: $\\tilde{X} = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times r^\\star}$, $\\Sigma \\in \\mathbb{R}^{r^\\star \\times r^\\star}$ is diagonal with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{r^\\star} \\ge 0$, and $V \\in \\mathbb{R}^{m \\times r^\\star}$. $r^\\star = \\operatorname{rank}(\\tilde{X})$.\n- **POD Energy Function**: The cumulative fraction of energy captured by the first $r$ modes is $E(r) = \\frac{\\sum_{k=1}^{r} \\sigma_k^2}{\\sum_{k=1}^{m} \\sigma_k^2}$.\n- **Conventions for $E(r)$**: $\\sigma_k = 0$ for $k > r^\\star$. The denominator is $\\|\\tilde{X}\\|_F^2$.\n- **Minimum Rank Function**: For a tolerance $\\epsilon \\in [0,1]$, $r_{\\min}(\\epsilon) = \\min\\{r \\in \\{0,1,\\dots,m\\} \\mid E(r) \\ge \\epsilon\\}$.\n- **Convention for $r_{\\min}(\\epsilon)$**: $E(0) = 0$.\n- **Task**: For each test case, compute $E(r_q)$ and $r_{\\min}(\\epsilon)$.\n- **Test Case 1**:\n  - $n = 64$, $m = 32$.\n  - $X_{ij} = \\sin\\!\\left(\\frac{2\\pi i}{n}\\right)\\cos\\!\\left(\\frac{2\\pi j}{m}\\right) + 0.4\\,\\sin\\!\\left(\\frac{4\\pi i}{n}\\right)\\sin\\!\\left(\\frac{3\\pi j}{m}\\right)$.\n  - $r_q = 5$.\n  - $\\epsilon = 0.95$.\n- **Test Case 2**:\n  - $n = 50$, $m = 50$.\n  - $X_{ij} = \\frac{i}{n} + \\frac{j}{m}$.\n  - $r_q = 2$.\n  - $\\epsilon = 0.999$.\n- **Test Case 3**:\n  - $n = 60$, $m = 45$.\n  - $X = \\sum_{p=1}^{3} a_p\\, u_p\\, v_p^\\top$, where $u_p(i) = \\sin\\!\\left(\\frac{2\\pi p i}{n}\\right)$, $v_p(j) = \\cos\\!\\left(\\frac{2\\pi p j}{m}\\right)$, and weights $a_1 = 2.0$, $a_2 = 1.0$, $a_3 = 0.5$.\n  - $r_q = 2$.\n  - $\\epsilon = 0.90$.\n- **Test Case 4**:\n  - $n = 40$, $m = 20$.\n  - $X_{ij} = 0.01\\,\\sin\\!\\left(\\frac{2\\pi i}{n}\\right)\\cos\\!\\left(\\frac{2\\pi j}{m}\\right)$.\n  - $r_q = 0$.\n  - $\\epsilon = 0.01$.\n- **General Rules**: Indices $i \\in \\{1,\\dots,n\\}$ and $j \\in \\{1,\\dots,m\\}$. Trigonometric functions use radians.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established validation criteria.\n1.  **Scientifically Grounded**: The problem is rooted in the established principles of linear algebra (SVD) and computational fluid dynamics (Proper Orthogonal Decomposition). POD is a standard, widely-used technique for model order reduction of complex systems. The definitions of snapshot energy, mean-centering, and cumulative energy fraction are standard in the field. The problem is scientifically sound.\n2.  **Well-Posed**: The problem provides a clear, unambiguous procedure to follow. For each test case, all inputs ($X$, $r_q$, $\\epsilon$) are explicitly defined. The outputs ($E(r_q)$, $r_{\\min}(\\epsilon)$) are uniquely determined by the application of the defined mathematical operations (mean-centering, SVD, summation). A unique and stable solution exists for each case.\n3.  **Objective**: The problem is stated in precise, formal mathematical language. There are no subjective claims, opinions, or ambiguities.\n4.  **Completeness and Consistency**: The problem is self-contained. All necessary matrices, parameters, and functions are defined. The definitions are internally consistent; for instance, the denominator of $E(r)$ is correctly identified as the squared Frobenius norm of $\\tilde{X}$, which in turn equals the sum of the squared singular values.\n5.  **Not Unrealistic or Infeasible**: The test cases involve constructing matrices from well-behaved mathematical functions and performing standard numerical computations. The dimensions of the matrices are modest and computationally tractable.\n6.  **Not Ill-Posed or Trivial**: The problem requires a non-trivial implementation of a numerical algorithm involving matrix construction, mean-centering, SVD, and analysis of singular values. The test cases are designed to probe different scenarios (multi-mode, low-rank, mean-subtraction effects), demonstrating conceptual depth beyond a simple calculation. For example, test cases 3 and 4 have a zero-mean structure, simplifying $\\tilde{X}=X$, while test case 2 has a non-zero mean which, upon subtraction, reduces the matrix rank to $1$. These are subtle and important features.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is scientifically grounded, well-posed, objective, and computationally tractable. A complete solution will be provided.\n\n### Solution and Algorithmic Design\n\nThe task is to compute two quantities for several test cases: the POD energy $E(r_q)$ for a given rank $r_q$, and the minimum rank $r_{\\min}(\\epsilon)$ required to capture a fraction $\\epsilon$ of the total energy. The procedure is based on the Singular Value Decomposition (SVD) of the mean-centered data.\n\nThe algorithm for each test case proceeds as follows:\n\n1.  **Construct the Snapshot Matrix $X$**: For a given test case with dimensions $n \\times m$, we first construct the matrix $X \\in \\mathbb{R}^{n \\times m}$. The elements $X_{ij}$ are defined by the provided formula, paying close attention to the $1$-based indexing for $i \\in \\{1, \\dots, n\\}$ and $j \\in \\{1, \\dots, m\\}$.\n\n2.  **Compute the Mean-Centered Matrix $\\tilde{X}$**: The foundation of POD for analyzing fluctuations is to subtract the mean state. We compute the time-averaged snapshot $\\bar{x} \\in \\mathbb{R}^n$ by averaging the columns of $X$:\n    $$ \\bar{x} = \\frac{1}{m} \\sum_{j=1}^{m} x_j $$\n    Then, we form the mean-centered (or fluctuation) matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times m}$ by subtracting $\\bar{x}$ from each column $x_j$:\n    $$ \\tilde{X} = X - \\bar{x}\\mathbf{1}^\\top $$\n    where $\\mathbf{1}^\\top$ is a row vector of $m$ ones.\n\n3.  **Perform Singular Value Decomposition**: We compute the SVD of the mean-centered matrix $\\tilde{X}$.\n    $$ \\tilde{X} = U \\Sigma V^\\top $$\n    This decomposition yields the left singular vectors (POD modes) in $U$, the right singular vectors in $V$, and the singular values $\\sigma_k$ on the diagonal of $\\Sigma$. For our purpose, only the singular values $\\sigma_k$ are required. We obtain them as a one-dimensional array $\\mathbf{s} = [\\sigma_1, \\sigma_2, \\dots, \\sigma_{\\min(n,m)}]$ from a standard numerical SVD routine.\n\n4.  **Calculate Cumulative Energy Spectrum**: The energy of each POD mode is proportional to the square of its corresponding singular value. The total energy in the fluctuations is the sum of all modal energies, which equals the squared Frobenius norm of $\\tilde{X}$:\n    $$ \\mathcal{E}_{\\text{total}} = \\|\\tilde{X}\\|_F^2 = \\sum_{k=1}^{r^\\star} \\sigma_k^2 $$\n    The energy captured by the first $r$ modes is $\\sum_{k=1}^{r} \\sigma_k^2$. The cumulative energy fraction is then:\n    $$ E(r) = \\frac{\\sum_{k=1}^{r} \\sigma_k^2}{\\mathcal{E}_{\\text{total}}} $$\n    We compute this for all possible $r$ values, from $1$ up to $r^\\star = \\operatorname{rank}(\\tilde{X})$. This is efficiently done by first squaring all singular values, then computing their cumulative sum. If $\\mathcal{E}_{\\text{total}}$ is numerically zero, all fluctuation energies are zero, and we define $E(r) = 0$ for $r > 0$.\n\n5.  **Determine $E(r_q)$**: Using the computed cumulative energy spectrum and the given query rank $r_q$:\n    - If $r_q = 0$, by definition, $E(0) = 0$.\n    - If $r_q > 0$, we find $E(r_q)$ from the pre-computed array of cumulative energy fractions. If $r_q$ is larger than the number of available singular values, it implies all energy has been captured, so $E(r_q) = 1$.\n\n6.  **Determine $r_{\\min}(\\epsilon)$**: Using the given tolerance $\\epsilon \\in [0,1]$:\n    - If $\\epsilon \\le 0$, the condition $E(r) \\ge \\epsilon$ is satisfied by $r=0$ since $E(0)=0$. Thus, $r_{\\min}(\\epsilon) = 0$.\n    - If $\\epsilon > 0$, we search for the smallest integer $r \\ge 1$ such that $E(r) \\ge \\epsilon$. This can be efficiently found by searching the sorted array of cumulative energy fractions for the first element that is greater than or equal to $\\epsilon$. The rank $r$ is the index of this element plus one.\n\nThis structured procedure is applied to each test case to generate the required pair of values $[E(r_q), r_{\\min}(\\epsilon)]$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the POD problem for all test cases.\n    \"\"\"\n\n    def generate_x_case1(n, m):\n        \"\"\"Generates the snapshot matrix X for Test Case 1.\"\"\"\n        i_vals = np.arange(1, n + 1).reshape(-1, 1)\n        j_vals = np.arange(1, m + 1).reshape(1, -1)\n        x = (np.sin(2 * np.pi * i_vals / n) * np.cos(2 * np.pi * j_vals / m) +\n             0.4 * np.sin(4 * np.pi * i_vals / n) * np.sin(3 * np.pi * j_vals / m))\n        return x\n\n    def generate_x_case2(n, m):\n        \"\"\"Generates the snapshot matrix X for Test Case 2.\"\"\"\n        i_vals = np.arange(1, n + 1).reshape(-1, 1)\n        j_vals = np.arange(1, m + 1).reshape(1, -1)\n        x = i_vals / n + j_vals / m\n        return x\n\n    def generate_x_case3(n, m):\n        \"\"\"Generates the snapshot matrix X for Test Case 3.\"\"\"\n        i_vals = np.arange(1, n + 1).reshape(-1, 1)\n        j_vals = np.arange(1, m + 1).reshape(1, -1)\n        weights = [2.0, 1.0, 0.5]\n        x = np.zeros((n, m))\n        for p_idx, p in enumerate(range(1, 4)):\n            a_p = weights[p_idx]\n            u_p = np.sin(2 * np.pi * p * i_vals / n)\n            v_p_T = np.cos(2 * np.pi * p * j_vals / m)\n            x += a_p * (u_p @ v_p_T)\n        return x\n\n    def generate_x_case4(n, m):\n        \"\"\"Generates the snapshot matrix X for Test Case 4.\"\"\"\n        i_vals = np.arange(1, n + 1).reshape(-1, 1)\n        j_vals = np.arange(1, m + 1).reshape(1, -1)\n        x = 0.01 * np.sin(2 * np.pi * i_vals / n) * np.cos(2 * np.pi * j_vals / m)\n        return x\n\n    test_cases = [\n        {'n': 64, 'm': 32, 'gen_x': generate_x_case1, 'r_q': 5, 'epsilon': 0.95},\n        {'n': 50, 'm': 50, 'gen_x': generate_x_case2, 'r_q': 2, 'epsilon': 0.999},\n        {'n': 60, 'm': 45, 'gen_x': generate_x_case3, 'r_q': 2, 'epsilon': 0.90},\n        {'n': 40, 'm': 20, 'gen_x': generate_x_case4, 'r_q': 0, 'epsilon': 0.01},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n, m, gen_x, r_q, epsilon = case['n'], case['m'], case['gen_x'], case['r_q'], case['epsilon']\n\n        # Step 1: Construct the snapshot matrix X\n        X = gen_x(n, m)\n\n        # Step 2: Compute the mean-centered matrix X_tilde\n        x_mean = X.mean(axis=1, keepdims=True)\n        X_tilde = X - x_mean\n\n        # Step 3: Perform SVD\n        # We only need the singular values, s.\n        s = np.linalg.svd(X_tilde, compute_uv=False)\n\n        # Step 4: Calculate cumulative energy spectrum\n        s_sq = s**2\n        total_energy = np.sum(s_sq)\n\n        cumulative_energy_frac = np.array([])\n        if total_energy > 1e-15:\n            cumulative_energy_frac = np.cumsum(s_sq) / total_energy\n            # Clamp last value to 1.0 to handle potential float precision issues\n            if cumulative_energy_frac.size > 0:\n                cumulative_energy_frac[-1] = 1.0\n\n\n        # Step 5: Determine E(r_q)\n        e_rq = 0.0\n        if r_q > 0:\n            if cumulative_energy_frac.size == 0:\n                 e_rq = 0.0\n            elif r_q > len(cumulative_energy_frac):\n                e_rq = 1.0\n            else:\n                e_rq = cumulative_energy_frac[r_q - 1]\n\n        # Step 6: Determine r_min(epsilon)\n        r_min = 0\n        if epsilon > 0:\n            if cumulative_energy_frac.size == 0:\n                # No energy, so need infinite modes for epsilon > 0.\n                # Per problem, max r is m. So we return m.\n                r_min = m\n            else:\n                # np.searchsorted finds the index where epsilon would be inserted\n                # to maintain order. The required rank is that index + 1.\n                found_idx = np.searchsorted(cumulative_energy_frac, epsilon)\n                if found_idx < len(cumulative_energy_frac):\n                    r_min = found_idx + 1\n                else: \n                    # Epsilon is > 1.0 or numerically greater than max cumulative energy.\n                    # This case implies we cannot reach the energy tolerance.\n                    # The problem constrains epsilon to [0, 1].\n                    # If max(E) is numerically  epsilon, it means we need all modes.\n                    r_min = len(cumulative_energy_frac)\n\n        results.append([e_rq, r_min])\n    \n    # Format the output exactly as specified.\n    # [ [ratio1,r1],[ratio2,r2],... ]\n    # Using str() on lists inserts spaces, so we build the string manually.\n    pairs_str = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output_str = f\"[{','.join(pairs_str)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While the standard Euclidean inner product is a common starting point for POD, it is often suboptimal for complex, multi-physics flows where different state variables have different physical units or energy scalings. This practice explores the critical concept of using a weighted inner product that reflects the physical invariants of the system, such as kinetic or elastic energy. By comparing ROMs built for a viscoelastic flow model using different physically-motivated norms , you will gain crucial insight into how to construct more accurate and stable models by embedding physics directly into the projection operator.",
            "id": "4101559",
            "problem": "Consider a dimensionless, linearized Oldroyd-B viscoelastic flow model in a truncated Galerkin representation for a planar channel. Let the state vector be $x(t) \\in \\mathbb{R}^{n}$ with $n = 6$, partitioned as $x(t) = \\begin{bmatrix} v(t) \\\\ \\tau(t) \\end{bmatrix}$ where $v(t) \\in \\mathbb{R}^{3}$ represents velocity modal coefficients and $\\tau(t) \\in \\mathbb{R}^{3}$ represents polymer stress modal coefficients. The evolution is governed by the first-order linear system\n$$\n\\frac{d x}{d t} = A(\\mathrm{Wi}, \\nu, \\alpha, \\beta)\\, x(t) + b\\, f(t),\n$$\nwhere $A \\in \\mathbb{R}^{6 \\times 6}$ is a block matrix encoding viscous dissipation and polymer stress coupling, $\\mathrm{Wi}$ is the Weissenberg number, $\\nu$ is the kinematic viscosity, $\\alpha$ is a coupling coefficient scaling the divergence of polymer stress in the momentum equation, and $\\beta$ is the polymer-to-total viscosity ratio. The external forcing is $f(t) = \\sin(\\omega t)$ with $\\omega$ in radians, and $b \\in \\mathbb{R}^{6}$ is a constant input direction acting on the velocity modes only.\n\nUse the following physically consistent and scientifically realistic parameterization:\n- Let the modal Laplacian for velocity be $L_v = \\mathrm{diag}(k_1^2, k_2^2, k_3^2)$ with $k_1 = 1$, $k_2 = 2$, $k_3 = 3$, and viscous dissipation block $A_{vv} = -\\nu L_v$ with $\\nu = 0.1$.\n- Let the divergence-like coupling from polymer stress to velocity be $A_{v\\tau} = \\alpha D$ with $\\alpha = 0.5$ and $D = \\mathrm{diag}(0.7, 0.6, 0.5)$.\n- Let the strain-to-stress coupling be $A_{\\tau v} = \\beta S$ with $S = \\mathrm{diag}(0.4, 0.35, 0.3)$.\n- Let the polymer relaxation be $A_{\\tau \\tau} = -\\frac{1}{\\mathrm{Wi}} I_3$ with $I_3$ the $3 \\times 3$ identity, yielding the block system\n$$\nA(\\mathrm{Wi}, \\nu, \\alpha, \\beta) = \n\\begin{bmatrix}\n-\\nu L_v  \\alpha D \\\\\n\\beta S  -\\frac{1}{\\mathrm{Wi}} I_3\n\\end{bmatrix}.\n$$\n- Let the forcing direction be $b = \\begin{bmatrix} b_v \\\\ 0 \\end{bmatrix}$ with $b_v = [1.0,\\, 0.5,\\, 0.3]^\\top$ and the stress block zero.\n- Let the angular frequency be $\\omega = 1.2$ (radians).\n\nDefine three symmetric positive definite inner-product matrices to construct Proper Orthogonal Decomposition (POD) bases and to perform Galerkin projection:\n- Euclidean inner product: $W_E = I_6$, the $6 \\times 6$ identity.\n- Mass-weighted inner product: $W_M = M$, where\n$$\nM = \\mathrm{diag}(m_v, m_\\tau), \\quad m_v = [1.0,\\, 1.5,\\, 2.0], \\quad m_\\tau = [0.8,\\, 0.9,\\, 1.1],\n$$\ninterpreted as a discretization mass matrix in modal coordinates.\n- Energy-weighted inner product: $W_\\mathcal{E} = E$, where\n$$\nE = \\mathrm{diag}(m_v, \\frac{1}{\\beta}\\mathbf{1}_3),\n$$\nwith $\\mathbf{1}_3$ the length-$3$ vector of ones, representing a quadratic approximation to kinetic plus elastic energy, weighting velocity by $m_v$ and polymer stress by $1/\\beta$.\n\nFor each inner product $W \\in \\{W_E, W_M, W_\\mathcal{E}\\}$, define a rank-$r$ POD basis $\\Phi_W \\in \\mathbb{R}^{6 \\times r}$ constructed from high-fidelity snapshots $X = [x(t_1), \\dots, x(t_N)] \\in \\mathbb{R}^{6 \\times N}$ such that the columns of $\\Phi_W$ are orthonormal under the $W$-inner product. Use the $W$-Galerkin projection to form the Reduced-Order Model (ROM)\n$$\n\\frac{d x_r}{d t} = A_r x_r + b_r f(t), \\quad A_r = \\Phi_W^\\top W A \\Phi_W, \\quad b_r = \\Phi_W^\\top W b,\n$$\nand reconstruct the high-dimensional approximation $\\hat{x}(t) = \\Phi_W x_r(t)$.\n\nMeasure accuracy using the energy-relative error over the time horizon:\n$$\n\\varepsilon_W = \\sqrt{\\frac{\\sum_{j=1}^N \\|x(t_j) - \\hat{x}(t_j)\\|_{E}^2}{\\sum_{j=1}^N \\|x(t_j)\\|_{E}^2}},\n$$\nwhere $\\|y\\|_{E}^2 = y^\\top E y$. All quantities are dimensionless; angles are in radians.\n\nYour program must:\n1. Integrate the full-order model with $x(0) = 0$ using a fixed-step fourth-order Runge–Kutta method over a time horizon $T = 10$ with time step $\\Delta t = 0.005$, i.e., $N = T/\\Delta t$ snapshots, and forcing $f(t) = \\sin(\\omega t)$.\n2. Construct POD bases of dimension $r$ for each $W \\in \\{W_E, W_M, W_\\mathcal{E}\\}$ using a numerically stable weighted decomposition in which the columns of $\\Phi_W$ are $W$-orthonormal.\n3. Build and integrate the ROM for each $W$ with the same integrator and reconstruct $\\hat{x}(t)$.\n4. Compute $\\varepsilon_W$ for each $W$ in each test case.\n\nTest suite:\n- Case $\\mathrm{A}$: $(\\mathrm{Wi}, \\beta, r) = (0.1,\\, 0.25,\\, 3)$.\n- Case $\\mathrm{B}$: $(\\mathrm{Wi}, \\beta, r) = (1.0,\\, 0.35,\\, 3)$.\n- Case $\\mathrm{C}$ (edge case with strong elasticity and smaller subspace): $(\\mathrm{Wi}, \\beta, r) = (5.0,\\, 0.50,\\, 2)$.\n\nFor each case, report the list $[\\varepsilon_{W_E}, \\varepsilon_{W_M}, \\varepsilon_{W_\\mathcal{E}}]$ as real numbers (dimensionless floats). Your program should produce a single line of output containing the results as a comma-separated list of these three-element lists, enclosed in square brackets, for the three cases, in order $\\mathrm{A}$, $\\mathrm{B}$, $\\mathrm{C}$. For example, the output format must be\n$$\n[[\\varepsilon_{W_E}^{(\\mathrm{A})}, \\varepsilon_{W_M}^{(\\mathrm{A})}, \\varepsilon_{W_\\mathcal{E}}^{(\\mathrm{A})}], [\\varepsilon_{W_E}^{(\\mathrm{B})}, \\varepsilon_{W_M}^{(\\mathrm{B})}, \\varepsilon_{W_\\mathcal{E}}^{(\\mathrm{B})}], [\\varepsilon_{W_E}^{(\\mathrm{C})}, \\varepsilon_{W_M}^{(\\mathrm{C})}, \\varepsilon_{W_\\mathcal{E}}^{(\\mathrm{C})}]].\n$$\nAll computations and outputs are dimensionless, and angles are in radians. The final output must have no units attached and must be in the specified single-line format.",
            "solution": "The problem requires the development and comparison of reduced-order models (ROMs) for a linearized viscoelastic flow system. The process involves simulating a full-order model (FOM), constructing ROMs using Proper Orthogonal Decomposition (POD) with three different inner products, and evaluating their accuracy.\n\n### 1. Full-Order Model (FOM)\nThe system dynamics are described by a linear ordinary differential equation (LDE) for the state vector $x(t) \\in \\mathbb{R}^{6}$. The state vector contains $3$ modal coefficients for velocity, $v(t)$, and $3$ for polymer stress, $\\tau(t)$, such that $x(t) = [v(t)^\\top, \\tau(t)^\\top]^\\top$. The governing equation is:\n$$\n\\frac{d x}{d t} = A(\\mathrm{Wi}, \\beta) x(t) + b f(t)\n$$\nThe forcing function is $f(t) = \\sin(\\omega t)$ with angular frequency $\\omega = 1.2$. The constant forcing vector $b \\in \\mathbb{R}^{6}$ acts only on the velocity modes: $b = [1.0, 0.5, 0.3, 0, 0, 0]^\\top$.\n\nThe state matrix $A \\in \\mathbb{R}^{6 \\times 6}$ is dependent on the Weissenberg number $\\mathrm{Wi}$ and the viscosity ratio $\\beta$. It is defined as a block matrix:\n$$\nA(\\mathrm{Wi}, \\beta) = \n\\begin{bmatrix}\nA_{vv}  A_{v\\tau} \\\\\nA_{\\tau v}  A_{\\tau \\tau}\n\\end{bmatrix} = \n\\begin{bmatrix}\n-\\nu L_v  \\alpha D \\\\\n\\beta S  -\\frac{1}{\\mathrm{Wi}} I_3\n\\end{bmatrix}\n$$\nThe blocks are defined using the given parameters and matrices:\n- Viscous dissipation: $A_{vv} = -\\nu L_v$, with kinematic viscosity $\\nu = 0.1$ and modal Laplacian $L_v = \\mathrm{diag}(1^2, 2^2, 3^2) = \\mathrm{diag}(1, 4, 9)$.\n- Stress-to-velocity coupling: $A_{v\\tau} = \\alpha D$, with coupling coefficient $\\alpha = 0.5$ and $D = \\mathrm{diag}(0.7, 0.6, 0.5)$.\n- Velocity-to-stress coupling: $A_{\\tau v} = \\beta S$, with $S = \\mathrm{diag}(0.4, 0.35, 0.3)$. The parameter $\\beta$ is specified in each test case.\n- Polymer relaxation: $A_{\\tau \\tau} = -\\frac{1}{\\mathrm{Wi}} I_3$, where $I_3$ is the $3 \\times 3$ identity matrix and $\\mathrm{Wi}$ is specified in each test case.\n\nTo generate data for model reduction, this FOM is numerically integrated from the initial condition $x(0)=0$ over the time horizon $T=10$ using a fixed-step fourth-order Runge-Kutta (RK4) method with a time step of $\\Delta t = 0.005$. This produces a sequence of $N+1$ state vectors, or \"snapshots,\" $x(t_j)$ for $t_j = j \\Delta t$, where $j=0, 1, \\dots, N$ and $N=T/\\Delta t=2000$. These snapshots are collected into a snapshot matrix $X = [x(t_0), x(t_1), ..., x(t_N)] \\in \\mathbb{R}^{6 \\times (N+1)}$.\n\n### 2. Weighted Proper Orthogonal Decomposition (POD)\nThe goal of POD is to find an optimal low-dimensional basis for representing the snapshot data. Given an inner product defined by a symmetric positive definite weight matrix $W$, we seek a basis $\\Phi_W \\in \\mathbb{R}^{6 \\times r}$ whose columns are orthonormal with respect to this inner product, i.e., $\\Phi_W^\\top W \\Phi_W = I_r$, where $r$ is the desired rank of the ROM.\n\nThe algorithm to compute this $W$-orthonormal basis from the snapshot matrix $X$ is as follows:\n1.  Perform a Cholesky decomposition of the weight matrix $W = L L^\\top$, where $L$ is a lower-triangular matrix.\n2.  Transform the snapshot data using this factor: $\\tilde{X} = L^\\top X$.\n3.  Compute the Singular Value Decomposition (SVD) of the transformed data: $\\tilde{X} = U \\Sigma V^\\top$. The columns of $U$ form a standard orthonormal basis that optimally captures the data in $\\tilde{X}$.\n4.  Truncate the basis by selecting the first $r$ columns of $U$, which correspond to the largest singular values: $\\tilde{\\Phi}_W = U[:, :r]$.\n5.  Transform this basis back to the original coordinate system to obtain the final POD basis: $\\Phi_W = (L^\\top)^{-1} \\tilde{\\Phi}_W$. This linear system is solved efficiently using back substitution since $L^\\top$ is upper triangular.\n\nThis procedure is performed for three distinct inner products defined by the matrices:\n- Euclidean: $W_E = I_6$.\n- Mass-weighted: $W_M = \\mathrm{diag}(1.0, 1.5, 2.0, 0.8, 0.9, 1.1)$.\n- Energy-weighted: $W_\\mathcal{E} = E = \\mathrm{diag}(1.0, 1.5, 2.0, 1/\\beta, 1/\\beta, 1/\\beta)$. Note that this matrix depends on the parameter $\\beta$ from the test case.\n\n### 3. Reduced-Order Model (ROM) Construction and Simulation\nUsing the computed POD basis $\\Phi_W$, a ROM is constructed via $W$-Galerkin projection. The full state $x(t)$ is approximated by a projection onto the basis, $x(t) \\approx \\hat{x}(t) = \\Phi_W x_r(t)$, where $x_r(t) \\in \\mathbb{R}^r$ is the reduced state. Substituting this into the FOM and projecting onto the basis using the $W$-inner product yields the ROM:\n$$\n\\frac{d x_r}{d t} = A_r x_r(t) + b_r f(t)\n$$\nwhere the reduced system matrices are given by:\n$$\nA_r = \\Phi_W^\\top W A \\Phi_W \\in \\mathbb{R}^{r \\times r}\n$$\n$$\nb_r = \\Phi_W^\\top W b \\in \\mathbb{R}^{r}\n$$\nThis smaller $r$-dimensional system is integrated using the same RK4 method and time step. The initial condition is $x_r(0) = \\Phi_W^\\top W x(0) = 0$. The resulting reduced-state trajectory $x_r(t_j)$ is then used to reconstruct the high-dimensional approximation $\\hat{x}(t_j) = \\Phi_W x_r(t_j)$.\n\n### 4. Error Evaluation\nThe accuracy of each ROM is quantified by the energy-relative error, $\\varepsilon_W$. It is crucial to note that for all three ROMs (constructed using $W_E, W_M, W_\\mathcal{E}$), the error is measured in the norm induced by the energy inner product matrix, $E = W_\\mathcal{E}$. The error is calculated as:\n$$\n\\varepsilon_W = \\sqrt{\\frac{\\sum_{j=0}^N \\|x(t_j) - \\hat{x}(t_j)\\|_{E}^2}{\\sum_{j=0}^N \\|x(t_j)\\|_{E}^2}}\n$$\nwhere the squared energy norm is $\\|y\\|_{E}^2 = y^\\top E y$.\n\nThe entire process—FOM simulation, POD basis construction for each of the three inner products, ROM simulation, and error calculation—is repeated for each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef rk4_step(f, t, y, dt, *args):\n    \"\"\"\n    Performs a single step of the fourth-order Runge-Kutta method.\n    \n    Args:\n        f (callable): The RHS function of the ODE, f(t, y, *args).\n        t (float): Current time.\n        y (np.ndarray): Current state vector (must be a column vector).\n        dt (float): Time step.\n        *args: Additional arguments passed to f.\n        \n    Returns:\n        np.ndarray: The state vector at time t + dt.\n    \"\"\"\n    k1 = dt * f(t, y, *args)\n    k2 = dt * f(t + 0.5 * dt, y + 0.5 * k1, *args)\n    k3 = dt * f(t + 0.5 * dt, y + 0.5 * k2, *args)\n    k4 = dt * f(t + dt, y + k3, *args)\n    return y + (k1 + 2 * k2 + 2 * k3 + k4) / 6.0\n\ndef odefun(t, x, A, b, omega):\n    \"\"\"\n    RHS of the ODE system dx/dt = Ax + b*f(t).\n    \n    Args:\n        t (float): Current time.\n        x (np.ndarray): Current state vector (column vector).\n        A (np.ndarray): State matrix.\n        b (np.ndarray): Forcing vector.\n        omega (float): Forcing frequency.\n        \n    Returns:\n        np.ndarray: The derivative dx/dt.\n    \"\"\"\n    return A @ x + b * np.sin(omega * t)\n\ndef get_pod_basis(snapshots, W, r):\n    \"\"\"\n    Computes a W-orthonormal POD basis of rank r from snapshots.\n    \n    Args:\n        snapshots (np.ndarray): Matrix of state snapshots (dims: n x N_snapshots).\n        W (np.ndarray): Symmetric positive definite weight matrix.\n        r (int): Rank of the desired basis.\n        \n    Returns:\n        np.ndarray: The POD basis matrix Phi (dims: n x r).\n    \"\"\"\n    # 1. Cholesky decomposition of W: W = L @ L.T\n    L = cholesky(W, lower=True)\n\n    # 2. Transform snapshots: X_tilde = L.T @ snapshots\n    X_tilde = L.T @ snapshots\n\n    # 3. SVD of transformed snapshots\n    U, _, _ = np.linalg.svd(X_tilde, full_matrices=False)\n\n    # 4. Truncate basis for transformed system\n    U_r = U[:, :r]\n\n    # 5. Transform basis back by solving L.T @ Phi = U_r for Phi\n    Phi = solve_triangular(L.T, U_r, lower=False, check_finite=False)\n    \n    return Phi\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    nu = 0.1\n    alpha = 0.5\n    omega = 1.2\n    T = 10.0\n    dt = 0.005\n\n    # Fixed matrices and vectors\n    k_vals = np.array([1.0, 2.0, 3.0])\n    L_v = np.diag(k_vals**2)\n    D = np.diag([0.7, 0.6, 0.5])\n    S = np.diag([0.4, 0.35, 0.3])\n\n    b_v = np.array([1.0, 0.5, 0.3])\n    b = np.zeros((6, 1))\n    b[0:3, 0] = b_v\n\n    m_v = np.array([1.0, 1.5, 2.0])\n    m_tau = np.array([0.8, 0.9, 1.1])\n    W_M = np.diag(np.concatenate((m_v, m_tau)))\n    W_E_mat = np.eye(6)\n\n    # Test suite\n    test_cases = [\n        (0.1, 0.25, 3),  # Case A\n        (1.0, 0.35, 3),  # Case B\n        (5.0, 0.50, 2)   # Case C\n    ]\n\n    all_results = []\n\n    for Wi, beta, r in test_cases:\n        # ---- 1. Assemble Full-Order Model (FOM) ----\n        A_vv = -nu * L_v\n        A_vt = alpha * D\n        A_tv = beta * S\n        A_tt = -(1.0 / Wi) * np.eye(3)\n        A = np.block([[A_vv, A_vt], [A_tv, A_tt]])\n        \n        # Energy matrix E depends on beta and is used for error measurement\n        E_mat = np.diag(np.concatenate((m_v, np.full(3, 1.0/beta))))\n\n        weight_matrices = [W_E_mat, W_M, E_mat]\n\n        # ---- 2. Simulate FOM to generate snapshots ----\n        n_steps = int(round(T / dt))\n        timesteps = np.linspace(0, T, n_steps + 1)\n        x = np.zeros((6, 1))\n        snapshots = np.zeros((6, n_steps + 1))\n        \n        for i in range(n_steps):\n            x = rk4_step(odefun, timesteps[i], x, dt, A, b, omega)\n            snapshots[:, i + 1] = x.flatten()\n\n        # ---- 3. Build and evaluate ROMs for each inner product ----\n        case_errors = []\n        for W in weight_matrices:\n            # 3a. Construct POD basis\n            Phi = get_pod_basis(snapshots, W, r)\n\n            # 3b. Build ROM\n            Ar = Phi.T @ W @ A @ Phi\n            br = Phi.T @ W @ b\n\n            # 3c. Simulate ROM\n            xr = np.zeros((r, 1))\n            rom_snapshots_r = np.zeros((r, n_steps + 1))\n            \n            for i in range(n_steps):\n                xr = rk4_step(odefun, timesteps[i], xr, dt, Ar, br, omega)\n                rom_snapshots_r[:, i + 1] = xr.flatten()\n            \n            # 3d. Reconstruct full state\n            reconstructed_snapshots = Phi @ rom_snapshots_r\n\n            # 3e. Compute energy-relative error\n            diff = snapshots - reconstructed_snapshots\n            \n            numerator = np.sum(np.sum(diff * (E_mat @ diff), axis=0))\n            denominator = np.sum(np.sum(snapshots * (E_mat @ snapshots), axis=0))\n            \n            relative_error = np.sqrt(numerator / denominator) if denominator  0 else 0.0\n            \n            case_errors.append(relative_error)\n            \n        all_results.append(case_errors)\n\n    # Final print statement in the exact required format.\n    sublist_strs = [f'[{\",\".join(map(str, sublist))}]' for sublist in all_results]\n    print(f\"[{','.join(sublist_strs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The primary motivation for employing ROMs is to achieve significant computational speedups, but for nonlinear problems, a standard Galerkin projection is often insufficient as it requires evaluating the nonlinearity on the full, high-dimensional mesh. This practice delves into the performance implications of hyper-reduction, a class of techniques essential for achieving true online efficiency. By implementing a roofline performance model, you will analytically predict and quantify the speedups gained by avoiding full-order computations , a key skill for assessing the practical viability of a ROM for a given application and hardware environment.",
            "id": "4101583",
            "problem": "You are asked to implement an analytical performance model to quantify online-stage speedups for a reduced-order model of a nonlinear complex flow using Proper Orthogonal Decomposition (POD) and the Galerkin method, with and without hyper-reduction. The focus is to profile floating point operation counts and memory traffic and to predict time-to-solution using the roofline performance model. You must implement a program that computes the predicted online speedup for several parameter sets.\n\nThe fundamental base for the derivation begins with a semi-discrete dynamical system obtained from a spatial discretization (e.g., finite volume or finite element) of a nonlinear conservation law, which can be written as\n$$\n\\frac{d \\mathbf{u}}{dt} = \\mathbf{A}\\,\\mathbf{u} + \\mathbf{N}(\\mathbf{u}) + \\mathbf{f},\n$$\nwhere $\\mathbf{u}\\in\\mathbb{R}^{N_h}$ is the high-dimensional state, $\\mathbf{A}\\in\\mathbb{R}^{N_h\\times N_h}$ is a linear operator, $\\mathbf{N}(\\cdot)$ is a nonlinear operator, and $\\mathbf{f}\\in\\mathbb{R}^{N_h}$ is a forcing term. A reduced basis $\\mathbf{V}\\in\\mathbb{R}^{N_h\\times r}$ is obtained via Proper Orthogonal Decomposition (POD), and the state is approximated as $\\mathbf{u}\\approx \\mathbf{V}\\,\\mathbf{a}$ with reduced coordinates $\\mathbf{a}\\in\\mathbb{R}^{r}$. The Galerkin projection yields an online reduced system\n$$\n\\frac{d \\mathbf{a}}{dt} = \\mathbf{V}^T\\left(\\mathbf{A}\\,\\mathbf{V}\\,\\mathbf{a} + \\mathbf{N}(\\mathbf{V}\\,\\mathbf{a}) + \\mathbf{f}\\right).\n$$\nFor the linear term, assume the offline precomputation of the reduced operator $\\mathbf{M}=\\mathbf{V}^T\\mathbf{A}\\mathbf{V}\\in\\mathbb{R}^{r\\times r}$ is available, so the online linear contribution is $\\mathbf{M}\\,\\mathbf{a}$. For the nonlinear term, consider two online strategies:\n- Without hyper-reduction: directly evaluate $\\mathbf{N}(\\mathbf{V}\\,\\mathbf{a})$ at full dimension $N_h$ and then project with $\\mathbf{V}^T$.\n- With hyper-reduction: use a sampling-based hyper-reduction such as the Discrete Empirical Interpolation Method (DEIM) or Energy-Conserving Sampling and Weighting (ECSW). Model it as follows: select $s$ sampled degrees of freedom via a sampling operator $\\mathbf{P}\\in\\mathbb{R}^{N_h\\times s}$, precompute a reduced sampling basis so that online one can form the sampled state using $\\mathbf{V}_s=\\mathbf{P}^T\\mathbf{V}\\in\\mathbb{R}^{s\\times r}$ and evaluate the sampled nonlinearity $\\mathbf{n}_s=\\mathbf{N}_s(\\mathbf{u}_s)\\in\\mathbb{R}^{s}$ with $\\mathbf{u}_s=\\mathbf{V}_s\\,\\mathbf{a}$. Assume a precomputed projection map $\\mathbf{W}\\in\\mathbb{R}^{r\\times s}$ such that the reduced nonlinear term is approximated by $\\mathbf{W}\\,\\mathbf{n}_s$.\n\nAdopt the following well-tested performance modeling assumptions:\n- A dense matrix-vector multiply of size $m\\times n$ costs $2\\,m\\,n$ floating point operations (FLOPs) and streams $8\\,(m\\,n + m + n)$ bytes when using double precision storage (each scalar occupies $8$ bytes).\n- Evaluating the nonlinear operator at $k$ degrees of freedom costs $c_n\\,k$ FLOPs and streams $b_n\\,k$ bytes, where $c_n$ (in FLOPs per entry) and $b_n$ (in bytes per entry) are given parameters modeling the numerical stencil and data access pattern.\n- The roofline model gives the time per online evaluation as\n$$\nT = \\max\\left(\\frac{F}{F_{\\text{peak}}},\\ \\frac{Q}{B_{\\text{peak}}}\\right),\n$$\nwhere $F$ is the FLOP count, $Q$ is the streamed bytes, $F_{\\text{peak}}$ is the hardware peak floating point rate (in FLOPs per second), and $B_{\\text{peak}}$ is the sustainable memory bandwidth (in bytes per second). Express time in seconds.\n\nFrom first principles of the above operations, derive and implement the following per time-step online cost models:\n\n- Without hyper-reduction:\n  - Compute $\\mathbf{u}=\\mathbf{V}\\,\\mathbf{a}$: FLOPs $F_1=2\\,N_h\\,r$, bytes $Q_1=8\\,(N_h\\,r + N_h + r)$.\n  - Evaluate $\\mathbf{n}=\\mathbf{N}(\\mathbf{u})$: FLOPs $F_2=c_n\\,N_h$, bytes $Q_2=b_n\\,N_h$.\n  - Project $\\mathbf{g}=\\mathbf{V}^T\\,\\mathbf{n}$: FLOPs $F_3=2\\,N_h\\,r$, bytes $Q_3=8\\,(N_h\\,r + N_h + r)$.\n  - Linear term $\\mathbf{M}\\,\\mathbf{a}$: FLOPs $F_4=2\\,r^2$, bytes $Q_4=8\\,(r^2 + 2\\,r)$.\n  - Totals: $F_{\\text{noHR}} = F_1 + F_2 + F_3 + F_4$ and $Q_{\\text{noHR}} = Q_1 + Q_2 + Q_3 + Q_4$.\n\n- With hyper-reduction:\n  - Compute sampled state $\\mathbf{u}_s=\\mathbf{V}_s\\,\\mathbf{a}$: FLOPs $f_1=2\\,s\\,r$, bytes $q_1=8\\,(s\\,r + s + r)$.\n  - Evaluate sampled nonlinearity $\\mathbf{n}_s$: FLOPs $f_2=c_n\\,s$, bytes $q_2=b_n\\,s$.\n  - Project $\\mathbf{W}\\,\\mathbf{n}_s$: FLOPs $f_3=2\\,r\\,s$, bytes $q_3=8\\,(r\\,s + r + s)$.\n  - Linear term $\\mathbf{M}\\,\\mathbf{a}$: FLOPs $f_4=2\\,r^2$, bytes $q_4=8\\,(r^2 + 2\\,r)$.\n  - Totals: $F_{\\text{HR}} = f_1 + f_2 + f_3 + f_4$ and $Q_{\\text{HR}} = q_1 + q_2 + q_3 + q_4$.\n\nUse the roofline model to compute $T_{\\text{noHR}}$ and $T_{\\text{HR}}$ in seconds and report the online speedup\n$$\nS \\equiv \\frac{T_{\\text{noHR}}}{T_{\\text{HR}}}.\n$$\n\nImplement a program that, for a given list of parameter sets, outputs a single line containing the list of speedups $S$ for each case, rounded to six decimal places. There are no angles or percentages in this problem. Time must be in seconds in intermediate modeling, but the reported speedup is dimensionless. Use double precision ($8$ bytes per scalar) consistently.\n\nTest suite (each tuple is $(N_h,\\ r,\\ s,\\ c_n,\\ b_n,\\ F_{\\text{peak}},\\ B_{\\text{peak}})$):\n- Case $1$ (general, moderately compute-bound): $(100000,\\ 60,\\ 300,\\ 60,\\ 16,\\ 2.0\\times 10^{12},\\ 2.0\\times 10^{11})$\n- Case $2$ (memory-bound scenario): $(100000,\\ 60,\\ 120,\\ 60,\\ 16,\\ 4.0\\times 10^{12},\\ 5.0\\times 10^{10})$\n- Case $3$ (no effective hyper-reduction, $s=N_h$): $(5000,\\ 40,\\ 5000,\\ 40,\\ 16,\\ 1.0\\times 10^{12},\\ 1.0\\times 10^{11})$\n- Case $4$ (small mesh, higher $c_n$ and $b_n$): $(1000,\\ 20,\\ 50,\\ 80,\\ 24,\\ 5.0\\times 10^{11},\\ 1.0\\times 10^{11})$\n- Case $5$ (large mesh, heavy nonlinearity): $(200000,\\ 80,\\ 200,\\ 200,\\ 32,\\ 2.5\\times 10^{12},\\ 1.5\\times 10^{11})$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is the speedup $S$ for the corresponding case, rounded to six decimal places as a decimal number.",
            "solution": "The objective is to develop and implement an analytical performance model to predict the online-stage speedup afforded by hyper-reduction in the context of a Proper Orthogonal Decomposition (POD) Galerkin reduced-order model (ROM). The speedup, denoted by $S$, is defined as the ratio of the time-to-solution per time step for a standard Galerkin projection ROM to that of a ROM employing hyper-reduction.\n\nThe analysis begins with the provided semi-discrete system for the high-dimensional state $\\mathbf{u} \\in \\mathbb{R}^{N_h}$:\n$$\n\\frac{d \\mathbf{u}}{dt} = \\mathbf{A}\\,\\mathbf{u} + \\mathbf{N}(\\mathbf{u}) + \\mathbf{f}\n$$\nThe state is approximated in a reduced basis $\\mathbf{V} \\in \\mathbb{R}^{N_h \\times r}$ as $\\mathbf{u} \\approx \\mathbf{V}\\,\\mathbf{a}$, where $\\mathbf{a} \\in \\mathbb{R}^{r}$ are the reduced coordinates and $r \\ll N_h$. The Galerkin projection yields the reduced system:\n$$\n\\frac{d \\mathbf{a}}{dt} = \\mathbf{V}^T\\mathbf{A}\\mathbf{V}\\,\\mathbf{a} + \\mathbf{V}^T\\mathbf{N}(\\mathbf{V}\\,\\mathbf{a}) + \\mathbf{V}^T\\mathbf{f}\n$$\nThe forcing term $\\mathbf{V}^T\\mathbf{f}$ and the linear operator $\\mathbf{M} = \\mathbf{V}^T\\mathbf{A}\\mathbf{V} \\in \\mathbb{R}^{r \\times r}$ are assumed to be precomputed offline. The primary online computational cost arises from the evaluation of the reduced nonlinear term, which we analyze under two scenarios.\n\nThe performance of each scenario is quantified using the roofline model, which provides an estimate for the time-to-solution, $T$, as:\n$$\nT = \\max\\left(\\frac{F}{F_{\\text{peak}}},\\ \\frac{Q}{B_{\\text{peak}}}\\right)\n$$\nHere, $F$ represents the total floating-point operations (FLOPs), $Q$ is the total memory traffic in bytes, $F_{\\text{peak}}$ is the peak computational rate of the hardware in FLOPs/second, and $B_{\\text{peak}}$ is the sustainable memory bandwidth in bytes/second. We assume double-precision floating-point numbers, where each scalar occupies $8$ bytes.\n\n**1. Online Cost without Hyper-Reduction ($T_{\\text{noHR}}$)**\n\nIn this standard Galerkin approach, the nonlinear term $\\mathbf{N}(\\mathbf{V}\\,\\mathbf{a})$ is evaluated at the full dimension $N_h$ and subsequently projected onto the reduced basis. The online computational steps per time-step are:\n\n1.  Reconstruct the full state: $\\mathbf{u}=\\mathbf{V}\\,\\mathbf{a}$. This is a dense matrix-vector product between a matrix of size $N_h \\times r$ and a vector of size $r$.\n    -   FLOPs: $F_1 = 2\\,N_h\\,r$.\n    -   Bytes: $Q_1 = 8\\,(N_h\\,r + N_h + r)$. This accounts for streaming the matrix $\\mathbf{V}$, the vector $\\mathbf{a}$, and the result vector $\\mathbf{u}$.\n\n2.  Evaluate the nonlinearity: $\\mathbf{n}=\\mathbf{N}(\\mathbf{u})$. This is performed on the full $N_h$-dimensional state vector.\n    -   FLOPs: $F_2 = c_n\\,N_h$.\n    -   Bytes: $Q_2 = b_n\\,N_h$. The parameters $c_n$ and $b_n$ model the per-entry computational and memory cost.\n\n3.  Project the nonlinear term: $\\mathbf{g}=\\mathbf{V}^T\\,\\mathbf{n}$. This is a dense matrix-vector product between a matrix of size $r \\times N_h$ and a vector of size $N_h$.\n    -   FLOPs: $F_3 = 2\\,r\\,N_h$.\n    -   Bytes: $Q_3 = 8\\,(r\\,N_h + r + N_h)$. This accounts for streaming $\\mathbf{V}^T$, $\\mathbf{n}$, and the result $\\mathbf{g}$.\n\n4.  Compute the linear term contribution: $\\mathbf{M}\\,\\mathbf{a}$. This is a dense matrix-vector product involving the precomputed $r \\times r$ matrix $\\mathbf{M}$.\n    -   FLOPs: $F_4 = 2\\,r^2$.\n    -   Bytes: $Q_4 = 8\\,(r^2 + 2\\,r)$. This streams $\\mathbf{M}$, $\\mathbf{a}$, and the result.\n\nThe total FLOPs and memory traffic per step are the sums of these contributions:\n$$\nF_{\\text{noHR}} = F_1 + F_2 + F_3 + F_4 = 4\\,N_h\\,r + c_n\\,N_h + 2\\,r^2\n$$\n$$\nQ_{\\text{noHR}} = Q_1 + Q_2 + Q_3 + Q_4 = 16\\,(N_h\\,r + N_h + r) + b_n\\,N_h + 8\\,(r^2 + 2\\,r)\n$$\nThe time-to-solution is then given by the roofline model:\n$$\nT_{\\text{noHR}} = \\max\\left(\\frac{F_{\\text{noHR}}}{F_{\\text{peak}}},\\ \\frac{Q_{\\text{noHR}}}{B_{\\text{peak}}}\\right)\n$$\n\n**2. Online Cost with Hyper-Reduction ($T_{\\text{HR}}$)**\n\nHyper-reduction methods like DEIM or ECSW avoid computations at the full dimension $N_h$ by approximating the nonlinear term using only $s$ judiciously chosen sample points, where $s$ is typically on the order of $r$. The online computational steps are:\n\n1.  Compute the sampled state: $\\mathbf{u}_s=\\mathbf{V}_s\\,\\mathbf{a}$. This uses the precomputed sampled basis $\\mathbf{V}_s \\in \\mathbb{R}^{s \\times r}$.\n    -   FLOPs: $f_1 = 2\\,s\\,r$.\n    -   Bytes: $q_1 = 8\\,(s\\,r + s + r)$.\n\n2.  Evaluate the sampled nonlinearity: $\\mathbf{n}_s=\\mathbf{N}_s(\\mathbf{u}_s)$. This is only evaluated at the $s$ sample points.\n    -   FLOPs: $f_2 = c_n\\,s$.\n    -   Bytes: $q_2 = b_n\\,s$.\n\n3.  Project the sampled nonlinearity to the reduced space: $\\mathbf{W}\\,\\mathbf{n}_s$. This uses a precomputed projection map $\\mathbf{W} \\in \\mathbb{R}^{r \\times s}$.\n    -   FLOPs: $f_3 = 2\\,r\\,s$.\n    -   Bytes: $q_3 = 8\\,(r\\,s + r + s)$.\n\n4.  Compute the linear term contribution: $\\mathbf{M}\\,\\mathbf{a}$. This step is identical to the non-hyper-reduced case.\n    -   FLOPs: $f_4 = 2\\,r^2$.\n    -   Bytes: $q_4 = 8\\,(r^2 + 2\\,r)$.\n\nThe total FLOPs and memory traffic per step are:\n$$\nF_{\\text{HR}} = f_1 + f_2 + f_3 + f_4 = 4\\,s\\,r + c_n\\,s + 2\\,r^2\n$$\n$$\nQ_{\\text{HR}} = q_1 + q_2 + q_3 + q_4 = 16\\,(s\\,r + s + r) + b_n\\,s + 8\\,(r^2 + 2\\,r)\n$$\nThe time-to-solution is:\n$$\nT_{\\text{HR}} = \\max\\left(\\frac{F_{\\text{HR}}}{F_{\\text{peak}}},\\ \\frac{Q_{\\text{HR}}}{B_{\\text{peak}}}\\right)\n$$\n\n**3. Online Speedup**\n\nThe online speedup $S$ is the ratio of the time-per-step without hyper-reduction to the time-per-step with hyper-reduction:\n$$\nS = \\frac{T_{\\text{noHR}}}{T_{\\text{HR}}}\n$$\nA value of $S  1$ indicates that hyper-reduction is beneficial for the online stage. The following program will implement these calculations to compute $S$ for the given parameter sets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predicted online speedup for a reduced-order model \n    with and without hyper-reduction, based on a roofline performance model.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (N_h, r, s, c_n, b_n, F_peak, B_peak)\n    test_cases = [\n        (100000, 60, 300, 60, 16, 2.0e12, 2.0e11),\n        (100000, 60, 120, 60, 16, 4.0e12, 5.0e10),\n        (5000, 40, 5000, 40, 16, 1.0e12, 1.0e11),\n        (1000, 20, 50, 80, 24, 5.0e11, 1.0e11),\n        (200000, 80, 200, 200, 32, 2.5e12, 1.5e11),\n    ]\n\n    results = []\n    for N_h, r, s, c_n, b_n, F_peak, B_peak in test_cases:\n        # --- Cost Model without Hyper-Reduction (noHR) ---\n        \n        # FLOPs calculation\n        F_noHR = 4 * N_h * r + c_n * N_h + 2 * r**2\n        \n        # Memory traffic calculation (in bytes)\n        Q_noHR = 16 * (N_h * r + N_h + r) + b_n * N_h + 8 * (r**2 + 2 * r)\n        \n        # Time-to-solution using Roofline model\n        T_noHR = max(F_noHR / F_peak, Q_noHR / B_peak)\n        \n        # --- Cost Model with Hyper-Reduction (HR) ---\n        \n        # FLOPs calculation\n        F_HR = 4 * s * r + c_n * s + 2 * r**2\n        \n        # Memory traffic calculation (in bytes)\n        Q_HR = 16 * (s * r + s + r) + b_n * s + 8 * (r**2 + 2 * r)\n        \n        # Time-to-solution using Roofline model\n        T_HR = max(F_HR / F_peak, Q_HR / B_peak)\n        \n        # --- Speedup Calculation ---\n        \n        # Handle case where T_HR might be zero to avoid division by zero error\n        if T_HR > 0:\n            speedup = T_noHR / T_HR\n        else:\n            # If T_HR is zero, speedup is effectively infinite, but this\n            # should not happen with the given positive parameters.\n            # We can define it as 1.0 (no speedup) if both times are zero.\n            speedup = 1.0 if T_noHR == 0 else float('inf')\n\n        results.append(round(speedup, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{res:.6f}' for res in results)}]\")\n\nsolve()\n```"
        }
    ]
}