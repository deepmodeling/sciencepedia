{
    "hands_on_practices": [
        {
            "introduction": "The mass matrix is a cornerstone of finite element methods, arising from the inner product of basis functions. In high-order methods like DG and SEM, using specific quadrature rules can lead to a diagonal mass matrix, dramatically improving computational efficiency for time-dependent problems. This practice guides you through the derivation of this property and a concrete calculation of the resulting mass matrix entries, revealing the elegant interplay between interpolation and integration. ",
            "id": "4089553",
            "problem": "Consider a one-dimensional reference element $[-1,1]$ used in high-order discretizations for computational complex fluids, such as the Discontinuous Galerkin (DG) method and the Spectral Element Method (SEM). Let $p$ denote the polynomial degree of the local approximation space. For functions $f$ and $g$ defined on $[-1,1]$, the continuous $L^{2}$ inner product is defined by\n$$\n(f,g)_{L^{2}([-1,1])} = \\int_{-1}^{1} f(x)\\,g(x)\\,dx.\n$$\nTo construct a discrete inner product that is exact for polynomial integrands up to degree $2p$, assume an $n$-point quadrature rule $(x_{k},w_{k})_{k=1}^{n}$ on $[-1,1]$ with nodes $x_{k}$ and positive weights $w_{k}$ such that\n$$\n\\int_{-1}^{1} q(x)\\,dx = \\sum_{k=1}^{n} w_{k}\\,q(x_{k})\n$$\nfor every polynomial $q$ of degree at most $2p$. Consider the nodal Lagrange basis $\\{\\ell_{i}(x)\\}_{i=1}^{n}$ of degree $p$ associated with the interpolation nodes $\\{x_{i}\\}_{i=1}^{n}$, where $\\ell_{i}(x_{j}) = \\delta_{ij}$ for $1 \\le i,j \\le n$.\n\nStarting from the definition of the $L^{2}$ inner product and the exactness property of the quadrature, derive the discrete inner product\n$$\n(f,g)_{h} \\equiv \\sum_{k=1}^{n} w_{k}\\,f(x_{k})\\,g(x_{k}),\n$$\nand then construct the corresponding element mass matrix $M$ with entries\n$$\nM_{ij} = (\\ell_{i},\\ell_{j})_{h}.\n$$\nShow under these assumptions that $M$ is diagonal when the quadrature nodes coincide with the interpolation nodes. Finally, specialize to $p=3$ on $[-1,1]$ using the $n=p+1=4$ point Gauss–Legendre quadrature (which is exact for polynomials up to degree $2n-1=7 \\ge 2p$). Compute the four diagonal entries of the mass matrix in exact closed form, ordering them by the nodes from left to right on $[-1,1]$.\n\nProvide exact values with no rounding. Express your final answer as a row matrix using the LaTeX $\\texttt{pmatrix}$ environment in the order $\\{x_{1},x_{2},x_{3},x_{4}\\}$ where $x_{1}x_{2}x_{3}x_{4}$ on $[-1,1]$. No physical units are needed.",
            "solution": "The problem asks for several derivations and a specific calculation related to high-order numerical methods on a one-dimensional reference element $[-1,1]$. We will address each part of the problem in sequence.\n\nFirst, we validate the problem statement.\nAll givens are extracted and analyzed.\n- Domain: $[-1,1]$.\n- Polynomial degree: $p$.\n- Continuous inner product: $(f,g)_{L^{2}([-1,1])} = \\int_{-1}^{1} f(x)\\,g(x)\\,dx$.\n- Quadrature rule: An $n$-point rule $(x_{k},w_{k})_{k=1}^{n}$ exact for polynomials of degree at most $2p$.\n- Lagrange basis: $\\{\\ell_{i}(x)\\}_{i=1}^{n}$ of degree $p$ on $n$ nodes $\\{x_i\\}$, with $\\ell_{i}(x_{j})=\\delta_{ij}$. For a unique polynomial of degree $p$ to be defined by $n$ nodal values, we must have $n=p+1$.\n- Discrete inner product: $(f,g)_{h} = \\sum_{k=1}^{n} w_{k}\\,f(x_{k})\\,g(x_{k})$.\n- Mass matrix: $M_{ij} = (\\ell_{i},\\ell_{j})_{h}$.\n- Specific case: $p=3$, $n=p+1=4$ point Gauss-Legendre quadrature, exact for polynomials of degree up to $2n-1=7$.\n\nThe problem is scientifically grounded in the theory of finite element and spectral methods. It is well-posed, objective, and internally consistent. The relationship $n=p+1$ is standard for nodal bases of degree $p$. The exactness requirement for the quadrature ($2p=6$) is satisfied by the chosen $4$-point Gauss-Legendre rule (exact up to degree $7$). The problem is therefore valid.\n\nPart 1: Derivation of the discrete inner product\nThe continuous $L^{2}$ inner product of two basis functions $\\ell_i(x)$ and $\\ell_j(x)$ is given by\n$$\n(\\ell_i, \\ell_j)_{L^2} = \\int_{-1}^{1} \\ell_i(x) \\ell_j(x) dx.\n$$\nThe basis functions $\\{\\ell_i(x)\\}_{i=1}^{n}$ are polynomials of degree $p=n-1$. The product $\\ell_i(x)\\ell_j(x)$ is therefore a polynomial of degree at most $2p$.\nThe problem states that we use a quadrature rule that is exact for any polynomial $q(x)$ of degree at most $2p$:\n$$\n\\int_{-1}^{1} q(x)\\,dx = \\sum_{k=1}^{n} w_{k}\\,q(x_{k}).\n$$\nApplying this quadrature rule to the integrand $\\ell_i(x)\\ell_j(x)$, which is a polynomial of degree at most $2p$, we find that the integral is computed exactly:\n$$\n(\\ell_i, \\ell_j)_{L^2} = \\int_{-1}^{1} \\ell_i(x) \\ell_j(x) dx = \\sum_{k=1}^{n} w_{k}\\,\\ell_i(x_{k})\\,\\ell_j(x_{k}).\n$$\nThe expression on the right is precisely the discrete inner product $(\\ell_i, \\ell_j)_{h}$ as defined in the problem statement. This discrete inner product is thus constructed to be an exact representation of the continuous $L^2$ inner product when applied to pairs of basis functions.\n\nPart 2: Construction of the element mass matrix $M$\nThe element mass matrix $M$ has entries $M_{ij}$ given by the discrete inner product of the basis functions:\n$$\nM_{ij} = (\\ell_{i},\\ell_{j})_{h} = \\sum_{k=1}^{n} w_{k}\\,\\ell_{i}(x_{k})\\,\\ell_{j}(x_{k}).\n$$\n\nPart 3: Proof that $M$ is diagonal\nThe problem adds the crucial assumption that the quadrature nodes $\\{x_k\\}_{k=1}^n$ coincide with the interpolation nodes $\\{x_i\\}_{i=1}^n$. We can therefore set $x_k=x_i$ for $k=i$ (after appropriate ordering). The defining property of the Lagrange basis is its value at the interpolation nodes: $\\ell_i(x_j) = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\nSubstituting this property into the expression for $M_{ij}$ (with quadrature nodes $\\{x_k\\}$ being the same set as $\\{x_j\\}$):\n$$\nM_{ij} = \\sum_{k=1}^{n} w_{k}\\,\\ell_{i}(x_{k})\\,\\ell_{j}(x_{k}).\n$$\nThe terms in the sum are products of the form $\\ell_{i}(x_{k})\\ell_{j}(x_{k})$. The property $\\ell_i(x_k)=\\delta_{ik}$ means $\\ell_i(x_k)$ is $1$ if $k=i$ and $0$ otherwise. Similarly, $\\ell_j(x_k)=\\delta_{jk}$.\nThe expression for $M_{ij}$ becomes:\n$$\nM_{ij} = \\sum_{k=1}^{n} w_{k}\\,\\delta_{ik}\\,\\delta_{jk}.\n$$\nIf $i \\neq j$, then for any given index $k$, it is impossible for both $k=i$ and $k=j$ to be true. Thus, the product $\\delta_{ik}\\delta_{jk}$ is always zero. This makes every term in the summation zero, so $M_{ij} = 0$ for $i \\neq j$.\nIf $i = j$, the expression becomes:\n$$\nM_{ii} = \\sum_{k=1}^{n} w_{k}\\,\\delta_{ik}\\,\\delta_{ik} = \\sum_{k=1}^{n} w_{k}\\,(\\delta_{ik})^2.\n$$\nSince $\\delta_{ik}$ is either $0$ or $1$, $(\\delta_{ik})^2 = \\delta_{ik}$.\n$$\nM_{ii} = \\sum_{k=1}^{n} w_{k}\\,\\delta_{ik}.\n$$\nThis sum has only one non-zero term, which occurs when $k=i$. The value of this term is $w_i \\cdot 1 = w_i$.\nTherefore, the entries of the mass matrix are $M_{ij} = w_i \\delta_{ij}$. This shows that $M$ is a diagonal matrix with the quadrature weights on its diagonal: $M = \\text{diag}(w_1, w_2, \\ldots, w_n)$.\n\nPart 4: Calculation of the diagonal entries for $p=3$\nWe specialize to the case where the polynomial degree is $p=3$. The basis consists of $n=p+1=4$ Lagrange polynomials. The quadrature rule used is the $4$-point Gauss-Legendre quadrature. The nodes $\\{x_k\\}$ are the roots of the Legendre polynomial of degree $4$, $P_4(x)$. The diagonal entries of the mass matrix are the corresponding quadrature weights, $M_{ii} = w_i$.\n\nThe nodes $x_k$ are the roots of $P_4(x) = \\frac{1}{8}(35x^4 - 30x^2 + 3)$. Setting $P_4(x)=0$ gives $35x^4 - 30x^2 + 3 = 0$. Let $y=x^2$. The quadratic equation for $y$ is $35y^2 - 30y + 3 = 0$, whose solutions are\n$$\ny = \\frac{30 \\pm \\sqrt{30^2 - 4(35)(3)}}{2(35)} = \\frac{30 \\pm \\sqrt{900-420}}{70} = \\frac{30 \\pm \\sqrt{480}}{70}.\n$$\nSince $\\sqrt{480} = \\sqrt{16 \\times 30} = 4\\sqrt{30}$, we have\n$$\ny = x^2 = \\frac{30 \\pm 4\\sqrt{30}}{70} = \\frac{15 \\pm 2\\sqrt{30}}{35}.\n$$\nThe four nodes, ordered from left to right, are $x_1  x_2  x_3  x_4$:\n$x_{1,4} = \\mp\\sqrt{\\frac{15+2\\sqrt{30}}{35}}$ (outer nodes) and $x_{2,3} = \\mp\\sqrt{\\frac{15-2\\sqrt{30}}{35}}$ (inner nodes).\n\nThe Gauss-Legendre weights $w_k$ can be calculated using the formula $w_k = \\frac{2}{(1-x_k^2)[P_n'(x_k)]^2}$. For $n=4$, we have $P_4'(x) = \\frac{1}{8}(140x^3-60x) = \\frac{5x}{2}(7x^2-3)$.\nA more convenient formula for the weights can be derived. Since $y_k=x_k^2$ are roots of $35y^2-30y+3=0$, we can express $y^2 = (30y-3)/35$. This leads to the identity $w_k = \\frac{1}{15(y_k - y_k^2)}$.\nWe have $y_k - y_k^2 = y_k - \\frac{30y_k-3}{35} = \\frac{35y_k - 30y_k + 3}{35} = \\frac{5y_k+3}{35}$.\nSo, the weight is given by the simple expression:\n$$\nw_k = \\frac{1}{15 \\left( \\frac{5y_k+3}{35} \\right)} = \\frac{35}{15(5y_k+3)} = \\frac{7}{3(5y_k+3)}.\n$$\nWe now compute the two distinct weight values. For the outer nodes $x_1, x_4$, we use $y_{outer} = \\frac{15+2\\sqrt{30}}{35}$:\n$$\nw_{outer} = \\frac{7}{3\\left(5\\left(\\frac{15+2\\sqrt{30}}{35}\\right)+3\\right)} = \\frac{7}{3\\left(\\frac{15+2\\sqrt{30}}{7}+3\\right)} = \\frac{7}{3\\left(\\frac{15+2\\sqrt{30}+21}{7}\\right)} = \\frac{49}{3(36+2\\sqrt{30})} = \\frac{49}{6(18+\\sqrt{30})}.\n$$\nRationalizing the denominator:\n$$\nw_{outer} = \\frac{49(18-\\sqrt{30})}{6(18+\\sqrt{30})(18-\\sqrt{30})} = \\frac{49(18-\\sqrt{30})}{6(18^2 - 30)} = \\frac{49(18-\\sqrt{30})}{6(324 - 30)} = \\frac{49(18-\\sqrt{30})}{6(294)} = \\frac{49(18-\\sqrt{30})}{6(6 \\times 49)} = \\frac{18-\\sqrt{30}}{36}.\n$$\nFor the inner nodes $x_2, x_3$, we use $y_{inner} = \\frac{15-2\\sqrt{30}}{35}$:\n$$\nw_{inner} = \\frac{7}{3\\left(5\\left(\\frac{15-2\\sqrt{30}}{35}\\right)+3\\right)} = \\frac{7}{3\\left(\\frac{15-2\\sqrt{30}}{7}+3\\right)} = \\frac{7}{3\\left(\\frac{15-2\\sqrt{30}+21}{7}\\right)} = \\frac{49}{3(36-2\\sqrt{30})} = \\frac{49}{6(18-\\sqrt{30})}.\n$$\nRationalizing the denominator:\n$$\nw_{inner} = \\frac{49(18+\\sqrt{30})}{6(18-\\sqrt{30})(18+\\sqrt{30})} = \\frac{49(18+\\sqrt{30})}{6(18^2-30)} = \\frac{49(18+\\sqrt{30})}{6(294)} = \\frac{18+\\sqrt{30}}{36}.\n$$\nThe diagonal entries of the mass matrix are these weights, ordered by the nodes $x_1x_2x_3x_4$:\n$M_{11} = w_1 = w_{outer} = \\frac{18-\\sqrt{30}}{36}$.\n$M_{22} = w_2 = w_{inner} = \\frac{18+\\sqrt{30}}{36}$.\n$M_{33} = w_3 = w_{inner} = \\frac{18+\\sqrt{30}}{36}$.\n$M_{44} = w_4 = w_{outer} = \\frac{18-\\sqrt{30}}{36}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{18-\\sqrt{30}}{36}  \\frac{18+\\sqrt{30}}{36}  \\frac{18+\\sqrt{30}}{36}  \\frac{18-\\sqrt{30}}{36} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "To solve problems on complex physical domains, high-order methods map a simple reference element, like a square, to curved elements in physical space. This geometric transformation requires a corresponding transformation of differential operators, such as the gradient, which is governed by the mapping's Jacobian matrix. This exercise will walk you through the fundamental derivation of this gradient transformation from the chain rule and its application in a practical scenario. ",
            "id": "4089509",
            "problem": "Consider a two-dimensional curvilinear isoparametric mapping used in high-order Discontinuous Galerkin (DG) and Spectral Element Method (SEM) discretizations for computational complex fluids. Let the reference element be the square $[-1,1] \\times [-1,1]$ with coordinates $\\boldsymbol{\\xi} = (\\xi,\\eta)$, and let the physical coordinates be $\\boldsymbol{x} = (x,y)$ given by the smooth mapping $\\boldsymbol{x} = \\boldsymbol{X}(\\boldsymbol{\\xi})$ with components\n$$\nx(\\xi,\\eta) = \\xi + \\frac{1}{5}\\,\\eta + \\frac{1}{10}\\,\\xi\\,\\eta + \\frac{1}{20}\\,(1 - \\xi^{2}) + \\frac{1}{12}\\,\\eta^{2},\n$$\n$$\ny(\\xi,\\eta) = \\frac{3}{10}\\,\\xi + \\eta + \\frac{3}{25}\\,\\xi\\,\\eta + \\frac{7}{100}\\,\\xi^{2} - \\frac{1}{20}\\,(1 - \\eta^{2}).\n$$\nLet $\\phi(\\xi,\\eta)$ be a tensor-product spectral basis function constructed from Legendre polynomials, specifically\n$$\n\\phi(\\xi,\\eta) = L_{2}(\\xi)\\,L_{3}(\\eta),\n$$\nwhere $L_{2}(\\xi) = \\frac{1}{2}\\,(3\\xi^{2} - 1)$ and $L_{3}(\\eta) = \\frac{1}{2}\\,(5\\eta^{3} - 3\\eta)$.\n\nStarting exclusively from the multivariable chain rule and the definition of the Jacobian matrix of the mapping, perform the following:\n\n1. Derive the operator that maps the reference gradient $\\nabla_{\\boldsymbol{\\xi}}$ of a scalar field to its physical gradient $\\nabla_{\\boldsymbol{x}}$ under the mapping $\\boldsymbol{X}(\\boldsymbol{\\xi})$. Your derivation must show how the transformation arises from first principles, and it must explicitly identify the role of the Jacobian matrix and its inverse transpose, without invoking any pre-stated transformation formulas.\n\n2. Using your result, evaluate the physical gradient $\\nabla_{\\boldsymbol{x}} \\phi$ at the reference point $(\\xi,\\eta) = \\left(\\frac{1}{2}, -\\frac{1}{3}\\right)$. Express your final answer as exact rational numbers.\n\nProvide the final result for the physical gradient as a row vector using the LaTeX $\\mathrm{pmatrix}$ environment. No rounding is required and no physical units are involved.",
            "solution": "The problem is validated as well-posed, scientifically grounded, and self-contained. It is a standard problem in the framework of computational methods for partial differential equations, specifically concerning the geometric transformations required in finite element and spectral methods. We can therefore proceed with the solution.\n\nThe problem is divided into two parts: first, a derivation of the gradient transformation operator from first principles, and second, the application of this operator to compute a specific physical gradient.\n\n**Part 1: Derivation of the Gradient Transformation Operator**\n\nLet $u$ be a scalar field defined over the physical domain, such that its value at a point $\\boldsymbol{x}=(x,y)$ is $u(\\boldsymbol{x})$. The mapping $\\boldsymbol{x} = \\boldsymbol{X}(\\boldsymbol{\\xi})$ provides a relationship between the physical coordinates $\\boldsymbol{x}$ and the reference coordinates $\\boldsymbol{\\xi}=(\\xi,\\eta)$. The basis function $\\phi(\\boldsymbol{\\xi})$ is defined on the reference element. We consider the scalar field $u(\\boldsymbol{x})$ whose representation on the reference element is $\\phi(\\boldsymbol{\\xi})$, i.e., $u(\\boldsymbol{X}(\\boldsymbol{\\xi})) = \\phi(\\boldsymbol{\\xi})$. We seek to find the physical gradient, $\\nabla_{\\boldsymbol{x}} u = \\begin{pmatrix} \\partial u / \\partial x \\\\ \\partial u / \\partial y \\end{pmatrix}$, in terms of the reference gradient, $\\nabla_{\\boldsymbol{\\xi}} \\phi = \\begin{pmatrix} \\partial \\phi / \\partial \\xi \\\\ \\partial \\phi / \\partial \\eta \\end{pmatrix}$. For notational simplicity as is common in the field, we will denote $\\nabla_{\\boldsymbol{x}}u$ by $\\nabla_{\\boldsymbol{x}}\\phi$.\n\nThe derivation proceeds from the multivariable chain rule. The partial derivatives of $\\phi$ with respect to the reference coordinates $\\xi$ and $\\eta$ are expressed in terms of the partial derivatives of $u$ with respect to the physical coordinates $x$ and $y$:\n$$\n\\frac{\\partial \\phi}{\\partial \\xi} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\xi} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial \\phi}{\\partial \\eta} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\eta} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\eta}\n$$\nThis is a system of two linear equations for the unknown components of the physical gradient, $\\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial u}{\\partial y}$. We can write this system in matrix form:\n$$\n\\begin{pmatrix} \\frac{\\partial \\phi}{\\partial \\xi} \\\\ \\frac{\\partial \\phi}{\\partial \\eta} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial x}{\\partial \\xi}  \\frac{\\partial y}{\\partial \\xi} \\\\ \\frac{\\partial x}{\\partial \\eta}  \\frac{\\partial y}{\\partial \\eta} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix}\n$$\nThe vector on the left is the reference gradient, $\\nabla_{\\boldsymbol{\\xi}} \\phi$. The vector on the far right is the physical gradient, $\\nabla_{\\boldsymbol{x}} u$. The matrix in the middle is recognized as the transpose of the Jacobian matrix of the coordinate transformation $\\boldsymbol{X}(\\boldsymbol{\\xi})$. The Jacobian matrix $\\boldsymbol{J}$ is defined as:\n$$\n\\boldsymbol{J} = \\frac{\\partial\\boldsymbol{x}}{\\partial\\boldsymbol{\\xi}} = \\begin{pmatrix} \\frac{\\partial x}{\\partial \\xi}  \\frac{\\partial x}{\\partial \\eta} \\\\ \\frac{\\partial y}{\\partial \\xi}  \\frac{\\partial y}{\\partial \\eta} \\end{pmatrix}\n$$\nIts transpose is:\n$$\n\\boldsymbol{J}^T = \\begin{pmatrix} \\frac{\\partial x}{\\partial \\xi}  \\frac{\\partial y}{\\partial \\xi} \\\\ \\frac{\\partial x}{\\partial \\eta}  \\frac{\\partial y}{\\partial \\eta} \\end{pmatrix}\n$$\nThus, the relationship between the gradients is:\n$$\n\\nabla_{\\boldsymbol{\\xi}} \\phi = \\boldsymbol{J}^T \\nabla_{\\boldsymbol{x}} \\phi\n$$\nTo find the physical gradient, we must invert this relationship. Assuming the Jacobian determinant is non-zero, the matrix $\\boldsymbol{J}^T$ is invertible. We can left-multiply by $(\\boldsymbol{J}^T)^{-1}$:\n$$\n\\nabla_{\\boldsymbol{x}} \\phi = (\\boldsymbol{J}^T)^{-1} \\nabla_{\\boldsymbol{\\xi}} \\phi\n$$\nThis equation defines the operator that maps the reference gradient to the physical gradient. The operator is multiplication by the inverse transpose of the Jacobian matrix of the mapping. This completes the derivation from first principles.\n\n**Part 2: Evaluation of the Physical Gradient**\n\nWe now apply this result to the specific functions and point given in the problem.\n\nFirst, we compute the components of the Jacobian matrix $\\boldsymbol{J}$ by differentiating the mapping functions:\n$x(\\xi,\\eta) = \\xi + \\frac{1}{5}\\,\\eta + \\frac{1}{10}\\,\\xi\\,\\eta + \\frac{1}{20}\\,(1 - \\xi^{2}) + \\frac{1}{12}\\,\\eta^{2}$\n$y(\\xi,\\eta) = \\frac{3}{10}\\,\\xi + \\eta + \\frac{3}{25}\\,\\xi\\,\\eta + \\frac{7}{100}\\,\\xi^{2} - \\frac{1}{20}\\,(1 - \\eta^{2})$\n\nThe partial derivatives are:\n$J_{11} = \\frac{\\partial x}{\\partial \\xi} = 1 + \\frac{1}{10}\\eta - \\frac{1}{10}\\xi$\n$J_{12} = \\frac{\\partial x}{\\partial \\eta} = \\frac{1}{5} + \\frac{1}{10}\\xi + \\frac{1}{6}\\eta$\n$J_{21} = \\frac{\\partial y}{\\partial \\xi} = \\frac{3}{10} + \\frac{3}{25}\\eta + \\frac{7}{50}\\xi$\n$J_{22} = \\frac{\\partial y}{\\partial \\eta} = 1 + \\frac{3}{25}\\xi + \\frac{1}{10}\\eta$\n\nNext, we evaluate these components at the specified reference point $(\\xi,\\eta) = \\left(\\frac{1}{2}, -\\frac{1}{3}\\right)$:\n$J_{11} = 1 + \\frac{1}{10}(-\\frac{1}{3}) - \\frac{1}{10}(\\frac{1}{2}) = 1 - \\frac{1}{30} - \\frac{1}{20} = \\frac{60 - 2 - 3}{60} = \\frac{55}{60} = \\frac{11}{12}$\n$J_{12} = \\frac{1}{5} + \\frac{1}{10}(\\frac{1}{2}) + \\frac{1}{6}(-\\frac{1}{3}) = \\frac{1}{5} + \\frac{1}{20} - \\frac{1}{18} = \\frac{36 + 9 - 10}{180} = \\frac{35}{180} = \\frac{7}{36}$\n$J_{21} = \\frac{3}{10} + \\frac{3}{25}(-\\frac{1}{3}) + \\frac{7}{50}(\\frac{1}{2}) = \\frac{3}{10} - \\frac{1}{25} + \\frac{7}{100} = \\frac{30 - 4 + 7}{100} = \\frac{33}{100}$\n$J_{22} = 1 + \\frac{3}{25}(\\frac{1}{2}) + \\frac{1}{10}(-\\frac{1}{3}) = 1 + \\frac{3}{50} - \\frac{1}{30} = \\frac{150 + 9 - 5}{150} = \\frac{154}{150} = \\frac{77}{75}$\n\nSo the Jacobian matrix at the point is $\\boldsymbol{J} = \\begin{pmatrix} \\frac{11}{12}  \\frac{7}{36} \\\\ \\frac{33}{100}  \\frac{77}{75} \\end{pmatrix}$.\n\nThe determinant of the Jacobian is:\n$\\det(\\boldsymbol{J}) = (\\frac{11}{12})(\\frac{77}{75}) - (\\frac{7}{36})(\\frac{33}{100}) = \\frac{847}{900} - \\frac{231}{3600} = \\frac{4 \\cdot 847 - 231}{3600} = \\frac{3388 - 231}{3600} = \\frac{3157}{3600}$.\nSince the determinant is non-zero, the inverse exists.\n\nThe transformation requires $(\\boldsymbol{J}^T)^{-1} = (\\boldsymbol{J}^{-1})^T$. We first find $\\boldsymbol{J}^{-1}$:\n$\\boldsymbol{J}^{-1} = \\frac{1}{\\det(\\boldsymbol{J})} \\begin{pmatrix} J_{22}  -J_{12} \\\\ -J_{21}  J_{11} \\end{pmatrix} = \\frac{3600}{3157} \\begin{pmatrix} \\frac{77}{75}  -\\frac{7}{36} \\\\ -\\frac{33}{100}  \\frac{11}{12} \\end{pmatrix}$.\nThe inverse transpose is therefore:\n$(\\boldsymbol{J}^T)^{-1} = (\\boldsymbol{J}^{-1})^T = \\frac{3600}{3157} \\begin{pmatrix} \\frac{77}{75}  -\\frac{33}{100} \\\\ -\\frac{7}{36}  \\frac{11}{12} \\end{pmatrix}$.\n\nNext, we compute the reference gradient of $\\phi(\\xi,\\eta) = L_{2}(\\xi)\\,L_{3}(\\eta)$:\nThe components of the gradient are $\\frac{\\partial \\phi}{\\partial \\xi} = L_{2}'(\\xi)\\,L_{3}(\\eta)$ and $\\frac{\\partial \\phi}{\\partial \\eta} = L_{2}(\\xi)\\,L_{3}'(\\eta)$.\n$L_{2}(\\xi) = \\frac{1}{2}(3\\xi^2 - 1) \\implies L_{2}'(\\xi) = 3\\xi$\n$L_{3}(\\eta) = \\frac{1}{2}(5\\eta^3 - 3\\eta) \\implies L_{3}'(\\eta) = \\frac{1}{2}(15\\eta^2 - 3)$\n\nEvaluate these functions and their derivatives at $(\\xi,\\eta) = (\\frac{1}{2}, -\\frac{1}{3})$:\n$L_{2}(\\frac{1}{2}) = \\frac{1}{2}(3(\\frac{1}{2})^2 - 1) = \\frac{1}{2}(\\frac{3}{4} - 1) = -\\frac{1}{8}$\n$L_{2}'(\\frac{1}{2}) = 3(\\frac{1}{2}) = \\frac{3}{2}$\n$L_{3}(-\\frac{1}{3}) = \\frac{1}{2}(5(-\\frac{1}{3})^3 - 3(-\\frac{1}{3})) = \\frac{1}{2}(-\\frac{5}{27} + 1) = \\frac{1}{2}(\\frac{22}{27}) = \\frac{11}{27}$\n$L_{3}'(-\\frac{1}{3}) = \\frac{1}{2}(15(-\\frac{1}{3})^2 - 3) = \\frac{1}{2}(\\frac{15}{9} - 3) = \\frac{1}{2}(\\frac{5}{3} - \\frac{9}{3}) = \\frac{1}{2}(-\\frac{4}{3}) = -\\frac{2}{3}$\n\nNow compute the components of the reference gradient:\n$\\frac{\\partial \\phi}{\\partial \\xi} = L_{2}'(\\frac{1}{2})\\,L_{3}(-\\frac{1}{3}) = (\\frac{3}{2})(\\frac{11}{27}) = \\frac{33}{54} = \\frac{11}{18}$\n$\\frac{\\partial \\phi}{\\partial \\eta} = L_{2}(\\frac{1}{2})\\,L_{3}'(-\\frac{1}{3}) = (-\\frac{1}{8})(-\\frac{2}{3}) = \\frac{2}{24} = \\frac{1}{12}$\nThe reference gradient is $\\nabla_{\\boldsymbol{\\xi}} \\phi = \\begin{pmatrix} 11/18 \\\\ 1/12 \\end{pmatrix}$.\n\nFinally, we compute the physical gradient $\\nabla_{\\boldsymbol{x}} \\phi = (\\boldsymbol{J}^T)^{-1} \\nabla_{\\boldsymbol{\\xi}} \\phi$:\n$$\n\\nabla_{\\boldsymbol{x}} \\phi = \\frac{3600}{3157} \\begin{pmatrix} \\frac{77}{75}  -\\frac{33}{100} \\\\ -\\frac{7}{36}  \\frac{11}{12} \\end{pmatrix} \\begin{pmatrix} \\frac{11}{18} \\\\ \\frac{1}{12} \\end{pmatrix}\n$$\nThe x-component is:\n$(\\nabla_{\\boldsymbol{x}} \\phi)_x = \\frac{3600}{3157} \\left( (\\frac{77}{75})(\\frac{11}{18}) - (\\frac{33}{100})(\\frac{1}{12}) \\right) = \\frac{3600}{3157} \\left( \\frac{847}{1350} - \\frac{33}{1200} \\right) = \\frac{3600}{3157} \\left( \\frac{847}{1350} - \\frac{11}{400} \\right)$\nThe common denominator for $1350$ and $400$ is $10800$.\n$(\\nabla_{\\boldsymbol{x}} \\phi)_x = \\frac{3600}{3157} \\left( \\frac{847 \\cdot 8}{10800} - \\frac{11 \\cdot 27}{10800} \\right) = \\frac{3600}{3157} \\frac{6776 - 297}{10800} = \\frac{3600}{10800} \\frac{6479}{3157} = \\frac{1}{3} \\frac{6479}{3157}$.\nWe note the factorizations $6479 = 11 \\cdot 19 \\cdot 31$ and $3157 = 7 \\cdot 11 \\cdot 41$.\n$(\\nabla_{\\boldsymbol{x}} \\phi)_x = \\frac{1}{3} \\frac{11 \\cdot 19 \\cdot 31}{7 \\cdot 11 \\cdot 41} = \\frac{19 \\cdot 31}{3 \\cdot 7 \\cdot 41} = \\frac{589}{861}$.\n\nThe y-component is:\n$(\\nabla_{\\boldsymbol{x}} \\phi)_y = \\frac{3600}{3157} \\left( -(\\frac{7}{36})(\\frac{11}{18}) + (\\frac{11}{12})(\\frac{1}{12}) \\right) = \\frac{3600}{3157} \\left( -\\frac{77}{648} + \\frac{11}{144} \\right)$\nThe common denominator for $648$ and $144$ is $1296$.\n$(\\nabla_{\\boldsymbol{x}} \\phi)_y = \\frac{3600}{3157} \\left( \\frac{-77 \\cdot 2}{1296} + \\frac{11 \\cdot 9}{1296} \\right) = \\frac{3600}{3157} \\frac{-154 + 99}{1296} = \\frac{3600}{1296} \\frac{-55}{3157}$.\nSimplifying the pre-factor: $\\frac{3600}{1296} = \\frac{100 \\cdot 36}{36 \\cdot 36} = \\frac{100}{36} = \\frac{25}{9}$.\n$(\\nabla_{\\boldsymbol{x}} \\phi)_y = \\frac{25}{9} \\frac{-55}{3157} = -\\frac{1375}{9 \\cdot 3157}$.\nUsing the factorizations $1375 = 11 \\cdot 125$ and $3157 = 7 \\cdot 11 \\cdot 41$:\n$(\\nabla_{\\boldsymbol{x}} \\phi)_y = -\\frac{11 \\cdot 125}{9 \\cdot 7 \\cdot 11 \\cdot 41} = -\\frac{125}{9 \\cdot 7 \\cdot 41} = -\\frac{125}{2583}$.\n\nThe physical gradient at the specified point is a vector whose components are these two values.\n$\\nabla_{\\boldsymbol{x}} \\phi = \\left(\\frac{589}{861}, -\\frac{125}{2583}\\right)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{589}{861}  -\\frac{125}{2583} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key advantage of high-order methods on structured or semi-structured elements is their computational efficiency, which stems from exploiting the tensor-product nature of the basis functions. Instead of forming and storing large, dense system matrices, operators can be applied using a \"sum-factorization\" technique that has a much lower computational cost. This exercise quantifies that advantage by having you derive the operational cost for both the efficient tensor-product approach and a naive dense matrix multiplication, highlighting the scaling that makes high-order methods viable for large-scale 3D simulations. ",
            "id": "4089482",
            "problem": "Consider the viscous diffusion contribution to the momentum equations in computational complex fluids, discretized on a single hexahedral element using a high-order tensor-product basis. Let the element mapping be affine and the viscosity be spatially constant, so that in the reference coordinates the weak-form stiffness operator is represented, under Gauss–Lobatto–Legendre quadrature and Lagrange interpolation, by the sum of three tensor-product terms. Specifically, for polynomial degree $p$ per coordinate direction, let $n = p+1$ denote the number of nodes in one dimension, and let $D \\in \\mathbb{R}^{n \\times n}$ be the one-dimensional derivative matrix, and $W \\in \\mathbb{R}^{n \\times n}$ the diagonal matrix of one-dimensional quadrature weights. The three-dimensional stiffness operator on nodal values $u \\in \\mathbb{R}^{n \\times n \\times n}$ then acts, in a standard strong-symmetric spectral element implementation, as\n$$\nA u \\;=\\; \\left(D^{\\top} \\otimes I \\otimes I\\right)\\left(W \\otimes W \\otimes W\\right)\\left(D \\otimes I \\otimes I\\right) u \\;+\\; \\left(I \\otimes D^{\\top} \\otimes I\\right)\\left(W \\otimes W \\otimes W\\right)\\left(I \\otimes D \\otimes I\\right) u \\;+\\; \\left(I \\otimes I \\otimes D^{\\top}\\right)\\left(W \\otimes W \\otimes W\\right)\\left(I \\otimes I \\otimes D\\right) u,\n$$\nwhere $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix, and $\\otimes$ denotes the Kronecker product. Assume a floating-point operation (flop) model in which each scalar multiplication or addition counts as $1$ flop, and ignore memory traffic and coefficient precomputation costs.\n\nUsing only this tensor-product structure and first principles of high-order spectral element evaluation, derive the total flop count required to apply $A$ to a nodal vector $u$ on this single hexahedral element via sum-factorization (tensor-product evaluation). Then derive the flop count required to apply the same operator via naive dense matrix multiplication using a preassembled dense stiffness matrix of size $N \\times N$ with $N = n^{3}$. Finally, provide the simplified symbolic expression for the ratio $R(p)$ of the naive dense matrix-multiplication flop count to the tensor-product flop count, expressed solely as a function of $p$.\n\nYour final answer must be this ratio $R(p)$ as a single closed-form analytic expression. No numerical evaluation is required and no rounding is permitted.",
            "solution": "The user wants me to solve a problem.\n\n### Step 1: Extract Givens\n-   **Context**: Viscous diffusion in computational complex fluids on a single hexahedral element.\n-   **Discretization**: High-order tensor-product basis, affine element mapping, constant viscosity.\n-   **Polynomial Degree**: $p$.\n-   **Nodes per dimension**: $n = p+1$.\n-   **1D Derivative Matrix**: $D \\in \\mathbb{R}^{n \\times n}$.\n-   **1D Quadrature Weights Matrix**: $W \\in \\mathbb{R}^{n \\times n}$, diagonal.\n-   **Identity Matrix**: $I \\in \\mathbb{R}^{n \\times n}$.\n-   **Kronecker Product**: $\\otimes$.\n-   **Nodal Values Vector**: $u \\in \\mathbb{R}^{n \\times n \\times n}$.\n-   **Stiffness Operator Action**: The three-dimensional stiffness operator $A$ acts on $u$ as:\n    $$\n    A u \\;=\\; \\left(D^{\\top} \\otimes I \\otimes I\\right)\\left(W \\otimes W \\otimes W\\right)\\left(D \\otimes I \\otimes I\\right) u \\;+\\; \\left(I \\otimes D^{\\top} \\otimes I\\right)\\left(W \\otimes W \\otimes W\\right)\\left(I \\otimes D \\otimes I\\right) u \\;+\\; \\left(I \\otimes I \\otimes D^{\\top}\\right)\\left(W \\otimes W \\otimes W\\right)\\left(I \\otimes I \\otimes D\\right) u\n    $$\n-   **Flop Model**: Each scalar multiplication or addition counts as $1$ floating-point operation (flop). Memory traffic and precomputation costs are ignored.\n-   **Task 1**: Derive the total flop count for applying $A$ to $u$ using sum-factorization (tensor-product evaluation) based on the provided structure.\n-   **Task 2**: Derive the flop count for applying $A$ via naive dense matrix multiplication, where $A$ is a preassembled $N \\times N$ matrix with $N = n^{3}$.\n-   **Task 3**: Provide the simplified symbolic expression for the ratio $R(p)$ of the naive dense matrix-multiplication flop count to the tensor-product flop count, as a function of $p$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is set within the well-established field of numerical methods for partial differential equations, specifically the spectral element method. The operator definition corresponds to the strong form of the Laplacian operator on a tensor-product grid, a standard construct in this field. The concepts of Kronecker products, sum-factorization, and flop counting are standard in numerical linear algebra and scientific computing.\n-   **Well-Posed**: The problem requests the derivation of two flop counts and their ratio based on a clear set of definitions and a specified computational procedure. The tasks are mathematically well-defined and lead to a unique analytical solution.\n-   **Objective**: The language is precise and uses standard mathematical and computational terminology. There are no subjective or ambiguous statements.\n\nThe problem is self-contained, scientifically sound, and well-posed. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe problem requires a comparative analysis of two methods for applying the stiffness operator $A$. We will first derive the operational cost (flop count) for each method and then compute their ratio.\n\n**1. Flop Count for Tensor-Product Evaluation (Sum-Factorization)**\n\nThe operator $A$ is a sum of three terms, which we can denote as $A_x$, $A_y$, and $A_z$:\n$$A u = A_x u + A_y u + A_z u$$\nThe total flop count, which we'll call $Flop_{SF}$, is the sum of the flops required to compute each term and the flops required to sum the results.\n\nLet's analyze the cost of computing $y_x = A_x u$, where\n$$y_x = \\left(D^{\\top} \\otimes I \\otimes I\\right)\\left(W \\otimes W \\otimes W\\right)\\left(D \\otimes I \\otimes I\\right) u$$\nThe evaluation of this expression proceeds from right to left as a sequence of operator applications. The vector $u$ represents nodal values on an $n \\times n \\times n$ grid.\n\n-   **Step 1: Apply $D \\otimes I \\otimes I$**\n    The operator $D \\otimes I \\otimes I$ applies the $n \\times n$ matrix $D$ along the first dimension of the $n \\times n \\times n$ data array $u$. This is equivalent to performing $n \\times n = n^2$ independent matrix-vector products of the form $D v$, where $v$ is a vector of length $n$.\n    The flop count for a single dense $n \\times n$ matrix-vector product is $n^2$ multiplications and $n(n-1)$ additions, totaling $n^2 + n(n-1) = 2n^2 - n$ flops.\n    Since this is performed for each of the $n^2$ \"columns\" in the first dimension, the total flop count for this step is:\n    $$Flop_1 = n^2 (2n^2 - n) = 2n^4 - n^3$$\n\n-   **Step 2: Apply $W \\otimes W \\otimes W$**\n    The matrix $W$ is diagonal. Therefore, the Kronecker product $W \\otimes W \\otimes W$ is also a diagonal matrix, of size $n^3 \\times n^3$. Applying a diagonal matrix to a vector involves a pointwise multiplication. The vector resulting from Step 1 has $n^3$ elements.\n    The total flop count for this step is $n^3$ multiplications:\n    $$Flop_2 = n^3$$\n\n-   **Step 3: Apply $D^{\\top} \\otimes I \\otimes I$**\n    This step is analogous to Step 1, but with the matrix $D^{\\top}$. Since $D^{\\top}$ is also a general $n \\times n$ matrix, applying it has the same cost as applying $D$. The operator acts on the $n^3$ vector resulting from Step 2. This involves $n^2$ matrix-vector products.\n    The total flop count for this step is:\n    $$Flop_3 = n^2 (2n^2 - n) = 2n^4 - n^3$$\n\nThe total cost to compute $y_x = A_x u$ is the sum of the costs of these three steps:\n$$Flop(A_x u) = Flop_1 + Flop_2 + Flop_3 = (2n^4 - n^3) + n^3 + (2n^4 - n^3) = 4n^4 - n^3$$\nDue to the symmetric structure of the operator $A$, the costs for computing $A_y u$ and $A_z u$ are identical:\n$$Flop(A_y u) = Flop(A_z u) = 4n^4 - n^3$$\n\n-   **Step 4: Sum the results**\n    Finally, we must sum the three resulting vectors: $y_x + y_y + y_z$. Each vector has $n^3$ elements. This requires two vector additions.\n    The flop count for two $n^3$-element vector additions is:\n    $$Flop_{add} = 2n^3$$\nThe total flop count for the sum-factorization method is:\n$$Flop_{SF} = Flop(A_x u) + Flop(A_y u) + Flop(A_z u) + Flop_{add}$$\n$$Flop_{SF} = 3(4n^4 - n^3) + 2n^3 = 12n^4 - 3n^3 + 2n^3 = 12n^4 - n^3$$\n\n**2. Flop Count for Naive Dense Matrix Multiplication**\n\nIn this method, the operator $A$ is first assembled into a single dense matrix of size $N \\times N$, where $N = n^3$. The application of the operator is then a single matrix-vector product $A u$.\nThe flop count for multiplying an $N \\times N$ dense matrix with an $N$-vector is $N^2$ multiplications and $N(N-1)$ additions.\n$$Flop_{dense} = N^2 + N(N-1) = 2N^2 - N$$\nSubstituting $N = n^3$:\n$$Flop_{dense} = 2(n^3)^2 - n^3 = 2n^6 - n^3$$\n\n**3. Ratio of Flop Counts**\n\nThe problem asks for the ratio $R(p)$ of the naive dense matrix-multiplication flop count to the tensor-product flop count. We first find the ratio $R(n)$ as a function of $n$.\n$$R(n) = \\frac{Flop_{dense}}{Flop_{SF}} = \\frac{2n^6 - n^3}{12n^4 - n^3}$$\nWe can factor out $n^3$ from the numerator and the denominator:\n$$R(n) = \\frac{n^3(2n^3 - 1)}{n^3(12n - 1)} = \\frac{2n^3 - 1}{12n - 1}$$\nThe problem requires the ratio as a function of the polynomial degree $p$, where $n = p+1$. We substitute $n=p+1$ into the expression for $R(n)$:\n$$R(p) = \\frac{2(p+1)^3 - 1}{12(p+1) - 1}$$\nNow, we expand the terms. The numerator is:\n$$2(p+1)^3 - 1 = 2(p^3 + 3p^2 + 3p + 1) - 1 = 2p^3 + 6p^2 + 6p + 2 - 1 = 2p^3 + 6p^2 + 6p + 1$$\nThe denominator is:\n$$12(p+1) - 1 = 12p + 12 - 1 = 12p + 11$$\nCombining these, the final expression for the ratio $R(p)$ is:\n$$R(p) = \\frac{2p^3 + 6p^2 + 6p + 1}{12p + 11}$$",
            "answer": "$$\\boxed{\\frac{2p^{3} + 6p^{2} + 6p + 1}{12p + 11}}$$"
        }
    ]
}