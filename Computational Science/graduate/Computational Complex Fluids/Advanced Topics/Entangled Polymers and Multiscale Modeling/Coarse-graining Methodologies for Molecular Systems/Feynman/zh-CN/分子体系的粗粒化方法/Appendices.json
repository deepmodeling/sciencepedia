{
    "hands_on_practices": [
        {
            "introduction": "粗粒化建模的核心任务之一是寻找一个有效的相互作用势，以再现目标结构特性，例如径向分布函数 $g(r)$。本练习将介绍一种现代且强大的方法：使用一组灵活的基函数来表示势函数，并通过数值方法优化其参数 。通过完成一个简化的梯度下降优化步骤，您将亲身体验许多高级势函数拟合技术背后的核心逻辑，并将统计力学概念 ($g(r) \\approx \\exp(-\\beta u(r))$) 与数值优化方法联系起来。",
            "id": "4081602",
            "problem": "考虑一个处于正则系综中的分子系统，其逆温度为 $\\beta$。在低密度极限下，对于对相互作用势 $u(r)$，径向分布函数 (RDF) $g(r)$ 可由 Boltzmann 因子近似表示为 $g(r) \\approx \\exp(-\\beta u(r))$。在粗粒化中，假设对势表示为一组径向基函数 $\\{\\phi_k(r)\\}_{k=1}^{K}$ 的线性展开，其系数为 $\\theta_k$，即\n$$\nu(r; \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_k \\, \\phi_k(r).\n$$\n设目标函数为模型RDF $g(r_i;\\boldsymbol{\\theta})$ 与目标RDF $g_{\\mathrm{target}}(r_i)$ 在离散网格 $\\{r_i\\}_{i=1}^{M}$ 上的加权最小二乘失配，\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2,\n$$\n其中 $w_i$ 为非负权重。使用低密度近似 $g(r;\\boldsymbol{\\theta}) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k \\phi_k(r)\\right)$ 以及基于线性响应的 $g(r)$ 相对于 $\\theta_k$ 的灵敏度，推导梯度分量 $\\partial J / \\partial \\theta_k$ 并实现单步梯度下降更新\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\, \\frac{\\partial J}{\\partial \\theta_k},\n$$\n其中 $\\alpha$ 是学习率。\n\n所有距离 $r$、基函数参数和能量均在一致的约化单位制中处理，其中所有量均为无量纲。逆温度 $\\beta$ 是无量纲的，势 $u(r)$ 也是无量纲的。此问题不涉及角度。\n\n对于数值实现，使用由下式定义的 Gaussian 径向基函数\n$$\n\\phi_k(r) = \\exp\\left(-\\frac{(r - c_k)^2}{2\\sigma_k^2}\\right),\n$$\n其中 $c_k$ 和 $\\sigma_k$ 分别是第 $k$ 个基函数的中心和宽度。\n\n您的程序必须：\n- 对每个测试用例，计算模型 $g(r_i;\\boldsymbol{\\theta})$、梯度 $\\partial J / \\partial \\theta_k$ 和更新后的参数 $\\boldsymbol{\\theta}^{\\mathrm{new}}$。\n- 生成单行输出，其中包含所有测试用例的更新后参数向量，格式为方括号括起来的逗号分隔列表，每个测试用例的更新后参数向量表示为 Python 风格的列表，且每个参数四舍五入到六位小数（例如：“[[0.123456,-0.234567],[...],...]”）。\n\n测试套件：\n- 用例1（一般场景）：\n  - $\\beta = 1.0$。\n  - $K = 3$，中心 $\\boldsymbol{c} = [0.8, 1.4, 2.0]$，宽度 $\\boldsymbol{\\sigma} = [0.15, 0.25, 0.20]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [1.0, -0.5, 0.2]$。\n  - 网格点 $\\boldsymbol{r} = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]$。\n  - 权重 $\\boldsymbol{w} = [1, 1, 1, 1, 1, 1, 1, 1]$。\n  - 目标RDF由以下解析式定义\n    $$\n    g_{\\mathrm{target}}(r) = 1 - 0.4 \\exp\\left(-\\frac{(r - 1.2)^2}{0.25^2}\\right) + 0.1 \\exp\\left(-\\frac{(r - 1.8)^2}{0.1^2}\\right).\n    $$\n  - 学习率 $\\alpha = 0.05$。\n\n- 用例2（一致性检查，模型初始时与目标相等）：\n  - $\\beta = 1.5$。\n  - $K = 2$，中心 $\\boldsymbol{c} = [1.0, 1.5]$，宽度 $\\boldsymbol{\\sigma} = [0.3, 0.25]$。\n  - 真实参数 $\\boldsymbol{\\theta}^{\\mathrm{true}} = [0.3, -0.1]$，且初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = \\boldsymbol{\\theta}^{\\mathrm{true}}$。\n  - 网格点 $\\boldsymbol{r} = [0.8, 1.0, 1.2, 1.4, 1.6]$。\n  - 权重 $\\boldsymbol{w} = [1, 1, 1, 1, 1]$。\n  - 目标RDF由具有真实参数的模型定义：\n    $$\n    g_{\\mathrm{target}}(r) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k^{\\mathrm{true}} \\, \\phi_k(r)\\right).\n    $$\n  - 学习率 $\\alpha = 0.10$。\n\n- 用例3（高温极限）：\n  - $\\beta = 0.1$。\n  - $K = 2$，中心 $\\boldsymbol{c} = [1.0, 2.0]$，宽度 $\\boldsymbol{\\sigma} = [0.4, 0.4]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [1.0, 1.0]$。\n  - 网格点 $\\boldsymbol{r} = [0.5, 1.0, 1.5, 2.0]$。\n  - 权重 $\\boldsymbol{w} = [1, 1, 1, 1]$。\n  - 目标RDF是均匀的：\n    $$\n    g_{\\mathrm{target}}(r) = 1.0.\n    $$\n  - 学习率 $\\alpha = 0.10$。\n\n- 用例4（单点边缘情况）：\n  - $\\beta = 2.0$。\n  - $K = 1$，中心 $\\boldsymbol{c} = [1.0]$，宽度 $\\boldsymbol{\\sigma} = [0.2]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [0.5]$。\n  - 单网格点 $\\boldsymbol{r} = [1.0]$。\n  - 权重 $\\boldsymbol{w} = [2.0]$。\n  - 指定的目标RDF：\n    $$\n    g_{\\mathrm{target}}(1.0) = 0.7.\n    $$\n  - 学习率 $\\alpha = 0.20$。\n\n- 用例5（零权重边界情况）：\n  - $\\beta = 1.0$。\n  - $K = 2$，中心 $\\boldsymbol{c} = [0.7, 1.3]$，宽度 $\\boldsymbol{\\sigma} = [0.2, 0.2]$。\n  - 初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}} = [-0.2, 0.4]$。\n  - 网格点 $\\boldsymbol{r} = [0.6, 0.9, 1.2]$。\n  - 权重 $\\boldsymbol{w} = [0.0, 0.0, 0.0]$。\n  - 目标RDF由以下解析式定义\n    $$\n    g_{\\mathrm{target}}(r) = 1 - 0.3 \\exp\\left(-\\frac{(r - 1.0)^2}{0.2^2}\\right).\n    $$\n  - 学习率 $\\alpha = 0.10$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有提供的测试用例的更新后参数向量，格式为方括号括起来的逗号分隔列表。每个测试用例的结果必须是 Python 风格的浮点数列表，每个浮点数四舍五入到六位小数。例如：\n\"[[theta_case1_1,theta_case1_2,...],[theta_case2_1,theta_case2_2,...],...]\"。",
            "solution": "目标是推导最小二乘目标函数 $J(\\boldsymbol{\\theta})$ 的梯度，并用它对参数 $\\boldsymbol{\\theta}$ 执行单步梯度下降更新。该问题设置在粗粒化的背景下，其中模型势 $u(r; \\boldsymbol{\\theta})$ 被优化以再现目标径向分布函数 $g_{\\mathrm{target}}(r)$。\n\n首先，我们重申关键定义。对势被参数化为 $K$ 个基函数 $\\{\\phi_k(r)\\}_{k=1}^{K}$ 的线性组合：\n$$\nu(r; \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_k \\, \\phi_k(r)\n$$\n在低密度极限下，径向分布函数 (RDF) $g(r)$ 由 Boltzmann 因子近似：\n$$\ng(r; \\boldsymbol{\\theta}) = \\exp(-\\beta u(r; \\boldsymbol{\\theta})) = \\exp\\left(-\\beta \\sum_{k=1}^{K} \\theta_k \\phi_k(r)\\right)\n$$\n其中 $\\beta$ 是逆温度。\n\n目标函数 $J(\\boldsymbol{\\theta})$ 衡量模型RDF $g(r_i; \\boldsymbol{\\theta})$ 与目标RDF $g_{\\mathrm{target}}(r_i)$ 在 $M$ 个径向点 $\\{r_i\\}_{i=1}^{M}$ 的离散集合上的加权平方差：\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2\n$$\n其中 $w_i \\ge 0$ 是权重。\n\n我们的主要任务是计算 $J(\\boldsymbol{\\theta})$ 相对于每个参数 $\\theta_k$ 的梯度。我们应用链式法则来微分 $J(\\boldsymbol{\\theta})$：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\left( \\frac{1}{2}\\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right]^2 \\right)\n$$\n微分和求和算子可以互换：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} \\frac{1}{2} w_i \\cdot 2 \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\cdot \\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k}\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k}\n$$\n下一步是求模型RDF $g(r_i; \\boldsymbol{\\theta})$ 相对于 $\\theta_k$ 的偏导数。该项表示RDF对第 $k$ 个参数变化的灵敏度。再次对 $g(r_i; \\boldsymbol{\\theta})$ 的表达式使用链式法则：\n$$\n\\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\exp\\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right)\n$$\n$$\n= \\exp\\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right) \\cdot \\frac{\\partial}{\\partial \\theta_k} \\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right)\n$$\n第一项就是 $g(r_i; \\boldsymbol{\\theta})$。由于基函数 $\\phi_j(r_i)$ 不依赖于 $\\boldsymbol{\\theta}$ 且 $\\frac{\\partial \\theta_j}{\\partial \\theta_k} = \\delta_{jk}$（Kronecker delta），所以和式对 $\\theta_k$ 的导数是 $-\\beta \\phi_k(r_i)$。\n$$\n\\frac{\\partial}{\\partial \\theta_k} \\left(-\\beta \\sum_{j=1}^{K} \\theta_j \\phi_j(r_i)\\right) = -\\beta \\sum_{j=1}^{K} \\frac{\\partial \\theta_j}{\\partial \\theta_k} \\phi_j(r_i) = -\\beta \\phi_k(r_i)\n$$\n因此，灵敏度为：\n$$\n\\frac{\\partial g(r_i;\\boldsymbol{\\theta})}{\\partial \\theta_k} = g(r_i;\\boldsymbol{\\theta}) \\cdot \\left(-\\beta \\, \\phi_k(r_i)\\right) = -\\beta \\, \\phi_k(r_i) \\, g(r_i;\\boldsymbol{\\theta})\n$$\n将此结果代回 $J(\\boldsymbol{\\theta})$ 梯度的表达式中：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] \\left(-\\beta \\, \\phi_k(r_i) \\, g(r_i;\\boldsymbol{\\theta})\\right)\n$$\n为了清晰和便于计算实现，重新整理：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} = -\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}) \\phi_k(r_i)\n$$\n这是梯度向量 $\\nabla J$ 的第 $k$ 个分量的最终表达式。\n\n问题要求进行单步梯度下降更新。给定一组初始参数 $\\boldsymbol{\\theta}^{\\mathrm{old}}$ 和一个学习率 $\\alpha$，更新后的参数 $\\boldsymbol{\\theta}^{\\mathrm{new}}$ 按如下方式计算：\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\left. \\frac{\\partial J}{\\partial \\theta_k} \\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}^{\\mathrm{old}}}\n$$\n代入我们推导出的梯度：\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} - \\alpha \\left(-\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) \\phi_k(r_i)\\right)\n$$\n$$\n\\theta_k^{\\mathrm{new}} = \\theta_k^{\\mathrm{old}} + \\alpha\\beta \\sum_{i=1}^{M} w_i \\left[g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) - g_{\\mathrm{target}}(r_i)\\right] g(r_i;\\boldsymbol{\\theta}^{\\mathrm{old}}) \\phi_k(r_i)\n$$\n该公式提供了计算更新后参数的算法。\n\n数值实现过程如下：\n1.  对每个测试用例，定义输入参数 $\\beta$、$\\boldsymbol{c}$、$\\boldsymbol{\\sigma}$、$\\boldsymbol{\\theta}^{\\mathrm{old}}$、$\\boldsymbol{r}$、$\\boldsymbol{w}$ 和 $\\alpha$。\n2.  通过在每个网格点 $r_i$ 对每个基 $k$ 求值，将 Gaussian 径向基函数 $\\phi_k(r) = \\exp\\left(-\\frac{(r - c_k)^2}{2\\sigma_k^2}\\right)$ 离散化。这会产生一个 $M \\times K$ 矩阵 $\\Phi$，其中 $\\Phi_{ik} = \\phi_k(r_i)$。\n3.  计算每个网格点上的模型势：$\\boldsymbol{u}^{\\mathrm{model}} = \\boldsymbol{\\Phi} \\boldsymbol{\\theta}^{\\mathrm{old}}$。\n4.  计算每个网格点上的模型RDF：$g_i^{\\mathrm{model}} = \\exp(-\\beta u_i^{\\mathrm{model}})$。\n5.  根据测试用例的具体解析函数，计算每个网格点上的目标RDF $g_i^{\\mathrm{target}}$。\n6.  对每个参数 $\\theta_k$，使用推导出的公式计算梯度分量 $\\frac{\\partial J}{\\partial \\theta_k}$，并用 $\\boldsymbol{\\theta}^{\\mathrm{old}}$ 进行求值。\n7.  使用梯度下降法则更新每个参数 $\\theta_k$ 以获得 $\\theta_k^{\\mathrm{new}}$。\n8.  收集每个测试用例的结果向量 $\\boldsymbol{\\theta}^{\\mathrm{new}}$，并按规定格式化最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used and thus not imported.\n\ndef solve():\n    \"\"\"\n    Solves for the updated coarse-graining parameters using a single\n    gradient descent step for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (General scenario)\n        {\n            \"beta\": 1.0,\n            \"centers\": np.array([0.8, 1.4, 2.0]),\n            \"widths\": np.array([0.15, 0.25, 0.20]),\n            \"theta_old\": np.array([1.0, -0.5, 0.2]),\n            \"r_grid\": np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": lambda r: 1 - 0.4 * np.exp(-(r - 1.2)**2 / 0.25**2) + 0.1 * np.exp(-(r - 1.8)**2 / 0.1**2),\n            \"alpha\": 0.05,\n        },\n        # Case 2 (Consistency check: model equals target)\n        {\n            \"beta\": 1.5,\n            \"centers\": np.array([1.0, 1.5]),\n            \"widths\": np.array([0.3, 0.25]),\n            \"theta_old\": np.array([0.3, -0.1]),\n            \"r_grid\": np.array([0.8, 1.0, 1.2, 1.4, 1.6]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": 'self', # special keyword to use the model's own output as target\n            \"alpha\": 0.10,\n        },\n        # Case 3 (High-temperature limit)\n        {\n            \"beta\": 0.1,\n            \"centers\": np.array([1.0, 2.0]),\n            \"widths\": np.array([0.4, 0.4]),\n            \"theta_old\": np.array([1.0, 1.0]),\n            \"r_grid\": np.array([0.5, 1.0, 1.5, 2.0]),\n            \"weights\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"g_target_func\": lambda r: np.ones_like(r),\n            \"alpha\": 0.10,\n        },\n        # Case 4 (Single-point edge case)\n        {\n            \"beta\": 2.0,\n            \"centers\": np.array([1.0]),\n            \"widths\": np.array([0.2]),\n            \"theta_old\": np.array([0.5]),\n            \"r_grid\": np.array([1.0]),\n            \"weights\": np.array([2.0]),\n            \"g_target_func\": lambda r: np.array([0.7]),\n            \"alpha\": 0.20,\n        },\n        # Case 5 (Zero-weight boundary case)\n        {\n            \"beta\": 1.0,\n            \"centers\": np.array([0.7, 1.3]),\n            \"widths\": np.array([0.2, 0.2]),\n            \"theta_old\": np.array([-0.2, 0.4]),\n            \"r_grid\": np.array([0.6, 0.9, 1.2]),\n            \"weights\": np.array([0.0, 0.0, 0.0]),\n            \"g_target_func\": lambda r: 1 - 0.3 * np.exp(-(r - 1.0)**2 / 0.2**2),\n            \"alpha\": 0.10,\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        beta = case[\"beta\"]\n        c = case[\"centers\"]\n        sigma = case[\"widths\"]\n        theta_old = case[\"theta_old\"]\n        r = case[\"r_grid\"]\n        w = case[\"weights\"]\n        g_target_func = case[\"g_target_func\"]\n        alpha = case[\"alpha\"]\n\n        K = len(theta_old)\n        M = len(r)\n\n        # Step 2: Discretize basis functions into an M x K matrix Phi\n        # phi_matrix[i, k] = phi_k(r_i)\n        r_col = r[:, np.newaxis]  # Shape (M, 1)\n        c_row = c[np.newaxis, :]  # Shape (1, K)\n        sigma_row = sigma[np.newaxis, :] # Shape (1, K)\n        phi_matrix = np.exp(-((r_col - c_row)**2) / (2 * sigma_row**2))\n        \n        # Step 3: Compute model potential u(r)\n        u_model = phi_matrix @ theta_old\n        \n        # Step 4: Compute model RDF g(r)\n        g_model = np.exp(-beta * u_model)\n        \n        # Step 5: Compute target RDF g_target(r)\n        if g_target_func == 'self':\n            # For Case 2, the target is the model itself\n            g_target = g_model\n        else:\n            g_target = g_target_func(r)\n\n        # Step 6: Compute gradient of J\n        # grad_J_k = -beta * sum_i( w_i * (g_model_i - g_target_i) * g_model_i * phi_ik )\n        residuals = w * (g_model - g_target) * g_model\n        grad_J = -beta * phi_matrix.T @ residuals\n\n        # Step 7: Perform gradient descent update\n        theta_new = theta_old - alpha * grad_J\n        \n        results.append(theta_new)\n\n    # Format the final output string exactly as specified.\n    # No spaces after commas, 6 decimal places.\n    formatted_results = []\n    for res_vec in results:\n        string_vec = [f\"{x:.6f}\" for x in res_vec]\n        formatted_results.append(f\"[{','.join(string_vec)}]\")\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "诸如迭代玻尔兹曼反演 (IBI) 等基于结构的粗粒化方法虽然能很好地再现体系的对结构，但往往无法同时精确匹配压力等热力学性质。本练习直面这一常见挑战，展示了一种用于压力校正的标准技术 。您将学习如何通过向势函数添加一个简单的、可解析处理的校正项来调节系统压力，同时又不过多地干扰已匹配好的结构，并推导校正势参数与维里压力变化之间的解析关系，这是构建稳健粗粒化模型的关键技能。",
            "id": "4081636",
            "problem": "考虑一个各向同性、均匀的单组分粗粒化流体，其数密度为 $\\rho$，温度为 $T$。相互作用由一个对心粗粒化势 $u(r)$ 表示，该势在截断距离 $r_{c}$ 处被截断并移动至零。您使用迭代玻尔兹曼反演 (IBI) 来匹配目标径向分布函数 $g_{\\mathrm{target}}(r)$，但您也希望通过添加一个由标量参数控制的长程尾部来校正残余的压力不匹配。为了避免改变主要决定 $g(r)$ 的区域中的结构，您引入了一个光滑的压力校正尾部，该尾部仅在 $0  r_{t}  r_{c}$ 的区间 $[r_{t}, r_{c}]$ 内起作用，定义为\n$$\nu_{\\mathrm{tail}}(r) =\n\\begin{cases}\n-\\lambda \\left(1 - \\frac{r}{r_{c}}\\right),  r_{t} \\leq r \\leq r_{c}, \\\\\n0,  \\text{其它情况},\n\\end{cases}\n$$\n其中 $\\lambda$ 是一个小的、可调的能量尺度参数。进入 IBI 的总势变为 $u(r) = u_{0}(r) + u_{\\mathrm{tail}}(r)$，其中 $u_{0}(r)$ 是已有的粗粒化势。\n\n从具有对心力的流体的压力力学定义出发，并使用维里定理，推导一个仅由添加 $u_{\\mathrm{tail}}(r)$ 引起的维里压力变化 $\\Delta P(\\lambda)$ 的解析表达式。明确说明您使用的任何近似。特别是，假设结构区域 $r  r_{t}$ 保持不变，并且在区间 $[r_{t}, r_{c}]$ 内对结构接近理想状态，因此 $g(r) \\approx 1$。\n\n您的最终答案必须是关于 $\\Delta P(\\lambda)$ 的单个闭合形式解析表达式，用 $\\rho$、$r_{t}$、$r_{c}$ 和 $\\lambda$ 表示。如果代入数值，最终的压力变化需以焦耳每立方米为单位表示。无需四舍五入。",
            "solution": "该问题是有效的，因为它在统计力学中有科学依据，是适定的、客观的，并包含足够的信息以得到唯一解。所要求的近似在粗粒化建模的背景下是标准的。\n\n一个由通过对心势 $u(r)$ 相互作用的粒子组成的均匀、各向同性流体的压力 $P$，可以使用维里定理表示为一个理想气体项和一个由分子间力引起的超额项之和：\n$$\nP = \\rho k_{B} T + P_{\\mathrm{virial}}\n$$\n其中 $\\rho$ 是数密度，$k_{B}$ 是玻尔兹曼常数，$T$ 是绝对温度。维里压力或超额压力 $P_{\\mathrm{virial}}$ 由对-维里函数的积分给出：\n$$\nP_{\\mathrm{virial}} = -\\frac{1}{6} \\int_V \\rho^{(2)}(\\mathbf{r}_1, \\mathbf{r}_2) \\, \\mathbf{r}_{12} \\cdot \\nabla_1 u(|\\mathbf{r}_{12}|) \\, d\\mathbf{r}_1 d\\mathbf{r}_2\n$$\n对于一个均匀且各向同性的系统，双体密度 $\\rho^{(2)}(\\mathbf{r}_1, \\mathbf{r}_2)$ 可以写成 $\\rho^2 g(|\\mathbf{r}_{12}|)$，其中 $g(r)$ 是径向分布函数且 $r = |\\mathbf{r}_{12}|$。维里压力的表达式简化为：\n$$\nP_{\\mathrm{virial}} = -\\frac{2\\pi\\rho^2}{3} \\int_{0}^{\\infty} r^3 \\frac{du(r)}{dr} g(r) dr\n$$\n问题指明势在截断距离 $r_{c}$ 处被截断，因此积分上限为 $r_{c}$。总势由基础势 $u_{0}(r)$ 和压力校正尾部势 $u_{\\mathrm{tail}}(r)$ 相加得到，即 $u(r) = u_{0}(r) + u_{\\mathrm{tail}}(r)$。\n\n问题要求计算仅由添加 $u_{\\mathrm{tail}}(r)$ 引起的维里压力变化 $\\Delta P(\\lambda)$。这个量是直接源于尾部势项对总维里压力的贡献。通过将 $u_{\\mathrm{tail}}(r)$ 代入维里压力公式来计算：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2}{3} \\int_{0}^{r_{c}} r^3 \\frac{du_{\\mathrm{tail}}(r)}{dr} g(r) dr\n$$\n此处，$g(r)$ 是具有总势 $u(r)$ 的系统的径向分布函数。\n\n尾部势 $u_{\\mathrm{tail}}(r)$ 定义为：\n$$\nu_{\\mathrm{tail}}(r) =\n\\begin{cases}\n-\\lambda \\left(1 - \\frac{r}{r_{c}}\\right),  r_{t} \\leq r \\leq r_{c}, \\\\\n0,  \\text{其它情况},\n\\end{cases}\n$$\n其中 $0  r_{t}  r_{c}$。$u_{\\mathrm{tail}}(r)$ 关于 $r$ 的导数是：\n$$\n\\frac{du_{\\mathrm{tail}}(r)}{dr} = \\frac{d}{dr} \\left(-\\lambda + \\frac{\\lambda r}{r_c}\\right) = \\frac{\\lambda}{r_c}\n$$\n对于 $r \\in [r_{t}, r_{c}]$。在此区间外，导数为 $0$。因此，$\\Delta P(\\lambda)$ 的积分只需要在区间 $[r_{t}, r_{c}]$ 上进行计算：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2}{3} \\int_{r_{t}}^{r_{c}} r^3 \\left(\\frac{\\lambda}{r_c}\\right) g(r) dr\n$$\n问题提供了一个关键的近似：在区间 $[r_{t}, r_{c}]$ 内，对结构接近理想状态，这使我们可以设定 $g(r) \\approx 1$。应用此近似可简化积分为：\n$$\n\\Delta P(\\lambda) \\approx -\\frac{2\\pi\\rho^2}{3} \\int_{r_{t}}^{r_{c}} r^3 \\left(\\frac{\\lambda}{r_c}\\right) (1) dr\n$$\n我们可以将常数，包括参数 $\\lambda$，从积分中提出：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda}{3r_c} \\int_{r_{t}}^{r_{c}} r^3 dr\n$$\n现在，我们计算 $r^3$ 的定积分：\n$$\n\\int_{r_{t}}^{r_{c}} r^3 dr = \\left[ \\frac{r^4}{4} \\right]_{r_{t}}^{r_{c}} = \\frac{r_c^4}{4} - \\frac{r_t^4}{4} = \\frac{1}{4}(r_c^4 - r_t^4)\n$$\n将此结果代回到 $\\Delta P(\\lambda)$ 的表达式中：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda}{3r_c} \\left( \\frac{r_c^4 - r_t^4}{4} \\right)\n$$\n简化数值因子后，得到由尾部势引起的维里压力变化的最终解析表达式：\n$$\n\\Delta P(\\lambda) = -\\frac{2\\pi\\rho^2\\lambda(r_c^4 - r_t^4)}{12r_c} = -\\frac{\\pi\\rho^2\\lambda}{6r_c} (r_c^4 - r_t^4)\n$$\n该表达式仅依赖于指定的参数 $\\rho$、$r_{t}$、$r_{c}$ 和 $\\lambda$。问题指出，如果代入数值，压力变化应以焦耳每立方米为单位。单位检查证实了这一点：当 $\\rho$ 的单位为 $m^{-3}$，$\\lambda$ 的单位为 $J$，$r_t, r_c$ 的单位为 $m$ 时，$\\Delta P(\\lambda)$ 的单位是 $\\frac{(m^{-3})^2 J}{m} (m^4) = J \\cdot m^{-3}$。由于 $1 \\, J = 1 \\, N \\cdot m$，这等价于 $N \\cdot m^{-2}$，即帕斯卡 ($Pa$)，这是压力的标准国际单位制单位。",
            "answer": "$$\n\\boxed{-\\frac{\\pi \\rho^{2} \\lambda}{6 r_{c}} (r_{c}^{4} - r_{t}^{4})}\n$$"
        },
        {
            "introduction": "为何基于结构的粗粒化模型常常无法捕捉正确的动力学行为？答案在于积分掉微观自由度时所产生的摩擦力和随机力。本练习通过从一个微观哈密顿量出发，推导广义朗之万方程 (GLE)，深入探讨了这一现象背后的基本理论 。通过显式地消除谐振子热浴，您将亲眼见证记忆效应（记忆核 $K(t)$）和涨落力 ($R(t)$) 是如何自然产生的，并验证涨落-耗散定理 (FDT)，从而巩固对耗散与热噪声之间深刻联系的理解，而这正是高级动力学粗粒化方法的理论基石。",
            "id": "4081629",
            "problem": "考虑一个质量为 $M$ 的粗粒化 (CG) 坐标 $X(t)$，它在一个光滑势 $U(X)$ 下演化，并与一个谐振子热浴线性耦合，该热浴用于模拟未解析的分子自由度。该热浴由独立的振子组成，其坐标为 $q_j$，质量为 $m_j$，频率为 $\\omega_j$，耦合系数为 $c_j$。总哈密顿量为\n$$\nH = \\frac{P^{2}}{2M} + U(X) + \\sum_{j} \\left[ \\frac{p_j^{2}}{2 m_j} + \\frac{1}{2} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)^{2} \\right],\n$$\n其中包含了标准的抵消项，该项在消除热浴后能够保持裸势 $U(X)$。假设热浴初始处于温度为 $T$ 的正则系综平衡状态，其抽样独立于初始的 CG 坐标和动量，玻尔兹曼常数为 $k_B$。\n\n从哈密顿动力学和平衡平均值的基本定义出发，消除热浴自由度，得到 $X(t)$ 的广义朗之万方程 (GLE)，其形式为\n$$\nM \\ddot{X}(t) + U'(X(t)) + \\int_{0}^{t} K(t-s)\\, \\dot{X}(s)\\, ds = R(t),\n$$\n其中 $K(t)$ 是记忆核，$R(t)$ 是由热浴初始条件决定的随机力。然后，通过由下式定义的谱密度 $J(\\omega)$，在连续极限下对热浴进行建模\n$$\nJ(\\omega) \\equiv \\frac{\\pi}{2} \\sum_{j} \\frac{c_j^{2}}{m_j \\omega_j} \\, \\delta(\\omega - \\omega_j),\n$$\n并采用 Drude–Lorentz 形式\n$$\nJ(\\omega) = \\gamma M \\, \\omega \\, \\frac{\\omega_c^{2}}{\\omega^{2} + \\omega_c^{2}},\n$$\n其中摩擦标度 $\\gamma  0$，截止频率 $\\omega_c  0$。计算得到的记忆核 $K(t)$ 的闭合解析形式。利用谐振子热浴的正则系综平衡，验证解析的涨落-耗散定理 (FDT)，即随机力自相关满足\n$$\n\\langle R(t) R(0) \\rangle = k_B T \\, K(t).\n$$\n\n答案规格：\n- 报告无量纲记忆核 $\\tilde{K}(t) \\equiv K(t)/(M \\gamma \\omega_c)$ 作为最终答案。\n- 无需数值计算；请提供精确的解析表达式。",
            "solution": "该问题要求推导与谐振子热浴耦合的粗粒化 (CG) 坐标 $X(t)$ 的广义朗之万方程 (GLE)，计算特定谱密度下的记忆核 $K(t)$，并验证涨落-耗散定理 (FDT)。\n\n**GLE的推导**\n我们首先使用哈密顿方程从给定的哈密顿量推导运动方程。\nCG坐标 $X$ 的运动方程为：\n$M\\ddot{X} = -\\frac{\\partial H}{\\partial X} = -U'(X) - \\frac{\\partial}{\\partial X} \\sum_{j} \\frac{1}{2} m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)^{2}$\n$$M\\ddot{X}(t) = -U'(X(t)) + \\sum_{j} c_j \\left( q_j(t) - \\frac{c_j}{m_j \\omega_j^{2}} X(t) \\right) \\quad (*)$$\n每个热浴振子坐标 $q_j$ 的运动方程为：\n$m_j\\ddot{q}_j = -\\frac{\\partial H}{\\partial q_j} = -m_j \\omega_j^{2} \\left( q_j - \\frac{c_j}{m_j \\omega_j^{2}} X \\right)$\n$$\\ddot{q}_j(t) + \\omega_j^{2} q_j(t) = \\frac{c_j}{m_j} X(t)$$\n这是一个受迫谐振子的方程，其通解为：\n$$q_j(t) = q_j(0) \\cos(\\omega_j t) + \\frac{\\dot{q}_j(0)}{\\omega_j} \\sin(\\omega_j t) + \\frac{c_j}{m_j \\omega_j} \\int_{0}^{t} \\sin(\\omega_j (t-s)) X(s) ds$$\n将 $q_j(t)$ 的解代入方程 $(*)$，并对积分项进行分部积分，在消去由哈密顿量中抵消项产生的项后，可得到：\n$$M\\ddot{X}(t) + U'(X(t)) + \\int_0^t \\left(\\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j(t-s))\\right) \\dot{X}(s) ds = R(t)$$\n由此我们确定记忆核 $K(t)$ 和随机力 $R(t)$ 为：\n$$K(t) = \\sum_{j} \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t)$$\n$$R(t) = \\sum_{j} \\left[ c_j \\left(q_j(0) - \\frac{c_j X(0)}{m_j\\omega_j^2}\\right) \\cos(\\omega_j t) + \\frac{c_j p_j(0)}{m_j \\omega_j} \\sin(\\omega_j t) \\right]$$\n\n**记忆核的计算**\n使用谱密度 $J(\\omega)$，记忆核可写成积分形式：\n$$K(t) = \\frac{2}{\\pi} \\int_0^\\infty \\frac{J(\\omega)}{\\omega} \\cos(\\omega t) d\\omega$$\n代入给定的 Drude-Lorentz 谱密度 $J(\\omega) = \\gamma M \\omega \\frac{\\omega_c^2}{\\omega^2 + \\omega_c^2}$：\n$K(t) = \\frac{2 \\gamma M \\omega_c^2}{\\pi} \\int_0^\\infty \\frac{\\cos(\\omega t)}{\\omega^2 + \\omega_c^2} d\\omega$.\n这是一个标准积分，对于 $t \\ge 0$，其值为 $\\frac{\\pi}{2\\omega_c} e^{-\\omega_c t}$。因此，\n$$K(t) = \\frac{2 \\gamma M \\omega_c^2}{\\pi} \\left( \\frac{\\pi}{2\\omega_c} e^{-\\omega_c t} \\right) = M \\gamma \\omega_c e^{-\\omega_c t}$$\n\n**验证涨落-耗散定理**\nFDT 表明 $\\langle R(t) R(0) \\rangle = k_B T K(t)$。随机力的相关函数 $\\langle R(t) R(0) \\rangle$ 需要对处于正则系综平衡的热浴初始条件进行平均。利用热浴振子初始位置和动量的统计独立性以及能量均分定理（$\\frac{1}{2} m_j \\omega_j^2 \\langle (q'_j)^2 \\rangle = \\frac{1}{2} k_B T$，其中 $q'_j$ 是移位坐标），可以得到：\n$\\langle R(t) R(0) \\rangle = \\sum_j c_j^2 \\cos(\\omega_j t) \\langle (q'_j)^2 \\rangle = \\sum_j c_j^2 \\cos(\\omega_j t) \\frac{k_B T}{m_j \\omega_j^2}$\n$$ \\langle R(t) R(0) \\rangle = k_B T \\left( \\sum_j \\frac{c_j^2}{m_j \\omega_j^2} \\cos(\\omega_j t) \\right) = k_B T K(t) $$\nFDT 得以验证。\n\n**最终答案**\n问题要求的是无量纲记忆核 $\\tilde{K}(t) \\equiv K(t)/(M \\gamma \\omega_c)$。使用已计算出的 $K(t)$，我们得到：\n$$\\tilde{K}(t) = \\frac{M \\gamma \\omega_c e^{-\\omega_c t}}{M \\gamma \\omega_c} = e^{-\\omega_c t}$$",
            "answer": "$$\\boxed{\\exp(-\\omega_c t)}$$"
        }
    ]
}