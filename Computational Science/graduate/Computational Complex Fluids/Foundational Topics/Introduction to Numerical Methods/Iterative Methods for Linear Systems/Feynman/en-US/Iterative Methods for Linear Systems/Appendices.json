{
    "hands_on_practices": [
        {
            "introduction": "While the Jacobi and Gauss-Seidel methods provide a conceptual entry point to iterative solvers, their convergence is not guaranteed for all systems. A rigorous understanding of their behavior requires analyzing the properties of the associated iteration matrix, $T$. This exercise  provides a hands-on look at this analysis by tasking you with deriving the Jacobi iteration matrix for a non-diagonally dominant system and calculating its eigenvalues. The spectral radius, $\\rho(T)$, determined from these eigenvalues, is the key to proving whether the method will converge or diverge for any initial guess.",
            "id": "2182318",
            "problem": "An iterative method is a numerical procedure that generates a sequence of improving approximate solutions for a class of problems. For a linear system of equations $A\\mathbf{x} = \\mathbf{b}$, one such method is the Jacobi method. The matrix $A$ can be decomposed as $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strictly lower-triangular part of $A$, and $-U$ is the strictly upper-triangular part of $A$.\n\nThe Jacobi iteration is defined by the formula:\n$$D\\mathbf{x}^{(k+1)} = (L+U)\\mathbf{x}^{(k)} + \\mathbf{b}$$\nThis can be rewritten in the form $\\mathbf{x}^{(k+1)} = T_J \\mathbf{x}^{(k)} + \\mathbf{c}$, where $T_J = D^{-1}(L+U)$ is known as the Jacobi iteration matrix. The convergence of the method is determined by the properties of this matrix.\n\nConsider the following system of linear equations:\n$$\n\\begin{align*}\n2x_1 + 3x_2 = 5 \\\\\n4x_1 + 2x_2 = 6\n\\end{align*}\n$$\nDetermine the two eigenvalues of the Jacobi iteration matrix $T_J$ corresponding to this system. Present your answer as a row matrix containing the two eigenvalues, ordered in ascending numerical value.",
            "solution": "The given system of linear equations is:\n$$\n\\begin{align*}\n2x_1 + 3x_2 = 5 \\\\\n4x_1 + 2x_2 = 6\n\\end{align*}\n$$\nThis can be written in matrix form $A\\mathbf{x} = \\mathbf{b}$, where:\n$$A = \\begin{pmatrix} 2  3 \\\\ 4  2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ 6 \\end{pmatrix}$$\n\nTo find the Jacobi iteration matrix $T_J$, we first decompose the matrix $A$ into its diagonal ($D$), strictly lower-triangular ($-L$), and strictly upper-triangular ($-U$) parts.\n$$A = D - L - U$$\nFrom the matrix $A$, we identify:\n$$D = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$$\n$$L = \\begin{pmatrix} 0  0 \\\\ -4  0 \\end{pmatrix}$$\n$$U = \\begin{pmatrix} 0  -3 \\\\ 0  0 \\end{pmatrix}$$\n\nThe Jacobi iteration matrix is defined as $T_J = D^{-1}(L+U)$.\nFirst, we find the inverse of the diagonal matrix $D$:\n$$D^{-1} = \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix}$$\nNext, we compute the sum of $L$ and $U$:\n$$L+U = \\begin{pmatrix} 0  0 \\\\ -4  0 \\end{pmatrix} + \\begin{pmatrix} 0  -3 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  -3 \\\\ -4  0 \\end{pmatrix}$$\nNow, we can compute the iteration matrix $T_J$:\n$$T_J = D^{-1}(L+U) = \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix} \\begin{pmatrix} 0  -3 \\\\ -4  0 \\end{pmatrix} = \\begin{pmatrix} (1/2)(0) + (0)(-4)  (1/2)(-3) + (0)(0) \\\\ (0)(0) + (1/2)(-4)  (0)(-3) + (1/2)(0) \\end{pmatrix}$$\n$$T_J = \\begin{pmatrix} 0  -3/2 \\\\ -2  0 \\end{pmatrix}$$\n\nTo find the eigenvalues of $T_J$, we solve the characteristic equation $\\det(T_J - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$T_J - \\lambda I = \\begin{pmatrix} 0  -3/2 \\\\ -2  0 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} -\\lambda  -3/2 \\\\ -2  -\\lambda \\end{pmatrix}$$\nThe determinant is:\n$$\\det(T_J - \\lambda I) = (-\\lambda)(-\\lambda) - (-3/2)(-2) = \\lambda^2 - 3$$\nSetting the determinant to zero gives the characteristic equation:\n$$\\lambda^2 - 3 = 0$$\nSolving for $\\lambda$, we get:\n$$\\lambda^2 = 3$$\n$$\\lambda = \\pm\\sqrt{3}$$\nThe two eigenvalues are $\\sqrt{3}$ and $-\\sqrt{3}$.\n\nThe problem asks for the eigenvalues to be presented in a row matrix in ascending order.\nThe ascending order is $-\\sqrt{3}$, then $\\sqrt{3}$.\nThe spectral radius of the iteration matrix is $\\rho(T_J) = \\max(|\\sqrt{3}|, |-\\sqrt{3}|) = \\sqrt{3} \\approx 1.732$. Since $\\rho(T_J) > 1$, the Jacobi method will diverge for this system.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\sqrt{3}  \\sqrt{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving beyond stationary methods, the Conjugate Gradient (CG) algorithm offers significantly faster convergence for systems that are symmetric and positive-definite (SPD). Its elegance and efficiency, however, depend critically on the algebraic properties inherited from the SPD structure. This practice problem  challenges you to explore the theoretical underpinnings of CG by analyzing a case where the positive-definite property is violated, revealing from first principles why the minimization principle at the heart of the method breaks down.",
            "id": "3245204",
            "problem": "Consider applying the Conjugate Gradient (CG) method to the linear system $A x = b$ where $A$ is symmetric but not positive definite. The CG method is classically defined by minimizing the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ along search directions $p_k$, with residuals $r_k = b - A x_k$, and the initial search direction $p_0 = r_0$. From first principles, the step length $\\alpha_k$ is determined by minimizing $\\phi(x_k + \\alpha p_k)$ with respect to the scalar $\\alpha$.\n\nWork with the explicit example\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}, \n\\quad \nb = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \n\\quad \nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the definition of $\\phi(x)$ and the directional derivative condition for optimality along $p_k$, derive the expression for the step length $\\alpha_0$ in terms of $p_0$ and $r_0$ and identify the denominator that must be strictly positive when $A$ is positive definite. Then, compute the scalar $p_0^{\\mathsf{T}} A p_0$ for the given $A$, $b$, and $x_0$, and use this value to explain whether the CG step is well-defined in this case and why the method breaks down for this non-positive-definite $A$.\n\nYour final reported answer should be the numerical value of $p_0^{\\mathsf{T}} A p_0$. No rounding is required.",
            "solution": "The problem asks for an analysis of the Conjugate Gradient (CG) method applied to a linear system $A x = b$ where the matrix $A$ is symmetric but not positive definite. We will first derive the general formula for the step length $\\alpha_k$ from first principles, then apply it to the specific case provided to demonstrate the method's breakdown.\n\nThe CG method aims to find the solution $x$ by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. The gradient of this functional is $\\nabla \\phi(x) = A x - b$, which is the negative of the residual, $r = b - A x$. The method proceeds iteratively by generating a sequence of approximations $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is a search direction and $\\alpha_k$ is a step length chosen to minimize $\\phi(x_k + \\alpha_k p_k)$.\n\nTo derive $\\alpha_k$, we define a function of a single variable $\\alpha$:\n$$f(\\alpha) = \\phi(x_k + \\alpha p_k) = \\frac{1}{2} (x_k + \\alpha p_k)^{\\mathsf{T}} A (x_k + \\alpha p_k) - b^{\\mathsf{T}} (x_k + \\alpha p_k)$$\nExpanding the terms, we get:\n$$f(\\alpha) = \\frac{1}{2} (x_k^{\\mathsf{T}} A x_k + \\alpha x_k^{\\mathsf{T}} A p_k + \\alpha p_k^{\\mathsf{T}} A x_k + \\alpha^2 p_k^{\\mathsf{T}} A p_k) - b^{\\mathsf{T}} x_k - \\alpha b^{\\mathsf{T}} p_k$$\nSince $A$ is symmetric, $A = A^{\\mathsf{T}}$, the scalar quantity $p_k^{\\mathsf{T}} A x_k = (x_k^{\\mathsf{T}} A^{\\mathsf{T}} p_k)^{\\mathsf{T}} = (x_k^{\\mathsf{T}} A p_k)^{\\mathsf{T}} = x_k^{\\mathsf{T}} A p_k$. We can group terms by powers of $\\alpha$:\n$$f(\\alpha) = \\left(\\frac{1}{2} x_k^{\\mathsf{T}} A x_k - b^{\\mathsf{T}} x_k\\right) + \\alpha (x_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}} p_k) + \\frac{1}{2} \\alpha^2 (p_k^{\\mathsf{T}} A p_k)$$\nThe first term is $\\phi(x_k)$. The term linear in $\\alpha$ can be rewritten using the residual $r_k = b - A x_k$.\n$$x_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}} p_k = -(b - A x_k)^{\\mathsf{T}} p_k = -r_k^{\\mathsf{T}} p_k$$\nSo, the function to minimize is:\n$$f(\\alpha) = \\phi(x_k) - \\alpha r_k^{\\mathsf{T}} p_k + \\frac{1}{2} \\alpha^2 p_k^{\\mathsf{T}} A p_k$$\nTo find the minimum, we take the derivative with respect to $\\alpha$ and set it to zero, as dictated by the \"directional derivative condition for optimality\":\n$$\\frac{df}{d\\alpha} = -r_k^{\\mathsf{T}} p_k + \\alpha p_k^{\\mathsf{T}} A p_k = 0$$\nSolving for $\\alpha$ yields the step length $\\alpha_k$:\n$$\\alpha_k = \\frac{r_k^{\\mathsf{T}} p_k}{p_k^{\\mathsf{T}} A p_k}$$\nFor the first iteration ($k=0$), where the search direction $p_0$ is set to be the initial residual $r_0$, this becomes:\n$$\\alpha_0 = \\frac{r_0^{\\mathsf{T}} r_0}{r_0^{\\mathsf{T}} A r_0}$$\nThe denominator in this expression is $p_k^{\\mathsf{T}} A p_k$. For the CG method to be well-defined and for the step to correspond to a minimum of $\\phi$ along the search direction, the second derivative of $f(\\alpha)$ must be positive:\n$$\\frac{d^2f}{d\\alpha^2} = p_k^{\\mathsf{T}} A p_k > 0$$\nIf $A$ is symmetric positive definite (SPD), then by definition, $v^{\\mathsf{T}} A v > 0$ for any non-zero vector $v$. Since the search directions $p_k$ generated by the CG algorithm are non-zero (unless the exact solution has been found), the denominator $p_k^{\\mathsf{T}} A p_k$ is guaranteed to be strictly positive when $A$ is SPD.\n\nNow we analyze the specific case provided:\n$$A = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe matrix $A$ is symmetric, but its eigenvalues are $1$ and $-1$, so it is not positive definite.\nFirst, we compute the initial residual $r_0$:\n$$r_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nThe initial search direction is $p_0 = r_0$, so $p_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe problem asks to compute the scalar $p_0^{\\mathsf{T}} A p_0$, which is the denominator required to find the step length $\\alpha_0$.\n$$p_0^{\\mathsf{T}} A p_0 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nFirst, we compute the product $A p_0$:\n$$A p_0 = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\nNow we compute the final inner product:\n$$p_0^{\\mathsf{T}} (A p_0) = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (1)(1) + (1)(-1) = 1 - 1 = 0$$\nThus, the value of the denominator is $p_0^{\\mathsf{T}} A p_0 = 0$.\n\nThe CG step is not well-defined because calculating the step length $\\alpha_0$ requires division by this quantity. The numerator is $r_0^{\\mathsf{T}} p_0 = r_0^{\\mathsf{T}} r_0 = 1^2 + 1^2 = 2$. Therefore, $\\alpha_0 = \\frac{2}{0}$, which is undefined. The algorithm breaks down at the first iteration.\n\nThis breakdown occurs because the condition $p_0^{\\mathsf{T}} A p_0 > 0$ is violated. When $p_0^{\\mathsf{T}} A p_0 = 0$, the function $f(\\alpha) = \\phi(x_0 + \\alpha p_0)$ is no longer a convex quadratic. It becomes a linear function:\n$$f(\\alpha) = \\phi(x_0) - \\alpha (r_0^{\\mathsf{T}} p_0) = \\phi(x_0) - 2\\alpha$$\nA non-constant linear function has no minimum on the real line. The CG method fails because its core assumption—that it can find a minimum of the quadratic functional along the search direction—is not met. This demonstrates a fundamental reason why the standard CG method is restricted to systems with symmetric positive definite matrices.\nThe numerical value of $p_0^{\\mathsf{T}} A p_0$ is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "In computational complex fluids, discretized models for incompressible flows frequently yield large, sparse linear systems with a saddle-point structure. These systems are inherently indefinite, precluding the direct use of methods like standard CG and necessitating sophisticated preconditioning techniques. This problem  brings you to the forefront of practical numerical methods by having you analyze a block preconditioner for a representative saddle-point system, exploring how regularization can stabilize the critical pressure Schur complement to ensure robust convergence.",
            "id": "4091477",
            "problem": "Consider the linear system arising from a discretized quasi-steady momentum balance in an incompressible complex fluid, written in $2 \\times 2$ saddle-point block form\n$$\n\\begin{pmatrix}\nA  B^{\\top} \\\\\nB  -C\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{u} \\\\\np\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{f} \\\\\ng\n\\end{pmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ represents a symmetric positive definite velocity block dominated by viscous diffusion, $B \\in \\mathbb{R}^{m \\times n}$ represents the discrete divergence, and $C \\in \\mathbb{R}^{m \\times m}$ represents any pressure stabilization (for an exactly divergence-free discretization, one may take $C = 0$). You are asked to construct a block Incomplete Lower–Upper (ILU) preconditioner and explain how block pivoting addresses near-singularity in the pressure Schur complement.\n\nStart from the fundamental block factorization principle for saddle-point systems and derive a block ILU(0) preconditioner that retains the block structure while using an incomplete Schur complement. In particular, consider a block pivoting strategy that replaces the block $A$ by a regularized block $A_{p} = A + \\epsilon I$, with $\\epsilon  0$ chosen to avoid division by a near-singular $A$ in the formation of the Schur complement. Explain, from first principles, how this block pivoting modifies the pressure Schur complement and why it mitigates near-singularity associated with the pressure null space.\n\nTo make this concrete, analyze a single-mode reduction in which the blocks are scalars extracted from Fourier analysis of a uniform grid discretization: take $A = a$, $B = b$, $C = c$, where $a = \\nu k^{2}$ with viscosity $ \\nu$, scalar wavenumber magnitude $k$, $b = k$, and $c = 0$. In this scalar setting, the block ILU preconditioner uses the incomplete Schur complement\n$$\n\\widehat{S} = -c - \\frac{b^{2}}{a_{p}} = - \\frac{k^{2}}{\\nu k^{2} + \\epsilon}.\n$$\nSuppose one wishes to achieve a target stabilized pressure Schur complement $\\widehat{S} = -\\sigma$ with $\\sigma = \\eta k^{2}$ for a given stabilization parameter $\\eta  0$ tied to a pressure mass scaling. Using $ \\nu = 1.0 \\times 10^{-3}$, $k = 10$, and $\\eta = 8.0$, determine the value of the block pivoting parameter $\\epsilon$ that makes $\\widehat{S} = -\\eta k^{2}$ hold exactly in this scalar-mode approximation.\n\nRound your answer for $\\epsilon$ to four significant figures. Express the final value of $\\epsilon$ as a dimensionless quantity.",
            "solution": "The problem asks for an analysis of a block preconditioner for a saddle-point system and to calculate a specific regularization parameter, $\\epsilon$.\n\nFirst, we understand the preconditioning strategy. The original system is:\n$$\n\\begin{pmatrix}\nA  B^{\\top} \\\\\nB  -C\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{u} \\\\\np\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{f} \\\\\ng\n\\end{pmatrix}\n$$\nSolving this system often involves the pressure Schur complement, $S = -C - B A^{-1} B^{\\top}$. Forming $A^{-1}$ is computationally expensive. The proposed strategy uses an approximate Schur complement based on a regularized block $A_p = A + \\epsilon I$. In the scalar-mode analysis, this translates to $a_p = a + \\epsilon$. This regularization makes the block $A_p$ better conditioned and ensures it is invertible, which in turn stabilizes the Schur complement.\n\nThe approximate Schur complement for the scalar case is given as:\n$$\n\\widehat{S} = -c - \\frac{b^2}{a_p}\n$$\nWe are provided the scalar equivalents from Fourier analysis: $a = \\nu k^2$, $b = k$, and $c = 0$. Substituting these into the expression for $\\widehat{S}$:\n$$\n\\widehat{S} = -0 - \\frac{k^2}{\\nu k^2 + \\epsilon} = -\\frac{k^2}{\\nu k^2 + \\epsilon}\n$$\nThe objective is to choose $\\epsilon$ such that this approximate Schur complement matches a target stabilized form, $\\widehat{S} = -\\sigma$, where $\\sigma = \\eta k^2$.\n\nWe set the two expressions for $\\widehat{S}$ equal to each other:\n$$\n-\\frac{k^2}{\\nu k^2 + \\epsilon} = -\\eta k^2\n$$\nGiven that $k=10$ (non-zero), we can divide both sides of the equation by $-k^2$:\n$$\n\\frac{1}{\\nu k^2 + \\epsilon} = \\eta\n$$\nNow, we solve this algebraic equation for $\\epsilon$:\n$$\n1 = \\eta (\\nu k^2 + \\epsilon)\n$$\n$$\n\\frac{1}{\\eta} = \\nu k^2 + \\epsilon\n$$\n$$\n\\epsilon = \\frac{1}{\\eta} - \\nu k^2\n$$\nThis is the symbolic expression for the block pivoting parameter $\\epsilon$. We now substitute the given numerical values:\n- $\\nu = 1.0 \\times 10^{-3}$\n- $k = 10$\n- $\\eta = 8.0$\n\nPlugging these values into the expression for $\\epsilon$:\n$$\n\\epsilon = \\frac{1}{8.0} - (1.0 \\times 10^{-3}) \\times (10)^2\n$$\n$$\n\\epsilon = 0.125 - (1.0 \\times 10^{-3}) \\times 100\n$$\n$$\n\\epsilon = 0.125 - 0.1\n$$\n$$\n\\epsilon = 0.025\n$$\nThe problem requires the answer to be expressed with four significant figures. As a dimensionless quantity, this is $0.02500$ or, in scientific notation, $2.500 \\times 10^{-2}$. The required answer format suggests scientific notation.",
            "answer": "$$\n\\boxed{2.500 \\times 10^{-2}}\n$$"
        }
    ]
}