{
    "hands_on_practices": [
        {
            "introduction": "We begin with a cornerstone of statistical mechanics: deriving the entropy of an ideal gas from first principles. This exercise guides you through counting microstates in the microcanonical ensemble, starting from the classical Hamiltonian. By performing this fundamental derivation , you will gain a deep appreciation for the role of phase space volume and the critical $1/N!$ correction factor that resolves the Gibbs paradox.",
            "id": "2650653",
            "problem": "Consider a classical, nonrelativistic, monatomic ideal gas of $N$ identical particles of mass $m$ confined in a container of volume $V$, with fixed total energy $E$ and no external fields. Work entirely within the microcanonical ensemble. Begin from first principles of classical Hamiltonian mechanics and the definition of microcanonical entropy. Treat the phase space as $6N$-dimensional with coordinates $(\\mathbf{q},\\mathbf{p})$, and use Liouville’s measure. Use the standard coarse-graining by Planck’s constant $h$ to render the microstate count dimensionless. \n\n(a) Carefully justify the appearance of the indistinguishability factor $1/N!$ in the classical counting of microstates on the grounds of overcounting due to particle permutations. Your justification should be based on the structure of phase space and symmetry under the permutation group, and it should resolve the Gibbs paradox by explaining why removal of a partition between two subsystems of identical gases at equal intensive variables does not produce an entropy increase.\n\n(b) Using the microcanonical definition of entropy, $S = k_{\\mathrm{B}} \\ln \\Omega$, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $\\Omega$ is an appropriate count of accessible microstates at fixed $E$, derive the explicit closed-form expression for the entropy $S(E,V,N)$ of this system. You must:\n- Start from the classical Hamiltonian $H = \\sum_{i=1}^{N} \\mathbf{p}_{i}^{2}/(2m)$.\n- Perform the full phase-space integration to obtain the energy-shell measure in terms of $E$, $V$, $N$, $m$, and $h$.\n- Treat indistinguishability via the factor $1/N!$ as justified in part (a).\n- For $N \\gg 1$, use Stirling-type asymptotics consistently and neglect only subextensive corrections smaller than order $N$ (e.g., terms scaling like $\\ln N$). State explicitly where such terms are dropped.\n\nProvide your final answer as a single closed-form expression involving only $E$, $V$, $N$, $m$, $h$, and $k_{\\mathrm{B}}$. Do not include any intermediate expressions or equations in the final boxed answer. No numerical evaluation or rounding is required.",
            "solution": "The problem as stated is scientifically sound, well-posed, and objective. It presents a standard, fundamental derivation in classical statistical mechanics. All necessary physical parameters and mathematical constraints are provided. The problem is therefore valid, and a solution will be provided.\n\n(a) Justification of the Indistinguishability Factor and Resolution of the Gibbs Paradox\n\nA microstate of the system is defined by a single point $(\\mathbf{q}, \\mathbf{p}) = (\\mathbf{q}_1, \\ldots, \\mathbf{q}_N, \\mathbf{p}_1, \\ldots, \\mathbf{p}_N)$ in the $6N$-dimensional phase space. The Hamiltonian of the system, $H = \\sum_{i=1}^{N} \\frac{\\mathbf{p}_{i}^{2}}{2m}$, is symmetric with respect to the permutation of the indices of the particles. This means that if we swap the coordinates and momenta of particle $i$ and particle $j$, the energy of the system remains unchanged.\n\nIf the $N$ particles were distinguishable, then a state described by the phase space point $(\\ldots, \\mathbf{q}_i, \\mathbf{p}_i, \\ldots, \\mathbf{q}_j, \\mathbf{p}_j, \\ldots)$ would be physically distinct from the state $(\\ldots, \\mathbf{q}_j, \\mathbf{p}_j, \\ldots, \\mathbf{q}_i, \\mathbf{p}_i, \\ldots)$. For a given set of $N$ position and momentum vectors, there are $N!$ ways to assign these vectors to the $N$ distinguishable particles. The classical phase space volume calculation for distinguishable particles counts these $N!$ permutations as distinct microstates.\n\nHowever, fundamental principles of quantum mechanics dictate that identical particles are truly indistinguishable. A physical state is completely specified by the set of occupied single-particle states, not by which particle occupies which state. Swapping two identical particles does not produce a new physical state. The classical framework must be corrected to be consistent with this fact, as classical statistical mechanics should emerge as a limit of quantum statistical mechanics. This correction, known as the Gibbs correction, is to divide the total number of classically counted states by $N!$, representing the number of permutations among the $N$ identical particles. This division rectifies the overcounting of states that are physically identical.\n\nThis correction is essential for resolving the Gibbs paradox. Consider a container of volume $2V$ divided by a partition into two equal halves, each of volume $V$. Each half contains an identical ideal gas with $N$ particles and energy $E$, meaning they are at the same temperature and pressure. The total entropy of the system before removing the partition is $S_{initial} = S(E,V,N) + S(E,V,N) = 2S(E,V,N)$.\n\nIf we omit the $1/N!$ factor, the entropy is not extensive. For example, it would contain a term proportional to $N k_{\\mathrm{B}} \\ln V$. The initial entropy would be $S_{initial} \\propto 2(N k_{\\mathrm{B}} \\ln V)$. After removing the partition, we have a single system with $2N$ particles in a volume $2V$. The final entropy would be $S_{final} \\propto (2N) k_{\\mathrm{B}} \\ln(2V)$. The change in entropy, the entropy of mixing, would be $\\Delta S = S_{final} - S_{initial} \\propto (2N)k_{\\mathrm{B}}\\ln(2V) - 2Nk_{\\mathrm{B}}\\ln(V) = 2Nk_{\\mathrm{B}}\\ln(2) > 0$. This implies a spontaneous entropy increase for a process that is macroscopically reversible and in which no net change occurs. This is the paradox.\n\nWhen the $1/N!$ factor is included correctly, the entropy becomes an extensive quantity. As will be derived below, the entropy has the form $S(E,V,N) = N k_{\\mathrm{B}} f(E/N, V/N)$, where the argument of the function $f$ involves only intensive variables. Specifically, it contains a term $N k_{\\mathrm{B}} \\ln(V/N)$.\nThe initial entropy is $S_{initial} = 2 S(E,V,N)$.\nThe final state has $2N$ particles in volume $2V$ with energy $2E$. Its entropy is $S_{final} = S(2E, 2V, 2N)$.\nBecause entropy is now extensive, $S(\\lambda E, \\lambda V, \\lambda N) = \\lambda S(E, V, N)$. Setting $\\lambda=2$, we find $S_{final} = 2S(E,V,N)$.\nThe change in entropy is $\\Delta S = S_{final} - S_{initial} = 2S(E,V,N) - 2S(E,V,N) = 0$. This correct result, $\\Delta S=0$, resolves the Gibbs paradox, confirming that mixing identical gases at the same intensive state is a reversible process with no change in entropy.\n\n(b) Derivation of the Entropy Expression\n\nWe begin with the microcanonical definition of entropy, $S = k_{\\mathrm{B}} \\ln \\Omega$, where $\\Omega$ is the number of accessible microstates. For a classical system, this is related to the volume of the accessible phase space. The Hamiltonian is $H = \\sum_{i=1}^{N} \\frac{\\mathbf{p}_{i}^{2}}{2m}$.\nThe number of states is computed by integrating over the phase space region consistent with the total energy $E$, and making the result dimensionless using Planck's constant $h$. We account for indistinguishability with the $1/N!$ factor. We first calculate the phase space volume $\\Phi(E)$ for states with energy less than or equal to $E$:\n$$ \\Phi(E) = \\frac{1}{N! h^{3N}} \\int_{H \\leq E} d^{3N}q \\, d^{3N}p $$\nSince the Hamiltonian does not depend on the positions $\\mathbf{q}_i$, the integral over the spatial coordinates is simply the volume of the container raised to the power of $N$:\n$$ \\int d^{3N}q = \\left( \\int_V d^3q \\right)^N = V^N $$\nThe momentum integral is over the region $\\sum_{i=1}^{N} \\mathbf{p}_{i}^{2} \\leq 2mE$. This corresponds to the volume of a $3N$-dimensional hypersphere in momentum space with radius $R_p = \\sqrt{2mE}$. The volume of a $d$-dimensional hypersphere of radius $R$ is given by $V_d(R) = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)} R^d$.\nFor our case, the dimension is $d = 3N$. The volume of the accessible momentum space is:\n$$ \\int_{\\sum \\mathbf{p}_i^2 \\le 2mE} d^{3N}p = \\frac{\\pi^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} (2mE)^{3N/2} $$\nCombining these results, the cumulative phase space volume is:\n$$ \\Phi(E) = \\frac{V^N}{N! h^{3N}} \\frac{\\pi^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} (2mE)^{3N/2} = \\frac{V^N}{N! h^{3N}} \\frac{(2\\pi mE)^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} $$\nThe number of microstates $\\Omega(E)$ is strictly the volume of a thin energy shell, $\\Omega(E) = (\\frac{\\partial \\Phi}{\\partial E})\\delta E$. However, for $N \\gg 1$, the function $\\Phi(E) \\propto E^{3N/2}$ grows extremely rapidly, so the overwhelming majority of states are located near the energy surface $E$. The entropy calculated using $S = k_{\\mathrm{B}} \\ln(\\Phi(E))$ differs from that using $S = k_{\\mathrm{B}} \\ln(\\Omega(E))$ by terms of order $k_{\\mathrm{B}} \\ln N$. The problem statement permits the neglect of such subextensive corrections. We therefore proceed with $S \\approx k_{\\mathrm{B}} \\ln(\\Phi(E))$.\n$$ \\frac{S}{k_{\\mathrm{B}}} = \\ln\\left[ \\frac{V^N (2\\pi mE)^{3N/2}}{N! h^{3N} \\Gamma(\\frac{3N}{2} + 1)} \\right] $$\n$$ \\frac{S}{k_{\\mathrm{B}}} = N \\ln V + \\frac{3N}{2}\\ln(2\\pi mE) - \\ln(N!) - 3N \\ln h - \\ln\\Gamma\\left(\\frac{3N}{2} + 1\\right) $$\nFor large $N$, we use Stirling's approximation $\\ln(x!) \\approx x \\ln x - x$. This also applies to the Gamma function, since $\\ln\\Gamma(x+1) = \\ln(x!)$.\n$$ \\ln(N!) \\approx N \\ln N - N $$\n$$ \\ln\\Gamma\\left(\\frac{3N}{2} + 1\\right) \\approx \\frac{3N}{2} \\ln\\left(\\frac{3N}{2}\\right) - \\frac{3N}{2} $$\nSubstituting these approximations into the expression for entropy:\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\ln V + \\frac{3N}{2}\\ln(2\\pi mE) - (N \\ln N - N) - 3N \\ln h - \\left(\\frac{3N}{2} \\ln\\left(\\frac{3N}{2}\\right) - \\frac{3N}{2}\\right) $$\nWe now group the terms:\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\ln V - N \\ln N + \\frac{3N}{2} \\ln E - \\frac{3N}{2} \\ln N + N + \\frac{3N}{2} + \\frac{3N}{2} \\ln(2\\pi m) - 3N \\ln h - \\frac{3N}{2} \\ln\\left(\\frac{3}{2}\\right) $$\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\ln\\left(\\frac{V}{N}\\right) + \\frac{3N}{2}\\ln\\left(\\frac{E}{N}\\right) + \\frac{5N}{2} + \\frac{3N}{2} \\left[ \\ln(2\\pi m) - 2 \\ln h - \\ln\\left(\\frac{3}{2}\\right) \\right] $$\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\left[ \\ln\\left(\\frac{V}{N}\\right) + \\frac{3}{2}\\ln\\left(\\frac{E}{N}\\right) + \\frac{3}{2} \\ln\\left(\\frac{2\\pi m \\cdot 2}{3 h^2}\\right) + \\frac{5}{2} \\right] $$\n$$ \\frac{S}{k_{\\mathrm{B}}} \\approx N \\left[ \\ln\\left(\\frac{V}{N}\\right) + \\frac{3}{2}\\ln\\left(\\frac{E}{N}\\right) + \\frac{3}{2} \\ln\\left(\\frac{4\\pi m}{3h^2}\\right) + \\frac{5}{2} \\right] $$\nFinally, we combine the logarithmic terms into a single argument to obtain the closed-form expression, known as the Sackur-Tetrode equation:\n$$ S = N k_{\\mathrm{B}} \\left[ \\ln\\left(\\frac{V}{N}\\right) + \\ln\\left(\\left(\\frac{E}{N}\\right)^{3/2}\\right) + \\ln\\left(\\left(\\frac{4\\pi m}{3h^2}\\right)^{3/2}\\right) + \\frac{5}{2} \\right] $$\n$$ S = N k_{\\mathrm{B}} \\left[ \\ln\\left( \\frac{V}{N} \\left(\\frac{4\\pi m E}{3N h^2}\\right)^{3/2} \\right) + \\frac{5}{2} \\right] $$\nThis expression for entropy is properly extensive, as demonstrated in part (a). All subextensive terms of order $\\ln N$ and lower have been systematically dropped through the use of Stirling's approximation, as required.",
            "answer": "$$ \\boxed{ N k_{\\mathrm{B}} \\left[ \\ln\\left( \\frac{V}{N} \\left( \\frac{4\\pi m E}{3N h^2} \\right)^{3/2} \\right) + \\frac{5}{2} \\right] } $$"
        },
        {
            "introduction": "How do different statistical ensembles describe phenomena like phase transitions, especially in the finite systems we simulate? This computational practice explores the signatures of a first-order transition from both the microcanonical and canonical perspectives. You will discover how a convex region in the microcanonical entropy $S(E)$ produces a \"back-bending\" caloric curve and corresponds to a bimodal energy distribution in the canonical ensemble , providing a powerful link between these two theoretical frameworks.",
            "id": "4093860",
            "problem": "Consider a finite complex fluid system with a discrete energy grid $E \\in [E_{\\min},E_{\\max}]$ and a specified model for its microcanonical density of states $\\Omega(E)$. In reduced units, define the microcanonical entropy as $S(E) = k_{\\mathrm{B}} \\ln \\Omega(E)$ with Boltzmann constant $k_{\\mathrm{B}} = 1$. The microcanonical temperature $T(E)$ is defined by the fundamental microcanonical relation $1/T(E) = \\partial S(E)/\\partial E$ at fixed particle number and volume. In canonical ensemble (CE), the energy distribution at inverse temperature $\\beta$ is $P_{\\beta}(E) \\propto \\Omega(E) \\exp(-\\beta E)$ and the canonical partition function is $Z(\\beta) = \\int \\Omega(E) \\exp(-\\beta E)\\,dE$, which you should approximate by a Riemann sum over the discrete energy grid. The appearance of back-bending in the microcanonical caloric curve $T(E)$ in finite systems indicates a first-order transition. Your task is to mathematically derive and computationally verify diagnostics that detect back-bending, working entirely from the above fundamental definitions, without introducing any shortcut formulas. All quantities are to be computed and reported in reduced units (that is, energies and temperatures are dimensionless with $k_{\\mathrm{B}}=1$), and any angle measure is not applicable here. \n\nYou will implement a program that, for each given test case of $\\Omega(E)$, constructs $S(E)$, computes $T(E)$ and its energy derivative, and proposes and evaluates diagnostics that indicate first-order phase transitions in finite systems. You must:\n- Start from the definitions of $S(E)$, $T(E)$, and $P_{\\beta}(E)$ and derive a diagnostic criterion that flags back-bending from the shape of $T(E)$, expressed in terms of derivatives of $S(E)$.\n- From your derivation, identify a rigorous diagnostic for negative microcanonical heat capacity, and compute its most negative value over the energy grid wherever it is defined and indicative of back-bending. Report this most negative value as a float in reduced units of energy per temperature.\n- From the equivalence between microcanonical and canonical ensembles via the Legendre transformation, define a canonical diagnostic based on multimodality of $P_{\\beta}(E)$. Construct a canonical $P_{\\beta}(E)$ at an inverse temperature $\\beta$ linked to the back-bending interval by a slope defined between endpoints of the interval in $S(E)$ space, and report whether $P_{\\beta}(E)$ is bimodal (two distinct local maxima) as a boolean.\n\nNumerically, use central finite differences on the energy grid $E$ to approximate $\\partial S/\\partial E$ and $\\partial^2 S/\\partial E^2$. Restrict attention to energies for which $T(E)$ is positive, and treat vanishing denominators carefully to avoid numerical instabilities. For canonical multimodality, search for local maxima of the discrete $P_{\\beta}(E)$ using comparisons of neighboring values on the grid. If no back-bending interval is found, set the canonical multimodality diagnostic to false and set the reported most negative microcanonical heat capacity to $0$.\n\nTest suite: Use an energy grid $E$ with $N=401$ points over $[0,400]$. Define the following three test densities of states as functions of $E$ via their entropies, then compute $\\Omega(E)$ accordingly (reduced units with $k_{\\mathrm{B}} = 1$):\n1. Mixture case (expected back-bending): $S(E) = \\ln\\!\\big(\\exp(S_1(E)) + \\exp(S_2(E))\\big)$ with\n   $S_1(E) = a_1 E - \\dfrac{(E-\\mu_1)^2}{2\\sigma_1^2} + c_1$ and $S_2(E) = a_2 E - \\dfrac{(E-\\mu_2)^2}{2\\sigma_2^2} + c_2$,\n   parameters $a_1 = 0.020$, $\\mu_1 = 110$, $\\sigma_1 = 60$, $c_1 = 0.0$, and $a_2 = 0.012$, $\\mu_2 = 290$, $\\sigma_2 = 80$, $c_2 = 1.0$.\n2. Single-phase case (no back-bending expected): $S(E) = S_1(E)$ with parameters $a = 0.018$, $\\mu = 200$, $\\sigma = 90$, $c = 0.0$.\n3. Weak-mixture case (borderline, likely no back-bending): $S(E) = \\ln\\!\\big(\\exp(S_1(E)) + \\exp(S_2(E))\\big)$ with\n   $S_1(E)$ parameters $a_1 = 0.018$, $\\mu_1 = 200$, $\\sigma_1 = 70$, $c_1 = 0.0$, and $S_2(E)$ parameters $a_2 = 0.0175$, $\\mu_2 = 240$, $\\sigma_2 = 70$, $c_2 = -2.0$.\n\nFor each test case, compute and return a triple of results in the following order:\n- A boolean indicating whether back-bending is present (true if there exists an interval of $E$ with positive $T(E)$ where $T(E)$ decreases with $E$).\n- A float equal to the most negative value of the microcanonical heat capacity over the energy grid where $T(E)$ is positive and the diagnostic is defined (use $0$ if no negative value is found).\n- A boolean indicating whether the canonical energy distribution $P_{\\beta}(E)$ at the inverse temperature set by the slope between the endpoints of the detected back-bending interval is bimodal; if no back-bending interval is found, report false.\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list of the three triples, enclosed in square brackets, such as \"[[b1,f1,b2],[b3,f3,b4],[b5,f5,b6]]\", where each $b_i$ is a boolean and each $f_i$ is a float. All energies and temperatures are in reduced units and any floats should be printed in standard decimal form without any unit symbol. No other text should be printed.",
            "solution": "The problem requires us to derive and computationally verify diagnostics for first-order phase transitions in finite systems, as indicated by the phenomenon of back-bending in the microcanonical caloric curve, $T(E)$. We are given the fundamental definitions of microcanonical entropy $S(E)$, temperature $T(E)$, and the canonical probability distribution $P_{\\beta}(E)$. All calculations are to be performed in reduced units where the Boltzmann constant $k_{\\mathrm{B}}=1$.\n\nOur approach involves three main steps for each test case:\n1.  Derive a diagnostic for back-bending based on derivatives of the microcanonical entropy $S(E)$.\n2.  From this, derive an expression for the microcanonical heat capacity $C_V(E)$ and identify its most negative value as a quantitative measure of the transition.\n3.  Establish a canonical ensemble diagnostic by examining the modality of the energy distribution $P_{\\beta}(E)$ at a specific inverse temperature $\\beta_{\\text{trans}}$ related to the back-bending region.\n\n**1. Microcanonical Diagnostics: Back-Bending and Negative Heat Capacity**\n\nThe microcanonical temperature $T(E)$ is defined by the fundamental relation:\n$$\n\\frac{1}{T(E)} = \\frac{\\partial S(E)}{\\partial E}\n$$\nAssuming the system has a physically meaningful positive temperature, $T(E) > 0$, it must be that $\\frac{\\partial S(E)}{\\partial E} > 0$. We can express the temperature as $T(E) = \\left(\\frac{\\partial S}{\\partial E}\\right)^{-1}$.\n\nBack-bending in the caloric curve refers to a region where the temperature decreases as energy increases, i.e., $\\frac{dT(E)}{dE}  0$. To find the condition for this, we differentiate $T(E)$ with respect to $E$ using the chain rule:\n$$\n\\frac{dT}{dE} = \\frac{d}{dE} \\left( \\left( \\frac{\\partial S}{\\partial E} \\right)^{-1} \\right) = -1 \\left( \\frac{\\partial S}{\\partial E} \\right)^{-2} \\left( \\frac{\\partial^2 S}{\\partial E^2} \\right)\n$$\nSince we are in a region where $T(E) > 0$, it follows that $\\frac{\\partial S}{\\partial E} > 0$. The term $-\\left(\\frac{\\partial S}{\\partial E}\\right)^{-2}$ is therefore strictly negative. Consequently, the sign of $\\frac{dT}{dE}$ is opposite to the sign of the second derivative of entropy, $\\frac{\\partial^2 S}{\\partial E^2}$.\n$$\n\\frac{dT}{dE}  0 \\quad \\iff \\quad \\frac{\\partial^2 S}{\\partial E^2} > 0\n$$\nPhysically, the entropy of a typical macroscopic system is a concave function of energy ($\\frac{\\partial^2 S}{\\partial E^2}  0$). The condition $\\frac{\\partial^2 S}{\\partial E^2} > 0$ indicates a region of local convexity in the entropy function, often termed a \"convex intruder.\" This local convexity is the microcanonical signature of a first-order-like phase transition in a finite system. Thus, our first diagnostic for back-bending is the existence of an energy interval where $\\frac{\\partial S}{\\partial E} > 0$ and $\\frac{\\partial^2 S}{\\partial E^2} > 0$.\n\nNext, we consider the microcanonical heat capacity, defined as $C_V(E) = \\frac{dE}{dT} = \\left(\\frac{dT}{dE}\\right)^{-1}$. Substituting our expression for $\\frac{dT}{dE}$:\n$$\nC_V(E) = \\frac{1}{-\\left(\\frac{\\partial S}{\\partial E}\\right)^{-2} \\left(\\frac{\\partial^2 S}{\\partial E^2}\\right)} = - \\frac{\\left(\\frac{\\partial S}{\\partial E}\\right)^2}{\\frac{\\partial^2 S}{\\partial E^2}}\n$$\nFrom this expression, it is evident that $C_V(E)$ is negative if and only if $\\frac{\\partial^2 S}{\\partial E^2} > 0$ (given that the numerator $\\left(\\frac{\\partial S}{\\partial E}\\right)^2$ is positive). The region of back-bending is therefore precisely the region of negative microcanonical heat capacity. Our second diagnostic is to compute the most negative value of $C_V(E)$ within this region. If no such region exists, the value is taken as $0$.\n\n**2. Canonical Diagnostic: Bimodal Energy Distribution**\n\nIn the canonical ensemble, a first-order phase transition is characterized by phase coexistence at the transition temperature $T_{\\text{trans}} = 1/\\beta_{\\text{trans}}$. This manifests as a bimodal energy probability distribution, $P_{\\beta_{\\text{trans}}}(E)$, with two peaks corresponding to the two coexisting phases (e.g., liquid and gas). The probability of observing the system with energy $E$ at inverse temperature $\\beta$ is given by:\n$$\nP_{\\beta}(E) \\propto \\Omega(E) e^{-\\beta E}\n$$\nUsing the definition of entropy, $S(E) = \\ln \\Omega(E)$ (with $k_{\\mathrm{B}}=1$), we can write this as:\n$$\nP_{\\beta}(E) \\propto e^{S(E) - \\beta E}\n$$\nThe convex intruder in $S(E)$ that causes back-bending in the microcanonical ensemble corresponds to the region of phase coexistence in the canonical ensemble. The problem specifies that the relevant inverse temperature, $\\beta_{\\text{trans}}$, should be determined by the slope of the chord connecting the endpoints of the back-bending interval on the $S(E)$ curve. Let the back-bending region span from energy $E_a$ to $E_b$. Then, $\\beta_{\\text{trans}}$ is given by a Maxwell-like construction:\n$$\n\\beta_{\\text{trans}} = \\frac{S(E_b) - S(E_a)}{E_b - E_a}\n$$\nOur third diagnostic is to construct the distribution $P_{\\beta_{\\text{trans}}}(E)$ and determine if it is bimodal, i.e., possesses exactly two distinct local maxima. If no back-bending interval exists, this diagnostic is reported as false.\n\n**3. Computational Strategy**\n\nFor each given entropy function $S(E)$:\n1.  Discretize the energy on the specified grid $E \\in [0, 400]$ with $N=401$ points.\n2.  Compute the array of entropy values $S$ for the grid points. For mixture cases, the numerically stable log-sum-exp formula, $\\ln(e^x + e^y) = \\max(x, y) + \\ln(1 + e^{-|x-y|})$, is used to prevent overflow.\n3.  Numerically compute the first and second derivatives, $\\frac{\\partial S}{\\partial E}$ and $\\frac{\\partial^2 S}{\\partial E^2}$, using central finite differences on the discrete grid.\n4.  Identify the back-bending region by finding all grid points where $\\frac{\\partial S}{\\partial E} > 0$ and $\\frac{\\partial^2 S}{\\partial E^2} > 0$.\n5.  If this region is non-empty, a back-bending is detected. The most negative value of $C_V(E)$ is then computed over this region. The endpoints $(E_a, S(E_a))$ and $(E_b, S(E_b))$ are identified to calculate $\\beta_{\\text{trans}}$. The canonical distribution $P_{\\beta_{\\text{trans}}}(E)$ is constructed, and the number of its local maxima is counted by comparing neighboring values. If the count is two, the distribution is bimodal.\n6.  If the back-bending region is empty, we report no back-bending, a heat capacity of $0$, and no bimodality.\n\nThis procedure, based directly on the fundamental principles of statistical mechanics, allows for a robust verification of the phase transition diagnostics.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the analysis for all test cases.\n    \"\"\"\n\n    def S_model(E, a, mu, sigma, c):\n        \"\"\"Standard model for the entropy of a single phase.\"\"\"\n        return a * E - np.power(E - mu, 2) / (2 * sigma**2) + c\n\n    def solve_case(S_func, E_grid):\n        \"\"\"\n        Analyzes a single case defined by an entropy function and an energy grid.\n        Derives and computes diagnostics for a first-order phase transition.\n        \"\"\"\n        E = E_grid\n        S = S_func(E)\n        h = E[1] - E[0]\n\n        # Use np.gradient for robust numerical differentiation (central differences)\n        dS_dE = np.gradient(S, h)\n        d2S_dE2 = np.gradient(dS_dE, h)\n\n        # Diagnostic 1: Back-bending presence.\n        # dT/dE  0 is equivalent to d2S/dE2  0 for T  0 (i.e., dS/dE  0).\n        # Use a small tolerance to avoid floating point issues near zero.\n        positive_dS_dE_mask = dS_dE  1e-12\n        back_bending_mask = (d2S_dE2  1e-12)  positive_dS_dE_mask\n        back_bending_present = np.any(back_bending_mask)\n\n        # Initialize results to default values for the no-transition case.\n        most_negative_cv = 0.0\n        is_bimodal = False\n\n        if not back_bending_present:\n            return [False, 0.0, False]\n\n        # If back-bending is present, calculate the diagnostics.\n        \n        # Diagnostic 2: Most negative microcanonical heat capacity.\n        # C_V = - (dS/dE)^2 / (d2S/dE2).\n        # This is only computed in the back-bending region where the denominator is positive.\n        cv_in_region = -np.power(dS_dE[back_bending_mask], 2) / d2S_dE2[back_bending_mask]\n        most_negative_cv = np.min(cv_in_region)\n\n        # Diagnostic 3: Canonical bimodality.\n        # Find endpoints of the back-bending interval.\n        back_bending_indices = np.where(back_bending_mask)[0]\n        idx_a = back_bending_indices[0]\n        idx_b = back_bending_indices[-1]\n\n        # Ensure we have a valid interval to define the slope for beta.\n        if idx_a = idx_b:\n            return [back_bending_present, most_negative_cv, False]\n\n        E_a, S_a = E[idx_a], S[idx_a]\n        E_b, S_b = E[idx_b], S[idx_b]\n\n        # Calculate inverse temperature beta_trans from the chord slope.\n        beta_trans = (S_b - S_a) / (E_b - E_a)\n\n        # Construct the unnormalized canonical probability distribution.\n        # P(E) propto exp(S(E) - beta*E).  Working with the log is more stable.\n        log_P = S - beta_trans * E\n\n        # Find the number of local maxima by comparing with neighbors.\n        num_maxima = 0\n        for i in range(1, len(log_P) - 1):\n            if log_P[i]  log_P[i-1] and log_P[i]  log_P[i+1]:\n                num_maxima += 1\n        \n        is_bimodal = (num_maxima == 2)\n        \n        return [back_bending_present, float(most_negative_cv), is_bimodal]\n\n    # Define energy grid and test cases.\n    E_grid = np.linspace(0, 400, 401)\n    \n    # Numerically stable log-sum-exp: log(exp(x) + exp(y))\n    def log_sum_exp(x, y):\n        maximum = np.maximum(x, y)\n        return maximum + np.log(1 + np.exp(-np.abs(x - y)))\n\n    # Test Case 1: Mixture case (expected back-bending)\n    params1 = {'a': 0.020, 'mu': 110, 'sigma': 60, 'c': 0.0}\n    params2 = {'a': 0.012, 'mu': 290, 'sigma': 80, 'c': 1.0}\n    s1 = S_model(E_grid, **params1)\n    s2 = S_model(E_grid, **params2)\n    S_func1 = lambda E: log_sum_exp(S_model(E, **params1), S_model(E, **params2))\n    \n    # Test Case 2: Single-phase case (no back-bending expected)\n    params3 = {'a': 0.018, 'mu': 200, 'sigma': 90, 'c': 0.0}\n    S_func2 = lambda E: S_model(E, **params3)\n\n    # Test Case 3: Weak-mixture case (borderline)\n    params4 = {'a': 0.018, 'mu': 200, 'sigma': 70, 'c': 0.0}\n    params5 = {'a': 0.0175, 'mu': 240, 'sigma': 70, 'c': -2.0}\n    S_func3 = lambda E: log_sum_exp(S_model(E, **params4), S_model(E, **params5))\n\n    test_cases = [S_func1, S_func2, S_func3]\n    \n    results = []\n    for s_func in test_cases:\n        # We need to pass the array, not the lambda, to solve_case\n        # because the lambda needs the grid which is defined locally.\n        # It's better to pass the lambda and the grid.\n        result = solve_case(s_func, E_grid)\n        results.append(result)\n\n    # Format the final output string as specified.\n    # The default str() for a list includes spaces, which is fine.\n    # The default str() for a bool is capitalized (True/False), which should also be fine.\n    # The problem skeleton's formatter is `','.join(map(str, results))`.\n    formatted_results = []\n    for r in results:\n        # Manual formatting to match the example style: lowercase bool, no spaces.\n        b1_str = str(r[0]).lower()\n        f1_str = f\"{r[1]:.7f}\"  # A reasonable precision for floats\n        b2_str = str(r[2]).lower()\n        formatted_results.append(f\"[{b1_str},{f1_str},{b2_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The grand canonical ensemble is particularly powerful for simulating mixtures and open systems, which are ubiquitous in the study of complex fluids. This practical exercise demonstrates how to transform raw data from a GCE simulation—the fluctuations in particle numbers—into key thermodynamic response functions. By analyzing a model histogram of fluctuations for a binary mixture , you will compute the isothermal compressibility and partial molar volumes, directly connecting microscopic fluctuations to macroscopic material properties.",
            "id": "4093890",
            "problem": "Consider a binary polymer solution composed of a solvent species $s$ and a polymer species $p$. You perform simulations in the Grand Canonical Ensemble (GCE), where the temperature $T$, volume $V$, and chemical potentials $\\mu_s$ and $\\mu_p$ are fixed, and the particle numbers $N_s$ and $N_p$ fluctuate. The grand canonical partition function $\\Xi(T,V,\\mu_s,\\mu_p)$ generates number fluctuations, and the pressure satisfies $pV = k_B T \\ln \\Xi$. In GCE, the average particle numbers satisfy $\\langle N_i \\rangle = \\frac{1}{\\beta} \\frac{\\partial \\ln \\Xi}{\\partial \\mu_i}$, and their covariances satisfy $\\langle \\Delta N_i \\Delta N_j \\rangle = \\frac{\\partial \\langle N_i \\rangle}{\\partial (\\beta \\mu_j)}$, where $\\beta = 1/(k_B T)$ and $k_B$ is the Boltzmann constant.\n\nA central goal in computational complex fluids is to extract bulk thermodynamic response properties, such as the isothermal compressibility $\\kappa_T$, and partial molar properties such as the partial molar volume $\\bar{v}_i$, directly from number fluctuation data. The isothermal compressibility is defined by $\\kappa_T = -\\frac{1}{V} \\left( \\frac{\\partial V}{\\partial p} \\right)_{T,\\{N_i\\}}$ and is a state function independent of ensemble. In GCE, $\\kappa_T$ can be related to number fluctuations. For mixtures, let $N_{\\mathrm{tot}} = N_s + N_p$, $\\boldsymbol{\\rho} = \\langle \\mathbf{N} \\rangle / V$ with $\\mathbf{N} = (N_s, N_p)^\\top$, and $C$ be the covariance matrix $C_{ij} = \\langle \\Delta N_i \\Delta N_j \\rangle$. The isothermal compressibility can be expressed as\n$$\n\\kappa_T = \\frac{\\mathrm{Var}(N_{\\mathrm{tot}})}{k_B T \\, V \\, \\rho_{\\mathrm{tot}}^2}\n$$\nwhere $\\mathrm{Var}(N_{\\mathrm{tot}}) = \\mathbf{1}^\\top C \\mathbf{1}$ and $\\rho_{\\mathrm{tot}} = \\rho_s + \\rho_p$. Partial molar volume is defined by $\\bar{v}_i = \\left( \\frac{\\partial V}{\\partial N_i} \\right)_{T,p,N_{j\\neq i}}$. Using linear response and the Gibbs-Duhem relation at constant temperature, a computational protocol that maps GCE number fluctuations to partial molar volumes is\n$$\nB = \\frac{C}{k_B T}, \\quad \\bar{\\mathbf{v}} = \\frac{B^{+} \\langle \\mathbf{N} \\rangle}{\\boldsymbol{\\rho}^\\top B^{+} \\langle \\mathbf{N} \\rangle}\n$$\nwhere $B^{+}$ denotes the Moore–Penrose pseudoinverse, and $\\bar{\\mathbf{v}} = (\\bar{v}_s, \\bar{v}_p)^\\top$. This returns $\\bar{v}_i$ in units of $\\mathrm{m}^3$ per molecule.\n\nYou are asked to implement a complete, runnable program that, given discrete number fluctuation histograms for $(N_s,N_p)$ from GCE simulations, computes $\\kappa_T$ and the partial molar volumes $\\bar{v}_s$ and $\\bar{v}_p$. Construct the discrete histograms using a bivariate normal weight evaluated on integer grids for $(N_s,N_p)$ and then normalized to a probability mass function. Use the following physically plausible test suite, where each case provides $T$ in $\\mathrm{K}$, $V$ in $\\mathrm{m}^3$, the mean vector $m = (\\langle N_s \\rangle, \\langle N_p \\rangle)$, the covariance matrix\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_s^2  \\sigma_{sp} \\\\\n\\sigma_{sp}  \\sigma_p^2\n\\end{pmatrix},\n$$\nand the grid ranges for $N_s$ and $N_p$:\n\n- Case $1$ (dilute polymer):\n  - $T = 300$, $V = 1 \\times 10^{-21}$\n  - $m = (100, 1)$\n  - $\\sigma_s^2 = 50$, $\\sigma_p^2 = 0.8$, $\\sigma_{sp} = -0.5$\n  - $N_s \\in \\{90,91,\\dots,110\\}$, $N_p \\in \\{0,1,2,3,4,5\\}$\n\n- Case $2$ (semi-dilute polymer):\n  - $T = 300$, $V = 2 \\times 10^{-21}$\n  - $m = (80, 20)$\n  - $\\sigma_s^2 = 60$, $\\sigma_p^2 = 15$, $\\sigma_{sp} = -8$\n  - $N_s \\in \\{60,61,\\dots,100\\}$, $N_p \\in \\{10,11,\\dots,30\\}$\n\n- Case $3$ (nearly ideal mixture):\n  - $T = 300$, $V = 1 \\times 10^{-21}$\n  - $m = (50, 5)$\n  - $\\sigma_s^2 = 45$, $\\sigma_p^2 = 5$, $\\sigma_{sp} = 0$\n  - $N_s \\in \\{35,36,\\dots,65\\}$, $N_p \\in \\{0,1,\\dots,12\\}$\n\n- Case $4$ (very dilute polymer, near singular covariance):\n  - $T = 300$, $V = 5 \\times 10^{-22}$\n  - $m = (120, 0.2)$\n  - $\\sigma_s^2 = 70$, $\\sigma_p^2 = 0.2$, $\\sigma_{sp} = -0.1$\n  - $N_s \\in \\{100,101,\\dots,140\\}$, $N_p \\in \\{0,1,2\\}$\n\nFor each case:\n- Construct the discrete histogram $P(N_s,N_p) \\propto \\exp\\left(-\\frac{1}{2} (\\mathbf{N}-m)^\\top \\Sigma^{-1} (\\mathbf{N}-m)\\right)$ over the specified integer grid and normalize it so that $\\sum_{N_s,N_p} P(N_s,N_p) = 1$.\n- Compute the sample mean vector $\\langle \\mathbf{N} \\rangle$ and covariance matrix $C$ from $P(N_s,N_p)$.\n- Compute the density vector $\\boldsymbol{\\rho} = \\langle \\mathbf{N} \\rangle / V$ and the total density $\\rho_{\\mathrm{tot}} = \\rho_s + \\rho_p$.\n- Compute the isothermal compressibility using $\\kappa_T = \\frac{\\mathbf{1}^\\top C \\mathbf{1}}{k_B T \\, V \\, \\rho_{\\mathrm{tot}}^2}$, and report it in units of $\\mathrm{Pa}^{-1}$ as a float.\n- Compute the partial molar volume vector using $\\bar{\\mathbf{v}} = \\frac{B^{+} \\langle \\mathbf{N} \\rangle}{\\boldsymbol{\\rho}^\\top B^{+} \\langle \\mathbf{N} \\rangle}$, with $B = C/(k_B T)$, and report $\\bar{v}_s$ and $\\bar{v}_p$ in $\\mathrm{nm}^3$ per molecule, where $1\\,\\mathrm{nm}^3 = 10^{-27}\\,\\mathrm{m}^3$.\n\nPhysical units requirement:\n- Report $\\kappa_T$ in $\\mathrm{Pa}^{-1}$.\n- Report $\\bar{v}_s$ and $\\bar{v}_p$ in $\\mathrm{nm}^3$ per molecule.\n\nYour program must produce a single line of output containing the results aggregated across all cases as a comma-separated list enclosed in square brackets. For each case, return a list with three floats in the order $[\\kappa_T,\\bar{v}_s,\\bar{v}_p]$, so the final output is a list of four such lists, for example $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$, where each $x_i$, $y_i$, and $z_i$ are the corresponding computed floats for case $i$.",
            "solution": "The problem has been analyzed and is deemed valid. It is scientifically grounded in the principles of statistical mechanics for open systems, specifically the Grand Canonical Ensemble (GCE). The provided formulas connecting particle number fluctuations to thermodynamic response functions like isothermal compressibility and partial molar volumes are standard results from linear response theory applied to fluid mixtures. The problem is well-posed, providing all necessary data and a clear, computationally tractable procedure. The parameters given in the test cases are physically plausible for simulations of binary solutions.\n\nThe objective is to compute the isothermal compressibility, $\\kappa_T$, and the partial molar volumes, $\\bar{v}_s$ and $\\bar{v}_p$, for a binary mixture based on a model of number fluctuations from GCE simulations. The fluctuation data is represented by a discrete probability mass function (PMF) constructed from a bivariate normal distribution over a specified integer grid of particle numbers $(N_s, N_p)$.\n\nThe Boltzmann constant $k_B$ is required for the calculations, and its value is taken as $k_B = 1.380649 \\times 10^{-23} \\, \\mathrm{J} \\cdot \\mathrm{K}^{-1}$.\n\nThe computational procedure for each test case is as follows:\n\n**1. Construct the Discrete Probability Mass Function (PMF)**\nFor each case, we are given the parameters for a continuous bivariate normal distribution: the mean vector $m = (\\langle N_s \\rangle_{in}, \\langle N_p \\rangle_{in})^\\top$ and the covariance matrix $\\Sigma$. We are also given integer grid ranges for the particle numbers $N_s$ and $N_p$. Let the vector of particle numbers be $\\mathbf{N} = (N_s, N_p)^\\top$.\n\nFirst, we generate a 2D grid of all integer pairs $(N_s, N_p)$ within the specified ranges. For each point $\\mathbf{N}$ on this grid, we calculate an unnormalized probability weight $W(\\mathbf{N})$ using the multivariate normal probability density function form:\n$$\nW(\\mathbf{N}) = \\exp\\left(-\\frac{1}{2} (\\mathbf{N}-m)^\\top \\Sigma^{-1} (\\mathbf{N}-m)\\right)\n$$\nwhere $\\Sigma^{-1}$ is the inverse of the given covariance matrix.\n\nThe discrete PMF, $P(\\mathbf{N})$, is obtained by normalizing these weights such that their sum over the entire grid equals $1$:\n$$\nP(\\mathbf{N}) = \\frac{W(\\mathbf{N})}{\\sum_{\\mathbf{N}' \\in \\text{grid}} W(\\mathbf{N}')}\n$$\n\n**2. Compute Sample Moments from the PMF**\nUsing the calculated PMF, we compute the sample mean vector $\\langle \\mathbf{N} \\rangle$ and the sample covariance matrix $C$. These are the moments of the discrete distribution and will slightly differ from the input parameters $m$ and $\\Sigma$ due to the discretization and truncation of the distribution by the finite grid.\n\nThe components of the mean vector are calculated as the expectation value of each particle number:\n$$\n\\langle N_i \\rangle = \\sum_{\\mathbf{N} \\in \\text{grid}} N_i P(\\mathbf{N})\n$$\nwhere $i \\in \\{s, p\\}$.\n\nThe elements of the covariance matrix $C$ are given by $C_{ij} = \\langle (N_i - \\langle N_i \\rangle)(N_j - \\langle N_j \\rangle) \\rangle$, which can be computed as:\n$$\nC_{ij} = \\left( \\sum_{\\mathbf{N} \\in \\text{grid}} N_i N_j P(\\mathbf{N}) \\right) - \\langle N_i \\rangle \\langle N_j \\rangle\n$$\nfor $i, j \\in \\{s, p\\}$.\n\n**3. Calculate Thermodynamic Properties a. Isothermal Compressibility, $\\kappa_T$**\nThe isothermal compressibility is calculated using the formula relating it to the total number fluctuations:\n$$\n\\kappa_T = \\frac{\\mathrm{Var}(N_{\\mathrm{tot}})}{k_B T V \\rho_{\\mathrm{tot}}^2}\n$$\nHere, $T$ and $V$ are the temperature and volume given for the case.\nThe total number density is $\\rho_{\\mathrm{tot}} = (\\langle N_s \\rangle + \\langle N_p \\rangle) / V$.\nThe variance of the total particle number, $\\mathrm{Var}(N_{\\mathrm{tot}})$, is the sum of all elements of the covariance matrix $C$:\n$$\n\\mathrm{Var}(N_{\\mathrm{tot}}) = \\mathrm{Var}(N_s+N_p) = C_{ss} + C_{pp} + 2C_{sp} = \\mathbf{1}^\\top C \\mathbf{1}\n$$\nwhere $\\mathbf{1} = (1, 1)^\\top$. Inserting all quantities in SI units ($k_B$ in $\\mathrm{J/K}$, $T$ in $\\mathrm{K}$, $V$ in $\\mathrm{m}^3$, $\\rho_{\\mathrm{tot}}$ in $\\mathrm{m}^{-3}$, and $C$ being dimensionless) yields $\\kappa_T$ in units of $\\mathrm{Pa}^{-1}$.\n\n**b. Partial Molar Volumes, $\\bar{\\mathbf{v}}$**\nThe partial molar volume vector $\\bar{\\mathbf{v}} = (\\bar{v}_s, \\bar{v}_p)^\\top$ is computed using the provided matrix formula.\nFirst, we define the matrix $B$:\n$$\nB = \\frac{C}{k_B T}\n$$\nNext, we compute the Moore-Penrose pseudoinverse of $B$, denoted as $B^{+}$. This is a robust approach that handles cases where $B$ (and thus $C$) might be singular or ill-conditioned, as could happen with limited sampling or strong correlations.\n\nThe partial molar volume vector is then given by:\n$$\n\\bar{\\mathbf{v}} = \\frac{B^{+} \\langle \\mathbf{N} \\rangle}{\\boldsymbol{\\rho}^\\top B^{+} \\langle \\mathbf{N} \\rangle}\n$$\nwhere $\\boldsymbol{\\rho} = \\langle \\mathbf{N} \\rangle / V$ is the density vector. The resulting vector $\\bar{\\mathbf{v}}$ has units of volume per particle, which will be $\\mathrm{m}^3$ if all inputs are in SI units. The final result must be converted to $\\mathrm{nm}^3$ per molecule by multiplying by $10^{27}$, since $1 \\, \\mathrm{m}^3 = 10^{27} \\, \\mathrm{nm}^3$.\n\nThis complete procedure will be implemented for all four test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes isothermal compressibility and partial molar volumes from\n    GCE number fluctuation data for a binary polymer solution.\n    \"\"\"\n    # Physical constants and conversion factors\n    k_B = 1.380649e-23  # Boltzmann constant in J/K\n    M3_TO_NM3 = 1e27   # Conversion factor from m^3 to nm^3\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"T\": 300, \"V\": 1e-21,\n            \"m\": np.array([100, 1]),\n            \"Sigma\": np.array([[50, -0.5], [-0.5, 0.8]]),\n            \"Ns_range\": (90, 110), \"Np_range\": (0, 5)\n        },\n        {\n            \"T\": 300, \"V\": 2e-21,\n            \"m\": np.array([80, 20]),\n            \"Sigma\": np.array([[60, -8], [-8, 15]]),\n            \"Ns_range\": (60, 100), \"Np_range\": (10, 30)\n        },\n        {\n            \"T\": 300, \"V\": 1e-21,\n            \"m\": np.array([50, 5]),\n            \"Sigma\": np.array([[45, 0], [0, 5]]),\n            \"Ns_range\": (35, 65), \"Np_range\": (0, 12)\n        },\n        {\n            \"T\": 300, \"V\": 5e-22,\n            \"m\": np.array([120, 0.2]),\n            \"Sigma\": np.array([[70, -0.1], [-0.1, 0.2]]),\n            \"Ns_range\": (100, 140), \"Np_range\": (0, 2)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        T = case[\"T\"]\n        V = case[\"V\"]\n        m = case[\"m\"]\n        Sigma = case[\"Sigma\"]\n        Ns_range = case[\"Ns_range\"]\n        Np_range = case[\"Np_range\"]\n\n        # Step 1: Construct the Discrete Probability Mass Function (PMF)\n        \n        # Create the integer grid for Ns and Np\n        Ns_vals = np.arange(Ns_range[0], Ns_range[1] + 1)\n        Np_vals = np.arange(Np_range[0], Np_range[1] + 1)\n        Ns_grid, Np_grid = np.meshgrid(Ns_vals, Np_vals)\n        \n        # Reshape grid into a list of (Ns, Np) coordinate pairs\n        N_grid = np.stack([Ns_grid.ravel(), Np_grid.ravel()], axis=1)\n        \n        # Calculate unnormalized probability weights using the multivariate normal PDF form\n        Sigma_inv = np.linalg.inv(Sigma)\n        diff = N_grid - m\n        # Efficiently compute the quadratic form (x-m)^T * Sigma^-1 * (x-m) for all grid points\n        exponent = -0.5 * np.sum((diff @ Sigma_inv) * diff, axis=1)\n        weights = np.exp(exponent)\n        \n        # Normalize weights to get the PMF\n        P = weights / np.sum(weights)\n\n        # Step 2: Compute Sample Moments from the PMF\n        \n        # Mean vector N\n        N_mean = np.sum(N_grid * P[:, np.newaxis], axis=0)\n        \n        # Covariance matrix C\n        # E[N_i * N_j]\n        outer_prod = N_grid[:, :, np.newaxis] * N_grid[:, np.newaxis, :]\n        N_outer_mean = np.sum(outer_prod * P[:, np.newaxis, np.newaxis], axis=0)\n        # C = E[N_i * N_j] - E[N_i] * E[N_j]\n        C = N_outer_mean - np.outer(N_mean, N_mean)\n\n        # Step 3: Calculate Thermodynamic Properties\n        \n        # Density vector and total density\n        rho_vec = N_mean / V\n        rho_tot = np.sum(rho_vec)\n\n        # a. Isothermal Compressibility, kappa_T\n        var_N_tot = np.sum(C)\n        beta_T = k_B * T\n        kappa_T = var_N_tot / (beta_T * V * rho_tot**2)\n\n        # b. Partial Molar Volumes, v_bar\n        B = C / beta_T\n        \n        # Use Moore-Penrose pseudoinverse for robustness\n        B_pinv = np.linalg.pinv(B)\n        \n        numerator_vec = B_pinv @ N_mean\n        denominator_scalar = rho_vec.T @ numerator_vec\n        \n        # The result is in m^3, convert to nm^3\n        v_bar = (numerator_vec / denominator_scalar) * M3_TO_NM3\n        \n        v_bar_s = v_bar[0]\n        v_bar_p = v_bar[1]\n\n        results.append([kappa_T, v_bar_s, v_bar_p])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists of floats.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}