{
    "hands_on_practices": [
        {
            "introduction": "Before applying any powerful computational method, it is crucial to understand its theoretical foundations. This first exercise provides a direct link between the abstract principles of statistical mechanics and the concrete formula for free energy perturbation. By analytically deriving the free energy difference for a simple harmonic oscillator in two different ways, you will solidify your understanding of the Zwanzig equation and its relationship to the canonical partition function .",
            "id": "3847564",
            "problem": "Consider a classical one-dimensional system at fixed temperature $T$ described by a single coordinate $x$ with two harmonic potential energy functions: $U_{A}(x)=\\tfrac{1}{2}k_{A}x^{2}$ and $U_{B}(x)=\\tfrac{1}{2}k_{B}x^{2}$, where $k_{A}0$ and $k_{B}0$ are force constants. Using only the canonical ensemble definitions and the basic facts of classical statistical mechanics, derive the free energy difference $\\Delta F \\equiv F_{B}-F_{A}$ between state $B$ and state $A$ in two ways:\n1. Starting from the definition of a canonical average under $U_{A}(x)$, compute the ensemble average $\\langle \\exp(-\\beta\\Delta U)\\rangle_{A}$, with $\\Delta U(x)\\equiv U_{B}(x)-U_{A}(x)$ and $\\beta\\equiv 1/(k_{\\mathrm{B}}T)$, where $k_{\\mathrm{B}}$ is the Boltzmann constant. Use this average to obtain an analytical expression for $\\Delta F$, making no appeal to any pre-stated identities beyond the canonical definitions.\n2. Independently compute the ratio of the exact configurational partition functions for $U_{B}$ and $U_{A}$ and use it to obtain $\\Delta F$. Show that the two results agree.\n\nAssume the Hamiltonian is separable into kinetic and potential parts, and note that any momentum contributions cancel in the free energy difference because the kinetic energy is identical in states $A$ and $B$. Provide your final answer as a single closed-form expression for $\\Delta F$ in terms of $k_{A}$, $k_{B}$, $k_{\\mathrm{B}}$, and $T$. Express the free energy in Joules. No numerical evaluation or rounding is required.",
            "solution": "The problem requires the derivation of the Helmholtz free energy difference, $\\Delta F \\equiv F_{B}-F_{A}$, between two states of a one-dimensional classical system described by harmonic potential energy functions $U_{A}(x)=\\tfrac{1}{2}k_{A}x^{2}$ and $U_{B}(x)=\\tfrac{1}{2}k_{B}x^{2}$. The derivation must be performed in two distinct ways to demonstrate their equivalence. We are given the inverse temperature $\\beta \\equiv 1/(k_{\\mathrm{B}}T)$, where $k_{\\mathrm{B}}$ is the Boltzmann constant and $T$ is the fixed temperature.\n\nFirst, we establish the fundamental relationship between free energy and the partition function. The Helmholtz free energy $F$ is defined in terms of the total canonical partition function $Z$ as:\n$$F = -k_{\\mathrm{B}}T \\ln Z = -\\frac{1}{\\beta} \\ln Z$$\nThe total partition function $Z$ for a one-dimensional system with a separable Hamiltonian, $H(p,x) = K(p) + U(x)$, is given by the integral over all phase space coordinates $(p,x)$:\n$$Z = \\frac{1}{h} \\int \\int \\exp(-\\beta H(p,x)) dp dx = \\left(\\frac{1}{h} \\int \\exp(-\\beta K(p)) dp\\right) \\left(\\int \\exp(-\\beta U(x)) dx\\right)$$\nwhere $h$ is a constant with units of action to make $Z$ dimensionless, typically Planck's constant. We can write this as $Z = Z_{\\mathrm{kin}} Q$, where $Z_{\\mathrm{kin}}$ is the kinetic contribution and $Q$ is the configurational partition function, $Q = \\int_{-\\infty}^{\\infty} \\exp(-\\beta U(x)) dx$.\n\nThe free energy difference $\\Delta F$ is:\n$$\\Delta F = F_{B} - F_{A} = \\left(-\\frac{1}{\\beta} \\ln Z_{B}\\right) - \\left(-\\frac{1}{\\beta} \\ln Z_{A}\\right) = -\\frac{1}{\\beta} \\ln\\left(\\frac{Z_{B}}{Z_{A}}\\right)$$\nSince the kinetic energy term $K(p)$ is the same for both states $A$ and $B$, their kinetic partition functions $Z_{\\mathrm{kin},A}$ and $Z_{\\mathrm{kin},B}$ are identical. Thus, the ratio of total partition functions simplifies to the ratio of configurational partition functions:\n$$\\frac{Z_{B}}{Z_{A}} = \\frac{Z_{\\mathrm{kin}} Q_{B}}{Z_{\\mathrm{kin}} Q_{A}} = \\frac{Q_{B}}{Q_{A}}$$\nTherefore, the free energy difference is directly related to the configurational partition functions:\n$$\\Delta F = -\\frac{1}{\\beta} \\ln\\left(\\frac{Q_{B}}{Q_{A}}\\right)$$\nThis relationship, derived from canonical definitions, will be central to both methods.\n\n**Method 1: Derivation using the Ensemble Average**\n\nThis method requires us to compute the ensemble average $\\langle \\exp(-\\beta\\Delta U)\\rangle_{A}$ and relate it to $\\Delta F$. First, we must derive this relationship from fundamental definitions as requested.\n\nThe canonical ensemble average of a quantity $O(x)$ in state $A$ is defined as:\n$$\\langle O(x) \\rangle_{A} = \\frac{\\int_{-\\infty}^{\\infty} O(x) \\exp(-\\beta U_{A}(x)) dx}{\\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{A}(x)) dx} = \\frac{\\int_{-\\infty}^{\\infty} O(x) \\exp(-\\beta U_{A}(x)) dx}{Q_{A}}$$\nWe are interested in the specific average of $O(x) = \\exp(-\\beta\\Delta U(x))$, where $\\Delta U(x) = U_{B}(x) - U_{A}(x)$. Substituting this into the definition:\n$$\\langle \\exp(-\\beta\\Delta U) \\rangle_{A} = \\frac{\\int_{-\\infty}^{\\infty} \\exp(-\\beta(U_{B}(x) - U_{A}(x))) \\exp(-\\beta U_{A}(x)) dx}{Q_{A}}$$\nBy combining the exponents in the numerator, we find:\n$$\\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{B}(x) + \\beta U_{A}(x) - \\beta U_{A}(x)) dx = \\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{B}(x)) dx = Q_{B}$$\nThus, the ensemble average is precisely the ratio of the configurational partition functions:\n$$\\langle \\exp(-\\beta\\Delta U) \\rangle_{A} = \\frac{Q_{B}}{Q_{A}}$$\nCombining this result with the expression for $\\Delta F$ derived earlier, we obtain the free energy perturbation identity:\n$$\\Delta F = -k_{\\mathrm{B}}T \\ln \\langle \\exp(-\\beta\\Delta U) \\rangle_{A}$$\nNow, we explicitly compute the average for the given potentials. The potential energy difference is:\n$$\\Delta U(x) = U_{B}(x) - U_{A}(x) = \\frac{1}{2}k_{B}x^{2} - \\frac{1}{2}k_{A}x^{2} = \\frac{1}{2}(k_{B} - k_{A})x^{2}$$\nThe ensemble average is:\n$$\\langle \\exp(-\\beta\\Delta U) \\rangle_{A} = \\frac{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\beta \\left(\\frac{1}{2}(k_{B}-k_{A})x^{2}\\right)\\right) \\exp\\left(-\\beta \\left(\\frac{1}{2}k_{A}x^{2}\\right)\\right) dx}{\\int_{-\\infty}^{\\infty} \\exp\\left(-\\beta \\left(\\frac{1}{2}k_{A}x^{2}\\right)\\right) dx}$$\nThe numerator's integral is:\n$$I_{\\mathrm{num}} = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta}{2}(k_{B}-k_{A}+k_{A})x^{2}\\right) dx = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k_{B}}{2}x^{2}\\right) dx$$\nThe denominator's integral is:\n$$I_{\\mathrm{den}} = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k_{A}}{2}x^{2}\\right) dx$$\nThese are standard Gaussian integrals of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^{2})dx = \\sqrt{\\pi/a}$.\nFor the numerator, $a = \\frac{\\beta k_{B}}{2}$, so $I_{\\mathrm{num}} = \\sqrt{\\frac{2\\pi}{\\beta k_{B}}}$.\nFor the denominator, $a = \\frac{\\beta k_{A}}{2}$, so $I_{\\mathrm{den}} = \\sqrt{\\frac{2\\pi}{\\beta k_{A}}}$.\nThe ensemble average is the ratio of these two results:\n$$\\langle \\exp(-\\beta\\Delta U) \\rangle_{A} = \\frac{\\sqrt{2\\pi/(\\beta k_{B})}}{\\sqrt{2\\pi/(\\beta k_{A})}} = \\sqrt{\\frac{2\\pi}{\\beta k_{B}} \\cdot \\frac{\\beta k_{A}}{2\\pi}} = \\sqrt{\\frac{k_{A}}{k_{B}}}$$\nFinally, we substitute this into the expression for $\\Delta F$:\n$$\\Delta F = -k_{\\mathrm{B}}T \\ln\\left(\\sqrt{\\frac{k_{A}}{k_{B}}}\\right) = -k_{\\mathrm{B}}T \\ln\\left(\\left(\\frac{k_{A}}{k_{B}}\\right)^{1/2}\\right) = -\\frac{1}{2}k_{\\mathrm{B}}T \\ln\\left(\\frac{k_{A}}{k_{B}}\\right)$$\nUsing the property of logarithms $\\ln(1/x) = -\\ln(x)$, this can be written as:\n$$\\Delta F = \\frac{1}{2}k_{\\mathrm{B}}T \\ln\\left(\\frac{k_{B}}{k_{A}}\\right)$$\n\n**Method 2: Derivation using the Ratio of Partition Functions**\n\nThis method directly calculates $\\Delta F$ from the ratio of the configurational partition functions, $Q_{B}/Q_{A}$. As established in the introduction, $\\Delta F = -k_{\\mathrm{B}}T \\ln(Q_{B}/Q_{A})$.\n\nThe configurational partition functions for states $A$ and $B$ are:\n$$Q_{A} = \\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{A}(x)) dx = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k_{A}}{2}x^{2}\\right) dx$$\n$$Q_{B} = \\int_{-\\infty}^{\\infty} \\exp(-\\beta U_{B}(x)) dx = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\beta k_{B}}{2}x^{2}\\right) dx$$\nUsing the Gaussian integral result $\\int_{-\\infty}^{\\infty} \\exp(-ax^{2})dx = \\sqrt{\\pi/a}$, we evaluate each partition function:\n$$Q_{A} = \\sqrt{\\frac{\\pi}{(\\beta k_{A}/2)}} = \\sqrt{\\frac{2\\pi}{\\beta k_{A}}}$$\n$$Q_{B} = \\sqrt{\\frac{\\pi}{(\\beta k_{B}/2)}} = \\sqrt{\\frac{2\\pi}{\\beta k_{B}}}$$\nNext, we compute their ratio:\n$$\\frac{Q_{B}}{Q_{A}} = \\frac{\\sqrt{2\\pi/(\\beta k_{B})}}{\\sqrt{2\\pi/(\\beta k_{A})}} = \\sqrt{\\frac{k_{A}}{k_{B}}}$$\nSubstituting this ratio into the equation for the free energy difference:\n$$\\Delta F = -k_{\\mathrm{B}}T \\ln\\left(\\frac{Q_{B}}{Q_{A}}\\right) = -k_{\\mathrm{B}}T \\ln\\left(\\sqrt{\\frac{k_{A}}{k_{B}}}\\right)$$\nThis simplifies to:\n$$\\Delta F = -\\frac{1}{2}k_{\\mathrm{B}}T \\ln\\left(\\frac{k_{A}}{k_{B}}\\right) = \\frac{1}{2}k_{\\mathrm{B}}T \\ln\\left(\\frac{k_{B}}{k_{A}}\\right)$$\n\n**Conclusion**\n\nBoth methods yield the identical result for the free energy difference: $\\Delta F = \\frac{1}{2}k_{\\mathrm{B}}T \\ln(k_{B}/k_{A})$. This confirms the consistency of the two approaches, which are both cornerstones of statistical mechanics and computational free energy calculations. The expression has units of energy (Joules in SI), as the product $k_{\\mathrm{B}}T$ has units of energy and the logarithmic term is dimensionless.",
            "answer": "$$\n\\boxed{\\frac{1}{2} k_{\\mathrm{B}}T \\ln\\left(\\frac{k_{B}}{k_{A}}\\right)}\n$$"
        },
        {
            "introduction": "A powerful formula is only useful if we understand its domain of validity. While the Zwanzig equation is exact in principle, its practical application with finite sampling is limited by statistical error, especially in cases of poor phase space overlap. This hands-on coding exercise  demonstrates this critical limitation by having you numerically determine the 'breaking point' of the single-step FEP estimator, providing a visceral understanding of why bridging states are often necessary in real-world calculations.",
            "id": "2455879",
            "problem": "You are to examine the single-step Free Energy Perturbation (FEP) estimator for a simple, analytically tractable system and determine when it becomes practically unusable, defined here as producing a relative absolute error strictly greater than $1$ (that is, greater than $100$ percent). Consider a classical, one-dimensional harmonic oscillator with initial state $0$ having potential energy $U_0(x) = \\tfrac{1}{2} k_0 x^2$ and perturbed state $1$ having potential energy $U_1(x) = \\tfrac{1}{2} k_1 x^2$. Work in nondimensional units such that the inverse thermal energy is $\\beta = 1$ and choose $k_0 = 1$, so $r = k_1/k_0 = k_1$ is the dimensionless perturbation ratio. The exact Helmholtz free energy difference is\n$$\n\\Delta F_{\\text{true}} = \\tfrac{1}{2} \\ln r,\n$$\nderived from the exact partition function of the harmonic oscillator. The single-step FEP estimator based on $N$ independent and identically distributed samples $x_i$ drawn from the equilibrium distribution of state $0$ with density $p_0(x) \\propto \\exp\\{-U_0(x)\\}$ is\n$$\n\\widehat{\\Delta F}_N = - \\ln\\left( \\frac{1}{N} \\sum_{i=1}^{N} \\exp\\left[-\\left(U_1(x_i) - U_0(x_i)\\right)\\right] \\right).\n$$\nDefine the relative absolute error as\n$$\n\\varepsilon = \n\\begin{cases}\n\\frac{\\left|\\widehat{\\Delta F}_N - \\Delta F_{\\text{true}}\\right|}{\\left|\\Delta F_{\\text{true}}\\right|},  \\text{if } \\Delta F_{\\text{true}} \\neq 0, \\\\\n0,  \\text{if } \\Delta F_{\\text{true}} = 0 \\text{ and } \\widehat{\\Delta F}_N = 0, \\\\\n+\\infty,  \\text{if } \\Delta F_{\\text{true}} = 0 \\text{ and } \\widehat{\\Delta F}_N \\neq 0.\n\\end{cases}\n$$\nFor a given finite set of candidate ratios $r \\in \\mathcal{R}$, define the perturbation magnitude as\n$$\nm(r) = \\max\\{r, 1/r\\},\n$$\nand define the threshold magnitude for a given test case as the smallest $m(r)$ among all $r \\in \\mathcal{R}$ for which $\\varepsilon  1$. If no $r \\in \\mathcal{R}$ yields $\\varepsilon  1$, output $-1.0$.\n\nYour task is to implement a complete program that, for each of the following test cases, computes the threshold magnitude as defined above. All computations are to be performed in the nondimensionalized setting described, and the final outputs must be floats.\n\nTest suite:\n- Case A: $N = 8$, $\\mathcal{R} = \\{1.0, 2.0, 4.0, 8.0, 16.0\\}$.\n- Case B: $N = 64$, $\\mathcal{R} = \\{0.9, 0.7, 0.5, 0.3, 0.2, 2.0, 4.0, 8.0\\}$.\n- Case C: $N = 1024$, $\\mathcal{R} = \\{0.2, 0.1, 8.0, 16.0\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the three threshold magnitudes for the cases A, B, C, respectively, as a comma-separated list enclosed in square brackets (for example, \"[2.0,-1.0,8.0]\"). All outputs are dimensionless floats.",
            "solution": "The problem requires an analysis of the single-step Free Energy Perturbation (FEP) estimator's practical usability limit, which is defined as the point where the relative absolute error $\\varepsilon$ exceeds $1$. The system under consideration is a one-dimensional classical harmonic oscillator transitioning from an initial state $0$ to a perturbed state $1$.\n\nThe potential energies for the states are given by $U_0(x) = \\frac{1}{2} k_0 x^2$ and $U_1(x) = \\frac{1}{2} k_1 x^2$. The problem operates in nondimensional units where the inverse thermal energy is $\\beta = (k_B T)^{-1} = 1$ and the initial spring constant is $k_0 = 1$. The perturbation is characterized by the dimensionless ratio $r = k_1/k_0 = k_1$.\n\nThe equilibrium probability distribution of the initial state $0$ is given by the Boltzmann distribution, $p_0(x) \\propto \\exp(-\\beta U_0(x))$. With the specified parameters, this becomes $p_0(x) \\propto \\exp(-\\frac{1}{2}x^2)$. This is the probability density function of a standard normal distribution, $\\mathcal{N}(0, 1)$. Therefore, generating the $N$ independent and identically distributed samples $\\{x_i\\}_{i=1}^N$ from the equilibrium distribution of state $0$ is correctly implemented by drawing $N$ random variates from $\\mathcal{N}(0, 1)$.\n\nThe single-step FEP estimator (also known as the Zwanzig equation) for the Helmholtz free energy difference, $\\Delta F = F_1 - F_0$, is given by:\n$$\n\\widehat{\\Delta F}_N = - \\frac{1}{\\beta} \\ln\\left\\langle \\exp\\left[-\\beta\\left(U_1(x) - U_0(x)\\right)\\right] \\right\\rangle_0\n$$\nThe angle brackets $\\langle \\cdot \\rangle_0$ denote an ensemble average over configurations $x$ drawn from the distribution of state $0$. For a finite sample of size $N$, this is approximated by a simple arithmetic mean:\n$$\n\\widehat{\\Delta F}_N = - \\frac{1}{\\beta} \\ln\\left( \\frac{1}{N} \\sum_{i=1}^{N} \\exp\\left[-\\beta\\left(U_1(x_i) - U_0(x_i)\\right)\\right] \\right)\n$$\nGiven $\\beta=1$, the potential energy difference is $\\Delta U(x_i) = U_1(x_i) - U_0(x_i) = \\frac{1}{2}k_1 x_i^2 - \\frac{1}{2}k_0 x_i^2 = \\frac{1}{2}(r-1)x_i^2$. The working formula for the estimator is therefore:\n$$\n\\widehat{\\Delta F}_N = - \\ln\\left( \\frac{1}{N} \\sum_{i=1}^{N} \\exp\\left[-\\frac{1}{2}(r-1)x_i^2\\right] \\right)\n$$\nThe exact analytical solution for the free energy difference for this system is $\\Delta F_{\\text{true}} = \\frac{1}{2\\beta} \\ln(k_1/k_0)$, which simplifies to $\\Delta F_{\\text{true}} = \\frac{1}{2}\\ln r$. The error is evaluated using the relative absolute error $\\varepsilon$, as defined in the problem statement.\n\nA critical consideration for the reliability of the FEP estimator is the variance of the exponential term, $W = \\exp(-\\beta \\Delta U)$. The variance of the estimator $\\widehat{\\Delta F}_N$ is related to $\\text{Var}(W) = \\langle W^2 \\rangle_0 - \\langle W \\rangle_0^2$. The second moment, $\\langle W^2 \\rangle_0$, is given by the integral:\n$$\n\\langle W^2 \\rangle_0 = \\int_{-\\infty}^{\\infty} \\exp\\left[-2\\beta \\Delta U(x)\\right] p_0(x) dx = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}\\exp\\left[-(r-1)x^2\\right] \\exp\\left[-\\frac{1}{2}x^2\\right] dx\n$$\n$$\n\\langle W^2 \\rangle_0 = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left[-\\left( (r-1) + \\frac{1}{2} \\right)x^2\\right] dx\n$$\nFor this Gaussian integral to converge, the quadratic term in the exponent must be positive: $(r-1) + \\frac{1}{2}  0$, which simplifies to $r  \\frac{1}{2}$. If $r \\le \\frac{1}{2}$, the variance of $W$ is infinite. This indicates that the FEP estimator is statistically ill-behaved; a finite sample average will be dominated by rare, extreme events and will not reliably converge. Consequently, large errors are expected for any $r \\le \\frac{1}{2}$.\n\nFurthermore, for $r \\gg 1$, the target potential $U_1$ is much narrower (steeper) than the sampling potential $U_0$. The sampling distribution $p_0(x)$ is broad, while the term being averaged, $ W(x) = \\exp[-\\frac{1}{2}(r-1)x^2] $, is very sharply peaked at $x=0$. This signifies poor phase space overlap. Most samples $x_i$ will be drawn from regions where $W(x_i)$ is practically zero, leading to a systematic underestimation of the true average. This undersampling also results in high variance and large errors.\n\nThus, the FEP estimator is expected to fail (i.e., yield $\\varepsilon  1$) for both large perturbation ratios ($r \\gg 1$) and for ratios $r \\le \\frac{1}{2}$. The task is to find the smallest perturbation magnitude, $m(r) = \\max\\{r, 1/r\\}$, for which this failure occurs for a given set of ratios $\\mathcal{R}$.\n\nThe computational algorithm proceeds as follows for each test case $(N, \\mathcal{R})$:\n$1$. A fixed random seed is used to ensure the stochastic results are reproducible.\n$2$. For each test case, initialize an empty list to store the magnitudes $m(r)$ for which the error criterion $\\varepsilon  1$ is met.\n$3$. Iterate through each ratio $r \\in \\mathcal{R}$:\n    a. If $r=1$, then $\\Delta F_{\\text{true}} = 0$ and $\\widehat{\\Delta F}_N = 0$, so $\\varepsilon=0$. The criterion is not met.\n    b. If $r \\ne 1$, generate $N$ samples $\\{x_i\\}$ from $\\mathcal{N}(0, 1)$.\n    c. Calculate $\\Delta F_{\\text{true}} = \\frac{1}{2}\\ln r$.\n    d. Calculate the estimate $\\widehat{\\Delta F}_N$ using the FEP formula.\n    e. Compute the relative absolute error $\\varepsilon = |\\widehat{\\Delta F}_N - \\Delta F_{\\text{true}}| / |\\Delta F_{\\text{true}}|$.\n    f. If $\\varepsilon  1$, calculate the perturbation magnitude $m(r) = \\max\\{r, 1/r\\}$ and add it to the list of failing magnitudes.\n$4$. After evaluating all $r \\in \\mathcal{R}$, the threshold magnitude is determined. If the list of failing magnitudes is empty, the result for the test case is $-1.0$. Otherwise, the result is the minimum value in this list. This procedure is repeated for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the threshold perturbation magnitude for FEP estimator failure.\n    \"\"\"\n    \n    # Set a fixed random seed for reproducibility of the stochastic simulation.\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (N, R_set)\n        (8, [1.0, 2.0, 4.0, 8.0, 16.0]),\n        # Case B: (N, R_set)\n        (64, [0.9, 0.7, 0.5, 0.3, 0.2, 2.0, 4.0, 8.0]),\n        # Case C: (N, R_set)\n        (1024, [0.2, 0.1, 8.0, 16.0]),\n    ]\n\n    results = []\n    \n    for N, R_set in test_cases:\n        failing_magnitudes = []\n        \n        for r in R_set:\n            # Case r=1 is a special case.\n            # Delta_F_true is 0. Delta_U is 0, so exp(-Delta_U) is 1.\n            # The average is 1, and log(1) is 0. So F_est is 0.\n            # Per problem definition, error is 0.\n            if r == 1.0:\n                error = 0.0\n            else:\n                # 1. Generate N samples from the equilibrium distribution of state 0,\n                # which is a standard normal distribution N(0, 1).\n                # The parameters k0=1 and beta=1 lead to variance = (k0*beta)^-1 = 1.\n                x_samples = np.random.randn(N)\n                \n                # 2. Calculate the true free energy difference.\n                delta_F_true = 0.5 * np.log(r)\n                \n                # 3. Calculate the FEP estimate.\n                # Delta_U = 0.5 * (k1 - k0) * x^2, with k1=r, k0=1.\n                delta_U = 0.5 * (r - 1.0) * x_samples**2\n                \n                # The argument of the logarithm in the FEP formula\n                fep_average = np.mean(np.exp(-delta_U))\n                \n                # The FEP estimate\n                # The average is always  0, so no domain error for log.\n                delta_F_est = -np.log(fep_average)\n                \n                # 4. Calculate the relative absolute error.\n                # The case delta_F_true == 0 is handled by r == 1.0 check.\n                error = np.abs(delta_F_est - delta_F_true) / np.abs(delta_F_true)\n\n            # 5. Check if the error exceeds the threshold.\n            if error  1.0:\n                perturbation_magnitude = max(r, 1.0 / r)\n                failing_magnitudes.append(perturbation_magnitude)\n                \n        # 6. Determine the threshold magnitude for the test case.\n        if not failing_magnitudes:\n            threshold_magnitude = -1.0\n        else:\n            threshold_magnitude = min(failing_magnitudes)\n        \n        results.append(threshold_magnitude)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The challenge of poor phase space overlap, explored in the previous practice, motivates the development of more advanced and robust methods. This capstone exercise  introduces the Multistate Bennett Acceptance Ratio (MBAR), a state-of-the-art technique that optimally combines data from multiple intermediate states to compute free energy differences with maximum statistical precision. Implementing the MBAR equations from the ground up will provide deep insight into modern free energy calculation and its powerful capabilities.",
            "id": "3847551",
            "problem": "You are given a collection of samples drawn from multiple thermodynamic states indexed by $k \\in \\{0,1,\\ldots,K-1\\}$, each characterized by a reduced potential $u_k(\\mathbf{x})$. The reduced potential is defined by $u_k(\\mathbf{x}) = \\beta U_k(\\mathbf{x}) + \\text{terms at constant thermodynamic parameters}$, where $U_k(\\mathbf{x})$ is the potential energy. In what follows, you may take $\\beta = 1$ (dimensionless temperature), and treat all reduced potentials as dimensionless. The goal is to estimate the dimensionless free energies $f_k$ up to an additive constant from data pooled across multiple states using the multi-state maximum likelihood approach known as the Multi-state Bennett Acceptance Ratio (MBAR). You must derive, implement, and apply the associated self-consistent equations and a numerically stable iterative scheme to compute both the free energies and their uncertainties.\n\nStarting from the fundamental base of equilibrium statistical mechanics and importance sampling, use only the following core facts:\n- The partition function is $Z_k = \\int d\\mathbf{x}\\, e^{-u_k(\\mathbf{x})}$, and the dimensionless free energy is $f_k = -\\ln Z_k$ up to an additive constant.\n- The samples are drawn from a mixture of $K$ equilibrium distributions corresponding to states $k$, with known per-state sample counts $N_k$ and total $N = \\sum_{k=0}^{K-1} N_k$.\n- Importance sampling identities and maximum likelihood estimation for mixture models with known sampling proportions yield self-consistent estimating equations for parameters that maximize the likelihood of the observed pooled data.\n- The asymptotic covariance of maximum likelihood estimators is given by the inverse of the observed Fisher information (the negative Hessian of the log-likelihood), projected to remove any non-identifiability due to additive gauge invariance of $f_k$.\n\nYour tasks are:\n- Derive the MBAR self-consistent equations that determine the $K$ unknown free energies $f_k$ (defined up to an additive constant) in terms of the reduced potentials $u_k(\\mathbf{x}_n)$ for all states $k$ and pooled samples $\\{\\mathbf{x}_n\\}_{n=1}^N$, and the known sample counts $N_k$.\n- Propose a numerically stable fixed-point iteration scheme to solve these equations. Your scheme must make explicit use of the log-sum-exp trick to ensure numerical stability, enforce the reference gauge by fixing $f_0 = 0$, and include a convergence criterion based on the maximum absolute change in $f_k$ below a user-specified tolerance.\n- Derive the observed Fisher information for the estimated $f_k$ in terms of the state-responsibility weights implied by the solution, explain how to handle the additive-constant non-identifiability by reducing the information matrix to a full-rank subspace (e.g., by anchoring $f_0 = 0$), and obtain asymptotic standard errors for free energy differences $\\Delta f_{k0} = f_k - f_0$.\n\nThen, implement a complete program that:\n- Constructs synthetic test data using one-dimensional harmonic reduced potentials with $\\beta = 1$. For each state $k$, define a harmonic reduced potential $u_k(x) = \\tfrac{1}{2}\\,k_{\\text{spring}}\\,(x - \\mu_k)^2$, and draw $N_k$ independent samples $x$ from the corresponding equilibrium distribution $p_k(x) \\propto \\exp\\!\\left[-u_k(x)\\right]$, which for a harmonic is a normal distribution with mean $\\mu_k$ and variance $1/k_{\\text{spring}}$.\n- From the pooled set of samples $\\{x_n\\}_{n=1}^N$, construct the full reduced-potential matrix $[u_k(x_n)]$ for all $k \\in \\{0,\\ldots,K-1\\}$ and $n \\in \\{1,\\ldots,N\\}$.\n- Solves the MBAR self-consistent equations for $f_k$ via your proposed iterative scheme, fixes the reference by setting $f_0 = 0$, computes $\\Delta f_{k0}$ for $k \\in \\{1,\\ldots,K-1\\}$, and computes their asymptotic standard errors via the observed Fisher information reduced to the identifiable subspace.\n\nScientific realism requirements:\n- Assume all samples are effectively uncorrelated so that effective sample size factors are unity. All quantities are dimensionless; no physical unit is required.\n- Use a numerically stable implementation that is robust to near-degenerate states.\n\nTest suite:\nYour program must run the following three cases, each specified by the number of states $K$, the per-state sample counts $\\{N_k\\}$, the harmonic means $\\{\\mu_k\\}$, the common spring constant $k_{\\text{spring}}$, and a random seed. For each case, draw exactly $N_k$ samples from $\\mathcal{N}(\\mu_k, 1/k_{\\text{spring}})$ independently for each state $k$, and then pool the samples across states. The random number generator must be seeded as specified to ensure determinism.\n- Case $1$: $K = 2$, $\\{\\mu_k\\} = [-1.0, 1.0]$, $k_{\\text{spring}} = 5.0$, $\\{N_k\\} = [80, 80]$, seed $= 246810$.\n- Case $2$: $K = 3$, $\\{\\mu_k\\} = [-1.0, 0.0, 1.5]$, $k_{\\text{spring}} = 4.0$, $\\{N_k\\} = [100, 10, 5]$, seed $= 13579$.\n- Case $3$: $K = 2$, $\\{\\mu_k\\} = [0.0, 0.2]$, $k_{\\text{spring}} = 10.0$, $\\{N_k\\} = [50, 50]$, seed $= 424242$.\n\nAnswer specification:\n- For each case, compute the free energy differences $\\Delta f_{k0}$ for $k \\in \\{1,\\ldots,K-1\\}$ and their asymptotic standard errors $s_{k0}$ obtained from the reduced observed Fisher information. All outputs are dimensionless floats.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order above, append the sequence $\\left[\\Delta f_{10}, s_{10}, \\Delta f_{20}, s_{20}, \\ldots, \\Delta f_{(K-1)0}, s_{(K-1)0}\\right]$ to a single flattened list. Each float must be rounded to exactly $6$ decimal places before printing. For example, a valid output could look like $[\\ldots]$ with all entries having $6$ decimal places.",
            "solution": "The user has provided a valid problem statement. The task is to derive and implement the Multi-state Bennett Acceptance Ratio (MBAR) method for estimating free energy differences from simulated data. This involves deriving the self-consistent equations for the free energies, proposing a numerically stable iterative solution, and deriving the formula for the asymptotic error of the estimates. The implementation will be tested on synthetic data generated from harmonic oscillators.\n\n### 1. Derivation of the MBAR Self-Consistent Equations\n\nWe begin with the fundamentals of statistical mechanics. The partition function of a thermodynamic state $k$ is given by $Z_k = \\int e^{-u_k(\\mathbf{x})} d\\mathbf{x}$, where $u_k(\\mathbf{x})$ is the reduced potential energy of the system in configuration $\\mathbf{x}$. The dimensionless free energy is $f_k = -\\ln Z_k$, defined up to an additive constant that depends on the choice of reference volume.\n\nWe are given a set of $N$ configurations, $\\{\\mathbf{x}_n\\}_{n=1}^N$, drawn from $K$ different thermodynamic states. For each state $k \\in \\{0, \\ldots, K-1\\}$, we have $N_k$ samples, such that the total number of samples is $N = \\sum_{k=0}^{K-1} N_k$.\n\nThe MBAR equations provide the minimum-variance unbiased estimates of the free energies. They can be derived from several perspectives, including maximum likelihood estimation. A straightforward derivation follows from the principle of optimal importance sampling. The partition function $Z_i$ for any state $i$ can be written as an integral:\n$$\nZ_i = \\int e^{-u_i(\\mathbf{x})} d\\mathbf{x}\n$$\nWe can estimate this integral using the pooled samples $\\{\\mathbf{x}_n\\}$ by reweighting them. An optimal combination of samples from all states leads to the following self-consistent estimator for $Z_i$:\n$$\n\\hat{Z}_i = \\sum_{n=1}^{N} \\frac{e^{-u_i(\\mathbf{x}_n)}}{\\sum_{j=0}^{K-1} N_j \\hat{Z}_j^{-1} e^{-u_j(\\mathbf{x}_n)}}\n$$\nThis equation states that the estimate for the partition function $\\hat{Z}_i$ is an average over all $N$ samples. Each sample $\\mathbf{x}_n$ is reweighted from its native \"mixed\" ensemble to the target ensemble $i$. The weight denominator, $\\sum_j N_j \\hat{Z}_j^{-1} e^{-u_j(\\mathbf{x}_n)}$, represents the unnormalized probability density of observing configuration $\\mathbf{x}_n$ in the mixture of all $K$ ensembles.\n\nTo obtain an equation for the free energies $f_k = -\\ln Z_k$, we substitute $\\hat{Z}_k = e^{-\\hat{f}_k}$ into the equation above:\n$$\ne^{-\\hat{f}_i} = \\sum_{n=1}^{N} \\frac{e^{-u_i(\\mathbf{x}_n)}}{\\sum_{j=0}^{K-1} N_j e^{\\hat{f}_j} e^{-u_j(\\mathbf{x}_n)}}\n$$\nRearranging gives the final form of the MBAR self-consistent equations:\n$$\n\\hat{f}_i = -\\ln \\left( \\sum_{n=1}^{N} \\frac{e^{-u_i(\\mathbf{x}_n)}}{\\sum_{j=0}^{K-1} N_j e^{\\hat{f}_j - u_j(\\mathbf{x}_n)}} \\right)\n$$\nThese $K$ coupled nonlinear equations must be solved for the $K$ free energies $\\hat{f}_0, \\ldots, \\hat{f}_{K-1}$. These free energies are only determined up to an arbitrary additive constant, as shifting all $\\hat{f}_j$ by a constant $c$ leaves the equations unchanged. We resolve this by fixing one free energy to a reference value, typically $\\hat{f}_0 = 0$.\n\n### 2. Numerically Stable Fixed-Point Iteration Scheme\n\nThe self-consistent equations can be solved using fixed-point iteration:\n$$\nf_i^{\\text{new}} = -\\ln \\left( \\sum_{n=1}^{N} \\frac{e^{-u_i(\\mathbf{x}_n)}}{\\sum_{j=0}^{K-1} N_j e^{f_j^{\\text{old}} - u_j(\\mathbf{x}_n)}} \\right)\n$$\nDirect evaluation of the exponentials in this form is numerically unstable due to the risk of floating-point overflow or underflow. We can construct a stable scheme using the log-sum-exp (LSE) trick, where $\\text{LSE}(\\mathbf{v}) = \\ln(\\sum_i e^{v_i})$. The iterative update for $f_i$ can be rewritten as:\n$$\nf_i^{\\text{new}} = -\\ln \\left( \\sum_{n=1}^{N} \\exp\\left[ -u_i(\\mathbf{x}_n) - \\ln\\left(\\sum_{j=0}^{K-1} \\exp\\left[ \\ln N_j + f_j^{\\text{old}} - u_j(\\mathbf{x}_n) \\right]\\right) \\right] \\right)\n$$\nThis expression is a nested application of LSE. Let us define the arguments for the LSE functions:\n- For the inner LSE (over states $j$ for each sample $n$): $A_{jn} = \\ln N_j + f_j^{\\text{old}} - u_j(\\mathbf{x}_n)$.\n- The result of the inner LSE is the log-denominator for each sample: $L_n = \\text{LSE}_j(A_{jn}) = \\ln(\\sum_j e^{A_{jn}})$.\n- For the outer LSE (over samples $n$ for each state $i$): $B_{in} = -u_i(\\mathbf{x}_n) - L_n$.\n- The un-shifted updated free energies are then $f'_i = -\\text{LSE}_n(B_{in}) = -\\ln(\\sum_n e^{B_{in}})$.\n\nAfter computing the un-shifted values $f'_i$, we enforce the gauge condition $f_0=0$ by shifting all free energies:\n$$\nf_i^{\\text{new}} = f'_i - f'_0 \\quad \\text{for } i \\in \\{0, \\ldots, K-1\\}\n$$\nThis ensures $f_0^{\\text{new}} = 0$. The iteration proceeds by starting with an initial guess (e.g., $f_i=0$ for all $i$) and repeating the update until the maximum absolute change in any free energy value between successive iterations falls below a specified tolerance $\\epsilon$:\n$$\n\\max_{i} |f_i^{\\text{new}} - f_i^{\\text{old}}|  \\epsilon\n$$\n\n### 3. Asymptotic Error Estimation\n\nThe asymptotic covariance matrix of the estimated free energies $\\hat{\\mathbf{f}}$ is given by the inverse of the observed Fisher information matrix, $\\mathbf{I}$. The Fisher information is the negative of the Hessian of the log-likelihood function evaluated at the maximum likelihood estimates. The relevant log-likelihood function (up to an additive constant) is:\n$$\n\\ln \\mathcal{L}(\\mathbf{f}) = \\sum_{k=0}^{K-1} N_k f_k - \\sum_{n=1}^{N} \\ln\\left( \\sum_{j=0}^{K-1} N_j e^{f_j - u_j(\\mathbf{x}_n)} \\right)\n$$\nThe first derivative with respect to $f_i$ is:\n$$\n\\frac{\\partial \\ln \\mathcal{L}}{\\partial f_i} = N_i - \\sum_{n=1}^{N} \\frac{N_i e^{f_i - u_i(\\mathbf{x}_n)}}{\\sum_{j=0}^{K-1} N_j e^{f_j - u_j(\\mathbf{x}_n)}} = N_i - \\sum_{n=1}^{N} W_{in}\n$$\nwhere $W_{in}$ is the probability that sample $n$ (drawn from any state) thermodynamically belongs to state $i$:\n$$\nW_{in} = \\frac{N_i e^{f_i - u_i(\\mathbf{x}_n)}}{\\sum_{j=0}^{K-1} N_j e^{f_j - u_j(\\mathbf{x}_n)}}\n$$\nNote that $\\sum_{i=0}^{K-1} W_{in} = 1$ for any sample $n$. At the maximum likelihood solution, $\\frac{\\partial \\ln \\mathcal{L}}{\\partial f_i} = 0$, which implies $N_i = \\sum_{n=1}^N W_{in}$.\n\nThe Hessian matrix elements $H_{ij} = \\frac{\\partial^2 \\ln \\mathcal{L}}{\\partial f_i \\partial f_j}$ are:\n$$\nH_{ij} = -\\sum_{n=1}^{N} \\frac{\\partial W_{in}}{\\partial f_j} = -\\sum_{n=1}^{N} (W_{in}\\delta_{ij} - W_{in}W_{jn})\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The observed Fisher information matrix is $\\mathbf{I} = -\\mathbf{H}$:\n$$\nI_{ij} = \\sum_{n=1}^{N} (W_{in}\\delta_{ij} - W_{in}W_{jn})\n$$\nThis $K \\times K$ matrix is singular, reflecting the non-identifiability of the absolute free energies. The vector $(1, 1, \\ldots, 1)^T$ is a null eigenvector because $\\sum_j I_{ij} = 0$.\n\nTo obtain a non-singular matrix, we work in the reduced space of free energy differences relative to state $0$, i.e., $\\Delta f_{k0} = f_k - f_0$ for $k \\in \\{1, \\ldots, K-1\\}$. This is equivalent to setting $f_0=0$ and estimating the remaining $K-1$ free energies. The Fisher information matrix for these $K-1$ parameters is the $(K-1) \\times (K-1)$ submatrix of $\\mathbf{I}$ obtained by removing the row and column corresponding to state $0$:\n$$\n\\mathbf{I}_{\\text{red}} = (I_{ij})_{i,j \\in \\{1,\\ldots,K-1\\}}\n$$\nThis reduced matrix is invertible. The covariance matrix for the free energy differences $(\\Delta f_{10}, \\ldots, \\Delta f_{(K-1)0})$ is given by its inverse:\n$$\n\\mathbf{C} = \\mathbf{I}_{\\text{red}}^{-1}\n$$\nThe asymptotic standard error for the estimate of $\\Delta f_{k0}$ is the square root of the corresponding diagonal element of $\\mathbf{C}$:\n$$\ns_{k0} = \\sqrt{C_{k-1, k-1}} \\quad \\text{for } k \\in \\{1, \\ldots, K-1\\}\n$$\n(using $0$-based indexing for the matrix $\\mathbf{C}$).",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve_mbar(u_kn, N_k, tol=1e-12, max_iter=10000):\n    \"\"\"\n    Solves the MBAR self-consistent equations for the free energies.\n\n    Args:\n        u_kn (np.ndarray): A KxN matrix of reduced potentials u_k(x_n).\n        N_k (np.ndarray): A K-element array of sample counts per state.\n        tol (float): The convergence tolerance.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing:\n            - f_k (np.ndarray): K-element array of estimated dimensionless free energies with f_0 = 0.\n            - df_k (np.ndarray): (K-1)-element array of standard errors for f_k - f_0.\n    \"\"\"\n    K, N = u_kn.shape\n    f_k = np.zeros(K, dtype=np.float64)\n    log_N_k = np.log(N_k)\n\n    for i in range(max_iter):\n        f_k_old = np.copy(f_k)\n\n        # Numerically stable calculation of new free energies\n        # A_kn = log(N_k) + f_k - u_kn\n        A_kn = log_N_k[:, np.newaxis] + f_k[:, np.newaxis] - u_kn\n        \n        # log_D_n = log(sum_k exp(A_kn))\n        log_D_n = logsumexp(A_kn, axis=0)\n        \n        # log [ exp(-u_kn) / D_n ] = -u_kn - log_D_n\n        log_term = -u_kn - log_D_n[np.newaxis, :]\n        \n        # f'_k = -log(sum_n exp(log_term))\n        f_k_new_unshifted = -logsumexp(log_term, axis=1)\n        \n        # Enforce gauge f_0 = 0\n        f_k = f_k_new_unshifted - f_k_new_unshifted[0]\n\n        if np.max(np.abs(f_k - f_k_old))  tol:\n            break\n    else:\n        raise RuntimeError(\"MBAR did not converge in {} iterations.\".format(max_iter))\n\n    # Calculate uncertainties\n    # A_kn = log(N_k) + f_k - u_kn for converged f_k\n    A_kn = log_N_k[:, np.newaxis] + f_k[:, np.newaxis] - u_kn\n    log_D_n = logsumexp(A_kn, axis=0)\n    \n    # W_kn = exp(A_kn) / D_n = exp(A_kn - log_D_n)\n    log_W_kn = A_kn - log_D_n[np.newaxis, :]\n    W_kn = np.exp(log_W_kn)\n\n    # Fisher information matrix I_ij = sum_n (W_in * delta_ij - W_in * W_jn)\n    # This can be written as I = diag(sum(W, axis=1)) - W @ W.T\n    I = np.diag(np.sum(W_kn, axis=1)) - W_kn @ W_kn.T\n\n    # Invert the reduced (K-1)x(K-1) Fisher information matrix\n    # for states 1..K-1 to get the covariance of (f_1-f_0, ..., f_{K-1}-f_0)\n    I_red = I[1:, 1:]\n    try:\n        C_red = np.linalg.inv(I_red)\n        df_k = np.sqrt(np.diag(C_red))\n    except np.linalg.LinAlgError:\n        # This can happen if states are not well-connected,\n        # but shouldn't for the given test cases.\n        df_k = np.full(K - 1, np.nan)\n\n    return f_k[1:], df_k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'K': 2, 'mu_k': [-1.0, 1.0], 'k_spring': 5.0, 'N_k': [80, 80], 'seed': 246810},\n        {'K': 3, 'mu_k': [-1.0, 0.0, 1.5], 'k_spring': 4.0, 'N_k': [100, 10, 5], 'seed': 13579},\n        {'K': 2, 'mu_k': [0.0, 0.2], 'k_spring': 10.0, 'N_k': [50, 50], 'seed': 424242},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        K = case['K']\n        mu_k = np.array(case['mu_k'])\n        k_spring = case['k_spring']\n        N_k = np.array(case['N_k'])\n        seed = case['seed']\n        \n        rng = np.random.default_rng(seed)\n\n        # 1. Generate synthetic data\n        samples_per_state = []\n        for k in range(K):\n            # Samples drawn from N(mu_k, sigma^2 = 1/k_spring)\n            std_dev = np.sqrt(1.0 / k_spring)\n            samples = rng.normal(loc=mu_k[k], scale=std_dev, size=N_k[k])\n            samples_per_state.append(samples)\n        \n        # Pool all samples\n        x_n = np.concatenate(samples_per_state)\n        N = x_n.shape[0]\n\n        # 2. Construct reduced potential matrix u_kn\n        # u_k(x_n) = 0.5 * k_spring * (x_n - mu_k)^2\n        u_kn = 0.5 * k_spring * (x_n[np.newaxis, :] - mu_k[:, np.newaxis])**2\n\n        # 3. Solve MBAR equations\n        f_k_diff, df_k_err = solve_mbar(u_kn, N_k)\n\n        # 4. Collect results for this case\n        case_results = []\n        for f, df in zip(f_k_diff, df_k_err):\n            case_results.extend([f, df])\n        all_results.extend(case_results)\n\n    # Format and print the final output\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}