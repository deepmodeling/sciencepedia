{
    "hands_on_practices": [
        {
            "introduction": "Data points from a molecular dynamics trajectory are not statistically independent, a fact that fundamentally impacts the calculation of uncertainties. This exercise provides the essential tools for handling time-correlated data by guiding you through the derivation of the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, and the effective sample size, $N_{\\mathrm{eff}}$. Mastering these concepts is the first step toward a rigorous statistical analysis of any simulation time series. ",
            "id": "3844668",
            "problem": "In computational chemical biology, enhanced sampling methods aim to reduce correlation between successive configurations so that time averages converge rapidly to ensemble averages. Consider a discrete-time trajectory from Molecular Dynamics (MD) or Markov Chain Monte Carlo (MCMC) for a stationary, ergodic observable $A_{n}$ sampled at times $t_{n} = n \\Delta t$ with $n = 1, \\dots, N$. Let the mean be $\\mu = \\mathbb{E}[A_{n}]$, the variance be $\\sigma^{2} = \\mathbb{E}[(A_{n} - \\mu)^{2}]$, and the autocovariance be $C(k) = \\mathbb{E}[(A_{n} - \\mu)(A_{n+k} - \\mu)]$. Define the normalized autocorrelation function $\\rho(k) = C(k)/C(0)$ and assume $\\sum_{k=1}^{\\infty} |\\rho(k)|  \\infty$. Starting only from the definitions of variance and covariance, and without invoking any pre-packaged formulas, derive the large-$N$ expression for the variance of the sample mean $\\bar{A} = \\frac{1}{N} \\sum_{n=1}^{N} A_{n}$ in terms of $\\rho(k)$, identify the factor that inflates this variance relative to $N$ independent samples, and define the Integrated Autocorrelation Time (IAT), denoted by $\\tau_{\\mathrm{int}}$, in terms of $\\rho(k)$ so that the variance inflation factor equals $2 \\tau_{\\mathrm{int}}$. Then define the effective sample size $N_{\\mathrm{eff}}$ by equating the variance of $\\bar{A}$ for the correlated samples to that of $N_{\\mathrm{eff}}$ independent samples, and express $N_{\\mathrm{eff}}$ in terms of $N$ and $\\tau_{\\mathrm{int}}$.\n\nNow consider the specific, scientifically plausible case of an exponentially decaying normalized autocorrelation function,\n$$\n\\rho(k) = \\exp\\!\\left(-\\frac{k \\, \\Delta t}{\\tau_{c}}\\right),\n$$\nwith correlation time $\\tau_{c}$. Compute a closed-form expression for $\\tau_{\\mathrm{int}}$ for this $\\rho(k)$ and, from it, obtain $N_{\\mathrm{eff}}$. Use the values $N = 5 \\times 10^{5}$, $\\Delta t = 0.002 \\,\\mathrm{ps}$, and $\\tau_{c} = 1 \\,\\mathrm{ps}$. Report the numerical value of $N_{\\mathrm{eff}}$ as a dimensionless quantity, rounded to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the statistical analysis of time-series data from molecular simulations, is well-posed with all necessary information provided, and is expressed in objective, formal language. We proceed with the solution.\n\nThe sample mean, $\\bar{A}$, of an observable $A$ is given by\n$$\n\\bar{A} = \\frac{1}{N} \\sum_{n=1}^{N} A_n\n$$\nwhere $A_n$ are the samples at discrete time steps. For a stationary process, the expectation of each sample is the true mean, $\\mathbb{E}[A_n] = \\mu$. The expectation of the sample mean is\n$$\n\\mathbb{E}[\\bar{A}] = \\mathbb{E}\\left[\\frac{1}{N} \\sum_{n=1}^{N} A_n\\right] = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{E}[A_n] = \\frac{1}{N} \\sum_{n=1}^{N} \\mu = \\frac{N\\mu}{N} = \\mu\n$$\nThe variance of the sample mean is defined as $\\mathrm{Var}(\\bar{A}) = \\mathbb{E}[(\\bar{A} - \\mathbb{E}[\\bar{A}])^2] = \\mathbb{E}[(\\bar{A} - \\mu)^2]$. Substituting the expression for $\\bar{A}$:\n$$\n\\mathrm{Var}(\\bar{A}) = \\mathbb{E}\\left[\\left(\\frac{1}{N} \\sum_{n=1}^{N} A_n - \\mu\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\frac{1}{N} \\sum_{n=1}^{N} (A_n - \\mu)\\right)^2\\right]\n$$\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1}{N^2} \\mathbb{E}\\left[\\left(\\sum_{i=1}^{N} (A_i - \\mu)\\right) \\left(\\sum_{j=1}^{N} (A_j - \\mu)\\right)\\right] = \\frac{1}{N^2} \\mathbb{E}\\left[\\sum_{i=1}^{N} \\sum_{j=1}^{N} (A_i - \\mu)(A_j - \\mu)\\right]\n$$\nBy linearity of expectation, we can move the operator inside the sums:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathbb{E}[(A_i - \\mu)(A_j - \\mu)]\n$$\nThe term inside the summation is the definition of the autocovariance, $C(k) = \\mathbb{E}[(A_n - \\mu)(A_{n+k} - \\mu)]$. Due to stationarity, this depends only on the time lag $|i-j|$. Thus, $\\mathbb{E}[(A_i - \\mu)(A_j - \\mu)] = C(|i-j|)$.\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} C(|i-j|)\n$$\nThe double summation contains $N^2$ terms. We can group them by the lag $k = |i-j|$.\nFor $k=0$, we have $i=j$, which occurs $N$ times. The contribution is $N C(0)$.\nFor $k  0$, the lag $|i-j|=k$ occurs for pairs $(i, i+k)$ and $(i+k, i)$. For a fixed $k$, there are $N-k$ such pairs of each type. Thus, there are $2(N-k)$ terms for a given lag $k \\in \\{1, 2, \\dots, N-1\\}$. The total contribution from these terms is $\\sum_{k=1}^{N-1} 2(N-k) C(k)$.\nSumming all contributions:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{1}{N^2} \\left[ N C(0) + 2 \\sum_{k=1}^{N-1} (N-k) C(k) \\right]\n$$\nWe can factor out $C(0)$ and $N$:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{C(0)}{N} \\left[ 1 + \\frac{2}{N} \\sum_{k=1}^{N-1} (N-k) \\frac{C(k)}{C(0)} \\right] = \\frac{C(0)}{N} \\left[ 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\frac{C(k)}{C(0)} \\right]\n$$\nUsing the definitions $\\sigma^2 = C(0)$ and $\\rho(k) = C(k)/C(0)$:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{\\sigma^2}{N} \\left[ 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho(k) \\right]\n$$\nFor large $N$, the term $k/N$ is negligible for the values of $k$ where $\\rho(k)$ is significantly different from zero (due to the condition $\\sum_{k=1}^{\\infty} |\\rho(k)|  \\infty$). Also, the upper limit of the sum can be extended to infinity as $\\rho(k)$ decays. This gives the large-$N$ expression:\n$$\n\\mathrm{Var}(\\bar{A}) \\approx \\frac{\\sigma^2}{N} \\left[ 1 + 2 \\sum_{k=1}^{\\infty} \\rho(k) \\right]\n$$\nFor $N$ independent samples, $\\rho(k)=0$ for $k0$, and the variance is $\\sigma^2/N$. The factor that inflates the variance for correlated samples is the term in the brackets:\n$$\n\\text{Variance Inflation Factor} = g = 1 + 2 \\sum_{k=1}^{\\infty} \\rho(k)\n$$\nThe problem defines the Integrated Autocorrelation Time, $\\tau_{\\mathrm{int}}$, such that this factor equals $2\\tau_{\\mathrm{int}}$.\n$$\n2\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho(k) \\quad \\implies \\quad \\tau_{\\mathrm{int}} = \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho(k)\n$$\nThis quantity $\\tau_{\\mathrm{int}}$ is dimensionless, representing the IAT in units of the sampling interval $\\Delta t$. The effective sample size, $N_{\\mathrm{eff}}$, is defined by equating the variance of the mean of the correlated samples to that of $N_{\\mathrm{eff}}$ independent samples:\n$$\n\\mathrm{Var}(\\bar{A}) = \\frac{\\sigma^2}{N_{\\mathrm{eff}}}\n$$\nComparing this with the derived expression $\\mathrm{Var}(\\bar{A}) = \\frac{\\sigma^2}{N} (2\\tau_{\\mathrm{int}})$:\n$$\n\\frac{\\sigma^2}{N_{\\mathrm{eff}}} = \\frac{\\sigma^2}{N} (2\\tau_{\\mathrm{int}}) \\quad \\implies \\quad N_{\\mathrm{eff}} = \\frac{N}{2\\tau_{\\mathrm{int}}}\n$$\nNow we consider the specific case $\\rho(k) = \\exp\\left(-\\frac{k \\, \\Delta t}{\\tau_{c}}\\right)$. We compute the sum:\n$$\n\\sum_{k=1}^{\\infty} \\rho(k) = \\sum_{k=1}^{\\infty} \\exp\\left(-\\frac{k \\Delta t}{\\tau_c}\\right) = \\sum_{k=1}^{\\infty} \\left[\\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)\\right]^k\n$$\nThis is a geometric series with first term $r = \\exp(-\\Delta t/\\tau_c)$ and ratio $r$. The sum is $\\frac{r}{1-r}$.\n$$\n\\sum_{k=1}^{\\infty} \\rho(k) = \\frac{\\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)}{1 - \\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)}\n$$\nWe substitute this into the expression for $\\tau_{\\mathrm{int}}$:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1}{2} + \\frac{\\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)}{1 - \\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)} = \\frac{1 - \\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right) + 2\\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)}{2\\left(1 - \\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)\\right)} = \\frac{1 + \\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)}{2\\left(1 - \\exp\\left(-\\frac{\\Delta t}{\\tau_c}\\right)\\right)}\n$$\nThis can be expressed using the hyperbolic cotangent function. Let $x = \\frac{\\Delta t}{\\tau_c}$. Then $\\tau_{\\mathrm{int}} = \\frac{1}{2} \\frac{1+e^{-x}}{1-e^{-x}}$. Multiplying the numerator and denominator by $e^x$ gives $\\frac{1}{2}\\frac{e^x+1}{e^x-1} = \\frac{1}{2}\\coth\\left(\\frac{x}{2}\\right)$. Thus, the closed-form expression for $\\tau_{\\mathrm{int}}$ is:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1}{2} \\coth\\left(\\frac{\\Delta t}{2\\tau_c}\\right)\n$$\nFrom this, we find $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{N}{2\\tau_{\\mathrm{int}}} = \\frac{N}{\\coth\\left(\\frac{\\Delta t}{2\\tau_c}\\right)}\n$$\nWe now substitute the given numerical values: $N = 5 \\times 10^5$, $\\Delta t = 0.002 \\, \\mathrm{ps}$, and $\\tau_c = 1 \\, \\mathrm{ps}$.\nThe argument of the hyperbolic cotangent is:\n$$\n\\frac{\\Delta t}{2\\tau_c} = \\frac{0.002 \\, \\mathrm{ps}}{2 \\times 1 \\, \\mathrm{ps}} = 0.001\n$$\nSo we need to compute:\n$$\nN_{\\mathrm{eff}} = \\frac{5 \\times 10^5}{\\coth(0.001)}\n$$\nThe value of $\\coth(0.001)$ is:\n$$\n\\coth(0.001) = \\frac{\\exp(0.001) + \\exp(-0.001)}{\\exp(0.001) - \\exp(-0.001)} \\approx 1000.0003333335\n$$\nSubstituting this value:\n$$\nN_{\\mathrm{eff}} = \\frac{5 \\times 10^5}{1000.0003333335} \\approx 499.999833333166...\n$$\nRounding this value to four significant figures gives $500.0$.",
            "answer": "$$\n\\boxed{500.0}\n$$"
        },
        {
            "introduction": "Building upon the concept of statistical inefficiency, this practice demonstrates its direct consequence on one of the most important outputs of enhanced sampling: the Potential of Mean Force (PMF). You will derive an expression for the variance of an estimated free energy difference, revealing how time correlations in the trajectory inflate the uncertainty of the resulting PMF. This exercise makes the abstract concept of statistical error tangible by connecting it to the confidence you can have in your calculated free energy landscapes. ",
            "id": "3844622",
            "problem": "In enhanced sampling Molecular Dynamics (MD) used in computational chemical biology, one often estimates a one-dimensional potential of mean force (PMF) along a reaction coordinate from a time series that exhibits time correlations. Consider a stationary, ergodic, effectively unbiased time series $\\{X_t\\}_{t=1}^{N}$ of length $N$ along a reaction coordinate discretized into non-overlapping bins indexed by $i$ and $j$. Define the indicator variables $I_i(t) = 1$ if $X_t$ is in bin $i$ and $I_i(t) = 0$ otherwise, and similarly for $I_j(t)$. Let $p_i = \\mathbb{E}[I_i(t)]$ and $p_j = \\mathbb{E}[I_j(t)]$ denote the true bin probabilities. The normalized autocorrelation function (ACF) for $I_i$ is $\\rho_i(\\tau) = \\frac{\\mathbb{E}[(I_i(t)-p_i)(I_i(t+\\tau)-p_i)]}{\\mathbb{V}\\mathrm{ar}[I_i(t)]}$ for lag $\\tau \\geq 0$, and analogously for $I_j$. The statistical inefficiency is defined by\n$$\ng \\equiv 1 + 2 \\sum_{\\tau=1}^{\\infty} \\rho(\\tau),\n$$\nwhere, for this problem, you may assume that the same $\\rho(\\tau)$ (and thus the same $g$) applies to both $I_i$ and $I_j$, and that time correlations inflate both variances and covariances of sample means by the same factor $g$ relative to independent sampling.\n\nThe PMF $w(x)$ is defined up to an additive constant by $w(x) = -k_B T \\ln p(x) + C$, where $k_B$ is the Boltzmann constant and $T$ is the absolute temperature. The free energy difference between bins $i$ and $j$ is $\\Delta w = w_i - w_j$. An estimator based on the trajectory is\n$$\n\\widehat{\\Delta w} = -k_B T \\ln \\widehat{p}_i + k_B T \\ln \\widehat{p}_j,\n$$\nwhere $\\widehat{p}_i = \\frac{1}{N} \\sum_{t=1}^{N} I_i(t)$ and $\\widehat{p}_j = \\frac{1}{N} \\sum_{t=1}^{N} I_j(t)$.\n\nStarting from the definitions above, the stationarity of the time series, and the central limit theorem for correlated stationary processes, derive the leading large-$N$ expression for the variance $\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}]$ that explicitly shows how the statistical inefficiency $g$ inflates the uncertainty due to time correlations. Assume disjoint bins so that $I_i(t) I_j(t) = 0$ for all $t$, and use only first-order error propagation (delta method) in $\\widehat{p}_i$ and $\\widehat{p}_j$.\n\nProvide your final result as a single closed-form analytical expression in terms of $k_B$, $T$, $g$, $N$, $p_i$, and $p_j$. Express the variance in Joules squared. Do not evaluate numerically.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It represents a standard derivation in the statistical analysis of molecular simulation data.\n\nThe objective is to find the variance of the free energy difference estimator, $\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}]$, for large sample size $N$. The estimator is given by:\n$$\n\\widehat{\\Delta w} = k_B T (\\ln \\widehat{p}_j - \\ln \\widehat{p}_i)\n$$\nThis is a function of the two random variables $\\widehat{p}_i$ and $\\widehat{p}_j$, which are the sample estimates of the true bin probabilities $p_i$ and $p_j$. To find the variance of $\\widehat{\\Delta w}$, we employ the first-order multivariate error propagation formula, also known as the delta method. For a function $f(X, Y)$, the variance is approximated as:\n$$\n\\mathbb{V}\\mathrm{ar}[f(X, Y)] \\approx \\left(\\frac{\\partial f}{\\partial X}\\right)^2 \\mathbb{V}\\mathrm{ar}[X] + \\left(\\frac{\\partial f}{\\partial Y}\\right)^2 \\mathbb{V}\\mathrm{ar}[Y] + 2 \\left(\\frac{\\partial f}{\\partial X}\\right) \\left(\\frac{\\partial f}{\\partial Y}\\right) \\mathbb{C}\\mathrm{ov}[X, Y]\n$$\nwhere the partial derivatives are evaluated at the expected values of the variables, i.e., at $X = \\mathbb{E}[X] = p_i$ and $Y = \\mathbb{E}[Y] = p_j$.\n\nIn our case, the function is $f(\\widehat{p}_i, \\widehat{p}_j) = k_B T (\\ln \\widehat{p}_j - \\ln \\widehat{p}_i)$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial \\widehat{p}_i} = -k_B T \\frac{1}{\\widehat{p}_i} \\quad \\implies \\quad \\left. \\frac{\\partial f}{\\partial \\widehat{p}_i} \\right|_{\\widehat{p}_i=p_i} = -\\frac{k_B T}{p_i}\n$$\n$$\n\\frac{\\partial f}{\\partial \\widehat{p}_j} = k_B T \\frac{1}{\\widehat{p}_j} \\quad \\implies \\quad \\left. \\frac{\\partial f}{\\partial \\widehat{p}_j} \\right|_{\\widehat{p}_j=p_j} = \\frac{k_B T}{p_j}\n$$\nSubstituting these derivatives into the error propagation formula yields:\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}] \\approx \\left(-\\frac{k_B T}{p_i}\\right)^2 \\mathbb{V}\\mathrm{ar}[\\widehat{p}_i] + \\left(\\frac{k_B T}{p_j}\\right)^2 \\mathbb{V}\\mathrm{ar}[\\widehat{p}_j] + 2 \\left(-\\frac{k_B T}{p_i}\\right) \\left(\\frac{k_B T}{p_j}\\right) \\mathbb{C}\\mathrm{ov}[\\widehat{p}_i, \\widehat{p}_j]\n$$\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}] \\approx (k_B T)^2 \\left( \\frac{\\mathbb{V}\\mathrm{ar}[\\widehat{p}_i]}{p_i^2} + \\frac{\\mathbb{V}\\mathrm{ar}[\\widehat{p}_j]}{p_j^2} - \\frac{2 \\mathbb{C}\\mathrm{ov}[\\widehat{p}_i, \\widehat{p}_j]}{p_i p_j} \\right)\n$$\nNext, we must determine the variances $\\mathbb{V}\\mathrm{ar}[\\widehat{p}_i]$, $\\mathbb{V}\\mathrm{ar}[\\widehat{p}_j]$, and the covariance $\\mathbb{C}\\mathrm{ov}[\\widehat{p}_i, \\widehat{p}_j]$. The problem states that time correlations inflate these quantities by a factor of the statistical inefficiency, $g$, relative to independent sampling. We first calculate the values for independent sampling.\n\nThe estimators $\\widehat{p}_i$ and $\\widehat{p}_j$ are sample means of the indicator variables $I_i(t)$ and $I_j(t)$. The indicator variable $I_i(t)$ is a Bernoulli random variable with success probability $p_i$. Its variance is:\n$$\n\\mathbb{V}\\mathrm{ar}[I_i(t)] = \\mathbb{E}[I_i(t)^2] - (\\mathbb{E}[I_i(t)])^2 = p_i - p_i^2 = p_i(1-p_i)\n$$\nFor a sample of $N$ independent and identically distributed (i.i.d.) variables, the variance of the sample mean $\\widehat{p}_i = \\frac{1}{N}\\sum_{t=1}^N I_i(t)$ is:\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{p}_i]_{\\text{ind}} = \\frac{\\mathbb{V}\\mathrm{ar}[I_i(t)]}{N} = \\frac{p_i(1-p_i)}{N}\n$$\nSimilarly, $\\mathbb{V}\\mathrm{ar}[\\widehat{p}_j]_{\\text{ind}} = \\frac{p_j(1-p_j)}{N}$.\n\nFor the covariance, we first find the covariance of the indicator variables at a single time point $t$. Since the bins are disjoint, $I_i(t)I_j(t) = 0$ for all $t$.\n$$\n\\mathbb{C}\\mathrm{ov}[I_i(t), I_j(t)] = \\mathbb{E}[I_i(t)I_j(t)] - \\mathbb{E}[I_i(t)]\\mathbb{E}[I_j(t)] = 0 - p_i p_j = -p_i p_j\n$$\nFor $N$ i.i.d. samples, the covariance of the sample means is:\n$$\n\\mathbb{C}\\mathrm{ov}[\\widehat{p}_i, \\widehat{p}_j]_{\\text{ind}} = \\mathbb{C}\\mathrm{ov}\\left[\\frac{1}{N}\\sum_{t=1}^N I_i(t), \\frac{1}{N}\\sum_{s=1}^N I_j(s)\\right] = \\frac{1}{N^2}\\sum_{t,s}\\mathbb{C}\\mathrm{ov}[I_i(t), I_j(s)]\n$$\nSince observations at different times are independent in the i.i.d. case, $\\mathbb{C}\\mathrm{ov}[I_i(t), I_j(s)] = 0$ for $t \\neq s$. The sum reduces to terms where $t=s$:\n$$\n\\mathbb{C}\\mathrm{ov}[\\widehat{p}_i, \\widehat{p}_j]_{\\text{ind}} = \\frac{1}{N^2}\\sum_{t=1}^N \\mathbb{C}\\mathrm{ov}[I_i(t), I_j(t)] = \\frac{N \\mathbb{C}\\mathrm{ov}[I_i(t), I_j(t)]}{N^2} = \\frac{-p_i p_j}{N}\n$$\nNow, we incorporate the effect of time correlation by multiplying these i.i.d. results by the statistical inefficiency $g$, as per the problem statement. This is the leading large-$N$ correction from the central limit theorem for stationary processes.\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{p}_i] = g \\cdot \\mathbb{V}\\mathrm{ar}[\\widehat{p}_i]_{\\text{ind}} = \\frac{g p_i(1-p_i)}{N}\n$$\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{p}_j] = g \\cdot \\mathbb{V}\\mathrm{ar}[\\widehat{p}_j]_{\\text{ind}} = \\frac{g p_j(1-p_j)}{N}\n$$\n$$\n\\mathbb{C}\\mathrm{ov}[\\widehat{p}_i, \\widehat{p}_j] = g \\cdot \\mathbb{C}\\mathrm{ov}[\\widehat{p}_i, \\widehat{p}_j]_{\\text{ind}} = \\frac{-g p_i p_j}{N}\n$$\nFinally, we substitute these expressions back into the formula for $\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}]$:\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}] \\approx (k_B T)^2 \\left( \\frac{1}{p_i^2} \\left[\\frac{g p_i(1-p_i)}{N}\\right] + \\frac{1}{p_j^2} \\left[\\frac{g p_j(1-p_j)}{N}\\right] - \\frac{2}{p_i p_j} \\left[\\frac{-g p_i p_j}{N}\\right] \\right)\n$$\nFactor out the common term $\\frac{g(k_B T)^2}{N}$:\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}] \\approx \\frac{g(k_B T)^2}{N} \\left( \\frac{1-p_i}{p_i} + \\frac{1-p_j}{p_j} - \\frac{2(-p_i p_j)}{p_i p_j} \\right)\n$$\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}] \\approx \\frac{g(k_B T)^2}{N} \\left( \\left(\\frac{1}{p_i} - 1\\right) + \\left(\\frac{1}{p_j} - 1\\right) + 2 \\right)\n$$\nSimplifying the terms inside the parenthesis:\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}] \\approx \\frac{g(k_B T)^2}{N} \\left( \\frac{1}{p_i} + \\frac{1}{p_j} - 2 + 2 \\right)\n$$\n$$\n\\mathbb{V}\\mathrm{ar}[\\widehat{\\Delta w}] \\approx \\frac{g(k_B T)^2}{N} \\left( \\frac{1}{p_i} + \\frac{1}{p_j} \\right)\n$$\nThis is the final expression for the variance of the estimated free energy difference, explicitly showing its dependence on the thermal energy scale $(k_B T)$, the statistical inefficiency $g$, the number of samples $N$, and the probabilities of the states $p_i$ and $p_j$.",
            "answer": "$$\n\\boxed{\\frac{g (k_B T)^2}{N} \\left(\\frac{1}{p_i} + \\frac{1}{p_j}\\right)}\n$$"
        },
        {
            "introduction": "The efficiency of many enhanced sampling methods depends critically on the choice of collective variables (CVs). This practice shifts our focus from post-simulation analysis to rational simulation design, introducing a powerful, data-driven method for CV selection. By working through the Variational Approach for Markov Processes (VAMP), you will learn how to score candidate CVs based on their ability to capture the slow dynamical modes of the system, enabling you to design more effective and insightful simulations from the outset. ",
            "id": "3844642",
            "problem": "You are given a set of time series feature trajectories representing putative collective variables (CVs) for molecular dynamics, and you must formalize and implement a principled model selection criterion grounded in the Variational Approach for Markov Processes (VAMP) to choose CV subsets that best capture slow dynamical modes at a specified lag time. Your tasks are to (i) derive a scoring functional from first principles that is invariant to invertible linear rescaling of features and depends only on empirical time-lagged covariances and singular values, and (ii) implement an algorithm that evaluates this score for candidate CV subsets and selects the one that maximizes it.\n\nStart from the following foundational base:\n- Consider a stationary, ergodic, time-homogeneous stochastic process with vector-valued observables (features) $\\,\\mathbf{x}_t \\in \\mathbb{R}^d\\,$ sampled at discrete time steps $\\,t = 0, 1, \\ldots, T-1\\,$. For a fixed lag $\\,\\tau \\in \\mathbb{N}\\,$ with $\\,0  \\tau  T\\,$, define paired samples $\\,(\\mathbf{x}_t, \\mathbf{x}_{t+\\tau})\\,$ for $\\,t = 0, \\ldots, T-\\tau-1\\,$. Assume the features have been centered to have zero mean within each block $\\,\\{\\mathbf{x}_t\\}\\,$ and $\\,\\{\\mathbf{x}_{t+\\tau}\\}\\,$.\n- Define the empirical covariance matrices using $\\,N = T - \\tau\\,$ pairs:\n  - The instantaneous covariance $\\,C_{00} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\mathbf{x}_t \\mathbf{x}_t^\\top\\,$,\n  - The time-lagged covariance $\\,C_{0\\tau} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\mathbf{x}_t \\mathbf{x}_{t+\\tau}^\\top\\,$,\n  - The lagged instantaneous covariance $\\,C_{\\tau\\tau} = \\frac{1}{N} \\sum_{t=0}^{N-1} \\mathbf{x}_{t+\\tau} \\mathbf{x}_{t+\\tau}^\\top\\,$.\n- The singular value decomposition (SVD) states that any real matrix $\\,A \\in \\mathbb{R}^{m \\times n}\\,$ admits $\\,A = U \\Sigma V^\\top\\,$ with orthogonal $\\,U, V\\,$ and nonnegative diagonal $\\,\\Sigma\\,$, whose diagonal entries are the singular values.\n\nYour derivation goal:\n- Construct a score that measures dynamical consistency at lag $\\,\\tau\\,$ for a linear model of the form $\\,\\mathbf{y}_t = W^\\top \\mathbf{x}_t\\,$, and is invariant to any invertible linear rescaling of the features $\\,\\mathbf{x}_t \\mapsto A \\mathbf{x}_t\\,$, where $\\,A \\in \\mathbb{R}^{d \\times d}\\,$ is invertible. The score must be expressible purely in terms of $\\,C_{00}\\,$, $\\,C_{0\\tau}\\,$, $\\,C_{\\tau\\tau}\\,$, and the singular values of a matrix constructed from them.\n- Show that an equivalent computable form of this score uses a “whitened” time-lagged object that depends on $\\,C_{00}^{-1/2}\\,$ and $\\,C_{\\tau\\tau}^{-1/2}\\,$, and yields a scalar equal to a sum of squared singular values.\n\nImplementation requirements:\n- You will implement a procedure that, given a data matrix $\\,X \\in \\mathbb{R}^{T \\times D}\\,$ of $\\,D\\,$ candidate features, a lag $\\,\\tau\\,$, a list of candidate index subsets $\\,\\mathcal{S}\\,$ (each subset $\\,S \\subset \\{0,1,\\ldots,D-1\\}\\,$ specifies which columns of $\\,X\\,$ to use), and a small Tikhonov regularization parameter $\\,\\epsilon  0\\,$, returns the index of the subset in $\\,\\mathcal{S}\\,$ that maximizes the score defined above together with the maximized score value. The regularization must be applied to $\\,C_{00}\\,$ and $\\,C_{\\tau\\tau}\\,$ as $\\,C_{00} + \\epsilon I\\,$ and $\\,C_{\\tau\\tau} + \\epsilon I\\,$ before matrix square-root inverses are computed, where $\\,I\\,$ is the identity matrix of appropriate size.\n- Center each block $\\,\\{\\mathbf{x}_t\\}\\,$ and $\\,\\{\\mathbf{x}_{t+\\tau}\\}\\,$ independently by subtracting their respective sample means prior to covariance estimation.\n- Use only algebraic operations, eigenvalue decompositions or SVD as needed to compute symmetric matrix square-root inverses and singular values.\n- When comparing candidate subsets, if multiple subsets obtain numerically equal maximized scores within absolute tolerance $\\,10^{-12}\\,$, choose the first occurrence in $\\,\\mathcal{S}\\,$.\n\nTest suite:\n- You must evaluate your implementation on the following three cases. In each case, indices are zero-based. All trigonometric functions take their arguments in radians.\n- Case $\\,1\\,$:\n  - Parameters: $\\,T = 200\\,$, $\\,D = 5\\,$, $\\,\\tau = 5\\,$, $\\,\\epsilon = 10^{-6}\\,$.\n  - For $\\,t \\in \\{0,1,\\ldots,199\\}\\,$ define features:\n    - $\\,f_1(t) = \\sin(0.1\\, t)\\,$,\n    - $\\,f_2(t) = \\cos(0.1\\, t)\\,$,\n    - $\\,f_3(t) = \\sin(0.05\\, t + 0.3)\\,$,\n    - $\\,f_4(t) = f_1(t) + 0.5\\, f_3(t)\\,$,\n    - $\\,f_5(t) = \\sin(0.2\\, t)\\,$.\n  - Let $\\,X \\in \\mathbb{R}^{200 \\times 5}\\,$ have columns $\\,f_1, f_2, f_3, f_4, f_5\\,$ in this order.\n  - Candidate subsets $\\,\\mathcal{S}_1 = \\{ [0,1], [3], [2,4], [0,2,3], [1,4] \\}\\,$.\n- Case $\\,2\\,$:\n  - Parameters: $\\,T = 150\\,$, $\\,D = 3\\,$, $\\,\\tau = 4\\,$, $\\,\\epsilon = 10^{-6}\\,$.\n  - For $\\,t \\in \\{0,1,\\ldots,149\\}\\,$ define:\n    - $\\,g_1(t) = \\sin(0.12\\, t + 0.1)\\,$,\n    - $\\,g_2(t) = g_1(t)\\,$,\n    - $\\,g_3(t) = \\cos(0.07\\, t)\\,$.\n  - Let $\\,X \\in \\mathbb{R}^{150 \\times 3}\\,$ have columns $\\,g_1, g_2, g_3\\,$ in this order.\n  - Candidate subsets $\\,\\mathcal{S}_2 = \\{ [0,1], [0,2], [1,2] \\}\\,$.\n- Case $\\,3\\,$:\n  - Parameters: $\\,T = 20\\,$, $\\,D = 2\\,$, $\\,\\tau = 15\\,$, $\\,\\epsilon = 10^{-6}\\,$.\n  - For $\\,t \\in \\{0,1,\\ldots,19\\}\\,$ define:\n    - $\\,h_1(t) = \\sin(0.5\\, t)\\,$,\n    - $\\,h_2(t) = \\cos(0.3\\, t)\\,$.\n  - Let $\\,X \\in \\mathbb{R}^{20 \\times 2}\\,$ have columns $\\,h_1, h_2\\,$ in this order.\n  - Candidate subsets $\\,\\mathcal{S}_3 = \\{ [0], [1], [0,1] \\}\\,$.\n\nYour program must:\n- For each case, compute the score for every subset in the specified candidate list and select the maximizing subset index (its position in the list $\\,\\mathcal{S}\\,$, starting at $\\,0\\,$) and the corresponding maximized score value.\n- Round each reported score to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of the form $[i, s]$ with $\\,i\\,$ the selected subset index (an integer) and $\\,s\\,$ the corresponding rounded score (a float). For example, a three-case output has the form $[[i_1,s_1],[i_2,s_2],[i_3,s_3]]$.",
            "solution": "The problem requires the derivation and implementation of a model selection criterion for collective variables (CVs) in molecular dynamics, based on the Variational Approach for Markov Processes (VAMP). This criterion, known as the VAMP-2 score, quantifies the ability of a set of features to capture the slow dynamical processes of a system.\n\n### Part I: Derivation of the VAMP-2 Score\n\nThe core principle of VAMP is to find linear combinations of a given set of basis functions (features) that best approximate the eigenfunctions of the underlying Koopman operator, which describes the evolution of observables. These eigenfunctions correspond to the slowest dynamical processes of the system.\n\nLet the feature set be represented by a vector-valued stochastic process $\\mathbf{x}_t \\in \\mathbb{R}^d$. We seek optimal linear projections of the form $u_t = \\mathbf{a}^\\top \\mathbf{x}_t$ and $v_{t+\\tau} = \\mathbf{b}^\\top \\mathbf{x}_{t+\\tau}$ that maximize the time-lagged correlation, subject to normalization constraints. This can be formulated as a constrained optimization problem:\n\n$$ \\max_{\\mathbf{a}, \\mathbf{b}} \\mathbb{E}[\\,(\\mathbf{a}^\\top \\mathbf{x}_t)(\\mathbf{b}^\\top \\mathbf{x}_{t+\\tau})\\,] \\quad \\text{subject to} \\quad \\mathbb{E}[(\\mathbf{a}^\\top \\mathbf{x}_t)^2] = 1 \\text{ and } \\mathbb{E}[(\\mathbf{b}^\\top \\mathbf{x}_{t+\\tau})^2] = 1 $$\n\nwhere $\\mathbb{E}[\\cdot]$ denotes the expectation value over the stationary process. Using the empirical covariance matrices defined in the problem for a set of $N = T - \\tau$ trajectory pairs $(\\mathbf{x}_t, \\mathbf{x}_{t+\\tau})$:\n- Instantaneous covariance: $C_{00} = \\mathbb{E}[\\mathbf{x}_t \\mathbf{x}_t^\\top]$\n- Lagged instantaneous covariance: $C_{\\tau\\tau} = \\mathbb{E}[\\mathbf{x}_{t+\\tau} \\mathbf{x}_{t+\\tau}^\\top]$\n- Time-lagged covariance: $C_{0\\tau} = \\mathbb{E}[\\mathbf{x}_t \\mathbf{x}_{t+\\tau}^\\top]$\n\nThe optimization problem can be expressed in terms of these matrices:\n\n$$ \\max_{\\mathbf{a}, \\mathbf{b}} \\mathbf{a}^\\top C_{0\\tau} \\mathbf{b} \\quad \\text{subject to} \\quad \\mathbf{a}^\\top C_{00} \\mathbf{a} = 1 \\text{ and } \\mathbf{b}^\\top C_{\\tau\\tau} \\mathbf{b} = 1 $$\n\nThis is a generalized singular value problem. To convert it into a standard singular value problem, we perform a \"whitening\" or \"sphering\" transformation. We define new vectors $\\tilde{\\mathbf{a}}$ and $\\tilde{\\mathbf{b}}$ such that the constraints become simple unit-norm conditions. Let:\n$$ \\tilde{\\mathbf{a}} = C_{00}^{1/2} \\mathbf{a} \\implies \\mathbf{a} = C_{00}^{-1/2} \\tilde{\\mathbf{a}} $$\n$$ \\tilde{\\mathbf{b}} = C_{\\tau\\tau}^{1/2} \\mathbf{b} \\implies \\mathbf{b} = C_{\\tau\\tau}^{-1/2} \\tilde{\\mathbf{b}} $$\n\nHere, $C_{00}^{1/2}$ and $C_{\\tau\\tau}^{1/2}$ are the symmetric matrix square roots of the positive definite covariance matrices. The constraints transform to:\n$$ \\mathbf{a}^\\top C_{00} \\mathbf{a} = (C_{00}^{-1/2} \\tilde{\\mathbf{a}})^\\top C_{00} (C_{00}^{-1/2} \\tilde{\\mathbf{a}}) = \\tilde{\\mathbf{a}}^\\top C_{00}^{-1/2} C_{00} C_{00}^{-1/2} \\tilde{\\mathbf{a}} = \\tilde{\\mathbf{a}}^\\top \\tilde{\\mathbf{a}} = 1 $$\n$$ \\mathbf{b}^\\top C_{\\tau\\tau} \\mathbf{b} = (C_{\\tau\\tau}^{-1/2} \\tilde{\\mathbf{b}})^\\top C_{\\tau\\tau} (C_{\\tau\\tau}^{-1/2} \\tilde{\\mathbf{b}}) = \\tilde{\\mathbf{b}}^\\top C_{\\tau\\tau}^{-1/2} C_{\\tau\\tau} C_{\\tau\\tau}^{-1/2} \\tilde{\\mathbf{b}} = \\tilde{\\mathbf{b}}^\\top \\tilde{\\mathbf{b}} = 1 $$\n\nThe objective function becomes:\n$$ \\mathbf{a}^\\top C_{0\\tau} \\mathbf{b} = (C_{00}^{-1/2} \\tilde{\\mathbf{a}})^\\top C_{0\\tau} (C_{\\tau\\tau}^{-1/2} \\tilde{\\mathbf{b}}) = \\tilde{\\mathbf{a}}^\\top (C_{00}^{-1/2} C_{0\\tau} C_{\\tau\\tau}^{-1/2}) \\tilde{\\mathbf{b}} $$\n\nLet $\\tilde{C}_{0\\tau} = C_{00}^{-1/2} C_{0\\tau} C_{\\tau\\tau}^{-1/2}$ be the whitened time-lagged covariance matrix. The problem is now to maximize $\\tilde{\\mathbf{a}}^\\top \\tilde{C}_{0\\tau} \\tilde{\\mathbf{b}}$ subject to $||\\tilde{\\mathbf{a}}||_2 = 1$ and $||\\tilde{\\mathbf{b}}||_2 = 1$. The solutions to this problem are the singular values of $\\tilde{C}_{0\\tau}$. The maximum value is the largest singular value, $\\sigma_1$, and subsequent solutions yield smaller singular values $\\sigma_2, \\sigma_3, \\ldots, \\sigma_d$, which represent the correlations of orthogonal dynamical modes.\n\nA comprehensive score for the quality of the feature set $\\mathbf{x}_t$ is the VAMP-2 score, which is the sum of the squares of all these singular values:\n$$ S = \\sum_{i=1}^d \\sigma_i^2 $$\nThis is equivalent to the squared Frobenius norm of the whitened covariance matrix:\n$$ S = ||\\tilde{C}_{0\\tau}||_F^2 = \\text{Tr}(\\tilde{C}_{0\\tau}^\\top \\tilde{C}_{0\\tau}) $$\n\nThis score quantifies the total kinetic information captured by the chosen feature set at lag time $\\tau$.\n\n**Invariance to Linear Rescaling:**\nWe must show that this score is invariant to an invertible linear transformation of the features, $\\mathbf{z}_t = A \\mathbf{x}_t$, where $A \\in \\mathbb{R}^{d \\times d}$ is an invertible matrix. The covariance matrices for the transformed features $\\mathbf{z}_t$ are:\n$C_{00}^z = A C_{00} A^\\top$\n$C_{\\tau\\tau}^z = A C_{\\tau\\tau} A^\\top$\n$C_{0\\tau}^z = A C_{0\\tau} A^\\top$\nThe score can be written as $S(\\mathbf{x}) = \\text{Tr}(C_{00}^{-1} C_{0\\tau} C_{\\tau\\tau}^{-1} C_{0\\tau}^\\top)$ using the cyclic property of the trace. For the transformed variables, the score is:\n$S(\\mathbf{z}) = \\text{Tr}((C_{00}^z)^{-1} C_{0\\tau}^z (C_{\\tau\\tau}^z)^{-1} (C_{0\\tau}^z)^\\top)$\nSubstituting the transformed matrices:\n$(C_{00}^z)^{-1} = (A C_{00} A^\\top)^{-1} = (A^\\top)^{-1} C_{00}^{-1} A^{-1}$\n$(C_{\\tau\\tau}^z)^{-1} = (A^\\top)^{-1} C_{\\tau\\tau}^{-1} A^{-1}$\n$(C_{0\\tau}^z)^\\top = (A C_{0\\tau} A^\\top)^\\top = A C_{0\\tau}^\\top A^\\top$\n$S(\\mathbf{z}) = \\text{Tr}( (A^\\top)^{-1} C_{00}^{-1} A^{-1} \\cdot A C_{0\\tau} A^\\top \\cdot (A^\\top)^{-1} C_{\\tau\\tau}^{-1} A^{-1} \\cdot A C_{0\\tau}^\\top A^\\top )$\nThe adjacent terms $A^{-1}A$ and $A^\\top(A^\\top)^{-1}$ cancel to identity matrices:\n$S(\\mathbf{z}) = \\text{Tr}( (A^\\top)^{-1} C_{00}^{-1} C_{0\\tau} C_{\\tau\\tau}^{-1} C_{0\\tau}^\\top A^\\top )$\nUsing the cyclic property of the trace, $\\text{Tr}(XYZ) = \\text{Tr}(ZXY)$:\n$S(\\mathbf{z}) = \\text{Tr}( A^\\top (A^\\top)^{-1} C_{00}^{-1} C_{0\\tau} C_{\\tau\\tau}^{-1} C_{0\\tau}^\\top ) = \\text{Tr}(C_{00}^{-1} C_{0\\tau} C_{\\tau\\tau}^{-1} C_{0\\tau}^\\top) = S(\\mathbf{x})$.\nThis confirms the score's invariance to invertible linear transformations of the feature space, a desirable property for a robust CV quality metric.\n\n### Part II: Algorithmic Implementation\n\nThe algorithm selects the best subset of features from a candidate pool by maximizing the derived VAMP-2 score. For each candidate subset:\n1.  **Data Preparation**: Given the full data matrix $X \\in \\mathbb{R}^{T \\times D}$, a candidate subset of column indices $S$, and a lag time $\\tau$, we form a new data matrix $X_S$ containing only the selected columns. This matrix is split into two blocks: $X_0 = X_S[0:T-\\tau, :]$ and $X_\\tau = X_S[\\tau:T, :]$.\n2.  **Centering**: Each block, $X_0$ and $X_\\tau$, is independently centered by subtracting its column-wise mean. This step is crucial for obtaining correct covariance matrices.\n3.  **Covariance Estimation**: The empirical covariance matrices $C_{00}$, $C_{\\tau\\tau}$, and $C_{0\\tau}$ are computed from the centered blocks. The number of data pairs is $N=T-\\tau$.\n    $C_{00} = \\frac{1}{N} X_0^\\top X_0$\n    $C_{\\tau\\tau} = \\frac{1}{N} X_\\tau^\\top X_\\tau$\n    $C_{0\\tau} = \\frac{1}{N} X_0^\\top X_\\tau$\n4.  **Regularization**: To ensure numerical stability and invertibility, a small Tikhonov regularization term $\\epsilon I$ is added to the instantaneous covariance matrices:\n    $C_{00}^{\\text{reg}} = C_{00} + \\epsilon I$\n    $C_{\\tau\\tau}^{\\text{reg}} = C_{\\tau\\tau} + \\epsilon I$\n5.  **Whitening Matrix Calculation**: The inverse square roots, $(C_{00}^{\\text{reg}})^{-1/2}$ and $(C_{\\tau\\tau}^{\\text{reg}})^{-1/2}$, are computed. For a symmetric positive definite matrix $M = U \\Lambda U^\\top$ (from eigendecomposition), its inverse square root is $M^{-1/2} = U \\Lambda^{-1/2} U^\\top$, where $\\Lambda^{-1/2}$ is a diagonal matrix with entries $1/\\sqrt{\\lambda_i}$.\n6.  **Score Calculation**: The whitened time-lagged covariance matrix $\\tilde{C}_{0\\tau} = (C_{00}^{\\text{reg}})^{-1/2} C_{0\\tau} (C_{\\tau\\tau}^{\\text{reg}})^{-1/2}$ is formed. The VAMP-2 score $S$ is then calculated as the sum of the squares of its singular values, which is equivalent to its squared Frobenius norm.\n7.  **Subset Selection**: The algorithm iterates through all provided candidate subsets, computes the score for each, and identifies the subset (and its index in the original list) that yields the maximum score. A numerical tolerance is used for comparing scores to handle floating-point arithmetic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_vamp_score(X: np.ndarray, tau: int, subset_indices: list, epsilon: float) - float:\n    \"\"\"\n    Calculates the VAMP-2 score for a given subset of features.\n    \n    Args:\n        X (np.ndarray): Full data matrix of shape (T, D).\n        tau (int): Lag time.\n        subset_indices (list): List of column indices for the feature subset.\n        epsilon (float): Tikhonov regularization parameter.\n        \n    Returns:\n        float: The VAMP-2 score for the subset.\n    \"\"\"\n    T, _ = X.shape\n    d = len(subset_indices)\n    \n    if d == 0:\n        return 0.0\n        \n    # 1. Data Preparation: Select features and create lagged blocks\n    x_sub = X[:, subset_indices]\n    N = T - tau\n    \n    # Handle edge case where N is not sufficient for analysis\n    if N = 1:\n        return 0.0\n\n    X0 = x_sub[0:N, :]\n    Xt = x_sub[tau:T, :]\n    \n    # 2. Centering: Center each block independently\n    X0_centered = X0 - np.mean(X0, axis=0)\n    Xt_centered = Xt - np.mean(Xt, axis=0)\n    \n    # 3. Covariance Estimation\n    C00 = (X0_centered.T @ X0_centered) / N\n    Ctt = (Xt_centered.T @ Xt_centered) / N # C_{\\tau\\tau}\n    C0t = (X0_centered.T @ Xt_centered) / N # C_{0\\tau}\n    \n    # 4. Regularization\n    I = np.eye(d)\n    C00_reg = C00 + epsilon * I\n    Ctt_reg = Ctt + epsilon * I\n    \n    # 5. Whitening Matrix Calculation (Inverse Square Root)\n    def matrix_inv_sqrt(S: np.ndarray) - np.ndarray:\n        \"\"\"Computes the inverse square root of a symmetric matrix via eigendecomposition.\"\"\"\n        evals, evecs = np.linalg.eigh(S)\n        \n        # Ensure eigenvalues are positive before sqrt\n        evals[evals  0] = 0\n        \n        inv_sqrt_diag = np.diag(1.0 / np.sqrt(evals))\n        return evecs @ inv_sqrt_diag @ evecs.T\n\n    C00_inv_sqrt = matrix_inv_sqrt(C00_reg)\n    Ctt_inv_sqrt = matrix_inv_sqrt(Ctt_reg)\n    \n    # 6. Score Calculation\n    # Form the whitened time-lagged covariance matrix\n    C0t_white = C00_inv_sqrt @ C0t @ Ctt_inv_sqrt\n    \n    # The score is the sum of squared singular values of C0t_white\n    s = np.linalg.svd(C0t_white, compute_uv=False)\n    score = np.sum(s**2)\n    \n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and find the best CV subset for each case.\n    \"\"\"\n    \n    # Test Case 1\n    T1, D1, tau1, epsilon1 = 200, 5, 5, 1e-6\n    t_vals1 = np.arange(T1)\n    f1 = np.sin(0.1 * t_vals1)\n    f2 = np.cos(0.1 * t_vals1)\n    f3 = np.sin(0.05 * t_vals1 + 0.3)\n    f4 = f1 + 0.5 * f3\n    f5 = np.sin(0.2 * t_vals1)\n    X1 = np.stack([f1, f2, f3, f4, f5], axis=1)\n    S1 = [[0, 1], [3], [2, 4], [0, 2, 3], [1, 4]]\n    \n    # Test Case 2\n    T2, D2, tau2, epsilon2 = 150, 3, 4, 1e-6\n    t_vals2 = np.arange(T2)\n    g1 = np.sin(0.12 * t_vals2 + 0.1)\n    g2 = g1.copy()\n    g3 = np.cos(0.07 * t_vals2)\n    X2 = np.stack([g1, g2, g3], axis=1)\n    S2 = [[0, 1], [0, 2], [1, 2]]\n    \n    # Test Case 3\n    T3, D3, tau3, epsilon3 = 20, 2, 15, 1e-6\n    t_vals3 = np.arange(T3)\n    h1 = np.sin(0.5 * t_vals3)\n    h2 = np.cos(0.3 * t_vals3)\n    X3 = np.stack([h1, h2], axis=1)\n    S3 = [[0], [1], [0, 1]]\n\n    test_cases = [\n        (X1, tau1, S1, epsilon1),\n        (X2, tau2, S2, epsilon2),\n        (X3, tau3, S3, epsilon3),\n    ]\n\n    all_results = []\n    TOLERANCE = 1e-12\n\n    for X, tau, S_list, epsilon in test_cases:\n        best_subset_idx = -1\n        max_score = -1.0\n        \n        for i, subset in enumerate(S_list):\n            current_score = calculate_vamp_score(X, tau, subset, epsilon)\n            \n            # Update if current score is larger, respecting the tie-breaking rule\n            # \"if multiple subsets obtain numerically equal maximized scores \n            # within absolute tolerance 10^-12, choose the first occurrence\"\n            # This is handled by a strict  comparison on score.\n            # To be more robust, we check if the difference is greater than the tolerance.\n            if current_score - max_score  TOLERANCE:\n                max_score = current_score\n                best_subset_idx = i\n\n        rounded_score = round(max_score, 6)\n        all_results.append(f\"[{best_subset_idx},{rounded_score}]\")\n        \n    print(f\"[[{all_results[0][1:-1]}],[{all_results[1][1:-1]}],[{all_results[2][1:-1]}]]\")\n\nsolve()\n```"
        }
    ]
}