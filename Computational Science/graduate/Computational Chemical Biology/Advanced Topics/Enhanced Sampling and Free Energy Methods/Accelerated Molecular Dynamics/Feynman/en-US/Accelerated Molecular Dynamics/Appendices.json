{
    "hands_on_practices": [
        {
            "introduction": "Accelerated Molecular Dynamics enhances sampling by modifying the potential energy surface, which directly alters the forces guiding the system's dynamics. This exercise provides a direct, quantitative look at this effect by asking you to calculate the force scaling factor, $s(V)$. By completing this practice, you will gain a concrete understanding of how aMD modulates forces differently across the potential energy landscape to accelerate transitions. ",
            "id": "3834984",
            "problem": "In accelerated molecular dynamics (aMD), the physical force in a molecular simulation is modified by introducing a boost potential to accelerate barrier crossing while preserving reweightability. Consider a system with original potential energy function $V(\\mathbf{r})$ and corresponding force $-\\nabla V(\\mathbf{r})$ by Newton’s second law. In aMD, the modified potential energy is defined as $V^{\\ast}(\\mathbf{r}) = V(\\mathbf{r}) + \\Delta V(V(\\mathbf{r}))$, where the boost potential $\\Delta V$ is applied only when the instantaneous potential energy is below a threshold energy $E$ and is given by the well-established aMD form\n$$\n\\Delta V(V) =\n\\begin{cases}\n\\dfrac{(E - V)^{2}}{\\alpha + (E - V)},  V  E, \\\\\n0,  V \\ge E,\n\\end{cases}\n$$\nwith $\\alpha  0$ the tuning parameter that sets the strength and smoothness of the boost. Because $\\Delta V$ is a function of $V$ alone, the modified force can be written using the chain rule as $-\\nabla V^{\\ast}(\\mathbf{r}) = -\\left(1 + \\dfrac{d\\Delta V}{dV}\\right)\\nabla V(\\mathbf{r})$. The multiplicative factor $s(V) \\equiv 1 + \\dfrac{d\\Delta V}{dV}$ quantifies the local modulation of the force magnitude relative to the original dynamics.\n\nUsing the above, compute the force scaling factors $s_i = s(V_i)$ for the following parameters and potential energy values, all of which satisfy $V_i  E$:\n- Threshold energy $E = 120$ kJ mol$^{-1}$,\n- Acceleration parameter $\\alpha = 30$ kJ mol$^{-1}$,\n- Potential energies $\\{V_i\\} = \\{110,\\ 100,\\ 80,\\ 40,\\ 0\\}$ kJ mol$^{-1}$.\n\nReport the scaling factors in the same order as the given $\\{V_i\\}$, as a single row vector. Express the final scaling factors as dimensionless decimal numbers rounded to four significant figures. Do not include units in your final answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and contains all necessary information.\n\n**1. Extraction of Givens:**\n- Modified potential energy in accelerated Molecular Dynamics (aMD): $V^{\\ast}(\\mathbf{r}) = V(\\mathbf{r}) + \\Delta V(V(\\mathbf{r}))$.\n- Boost potential for $V  E$: $\\Delta V(V) = \\dfrac{(E - V)^{2}}{\\alpha + (E - V)}$.\n- Boost potential for $V \\ge E$: $\\Delta V(V) = 0$.\n- Modified force expression: $-\\nabla V^{\\ast}(\\mathbf{r}) = -s(V)\\nabla V(\\mathbf{r})$, where $s(V)$ is the force scaling factor.\n- Definition of the force scaling factor: $s(V) \\equiv 1 + \\dfrac{d\\Delta V}{dV}$.\n- Threshold energy: $E = 120$ kJ mol$^{-1}$.\n- Acceleration parameter: $\\alpha = 30$ kJ mol$^{-1}$.\n- A set of potential energy values: $\\{V_i\\} = \\{110,\\ 100,\\ 80,\\ 40,\\ 0\\}$ kJ mol$^{-1}$.\n- It is specified that all given $V_i$ satisfy the condition $V_i  E$.\n\n**2. Validation:**\nThe problem is scientifically sound, as it uses the standard formulation for dual-boost aMD, a widely used enhanced sampling technique in computational chemistry. All definitions are standard, and the parameters provided ($E$, $\\alpha$, $V_i$) are physically realistic and dimensionally consistent. The problem is well-posed, complete, and objective, providing all necessary information for a unique solution. Therefore, the problem is deemed valid.\n\n**3. Solution Derivation:**\nThe task is to compute the force scaling factors $s_i = s(V_i)$ for the given potential energy values. The scaling factor is defined as $s(V) = 1 + \\dfrac{d\\Delta V}{dV}$.\n\nFirst, we must find the derivative of the boost potential $\\Delta V(V)$ with respect to $V$ for the case $V  E$. The boost potential is given by:\n$$\n\\Delta V(V) = \\dfrac{(E - V)^{2}}{\\alpha + (E - V)}\n$$\nTo find the derivative $\\dfrac{d\\Delta V}{dV}$, we apply the quotient rule for differentiation, which states that for $f(V)/g(V)$, the derivative is $\\dfrac{f'(V)g(V) - f(V)g'(V)}{[g(V)]^2}$.\nLet $f(V) = (E - V)^2$ and $g(V) = \\alpha + (E - V)$. Their derivatives with respect to $V$ are:\n$$\nf'(V) = \\frac{d}{dV}(E - V)^2 = 2(E - V) \\cdot (-1) = -2(E - V)\n$$\n$$\ng'(V) = \\frac{d}{dV}(\\alpha + E - V) = -1\n$$\nApplying the quotient rule:\n$$\n\\frac{d\\Delta V}{dV} = \\frac{[-2(E - V)][\\alpha + (E - V)] - [(E - V)^2][-1]}{[\\alpha + (E - V)]^2}\n$$\n$$\n\\frac{d\\Delta V}{dV} = \\frac{-2\\alpha(E - V) - 2(E - V)^2 + (E - V)^2}{[\\alpha + (E - V)]^2}\n$$\n$$\n\\frac{d\\Delta V}{dV} = \\frac{-2\\alpha(E - V) - (E - V)^2}{[\\alpha + (E - V)]^2}\n$$\nNow, substitute this derivative into the expression for the scaling factor $s(V)$:\n$$\ns(V) = 1 + \\frac{d\\Delta V}{dV} = 1 - \\frac{2\\alpha(E - V) + (E - V)^2}{[\\alpha + (E - V)]^2}\n$$\nTo simplify, we find a common denominator:\n$$\ns(V) = \\frac{[\\alpha + (E - V)]^2}{[\\alpha + (E - V)]^2} - \\frac{2\\alpha(E - V) + (E - V)^2}{[\\alpha + (E - V)]^2}\n$$\n$$\ns(V) = \\frac{\\alpha^2 + 2\\alpha(E - V) + (E - V)^2 - [2\\alpha(E - V) + (E - V)^2]}{[\\alpha + (E - V)]^2}\n$$\nThe terms $2\\alpha(E - V)$ and $(E - V)^2$ in the numerator cancel out:\n$$\ns(V) = \\frac{\\alpha^2}{[\\alpha + (E - V)]^2}\n$$\nThis can be written more compactly as:\n$$\ns(V) = \\left(\\frac{\\alpha}{\\alpha + E - V}\\right)^2\n$$\nThis simplified expression is used for the numerical calculations.\n\n**4. Numerical Calculation:**\nWe are given $E = 120$ kJ mol$^{-1}$ and $\\alpha = 30$ kJ mol$^{-1}$. We calculate $s_i = s(V_i)$ for each $V_i$ in the set $\\{110, 100, 80, 40, 0\\}$. The units (kJ mol$^{-1}$) cancel out, making $s(V)$ a dimensionless quantity as expected. The results are rounded to four significant figures.\n\nFor $V_1 = 110$:\n$$\ns_1 = \\left(\\frac{30}{30 + 120 - 110}\\right)^2 = \\left(\\frac{30}{40}\\right)^2 = \\left(\\frac{3}{4}\\right)^2 = 0.5625\n$$\n\nFor $V_2 = 100$:\n$$\ns_2 = \\left(\\frac{30}{30 + 120 - 100}\\right)^2 = \\left(\\frac{30}{50}\\right)^2 = \\left(\\frac{3}{5}\\right)^2 = 0.36\n$$\nTo four significant figures, this is $0.3600$.\n\nFor $V_3 = 80$:\n$$\ns_3 = \\left(\\frac{30}{30 + 120 - 80}\\right)^2 = \\left(\\frac{30}{70}\\right)^2 = \\left(\\frac{3}{7}\\right)^2 = \\frac{9}{49} \\approx 0.183673...\n$$\nRounded to four significant figures, this is $0.1837$.\n\nFor $V_4 = 40$:\n$$\ns_4 = \\left(\\frac{30}{30 + 120 - 40}\\right)^2 = \\left(\\frac{30}{110}\\right)^2 = \\left(\\frac{3}{11}\\right)^2 = \\frac{9}{121} \\approx 0.074380...\n$$\nRounded to four significant figures, this is $0.07438$.\n\nFor $V_5 = 0$:\n$$\ns_5 = \\left(\\frac{30}{30 + 120 - 0}\\right)^2 = \\left(\\frac{30}{150}\\right)^2 = \\left(\\frac{1}{5}\\right)^2 = 0.04\n$$\nTo four significant figures, this is $0.04000$.\n\nThe resulting set of scaling factors $\\{s_i\\}$ is $\\{0.5625, 0.3600, 0.1837, 0.07438, 0.04000\\}$. These are reported as a single row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5625  0.3600  0.1837  0.07438  0.04000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key challenge in any enhanced sampling method is recovering the correct thermodynamic averages from a biased simulation. In aMD, this is done by reweighting, a process whose theoretical foundation is rooted in statistical mechanics. This exercise guides you through the cumulant expansion, a powerful technique to approximate the free energy correction factor from the statistics of the applied boost potential, highlighting the connection between the simulation's dynamics and its thermodynamic reinterpretation. ",
            "id": "3834957",
            "problem": "In accelerated molecular dynamics (aMD), reweighting from the modified ensemble back to the canonical ensemble introduces the factor $\\langle \\exp(\\beta \\Delta V) \\rangle$, where $\\Delta V$ is the boost potential added to the physical potential and $\\beta$ is the inverse thermal energy. Consider an aMD simulation of a biomolecular system where energies are reported per mole. In this convention, the appropriate inverse thermal energy is $\\beta = 1/(R T)$, where $R$ is the molar gas constant and $T$ is the absolute temperature. Let $\\Delta V$ be a scalar random variable representing the boost potential sampled from the stationary aMD ensemble.\n\n(a) Starting from the definition of the moment generating function $M_{X}(t) = \\langle \\exp(t X) \\rangle$ and the cumulant generating function $K_{X}(t) = \\ln M_{X}(t)$, with cumulants $\\kappa_{n}$ defined by $\\kappa_{n} = K_{X}^{(n)}(0)$, derive a power series representation for $\\ln \\langle \\exp(\\beta \\Delta V) \\rangle$ in powers of $\\beta$ in terms of the cumulants of $\\Delta V$. Your derivation should begin from these definitions and the Taylor series of a smooth function around $t=0$, and proceed to an explicit series whose coefficients are the cumulants of $\\Delta V$.\n\n(b) In many practical aMD applications, the distribution of $\\Delta V$ is empirically close to Gaussian. Using only the defining properties of a Gaussian random variable, justify the truncation of the series at order $m=2$, and state precisely what terms are neglected and why they vanish or are expected to be small under approximate Gaussianity.\n\n(c) A particular aMD trajectory at temperature $T = 300 \\ \\mathrm{K}$ produces a boost potential $\\Delta V$ with sample mean $\\mu_{\\Delta V} = 3.0 \\ \\mathrm{kJ} \\ \\mathrm{mol}^{-1}$ and sample variance $\\sigma_{\\Delta V}^{2} = 9.0 \\ \\mathrm{(kJ \\ mol^{-1})^{2}}$. Take the molar gas constant as $R = 8.314462618 \\times 10^{-3} \\ \\mathrm{kJ} \\ \\mathrm{mol}^{-1} \\ \\mathrm{K}^{-1}$. Using your result from part (a) truncated at $m=2$ as justified in part (b), compute the numerical value of $\\ln \\langle \\exp(\\beta \\Delta V) \\rangle$ to second order in $\\beta$. Express your final answer as a dimensionless number and round to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in statistical mechanics and computational chemistry, well-posed with all necessary information provided, and objective in its formulation.\n\n(a) Derivation of the power series representation for $\\ln \\langle \\exp(\\beta \\Delta V) \\rangle$.\n\nWe are asked to derive a power series representation for $\\ln \\langle \\exp(\\beta \\Delta V) \\rangle$ in terms of the cumulants of the random variable $\\Delta V$. The starting points are the definitions of the moment generating function (MGF), the cumulant generating function (CGF), and the cumulants.\n\nLet $X$ be a random variable. The MGF is defined as $M_{X}(t) = \\langle \\exp(t X) \\rangle$, and the CGF is defined as $K_{X}(t) = \\ln M_{X}(t)$. The cumulants, $\\kappa_n$, are defined as the derivatives of the CGF evaluated at $t=0$:\n$$\n\\kappa_n = \\frac{d^n K_X(t)}{dt^n} \\bigg|_{t=0} = K_{X}^{(n)}(0)\n$$\nWe are interested in the quantity $\\ln \\langle \\exp(\\beta \\Delta V) \\rangle$. Comparing this with the definitions, we can identify the random variable as $X = \\Delta V$ and the parameter as $t = \\beta$. Therefore, the quantity we seek is precisely the CGF of $\\Delta V$ evaluated at $\\beta$:\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle = K_{\\Delta V}(\\beta)\n$$\nTo find a power series representation, we can express $K_{\\Delta V}(t)$ as a Maclaurin series (a Taylor series expansion around $t=0$). For a smooth function $f(t)$, the series is given by:\n$$\nf(t) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!} t^n\n$$\nApplying this to the CGF, $K_{\\Delta V}(t)$, we have:\n$$\nK_{\\Delta V}(t) = \\sum_{n=0}^{\\infty} \\frac{K_{\\Delta V}^{(n)}(0)}{n!} t^n\n$$\nBy the definition of the cumulants, $\\kappa_n = K_{\\Delta V}^{(n)}(0)$, we can substitute this into the series:\n$$\nK_{\\Delta V}(t) = \\sum_{n=0}^{\\infty} \\frac{\\kappa_n}{n!} t^n\n$$\nLet's examine the first term of the series, for $n=0$:\n$$\n\\kappa_0 = K_{\\Delta V}^{(0)}(0) = K_{\\Delta V}(0) = \\ln M_{\\Delta V}(0) = \\ln \\langle \\exp(0 \\cdot \\Delta V) \\rangle = \\ln \\langle 1 \\rangle = \\ln(1) = 0\n$$\nSince $\\kappa_0 = 0$, the summation can start from $n=1$:\n$$\nK_{\\Delta V}(t) = \\sum_{n=1}^{\\infty} \\frac{\\kappa_n}{n!} t^n = \\frac{\\kappa_1}{1!} t + \\frac{\\kappa_2}{2!} t^2 + \\frac{\\kappa_3}{3!} t^3 + \\dots\n$$\nFinally, to obtain the desired expression, we substitute $t=\\beta$:\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle = K_{\\Delta V}(\\beta) = \\sum_{n=1}^{\\infty} \\frac{\\kappa_n}{n!} \\beta^n\n$$\nThis is the power series representation for $\\ln \\langle \\exp(\\beta \\Delta V) \\rangle$ in powers of $\\beta$, with coefficients given in terms of the cumulants $\\kappa_n$ of the boost potential $\\Delta V$.\n\n(b) Justification for truncating the series at order $m=2$ for a nearly Gaussian distribution.\n\nThe series derived in part (a) is exact and infinite for a general distribution. We are asked to justify truncating it after the second-order term ($n=2$) under the approximation that the distribution of $\\Delta V$ is Gaussian.\n\nA Gaussian (or normal) random variable $X$ with mean $\\mu$ and variance $\\sigma^2$ has a moment generating function given by:\n$$\nM_X(t) = \\exp\\left(\\mu t + \\frac{1}{2} \\sigma^2 t^2\\right)\n$$\nThe corresponding cumulant generating function is:\n$$\nK_X(t) = \\ln(M_X(t)) = \\ln\\left(\\exp\\left(\\mu t + \\frac{1}{2} \\sigma^2 t^2\\right)\\right) = \\mu t + \\frac{1}{2} \\sigma^2 t^2\n$$\nThe cumulants $\\kappa_n$ are found by differentiating $K_X(t)$ with respect to $t$ and evaluating at $t=0$.\nThe first cumulant is:\n$$\n\\kappa_1 = K_X'(0) = \\left. \\frac{d}{dt}\\left(\\mu t + \\frac{1}{2} \\sigma^2 t^2\\right) \\right|_{t=0} = \\left. (\\mu + \\sigma^2 t) \\right|_{t=0} = \\mu\n$$\nThe second cumulant is:\n$$\n\\kappa_2 = K_X''(0) = \\left. \\frac{d^2}{dt^2}\\left(\\mu t + \\frac{1}{2} \\sigma^2 t^2\\right) \\right|_{t=0} = \\left. \\sigma^2 \\right|_{t=0} = \\sigma^2\n$$\nThe third and all higher-order derivatives are:\n$$\nK_X^{(n)}(t) = 0 \\quad \\text{for } n \\ge 3\n$$\nTherefore, for $n \\ge 3$, the higher-order cumulants are identically zero:\n$$\n\\kappa_n = K_X^{(n)}(0) = 0 \\quad \\text{for } n \\ge 3\n$$\nThis is a defining property of the Gaussian distribution: all cumulants of order greater than $2$ are zero.\n\nIf the distribution of $\\Delta V$ were perfectly Gaussian, the cumulant expansion from part (a) would terminate exactly:\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle = \\frac{\\kappa_1}{1!} \\beta + \\frac{\\kappa_2}{2!} \\beta^2 + \\sum_{n=3}^{\\infty} \\frac{0}{n!} \\beta^n = \\kappa_1 \\beta + \\frac{1}{2} \\kappa_2 \\beta^2\n$$\nThe problem states that the distribution of $\\Delta V$ is *empirically close to Gaussian*. This implies that the higher-order cumulants, $\\kappa_n$ for $n \\ge 3$, are not exactly zero but are expected to be negligibly small compared to $\\kappa_1$ and $\\kappa_2$. Thus, truncating the series at order $m=2$ is a well-justified approximation. The neglected terms are all terms in the series for $n \\ge 3$:\n$$\n\\text{Neglected terms} = \\sum_{n=3}^{\\infty} \\frac{\\kappa_n}{n!} \\beta^n = \\frac{\\kappa_3}{6} \\beta^3 + \\frac{\\kappa_4}{24} \\beta^4 + \\dots\n$$\nThese terms are neglected because the near-Gaussian nature of $\\Delta V$ implies $\\kappa_n \\approx 0$ for $n \\ge 3$.\n\n(c) Numerical calculation of the second-order approximation.\n\nWe use the truncated series from part (b) with the given sample statistics as estimators for the first two cumulants:\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx \\kappa_1 \\beta + \\frac{1}{2} \\kappa_2 \\beta^2\n$$\nThe given values are:\n$\\kappa_1 = \\mu_{\\Delta V} = 3.0 \\ \\mathrm{kJ} \\ \\mathrm{mol}^{-1}$\n$\\kappa_2 = \\sigma_{\\Delta V}^{2} = 9.0 \\ \\mathrm{(kJ \\ mol^{-1})^{2}}$\n$T = 300 \\ \\mathrm{K}$\n$R = 8.314462618 \\times 10^{-3} \\ \\mathrm{kJ} \\ \\mathrm{mol}^{-1} \\ \\mathrm{K}^{-1}$\n\nFirst, we calculate the inverse thermal energy $\\beta = 1/(RT)$:\n$$\n\\beta = \\frac{1}{R T} = \\frac{1}{(8.314462618 \\times 10^{-3} \\ \\mathrm{kJ} \\ \\mathrm{mol}^{-1} \\ \\mathrm{K}^{-1}) \\times (300 \\ \\mathrm{K})}\n$$\n$$\n\\beta = \\frac{1}{2.4943387854 \\ \\mathrm{kJ} \\ \\mathrm{mol}^{-1}} \\approx 0.400907834 \\ (\\mathrm{kJ} \\ \\mathrm{mol}^{-1})^{-1}\n$$\nNow, we substitute the values of $\\kappa_1$, $\\kappa_2$, and $\\beta$ into the second-order approximation:\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx (3.0 \\ \\mathrm{kJ} \\ \\mathrm{mol}^{-1}) \\beta + \\frac{1}{2} (9.0 \\ \\mathrm{(kJ \\ mol^{-1})^{2}}) \\beta^2\n$$\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx 3.0 \\beta + 4.5 \\beta^2\n$$\nPlugging in the value for $\\beta$:\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx (3.0)(0.400907834) + (4.5)(0.400907834)^2\n$$\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx 1.202723502 + (4.5)(0.160727092)\n$$\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx 1.202723502 + 0.723271914\n$$\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx 1.925995416\n$$\nThe problem asks for the answer to be rounded to four significant figures. The result is a dimensionless number as required.\n$$\n\\ln \\langle \\exp(\\beta \\Delta V) \\rangle \\approx 1.926\n$$",
            "answer": "$$\n\\boxed{1.926}\n$$"
        },
        {
            "introduction": "Theory comes to life when applied to data, and this practice simulates the final, crucial stage of an aMD analysis pipeline. You will take raw simulation output—boost potentials and an observable's values—and compute the correct, reweighted ensemble average. More importantly, this exercise introduces a robust bootstrap method to estimate the statistical uncertainty of your result, a non-negotiable step for producing reliable scientific conclusions from enhanced sampling simulations. ",
            "id": "3834993",
            "problem": "You are studying Accelerated Molecular Dynamics (aMD), which modifies the potential energy surface to enhance sampling of rare events. The sampling distribution in aMD differs from the true canonical distribution, so reweighting is required to recover unbiased ensemble averages. Starting from the canonical ensemble in Molecular Dynamics (MD), where the probability density is proportional to the Boltzmann factor of the true potential energy, derive from first principles why reweighting is necessary when sampling under a modified potential in aMD, and how the importance weights arise. Then, given sampled boost potential shifts and observable values, implement an algorithm to compute the reweighted average and estimate its uncertainty using a bootstrap that resamples weights to address heavy-tailed importance weights.\n\nUse the following context and requirements:\n\n- Begin from the canonical ensemble of equilibrium statistical mechanics. The true potential energy is denoted by $U(\\mathbf{r})$, the modified potential under accelerated molecular dynamics is $U^{\\ast}(\\mathbf{r})$, and the boost potential is $\\Delta V(\\mathbf{r})$ such that $U^{\\ast}(\\mathbf{r}) = U(\\mathbf{r}) + \\Delta V(\\mathbf{r})$.\n- The canonical ensemble at absolute temperature $T$ has probability density proportional to $\\exp\\left(-\\beta U(\\mathbf{r})\\right)$, where the inverse thermal energy is $\\beta = 1/(k_{\\mathrm{B}} T)$ and $k_{\\mathrm{B}}$ is the Boltzmann constant.\n- From the above canonical ensemble facts and the definition of the modified potential, derive the importance sampling weights needed to reweight expectation values computed from the modified distribution back to the true distribution.\n\nProgram requirements:\n\n- Input parameters are hardcoded in your program. There is no user input.\n- Physical units: use $\\Delta V$ in kilocalories per mole (kcal/mol), temperature in kelvin (K), and set $k_{\\mathrm{B}} = 0.0019872041$ in units of kilocalories per mole per kelvin (kcal/mol/K). Compute $\\beta = 1/(k_{\\mathrm{B}} T)$ in reciprocal kilocalories per mole.\n- For each test case, you are provided:\n  - A temperature $T$ in kelvin.\n  - A list of sampled boost potential values $\\{\\Delta V_i\\}$ in kilocalories per mole.\n  - A list of observable values $\\{A_i\\}$ (dimensionless).\n- Compute the reweighted average of the observable, denoted $\\hat{A}$, using the weights derived from first principles. Implement numerically stable computations to handle heavy-tailed weights. Then estimate the uncertainty as the bootstrap standard deviation by resampling the weights multiplicatively with independent positive random factors to capture the variability caused by heavy-tailed importance weights. Use $5000$ bootstrap replicates and a fixed random seed for determinism.\n- Your algorithm must handle boundary and edge cases robustly, including the case with all $\\Delta V_i$ equal and the case with a single sample.\n- The final outputs for each test case are two real numbers: the reweighted mean $\\hat{A}$ and the bootstrap uncertainty $\\sigma_{\\hat{A}}$. Express both as decimal floats.\n- Test suite:\n  1. $T = 300$, $\\Delta V = [-0.4, 0.1, 0.0, 1.2, -0.8, 0.5, 0.3]$, $A = [1.25, 0.72, 0.95, 1.85, 1.05, 1.42, 0.88]$.\n  2. $T = 310$, $\\Delta V = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$, $A = [1.10, 0.90, 1.00, 1.20, 0.80, 1.05]$.\n  3. $T = 300$, $\\Delta V = [0.0, 8.0, -2.0, 6.0, 0.5, -0.3, 4.0]$, $A = [0.90, 2.50, 0.70, 1.80, 1.20, 1.00, 2.00]$.\n  4. $T = 298$, $\\Delta V = [0.7]$, $A = [1.33]$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be flattened in the order $[\\hat{A}_1, \\sigma_{\\hat{A},1}, \\hat{A}_2, \\sigma_{\\hat{A},2}, \\hat{A}_3, \\sigma_{\\hat{A},3}, \\hat{A}_4, \\sigma_{\\hat{A},4}]$, where the subscript indicates the test case index. Round each float to $6$ decimal places.\n\nYour implementation must be a complete, runnable program that computes the required outputs for the provided test suite and prints them in the exact specified format.",
            "solution": "The problem requires a derivation from first principles of the reweighting scheme used in accelerated molecular dynamics (aMD) and an implementation of an algorithm to compute reweighted observables and their uncertainties.\n\n### Principles and Derivation\n\nThe foundation of this problem lies in equilibrium statistical mechanics and the theory of importance sampling. In a classical system at thermal equilibrium with a heat bath at constant volume $V$ and temperature $T$, the probability of finding the system in a particular microstate with configuration $\\mathbf{r}$ and potential energy $U(\\mathbf{r})$ is given by the Boltzmann distribution from the canonical ensemble.\n\nThe probability density function $p_U(\\mathbf{r})$ is proportional to the Boltzmann factor:\n$$ p_U(\\mathbf{r}) = \\frac{e^{-\\beta U(\\mathbf{r})}}{Z_U} $$\nwhere $\\beta = 1/(k_{\\mathrm{B}} T)$ is the inverse thermal energy, $k_{\\mathrm{B}}$ is the Boltzmann constant, and $Z_U$ is the canonical partition function, which acts as a normalization constant:\n$$ Z_U = \\int e^{-\\beta U(\\mathbf{r})} \\, d\\mathbf{r} $$\nThe integral is over all possible configurations $\\mathbf{r}$.\n\nThe ensemble average of a physical observable $A(\\mathbf{r})$ is its expectation value over this probability distribution:\n$$ \\langle A \\rangle_U = \\int A(\\mathbf{r}) p_U(\\mathbf{r}) \\, d\\mathbf{r} = \\frac{\\int A(\\mathbf{r}) e^{-\\beta U(\\mathbf{r})} \\, d\\mathbf{r}}{\\int e^{-\\beta U(\\mathbf{r})} \\, d\\mathbf{r}} $$\n\nIn accelerated molecular dynamics (aMD), the potential energy surface $U(\\mathbf{r})$ is modified by adding a non-negative boost potential $\\Delta V(\\mathbf{r})$ when the original potential is below a certain threshold, and zero otherwise. For generality, we express the modified potential as $U^{\\ast}(\\mathbf{r}) = U(\\mathbf{r}) + \\Delta V(\\mathbf{r})$. A simulation run under this modified potential samples configurations from a different probability distribution, $p_{U^{\\ast}}(\\mathbf{r})$:\n$$ p_{U^{\\ast}}(\\mathbf{r}) = \\frac{e^{-\\beta U^{\\ast}(\\mathbf{r})}}{Z_{U^{\\ast}}} $$\nwhere $Z_{U^{\\ast}} = \\int e^{-\\beta U^{\\ast}(\\mathbf{r})} \\, d\\mathbf{r}$.\n\nA direct average of an observable $A$ over an aMD trajectory yields an incorrect, biased estimate $\\langle A \\rangle_{U^{\\ast}}$. To recover the correct canonical average $\\langle A \\rangle_U$, we must reweight the samples. This is an application of importance sampling.\n\nWe can express $\\langle A \\rangle_U$ by rewriting the integrals in terms of the modified ensemble. We multiply and divide the integrands in the expression for $\\langle A \\rangle_U$ by $e^{-\\beta U^{\\ast}(\\mathbf{r})}$:\n$$ \\langle A \\rangle_U = \\frac{\\int A(\\mathbf{r}) \\frac{e^{-\\beta U(\\mathbf{r})}}{e^{-\\beta U^{\\ast}(\\mathbf{r})}} e^{-\\beta U^{\\ast}(\\mathbf{r})} \\, d\\mathbf{r}}{\\int \\frac{e^{-\\beta U(\\mathbf{r})}}{e^{-\\beta U^{\\ast}(\\mathbf{r})}} e^{-\\beta U^{\\ast}(\\mathbf{r})} \\, d\\mathbf{r}} $$\nThe ratio inside the integrals serves as the reweighting factor, or importance weight. Let's define this weight $w(\\mathbf{r})$:\n$$ w(\\mathbf{r}) = \\frac{e^{-\\beta U(\\mathbf{r})}}{e^{-\\beta U^{\\ast}(\\mathbf{r})}} = \\frac{e^{-\\beta U(\\mathbf{r})}}{e^{-\\beta (U(\\mathbf{r}) + \\Delta V(\\mathbf{r}))}} = \\frac{e^{-\\beta U(\\mathbf{r})}}{e^{-\\beta U(\\mathbf{r})} e^{-\\beta \\Delta V(\\mathbf{r})}} = e^{\\beta \\Delta V(\\mathbf{r})} $$\nThe weight for each configuration is the exponential of the boost potential applied at that configuration, scaled by $\\beta$.\n\nSubstituting $w(\\mathbf{r})$ back into the expression for $\\langle A \\rangle_U$:\n$$ \\langle A \\rangle_U = \\frac{\\int A(\\mathbf{r}) w(\\mathbf{r}) e^{-\\beta U^{\\ast}(\\mathbf{r})} \\, d\\mathbf{r}}{\\int w(\\mathbf{r}) e^{-\\beta U^{\\ast}(\\mathbf{r})} \\, d\\mathbf{r}} $$\nBy dividing the numerator and denominator by the modified partition function $Z_{U^{\\ast}}$, we can express this in terms of expectation values computed in the modified ($U^{\\ast}$) ensemble:\n$$ \\langle A \\rangle_U = \\frac{\\langle A \\cdot w \\rangle_{U^{\\ast}}}{\\langle w \\rangle_{U^{\\ast}}} $$\nThis is the fundamental reweighting equation.\n\nIn a simulation, we generate a discrete trajectory of $N$ snapshots, $\\{\\mathbf{r}_1, \\mathbf{r}_2, \\dots, \\mathbf{r}_N\\}$, sampled from $p_{U^{\\ast}}$. The ensemble averages are estimated by arithmetic means over these snapshots. The estimator for the true canonical average, denoted $\\hat{A}$, is:\n$$ \\hat{A} = \\frac{\\frac{1}{N} \\sum_{i=1}^{N} A(\\mathbf{r}_i) w(\\mathbf{r}_i)}{\\frac{1}{N} \\sum_{i=1}^{N} w(\\mathbf{r}_i)} = \\frac{\\sum_{i=1}^{N} A_i w_i}{\\sum_{i=1}^{N} w_i} $$\nwhere $A_i = A(\\mathbf{r}_i)$ and $w_i = e^{\\beta \\Delta V_i}$ with $\\Delta V_i = \\Delta V(\\mathbf{r}_i)$.\n\n### Algorithmic Design\n\n**1. Reweighted Average Calculation:**\nGiven a set of sampled boost potentials $\\{\\Delta V_i\\}$ and corresponding observables $\\{A_i\\}$, the calculation proceeds as follows.\nFirst, compute $\\beta = 1/(k_{\\mathrm{B}} T)$.\nThen, compute the weights $w_i = e^{\\beta \\Delta V_i}$. The values of $\\beta \\Delta V_i$ can be large, leading to numerical overflow. To ensure stability, we use a standard 'log-sum-exp' trick. Let $c = \\max_i(\\beta \\Delta V_i)$. We compute scaled weights $w'_i = e^{\\beta \\Delta V_i - c}$. The reweighted average is then:\n$$ \\hat{A} = \\frac{\\sum_{i=1}^{N} A_i e^{\\beta \\Delta V_i}}{\\sum_{i=1}^{N} e^{\\beta \\Delta V_i}} = \\frac{\\sum_{i=1}^{N} A_i e^{\\beta \\Delta V_i - c}}{\\sum_{i=1}^{N} e^{\\beta \\Delta V_i - c}} = \\frac{\\sum_{i=1}^{N} A_i w'_i}{\\sum_{i=1}^{N} w'_i} $$\nThis formulation is numerically robust as the maximum exponent is now $0$.\n\n**2. Uncertainty Estimation:**\nThe weights $w_i$ often have a heavy-tailed distribution, meaning a few snapshots can dominate the sum. Standard error formulas are unreliable. The problem specifies a bootstrap procedure suitable for such cases. It involves multiplicatively resampling the weights with independent positive random factors. This is a variant of the Bayesian bootstrap, where the new weights are generated by multiplying the original weights by random numbers drawn from an exponential distribution with a mean of $1$.\n\nThe algorithm is as follows:\n1. Set a fixed random seed for reproducibility.\n2. For a fixed number of bootstrap replicates, $B = 5000$:\n   a. For each of the $N$ original samples, draw a random number $g_i$ from an exponential distribution, $g_i \\sim \\text{Exponential}(1)$.\n   b. Create a new set of bootstrapped weights for replicate $b$: $w''_{i,b} = w'_i \\cdot g_i$.\n   c. Calculate the bootstrap estimate of the average: $\\hat{A}_b = \\frac{\\sum_{i=1}^{N} A_i w''_{i,b}}{\\sum_{i=1}^{N} w''_{i,b}}$.\n3. The collection of $\\{\\hat{A}_1, \\hat{A}_2, \\dots, \\hat{A}_B\\}$ forms the bootstrap distribution of the mean.\n4. The uncertainty $\\sigma_{\\hat{A}}$ is estimated as the sample standard deviation of this bootstrap distribution:\n$$ \\sigma_{\\hat{A}} = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{A}_b - \\bar{A}_{\\text{boot}})^2} $$\nwhere $\\bar{A}_{\\text{boot}}$ is the mean of the bootstrap estimates.\n\n**Handling Edge Cases:**\n- If all $\\Delta V_i$ are identical, all weights $w_i$ are equal. The formula for $\\hat{A}$ correctly reduces to the simple arithmetic mean of $\\{A_i\\}$.\n- If only one sample ($N=1$) is given, the reweighted average is simply its own value, $\\hat{A} = A_1$. The bootstrap procedure will yield $\\hat{A}_b = A_1$ for all replicates, resulting in an uncertainty of $\\sigma_{\\hat{A}} = 0$, which is a logical outcome for a single-sample estimate.\n\nThis principled approach provides both a robust estimate of the canonical average and a reliable measure of its statistical uncertainty.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_reweighted_stats(T, dV, A, k_B, n_bootstrap, rng):\n    \"\"\"\n    Computes the reweighted average and its uncertainty for aMD data.\n\n    Args:\n        T (float): Temperature in Kelvin.\n        dV (list or np.ndarray): List of sampled boost potential values in kcal/mol.\n        A (list or np.ndarray): List of corresponding observable values.\n        k_B (float): Boltzmann constant in kcal/mol/K.\n        n_bootstrap (int): Number of bootstrap replicates.\n        rng (np.random.Generator): NumPy random number generator for reproducibility.\n\n    Returns:\n        tuple: A tuple containing the reweighted mean and its bootstrap uncertainty.\n    \"\"\"\n    dV = np.asarray(dV, dtype=np.float64)\n    A = np.asarray(A, dtype=np.float64)\n    N = len(A)\n\n    # Handle edge case: no samples\n    if N == 0:\n        return np.nan, np.nan\n\n    # Handle edge case: single sample\n    if N == 1:\n        return A[0], 0.0\n\n    # Calculate inverse thermal energy\n    beta = 1.0 / (k_B * T)\n\n    # Calculate log-weights for numerical stability\n    beta_dV = beta * dV\n    # The max subtraction ensures the largest weight is exp(0)=1, preventing overflow.\n    log_weights = beta_dV - np.max(beta_dV)\n    weights = np.exp(log_weights)\n\n    # Calculate the reweighted average\n    sum_weights = np.sum(weights)\n    if sum_weights > 0:\n        reweighted_mean = np.sum(A * weights) / sum_weights\n    else:\n        # This case is highly unlikely with the stable weight calculation unless N=0\n        reweighted_mean = np.mean(A)\n\n    # Estimate uncertainty using a multiplicative (Bayesian-like) bootstrap\n    bootstrap_means = np.empty(n_bootstrap)\n    for i in range(n_bootstrap):\n        # Generate positive random factors from an Exponential(1) distribution\n        g = rng.exponential(scale=1.0, size=N)\n        \n        # Create bootstrap weights by re-scaling original weights\n        bootstrap_weights = weights * g\n        \n        sum_bootstrap_weights = np.sum(bootstrap_weights)\n        if sum_bootstrap_weights > 0:\n            bootstrap_means[i] = np.sum(A * bootstrap_weights) / sum_bootstrap_weights\n        else:\n            # This case is also extremely unlikely\n            bootstrap_means[i] = np.nan\n\n    # Compute bootstrap standard deviation (ddof=1 for sample standard deviation)\n    uncertainty = np.nanstd(bootstrap_means, ddof=1)\n    \n    return reweighted_mean, uncertainty\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the final results.\n    \"\"\"\n    # Define physical constants and algorithm parameters\n    k_B = 0.0019872041  # Boltzmann constant in kcal/mol/K\n    N_BOOTSTRAP = 5000\n    SEED = 42  # Fixed seed for deterministic results\n\n    # Initialize a random number generator\n    rng = np.random.default_rng(SEED)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'T': 300, 'dV': [-0.4, 0.1, 0.0, 1.2, -0.8, 0.5, 0.3], 'A': [1.25, 0.72, 0.95, 1.85, 1.05, 1.42, 0.88]},\n        {'T': 310, 'dV': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'A': [1.10, 0.90, 1.00, 1.20, 0.80, 1.05]},\n        {'T': 300, 'dV': [0.0, 8.0, -2.0, 6.0, 0.5, -0.3, 4.0], 'A': [0.90, 2.50, 0.70, 1.80, 1.20, 1.00, 2.00]},\n        {'T': 298, 'dV': [0.7], 'A': [1.33]}\n    ]\n\n    results = []\n    for case in test_cases:\n        mean, uncertainty = compute_reweighted_stats(\n            case['T'], case['dV'], case['A'], k_B, N_BOOTSTRAP, rng\n        )\n        results.extend([mean, uncertainty])\n\n    # Final print statement in the exact required format.\n    # Each value is formatted to 6 decimal places.\n    formatted_results = [f'{x:.6f}' for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}