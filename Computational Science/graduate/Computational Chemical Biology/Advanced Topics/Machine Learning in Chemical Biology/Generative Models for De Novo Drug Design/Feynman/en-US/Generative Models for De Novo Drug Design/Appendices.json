{
    "hands_on_practices": [
        {
            "introduction": "Effective de novo drug design is a multi-objective challenge where high target affinity must be balanced with crucial pharmaceutical properties like synthetic accessibility and safety. This exercise  introduces a foundational technique for navigating this trade-off: using a penalized scoring function. By deriving and applying a surrogate objective based on Lagrangian relaxation, you will gain a practical understanding of how generative models can be guided to produce molecules that are not only potent but also adhere to essential medicinal chemistry rules.",
            "id": "5247455",
            "problem": "A de novo molecular generator is being trained to propose candidate ligands that optimize a normalized binding objective $A \\in [0,1]$ while satisfying medicinal chemistry rules such as Lipinski's Rule of Five, Pan-Assay Interference Compounds (PAINS) filtering, and synthetic accessibility constraints. Let $V \\in \\{0,1,2,\\dots\\}$ denote the count of rule violations observed for a given molecule under this rule set, and suppose the design goal is the constrained optimization problem to maximize $A$ subject to $V=0$. In practice, constrained optimization is often relaxed via the method of Lagrange multipliers by constructing an unconstrained surrogate objective that incorporates a penalty for constraint violation. Assume the penalty is additive across violations and, to first order, linear in the violation count, with a nonnegative scalar penalty weight $\\lambda \\geq 0$ that reflects the trade-off between $A$ and rule compliance. Starting from this constrained formulation and the principles of Lagrangian relaxation, derive the unconstrained surrogate objective $S(A,V;\\lambda)$ and then evaluate it for a candidate with $A=0.8$, $V=3$, and $\\lambda=0.1$. The score $S$ is dimensionless. Provide the exact value as a decimal. Additionally, briefly discuss the sensitivity of $S$ to changes in $\\lambda$ at fixed $A$ and $V$, and interpret this sensitivity in the context of tuning the generator for pharmaceutical rule compliance. Do not round the numerical score.",
            "solution": "The problem statement will be validated by first extracting the given information and then assessing its scientific and logical soundness.\n\n### Step 1: Extract Givens\n-   **Objective function**: A normalized binding objective $A \\in [0,1]$. The goal is to maximize $A$.\n-   **Constraint function**: A count of rule violations $V \\in \\{0, 1, 2, \\dots\\}$.\n-   **Constraint**: The optimization is subject to the condition $V=0$.\n-   **Method**: Lagrangian relaxation is used to create an unconstrained surrogate objective, $S$.\n-   **Penalty term**: The penalty for constraint violation is additive, linear in the violation count $V$, and weighted by a nonnegative scalar penalty weight $\\lambda \\geq 0$.\n-   **Surrogate objective**: $S(A,V;\\lambda)$ is the function to be derived.\n-   **Evaluation point**: The values for evaluation are $A=0.8$, $V=3$, and $\\lambda=0.1$.\n-   **Additional tasks**: Discuss the sensitivity of $S$ to changes in $\\lambda$ and interpret this sensitivity.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded**: The problem is well-grounded in the fields of medicinal chemistry and computational optimization. Concepts like Lipinski's rules, PAINS filters, synthetic accessibility, and binding objectives are standard in drug design. The use of a penalized scoring function, derived from principles of Lagrangian relaxation, is a common and valid technique for handling constraints in generative models and optimization algorithms.\n-   **Well-Posed**: The problem is well-posed. It specifies a clear optimization goal (maximize $A$ subject to $V=0$) and a standard method (Lagrangian relaxation with a linear penalty) to transform it into an unconstrained problem. This setup provides sufficient information to uniquely derive the form of the surrogate objective $S$.\n-   **Objective**: The language is precise, technical, and free from subjective or ambiguous statements.\n-   **Completeness and Consistency**: The problem is self-contained and its components are consistent. It provides all necessary information to derive the surrogate objective, evaluate it, and analyze its sensitivity.\n-   **Realism**: The provided numerical values ($A=0.8$, $V=3$, $\\lambda=0.1$) are plausible within the context of scoring functions for molecular design.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically sound, well-posed, objective, and complete. A full solution will be provided.\n\n### Derivation of the Surrogate Objective\nThe original constrained optimization problem is stated as:\n$$\n\\text{maximize } A \\quad \\text{subject to } V = 0\n$$\nThe method of Lagrange multipliers is employed to convert this into an unconstrained problem. The Lagrangian function, $\\mathcal{L}$, for a problem of maximizing a function $f(x)$ subject to a constraint $g(x)=c$ is generally written as $\\mathcal{L}(x, \\mu) = f(x) - \\mu(g(x)-c)$, where $\\mu$ is the Lagrange multiplier.\n\nIn this context, the function to be maximized is the binding objective $A$, and the constraint is $V=0$. Therefore, we can identify $f \\rightarrow A$, $g \\rightarrow V$, and the constraint value $c \\rightarrow 0$. The penalty weight $\\lambda$ plays the role of the Lagrange multiplier $\\mu$.\n\nThe Lagrangian function is therefore:\n$$\n\\mathcal{L}(A, V; \\lambda) = A - \\lambda(V - 0) = A - \\lambda V\n$$\nThis Lagrangian serves as the unconstrained surrogate objective $S(A, V; \\lambda)$ which the de novo generator seeks to maximize. A higher value of $S$ corresponds to a more desirable candidate molecule. This form correctly reflects the problem's requirements: it is an unconstrained objective that incorporates an additive penalty, which is linear in the violation count $V$ with a weight $\\lambda$.\n$$\nS(A, V; \\lambda) = A - \\lambda V\n$$\n\n### Evaluation of the Surrogate Objective\nWe are asked to evaluate $S(A, V; \\lambda)$ for a candidate molecule with $A=0.8$, $V=3$, and with a penalty weight of $\\lambda=0.1$.\nSubstituting these values into the derived expression for $S$:\n$$\nS(0.8, 3; 0.1) = 0.8 - (0.1)(3)\n$$\n$$\nS = 0.8 - 0.3\n$$\n$$\nS = 0.5\n$$\nThe score for this candidate molecule is $0.5$.\n\n### Sensitivity Analysis and Interpretation\nThe sensitivity of the score $S$ to changes in the penalty weight $\\lambda$ is given by the partial derivative of $S$ with respect to $\\lambda$, holding $A$ and $V$ constant.\n$$\n\\frac{\\partial S}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} (A - \\lambda V)\n$$\nSince $A$ and $V$ are properties of the given molecule, they are treated as constants in this differentiation.\n$$\n\\frac{\\partial S}{\\partial \\lambda} = -V\n$$\n**Interpretation:**\nThe sensitivity of the score $S$ with respect to the penalty weight $\\lambda$ is equal to the negative of the number of rule violations, $-V$. This result has a clear and significant interpretation in the context of tuning the molecular generator:\n\n1.  **Dependence on Violations**: The impact of changing $\\lambda$ on a molecule's score is directly proportional to its number of violations.\n    -   For a fully compliant molecule ($V=0$), the sensitivity is $\\frac{\\partial S}{\\partial \\lambda} = 0$. Its score is $S=A$, which is independent of the penalty weight $\\lambda$. This is logical, as there is no penalty to be weighted.\n    -   For a non-compliant molecule ($V>0$), the sensitivity is negative. This means that increasing the penalty weight $\\lambda$ will decrease the molecule's overall score.\n\n2.  **Tuning Parameter**: The parameter $\\lambda$ acts as a \"tuning knob\" for the drug designer to control the behavior of the generative model.\n    -   By increasing $\\lambda$, the designer places greater importance on rule compliance. Molecules with a high number of violations (large $V$) will be more heavily penalized and thus will be less likely to be proposed by the generator as \"optimal\". This steers the generator towards producing more \"drug-like\" candidates that satisfy the specified medicinal chemistry rules.\n    -   Conversely, if the generator becomes too conservative, producing only rule-compliant molecules but with suboptimal binding objectives ($A$), the designer can decrease $\\lambda$. This reduces the penalty for violations, allowing the generator to explore a wider chemical space that may contain molecules with higher binding potential, even if they have some rule violations.\n\nIn summary, the sensitivity $\\frac{\\partial S}{\\partial \\lambda} = -V$ quantifies the trade-off between maximizing the primary objective ($A$) and satisfying the constraints ($V=0$). It provides a mechanism by which molecules with more violations are more strongly disfavored as the stringency of the rule-based filter ($\\lambda$) is increased.",
            "answer": "$$\n\\boxed{0.5}\n$$"
        },
        {
            "introduction": "Many generative models conceptualize molecules as a sequence of characters or 'tokens', akin to a language model writing a sentence. This exercise  explores the core mechanism of such a generator that produces SMILES strings, focusing on how the model's output probabilities are controlled. Starting from the principle of maximum entropy, you will derive the temperature-scaled softmax distribution and see how tuning the temperature parameter $\\tau$ directly influences the diversity and validity of the generated molecular structures.",
            "id": "3847976",
            "problem": "Consider a generative language model over the Simplified Molecular Input Line Entry System (SMILES), which emits a sequence of tokens from a finite vocabulary $\\mathcal{V}$ to construct molecular strings. At generation step $t \\in \\{1, 2, \\dots, L\\}$, the model assigns real-valued scores (logits) $\\{z_{i}^{(t)}\\}_{i \\in \\mathcal{V}}$ to tokens, which determine the sampling distribution via a maximum-entropy principle with a fixed expected score and a normalization constraint. The maximum-entropy solution introduces a scalar temperature parameter $\\tau > 0$ that controls the dispersion of the distribution over tokens. \n\nAssume a simplified validity criterion for SMILES construction in which, for each step $t$, there exists a context-dependent subset $S_{t} \\subseteq \\mathcal{V}$ of tokens that keep the partial string valid with respect to local constraints (for example, branch parenthesis balancing or ring closure tokens permitted by the current state). Under an independence approximation for validity across steps conditioned on the model’s distributions, the probability that an independently sampled token at step $t$ results in a valid partial string is the total probability mass assigned to $S_{t}$ by the model’s temperature-controlled distribution. The overall validity of an $L$-token sequence is then the product of these stepwise validities.\n\nStarting from the definition of Shannon entropy and the constraints of normalization and fixed expected score, derive the temperature-controlled distribution over tokens at each step. Then, express the expected validity rate $R(\\tau)$ of independently generated $L$-length SMILES as a closed-form analytic function of $\\tau$, the logits $\\{z_{i}^{(t)}\\}$, and the valid sets $\\{S_{t}\\}$. Your final answer must be a single closed-form expression for $R(\\tau)$ that depends only on $\\tau$, $\\{z_{i}^{(t)}\\}$, and $\\{S_{t}\\}$. No numerical evaluation is required.",
            "solution": "The problem requires the derivation of a temperature-controlled sampling distribution based on the maximum-entropy principle and subsequently using this distribution to find a closed-form expression for the expected validity rate of generated sequences. The process is divided into two main parts: first, deriving the probability distribution, and second, calculating the validity rate.\n\nPart 1: Derivation of the Temperature-Controlled Distribution\n\nThe problem states that at each generation step $t \\in \\{1, 2, \\dots, L\\}$, the sampling distribution over the vocabulary $\\mathcal{V}$ is determined by maximizing the Shannon entropy subject to certain constraints. Let $p_i^{(t)}$ be the probability of sampling token $i \\in \\mathcal{V}$ at step $t$. For clarity, we will omit the superscript $(t)$ during the derivation, as the logic is identical for each step. The Shannon entropy is given by:\n$$H = -\\sum_{i \\in \\mathcal{V}} p_i \\ln(p_i)$$\n\nThis maximization is subject to two constraints:\n1.  Normalization: The sum of probabilities over all tokens in the vocabulary must be $1$.\n    $$\\sum_{i \\in \\mathcal{V}} p_i = 1$$\n2.  Fixed expected score: The expected value of the logits $\\{z_i\\}_{i \\in \\mathcal{V}}$ must equal a fixed, but unspecified, constant $E$.\n    $$\\sum_{i \\in \\mathcal{V}} p_i z_i = E$$\n\nWe employ the method of Lagrange multipliers to solve this constrained optimization problem. The Lagrangian $\\mathcal{L}$ is constructed as:\n$$\\mathcal{L}(p_i, \\lambda_0, \\lambda_1) = H - \\lambda_0 \\left(\\sum_{i \\in \\mathcal{V}} p_i - 1\\right) - \\lambda_1 \\left(\\sum_{i \\in \\mathcal{V}} p_i z_i - E\\right)$$\nSubstituting the expression for $H$:\n$$\\mathcal{L}(p_i, \\lambda_0, \\lambda_1) = -\\sum_{i \\in \\mathcal{V}} p_i \\ln(p_i) - \\lambda_0 \\sum_{i \\in \\mathcal{V}} p_i + \\lambda_0 - \\lambda_1 \\sum_{i \\in \\mathcal{V}} p_i z_i + \\lambda_1 E$$\n\nTo find the probabilities $p_k$ that maximize $\\mathcal{L}$, we take the partial derivative with respect to each $p_k$ and set it to $0$:\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( -p_k \\ln(p_k) - \\lambda_0 p_k - \\lambda_1 p_k z_k \\right) = 0$$\nUsing the product rule for differentiation, $\\frac{d}{dx}(x\\ln x) = \\ln x + 1$:\n$$-(\\ln(p_k) + 1) - \\lambda_0 - \\lambda_1 z_k = 0$$\n$$\\ln(p_k) = -1 - \\lambda_0 - \\lambda_1 z_k$$\nSolving for $p_k$:\n$$p_k = \\exp(-1 - \\lambda_0 - \\lambda_1 z_k) = \\exp(-1 - \\lambda_0) \\exp(-\\lambda_1 z_k)$$\n\nNow, we use the normalization constraint to determine the term involving $\\lambda_0$.\n$$\\sum_{k \\in \\mathcal{V}} p_k = \\sum_{k \\in \\mathcal{V}} \\exp(-1 - \\lambda_0) \\exp(-\\lambda_1 z_k) = 1$$\n$$\\exp(-1 - \\lambda_0) \\sum_{k \\in \\mathcal{V}} \\exp(-\\lambda_1 z_k) = 1$$\n$$\\exp(-1 - \\lambda_0) = \\frac{1}{\\sum_{j \\in \\mathcal{V}} \\exp(-\\lambda_1 z_j)}$$\n\nSubstituting this back into the expression for $p_k$:\n$$p_k = \\frac{\\exp(-\\lambda_1 z_k)}{\\sum_{j \\in \\mathcal{V}} \\exp(-\\lambda_1 z_j)}$$\n\nThe problem introduces a temperature parameter $\\tau > 0$. In statistical mechanics and machine learning, the Lagrange multiplier associated with an energy-like constraint is related to inverse temperature. The conventional form of temperature scaling for logits involves dividing the logits by $\\tau$. To match this convention, we define the relationship between our Lagrange multiplier $\\lambda_1$ and the temperature $\\tau$ as $\\lambda_1 = -1/\\tau$. This choice gives the desired form.\n\nSubstituting $\\lambda_1 = -1/\\tau$ into the probability expression and reintroducing the step index $(t)$, we obtain the temperature-controlled distribution, which is the softmax function:\n$$p_i^{(t)}(\\tau) = \\frac{\\exp(z_i^{(t)}/\\tau)}{\\sum_{j \\in \\mathcal{V}} \\exp(z_j^{(t)}/\\tau)}$$\n\nPart 2: Derivation of the Expected Validity Rate $R(\\tau)$\n\nThe problem defines a simplified validity criterion where, at each step $t$, a subset of tokens $S_t \\subseteq \\mathcal{V}$ maintains the validity of the partial sequence. The probability that an independently sampled token at step $t$ is valid, which we denote $V_t(\\tau)$, is the sum of the probabilities of the tokens in this valid set $S_t$.\n$$V_t(\\tau) = \\sum_{i \\in S_t} p_i^{(t)}(\\tau)$$\n\nSubstituting the derived expression for $p_i^{(t)}(\\tau)$:\n$$V_t(\\tau) = \\sum_{i \\in S_t} \\frac{\\exp(z_i^{(t)}/\\tau)}{\\sum_{j \\in \\mathcal{V}} \\exp(z_j^{(t)}/\\tau)} = \\frac{\\sum_{i \\in S_t} \\exp(z_i^{(t)}/\\tau)}{\\sum_{j \\in \\mathcal{V}} \\exp(z_j^{(t)}/\\tau)}$$\n\nThe problem states that the overall validity of an $L$-token sequence is the product of these stepwise validity probabilities, due to an independence approximation. The expected validity rate, $R(\\tau)$, is this overall validity probability.\n$$R(\\tau) = \\prod_{t=1}^{L} V_t(\\tau)$$\n\nBy substituting the expression for $V_t(\\tau)$, we arrive at the final closed-form expression for the expected validity rate $R(\\tau)$ as a function of the temperature $\\tau$, the logits $\\{z_i^{(t)}\\}$, and the valid sets $\\{S_t\\}$.\n$$R(\\tau) = \\prod_{t=1}^{L} \\left( \\frac{\\sum_{i \\in S_t} \\exp(z_i^{(t)}/\\tau)}{\\sum_{j \\in \\mathcal{V}} \\exp(z_j^{(t)}/\\tau)} \\right)$$\nThis expression is the required closed-form analytic function.",
            "answer": "$$ \\boxed{ \\prod_{t=1}^{L} \\left( \\frac{\\sum_{i \\in S_t} \\exp(z_i^{(t)}/\\tau)}{\\sum_{j \\in \\mathcal{V}} \\exp(z_j^{(t)}/\\tau)} \\right) } $$"
        },
        {
            "introduction": "Generating molecular structures directly in three-dimensional space represents a major advance, but requires that models respect the fundamental symmetries of physics. This hands-on practice  delves into the critical concept of E(3) equivariance, a property ensuring that a model's predictions behave correctly under rotations and translations. You will implement the standard alignment-invariant metric, Root-Mean-Square Deviation (RMSD), to quantitatively demonstrate why equivariant models are essential for producing high-quality, physically plausible conformers compared to their non-equivariant counterparts.",
            "id": "3847969",
            "problem": "You are evaluating coordinate-generating models for de novo drug design, where each model outputs three-dimensional atomic coordinates for a single conformer of a small molecule. A robust comparison requires an alignment-invariant error metric between a generated conformer and a reference conformer. The canonical metric is the root-mean-square deviation (RMSD), computed after optimal rigid alignment that preserves chirality. In the context of Euclidean transformations, an Equivariance to the three-dimensional Euclidean group (E(3)) implies that if the inputs are rotated or translated, the model outputs transform accordingly by the same rotation and translation without changing molecular shape.\n\nYour task is to implement, from first principles, a program that computes the alignment-invariant root-mean-square deviation (RMSD) between a predicted conformer and a fixed reference conformer using the optimal rotation that minimizes the mean squared error under a rigid transformation with determinant constraint $+\\!1$ (no reflections), and to analyze the reduction in RMSD achieved by Euclidean group equivariant architectures relative to vanilla architectures.\n\nFundamental base to use:\n- Euclidean distance in three dimensions: for points $p,q \\in \\mathbb{R}^3$, the squared distance is $\\lVert p - q \\rVert_2^2$.\n- Properties of orthogonal matrices: a matrix $R \\in \\mathbb{R}^{3 \\times 3}$ is a rotation if $R^\\top R = I$ and $\\det(R) = 1$.\n- Singular Value Decomposition (SVD): for any matrix $H \\in \\mathbb{R}^{3 \\times 3}$, there exist orthogonal matrices $U,V \\in \\mathbb{R}^{3 \\times 3}$ and a diagonal matrix $\\Sigma$ with nonnegative entries such that $H = U \\Sigma V^\\top$.\n- The optimal Procrustes/Kabsch alignment: the optimal rotation that minimizes the sum of squared distances between centered point sets is obtained from the SVD of the covariance matrix, with a determinant correction to enforce $\\det(R)=1$.\n\nMathematical definitions to implement:\n- For $N$ atoms, let the reference conformer be $X \\in \\mathbb{R}^{N \\times 3}$ with rows $x_i \\in \\mathbb{R}^3$, and a prediction be $Y \\in \\mathbb{R}^{N \\times 3}$ with rows $y_i \\in \\mathbb{R}^3$. Define the centroids $c_X = \\frac{1}{N}\\sum_{i=1}^N x_i$ and $c_Y = \\frac{1}{N}\\sum_{i=1}^N y_i$. Let $\\tilde{X} = X - \\mathbf{1} c_X^\\top$ and $\\tilde{Y} = Y - \\mathbf{1} c_Y^\\top$. Form the covariance $H = \\tilde{Y}^\\top \\tilde{X}$. Compute the singular value decomposition $H = U \\Sigma V^\\top$. Define $R^\\star = V U^\\top$ and if $\\det(R^\\star) < 0$, flip the sign of the last column of $V$ and recompute $R^\\star = V U^\\top$ to enforce $\\det(R^\\star)=1$. The optimal translation is $t^\\star = c_X - R^\\star c_Y$. The aligned prediction is $Y_{\\text{aligned}} = \\tilde{Y} R^\\star$ (equivalently $Y R^\\star + \\mathbf{1} t^{\\star\\top}$).\n- The alignment-invariant root-mean-square deviation is $\\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\lVert (y_i - c_Y) R^\\star - (x_i - c_X) \\rVert_2^2}$.\n- Define the improvement metric for test case $k$ as $\\Delta_k = \\mathrm{RMSD}_{\\text{vanilla},k} - \\mathrm{RMSD}_{\\text{equivariant},k}$.\n\nUnits:\n- All coordinates and translations are in Angstroms. All requested outputs involving distances must be expressed in Angstroms as real-valued numbers (floats).\n- All rotation angles are given in radians.\n\nReference conformer:\nLet $N = 7$, and the fixed reference conformer coordinates $X \\in \\mathbb{R}^{7 \\times 3}$ in Angstroms be the following seven points (rows), representing a planar hexagon with one out-of-plane substituent:\n- $x_0 = (1.400, 0.000, 0.000)$\n- $x_1 = (0.700, 1.212435, 0.000)$\n- $x_2 = (-0.700, 1.212435, 0.000)$\n- $x_3 = (-1.400, 0.000, 0.000)$\n- $x_4 = (-0.700, -1.212435, 0.000)$\n- $x_5 = (0.700, -1.212435, 0.000)$\n- $x_6 = (0.000, 0.000, 1.100)$\n\nDeterministic noise model:\nFor atom index $i \\in \\{0,1,2,3,4,5,6\\}$ and amplitude $\\epsilon \\ge 0$, define the deterministic noise vector\n$n_i(\\epsilon) = \\epsilon \\cdot \\big(\\sin(i), \\cos(2i), \\sin(3i)\\big)$,\nwith all trigonometric functions taking arguments in radians.\n\nRotation parameterization:\nUse intrinsic rotations about axes in the order $z$ then $y$ then $x$. Given angles $(\\alpha,\\beta,\\gamma)$ in radians, define $R(\\alpha,\\beta,\\gamma) = R_z(\\gamma) R_y(\\beta) R_x(\\alpha)$, where\n$R_x(\\alpha) = \\begin{bmatrix}1 & 0 & 0\\\\ 0 & \\cos\\alpha & -\\sin\\alpha\\\\ 0 & \\sin\\alpha & \\cos\\alpha\\end{bmatrix}$,\n$R_y(\\beta) = \\begin{bmatrix}\\cos\\beta & 0 & \\sin\\beta\\\\ 0 & 1 & 0\\\\ -\\sin\\beta & 0 & \\cos\\beta\\end{bmatrix}$,\n$R_z(\\gamma) = \\begin{bmatrix}\\cos\\gamma & -\\sin\\gamma & 0\\\\ \\sin\\gamma & \\cos\\gamma & 0\\\\ 0 & 0 & 1\\end{bmatrix}$.\n\nTest suite:\nConstruct four test cases. For each case, build two predictions $Y_{\\text{eq}}$ (equivariant-like) and $Y_{\\text{van}}$ (vanilla-like) as follows:\n- Given a base linear transformation $A \\in \\mathbb{R}^{3 \\times 3}$ and translation $t \\in \\mathbb{R}^3$, define the transformed coordinates $Z = X A^\\top + \\mathbf{1} t^\\top$. Then add deterministic noise row-wise: the $i$-th row is $z_i + n_i(\\epsilon)$ for a specified $\\epsilon$.\n\nCase $1$ (happy path: anisotropic scaling harms vanilla):\n- Equivariant-like: $A_{\\text{eq}} = R(0.5, -0.2, 0.3)$, $t_{\\text{eq}} = (1.2, -0.7, 0.4)$, $\\epsilon_{\\text{eq}} = 0.05$.\n- Vanilla-like: $A_{\\text{van}} = S R(0.5, -0.2, 0.3)$ with $S = \\mathrm{diag}(1.03, 0.97, 1.02)$, $t_{\\text{van}} = (1.2, -0.7, 0.4)$, $\\epsilon_{\\text{van}} = 0.20$.\n\nCase $2$ (boundary: reflection cannot be removed by proper rotation):\n- Equivariant-like: $A_{\\text{eq}} = R(1.0, 0.2, -0.7)$, $t_{\\text{eq}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{eq}} = 0.00$.\n- Vanilla-like: $A_{\\text{van}} = M R(1.0, 0.2, -0.7)$ where $M = \\mathrm{diag}(1.0, 1.0, -1.0)$ (a reflection with $\\det(M) = -1$), $t_{\\text{van}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{van}} = 0.00$.\n\nCase $3$ (edge: shear and higher noise harm vanilla):\n- Equivariant-like: $A_{\\text{eq}} = R(0.3, 0.4, 0.2)$, $t_{\\text{eq}} = (-0.5, 0.8, -0.3)$, $\\epsilon_{\\text{eq}} = 0.10$.\n- Vanilla-like: $A_{\\text{van}} = B R(0.3, 0.4, 0.2)$ with $B = \\begin{bmatrix}1.00 & 0.04 & 0.02\\\\ 0.01 & 1.00 & 0.03\\\\ -0.02 & 0.00 & 1.02\\end{bmatrix}$, $t_{\\text{van}} = (-0.5, 0.8, -0.3)$, $\\epsilon_{\\text{van}} = 0.30$.\n\nCase $4$ (boundary: identity, no noise):\n- Equivariant-like: $A_{\\text{eq}} = I_{3}$, $t_{\\text{eq}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{eq}} = 0.00$.\n- Vanilla-like: $A_{\\text{van}} = I_{3}$, $t_{\\text{van}} = (0.0, 0.0, 0.0)$, $\\epsilon_{\\text{van}} = 0.00$.\n\nComputation requirements:\n- Implement the optimal rigid alignment with determinant constraint $+\\!1$ as described, compute $\\mathrm{RMSD}(Y,X)$ for both $Y_{\\text{eq}}$ and $Y_{\\text{van}}$ in each case, and then compute the improvement $\\Delta_k$ for $k \\in \\{1,2,3,4\\}$ as $\\Delta_k = \\mathrm{RMSD}_{\\text{vanilla},k} - \\mathrm{RMSD}_{\\text{equivariant},k}$.\n- All RMSD values and improvements must be computed and reported in Angstroms as real numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the four improvement values $\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4$ as a comma-separated list enclosed in square brackets, for example $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$. Use the exact order of cases $1$ through $4$.\n\nNote: Do not use any randomness; use the deterministic noise $n_i(\\epsilon)$ exactly as specified. Angles are in radians, and distances are in Angstroms.",
            "solution": "The task is to implement a function to compute the alignment-invariant root-mean-square deviation (RMSD) between two molecular conformers and to use this function to evaluate the performance difference between an equivariant and a non-equivariant generative model. The problem is scientifically sound and well-posed, grounded in the established principles of structural alignment in computational biology, specifically the Kabsch algorithm for optimal rigid superposition.\n\nBefore proceeding, it is necessary to clarify minor notational inconsistencies in the problem statement to ensure a correct implementation. The problem defines coordinates as row vectors in matrices $X, Y \\in \\mathbb{R}^{N \\times 3}$. Standard matrix operations for such a convention are assumed.\n1.  Centering coordinates: The expression $\\tilde{X} = X - \\mathbf{1} c_X^\\top$ is notationally problematic. Assuming $c_X$ is a $1 \\times 3$ row vector (the mean of the rows of $X$), and $\\mathbf{1}$ is an $N \\times 1$ column vector of ones, the operation should be $\\tilde{X} = X - \\mathbf{1} c_X$. In practice, this is achieved by subtracting the mean vector from each row of the coordinate matrix, an operation well-supported by broadcasting in numerical libraries.\n2.  Optimal translation: The expression for the optimal translation vector $t^\\star = c_X - R^\\star c_Y$ involves an invalid matrix-vector product $R^\\star c_Y$ for a $3 \\times 3$ matrix $R^\\star$ and a $1 \\times 3$ row vector $c_Y$. The correct expression for row vectors is $t^\\star = c_X - c_Y R^\\star$. However, the specified RMSD formula, $\\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\lVert (y_i - c_Y) R^\\star - (x_i - c_X) \\rVert_2^2}$, correctly depends only on the centered coordinates. This means the translational component is implicitly handled by the centering step, and the explicit calculation of $t^\\star$ is not required for the RMSD.\n\nWith these clarifications, the solution proceeds by implementing the Kabsch algorithm and applying it to the specified test cases.\n\nFirst, we define a function to compute the RMSD. Let the reference coordinates be $X \\in \\mathbb{R}^{N \\times 3}$ and the predicted coordinates be $Y \\in \\mathbb{R}^{N \\times 3}$.\n\nStep 1: Center the Coordinate Sets\nThe first step in rigid alignment is to remove the translational component by moving the centroid of each conformer to the origin. The centroids are calculated as:\n$$ c_X = \\frac{1}{N} \\sum_{i=1}^N x_i $$\n$$ c_Y = \\frac{1}{N} \\sum_{i=1}^N y_i $$\nThe centered coordinate matrices, $\\tilde{X}$ and $\\tilde{Y}$, are then obtained by subtracting the respective centroids from each coordinate vector:\n$$ \\tilde{X} = X - \\mathbf{1}c_X $$\n$$ \\tilde{Y} = Y - \\mathbf{1}c_Y $$\nHere, $x_i$ and $y_i$ are the $i$-th rows of $X$ and $Y$, and $\\mathbf{1}$ is an $N \\times 1$ column vector of ones.\n\nStep 2: Compute the Covariance Matrix\nThe optimal rotation is found by maximizing the overlap between the two centered point sets. This is achieved by finding the rotation $R$ that minimizes $\\sum_{i=1}^N \\|(y_i - c_Y)R - (x_i - c_X)\\|^2$. The solution involves the covariance matrix $H$ between the two centered sets:\n$$ H = \\tilde{Y}^\\top \\tilde{X} \\in \\mathbb{R}^{3 \\times 3} $$\n\nStep 3: Singular Value Decomposition (SVD)\nThe covariance matrix $H$ is decomposed using SVD:\n$$ H = U \\Sigma V^\\top $$\nwhere $U$ and $V$ are $3 \\times 3$ orthogonal matrices, and $\\Sigma$ is a $3 \\times 3$ diagonal matrix of non-negative singular values.\n\nStep 4: Compute the Optimal Rotation Matrix\nThe optimal rotation matrix $R^\\star$ that maximizes the overlap is given by:\n$$ R^\\star = V U^\\top $$\nHowever, this matrix $R^\\star$ could represent a rotation ($\\det(R^\\star) = +1$) or a reflection-rotation ($\\det(R^\\star) = -1$). To ensure we find a pure rotation that preserves chirality, we must enforce the constraint $\\det(R^\\star) = +1$. This is done by checking the sign of the determinant. If $\\det(R^\\star) = -1$, it indicates that the optimal transformation is improper. To find the best *proper* rotation, we invert the component of the transformation corresponding to the smallest singular value. As specified, if $\\det(V U^\\top) < 0$, we flip the sign of the last column of $V$ (assuming singular values are ordered descendingly) and recompute $R^\\star$:\n$$ \\text{If } \\det(V U^\\top) = -1, \\text{ let } V' = V \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix}, \\text{ then } R^\\star = V' U^\\top $$\nThis procedure guarantees that $\\det(R^\\star)=1$.\n\nStep 5: Calculate the RMSD\nWith the optimal proper rotation $R^\\star$, the RMSD is calculated as the square root of the mean of the squared Euclidean distances between the aligned predicted coordinates and the reference coordinates:\n$$ \\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\lVert (y_i - c_Y) R^\\star - (x_i - c_X) \\rVert_2^2} $$\nThis is equivalent to calculating the Frobenius norm of the difference between the aligned centered matrices:\n$$ \\mathrm{RMSD}(Y,X) = \\sqrt{\\frac{1}{N} \\lVert \\tilde{Y} R^\\star - \\tilde{X} \\rVert_F^2} $$\n\nNext, we programmatically construct the test cases. A helper function is created to generate rotation matrices $R(\\alpha, \\beta, \\gamma) = R_z(\\gamma) R_y(\\beta) R_x(\\alpha)$ given Euler angles. Another function generates the deterministic noise vector $n_i(\\epsilon)$.\n\nFor each of the four test cases, we generate the \"equivariant-like\" coordinates $Y_{\\text{eq}}$ and \"vanilla-like\" coordinates $Y_{\\text{van}}$. For a given affine transformation $(A, t)$ and noise level $\\epsilon$, the predicted coordinates $Y$ are generated from the reference $X$ by applying the linear transformation and translation, followed by the addition of noise:\n$$ Y_i = X_i A^\\top + t + n_i(\\epsilon) \\quad \\text{for each atom } i=0, \\dots, 6 $$\n\nFinally, for each case $k \\in \\{1,2,3,4\\}$, we compute $\\mathrm{RMSD}_{\\text{equivariant},k} = \\mathrm{RMSD}(Y_{\\text{eq},k}, X)$ and $\\mathrm{RMSD}_{\\text{vanilla},k} = \\mathrm{RMSD}(Y_{\\text{van},k}, X)$. The improvement metric is then calculated as $\\Delta_k = \\mathrm{RMSD}_{\\text{vanilla},k} - \\mathrm{RMSD}_{\\text{equivariant},k}$. The expected results are:\n-   Case 1: $\\Delta_1 > 0$. The vanilla model's output is distorted by anisotropic scaling, which cannot be corrected by a rigid alignment.\n-   Case 2: $\\Delta_2 > 0$. The vanilla model's output is a reflection of the true structure. The alignment must find a proper rotation, leaving a large residual RMSD. The equivariant model, with $\\epsilon=0$, should have $\\mathrm{RMSD}=0$.\n-   Case 3: $\\Delta_3 > 0$. The shear transformation applied to the vanilla model causes a non-rigid distortion, leading to a higher RMSD.\n-   Case 4: $\\Delta_4 = 0$. Both models produce the exact reference structure, resulting in zero RMSD for both and thus zero improvement. This serves as a sanity check.\n\nThe implementation will follow these steps to calculate the four $\\Delta_k$ values.",
            "answer": "[0.1559495101683416,1.0416950269389255,0.2091209355106198,0.0]"
        }
    ]
}