{
    "hands_on_practices": [
        {
            "introduction": "深度学习模型在预测蛋白质结构时，不仅要捕捉整体折叠的正确性，还必须生成符合物理化学原理的原子排布。为了实现这一点，一个关键技术是在损失函数中引入惩罚项，对预测结构中不合理的键长和键角进行惩罚。通过这个练习，你将亲手计算一个简单的肽段片段的共价几何能量惩罚，从而直观地理解物理先验知识如何被编码为可微的损失项，并指导模型的训练过程。",
            "id": "3842251",
            "problem": "在训练用于蛋白质结构预测的深度学习模型中的可微几何模块时，共价几何的违规通常通过对键长和键角的加权二次项进行惩罚。考虑一个单一的肽主链片段，其中有三个重原子，标记为 $A \\equiv \\mathrm{N}$、$B \\equiv \\mathrm{C_{\\alpha}}$ 和 $C \\equiv \\mathrm{C}$。模型预测这些原子的笛卡尔坐标（单位为埃）为 $\\mathbf{r}_{A} = \\left(1.500, 0.000, 0.000\\right)$、$\\mathbf{r}_{B} = \\left(0.000, 0.000, 0.000\\right)$ 和 $\\mathbf{r}_{C} = \\left(-0.750, 1.299038106, 0.000\\right)$。给定一组目标共价几何值：$A\\!-\\!B$ 的目标键长为 $1.458$，$B\\!-\\!C$ 的目标键长为 $1.525$，以及在 $B$ 处的 $\\angle A\\!-\\!B\\!-\\!C$ 的目标键角为 $2.000$ 弧度。二次惩罚的相应权重为：每个键长约束为 $100$，键角约束为 $50$。\n\n仅使用欧几里得几何和向量微积分的第一性原理，计算由下式定义的总违规惩罚\n$$\nL \\;=\\; \\sum_{i} w_{i} \\left(d_{i} - d_{i}^{\\ast}\\right)^{2} \\;+\\; \\sum_{j} \\tilde{w}_{j} \\left(\\theta_{j} - \\theta_{j}^{\\ast}\\right)^{2},\n$$\n其中 $d_{i}$ 是从预测坐标计算出的键长，$\\theta_{j}$ 是从预测坐标计算出的键角，$d_{i}^{\\ast}$ 和 $\\theta_{j}^{\\ast}$ 是相应的目标值，而 $w_{i}$、$\\tilde{w}_{j}$ 是给定的权重。使用标准的欧几里得范数和非零向量 $\\mathbf{u}$ 与 $\\mathbf{v}$ 之间夹角的定义 $\\theta = \\arccos\\!\\big( (\\mathbf{u}\\cdot\\mathbf{v})/(\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|) \\big)$。所有角度必须以弧度为单位。将 $L$ 的最终值表示为实数，并将您的答案四舍五入到四位有效数字。最终答案中不要包含单位。",
            "solution": "该问题要求计算一个三原子肽主链片段的总违规惩罚 $L$。惩罚函数是键长和键角与其各自目标值偏差的二次项之和。\n\n首先，我们必须验证问题陈述。\n\n### 步骤1：提取给定信息\n提供的数据如下：\n- 原子坐标：\n  - $\\mathbf{r}_{A} = \\left(1.500, 0.000, 0.000\\right)$\n  - $\\mathbf{r}_{B} = \\left(0.000, 0.000, 0.000\\right)$\n  - $\\mathbf{r}_{C} = \\left(-0.750, 1.299038106, 0.000\\right)$\n- 目标键长：\n  - $d_{A-B}^{\\ast} = 1.458$\n  - $d_{B-C}^{\\ast} = 1.525$\n- 目标键角：\n  - $\\theta_{A-B-C}^{\\ast} = 2.000$ 弧度\n- 惩罚权重：\n  - $w_i = 100$ 对每个键长\n  - $\\tilde{w}_j = 50$ 对键角\n- 惩罚函数：\n  - $L = \\sum_{i} w_{i} \\left(d_{i} - d_{i}^{\\ast}\\right)^{2} \\;+\\; \\sum_{j} \\tilde{w}_{j} \\left(\\theta_{j} - \\theta_{j}^{\\ast}\\right)^{2}$\n- 角度定义：\n  - $\\theta = \\arccos\\!\\big( (\\mathbf{u}\\cdot\\mathbf{v})/(\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|) \\big)$\n\n### 步骤2：使用提取的给定信息进行验证\n该问题具有科学依据，因为它描述了分子力学力场和用于蛋白质结构的深度学习模型中使用的标准能量项。所提供的坐标、键长和键角的目标值以及惩罚权重对于肽主链片段都是物理上现实的。该问题是适定的，提供了所有必要的信息和明确的目标。语言精确且无歧义。因此，该问题是有效的。\n\n### 步骤3：执行\n问题有效。我们继续进行求解。\n\n总惩罚 $L$ 是两个键长（$A-B$ 和 $B-C$）和一个键角（$\\angle A-B-C$）的惩罚之和。该公式可以明确写为：\n$$\nL = w_{A-B} (d_{A-B} - d_{A-B}^{\\ast})^2 + w_{B-C} (d_{B-C} - d_{B-C}^{\\ast})^2 + \\tilde{w}_{A-B-C} (\\theta_{A-B-C} - \\theta_{A-B-C}^{\\ast})^2\n$$\n我们将按顺序计算这个和的每个分量。\n\n首先，我们确定表示化学键的向量，这些向量以中心原子 $B$ 为起点。\n$B-A$ 键的向量是 $\\mathbf{u} = \\mathbf{r}_{A} - \\mathbf{r}_{B}$：\n$$\n\\mathbf{u} = (1.500, 0.000, 0.000) - (0.000, 0.000, 0.000) = (1.500, 0.000, 0.000)\n$$\n$B-C$ 键的向量是 $\\mathbf{v} = \\mathbf{r}_{C} - \\mathbf{r}_{B}$：\n$$\n\\mathbf{v} = (-0.750, 1.299038106, 0.000) - (0.000, 0.000, 0.000) = (-0.750, 1.299038106, 0.000)\n$$\n\n接下来，我们计算预测的键长 $d_{A-B}$ 和 $d_{B-C}$，它们是向量 $\\mathbf{u}$ 和 $\\mathbf{v}$ 的欧几里得范数。\n$$\nd_{A-B} = \\|\\mathbf{u}\\| = \\sqrt{(1.500)^2 + (0.000)^2 + (0.000)^2} = \\sqrt{2.25} = 1.500\n$$\n$$\nd_{B-C} = \\|\\mathbf{v}\\| = \\sqrt{(-0.750)^2 + (1.299038106)^2 + (0.000)^2}\n$$\n我们注意到 $1.299038106$ 是 $0.750 \\times \\sqrt{3}$ 的一个高精度值。让我们来验证一下。$(0.750 \\times \\sqrt{3})^2 = (0.750)^2 \\times 3 = 0.5625 \\times 3 = 1.6875$。计算过程如下：\n$$\nd_{B-C} = \\sqrt{0.5625 + 1.6875} = \\sqrt{2.25} = 1.500\n$$\n所以，预测的键长为 $d_{A-B} = 1.500$ 和 $d_{B-C} = 1.500$。\n\n现在，我们计算键长惩罚项 $L_{\\text{length}}$：\n$$\nL_{\\text{length}} = w_{A-B}(d_{A-B} - d_{A-B}^{\\ast})^2 + w_{B-C}(d_{B-C} - d_{B-C}^{\\ast})^2\n$$\n代入给定值（$w_{A-B} = w_{B-C} = 100$，$d_{A-B}^{\\ast} = 1.458$，$d_{B-C}^{\\ast} = 1.525$）：\n$$\nL_{\\text{length}} = 100(1.500 - 1.458)^2 + 100(1.500 - 1.525)^2\n$$\n$$\nL_{\\text{length}} = 100(0.042)^2 + 100(-0.025)^2 = 100(0.001764) + 100(0.000625)\n$$\n$$\nL_{\\text{length}} = 0.1764 + 0.0625 = 0.2389\n$$\n\n接下来，我们计算预测的键角 $\\theta_{A-B-C}$。我们使用点积公式：\n$$\n\\theta_{A-B-C} = \\arccos\\left(\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\right)\n$$\n首先，计算点积 $\\mathbf{u} \\cdot \\mathbf{v}$：\n$$\n\\mathbf{u} \\cdot \\mathbf{v} = (1.500)(-0.750) + (0.000)(1.299038106) + (0.000)(0.000) = -1.125\n$$\n范数的乘积是 $\\|\\mathbf{u}\\| \\|\\mathbf{v}\\| = (1.500)(1.500) = 2.25$。\n因此，该角度的余弦值为：\n$$\n\\cos(\\theta_{A-B-C}) = \\frac{-1.125}{2.25} = -0.5\n$$\n因此，该角度为：\n$$\n\\theta_{A-B-C} = \\arccos(-0.5) = \\frac{2\\pi}{3} \\text{ 弧度}\n$$\n\n现在，我们计算键角惩罚项 $L_{\\text{angle}}$：\n$$\nL_{\\text{angle}} = \\tilde{w}_{A-B-C}(\\theta_{A-B-C} - \\theta_{A-B-C}^{\\ast})^2\n$$\n代入给定值（$\\tilde{w}_{A-B-C} = 50$，$\\theta_{A-B-C}^{\\ast} = 2.000$）：\n$$\nL_{\\text{angle}} = 50\\left(\\frac{2\\pi}{3} - 2.000\\right)^2\n$$\n使用 $\\pi$ 的数值近似值 $\\pi \\approx 3.14159265$，我们得到 $\\frac{2\\pi}{3} \\approx 2.0943951$。\n$$\nL_{\\text{angle}} \\approx 50(2.0943951 - 2.000)^2 = 50(0.0943951)^2\n$$\n$$\nL_{\\text{angle}} \\approx 50(0.008910439) \\approx 0.445522\n$$\n\n最后，我们通过将键长和键角惩罚相加来计算总惩罚 $L$：\n$$\nL = L_{\\text{length}} + L_{\\text{angle}} \\approx 0.2389 + 0.445522 = 0.684422\n$$\n问题要求将答案四舍五入到四位有效数字。\n$$\nL \\approx 0.6844\n$$\n这就是总违规惩罚。",
            "answer": "$$\n\\boxed{0.6844}\n$$"
        },
        {
            "introduction": "最先进的蛋白质结构预测模型，如 AlphaFold，通过直接优化一个评估最终结构质量的指标来进行训练。然而，像 lDDT 这样的标准指标本身是不可微的，无法直接用于基于梯度的优化。这个练习将引导你构建 lDDT 的一个可微“代理”版本，并计算其相对于原子坐标的梯度，从而揭示端到端结构预测模型训练的核心思想。",
            "id": "4554883",
            "problem": "给定两组三维坐标集，分别代表蛋白质中残基的α碳原子（Cα）的预测位置和真实位置。您的任务有两部分：首先，使用标准的基于指示函数的定义，计算每个残基的局部距离差异检验（lDDT-Cα）分数；其次，构建lDDT的可微代理，并执行反向传播，以计算标量损失函数相对于预测坐标的梯度。所有距离都以埃（ångström，缩写为 Å）为单位测量，最终的lDDT分数是无量纲的，位于区间 $[0,1]$ 内。\n\n基本定义和基础：\n- 局部距离差异检验（lDDT）是一种基于接触的度量，用于评估局部几何结构的一致性。对于每个残基 $i$，定义其邻居集合 $$\\mathcal{N}_i = \\{ j \\in \\{1,\\dots,N\\} \\setminus \\{i\\} \\mid d^{\\mathrm{true}}_{ij} \\le R \\},$$ 其中 $d^{\\mathrm{true}}_{ij}$ 是残基 $i$ 和 $j$ 的真实Cα坐标之间的欧几里得距离，而 $R$ 是一个以 Å 为单位的固定邻居半径。每个残基的lDDT分数是其邻居中预测距离误差落在指定阈值内的比例。给定阈值集合 $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$（单位均为 Å），将基于硬指示函数的每个残基的lDDT分数定义为 $$\\mathrm{lDDT}_i^{\\mathrm{hard}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} \\mathbf{1}\\left( \\left| d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij} \\right|  t \\right) \\right),  |\\mathcal{N}_i| > 0, \\\\ 0,  |\\mathcal{N}_i| = 0, \\end{cases}$$ 其中 $d^{\\mathrm{pred}}_{ij}$ 是残基 $i$ 和 $j$ 的预测Cα坐标之间的欧几里得距离，$\\mathbf{1}(\\cdot)$ 是指示函数。\n- 可微代理使用平滑函数替换不可微的绝对值和指示函数。令 $$e_{ij} = d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}, \\quad a_{ij} = \\sqrt{\\varepsilon + e_{ij}^2},$$ 其中 $\\varepsilon > 0$ 是一个小的平滑参数。对于每个阈值 $t \\in \\mathcal{T}$，定义软满足分数为 $$s_{ij}(t) = \\sigma\\!\\left( -\\dfrac{a_{ij} - t}{\\beta} \\right),$$ 其中 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ 是逻辑斯谛函数，$\\beta > 0$ 是一个以 Å 为单位的平滑度尺度。平滑的每个残基的lDDT分数为 $$\\mathrm{lDDT}_i^{\\mathrm{soft}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} s_{ij}(t) \\right),  |\\mathcal{N}_i| > 0, \\\\ 0,  |\\mathcal{N}_i| = 0. \\end{cases}$$\n- 定义标量损失 $$\\mathcal{L} = 1 - \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}},$$ 并计算其相对于每个预测坐标向量 $\\mathbf{x}_i^{\\mathrm{pred}} \\in \\mathbb{R}^3$ 的梯度。使用欧几里得范数定义 $$d^{\\mathrm{pred}}_{ij} = \\left\\| \\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}} \\right\\|_2,$$ 并应用链式法则，注意当 $d^{\\mathrm{pred}}_{ij} > 0$ 时，\n$$\\dfrac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} = \\dfrac{\\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}, \\quad \\dfrac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_j^{\\mathrm{pred}}} = \\dfrac{\\mathbf{x}_j^{\\mathrm{pred}} - \\mathbf{x}_i^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}.$$\n当 $d^{\\mathrm{pred}}_{ij} = 0$ 时，将导数设置为零向量以避免除以零。逻辑斯谛函数的导数是 $\\sigma'(z) = \\sigma(z)\\left(1 - \\sigma(z)\\right)$。\n\n您的程序必须实现以下内容：\n1. 计算所有残基 $i$ 的 $\\mathrm{lDDT}_i^{\\mathrm{hard}}$，然后计算平均硬lDDT分数 $$\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}} = \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{hard}}.$$\n2. 计算所有残基 $i$ 的 $\\mathrm{lDDT}_i^{\\mathrm{soft}}$，然后计算平均软lDDT分数 $$\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}} = \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}}.$$\n3. 计算上面定义的标量损失 $\\mathcal{L}$。\n4. 进行反向传播以计算梯度 $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$，其中 $\\mathbf{X}^{\\mathrm{pred}} \\in \\mathbb{R}^{N \\times 3}$ 堆叠了所有预测坐标。报告该梯度的欧几里得范数 $$\\left\\| \\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\right\\|_2 = \\sqrt{ \\sum_{i=1}^N \\left\\| \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} \\right\\|_2^2 }.$$\n\n测试套件：\n使用以下五个测试用例。在所有情况下，距离和阈值的单位均为 Å，不使用角度。输出必须表示为十进制浮点数（输出中不含百分号或其他单位）。\n\n- 用例 1 (完美匹配，典型半径): $N = 5$，真实坐标 $$\\mathbf{X}^{\\mathrm{true}} = \\begin{bmatrix} 0  0  0 \\\\ 3.8  0  0 \\\\ 7.6  0  0 \\\\ 11.4  0  0 \\\\ 15.2  0  0 \\end{bmatrix},$$ 预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}},$$ 邻居半径 $R = 10.0$，阈值 $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$，平滑度 $\\beta = 0.5$，平滑参数 $\\varepsilon = 10^{-6}$。\n- 用例 2 (小扰动): 与用例1相同的 $\\mathbf{X}^{\\mathrm{true}}$、$R$、阈值、$\\beta$、$\\varepsilon$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}} + \\Delta,$$ 其中 $$\\Delta = \\begin{bmatrix} 0.1  -0.2  0.05 \\\\ -0.05  0.15  -0.1 \\\\ 0.2  0.0  0.2 \\\\ -0.1  -0.15  0.0 \\\\ 0.05  0.05  -0.2 \\end{bmatrix}.$$\n- 用例 3 (平移不变性): 与用例1相同的 $\\mathbf{X}^{\\mathrm{true}}$、$R$、阈值、$\\beta$、$\\varepsilon$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}} + \\mathbf{t}, \\quad \\mathbf{t} = \\begin{bmatrix} 10.0  -5.0  3.0 \\end{bmatrix},$$ 应用于每个残基。\n- 用例 4 (无邻居的边界情况): 与用例1相同的 $\\mathbf{X}^{\\mathrm{true}}$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}},$$ 邻居半径 $R = 2.0$，阈值 $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$，平滑度 $\\beta = 0.5$，平滑参数 $\\varepsilon = 10^{-6}$。\n- 用例 5 (全局缩放失真): 与用例1相同的 $\\mathbf{X}^{\\mathrm{true}}$、$R$、阈值、$\\beta$、$\\varepsilon$。预测坐标 $$\\mathbf{X}^{\\mathrm{pred}} = 2.0 \\cdot \\mathbf{X}^{\\mathrm{true}}.$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的结果，格式为逗号分隔的列表的列表，每个内部列表对应一个测试用例，并包含四个浮点数 $[\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}}, \\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}, \\mathcal{L}, \\left\\| \\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\right\\|_2]$。例如，输出应如下所示 $$\\left[ [r_{11}, r_{12}, r_{13}, r_{14}], [r_{21}, r_{22}, r_{23}, r_{24}], [r_{31}, r_{32}, r_{33}, r_{34}], [r_{41}, r_{42}, r_{43}, r_{44}], [r_{51}, r_{52}, r_{53}, r_{54}] \\right],$$ 其中每个 $r_{ij}$ 都是一个十进制浮点数。",
            "solution": "该问题定义明确，具有科学依据，并为获得唯一解提供了所有必要信息。因此，该问题被认定为有效。解决方案要求实现用于蛋白质结构评估的局部距离差异检验（lDDT-Cα）分数、其可微代理，以及从该代理派生的损失函数的梯度。\n\n解决方案分为四个主要步骤：\n1.  计算标准的、基于指示函数的硬lDDT分数。\n2.  使用平滑近似计算可微的软lDDT分数。\n3.  基于软lDDT计算标量损失 $\\mathcal{L}$。\n4.  推导并计算损失函数 $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$ 相对于预测坐标的梯度。\n\n此问题的核心在于仔细应用向量微积分，特别是链式法则，来计算用于反向传播的梯度。所有计算都在提供的测试用例上执行。\n\n### 1. 准备工作：成对距离和邻居集合\n\n两种lDDT变体的基础都是成对欧几里得距离集合。给定真实坐标 $\\mathbf{X}^{\\mathrm{true}} \\in \\mathbb{R}^{N \\times 3}$ 和预测坐标 $\\mathbf{X}^{\\mathrm{pred}} \\in \\mathbb{R}^{N \\times 3}$，我们计算两个距离矩阵 $D^{\\mathrm{true}}$ 和 $D^{\\mathrm{pred}}$，其中 $D_{ij} = \\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2$。\n\n对于每个残基 $i$，邻居集合 $\\mathcal{N}_i$ 是根据真实距离和固定半径 $R$ 确定的：\n$$\n\\mathcal{N}_i = \\{ j \\in \\{1,\\dots,N\\} \\setminus \\{i\\} \\mid d^{\\mathrm{true}}_{ij} \\le R \\}\n$$\n该集合的大小 $|\\mathcal{N}_i|$ 用于归一化。\n\n### 2. 硬lDDT分数（$\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}}$）\n\n残基 $i$ 的硬lDDT分数 $\\mathrm{lDDT}_i^{\\mathrm{hard}}$，是指在特定误差阈值内得以保留的局部原子间距离的比例。一对 $(i, j)$ 的距离误差是 $|d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}|$。对于每个邻居 $j \\in \\mathcal{N}_i$，我们计算满足了多少个阈值 $t \\in \\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$，即误差小于 $t$ 的情况。这由指示函数 $\\mathbf{1}(\\cdot)$ 正式表示。\n\n公式为：\n$$\n\\mathrm{lDDT}_i^{\\mathrm{hard}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} \\mathbf{1}\\left( \\left| d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij} \\right|  t \\right) \\right),  |\\mathcal{N}_i|  0 \\\\ 0,  |\\mathcal{N}_i| = 0 \\end{cases}\n$$\n平均硬lDDT分数是所有残基的平均值：$\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{hard}}$。\n\n### 3. 软lDDT分数（$\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}$）和损失（$\\mathcal{L}$）\n\n为了可微性，非平滑的绝对值和指示函数被平滑代理所取代。距离误差为 $e_{ij} = d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}$。\n- 绝对值 $|e_{ij}|$ 被替换为 $a_{ij} = \\sqrt{\\varepsilon + e_{ij}^2}$，其中 $\\varepsilon$ 是一个小的正常数，以防止导数在 $e_{ij}=0$ 时无定义。\n- 指示函数 $\\mathbf{1}(|e_{ij}|  t)$，等价于 $\\mathbf{1}(t - |e_{ij}|  0)$，被逻辑斯谛函数 $\\sigma(z) = (1 + e^{-z})^{-1}$ 所取代。对于阈值 $t$ 的软满足分数是 $s_{ij}(t) = \\sigma\\left( \\frac{t - a_{ij}}{\\beta} \\right) = \\sigma\\left( -\\frac{a_{ij} - t}{\\beta} \\right)$，其中 $\\beta$ 控制过渡的“平滑度”。\n\n那么，残基 $i$ 的软lDDT分数为：\n$$\n\\mathrm{lDDT}_i^{\\mathrm{soft}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} s_{ij}(t) \\right),  |\\mathcal{N}_i|  0 \\\\ 0,  |\\mathcal{N}_i| = 0 \\end{cases}\n$$\n平均软lDDT分数是 $\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}}$。标量损失定义为 $\\mathcal{L} = 1 - \\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}$。\n\n### 4. 梯度计算（$\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$）\n\n为了计算损失 $\\mathcal{L}$ 相对于预测坐标 $\\mathbf{X}^{\\mathrm{pred}}$ 的梯度，我们系统地应用链式法则。单个坐标向量 $\\mathbf{x}_k^{\\mathrm{pred}}$ 的梯度为 $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}}$。\n\n损失 $\\mathcal{L}$ 依赖于每个 $\\mathrm{lDDT}_i^{\\mathrm{soft}}$，而后者又依赖于距离 $d^{\\mathrm{pred}}_{ij}$。每个距离 $d^{\\mathrm{pred}}_{ij}$ 依赖于坐标 $\\mathbf{x}_i^{\\mathrm{pred}}$ 和 $\\mathbf{x}_j^{\\mathrm{pred}}$。最直接的方法是找到损失函数相对于每个成对距离 $d^{\\mathrm{pred}}_{ij}$ 的导数，然后将此导数反向传播到坐标。\n\n$\\mathcal{L}$ 相对于单个距离 $d^{\\mathrm{pred}}_{ij}$（当 $i \\neq j$）的导数，从 $\\mathrm{lDDT}_i^{\\mathrm{soft}}$（其中 $d^{\\mathrm{pred}}_{ij}$ 出现在对邻居的求和中）和 $\\mathrm{lDDT}_j^{\\mathrm{soft}}$（其中 $d^{\\mathrm{pred}}_{ji} = d^{\\mathrm{pred}}_{ij}$ 出现）获得贡献。只有当 $j \\in \\mathcal{N}_i$（这意味着 $i \\in \\mathcal{N}_j$）时，这才是非零的。\n\n我们来追踪导数：\n1.  $\\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}} = -\\dfrac{1}{N}$\n2.  $\\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial s_{ij}(t)} = \\dfrac{1}{|\\mathcal{N}_i| |\\mathcal{T}|}$ (如果 $|\\mathcal{N}_i|0$，否则为 $0$)\n3.  $\\dfrac{\\partial s_{ij}(t)}{\\partial a_{ij}} = \\sigma'\\!\\left(-\\dfrac{a_{ij}-t}{\\beta}\\right) \\cdot \\left(-\\dfrac{1}{\\beta}\\right)$, 其中 $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$。\n4.  $\\dfrac{\\partial a_{ij}}{\\partial e_{ij}} = \\dfrac{e_{ij}}{\\sqrt{\\varepsilon + e_{ij}^2}} = \\dfrac{e_{ij}}{a_{ij}}$\n5.  $\\dfrac{\\partial e_{ij}}{\\partial d^{\\mathrm{pred}}_{ij}} = 1$\n\n结合这些，$\\mathcal{L}$ 相对于 $d^{\\mathrm{pred}}_{ij}$ 的导数是：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}} \\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}} + \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_j^{\\mathrm{soft}}} \\dfrac{\\partial \\mathrm{lDDT}_j^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}}\n$$\n其中\n$$\n\\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{1}{|\\mathcal{N}_i||\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\dfrac{\\partial s_{ij}(t)}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{1}{|\\mathcal{N}_i||\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\left( \\sigma'\\!\\left(-\\tfrac{a_{ij}-t}{\\beta}\\right) \\left(-\\tfrac{1}{\\beta}\\right) \\tfrac{e_{ij}}{a_{ij}} \\right)\n$$\n令 $S'_{ij} = \\sum_{t \\in \\mathcal{T}} \\sigma'\\!\\left(-\\frac{a_{ij}-t}{\\beta}\\right)$。\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\left(-\\dfrac{1}{N}\\right) \\left[ \\left(-\\dfrac{e_{ij}}{a_{ij} |\\mathcal{N}_i| |\\mathcal{T}| \\beta} S'_{ij}\\right) + \\left(-\\dfrac{e_{ji}}{a_{ji} |\\mathcal{N}_j| |\\mathcal{T}| \\beta} S'_{ji}\\right) \\right]\n$$\n因为 $e_{ij}=e_{ji}$，$a_{ij}=a_{ji}$，且 $S'_{ij}=S'_{ji}$，这可以简化为：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{e_{ij} S'_{ij}}{N a_{ij} |\\mathcal{T}| \\beta} \\left( \\dfrac{1}{|\\mathcal{N}_i|} + \\dfrac{1}{|\\mathcal{N}_j|} \\right)\n$$\n对于每个满足 $j \\in \\mathcal{N}_i$ 的对 $(i, j)$，都会计算这个标量导数。如果邻居集合为空，则包含 $1/|\\mathcal{N}|$ 的项设置为零。\n\n最后，我们使用 $\\frac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} = \\frac{\\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}$ 将其传播到坐标（如果 $d^{\\mathrm{pred}}_{ij}0$，否则为 $\\mathbf{0}$）。\n坐标 $\\mathbf{x}_k^{\\mathrm{pred}}$ 的梯度是所有涉及它的距离贡献的累加：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}} = \\sum_{j \\neq k, j \\in \\mathcal{N}_k} \\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{kj}} \\dfrac{\\partial d^{\\mathrm{pred}}_{kj}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}} = \\sum_{j \\in \\mathcal{N}_k} \\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{kj}} \\left( \\dfrac{\\mathbf{x}_k^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{kj}} \\right)\n$$\n完整的梯度矩阵 $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\in \\mathbb{R}^{N \\times 3}$ 是通过为每个 $k \\in \\{1, \\dots, N\\}$ 计算该向量来组合的。最终报告的值是其欧几里得（弗罗贝尼乌斯）范数：$\\|\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}\\|_2$。\n\n这个完整的分析框架允许为给定的测试用例计算所有必需的量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the lDDT problem for a suite of test cases.\n    \"\"\"\n    \n    # Test case definitions\n    X_true_base = np.array([\n        [0.0, 0.0, 0.0],\n        [3.8, 0.0, 0.0],\n        [7.6, 0.0, 0.0],\n        [11.4, 0.0, 0.0],\n        [15.2, 0.0, 0.0]\n    ])\n\n    delta = np.array([\n        [0.1, -0.2, 0.05],\n        [-0.05, 0.15, -0.1],\n        [0.2, 0.0, 0.2],\n        [-0.1, -0.15, 0.0],\n        [0.05, 0.05, -0.2]\n    ])\n    \n    t_vec = np.array([10.0, -5.0, 3.0])\n\n    test_cases = [\n        # Case 1: Perfect match\n        (X_true_base, X_true_base.copy(), 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 2: Small perturbation\n        (X_true_base, X_true_base + delta, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 3: Translation invariance\n        (X_true_base, X_true_base + t_vec, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 4: No neighbors edge case\n        (X_true_base, X_true_base.copy(), 2.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 5: Global scaling distortion\n        (X_true_base, 2.0 * X_true_base, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6)\n    ]\n\n    results = []\n    for params in test_cases:\n        case_result = _calculate_all_metrics(*params)\n        results.append(case_result)\n\n    # Format output exactly as specified\n    inner_list_strs = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_str = f\"[{','.join(inner_list_strs)}]\"\n    print(final_str)\n\ndef _calculate_all_metrics(X_true, X_pred, R, T_set, beta, eps):\n    \"\"\"\n    Computes all required metrics for a single test case.\n    \"\"\"\n    N = X_true.shape[0]\n    T = np.array(sorted(list(T_set)))\n    num_T = len(T)\n\n    # Pairwise distances\n    diff_true = X_true[:, np.newaxis, :] - X_true[np.newaxis, :, :]\n    d_true = np.sqrt(np.sum(diff_true**2, axis=-1))\n    \n    diff_pred = X_pred[:, np.newaxis, :] - X_pred[np.newaxis, :, :]\n    d_pred = np.sqrt(np.sum(diff_pred**2, axis=-1))\n\n    # Neighbor sets\n    neighbor_mask = (d_true = R)  (d_true > 0)\n    N_i_sizes = np.sum(neighbor_mask, axis=1)\n\n    # 1. Hard lDDT\n    dist_err = np.abs(d_pred - d_true)\n    lddt_hard_per_res = np.zeros(N)\n    for i in range(N):\n        if N_i_sizes[i] > 0:\n            neighbors_j = np.where(neighbor_mask[i])[0]\n            score_sum = 0\n            for j in neighbors_j:\n                satisfied_count = np.sum(dist_err[i, j]  T)\n                score_sum += satisfied_count / num_T\n            lddt_hard_per_res[i] = score_sum / N_i_sizes[i]\n    mean_lddt_hard = np.mean(lddt_hard_per_res)\n\n    # 2. Soft lDDT\n    e_ij = d_pred - d_true\n    a_ij = np.sqrt(eps + e_ij**2)\n    \n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    lddt_soft_per_res = np.zeros(N)\n    s_ij_t_sum = np.zeros((N, N))\n    for t in T:\n        s_ij_t = sigma(-(a_ij - t) / beta)\n        s_ij_t_sum += s_ij_t\n\n    for i in range(N):\n        if N_i_sizes[i] > 0:\n            neighbors_j = np.where(neighbor_mask[i])[0]\n            per_neighbor_scores = (1/num_T) * s_ij_t_sum[i, neighbors_j]\n            lddt_soft_per_res[i] = np.sum(per_neighbor_scores) / N_i_sizes[i]\n\n    mean_lddt_soft = np.mean(lddt_soft_per_res)\n\n    # 3. Loss\n    loss = 1.0 - mean_lddt_soft\n\n    # 4. Gradient\n    grad = np.zeros_like(X_pred)\n\n    def sigma_prime(z):\n        s = sigma(z)\n        return s * (1.0 - s)\n        \n    # Sum of sigma' over thresholds\n    S_prime_ij = np.zeros((N, N))\n    for t in T:\n        z = -(a_ij - t) / beta\n        S_prime_ij += sigma_prime(z)\n        \n    # Pre-calculate factors to avoid recomputing in loop\n    inv_Ni_sizes = np.zeros(N)\n    inv_Ni_sizes[N_i_sizes > 0] = 1.0 / N_i_sizes[N_i_sizes > 0]\n\n    # Iterate over unique pairs (i, j)\n    for i in range(N):\n        for j in range(i + 1, N):\n            if neighbor_mask[i, j]:\n                # Scalar derivative part\n                # dL/dd_pred_ij\n                if a_ij[i, j] > 1e-9: # Avoid division by zero\n                    common_factor = (e_ij[i, j] * S_prime_ij[i, j]) / (N * a_ij[i, j] * num_T * beta)\n                else:\n                    common_factor = 0.0\n\n                dL_dd = common_factor * (inv_Ni_sizes[i] + inv_Ni_sizes[j])\n                \n                # Vector part\n                # dd_pred_ij/dx_i\n                if d_pred[i, j] > 1e-9: # Avoid division by zero\n                    unit_vec = diff_pred[i, j, :] / d_pred[i, j]\n                else:\n                    unit_vec = np.zeros(3)\n\n                # Accumulate gradients\n                grad[i, :] += dL_dd * unit_vec\n                grad[j, :] -= dL_dd * unit_vec\n    \n    grad_norm = np.linalg.norm(grad)\n\n    return [mean_lddt_hard, mean_lddt_soft, loss, grad_norm]\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "一个预测出的蛋白质结构，如果缺少对其准确性的评估，其实用性将大打折扣。现代预测模型会为每个残基提供一个置信度分数（如 pLDDT），以评估局部结构预测的可靠性。本练习将带你深入了解 pLDDT 这类置信度分数是如何从模型内部的预测误差分布中计算出来的，并引入模型校准的关键概念。通过计算预期校准误差（ECE），你将学会如何定量评估模型置信度预测的可靠性，这是批判性地使用和评估任何预测模型都不可或缺的技能。",
            "id": "4554904",
            "problem": "给定来自一个深度学习蛋白质结构预测模型的逐残基预测误差分布。每个残基的预测是一个离散的概率质量函数，表示为在绝对距离误差大小上的固定区间中点。您需要计算一个类似于预测局部距离差异检验 (pLDDT) 的逐残基置信度分数，然后通过预期校准误差 (ECE) 来评估其校准情况。\n\n定义和基本原理：\n- 一个残基的局部距离差异检验 (LDDT) 分数定义为在指定的容忍度阈值内的局部成对距离所占的比例。我们将使用此比例的概率期望作为置信度的代理指标。设容忍度阈值集合为 $\\{\\tau_k\\}_{k=1}^K$（单位为埃，记作 Å）。\n- 对于给定的残基，令随机变量 $E$ 表示绝对距离误差的大小（单位为 Å）。对于每个阈值 $\\tau_k$，如果误差低于该阈值，指示函数 $\\mathbf{1}(E  \\tau_k)$ 等于 $1$，否则为 $0$。\n- 逐残基的类 pLDDT 置信度定义为满足阈值的预期比例：$C = 100 \\times \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{E}\\left[\\mathbf{1}(E  \\tau_k)\\right]$。这将产生一个在 $[0,100]$ 范围内的值。\n- 校准是通过比较预测置信度与经验准确率来评估的。令 $c \\in [0,1]$ 为归一化置信度 $C/100$，令 $t \\in [0,1]$ 为根据真实误差 $e$ 计算的经验归一化 LDDT 分数，即 $t = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{1}(e  \\tau_k)$。\n- 使用 $M$ 个区间的预期校准误差 (ECE) 将 $[0,1]$ 划分成 $M$ 个不相交的区间。对于区间 $m$，令 $B_m$ 为预测置信度 $c$ 落入该区间的残基集合。令 $\\bar{c}_m$ 为 $B_m$ 上 $c$ 的均值，$\\bar{t}_m$ 为 $B_m$ 上 $t$ 的均值。设残基总数为 $N$，则 ECE 为 $$\\mathrm{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{N} \\left| \\bar{c}_m - \\bar{t}_m \\right|.$$ 如果 $B_m$ 为空，其贡献为零。\n\n离散误差分布的近似：\n- 每个残基的预测分布由区间中点 $\\{m_i\\}_{i=1}^{I}$（单位为 Å）和概率 $\\{p_i\\}_{i=1}^{I}$ 指定，且 $\\sum_{i=1}^{I} p_i = 1$。我们通过对中点低于阈值的区间的概率求和来近似 $P(E  \\tau_k)$：$$P(E  \\tau_k) \\approx \\sum_{i=1}^{I} p_i \\, \\mathbf{1}(m_i  \\tau_k)。$$\n\n您的任务：\n- 对于每个测试用例，通过上述定义和离散近似计算每个残基的 $C$ 和归一化值 $c = C/100$。然后，使用真实误差 $e$ 和给定的阈值计算每个残基的 $t$。最后，使用在 $[0,1]$ 上的 $M$ 个等宽区间计算 ECE。\n\n单位和输出：\n- 所有距离值和阈值都必须以埃（Å）为单位处理。\n- 每个测试用例的最终输出是一个等于 ECE 的浮点数，四舍五入到六位小数（十进制格式，无百分号）。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，\"[result1,result2,result3]\"）。\n\n测试套件：\n使用以下参数集。在每种情况下，$K=4$ 个阈值 $\\tau = [0.5, 1.0, 2.0, 4.0]$ Å，$I=6$ 个区间中点 $m = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]$ Å，以及 $M=10$ 个校准区间。\n\n- 情况 1 (一般情况，接近校准):\n    - 残基数：$5$\n    - 每个残基的预测概率 (每个列表总和为 $1$):\n        - 残基 1: $[0.35, 0.25, 0.20, 0.10, 0.06, 0.04]$\n        - 残基 2: $[0.15, 0.20, 0.25, 0.20, 0.10, 0.10]$\n        - 残基 3: $[0.05, 0.10, 0.15, 0.25, 0.25, 0.20]$\n        - 残基 4: $[0.40, 0.30, 0.15, 0.10, 0.04, 0.01]$\n        - 残基 5: $[0.10, 0.15, 0.25, 0.25, 0.15, 0.10]$\n    - 真实误差 (Å): $[0.6, 1.2, 3.5, 0.4, 2.5]$\n\n- 情况 2 (过度自信，校准不良):\n    - 残基数：$4$\n    - 每个残基的预测概率 (所有残基相同): $[0.60, 0.20, 0.10, 0.05, 0.03, 0.02]$\n    - 真实误差 (Å): $[5.5, 6.5, 8.0, 7.0]$\n\n- 情况 3 (均匀预测，混合真实误差):\n    - 残基数：$6$\n    - 每个残基的预测概率 (所有残基相同): $[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$\n    - 真实误差 (Å): $[0.1, 0.8, 1.9, 2.1, 3.9, 10.0]$\n\n最终输出规范：\n- 您的程序应计算三种情况中每一种的 ECE，并在一行中打印出用方括号括起来的逗号分隔列表。每个值必须四舍五入到六位小数，并表示为十进制数（例如，$0.123456$）。输出必须具有确切的格式：$[x_1,x_2,x_3]$，其中 $x_j$ 是情况 $j$ 的 ECE。",
            "solution": "该问题陈述已被验证为科学上合理、定义明确且完整。它基于机器学习模型评估的既定原则，特别是在蛋白质结构预测的背景下，提出了一个清晰、可形式化的任务。解决方案的方法论直接源自所提供的定义。计算过程包括三个主要阶段：确定每个残基的预测置信度，计算每个残基的经验准确率，以及汇总这些值以计算预期校准误差 (ECE)。\n\n目标是计算模型校准的统计度量，即 ECE，这需要比较模型的预测置信度分数与观察到的正确性频率。该问题基于局部距离差异检验 (LDDT) 框架，为置信度和正确性提供了代理指标。\n\n步骤 1：计算每个残基的预测置信度 ($c_j$)\n对于给定的残基 $j$，预测的类 pLDDT 置信度分数 $C_j$ 定义为满足距离容忍度阈值的经过缩放的期望比例。这 $K$ 个阈值的集合表示为 $\\{\\tau_k\\}_{k=1}^K$。置信度由以下公式给出：\n$$\nC_j = 100 \\times \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{E}\\left[\\mathbf{1}(E_j  \\tau_k)\\right]\n$$\n其中 $E_j$ 是残基 $j$ 的绝对误差的随机变量。指示函数的期望 $\\mathbb{E}\\left[\\mathbf{1}(E_j  \\tau_k)\\right]$ 等价于概率 $P(E_j  \\tau_k)$。问题指定使用离散误差分布，该分布由每个残基 $j$ 的 $I$ 个区间中点 $\\{m_i\\}_{i=1}^I$ 及其相关概率 $\\{p_{j,i}\\}_{i=1}^I$ 定义。概率 $P(E_j  \\tau_k)$ 通过对所有中点严格小于阈值 $\\tau_k$ 的区间的概率求和来近似：\n$$\nP(E_j  \\tau_k) \\approx \\sum_{i=1}^{I} p_{j,i} \\, \\mathbf{1}(m_i  \\tau_k)\n$$\nECE 计算所需的归一化置信度 $c_j \\in [0, 1]$ 由 $c_j = C_j / 100$ 给出。因此，对于每个残基 $j$，我们计算：\n$$\nc_j = \\frac{1}{K} \\sum_{k=1}^{K} \\left( \\sum_{i=1}^{I} p_{j,i} \\, \\mathbf{1}(m_i  \\tau_k) \\right)\n$$\n\n步骤 2：计算每个残基的经验准确率 ($t_j$)\n残基 $j$ 的经验准确率，记作 $t_j$，作为与预测置信度 $c_j$ 进行比较的基准真相。它被定义为使用已知的真实误差 $e_j$ 计算出的满足阈值的实际比例。\n$$\nt_j = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{1}(e_j  \\tau_k)\n$$\n这个计算是确定性的：对于每个残基 $j$，我们计算出大于其真实误差 $e_j$ 的阈值 $\\tau_k$ 的数量，然后将该数量除以阈值的总数 $K$。\n\n步骤 3：计算预期校准误差 (ECE)\nECE 量化了预测置信度与经验准确率之间的差异。该过程涉及将置信度范围 $[0, 1]$ 划分为 $M$ 个不相交的等宽区间。对于此问题，$M=10$，因此区间为 $[0, 0.1), [0.1, 0.2), \\dots, [0.9, 1.0]$。\n$1$. $N$ 个残基中的每一个都根据其预测置信度值 $c_j$ 被分配到一个区间 $B_m$。\n$2$. 对于每个非空区间 $B_m$，我们计算该区间内残基的平均置信度 $\\bar{c}_m$ 和平均经验准确率 $\\bar{t}_m$。\n   $$\n   \\bar{c}_m = \\frac{1}{|B_m|} \\sum_{j \\in B_m} c_j\n   $$\n   $$\n   \\bar{t}_m = \\frac{1}{|B_m|} \\sum_{j \\in B_m} t_j\n   $$\n$3$. ECE 是所有区间上平均置信度和平均准确率之间绝对差的加权平均值。每个区间的权重是其大小比例 $|B_m|/N$。\n   $$\n   \\mathrm{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{N} \\left| \\bar{c}_m - \\bar{t}_m \\right|\n   $$\n如果一个区间 $B_m$ 是空的，它对总和的贡献为 $0$。\n\n此过程使用指定的参数系统地应用于三个测试案例中的每一个：$K=4$ 个阈值 $\\tau = [0.5, 1.0, 2.0, 4.0]$ Å，$I=6$ 个区间中点 $m = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]$ Å，以及 $M=10$ 个校准区间。每个案例的最终结果是计算出的 ECE，四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_ece(probs, true_errors, thresholds, bin_midpoints, num_ece_bins):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE) for a set of residues.\n\n    Args:\n        probs (list of list of float): Predicted probabilities for each residue.\n        true_errors (list of float): True errors for each residue.\n        thresholds (list of float): Tolerance thresholds for LDDT score.\n        bin_midpoints (list of float): Midpoints of the error distribution bins.\n        num_ece_bins (int): Number of bins for ECE calculation.\n\n    Returns:\n        float: The computed Expected Calibration Error.\n    \"\"\"\n    num_residues = len(true_errors)\n    num_thresholds = len(thresholds)\n\n    # Convert to numpy arrays for efficient computation\n    np_probs = np.array(probs)\n    np_true_errors = np.array(true_errors)\n    np_thresholds = np.array(thresholds)\n    np_bin_midpoints = np.array(bin_midpoints)\n\n    predicted_confidences = []\n    empirical_accuracies = []\n\n    # Step 1  2: Calculate predicted confidence (c) and empirical accuracy (t) for each residue\n    for j in range(num_residues):\n        p_j = np_probs[j]\n        e_j = np_true_errors[j]\n        \n        # Calculate P(E  tau_k) for each threshold to get expected indicators\n        expected_indicators = []\n        for tau_k in np_thresholds:\n            # Sum probabilities of bins where midpoint is less than the threshold\n            mask = np_bin_midpoints  tau_k\n            prob_less_than_tau = np.sum(p_j[mask])\n            expected_indicators.append(prob_less_than_tau)\n        \n        # Calculate normalized confidence c_j (mean of expected indicators)\n        c_j = np.mean(expected_indicators)\n        predicted_confidences.append(c_j)\n        \n        # Calculate empirical accuracy t_j\n        satisfied_thresholds = np.sum(e_j  np_thresholds)\n        t_j = satisfied_thresholds / num_thresholds\n        empirical_accuracies.append(t_j)\n    \n    np_confidences = np.array(predicted_confidences)\n    np_accuracies = np.array(empirical_accuracies)\n\n    # Step 3: Compute ECE\n    ece = 0.0\n    \n    # Assign each residue to an ECE bin based on its confidence.\n    # Bins are [0, 0.1), [0.1, 0.2), ..., [0.9, 1.0].\n    # A confidence of 1.0 should go into the last bin.\n    bin_indices = np.floor(np_confidences * num_ece_bins)\n    bin_indices = np.clip(bin_indices, 0, num_ece_bins - 1).astype(int)\n\n    for m in range(num_ece_bins):\n        # Find residues in the current bin\n        in_bin_mask = (bin_indices == m)\n        num_in_bin = np.sum(in_bin_mask)\n\n        if num_in_bin > 0:\n            # Get confidences and accuracies for residues in this bin\n            bin_confidences = np_confidences[in_bin_mask]\n            bin_accuracies = np_accuracies[in_bin_mask]\n            \n            # Calculate average confidence and average accuracy in the bin\n            avg_confidence_in_bin = np.mean(bin_confidences)\n            avg_accuracy_in_bin = np.mean(bin_accuracies)\n            \n            # Add weighted difference to ECE\n            weight = num_in_bin / num_residues\n            ece += weight * np.abs(avg_confidence_in_bin - avg_accuracy_in_bin)\n            \n    return ece\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Common parameters\n    thresholds = [0.5, 1.0, 2.0, 4.0]\n    bin_midpoints = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]\n    num_ece_bins = 10\n\n    test_cases = [\n        # Case 1\n        {\n            \"probs\": [\n                [0.35, 0.25, 0.20, 0.10, 0.06, 0.04],\n                [0.15, 0.20, 0.25, 0.20, 0.10, 0.10],\n                [0.05, 0.10, 0.15, 0.25, 0.25, 0.20],\n                [0.40, 0.30, 0.15, 0.10, 0.04, 0.01],\n                [0.10, 0.15, 0.25, 0.25, 0.15, 0.10]\n            ],\n            \"true_errors\": [0.6, 1.2, 3.5, 0.4, 2.5]\n        },\n        # Case 2\n        {\n            \"probs\": [[0.60, 0.20, 0.10, 0.05, 0.03, 0.02]] * 4,\n            \"true_errors\": [5.5, 6.5, 8.0, 7.0]\n        },\n        # Case 3\n        {\n            \"probs\": [[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]] * 6,\n            \"true_errors\": [0.1, 0.8, 1.9, 2.1, 3.9, 10.0]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        ece_result = calculate_ece(\n            probs=case[\"probs\"],\n            true_errors=case[\"true_errors\"],\n            thresholds=thresholds,\n            bin_midpoints=bin_midpoints,\n            num_ece_bins=num_ece_bins\n        )\n        results.append(ece_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}