{
    "hands_on_practices": [
        {
            "introduction": "A successful structure prediction model must generate not only the correct overall fold but also physically plausible local geometry. This practice problem  delves into a core component of the loss functions used in state-of-the-art models: the stereochemical penalty term. By calculating the penalty, $L$, for violations in bond lengths and angles, you will gain a concrete understanding of how deep learning frameworks are trained to respect the fundamental laws of chemistry.",
            "id": "3842251",
            "problem": "In training a differentiable geometric module within a deep learning model for protein structure prediction, violations of covalent geometry are commonly penalized by weighted quadratic terms on bond lengths and bond angles. Consider a single peptide backbone fragment with three heavy atoms labeled $A \\equiv \\mathrm{N}$, $B \\equiv C_{\\alpha}$, and $C \\equiv \\mathrm{C}$. The model predicts Cartesian coordinates (in angstroms) for these atoms as $\\mathbf{r}_{A} = \\left(1.500, 0.000, 0.000\\right)$, $\\mathbf{r}_{B} = \\left(0.000, 0.000, 0.000\\right)$, and $\\mathbf{r}_{C} = \\left(-0.750, 1.299038106, 0.000\\right)$. You are given a set of target covalent geometry values: target bond length for $A\\!-\\!B$ is $1.458$, target bond length for $B\\!-\\!C$ is $1.525$, and target bond angle at $B$ for $\\angle A\\!-\\!B\\!-\\!C$ is $2.000$ radians. The corresponding weights for the quadratic penalties are $100$ for each bond length constraint and $50$ for the bond angle constraint.\n\nUsing only first principles of Euclidean geometry and vector calculus, compute the total violation penalty defined by\n$$\nL \\;=\\; \\sum_{i} w_{i} \\left(d_{i} - d_{i}^{\\ast}\\right)^{2} \\;+\\; \\sum_{j} \\tilde{w}_{j} \\left(\\theta_{j} - \\theta_{j}^{\\ast}\\right)^{2},\n$$\nwhere $d_{i}$ are bond lengths computed from the predicted coordinates, $\\theta_{j}$ are bond angles computed from the predicted coordinates, $d_{i}^{\\ast}$ and $\\theta_{j}^{\\ast}$ are the corresponding target values, and $w_{i}$, $\\tilde{w}_{j}$ are the given weights. Use the standard Euclidean norm and the definition $\\theta = \\arccos\\!\\big( (\\mathbf{u}\\cdot\\mathbf{v})/(\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|) \\big)$ for the angle between nonzero vectors $\\mathbf{u}$ and $\\mathbf{v}$. All angles must be in radians. Express the final value of $L$ as a real number and round your answer to four significant figures. Do not include units in your final answer.",
            "solution": "The problem requires the calculation of a total violation penalty, $L$, for a three-atom peptide backbone fragment. The penalty function is a sum of quadratic terms for deviations in bond lengths and bond angles from their respective target values.\n\nFirst, we must validate the problem statement.\n\n### Step 1: Extract Givens\nThe data provided are as follows:\n- Atom coordinates:\n  - $\\mathbf{r}_{A} = \\left(1.500, 0.000, 0.000\\right)$\n  - $\\mathbf{r}_{B} = \\left(0.000, 0.000, 0.000\\right)$\n  - $\\mathbf{r}_{C} = \\left(-0.750, 1.299038106, 0.000\\right)$\n- Target bond lengths:\n  - $d_{A-B}^{\\ast} = 1.458$\n  - $d_{B-C}^{\\ast} = 1.525$\n- Target bond angle:\n  - $\\theta_{A-B-C}^{\\ast} = 2.000$ radians\n- Penalty weights:\n  - $w_i = 100$ for each bond length\n  - $\\tilde{w}_j = 50$ for the bond angle\n- Penalty function:\n  - $L = \\sum_{i} w_{i} \\left(d_{i} - d_{i}^{\\ast}\\right)^{2} \\;+\\; \\sum_{j} \\tilde{w}_{j} \\left(\\theta_{j} - \\theta_{j}^{\\ast}\\right)^{2}$\n- Angle definition:\n  - $\\theta = \\arccos\\!\\big( (\\mathbf{u}\\cdot\\mathbf{v})/(\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|) \\big)$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it describes a standard energy term used in molecular mechanics force fields and deep learning models for protein structure. The provided coordinates, target values for bond lengths and angles, and penalty weights are all physically realistic for a peptide backbone fragment. The problem is well-posed, providing all necessary information and a clear objective. The language is precise and unambiguous. Therefore, a solution can be derived.\n\n### Step 3: Action\nThe problem is valid. We proceed with the solution.\n\nThe total penalty $L$ is the sum of penalties from two bond lengths ($A-B$ and $B-C$) and one bond angle ($\\angle A-B-C$). The formula can be written explicitly as:\n$$\nL = w_{A-B} (d_{A-B} - d_{A-B}^{\\ast})^2 + w_{B-C} (d_{B-C} - d_{B-C}^{\\ast})^2 + \\tilde{w}_{A-B-C} (\\theta_{A-B-C} - \\theta_{A-B-C}^{\\ast})^2\n$$\nWe will compute each component of this sum in sequence.\n\nFirst, we determine the vectors representing the bonds, originating from the central atom $B$.\nThe vector for the bond $B-A$ is $\\mathbf{u} = \\mathbf{r}_{A} - \\mathbf{r}_{B}$:\n$$\n\\mathbf{u} = (1.500, 0.000, 0.000) - (0.000, 0.000, 0.000) = (1.500, 0.000, 0.000)\n$$\nThe vector for the bond $B-C$ is $\\mathbf{v} = \\mathbf{r}_{C} - \\mathbf{r}_{B}$:\n$$\n\\mathbf{v} = (-0.750, 1.299038106, 0.000) - (0.000, 0.000, 0.000) = (-0.750, 1.299038106, 0.000)\n$$\n\nNext, we compute the predicted bond lengths, $d_{A-B}$ and $d_{B-C}$, which are the Euclidean norms of vectors $\\mathbf{u}$ and $\\mathbf{v}$.\n$$\nd_{A-B} = \\|\\mathbf{u}\\| = \\sqrt{(1.500)^2 + (0.000)^2 + (0.000)^2} = \\sqrt{2.25} = 1.500\n$$\n$$\nd_{B-C} = \\|\\mathbf{v}\\| = \\sqrt{(-0.750)^2 + (1.299038106)^2 + (0.000)^2}\n$$\nWe note that $1.299038106$ is a high-precision value for $0.750 \\times \\sqrt{3}$. Let's verify this. $(0.750 \\times \\sqrt{3})^2 = (0.750)^2 \\times 3 = 0.5625 \\times 3 = 1.6875$. The calculation is:\n$$\nd_{B-C} = \\sqrt{0.5625 + 1.6875} = \\sqrt{2.25} = 1.500\n$$\nSo, the predicted bond lengths are $d_{A-B} = 1.500$ and $d_{B-C} = 1.500$.\n\nNow, we calculate the bond length penalty term, $L_{\\text{length}}$:\n$$\nL_{\\text{length}} = w_{A-B}(d_{A-B} - d_{A-B}^{\\ast})^2 + w_{B-C}(d_{B-C} - d_{B-C}^{\\ast})^2\n$$\nSubstituting the given values ($w_{A-B} = w_{B-C} = 100$, $d_{A-B}^{\\ast} = 1.458$, $d_{B-C}^{\\ast} = 1.525$):\n$$\nL_{\\text{length}} = 100(1.500 - 1.458)^2 + 100(1.500 - 1.525)^2\n$$\n$$\nL_{\\text{length}} = 100(0.042)^2 + 100(-0.025)^2 = 100(0.001764) + 100(0.000625)\n$$\n$$\nL_{\\text{length}} = 0.1764 + 0.0625 = 0.2389\n$$\n\nNext, we compute the predicted bond angle, $\\theta_{A-B-C}$. We use the dot product formula:\n$$\n\\theta_{A-B-C} = \\arccos\\left(\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\right)\n$$\nFirst, the dot product $\\mathbf{u} \\cdot \\mathbf{v}$:\n$$\n\\mathbf{u} \\cdot \\mathbf{v} = (1.500)(-0.750) + (0.000)(1.299038106) + (0.000)(0.000) = -1.125\n$$\nThe product of the norms is $\\|\\mathbf{u}\\| \\|\\mathbf{v}\\| = (1.500)(1.500) = 2.25$.\nThus, the cosine of the angle is:\n$$\n\\cos(\\theta_{A-B-C}) = \\frac{-1.125}{2.25} = -0.5\n$$\nThe angle is therefore:\n$$\n\\theta_{A-B-C} = \\arccos(-0.5) = \\frac{2\\pi}{3} \\text{ radians}\n$$\n\nNow, we calculate the bond angle penalty term, $L_{\\text{angle}}$:\n$$\nL_{\\text{angle}} = \\tilde{w}_{A-B-C}(\\theta_{A-B-C} - \\theta_{A-B-C}^{\\ast})^2\n$$\nSubstituting the given values ($\\tilde{w}_{A-B-C} = 50$, $\\theta_{A-B-C}^{\\ast} = 2.000$):\n$$\nL_{\\text{angle}} = 50\\left(\\frac{2\\pi}{3} - 2.000\\right)^2\n$$\nUsing a numerical approximation for $\\pi \\approx 3.14159265$, we get $\\frac{2\\pi}{3} \\approx 2.0943951$.\n$$\nL_{\\text{angle}} \\approx 50(2.0943951 - 2.000)^2 = 50(0.0943951)^2\n$$\n$$\nL_{\\text{angle}} \\approx 50(0.008910439) \\approx 0.445522\n$$\n\nFinally, we compute the total penalty $L$ by summing the length and angle penalties:\n$$\nL = L_{\\text{length}} + L_{\\text{angle}} \\approx 0.2389 + 0.445522 = 0.684422\n$$\nThe problem asks for the answer to be rounded to four significant figures.\n$$\nL \\approx 0.6844\n$$\nThis is the total violation penalty.",
            "answer": "$$\n\\boxed{0.6844}\n$$"
        },
        {
            "introduction": "While local stereochemistry is vital, the ultimate goal is to predict the correct three-dimensional fold, which requires a loss function that measures overall structural similarity. This exercise  introduces the Local Distance Difference Test (lDDT), a sophisticated metric for structure quality, and then guides you through the critical process of creating a differentiable surrogate. By implementing this \"soft\" lDDT and its gradient, you will see firsthand how an evaluation metric is transformed into a powerful tool for training a neural network.",
            "id": "4554883",
            "problem": "You are given two three-dimensional coordinate sets representing the positions of alpha carbon atoms (Cα) for residues in a protein: a predicted set and a ground-truth set. Your task is twofold: first, compute the Local Distance Difference Test (lDDT-Cα) score per residue using the standard indicator-based definition; second, construct a differentiable surrogate of lDDT and perform backpropagation to compute the gradient of a scalar loss with respect to the predicted coordinates. All distances are measured in ångström, which is abbreviated as Å, and the final lDDT scores are dimensionless in the interval $[0,1]$.\n\nFundamental definitions and base:\n- The Local Distance Difference Test (lDDT) is a contact-based metric that evaluates local geometric agreement. For each residue $i$, define the neighbor set $$\\mathcal{N}_i = \\{ j \\in \\{1,\\dots,N\\} \\setminus \\{i\\} \\mid d^{\\mathrm{true}}_{ij} \\le R \\},$$ where $d^{\\mathrm{true}}_{ij}$ is the Euclidean distance between ground-truth Cα coordinates of residues $i$ and $j$, and $R$ is a fixed neighbor radius in Å. The per-residue lDDT is the fraction of neighbors for which the predicted distance error falls within specified thresholds. Given the set of thresholds $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$ (all in Å), define the hard indicator-based per-residue lDDT score as $$\\mathrm{lDDT}_i^{\\mathrm{hard}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} \\mathbf{1}\\left( \\left| d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij} \\right| < t \\right) \\right), & |\\mathcal{N}_i| > 0, \\\\[8pt] 0, & |\\mathcal{N}_i| = 0, \\end{cases}$$ where $d^{\\mathrm{pred}}_{ij}$ is the Euclidean distance between predicted Cα coordinates of residues $i$ and $j$, and $\\mathbf{1}(\\cdot)$ is the indicator function.\n- The differentiable surrogate replaces the non-differentiable absolute value and indicator with smooth functions. Let $$e_{ij} = d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}, \\quad a_{ij} = \\sqrt{\\varepsilon + e_{ij}^2},$$ with a small smoothing parameter $\\varepsilon > 0$. For each threshold $t \\in \\mathcal{T}$, define the soft satisfaction score $$s_{ij}(t) = \\sigma\\!\\left( -\\dfrac{a_{ij} - t}{\\beta} \\right),$$ where $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ is the logistic function and $\\beta > 0$ is a softness scale in Å. The smooth per-residue lDDT score is $$\\mathrm{lDDT}_i^{\\mathrm{soft}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} s_{ij}(t) \\right), & |\\mathcal{N}_i| > 0, \\\\[8pt] 0, & |\\mathcal{N}_i| = 0. \\end{cases}$$\n- Define the scalar loss $$\\mathcal{L} = 1 - \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}},$$ and compute its gradient with respect to each predicted coordinate vector $\\mathbf{x}_i^{\\mathrm{pred}} \\in \\mathbb{R}^3$. Use the Euclidean norm definition $$d^{\\mathrm{pred}}_{ij} = \\left\\| \\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}} \\right\\|_2,$$ and apply the chain rule, noting that for $d^{\\mathrm{pred}}_{ij} > 0$,\n$$\\dfrac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} = \\dfrac{\\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}, \\quad \\dfrac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_j^{\\mathrm{pred}}} = \\dfrac{\\mathbf{x}_j^{\\mathrm{pred}} - \\mathbf{x}_i^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}.$$\nAt $d^{\\mathrm{pred}}_{ij} = 0$, set the derivatives to the zero vector to avoid division by zero. The logistic derivative is $\\sigma'(z) = \\sigma(z)\\left(1 - \\sigma(z)\\right)$.\n\nYour program must implement the following:\n1. Compute $\\mathrm{lDDT}_i^{\\mathrm{hard}}$ for all residues $i$, and then compute the mean hard lDDT $$\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}} = \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{hard}}.$$\n2. Compute $\\mathrm{lDDT}_i^{\\mathrm{soft}}$ for all residues $i$, and then compute the mean soft lDDT $$\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}} = \\dfrac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}}.$$\n3. Compute the scalar loss $\\mathcal{L}$ defined above.\n4. Backpropagate to compute the gradient $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$, where $\\mathbf{X}^{\\mathrm{pred}} \\in \\mathbb{R}^{N \\times 3}$ stacks all predicted coordinates. Report the Euclidean norm of this gradient $$\\left\\| \\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\right\\|_2 = \\sqrt{ \\sum_{i=1}^N \\left\\| \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} \\right\\|_2^2 }.$$\n\nTest suite:\nUse the following five test cases. In all cases, distances and thresholds are in Å, and angles are not used. The outputs must be expressed as decimal floats (no percentage signs or other units in the output).\n\n- Case $1$ (perfect match, typical radius): $N = 5$, ground-truth coordinates $$\\mathbf{X}^{\\mathrm{true}} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 3.8 & 0 & 0 \\\\ 7.6 & 0 & 0 \\\\ 11.4 & 0 & 0 \\\\ 15.2 & 0 & 0 \\end{bmatrix},$$ predicted coordinates $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}},$$ neighbor radius $R = 10.0$, thresholds $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$, softness $\\beta = 0.5$, smoothing $\\varepsilon = 10^{-6}$.\n- Case $2$ (small perturbation): Same $\\mathbf{X}^{\\mathrm{true}}$ and $R$, thresholds, $\\beta$, $\\varepsilon$ as Case $1$. Predicted coordinates $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}} + \\Delta,$$ where $$\\Delta = \\begin{bmatrix} 0.1 & -0.2 & 0.05 \\\\ -0.05 & 0.15 & -0.1 \\\\ 0.2 & 0.0 & 0.2 \\\\ -0.1 & -0.15 & 0.0 \\\\ 0.05 & 0.05 & -0.2 \\end{bmatrix}.$$\n- Case $3$ (translation invariance): Same $\\mathbf{X}^{\\mathrm{true}}$, $R$, thresholds, $\\beta$, $\\varepsilon$ as Case $1$. Predicted coordinates $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}} + \\mathbf{t}, \\quad \\mathbf{t} = \\begin{bmatrix} 10.0 & -5.0 & 3.0 \\end{bmatrix},$$ applied to every residue.\n- Case $4$ (no neighbors edge case): Same $\\mathbf{X}^{\\mathrm{true}}$ as Case $1$. Predicted coordinates $$\\mathbf{X}^{\\mathrm{pred}} = \\mathbf{X}^{\\mathrm{true}},$$ neighbor radius $R = 2.0$, thresholds $\\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$, softness $\\beta = 0.5$, smoothing $\\varepsilon = 10^{-6}$.\n- Case $5$ (global scaling distortion): Same $\\mathbf{X}^{\\mathrm{true}}$, $R$, thresholds, $\\beta$, $\\varepsilon$ as Case $1$. Predicted coordinates $$\\mathbf{X}^{\\mathrm{pred}} = 2.0 \\cdot \\mathbf{X}^{\\mathrm{true}}.$$\n\nFinal output format:\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list of lists, where each inner list corresponds to one test case and contains four floats $[\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}}, \\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}, \\mathcal{L}, \\left\\| \\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\right\\|_2]$. For example, the output should look like $$\\left[ [r_{11}, r_{12}, r_{13}, r_{14}], [r_{21}, r_{22}, r_{23}, r_{24}], [r_{31}, r_{32}, r_{33}, r_{34}], [r_{41}, r_{42}, r_{43}, r_{44}], [r_{51}, r_{52}, r_{53}, r_{54}] \\right],$$ where each $r_{ij}$ is a decimal float.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. It is therefore deemed valid. The solution requires the implementation of the Local Distance Difference Test (lDDT-Cα) score for protein structure assessment, its differentiable surrogate, and the gradient of a loss function derived from this surrogate.\n\nThe solution proceeds in four main steps:\n1.  Calculation of the standard, indicator-based hard lDDT score.\n2.  Calculation of the differentiable soft lDDT score using smooth approximations.\n3.  Calculation of the scalar loss $\\mathcal{L}$ based on the soft lDDT.\n4.  Derivation and computation of the gradient of the loss, $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$, with respect to the predicted coordinates.\n\nThe core of this problem lies in the careful application of vector calculus, specifically the chain rule, to compute the gradient for backpropagation. All calculations are performed on the provided test cases.\n\n### 1. Preliminaries: Pairwise Distances and Neighbor Sets\n\nThe foundation for both lDDT variants is the set of pairwise Euclidean distances. Given the ground-truth coordinates $\\mathbf{X}^{\\mathrm{true}} \\in \\mathbb{R}^{N \\times 3}$ and predicted coordinates $\\mathbf{X}^{\\mathrm{pred}} \\in \\mathbb{R}^{N \\times 3}$, we compute two distance matrices, $D^{\\mathrm{true}}$ and $D^{\\mathrm{pred}}$, where $D_{ij} = \\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2$.\n\nFor each residue $i$, the neighbor set $\\mathcal{N}_i$ is determined based on the ground-truth distances and a fixed radius $R$:\n$$\n\\mathcal{N}_i = \\{ j \\in \\{1,\\dots,N\\} \\setminus \\{i\\} \\mid d^{\\mathrm{true}}_{ij} \\le R \\}\n$$\nThe size of this set, $|\\mathcal{N}_i|$, is used for normalization.\n\n### 2. Hard lDDT Score ($\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}}$)\n\nThe hard lDDT score for residue $i$, $\\mathrm{lDDT}_i^{\\mathrm{hard}}$, is the fraction of local atomic distances that are preserved within certain error thresholds. The distance error for a pair $(i, j)$ is $|d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}|$. For each neighbor $j \\in \\mathcal{N}_i$, we count how many thresholds $t \\in \\mathcal{T} = \\{0.5, 1.0, 2.0, 4.0\\}$ are met, i.e., for which the error is less than $t$. This is formally captured by the indicator function $\\mathbf{1}(\\cdot)$.\n\nThe formula is:\n$$\n\\mathrm{lDDT}_i^{\\mathrm{hard}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} \\mathbf{1}\\left( \\left| d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij} \\right| < t \\right) \\right), & |\\mathcal{N}_i| > 0 \\\\ 0, & |\\mathcal{N}_i| = 0 \\end{cases}\n$$\nThe mean hard lDDT score is the average over all residues: $\\overline{\\mathrm{lDDT}}^{\\mathrm{hard}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{hard}}$.\n\n### 3. Soft lDDT Score ($\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}$) and Loss ($\\mathcal{L}$)\n\nFor differentiability, the non-smooth absolute value and indicator functions are replaced by smooth surrogates. The distance error is $e_{ij} = d^{\\mathrm{pred}}_{ij} - d^{\\mathrm{true}}_{ij}$.\n- The absolute value $|e_{ij}|$ is replaced by $a_{ij} = \\sqrt{\\varepsilon + e_{ij}^2}$, where $\\varepsilon$ is a small positive constant to prevent the derivative from being undefined at $e_{ij}=0$.\n- The indicator function $\\mathbf{1}(|e_{ij}| < t)$, equivalent to $\\mathbf{1}(t - |e_{ij}| > 0)$, is replaced by the logistic function $\\sigma(z) = (1 + e^{-z})^{-1}$. The soft satisfaction score for threshold $t$ is $s_{ij}(t) = \\sigma\\left( \\frac{t - a_{ij}}{\\beta} \\right) = \\sigma\\left( -\\frac{a_{ij} - t}{\\beta} \\right)$, where $\\beta$ controls the \"softness\" of the transition.\n\nThe soft lDDT score for residue $i$ is then:\n$$\n\\mathrm{lDDT}_i^{\\mathrm{soft}} = \\begin{cases} \\dfrac{1}{|\\mathcal{N}_i|} \\sum\\limits_{j \\in \\mathcal{N}_i} \\left( \\dfrac{1}{|\\mathcal{T}|} \\sum\\limits_{t \\in \\mathcal{T}} s_{ij}(t) \\right), & |\\mathcal{N}_i| > 0 \\\\ 0, & |\\mathcal{N}_i| = 0 \\end{cases}\n$$\nThe mean soft lDDT is $\\overline{\\mathrm{lDDT}}^{\\mathrm{soft}} = \\frac{1}{N} \\sum_{i=1}^N \\mathrm{lDDT}_i^{\\mathrm{soft}}$. The scalar loss is defined as $\\mathcal{L} = 1 - \\overline{\\mathrm{lDDT}}^{\\mathrm{soft}}$.\n\n### 4. Gradient Calculation ($\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}$)\n\nTo compute the gradient of the loss $\\mathcal{L}$ with respect to the predicted coordinates $\\mathbf{X}^{\\mathrm{pred}}$, we apply the chain rule systematically. The gradient for a single coordinate vector $\\mathbf{x}_k^{\\mathrm{pred}}$ is $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}}$.\n\nThe loss $\\mathcal{L}$ depends on each $\\mathrm{lDDT}_i^{\\mathrm{soft}}$, which in turn depends on distances $d^{\\mathrm{pred}}_{ij}$. Each distance $d^{\\mathrm{pred}}_{ij}$ depends on coordinates $\\mathbf{x}_i^{\\mathrm{pred}}$ and $\\mathbf{x}_j^{\\mathrm{pred}}$. The most direct approach is to find the derivative of the loss with respect to each pairwise distance $d^{\\mathrm{pred}}_{ij}$ and then propagate this derivative back to the coordinates.\n\nThe derivative of $\\mathcal{L}$ with respect to a single distance $d^{\\mathrm{pred}}_{ij}$ (for $i \\neq j$) receives contributions from $\\mathrm{lDDT}_i^{\\mathrm{soft}}$ (where $d^{\\mathrm{pred}}_{ij}$ appears in the sum over neighbors) and $\\mathrm{lDDT}_j^{\\mathrm{soft}}$ (where $d^{\\mathrm{pred}}_{ji} = d^{\\mathrm{pred}}_{ij}$ appears). This is only non-zero if $j \\in \\mathcal{N}_i$ (which implies $i \\in \\mathcal{N}_j$).\n\nLet's trace the derivatives:\n1.  $\\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}} = -\\dfrac{1}{N}$\n2.  $\\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial s_{ij}(t)} = \\dfrac{1}{|\\mathcal{N}_i| |\\mathcal{T}|}$ (if $|\\mathcal{N}_i|>0$, else $0$)\n3.  $\\dfrac{\\partial s_{ij}(t)}{\\partial a_{ij}} = \\sigma'\\!\\left(-\\dfrac{a_{ij}-t}{\\beta}\\right) \\cdot \\left(-\\dfrac{1}{\\beta}\\right)$, where $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$.\n4.  $\\dfrac{\\partial a_{ij}}{\\partial e_{ij}} = \\dfrac{e_{ij}}{\\sqrt{\\varepsilon + e_{ij}^2}} = \\dfrac{e_{ij}}{a_{ij}}$\n5.  $\\dfrac{\\partial e_{ij}}{\\partial d^{\\mathrm{pred}}_{ij}} = 1$\n\nCombining these, the derivative of $\\mathcal{L}$ w.r.t $d^{\\mathrm{pred}}_{ij}$ is:\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}} \\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}} + \\dfrac{\\partial \\mathcal{L}}{\\partial \\mathrm{lDDT}_j^{\\mathrm{soft}}} \\dfrac{\\partial \\mathrm{lDDT}_j^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}}\n$$\nwhere\n$$\n\\dfrac{\\partial \\mathrm{lDDT}_i^{\\mathrm{soft}}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{1}{|\\mathcal{N}_i||\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\dfrac{\\partial s_{ij}(t)}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{1}{|\\mathcal{N}_i||\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\left( \\sigma'\\!\\left(-\\tfrac{a_{ij}-t}{\\beta}\\right) \\left(-\\tfrac{1}{\\beta}\\right) \\tfrac{e_{ij}}{a_{ij}} \\right)\n$$\nLet $S'_{ij} = \\sum_{t \\in \\mathcal{T}} \\sigma'\\!\\left(-\\frac{a_{ij}-t}{\\beta}\\right)$.\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\left(-\\dfrac{1}{N}\\right) \\left[ \\left(-\\dfrac{e_{ij}}{a_{ij} |\\mathcal{N}_i| |\\mathcal{T}| \\beta} S'_{ij}\\right) + \\left(-\\dfrac{e_{ji}}{a_{ji} |\\mathcal{N}_j| |\\mathcal{T}| \\beta} S'_{ji}\\right) \\right]\n$$\nSince $e_{ij}=e_{ji}$, $a_{ij}=a_{ji}$, and $S'_{ij}=S'_{ji}$, this simplifies to:\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{ij}} = \\dfrac{e_{ij} S'_{ij}}{N a_{ij} |\\mathcal{T}| \\beta} \\left( \\dfrac{1}{|\\mathcal{N}_i|} + \\dfrac{1}{|\\mathcal{N}_j|} \\right)\n$$\nThis scalar derivative is computed for each pair $(i, j)$ where $j \\in \\mathcal{N}_i$. The terms containing $1/|\\mathcal{N}|$ are set to zero if the neighbor set is empty.\n\nFinally, we propagate this to the coordinates using $\\frac{\\partial d^{\\mathrm{pred}}_{ij}}{\\partial \\mathbf{x}_i^{\\mathrm{pred}}} = \\frac{\\mathbf{x}_i^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{ij}}$ (if $d^{\\mathrm{pred}}_{ij}>0$, else $\\mathbf{0}$).\nThe gradient for coordinate $\\mathbf{x}_k^{\\mathrm{pred}}$ is an accumulation of contributions from all distances involving it:\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}} = \\sum_{j \\neq k, j \\in \\mathcal{N}_k} \\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{kj}} \\dfrac{\\partial d^{\\mathrm{pred}}_{kj}}{\\partial \\mathbf{x}_k^{\\mathrm{pred}}} = \\sum_{j \\in \\mathcal{N}_k} \\dfrac{\\partial \\mathcal{L}}{\\partial d^{\\mathrm{pred}}_{kj}} \\left( \\dfrac{\\mathbf{x}_k^{\\mathrm{pred}} - \\mathbf{x}_j^{\\mathrm{pred}}}{d^{\\mathrm{pred}}_{kj}} \\right)\n$$\nThe full gradient matrix $\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L} \\in \\mathbb{R}^{N \\times 3}$ is assembled by computing this vector for each $k \\in \\{1, \\dots, N\\}$. The final reported value is its Euclidean (Frobenius) norm: $\\|\\nabla_{\\mathbf{X}^{\\mathrm{pred}}} \\mathcal{L}\\|_2$.\n\nThis complete analytical framework allows for the calculation of all required quantities for the given test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the lDDT problem for a suite of test cases.\n    \"\"\"\n    \n    # Test case definitions\n    X_true_base = np.array([\n        [0.0, 0.0, 0.0],\n        [3.8, 0.0, 0.0],\n        [7.6, 0.0, 0.0],\n        [11.4, 0.0, 0.0],\n        [15.2, 0.0, 0.0]\n    ])\n\n    delta = np.array([\n        [0.1, -0.2, 0.05],\n        [-0.05, 0.15, -0.1],\n        [0.2, 0.0, 0.2],\n        [-0.1, -0.15, 0.0],\n        [0.05, 0.05, -0.2]\n    ])\n    \n    t_vec = np.array([10.0, -5.0, 3.0])\n\n    test_cases = [\n        # Case 1: Perfect match\n        (X_true_base, X_true_base.copy(), 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 2: Small perturbation\n        (X_true_base, X_true_base + delta, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 3: Translation invariance\n        (X_true_base, X_true_base + t_vec, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 4: No neighbors edge case\n        (X_true_base, X_true_base.copy(), 2.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6),\n        # Case 5: Global scaling distortion\n        (X_true_base, 2.0 * X_true_base, 10.0, {0.5, 1.0, 2.0, 4.0}, 0.5, 1e-6)\n    ]\n\n    results = []\n    for params in test_cases:\n        case_result = _calculate_all_metrics(*params)\n        results.append(case_result)\n\n    # Format output exactly as specified\n    inner_list_strs = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_str = f\"[{','.join(inner_list_strs)}]\"\n    print(final_str)\n\ndef _calculate_all_metrics(X_true, X_pred, R, T_set, beta, eps):\n    \"\"\"\n    Computes all required metrics for a single test case.\n    \"\"\"\n    N = X_true.shape[0]\n    T = np.array(sorted(list(T_set)))\n    num_T = len(T)\n\n    # Pairwise distances\n    diff_true = X_true[:, np.newaxis, :] - X_true[np.newaxis, :, :]\n    d_true = np.sqrt(np.sum(diff_true**2, axis=-1))\n    \n    diff_pred = X_pred[:, np.newaxis, :] - X_pred[np.newaxis, :, :]\n    d_pred = np.sqrt(np.sum(diff_pred**2, axis=-1))\n\n    # Neighbor sets\n    neighbor_mask = (d_true = R)  (d_true > 0)\n    N_i_sizes = np.sum(neighbor_mask, axis=1)\n\n    # 1. Hard lDDT\n    dist_err = np.abs(d_pred - d_true)\n    lddt_hard_per_res = np.zeros(N)\n    for i in range(N):\n        if N_i_sizes[i] > 0:\n            neighbors_j = np.where(neighbor_mask[i])[0]\n            score_sum = 0\n            for j in neighbors_j:\n                satisfied_count = np.sum(dist_err[i, j]  T)\n                score_sum += satisfied_count / num_T\n            lddt_hard_per_res[i] = score_sum / N_i_sizes[i]\n    mean_lddt_hard = np.mean(lddt_hard_per_res)\n\n    # 2. Soft lDDT\n    e_ij = d_pred - d_true\n    a_ij = np.sqrt(eps + e_ij**2)\n    \n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    lddt_soft_per_res = np.zeros(N)\n    s_ij_t_sum = np.zeros((N, N))\n    for t in T:\n        s_ij_t = sigma(-(a_ij - t) / beta)\n        s_ij_t_sum += s_ij_t\n\n    for i in range(N):\n        if N_i_sizes[i] > 0:\n            neighbors_j = np.where(neighbor_mask[i])[0]\n            per_neighbor_scores = (1/num_T) * s_ij_t_sum[i, neighbors_j]\n            lddt_soft_per_res[i] = np.sum(per_neighbor_scores) / N_i_sizes[i]\n\n    mean_lddt_soft = np.mean(lddt_soft_per_res)\n\n    # 3. Loss\n    loss = 1.0 - mean_lddt_soft\n\n    # 4. Gradient\n    grad = np.zeros_like(X_pred)\n\n    def sigma_prime(z):\n        s = sigma(z)\n        return s * (1.0 - s)\n        \n    # Sum of sigma' over thresholds\n    S_prime_ij = np.zeros((N, N))\n    for t in T:\n        z = -(a_ij - t) / beta\n        S_prime_ij += sigma_prime(z)\n        \n    # Pre-calculate factors to avoid recomputing in loop\n    inv_Ni_sizes = np.zeros(N)\n    inv_Ni_sizes[N_i_sizes > 0] = 1.0 / N_i_sizes[N_i_sizes > 0]\n\n    # Iterate over unique pairs (i, j)\n    for i in range(N):\n        for j in range(i + 1, N):\n            if neighbor_mask[i, j]:\n                # Scalar derivative part\n                # dL/dd_pred_ij\n                if a_ij[i, j] > 1e-9: # Avoid division by zero\n                    common_factor = (e_ij[i, j] * S_prime_ij[i, j]) / (N * a_ij[i, j] * num_T * beta)\n                else:\n                    common_factor = 0.0\n\n                dL_dd = common_factor * (inv_Ni_sizes[i] + inv_Ni_sizes[j])\n                \n                # Vector part\n                # dd_pred_ij/dx_i\n                if d_pred[i, j] > 1e-9: # Avoid division by zero\n                    unit_vec = diff_pred[i, j, :] / d_pred[i, j]\n                else:\n                    unit_vec = np.zeros(3)\n\n                # Accumulate gradients\n                grad[i, :] += dL_dd * unit_vec\n                grad[j, :] -= dL_dd * unit_vec\n    \n    grad_norm = np.linalg.norm(grad)\n\n    return [mean_lddt_hard, mean_lddt_soft, loss, grad_norm]\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "A key innovation in modern structure prediction is the ability of models to estimate the accuracy of their own predictions on a per-residue basis. This practice problem  explores the principles behind AlphaFold's famous pLDDT confidence score by having you compute a similar metric from a predicted error distribution. You will then take the crucial next step of assessing the model's \"self-awareness\" by calculating the Expected Calibration Error (ECE), a powerful technique to determine if a model's confidence scores are truly reliable.",
            "id": "4554904",
            "problem": "You are given per-residue predicted error distributions from a deep learning model for protein structure prediction. Each residue’s prediction is a discrete probability mass function over absolute distance error magnitudes, represented by fixed bin midpoints. You will compute a per-residue confidence score analogous to the Predicted Local Distance Difference Test (pLDDT), and then assess calibration via the Expected Calibration Error (ECE).\n\nDefinitions and fundamental base:\n- The Local Distance Difference Test (LDDT) score for a residue is defined as the fraction of local pairwise distances within specified tolerance thresholds. We will use the probabilistic expectation of this fraction as a confidence proxy. Let the set of tolerance thresholds be $\\{\\tau_k\\}_{k=1}^K$ (in Ångström, denoted Å).\n- For a given residue, let the random variable $E$ denote the absolute distance error magnitude (in Å). For each threshold $\\tau_k$, the indicator $\\mathbf{1}(E  \\tau_k)$ equals $1$ if the error is below the threshold and $0$ otherwise.\n- The per-residue pLDDT-like confidence is defined as the expected fraction of thresholds satisfied: $C = 100 \\times \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{E}\\left[\\mathbf{1}(E  \\tau_k)\\right]$. This yields a value in $[0,100]$.\n- Calibration is assessed by comparing predicted confidences with empirical accuracies. Let $c \\in [0,1]$ be the normalized confidence $C/100$, and let $t \\in [0,1]$ be the empirical normalized LDDT fraction computed from the true error $e$ as $t = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{1}(e  \\tau_k)$.\n- The Expected Calibration Error (ECE) with $M$ bins partitions $[0,1]$ into $M$ disjoint intervals. For bin $m$, let $B_m$ be the set of residues whose predicted confidence $c$ falls into that bin. Let $\\bar{c}_m$ be the mean of $c$ over $B_m$, and $\\bar{t}_m$ be the mean of $t$ over $B_m$. With total residues $N$, the ECE is $$\\mathrm{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{N} \\left| \\bar{c}_m - \\bar{t}_m \\right|.$$ If $B_m$ is empty, its contribution is zero.\n\nApproximation for discrete error distributions:\n- Each residue’s predicted distribution is specified by bin midpoints $\\{m_i\\}_{i=1}^{I}$ (in Å) and probabilities $\\{p_i\\}_{i=1}^{I}$, with $\\sum_{i=1}^{I} p_i = 1$. We approximate $P(E\\tau_k)$ by summing probabilities of bins whose midpoints fall below the threshold: $$P(E  \\tau_k) \\approx \\sum_{i=1}^{I} p_i \\, \\mathbf{1}(m_i  \\tau_k).$$\n\nYour task:\n- For each test case, compute per-residue $C$ and the normalized $c = C/100$ via the definition above using the discrete approximation. Then compute $t$ for each residue using the true error $e$ and the given thresholds. Finally, compute the ECE with $M$ equal-width bins on $[0,1]$.\n\nUnits and output:\n- All distance values and thresholds must be treated in Ångström (Å).\n- The final output for each test case is a single float equal to the ECE, rounded to six decimal places (decimal format, no percentage sign).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nTest suite:\nUse the following parameter sets. In each case, $K=4$ thresholds $\\tau = [0.5, 1.0, 2.0, 4.0]$ Å, $I=6$ bin midpoints $m = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]$ Å, and $M=10$ calibration bins.\n\n- Case 1 (general, near-calibrated):\n    - Residues: $5$\n    - Predicted probabilities per residue (each list sums to $1$):\n        - Residue $1$: $[0.35, 0.25, 0.20, 0.10, 0.06, 0.04]$\n        - Residue $2$: $[0.15, 0.20, 0.25, 0.20, 0.10, 0.10]$\n        - Residue $3$: $[0.05, 0.10, 0.15, 0.25, 0.25, 0.20]$\n        - Residue $4$: $[0.40, 0.30, 0.15, 0.10, 0.04, 0.01]$\n        - Residue $5$: $[0.10, 0.15, 0.25, 0.25, 0.15, 0.10]$\n    - True errors (Å): $[0.6, 1.2, 3.5, 0.4, 2.5]$\n\n- Case 2 (overconfident, miscalibrated):\n    - Residues: $4$\n    - Predicted probabilities per residue (identical across residues): $[0.60, 0.20, 0.10, 0.05, 0.03, 0.02]$\n    - True errors (Å): $[5.5, 6.5, 8.0, 7.0]$\n\n- Case 3 (uniform predictions, mixed true errors):\n    - Residues: $6$\n    - Predicted probabilities per residue (identical across residues): $[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$\n    - True errors (Å): $[0.1, 0.8, 1.9, 2.1, 3.9, 10.0]$\n\nFinal output specification:\n- Your program should compute the ECE for each of the three cases and print a single line with a comma-separated list in brackets. Each value must be rounded to six decimal places and expressed as a decimal (e.g., $0.123456$). The output must have the exact format: $[x_1,x_2,x_3]$ where $x_j$ is the ECE for case $j$.",
            "solution": "The problem statement has been validated as scientifically sound, well-posed, and complete. It presents a clear, formalizable task based on established principles in machine learning model evaluation, specifically within the context of protein structure prediction. The methodology for the solution is derived directly from the provided definitions. The calculation involves three principal stages: determination of per-residue predicted confidence, calculation of per-residue empirical accuracy, and aggregation of these values to compute the Expected Calibration Error (ECE).\n\nThe objective is to compute a statistical measure of model calibration, the ECE, which requires a comparison between the model's predicted confidence scores and the observed frequencies of correctness. The problem provides proxies for both confidence and correctness based on the Local Distance Difference Test (LDDT) framework.\n\nStep 1: Computation of Per-Residue Predicted Confidence ($c_j$)\nFor a given residue $j$, the predicted pLDDT-like confidence score, $C_j$, is defined as the scaled, expected fraction of satisfied distance tolerance thresholds. The set of $K$ thresholds is denoted $\\{\\tau_k\\}_{k=1}^K$. The confidence is given by the formula:\n$$\nC_j = 100 \\times \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{E}\\left[\\mathbf{1}(E_j  \\tau_k)\\right]\n$$\nwhere $E_j$ is the random variable for the absolute error of residue $j$. The expectation of the indicator function, $\\mathbb{E}\\left[\\mathbf{1}(E_j  \\tau_k)\\right]$, is equivalent to the probability $P(E_j  \\tau_k)$. The problem specifies using a discrete error distribution, defined by $I$ bin midpoints $\\{m_i\\}_{i=1}^I$ and their associated probabilities $\\{p_{j,i}\\}_{i=1}^I$ for each residue $j$. The probability $P(E_j  \\tau_k)$ is approximated by summing the probabilities of all bins whose midpoints are strictly less than the threshold $\\tau_k$:\n$$\nP(E_j  \\tau_k) \\approx \\sum_{i=1}^{I} p_{j,i} \\, \\mathbf{1}(m_i  \\tau_k)\n$$\nThe normalized confidence, $c_j \\in [0, 1]$, required for the ECE calculation, is given by $c_j = C_j / 100$. Therefore, for each residue $j$, we compute:\n$$\nc_j = \\frac{1}{K} \\sum_{k=1}^{K} \\left( \\sum_{i=1}^{I} p_{j,i} \\, \\mathbf{1}(m_i  \\tau_k) \\right)\n$$\n\nStep 2: Computation of Per-Residue Empirical Accuracy ($t_j$)\nThe empirical accuracy for a residue $j$, denoted $t_j$, serves as the ground truth against which the predicted confidence $c_j$ is compared. It is defined as the actual fraction of satisfied thresholds, computed using the known true error, $e_j$.\n$$\nt_j = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbf{1}(e_j  \\tau_k)\n$$\nThis calculation is deterministic: for each residue $j$, we count the number of thresholds $\\tau_k$ that are greater than its true error $e_j$ and divide this count by the total number of thresholds, $K$.\n\nStep 3: Computation of Expected Calibration Error (ECE)\nThe ECE quantifies the discrepancy between predicted confidences and empirical accuracies. The process involves partitioning the confidence range $[0, 1]$ into $M$ disjoint, equal-width bins. For this problem, $M=10$, so the bins are $[0, 0.1), [0.1, 0.2), \\dots, [0.9, 1.0]$.\n$1$. Each of the $N$ residues is assigned to a bin $B_m$ based on its predicted confidence value $c_j$.\n$2$. For each non-empty bin $B_m$, we calculate the average confidence, $\\bar{c}_m$, and the average empirical accuracy, $\\bar{t}_m$, of the residues within that bin.\n   $$\n   \\bar{c}_m = \\frac{1}{|B_m|} \\sum_{j \\in B_m} c_j\n   $$\n   $$\n   \\bar{t}_m = \\frac{1}{|B_m|} \\sum_{j \\in B_m} t_j\n   $$\n$3$. The ECE is the weighted average of the absolute difference between the average confidence and average accuracy across all bins. The weight for each bin is its fractional size, $|B_m|/N$.\n   $$\n   \\mathrm{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{N} \\left| \\bar{c}_m - \\bar{t}_m \\right|\n   $$\nIf a bin $B_m$ is empty, its contribution to the sum is $0$.\n\nThis procedure is applied systematically to each of the three test cases using the specified parameters: $K=4$ thresholds $\\tau = [0.5, 1.0, 2.0, 4.0]$ Å, $I=6$ bin midpoints $m = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]$ Å, and $M=10$ calibration bins. The final result for each case is the computed ECE, rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_ece(probs, true_errors, thresholds, bin_midpoints, num_ece_bins):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE) for a set of residues.\n\n    Args:\n        probs (list of list of float): Predicted probabilities for each residue.\n        true_errors (list of float): True errors for each residue.\n        thresholds (list of float): Tolerance thresholds for LDDT score.\n        bin_midpoints (list of float): Midpoints of the error distribution bins.\n        num_ece_bins (int): Number of bins for ECE calculation.\n\n    Returns:\n        float: The computed Expected Calibration Error.\n    \"\"\"\n    num_residues = len(true_errors)\n    num_thresholds = len(thresholds)\n\n    # Convert to numpy arrays for efficient computation\n    np_probs = np.array(probs)\n    np_true_errors = np.array(true_errors)\n    np_thresholds = np.array(thresholds)\n    np_bin_midpoints = np.array(bin_midpoints)\n\n    predicted_confidences = []\n    empirical_accuracies = []\n\n    # Step 1  2: Calculate predicted confidence (c) and empirical accuracy (t) for each residue\n    for j in range(num_residues):\n        p_j = np_probs[j]\n        e_j = np_true_errors[j]\n        \n        # Calculate P(E  tau_k) for each threshold to get expected indicators\n        expected_indicators = []\n        for tau_k in np_thresholds:\n            # Sum probabilities of bins where midpoint is less than the threshold\n            mask = np_bin_midpoints  tau_k\n            prob_less_than_tau = np.sum(p_j[mask])\n            expected_indicators.append(prob_less_than_tau)\n        \n        # Calculate normalized confidence c_j (mean of expected indicators)\n        c_j = np.mean(expected_indicators)\n        predicted_confidences.append(c_j)\n        \n        # Calculate empirical accuracy t_j\n        satisfied_thresholds = np.sum(e_j  np_thresholds)\n        t_j = satisfied_thresholds / num_thresholds\n        empirical_accuracies.append(t_j)\n    \n    np_confidences = np.array(predicted_confidences)\n    np_accuracies = np.array(empirical_accuracies)\n\n    # Step 3: Compute ECE\n    ece = 0.0\n    \n    # Assign each residue to an ECE bin based on its confidence.\n    # Bins are [0, 0.1), [0.1, 0.2), ..., [0.9, 1.0].\n    # A confidence of 1.0 should go into the last bin.\n    bin_indices = np.floor(np_confidences * num_ece_bins)\n    bin_indices = np.clip(bin_indices, 0, num_ece_bins - 1).astype(int)\n\n    for m in range(num_ece_bins):\n        # Find residues in the current bin\n        in_bin_mask = (bin_indices == m)\n        num_in_bin = np.sum(in_bin_mask)\n\n        if num_in_bin > 0:\n            # Get confidences and accuracies for residues in this bin\n            bin_confidences = np_confidences[in_bin_mask]\n            bin_accuracies = np_accuracies[in_bin_mask]\n            \n            # Calculate average confidence and average accuracy in the bin\n            avg_confidence_in_bin = np.mean(bin_confidences)\n            avg_accuracy_in_bin = np.mean(bin_accuracies)\n            \n            # Add weighted difference to ECE\n            weight = num_in_bin / num_residues\n            ece += weight * np.abs(avg_confidence_in_bin - avg_accuracy_in_bin)\n            \n    return ece\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Common parameters\n    thresholds = [0.5, 1.0, 2.0, 4.0]\n    bin_midpoints = [0.25, 0.75, 1.5, 3.0, 6.0, 10.0]\n    num_ece_bins = 10\n\n    test_cases = [\n        # Case 1\n        {\n            \"probs\": [\n                [0.35, 0.25, 0.20, 0.10, 0.06, 0.04],\n                [0.15, 0.20, 0.25, 0.20, 0.10, 0.10],\n                [0.05, 0.10, 0.15, 0.25, 0.25, 0.20],\n                [0.40, 0.30, 0.15, 0.10, 0.04, 0.01],\n                [0.10, 0.15, 0.25, 0.25, 0.15, 0.10]\n            ],\n            \"true_errors\": [0.6, 1.2, 3.5, 0.4, 2.5]\n        },\n        # Case 2\n        {\n            \"probs\": [[0.60, 0.20, 0.10, 0.05, 0.03, 0.02]] * 4,\n            \"true_errors\": [5.5, 6.5, 8.0, 7.0]\n        },\n        # Case 3\n        {\n            \"probs\": [[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]] * 6,\n            \"true_errors\": [0.1, 0.8, 1.9, 2.1, 3.9, 10.0]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        ece_result = calculate_ece(\n            probs=case[\"probs\"],\n            true_errors=case[\"true_errors\"],\n            thresholds=thresholds,\n            bin_midpoints=bin_midpoints,\n            num_ece_bins=num_ece_bins\n        )\n        results.append(ece_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}