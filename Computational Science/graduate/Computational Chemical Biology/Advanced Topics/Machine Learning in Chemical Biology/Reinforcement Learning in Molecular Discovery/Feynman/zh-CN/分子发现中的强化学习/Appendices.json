{
    "hands_on_practices": [
        {
            "introduction": "在将强化学习应用于分子从头设计之前，我们必须首先将分子生成过程构建为一个马尔可夫决策过程（MDP）。这项练习旨在通过一个简化的、基于图编辑的生成模型，帮助您理解和计算在给定策略下生成特定分子的轨迹概率，这是理解策略梯度方法的基础。",
            "id": "3861968",
            "problem": "考虑一个用于强化学习中分子图构建的有限期马尔可夫决策过程（MDP），其中分子从空图开始，通过一系列图编辑操作顺序构建。状态是当前的分子图，动作是图编辑操作，并且给定动作，转移是确定性的（图编辑确定性地更新图）。当执行终止动作时，回合结束，终止后不再发生任何动作。\n\n设有限期为 $T = 4$。动作集为 $\\{a_{A}, a_{B}, a_{\\mathrm{bond}}, a_{\\mathrm{stop}}\\}$，分别对应于添加原子类型 $A$、添加原子类型 $B$、在现有的 $A$ 和 $B$ 原子之间添加一个单键（仅当 $A$ 和 $B$ 都存在且尚未成键时才允许），以及终止。初始状态是空图 $s_{\\emptyset}$。目标分子是成键的二聚体 $A\\text{-}B$，我们定义生成的分子是终止时的终端图。\n\n策略是平稳的，表示为在状态 $s$ 下的可行动作上的 $\\pi(a \\mid s)$。在某个状态下，所有未被列为可行的动作的概率为 $0$。在通往目标分子的路径上可达状态中，可行动作的策略值由以下符号指定：\n- 在空图 $s_{\\emptyset}$ 处：$\\pi(a_{A}\\mid s_{\\emptyset})=\\alpha$，$\\pi(a_{B}\\mid s_{\\emptyset})=\\beta$，以及 $\\pi(a_{\\mathrm{stop}}\\mid s_{\\emptyset})=1-\\alpha-\\beta$，其中 $\\alpha,\\beta \\in (0,1)$ 且 $\\alpha+\\beta \\leq 1$。\n- 在单原子 $A$ 图 $s_{A}$ 处：$\\pi(a_{B}\\mid s_{A})=\\gamma$ 以及 $\\pi(a_{\\mathrm{stop}}\\mid s_{A})=1-\\gamma$，其中 $\\gamma \\in (0,1]$。\n- 在单原子 $B$ 图 $s_{B}$ 处：$\\pi(a_{A}\\mid s_{B})=\\delta$ 以及 $\\pi(a_{\\mathrm{stop}}\\mid s_{B})=1-\\delta$，其中 $\\delta \\in (0,1]$。\n- 在双原子未成键图 $s_{AB}^{\\mathrm{no\\ bond}}$ 处：$\\pi(a_{\\mathrm{bond}}\\mid s_{AB}^{\\mathrm{no\\ bond}})=\\eta$ 以及 $\\pi(a_{\\mathrm{stop}}\\mid s_{AB}^{\\mathrm{no\\ bond}})=1-\\eta$，其中 $\\eta \\in (0,1]$。\n- 在成键二聚体状态 $s_{AB}^{\\mathrm{bonded}}$ 处：$\\pi(a_{\\mathrm{stop}}\\mid s_{AB}^{\\mathrm{bonded}})=1$。\n\n假设跨时间步的动作的随机选择在给定所访问状态的条件下是独立的，并且状态转移是如指定的确定性转移。从时间 $t=1$ 时的 $s_{\\emptyset}$ 开始，以闭式形式计算智能体在期限 $T=4$ 内生成成键二聚体 $A\\text{-}B$ 作为终端分子的总概率（作为 $\\alpha,\\beta,\\gamma,\\delta,\\eta$ 的函数）。\n\n您的最终答案必须是关于 $\\alpha,\\beta,\\gamma,\\delta,\\eta$ 的单个闭式解析表达式。不包含任何单位。无需四舍五入。",
            "solution": "用户希望我遵循严格的验证和求解协议来解决问题。我必须首先验证问题陈述，如果有效，则提供详细的解决方案。\n\n### 第 1 步：提取已知条件\n- **框架**：用于分子图构建的有限期马尔可夫决策过程（MDP）。\n- **期限**：$T = 4$。\n- **初始状态**：时间 $t=1$ 时为 $s_1 = s_{\\emptyset}$（空图）。\n- **状态**：$s_{\\emptyset}$（空），$s_{A}$（单原子 A），$s_{B}$（单原子 B），$s_{AB}^{\\mathrm{no\\ bond}}$（两个未成键的原子 A 和 B），$s_{AB}^{\\mathrm{bonded}}$（成键二聚体 A-B）。\n- **动作**：$a_{A}$（添加原子 A），$a_{B}$（添加原子 B），$a_{\\mathrm{bond}}$（在 A 和 B 之间添加键），$a_{\\mathrm{stop}}$（终止）。\n- **转移**：确定性的。在状态 $s$ 下的动作 $a$ 会导致唯一的下一状态 $s'$。\n- **终止**：执行动作 $a_{\\mathrm{stop}}$ 后回合结束。生成的分子是执行 $a_{\\mathrm{stop}}$ 时所在状态的图。\n- **目标分子**：成键二聚体 $A\\text{-}B$，对应于状态 $s_{AB}^{\\mathrm{bonded}}$。\n- **策略**：针对可行动作的平稳策略 $\\pi(a \\mid s)$。\n  - 在状态 $s_{\\emptyset}$：$\\pi(a_{A}\\mid s_{\\emptyset})=\\alpha$, $\\pi(a_{B}\\mid s_{\\emptyset})=\\beta$, $\\pi(a_{\\mathrm{stop}}\\mid s_{\\emptyset})=1-\\alpha-\\beta$。\n  - 在状态 $s_{A}$：$\\pi(a_{B}\\mid s_{A})=\\gamma$, $\\pi(a_{\\mathrm{stop}}\\mid s_{A})=1-\\gamma$。\n  - 在状态 $s_{B}$：$\\pi(a_{A}\\mid s_{B})=\\delta$, $\\pi(a_{\\mathrm{stop}}\\mid s_{B})=1-\\delta$。\n  - 在状态 $s_{AB}^{\\mathrm{no\\ bond}}$：$\\pi(a_{\\mathrm{bond}}\\mid s_{AB}^{\\mathrm{no\\ bond}})=\\eta$, $\\pi(a_{\\mathrm{stop}}\\mid s_{AB}^{\\mathrm{no\\ bond}})=1-\\eta$。\n  - 在状态 $s_{AB}^{\\mathrm{bonded}}$：$\\pi(a_{\\mathrm{stop}}\\mid s_{AB}^{\\mathrm{bonded}})=1$。\n- **参数约束**：$\\alpha, \\beta \\in (0,1)$, $\\alpha+\\beta \\leq 1$, $\\gamma \\in (0,1]$, $\\delta \\in (0,1]$, $\\eta \\in (0,1]$。\n- **目标**：计算智能体在期限 $T=4$ 内生成成键二聚体 $A\\text{-}B$ 作为终端分子的总概率。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题是在具有有限期的离散状态空间图上进行的明确定义的概率计算。\n- **科学基础**：该问题使用了马尔可夫决策过程这一成熟的数学框架。其应用背景——通过强化学习进行从头分子设计——是计算化学领域一个有效且活跃的研究方向。该模型是一个简化模型，但并未违反任何科学原理。\n- **定义明确**：问题结构清晰。初始状态、动作空间、转移动态和策略都已明确定义。它要求根据给定的符号参数计算一个单一的可计算概率。存在唯一解。\n- **客观性**：问题以精确、正式的语言陈述，没有主观性或偏见。\n- **完整性和一致性**：提供了所有必要信息（概率、状态、动作、期限）。对概率参数（$\\alpha, \\beta, \\gamma, \\delta, \\eta$）的约束与其定义一致。每个状态下可行动作的定义隐含地定义了状态空间图的结构，该结构是自洽的。\n\n### 第 3 步：结论和行动\n问题是有效的。它具有科学基础，定义明确，客观且信息自洽。我将继续进行求解。\n\n目标是计算从 $t=1$ 时的初始状态 $s_{\\emptyset}$ 开始的所有成功轨迹的总概率。如果一条轨迹在时间期限 $T=4$ 内通过生成成键二聚体 $A\\text{-}B$ 而终止，则该轨迹是成功的。\n\n根据问题陈述，“生成的分子是终止时的终端图”。这意味着成功的轨迹必须以在状态 $s_{AB}^{\\mathrm{bonded}}$ 处执行动作 $a_{\\mathrm{stop}}$ 结束。设状态序列为 $s_1, s_2, \\dots, s_k$，动作序列为 $a_1, a_2, \\dots, a_k$。对于长度为 $k$ 的成功轨迹，我们必须有 $s_1=s_{\\emptyset}$，$s_k=s_{AB}^{\\mathrm{bonded}}$，以及 $a_k=a_{\\mathrm{stop}}$。轨迹的总长度 $k$ 必须小于或等于期限，即 $k \\le T=4$。\n\n我们必须首先确定从 $s_{\\emptyset}$ 到达状态 $s_{AB}^{\\mathrm{bonded}}$ 所需的动作序列。给定动作，状态转移是确定性的。让我们追踪到达 $s_{AB}^{\\mathrm{bonded}}$ 的路径：\n1.  要到达 $s_{AB}^{\\mathrm{bonded}}$，智能体必须处于状态 $s_{AB}^{\\mathrm{no\\ bond}}$ 并执行动作 $a_{\\mathrm{bond}}$。\n2.  要到达 $s_{AB}^{\\mathrm{no\\ bond}}$，智能体必须已经添加了一个 $A$ 型原子和一个 $B$ 型原子。这可以按两种不同的顺序发生：\n    - 路径 A：先添加原子 $A$，再添加原子 $B$。状态序列：$s_{\\emptyset} \\rightarrow s_{A} \\rightarrow s_{AB}^{\\mathrm{no\\ bond}}$。这需要两个动作：先 $a_{A}$，后 $a_{B}$。\n    - 路径 B：先添加原子 $B$，再添加原子 $A$。状态序列：$s_{\\emptyset} \\rightarrow s_{B} \\rightarrow s_{AB}^{\\mathrm{no\\ bond}}$。这需要两个动作：先 $a_{B}$，后 $a_{A}$。\n\n综合这些步骤，有两条到达状态 $s_{AB}^{\\mathrm{bonded}}$ 的最小动作序列：\n- 序列 1：$(a_{A}, a_{B}, a_{\\mathrm{bond}})$。这 3 个动作的序列将状态转换如下：$s_{\\emptyset} \\xrightarrow{a_A} s_A \\xrightarrow{a_B} s_{AB}^{\\mathrm{no\\ bond}} \\xrightarrow{a_{\\mathrm{bond}}} s_{AB}^{\\mathrm{bonded}}$。\n- 序列 2：$(a_{B}, a_{A}, a_{\\mathrm{bond}})$。这 3 个动作的序列将状态转换如下：$s_{\\emptyset} \\xrightarrow{a_B} s_B \\xrightarrow{a_A} s_{AB}^{\\mathrm{no\\ bond}} \\xrightarrow{a_{\\mathrm{bond}}} s_{AB}^{\\mathrm{bonded}}$。\n\n在这两种情况下，到达目标状态 $s_{AB}^{\\mathrm{bonded}}$ 都需要恰好 3 个动作。这些动作将在时间 $t=1, 2, 3$ 执行。在时间步 $t=4$ 开始时，系统处于状态 $s_{AB}^{\\mathrm{bonded}}$。\n\n为了使轨迹成功，智能体必须接着在状态 $s_{AB}^{\\mathrm{bonded}}$ 执行动作 $a_{\\mathrm{stop}}$。这将是第四个动作，在 $t=4$ 执行。这样一条成功轨迹的总长度为 $k=4$。由于期限为 $T=4$，这些是可能的最长成功轨迹。任何更早终止的轨迹（在 $t=1, 2,$ 或 $3$）都将未达到状态 $s_{AB}^{\\mathrm{bonded}}$，因此是不成功的。例如，轨迹 $(a_A, a_B, a_{\\mathrm{stop}})$ 在 $t=3$ 终止，其终端分子是 $s_{AB}^{\\mathrm{no\\ bond}}$。\n\n因此，我们只需要考虑长度恰好为 4 的轨迹。两条成功的、互斥的轨迹是：\n1.  轨迹 1 ($\\tau_1$)：动作序列 $(a_A, a_B, a_{\\mathrm{bond}}, a_{\\mathrm{stop}})$。\n    - 在 $t=1$，状态为 $s_{\\emptyset}$，执行动作 $a_A$。概率：$\\pi(a_A \\mid s_{\\emptyset}) = \\alpha$。\n    - 在 $t=2$，状态为 $s_A$，执行动作 $a_B$。概率：$\\pi(a_B \\mid s_A) = \\gamma$。\n    - 在 $t=3$，状态为 $s_{AB}^{\\mathrm{no\\ bond}}$，执行动作 $a_{\\mathrm{bond}}$。概率：$\\pi(a_{\\mathrm{bond}} \\mid s_{AB}^{\\mathrm{no\\ bond}}) = \\eta$。\n    - 在 $t=4$，状态为 $s_{AB}^{\\mathrm{bonded}}$，执行动作 $a_{\\mathrm{stop}}$。概率：$\\pi(a_{\\mathrm{stop}} \\mid s_{AB}^{\\mathrm{bonded}}) = 1$。\n    此轨迹的概率 $P(\\tau_1)$ 是这些概率的乘积：\n    $$P(\\tau_1) = \\alpha \\cdot \\gamma \\cdot \\eta \\cdot 1 = \\alpha\\gamma\\eta$$\n\n2.  轨迹 2 ($\\tau_2$)：动作序列 $(a_B, a_A, a_{\\mathrm{bond}}, a_{\\mathrm{stop}})$。\n    - 在 $t=1$，状态为 $s_{\\emptyset}$，执行动作 $a_B$。概率：$\\pi(a_B \\mid s_{\\emptyset}) = \\beta$。\n    - 在 $t=2$，状态为 $s_B$，执行动作 $a_A$。概率：$\\pi(a_A \\mid s_B) = \\delta$。\n    - 在 $t=3$，状态为 $s_{AB}^{\\mathrm{no\\ bond}}$，执行动作 $a_{\\mathrm{bond}}$。概率：$\\pi(a_{\\mathrm{bond}} \\mid s_{AB}^{\\mathrm{no\\ bond}}) = \\eta$。\n    - 在 $t=4$，状态为 $s_{AB}^{\\mathrm{bonded}}$，执行动作 $a_{\\mathrm{stop}}$。概率：$\\pi(a_{\\mathrm{stop}} \\mid s_{AB}^{\\mathrm{bonded}}) = 1$。\n    此轨迹的概率 $P(\\tau_2)$ 是乘积：\n    $$P(\\tau_2) = \\beta \\cdot \\delta \\cdot \\eta \\cdot 1 = \\beta\\delta\\eta$$\n\n生成目标分子的总概率是这两条不相交轨迹的概率之和。\n$$P_{\\mathrm{total}} = P(\\tau_1) + P(\\tau_2) = \\alpha\\gamma\\eta + \\beta\\delta\\eta$$\n此表达式可以因式分解得到最终结果：\n$$P_{\\mathrm{total}} = (\\alpha\\gamma + \\beta\\delta)\\eta$$",
            "answer": "$$\n\\boxed{(\\alpha\\gamma + \\beta\\delta)\\eta}\n$$"
        },
        {
            "introduction": "构建了MDP模型后，我们的核心任务是训练一个策略来生成具有期望性质的分子。本练习将指导您推导REINFORCE算法的策略梯度，这是一种在基于序列（如SMILES）的分子生成中广泛使用的核心算法，并探讨基线（baseline）和长度惩罚等实际考量。",
            "id": "3861943",
            "problem": "考虑用于从头分子设计的简体分子线性输入系统 (SMILES) 字符串的自回归序列生成。分子被表示为一个从有限词汇表 $\\mathcal{V}$ 中提取的词元序列，该词汇表包含一个特殊的序列结束 (EOS) 词元。定义一个马尔可夫决策过程 (MDP)，其中时刻 $t$ 的状态（表示为 $s_t$）是 SMILES 字符串的长度为 $t$ 的前缀，而动作 $a_t \\in \\mathcal{V}$ 是要追加的下一个词元。一个回合从空前缀开始，在采样到 EOS 或达到最大长度 $L_{\\max}$ 时终止。随机策略 $\\pi_{\\theta}(a \\mid s)$ 由 $\\theta$ 参数化，并以如下概率密度导出一系列轨迹 $\\tau = (s_1,a_1,\\dots,s_{L(\\tau)},a_{L(\\tau)})$\n$$\np_{\\theta}(\\tau) \\;=\\; \\prod_{t=1}^{L(\\tau)} \\pi_{\\theta}(a_t \\mid s_t),\n$$\n其中 $L(\\tau) \\in \\{1,\\dots,L_{\\max}\\}$ 是（随机的）回合长度。\n\n设终点奖励为\n$$\nR(\\tau) \\;=\\; S\\!\\big(m(\\tau)\\big) \\;-\\; \\lambda\\, L(\\tau),\n$$\n其中 $S(m)$ 是由完成的 SMILES 实现的分子 $m(\\tau)$ 的一个可微性质评分函数（例如，预测的生物活性），$\\lambda \\ge 0$ 是一个惩罚较长序列的长度惩罚系数。优化目标是期望回合回报\n$$\nJ(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\big[\\,R(\\tau)\\,\\big].\n$$\n你可以使用一个不依赖于动作 $a_t$ 的、依赖于状态的控制变量（基线）$b(s_t)$ 来减小梯度估计器的方差。\n\n仅从马尔可夫决策过程 (MDP) 的基本定义、似然比恒等式，以及添加一个独立于 $a_t$ 的基线 $b(s_t)$ 不会改变期望策略梯度这一事实出发，为这个带有长度惩罚的、基于 SMILES 的序列生成问题推导 REINFORCE 更新（即一个无偏随机梯度估计器）。你的推导必须清楚地说明长度惩罚是如何进入估计器的。然后，考虑一种教师强制的形式，其中每个时间步的行为策略是以下混合形式\n$$\nq_{\\eta}(a \\mid s) \\;=\\; \\eta\\, \\mu(a \\mid s) \\;+\\; (1-\\eta)\\, \\pi_{\\theta}(a \\mid s),\n$$\n其中 $\\eta \\in [0,1]$，$\\mu(a \\mid s)$ 是一个固定的、数据驱动的教师策略，它从一个精选的分子数据集中近似下一个词元的分布。假设轨迹是从 $q_{\\eta}$ 中采样的，但计算了一个朴素的 REINFORCE 估计器，而没有进行重要性采样校正。定性和定量地解释这种教师强制如何改变状态-动作访问分布，并用 $\\eta$、$\\mu$、$\\pi_{\\theta}$ 和 $R(\\tau)$ 推导出梯度估计器中由此产生的偏差的闭式表达式。\n\n将你的最终答案表示为带有长度惩罚和基线 $b(s_t)$ 的 $\\nabla_{\\theta} J(\\theta)$ 的无偏 REINFORCE 更新方向的闭式解析表达式，用 $\\pi_{\\theta}$、$S(m(\\tau))$、$\\lambda$、$L(\\tau)$ 和 $b(s_t)$ 写出。不要包含任何数值计算。不需要单位。最终答案必须是单个闭式表达式。",
            "solution": "该问题要求进行两个与用于分子序列生成的强化学习相关的主要推导。首先，推导包含长度惩罚和基线的 REINFORCE 策略梯度估计器。其次，分析在没有重要性采样校正的情况下使用混合策略进行采样所引入的偏差。\n\n第 1 部分：带有长度惩罚的 REINFORCE 更新的推导\n\n目标是最大化期望回合回报 $J(\\theta)$，其定义如下：\n$$\nJ(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\big[\\,R(\\tau)\\,\\big]\n$$\n其中 $\\tau$ 是一条轨迹（一个 SMILES 字符串），$p_{\\theta}(\\tau)$ 是在策略 $\\pi_{\\theta}$ 下采样到该轨迹的概率，$R(\\tau)$ 是终点奖励。目标是找到目标函数关于策略参数的梯度 $\\nabla_{\\theta} J(\\theta)$，以执行梯度上升。\n\n我们首先将期望写成对所有可能轨迹的求和：\n$$\nJ(\\theta) \\;=\\; \\sum_{\\tau} p_{\\theta}(\\tau) R(\\tau)\n$$\n则关于 $\\theta$ 的梯度为：\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\nabla_{\\theta} \\sum_{\\tau} p_{\\theta}(\\tau) R(\\tau) \\;=\\; \\sum_{\\tau} (\\nabla_{\\theta} p_{\\theta}(\\tau)) R(\\tau)\n$$\n此处，我们假设奖励函数 $R(\\tau)$ 不依赖于策略参数 $\\theta$。我们现在使用似然比恒等式，也称为得分函数恒等式，即 $\\nabla_{\\theta} p_{\\theta}(\\tau) = p_{\\theta}(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)$。将此代入梯度表达式，得到：\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\sum_{\\tau} p_{\\theta}(\\tau) (\\nabla_{\\theta} \\ln p_{\\theta}(\\tau)) R(\\tau)\n$$\n这可以重写为一个期望，这是策略梯度定理的核心：\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\big[\\,(\\nabla_{\\theta} \\ln p_{\\theta}(\\tau)) R(\\tau)\\,\\big]\n$$\n轨迹 $\\tau=(s_1, a_1, \\dots, s_{L(\\tau)}, a_{L(\\tau)})$ 的概率由每一步的策略概率的乘积给出：\n$$\np_{\\theta}(\\tau) \\;=\\; \\prod_{t=1}^{L(\\tau)} \\pi_{\\theta}(a_t \\mid s_t)\n$$\n取对数，我们得到：\n$$\n\\ln p_{\\theta}(\\tau) \\;=\\; \\sum_{t=1}^{L(\\tau)} \\ln \\pi_{\\theta}(a_t \\mid s_t)\n$$\n因此，轨迹的对数概率的梯度是所采取动作的对数概率的梯度的总和：\n$$\n\\nabla_{\\theta} \\ln p_{\\theta}(\\tau) \\;=\\; \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)\n$$\n将此代回 $\\nabla_{\\theta} J(\\theta)$ 的表达式中：\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\left[\\, \\left( \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\right) R(\\tau) \\,\\right]\n$$\n这是原始的策略梯度表达式。从一条轨迹 $\\tau$ 中获得的该梯度的单样本蒙特卡洛估计器是 $g = \\left( \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\right) R(\\tau)$。\n\n为了减小该估计器的方差，可以引入一个依赖于状态但不依赖于动作的基线 $b(s_t)$。问题指出，添加这样一个基线不会改变期望策略梯度。让我们对常用形式进行证明，其中基线从和中的每一项的奖励中减去：\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\left[\\, \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) (R(\\tau) - b(s_t)) \\,\\right]\n$$\n如果增加项的期望值为零，则这是一个无偏估计器：\n$$\n\\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\left[\\, \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) (-b(s_t)) \\,\\right] \\;=\\; 0\n$$\n根据期望的线性性质并交换求和与期望：\n$$\n-\\sum_{t=1}^{L_{\\max}} \\mathbb{E}_{\\tau \\sim p_{\\theta}, L(\\tau) \\ge t} \\!\\left[\\, \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) b(s_t) \\,\\right]\n$$\n该期望可以在状态和动作上展开：\n$$\n= -\\sum_{t=1}^{L_{\\max}} \\mathbb{E}_{s_t \\sim p_{\\theta}(s_t)} \\!\\left[\\, \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(a_t|s_t)} \\!\\left[\\, \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) b(s_t) \\,\\right] \\,\\right]\n$$\n由于 $b(s_t)$ 不依赖于 $a_t$，它可以从内层期望中提出：\n$$\n= -\\sum_{t=1}^{L_{\\max}} \\mathbb{E}_{s_t \\sim p_{\\theta}(s_t)} \\!\\left[\\, b(s_t) \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(a_t|s_t)} \\!\\left[\\, \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\,\\right] \\,\\right]\n$$\n内层期望为 $\\mathbb{E}_{a_t \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)] = \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} = \\nabla_{\\theta} \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) = \\nabla_{\\theta}(1) = 0$。因此，整个表达式为零，减去基线是无偏的。\n\n一条轨迹 $\\tau$ 的单样本随机梯度估计器（REINFORCE 更新方向）是：\n$$\ng \\;=\\; \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\left( R(\\tau) - b(s_t) \\right)\n$$\n最后，我们代入给定的奖励函数 $R(\\tau) = S(m(\\tau)) - \\lambda L(\\tau)$：\n$$\ng \\;=\\; \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\left( S(m(\\tau)) - \\lambda L(\\tau) - b(s_t) \\right)\n$$\n长度惩罚 $\\lambda L(\\tau)$ 作为优势估计 $A_t = S(m(\\tau)) - \\lambda L(\\tau) - b(s_t)$ 中的一个负项进入估计器。这一项降低了较长序列的奖励，从而抑制策略生成它们。每一步 $t$ 的梯度更新都由该优势加权，因此具有较大惩罚（较长长度）的轨迹将获得一个更小或更负的权重因子，从而在其他条件相同的情况下，推动策略参数 $\\theta$ 朝着生成更短序列的方向发展。\n\n第 2 部分：无重要性采样的混合策略所带来的偏差\n\n问题的第二部分考虑了一种情景，其中轨迹不是从智能体策略 $\\pi_{\\theta}$ 中采样，而是从混合策略 $q_{\\eta}(a|s) = \\eta \\mu(a|s) + (1-\\eta) \\pi_{\\theta}(a|s)$ 中采样。使用了一个朴素的 REINFORCE 估计器，它忽略了这一事实，并假设轨迹是从 $\\pi_{\\theta}$ 中采样来计算梯度。\n\n定性地看，使用混合策略 $q_{\\eta}$ 进行采样会改变状态-动作访问分布。源自精选数据集的教师策略 $\\mu$ 会引导探索朝向已知对应有效或理想分子结构的状态-动作空间区域。因此，智能体会接触到它可能无法自行发现的状态和动作。然而，朴素估计器未能考虑到这种引导。它根据智能体自身的策略概率计算梯度更新 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)$。当由于教师的引导（即 $\\mu(a_t|s_t)$ 很高）而找到一条高奖励轨迹，但智能体对该动作的策略概率很低（即 $\\pi_{\\theta}(a_t|s_t)$ 很低）时，$\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)$ 项可能会很大。智能体错误地将教师的“发现”归功于自己，导致一个有偏梯度，该梯度并未指向 $J(\\theta)$ 的真正上升方向。\n\n定量地看，偏差是朴素估计器的期望值与真实梯度之间的差值。真实梯度为 $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]$。一条轨迹 $\\tau$ 的朴素估计器是 $\\hat{g}_{\\text{naive}} = R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)$。由于轨迹是从 $q_{\\eta}$ 中采样的，该估计器的期望值为：\n$$\n\\mathbb{E}_{\\tau \\sim q_{\\eta}}[\\hat{g}_{\\text{naive}}] \\;=\\; \\mathbb{E}_{\\tau \\sim q_{\\eta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]\n$$\n偏差是这个差值：\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim q_{\\eta}}[\\hat{g}_{\\text{naive}}] \\;-\\; \\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim q_{\\eta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)] \\;-\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]\n$$\n这可以通过使用重要性采样权重，表示为关于智能体策略分布 $p_{\\theta}$ 的单个期望：\n$$\n\\mathbb{E}_{\\tau \\sim q_{\\eta}}[f(\\tau)] \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} f(\\tau)\\right]\n$$\n将此应用于偏差表达式，得到：\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)\\right] - \\mathbb{E}_{\\tau \\sim p_{\\theta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]\n$$\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[\\left(\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} - 1\\right) R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)\\right]\n$$\n重要性权重 $\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)}$ 可以用给定的策略表示：\n$$\n\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} \\;=\\; \\frac{\\prod_{t=1}^{L(\\tau)} q_{\\eta}(a_t|s_t)}{\\prod_{t=1}^{L(\\tau)} \\pi_{\\theta}(a_t|s_t)} \\;=\\; \\prod_{t=1}^{L(\\tau)} \\frac{\\eta \\mu(a_t|s_t) + (1-\\eta) \\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} \\;=\\; \\prod_{t=1}^{L(\\tau)} \\left(1 + \\eta \\left(\\frac{\\mu(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} - 1\\right) \\right)\n$$\n代入这个比率和 $\\nabla_{\\theta} \\ln p_{\\theta}(\\tau) = \\sum_{k=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_k|s_k)$，我们得到偏差的最终闭式表达式：\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[ \\left( \\prod_{t=1}^{L(\\tau)} \\left[1 + \\eta \\left(\\frac{\\mu(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} - 1\\right) \\right] - 1 \\right) R(\\tau) \\left( \\sum_{k=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_k|s_k) \\right) \\right]\n$$\n这个表达式用 $\\eta$、$\\mu$ 和 $\\pi_{\\theta}$ 捕捉了偏差，显示了采样分布与策略分布之间的偏差（由奖励加权）如何在梯度估计中引入系统误差。",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\left( S(m(\\tau)) - \\lambda L(\\tau) - b(s_t) \\right)}\n$$"
        },
        {
            "introduction": "在实际应用中，我们常常利用已有的分子数据集进行策略的预训练或微调，但这会引入分布偏移（distributional shift）的问题。这项练习将引导您分析数据集偏差如何导致策略偏差，并从第一性原理出发推导重要性采样权重以纠正这种偏差，这是离线强化学习中的一个关键技术。",
            "id": "3861926",
            "problem": "在将分子设计表述为马尔可夫决策过程 (MDP) 的强化学习 (RL) 公式中，考虑一个随机策略 $\\pi_{\\theta}$，它从一个初始骨架类别 $S \\in \\{A, B, C\\}$ 生成分子轨迹 $\\tau$，其中 $S$ 表示最终分子的 Bemis–Murcko 骨架类别。设回报为 $R(\\tau)$，并考虑分数函数策略梯度估计器，其每条轨迹的贡献为 $G(\\tau) \\equiv R(\\tau) \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{t} \\mid x_{t})$。假设在给定骨架类别 $S$ 的条件下，轨迹和回报的分布与用于获取初始骨架的采样过程无关。\n\n您可以使用一个离线数据集 $\\mathcal{D}$，该数据集中的轨迹由外部过程生成，其骨架类别的边缘分布为 $p_{\\mathcal{D}}(S)$。您希望训练策略，就好像轨迹是从目标骨架分布 $q(S)$ 中抽取的一样，以在训练期间均衡骨架的边缘分布，从而减轻策略对过度表示的骨架的偏差。\n\n假设 $p_{\\mathcal{D}}(A) = 0.7$，$p_{\\mathcal{D}}(B) = 0.2$，$p_{\\mathcal{D}}(C) = 0.1$，且目标分布为均匀分布，$q(A) = q(B) = q(C) = \\frac{1}{3}$。假设只要 $q(S) > 0$，就有 $p_{\\mathcal{D}}(S) > 0$。\n\n1) 从目标骨架边缘分布 $q(S)$ 下的真实策略梯度的定义，以及使用来自 $p_{\\mathcal{D}}(S)$ 的未加权样本的离线估计器出发，推导在骨架层面使用未加权离线估计器所引入的偏差的表达式。用 $p_{\\mathcal{D}}(S)$、$q(S)$ 和骨架条件期望 $g(S) \\equiv \\mathbb{E}[G(\\tau) \\mid S]$ 来表示该偏差。\n\n2) 推导一个关于非负重加权函数 $w(S)$ 的充分条件，使得加权离线估计器具有与目标相同的骨架边缘期望，即，对于每个仅通过 $S$ 依赖于 $\\tau$ 的有界可测函数 $h$，等式 $\\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[w(S) \\, h(S)] = \\mathbb{E}_{S \\sim q}[h(S)]$ 成立。仅使用离散分布下期望的基本原理，并避免调用任何特定的预先推导的公式。\n\n3) 在所有此类解中，施加额外的归一化约束 $\\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[w(S)] = 1$，并确定唯一的归一化解 $w(S)$ 的闭式形式。\n\n4) 使用给定的 $p_{\\mathcal{D}}$ 和 $q$，计算权重 $w(B)$ 的数值。将您的最终答案表示为精确分数。不要四舍五入。",
            "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于强化学习和统计学的原理，特别是通过重要性采样来解决离策略校正问题。该问题提法恰当、客观，并包含完整解答所需的所有信息。\n\n问题的核心是校正用于训练策略的离线数据集的分布 $p_{\\mathcal{D}}(S)$ 与期望的目标分布 $q(S)$ 之间的不匹配。这是强化学习中一个常见的情景，称为离策略学习或离线学习。校正是通过对离线数据集中的样本进行重加权来实现的。\n\n设 $S$ 表示骨架类别，可以是 $\\{A, B, C\\}$ 中的一种。我们已知离线数据集 $\\mathcal{D}$ 中骨架的边缘分布为：$p_{\\mathcal{D}}(A) = 0.7$，$p_{\\mathcal{D}}(B) = 0.2$ 和 $p_{\\mathcal{D}}(C) = 0.1$。目标分布是均匀分布：$q(A) = q(B) = q(C) = \\frac{1}{3}$。\n\n我们已知对策略梯度估计器的每条轨迹贡献为 $G(\\tau) \\equiv R(\\tau) \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{t} \\mid x_{t})$。我们将该量的骨架条件期望定义为 $g(S) \\equiv \\mathbb{E}[G(\\tau) \\mid S]$。\n\n1) 偏差的推导：\n相对于目标骨架分布 $q(S)$ 的真实策略梯度，是根据目标过程抽取的轨迹上 $G(\\tau)$ 的期望。这可以使用全期望定律，以骨架类别 $S$ 为条件来表示：\n$$ \\nabla_{\\theta} J_{q} = \\mathbb{E}_{\\tau \\sim q}[G(\\tau)] = \\mathbb{E}_{S \\sim q}[\\mathbb{E}[G(\\tau) \\mid S]] $$\n使用 $g(S)$ 的定义，这变为：\n$$ \\nabla_{\\theta} J_{q} = \\mathbb{E}_{S \\sim q}[g(S)] = \\sum_{S \\in \\{A,B,C\\}} q(S) g(S) $$\n未加权的离线估计器使用来自离线数据集的轨迹来计算期望，这些轨迹根据 $p_{\\mathcal{D}}(S)$ 分布。该估计器的期望是：\n$$ \\nabla_{\\theta} \\hat{J}_{p_{\\mathcal{D}}} = \\mathbb{E}_{\\tau \\sim p_{\\mathcal{D}}}[G(\\tau)] = \\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[\\mathbb{E}[G(\\tau) \\mid S]] $$\n这可以简化为：\n$$ \\nabla_{\\theta} \\hat{J}_{p_{\\mathcal{D}}} = \\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[g(S)] = \\sum_{S \\in \\{A,B,C\\}} p_{\\mathcal{D}}(S) g(S) $$\n未加权离线估计器的偏差是估计器的期望与真实梯度之间的差值：\n$$ \\text{Bias} = \\nabla_{\\theta} \\hat{J}_{p_{\\mathcal{D}}} - \\nabla_{\\theta} J_{q} $$\n代入上述表达式：\n$$ \\text{Bias} = \\sum_{S \\in \\{A,B,C\\}} p_{\\mathcal{D}}(S) g(S) - \\sum_{S \\in \\{A,B,C\\}} q(S) g(S) $$\n这可以更紧凑地写为：\n$$ \\text{Bias} = \\sum_{S \\in \\{A,B,C\\}} [p_{\\mathcal{D}}(S) - q(S)] g(S) $$\n\n2) 重加权函数 $w(S)$ 的充分条件的推导：\n我们正在寻找一个非负函数 $w(S)$，使得对于任何仅依赖于骨架类别 $S$ 的有界可测函数 $h$，以下等式成立：\n$$ \\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[w(S) \\, h(S)] = \\mathbb{E}_{S \\sim q}[h(S)] $$\n写出关于 $S$ 的离散分布的期望：\n左侧是 $\\sum_{S \\in \\{A,B,C\\}} p_{\\mathcal{D}}(S) w(S) h(S)$。\n右侧是 $\\sum_{S \\in \\{A,B,C\\}} q(S) h(S)$。\n因此，我们要求：\n$$ \\sum_{S \\in \\{A,B,C\\}} p_{\\mathcal{D}}(S) w(S) h(S) = \\sum_{S \\in \\{A,B,C\\}} q(S) h(S) $$\n这可以重新排列为：\n$$ \\sum_{S \\in \\{A,B,C\\}} [p_{\\mathcal{D}}(S) w(S) - q(S)] h(S) = 0 $$\n这个等式必须对*任何*有界可测函数 $h(S)$ 都成立。这当且仅当对于 $S$ 的每个值，$h(S)$ 的系数都为零时才可能。为了证明这一点，考虑选择特定的函数 $h(S)$。例如，设 $h_A(S)$ 是一个指示函数，使得 $h_A(A) = 1$ 且当 $S \\neq A$ 时 $h_A(S) = 0$。将 $h_A(S)$ 代入方程得到：\n$$ [p_{\\mathcal{D}}(A) w(A) - q(A)] \\cdot 1 + [p_{\\mathcal{D}}(B) w(B) - q(B)] \\cdot 0 + [p_{\\mathcal{D}}(C) w(C) - q(C)] \\cdot 0 = 0 $$\n这意味着 $p_{\\mathcal{D}}(A) w(A) - q(A) = 0$。通过为 $B$ 和 $C$ 选择类似的指示函数，我们发现该方程必须逐项成立。因此，一个充分条件是：\n$$ p_{\\mathcal{D}}(S) w(S) - q(S) = 0 \\quad \\forall S \\in \\{A,B,C\\} $$\n这等价于对所有 $S$ 都有 $p_{\\mathcal{D}}(S) w(S) = q(S)$。\n\n3) 唯一归一化解 $w(S)$ 的推导：\n根据第 2) 部分推导出的充分条件，我们可以解出 $w(S)$：\n$$ w(S) = \\frac{q(S)}{p_{\\mathcal{D}}(S)} $$\n这是良定义的，因为问题陈述中说明了只要 $q(S) > 0$，就有 $p_{\\mathcal{D}}(S) > 0$。问题施加了一个额外的归一化约束：$\\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[w(S)] = 1$。让我们检查一下我们的 $w(S)$ 解是否满足此约束。\n$$ \\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[w(S)] = \\sum_{S \\in \\{A,B,C\\}} p_{\\mathcal{D}}(S) w(S) $$\n代入我们的 $w(S)$ 表达式：\n$$ \\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[w(S)] = \\sum_{S \\in \\{A,B,C\\}} p_{\\mathcal{D}}(S) \\left( \\frac{q(S)}{p_{\\mathcal{D}}(S)} \\right) = \\sum_{S \\in \\{A,B,C\\}} q(S) $$\n由于 $q(S)$ 是一个概率分布，所有可能结果的总和为 1。\n$$ \\sum_{S \\in \\{A,B,C\\}} q(S) = 1 $$\n因此，条件 $\\mathbb{E}_{S \\sim p_{\\mathcal{D}}}[w(S)] = 1$ 被解 $w(S) = \\frac{q(S)}{p_{\\mathcal{D}}(S)}$ 自动满足。第 2) 部分推导出的条件对于所有函数 $h(S)$ 的期望匹配成立是充分且必要的，这在 $p_{\\mathcal{D}}(S) > 0$ 的地方唯一地确定了 $w(S)$。因此，唯一的解是：\n$$ w(S) = \\frac{q(S)}{p_{\\mathcal{D}}(S)} $$\n\n4) $w(B)$ 数值的计算：\n使用第 3) 部分推导的公式，我们可以计算骨架类别 $B$ 的权重。我们已知：\n- 骨架 $B$ 的目标概率：$q(B) = \\frac{1}{3}$。\n- 骨架 $B$ 的数据概率：$p_{\\mathcal{D}}(B) = 0.2$。\n为了将结果表示为精确分数，我们首先将 $p_{\\mathcal{D}}(B)$ 写成分数形式：$0.2 = \\frac{2}{10} = \\frac{1}{5}$。\n现在我们可以计算 $w(B)$：\n$$ w(B) = \\frac{q(B)}{p_{\\mathcal{D}}(B)} = \\frac{1/3}{1/5} $$\n$$ w(B) = \\frac{1}{3} \\times \\frac{5}{1} = \\frac{5}{3} $$\n这是必须应用于最终骨架类别为 $B$ 的轨迹的重加权因子，以校正离线数据和目标均匀分布之间的分布不匹配。",
            "answer": "$$\\boxed{\\frac{5}{3}}$$"
        }
    ]
}