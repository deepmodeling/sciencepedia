{
    "hands_on_practices": [
        {
            "introduction": "Before applying reinforcement learning, we must first frame molecular generation in the language of a Markov Decision Process (MDP). This foundational exercise  solidifies the core concepts of states, actions, policies, and trajectories. By calculating the probability of generating a simple target molecule under a given policy, you will gain a concrete understanding of how the agent's choices probabilistically determine the outcome of the generation process.",
            "id": "3861968",
            "problem": "Consider a finite-horizon Markov Decision Process (MDP) for molecular graph construction in reinforcement learning, where a molecule is built by sequential graph edits starting from the empty graph. The state is the current molecular graph, the action is a graph-edit operation, and transitions are deterministic given the action (graph edits deterministically update the graph). The episode terminates when a termination action is taken, and no further actions occur after termination.\n\nLet the finite horizon be $T = 4$. The action set is $\\{a_{A}, a_{B}, a_{\\mathrm{bond}}, a_{\\mathrm{stop}}\\}$ corresponding to adding atom type $A$, adding atom type $B$, adding a single bond between existing $A$ and $B$ atoms (only admissible when both $A$ and $B$ exist and are not yet bonded), and terminating, respectively. The initial state is the empty graph $s_{\\emptyset}$. The target molecule is the bonded dimer $A\\text{-}B$, and we define that the generated molecule is the terminal graph at the time of termination.\n\nThe policy is stationary and denoted by $\\pi(a \\mid s)$ over admissible actions in state $s$. All actions not listed as admissible in a state have probability $0$. The policy values for admissible actions in the states reachable on the way to the target molecule are specified by the following symbols:\n- At the empty graph $s_{\\emptyset}$: $\\pi(a_{A}\\mid s_{\\emptyset})=\\alpha$, $\\pi(a_{B}\\mid s_{\\emptyset})=\\beta$, and $\\pi(a_{\\mathrm{stop}}\\mid s_{\\emptyset})=1-\\alpha-\\beta$, with $\\alpha,\\beta \\in (0,1)$ and $\\alpha+\\beta \\leq 1$.\n- At a single-atom $A$ graph $s_{A}$: $\\pi(a_{B}\\mid s_{A})=\\gamma$ and $\\pi(a_{\\mathrm{stop}}\\mid s_{A})=1-\\gamma$, with $\\gamma \\in (0,1]$.\n- At a single-atom $B$ graph $s_{B}$: $\\pi(a_{A}\\mid s_{B})=\\delta$ and $\\pi(a_{\\mathrm{stop}}\\mid s_{B})=1-\\delta$, with $\\delta \\in (0,1]$.\n- At the two-atom unbonded graph $s_{AB}^{\\mathrm{no\\ bond}}$: $\\pi(a_{\\mathrm{bond}}\\mid s_{AB}^{\\mathrm{no\\ bond}})=\\eta$ and $\\pi(a_{\\mathrm{stop}}\\mid s_{AB}^{\\mathrm{no\\ bond}})=1-\\eta$, with $\\eta \\in (0,1]$.\n- At the bonded dimer state $s_{AB}^{\\mathrm{bonded}}$: $\\pi(a_{\\mathrm{stop}}\\mid s_{AB}^{\\mathrm{bonded}})=1$.\n\nAssume that the stochastic choices of actions across time steps are independent conditioned on the visited states, and that state transitions are deterministic as specified. Starting from $s_{\\emptyset}$ at time $t=1$, compute, in closed form, the total probability (as a function of $\\alpha,\\beta,\\gamma,\\delta,\\eta$) that the agent generates the bonded dimer $A\\text{-}B$ as the terminal molecule within horizon $T=4$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\alpha,\\beta,\\gamma,\\delta,\\eta$. Do not include any units. No rounding is required.",
            "solution": "The objective is to calculate the total probability of all successful trajectories starting from the initial state $s_{\\emptyset}$ at $t=1$. A trajectory is successful if it terminates by generating the bonded dimer $A\\text{-}B$ within the time horizon $T=4$. According to the problem statement, \"the generated molecule is the terminal graph at the time of termination\". This means a successful trajectory must end with the action $a_{\\mathrm{stop}}$ being taken from the state $s_{AB}^{\\mathrm{bonded}}$. Let the sequence of states be $s_1, s_2, \\dots, s_k$ and actions be $a_1, a_2, \\dots, a_k$. For a successful trajectory of length $k$, we must have $s_1=s_{\\emptyset}$, $s_k=s_{AB}^{\\mathrm{bonded}}$, and $a_k=a_{\\mathrm{stop}}$. The total length of the trajectory, $k$, must be less than or equal to the horizon, $k \\le T=4$.\n\nWe must first determine the sequence of actions required to reach the state $s_{AB}^{\\mathrm{bonded}}$ from $s_{\\emptyset}$. The state transitions are deterministic given the actions. Let's trace the paths to $s_{AB}^{\\mathrm{bonded}}$:\n1.  To reach $s_{AB}^{\\mathrm{bonded}}$, the agent must be in state $s_{AB}^{\\mathrm{no\\ bond}}$ and take action $a_{\\mathrm{bond}}$.\n2.  To reach $s_{AB}^{\\mathrm{no\\ bond}}$, the agent must have added both an atom of type $A$ and an atom of type $B$. This can occur in two distinct orders:\n    - Path A: Add atom $A$, then add atom $B$. Sequence of states: $s_{\\emptyset} \\rightarrow s_{A} \\rightarrow s_{AB}^{\\mathrm{no\\ bond}}$. This requires two actions: $a_{A}$ followed by $a_{B}$.\n    - Path B: Add atom $B$, then add atom $A$. Sequence of states: $s_{\\emptyset} \\rightarrow s_{B} \\rightarrow s_{AB}^{\\mathrm{no\\ bond}}$. This requires two actions: $a_{B}$ followed by $a_{A}$.\n\nCombining these steps, there are two minimal sequences of actions to reach the state $s_{AB}^{\\mathrm{bonded}}$:\n- Sequence 1: $(a_{A}, a_{B}, a_{\\mathrm{bond}})$. This sequence of $3$ actions transforms the state as follows: $s_{\\emptyset} \\xrightarrow{a_A} s_A \\xrightarrow{a_B} s_{AB}^{\\mathrm{no\\ bond}} \\xrightarrow{a_{\\mathrm{bond}}} s_{AB}^{\\mathrm{bonded}}$.\n- Sequence 2: $(a_{B}, a_{A}, a_{\\mathrm{bond}})$. This sequence of $3$ actions transforms the state as follows: $s_{\\emptyset} \\xrightarrow{a_B} s_B \\xrightarrow{a_A} s_{AB}^{\\mathrm{no\\ bond}} \\xrightarrow{a_{\\mathrm{bond}}} s_{AB}^{\\mathrm{bonded}}$.\n\nIn both cases, reaching the target state $s_{AB}^{\\mathrm{bonded}}$ requires exactly $3$ actions. These actions would be taken at times $t=1, 2, 3$. At the beginning of time step $t=4$, the system is in state $s_{AB}^{\\mathrm{bonded}}$.\n\nFor the trajectory to be successful, the agent must then take the action $a_{\\mathrm{stop}}$ from state $s_{AB}^{\\mathrm{bonded}}$. This would be the fourth action, taken at $t=4$. The total length of such a successful trajectory is $k=4$. Since the horizon is $T=4$, these are the longest possible successful trajectories. Any trajectory that terminates earlier (at $t=1, 2,$ or $3$) will not have reached state $s_{AB}^{\\mathrm{bonded}}$ and will thus be unsuccessful. For example, a trajectory $(a_A, a_B, a_{\\mathrm{stop}})$ terminates at $t=3$ with the terminal molecule being $s_{AB}^{\\mathrm{no\\ bond}}$.\n\nThus, we only need to consider trajectories of length exactly $4$. The two successful, mutually exclusive trajectories are:\n1.  Trajectory 1 ($\\tau_1$): The sequence of actions $(a_A, a_B, a_{\\mathrm{bond}}, a_{\\mathrm{stop}})$.\n    - At $t=1$, in state $s_{\\emptyset}$, take action $a_A$. Probability: $\\pi(a_A \\mid s_{\\emptyset}) = \\alpha$.\n    - At $t=2$, in state $s_A$, take action $a_B$. Probability: $\\pi(a_B \\mid s_A) = \\gamma$.\n    - At $t=3$, in state $s_{AB}^{\\mathrm{no\\ bond}}$, take action $a_{\\mathrm{bond}}$. Probability: $\\pi(a_{\\mathrm{bond}} \\mid s_{AB}^{\\mathrm{no\\ bond}}) = \\eta$.\n    - At $t=4$, in state $s_{AB}^{\\mathrm{bonded}}$, take action $a_{\\mathrm{stop}}$. Probability: $\\pi(a_{\\mathrm{stop}} \\mid s_{AB}^{\\mathrm{bonded}}) = 1$.\n    The probability of this trajectory, $P(\\tau_1)$, is the product of these probabilities:\n    $$P(\\tau_1) = \\alpha \\cdot \\gamma \\cdot \\eta \\cdot 1 = \\alpha\\gamma\\eta$$\n\n2.  Trajectory 2 ($\\tau_2$): The sequence of actions $(a_B, a_A, a_{\\mathrm{bond}}, a_{\\mathrm{stop}})$.\n    - At $t=1$, in state $s_{\\emptyset}$, take action $a_B$. Probability: $\\pi(a_B \\mid s_{\\emptyset}) = \\beta$.\n    - At $t=2$, in state $s_B$, take action $a_A$. Probability: $\\pi(a_A \\mid s_B) = \\delta$.\n    - At $t=3$, in state $s_{AB}^{\\mathrm{no\\ bond}}$, take action $a_{\\mathrm{bond}}$. Probability: $\\pi(a_{\\mathrm{bond}} \\mid s_{AB}^{\\mathrm{no\\ bond}}) = \\eta$.\n    - At $t=4$, in state $s_{AB}^{\\mathrm{bonded}}$, take action $a_{\\mathrm{stop}}$. Probability: $\\pi(a_{\\mathrm{stop}} \\mid s_{AB}^{\\mathrm{bonded}}) = 1$.\n    The probability of this trajectory, $P(\\tau_2)$, is the product:\n    $$P(\\tau_2) = \\beta \\cdot \\delta \\cdot \\eta \\cdot 1 = \\beta\\delta\\eta$$\n\nThe total probability of generating the target molecule is the sum of the probabilities of these two disjoint trajectories.\n$$P_{\\mathrm{total}} = P(\\tau_1) + P(\\tau_2) = \\alpha\\gamma\\eta + \\beta\\delta\\eta$$\nThis expression can be factored to give the final result:\n$$P_{\\mathrm{total}} = (\\alpha\\gamma + \\beta\\delta)\\eta$$",
            "answer": "$$\n\\boxed{(\\alpha\\gamma + \\beta\\delta)\\eta}\n$$"
        },
        {
            "introduction": "Once the problem is formulated as an MDP, the goal is to learn a policy that maximizes the generation of high-reward molecules. The REINFORCE algorithm is a fundamental policy gradient method for this task. This practice  guides you through the derivation of the REINFORCE estimator, demonstrating how to incorporate rewards and handle practical considerations like sequence length penalties, variance reduction using baselines, and the bias introduced by off-policy sampling schemes like teacher forcing.",
            "id": "3861943",
            "problem": "Consider autoregressive sequence generation of Simplified Molecular Input Line Entry System (SMILES) strings for de novo molecular design. A molecule is represented as a sequence of tokens drawn from a finite vocabulary $\\mathcal{V}$ that contains a distinguished end-of-sequence (EOS) token. Define a Markov Decision Process (MDP) in which the state at time $t$, denoted $s_t$, is the length-$t$ prefix of the SMILES string, and the action $a_t \\in \\mathcal{V}$ is the next token to append. An episode begins at the empty prefix and terminates when EOS is sampled or a maximum length $L_{\\max}$ is reached. The stochastic policy $\\pi_{\\theta}(a \\mid s)$ is parameterized by $\\theta$ and induces a trajectory $\\tau = (s_1,a_1,\\dots,s_{L(\\tau)},a_{L(\\tau)})$ with probability density\n$$\np_{\\theta}(\\tau) \\;=\\; \\prod_{t=1}^{L(\\tau)} \\pi_{\\theta}(a_t \\mid s_t),\n$$\nwhere $L(\\tau) \\in \\{1,\\dots,L_{\\max}\\}$ is the (random) episode length.\n\nLet the terminal reward be\n$$\nR(\\tau) \\;=\\; S\\!\\big(m(\\tau)\\big) \\;-\\; \\lambda\\, L(\\tau),\n$$\nwhere $S(m)$ is a differentiable property scoring function of the molecule $m(\\tau)$ realized by the completed SMILES (e.g., a predicted bioactivity), and $\\lambda \\ge 0$ is a length penalty coefficient that penalizes longer sequences. The optimization objective is the expected episodic return\n$$\nJ(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\big[\\,R(\\tau)\\,\\big].\n$$\nYou may employ a state-dependent control variate (baseline) $b(s_t)$ that does not depend on the action $a_t$ to reduce the variance of the gradient estimator.\n\nStarting only from fundamental definitions in Markov Decision Processes (MDPs), the likelihood-ratio identity, and the fact that adding a baseline $b(s_t)$ independent of $a_t$ does not change the expected policy gradient, derive the Reinforcement Learning with REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility (REINFORCE) update (i.e., an unbiased stochastic gradient estimator) for this SMILES-based sequence generation problem with the length penalty. Your derivation must make clear how the length penalty enters the estimator. Then, consider a form of teacher forcing in which the behavior policy at each time step is the mixture\n$$\nq_{\\eta}(a \\mid s) \\;=\\; \\eta\\, \\mu(a \\mid s) \\;+\\; (1-\\eta)\\, \\pi_{\\theta}(a \\mid s),\n$$\nwith $\\eta \\in [0,1]$ and a fixed data-driven teacher policy $\\mu(a \\mid s)$ that approximates next-token distributions from a curated molecular dataset. Suppose trajectories are sampled from $q_{\\eta}$ but a naive REINFORCE estimator is computed without importance sampling corrections. Explain qualitatively and quantitatively how this teacher forcing changes the state-action visitation distribution and derive a closed-form expression for the resulting bias in the gradient estimator in terms of $\\eta$, $\\mu$, $\\pi_{\\theta}$, and $R(\\tau)$.\n\nExpress your final answer as the closed-form analytic expression for the unbiased REINFORCE update direction for $\\nabla_{\\theta} J(\\theta)$ with the length penalty and baseline $b(s_t)$, written in terms of $\\pi_{\\theta}$, $S(m(\\tau))$, $\\lambda$, $L(\\tau)$, and $b(s_t)$. Do not include any numeric evaluation. No units are required. The final answer must be a single closed-form expression.",
            "solution": "The problem asks for two main derivations related to reinforcement learning for molecular sequence generation. First, the derivation of the REINFORCE policy gradient estimator including a length penalty and a baseline. Second, an analysis of the bias introduced by using a mixture policy for sampling without importance sampling corrections.\n\nPart 1: Derivation of the REINFORCE Update with Length Penalty\n\nThe objective is to maximize the expected episodic return, $J(\\theta)$, defined as:\n$$\nJ(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\big[\\,R(\\tau)\\,\\big]\n$$\nwhere $\\tau$ is a trajectory (a SMILES string), $p_{\\theta}(\\tau)$ is the probability of sampling that trajectory under the policy $\\pi_{\\theta}$, and $R(\\tau)$ is the terminal reward. The goal is to find the gradient of the objective with respect to the policy parameters, $\\nabla_{\\theta} J(\\theta)$, to perform gradient ascent.\n\nWe begin by writing the expectation as a sum over all possible trajectories:\n$$\nJ(\\theta) \\;=\\; \\sum_{\\tau} p_{\\theta}(\\tau) R(\\tau)\n$$\nThe gradient with respect to $\\theta$ is then:\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\nabla_{\\theta} \\sum_{\\tau} p_{\\theta}(\\tau) R(\\tau) \\;=\\; \\sum_{\\tau} (\\nabla_{\\theta} p_{\\theta}(\\tau)) R(\\tau)\n$$\nHere, we assume that the reward function $R(\\tau)$ does not depend on the policy parameters $\\theta$. We now employ the likelihood-ratio identity, also known as the score function identity, which states $\\nabla_{\\theta} p_{\\theta}(\\tau) = p_{\\theta}(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)$. Substituting this into the gradient expression gives:\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\sum_{\\tau} p_{\\theta}(\\tau) (\\nabla_{\\theta} \\ln p_{\\theta}(\\tau)) R(\\tau)\n$$\nThis can be rewritten as an expectation, which is the core of the Policy Gradient Theorem:\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\big[\\,(\\nabla_{\\theta} \\ln p_{\\theta}(\\tau)) R(\\tau)\\,\\big]\n$$\nThe probability of a trajectory $\\tau=(s_1, a_1, \\dots, s_{L(\\tau)}, a_{L(\\tau)})$ is given by the product of the policy probabilities at each step:\n$$\np_{\\theta}(\\tau) \\;=\\; \\prod_{t=1}^{L(\\tau)} \\pi_{\\theta}(a_t \\mid s_t)\n$$\nTaking the logarithm, we get:\n$$\n\\ln p_{\\theta}(\\tau) \\;=\\; \\sum_{t=1}^{L(\\tau)} \\ln \\pi_{\\theta}(a_t \\mid s_t)\n$$\nThe gradient of the log-probability of the trajectory is therefore a sum of the gradients of the log-probabilities of the actions taken:\n$$\n\\nabla_{\\theta} \\ln p_{\\theta}(\\tau) \\;=\\; \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)\n$$\nSubstituting this back into the expression for $\\nabla_{\\theta} J(\\theta)$:\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\left[\\, \\left( \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\right) R(\\tau) \\,\\right]\n$$\nThis is the vanilla policy gradient expression. A single-sample Monte Carlo estimator for this gradient, obtained from one trajectory $\\tau$, is $g = \\left( \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\right) R(\\tau)$.\n\nTo reduce the variance of this estimator, a baseline $b(s_t)$ that depends on the state but not the action can be introduced. The problem states that adding such a baseline does not change the expected policy gradient. Let's demonstrate this for the commonly used form where the baseline is subtracted from the reward for each term in the sum:\n$$\n\\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\left[\\, \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) (R(\\tau) - b(s_t)) \\,\\right]\n$$\nThis is an unbiased estimator if the expected value of the added term is zero:\n$$\n\\mathbb{E}_{\\tau \\sim p_{\\theta}}\\!\\left[\\, \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) (-b(s_t)) \\,\\right] \\;=\\; 0\n$$\nBy linearity of expectation and swapping the sum and expectation:\n$$\n-\\sum_{t=1}^{L_{\\max}} \\mathbb{E}_{\\tau \\sim p_{\\theta}, L(\\tau) \\ge t} \\!\\left[\\, \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) b(s_t) \\,\\right]\n$$\nThe expectation can be expanded over states and actions:\n$$\n= -\\sum_{t=1}^{L_{\\max}} \\mathbb{E}_{s_t \\sim p_{\\theta}(s_t)} \\!\\left[\\, \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(a_t|s_t)} \\!\\left[\\, \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) b(s_t) \\,\\right] \\,\\right]\n$$\nSince $b(s_t)$ does not depend on $a_t$, it can be pulled out of the inner expectation:\n$$\n= -\\sum_{t=1}^{L_{\\max}} \\mathbb{E}_{s_t \\sim p_{\\theta}(s_t)} \\!\\left[\\, b(s_t) \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(a_t|s_t)} \\!\\left[\\, \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\,\\right] \\,\\right]\n$$\nThe inner expectation is $\\mathbb{E}_{a_t \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t|s_t)] = \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} = \\nabla_{\\theta} \\sum_{a_t} \\pi_{\\theta}(a_t|s_t) = \\nabla_{\\theta}(1) = 0$. Thus, the entire expression is zero, and subtracting the baseline is unbiased.\n\nThe single-sample stochastic gradient estimator (the REINFORCE update direction) for a trajectory $\\tau$ is:\n$$\ng \\;=\\; \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\left( R(\\tau) - b(s_t) \\right)\n$$\nFinally, we substitute the given reward function $R(\\tau) = S(m(\\tau)) - \\lambda L(\\tau)$:\n$$\ng \\;=\\; \\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\left( S(m(\\tau)) - \\lambda L(\\tau) - b(s_t) \\right)\n$$\nThe length penalty $\\lambda L(\\tau)$ enters the estimator as a negative term in the advantage estimate $A_t = S(m(\\tau)) - \\lambda L(\\tau) - b(s_t)$. This term reduces the reward for longer sequences, thereby discouraging the policy from generating them. The gradient update at each step $t$ is weighted by this advantage, so trajectories with a larger penalty (longer length) will receive a smaller, or more negative, weighting factor, pushing the policy parameters $\\theta$ towards generating shorter sequences, all else being equal.\n\nPart 2: Bias from Mixture Policy without Importance Sampling\n\nThe second part of the problem considers a scenario where trajectories are not sampled from the agent's policy $\\pi_{\\theta}$, but from a mixture policy $q_{\\eta}(a|s) = \\eta \\mu(a|s) + (1-\\eta) \\pi_{\\theta}(a|s)$. A naive REINFORCE estimator is used, which ignores this fact and computes the gradient as if trajectories were sampled from $\\pi_{\\theta}$.\n\nQualitatively, using the mixture policy $q_{\\eta}$ for sampling alters the state-action visitation distribution. The teacher policy $\\mu$, derived from a curated dataset, guides the exploration towards regions of the state-action space that are known to correspond to valid or desirable molecular structures. The agent is thus exposed to states and actions it might not have discovered on its own. The naive estimator, however, fails to account for this guidance. It computes the gradient update $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)$ based on the agent's own policy probability. When a high-reward trajectory is found due to the teacher's guidance (i.e., $\\mu(a_t|s_t)$ was high), but the agent's policy for that action was low (i.e., $\\pi_{\\theta}(a_t|s_t)$ was low), the term $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t)$ can be large. The agent incorrectly credits itself for a \"discovery\" made by the teacher, leading to a biased gradient that does not point in the true direction of ascent for $J(\\theta)$.\n\nQuantitatively, the bias is the difference between the expected value of the naive estimator and the true gradient. The true gradient is $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]$.\nThe naive estimator for a trajectory $\\tau$ is $\\hat{g}_{\\text{naive}} = R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)$. Since trajectories are sampled from $q_{\\eta}$, the expected value of this estimator is:\n$$\n\\mathbb{E}_{\\tau \\sim q_{\\eta}}[\\hat{g}_{\\text{naive}}] \\;=\\; \\mathbb{E}_{\\tau \\sim q_{\\eta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]\n$$\nThe bias is the difference:\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim q_{\\eta}}[\\hat{g}_{\\text{naive}}] \\;-\\; \\nabla_{\\theta} J(\\theta) \\;=\\; \\mathbb{E}_{\\tau \\sim q_{\\eta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)] \\;-\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]\n$$\nThis can be expressed as a single expectation with respect to the agent's policy distribution $p_{\\theta}$ by using importance sampling weights:\n$$\n\\mathbb{E}_{\\tau \\sim q_{\\eta}}[f(\\tau)] \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} f(\\tau)\\right]\n$$\nApplying this to the bias expression gives:\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)\\right] - \\mathbb{E}_{\\tau \\sim p_{\\theta}}[R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)]\n$$\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[\\left(\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} - 1\\right) R(\\tau) \\nabla_{\\theta} \\ln p_{\\theta}(\\tau)\\right]\n$$\nThe importance weight $\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)}$ can be expressed in terms of the given policies:\n$$\n\\frac{q_{\\eta}(\\tau)}{p_{\\theta}(\\tau)} \\;=\\; \\frac{\\prod_{t=1}^{L(\\tau)} q_{\\eta}(a_t|s_t)}{\\prod_{t=1}^{L(\\tau)} \\pi_{\\theta}(a_t|s_t)} \\;=\\; \\prod_{t=1}^{L(\\tau)} \\frac{\\eta \\mu(a_t|s_t) + (1-\\eta) \\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} \\;=\\; \\prod_{t=1}^{L(\\tau)} \\left(1 + \\eta \\left(\\frac{\\mu(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} - 1\\right) \\right)\n$$\nSubstituting this ratio and $\\nabla_{\\theta} \\ln p_{\\theta}(\\tau) = \\sum_{k=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_k|s_k)$, we get the final closed-form expression for the bias:\n$$\n\\text{Bias} \\;=\\; \\mathbb{E}_{\\tau \\sim p_{\\theta}}\\left[ \\left( \\prod_{t=1}^{L(\\tau)} \\left[1 + \\eta \\left(\\frac{\\mu(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} - 1\\right) \\right] - 1 \\right) R(\\tau) \\left( \\sum_{k=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_k|s_k) \\right) \\right]\n$$\nThis expression captures the bias in terms of $\\eta$, $\\mu$, and $\\pi_{\\theta}$, showing how the deviation of the sampling distribution from the policy distribution, weighted by the reward, introduces a systematic error in the gradient estimate.",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{L(\\tau)} \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid s_t) \\left( S(m(\\tau)) - \\lambda L(\\tau) - b(s_t) \\right)}\n$$"
        },
        {
            "introduction": "A critical challenge in generative chemistry is ensuring that the molecules produced by an agent are chemically valid. An RL agent that edits molecular graphs can easily propose invalid structures, such as atoms exceeding their natural valence. This advanced, hands-on practice  tackles this issue by having you implement a projection operator that maps any invalid graph to the nearest valid chemical structure, exploring a practical solution to a ubiquitous problem and analyzing its theoretical impact on learning.",
            "id": "3861957",
            "problem": "Consider molecular graphs represented as labeled, undirected graphs with bond orders. Let the set of allowed heavy-atom labels be $\\mathcal{A} = \\{\\mathrm{C}, \\mathrm{N}, \\mathrm{O}\\}$, with valence limits given by the function $\\nu$ defined by $\\nu(\\mathrm{C}) = 4$, $\\nu(\\mathrm{N}) = 3$, and $\\nu(\\mathrm{O}) = 2$. Bonds are encoded by the set of bond orders $\\mathcal{B} = \\{0,1,2\\}$, where $0$ denotes no bond, $1$ denotes a single bond, and $2$ denotes a double bond. A graph $G$ has a node set $V_G$ and an adjacency matrix $A_G \\in \\mathcal{B}^{|V_G|\\times|V_G|}$ that is symmetric with zero diagonal. Each node $v \\in V_G$ carries a label $\\ell_G(v) \\in \\mathcal{A}$ or a placeholder label $\\mathrm{X}$ for an unknown or invalid atom type. A graph $G$ is valid if and only if: (i) for every node $v \\in V_G$, $\\sum_{u \\in V_G} A_G[v,u] \\le \\nu(\\ell_G(v))$, (ii) $G$ is connected when considering edges with strictly positive bond order, and (iii) all labels $\\ell_G(v)$ belong to $\\mathcal{A}$. Unused valences are assumed to be saturated by implicit hydrogen atoms and therefore do not invalidate the structure.\n\nDefine an edit distance $d(G,H)$ between two graphs $G$ and $H$ as follows. Let $N = \\max(|V_G|,|V_H|)$. Pad the smaller graph with $N - |V|$ placeholder nodes labeled $\\mathrm{PAD}$ and adjacency rows and columns identically zero. Define a bijective mapping $\\sigma$ from the index set $\\{1,\\dots,N\\}$ of the padded $G$ to the index set $\\{1,\\dots,N\\}$ of the padded $H$. For a given mapping $\\sigma$, define the label mismatch cost\n$$\nC_\\ell(G,H,\\sigma) = \\sum_{i=1}^{N} \\mathbf{1}\\big[\\ell_G(i) \\ne \\ell_H(\\sigma(i))\\big],\n$$\nwhere $\\ell_G(i)$ denotes the label of the $i$-th node in padded $G$ (with $\\mathrm{PAD}$ allowed), and similarly for $\\ell_H$. Define the edge mismatch cost\n$$\nC_e(G,H,\\sigma) = \\sum_{1 \\le i < j \\le N} \\mathbf{1}\\big[A_G[i,j] \\ne A_H[\\sigma(i),\\sigma(j)]\\big].\n$$\nThe edit distance is then\n$$\nd(G,H) = \\min_{\\sigma \\in S_N} \\big( C_\\ell(G,H,\\sigma) + C_e(G,H,\\sigma) \\big),\n$$\nwhere $S_N$ denotes the set of all permutations on $N$ elements. This distance counts a unit cost for each node insertion or deletion (via mismatches with $\\mathrm{PAD}$), each node label change, each edge insertion or deletion, and each bond-order change. Let $\\mathcal{V}$ be the set of all valid graphs with at most $3$ heavy atoms, i.e., $|V_H| \\le 3$ for all $H \\in \\mathcal{V}$, with labels in $\\mathcal{A}$ and bond orders in $\\mathcal{B}$, satisfying the validity criteria above.\n\nDefine the projection operator $\\Pi(G)$ that maps any (possibly invalid) graph $G$ to the nearest valid graph under the specified edit distance:\n$$\n\\Pi(G) = \\underset{H \\in \\mathcal{V}}{\\arg\\min}\\; d(G,H),\n$$\nwith deterministic tie-breaking as follows: among all minimizers, choose the one with smallest number of heavy atoms $|V_H|$; if still tied, choose the one with smallest total bond order $\\sum_{1 \\le i < j \\le |V_H|} A_H[i,j]$; if still tied, choose the one whose multiset of labels has lexicographically minimal order under the ordering $\\mathrm{C} \\prec \\mathrm{N} \\prec \\mathrm{O}$.\n\nNow consider an off-policy reinforcement learning scenario for molecular discovery formalized as a Markov Decision Process (MDP) $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$, where states $s \\in \\mathcal{S}$ are molecular graphs (valid or invalid), actions $a \\in \\mathcal{A}$ are graph-edit operations, $P$ is the transition kernel, $r$ is a reward function, and $\\gamma \\in (0,1)$ is the discount factor. Assume the reward $r(s)$ is $L_r$-Lipschitz with respect to the edit distance $d$, and the value function $V^\\pi(s)$ under a target policy $\\pi$ is $L_V$-Lipschitz with respect to $d$. When intermediate invalid states arise, suppose the learner projects them via $\\hat{s} = \\Pi(s)$ before computing temporal-difference targets. The one-step temporal-difference target discrepancy induced by projection can be bounded by\n$$\n\\Delta(s) \\le L_r \\, d(s,\\hat{s}) + \\gamma L_V \\, \\mathbb{E}\\big[ d(s', \\hat{s}') \\mid s \\big],\n$$\nwhere $s'$ denotes the next state and $\\hat{s}' = \\Pi(s')$. For the purpose of this problem, approximate the bound by replacing the conditional expectation with its upper bound $d(s,\\hat{s})$, yielding\n$$\nB(G) = \\big(L_r + \\gamma L_V\\big) \\, d\\big(G,\\Pi(G)\\big).\n$$\n\nYour task is to implement the projection operator $\\Pi(G)$ for graphs with up to $3$ heavy atoms, compute the nearest valid graph under $d$, and analyze the effect on off-policy learning bias using the bound $B(G)$ above. Use the following parameterization: $L_r = 1$, $L_V = 1$, and $\\gamma = 0.95$.\n\nTest Suite. For each test case $G_i$, you are given a label list and an adjacency matrix. Labels outside $\\mathcal{A}$ are denoted by $\\mathrm{X}$ and represent invalid atoms. The adjacency matrix entries may exceed the allowed bond orders, representing invalid bonds. The four test cases are:\n\n- Case $1$ (general invalid, mixed labels and bonds): labels $[\\mathrm{C}, \\mathrm{O}, \\mathrm{N}]$, adjacency\n$$\n\\begin{bmatrix}\n0 & 3 & 0 \\\\\n3 & 0 & 2 \\\\\n0 & 2 & 0\n\\end{bmatrix}.\n$$\n\n- Case $2$ (already valid, boundary): labels $[\\mathrm{C}, \\mathrm{O}]$, adjacency\n$$\n\\begin{bmatrix}\n0 & 2 \\\\\n2 & 0\n\\end{bmatrix}.\n$$\n\n- Case $3$ (invalid single-node label, tie-case across single-atom graphs): labels $[\\mathrm{X}]$, adjacency\n$$\n\\begin{bmatrix}\n0\n\\end{bmatrix}.\n$$\n\n- Case $4$ (empty graph, edge case): labels $[\\;]$, adjacency is the $0 \\times 0$ zero matrix.\n\nFor each case, compute: (i) the number of heavy atoms in the projected graph $|V_{\\Pi(G_i)}|$, (ii) the sum of bond orders over the upper triangle of the adjacency matrix of $\\Pi(G_i)$, (iii) the minimal edit distance $d(G_i,\\Pi(G_i))$, and (iv) the off-policy bias bound $B(G_i)$ with $\\gamma = 0.95$, $L_r = 1$, and $L_V = 1$. Express $B(G_i)$ as a float rounded to four decimal places. All other quantities must be integers.\n\nFinal Output Format. Your program should produce a single line containing the results for the four cases as a comma-separated list enclosed in square brackets, where each caseâ€™s result is a list of the four quantities described above. For example: $[[n_1,b_1,d_1,B_1],[n_2,b_2,d_2,B_2],[n_3,b_3,d_3,B_3],[n_4,b_4,d_4,B_4]]$, where each $n_i$, $b_i$, and $d_i$ is an integer and each $B_i$ is a float rounded to four decimal places.",
            "solution": "The solution to this problem involves implementing the projection operator $\\Pi(G)$ and applying it to the four test cases. This process can be broken down into four main steps.\n\n**Step 1: Generate the Set of Valid Graphs $\\mathcal{V}$**\nFirst, we must exhaustively generate the set of all valid molecular graphs $\\mathcal{V}$ containing up to 3 heavy atoms from the set $\\{\\mathrm{C}, \\mathrm{N}, \\mathrm{O}\\}$. A graph is considered valid if it satisfies three conditions:\n1.  **Valence:** For every atom, the sum of orders of its incident bonds does not exceed its maximum valence ($\\nu(\\mathrm{C})=4, \\nu(\\mathrm{N})=3, \\nu(\\mathrm{O})=2$).\n2.  **Connectivity:** The graph is connected (considering only bonds with order > 0).\n3.  **Labels:** All atom labels are from the allowed set.\nThis generation process can be done by iterating through all possible combinations of 1, 2, and 3 atoms, and all possible non-isomorphic bond configurations, filtering for the ones that meet the validity criteria.\n\n**Step 2: Implement the Graph Edit Distance $d(G, H)$**\nThe specified edit distance $d(G, H)$ requires finding the minimum cost to transform graph $G$ to graph $H$. Since the number of atoms is small ($N \\le 3$), this can be computed by trying all $N!$ possible permutations of node mappings between the two graphs (after padding the smaller graph with 'PAD' nodes to equalize size). For each permutation, the cost is the sum of label mismatches and bond mismatches. The minimum cost over all permutations is the distance.\n\n**Step 3: Implement the Projection Operator $\\Pi(G)$**\nWith the set $\\mathcal{V}$ and the distance function $d$, the projection $\\Pi(G)$ is found by calculating the distance from the input graph $G$ to every valid graph $H \\in \\mathcal{V}$. The graph(s) $H$ that yield the minimum distance are identified. If there is more than one such graph, the tie-breaking rules are applied in order:\n1.  Choose the graph with the fewest heavy atoms.\n2.  Then, the one with the smallest sum of bond orders.\n3.  Finally, the one whose sorted list of atom labels comes first lexicographically.\nThis process yields a single, unique nearest valid graph $\\Pi(G)$.\n\n**Step 4: Compute Results for Test Cases**\nApplying this procedure to each of the four test cases yields the projected graph $\\Pi(G_i)$ and the minimum distance $d_i = d(G_i, \\Pi(G_i))$. The required metrics are then calculated:\n- $n_i$: Number of atoms in $\\Pi(G_i)$.\n- $b_i$: Sum of bond orders in $\\Pi(G_i)$.\n- $d_i$: The minimum edit distance.\n- $B_i$: The bias bound, calculated as $B_i = (L_r + \\gamma L_V)d_i = (1 + 0.95 \\times 1)d_i = 1.95 \\times d_i$.\n\nThe results for each case are as follows:\n\n-   **Case 1:** The input graph is highly invalid due to out-of-range bond orders and valence violations. The nearest valid graph is found to be formaldehyde (C=O), which has 2 atoms and a total bond order of 2. The edit distance is 3 (1 for deleting atom N, 1 for changing bond C-O from 3 to 2, 1 for deleting bond O-N).\n    - $n_1=2, b_1=2, d_1=3, B_1 = 1.95 \\times 3 = 5.8500$.\n-   **Case 2:** The input graph is C=O, which is already a valid graph in $\\mathcal{V}$. The projection is the graph itself, and the distance is 0.\n    - $n_2=2, b_2=2, d_2=0, B_2 = 1.95 \\times 0 = 0.0000$.\n-   **Case 3:** The input is an invalid single atom 'X'. The nearest valid graphs are single atoms C, N, and O, all at a distance of 1 (one label change). The tie-breaking rules select 'C' due to lexicographical order.\n    - $n_3=1, b_3=0, d_3=1, B_3 = 1.95 \\times 1 = 1.9500$.\n-   **Case 4:** The input is the empty graph. The nearest valid graphs are the single-atom graphs, all at a distance of 1 (one node insertion). The tie-breaking rules again select 'C'.\n    - $n_4=1, b_4=0, d_4=1, B_4 = 1.95 \\times 1 = 1.9500$.\n\nThese calculations lead to the final numerical result.",
            "answer": "[[2,2,3,5.8500],[2,2,0,0.0000],[1,0,1,1.9500],[1,0,1,1.9500]]"
        }
    ]
}