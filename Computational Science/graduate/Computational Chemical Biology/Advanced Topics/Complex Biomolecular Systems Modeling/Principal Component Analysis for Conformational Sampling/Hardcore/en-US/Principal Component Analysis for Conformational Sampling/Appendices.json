{
    "hands_on_practices": [
        {
            "introduction": "This practice focuses on the fundamental operation of using a pre-computed PCA model. By deriving and applying the projection formula, you will understand how to place a new molecular conformation into the low-dimensional landscape defined by a training ensemble, a critical step for classifying or analyzing novel structures. ",
            "id": "3859043",
            "problem": "Consider a molecular dynamics ensemble of protein backbone conformations represented by a $d$-dimensional feature vector $x \\in \\mathbb{R}^{d}$ constructed from internal coordinate descriptors (e.g., torsion-angle derived features) after a fixed featurization and alignment procedure. Principal Component Analysis (PCA) is performed on a training set of $n$ conformations $\\{x_{i}\\}_{i=1}^{n}$ using the standard mean-centering preprocessing, where the training mean is $\\bar{x} \\in \\mathbb{R}^{d}$ and the empirical covariance matrix is defined by\n$$\nS \\equiv \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})(x_{i} - \\bar{x})^{\\top}.\n$$\nLet $S$ have the orthonormal eigen-decomposition $S = V \\Lambda V^{\\top}$ with $V \\in \\mathbb{R}^{d \\times d}$ containing orthonormal eigenvectors as columns and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ diagonal with eigenvalues ordered nonincreasingly. The first $k$ columns of $V$ are called the loading vectors for the leading $k$ principal components.\n\nStarting strictly from the above definitions and properties of orthonormal bases, derive the expression for the coefficients that represent a new conformation $x_{\\text{new}} \\in \\mathbb{R}^{d}$ in the $k$-dimensional principal component basis associated with the training covariance $S$. Then, explain why applying the identical preprocessing pipeline (including featurization, alignment, and mean-centering with the training mean $\\bar{x}$) to $x_{\\text{new}}$ is necessary to obtain scientifically meaningful projections.\n\nFinally, evaluate the derived expression for the following concrete case with mean-centering only (no scaling):\n- Dimension $d = 4$ and number of principal components $k = 2$.\n- The loading matrix for the first two principal components is\n$$\nV_{(2)} = \\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2} \\\\\n\\frac{1}{2}  -\\frac{1}{2} \\\\\n\\frac{1}{2}  \\frac{1}{2} \\\\\n\\frac{1}{2}  -\\frac{1}{2}\n\\end{pmatrix}.\n$$\n- The training mean vector is\n$$\n\\bar{x} = \\begin{pmatrix}\n0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0\n\\end{pmatrix}.\n$$\n- The new conformation is\n$$\nx_{\\text{new}} = \\begin{pmatrix}\n0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1\n\\end{pmatrix}.\n$$\n\nCompute the two principal component coefficients for $x_{\\text{new}}$ using the derived expression and present the pair of values as a single row vector. Round your final numerical values to four significant figures. No physical units are required for the final numerical answer.",
            "solution": "The problem statement is analyzed and validated before proceeding to a solution.\n\n### Step 1: Problem Validation\n\nThe givens are extracted verbatim:\n- A $d$-dimensional feature vector $x \\in \\mathbb{R}^{d}$ represents a protein backbone conformation.\n- The training set is $\\{x_{i}\\}_{i=1}^{n}$.\n- Preprocessing is mean-centering with training mean $\\bar{x} \\in \\mathbb{R}^{d}$.\n- The empirical covariance matrix is $S \\equiv \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})(x_{i} - \\bar{x})^{\\top}$.\n- The eigen-decomposition of $S$ is $S = V \\Lambda V^{\\top}$, with $V \\in \\mathbb{R}^{d \\times d}$ having orthonormal eigenvectors as columns and $\\Lambda$ being a diagonal matrix of non-increasing eigenvalues.\n- The first $k$ columns of $V$ are the loading vectors.\n- A new conformation is given by $x_{\\text{new}} \\in \\mathbb{R}^{d}$.\n- For the numerical case:\n  - $d = 4$ and $k = 2$.\n  - Loading matrix for the first two components: $V_{(2)} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  -\\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix}$.\n  - Training mean vector: $\\bar{x} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0 \\end{pmatrix}$.\n  - New conformation: $x_{\\text{new}} = \\begin{pmatrix} 0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}$.\n\nThe problem is subjected to validation:\n- **Scientifically Grounded**: The problem describes a standard and fundamental application of Principal Component Analysis (PCA) to conformational data from molecular dynamics, a core technique in computational biophysics. The definitions of the covariance matrix, its eigen-decomposition, and principal components are standard and correct.\n- **Well-Posed**: The problem is well-posed. It asks for a standard derivation, a conceptual explanation, and a concrete calculation for which all necessary data are provided. The existence of a unique and meaningful solution is guaranteed.\n- **Objective**: The problem is stated in precise mathematical terms, free of ambiguity or subjectivity.\n- **Consistency**: The provided loading matrix $V_{(2)}$ consists of two columns. Let these be $v_1$ and $v_2$. We check for orthonormality: $v_1^\\top v_1 = (\\frac{1}{2})^2 \\times 4 = 1$, $v_2^\\top v_2 = (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 = 1$, and $v_1^\\top v_2 = \\frac{1}{4} - \\frac{1}{4} + \\frac{1}{4} - \\frac{1}{4} = 0$. The columns are indeed orthonormal, confirming the internal consistency of the problem statement.\n\nThe problem is deemed valid as it is scientifically sound, self-contained, consistent, and well-posed.\n\n### Step 2: Solution\n\n#### Derivation of the Principal Component Coefficients\n\nThe principal components, which are the columns of the matrix $V$, form an orthonormal basis for the space $\\mathbb{R}^d$. Let these column vectors be denoted by $\\{v_j\\}_{j=1}^d$. The core idea of PCA is to describe the data's variance relative to its mean, $\\bar{x}$. Therefore, any new data point, $x_{\\text{new}}$, must first be transformed into this centered coordinate system. The centered vector is:\n$$\nx'_{\\text{new}} = x_{\\text{new}} - \\bar{x}\n$$\nWe wish to express this centered vector, $x'_{\\text{new}}$, as a linear combination of the first $k$ principal component basis vectors. The full representation in the $d$-dimensional basis is:\n$$\nx'_{\\text{new}} = \\sum_{j=1}^{d} c_j v_j\n$$\nwhere $c_j$ is the coefficient, or score, corresponding to the $j$-th principal component $v_j$.\n\nTo find the value of a specific coefficient, say $c_i$, we utilize the orthonormality of the basis vectors. Taking the dot product (inner product) of both sides of the equation with the vector $v_i$:\n$$\nv_i^{\\top} x'_{\\text{new}} = v_i^{\\top} \\left( \\sum_{j=1}^{d} c_j v_j \\right)\n$$\nBy the linearity of the dot product, we can move $v_i^{\\top}$ inside the summation:\n$$\nv_i^{\\top} x'_{\\text{new}} = \\sum_{j=1}^{d} c_j (v_i^{\\top} v_j)\n$$\nSince the basis $\\{v_j\\}$ is orthonormal, the inner product $v_i^{\\top} v_j$ is equal to the Kronecker delta, $\\delta_{ij}$, which is $1$ if $i=j$ and $0$ if $i \\neq j$.\n$$\nv_i^{\\top} x'_{\\text{new}} = \\sum_{j=1}^{d} c_j \\delta_{ij}\n$$\nThe sum on the right-hand side collapses to a single term, $c_i$:\n$$\nc_i = v_i^{\\top} x'_{\\text{new}} = v_i^{\\top} (x_{\\text{new}} - \\bar{x})\n$$\nThis expression gives the coefficient for the $i$-th principal component. To find the set of coefficients for the first $k$ components, we collect the coefficients $\\{c_1, c_2, \\dots, c_k\\}$. This can be expressed in matrix form. Let $V_{(k)} \\in \\mathbb{R}^{d \\times k}$ be the matrix whose columns are the first $k$ eigenvectors, $v_1, \\dots, v_k$. Let $c \\in \\mathbb{R}^k$ be the column vector of coefficients. Then:\n$$\nc = \\begin{pmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_k \\end{pmatrix} = \\begin{pmatrix} v_1^{\\top} \\\\ v_2^{\\top} \\\\ \\vdots \\\\ v_k^{\\top} \\end{pmatrix} (x_{\\text{new}} - \\bar{x}) = V_{(k)}^{\\top} (x_{\\text{new}} - \\bar{x})\n$$\nThis is the derived expression for the coefficients that represent the new conformation in the $k$-dimensional principal component basis.\n\n#### Justification for Preprocessing Pipeline\n\nThe application of an identical preprocessing pipeline is a critical requirement for scientific validity.\n1.  **Shared Frame of Reference**: PCA identifies a new coordinate system (the principal components) whose origin is the mean of the training data, $\\bar{x}$, and whose axes are aligned with the directions of maximum variance within that training data. The covariance matrix $S$ and its eigenvectors $V$ are defined entirely with respect to this statistical context. Projecting a new data point $x_{\\text{new}}$ onto this basis is only meaningful if $x_{\\text{new}}$ is first expressed in the same coordinate system. Subtracting the *training mean* $\\bar{x}$ from $x_{\\text{new}}$ translates the new point into this shared frame of reference. Failure to do so would be analogous to measuring a location relative to one coordinate origin (e.g., the global origin) but plotting it on a map centered elsewhere, yielding a meaningless position.\n2.  **Consistent Feature Space**: The featurization and alignment procedures are even more fundamental. They define the very vector space $\\mathbb{R}^d$ in which the analysis takes place. A conformation is not inherently a vector; it is transformed into one by a specific set of rules (e.g., calculating all $\\phi/\\psi$ torsion angles). If $x_{\\text{new}}$ were featurized or aligned differently from the training set, it would exist in a different feature space altogether. Projecting a vector from one space onto a basis defined in another is a mathematically and scientifically invalid operation. Therefore, to compare or represent $x_{\\text{new}}$ in the context of the conformational landscape learned from the training set, it must undergo the exact same sequence of transformations.\n\n#### Numerical Evaluation\n\nWe are given:\n- Dimension $d = 4$ and number of principal components $k = 2$.\n- The loading matrix $V_{(2)} = \\begin{pmatrix} 0.5  0.5 \\\\ 0.5  -0.5 \\\\ 0.5  0.5 \\\\ 0.5  -0.5 \\end{pmatrix}$.\n- The training mean $\\bar{x} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0 \\end{pmatrix}$.\n- The new conformation $x_{\\text{new}} = \\begin{pmatrix} 0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}$.\n\nFirst, we compute the centered vector $x'_{\\text{new}} = x_{\\text{new}} - \\bar{x}$:\n$$\nx'_{\\text{new}} = \\begin{pmatrix} 0.3 \\\\ 0.1 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix} - \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.3 - 0.2 \\\\ 0.1 - (-0.1) \\\\ 0.0 - 0.3 \\\\ -0.1 - 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.1 \\\\ 0.2 \\\\ -0.3 \\\\ -0.1 \\end{pmatrix}\n$$\nNext, we compute the coefficient vector $c \\in \\mathbb{R}^2$ using the derived expression $c = V_{(2)}^{\\top} x'_{\\text{new}}$. The transpose of $V_{(2)}$ is:\n$$\nV_{(2)}^{\\top} = \\begin{pmatrix} 0.5  0.5  0.5  0.5 \\\\ 0.5  -0.5  0.5  -0.5 \\end{pmatrix}\n$$\nNow we perform the matrix-vector multiplication:\n$$\nc = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} 0.5  0.5  0.5  0.5 \\\\ 0.5  -0.5  0.5  -0.5 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ 0.2 \\\\ -0.3 \\\\ -0.1 \\end{pmatrix}\n$$\nThe first coefficient $c_1$ is:\n$$\nc_1 = (0.5)(0.1) + (0.5)(0.2) + (0.5)(-0.3) + (0.5)(-0.1) = 0.05 + 0.10 - 0.15 - 0.05 = -0.05\n$$\nThe second coefficient $c_2$ is:\n$$\nc_2 = (0.5)(0.1) + (-0.5)(0.2) + (0.5)(-0.3) + (-0.5)(-0.1) = 0.05 - 0.10 - 0.15 + 0.05 = -0.15\n$$\nThe resulting coefficient vector is $c = \\begin{pmatrix} -0.05 \\\\ -0.15 \\end{pmatrix}$. The problem requires the answer as a single row vector, with values rounded to four significant figures.\n- $-0.05$ becomes $-0.05000$.\n- $-0.15$ becomes $-0.1500$.\nThe final row vector is $\\begin{pmatrix} -0.05000  -0.1500 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.05000  -0.1500\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the concept of projection, this exercise explores the inverse process: reconstruction from a truncated Principal Component Analysis (PCA) representation. You will implement an algorithm to approximate the original high-dimensional coordinates from a few principal components and quantify the information loss using Root Mean Square Deviation (RMSD). This provides a tangible measure of how well a low-dimensional model captures the essential structural features. ",
            "id": "3859135",
            "problem": "You are given an aligned molecular dynamics trajectory represented by a matrix of Cartesian coordinates, where each row is a frame and each row concatenates all atomic coordinates for that frame. In computational chemical biology, Principal Component Analysis (PCA) of Cartesian coordinates is a widely used approach for conformational sampling, where frames are assumed pre-aligned to remove global translations and rotations. The task is to construct an algorithm that, given a PCA representation truncated to a specified number of components, reconstructs approximate Cartesian coordinates and quantifies reconstruction error via Root Mean Square Deviation (RMSD).\n\nStart from the following accepted bases:\n- The definition of Principal Component Analysis (PCA) for a centered data matrix follows from the eigen-decomposition of the covariance matrix or equivalently from the singular value decomposition (SVD) of the centered data. Let $X \\in \\mathbb{R}^{F \\times 3N}$ be the data matrix, where $F$ is the number of frames and $N$ is the number of atoms. Let $\\mu \\in \\mathbb{R}^{3N}$ be the mean coordinate vector across frames, and let the centered matrix be $X_c = X - \\mathbf{1}\\mu^\\top$. The SVD is $X_c = W S V^\\top$, where $W \\in \\mathbb{R}^{F \\times F}$, $S \\in \\mathbb{R}^{F \\times 3N}$ is diagonal (rectangular, with nonnegative singular values), and $V \\in \\mathbb{R}^{3N \\times 3N}$ has orthonormal columns. The principal axes (loadings) are the columns of $V$. The scores are given by $Z = X_c V \\in \\mathbb{R}^{F \\times 3N}$.\n- Reconstruction from the first $k$ principal components is defined by $X_c^{(k)} = Z_{(:,1:k)} V_{(:,1:k)}^\\top$, and the approximate coordinates are $X^{(k)} = X_c^{(k)} + \\mathbf{1}\\mu^\\top$.\n- The Root Mean Square Deviation (RMSD) between an original frame $x_f \\in \\mathbb{R}^{3N}$ and its reconstruction $\\hat{x}_f \\in \\mathbb{R}^{3N}$ is defined as\n$$\n\\mathrm{RMSD}(f) = \\sqrt{\\frac{1}{N} \\sum_{j=1}^{N} \\left\\| r_j^{(f)} - \\hat{r}_j^{(f)} \\right\\|_2^2 },\n$$\nwhere $r_j^{(f)} \\in \\mathbb{R}^3$ is the coordinate of atom $j$ in frame $f$. This can be equivalently computed using the $3N$-vector representation as\n$$\n\\mathrm{RMSD}(f) = \\sqrt{\\frac{\\| x_f - \\hat{x}_f \\|_2^2 }{N}}.\n$$\n\nAll coordinates are assumed to be expressed in Angstroms, and all RMSD values must be reported in Angstroms.\n\nYour program must:\n1. Construct the specified synthetic test datasets using exact numeric values given below.\n2. For each dataset, compute PCA via SVD of the centered data, reconstruct coordinates using a truncated PCA representation of size $k$, and compute the average RMSD across all frames.\n3. Produce a single line of output containing the average RMSD values for all test cases, formatted as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$). Each $r_i$ is a floating-point number in Angstroms.\n\nUse the following test suite of parameter values. All vectors must be constructed exactly as specified.\n\nDataset A:\n- Number of atoms $N = 2$, number of frames $F = 5$, so each frame has $3N = 6$ Cartesian coordinates.\n- Mean coordinate vector:\n$$\n\\mu = [\\, 1.0,\\, 0.0,\\, -1.0,\\, 2.0,\\, 0.5,\\, -2.0 \\,].\n$$\n- Define the basis vectors and normalize each to unit length:\n$$\nv_1^{\\text{base}} = [\\, 1.0,\\, 0.0,\\, 0.0,\\, 1.0,\\, 0.0,\\, 0.0 \\,], \\quad\nv_2^{\\text{base}} = [\\, 0.0,\\, 1.0,\\, 0.0,\\, 0.0,\\, -1.0,\\, 0.0 \\,], \\quad\nv_3^{\\text{base}} = [\\, 0.0,\\, 0.0,\\, 1.0,\\, 0.0,\\, 0.0,\\, -1.0 \\,],\n$$\nand\n$$\nv_i = \\frac{v_i^{\\text{base}}}{\\| v_i^{\\text{base}} \\|_2} \\quad \\text{for} \\quad i \\in \\{1,2,3\\}.\n$$\n- Coefficients per frame:\n$$\na = [\\, 0.5,\\, -0.5,\\, 1.0,\\, -1.0,\\, 0.0 \\,], \\quad\nb = [\\, 0.2,\\, 0.2,\\, -0.2,\\, -0.2,\\, 0.0 \\,], \\quad\nc = [\\, 0.05,\\, -0.05,\\, 0.05,\\, -0.05,\\, 0.0 \\,].\n$$\n- Construct frames for Dataset A by\n$$\nx_f = \\mu + a_f v_1 + b_f v_2 + c_f v_3, \\quad \\text{for} \\quad f = 1,2,3,4,5.\n$$\n- Test cases for Dataset A:\n  - Case $A_1$: reconstruct with $k = 1$ and report the average RMSD across frames.\n  - Case $A_2$: reconstruct with $k = 0$ and report the average RMSD across frames.\n  - Case $A_3$: reconstruct with $k = 3$ and report the average RMSD across frames.\n  - Case $A_4$: reconstruct with $k = 10$ and report the average RMSD across frames. The algorithm must cap $k$ at the numerical rank of the centered data.\n\nAlgorithmic requirements:\n- Compute the empirical mean $\\mu$ from the data as $\\mu = \\frac{1}{F} \\sum_{f=1}^F x_f$.\n- Center the data: $X_c = X - \\mathbf{1}\\mu^\\top$.\n- Compute the SVD $X_c = W S V^\\top$, sort principal directions by descending singular values (they are provided in descending order by most SVD implementations).\n- Compute scores $Z = X_c V$.\n- For each $k$, reconstruct $X^{(k)} = Z_{(:,1:k)} V_{(:,1:k)}^\\top + \\mathbf{1}\\mu^\\top$.\n- Compute per-frame RMSD and then the average across frames:\n$$\n\\overline{\\mathrm{RMSD}} = \\frac{1}{F} \\sum_{f=1}^{F} \\mathrm{RMSD}(f),\n$$\nand express the result in Angstroms.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[A_1, A_2, A_3, A_4]$, where each element is the average RMSD in Angstroms for the corresponding test case.",
            "solution": "The problem requires the implementation of Principal Component Analysis (PCA) for a synthetically generated molecular dynamics trajectory. The objective is to reconstruct the trajectory coordinates using a truncated set of principal components and to quantify the reconstruction accuracy using the Root Mean Square Deviation (RMSD). The entire procedure must be performed according to the specified mathematical definitions and algorithmic steps.\n\nFirst, we formalize the problem. We are given a data matrix $X \\in \\mathbb{R}^{F \\times 3N}$, where $F$ is the number of frames and $N$ is the number of atoms. The coordinates are assumed to be pre-aligned.\n\nThe core of PCA involves an eigen-decomposition of the covariance matrix of the data. This is equivalent to performing a Singular Value Decomposition (SVD) on the centered data matrix, $X_c$.\n\n1.  **Data Centering**: The first step is to compute the mean coordinate vector $\\mu \\in \\mathbb{R}^{3N}$ by averaging across all $F$ frames:\n    $$\n    \\mu = \\frac{1}{F} \\sum_{f=1}^{F} x_f\n    $$\n    where $x_f$ is the row vector of coordinates for frame $f$. The data matrix $X$ is then centered by subtracting this mean from each frame's coordinates:\n    $$\n    X_c = X - \\mathbf{1}\\mu^\\top\n    $$\n    where $\\mathbf{1}$ is a column vector of ones of length $F$.\n\n2.  **Singular Value Decomposition (SVD)**: The SVD of the centered matrix $X_c$ is computed as:\n    $$\n    X_c = W S V^\\top\n    $$\n    Here, $W \\in \\mathbb{R}^{F \\times p}$, $S \\in \\mathbb{R}^{p \\times p}$ is a diagonal matrix of non-negative singular values $s_i$ sorted in descending order, and $V \\in \\mathbb{R}^{3N \\times p}$ is a matrix whose columns, $v_i$, are the principal components (PCs) or principal axes. The dimension $p$ is the rank of the matrix $X_c$, which is at most $\\min(F-1, 3N)$. The columns of $V$ form an orthonormal basis for the subspace spanned by the centered data.\n\n3.  **Projection and Scores**: The projection of the centered data onto the principal components yields the scores matrix $Z \\in \\mathbb{R}^{F \\times p}$:\n    $$\n    Z = X_c V\n    $$\n    From the SVD equation, we can substitute $X_c$ to find $Z = (W S V^\\top) V = W S (V^\\top V) = W S$, since $V^\\top V = I$. Each column of $Z$ represents the coordinates of the trajectory along the corresponding principal component.\n\n4.  **Truncated Reconstruction**: To reconstruct an approximation of the trajectory using only the first $k$ principal components, we use a truncated set of scores and principal components. Let $Z_{(:,1:k)}$ be the first $k$ columns of $Z$ and $V_{(:,1:k)}$ be the first $k$ columns of $V$. The reconstructed centered data is:\n    $$\n    X_c^{(k)} = Z_{(:,1:k)} V_{(:,1:k)}^\\top\n    $$\n    This is equivalent to summing the outer products of the first $k$ components, scaled by their singular values: $X_c^{(k)} = \\sum_{i=1}^{k} s_i w_i v_i^\\top$, where $w_i$ are the columns of $W$. Finally, the mean coordinate vector is added back to obtain the reconstructed coordinates in the original frame of reference:\n    $$\n    X^{(k)} = X_c^{(k)} + \\mathbf{1}\\mu^\\top\n    $$\n    The number of components $k$ must be less than or equal to the numerical rank of $X_c$. If a larger $k$ is requested, it must be capped at the rank.\n\n5.  **Error Quantification (RMSD)**: The error of the reconstruction for a single frame $f$ is quantified by the RMSD between the original coordinates $x_f$ and the reconstructed coordinates $x_f^{(k)}$. This is given by:\n    $$\n    \\mathrm{RMSD}(f) = \\sqrt{\\frac{\\| x_f - x_f^{(k)} \\|_2^2}{N}}\n    $$\n    where $N$ is the number of atoms. The overall reconstruction error is the average RMSD over all frames:\n    $$\n    \\overline{\\mathrm{RMSD}} = \\frac{1}{F} \\sum_{f=1}^{F} \\mathrm{RMSD}(f)\n    $$\n\nThe problem specifies a synthetic dataset (Dataset A) constructed from a set of orthonormal basis vectors and coefficients. The data matrix $X$ is generated with $F=5$ frames, $N=2$ atoms ($3N=6$ dimensions), and a given mean vector $\\mu$. The frames are constructed as $x_f = \\mu + a_f v_1 + b_f v_2 + c_f v_3$. Since the coefficient vectors $a, b, c$ are constructed to have a mean of zero, the empirical mean of the generated data $X$ will be exactly the construction vector $\\mu$. The data's variance is deliberately aligned with three orthogonal directions $v_1, v_2, v_3$. Consequently, PCA is expected to identify these directions as the principal components. The variance along these directions is determined by the sum of squares of the coefficients ($a_f, b_f, c_f$), which dictates the ordering of the discovered PCs.\n\nWe proceed by applying the above algorithm to the specified test cases for Dataset A with $k \\in \\{1, 0, 3, 10\\}$.\n- For $k=1$, we reconstruct using only the most significant principal component. The error will correspond to the motion captured by the neglected components.\n- For $k=0$, the reconstruction consists solely of the mean coordinates ($X^{(0)} = \\mathbf{1}\\mu^\\top$). The error is the total deviation of the trajectory from its average structure.\n- For $k=3$, we use all three components that were used to construct the data. Since the rank of the centered data is $3$, this should result in a nearly perfect reconstruction, limited only by floating-point precision.\n- For $k=10$, since the rank of the data is $3$, the number of components is capped at $k=3$, and the result must be identical to the $k=3$ case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA reconstruction and RMSD calculation problem for the given test cases.\n    \"\"\"\n    # Define the parameters for Dataset A\n    N = 2\n    F = 5\n    \n    # Mean coordinate vector\n    mu_construct = np.array([1.0, 0.0, -1.0, 2.0, 0.5, -2.0])\n    \n    # Base vectors for construction\n    v1_base = np.array([1.0, 0.0, 0.0, 1.0, 0.0, 0.0])\n    v2_base = np.array([0.0, 1.0, 0.0, 0.0, -1.0, 0.0])\n    v3_base = np.array([0.0, 0.0, 1.0, 0.0, 0.0, -1.0])\n    \n    # Normalize base vectors\n    v1 = v1_base / np.linalg.norm(v1_base)\n    v2 = v2_base / np.linalg.norm(v2_base)\n    v3 = v3_base / np.linalg.norm(v3_base)\n    \n    # Coefficients per frame\n    a = np.array([0.5, -0.5, 1.0, -1.0, 0.0])\n    b = np.array([0.2, 0.2, -0.2, -0.2, 0.0])\n    c = np.array([0.05, -0.05, 0.05, -0.05, 0.0])\n    \n    # Construct the data matrix X\n    X = np.zeros((F, 3 * N))\n    for f in range(F):\n        X[f, :] = mu_construct + a[f] * v1 + b[f] * v2 + c[f] * v3\n    \n    # Define test cases for Dataset A\n    test_cases_k = [1, 0, 3, 10]\n    results = []\n\n    for k in test_cases_k:\n        # Step 1: Compute empirical mean\n        mu_emp = np.mean(X, axis=0)\n        \n        # Step 2: Center the data\n        X_c = X - mu_emp\n        \n        # Step 3: Compute SVD\n        # full_matrices=False is more efficient\n        # U corresponds to W, s to singular values, Vt to V.T in the problem statement\n        U, s, Vt = np.linalg.svd(X_c, full_matrices=False)\n        \n        # Step 4: Cap k at the numerical rank of the centered data\n        # Numerical rank is the number of singular values above a small tolerance\n        numerical_rank = np.sum(s > 1e-12)\n        k_eff = min(k, numerical_rank)\n        \n        # Step 5: Reconstruct coordinates\n        if k_eff == 0:\n            X_k = np.tile(mu_emp, (F, 1))\n        else:\n            # Reconstruct centered data from truncated SVD\n            X_c_k = U[:, :k_eff] @ np.diag(s[:k_eff]) @ Vt[:k_eff, :]\n            # Add back the mean\n            X_k = X_c_k + mu_emp\n        \n        # Step 6: Compute average RMSD\n        # Calculate per-frame squared Euclidean distance\n        # ||x_f - x_f_k||^2\n        sq_diff = np.sum((X - X_k)**2, axis=1)\n        \n        # Calculate per-frame RMSD\n        # sqrt( (1/N) * ||x_f - x_f_k||^2 )\n        rmsd_per_frame = np.sqrt(sq_diff / N)\n        \n        # Average RMSD across all frames\n        avg_rmsd = np.mean(rmsd_per_frame)\n        \n        results.append(avg_rmsd)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice moves from analysis to synthesis, a key application of Principal Component Analysis (PCA) in enhanced sampling and drug design. You will develop a generative model that samples new conformations from the learned low-dimensional space and then filters them for physical plausibility. This demonstrates how PCA can be used to efficiently explore the conformational landscape beyond the original simulation data. ",
            "id": "3859035",
            "problem": "You are given a task that integrates principal component analysis (PCA) with conformational sampling for a coarse-grained peptide backbone model in computational chemical biology. The goal is to derive, implement, and validate a method to generate synthetic structures by sampling in principal component space and back-mapping to Cartesian coordinates, followed by filtering physically implausible conformations using simple steric checks. All molecular distances are to be treated in Ångströms, and any random number generation must be deterministic via a fixed pseudo-random seed.\n\nStart from the following fundamental base, which must guide your derivation and algorithmic design:\n- The input conformational ensemble is represented as a set of mean-free coordinate fluctuations of a molecular system, where each conformation is a vector of Cartesian coordinates of $N$ atoms, resulting in a vector of length $3N$.\n- The ensemble covariance matrix is defined from the mean-free data matrix as a symmetric, positive semi-definite matrix, and its eigen-decomposition yields orthonormal eigenvectors and non-negative eigenvalues.\n- Principal component analysis (PCA) is defined as the orthogonal transformation that diagonalizes the covariance matrix, projecting mean-free coordinates onto a basis where coordinate variances are ordered and decoupled.\n\nYou must implement the following, deriving the necessary transformations strictly from the base above:\n1. Construct a deterministic training ensemble of $T$ conformations for a backbone motif with $N = 6$ atoms labeled as $\\{N, CA, C, N, CA, C\\}$ arranged linearly along a backbone graph with bonds between successive atoms. Use a fixed pseudo-random seed, apply small random rigid rotations about the $z$-axis and independent Gaussian positional noise to a physically plausible mean structure, with all distances in Ångströms. The backbone graph is a simple chain such that the graph distance between atom indices $i$ and $j$ is $|i - j|$.\n2. Compute the $3N \\times 3N$ covariance matrix from the mean-free flattened coordinate ensemble and perform an eigen-decomposition to obtain ordered principal components. Use the ordering of eigenvalues from largest to smallest.\n3. Sample synthetic structures by drawing Gaussian coefficients in the principal component space of dimension $k$, with zero mean and component-wise variances proportional to the corresponding top $k$ eigenvalues, scaled by a dimensionless factor $s$. Back-map these samples to the full $3N$-dimensional Cartesian coordinate space using the top $k$ principal component directions and re-add the mean.\n4. Filter unphysical conformations using a simple steric check: for all atom pairs with graph distance greater than or equal to $3$ (i.e., $|i - j| >= 3$), reject a conformation if any Euclidean interatomic distance is strictly less than $\\alpha \\cdot (r_i + r_j)$, where $r_i$ and $r_j$ are the van der Waals radii of atoms $i$ and $j$ respectively, with $r_{\\mathrm{N}} = 1.55$ Å and $r_{\\mathrm{C}} = 1.70$ Å and $r_{\\mathrm{CA}} = r_{\\mathrm{C}}$. The angle unit for any rotation must be radians. Report an acceptance fraction defined as the number of sampled conformations that pass the steric filter divided by the total number of samples, expressed as a decimal.\n\nTest suite and required outputs:\n- Use a single, deterministic training ensemble with $T = 1000$ conformations generated from the mean structure with Gaussian positional noise of standard deviation $0.1$ Å and rotation angle standard deviation $0.05$ radians applied about the $z$-axis. All randomness must be controlled by a fixed pseudo-random seed.\n- Apply the method to the following four test cases, each specified as a tuple $(k, M, s, \\alpha)$, where $k$ is the number of principal components retained, $M$ is the number of synthetic samples to draw, $s$ is the dimensionless scaling of component variances, and $\\alpha$ is the steric scaling factor:\n    - Case $1$: $(k = 2, M = 200, s = 1.0, \\alpha = 1.0)$.\n    - Case $2$: $(k = 1, M = 200, s = 0.5, \\alpha = 1.0)$.\n    - Case $3$: $(k = 18, M = 200, s = 0.0, \\alpha = 1.0)$, where $18 = 3N$.\n    - Case $4$: $(k = 4, M = 200, s = 3.0, \\alpha = 1.0)$.\n- For each case, compute the acceptance fraction as a decimal float. Your program should produce a single line of output containing the results as a comma-separated list of four decimal numbers rounded to three decimal places and enclosed in square brackets (e.g., $[x_1,x_2,x_3,x_4]$).\n\nYour implementation must be a complete, runnable program that performs the ensemble generation, PCA, sampling, back-mapping, steric filtering, and aggregation of acceptance fractions for the four test cases, and prints the final single-line output in the specified format.",
            "solution": "The posed problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique, verifiable solution. It outlines a standard procedure in computational biophysics for analyzing and generating molecular structures using principal component analysis (PCA), a cornerstone of statistical data analysis. We shall now proceed with a formal derivation and solution.\n\nThe overarching goal is to generate a set of synthetic molecular conformations from a low-dimensional latent space defined by PCA and to filter these conformations for physical plausibility. The methodology involves four primary stages: 1. Generation of a training conformational ensemble. 2. Application of PCA to identify the principal modes of structural variation. 3. stochastic sampling in the principal component space and back-mapping to Cartesian coordinates. 4. Steric filtering of the generated structures.\n\nLet the system consist of $N$ atoms. A single conformation is represented by a vector $\\mathbf{x} \\in \\mathbb{R}^{3N}$ containing the flattened Cartesian coordinates $(x_1, y_1, z_1, x_2, y_2, z_2, \\dots, x_N, y_N, z_N)$. All coordinates and distances are in units of Ångströms (Å).\n\n1.  **Ensemble Generation and Covariance Analysis**\n\nWe begin by generating a training ensemble of $T$ conformations, denoted as $\\{\\mathbf{x}^{(m)}\\}_{m=1}^T$. This ensemble is created by applying stochastic perturbations to a reference mean structure. The coordinates of the $m$-th conformation form the vector $\\mathbf{x}^{(m)}$. These vectors are collected as rows of a data matrix $X$ of size $T \\times 3N$.\n\nThe first step in PCA is to center the data. We compute the mean conformation $\\bar{\\mathbf{x}}$ over the ensemble:\n$$\n\\bar{\\mathbf{x}} = \\frac{1}{T} \\sum_{m=1}^{T} \\mathbf{x}^{(m)}\n$$\nThe mean-free (or centered) conformations are given by $\\bar{\\mathbf{x}}^{(m)} = \\mathbf{x}^{(m)} - \\bar{\\mathbf{x}}$. The sample covariance matrix $C$, a $3N \\times 3N$ symmetric positive semi-definite matrix, is then computed from these centered vectors:\n$$\nC = \\frac{1}{T-1} \\sum_{m=1}^{T} \\bar{\\mathbf{x}}^{(m)} (\\bar{\\mathbf{x}}^{(m)})^T\n$$\nThe covariance matrix quantifies the correlated fluctuations between all pairs of atomic coordinates.\n\n2.  **Principal Component Analysis**\n\nPCA is performed by finding the eigenvalues and eigenvectors of the covariance matrix $C$. This is an eigen-decomposition problem:\n$$\nC \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i, \\quad i = 1, \\dots, 3N\n$$\nHere, $\\lambda_i$ are the eigenvalues and $\\mathbf{v}_i$ are the corresponding eigenvectors. The eigenvectors, called principal components (PCs), form an orthonormal basis that diagonalizes the covariance matrix. Each eigenvalue $\\lambda_i$ represents the variance of the data projected onto the corresponding eigenvector $\\mathbf{v}_i$. The PCs are ordered according to their corresponding eigenvalues in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{3N} \\ge 0$. Let $V$ be the orthogonal matrix whose columns are the ordered eigenvectors, $V = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_{3N}]$.\n\nAny mean-free conformation $\\bar{\\mathbf{x}}^{(m)}$ can be exactly represented as a linear combination of the PCs:\n$$\n\\bar{\\mathbf{x}}^{(m)} = \\sum_{i=1}^{3N} p_{i}^{(m)} \\mathbf{v}_i = V \\mathbf{p}^{(m)}\n$$\nwhere $\\mathbf{p}^{(m)}$ is the vector of principal coordinates for conformation $m$, obtained by projection: $\\mathbf{p}^{(m)} = V^T \\bar{\\mathbf{x}}^{(m)}$.\n\n3.  **Synthetic Conformation Generation**\n\nWe can generate new, synthetic conformations by sampling in the low-dimensional space spanned by the top $k$ principal components, where $k \\le 3N$. A new vector of principal coordinates, $\\mathbf{p}_{\\text{synth}}$, is generated. Its first $k$ components, $\\{c_i\\}_{i=1}^k$, are drawn from independent Gaussian distributions, while the remaining $3N-k$ components are set to zero. The distribution for each component $c_i$ is chosen to mimic the variance observed in the training data:\n$$\nc_i \\sim \\mathcal{N}(\\mu=0, \\sigma^2=s \\lambda_i), \\quad \\text{for } i=1, \\dots, k\n$$\nHere, $s$ is a dimensionless scaling factor that allows modulation of the sampled variance. The full vector of synthetic principal coordinates is $\\mathbf{p}_{\\text{synth}} = (c_1, c_2, \\dots, c_k, 0, \\dots, 0)^T$.\n\nThe synthetic conformation is then reconstructed in Cartesian coordinates via back-mapping. First, the mean-free synthetic conformation $\\bar{\\mathbf{x}}_{\\text{synth}}$ is obtained:\n$$\n\\bar{\\mathbf{x}}_{\\text{synth}} = \\sum_{i=1}^{k} c_i \\mathbf{v}_i\n$$\nThen, the mean conformation is added back to yield the final structure:\n$$\n\\mathbf{x}_{\\text{synth}} = \\bar{\\mathbf{x}}_{\\text{synth}} + \\bar{\\mathbf{x}}\n$$\n\n4.  **Steric Filtering**\n\nThe generated structures, particularly when sampled with large variance ($s > 1$) or from a limited number of PCs, may not be physically realistic. A common issue is steric clashes, where non-bonded atoms are unphysically close. We apply a filter to reject such structures. A conformation is deemed invalid if the Euclidean distance $d_{ij}$ between any pair of atoms $(i, j)$ that are not closely connected in the molecular graph violates a distance criterion.\n\nFor the given backbone with $N=6$ atoms, the graph distance is $|i-j|$. The check is performed for all pairs with $|i-j| \\ge 3$. A conformation is rejected if for any such pair:\n$$\nd_{ij}  \\alpha \\cdot (r_i + r_j)\n$$\nwhere $r_i$ and $r_j$ are the van der Waals radii of atoms $i$ and $j$, and $\\alpha$ is a scaling factor. The specified radii are $r_{\\mathrm{N}} = 1.55$ Å and $r_{\\mathrm{C}} = r_{\\mathrm{CA}} = 1.70$ Å. The acceptance fraction is the ratio of conformations that pass this filter to the total number of generated samples, $M$.\n\nFor the specific case where the variance scaling factor $s=0$, all generated coefficients $c_i$ will be exactly $0$. Consequently, $\\bar{\\mathbf{x}}_{\\text{synth}}$ will be the zero vector, and every synthetic conformation $\\mathbf{x}_{\\text{synth}}$ will be identical to the mean conformation $\\bar{\\mathbf{x}}$. The acceptance fraction will then be $1.0$ if the mean structure itself is sterically valid, and $0.0$ otherwise. Given that the mean structure is an average of physically plausible conformations, it is expected to be valid, leading to an acceptance fraction of $1.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs PCA-based conformational sampling and steric filtering for a coarse-grained peptide model.\n    \"\"\"\n    #\n    # --- GLOBAL PARAMETERS AND CONSTANTS ---\n    #\n    RANDOM_SEED = 0  # Fixed seed for deterministic randomness\n    \n    # Ensemble generation parameters\n    T = 1000  # Number of training conformations\n    N = 6     # Number of atoms\n    POS_NOISE_STD = 0.1  # Std dev for Gaussian positional noise (Å)\n    ROT_NOISE_STD = 0.05  # Std dev for z-axis rotation noise (radians)\n\n    # Atom properties\n    ATOM_LABELS = ['N', 'CA', 'C', 'N', 'CA', 'C']\n    VDW_RADII_MAP = {'N': 1.55, 'C': 1.70, 'CA': 1.70}\n    ATOM_RADII = np.array([VDW_RADII_MAP[label] for label in ATOM_LABELS])\n\n    # Bond lengths for mean structure (Å)\n    BOND_N_CA = 1.46\n    BOND_CA_C = 1.52\n    BOND_C_N_PEPTIDE = 1.33\n\n    # Test cases: (k, M, s, alpha)\n    test_cases = [\n        (2, 200, 1.0, 1.0),\n        (1, 200, 0.5, 1.0),\n        (18, 200, 0.0, 1.0),\n        (4, 200, 3.0, 1.0),\n    ]\n\n    #\n    # --- STEP 1: GENERATE TRAINING ENSEMBLE ---\n    #\n    # Define a simple, reproducible mean structure (linear along x-axis)\n    mean_structure_coords = np.zeros((N, 3))\n    x_pos = 0.0\n    mean_structure_coords[1] = [x_pos := x_pos + BOND_N_CA, 0, 0]\n    mean_structure_coords[2] = [x_pos := x_pos + BOND_CA_C, 0, 0]\n    mean_structure_coords[3] = [x_pos := x_pos + BOND_C_N_PEPTIDE, 0, 0]\n    mean_structure_coords[4] = [x_pos := x_pos + BOND_N_CA, 0, 0]\n    mean_structure_coords[5] = [x_pos := x_pos + BOND_CA_C, 0, 0]\n    \n    rng_ensemble = np.random.default_rng(RANDOM_SEED)\n\n    ensemble_matrix_X = np.zeros((T, N * 3))\n\n    for i in range(T):\n        # Generate random z-rotation\n        theta = rng_ensemble.normal(0, ROT_NOISE_STD)\n        cos_t, sin_t = np.cos(theta), np.sin(theta)\n        rotation_matrix = np.array([\n            [cos_t, -sin_t, 0],\n            [sin_t, cos_t, 0],\n            [0, 0, 1]\n        ])\n        \n        # Apply rotation to the mean structure\n        rotated_coords = mean_structure_coords @ rotation_matrix.T\n        \n        # Add independent Gaussian positional noise\n        noise = rng_ensemble.normal(0, POS_NOISE_STD, size=(N, 3))\n        perturbed_coords = rotated_coords + noise\n        \n        # Store flattened coordinates\n        ensemble_matrix_X[i, :] = perturbed_coords.flatten()\n\n    #\n    # --- STEP 2: PERFORM PRINCIPAL COMPONENT ANALYSIS ---\n    #\n    # Compute mean conformation and mean-free data\n    mean_conformation = np.mean(ensemble_matrix_X, axis=0)\n    mean_free_X = ensemble_matrix_X - mean_conformation\n\n    # Compute covariance matrix\n    # Using T-1 for the unbiased sample covariance estimator\n    covariance_matrix = (mean_free_X.T @ mean_free_X) / (T - 1)\n    \n    # Eigen-decomposition of the symmetric covariance matrix\n    # np.linalg.eigh returns eigenvalues in ascending order, so we reverse them\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    results = []\n    \n    rng_sampling = np.random.default_rng(RANDOM_SEED + 1) # Separate RNG for sampling\n\n    #\n    # --- STEPS 3  4: SAMPLING, BACK-MAPPING, AND FILTERING LOOP ---\n    #\n    for k, M, s, alpha in test_cases:\n        passed_count = 0\n        for _ in range(M):\n            # Step 3: Sample in PC space and back-map\n            \n            # Sample coefficients for the top k components\n            # Handle s=0 case where variance is 0, avoiding sqrt of negative tiny numbers\n            stdevs = np.sqrt(np.maximum(0, s * sorted_eigenvalues[:k]))\n            coeffs = rng_sampling.normal(0, stdevs)\n\n            # Reconstruct mean-free conformation\n            mean_free_synth = sorted_eigenvectors[:, :k] @ coeffs\n            \n            # Add mean to get full conformation\n            synth_conformation_flat = mean_free_synth + mean_conformation\n            synth_conformation = synth_conformation_flat.reshape((N, 3))\n\n            # Step 4: Steric filtering\n            is_valid = True\n            for i in range(N):\n                for j in range(i + 1, N):\n                    # Check only pairs with graph distance >= 3\n                    if abs(i - j) >= 3:\n                        dist_sq = np.sum((synth_conformation[i] - synth_conformation[j])**2)\n                        min_dist_sq = (alpha * (ATOM_RADII[i] + ATOM_RADII[j]))**2\n                        \n                        if dist_sq  min_dist_sq:\n                            is_valid = False\n                            break\n                if not is_valid:\n                    break\n            \n            if is_valid:\n                passed_count += 1\n        \n        acceptance_fraction = passed_count / M\n        results.append(acceptance_fraction)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}