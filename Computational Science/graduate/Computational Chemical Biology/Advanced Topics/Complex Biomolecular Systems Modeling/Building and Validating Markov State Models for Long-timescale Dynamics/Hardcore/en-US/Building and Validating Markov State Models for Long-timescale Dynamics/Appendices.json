{
    "hands_on_practices": [
        {
            "introduction": "The predictive power of a Markov State Model is fundamentally limited by the input features used to define its states. Choosing coordinates that poorly resolve the slow conformational changes of a system will result in a model that fails to capture the essential long-timescale dynamics. This practice introduces a principled method for feature selection using the Variational Approach for Markov Processes (VAMP), specifically the VAMP-2 score, to quantitatively compare competing featurizations and select the one that best preserves the slow dynamical information from your simulation data .",
            "id": "3838057",
            "problem": "You are given two competing featurizations of a stochastic dynamical process and asked to select which featurization better captures the slow dynamical modes based on the Variational Approach for Markov Processes (VAMP) with the squared objective (VAMP-2). You must implement a program that, for a small suite of synthetic cases, computes a cross-validated VAMP-2 score for each featurization and outputs which featurization is superior in each case.\n\nThe scientific foundation is as follows. Consider a discrete-time, stationary, ergodic stochastic process with state features $\\mathbf{x}_t \\in \\mathbb{R}^d$. For a fixed lag time $\\tau \\in \\mathbb{N}$, define the time-lagged pair $\\left(\\mathbf{x}_t, \\mathbf{x}_{t+\\tau}\\right)$. Let the sample means be $\\bar{\\mathbf{x}}_0 = \\mathbb{E}[\\mathbf{x}_t]$ and $\\bar{\\mathbf{x}}_\\tau = \\mathbb{E}[\\mathbf{x}_{t+\\tau}]$. Define the covariance and cross-covariance matrices by\n$$\n\\mathbf{C}_{00} = \\mathbb{E}\\left[\\left(\\mathbf{x}_t - \\bar{\\mathbf{x}}_0\\right)\\left(\\mathbf{x}_t - \\bar{\\mathbf{x}}_0\\right)^\\top\\right], \\quad\n\\mathbf{C}_{\\tau\\tau} = \\mathbb{E}\\left[\\left(\\mathbf{x}_{t+\\tau} - \\bar{\\mathbf{x}}_\\tau\\right)\\left(\\mathbf{x}_{t+\\tau} - \\bar{\\mathbf{x}}_\\tau\\right)^\\top\\right],\n$$\n$$\n\\mathbf{C}_{0\\tau} = \\mathbb{E}\\left[\\left(\\mathbf{x}_t - \\bar{\\mathbf{x}}_0\\right)\\left(\\mathbf{x}_{t+\\tau} - \\bar{\\mathbf{x}}_\\tau\\right)^\\top\\right].\n$$\nThe VAMP-2 score is the squared Frobenius norm of the whitened time-lagged cross-covariance,\n$$\n\\mathrm{VAMP2} = \\left\\|\\mathbf{C}_{00}^{-1/2}\\,\\mathbf{C}_{0\\tau}\\,\\mathbf{C}_{\\tau\\tau}^{-1/2}\\right\\|_F^2,\n$$\nwhich, under the variational principle for reversible and nonreversible Markov processes, estimates the sum of squared singular values of the optimal finite-dimensional approximation of the Koopman operator at lag time $\\tau$. In practice, the expectations are replaced by empirical averages, and numerical stabilization is achieved by Tikhonov regularization with a small positive scalar $\\varepsilon$, forming $\\mathbf{C}_{00} + \\varepsilon \\mathbf{I}$ and $\\mathbf{C}_{\\tau\\tau} + \\varepsilon \\mathbf{I}$. For model selection, cross-validation is adopted: whitening factors are computed from a training set, and the score is evaluated on a disjoint test set as\n$$\n\\mathrm{VAMP2}_{\\mathrm{CV}} = \\left\\|\\left(\\mathbf{C}_{00}^{\\mathrm{train}} + \\varepsilon \\mathbf{I}\\right)^{-1/2}\\,\\mathbf{C}_{0\\tau}^{\\mathrm{test}}\\,\\left(\\mathbf{C}_{\\tau\\tau}^{\\mathrm{train}} + \\varepsilon \\mathbf{I}\\right)^{-1/2}\\right\\|_F^2.\n$$\n\nSynthetic data generation is based on a linear Gaussian vector autoregression of order one (VAR(1)) in $\\mathbb{R}^2$. Let $\\mathbf{z}_t \\in \\mathbb{R}^2$ denote the latent process with dynamics\n$$\n\\mathbf{z}_{t+1} = \\mathbf{A}\\,\\mathbf{z}_t + \\boldsymbol{\\eta}_t, \\quad \\boldsymbol{\\eta}_t \\sim \\mathcal{N}\\left(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\eta\\right), \\quad \\mathbf{A} = \\mathrm{diag}(\\rho_{\\mathrm{slow}}, \\rho_{\\mathrm{fast}}),\n$$\nwhere $|\\rho_{\\mathrm{slow}}|  1$ and $|\\rho_{\\mathrm{fast}}|  1$ are scalar autoregressive coefficients with $\\rho_{\\mathrm{slow}}$ closer to $1$ to encode a slow timescale, and $\\boldsymbol{\\eta}_t$ are independent Gaussian innovations. Choose $\\boldsymbol{\\Sigma}_\\eta$ such that the stationary covariance of $\\mathbf{z}_t$ is the identity matrix, which is achieved by setting the innovation variances to $1 - \\rho_{\\mathrm{slow}}^2$ and $1 - \\rho_{\\mathrm{fast}}^2$ along the diagonal of $\\boldsymbol{\\Sigma}_\\eta$.\n\nTwo featurizations are defined as linear-noisy observations of $\\mathbf{z}_t$:\n- Featurization $\\mathcal{F}_0$: $\\mathbf{x}_t^{(0)} = \\mathbf{z}_t \\in \\mathbb{R}^2$ (both slow and fast coordinates observed).\n- Featurization $\\mathcal{F}_1$: $\\mathbf{x}_t^{(1)} = \\begin{bmatrix} 0.1\\,z_{t,1} + z_{t,2} + \\xi_t \\\\ \\nu_t \\end{bmatrix}$, where $z_{t,1}$ is the slow coordinate, $z_{t,2}$ is the fast coordinate, and $\\xi_t \\sim \\mathcal{N}(0, \\sigma_\\xi^2)$, $\\nu_t \\sim \\mathcal{N}(0, \\sigma_\\nu^2)$ are independent observational noises. This featurization mixes the slow coordinate weakly into the first channel and includes a second channel that is pure noise, degrading slow-timescale recovery.\n\nYour program must, for each case in the test suite, do the following:\n- Simulate a VAR(1) trajectory of length $T$ in $\\mathbb{R}^2$ using given $\\rho_{\\mathrm{slow}}$, $\\rho_{\\mathrm{fast}}$, and a pseudorandom seed. Discard no burn-in; the stationary variance choice stabilizes transients rapidly for the large $T$ used.\n- Construct $\\mathbf{x}_t^{(0)}$ and $\\mathbf{x}_t^{(1)}$ as above with specified $\\sigma_\\xi$ and $\\sigma_\\nu$.\n- Split the resulting time series into a training block of length $\\lfloor \\alpha\\,(T-\\tau)\\rfloor$ consecutive time-lagged pairs and a test block of the remaining consecutive time-lagged pairs, where $\\alpha \\in (0,1)$ is provided and $\\tau \\in \\mathbb{N}$ is the lag.\n- For each featurization, compute the training covariance matrices $\\mathbf{C}_{00}^{\\mathrm{train}}$, $\\mathbf{C}_{\\tau\\tau}^{\\mathrm{train}}$, and the test cross-covariance $\\mathbf{C}_{0\\tau}^{\\mathrm{test}}$, with empirical means subtracted in each respective block. Use Tikhonov regularization with scalar $\\varepsilon  0$ when forming inverse square roots.\n- Compute the cross-validated VAMP-2 score $\\mathrm{VAMP2}_{\\mathrm{CV}}$ for each featurization and select the featurization with the larger score. Break ties by choosing the smaller index (that is, choose $\\mathcal{F}_0$ if the scores are exactly equal up to numerical precision).\n- For numerical linear algebra, compute inverse square roots of symmetric positive-definite matrices by eigenvalue decomposition: if $\\mathbf{M} = \\mathbf{Q}\\,\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)\\,\\mathbf{Q}^\\top$ with $\\lambda_i  0$, then $\\mathbf{M}^{-1/2} = \\mathbf{Q}\\,\\mathrm{diag}(\\lambda_1^{-1/2},\\dots,\\lambda_d^{-1/2})\\,\\mathbf{Q}^\\top$.\n\nAll quantities are dimensionless; no physical units or angles are involved. The program output must be a single line containing a list of integers, one per test case, each integer being either $0$ or $1$ indicating the index of the selected featurization.\n\nTest suite:\n- Case A (happy path): $T = 60000$, $\\tau = 10$, $\\rho_{\\mathrm{slow}} = 0.995$, $\\rho_{\\mathrm{fast}} = 0.5$, $\\sigma_\\xi = 0.05$, $\\sigma_\\nu = 1.0$, $\\varepsilon = 10^{-6}$, $\\alpha = 0.6$, seed $= 123$.\n- Case B (short lag): $T = 30000$, $\\tau = 1$, $\\rho_{\\mathrm{slow}} = 0.995$, $\\rho_{\\mathrm{fast}} = 0.5$, $\\sigma_\\xi = 0.05$, $\\sigma_\\nu = 1.0$, $\\varepsilon = 10^{-6}$, $\\alpha = 0.6$, seed $= 456$.\n- Case C (longer lag): $T = 60000$, $\\tau = 50$, $\\rho_{\\mathrm{slow}} = 0.995$, $\\rho_{\\mathrm{fast}} = 0.5$, $\\sigma_\\xi = 0.05$, $\\sigma_\\nu = 1.0$, $\\varepsilon = 10^{-6}$, $\\alpha = 0.6$, seed $= 789$.\n- Case D (lower sample, robustness): $T = 8000$, $\\tau = 5$, $\\rho_{\\mathrm{slow}} = 0.995$, $\\rho_{\\mathrm{fast}} = 0.4$, $\\sigma_\\xi = 0.1$, $\\sigma_\\nu = 1.0$, $\\varepsilon = 10^{-5}$, $\\alpha = 0.6$, seed $= 321$.\n\nFinal output format:\n- Your program should produce a single line of output containing the selected featurization indices for Cases A through D, in order, as a comma-separated list enclosed in square brackets (for example, $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C,\\text{result}_D\\right]$).",
            "solution": "The user has provided a problem that is scientifically grounded, well-posed, and objective. It is based on established principles of the Variational Approach for Markov Processes (VAMP) and its application to model selection in computational science. All required parameters, methodologies, and definitions are provided, making the problem fully specified and solvable. The task involves implementing a standard, albeit nuanced, numerical procedure. Consequently, the problem is deemed **valid**.\n\nThe objective is to compare two different featurizations, $\\mathcal{F}_0$ and $\\mathcal{F}_1$, of a latent stochastic process by computing their respective cross-validated VAMP-2 scores. The featurization yielding a higher score is considered superior as it better captures the slow dynamical modes of the underlying system. The entire procedure can be broken down into a sequence of well-defined computational steps.\n\nFirst, we define the mathematical foundation. The VAMP-2 score for a given featurization $\\mathbf{x}_t$ and lag time $\\tau$ is given by\n$$\n\\mathrm{VAMP2} = \\left\\|\\mathbf{C}_{00}^{-1/2}\\,\\mathbf{C}_{0\\tau}\\,\\mathbf{C}_{\\tau\\tau}^{-1/2}\\right\\|_F^2\n$$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm, and $\\mathbf{C}_{00}$, $\\mathbf{C}_{\\tau\\tau}$, and $\\mathbf{C}_{0\\tau}$ are the instantaneous and time-lagged covariance matrices. For robust model selection, we employ a cross-validation scheme. The data is split into a training set and a test set. The whitening matrices, $(\\mathbf{C}_{00}^{\\mathrm{train}} + \\varepsilon \\mathbf{I})^{-1/2}$ and $(\\mathbf{C}_{\\tau\\tau}^{\\mathrm{train}} + \\varepsilon \\mathbf{I})^{-1/2}$, are computed from the training data, incorporating a Tikhonov regularization term $\\varepsilon \\mathbf{I}$ for numerical stability. The score is then evaluated on the test set's cross-covariance matrix, $\\mathbf{C}_{0\\tau}^{\\mathrm{test}}$:\n$$\n\\mathrm{VAMP2}_{\\mathrm{CV}} = \\left\\|\\left(\\mathbf{C}_{00}^{\\mathrm{train}} + \\varepsilon \\mathbf{I}\\right)^{-1/2}\\,\\mathbf{C}_{0\\tau}^{\\mathrm{test}}\\,\\left(\\mathbf{C}_{\\tau\\tau}^{\\mathrm{train}} + \\varepsilon \\mathbf{I}\\right)^{-1/2}\\right\\|_F^2\n$$\n\nThe algorithmic procedure for each test case is as follows:\n\n1.  **Synthetic Data Generation**: A trajectory of a latent $2$-dimensional vector autoregressive process of order one (VAR(1)), $\\mathbf{z}_t \\in \\mathbb{R}^2$, is simulated for $T$ timesteps. The dynamics are governed by\n    $$\n    \\mathbf{z}_{t+1} = \\mathbf{A}\\,\\mathbf{z}_t + \\boldsymbol{\\eta}_t\n    $$\n    where $\\mathbf{A} = \\mathrm{diag}(\\rho_{\\mathrm{slow}}, \\rho_{\\mathrm{fast}})$ and $\\boldsymbol{\\eta}_t$ is sampled from a zero-mean Gaussian distribution $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\eta)$. The autoregressive coefficients $|\\rho_{\\mathrm{slow}}|1$ and $|\\rho_{\\mathrm{fast}}|1$ ensure stationarity. The innovation covariance $\\boldsymbol{\\Sigma}_\\eta = \\mathrm{diag}(1-\\rho_{\\mathrm{slow}}^2, 1-\\rho_{\\mathrm{fast}}^2)$ is chosen such that the stationary covariance of the latent process $\\mathbf{z}_t$ is the identity matrix $\\mathbf{I}$. This is derived from the discrete Lyapunov equation for the stationary covariance $\\mathbf{C}_z$, which states $\\mathbf{C}_z = \\mathbf{A}\\,\\mathbf{C}_z\\,\\mathbf{A}^\\top + \\boldsymbol{\\Sigma}_\\eta$. Setting $\\mathbf{C}_z=\\mathbf{I}$ yields $\\mathbf{I} = \\mathbf{A}\\,\\mathbf{I}\\,\\mathbf{A}^\\top + \\boldsymbol{\\Sigma}_\\eta$, so $\\boldsymbol{\\Sigma}_\\eta = \\mathbf{I} - \\mathbf{A}^2$.\n\n2.  **Featurization**: From the latent trajectory $\\mathbf{z}_t = [z_{t,1}, z_{t,2}]^\\top$, two featurized time series, $\\mathbf{x}_t^{(0)}$ and $\\mathbf{x}_t^{(1)}$, are constructed.\n    -   $\\mathcal{F}_0$ (ideal featurization): $\\mathbf{x}_t^{(0)} = \\mathbf{z}_t$. This featurization directly observes both a slow ($z_{t,1}$) and a fast ($z_{t,2}$) coordinate.\n    -   $\\mathcal{F}_1$ (poor featurization): $\\mathbf{x}_t^{(1)} = \\begin{bmatrix} 0.1\\,z_{t,1} + z_{t,2} + \\xi_t \\\\ \\nu_t \\end{bmatrix}$. Here, the slow coordinate $z_{t,1}$ is weakly mixed with the fast coordinate and corrupted by observational noise $\\xi_t \\sim \\mathcal{N}(0, \\sigma_\\xi^2)$. The second channel is pure noise, $\\nu_t \\sim \\mathcal{N}(0, \\sigma_\\nu^2)$, containing no information about the system's dynamics. Intuitively, $\\mathcal{F}_0$ should yield a much higher VAMP-2 score.\n\n3.  **Data Splitting**: For a given featurization $\\mathbf{x}_t$ and lag time $\\tau$, we form $T-\\tau$ time-lagged pairs $(\\mathbf{x}_t, \\mathbf{x}_{t+\\tau})$. This dataset of pairs is partitioned chronologically into a training block and a test block. The training block consists of the first $n_{\\mathrm{train}} = \\lfloor \\alpha (T-\\tau) \\rfloor$ pairs, and the test block consists of the remaining $n_{\\mathrm{test}}$ pairs.\n\n4.  **Covariance Estimation**: The required covariance matrices are estimated empirically.\n    -   For the training set, we compute means $\\bar{\\mathbf{x}}_0^{\\mathrm{train}}$ and $\\bar{\\mathbf{x}}_\\tau^{\\mathrm{train}}$. The centered data is then used to compute the covariance matrices:\n        $$\n        \\mathbf{C}_{00}^{\\mathrm{train}} = \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=0}^{n_{\\mathrm{train}}-1} (\\mathbf{x}_i - \\bar{\\mathbf{x}}_0^{\\mathrm{train}})(\\mathbf{x}_i - \\bar{\\mathbf{x}}_0^{\\mathrm{train}})^\\top, \\quad\n        \\mathbf{C}_{\\tau\\tau}^{\\mathrm{train}} = \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=0}^{n_{\\mathrm{train}}-1} (\\mathbf{x}_{i+\\tau} - \\bar{\\mathbf{x}}_\\tau^{\\mathrm{train}})(\\mathbf{x}_{i+\\tau} - \\bar{\\mathbf{x}}_\\tau^{\\mathrm{train}})^\\top\n        $$\n    -   For the test set, we compute means $\\bar{\\mathbf{x}}_0^{\\mathrm{test}}$ and $\\bar{\\mathbf{x}}_\\tau^{\\mathrm{test}}$ and the cross-covariance matrix:\n        $$\n        \\mathbf{C}_{0\\tau}^{\\mathrm{test}} = \\frac{1}{n_{\\mathrm{test}}} \\sum_{j=n_{\\mathrm{train}}}^{T-\\tau-1} (\\mathbf{x}_j - \\bar{\\mathbf{x}}_0^{\\mathrm{test}})(\\mathbf{x}_{j+\\tau} - \\bar{\\mathbf{x}}_\\tau^{\\mathrm{test}})^\\top\n        $$\n\n5.  **VAMP-2 Score Calculation**: The core of the calculation involves matrix inverse square roots. For a symmetric positive-definite matrix $\\mathbf{M}$ with eigendecomposition $\\mathbf{M} = \\mathbf{Q}\\,\\boldsymbol{\\Lambda}\\,\\mathbf{Q}^\\top$, where $\\boldsymbol{\\Lambda} = \\mathrm{diag}(\\lambda_i)$, its inverse square root is $\\mathbf{M}^{-1/2} = \\mathbf{Q}\\,\\boldsymbol{\\Lambda}^{-1/2}\\,\\mathbf{Q}^\\top$. We first form the regularized training covariance matrices $\\mathbf{K}_{00} = \\mathbf{C}_{00}^{\\mathrm{train}} + \\varepsilon \\mathbf{I}$ and $\\mathbf{K}_{\\tau\\tau} = \\mathbf{C}_{\\tau\\tau}^{\\mathrm{train}} + \\varepsilon \\mathbf{I}$. We then compute their inverse square roots, $\\mathbf{K}_{00}^{-1/2}$ and $\\mathbf{K}_{\\tau\\tau}^{-1/2}$. The whitened test cross-covariance matrix is formed as $\\mathbf{W} = \\mathbf{K}_{00}^{-1/2} \\mathbf{C}_{0\\tau}^{\\mathrm{test}} \\mathbf{K}_{\\tau\\tau}^{-1/2}$. The final score is the squared Frobenius norm of this matrix: $\\mathrm{VAMP2}_{\\mathrm{CV}} = \\sum_{i,j} W_{ij}^2$.\n\n6.  **Model Selection**: For each test case, the $\\mathrm{VAMP2}_{\\mathrm{CV}}$ scores for $\\mathcal{F}_0$ and $\\mathcal{F}_1$ are computed. The featurization with the higher score is selected. In the case of a tie, $\\mathcal{F}_0$ is chosen by convention. The index of the selected featurization ($0$ or $1$) is recorded. The final output is a list of these indices for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to execute the VAMP-2 score comparison for all test cases.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {'T': 60000, 'tau': 10, 'rho_slow': 0.995, 'rho_fast': 0.5, 'sigma_xi': 0.05, 'sigma_nu': 1.0, 'eps': 1e-6, 'alpha': 0.6, 'seed': 123},\n        # Case B (short lag)\n        {'T': 30000, 'tau': 1, 'rho_slow': 0.995, 'rho_fast': 0.5, 'sigma_xi': 0.05, 'sigma_nu': 1.0, 'eps': 1e-6, 'alpha': 0.6, 'seed': 456},\n        # Case C (longer lag)\n        {'T': 60000, 'tau': 50, 'rho_slow': 0.995, 'rho_fast': 0.5, 'sigma_xi': 0.05, 'sigma_nu': 1.0, 'eps': 1e-6, 'alpha': 0.6, 'seed': 789},\n        # Case D (lower sample, robustness)\n        {'T': 8000, 'tau': 5, 'rho_slow': 0.995, 'rho_fast': 0.4, 'sigma_xi': 0.1, 'sigma_nu': 1.0, 'eps': 1e-5, 'alpha': 0.6, 'seed': 321},\n    ]\n\n    results = []\n    for params in test_cases:\n        # Unpack parameters for the current case.\n        T, tau, rho_slow, rho_fast, sigma_xi, sigma_nu, eps, alpha, seed = \\\n            params['T'], params['tau'], params['rho_slow'], params['rho_fast'], \\\n            params['sigma_xi'], params['sigma_nu'], params['eps'], params['alpha'], \\\n            params['seed']\n\n        # Set the seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Synthetic Data Generation (Latent Process)\n        A = np.diag([rho_slow, rho_fast])\n        # Innovation covariance for stationary covariance of z to be identity\n        sigma_eta_sq = np.array([1 - rho_slow**2, 1 - rho_fast**2])\n        Sigma_eta = np.diag(sigma_eta_sq)\n\n        z = np.zeros((T, 2))\n        # Initial condition z_0 sampled from stationary distribution N(0, I)\n        z[0] = rng.normal(size=2)\n        for t in range(T - 1):\n            eta_t = rng.multivariate_normal(np.zeros(2), Sigma_eta)\n            z[t + 1] = A @ z[t] + eta_t\n\n        # 2. Featurizations\n        # Featurization F_0\n        x_feat0 = z\n\n        # Featurization F_1\n        z1, z2 = z[:, 0], z[:, 1]\n        xi = rng.normal(0, sigma_xi, size=T)\n        nu = rng.normal(0, sigma_nu, size=T)\n        x_feat1_ch1 = 0.1 * z1 + z2 + xi\n        x_feat1_ch2 = nu\n        x_feat1 = np.vstack((x_feat1_ch1, x_feat1_ch2)).T\n\n        # Calculate VAMP-2 scores for both featurizations\n        vamp2_score_0 = calculate_vamp2_cv(x_feat0, tau, alpha, eps)\n        vamp2_score_1 = calculate_vamp2_cv(x_feat1, tau, alpha, eps)\n        \n        # 3. Model Selection\n        # Select the featurization with the larger score. Tie goes to index 0.\n        if vamp2_score_1 > vamp2_score_0:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef mat_inv_sqrt(M):\n    \"\"\"\n    Computes the inverse square root M^{-1/2} of a symmetric positive definite matrix M\n    via eigenvalue decomposition.\n    \"\"\"\n    eigvals, eigvecs = np.linalg.eigh(M)\n    # Eigenvalues must be positive due to regularization\n    D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n    return eigvecs @ D_inv_sqrt @ eigvecs.T\n\ndef calculate_vamp2_cv(X, tau, alpha, eps):\n    \"\"\"\n    Calculates the cross-validated VAMP-2 score for a given time series.\n    \"\"\"\n    T, d = X.shape\n    \n    # Create time-lagged data\n    X0 = X[:-tau, :]\n    Xt = X[tau:, :]\n    \n    num_pairs = len(X0)\n    \n    # 3. Data Splitting\n    n_train = int(alpha * num_pairs)\n    \n    X0_train, Xt_train = X0[:n_train], Xt[:n_train]\n    X0_test, Xt_test = X0[n_train:], Xt[n_train:]\n    \n    n_test = len(X0_test)\n\n    # 4. Covariance Estimation\n    # Center training data and compute covariance matrices\n    mu0_train = np.mean(X0_train, axis=0)\n    mut_train = np.mean(Xt_train, axis=0)\n    X0_train_c = X0_train - mu0_train\n    Xt_train_c = Xt_train - mut_train\n    \n    C00_train = (X0_train_c.T @ X0_train_c) / n_train\n    Ctt_train = (Xt_train_c.T @ Xt_train_c) / n_train\n    \n    # Center test data and compute cross-covariance\n    mu0_test = np.mean(X0_test, axis=0)\n    mut_test = np.mean(Xt_test, axis=0)\n    X0_test_c = X0_test - mu0_test\n    Xt_test_c = Xt_test - mut_test\n    \n    C0t_test = (X0_test_c.T @ Xt_test_c) / n_test\n    \n    # 5. VAMP-2 Score Calculation\n    # Regularize training covariances\n    K00_reg = C00_train + eps * np.identity(d)\n    Ktt_reg = Ctt_train + eps * np.identity(d)\n    \n    # Compute inverse square roots for whitening\n    K00_inv_sqrt = mat_inv_sqrt(K00_reg)\n    Ktt_inv_sqrt = mat_inv_sqrt(Ktt_reg)\n    \n    # Whiten the test cross-covariance matrix\n    W = K00_inv_sqrt @ C0t_test @ Ktt_inv_sqrt\n    \n    # Compute squared Frobenius norm\n    vamp2_score = np.sum(W**2)\n    \n    return vamp2_score\n\nsolve()\n```"
        },
        {
            "introduction": "A cornerstone of building a valid MSM is satisfying the Markov assumption, which in practice means choosing a lag time $\\tau$ long enough for the system to lose 'memory' of past microstates. A powerful diagnostic for this is the implied timescale plot, which shows the model's relaxation timescales as a function of $\\tau$. In this exercise, you will compute and analyze these implied timescales for a model system, learning to identify the characteristic 'plateau' that indicates a suitable range of lag times where the model is approximately Markovian .",
            "id": "3838048",
            "problem": "You are given the task of computing implied timescales for an example transition probability matrix as a function of lag time and assessing whether the slowest timescales have converged, in the context of building and validating Markov state models (MSMs) for long-timescale dynamics in computational chemical biology. Begin from the following definitions and facts. A Markov state model (MSM) represents the dynamics of a system projected onto a finite set of discrete states by a transition probability matrix $T(\\tau)$ that advances state probabilities by a lag time $\\tau$. A transition probability matrix $T(\\tau)$ is a row-stochastic matrix with nonnegative entries and row sums equal to $1$; its largest eigenvalue equals $1$. In continuous-time kinetics, a rate matrix (generator) $K$ has nonnegative off-diagonal elements $K_{ij}$ for $i \\neq j$ and diagonal elements $K_{ii} = - \\sum_{j \\neq i} K_{ij}$. For a continuous-time Markov chain (CTMC), the corresponding lagged transition probability matrix is $T(\\tau) = \\exp(\\tau K)$, where $\\exp$ denotes the matrix exponential. The implied relaxation timescales $t_i(\\tau)$ of $T(\\tau)$ are defined via its eigenvalues $\\lambda_i(\\tau)$ (ordered by decreasing magnitude), excluding the trivial stationary eigenvalue $\\lambda_1(\\tau)=1$, by the formula $t_i(\\tau) = - \\tau / \\ln |\\lambda_i(\\tau)|$. A slow implied timescale is one associated with the largest nontrivial eigenvalue magnitude $|\\lambda_2(\\tau)|$. Convergence of the slowest implied timescale across increasing $\\tau$ is expected for a well-constructed MSM and indicates that the model captures approximate Markovian kinetics at those lag times. Your program must compute $t_2(\\tau)$ for multiple lag times and assess convergence over the larger-lag subset by a relative range criterion.\n\nScientific realism requirement: You will construct $T(\\tau)$ matrices from physically plausible continuous-time rate matrices $K$ via $T(\\tau) = \\exp(\\tau K)$, and then add small deterministic perturbations to mimic finite-sample estimation noise. The perturbations must preserve row-stochasticity and nonnegativity by clipping negative entries to $0$ and renormalizing rows to sum to $1$.\n\nUnits: All lag times $\\tau$ and implied timescales $t_i(\\tau)$ must be expressed in microseconds. Your program’s final numerical outputs must be in microseconds.\n\nAlgorithm to implement:\n- For each test case and each lag time $\\tau$, form the unperturbed $T_0(\\tau) = \\exp(\\tau K)$.\n- Add a deterministic perturbation matrix $\\Delta(\\tau) = \\delta(\\tau) N$, where $N$ is a fixed off-diagonal pattern matrix with $N_{ij} = 1$ for $i \\neq j$ and $N_{ii} = 0$, and $\\delta(\\tau)$ is a small nonnegative scalar provided per test case.\n- Set $\\tilde{T}(\\tau) = T_0(\\tau) + \\Delta(\\tau)$, then project $\\tilde{T}(\\tau)$ back to the set of row-stochastic matrices by elementwise clipping to nonnegativity and row normalization: for each row $i$, set $\\tilde{T}_{ij}(\\tau) \\leftarrow \\max\\{ \\tilde{T}_{ij}(\\tau), 0 \\}$, then divide the row by its sum.\n- Compute all eigenvalues $\\lambda_i(\\tau)$ of $\\tilde{T}(\\tau)$, sort them by descending magnitude $|\\lambda_i(\\tau)|$, exclude the trivial eigenvalue closest to $1$, and compute the slowest implied timescale $t_2(\\tau) = - \\tau / \\ln |\\lambda_2(\\tau)|$. If $|\\lambda_2(\\tau)| = 0$, define $t_2(\\tau) = 0$.\n- Convergence assessment: For the set of slowest implied timescales $\\{ t_2(\\tau_k) \\}$ across all lag times sorted in increasing $\\tau_k$, consider only the larger-lag subset consisting of the last half of the values (rounding down the split index), compute the mean $\\bar{t}$ of this subset, its maximum $t_{\\max}$ and minimum $t_{\\min}$, and declare convergence if $(t_{\\max} - t_{\\min}) / \\bar{t} \\leq \\epsilon$, where $\\epsilon$ is a tolerance specified per test case. If $\\bar{t} = 0$, declare convergence if and only if $t_{\\max} = t_{\\min}$.\n\nYour program should produce a single line of output containing the results aggregated across all test cases as a comma-separated list enclosed in square brackets. For each test case, output a list with two elements: the list of slowest implied timescales $[t_2(\\tau_1), t_2(\\tau_2), \\dots]$ in microseconds rounded to six decimal places, and the boolean value of the convergence assessment for that test case. The final output line must therefore look like $[[[t_{21}, t_{22}, \\dots], b_1], [[t_{31}, t_{32}, \\dots], b_2], \\dots]$ where each $t_{ij}$ is a float in microseconds and each $b_i$ is a boolean.\n\nTest suite specification with explicit parameters:\n\n- Test case $1$ (happy path, reversible symmetric clusters with decreasing perturbation):\n  - Number of states: $4$.\n  - Rate matrix $K$ defined by two clusters $\\{0,1\\}$ and $\\{2,3\\}$ with strong intra-cluster rates and weak inter-cluster rates: for $i \\neq j$, set $K_{ij} = 6.0$ if $i,j \\in \\{0,1\\}$ or $i,j \\in \\{2,3\\}$, and set $K_{ij} = 0.2$ otherwise. Set each $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n  - Lag times (microseconds): $\\tau \\in \\{ 1.0, 2.0, 5.0, 10.0 \\}$.\n  - Perturbation magnitudes: $\\delta(\\tau) \\in \\{ 0.02, 0.01, 0.005, 0.002 \\}$ respectively.\n  - Convergence tolerance: $\\epsilon = 0.1$.\n\n- Test case $2$ (edge case, $3$-state asymmetric ring with nondecreasing perturbation leading to nonconvergence under tight tolerance):\n  - Number of states: $3$.\n  - Rate matrix $K$ defined by an asymmetric ring: $K_{01} = 1.5$, $K_{12} = 1.5$, $K_{20} = 1.5$, $K_{10} = 1.2$, $K_{21} = 1.2$, $K_{02} = 0.8$, with $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n  - Lag times (microseconds): $\\tau \\in \\{ 0.5, 1.0, 2.0, 4.0 \\}$.\n  - Perturbation magnitudes: $\\delta(\\tau) \\in \\{ 0.03, 0.03, 0.03, 0.03 \\}$.\n  - Convergence tolerance: $\\epsilon = 0.05$.\n\n- Test case $3$ (boundary condition, $5$-state with a very slow inter-cluster bridge and small lags including very small $\\tau$):\n  - Number of states: $5$.\n  - Rate matrix $K$ defined by clusters $\\{0,1,2\\}$ and $\\{3,4\\}$ with strong intra-cluster rates and very weak inter-cluster rates: for $i \\neq j$, set $K_{ij} = 4.0$ if $i,j \\in \\{0,1,2\\}$ or $i,j \\in \\{3,4\\}$, and set $K_{ij} = 0.05$ otherwise. Set each $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n  - Lag times (microseconds): $\\tau \\in \\{ 0.1, 0.5, 1.0, 2.0 \\}$.\n  - Perturbation magnitudes: $\\delta(\\tau) \\in \\{ 0.02, 0.01, 0.005, 0.003 \\}$.\n  - Convergence tolerance: $\\epsilon = 0.15$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element in this list corresponds to one test case and must be a two-element list whose first element is the list of slowest implied timescales for the specified lag times (in microseconds, rounded to six decimal places) and whose second element is a boolean indicating whether the slowest timescales have converged according to the criterion above. For example, the output should look like $[[[t_{11},t_{12},t_{13},t_{14}],b_1],[[t_{21},t_{22},t_{23},t_{24}],b_2],[[t_{31},t_{32},t_{33},t_{34}],b_3]]$ with numerical values replacing the $t_{ij}$ placeholders.",
            "solution": "The user has provided a problem that requires the computation and analysis of implied timescales for several Markov State Models (MSMs). I will first validate the problem statement according to the specified criteria.\n\n### Step 1: Extract Givens\n\n**Definitions and Core Concepts:**\n- A Markov State Model (MSM) is represented by a transition probability matrix $T(\\tau)$ that advances state probabilities by a lag time $\\tau$.\n- A transition probability matrix $T(\\tau)$ is a row-stochastic matrix with nonnegative entries and row sums equal to $1$. Its largest eigenvalue equals $1$.\n- A continuous-time rate matrix (generator) $K$ has nonnegative off-diagonal elements $K_{ij}$ for $i \\neq j$ and diagonal elements $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n- The relationship between $K$ and $T(\\tau)$ for a continuous-time Markov chain (CTMC) is $T(\\tau) = \\exp(\\tau K)$, where $\\exp$ is the matrix exponential.\n- The implied relaxation timescales $t_i(\\tau)$ are defined by the eigenvalues $\\lambda_i(\\tau)$ of $T(\\tau)$ (ordered by decreasing magnitude $|\\lambda_i(\\tau)|$) as $t_i(\\tau) = - \\tau / \\ln |\\lambda_i(\\tau)|$, for all $i$ where $\\lambda_i(\\tau)$ is not the trivial stationary eigenvalue $\\lambda_1(\\tau)=1$.\n- The slowest implied timescale is $t_2(\\tau)$, associated with the largest nontrivial eigenvalue magnitude $|\\lambda_2(\\tau)|$.\n- If $|\\lambda_2(\\tau)| = 0$, then $t_2(\\tau)$ is defined as $0$.\n\n**Algorithmic Steps:**\n1.  For each test case and lag time $\\tau$, form the unperturbed transition matrix $T_0(\\tau) = \\exp(\\tau K)$.\n2.  Define a perturbation matrix $\\Delta(\\tau) = \\delta(\\tau) N$, where $N$ is a fixed off-diagonal pattern matrix with $N_{ij} = 1$ for $i \\neq j$ and $N_{ii} = 0$. $\\delta(\\tau)$ is a given small nonnegative scalar.\n3.  Construct the perturbed matrix $\\tilde{T}(\\tau) = T_0(\\tau) + \\Delta(\\tau)$.\n4.  Project $\\tilde{T}(\\tau)$ back to a row-stochastic matrix:\n    a. Element-wise clipping: $\\tilde{T}_{ij}(\\tau) \\leftarrow \\max\\{ \\tilde{T}_{ij}(\\tau), 0 \\}$.\n    b. Row normalization: Divide each row by its sum.\n5.  Compute eigenvalues $\\lambda_i(\\tau)$ of $\\tilde{T}(\\tau)$, sort by descending magnitude, and identify the second largest magnitude, $|\\lambda_2(\\tau)|$.\n6.  Compute the slowest implied timescale $t_2(\\tau) = - \\tau / \\ln |\\lambda_2(\\tau)|$.\n7.  Assess convergence: For the set of timescales $\\{ t_2(\\tau_k) \\}$ over all lag times, consider the latter half of the values (rounding down the split index). Compute the mean $\\bar{t}$, maximum $t_{\\max}$, and minimum $t_{\\min}$ of this subset. Convergence is declared if $(t_{\\max} - t_{\\min}) / \\bar{t} \\leq \\epsilon$. If $\\bar{t} = 0$, convergence is declared if and only if $t_{\\max} = t_{\\min}$.\n\n**Units and Formatting:**\n- All time units ($\\tau$, $t_i(\\tau)$) are in microseconds.\n- Final numerical output for timescales must be rounded to six decimal places.\n- The final output is a single line: a string representation of a list of lists, where each inner list contains a list of timescales and a boolean convergence flag. Format: `[[[t_11, t_12,...], b_1], [[t_21, t_22,...], b_2], ...]`.\n\n**Test Suite Parameters:**\n\n*   **Test Case 1:**\n    *   States: $4$.\n    *   Rate matrix $K$: $K_{ij} = 6.0$ for intra-cluster transitions ($\\{0,1\\}$ and $\\{2,3\\}$) and $K_{ij} = 0.2$ for inter-cluster, for $i \\neq j$. $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n    *   Lag times $\\tau$ (μs): $\\{ 1.0, 2.0, 5.0, 10.0 \\}$.\n    *   Perturbations $\\delta(\\tau)$: $\\{ 0.02, 0.01, 0.005, 0.002 \\}$.\n    *   Tolerance $\\epsilon$: $0.1$.\n*   **Test Case 2:**\n    *   States: $3$.\n    *   Rate matrix $K$: Asymmetric ring with $K_{01} = 1.5, K_{12} = 1.5, K_{20} = 1.5, K_{10} = 1.2, K_{21} = 1.2, K_{02} = 0.8$. $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n    *   Lag times $\\tau$ (μs): $\\{ 0.5, 1.0, 2.0, 4.0 \\}$.\n    *   Perturbations $\\delta(\\tau)$: $\\{ 0.03, 0.03, 0.03, 0.03 \\}$.\n    *   Tolerance $\\epsilon$: $0.05$.\n*   **Test Case 3:**\n    *   States: $5$.\n    *   Rate matrix $K$: $K_{ij} = 4.0$ for intra-cluster transitions ($\\{0,1,2\\}$ and $\\{3,4\\}$) and $K_{ij} = 0.05$ for inter-cluster, for $i \\neq j$. $K_{ii} = - \\sum_{j \\neq i} K_{ij}$.\n    *   Lag times $\\tau$ (μs): $\\{ 0.1, 0.5, 1.0, 2.0 \\}$.\n    *   Perturbations $\\delta(\\tau)$: $\\{ 0.02, 0.01, 0.005, 0.003 \\}$.\n    *   Tolerance $\\epsilon$: $0.15$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientific or Factual Unsoundness:** The problem is firmly grounded in the theory of continuous-time Markov chains and their application in constructing Markov State Models, a standard and powerful technique in computational chemistry and biophysics. The definitions of the rate matrix $K$, transition matrix $T(\\tau)$, matrix exponential relation $T(\\tau) = \\exp(\\tau K)$, and the formula for implied timescales are all standard and correct. The concept of implied timescale convergence is a cornerstone of MSM validation. The problem is scientifically sound.\n2.  **Non-Formalizable or Irrelevant:** The problem is highly formalizable. It provides a precise, step-by-step algorithm and is directly relevant to the specified topic of *building and validating Markov state models (MSMs) for long-timescale dynamics* in computational chemical biology.\n3.  **Incomplete or Contradictory Setup:** The problem is self-contained. All necessary parameters for each test case—number of states, rules for constructing the rate matrix $K$, lists of lag times $\\tau$ and perturbation magnitudes $\\delta(\\tau)$, and the convergence tolerance $\\epsilon$—are explicitly provided. There are no contradictions.\n4.  **Unrealistic or Infeasible:** The rate matrices are constructed to represent physically plausible scenarios, such as kinetically distinct clusters of states with fast intra-cluster dynamics and slow inter-cluster transitions. The introduction of a small, state-dependent perturbation is a reasonable simplification to model finite-sampling noise in transition count estimation from molecular dynamics trajectories. The parameter values are within realistic ranges.\n5.  **Ill-Posed or Poorly Structured:** The problem is well-posed. The algorithm is deterministic and, given the inputs, will produce a unique, meaningful output. The definitions are clear and unambiguous. The edge case of a zero eigenvalue is explicitly handled.\n6.  **Pseudo-Profound, Trivial, or Tautological:** The problem requires the application of non-trivial mathematical concepts and numerical methods, including matrix exponentiation and eigenvalue decomposition. It is a substantive computational task that encapsulates a key part of the MSM validation workflow, not a trivial exercise.\n7.  **Outside Scientific Verifiability:** The problem is purely computational. The results can be independently verified by implementing the same specified algorithm with the same inputs.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is scientifically sound, well-posed, and complete. I will proceed with generating the solution.\n\nThe solution requires implementing the specified algorithm to compute the slowest implied timescale, $t_2(\\tau)$, for a series of lag times $\\tau$ across three distinct test cases. Each test case is defined by a rate matrix $K$, which describes the underlying continuous-time kinetics. The process involves several key steps rooted in the theory of Markov processes.\n\nFirst, for each test case, we construct the rate matrix $K$. The elements $K_{ij}$ for $i \\neq j$ represent the rate of transitioning from state $i$ to state $j$. The diagonal elements $K_{ii}$ are fixed by the condition that each row of the rate matrix must sum to zero, i.e., $K_{ii} = -\\sum_{j \\neq i} K_{ij}$. This ensures conservation of probability.\n\nNext, for each specified lag time $\\tau$, we compute the \"ideal\" transition probability matrix $T_0(\\tau)$. For a continuous-time Markov process described by $K$, the probability of transitioning from state $i$ to state $j$ in a time interval $\\tau$ is given by the matrix element $[T(\\tau)]_{ij}$. This matrix is calculated via the matrix exponential, $T(\\tau) = \\exp(\\tau K)$.\n\nThe problem introduces a realistic complication: noise. In practice, transition probabilities are estimated from finite simulation data and are subject to statistical error. To mimic this, we add a deterministic perturbation $\\Delta(\\tau) = \\delta(\\tau) N$ to the ideal matrix $T_0(\\tau)$, where $N$ is a matrix of ones on the off-diagonal and zeros on the diagonal, and $\\delta(\\tau)$ is a small, lag-dependent magnitude. This yields a perturbed matrix $\\tilde{T}(\\tau) = T_0(\\tau) + \\Delta(\\tau)$.\n\nThis perturbed matrix $\\tilde{T}(\\tau)$ is not guaranteed to be a valid row-stochastic matrix; its entries may be negative, and its rows may not sum to $1$. Therefore, we must project it back onto the space of valid stochastic matrices. This is achieved in two steps: first, all negative entries are clipped to zero, $\\tilde{T}_{ij}(\\tau) \\leftarrow \\max\\{\\tilde{T}_{ij}(\\tau), 0\\}$. Second, each row is normalized by dividing its elements by the row sum. This two-step process ensures both non-negativity and the row-sum-to-one property.\n\nWith a valid, albeit noisy, transition matrix $\\tilde{T}(\\tau)$ for each lag time, we proceed to compute its implied timescales. This involves calculating the eigenvalues $\\lambda_i(\\tau)$ of $\\tilde{T}(\\tau)$. The eigenvalues of a stochastic matrix have magnitudes less than or equal to $1$. The largest eigenvalue is always $\\lambda_1(\\tau) = 1$, corresponding to the stationary distribution. The other eigenvalues, $|\\lambda_i(\\tau)|  1$ for $i  1$, describe the timescales of relaxation toward this stationary distribution. The implied timescale $t_i(\\tau)$ associated with eigenvalue $\\lambda_i(\\tau)$ is given by the fundamental relation $t_i(\\tau) = -\\tau / \\ln|\\lambda_i(\\tau)|$. The slowest process in the system, beyond reaching stationarity, is captured by the largest non-unit eigenvalue magnitude, $|\\lambda_2(\\tau)|$. We therefore calculate $t_2(\\tau)$ for each $\\tau$. Per the problem, if $|\\lambda_2(\\tau)| = 0$, we define $t_2(\\tau)=0$.\n\nFinally, a crucial part of MSM validation is to check if the computed implied timescales are constant with respect to the choice of lag time $\\tau$. If an MSM is a good approximation of the underlying dynamics (i.e., it is Markovian), the physical relaxation timescales of the system should not depend on the lag time used to build the model, provided $\\tau$ is large enough to resolve the slow processes but not so large as to average them out. We assess this convergence by examining the set of slowest timescales $\\{ t_2(\\tau_k) \\}$. We focus on the latter half of these values, corresponding to larger lag times where Markovianity is more likely to hold. We compute the relative range of this subset, $(t_{\\max} - t_{\\min})/\\bar{t}$, and compare it to a given tolerance $\\epsilon$. If the relative range is within the tolerance, we conclude that the slowest timescale has converged. A special case is handled where the mean timescale $\\bar{t}$ is zero.\n\nThis entire procedure is applied to each of the three test cases, and the results are compiled into the required output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Computes and validates implied timescales for Markov state models.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (happy path, reversible symmetric clusters)\n        {\n            \"n_states\": 4,\n            \"k_rules\": lambda i, j: 6.0 if (i in {0, 1} and j in {0, 1}) or (i in {2, 3} and j in {2, 3}) else 0.2,\n            \"lag_times\": [1.0, 2.0, 5.0, 10.0],\n            \"perturbations\": [0.02, 0.01, 0.005, 0.002],\n            \"epsilon\": 0.1,\n        },\n        # Test case 2 (edge case, 3-state asymmetric ring)\n        {\n            \"n_states\": 3,\n            \"k_rules\": {\n                (0, 1): 1.5, (1, 2): 1.5, (2, 0): 1.5,\n                (1, 0): 1.2, (2, 1): 1.2, (0, 2): 0.8\n            },\n            \"lag_times\": [0.5, 1.0, 2.0, 4.0],\n            \"perturbations\": [0.03, 0.03, 0.03, 0.03],\n            \"epsilon\": 0.05,\n        },\n        # Test case 3 (boundary condition, 5-state with slow bridge)\n        {\n            \"n_states\": 5,\n            \"k_rules\": lambda i, j: 4.0 if (i in {0, 1, 2} and j in {0, 1, 2}) or (i in {3, 4} and j in {3, 4}) else 0.05,\n            \"lag_times\": [0.1, 0.5, 1.0, 2.0],\n            \"perturbations\": [0.02, 0.01, 0.005, 0.003],\n            \"epsilon\": 0.15,\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n = case[\"n_states\"]\n        k_rules = case[\"k_rules\"]\n        lag_times = case[\"lag_times\"]\n        perturbations = case[\"perturbations\"]\n        epsilon = case[\"epsilon\"]\n        \n        # 1. Construct the rate matrix K\n        K = np.zeros((n, n), dtype=float)\n        if callable(k_rules):\n            for i in range(n):\n                for j in range(n):\n                    if i != j:\n                        K[i, j] = k_rules(i, j)\n        else: # Dictionary-based rules for Test Case 2\n            for (i, j), val in k_rules.items():\n                K[i, j] = val\n        \n        for i in range(n):\n            K[i, i] = -np.sum(K[i, :])\n\n        # Define the off-diagonal pattern matrix N\n        N = np.ones((n, n)) - np.eye(n)\n        \n        slowest_implied_timescales = []\n        \n        for i, tau in enumerate(lag_times):\n            delta_tau = perturbations[i]\n            \n            # 2. Form T0(tau) = exp(tau * K)\n            T0_tau = expm(tau * K)\n            \n            # 3. Add perturbation\n            Delta_tau = delta_tau * N\n            T_tilde_tau = T0_tau + Delta_tau\n            \n            # 4. Project back to row-stochastic\n            # a. Clip to non-negativity\n            T_tilde_tau = np.maximum(T_tilde_tau, 0)\n            # b. Row normalize\n            row_sums = T_tilde_tau.sum(axis=1)\n            # Avoid division by zero for rows that are all zero.\n            non_zero_rows = row_sums > 0\n            T_tilde_tau[non_zero_rows] /= row_sums[non_zero_rows, np.newaxis]\n\n            # 5. Compute eigenvalues and find |lambda_2(tau)|\n            eigenvalues = np.linalg.eigvals(T_tilde_tau)\n            # Sort by descending magnitude\n            sorted_magnitudes = sorted(np.abs(eigenvalues), reverse=True)\n            \n            # The second largest magnitude eigenvalue\n            lambda2_mag = sorted_magnitudes[1]\n            \n            # 6. Compute slowest implied timescale t_2(tau)\n            if lambda2_mag > 0:\n                t2_tau = -tau / np.log(lambda2_mag)\n            else:\n                # As per problem spec, if |lambda_2| = 0, t_2 = 0.\n                t2_tau = 0.0\n            \n            slowest_implied_timescales.append(t2_tau)\n        \n        # 7. Assess convergence on the latter half of the timescales\n        split_index = len(slowest_implied_timescales) // 2\n        later_timescales = np.array(slowest_implied_timescales[split_index:])\n        \n        is_converged = False\n        if len(later_timescales) > 0:\n            t_mean = np.mean(later_timescales)\n            if t_mean > 0:\n                t_max = np.max(later_timescales)\n                t_min = np.min(later_timescales)\n                relative_range = (t_max - t_min) / t_mean\n                if relative_range = epsilon:\n                    is_converged = True\n            else: # t_mean is 0\n                # Convergence if and only if all values are identical (and thus 0)\n                if np.max(later_timescales) == np.min(later_timescales):\n                    is_converged = True\n\n        rounded_timescales = [round(t, 6) for t in slowest_implied_timescales]\n        all_results.append([rounded_timescales, is_converged])\n\n    # Final print statement in the exact required format.\n    # We construct the string manually to match the required format including spaces etc. 'str' on a list is perfect.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While implied timescale analysis provides a crucial visual diagnostic, the Chapman-Kolmogorov (CK) test offers a more rigorous, quantitative assessment of a model's validity. The test directly verifies if the model's predictions for long-timescale dynamics are consistent with the dynamics estimated directly from the data, all within the bounds of statistical uncertainty. This hands-on exercise will guide you through implementing the CK test from first principles, including the generation of synthetic data and the proper use of statistical intervals to make a definitive judgment on model quality .",
            "id": "3838019",
            "problem": "You are asked to implement and apply the Chapman–Kolmogorov test to assess the validity of a Markov State Model (MSM) constructed from synthetic molecular dynamics-like trajectories. You will formalize the test in purely mathematical terms and quantify whether the MSM is consistent with the Markov property within statistical uncertainties. The test must be applied across several lag steps and on multiple synthetic systems to probe both valid and invalid scenarios.\n\nUse the following foundational definitions and facts.\n\nA discrete-time Markov chain on a finite state space has a row-stochastic transition matrix at a given lag time, denoted by $T(\\tau)$, where each element $T(\\tau)_{ij}$ satisfies $T(\\tau)_{ij} \\ge 0$ and $\\sum_{j} T(\\tau)_{ij} = 1$. For an integer multiple $k$ of the base lag $\\tau$, the Chapman–Kolmogorov property states\n$$\nT(k \\tau) = \\left(T(\\tau)\\right)^k,\n$$\nwhere the superscript denotes standard matrix multiplication. This implication underpins the MSM validity at longer time scales.\n\nGiven a trajectory of states $\\{x_t\\}_{t=0}^{N-1}$ sampled at uniform discrete time steps of base lag $\\tau$, define the lag-$L$ transition count matrix $C^{(L)}$ by\n$$\nC_{ij}^{(L)} = \\left|\\left\\{ t \\in \\{0,\\dots,N-L-1\\} : x_t = i,\\, x_{t+L} = j \\right\\}\\right|,\n$$\nwith row sums $n_i^{(L)} = \\sum_{j} C_{ij}^{(L)}$. The maximum likelihood estimate of the lag-$L$ transition matrix is the row-normalized counts,\n$$\n\\hat{T}_{ij}^{(L)} = \n\\begin{cases}\n\\dfrac{C_{ij}^{(L)}}{n_i^{(L)}},  n_i^{(L)}  0, \\\\\n0,  n_i^{(L)} = 0.\n\\end{cases}\n$$\n\nAssume a Dirichlet prior with symmetric concentration parameters $\\alpha_0$ for each outcome in a row, leading to a Dirichlet posterior $\\text{Dir}(\\alpha_0 + \\mathbf{n}_i)$ for row $i$, where $\\mathbf{n}_i$ denotes the vector of row counts. The marginal posterior for element $p_{ij}^{(L)}$ is a Beta distribution,\n$$\np_{ij}^{(L)} \\sim \\text{Beta}\\left(C_{ij}^{(L)} + \\alpha_0,\\, n_i^{(L)} - C_{ij}^{(L)} + (m-1)\\alpha_0\\right),\n$$\nwhere $m$ is the number of states. A $100\\gamma\\%$ credible interval for $p_{ij}^{(L)}$ is obtained via the $\\gamma$-level central quantiles of the Beta distribution.\n\nThe Chapman–Kolmogorov test proceeds as follows for a set of integer lag multipliers $\\{k\\}$:\n1. Estimate the base-lag matrix $\\hat{T}^{(1)}$ from $C^{(1)}$.\n2. For each $k$, compute the Chapman–Kolmogorov prediction $\\left(\\hat{T}^{(1)}\\right)^k$, and estimate the empirical lag-$k$ matrix $\\hat{T}^{(k)}$ from $C^{(k)}$.\n3. For each row $i$ with $n_i^{(k)} \\ge n_{\\min}$ and for each column $j$, compute the posterior credible interval $[L_{ij}^{(k)}, U_{ij}^{(k)}]$ for $p_{ij}^{(k)}$ using the Beta marginal and test whether the predicted entry $\\left(\\hat{T}^{(1)}\\right)^k_{ij}$ lies inside this interval.\n4. Define the coverage fraction at lag $k$ as\n$$\n\\text{cov}(k) = \\frac{\\text{number of covered entries}}{\\text{number of tested entries}},\n$$\nwhere tested entries are those with $n_i^{(k)} \\ge n_{\\min}$. The MSM passes at lag $k$ if $\\text{cov}(k) \\ge \\theta$, where $\\theta$ is a coverage threshold matching the chosen credible level. The MSM is declared valid for the test suite if it passes for all $k$ in the set.\n\nYour task is to implement a program that:\n- Generates synthetic trajectories from specified ground-truth transition matrices.\n- Constructs MSMs at a base lag, performs Chapman–Kolmogorov predictions for several $k$, computes empirical lag-$k$ transition matrices, constructs posterior credible intervals under a symmetric Dirichlet prior, and evaluates coverage.\n\nYour program must process the following test suite of parameterized cases.\n\nTest case A (valid MSM, long trajectory):\n- Number of states $m = 4$.\n- Ground-truth base-lag transition matrix $T^{\\text{true}}$ is\n$$\n\\begin{bmatrix}\n0.90  0.07  0.02  0.01 \\\\\n0.06  0.88  0.05  0.01 \\\\\n0.03  0.05  0.88  0.04 \\\\\n0.01  0.02  0.06  0.91\n\\end{bmatrix}.\n$$\n- Trajectory length $N = 200000$ steps.\n- Lag multipliers $\\{k\\} = \\{2,\\,5,\\,10\\}$.\n- Symmetric Dirichlet prior concentration $\\alpha_0 = 0.5$ (Jeffreys prior).\n- Credible level $\\gamma = 0.95$.\n- Minimum row count threshold $n_{\\min} = 50$.\n- Coverage threshold $\\theta = 0.95$.\n\nTest case B (valid MSM, shorter trajectory):\n- Number of states $m = 4$.\n- Ground-truth base-lag transition matrix $T^{\\text{true}}$ identical to Test case A.\n- Trajectory length $N = 10000$ steps.\n- Lag multipliers $\\{k\\} = \\{2,\\,10,\\,20\\}$.\n- Symmetric Dirichlet prior concentration $\\alpha_0 = 0.5$.\n- Credible level $\\gamma = 0.95$.\n- Minimum row count threshold $n_{\\min} = 50$.\n- Coverage threshold $\\theta = 0.95$.\n\nTest case C (invalid MSM under projection, non-Markov coarse-graining):\n- Underlying number of microstates $m_{\\text{micro}} = 3$, labeled $A$, $B$, $C$.\n- Ground-truth base-lag transition matrix on microstates is\n$$\n\\begin{bmatrix}\n0.75  0.25  0.00 \\\\\n0.10  0.80  0.10 \\\\\n0.00  0.005  0.995\n\\end{bmatrix}.\n$$\n- Observed coarse-graining maps microstates into two macrostates $X$ and $Y$ via $(A \\rightarrow X)$, $(B \\rightarrow Y)$, $(C \\rightarrow X)$, yielding an observed $m = 2$ state process that is not Markovian due to memory in $X$ arising from distinct escape propensities of $A$ and $C$.\n- Trajectory length $N = 200000$ steps on microstates, subsequently mapped to observed macrostates.\n- Lag multipliers $\\{k\\} = \\{2,\\,5,\\,10\\}$.\n- Symmetric Dirichlet prior concentration $\\alpha_0 = 0.5$.\n- Credible level $\\gamma = 0.95$.\n- Minimum row count threshold $n_{\\min} = 50$.\n- Coverage threshold $\\theta = 0.95$.\n\nImplementation requirements:\n- Use a fixed random seed for reproducibility.\n- For each test case, simulate a trajectory from the specified ground-truth transition matrix at base lag $1$, estimate $\\hat{T}^{(1)}$, compute predicted $\\left(\\hat{T}^{(1)}\\right)^k$, compute empirical $\\hat{T}^{(k)}$, form posterior credible intervals based on $\\text{Beta}\\left(C_{ij}^{(k)} + \\alpha_0,\\, n_i^{(k)} - C_{ij}^{(k)} + (m-1)\\alpha_0\\right)$ at level $\\gamma$, and evaluate $\\text{cov}(k)$ for each $k$. The test case returns a boolean indicating whether $\\text{cov}(k) \\ge \\theta$ for all $k$ in the specified set.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\"), where each entry is a boolean corresponding to the pass or fail outcome for the respective test case.",
            "solution": "The problem is valid. It presents a well-defined computational task grounded in the standard theory of Markov State Model (MSM) validation from computational statistical mechanics. It is scientifically sound, objective, and complete. All parameters and procedures are explicitly specified.\n\nThe core of the problem is to implement and apply the Chapman–Kolmogorov (C-K) test for MSM validation. An MSM is a useful approximation if the underlying dynamics are Markovian on the chosen state space. The C-K property, $T(k\\tau) = [T(\\tau)]^k$, provides a necessary condition for this Markovianity. It asserts that the transition probabilities for a long lag time $k\\tau$ can be predicted by simply propagating the transition probabilities of a short base lag time $\\tau$.\n\nThe validation procedure involves comparing this prediction with an empirical estimate of the transition matrix at the longer lag time. If the system is truly Markovian, the prediction should agree with the empirical measurement within statistical uncertainty. The procedure is as follows:\n\n1.  **Synthetic Data Generation**: For each test case, a discrete-time trajectory of states $\\{x_t\\}_{t=0}^{N-1}$ is simulated. This is achieved by starting in a random state (e.g., state $0$) and iteratively drawing the next state $x_{t+1}$ from the multinomial distribution defined by row $x_t$ of the ground-truth transition matrix, $T^{\\text{true}}$. For Test Case C, this simulation is performed on the underlying $3$-state microstate space, and the resulting trajectory is then mapped to the observed $2$-state macrostate space according to the specified coarse-graining rule: state $A \\to X$, $B \\to Y$, $C \\to X$. This coarse-graining is known to induce non-Markovian dynamics in the observed process.\n\n2.  **Estimation of Transition Matrices**:\n    -   From the generated trajectory, we first estimate the base-lag transition matrix, $\\hat{T}^{(1)}$, where the lag is $\\tau=1$ time step. This is done by first computing the transition count matrix $C^{(1)}$, where $C_{ij}^{(1)}$ is the number of observed transitions from state $i$ to state $j$ in one step. The maximum likelihood estimate (MLE) is then the row-normalized count matrix: $\\hat{T}_{ij}^{(1)} = C_{ij}^{(1)} / \\sum_k C_{ik}^{(1)}$.\n    -   For each lag multiplier $k$ in the test set, we compute the C-K prediction, which is the base-lag MLE matrix raised to the $k$-th power: $T^{\\text{pred}}(k) = (\\hat{T}^{(1)})^k$.\n    -   We also compute the empirical transition matrix at lag $k$, $\\hat{T}^{(k)}$, by directly counting transitions over $k$ steps in the trajectory to form $C^{(k)}$ and then row-normalizing.\n\n3.  **Statistical Comparison via Credible Intervals**: The key step is to determine if the difference between the prediction $T^{\\text{pred}}(k)$ and the empirical estimate $\\hat{T}^{(k)}$ is statistically significant. We treat the empirical estimate not as a single point but as a distribution reflecting finite-sampling uncertainty.\n    -   Using a Bayesian framework with a symmetric Dirichlet prior (parameterized by $\\alpha_0$), the posterior distribution for each transition probability $p_{ij}^{(k)}$ is a Beta distribution: $p_{ij}^{(k)} \\sim \\text{Beta}(C_{ij}^{(k)} + \\alpha_0, n_i^{(k)} - C_{ij}^{(k)} + (m-1)\\alpha_0)$, where $n_i^{(k)} = \\sum_j C_{ij}^{(k)}$ is the total number of transitions starting from state $i$ at lag $k$, and $m$ is the number of states.\n    -   For each element $(i,j)$ in a row $i$ with sufficient statistics ($n_i^{(k)} \\ge n_{\\min}$), we compute a $100\\gamma\\%$ credible interval $[L_{ij}^{(k)}, U_{ij}^{(k)}]$. This interval is defined by the lower and upper quantiles of the Beta posterior distribution, specifically the $(\\frac{1-\\gamma}{2})$ and $(1 - \\frac{1-\\gamma}{2})$ quantiles.\n    -   We then check if the predicted value, $(T^{\\text{pred}}(k))_{ij}$, falls within this interval: $L_{ij}^{(k)} \\le (T^{\\text{pred}}(k))_{ij} \\le U_{ij}^{(k)}$.\n\n4.  **Verdict Formulation**:\n    -   The coverage fraction at lag $k$, $\\text{cov}(k)$, is calculated as the fraction of tested matrix elements for which the prediction falls inside the credible interval.\n    -   The MSM is considered to have passed the test at lag $k$ if this coverage fraction is greater than or equal to a threshold $\\theta$, i.e., $\\text{cov}(k) \\ge \\theta$. The threshold $\\theta$ is typically set equal to the credible level $\\gamma$, reflecting the expectation that, for a valid model, approximately $100\\gamma\\%$ of the predictions should lie within their corresponding $100\\gamma\\%$ credible intervals.\n    -   The final result for a given test case is a single boolean value: `True` if the MSM passes the test for all specified lag multipliers $\\{k\\}$, and `False` otherwise.\n\n-   **Test Case A (Valid MSM, Long Trajectory)**: The underlying process is Markovian and the long trajectory provides good statistics. We expect the C-K prediction to be accurate and the coverage fraction to be high (close to $\\gamma=0.95$), thus passing the test.\n-   **Test Case B (Valid MSM, Shorter Trajectory)**: The process is still Markovian, but the shorter trajectory leads to greater statistical uncertainty. This means the credible intervals for $\\hat{T}^{(k)}$ will be wider, but the initial estimate $\\hat{T}^{(1)}$ will also be less precise, leading to larger errors in the prediction $(\\hat{T}^{(1)})^k$. The test result depends on which of these effects dominates. Typically, the increased width of the credible intervals is sufficient to maintain high coverage. We expect this case to pass.\n-   **Test Case C (Invalid MSM, Coarse-Graining)**: The coarse-graining introduces memory into the observed dynamics, violating the Markov assumption. For example, the probability of transitioning from macrostate $X$ to $Y$ depends on whether the system is in microstate $A$ or $C$. The MSM, which averages over these microstates, cannot capture this. The C-K prediction, based on the faulty Markov assumption, will systematically deviate from the true long-term dynamics. We expect the coverage fraction to fall significantly below the threshold $\\theta$, causing the test to fail.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Main function to run the Chapman-Kolmogorov test suite.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    T_true_A_B = np.array([\n        [0.90, 0.07, 0.02, 0.01],\n        [0.06, 0.88, 0.05, 0.01],\n        [0.03, 0.05, 0.88, 0.04],\n        [0.01, 0.02, 0.06, 0.91]\n    ])\n\n    T_true_C_micro = np.array([\n        [0.75, 0.25, 0.00],\n        [0.10, 0.80, 0.10],\n        [0.00, 0.005, 0.995]\n    ])\n\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"m\": 4,\n            \"T_true\": T_true_A_B,\n            \"N\": 200000,\n            \"lags\": [2, 5, 10],\n            \"alpha0\": 0.5,\n            \"gamma\": 0.95,\n            \"n_min\": 50,\n            \"theta\": 0.95\n        },\n        {\n            \"name\": \"B\",\n            \"m\": 4,\n            \"T_true\": T_true_A_B,\n            \"N\": 10000,\n            \"lags\": [2, 10, 20],\n            \"alpha0\": 0.5,\n            \"gamma\": 0.95,\n            \"n_min\": 50,\n            \"theta\": 0.95\n        },\n        {\n            \"name\": \"C\",\n            \"m\": 2, # Number of macrostates\n            \"m_micro\": 3,\n            \"T_true\": T_true_C_micro,\n            \"N\": 200000,\n            \"lags\": [2, 5, 10],\n            \"alpha0\": 0.5,\n            \"gamma\": 0.95,\n            \"n_min\": 50,\n            \"theta\": 0.95,\n            \"coarse_grain_map\": np.array([0, 1, 0]) # A->X(0), B->Y(1), C->X(0)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        case_result = run_ck_test(case)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map to str is required for Python's bool capitalization (True vs true)\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\n\ndef generate_trajectory(T_true, N, start_state=0):\n    \"\"\"\n    Generates a trajectory from a discrete-time Markov chain.\n    \"\"\"\n    num_states = T_true.shape[0]\n    traj = np.zeros(N, dtype=int)\n    traj[0] = start_state\n    for t in range(N - 1):\n        next_state_probs = T_true[traj[t], :]\n        traj[t+1] = np.random.choice(num_states, p=next_state_probs)\n    return traj\n\n\ndef get_count_matrix(trajectory, m, lag):\n    \"\"\"\n    Computes the transition count matrix for a given lag time.\n    \"\"\"\n    C = np.zeros((m, m), dtype=int)\n    for t in range(len(trajectory) - lag):\n        i = trajectory[t]\n        j = trajectory[t+lag]\n        C[i, j] += 1\n    return C\n\n\ndef get_mle_t_matrix(C):\n    \"\"\"\n    Computes the MLE transition matrix from a count matrix.\n    \"\"\"\n    m = C.shape[0]\n    T_mle = np.zeros((m, m))\n    row_sums = C.sum(axis=1)\n    # Using np.where to avoid division by zero\n    T_mle = np.where(row_sums[:, None] > 0, C / row_sums[:, None], 0)\n    return T_mle\n\n\ndef run_ck_test(params):\n    \"\"\"\n    Runs the full Chapman-Kolmogorov test for a single parameter set.\n    \"\"\"\n    m = params[\"m\"]\n    N = params[\"N\"]\n    T_true = params[\"T_true\"]\n    lags = params[\"lags\"]\n    alpha0 = params[\"alpha0\"]\n    gamma = params[\"gamma\"]\n    n_min = params[\"n_min\"]\n    theta = params[\"theta\"]\n\n    # --- 1. Generate Trajectory ---\n    if \"coarse_grain_map\" in params:\n        micro_traj = generate_trajectory(T_true, N)\n        trajectory = params[\"coarse_grain_map\"][micro_traj]\n    else:\n        trajectory = generate_trajectory(T_true, N)\n\n    # --- 2. Estimate Base-Lag Matrix ---\n    C1 = get_count_matrix(trajectory, m, lag=1)\n    T_hat_1 = get_mle_t_matrix(C1)\n\n    case_passes = True\n    for k in lags:\n        # --- 3. C-K Prediction ---\n        T_pred_k = np.linalg.matrix_power(T_hat_1, k)\n\n        # --- 4. Empirical Lag-k Matrix ---\n        Ck = get_count_matrix(trajectory, m, lag=k)\n        \n        # --- 5. Statistical Comparison ---\n        covered_entries = 0\n        tested_entries = 0\n        \n        row_sums_k = Ck.sum(axis=1)\n        \n        for i in range(m):\n            if row_sums_k[i]  n_min:\n                continue\n            \n            for j in range(m):\n                tested_entries += 1\n                \n                # Beta posterior parameters\n                alpha_post = Ck[i, j] + alpha0\n                beta_post = row_sums_k[i] - Ck[i, j] + (m - 1) * alpha0\n                \n                # Credible interval\n                q_low = (1.0 - gamma) / 2.0\n                q_high = 1.0 - q_low\n                \n                lower_bound = beta.ppf(q_low, alpha_post, beta_post)\n                upper_bound = beta.ppf(q_high, alpha_post, beta_post)\n\n                # Check if prediction is covered\n                prediction = T_pred_k[i, j]\n                if lower_bound = prediction = upper_bound:\n                    covered_entries += 1\n        \n        # --- 6. Verdict ---\n        if tested_entries > 0:\n            coverage_fraction = covered_entries / tested_entries\n        else: # No rows met n_min, technically passes as no violations found.\n            coverage_fraction = 1.0\n\n        if coverage_fraction  theta:\n            case_passes = False\n            break # Fail fast, no need to check other lags for this case\n\n    return case_passes\n\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}