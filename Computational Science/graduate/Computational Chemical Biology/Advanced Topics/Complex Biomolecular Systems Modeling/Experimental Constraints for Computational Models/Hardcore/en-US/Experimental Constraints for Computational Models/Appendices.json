{
    "hands_on_practices": [
        {
            "introduction": "A fundamental task in computational modeling is to quantitatively evaluate how well a model agrees with experimental reality. This practice introduces the chi-square ($\\chi^2$) goodness-of-fit test, a cornerstone of statistical validation, which allows us to move beyond qualitative comparisons. By implementing this test , you will learn to calculate whether the deviations between a model's predictions and measured data are statistically plausible given experimental uncertainties.",
            "id": "3845717",
            "problem": "You are given multiple independent datasets of experimental constraints for a fixed computational chemical biology model that predicts measurable quantities. Each dataset comprises observed values, model-predicted values, and estimated measurement uncertainties. Assume that each measurement error is independent and normally distributed, and that some datasets have had a subset of model parameters tuned using their data. Under these assumptions, standardized residuals follow a standard normal distribution and their sum of squares follows a chi-square distribution. Your task is to implement a complete program that computes per-dataset residuals, chi-square test statistics, and goodness-of-fit decisions, and also aggregates these into a global goodness-of-fit across datasets.\n\nUse the following foundational base:\n- Under independent measurement errors modeled as normally distributed with known standard deviations, if the model predicts latent mean values, then the standardized residuals $$r_i = \\frac{y_i - m_i}{\\sigma_i}$$ are approximately independent draws from a standard normal distribution.\n- By properties of the normal distribution, the sum $$X^2 = \\sum_{i=1}^{n} r_i^2$$ follows a chi-square distribution with degrees of freedom (DoF) $$\\nu = n - k,$$ where $$n$$ is the number of independent measurements in the dataset and $$k$$ is the number of model parameters effectively fit using that dataset.\n- For a chi-square random variable $$X^2 \\sim \\chi^2_{\\nu}$$, the tail probability (goodness-of-fit p-value) equals $$p = 1 - F_{\\chi^2_{\\nu}}(X^2),$$ where $$F_{\\chi^2_{\\nu}}$$ denotes the cumulative distribution function (CDF) of the chi-square distribution.\n\nProgram requirements:\n- For each dataset, compute the residual vector $$\\mathbf{r} = \\left[\\frac{y_1 - m_1}{\\sigma_1}, \\ldots, \\frac{y_n - m_n}{\\sigma_n}\\right],$$ the chi-square statistic $$X^2 = \\sum_{i=1}^{n} r_i^2,$$ the degrees of freedom $$\\nu = n - k,$$ the p-value $$p = 1 - F_{\\chi^2_{\\nu}}(X^2),$$ and the test decision at significance level $$\\alpha$$, defined as $$\\text{pass} = (p \\ge \\alpha).$$\n- Independently across datasets, aggregate a global goodness-of-fit by summing chi-square statistics and degrees of freedom: $$X^2_{\\text{global}} = \\sum_{d} X^2_d,$$ $$\\nu_{\\text{global}} = \\sum_{d} \\nu_d,$$ and $$p_{\\text{global}} = 1 - F_{\\chi^2_{\\nu_{\\text{global}}}}(X^2_{\\text{global}}),$$ with $$\\text{pass}_{\\text{global}} = (p_{\\text{global}} \\ge \\alpha).$$\n- Round all floating-point outputs (residuals, chi-square statistics, and p-values) to six decimal places.\n\nInput specification embedded in the program:\n- There is no external input. Your program must internally define the following test suite of cases, each containing multiple datasets. The measurement units are consistent and the outputs (residuals, chi-square statistics, and p-values) are dimensionless. The arrays are given in the order $[y_1, \\ldots, y_n]$, $[m_1, \\ldots, m_n]$, and $[\\sigma_1, \\ldots, \\sigma_n]$ with $k$ indicating the number of fitted parameters subtracted from the degrees of freedom.\n\nTest Suite:\n- Case 1 (general consistency, homogeneous uncertainties, moderate residuals):\n  - Significance level: $$\\alpha = 0.05.$$\n  - Dataset A:\n    - Observed: $$\\mathbf{y}_A = [1.0, 0.8, 1.2, 0.9, 1.1].$$\n    - Predicted: $$\\mathbf{m}_A = [1.0, 0.75, 1.15, 0.95, 1.05].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_A = [0.1, 0.1, 0.1, 0.1, 0.1].$$\n    - Fitted parameters: $$k_A = 1.$$\n  - Dataset B:\n    - Observed: $$\\mathbf{y}_B = [2.0, 1.8, 2.1, 1.9].$$\n    - Predicted: $$\\mathbf{m}_B = [2.05, 1.85, 2.05, 1.95].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_B = [0.1, 0.1, 0.1, 0.1].$$\n    - Fitted parameters: $$k_B = 1.$$\n- Case 2 (heteroscedastic uncertainties and mixed residual magnitudes):\n  - Significance level: $$\\alpha = 0.05.$$\n  - Dataset C:\n    - Observed: $$\\mathbf{y}_C = [5.0, 5.3, 4.8, 5.2].$$\n    - Predicted: $$\\mathbf{m}_C = [5.1, 5.2, 5.0, 5.1].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_C = [0.2, 0.3, 0.2, 0.4].$$\n    - Fitted parameters: $$k_C = 2.$$\n  - Dataset D:\n    - Observed: $$\\mathbf{y}_D = [3.0, 3.2, 2.9].$$\n    - Predicted: $$\\mathbf{m}_D = [3.1, 3.0, 3.0].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_D = [0.2, 0.2, 0.1].$$\n    - Fitted parameters: $$k_D = 1.$$\n- Case 3 (strong inconsistency, small uncertainties leading to large residuals):\n  - Significance level: $$\\alpha = 0.05.$$\n  - Dataset E:\n    - Observed: $$\\mathbf{y}_E = [0.9, 1.1, 0.95, 1.05, 1.0].$$\n    - Predicted: $$\\mathbf{m}_E = [1.2, 1.2, 1.2, 1.2, 1.2].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_E = [0.05, 0.05, 0.05, 0.05, 0.05].$$\n    - Fitted parameters: $$k_E = 0.$$\n  - Dataset F:\n    - Observed: $$\\mathbf{y}_F = [10.0, 9.5, 10.5, 9.8].$$\n    - Predicted: $$\\mathbf{m}_F = [10.0, 10.0, 10.0, 10.0].$$\n    - Uncertainties: $$\\boldsymbol{\\sigma}_F = [0.2, 0.2, 0.2, 0.2].$$\n    - Fitted parameters: $$k_F = 0.$$\n\nOutput specification:\n- For each case, output a list comprising one entry per dataset and one final global entry. Each dataset entry must be a list $$[\\mathbf{r}, X^2, p, \\text{pass}]$$ where $$\\mathbf{r}$$ is the residual vector rounded to six decimals, $$X^2$$ is the chi-square statistic rounded to six decimals, $$p$$ is the p-value rounded to six decimals, and $$\\text{pass}$$ is a boolean. The final global entry must be a list $$[X^2_{\\text{global}}, p_{\\text{global}}, \\text{pass}_{\\text{global}}]$$ with the same rounding for floating values.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, with no spaces anywhere. That is, the top-level output has the form $$[\\text{case}_1,\\text{case}_2,\\text{case}_3]$$ where each $$\\text{case}_i$$ is itself a list constructed as specified above.",
            "solution": "The starting point is the probabilistic model of experimental constraints for a computational chemical biology system with a fixed parameterization. Let $$y_i$$ denote the observed measurement and $$m_i$$ denote the corresponding model-predicted mean for the same experimental condition, with $$\\sigma_i$$ denoting the known or estimated standard deviation of the measurement error. We assume independent, normally distributed errors so that $$y_i = m_i + \\varepsilon_i$$ with $$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$$ and errors across different measurements and datasets are independent. This assumption is justified when measurement protocols are independent and replicate variability is well characterized, which is a common scenario within computational chemical biology when integrating orthogonal assays.\n\nUnder this model, each standardized residual $$r_i = \\frac{y_i - m_i}{\\sigma_i}$$ follows a standard normal distribution $$\\mathcal{N}(0,1)$$ and, due to independence, the vector $$\\mathbf{r}$$ is composed of independent standard normal entries. A core statistical fact is that the sum of squares of independent standard normal variables follows a chi-square distribution. Specifically, the statistic\n$$\nX^2 = \\sum_{i=1}^{n} r_i^2\n$$\nhas a chi-square distribution with degrees of freedom $$\\nu = n - k$$, where $$n$$ is the number of measurements in the dataset and $$k$$ is the number of effectively fitted model parameters used to tune predictions with that dataset. The subtraction by $$k$$ accounts for the loss of degrees of freedom incurred by parameter estimation, a standard correction in goodness-of-fit testing.\n\nTo assess goodness-of-fit, we compute the p-value as the upper-tail probability of the chi-square distribution:\n$$\np = \\mathbb{P}\\left(\\chi^2_{\\nu} \\ge X^2\\right) = 1 - F_{\\chi^2_{\\nu}}(X^2),\n$$\nwhere $$F_{\\chi^2_{\\nu}}$$ is the cumulative distribution function (CDF) of the chi-square distribution with $$\\nu$$ degrees of freedom. Given a significance level $$\\alpha$$, the test decision is\n$$\n\\text{pass} =\n\\begin{cases}\n\\text{True},  \\text{if } p \\ge \\alpha, \\\\\n\\text{False},  \\text{if } p  \\alpha.\n\\end{cases}\n$$\n\nWhen combining datasets that are experimentally independent, the global goodness-of-fit statistic is the sum of individual chi-square statistics, and the global degrees of freedom is the sum of individual degrees of freedom:\n$$\nX^2_{\\text{global}} = \\sum_{d} X^2_d, \\quad \\nu_{\\text{global}} = \\sum_{d} \\nu_d.\n$$\nBy the additivity property of chi-square variables derived from sums of independent squared standard normals, $$X^2_{\\text{global}} \\sim \\chi^2_{\\nu_{\\text{global}}}$$ under the model assumptions. Consequently,\n$$\np_{\\text{global}} = 1 - F_{\\chi^2_{\\nu_{\\text{global}}}}(X^2_{\\text{global}})\n$$\nwith the same decision rule $$\\text{pass}_{\\text{global}} = (p_{\\text{global}} \\ge \\alpha)$$.\n\nAlgorithmic design:\n1. For each dataset, read $$\\mathbf{y}, \\mathbf{m}, \\boldsymbol{\\sigma}, k$$ and $$\\alpha$$. Compute the residual vector as $$\\mathbf{r} = (\\mathbf{y} - \\mathbf{m}) \\oslash \\boldsymbol{\\sigma}$$, where $$\\oslash$$ denotes element-wise division.\n2. Compute $$X^2 = \\sum r_i^2$$ and $$\\nu = n - k$$. Check that $$\\nu > 0$$ to ensure validity of the chi-square goodness-of-fit test.\n3. Compute the p-value $$p = 1 - F_{\\chi^2_{\\nu}}(X^2)$$ using a reliable numerical routine for the chi-square survival function, which directly computes upper-tail probabilities to maintain numerical stability.\n4. Compare $$p$$ against $$\\alpha$$ and set $$\\text{pass}$$ accordingly.\n5. Aggregate across datasets by summing $$X^2$$ and $$\\nu$$ to obtain $$X^2_{\\text{global}}$$ and $$\\nu_{\\text{global}}$$, then compute $$p_{\\text{global}}$$ and $$\\text{pass}_{\\text{global}}$$.\n6. Round all floating outputs to six decimals, as required, and construct the output structure per the specification.\n\nApplication to the test suite:\n- Case 1 uses homogeneous uncertainties $$\\boldsymbol{\\sigma}_A = [0.1, 0.1, 0.1, 0.1, 0.1]$$ and $$\\boldsymbol{\\sigma}_B = [0.1, 0.1, 0.1, 0.1]$$ with moderate residuals, and fitted parameters $$k_A = 1$$, $$k_B = 1$$. The computed $$X^2$$ values are small relative to their respective $$\\nu$$, yielding large p-values and passing the test at $$\\alpha = 0.05$$.\n- Case 2 demonstrates heteroscedastic uncertainties and mixed residuals, with $$k_C = 2$$, $$k_D = 1$$. The per-dataset $$X^2$$ are moderate and the aggregated global $$X^2_{\\text{global}}$$ with $$\\nu_{\\text{global}}$$ yields a non-small p-value, passing at $$\\alpha = 0.05$$.\n- Case 3 has small uncertainties and large discrepancies, leading to large $$X^2$$ for both datasets and a very small global p-value; the test fails at $$\\alpha = 0.05$$.\n\nThe program implements these steps and prints a single line containing the list of results for all cases. Each dataset entry is of the form $$[\\mathbf{r}, X^2, p, \\text{pass}]$$ and each global entry is $$[X^2_{\\text{global}}, p_{\\text{global}}, \\text{pass}_{\\text{global}}]$$, all rounded to six decimals. The top-level list contains the three cases in order, formatted with commas and brackets without spaces, enabling automated parsing and verification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef compute_dataset_metrics(y, m, s, k, alpha):\n    \"\"\"\n    Compute residuals, chi-square statistic, p-value, and pass/fail for one dataset.\n    y, m, s are numpy arrays of equal length; k is integer; alpha is float.\n    \"\"\"\n    # Standardized residuals\n    residuals = (y - m) / s\n    # Chi-square statistic\n    chi_sq = float(np.sum(residuals ** 2))\n    # Degrees of freedom\n    nu = int(len(y) - k)\n    if nu = 0:\n        raise ValueError(\"Degrees of freedom must be positive (n - k > 0).\")\n    # p-value (upper-tail probability)\n    p_value = float(chi2.sf(chi_sq, df=nu))\n    # Decision\n    passed = bool(p_value >= alpha)\n    return residuals, chi_sq, p_value, passed, nu\n\ndef round_float(x, ndigits=6):\n    return float(np.round(x, ndigits))\n\ndef round_list(lst, ndigits=6):\n    return [round_float(float(v), ndigits) for v in lst]\n\ndef format_element_no_space(el):\n    \"\"\"\n    Recursively format nested lists and primitive types into a string\n    without spaces, rounding floats to six decimals.\n    \"\"\"\n    if isinstance(el, list):\n        return \"[\" + \",\".join(format_element_no_space(x) for x in el) + \"]\"\n    elif isinstance(el, (float, np.floating)):\n        return f\"{float(el):.6f}\"\n    elif isinstance(el, (int, np.integer)):\n        return str(int(el))\n    elif isinstance(el, bool):\n        return \"True\" if el else \"False\"\n    else:\n        # Attempt to handle numpy arrays by converting to list\n        if isinstance(el, np.ndarray):\n            return format_element_no_space(el.tolist())\n        # Fallback: convert to string (should not occur in this problem)\n        return str(el)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"alpha\": 0.05,\n            \"datasets\": [\n                {\n                    \"y\": np.array([1.0, 0.8, 1.2, 0.9, 1.1]),\n                    \"m\": np.array([1.0, 0.75, 1.15, 0.95, 1.05]),\n                    \"s\": np.array([0.1, 0.1, 0.1, 0.1, 0.1]),\n                    \"k\": 1,\n                },\n                {\n                    \"y\": np.array([2.0, 1.8, 2.1, 1.9]),\n                    \"m\": np.array([2.05, 1.85, 2.05, 1.95]),\n                    \"s\": np.array([0.1, 0.1, 0.1, 0.1]),\n                    \"k\": 1,\n                },\n            ],\n        },\n        # Case 2\n        {\n            \"alpha\": 0.05,\n            \"datasets\": [\n                {\n                    \"y\": np.array([5.0, 5.3, 4.8, 5.2]),\n                    \"m\": np.array([5.1, 5.2, 5.0, 5.1]),\n                    \"s\": np.array([0.2, 0.3, 0.2, 0.4]),\n                    \"k\": 2,\n                },\n                {\n                    \"y\": np.array([3.0, 3.2, 2.9]),\n                    \"m\": np.array([3.1, 3.0, 3.0]),\n                    \"s\": np.array([0.2, 0.2, 0.1]),\n                    \"k\": 1,\n                },\n            ],\n        },\n        # Case 3\n        {\n            \"alpha\": 0.05,\n            \"datasets\": [\n                {\n                    \"y\": np.array([0.9, 1.1, 0.95, 1.05, 1.0]),\n                    \"m\": np.array([1.2, 1.2, 1.2, 1.2, 1.2]),\n                    \"s\": np.array([0.05, 0.05, 0.05, 0.05, 0.05]),\n                    \"k\": 0,\n                },\n                {\n                    \"y\": np.array([10.0, 9.5, 10.5, 9.8]),\n                    \"m\": np.array([10.0, 10.0, 10.0, 10.0]),\n                    \"s\": np.array([0.2, 0.2, 0.2, 0.2]),\n                    \"k\": 0,\n                },\n            ],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        case_results = []\n        chi_sq_global = 0.0\n        nu_global = 0\n        for ds in case[\"datasets\"]:\n            y = ds[\"y\"]\n            m = ds[\"m\"]\n            s = ds[\"s\"]\n            k = ds[\"k\"]\n            residuals, chi_sq, p_value, passed, nu = compute_dataset_metrics(y, m, s, k, alpha)\n            chi_sq_global += chi_sq\n            nu_global += nu\n            # Round values\n            residuals_rounded = round_list(residuals.tolist(), ndigits=6)\n            chi_sq_rounded = round_float(chi_sq, ndigits=6)\n            p_value_rounded = round_float(p_value, ndigits=6)\n            case_results.append([residuals_rounded, chi_sq_rounded, p_value_rounded, passed])\n        # Global aggregation\n        p_global = float(chi2.sf(chi_sq_global, df=nu_global))\n        pass_global = bool(p_global >= alpha)\n        chi_sq_global_rounded = round_float(chi_sq_global, ndigits=6)\n        p_global_rounded = round_float(p_global, ndigits=6)\n        case_results.append([chi_sq_global_rounded, p_global_rounded, pass_global])\n        results.append(case_results)\n\n    # Final print statement in the exact required format: no spaces anywhere.\n    print(format_element_no_space(results))\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simple validation, experimental data serves as a powerful tool for refining and parameterizing flexible computational models, such as conformational ensembles. This exercise  guides you through a state-of-the-art approach that uses Bayesian inference to determine the population weights of an ensemble that best explain experimental measurements. You will formulate this as a convex optimization problem, combining a data-driven likelihood with an entropic prior to achieve a robust and physically meaningful solution.",
            "id": "3845696",
            "problem": "Consider a computational chemical biology setting in which the relative population weights of an ensemble of $N$ discrete molecular states are collected into a vector $w \\in \\mathbb{R}^N$ constrained by $w_i \\ge 0$ for all $i$ and $\\sum_{i=1}^N w_i = 1$. A set of $M$ experimental observables is modeled as a linear forward map with a matrix $A \\in \\mathbb{R}^{M \\times N}$, so that the predicted observables are $A w$. The measured observables form a vector $y \\in \\mathbb{R}^M$. Measurement noise is modeled as zero-mean multivariate Gaussian with diagonal covariance $\\Sigma = \\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_M^2)$, where $\\sigma_j  0$ is known. A reference prior on the weights encodes prior knowledge or an uninformative baseline through a strictly positive reference vector $w^{(0)} \\in \\mathbb{R}^N$ with $\\sum_{i=1}^N w^{(0)}_i = 1$. The goal is to obtain the Maximum A Posteriori (MAP) estimate of $w$ under the Gaussian likelihood and a convex entropic regularizer derived from the Kullback-Leibler (KL) divergence with strength parameter $\\lambda  0$, subject to the simplex constraints stated above. You must derive the convex optimization objective from the fundamental base consisting of Bayes’ theorem, properties of the multivariate Gaussian distribution, and the principle of maximum entropy, and then implement a program to compute the MAP estimate for several test cases by solving the resulting constrained convex optimization problem. The verification must include a check of normalization and nonnegativity of the output weights.\n\nDerivation requirements: Starting from Bayes’ theorem and the log-likelihood of a multivariate Gaussian with mean $A w$ and covariance $\\Sigma$, and using the maximum entropy principle to motivate an entropic prior relative to $w^{(0)}$, derive the convex objective minimized by the MAP estimator of $w$ together with its domain constraints, without introducing any additional non-fundamental assumptions. Justify convexity of the resulting problem from first principles. Do not provide shortcut formulas in the problem statement.\n\nNumerical and algorithmic requirements: Implement a solver that computes the MAP weights $w$ for each test case using convex optimization under linear equality and bound constraints. Use a numerical method appropriate for smooth convex problems and provide gradients to the optimizer to ensure reliable convergence. After computing $w$, verify that the normalization $\\sum_{i=1}^N w_i = 1$ holds within an absolute tolerance of $10^{-8}$ and that $w_i \\ge 0$ holds within an absolute tolerance of $10^{-12}$. For each test case, report a result list that concatenates the optimized weights followed by two booleans indicating whether normalization and positivity checks passed, respectively.\n\nTest suite: For each case below, $A$, $y$, $\\sigma$, $\\lambda$, and $w^{(0)}$ are specified. Here $A$ is provided as a matrix with rows separated by semicolons and columns separated by commas. All numbers below are given explicitly and must be used as provided.\n\nCase $1$:\n$$\nA = \\begin{bmatrix}\n1.0  0.0  0.5 \\\\\n0.2  0.8  0.5\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix} 0.7 \\\\ 0.5 \\end{bmatrix}, \\quad\n\\sigma = \\begin{bmatrix} 0.05 \\\\ 0.05 \\end{bmatrix}, \\quad\n\\lambda = 0.1, \\quad\nw^{(0)} = \\begin{bmatrix} \\tfrac{1}{3} \\\\ \\tfrac{1}{3} \\\\ \\tfrac{1}{3} \\end{bmatrix}.\n$$\n\nCase $2$:\n$$\nA = \\begin{bmatrix}\n1.0  0.2  0.2  0.0 \\\\\n0.0  0.7  0.1  0.9 \\\\\n0.5  0.1  0.8  0.3\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix} 0.2 \\\\ 0.6 \\\\ 0.4 \\end{bmatrix}, \\quad\n\\sigma = \\begin{bmatrix} 0.02 \\\\ 0.02 \\\\ 0.05 \\end{bmatrix}, \\quad\n\\lambda = 0.05, \\quad\nw^{(0)} = \\begin{bmatrix} 0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25 \\end{bmatrix}.\n$$\n\nCase $3$:\n$$\nA = \\begin{bmatrix}\n0.5  0.49  0.1 \\\\\n0.5  0.51  0.9\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}, \\quad\n\\sigma = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}, \\quad\n\\lambda = 1.0, \\quad\nw^{(0)} = \\begin{bmatrix} 0.2 \\\\ 0.2 \\\\ 0.6 \\end{bmatrix}.\n$$\n\nCase $4$:\n$$\nA = \\begin{bmatrix}\n0.9  0.1  0.1 \\\\\n0.8  0.2  0.2\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix} 0.9 \\\\ 0.8 \\end{bmatrix}, \\quad\n\\sigma = \\begin{bmatrix} 0.01 \\\\ 0.01 \\end{bmatrix}, \\quad\n\\lambda = 0.01, \\quad\nw^{(0)} = \\begin{bmatrix} \\tfrac{1}{3} \\\\ \\tfrac{1}{3} \\\\ \\tfrac{1}{3} \\end{bmatrix}.\n$$\n\nOutput specification: Your program must compute the MAP weights for each case and, for each case, output a list consisting of the optimized weights in order followed by two booleans: the first boolean is true if the normalization check passes within the stated tolerance and false otherwise, and the second boolean is true if the positivity check passes within the stated tolerance and false otherwise. Round each reported weight to $6$ decimal places for output. Aggregate the per-case lists into a single top-level list in the same order as the cases. The program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, for example:\n$$\n[\\,[w_{1,1},\\dots,w_{1,N_1},b_{1,\\mathrm{norm}},b_{1,\\mathrm{pos}}],\\,[w_{2,1},\\dots],\\,\\dots\\,]\n$$\nwhere $w_{k,i}$ denotes the $i$-th weight for case $k$ and $b_{k,\\cdot}$ are booleans.\n\nAngle units are not applicable. No physical units are required. All outputs must be numeric floats or booleans as specified, and all lists must follow the exact formatting stated above.",
            "solution": "The posed problem is a well-defined task in Bayesian inference, specifically Maximum A Posteriori (MAP) estimation, applied to a common scenario in computational chemical biology. The objective is to determine the optimal population weights, $w$, for an ensemble of molecular states that best explain a set of experimental observables, $y$, while being regularized by prior knowledge. We begin by validating the problem statement. The problem provides all necessary components: a linear forward model ($A$), experimental data ($y$) with specified uncertainties ($\\sigma$), a reference weight distribution ($w^{(0)}$), and a regularization parameter ($\\lambda$). The constraints on the weights, $\\sum_{i=1}^N w_i = 1$ and $w_i \\ge 0$, define the standard simplex, ensuring a physically meaningful probability distribution. The use of a Gaussian likelihood and an entropic prior (Kullback-Leibler divergence) are standard, principled choices in statistical data analysis. The problem is scientifically grounded, self-contained, and mathematically well-posed. Thus, it is deemed valid, and we may proceed with a full derivation and solution.\n\nOur derivation of the objective function starts from Bayes' theorem, which states that the posterior probability of the weights $w$ given the data $y$ is proportional to the product of the likelihood and the prior probability:\n$$\np(w | y) \\propto p(y | w) p(w)\n$$\nFinding the MAP estimate, $\\hat{w}_{\\mathrm{MAP}}$, is equivalent to maximizing this posterior probability, which in turn is equivalent to minimizing its negative logarithm:\n$$\n\\hat{w}_{\\mathrm{MAP}} = \\arg\\max_{w} p(w | y) = \\arg\\min_{w} \\left( -\\log p(w | y) \\right)\n$$\n\nThe first term, $p(y | w)$, is the likelihood of observing the data $y$ given the weights $w$. The problem specifies that the measurement noise is a zero-mean multivariate Gaussian with a diagonal covariance matrix $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_M^2)$. The mean of this distribution is given by the forward model, $Aw$. The probability density function for this multivariate Gaussian distribution is:\n$$\np(y | w) = \\frac{1}{(2\\pi)^{M/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (y - Aw)^T \\Sigma^{-1} (y - Aw)\\right)\n$$\nTaking the negative logarithm of the likelihood gives the data-fitting term of our objective function. Ignoring the constant terms that do not depend on $w$, we get:\n$$\n-\\log p(y | w) \\propto \\frac{1}{2} (Aw - y)^T \\Sigma^{-1} (Aw - y) = \\frac{1}{2} \\sum_{j=1}^{M} \\frac{((Aw)_j - y_j)^2}{\\sigma_j^2}\n$$\nThis term is a weighted sum of squared residuals, often referred to as the chi-squared ($\\chi^2$) statistic.\n\nThe second term, $p(w)$, is the prior probability of the weights. The problem directs us to use an entropic regularizer derived from the Kullback-Leibler (KL) divergence, motivated by the principle of maximum entropy. This principle suggests choosing a distribution that is as close as possible to a reference (or prior) distribution, $w^{(0)}$, while being consistent with observed data. The KL divergence, or relative entropy, measures the \"distance\" between two probability distributions $w$ and $w^{(0)}$:\n$$\nD_{KL}(w || w^{(0)}) = \\sum_{i=1}^{N} w_i \\log\\left(\\frac{w_i}{w^{(0)}_i}\\right)\n$$\nA standard choice for an entropic prior takes the form $p(w) \\propto \\exp(-\\alpha D_{KL}(w || w^{(0)}))$, where $\\alpha$ is a parameter controlling the strength of the prior. The problem specifies a regularization strength parameter $\\lambda  0$. In the context of a penalized likelihood, this parameter multiplies the regularization term in the log-posterior space. Therefore, the negative log-prior is:\n$$\n-\\log p(w) \\propto \\lambda D_{KL}(w || w^{(0)}) = \\lambda \\sum_{i=1}^{N} w_i \\log\\left(\\frac{w_i}{w^{(0)}_i}\\right)\n$$\n\nCombining the negative log-likelihood and negative log-prior, we obtain the final objective function $F(w)$ to be minimized:\n$$\nF(w) = \\frac{1}{2} \\sum_{j=1}^{M} \\left(\\frac{(Aw)_j - y_j}{\\sigma_j}\\right)^2 + \\lambda \\sum_{i=1}^{N} w_i \\log\\left(\\frac{w_i}{w^{(0)}_i}\\right)\n$$\nThis function must be minimized subject to the simplex constraints: $\\sum_{i=1}^N w_i = 1$ and $w_i \\ge 0$ for all $i \\in \\{1, \\dots, N\\}$.\n\nWe now justify the convexity of this optimization problem from first principles. The objective function $F(w)$ is a sum of two functions, $F(w) = F_1(w) + F_2(w)$.\nThe first term is the data-fitting term, $F_1(w) = \\frac{1}{2} \\sum_{j=1}^{M} \\left(\\frac{(Aw)_j - y_j}{\\sigma_j}\\right)^2$. This can be written in matrix form as $F_1(w) = \\frac{1}{2} (Aw-y)^T\\Sigma^{-1}(Aw-y)$. The gradient of this term with respect to $w$ is $\\nabla_w F_1(w) = A^T\\Sigma^{-1}(Aw-y)$. The Hessian matrix is the second derivative, $\\nabla_w^2 F_1(w) = A^T\\Sigma^{-1}A$. Since $\\Sigma^{-1}$ is a diagonal matrix with strictly positive entries $1/\\sigma_j^2$, it is positive definite. For any vector $z \\in \\mathbb{R}^N$, the quadratic form $z^T (A^T\\Sigma^{-1}A) z = (Az)^T \\Sigma^{-1} (Az) \\ge 0$. Thus, the Hessian is positive semidefinite, proving that $F_1(w)$ is a convex function.\n\nThe second term is the entropic regularizer, $F_2(w) = \\lambda \\sum_{i=1}^{N} w_i \\log\\left(\\frac{w_i}{w^{(0)}_i}\\right) = \\lambda \\sum_{i=1}^{N} (w_i \\log w_i - w_i \\log w^{(0)}_i)$. The gradient is $\\nabla_w F_2(w)_k = \\lambda (\\log(w_k/w^{(0)}_k) + 1)$. The Hessian matrix is diagonal with entries $(\\nabla_w^2 F_2(w))_{kk} = \\frac{\\partial^2 F_2}{\\partial w_k^2} = \\frac{\\lambda}{w_k}$. In the domain of the problem, $w_i  0$ and $\\lambda  0$, so all diagonal entries of the Hessian are strictly positive. This means the Hessian of $F_2(w)$ is positive definite, and therefore $F_2(w)$ is a strictly convex function.\n\nThe total objective function $F(w)$ is the sum of a convex function ($F_1$) and a strictly convex function ($F_2$), and is therefore strictly convex. The feasible region, defined by the constraints $\\sum_{i=1}^N w_i = 1$ and $w_i \\ge 0$, is a standard simplex, which is a compact and convex set. Minimizing a strictly convex function over a non-empty, compact, convex set guarantees that a unique solution exists.\n\nFor numerical solution, we employ a quasi-Newton method suitable for constrained nonlinear optimization. Specifically, the Sequential Least Squares Programming (SLSQP) algorithm, available in `scipy.optimize.minimize`, is chosen. This algorithm can handle both equality and inequality (bound) constraints. To ensure robust and efficient convergence, we must provide the analytical gradient of the objective function, $\\nabla_w F(w) = \\nabla_w F_1(w) + \\nabla_w F_2(w)$, to the optimizer:\n$$\n\\nabla_w F(w) = A^T\\Sigma^{-1}(Aw-y) + \\lambda \\left[ \\log\\left(\\frac{w_1}{w^{(0)}_1}\\right)+1, \\dots, \\log\\left(\\frac{w_N}{w^{(0)}_N}\\right)+1 \\right]^T\n$$\nThe constraints are implemented as a linear equality constraint, `{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}`, and bound constraints $w_i \\ge 0$. The reference weights $w^{(0)}$ provide a natural and well-behaved initial guess for the iterative optimization process. After obtaining the optimized weights $\\hat{w}$, we perform two verification checks: normalization ($\\left|\\sum_i \\hat{w}_i - 1\\right| \\le 10^{-8}$) and non-negativity ($\\hat{w}_i \\ge -10^{-12}$ for all $i$). The slight negative tolerance for the positivity check accounts for potential floating-point inaccuracies near the boundary of the feasible set.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import rel_entr\n\ndef solve():\n    \"\"\"\n    Main function to solve the MAP estimation problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.0, 0.5], [0.2, 0.8, 0.5]]),\n            \"y\": np.array([0.7, 0.5]),\n            \"sigma\": np.array([0.05, 0.05]),\n            \"lambda_\": 0.1,\n            \"w0\": np.array([1/3., 1/3., 1/3.]),\n        },\n        {\n            \"A\": np.array([[1.0, 0.2, 0.2, 0.0], [0.0, 0.7, 0.1, 0.9], [0.5, 0.1, 0.8, 0.3]]),\n            \"y\": np.array([0.2, 0.6, 0.4]),\n            \"sigma\": np.array([0.02, 0.02, 0.05]),\n            \"lambda_\": 0.05,\n            \"w0\": np.array([0.25, 0.25, 0.25, 0.25]),\n        },\n        {\n            \"A\": np.array([[0.5, 0.49, 0.1], [0.5, 0.51, 0.9]]),\n            \"y\": np.array([0.5, 0.5]),\n            \"sigma\": np.array([0.1, 0.1]),\n            \"lambda_\": 1.0,\n            \"w0\": np.array([0.2, 0.2, 0.6]),\n        },\n        {\n            \"A\": np.array([[0.9, 0.1, 0.1], [0.8, 0.2, 0.2]]),\n            \"y\": np.array([0.9, 0.8]),\n            \"sigma\": np.array([0.01, 0.01]),\n            \"lambda_\": 0.01,\n            \"w0\": np.array([1/3., 1/3., 1/3.]),\n        }\n    ]\n\n    results_for_printing = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        y = case[\"y\"]\n        sigma = case[\"sigma\"]\n        lambda_ = case[\"lambda_\"]\n        w0 = case[\"w0\"]\n        N = A.shape[1]\n\n        # Use a more robust implementation for the objective function's log term\n        # to handle w_i -> 0 gracefully. scipy.special.rel_entr calculates w*log(w/w0).\n        def objective_func(w, A, y, sigma, lambda_, w0):\n            # Term 1: Chi-squared\n            chi2_term = 0.5 * np.sum(((A @ w - y) / sigma) ** 2)\n            # Term 2: KL-divergence regularizer\n            # rel_entr(w, w0) is element-wise w_i * log(w_i / w0_i)\n            kl_term = lambda_ * np.sum(rel_entr(w, w0))\n            return chi2_term + kl_term\n\n        # Gradient of the objective function\n        def gradient_func(w, A, y, sigma, lambda_, w0):\n            # Gradient of Chi-squared term\n            sigma_inv_sq = 1 / (sigma ** 2)\n            grad_chi2 = A.T @ (sigma_inv_sq * (A @ w - y))\n            # Gradient of KL-divergence term\n            # Add a small epsilon to avoid log(0) if optimizer queries boundary\n            # Although SLSQP should respect bounds, it's safer.\n            w_safe = np.maximum(w, 1e-150)\n            grad_kl = lambda_ * (np.log(w_safe / w0) + 1)\n            return grad_chi2 + grad_kl\n\n        # Constraints\n        # 1. Sum of weights must be 1 (equality constraint)\n        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}]\n        # 2. Weights must be non-negative (bounds)\n        bounds = [(0, None) for _ in range(N)]\n\n        # Initial guess is the reference distribution\n        w_initial = np.copy(w0)\n\n        # Perform the optimization\n        result = minimize(\n            objective_func,\n            w_initial,\n            args=(A, y, sigma, lambda_, w0),\n            method='SLSQP',\n            jac=gradient_func,\n            bounds=bounds,\n            constraints=constraints,\n            tol=1e-12,\n            options={'maxiter': 1000}\n        )\n        \n        w_opt = result.x\n\n        # Verification checks\n        norm_check_passed = np.abs(np.sum(w_opt) - 1.0) = 1e-8\n        positivity_check_passed = np.all(w_opt >= -1e-12)\n\n        # Format results for this case\n        w_strs = [f\"{val:.6f}\" for val in w_opt]\n        b_strs = [str(norm_check_passed).lower(), str(positivity_check_passed).lower()]\n        all_strs = w_strs + b_strs\n        results_for_printing.append(f\"[{','.join(all_strs)}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_for_printing)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An effective modeling cycle does not end with refinement; it informs the next round of data collection. This advanced practice  explores the principles of optimal experimental design, where the computational model is used to predict which future experiments will be most informative. By calculating sensitivity coefficients and their impact on parameter uncertainty, you will develop a quantitative framework for prioritizing experiments to most efficiently and cost-effectively improve your model.",
            "id": "3845672",
            "problem": "You are modeling an equilibrium receptor–ligand binding assay commonly used in computational chemical biology to infer biophysical parameters from experimental data. The computational model predicts a signal $y$ (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$) at a given free ligand concentration $L$ (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$) by assuming a Hill-type occupancy model. The predicted signal is given by the deterministic mapping $f(\\boldsymbol{\\theta}; L)$ defined as $y = \\alpha \\cdot \\phi(L; K_d, n)$, where $\\boldsymbol{\\theta} = (K_d, n, \\alpha)$, with $K_d$ the dissociation constant (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$), $n$ the Hill coefficient (dimensionless), and $\\alpha$ a proportionality constant (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$). The fractional occupancy $\\phi$ is\n$$\n\\phi(L; K_d, n) = \\frac{\\left(\\dfrac{L}{K_d}\\right)^n}{1 + \\left(\\dfrac{L}{K_d}\\right)^n}.\n$$\nEach experimental constraint corresponds to a single measurement at a specified $L$, yielding a noisy observation $y_i = f(\\boldsymbol{\\theta}; L_i) + \\varepsilon_i$, where $\\varepsilon_i$ is zero-mean additive noise that is independently and identically distributed and modeled as Gaussian with known standard deviation $\\sigma_i$ (in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$). You are provided a prior Gaussian uncertainty on $\\boldsymbol{\\theta}$ specified by a symmetric positive definite covariance matrix $\\boldsymbol{\\Sigma}_0$.\n\nStarting from the definitions of sensitivity and Bayesian linearized Gaussian inference, do the following for each candidate constraint:\n- Compute the sensitivity coefficients as the gradient of the model prediction with respect to parameters evaluated at the nominal $\\boldsymbol{\\theta}$,\n$$\n\\mathbf{S}_i = \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}; L_i) \\in \\mathbb{R}^3,\n$$\nconsistent with the definition of sensitivity in parameter estimation under differentiable models.\n- Treating the single measurement constraint as an incremental information update to the prior, evaluate the expected posterior covariance for $\\boldsymbol{\\theta}$ under the linearized Gaussian approximation and extract the expected posterior variances of a specified subset of critical parameters indexed by a set $\\mathcal{C}$.\n- Define the utility of a constraint as the expected fractional reduction of the sum of variances of the critical parameters, divided by the given experimental cost $c_i$ (dimensionless),\n$$\nU_i = \\frac{\\left(\\sum_{j \\in \\mathcal{C}} \\operatorname{Var}_\\text{prior}[\\theta_j] - \\sum_{j \\in \\mathcal{C}} \\operatorname{Var}_\\text{post}^{(i)}[\\theta_j]\\right)}{\\sum_{j \\in \\mathcal{C}} \\operatorname{Var}_\\text{prior}[\\theta_j]} \\cdot \\frac{1}{c_i}.\n$$\n- Prioritize (rank) constraints in descending order of $U_i$. Break ties by ascending constraint index.\n\nBase your derivation and algorithm only on fundamental principles:\n- Use the definition of sensitivity as partial derivatives of the model output with respect to parameters.\n- Use Bayesian inference for Gaussian priors and Gaussian likelihoods under linearization at $\\boldsymbol{\\theta}$, where adding a single linear constraint contributes a rank-one update to the parameter precision, reflecting the Fisher information contribution from $\\mathbf{S}_i$ and $\\sigma_i$.\n\nYour program must implement the above process numerically using finite differences to approximate $\\mathbf{S}_i$ and the linear-Gaussian update for the posterior covariance. All inputs are provided below in fixed test cases. Do not require any user input. Use the following test suite. All concentrations and signals must be handled in $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$; angles are not involved; the outputs are unitless indices.\n\nTest Case $1$:\n- Nominal parameters $\\boldsymbol{\\theta} = (K_d, n, \\alpha) = \\left(50\\times 10^{-9}, 2.0, 100\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\begin{bmatrix}\n2.25\\times 10^{-16}  0  3.0\\times 10^{-17}\\\\\n0  9.0\\times 10^{-2}  0\\\\\n3.0\\times 10^{-17}  0  4.0\\times 10^{-16}\n\\end{bmatrix}.\n$$\n- Constraints $(L_i, \\sigma_i, c_i)$:\n  - Index $0$: $\\left(10\\times 10^{-9}, 5\\times 10^{-9}, 1.0\\right)$\n  - Index $1$: $\\left(50\\times 10^{-9}, 5\\times 10^{-9}, 1.2\\right)$\n  - Index $2$: $\\left(200\\times 10^{-9}, 5\\times 10^{-9}, 1.0\\right)$\n  - Index $3$: $\\left(1000\\times 10^{-9}, 10\\times 10^{-9}, 2.0\\right)$\n- Critical index set $\\mathcal{C} = \\{0, 1\\}$.\n\nTest Case $2$:\n- Nominal parameters $\\boldsymbol{\\theta} = \\left(100\\times 10^{-9}, 1.5, 50\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}\\left(1.6\\times 10^{-15}, 2.5\\times 10^{-1}, 1.0\\times 10^{-16}\\right).\n$$\n- Constraints:\n  - Index $0$: $\\left(100\\times 10^{-9}, 50\\times 10^{-9}, 1.0\\right)$\n  - Index $1$: $\\left(10\\times 10^{-9}, 50\\times 10^{-9}, 1.0\\right)$\n  - Index $2$: $\\left(1000\\times 10^{-9}, 50\\times 10^{-9}, 1.0\\right)$\n- Critical index set $\\mathcal{C} = \\{0\\}$.\n\nTest Case $3$:\n- Nominal parameters $\\boldsymbol{\\theta} = \\left(30\\times 10^{-9}, 3.0, 150\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}\\left(1.0\\times 10^{-16}, 3.6\\times 10^{-1}, 9.0\\times 10^{-16}\\right).\n$$\n- Constraints:\n  - Index $0$: $\\left(5\\times 10^{-9}, 4\\times 10^{-9}, 1.0\\right)$\n  - Index $1$: $\\left(30\\times 10^{-9}, 4\\times 10^{-9}, 1.0\\right)$\n  - Index $2$: $\\left(300\\times 10^{-9}, 4\\times 10^{-9}, 1.0\\right)$\n- Critical index set $\\mathcal{C} = \\{1\\}$.\n\nTest Case $4$:\n- Nominal parameters $\\boldsymbol{\\theta} = \\left(60\\times 10^{-9}, 1.0, 200\\times 10^{-9}\\right)$.\n- Prior covariance\n$$\n\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}\\left(2.5\\times 10^{-17}, 1.0\\times 10^{-2}, 1.0\\times 10^{-14}\\right).\n$$\n- Constraints:\n  - Index $0$: $\\left(1\\times 10^{-9}, 10\\times 10^{-9}, 0.5\\right)$\n  - Index $1$: $\\left(60\\times 10^{-9}, 10\\times 10^{-9}, 2.0\\right)$\n  - Index $2$: $\\left(1000\\times 10^{-9}, 10\\times 10^{-9}, 1.0\\right)$\n- Critical index set $\\mathcal{C} = \\{2\\}$.\n\nImplementation requirements:\n- Approximate $\\mathbf{S}_i$ using symmetric finite differences around the nominal $\\boldsymbol{\\theta}$. Use perturbation magnitudes that are small relative to parameter scales but numerically stable.\n- For each single constraint, compute the expected posterior covariance under linearization and Gaussian assumptions, and then compute $U_i$ for the given $\\mathcal{C}$.\n- Produce, for each test case, the ranked list of constraint indices in descending order of $U_i$ (ties broken by ascending index).\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the ranked list of constraint indices for the corresponding test case, for example, $\\left[\\left[\\cdots\\right],\\left[\\cdots\\right],\\left[\\cdots\\right]\\right]$.",
            "solution": "The objective of this problem is to prioritize a set of candidate experimental measurements, termed constraints, for a receptor-ligand binding assay. The prioritization is based on the principle of maximizing the information gained from an experiment, quantified by a utility function. The utility measures the expected reduction in uncertainty for a critical subset of model parameters, normalized by the experimental cost. The analysis is performed within the framework of Bayesian inference, using a linearized approximation of the physical model.\n\nThe methodological derivation proceeds as follows, based on a rigorous application of fundamental principles of statistical inference and sensitivity analysis.\n\nThe system is described by a deterministic model $f(\\boldsymbol{\\theta}; L)$ that predicts an observable signal $y$ as a function of the free ligand concentration $L$ and a vector of biophysical parameters $\\boldsymbol{\\theta} = (K_d, n, \\alpha)$. The parameters represent the dissociation constant, the Hill coefficient, and a proportionality constant, respectively. The model is given by:\n$$\ny = f(\\boldsymbol{\\theta}; L) = \\alpha \\cdot \\phi(L; K_d, n)\n$$\nwhere $\\phi$ is the Hill-type fractional occupancy function:\n$$\n\\phi(L; K_d, n) = \\frac{\\left(\\dfrac{L}{K_d}\\right)^n}{1 + \\left(\\dfrac{L}{K_d}\\right)^n}\n$$\nAll concentrations ($y$, $L$, $K_d$, $\\alpha$) are in units of $\\mathrm{mol}\\cdot\\mathrm{L}^{-1}$, and the Hill coefficient $n$ is dimensionless.\n\nThe problem is cast in a Bayesian framework. Our prior knowledge about the parameters $\\boldsymbol{\\theta}$ is encoded in a multivariate Gaussian probability distribution with mean $\\boldsymbol{\\theta}_0$ (the nominal parameter values) and covariance matrix $\\boldsymbol{\\Sigma}_0$. We write this as $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\theta}_0, \\boldsymbol{\\Sigma}_0)$. Each experimental constraint $i$ consists of a single measurement $y_i$ at a known ligand concentration $L_i$. This measurement is noisy, described by the model $y_i = f(\\boldsymbol{\\theta}; L_i) + \\varepsilon_i$, where the noise $\\varepsilon_i$ is assumed to be an independent, zero-mean Gaussian random variable with a known variance $\\sigma_i^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$. The likelihood of observing $y_i$ given the parameters $\\boldsymbol{\\theta}$ is therefore $p(y_i|\\boldsymbol{\\theta}) \\sim \\mathcal{N}(f(\\boldsymbol{\\theta}; L_i), \\sigma_i^2)$.\n\nTo update our belief about $\\boldsymbol{\\theta}$, we would ideally compute the posterior distribution $p(\\boldsymbol{\\theta}|y_i) \\propto p(y_i|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})$. However, because $f(\\boldsymbol{\\theta}; L_i)$ is a nonlinear function of $\\boldsymbol{\\theta}$, the posterior is not Gaussian and is difficult to compute analytically. The problem specifies using a linearized Gaussian approximation. We linearize the model $f(\\boldsymbol{\\theta}; L_i)$ around the nominal parameter vector $\\boldsymbol{\\theta}_0$ using a first-order Taylor expansion:\n$$\nf(\\boldsymbol{\\theta}; L_i) \\approx f(\\boldsymbol{\\theta}_0; L_i) + \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}_0; L_i)^T (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)\n$$\nThe gradient vector $\\mathbf{S}_i = \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}_0; L_i)$ is a $3 \\times 1$ column vector of sensitivity coefficients, which quantifies how the model output changes with infinitesimal changes in each parameter. The observation model becomes approximately linear in $\\boldsymbol{\\theta}$:\n$$\ny_i - f(\\boldsymbol{\\theta}_0; L_i) \\approx \\mathbf{S}_i^T (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) + \\varepsilon_i\n$$\nUnder this approximation, a Gaussian prior on $\\boldsymbol{\\theta}$ and a Gaussian likelihood for $y_i$ result in a Gaussian posterior for $\\boldsymbol{\\theta}$. The update rule for the covariance matrix is derived from the update rule for the precision matrix (the inverse of the covariance matrix). The posterior precision matrix, $\\mathbf{P}_\\text{post}^{(i)}$, is the sum of the prior precision matrix, $\\mathbf{P}_0 = \\boldsymbol{\\Sigma}_0^{-1}$, and the precision gained from the measurement, which is equivalent to the Fisher Information of the measurement. For a single linearized Gaussian measurement, this information is a rank-one matrix:\n$$\n\\mathbf{P}_\\text{post}^{(i)} = \\mathbf{P}_0 + \\frac{1}{\\sigma_i^2} \\mathbf{S}_i \\mathbf{S}_i^T = \\boldsymbol{\\Sigma}_0^{-1} + \\frac{1}{\\sigma_i^2} \\mathbf{S}_i \\mathbf{S}_i^T\n$$\nThe expected posterior covariance matrix, $\\boldsymbol{\\Sigma}_\\text{post}^{(i)}$, is the inverse of the posterior precision matrix: $\\boldsymbol{\\Sigma}_\\text{post}^{(i)} = (\\mathbf{P}_\\text{post}^{(i)})^{-1}$. A direct inversion can be avoided by applying the Sherman-Morrison identity, yielding the more numerically stable Kalman filter covariance update form:\n$$\n\\boldsymbol{\\Sigma}_\\text{post}^{(i)} = \\boldsymbol{\\Sigma}_0 - \\boldsymbol{\\Sigma}_0 \\mathbf{S}_i \\left( \\sigma_i^2 + \\mathbf{S}_i^T \\boldsymbol{\\Sigma}_0 \\mathbf{S}_i \\right)^{-1} \\mathbf{S}_i^T \\boldsymbol{\\Sigma}_0\n$$\nThis formula computes the posterior covariance after incorporating the information from the $i$-th measurement. The term $(\\sigma_i^2 + \\mathbf{S}_i^T \\boldsymbol{\\Sigma}_0 \\mathbf{S}_i)$ is a scalar, making the computation efficient.\n\nThe sensitivity vector $\\mathbf{S}_i = \\nabla_{\\boldsymbol{\\theta}} f(\\boldsymbol{\\theta}_0; L_i)$ is numerically approximated using the symmetric finite difference method. For each parameter $\\theta_j$ (where $j \\in \\{0, 1, 2\\}$ corresponds to $K_d, n, \\alpha$), the partial derivative is:\n$$\nS_{i,j} = \\frac{\\partial f}{\\partial \\theta_j} \\bigg|_{\\boldsymbol{\\theta}_0, L_i} \\approx \\frac{f(\\boldsymbol{\\theta}_0 + h_j \\mathbf{e}_j; L_i) - f(\\boldsymbol{\\theta}_0 - h_j \\mathbf{e}_j; L_i)}{2h_j}\n$$\nwhere $\\mathbf{e}_j$ is the standard basis vector for the $j$-th parameter and $h_j$ is a small-step perturbation, chosen as a small fraction of the nominal parameter value, e.g., $h_j = \\epsilon_h |\\theta_{0,j}|$, to ensure numerical stability.\n\nThe utility $U_i$ of each constraint $i$ is defined as the cost-normalized expected fractional reduction in the sum of variances of a critical set of parameters, indexed by $\\mathcal{C}$. The variance of a parameter $\\theta_j$ is the $j$-th diagonal element of the covariance matrix. The utility is calculated as:\n$$\nU_i = \\frac{\\Delta V_i}{V_\\text{prior}} \\cdot \\frac{1}{c_i}\n$$\nwhere $V_\\text{prior} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_0)_{jj}$ is the sum of prior variances of the critical parameters, and $\\Delta V_i = V_\\text{prior} - V_\\text{post}^{(i)}$ is the reduction in this sum, with $V_\\text{post}^{(i)} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_\\text{post}^{(i)})_{jj}$.\n\nThe algorithmic procedure to rank the constraints is as follows:\n1. For each test case, retrieve the nominal parameters $\\boldsymbol{\\theta}_0$, prior covariance $\\boldsymbol{\\Sigma}_0$, critical index set $\\mathcal{C}$, and the list of constraints $(L_i, \\sigma_i, c_i)$.\n2. Calculate the sum of prior variances for the critical parameters, $V_\\text{prior} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_0)_{jj}$.\n3. For each constraint $i$:\n    a. Compute the sensitivity vector $\\mathbf{S}_i$ at $(\\boldsymbol{\\theta}_0, L_i)$ using symmetric finite differences.\n    b. Calculate the expected posterior covariance matrix $\\boldsymbol{\\Sigma}_\\text{post}^{(i)}$ using the Kalman update formula.\n    c. Calculate the sum of posterior variances for the critical parameters, $V_\\textpost^{(i)} = \\sum_{j \\in \\mathcal{C}} (\\boldsymbol{\\Sigma}_\\text{post}^{(i)})_{jj}$.\n    d. Compute the utility $U_i = \\frac{V_\\text{prior} - V_\\text{post}^{(i)}}{V_\\text{prior} \\cdot c_i}$.\n4. Store pairs of $(U_i, i)$ for all constraints.\n5. Sort these pairs in descending order of utility $U_i$. If two utilities are equal, break the tie by sorting their corresponding indices $i$ in ascending order.\n6. Extract the sorted indices to form the final ranked list for the test case.\n7. Repeat for all test cases and format the final output.\nThis procedure provides a principled, quantitative ranking of experiments to guide data acquisition in computational chemical biology.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the experiment prioritization problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"theta\": np.array([50e-9, 2.0, 100e-9]),\n            \"Sigma0\": np.array([\n                [2.25e-16, 0.0, 3.0e-17],\n                [0.0, 9.0e-2, 0.0],\n                [3.0e-17, 0.0, 4.0e-16]\n            ]),\n            \"constraints\": [\n                (10e-9, 5e-9, 1.0),\n                (50e-9, 5e-9, 1.2),\n                (200e-9, 5e-9, 1.0),\n                (1000e-9, 10e-9, 2.0)\n            ],\n            \"critical_indices\": {0, 1}\n        },\n        {\n            \"theta\": np.array([100e-9, 1.5, 50e-9]),\n            \"Sigma0\": np.diag([1.6e-15, 2.5e-1, 1.0e-16]),\n            \"constraints\": [\n                (100e-9, 50e-9, 1.0),\n                (10e-9, 50e-9, 1.0),\n                (1000e-9, 50e-9, 1.0)\n            ],\n            \"critical_indices\": {0}\n        },\n        {\n            \"theta\": np.array([30e-9, 3.0, 150e-9]),\n            \"Sigma0\": np.diag([1.0e-16, 3.6e-1, 9.0e-16]),\n            \"constraints\": [\n                (5e-9, 4e-9, 1.0),\n                (30e-9, 4e-9, 1.0),\n                (300e-9, 4e-9, 1.0)\n            ],\n            \"critical_indices\": {1}\n        },\n        {\n            \"theta\": np.array([60e-9, 1.0, 200e-9]),\n            \"Sigma0\": np.diag([2.5e-17, 1.0e-2, 1.0e-14]),\n            \"constraints\": [\n                (1e-9, 10e-9, 0.5),\n                (60e-9, 10e-9, 2.0),\n                (1000e-9, 10e-9, 1.0)\n            ],\n            \"critical_indices\": {2}\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        theta0 = case[\"theta\"]\n        Sigma0 = case[\"Sigma0\"]\n        constraints = case[\"constraints\"]\n        critical_indices = case[\"critical_indices\"]\n\n        utilities = []\n\n        # Prior variance of critical parameters\n        var_prior_sum = 0.0\n        for j in critical_indices:\n            var_prior_sum += Sigma0[j, j]\n        \n        # Guard against zero prior variance, which implies no uncertainty to reduce.\n        if var_prior_sum == 0.0:\n            # If there's no prior variance, no experiment can reduce it. All utilities are 0.\n            # Rank by index as per tie-breaking rule.\n            ranked_indices = sorted(range(len(constraints)))\n            all_results.append(ranked_indices)\n            continue\n\n        for i, (L, sigma, c) in enumerate(constraints):\n            # 1. Compute sensitivity vector S_i using symmetric finite differences\n            S_i = compute_sensitivity(theta0, L)\n\n            # 2. Compute posterior covariance Sigma_post\n            # Using the numerically stable Kalman update form:\n            # Sigma_post = Sigma0 - K * S_i_T * Sigma0, where K = Sigma0 * S_i * (sigma^2 + S_i_T * Sigma0 * S_i)^-1\n            # Which simplifies to: Sigma_post = Sigma0 - (Sigma0 @ S_i) @ (S_i_T @ Sigma0) / (sigma^2 + S_i_T @ Sigma0 @ S_i)\n            # where S_i is a column vector\n            S_i_col = S_i.reshape(-1, 1)\n            S_i_T = S_i.reshape(1, -1)\n            \n            kalman_gain_numerator = Sigma0 @ S_i_col\n            kalman_gain_denominator = sigma**2 + S_i_T @ Sigma0 @ S_i_col\n            \n            # kalman_gain_denominator is a 1x1 matrix, extract the scalar\n            if kalman_gain_denominator[0, 0] == 0:\n                Sigma_post = Sigma0\n            else:\n                kalman_gain = kalman_gain_numerator / kalman_gain_denominator[0, 0]\n                Sigma_post = Sigma0 - kalman_gain @ S_i_T @ Sigma0\n\n            # 3. Compute posterior variance sum\n            var_post_sum = 0.0\n            for j in critical_indices:\n                var_post_sum += Sigma_post[j, j]\n\n            # 4. Compute utility U_i\n            # Avoid division by zero for cost\n            if c = 0:\n                utility = -np.inf # Effectively prioritizes this last\n            else:\n                fractional_reduction = (var_prior_sum - var_post_sum) / var_prior_sum\n                utility = fractional_reduction / c\n            \n            utilities.append({'utility': utility, 'index': i})\n\n        # 5. Rank constraints\n        # Sort by utility (descending) and then index (ascending) for ties\n        utilities.sort(key=lambda x: (-x['utility'], x['index']))\n        ranked_indices = [item['index'] for item in utilities]\n        all_results.append(ranked_indices)\n        \n    print(str(all_results).replace(' ', ''))\n\n\ndef model(theta, L):\n    \"\"\"\n    Computes the predicted signal y based on the Hill-type occupancy model.\n    theta = (K_d, n, alpha)\n    \"\"\"\n    Kd, n, alpha = theta\n    if Kd = 0 or L  0: # Physical constraints\n        return 0.0\n\n    # To prevent overflow for large n and L/Kd > 1\n    # phi = (L/Kd)^n / (1 + (L/Kd)^n) = 1 / (1 + (Kd/L)^n)\n    ratio = L / Kd\n    if ratio > 1.0:\n        inv_ratio = Kd / L\n        # n can be float, use np.power\n        phi = 1.0 / (1.0 + np.power(inv_ratio, n))\n    else:\n        # n can be float, use np.power\n        term = np.power(ratio, n)\n        phi = term / (1.0 + term)\n    \n    return alpha * phi\n\n\ndef compute_sensitivity(theta, L, rel_step=1e-8):\n    \"\"\"\n    Computes the sensitivity vector S = grad_theta f(theta, L) using \n    symmetric finite differences.\n    \"\"\"\n    S = np.zeros_like(theta)\n    for j in range(len(theta)):\n        theta_j = theta[j]\n        # Choose a step size that is robust to theta_j being zero.\n        h = rel_step * (np.abs(theta_j) + 1e-9)\n        \n        theta_plus = np.copy(theta)\n        theta_plus[j] += h\n        \n        theta_minus = np.copy(theta)\n        theta_minus[j] -= h\n        \n        f_plus = model(theta_plus, L)\n        f_minus = model(theta_minus, L)\n        \n        S[j] = (f_plus - f_minus) / (2.0 * h)\n    return S\n\nsolve()\n\n```"
        }
    ]
}