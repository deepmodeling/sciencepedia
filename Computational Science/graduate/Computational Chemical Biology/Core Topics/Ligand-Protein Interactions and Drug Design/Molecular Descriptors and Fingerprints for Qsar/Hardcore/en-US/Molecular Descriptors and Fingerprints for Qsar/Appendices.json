{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Quantitative Structure-Activity Relationship (QSAR) modeling is the art of translating complex molecular structures into simple, informative numbers called descriptors. This first practice focuses on one of the most fundamental one-dimensional descriptors: the Rotatable Bond Count (RBC). By working through this exercise , you will not only learn the standard rules for calculating this descriptor but also connect this simple count to the profound concept of conformational entropy, gaining a deeper appreciation for how even basic descriptors can encapsulate key principles of physical chemistry.",
            "id": "3854314",
            "problem": "Aromatic amides with ether side chains are widely used test cases for understanding one-dimensional molecular descriptors in Quantitative Structure–Activity Relationship (QSAR). Consider the molecule 4-(2-methoxyethoxy)-N-ethylbenzamide, which can be represented by the Simplified Molecular Input Line Entry System (SMILES) string \"COCCOc1ccc(C(=O)NCC)cc1\". The core is a benzene ring bearing two para substituents: an amide group and a 2-methoxyethoxy group. The amide substituent is a benzamide with an N-ethyl side chain, that is, an aromatic acyl group $-\\text{C(=O)}-$ attached to $-\\text{NH-CH}_2\\text{-CH}_3$. The ether substituent is $-\\text{O-CH}_2\\text{-CH}_2\\text{-O-CH}_3$ attached to the ring oxygen.\n\nUsing the following standard rotatable bond rules grounded in chemical graph theory and resonance constraints, compute the rotatable bond count (RBC), and then briefly explain, starting from statistical mechanics, how this one-dimensional descriptor relates to conformational flexibility and entropy:\n- A rotatable bond is any single bond between two non-hydrogen atoms.\n- Exclude any bond that is part of a ring.\n- Exclude any single bond with restricted rotation due to strong $\\pi$-resonance in amide functional groups, specifically the amide $\\text{C-N}$ bond in $-\\text{C(=O)-NH}-$.\n- Bonds to hydrogen atoms are not considered.\n- Single bonds such as $\\text{Ar-C(=O)}$ and $\\text{Ar-O}$ are to be treated according to the above rules (they are single, exocyclic, non-hydrogen bonds and not themselves amide $\\text{C-N}$ bonds).\n\nYour answer must be a single real-valued integer equal to the RBC of this molecule. No rounding is required, and no units should be reported. In your explanation, relate RBC to conformational entropy using the Boltzmann framework, without invoking any empirical shortcut formulas; focus on the conceptual link between the count of rotatable single bonds and the number of accessible torsional microstates at temperature $T$.",
            "solution": "The problem is first validated to ensure its scientific and logical integrity.\n\n**Step 1: Extract Givens**\n- **Molecule Name**: 4-(2-methoxyethoxy)-N-ethylbenzamide\n- **SMILES String**: `COCCOc1ccc(C(=O)NCC)cc1`\n- **Structure**: A benzene ring with two para substituents: an amide group (`-C(=O)NHCH2CH3`) and a 2-methoxyethoxy group (`-OCH2CH2OCH3`).\n- **Task**: Compute the Rotatable Bond Count (RBC) and explain its relationship to conformational entropy from a statistical mechanics perspective.\n- **Rules for Rotatable Bond Count (RBC)**:\n    1. A rotatable bond is any single bond between two non-hydrogen atoms.\n    2. Exclude any bond that is part of a ring.\n    3. Exclude any single bond with restricted rotation due to strong $\\pi$-resonance in amide functional groups, specifically the amide $\\text{C-N}$ bond in $-\\text{C(=O)-NH}-$.\n    4. Bonds to hydrogen atoms are not considered.\n    5. Single bonds such as $\\text{Ar-C(=O)}$ and $\\text{Ar-O}$ are rotatable if they meet the other criteria.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. The molecule is chemically valid, and its structure is consistent with the provided name and SMILES string. The rules for calculating the Rotatable Bond Count are explicit and standard within the field of computational chemistry and QSAR. The second part of the task requires a conceptual explanation based on fundamental principles of statistical mechanics (the Boltzmann entropy formulation), which is a standard and verifiable topic in physical chemistry. The problem contains no scientific flaws, ambiguities, or contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be provided.\n\n**Part 1: Calculation of the Rotatable Bond Count (RBC)**\n\nTo compute the RBC, we will systematically analyze the single bonds between non-hydrogen atoms in the molecule 4-(2-methoxyethoxy)-N-ethylbenzamide, whose structure is `CH3-O-CH2-CH2-O-C6H4-C(=O)-NH-CH2-CH3`. We apply the given rules to each bond.\n\nThe molecule can be broken down into its constituent parts: the 2-methoxyethoxy side chain, the benzene ring, and the N-ethylbenzamide side chain.\n\n1.  **The 2-methoxyethoxy side chain (`CH3-O-CH2-CH2-O-`) attached to the benzene ring:**\n    - The $\\text{CH}_3\\text{-O}$ bond: This is a single bond between two non-hydrogen atoms and is not in a ring. It is rotatable. (Count = $1$)\n    - The $\\text{O-CH}_2$ bond: This is a single bond between two non-hydrogen atoms and is not in a ring. It is rotatable. (Count = $2$)\n    - The $\\text{CH}_2\\text{-CH}_2$ bond: This is a single bond between two non-hydrogen atoms and is not in a ring. It is rotatable. (Count = $3$)\n    - The $\\text{CH}_2\\text{-O}$ bond: This is a single bond between two non-hydrogen atoms and is not in a ring. It is rotatable. (Count = $4$)\n    - The $\\text{O-C}_{\\text{aromatic}}$ bond (the ether linkage to the ring): This is a single bond between two non-hydrogen atoms, is not in a ring, and is not an amide $\\text{C-N}$ bond. It is rotatable. (Count = $5$)\n\n2.  **The benzene ring itself:**\n    - All $\\text{C-C}$ bonds within the benzene ring are part of a ring system and are explicitly excluded by the rules. (No additional count)\n\n3.  **The N-ethylbenzamide side chain (`-C(=O)-NH-CH2-CH3`) attached to the benzene ring:**\n    - The $\\text{C}_{\\text{aromatic}}\\text{-C(=O)}$ bond (the carbonyl group's attachment to the ring): This is a single bond between two non-hydrogen atoms, is not in a ring, and is not an amide $\\text{C-N}$ bond. It is rotatable. (Count = $6$)\n    - The $\\text{C(=O)-N}$ bond (the amide bond): This single bond has a significant double bond character due to $\\pi$-resonance ($O=C-N \\leftrightarrow {}^{-}O-C=N^{+}$). This restricts rotation, and the bond is explicitly excluded by the rules. (No additional count)\n    - The $\\text{N-CH}_2$ bond: This is a single bond between two non-hydrogen atoms, not in a ring, and is not the restricted amide $\\text{C-N}$ bond. It is rotatable. (Count = $7$)\n    - The $\\text{CH}_2\\text{-CH}_3$ bond: This is a single bond between two non-hydrogen atoms and is not in a ring. It is rotatable. (Count = $8$)\n\nSumming these counts gives the total Rotatable Bond Count (RBC).\nTotal RBC = $1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 = 8$.\n\n**Part 2: Relationship Between RBC and Conformational Entropy**\n\nThe relationship between the Rotatable Bond Count (RBC) and conformational entropy is rooted in the principles of statistical mechanics, specifically the Boltzmann entropy formula.\n\nFrom statistical mechanics, the entropy $S$ of a system is related to the number of thermally accessible microstates $W$ by the Boltzmann equation:\n$$S = k_B \\ln(W)$$\nwhere $k_B$ is the Boltzmann constant. A microstate represents a specific configuration of a system (in this case, a specific conformation of a molecule).\n\nConformational entropy, $S_{conf}$, is the component of the total entropy that arises from the molecule's structural flexibility, i.e., its ability to adopt different spatial arrangements through rotation about single bonds. The RBC is a direct measure of the number of such internal rotational degrees of freedom.\n\nFor a single rotatable bond, the molecule can exist in several low-energy torsional states, or \"rotamers\" (e.g., staggered conformations like anti, gauche+, and gauche-). Let's denote the number of distinct, accessible rotameric states for the $i$-th rotatable bond as $n_i$. The accessibility of these states depends on the thermal energy $k_B T$ available to overcome the rotational energy barriers at a given absolute temperature $T$.\n\nIf we make the simplifying assumption that the rotations about different single bonds are independent, the total number of accessible conformational microstates for the entire molecule, $W_{conf}$, is the product of the number of states available for each individual rotatable bond:\n$$W_{conf} = \\prod_{i=1}^{RBC} n_i$$\nTo illustrate the conceptual link, one can make a further approximation that each rotatable bond contributes roughly the same number of accessible states, $n$. In this case, the total number of conformations grows exponentially with the RBC:\n$$W_{conf} \\approx n^{RBC}$$\nFor example, a common approximation is $n=3$, representing the anti and two gauche conformations.\n\nSubstituting this expression for $W_{conf}$ into the Boltzmann equation gives the conformational entropy:\n$$S_{conf} = k_B \\ln(W_{conf}) \\approx k_B \\ln(n^{RBC})$$\n$$S_{conf} \\approx (RBC) \\cdot k_B \\ln(n)$$\nThis final expression shows a direct, approximately linear relationship between the conformational entropy $S_{conf}$ and the Rotatable Bond Count (RBC). A higher RBC implies more axes of internal rotation, leading to an exponentially larger number of possible conformations ($W_{conf}$). This greater conformational \"disorder\" or flexibility is quantified as a higher conformational entropy. Therefore, the RBC serves as a simple yet powerful molecular descriptor that correlates with the molecule's conformational flexibility and its contribution to the system's entropy.",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "While simple descriptors are useful, modern QSAR often relies on more holistic representations like molecular fingerprints, which encode thousands of structural features into a single bit vector. A primary application of fingerprints is to quantify the similarity between molecules. This practice  challenges you to calculate and compare two of the most ubiquitous similarity metrics in cheminformatics: the Tanimoto and Dice coefficients. This exercise will highlight how the underlying mathematical formulation of a similarity metric can subtly alter the perceived similarity between compounds, a crucial consideration in virtual screening and chemical library design.",
            "id": "3854355",
            "problem": "Consider two small molecules used in a Quantitative Structure–Activity Relationship (QSAR) model, each encoded as a $1024$-bit Extended Connectivity Fingerprint (ECFP). Let the set of bit positions set to $1$ for molecule A be denoted by $A \\subset \\{1,\\dots,1024\\}$ and for molecule B by $B \\subset \\{1,\\dots,1024\\}$. Empirically, the following cardinalities are observed: $|A|=a=100$, $|B|=b=120$, and $|A \\cap B|=c=80$. In cheminformatics, set-based similarity metrics between binary fingerprints are defined from first principles of set theory. The Tanimoto similarity (also known as the Jaccard index) is defined as the ratio of the intersection cardinality to the union cardinality, and the Dice similarity (also known as the Sørensen–Dice coefficient) is defined as twice the intersection cardinality divided by the sum of individual cardinalities.\n\nStarting from the set-theoretic identity $|A \\cup B|=|A|+|B|-|A \\cap B|$, derive the expressions for the Tanimoto similarity and the Dice similarity in terms of $a$, $b$, and $c$, evaluate them for the given values, and then compute the difference $D - T$. Express your final answer for $D - T$ in exact form as a single simplified analytic expression. Additionally, explain in your derivation why the two metrics differ and how this difference arises from their definitions in the context of binary fingerprints for QSAR.",
            "solution": "We begin with the foundational set-theoretic definitions for similarity metrics used with binary molecular fingerprints in Quantitative Structure–Activity Relationship (QSAR) modeling. The fingerprint for each molecule is a binary vector, and the positions of bits set to $1$ correspond to features present in the molecule. The sets $A$ and $B$ represent the indices of set bits for molecules A and B, respectively. The intersection $A \\cap B$ represents shared features, and the union $A \\cup B$ represents features present in at least one molecule.\n\nThe Tanimoto similarity, synonymous with the Jaccard index, is defined from first principles as the fraction of shared features over the total number of unique features. In set-theoretic terms, this is\n$$\nT \\equiv \\frac{|A \\cap B|}{|A \\cup B|}.\n$$\nUsing the identity\n$$\n|A \\cup B| = |A| + |B| - |A \\cap B|,\n$$\nwe can express $T$ in terms of the cardinalities $a$, $b$, and $c$ as\n$$\nT = \\frac{c}{a + b - c}.\n$$\n\nThe Dice similarity, also known as the Sørensen–Dice coefficient, emphasizes the intersection by scaling it relative to the total size of both sets. Its set-theoretic definition is\n$$\nD \\equiv \\frac{2|A \\cap B|}{|A| + |B|},\n$$\nwhich in terms of $a$, $b$, and $c$ becomes\n$$\nD = \\frac{2c}{a + b}.\n$$\n\nWe now substitute the given values $a=100$, $b=120$, and $c=80$.\n\nFor the Tanimoto similarity,\n$$\nT = \\frac{c}{a + b - c} = \\frac{80}{100 + 120 - 80} = \\frac{80}{140} = \\frac{4}{7}.\n$$\n\nFor the Dice similarity,\n$$\nD = \\frac{2c}{a + b} = \\frac{160}{220} = \\frac{16}{22} = \\frac{8}{11}.\n$$\n\nThe requested final quantity is the difference $D - T$. Compute this difference in exact form:\n$$\nD - T = \\frac{8}{11} - \\frac{4}{7}.\n$$\nUse a common denominator to simplify:\n$$\n\\frac{8}{11} - \\frac{4}{7} = \\frac{8 \\cdot 7}{77} - \\frac{4 \\cdot 11}{77} = \\frac{56 - 44}{77} = \\frac{12}{77}.\n$$\nThus, the exact simplified expression for the difference is\n$$\nD - T = \\frac{12}{77}.\n$$\n\nInterpretation of the difference derived from definitions: The Tanimoto similarity $T$ normalizes the intersection $|A \\cap B|$ by the union $|A \\cup B|$, which penalizes mismatched features by including them in the denominator as $a + b - c$. The Dice similarity $D$ normalizes the intersection by the sum $a + b$ and doubles the intersection, effectively giving more weight to shared features relative to total features. Because $|A \\cup B| = a + b - c$ is strictly less than $a + b$ when $c>0$, the Dice denominator is larger while its numerator is $2c$, making $D$ typically larger than $T$ for nontrivial overlap. In this case, the difference $D - T = \\frac{12}{77}$ arises from the extra emphasis that Dice places on the intersection through the factor of $2$ and the absence of subtracting $c$ from the denominator. In QSAR screening with binary fingerprints, this implies that Dice is more sensitive to shared bits and may rank pairs with substantial overlap higher than Tanimoto, potentially affecting hit enrichment and threshold selection depending on the desired balance between rewarding overlap and penalizing mismatches.",
            "answer": "$$\\boxed{\\frac{12}{77}}$$"
        },
        {
            "introduction": "Building a QSAR model typically involves analyzing not just one or two molecules, but a large dataset where each compound is defined by numerous descriptors, creating a high-dimensional data matrix. Making sense of such complexity requires powerful data analysis techniques. This final practice introduces Principal Component Analysis (PCA), a foundational method for dimensionality reduction and pattern recognition in multivariate data . By implementing PCA from its linear algebra roots, you will learn how to compress a complex descriptor space into a few informative dimensions, interpret the underlying descriptor contributions, and project your data for visualization and subsequent modeling.",
            "id": "3854316",
            "problem": "You are given small, standardized molecular descriptor matrices representing collections of compounds used in Quantitative Structure–Activity Relationship (QSAR). Principal Component Analysis (PCA) is to be performed entirely from first principles by computing the eigendecomposition of the sample covariance of the standardized descriptors, projecting the samples onto the first two principal components, and interpreting component loadings.\n\nBase definitions and facts to use:\n- Standardized descriptors are arranged in a matrix $Z \\in \\mathbb{R}^{n \\times p}$ whose columns (descriptors) have zero mean across samples. The sample covariance matrix is defined as $$C = \\frac{1}{n-1} Z^\\top Z.$$\n- Principal Component Analysis (PCA) seeks orthonormal directions (principal axes) $v_1, v_2, \\dots, v_p \\in \\mathbb{R}^p$ that maximize the variance of the projected data under unit-length constraints. For the first principal component direction $v_1$, this is equivalent to maximizing the Rayleigh quotient $v^\\top C v$ subject to $\\|v\\|_2 = 1$, which yields that $v_1$ is the eigenvector of $C$ corresponding to the largest eigenvalue $\\lambda_1$. Subsequent principal components $v_2, \\dots$ follow similarly under orthogonality constraints.\n- Given the eigen-decomposition $$C = V \\Lambda V^\\top,$$ with $V$ orthonormal and $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_p)$ sorted so that $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p \\ge 0$, the projection (scores) of samples onto the first two principal components is $$T_{(2)} = Z \\, V_{(2)},$$ where $V_{(2)}$ contains the first two eigenvectors as columns. The component loadings for the first two components are defined as $$L_{(2)} = V_{(2)} \\, \\mathrm{diag}\\!\\big(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}\\big).$$\n- The explained variance ratio for principal component $j$ is $$r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{p} \\lambda_k}.$$\n\nYour program must:\n1. For each provided test case, compute the sample covariance $C$, its eigenvalues and eigenvectors, sort eigenvalues in descending order with their eigenvectors accordingly, obtain the first two eigenvalues $[\\lambda_1, \\lambda_2]$, the first two eigenvectors $V_{(2)}$, the scores $T_{(2)}$, and the loadings $L_{(2)}$.\n2. Interpret the loadings by identifying, for each of the first two principal components, the index (using $0$-based indexing) of the descriptor with the maximum absolute loading, i.e., for component $j \\in \\{1,2\\}$, return $$\\arg\\max_{i \\in \\{0,\\dots,p-1\\}} |L_{(2)}[i,j-1]|.$$\n3. Compute the explained variance ratios $[r_1, r_2]$.\n4. Return, for each test case, a nested list containing:\n   - The first two eigenvalues $[\\lambda_1, \\lambda_2]$,\n   - The explained variance ratios $[r_1, r_2]$,\n   - The indices of descriptors with maximum absolute loadings for the first two components $[\\mathrm{idx}_1, \\mathrm{idx}_2]$,\n   - The first sample’s two-dimensional projection (scores) $[t_{1,1}, t_{1,2}]$ where $t_{1,j}$ is the score of the first sample on component $j$.\n\nAll floating-point outputs must be rounded to $6$ decimal places. All descriptor indices must be integers via $0$-based indexing. The final output must be a single line that aggregates the results for all test cases into one list, with no spaces, e.g., `[case_1,case_2,case_3]`, where each `case_k` is the nested list described above.\n\nTest suite (each $Z^{(k)}$ is standardized to zero mean per descriptor):\n- Case 1: $$Z^{(1)} = \\begin{bmatrix}\n-1.2  -1.0  \\phantom{-}0.8  \\phantom{-}0.5 \\\\\n\\phantom{-}0.4  \\phantom{-}0.3  -0.6  -0.2 \\\\\n\\phantom{-}0.8  \\phantom{-}0.9  -0.7  \\phantom{-}0.1 \\\\\n-0.5  -0.7  \\phantom{-}0.9  -0.4 \\\\\n\\phantom{-}0.3  \\phantom{-}0.4  -0.2  \\phantom{-}0.0 \\\\\n\\phantom{-}0.2  \\phantom{-}0.1  -0.2  \\phantom{-}0.0 \\\\\n\\end{bmatrix}$$\n- Case 2 (boundary condition with perfect collinearity between the first two descriptors): $$Z^{(2)} = \\begin{bmatrix}\n-1.5  -3.0  \\phantom{-}0.6 \\\\\n-0.5  -1.0  -0.2 \\\\\n\\phantom{-}0.5  \\phantom{-}1.0  -0.1 \\\\\n\\phantom{-}1.0  \\phantom{-}2.0  -0.2 \\\\\n\\phantom{-}0.5  \\phantom{-}1.0  -0.1 \\\\\n\\end{bmatrix}$$\n- Case 3 (edge case with a zero-variance descriptor): $$Z^{(3)} = \\begin{bmatrix}\n\\phantom{-}0.5  -0.3  \\phantom{-}0.0  \\phantom{-}0.7 \\\\\n-0.5  \\phantom{-}0.3  \\phantom{-}0.0  -0.7 \\\\\n\\phantom{-}0.4  -0.2  \\phantom{-}0.0  \\phantom{-}0.6 \\\\\n-0.4  \\phantom{-}0.2  \\phantom{-}0.0  -0.6 \\\\\n\\end{bmatrix}$$\n\nYour program should produce a single line of output containing a list of results for the three cases, with each case represented as a nested list `[[\\lambda_1,\\lambda_2],[r_1,r_2],[idx_1,idx_2],[t_{1,1},t_{1,2}]]`, all floats rounded to $6$ decimal places and no spaces anywhere in the line. Angles are not involved, so no angle units are required, and the explained variance ratios must be expressed as decimals.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the principles of linear algebra and multivariate statistics, specifically Principal Component Analysis (PCA), and applies these concepts to a relevant problem in computational chemical biology (QSAR). The problem is well-posed, with all necessary data and definitions provided to compute a unique and verifiable solution. The test cases include standard, collinear, and zero-variance scenarios, which are appropriate for assessing a robust implementation.\n\nThe task is to perform PCA on given standardized molecular descriptor matrices ($Z$) from first principles. This involves computing the sample covariance matrix ($C$), performing its eigendecomposition to find the principal components, and then calculating derived quantities such as scores, loadings, and explained variance ratios.\n\nThe core of PCA lies in finding a new set of orthogonal axes, called principal components, that align with the directions of maximum variance in the data. For a data matrix $Z \\in \\mathbb{R}^{n \\times p}$ with $n$ samples and $p$ descriptors, where each descriptor (column) is centered to have zero mean, the sample covariance matrix is given by:\n$$C = \\frac{1}{n-1} Z^\\top Z$$\nThe principal components are the eigenvectors of this covariance matrix $C$. Let the eigendecomposition of $C$ be $C = V \\Lambda V^\\top$, where $V$ is an orthonormal matrix whose columns $v_j$ are the eigenvectors, and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_j$. The eigenvalues are sorted in descending order, $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0$, such that the first eigenvector $v_1$ corresponds to the direction of maximum variance in the data.\n\nThe amount of variance captured by each principal component $j$ is given by its eigenvalue $\\lambda_j$. The explained variance ratio, $r_j$, normalizes this value by the total variance in the data, which is the sum of all eigenvalues (equal to the trace of $C$):\n$$r_j = \\frac{\\lambda_j}{\\sum_{k=1}^{p} \\lambda_k}$$\n\nThe original data can be projected onto the new coordinate system defined by the principal components. The resulting coordinates are called scores. The scores for the first two principal components are calculated by projecting the data matrix $Z$ onto the first two eigenvectors, $V_{(2)} = [v_1, v_2]$:\n$$T_{(2)} = Z V_{(2)}$$\n\nThe component loadings, which measure the correlation between the original descriptors and the principal components, are calculated for the first two components using the formula:\n$$L_{(2)} = V_{(2)} \\mathrm{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2})$$\nThe magnitude of a loading value indicates the importance of the corresponding original descriptor in defining that principal component. Therefore, for each of the first two components, we identify the descriptor with the maximum absolute loading to interpret which original variable contributes most to it.\n\nThe overall algorithm to be implemented for each test case is as follows:\n1.  Given the standardized descriptor matrix $Z$ with $n$ samples and $p$ descriptors.\n2.  Compute the $p \\times p$ sample covariance matrix $C = \\frac{1}{n-1}Z^\\top Z$.\n3.  Compute the eigenvalues and eigenvectors of $C$. A numerical routine for symmetric matrices, such as `numpy.linalg.eigh`, is appropriate, as it guarantees real eigenvalues and orthonormal eigenvectors.\n4.  Sort the eigenvalues in descending order and arrange the corresponding eigenvectors accordingly. Let the sorted eigenvalues be $[\\lambda_1, \\lambda_2, \\dots, \\lambda_p]$ and the corresponding matrix of eigenvectors be $V=[v_1, v_2, \\dots, v_p]$.\n5.  Extract the first two eigenvalues $[\\lambda_1, \\lambda_2]$ and the first two eigenvectors into a matrix $V_{(2)} = [v_1, v_2]$.\n6.  Calculate the explained variance ratios $[r_1, r_2]$ using the total variance $\\sum_{k=1}^{p} \\lambda_k$.\n7.  Calculate the component loadings $L_{(2)} = V_{(2)} \\mathrm{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2})$.\n8.  For each of the two components, find the $0$-based index of the descriptor with the maximum absolute loading: $[\\arg\\max_i |L_{(2)}[i,0]|, \\arg\\max_i |L_{(2)}[i,1]|]$.\n9.  Calculate the scores of the first sample (the first row of $Z$) on the first two components by computing the first row of the matrix product $T_{(2)} = Z V_{(2)}$.\n10. Aggregate these results—$[\\lambda_1, \\lambda_2]$, $[r_1, r_2]$, the two indices, and the two scores for the first sample—into a nested list. All floating-point values are rounded to $6$ decimal places.\n\nThis procedure will be applied to all provided test cases. The special cases of perfect collinearity (Case $2$) and a zero-variance descriptor (Case $3$) are handled correctly by the eigendecomposition, which will yield zero eigenvalues, reflecting the reduced rank of the covariance matrix. The final output is a single-line string representing a list of the results from all test cases, with no extraneous characters or spacing.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [-1.2, -1.0, 0.8, 0.5],\n            [0.4, 0.3, -0.6, -0.2],\n            [0.8, 0.9, -0.7, 0.1],\n            [-0.5, -0.7, 0.9, -0.4],\n            [0.3, 0.4, -0.2, 0.0],\n            [0.2, 0.1, -0.2, 0.0]\n        ]),\n        np.array([\n            [-1.5, -3.0, 0.6],\n            [-0.5, -1.0, -0.2],\n            [0.5, 1.0, -0.1],\n            [1.0, 2.0, -0.2],\n            [0.5, 1.0, -0.1]\n        ]),\n        np.array([\n            [0.5, -0.3, 0.0, 0.7],\n            [-0.5, 0.3, 0.0, -0.7],\n            [0.4, -0.2, 0.0, 0.6],\n            [-0.4, 0.2, 0.0, -0.6]\n        ])\n    ]\n\n    results = []\n    for Z in test_cases:\n        case_result = perform_pca(Z)\n        results.append(case_result)\n    \n    # Format the final output string to remove all spaces as per the requirement.\n    # str(list) creates a string representation like '[item1, item2, ...]'\n    # replace(' ', '') removes the spaces after commas and inside lists.\n    final_output_string = str(results).replace(' ', '')\n    print(final_output_string)\n\ndef perform_pca(Z):\n    \"\"\"\n    Performs PCA on a given standardized data matrix Z.\n\n    Args:\n        Z (np.ndarray): The n x p standardized descriptor matrix.\n\n    Returns:\n        list: A nested list containing PCA results for the first two components.\n              [[lambda1, lambda2], [r1, r2], [idx1, idx2], [t11, t12]]\n    \"\"\"\n    n, p = Z.shape\n    \n    # 1. Compute the sample covariance matrix C\n    # The definition is C = 1/(n-1) * Z^T * Z\n    C = (Z.T @ Z) / (n - 1)\n\n    # 2. Perform eigendecomposition of C\n    # np.linalg.eigh is for Hermitian (symmetric) matrices and returns eigenvalues\n    # in ascending order.\n    eigenvalues, eigenvectors = np.linalg.eigh(C)\n\n    # 3. Sort eigenvalues and eigenvectors in descending order\n    sort_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sort_indices]\n    sorted_eigenvectors = eigenvectors[:, sort_indices]\n\n    # 4. Extract first two eigenvalues and eigenvectors\n    lambda12 = sorted_eigenvalues[0:2]\n    V2 = sorted_eigenvectors[:, 0:2]\n\n    # 5. Compute explained variance ratios\n    total_variance = np.sum(sorted_eigenvalues)\n    # Handle division by zero for cases where total variance is zero\n    if total_variance  0:\n        ratios = lambda12 / total_variance\n    else:\n        ratios = np.zeros(2)\n\n    # 6. Compute scores for the first sample\n    # T2 = Z @ V2\n    # first_sample_scores = T2[0, :]\n    first_sample_scores = Z[0, :] @ V2\n\n    # 7. Compute loadings and find indices of max absolute loading\n    # L2 = V2 @ diag(sqrt(lambda12))\n    # We must handle potentially negative eigenvalues from numerical errors, although\n    # covariance matrices are positive semidefinite. Taking abs() before sqrt is safe.\n    sqrt_lambda12 = np.sqrt(np.abs(lambda12))\n    L2 = V2 @ np.diag(sqrt_lambda12)\n    \n    max_loadings_indices = [int(np.argmax(np.abs(L2[:, 0]))), int(np.argmax(np.abs(L2[:, 1])))]\n\n    # 8. Assemble results and round floats to 6 decimal places\n    result = [\n        [round(val, 6) for val in lambda12],\n        [round(val, 6) for val in ratios],\n        max_loadings_indices,\n        [round(val, 6) for val in first_sample_scores]\n    ]\n\n    return result\n\nsolve()\n```"
        }
    ]
}