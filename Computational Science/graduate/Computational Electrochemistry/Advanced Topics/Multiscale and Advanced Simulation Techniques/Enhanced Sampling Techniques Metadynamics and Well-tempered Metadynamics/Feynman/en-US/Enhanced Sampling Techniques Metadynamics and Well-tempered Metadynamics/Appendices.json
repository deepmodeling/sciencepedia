{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of a Well-Tempered Metadynamics simulation hinges on the choice of the bias factor, $\\gamma$. This parameter orchestrates a delicate balance: a small $\\gamma$ may not provide enough bias to overcome significant energy barriers, while a large $\\gamma$ can lead to slow statistical convergence. This practice problem  will help you develop a quantitative intuition for selecting an optimal $\\gamma$ by relating it to the physical energy scales of the system you are studying.",
            "id": "4244364",
            "problem": "In an atomistic simulation of an electrified aqueous interface relevant to computational electrochemistry, you plan to accelerate sampling of a rare interfacial ion-transfer event described by a one-dimensional collective variable $s$ using Well-Tempered Metadynamics (WTMetaD). Let the equilibrium free energy surface along $s$ be $F(s)$ at temperature $T$, with a dominant activation barrier of height $\\Delta F_b$ separating two basins. In WTMetaD, the bias factor $\\gamma$ controls how much the deposited bias potential modifies the stationary sampling along $s$.\n\nYou estimate from preliminary umbrella sampling that $\\Delta F_b \\approx 0.60\\ \\mathrm{eV}$ at $T=298\\ \\mathrm{K}$ (Boltzmann constant $k_B \\approx 8.617\\times 10^{-5}\\ \\mathrm{eV/K}$). From first principles of statistical mechanics and the definition of WTMetaD, reason qualitatively about how $\\gamma$ modifies the effective barrier and the stationary distribution along $s$, and choose the option that correctly explains why choosing $\\gamma$ too small leads to underfilling of basins, choosing $\\gamma$ too large leads to slow convergence of free-energy estimates, and proposes a quantitatively reasonable heuristic for selecting $\\gamma$ based on $\\Delta F_b$ for this system. Your answer must include a defensible numerical value of $\\gamma$ for the given $\\Delta F_b$ and $T$.\n\nA. In WTMetaD, the stationary distribution along $s$ becomes broader as $\\gamma$ increases, so the effective barrier is reduced approximately to $\\Delta F_b/\\gamma$. If $\\gamma$ is too close to $1$, the deposited bias cannot compensate the barrier, transitions remain infrequent (underfilling), and free-energy reconstruction is poor. If $\\gamma$ is very large, the stationary distribution becomes nearly flat, the system spends substantial time diffusing through high free-energy regions, and reweighting suffers from large variance, leading to slow statistical convergence. A practical heuristic is to choose $\\gamma \\approx \\Delta F_b/(m k_B T)$ with $m$ in the range $3$–$5$ to reduce the effective barrier to a few $k_B T$. For $\\Delta F_b=0.60\\ \\mathrm{eV}$ and $T=298\\ \\mathrm{K}$, taking $m=4$ gives $\\gamma \\approx 0.60/\\left(4\\times 8.617\\times 10^{-5}\\times 298\\right)\\approx 5.8$, i.e., $\\gamma \\approx 6$.\n\nB. In WTMetaD, the tempering suppresses bias growth at large times, so taking $\\gamma$ small strongly damps the bias and prevents overshoot, which leads to faster convergence. Large $\\gamma$ overfills wells and causes hysteresis. A robust heuristic is to pick $\\gamma \\approx 2$ regardless of $\\Delta F_b$, because this halves any barrier. For $\\Delta F_b=0.60\\ \\mathrm{eV}$ and $T=298\\ \\mathrm{K}$ this choice is optimal.\n\nC. In WTMetaD, larger $\\gamma$ always increases the bias and thus always improves convergence because barriers vanish in the limit of large $\\gamma$. The best heuristic is to make the effective barrier negligible by choosing $\\gamma \\gg \\Delta F_b/(k_B T)$; for the given system a safe choice is $\\gamma \\approx 40$.\n\nD. The most efficient sampling is obtained when the biased barrier equals approximately $k_B T$, which minimizes variance in reweighting. Therefore set $\\gamma \\approx \\Delta F_b/(k_B T)$, so the barrier is $\\Delta F_b/\\gamma \\approx k_B T$. For $\\Delta F_b=0.60\\ \\mathrm{eV}$ and $T=298\\ \\mathrm{K}$ this gives $\\gamma \\approx 23$, which ensures both rapid transitions and minimal noise because larger $\\gamma$ strengthens tempering and thus reduces fluctuations.",
            "solution": "This problem asks for the correct reasoning and a quantitative heuristic for choosing the bias factor $\\gamma$ in a Well-Tempered Metadynamics (WTMetaD) simulation. The goal is to balance efficient barrier crossing with good statistical convergence of the reconstructed free energy surface.\n\n#### Theoretical Foundation\nIn a WTMetaD simulation, the system samples a modified stationary probability distribution along the collective variable $s$, given by:\n$$\nP_{\\text{WT}}(s) \\propto \\left[P_{\\text{unbiased}}(s)\\right]^{1/\\gamma} \\propto \\exp\\left(-\\frac{\\beta F(s)}{\\gamma}\\right)\n$$\nwhere $\\beta = 1/(k_B T)$ and $F(s)$ is the true, unbiased free energy. This is equivalent to sampling the original free energy landscape $F(s)$ at a higher effective temperature $T_{\\text{eff}} = \\gamma T$.\n\nConsequently, the effective free energy barrier that the system experiences is scaled down:\n$$\n\\Delta F_{\\text{eff}} = \\frac{\\Delta F_b}{\\gamma}\n$$\n\n#### Analysis of the Trade-off\nThe choice of $\\gamma$ involves a critical trade-off:\n1.  **If $\\gamma$ is too small (close to 1):** The effective barrier $\\Delta F_{\\text{eff}} \\approx \\Delta F_b$ remains large. The simulation will not be significantly accelerated, and the system may remain trapped in the initial free energy basin for a long time. This is often referred to as \"underfilling,\" as the bias potential grows too slowly to fill the well and push the system over the barrier.\n2.  **If $\\gamma$ is too large:** The effective barrier $\\Delta F_{\\text{eff}}$ becomes very small, and the stationary distribution $P_{\\text{WT}}(s)$ becomes nearly flat. While this allows for rapid diffusion across the entire range of $s$, it comes at a cost. The system spends a large fraction of its time in high free-energy regions that are physically irrelevant. Recovering the true free energy requires reweighting, which becomes statistically unreliable (suffers from high variance) when the biased and unbiased distributions are very different. This leads to slow convergence of the free energy estimate.\n\nThe optimal strategy is to choose a $\\gamma$ that reduces the effective barrier to a \"sweet spot\" where transitions are frequent but the landscape is not completely flattened. A common and robust heuristic is to aim for an effective barrier of a few $k_B T$, say $\\Delta F_{\\text{eff}} \\approx m k_B T$ with $m$ being a small integer like 3, 4, or 5. This ensures that barrier crossings are no longer rare events but does not push the system into excessively high-energy regions.\n\n#### Quantitative Estimation\nFollowing the heuristic $\\Delta F_b / \\gamma \\approx m k_B T$, we get $\\gamma \\approx \\Delta F_b / (m k_B T)$.\nGiven values:\n- Barrier height $\\Delta F_b = 0.60\\ \\mathrm{eV}$\n- Temperature $T = 298\\ \\mathrm{K}$\n- Boltzmann constant $k_B \\approx 8.617 \\times 10^{-5}\\ \\mathrm{eV/K}$\n\nFirst, calculate the thermal energy $k_B T$:\n$$\nk_B T \\approx (8.617 \\times 10^{-5}\\ \\mathrm{eV/K}) \\times 298\\ \\mathrm{K} \\approx 0.02568\\ \\mathrm{eV}\n$$\nNow, calculate $\\gamma$ using the heuristic with $m=4$:\n$$\n\\gamma \\approx \\frac{0.60\\ \\mathrm{eV}}{4 \\times 0.02568\\ \\mathrm{eV}} \\approx \\frac{0.60}{0.10272} \\approx 5.84\n$$\nA value of $\\gamma \\approx 6$ is a very reasonable choice based on this heuristic.\n\n#### Option-by-Option Analysis\n*   **A. Correct.** This option correctly describes the trade-off, explains that the effective barrier is $\\Delta F_b/\\gamma$, and presents the standard heuristic of reducing the barrier to a few $k_B T$. The numerical calculation is correct, yielding $\\gamma \\approx 6$.\n*   **B. Incorrect.** This option incorrectly claims that small $\\gamma$ leads to faster convergence. It leads to underfilling. The heuristic of $\\gamma \\approx 2$ is arbitrary and not based on the system's energy scales.\n*   **C. Incorrect.** This option incorrectly claims that larger $\\gamma$ always improves convergence. While it makes barriers smaller, it severely degrades the statistical quality of the reweighted free energy estimate. A choice of $\\gamma \\approx 40$ would be far too high and lead to very slow statistical convergence.\n*   **D. Incorrect.** While reducing the barrier to $\\approx k_B T$ is a possible strategy, it is very aggressive and yields a high bias factor of $\\gamma \\approx 23$. This large $\\gamma$ would likely result in slow statistical convergence due to the reasons outlined above. The statement that larger $\\gamma$ reduces fluctuations is misleading; it makes the *biased landscape* smoother, but increases the statistical error (variance) in the *reconstructed unbiased free energy*.\n\nTherefore, option A provides the most accurate and well-justified protocol.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After successfully running a Well-Tempered Metadynamics simulation, the primary output is the converged bias potential, $V_b(s)$. To extract the physically meaningful free energy surface, $F(s)$, one must understand the precise mathematical relationship between these two quantities. This exercise  walks you through the derivation of this relationship from the fundamental principles of biased statistical ensembles, equipping you with the essential tool for analyzing your simulation data.",
            "id": "4244349",
            "problem": "In a constant-potential molecular dynamics study of a proton-coupled electron transfer at an electrified interface, you wish to reconstruct the free energy surface $F(s)$ along a collective variable $s$ (for example, a charge-transfer coordinate) using Well-Tempered Metadynamics (WTMetaD). The system is thermostatted at temperature $T$ and the WTMetaD bias $V_b(s,t)$ is built by depositing Gaussian hills with an instantaneous rate that is modulated according to the WTMetaD prescription. Assume the following foundational facts:\n\n- Statistical mechanics of biased ensembles implies that the stationary distribution of the collective variable under the applied bias is $P_b(s) \\propto \\exp\\!\\left(-\\beta\\,[F(s) + V_b(s)]\\right)$, where $\\beta = 1/(k_B T)$, $k_B$ is the Boltzmann constant, and $T$ is the physical temperature.\n- In Well-Tempered Metadynamics (WTMetaD), the instantaneous bias deposition rate at a given $s$ is proportional to $\\exp\\!\\left(-V_b(s,t)/(k_B \\Delta T)\\right)$, where $\\Delta T$ is the bias temperature. In the infinite-time quasi-stationary regime, the expected rate of bias accumulation is uniform across $s$, which implies $P_b(s)\\,\\exp\\!\\left(-V_b(s)/(k_B \\Delta T)\\right)$ is independent of $s$.\n- The WTMetaD bias factor $\\gamma$ is defined by $\\gamma = (T + \\Delta T)/T$.\n\nUsing only these facts, derive the infinite-time relation that connects $V_b(s)$ and $F(s)$ and determine the scaling factor $c$ such that $F(s) = c\\,V_b(s)$ up to an additive constant. Then, for a simulation conducted at $T = 300\\ \\mathrm{K}$ with bias factor $\\gamma = 10$, compute the bias temperature $\\Delta T$ (express your answer in $\\mathrm{K}$) and the dimensionless scaling factor $c$ in the infinite-time limit. Provide your final answer as a row matrix $\\begin{pmatrix}\\Delta T & c\\end{pmatrix}$. Do not round your answer.",
            "solution": "The problem requires the derivation of the relationship between the free energy surface $F(s)$ and the converged Well-Tempered Metadynamics (WTMetaD) bias potential $V_b(s)$, and the calculation of specific parameters for a given system. The derivation will be based on the foundational principles of biased statistical mechanical ensembles and WTMetaD, as provided in the problem statement.\n\nThe first principle states that the stationary probability distribution of the collective variable $s$ under the influence of both the intrinsic free energy $F(s)$ and the applied bias $V_b(s)$ is given by:\n$$P_b(s) \\propto \\exp\\left(-\\beta[F(s) + V_b(s)]\\right)$$\nwhere $\\beta = 1/(k_B T)$, $k_B$ is the Boltzmann constant, and $T$ is the temperature of the system. We can write this with an explicit normalization constant $C_1$:\n$$P_b(s) = C_1 \\exp\\left(-\\beta F(s) - \\beta V_b(s)\\right)$$\n\nThe second principle describes the condition for the quasi-stationary state in WTMetaD, achieved in the infinite-time limit. It states that the effective sampling rate is uniform across the space of the collective variable $s$. This is mathematically expressed as the product of the biased probability $P_b(s)$ and a tempering factor being constant:\n$$P_b(s) \\exp\\left(-\\frac{V_b(s)}{k_B \\Delta T}\\right) = C_2$$\nwhere $\\Delta T$ is the bias temperature and $C_2$ is a constant. From this expression, we can isolate $P_b(s)$:\n$$P_b(s) = C_2 \\exp\\left(\\frac{V_b(s)}{k_B \\Delta T}\\right)$$\n\nWe now have two expressions for the same stationary distribution $P_b(s)$. Equating them yields:\n$$C_1 \\exp\\left(-\\beta F(s) - \\beta V_b(s)\\right) = C_2 \\exp\\left(\\frac{V_b(s)}{k_B \\Delta T}\\right)$$\nTo solve for $F(s)$, we take the natural logarithm of both sides:\n$$\\ln(C_1) - \\beta F(s) - \\beta V_b(s) = \\ln(C_2) + \\frac{V_b(s)}{k_B \\Delta T}$$\nNext, we rearrange the terms to isolate $\\beta F(s)$:\n$$-\\beta F(s) = \\beta V_b(s) + \\frac{V_b(s)}{k_B \\Delta T} + \\ln(C_2) - \\ln(C_1)$$\nWe can factor out $V_b(s)$ from the first two terms on the right-hand side and combine the logarithmic terms into a single constant, $C' = \\ln(C_2) - \\ln(C_1)$:\n$$-\\beta F(s) = V_b(s)\\left(\\beta + \\frac{1}{k_B \\Delta T}\\right) + C'$$\nSolving for $F(s)$ by dividing by $-\\beta$:\n$$F(s) = -\\frac{1}{\\beta} V_b(s)\\left(\\beta + \\frac{1}{k_B \\Delta T}\\right) - \\frac{C'}{\\beta}$$\nLetting $C_{add} = -C'/\\beta$ be the new additive constant, we simplify the term in the parenthesis:\n$$F(s) = -V_b(s)\\left(1 + \\frac{1}{\\beta k_B \\Delta T}\\right) + C_{add}$$\nSubstituting the definition $\\beta = 1/(k_B T)$:\n$$F(s) = -V_b(s)\\left(1 + \\frac{k_B T}{k_B \\Delta T}\\right) + C_{add} = -V_b(s)\\left(1 + \\frac{T}{\\Delta T}\\right) + C_{add}$$\nThis is the sought-after relationship between $F(s)$ and $V_b(s)$. The problem asks for this relationship to be expressed in terms of the bias factor $\\gamma$, defined as $\\gamma = (T + \\Delta T)/T$. Let us manipulate the scaling term:\n$$1 + \\frac{T}{\\Delta T} = \\frac{\\Delta T + T}{\\Delta T}$$\nFrom the definition of $\\gamma$, we have $T + \\Delta T = \\gamma T$ and $\\Delta T = \\gamma T - T = T(\\gamma-1)$. Substituting these into the scaling term gives:\n$$\\frac{\\Delta T + T}{\\Delta T} = \\frac{\\gamma T}{T(\\gamma-1)} = \\frac{\\gamma}{\\gamma-1}$$\nThus, the final relation between the free energy and the converged bias potential is:\n$$F(s) = -\\frac{\\gamma}{\\gamma-1}V_b(s) + C_{add}$$\nThis equation shows that the free energy surface $F(s)$ is linearly proportional to the converged WTMetaD bias potential $V_b(s)$, up to an additive constant.\n\nThe problem defines a scaling factor $c$ such that $F(s) = c\\,V_b(s)$ up to an additive constant. By direct comparison with our derived relation, we find:\n$$c = -\\frac{\\gamma}{\\gamma-1}$$\n\nNow, we proceed to the numerical calculation part of the problem. We are given the system temperature $T = 300\\ \\mathrm{K}$ and the bias factor $\\gamma = 10$.\n\nFirst, we calculate the bias temperature $\\Delta T$. Using the rearranged definition of $\\gamma$:\n$$\\Delta T = T(\\gamma - 1)$$\nSubstituting the given values:\n$$\\Delta T = 300 \\cdot (10 - 1) = 300 \\cdot 9 = 2700\\ \\mathrm{K}$$\n\nSecond, we calculate the dimensionless scaling factor $c$:\n$$c = -\\frac{\\gamma}{\\gamma-1}$$\nSubstituting the value of $\\gamma$:\n$$c = -\\frac{10}{10-1} = -\\frac{10}{9}$$\n\nThe final answer is required as a row matrix $\\begin{pmatrix}\\Delta T & c\\end{pmatrix}$. The values are $\\Delta T = 2700$ and $c = -10/9$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2700 & -\\frac{10}{9}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A calculated free energy profile is only as reliable as its error bars. Estimating statistical uncertainty in metadynamics is non-trivial due to strong temporal correlations in the trajectory and the time-dependent nature of the bias potential. This problem  introduces a rigorous block bootstrap protocol to quantify these uncertainties, guiding you through the advanced techniques required to ensure the statistical robustness of your findings.",
            "id": "4244420",
            "problem": "A computational electrochemistry group performs Well-Tempered Metadynamics (WTMetaD) Molecular Dynamics (MD) to estimate the free energy profile $F(s)$ along a collective variable $s$ that quantifies the progress of an interfacial electron-transfer event at a metal electrode held at constant potential. The trajectory is biased by a time-dependent metadynamics potential $V(s,t)$ and recorded at a sampling interval $\\Delta t$. The group wants a principled protocol to estimate statistical uncertainties in the reconstructed $F(s)$ using a bootstrap approach that resamples time blocks from the biased trajectory.\n\nStarting from first principles, recall that the unbiased free energy is defined via the unbiased probability density $P(s)$ as $F(s) = -k_{\\mathrm{B}} T \\ln P(s) + C$, where $k_{\\mathrm{B}}$ is the Boltzmann constant, $T$ is the absolute temperature, and $C$ is an arbitrary constant. At a given time $t$, the biased sampling density for $s$ is proportional to $\\pi_{t}(s) \\propto \\exp\\left[-\\beta \\left(F(s) + V(s,t)\\right)\\right]$, with $\\beta = 1/(k_{\\mathrm{B}} T)$. Importance sampling implies that an unbiased histogram for $s$ can be formed by assigning to each snapshot at time $t$ the weight $w(t)$ proportional to the ratio $P(s(t))/\\pi_{t}(s(t))$, which does not require knowledge of $F(s)$ but depends on $V(s,t)$.\n\nBecause MD time series are strongly autocorrelated, naive independent resampling underestimates the variance. Let $X_{t}^{(k)} = w(t) \\,\\mathbf{1}\\{s(t) \\in \\mathcal{B}_{k}\\}$ denote the reweighted bin-count contribution to bin $\\mathcal{B}_{k}$ at time $t$, and define its normalized autocorrelation function $\\rho_{X^{(k)}}(\\tau)$ for lag $\\tau \\in \\mathbb{N}$. The integrated autocorrelation time is $\\tau_{\\mathrm{int}}^{(k)} = 1 + 2 \\sum_{\\tau=1}^{\\infty} \\rho_{X^{(k)}}(\\tau)$, and a conservative block length $L$ should satisfy $L \\gtrsim c \\,\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$ for some $c$ in the range $[5,10]$ to approximate independence across blocks. Moreover, because $V(s,t)$ is time dependent, one should avoid mixing early non-stationary bias-deposition segments with late quasi-stationary segments. A practical way is to define a stationarity onset time $t^{\\ast}$ by monitoring the average magnitude of bias increments and requiring it to be much smaller than $k_{\\mathrm{B}} T$ per deposition.\n\nConsider the following candidate protocols to bootstrap uncertainties in $F(s)$ from a single long WTMetaD trajectory. Which protocol is asymptotically consistent and scientifically justified?\n\nA. Build a reweighted histogram using $w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right]$ for each recorded frame. Restrict the resampling pool to times $t \\ge t^{\\ast}$ where the average bias increment per deposition obeys $\\langle |\\Delta V| \\rangle \\le \\varepsilon k_{\\mathrm{B}} T$ with a small $\\varepsilon$, ensuring quasi-stationarity. Perform a nonoverlapping block bootstrap by partitioning the post-$t^{\\ast}$ time series into contiguous blocks of length $L$ chosen as $L = c \\,\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$ with $c \\in [5,10]$, where $\\tau_{\\mathrm{int}}^{(k)}$ is computed from $X_{t}^{(k)}$. Resample blocks with replacement to build bootstrap replicates, reconstruct $F^{\\ast}(s) = -k_{\\mathrm{B}} T \\ln H^{\\ast}(s) + C^{\\prime}$ for each replicate from the reweighted histogram $H^{\\ast}(s)$, and report uncertainties from the spread (for example, standard deviation or percentile intervals) across replicates.\n\nB. Build the histogram using $w(t) = \\exp\\left[-\\beta V\\left(s(t),t\\right)\\right]$ and apply a standard i.i.d. bootstrap by resampling individual frames regardless of temporal order. Use all frames from the full trajectory, because block resampling is unnecessary once reweighting is applied, and choose $L = \\Delta t$ to maximize the number of resampled units.\n\nC. Choose the block length by the heuristic $L = \\sigma_{s}^{2}/D_{s}$, where $\\sigma_{s}$ is the kernel width in $s$ used to deposit Gaussians and $D_{s}$ is the diffusion coefficient of $s$ estimated from the biased trajectory, and set $w(t) = 1$ because the well-tempered bias makes the sampled distribution stationary. Resample blocks of length $L$ from the entire trajectory and compute $F(s)$ by direct logarithm of the biased histogram.\n\nD. Use overlapping blocks of length equal to one bias-deposition stride and set $w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)/\\gamma\\right]$, where $\\gamma$ is the well-tempered bias factor. Combine early and late trajectory segments to maximize sample size, and estimate uncertainties from the variance of $F(s)$ across $M$ bootstrap replicates without checking autocorrelation.\n\nE. Exploit the stationary distribution of WTMetaD, $p_{\\mathrm{WT}}(s) \\propto \\exp\\left[-\\beta F(s)/\\gamma\\right]$, to avoid reweighting and construct $F(s)$ directly from the biased histogram. Use short blocks of length $L \\approx \\tau_{\\mathrm{int}}/2$ to increase the number of blocks, and include the entire trajectory because the WTMetaD bias ensures rapid decorrelation.\n\nSelect the correct option(s).",
            "solution": "Begin from the definition of the unbiased free energy along a collective variable. The free energy $F(s)$ is defined by the unbiased probability density $P(s)$ via $F(s) = -k_{\\mathrm{B}} T \\ln P(s) + C$. In a biased simulation with time-dependent bias $V(s,t)$, the instantaneous sampling density for $s$ at time $t$ is proportional to\n$$\n\\pi_{t}(s) \\propto \\exp\\left[-\\beta\\left(F(s) + V(s,t)\\right)\\right],\n$$\nassuming the dynamical propagator preserves detailed balance with respect to the biased potential and neglecting Jacobian corrections for $s$.\n\nImportance sampling states that, for any observable $g(s)$, the unbiased expectation can be recovered from biased samples $\\{s(t)\\}$ by\n$$\n\\langle g(s)\\rangle_{\\text{unbiased}} = \\frac{\\left\\langle g\\left(s(t)\\right)\\, w(t)\\right\\rangle_{\\text{biased}}}{\\left\\langle w(t)\\right\\rangle_{\\text{biased}}},\n$$\nwhere the weight $w(t)$ is proportional to the ratio of the target density to the sampling density evaluated at $s(t)$. For the target density $P(s) \\propto \\exp[-\\beta F(s)]$ and the sampling density $\\pi_{t}(s) \\propto \\exp[-\\beta(F(s)+V(s,t))]$, the ratio is\n$$\nw(t) \\propto \\frac{\\exp\\left[-\\beta F\\left(s(t)\\right)\\right]}{\\exp\\left[-\\beta\\left(F\\left(s(t)\\right)+V\\left(s(t),t\\right)\\right)\\right]} = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right].\n$$\nAny time-dependent normalization factor cancels in ratios or can be absorbed into a constant that does not affect relative bin weights in a histogram. Therefore, a consistent reweighted histogram estimator for $P(s)$ is\n$$\nH(s) = \\sum_{t} w(t)\\,\\delta\\left(s - s(t)\\right), \\quad \\text{with} \\quad w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right],\n$$\nand the corresponding free energy estimator is $\\hat{F}(s) = -k_{\\mathrm{B}} T \\ln H(s) + C^{\\prime}$, where $C^{\\prime}$ enforces a chosen reference.\n\nNext, address statistical uncertainty under autocorrelation. For a time series $Y_{t}$ with normalized autocorrelation function $\\rho_{Y}(\\tau)$, the integrated autocorrelation time is\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{\\tau=1}^{\\infty} \\rho_{Y}(\\tau),\n$$\nwhich quantifies the factor by which correlations inflate the variance relative to independent samples. For a reweighted histogram, a natural choice is $Y_{t} = X_{t}^{(k)} = w(t)\\,\\mathbf{1}\\{s(t)\\in \\mathcal{B}_{k}\\}$ for each bin $\\mathcal{B}_{k}$, because the variance of bin counts directly enters the uncertainty of $\\hat{F}(s)$ via propagation through the logarithm. To capture the slowest timescale among bins, use $\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$.\n\nBlock bootstrap resamples contiguous segments of the time series to preserve intra-block correlation structures while approximating independence between blocks if the block length $L$ exceeds the correlation length. A principled choice is $L \\gtrsim c \\,\\max_{k}\\tau_{\\mathrm{int}}^{(k)}$ with $c$ in $[5,10]$, which reduces bias in bootstrap variance estimates for strongly correlated series. Because $V(s,t)$ is time dependent, the sampling density changes during the run; hence, mixing early times when $V(s,t)$ grows rapidly with later times when $V(s,t)$ fluctuates around a quasi-stationary value can introduce non-stationarity into resampling. A practical remedy is to define a stationarity onset time $t^{\\ast}$ by monitoring bias increments per deposition,\n$$\n\\Delta V_{n} = V\\left(s(t_{n}),t_{n}\\right) - V\\left(s(t_{n^{-}}),t_{n^{-}}\\right),\n$$\nwhere $t_{n}$ enumerates deposition events. When $\\langle |\\Delta V_{n}| \\rangle \\le \\varepsilon k_{\\mathrm{B}} T$ with a small $\\varepsilon$ (for example, $\\varepsilon \\in [0.05,0.2]$), subsequent segments can be regarded as approximately stationary for reweighting and block resampling.\n\nWith these principles in place, evaluate each option:\n\nOption A: This protocol uses $w(t) = \\exp\\left[\\beta V\\left(s(t),t\\right)\\right]$, which is the correct importance sampling weight derived above. It restricts resampling to a quasi-stationary regime defined by a threshold on average bias increments per deposition, which is a scientifically sound criterion to mitigate non-stationarity arising from time-dependent bias. For block length, it computes $\\tau_{\\mathrm{int}}^{(k)}$ from $X_{t}^{(k)}$ and sets $L = c \\,\\max_{k} \\tau_{\\mathrm{int}}^{(k)}$ with $c$ in $[5,10]$, which is a conservative choice that aligns with the need to exceed the longest correlation time across bins. It then resamples nonoverlapping blocks and reconstructs $F^{\\ast}(s)$ for each bootstrap replicate from reweighted histograms. This is asymptotically consistent and scientifically justified for estimating uncertainties in $\\hat{F}(s)$. Verdict — Correct.\n\nOption B: The weight choice $w(t) = \\exp\\left[-\\beta V\\left(s(t),t\\right)\\right]$ is the inverse of the correct ratio. From the derivation, the correct weight increases with $V$ as $\\exp(+\\beta V)$; using $\\exp(-\\beta V)$ would reweight in the wrong direction and bias the estimator for $P(s)$ and thus $F(s)$. Furthermore, applying an i.i.d. bootstrap at the frame level ignores autocorrelation, which leads to underestimation of uncertainties for correlated MD data. Choosing $L = \\Delta t$ does not address correlation. Verdict — Incorrect.\n\nOption C: The block length heuristic $L=\\sigma_{s}^{2}/D_{s}$ is dimensionally inconsistent unless accompanied by a specific stochastic model, and it does not follow from the integrated autocorrelation time of the reweighted observables that determine uncertainty in $\\hat{F}(s)$. In addition, setting $w(t)=1$ ignores the need to reweight from the biased sampling density back to the unbiased target; while WTMetaD can yield a modified stationary distribution $p_{\\mathrm{WT}}(s) \\propto \\exp\\left[-\\beta F(s)/\\gamma\\right]$, directly using the biased histogram without reweighting does not recover $P(s)$. This violates the importance sampling principle derived above. Verdict — Incorrect.\n\nOption D: Overlapping blocks are sometimes used in block bootstrap variants to reduce variance of bootstrap estimates, but choosing a block length equal to one deposition stride is not justified by the integrated autocorrelation time of the reweighted bin counts and is typically far too short to ensure approximate independence between blocks. The weight $w(t) = \\exp\\left[\\beta V(s(t),t)/\\gamma\\right]$ is not the correct importance sampling ratio for recovering the unbiased distribution; the derivation shows that the ratio relative to the biased density is $\\exp\\left[\\beta V(s(t),t)\\right]$ without division by $\\gamma$. Combining early and late segments without checking stationarity can inject non-stationarity into resampling and distort uncertainty estimates. Verdict — Incorrect.\n\nOption E: While WTMetaD achieves a stationary modified distribution $p_{\\mathrm{WT}}(s) \\propto \\exp\\left[-\\beta F(s)/\\gamma\\right]$, computing $F(s)$ from the biased histogram without reweighting yields the scaled free energy $F(s)/\\gamma$ and does not recover the unbiased $F(s)$ unless additional calibration is performed; the problem explicitly seeks uncertainties in $F(s)$, not $F(s)/\\gamma$. Choosing $L \\approx \\tau_{\\mathrm{int}}/2$ is too short to decorrelate blocks and will underestimate uncertainty. Including the entire trajectory, including non-stationary early deposition, ignores the time dependence of $V(s,t)$ and can bias both the estimator and its uncertainty. Verdict — Incorrect.\n\nTherefore, the scientifically grounded and asymptotically consistent protocol is given in Option A.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}