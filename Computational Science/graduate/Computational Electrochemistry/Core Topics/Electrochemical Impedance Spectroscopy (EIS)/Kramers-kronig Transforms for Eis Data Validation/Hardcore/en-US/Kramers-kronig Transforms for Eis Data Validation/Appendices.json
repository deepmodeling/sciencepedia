{
    "hands_on_practices": [
        {
            "introduction": "To begin our exploration of the Kramers-Kronig (KK) relations, we first ground the abstract mathematical framework in a tangible physical model. This practice involves explicitly calculating the Hilbert transform for a function representing the real part of an ideal Debye element's impedance, which models a simple parallel resistor-capacitor (RC) circuit. Successfully completing this exercise  will provide a concrete understanding of how the real (resistive) and imaginary (capacitive) components of a causal impedance are intrinsically linked.",
            "id": "4249748",
            "problem": "A key step in validating Electrochemical Impedance Spectroscopy (EIS) data via Kramers-Kronig (KK) transforms is to verify that the measured spectrum can be interpreted as the boundary values of a causal, linear, time-invariant response function analytic in the upper half of the complex frequency plane. The KK relations are equivalent to a Hilbert transform connecting the real and imaginary parts of such a response. The Hilbert transform operator $\\mathcal{H}[\\cdot]$ acting on a real-valued function $f(\\omega)$ is defined by\n$$\n\\mathcal{H}[f](\\omega) \\equiv \\frac{1}{\\pi}\\,\\mathrm{PV}\\int_{-\\infty}^{\\infty}\\frac{f(\\xi)}{\\omega - \\xi}\\,d\\xi,\n$$\nwhere $\\mathrm{PV}$ denotes the Cauchy principal value, and $\\omega$ is the angular frequency.\n\nConsider the real-valued function $f(\\omega)=\\frac{1}{1+\\omega^{2}}$, which is even in $\\omega$ and decays as $\\omega\\to\\pm\\infty$. This $f(\\omega)$ can arise as the real part of a normalized first-order (single time-constant) Debye-type response that models a simple resistor-capacitor (RC) element under standard causality conventions for frequency-domain response functions. Starting from the above definition of the Hilbert transform and the causality-analyticity framework that underpins KK relations, compute the Hilbert transform $\\mathcal{H}[f](\\omega)$ explicitly as a closed-form expression in $\\omega$. Then, interpret the result in the context of KK consistency for the simplest RC element (single Debye relaxation), explaining the mapping between the Hilbert transform of $f(\\omega)$ and the corresponding dissipative component associated with the RC time constant, and why this satisfies the requirements for EIS data validation.\n\nExpress the final answer as a single closed-form analytic expression. No rounding is required, and no units are involved.",
            "solution": "The problem is to compute the Hilbert transform of the function $f(\\omega) = \\frac{1}{1+\\omega^2}$ and interpret the result within the context of Kramers-Kronig (KK) relations for Electrochemical Impedance Spectroscopy (EIS) data validation.\n\nFirst, we perform the calculation of the Hilbert transform. The Hilbert transform of a function $f(\\omega)$ is defined as:\n$$\n\\mathcal{H}[f](\\omega) = \\frac{1}{\\pi}\\,\\mathrm{PV}\\int_{-\\infty}^{\\infty}\\frac{f(\\xi)}{\\omega - \\xi}\\,d\\xi\n$$\nwhere $\\mathrm{PV}$ denotes the Cauchy principal value. Substituting $f(\\xi) = \\frac{1}{1+\\xi^2}$, we need to evaluate the integral:\n$$\n\\mathcal{H}[f](\\omega) = \\frac{1}{\\pi}\\,\\mathrm{PV}\\int_{-\\infty}^{\\infty}\\frac{1}{(1+\\xi^2)(\\omega - \\xi)}\\,d\\xi\n$$\nThis integral is most efficiently computed using complex analysis and the residue theorem. Let's consider the complex function $F(z)$:\n$$\nF(z) = \\frac{1}{(1+z^2)(\\omega-z)} = \\frac{1}{(z-i)(z+i)(\\omega-z)}\n$$\nThis function has simple poles at $z=i$, $z=-i$, and $z=\\omega$. The pole at $z=\\omega$ lies on the real axis, which is the path of integration, necessitating the use of the principal value.\n\nWe integrate $F(z)$ along a closed contour $C$ in the upper half-plane. The contour consists of a line segment along the real axis from $-R$ to $R$, indented by a small semi-circle $\\gamma_{\\epsilon}$ of radius $\\epsilon$ into the upper half-plane to avoid the pole at $z=\\omega$, and closed by a large semi-circle $\\Gamma_R$ of radius $R$ in the upper half-plane. For sufficiently large $R$ and small $\\epsilon$, the only pole enclosed by this contour is at $z=i$.\n\nBy the residue theorem, the integral around the closed contour $C$ is given by:\n$$\n\\oint_C F(z)\\,dz = 2\\pi i \\, \\mathrm{Res}(F, i)\n$$\nThe residue at the simple pole $z=i$ is:\n$$\n\\mathrm{Res}(F, i) = \\lim_{z \\to i} (z-i)F(z) = \\lim_{z \\to i} \\frac{1}{(z+i)(\\omega-z)} = \\frac{1}{(i+i)(\\omega-i)} = \\frac{1}{2i(\\omega-i)}\n$$\n$$\n\\mathrm{Res}(F, i) = \\frac{1}{2i\\omega - 2i^2} = \\frac{1}{2+2i\\omega} = \\frac{1-i\\omega}{2(1-i\\omega)(1+i\\omega)} = \\frac{1-i\\omega}{2(1+\\omega^2)}\n$$\nThus, the value of the contour integral is:\n$$\n\\oint_C F(z)\\,dz = 2\\pi i \\left( \\frac{1-i\\omega}{2(1+\\omega^2)} \\right) = \\frac{\\pi i (1-i\\omega)}{1+\\omega^2} = \\frac{\\pi i - \\pi i^2 \\omega}{1+\\omega^2} = \\frac{\\pi\\omega + i\\pi}{1+\\omega^2}\n$$\nThe integral over the closed contour $C$ can be decomposed into its parts:\n$$\n\\oint_C F(z)\\,dz = \\mathrm{PV}\\int_{-\\infty}^{\\infty} F(x)\\,dx + \\text{contribution from indentation} + \\text{contribution from large arc}\n$$\nIn the limit as $R \\to \\infty$ and $\\epsilon \\to 0$:\n1.  The sum of the integrals along the real axis segments becomes the Cauchy principal value of the integral.\n2.  The integral over the large semi-circle $\\Gamma_R$ vanishes. For large $|z|$, $|F(z)| \\sim O(1/|z|^3)$, so by the estimation lemma, the integral goes to $0$ as $R \\to \\infty$.\n3.  The contribution from integrating over an infinitesimally small semi-circle that bypasses the simple pole at $z=\\omega$ into the upper half-plane is $-i\\pi \\mathrm{Res}(F, \\omega)$. The residue of $F(z)$ at $z=\\omega$ is $\\lim_{z\\to\\omega}(z-\\omega)F(z) = \\frac{-1}{1+\\omega^2}$. Hence the integral's contribution from this path is $(-i\\pi)\\left(\\frac{-1}{1+\\omega^2}\\right) = \\frac{i\\pi}{1+\\omega^2}$.\n\nCombining these results:\n$$\n\\frac{\\pi\\omega + i\\pi}{1+\\omega^2} = \\mathrm{PV}\\int_{-\\infty}^{\\infty} \\frac{1}{(1+\\xi^2)(\\omega - \\xi)}\\,d\\xi + \\frac{i\\pi}{1+\\omega^2} + 0\n$$\nSolving for the principal value integral:\n$$\n\\mathrm{PV}\\int_{-\\infty}^{\\infty} \\frac{1}{(1+\\xi^2)(\\omega - \\xi)}\\,d\\xi = \\frac{\\pi\\omega + i\\pi}{1+\\omega^2} - \\frac{i\\pi}{1+\\omega^2} = \\frac{\\pi\\omega}{1+\\omega^2}\n$$\nNow, we find the Hilbert transform:\n$$\n\\mathcal{H}[f](\\omega) = \\frac{1}{\\pi} \\left( \\frac{\\pi\\omega}{1+\\omega^2} \\right) = \\frac{\\omega}{1+\\omega^2}\n$$\nThis is the closed-form expression for the Hilbert transform of $f(\\omega)$.\n\nNow, we interpret this result in the context of EIS and KK relations. The problem states that the system should be a causal response function analytic in the upper half of the complex frequency plane. This implies a Fourier transform convention of the form $e^{i\\omega t}$. For a simple parallel Resistor-Capacitor (RC) element (Debye model), the impedance $Z(\\omega)$ that is analytic in the upper half-plane has the form:\n$$\nZ(\\omega) = \\frac{R_p}{1-i\\omega R_p C}\n$$\nThis has a pole at $\\omega=-i/(R_p C)$ in the lower half-plane. Let's normalize by choosing $R_p=1$ and the time constant $\\tau=R_p C = 1$. The impedance function is:\n$$\nZ(\\omega) = \\frac{1}{1-i\\omega}\n$$\nWe can separate this into its real and imaginary parts, $Z(\\omega) = Z'(\\omega) + iZ''(\\omega)$:\n$$\nZ(\\omega) = \\frac{1}{1-i\\omega} \\cdot \\frac{1+i\\omega}{1+i\\omega} = \\frac{1+i\\omega}{1+\\omega^2} = \\frac{1}{1+\\omega^2} + i\\frac{\\omega}{1+\\omega^2}\n$$\nThe real part is $Z'(\\omega) = \\frac{1}{1+\\omega^2}$, which is exactly the function $f(\\omega)$ given in the problem. The imaginary part is $Z''(\\omega) = \\frac{\\omega}{1+\\omega^2}$. The imaginary part of the impedance represents the reactive or dissipative processes in the system.\n\nAccording to the Kramers-Kronig relations for a causal function analytic in the upper half-plane, the imaginary part is the Hilbert transform of the real part:\n$$\nZ''(\\omega) = \\mathcal{H}[Z'](\\omega) = \\frac{1}{\\pi}\\,\\mathrm{PV}\\int_{-\\infty}^{\\infty}\\frac{Z'(\\xi)}{\\omega - \\xi}\\,d\\xi\n$$\nOur direct calculation yielded $\\mathcal{H}\\left[\\frac{1}{1+\\omega^2}\\right] = \\frac{\\omega}{1+\\omega^2}$. This is precisely the imaginary part, $Z''(\\omega)$, of our chosen causal impedance model.\n\nThis result perfectly illustrates the principle of KK consistency. It shows that for an ideal, causal system like a simple RC element, the real (resistive) and imaginary (capacitive/dissipative) components of its impedance are not independent but are related through the Hilbert transform. The validation of experimental EIS data involves numerically performing this transform on the measured real part of the spectrum and comparing the result with the measured imaginary part. A close match confirms that the experimental data are consistent with the fundamental physical principles of causality, linearity, and time-invariance, thus ensuring the data's validity for further analysis and modeling. The function $f(\\omega)$ provides the archetypal example of this consistency.",
            "answer": "$$\n\\boxed{\\frac{\\omega}{1+\\omega^2}}\n$$"
        },
        {
            "introduction": "While analytical examples demonstrate the principle of KK consistency, real-world Electrochemical Impedance Spectroscopy (EIS) data is always corrupted by noise. This practice  challenges you to move from idealized models to the statistical analysis of experimental measurements. By deriving a chi-square goodness-of-fit statistic, you will construct a rigorous tool to quantify the agreement between measured data and the predictions of the KK relations, which is the cornerstone of data validation.",
            "id": "4249747",
            "problem": "A laboratory collects Electrochemical Impedance Spectroscopy (EIS) data at $N$ discrete angular frequencies $\\{\\omega_{k}\\}_{k=1}^{N}$, yielding measured complex impedances $Z_{\\text{meas}}(\\omega_{k}) = X_{k} + \\mathrm{i} Y_{k}$, where $X_{k} = \\operatorname{Re} Z_{\\text{meas}}(\\omega_{k})$ and $Y_{k} = \\operatorname{Im} Z_{\\text{meas}}(\\omega_{k})$. Under the assumptions of linearity, time invariance, stability, and causality, the Kramers-Kronig (KK) relations imply that the real part can be predicted from the imaginary part by a linear integral transform. In numerical practice on a finite frequency grid, this is represented by a known matrix operator $H \\in \\mathbb{R}^{N \\times N}$ that approximates the KK transform from the vector of imaginary parts to the vector of real parts, up to a known high-frequency intercept $Z_{\\infty} \\in \\mathbb{R}$. Let $\\mathbf{X} = (X_{1},\\dots,X_{N})^{\\top}$ and $\\mathbf{Y} = (Y_{1},\\dots,Y_{N})^{\\top}$ denote the measured real and imaginary parts, respectively, and let $\\mathbf{c} = Z_{\\infty} \\mathbf{1}$, where $\\mathbf{1} \\in \\mathbb{R}^{N}$ has all entries equal to $1$.\n\nAssume that the measurement noise in $\\mathbf{X}$ and $\\mathbf{Y}$ is zero-mean Gaussian and independent between the two channels: $\\mathbf{X} = \\mathbf{X}_{0} + \\boldsymbol{\\varepsilon}_{X}$ and $\\mathbf{Y} = \\mathbf{Y}_{0} + \\boldsymbol{\\varepsilon}_{Y}$, with $\\boldsymbol{\\varepsilon}_{X} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_{X})$, $\\boldsymbol{\\varepsilon}_{Y} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_{Y})$, and $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_{X}, \\boldsymbol{\\varepsilon}_{Y}) = \\mathbf{0}$. The true, noise-free signals satisfy the KK-consistency condition $\\mathbf{X}_{0} = H \\mathbf{Y}_{0} + \\mathbf{c}$.\n\nStarting from these assumptions and the definitions of multivariate Gaussian random variables, do the following:\n\n1. Derive the covariance matrix of the residual vector defined by the difference between the measured real part and the KK-predicted real part from the measured imaginary part, namely $\\Delta \\mathbf{Z} \\equiv \\mathbf{X} - (H \\mathbf{Y} + \\mathbf{c})$.\n\n2. Define a normalized residual vector $\\mathbf{r}$ that is dimensionless and has identity covariance under the stated noise model.\n\n3. Using the probability density of the multivariate Gaussian distribution and a log-likelihood argument, derive a chi-square goodness-of-fit statistic that is additive across frequencies and dimensionless, expressed in compact matrix form in terms of $\\Delta \\mathbf{Z}$ and the covariance you derived.\n\nProvide your final answer as a single closed-form analytic expression for this chi-square statistic in terms of $\\Delta \\mathbf{Z}$, $H$, $\\Sigma_{X}$, and $\\Sigma_{Y}$. Do not report an equation or inequality; report only the expression. No numerical evaluation is required, and no rounding is needed. The statistic is dimensionless, so no units should be reported.",
            "solution": "The problem requires the derivation of a chi-square goodness-of-fit statistic for validating Electrochemical Impedance Spectroscopy (EIS) data against the Kramers-Kronig (KK) relations, based on a given statistical model for measurement noise. I will first validate the problem statement and then proceed with a step-by-step derivation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Discrete angular frequencies: $\\{\\omega_{k}\\}_{k=1}^{N}$.\n-   Measured complex impedance: $Z_{\\text{meas}}(\\omega_{k}) = X_{k} + \\mathrm{i} Y_{k}$.\n-   Vector of measured real parts: $\\mathbf{X} = (X_{1},\\dots,X_{N})^{\\top}$.\n-   Vector of measured imaginary parts: $\\mathbf{Y} = (Y_{1},\\dots,Y_{N})^{\\top}$.\n-   KK transform matrix operator: $H \\in \\mathbb{R}^{N \\times N}$.\n-   High-frequency intercept: $Z_{\\infty} \\in \\mathbb{R}$.\n-   Offset vector: $\\mathbf{c} = Z_{\\infty} \\mathbf{1}$, where $\\mathbf{1} \\in \\mathbb{R}^{N}$ is a vector of ones.\n-   Noise model for real parts: $\\mathbf{X} = \\mathbf{X}_{0} + \\boldsymbol{\\varepsilon}_{X}$, with $\\boldsymbol{\\varepsilon}_{X} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_{X})$.\n-   Noise model for imaginary parts: $\\mathbf{Y} = \\mathbf{Y}_{0} + \\boldsymbol{\\varepsilon}_{Y}$, with $\\boldsymbol{\\varepsilon}_{Y} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_{Y})$.\n-   Noise independence: $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_{X}, \\boldsymbol{\\varepsilon}_{Y}) = \\mathbf{0}$.\n-   True signal KK-consistency condition: $\\mathbf{X}_{0} = H \\mathbf{Y}_{0} + \\mathbf{c}$.\n-   Residual vector definition: $\\Delta \\mathbf{Z} \\equiv \\mathbf{X} - (H \\mathbf{Y} + \\mathbf{c})$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly rooted in the standard practice of EIS data validation using KK transforms. The assumptions of linearity, stability, causality, and time-invariance are the fundamental prerequisites for the KK relations to apply. The use of a multivariate Gaussian model for measurement noise is a common and appropriate statistical assumption in signal processing. The problem is scientifically sound.\n-   **Well-Posed:** The problem provides a complete set of definitions and assumptions and asks for a specific derivation. The path to the solution involves standard manipulations of multivariate random variables and their probability distributions. A unique and meaningful solution exists.\n-   **Objective:** The problem is stated in precise, formal mathematical and scientific terms, free of ambiguity or subjectivity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-posed, scientifically grounded question in computational electrochemistry and applied statistics. I will now proceed with the solution.\n\n### Derivation\n\nThe derivation proceeds in three parts as requested by the problem statement.\n\n**Part 1: Covariance Matrix of the Residual Vector $\\Delta \\mathbf{Z}$**\n\nThe residual vector $\\Delta \\mathbf{Z}$ is defined as the difference between the measured real part and the real part predicted from the measured imaginary part via the KK transform:\n$$\n\\Delta \\mathbf{Z} \\equiv \\mathbf{X} - (H \\mathbf{Y} + \\mathbf{c})\n$$\nWe substitute the given noise models for $\\mathbf{X}$ and $\\mathbf{Y}$:\n$$\n\\Delta \\mathbf{Z} = (\\mathbf{X}_{0} + \\boldsymbol{\\varepsilon}_{X}) - (H (\\mathbf{Y}_{0} + \\boldsymbol{\\varepsilon}_{Y}) + \\mathbf{c})\n$$\nRearranging the terms to group the true signals and the noise terms separately:\n$$\n\\Delta \\mathbf{Z} = (\\mathbf{X}_{0} - H \\mathbf{Y}_{0} - \\mathbf{c}) + (\\boldsymbol{\\varepsilon}_{X} - H \\boldsymbol{\\varepsilon}_{Y})\n$$\nThe problem states that the true, noise-free signals $\\mathbf{X}_{0}$ and $\\mathbf{Y}_{0}$ are KK-consistent, meaning they satisfy the relation $\\mathbf{X}_{0} = H \\mathbf{Y}_{0} + \\mathbf{c}$. Therefore, the first term in the expression for $\\Delta \\mathbf{Z}$ is the zero vector:\n$$\n\\mathbf{X}_{0} - H \\mathbf{Y}_{0} - \\mathbf{c} = \\mathbf{0}\n$$\nThis simplifies the residual vector to a linear combination of the noise terms:\n$$\n\\Delta \\mathbf{Z} = \\boldsymbol{\\varepsilon}_{X} - H \\boldsymbol{\\varepsilon}_{Y}\n$$\nThe residual vector $\\Delta \\mathbf{Z}$ is a random variable. We first determine its mean, $\\mathbb{E}[\\Delta \\mathbf{Z}]$. Using the linearity of the expectation operator:\n$$\n\\mathbb{E}[\\Delta \\mathbf{Z}] = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{X} - H \\boldsymbol{\\varepsilon}_{Y}] = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{X}] - H \\mathbb{E}[\\boldsymbol{\\varepsilon}_{Y}]\n$$\nGiven that the noise processes are zero-mean, $\\mathbb{E}[\\boldsymbol{\\varepsilon}_{X}] = \\mathbf{0}$ and $\\mathbb{E}[\\boldsymbol{\\varepsilon}_{Y}] = \\mathbf{0}$, we find that the residual vector is also zero-mean under the null hypothesis of KK-consistency:\n$$\n\\mathbb{E}[\\Delta \\mathbf{Z}] = \\mathbf{0} - H \\mathbf{0} = \\mathbf{0}\n$$\nNext, we derive the covariance matrix of $\\Delta \\mathbf{Z}$, denoted $\\Sigma_{\\Delta Z}$. By definition, for a zero-mean random vector, $\\operatorname{Cov}(\\mathbf{z}) = \\mathbb{E}[\\mathbf{z}\\mathbf{z}^{\\top}]$.\n$$\n\\Sigma_{\\Delta Z} = \\operatorname{Cov}(\\Delta \\mathbf{Z}) = \\mathbb{E}[\\Delta \\mathbf{Z} \\Delta \\mathbf{Z}^{\\top}] = \\mathbb{E}[(\\boldsymbol{\\varepsilon}_{X} - H \\boldsymbol{\\varepsilon}_{Y})(\\boldsymbol{\\varepsilon}_{X} - H \\boldsymbol{\\varepsilon}_{Y})^{\\top}]\n$$\nExpanding the product:\n$$\n\\Sigma_{\\Delta Z} = \\mathbb{E}[(\\boldsymbol{\\varepsilon}_{X} - H \\boldsymbol{\\varepsilon}_{Y})(\\boldsymbol{\\varepsilon}_{X}^{\\top} - \\boldsymbol{\\varepsilon}_{Y}^{\\top}H^{\\top})] = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{X}\\boldsymbol{\\varepsilon}_{X}^{\\top} - \\boldsymbol{\\varepsilon}_{X}\\boldsymbol{\\varepsilon}_{Y}^{\\top}H^{\\top} - H\\boldsymbol{\\varepsilon}_{Y}\\boldsymbol{\\varepsilon}_{X}^{\\top} + H\\boldsymbol{\\varepsilon}_{Y}\\boldsymbol{\\varepsilon}_{Y}^{\\top}H^{\\top}]\n$$\nApplying the linearity of expectation:\n$$\n\\Sigma_{\\Delta Z} = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{X}\\boldsymbol{\\varepsilon}_{X}^{\\top}] - \\mathbb{E}[\\boldsymbol{\\varepsilon}_{X}\\boldsymbol{\\varepsilon}_{Y}^{\\top}]H^{\\top} - H\\mathbb{E}[\\boldsymbol{\\varepsilon}_{Y}\\boldsymbol{\\varepsilon}_{X}^{\\top}] + H\\mathbb{E}[\\boldsymbol{\\varepsilon}_{Y}\\boldsymbol{\\varepsilon}_{Y}^{\\top}]H^{\\top}\n$$\nWe now use the given covariance properties:\n-   $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_{X}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{X}\\boldsymbol{\\varepsilon}_{X}^{\\top}] = \\Sigma_{X}$\n-   $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_{Y}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{Y}\\boldsymbol{\\varepsilon}_{Y}^{\\top}] = \\Sigma_{Y}$\n-   $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_{X}, \\boldsymbol{\\varepsilon}_{Y}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{X}\\boldsymbol{\\varepsilon}_{Y}^{\\top}] = \\mathbf{0}$ (since the noise is independent between channels)\n-   $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_{Y}, \\boldsymbol{\\varepsilon}_{X}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}_{Y}\\boldsymbol{\\varepsilon}_{X}^{\\top}] = \\operatorname{Cov}(\\boldsymbol{\\varepsilon}_{X}, \\boldsymbol{\\varepsilon}_{Y})^{\\top} = \\mathbf{0}^{\\top} = \\mathbf{0}$\n\nSubstituting these into the expression for $\\Sigma_{\\Delta Z}$:\n$$\n\\Sigma_{\\Delta Z} = \\Sigma_{X} - \\mathbf{0} \\cdot H^{\\top} - H \\cdot \\mathbf{0} + H \\Sigma_{Y} H^{\\top}\n$$\nThus, the covariance matrix of the residual vector is:\n$$\n\\Sigma_{\\Delta Z} = \\Sigma_{X} + H \\Sigma_{Y} H^{\\top}\n$$\nSo, under the null hypothesis that the data is KK-consistent, the residual vector follows a multivariate normal distribution $\\Delta \\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_{X} + H \\Sigma_{Y} H^{\\top})$.\n\n**Part 2: Normalized Residual Vector $\\mathbf{r}$**\n\nA normalized random vector $\\mathbf{r}$ (also known as a whitened vector) is one with a zero mean and an identity covariance matrix, $\\operatorname{Cov}(\\mathbf{r}) = I$. We can obtain such a vector from $\\Delta \\mathbf{Z}$ by applying a linear transformation that \"removes\" the correlation structure. Let $\\Sigma_{\\Delta Z} = L L^{\\top}$ be the Cholesky decomposition of the covariance matrix $\\Sigma_{\\Delta Z}$, where $L$ is a lower triangular matrix. The normalized residual vector $\\mathbf{r}$ is then defined as:\n$$\n\\mathbf{r} = L^{-1} \\Delta \\mathbf{Z}\n$$\nWe can verify that its covariance is the identity matrix $I$:\n$$\n\\operatorname{Cov}(\\mathbf{r}) = \\operatorname{Cov}(L^{-1} \\Delta \\mathbf{Z}) = L^{-1} \\operatorname{Cov}(\\Delta \\mathbf{Z}) (L^{-1})^{\\top} = L^{-1} (\\Sigma_{\\Delta Z}) (L^{\\top})^{-1} = L^{-1} (L L^{\\top}) (L^{\\top})^{-1} = (L^{-1}L)(L^{\\top}(L^{\\top})^{-1}) = I \\cdot I = I\n$$\nSince $\\Delta \\mathbf{Z}$ is Gaussian, $\\mathbf{r}$ is also Gaussian. Specifically, $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, I)$. Its components are independent standard normal variables.\n\n**Part 3: Chi-Square Goodness-of-Fit Statistic**\n\nThe chi-square ($\\chi^2$) distribution arises from the sum of squares of independent standard normal random variables. The components of the normalized residual vector $\\mathbf{r} = (r_1, \\dots, r_N)^{\\top}$ are independent standard normal variables. Therefore, the sum of their squares follows a $\\chi^2$ distribution with $N$ degrees of freedom:\n$$\n\\chi^2 = \\sum_{i=1}^{N} r_i^2 = \\mathbf{r}^{\\top}\\mathbf{r}\n$$\nThis statistic is dimensionless and, by construction, is additive in the squared components of the normalized residual vector. To express this statistic in terms of the original residual vector $\\Delta \\mathbf{Z}$ and its covariance matrix, we substitute the definition of $\\mathbf{r}$:\n$$\n\\chi^2 = (L^{-1} \\Delta \\mathbf{Z})^{\\top} (L^{-1} \\Delta \\mathbf{Z}) = \\Delta \\mathbf{Z}^{\\top} (L^{-1})^{\\top} L^{-1} \\Delta \\mathbf{Z}\n$$\nFrom the property of matrix inverses, $(L^{-1})^{\\top} = (L^{\\top})^{-1}$. Also, from the Cholesky decomposition, $\\Sigma_{\\Delta Z}^{-1} = (L L^{\\top})^{-1} = (L^{\\top})^{-1} L^{-1}$. Therefore, we can identify the central term as the inverse of the covariance matrix:\n$$\n\\chi^2 = \\Delta \\mathbf{Z}^{\\top} \\Sigma_{\\Delta Z}^{-1} \\Delta \\mathbf{Z}\n$$\nThis expression is known as the squared Mahalanobis distance of the vector $\\Delta \\mathbf{Z}$ from its mean ($\\mathbf{0}$). It is the quantity in the exponent of the multivariate Gaussian probability density function for $\\Delta \\mathbf{Z}$:\n$$\np(\\Delta \\mathbf{Z}) \\propto \\exp\\left(-\\frac{1}{2}\\Delta \\mathbf{Z}^{\\top}\\Sigma_{\\Delta Z}^{-1}\\Delta \\mathbf{Z}\\right) = \\exp\\left(-\\frac{\\chi^2}{2}\\right)\n$$\nMaximizing the likelihood of observing the data is equivalent to minimizing this $\\chi^2$ value, which forms the basis for least-squares fitting and goodness-of-fit testing.\n\nFinally, we substitute the expression for $\\Sigma_{\\Delta Z}$ from Part 1 to obtain the chi-square statistic in terms of the quantities specified in the problem statement:\n$$\n\\chi^2 = \\Delta \\mathbf{Z}^{\\top} (\\Sigma_{X} + H \\Sigma_{Y} H^{\\top})^{-1} \\Delta \\mathbf{Z}\n$$\nThis is the required goodness-of-fit statistic. It is a scalar, dimensionless quantity that aggregates the deviations from KK-consistency across all measured frequencies, properly weighted by their uncertainties and correlations.",
            "answer": "$$\\boxed{\\Delta \\mathbf{Z}^{\\top} (\\Sigma_{X} + H \\Sigma_{Y} H^{\\top})^{-1} \\Delta \\mathbf{Z}}$$"
        },
        {
            "introduction": "The final stage of mastering the KK transform is to implement it computationally, enabling the validation of any given experimental dataset. This exercise  guides you through the process of developing a numerical routine to compute the KK transform, from handling the Cauchy Principal Value singularity to mapping the infinite integration domain. By writing and testing this code against known analytical models, you will gain the practical skills needed to apply KK validation in your own research.",
            "id": "4249776",
            "problem": "You are tasked with implementing a computational Electrochemical Impedance Spectroscopy (EIS) validation routine based on Kramers-Kronig (KK) transforms. In a causal linear time-invariant electrochemical system, the complex impedance $Z(\\omega)$, with angular frequency $\\omega$ in $\\mathrm{rad}/\\mathrm{s}$, has real and imaginary parts related by a Hilbert-transform-type relation derived from analyticity in the upper half-plane. Assume a physically realistic impedance $Z(\\omega)$ with a finite high-frequency limit $Z(\\infty)$ and an imaginary part $\\operatorname{Im}Z(\\omega)$ that is known and sufficiently decays at high $\\omega$.\n\nStarting from the principles of causality and the analyticity of $Z(\\omega)$ for $\\operatorname{Im}\\omega  0$, derive the appropriate Cauchy Principal Value (CPV) integral that connects $\\operatorname{Re}Z(\\omega)$ to $\\operatorname{Im}Z(\\omega)$ restricted to positive frequencies, and implement a numerical algorithm to compute $\\operatorname{Re}Z_{\\text{KK}}(\\omega)$ from $\\operatorname{Im}Z(\\omega)$ using adaptive Gauss-Kronrod quadrature with explicit principal value handling. Your implementation must:\n- Work on the semi-infinite domain $\\omega \\in [0,\\infty)$ by mapping to a finite interval through a smooth bijection.\n- Handle the CPV singularity at the pole frequency where the denominator of the kernel becomes zero.\n- Be robust across a range of $\\omega$ including small and large values.\n- Produce results expressed in $\\Omega$ (ohms) as floating-point numbers.\n\nDefine the transformation and algorithm rigorously in your solution. Then write a program that carries out the computation for the following test suite. For each case, evaluate $\\operatorname{Re}Z_{\\text{KK}}(\\omega)$ on the frequency grid $\\omega \\in \\{0.01, 0.1, 1, 10, 100\\}\\,\\mathrm{rad}/\\mathrm{s}$ and compare against the known closed-form $\\operatorname{Re}Z(\\omega)$ for that model. Report, for each test case, the maximum absolute deviation in $\\Omega$ across the grid as a floating-point number.\n\nTest suite:\n- Case $1$ (Debye element without series resistance): $R = 5\\,\\Omega$, $\\tau = 0.2\\,\\mathrm{s}$, $Z(\\infty) = 0\\,\\Omega$. The imaginary part is $\\operatorname{Im}Z(\\omega) = -\\dfrac{R\\,\\omega\\,\\tau}{1 + (\\omega \\tau)^2}$ and the true real part is $\\operatorname{Re}Z(\\omega) = \\dfrac{R}{1 + (\\omega \\tau)^2}$.\n- Case $2$ (Debye element with series resistance): $R_s = 2\\,\\Omega$, $R_p = 8\\,\\Omega$, $\\tau = 0.05\\,\\mathrm{s}$, $Z(\\infty) = R_s$. The imaginary part is $\\operatorname{Im}Z(\\omega) = -\\dfrac{R_p\\,\\omega\\,\\tau}{1 + (\\omega \\tau)^2}$ and the true real part is $\\operatorname{Re}Z(\\omega) = R_s + \\dfrac{R_p}{1 + (\\omega \\tau)^2}$.\n- Case $3$ (Zero imaginary part): $\\operatorname{Im}Z(\\omega) = 0$, $Z(\\infty) = 3\\,\\Omega$. The true real part is $\\operatorname{Re}Z(\\omega) = 3\\,\\Omega$ for all $\\omega$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result1},\\text{result2},\\text{result3}]$), where each entry is the maximum absolute deviation in $\\Omega$ for the corresponding test case evaluated on the specified frequency grid. All computations must be performed in the International System of Units (SI), with angular frequency in $\\mathrm{rad}/\\mathrm{s}$ and impedance in $\\Omega$.",
            "solution": "The problem requires the derivation and implementation of a Kramers-Kronig (KK) transform to calculate the real part of an electrochemical impedance, $\\operatorname{Re}Z(\\omega)$, from its imaginary part, $\\operatorname{Im}Z(\\omega)$, and its high-frequency limit, $Z(\\infty)$. This serves as a method for validating the consistency of Electrochemical Impedance Spectroscopy (EIS) data.\n\nThe foundation of the Kramers-Kronig relations lies in the principle of causality. For a linear time-invariant (LTI) system, the output cannot precede the input. In the frequency domain, this property implies that the system's transfer function—in this case, the complex impedance $Z(\\omega)$—must be an analytic function in the upper half of the complex frequency plane ($\\operatorname{Im}(\\omega)  0$). Furthermore, for a real physical system, the impulse response function must be real-valued, which implies that $Z(-\\omega) = Z^*(\\omega)$, where $Z^*$ is the complex conjugate. This symmetry means that $\\operatorname{Re}Z(\\omega)$ is an even function of frequency, and $\\operatorname{Im}Z(\\omega)$ is an odd function.\n\nWe consider the function $f(\\zeta) = Z(\\zeta) - Z(\\infty)$, which is analytic in the upper half-plane and vanishes as $|\\zeta| \\to \\infty$. By applying Cauchy's integral theorem, we can relate the value of this function at a point on the real axis, $\\omega$, to an integral over the real axis. The Sokhotski–Plemelj theorem provides the following relation for a function $F(x)$ and a point $\\omega$ on the real axis:\n$$ \\lim_{\\epsilon \\to 0^+} \\int_{-\\infty}^{\\infty} \\frac{F(x)}{x - (\\omega + i\\epsilon)} dx = i\\pi F(\\omega) + P.V. \\int_{-\\infty}^{\\infty} \\frac{F(x)}{x-\\omega} dx $$\nFrom the analyticity of $Z(\\zeta) - Z(\\infty)$, we can write:\n$$ Z(\\omega) - Z(\\infty) = \\frac{1}{2\\pi i} \\left( P.V.\\int_{-\\infty}^{\\infty} \\frac{Z(x) - Z(\\infty)}{x - \\omega} dx + i\\pi (Z(\\omega) - Z(\\infty)) \\right) $$\nThis simplifies to:\n$$ i\\pi (Z(\\omega) - Z(\\infty)) = P.V. \\int_{-\\infty}^{\\infty} \\frac{Z(x) - Z(\\infty)}{x - \\omega} dx $$\nwhere $P.V.$ denotes the Cauchy Principal Value of the integral. Expanding $Z(\\omega) = \\operatorname{Re}Z(\\omega) + i\\operatorname{Im}Z(\\omega)$ and equating the imaginary parts of both sides yields:\n$$ \\pi (\\operatorname{Re}Z(\\omega) - Z(\\infty)) = P.V. \\int_{-\\infty}^{\\infty} \\frac{\\operatorname{Im}Z(x)}{x - \\omega} dx $$\nNote that $\\operatorname{Im}Z(\\infty) = 0$ is assumed for a resistive high-frequency limit. This gives one of the Kramers-Kronig relations:\n$$ \\operatorname{Re}Z(\\omega) - Z(\\infty) = \\frac{1}{\\pi} P.V. \\int_{-\\infty}^{\\infty} \\frac{\\operatorname{Im}Z(x)}{x - \\omega} dx $$\nUsing the odd symmetry of $\\operatorname{Im}Z(x)$, i.e., $\\operatorname{Im}Z(-x) = -\\operatorname{Im}Z(x)$, we can transform the integral over $(-\\infty, \\infty)$ to an integral over $(0, \\infty)$:\n$$ \\int_{-\\infty}^{\\infty} \\frac{\\operatorname{Im}Z(x)}{x - \\omega} dx = \\int_{0}^{\\infty} \\operatorname{Im}Z(x) \\left( \\frac{1}{x - \\omega} + \\frac{1}{x + \\omega} \\right) dx = \\int_{0}^{\\infty} \\frac{2x \\operatorname{Im}Z(x)}{x^2 - \\omega^2} dx $$\nSubstituting this back, we obtain the required KK transform for the real part of the impedance:\n$$ \\operatorname{Re}Z(\\omega) = Z(\\infty) + \\frac{2}{\\pi} P.V. \\int_{0}^{\\infty} \\frac{x \\operatorname{Im}Z(x)}{x^2 - \\omega^2} dx $$\nThis integral is singular at $x=\\omega$ and must be evaluated as a Cauchy Principal Value. For numerical computation, the singularity can be handled by subtracting it out. We rewrite the integrand as:\n$$ \\frac{x \\operatorname{Im}Z(x)}{x^2 - \\omega^2} = \\frac{x \\operatorname{Im}Z(x) - \\omega \\operatorname{Im}Z(\\omega)}{x^2 - \\omega^2} + \\frac{\\omega \\operatorname{Im}Z(\\omega)}{x^2 - \\omega^2} $$\nThe first term is now regular at $x=\\omega$, having a finite limit, and can be integrated numerically. The second term, $\\frac{\\omega \\operatorname{Im}Z(\\omega)}{x^2 - \\omega^2}$, has a known principal value integral: $P.V. \\int_0^\\infty \\frac{dx}{x^2-\\omega^2} = 0$. Therefore, the transform simplifies to an ordinary improper integral:\n$$ \\operatorname{Re}Z(\\omega) = Z(\\infty) + \\frac{2}{\\pi} \\int_{0}^{\\infty} \\frac{x \\operatorname{Im}Z(x) - \\omega \\operatorname{Im}Z(\\omega)}{x^2 - \\omega^2} dx $$\nTo implement this numerically, the problem asks to map the semi-infinite integration domain $[0, \\infty)$ to a finite one. A suitable smooth bijection is the substitution $x = \\frac{u}{1-u}$, for which $dx = \\frac{1}{(1-u)^2} du$. As $x$ goes from $0$ to $\\infty$, $u$ goes from $0$ to $1$. The integral becomes:\n$$ \\int_{0}^{1} \\left( \\frac{\\frac{u}{1-u} \\operatorname{Im}Z(\\frac{u}{1-u}) - \\omega \\operatorname{Im}Z(\\omega)}{(\\frac{u}{1-u})^2 - \\omega^2} \\right) \\frac{1}{(1-u)^2} du $$\nSimplifying the denominator gives:\n$$ (\\frac{u}{1-u})^2 - \\omega^2 = \\frac{u^2 - \\omega^2(1-u)^2}{(1-u)^2} $$\nThe final integrand over the finite domain $u \\in [0,1]$ is:\n$$ I_u(u, \\omega) = \\frac{\\frac{u}{1-u} \\operatorname{Im}Z(\\frac{u}{1-u}) - \\omega \\operatorname{Im}Z(\\omega)}{u^2 - \\omega^2(1-u)^2} $$\nThis expression has a removable singularity at $u_{crit} = \\frac{\\omega}{1+\\omega}$ (which corresponds to $x=\\omega$) and is suitable for robust numerical quadrature, such as adaptive Gauss-Kronrod, as implemented in `scipy.integrate.quad`.\n\nThe algorithm proceeds as follows for each test case:\n1.  Define the function for the known imaginary impedance component, $\\operatorname{Im}Z(x)$.\n2.  Define the integrand $I_u(u, \\omega)$ using the expression above.\n3.  For each frequency $\\omega_k$ in the evaluation grid, compute the integral $\\int_0^1 I_u(u, \\omega_k) du$ using adaptive quadrature.\n4.  Calculate the Kramers-Kronig estimate $\\operatorname{Re}Z_{\\text{KK}}(\\omega_k) = Z(\\infty) + \\frac{2}{\\pi} \\int_0^1 I_u(u, \\omega_k) du$.\n5.  Compare $\\operatorname{Re}Z_{\\text{KK}}(\\omega_k)$ with the known analytical value $\\operatorname{Re}Z(\\omega_k)$.\n6.  The result for the test case is the maximum absolute deviation $|\\operatorname{Re}Z_{\\text{KK}}(\\omega_k) - \\operatorname{Re}Z(\\omega_k)|$ over the entire frequency grid.\nThis procedure will be applied to the three specified test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy import integrate\n\ndef solve():\n    \"\"\"\n    Solves the Kramers-Kronig transform problem for a suite of EIS test cases.\n\n    The core of the solution is the `kramers_kronig_transform` function, which\n    implements the numerical evaluation of the KK transform based on the\n    principles derived in the solution description.\n    \"\"\"\n\n    omega_grid = np.array([0.01, 0.1, 1, 10, 100])\n\n    # Test Case 1: Debye element without series resistance\n    case1_params = {'R': 5.0, 'tau': 0.2, 'Z_inf': 0.0}\n    def im_Z_case1(w, p):\n        return - (p['R'] * w * p['tau']) / (1 + (w * p['tau'])**2)\n    def re_Z_true_case1(w, p):\n        return p['R'] / (1 + (w * p['tau'])**2)\n\n    # Test Case 2: Debye element with series resistance\n    case2_params = {'Rs': 2.0, 'Rp': 8.0, 'tau': 0.05, 'Z_inf': 2.0}\n    def im_Z_case2(w, p):\n        return - (p['Rp'] * w * p['tau']) / (1 + (w * p['tau'])**2)\n    def re_Z_true_case2(w, p):\n        return p['Rs'] + p['Rp'] / (1 + (w * p['tau'])**2)\n\n    # Test Case 3: Purely resistive element (zero imaginary part)\n    case3_params = {'R': 3.0, 'Z_inf': 3.0}\n    def im_Z_case3(w, p):\n        return 0.0\n    def re_Z_true_case3(w, p):\n        return p['R']\n\n    test_cases = [\n        (case1_params, im_Z_case1, re_Z_true_case1),\n        (case2_params, im_Z_case2, re_Z_true_case2),\n        (case3_params, im_Z_case3, re_Z_true_case3),\n    ]\n\n    def kramers_kronig_transform(im_Z_func, params, omega):\n        \"\"\"\n        Calculates the real part of the impedance at a given frequency omega\n        using the Kramers-Kronig transform of the imaginary part.\n        \"\"\"\n        Z_inf = params.get('Z_inf', 0.0)\n\n        # The imaginary part at the target frequency omega.\n        im_Z_at_omega = im_Z_func(omega, params)\n\n        def integrand_u(u, w):\n            \"\"\"\n            Defines the integrand for the KK transform after a change of\n            variables x = u / (1 - u) to map the integration domain to [0, 1].\n            This form handles the Cauchy Principal Value by using a subtraction\n            technique, resulting in a regular integral.\n            \"\"\"\n            # Avoid division by zero at u=1, which corresponds to x=inf.\n            if np.isclose(u, 1.0):\n                return 0.0\n\n            x = u / (1.0 - u)\n            \n            numerator = x * im_Z_func(x, params) - w * im_Z_at_omega\n            \n            # The denominator after the change of variables u = x / (1+x)\n            den_u = u**2 - w**2 * (1-u)**2\n            if np.isclose(den_u, 0.0):\n                # This corresponds to the removable singularity at x=w.\n                # A robust quadrature like `quad` can often handle this if the\n                # function is defined to be continuous. We can return 0.0\n                # as the adaptive algorithm will effectively ignore this point.\n                return 0.0\n\n            return numerator / den_u\n\n\n        # Integrate using adaptive Gauss-Kronrod quadrature on the finite interval [0, 1].\n        integral_val, _ = integrate.quad(integrand_u, 0, 1, args=(omega,))\n        \n        # Apply the full Kramers-Kronig formula.\n        re_Z_kk = Z_inf + (2.0 / np.pi) * integral_val\n        return re_Z_kk\n\n    results = []\n    for params, im_func, re_true_func in test_cases:\n        deviations = []\n        for w in omega_grid:\n            re_kk = kramers_kronig_transform(im_func, params, w)\n            re_true = re_true_func(w, params)\n            deviations.append(np.abs(re_kk - re_true))\n        \n        max_deviation = np.max(deviations)\n        results.append(max_deviation)\n\n    print(f\"[{','.join(f'{r:.12g}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}