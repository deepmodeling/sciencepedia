{
    "hands_on_practices": [
        {
            "introduction": "To effectively choose between direct and iterative solvers, we must first quantify the cost of direct methods. This exercise guides you through modeling the memory requirements of a sparse direct solver by analyzing the fill-in generated during factorization. By applying the planar separator theorem to a Nested Dissection ordering, you will derive the classic complexity result for 2D problems, building a first-principles intuition for how solver cost scales with problem size .",
            "id": "4137399",
            "problem": "Consider the steady-state linear system arising in computational acoustics from a second-order finite-difference discretization of the two-dimensional scalar acoustic Helmholtz operator on a square domain with absorbing boundary conditions chosen so that the resulting sparse matrix is strictly diagonally dominant. The nonzero pattern of the coefficient matrix is identical to that of the standard five-point stencil on an $m \\times m$ structured grid, with $n = m^{2}$ interior unknowns. Assume an ordering based on Nested Dissection (ND) and a direct multifrontal $\\mathrm{LU}$ factorization. Using the planar separator theorem and the fact that elimination along ND produces dense Schur complements on separators, construct a simple, level-wise model for the predicted number of structurally nonzero entries in $L+U$ as a function of $n$ for $m = 2^{k}$, where $k \\in \\mathbb{N}$. You may assume that, at each ND level, the dominant storage contribution is proportional to the sum of squared separator sizes across all separators at that level, and that these separators alternate orientation and halve in length every other level. Derive a closed-form analytic expression for the leading-order prediction in terms of $n$ and the base-$2$ logarithm. Then, validate the model qualitatively against the $m=4$ grid by enumerating separator sizes and counts and discussing whether lower-order terms are negligible at that mesh size. Express the final model prediction as a single closed-form expression. No rounding is required, and no physical units apply.",
            "solution": "The problem requires the derivation of a model for the number of structurally nonzero entries in the $L+U$ factors of a sparse matrix arising from the finite-difference discretization of the 2D Helmholtz equation on a square grid. The model is to be based on a Nested Dissection (ND) ordering and a multifrontal factorization method. The fundamental assumption is that the fill-in is dominated by the storage required for the dense frontal matrices (Schur complements) associated with the separators at each level of the dissection. This storage is assumed to be proportional to the square of the separator size.\n\nLet the structured grid have $m \\times m$ interior nodes, where $m = 2^k$ for some integer $k \\in \\mathbb{N}$. The total number of unknowns is $n = m^2$. The ND process recursively bisects the grid. According to the problem statement, the separators alternate in orientation and their length is halved at every other level. We can model this process by indexing the levels of dissection starting from $j=0$ for the top-level separator.\n\nAt level $j$, let $N_j$ be the number of separators and $S_j$ be the size (number of nodes) of each separator.\nThe top-level separator ($j=0$) splits the $m \\times m$ grid. Let's assume it is a vertical separator. It will have size $S_0 = m$. There is $N_0 = 1 = 2^0$ such separator. This dissection creates two subdomains.\n\nAt the next level ($j=1$), each of the two subdomains is bisected. The problem states the orientation alternates, so these will be horizontal separators. The length of a subdomain is now approximately $m/2$. Thus, the separator size is $S_1 = m/2$. There are $N_1 = 2 = 2^1$ such separators. This creates four square-like subdomains.\n\nAt level $j=2$, we bisect the four new subdomains with vertical separators. The problem states the length halves every *other* level. The length at levels $j=0$ and $j=1$ were $m$ and $m/2$, respectively. Following the pattern, the length at level $j=2$ should be the same as at level $j=1$, which is $m/2$. Hence, $S_2 = m/2$. There are $N_2 = 4 = 2^2$ such separators.\n\nAt level $j=3$, we use horizontal separators. The length now halves again, so $S_3 = (m/2)/2 = m/4$. There are $N_3 = 8 = 2^3$ such separators.\n\nWe can generalize this pattern. The number of separators at level $j$ is $N_j = 2^j$. The size of a separator at level $j$ is given by $S_j = m \\cdot 2^{-\\lceil j/2 \\rceil}$. An equivalent expression is $S_j = m \\cdot 2^{-\\lfloor(j+1)/2\\rfloor}$. Let's verify this:\n- $j=0: S_0 = m \\cdot 2^{-\\lfloor 1/2 \\rfloor} = m \\cdot 2^0 = m$. Correct.\n- $j=1: S_1 = m \\cdot 2^{-\\lfloor 2/2 \\rfloor} = m \\cdot 2^{-1} = m/2$. Correct.\n- $j=2: S_2 = m \\cdot 2^{-\\lfloor 3/2 \\rfloor} = m \\cdot 2^{-1} = m/2$. Correct.\n- $j=3: S_3 = m \\cdot 2^{-\\lfloor 4/2 \\rfloor} = m \\cdot 2^{-2} = m/4$. Correct.\n\nThe recursion terminates when the subdomains are trivial. Since the original grid side length is $m=2^k$, and the side length is halved every two levels of dissection, the process will take $k$ pairs of levels. The levels will run from $j=0$ to a maximum where the separator size becomes $1$. This happens when $S_j \\approx 1$, so $m \\cdot 2^{-\\lfloor(j+1)/2\\rfloor} \\approx 1$, which gives $2^k \\approx 2^{\\lfloor(j+1)/2\\rfloor}$, or $k \\approx \\lfloor(j+1)/2\\rfloor$. For $j=2k-1$, $\\lfloor(2k-1+1)/2\\rfloor = k$, so $S_{2k-1}=1$. The range of dissection levels is $j = 0, 1, \\dots, 2k-1$.\n\nThe model for the total number of nonzero entries in $L+U$, which we denote $F$, is the sum of the squares of the separator sizes over all separators at all levels.\n$$F = \\sum_{j=0}^{2k-1} N_j S_j^2$$\nSubstituting the expressions for $N_j$ and $S_j$:\n$$F = \\sum_{j=0}^{2k-1} 2^j \\left( m \\cdot 2^{-\\lfloor(j+1)/2\\rfloor} \\right)^2 = m^2 \\sum_{j=0}^{2k-1} 2^j \\cdot 2^{-2\\lfloor(j+1)/2\\rfloor} = n \\sum_{j=0}^{2k-1} 2^{j - 2\\lfloor(j+1)/2\\rfloor}$$\nWe analyze the exponent $j - 2\\lfloor(j+1)/2\\rfloor$ for even and odd $j$.\n- If $j$ is even, let $j=2p$ for $p \\in \\{0, 1, \\dots, k-1\\}$. The exponent is $2p - 2\\lfloor(2p+1)/2\\rfloor = 2p - 2p = 0$. The term is $2^0=1$.\n- If $j$ is odd, let $j=2p-1$ for $p \\in \\{1, 2, \\dots, k\\}$. The exponent is $(2p-1) - 2\\lfloor(2p-1+1)/2\\rfloor = (2p-1) - 2p = -1$. The term is $2^{-1}=1/2$.\n\nThe total summation can be split into sums over even and odd $j$:\n$$F = n \\left( \\sum_{p=0}^{k-1} 1 + \\sum_{p=1}^{k} \\frac{1}{2} \\right)$$\nThe first sum has $k$ terms, each equal to $1$. The second sum has $k$ terms, each equal to $1/2$.\n$$F = n \\left( k \\cdot 1 + k \\cdot \\frac{1}{2} \\right) = n \\left(k + \\frac{k}{2}\\right) = \\frac{3}{2} k n$$\nTo get the final expression in terms of $n$, we relate $k$ to $n$. We have $n = m^2 = (2^k)^2 = 2^{2k}$. Taking the base-$2$ logarithm of both sides gives $\\log_2(n) = 2k$, so $k = \\frac{1}{2}\\log_2(n)$.\nSubstituting this into the expression for $F$:\n$$F(n) = \\frac{3}{2} n \\left(\\frac{1}{2}\\log_2(n)\\right) = \\frac{3}{4} n \\log_2(n)$$\nThis is the derived closed-form analytic expression for the leading-order prediction.\n\nNext, we validate this model qualitatively for the $m=4$ grid.\nFor $m=4$, we have $k=2$ and $n=m^2=16$. The levels of dissection run from $j=0$ to $j=2k-1=3$.\nLet's enumerate the separator properties for each level:\n- Level $j=0$: $N_0 = 2^0 = 1$, $S_0 = 4 \\cdot 2^{-\\lfloor 1/2 \\rfloor} = 4$. Contribution to $F$: $N_0 S_0^2 = 1 \\cdot 4^2 = 16$.\n- Level $j=1$: $N_1 = 2^1 = 2$, $S_1 = 4 \\cdot 2^{-\\lfloor 2/2 \\rfloor} = 2$. Contribution to $F$: $N_1 S_1^2 = 2 \\cdot 2^2 = 8$.\n- Level $j=2$: $N_2 = 2^2 = 4$, $S_2 = 4 \\cdot 2^{-\\lfloor 3/2 \\rfloor} = 2$. Contribution to $F$: $N_2 S_2^2 = 4 \\cdot 2^2 = 16$.\n- Level $j=3$: $N_3 = 2^3 = 8$, $S_3 = 4 \\cdot 2^{-\\lfloor 4/2 \\rfloor} = 1$. Contribution to $F$: $N_3 S_3^2 = 8 \\cdot 1^2 = 8$.\n\nThe total predicted fill-in from this enumeration is $F_{m=4} = 16 + 8 + 16 + 8 = 48$.\nNow, we check this against our final formula with $n=16$:\n$$F(16) = \\frac{3}{4} \\cdot 16 \\cdot \\log_2(16) = \\frac{3}{4} \\cdot 16 \\cdot 4 = 3 \\cdot 16 = 48$$\nThe results match, confirming the consistency of the derivation.\n\nFinally, we discuss the role of lower-order terms for this small mesh size. The derived model, $F(n) = \\frac{3}{4}n \\log_2(n)$, represents the leading-order term in the complexity analysis of fill-in for ND. A more precise analysis of the fill-in for a 2D grid would yield an expression of the form $F_{\\text{true}}(n) = c_1 n \\log_2(n) + c_2 n + O(n^{1/2} \\log n)$, where the $c_1 n \\log_2(n)$ term comes from the separator interactions (as modeled here) and the $O(n)$ term, among others, comes from fill-in within the leaf-level blocks of the elimination tree and other boundary effects of the separators. Our model effectively sets $c_2$ and other lower-order coefficients to zero.\n\nFor $m=4$ ($n=16$), the factor $\\log_2(n) = \\log_2(16) = 4$ is a small number. The ratio of the leading-order term to the next-order ($O(n)$) term is proportional to $\\log_2(n)$. Since this logarithmic factor is not large, the magnitude of the leading term ($48$) is not necessarily dominant over the lower-order terms. For example, a hypothetical lower-order term of size $-2n = -32$ would change the total significantly. Thus, for a small grid such as $m=4$, the lower-order terms neglected by this simple model are likely not negligible. The model is qualitatively valuable in predicting the asymptotic growth rate but may lack quantitative accuracy for small values of $n$ where the asymptotic regime has not yet been reached.",
            "answer": "$$\\boxed{\\frac{3}{4}n\\log_{2}(n)}$$"
        },
        {
            "introduction": "Having appreciated the scaling limitations of direct solvers, we turn to iterative methods, which offer a compelling alternative for large-scale problems. This exercise challenges you to compare two canonical Krylov subspace solvers: GMRES, which guarantees optimal residual reduction at the cost of growing memory, and BiCGStab, which maintains a fixed memory footprint but has less robust convergence properties. Through a guided theoretical analysis, you will uncover the core trade-offs that govern the choice of an iterative solver in practice .",
            "id": "4137402",
            "problem": "You are solving a frequency-domain acoustic scattering problem governed by the Helmholtz equation with first-order absorbing boundary conditions. A standard finite element discretization yields a linear system $A x = b$, where $A \\in \\mathbb{C}^{n \\times n}$ is complex symmetric and indefinite, that is $A = A^{T} \\neq A^{H}$. Consider two Krylov subspace methods: Bi-Conjugate Gradient Stabilized (BiCGStab) and Generalized Minimal Residual (GMRES). Starting only from the definitions of a Krylov subspace, residuals, and orthogonality with respect to the standard conjugate inner product, perform the following reasoning steps:\n\n- For Generalized Minimal Residual (GMRES), derive the recurrence structure that arises from orthonormalizing the Krylov basis vectors generated by $A$ acting on the initial residual $r_{0} = b - A x_{0}$, and explain why at iteration $m$ it provides the approximate solution that minimizes the residual $2$-norm over the affine space $x_{0} + \\mathcal{K}_{m}(A, r_{0})$.\n\n- For Bi-Conjugate Gradient Stabilized (BiCGStab), start from the bi-Lanczos idea applied to $A$ and $A^{H}$ with a fixed shadow residual $\\hat{r}_{0}$ to enforce a bi-orthogonality condition, and derive the short recurrences for the search direction, intermediate residual, and stabilization steps that produce an updated iterate and residual.\n\nThen, based on your derivations and on fundamental properties of these methods, select all statements that are correct for the application to complex symmetric indefinite systems arising in computational acoustics:\n\nA. Bi-Conjugate Gradient Stabilized (BiCGStab) uses short recurrences and stores a fixed number of vectors, yielding memory cost $\\mathcal{O}(n)$ independent of iteration count, but it may break down when a scalar denominator in the recurrence becomes zero (for example, when a bi-orthogonality inner product vanishes or when a stabilization parameter becomes zero). Generalized Minimal Residual (GMRES) builds an orthonormal Krylov basis by an Arnoldi-type process and must store all basis vectors up to the current or restart length, with memory cost $\\mathcal{O}(m n)$ after $m$ iterations.\n\nB. For a complex symmetric matrix $A = A^{T}$, taking the Bi-Conjugate Gradient Stabilized (BiCGStab) shadow residual $\\hat{r}_{0} = r_{0}$ and using no conjugation (that is, replacing conjugate-transposes by pure transposes) preserves the formal recurrences and guarantees a monotonically decreasing residual $2$-norm.\n\nC. Full (non-restarted) Generalized Minimal Residual (GMRES) minimizes the residual $2$-norm at each iteration over the current Krylov subspace, producing a nonincreasing sequence of residual norms and experiencing no true breakdown in exact arithmetic except the so-called happy breakdown when the exact solution is reached within the Krylov subspace, at the cost of increasing storage and orthogonalization work per iteration.\n\nD. Bi-Conjugate Gradient Stabilized (BiCGStab) is guaranteed to converge in at most $n$ steps for any diagonalizable matrix $A \\in \\mathbb{C}^{n \\times n}$ in exact arithmetic.\n\nE. For complex symmetric indefinite $A$, formulating Bi-Conjugate Gradient Stabilized (BiCGStab) with the transpose bilinear form $x^{T} y$ in place of the standard conjugate inner product $x^{H} y$ eliminates breakdowns entirely in exact arithmetic.\n\nF. In high-wave-number Helmholtz problems, the nonnormality and indefiniteness of $A$ commonly lead to stagnation in restarted Generalized Minimal Residual (GMRES) and erratic convergence of Bi-Conjugate Gradient Stabilized (BiCGStab); in practice the preconditioner quality largely dictates convergence behavior for both methods.\n\nSelect all that apply. Provide no derivations in your final choice; the derivations should only inform your selection internally.",
            "solution": "The problem statement provides a valid scenario from computational acoustics and asks for an analysis of two standard Krylov subspace methods, Generalized Minimal Residual (GMRES) and Bi-Conjugate Gradient Stabilized (BiCGStab), for solving the resulting linear system. The system matrix $A$ is complex symmetric ($A=A^T$), indefinite, and non-Hermitian ($A \\neq A^H$). The problem is scientifically grounded, well-posed, and objective. We will proceed by first deriving the essential properties of GMRES and BiCGStab as requested, and then evaluating each statement.\n\n**Generalized Minimal Residual (GMRES) Derivation and Properties**\n\nThe goal of GMRES at iteration $m$ is to find an approximate solution $x_m$ from the affine Krylov subspace $x_0 + \\mathcal{K}_m(A, r_0)$ that minimizes the Euclidean norm ($2$-norm) of the corresponding residual. The affine subspace is defined as $x_0 + \\text{span}\\{r_0, Ar_0, \\ldots, A^{m-1}r_0\\}$, where $x_0$ is an initial guess and $r_0 = b - Ax_0$ is the initial residual.\n\nLet $x_m = x_0 + z_m$, where $z_m \\in \\mathcal{K}_m(A, r_0)$. The residual is $r_m = b - Ax_m = b - A(x_0 + z_m) = r_0 - Az_m$. The minimization problem is to find $z_m \\in \\mathcal{K}_m(A, r_0)$ such that $\\|r_m\\|_2 = \\|r_0 - Az_m\\|_2$ is minimized.\n\nThe Arnoldi iteration is used to construct an orthonormal basis $\\{v_1, v_2, \\ldots, v_m\\}$ for the Krylov subspace $\\mathcal{K}_m(A, r_0)$. The process starts with $v_1 = r_0 / \\|r_0\\|_2$. For $j=1, 2, \\ldots, m$, it computes:\n$$ \\hat{w} = A v_j $$\n$$ h_{i,j} = v_i^H \\hat{w} \\quad \\text{for } i=1, \\ldots, j $$\n$$ w = \\hat{w} - \\sum_{i=1}^{j} h_{i,j} v_i $$\n$$ h_{j+1,j} = \\|w\\|_2 $$\n$$ v_{j+1} = w / h_{j+1,j} $$\nThis process generates a set of orthonormal vectors $V_{m+1} = [v_1, \\ldots, v_{m+1}]$ and an $(m+1) \\times m$ upper Hessenberg matrix $\\tilde{H}_m$ with entries $h_{i,j}$. These matrices are related by the Arnoldi relation:\n$$ A V_m = V_{m+1} \\tilde{H}_m $$\nwhere $V_m = [v_1, \\ldots, v_m]$.\n\nAny vector $z_m \\in \\mathcal{K}_m(A, r_0)$ can be expressed as a linear combination of the basis vectors, $z_m = V_m y$ for some coefficient vector $y \\in \\mathbb{C}^m$. Substituting this into the residual expression:\n$$ r_m = r_0 - A(V_m y) = r_0 - V_{m+1}\\tilde{H}_m y $$\nUsing $r_0 = \\|r_0\\|_2 v_1$ and noting that $v_1 = V_{m+1}e_1$ (where $e_1$ is the first standard basis vector in $\\mathbb{R}^{m+1}$):\n$$ r_m = \\|r_0\\|_2 V_{m+1}e_1 - V_{m+1}\\tilde{H}_m y = V_{m+1}(\\|r_0\\|_2 e_1 - \\tilde{H}_m y) $$\nSince the columns of $V_{m+1}$ are orthonormal ($V_{m+1}^H V_{m+1} = I$), the $2$-norm is preserved under multiplication by $V_{m+1}^H$. Therefore, minimizing $\\|r_m\\|_2$ is equivalent to solving the following least-squares problem for $y$:\n$$ \\min_{y \\in \\mathbb{C}^m} \\|\\, \\|r_0\\|_2 e_1 - \\tilde{H}_m y \\,\\|_2 $$\nThis is a small, dense least-squares problem of size $(m+1) \\times m$ that can be solved efficiently (e.g., via QR decomposition). Once the optimal $y$ is found, the solution update is $x_m = x_0 + V_m y$. By construction, this procedure finds the solution in $x_0 + \\mathcal{K}_m(A, r_0)$ that has the minimal residual $2$-norm.\n\n**Bi-Conjugate Gradient Stabilized (BiCGStab) Derivation and Properties**\n\nBiCGStab is a hybrid method designed to smooth the often erratic convergence of the Bi-Conjugate Gradient (BiCG) method. BiCG itself is based on the bi-Lanczos process, which generates two sequences of vectors, $v_j$ and $\\hat{v}_j$, that are bi-orthogonal ($v_j^H \\hat{v}_k = 0$ for $j \\neq k$). This is applied to two Krylov subspaces, $\\mathcal{K}_m(A, r_0)$ and $\\mathcal{K}_m(A^H, \\hat{r}_0)$, where $\\hat{r}_0$ is an arbitrary \"shadow\" initial residual. For the given problem where $A=A^T$, a common choice is to replace $A^H$ with $A^T=A$ and use a bilinear form $u^T v$ instead of the inner product $u^H v$.\n\nBiCGStab proceeds in two main parts per iteration $k$: a BiCG step followed by a stabilizing GMRES(1) step.\nStarting with an initial guess $x_0$ and residual $r_0 = b-Ax_0$. Choose a shadow residual $\\hat{r}_0$ (often $r_0$). Set $p_0=r_0$.\n\nFor $k=1, 2, \\ldots$:\n1.  **BiCG part**: This step advances the solution along a search direction $p_k$ determined by the BiCG recurrence.\n    -   Compute the scalars for the recurrences. A common formulation uses $\\rho_{k-1} = \\hat{r}_0^T r_{k-1}$. A breakdown occurs if $\\rho_{k-1}=0$.\n    -   The search direction $p_k$ is updated via a short recurrence:\n        $$ \\beta_{k-1} = \\frac{\\rho_{k-1}}{\\rho_{k-2}} \\frac{\\alpha_{k-1}}{\\omega_{k-1}} $$\n        $$ p_k = r_{k-1} + \\beta_{k-1} (p_{k-1} - \\omega_{k-1} A p_{k-1}) $$\n    -   The optimal step size $\\alpha_k$ along $p_k$ is computed:\n        $$ v_k = A p_k $$\n        $$ \\alpha_k = \\frac{\\rho_{k-1}}{\\hat{r}_0^T v_k} $$\n        A breakdown occurs if the denominator $\\hat{r}_0^T v_k = 0$.\n    -   An intermediate solution and residual are formed:\n        $$ s_k = r_{k-1} - \\alpha_k v_k $$\n        $$ x_{k}' = x_{k-1} + \\alpha_k p_k $$\n        The vector $s_k$ is the residual corresponding to $x_{k}'$.\n\n2.  **Stabilization part**: This step aims to reduce the norm of the intermediate residual $s_k$ using a minimal residual step of length one.\n    -   Compute $t_k = A s_k$.\n    -   The step length $\\omega_k$ is chosen to minimize $\\|s_k - \\omega_k t_k\\|_2$. The solution to this one-dimensional least-squares problem is:\n        $$ \\omega_k = \\frac{t_k^H s_k}{t_k^H t_k} $$\n        A breakdown can occur if $t_k=0$ or, if using a bilinear form, $t_k^T t_k=0$.\n    -   The final updates for the solution and residual at step $k$ are:\n        $$ x_k = x_{k}' + \\omega_k s_k = x_{k-1} + \\alpha_k p_k + \\omega_k s_k $$\n        $$ r_k = s_k - \\omega_k t_k $$\nBiCGStab uses short-term recurrences. At each step $k$, it only needs vectors from step $k-1$. Thus, it requires storing a fixed, small number of vectors (e.g., $r, \\hat{r}_0, p, v, s, t$), leading to a memory cost of $\\mathcal{O}(n)$. However, the algorithm can suffer from several types of breakdown if any of the scalar denominators become zero or near-zero.\n\n**Evaluation of Statements**\n\n-   **Statement A**: Bi-Conjugate Gradient Stabilized (BiCGStab) uses short recurrences and stores a fixed number of vectors, yielding memory cost $\\mathcal{O}(n)$ independent of iteration count, but it may break down when a scalar denominator in the recurrence becomes zero (for example, when a bi-orthogonality inner product vanishes or when a stabilization parameter becomes zero). Generalized Minimal Residual (GMRES) builds an orthonormal Krylov basis by an Arnoldi-type process and must store all basis vectors up to the current or restart length, with memory cost $\\mathcal{O}(m n)$ after $m$ iterations.\n    -   This statement is a correct summary of the fundamental complexity and stability properties of the two algorithms. Our derivation shows BiCGStab uses short recurrences (fixed storage, $\\mathcal{O}(n)$) and has potential breakdown points (zero denominators for $\\alpha_k, \\beta_k, \\omega_k$). Our derivation of GMRES confirms it uses the long-recurrence Arnoldi process, requiring storage of the entire basis of $m$ vectors, leading to $\\mathcal{O}(mn)$ memory cost.\n    -   **Verdict: Correct.**\n\n-   **Statement B**: For a complex symmetric matrix $A = A^{T}$, taking the Bi-Conjugate Gradient Stabilized (BiCGStab) shadow residual $\\hat{r}_{0} = r_{0}$ and using no conjugation (that is, replacing conjugate-transposes by pure transposes) preserves the formal recurrences and guarantees a monotonically decreasing residual $2$-norm.\n    -   While the proposed modifications define a valid variant of BiCGStab for complex symmetric systems, the assertion of a guaranteed monotonically decreasing residual $2$-norm is false. BiCGStab's \"stabilization\" step performs a local minimization of the residual norm, which helps to smooth convergence but does not guarantee global monotonicity. The residual norm can, and often does, exhibit non-monotonic behavior. Only GMRES has this guaranteed non-increasing residual property due to its global minimization over nested subspaces.\n    -   **Verdict: Incorrect.**\n\n-   **Statement C**: Full (non-restarted) Generalized Minimal Residual (GMRES) minimizes the residual $2$-norm at each iteration over the current Krylov subspace, producing a nonincreasing sequence of residual norms and experiencing no true breakdown in exact arithmetic except the so-called happy breakdown when the exact solution is reached within the Krylov subspace, at the cost of increasing storage and orthogonalization work per iteration.\n    -   This statement accurately describes the theoretical properties of full GMRES. As shown in our derivation, it minimizes $\\|r_m\\|_2$ over $x_m \\in x_0+\\mathcal{K}_m(A, r_0)$. Since $\\mathcal{K}_m \\subseteq \\mathcal{K}_{m+1}$, the minimum over the larger space cannot be worse, so $\\|r_{m+1}\\|_2 \\le \\|r_m\\|_2$. A breakdown ($h_{m+1,m}=0$) implies the subspace is invariant under $A$, which means the exact solution has been found. This \"happy breakdown\" is the only kind possible in exact arithmetic. The costs ($V_m$ storage and orthogonalization work) both grow as $\\mathcal{O}(mn)$, as stated.\n    -   **Verdict: Correct.**\n\n-   **Statement D**: Bi-Conjugate Gradient Stabilized (BiCGStab) is guaranteed to converge in at most $n$ steps for any diagonalizable matrix $A \\in \\mathbb{C}^{n \\times n}$ in exact arithmetic.\n    -   This statement is too strong and incorrect. While BiCG (on which BiCGStab is based) has a finite termination property in at most $n$ steps *if it does not break down*, this guarantee does not automatically extend to BiCGStab for *any* diagonalizable matrix. The primary issue is the possibility of breakdown, which the statement ignores. The finite termination property is not guaranteed.\n    -   **Verdict: Incorrect.**\n\n-   **Statement E**: For complex symmetric indefinite $A$, formulating Bi-Conjugate Gradient Stabilized (BiCGStab) with the transpose bilinear form $x^{T} y$ in place of the standard conjugate inner product $x^{H} y$ eliminates breakdowns entirely in exact arithmetic.\n    -   This claim is false. Using the bilinear form $x^T y$ is a common technique for complex symmetric systems, but it does not eliminate breakdowns. For example, a denominator of the form $v^T v$ can become zero for a non-zero complex vector $v$ (e.g., $v = [1, i]^T \\implies v^T v = 1^2 + i^2 = 0$). This is a well-known potential cause of breakdown in such algorithms.\n    -   **Verdict: Incorrect.**\n\n-   **Statement F**: In high-wave-number Helmholtz problems, the nonnormality and indefiniteness of $A$ commonly lead to stagnation in restarted Generalized Minimal Residual (GMRES) and erratic convergence of Bi-Conjugate Gradient Stabilized (BiCGStab); in practice the preconditioner quality largely dictates convergence behavior for both methods.\n    -   This statement accurately reflects the practical reality of solving large-scale Helmholtz problems. The resulting matrices are highly indefinite and non-normal, which are challenging properties for most iterative solvers. Restarted GMRES often stagnates because the small Krylov subspace generated between restarts is insufficient to model the complex behavior of the operator. BiCGStab's convergence can be irregular and slow. For such difficult problems, unpreconditioned solvers are rarely effective, and the development and application of a powerful preconditioner is almost always the key factor determining whether a solution can be found efficiently.\n    -   **Verdict: Correct.**\n\nFinal selection includes statements A, C, and F.",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "The theoretical promise of iterative solvers is only realized through effective preconditioning, a technique that transforms the linear system to make it easier to solve. This hands-on coding exercise takes you from the governing acoustic PDE to a full implementation and spectral analysis of a preconditioned system. You will investigate how physical heterogeneities in the acoustic medium directly impact the eigenvalue distribution of the system matrix and, consequently, the effectiveness of a simple Jacobi preconditioner .",
            "id": "4137503",
            "problem": "Consider the time-harmonic linear acoustics pressure formulation derived from the linearized momentum balance and compressibility relation under the assumption of sinusoidal time dependence with angular frequency $\\omega$ (in radians per second). The governing Partial Differential Equation (PDE) for pressure $p$ in a heterogeneous medium is\n$$\n-\\nabla \\cdot \\left(\\alpha(\\mathbf{x}) \\nabla p(\\mathbf{x})\\right) + \\beta(\\mathbf{x})\\,p(\\mathbf{x}) = s(\\mathbf{x}),\n$$\nwhere $\\alpha(\\mathbf{x}) = 1/\\rho(\\mathbf{x})$ with $\\rho$ the mass density (in $\\mathrm{kg/m^3}$), and $\\beta(\\mathbf{x}) = \\omega^2/\\kappa(\\mathbf{x})$ with $\\kappa$ the bulk modulus (in $\\mathrm{Pa}$). Assume homogeneous Dirichlet boundary conditions $p=0$ on the boundary of a square domain of side length $L$ (in meters), and discretize the operator on a uniform Cartesian grid of $N \\times N$ interior points with grid spacing $h = L/(N+1)$ using a second-order finite-volume or finite-difference approach derived from the divergence theorem and face-normal flux continuity. The diagonal entries of the resulting Symmetric Positive Definite (SPD) linear system matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n=N^2$ reflect the sum of face flux coefficients and local reaction (mass) terms.\n\nDefine the damped Jacobi preconditioner by $M^{-1} = \\omega_J D^{-1}$, where $D=\\mathrm{diag}(A)$ is the diagonal of $A$ and $\\omega_J \\in (0,1)$ is a dimensionless damping factor. The preconditioned operator is $M^{-1}A$, which is similar to the symmetric matrix $D^{-1/2} A D^{-1/2}$ and therefore has a real, nonnegative spectrum. Your task is to:\n- Construct $A$ for each test case, using a physically consistent discretization based on face-centered fluxes with appropriate averaging of $\\alpha$ at faces to handle material discontinuities.\n- Form the preconditioned operator and compute the eigenvalues of the similar symmetric matrix $D^{-1/2} A D^{-1/2}$, which equal the eigenvalues of $D^{-1}A$.\n- Quantify how the diagonal scaling inherent to the damped Jacobi preconditioner interacts with heterogeneities in $\\rho$ and $\\kappa$ by reporting, for each test case:\n  1. The smallest eigenvalue $\\lambda_{\\min}$ of $D^{-1/2} A D^{-1/2}$,\n  2. The largest eigenvalue $\\lambda_{\\max}$ of $D^{-1/2} A D^{-1/2}$,\n  3. The spectral radius of $M^{-1}A$, which equals $\\omega_J \\lambda_{\\max}$,\n  4. The coefficient of variation of the eigenvalues of $M^{-1}A$, defined as the standard deviation divided by the mean.\nAll four quantities are dimensionless and must be returned as floating-point numbers.\n\nUse the following test suite of parameter values and heterogeneous fields. In all cases, take $L = 1.0$ meters, $h = L/(N+1)$, and $\\omega = 2\\pi f$ with $f$ in Hertz. The program must implement the discretization from first principles without relying on shortcut formulas and must compute eigenvalues numerically.\n\n- Test case $1$ (baseline homogeneous medium):\n  - $N = 10$, $f = 1000$ Hertz, $\\omega_J = 0.66$,\n  - $\\rho(x,y) = 1000$ $\\mathrm{kg/m^3}$ everywhere,\n  - $\\kappa(x,y) = 2.2\\times 10^9$ $\\mathrm{Pa}$ everywhere.\n- Test case $2$ (density contrast slab, constant bulk modulus):\n  - $N = 10$, $f = 1000$ Hertz, $\\omega_J = 0.66$,\n  - $\\rho(x,y) = 1000$ $\\mathrm{kg/m^3}$ for $x  0.5$, $\\rho(x,y) = 2500$ $\\mathrm{kg/m^3}$ for $x \\ge 0.5$,\n  - $\\kappa(x,y) = 2.2\\times 10^9$ $\\mathrm{Pa}$ everywhere.\n- Test case $3$ (bulk modulus contrast slab, constant density):\n  - $N = 10$, $f = 1000$ Hertz, $\\omega_J = 0.66$,\n  - $\\rho(x,y) = 1000$ $\\mathrm{kg/m^3}$ everywhere,\n  - $\\kappa(x,y) = 1.0\\times 10^9$ $\\mathrm{Pa}$ for $y \\ge 0.5$, $\\kappa(x,y) = 5.0\\times 10^9$ $\\mathrm{Pa}$ for $y  0.5$.\n- Test case $4$ (checkerboard heterogeneity in both $\\rho$ and $\\kappa$, strong damping):\n  - $N = 10$, $f = 1000$ Hertz, $\\omega_J = 0.95$,\n  - $\\rho(x,y) = 800$ $\\mathrm{kg/m^3}$ if $(i+j)$ is even, $\\rho(x,y) = 1600$ $\\mathrm{kg/m^3}$ if $(i+j)$ is odd, where $(i,j)$ index interior grid points,\n  - $\\kappa(x,y) = 1.5\\times 10^9$ $\\mathrm{Pa}$ if $(i+j)$ is even, $\\kappa(x,y) = 3.0\\times 10^9$ $\\mathrm{Pa}$ if $(i+j)$ is odd.\n- Test case $5$ (extreme localized heterogeneity, moderate damping):\n  - $N = 6$, $f = 1000$ Hertz, $\\omega_J = 0.50$,\n  - $\\rho(x,y) = 100$ $\\mathrm{kg/m^3}$ in the central $2\\times 2$ interior block, $\\rho(x,y) = 5000$ $\\mathrm{kg/m^3}$ elsewhere,\n  - $\\kappa(x,y) = 0.5\\times 10^9$ $\\mathrm{Pa}$ in the central $2\\times 2$ interior block, $\\kappa(x,y) = 10.0\\times 10^9$ $\\mathrm{Pa}$ elsewhere.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself a list of the four floating-point quantities in the order $[\\lambda_{\\min}, \\lambda_{\\max}, \\text{spectral\\_radius}, \\text{coefficient\\_of\\_variation}]$. For example, the output format must look like $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots]$ where each $a_k$, $b_k$, $c_k$, $d_k$ is a floating-point number. All reported quantities are dimensionless; no physical units are required in the output.",
            "solution": "The problem requires an analysis of the spectral properties of a damped Jacobi preconditioned linear system arising from the finite-volume discretization of a time-harmonic acoustic pressure wave equation in a heterogeneous medium. We will first derive the discretization to construct the system matrix $A$, then define the preconditioned operator, and finally outline the computation of the required spectral metrics.\n\nThe governing PDE for the acoustic pressure $p(\\mathbf{x})$ is the heterogeneous Helmholtz equation:\n$$\n-\\nabla \\cdot \\left(\\alpha(\\mathbf{x}) \\nabla p(\\mathbf{x})\\right) + \\beta(\\mathbf{x})\\,p(\\mathbf{x}) = s(\\mathbf{x})\n$$\nwhere $\\mathbf{x}=(x,y)$, $\\alpha(\\mathbf{x}) = 1/\\rho(\\mathbf{x})$ is the reciprocal of mass density, and $\\beta(\\mathbf{x}) = \\omega^2/\\kappa(\\mathbf{x})$ is related to the bulk modulus and angular frequency. The domain is a square of side length $L$ with homogeneous Dirichlet boundary conditions, $p(\\mathbf{x})=0$ on the boundary.\n\nWe discretize this equation on a uniform Cartesian grid of $N \\times N$ interior points. The grid spacing is $h = L/(N+1)$, and coordinates of grid points are $(x_i, y_j) = (ih, jh)$ for $i,j \\in \\{1, \\dots, N\\}$. The total number of degrees of freedom is $n=N^2$. We will use a cell-centered finite-volume method. Integrating the PDE over a square control volume $V_{ij}$ of side length $h$ centered at $(x_i, y_j)$ and applying the divergence theorem yields:\n$$\n-\\oint_{\\partial V_{ij}} (\\alpha \\nabla p) \\cdot \\mathbf{n} \\, dS + \\int_{V_{ij}} \\beta p \\, dV = \\int_{V_{ij}} s \\, dV\n$$\nThe boundary integral is a sum of fluxes across the four faces (East, West, North, South) of the control volume. The flux across the East face, for instance, is approximated using a second-order central difference for the gradient:\n$$\n\\int_{\\text{East face}} (\\alpha \\nabla p) \\cdot \\mathbf{n} \\, dS \\approx h \\left( \\alpha_{i+1/2,j} \\frac{p_{i+1,j} - p_{i,j}}{h} \\right) = \\alpha_{i+1/2,j} (p_{i+1,j} - p_{i,j})\n$$\nHere, $p_{i,j}$ denotes $p(x_i, y_j)$, and $\\alpha_{i+1/2,j}$ is the value of $\\alpha$ at the face between cells $(i,j)$ and $(i+1,j)$. To ensure physical consistency (continuity of flux $\\alpha \\nabla p$) across material interfaces, we use the harmonic mean for the coefficient $\\alpha$:\n$$\n\\alpha_{i+1/2, j} = \\frac{2 \\alpha_{i,j} \\alpha_{i+1,j}}{\\alpha_{i,j} + \\alpha_{i+1,j}}\n$$\nApproximating all fluxes and the volume integrals (using a midpoint rule, e.g., $\\int_{V_{ij}} \\beta p \\, dV \\approx (\\beta p)_{i,j} h^2$), and taking the source term $s(\\mathbf{x})=0$ as it does not affect the matrix $A$, we obtain the discrete equation for grid point $(i,j)$:\n$$\n-\\left[ \\alpha_{E}(p_{i+1,j}-p_{i,j}) - \\alpha_{W}(p_{i,j}-p_{i-1,j}) + \\alpha_{N}(p_{i,j+1}-p_{i,j}) - \\alpha_{S}(p_{i,j}-p_{i,j-1}) \\right] + h^2 \\beta_{i,j} p_{i,j} = 0\n$$\nwhere $\\alpha_E, \\alpha_W, \\alpha_N, \\alpha_S$ are the harmonic means of $\\alpha$ on the East, West, North, and South faces, respectively. Rearranging this gives the stencil for the linear system $Ap=0$:\n$$\n(\\alpha_E + \\alpha_W + \\alpha_N + \\alpha_S + h^2\\beta_{i,j})p_{i,j} - \\alpha_E p_{i+1,j} - \\alpha_W p_{i-1,j} - \\alpha_N p_{i,j+1} - \\alpha_S p_{i,j-1} = 0\n$$\nTo construct the $n \\times n$ matrix $A$, we map the 2D grid of unknowns to a 1D vector using row-major ordering: the index $k$ for point $(i,j)$ is $k = (j-1)N + (i-1)$. The resulting matrix $A$ is a sparse, banded, symmetric matrix. Since $\\alpha > 0$ and $\\beta > 0$, the coefficients on the main diagonal are positive and greater than the sum of the absolute values of the off-diagonal elements in the same row. This strict diagonal dominance, combined with symmetry and positive diagonal entries, guarantees that $A$ is Symmetric Positive Definite (SPD).\n\nThe problem asks for an analysis of the damped Jacobi preconditioner, defined by $M^{-1} = \\omega_J D^{-1}$, where $D=\\mathrm{diag}(A)$ is the diagonal of $A$ and $\\omega_J \\in (0,1)$ is the damping factor. We analyze the eigenvalues of the preconditioned operator $M^{-1}A = \\omega_J D^{-1}A$. The eigenvalues of $D^{-1}A$ are real and positive because $D^{-1}A$ is similar to the symmetric matrix $B = D^{-1/2} A D^{-1/2}$ (since $A$ is SPD, $D$ has positive entries, so $D^{-1/2}$ is real and well-defined). The eigenvalues of $M^{-1}A$ are simply $\\omega_J$ times the eigenvalues of $D^{-1}A$.\n\nThe procedure for each test case is as follows:\n1.  Discretize the domain and construct the material property fields $\\rho(\\mathbf{x})$ and $\\kappa(\\mathbf{x})$ on an $(N+2) \\times (N+2)$ grid, representing the $N \\times N$ interior points and a layer of ghost cells for boundary calculations. The ghost cell values are set by zero-order extrapolation from the nearest interior cell.\n2.  From $\\rho$ and $\\kappa$, compute the coefficient fields $\\alpha = 1/\\rho$ and $\\beta = \\omega^2/\\kappa$.\n3.  Construct the $n \\times n$ matrix $A$ based on the derived finite-volume stencil, using harmonic averaging for $\\alpha$ at cell faces.\n4.  Extract the diagonal $D$ of $A$ and form the symmetrically scaled matrix $B = D^{-1/2} A D^{-1/2}$.\n5.  Compute the eigenvalues of the symmetric matrix $B$ using a numerical eigensolver. Let these be $\\{\\lambda_i\\}_{i=1}^n$. These are also the eigenvalues of $D^{-1}A$.\n6.  Calculate the four required dimensionless quantities:\n    a. The smallest eigenvalue $\\lambda_{\\min} = \\min(\\{\\lambda_i\\})$.\n    b. The largest eigenvalue $\\lambda_{\\max} = \\max(\\{\\lambda_i\\})$.\n    c. The spectral radius of $M^{-1}A$, which is $\\rho(M^{-1}A) = |\\omega_J| \\cdot \\rho(D^{-1}A) = \\omega_J \\lambda_{\\max}$.\n    d. The coefficient of variation of the eigenvalues of $M^{-1}A$. Since the eigenvalues of $M^{-1}A$ are just scaled versions of the eigenvalues of $D^{-1}A$ (by a constant factor $\\omega_J$), their coefficient of variation (std/mean) is identical. We compute $\\text{std}(\\{\\lambda_i\\}) / \\text{mean}(\\{\\lambda_i\\})$.\n\nThis process is implemented for each of the five test cases defined in the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(case_params):\n    \"\"\"\n    Constructs the system matrix for the acoustic problem and computes\n    spectral properties of the Jacobi-preconditioned operator.\n    \"\"\"\n    # Unpack parameters\n    N = case_params['N']\n    f = case_params['f']\n    omega_J = case_params['omega_J']\n    rho_def = case_params['rho_def']\n    kappa_def = case_params['kappa_def']\n    \n    # Constants and grid parameters\n    L = 1.0\n    h = L / (N + 1)\n    n = N * N\n    omega = 2.0 * np.pi * f\n\n    # Material property grids (including ghost cells)\n    # Array indices (i,j) map to (y,x) in coordinate system\n    # Indices 1...N are interior points\n    rho = np.zeros((N + 2, N + 2))\n    kappa = np.zeros((N + 2, N + 2))\n\n    # Fill interior points based on case definitions\n    for j_idx in range(1, N + 1):  # Corresponds to y-direction\n        for i_idx in range(1, N + 1):  # Corresponds to x-direction\n            x = i_idx * h\n            y = j_idx * h\n            rho[j_idx, i_idx] = rho_def(x, y, i_idx, j_idx, N)\n            kappa[j_idx, i_idx] = kappa_def(x, y, i_idx, j_idx, N)\n\n    # Extrapolate to ghost cells\n    rho[0, :] = rho[1, :]\n    rho[N + 1, :] = rho[N, :]\n    rho[:, 0] = rho[:, 1]\n    rho[:, N + 1] = rho[:, N]\n\n    kappa[0, :] = kappa[1, :]\n    kappa[N + 1, :] = kappa[N, :]\n    kappa[:, 0] = kappa[:, 1]\n    kappa[:, N + 1] = kappa[:, N]\n\n    # Derived coefficient fields\n    alpha = 1.0 / rho\n    beta = omega**2 / kappa\n\n    # Construct the system matrix A\n    A = np.zeros((n, n))\n    for j_idx in range(1, N + 1):\n        for i_idx in range(1, N + 1):\n            k = (j_idx - 1) * N + (i_idx - 1)\n            \n            # Harmonic means for alpha at cell faces\n            alpha_E = 2.0 * alpha[j_idx, i_idx] * alpha[j_idx, i_idx + 1] / (alpha[j_idx, i_idx] + alpha[j_idx, i_idx + 1])\n            alpha_W = 2.0 * alpha[j_idx, i_idx] * alpha[j_idx, i_idx - 1] / (alpha[j_idx, i_idx] + alpha[j_idx, i_idx - 1])\n            alpha_N = 2.0 * alpha[j_idx, i_idx] * alpha[j_idx + 1, i_idx] / (alpha[j_idx, i_idx] + alpha[j_idx + 1, i_idx])\n            alpha_S = 2.0 * alpha[j_idx, i_idx] * alpha[j_idx - 1, i_idx] / (alpha[j_idx, i_idx] + alpha[j_idx - 1, i_idx])\n            \n            # Diagonal entry\n            A[k, k] = alpha_E + alpha_W + alpha_N + alpha_S + h**2 * beta[j_idx, i_idx]\n            \n            # Off-diagonal entries\n            if i_idx  N:  # East neighbor\n                l = k + 1\n                A[k, l] = -alpha_E\n            if i_idx  1:  # West neighbor\n                l = k - 1\n                A[k, l] = -alpha_W\n            if j_idx  N:  # North neighbor\n                l = k + N\n                A[k, l] = -alpha_N\n            if j_idx  1:  # South neighbor\n                l = k - N\n                A[k, l] = -alpha_S\n\n    # Form the symmetrically scaled matrix B = D^{-1/2} A D^{-1/2}\n    D_diag = np.diag(A)\n    D_inv_sqrt = np.diag(1.0 / np.sqrt(D_diag))\n    B = D_inv_sqrt @ A @ D_inv_sqrt\n\n    # Compute eigenvalues of B (which are the same as D^{-1}A)\n    # Using eigvalsh for symmetric matrices is more efficient and stable.\n    eigs = np.linalg.eigvalsh(B)\n    \n    # Calculate the four required metrics\n    lambda_min = eigs.min()\n    lambda_max = eigs.max()\n    spectral_radius = omega_J * lambda_max\n    \n    mean_eig = np.mean(eigs)\n    std_eig = np.std(eigs)\n    coeff_var = std_eig / mean_eig if mean_eig != 0 else 0.0\n\n    return [lambda_min, lambda_max, spectral_radius, coeff_var]\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Homogeneous medium\n        {\n            'N': 10, 'f': 1000, 'omega_J': 0.66,\n            'rho_def': lambda x, y, i, j, N: 1000,\n            'kappa_def': lambda x, y, i, j, N: 2.2e9\n        },\n        # Case 2: Density contrast slab\n        {\n            'N': 10, 'f': 1000, 'omega_J': 0.66,\n            'rho_def': lambda x, y, i, j, N: 1000 if x  0.5 else 2500,\n            'kappa_def': lambda x, y, i, j, N: 2.2e9\n        },\n        # Case 3: Bulk modulus contrast slab\n        {\n            'N': 10, 'f': 1000, 'omega_J': 0.66,\n            'rho_def': lambda x, y, i, j, N: 1000,\n            'kappa_def': lambda x, y, i, j, N: 1.0e9 if y = 0.5 else 5.0e9\n        },\n        # Case 4: Checkerboard heterogeneity\n        {\n            'N': 10, 'f': 1000, 'omega_J': 0.95,\n            'rho_def': lambda x, y, i, j, N: 800 if (i + j) % 2 == 0 else 1600,\n            'kappa_def': lambda x, y, i, j, N: 1.5e9 if (i + j) % 2 == 0 else 3.0e9\n        },\n        # Case 5: Extreme localized heterogeneity\n        {\n            'N': 6, 'f': 1000, 'omega_J': 0.50,\n            'rho_def': lambda x, y, i, j, N: 100 if (i in [3,4] and j in [3,4]) else 5000,\n            'kappa_def': lambda x, y, i, j, N: 0.5e9 if (i in [3,4] and j in [3,4]) else 10.0e9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        metrics = compute_metrics(case)\n        results.append(metrics)\n\n    # Format output as a list of lists of floats\n    result_str = '[' + ','.join([f\"[{m[0]},{m[1]},{m[2]},{m[3]}]\" for m in results]) + ']'\n    print(result_str)\n\nsolve()\n```"
        }
    ]
}