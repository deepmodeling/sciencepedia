{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any Physics-Informed Neural Network (PINN) lies in its ability to quantify how well a neural network's output satisfies a given partial differential equation (PDE). This is accomplished by calculating the PDE residual. This first practice provides a direct, hands-on experience in constructing and evaluating this fundamental component for the 1D wave equation. By working through the analytical differentiation of a neural network ansatz and implementing the residual calculation on a defined grid, you will build a concrete understanding of how physics constraints are translated into a computable loss function that drives the learning process .",
            "id": "4134325",
            "problem": "Consider the one-dimensional acoustic pressure field governed by the homogeneous wave equation on a normalized spatial domain and time interval. Let the spatial domain be $\\Omega=[0,1]$ and the time interval be $[0,1]$, with nondimensionalized wave speed $c=1$. The governing partial differential equation is the homogeneous wave equation\n$$\n\\partial_{tt}p(x,t)-\\partial_{xx}p(x,t)=0,\n$$\nwhere $p(x,t)$ is the acoustic pressure. Physics-Informed Neural Networks (PINNs) use a neural-network ansatz to represent $p(x,t)$ and enforce the physics by minimizing pointwise residuals of the governing equations and constraints.\n\nStarting from the fundamental base of the wave equation, the chain rule, and standard definitions of a feedforward neural network, construct the pointwise PINN residual for the partial differential equation and the constraints. Use a single-hidden-layer feedforward neural network with hyperbolic tangent activation, defined by\n$$\np_{\\theta}(x,t)=\\sum_{i=1}^{W}w_i\\,\\tanh\\!\\big(a_i\\,x+b_i\\,t+d_i\\big)+e\\,x+f\\,t+g,\n$$\nwhere $W$ is the hidden width, and $\\theta=\\{(w_i,a_i,b_i,d_i)_{i=1}^{W},e,f,g\\}$ are trainable parameters. The physics residual is the function\n$$\nR_{\\mathrm{PDE}}(x,t;\\theta)=\\partial_{tt}p_{\\theta}(x,t)-\\partial_{xx}p_{\\theta}(x,t),\n$$\nand the constraint residuals are\n$$\nR_{\\mathrm{IC},p}(x;\\theta)=p_{\\theta}(x,0)-\\sin(\\pi x),\\quad R_{\\mathrm{IC},v}(x;\\theta)=\\partial_t p_{\\theta}(x,0)-0,\n$$\n$$\nR_{\\mathrm{BC},L}(t;\\theta)=p_{\\theta}(0,t)-0,\\quad R_{\\mathrm{BC},R}(t;\\theta)=p_{\\theta}(1,t)-0,\n$$\ncorresponding to the initial displacement, initial velocity, and Dirichlet boundary conditions at $x=0$ and $x=1$. All quantities are nondimensional and should be treated as dimensionless throughout.\n\nYou must specify four disjoint collocation sets used to evaluate the PINN residual components:\n- A partial differential equation interior set $\\mathcal{X}_{\\mathrm{PDE}}=\\{(x_j,t_j)\\}_{j=1}^{N_{\\mathrm{PDE}}}$ with $x_j\\in(0,1)$ and $t_j\\in(0,1)$,\n- An initial-condition set for displacement $\\mathcal{X}_{\\mathrm{IC},p}=\\{x_k\\}_{k=1}^{N_{\\mathrm{IC},p}}$ at $t=0$,\n- An initial-condition set for velocity $\\mathcal{X}_{\\mathrm{IC},v}=\\{x_\\ell\\}_{\\ell=1}^{N_{\\mathrm{IC},v}}$ at $t=0$,\n- A boundary-condition set $\\mathcal{X}_{\\mathrm{BC}}=\\{(x_m,t_m)\\}_{m=1}^{N_{\\mathrm{BC}}}$ with $x_m\\in\\{0,1\\}$ and $t_m\\in[0,1]$.\n\nConstruct these collocation sets deterministically by uniform sampling as follows:\n- Choose $N_x^{\\mathrm{int}}=20$ and $N_t^{\\mathrm{int}}=20$ interior grid counts, and set $N_{\\mathrm{PDE}}=N_x^{\\mathrm{int}}\\cdot N_t^{\\mathrm{int}}=400$. Define $x$-coordinates by $x_i=\\frac{i}{N_x^{\\mathrm{int}}+1}$ for $i=1,2,\\dots,20$, and $t$-coordinates by $t_j=\\frac{j}{N_t^{\\mathrm{int}}+1}$ for $j=1,2,\\dots,20$. The interior set $\\mathcal{X}_{\\mathrm{PDE}}$ is all pairs $(x_i,t_j)$.\n- Choose $N_{\\mathrm{IC},p}=50$ and define $x_k=\\frac{k}{N_{\\mathrm{IC},p}-1}$ for $k=0,1,2,\\dots,49$ at $t=0$ to form $\\mathcal{X}_{\\mathrm{IC},p}$.\n- Choose $N_{\\mathrm{IC},v}=50$ and define $x_\\ell=\\frac{\\ell}{N_{\\mathrm{IC},v}-1}$ for $\\ell=0,1,2,\\dots,49$ at $t=0$ to form $\\mathcal{X}_{\\mathrm{IC},v}$.\n- Choose $N_t^{\\mathrm{bc}}=80$ and define $t_m=\\frac{m}{N_t^{\\mathrm{bc}}-1}$ for $m=0,1,2,\\dots,79$ at both $x=0$ and $x=1$ to form $\\mathcal{X}_{\\mathrm{BC}}$, so $N_{\\mathrm{BC}}=2\\cdot N_t^{\\mathrm{bc}}=160$.\n\nFor each test case defined below, compute the mean-squared residuals over these sets:\n$$\nE_{\\mathrm{PDE}}(\\theta)=\\frac{1}{N_{\\mathrm{PDE}}}\\sum_{(x,t)\\in\\mathcal{X}_{\\mathrm{PDE}}} \\big(R_{\\mathrm{PDE}}(x,t;\\theta)\\big)^2,\n$$\n$$\nE_{\\mathrm{IC},p}(\\theta)=\\frac{1}{N_{\\mathrm{IC},p}}\\sum_{x\\in\\mathcal{X}_{\\mathrm{IC},p}} \\big(R_{\\mathrm{IC},p}(x;\\theta)\\big)^2,\\quad\nE_{\\mathrm{IC},v}(\\theta)=\\frac{1}{N_{\\mathrm{IC},v}}\\sum_{x\\in\\mathcal{X}_{\\mathrm{IC},v}} \\big(R_{\\mathrm{IC},v}(x;\\theta)\\big)^2,\n$$\n$$\nE_{\\mathrm{BC}}(\\theta)=\\frac{1}{N_{\\mathrm{BC}}}\\sum_{(x,t)\\in\\mathcal{X}_{\\mathrm{BC}}} \\big(p_{\\theta}(x,t)\\big)^2.\n$$\nYour program must evaluate and report these four mean-squared residuals for each parameter set below, without performing any training.\n\nTest suite. Use the following three parameter sets, each given by hidden width $W$ and the arrays of parameters. All arrays are ordered as $[\\,\\cdot\\,]$ with each entry written explicitly:\n- Test case $1$:\n  - $W=3$\n  - $\\mathbf{a}=[\\,1.0,\\,-2.0,\\,0.5\\,]$\n  - $\\mathbf{b}=[\\,1.5,\\,0.5,\\,-1.0\\,]$\n  - $\\mathbf{d}=[\\,0.1,\\,-0.2,\\,0.3\\,]$\n  - $\\mathbf{w}=[\\,0.7,\\,-1.0,\\,0.3\\,]$\n  - $e=0.0$, $f=0.0$, $g=0.0$\n- Test case $2$:\n  - $W=0$\n  - $\\mathbf{a}=[\\,\\,]$\n  - $\\mathbf{b}=[\\,\\,]$\n  - $\\mathbf{d}=[\\,\\,]$\n  - $\\mathbf{w}=[\\,\\,]$\n  - $e=0.0$, $f=0.0$, $g=0.0$\n- Test case $3$:\n  - $W=3$\n  - $\\mathbf{a}=[\\,3.0,\\,-3.0,\\,1.0\\,]$\n  - $\\mathbf{b}=[\\,3.0,\\,3.0,\\,-1.0\\,]$\n  - $\\mathbf{d}=[\\,-0.5,\\,0.25,\\,0.0\\,]$\n  - $\\mathbf{w}=[\\,0.2,\\,-0.2,\\,0.1\\,]$\n  - $e=0.0$, $f=0.0$, $g=0.0$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of four floats $[\\,E_{\\mathrm{PDE}},E_{\\mathrm{IC},p},E_{\\mathrm{IC},v},E_{\\mathrm{BC}}\\,]$. For example, the overall structure must be\n$$\n[\\,[\\,E_{\\mathrm{PDE}}^{(1)},E_{\\mathrm{IC},p}^{(1)},E_{\\mathrm{IC},v}^{(1)},E_{\\mathrm{BC}}^{(1)}\\,],\\,[\\,E_{\\mathrm{PDE}}^{(2)},E_{\\mathrm{IC},p}^{(2)},E_{\\mathrm{IC},v}^{(2)},E_{\\mathrm{BC}}^{(2)}\\,],\\,[\\,E_{\\mathrm{PDE}}^{(3)},E_{\\mathrm{IC},p}^{(3)},E_{\\mathrm{IC},v}^{(3)},E_{\\mathrm{BC}}^{(3)}\\,]\\,].\n$$\nAll quantities are nondimensional floats and should be printed as raw decimals with no units.",
            "solution": "The problem requires the computation of mean-squared residuals for a Physics-Informed Neural Network (PINN) applied to a 1D acoustic wave equation. The problem is valid as it is scientifically grounded in established principles of wave physics and computational mathematics, is well-posed with all necessary information provided, and is formulated objectively. The task is to evaluate, not train, a given neural network model, which is a direct and unambiguous computational procedure.\n\nWe will first derive the analytical expressions for the PINN residuals, then describe the construction of the collocation point sets, and finally detail the computation of the mean-squared error for each test case.\n\nThe governing partial differential equation (PDE) is the homogeneous 1D wave equation:\n$$\n\\partial_{tt}p(x,t) - c^2\\partial_{xx}p(x,t) = \\partial_{tt}p - \\partial_{xx}p = 0\n$$\nwhere $p(x,t)$ is the acoustic pressure, $x \\in [0,1]$, $t \\in [0,1]$, and the wave speed is $c=1$.\n\nThe problem proposes the following neural network ansatz for $p(x,t)$:\n$$\np_{\\theta}(x,t) = \\sum_{i=1}^{W} w_i \\tanh(\\phi_i(x,t)) + e x + f t + g\n$$\nwhere $\\phi_i(x,t) = a_i x + b_i t + d_i$. The set of trainable parameters is $\\theta = \\{(w_i, a_i, b_i, d_i)_{i=1}^{W}, e, f, g\\}$.\n\nTo evaluate the PINN residuals, we must compute the necessary partial derivatives of $p_{\\theta}(x,t)$. We use the chain rule and the identity $\\frac{d}{dz}\\tanh(z) = \\text{sech}^2(z) = 1 - \\tanh^2(z)$.\n\nFirst partial derivative with respect to time $t$:\n$$\n\\partial_t p_{\\theta}(x,t) = \\frac{\\partial}{\\partial t} \\left( \\sum_{i=1}^{W} w_i \\tanh(\\phi_i) + e x + f t + g \\right) = \\sum_{i=1}^{W} w_i \\frac{d}{d\\phi_i}(\\tanh(\\phi_i)) \\frac{\\partial \\phi_i}{\\partial t} + f\n$$\nSince $\\frac{\\partial \\phi_i}{\\partial t} = b_i$, this becomes:\n$$\n\\partial_t p_{\\theta}(x,t) = \\sum_{i=1}^{W} w_i b_i \\text{sech}^2(\\phi_i) + f\n$$\n\nSecond partial derivative with respect to time $t$:\nUsing the identity $\\frac{d}{dz}\\text{sech}^2(z) = -2\\text{sech}^2(z)\\tanh(z)$, we have:\n$$\n\\partial_{tt} p_{\\theta}(x,t) = \\frac{\\partial}{\\partial t} \\left( \\sum_{i=1}^{W} w_i b_i \\text{sech}^2(\\phi_i) + f \\right) = \\sum_{i=1}^{W} w_i b_i \\frac{d}{d\\phi_i}(\\text{sech}^2(\\phi_i)) \\frac{\\partial \\phi_i}{\\partial t}\n$$\n$$\n\\partial_{tt} p_{\\theta}(x,t) = \\sum_{i=1}^{W} w_i b_i \\left( -2 \\text{sech}^2(\\phi_i) \\tanh(\\phi_i) \\right) b_i = -2 \\sum_{i=1}^{W} w_i b_i^2 \\text{sech}^2(\\phi_i) \\tanh(\\phi_i)\n$$\n\nSecond partial derivative with respect to space $x$:\nBy symmetry, the derivation is analogous to the time derivative, replacing $t$ with $x$, $f$ with $e$, and $b_i$ with $a_i$.\n$$\n\\partial_x p_{\\theta}(x,t) = \\sum_{i=1}^{W} w_i a_i \\text{sech}^2(\\phi_i) + e\n$$\n$$\n\\partial_{xx} p_{\\theta}(x,t) = -2 \\sum_{i=1}^{W} w_i a_i^2 \\text{sech}^2(\\phi_i) \\tanh(\\phi_i)\n$$\n\nNow we can write the explicit forms of the residuals.\n$1$. The PDE residual, $R_{\\mathrm{PDE}}(x,t;\\theta) = \\partial_{tt}p_{\\theta} - \\partial_{xx}p_{\\theta}$:\n$$\nR_{\\mathrm{PDE}}(x,t;\\theta) = -2 \\sum_{i=1}^{W} w_i b_i^2 \\text{sech}^2(\\phi_i) \\tanh(\\phi_i) - \\left( -2 \\sum_{i=1}^{W} w_i a_i^2 \\text{sech}^2(\\phi_i) \\tanh(\\phi_i) \\right)\n$$\n$$\nR_{\\mathrm{PDE}}(x,t;\\theta) = 2 \\sum_{i=1}^{W} w_i (a_i^2 - b_i^2) \\text{sech}^2(\\phi_i) \\tanh(\\phi_i)\n$$\n\n$2$. The initial condition residual for pressure (displacement), $R_{\\mathrm{IC},p}(x;\\theta) = p_{\\theta}(x,0) - \\sin(\\pi x)$:\n$$\nR_{\\mathrm{IC},p}(x;\\theta) = \\left( \\sum_{i=1}^{W} w_i \\tanh(a_i x + d_i) + e x + g \\right) - \\sin(\\pi x)\n$$\n\n$3$. The initial condition residual for velocity, $R_{\\mathrm{IC},v}(x;\\theta) = \\partial_t p_{\\theta}(x,0) - 0$:\n$$\nR_{\\mathrm{IC},v}(x;\\theta) = \\sum_{i=1}^{W} w_i b_i \\text{sech}^2(a_i x + d_i) + f\n$$\n\n$4$. The boundary condition residuals, $R_{\\mathrm{BC},L}(t;\\theta)=p_{\\theta}(0,t) - 0$ and $R_{\\mathrm{BC},R}(t;\\theta)=p_{\\theta}(1,t) - 0$. The value to be squared and averaged for the boundary error $E_{\\mathrm{BC}}$ is simply $p_{\\theta}(x,t)$ evaluated at the boundary points. For $x=0$:\n$$\np_{\\theta}(0,t) = \\sum_{i=1}^{W} w_i \\tanh(b_i t + d_i) + f t + g\n$$\nAnd for $x=1$:\n$$\np_{\\theta}(1,t) = \\sum_{i=1}^{W} w_i \\tanh(a_i + b_i t + d_i) + e + f t + g\n$$\n\nThe residuals are evaluated on four distinct collocation point sets, constructed as specified:\n-   $\\mathcal{X}_{\\mathrm{PDE}}$: A $N_x^{\\mathrm{int}} \\times N_t^{\\mathrm{int}} = 20 \\times 20 = 400$ point grid on $(0,1) \\times (0,1)$, where $x_i=\\frac{i}{21}$ for $i=1,\\dots,20$ and $t_j=\\frac{j}{21}$ for $j=1,\\dots,20$.\n-   $\\mathcal{X}_{\\mathrm{IC},p}$: $N_{\\mathrm{IC},p}=50$ points on $[0,1]$ at $t=0$, with $x_k=\\frac{k}{49}$ for $k=0,\\dots,49$.\n-   $\\mathcal{X}_{\\mathrm{IC},v}$: $N_{\\mathrm{IC},v}=50$ points on $[0,1]$ at $t=0$, with $x_\\ell=\\frac{\\ell}{49}$ for $\\ell=0,\\dots,49$.\n-   $\\mathcal{X}_{\\mathrm{BC}}$: $N_{\\mathrm{BC}} = 2 \\cdot N_t^{\\mathrm{bc}} = 160$ points on the boundary, with $N_t^{\\mathrm{bc}}=80$ points for time $t_m=\\frac{m}{79}$ for $m=0,\\dots,79$ at both $x=0$ and $x=1$.\n\nThe mean-squared errors (MSE) are computed by averaging the square of the corresponding residual over its collocation set:\n$$\nE_{\\mathrm{PDE}}(\\theta) = \\frac{1}{400} \\sum_{(x,t)\\in\\mathcal{X}_{\\mathrm{PDE}}} \\big(R_{\\mathrm{PDE}}(x,t;\\theta)\\big)^2\n$$\n$$\nE_{\\mathrm{IC},p}(\\theta) = \\frac{1}{50} \\sum_{x\\in\\mathcal{X}_{\\mathrm{IC},p}} \\big(R_{\\mathrm{IC},p}(x;\\theta)\\big)^2\n$$\n$$\nE_{\\mathrm{IC},v}(\\theta) = \\frac{1}{50} \\sum_{x\\in\\mathcal{X}_{\\mathrm{IC},v}} \\big(R_{\\mathrm{IC},v}(x;\\theta)\\big)^2\n$$\n$$\nE_{\\mathrm{BC}}(\\theta) = \\frac{1}{160} \\sum_{(x,t)\\in\\mathcal{X}_{\\mathrm{BC}}} \\big(p_{\\theta}(x,t)\\big)^2\n$$\nThe procedure for the solution involves implementing these formulas for each of the three test cases, calculating the four MSE values, and reporting them in the specified format. The calculations will be performed using vectorized operations for efficiency.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the mean-squared residuals for a PINN applied to the 1D wave equation\n    for three given sets of network parameters.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"W\": 3,\n            \"a\": [1.0, -2.0, 0.5],\n            \"b\": [1.5, 0.5, -1.0],\n            \"d\": [0.1, -0.2, 0.3],\n            \"w\": [0.7, -1.0, 0.3],\n            \"e\": 0.0, \"f\": 0.0, \"g\": 0.0\n        },\n        {\n            \"W\": 0,\n            \"a\": [], \"b\": [], \"d\": [], \"w\": [],\n            \"e\": 0.0, \"f\": 0.0, \"g\": 0.0\n        },\n        {\n            \"W\": 3,\n            \"a\": [3.0, -3.0, 1.0],\n            \"b\": [3.0, 3.0, -1.0],\n            \"d\": [-0.5, 0.25, 0.0],\n            \"w\": [0.2, -0.2, 0.1],\n            \"e\": 0.0, \"f\": 0.0, \"g\": 0.0\n        },\n    ]\n\n    # Generate collocation points\n    # PDE interior set\n    N_x_int, N_t_int = 20, 20\n    x_pde_coords = np.arange(1, N_x_int + 1) / (N_x_int + 1)\n    t_pde_coords = np.arange(1, N_t_int + 1) / (N_t_int + 1)\n    X_pde_grid, T_pde_grid = np.meshgrid(x_pde_coords, t_pde_coords)\n    x_pde = X_pde_grid.flatten()\n    t_pde = T_pde_grid.flatten()\n    N_PDE = len(x_pde)\n\n    # Initial condition sets\n    N_IC_p = 50\n    x_icp = np.linspace(0, 1, N_IC_p)\n    t_icp = np.zeros_like(x_icp)\n\n    N_IC_v = 50\n    x_icv = np.linspace(0, 1, N_IC_v)\n    t_icv = np.zeros_like(x_icv)\n\n    # Boundary condition set\n    N_t_bc = 80\n    t_bc_coords = np.linspace(0, 1, N_t_bc)\n    x_bc = np.concatenate([np.zeros(N_t_bc), np.ones(N_t_bc)])\n    t_bc = np.concatenate([t_bc_coords, t_bc_coords])\n    N_BC = len(x_bc)\n    \n    results = []\n    for case in test_cases:\n        W = case[\"W\"]\n        a = np.array(case[\"a\"])\n        b = np.array(case[\"b\"])\n        d = np.array(case[\"d\"])\n        w = np.array(case[\"w\"])\n        e = case[\"e\"]\n        f = case[\"f\"]\n        g = case[\"g\"]\n\n        # If W=0, the sum part of all expressions is zero.\n        # The vectorized implementation handles this naturally.\n        \n        # 1. E_PDE calculation\n        # Shape of x_pde and t_pde is (N_PDE,)\n        # Shapes of a,b,d,w are (W,)\n        # We need to broadcast them to (N_PDE, W)\n        phi_pde = x_pde[:, np.newaxis] * a[np.newaxis, :] + \\\n                  t_pde[:, np.newaxis] * b[np.newaxis, :] + \\\n                  d[np.newaxis, :]\n        tanh_phi_pde = np.tanh(phi_pde)\n        sech_phi_pde = 1.0 / np.cosh(phi_pde)\n        sech2_phi_pde = sech_phi_pde**2\n        \n        R_pde_sum_part = 2 * w[np.newaxis, :] * (a[np.newaxis, :]**2 - b[np.newaxis, :]**2) * \\\n                         sech2_phi_pde * tanh_phi_pde\n        R_pde = np.sum(R_pde_sum_part, axis=1)\n        E_pde = np.mean(R_pde**2)\n\n        # 2. E_IC,p calculation\n        phi_icp = x_icp[:, np.newaxis] * a[np.newaxis, :] + d[np.newaxis, :]\n        p_theta_icp_sum_part = w[np.newaxis, :] * np.tanh(phi_icp)\n        p_theta_icp = np.sum(p_theta_icp_sum_part, axis=1) + e * x_icp + f * t_icp + g\n        R_icp = p_theta_icp - np.sin(np.pi * x_icp)\n        E_icp = np.mean(R_icp**2)\n\n        # 3. E_IC,v calculation\n        phi_icv = x_icv[:, np.newaxis] * a[np.newaxis, :] + d[np.newaxis, :]\n        sech_phi_icv = 1.0 / np.cosh(phi_icv)\n        sech2_phi_icv = sech_phi_icv**2\n        \n        dt_p_theta_icv_sum_part = w[np.newaxis, :] * b[np.newaxis, :] * sech2_phi_icv\n        dt_p_theta_icv = np.sum(dt_p_theta_icv_sum_part, axis=1) + f\n        R_icv = dt_p_theta_icv # Target is 0\n        E_icv = np.mean(R_icv**2)\n\n        # 4. E_BC calculation\n        phi_bc = x_bc[:, np.newaxis] * a[np.newaxis, :] + \\\n                 t_bc[:, np.newaxis] * b[np.newaxis, :] + \\\n                 d[np.newaxis, :]\n        p_theta_bc_sum_part = w[np.newaxis, :] * np.tanh(phi_bc)\n        p_theta_bc = np.sum(p_theta_bc_sum_part, axis=1) + e * x_bc + f * t_bc + g\n        # Residual is p_theta_bc - 0\n        E_bc = np.mean(p_theta_bc**2)\n\n        results.append([E_pde, E_icp, E_icv, E_bc])\n\n    # Final print statement in the exact required format.\n    # The default str() representation of a list matches the required format '[item1, item2, ...]'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simply enforcing the PDE, the choice of neural network architecture itself is critical for successfully modeling physical phenomena. This is especially true for wave equations, which describe oscillatory solutions that are challenging for standard networks to represent. This exercise delves into the theoretical underpinnings of architectural design by comparing a Sinusoidal Representation Network (SIREN) with a traditional ReLU network . Through analytical derivation of their derivatives and an analysis of their Fourier spectra, you will discover why architectures with an intrinsic spectral bias, like SIRENs, are exceptionally well-suited for capturing the high-frequency details inherent in wave physics.",
            "id": "4134308",
            "problem": "Consider the application of Physics-Informed Neural Networks (PINNs) to the one-dimensional acoustic wavefield, where the predicted acoustic pressure $p(x)$ is parameterized by a neural network and must satisfy the governing linear wave dynamics. In this setting, Sinusoidal Representation Networks (SIREN) with sine activations are used to represent high-frequency wavefields. A Sinusoidal Representation Network (SIREN) is defined here as a single-hidden-layer mapping with sine activation that outputs\n$$\np_{\\theta}(x) \\;=\\; \\sum_{j=1}^{m} a_{j} \\,\\sin\\!\\big(\\omega_{0} \\,(w_{j}\\,x + b_{j})\\big),\n$$\nwhere $m$ is a positive integer, and $\\theta = \\{a_{j}, w_{j}, b_{j}\\}_{j=1}^{m}$ are real parameters; $\\omega_{0} > 0$ is a fixed angular frequency scale. For comparison, consider a network with Rectified Linear Unit (ReLU) activations, defined as\n$$\nr_{\\phi}(x) \\;=\\; \\sum_{j=1}^{n} c_{j}\\,\\max\\!\\big(0,\\, u_{j}\\,x + v_{j}\\big),\n$$\nwhere $n$ is a positive integer and $\\phi = \\{c_{j}, u_{j}, v_{j}\\}_{j=1}^{n}$ are real parameters. Assume $u_{j} \\neq 0$ for all $j$ so that the kink locations are well-defined.\n\nUse only fundamental laws and well-tested formulas, including the chain rule for differentiation, properties of the Dirac delta distribution, and the Fourier transform of basic functions and distributions. In particular, adopt the unitary angular-frequency Fourier transform convention\n$$\n\\widehat{f}(\\xi) \\;=\\; \\int_{-\\infty}^{\\infty} f(x)\\,\\exp(-\\,\\mathrm{i}\\,\\xi\\,x)\\,\\mathrm{d}x,\n$$\nand recall that for distributions the Fourier transform of the Heaviside step function $H(x)$ satisfies\n$$\n\\widehat{H}(\\xi) \\;=\\; \\pi\\,\\delta(\\xi)\\;+\\;\\mathcal{P}\\!\\left(\\frac{1}{\\mathrm{i}\\,\\xi}\\right),\n$$\nwhere $\\delta(\\cdot)$ is the Dirac delta distribution and $\\mathcal{P}(\\cdot)$ denotes Cauchy principal value.\n\nTasks:\n$1.$ Derive closed-form expressions for the first and second spatial derivatives $p_{\\theta}'(x)$ and $p_{\\theta}''(x)$.\n$2.$ For the SIREN derivative $p_{\\theta}'(x)$, compute the spectral support of $\\widehat{p_{\\theta}'}(\\xi)$ and characterize its content in terms of Dirac deltas.\n$3.$ For the ReLU derivative $r_{\\phi}'(x)$, express it in terms of Heaviside functions and determine the asymptotic decay of $|\\widehat{r_{\\phi}'}(\\xi)|$ as $|\\xi|\\to\\infty$.\n$4.$ Define the high-frequency tail energy of a function $f$ at cutoff $K>0$ as\n$$\nE_{f}(K) \\;=\\; \\int_{|\\xi|\\ge K} \\big|\\widehat{f}(\\xi)\\big|^{2}\\,\\mathrm{d}\\xi.\n$$\nLet $K$ be any real number strictly greater than $\\max_{1\\le j\\le m}\\big|\\omega_{0}\\,w_{j}\\big|$. Compute the exact value of the ratio\n$$\nR(K) \\;=\\; \\frac{E_{p_{\\theta}'}(K)}{E_{r_{\\phi}'}(K)}.\n$$\n\nYour final answer must be the single exact value of $R(K)$ as a real number. No rounding is required. No physical units are required for the ratio. Express the final answer without additional commentary, following the specified output structure.",
            "solution": "The user has asked for the computation of a ratio of high-frequency tail energies, $R(K)$, for the derivatives of two neural network models: a Sinusoidal Representation Network (SIREN) and a Rectified Linear Unit (ReLU) network. The solution proceeds by executing the specified tasks in order.\n\n**Task 1: Derivatives of the SIREN representation**\nThe SIREN output is defined as $p_{\\theta}(x) = \\sum_{j=1}^{m} a_{j} \\sin(\\omega_{0} (w_{j}x + b_{j}))$. The first and second spatial derivatives are found by applying the chain rule of differentiation.\n\nThe first derivative, $p_{\\theta}'(x)$, is:\n$$\np_{\\theta}'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( \\sum_{j=1}^{m} a_{j} \\sin(\\omega_{0} (w_{j}x + b_{j})) \\right) = \\sum_{j=1}^{m} a_{j} \\cos(\\omega_{0} (w_{j}x + b_{j})) \\cdot \\frac{\\mathrm{d}}{\\mathrm{d}x}(\\omega_{0} (w_{j}x + b_{j}))\n$$\n$$\np_{\\theta}'(x) = \\sum_{j=1}^{m} a_{j} \\omega_{0} w_{j} \\cos(\\omega_{0} w_{j}x + \\omega_{0} b_{j}).\n$$\nThe second derivative, $p_{\\theta}''(x)$, is the derivative of $p_{\\theta}'(x)$:\n$$\np_{\\theta}''(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( \\sum_{j=1}^{m} a_{j} \\omega_{0} w_{j} \\cos(\\omega_{0} w_{j}x + \\omega_{0} b_{j}) \\right) = \\sum_{j=1}^{m} a_{j} \\omega_{0} w_{j} \\left( -\\sin(\\omega_{0} w_{j}x + \\omega_{0} b_{j}) \\right) \\cdot (\\omega_{0} w_{j})\n$$\n$$\np_{\\theta}''(x) = -\\sum_{j=1}^{m} a_{j} (\\omega_{0} w_{j})^{2} \\sin(\\omega_{0} w_{j}x + \\omega_{0} b_{j}).\n$$\n\n**Task 2: Spectral analysis of the SIREN derivative**\nTo find the spectral support of $p_{\\theta}'(x)$, we compute its Fourier transform, $\\widehat{p_{\\theta}'}(\\xi)$, using the provided convention $\\widehat{f}(\\xi) = \\int_{-\\infty}^{\\infty} f(x)\\exp(-\\mathrm{i}\\xi x)\\mathrm{d}x$. With this convention, the Fourier transform of a complex exponential is $\\mathcal{F}\\{\\exp(\\mathrm{i}\\alpha x)\\}(\\xi) = 2\\pi\\delta(\\xi - \\alpha)$.\nWe express $p_{\\theta}'(x)$ using Euler's formula, $\\cos(\\alpha) = \\frac{1}{2}(\\exp(\\mathrm{i}\\alpha) + \\exp(-\\mathrm{i}\\alpha))$:\n$$\np_{\\theta}'(x) = \\sum_{j=1}^{m} \\frac{a_{j} \\omega_{0} w_{j}}{2} \\left[ \\exp(\\mathrm{i}(\\omega_{0} w_{j}x + \\omega_{0} b_{j})) + \\exp(-\\mathrm{i}(\\omega_{0} w_{j}x + \\omega_{0} b_{j})) \\right].\n$$\nApplying the Fourier transform and using its linearity:\n$$\n\\widehat{p_{\\theta}'}(\\xi) = \\sum_{j=1}^{m} \\frac{a_{j} \\omega_{0} w_{j}}{2} \\left[ \\exp(\\mathrm{i}\\omega_{0} b_{j}) \\mathcal{F}\\{\\exp(\\mathrm{i}\\omega_{0} w_{j}x)\\} + \\exp(-\\mathrm{i}\\omega_{0} b_{j}) \\mathcal{F}\\{\\exp(-\\mathrm{i}\\omega_{0} w_{j}x)\\} \\right](\\xi)\n$$\n$$\n\\widehat{p_{\\theta}'}(\\xi) = \\sum_{j=1}^{m} \\frac{a_{j} \\omega_{0} w_{j}}{2} \\left[ \\exp(\\mathrm{i}\\omega_{0} b_{j}) (2\\pi\\delta(\\xi - \\omega_{0} w_{j})) + \\exp(-\\mathrm{i}\\omega_{0} b_{j}) (2\\pi\\delta(\\xi + \\omega_{0} w_{j})) \\right]\n$$\n$$\n\\widehat{p_{\\theta}'}(\\xi) = \\pi \\sum_{j=1}^{m} a_{j} \\omega_{0} w_{j} \\left[ \\exp(\\mathrm{i}\\omega_{0} b_{j}) \\delta(\\xi - \\omega_{0} w_{j}) + \\exp(-\\mathrm{i}\\omega_{0} b_{j}) \\delta(\\xi + \\omega_{0} w_{j}) \\right].\n$$\nThe spectral support is the set of frequencies $\\xi$ where $\\widehat{p_{\\theta}'}(\\xi)$ is non-zero. The transform consists of a sum of Dirac delta distributions, so the support is the discrete set of frequencies where these distributions are located:\n$$\n\\text{supp}(\\widehat{p_{\\theta}'}) = \\{ \\pm \\omega_{0} w_{j} \\}_{j=1}^{m}.\n$$\nThe spectral content is composed entirely of these Dirac deltas.\n\n**Task 3: Spectral analysis of the ReLU derivative**\nThe ReLU network is given by $r_{\\phi}(x) = \\sum_{j=1}^{n} c_{j}\\max(0, u_{j}x + v_{j})$. The derivative of $\\max(0, z)$ with respect to $z$ is the Heaviside step function $H(z)$. Using the chain rule, the derivative $r_{\\phi}'(x)$ is:\n$$\nr_{\\phi}'(x) = \\sum_{j=1}^{n} c_{j}u_{j} H(u_{j}x + v_{j}).\n$$\nTo find the asymptotic decay of its Fourier transform, $\\widehat{r_{\\phi}'}(\\xi)$, we use the provided transform of the Heaviside function, $\\widehat{H}(\\xi) = \\pi\\delta(\\xi) + \\mathcal{P}(\\frac{1}{\\mathrm{i}\\xi})$, and the Fourier transform property for a shifted and scaled function, $\\mathcal{F}\\{f(ax+b)\\}(\\xi) = \\frac{1}{|a|}\\exp(\\mathrm{i}\\xi b/a)\\widehat{f}(\\xi/a)$.\nApplying this to $H(u_j x + v_j)$:\n$$\n\\widehat{H(u_j x + v_j)}(\\xi) = \\frac{1}{|u_j|} \\exp(\\mathrm{i}\\xi v_j/u_j) \\widehat{H}(\\xi/u_j) = \\frac{1}{|u_j|} \\exp(\\mathrm{i}\\xi v_j/u_j) \\left[\\pi\\delta(\\xi/u_j) + \\mathcal{P}\\left(\\frac{1}{\\mathrm{i}(\\xi/u_j)}\\right)\\right].\n$$\nUsing the scaling property of the delta function, $\\delta(\\alpha \\xi) = \\frac{1}{|\\alpha|}\\delta(\\xi)$, we get $\\delta(\\xi/u_j) = |u_j|\\delta(\\xi)$.\n$$\n\\widehat{H(u_j x + v_j)}(\\xi) = \\exp(\\mathrm{i}\\xi v_j/u_j) \\left[\\pi\\delta(\\xi) + \\frac{u_j}{|u_j|} \\mathcal{P}\\left(\\frac{1}{\\mathrm{i}\\xi}\\right)\\right] = \\pi\\delta(\\xi) + \\text{sgn}(u_j) \\exp(\\mathrm{i}\\xi v_j/u_j) \\mathcal{P}\\left(\\frac{1}{\\mathrm{i}\\xi}\\right).\n$$\nThe Fourier transform of the full derivative $r_{\\phi}'(x)$ is:\n$$\n\\widehat{r_{\\phi}'}(\\xi) = \\sum_{j=1}^{n} c_{j}u_{j} \\left[ \\pi\\delta(\\xi) + \\text{sgn}(u_j) \\exp(\\mathrm{i}\\xi v_j/u_j) \\mathcal{P}\\left(\\frac{1}{\\mathrm{i}\\xi}\\right) \\right] = \\left(\\pi \\sum_{j=1}^{n} c_j u_j\\right) \\delta(\\xi) + \\mathcal{P}\\left(\\frac{1}{\\mathrm{i}\\xi}\\right) \\sum_{j=1}^{n} c_j |u_j| \\exp(\\mathrm{i}\\xi v_j/u_j).\n$$\nFor $|\\xi|\\to\\infty$, the $\\delta(\\xi)$ term is zero and $\\mathcal{P}(\\frac{1}{\\mathrm{i}\\xi}) = \\frac{1}{\\mathrm{i}\\xi}$. The asymptotic behavior is:\n$$\n\\widehat{r_{\\phi}'}(\\xi) \\sim \\frac{1}{\\mathrm{i}\\xi} \\sum_{j=1}^{n} c_j |u_j| \\exp(\\mathrm{i}\\xi v_j/u_j) \\quad \\text{as } |\\xi| \\to \\infty.\n$$\nThe sum is a bounded function of $\\xi$. Therefore, the magnitude $|\\widehat{r_{\\phi}'}(\\xi)|$ has an asymptotic decay of $O(|\\xi|^{-1})$.\n\n**Task 4: Computation of the ratio $R(K)$**\nThe high-frequency tail energy is $E_{f}(K) = \\int_{|\\xi|\\ge K} |\\widehat{f}(\\xi)|^{2}\\,\\mathrm{d}\\xi$. We must compute the ratio $R(K) = \\frac{E_{p_{\\theta}'}(K)}{E_{r_{\\phi}'}(K)}$ under the condition $K > \\max_{1\\le j\\le m}|\\omega_{0}w_{j}|$.\n\nFirst, we calculate the numerator, $E_{p_{\\theta}'}(K)$. The support of $\\widehat{p_{\\theta}'}(\\xi)$ is the set $\\{\\pm \\omega_{0} w_{j}\\}_{j=1}^{m}$. The condition on $K$ ensures that $K$ is strictly greater than the magnitude of any frequency in this support. The domain of integration for the energy is $|\\xi| \\ge K$, which corresponds to the union of intervals $(-\\infty, -K] \\cup [K, \\infty)$. This domain does not contain any of the points from the support of $\\widehat{p_{\\theta}'}(\\xi)$. Consequently, $\\widehat{p_{\\theta}'}(\\xi)$ is identically zero for all $\\xi$ in the integration region. Thus, the integrand $|\\widehat{p_{\\theta}'}(\\xi)|^2$ is zero over the entire domain of integration. The tail energy is therefore:\n$$\nE_{p_{\\theta}'}(K) = \\int_{|\\xi|\\ge K} |0|^{2}\\,\\mathrm{d}\\xi = 0.\n$$\nNext, we consider the denominator, $E_{r_{\\phi}'}(K)$. Since $K>0$, the integration domain $|\\xi| \\ge K$ does not include $\\xi=0$, so the Dirac delta term in $\\widehat{r_{\\phi}'}(\\xi)$ does not contribute to the integral.\n$$\nE_{r_{\\phi}'}(K) = \\int_{|\\xi|\\ge K} \\left| \\frac{1}{\\mathrm{i}\\xi} \\sum_{j=1}^{n} c_j |u_j| \\exp(\\mathrm{i}\\xi v_j/u_j) \\right|^2 \\mathrm{d}\\xi = \\int_{|\\xi|\\ge K} \\frac{1}{\\xi^2} \\left| \\sum_{j=1}^{n} c_j |u_j| \\exp(\\mathrm{i}\\xi v_j/u_j) \\right|^2 \\mathrm{d}\\xi.\n$$\nThe term $|\\sum_{j=1}^{n} c_j |u_j| \\exp(\\mathrm{i}\\xi v_j/u_j)|^2$ is a non-negative function. For a non-trivial ReLU network (where not all parameters $c_j$ are zero), this sum is not identically zero. The integral of a non-negative function that is not identically zero over a set of positive measure is strictly positive. As the integral $\\int_{|\\xi|\\ge K} \\xi^{-2} \\mathrm{d}\\xi = 2/K$ is finite and positive, and the sum term is bounded, the energy $E_{r_{\\phi}'}(K)$ is well-defined and positive for any non-trivial $r_{\\phi}$.\n\nFinally, we compute the ratio $R(K)$:\n$$\nR(K) = \\frac{E_{p_{\\theta}'}(K)}{E_{r_{\\phi}'}(K)} = \\frac{0}{E_{r_{\\phi}'}(K)}.\n$$\nSince the numerator is $0$ and the denominator is positive, the ratio is exactly $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Training a PINN effectively often requires more than just a static set of collocation points. For problems with sharp gradients, shocks, or localized features, uniformly sampling the domain can be highly inefficient. This advanced practice introduces a powerful technique to overcome this limitation: adaptive refinement . You will first derive an error indicator based on the gradient of the physics residual and then implement an algorithm that uses this indicator to dynamically add more collocation points to regions where the network struggles to satisfy the PDE. This exercise demonstrates a crucial step toward creating robust and efficient PINN solvers for complex, real-world wave propagation problems.",
            "id": "4134321",
            "problem": "Consider the one-dimensional acoustic pressure field modeled by a Physics-Informed Neural Network (PINN) surrogate $p_{\\theta}(x,t)$ over the nondimensional space-time domain $x \\in [0,1]$ and $t \\in [0,1]$. The governing equation for small-amplitude acoustics in one dimension is the homogeneous linear wave equation\n$$\n\\frac{\\partial^2 p}{\\partial t^2}(x,t) - c^2 \\frac{\\partial^2 p}{\\partial x^2}(x,t) = 0,\n$$\nwhere $c$ is the nondimensional speed of sound. A PINN enforces this equation through minimizing the physics residual at collocation points. Define the physics residual for the surrogate $p_{\\theta}$ as\n$$\nR(x,t) = \\frac{\\partial^2 p_{\\theta}}{\\partial t^2}(x,t) - c^2 \\frac{\\partial^2 p_{\\theta}}{\\partial x^2}(x,t).\n$$\n\nStarting from first principles of the wave equation and standard differential calculus, derive a gradient-based error indicator that quantifies rapid spatio-temporal variation of the physics residual. The indicator must be constructed from the gradient of $R$ in space-time and must not rely on unphysical heuristics. Then, propose an algorithm that adaptively refines the collocation set by adding points near steep gradients of $p_{\\theta}$, identified via the gradient of $R$. The algorithm must be expressed in a way that is implementable on any modern programming platform. The derivation must begin from the definitions of the residual and the wave equation, and proceed using lawful calculus steps and standard numerical approximation concepts when necessary.\n\nFor concreteness and testability, take the surrogate to be the differentiable traveling Gaussian\n$$\np_{\\theta}(x,t) = A \\exp\\left(-\\frac{(x - v t)^2}{2 w^2}\\right),\n$$\nwith amplitude $A$, wave speed parameter $v$ (not necessarily equal to $c$), and width $w$. All quantities are nondimensional. Show how to obtain $\\frac{\\partial^2 p_{\\theta}}{\\partial t^2}$ and $\\frac{\\partial^2 p_{\\theta}}{\\partial x^2}$, then derive $R(x,t)$ and its gradient components $\\frac{\\partial R}{\\partial x}(x,t)$ and $\\frac{\\partial R}{\\partial t}(x,t)$. Define the error indicator at a collocation point $(x,t)$ by the Euclidean norm\n$$\n\\mathcal{E}(x,t) = \\left\\| \\nabla R(x,t) \\right\\|_2 = \\sqrt{\\left(\\frac{\\partial R}{\\partial x}(x,t)\\right)^2 + \\left(\\frac{\\partial R}{\\partial t}(x,t)\\right)^2}.\n$$\n\nDesign an adaptive refinement algorithm that:\n- Initializes a uniform Cartesian set of collocation points $\\{(x_i,t_j)\\}$ over $[0,1]\\times[0,1]$.\n- Evaluates $\\mathcal{E}(x_i,t_j)$ at all points.\n- Identifies a fraction of points with the largest $\\mathcal{E}$ values.\n- Adds new points near each identified point with offsets that shrink by a factor of $2$ per refinement iteration, ensuring all new points remain inside $[0,1]\\times[0,1]$.\n- Repeats for a specified number of refinement iterations.\n- Reports the ratio of the final maximum indicator to the initial maximum indicator,\n$$\n\\rho = \\frac{\\max \\mathcal{E}_{\\text{final}}}{\\max \\mathcal{E}_{\\text{initial}}},\n$$\nwhich is a nondimensional float.\n\nYour program must implement the derived formulas and the adaptive algorithm using exact analytic derivatives for $p_{\\theta}$ specified above. Use nondimensional units for all quantities. Angles are not used. The final answer for each test case must be the single float $\\rho$.\n\nTest Suite:\nUse the following four parameter sets to test different facets of the algorithm (happy path, steep gradients, smooth field, and a no-refinement boundary case). Each test case is given as a tuple $(A,c,v,w,N_x,N_t,f,I)$ where $N_x$ and $N_t$ are the initial counts of points along $x$ and $t$, $f$ is the fraction of points selected for refinement in each iteration, and $I$ is the number of refinement iterations:\n1. Case $1$: $(A,c,v,w,N_x,N_t,f,I) = (1.0, 1.0, 0.9, 0.12, 25, 25, 0.2, 2)$.\n2. Case $2$: $(A,c,v,w,N_x,N_t,f,I) = (1.0, 1.0, 0.7, 0.05, 25, 25, 0.3, 3)$.\n3. Case $3$: $(A,c,v,w,N_x,N_t,f,I) = (1.0, 1.0, 1.1, 0.30, 25, 25, 0.2, 2)$.\n4. Case $4$: $(A,c,v,w,N_x,N_t,f,I) = (1.0, 1.0, 0.8, 0.10, 25, 25, 0.0, 3)$.\n\nAnswer Specification:\n- For each test case, compute the float $\\rho$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite, rounded to $6$ decimal places (e.g., $[0.123456,0.654321,0.000001,1.000000]$).",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and provides a complete and consistent setup for a computational task. It is based on the fundamental linear wave equation and standard principles of differential calculus and numerical methods. The task is to derive an error indicator for a Physics-Informed Neural Network (PINN) surrogate, and then implement an adaptive refinement algorithm based on this indicator. The problem is deemed valid.\n\nThe solution proceeds in two stages: first, the analytical derivation of the required mathematical quantities, and second, the formulation and implementation of the adaptive refinement algorithm.\n\n**1. Analytical Derivation**\n\nThe core of the task is to derive the error indicator $\\mathcal{E}(x,t)$, which is defined as the Euclidean norm of the gradient of the physics residual $R(x,t)$.\n\nThe governing equation is the one-dimensional homogeneous wave equation:\n$$\n\\frac{\\partial^2 p}{\\partial t^2}(x,t) - c^2 \\frac{\\partial^2 p}{\\partial x^2}(x,t) = 0\n$$\nThe PINN surrogate function is given as a traveling Gaussian wave packet:\n$$\np_{\\theta}(x,t) = A \\exp\\left(-\\frac{(x - v t)^2}{2 w^2}\\right)\n$$\nTo simplify the differentiation, let $\\xi(x,t) = x - v t$. The surrogate function becomes $p_{\\theta}(\\xi) = A \\exp\\left(-\\frac{\\xi^2}{2 w^2}\\right)$. Using the chain rule, the partial derivatives with respect to $x$ and $t$ can be expressed in terms of the total derivative with respect to $\\xi$:\n$$\n\\frac{\\partial}{\\partial x} = \\frac{\\partial \\xi}{\\partial x} \\frac{d}{d \\xi} = (1) \\frac{d}{d \\xi} = \\frac{d}{d \\xi}\n$$\n$$\n\\frac{\\partial}{\\partial t} = \\frac{\\partial \\xi}{\\partial t} \\frac{d}{d \\xi} = (-v) \\frac{d}{d \\xi} = -v \\frac{d}{d \\xi}\n$$\nThis relationship, $\\frac{\\partial}{\\partial t} = -v \\frac{\\partial}{\\partial x}$, holds for any function of the form $f(x-vt)$.\n\nFirst, we compute the second partial derivatives of $p_{\\theta}$ required for the physics residual.\nThe first derivative with respect to $x$ is:\n$$\n\\frac{\\partial p_{\\theta}}{\\partial x} = \\frac{d p_{\\theta}}{d \\xi} = A \\exp\\left(-\\frac{\\xi^2}{2 w^2}\\right) \\left(-\\frac{2\\xi}{2w^2}\\right) = -p_{\\theta} \\frac{\\xi}{w^2}\n$$\nThe second derivative with respect to $x$ is found using the product rule:\n$$\n\\frac{\\partial^2 p_{\\theta}}{\\partial x^2} = \\frac{d}{d \\xi} \\left(-p_{\\theta} \\frac{\\xi}{w^2}\\right) = -\\frac{d p_{\\theta}}{d \\xi} \\frac{\\xi}{w^2} - p_{\\theta} \\frac{1}{w^2} = - \\left(-p_{\\theta} \\frac{\\xi}{w^2}\\right) \\frac{\\xi}{w^2} - \\frac{p_{\\theta}}{w^2} = p_{\\theta} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right)\n$$\nUsing the chain rule relationship, the second derivative with respect to $t$ is:\n$$\n\\frac{\\partial^2 p_{\\theta}}{\\partial t^2} = (-v)^2 \\frac{\\partial^2 p_{\\theta}}{\\partial x^2} = v^2 p_{\\theta} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right)\n$$\n\nNow, we can formulate the physics residual, $R(x,t)$:\n$$\nR(x,t) = \\frac{\\partial^2 p_{\\theta}}{\\partial t^2} - c^2 \\frac{\\partial^2 p_{\\theta}}{\\partial x^2} = v^2 p_{\\theta} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right) - c^2 p_{\\theta} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right)\n$$\n$$\nR(x,t) = (v^2 - c^2) p_{\\theta} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right)\n$$\nIf the surrogate's speed parameter $v$ equals the medium's speed of sound $c$, then $R(x,t)=0$, and the surrogate is an exact solution.\n\nNext, we derive the gradient of the residual, $\\nabla R(x,t) = \\left(\\frac{\\partial R}{\\partial x}, \\frac{\\partial R}{\\partial t}\\right)$. Again, we leverage the chain rule: $\\frac{\\partial R}{\\partial t} = -v \\frac{\\partial R}{\\partial x}$. We only need to find $\\frac{\\partial R}{\\partial x} = \\frac{dR}{d\\xi}$. Let $K = v^2-c^2$.\n$$\n\\frac{\\partial R}{\\partial x} = \\frac{d}{d\\xi} \\left[ K \\cdot p_{\\theta}(\\xi) \\cdot \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right) \\right]\n$$\nApplying the product rule:\n$$\n\\frac{\\partial R}{\\partial x} = K \\left[ \\frac{d p_{\\theta}}{d \\xi} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right) + p_{\\theta} \\frac{d}{d \\xi} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right) \\right]\n$$\nSubstituting the known derivatives $\\frac{d p_{\\theta}}{d \\xi} = -p_{\\theta} \\frac{\\xi}{w^2}$ and $\\frac{d}{d\\xi} \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right) = \\frac{2\\xi}{w^4}$:\n$$\n\\frac{\\partial R}{\\partial x} = K \\left[ \\left(-p_{\\theta} \\frac{\\xi}{w^2}\\right) \\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right) + p_{\\theta} \\left(\\frac{2\\xi}{w^4}\\right) \\right]\n$$\nFactoring out common terms $K$, $p_{\\theta}$, and $\\xi$:\n$$\n\\frac{\\partial R}{\\partial x} = K p_{\\theta} \\xi \\left[ -\\frac{1}{w^2}\\left(\\frac{\\xi^2}{w^4} - \\frac{1}{w^2}\\right) + \\frac{2}{w^4} \\right] = K p_{\\theta} \\xi \\left[ -\\frac{\\xi^2}{w^6} + \\frac{1}{w^4} + \\frac{2}{w^4} \\right]\n$$\n$$\n\\frac{\\partial R}{\\partial x} = K p_{\\theta} \\xi \\left( \\frac{3}{w^4} - \\frac{\\xi^2}{w^6} \\right) = K \\frac{p_{\\theta} \\xi}{w^4} \\left( 3 - \\frac{\\xi^2}{w^2} \\right)\n$$\nSubstituting back $K = v^2 - c^2$ and $\\xi = x-vt$:\n$$\n\\frac{\\partial R}{\\partial x}(x,t) = (v^2-c^2) A \\exp\\left(-\\frac{(x - v t)^2}{2 w^2}\\right) \\frac{x-vt}{w^4} \\left(3 - \\frac{(x-vt)^2}{w^2}\\right)\n$$\nAnd the temporal partial derivative is:\n$$\n\\frac{\\partial R}{\\partial t}(x,t) = -v \\frac{\\partial R}{\\partial x}(x,t)\n$$\n\nFinally, we construct the error indicator $\\mathcal{E}(x,t)$:\n$$\n\\mathcal{E}(x,t) = \\sqrt{\\left(\\frac{\\partial R}{\\partial x}\\right)^2 + \\left(\\frac{\\partial R}{\\partial t}\\right)^2} = \\sqrt{\\left(\\frac{\\partial R}{\\partial x}\\right)^2 + \\left(-v\\frac{\\partial R}{\\partial x}\\right)^2} = \\sqrt{\\left(\\frac{\\partial R}{\\partial x}\\right)^2 (1+v^2)}\n$$\n$$\n\\mathcal{E}(x,t) = \\left| \\frac{\\partial R}{\\partial x}(x,t) \\right| \\sqrt{1+v^2}\n$$\nThis expression is the analytical formula used in the algorithm.\n\n**2. Adaptive Refinement Algorithm**\n\nThe algorithm adaptively adds collocation points to regions where the error indicator $\\mathcal{E}(x,t)$ is large, signifying rapid spatio-temporal variation in the physics residual.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: An initial set of collocation points $P_0$ is created as a uniform Cartesian grid of size $N_x \\times N_t$ covering the domain $[0,1] \\times [0,1]$.\n2.  **Initial Evaluation**: The error indicator $\\mathcal{E}(x,t)$ is calculated for all points in $P_0$ using the derived analytical formula. The maximum value, $\\max \\mathcal{E}_{\\text{initial}}$, is stored.\n3.  **Iterative Refinement**: The following steps are repeated for a total of $I$ iterations. Let $P_k$ be the set of points at iteration $k$.\n    a.  **Selection**: The error indicator $\\mathcal{E}(x,t)$ is evaluated for all points in $P_k$. The number of points to refine is determined as $N_{\\text{refine}} = \\lfloor f \\cdot |P_k| \\rfloor$, where $f$ is the refinement fraction and $|P_k|$ is the number of points in the current set. The $N_{\\text{refine}}$ points with the largest $\\mathcal{E}$ values are selected.\n    b.  **Refinement**: For each selected point $(x_p, t_p)$, new points are added in its vicinity. The offset distance for adding new points shrinks with each iteration. We define a base offset related to the initial grid spacing, $d_0 = \\frac{1}{2(\\max(N_x, N_t)-1)}$. At iteration $i$ (for $i=1, \\dots, I$), the offset distance is $d_i = d_0 / 2^{i-1}$. Four new candidate points are generated at $(x_p \\pm d_i, t_p)$ and $(x_p, t_p \\pm d_i)$.\n    c.  **Domain Clamping and Uniqueness**: The coordinates of any new point falling outside the domain $[0,1] \\times [0,1]$ are clamped to the boundary values $0$ or $1$. The collection of all new points is combined with the existing set $P_k$, and duplicate points are removed to form the new set $P_{k+1}$.\n4.  **Final Evaluation**: After $I$ iterations, the final set of points is $P_{\\text{final}}$. The error indicator $\\mathcal{E}(x,t)$ is evaluated for all points in $P_{\\text{final}}$, and the maximum value, $\\max \\mathcal{E}_{\\text{final}}$, is determined.\n5.  **Metric Calculation**: The final result is the ratio $\\rho = \\frac{\\max \\mathcal{E}_{\\text{final}}}{\\max \\mathcal{E}_{\\text{initial}}}$. This metric quantifies the change in the maximum observed error indicator due to the adaptive refinement process. If no refinement occurs (e.g., $f=0$), $P_{\\text{final}} = P_0$, and $\\rho=1$.\n\nThis procedure provides a deterministic and implementable method for adaptively refining the collocation point set for a PINN based on the gradient of its physics residual.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the adaptive refinement algorithm for all test cases.\n    \"\"\"\n    test_cases = [\n        # (A, c, v, w, Nx, Nt, f, I)\n        (1.0, 1.0, 0.9, 0.12, 25, 25, 0.2, 2),\n        (1.0, 1.0, 0.7, 0.05, 25, 25, 0.3, 3),\n        (1.0, 1.0, 1.1, 0.30, 25, 25, 0.2, 2),\n        (1.0, 1.0, 0.8, 0.10, 25, 25, 0.0, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        rho = run_simulation(*case)\n        results.append(rho)\n\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\ndef error_indicator(points, A, c, v, w):\n    \"\"\"\n    Calculates the error indicator E(x,t) for an array of (x,t) points.\n    \n    Args:\n        points (np.ndarray): An (N, 2) array of (x, t) coordinates.\n        A, c, v, w (float): Parameters of the surrogate and wave equation.\n        \n    Returns:\n        np.ndarray: An (N,) array of error indicator values.\n    \"\"\"\n    x = points[:, 0]\n    t = points[:, 1]\n    \n    if w = 0:\n        return np.zeros_like(x)\n\n    xi = x - v * t\n    w_sq = w**2\n    \n    # Calculate dR/dx = (v^2-c^2) * A * exp(...) * ( (x-vt)/w^4 ) * (3 - (x-vt)^2/w^2)\n    # This is the spatial gradient of the physics residual.\n    \n    # This term is zero if v=c, making E zero everywhere.\n    term1 = v**2 - c**2\n    if term1 == 0:\n        return np.zeros_like(x)\n        \n    p_theta_no_A = np.exp(-xi**2 / (2 * w_sq))\n    \n    term2 = xi / w_sq**2\n    term3 = 3 - xi**2 / w_sq\n    \n    dR_dx = term1 * A * p_theta_no_A * term2 * term3\n    \n    # E = |dR/dx| * sqrt(1 + v^2)\n    E = np.abs(dR_dx) * np.sqrt(1 + v**2)\n    \n    return E\n\ndef run_simulation(A, c, v, w, Nx, Nt, f, I):\n    \"\"\"\n    Runs one instance of the adaptive refinement simulation.\n    \n    Args:\n        A, c, v, w (float): Parameters.\n        Nx, Nt (int): Initial grid dimensions.\n        f (float): Fraction of points to refine.\n        I (int): Number of refinement iterations.\n        \n    Returns:\n        float: The ratio rho = max(E_final) / max(E_initial).\n    \"\"\"\n    # 1. Initialization\n    x_coords = np.linspace(0, 1, Nx)\n    t_coords = np.linspace(0, 1, Nt)\n    xx, tt = np.meshgrid(x_coords, t_coords)\n    current_points = np.vstack([xx.ravel(), tt.ravel()]).T\n\n    # 2. Initial Evaluation\n    initial_E = error_indicator(current_points, A, c, v, w)\n    max_E_initial = np.max(initial_E)\n\n    if max_E_initial == 0:\n        # If the initial error is 0, it will remain 0. The ratio is 1 (no change).\n        return 1.0\n\n    # 3. Iterative Refinement\n    if I > 0 and f > 0.0:\n        # Base offset distance, related to initial grid resolution.\n        # N-1 intervals for N points.\n        base_offset = 0.5 / (max(Nx, Nt) - 1 if max(Nx, Nt) > 1 else 1)\n\n        for i in range(1, I + 1):\n            num_points = len(current_points)\n            num_refine = int(np.floor(f * num_points))\n\n            if num_refine == 0:\n                break\n            \n            # a. Selection\n            E_values = error_indicator(current_points, A, c, v, w)\n            # Get indices of the top `num_refine` points with largest E values.\n            refine_indices = np.argsort(E_values)[-num_refine:]\n            points_to_refine = current_points[refine_indices]\n\n            # b. Refinement\n            # Offset shrinks by a factor of 2 each iteration.\n            offset = base_offset / (2**(i - 1))\n            new_points_list = []\n            for p in points_to_refine:\n                px, pt = p[0], p[1]\n                new_points_list.append([px + offset, pt])\n                new_points_list.append([px - offset, pt])\n                new_points_list.append([px, pt + offset])\n                new_points_list.append([px, pt - offset])\n            \n            new_points = np.array(new_points_list)\n\n            # c. Domain Clamping and Uniqueness\n            # Ensure new points are within the [0, 1] x [0, 1] domain.\n            new_points = np.clip(new_points, 0, 1)\n\n            # Add new points to the existing set and remove duplicates.\n            current_points = np.vstack([current_points, new_points])\n            current_points = np.unique(current_points, axis=0)\n\n    # 4. Final Evaluation\n    final_points = current_points\n    final_E = error_indicator(final_points, A, c, v, w)\n    max_E_final = np.max(final_E)\n    \n    # 5. Metric Calculation\n    rho = max_E_final / max_E_initial\n    return rho\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}