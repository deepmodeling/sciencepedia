{
    "hands_on_practices": [
        {
            "introduction": "头相关传递函数 (HRTF) 本质上是定义在球面上的函数。为了在渲染中有效地使用它们，我们需要从离散的测量点构建一个连续的数学表示。球谐函数为球面上的函数提供了一个功能强大且标准正交的基。这项练习  将让你掌握将离散的HRTF样本转换为球谐函数表示的核心技能，这对于紧凑存储和高效插值至关重要，同时你也将学会如何分析和量化由实际采样限制引入的误差。",
            "id": "4125584",
            "problem": "考虑一个在角频率 $\\omega$ 处的复值头相关传递函数 (HRTF)，它被建模为单位球面上的函数 $H(\\omega,\\Omega)$，其中 $\\Omega$ 表示球面上的方向，由余纬 $\\theta \\in [0,\\pi]$ 和方位角 $\\phi \\in [0,2\\pi)$ 参数化，两者均以弧度为单位。令 $\\{Y_{n}^{m}(\\theta,\\phi)\\}$ 表示单位球面上的归一化正交复球谐函数基，采用 Condon-Shortley 相位约定，其归一化满足 $\\int_{S^{2}} Y_{n}^{m}(\\Omega) Y_{n'}^{m'*}(\\Omega)\\,d\\Omega = \\delta_{nn'}\\delta_{mm'}$，其中 $d\\Omega = \\sin\\theta\\,d\\theta\\,d\\phi$，$*$ 表示复共轭。对于一个固定的截断阶数 $N$，HRTF 的截断球谐展开为\n$$\nH(\\omega,\\Omega) \\approx \\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\alpha_{n}^{m}(\\omega)\\, Y_{n}^{m}(\\Omega).\n$$\n展开系数 $\\alpha_{n}^{m}(\\omega)$ 由球面内积定义\n$$\n\\alpha_{n}^{m}(\\omega) = \\int_{S^{2}} H(\\omega,\\Omega)\\, Y_{n}^{m*}(\\Omega)\\, d\\Omega.\n$$\n您必须从第一性原理出发，推导如何使用近似球面积分的求积权重 $\\{w_{i}\\}_{i=1}^{I}$，从离散方向样本 $\\{H(\\omega,\\Omega_{i})\\}_{i=1}^{I}$ 来近似 $\\alpha_{n}^{m}(\\omega)$。然后，分析当采样未能满足直到 $N$ 阶的求积精度时所产生的误差，包括方位角上的混叠机制和仰角-权重的不精确性。您的推导必须从球谐函数的归一化正交性以及将球面求积定义为面积分的近似开始，并且不得依赖于任何简便公式。\n\n为了进行数值验证，通过为最高达合成阶数 $K \\leq N$ 的阶数指定一组基准真相系数 $\\{\\alpha_{n}^{m}(\\omega)\\}$ 来构建一个合成的 HRTF $H(\\omega,\\Omega)$，然后在其球谐展开式的一组采样方向上评估 $H(\\omega,\\Omega)$。使用以下合成系数模型：\n$$\n\\alpha_{n}^{m}(\\omega) = \\exp\\!\\left(-\\left(\\frac{\\omega}{\\omega_{0}}\\right)^{2}\\right)\\,\\frac{(-1)^{m}}{n+1}\\,\\exp\\!\\big(j\\,0.2\\,(m+n)\\big),\n$$\n其中 $\\omega_{0} = 3000$ 单位为 $\\mathrm{rad/s}$，$j$ 表示虚数单位。此选择确保了系数为非平凡的复数，且没有病态对称性。所有角度必须以弧度为单位，所有角频率必须以 $\\mathrm{rad/s}$ 为单位。\n\n通过离散求积来近似系数\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{i=1}^{I} w_{i}\\, H(\\omega,\\Omega_{i})\\, Y_{n}^{m*}(\\Omega_{i}),\n$$\n用于两种类型的球面求积方案：\n- 余纬上使用高斯-勒让德求积，方位角上使用均匀梯形求积（记为 \"GL\" 类型）：令 $u=\\cos\\theta \\in [-1,1]$，其高斯-勒让德节点和权重为 $\\{u_{j},w_{j}^{(u)}\\}_{j=1}^{L_{\\theta}}$。令 $\\phi_{k} = \\frac{2\\pi k}{M_{\\phi}}$，其中 $k=0,1,\\dots,M_{\\phi}-1$，均匀方位角步长为 $\\Delta\\phi = \\frac{2\\pi}{M_{\\phi}}$。采样方向是积性网格 $\\{(\\theta_{j},\\phi_{k})\\}$，其中 $\\theta_{j}=\\arccos(u_{j})$，相应的权重为 $w_{jk}=w_{j}^{(u)}\\,\\Delta\\phi$。\n- 余纬上使用等角中点采样，方位角上使用均匀梯形求积（记为 \"EQ\" 类型）：令 $\\theta_{j} = \\frac{\\pi}{L_{\\theta}}\\left(j+\\frac{1}{2}\\right)$，其中 $j=0,1,\\dots,L_{\\theta}-1$，以及 $\\phi_{k} = \\frac{2\\pi k}{M_{\\phi}}$，其中 $k=0,1,\\dots,M_{\\phi}-1$。近似权重为 $w_{jk} = \\sin\\theta_{j}\\,\\Delta\\theta\\,\\Delta\\phi$，其中 $\\Delta\\theta = \\frac{\\pi}{L_{\\theta}}$ 和 $\\Delta\\phi = \\frac{2\\pi}{M_{\\phi}}$。\n\n对于下面的每个测试用例，计算在所有阶数 $n \\in \\{0,\\dots,N\\}$ 和次数 $m \\in \\{-n,\\dots,n\\}$ 上的相对均方根 (RMS) 系数误差：\n$$\n\\varepsilon_{\\mathrm{rel}} = \\sqrt{\\frac{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\widehat{\\alpha}_{n}^{m}(\\omega) - \\alpha_{n}^{m}(\\omega)\\right|^{2}}{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\alpha_{n}^{m}(\\omega)\\right|^{2}}}.\n$$\n将每个 $\\varepsilon_{\\mathrm{rel}}$ 报告为无量纲的浮点数。\n\n测试套件规范：\n- 用例 1（理想情况，旨在精确）：类型 \"GL\"，$N=3$，$K=3$，$L_{\\theta}=4$，$M_{\\phi}=7$，$\\omega=2000$ 单位 $\\mathrm{rad/s}$。\n- 用例 2（边缘情况，方位角欠采样混叠）：类型 \"GL\"，$N=3$，$K=3$，$L_{\\theta}=4$，$M_{\\phi}=3$，$\\omega=2000$ 单位 $\\mathrm{rad/s}$。\n- 用例 3（边缘情况，仰角-权重不精确性）：类型 \"EQ\"，$N=3$，$K=3$，$L_{\\theta}=4$，$M_{\\phi}=7$，$\\omega=2000$ 单位 $\\mathrm{rad/s}$。\n- 用例 4（边界情况，单极子）：类型 \"EQ\"，$N=0$，$K=0$，$L_{\\theta}=2$，$M_{\\phi}=3$，$\\omega=2000$ 单位 $\\mathrm{rad/s}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，\"[result1,result2,result3,result4]\"），其中每个 \"result\" 是对应测试用例的 $\\varepsilon_{\\mathrm{rel}}$，由默认的浮点数到字符串转换进行四舍五入。角度必须以弧度为单位，角频率必须以 $\\mathrm{rad/s}$ 为单位。输出是无量纲的。",
            "solution": "从离散样本近似球谐系数是在利用球面数据的领域（如声学、地球物理学和计算机图形学）中的一个核心任务。推导始于球谐变换及其通过求积进行离散近似的基本定义。\n\n令 $H(\\omega, \\Omega)$ 为单位球面 $S^2$ 上的一个复值函数，表示固定角频率 $\\omega$ 处的头相关传递函数 (HRTF)。方向 $\\Omega$ 由余纬 $\\theta \\in [0, \\pi]$ 和方位角 $\\phi \\in [0, 2\\pi)$ 参数化。函数 $H(\\omega, \\Omega)$ 可以在归一化正交复球谐函数基 $Y_n^m(\\Omega)$ 中展开为：\n$$\nH(\\omega, \\Omega) = \\sum_{n=0}^{\\infty}\\sum_{m=-n}^{n} \\alpha_{n}^{m}(\\omega)\\, Y_{n}^{m}(\\Omega)\n$$\n展开系数 $\\alpha_{n}^{m}(\\omega)$ 由球面上平方可积函数空间 $L^2(S^2)$ 上的内积确定：\n$$\n\\alpha_{n}^{m}(\\omega) = \\langle H, Y_n^m \\rangle = \\int_{S^2} H(\\omega,\\Omega)\\, Y_{n}^{m*}(\\Omega)\\, d\\Omega\n$$\n其中 $d\\Omega = \\sin\\theta\\,d\\theta\\,d\\phi$ 是曲面面积元，$*$ 表示复共轭。球谐函数的归一化正交性表示为：\n$$\n\\langle Y_{n'}^{m'}, Y_n^m \\rangle = \\int_{S^2} Y_{n'}^{m'}(\\Omega)\\, Y_{n}^{m*}(\\Omega)\\, d\\Omega = \\delta_{nn'}\\delta_{mm'}\n$$\n其中 $\\delta_{ij}$ 是克罗内克 δ (Kronecker delta)。\n\n在实践中，$H(\\omega, \\Omega)$ 通常仅在一组离散的采样方向 $\\{\\Omega_i\\}_{i=1}^I$ 上是已知的。用于计算 $\\alpha_{n}^{m}(\\omega)$ 的连续积分必须通过离散和来近似。这可以通过使用由一组采样点 $\\{\\Omega_i\\}$ 和相应权重 $\\{w_i\\}$ 定义的球面求积法则来实现：\n$$\n\\int_{S^2} f(\\Omega)\\, d\\Omega \\approx \\sum_{i=1}^{I} w_i f(\\Omega_i)\n$$\n将此应用于系数积分，我们得到离散球谐变换，从而产生估计系数 $\\widehat{\\alpha}_{n}^{m}(\\omega)$：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{i=1}^{I} w_i H(\\omega, \\Omega_i) Y_n^{m*}(\\Omega_i)\n$$\n为了分析此近似中的误差，我们假设真实的 HRTF 是带限的，其合成阶数为 $K$。这意味着其球谐展开是有限的：\n$$\nH(\\omega, \\Omega) = \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega)\\, Y_{k}^{l}(\\Omega)\n$$\n对于给定的分析阶数 $N$（通常 $N \\ge K$），我们通过将此展开式代入 $\\widehat{\\alpha}_{n}^{m}$ 的公式来计算估计系数 $\\widehat{\\alpha}_{n}^{m}$：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{i=1}^{I} w_i \\left( \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega)\\, Y_{k}^{l}(\\Omega_i) \\right) Y_n^{m*}(\\Omega_i)\n$$\n通过交换求和顺序，我们得到：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega) \\left( \\sum_{i=1}^{I} w_i Y_{k}^{l}(\\Omega_i) Y_n^{m*}(\\Omega_i) \\right)\n$$\n括号中的项是内积 $\\langle Y_k^l, Y_n^m \\rangle$ 的基于求积的近似：\n$$\n\\sum_{i=1}^{I} w_i Y_{k}^{l}(\\Omega_i) Y_n^{m*}(\\Omega_i) \\approx \\int_{S^2} Y_{k}^{l}(\\Omega) Y_n^{m*}(\\Omega) d\\Omega = \\delta_{kl}\\delta_{mn}\n$$\n两个球谐函数 $Y_{k}^{l}(\\Omega) Y_{n}^{m*}(\\Omega)$ 的乘积是一个最高阶为 $k+n$ 的球面多项式。如果一个求积法则能够精确地积分所有最高阶为 $D_{\\text{max}}$ 的球面多项式，则称该法则是精确到 $D_{\\text{max}}$ 阶的。如果我们的求积法则精确到 $K+N$ 阶，那么离散和将完全等于积分，因此：\n$$\n\\sum_{i=1}^{I} w_i Y_{k}^{l}(\\Omega_i) Y_n^{m*}(\\Omega_i) = \\delta_{kl}\\delta_{mn} \\quad \\text{for all } k \\le K, n \\le N\n$$\n在这种理想情况下，估计系数变为：\n$$\n\\widehat{\\alpha}_{n}^{m}(\\omega) = \\sum_{k=0}^{K}\\sum_{l=-k}^{k} \\alpha_{k}^{l}(\\omega)\\, \\delta_{kl}\\delta_{mn} = \\begin{cases} \\alpha_{n}^{m}(\\omega)  \\text{if } n \\le K \\\\ 0  \\text{if } n  K \\end{cases}\n$$\n这表明，如果求积精确到 $K+N$ 阶，则最高达 $K$ 阶的系数可以被完美恢复。对于 $n  K$ 的系数，由于带限假设，真实的 $\\alpha_n^m$ 为零，因此它们也被正确地识别为零。\n\n当求积不精确到 $K+N$ 阶时，就会产生误差。我们考虑两种积性求积方案。积性法则将 $d\\Omega = d u\\,d\\phi$（其中 $u=\\cos\\theta$）上的积分分解为两个一维积分。\n球谐函数的方位角部分是 $e^{im\\phi}$。乘积 $Y_{k}^{l}Y_{n}^{m*}$ 包含形式为 $e^{i(l-m)\\phi}$ 的方位角项。具有 $M_{\\phi}$ 个等距点的梯形法则对于 $|q|  M_{\\phi}$ 的情况可以精确积分 $e^{iq\\phi}$。$|l-m|$ 的最大值为 $K+N$（因为 $|l| \\le K$ 和 $|m| \\le N$）。因此，为了实现精确的方位角积分，我们需要 $M_{\\phi}  K+N$。如果违反此条件（例如，$M_{\\phi} \\le K+N$），则会发生方位角混叠：由于方位角基函数的离散傅里叶变换不再是正交的，因此会出现来自 $k \\ne n$ 或 $l \\ne m$ 的非零贡献。\n\n余纬部分涉及连带勒让德多项式乘积的积分，这些是关于 $u=\\cos\\theta$ 的多项式。被积函数的多项式阶数最多为 $K+N$。\n- **高斯-勒让德 (GL) 求积：** 一个 $L_{\\theta}$ 点的高斯-勒让德求积法则对于 $u$ 中最高达 $2L_{\\theta}-1$ 阶的多项式是精确的。为确保余纬部分的精确性，我们需要 $2L_{\\theta}-1 \\ge K+N$。\n- **等角 (EQ) 求积：** 此方案使用等距的 $\\theta_j$ 和权重 $w_{jk} \\propto \\sin\\theta_j$。这是积分 $\\int f(\\theta)\\sin\\theta\\,d\\theta$ 的一个简单黎曼和近似，并且不是为多项式精确性而设计的。它只是一种一阶方法，对于不是很平滑或非恒定的函数会引入显著误差，因为它甚至无法精确积分低阶的勒让德多项式。误差是由于权重和节点对于多项式积分的非最优性造成的，我们称之为仰角-权重不精确性。\n\n总而言之，$\\widehat{\\alpha}_{n}^{m}$ 中的总误差是以下几项的组合：\n1.  **方位角混叠**：如果 $M_{\\phi} \\le K+N$，则会发生，导致系数之间的泄漏。\n2.  **仰角-权重不精确性**：如果余纬求积不精确到 $K+N$ 阶，则会发生。这在 EQ 方案中是固有的，但在 GL 方案中可以通过足够数量的点（$2L_{\\theta}-1 \\ge K+N$）来避免。\n\n相对均方根误差 $\\varepsilon_{\\mathrm{rel}}$ 的计算为这些效应提供了一个定量度量。\n$$\n\\varepsilon_{\\mathrm{rel}} = \\sqrt{\\frac{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\widehat{\\alpha}_{n}^{m}(\\omega) - \\alpha_{n}^{m}(\\omega)\\right|^{2}}{\\sum_{n=0}^{N}\\sum_{m=-n}^{n} \\left|\\alpha_{n}^{m}(\\omega)\\right|^{2}}}\n$$\n对于基准真相系数 $\\alpha_n^m(\\omega)$，任何 $n  K$ 的项都为零。我们的计算必须反映这一点。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import sph_harm, roots_legendre\n\ndef solve():\n    \"\"\"\n    Solves the HRTF spherical harmonic coefficient estimation problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # (Type, N, K, L_theta, M_phi, omega)\n        (\"GL\", 3, 3, 4, 7, 2000.0),  # Case 1: Happy path, exactness aimed\n        (\"GL\", 3, 3, 4, 3, 2000.0),  # Case 2: Azimuthal undersampling\n        (\"EQ\", 3, 3, 4, 7, 2000.0),  # Case 3: Elevation-weight inexactness\n        (\"EQ\", 0, 0, 2, 3, 2000.0),  # Case 4: Monopole\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        quad_type, N, K, L_theta, M_phi, omega = case\n        omega_0 = 3000.0\n        j_unit = 1j\n\n        # Helper function to get a flat index from (n, m)\n        # sph_harm order is m, n\n        def nm_to_idx(n, m):\n            return n * n + n + m\n\n        # Step 1: Compute ground-truth coefficients alpha_n^m\n        # The problem asks to compute error up to degree N, so we need ground truth up to N.\n        # But the HRTF is synthesized from coeffs up to K. So, alpha_n^m = 0 for n > K.\n        max_deg_for_alpha = max(N, K)\n        num_coeffs_total = (max_deg_for_alpha + 1)**2\n        alpha_true = np.zeros(num_coeffs_total, dtype=np.complex128)\n\n        freq_term = np.exp(-(omega / omega_0)**2)\n        for n_k in range(K + 1):\n            for m_l in range(-n_k, n_k + 1):\n                idx = nm_to_idx(n_k, m_l)\n                alpha_true[idx] = freq_term * ((-1)**m_l / (n_k + 1)) * np.exp(j_unit * 0.2 * (m_l + n_k))\n        \n        # Step 2: Define sampling grid and weights\n        if quad_type == \"GL\":\n            # Gauss-Legendre in colatitude, trapezoidal in azimuth\n            u_nodes, u_weights = roots_legendre(L_theta)  # u = cos(theta)\n            theta_nodes = np.arccos(u_nodes)\n            phi_nodes = np.linspace(0, 2 * np.pi, M_phi, endpoint=False)\n            \n            delta_phi = 2 * np.pi / M_phi\n            weights = u_weights * delta_phi\n            \n            # Create meshgrid\n            phi_grid, theta_grid = np.meshgrid(phi_nodes, theta_nodes)\n            weights_grid = np.meshgrid(np.zeros(M_phi), weights)[1]\n\n        elif quad_type == \"EQ\":\n            # Equiangular in colatitude, trapezoidal in azimuth\n            delta_theta = np.pi / L_theta\n            theta_nodes = delta_theta * (np.arange(L_theta) + 0.5)\n            phi_nodes = np.linspace(0, 2 * np.pi, M_phi, endpoint=False)\n            \n            delta_phi = 2 * np.pi / M_phi\n            \n            # Create meshgrid and weights\n            phi_grid, theta_grid = np.meshgrid(phi_nodes, theta_nodes)\n            weights_grid = np.sin(theta_grid) * delta_theta * delta_phi\n        \n        else:\n            raise ValueError(\"Unknown quadrature type\")\n\n        # Step 3: Synthesize HRTF field H(omega, Omega_i) on the grid\n        H_samples = np.zeros_like(theta_grid, dtype=np.complex128)\n        for n_k in range(K + 1):\n            for m_l in range(-n_k, n_k + 1):\n                idx = nm_to_idx(n_k, m_l)\n                if np.abs(alpha_true[idx]) > 0:\n                    # scipy.special.sph_harm(m, n, azimuth, polar)\n                    Y_kl = sph_harm(m_l, n_k, phi_grid, theta_grid)\n                    H_samples += alpha_true[idx] * Y_kl\n\n        # Step 4: Approximate coefficients alpha_hat via quadrature\n        num_coeffs_analysis = (N + 1)**2\n        alpha_hat = np.zeros(num_coeffs_analysis, dtype=np.complex128)\n        \n        for n in range(N + 1):\n            for m in range(-n, n + 1):\n                # scipy.special.sph_harm(m, n, azimuth, polar)\n                # We need the complex conjugate Y_n^m*\n                Ynm_conj = np.conj(sph_harm(m, n, phi_grid, theta_grid))\n                \n                # Perform the weighted sum\n                integrand = H_samples * Ynm_conj * weights_grid\n                alpha_hat_nm = np.sum(integrand)\n                \n                idx = nm_to_idx(n, m)\n                alpha_hat[idx] = alpha_hat_nm\n\n        # Step 5: Calculate relative RMS error\n        # We need to compare over all degrees up to N\n        # Ground truth coefficients for n > K are zero.\n        alpha_true_for_err = np.zeros(num_coeffs_analysis, dtype=np.complex128)\n        \n        # Copy relevant ground truth coeffs\n        max_deg_for_copy = min(N,K)\n        for n in range(max_deg_for_copy + 1):\n            for m in range(-n, n + 1):\n                idx = nm_to_idx(n, m)\n                alpha_true_for_err[idx] = alpha_true[idx]\n\n        #\n        # E_rel = sqrt( sum(|alpha_hat - alpha_true|^2) / sum(|alpha_true|^2) )\n        # Sums are over n=0..N, m=-n..n\n        #\n        \n        diff_coeffs = alpha_hat - alpha_true_for_err\n        \n        numerator = np.sum(np.abs(diff_coeffs)**2)\n        denominator = np.sum(np.abs(alpha_true_for_err)**2)\n        \n        if denominator  1e-30: # handles cases where true coeffs are all zero\n            if numerator  1e-30:\n                error = 0.0\n            else:\n                error = np.inf\n        else:\n            error = np.sqrt(numerator / denominator)\n        \n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "完整的HRTF数据集非常庞大，这给实时应用和个性化带来了挑战。主成分分析 (PCA) 是一种强大的降维技术，能够识别数据集中的主要变化模式，从而建立高效的模型。这项练习  将指导你使用PCA方法来压缩HRTF数据集，并探索模型压缩率与感知效果之间的权衡。你将学习如何量化重构误差，并将其与双耳声渲染中的关键感知线索（如耳间级差和频谱失真）联系起来，从而深入理解模型精度对听觉保真度的影响。",
            "id": "4125598",
            "problem": "考虑一个头相关传递函数 (HRTF) 数据集，它表示为一个实值矩阵 $\\mathbf{X} \\in \\mathbb{R}^{M \\times N}$，其中行索引耳-频率单元，列索引声源方向。假设数据源于线性时不变传播，其非线性可忽略不计，并且通过对 $\\mathbf{X}$ 进行奇异值分解 (SVD) 来应用主成分分析。令 $\\mathbf{X} = \\mathbf{U}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top$ 表示其 SVD 分解，其中 $\\mathbf{U} \\in \\mathbb{R}^{M \\times M}$ 和 $\\mathbf{V} \\in \\mathbb{R}^{N \\times N}$ 是标准正交矩阵，且 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{M \\times N}$ 的对角线上有非负奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0$。将最佳秩-$k$ 近似 $\\mathbf{X}_k$ 定义为在所有秩-$k$ 矩阵中使重构误差的弗罗贝尼乌斯范数最小化的矩阵。从弗罗贝尼乌斯范数的标准正交不变性和 SVD 分解出发，推导最佳秩-$k$ 近似的相对重构误差（用奇异值表示），并为指定的 $k$ 计算该误差。\n\n为了将重构误差与感知结果相关联，请考虑以下双耳线索和音色度量：\n- 双耳声级差 (ILD)，为每个频率-方向单元定义为 $\\mathrm{ILD} = 20 \\log_{10}\\!\\left(\\frac{|H_L|}{|H_R|}\\right)$，单位为分贝 (dB)，其中 $|H_L|$ 和 $|H_R|$ 分别是左耳和右耳的幅度响应（无量纲的振幅比）。\n- 每只耳朵的对数谱距离 (LSD)，定义为所有单元上 $20 \\log_{10}\\!\\left(\\frac{|H_{\\text{ear}}^{\\text{orig}}|}{|H_{\\text{ear}}^{\\text{recon}}|}\\right)$ 的均方根，单位为分贝 (dB)，并对左耳和右耳的结果取平均以获得单个音色渲染的代理指标。\n\n假设以下经过充分测试的感知阈值：\n- ILD 最小可觉差 (JND)：$0.5$ dB。\n- LSD 音色渲染可闻阈值：$1.0$ dB。\n\n对于每个指定的 $k$，计算 $\\mathbf{X}_k$ 相对于 $\\mathbf{X}$ 的相对弗罗贝尼乌斯重构误差，并判断 ILD 均方根误差和平均 LSD 是否超过上述阈值。\n\n使用以下合成的、科学上合理的 HRTF 数据集。频率和方向如下：\n- 频率 (Hz)：$[500, 1500, 3000, 6000]$。\n- 方位角 (度)：$[-60, -15, 15, 60]$。\n\n通过为每个方向堆叠每个频率的左耳然后右耳的幅度响应（无量纲线性幅度）来构建 $\\mathbf{X}$。对于每个方向列 $j$，将 $|H_L|(\\text{freq}, j)$ 和 $|H_R|(\\text{freq}, j)$ 定义为：\n- 方向 $-60^\\circ$：\n  - $|H_L|$: $[1.20, 1.30, 1.50, 1.80]$,\n  - $|H_R|$: $[0.80, 0.70, 0.60, 0.50]$.\n- 方向 $-15^\\circ$：\n  - $|H_L|$: $[1.10, 1.15, 1.20, 1.30]$,\n  - $|H_R|$: $[0.95, 0.90, 0.85, 0.80]$.\n- 方向 $15^\\circ$：\n  - $|H_L|$: $[0.95, 0.90, 0.85, 0.80]$,\n  - $|H_R|$: $[1.10, 1.15, 1.20, 1.30]$.\n- 方向 $60^\\circ$：\n  - $|H_L|$: $[0.80, 0.70, 0.60, 0.50]$,\n  - $|H_R|$: $[1.20, 1.30, 1.50, 1.80]$.\n\n因此，$\\mathbf{X} \\in \\mathbb{R}^{8 \\times 4}$，其中每一列等于通过连接相应方向的 $|H_L|$ 四频率向量和 $|H_R|$ 四频率向量而形成的 $8 \\times 1$ 向量。\n\n对于测试套件中的每个秩 $k$，执行以下计算：\n1. 计算相对重构误差，即 $\\mathbf{X} - \\mathbf{X}_k$ 的弗罗贝尼乌斯范数除以 $\\mathbf{X}$ 的弗罗贝尼乌斯范数，结果为一个无量纲浮点数。\n2. 计算所有频率-方向单元上原始 ILD 和重构 ILD 之间的均方根误差 (dB)；然后确定一个布尔值，指示该误差是否超过 $0.5$ dB。\n3. 计算所有单元上双耳的平均 LSD (dB)；然后确定一个布尔值，指示该值是否超过 $1.0$ dB。\n\n角度必须以度为单位处理；幅度响应是无量纲的线性量；ILD 和 LSD 必须以分贝 (dB) 为单位进行计算和内部报告。最终程序输出必须将所有给定测试用例的结果汇总到单行中，格式为方括号内以逗号分隔的列表，其中每个测试用例的结果是一个三元组 $[\\text{relative\\_error}, \\text{ILD\\_exceeds}, \\text{LSD\\_exceeds}]$。相对误差使用浮点数，阈值超出情况使用布尔值。\n\n测试套件：\n- 用例 1：$k=0$（边界：无分量）。\n- 用例 2：$k=1$（强降维）。\n- 用例 3：$k=2$（中等降维）。\n- 用例 4：$k=4$（边界：满秩，等于 $\\min(M,N)$）。\n\n你的程序应生成单行输出，包含方括号内以逗号分隔的列表形式的结果（例如，“[[0.123456,True,False],[...],...]”）。",
            "solution": "该问题经评估有效。其科学基础是计算声学和线性代数的原理，特别是通过奇异值分解 (SVD) 将主成分分析 (PCA) 应用于头相关传递函数 (HRTF) 数据。该问题是适定的，提供了所有必要的数据、定义和约束，确保了唯一且可计算的解。语言客观、精确，设定无矛盾或歧义。\n\n我们首先为 HRTF 矩阵 $\\mathbf{X}$ 的降维和重构误差的计算建立理论基础。随后，我们定义感知度量并概述要遵循的计算过程。\n\n**1. 理论框架：PCA 与重构误差**\n\n问题将 HRTF 数据集建模为一个实值矩阵 $\\mathbf{X} \\in \\mathbb{R}^{M \\times N}$，其中 $M=8$ 是耳-频率单元的数量，$N=4$ 是声源方向的数量。通过对 $\\mathbf{X}$ 应用奇异值分解 (SVD) 来执行主成分分析 (PCA)：\n$$\n\\mathbf{X} = \\mathbf{U}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top\n$$\n在此，$\\mathbf{U} \\in \\mathbb{R}^{M \\times M}$ 和 $\\mathbf{V} \\in \\mathbb{R}^{N \\times N}$ 是标准正交矩阵，其列（主成分或基向量）分别为 $\\mathbf{u}_i$ 和 $\\mathbf{v}_i$。$\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{M \\times N}$ 是一个矩形对角矩阵，按降序包含非负奇异值 $\\sigma_i$，即 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r  0$，其中 $r = \\text{rank}(\\mathbf{X})$。\n\nEckart-Young-Mirsky 定理指出，$\\mathbf{X}$ 的最佳秩-$k$ 近似（表示为 $\\mathbf{X}_k$），即在所有秩为 $k$ 的矩阵 $\\mathbf{A}$ 中使误差的弗罗贝尼乌斯范数 $\\|\\mathbf{X} - \\mathbf{A}\\|_F$ 最小化的矩阵，由截断 SVD 给出：\n$$\n\\mathbf{X}_k = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top = \\mathbf{U}_k\\,\\boldsymbol{\\Sigma}_k\\,\\mathbf{V}_k^\\top\n$$\n其中 $\\mathbf{U}_k$ 和 $\\mathbf{V}_k$ 分别包含 $\\mathbf{U}$ 和 $\\mathbf{V}$ 的前 $k$ 列，$\\boldsymbol{\\Sigma}_k$ 是一个包含前 $k$ 个奇异值的 $k \\times k$ 对角矩阵。\n\n相对重构误差是误差矩阵的弗罗贝尼乌斯范数与原始矩阵的弗罗贝尼乌斯范数之比。弗罗贝尼乌斯范数 $\\|\\mathbf{A}\\|_F$ 在乘以标准正交矩阵时保持不变。原始矩阵 $\\mathbf{X}$ 的范数为：\n$$\n\\|\\mathbf{X}\\|_F^2 = \\|\\mathbf{U}\\,\\boldsymbol{\\Sigma}\\,\\mathbf{V}^\\top\\|_F^2 = \\|\\boldsymbol{\\Sigma}\\|_F^2 = \\sum_{i=1}^r \\sigma_i^2\n$$\n误差矩阵为 $\\mathbf{E}_k = \\mathbf{X} - \\mathbf{X}_k = \\sum_{i=k+1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top$。误差的范数为：\n$$\n\\|\\mathbf{X} - \\mathbf{X}_k\\|_F^2 = \\left\\| \\sum_{i=k+1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top \\right\\|_F^2 = \\sum_{i=k+1}^r \\sigma_i^2\n$$\n因此，相对重构误差由下式给出：\n$$\nE_{\\text{rel}}(k) = \\frac{\\|\\mathbf{X} - \\mathbf{X}_k\\|_F}{\\|\\mathbf{X}\\|_F} = \\sqrt{\\frac{\\sum_{i=k+1}^r \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2}}\n$$\n\n**2. 感知度量与评估**\n\n为了评估近似的感知影响，我们评估两个标准度量：\n\n-   **双耳声级差 (ILD)**：对于每个频率-方向单元，ILD 定义为（单位：分贝 (dB)）：\n    $$\n    \\mathrm{ILD} = 20 \\log_{10}\\!\\left(\\frac{|H_L|}{|H_R|}\\right)\n    $$\n    其中 $|H_L|$ 和 $|H_R|$ 分别是左耳和右耳的幅度响应。感知误差通过所有单元上原始 ILD 值和重构 ILD 值之间的均方根 (RMS) 误差来量化。如果此误差超过 $0.5$ dB 的最小可觉差 (JND)，则设置一个布尔标志。\n\n-   **对数谱距离 (LSD)**：该度量量化了音色渲染。对于每只耳朵，它是所有单元上对数谱差的均方根：\n    $$\n    \\mathrm{LSD}_{\\text{ear}} = \\sqrt{\\text{mean}\\left( \\left( 20 \\log_{10}\\!\\left(\\frac{|H_{\\text{ear}}^{\\text{orig}}|}{|H_{\\text{ear}}^{\\text{recon}}|}\\right) \\right)^2 \\right)}\n    $$\n    最终的 LSD 度量是左耳和右耳 LSD 值的平均值。如果该平均值超过 $1.0$ dB 的可闻阈值，则设置一个布尔标志。\n\n当重构的幅度 $|H^{\\text{recon}}|$ 为零或负数时，会出现一个关键的数值问题，导致对数无定义。为确保稳定性，在计算 ILD 和 LSD 之前，我们将所有重构的幅度裁剪到一个小的正底值 $\\epsilon$（例如，浮点数的机器 epsilon）。\n\n**3. 算法步骤**\n\n对于每个秩 $k \\in \\{0, 1, 2, 4\\}$，执行以下步骤：\n\n1.  **数据矩阵构建**：$8 \\times 4$ 矩阵 $\\mathbf{X}$ 由提供的左耳 ($|H_L|$) 和右耳 ($|H_R|$) 幅度响应向量组装而成。$\\mathbf{X}$ 的每一列对应一个声源方向，由垂直堆叠 $4$ 频率的 $|H_L|$ 向量和 $4$ 频率的 $|H_R|$ 向量形成。\n\n2.  **SVD 计算**：计算 $\\mathbf{X}$ 的 SVD 以找到矩阵 $\\mathbf{U}$、$\\boldsymbol{\\Sigma}$（表示为奇异值向量 $\\boldsymbol{\\sigma}$）和 $\\mathbf{V}^\\top$。\n\n3.  **秩-k 重构**：构建近似矩阵 $\\mathbf{X}_k$。对于 $k=0$，$\\mathbf{X}_0$ 是一个零矩阵。对于 $k  0$，$\\mathbf{X}_k$ 使用前 $k$ 个奇异值和相应的奇异向量构成。对于 $k=4$，即给定矩阵的满秩，$\\mathbf{X}_4$ 将是 $\\mathbf{X}$ 的近乎完美的重构。\n\n4.  **误差计算**：\n    a.  计算相对弗罗贝尼乌斯误差 $E_{\\text{rel}}(k)$。\n    b.  将原始矩阵 $\\mathbf{X}$ 和重构矩阵 $\\mathbf{X}_k$ 分割成它们的左耳和右耳子矩阵：$\\mathbf{H}_L^{\\text{orig}}$、$\\mathbf{H}_R^{\\text{orig}}$、$\\mathbf{H}_L^{\\text{recon}}$ 和 $\\mathbf{H}_R^{\\text{recon}}$。\n    c.  将重构的幅度矩阵裁剪为不小于一个小的正常数 $\\epsilon$。\n    d.  计算原始 ILD 和重构 ILD 矩阵，并求出 RMS 误差。将其与 $0.5$ dB 阈值进行比较。\n    e.  计算左耳 LSD 和右耳 LSD，并计算它们的平均值。将其与 $1.0$ dB 阈值进行比较。\n\n5.  **结果聚合**：为每个 $k$ 值收集计算出的相对误差以及用于指示 ILD 和 LSD 阈值是否超出的两个布尔标志。\n\n此过程提供了一个定量评估，说明基于 PCA 的降维如何影响 HRTF 表示的数值准确性和感知保真度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the HRTF analysis problem by performing SVD-based PCA,\n    calculating reconstruction errors, and evaluating perceptual metrics.\n    \"\"\"\n    # Define the HRTF magnitude data as per the problem statement.\n    # Each list corresponds to the 4 specified frequencies.\n    # Directions: -60, -15, 15, 60 degrees.\n    data = {\n        -60: {\n            'L': [1.20, 1.30, 1.50, 1.80],\n            'R': [0.80, 0.70, 0.60, 0.50]\n        },\n        -15: {\n            'L': [1.10, 1.15, 1.20, 1.30],\n            'R': [0.95, 0.90, 0.85, 0.80]\n        },\n        15: {\n            'L': [0.95, 0.90, 0.85, 0.80],\n            'R': [1.10, 1.15, 1.20, 1.30]\n        },\n        60: {\n            'L': [0.80, 0.70, 0.60, 0.50],\n            'R': [1.20, 1.30, 1.50, 1.80]\n        }\n    }\n    \n    directions = [-60, -15, 15, 60]\n    \n    # Construct the M x N data matrix X, where M=8 and N=4.\n    columns = []\n    for az in directions:\n        h_l = np.array(data[az]['L'])\n        h_r = np.array(data[az]['R'])\n        column = np.concatenate((h_l, h_r))\n        columns.append(column)\n    X = np.stack(columns, axis=1)\n\n    # Perform Singular Value Decomposition (SVD).\n    # Since M > N, full_matrices=False is efficient.\n    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    \n    # Calculate the Frobenius norm of the original matrix.\n    norm_X = np.linalg.norm(X, 'fro')\n\n    # Define perceptual thresholds.\n    ild_jnd = 0.5  # dB\n    lsd_threshold = 1.0  # dB\n    \n    # Small epsilon for numerical stability of log operations.\n    epsilon = np.finfo(float).eps\n\n    # Separate original data into Left and Right ear matrices (4 freqs x 4 dirs).\n    num_freqs = 4\n    X_L_orig = X[:num_freqs, :]\n    X_R_orig = X[num_freqs:, :]\n\n    # Pre-calculate original ILD.\n    # Clip to avoid division by zero, although not expected for original data.\n    X_R_orig_clipped = np.maximum(X_R_orig, epsilon)\n    ILD_orig = 20 * np.log10(X_L_orig / X_R_orig_clipped)\n    \n    test_cases = [0, 1, 2, 4]\n    results = []\n\n    for k in test_cases:\n        # 1. Reconstruct the rank-k approximation matrix X_k.\n        if k == 0:\n            X_k = np.zeros_like(X)\n        else:\n            # Reconstruct from the first k components.\n            U_k = U[:, :k]\n            s_k = s[:k]\n            Vt_k = Vt[:k, :]\n            X_k = (U_k * s_k) @ Vt_k\n\n        # 2. Compute the relative Frobenius reconstruction error.\n        relative_error = np.linalg.norm(X - X_k, 'fro') / norm_X\n        # An alternative using singular values directly:\n        # if k  len(s):\n        #     error_norm_sq = np.sum(s[k:]**2)\n        #     total_norm_sq = np.sum(s**2)\n        #     relative_error = np.sqrt(error_norm_sq / total_norm_sq)\n        # else:\n        #     relative_error = 0.0\n\n        # 3. Compute ILD RMS error.\n        # Separate reconstructed data into Left and Right ear matrices.\n        X_L_recon = X_k[:num_freqs, :]\n        X_R_recon = X_k[num_freqs:, :]\n        \n        # Clip reconstructed magnitudes to avoid log(0) or log(negative).\n        X_L_recon_clipped = np.maximum(X_L_recon, epsilon)\n        X_R_recon_clipped = np.maximum(X_R_recon, epsilon)\n        \n        # Compute reconstructed ILD.\n        ILD_recon = 20 * np.log10(X_L_recon_clipped / X_R_recon_clipped)\n        \n        # Calculate ILD RMS error and check against JND.\n        ild_rms_error = np.sqrt(np.mean((ILD_orig - ILD_recon)**2))\n        ild_exceeds = ild_rms_error > ild_jnd\n\n        # 4. Compute average LSD.\n        # Clip original magnitudes for ratio computation (best practice, though not needed here).\n        X_L_orig_clipped = np.maximum(X_L_orig, epsilon)\n        X_R_orig_clipped = np.maximum(X_R_orig, epsilon)\n        \n        # Calculate Log-Spectral Distance (LSD) for each ear.\n        lsd_L = np.sqrt(np.mean((20 * np.log10(X_L_orig_clipped / X_L_recon_clipped))**2))\n        lsd_R = np.sqrt(np.mean((20 * np.log10(X_R_orig_clipped / X_R_recon_clipped))**2))\n        \n        # Calculate average LSD and check against threshold.\n        avg_lsd = (lsd_L + lsd_R) / 2.0\n        lsd_exceeds = avg_lsd > lsd_threshold\n\n        # Store the results for this k.\n        results.append([relative_error, ild_exceeds, lsd_exceeds])\n\n    # Format and print the final output as a single line.\n    # The map(str, ...) converts each inner list to its string representation.\n    # The join and f-string formatting matches the required output.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "开发一个新的HRTF模型（例如个性化模型）是一回事，但要证明它在感知上优于通用模型则需要严谨的科学方法。统计假设检验是从主观听音实验中得出有意义结论的基石。这项最终的练习  将带你从模型构建转向效果评估，让你掌握统计验证这项关键技能。你将分析来自模拟听音测试的配对数据，学习如何根据数据特性选择合适的统计检验方法，并最终判断个性化HRTF是否带来了显著的感知提升。",
            "id": "4125608",
            "problem": "您将获得个性化头部相关传递函数 (HRTFs) 与通用 HRTFs 在双耳渲染中的双盲被试内评估的配对测量数据。在每个配对样本中，第一个值对应同一听者和条件下使用通用 HRTF 的性能，第二个值对应使用个性化 HRTF 的性能。目标是根据配对差异的正态性评估，为每个指标选择合适的单边配对假设检验，以评估个性化是否比通用 HRTF 带来了统计上显著的改进。\n\n请使用以下基本原则进行假设检验和检验方法选择：\n- 样本 $i$ 的配对差异定义为 $d_i = x_i - y_i$，其中 $x_i$ 和 $y_i$ 分别是通用和个性化条件下的配对测量值，并且 $d_i$ 的符号选择使得改进对应于 $d_i  0$。\n- 在原假设 $H_0$ 下，对于 Student 配对 $t$ 检验，差异的均值为零；对于 Wilcoxon 符号秩检验，差异的中位数为零。\n- 当样本量 $n \\geq 8$ 时，首先使用 Shapiro–Wilk 检验在显著性水平 $\\alpha_\\text{norm} = 0.05$ 下评估差异的正态性来选择检验方法。如果正态性未被拒绝（即 Shapiro–Wilk 的 p 值大于或等于 $\\alpha_\\text{norm}$），则使用配对 $t$ 检验。否则，使用 Wilcoxon 符号秩检验。对于 $n  8$ 的情况，使用 Wilcoxon 符号秩检验。将全为零的差异视为不显著，p 值为 $1.0$。\n- 使用对应于改进 ($d_i  0$) 的单边备择假设。对于数值越低越好的指标（例如误差或失真），定义 $d_i = x_i - y_i$ 以使改进产生正差异。对于数值越高越好的指标（例如质量评分），定义 $d_i = y_i - x_i$ 以使改进产生正差异。\n\n对于每个测试用例，计算 p 值并确定在 $\\alpha = 0.05$ 水平下改进是否具有统计显著性。使用代码报告所用的检验方法：Student 配对 $t$ 检验为 $0$，Wilcoxon 符号秩检验为 $1$。\n\n物理和数值单位：\n- 定位误差角度必须以度为单位处理；在描述数据时以度报告角度（所有提供的角度值均已是度）。\n- 频谱失真必须以分贝为单位处理；在描述数据时以分贝报告数值（所有提供的分贝值均已是分贝）。\n- 质量评分是在 $0$ 到 $100$ 范围内的无单位数值。\n- 所有假设检验决策和 p 值都是无单位的。\n\n您的程序必须处理以下测试套件。对于每个测试用例，程序必须：\n- 以正确的方向（数值越低越好或数值越高越好）构建配对差异。\n- 当 $n \\geq 8$ 时，对差异应用 Shapiro–Wilk 正态性检验以选择配对 $t$ 检验或 Wilcoxon 符号秩检验；当 $n  8$ 时，使用 Wilcoxon 符号秩检验。\n- 计算改进的单边 p 值。\n- 在 $\\alpha = 0.05$ 水平下判断显著性。\n- 将最终的 p 值四舍五入到六位小数。\n\n测试套件：\n1. 定位误差（度），数值越低越好。每个听者的通用与个性化对比：\n   - 通用：$\\{12.4, 10.1, 9.8, 14.3, 11.7, 13.0, 10.9, 12.1, 9.5, 15.2, 13.6, 11.3, 12.8, 10.5, 11.9, 9.7, 12.2, 13.1, 10.4, 11.5, 12.0, 14.0, 13.4, 10.8\\}$\n   - 个性化：$\\{8.7, 7.9, 7.5, 10.2, 8.8, 9.3, 7.6, 8.9, 6.8, 11.1, 9.7, 8.3, 9.4, 7.7, 8.6, 7.0, 9.1, 9.8, 7.9, 8.5, 9.2, 10.5, 9.9, 7.8\\}$\n   - 角度单位：度。\n2. 频谱失真（分贝），数值越低越好：\n   - 通用：$\\{5.6, 4.9, 6.1, 5.2, 4.7, 5.8, 6.3, 5.0, 5.4, 4.8, 6.0, 5.5\\}$\n   - 个性化：$\\{4.9, 4.4, 5.5, 4.8, 4.3, 5.1, 5.7, 4.7, 5.0, 4.5, 5.4, 5.1\\}$\n   - 单位：分贝。\n3. 外部化质量评分（$0$ 到 $100$），数值越高越好：\n   - 通用：$\\{62, 58, 70, 65, 60, 72, 68, 61, 64, 59, 66, 67, 63, 71, 69, 60, 62, 64\\}$\n   - 个性化：$\\{75, 72, 82, 78, 74, 85, 80, 73, 76, 70, 79, 81, 77, 84, 83, 73, 76, 78\\}$\n   - 单位：在 $\\{0, \\dots, 100\\}$ 上的无单位评分。\n4. 定位误差（度），数值越低越好，极小变化（边界情况）：\n   - 通用：$\\{10.2, 9.8, 10.0, 9.7, 10.1, 10.3, 9.9, 10.0, 9.6, 10.2\\}$\n   - 个性化：$\\{10.1, 9.9, 10.1, 9.8, 10.2, 10.2, 10.0, 10.1, 9.7, 10.3\\}$\n   - 角度单位：度。\n5. 外部化质量评分（$0$ 到 $100$），数值越高越好，带有结的极小样本：\n   - 通用：$\\{70, 75, 80\\}$\n   - 个性化：$\\{70, 78, 82\\}$\n   - 单位：在 $\\{0, \\dots, 100\\}$ 上的无单位评分。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，并且本身是一个形如 $[p\\_value, decision, test\\_code]$ 的三元素列表。其中，$p\\_value$ 是一个四舍五入到六位小数的浮点数，$decision$ 是一个布尔值，指示在 $\\alpha = 0.05$ 水平下改进是否显著，$test\\_code$ 是一个整数，Student 配对 $t$ 检验为 $0$，Wilcoxon 符号秩检验为 $1$。例如，输出应类似于 $\\big[[p\\_1, d\\_1, t\\_1], [p\\_2, d\\_2, t\\_2], \\dots\\big]$。\n\n所有角度必须解释为度，所有频谱失真解释为分贝，所有评分解释为无单位整数。不应读取任何外部输入；直接在程序中使用提供的测试套件。",
            "solution": "该问题要求进行一系列单边配对假设检验，以确定个性化头部相关传递函数 (HRTFs) 是否比通用 HRTF 提供了统计上显著的改进。分析基于来自被试内实验的配对数据。任务的核心是为每个数据集选择合适的统计检验，执行检验并解释结果。\n\n基本步骤包括以下几步：\n1.  **构建配对差异**：对于每对测量值 $(x_i, y_i)$（分别代表通用和个性化条件），计算差异 $d_i$。差异的符号定义为正值 $d_i  0$ 对应于改进。\n    -   对于数值越低越好的指标（例如误差），差异为 $d_i = x_i - y_i$。\n    -   对于数值越高越好的指标（例如质量评分），差异为 $d_i = y_i - x_i$。\n\n2.  **陈述假设**：原假设 $H_0$ 假定个性化没有带来改进。备择假设 $H_A$ 假定有显著改进。\n    -   $H_0$：差异的集中趋势（均值或中位数）为零。\n    -   $H_A$：差异的集中趋势大于零。\n\n3.  **选择统计检验**：选择参数 Student 配对 $t$ 检验还是非参数 Wilcoxon 符号秩检验，取决于配对差异 $d_i$ 的分布。\n    -   使用 Shapiro-Wilk 检验在显著性水平 $\\alpha_{\\text{norm}} = 0.05$ 下评估差异的正态性。该检验仅在样本量 $n$ 至少为 $8$ 时应用。\n    -   如果 $n \\ge 8$ 且 Shapiro-Wilk 检验的 p 值大于或等于 $\\alpha_{\\text{norm}}$，则不拒绝正态性的原假设，并选择 Student 配对 $t$ 检验。检验代码为 $0$。\n    -   如果 $n \\ge 8$ 且 Shapiro-Wilk 检验的 p 值小于 $\\alpha_{\\text{norm}}$，则数据被认为不服从正态分布，并选择 Wilcoxon 符号秩检验。检验代码为 $1$。\n    -   如果样本量很小，$n  8$，则默认使用 Wilcoxon 符号秩检验。检验代码为 $1$。\n\n4.  **计算 p 值并做出决策**：执行所选的单边假设检验。将得到的 p 值与显著性水平 $\\alpha = 0.05$ 进行比较。如果 p 值小于 $\\alpha$，则拒绝原假设，支持备择假设，表明存在统计上显著的改进。\n\n一种特殊情况是当所有差异 $d_i$ 均为零时，此时结果被视为不显著，p 值为 $1.0$。\n\n我们现在将此方法应用于所提供的五个测试用例中的每一个。\n\n**测试用例 1：定位误差**\n-   **指标**：定位误差（度），一个数值越低越好的指标。\n-   **数据**：$n=24$ 组配对测量值。\n-   **差异**：$d_i = x_i - y_i$（通用 - 个性化）。差异根据所提供的数据计算得出。\n-   **检验选择**：样本量为 $n=24$，满足 $\\ge 8$。我们对差异 $d_i$ 进行 Shapiro-Wilk 检验：\n    $$d = \\{3.7, 2.2, 2.3, 4.1, 2.9, 3.7, 3.3, 3.2, 2.7, 4.1, 3.9, 3.0, 3.4, 2.8, 3.3, 2.7, 3.1, 3.3, 2.5, 3.0, 2.8, 3.5, 3.5, 3.0\\}$$\n    Shapiro-Wilk 检验得出 p 值约为 $0.517$。由于 $0.517 \\ge \\alpha_{\\text{norm}} = 0.05$，我们不拒绝正态性的原假设。因此，选择 Student 配对 $t$ 检验。检验代码为 $0$。\n-   **假设检验**：我们执行一个单边配对 $t$ 检验（$H_A: \\mu_d  0$）。这得出的 p 值约为 $1.13 \\times 10^{-14}$。\n-   **决策**：p 值远小于 $\\alpha = 0.05$。改进具有统计显著性。\n-   **结果**：$[0.000000, \\text{True}, 0]$。\n\n**测试用例 2：频谱失真**\n-   **指标**：频谱失真（分贝），一个数值越低越好的指标。\n-   **数据**：$n=12$ 组配对测量值。\n-   **差异**：$d_i = x_i - y_i$。\n-   **检验选择**：样本量为 $n=12 \\ge 8$。对差异应用 Shapiro-Wilk 检验：\n    $$d = \\{0.7, 0.5, 0.6, 0.4, 0.4, 0.7, 0.6, 0.3, 0.4, 0.3, 0.6, 0.4\\}$$\n    该检验得出 p 值约为 $0.106$。由于 $0.106 \\ge 0.05$，我们选择 Student 配对 $t$ 检验。检验代码为 $0$。\n-   **假设检验**：单边配对 $t$ 检验得出的 p 值约为 $1.01 \\times 10^{-7}$。\n-   **决策**：p 值小于 $\\alpha = 0.05$。改进具有统计显著性。\n-   **结果**：$[0.000001, \\text{True}, 0]$。\n\n**测试用例 3：外部化质量评分**\n-   **指标**：质量评分，一个数值越高越好的指标。\n-   **数据**：$n=18$ 组配对测量值。\n-   **差异**：$d_i = y_i - x_i$（个性化 - 通用）。\n-   **检验选择**：样本量为 $n=18 \\ge 8$。对差异应用 Shapiro-Wilk 检验：\n    $$d = \\{13, 14, 12, 13, 14, 13, 12, 12, 12, 11, 13, 14, 14, 13, 14, 13, 14, 14\\}$$\n    该检验得出 p 值约为 $0.002$。由于 $0.002  0.05$，我们拒绝正态性假设，选择非参数 Wilcoxon 符号秩检验。检验代码为 $1$。\n-   **假设检验**：单边 Wilcoxon 符号秩检验得出 p 值约为 $3.81 \\times 10^{-6}$。所有差异均为正，代表了反对原假设的最强证据。\n-   **决策**：p 值小于 $\\alpha = 0.05$。改进具有统计显著性。\n-   **结果**：$[0.000004, \\text{True}, 1]$。\n\n**测试用例 4：定位误差（极小变化）**\n-   **指标**：定位误差（度），一个数值越低越好的指标。\n-   **数据**：$n=10$ 组配对测量值。\n-   **差异**：$d_i = x_i - y_i$。\n-   **检验选择**：样本量为 $n=10 \\ge 8$。对差异进行 Shapiro-Wilk 检验：\n    $$d = \\{0.1, -0.1, -0.1, -0.1, -0.1, 0.1, -0.1, -0.1, -0.1, -0.1\\}$$\n    得出的 p 值非常小，约为 $6.0 \\times 10^{-5}$。由于该值小于 $0.05$，我们拒绝正态性，选择 Wilcoxon 符号秩检验。检验代码为 $1$。\n-   **假设检验**：进行单边 Wilcoxon 符号秩检验，以检验差异的中位数是否大于 $0$。数据包含两个正差异和八个大小相同的负差异。该检验得出 p 值约为 $0.967$。\n-   **决策**：p 值远大于 $\\alpha = 0.05$。没有统计上显著的改进。\n-   **结果**：$[0.966797, \\text{False}, 1]$。\n\n**测试用例 5：外部化质量评分（小样本）**\n-   **指标**：质量评分，一个数值越高越好的指标。\n-   **数据**：$n=3$ 组配对测量值。\n-   **差异**：$d_i = y_i - x_i$。差异为 $d = \\{0, 3, 2\\}$。\n-   **检验选择**：样本量为 $n=3$，小于 $8$。因此，默认使用 Wilcoxon 符号秩检验。检验代码为 $1$。\n-   **假设检验**：对差异执行 Wilcoxon 检验。该检验通过丢弃零差异来自动处理它，从而将有效样本量减少到 $2$。剩余差异为 $\\{3, 2\\}$。两者均为正。单边检验得出 p 值为 $0.25$。\n-   **决策**：p 值为 $0.25$，大于 $\\alpha=0.05$。改进不具有统计显著性，这对于如此小的样本量是预料之中的。\n-   **结果**：$[0.250000, \\text{False}, 1]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the hypothesis testing problem for personalized vs. generic HRTFs.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"name\": \"Localization error (degrees)\",\n            \"generic\": [12.4, 10.1, 9.8, 14.3, 11.7, 13.0, 10.9, 12.1, 9.5, 15.2, 13.6, 11.3, 12.8, 10.5, 11.9, 9.7, 12.2, 13.1, 10.4, 11.5, 12.0, 14.0, 13.4, 10.8],\n            \"personalized\": [8.7, 7.9, 7.5, 10.2, 8.8, 9.3, 7.6, 8.9, 6.8, 11.1, 9.7, 8.3, 9.4, 7.7, 8.6, 7.0, 9.1, 9.8, 7.9, 8.5, 9.2, 10.5, 9.9, 7.8],\n            \"improvement_type\": \"lower_is_better\"\n        },\n        {\n            \"name\": \"Spectral distortion (decibels)\",\n            \"generic\": [5.6, 4.9, 6.1, 5.2, 4.7, 5.8, 6.3, 5.0, 5.4, 4.8, 6.0, 5.5],\n            \"personalized\": [4.9, 4.4, 5.5, 4.8, 4.3, 5.1, 5.7, 4.7, 5.0, 4.5, 5.4, 5.1],\n            \"improvement_type\": \"lower_is_better\"\n        },\n        {\n            \"name\": \"Externalization quality rating\",\n            \"generic\": [62, 58, 70, 65, 60, 72, 68, 61, 64, 59, 66, 67, 63, 71, 69, 60, 62, 64],\n            \"personalized\": [75, 72, 82, 78, 74, 85, 80, 73, 76, 70, 79, 81, 77, 84, 83, 73, 76, 78],\n            \"improvement_type\": \"higher_is_better\"\n        },\n        {\n            \"name\": \"Localization error (degrees), minimal change\",\n            \"generic\": [10.2, 9.8, 10.0, 9.7, 10.1, 10.3, 9.9, 10.0, 9.6, 10.2],\n            \"personalized\": [10.1, 9.9, 10.1, 9.8, 10.2, 10.2, 10.0, 10.1, 9.7, 10.3],\n            \"improvement_type\": \"lower_is_better\"\n        },\n        {\n            \"name\": \"Externalization quality rating, small sample\",\n            \"generic\": [70, 75, 80],\n            \"personalized\": [70, 78, 82],\n            \"improvement_type\": \"higher_is_better\"\n        }\n    ]\n\n    alpha = 0.05\n    alpha_norm = 0.05\n    results = []\n\n    for case in test_cases:\n        generic = np.array(case[\"generic\"])\n        personalized = np.array(case[\"personalized\"])\n        \n        if case[\"improvement_type\"] == \"lower_is_better\":\n            differences = generic - personalized\n        else: # higher_is_better\n            differences = personalized - generic\n            \n        n = len(differences)\n        \n        # Determine which test to use initially\n        test_code = 1 # Wilcoxon by default\n        if n >= 8:\n            # Only run Shapiro-Wilk if there's variance in the data\n            if np.std(differences) > 0:\n                shapiro_p = stats.shapiro(differences).pvalue\n                if shapiro_p >= alpha_norm:\n                    test_code = 0 # t-test\n            else: # No variance, so perfectly normal\n                 test_code = 0 # t-test\n        \n        # Handle the special case of all-zero differences\n        if np.all(differences == 0):\n            p_value = 1.0\n            # Test code can be based on the n-based selection logic above\n        else:\n            # Perform the selected test\n            if test_code == 0: # Student's paired t-test\n                p_value = stats.ttest_1samp(differences, 0, alternative='greater').pvalue\n            else: # Wilcoxon signed-rank test\n                # The Wilcoxon test automatically handles zero differences by default\n                # by discarding them. If all differences are zero, it would raise an error,\n                # but we've handled that case above.\n                p_value = stats.wilcoxon(differences, alternative='greater').pvalue\n\n        decision = p_value  alpha\n        rounded_p_value = round(p_value, 6)\n        \n        results.append([rounded_p_value, decision, test_code])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}