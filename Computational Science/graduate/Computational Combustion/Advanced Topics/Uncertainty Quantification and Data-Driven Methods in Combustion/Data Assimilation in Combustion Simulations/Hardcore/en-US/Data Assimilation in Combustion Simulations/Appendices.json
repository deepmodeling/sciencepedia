{
    "hands_on_practices": [
        {
            "introduction": "The Extended Kalman Filter (EKF) serves as a cornerstone for data assimilation in nonlinear systems, extending the classic Kalman filter by applying local linearization. This practice targets the crucial analysis (or update) step, where a new measurement is incorporated to refine the state estimate. You will derive the Jacobian of a nonlinear observation model representing chemiluminescence—a common diagnostic in combustion—and then implement the EKF update equations to correct the state of a simplified reacting system . This exercise builds foundational skills in applying the EKF, connecting the mathematical theory of linearization with a practical implementation for a physically-motivated problem.",
            "id": "4016225",
            "problem": "Consider a discretized, single-cell surrogate of a chemically reacting flow used in computational combustion, where the chemiluminescence signal is modeled as proportional to the volumetric heat release rate. Assume a single-step, irreversible, exothermic reaction between a generic fuel and oxidizer. The observation operator is defined by the nondimensionalized Arrhenius form and mass-action law as follows. Let the state vector be $x = [T, Y_F, Y_O]^T$, where $T$ is the nondimensional temperature, $Y_F$ is the nondimensional mass fraction of fuel, and $Y_O$ is the nondimensional mass fraction of oxidizer. The observation (chemiluminescence intensity) is\n$$\nh(x) = y = \\int \\dot{q}(x)\\,\\mathrm{d}V = A \\exp\\!\\left(-\\frac{E}{R\\,T}\\right) Y_F Y_O,\n$$\nwhere $A$ is a nondimensional pre-exponential factor (absorbing constant density and volume), $E$ is a nondimensional activation energy, and $R$ is a nondimensional gas constant. Assume $V = 1$ and a constant heat release per reaction also nondimensionalized into $A$. This surrogate follows from the well-tested Arrhenius kinetics and the proportionality of chemiluminescence to heat release rate when self-absorption is negligible.\n\nTask:\n- Starting from the Arrhenius law and the mass-action rate expression, derive the Jacobian of the observation operator $h(x)$ with respect to the state $(T, Y_F, Y_O)$.\n- Use the derived Jacobian in an Extended Kalman Filter (EKF) analysis (update) step for a scalar measurement $y^{\\mathrm{meas}}$ with scalar measurement noise variance $R^{\\mathrm{meas}}$. The EKF analysis step updates the forecast mean $x^f$ and forecast covariance $P^f$ to the analysis mean $x^a$ and analysis covariance $P^a$ using the linearization of $h$ at $x^f$.\n\nAssumptions:\n- All quantities are nondimensional.\n- The single-cell surrogate implies that the spatial integral reduces to a product with $V=1$.\n- The gas density is absorbed into $A$, and stoichiometric exponents are $1$ for both reactants in the mass-action law.\n\nYour program must:\n- Implement the observation operator $h(x)$ and its Jacobian $H(x) = \\partial h/\\partial x$ as derived.\n- Implement the EKF analysis step to compute $x^a$ and $P^a$ for each test case.\n\nRequired output:\n- For each test case, output the analysis state $x^a = [T^a, Y_F^a, Y_O^a]$.\n- The final program output must be a single line containing the flattened list of the analysis states for all test cases, in order, as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal digits (e.g., $[T_1^a,Y_{F,1}^a,Y_{O,1}^a,T_2^a,Y_{F,2}^a,Y_{O,2}^a,\\dots]$).\n\nAngle units: no angles are involved.\nPhysical units: all variables are nondimensional.\n\nTest suite:\n- Case $1$ (happy path):\n  - $A = 2.0$, $E = 5.0$, $R = 1.0$.\n  - Forecast mean $x^f = [1.0, 0.2, 0.5]^T$.\n  - Forecast covariance $P^f = \\mathrm{diag}([1.0\\times 10^{-3}, 2.5\\times 10^{-4}, 2.5\\times 10^{-4}])$.\n  - Measurement $y^{\\mathrm{meas}} = 0.0017$.\n  - Measurement variance $R^{\\mathrm{meas}} = 1.0\\times 10^{-4}$.\n- Case $2$ (boundary: near-zero fuel):\n  - $A = 2.0$, $E = 5.0$, $R = 1.0$.\n  - Forecast mean $x^f = [1.0, 1.0\\times 10^{-6}, 0.3]^T$.\n  - Forecast covariance $P^f = \\mathrm{diag}([1.0\\times 10^{-3}, 1.0\\times 10^{-10}, 2.5\\times 10^{-4}])$.\n  - Measurement $y^{\\mathrm{meas}} = 4.0\\times 10^{-9}$.\n  - Measurement variance $R^{\\mathrm{meas}} = 1.0\\times 10^{-8}$.\n- Case $3$ (edge: high temperature):\n  - $A = 2.0$, $E = 5.0$, $R = 1.0$.\n  - Forecast mean $x^f = [5.0, 0.4, 0.4]^T$.\n  - Forecast covariance $P^f = \\mathrm{diag}([1.0\\times 10^{-3}, 2.5\\times 10^{-4}, 2.5\\times 10^{-4}])$.\n  - Measurement $y^{\\mathrm{meas}} = 0.11770000$.\n  - Measurement variance $R^{\\mathrm{meas}} = 1.0\\times 10^{-4}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$), where each result is a float value rounded to $6$ decimal digits and the list is the concatenation of all analysis states, in order of the test cases.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of chemical kinetics (Arrhenius law, mass-action law) and data assimilation (Extended Kalman Filter), well-posed with a complete set of givens, and expressed in objective, formal language. The provided parameters and conditions are consistent and sufficient for deriving a unique, verifiable solution.\n\nThe task is to compute the analysis state, $x^a$, of an Extended Kalman Filter (EKF) update step for a single-cell combustion model. This involves two main parts: first, deriving the Jacobian of the nonlinear observation operator, and second, applying the standard EKF analysis equations.\n\nLet the state vector be $x = [T, Y_F, Y_O]^T$, where $T$ is the nondimensional temperature, $Y_F$ is the nondimensional fuel mass fraction, and $Y_O$ is the nondimensional oxidizer mass fraction.\n\nThe observation operator, $h(x)$, models the measured chemiluminescence intensity and is given as:\n$$\nh(x) = A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O\n$$\nwhere $A$ is the pre-exponential factor, $E$ is the activation energy, and $R$ is the gas constant, all in nondimensional form.\n\n**1. Derivation of the Jacobian**\n\nThe Jacobian of the observation operator, $H(x)$, is a $1 \\times 3$ matrix (a row vector) defined as $H(x) = \\frac{\\partial h}{\\partial x} = \\begin{bmatrix} \\frac{\\partial h}{\\partial T} & \\frac{\\partial h}{\\partial Y_F} & \\frac{\\partial h}{\\partial Y_O} \\end{bmatrix}$. We compute each partial derivative.\n\nThe partial derivative with respect to temperature $T$ is found using the chain rule:\n$$\n\\frac{\\partial h}{\\partial T} = \\frac{\\partial}{\\partial T} \\left( A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O \\right) = A Y_F Y_O \\cdot \\exp\\left(-\\frac{E}{RT}\\right) \\cdot \\frac{\\partial}{\\partial T}\\left(-\\frac{E}{RT}\\right)\n$$\n$$\n\\frac{\\partial h}{\\partial T} = \\left( A \\exp\\left(-\\fracE}{RT}\\right) Y_F Y_O \\right) \\cdot \\left(\\frac{E}{RT^2}\\right) = h(x) \\frac{E}{RT^2}\n$$\n\nThe partial derivative with respect to fuel mass fraction $Y_F$ is:\n$$\n\\frac{\\partial h}{\\partial Y_F} = \\frac{\\partial}{\\partial Y_F} \\left( A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O \\right) = A \\exp\\left(-\\frac{E}{RT}\\right) Y_O\n$$\n\nThe partial derivative with respect to oxidizer mass fraction $Y_O$ is:\n$$\n\\frac{\\partial h}{\\partial Y_O} = \\frac{\\partial}{\\partial Y_O} \\left( A \\exp\\left(-\\frac{E}{RT}\\right) Y_F Y_O \\right) = A \\exp\\left(-\\frac{E}{RT}\\right) Y_F\n$$\n\nCombining these, the Jacobian of the observation operator is:\n$$\nH(x) = \\left[ h(x) \\frac{E}{RT^2}, \\quad A \\exp\\left(-\\frac{E}{RT}\\right) Y_O, \\quad A \\exp\\left(-\\frac{E}{RT}\\right) Y_F \\right]\n$$\n\n**2. Extended Kalman Filter (EKF) Analysis Step**\n\nThe EKF analysis step updates the forecast state mean $x^f$ and covariance $P^f$ to the analysis state mean $x^a$ and covariance $P^a$ using a measurement $y^{\\mathrm{meas}}$ with variance $R^{\\mathrm{meas}}$. The update equations are as follows:\n\nLet $H$ be the Jacobian evaluated at the forecast mean, $H = H(x^f)$.\n\na. **Innovation (or Residual):** The difference between the measurement and the predicted measurement. This is a scalar value, $\\nu$.\n$$\n\\nu = y^{\\mathrm{meas}} - h(x^f)\n$$\n\nb. **Innovation Covariance:** The variance of the innovation. This is also a scalar, $S$.\n$$\nS = H P^f H^T + R^{\\mathrm{meas}}\n$$\n\nc. **Kalman Gain:** The weight given to the innovation. It is a $3 \\times 1$ column vector, $K$.\n$$\nK = P^f H^T S^{-1} = \\frac{P^f H^T}{S}\n$$\n\nd. **Analysis Mean:** The updated state vector.\n$$\nx^a = x^f + K \\nu\n$$\n\ne. **Analysis Covariance:** The updated covariance matrix. Though not required for the final output, the formula is:\n$$\nP^a = (I - KH)P^f\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix.\n\nThe procedure for each test case is to substitute the given values of $A, E, R, x^f, P^f, y^{\\mathrm{meas}},$ and $R^{\\mathrm{meas}}$ into these equations to compute the analysis state $x^a$. The following program implements this procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the EKF analysis problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"params\": {\"A\": 2.0, \"E\": 5.0, \"R\": 1.0},\n            \"x_f\": np.array([1.0, 0.2, 0.5]),\n            \"P_f\": np.diag([1.0e-3, 2.5e-4, 2.5e-4]),\n            \"y_meas\": 0.0017,\n            \"R_meas\": 1.0e-4,\n        },\n        {\n            \"params\": {\"A\": 2.0, \"E\": 5.0, \"R\": 1.0},\n            \"x_f\": np.array([1.0, 1.0e-6, 0.3]),\n            \"P_f\": np.diag([1.0e-3, 1.0e-10, 2.5e-4]),\n            \"y_meas\": 4.0e-9,\n            \"R_meas\": 1.0e-8,\n        },\n        {\n            \"params\": {\"A\": 2.0, \"E\": 5.0, \"R\": 1.0},\n            \"x_f\": np.array([5.0, 0.4, 0.4]),\n            \"P_f\": np.diag([1.0e-3, 2.5e-4, 2.5e-4]),\n            \"y_meas\": 0.11770000,\n            \"R_meas\": 1.0e-4,\n        },\n    ]\n\n    all_results = []\n\n    def h_operator(x, A, E, R_const):\n        \"\"\"\n        Computes the observation operator h(x).\n        x = [T, Y_F, Y_O]\n        \"\"\"\n        T, Y_F, Y_O = x\n        if T <= 0:  # Avoid division by zero or log of non-positive\n            return 0.0\n        return A * np.exp(-E / (R_const * T)) * Y_F * Y_O\n\n    def H_jacobian(x, A, E, R_const):\n        \"\"\"\n        Computes the Jacobian of the observation operator, H(x) = dh/dx.\n        x = [T, Y_F, Y_O]\n        \"\"\"\n        T, Y_F, Y_O = x\n        if T <= 0:\n            return np.zeros(3)\n\n        exp_term = np.exp(-E / (R_const * T))\n        h_val = A * exp_term * Y_F * Y_O\n\n        # Partial derivatives\n        dh_dT = h_val * E / (R_const * T**2)\n        dh_dYF = A * exp_term * Y_O\n        dh_dYO = A * exp_term * Y_F\n\n        return np.array([dh_dT, dh_dYF, dh_dYO])\n\n    for case in test_cases:\n        # Extract parameters for the current case\n        params = case[\"params\"]\n        A, E, R_const = params[\"A\"], params[\"E\"], params[\"R\"]\n        x_f = case[\"x_f\"]\n        P_f = case[\"P_f\"]\n        y_meas = case[\"y_meas\"]\n        R_meas = case[\"R_meas\"]\n\n        # 1. Linearize observation model at the forecast mean x_f\n        # Predicted measurement h(x_f)\n        h_xf = h_operator(x_f, A, E, R_const)\n        \n        # Jacobian H at x_f. H is a 1x3 row vector (1D numpy array).\n        H = H_jacobian(x_f, A, E, R_const)\n\n        # 2. Compute Innovation\n        innovation = y_meas - h_xf\n\n        # 3. Compute Innovation Covariance (scalar)\n        # S = H @ P_f @ H^T + R_meas\n        # In numpy, for 1D H, H @ P_f @ H is equivalent to H P H^T\n        S = H @ P_f @ H + R_meas\n        \n        # 4. Compute Kalman Gain (3x1 vector)\n        # K = P_f @ H^T / S\n        K = (P_f @ H) / S\n        \n        # 5. Compute Analysis Mean (updated state)\n        # x_a = x_f + K * innovation\n        x_a = x_f + K * innovation\n\n        all_results.extend(x_a)\n\n    # Format the final output as a comma-separated list of floats rounded to 6 decimal places.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "State variables in combustion models, such as species mass fractions $Y_k$, must obey strict physical laws, including positivity ($Y_k \\gt 0$) and conservation ($\\sum Y_k = 1$). This practice confronts the challenge of enforcing these constraints within a data assimilation framework that typically assumes unconstrained, Gaussian variables. You will derive and apply the inverse of the Additive Log-Ratio (ALR) transform, a standard method for mapping an analysis from an unconstrained computational space back to the physically valid compositional space . Mastering this technique is essential for preventing non-physical results and ensuring the thermodynamic consistency of state estimates.",
            "id": "4016180",
            "problem": "Consider a compressible reacting mixture in a computational combustion simulation with species mass fractions $Y_{k}$ for $k \\in \\{1,\\dots,N\\}$. By definition, each species mass fraction satisfies $Y_{k} > 0$ and the mixture conservation constraint $\\sum_{k=1}^{N} Y_{k} = 1$. In data assimilation for reacting flows, the state estimation is commonly performed on an unconstrained Euclidean control vector to apply Gaussian updates (for example, in the Ensemble Kalman Filter (EnKF)). To ensure thermodynamic consistency in the analysis, the mass fractions must remain positive and normalized when mapping the analysis back to physical space.\n\nSuppose the species mass fractions are represented in the control vector using a forward transform defined with respect to a reference species index $r \\in \\{1,\\dots,N\\}$:\n$$\n\\eta_{k} = \\ln\\!\\left(\\frac{Y_{k}}{Y_{r}}\\right) \\quad \\text{for all } k \\in \\{1,\\dots,N\\} \\text{ with } k \\neq r,\n$$\nand note that $Y_{r}$ is implicitly determined by the conservation constraint. Assume the analysis step produces updated control variables $\\hat{\\eta}_{k} \\in \\mathbb{R}$ for all $k \\neq r$.\n\nStarting from the physical constraints $Y_{k} > 0$ and $\\sum_{k=1}^{N} Y_{k} = 1$, and the definition of the forward transform above, derive the inverse mapping that returns $\\{Y_{k}\\}$ from $\\{\\hat{\\eta}_{k}\\}$ while strictly enforcing positivity and the sum-to-one constraint. Then, for a three-species hydrogen–oxygen–water mixture with $N = 3$ and species set $\\{\\mathrm{H_{2}}, \\mathrm{O_{2}}, \\mathrm{H_{2}O}\\}$, choose the reference species $r$ to be $\\mathrm{H_{2}O}$. Given the analysis values $\\hat{\\eta}_{\\mathrm{H_{2}}} = 0.7$ and $\\hat{\\eta}_{\\mathrm{O_{2}}} = -1.2$, compute the physical-space mass fraction $Y_{\\mathrm{H_{2}}}$. Express your final answer as a dimensionless fraction and round your answer to four significant figures.",
            "solution": "The problem is subjected to validation before proceeding.\n\n### Step 1: Extract Givens\n- Species mass fractions: $Y_{k}$ for $k \\in \\{1, \\dots, N\\}$.\n- Physical constraints: $Y_{k} > 0$ for all $k$, and $\\sum_{k=1}^{N} Y_{k} = 1$.\n- Forward transform (control variables): $\\eta_{k} = \\ln\\left(\\frac{Y_{k}}{Y_{r}}\\right)$ for all $k \\in \\{1,\\dots,N\\}$ with $k \\neq r$, where $r$ is a reference species index.\n- Updated control variables: $\\hat{\\eta}_{k} \\in \\mathbb{R}$ for all $k \\neq r$.\n- Specific case:\n    - Number of species: $N = 3$.\n    - Species set: $\\{\\mathrm{H_{2}}, \\mathrm{O_{2}}, \\mathrm{H_{2}O}\\}$.\n    - Reference species: $r$ corresponds to $\\mathrm{H_{2}O}$.\n    - Analysis values: $\\hat{\\eta}_{\\mathrm{H_{2}}} = 0.7$ and $\\hat{\\eta}_{\\mathrm{O_{2}}} = -1.2$.\n- Task:\n    1. Derive the inverse mapping that returns $\\{Y_{k}\\}$ from $\\{\\hat{\\eta}_{k}\\}$ while enforcing the physical constraints.\n    2. Compute the physical-space mass fraction $Y_{\\mathrm{H_{2}}}$ for the given specific case.\n    3. Round the final answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem describes the Additive Log-Ratio (ALR) transformation, a standard and rigorous method in compositional data analysis for mapping a constrained simplex (like mass fractions) to an unconstrained Euclidean space. Its application to data assimilation in computational combustion is scientifically sound and relevant.\n- **Well-Posed**: The problem is well-posed. The forward transformation is given, and the task is to find its inverse, which is a mathematically determinate problem. The numerical values provided are sufficient for a unique solution.\n- **Objectivity**: The problem is stated using precise, objective mathematical and scientific language.\n- **Completeness and Consistency**: All necessary definitions, constraints, and data are provided. The definitions are internally consistent and do not lead to contradictions.\n- **Realism**: The context and methods are realistic within the field of computational science and engineering.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, and complete. A solution will be developed.\n\n### Solution Derivation\n\nThe goal is to derive the inverse mapping from the updated control variables $\\{\\hat{\\eta}_{k}\\}$ back to the physical mass fractions $\\{Y_{k}\\}$. The derivation must ensure the physical constraints $Y_{k} > 0$ and $\\sum_{i=1}^{N} Y_{i} = 1$ are satisfied.\n\nThe forward transformation is given for $k \\neq r$:\n$$\n\\hat{\\eta}_{k} = \\ln\\left(\\frac{Y_{k}}{Y_{r}}\\right)\n$$\nTo find the inverse, we first solve for $Y_{k}$ in terms of $Y_{r}$. Exponentiating both sides gives:\n$$\n\\exp(\\hat{\\eta}_{k}) = \\frac{Y_{k}}{Y_{r}}\n$$\n$$\nY_{k} = Y_{r} \\exp(\\hat{\\eta}_{k}) \\quad \\text{for } k \\neq r\n$$\nThis equation provides expressions for the $N-1$ non-reference species mass fractions. The reference mass fraction, $Y_{r}$, is still unknown. To find it, we use the sum-to-one constraint:\n$$\n\\sum_{i=1}^{N} Y_{i} = 1\n$$\nWe can split the summation into the reference species and the remaining species:\n$$\nY_{r} + \\sum_{k \\neq r} Y_{k} = 1\n$$\nNow, substitute the expression for $Y_{k}$ into the summation:\n$$\nY_{r} + \\sum_{k \\neq r} \\left(Y_{r} \\exp(\\hat{\\eta}_{k})\\right) = 1\n$$\nFactor out $Y_{r}$ from the left-hand side:\n$$\nY_{r} \\left(1 + \\sum_{k \\neq r} \\exp(\\hat{\\eta}_{k})\\right) = 1\n$$\nSolving for $Y_{r}$ yields the inverse mapping for the reference species:\n$$\nY_{r} = \\frac{1}{1 + \\sum_{j \\neq r} \\exp(\\hat{\\eta}_{j})}\n$$\nwhere the summation index has been changed to $j$ for clarity. This expression for $Y_{r}$ guarantees positivity, as the exponential function is always positive, making the denominator strictly greater than $1$.\n\nNow we can find the general expression for any non-reference species $Y_{k}$ ($k \\neq r$) by substituting the expression for $Y_{r}$ back into $Y_{k} = Y_{r} \\exp(\\hat{\\eta}_{k})$:\n$$\nY_{k} = \\frac{\\exp(\\hat{\\eta}_{k})}{1 + \\sum_{j \\neq r} \\exp(\\hat{\\eta}_{j})} \\quad \\text{for } k \\neq r\n$$\nThese derived inverse mappings for $Y_r$ and $Y_k$ inherently satisfy the physical constraints. Positivity ($Y_k > 0$) is guaranteed because the exponential function is always positive. The sum-to-one constraint is satisfied by construction:\n$$\n\\sum_{i=1}^{N} Y_{i} = Y_{r} + \\sum_{k \\neq r} Y_{k} = \\frac{1}{1 + \\sum_{j \\neq r} \\exp(\\hat{\\eta}_{j})} + \\sum_{k \\neq r} \\frac{\\exp(\\hat{\\eta}_{k})}{1 + \\sum_{j \\neq r} \\exp(\\hat{\\eta}_{j})} = \\frac{1 + \\sum_{k \\neq r} \\exp(\\hat{\\eta}_{k})}{1 + \\sum_{j \\neq r} \\exp(\\hat{\\eta}_{j})} = 1\n$$\n\n### Application to the Specific Case\n\nWe are given a three-species mixture, $N=3$, with species $\\{\\mathrm{H_{2}}, \\mathrm{O_{2}}, \\mathrm{H_{2}O}\\}$.\nThe reference species is $\\mathrm{H_{2}O}$, so $r$ corresponds to $\\mathrm{H_{2}O}$.\nThe non-reference species indices are $k \\in \\{\\mathrm{H_{2}}, \\mathrm{O_{2}}\\}$.\nThe given analysis-space values are $\\hat{\\eta}_{\\mathrm{H_{2}}} = 0.7$ and $\\hat{\\eta}_{\\mathrm{O_{2}}} = -1.2$.\n\nWe need to compute the mass fraction of hydrogen, $Y_{\\mathrm{H_{2}}}$. Since $\\mathrm{H_{2}}$ is not the reference species, we use the formula for $Y_k$ with $k \\neq r$:\n$$\nY_{\\mathrm{H_{2}}} = \\frac{\\exp(\\hat{\\eta}_{\\mathrm{H_{2}}})}{1 + \\sum_{j \\in \\{\\mathrm{H_{2}}, \\mathrm{O_{2}}\\}} \\exp(\\hat{\\eta}_{j})}\n$$\nThe summation in the denominator is over all non-reference species:\n$$\n\\sum_{j \\in \\{\\mathrm{H_{2}}, \\mathrm{O_{2}}\\}} \\exp(\\hat{\\eta}_{j}) = \\exp(\\hat{\\eta}_{\\mathrm{H_{2}}}) + \\exp(\\hat{\\eta}_{\\mathrm{O_{2}}})\n$$\nSubstituting the given values:\n$$\nY_{\\mathrm{H_{2}}} = \\frac{\\exp(0.7)}{1 + \\exp(0.7) + \\exp(-1.2)}\n$$\nNow, we compute the numerical values:\n$$\n\\exp(0.7) \\approx 2.013752707\n$$\n$$\n\\exp(-1.2) \\approx 0.3011942119\n$$\nSubstituting these into the expression for $Y_{\\mathrm{H_{2}}}$:\n$$\nY_{\\mathrm{H_{2}}} \\approx \\frac{2.013752707}{1 + 2.013752707 + 0.3011942119} = \\frac{2.013752707}{3.314946919}\n$$\n$$\nY_{\\mathrm{H_{2}}} \\approx 0.60747444\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nY_{\\mathrm{H_{2}}} \\approx 0.6075\n$$\nThis is the final numerical answer for the mass fraction of $\\mathrm{H_2}$.",
            "answer": "$$\n\\boxed{0.6075}\n$$"
        },
        {
            "introduction": "The Gaussian assumption underlying the Kalman filter family can be overly restrictive for many combustion phenomena, such as ignition events or optical measurements dominated by shot noise. This practice introduces the particle filter, a powerful sequential Monte Carlo method capable of handling arbitrary nonlinear models and non-Gaussian probability distributions. You will implement the core weight update step based on importance sampling, using a Poisson likelihood to model discrete photon counts, and learn to diagnose filter performance by computing the Effective Sample Size ($N_{\\mathrm{eff}}$) to detect sample degeneracy . This exercise provides hands-on experience with a more general class of assimilation methods, highlighting their flexibility and key practical challenges.",
            "id": "4016190",
            "problem": "Consider a one-step update of a Sequential Importance Resampling particle method for Data Assimilation (DA) applied to computational combustion with a nonlinear chemiluminescence observation. Let the hidden combustion state be represented by a collection of particles $\\{x_n^{(j)}\\}_{j=1}^N$, where each $x_n^{(j)}$ contains the temperature $T_n^{(j)}$ in Kelvin and a reaction progress variable $q_n^{(j)}$ (dimensionless). The observation at time index $n$ is the photon count $y_n$ measured by a photomultiplier over an integration time $\\Delta t$ in seconds. Assume the expected photon count from chemiluminescence for a given state is modeled by an Arrhenius-type intensity:\n$$\n\\lambda(x) = \\kappa\\, q^\\alpha \\exp\\!\\left(-\\frac{E_a}{R\\,T}\\right),\n$$\nwhere $\\kappa$ has units of per second, $q$ is dimensionless, $E_a$ is the activation energy in Joules per mole, $R$ is the universal gas constant in Joules per mole per Kelvin, and $T$ is temperature in Kelvin. The mean photon count over the integration interval is\n$$\n\\mu(x) = \\Delta t \\,\\lambda(x).\n$$\nAssume a non-Gaussian observation model in which the observed count $y_n$ is drawn from a Poisson distribution with mean $\\mu(x_n)$, so the likelihood is\n$$\np(y_n \\mid x_n) = \\mathrm{Poisson}\\big(\\mu(x_n)\\big).\n$$\nStarting from Bayes' rule and importance sampling with a proposal equal to the prior Markov transition (the bootstrap choice), derive the particle weight update and implement it in a program that, given a set of previous normalized weights $\\{w_{n-1}^{(j)}\\}$ and current particles $\\{x_n^{(j)}\\}$, computes the updated normalized weights $\\{w_n^{(j)}\\}$ and the Effective Sample Size (ESS). The Effective Sample Size (ESS) is defined by\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{j=1}^N \\left(w_n^{(j)}\\right)^2}.\n$$\nThen, analyze degeneracy by declaring a degeneracy flag that is true if $N_{\\mathrm{eff}} < \\beta N$, where $\\beta$ is a fixed fraction and $N$ is the number of particles.\n\nScientific and computational requirements:\n- Use the physically motivated Arrhenius form with constants $R = 8.314$ Joules per mole per Kelvin, $E_a$ and $\\kappa$ as given per test case, and an exponent $\\alpha$.\n- The photon count $y_n$ is an integer count (dimensionless). Temperature $T$ must be in Kelvin, and time $\\Delta t$ must be in seconds. Any expression involving $T$ and $\\Delta t$ must use these units.\n- Compute the likelihood using the Poisson probability mass function. To ensure numerical stability, perform the computation of the likelihood in the logarithmic domain, and then normalize the weights. Use the gamma function identity $\\log(y!) = \\log\\Gamma(y+1)$ for integer $y$.\n\nYour program must implement the weight update and ESS computation for the following test suite, each test defined by the tuple of parameters $(\\{T_n^{(j)}\\}, \\{q_n^{(j)}\\}, \\{w_{n-1}^{(j)}\\}, y_n, \\Delta t, \\kappa, E_a, \\alpha, \\beta)$:\n\n- Test $1$ (happy path, moderate counts):\n    - $N = 5$\n    - $\\{T_n^{(j)}\\} = [1950, 2000, 2050, 1900, 2100]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.85, 0.92, 0.88, 0.80, 0.95]$ (dimensionless)\n    - $\\{w_{n-1}^{(j)}\\} = [0.2, 0.2, 0.2, 0.2, 0.2]$ (normalized)\n    - $y_n = 200$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 1.5\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n- Test $2$ (skewed prior weights, higher counts):\n    - $N = 8$\n    - $\\{T_n^{(j)}\\} = [1800, 1850, 1900, 1950, 2000, 2050, 2100, 2150]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 0.98]$\n    - $\\{w_{n-1}^{(j)}\\} = [0.05, 0.05, 0.10, 0.10, 0.20, 0.20, 0.15, 0.15]$ (normalized)\n    - $y_n = 320$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 1.5\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n- Test $3$ (edge case, zero counts):\n    - $N = 6$\n    - $\\{T_n^{(j)}\\} = [1600, 1650, 1700, 1750, 1800, 1850]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.40, 0.45, 0.50, 0.55, 0.60, 0.65]$\n    - $\\{w_{n-1}^{(j)}\\} = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$ (normalized)\n    - $y_n = 0$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 1.5\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n- Test $4$ (stronger nonlinearity via larger activation energy):\n    - $N = 7$\n    - $\\{T_n^{(j)}\\} = [1900, 1950, 2000, 2050, 2100, 2150, 2200]$ Kelvin\n    - $\\{q_n^{(j)}\\} = [0.80, 0.82, 0.85, 0.88, 0.90, 0.92, 0.95]$\n    - $\\{w_{n-1}^{(j)}\\} = [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7]$ (normalized)\n    - $y_n = 20$ (counts)\n    - $\\Delta t = 5.0\\times 10^{-4}$ seconds\n    - $\\kappa = 3.0\\times 10^{9}$ per second\n    - $E_a = 2.0\\times 10^{5}$ Joules per mole\n    - $\\alpha = 1.0$\n    - $\\beta = 0.5$\n\nProgram output requirements:\n- For each test case, compute the updated normalized weights $\\{w_n^{(j)}\\}$ and $N_{\\mathrm{eff}}$.\n- Declare degeneracy if $N_{\\mathrm{eff}} < \\beta N$.\n- Round $N_{\\mathrm{eff}}$ to three decimal places. Round each normalized weight $w_n^{(j)}$ to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[N_{\\mathrm{eff}}, \\text{degeneracy\\_flag}, \\text{weights}]$ where $N_{\\mathrm{eff}}$ is a float, $\\text{degeneracy\\_flag}$ is a boolean, and $\\text{weights}$ is the list of rounded floats. For the four tests, the final output must look like:\n```\n[ [N_eff_1,deg_1,[w_1^(1),...,w_1^(N)]], [N_eff_2,deg_2,[...]], [N_eff_3,deg_3,[...]], [N_eff_4,deg_4,[...]] ]\n```\nThis single line is the only output.",
            "solution": "### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and models for a one-step update of a Sequential Importance Resampling particle filter.\n\n**State and Observation Models:**\n1.  **Hidden State:** The hidden state for each particle $j$ at time step $n$ is $x_n^{(j)} = (T_n^{(j)}, q_n^{(j)})$, where $T_n^{(j)}$ is temperature in Kelvin and $q_n^{(j)}$ is a dimensionless reaction progress variable.\n2.  **Observation:** The observation is a photon count $y_n$ (integer, dimensionless).\n3.  **Chemiluminescence Intensity Model:** The expected intensity $\\lambda(x)$ for a state $x=(T, q)$ is given by an Arrhenius-type expression:\n    $$ \\lambda(x) = \\kappa\\, q^\\alpha \\exp\\!\\left(-\\frac{E_a}{R\\,T}\\right) $$\n    where $\\kappa$ is a pre-exponential factor (units: s$^{-1}$), $\\alpha$ is an exponent, $E_a$ is the activation energy (J/mol), and $R$ is the universal gas constant.\n4.  **Mean Photon Count:** The mean count $\\mu(x)$ over an integration time $\\Delta t$ (seconds) is:\n    $$ \\mu(x) = \\Delta t \\,\\lambda(x) $$\n5.  **Observation Likelihood:** The observation $y_n$ is drawn from a Poisson distribution with mean $\\mu(x_n)$:\n    $$ p(y_n \\mid x_n) = \\mathrm{Poisson}\\big(y_n; \\mu(x_n)\\big) = \\frac{\\mu(x_n)^{y_n} e^{-\\mu(x_n)}}{y_n!} $$\n\n**Particle Filter Update Rules:**\n1.  **Inputs:** Previous normalized weights $\\{w_{n-1}^{(j)}\\}_{j=1}^N$ and current particle states $\\{x_n^{(j)}\\}_{j=1}^N$.\n2.  **Weight Update:** Based on Bayes' rule and importance sampling (bootstrap filter), the unnormalized weights $\\tilde{w}_n^{(j)}$ are updated as:\n    $$ \\tilde{w}_n^{(j)} \\propto w_{n-1}^{(j)} p(y_n | x_n^{(j)}) $$\n    The normalized weights $w_n^{(j)}$ are:\n    $$ w_n^{(j)} = \\frac{\\tilde{w}_n^{(j)}}{\\sum_{k=1}^N \\tilde{w}_n^{(k)}} $$\n3.  **Effective Sample Size (ESS):**\n    $$ N_{\\mathrm{eff}} = \\frac{1}{\\sum_{j=1}^N \\left(w_n^{(j)}\\right)^2} $$\n4.  **Degeneracy Condition:** Degeneracy is declared if $N_{\\mathrm{eff}} < \\beta N$, where $N$ is the number of particles and $\\beta$ is a given threshold.\n\n**Constants and Numerical Requirements:**\n-   Universal Gas Constant: $R = 8.314$ J/(mol·K).\n-   Numerical Stability: Calculations for the likelihood must be performed in the logarithmic domain.\n-   Log-Gamma Identity: $\\log(y!) = \\log\\Gamma(y+1)$, where $\\Gamma$ is the Gamma function.\n\n**Test Cases:**\nThe problem defines four test cases, each specified by a tuple $(\\{T_n^{(j)}\\}, \\{q_n^{(j)}\\}, \\{w_{n-1}^{(j)}\\}, y_n, \\Delta t, \\kappa, E_a, \\alpha, \\beta)$. The specific values are provided in the problem statement.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem is firmly rooted in established principles. The Arrhenius equation is a cornerstone of chemical kinetics. The Poisson distribution is the correct statistical model for counting discrete, independent events like photon arrivals (shot noise). The particle filter (specifically, the SIR variant) is a standard and widely used method for state estimation in nonlinear, non-Gaussian systems, which is the context of data assimilation. The problem is scientifically sound.\n-   **Well-Posed:** For each test case, all necessary inputs are provided. The mathematical formulas for the weight update, ESS, and degeneracy check are deterministic and lead to a unique solution. The problem is well-posed.\n-   **Objective:** The problem is defined with precise mathematical equations, constants, and data. The language is objective and free of ambiguity or subjective claims.\n-   **Completeness and Consistency:** All parameters ($\\kappa, E_a, \\alpha, \\Delta t, \\beta$), constants ($R$), state variables ($T, q$), prior weights ($w_{n-1}$), and the observation ($y_n$) are specified for each test. The units are explicitly defined (Kelvin, seconds, J/mol) and are physically consistent. The prior weights in each test case are verified to sum to $1$. The setup is complete and internally consistent.\n-   **No Other Flaws:** The problem does not exhibit any other flaws from the checklist such as being non-formalizable, unrealistic, ill-posed, trivial, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation and implementation of the solution.\n\n### Principle-Based Design and Solution Derivation\n\nThe core task is to implement the measurement update step of a bootstrap particle filter. This involves re-weighting a set of particles based on how well each particle's predicted state explains a given measurement.\n\n**1. Derivation of the Weight Update Rule**\n\nAccording to the principles of sequential importance sampling, the posterior probability of the state $x_n$ given all observations up to time $n$, $y_{1:n}$, is recursively updated. Using Bayes' theorem:\n$$\np(x_n | y_{1:n}) \\propto p(y_n | x_n) \\, p(x_n | y_{1:n-1})\n$$\nThe term $p(x_n | y_{1:n-1})$ is the prior probability of the state at time $n$, predicted from the state at time $n-1$. In a particle filter, this prior is represented by the propagated particles $\\{x_n^{(j)}\\}$ with their inherited weights $\\{w_{n-1}^{(j)}\\}$. The term $p(y_n | x_n)$ is the likelihood of observing $y_n$ given the state $x_n$.\n\nFor a bootstrap filter, the proposal distribution is chosen to be the state transition prior. The importance weights for the particles are updated by multiplying the previous weight by the likelihood of the new observation given the new particle state:\n$$\n\\tilde{w}_n^{(j)} = w_{n-1}^{(j)} \\, p(y_n | x_n^{(j)})\n$$\nThese unnormalized weights, $\\tilde{w}_n^{(j)}$, are then normalized to sum to unity:\n$$\nw_n^{(j)} = \\frac{\\tilde{w}_n^{(j)}}{\\sum_{k=1}^N \\tilde{w}_n^{(k)}}\n$$\n\n**2. Computation of the Likelihood in Logarithmic Domain**\n\nTo maintain numerical stability, especially when dealing with very small probabilities, we work with log-likelihoods. The likelihood is given by the Poisson probability mass function:\n$$\np(y_n | x_n^{(j)}) = \\frac{(\\mu_n^{(j)})^{y_n} e^{-\\mu_n^{(j)}}}{y_n!}\n$$\nwhere $\\mu_n^{(j)} = \\mu(x_n^{(j)})$. Taking the natural logarithm:\n$$\n\\log p(y_n | x_n^{(j)}) = y_n \\log(\\mu_n^{(j)}) - \\mu_n^{(j)} - \\log(y_n!)\n$$\nUsing the provided identity $\\log(y_n!) = \\log\\Gamma(y_n+1)$, which is implemented numerically by the log-gamma function (`gammaln` in SciPy), the log-likelihood $\\ell_n^{(j)}$ is:\n$$\n\\ell_n^{(j)} = y_n \\log(\\mu_n^{(j)}) - \\mu_n^{(j)} - \\mathrm{gammaln}(y_n+1)\n$$\nThe mean photon count $\\mu_n^{(j)}$ is calculated for each particle $j$ using its state $(T_n^{(j)}, q_n^{(j)})$ and the given parameters:\n$$\n\\mu_n^{(j)} = \\Delta t \\cdot \\kappa \\cdot (q_n^{(j)})^\\alpha \\exp\\left(-\\frac{E_a}{R T_n^{(j)}}\\right)\n$$\nAn edge case occurs if an observation $y_n=0$ is recorded. The log-likelihood formula simplifies to $\\ell_n^{(j)} = - \\mu_n^{(j)}$ since $\\log(0!) = 0$ and the term $y_n \\log(\\mu_n^{(j)})$ vanishes (as long as handled correctly for $\\mu_n^{(j)} \\to 0$).\n\n**3. Normalization using the Log-Sum-Exp Trick**\n\nThe update equation for the weights involves an exponential and a sum, which can lead to overflow or underflow. We use the log-sum-exp trick for robust normalization. First, we compute the unnormalized log-weights:\n$$\n\\log \\tilde{w}_n^{(j)} = \\log(w_{n-1}^{(j)}) + \\ell_n^{(j)}\n$$\nLet $L_{\\max} = \\max_k(\\log \\tilde{w}_n^{(k)})$. We can write the normalized weight as:\n$$\nw_n^{(j)} = \\frac{\\exp(\\log \\tilde{w}_n^{(j)})}{\\sum_{k=1}^N \\exp(\\log \\tilde{w}_n^{(k)})} = \\frac{\\exp(\\log \\tilde{w}_n^{(j)} - L_{\\max})}{\\sum_{k=1}^N \\exp(\\log \\tilde{w}_n^{(k)} - L_{\\max})}\n$$\nThis computation avoids overflow because the largest exponentiated term is $e^0=1$. It also mitigates underflow by scaling all terms up.\n\n**4. Calculation of ESS and Degeneracy Check**\n\nAfter obtaining the normalized weights $\\{w_n^{(j)}\\}$, the Effective Sample Size is computed directly from its definition:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{j=1}^N (w_n^{(j)})^2}\n$$\n$N_{\\mathrm{eff}}$ provides a measure of weight degeneracy. A value close to the total number of particles $N$ indicates that weights are evenly distributed, while a value close to $1$ indicates that a single particle has a weight near unity, and all others are near zero.\n\nFinally, the degeneracy flag is set by comparing $N_{\\mathrm{eff}}$ against the threshold specified:\n$$\n\\text{degeneracy\\_flag} = (N_{\\mathrm{eff}} < \\beta N)\n$$\nThis systematic procedure forms the basis for the implementation.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\nimport json\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each test case and formats the final output.\n    \"\"\"\n    \n    # Universal Gas Constant in J/(mol·K)\n    R = 8.314\n\n    test_cases = [\n        # Test 1 (happy path, moderate counts)\n        (\n            [1950, 2000, 2050, 1900, 2100],  # T_n\n            [0.85, 0.92, 0.88, 0.80, 0.95],  # q_n\n            [0.2, 0.2, 0.2, 0.2, 0.2],  # w_n-1\n            200,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            1.5e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n        # Test 2 (skewed prior weights, higher counts)\n        (\n            [1800, 1850, 1900, 1950, 2000, 2050, 2100, 2150],  # T_n\n            [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00, 0.98],  # q_n\n            [0.05, 0.05, 0.10, 0.10, 0.20, 0.20, 0.15, 0.15], # w_n-1\n            320,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            1.5e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n        # Test 3 (edge case, zero counts)\n        (\n            [1600, 1650, 1700, 1750, 1800, 1850],  # T_n\n            [0.40, 0.45, 0.50, 0.55, 0.60, 0.65],  # q_n\n            [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],  # w_n-1\n            0,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            1.5e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n        # Test 4 (stronger nonlinearity via larger activation energy)\n        (\n            [1900, 1950, 2000, 2050, 2100, 2150, 2200],  # T_n\n            [0.80, 0.82, 0.85, 0.88, 0.90, 0.92, 0.95],  # q_n\n            [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7],  # w_n-1\n            20,  # y_n\n            5.0e-4,  # delta_t\n            3.0e9,  # kappa\n            2.0e5,  # E_a\n            1.0,  # alpha\n            0.5,  # beta\n        ),\n    ]\n\n    results = []\n    for params in test_cases:\n        T_n, q_n, w_prev, y_n, delta_t, kappa, E_a, alpha, beta = params\n        \n        # Convert inputs to numpy arrays for vectorized operations\n        T_n = np.array(T_n)\n        q_n = np.array(q_n)\n        w_prev = np.array(w_prev)\n        N = len(T_n)\n\n        # 1. Calculate the mean photon count mu for each particle\n        # lambda(x) = kappa * q^alpha * exp(-Ea / (R * T))\n        # mu(x) = delta_t * lambda(x)\n        exponent = -E_a / (R * T_n)\n        mu = delta_t * kappa * (q_n**alpha) * np.exp(exponent)\n\n        # 2. Compute the log-likelihood for each particle\n        # log L = y * log(mu) - mu - log(y!)\n        # Handle the edge case of y_n = 0 separately for robustness\n        if y_n == 0:\n            # For y=0, log L simplifies to -mu, avoiding log(0) issues.\n            log_likelihood = -mu\n        else:\n            # For y > 0, use the full formula.\n            # Handle potential mu = 0 cases to avoid log(0) = -inf.\n            # If mu is 0, log L is -inf. np.log(0) correctly returns -inf.\n            # 0 * log(0) is nan, but our problem structure ensures mu > 0 if y > 0.\n            # Even if mu=0, the log-likelihood should be -inf, resulting in zero weight.\n            # A numerically safe way to compute y*log(mu):\n            log_mu = np.full_like(mu, -np.inf) # Initialize with -inf\n            non_zero_mu_mask = mu > 0\n            log_mu[non_zero_mu_mask] = np.log(mu[non_zero_mu_mask])\n\n            log_likelihood = y_n * log_mu - mu - gammaln(y_n + 1)\n        \n        # 3. Update weights in the log domain and normalize\n        # log(w_unnorm) = log(w_prev) + log_likelihood\n        # np.log handles w_prev=0 by returning -inf, which is correct.\n        log_w_unnorm = np.log(w_prev) + log_likelihood\n\n        # Use log-sum-exp trick for normalization\n        # Find max of finite log-weights to avoid issues with -inf\n        if np.all(np.isneginf(log_w_unnorm)):\n            # All weights are zero, which implies an inconsistency or extreme case.\n            # Assign uniform weights to prevent division by zero.\n            w_n = np.full(N, 1.0/N)\n        else:\n            max_log_w = np.max(log_w_unnorm[np.isfinite(log_w_unnorm)])\n            w_shifted = np.exp(log_w_unnorm - max_log_w)\n            w_n = w_shifted / np.sum(w_shifted)\n            # Ensure no NaNs, e.g., if all weights become zero\n            w_n = np.nan_to_num(w_n, nan=1.0/N)\n            w_n /= np.sum(w_n)\n\n        # 4. Calculate Effective Sample Size (ESS)\n        # N_eff = 1 / sum(w_n^2)\n        sum_sq_weights = np.sum(w_n**2)\n        N_eff = 1.0 / sum_sq_weights\n\n        # 5. Check for degeneracy\n        # is_degenerate is True if N_eff  beta * N\n        is_degenerate = N_eff  beta * N\n\n        # 6. Format results according to specifications\n        N_eff_rounded = round(N_eff, 3)\n        weights_rounded = [round(w, 6) for w in w_n]\n        \n        results.append([N_eff_rounded, is_degenerate, weights_rounded])\n\n    # 7. Print the final output in the required single-line format\n    # The string representation of a list of lists handled by Python's `str`\n    # and subsequent replacement gives the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}