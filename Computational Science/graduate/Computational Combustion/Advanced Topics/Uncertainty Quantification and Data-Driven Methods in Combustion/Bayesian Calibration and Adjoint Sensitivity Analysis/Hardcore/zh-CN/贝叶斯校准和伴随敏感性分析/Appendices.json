{
    "hands_on_practices": [
        {
            "introduction": "本练习构成了基于梯度的贝叶斯推断的数学基石。我们将推导对数后验概率分布相对于模型参数的梯度表达式。通过将伴随灵敏度分析方法应用于一个典型的燃烧动力学问题 ()，你将学习如何高效地计算该梯度，这对于诸如哈密顿蒙特卡洛（HMC）等高级采样算法以及寻找最大后验（MAP）估计的优化方法至关重要。",
            "id": "4009554",
            "problem": "考虑一个在计算燃烧学中用于预测点火延迟的均质、绝热、恒容反应器模型。令反应器状态为 $y(t;\\theta)\\in\\mathbb{R}^{n}$，其包含了温度和各组分的质量分数，并遵循以下形式的常微分方程 (ODE) 演化：\n$$\n\\frac{d y}{d t} \\;=\\; F_{i}\\!\\big(y(t;\\theta),\\theta\\big), \\quad y(0;\\theta)=y_{i0}(\\theta),\n$$\n其中 $i=1,\\dots,N$ 代表不同的实验条件（每个条件都有其自身的右端项 $F_{i}$ 和由该实验的混合物成分、压力和温度决定的初始状态 $y_{i0}$）。反应速率采用 Arrhenius 型动力学模型，因此 $F_{i}$ 通过速率常数 $k_{r}(T;\\theta)$ 依赖于参数 $\\theta$，其形式为 $k_{r}(T;\\theta)=A_{r}\\exp\\!\\big(-E_{r}/(R\\,T)\\big)$，其中 $A_{r}$ 和 $E_{r}$ 是 $\\theta$ 的分量，$R$ 是普适气体常数，$T$ 是 $y$ 中的温度分量。\n\n将实验 $i$ 的预测点火延迟 $\\hat{\\tau}_{i}(\\theta)$ 定义为事件函数 $s_{i}\\!\\big(y(t;\\theta),\\theta\\big)$ 值变为零的第一个时间点 $t$：\n$$\ns_{i}\\!\\big(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta\\big) \\;=\\; 0,\n$$\n其中 $s_{i}$ 对 $y$ 和 $\\theta$ 均是光滑的。对于一个化学上符合实际的标准，可以取 $s_{i}(y,\\theta)=Y_{\\mathrm{OH}}(y)-Y_{\\mathrm{OH}}^{\\star}$，其中 $Y_{\\mathrm{OH}}$ 是 $y$ 中隐含的羟基自由基质量分数，$Y_{\\mathrm{OH}}^{\\star}$ 是一个固定的阈值（因此 $\\partial s_{i}/\\partial\\theta=0$），不过下面的推导对一般的处处光滑的 $s_{i}$ 均成立。\n\n假设我们有点火延迟的测量数据 $d=\\{\\tau_{i}\\}_{i=1}^{N}$，并采用高斯误差模型\n$$\n\\tau_{i} \\;=\\; \\hat{\\tau}_{i}(\\theta) + \\varepsilon_{i}, \\qquad \\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2}),\n$$\n其中方差 $\\sigma^{2}>0$ 已知，且不同 $i$ 之间的误差是独立的。对 $\\theta$ 设置一个高斯先验，\n$$\n\\theta \\sim \\mathcal{N}(\\theta_{0},\\Sigma_{0}),\n$$\n其均值为 $\\theta_{0}$，协方差为正定矩阵 $\\Sigma_{0}$。\n\n利用贝叶斯法则以及高斯似然和先验的假设，推导对数后验概率 $\\log p(\\theta\\mid d)$ 的显式表达式，并计算其梯度 $\\nabla_{\\theta}\\log p(\\theta\\mid d)$。对于梯度，您必须使用伴随灵敏度分析来表示 $\\nabla_{\\theta}\\hat{\\tau}_{i}(\\theta)$，其中事件时间由 $s_{i}\\!\\big(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta\\big)=0$ 隐式定义。引入伴随向量 $\\lambda_{i}(t)\\in\\mathbb{R}^{n}$，其解满足\n$$\n-\\frac{d \\lambda_{i}}{d t} \\;=\\; \\left[\\frac{\\partial F_{i}}{\\partial y}\\!\\big(y(t;\\theta),\\theta\\big)\\right]^{\\!\\top} \\lambda_{i}(t), \\qquad \\lambda_{i}\\!\\big(\\hat{\\tau}_{i}(\\theta)\\big) \\;=\\; q_{i},\n$$\n其中 $q_{i} := \\left.\\frac{\\partial s_{i}}{\\partial y}\\!\\big(y,\\theta\\big)\\right|_{y=y(\\hat{\\tau}_{i}(\\theta);\\theta)}$。您的最终表达式必须是封闭形式的，并且可以包含由前向解和伴随解构成的已知函数的积分。以符号形式表示最终答案。无需进行数值评估。如果需要任何近似，则需要根据有效数字进行说明；然而，本问题要求一个精确的符号答案。对数后验是无量纲的；请将梯度报告为一个无量纲的向量表达式。",
            "solution": "首先验证问题，以确保其具有科学依据、良态 (well-posed) 且客观。\n\n### 步骤 1：提取已知条件\n-   **状态向量**：$y(t;\\theta)\\in\\mathbb{R}^{n}$，代表温度和组分质量分数。\n-   **控制 ODE**：$\\frac{d y}{d t} = F_{i}(y(t;\\theta),\\theta)$，适用于 $i=1,\\dots,N$ 个实验条件。\n-   **初始条件**：$y(0;\\theta)=y_{i0}(\\theta)$。\n-   **参数依赖性**：反应速率 $k_{r}(T;\\theta)=A_{r}\\exp(-E_{r}/(R\\,T))$ 是参数 $\\theta$ 的函数，其中 $A_{r}$ 和 $E_{r}$ 是 $\\theta$ 的分量。\n-   **预测的点火延迟**：$\\hat{\\tau}_{i}(\\theta)$ 由事件函数 $s_{i}(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta) = 0$ 隐式定义。函数 $s_{i}$ 是光滑的。\n-   **测量数据**：$d=\\{\\tau_{i}\\}_{i=1}^{N}$。\n-   **似然模型**：一个独立同分布的高斯误差模型：$\\tau_{i} = \\hat{\\tau}_{i}(\\theta) + \\varepsilon_{i}$，其中 $\\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2})$ 且方差 $\\sigma^{2}>0$ 已知。\n-   **先验模型**：参数 $\\theta$ 上的高斯先验：$\\theta \\sim \\mathcal{N}(\\theta_{0},\\Sigma_{0})$，其均值为 $\\theta_{0}$，协方差为正定矩阵 $\\Sigma_{0}$。\n-   **伴随系统**：伴随向量 $\\lambda_{i}(t)\\in\\mathbb{R}^{n}$ 由 ODE $-\\frac{d \\lambda_{i}}{d t} = [\\frac{\\partial F_{i}}{\\partial y}(y(t;\\theta),\\theta)]^{\\top} \\lambda_{i}(t)$ 和终端条件 $\\lambda_{i}(\\hat{\\tau}_{i}(\\theta)) = q_{i}$ 定义。\n-   **伴随终端条件源**：$q_{i} := \\left.\\frac{\\partial s_{i}}{\\partial y}(y,\\theta)\\right|_{y=y(\\hat{\\tau}_{i}(\\theta);\\theta)}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题陈述是化学动力学中参数估计的贝叶斯反演问题的标准表述，这是计算燃烧学的一个核心课题。使用均质反应器模型、Arrhenius 动力学以及点火延迟的隐式定义都是标准做法。应用具有高斯先验和似然的贝叶斯推断是一个成熟的统计框架。此外，使用伴随灵敏度分析是计算此类问题中所需梯度的一种先进的、数学上严谨且计算上高效的技术。该问题是自洽的，提供了所有必要的定义和模型。没有科学或事实上的不健全之处，没有矛盾，问题是客观且良态的。它直接涉及*计算燃烧学*领域内的*贝叶斯校准和伴随灵敏度分析*这一主题。\n\n### 步骤 3：结论与行动\n该问题被判定为**有效**。将提供一个完整的、有理有据的解答。\n\n### 对数后验及其梯度的推导\n\n目标是推导对数后验概率密度函数 $\\log p(\\theta\\mid d)$ 及其梯度 $\\nabla_{\\theta}\\log p(\\theta\\mid d)$。\n\n根据贝叶斯法则，后验概率正比于似然与先验的乘积：\n$$p(\\theta\\mid d) \\propto p(d\\mid\\theta)p(\\theta)$$\n取对数，我们得到：\n$$\\log p(\\theta\\mid d) = \\log p(d\\mid\\theta) + \\log p(\\theta) + C$$\n其中 $C$ 是一个不依赖于参数 $\\theta$ 的归一化常数。\n\n**1. 对数似然项：**\n假设测量值 $\\{\\tau_i\\}$ 是独立的。似然是每个测量概率的乘积：\n$$p(d\\mid\\theta) = \\prod_{i=1}^{N} p(\\tau_i\\mid\\theta)$$\n给定误差模型 $\\tau_i \\sim \\mathcal{N}(\\hat{\\tau}_i(\\theta), \\sigma^2)$，单个测量值的概率密度为：\n$$p(\\tau_i\\mid\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\tau_i - \\hat{\\tau}_i(\\theta))^2}{2\\sigma^2}\\right)$$\n总对数似然是所有 $N$ 个实验的总和：\n$$\\log p(d\\mid\\theta) = \\sum_{i=1}^{N} \\log p(\\tau_i\\mid\\theta) = \\sum_{i=1}^{N} \\left(-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\tau_i - \\hat{\\tau}_i(\\theta))^2\\right)$$\n去掉常数项，与 $\\theta$ 相关的对数似然部分为：\n$$\\log p(d\\mid\\theta) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2 + \\text{const.}$$\n\n**2. 对数先验项：**\n$\\theta$ 的先验被给定为多元正态分布，$\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$。其概率密度函数为：\n$$p(\\theta) = \\frac{1}{\\sqrt{(2\\pi)^k |\\det(\\Sigma_0)|}} \\exp\\left(-\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)\\right)$$\n其中 $k$ 是 $\\theta$ 的维度。对数先验为：\n$$\\log p(\\theta) = -\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0) + \\text{const.}$$\n\n**3. 对数后验表达式：**\n结合对数似然和对数先验，对数后验（不计一个加性常数）为：\n$$\\log p(\\theta\\mid d) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2 - \\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)$$\n\n**4. 对数后验的梯度：**\n我们现在计算 $\\log p(\\theta\\mid d)$ 关于 $\\theta$ 的梯度：\n$$\\nabla_{\\theta}\\log p(\\theta\\mid d) = \\nabla_{\\theta}\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2\\right) + \\nabla_{\\theta}\\left(-\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)\\right)$$\n先验项的梯度是二次型的标准结果：\n$$\\nabla_{\\theta}\\left(-\\frac{1}{2}(\\theta - \\theta_0)^{\\top}\\Sigma_0^{-1}(\\theta - \\theta_0)\\right) = -\\Sigma_0^{-1}(\\theta - \\theta_0)$$\n似然项的梯度为：\n$$\\nabla_{\\theta}\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))^2\\right) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} 2(\\tau_i - \\hat{\\tau}_i(\\theta))(-\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$$\n剩下的任务是找到点火延迟灵敏度 $\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$ 的表达式。\n\n**5. $\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$ 的伴随灵敏度分析：**\n点火延迟 $\\hat{\\tau}_i$ 由 $s_{i}(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta) = 0$ 隐式定义。我们使用链式法则对该恒等式关于 $\\theta$ 求导：\n$$\\frac{d}{d\\theta}\\left[s_{i}(y(\\hat{\\tau}_{i}(\\theta);\\theta),\\theta)\\right] = 0$$\n$$\\frac{\\partial s_{i}}{\\partial y}\\frac{d y}{d\\theta} + \\frac{\\partial s_{i}}{\\partial\\theta} = 0$$\n$y$ 关于 $\\theta$ 的全导数是 $\\frac{dy}{d\\theta} = \\frac{\\partial y}{\\partial t}\\frac{d\\hat{\\tau}_i}{d\\theta} + \\frac{\\partial y}{\\partial\\theta}$。由于 $\\nabla_{\\theta}$ 是一个行向量算子，这是针对单个分量 $\\theta_j$ 写的，然后进行推广。用向量符号表示：\n$$\\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\left( \\frac{dy}{dt}\\nabla_{\\theta}\\hat{\\tau}_i + \\nabla_{\\theta}y \\right) + \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top}= 0$$\n所有导数都在 $t=\\hat{\\tau}_i(\\theta)$ 处求值。重新整理以求解梯度 $\\nabla_{\\theta}\\hat{\\tau}_i$：\n$$\\left(\\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\frac{dy}{dt}\\right)\\nabla_{\\theta}\\hat{\\tau}_i = -\\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\nabla_{\\theta}y - \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top}$$\n令 $\\dot{s}_{i}(\\hat{\\tau}_{i}) = \\left(\\frac{\\partial s_{i}}{\\partial y}\\right)^{\\!\\top}\\frac{dy}{dt}\\big|_{t=\\hat{\\tau}_i} = q_{i}^{\\top}F_{i}(y(\\hat{\\tau}_i;\\theta),\\theta)$。这是一个标量，表示事件函数在事件发生时刻的变化率。\n$$\\nabla_{\\theta}\\hat{\\tau}_i = -\\frac{1}{\\dot{s}_{i}(\\hat{\\tau}_{i})} \\left(q_{i}^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta) + \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top} \\right)$$\n$q_{i}^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta)$ 项需要前向灵敏度矩阵 $\\nabla_{\\theta}y$，其计算成本很高。伴随方法提供了一种高效的替代方案。标准的伴随灵敏度恒等式指出，对于满足给定 ODE 和终端条件的伴随状态 $\\lambda_i(t)$：\n$$q_{i}^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta) = \\lambda_i(\\hat{\\tau}_i)^{\\top}\\nabla_{\\theta}y(\\hat{\\tau}_i;\\theta) = \\lambda_i(0)^{\\top}\\nabla_{\\theta}y_{i0}(\\theta) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\lambda_i(t)^{\\top} \\frac{\\partial F_i}{\\partial\\theta}(y(t;\\theta),\\theta) dt$$\n将此代回 $\\nabla_{\\theta}\\hat{\\tau}_i$ 的表达式中：\n$$\\nabla_{\\theta}\\hat{\\tau}_i(\\theta) = -\\frac{1}{\\dot{s}_{i}(\\hat{\\tau}_{i})} \\left( \\left(\\frac{\\partial s_{i}}{\\partial\\theta}\\right)^{\\!\\top} + \\lambda_i(0)^{\\top}\\nabla_{\\theta}y_{i0}(\\theta) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\lambda_i(t)^{\\top} \\frac{\\partial F_i}{\\partial\\theta} dt \\right)$$\n为简洁起见，并为了与梯度为列向量的向量微积分约定相匹配，我们对结果进行转置：\n$$\\nabla_{\\theta}\\hat{\\tau}_i(\\theta) = -\\frac{1}{\\dot{s}_{i}(\\hat{\\tau}_{i})} \\left( \\frac{\\partial s_{i}}{\\partial\\theta} + (\\nabla_{\\theta}y_{i0}(\\theta))^{\\top}\\lambda_i(0) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\left(\\frac{\\partial F_i}{\\partial\\theta}\\right)^{\\!\\top} \\lambda_i(t) dt \\right)$$\n\n**6. 最终梯度表达式：**\n最后，我们将 $\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$ 的表达式代入对数后验的梯度中：\n$$\\nabla_{\\theta}\\log p(\\theta\\mid d) = -\\Sigma_0^{-1}(\\theta - \\theta_0) + \\frac{1}{\\sigma^2}\\sum_{i=1}^{N}(\\tau_i - \\hat{\\tau}_i(\\theta))\\nabla_{\\theta}\\hat{\\tau}_i(\\theta)$$\n令 $\\delta_i = \\tau_i - \\hat{\\tau}_i(\\theta)$ 为实验 $i$ 的残差。\n$$\\nabla_{\\theta}\\log p(\\theta\\mid d) = -\\Sigma_0^{-1}(\\theta - \\theta_0) - \\sum_{i=1}^{N} \\frac{\\delta_i}{\\sigma^2 \\dot{s}_{i}(\\hat{\\tau}_{i})} \\left( \\frac{\\partial s_{i}}{\\partial\\theta} + (\\nabla_{\\theta}y_{i0}(\\theta))^{\\top}\\lambda_i(0) + \\int_{0}^{\\hat{\\tau}_i(\\theta)} \\left(\\frac{\\partial F_i}{\\partial\\theta}\\right)^{\\!\\top} \\lambda_i(t) dt \\right)$$\n这是对数后验梯度的最终表达式。它用前向状态解 $y(t;\\theta)$、后向伴随解 $\\lambda_i(t)$ 以及模型函数的已知偏导数来表示。\n项 $\\dot{s}_{i}(\\hat{\\tau}_{i})$ 定义为 $q_i^{\\top}F_i(y(\\hat{\\tau}_i;\\theta),\\theta)$。此外，$(\\nabla_{\\theta}y_{i0}(\\theta))^{\\top}\\lambda_i(0)$ 可写为 $\\lambda_i(0)^{\\top}\\nabla_{\\theta}y_{i0}(\\theta)$，而 $(\\frac{\\partial F_i}{\\partial\\theta})^{\\top} \\lambda_i(t)$ 可写为 $\\lambda_i(t)^{\\top}\\frac{\\partial F_i}{\\partial\\theta}$，这取决于雅可比矩阵 $\\frac{\\partial F_i}{\\partial\\theta}$ 的布局约定。我们将保留从推导中自然产生的形式。",
            "answer": "$$\\boxed{-\\Sigma_{0}^{-1}(\\theta - \\theta_{0}) - \\sum_{i=1}^{N} \\frac{\\tau_{i} - \\hat{\\tau}_{i}(\\theta)}{\\sigma^{2} \\left(q_{i}^{\\top}F_{i}(y(\\hat{\\tau}_{i};\\theta),\\theta)\\right)} \\left( \\frac{\\partial s_{i}}{\\partial\\theta} + \\left(\\nabla_{\\theta}y_{i0}(\\theta)\\right)^{\\!\\top}\\lambda_{i}(0) + \\int_{0}^{\\hat{\\tau}_{i}(\\theta)} \\left(\\frac{\\partial F_{i}}{\\partial\\theta}\\right)^{\\!\\top}\\lambda_{i}(t) dt \\right)}$$"
        },
        {
            "introduction": "解析梯度只有在数值实现正确时才有用。本练习深入探讨代码验证这一关键步骤，这是计算科学中的一项基本技能。通过一系列概念性问题 ()，你将学会如何执行梯度检查，诊断常见的误差来源——例如离散化不一致和数值噪声——并理解正确的伴随边界条件的关键重要性。",
            "id": "4009530",
            "problem": "你正在一个代表层流预混火焰的稳态一维反应传导模型中校准单个动力学参数，目标是进行贝叶斯校准和不确定性量化。状态变量是定义域 $x \\in [0,L]$ 上的温度场 $T(x;A)$，它由包含傅里叶传导和体积放热率的能量守恒定律决定：\n$$\n-\\frac{d}{dx}\\left(k \\frac{dT}{dx}\\right) + q\\,\\omega(T;A) = 0,\\quad x\\in(0,L),\n$$\n边界条件为 $T(0)=T_{\\mathrm{in}}$ 和 $k\\,\\frac{dT}{dx}\\big|_{x=L}=0$。此处 $k>0$ 是热导率，$q>0$ 是单位质量的反应热，反应速率遵循 Arrhenius 形式 $\\omega(T;A)=A \\exp\\!\\left(-\\frac{E}{R T}\\right)$，其中指前因子 $A>0$，活化能 $E>0$，通用气体常数 $R>0$。你在位置 $\\{x_i\\}_{i=1}^N$ 处有 $T$ 的测量值 $\\{y_i\\}_{i=1}^N$，测量噪声为方差是 $\\sigma^2$ 的独立高斯噪声。在 $\\ln A$ 的高斯先验均值为 $\\mu_A$、方差为 $\\sigma_A^2$ 的情况下，负对数后验（相差一个可加常数）为\n$$\nJ(A) = \\frac{1}{2\\sigma^2}\\sum_{i=1}^N\\left(T(x_i;A)-y_i\\right)^2 + \\frac{1}{2\\sigma_A^2}\\left(\\ln A - \\mu_A\\right)^2.\n$$\n\n你通过构造一个拉格朗日量，使用一个拉格朗日乘子（伴随）场 $\\lambda(x)$ 来强制执行状态方程，从而实现了一个连续伴随方法来计算梯度 $\\frac{dJ}{dA}$。然后，你求解伴随边值问题以消除对 $\\frac{\\partial T}{\\partial A}$ 的依赖。接着，你通过将你的伴随梯度与中心有限差分近似进行比较来进行梯度检查\n$$\ng_{\\mathrm{FD}}(h) = \\frac{J(A+h)-J(A-h)}{2h}\n$$\n对于步长 $h>0$，同时改变网格间距 $\\Delta x$ 和用于收敛离散化的正向和伴随方程的非线性求解器容差。\n\n从第一性原理出发，选择所有正确的陈述来回答以下问题。你的推理应从拉格朗日形式化的定义、伴随与边界条件的一致性属性，以及针对离散化和数值求解器误差的标准截断误差和扰动分析开始。以下哪些陈述是正确的？\n\nA. 如果在严格的非线性求解器容差和固定的中心差分步长 $h$（小到足以避免截断误差）下，随着网格加密，差异 $\\left|\\frac{dJ}{dA}\\big|_{\\mathrm{adj}} - g_{\\mathrm{FD}}(h)\\right|$ 与 $\\Delta x$ 的某个幂成比例减小，这表明连续伴随和离散正向问题之间存在离散化一致性误差。在这种情况下，使用与离散化正向残差一致的离散伴随（或提高伴随离散化的阶数）将消除主阶不匹配。\n\nB. 如果在固定的网格上，其他设置相同的情况下，有限差分梯度检查 $\\left| \\frac{dJ}{dA}\\big|_{\\mathrm{adj}} - g_{\\mathrm{FD}}(h)\\right|$ 随着 $h$ 减小而改善，直到一个最优的 $h^\\star$，但随着 $h \\to 0$ 却停滞或恶化，这表明受到了舍入误差和非精确非线性求解的污染；收紧非线性求解器容差会降低噪声基底，并将 $h^\\star$ 向更小的值移动，从而改善一致性。\n\nC. 如果伴随边界条件不是分部积分产生的边界项所要求的齐次对应项（例如，当正向问题施加 $T(0)=T_{\\mathrm{in}}$ 时，取 $\\lambda(0)$ 为自由），那么即使 $\\Delta x$ 趋于零且求解器容差很小，伴随梯度也会表现出一个 $\\mathcal{O}(1)$ 的偏差，该偏差不会随着网格加密而收敛消失；强制执行正确的伴随边界条件是梯度检查通过的必要条件。\n\nD. 在贝叶斯校准中，如果马尔可夫链蒙特卡洛（MCMC）采样器看起来收敛了，那么梯度验证就是不必要的；有偏的梯度不会对后验推断产生实质性影响，因此可以跳过梯度检查。\n\nE. 当使用从离散化正向残差导出的离散伴随时，无论正向和伴随非线性系统求解得多么松散，伴随梯度都等于离散目标函数相对于 $A$ 的精确导数；因此，即使非线性残差容差很大，有限差分梯度检查也会通过。\n\n选择所有适用项。",
            "solution": "用户要求对问题陈述进行严格验证，然后对每个选项进行第一性原理推导和评估。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- **控制方程：** $-\\frac{d}{dx}\\left(k \\frac{dT}{dx}\\right) + q\\,\\omega(T;A) = 0$ for $x\\in(0,L)$。\n- **状态变量：** $T(x;A)$，温度场。\n- **待校准参数：** $A$，指前因子。\n- **反应速率：** $\\omega(T;A)=A \\exp\\!\\left(-\\frac{E}{R T}\\right)$。\n- **边界条件：** $T(0)=T_{\\mathrm{in}}$ (Dirichlet)，$k\\,\\frac{dT}{dx}\\big|_{x=L}=0$ (Neumann)。\n- **其他常数：** $k>0$, $q>0$, $E>0$, $R>0$。\n- **目标函数（负对数后验）：** $J(A) = \\frac{1}{2\\sigma^2}\\sum_{i=1}^N\\left(T(x_i;A)-y_i\\right)^2 + \\frac{1}{2\\sigma_A^2}\\left(\\ln A - \\mu_A\\right)^2$。\n- **数据：** 在位置 $\\{x_i\\}_{i=1}^N$ 处的测量值 $\\{y_i\\}_{i=1}^N$。\n- **噪声模型：** 方差为 $\\sigma^2$ 的独立高斯噪声。\n- **先验模型：** $\\ln A$ 的高斯先验，均值为 $\\mu_A$，方差为 $\\sigma_A^2$。\n- **方法论：** 使用伴随场 $\\lambda(x)$ 构造拉格朗日量 $\\mathcal{L}$ 的连续伴随方法计算梯度 $\\frac{dJ}{dA}$。\n- **验证：** 将伴随梯度 $\\frac{dJ}{dA}\\big|_{\\mathrm{adj}}$ 与中心有限差分近似 $g_{\\mathrm{FD}}(h) = \\frac{J(A+h)-J(A-h)}{2h}$ 进行比较的梯度检查。\n- **数值参数：** 网格间距 $\\Delta x$ 和非线性求解器容差是可变的。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题描述了一个简化但标准的用于一维火焰传播的模型（一个传导-反应问题）。控制方程基于能量守恒原理。Arrhenius 动力学模型是化学反应工程的基础。用于参数估计的贝叶斯框架是一种标准的统计方法。使用伴随方法进行灵敏度分析是现代计算科学的基石。该问题在科学上和数学上都是合理的。\n- **良定性：** 温度的边值问题在典型条件下是良定的。目标函数定义明确。问题要求对伴随求解器的验证过程进行概念性分析，这是数值方法中一个标准且有意义的任务。\n- **客观性：** 问题以精确、客观的数学和计算术语陈述（例如，“连续伴随方法”、“离散化一致性误差”、“非线性求解器容差”）。没有主观或模棱两可的陈述。\n\n**步骤3：结论与行动**\n问题陈述是有效的。它提出了计算科学中一个标准、良定的场景，并就所涉及的数值方法的验证提出了清晰、技术性强的问题。我将继续对每个选项进行详细分析。\n\n### 第一性原理分析\n问题的核心是计算目标函数 $J(A)$ 的梯度，其中状态变量 $T$ 通过一个控制微分方程成为参数 $A$ 的隐式函数，我们可以将其抽象地写为 $R(T(A), A) = 0$。全导数为 $\\frac{dJ}{dA} = \\frac{\\partial J}{\\partial A} + \\frac{\\partial J}{\\partial T}\\frac{dT}{dA}$。\n\n连续伴随方法引入一个拉格朗日量 $\\mathcal{L}(T, A, \\lambda) = J(T, A) + \\int_0^L \\lambda(x) R(T, A) \\, dx$。\n选择伴随场 $\\lambda(x)$ 以满足伴随方程，该方程由 $\\mathcal{L}$ 相对于 $T$ 的泛函导数为零的条件导出：$\\frac{\\delta \\mathcal{L}}{\\delta T} = 0$。这种选择消除了计算灵敏度 $\\frac{dT}{dA}$ 的需要，因为它确保了涉及该灵敏度的项 $\\langle \\frac{\\delta\\mathcal{L}}{\\delta T}, \\frac{dT}{dA} \\rangle$ 为零。梯度则由 $\\frac{dJ}{dA} = \\frac{d\\mathcal{L}}{dA} = \\frac{\\partial \\mathcal{L}}{\\partial A}$ 给出。\n\n伴随方程是 $\\lambda(x)$ 的欧拉-拉格朗日方程，其通过分部积分的推导过程也得出了所需的伴随边界条件。对于给定的正向问题，其边界条件为 $T(0)=T_{\\mathrm{in}}$ (Dirichlet) 和 $k\\frac{dT}{dx}|_{x=L}=0$ (Neumann)，相应的伴随边界条件是 $\\lambda(0)=0$ (齐次 Dirichlet) 和 $k\\frac{d\\lambda}{dx}|_{x=L}=0$ (齐次 Neumann)。\n\n伴随实现的验证涉及将伴随梯度 $\\frac{dJ}{dA}\\big|_{\\mathrm{adj}}$ 与有限差分近似 $g_{\\mathrm{FD}}(h)$ 进行比较。这个过程对几种误差源很敏感：\n1.  **离散化误差：** 在间距为 $\\Delta x$ 的有限网格上近似连续正向和伴随问题时产生的误差。\n2.  **有限差分截断误差：** 用有限步长 $h$ 近似导数时的误差，例如，对于中心差分是 $\\mathcal{O}(h^2)$。\n3.  **数值噪声：** 来自有限机器精度（舍入）和非精确迭代求解（求解器容差）的误差。\n\n### 逐项分析\n\n**A. 如果在严格的非线性求解器容差和固定的中心差分步长 $h$（小到足以避免截断误差）下，随着网格加密，差异 $\\left|\\frac{dJ}{dA}\\big|_{\\mathrm{adj}} - g_{\\mathrm{FD}}(h)\\right|$ 与 $\\Delta x$ 的某个幂成比例减小，这表明连续伴随和离散正向问题之间存在离散化一致性误差。在这种情况下，使用与离散化正向残差一致的离散伴随（或提高伴随离散化的阶数）将消除主阶不匹配。**\n\n该陈述准确地描述了“先微分后离散”（连续伴随）与“先离散后微分”（在数值求解器上进行有限差分）的范式。伴随梯度是通过离散化连续伴随方程来计算的。有限差分梯度是通过重复求解离散化的正向方程来计算的。这两种方法计算的是略有不同的东西的梯度：一种近似连续问题的梯度，另一种近似离散问题的梯度。它们之间的差异，即所谓的离散化一致性误差，预计会随着网格加密（$\\Delta x \\to 0$）而减小，其速率通常与离散化的精度阶数有关。离散伴随是从离散化的正向方程解析推导出来的，它计算的是离散目标函数的精确梯度。因此，将离散伴随梯度与（在同一离散问题上的）有限差分梯度进行比较，将在有限差分截断误差和数值噪声的范围内显示一致性，从而消除了依赖于 $\\Delta x$ 的误差。提高伴随离散化的阶数是另一种方法，可以减少但不能完全消除一致性误差。该陈述是对伴随方法中这一基本概念的正确而精确的描述。\n**结论：正确**\n\n**B. 如果在固定的网格上，其他设置相同的情况下，有限差分梯度检查 $\\left| \\frac{dJ}{dA}\\big|_{\\mathrm{adj}} - g_{\\mathrm{FD}}(h)\\right|$ 随着 $h$ 减小而改善，直到一个最优的 $h^\\star$，但随着 $h \\to 0$ 却停滞或恶化，这表明受到了舍入误差和非精确非线性求解的污染；收紧非线性求解器容差会降低噪声基底，并将 $h^\\star$ 向更小的值移动，从而改善一致性。**\n\n该陈述准确地描述了在存在数值噪声的情况下，有限差分导数近似的典型行为。$g_{\\mathrm{FD}}(h)$ 的总误差是截断误差（其量级为 $\\mathcal{O}(h^2)$）和来自分子 $J(A+h) - J(A-h)$ 的条件/噪声误差之和。当 $h \\to 0$ 时，这变成两个几乎相等的数相减，放大了舍入误差，其量级为 $\\mathcal{O}(\\epsilon_{\\text{mach}}/h)$。在计算实践中，由于有限的求解器容差（比如 $\\tau_{\\text{tol}}$），函数求值 $J(A\\pm h)$ 并不是精确的。这引入了一个额外的误差，其量级也为 $\\mathcal{O}(\\tau_{\\text{tol}}/h)$。对于小的 $h$，组合的噪声项占主导地位，导致总误差增加。这种权衡导致了一个最优步长 $h^\\star$。通过收紧求解器容差（减小 $\\tau_{\\text{tol}}$），噪声基底被降低。这减小了 $\\mathcal{O}(1/h)$ 误差项的量级，从而将最优步长 $h^\\star$ 移向更小的值，允许更精确的有限差分近似和更好的梯度检查。该描述是数值误差分析的一个教科书式的例子。\n**结论：正确**\n\n**C. 如果伴随边界条件不是分部积分产生的边界项所要求的齐次对应项（例如，当正向问题施加 $T(0)=T_{\\mathrm{in}}$ 时，取 $\\lambda(0)$ 为自由），那么即使 $\\Delta x$ 趋于零且求解器容差很小，伴随梯度也会表现出一个 $\\mathcal{O}(1)$ 的偏差，该偏差不会随着网格加密而收敛消失；强制执行正确的伴随边界条件是梯度检查通过的必要条件。**\n\n伴随方程的推导依赖于分部积分，这会产生边界项。为了确保伴随算子被正确定义，并且灵敏度项 $\\frac{\\partial J}{\\partial T}\\frac{dT}{dA}$ 被恰当抵消，这些边界项必须对于状态变量的所有容许变分都为零。这个过程唯一地确定了伴随边界条件。对于正向变量的 Dirichlet 条件（例如，$T(0)=T_{\\mathrm{in}}$），相应的伴随边界条件是齐次 Dirichlet 条件（例如，$\\lambda(0)=0$）。如果施加了不正确的边界条件，伴随形式化在数学上就是错误的。得到的“梯度”从根本上就是不正确的，因为它缺少边界贡献或未能抵消状态灵敏度。这不是一个随着 $\\Delta x \\to 0$ 而消失的离散化误差；它是连续公式本身的一个 $\\mathcal{O}(1)$ 误差。因此，这个不正确的伴随梯度与真实梯度之间的差异不会收敛到零。该陈述完全正确，并指出了一个关键且常见的实现陷阱。\n**结论：正确**\n\n**D. 在贝叶斯校准中，如果马尔可夫链蒙特卡洛（MCMC）采样器看起来收敛了，那么梯度验证就是不必要的；有偏的梯度不会对后验推断产生实质性影响，因此可以跳过梯度检查。**\n\n这个陈述是危险且错误的。像哈密顿蒙特卡洛（HMC）或无U形转弯采样器（NUTS）这样的高级MCMC算法之所以高效，是因为它们使用对数后验的梯度来引导对参数空间的探索。这些采样器的数学基础（保证它们从正确的后验分布中采样）关键地依赖于拥有正确的梯度。有偏的梯度将导致采样器靶向一个不同的、不正确的分布。虽然收敛诊断可能表明链已经稳定，但它将稳定在错误的目标上。这将导致对参数及其不确定性的有偏估计。声称这不会“实质性影响”推断是错误的；其影响可能是深远的，导致科学上无效的结论。在部署任何基于梯度的推断算法之前，梯度验证是必不可少的一步。\n**结论：不正确**\n\n**E. 当使用从离散化正向残差导出的离散伴随时，无论正向和伴随非线性系统求解得多么松散，伴随梯度都等于离散目标函数相对于 $A$ 的精确导数；因此，即使非线性残差容差很大，有限差分梯度检查也会通过。**\n\n离散伴随方法确实提供了离散目标函数的梯度，其精度可达机器精度。然而，这依赖于底层的离散控制方程得到满足。离散目标函数 $J_{\\Delta x}(A)$ 是通过非线性方程组 $R_{\\Delta x}(T_{\\Delta x}, A)=0$ 的解 $T_{\\Delta x}$ 隐式定义的。如果这个系统被“松散地”求解（即，容差很大），得到的解 $\\tilde{T}_{\\Delta x}$ 并不满足该方程；相反，$R_{\\Delta x}(\\tilde{T}_{\\Delta x}, A) \\approx \\epsilon \\neq 0$。计算出的伴随梯度和有限差分梯度都将基于这些不精确的、未收敛的解。如选项B中所解释的，有限差分计算将充满噪声。离散伴随计算本身，由于使用了未收敛的解 $\\tilde{T}_{\\Delta x}$ 和未收敛的伴随解 $\\tilde{\\lambda}_{\\Delta x}$，也不再对应于 $J_{\\Delta x}(A)$ 的真实导数。无法保证这两个有缺陷的计算会一致。在实践中，正向和伴随系统都需要紧密收敛，梯度检查才能通过。因此，这个陈述是错误的。\n**结论：不正确**",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "在这里，我们将理论付诸实践，见证伴随方法的真正威力。本练习要求你构建一个完整的贝叶斯校准流程，比较一个现代的、基于梯度的采样器（哈密顿蒙特卡洛）与一个传统的、无梯度的采样器（随机游走梅特罗波利斯）。通过量化采样效率的提升 ()，你将对伴随方法派生的梯度为何能彻底改变复杂物理模型中的不确定性量化问题，获得切实的理解。",
            "id": "4009504",
            "problem": "考虑一个标量简化燃烧机理，其由一个归一化进程变量 $Y(t) \\in [0,1]$ 的Arrhenius型一级反应控制，其动力学方程为\n$$\n\\frac{dY}{dt} = f(Y,t;\\theta) = -k(t;\\theta)\\,Y(t),\n$$\n其中反应速率为\n$$\nk(t;\\theta) = A\\,\\exp\\!\\left(-\\frac{E}{R\\,T(t)}\\right),\n$$\n$A$ 是指前因子，$E$ 是活化能，$R$ 是普适气体常数，$T(t)$ 是一个给定的开尔文温度历史。设初始条件为 $Y(0)=1$，温度为线性斜坡，\n$$\nT(t) = T_0 + \\frac{T_f - T_0}{t_f}\\,t,\\quad t\\in[0,t_f],\n$$\n其中 $t_f>0$。在离散的时间点集 $\\{t_i\\}$ 上可以获得观测值，形式为 $y_i = Y(t_i;\\theta_{\\text{true}}) + \\eta_i$，其中 $\\eta_i$ 是独立的、均值为零、标准差为 $\\sigma$ 的高斯误差。我们的目标是，在一个对变换后参数 $\\phi = (\\phi_A,\\phi_E) = (\\ln A, \\ln E)$ 具有高斯先验的贝叶斯模型下，根据这些观测值校准参数 $\\theta=(A,E)$，\n$$\n\\phi \\sim \\mathcal{N}(\\phi_0, \\Sigma_0),\n$$\n其中 $\\phi_0$ 和 $\\Sigma_0$ 是指定的。\n\n定义 $\\phi$ 的负对数后验（不计一个加法常数）为\n$$\nJ(\\phi) = \\frac{1}{2}\\sum_{i} \\frac{\\left(Y(t_i;\\theta(\\phi)) - y_i\\right)^2}{\\sigma^2} + \\frac{1}{2}(\\phi - \\phi_0)^\\top \\Sigma_0^{-1}(\\phi - \\phi_0),\n$$\n其中变换关系为 $\\theta(\\phi)=(\\exp(\\phi_A),\\exp(\\phi_E))$。设正向模型 $Y(t;\\theta)$ 是通过在均匀网格上使用稳定的时间步进方法数值计算得到的。\n\n将使用伴随灵敏度分析来计算数据失配项相对于 $\\theta$ 的梯度。对于对应于离散时间失配\n$$\n\\mathcal{J}_{\\text{data}}(\\theta) = \\frac{1}{2}\\sum_i \\frac{\\left(Y(t_i;\\theta)-y_i\\right)^2}{\\sigma^2},\n$$\n的连续时间标量伴随 $\\lambda(t)$，伴随变量满足终端条件 $\\lambda(t_f)=0$，微分方程\n$$\n\\frac{d\\lambda}{dt} = -\\frac{\\partial f}{\\partial Y}(Y(t),t;\\theta)\\,\\lambda(t),\n$$\n以及在观测时间 $t_i$ 的跳跃条件，\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\frac{Y(t_i;\\theta)-y_i}{\\sigma^2}.\n$$\n数据失配相对于参数的梯度为\n$$\n\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial \\theta} = \\int_0^{t_f} \\lambda(t)\\,\\frac{\\partial f}{\\partial \\theta}(Y(t),t;\\theta)\\,dt,\n$$\n而相对于 $\\phi$ 的梯度则通过链式法则得出：\n$$\n\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial \\phi_A} = A\\,\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial A},\\quad\n\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial \\phi_E} = E\\,\\frac{\\partial \\mathcal{J}_{\\text{data}}}{\\partial E}.\n$$\n先验梯度为 $\\frac{\\partial \\mathcal{J}_{\\text{prior}}}{\\partial \\phi} = \\Sigma_0^{-1}(\\phi - \\phi_0)$。\n\n任务：\n- 实现哈密顿蒙特卡洛（HMC），使用伴随法推导的 $J(\\phi)$ 的梯度，从 $\\phi$ 的后验分布中采样。\n- 实现一个关于 $\\phi$ 的、使用高斯提议的无梯度的随机游走Metropolis（RWM）采样器，以从同一后验分布中采样。\n- 使用指定的基准真相 $\\theta_{\\text{true}}$ 和标准差为 $\\sigma$ 的高斯噪声生成合成观测数据 $y_i$。\n\n使用以下常数、单位和离散化设置：\n- 普适气体常数 $R = 8.314\\ \\text{J}/(\\text{mol}\\cdot\\text{K})$。\n- 时间范围 $t_f = 0.02\\ \\text{s}$，均匀离散为 $N=200$ 步，因此 $\\Delta t = t_f/N$。\n- 观测时间位于索引 $i \\in \\{20, 60, 100, 140, 180\\}$ 处，对应于 $t_i = i\\,\\Delta t$。\n- 初始条件 $Y(0)=1$。\n- 基准真相参数 $A_{\\text{true}}=10^5\\ \\text{s}^{-1}$ 和 $E_{\\text{true}}=8.0\\times 10^4\\ \\text{J/mol}$。\n- 先验均值 $\\phi_0 = (\\ln(10^5), \\ln(8.0\\times 10^4))$ 和先验协方差\n$$\n\\Sigma_0 = \\begin{bmatrix} 0.5^2  0 \\\\ 0  0.4^2 \\end{bmatrix}.\n$$\n\n设计三个测试案例（温度方案和噪声水平）：\n- 案例1（基准）：$T_0=1500\\ \\text{K}$，$T_f=1800\\ \\text{K}$，$\\sigma=0.02$。\n- 案例2（较低温度，较弱的动力学）：$T_0=1200\\ \\text{K}$，$T_f=1400\\ \\text{K}$，$\\sigma=0.02$。\n- 案例3（较高温度，信息量更大的数据）：$T_0=1800\\ \\text{K}$，$T_f=2000\\ \\text{K}$，$\\sigma=0.005$。\n\n采样要求：\n- 使用哈密顿蒙特卡洛（HMC），设置单位质量矩阵，蛙跳步长 $\\epsilon=0.02$，每步提议含 $L=10$ 个步骤；抽取 $150$ 个样本，并将前 $30$ 个作为预烧期丢弃。\n- 使用随机游走Metropolis（RWM），对 $\\phi$ 使用高斯提议，每个分量的独立标准差为 $0.05$；抽取 $300$ 个样本，并将前 $60$ 个作为预烧期丢弃。\n\n对于每个案例，从HMC样本和RWM样本中估计 $A$ 和 $E$ 的后验方差，并计算方差减少量（以小数形式表示）\n$$\nr_A = 1 - \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{HMC}}(A)}{\\widehat{\\mathrm{Var}}_{\\mathrm{RWM}}(A)},\\quad\nr_E = 1 - \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{HMC}}(E)}{\\widehat{\\mathrm{Var}}_{\\mathrm{RWM}}(E)}.\n$$\n这些 $r_A$ 和 $r_E$ 是无量纲的。\n\n您的程序应生成单行输出，其中包含一个包含在方括号中的逗号分隔列表的结果。该列表必须按以下顺序包含六个浮点数：\n$$\n[\\ r_A^{(1)},\\ r_E^{(1)},\\ r_A^{(2)},\\ r_E^{(2)},\\ r_A^{(3)},\\ r_E^{(3)}\\ ],\n$$\n其中上标表示案例编号。\n\n所有物理量必须按上文指定的自然单位（温度用开尔文，时间用秒）进行计算。最终输出是无量纲的分数。此问题不涉及角度。\n\n解决方案必须是一个完整的、可运行的程序，实现正向模型、基于伴随的梯度计算、哈密顿蒙特卡洛（HMC）和随机游走Metropolis（RWM），然后执行三个测试案例并按所述的确切格式打印结果。",
            "solution": "用户提供的问题已经过评估，并被确定为**有效**。该问题具有科学依据、问题设定良好、目标明确，并包含进行唯一可验证求解所需的所有必要信息。\n\n该问题要求实现并比较两种贝叶斯采样算法——随机游走Metropolis（RWM）和哈密顿蒙特卡洛（HMC）——用于一个简化燃烧模型中的参数推断。目标是通过比较所得后验参数样本的方差，来量化基于梯度的HMC方法相对于无梯度的RWM方法的效率增益。HMC所需的梯度将使用伴随灵敏度分析方法高效计算。\n\n问题的核心是一个单步一级化学反应，其状态由一个进程变量 $Y(t)$ 描述。其动力学由常微分方程（ODE）控制：\n$$\n\\frac{dY}{dt} = -k(t;\\theta)\\,Y(t)\n$$\n初始条件为 $Y(0)=1$。待推断的参数是 $\\theta = (A, E)$，它们是Arrhenius反应速率表达式中的指前因子和活化能：\n$$\nk(t;\\theta) = A\\,\\exp\\!\\left(-\\frac{E}{R\\,T(t)}\\right)\n$$\n温度 $T(t)$ 遵循一个给定的线性斜坡，$R$ 是普适气体常数。\n\n解决方案将通过以下步骤进行开发：\n1.  **数值正向模型**：给定一组参数 $\\theta$，离散并求解 $Y(t)$ 的控制ODE。\n2.  **贝叶斯公式**：定义负对数后验概率密度函数，该函数作为MCMC采样器的目标。\n3.  **基于伴随的梯度计算**：实现伴随方法以计算负对数后验相对于参数的梯度。这对HMC采样器至关重要。\n4.  **MCMC采样器**：实现RWM和HMC两种算法。\n5.  **分析与比较**：对于每个指定的测试案例，生成合成数据，运行两种采样器，并计算所需的方差减少率。\n\n**1. 数值正向模型**\n\n时间域 $[0, t_f]$ 被离散为 $N$ 个大小为 $\\Delta t = t_f/N$ 的均匀步长。$Y(t)$ 的ODE在这个网格上求解。一个简单且稳定的数值积分器选择是前向Euler方法。在每个时间步 $t_n = n\\,\\Delta t$，更新规则是：\n$$\nY_{n+1} = Y_n + \\Delta t \\cdot f(Y_n, t_n; \\theta) = Y_n - \\Delta t \\cdot k(t_n;\\theta) Y_n = Y_n (1 - \\Delta t \\cdot k_n)\n$$\n其中 $k_n = k(t_n; \\theta)$。如果 $|\\,1 - \\Delta t \\cdot k_n\\,| \\le 1$，该格式在数值上是稳定的，由于 $k_n > 0$，这简化为 $\\Delta t \\cdot k_n \\le 2$。对于给定的参数，此条件得到满足，使得前向Euler成为一个合适的选择。\n\n**2. 贝叶斯公式**\n\n推断是在变换后的参数 $\\phi = (\\ln A, \\ln E)$ 上执行的。后验分布正比于似然乘以先验。负对数后验 $J(\\phi)$（不计一个加法常数）是数据失配项（负对数似然）和先验项（负对数先验）之和：\n$$\nJ(\\phi) = J_{\\text{data}}(\\phi) + J_{\\text{prior}}(\\phi)\n$$\n假设观测误差是独立的高斯分布，数据失配项为：\n$$\nJ_{\\text{data}}(\\phi) = \\frac{1}{2}\\sum_{i} \\frac{\\left(Y(t_i;\\theta(\\phi)) - y_i\\right)^2}{\\sigma^2}\n$$\n其中 $y_i$ 是在时间 $t_i$ 的观测值。$\\phi$ 上的先验是高斯分布，$\\phi \\sim \\mathcal{N}(\\phi_0, \\Sigma_0)$，所以先验项为：\n$$\nJ_{\\text{prior}}(\\phi) = \\frac{1}{2}(\\phi - \\phi_0)^\\top \\Sigma_0^{-1}(\\phi - \\phi_0)\n$$\n\n**3. 基于伴随的梯度计算**\n\nHMC需要势能 $\\nabla_\\phi J(\\phi)$ 的梯度。先验项的梯度很简单：$\\nabla_\\phi J_{\\text{prior}}(\\phi) = \\Sigma_0^{-1}(\\phi - \\phi_0)$。数据失配项的梯度使用连续伴随方法计算。\n\n该过程包括三个阶段：\na. **正向求解**：状态方程 $Y(t)$ 从 $t=0$ 到 $t=t_f$ 向前求解，并存储其轨迹。\nb. **反向伴随求解**：伴随变量 $\\lambda(t)$ 从 $t=t_f$ 到 $t=0$ 向后求解。伴随方程为：\n$$\n\\frac{d\\lambda}{dt} = -\\frac{\\partial f}{\\partial Y}(Y(t),t;\\theta)\\,\\lambda(t) = k(t;\\theta)\\,\\lambda(t)\n$$\n终端条件为 $\\lambda(t_f)=0$。在每个观测时间 $t_i$，伴随变量经历一次跳跃：\n$$\n\\lambda(t_i^-) = \\lambda(t_i^+) + \\frac{Y(t_i;\\theta)-y_i}{\\sigma^2}\n$$\n我们向后积分这个ODE。由于其结构，显式Euler格式会不稳定。我们使用隐式（后向）Euler格式以保证无条件稳定性。对于从 $t_n$ 到 $t_{n-1}$ 的时间步，更新为：\n$$\n\\frac{\\lambda_n - \\lambda_{n-1}}{\\Delta t} = k_{n-1} \\lambda_{n-1} \\implies \\lambda_{n-1} = \\frac{\\lambda_n}{1 + \\Delta t \\cdot k_{n-1}}\n$$\n在每个观测索引处，通过在下一次积分步骤之前将强迫项加到 $\\lambda$ 上来应用跳跃条件。\n\nc. **梯度积分**：相对于原始参数 $\\theta=(A,E)$ 的梯度通过积分伴随变量与右侧函数 $f$ 相对于各参数的灵敏度的乘积来计算：\n$$\n\\frac{\\partial J_{\\text{data}}}{\\partial A} = \\int_0^{t_f} \\lambda(t) \\frac{\\partial f}{\\partial A} dt = \\int_0^{t_f} \\lambda(t) \\left(-\\frac{k(t)}{A} Y(t)\\right) dt\n$$\n$$\n\\frac{\\partial J_{\\text{data}}}{\\partial E} = \\int_0^{t_f} \\lambda(t) \\frac{\\partial f}{\\partial E} dt = \\int_0^{t_f} \\lambda(t) \\left(\\frac{k(t)}{R T(t)} Y(t)\\right) dt\n$$\n这些积分在离散化时间网格上使用梯形法则进行数值计算。\n\n最后，应用链式法则以获得相对于变换后参数 $\\phi$ 的梯度：\n$$\n\\frac{\\partial J_{\\text{data}}}{\\partial \\phi_A} = \\frac{\\partial J_{\\text{data}}}{\\partial A} \\frac{\\partial A}{\\partial \\phi_A} = A \\frac{\\partial J_{\\text{data}}}{\\partial A}, \\qquad \\frac{\\partial J_{\\text{data}}}{\\partial \\phi_E} = E \\frac{\\partial J_{\\text{data}}}{\\partial E}\n$$\n总梯度则为 $\\nabla_\\phi J = \\nabla_\\phi J_{\\text{data}} + \\nabla_\\phi J_{\\text{prior}}$。\n\n**4. MCMC采样器**\n\na. **随机游走Metropolis (RWM)**：这种无梯度方法通过向当前状态 $\\phi_{\\text{curr}}$ 添加一个随机扰动来生成一个新的候选状态 $\\phi_{\\text{prop}}$，即 $\\phi_{\\text{prop}} = \\phi_{\\text{curr}} + \\mathcal{N}(0, \\Sigma_{\\text{prop}})$。候选状态以概率 $\\alpha = \\min\\left(1, \\exp(J(\\phi_{\\text{curr}}) - J(\\phi_{\\text{prop}}))\\right)$ 被接受。\n\nb. **哈密顿蒙特卡洛 (HMC)**：这种基于梯度的方法通过引入一个辅助动量变量 $p$ 来提高采样效率。它模拟了一个虚拟粒子在势能面 $U(\\phi) = J(\\phi)$ 上的动力学。通过使用一个辛积分器（通常是蛙跳法）演化系统 $(\\phi, p)$ 一段时间 $L\\epsilon$ 来生成一个提议。然后基于总哈密顿量 $H(\\phi, p) = U(\\phi) + K(p)$（其中 $K(p)$ 是动能）的变化来接受或拒绝该提议。梯度的使用使HMC能够在参数空间中进行大的、有方向的移动，通常导致比RWM更快的收敛和更少相关的样本。\n\n**5. 分析与比较**\n\n对于三个测试案例中的每一个，我们首先使用真实参数 $\\theta_{\\text{true}}$ 和指定的噪声水平 $\\sigma$ 生成合成数据。然后，我们运行RWM和HMC采样器，从 $\\phi$ 的后验分布中生成样本链。在丢弃初始的预烧期后，剩余的样本通过指数运算转换回原始参数空间 $(A,E)$。从两个链中估计 $A$ 和 $E$ 后验的样本方差。对于一个参数 $\\xi \\in \\{A, E\\}$，方差减少率计算如下：\n$$\nr_\\xi = 1 - \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{HMC}}(\\xi)}{\\widehat{\\mathrm{Var}}_{\\mathrm{RWM}}(\\xi)}\n$$\n$r_\\xi$ 的正值表明HMC采样器产生了方差更低的估计，意味着采样效率更高。最终输出包含所有三个测试案例计算出的 $r_A$ 和 $r_E$ 的值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the final result.\n    \"\"\"\n    # Set a seed for reproducibility of synthetic data and MCMC runs\n    np.random.seed(42)\n\n    # --- Global Constants and Setup ---\n    R = 8.314  # J/(mol.K)\n    t_f = 0.02  # s\n    N = 200  # Number of time steps\n    dt = t_f / N\n    t_grid = np.linspace(0, t_f, N + 1)\n    \n    # Observation time indices\n    obs_indices = np.array([20, 60, 100, 140, 180])\n\n    # Ground truth parameters\n    A_true = 1e5\n    E_true = 8.0e4\n    phi_true = np.array([np.log(A_true), np.log(E_true)])\n\n    # Prior distribution parameters\n    phi_0 = np.array([np.log(1e5), np.log(8.0e4)])\n    Sigma_0 = np.array([[0.5**2, 0], [0, 0.4**2]])\n    Sigma0_inv = np.linalg.inv(Sigma_0)\n\n    # MCMC settings\n    hmc_params = {'n_samples': 150, 'n_burn': 30, 'epsilon': 0.02, 'L': 10}\n    rwm_params = {'n_samples': 300, 'n_burn': 60, 'proposal_std': np.array([0.05, 0.05])}\n\n    # Test cases\n    test_cases = [\n        {'T0': 1500.0, 'Tf': 1800.0, 'sigma': 0.02},  # Case 1\n        {'T0': 1200.0, 'Tf': 1400.0, 'sigma': 0.02},  # Case 2\n        {'T0': 1800.0, 'Tf': 2000.0, 'sigma': 0.005}, # Case 3\n    ]\n\n    results = []\n\n    # --- Core Functions ---\n\n    def forward_model(phi, T_t):\n        \"\"\"Solves the forward ODE using Forward Euler.\"\"\"\n        A = np.exp(phi[0])\n        E = np.exp(phi[1])\n        \n        Y = np.zeros(N + 1)\n        Y[0] = 1.0\n        \n        k_t = A * np.exp(-E / (R * T_t))\n\n        for n in range(N):\n            Y[n + 1] = Y[n] * (1.0 - dt * k_t[n])\n        return Y, k_t\n\n    def get_log_posterior_and_grad(phi, T_t, y_obs, sigma):\n        \"\"\"\n        Computes the negative log posterior and its gradient using the adjoint method.\n        \"\"\"\n        A = np.exp(phi[0])\n        E = np.exp(phi[1])\n\n        # 1. Forward solve\n        Y_t, k_t = forward_model(phi, T_t)\n        \n        # 2. Compute log posterior value\n        Y_model_obs = Y_t[obs_indices]\n        misfit = 0.5 * np.sum(((Y_model_obs - y_obs) / sigma)**2)\n        prior_term = 0.5 * (phi - phi_0).T @ Sigma0_inv @ (phi - phi_0)\n        log_post_val = misfit + prior_term\n\n        # 3. Adjoint solve (backward in time)\n        lam = np.zeros(N + 1)\n        obs_map = {idx: i for i, idx in enumerate(obs_indices)}\n        \n        for n in range(N, 0, -1):\n            lam_plus = lam[n]\n            if n in obs_map:\n                obs_idx = obs_map[n]\n                lam_plus += (Y_t[n] - y_obs[obs_idx]) / sigma**2\n            \n            lam[n - 1] = lam_plus / (1.0 + dt * k_t[n - 1])\n        \n        # 4. Gradient integral calculation\n        integrand_A = lam * (-k_t / A * Y_t)\n        # Handle potential division by zero if T=0, though not possible in this problem setup\n        T_t_safe = np.maximum(T_t, 1e-6)\n        integrand_E = lam * (k_t / (R * T_t_safe) * Y_t)\n        \n        grad_J_data_A = np.trapz(integrand_A, x=t_grid)\n        grad_J_data_E = np.trapz(integrand_E, x=t_grid)\n\n        # 5. Chain rule for gradient wrt phi\n        grad_J_data_phi_A = A * grad_J_data_A\n        grad_J_data_phi_E = E * grad_J_data_E\n        grad_J_data_phi = np.array([grad_J_data_phi_A, grad_J_data_phi_E])\n        \n        # 6. Add prior gradient\n        grad_J_prior_phi = Sigma0_inv @ (phi - phi_0)\n        \n        total_grad = grad_J_data_phi + grad_J_prior_phi\n        \n        return log_post_val, total_grad\n\n    def hmc_sampler(initial_phi, T_t, y_obs, sigma):\n        \"\"\"Hamiltonian Monte Carlo sampler.\"\"\"\n        n_samples = hmc_params['n_samples']\n        epsilon = hmc_params['epsilon']\n        L = hmc_params['L']\n        \n        samples = np.zeros((n_samples, 2))\n        phi = np.copy(initial_phi)\n        \n        current_U, grad_U = get_log_posterior_and_grad(phi, T_t, y_obs, sigma)\n        \n        for i in range(n_samples):\n            p = np.random.normal(size=2)\n            current_K = 0.5 * np.sum(p**2)\n            \n            phi_prop, p_prop = np.copy(phi), np.copy(p)\n            \n            # Leapfrog integration\n            p_prop -= 0.5 * epsilon * grad_U\n            for j in range(L):\n                phi_prop += epsilon * p_prop\n                _, grad_U_prop = get_log_posterior_and_grad(phi_prop, T_t, y_obs, sigma)\n                if j  L - 1:\n                    p_prop -= epsilon * grad_U_prop\n            p_prop -= 0.5 * epsilon * grad_U_prop\n            \n            p_prop = -p_prop # Negate momentum for symmetric proposal\n            \n            proposed_U, grad_U_prop = get_log_posterior_and_grad(phi_prop, T_t, y_obs, sigma)\n            proposed_K = 0.5 * np.sum(p_prop**2)\n            \n            # Acceptance probability\n            delta_H = proposed_U - current_U + proposed_K - current_K\n            if np.log(np.random.rand())  -delta_H:\n                phi = phi_prop\n                current_U = proposed_U\n                grad_U = grad_U_prop\n            \n            samples[i] = phi\n        \n        return samples\n\n    def rwm_sampler(initial_phi, T_t, y_obs, sigma):\n        \"\"\"Random-Walk Metropolis sampler.\"\"\"\n        n_samples = rwm_params['n_samples']\n        proposal_std = rwm_params['proposal_std']\n        \n        samples = np.zeros((n_samples, 2))\n        phi = np.copy(initial_phi)\n        \n        current_log_post, _ = get_log_posterior_and_grad(phi, T_t, y_obs, sigma)\n        \n        for i in range(n_samples):\n            proposal = phi + np.random.normal(loc=0, scale=proposal_std)\n            proposed_log_post, _ = get_log_posterior_and_grad(proposal, T_t, y_obs, sigma)\n            \n            log_alpha = current_log_post - proposed_log_post\n            if np.log(np.random.rand())  log_alpha:\n                phi = proposal\n                current_log_post = proposed_log_post\n            \n            samples[i] = phi\n        \n        return samples\n\n    # --- Main Loop for Test Cases ---\n    for case in test_cases:\n        T0, Tf, sigma = case['T0'], case['Tf'], case['sigma']\n        T_t = T0 + (Tf - T0) / t_f * t_grid\n        \n        # 1. Generate synthetic data\n        Y_true, _ = forward_model(phi_true, T_t)\n        Y_obs_true = Y_true[obs_indices]\n        noise = np.random.normal(0, sigma, size=len(obs_indices))\n        y_obs = Y_obs_true + noise\n\n        # 2. Run samplers\n        initial_phi = np.copy(phi_0)\n        hmc_samples_phi = hmc_sampler(initial_phi, T_t, y_obs, sigma)\n        rwm_samples_phi = rwm_sampler(initial_phi, T_t, y_obs, sigma)\n\n        # 3. Post-processing and analysis\n        hmc_burn = hmc_params['n_burn']\n        rwm_burn = rwm_params['n_burn']\n        \n        hmc_samples_phi_analysis = hmc_samples_phi[hmc_burn:]\n        rwm_samples_phi_analysis = rwm_samples_phi[rwm_burn:]\n\n        # Convert samples from phi=(lnA, lnE) to theta=(A,E)\n        hmc_samples_A = np.exp(hmc_samples_phi_analysis[:, 0])\n        hmc_samples_E = np.exp(hmc_samples_phi_analysis[:, 1])\n        rwm_samples_A = np.exp(rwm_samples_phi_analysis[:, 0])\n        rwm_samples_E = np.exp(rwm_samples_phi_analysis[:, 1])\n        \n        # Calculate sample variances\n        var_A_hmc = np.var(hmc_samples_A, ddof=1)\n        var_E_hmc = np.var(hmc_samples_E, ddof=1)\n        var_A_rwm = np.var(rwm_samples_A, ddof=1)\n        var_E_rwm = np.var(rwm_samples_E, ddof=1)\n        \n        # Calculate variance reduction ratios\n        r_A = 1.0 - var_A_hmc / var_A_rwm if var_A_rwm > 0 else 0\n        r_E = 1.0 - var_E_hmc / var_E_rwm if var_E_rwm > 0 else 0\n        \n        results.extend([r_A, r_E])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}