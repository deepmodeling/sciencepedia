{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in applying machine learning to physical sciences is ensuring that models abide by fundamental conservation laws. Standard neural networks, when trained on data alone, offer no guarantee of respecting physical principles. This exercise focuses on a powerful strategy to overcome this limitation: designing a model architecture that enforces mass conservation by construction. You will derive an output parameterization that ensures the sum of all species mass source terms, $\\sum_{i} \\dot{\\omega}_i$, is identically zero, transforming a physical constraint into a structural property of the network itself .",
            "id": "4037295",
            "problem": "Consider a homogeneous, constant-density reacting mixture used in computational combustion with $N_s$ chemical species. Let the mixture density be $\\rho$, the temperature be $T$, and the species mass fractions be $\\mathbf{Y} = (Y_1,\\dots,Y_{N_s})$. In a chemically balanced mechanism, the law of conservation of mass requires that the sum of the species mass source terms satisfies $\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0$, where $\\dot{\\omega}_i$ denotes the mass production rate of species $i$. You are tasked with constructing a machine-learning surrogate for chemistry tabulation and acceleration that maps the thermochemical state $(T, \\mathbf{Y})$ to species source terms. The surrogate predicts $N_s - 1$ raw outputs $\\{r_i\\}_{i=1}^{N_s-1}$, where each $r_i \\in \\mathbb{R}$ is unconstrained. To obtain physical source terms with correct scale, define the first $N_s - 1$ species source terms by $\\dot{\\omega}_i = \\alpha s_i r_i$ for $i = 1,\\dots,N_s-1$, where $\\alpha > 0$ is a shared scalar scaling that may depend on $(T, \\mathbf{Y})$ and each $s_i > 0$ is a fixed per-species scaling constant arising from dataset normalization.\n\nDerive a constrained output parameterization for the final species that enforces the conservation of total mass production rate exactly. Specifically, produce a closed-form analytic expression for $\\dot{\\omega}_{N_s}$ in terms of $\\alpha$, $\\{s_i\\}_{i=1}^{N_s-1}$, and $\\{r_i\\}_{i=1}^{N_s-1}$ such that $\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0$ holds for all surrogate outputs $\\{r_i\\}$. Your final answer must be a single analytic expression; do not include any units.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of chemical kinetics and computational combustion, is well-posed with sufficient information for a unique solution, and is stated objectively using formal scientific language.\n\nThe objective is to derive a closed-form expression for the mass production rate of the final species, $\\dot{\\omega}_{N_s}$, such that the law of conservation of mass is exactly satisfied. The governing physical principle is the conservation of total mass in a chemically reacting system, which dictates that the sum of the mass production rates (source terms) of all species must be zero. This is given by the equation:\n$$\n\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0\n$$\nwhere $N_s$ is the total number of species in the mixture.\n\nTo derive the expression for $\\dot{\\omega}_{N_s}$, we can separate this term from the sum. The summation can be expanded as:\n$$\n\\left( \\sum_{i=1}^{N_s-1} \\dot{\\omega}_i \\right) + \\dot{\\omega}_{N_s} = 0\n$$\nBy rearranging this equation, we can isolate $\\dot{\\omega}_{N_s}$ on one side:\n$$\n\\dot{\\omega}_{N_s} = - \\sum_{i=1}^{N_s-1} \\dot{\\omega}_i\n$$\nThis equation enforces the conservation law by defining the source term of the last species as the negative of the sum of the source terms of all other species.\n\nThe problem states that a machine-learning surrogate model is used to predict the source terms for the first $N_s-1$ species. The model's raw outputs, $\\{r_i\\}_{i=1}^{N_s-1}$, are transformed into physical source terms using the following parameterization for $i = 1, \\dots, N_s-1$:\n$$\n\\dot{\\omega}_i = \\alpha s_i r_i\n$$\nHere, $\\alpha$ is a shared positive scalar scaling factor, and each $s_i$ is a fixed positive per-species scaling constant.\n\nWe now substitute this definition for $\\dot{\\omega}_i$ into the expression for $\\dot{\\omega}_{N_s}$:\n$$\n\\dot{\\omega}_{N_s} = - \\sum_{i=1}^{N_s-1} (\\alpha s_i r_i)\n$$\nSince the scaling factor $\\alpha$ is a shared scalar, it is constant with respect to the summation index $i$. Therefore, it can be factored out of the summation:\n$$\n\\dot{\\omega}_{N_s} = - \\alpha \\sum_{i=1}^{N_s-1} s_i r_i\n$$\nThis is the final closed-form analytic expression for $\\dot{\\omega}_{N_s}$ in terms of the shared scaling factor $\\alpha$, the per-species scaling constants $\\{s_i\\}_{i=1}^{N_s-1}$, and the raw surrogate outputs $\\{r_i\\}_{i=1}^{N_s-1}$. This parameterization guarantees that $\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0$ holds for any set of unconstrained real-valued outputs $\\{r_i\\}$ from the surrogate model, thereby embedding the physical conservation law directly into the model's output layer. The variables for the thermochemical state, such as temperature $T$ and species mass fractions $\\mathbf{Y}$, are inputs to the machine learning model which produces the $\\{r_i\\}$ values and possibly determines $\\alpha$, but they do not appear in the final algebraic relationship that enforces the conservation constraint itself.",
            "answer": "$$\n\\boxed{- \\alpha \\sum_{i=1}^{N_s-1} s_i r_i}\n$$"
        },
        {
            "introduction": "Once a machine learning model is trained, it is often deployed within a larger simulation framework, for instance, through chemistry tabulation. This practice investigates a critical, and often overlooked, source of error that can arise during this deployment step: interpolation. Even if the ML model's predictions at discrete points in a table perfectly conserve mass, the process of linear interpolation between these points can introduce a numerical conservation error. This exercise will provide hands-on experience with barycentric interpolation inside a simplex and guide you to quantify the mass conservation residual, highlighting a key practical challenge in building robust tabulated chemistry models .",
            "id": "4037246",
            "problem": "In tabulated chemical kinetics used for Machine Learning (ML) acceleration of computational combustion, source terms are stored at vertices of simplices in a reduced-variable space and interpolated within each simplex to query states. Consider a triangular simplex in the $(Z,c)$-space, where $Z$ is the mixture fraction and $c$ is a normalized progress variable. The vertices are at $\\boldsymbol{v}_1 = (Z,c) = (0.1, 0.2)$, $\\boldsymbol{v}_2 = (0.4, 0.2)$, and $\\boldsymbol{v}_3 = (0.2, 0.6)$, and the query point is $\\boldsymbol{q} = (0.3, 0.4)$, which lies inside the convex hull of the vertices. At each vertex, a trained ML surrogate provides species mass source terms $\\dot{\\omega}_i$ in $\\mathrm{kg}\\,\\mathrm{m}^{-3}\\,\\mathrm{s}^{-1}$ for the four-species system $\\{\\mathrm{CH}_4, \\mathrm{O}_2, \\mathrm{CO}_2, \\mathrm{H}_2\\mathrm{O}\\}$:\n- At $\\boldsymbol{v}_1$: $\\dot{\\omega}_{\\mathrm{CH}_4}(\\boldsymbol{v}_1) = -0.0120$, $\\dot{\\omega}_{\\mathrm{O}_2}(\\boldsymbol{v}_1) = -0.0360$, $\\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{v}_1) = 0.0300$, $\\dot{\\omega}_{\\mathrm{H}_2\\mathrm{O}}(\\boldsymbol{v}_1) = 0.0178$.\n- At $\\boldsymbol{v}_2$: $\\dot{\\omega}_{\\mathrm{CH}_4}(\\boldsymbol{v}_2) = -0.0100$, $\\dot{\\omega}_{\\mathrm{O}_2}(\\boldsymbol{v}_2) = -0.0320$, $\\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{v}_2) = 0.0270$, $\\dot{\\omega}_{\\mathrm{H}_2\\mathrm{O}}(\\boldsymbol{v}_2) = 0.0152$.\n- At $\\boldsymbol{v}_3$: $\\dot{\\omega}_{\\mathrm{CH}_4}(\\boldsymbol{v}_3) = -0.0060$, $\\dot{\\omega}_{\\mathrm{O}_2}(\\boldsymbol{v}_3) = -0.0190$, $\\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{v}_3) = 0.0165$, $\\dot{\\omega}_{\\mathrm{H}_2\\mathrm{O}}(\\boldsymbol{v}_3) = 0.0082$.\n\nAssume linear interpolation in barycentric coordinates within the simplex. Starting from the geometric definition of barycentric coordinates and the fundamental mass conservation principle for chemically reacting flows, namely that the exact kinetics satisfy $\\sum_{i} \\dot{\\omega}_i = 0$, perform the following:\n1. Derive the barycentric weights $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ at $\\boldsymbol{q}$ from the vertex positions.\n2. Using these weights, compute the interpolated source term $\\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{q})$.\n3. Assess the mass conservation residual induced by interpolation, defined as $\\varepsilon_m = \\left|\\sum_{i \\in \\{\\mathrm{CH}_4,\\mathrm{O}_2,\\mathrm{CO}_2,\\mathrm{H}_2\\mathrm{O}\\}} \\dot{\\omega}_i(\\boldsymbol{q})\\right|$.\n\nReport only the final value of $\\varepsilon_m$ as your numeric answer. Express the final residual in $\\mathrm{kg}\\,\\mathrm{m}^{-3}\\,\\mathrm{s}^{-1}$ and round your answer to four significant figures.",
            "solution": "The problem requires interpolating source terms within a triangular simplex using barycentric coordinates and then evaluating the mass conservation residual at the query point. The foundational principles are:\n- Geometric barycentric coordinates in a triangle, defined by the area ratios, with weights $\\lambda_1$, $\\lambda_2$, $\\lambda_3$ satisfying $\\boldsymbol{q} = \\lambda_1 \\boldsymbol{v}_1 + \\lambda_2 \\boldsymbol{v}_2 + \\lambda_3 \\boldsymbol{v}_3$ and $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$, with all $\\lambda_j \\geq 0$ inside the triangle.\n- Mass conservation in chemically reacting flows, which for exact kinetics implies $\\sum_i \\dot{\\omega}_i = 0$; any deviation in the interpolated sum quantifies a conservation error.\n\nStep 1: Compute barycentric weights at $\\boldsymbol{q}$ by the area ratio definition. For points in $\\mathbb{R}^2$, the signed area of the triangle formed by points $\\boldsymbol{a} = (x_a, y_a)$ and $\\boldsymbol{b} = (x_b, y_b)$ and $\\boldsymbol{c} = (x_c, y_c)$ is given by\n$$\nA(\\boldsymbol{a}, \\boldsymbol{b}, \\boldsymbol{c}) = \\frac{1}{2} \\left|(x_b - x_a)(y_c - y_a) - (y_b - y_a)(x_c - x_a)\\right|.\n$$\nThe barycentric weights relative to $\\boldsymbol{v}_1$, $\\boldsymbol{v}_2$, and $\\boldsymbol{v}_3$ are\n$$\n\\lambda_1 = \\frac{A(\\boldsymbol{q}, \\boldsymbol{v}_2, \\boldsymbol{v}_3)}{A(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3)}, \\quad\n\\lambda_2 = \\frac{A(\\boldsymbol{q}, \\boldsymbol{v}_3, \\boldsymbol{v}_1)}{A(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3)}, \\quad\n\\lambda_3 = \\frac{A(\\boldsymbol{q}, \\boldsymbol{v}_1, \\boldsymbol{v}_2)}{A(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3)}.\n$$\n\nCompute $A(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3)$. With $\\boldsymbol{v}_1 = (0.1, 0.2)$, $\\boldsymbol{v}_2 = (0.4, 0.2)$, and $\\boldsymbol{v}_3 = (0.2, 0.6)$,\n$$\n\\boldsymbol{v}_2 - \\boldsymbol{v}_1 = (0.3, 0.0), \\quad \\boldsymbol{v}_3 - \\boldsymbol{v}_1 = (0.1, 0.4),\n$$\nso\n$$\nA(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3) = \\frac{1}{2} \\left| (0.3)(0.4) - (0.0)(0.1) \\right| = \\frac{1}{2} \\cdot 0.12 = 0.06.\n$$\n\nCompute $A(\\boldsymbol{q}, \\boldsymbol{v}_2, \\boldsymbol{v}_3)$ with $\\boldsymbol{q} = (0.3, 0.4)$:\n$$\n\\boldsymbol{v}_2 - \\boldsymbol{q} = (0.1, -0.2), \\quad \\boldsymbol{v}_3 - \\boldsymbol{q} = (-0.1, 0.2),\n$$\nso the determinant is $(0.1)(0.2) - (-0.2)(-0.1) = 0.02 - 0.02 = 0$, hence\n$$\nA(\\boldsymbol{q}, \\boldsymbol{v}_2, \\boldsymbol{v}_3) = 0,\n$$\nand therefore $\\lambda_1 = 0$.\n\nCompute $A(\\boldsymbol{q}, \\boldsymbol{v}_3, \\boldsymbol{v}_1)$:\n$$\n\\boldsymbol{v}_3 - \\boldsymbol{q} = (-0.1, 0.2), \\quad \\boldsymbol{v}_1 - \\boldsymbol{q} = (-0.2, -0.2),\n$$\ndeterminant $(-0.1)(-0.2) - (0.2)(-0.2) = 0.02 + 0.04 = 0.06$, thus\n$$\nA(\\boldsymbol{q}, \\boldsymbol{v}_3, \\boldsymbol{v}_1) = \\frac{1}{2} \\cdot 0.06 = 0.03,\n$$\nso\n$$\n\\lambda_2 = \\frac{0.03}{0.06} = 0.5.\n$$\n\nCompute $A(\\boldsymbol{q}, \\boldsymbol{v}_1, \\boldsymbol{v}_2)$:\n$$\n\\boldsymbol{v}_1 - \\boldsymbol{q} = (-0.2, -0.2), \\quad \\boldsymbol{v}_2 - \\boldsymbol{q} = (0.1, -0.2),\n$$\ndeterminant $(-0.2)(-0.2) - (-0.2)(0.1) = 0.04 + 0.02 = 0.06$, hence\n$$\nA(\\boldsymbol{q}, \\boldsymbol{v}_1, \\boldsymbol{v}_2) = \\frac{1}{2} \\cdot 0.06 = 0.03,\n$$\nso\n$$\n\\lambda_3 = \\frac{0.03}{0.06} = 0.5.\n$$\n\nThus, the barycentric weights are\n$$\n\\lambda_1 = 0, \\quad \\lambda_2 = 0.5, \\quad \\lambda_3 = 0.5,\n$$\nconsistent with $\\boldsymbol{q}$ lying on the line segment between $\\boldsymbol{v}_2$ and $\\boldsymbol{v}_3$.\n\nStep 2: Interpolate the $\\mathrm{CO}_2$ source term. Linear interpolation in barycentric coordinates yields\n$$\n\\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{q}) = \\lambda_1 \\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{v}_1) + \\lambda_2 \\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{v}_2) + \\lambda_3 \\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{v}_3).\n$$\nSubstitute the values:\n$$\n\\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{q}) = 0 \\cdot 0.0300 + 0.5 \\cdot 0.0270 + 0.5 \\cdot 0.0165 = 0.0135 + 0.00825 = 0.02175.\n$$\n\nStep 3: Assess the mass conservation residual $\\varepsilon_m$. Interpolate each species source term similarly:\n$$\n\\dot{\\omega}_{\\mathrm{CH}_4}(\\boldsymbol{q}) = 0 \\cdot (-0.0120) + 0.5 \\cdot (-0.0100) + 0.5 \\cdot (-0.0060) = -0.0080,\n$$\n$$\n\\dot{\\omega}_{\\mathrm{O}_2}(\\boldsymbol{q}) = 0 \\cdot (-0.0360) + 0.5 \\cdot (-0.0320) + 0.5 \\cdot (-0.0190) = -0.0255,\n$$\n$$\n\\dot{\\omega}_{\\mathrm{CO}_2}(\\boldsymbol{q}) = 0.02175 \\quad \\text{(computed above)},\n$$\n$$\n\\dot{\\omega}_{\\mathrm{H}_2\\mathrm{O}}(\\boldsymbol{q}) = 0 \\cdot 0.0178 + 0.5 \\cdot 0.0152 + 0.5 \\cdot 0.0082 = 0.0117.\n$$\nSum the interpolated source terms:\n$$\n\\sum_{i} \\dot{\\omega}_i(\\boldsymbol{q}) = (-0.0080) + (-0.0255) + 0.02175 + 0.0117 = -0.00005.\n$$\nThe residual magnitude is\n$$\n\\varepsilon_m = \\left| -0.00005 \\right| = 0.00005.\n$$\n\nFinally, round to four significant figures. The value $0.00005$ is $5.000 \\times 10^{-5}$ in scientific notation to four significant figures. The requested unit is $\\mathrm{kg}\\,\\mathrm{m}^{-3}\\,\\mathrm{s}^{-1}$.",
            "answer": "$$\\boxed{5.000 \\times 10^{-5}}$$"
        },
        {
            "introduction": "A powerful application of ML surrogates is the direct replacement of computationally expensive chemistry calculations within an Ordinary Differential Equation (ODE) solver, promising significant acceleration. However, the success of this approach hinges on numerical stability, especially for the stiff systems characteristic of combustion. The stability of an explicit time-integration scheme is governed by the eigenvalues of the system's Jacobian matrix. This advanced practice explores how to analyze the stability of an ML-surrogated ODE system by computing the surrogate's Jacobian via automatic differentiation and using its spectral properties to determine the maximum stable time step, $h_{\\max}$ .",
            "id": "4037322",
            "problem": "Consider a reduced stiff chemical system where the species mass fraction vector $\\mathbf{Y} \\in \\mathbb{R}^{n_s}$ evolves according to an ordinary differential equation (ODE) of the form $\\dot{\\mathbf{Y}} = \\boldsymbol{\\omega}(\\mathbf{Y})$, where $\\boldsymbol{\\omega}(\\mathbf{Y})$ represents the chemical source terms. To accelerate chemistry and enable tabulation, one can replace the exact source terms with an Artificial Neural Network (ANN) surrogate trained to approximate $\\boldsymbol{\\omega}(\\mathbf{Y})$. Let the surrogate be a two-layer feedforward ANN with hyperbolic tangent activation, defined for input $\\mathbf{Y} \\in \\mathbb{R}^{3}$ as\n$$\n\\boldsymbol{f}(\\mathbf{Y}) = \\mathbf{W}_2 \\, \\tanh(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1) + \\mathbf{b}_2,\n$$\nwhere $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times 3}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{3 \\times m}$, and $\\mathbf{b}_2 \\in \\mathbb{R}^{3}$, with $m=3$. The surrogate output approximates $\\dot{\\mathbf{Y}}$ and has units of $\\mathrm{s}^{-1}$ when appropriately scaled. The Jacobian matrix $\\mathbf{J}(\\mathbf{Y})$ of the surrogate is defined as\n$$\n\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}(\\mathbf{Y})}{\\partial \\mathbf{Y}},\n$$\nwith entries having units of $\\mathrm{s}^{-1}$. Automatic Differentiation (AD) should be used to compute $\\mathbf{J}(\\mathbf{Y})$ based on the chain rule applied to the specified ANN structure.\n\nStarting from the linearization principle for stiff ODEs and the explicit Euler method, recall the fundamental stability consideration for the scalar linear test equation $\\dot{y} = \\lambda y$ that the explicit Euler step $y_{n+1} = y_n + h \\lambda y_n$ is linearly stable when the amplification factor satisfies $|1 + h \\lambda| \\leq 1$. Generalize this consideration to the matrix case by analyzing the eigenvalues of the Jacobian $\\mathbf{J}(\\mathbf{Y})$, and determine a conservative bound on the stable explicit Euler time step $h$ when eigenvalues have large negative real parts. You must express the stable step bound $h_{\\max}$ in seconds.\n\nImplement a complete program that:\n- Constructs the ANN surrogate $\\boldsymbol{f}(\\mathbf{Y})$ as specified.\n- Uses Automatic Differentiation, applied to the ANN layers and activation function, to compute the Jacobian $\\mathbf{J}(\\mathbf{Y})$ at specified $\\mathbf{Y}$.\n- Computes the eigenvalues of $\\mathbf{J}(\\mathbf{Y})$.\n- Determines a conservative maximum stable explicit Euler time step $h_{\\max}$ (in seconds) based on the eigenvalues, using the scalar stability condition generalized to each eigenvalue with possibly nonzero imaginary part.\n- Evaluates linear stability for a set of specified explicit Euler time steps $h$ by checking the spectral radius of the one-step amplification matrix $\\mathbf{A}(h) = \\mathbf{I} + h \\mathbf{J}(\\mathbf{Y})$ and testing whether it is less than or equal to $1$.\n\nYou must consider three test cases. For each case, you are given the network parameters and the evaluation point $\\mathbf{Y}$ along with three explicit Euler step sizes $h$ in seconds. Use the same ANN architecture described above with $m=3$. The parameters are:\n\nCase 1 (stiff with off-diagonal coupling):\n- $\\mathbf{W}_1 = \\begin{bmatrix} 3.0 & -1.0 & 0.5 \\\\ -2.0 & 4.0 & -0.5 \\\\ 1.0 & 1.0 & -3.0 \\end{bmatrix}$,\n- $\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$,\n- $\\mathbf{W}_2 = \\begin{bmatrix} -1000.0 & 50.0 & 0.0 \\\\ 0.0 & -50.0 & 20.0 \\\\ 10.0 & -5.0 & -1.0 \\end{bmatrix}$,\n- $\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $\\mathbf{Y} = \\begin{bmatrix} 0.1 \\\\ 0.3 \\\\ 0.6 \\end{bmatrix}$,\n- Step sizes $h \\in \\{10^{-5} \\text{ s}, 10^{-3} \\text{ s}, 10^{-2} \\text{ s}\\}$.\n\nCase 2 (moderately stiff):\n- $\\mathbf{W}_1 = \\begin{bmatrix} 1.0 & 0.2 & -0.3 \\\\ 0.5 & -0.1 & 0.4 \\\\ -0.7 & 0.8 & -0.2 \\end{bmatrix}$,\n- $\\mathbf{b}_1 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\\\ 0.02 \\end{bmatrix}$,\n- $\\mathbf{W}_2 = \\begin{bmatrix} -10.0 & 1.0 & 0.0 \\\\ 0.5 & -5.0 & 0.5 \\\\ 0.0 & 0.2 & -1.0 \\end{bmatrix}$,\n- $\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $\\mathbf{Y} = \\begin{bmatrix} 0.9 \\\\ 0.05 \\\\ 0.05 \\end{bmatrix}$,\n- Step sizes $h \\in \\{10^{-3} \\text{ s}, 5 \\cdot 10^{-2} \\text{ s}, 10^{-1} \\text{ s}\\}$.\n\nCase 3 (extremely stiff with activation saturation effects):\n- $\\mathbf{W}_1 = \\begin{bmatrix} 8.0 & -6.0 & 4.0 \\\\ -9.0 & 5.0 & -7.0 \\\\ 6.5 & -8.0 & 10.0 \\end{bmatrix}$,\n- $\\mathbf{b}_1 = \\begin{bmatrix} 0.5 \\\\ -0.3 \\\\ 0.1 \\end{bmatrix}$,\n- $\\mathbf{W}_2 = \\begin{bmatrix} -5000.0 & 200.0 & 0.0 \\\\ 0.0 & -100.0 & 40.0 \\\\ 30.0 & -10.0 & -20.0 \\end{bmatrix}$,\n- $\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $\\mathbf{Y} = \\begin{bmatrix} 0.33 \\\\ 0.33 \\\\ 0.34 \\end{bmatrix}$,\n- Step sizes $h \\in \\{10^{-5} \\text{ s}, 10^{-4} \\text{ s}, 10^{-3} \\text{ s}\\}$.\n\nFor each case:\n1. Use AD for the ANN to compute $\\mathbf{J}(\\mathbf{Y})$.\n2. Compute all eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{J}(\\mathbf{Y})$.\n3. From the scalar stability condition applied to each $\\lambda_i$, determine a conservative bound $h_{\\max}$ in seconds that ensures $|1 + h \\lambda_i| \\leq 1$ for all eigenvalues simultaneously.\n4. For each specified $h$ in the case, compute the spectral radius $\\rho(\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y}))$ and report whether the step is linearly stable, i.e., whether $\\rho(\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y})) \\leq 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one case and should be a list of the form $[h_{\\max}, s_1, s_2, s_3]$ where $h_{\\max}$ is a float in seconds and $s_k$ are booleans indicating stability for the $k$-th step size of that case. For example, the final output must look like\n$$\n[\\,[h_{\\max}^{(1)}, s_1^{(1)}, s_2^{(1)}, s_3^{(1)}],[h_{\\max}^{(2)}, s_1^{(2)}, s_2^{(2)}, s_3^{(2)}],[h_{\\max}^{(3)}, s_1^{(3)}, s_2^{(3)}, s_3^{(3)}]\\,].\n$$\nAll $h_{\\max}$ values must be expressed in seconds, and no other text should be printed.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n- **Governing Equation:** The species mass fraction vector $\\mathbf{Y} \\in \\mathbb{R}^{n_s}$ evolves according to the Ordinary Differential Equation (ODE) $\\dot{\\mathbf{Y}} = \\boldsymbol{\\omega}(\\mathbf{Y})$.\n- **ANN Surrogate:** The source term $\\boldsymbol{\\omega}(\\mathbf{Y})$ is approximated by a two-layer feedforward Artificial Neural Network (ANN) $\\boldsymbol{f}(\\mathbf{Y})$ for $\\mathbf{Y} \\in \\mathbb{R}^{3}$.\n- **ANN Architecture:** $\\boldsymbol{f}(\\mathbf{Y}) = \\mathbf{W}_2 \\, \\tanh(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1) + \\mathbf{b}_2$.\n- **ANN Parameters:** $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times 3}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{3 \\times m}$, $\\mathbf{b}_2 \\in \\mathbb{R}^{3}$, with hidden layer size $m=3$.\n- **Jacobian Matrix:** $\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}(\\mathbf{Y})}{\\partial \\mathbf{Y}}$.\n- **Stability Condition (Scalar):** For $\\dot{y} = \\lambda y$, the explicit Euler step is stable if $|1 + h \\lambda| \\leq 1$.\n- **Stability Condition (System):** For a given step size $h$, stability is assessed by checking if the spectral radius of the amplification matrix, $\\rho(\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y}))$, is less than or equal to $1$.\n- **Objective:** For three test cases, compute:\n    1. The Jacobian $\\mathbf{J}(\\mathbf{Y})$ using Automatic Differentiation (AD).\n    2. The eigenvalues of $\\mathbf{J}(\\mathbf{Y})$.\n    3. A conservative maximum stable explicit Euler time step $h_{\\max}$.\n    4. The stability for a given set of time steps $h$.\n- **Test Cases:** Three cases are provided, each with specific matrices $\\mathbf{W}_1, \\mathbf{W}_2$, vectors $\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{Y}$, and a set of step sizes $h$ to test.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on fundamental principles of numerical analysis (stability of ODE solvers), linear algebra (eigenvalue analysis), and standard machine learning models (feedforward neural networks). The application context, accelerating stiff chemical kinetics in computational combustion, is a well-established and active area of scientific research.\n- **Well-Posed:** The problem is clearly defined with a unique, computable solution. The ANN architecture, parameters, and the point of evaluation are all specified, leading to a deterministic Jacobian matrix and subsequent stability analysis.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased mathematical language.\n- **Completeness and Consistency:** All necessary information (network parameters, evaluation points, stability criteria) is provided. The dimensions of all matrices and vectors are consistent for the prescribed operations.\n- **Realism:** The setup is realistic. Stiff chemical systems are common, and the use of ANN surrogates is a modern technique. The large negative magnitudes in the weight matrices (e.g., $-1000.0, -5000.0$) are characteristic of models for stiff phenomena, which correctly imply a need for very small explicit time steps.\n- **Non-triviality:** The problem is substantive, requiring the application of matrix calculus (chain rule for the Jacobian), numerical linear algebra (eigenvalue computation), and the theory of numerical stability for systems of ODEs.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a well-defined, scientifically-grounded problem in applied mathematics and computational science. The solution process will now proceed.\n\n---\n\n### Principle-Based Solution\n\nThe core of the problem is to assess the stability of the explicit Euler numerical integration scheme when applied to a system of Ordinary Differential Equations (ODEs) whose right-hand side is approximated by an Artificial Neural Network (ANN).\n\n**1. Jacobian of the ANN via Automatic Differentiation (AD)**\n\nThe ANN acts as a surrogate for the chemical source terms, $\\dot{\\mathbf{Y}} \\approx \\boldsymbol{f}(\\mathbf{Y})$. The local behavior of this system is governed by its linearization, which requires the Jacobian matrix $\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{Y}}$. We compute this using the chain rule, which is the cornerstone of reverse-mode automatic differentiation.\n\nThe network is defined as $\\boldsymbol{f}(\\mathbf{Y}) = \\mathbf{W}_2 \\, \\tanh(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1) + \\mathbf{b}_2$. Let's decompose this into intermediate steps:\n1.  The pre-activation of the hidden layer: $\\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1$.\n2.  The activation of the hidden layer: $\\mathbf{a}_1 = \\tanh(\\mathbf{z}_1)$.\n3.  The output of the network: $\\boldsymbol{f} = \\mathbf{W}_2 \\mathbf{a}_1 + \\mathbf{b}_2$.\n\nApplying the chain rule for differentiation:\n$$\n\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{Y}} = \\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{a}_1} \\frac{\\partial \\mathbf{a}_1}{\\partial \\mathbf{z}_1} \\frac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{Y}}\n$$\nLet's evaluate each partial derivative:\n-   $\\frac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{Y}}$: From the linear transformation $\\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1$, the derivative with respect to $\\mathbf{Y}$ is simply the weight matrix $\\mathbf{W}_1$.\n-   $\\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{a}_1}$: From the output transformation $\\boldsymbol{f} = \\mathbf{W}_2 \\mathbf{a}_1 + \\mathbf{b}_2$, the derivative with respect to $\\mathbf{a}_1$ is the weight matrix $\\mathbf{W}_2$.\n-   $\\frac{\\partial \\mathbf{a}_1}{\\partial \\mathbf{z}_1}$: The activation function $\\tanh$ is applied element-wise. Therefore, its derivative is a diagonal matrix. The derivative of $\\tanh(x)$ is $1 - \\tanh^2(x)$. The resulting Jacobian of this layer is $\\mathbf{D} = \\text{diag}(1 - \\tanh^2(z_{1,i}))$, where $z_{1,i}$ are the elements of the vector $\\mathbf{z}_1$.\n\nCombining these components yields the analytical expression for the Jacobian of the ANN:\n$$\n\\mathbf{J}(\\mathbf{Y}) = \\mathbf{W}_2 \\cdot \\text{diag}(1 - \\tanh^2(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1)) \\cdot \\mathbf{W}_1\n$$\nThis formula is implemented to compute the Jacobian at a given state $\\mathbf{Y}$.\n\n**2. Explicit Euler Stability Analysis**\n\nThe explicit Euler method approximates the solution to $\\dot{\\mathbf{Y}} = \\boldsymbol{f}(\\mathbf{Y})$ by linearizing the system locally as $\\dot{\\boldsymbol{\\eta}} = \\mathbf{J}(\\mathbf{Y}_n) \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta}$ is a perturbation from $\\mathbf{Y}_n$. The update step is:\n$$\n\\mathbf{Y}_{n+1} = \\mathbf{Y}_n + h \\boldsymbol{f}(\\mathbf{Y}_n)\n$$\nFor stability analysis, we consider how perturbations evolve. A perturbation $\\boldsymbol{\\epsilon}_n$ at step $n$ evolves to $\\boldsymbol{\\epsilon}_{n+1}$ at step $n+1$ according to:\n$$\n\\mathbf{Y}_{n+1} + \\boldsymbol{\\epsilon}_{n+1} = (\\mathbf{Y}_n + \\boldsymbol{\\epsilon}_n) + h \\boldsymbol{f}(\\mathbf{Y}_n + \\boldsymbol{\\epsilon}_n) \\approx \\mathbf{Y}_n + \\boldsymbol{\\epsilon}_n + h (\\boldsymbol{f}(\\mathbf{Y}_n) + \\mathbf{J}(\\mathbf{Y}_n) \\boldsymbol{\\epsilon}_n)\n$$\nSubtracting the unperturbed equation, we get the evolution of the perturbation:\n$$\n\\boldsymbol{\\epsilon}_{n+1} = (\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y}_n)) \\boldsymbol{\\epsilon}_n\n$$\nThe matrix $\\mathbf{A}(h) = \\mathbf{I} + h \\mathbf{J}$ is the amplification matrix. For the method to be stable, perturbations must not grow, which requires the spectral radius of $\\mathbf{A}(h)$ to be no greater than $1$:\n$$\n\\rho(\\mathbf{A}(h)) \\le 1\n$$\nThe eigenvalues of $\\mathbf{A}(h)$, denoted $\\mu_i$, are related to the eigenvalues of $\\mathbf{J}$, denoted $\\lambda_i$, by $\\mu_i = 1 + h \\lambda_i$. Thus, the stability condition becomes $|1 + h \\lambda_i| \\leq 1$ for all eigenvalues $\\lambda_i$ of the Jacobian $\\mathbf{J}$.\n\n**3. Maximum Stable Time Step $h_{\\max}$**\n\nLet an eigenvalue of $\\mathbf{J}$ be a complex number $\\lambda = x + iy$. The stability condition is $|1 + h(x+iy)|^2 \\leq 1$, which expands to:\n$$\n(1 + hx)^2 + (hy)^2 \\leq 1 \\implies 1 + 2hx + h^2x^2 + h^2y^2 \\leq 1 \\implies h(2x + h(x^2+y^2)) \\leq 0\n$$\nSince the time step $h$ must be positive, we require $2x + h(x^2+y^2) \\leq 0$. For a stiff system, we expect the real parts of the eigenvalues to be negative, i.e., $x = \\text{Re}(\\lambda) < 0$. If any $\\text{Re}(\\lambda) > 0$, the system is locally unstable, and no positive explicit time step can be stable, so $h_{\\max} = 0$. For $\\text{Re}(\\lambda) < 0$, the condition on $h$ is:\n$$\nh \\leq \\frac{-2x}{x^2+y^2} = \\frac{-2 \\text{Re}(\\lambda)}{|\\lambda|^2}\n$$\nTo ensure stability for all modes of the system, $h$ must satisfy this condition for every eigenvalue. Therefore, the maximum stable time step is the most restrictive of these bounds:\n$$\nh_{\\max} = \\min_{\\lambda_i : \\text{Re}(\\lambda_i) < 0} \\left( \\frac{-2 \\text{Re}(\\lambda_i)}{|\\lambda_i|^2} \\right)\n$$\n\n**4. Algorithmic Procedure**\n\nFor each test case, the following steps are performed:\n1.  Construct the NumPy-based representations of the network parameters $\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2$ and the state vector $\\mathbf{Y}$. Vectors are shaped as column vectors.\n2.  Calculate the Jacobian $\\mathbf{J}$ at $\\mathbf{Y}$ using the derived analytical formula.\n3.  Compute the eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{J}$ using `numpy.linalg.eigvals`.\n4.  Calculate $h_{\\max}$ by iterating through the eigenvalues and applying the formula derived above. An initial value of infinity is used for the minimum search. If any eigenvalue has a positive real part, $h_{\\max}$ is set to $0$.\n5.  For each specified step size $h$ in the test case:\n    a. Form the amplification matrix $\\mathbf{A}(h) = \\mathbf{I} + h \\mathbf{J}$.\n    b. Compute the eigenvalues of $\\mathbf{A}(h)$.\n    c. Calculate the spectral radius $\\rho(\\mathbf{A}(h))$ by finding the maximum absolute value among these eigenvalues.\n    d. The step $h$ is deemed stable if $\\rho(\\mathbf{A}(h)) \\leq 1$, and unstable otherwise.\n6.  The results ($h_{\\max}$ and the three stability booleans) for each case are collected and formatted into the required final output string.",
            "answer": "[[0.002047463777592474,True,False,False],[0.19191973618301548,True,True,False],[0.0003444976721532822,True,False,False]]"
        }
    ]
}