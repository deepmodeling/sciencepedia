{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in developing machine learning surrogates for physical systems is ensuring they respect fundamental conservation laws. This practice introduces an elegant architectural solution to enforce mass conservation in a chemical kinetics model . You will derive an output parameterization that guarantees the sum of species mass production rates, $\\sum_i \\dot{\\omega}_i$, is exactly zero, moving beyond simple post-processing corrections to build physical consistency directly into the model's structure.",
            "id": "4037295",
            "problem": "Consider a homogeneous, constant-density reacting mixture used in computational combustion with $N_s$ chemical species. Let the mixture density be $\\rho$, the temperature be $T$, and the species mass fractions be $\\mathbf{Y} = (Y_1,\\dots,Y_{N_s})$. In a chemically balanced mechanism, the law of conservation of mass requires that the sum of the species mass source terms satisfies $\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0$, where $\\dot{\\omega}_i$ denotes the mass production rate of species $i$. You are tasked with constructing a machine-learning surrogate for chemistry tabulation and acceleration that maps the thermochemical state $(T, \\mathbf{Y})$ to species source terms. The surrogate predicts $N_s - 1$ raw outputs $\\{r_i\\}_{i=1}^{N_s-1}$, where each $r_i \\in \\mathbb{R}$ is unconstrained. To obtain physical source terms with correct scale, define the first $N_s - 1$ species source terms by $\\dot{\\omega}_i = \\alpha s_i r_i$ for $i = 1,\\dots,N_s-1$, where $\\alpha > 0$ is a shared scalar scaling that may depend on $(T, \\mathbf{Y})$ and each $s_i > 0$ is a fixed per-species scaling constant arising from dataset normalization.\n\nDerive a constrained output parameterization for the final species that enforces the conservation of total mass production rate exactly. Specifically, produce a closed-form analytic expression for $\\dot{\\omega}_{N_s}$ in terms of $\\alpha$, $\\{s_i\\}_{i=1}^{N_s-1}$, and $\\{r_i\\}_{i=1}^{N_s-1}$ such that $\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0$ holds for all surrogate outputs $\\{r_i\\}$. Your final answer must be a single analytic expression; do not include any units.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of chemical kinetics and computational combustion, is well-posed with sufficient information for a unique solution, and is stated objectively using formal scientific language.\n\nThe objective is to derive a closed-form expression for the mass production rate of the final species, $\\dot{\\omega}_{N_s}$, such that the law of conservation of mass is exactly satisfied. The governing physical principle is the conservation of total mass in a chemically reacting system, which dictates that the sum of the mass production rates (source terms) of all species must be zero. This is given by the equation:\n$$\n\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0\n$$\nwhere $N_s$ is the total number of species in the mixture.\n\nTo derive the expression for $\\dot{\\omega}_{N_s}$, we can separate this term from the sum. The summation can be expanded as:\n$$\n\\left( \\sum_{i=1}^{N_s-1} \\dot{\\omega}_i \\right) + \\dot{\\omega}_{N_s} = 0\n$$\nBy rearranging this equation, we can isolate $\\dot{\\omega}_{N_s}$ on one side:\n$$\n\\dot{\\omega}_{N_s} = - \\sum_{i=1}^{N_s-1} \\dot{\\omega}_i\n$$\nThis equation enforces the conservation law by defining the source term of the last species as the negative of the sum of the source terms of all other species.\n\nThe problem states that a machine-learning surrogate model is used to predict the source terms for the first $N_s-1$ species. The model's raw outputs, $\\{r_i\\}_{i=1}^{N_s-1}$, are transformed into physical source terms using the following parameterization for $i = 1, \\dots, N_s-1$:\n$$\n\\dot{\\omega}_i = \\alpha s_i r_i\n$$\nHere, $\\alpha$ is a shared positive scalar scaling factor, and each $s_i$ is a fixed positive per-species scaling constant.\n\nWe now substitute this definition for $\\dot{\\omega}_i$ into the expression for $\\dot{\\omega}_{N_s}$:\n$$\n\\dot{\\omega}_{N_s} = - \\sum_{i=1}^{N_s-1} (\\alpha s_i r_i)\n$$\nSince the scaling factor $\\alpha$ is a shared scalar, it is constant with respect to the summation index $i$. Therefore, it can be factored out of the summation:\n$$\n\\dot{\\omega}_{N_s} = - \\alpha \\sum_{i=1}^{N_s-1} s_i r_i\n$$\nThis is the final closed-form analytic expression for $\\dot{\\omega}_{N_s}$ in terms of the shared scaling factor $\\alpha$, the per-species scaling constants $\\{s_i\\}_{i=1}^{N_s-1}$, and the raw surrogate outputs $\\{r_i\\}_{i=1}^{N_s-1}$. This parameterization guarantees that $\\sum_{i=1}^{N_s} \\dot{\\omega}_i = 0$ holds for any set of unconstrained real-valued outputs $\\{r_i\\}$ from the surrogate model, thereby embedding the physical conservation law directly into the model's output layer. The variables for the thermochemical state, such as temperature $T$ and species mass fractions $\\mathbf{Y}$, are inputs to the machine learning model which produces the $\\{r_i\\}$ values and possibly determines $\\alpha$, but they do not appear in the final algebraic relationship that enforces the conservation constraint itself.",
            "answer": "$$\n\\boxed{- \\alpha \\sum_{i=1}^{N_s-1} s_i r_i}\n$$"
        },
        {
            "introduction": "Even a well-designed model can produce unphysical predictions, such as negative concentrations, which must be corrected. This exercise  explores the critical interplay between different physical constraints: positivity and elemental conservation. You will implement a common procedure where a prediction is first 'clipped' to enforce positivity, and then a correction is calculated to restore elemental conservation, which was broken by the initial clipping. This highlights the practical, multi-step process often required to ensure physical realism in simulation.",
            "id": "4037315",
            "problem": "Consider a constant-pressure, isothermal, spatially homogeneous reactor step in which a Machine Learning (ML) surrogate is used to accelerate chemistry tabulation by predicting concentration increments for a subset of species. The species set is $\\{\\mathrm{H_2}, \\mathrm{O_2}, \\mathrm{H_2O}\\}$. Let the molar concentrations at the beginning of the step be $\\mathbf{c} = [c_{\\mathrm{H_2}}, c_{\\mathrm{O_2}}, c_{\\mathrm{H_2O}}]^{\\top} = [\\,0.03,\\, 0.20,\\, 0.01\\,]^{\\top}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$. The ML surrogate predicts raw concentration updates $\\Delta \\hat{\\mathbf{c}} = [\\,\\Delta \\hat{c}_{\\mathrm{H_2}},\\, \\Delta \\hat{c}_{\\mathrm{O_2}},\\, \\Delta \\hat{c}_{\\mathrm{H_2O}}\\,]^{\\top} = [\\,-0.08,\\,-0.04,\\,+0.08\\,]^{\\top}$ in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$ for this time step.\n\nAssume the surrogate respects elemental conservation in its raw output, so that the total hydrogen and oxygen atoms are conserved to machine precision. The elemental incidence matrix mapping species to atom counts is\n$$\nA \\;=\\; \\begin{bmatrix}\n2 & 0 & 2 \\\\\n0 & 2 & 1\n\\end{bmatrix},\n$$\nwhere row $1$ is hydrogen atoms and row $2$ is oxygen atoms.\n\nBecause the predicted update would drive $\\mathrm{H_2}$ negative, positivity is enforced by clipping only the $\\mathrm{H_2}$ update so that the post-update $\\mathrm{H_2}$ concentration is exactly zero, i.e., set $\\Delta c_{\\mathrm{H_2}} = -c_{\\mathrm{H_2}}$ while leaving $\\Delta c_{\\mathrm{O_2}}$ and $\\Delta c_{\\mathrm{H_2O}}$ to be corrected. To restore elemental conservation exactly after this clipping, compute a correction $\\mathbf{x} = [\\,x_{\\mathrm{O_2}},\\, x_{\\mathrm{H_2O}}\\,]^{\\top}$ applied to the remaining species updates such that the final correction satisfies $A \\Delta \\mathbf{c} = \\mathbf{0}$, where $\\Delta \\mathbf{c} = [\\,\\Delta c_{\\mathrm{H_2}},\\, \\Delta \\hat{c}_{\\mathrm{O_2}} + x_{\\mathrm{O_2}},\\, \\Delta \\hat{c}_{\\mathrm{H_2O}} + x_{\\mathrm{H_2O}}\\,]^{\\top}$.\n\nUsing only the stated physical laws and definitions (nonnegativity of concentrations and elemental conservation), determine the corrected oxygen concentration $c_{\\mathrm{O_2}}^{\\mathrm{new}} = c_{\\mathrm{O_2}} + \\Delta \\hat{c}_{\\mathrm{O_2}} + x_{\\mathrm{O_2}}$ after applying positivity clipping and conservation correction. Express your final answer in $\\mathrm{mol}\\,\\mathrm{m}^{-3}$ and round to four significant figures.",
            "solution": "The problem requires us to determine the final concentration of oxygen, $c_{\\mathrm{O_2}}^{\\mathrm{new}}$, after a two-step correction process applied to a raw concentration update predicted by a machine learning model. The two steps are: first, enforcing non-negativity of concentrations by clipping, and second, restoring elemental conservation by applying a correction to the remaining species.\n\nFirst, we restate the givens.\nThe species are $\\mathrm{H_2}$, $\\mathrm{O_2}$, and $\\mathrm{H_2O}$.\nThe initial molar concentrations are given by the vector $\\mathbf{c}$:\n$$\n\\mathbf{c} = \\begin{bmatrix} c_{\\mathrm{H_2}} \\\\ c_{\\mathrm{O_2}} \\\\ c_{\\mathrm{H_2O}} \\end{bmatrix} = \\begin{bmatrix} 0.03 \\\\ 0.20 \\\\ 0.01 \\end{bmatrix} \\, \\mathrm{mol}\\,\\mathrm{m}^{-3}\n$$\nThe raw concentration updates predicted by the ML surrogate are given by the vector $\\Delta \\hat{\\mathbf{c}}$:\n$$\n\\Delta \\hat{\\mathbf{c}} = \\begin{bmatrix} \\Delta \\hat{c}_{\\mathrm{H_2}} \\\\ \\Delta \\hat{c}_{\\mathrm{O_2}} \\\\ \\Delta \\hat{c}_{\\mathrm{H_2O}} \\end{bmatrix} = \\begin{bmatrix} -0.08 \\\\ -0.04 \\\\ 0.08 \\end{bmatrix} \\, \\mathrm{mol}\\,\\mathrm{m}^{-3}\n$$\nThe elemental incidence matrix $A$ maps species concentrations to atom concentrations (row $1$: H, row $2$: O):\n$$\nA = \\begin{bmatrix} 2 & 0 & 2 \\\\ 0 & 2 & 1 \\end{bmatrix}\n$$\nThe problem states that the raw update $\\Delta \\hat{\\mathbf{c}}$ conserves elements, meaning $A \\Delta \\hat{\\mathbf{c}} = \\mathbf{0}$.\n\nThe first step is to enforce positivity. The predicted concentration of $\\mathrm{H_2}$ would be $c_{\\mathrm{H_2}} + \\Delta \\hat{c}_{\\mathrm{H_2}} = 0.03 + (-0.08) = -0.05 \\, \\mathrm{mol}\\,\\mathrm{m}^{-3}$. Since this is unphysical, the update for $\\mathrm{H_2}$ is clipped such that the final concentration of $\\mathrm{H_2}$ is exactly zero. The clipped update, $\\Delta c_{\\mathrm{H_2}}$, is therefore:\n$$\n\\Delta c_{\\mathrm{H_2}} = -c_{\\mathrm{H_2}} = -0.03 \\, \\mathrm{mol}\\,\\mathrm{m}^{-3}\n$$\nThis clipping action violates elemental conservation. To restore it, we introduce a correction vector $\\mathbf{x} = [\\,x_{\\mathrm{O_2}},\\, x_{\\mathrm{H_2O}}\\,]^{\\top}$ that is applied to the updates of the other species. The final, corrected concentration update vector, $\\Delta \\mathbf{c}$, is:\n$$\n\\Delta \\mathbf{c} = \\begin{bmatrix} \\Delta c_{\\mathrm{H_2}} \\\\ \\Delta \\hat{c}_{\\mathrm{O_2}} + x_{\\mathrm{O_2}} \\\\ \\Delta \\hat{c}_{\\mathrm{H_2O}} + x_{\\mathrm{H_2O}} \\end{bmatrix} = \\begin{bmatrix} -0.03 \\\\ -0.04 + x_{\\mathrm{O_2}} \\\\ 0.08 + x_{\\mathrm{H_2O}} \\end{bmatrix}\n$$\nThis final update vector must satisfy the elemental conservation law, $A \\Delta \\mathbf{c} = \\mathbf{0}$. We can write this as a system of linear equations:\n$$\n\\begin{bmatrix} 2 & 0 & 2 \\\\ 0 & 2 & 1 \\end{bmatrix}\n\\begin{bmatrix} -0.03 \\\\ -0.04 + x_{\\mathrm{O_2}} \\\\ 0.08 + x_{\\mathrm{H_2O}} \\end{bmatrix}\n= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\nThis gives us two equations for our two unknowns, $x_{\\mathrm{O_2}}$ and $x_{\\mathrm{H_2O}}$.\n\nThe first row corresponds to the conservation of hydrogen atoms:\n$$\n2(-0.03) + 0(-0.04 + x_{\\mathrm{O_2}}) + 2(0.08 + x_{\\mathrm{H_2O}}) = 0\n$$\n$$\n-0.06 + 0.16 + 2x_{\\mathrm{H_2O}} = 0\n$$\n$$\n0.10 + 2x_{\\mathrm{H_2O}} = 0\n$$\n$$\n2x_{\\mathrm{H_2O}} = -0.10 \\implies x_{\\mathrm{H_2O}} = -0.05\n$$\nThe second row corresponds to the conservation of oxygen atoms:\n$$\n0(-0.03) + 2(-0.04 + x_{\\mathrm{O_2}}) + 1(0.08 + x_{\\mathrm{H_2O}}) = 0\n$$\n$$\n-0.08 + 2x_{\\mathrm{O_2}} + 0.08 + x_{\\mathrm{H_2O}} = 0\n$$\n$$\n2x_{\\mathrm{O_2}} + x_{\\mathrm{H_2O}} = 0\n$$\nSubstituting the value we found for $x_{\\mathrm{H_2O}}$:\n$$\n2x_{\\mathrm{O_2}} + (-0.05) = 0\n$$\n$$\n2x_{\\mathrm{O_2}} = 0.05 \\implies x_{\\mathrm{O_2}} = 0.025\n$$\nSo, the correction for the oxygen update is $x_{\\mathrm{O_2}} = 0.025 \\, \\mathrm{mol}\\,\\mathrm{m}^{-3}$.\n\nThe problem asks for the final corrected oxygen concentration, $c_{\\mathrm{O_2}}^{\\mathrm{new}}$. This is calculated by adding the corrected update to the initial concentration:\n$$\nc_{\\mathrm{O_2}}^{\\mathrm{new}} = c_{\\mathrm{O_2}} + \\Delta c_{\\mathrm{O_2}} = c_{\\mathrm{O_2}} + (\\Delta \\hat{c}_{\\mathrm{O_2}} + x_{\\mathrm{O_2}})\n$$\nSubstituting the numerical values:\n$$\nc_{\\mathrm{O_2}}^{\\mathrm{new}} = 0.20 + (-0.04 + 0.025)\n$$\n$$\nc_{\\mathrm{O_2}}^{\\mathrm{new}} = 0.20 - 0.015\n$$\n$$\nc_{\\mathrm{O_2}}^{\\mathrm{new}} = 0.185 \\, \\mathrm{mol}\\,\\mathrm{m}^{-3}\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n0.1850 \\, \\mathrm{mol}\\,\\mathrm{m}^{-3}\n$$",
            "answer": "$$\n\\boxed{0.1850}\n$$"
        },
        {
            "introduction": "The ultimate goal of a surrogate model in this context is to accelerate simulations, a task deeply connected to the numerical stability of the underlying differential equations. This practice  bridges the gap between the machine learning model and the numerical solver by analyzing the model's Jacobian matrix, $\\mathbf{J}$. By computing the eigenvalues of $\\mathbf{J}$, you will determine the maximum stable time step for an explicit integrator, revealing how the 'stiffness' of the ML surrogate directly impacts the computational speed-up you can achieve.",
            "id": "4037322",
            "problem": "Consider a reduced stiff chemical system where the species mass fraction vector $\\mathbf{Y} \\in \\mathbb{R}^{n_s}$ evolves according to an ordinary differential equation (ODE) of the form $\\dot{\\mathbf{Y}} = \\boldsymbol{\\omega}(\\mathbf{Y})$, where $\\boldsymbol{\\omega}(\\mathbf{Y})$ represents the chemical source terms. To accelerate chemistry and enable tabulation, one can replace the exact source terms with an Artificial Neural Network (ANN) surrogate trained to approximate $\\boldsymbol{\\omega}(\\mathbf{Y})$. Let the surrogate be a two-layer feedforward ANN with hyperbolic tangent activation, defined for input $\\mathbf{Y} \\in \\mathbb{R}^{3}$ as\n$$\n\\boldsymbol{f}(\\mathbf{Y}) = \\mathbf{W}_2 \\, \\tanh(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1) + \\mathbf{b}_2,\n$$\nwhere $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times 3}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{3 \\times m}$, and $\\mathbf{b}_2 \\in \\mathbb{R}^{3}$, with $m=3$. The surrogate output approximates $\\dot{\\mathbf{Y}}$ and has units of $\\mathrm{s}^{-1}$ when appropriately scaled. The Jacobian matrix $\\mathbf{J}(\\mathbf{Y})$ of the surrogate is defined as\n$$\n\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}(\\mathbf{Y})}{\\partial \\mathbf{Y}},\n$$\nwith entries having units of $\\mathrm{s}^{-1}$. Automatic Differentiation (AD) should be used to compute $\\mathbf{J}(\\mathbf{Y})$ based on the chain rule applied to the specified ANN structure.\n\nStarting from the linearization principle for stiff ODEs and the explicit Euler method, recall the fundamental stability consideration for the scalar linear test equation $\\dot{y} = \\lambda y$ that the explicit Euler step $y_{n+1} = y_n + h \\lambda y_n$ is linearly stable when the amplification factor satisfies $|1 + h \\lambda| \\leq 1$. Generalize this consideration to the matrix case by analyzing the eigenvalues of the Jacobian $\\mathbf{J}(\\mathbf{Y})$, and determine a conservative bound on the stable explicit Euler time step $h$ when eigenvalues have large negative real parts. You must express the stable step bound $h_{\\max}$ in seconds.\n\nImplement a complete program that:\n- Constructs the ANN surrogate $\\boldsymbol{f}(\\mathbf{Y})$ as specified.\n- Uses Automatic Differentiation, applied to the ANN layers and activation function, to compute the Jacobian $\\mathbf{J}(\\mathbf{Y})$ at specified $\\mathbf{Y}$.\n- Computes the eigenvalues of $\\mathbf{J}(\\mathbf{Y})$.\n- Determines a conservative maximum stable explicit Euler time step $h_{\\max}$ (in seconds) based on the eigenvalues, using the scalar stability condition generalized to each eigenvalue with possibly nonzero imaginary part.\n- Evaluates linear stability for a set of specified explicit Euler time steps $h$ by checking the spectral radius of the one-step amplification matrix $\\mathbf{A}(h) = \\mathbf{I} + h \\mathbf{J}(\\mathbf{Y})$ and testing whether it is less than or equal to $1$.\n\nYou must consider three test cases. For each case, you are given the network parameters and the evaluation point $\\mathbf{Y}$ along with three explicit Euler step sizes $h$ in seconds. Use the same ANN architecture described above with $m=3$. The parameters are:\n\nCase 1 (stiff with off-diagonal coupling):\n- $\\mathbf{W}_1 = \\begin{bmatrix} 3.0 & -1.0 & 0.5 \\\\ -2.0 & 4.0 & -0.5 \\\\ 1.0 & 1.0 & -3.0 \\end{bmatrix}$,\n- $\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$,\n- $\\mathbf{W}_2 = \\begin{bmatrix} -1000.0 & 50.0 & 0.0 \\\\ 0.0 & -50.0 & 20.0 \\\\ 10.0 & -5.0 & -1.0 \\end{bmatrix}$,\n- $\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $\\mathbf{Y} = \\begin{bmatrix} 0.1 \\\\ 0.3 \\\\ 0.6 \\end{bmatrix}$,\n- Step sizes $h \\in \\{10^{-5} \\text{ s}, 10^{-3} \\text{ s}, 10^{-2} \\text{ s}\\}$.\n\nCase 2 (moderately stiff):\n- $\\mathbf{W}_1 = \\begin{bmatrix} 1.0 & 0.2 & -0.3 \\\\ 0.5 & -0.1 & 0.4 \\\\ -0.7 & 0.8 & -0.2 \\end{bmatrix}$,\n- $\\mathbf{b}_1 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\\\ 0.02 \\end{bmatrix}$,\n- $\\mathbf{W}_2 = \\begin{bmatrix} -10.0 & 1.0 & 0.0 \\\\ 0.5 & -5.0 & 0.5 \\\\ 0.0 & 0.2 & -1.0 \\end{bmatrix}$,\n- $\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $\\mathbf{Y} = \\begin{bmatrix} 0.9 \\\\ 0.05 \\\\ 0.05 \\end{bmatrix}$,\n- Step sizes $h \\in \\{10^{-3} \\text{ s}, 5 \\cdot 10^{-2} \\text{ s}, 10^{-1} \\text{ s}\\}$.\n\nCase 3 (extremely stiff with activation saturation effects):\n- $\\mathbf{W}_1 = \\begin{bmatrix} 8.0 & -6.0 & 4.0 \\\\ -9.0 & 5.0 & -7.0 \\\\ 6.5 & -8.0 & 10.0 \\end{bmatrix}$,\n- $\\mathbf{b}_1 = \\begin{bmatrix} 0.5 \\\\ -0.3 \\\\ 0.1 \\end{bmatrix}$,\n- $\\mathbf{W}_2 = \\begin{bmatrix} -5000.0 & 200.0 & 0.0 \\\\ 0.0 & -100.0 & 40.0 \\\\ 30.0 & -10.0 & -20.0 \\end{bmatrix}$,\n- $\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $\\mathbf{Y} = \\begin{bmatrix} 0.33 \\\\ 0.33 \\\\ 0.34 \\end{bmatrix}$,\n- Step sizes $h \\in \\{10^{-5} \\text{ s}, 10^{-4} \\text{ s}, 10^{-3} \\text{ s}\\}$.\n\nFor each case:\n1. Use AD for the ANN to compute $\\mathbf{J}(\\mathbf{Y})$.\n2. Compute all eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{J}(\\mathbf{Y})$.\n3. From the scalar stability condition applied to each $\\lambda_i$, determine a conservative bound $h_{\\max}$ in seconds that ensures $|1 + h \\lambda_i| \\leq 1$ for all eigenvalues simultaneously.\n4. For each specified $h$ in the case, compute the spectral radius $\\rho(\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y}))$ and report whether the step is linearly stable, i.e., whether $\\rho(\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y})) \\leq 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one case and should be a list of the form $[h_{\\max}, s_1, s_2, s_3]$ where $h_{\\max}$ is a float in seconds and $s_k$ are booleans indicating stability for the $k$-th step size of that case. For example, the final output must look like\n$$\n[\\,[h_{\\max}^{(1)}, s_1^{(1)}, s_2^{(1)}, s_3^{(1)}],[h_{\\max}^{(2)}, s_1^{(2)}, s_2^{(2)}, s_3^{(2)}],[h_{\\max}^{(3)}, s_1^{(3)}, s_2^{(3)}, s_3^{(3)}]\\,].\n$$\nAll $h_{\\max}$ values must be expressed in seconds, and no other text should be printed.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n- **Governing Equation:** The species mass fraction vector $\\mathbf{Y} \\in \\mathbb{R}^{n_s}$ evolves according to the Ordinary Differential Equation (ODE) $\\dot{\\mathbf{Y}} = \\boldsymbol{\\omega}(\\mathbf{Y})$.\n- **ANN Surrogate:** The source term $\\boldsymbol{\\omega}(\\mathbf{Y})$ is approximated by a two-layer feedforward Artificial Neural Network (ANN) $\\boldsymbol{f}(\\mathbf{Y})$ for $\\mathbf{Y} \\in \\mathbb{R}^{3}$.\n- **ANN Architecture:** $\\boldsymbol{f}(\\mathbf{Y}) = \\mathbf{W}_2 \\, \\tanh(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1) + \\mathbf{b}_2$.\n- **ANN Parameters:** $\\mathbf{W}_1 \\in \\mathbb{R}^{m \\times 3}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{3 \\times m}$, $\\mathbf{b}_2 \\in \\mathbb{R}^{3}$, with hidden layer size $m=3$.\n- **Jacobian Matrix:** $\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}(\\mathbf{Y})}{\\partial \\mathbf{Y}}$.\n- **Stability Condition (Scalar):** For $\\dot{y} = \\lambda y$, the explicit Euler step is stable if $|1 + h \\lambda| \\leq 1$.\n- **Stability Condition (System):** For a given step size $h$, stability is assessed by checking if the spectral radius of the amplification matrix, $\\rho(\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y}))$, is less than or equal to $1$.\n- **Objective:** For three test cases, compute:\n    1. The Jacobian $\\mathbf{J}(\\mathbf{Y})$ using Automatic Differentiation (AD).\n    2. The eigenvalues of $\\mathbf{J}(\\mathbf{Y})$.\n    3. A conservative maximum stable explicit Euler time step $h_{\\max}$.\n    4. The stability for a given set of time steps $h$.\n- **Test Cases:** Three cases are provided, each with specific matrices $\\mathbf{W}_1, \\mathbf{W}_2$, vectors $\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{Y}$, and a set of step sizes $h$ to test.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on fundamental principles of numerical analysis (stability of ODE solvers), linear algebra (eigenvalue analysis), and standard machine learning models (feedforward neural networks). The application context, accelerating stiff chemical kinetics in computational combustion, is a well-established and active area of scientific research.\n- **Well-Posed:** The problem is clearly defined with a unique, computable solution. The ANN architecture, parameters, and the point of evaluation are all specified, leading to a deterministic Jacobian matrix and subsequent stability analysis.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased mathematical language.\n- **Completeness and Consistency:** All necessary information (network parameters, evaluation points, stability criteria) is provided. The dimensions of all matrices and vectors are consistent for the prescribed operations.\n- **Realism:** The setup is realistic. Stiff chemical systems are common, and the use of ANN surrogates is a modern technique. The large negative magnitudes in the weight matrices (e.g., $-1000.0, -5000.0$) are characteristic of models for stiff phenomena, which correctly imply a need for very small explicit time steps.\n- **Non-triviality:** The problem is substantive, requiring the application of matrix calculus (chain rule for the Jacobian), numerical linear algebra (eigenvalue computation), and the theory of numerical stability for systems of ODEs.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a well-defined, scientifically-grounded problem in applied mathematics and computational science. The solution process will now proceed.\n\n---\n\n### Principle-Based Solution\n\nThe core of the problem is to assess the stability of the explicit Euler numerical integration scheme when applied to a system of Ordinary Differential Equations (ODEs) whose right-hand side is approximated by an Artificial Neural Network (ANN).\n\n**1. Jacobian of the ANN via Automatic Differentiation (AD)**\n\nThe ANN acts as a surrogate for the chemical source terms, $\\dot{\\mathbf{Y}} \\approx \\boldsymbol{f}(\\mathbf{Y})$. The local behavior of this system is governed by its linearization, which requires the Jacobian matrix $\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{Y}}$. We compute this using the chain rule, which is the cornerstone of reverse-mode automatic differentiation.\n\nThe network is defined as $\\boldsymbol{f}(\\mathbf{Y}) = \\mathbf{W}_2 \\, \\tanh(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1) + \\mathbf{b}_2$. Let's decompose this into intermediate steps:\n1.  The pre-activation of the hidden layer: $\\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1$.\n2.  The activation of the hidden layer: $\\mathbf{a}_1 = \\tanh(\\mathbf{z}_1)$.\n3.  The output of the network: $\\boldsymbol{f} = \\mathbf{W}_2 \\mathbf{a}_1 + \\mathbf{b}_2$.\n\nApplying the chain rule for differentiation:\n$$\n\\mathbf{J}(\\mathbf{Y}) = \\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{Y}} = \\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{a}_1} \\frac{\\partial \\mathbf{a}_1}{\\partial \\mathbf{z}_1} \\frac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{Y}}\n$$\nLet's evaluate each partial derivative:\n-   $\\frac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{Y}}$: From the linear transformation $\\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1$, the derivative with respect to $\\mathbf{Y}$ is simply the weight matrix $\\mathbf{W}_1$.\n-   $\\frac{\\partial \\boldsymbol{f}}{\\partial \\mathbf{a}_1}$: From the output transformation $\\boldsymbol{f} = \\mathbf{W}_2 \\mathbf{a}_1 + \\mathbf{b}_2$, the derivative with respect to $\\mathbf{a}_1$ is the weight matrix $\\mathbf{W}_2$.\n-   $\\frac{\\partial \\mathbf{a}_1}{\\partial \\mathbf{z}_1}$: The activation function $\\tanh$ is applied element-wise. Therefore, its derivative is a diagonal matrix. The derivative of $\\tanh(x)$ is $1 - \\tanh^2(x)$. The resulting Jacobian of this layer is $\\mathbf{D} = \\text{diag}(1 - \\tanh^2(z_{1,i}))$, where $z_{1,i}$ are the elements of the vector $\\mathbf{z}_1$.\n\nCombining these components yields the analytical expression for the Jacobian of the ANN:\n$$\n\\mathbf{J}(\\mathbf{Y}) = \\mathbf{W}_2 \\cdot \\text{diag}(1 - \\tanh^2(\\mathbf{W}_1 \\mathbf{Y} + \\mathbf{b}_1)) \\cdot \\mathbf{W}_1\n$$\nThis formula is implemented to compute the Jacobian at a given state $\\mathbf{Y}$.\n\n**2. Explicit Euler Stability Analysis**\n\nThe explicit Euler method approximates the solution to $\\dot{\\mathbf{Y}} = \\boldsymbol{f}(\\mathbf{Y})$ by linearizing the system locally as $\\dot{\\boldsymbol{\\eta}} = \\mathbf{J}(\\mathbf{Y}_n) \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta}$ is a perturbation from $\\mathbf{Y}_n$. The update step is:\n$$\n\\mathbf{Y}_{n+1} = \\mathbf{Y}_n + h \\boldsymbol{f}(\\mathbf{Y}_n)\n$$\nFor stability analysis, we consider how perturbations evolve. A perturbation $\\boldsymbol{\\epsilon}_n$ at step $n$ evolves to $\\boldsymbol{\\epsilon}_{n+1}$ at step $n+1$ according to:\n$$\n\\mathbf{Y}_{n+1} + \\boldsymbol{\\epsilon}_{n+1} = (\\mathbf{Y}_n + \\boldsymbol{\\epsilon}_n) + h \\boldsymbol{f}(\\mathbf{Y}_n + \\boldsymbol{\\epsilon}_n) \\approx \\mathbf{Y}_n + \\boldsymbol{\\epsilon}_n + h (\\boldsymbol{f}(\\mathbf{Y}_n) + \\mathbf{J}(\\mathbf{Y}_n) \\boldsymbol{\\epsilon}_n)\n$$\nSubtracting the unperturbed equation, we get the evolution of the perturbation:\n$$\n\\boldsymbol{\\epsilon}_{n+1} = (\\mathbf{I} + h \\mathbf{J}(\\mathbf{Y}_n)) \\boldsymbol{\\epsilon}_n\n$$\nThe matrix $\\mathbf{A}(h) = \\mathbf{I} + h \\mathbf{J}$ is the amplification matrix. For the method to be stable, perturbations must not grow, which requires the spectral radius of $\\mathbf{A}(h)$ to be no greater than $1$:\n$$\n\\rho(\\mathbf{A}(h)) \\le 1\n$$\nThe eigenvalues of $\\mathbf{A}(h)$, denoted $\\mu_i$, are related to the eigenvalues of $\\mathbf{J}$, denoted $\\lambda_i$, by $\\mu_i = 1 + h \\lambda_i$. Thus, the stability condition becomes $|1 + h \\lambda_i| \\leq 1$ for all eigenvalues $\\lambda_i$ of the Jacobian $\\mathbf{J}$.\n\n**3. Maximum Stable Time Step $h_{\\max}$**\n\nLet an eigenvalue of $\\mathbf{J}$ be a complex number $\\lambda = x + iy$. The stability condition is $|1 + h(x+iy)|^2 \\leq 1$, which expands to:\n$$\n(1 + hx)^2 + (hy)^2 \\leq 1 \\implies 1 + 2hx + h^2x^2 + h^2y^2 \\leq 1 \\implies h(2x + h(x^2+y^2)) \\leq 0\n$$\nSince the time step $h$ must be positive, we require $2x + h(x^2+y^2) \\leq 0$. For a stiff system, we expect the real parts of the eigenvalues to be negative, i.e., $x = \\text{Re}(\\lambda) < 0$. If any $\\text{Re}(\\lambda) > 0$, the system is locally unstable, and no positive explicit time step can be stable, so $h_{\\max} = 0$. For $\\text{Re}(\\lambda) < 0$, the condition on $h$ is:\n$$\nh \\leq \\frac{-2x}{x^2+y^2} = \\frac{-2 \\text{Re}(\\lambda)}{|\\lambda|^2}\n$$\nTo ensure stability for all modes of the system, $h$ must satisfy this condition for every eigenvalue. Therefore, the maximum stable time step is the most restrictive of these bounds:\n$$\nh_{\\max} = \\min_{\\lambda_i : \\text{Re}(\\lambda_i) < 0} \\left( \\frac{-2 \\text{Re}(\\lambda_i)}{|\\lambda_i|^2} \\right)\n$$\n\n**4. Algorithmic Procedure**\n\nFor each test case, the following steps are performed:\n1.  Construct the NumPy-based representations of the network parameters $\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2$ and the state vector $\\mathbf{Y}$. Vectors are shaped as column vectors.\n2.  Calculate the Jacobian $\\mathbf{J}$ at $\\mathbf{Y}$ using the derived analytical formula.\n3.  Compute the eigenvalues $\\{\\lambda_i\\}$ of $\\mathbf{J}$ using `numpy.linalg.eigvals`.\n4.  Calculate $h_{\\max}$ by iterating through the eigenvalues and applying the formula derived above. An initial value of infinity is used for the minimum search. If any eigenvalue has a positive real part, $h_{\\max}$ is set to $0$.\n5.  For each specified step size $h$ in the test case:\n    a. Form the amplification matrix $\\mathbf{A}(h) = \\mathbf{I} + h \\mathbf{J}$.\n    b. Compute the eigenvalues of $\\mathbf{A}(h)$.\n    c. Calculate the spectral radius $\\rho(\\mathbf{A}(h))$ by finding the maximum absolute value among these eigenvalues.\n    d. The step $h$ is deemed stable if $\\rho(\\mathbf{A}(h)) \\leq 1$, and unstable otherwise.\n6.  The results ($h_{\\max}$ and the three stability booleans) for each case are collected and formatted into the required final output string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of determining explicit Euler stability for an ANN-surrogated ODE system.\n    \"\"\"\n    test_cases = [\n        {\n            \"W1\": np.array([[3.0, -1.0, 0.5], [-2.0, 4.0, -0.5], [1.0, 1.0, -3.0]]),\n            \"b1\": np.array([0.1, -0.2, 0.3]),\n            \"W2\": np.array([[-1000.0, 50.0, 0.0], [0.0, -50.0, 20.0], [10.0, -5.0, -1.0]]),\n            \"b2\": np.array([0.0, 0.0, 0.0]),\n            \"Y\": np.array([0.1, 0.3, 0.6]),\n            \"h_steps\": [1e-5, 1e-3, 1e-2]\n        },\n        {\n            \"W1\": np.array([[1.0, 0.2, -0.3], [0.5, -0.1, 0.4], [-0.7, 0.8, -0.2]]),\n            \"b1\": np.array([0.05, -0.05, 0.02]),\n            \"W2\": np.array([[-10.0, 1.0, 0.0], [0.5, -5.0, 0.5], [0.0, 0.2, -1.0]]),\n            \"b2\": np.array([0.0, 0.0, 0.0]),\n            \"Y\": np.array([0.9, 0.05, 0.05]),\n            \"h_steps\": [1e-3, 5e-2, 1e-1]\n        },\n        {\n            \"W1\": np.array([[8.0, -6.0, 4.0], [-9.0, 5.0, -7.0], [6.5, -8.0, 10.0]]),\n            \"b1\": np.array([0.5, -0.3, 0.1]),\n            \"W2\": np.array([[-5000.0, 200.0, 0.0], [0.0, -100.0, 40.0], [30.0, -10.0, -20.0]]),\n            \"b2\": np.array([0.0, 0.0, 0.0]),\n            \"Y\": np.array([0.33, 0.33, 0.34]),\n            \"h_steps\": [1e-5, 1e-4, 1e-3]\n        }\n    ]\n\n    all_case_results = []\n\n    for case in test_cases:\n        W1, b1, W2, b2, Y, h_steps = case.values()\n        \n        # Ensure vectors are column vectors (N, 1) for matrix operations\n        Y_col = Y.reshape(-1, 1)\n        b1_col = b1.reshape(-1, 1)\n\n        # 1. Compute Jacobian J(Y) using Automatic Differentiation formula\n        z1 = W1 @ Y_col + b1_col\n        a1 = np.tanh(z1)\n        \n        # Derivative of tanh is 1 - tanh^2, forms a diagonal matrix\n        d_tanh_dz1 = 1 - a1**2\n        D = np.diag(d_tanh_dz1.flatten())\n        \n        J = W2 @ D @ W1\n\n        # 2. Compute eigenvalues of J(Y)\n        eigs_J = np.linalg.eigvals(J)\n\n        # 3. Determine max stable time step h_max\n        h_max = np.inf\n        unstable_mode_found = False\n        for lam in eigs_J:\n            re_lam = lam.real\n            abs_lam_sq = np.abs(lam)**2\n            \n            if re_lam > 1e-12: # Check for positive real part (instability)\n                unstable_mode_found = True\n                break\n            \n            if re_lam < -1e-12 and abs_lam_sq > 1e-12:\n                h_bound = -2.0 * re_lam / abs_lam_sq\n                if h_bound < h_max:\n                    h_max = h_bound\n        \n        if unstable_mode_found:\n            h_max = 0.0\n\n        # 4. Check stability for each specified h\n        I = np.identity(J.shape[0])\n        stability_results = []\n        for h in h_steps:\n            # Amplification matrix A = I + hJ\n            A = I + h * J\n            # Eigenvalues of A\n            eigs_A = np.linalg.eigvals(A)\n            # Spectral radius of A\n            spectral_radius = np.max(np.abs(eigs_A))\n            is_stable = spectral_radius <= 1.0\n            stability_results.append(is_stable)\n\n        # Format results for the current case\n        case_result_list = [h_max] + stability_results\n        all_case_results.append(case_result_list)\n\n    # Format the final output string to match the required format without spaces\n    # Example: [[h1,True,False,False],[h2,True,True,False]]\n    formatted_results = []\n    for res_list in all_case_results:\n        # Convert each item to its string representation and join with commas\n        res_str = \",\".join(map(str, res_list))\n        formatted_results.append(f\"[{res_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}