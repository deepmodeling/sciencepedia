{
    "hands_on_practices": [
        {
            "introduction": "The foundation of a particle-based Filtered Density Function (FDF) method is its use of a discrete number of Lagrangian particles to represent the continuous subgrid-scale probability distribution of scalars. This representation is inherently statistical, and the accuracy of any computed filtered quantity, such as the Favre-filtered scalar $\\tilde{\\phi}$, is subject to sampling error. This exercise  provides a practical way to quantify this statistical error and demonstrates how it scales with the number of particles $N$, allowing you to determine the computational cost required to achieve a target precision.",
            "id": "4024881",
            "problem": "In a Large-Eddy Simulation (LES) of turbulent reacting flow, a particle-based Filtered Density Function (FDF) method is employed to estimate the Favre-filtered scalar field. The Favre-filtered scalar is defined by $\\tilde{\\phi} = \\overline{\\rho \\phi}/\\overline{\\rho}$, where $\\rho$ is the density, $\\phi$ is a scalar (e.g., mixture fraction), and the overbar denotes spatial filtering. Within a single LES control volume at a given time, you draw $N$ independent particle realizations $\\{\\phi_{i}\\}_{i=1}^{N}$ from the filtered density function, each representing equal mass and thus equal particle weight in the estimator. The Monte Carlo estimator of $\\tilde{\\phi}$, for equal masses, reduces to the simple sample mean $\\hat{\\tilde{\\phi}} = \\frac{1}{N}\\sum_{i=1}^{N}\\phi_{i}$.\n\nAssume the following scientifically justified conditions:\n- The particle samples are independent and identically distributed with finite variance, so that the Central Limit Theorem applies to $\\hat{\\tilde{\\phi}}$.\n- The subgrid scalar variance in this control volume, estimated from a prior step, is $\\operatorname{Var}(\\phi) = 2.5 \\times 10^{-3}$ (dimensionless).\n- You require a symmetric two-sided confidence interval for $\\tilde{\\phi}$ at confidence level $0.95$ whose half-width does not exceed $\\Delta = 1.0 \\times 10^{-3}$ (dimensionless).\n\nStarting from the Favre filtering definition and the Central Limit Theorem, derive how the statistical error of $\\hat{\\tilde{\\phi}}$ scales with $N$ and construct the corresponding normal-approximation confidence interval. Then determine the minimal integer $N$ such that the half-width of this interval is at most $\\Delta$ at confidence level $0.95$. Express your final answer as the single integer $N$.",
            "solution": "The problem requires determining the minimum number of particles, $N$, for a Monte Carlo estimation of a Favre-filtered scalar, $\\tilde{\\phi}$, to achieve a specified statistical precision. We begin by validating the problem statement.\n\n### Step 1: Extract Givens\n-   Favre-filtered scalar definition: $\\tilde{\\phi} = \\overline{\\rho \\phi}/\\overline{\\rho}$.\n-   Monte Carlo estimator for $\\tilde{\\phi}$: $\\hat{\\tilde{\\phi}} = \\frac{1}{N}\\sum_{i=1}^{N}\\phi_{i}$, where $\\{\\phi_{i}\\}_{i=1}^{N}$ are independent and identically distributed (i.i.d.) samples.\n-   The Central Limit Theorem (CLT) applies to $\\hat{\\tilde{\\phi}}$.\n-   Subgrid scalar variance: $\\operatorname{Var}(\\phi) = 2.5 \\times 10^{-3}$.\n-   Confidence level for a symmetric two-sided interval: $0.95$.\n-   Maximum allowed half-width of the confidence interval: $\\Delta = 1.0 \\times 10^{-3}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the theory of Large-Eddy Simulation (LES) and Filtered Density Function (FDF) methods, a standard approach in computational combustion. The use of a Monte Carlo estimator and the application of the Central Limit Theorem are fundamental statistical principles correctly applied to this context. The problem is well-posed, providing all necessary information—the variance of the underlying distribution, the desired confidence level, and the required precision—to uniquely determine the sample size $N$. The language is objective and unambiguous. The problem is therefore deemed valid.\n\n### Step 3: Derivation and Solution\nThe Monte Carlo estimator for the Favre-filtered scalar $\\tilde{\\phi}$ is given as the sample mean of $N$ i.i.d. particle realizations $\\{\\phi_i\\}$:\n$$ \\hat{\\tilde{\\phi}} = \\frac{1}{N}\\sum_{i=1}^{N}\\phi_{i} $$\nThe samples $\\phi_i$ are drawn from a distribution whose mean is the true Favre-filtered scalar, $E[\\phi_i] = \\tilde{\\phi}$, and whose variance is the subgrid scalar variance, $\\sigma^2 = \\operatorname{Var}(\\phi)$.\n\nThe expected value of the estimator is:\n$$ E[\\hat{\\tilde{\\phi}}] = E\\left[\\frac{1}{N}\\sum_{i=1}^{N}\\phi_{i}\\right] = \\frac{1}{N}\\sum_{i=1}^{N}E[\\phi_{i}] = \\frac{1}{N}(N\\tilde{\\phi}) = \\tilde{\\phi} $$\nThis confirms that the estimator is unbiased.\n\nThe variance of the estimator, due to the independence of the samples, is:\n$$ \\operatorname{Var}(\\hat{\\tilde{\\phi}}) = \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\phi_{i}\\right) = \\frac{1}{N^2}\\sum_{i=1}^{N}\\operatorname{Var}(\\phi_{i}) = \\frac{1}{N^2}(N\\sigma^2) = \\frac{\\sigma^2}{N} $$\nThe standard deviation of the estimator, known as the standard error, is $\\sigma_{\\hat{\\tilde{\\phi}}} = \\frac{\\sigma}{\\sqrt{N}}$. This demonstrates that the statistical error of the estimator scales with $N^{-1/2}$.\n\nAccording to the Central Limit Theorem, for a sufficiently large $N$, the distribution of the sample mean $\\hat{\\tilde{\\phi}}$ is approximately normal with mean $\\tilde{\\phi}$ and variance $\\frac{\\sigma^2}{N}$:\n$$ \\hat{\\tilde{\\phi}} \\sim \\mathcal{N}\\left(\\tilde{\\phi}, \\frac{\\sigma^2}{N}\\right) $$\nTo construct a confidence interval, we standardize the estimator to obtain a standard normal variable $Z$:\n$$ Z = \\frac{\\hat{\\tilde{\\phi}} - \\tilde{\\phi}}{\\sigma/\\sqrt{N}} \\sim \\mathcal{N}(0, 1) $$\nWe seek a symmetric two-sided confidence interval for $\\tilde{\\phi}$ at a confidence level of $1-\\alpha = 0.95$. This implies a significance level of $\\alpha = 0.05$. The interval is defined by:\n$$ P(-z_{\\alpha/2} \\le Z \\le z_{\\alpha/2}) = 1 - \\alpha $$\nwhere $z_{\\alpha/2}$ is the critical value from the standard normal distribution that leaves a tail probability of $\\alpha/2$. For our case, $\\alpha/2 = 0.025$. The corresponding critical value is $z_{0.025}$, which corresponds to a cumulative probability of $1 - 0.025 = 0.975$. The value is $z_{0.025} \\approx 1.96$.\n\nSubstituting the expression for $Z$ and rearranging the inequality to isolate $\\tilde{\\phi}$:\n$$ P\\left(\\hat{\\tilde{\\phi}} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}} \\le \\tilde{\\phi} \\le \\hat{\\tilde{\\phi}} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}}\\right) = 1 - \\alpha $$\nThe half-width of this confidence interval is the quantity added to and subtracted from the estimate $\\hat{\\tilde{\\phi}}$:\n$$ \\text{Half-width} = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}} $$\nThe problem requires this half-width to be at most $\\Delta = 1.0 \\times 10^{-3}$:\n$$ z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{N}} \\le \\Delta $$\nWe can now solve this inequality for the minimum integer $N$. We are given:\n-   $\\operatorname{Var}(\\phi) = \\sigma^2 = 2.5 \\times 10^{-3}$\n-   $\\Delta = 1.0 \\times 10^{-3}$\n-   $z_{0.025} \\approx 1.96$\n\nFirst, we find the standard deviation $\\sigma$:\n$$ \\sigma = \\sqrt{\\sigma^2} = \\sqrt{2.5 \\times 10^{-3}} = \\sqrt{25 \\times 10^{-4}} = 5 \\times 10^{-2} = 0.05 $$\nSubstituting the known values into the inequality:\n$$ 1.96 \\times \\frac{0.05}{\\sqrt{N}} \\le 1.0 \\times 10^{-3} $$\nRearranging to solve for $\\sqrt{N}$:\n$$ \\sqrt{N} \\ge \\frac{1.96 \\times 0.05}{1.0 \\times 10^{-3}} $$\n$$ \\sqrt{N} \\ge \\frac{0.098}{10^{-3}} $$\n$$ \\sqrt{N} \\ge 98 $$\nSquaring both sides to find the condition on $N$:\n$$ N \\ge 98^2 $$\n$$ N \\ge 9604 $$\nSince $N$ must be an integer, the minimal number of particles required is $9604$.",
            "answer": "$$\\boxed{9604}$$"
        },
        {
            "introduction": "In FDF methods, the evolution of scalar properties on Lagrangian particles is governed by models for processes like micromixing, which describes scalar mixing at the subgrid scale. While the widely used Interaction by Exchange with the Mean (IEM) model is simple and intuitive, its numerical implementation can lead to unphysical results. This practice problem  guides you through a rigorous analysis to see how and why a standard explicit time-stepping scheme can violate physical bounds (realizability), and how to implement a simple modification to enforce them, a critical skill for building robust FDF solvers.",
            "id": "4024972",
            "problem": "You are tasked with constructing and analyzing a mathematically rigorous test to demonstrate that the Interaction by Exchange with the Mean (IEM) micromixing model used in Filtered Density Function (FDF) methods for Large-Eddy Simulation (LES) can violate realizability for a bounded scalar, and then showing how a modified IEM update with clipping enforces boundedness. The scalar of interest, denoted by $\\xi$, is physically bounded in the interval $\\xi \\in [0,1]$ (for example, a mixture fraction or a normalized progress variable). The analysis and computations must be performed in purely mathematical terms and must be understandable and implementable in software. All physical parameters must be given in units of seconds where appropriate.\n\nBegin from the following base:\n\n- The Large-Eddy Simulation (LES) filtered density function (FDF) framework represents the subfilter statistics of scalars by an ensemble of Lagrangian particles whose states sample the filtered density function in composition space.\n- The Interaction by Exchange with the Mean (IEM) micromixing model for a passive, bounded scalar $\\xi$ prescribes the continuous-in-time evolution for each Lagrangian sample as\n$$\n\\frac{d\\xi}{dt}=-\\frac{\\xi-\\tilde{\\xi}}{\\tau_{\\text{mix}}},\n$$\nwhere $\\tilde{\\xi}$ is the filtered (conditional) mean of $\\xi$ and $\\tau_{\\text{mix}}$ is the micromixing time scale expressed in seconds.\n\nYour tasks:\n\n1) Derive the explicit first-order time-discretized update for the IEM model applied to a single forward Euler step over a time increment $\\Delta t$ (in seconds). Use $\\alpha = \\Delta t/\\tau_{\\text{mix}}$ to express the update and identify the condition under which this discrete update remains a convex combination of the current sample value and the filtered mean, hence respecting realizability $\\xi \\in [0,1]$ for all samples initially in $[0,1]$ and $\\tilde{\\xi} \\in [0,1]$.\n\n2) Using only logic and algebra, and assuming the initial ensemble of particles has $\\xi$ uniformly distributed on $[0,1]$ and the filtered mean $\\tilde{\\xi}=m \\in [0,1]$ is constant over the time step, derive a closed-form expression for the fraction of samples that will lie outside $[0,1]$ after one explicit Euler IEM update when $\\alpha > 1$. Your expression must separately account for the measure of samples that fall below $0$ and above $1$ and then combine them to produce the total fraction. No numerical integration or random sampling may be used in this derivation.\n\n3) Define a modified IEM update in which any sample value after the Euler update is clipped to the admissible interval $[0,1]$, i.e., replaced by $\\max(0, \\min(1, \\xi^{\\text{new}}))$. Explain why, under this modification, realizability is enforced for any $\\alpha > 0$ and $m \\in [0,1]$ even though the unmodified explicit Euler update may violate realizability for $\\alpha > 1$.\n\n4) Implement a program that, for each test case specified below, computes and outputs two quantities:\n   - The analytically derived fraction (as a decimal) of samples that violate $[0,1]$ after one unmodified explicit Euler IEM step.\n   - The fraction (as a decimal) of samples that violate $[0,1]$ after one modified IEM step with clipping.\n\nIn all cases use $\\tau_{\\text{mix}}$ and $\\Delta t$ in seconds, and express the final fractions as decimals.\n\nTest suite (each case provides $(\\tau_{\\text{mix}}, \\Delta t, m)$):\n- Case 1 (happy path): $\\tau_{\\text{mix}}=0.1$ s, $\\Delta t=0.01$ s, $m=0.5$.\n- Case 2 (boundary condition): $\\tau_{\\text{mix}}=0.02$ s, $\\Delta t=0.02$ s, $m=0.2$.\n- Case 3 (lower-bound violation): $\\tau_{\\text{mix}}=0.1$ s, $\\Delta t=0.2$ s, $m=0.3$.\n- Case 4 (upper-bound violation): $\\tau_{\\text{mix}}=0.05$ s, $\\Delta t=0.1$ s, $m=0.8$.\n- Case 5 (both-side violation): $\\tau_{\\text{mix}}=0.1$ s, $\\Delta t=0.3$ s, $m=0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for each case the two fractions $[\\text{unmodified}, \\text{modified}]$ flattened across the five cases as\n$$\n[\\text{C1 unmodified},\\text{C1 modified},\\text{C2 unmodified},\\text{C2 modified},\\text{C3 unmodified},\\text{C3 modified},\\text{C4 unmodified},\\text{C4 modified},\\text{C5 unmodified},\\text{C5 modified}].\n$$\nAll numeric outputs must be decimals. No other text should be printed.",
            "solution": "This analysis addresses the realizability of the Interaction by Exchange with the Mean (IEM) micromixing model when integrated with an explicit first-order Euler scheme. The scalar $\\xi$ is bounded within the interval $[0, 1]$. The continuous evolution of a Lagrangian sample $\\xi$ is given by the ordinary differential equation:\n$$\n\\frac{d\\xi}{dt}=-\\frac{\\xi-\\tilde{\\xi}}{\\tau_{\\text{mix}}}\n$$\nwhere $\\tilde{\\xi}$ is the filtered mean and $\\tau_{\\text{mix}}$ is the micromixing time scale.\n\n### 1. Discretization and Realizability Condition\n\nTo discretize the IEM equation, we apply a forward Euler scheme over a time step $\\Delta t$. Let $\\xi^n$ be the value of the scalar at time $t_n$ and $\\xi^{n+1}$ be the value at time $t_{n+1} = t_n + \\Delta t$. The time derivative is approximated as $\\frac{d\\xi}{dt} \\approx \\frac{\\xi^{n+1} - \\xi^n}{\\Delta t}$. Substituting this into the IEM equation gives:\n$$\n\\frac{\\xi^{n+1} - \\xi^n}{\\Delta t} = -\\frac{\\xi^n - \\tilde{\\xi}}{\\tau_{\\text{mix}}}\n$$\nSolving for $\\xi^{n+1}$, we obtain the explicit first-order update rule:\n$$\n\\xi^{n+1} = \\xi^n - \\frac{\\Delta t}{\\tau_{\\text{mix}}}(\\xi^n - \\tilde{\\xi})\n$$\nWe define the non-dimensional time step parameter $\\alpha = \\frac{\\Delta t}{\\tau_{\\text{mix}}}$. The update rule can then be written as:\n$$\n\\xi^{n+1} = \\xi^n - \\alpha(\\xi^n - \\tilde{\\xi})\n$$\nRearranging the terms to express $\\xi^{n+1}$ as a linear combination of $\\xi^n$ and $\\tilde{\\xi}$:\n$$\n\\xi^{n+1} = (1-\\alpha)\\xi^n + \\alpha\\tilde{\\xi}\n$$\nThis update represents a convex combination if the coefficients $(1-\\alpha)$ and $\\alpha$ are non-negative and sum to $1$. Their sum is $(1-\\alpha) + \\alpha = 1$. The non-negativity conditions are $\\alpha \\ge 0$ and $1-\\alpha \\ge 0$. Since $\\Delta t$ and $\\tau_{\\text{mix}}$ are both positive physical quantities, $\\alpha \\ge 0$ is always satisfied. The remaining condition for a convex combination is:\n$$\n1-\\alpha \\ge 0 \\implies \\alpha \\le 1\n$$\nIf this condition holds, i.e., $\\Delta t \\le \\tau_{\\text{mix}}$, and if both the current sample value $\\xi^n$ and the filtered mean $\\tilde{\\xi}$ are in the realizable interval $[0, 1]$, then the new sample value $\\xi^{n+1}$ is guaranteed to also be in $[0, 1]$. This is a fundamental property of convex combinations: they map a convex set (like the interval $[0, 1]$) into itself. Thus, the condition $\\alpha \\le 1$ is sufficient to ensure realizability is respected by the discrete update.\n\n### 2. Analytical Fraction of Non-Realizable Samples\n\nWe now derive the fraction of samples that violate the bounds $[0, 1]$ when $\\alpha > 1$. We assume the initial ensemble of particles has scalar values $\\xi_0$ that are uniformly distributed on $[0, 1]$. The probability density function is $p(\\xi_0) = 1$ for $\\xi_0 \\in [0,1]$ and $p(\\xi_0) = 0$ otherwise. The filtered mean is constant, $\\tilde{\\xi} = m \\in [0,1]$.\nThe updated value is $\\xi_1 = (1-\\alpha)\\xi_0 + \\alpha m$. Since $\\alpha > 1$, the coefficient $(1-\\alpha)$ is negative.\n\nA sample becomes non-realizable if $\\xi_1  0$ or $\\xi_1 > 1$.\nThe fraction of samples violating the bounds is the measure of the set of $\\xi_0 \\in [0,1]$ for which $\\xi_1 \\notin [0,1]$.\n\n**Violation Below $0$**:\nThe condition is $\\xi_1  0$, which translates to:\n$$\n(1-\\alpha)\\xi_0 + \\alpha m  0 \\implies (1-\\alpha)\\xi_0  -\\alpha m\n$$\nSince $(1-\\alpha)$ is negative, dividing by it reverses the inequality:\n$$\n\\xi_0 > \\frac{-\\alpha m}{1-\\alpha} = \\frac{\\alpha m}{\\alpha-1}\n$$\nThe values of $\\xi_0$ in the initial $[0, 1]$ range that cause this violation are those in the interval $(\\frac{\\alpha m}{\\alpha-1}, 1]$. The fraction of such samples, which is the length of this interval (as $p(\\xi_0)=1$), is non-zero only if $\\frac{\\alpha m}{\\alpha-1}  1$. This fraction, $F_{0}$, is:\n$$\nF_{0} = \\max\\left(0, 1 - \\frac{\\alpha m}{\\alpha-1}\\right)\n$$\n\n**Violation Above $1$**:\nThe condition is $\\xi_1 > 1$, which translates to:\n$$\n(1-\\alpha)\\xi_0 + \\alpha m > 1 \\implies (1-\\alpha)\\xi_0 > 1 - \\alpha m\n$$\nAgain, dividing by the negative $(1-\\alpha)$ reverses the inequality:\n$$\n\\xi_0  \\frac{1-\\alpha m}{1-\\alpha} = \\frac{\\alpha m-1}{\\alpha-1}\n$$\nThe values of $\\xi_0$ in $[0, 1]$ that cause this violation are those in the interval $[0, \\frac{\\alpha m-1}{\\alpha-1})$. The fraction of such samples, $F_{>1}$, is non-zero only if $\\frac{\\alpha m-1}{\\alpha-1} > 0$. This fraction is:\n$$\nF_{>1} = \\max\\left(0, \\frac{\\alpha m-1}{\\alpha-1}\\right)\n$$\n\nThe two conditions on $\\xi_0$ are mutually exclusive, as $\\frac{\\alpha m}{\\alpha-1} > \\frac{\\alpha m-1}{\\alpha-1}$. Therefore, the total fraction of non-realizable samples, $F_{\\text{total}}$, is the sum of the fractions from the two cases:\n$$\nF_{\\text{total}}(\\alpha, m) = \\begin{cases} 0  \\text{if } \\alpha \\le 1 \\\\ \\max\\left(0, 1 - \\frac{\\alpha m}{\\alpha-1}\\right) + \\max\\left(0, \\frac{\\alpha m - 1}{\\alpha-1}\\right)  \\text{if } \\alpha > 1 \\end{cases}\n$$\n\n### 3. Modified IEM Update with Clipping\n\nTo enforce realizability regardless of the value of $\\alpha$, the IEM update can be modified by applying a clipping operation. After computing the new value with the explicit Euler step, $\\xi^{\\text{new}} = (1-\\alpha)\\xi^n + \\alpha \\tilde{\\xi}$, it is forced back into the admissible interval $[0, 1]$. The modified update is:\n$$\n\\xi^{\\text{new}}_{\\text{clipped}} = \\max(0, \\min(1, \\xi^{\\text{new}}))\n$$\nThis operation works in two steps:\n1. The inner function, $\\min(1, \\xi^{\\text{new}})$, ensures that any value greater than $1$ is replaced by $1$. The result of this step is always less than or equal to $1$.\n2. The outer function, $\\max(0, \\dots)$, ensures that any value less than $0$ is replaced by $0$. The result of this step is always greater than or equal to $0$.\n\nCombined, the clipping function maps any real number $\\xi^{\\text{new}}$ to the interval $[0, 1]$. Consequently, for any positive $\\alpha$ and any initial $\\xi^n \\in [0,1]$ and $\\tilde{\\xi}=m \\in [0,1]$, the resulting scalar value $\\xi^{\\text{new}}_{\\text{clipped}}$ is guaranteed to be realizable. The fraction of samples that violate realizability after this modified step is, by definition, always $0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_violation_fractions(tau_mix: float, delta_t: float, m: float) - tuple[float, float]:\n    \"\"\"\n    Calculates the fraction of samples violating realizability for IEM model.\n\n    Args:\n        tau_mix: The micromixing time scale in seconds.\n        delta_t: The time step size in seconds.\n        m: The constant filtered mean, m = tilde{xi}.\n\n    Returns:\n        A tuple containing:\n        - The analytically derived fraction for the unmodified explicit Euler update.\n        - The fraction for the modified update with clipping.\n    \"\"\"\n    if tau_mix = 0:\n        # Avoid division by zero, though problem constraints imply tau_mix  0.\n        return 0.0, 0.0\n\n    alpha = delta_t / tau_mix\n    unmodified_fraction = 0.0\n\n    # The unmodified Euler step violates realizability only if alpha  1.\n    if alpha  1.0:\n        # Denominator for the violation expressions\n        denom = alpha - 1.0\n\n        # Fraction of samples that fall below 0\n        # This corresponds to initial xi_0  (alpha * m) / (alpha - 1)\n        # The fraction is max(0, 1 - (alpha * m) / (alpha - 1))\n        frac_below_zero = np.maximum(0.0, 1.0 - (alpha * m) / denom)\n\n        # Fraction of samples that fall above 1\n        # This corresponds to initial xi_0  (alpha * m - 1) / (alpha - 1)\n        # The fraction is max(0, (alpha * m - 1.0) / (alpha - 1))\n        frac_above_one = np.maximum(0.0, (alpha * m - 1.0) / denom)\n\n        # The two violation regions for initial xi_0 are disjoint,\n        # so the total fraction is their sum.\n        unmodified_fraction = frac_below_zero + frac_above_one\n\n    # The modified IEM update with clipping enforces realizability by construction.\n    # Therefore, the fraction of samples violating the bounds is always 0.\n    modified_fraction = 0.0\n\n    return unmodified_fraction, modified_fraction\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite and prints the results.\n    \"\"\"\n    # Test suite from the problem statement: (tau_mix, delta_t, m)\n    test_cases = [\n        (0.1, 0.01, 0.5),  # Case 1 (happy path, alpha  1)\n        (0.02, 0.02, 0.2),  # Case 2 (boundary condition, alpha = 1)\n        (0.1, 0.2, 0.3),   # Case 3 (lower-bound violation, alpha  1)\n        (0.05, 0.1, 0.8),   # Case 4 (upper-bound violation, alpha  1)\n        (0.1, 0.3, 0.5),   # Case 5 (both-side violation, alpha  1)\n    ]\n\n    results = []\n    for case in test_cases:\n        tau_mix, delta_t, m = case\n        unmod_frac, mod_frac = calculate_violation_fractions(tau_mix, delta_t, m)\n        results.append(unmod_frac)\n        results.append(mod_frac)\n\n    # Format the output as a single comma-separated list in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n\n```"
        },
        {
            "introduction": "A key advantage of the FDF methodology in simulating turbulent combustion is its ability to treat the highly nonlinear chemical source terms without modeling assumptions. However, the Monte Carlo evaluation of the filtered reaction rate can be computationally expensive, as reactions are often confined to rare events within the distribution (e.g., regions of high temperature). This exercise  introduces importance sampling, a powerful variance-reduction technique, to address this challenge and demonstrates its superior efficiency in accurately calculating filtered reaction rates.",
            "id": "4024869",
            "problem": "Consider the Large Eddy Simulation (LES) methodology for turbulent reacting flows, and the Filtered Density Function (FDF) approach to modeling unresolved scalar fluctuations. In the FDF framework, the filtered reaction rate for a local subgrid state is defined as the expectation of a reaction source functional over the filtered joint probability density function of scalars. Let the scalar vector be $(Z, \\Theta)$, where $Z \\in [0,1]$ is a dimensionless mixture fraction and $\\Theta \\in (0,\\infty)$ is a dimensionless temperature. Assume $(Z,\\Theta)$ are statistically independent at the filter scale. The filtered reaction rate is\n$$\n\\tilde{R} = \\iint S(Z,\\Theta)\\tilde{p}_{Z}(Z)\\tilde{p}_{\\Theta}(\\Theta) \\mathrm{d}Z \\mathrm{d}\\Theta,\n$$\nwhere $S(Z,\\Theta)$ is a nonnegative reaction source functional depending on local reactive states, and $\\tilde{p}_{Z}$ and $\\tilde{p}_{\\Theta}$ are the filtered marginal densities of $Z$ and $\\Theta$, respectively.\n\nStarting from first principles (definition of expectation and importance sampling in probability theory), derive an unbiased importance sampling estimator for $\\tilde{R}$ using a proposal density $q(Z,\\Theta)$ with full support over the support of $\\tilde{p}_{Z}(Z)\\tilde{p}_{\\Theta}(\\Theta)$. Explicitly show the conditions required for unbiasedness, including the weight definition and the expectation under the proposal measure.\n\nFor numerical evaluation, consider the following scientifically sound setup:\n- The reaction functional is\n$$\nS(Z,\\Theta) = Z(1-Z)\\exp\\left(-\\frac{\\beta}{\\Theta}\\right),\n$$\nwith nondimensional activation parameter $\\beta > 0$.\n- The filtered densities are independent and given by\n$$\n\\tilde{p}_{Z}(Z) = \\frac{Z^{a-1}(1-Z)^{b-1}}{B(a,b)}, \\quad Z\\in[0,1], \\qquad B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\n$$\nand\n$$\n\\tilde{p}_{\\Theta}(\\Theta) = \\frac{\\Theta^{k-1}\\exp(-\\Theta/s)}{\\Gamma(k)s^{k}}, \\quad \\Theta\\in(0,\\infty),\n$$\nwhere $a>0$, $b>0$ are the Beta parameters, and $k>1$, $s>0$ are the Gamma shape and scale parameters. These choices ensure physically plausible, positive-valued scalars and finite integrals.\n- The proposal density factorizes as $q(Z,\\Theta)=q_{Z}(Z)q_{\\Theta}(\\Theta)$ with\n$$\nq_{Z}(Z) = \\frac{Z^{c-1}(1-Z)^{d-1}}{B(c,d)}, \\quad Z\\in[0,1],\n$$\nand\n$$\nq_{\\Theta}(\\Theta) = \\frac{\\Theta^{k_q-1}\\exp(-\\Theta/s_q)}{\\Gamma(k_q)s_q^{k_q}}, \\quad \\Theta\\in(0,\\infty),\n$$\nfor parameters $c>0$, $d>0$, $k_q>1$, $s_q>0$ chosen to increase the frequency of rare reactive states (e.g., moderate $Z$ and large $\\Theta$).\n\nTasks:\n- Derivation. Using only the definition of expectation and the properties of probability density functions, derive the unbiased importance sampling estimator for $\\tilde{R}$ with reweighted particles drawn from $q(Z,\\Theta)$. Do not assume any shortcut formulas; begin from the definition $\\tilde{R}=\\mathbb{E}_{\\tilde{p}}[S(Z,\\Theta)]$ and construct the estimator step by step. State precisely the weight definition and the unbiasedness condition.\n- Algorithm design. Implement a program that:\n  1. Generates $N$ independent samples $(Z_i,\\Theta_i)$ from $q(Z,\\Theta)$ and computes the importance sampling estimator using weights $w(Z_i,\\Theta_i)=\\frac{\\tilde{p}_{Z}(Z_i)\\tilde{p}_{\\Theta}(\\Theta_i)}{q_{Z}(Z_i)q_{\\Theta}(\\Theta_i)}$.\n  2. Generates $N$ independent samples from $\\tilde{p}_{Z}(Z)\\tilde{p}_{\\Theta}(\\Theta)$ and computes the naive Monte Carlo estimator (i.e., simple average) for comparison.\n  3. Computes a high-accuracy numerical reference value for $\\tilde{R}$ using quadrature. Use the independence to factor the integral into $E_Z \\cdot E_\\Theta$, where\n     $$\n     E_Z = \\int_{0}^{1} Z(1-Z)\\tilde{p}_{Z}(Z)\\mathrm{d}Z = \\mathbb{E}[Z]-\\mathbb{E}[Z^{2}] = \\frac{a}{a+b} - \\frac{a(a+1)}{(a+b)(a+b+1)},\n     $$\n     and\n     $$\n     E_\\Theta = \\int_{0}^{\\infty} \\exp\\left(-\\frac{\\beta}{\\Theta}\\right)\\tilde{p}_{\\Theta}(\\Theta)\\mathrm{d}\\Theta,\n     $$\n     which should be evaluated numerically with sufficient accuracy, acknowledging that a closed form is not generally available for arbitrary $(k,s,\\beta)$.\n- Test suite. Use the following parameter sets, each with the same sample count $N$:\n  - Case A (moderately rare reactive states):\n    - $a=2$, $b=10$, $k=2$, $s=0.5$, $\\beta=3$,\n    - $c=3$, $d=3$, $k_q=2$, $s_q=1.2$,\n    - $N=50000$.\n  - Case B (severely rare reactive states):\n    - $a=2$, $b=20$, $k=2$, $s=0.2$, $\\beta=10$,\n    - $c=3$, $d=3$, $k_q=2$, $s_q=1.5$,\n    - $N=50000$.\n  - Case C (proposal equals target; boundary consistency check):\n    - $a=2$, $b=10$, $k=2$, $s=0.5$, $\\beta=3$,\n    - $c=2$, $d=10$, $k_q=2$, $s_q=0.5$,\n    - $N=50000$.\n- Output specification. For each case, compute the absolute error of the naive estimator and the absolute error of the importance sampling estimator relative to the quadrature reference. The program must output a single line containing a comma-separated list enclosed in square brackets with the error ratios (naive absolute error divided by importance sampling absolute error) for the three cases, each rounded to six decimal places. The final output format must be exactly of the form $[\\text{r}_A,\\text{r}_B,\\text{r}_C]$ where $\\text{r}_\\cdot$ are floats rounded to six decimal places. All quantities are dimensionless.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard, albeit advanced, problem in computational combustion that can be solved using established principles of probability theory and numerical methods. All necessary data and definitions are provided.\n\nThe derivation and algorithmic design will proceed as follows.\n\nThe filtered reaction rate $\\tilde{R}$ is defined as the expectation of the source functional $S(Z,\\Theta)$ with respect to the joint filtered density function $\\tilde{p}(Z,\\Theta)$. Given the statistical independence of the scalars $Z$ and $\\Theta$, the joint density is the product of the marginals: $\\tilde{p}(Z,\\Theta) = \\tilde{p}_{Z}(Z)\\tilde{p}_{\\Theta}(\\Theta)$. The integral to be estimated is:\n$$\n\\tilde{R} = \\mathbb{E}_{\\tilde{p}}[S(Z,\\Theta)] = \\int_{0}^{\\infty}\\int_{0}^{1} S(Z,\\Theta)\\,\\tilde{p}_{Z}(Z)\\,\\tilde{p}_{\\Theta}(\\Theta)\\, \\mathrm{d}Z\\, \\mathrm{d}\\Theta\n$$\n\nA naive Monte Carlo estimator for $\\tilde{R}$ is constructed by drawing $N$ independent and identically distributed (i.i.d.) samples $(Z_i, \\Theta_i)$ from the target distribution $\\tilde{p}(Z,\\Theta)$ and computing the sample mean of the functional $S(Z,\\Theta)$:\n$$\n\\hat{R}_{\\text{naive}} = \\frac{1}{N} \\sum_{i=1}^{N} S(Z_i, \\Theta_i), \\quad \\text{where } (Z_i, \\Theta_i) \\sim \\tilde{p}(Z, \\Theta)\n$$\nThis estimator is unbiased, i.e., $\\mathbb{E}_{\\tilde{p}}[\\hat{R}_{\\text{naive}}] = \\tilde{R}$, but can have high variance if the important regions of the integrand (where $S(Z,\\Theta)$ is large) are sampled infrequently by $\\tilde{p}(Z,\\Theta)$.\n\nImportance sampling provides a method to reduce this variance by sampling from a different distribution, the proposal density $q(Z,\\Theta)$, which is chosen to more frequently sample the important regions. The derivation begins by rewriting the integral for $\\tilde{R}$ by multiplying and dividing the integrand by the proposal density $q(Z,\\Theta)$. This is permissible provided that the support of $q(Z,\\Theta)$ covers the support of $\\tilde{p}(Z,\\Theta)S(Z,\\Theta)$, meaning $q(Z,\\Theta) > 0$ whenever $\\tilde{p}(Z,\\Theta)S(Z,\\Theta) \\neq 0$. The problem states that $q(Z,\\Theta)$ has full support over the support of $\\tilde{p}(Z,\\Theta)$, which satisfies this condition.\n\n$$\n\\tilde{R} = \\iint S(Z,\\Theta)\\,\\tilde{p}(Z,\\Theta) \\left(\\frac{q(Z,\\Theta)}{q(Z,\\Theta)}\\right) \\, \\mathrm{d}Z\\, \\mathrm{d}\\Theta = \\iint \\left( S(Z,\\Theta) \\frac{\\tilde{p}(Z,\\Theta)}{q(Z,\\Theta)} \\right) q(Z,\\Theta) \\, \\mathrm{d}Z\\, \\mathrm{d}\\Theta\n$$\n\nThis expression can be interpreted as the expectation of a new random variable, $S(Z,\\Theta) \\frac{\\tilde{p}(Z,\\Theta)}{q(Z,\\Theta)}$, with respect to the proposal density $q(Z,\\Theta)$. Let us define the importance weight as:\n$$\nw(Z,\\Theta) = \\frac{\\tilde{p}(Z,\\Theta)}{q(Z,\\Theta)} = \\frac{\\tilde{p}_{Z}(Z)\\tilde{p}_{\\Theta}(\\Theta)}{q_{Z}(Z)q_{\\Theta}(\\Theta)}\n$$\nWith this definition, the expression for $\\tilde{R}$ becomes:\n$$\n\\tilde{R} = \\mathbb{E}_{q}[S(Z,\\Theta) w(Z,\\Theta)]\n$$\n\nThe importance sampling Monte Carlo estimator, $\\hat{R}_{\\text{IS}}$, is formed by drawing $N$ i.i.d. samples $(Z_i, \\Theta_i)$ from the proposal distribution $q(Z,\\Theta)$ and calculating the sample mean of the reweighted function:\n$$\n\\hat{R}_{\\text{IS}} = \\frac{1}{N} \\sum_{i=1}^{N} S(Z_i, \\Theta_i) w(Z_i, \\Theta_i), \\quad \\text{where } (Z_i, \\Theta_i) \\sim q(Z, \\Theta)\n$$\n\nTo demonstrate that this estimator is unbiased, we take its expectation with respect to the proposal distribution $q(Z,\\Theta)$:\n$$\n\\mathbb{E}_{q}[\\hat{R}_{\\text{IS}}] = \\mathbb{E}_{q}\\left[ \\frac{1}{N} \\sum_{i=1}^{N} S(Z_i, \\Theta_i) w(Z_i, \\Theta_i) \\right]\n$$\nBy the linearity of expectation, we can move the expectation operator inside the sum:\n$$\n\\mathbb{E}_{q}[\\hat{R}_{\\text{IS}}] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}_{q}[S(Z_i, \\Theta_i) w(Z_i, \\Theta_i)]\n$$\nSince all samples $(Z_i, \\Theta_i)$ are i.i.d. draws from $q(Z,\\Theta)$, the expectation term is the same for all $i$. Let $(Z, \\Theta)$ represent a generic random variable pair with distribution $q(Z,\\Theta)$.\n$$\n\\mathbb{E}_{q}[\\hat{R}_{\\text{IS}}] = \\frac{1}{N} \\cdot N \\cdot \\mathbb{E}_{q}[S(Z, \\Theta) w(Z, \\Theta)] = \\mathbb{E}_{q}[S(Z, \\Theta) w(Z, \\Theta)]\n$$\nBy the definition of expectation, this is equivalent to the integral we derived earlier:\n$$\n\\mathbb{E}_{q}[S(Z, \\Theta) w(Z, \\Theta)] = \\iint S(Z,\\Theta) w(Z,\\Theta) q(Z,\\Theta) \\, \\mathrm{d}Z\\, \\mathrm{d}\\Theta\n$$\nSubstituting the definition of the weight $w(Z,\\Theta) = \\frac{\\tilde{p}(Z,\\Theta)}{q(Z,\\Theta)}$:\n$$\n\\mathbb{E}_{q}[\\hat{R}_{\\text{IS}}] = \\iint S(Z,\\Theta) \\left(\\frac{\\tilde{p}(Z,\\Theta)}{q(Z,\\Theta)}\\right) q(Z,\\Theta) \\, \\mathrm{d}Z\\, \\mathrm{d}\\Theta = \\iint S(Z,\\Theta) \\tilde{p}(Z,\\Theta) \\, \\mathrm{d}Z\\, \\mathrm{d}\\Theta\n$$\nThis final integral is the definition of $\\tilde{R}$. Thus, we have shown that $\\mathbb{E}_{q}[\\hat{R}_{\\text{IS}}] = \\tilde{R}$, confirming that the importance sampling estimator is unbiased.\n\nThe algorithm design for numerical evaluation follows from these principles.\n$1$. A high-accuracy reference value $\\tilde{R}_{\\text{ref}}$ is computed. Due to statistical independence, $\\tilde{R} = E_Z \\cdot E_\\Theta$. The term $E_Z$ is computed analytically as\n$$\nE_Z = \\frac{a}{a+b} - \\frac{a(a+1)}{(a+b)(a+b+1)}\n$$\nThe term $E_\\Theta$ is computed using high-precision numerical quadrature (e.g., `scipy.integrate.quad`):\n$$\nE_\\Theta = \\int_{0}^{\\infty} \\exp\\left(-\\frac{\\beta}{\\Theta}\\right)\\tilde{p}_{\\Theta}(\\Theta)\\mathrm{d}\\Theta\n$$\n$2$. The naive estimator $\\hat{R}_{\\text{naive}}$ is computed by drawing $N$ samples from the target distributions $\\tilde{p}_{Z}(Z) \\sim \\text{Beta}(a,b)$ and $\\tilde{p}_{\\Theta}(\\Theta) \\sim \\text{Gamma}(k,s)$ and averaging $S(Z_i, \\Theta_i)$.\n$3$. The importance sampling estimator $\\hat{R}_{\\text{IS}}$ is computed by drawing $N$ samples from the proposal distributions $q_{Z}(Z) \\sim \\text{Beta}(c,d)$ and $q_{\\Theta}(\\Theta) \\sim \\text{Gamma}(k_q,s_q)$, calculating the weights $w_i = \\frac{\\tilde{p}(Z_i, \\Theta_i)}{q(Z_i, \\Theta_i)}$ for each sample, and averaging the product $S(Z_i, \\Theta_i)w_i$.\n$4$. For each case, the absolute errors $|\\hat{R}_{\\text{naive}} - \\tilde{R}_{\\text{ref}}|$ and $|\\hat{R}_{\\text{IS}} - \\tilde{R}_{\\text{ref}}|$ are calculated, and their ratio is reported. To ensure numerical stability, the weights are computed using log-probabilities: $\\log w_i = \\log \\tilde{p}(Z_i,\\Theta_i) - \\log q(Z_i,\\Theta_i)$, and then $w_i = \\exp(\\log w_i)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta, gamma\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes and compares naive and importance sampling Monte Carlo estimators\n    for a filtered reaction rate in an FDF context for LES of turbulent combustion.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderately rare reactive states)\n        {\n            'name': 'A', 'N': 50000,\n            'target_params': {'a': 2.0, 'b': 10.0, 'k': 2.0, 's': 0.5, 'beta': 3.0},\n            'proposal_params': {'c': 3.0, 'd': 3.0, 'k_q': 2.0, 's_q': 1.2}\n        },\n        # Case B (severely rare reactive states)\n        {\n            'name': 'B', 'N': 50000,\n            'target_params': {'a': 2.0, 'b': 20.0, 'k': 2.0, 's': 0.2, 'beta': 10.0},\n            'proposal_params': {'c': 3.0, 'd': 3.0, 'k_q': 2.0, 's_q': 1.5}\n        },\n        # Case C (proposal equals target; boundary consistency check)\n        {\n            'name': 'C', 'N': 50000,\n            'target_params': {'a': 2.0, 'b': 10.0, 'k': 2.0, 's': 0.5, 'beta': 3.0},\n            'proposal_params': {'c': 2.0, 'd': 10.0, 'k_q': 2.0, 's_q': 0.5}\n        }\n    ]\n\n    results = []\n    \n    # Setting a fixed seed for reproducibility of the random numbers.\n    # While not strictly requested, it ensures the output is deterministic.\n    np.random.seed(42)\n\n    for case in test_cases:\n        # Extract parameters for the current case\n        N = case['N']\n        a, b, k, s, beta_val = case['target_params'].values()\n        c, d, k_q, s_q = case['proposal_params'].values()\n\n        # 1. Compute high-accuracy numerical reference value using quadrature\n        # E_Z term\n        E_Z = (a / (a + b)) - (a * (a + 1)) / ((a + b) * (a + b + 1))\n        \n        # E_Theta term integrand\n        def E_theta_integrand(theta):\n            return np.exp(-beta_val / theta) * gamma.pdf(theta, k, scale=s)\n        \n        # Numerical integration for E_Theta\n        E_Theta, _ = quad(E_theta_integrand, 0, np.inf)\n        \n        ref_R = E_Z * E_Theta\n\n        # 2. Compute the naive Monte Carlo estimator\n        # Generate samples from the target distribution p(Z, Theta)\n        Z_target_samples = np.random.beta(a, b, N)\n        Theta_target_samples = np.random.gamma(k, s, N)\n        \n        # Reaction source term S(Z, Theta)\n        S_target = Z_target_samples * (1 - Z_target_samples) * np.exp(-beta_val / Theta_target_samples)\n        \n        # Naive estimator\n        naive_R = np.mean(S_target)\n\n        # 3. Compute the importance sampling estimator\n        # Generate samples from the proposal distribution q(Z, Theta)\n        Z_proposal_samples = np.random.beta(c, d, N)\n        Theta_proposal_samples = np.random.gamma(k_q, s_q, N)\n\n        # Compute log-probabilities for numerical stability\n        # Log-pdf of target distribution at proposal samples\n        log_p = beta.logpdf(Z_proposal_samples, a, b) + gamma.logpdf(Theta_proposal_samples, k, scale=s)\n        \n        # Log-pdf of proposal distribution at proposal samples\n        log_q = beta.logpdf(Z_proposal_samples, c, d) + gamma.logpdf(Theta_proposal_samples, k_q, scale=s_q)\n        \n        # Importance weights\n        weights = np.exp(log_p - log_q)\n        \n        # Reaction source term evaluated at proposal samples\n        S_proposal = Z_proposal_samples * (1 - Z_proposal_samples) * np.exp(-beta_val / Theta_proposal_samples)\n        \n        # Importance sampling estimator\n        is_R = np.mean(S_proposal * weights)\n\n        # 4. Compute errors and their ratio\n        abs_err_naive = np.abs(naive_R - ref_R)\n        abs_err_is = np.abs(is_R - ref_R)\n        \n        # Handle case where importance sampling error is zero to avoid division by zero\n        if abs_err_is == 0:\n            # If IS is perfect and naive is not, ratio is infinite.\n            # If both are perfect, ratio is undefined, 1.0 is a reasonable assignment.\n            ratio = np.inf if abs_err_naive  0 else 1.0\n        else:\n            ratio = abs_err_naive / abs_err_is\n        \n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}