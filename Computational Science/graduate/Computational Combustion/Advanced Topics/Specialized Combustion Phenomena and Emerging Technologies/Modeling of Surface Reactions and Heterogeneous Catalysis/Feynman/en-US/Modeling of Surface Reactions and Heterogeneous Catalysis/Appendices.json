{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any reliable kinetic model lies in the accurate determination of its parameters. This practice focuses on extracting a fundamental kinetic parameter, the activation energy of desorption $E_{\\text{des}}$, from experimental data. You will engage with data from Temperature Programmed Desorption (TPD), a cornerstone technique in surface science, to connect a macroscopic observable (the peak desorption temperature, $T_{\\text{p}}$) to a microscopic energy barrier. This exercise will guide you through the first-principles derivation of the celebrated Redhead equation, providing a direct link between kinetic theory and experimental analysis .",
            "id": "4042357",
            "problem": "A catalytic carbon monoxide desorption experiment relevant to computational combustion is performed on a platinum surface using Temperature Programmed Desorption (TPD), where the temperature is ramped linearly in time. Assume first-order desorption kinetics with no readsorption and negligible lateral interactions. The desorption rate follows the Polanyi–Wigner form and the desorption activation barrier is independent of coverage.\n\nGiven the following TPD observables and kinetic prefactor: peak temperature $T_{\\text{p}} = 540\\,\\mathrm{K}$, constant heating rate $\\beta = 2.0\\,\\mathrm{K\\,s^{-1}}$, and a temperature-independent pre-exponential (attempt frequency) $\\nu = 1.0\\times 10^{13}\\,\\mathrm{s^{-1}}$, derive from first principles a closed-form analytic expression for the desorption activation energy $E_{\\text{des}}$ in terms of $T_{\\text{p}}$, $\\beta$, and $\\nu$, and then evaluate it numerically. Use the universal gas constant $R = 8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$. Express the final energy in $\\mathrm{kJ\\,mol^{-1}}$ and round your answer to four significant figures.",
            "solution": "The desorption process is described by the Polanyi–Wigner equation, which gives the rate of change of surface coverage $\\theta$ with time $t$:\n$$ r_{\\text{des}} = -\\frac{d\\theta}{dt} = \\nu \\theta^n \\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right) $$\nHere, $n$ is the order of the desorption reaction, $\\nu$ is the pre-exponential factor, $E_{\\text{des}}$ is the activation energy for desorption, $R$ is the universal gas constant, and $T$ is the absolute temperature.\n\nThe problem states that the desorption is a first-order process, so we set $n=1$:\n$$ -\\frac{d\\theta}{dt} = \\nu \\theta \\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right) $$\nIn a TPD experiment, the temperature is increased linearly with time, $T(t) = T_0 + \\beta t$, where $\\beta$ is the constant heating rate. This implies $\\frac{dT}{dt} = \\beta$, or $dt = \\frac{dT}{\\beta}$.\n\nThe desorption rate, which is the quantity measured in a TPD spectrum, is at its maximum at the peak temperature $T_{\\text{p}}$. To find this maximum, we must set the derivative of the rate with respect to time (or temperature) to zero. Let's differentiate the rate equation with respect to time $t$:\n$$ \\frac{d}{dt}\\left(-\\frac{d\\theta}{dt}\\right) = \\frac{d}{dt}\\left[\\nu \\theta \\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right)\\right] = 0 \\quad \\text{at} \\quad T = T_{\\text{p}} $$\nApplying the product rule to the right-hand side gives:\n$$ \\nu \\left[ \\frac{d\\theta}{dt} \\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right) + \\theta \\frac{d}{dt}\\left(\\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right)\\right) \\right]_{T=T_{\\text{p}}} = 0 $$\nWe evaluate the terms at the peak temperature $T_{\\text{p}}$.\nThe first term inside the bracket contains $\\frac{d\\theta}{dt}$, which we substitute from the rate equation itself:\n$$ \\frac{d\\theta}{dt} = -\\nu \\theta \\exp\\left(-\\frac{E_{\\text{des}}}{RT_{\\text{p}}}\\right) $$\nThe second term contains the time derivative of the exponential factor, which we evaluate using the chain rule:\n$$ \\frac{d}{dt}\\left(\\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right)\\right) = \\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right) \\cdot \\left(\\frac{E_{\\text{des}}}{RT^2}\\right) \\cdot \\frac{dT}{dt} = \\exp\\left(-\\frac{E_{\\text{des}}}{RT}\\right) \\cdot \\frac{E_{\\text{des}}\\beta}{RT^2} $$\nSubstituting these expressions back into the condition for the maximum rate, evaluated at $T=T_{\\text{p}}$, and dividing by the non-zero quantity $\\nu$:\n$$ \\left[ \\left(-\\nu \\theta \\exp\\left(-\\frac{E_{\\text{des}}}{RT_{\\text{p}}}\\right)\\right) \\exp\\left(-\\frac{E_{\\text{des}}}{RT_{\\text{p}}}\\right) + \\theta \\left(\\exp\\left(-\\frac{E_{\\text{des}}}{RT_{\\text{p}}}\\right) \\frac{E_{\\text{des}}\\beta}{RT_{\\text{p}}^2}\\right) \\right] = 0 $$\nWe can factor out the common term $\\theta(T_{\\text{p}}) \\exp\\left(-\\frac{E_{\\text{des}}}{RT_{\\text{p}}}\\right)$, which is non-zero at the peak:\n$$ -\\nu \\exp\\left(-\\frac{E_{\\text{des}}}{RT_{\\text{p}}}\\right) + \\frac{E_{\\text{des}}\\beta}{RT_{\\text{p}}^2} = 0 $$\nRearranging this equation gives the well-known Redhead equation, which implicitly relates $E_{\\text{des}}$ to the experimental observables:\n$$ \\frac{E_{\\text{des}}}{RT_{\\text{p}}^2} = \\frac{\\nu}{\\beta} \\exp\\left(-\\frac{E_{\\text{des}}}{RT_{\\text{p}}}\\right) $$\nTo find a closed-form analytic expression for $E_{\\text{des}}$, we rearrange this transcendental equation. Let $x = \\frac{E_{\\text{des}}}{RT_{\\text{p}}}$. The equation becomes:\n$$ \\frac{x RT_{\\text{p}}}{RT_{\\text{p}}^2} = \\frac{\\nu}{\\beta} \\exp(-x) $$\n$$ \\frac{x}{T_{\\text{p}}} = \\frac{\\nu}{\\beta} \\exp(-x) $$\nMultiplying by $T_{\\text{p}} \\exp(x)$, we get:\n$$ x \\exp(x) = \\frac{\\nu T_{\\text{p}}}{\\beta} $$\nThis equation is in the canonical form $z \\exp(z) = y$, where the solution for $z$ is given by the Lambert W function, $z = W(y)$. In our case, $z=x$ and $y = \\frac{\\nu T_{\\text{p}}}{\\beta}$.\nTherefore, the solution for $x$ is:\n$$ x = W\\left(\\frac{\\nu T_{\\text{p}}}{\\beta}\\right) $$\nSubstituting back $x = \\frac{E_{\\text{des}}}{RT_{\\text{p}}}$, we obtain the closed-form analytic expression for the desorption activation energy:\n$$ E_{\\text{des}} = RT_{\\text{p}} W\\left(\\frac{\\nu T_{\\text{p}}}{\\beta}\\right) $$\nNow, we evaluate this expression numerically using the given values:\n$T_{\\text{p}} = 540\\,\\mathrm{K}$\n$\\beta = 2.0\\,\\mathrm{K\\,s^{-1}}$\n$\\nu = 1.0\\times 10^{13}\\,\\mathrm{s^{-1}}$\n$R = 8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$\n\nFirst, we calculate the argument of the Lambert W function:\n$$ \\frac{\\nu T_{\\text{p}}}{\\beta} = \\frac{(1.0 \\times 10^{13}\\,\\mathrm{s^{-1}}) \\times (540\\,\\mathrm{K})}{2.0\\,\\mathrm{K\\,s^{-1}}} = 2.7 \\times 10^{15} $$\nNext, we evaluate the Lambert W function for this argument. For large arguments $y$, $W(y) \\approx \\ln(y) - \\ln(\\ln(y))$.\n$$ W(2.7 \\times 10^{15}) \\approx 32.064299 $$\nNow, we substitute this value back into the expression for $E_{\\text{des}}$:\n$$ E_{\\text{des}} = (8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}) \\times (540\\,\\mathrm{K}) \\times (32.064299) $$\n$$ E_{\\text{des}} \\approx 143969.8\\,\\mathrm{J\\,mol^{-1}} $$\nThe problem asks for the answer in units of $\\mathrm{kJ\\,mol^{-1}}$, rounded to four significant figures.\n$$ E_{\\text{des}} = 143.9698\\,\\mathrm{kJ\\,mol^{-1}} $$\nRounding to four significant figures gives:\n$$ E_{\\text{des}} = 144.0\\,\\mathrm{kJ\\,mol^{-1}} $$",
            "answer": "$$\n\\boxed{144.0}\n$$"
        },
        {
            "introduction": "While experiments provide crucial kinetic data, modern computational catalysis increasingly relies on first-principles calculations like Density Functional Theory (DFT) to explore reaction networks. However, computing the activation barrier for every elementary step can be prohibitively expensive. This exercise introduces a powerful data-driven approach using the Brønsted–Evans–Polanyi (BEP) principle, which posits a linear relationship between activation energies and reaction energies for similar reactions. You will implement a statistically rigorous method to fit a BEP relation from a sparse dataset and, most importantly, learn to quantify the uncertainty in your predictions—a critical skill for building robust microkinetic models .",
            "id": "4042389",
            "problem": "Consider a sequence of elementary catalytic surface reactions relevant to computational combustion. For each reaction step, Density Functional Theory (DFT) provides a reaction energy $\\Delta E$ in electronvolts (eV) and, for a subset of steps, an activation barrier $E_a$ in eV. Brønsted–Evans–Polanyi (BEP) correlations often relate the activation barrier and reaction energy for classes of similar reactions. Starting from Transition State Theory (TST), where the rate constant is given by $k = \\nu \\exp(-E_a/(k_B T))$ with $k_B$ the Boltzmann constant and $T$ the absolute temperature, and from the Hammond postulate suggesting proximity of transition states to reactants or products depending on exothermicity, develop a statistically principled linear estimator that maps $\\Delta E$ to $E_a$ using training pairs with known $E_a$ and then apply it to predict $E_a$ for steps where $E_a$ is missing. Assume Gaussian-distributed errors for measured activation barriers, with known standard deviations, and account for uncertainty in predicted inputs $\\Delta E$ via first-order uncertainty propagation.\n\nTasks:\n1. Starting from a Gaussian noise model on the measured activation barriers, derive the estimator for two unknown parameters that linearly map the reaction energy to the activation barrier. Express your estimator using the design matrix built from the reaction energies, a diagonal weight matrix, and the activation barrier vector. Derive the covariance matrix of the parameter estimates and an inflation rule based on the reduced chi-squared to account for potential model discrepancy.\n2. For a new reaction energy with an associated standard deviation, derive the mean predicted activation barrier and its one-standard-deviation uncertainty by propagating parameter uncertainty and input uncertainty (errors-in-variables) using a first-order delta method.\n3. Implement the estimator and predictive uncertainty in a program that, for each of the test cases below, fits the linear map on the provided training set and outputs the fitted parameters and predictions.\n\nDefinitions:\n- Brønsted–Evans–Polanyi (BEP) relation: an empirical linear correlation between the activation barrier and the reaction energy within a homologous series of reactions on a given surface.\n- Transition State Theory (TST): a theory that relates reaction rates to the free energy of activation via $k = \\nu \\exp(-E_a/(k_B T))$.\n\nAll energies must be treated and reported in eV. The final program output must be a single line containing a list of results for all test cases, each result consisting of the fitted slope and intercept, followed by the list of predicted activation barriers and the list of their one-standard-deviation uncertainties, all in eV. The exact final output format must be:\n- A single line printed as one Python list, where each test case result is a list of the form $[\\alpha, \\beta, [\\mu_1, \\mu_2, \\ldots], [\\sigma_1, \\sigma_2, \\ldots]]$, with $\\alpha$ and $\\beta$ the fitted parameters, $\\mu_j$ the predicted activation barrier for the $j$-th missing step, and $\\sigma_j$ its uncertainty.\n- Angles do not appear.\n- No percentages should be printed; the one-standard-deviation uncertainties are floats in eV.\n\nTest suite:\n- Case A (general case): Training data $(\\Delta E_i, E_{a,i}, \\sigma_{y,i})$ with\n  $\\Delta E$ values $[-0.45, 0.10, 0.35, 0.80, 1.20, -0.10, 0.55, 0.95]$ eV,\n  $E_a$ values $[0.62, 0.55, 0.60, 0.78, 0.92, 0.50, 0.72, 0.86]$ eV,\n  $\\sigma_y$ values $[0.05, 0.04, 0.04, 0.05, 0.06, 0.04, 0.05, 0.05]$ eV.\n  Predict for $\\Delta E^* = [0.00, 0.40, 1.00]$ eV, each with $\\sigma_x = 0.02$ eV.\n- Case B (boundary case near thermoneutrality): Training data\n  $\\Delta E = [-0.80, -0.30, 0.00, 0.20, 0.50]$ eV,\n  $E_a = [0.70, 0.58, 0.56, 0.60, 0.68]$ eV,\n  $\\sigma_y = [0.07, 0.05, 0.04, 0.05, 0.06]$ eV.\n  Predict for $\\Delta E^* = [0.00, -0.05, 0.05]$ eV, each with $\\sigma_x = 0.01$ eV.\n- Case C (edge case with minimal training data): Training data\n  $\\Delta E = [0.20, 0.90]$ eV,\n  $E_a = [0.60, 0.85]$ eV,\n  $\\sigma_y = [0.08, 0.08]$ eV.\n  Predict for $\\Delta E^* = [-0.10, 0.50, 1.20]$ eV, each with $\\sigma_x = 0.03$ eV.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[[\\alpha,\\beta,[\\mu_1,\\ldots],[\\sigma_1,\\ldots]], \\ldots]$, with all numeric entries given in eV. The floats must be printed using standard decimal notation.",
            "solution": "The problem requires the development and implementation of a statistically principled method for estimating parameters of a linear Brønsted–Evans–Polanyi (BEP) relation and predicting new activation barriers with quantified uncertainty. We will proceed by first deriving the necessary statistical framework based on weighted least squares and first-order uncertainty propagation, and then implementing this framework to solve the provided test cases.\n\n### 1. Parameter Estimation for the Linear Model\n\nThe BEP relation posits a linear relationship between the activation barrier $E_a$ and the reaction energy $\\Delta E$. For a set of $N$ training reactions, this relationship can be modeled as:\n$$\nE_{a,i} = \\alpha \\Delta E_i + \\beta + \\epsilon_i \\quad \\text{for } i = 1, \\ldots, N\n$$\nwhere $(\\alpha, \\beta)$ are the unknown linear parameters (slope and intercept), and $\\epsilon_i$ represents the error term for the $i$-th observation. The problem states that the measured activation barriers, which we denote as $y_i \\equiv E_{a,i}$, are subject to Gaussian-distributed errors with known standard deviations $\\sigma_{y,i}$. We assume the reaction energies for the training set, denoted $x_i \\equiv \\Delta E_i$, are known precisely. The error terms $\\epsilon_i$ are thus independent random variables with $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_{y,i}^2)$.\n\nThis framework leads to a weighted least squares (WLS) problem. The optimal parameters $(\\hat{\\alpha}, \\hat{\\beta})$ are those that minimize the sum of squared, weighted residuals, also known as the chi-squared ($\\chi^2$) statistic:\n$$\n\\chi^2(\\alpha, \\beta) = \\sum_{i=1}^{N} \\left(\\frac{y_i - (\\alpha x_i + \\beta)}{\\sigma_{y,i}}\\right)^2\n$$\nMinimizing $\\chi^2$ is equivalent to maximizing the likelihood of the data under the Gaussian error model. To formalize the solution, we use matrix notation. Let the vector of parameters be $\\mathbf{\\theta} = [\\alpha, \\beta]^T$. The model is written as $\\mathbf{y} \\approx \\mathbf{X}\\mathbf{\\theta}$, where:\n- $\\mathbf{y} = [y_1, y_2, \\ldots, y_N]^T$ is the vector of observed activation barriers.\n- $\\mathbf{X}$ is the $N \\times 2$ design matrix:\n$$\n\\mathbf{X} = \\begin{pmatrix}\nx_1 & 1 \\\\\nx_2 & 1 \\\\\n\\vdots & \\vdots \\\\\nx_N & 1\n\\end{pmatrix}\n$$\nThe $\\chi^2$ statistic is then:\n$$\n\\chi^2(\\mathbf{\\theta}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta})^T \\mathbf{W} (\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta})\n$$\nwhere $\\mathbf{W}$ is the diagonal weight matrix with entries $W_{ii} = 1/\\sigma_{y,i}^2$.\n\nTo find the estimator $\\hat{\\mathbf{\\theta}}$ that minimizes $\\chi^2$, we set the gradient with respect to $\\mathbf{\\theta}$ to zero:\n$$\n\\frac{\\partial \\chi^2}{\\partial \\mathbf{\\theta}} = -2\\mathbf{X}^T \\mathbf{W} (\\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{\\theta}}) = \\mathbf{0}\n$$\nThis yields the normal equations for WLS:\n$$\n(\\mathbf{X}^T \\mathbf{W} \\mathbf{X}) \\hat{\\mathbf{\\theta}} = \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nProvided that the matrix $(\\mathbf{X}^T \\mathbf{W} \\mathbf{X})$ is invertible (which requires at least two distinct $x_i$ values), the estimator for the parameters is:\n$$\n\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} (\\mathbf{X}^T \\mathbf{W} \\mathbf{y})\n$$\n\n### 2. Covariance of Parameter Estimates and Model Discrepancy\n\nThe uncertainty in the estimated parameters $\\hat{\\mathbf{\\theta}}$ arises from the uncertainty in the input measurements $\\mathbf{y}$. The covariance matrix of $\\mathbf{y}$ is $\\mathbf{C}_y = \\mathbf{W}^{-1} = \\text{diag}(\\sigma_{y,1}^2, \\ldots, \\sigma_{y,N}^2)$. Propagating this uncertainty through the linear estimator equation gives the covariance matrix of the parameters, $\\mathbf{C}_{\\theta}$:\n$$\n\\mathbf{C}_{\\theta} = \\text{Cov}(\\hat{\\mathbf{\\theta}}) = ((\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}) \\mathbf{C}_y ((\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W})^T\n$$\nSubstituting $\\mathbf{C}_y = \\mathbf{W}^{-1}$ and noting that $\\mathbf{W}$ is symmetric, this simplifies to:\n$$\n\\mathbf{C}_{\\theta} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1}\n$$\nThe diagonal elements of $\\mathbf{C}_{\\theta}$ are the variances of the parameters, $\\sigma_\\alpha^2$ and $\\sigma_\\beta^2$, and the off-diagonal elements are their covariance, $\\text{Cov}(\\alpha, \\beta)$.\n\nThe minimized value of $\\chi^2$ at $\\hat{\\mathbf{\\theta}}$, denoted $\\chi^2_{\\text{min}}$, serves as a goodness-of-fit metric. The reduced chi-squared, $\\chi^2_{\\text{red}} = \\chi^2_{\\text{min}} / \\nu$, where $\\nu = N - p$ are the degrees of freedom ($N$ data points, $p=2$ parameters), is expected to be approximately $1$ if the model is correct and the error estimates $\\sigma_{y,i}$ are accurate. A value of $\\chi^2_{\\text{red}} > 1$ suggests either that the model is a poor representation of the data or that the measurement errors were underestimated. A common practice is to inflate the parameter covariance matrix by this factor to account for the observed excess scatter:\n$$\n\\mathbf{C}_{\\theta}^{\\text{inflated}} = S^2 \\cdot \\mathbf{C}_{\\theta} \\quad \\text{where} \\quad S^2 = \\max(1, \\chi^2_{\\text{red}})\n$$\nThis inflation rule ensures that the uncertainty in the parameters reflects the actual scatter of the data around the fitted model, not just the reported measurement errors. In the special case where $N=p$ (e.g., fitting a line to two points), $\\nu=0$ and the fit is perfect, so $\\chi^2_{\\text{min}}=0$. The fraction $\\chi^2_{\\text{red}}$ becomes $0/0$. In this scenario, we have no information to assess model discrepancy, so we take $S^2=1$ by convention.\n\n### 3. Prediction and Uncertainty for New Data\n\nGiven a new reaction energy $\\Delta E^* \\equiv x^*$ with an associated uncertainty $\\sigma_x$, we wish to predict the corresponding activation barrier $E_a^*$ and its uncertainty.\n\nThe mean predicted activation barrier, $\\mu$, is obtained by simply evaluating the fitted linear model at $x^*$:\n$$\n\\mu = \\hat{\\alpha} x^* + \\hat{\\beta} = [x^*, 1] \\hat{\\mathbf{\\theta}} = \\mathbf{x}^{*T} \\hat{\\mathbf{\\theta}}\n$$\nwhere $\\mathbf{x}^* = [x^*, 1]^T$.\n\nThe uncertainty in this prediction arises from two independent sources: (1) the uncertainty in the fitted parameters $\\hat{\\mathbf{\\theta}}$, and (2) the uncertainty in the input value $x^*$. We use a first-order Taylor expansion (the delta method) to propagate these uncertainties. The variance of the prediction, $\\sigma_\\mu^2$, is given by:\n$$\n\\sigma_\\mu^2 \\approx \\nabla f^T \\mathbf{C} \\nabla f\n$$\nwhere $f$ is the prediction function $f(x^*, \\alpha, \\beta) = \\alpha x^* + \\beta$, $\\nabla f$ is the gradient of $f$ with respect to its random variables, and $\\mathbf{C}$ is their covariance matrix.\n\nThe total variance $\\sigma_{\\mu}^2$ is the sum of the variances from parameter uncertainty and input uncertainty:\n$$\n\\sigma_{\\mu}^2 = \\sigma_{\\text{param}}^2 + \\sigma_{\\text{input}}^2\n$$\n\n1.  **Parameter Uncertainty Contribution**: The variance due to the uncertainty in $\\hat{\\mathbf{\\theta}}$ is:\n    $$\n    \\sigma_{\\text{param}}^2 = \\mathbf{x}^{*T} \\mathbf{C}_{\\theta}^{\\text{inflated}} \\mathbf{x}^*\n    $$\n\n2.  **Input Uncertainty Contribution (Errors-in-Variables)**: The variance due to the uncertainty in $x^*$ is:\n    $$\n    \\sigma_{\\text{input}}^2 = \\left(\\frac{\\partial f}{\\partial x^*}\\right)^2 \\sigma_x^2 = \\hat{\\alpha}^2 \\sigma_x^2\n    $$\n\nCombining these, the total variance of the prediction is:\n$$\n\\sigma_{\\mu}^2 = \\mathbf{x}^{*T} \\mathbf{C}_{\\theta}^{\\text{inflated}} \\mathbf{x}^* + \\hat{\\alpha}^2 \\sigma_x^2\n$$\nThe final one-standard-deviation uncertainty for the prediction is $\\sigma_{\\mu} = \\sqrt{\\sigma_{\\mu}^2}$.\n\nThis completes the derivation of the required estimator and its associated predictive uncertainty. The algorithm will now be implemented to solve the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(train_DE, train_Ea, train_sigma_y, pred_DE, pred_sigma_x):\n    \"\"\"\n    Fits a linear BEP model using weighted least squares and predicts Ea values.\n\n    Args:\n        train_DE (list): Training reaction energies (x-values).\n        train_Ea (list): Training activation barriers (y-values).\n        train_sigma_y (list): Standard deviations of training activation barriers.\n        pred_DE (list): Reaction energies for which to predict Ea.\n        pred_sigma_x (float or list): Standard deviation(s) of prediction reaction energies.\n\n    Returns:\n        tuple: (alpha, beta, mu_list, sigma_list)\n               - alpha (float): Fitted slope.\n               - beta (float): Fitted intercept.\n               - mu_list (list): Predicted mean activation barriers.\n               - sigma_list (list): Predicted one-sigma uncertainties.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    x = np.array(train_DE)\n    y = np.array(train_Ea)\n    sigma_y = np.array(train_sigma_y)\n    \n    # --- 1. Parameter Estimation via Weighted Least Squares ---\n    \n    # Number of training points\n    N = len(x)\n    \n    # Design matrix X\n    X = np.vstack((x, np.ones(N))).T\n    \n    # Weight matrix W (diagonal with 1/sigma_y^2)\n    W = np.diag(1.0 / sigma_y**2)\n    \n    # Calculate (X^T * W * X)\n    X_T_W = X.T @ W\n    X_T_W_X = X_T_W @ X\n    \n    # Inverse of (X^T * W * X)\n    try:\n        X_T_W_X_inv = np.linalg.inv(X_T_W_X)\n    except np.linalg.LinAlgError:\n        # This would happen if X is not full rank (e.g., all x are the same)\n        # Not expected for the given test cases\n        return float('nan'), float('nan'), [], []\n\n    # Calculate (X^T * W * y)\n    X_T_W_y = X_T_W @ y\n    \n    # Estimate parameters theta_hat = [alpha, beta]\n    theta_hat = X_T_W_X_inv @ X_T_W_y\n    alpha, beta = theta_hat[0], theta_hat[1]\n    \n    # --- 2. Covariance and Model Discrepancy ---\n\n    # Basic parameter covariance matrix\n    C_theta = X_T_W_X_inv\n    \n    # Calculate reduced chi-squared and inflation factor S^2\n    p = 2  # number of parameters (alpha, beta)\n    dof = N - p\n    \n    if dof > 0:\n        residuals = y - (alpha * x + beta)\n        chi_squared_min = np.sum((residuals / sigma_y)**2)\n        chi_squared_red = chi_squared_min / dof\n        inflation_factor_sq = max(1.0, chi_squared_red)\n    else:\n        # For N=p, dof=0. No information to assess goodness of fit.\n        # By convention, assume no model discrepancy, so inflation factor is 1.\n        inflation_factor_sq = 1.0\n\n    # Inflated covariance matrix\n    C_theta_inflated = inflation_factor_sq * C_theta\n    \n    # --- 3. Prediction and Uncertainty Propagation ---\n    \n    pred_x = np.array(pred_DE)\n    if isinstance(pred_sigma_x, (int, float)):\n        pred_sx = np.full_like(pred_x, pred_sigma_x)\n    else:\n        pred_sx = np.array(pred_sigma_x)\n\n    mu_list = []\n    sigma_list = []\n    \n    for x_star, sigma_x in zip(pred_x, pred_sx):\n        # Mean prediction\n        mu = alpha * x_star + beta\n        mu_list.append(mu)\n        \n        # Prediction vector\n        x_star_vec = np.array([x_star, 1.0])\n        \n        # Variance from parameter uncertainty\n        var_param = x_star_vec.T @ C_theta_inflated @ x_star_vec\n        \n        # Variance from input uncertainty\n        var_input = (alpha**2) * (sigma_x**2)\n        \n        # Total variance and standard deviation\n        total_var = var_param + var_input\n        sigma = np.sqrt(total_var)\n        sigma_list.append(sigma)\n        \n    return alpha, beta, mu_list, sigma_list\n    \ndef solve():\n    \"\"\"\n    Defines and runs the test cases, then prints the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"train_DE\": [-0.45, 0.10, 0.35, 0.80, 1.20, -0.10, 0.55, 0.95],\n            \"train_Ea\": [0.62, 0.55, 0.60, 0.78, 0.92, 0.50, 0.72, 0.86],\n            \"train_sigma_y\": [0.05, 0.04, 0.04, 0.05, 0.06, 0.04, 0.05, 0.05],\n            \"pred_DE\": [0.00, 0.40, 1.00],\n            \"pred_sigma_x\": 0.02,\n        },\n        {\n            \"train_DE\": [-0.80, -0.30, 0.00, 0.20, 0.50],\n            \"train_Ea\": [0.70, 0.58, 0.56, 0.60, 0.68],\n            \"train_sigma_y\": [0.07, 0.05, 0.04, 0.05, 0.06],\n            \"pred_DE\": [0.00, -0.05, 0.05],\n            \"pred_sigma_x\": 0.01,\n        },\n        {\n            \"train_DE\": [0.20, 0.90],\n            \"train_Ea\": [0.60, 0.85],\n            \"train_sigma_y\": [0.08, 0.08],\n            \"pred_DE\": [-0.10, 0.50, 1.20],\n            \"pred_sigma_x\": 0.03,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta, mu_list, sigma_list = process_case(\n            case[\"train_DE\"],\n            case[\"train_Ea\"],\n            case[\"train_sigma_y\"],\n            case[\"pred_DE\"],\n            case[\"pred_sigma_x\"]\n        )\n        \n        # Format the result for the current case as required\n        case_result = [alpha, beta, mu_list, sigma_list]\n        results.append(case_result)\n\n    # Convert the list of results to the required string format\n    # The default str() on a list provides the correct '[...]' formatting.\n    # Joining these with ',' and enclosing in '[]' gives the final output.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A kinetic model of surface reactions becomes truly powerful when integrated into a larger simulation of a reactor or process. This requires correctly coupling the chemistry occurring at a surface with the transport of species in the adjacent gas phase. This final practice bridges the gap between surface kinetics and computational fluid dynamics (CFD) by focusing on the formulation of a scientifically accurate boundary condition. By applying the principle of species conservation at an interface, you will derive the precise mathematical relationship between the surface reaction rate vector $\\boldsymbol{\\sigma}$ and the multicomponent diffusive fluxes described by the Maxwell–Stefan equations. Mastering this concept is essential for implementing catalytic chemistry in advanced transport simulation software .",
            "id": "4042420",
            "problem": "Consider a steady, isothermal, isobaric, multicomponent ideal-gas mixture of $N$ species adjacent to a planar catalytic wall in a computational combustion system. Let the unit normal vector $\\boldsymbol{n}$ point from the wall into the gas. Denote by $\\boldsymbol{N} \\in \\mathbb{R}^{N}$ the vector of species molar fluxes relative to the stationary laboratory frame, by $\\boldsymbol{J} \\in \\mathbb{R}^{N}$ the vector of Maxwell–Stefan diffusive molar fluxes defined relative to the molar-average velocity, and by $\\boldsymbol{x} \\in \\mathbb{R}^{N}$ the vector of gas-phase mole fractions. These satisfy the fundamental decomposition $\\boldsymbol{N} = \\boldsymbol{J} + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{N})\\,\\boldsymbol{x}$ with the constraint $\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{J} = 0$, where $\\mathbf{1} \\in \\mathbb{R}^{N}$ is the vector of ones. The wall hosts heterogeneous reactions that impose a specified surface consumption vector $\\boldsymbol{\\sigma} \\in \\mathbb{R}^{N}$ with components $\\sigma_{i}$ in $\\mathrm{mol\\,m^{-2}\\,s^{-1}}$, defined such that $\\sigma_{i} > 0$ indicates net consumption of gas-phase species $i$ by the surface, and $\\sigma_{i} < 0$ indicates net production of species $i$ from the surface into the gas.\n\nAssume no interfacial storage and that all interfacial source/sink terms are represented by $\\boldsymbol{\\sigma}$. Denote by $\\boldsymbol{x}_{w}$ the mole-fraction vector evaluated on the gas side at the wall. Using only species conservation at an infinitesimal interfacial control volume and the above flux decomposition, derive the boundary condition that relates the normal component of the Maxwell–Stefan diffusive molar flux at the wall, $\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}$, to the specified surface consumption vector $\\boldsymbol{\\sigma}$ and the wall-adjacent mole fractions $\\boldsymbol{x}_{w}$. Express your final answer as a single compact analytic expression in vector form involving $\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}$, $\\boldsymbol{\\sigma}$, $\\boldsymbol{x}_{w}$, and $\\mathbf{1}$. No numerical evaluation is required, and no rounding is needed. Do not include units in your final expression.",
            "solution": "The derivation is based on the principle of species conservation at the gas-surface interface.\n\nFirst, we establish the mass balance for each species $i$ at the interface. The system is at steady state, and there is no storage of species at the interface. Therefore, for each species $i$, the molar flux of that species from the gas phase to the wall must be equal to the net rate at which it is consumed by surface reactions.\n\nThe molar flux of species $i$ is a vector quantity, which we shall denote $\\boldsymbol{N}_i$. The component of this flux normal to the wall and pointing into the gas is given by $\\boldsymbol{n} \\cdot \\boldsymbol{N}_i$. The problem uses a compact vector notation where $\\boldsymbol{N} \\in \\mathbb{R}^N$ is the vector of these normal flux components, such that the $i$-th component of a vector like $\\boldsymbol{N}$ is the scalar normal flux of species $i$. Let's denote the vector of normal components of the total molar flux at the wall as $\\boldsymbol{N}_{n,w}$.\n\nThe problem states that $\\sigma_i$ is the surface consumption rate of species $i$. Therefore, the surface *production* rate for species $i$ is $-\\sigma_i$. The species conservation law dictates that the flux of species $i$ leaving the surface and entering the gas, $(\\boldsymbol{N}_{n,w})_i$, must equal its production rate at the surface. In vector form, this conservation law is:\n$$\n\\boldsymbol{N}_{n,w} = -\\boldsymbol{\\sigma}\n$$\nHere, $\\boldsymbol{N}_{n,w}$ is the vector of molar fluxes normal to the wall, evaluated at the wall.\n\nThe problem provides the fundamental decomposition of the total molar flux into a diffusive part and a convective part. In our notation for normal components, this is:\n$$\n\\boldsymbol{N}_{n} = \\boldsymbol{J}_{n} + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{N}_{n})\\,\\boldsymbol{x}\n$$\nwhere $\\boldsymbol{J}_{n}$ is the vector of normal components of the Maxwell-Stefan diffusive fluxes. The problem denotes this quantity as $\\boldsymbol{n} \\cdot \\boldsymbol{J}$. Let us adopt this notation for the remainder of the derivation. Evaluated at the wall, the decomposition is:\n$$\n\\boldsymbol{N}_{n,w} = (\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}) + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{N}_{n,w})\\,\\boldsymbol{x}_{w}\n$$\n\nOur goal is to find an expression for $\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}$. We can rearrange the equation above to isolate this term:\n$$\n\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w} = \\boldsymbol{N}_{n,w} - (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{N}_{n,w})\\,\\boldsymbol{x}_{w}\n$$\n\nNow, we substitute the species conservation relationship, $\\boldsymbol{N}_{n,w} = -\\boldsymbol{\\sigma}$, into this equation:\n$$\n\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w} = (-\\boldsymbol{\\sigma}) - (\\mathbf{1}^{\\mathsf{T}} (-\\boldsymbol{\\sigma}))\\,\\boldsymbol{x}_{w}\n$$\n\nThis expression can be simplified. The scalar product $\\mathbf{1}^{\\mathsf{T}} (-\\boldsymbol{\\sigma})$ is equal to $-(\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma})$.\n$$\n\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w} = -\\boldsymbol{\\sigma} - (-(\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma}))\\,\\boldsymbol{x}_{w}\n$$\n$$\n\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w} = -\\boldsymbol{\\sigma} + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma})\\,\\boldsymbol{x}_{w}\n$$\nThis is the desired boundary condition. It relates the diffusive flux at the wall to the surface reaction rates and the local mole fractions.\n\nTo verify the consistency of this result, we can check if it satisfies the given constraint $\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{J} = 0$. Applying the operator $\\mathbf{1}^{\\mathsf{T}}$ to our derived expression for $\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}$:\n$$\n\\mathbf{1}^{\\mathsf{T}} (\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}) = \\mathbf{1}^{\\mathsf{T}} \\left( -\\boldsymbol{\\sigma} + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma})\\,\\boldsymbol{x}_{w} \\right)\n$$\nDistributing the transpose:\n$$\n\\mathbf{1}^{\\mathsf{T}} (\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}) = -\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma} + \\mathbf{1}^{\\mathsf{T}} \\left( ((\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma})\\,\\boldsymbol{x}_{w}) \\right)\n$$\nThe term $(\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma})$ is a scalar, representing the total molar rate of consumption at the surface. It can be factored out of the second term:\n$$\n\\mathbf{1}^{\\mathsf{T}} (\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}) = -\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma} + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma}) (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{x}_{w})\n$$\nBy definition, the sum of mole fractions is unity: $\\mathbf{1}^{\\mathsf{T}}\\boldsymbol{x}_{w} = \\sum_{i=1}^{N} x_{i,w} = 1$. Substituting this into the equation:\n$$\n\\mathbf{1}^{\\mathsf{T}} (\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}) = -\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma} + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma})(1) = -\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma} + \\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma} = 0\n$$\nThe derived expression is indeed consistent with the fundamental constraint on the Maxwell-Stefan diffusive fluxes.\n\nThe final expression for the boundary condition is therefore:\n$$\n\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w} = -\\boldsymbol{\\sigma} + (\\mathbf{1}^{\\mathsf{T}} \\boldsymbol{\\sigma})\\,\\boldsymbol{x}_{w}\n$$\nThe problem asks for the single compact analytic expression that $\\boldsymbol{n} \\cdot \\boldsymbol{J}_{w}$ is related to. This is the right-hand side of the derived equation.",
            "answer": "$$\n\\boxed{-\\boldsymbol{\\sigma} + (\\mathbf{1}^{\\mathsf{T}}\\boldsymbol{\\sigma})\\boldsymbol{x}_{w}}\n$$"
        }
    ]
}