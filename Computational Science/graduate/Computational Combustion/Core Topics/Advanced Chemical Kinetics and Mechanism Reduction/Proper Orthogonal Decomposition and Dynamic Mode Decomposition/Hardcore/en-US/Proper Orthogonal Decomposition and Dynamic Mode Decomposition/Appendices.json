{
    "hands_on_practices": [
        {
            "introduction": "Proper Orthogonal Decomposition (POD) is renowned for its ability to extract the most \"energetic\" structures from a dataset, making it a cornerstone of model reduction. However, the definition of energy is not universal and depends critically on the inner product chosen for the analysis. This exercise  demonstrates how a physically-motivated, weighted inner product can prioritize dynamically relevant modes over those with high variance but little physical importance, a crucial step in building meaningful reduced-order models.",
            "id": "3987525",
            "problem": "Consider a reduced two-component state extracted from a compressible flow simulation in aerospace Computational Fluid Dynamics (CFD). The state at time index $k$ is $x_{k} \\in \\mathbb{R}^{2}$ with components $x_{k} = \\begin{pmatrix} c_{k} \\\\ u'_{k} \\end{pmatrix}$, where $c_{k}$ represents a passive scalar concentration fluctuation driven by injection and $u'_{k}$ represents the streamwise velocity fluctuation. You are given four snapshots,\n$$\nx_{1} = \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix},\\quad\nx_{2} = \\begin{pmatrix} -4 \\\\ 0 \\end{pmatrix},\\quad\nx_{3} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix},\\quad\nx_{4} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nThe sample size is $m = 4$. The passive scalar component has larger variance than the velocity component in these data, but kinetic energy is physically associated with $u'_{k}$ rather than $c_{k}$.\n\nDefine two inner products on $\\mathbb{R}^{2}$:\n- The Euclidean inner product $\\langle a, b \\rangle = a^{\\top} b$.\n- The energy-weighted inner product $\\langle a, b \\rangle_{W} = a^{\\top} W b$ with $W = \\operatorname{diag}(\\alpha, 1)$ and $\\alpha = 0.01$, which downweights the passive scalar in proportion to its lack of physical relevance to kinetic energy.\n\nStarting from the definition of Proper Orthogonal Decomposition (POD) as the problem of finding directions that maximize the mean-square projection of the data subject to orthonormality in the chosen inner product, derive the leading POD mode under the Euclidean inner product and under the energy-weighted inner product. Then explain, from first principles, how and why the choice of inner product reorders the leading POD modes in this example, and discuss the implications this has for model reduction and for interpreting modes relative to Dynamic Mode Decomposition (DMD) (define the acronym and its core aim before discussing).\n\nReport the components of the leading POD modes as a single dimensionless row vector\n$$\n\\left((\\phi^{\\mathrm{E}}_{1})_{1},\\;(\\phi^{\\mathrm{E}}_{1})_{2},\\;(\\phi^{\\mathrm{W}}_{1})_{1},\\;(\\phi^{\\mathrm{W}}_{1})_{2}\\right),\n$$\nwhere $\\phi^{\\mathrm{E}}_{1}$ is the leading POD mode under the Euclidean inner product and $\\phi^{\\mathrm{W}}_{1}$ is the leading POD mode under the energy-weighted inner product. No rounding is required.",
            "solution": "Proper Orthogonal Decomposition (POD) finds an orthonormal basis $\\{\\phi_j\\}$ that optimally represents a dataset in an energy sense. The leading POD mode, $\\phi_1$, is the direction that maximizes the mean-square projection of the data: $\\arg\\max_{\\|\\phi\\|=1} \\frac{1}{m} \\sum_{k=1}^{m} |\\langle x_k, \\phi \\rangle|^2$. This is equivalent to finding the eigenvector corresponding to the largest eigenvalue of the correlation matrix $C = \\frac{1}{m} X X^\\top$, where $X$ is the snapshot matrix. The norm $\\|\\cdot\\|$ and inner product $\\langle \\cdot, \\cdot \\rangle$ depend on the chosen metric.\n\nThe snapshot matrix $X$ is formed by the given vectors:\n$$\nX = \\begin{pmatrix} 4 & -4 & 3 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}\n$$\n\n**1. Euclidean Inner Product**\n\nFor the Euclidean inner product, $\\langle a, b \\rangle = a^{\\top} b$, the correlation matrix (proportional to $XX^\\top$) is:\n$$\nX X^\\top = \\begin{pmatrix} 4^2 + (-4)^2 + 3^2 & 0 \\\\ 0 & 1^2 \\end{pmatrix} = \\begin{pmatrix} 16+16+9 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 41 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis matrix is diagonal, so its eigenvalues are $\\lambda_1 = 41$ and $\\lambda_2 = 1$. The leading eigenvalue is $\\lambda_1 = 41$. The corresponding eigenvector is $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. This vector is already normalized with the Euclidean norm. Thus, the leading POD mode is:\n$$\n\\phi^{\\mathrm{E}}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThis mode aligns with the scalar concentration component $c_k$, which has the largest variance in the data.\n\n**2. Energy-Weighted Inner Product**\n\nFor the weighted inner product, $\\langle a, b \\rangle_W = a^{\\top} W b$, we can simplify the problem by transforming the coordinates. Let $y = W^{1/2} x$ and $\\psi = W^{1/2} \\phi$. The weighted inner product becomes a standard Euclidean inner product in the new coordinates: $\\langle x, \\phi \\rangle_W = (W^{1/2}x)^\\top(W^{1/2}\\phi) = y^\\top\\psi$. The POD problem is now to find the leading eigenvector $\\psi_1$ of the correlation matrix of the transformed data, $C_y = \\frac{1}{m} Y Y^\\top$, where $Y = W^{1/2}X$.\n\nFirst, we compute the matrix proportional to $C_y$:\n$$ W = \\begin{pmatrix} 0.01 & 0 \\\\ 0 & 1 \\end{pmatrix} \\implies W^{1/2} = \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$\nY Y^\\top = (W^{1/2} X)(W^{1/2} X)^\\top = W^{1/2} (X X^\\top) W^{1/2} = \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 41 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.41 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe eigenvalues are now $\\lambda'_1 = 1$ and $\\lambda'_2 = 0.41$. The leading eigenvalue is $\\lambda'_1=1$, and its corresponding eigenvector is $\\psi_1 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nTo find the POD mode $\\phi^{\\mathrm{W}}_{1}$ in the original coordinates, we transform back: $\\phi^{\\mathrm{W}}_{1} = W^{-1/2}\\psi_1$.\n$$\n\\phi^{\\mathrm{W}}_{1} = \\begin{pmatrix} 10 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nWe verify that this mode is normalized in the weighted norm: $(\\phi^{\\mathrm{W}}_{1})^\\top W \\phi^{\\mathrm{W}}_{1} = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.01 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1$. The result is correct.\n\n**Discussion**\n\nThe choice of inner product redefines the \"energy\" that POD seeks to maximize.\n-   Under the Euclidean inner product, \"energy\" is simply variance. Since the scalar concentration $c_k$ has a variance of $41/4$, while the velocity $u'_k$ has a variance of $1/4$, the leading mode $\\phi^{\\mathrm{E}}_{1}$ aligns with the $c_k$ axis.\n-   Under the energy-weighted inner product, the contribution of the scalar concentration to the total energy is down-weighted by a factor of $\\alpha=0.01$. The weighted energy of the first component becomes $41 \\times 0.01 = 0.41$, which is now less than the weighted energy of the second component, $1 \\times 1 = 1$. Consequently, POD prioritizes the velocity fluctuation, and the leading mode $\\phi^{\\mathrm{W}}_{1}$ aligns with the $u'_k$ axis.\n\nThis has profound implications for model reduction. A model built on the Euclidean POD basis would prioritize a passive scalar, likely missing the key kinetic energy dynamics. A model built on the physically-motivated weighted basis correctly identifies the velocity fluctuation as the most \"energetic\" and dynamically important feature. This also helps align POD modes with dynamically coherent structures that would be identified by **Dynamic Mode Decomposition (DMD)**, a method that decomposes data into modes with a single frequency and growth rate. Using a physical inner product for POD makes its energy-based ranking more likely to correlate with the dynamic importance ranking of DMD.\n\nThe components of the leading modes are $(\\phi^{\\mathrm{E}}_{1})_{1}=1$, $(\\phi^{\\mathrm{E}}_{1})_{2}=0$, $(\\phi^{\\mathrm{W}}_{1})_{1}=0$, and $(\\phi^{\\mathrm{W}}_{1})_{2}=1$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0 & 0 & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Dynamic Mode Decomposition (DMD) excels at identifying flow structures that evolve with a single frequency and growth or decay rate, making it invaluable for stability analysis. The output of a DMD analysis is a set of eigenvalues of a best-fit linear operator, which can seem abstract at first. This practice problem  provides a direct method for translating these discrete-time DMD eigenvalues $\\lambda$ into the continuous-time physical characteristics of growth rate $\\sigma$ and oscillation frequency $f$, enabling you to interpret the stability of the underlying system.",
            "id": "3987563",
            "problem": "An unsteady separated flow past a pitching airfoil is simulated using the incompressible Navierâ€“Stokes equations in a high-fidelity Computational Fluid Dynamics (CFD) solver. A sequence of velocity-field snapshots is collected at uniform sampling interval $\\Delta t$ from a statistically stationary regime. The snapshots are assembled into a snapshot matrix in the spirit of Proper Orthogonal Decomposition (POD), and a Dynamic Mode Decomposition (DMD) is performed to approximate the one-step linear flow map that advances the state by $\\Delta t$.\n\nAssume the flow evolution over short times is well-approximated by a linear operator advancing the state according to a discrete-time map derived from the continuous-time dynamics, and that DMD computes eigenvalues $\\lambda$ of this map. Consider three dominant DMD modes whose eigenvalues, extracted from the CFD snapshot sequence sampled at $\\Delta t = 0.005$ $\\mathrm{s}$, are\n$$\n\\lambda_{1} = 1.02 \\exp(i\\,0.40),\\quad\n\\lambda_{2} = 0.98 \\exp(i\\,2.30),\\quad\n\\lambda_{3} = 0.995 \\exp(-i\\,0.05),\n$$\nwhere $i$ denotes the imaginary unit and each complex argument is taken in the principal range $(-\\pi,\\pi]$.\n\nStarting from the definition of the discrete-time flow map and its relationship to the continuous-time generator of the dynamics, translate each DMD eigenvalue $\\lambda_{j}$ to its corresponding continuous-time exponential growth or decay rate $\\sigma_{j}$ (in $\\mathrm{s}^{-1}$) and signed oscillation frequency $f_{j}$ (in $\\mathrm{Hz}$). Use the principal branch of the complex logarithm in this translation. Then, interpret for each mode whether the dynamics is unstable, neutrally stable, or damped based on the sign of $\\sigma_{j}$, and whether it is oscillatory and at what signed frequency.\n\nRound all reported numerical values for $\\sigma_{j}$ and $f_{j}$ to four significant figures. Express frequencies in $\\mathrm{Hz}$ and growth/decay rates in $\\mathrm{s}^{-1}$. Your final answer must provide, in order, the six numbers $\\sigma_{1}, f_{1}, \\sigma_{2}, f_{2}, \\sigma_{3}, f_{3}$.",
            "solution": "The problem requires the translation of discrete-time eigenvalues from a Dynamic Mode Decomposition (DMD) analysis into continuous-time dynamic characteristics: the exponential growth/decay rate $\\sigma_j$ and the signed oscillation frequency $f_j$.\n\nLet the state of the fluid flow at discrete time instances $t_k = k \\Delta t$ be represented by a vector $x_k$. The DMD algorithm seeks to find a linear operator, represented by a matrix $F$, that best approximates the evolution of the state from one snapshot to the next, such that $x_{k+1} \\approx F x_k$. The eigenvalues $\\lambda_j$ provided in the problem are the eigenvalues of this discrete-time operator $F$.\n\nThe underlying assumption is that the flow dynamics can be modeled by a continuous-time linear system of the form $\\frac{dx}{dt} = Ax$, where $A$ is a time-invariant linear operator. The solution to this system is $x(t) = \\exp(At) x(0)$. The discrete-time map $F$ is therefore the exponentiation of the continuous-time generator $A$ over the time step $\\Delta t$, i.e., $F = \\exp(A \\Delta t)$.\n\nThe relationship between the eigenvalues $\\lambda_j$ of $F$ and the eigenvalues $\\omega_j$ of $A$ is given by $\\lambda_j = \\exp(\\omega_j \\Delta t)$. To find the continuous-time eigenvalues $\\omega_j$, we take the natural logarithm of this equation. The problem specifies using the principal branch of the complex logarithm.\n$$\n\\ln(\\lambda_j) = \\ln(\\exp(\\omega_j \\Delta t)) = \\omega_j \\Delta t\n$$\nThus, the continuous-time eigenvalues are given by:\n$$\n\\omega_j = \\frac{\\ln(\\lambda_j)}{\\Delta t}\n$$\nEach eigenvalue $\\omega_j$ is a complex number that can be expressed in terms of its real and imaginary parts: $\\omega_j = \\sigma_j + i \\Omega_j$. The real part, $\\sigma_j$, is the exponential growth rate (if $\\sigma_j > 0$) or decay rate (if $\\sigma_j < 0$). The imaginary part, $\\Omega_j$, is the angular frequency in radians per unit time. The problem asks for the signed oscillation frequency $f_j$ in Hertz, which is related to $\\Omega_j$ by $\\Omega_j = 2\\pi f_j$.\n\nTo perform the calculation, we first express the DMD eigenvalue $\\lambda_j$ in polar form, $\\lambda_j = |\\lambda_j| \\exp(i\\theta_j)$, where $\\theta_j$ is the principal argument. The principal logarithm is then $\\ln(\\lambda_j) = \\ln(|\\lambda_j|) + i\\theta_j$. Substituting this into the expression for $\\omega_j$:\n$$\n\\omega_j = \\sigma_j + i(2\\pi f_j) = \\frac{\\ln(|\\lambda_j|) + i\\theta_j}{\\Delta t} = \\frac{\\ln(|\\lambda_j|)}{\\Delta t} + i \\frac{\\theta_j}{\\Delta t}\n$$\nBy equating the real and imaginary parts, we obtain the formulas for $\\sigma_j$ and $f_j$:\n$$\n\\sigma_j = \\frac{\\ln(|\\lambda_j|)}{\\Delta t} \\quad (\\text{in } \\mathrm{s}^{-1})\n$$\n$$\nf_j = \\frac{\\theta_j}{2\\pi \\Delta t} \\quad (\\text{in } \\mathrm{Hz})\n$$\nThe problem provides the sampling interval $\\Delta t = 0.005 \\mathrm{ s}$ and three DMD eigenvalues. We now process each one.\n\nFor Mode 1: $\\lambda_{1} = 1.02 \\exp(i\\,0.40)$.\nThe magnitude is $|\\lambda_1| = 1.02$ and the argument is $\\theta_1 = 0.40$ radians.\nThe growth rate is:\n$$\n\\sigma_1 = \\frac{\\ln(1.02)}{0.005} \\approx \\frac{0.0198026}{0.005} \\approx 3.96052 \\mathrm{ s}^{-1}\n$$\nRounded to four significant figures, $\\sigma_1 = 3.961 \\mathrm{ s}^{-1}$.\nSince $|\\lambda_1| = 1.02 > 1$, which corresponds to $\\sigma_1 > 0$, this mode is unstable. Its amplitude grows exponentially over time.\nThe frequency is:\n$$\nf_1 = \\frac{0.40}{2\\pi (0.005)} = \\frac{0.40}{0.01\\pi} \\approx 12.7324 \\mathrm{ Hz}\n$$\nRounded to four significant figures, $f_1 = 12.73 \\mathrm{ Hz}$.\nThe mode is oscillatory with a frequency of approximately $12.73 \\mathrm{ Hz}$.\n\nFor Mode 2: $\\lambda_{2} = 0.98 \\exp(i\\,2.30)$.\nThe magnitude is $|\\lambda_2| = 0.98$ and the argument is $\\theta_2 = 2.30$ radians.\nThe growth/decay rate is:\n$$\n\\sigma_2 = \\frac{\\ln(0.98)}{0.005} \\approx \\frac{-0.0202027}{0.005} \\approx -4.04054 \\mathrm{ s}^{-1}\n$$\nRounded to four significant figures, $\\sigma_2 = -4.041 \\mathrm{ s}^{-1}$.\nSince $|\\lambda_2| = 0.98 < 1$, which corresponds to $\\sigma_2 < 0$, this mode is stable and damped. Its amplitude decays exponentially over time.\nThe frequency is:\n$$\nf_2 = \\frac{2.30}{2\\pi (0.005)} = \\frac{2.30}{0.01\\pi} \\approx 73.2051 \\mathrm{ Hz}\n$$\nRounded to four significant figures, $f_2 = 73.21 \\mathrm{ Hz}$.\nThe mode is oscillatory with a frequency of approximately $73.21 \\mathrm{ Hz}$.\n\nFor Mode 3: $\\lambda_{3} = 0.995 \\exp(-i\\,0.05)$.\nThe magnitude is $|\\lambda_3| = 0.995$ and the argument is $\\theta_3 = -0.05$ radians.\nThe growth/decay rate is:\n$$\n\\sigma_3 = \\frac{\\ln(0.995)}{0.005} \\approx \\frac{-0.0050125}{0.005} \\approx -1.0025 \\mathrm{ s}^{-1}\n$$\nRounded to four significant figures, $\\sigma_3 = -1.003 \\mathrm{ s}^{-1}$.\nSince $|\\lambda_3| = 0.995 < 1$, which corresponds to $\\sigma_3 < 0$, this mode is also stable and damped, though its decay is slower than that of Mode 2.\nThe frequency is:\n$$\nf_3 = \\frac{-0.05}{2\\pi (0.005)} = \\frac{-0.05}{0.01\\pi} \\approx -1.59155 \\mathrm{ Hz}\n$$\nRounded to four significant figures, $f_3 = -1.592 \\mathrm{ Hz}$.\nThe mode is oscillatory with a signed frequency of approximately $-1.592 \\mathrm{ Hz}$. The negative sign indicates the direction of rotation in the complex plane, which can be related to the direction of propagation of the modal structure in physical space.\n\nThe six requested values are $\\sigma_1$, $f_1$, $\\sigma_2$, $f_2$, $\\sigma_3$, and $f_3$.\n$\\sigma_1 = 3.961 \\mathrm{ s}^{-1}$\n$f_1 = 12.73 \\mathrm{ Hz}$\n$\\sigma_2 = -4.041 \\mathrm{ s}^{-1}$\n$f_2 = 73.21 \\mathrm{ Hz}$\n$\\sigma_3 = -1.003 \\mathrm{ s}^{-1}$\n$f_3 = -1.592 \\mathrm{ Hz}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3.961 & 12.73 & -4.041 & 73.21 & -1.003 & -1.592 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the theory of DMD provides a clean connection between system dynamics and eigenvalues, applying it to finite and noisy data introduces practical challenges that can bias the results. The finite observation window and inherent signal damping can systematically alter the eigenvalue estimates, affecting the accuracy of stability and frequency predictions. This hands-on coding exercise  guides you through quantifying this bias and implementing correction techniques, such as exponential windowing and tapering, to improve the fidelity of your DMD analysis.",
            "id": "4054938",
            "problem": "A single thermoacoustic mode in a combustor can be locally approximated by a linear ordinary differential equation for a scalar observable amplitude $q(t)$, $t \\in \\mathbb{R}^{+}$, satisfying $dq/dt = \\mu q$ with continuous-time eigenvalue $\\mu = \\sigma + i \\omega$, where $\\sigma \\in \\mathbb{R}$ is the growth rate (negative for damping) in $\\mathrm{s}^{-1}$ and $\\omega \\in \\mathbb{R}^{+}$ is the angular frequency in $\\mathrm{rad/s}$. When sampled at uniform time step $\\Delta t$ to produce $N$ samples $q_k = q(k \\Delta t)$, $k = 0, 1, \\dots, N-1$, the discrete-time eigenvalue is $\\lambda = e^{\\mu \\Delta t}$. Dynamic Mode Decomposition (DMD) relies on a linear regression between consecutive snapshots to estimate $\\lambda$ and recover $\\mu$ via the complex logarithm. In finite time windows, boundary effects and amplitude modulation due to damping bias the DMD eigenvalue estimate. Proper Orthogonal Decomposition (POD) using the singular value decomposition is frequently used inside DMD to project data onto a low-dimensional subspace.\n\nYour task is to implement a program that, for a set of specified test cases, generates a damped oscillatory signal, constructs a Hankel time-delay embedding to form snapshot matrices, computes the DMD eigenvalue and its continuous-time counterpart, quantifies the bias induced by a finite window, and demonstrates bias reduction using either exponential windowing or tapering. The signal to be analyzed is the real-valued scalar series\n$$\ns_k = e^{\\sigma t_k} \\cos(\\omega t_k) + \\eta \\, \\xi_k,\n$$\nwhere $t_k = k \\Delta t$, $\\eta \\in \\mathbb{R}^{+}$ is a noise amplitude coefficient, and $\\xi_k$ are independent samples from a zero-mean unit-variance Gaussian distribution. Use a pseudorandom generator with fixed seed $42$ to ensure reproducibility. The Dynamic Mode Decomposition (DMD) should be performed via a standard projection-based approach that first computes the singular value decomposition of the snapshot matrix constructed from a Hankel time-delay embedding, which serves as the Proper Orthogonal Decomposition (POD) step.\n\nImplement the following steps for each test case:\n- Generate $N$ samples of $s_k$ at time step $\\Delta t$ using the parameters $(\\sigma,\\omega,\\Delta t,N,\\eta)$.\n- Construct a Hankel time-delay embedding of dimension $d$ with $d = \\min(25, \\max(10, \\lfloor N/5 \\rfloor))$ to build snapshot matrices $X \\in \\mathbb{R}^{d \\times m}$ and $Y \\in \\mathbb{R}^{d \\times m}$, with $m = N - d$, where the $j$-th column of $X$ is $[s_j, s_{j+1}, \\dots, s_{j+d-1}]^{\\top}$ and the $j$-th column of $Y$ is $[s_{j+1}, s_{j+2}, \\dots, s_{j+d}]^{\\top}$ for $j = 0, 1, \\dots, m-1$.\n- Compute the singular value decomposition $X \\approx U \\Sigma V^{\\ast}$, form the reduced operator $\\tilde{A} = U^{\\ast} Y V \\Sigma^{-1}$, and obtain its eigenvalues. Select the eigenvalue $\\lambda$ with positive imaginary part and largest magnitude among those; if none has positive imaginary part, select the eigenvalue with largest magnitude.\n- Convert the selected discrete-time eigenvalue to continuous time via $\\hat{\\mu} = \\log(\\lambda)/\\Delta t$ using the principal branch of the complex logarithm.\n- Compute the naive bias components $b_{\\mathrm{naive},\\sigma} = |\\operatorname{Re}(\\hat{\\mu}) - \\sigma|$ in $\\mathrm{s}^{-1}$ and $b_{\\mathrm{naive},\\omega} = |\\operatorname{Im}(\\hat{\\mu}) - \\omega|$ in $\\mathrm{rad/s}$.\n- Exponential windowing correction: define $\\alpha = -\\sigma$, construct $s^{(\\exp)}_k = e^{\\alpha t_k} s_k$, recompute $\\hat{\\mu}^{(\\exp)}$ via DMD on $s^{(\\exp)}_k$, and correct back by $\\hat{\\mu}_{\\mathrm{corr}}^{(\\exp)} = \\hat{\\mu}^{(\\exp)} - \\alpha$. Compute $b_{\\exp,\\sigma} = |\\operatorname{Re}(\\hat{\\mu}_{\\mathrm{corr}}^{(\\exp)}) - \\sigma|$ in $\\mathrm{s}^{-1}$ and $b_{\\exp,\\omega} = |\\operatorname{Im}(\\hat{\\mu}_{\\mathrm{corr}}^{(\\exp)}) - \\omega|$ in $\\mathrm{rad/s}$.\n- Tapering correction: construct the Hann window $w_k = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi k}{N-1}\\right)\\right)$, compute $s^{(\\mathrm{tap})}_k = w_k s_k$, recompute $\\hat{\\mu}^{(\\mathrm{tap})}$ via DMD on $s^{(\\mathrm{tap})}_k$, and set $\\hat{\\mu}_{\\mathrm{corr}}^{(\\mathrm{tap})} = \\hat{\\mu}^{(\\mathrm{tap})}$. Compute $b_{\\mathrm{tap},\\sigma} = |\\operatorname{Re}(\\hat{\\mu}_{\\mathrm{corr}}^{(\\mathrm{tap})}) - \\sigma|$ in $\\mathrm{s}^{-1}$ and $b_{\\mathrm{tap},\\omega} = |\\operatorname{Im}(\\hat{\\mu}_{\\mathrm{corr}}^{(\\mathrm{tap})}) - \\omega|$ in $\\mathrm{rad/s}$.\n\nUse the following test suite of parameters $(\\sigma,\\omega,\\Delta t,N,\\eta)$:\n- Case $1$: $(\\sigma,\\omega,\\Delta t,N,\\eta) = (-5.0, 200.0, 0.001, 500, 0.001)$.\n- Case $2$: $(\\sigma,\\omega,\\Delta t,N,\\eta) = (-5.0, 200.0, 0.001, 60, 0.001)$.\n- Case $3$: $(\\sigma,\\omega,\\Delta t,N,\\eta) = (-50.0, 200.0, 0.001, 300, 0.001)$.\n- Case $4$: $(\\sigma,\\omega,\\Delta t,N,\\eta) = (-1.0, 600.0, 0.005, 400, 0.001)$.\n- Case $5$: $(\\sigma,\\omega,\\Delta t,N,\\eta) = (-0.5, 120.0, 0.002, 150, 0.005)$.\n\nAngle measurements must be in radians. Express growth rate biases in $\\mathrm{s}^{-1}$ and frequency biases in $\\mathrm{rad/s}$. For each case, the program must produce a list of six floats $[b_{\\mathrm{naive},\\sigma}, b_{\\mathrm{naive},\\omega}, b_{\\exp,\\sigma}, b_{\\exp,\\omega}, b_{\\mathrm{tap},\\sigma}, b_{\\mathrm{tap},\\omega}]$, each rounded to six decimal places. Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, with each case represented as a list in the order above (for example, $[[x_1,x_2,\\dots,x_6],[y_1,\\dots,y_6],\\dots]$).",
            "solution": "The solution involves implementing a program that carries out the specified Dynamic Mode Decomposition (DMD) analysis for a series of test cases. For each case, the analysis is applied to three versions of a generated signal: the original (naive) signal, an exponentially windowed signal, and a Hann-tapered signal.\n\nThe core of the analysis is a function that performs projection-based DMD. For a given time series $s_k$, it executes the following steps:\n1.  **Time-delay embedding**: The scalar time series $s_k$ is embedded into a higher-dimensional space by constructing Hankel snapshot matrices. As specified, we form matrices $X \\in \\mathbb{R}^{d \\times m}$ and $Y \\in \\mathbb{R}^{d \\times m}$, where $d = \\min(25, \\max(10, \\lfloor N/5 \\rfloor))$ is the embedding dimension and $m = N-d$. The $j$-th column of $X$ is the vector $[s_j, s_{j+1}, \\dots, s_{j+d-1}]^{\\top}$, and the $j$-th column of $Y$ follows as $[s_{j+1}, s_{j+2}, \\dots, s_{j+d}]^{\\top}$. These matrices represent shifted sequences of state vectors, with $Y \\approx AX$ for some linear operator $A$.\n\n2.  **Proper Orthogonal Decomposition (POD)**: The Singular Value Decomposition (SVD) of the matrix $X$ is computed: $X \\approx U \\Sigma V^{\\ast}$. The matrix $U$ contains the Proper Orthogonal Modes (POD modes), which form an optimal orthonormal basis for the data in $X$. This step is used to project the dynamics onto a low-dimensional subspace.\n\n3.  **Low-rank Operator Estimation**: The high-dimensional operator $A$ is approximated by a low-rank operator $\\tilde{A}$ that evolves the POD mode coefficients. This reduced operator is computed via projection using the standard formula:\n    $$\n    \\tilde{A} = U^{\\ast} Y V \\Sigma^{-1}\n    $$\n    The eigenvalues of $\\tilde{A}$ approximate the eigenvalues of the full operator $A$.\n\n4.  **Eigenvalue Analysis**: The eigenvalues $\\lambda$ of $\\tilde{A}$ are computed. Since the underlying signal is a real-valued oscillation, we expect a pair of complex conjugate eigenvalues. Following the problem's selection rule, the eigenvalue with a positive imaginary part and the largest magnitude is selected to represent the physical mode.\n\n5.  **Continuous-time Eigenvalue Recovery**: The selected discrete-time eigenvalue $\\lambda$ is mapped back to its continuous-time counterpart $\\hat{\\mu}$ using the principal branch of the complex logarithm:\n    $$\n    \\hat{\\mu} = \\frac{\\log(\\lambda)}{\\Delta t}\n    $$\n    The real part of $\\hat{\\mu}$ is the estimated growth/damping rate $\\hat{\\sigma}$, and the imaginary part is the estimated angular frequency $\\hat{\\omega}$.\n\nFor each test case, this analysis is performed on three signals:\n-   **Naive Signal**: The raw signal $s_k = e^{\\sigma t_k} \\cos(\\omega t_k) + \\eta \\, \\xi_k$. The bias in the resulting $\\hat{\\mu}$ is due to spectral leakage from the abrupt start and end of the finite signal.\n-   **Exponentially Windowed Signal**: The signal is multiplied by $e^{-\\sigma t_k}$ to create a non-decaying signal, $s^{(\\exp)}_k$. This removes the amplitude modulation, making DMD's linear approximation more accurate. The resulting growth rate estimate is then corrected by adding back the known damping factor. This method is expected to significantly reduce bias in the growth rate estimate.\n-   **Tapered Signal**: The signal is multiplied by a Hann window, which smoothly brings the amplitude to zero at the boundaries to reduce spectral leakage. However, this alters the signal's envelope, which can be misinterpreted by DMD as stronger damping.\n\nFinally, the biases for the growth rate ($b_\\sigma = |\\hat{\\sigma}_{\\text{est}} - \\sigma|$) and frequency ($b_\\omega = |\\hat{\\omega}_{\\text{est}} - \\omega|$) are computed for all three approaches. A fixed random seed ensures the reproducibility of the noise component and the entire calculation.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import hankel\n\ndef run_dmd(s: np.ndarray, dt: float) -> complex:\n    \"\"\"\n    Performs Dynamic Mode Decomposition on a time series.\n\n    Args:\n        s: The input time series (1D numpy array).\n        dt: The time step between samples.\n\n    Returns:\n        The estimated continuous-time eigenvalue mu_hat.\n    \"\"\"\n    N = len(s)\n    \n    # Calculate embedding dimension d\n    d = int(min(25, max(10, np.floor(N / 5))))\n    m = N - d\n\n    # Construct Hankel matrices X and Y\n    # X columns are [s_j, ..., s_{j+d-1}] for j=0..m-1\n    # Y columns are [s_{j+1}, ..., s_{j+d}] for j=0..m-1\n    X = hankel(s[0:d], s[d - 1 : N - 1])\n    Y = hankel(s[1 : d + 1], s[d:N])\n\n    # Compute SVD of X (POD step)\n    try:\n        U, s_vals, Vt = np.linalg.svd(X, full_matrices=False)\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD fails, though unlikely with noisy data\n        return complex(np.nan, np.nan)\n\n    # Compute the pseudo-inverse of the singular values matrix\n    # Avoid division by zero for any singular values that are numerically zero\n    s_vals_inv = np.zeros_like(s_vals)\n    # A small tolerance to identify non-zero singular values\n    tol = np.finfo(float).eps * max(d, m) * s_vals[0]\n    non_zero_indices = s_vals > tol\n    s_vals_inv[non_zero_indices] = 1.0 / s_vals[non_zero_indices]\n    Sigma_inv = np.diag(s_vals_inv)\n    \n    V = Vt.T\n\n    # Compute the reduced operator Atilde\n    Atilde = U.T @ Y @ V @ Sigma_inv\n\n    # Eigenvalues of the reduced operator\n    dmd_eigvals = np.linalg.eigvals(Atilde)\n\n    # Select the eigenvalue according to the specified rule\n    pos_imag_eigvals = dmd_eigvals[np.imag(dmd_eigvals) > 0]\n    \n    if len(pos_imag_eigvals) > 0:\n        # Among eigenvalues with positive imaginary part, find the one with largest magnitude\n        magnitudes = np.abs(pos_imag_eigvals)\n        selected_lambda = pos_imag_eigvals[np.argmax(magnitudes)]\n    else:\n        # If none have positive imaginary part, find the one with largest magnitude overall\n        magnitudes = np.abs(dmd_eigvals)\n        selected_lambda = dmd_eigvals[np.argmax(magnitudes)]\n    \n    # Convert discrete-time eigenvalue to continuous-time\n    mu_hat = np.log(selected_lambda) / dt\n    \n    return mu_hat\n\ndef process_case(sigma: float, omega: float, dt: float, N: int, eta: float, rng: np.random.Generator) -> list[float]:\n    \"\"\"\n    Processes a single test case according to the problem description.\n\n    Args:\n        sigma, omega, dt, N, eta: Parameters for the test case.\n        rng: A numpy random number generator instance.\n\n    Returns:\n        A list of six bias values, rounded to six decimal places.\n    \"\"\"\n    # Generate signal\n    t = np.arange(N) * dt\n    noise = eta * rng.normal(size=N)\n    s_naive = np.exp(sigma * t) * np.cos(omega * t) + noise\n\n    # 1. Naive DMD\n    mu_hat_naive = run_dmd(s_naive, dt)\n    b_naive_sigma = np.abs(mu_hat_naive.real - sigma)\n    b_naive_omega = np.abs(mu_hat_naive.imag - omega)\n\n    # 2. Exponential Windowing Correction\n    alpha = -sigma\n    s_exp = np.exp(alpha * t) * s_naive\n    mu_hat_exp = run_dmd(s_exp, dt)\n    mu_hat_corr_exp = mu_hat_exp - alpha\n    b_exp_sigma = np.abs(mu_hat_corr_exp.real - sigma)\n    b_exp_omega = np.abs(mu_hat_corr_exp.imag - omega)\n\n    # 3. Tapering Correction\n    w_hann = 0.5 * (1 - np.cos(2 * np.pi * np.arange(N) / (N - 1)))\n    s_tap = w_hann * s_naive\n    mu_hat_tap = run_dmd(s_tap, dt)\n    # The problem specifies mu_hat_corr_tap = mu_hat_tap\n    b_tap_sigma = np.abs(mu_hat_tap.real - sigma)\n    b_tap_omega = np.abs(mu_hat_tap.imag - omega)\n\n    return [\n        round(b_naive_sigma, 6), round(b_naive_omega, 6),\n        round(b_exp_sigma, 6), round(b_exp_omega, 6),\n        round(b_tap_sigma, 6), round(b_tap_omega, 6)\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Use a fixed seed for reproducibility.\n    rng = np.random.default_rng(42)\n\n    # Test cases as defined in the problem\n    test_cases = [\n        (-5.0, 200.0, 0.001, 500, 0.001), # Case 1\n        (-5.0, 200.0, 0.001, 60, 0.001),  # Case 2\n        (-50.0, 200.0, 0.001, 300, 0.001), # Case 3\n        (-1.0, 600.0, 0.005, 400, 0.001), # Case 4\n        (-0.5, 120.0, 0.002, 150, 0.005)  # Case 5\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        sigma, omega, dt, N, eta = case_params\n        case_result = process_case(sigma, omega, dt, N, eta, rng)\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required\n    case_strings = []\n    for result_case in all_results:\n        num_strings = [f\"{x:.6f}\" for x in result_case]\n        case_strings.append(f\"[{','.join(num_strings)}]\")\n    \n    final_output_string = f\"[{','.join(case_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}