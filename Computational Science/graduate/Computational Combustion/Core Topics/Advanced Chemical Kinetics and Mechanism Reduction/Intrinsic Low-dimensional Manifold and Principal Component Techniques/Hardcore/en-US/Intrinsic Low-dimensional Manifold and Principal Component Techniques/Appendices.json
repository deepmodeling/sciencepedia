{
    "hands_on_practices": [
        {
            "introduction": "Before applying any dimensionality reduction technique, it's crucial to understand the intrinsic constraints that govern a chemical system's composition. This foundational exercise explores how the law of conservation of elements imposes a set of linear constraints on the species mass fractions, confining all possible states to a lower-dimensional affine subspace . By working through this derivation, you will quantify the dimension of this accessible state space, providing the theoretical justification for why complex chemical systems can be described by a handful of variables.",
            "id": "4032863",
            "problem": "Consider a chemically reacting mixture relevant to computational combustion with $N_s$ chemical species and $M$ chemical elements. Let $Y_i$ denote the species mass fraction of species $i$ for $i \\in \\{1,\\dots,N_s\\}$, and let $a_{\\alpha i}$ denote the number of atoms of element $\\alpha$ in species $i$ for $\\alpha \\in \\{1,\\dots,M\\}$. Let $W_{\\alpha}$ be the atomic weight of element $\\alpha$ and $W_i$ be the molecular weight of species $i$. Suppose the mixture is well-mixed and closed, so only chemical reactions redistribute mass among species while conserving the mass of each element.\n\nStarting from the physical definition that the mass of an individual species is the sum of the masses of its constituent elements and using the conservation of each elementâ€™s total mass in the mixture, derive the linear elemental conservation constraints on the species mass fractions in the form $E Y = b$, where $Y \\in \\mathbb{R}^{N_s}$ collects $(Y_1,\\dots,Y_{N_s})$, $b \\in \\mathbb{R}^{M}$ collects the element mass fractions $(b_1,\\dots,b_M)$, and $E \\in \\mathbb{R}^{M \\times N_s}$ is determined by stoichiometry and atomic/molecular weights. Show that the normalization $\\sum_{i=1}^{N_s} Y_i = 1$ follows from the elemental mass fraction definition and is therefore not an independent additional constraint when $b$ represents element mass fractions in the mixture. Then, after imposing the nonnegativity $Y_i \\ge 0$ for all $i$, quantify the dimension of the admissible composition space of $Y$ consistent with the given $b$ in terms of $N_s$ and the rank $r$ of $E$.\n\nYour final answer should be a single closed-form analytic expression in terms of $N_s$ and $r$. No numerical evaluation is required. This dimensionality underpins reduced composition models used in Intrinsic Low-Dimensional Manifolds (ILDM) and informs basis selection in Principal Component Analysis (PCA) for combustion kinetics.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and contains sufficient information for a rigorous solution.\n\n**Step 1: Extract Givens**\n- $N_s$: Number of chemical species.\n- $M$: Number of chemical elements.\n- $Y_i$: Mass fraction of species $i$, for $i \\in \\{1, \\dots, N_s\\}$.\n- $a_{\\alpha i}$: Number of atoms of element $\\alpha$ in a molecule of species $i$, for $\\alpha \\in \\{1, \\dots, M\\}$.\n- $W_{\\alpha}$: Atomic weight of element $\\alpha$.\n- $W_i$: Molecular weight of species $i$.\n- The system is a closed, well-mixed chemical mixture.\n- The total mass of each element is conserved.\n- The final state vector $Y$ must satisfy $Y_i \\ge 0$ for all $i$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically sound, as it is based on the fundamental principle of mass conservation in chemical systems. The terminology and variables are standard in chemistry, chemical engineering, and combustion theory. The task is to derive a standard set of linear constraints and then determine the dimension of the resulting solution space, which is a well-posed mathematical question with direct relevance to computational combustion topics like Intrinsic Low-Dimensional Manifolds (ILDM) and Principal Component Analysis (PCA). The problem is objective, self-contained, and free of contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n**Derivation of the Elemental Conservation Constraints**\n\nLet us begin by establishing the relationship between the quantities provided. The molecular weight of species $i$, $W_i$, is the sum of the masses of its constituent atoms. This is expressed as:\n$$\nW_i = \\sum_{\\alpha=1}^{M} a_{\\alpha i} W_{\\alpha}\n$$\nThe mass fraction of element $\\alpha$ within a molecule of species $i$ is the ratio of the total mass of element $\\alpha$ in the molecule to the total mass of the molecule. This is given by $\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}$.\n\nIn a mixture with total mass $m_{total}$, the mass of species $i$ is $m_i = Y_i m_{total}$. The mass of element $\\alpha$ contributed by species $i$ is the mass of species $i$ multiplied by the mass fraction of element $\\alpha$ in that species:\n$$\nm_{\\alpha,i} = \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) m_i = \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i m_{total}\n$$\nThe total mass of element $\\alpha$ in the mixture, $m_{\\alpha}$, is obtained by summing the contributions from all $N_s$ species:\n$$\nm_{\\alpha} = \\sum_{i=1}^{N_s} m_{\\alpha,i} = \\sum_{i=1}^{N_s} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i m_{total}\n$$\nThe element mass fraction of element $\\alpha$ in the mixture, denoted $b_{\\alpha}$, is defined as the total mass of element $\\alpha$ divided by the total mass of the mixture, $b_{\\alpha} = \\frac{m_{\\alpha}}{m_{total}}$. Substituting the expression for $m_{\\alpha}$:\n$$\nb_{\\alpha} = \\frac{1}{m_{total}} \\left( \\sum_{i=1}^{N_s} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i m_{total} \\right) = \\sum_{i=1}^{N_s} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i\n$$\nThis represents a single linear equation for each element $\\alpha \\in \\{1, \\dots, M\\}$. We can assemble these $M$ equations into a single matrix-vector equation $E Y = b$. The vector of species mass fractions is $Y = [Y_1, Y_2, \\dots, Y_{N_s}]^T \\in \\mathbb{R}^{N_s}$. The vector of element mass fractions is $b = [b_1, b_2, \\dots, b_M]^T \\in \\mathbb{R}^{M}$. The matrix $E$ is an $M \\times N_s$ matrix whose entries are the coefficients of the $Y_i$ terms:\n$$\nE_{\\alpha i} = \\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\n$$\nThus, we have derived the system of linear elemental conservation constraints $E Y = b$.\n\n**Redundancy of the Normalization Constraint**\n\nThe problem next asks to show that the normalization of species mass fractions, $\\sum_{i=1}^{N_s} Y_i = 1$, is not an independent constraint. By definition, the sum of all element mass fractions in the mixture must be unity:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = 1\n$$\nLet us sum the $M$ linear equations in the system $E Y = b$ over the index $\\alpha$:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = \\sum_{\\alpha=1}^{M} \\left( \\sum_{i=1}^{N_s} E_{\\alpha i} Y_i \\right)\n$$\nSubstituting the expression for $E_{\\alpha i}$ and swapping the order of summation:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = \\sum_{i=1}^{N_s} \\sum_{\\alpha=1}^{M} \\left(\\frac{a_{\\alpha i} W_{\\alpha}}{W_i}\\right) Y_i = \\sum_{i=1}^{N_s} Y_i \\left( \\frac{1}{W_i} \\sum_{\\alpha=1}^{M} a_{\\alpha i} W_{\\alpha} \\right)\n$$\nFrom the definition of molecular weight, the term in the parenthesis is $\\frac{1}{W_i} (W_i) = 1$. Therefore, the equation simplifies to:\n$$\n\\sum_{\\alpha=1}^{M} b_{\\alpha} = \\sum_{i=1}^{N_s} Y_i\n$$\nSince $\\sum_{\\alpha=1}^{M} b_{\\alpha} = 1$, it follows directly that $\\sum_{i=1}^{N_s} Y_i = 1$. This demonstrates that the species mass fraction normalization is a linear combination of the elemental conservation equations. It is therefore a redundant constraint, and the rows of the matrix $E$ are linearly dependent. This implies that the rank of $E$, denoted by $r$, is strictly less than $M$.\n\n**Dimensionality of the Admissible Composition Space**\n\nThe admissible composition space is the set of all vectors $Y$ that satisfy the physical constraints of the system. This set, which we can call $\\mathcal{A}$, is defined as:\n$$\n\\mathcal{A} = \\{ Y \\in \\mathbb{R}^{N_s} \\mid E Y = b \\text{ and } Y_i \\ge 0 \\text{ for } i=1, \\dots, N_s \\}\n$$\nThe species composition vector $Y$ lives in the $N_s$-dimensional space $\\mathbb{R}^{N_s}$. The equation $E Y = b$ represents a system of linear equations. The set of all solutions to this system forms an affine subspace of $\\mathbb{R}^{N_s}$. Let $r = \\text{rank}(E)$ be the number of linearly independent elemental conservation constraints. Each independent linear constraint reduces the dimension of the solution space by one.\nTherefore, the dimension of the affine subspace defined by $E Y = b$ is the initial dimension of the ambient space, $N_s$, minus the number of independent constraints, $r$.\n\nThe dimension of the affine subspace of solutions to $E Y = b$ is given by $d = N_s - r$. This result can also be seen from the Rank-Nullity Theorem, which states that for the linear map represented by $E: \\mathbb{R}^{N_s} \\to \\mathbb{R}^{M}$, the dimension of the domain is the sum of the rank and the dimension of the kernel (null space): $\\dim(\\mathbb{R}^{N_s}) = \\text{rank}(E) + \\dim(\\ker(E))$. The dimension of the kernel, $\\dim(\\ker(E))$, which is the dimension of the solution space to the homogeneous equation $EY=0$, is $N_s - r$. The solution space for the inhomogeneous equation $EY=b$ is a translation of this kernel, and thus has the same dimension, $N_s - r$.\n\nThe additional constraints $Y_i \\ge 0$ for all $i$ restrict the solution space to the non-negative orthant of $\\mathbb{R}^{N_s}$. The intersection of the affine subspace with this orthant forms a convex set (specifically, a convex polytope, as it is also bounded since $\\sum Y_i = 1$). The dimension of a convex set is defined as the dimension of its affine hull. Assuming that the mixture composition $b$ is physically realizable, the set $\\mathcal{A}$ is non-empty. In any realistic scenario, $\\mathcal{A}$ is not a single point but a continuous region. The affine hull of $\\mathcal{A}$ is the affine subspace defined by $EY=b$ itself. Therefore, the non-negativity constraints define the boundaries (facets) of the accessible region but do not alter its intrinsic dimension.\n\nThe dimension of the admissible composition space is thus the dimension of the affine subspace defined by the independent elemental conservation equations, which is $N_s - r$.",
            "answer": "$$\\boxed{N_s - r}$$"
        },
        {
            "introduction": "Once a low-dimensional manifold is constructed, a key challenge is integrating it into a larger simulation, such as a Computational Fluid Dynamics (CFD) code. This practical exercise demonstrates how to project a high-dimensional state vector from a simulation onto the manifold, a necessary step to enforce the reduced model's constraints . You will solve a constrained least-squares problem to find the optimal low-dimensional coordinates, gaining direct experience with the mechanics of coupling reduced models with transport solvers.",
            "id": "4032855",
            "problem": "A tabulated intrinsic low-dimensional manifold (ILDM) for a reacting mixture is constructed by sampling quasi-steady compositions and thermochemical states and then applying Principal Component Analysis (PCA) to obtain an orthonormal basis of dominant directions. At a given cell in a Computational Fluid Dynamics (CFD) calculation, a high-dimensional state vector is denoted by $y^{\\ast} \\in \\mathbb{R}^{n}$. The manifold is parameterized in a local affine form $y(c) = y_{0} + U c$, where $y_{0} \\in \\mathbb{R}^{n}$ is the local mean state, $U \\in \\mathbb{R}^{n \\times r}$ contains $r$ principal directions, and $c \\in \\mathbb{R}^{r}$ are reduced coordinates. To compare states with disparate physical units, a weighted Euclidean metric is defined by a symmetric positive-definite weight $W \\in \\mathbb{R}^{n \\times n}$, and the projection is formulated as a constrained least-squares problem that enforces exact invariants (e.g., mixture fraction) via linear equality constraints.\n\nStarting from the foundational modeling assumptions that (i) the closest-point projection in the metric induced by $W$ minimizes the squared weighted norm $\\|W\\,(y_{0} + U c - y^{\\ast})\\|_{2}^{2}$, and (ii) invariants such as mixture fraction are preserved exactly across fast relaxation to the ILDM and are represented as linear constraints of the form $C\\,(y_{0} + U c) = d$, derive and then compute the optimal reduced coordinates for the following specific instance.\n\nLet the state vector be ordered as $y = [Y_{1},\\,Y_{2},\\,\\tilde{h},\\,Z]^{\\top}$, where $Y_{1}$ and $Y_{2}$ are the mass fractions of two lumped species (dimensionless), $\\tilde{h}$ is a nondimensionalized sensible enthalpy, and $Z$ is the mixture fraction. The PCA-derived manifold representation is\n$$\ny_{0} \\;=\\;\n\\begin{pmatrix}\n0.5 \\\\\n0.5 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n\\qquad\nU \\;=\\;\n\\begin{pmatrix}\n0.6  0 \\\\\n-0.6  0 \\\\\n0  1 \\\\\n0.2  0.5\n\\end{pmatrix},\n\\qquad\nW \\;=\\; I_{4},\n$$\nso that the affine subspace $y(c) = y_{0} + U c$ automatically enforces $Y_{1} + Y_{2} = 1$. The CFD state to be projected is\n$$\ny^{\\ast} \\;=\\;\n\\begin{pmatrix}\n0.7 \\\\\n0.3 \\\\\n0.4 \\\\\n0.25\n\\end{pmatrix}.\n$$\nEnforce the exact invariance of the mixture fraction by the linear constraint\n$$\nC\\,(y_{0} + U c) \\;=\\; d,\n\\qquad\nC \\;=\\;\n\\begin{pmatrix}\n0  0  0  1\n\\end{pmatrix},\n\\qquad\nd \\;=\\; Z^{\\ast} \\;=\\; 0.25,\n$$\nand determine the optimal reduced coordinates $c^{\\star} \\in \\mathbb{R}^{2}$ that solve\n$$\n\\min_{c \\in \\mathbb{R}^{2}} \\;\\big\\|W\\,(y_{0} + U c - y^{\\ast})\\big\\|_{2}^{2}\n\\quad\\text{subject to}\\quad\nC\\,(y_{0} + U c) = d.\n$$\nExpress your final answer as the row vector $\\big(c_{1}^{\\star},\\,c_{2}^{\\star}\\big)$ in exact form. No rounding is required and no units are needed.",
            "solution": "The problem asks for the optimal reduced coordinates $c^{\\star} \\in \\mathbb{R}^{2}$ that solve the constrained minimization problem:\n$$\n\\min_{c \\in \\mathbb{R}^{2}} \\;\\big\\|W\\,(y_{0} + U c - y^{\\ast})\\big\\|_{2}^{2}\n\\quad\\text{subject to}\\quad\nC\\,(y_{0} + U c) = d.\n$$\nThis is a quadratic programming problem with linear equality constraints. A standard method for solving such problems is the method of Lagrange multipliers.\n\nFirst, let us define the objective function $\\mathcal{J}(c)$ and the constraint.\nThe objective function is $\\mathcal{J}(c) = \\|W(y(c) - y^{\\ast})\\|_{2}^{2}$, where $y(c) = y_0 + U c$.\nLet $\\Delta y = y^{\\ast} - y_0$. The expression inside the norm is $Uc - \\Delta y$.\nThe objective function becomes $\\mathcal{J}(c) = \\|W(Uc - \\Delta y)\\|_{2}^{2}$.\nUsing the definition of the squared Euclidean norm, this is $\\mathcal{J}(c) = (Uc - \\Delta y)^{\\top}W^{\\top}W(Uc - \\Delta y)$.\nThe problem provides $W = I_{4}$, the $4 \\times 4$ identity matrix. Thus, $W^{\\top}W = I_{4}^{\\top}I_{4} = I_{4}$. The objective function simplifies to the standard (unweighted) least-squares form:\n$$\n\\mathcal{J}(c) = \\|Uc - \\Delta y\\|_{2}^{2} = (Uc - \\Delta y)^{\\top}(Uc - \\Delta y) = c^{\\top}U^{\\top}Uc - 2c^{\\top}U^{\\top}\\Delta y + \\Delta y^{\\top}\\Delta y.\n$$\nThe linear constraint is $C(y_0 + Uc) = d$, which can be rearranged as:\n$$\nCUc - (d - Cy_0) = 0.\n$$\nWe form the Lagrangian function $\\Lambda(c, \\lambda)$ by adding the constraint multiplied by a Lagrange multiplier $\\lambda \\in \\mathbb{R}^{1}$ (since there is one scalar constraint):\n$$\n\\Lambda(c, \\lambda) = \\mathcal{J}(c) + \\lambda \\big(CUc - (d - Cy_0)\\big).\n$$\nThe optimal coordinates $c^{\\star}$ must satisfy the Karush-Kuhn-Tucker (KKT) conditions. For this problem, they are found by setting the gradients of the Lagrangian with respect to $c$ and $\\lambda$ to zero.\n\nThe gradient with respect to $c$ is:\n$$\n\\nabla_{c} \\Lambda(c, \\lambda) = \\nabla_{c} \\left( c^{\\top}U^{\\top}Uc - 2c^{\\top}U^{\\top}\\Delta y + \\Delta y^{\\top}\\Delta y + \\lambda(CUc - (d - Cy_0)) \\right) = 0.\n$$\nUsing standard matrix calculus identities, we get:\n$$\n2U^{\\top}Uc - 2U^{\\top}\\Delta y + \\lambda(CU)^{\\top} = 0\n\\implies 2U^{\\top}Uc - 2U^{\\top}\\Delta y + \\lambda U^{\\top}C^{\\top} = 0.\n$$\nDividing by $2$ and letting $\\mu = \\lambda/2$, we obtain the first equation:\n$$\n(U^{\\top}U)c + \\mu U^{\\top}C^{\\top} = U^{\\top}\\Delta y.\n$$\nThe derivative with respect to $\\lambda$ simply recovers the constraint:\n$$\n\\frac{\\partial \\Lambda}{\\partial \\lambda} = CUc - (d - Cy_0) = 0 \\implies (CU)c = d - Cy_0.\n$$\nThese two equations form a system of linear equations for $c$ and $\\mu$:\n$$\n\\begin{pmatrix} U^{\\top}U  U^{\\top}C^{\\top} \\\\ CU  0 \\end{pmatrix} \\begin{pmatrix} c \\\\ \\mu \\end{pmatrix} = \\begin{pmatrix} U^{\\top}\\Delta y \\\\ d - Cy_0 \\end{pmatrix}.\n$$\nNow we compute the specific matrices and vectors for the given problem.\nThe given data are:\n$$\ny_{0} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\\nU = \\begin{pmatrix} 0.6  0 \\\\ -0.6  0 \\\\ 0  1 \\\\ 0.2  0.5 \\end{pmatrix}, \\\ny^{\\ast} = \\begin{pmatrix} 0.7 \\\\ 0.3 \\\\ 0.4 \\\\ 0.25 \\end{pmatrix}, \\\nC = \\begin{pmatrix} 0  0  0  1 \\end{pmatrix}, \\\nd = 0.25.\n$$\nFirst, calculate the vector $\\Delta y = y^{\\ast} - y_0$:\n$$\n\\Delta y = \\begin{pmatrix} 0.7 - 0.5 \\\\ 0.3 - 0.5 \\\\ 0.4 - 0 \\\\ 0.25 - 0 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ -0.2 \\\\ 0.4 \\\\ 0.25 \\end{pmatrix}.\n$$\nNext, we compute the components of the block matrix system.\n$$\nU^{\\top}U = \\begin{pmatrix} 0.6  -0.6  0  0.2 \\\\ 0  0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 0.6  0 \\\\ -0.6  0 \\\\ 0  1 \\\\ 0.2  0.5 \\end{pmatrix} = \\begin{pmatrix} 0.36 + 0.36 + 0.04  0.1 \\\\ 0.1  1 + 0.25 \\end{pmatrix} = \\begin{pmatrix} 0.76  0.1 \\\\ 0.1  1.25 \\end{pmatrix}.\n$$\n$$\nU^{\\top}C^{\\top} = \\begin{pmatrix} 0.6  -0.6  0  0.2 \\\\ 0  0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.2 \\\\ 0.5 \\end{pmatrix}.\n$$\n$$\nCU = \\begin{pmatrix} 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0.6  0 \\\\ -0.6  0 \\\\ 0  1 \\\\ 0.2  0.5 \\end{pmatrix} = \\begin{pmatrix} 0.2  0.5 \\end{pmatrix}.\n$$\n$$\nU^{\\top}\\Delta y = \\begin{pmatrix} 0.6  -0.6  0  0.2 \\\\ 0  0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ -0.2 \\\\ 0.4 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} 0.12 + 0.12 + 0.05 \\\\ 0.4 + 0.125 \\end{pmatrix} = \\begin{pmatrix} 0.29 \\\\ 0.525 \\end{pmatrix}.\n$$\n$$\nCy_0 = \\begin{pmatrix} 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0.\n$$\n$$\nd - Cy_0 = 0.25 - 0 = 0.25.\n$$\nLet $c = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$. The system of equations is:\n$$\n\\begin{pmatrix} 0.76  0.1  0.2 \\\\ 0.1  1.25  0.5 \\\\ 0.2  0.5  0 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\\\ \\mu \\end{pmatrix} = \\begin{pmatrix} 0.29 \\\\ 0.525 \\\\ 0.25 \\end{pmatrix}.\n$$\nThis represents the following three linear equations:\n1. $0.76 c_1 + 0.1 c_2 + 0.2 \\mu = 0.29$\n2. $0.1 c_1 + 1.25 c_2 + 0.5 \\mu = 0.525$\n3. $0.2 c_1 + 0.5 c_2 = 0.25$\n\nTo maintain precision, we convert to fractions: $0.76=\\frac{19}{25}$, $0.1=\\frac{1}{10}$, $0.2=\\frac{1}{5}$, $1.25=\\frac{5}{4}$, $0.5=\\frac{1}{2}$, $0.29=\\frac{29}{100}$, $0.525=\\frac{21}{40}$, $0.25=\\frac{1}{4}$.\nThe system is:\n1. $\\frac{19}{25} c_1 + \\frac{1}{10} c_2 + \\frac{1}{5} \\mu = \\frac{29}{100}$\n2. $\\frac{1}{10} c_1 + \\frac{5}{4} c_2 + \\frac{1}{2} \\mu = \\frac{21}{40}$\n3. $\\frac{1}{5} c_1 + \\frac{1}{2} c_2 = \\frac{1}{4}$\n\nFrom equation (3), we can solve for $c_2$ in terms of $c_1$ (or vice versa). Multiplying by $10$ gives $2c_1 + 5c_2 = 2.5$, or $4c_1 + 10c_2 = 5$.\nLet's eliminate $\\mu$. Multiply equation (1) by $5$ and equation (2) by $2$:\n$5 \\times (1): \\frac{19}{5} c_1 + \\frac{1}{2} c_2 + \\mu = \\frac{29}{20}$\n$2 \\times (2): \\frac{1}{5} c_1 + \\frac{5}{2} c_2 + \\mu = \\frac{21}{20}$\nSubtracting the second new equation from the first:\n$$\n\\left(\\frac{19}{5} - \\frac{1}{5}\\right) c_1 + \\left(\\frac{1}{2} - \\frac{5}{2}\\right) c_2 = \\frac{29}{20} - \\frac{21}{20}\n$$\n$$\n\\frac{18}{5} c_1 - 2 c_2 = \\frac{8}{20} = \\frac{2}{5}\n$$\nMultiplying by $5$ gives $18 c_1 - 10 c_2 = 2$, which simplifies to $9 c_1 - 5 c_2 = 1$.\nWe now have a system of two equations for $c_1$ and $c_2$:\n(A) $9 c_1 - 5 c_2 = 1$\n(B) $\\frac{1}{5} c_1 + \\frac{1}{2} c_2 = \\frac{1}{4} \\implies 4 c_1 + 10 c_2 = 5$\n\nFrom (A), $5c_2 = 9c_1 - 1$. Substitute into (B):\n$4 c_1 + 2(5 c_2) = 5$\n$4 c_1 + 2(9 c_1 - 1) = 5$\n$4 c_1 + 18 c_1 - 2 = 5$\n$22 c_1 = 7 \\implies c_1^{\\star} = \\frac{7}{22}$.\n\nNow substitute $c_1^{\\star}$ back to find $c_2^{\\star}$:\n$5 c_2 = 9 \\left(\\frac{7}{22}\\right) - 1 = \\frac{63}{22} - \\frac{22}{22} = \\frac{41}{22}$.\n$c_2^{\\star} = \\frac{41}{22 \\times 5} = \\frac{41}{110}$.\n\nThe optimal reduced coordinates are $c^{\\star} = (c_1^{\\star}, c_2^{\\star}) = (\\frac{7}{22}, \\frac{41}{110})$.\n\nFinal check: The constraint must be satisfied.\n$0.2 c_1 + 0.5 c_2 = \\frac{1}{5}\\left(\\frac{7}{22}\\right) + \\frac{1}{2}\\left(\\frac{41}{110}\\right) = \\frac{7}{110} + \\frac{41}{220} = \\frac{14}{220} + \\frac{41}{220} = \\frac{55}{220} = \\frac{1}{4} = 0.25$.\nThe constraint is satisfied. The solution is correct.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{22}  \\frac{41}{110}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Principal Component Analysis (PCA) is a powerful data-driven tool, but its application to compositional data like mass fractions requires special care due to the sum-to-one constraint. This exercise delves into the mathematical subtleties of applying PCA correctly in this context, comparing different valid approaches . By evaluating the theoretical properties of each method, you will gain a robust understanding of how to correctly extract principal components from constrained datasets, a critical skill for building reliable reduced models.",
            "id": "4032808",
            "problem": "Consider a reacting flow computation in which the thermochemical state is represented by species mass fractions $\\mathbf{Y}^{(k)} \\in \\mathbb{R}^n$ at $m$ spatial or temporal samples indexed by $k = 1, \\dots, m$, with the physical constraints $Y_i^{(k)} \\ge 0$ for all $i$ and the compositional closure $\\mathbf{1}^\\top \\mathbf{Y}^{(k)} = \\sum_{i=1}^n Y_i^{(k)} = 1$. We seek to extract low-dimensional directions for model reduction consistent with an Intrinsic Low-Dimensional Manifold (ILDM) using Principal Component Analysis (PCA) on these compositions while respecting the closure constraint.\n\nTwo common approaches to incorporate the sum-to-one constraint are:\n\n- Affine centering within the simplex: compute the sample mean $\\bar{\\mathbf{Y}} = \\frac{1}{m} \\sum_{k=1}^m \\mathbf{Y}^{(k)}$, and form centered data $\\mathbf{X}^{(k)} = \\mathbf{Y}^{(k)} - \\bar{\\mathbf{Y}}$, restricting analysis to the affine subspace defined by $\\mathbf{1}^\\top \\mathbf{x} = 0$.\n\n- Orthonormal null-space basis: construct a matrix $Q \\in \\mathbb{R}^{n \\times (n-1)}$ with $Q^\\top Q = I_{n-1}$ and $Q^\\top \\mathbf{1} = \\mathbf{0}$, project centered compositions into reduced coordinates $\\mathbf{Z}^{(k)} = Q^\\top (\\mathbf{Y}^{(k)} - \\bar{\\mathbf{Y}})$, and perform PCA in $\\mathbb{R}^{n-1}$.\n\nFrom the fundamental constraints and the definition of PCA, determine which of the following statements are correct. Select all that apply.\n\nA. The sample covariance of the centered compositions, $S_X = \\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{X}^{(k)} \\mathbf{X}^{(k)\\top}$, satisfies $S_X \\mathbf{1} = \\mathbf{0}$ and has rank at most $n-1$, so principal components extracted from $\\mathbf{X}^{(k)}$ lie in the subspace orthogonal to $\\mathbf{1}$.\n\nB. In the orthonormal basis approach, any reconstruction of the form $\\mathbf{Y} \\approx \\bar{\\mathbf{Y}} + Q \\mathbf{z}$ automatically preserves non-negativity of mass fractions for arbitrary principal scores $\\mathbf{z}$ because $Q$ is orthonormal.\n\nC. For small perturbations around a mean with strictly positive entries, the centered log-ratio perturbations are, to first order, related to the reduced coordinates by a fixed linear map that depends on $\\bar{\\mathbf{Y}}$, so PCA on $\\mathbf{Z}^{(k)}$ is locally equivalent to PCA on centered log-ratio perturbations up to a linear change of variables.\n\nD. Performing PCA directly on the unprojected compositions $\\mathbf{Y}^{(k)}$ without centering or projection yields a full-rank covariance with a dominant eigenvalue associated with $\\mathbf{1}$, so the principal directions capture unconstrained variability.\n\nE. In the orthonormal basis approach, the reduced covariance $S_Z = \\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{Z}^{(k)} \\mathbf{Z}^{(k)\\top}$ satisfies $S_Z = Q^\\top S_X Q$, and each eigenpair $(\\lambda, \\mathbf{u})$ of $S_Z$ corresponds to an eigenpair $(\\lambda, Q \\mathbf{u})$ of $S_X$ restricted to the constraint subspace, enabling reconstructions $\\mathbf{Y} \\approx \\bar{\\mathbf{Y}} + Q \\mathbf{u} \\alpha$ that preserve the sum-to-one constraint for scalar scores $\\alpha$.",
            "solution": "**Analysis of Options**\n\n**A. The sample covariance of the centered compositions, $S_X = \\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{X}^{(k)} \\mathbf{X}^{(k)\\top}$, satisfies $S_X \\mathbf{1} = \\mathbf{0}$ and has rank at most $n-1$, so principal components extracted from $\\mathbf{X}^{(k)}$ lie in the subspace orthogonal to $\\mathbf{1}$.**\n\nFirst, we analyze the centered data vectors $\\mathbf{X}^{(k)}$. The original data points $\\mathbf{Y}^{(k)}$ satisfy $\\mathbf{1}^\\top \\mathbf{Y}^{(k)} = 1$. The mean vector $\\bar{\\mathbf{Y}}$ also satisfies this constraint:\n$$ \\mathbf{1}^\\top \\bar{\\mathbf{Y}} = \\mathbf{1}^\\top \\left(\\frac{1}{m} \\sum_{k=1}^m \\mathbf{Y}^{(k)}\\right) = \\frac{1}{m} \\sum_{k=1}^m (\\mathbf{1}^\\top \\mathbf{Y}^{(k)}) = \\frac{1}{m} \\sum_{k=1}^m 1 = 1 $$\nTherefore, for each centered data point $\\mathbf{X}^{(k)} = \\mathbf{Y}^{(k)} - \\bar{\\mathbf{Y}}$, we have:\n$$ \\mathbf{1}^\\top \\mathbf{X}^{(k)} = \\mathbf{1}^\\top (\\mathbf{Y}^{(k)} - \\bar{\\mathbf{Y}}) = \\mathbf{1}^\\top \\mathbf{Y}^{(k)} - \\mathbf{1}^\\top \\bar{\\mathbf{Y}} = 1 - 1 = 0 $$\nThis shows that all centered data vectors $\\mathbf{X}^{(k)}$ are orthogonal to the vector $\\mathbf{1}$.\n\nNow, consider the product of the covariance matrix $S_X$ and the vector $\\mathbf{1}$:\n$$ S_X \\mathbf{1} = \\left(\\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{X}^{(k)} \\mathbf{X}^{(k)\\top}\\right) \\mathbf{1} = \\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{X}^{(k)} (\\mathbf{X}^{(k)\\top} \\mathbf{1}) $$\nSince $\\mathbf{X}^{(k)\\top} \\mathbf{1} = (\\mathbf{1}^\\top \\mathbf{X}^{(k)})^\\top = 0^\\top = 0$, we have:\n$$ S_X \\mathbf{1} = \\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{X}^{(k)} (0) = \\mathbf{0} $$\nThis confirms that $\\mathbf{1}$ is an eigenvector of $S_X$ with an eigenvalue of $0$. A non-zero vector in the null space implies that $S_X$ is singular, so its rank is less than $n$. Since the data cloud spans a subspace of dimension at most $n-1$, the rank of $S_X$ is at most $n-1$.\n\nThe principal components are the eigenvectors of $S_X$ corresponding to non-zero eigenvalues. Let $\\mathbf{u}$ be such an eigenvector with eigenvalue $\\lambda \\neq 0$, so $S_X \\mathbf{u} = \\lambda \\mathbf{u}$. We can show that $\\mathbf{u}$ must be orthogonal to $\\mathbf{1}$:\n$$ \\lambda (\\mathbf{1}^\\top \\mathbf{u}) = \\mathbf{1}^\\top (\\lambda \\mathbf{u}) = \\mathbf{1}^\\top (S_X \\mathbf{u}) = (\\mathbf{1}^\\top S_X) \\mathbf{u} $$\nSince $S_X$ is symmetric, $\\mathbf{1}^\\top S_X = (S_X \\mathbf{1})^\\top = \\mathbf{0}^\\top$. Thus:\n$$ \\lambda (\\mathbf{1}^\\top \\mathbf{u}) = \\mathbf{0}^\\top \\mathbf{u} = 0 $$\nBecause $\\lambda \\neq 0$, we must have $\\mathbf{1}^\\top \\mathbf{u} = 0$. This confirms that all principal components lie in the subspace orthogonal to $\\mathbf{1}$.\n\nVerdict: **Correct**.\n\n**B. In the orthonormal basis approach, any reconstruction of the form $\\mathbf{Y} \\approx \\bar{\\mathbf{Y}} + Q \\mathbf{z}$ automatically preserves non-negativity of mass fractions for arbitrary principal scores $\\mathbf{z}$ because $Q$ is orthonormal.**\n\nThe reconstruction is given by $\\mathbf{Y}_{\\text{approx}} = \\bar{\\mathbf{Y}} + Q \\mathbf{z}$. The non-negativity constraint requires $Y_{\\text{approx}, i} \\ge 0$ for all components $i=1, \\dots, n$. While $\\bar{\\mathbf{Y}}$ typically has all positive components, the term $Q \\mathbf{z}$ is a linear combination of the basis vectors of the constraint subspace. These basis vectors (the columns of $Q$) are not guaranteed to have all non-negative or non-positive entries. For a sufficiently large score vector $\\mathbf{z}$ in an appropriate direction, the term $Q\\mathbf{z}$ can have negative components that are large enough in magnitude to make some components of $\\mathbf{Y}_{\\text{approx}}$ negative.\n\nConsider a simple example with $n=3$. Let $\\bar{\\mathbf{Y}} = [1/3, 1/3, 1/3]^\\top$. A possible column for $Q$ is $\\mathbf{q}_1 = [1/\\sqrt{2}, -1/\\sqrt{2}, 0]^\\top$. If we take a reconstruction along this direction, $\\mathbf{Y}_{\\text{approx}} = \\bar{\\mathbf{Y}} + \\alpha \\mathbf{q}_1 = [1/3 + \\alpha/\\sqrt{2}, 1/3 - \\alpha/\\sqrt{2}, 1/3]^\\top$. For any $\\alpha > \\frac{\\sqrt{2}}{3}$, the second component becomes negative. Orthonormality of $Q$ is a geometric property that simplifies calculations in the projected space but provides no mechanism to enforce the inequality constraint $Y_i \\ge 0$ in the original space.\n\nVerdict: **Incorrect**.\n\n**C. For small perturbations around a mean with strictly positive entries, the centered log-ratio perturbations are, to first order, related to the reduced coordinates by a fixed linear map that depends on $\\bar{\\mathbf{Y}}$, so PCA on $\\mathbf{Z}^{(k)}$ is locally equivalent to PCA on centered log-ratio perturbations up to a linear change of variables.**\n\nLet $\\mathbf{Y} = \\bar{\\mathbf{Y}} + \\delta\\mathbf{Y}$ be a small perturbation. The reduced coordinate is $\\mathbf{Z} = Q^\\top(\\mathbf{Y} - \\bar{\\mathbf{Y}}) = Q^\\top \\delta\\mathbf{Y}$. The centered log-ratio (clr) transformation is $\\text{clr}(\\mathbf{Y})$. The perturbation of the clr vector around $\\text{clr}(\\bar{\\mathbf{Y}})$ is, to first order:\n$$ \\delta(\\text{clr}(\\mathbf{Y})) \\approx P_{\\mathbf{1}} \\text{diag}(\\bar{\\mathbf{Y}})^{-1} \\delta\\mathbf{Y} $$\nwhere $P_{\\mathbf{1}} = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$ is the projection matrix onto the subspace orthogonal to $\\mathbf{1}$.\nSince the columns of $Q$ form an orthonormal basis for this subspace, we have $P_{\\mathbf{1}} = QQ^\\top$.\nA PCA on clr perturbations would be performed in a coordinate system. A natural choice is the coordinate system defined by $Q$. The coordinates of the clr perturbation vector in this basis are:\n$$ \\mathbf{z}_{\\text{clr}} = Q^\\top \\delta(\\text{clr}(\\mathbf{Y})) \\approx Q^\\top (QQ^\\top) \\text{diag}(\\bar{\\mathbf{Y}})^{-1} \\delta\\mathbf{Y} = Q^\\top \\text{diag}(\\bar{\\mathbf{Y}})^{-1} \\delta\\mathbf{Y} $$\nThe perturbation $\\delta\\mathbf{Y}$ lies in the constraint subspace, so $\\delta\\mathbf{Y} = QQ^\\top\\delta\\mathbf{Y} = Q(Q^\\top\\delta\\mathbf{Y}) = Q\\mathbf{Z}$. Substituting this gives:\n$$ \\mathbf{z}_{\\text{clr}} \\approx Q^\\top \\text{diag}(\\bar{\\mathbf{Y}})^{-1} (Q\\mathbf{Z}) = (Q^\\top \\text{diag}(\\bar{\\mathbf{Y}})^{-1} Q)\\mathbf{Z} $$\nLet $M = Q^\\top \\text{diag}(\\bar{\\mathbf{Y}})^{-1} Q$. This is a fixed $(n-1) \\times (n-1)$ linear map that depends on $\\bar{\\mathbf{Y}}$. The vector of clr coordinates $\\mathbf{z}_{\\text{clr}}$ is approximately linearly related to the vector of standard reduced coordinates $\\mathbf{Z}$.\nPCA on $\\{\\mathbf{Z}^{(k)}\\}$ analyzes the covariance $S_Z$. PCA on $\\{\\mathbf{z}_{\\text{clr}}^{(k)}\\}$ analyzes the covariance $S_{\\text{clr}} \\approx M S_Z M^\\top$. While the PCs are not identical, they are related through a fixed linear map $M$. This is what \"locally equivalent up to a linear change of variables\" means. This relationship is a cornerstone of compositional data analysis, linking the Euclidean geometry of the simplex (used in the affine centering approach) to the Aitchison geometry (used in the log-ratio approach).\n\nVerdict: **Correct**.\n\n**D. Performing PCA directly on the unprojected compositions $\\mathbf{Y}^{(k)}$ without centering or projection yields a full-rank covariance with a dominant eigenvalue associated with $\\mathbf{1}$, so the principal directions capture unconstrained variability.**\n\nStandard PCA operates on centered data. The covariance matrix is therefore $S_X = \\frac{1}{m-1} \\sum_{k=1}^m (\\mathbf{Y}^{(k)} - \\bar{\\mathbf{Y}}) (\\mathbf{Y}^{(k)} - \\bar{\\mathbf{Y}})^\\top$. As shown in the analysis for option A, the data vectors $\\mathbf{Y}^{(k)} - \\bar{\\mathbf{Y}}$ are confined to an $(n-1)$-dimensional subspace orthogonal to $\\mathbf{1}$. Consequently, the covariance matrix $S_X$ has a rank of at most $n-1$, so it is not full-rank.\nFurthermore, as also shown in A, $S_X \\mathbf{1} = \\mathbf{0}$, meaning $\\mathbf{1}$ is an eigenvector corresponding to an eigenvalue of $0$. A zero eigenvalue is never dominant (unless all variance is zero). The closure constraint fundamentally restricts the variability of the data, so the principal directions cannot capture \"unconstrained\" variability. The entire premise of this statement is flawed.\n\nVerdict: **Incorrect**.\n\n**E. In the orthonormal basis approach, the reduced covariance $S_Z = \\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{Z}^{(k)} \\mathbf{Z}^{(k)\\top}$ satisfies $S_Z = Q^\\top S_X Q$, and each eigenpair $(\\lambda, \\mathbf{u})$ of $S_Z$ corresponds to an eigenpair $(\\lambda, Q \\mathbf{u})$ of $S_X$ restricted to the constraint subspace, enabling reconstructions $\\mathbf{Y} \\approx \\bar{\\mathbf{Y}} + Q \\mathbf{u} \\alpha$ that preserve the sum-to-one constraint for scalar scores $\\alpha$.**\n\n1.  **Covariance relationship:**\n    $$ S_Z = \\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{Z}^{(k)} \\mathbf{Z}^{(k)\\top} = \\frac{1}{m-1} \\sum_{k=1}^m (Q^\\top \\mathbf{X}^{(k)})(Q^\\top \\mathbf{X}^{(k)})^\\top $$\n    $$ S_Z = \\frac{1}{m-1} \\sum_{k=1}^m Q^\\top \\mathbf{X}^{(k)} \\mathbf{X}^{(k)\\top} Q = Q^\\top \\left(\\frac{1}{m-1} \\sum_{k=1}^m \\mathbf{X}^{(k)} \\mathbf{X}^{(k)\\top}\\right) Q = Q^\\top S_X Q $$\n    This relationship is correct.\n\n2.  **Eigenpair correspondence:**\n    Let $(\\lambda, \\mathbf{u})$ be an eigenpair of $S_Z$, where $\\mathbf{u} \\in \\mathbb{R}^{n-1}$. So $S_Z \\mathbf{u} = \\lambda \\mathbf{u}$.\n    Substituting $S_Z = Q^\\top S_X Q$, we get $Q^\\top S_X Q \\mathbf{u} = \\lambda \\mathbf{u}$.\n    Multiplying from the left by $Q$: $Q Q^\\top S_X Q \\mathbf{u} = \\lambda Q \\mathbf{u}$.\n    Let $\\mathbf{v} = Q \\mathbf{u}$. Note that $\\mathbf{v} \\in \\mathbb{R}^n$. The matrix $P = QQ^\\top$ is the orthogonal projector onto the subspace spanned by the columns of $Q$ (the constraint subspace). Any eigenvector $\\mathbf{w}$ of $S_X$ with a non-zero eigenvalue must lie in this subspace (from A), so $P\\mathbf{w}=\\mathbf{w}$.\n    The vector $S_X \\mathbf{v}$ also lies in this subspace. Therefore, $Q Q^\\top (S_X \\mathbf{v}) = S_X \\mathbf{v}$.\n    Our equation becomes $S_X (Q \\mathbf{u}) = \\lambda (Q \\mathbf{u})$, or $S_X \\mathbf{v} = \\lambda \\mathbf{v}$. This shows that $(\\lambda, \\mathbf{v}) = (\\lambda, Q\\mathbf{u})$ is an eigenpair of $S_X$. The vector $Q\\mathbf{u}$ is in the constraint subspace by construction. This part is correct.\n\n3.  **Reconstruction constraint:**\n    Consider the reconstruction $\\mathbf{Y}_{\\text{approx}} = \\bar{\\mathbf{Y}} + Q \\mathbf{u} \\alpha$. To check the sum-to-one constraint, we compute its dot product with $\\mathbf{1}$:\n    $$ \\mathbf{1}^\\top \\mathbf{Y}_{\\text{approx}} = \\mathbf{1}^\\top \\bar{\\mathbf{Y}} + \\mathbf{1}^\\top (Q \\mathbf{u} \\alpha) $$\n    We know $\\mathbf{1}^\\top \\bar{\\mathbf{Y}} = 1$. By definition of $Q$, its columns are orthogonal to $\\mathbf{1}$, so $\\mathbf{1}^\\top Q = \\mathbf{0}^\\top$.\n    $$ \\mathbf{1}^\\top (Q \\mathbf{u} \\alpha) = (\\mathbf{1}^\\top Q) \\mathbf{u} \\alpha = \\mathbf{0}^\\top \\mathbf{u} \\alpha = 0 $$\n    Thus, $\\mathbf{1}^\\top \\mathbf{Y}_{\\text{approx}} = 1 + 0 = 1$. The reconstruction preserves the sum-to-one constraint.\n\nAll parts of the statement are mathematically sound.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}