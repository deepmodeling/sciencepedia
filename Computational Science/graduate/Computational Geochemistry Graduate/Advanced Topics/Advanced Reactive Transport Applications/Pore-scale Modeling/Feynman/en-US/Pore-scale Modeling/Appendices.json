{
    "hands_on_practices": [
        {
            "introduction": "Raw binary images obtained from segmenting micro-CT scans are rarely perfect and often contain artifacts like \"salt-and-pepper\" noise. This practice introduces mathematical morphology, a fundamental toolset in image processing, to clean and regularize these digital rock geometries. By implementing morphological opening and closing operations, you will gain hands-on experience in filtering noise and learn how to quantify the impact of these operations on a crucial geometric property, the pore-solid interfacial area .",
            "id": "4095925",
            "problem": "You are given a three-dimensional binary image representing a Digital Rock Physics segmentation at the pore scale, modeled on a cubic lattice with voxel edge length $\\Delta$ measured in meters. The binary field is $X:\\mathbb{Z}^3\\to\\{0,1\\}$, where $X(\\mathbf{i})=1$ denotes a pore voxel and $X(\\mathbf{i})=0$ denotes a rock voxel. Consider Mathematical Morphology operations defined on sets in a discrete lattice. Let $B\\subset \\mathbb{Z}^3$ be a spherical structuring element of radius $r$ voxels centered at the origin, and let $X$ denote the pore set. The erosion $X\\ominus B$ removes pore voxels that cannot fully contain $B$, and the dilation $X\\oplus B$ adds pore voxels that can be reached by translating $B$. The morphological opening is $(X\\ominus B)\\oplus B$ and the morphological closing is $(X\\oplus B)\\ominus B$. These operations on $X$ can be used to suppress salt-and-pepper speckle artifacts and smooth jagged interfaces between phases. The interfacial surface area between pore and rock phases in the discretized lattice can be computed in terms of voxel faces at interfaces. Starting from the discrete lattice description and the definition that each interfacial face contributes area equal to $\\Delta^2$ (the area of one voxel face), derive a voxel-based estimator for the total surface area $S(X,\\Delta)$ of the pore–rock interface by counting interfacial faces along the principal axes of the cubic lattice and aggregating their contributions to area without double-counting. Use this estimator to quantify how morphological opening and morphological closing change the interfacial area.\n\nYour task is to implement the morphological opening and closing on the pore set $X$ using a spherical structuring element $B$ of radius $r$ voxels, and to compute $S(X,\\Delta)$ before and after each operation. Then compute the changes $\\Delta S_{\\mathrm{open}}=S(X_{\\mathrm{open}},\\Delta)-S(X,\\Delta)$ and $\\Delta S_{\\mathrm{close}}=S(X_{\\mathrm{close}},\\Delta)-S(X,\\Delta)$, where $X_{\\mathrm{open}}$ and $X_{\\mathrm{close}}$ are the results of opening and closing applied to $X$, respectively. All surface areas must be expressed in $\\mathrm{m}^2$.\n\nImplement a program that constructs the following scientifically realistic test suite of synthetic binary volumes and parameters:\n\n- Case $A$ (general case with spherical pore and speckle noise):\n    - Volume size: $N=(64,64,64)$ voxels.\n    - Voxel size: $\\Delta=2\\times 10^{-6}\\ \\mathrm{m}$.\n    - Pore geometry: a single spherical pore of radius $15$ voxels, centered at $(32,32,32)$.\n    - Speckle artifacts: flip the state of a fraction $p=5\\times 10^{-3}$ of voxels uniformly at random over the entire volume (salt-and-pepper noise affecting both phases).\n    - Structuring element: spherical $B$ with radius $r=2$ voxels.\n    - Random seed for reproducibility: $42$.\n\n- Case $B$ (boundary condition with planar interface and no noise):\n    - Volume size: $N=(64,64,64)$ voxels.\n    - Voxel size: $\\Delta=2\\times 10^{-6}\\ \\mathrm{m}$.\n    - Pore geometry: a planar half-space pore where $X(i_x,i_y,i_z)=1$ if $i_x<32$ and $X(i_x,i_y,i_z)=0$ otherwise.\n    - Speckle artifacts: none, $p=0$.\n    - Structuring element: spherical $B$ with radius $r=2$ voxels.\n\n- Case $C$ (edge case with tiny pores near the morphological threshold plus noise):\n    - Volume size: $N=(32,32,32)$ voxels.\n    - Voxel size: $\\Delta=1\\times 10^{-6}\\ \\mathrm{m}$.\n    - Pore geometry: three tiny spherical pores of radius $1$ voxel centered at $(8,8,8)$, $(16,20,12)$, and $(24,10,24)$ in an otherwise rock matrix.\n    - Speckle artifacts: flip the state of a fraction $p=10^{-2}$ of voxels uniformly at random.\n    - Structuring element: spherical $B$ with radius $r=2$ voxels.\n    - Random seed for reproducibility: $7$.\n\nFor each case, compute the list $[S(X,\\Delta), S(X_{\\mathrm{open}},\\Delta), S(X_{\\mathrm{close}},\\Delta), \\Delta S_{\\mathrm{open}}, \\Delta S_{\\mathrm{close}}]$. Express all surface areas in $\\mathrm{m}^2$ as floating-point numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the list aggregates the results for Cases $A$, $B$, and $C$ in order as a list of lists, for example $[[\\dots],[\\dots],[\\dots]]$.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of mathematical morphology and computational physics, specifically within the domain of Digital Rock Physics. It is well-posed, providing all necessary parameters, including volume dimensions, voxel resolution, geometric definitions, structuring element specifications, and random seeds for reproducibility. The problem statement is objective, using precise, formal language. It does not exhibit any of the flaws listed in the validation criteria, such as factual unsoundness, ambiguity, or being ill-posed. Therefore, a reasoned solution will be provided.\n\nThe core of this problem lies in implementing and quantifying the effect of two fundamental morphological operations, opening and closing, on a discrete, 3D binary representation of a porous medium. These operations are used to filter noise and regularize interfaces, which in turn affects geometric properties like interfacial surface area.\n\nFirst, we must formalize the definitions within the context of a discrete cubic lattice. Let the binary image be represented by a function $X(\\mathbf{i}): \\mathbb{Z}^3 \\to \\{0, 1\\}$, where $\\mathbf{i}=(i_x, i_y, i_z)$ is a voxel index. $X(\\mathbf{i})=1$ denotes the pore phase and $X(\\mathbf{i})=0$ denotes the solid (rock) phase. The set of all pore voxels is $P = \\{\\mathbf{i} \\in \\mathbb{Z}^3 \\mid X(\\mathbf{i}) = 1\\}$.\n\nA structuring element $B$ is a set of voxel offsets relative to an origin, also defined on $\\mathbb{Z}^3$. The problem specifies a spherical structuring element of radius $r$ voxels, which is the set of all integer grid points whose Euclidean distance from the origin is less than or equal to $r$:\n$$ B = \\{ \\mathbf{b} \\in \\mathbb{Z}^3 \\mid \\|\\mathbf{b}\\|_2 \\le r \\} = \\{ (b_x, b_y, b_z) \\in \\mathbb{Z}^3 \\mid \\sqrt{b_x^2 + b_y^2 + b_z^2} \\le r \\} $$\n\nThe fundamental morphological operations of erosion and dilation of the pore set $P$ by the structuring element $B$ are defined as:\n- **Erosion**: $P \\ominus B = \\{ \\mathbf{p} \\in \\mathbb{Z}^3 \\mid B_{\\mathbf{p}} \\subseteq P \\}$, where $B_{\\mathbf{p}} = \\{ \\mathbf{p} + \\mathbf{b} \\mid \\mathbf{b} \\in B \\}$ is the translation of $B$ to the point $\\mathbf{p}$. Erosion effectively shrinks the pore space by removing any pore voxel from which the structuring element cannot be contained entirely within the original pore space.\n- **Dilation**: $P \\oplus B = \\bigcup_{\\mathbf{p} \\in P} B_{\\mathbf{p}}$. Dilation expands the pore space. A voxel is included in the dilated set if the structuring element, when centered at that voxel, overlaps with the original pore space. For a symmetric structuring element like a sphere, this is equivalent to $\\{ \\mathbf{q} \\in \\mathbb{Z}^3 \\mid B_{\\mathbf{q}} \\cap P \\neq \\emptyset \\}$.\n\nBased on these, the opening and closing operations are defined as compositions:\n- **Opening**: $P_{\\mathrm{open}} = (P \\ominus B) \\oplus B$. Opening smooths the contours of the pore space, breaks narrow connections (isthmuses), and eliminates small, isolated pore regions (islands) that are smaller than the structuring element.\n- **Closing**: $P_{\\mathrm{close}} = (P \\oplus B) \\ominus B$. Closing also smooths contours but does so by filling narrow gaps and small holes in the pore space that are smaller than the structuring element.\n\nNext, we address the estimation of the interfacial surface area, $S(X, \\Delta)$. On a cubic lattice where each voxel has an edge length of $\\Delta$, the face of a voxel has an area of $\\Delta^2$. The total interfacial area is the sum of the areas of all faces shared between a pore voxel and a rock voxel. A robust method for counting these faces without duplication is to iterate through all voxels and, for each voxel, check its neighbors in the positive axial directions. An interface exists between voxel $\\mathbf{i}$ and its neighbor $\\mathbf{i}+\\mathbf{e}_k$ (where $\\mathbf{e}_k$ is the unit vector along axis $k \\in \\{x,y,z\\}$) if their phase values differ, i.e., $X(\\mathbf{i}) \\neq X(\\mathbf{i}+\\mathbf{e}_k)$. The total number of such interfacial faces, $N_{\\text{faces}}$, is:\n$$ N_{\\text{faces}}(X) = \\sum_{i_x, i_y, i_z} \\left( \\mathbb{I}[X(i_x, i_y, i_z) \\neq X(i_x+1, i_y, i_z)] + \\mathbb{I}[X(i_x, i_y, i_z) \\neq X(i_x, i_y+1, i_z)] + \\mathbb{I}[X(i_x, i_y, i_z) \\neq X(i_x, i_y, i_z+1)] \\right) $$\nwhere $\\mathbb{I}[\\cdot]$ is the indicator function, and the summation is performed over all indices where the pairs are valid. The total surface area is then:\n$$ S(X, \\Delta) = N_{\\text{faces}}(X) \\cdot \\Delta^2 $$\n\nThe implementation will proceed by first constructing the binary volumes for each specified test case. The morphological operations are computationally intensive and are best performed using optimized library functions, such as those available in `scipy.ndimage`. The `scipy.ndimage.binary_opening` and `scipy.ndimage.binary_closing` functions directly implement the required operations using a provided structuring element. The surface area will be computed using the discrete summation formula, which can be efficiently implemented using `numpy` array operations by comparing shifted versions of the volume array.\n\nFinally, for each case, we will compute the initial surface area $S(X, \\Delta)$, the area after opening $S(X_{\\mathrm{open}}, \\Delta)$, and the area after closing $S(X_{\\mathrm{close}}, \\Delta)$. The changes in surface area are $\\Delta S_{\\mathrm{open}} = S(X_{\\mathrm{open}}, \\Delta) - S(X, \\Delta)$ and $\\Delta S_{\\mathrm{close}} = S(X_{\\mathrm{close}}, \\Delta) - S(X, \\Delta)$. Both opening and closing are smoothing operations that typically reduce high-frequency spatial variations, such as noise and sharp corners, thereby generally leading to a decrease in the total interfacial surface area (i.e., $\\Delta S \\le 0$).\n\nThe procedure for each test case is as follows:\n1.  Define the parameters: volume size $N$, voxel size $\\Delta$, structuring element radius $r$, and noise parameters.\n2.  Generate the initial 3D binary volume $X$ according to the geometric prescription and add noise if specified, using the given random seed for reproducibility.\n3.  Generate the spherical structuring element $B$ of radius $r$.\n4.  Compute the initial surface area $S_0 = S(X, \\Delta)$.\n5.  Apply morphological opening to $X$ using $B$ to obtain $X_{\\mathrm{open}}$. Compute the new surface area $S_{\\mathrm{open}} = S(X_{\\mathrm{open}}, \\Delta)$.\n6.  Apply morphological closing to $X$ using $B$ to obtain $X_{\\mathrm{close}}$. Compute the new surface area $S_{\\mathrm{close}} = S(X_{\\mathrm{close}}, \\Delta)$.\n7.  Calculate the differences $\\Delta S_{\\mathrm{open}} = S_{\\mathrm{open}} - S_0$ and $\\Delta S_{\\mathrm{close}} = S_{\\mathrm{close}} - S_0$.\n8.  Aggregate the five computed values $[S_0, S_{\\mathrm{open}}, S_{\\mathrm{close}}, \\Delta S_{\\mathrm{open}}, \\Delta S_{\\mathrm{close}}]$.\nThe final output will be a list containing the aggregated results for all three test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy import ndimage\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the specified test cases.\n    \"\"\"\n\n    def generate_sphere_strele(radius: int) -> np.ndarray:\n        \"\"\"\n        Generates a 3D spherical structuring element.\n        \n        Args:\n            radius: The radius of the sphere in voxels.\n\n        Returns:\n            A boolean numpy array representing the spherical structuring element.\n        \"\"\"\n        diameter = 2 * radius + 1\n        x, y, z = np.mgrid[-radius:radius+1, -radius:radius+1, -radius:radius+1]\n        dist_sq = x**2 + y**2 + z**2\n        return dist_sq <= radius**2\n\n    def calculate_surface_area(volume: np.ndarray, delta: float) -> float:\n        \"\"\"\n        Calculates the interfacial surface area of a 3D binary volume.\n\n        Args:\n            volume: A 3D numpy array with 0s and 1s.\n            delta: The edge length of a single voxel in meters.\n\n        Returns:\n            The total interfacial surface area in square meters.\n        \"\"\"\n        if volume.ndim != 3:\n            raise ValueError(\"Input volume must be a 3D array.\")\n        \n        # Count interfaces along each axis by comparing neighboring voxels\n        # An interface exists if adjacent voxels have different values (0 vs 1)\n        faces_x = np.sum(volume[:-1, :, :] != volume[1:, :, :])\n        faces_y = np.sum(volume[:, :-1, :] != volume[:, 1:, :])\n        faces_z = np.sum(volume[:, :, :-1] != volume[:, :, 1:])\n        \n        total_faces = faces_x + faces_y + faces_z\n        return total_faces * (delta ** 2)\n\n    def generate_noisy_volume(base_volume: np.ndarray, noise_fraction: float, seed: int) -> np.ndarray:\n        \"\"\"Applies salt-and-pepper noise to a binary volume.\"\"\"\n        if noise_fraction == 0:\n            return base_volume.copy()\n            \n        rng = np.random.default_rng(seed)\n        noisy_volume = base_volume.copy()\n        total_voxels = noisy_volume.size\n        num_flips = int(noise_fraction * total_voxels)\n        \n        # Generate unique indices to flip\n        indices_to_flip = rng.choice(total_voxels, size=num_flips, replace=False)\n        flat_volume = noisy_volume.flatten()\n        flat_volume[indices_to_flip] = 1 - flat_volume[indices_to_flip]\n        \n        return flat_volume.reshape(base_volume.shape)\n\n    def process_case(params: dict):\n        \"\"\"Processes a single test case.\"\"\"\n        N = params['N']\n        delta = params['delta']\n        r = params['r']\n        noise_p = params['p']\n        seed = params['seed']\n\n        # Generate base geometry\n        if params['case_id'] == 'A':\n            center = np.array(params['pore_center'])\n            radius = params['pore_radius']\n            coords = np.mgrid[0:N[0], 0:N[1], 0:N[2]]\n            dist_sq = np.sum((coords - center[:, np.newaxis, np.newaxis, np.newaxis])**2, axis=0)\n            initial_volume = (dist_sq <= radius**2).astype(np.int8)\n        elif params['case_id'] == 'B':\n            initial_volume = np.zeros(N, dtype=np.int8)\n            initial_volume[:32, :, :] = 1\n        elif params['case_id'] == 'C':\n            initial_volume = np.zeros(N, dtype=np.int8)\n            coords = np.mgrid[0:N[0], 0:N[1], 0:N[2]]\n            for center_info in params['pore_info']:\n                center, radius = np.array(center_info[0]), center_info[1]\n                dist_sq = np.sum((coords - center[:, np.newaxis, np.newaxis, np.newaxis])**2, axis=0)\n                initial_volume[dist_sq <= radius**2] = 1\n        else:\n            raise ValueError(f\"Unknown case_id: {params['case_id']}\")\n\n        # Add noise\n        X = generate_noisy_volume(initial_volume, noise_p, seed)\n        \n        # Generate structuring element\n        B = generate_sphere_strele(r)\n\n        # Calculate initial surface area\n        S_initial = calculate_surface_area(X, delta)\n\n        # Morphological opening\n        X_open = ndimage.binary_opening(X, structure=B).astype(X.dtype)\n        S_open = calculate_surface_area(X_open, delta)\n        dS_open = S_open - S_initial\n\n        # Morphological closing\n        X_close = ndimage.binary_closing(X, structure=B).astype(X.dtype)\n        S_close = calculate_surface_area(X_close, delta)\n        dS_close = S_close - S_initial\n\n        return [S_initial, S_open, S_close, dS_open, dS_close]\n\n    test_cases = [\n        {\n            'case_id': 'A',\n            'N': (64, 64, 64), 'delta': 2e-6,\n            'pore_radius': 15, 'pore_center': (32, 32, 32),\n            'p': 5e-3, 'r': 2, 'seed': 42\n        },\n        {\n            'case_id': 'B',\n            'N': (64, 64, 64), 'delta': 2e-6,\n            'p': 0.0, 'r': 2, 'seed': None\n        },\n        {\n            'case_id': 'C',\n            'N': (32, 32, 32), 'delta': 1e-6,\n            'pore_info': [\n                ((8, 8, 8), 1),\n                ((16, 20, 12), 1),\n                ((24, 10, 24), 1)\n            ],\n            'p': 1e-2, 'r': 2, 'seed': 7\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result_list = process_case(case_params)\n        results.append(result_list)\n    \n    # Format output as a list of lists string\n    # e.g., [[val1, val2], [val3, val4]]\n    # str() on a list produces the correct string representation\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "After preparing a clean digital rock geometry, we can simulate physical processes like fluid flow to calculate properties such as permeability. However, any numerical simulation on a discrete grid introduces errors, and it is crucial to verify that our results are accurate and converge to the true physical value as resolution increases. This exercise guides you through a grid refinement study, applying Richardson extrapolation to simulation data to determine the numerical scheme's order of accuracy and to estimate the true continuum-limit permeability, a cornerstone of verification and validation in computational science .",
            "id": "4096000",
            "problem": "Consider a two-dimensional pore-scale flow problem in computational geochemistry framed within Digital Rock Physics (DRP) and the Lattice Boltzmann Method (LBM). A straight channel of height $H$ discretized on a regular lattice is used as a representative \"digital rock\" to estimate permeability, and the wall boundary scheme is varied to study its impact on resolution-dependent error. The continuum-limit permeability of planar Poiseuille flow is defined by Darcy's law as a constitutive relation: for a channel of height $H$, the continuum-limit permeability is $k_{\\infty} = H^2 / 12$ in dimensionless lattice units, which is a widely accepted fact in viscous laminar flow between parallel plates. In numerical discretizations of LBM with no-slip walls, the computed permeability $k(h)$ deviates from the continuum-limit $k_{\\infty}$ due to truncation errors that depend on the grid spacing $h$ and the wall boundary scheme. We adopt the standard numerical analysis model that, in the asymptotic regime of grid refinement where $h \\rightarrow 0$, the leading-order error behaves as $k(h) = k_{\\infty} + C \\, h^{p}$ where $C$ is a scheme-dependent constant and $p$ is the order of accuracy of the wall boundary scheme for the given geometry. The objective is to estimate $p$ and $k_{\\infty}$ from a grid refinement study and then quantify the relative permeability error at each resolution.\n\nYour task is to write a complete program that, given measured permeability values from LBM for three successive grid refinements with a constant refinement ratio and two distinct wall boundary schemes, will:\n\n- Estimate the apparent order of accuracy $p$ and the continuum-limit permeability $k_{\\infty}$ by grid refinement and extrapolation to the continuum limit using the asymptotic error model $k(h) = k_{\\infty} + C \\, h^{p}$.\n- Compute the relative error $E = |k(h) - k_{\\infty}| / k_{\\infty}$ for each resolution.\n- Aggregate the results across all test cases in the specified output format.\n\nAll quantities in this problem are dimensionless lattice units. Angles do not appear and no physical unit conversion is required.\n\nThe test suite consists of two boundary schemes applied to the same channel geometry height $H = 1$, with three grid resolutions characterized by the number of nodes across the channel $N \\in \\{32, 64, 128\\}$ so that $h = H/N$. The constant refinement ratio is thus $r = h_1 / h_2 = h_2 / h_3 = 2$.\n\n- Test Case $1$ (Scheme A: basic bounce-back on the staircased wall):\n  - Channel height $H = 1$.\n  - Resolutions $N = [32, 64, 128]$.\n  - Measured permeabilities $k(h)$ for each $N$: $[0.09895833333333333, 0.09114583333333333, 0.08723958333333333]$.\n\n- Test Case $2$ (Scheme B: improved wall treatment emulating interpolated bounce-back):\n  - Channel height $H = 1$.\n  - Resolutions $N = [32, 64, 128]$.\n  - Measured permeabilities $k(h)$ for each $N$: $[0.08338216145833333, 0.08334554036458333, 0.08333638509114583]$.\n\nYour program must, for each test case, compute:\n1. The estimated order $p$ of the wall boundary scheme from the three-level grid refinement.\n2. The extrapolated continuum-limit permeability $k_{\\infty}$.\n3. The relative errors $E$ at each resolution $N = 32, 64, 128$ computed as $E = |k(h) - k_{\\infty}| / k_{\\infty}$, expressed as decimal numbers (not as percentages).\n\nFinal output format specification:\n- Your program should produce a single line of output containing the aggregated results across the two test cases, as a comma-separated list enclosed in square brackets.\n- The order of entries must be: $[k_{\\infty}^{(A)}, p^{(A)}, E_{32}^{(A)}, E_{64}^{(A)}, E_{128}^{(A)}, k_{\\infty}^{(B)}, p^{(B)}, E_{32}^{(B)}, E_{64}^{(B)}, E_{128}^{(B)}]$.\n- All outputs must be floating-point numbers.\n\nYour solution must be implementable by any developer using a modern programming language and must rely on the following fundamental bases without invoking any shortcut formulas in the problem statement:\n- Darcy’s law defining permeability as a proportionality constant between average flow and driving force in the low Reynolds number regime.\n- The asymptotic truncation error model $k(h) = k_{\\infty} + C \\, h^{p}$ for sufficiently refined grids.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard numerical verification task from the field of computational fluid dynamics, specifically using grid refinement data from a Lattice Boltzmann Method (LBM) simulation to determine the order of accuracy and extrapolate to the continuum limit. All data and conditions are provided, and the methodology is based on established principles of numerical analysis.\n\nThe fundamental principle for solving this problem is the application of Richardson extrapolation to an asymptotic error expansion. The problem states that for a sufficiently small grid spacing $h$, the numerically computed permeability $k(h)$ is related to the true continuum-limit permeability $k_{\\infty}$ by the model:\n$$k(h) = k_{\\infty} + C h^p$$\nwhere $p$ is the order of accuracy of the numerical scheme, and $C$ is a constant that depends on the scheme but not on the grid resolution $h$. Our goal is to determine the unknown parameters $p$ and $k_{\\infty}$ from a set of three measurements and then to quantify the error.\n\nWe are given three measurements of permeability, $k_1$, $k_2$, and $k_3$, corresponding to three grid spacings $h_1 > h_2 > h_3$. The grid is refined by a constant ratio $r$, such that $h_2 = h_1/r$ and $h_3 = h_2/r = h_1/r^2$. In this problem, the resolutions are given as $N_1=32$, $N_2=64$, and $N_3=128$ nodes across a channel of height $H=1$. The grid spacing $h$ is defined as $h=H/N = 1/N$. Thus, $h_1 = 1/32$, $h_2 = 1/64$, and $h_3 = 1/128$, which gives a constant refinement ratio $r = h_1/h_2 = h_2/h_3 = 2$.\n\nFrom the error model, we can write a system of three equations:\n1. $k_1 = k(h_1) = k_{\\infty} + C h_1^p$\n2. $k_2 = k(h_2) = k_{\\infty} + C h_2^p = k_{\\infty} + C (h_1/r)^p$\n3. $k_3 = k(h_3) = k_{\\infty} + C h_3^p = k_{\\infty} + C (h_2/r)^p$\n\nTo find the order of accuracy $p$, we first eliminate $k_{\\infty}$ by taking the differences between consecutive equations:\n$$k_1 - k_2 = (k_{\\infty} + C h_1^p) - (k_{\\infty} + C (h_1/r)^p) = C h_1^p (1 - r^{-p})$$\n$$k_2 - k_3 = (k_{\\infty} + C (h_1/r)^p) - (k_{\\infty} + C (h_1/r^2)^p) = C (h_1/r)^p (1 - r^{-p})$$\nNext, we take the ratio of these two differences to eliminate the constant $C$ and the dependency on $h_1$:\n$$\\frac{k_1 - k_2}{k_2 - k_3} = \\frac{C h_1^p (1 - r^{-p})}{C (h_1/r)^p (1 - r^{-p})} = \\frac{h_1^p}{(h_1/r)^p} = r^p$$\nThis provides a direct formula for the apparent order of accuracy, $p$. By taking the logarithm of both sides, we can solve for $p$:\n$$p = \\frac{\\log\\left(\\frac{k_1 - k_2}{k_2 - k_3}\\right)}{\\log(r)}$$\nOnce $p$ is determined, we can find an improved estimate for $k_{\\infty}$ through extrapolation. For higher accuracy, we use the two finest grid solutions, $k_2$ and $k_3$. The errors for these solutions are $E_2 = k_2 - k_{\\infty} \\approx C h_2^p$ and $E_3 = k_3 - k_{\\infty} \\approx C h_3^p = C (h_2/r)^p = E_2 / r^p$. From this, we have $E_2 \\approx r^p E_3$, which leads to:\n$$k_2 - k_{\\infty} \\approx r^p (k_3 - k_{\\infty})$$\n$$k_2 - k_{\\infty} \\approx r^p k_3 - r^p k_{\\infty}$$\n$$k_{\\infty} (r^p - 1) \\approx r^p k_3 - k_2$$\nSolving for $k_{\\infty}$ gives the Richardson extrapolation formula:\n$$k_{\\infty} = \\frac{k_3 r^p - k_2}{r^p - 1}$$\nThis formula provides the extrapolated value of the permeability at the continuum limit ($h \\rightarrow 0$).\n\nFinally, with the estimated $k_{\\infty}$, we can compute the relative error $E$ for each resolution $N \\in \\{32, 64, 128\\}$ (corresponding to permeabilities $k_1, k_2, k_3$) using the definition:\n$$E_i = \\frac{|k_i - k_{\\infty}|}{k_{\\infty}}$$\n\nWe now apply this procedure to the two test cases provided. The refinement ratio is $r=2$.\n\n**Test Case 1 (Scheme A):**\n- Given measured permeabilities:\n  - $k_1 = k(1/32) = 0.09895833333333333$\n  - $k_2 = k(1/64) = 0.09114583333333333$\n  - $k_3 = k(1/128) = 0.08723958333333333$\n- First, we compute the ratio of differences:\n  $$\\frac{k_1 - k_2}{k_2 - k_3} = \\frac{0.09895833333333333 - 0.09114583333333333}{0.09114583333333333 - 0.08723958333333333} = \\frac{0.0078125}{0.00390625} = 2.0$$\n- The order of accuracy $p^{(A)}$ is:\n  $$p^{(A)} = \\frac{\\log(2.0)}{\\log(2)} = 1.0$$\n- Using $p=1$, we extrapolate to find $k_{\\infty}^{(A)}$:\n  $$k_{\\infty}^{(A)} = \\frac{k_3 \\cdot 2^1 - k_2}{2^1 - 1} = 2 k_3 - k_2 = 2(0.08723958333333333) - 0.09114583333333333 = 0.08333333333333333$$\n  This value corresponds perfectly to the theoretical continuum-limit permeability $H^2/12 = 1^2/12 \\approx 0.08333...$\n- The relative errors are:\n  - $E_{32}^{(A)} = |0.09895833333333333 - 1/12| / (1/12) = 0.015625 / (1/12) = 0.1875$\n  - $E_{64}^{(A)} = |0.09114583333333333 - 1/12| / (1/12) = 0.0078125 / (1/12) = 0.09375$\n  - $E_{128}^{(A)} = |0.08723958333333333 - 1/12| / (1/12) = 0.00390625 / (1/12) = 0.046875$\n\n**Test Case 2 (Scheme B):**\n- Given measured permeabilities:\n  - $k_1 = k(1/32) = 0.08338216145833333$\n  - $k_2 = k(1/64) = 0.08334554036458333$\n  - $k_3 = k(1/128) = 0.08333638509114583$\n- First, we compute the ratio of differences:\n  $$\\frac{k_1 - k_2}{k_2 - k_3} = \\frac{0.08338216145833333 - 0.08334554036458333}{0.08334554036458333 - 0.08333638509114583} = \\frac{3.6621... \\times 10^{-5}}{9.1552... \\times 10^{-6}} = 4.0$$\n- The order of accuracy $p^{(B)}$ is:\n  $$p^{(B)} = \\frac{\\log(4.0)}{\\log(2)} = 2.0$$\n- Using $p=2$, we extrapolate to find $k_{\\infty}^{(B)}$:\n  $$k_{\\infty}^{(B)} = \\frac{k_3 \\cdot 2^2 - k_2}{2^2 - 1} = \\frac{4k_3 - k_2}{3} = \\frac{4(0.08333638509114583) - 0.08334554036458333}{3} = \\frac{0.25}{3} = 0.08333333333333333$$\n  Again, the extrapolated value is exactly $1/12$.\n- The relative errors are:\n  - $E_{32}^{(B)} = |0.08338216145833333 - 1/12| / (1/12) = 4.8828125 \\times 10^{-5} / (1/12) = 0.0005859375$\n  - $E_{64}^{(B)} = |0.08334554036458333 - 1/12| / (1/12) = 1.220703125 \\times 10^{-5} / (1/12) = 0.000146484375$\n  - $E_{128}^{(B)} = |0.08333638509114583 - 1/12| / (1/12) = 3.0517578125 \\times 10^{-6} / (1/12) = 0.00003662109375$\n\nThe program implementation will follow this logic to compute the required values for each scheme and aggregate them into the final specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the apparent order of accuracy (p), extrapolated continuum-limit\n    permeability (k_inf), and relative errors (E) for two different numerical\n    schemes based on a three-level grid refinement study.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each inner list contains measured permeabilities [k_coarse, k_medium, k_fine]\n    test_cases = [\n        # Test Case 1 (Scheme A)\n        [0.09895833333333333, 0.09114583333333333, 0.08723958333333333],\n        # Test Case 2 (Scheme B)\n        [0.08338216145833333, 0.08334554036458333, 0.08333638509114583]\n    ]\n\n    # Grid refinement ratio r = h_coarse/h_medium = h_medium/h_fine\n    r = 2.0\n\n    # List to store all results in the final required order.\n    aggregated_results = []\n\n    for case_data in test_cases:\n        k1, k2, k3 = case_data  # k1: coarse, k2: medium, k3: fine\n\n        # Step 1: Estimate the apparent order of accuracy 'p'\n        # The formula is derived from the ratio of differences in permeability:\n        # r^p = (k1 - k2) / (k2 - k3)\n        # p = log((k1 - k2) / (k2 - k3)) / log(r)\n        \n        diff1 = k1 - k2\n        diff2 = k2 - k3\n        \n        # In a well-behaved convergence study, diff2 should be > 0.\n        # If diff2 is zero or negative, the method is not applicable, but\n        # problem validation ensures the data is well-behaved.\n        if diff2 <= 0:\n             # This case is not expected based on problem description but is a good practice.\n            raise ValueError(\"Permeability is not converging monotonically.\")\n\n        ratio_of_differences = diff1 / diff2\n        p = np.log(ratio_of_differences) / np.log(r)\n\n        # Step 2: Estimate the continuum-limit permeability 'k_inf'\n        # Richardson extrapolation formula using the two finest grid results:\n        # k_inf = (k3 * r^p - k2) / (r^p - 1)\n        \n        rp = r**p\n        k_inf = (k3 * rp - k2) / (rp - 1)\n\n        # Step 3: Compute the relative errors for each resolution\n        # E = |k(h) - k_inf| / k_inf\n        \n        e1 = abs(k1 - k_inf) / k_inf\n        e2 = abs(k2 - k_inf) / k_inf\n        e3 = abs(k3 - k_inf) / k_inf\n\n        # Append the results for the current test case to the aggregate list.\n        aggregated_results.extend([k_inf, p, e1, e2, e3])\n\n    # Final print statement in the exact required format.\n    # The format is a single-line comma-separated list of floating point numbers\n    # enclosed in square brackets.\n    output_str = \",\".join(map(str, aggregated_results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Realistic, high-resolution pore-scale simulations are computationally intensive and often necessitate the use of high-performance computing (HPC) with thousands of processor cores. To use these powerful systems efficiently, it is vital to understand and analyze the parallel performance of the simulation code. In this practice, you will build an analytical performance model to evaluate strong and weak scaling efficiencies and diagnose common bottlenecks, such as communication latency and load imbalance, which are critical skills for conducting large-scale computational research .",
            "id": "4095983",
            "problem": "You are asked to model and analyze the parallel performance of a pore-scale Lattice Boltzmann Method (LBM) solver used in Digital Rock Physics (DRP) for Computational Geochemistry at the pore scale. The goal is to evaluate strong and weak scaling up to $10000$ cores and identify bottlenecks related to halo exchange and load imbalance arising from porosity variations. You must produce a complete, runnable program that computes the requested metrics from first principles using an explicit performance model grounded in standard parallel performance laws and LBM work accounting. All parameters are to be treated deterministically; no randomization is permitted.\n\nThe Lattice Boltzmann Method (LBM) is a mesoscopic approach where fluid dynamics are approximated by streaming and colliding particle distribution functions across a discrete lattice. A commonly employed stencil is the three-dimensional, $19$-velocity model (D3Q19). In pore-scale Computational Geochemistry and Digital Rock Physics (DRP), fluid nodes (pores) incur a higher computational cost per time step than solid nodes (grains), and porosity variations across subdomains induce load imbalance. Message Passing Interface (MPI) halo exchange is required each time step to pass boundary-layer data to neighboring subdomains. Parallel performance is characterized via strong scaling (fixed global problem size with increasing core count) and weak scaling (fixed local problem size per core with increasing core count). The required performance model shall be derived from the following well-tested foundational principles:\n\n- Amdahl's law for strong scaling efficiency, where the efficiency is $E_\\mathrm{strong}(N_c) = \\frac{T(1)}{N_c \\, T(N_c)}$, with $T(N_c)$ the time per time step on $N_c$ cores for a fixed global lattice and $T(1)$ the time per time step on one core for the same global lattice.\n- Gustafson's formulation for weak scaling efficiency interpreted as $E_\\mathrm{weak}(N_c) = \\frac{T_\\mathrm{local}(1)}{T_\\mathrm{local}(N_c)}$, where $T_\\mathrm{local}(N_c)$ is the time per time step when each core holds an identical local subdomain and the total global problem scales proportionally with $N_c$.\n- The Hockney model for message passing time per message: $T_\\mathrm{msg} = \\alpha + \\frac{m}{B}$, where $\\alpha$ is latency (in seconds), $m$ is the message size (in bytes), and $B$ is the bandwidth (in bytes per second). Halo exchange comprises messages across the six faces of the local subdomain (two faces per dimension).\n- Work per node accounting for LBM: fluid nodes incur a higher floating-point operation count per time step than solid nodes. Let $f_\\mathrm{fluid}$ and $f_\\mathrm{solid}$ denote the floating-point operations per node per time step for fluid and solid nodes, respectively. The per-core average compute cost depends on the porosity (fraction of fluid nodes) $p$. With porosity variation across subdomains modeled by a mean $\\bar{p}$ and a standard deviation $\\sigma_p$, the straggler core has porosity $p_\\mathrm{max} \\approx \\min\\{1, \\bar{p} + \\sigma_p\\}$, producing the longest compute time. The per-core compute time is the operations divided by the per-core floating-point throughput $F$ (in floating-point operations per second).\n- Halo width $w$ in lattice units and the number of distributions $q$ per node determine the halo volume per face. For D3Q19, $q = 19$. Assume double precision storage of distributions with $b=8$ bytes per value. For face of area $A$, the message size is $m = q \\, b \\, w \\, A$.\n\nYou must implement a program that, for each test case defined in the test suite below, performs the following computations in seconds (for all time quantities):\n1. Compute the per-step strong scaling efficiency $E_\\mathrm{strong}(N_c)$ for a fixed global lattice $(N_x, N_y, N_z)$ using $N_c$ cores.\n2. Compute the per-step weak scaling efficiency $E_\\mathrm{weak}(N_c)$ for fixed local lattice $(n_x, n_y, n_z)$ per core using $N_c$ cores.\n3. Identify the dominant bottleneck among:\n   - Compute-bound (code $0$): dominated by average compute time.\n   - Halo latency-bound (code $1$): dominated by latency contributions $\\alpha$ from halo exchange.\n   - Halo bandwidth-bound (code $2$): dominated by $\\frac{m}{B}$ contributions from halo exchange.\n   - Load-imbalance-bound (code $3$): dominated by the excess compute time due to porosity variation (difference between straggler compute time and average compute time).\n   \n   Use the largest fractional contribution to the total time per step on $N_c$ cores to assign the code. Specifically, decompose the total time $T(N_c)$ into four nonnegative components: average compute $T_\\mathrm{comp,avg}$, load imbalance $T_\\mathrm{imb} = T_\\mathrm{comp,max}-T_\\mathrm{comp,avg}$, halo latency $T_\\mathrm{lat}$, and halo bandwidth $T_\\mathrm{bw}$. The total time is $T(N_c)=T_\\mathrm{comp,max}+T_\\mathrm{comm}$ where $T_\\mathrm{comm}=T_\\mathrm{lat}+T_\\mathrm{bw}$. Assign the bottleneck to whichever of $\\left\\{\\frac{T_\\mathrm{comp,avg}}{T(N_c)}, \\frac{T_\\mathrm{lat}}{T(N_c)}, \\frac{T_\\mathrm{bw}}{T(N_c)}, \\frac{T_\\mathrm{imb}}{T(N_c)}\\right\\}$ is largest. Ties may be broken by choosing the smallest code.\n\nDomain decomposition into a three-dimensional processor grid $(P_x, P_y, P_z)$ must satisfy $P_x P_y P_z = N_c$ and be chosen to minimize the surface-to-volume ratio by making $P_x$, $P_y$, and $P_z$ as close as possible to each other. For strong scaling, the local subdomain sizes are $(L_x, L_y, L_z) = \\left(\\frac{N_x}{P_x}, \\frac{N_y}{P_y}, \\frac{N_z}{P_z}\\right)$. For weak scaling, the local subdomain sizes are fixed $(n_x, n_y, n_z)$ regardless of $N_c$; halo areas are computed directly as $A_x = n_y n_z$, $A_y = n_x n_z$, $A_z = n_x n_y$. For strong scaling, the halo areas are $A_x = L_y L_z$, $A_y = L_x L_z$, $A_z = L_x L_y$. Assume each core exchanges halos with six neighbors every step; boundary cores also send and receive messages, and messages are not overlapped with computation.\n\nAll times must be computed in seconds. The final outputs (efficiencies and bottleneck codes) are unitless. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list in the form $[E_\\mathrm{strong}, E_\\mathrm{weak}, \\text{bottleneck\\_code}]$. Efficiencies must be reported as decimal floats.\n\nUse the following scientifically plausible test suite. For each case, all symbols are as defined above:\n\n- Case $1$ (happy path, moderate core count):\n  - Global strong-scaling lattice: $(N_x, N_y, N_z) = (512, 512, 512)$.\n  - Weak-scaling local lattice per core: $(n_x, n_y, n_z) = (128, 128, 128)$.\n  - Core count: $N_c = 64$.\n  - Halo width: $w = 2$.\n  - Distributions per node: $q = 19$.\n  - Bytes per distribution value: $b = 8$.\n  - Mean porosity: $\\bar{p} = 0.25$.\n  - Porosity standard deviation: $\\sigma_p = 0.02$.\n  - Fluid-node operations per time step: $f_\\mathrm{fluid} = 400$ flops.\n  - Solid-node operations per time step: $f_\\mathrm{solid} = 50$ flops.\n  - Per-core compute throughput: $F = 1.2 \\times 10^{10}$ flops/s.\n  - Network latency: $\\alpha = 2 \\times 10^{-6}$ s.\n  - Network bandwidth: $B = 8 \\times 10^{9}$ bytes/s.\n\n- Case $2$ (extreme core count, halo exchange stress):\n  - Global strong-scaling lattice: $(N_x, N_y, N_z) = (500, 500, 500)$.\n  - Weak-scaling local lattice per core: $(n_x, n_y, n_z) = (50, 50, 50)$.\n  - Core count: $N_c = 10000$.\n  - Halo width: $w = 2$.\n  - Distributions per node: $q = 19$.\n  - Bytes per distribution value: $b = 8$.\n  - Mean porosity: $\\bar{p} = 0.25$.\n  - Porosity standard deviation: $\\sigma_p = 0.02$.\n  - Fluid-node operations per time step: $f_\\mathrm{fluid} = 400$ flops.\n  - Solid-node operations per time step: $f_\\mathrm{solid} = 50$ flops.\n  - Per-core compute throughput: $F = 1.2 \\times 10^{10}$ flops/s.\n  - Network latency: $\\alpha = 2 \\times 10^{-6}$ s.\n  - Network bandwidth: $B = 8 \\times 10^{9}$ bytes/s.\n\n- Case $3$ (high porosity variation, load imbalance stress):\n  - Global strong-scaling lattice: $(N_x, N_y, N_z) = (512, 512, 512)$.\n  - Weak-scaling local lattice per core: $(n_x, n_y, n_z) = (128, 128, 128)$.\n  - Core count: $N_c = 1024$.\n  - Halo width: $w = 2$.\n  - Distributions per node: $q = 19$.\n  - Bytes per distribution value: $b = 8$.\n  - Mean porosity: $\\bar{p} = 0.35$.\n  - Porosity standard deviation: $\\sigma_p = 0.10$.\n  - Fluid-node operations per time step: $f_\\mathrm{fluid} = 400$ flops.\n  - Solid-node operations per time step: $f_\\mathrm{solid} = 50$ flops.\n  - Per-core compute throughput: $F = 1.2 \\times 10^{10}$ flops/s.\n  - Network latency: $\\alpha = 2 \\times 10^{-6}$ s.\n  - Network bandwidth: $B = 8 \\times 10^{9}$ bytes/s.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list $[E_\\mathrm{strong}, E_\\mathrm{weak}, \\text{bottleneck\\_code}]$ corresponding to the three cases above, in order. For example: $[[0.95,0.92,0],[0.70,0.85,2],[0.80,0.65,3]]$.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of high-performance computing and computational fluid dynamics, specifically for Lattice Boltzmann Method (LBM) simulations. The problem is well-posed, providing a complete and consistent set of parameters, definitions, and deterministic models (Amdahl's law, Gustafson's formulation, Hockney model) required to compute a unique solution. The language is objective and unambiguous. We may, therefore, proceed with a formal solution.\n\nThe solution requires developing an analytical performance model for a parallel LBM solver. This model will be used to compute strong and weak scaling efficiencies and to identify performance bottlenecks for a given set of hardware and simulation parameters. The methodology is structured as follows: first, we formulate expressions for computation and communication time per time step; second, we use these to define scaling efficiencies; third, we establish a method for bottleneck identification; finally, we apply this framework to the specified test cases.\n\n### Performance Model Formulation\n\nThe total wall-clock time per time step for a single core within a parallel simulation on $N_c$ cores, denoted $T(N_c)$, is the sum of the maximum computation time across all cores and the communication time, as computation and communication are not overlapped.\n$$\nT(N_c) = T_{\\mathrm{comp,max}} + T_{\\mathrm{comm}}\n$$\n\n**1. Computation Time**\n\nThe computational work varies between fluid and solid nodes. Let $f_{\\mathrm{fluid}}$ and $f_{\\mathrm{solid}}$ be the floating-point operations (flops) per time step for a fluid and a solid node, respectively. For a local subdomain with $V_{\\mathrm{local}}$ nodes and porosity $p$ (fraction of fluid nodes), the total flops are:\n$$\n\\mathrm{Flops}_{\\mathrm{local}}(p) = V_{\\mathrm{local}} \\cdot \\left( p \\cdot f_{\\mathrm{fluid}} + (1-p) \\cdot f_{\\mathrm{solid}} \\right)\n$$\nGiven a per-core floating-point throughput of $F$ (in flops/second), the compute time for this subdomain is $\\mathrm{Flops}_{\\mathrm{local}}(p) / F$.\n\nLoad imbalance arises from porosity variations across subdomains. With a mean porosity $\\bar{p}$ and standard deviation $\\sigma_p$, the average compute time per core is based on $\\bar{p}$, while the bottleneck (straggler) core is assumed to have the maximum porosity $p_{\\mathrm{max}} = \\min\\{1, \\bar{p} + \\sigma_p\\}$.\n\nThe average compute time per core is:\n$$\nT_{\\mathrm{comp,avg}} = \\frac{V_{\\mathrm{local}}}{F} \\left( \\bar{p} \\cdot f_{\\mathrm{fluid}} + (1-\\bar{p}) \\cdot f_{\\mathrm{solid}} \\right)\n$$\nThe maximum compute time (straggler time) is:\n$$\nT_{\\mathrm{comp,max}} = \\frac{V_{\\mathrm{local}}}{F} \\left( p_{\\mathrm{max}} \\cdot f_{\\mathrm{fluid}} + (1-p_{\\mathrm{max}}) \\cdot f_{\\mathrm{solid}} \\right)\n$$\nThe time penalty due to load imbalance is the difference between the maximum and average compute times:\n$$\nT_{\\mathrm{imb}} = T_{\\mathrm{comp,max}} - T_{\\mathrm{comp,avg}}\n$$\n\n**2. Communication Time**\n\nCommunication time is modeled using the Hockney model for a single message: $T_{\\mathrm{msg}} = \\alpha + m/B$, where $\\alpha$ is network latency, $m$ is message size, and $B$ is network bandwidth. For a D3Q19 LBM simulation, halo exchange is required on all six faces of a cubic or cuboidal subdomain. The message size $m$ for a face of area $A$ (in lattice units) depends on the halo width $w$, the number of distributions per node $q$, and the bytes per value $b$:\n$$\nm = q \\cdot b \\cdot w \\cdot A\n$$\nAssuming each core communicates with six neighbors, the total communication time $T_{\\mathrm{comm}}$ is the sum of the time for six messages (one for each face: $+x, -x, +y, -y, +z, -z$). For a local subdomain of size $(l_x, l_y, l_z)$, the face areas are $A_x = l_y l_z$, $A_y = l_x l_z$, and $A_z = l_x l_y$. The total communication time is decomposed into latency and bandwidth components:\n$$\nT_{\\mathrm{comm}} = T_{\\mathrm{lat}} + T_{\\mathrm{bw}}\n$$\nwhere:\n$$\nT_{\\mathrm{lat}} = 6\\alpha\n$$\n$$\nT_{\\mathrm{bw}} = \\frac{2 \\cdot (q \\cdot b \\cdot w \\cdot A_x)}{B} + \\frac{2 \\cdot (q \\cdot b \\cdot w \\cdot A_y)}{B} + \\frac{2 \\cdot (q \\cdot b \\cdot w \\cdot A_z)}{B} = \\frac{2qbw}{B}(A_x+A_y+A_z)\n$$\nThe factor of $2$ for each dimension accounts for sending to two neighbors (e.g., in the $+x$ and $-x$ directions).\n\nThe total time per step $T(N_c)$ can be expressed as the sum of its four fundamental components:\n$$\nT(N_c) = T_{\\mathrm{comp,avg}} + T_{\\mathrm{imb}} + T_{\\mathrm{lat}} + T_{\\mathrm{bw}}\n$$\n\n### Scaling Efficiency Calculations\n\n**1. Strong Scaling**\n\nStrong scaling efficiency $E_{\\mathrm{strong}}$ measures performance for a fixed global problem size $(N_x, N_y, N_z)$ as the number of cores $N_c$ increases. It is defined as:\n$$\nE_{\\mathrm{strong}}(N_c) = \\frac{T(1)}{N_c \\cdot T(N_c)}\n$$\n- $T(1)$ is the serial execution time on one core. For this case, the entire global lattice $(N_x, N_y, N_z)$ is the local domain. There is no communication ($T_{\\mathrm{comm}}=0$) and no load imbalance ($T_{\\mathrm{imb}}=0$).\n- $T(N_c)$ is the parallel execution time on $N_c$ cores. The global lattice is decomposed into a $P_x \\times P_y \\times P_z$ processor grid, where $P_x P_y P_z = N_c$. The dimensions $(P_x, P_y, P_z)$ are chosen to be as close to $N_c^{1/3}$ as possible to minimize the surface-to-volume ratio and thus communication. The local subdomain size for each core is $(L_x, L_y, L_z) = (\\frac{N_x}{P_x}, \\frac{N_y}{P_y}, \\frac{N_z}{P_z})$. These dimensions are used to calculate $V_{\\mathrm{local}}$ and the face areas for the time components.\n\n**2. Weak Scaling**\n\nWeak scaling efficiency $E_{\\mathrm{weak}}$ measures performance for a fixed local problem size per core, $(n_x, n_y, n_z)$, as the number of cores $N_c$ increases. The global problem size thus scales with $N_c$. It is defined as:\n$$\nE_{\\mathrm{weak}}(N_c) = \\frac{T_{\\mathrm{local}}(1)}{T_{\\mathrm{local}}(N_c)}\n$$\n- $T_{\\mathrm{local}}(1)$ is the time to run the fixed-size local problem on a single core. There is no communication or load imbalance.\n- $T_{\\mathrm{local}}(N_c)$ is the time per step for one core in an $N_c$-core simulation. The local subdomain size is fixed at $(n_x, n_y, n_z)$, which is used to calculate $V_{\\mathrm{local}}$ and face areas. The compute and communication components are then calculated as described previously. In an ideal weak scaling scenario, if there were no communication or imbalance overheads, $T_{\\mathrm{local}}(N_c)$ would equal $T_{\\mathrm{local}}(1)$, yielding an efficiency of $1$.\n\n### Bottleneck Identification\n\nThe dominant performance bottleneck is identified for the strong scaling case at $N_c$ cores. It is determined by finding the largest fractional contribution to the total time $T(N_c)$. The fractions for average compute, load imbalance, latency, and bandwidth are calculated as follows:\n$$\nf_{\\mathrm{comp}} = \\frac{T_{\\mathrm{comp,avg}}}{T(N_c)}, \\quad f_{\\mathrm{imb}} = \\frac{T_{\\mathrm{imb}}}{T(N_c)}, \\quad f_{\\mathrm{lat}} = \\frac{T_{\\mathrm{lat}}}{T(N_c)}, \\quad f_{\\mathrm{bw}} = \\frac{T_{\\mathrm{bw}}}{T(N_c)}\n$$\nThe bottleneck is assigned a code based on which fraction is the maximum: $0$ for compute, $1$ for latency, $2$ for bandwidth, and $3$ for load imbalance. Ties are broken by selecting the smallest code.\n\n### Algorithmic Implementation\n\nThe solution is implemented as a Python program. A key component is a function that determines the optimal 3D processor grid $(P_x, P_y, P_z)$ for a given number of cores $N_c$ by finding three integer factors of $N_c$ that are closest to $N_c^{1/3}$. The main logic then proceeds for each test case by systematically calculating the time components, the two scaling efficiencies, and the bottleneck code, and formats the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_proc_dims(n_cores):\n    \"\"\"\n    Decomposes n_cores into three integer factors Px, Py, Pz that are as\n    close to each other as possible (as cubical as possible).\n    \"\"\"\n    if n_cores == 1:\n        return 1, 1, 1\n\n    best_factors = None\n    min_surface_area = float('inf')\n\n    # Iterate through possible factors for the first dimension\n    for p_x in range(1, int(n_cores**0.5) + 2):\n        if n_cores % p_x == 0:\n            rem1 = n_cores // p_x\n            # Iterate through possible factors for the second dimension\n            for p_y in range(p_x, int(rem1**0.5) + 2):\n                if rem1 % p_y == 0:\n                    p_z = rem1 // p_y\n                    factors = sorted((p_x, p_y, p_z))\n                    # Minimize surface area proxy Px*Py + Px*Pz + Py*Pz\n                    surface_area = factors[0]*factors[1] + factors[0]*factors[2] + factors[1]*factors[2]\n                    if surface_area < min_surface_area:\n                        min_surface_area = surface_area\n                        best_factors = tuple(factors)\n    \n    # A more direct approach that's often sufficient and faster\n    px, py, pz = 1,1,1\n    root = int(round(n_cores**(1/3.0)))\n    for p1 in range(root, 0, -1):\n        if n_cores % p1 == 0:\n            rem = n_cores // p1\n            rem_root = int(round(rem**0.5))\n            for p2 in range(rem_root, 0, -1):\n                if rem % p2 == 0:\n                    p3 = rem // p2\n                    return tuple(sorted((p1, p2, p3)))\n    \n    # Fallback to the more thorough search if the heuristic fails, though it shouldn't for these cases.\n    return best_factors\n\ndef calculate_performance(case):\n    \"\"\"\n    Calculates performance metrics for a single test case.\n    \"\"\"\n    # Unpack parameters\n    (Nx, Ny, Nz) = case[\"global_strong_lattice\"]\n    (nx, ny, nz) = case[\"weak_local_lattice\"]\n    Nc = case[\"Nc\"]\n    w, q, b = case[\"w\"], case[\"q\"], case[\"b\"]\n    p_bar, sigma_p = case[\"p_bar\"], case[\"sigma_p\"]\n    f_fluid, f_solid = case[\"f_fluid\"], case[\"f_solid\"]\n    F, alpha, B = case[\"F\"], case[\"alpha\"], case[\"B\"]\n    \n    p_max = min(1.0, p_bar + sigma_p)\n\n    # --- Helper function to calculate time components ---\n    def get_time_components(local_dims):\n        lx, ly, lz = local_dims\n        V_local = lx * ly * lz\n\n        # Compute part\n        flops_avg = V_local * (p_bar * f_fluid + (1 - p_bar) * f_solid)\n        flops_max = V_local * (p_max * f_fluid + (1 - p_max) * f_solid)\n        \n        T_comp_avg = flops_avg / F\n        T_comp_max = flops_max / F\n        T_imb = T_comp_max - T_comp_avg\n\n        # Communication part\n        Ax, Ay, Az = ly * lz, lx * lz, lx * ly\n        \n        m_x = q * b * w * Ax\n        m_y = q * b * w * Ay\n        m_z = q * b * w * Az\n\n        T_lat = 6 * alpha\n        T_bw = (2 * m_x + 2 * m_y + 2 * m_z) / B\n        \n        return T_comp_avg, T_imb, T_lat, T_bw\n\n    # --- Strong Scaling Calculation ---\n    # Serial time T(1)\n    V_global = Nx * Ny * Nz\n    flops_serial = V_global * (p_bar * f_fluid + (1 - p_bar) * f_solid)\n    T1 = flops_serial / F\n\n    # Parallel time T(Nc)\n    Px, Py, Pz = get_proc_dims(Nc)\n    # Re-order to match global lattice shape if non-cubic\n    proc_dims = sorted((Px, Py, Pz), reverse=True)\n    Lx, Ly, Lz = Nx / proc_dims[0], Ny / proc_dims[1], Nz / proc_dims[2]\n    \n    T_ca_s, T_i_s, T_l_s, T_b_s = get_time_components((Lx, Ly, Lz))\n    T_Nc_strong = T_ca_s + T_i_s + T_l_s + T_b_s\n    \n    E_strong = T1 / (Nc * T_Nc_strong) if Nc * T_Nc_strong > 0 else 0.0\n\n    # --- Weak Scaling Calculation ---\n    # \"Serial\" time on local block T_local(1)\n    V_local_weak = nx * ny * nz\n    flops_local_serial = V_local_weak * (p_bar * f_fluid + (1 - p_bar) * f_solid)\n    T_local_1 = flops_local_serial / F\n    \n    # Time on one core in parallel ensemble T_local(Nc)\n    T_ca_w, T_i_w, T_l_w, T_b_w = get_time_components((nx, ny, nz))\n    T_local_Nc = T_ca_w + T_i_w + T_l_w + T_b_w\n    \n    E_weak = T_local_1 / T_local_Nc if T_local_Nc > 0 else 0.0\n    \n    # --- Bottleneck Identification (based on strong scaling run at Nc) ---\n    T_total = T_Nc_strong\n    if T_total == 0:\n        bottleneck_code = 0\n    else:\n        # Per problem statement, bottleneck is based on T_comp,avg, T_lat, T_bw, and T_imb\n        time_components = np.array([T_ca_s, T_l_s, T_b_s, T_i_s])\n        bottleneck_code = np.argmax(time_components) \n\n    return [E_strong, E_weak, bottleneck_code]\n\n\ndef solve():\n    test_cases = [\n        # Case 1\n        {\n            \"global_strong_lattice\": (512, 512, 512), \"weak_local_lattice\": (128, 128, 128),\n            \"Nc\": 64, \"w\": 2, \"q\": 19, \"b\": 8, \"p_bar\": 0.25, \"sigma_p\": 0.02,\n            \"f_fluid\": 400, \"f_solid\": 50, \"F\": 1.2e10, \"alpha\": 2e-6, \"B\": 8e9\n        },\n        # Case 2\n        {\n            \"global_strong_lattice\": (500, 500, 500), \"weak_local_lattice\": (50, 50, 50),\n            \"Nc\": 10000, \"w\": 2, \"q\": 19, \"b\": 8, \"p_bar\": 0.25, \"sigma_p\": 0.02,\n            \"f_fluid\": 400, \"f_solid\": 50, \"F\": 1.2e10, \"alpha\": 2e-6, \"B\": 8e9\n        },\n        # Case 3\n        {\n            \"global_strong_lattice\": (512, 512, 512), \"weak_local_lattice\": (128, 128, 128),\n            \"Nc\": 1024, \"w\": 2, \"q\": 19, \"b\": 8, \"p_bar\": 0.35, \"sigma_p\": 0.10,\n            \"f_fluid\": 400, \"f_solid\": 50, \"F\": 1.2e10, \"alpha\": 2e-6, \"B\": 8e9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_performance(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., [[0.95,0.92,0],[0.70,0.85,2],[0.80,0.65,3]]\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}