{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a foundational exercise in parameter estimation. You will take a classic thermodynamic relationship, the van 't Hoff equation, linearize it, and apply a weighted least squares regression to estimate enthalpy and entropy from experimental data. This problem  is fundamental for learning how to properly incorporate measurement uncertainty into a model-fitting procedure and extract meaningful physical parameters.",
            "id": "4082921",
            "problem": "You are tasked with implementing an inverse modeling procedure in computational geochemistry to estimate the standard reaction enthalpy $\\Delta H^\\circ$ and standard reaction entropy $\\Delta S^\\circ$ from equilibrium constant data as a function of temperature. The fundamental base consists of the relation between the standard Gibbs energy $\\Delta G^\\circ$ and the equilibrium constant $K$ for a reaction, defined by $\\Delta G^\\circ = - R T \\ln K$, and the thermodynamic identity $\\Delta G^\\circ = \\Delta H^\\circ - T \\Delta S^\\circ$ valid for a reaction with temperature-independent $\\Delta H^\\circ$ and $\\Delta S^\\circ$ over a limited temperature range. Combining these two well-tested formulas implies a linear relationship between $\\ln K$ and $1/T$, where $T$ is the absolute temperature in Kelvin. Measurements of $K$ have associated uncertainties that must be propagated to uncertainties in $\\ln K$ and used as weights in a statistically sound weighted linear regression.\n\nYour program must:\n- Formulate the weighted least squares problem for the linear model relating $\\ln K$ and $1/T$.\n- Use measurement uncertainties in $K$ to compute uncertainties in $\\ln K$ through first-order error propagation, with $\\sigma_{\\ln K,i} \\approx \\sigma_{K,i} / K_i$ for each measurement $i$.\n- Solve the weighted linear regression problem to estimate the slope and intercept, then convert these into $\\Delta H^\\circ$ and $\\Delta S^\\circ$ using the gas constant $R$.\n- Compute the posterior one-standard-deviation (one-sigma) uncertainties of $\\Delta H^\\circ$ and $\\Delta S^\\circ$ based on the weighted least squares estimator covariance, assuming independent Gaussian errors with known variances.\n- Express $\\Delta H^\\circ$ in kilojoules per mole and $\\Delta S^\\circ$ in joules per mole per kelvin, both rounded to six decimal places.\n\nUse the following gas constant: $R = 8.31446261815324 \\ \\mathrm{J \\ mol^{-1} \\ K^{-1}}$.\n\nImplement the solution for the following test suite of data sets. Each test case provides temperatures $T$ in Kelvin, equilibrium constants $K$ (dimensionless), and absolute uncertainties $\\sigma_K$ (same dimension as $K$), for multiple measurements. All numbers below must be treated exactly as given. The data sets are:\n\n- Case A (general, varied uncertainties):\n  - $T$: $\\{298.0, 310.0, 330.0, 360.0\\}$ (Kelvin)\n  - $K$: $\\{3000.0, 1200.0, 300.0, 120.0\\}$ (dimensionless)\n  - $\\sigma_K$: $\\{150.0, 48.0, 30.0, 9.6\\}$ (dimensionless)\n- Case B (boundary case: minimal number of points):\n  - $T$: $\\{298.0, 350.0\\}$ (Kelvin)\n  - $K$: $\\{2500.0, 500.0\\}$ (dimensionless)\n  - $\\sigma_K$: $\\{125.0, 25.0\\}$ (dimensionless)\n- Case C (edge case: one measurement with very small uncertainty dominating weights):\n  - $T$: $\\{280.0, 300.0, 320.0, 340.0, 360.0\\}$ (Kelvin)\n  - $K$: $\\{5000.0, 2000.0, 900.0, 420.0, 200.0\\}$ (dimensionless)\n  - $\\sigma_K$: $\\{500.0, 200.0, 9.0, 42.0, 20.0\\}$ (dimensionless)\n- Case D (edge case: temperatures tightly clustered):\n  - $T$: $\\{300.0, 301.0, 302.0, 303.0\\}$ (Kelvin)\n  - $K$: $\\{2000.0, 1900.0, 1800.0, 1700.0\\}$ (dimensionless)\n  - $\\sigma_K$: $\\{50.0, 50.0, 50.0, 50.0\\}$ (dimensionless)\n\nYour program must output a single line containing a comma-separated list of results for all cases, enclosed in square brackets. Each case’s result must be a list in the form $[\\Delta H^\\circ \\ (\\mathrm{kJ \\ mol^{-1}}), \\Delta S^\\circ \\ (\\mathrm{J \\ mol^{-1} \\ K^{-1}}), \\sigma_{\\Delta H^\\circ} \\ (\\mathrm{kJ \\ mol^{-1}}), \\sigma_{\\Delta S^\\circ} \\ (\\mathrm{J \\ mol^{-1} \\ K^{-1}})]$, with each numerical value rounded to six decimal places. For example, the final output must look like $[[h_1,s_1,sh_1,ss_1],[h_2,s_2,sh_2,ss_2],[h_3,s_3,sh_3,ss_3],[h_4,s_4,sh_4,ss_4]]$, where each $h_i$, $s_i$, $sh_i$, and $ss_i$ is a float.\n\nAll inputs are in Kelvin and dimensionless units as indicated. All outputs must be expressed in $\\mathrm{kJ \\ mol^{-1}}$ for $\\Delta H^\\circ$ and $\\mathrm{J \\ mol^{-1} \\ K^{-1}}$ for $\\Delta S^\\circ$, as specified. Angles are not involved. Percentages must not be used; uncertainties must be provided and used as absolute values.\n\nYour implementation must be self-contained and must not read any external input. It must adhere to weighted linear regression principles that are applicable generally across modern programming languages, focusing purely on the mathematical logic stated above.",
            "solution": "The problem requires the estimation of standard reaction enthalpy, $\\Delta H^\\circ$, and standard reaction entropy, $\\Delta S^\\circ$, from experimental data of the equilibrium constant, $K$, at various temperatures, $T$. This is a classic inverse problem in physical chemistry, and its solution is grounded in fundamental thermodynamic principles and statistical regression analysis.\n\n### 1. Theoretical Formulation: The van 't Hoff Equation\n\nThe analysis begins with two fundamental equations from chemical thermodynamics. The first relates the standard Gibbs free energy of reaction, $\\Delta G^\\circ$, to the equilibrium constant, $K$:\n$$ \\Delta G^\\circ = - R T \\ln K $$\nwhere $R$ is the universal gas constant and $T$ is the absolute temperature in Kelvin.\n\nThe second is the definition of Gibbs free energy in terms of enthalpy and entropy:\n$$ \\Delta G^\\circ = \\Delta H^\\circ - T \\Delta S^\\circ $$\nThe problem assumes that over the limited temperature range of the experiments, both $\\Delta H^\\circ$ and $\\Delta S^\\circ$ are constant.\n\nBy equating these two expressions for $\\Delta G^\\circ$, we obtain:\n$$ - R T \\ln K = \\Delta H^\\circ - T \\Delta S^\\circ $$\nTo establish a linear relationship suitable for regression analysis, we rearrange this equation by dividing all terms by $-RT$:\n$$ \\ln K = -\\frac{\\Delta H^\\circ}{R T} + \\frac{T \\Delta S^\\circ}{R T} $$\n$$ \\ln K = \\left(-\\frac{\\Delta H^\\circ}{R}\\right) \\frac{1}{T} + \\left(\\frac{\\Delta S^\\circ}{R}\\right) $$\nThis equation is in the form of a straight line, $y = mx + c$, where:\n- The dependent variable is $y = \\ln K$.\n- The independent variable is $x = 1/T$.\n- The slope is $m = -\\frac{\\Delta H^\\circ}{R}$.\n- The y-intercept is $c = \\frac{\\Delta S^\\circ}{R}$.\n\n### 2. Statistical Method: Weighted Least Squares (WLS)\n\nThe experimental measurements are for $K$ and have associated uncertainties, $\\sigma_K$. To perform a linear regression on the transformed variables $y = \\ln K$ and $x = 1/T$, we must propagate these uncertainties. Using a first-order Taylor expansion (error propagation), the uncertainty $\\sigma_{y,i}$ in each value $y_i = \\ln K_i$ is given by:\n$$ \\sigma_{y,i} = \\sigma_{\\ln K,i} \\approx \\left| \\frac{d(\\ln K)}{dK} \\right|_{K=K_i} \\sigma_{K,i} = \\frac{1}{K_i} \\sigma_{K,i} $$\nSince these uncertainties are not uniform across the data points, a simple least squares regression is inappropriate. The statistically correct approach is Weighted Least Squares (WLS), where each data point's contribution to the fit is weighted by the inverse of its variance. The weight for the $i$-th data point is:\n$$ w_i = \\frac{1}{\\sigma_{y,i}^2} = \\left(\\frac{K_i}{\\sigma_{K,i}}\\right)^2 $$\nThe WLS method finds the parameters $m$ and $c$ that minimize the weighted sum of squared residuals, $\\chi^2$:\n$$ \\chi^2(m, c) = \\sum_{i=1}^{n} w_i (y_i - (mx_i + c))^2 $$\nwhere $n$ is the number of data points.\n\nThis minimization problem is most efficiently solved using matrix algebra. We define the following:\n- The vector of observations, $y = [y_1, y_2, \\dots, y_n]^T = [\\ln K_1, \\ln K_2, \\dots, \\ln K_n]^T$.\n- The design matrix, $X$, which contains a column of ones for the intercept and a column for the independent variable values:\n$$ X = \\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix} = \\begin{pmatrix} 1 & 1/T_1 \\\\ 1 & 1/T_2 \\\\ \\vdots & \\vdots \\\\ 1 & 1/T_n \\end{pmatrix} $$\n- The vector of parameters, $\\beta = [c, m]^T$.\n- The diagonal weight matrix, $W$, with the weights $w_i$ on its diagonal:\n$$ W = \\begin{pmatrix} w_1 & 0 & \\dots & 0 \\\\ 0 & w_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & w_n \\end{pmatrix} $$\nThe WLS estimate for the parameter vector, $\\hat{\\beta}$, is given by the normal equations:\n$$ \\hat{\\beta} = (X^T W X)^{-1} X^T W y $$\n\n### 3. Parameter and Uncertainty Estimation\n\nOnce the best-fit parameters $\\hat{\\beta} = [\\hat{c}, \\hat{m}]^T$ are determined, we can calculate the estimates for the thermodynamic quantities:\n$$ \\hat{\\Delta H^\\circ} = -\\hat{m} \\cdot R $$\n$$ \\hat{\\Delta S^\\circ} = \\hat{c} \\cdot R $$\nThe units for $\\hat{\\Delta H^\\circ}$ will be $\\mathrm{J \\ mol^{-1}}$, which must be converted to $\\mathrm{kJ \\ mol^{-1}}$ by dividing by $1000$. The units for $\\hat{\\Delta S^\\circ}$ will be $\\mathrm{J \\ mol^{-1} \\ K^{-1}}$, as required.\n\nThe problem also requires the one-standard-deviation uncertainties in these estimates. Assuming the model is correct and the measurement errors are independent and Gaussian with known variances (as specified), the covariance matrix of the parameter estimator $\\hat{\\beta}$ is given by:\n$$ \\text{Cov}(\\hat{\\beta}) = (X^T W X)^{-1} $$\nThis is a $2 \\times 2$ matrix:\n$$ \\text{Cov}(\\hat{\\beta}) = \\begin{pmatrix} \\sigma_c^2 & \\text{cov}(c, m) \\\\ \\text{cov}(c, m) & \\sigma_m^2 \\end{pmatrix} $$\nThe diagonal elements, $\\sigma_c^2$ and $\\sigma_m^2$, are the variances of the estimated intercept and slope, respectively. The one-standard-deviation uncertainties are their square roots, $\\sigma_c$ and $\\sigma_m$.\n\nThe uncertainties in $\\Delta H^\\circ$ and $\\Delta S^\\circ$ are found by propagating the uncertainties from $\\hat{m}$ and $\\hat{c}$:\n$$ \\sigma_{\\Delta H^\\circ} = \\left| \\frac{\\partial (\\Delta H^\\circ)}{\\partial m} \\right| \\sigma_m = R \\cdot \\sigma_m $$\n$$ \\sigma_{\\Delta S^\\circ} = \\left| \\frac{\\partial (\\Delta S^\\circ)}{\\partial c} \\right| \\sigma_c = R \\cdot \\sigma_c $$\nThe uncertainty $\\sigma_{\\Delta H^\\circ}$ must also be converted to $\\mathrm{kJ \\ mol^{-1}}$.\n\n### 4. Algorithm Summary\n\nFor each provided test case, the following computational steps are executed:\n\n1.  Given the data sets $\\{T_i, K_i, \\sigma_{K,i}\\}$, construct the arrays for the independent variable $x_i = 1/T_i$ and the dependent variable $y_i = \\ln K_i$.\n2.  Calculate the variance for each $y_i$ as $\\sigma_{y,i}^2 = (\\sigma_{K,i} / K_i)^2$.\n3.  Construct the diagonal weight matrix $W$ where $W_{ii} = w_i = 1/\\sigma_{y,i}^2$.\n4.  Construct the design matrix $X$.\n5.  Solve for the parameter vector $\\hat{\\beta} = [\\hat{c}, \\hat{m}]^T$ using $\\hat{\\beta} = (X^T W X)^{-1} X^T W y$.\n6.  Calculate the covariance matrix $\\text{Cov}(\\hat{\\beta}) = (X^T W X)^{-1}$.\n7.  Extract the parameter variances: $\\sigma_c^2 = \\text{Cov}(\\hat{\\beta})_{0,0}$ and $\\sigma_m^2 = \\text{Cov}(\\hat{\\beta})_{1,1}$.\n8.  Calculate the thermodynamic estimates using the gas constant $R = 8.31446261815324 \\ \\mathrm{J \\ mol^{-1} \\ K^{-1}}$:\n    - $\\hat{\\Delta H^\\circ} = (-\\hat{m} \\cdot R) / 1000 \\quad (\\mathrm{in \\ kJ \\ mol^{-1}})$\n    - $\\hat{\\Delta S^\\circ} = \\hat{c} \\cdot R \\quad (\\mathrm{in \\ J \\ mol^{-1} \\ K^{-1}})$\n9.  Calculate the corresponding one-sigma uncertainties:\n    - $\\sigma_{\\Delta H^\\circ} = (\\sqrt{\\sigma_m^2} \\cdot R) / 1000 \\quad (\\mathrm{in \\ kJ \\ mol^{-1}})$\n    - $\\sigma_{\\Delta S^\\circ} = \\sqrt{\\sigma_c^2} \\cdot R \\quad (\\mathrm{in \\ J \\ mol^{-1} \\ K^{-1}})$\n10. Format the four resulting values, rounded to six decimal places, into the specified list structure for the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the inverse modeling problem for multiple test cases to estimate\n    thermodynamic parameters from equilibrium constant data using weighted\n    linear regression.\n    \"\"\"\n\n    # Universal gas constant in J mol^-1 K^-1\n    R = 8.31446261815324\n\n    test_cases = [\n        # Case A (general, varied uncertainties)\n        {\n            \"T\": np.array([298.0, 310.0, 330.0, 360.0]),\n            \"K\": np.array([3000.0, 1200.0, 300.0, 120.0]),\n            \"sigma_K\": np.array([150.0, 48.0, 30.0, 9.6])\n        },\n        # Case B (boundary case: minimal number of points)\n        {\n            \"T\": np.array([298.0, 350.0]),\n            \"K\": np.array([2500.0, 500.0]),\n            \"sigma_K\": np.array([125.0, 25.0])\n        },\n        # Case C (edge case: one measurement with very small uncertainty dominating)\n        {\n            \"T\": np.array([280.0, 300.0, 320.0, 340.0, 360.0]),\n            \"K\": np.array([5000.0, 2000.0, 900.0, 420.0, 200.0]),\n            \"sigma_K\": np.array([500.0, 200.0, 9.0, 42.0, 20.0])\n        },\n        # Case D (edge case: temperatures tightly clustered)\n        {\n            \"T\": np.array([300.0, 301.0, 302.0, 303.0]),\n            \"K\": np.array([2000.0, 1900.0, 1800.0, 1700.0]),\n            \"sigma_K\": np.array([50.0, 50.0, 50.0, 50.0])\n        }\n    ]\n\n    all_results = []\n\n    for case_data in test_cases:\n        T = case_data[\"T\"]\n        K = case_data[\"K\"]\n        sigma_K = case_data[\"sigma_K\"]\n\n        # 1. Transform variables for linear model y = c + mx\n        x = 1.0 / T        # Independent variable: 1/T\n        y = np.log(K)      # Dependent variable: ln(K)\n\n        # 2. Propagate uncertainty to ln(K) and calculate weights\n        # sigma_y = sigma_K / K\n        # weight w = 1 / sigma_y^2 = (K / sigma_K)^2\n        sigma_y_sq = (sigma_K / K)**2\n        w = 1.0 / sigma_y_sq\n\n        # 3. Construct matrices for Weighted Least Squares (WLS)\n        # Design matrix X\n        X = np.vstack((np.ones_like(x), x)).T\n        \n        # Weight matrix W\n        W = np.diag(w)\n\n        # 4. Solve for parameters beta_hat = [c, m]^T\n        # beta_hat = (X^T W X)^-1 X^T W y\n        try:\n            XTWX = X.T @ W @ X\n            XTWX_inv = np.linalg.inv(XTWX)\n            XTWy = X.T @ W @ y\n            beta_hat = XTWX_inv @ XTWy\n        except np.linalg.LinAlgError:\n            # Handle cases where the matrix is singular, though not expected\n            # with the given test data.\n            all_results.append([np.nan, np.nan, np.nan, np.nan])\n            continue\n\n        c_hat = beta_hat[0]  # Intercept\n        m_hat = beta_hat[1]  # Slope\n\n        # 5. Calculate covariance matrix of parameters\n        # Cov(beta_hat) = (X^T W X)^-1\n        cov_beta = XTWX_inv\n        sigma_c_sq = cov_beta[0, 0]\n        sigma_m_sq = cov_beta[1, 1]\n        \n        sigma_c = np.sqrt(sigma_c_sq)\n        sigma_m = np.sqrt(sigma_m_sq)\n\n        # 6. Calculate thermodynamic quantities and their uncertainties\n        # Delta_H = -m * R\n        # Delta_S = c * R\n        delta_H = -m_hat * R\n        delta_S = c_hat * R\n        \n        sigma_delta_H = sigma_m * R\n        sigma_delta_S = sigma_c * R\n\n        # 7. Convert units (H to kJ/mol) and round\n        delta_H_kJ = delta_H / 1000.0\n        sigma_delta_H_kJ = sigma_delta_H / 1000.0\n\n        # Store results for this case\n        case_result = [\n            round(delta_H_kJ, 6),\n            round(delta_S, 6),\n            round(sigma_delta_H_kJ, 6),\n            round(sigma_delta_S, 6)\n        ]\n        all_results.append(case_result)\n\n    # 8. Format the final output string as specified: [[...],[...]] without spaces\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond the standard error estimates from linear regression, this exercise introduces a more powerful and general method for uncertainty analysis. By constructing the profile likelihood for the activation energy ($E_a$) in an Arrhenius model, you will learn how to derive confidence intervals that are robust to the nonlinearities often encountered in real-world data. This practice  is key to understanding how the shape of the likelihood surface, not just its peak, defines our confidence in an estimated parameter.",
            "id": "4082889",
            "problem": "You are given a dataset of temperature-dependent reaction rate constants for a single-step geochemical reaction obeying the Arrhenius law. The Arrhenius law states that the rate constant $k(T)$ as a function of temperature $T$ (in Kelvin) is given by $k(T) = A \\exp(-E_a/(R T))$, where $A$ is the pre-exponential factor (in s$^{-1}$), $E_a$ is the activation energy (in J/mol), and $R$ is the universal gas constant equal to $R = 8.314462618$ J mol$^{-1}$ K$^{-1}$. Assume that measurements of the rate constants $k_i$ at temperatures $T_i$ are subject to multiplicative noise that is log-normal. Equivalently, the log-transformed responses $y_i = \\ln(k_i)$ can be modeled as Gaussian with unknown variance, and the Arrhenius model becomes linear in the log-space: $y_i = \\ln(A) - E_a/R \\cdot (1/T_i) + \\varepsilon_i$, with $\\varepsilon_i$ independent Gaussian noise.\n\nYour tasks are:\n- Formulate the maximum likelihood estimation problem under the above assumptions, treating the variance in the log-space as an unknown parameter and treating $A$ and $E_a$ as parameters to be estimated.\n- Construct the profile likelihood for $E_a$ by maximizing the likelihood over the nuisance parameter $A$ for each fixed $E_a$.\n- Implement the likelihood ratio statistic for testing the fixed value of $E_a$ against the unconstrained maximum likelihood estimate, and use this to compute a confidence interval for $E_a$ (in kJ/mol) at a specified confidence level. Interpret this interval under nonlinearity (non-quadratic profile nature).\n\nYour program must:\n- Use the provided datasets (test suite) and, for each dataset, evaluate the profile likelihood over a grid of activation energy values $E_a$ (in kJ/mol) spanning the specified bounds and with a specified step size. For each grid point, maximize over $A$ and compute the likelihood ratio statistic relative to the unconstrained maximum likelihood solution.\n- Use the large-sample likelihood ratio calibration for one degree of freedom to define the confidence set for $E_a$ as the set of values for which the likelihood ratio statistic does not exceed the $(\\text{confidence level})$ quantile of the chi-square distribution with one degree of freedom.\n- Return the smallest closed interval within the grid that contains the confidence set. If the confidence set is empty on the provided grid, return the grid bounds as the interval. If the confidence set connects to a boundary, the interval endpoints may lie on the grid bounds.\n\nPhysical units:\n- Input temperatures must be in Kelvin.\n- Input rate constants must be in s$^{-1}$.\n- Output confidence interval endpoints must be expressed in kJ/mol, rounded to six decimal places.\n\nAngle units are not applicable. Any fraction or percentage must be represented as a decimal.\n\nTest suite:\nFor each case, you are provided with temperatures $T_i$ (in K), measured rate constants $k_i$ (in s$^{-1}$), the activation energy grid bounds (in kJ/mol), and the grid step (in kJ/mol). The confidence level is given as a decimal. The universal gas constant value is $R = 8.314462618$ J mol$^{-1}$ K$^{-1}$.\n\n- Case 1 (well-spaced temperatures, moderate noise; \"happy path\"):\n    - $T = [280, 300, 320, 340, 360]$ K\n    - $k = [5.751 \\times 10^{-6}, 3.938 \\times 10^{-5}, 1.28 \\times 10^{-4}, 6.258 \\times 10^{-4}, 2.376 \\times 10^{-3}]$ s$^{-1}$\n    - $E_a$ grid: lower bound $30$ kJ/mol, upper bound $120$ kJ/mol, step $0.1$ kJ/mol\n    - Confidence level: $0.95$\n\n- Case 2 (narrow temperature span; increased nonlinearity):\n    - $T = [295, 300, 305]$ K\n    - $k = [7.00 \\times 10^{-4}, 9.31 \\times 10^{-4}, 1.429 \\times 10^{-3}]$ s$^{-1}$\n    - $E_a$ grid: lower bound $20$ kJ/mol, upper bound $120$ kJ/mol, step $0.5$ kJ/mol\n    - Confidence level: $0.95$\n\n- Case 3 (high noise, grid-bound confidence set possible):\n    - $T = [290, 310, 330, 350]$ K\n    - $k = [1.18 \\times 10^{-8}, 9.90 \\times 10^{-9}, 1.08 \\times 10^{-6}, 2.28 \\times 10^{-7}]$ s$^{-1}$\n    - $E_a$ grid: lower bound $40$ kJ/mol, upper bound $90$ kJ/mol, step $0.25$ kJ/mol\n    - Confidence level: $0.95$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the lower and upper confidence interval endpoints (in kJ/mol, rounded to six decimal places) for Case 1, then Case 2, then Case 3, in that order. For example:\n- $\"[E1_{\\text{lower}},E1_{\\text{upper}},E2_{\\text{lower}},E2_{\\text{upper}},E3_{\\text{lower}},E3_{\\text{upper}}]\"$.",
            "solution": "### I. Theoretical Formulation\n\nThe problem requires the estimation of a confidence interval for the activation energy, $E_a$, of a geochemical reaction. The reaction's rate constant, $k$, is known to follow the Arrhenius law as a function of absolute temperature $T$:\n$$ k(T) = A \\exp\\left(-\\frac{E_a}{RT}\\right) $$\nwhere $A$ is the pre-exponential factor, $E_a$ is the activation energy, and $R$ is the universal gas constant.\n\nTo facilitate linear analysis, the model is transformed by taking the natural logarithm:\n$$ \\ln(k(T)) = \\ln(A) - \\frac{E_a}{R} \\frac{1}{T} $$\nWe are given a set of $n$ measurements $(T_i, k_i)$. Let $y_i = \\ln(k_i)$ and $x_i = 1/T_i$. The model can be expressed as a simple linear regression:\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $$\nHere, the regression parameters are related to the physical parameters by $\\beta_0 = \\ln(A)$ and $\\beta_1 = -E_a/R$. The error terms $\\varepsilon_i$ are assumed to be independent and identically distributed normal random variables with mean $0$ and unknown variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim N(0, \\sigma^2)$.\n\nThe likelihood function for the observed data $\\mathbf{y} = (y_1, ..., y_n)$ given the parameters $\\beta_0$, $\\beta_1$, and $\\sigma^2$ is:\n$$ L(\\beta_0, \\beta_1, \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - (\\beta_0 + \\beta_1 x_i))^2}{2\\sigma^2}\\right) $$\nThe log-likelihood function is:\n$$ \\ln L(\\beta_0, \\beta_1, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2 $$\nMaximizing this log-likelihood is equivalent to minimizing the sum of squared residuals, $SSR(\\beta_0, \\beta_1) = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2$.\n\nFor any fixed $\\beta_0$ and $\\beta_1$, the maximum likelihood estimate (MLE) of $\\sigma^2$ is $\\hat{\\sigma}^2 = SSR(\\beta_0, \\beta_1)/n$. Substituting this back into the log-likelihood gives the profile log-likelihood for $\\beta_0$ and $\\beta_1$:\n$$ \\ln L_p(\\beta_0, \\beta_1) = -\\frac{n}{2}\\left(\\ln(2\\pi) + \\ln\\left(\\frac{SSR(\\beta_0, \\beta_1)}{n}\\right) + 1\\right) $$\nMaximizing this is equivalent to minimizing $SSR(\\beta_0, \\beta_1)$.\n\n### II. Unconstrained Maximum Likelihood Estimation\n\nThe unconstrained MLEs for $\\beta_0$ and $\\beta_1$ are found by standard Ordinary Least Squares (OLS) regression. Let $\\bar{x}$ and $\\bar{y}$ be the sample means of $x_i$ and $y_i$. Define $S_{xx} = \\sum(x_i - \\bar{x})^2$ and $S_{xy} = \\sum(x_i - \\bar{x})(y_i - \\bar{y})$. The MLEs are:\n$$ \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}, \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nThe minimum sum of squared residuals for the unconstrained model is $SSR_{unconstrained} = \\sum(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2$. The maximized log-likelihood is $\\ln L_{max} = \\ln L_p(\\hat{\\beta}_0, \\hat{\\beta}_1)$.\n\n### III. Profile Likelihood and Confidence Interval for $E_a$\n\nWe are interested in the confidence interval for $E_a$. We construct the profile likelihood for $E_a$. For a fixed value of the activation energy, $E_a = E_{a,0}$, the slope parameter is also fixed: $\\beta_1 = \\beta_{1,0} = -E_{a,0}/R$. The model becomes $y_i - \\beta_{1,0}x_i = \\beta_0 + \\varepsilon_i$.\n\nThe nuisance parameter $\\beta_0$ is estimated by maximizing the likelihood for the fixed $\\beta_{1,0}$. This is equivalent to minimizing $SSR(\\beta_0) = \\sum(y_i - \\beta_{1,0}x_i - \\beta_0)^2$. The solution is the conditional MLE for $\\beta_0$:\n$$ \\hat{\\beta}_0(E_{a,0}) = \\frac{1}{n}\\sum(y_i - \\beta_{1,0}x_i) = \\bar{y} - \\beta_{1,0}\\bar{x} $$\nThe corresponding sum of squared residuals for this constrained model is:\n$$ SSR_{constrained}(E_{a,0}) = \\sum (y_i - (\\hat{\\beta}_0(E_{a,0}) + \\beta_{1,0} x_i))^2 $$\nThis can be computed efficiently as $SSR_{constrained}(E_{a,0}) = S_{yy} - 2\\beta_{1,0}S_{xy} + \\beta_{1,0}^2 S_{xx}$, where $S_{yy} = \\sum(y_i - \\bar{y})^2$.\n\nThe profile log-likelihood for $E_a$ is the maximized log-likelihood for each fixed $E_a$:\n$$ \\ln L(E_{a,0}) = -\\frac{n}{2}\\left(\\ln(2\\pi) + \\ln\\left(\\frac{SSR_{constrained}(E_{a,0})}{n}\\right) + 1\\right) $$\nThe likelihood ratio test statistic for testing the hypothesis $H_0: E_a = E_{a,0}$ is:\n$$ \\Lambda(E_{a,0}) = -2(\\ln L(E_{a,0}) - \\ln L_{max}) = n \\ln\\left(\\frac{SSR_{constrained}(E_{a,0})}{SSR_{unconstrained}}\\right) $$\nAccording to Wilks' theorem, this statistic asymptotically follows a chi-square distribution with one degree of freedom, $\\chi^2_1$, under the null hypothesis.\n\nA $(1-\\alpha)$ confidence interval for $E_a$ is formed by the set of all values $E_{a,0}$ that are not rejected by the LRT at significance level $\\alpha$. This is the set:\n$$ \\{ E_a \\mid \\Lambda(E_a) \\le C \\} $$\nwhere $C$ is the $(1-\\alpha)$ quantile of the $\\chi^2_1$ distribution (e.g., for a $95\\%$ confidence level, $\\alpha=0.05$, $C \\approx 3.841$). This method is superior to simpler methods like the Wald interval, as it does not assume that the log-likelihood function is quadratic. It naturally produces asymmetric intervals when the profile likelihood is skewed, which often occurs with small datasets or highly correlated parameters, such as in Arrhenius analysis with a narrow temperature range.\n\n### IV. Algorithmic Implementation\n\nFor each test case, the following procedure is implemented:\n1.  The input data $(T_i, k_i)$ are transformed to $(x_i, y_i) = (1/T_i, \\ln(k_i))$.\n2.  The number of data points $n$ and the universal gas constant $R=8.314462618$ J mol$^{-1}$ K$^{-1}$ are defined.\n3.  The summary statistics $\\bar{x}$, $\\bar{y}$, $S_{xx}$, $S_{yy}$, and $S_{xy}$ are computed.\n4.  The unconstrained MLEs $\\hat{\\beta}_1$ and $\\hat{\\beta}_0$ are calculated, along with the minimum sum of squared residuals, $SSR_{unconstrained} = S_{yy} - \\hat{\\beta}_1 S_{xy}$.\n5.  The critical value $C$ is determined from the $\\chi^2_1$ distribution for the given confidence level.\n6.  A grid of $E_a$ values is generated according to the specified bounds and step size.\n7.  For each grid value $E_{a,0}$ (in kJ/mol):\n    a.  Convert $E_{a,0}$ to J/mol by multiplying by $1000$.\n    b.  Calculate the corresponding fixed slope $\\beta_{1,0} = -E_{a,0}/R$.\n    c.  Compute the constrained sum of squared residuals $SSR_{constrained}(E_{a,0}) = S_{yy} - 2\\beta_{1,0}S_{xy} + \\beta_{1,0}^2 S_{xx}$.\n    d.  Compute the likelihood ratio statistic $\\Lambda(E_{a,0}) = n \\ln(SSR_{constrained}(E_{a,0}) / SSR_{unconstrained})$.\n    e.  If $\\Lambda(E_{a,0}) \\le C$, the grid value $E_{a,0}$ is added to a set of acceptable values.\n8.  If the set of acceptable values is empty, the interval is defined by the grid boundaries. Otherwise, the final confidence interval is determined by the minimum and maximum values in the set.\n9.  The lower and upper bounds of the interval are rounded to six decimal places and formatted for output.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef calculate_confidence_interval(T, k, Ea_grid_params, conf_level):\n    \"\"\"\n    Calculates the confidence interval for Ea using the profile likelihood method.\n    \"\"\"\n    R = 8.314462618  # J mol^-1 K^-1\n    T_arr = np.array(T, dtype=np.float64)\n    k_arr = np.array(k, dtype=np.float64)\n    n = len(T_arr)\n\n    # 1. Transform data to linear form: y = b0 + b1*x\n    # y = ln(k), x = 1/T\n    # b0 = ln(A), b1 = -Ea/R\n    y = np.log(k_arr)\n    x = 1.0 / T_arr\n\n    # 2. Calculate summary statistics for OLS\n    x_bar = np.mean(x)\n    y_bar = np.mean(y)\n    \n    S_xx = np.sum((x - x_bar)**2)\n    S_yy = np.sum((y - y_bar)**2)\n    S_xy = np.sum((x - x_bar) * (y - y_bar))\n\n    # 3. Unconstrained Maximum Likelihood Estimation\n    # These are the standard OLS estimates\n    beta1_hat = S_xy / S_xx\n    # beta0_hat = y_bar - beta1_hat * x_bar # Not strictly needed for LRT\n    \n    # Calculate Sum of Squared Residuals for the unconstrained model\n    SSR_unconstrained = S_yy - beta1_hat * S_xy\n    if SSR_unconstrained <= 0: # Should not happen with real data\n        SSR_unconstrained = 1e-20\n\n    # 4. Get critical value from Chi-square distribution\n    # df = 1 because we are constraining one parameter (Ea, which is equivalent to b1)\n    critical_value = chi2.ppf(conf_level, df=1)\n\n    # 5. Grid search for Ea\n    Ea_lower_kj, Ea_upper_kj, Ea_step_kj = Ea_grid_params\n    Ea_grid_kj = np.arange(Ea_lower_kj, Ea_upper_kj + Ea_step_kj, Ea_step_kj)\n    \n    valid_Ea_values = []\n\n    for Ea_kj in Ea_grid_kj:\n        Ea_j = Ea_kj * 1000.0  # Convert from kJ/mol to J/mol\n        \n        # This is the fixed slope under Ho: Ea = Ea_j\n        beta1_0 = -Ea_j / R\n        \n        # Calculate SSR for the constrained model (where slope is fixed)\n        # SSR_constrained = sum(( (y-y_bar) - b1_0*(x-x_bar) )^2)\n        # which expands to:\n        SSR_constrained = S_yy - 2 * beta1_0 * S_xy + beta1_0**2 * S_xx\n        \n        # LRT statistic\n        if SSR_constrained < SSR_unconstrained:\n            # Due to floating point issues, might be slightly smaller.\n            # The test statistic must be non-negative.\n            lrt_statistic = 0.0\n        else:\n            lrt_statistic = n * np.log(SSR_constrained / SSR_unconstrained)\n        \n        if lrt_statistic <= critical_value:\n            valid_Ea_values.append(Ea_kj)\n\n    # 6. Determine interval from the set of valid Ea values\n    if not valid_Ea_values:\n        return Ea_lower_kj, Ea_upper_kj\n    else:\n        return min(valid_Ea_values), max(valid_Ea_values)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": [280, 300, 320, 340, 360],\n            \"k\": [5.751e-6, 3.938e-5, 1.28e-4, 6.258e-4, 2.376e-3],\n            \"grid\": (30, 120, 0.1),\n            \"level\": 0.95\n        },\n        {\n            \"T\": [295, 300, 305],\n            \"k\": [7.00e-4, 9.31e-4, 1.429e-3],\n            \"grid\": (20, 120, 0.5),\n            \"level\": 0.95\n        },\n        {\n            \"T\": [290, 310, 330, 350],\n            \"k\": [1.18e-8, 9.90e-9, 1.08e-6, 2.28e-7],\n            \"grid\": (40, 90, 0.25),\n            \"level\": 0.95\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        lower_bound, upper_bound = calculate_confidence_interval(\n            case[\"T\"], case[\"k\"], case[\"grid\"], case[\"level\"]\n        )\n        results.append(lower_bound)\n        results.append(upper_bound)\n    \n    # Format results to six decimal places for the final output string\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After estimating parameters for a given model, a critical next step is to ask whether a more complex model is truly better. This practice tackles the core issue of model selection by comparing an ideal-solution model to the more complex Debye-Hückel model. You will use the Akaike (AIC) and Bayesian (BIC) Information Criteria  to formalize the principle of parsimony, learning to balance a model's goodness-of-fit against its complexity to avoid overfitting.",
            "id": "4082910",
            "problem": "You are tasked with model selection in computational geochemistry using inverse modeling, parameter estimation, and optimization. Consider mean activity coefficients for symmetric electrolytes in aqueous solution at standard laboratory temperature where the extended Debye-Hückel theory is applicable. You are to compare an ideal-solution model versus a Debye-Hückel model for the same datasets using the principles of maximum likelihood and information criteria.\n\nFundamental base for modeling:\n- For a symmetric electrolyte with cation and anion valences of equal magnitude $z$ and opposite sign, the ideal-solution model assumes unit activity coefficients, so the mean activity coefficient satisfies $\\gamma_{\\pm} = 1$, hence $\\log_{10}\\gamma_{\\pm} = 0$.\n- The extended Debye-Hückel model for symmetric electrolytes gives the mean activity coefficient in base-$10$ logarithm form as\n$$\n\\log_{10}\\gamma_{\\pm}(I; z, a) = -\\frac{A\\, z^2 \\sqrt{I}}{1 + B\\, a \\sqrt{I}},\n$$\nwhere $I$ is the ionic strength in $\\text{mol}\\,\\text{L}^{-1}$, $z$ is the magnitude of the ionic charge, $a$ is an effective ion-size parameter in $\\text{\\AA}$, and $A$ and $B$ are solvent-dependent constants at the specified temperature. Use $A = 0.509$ and $B = 0.328$ for water at $25\\,^{\\circ}\\text{C}$.\n\nInverse modeling and information criteria requirements:\n- Assume independent, Gaussian-distributed measurement errors on the observed responses $y_i = \\log_{10}\\gamma_{\\pm}$ with known standard deviations $\\sigma_i$ for each observation. Under this assumption, the log-likelihood for a model prediction $m_i$ at observation $i$ is a sum of Gaussian log-likelihoods.\n- Define the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) mathematically from first principles of maximum likelihood and model dimensionality, and apply them to compare the ideal-solution model and the Debye-Hückel model for each dataset. The ideal-solution model has zero fitted parameters ($k = 0$). For the Debye-Hückel model in this problem, treat $a$ as the only fitted parameter ($k = 1$), with all other quantities specified. When computing the information criteria, use the number of observations $n$ appropriate to each dataset.\n\nParameter estimation setup:\n- For the Debye-Hückel model, estimate $a$ by maximum likelihood under the stated Gaussian error model with known $\\sigma_i$. Constrain $a$ to the physically plausible interval $[1, 20]$ in $\\text{\\AA}$.\n\nTest suite:\nApply your methodology to the following four cases. For each case, the independent variable is ionic strength $I$ in $\\text{mol}\\,\\text{L}^{-1}$, the charge magnitude is $z$, the observed response $y$ is $\\log_{10}\\gamma_{\\pm}$ (dimensionless), and the observation standard deviations are $\\sigma$ (dimensionless). The provided $y$ values are scientifically plausible and consistent with the extended Debye-Hückel model at $25\\,^{\\circ}\\text{C}$ using the stated $A$ and $B$ and an underlying true $a$; no further transformation is required.\n\n- Case (a) low ionic strength, monovalent symmetric electrolyte:\n  - $I = [0.001, 0.010, 0.050]$\n  - $z = 1$\n  - $y = [-0.015460, -0.045000, -0.087970]$\n  - $\\sigma = [0.010, 0.010, 0.010]$\n  - Units: $I$ in $\\text{mol}\\,\\text{L}^{-1}$, $a$ in $\\text{\\AA}$, $\\sigma$ dimensionless.\n\n- Case (b) low-to-moderate ionic strength, divalent symmetric electrolyte:\n  - $I = [0.001, 0.010, 0.030]$\n  - $z = 2$\n  - $y = [-0.058950, -0.157200, -0.233200]$\n  - $\\sigma = [0.020, 0.020, 0.020]$\n  - Units: $I$ in $\\text{mol}\\,\\text{L}^{-1}$, $a$ in $\\text{\\AA}$, $\\sigma$ dimensionless.\n\n- Case (c) boundary condition at zero ionic strength, monovalent symmetric electrolyte:\n  - $I = [0.000]$\n  - $z = 1$\n  - $y = [0.000000]$\n  - $\\sigma = [0.001]$\n  - Units: $I$ in $\\text{mol}\\,\\text{L}^{-1}$, $a$ in $\\text{\\AA}$, $\\sigma$ dimensionless.\n\n- Case (d) higher ionic strength approaching the validity limit of extended Debye-Hückel, monovalent symmetric electrolyte:\n  - $I = [0.080, 0.100]$\n  - $z = 1$\n  - $y = [-0.104300, -0.113800]$\n  - $\\sigma = [0.020, 0.020]$\n  - Units: $I$ in $\\text{mol}\\,\\text{L}^{-1}$, $a$ in $\\text{\\AA}$, $\\sigma$ dimensionless.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the given order, output the pair of floating-point differences $[\\Delta \\text{AIC}, \\Delta \\text{BIC}]$ where $\\Delta \\text{AIC} = \\text{AIC}_{\\text{Debye-H\\\"uckel}} - \\text{AIC}_{\\text{Ideal}}$ and $\\Delta \\text{BIC} = \\text{BIC}_{\\text{Debye-H\\\"uckel}} - \\text{BIC}_{\\text{Ideal}}$. Aggregate the four case results into a list of lists, in the form\n$$\n[[\\Delta \\text{AIC}_a, \\Delta \\text{BIC}_a], [\\Delta \\text{AIC}_b, \\Delta \\text{BIC}_b], [\\Delta \\text{AIC}_c, \\Delta \\text{BIC}_c], [\\Delta \\text{AIC}_d, \\Delta \\text{BIC}_d]].\n$$\nNo units are required for these final values. The single printed line should match the textual list representation of this nested list exactly.",
            "solution": "The task is to compare two models for the mean activity coefficient ($\\gamma_{\\pm}$) of a symmetric electrolyte: an ideal-solution model and the extended Debye-Hückel model. The comparison will be performed using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), based on parameter estimation via maximum likelihood.\n\nThe core of the methodology rests on the principle of maximum likelihood. For a set of $n$ independent observations $y_i$ with associated Gaussian errors of known standard deviation $\\sigma_i$, the likelihood of a model that produces predictions $m_i$ is given by the product of individual probability densities:\n$$\nL = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma_i} \\exp\\left(-\\frac{(y_i - m_i)^2}{2\\sigma_i^2}\\right)\n$$\nMaximizing this likelihood is equivalent to minimizing its negative logarithm. The negative log-likelihood, $\\mathcal{L} = -\\ln L$, is:\n$$\n\\mathcal{L} = \\frac{n}{2}\\ln(2\\pi) + \\sum_{i=1}^{n}\\ln(\\sigma_i) + \\frac{1}{2} \\sum_{i=1}^{n} \\left(\\frac{y_i - m_i}{\\sigma_i}\\right)^2\n$$\nThe parameters of a model are estimated by minimizing this function. The summation term is the weighted sum of squared residuals, commonly denoted as $\\chi^2$.\n$$\n\\chi^2 = \\sum_{i=1}^{n} \\left(\\frac{y_i - m_i}{\\sigma_i}\\right)^2\n$$\nThus, maximizing the likelihood is equivalent to minimizing $\\chi^2$. Let the minimum value of this function for a given model be $\\chi^2_{\\text{min}}$.\n\nThe two models to be compared are:\n1.  **The Ideal-Solution Model ($M_0$)**: This model posits that $\\log_{10}\\gamma_{\\pm} = 0$. The model predictions are $m_{0,i} = 0$ for all observations. This model has no adjustable parameters, so its dimensionality is $k_0 = 0$. The goodness-of-fit is calculated as:\n    $$\n    \\chi^2_{\\text{min, Ideal}} = \\sum_{i=1}^{n} \\left(\\frac{y_i - 0}{\\sigma_i}\\right)^2 = \\sum_{i=1}^{n} \\frac{y_i^2}{\\sigma_i^2}\n    $$\n\n2.  **The Extended Debye-Hückel Model ($M_1$)**: This model gives the prediction for $\\log_{10}\\gamma_{\\pm}$ as a function of ionic strength $I$, charge magnitude $z$, and an effective ion-size parameter $a$:\n    $$\n    m_{1,i}(a) = \\log_{10}\\gamma_{\\pm}(I_i; z, a) = -\\frac{A\\, z^2 \\sqrt{I_i}}{1 + B\\, a \\sqrt{I_i}}\n    $$\n    The constants are given as $A = 0.509$ and $B = 0.328$. The parameter $a$ is to be estimated from the data, so this model has a dimensionality of $k_1 = 1$. The value of $a$ is found by minimizing the $\\chi^2$ function for this model:\n    $$\n    \\chi^2_{M_1}(a) = \\sum_{i=1}^{n} \\left(\\frac{y_i - m_{1,i}(a)}{\\sigma_i}\\right)^2\n    $$\n    This is a one-dimensional optimization problem over the interval $a \\in [1, 20]\\, \\text{\\AA}$. The minimum value found is $\\chi^2_{\\text{min, DH}}$.\n\nModel selection is performed using information criteria, which penalize models for complexity (number of parameters) to avoid overfitting.\nThe Akaike Information Criterion (AIC) is defined as:\n$$\n\\text{AIC} = -2 \\ln(L_{\\text{max}}) + 2k\n$$\nwhere $L_{\\text{max}}$ is the maximum likelihood. Substituting the expression for the log-likelihood involving $\\chi^2_{\\text{min}}$:\n$$\n\\text{AIC} = 2\\mathcal{L}_{\\text{min}} + 2k = n\\ln(2\\pi) + 2\\sum_{i=1}^{n}\\ln(\\sigma_i) + \\chi^2_{\\text{min}} + 2k\n$$\nThe Bayesian Information Criterion (BIC) is defined as:\n$$\n\\text{BIC} = -2 \\ln(L_{\\text{max}}) + k\\ln(n) = n\\ln(2\\pi) + 2\\sum_{i=1}^{n}\\ln(\\sigma_i) + \\chi^2_{\\text{min}} + k\\ln(n)\n$$\nwhere $n$ is the number of observations.\n\nFor comparing two models, we compute the differences $\\Delta\\text{AIC}$ and $\\Delta\\text{BIC}$. The constant terms involving $\\pi$ and $\\sigma_i$ cancel out.\n$$\n\\Delta \\text{AIC} = \\text{AIC}_{\\text{DH}} - \\text{AIC}_{\\text{Ideal}} = (\\chi^2_{\\text{min, DH}} + 2k_1) - (\\chi^2_{\\text{min, Ideal}} + 2k_0)\n$$\nWith $k_1=1$ and $k_0=0$:\n$$\n\\Delta \\text{AIC} = \\chi^2_{\\text{min, DH}} - \\chi^2_{\\text{min, Ideal}} + 2\n$$\nSimilarly for BIC:\n$$\n\\Delta \\text{BIC} = \\text{BIC}_{\\text{DH}} - \\text{BIC}_{\\text{Ideal}} = (\\chi^2_{\\text{min, DH}} + k_1\\ln n) - (\\chi^2_{\\text{min, Ideal}} + k_0\\ln n)\n$$\nWith $k_1=1$ and $k_0=0$:\n$$\n\\Delta \\text{BIC} = \\chi^2_{\\text{min, DH}} - \\chi^2_{\\text{min, Ideal}} + \\ln n\n$$\nNegative values for these differences suggest that the more complex Debye-Hückel model provides a better description of the data, even after penalizing for the additional parameter.\n\nThe computational procedure for each test case is as follows:\n1.  Convert the input data lists for $I$, $y$, and $\\sigma$ into numerical arrays. Determine the number of observations $n$.\n2.  Calculate $\\chi^2_{\\text{min, Ideal}}$ using its definition.\n3.  Define the Debye-Hückel model and the corresponding $\\chi^2_{M_1}(a)$ as an objective function.\n4.  Use a numerical optimization routine to find the value $\\hat{a}$ in the range $[1, 20]$ that minimizes $\\chi^2_{M_1}(a)$, yielding $\\chi^2_{\\text{min, DH}}$.\n5.  Use the calculated $\\chi^2$ values to compute $\\Delta \\text{AIC}$ and $\\Delta \\text{BIC}$.\n\nA special case is Case (c), where $I = [0.0]$. At zero ionic strength, both models predict $\\log_{10}\\gamma_{\\pm} = 0$. Since the observation is also $y=[0.0]$, both models fit perfectly. Therefore, $\\chi^2_{\\text{min, Ideal}} = 0$ and $\\chi^2_{\\text{min, DH}} = 0$. The number of observations is $n=1$. The differences become:\n$$\n\\Delta \\text{AIC} = 0 - 0 + 2 = 2\n$$\n$$\n\\Delta \\text{BIC} = 0 - 0 + \\ln(1) = 0\n$$\nThis demonstrates that for a single data point where both models are exact, AIC penalizes the more complex model, while BIC, with its $\\ln(n)$ penalty term, does not.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for four test cases in computational geochemistry.\n    Compares an ideal-solution model with the extended Debye-Hückel model using\n    Akaike (AIC) and Bayesian (BIC) information criteria.\n    \"\"\"\n    \n    A = 0.509  # Debye-Hückel constant for water at 25 C\n    B = 0.328  # Debye-Hückel constant for water at 25 C\n    a_bounds = (1.0, 20.0) # Physical bounds for ion-size parameter in Angstroms\n\n    test_cases = [\n        # Case (a) low ionic strength, monovalent\n        {\n            \"I\": np.array([0.001, 0.010, 0.050]),\n            \"z\": 1.0,\n            \"y\": np.array([-0.015460, -0.045000, -0.087970]),\n            \"sigma\": np.array([0.010, 0.010, 0.010]),\n        },\n        # Case (b) low-to-moderate ionic strength, divalent\n        {\n            \"I\": np.array([0.001, 0.010, 0.030]),\n            \"z\": 2.0,\n            \"y\": np.array([-0.058950, -0.157200, -0.233200]),\n            \"sigma\": np.array([0.020, 0.020, 0.020]),\n        },\n        # Case (c) boundary condition at zero ionic strength, monovalent\n        {\n            \"I\": np.array([0.000]),\n            \"z\": 1.0,\n            \"y\": np.array([0.000000]),\n            \"sigma\": np.array([0.001]),\n        },\n        # Case (d) higher ionic strength, monovalent\n        {\n            \"I\": np.array([0.080, 0.100]),\n            \"z\": 1.0,\n            \"y\": np.array([-0.104300, -0.113800]),\n            \"sigma\": np.array([0.020, 0.020]),\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        I, z, y, sigma = case[\"I\"], case[\"z\"], case[\"y\"], case[\"sigma\"]\n        n = len(y)\n\n        # 1. Ideal-Solution Model (M0)\n        # Model prediction is m = 0. k=0 parameters.\n        chi2_ideal = np.sum((y / sigma)**2)\n        k_ideal = 0\n\n        # 2. Extended Debye-Hückel Model (M1)\n        # Model prediction is a function of parameter 'a'. k=1 parameter.\n        def debye_huckel_model(I_vals, z_val, a_val, A_val, B_val):\n            sqrt_I = np.sqrt(I_vals)\n            # This calculation is robust against I=0, as sqrt(0)=0 and the numerator becomes 0.\n            return -A_val * z_val**2 * sqrt_I / (1 + B_val * a_val * sqrt_I)\n\n        def objective_function(a, I_vals, z_val, y_vals, sigma_vals, A_val, B_val):\n            y_pred = debye_huckel_model(I_vals, z_val, a, A_val, B_val)\n            residuals = y_vals - y_pred\n            return np.sum((residuals / sigma_vals)**2)\n\n        # Handle the special case where I=0, y=0.\n        # The objective function is identically zero, so optimization is not needed.\n        if np.all(I == 0):\n             chi2_min_dh = 0.0\n        else:\n            opt_result = minimize_scalar(\n                fun=objective_function,\n                bounds=a_bounds,\n                args=(I, z, y, sigma, A, B),\n                method='bounded'\n            )\n            chi2_min_dh = opt_result.fun\n\n        k_dh = 1\n\n        # 3. Calculate Information Criteria Differences\n        # delta_AIC = (chi2_dh + 2*k_dh) - (chi2_ideal + 2*k_ideal)\n        delta_aic = (chi2_min_dh - chi2_ideal) + 2 * (k_dh - k_ideal)\n\n        # delta_BIC = (chi2_dh + k_dh*ln(n)) - (chi2_ideal + k_ideal*ln(n))\n        # Handle n=1 where ln(1)=0\n        if n > 0:\n            ln_n = np.log(n)\n        else:\n            ln_n = 0 # should not happen with given data\n        delta_bic = (chi2_min_dh - chi2_ideal) + (k_dh - k_ideal) * ln_n\n\n        results.append([delta_aic, delta_bic])\n        \n    # The required output is the string representation of the list of lists.\n    # The placeholder `f\"[{','.join(map(str, results))}]\"` doesn't produce standard\n    # list-of-list formatting with spaces. Direct str() casting does.\n    # e.g., str([[1.2, 3.4], [5.6, 7.8]]) is '[[1.2, 3.4], [5.6, 7.8]]'\n    # The prompt's example format shows spaces after commas, so str() is correct.\n    # The example print format in the prompt template might be misleading for nested lists.\n    # I will adapt it to produce the correct textual representation.\n    \n    # Correction to match textual representation with spaces as in standard `str(list)`\n    final_output_str = str(results)\n    \n    # A custom formatter to match the format from the prompt's `f\"[{','.join...}]\"` exactly\n    # just in case that's the intended interpretation (no spaces).\n    # final_output_str = f\"[{','.join(str(item) for item in results)}]\"\n    # This results in '[[...], [...]...]' with spaces inside but not between sublists.\n    # `str(list)` is the most standard interpretation of \"textual list representation\".\n    \n    print(final_output_str)\n\n\nsolve()\n```"
        }
    ]
}