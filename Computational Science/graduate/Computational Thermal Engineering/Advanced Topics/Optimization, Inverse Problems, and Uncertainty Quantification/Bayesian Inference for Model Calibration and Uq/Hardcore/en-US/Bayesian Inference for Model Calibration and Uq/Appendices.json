{
    "hands_on_practices": [
        {
            "introduction": "Before deploying computationally expensive sampling methods, it's often valuable to obtain a quick, analytical approximation of the posterior distribution. This exercise introduces the Laplace approximation, a powerful technique that characterizes the posterior with a Gaussian centered at its peak, the Maximum A Posteriori (MAP) estimate. By working through this problem , you will practice finding the MAP and using the local curvature (Hessian) of the log-posterior to estimate parameter uncertainty, and even assess the validity of the Gaussian assumption itself.",
            "id": "3938004",
            "problem": "A calibration problem in steady convective heat transfer is considered. A flat isothermal plate at surface temperature experiences a convective heat flux modeled as $q = h(\\theta)\\,\\Delta T$, where $\\Delta T$ is a known temperature difference and $h(\\theta)$ is a convective heat transfer coefficient parameterized by a single scalar parameter $\\theta$. Motivated by near-exponential sensitivity of some convective correlations to an exponent-like parameter, assume $h(\\theta) = h_{\\mathrm{ref}} \\exp(\\theta)$, with $h_{\\mathrm{ref}}$ known. Noisy measurements $\\{y_i\\}_{i=1}^N$ of the heat flux under known temperature differences $\\{\\Delta T_i\\}_{i=1}^N$ satisfy\n$$\ny_i \\;=\\; h(\\theta)\\,\\Delta T_i \\;+\\; \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\\quad i=1,\\dots,N,\n$$\nwith $\\sigma^2$ known, and the $\\varepsilon_i$ statistically independent. A Gaussian prior is placed on $\\theta$: $\\theta \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$, with $\\mu_0$ and $\\sigma_0^2$ known.\n\nYou are given $N=3$ experiments with $h_{\\mathrm{ref}}=100\\,\\text{W m}^{-2}\\text{K}^{-1}$, $\\mu_0=0$, $\\sigma_0=0.5$, $\\sigma=100\\,\\text{W m}^{-2}$, and\n$$\n\\Delta T_1=10\\,\\text{K},\\quad \\Delta T_2=20\\,\\text{K},\\quad \\Delta T_3=30\\,\\text{K},\n$$\nwith measured fluxes\n$$\ny_1=1000\\,\\text{W m}^{-2},\\quad y_2=2000\\,\\text{W m}^{-2},\\quad y_3=3000\\,\\text{W m}^{-2}.\n$$\n\nUsing Bayes’ theorem and the Gaussian noise and prior models:\n\n1) Derive the negative log posterior $U(\\theta)$ up to an additive constant in terms of $\\{y_i\\}$, $\\{\\Delta T_i\\}$, $h_{\\mathrm{ref}}$, $\\sigma^2$, $\\mu_0$, and $\\sigma_0^2$.\n\n2) Identify the maximum a posteriori estimate $\\hat{\\theta}$ by solving the stationarity condition $U'(\\hat{\\theta})=0$.\n\n3) Derive the Hessian $U''(\\theta)$ of the negative log posterior and the Laplace (second-order) Gaussian approximation of the posterior around $\\hat{\\theta}$, i.e., the Gaussian with mean $\\hat{\\theta}$ and variance $[U''(\\hat{\\theta})]^{-1}$.\n\n4) To assess accuracy for a mildly nonlinear model, compute the standardized non-Gaussianity indicator\n$$\n\\eta \\;=\\; \\frac{|U^{(3)}(\\hat{\\theta})|}{\\big(U''(\\hat{\\theta})\\big)^{3/2}},\n$$\nwhere $U^{(3)}(\\theta)$ is the third derivative of $U(\\theta)$. Evaluate $\\eta$ numerically for the given data.\n\nExpress your final result for $\\eta$ as a single real number, rounded to four significant figures. No units are required for $\\eta$ since it is dimensionless.",
            "solution": "The user has provided a well-posed problem in Bayesian inference applied to a heat transfer model. All necessary data and model forms are specified, and the questions are mathematically precise. The problem is scientifically grounded and internally consistent.\n\nThe problem requires a four-part solution involving the derivation of the posterior distribution, finding the maximum a posteriori (MAP) estimate, characterizing the second-order Gaussian approximation to the posterior, and calculating a non-Gaussianity indicator.\n\nThe physical model for the heat flux is $q_i = h(\\theta) \\Delta T_i$, with the parameterization $h(\\theta) = h_{\\mathrm{ref}} \\exp(\\theta)$. The measurement model is $y_i = h(\\theta) \\Delta T_i + \\varepsilon_i$, where the noise terms $\\varepsilon_i$ are independent and identically distributed (i.i.d.) as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. A Gaussian prior is specified for the parameter $\\theta$, $\\theta \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n\n**Part 1: Derivation of the Negative Log Posterior $U(\\theta)$**\n\nAccording to Bayes' theorem, the posterior probability density function (PDF) $p(\\theta|\\{y_i\\}_{i=1}^N)$ is proportional to the product of the likelihood function $p(\\{y_i\\}_{i=1}^N|\\theta)$ and the prior PDF $p(\\theta)$.\n$$\np(\\theta|\\{y_i\\}) \\propto p(\\{y_i\\}|\\theta) p(\\theta)\n$$\nThe likelihood for a single measurement $y_i$ is given by the Gaussian noise model:\n$$\np(y_i|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - h(\\theta)\\Delta T_i)^2}{2\\sigma^2}\\right)\n$$\nSince the measurements are statistically independent, the joint likelihood is the product of individual likelihoods:\n$$\np(\\{y_i\\}|\\theta) = \\prod_{i=1}^{N} p(y_i|\\theta) = \\left(2\\pi\\sigma^2\\right)^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h(\\theta)\\Delta T_i)^2\\right)\n$$\nThe prior PDF for $\\theta$ is given as:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\n$$\nThe posterior PDF is therefore proportional to:\n$$\np(\\theta|\\{y_i\\}) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)^2 - \\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\n$$\nThe negative log posterior, $U(\\theta) = -\\ln(p(\\theta|\\{y_i\\}))$, up to an additive constant, is:\n$$\nU(\\theta) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)^2 + \\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\n$$\n\n**Part 2: Maximum a Posteriori (MAP) Estimate $\\hat{\\theta}$**\n\nThe MAP estimate $\\hat{\\theta}$ is the value of $\\theta$ that minimizes $U(\\theta)$. We find it by solving the stationarity condition $U'(\\hat{\\theta}) = \\frac{dU}{d\\theta}\\big|_{\\theta=\\hat{\\theta}} = 0$.\nThe first derivative of $U(\\theta)$ is:\n$$\nU'(\\theta) = \\frac{d}{d\\theta} \\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)^2 + \\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2} \\right]\n$$\n$$\nU'(\\theta) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} 2(y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)(-h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i) + \\frac{2(\\theta - \\mu_0)}{2\\sigma_0^2}\n$$\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}\\exp(\\theta)}{\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)\\Delta T_i + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\nThe data provided are $h_{\\mathrm{ref}}=100\\,\\text{W m}^{-2}\\text{K}^{-1}$, $\\Delta T_1=10\\,\\text{K}$, $\\Delta T_2=20\\,\\text{K}$, $\\Delta T_3=30\\,\\text{K}$, and $y_1=1000\\,\\text{W m}^{-2}$, $y_2=2000\\,\\text{W m}^{-2}$, $y_3=3000\\,\\text{W m}^{-2}$. We observe that for all $i$, $y_i = 100 \\Delta T_i = h_{\\mathrm{ref}} \\Delta T_i$.\nSubstituting $y_i = h_{\\mathrm{ref}} \\Delta T_i$ into the expression for $U'(\\theta)$:\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}\\exp(\\theta)}{\\sigma^2} \\sum_{i=1}^{N} (h_{\\mathrm{ref}}\\Delta T_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)\\Delta T_i + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}^2\\exp(\\theta)(1 - \\exp(\\theta))}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\nWe need to solve $U'(\\hat{\\theta})=0$. The prior parameters are $\\mu_0=0$ and $\\sigma_0=0.5$. Let's test the solution $\\hat{\\theta}=0$. At $\\theta=0$, $\\exp(\\theta)=1$, which makes the term $(1 - \\exp(\\theta))$ equal to $0$. The first term of $U'(\\theta)$ vanishes. The second term becomes $(\\hat{\\theta} - \\mu_0)/\\sigma_0^2 = (0 - 0)/\\sigma_0^2 = 0$. Thus, $U'(0) = 0$.\nThe MAP estimate is $\\hat{\\theta} = 0$. This result arises because the data perfectly match the model prediction at the peak of the prior distribution.\n\n**Part 3: Hessian $U''(\\theta)$ and Laplace Approximation**\n\nWe compute the second derivative of $U(\\theta)$, which is the Hessian $U''(\\theta)$, by differentiating $U'(\\theta)$:\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - \\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\nDifferentiating with respect to $\\theta$:\n$$\nU''(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - 2\\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\nThe Laplace approximation is a Gaussian distribution centered at $\\hat{\\theta}$ with variance $[U''(\\hat{\\theta})]^{-1}$. We evaluate $U''(\\theta)$ at $\\hat{\\theta}=0$:\n$$\nU''(\\hat{\\theta}) = U''(0) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(0) - 2\\exp(0)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\n$$\nU''(\\hat{\\theta}) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(1 - 2) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\n$$\nU''(\\hat{\\theta}) = \\frac{h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\nThe Laplace approximation is the Gaussian PDF $p_{L}(\\theta) = \\mathcal{N}(\\hat{\\theta}, [U''(\\hat{\\theta})]^{-1})$, with $\\hat{\\theta}=0$ and the variance given by the inverse of the expression above.\n\n**Part 4: Non-Gaussianity Indicator $\\eta$**\n\nThe non-Gaussianity indicator is defined as $\\eta = |U^{(3)}(\\hat{\\theta})| / (U''(\\hat{\\theta}))^{3/2}$. We first compute the third derivative, $U^{(3)}(\\theta)$, by differentiating $U''(\\theta)$:\n$$\nU''(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - 2\\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\n$$\nU^{(3)}(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - 4\\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\nEvaluating at $\\hat{\\theta}=0$:\n$$\nU^{(3)}(\\hat{\\theta}) = U^{(3)}(0) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(0) - 4\\exp(0)) \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\n$$\nU^{(3)}(\\hat{\\theta}) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(1 - 4) \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\n$$\nU^{(3)}(\\hat{\\theta}) = \\frac{3h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\nNow we compute the numerical values.\nThe sum of squares of the temperature differences is:\n$$\n\\sum_{i=1}^{3} (\\Delta T_i)^2 = (10\\,\\text{K})^2 + (20\\,\\text{K})^2 + (30\\,\\text{K})^2 = 100 + 400 + 900 = 1400\\,\\text{K}^2\n$$\nThe given parameters are $h_{\\mathrm{ref}}=100\\,\\text{W m}^{-2}\\text{K}^{-1}$, $\\sigma=100\\,\\text{W m}^{-2}$, and $\\sigma_0=0.5$.\nSo, $\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2} = \\frac{100^2}{100^2} = 1\\,\\text{K}^{-2}$ and $\\sigma_0^2 = 0.5^2 = 0.25$.\n\nLet's compute $U''(\\hat{\\theta})$ and $U^{(3)}(\\hat{\\theta})$ numerically:\n$$\nU''(\\hat{\\theta}) = \\frac{h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2} = (1) \\cdot (1400) + \\frac{1}{0.25} = 1400 + 4 = 1404\n$$\n$$\nU^{(3)}(\\hat{\\theta}) = \\frac{3h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 = 3 \\cdot (1) \\cdot (1400) = 4200\n$$\nThe non-Gaussianity indicator $\\eta$ is:\n$$\n\\eta = \\frac{|U^{(3)}(\\hat{\\theta})|}{(U''(\\hat{\\theta}))^{3/2}} = \\frac{|4200|}{(1404)^{3/2}} = \\frac{4200}{1404\\sqrt{1404}}\n$$\nCalculating the numerical value:\n$$\n\\eta = \\frac{4200}{52607.889...} \\approx 0.0798364...\n$$\nRounding to four significant figures, we get $\\eta \\approx 0.07984$.",
            "answer": "$$\\boxed{0.07984}$$"
        },
        {
            "introduction": "While analytical approximations are useful, Markov Chain Monte Carlo (MCMC) methods are the gold standard for exploring the full shape of complex posterior distributions. However, running an MCMC sampler is only half the battle; ensuring the generated samples are a reliable representation of the posterior is a critical diagnostic step. This practice problem  challenges you to interpret key MCMC diagnostics, such as the potential scale reduction factor ($\\hat{R}$) and autocorrelation, to diagnose poor sampler performance and link it directly to the physics of the underlying thermal model.",
            "id": "3938052",
            "problem": "A one-dimensional, steady-state conduction problem is used to calibrate the thermal conductivity $k$ (units $\\mathrm{W\\,m^{-1}\\,K^{-1}}$) and the convective heat transfer coefficient $h$ (units $\\mathrm{W\\,m^{-2}\\,K^{-1}}$) of a homogeneous slab of thickness $L$ under a known, uniform applied heat flux $q_{\\mathrm{in}}$ at $x=0$ and convective cooling to an ambient temperature $T_{\\infty}$ at $x=L$. Measurements of the temperature field $T(x)$ are collected at locations $\\{x_i\\}_{i=1}^{m}$ with independent Gaussian sensor noise of unknown variance bounded by engineering specification (assume it is known and equal to a constant, denoted $\\sigma^2$). The posterior distribution for $(k,h)$ is explored with three independent Markov Chain Monte Carlo (MCMC) chains of equal length, producing the following pooled diagnostics for the stationary segments after warmup:\n\n- There are $3$ chains with post-warmup length $n=3000$ each. The pooled estimates of the first-lag autocorrelation are $\\rho_1(k)=0.95$ and $\\rho_1(h)=0.93$. The estimated integrated autocorrelation times are $\\hat{\\tau}_{\\mathrm{int}}(k)=120$ and $\\hat{\\tau}_{\\mathrm{int}}(h)=90$.\n- The estimated correlation in the log-parameterization is $\\operatorname{corr}(\\log k,\\log h)=-0.98$.\n- The chain means for $k$ are $(170,200,185)$ and the corresponding within-chain sample variances are $(1200,1500,1100)$; for $h$ the chain means are $(70,85,75)$ and the corresponding within-chain sample variances are $(400,500,350)$.\n- The estimated Gelman–Rubin potential scale reduction factors $\\hat{R}$ (computed in the classical unsplit form) are reported as $\\hat{R}(k)\\approx 1.11$ and $\\hat{R}(h)\\approx 1.09$.\n\nAssume the deterministic forward model follows the physical laws of conduction and convection with constant properties and steady-state conditions, and that the likelihood of the data under $(k,h)$ is Gaussian with mean given by the model prediction at each $x_i$ and variance $\\sigma^2$. Based on the foundational physical relations for conduction and convection, and the core definitions of effective sample size, autocorrelation, and $\\hat{R}$, which of the following options most appropriately diagnoses the mixing and convergence behavior of the chains and proposes an effective remedy that targets the root cause?\n\nA. The posterior exhibits a narrow, strongly correlated ridge in $(k,h)$ induced by the additive thermal resistances $L/k$ and $1/h$, consistent with $\\operatorname{corr}(\\log k,\\log h)=-0.98$, small effective sample sizes, and $\\hat{R}1.05$. Remedy: sample $(k,h)$ jointly using a covariance-adapted blocked proposal aligned with the posterior covariance, or reparameterize to $(r_{\\mathrm{tot}},\\mathrm{Bi})$ where $r_{\\mathrm{tot}}=L/k+1/h$ and $\\mathrm{Bi}=hL/k$, with a log-transform to enforce positivity; alternatively, use geometry-aware MCMC such as Hamiltonian Monte Carlo with mass matrix adaptation. These actions should increase effective sample size and reduce $\\hat{R}$ towards $1$.\n\nB. The high autocorrelation is best addressed by thinning each chain by a factor of $10$, which will increase the effective sample size by roughly $10\\times$ and reduce $\\hat{R}$ below $1.05$ without any need for reparameterization or blocking.\n\nC. Increasing the number of chains from $3$ to $10$ while keeping the same sampler and per-chain length will reduce $\\hat{R}$ towards $1$ and resolve the mixing pathologies even if the autocorrelation and parameter correlation remain high.\n\nD. Decreasing the random-walk proposal step size will reduce autocorrelation and increase effective sample size; keeping single-parameter updates of $k$ and $h$ is sufficient, so no blocking or reparameterization is necessary.\n\nSelect the single best option.",
            "solution": "The user requires a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Physical System:** One-dimensional, steady-state heat conduction in a homogeneous slab.\n-   **Thickness:** $L$.\n-   **Parameters for Calibration:** Thermal conductivity $k$ ($\\mathrm{W\\,m^{-1}\\,K^{-1}}$) and convective heat transfer coefficient $h$ ($\\mathrm{W\\,m^{-2}\\,K^{-1}}$).\n-   **Boundary Condition at $x=0$:** Uniform applied heat flux, $q_{\\mathrm{in}}$.\n-   **Boundary Condition at $x=L$:** Convection to ambient temperature $T_{\\infty}$.\n-   **Measurement Data:** Temperature measurements $T(x_i)$ at locations $\\{x_i\\}_{i=1}^{m}$.\n-   **Error Model:** Independent Gaussian sensor noise with known, constant variance $\\sigma^2$.\n-   **Inference Method:** Markov Chain Monte Carlo (MCMC).\n-   **MCMC Diagnostics:**\n    -   Number of chains: $3$.\n    -   Post-warmup samples per chain: $n=3000$.\n    -   First-lag autocorrelations: $\\rho_1(k)=0.95$, $\\rho_1(h)=0.93$.\n    -   Estimated integrated autocorrelation times (IAT): $\\hat{\\tau}_{\\mathrm{int}}(k)=120$, $\\hat{\\tau}_{\\mathrm{int}}(h)=90$.\n    -   Log-parameter correlation: $\\operatorname{corr}(\\log k,\\log h)=-0.98$.\n    -   Chain means for $k$: $(170, 200, 185)$.\n    -   Within-chain sample variances for $k$: $(1200, 1500, 1100)$.\n    -   Chain means for $h$: $(70, 85, 75)$.\n    -   Within-chain sample variances for $h$: $(400, 500, 350)$.\n    -   Gelman–Rubin potential scale reduction factor: $\\hat{R}(k)\\approx 1.11$, $\\hat{R}(h)\\approx 1.09$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding:** The problem is based on the fundamental laws of heat transfer. For 1D steady-state conduction with constant properties, the governing equation is $\\frac{d^2T}{dx^2}=0$. The boundary conditions are a Neumann condition at $x=0$ ($-k\\frac{dT}{dx} = q_{\\mathrm{in}}$) and a Robin condition at $x=L$ ($-k\\frac{dT}{dx} = h(T(L)-T_{\\infty})$). The solution to this BVP is:\n    $$T(x) = q_{\\mathrm{in}} \\left( \\frac{L-x}{k} + \\frac{1}{h} \\right) + T_{\\infty}$$\n    This model provides a deterministic link between the parameters $(k,h)$ and the observable temperature $T(x)$. The use of MCMC with a Gaussian likelihood to infer parameters from noisy data is a standard and well-established methodology in Bayesian statistical inference and uncertainty quantification (UQ). All acronyms and diagnostic tools ($\\rho_1$, $\\hat{\\tau}_{\\mathrm{int}}$, $\\hat{R}$, $\\operatorname{corr}$) are standard in the field of computational statistics. The problem is scientifically sound.\n\n2.  **Well-Posedness  Consistency:** The structure of the forward model, $T(x) \\propto \\frac{L-x}{k} + \\frac{1}{h}$, reveals a strong coupling between the parameters $k$ and $h$. Specifically, the terms $L/k$ (conductive thermal resistance) and $1/h$ (convective thermal resistance) are additive for the temperature at $x=0$. Any pair $(k,h)$ that provides a similar value for expressions of the form $(A/k + B/h)$ will yield similar model predictions, leading to a likelihood function with a long, narrow, curved ridge or valley. This indicates a strong negative correlation between $k$ and $h$ (or positive correlation between their reciprocals), which makes parameter estimation challenging. The provided diagnostic $\\operatorname{corr}(\\log k,\\log h)=-0.98$ is a direct and expected consequence of this physical model structure. The high autocorrelations ($\\rho_1  0.9$), high IATs ($\\hat{\\tau}_{\\mathrm{int}} \\ge 90$), and large Gelman-Rubin factors ($\\hat{R}  1.05$) are all consistent and textbook symptoms of an MCMC sampler struggling with a highly correlated posterior distribution. A quick check of the given chain statistics confirms the reported $\\hat{R}$ values are reasonable, indicating the problem data are internally consistent. The problem is well-posed.\n\n3.  **Objectivity  Clarity:** The problem is stated using precise, objective, and quantitative language. No ambiguity or subjective claims are present.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically grounded, internally consistent, and clearly formulated. The provided data represent a classic case of poor MCMC mixing due to strong parameter correlation, which is physically motivated. The task is to diagnose this situation and identify the most appropriate remedy.\n\n### Solution Derivation and Option Analysis\n\nThe core of the problem lies in interpreting the MCMC diagnostics to understand the sampler's performance and its root cause.\n\n1.  **Diagnosis of MCMC Performance:**\n    -   **Convergence:** The Gelman-Rubin diagnostics $\\hat{R}(k)\\approx 1.11$ and $\\hat{R}(h)\\approx 1.09$ are significantly greater than $1$. Conventionally, values should be close to $1.0$ (e.g., $1.05$) to declare convergence. This indicates that the multiple chains have not successfully explored the same distribution; the between-chain variance is substantial compared to the within-chain variance.\n    -   **Mixing and Efficiency:** The first-lag autocorrelations, $\\rho_1(k)=0.95$ and $\\rho_1(h)=0.93$, are extremely high. This means consecutive samples are nearly identical, and the chain explores the parameter space very slowly (poor mixing). This is quantified by the integrated autocorrelation time (IAT), $\\hat{\\tau}_{\\mathrm{int}}$. An ideal sampler would have $\\tau_{\\mathrm{int}} \\approx 1$. Here, the values $\\hat{\\tau}_{\\mathrm{int}}(k)=120$ and $\\hat{\\tau}_{\\mathrm{int}}(h)=90$ are very high.\n    -   **Effective Sample Size (ESS):** The ESS is the number of independent samples that would carry the same amount of information as the autocorrelated samples. It is calculated as $ESS = N_{total} / \\hat{\\tau}_{\\mathrm{int}}$. With $N_{total} = 3 \\times 3000 = 9000$ samples, we get $ESS(k) = 9000/120 = 75$ and $ESS(h) = 9000/90 = 100$. These are extremely low values, insufficient for reliable posterior inference.\n\n2.  **Root Cause Analysis:**\n    The extremely high negative correlation, $\\operatorname{corr}(\\log k,\\log h)=-0.98$, is the key explanatory variable for all the above pathologies. This correlation is not a numerical artifact but is induced by the physics of the forward model, as shown in the validation step. The likelihood is high only along a very narrow region in the $(k,h)$ parameter space. A standard MCMC algorithm that makes proposals along the coordinate axes (e.g., updating $k$ while holding $h$ fixed, then vice versa) will be exceptionally inefficient. Any reasonably-sized step along one axis is likely to move the proposed state far from the high-probability ridge, leading to a high rejection rate and forcing the sampler to take minuscule steps, hence the high autocorrelation.\n\n3.  **Evaluating Proposed Remedies:**\n    An effective remedy must directly address the strong parameter correlation.\n\n**A. The posterior exhibits a narrow, strongly correlated ridge in $(k,h)$ induced by the additive thermal resistances $L/k$ and $1/h$, consistent with $\\operatorname{corr}(\\log k,\\log h)=-0.98$, small effective sample sizes, and $\\hat{R}1.05$. Remedy: sample $(k,h)$ jointly using a covariance-adapted blocked proposal aligned with the posterior covariance, or reparameterize to $(r_{\\mathrm{tot}},\\mathrm{Bi})$ where $r_{\\mathrm{tot}}=L/k+1/h$ and $\\mathrm{Bi}=hL/k$, with a log-transform to enforce positivity; alternatively, use geometry-aware MCMC such as Hamiltonian Monte Carlo with mass matrix adaptation. These actions should increase effective sample size and reduce $\\hat{R}$ towards $1$.**\n\n-   **Analysis:** This option provides a perfect diagnosis, linking the physical model ($L/k$ and $1/h$) to the statistical issue (correlated ridge), and correctly connecting this to all the observed diagnostic values. It then proposes three powerful, state-of-the-art remedies that directly target the root cause of correlation:\n    1.  **Covariance-adapted blocking:** Proposing joint updates for $(k,h)$ using a proposal distribution shaped like the posterior (i.e., elongated along the ridge) allows for large moves within the high-probability region, drastically improving mixing.\n    2.  **Reparameterization:** Changing variables to a less correlated set, such as one based on total resistance ($r_{\\mathrm{tot}}$) and the Biot number ($\\mathrm{Bi}$), can transform the difficult posterior geometry into a much simpler one that is easily explored by standard samplers.\n    3.  **Geometry-aware MCMC (HMC):** These methods use gradient information to intelligently navigate complex posterior geometries and are particularly effective for highly correlated parameters.\n-   **Verdict:** This option is scientifically and statistically flawless. It correctly diagnoses the problem and proposes the most appropriate and effective solutions. **Correct**.\n\n**B. The high autocorrelation is best addressed by thinning each chain by a factor of $10$, which will increase the effective sample size by roughly $10\\times$ and reduce $\\hat{R}$ below $1.05$ without any need for reparameterization or blocking.**\n\n-   **Analysis:** This option makes a fundamentally incorrect claim. Thinning is the process of discarding samples to reduce autocorrelation in the stored chain and save memory. It does **not** generate new information or improve the efficiency of the underlying sampler. The effective sample size ($ESS$) is a measure of the information content of the entire chain *before* thinning. $ESS$ is not increased by thinning; at best, it remains unchanged. The statement that $ESS$ will \"increase by roughly $10\\times$\" is false. Thinning does not fix the poor mixing or the lack of convergence, and therefore it will not resolve the high $\\hat{R}$ value.\n-   **Verdict:** The proposed remedy is based on a critical misunderstanding of MCMC theory. **Incorrect**.\n\n**C. Increasing the number of chains from $3$ to $10$ while keeping the same sampler and per-chain length will reduce $\\hat{R}$ towards $1$ and resolve the mixing pathologies even if the autocorrelation and parameter correlation remain high.**\n\n-   **Analysis:** Running more parallel chains is useful for more robustly diagnosing non-convergence, but it does not fix it. If the sampler is inefficient, each of the $10$ chains will be just as stuck and slow-mixing as the original $3$ chains were. $\\hat{R}$ is a ratio of between-chain to within-chain variance; if the chains remain stuck in different locations, this ratio will remain large. Adding more chains will not \"resolve the mixing pathologies.\" The problem lies with the sampler's inability to navigate the parameter space, not with the number of attempts to do so.\n-   **Verdict:** This confuses a diagnostic tool with a therapeutic one and fails to address the root cause. **Incorrect**.\n\n**D. Decreasing the random-walk proposal step size will reduce autocorrelation and increase effective sample size; keeping single-parameter updates of $k$ and $h$ is sufficient, so no blocking or reparameterization is necessary.**\n\n-   **Analysis:** This option displays a misunderstanding of both sampler tuning and the core problem. First, decreasing the proposal step size for a random-walk sampler typically *increases* the acceptance rate but also *increases* autocorrelation because the chain takes tiny steps and explores the space more slowly. Second, and more importantly, it asserts that single-parameter updates are sufficient. This is precisely the opposite of what is true. The failure of the sampler is due to the strong correlation that makes single-parameter (component-wise) updates ineffective, regardless of step size tuning. No amount of simple step-size adjustment can overcome the geometric challenge of the correlated posterior ridge.\n-   **Verdict:** The proposed remedy is counterproductive and ignores the true cause of the problem. **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Bayesian inference is not just a tool for learning from data you already have; it is also a powerful framework for designing future experiments to be maximally informative. This exercise  introduces the principles of Bayesian optimal experimental design (OED), where we use pre-posterior analysis to make decisions that maximize an expected utility. You will learn to select optimal sensor locations by calculating the expected reduction in the posterior uncertainty of a critical engineering design quantity, demonstrating a proactive approach to uncertainty quantification.",
            "id": "3938077",
            "problem": "Consider a one-dimensional, steady-state heat conduction problem in a homogeneous rod of length $L$ with constant thermal conductivity, fixed boundary temperatures, and uniform volumetric heat generation. The rod has boundary temperatures $T(0) = T_0$ and $T(L) = T_L$. Assume conservation of energy and Fourier’s law apply. The steady-state governing equation is $\\dfrac{d}{dx}\\left(k \\dfrac{dT}{dx}\\right) + q = 0$ for constant $k$ and $q$, leading to a temperature field that is a quadratic function of position. Let the ratio $s = q/k$ be uncertain and modeled as a scalar parameter.\n\nA critical design quantity is the temperature at the rod midpoint $x = L/2$, denoted $Q = T(L/2)$, which determines component safety. Temperature measurements are obtained by sensors at positions $x$ with additive measurement noise. The design goal is to perform pre-posterior analysis, a concept from Bayesian decision theory, to select sensor locations that maximize the expected utility defined as the reduction in the posterior variance of the critical quantity $Q$, before data are collected. The assumed probabilistic model is a Gaussian prior for $s$ and Gaussian measurement noise. You must start from energy conservation in steady-state conduction and proceed through Bayesian analysis to quantify the expected reduction.\n\nYou must adhere to the following model and data assumptions:\n- The one-dimensional steady-state model with uniform generation yields a temperature field $T(x)$ that is quadratic in $x$. The parameterized temperature field can be expressed as a sum of a deterministic boundary-driven term and an $s$-dependent term. For the purposes of Bayesian analysis, the scalar parameter $s$ enters linearly into the temperature field through a known sensitivity function of $x$.\n- The measurement model at a location $x$ is $y(x) = T(x) + \\varepsilon$, where $\\varepsilon$ is zero-mean Gaussian noise with variance $\\sigma^2$, and $T(x)$ depends linearly on $s$.\n- The prior distribution of $s$ is Gaussian with a specified mean and variance, independent of measurement noise.\n- The expected utility is the reduction in the posterior variance of $Q$ averaged over the predictive distribution of the data. In the linear-Gaussian setting, this reduction depends on the sensor locations solely through the sensitivity functions and noise variance.\n\nYour task is to derive from first principles how to compute the expected reduction in the posterior variance of $Q$ for arbitrary sets of sensor locations, and then implement a program that selects optimal sensor locations from provided candidate sets.\n\nParameters and units to use throughout:\n- Rod length $L = 0.1$ meters.\n- Boundary temperatures $T_0 = 300$ kelvin and $T_L = 310$ kelvin.\n- Define $s = q/k$ in units of kelvin per square meter.\n- Prior for $s$: Gaussian with mean $\\mu_s = 1000$ kelvin per square meter and variance $\\tau^2 = 40000$ (kelvin per square meter)$^2$.\n- Measurement noise variance $\\sigma^2$ specified per test case in kelvin squared.\n- The critical quantity is $Q = T(L/2)$ in kelvin, and variances of $Q$ must be reported in kelvin squared.\n- Angles do not appear in this problem.\n\nYou must use zero-based indexing for sensor location indices when reporting choices. If multiple sensor sets tie for maximum expected utility, break ties by selecting the set with the lexicographically smallest indices.\n\nTest Suite:\n- Test Case 1 (single sensor selection; happy path): Candidates are the set $\\{0, L/4, L/2, 3L/4, L\\}$ meters. The measurement noise variance is $\\sigma^2 = 0.04$ kelvin squared. Output the index of the optimal single sensor and the expected reduction in the posterior variance of $Q$ in kelvin squared.\n- Test Case 2 (two-sensor selection; coverage of multi-sensor design): Candidates are the set $\\{0, L/4, L/2, 3L/4, L\\}$ meters; you may place two sensors and you are allowed to place both sensors at the same location. The measurement noise variance is $\\sigma^2 = 0.04$ kelvin squared. Output the indices of the optimal pair of sensors (two integers) and the expected reduction in the posterior variance of $Q$ in kelvin squared.\n- Test Case 3 (single sensor, high-noise edge case): Candidates are the set $\\{0, L/4, L/2, 3L/4, L\\}$ meters. The measurement noise variance is $\\sigma^2 = 4.0$ kelvin squared. Output the index of the optimal single sensor and the expected reduction in the posterior variance of $Q$ in kelvin squared.\n- Test Case 4 (single sensor, boundary-only edge case): Candidates are the set $\\{0, L\\}$ meters. The measurement noise variance is $\\sigma^2 = 0.04$ kelvin squared. Output the index of the optimal single sensor and the expected reduction in the posterior variance of $Q$ in kelvin squared.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concatenate the results of all test cases in order, where each test case contributes:\n- For Test Case 1: $[\\text{index}, \\text{reduction}]$,\n- For Test Case 2: $[\\text{index}_1, \\text{index}_2, \\text{reduction}]$,\n- For Test Case 3: $[\\text{index}, \\text{reduction}]$,\n- For Test Case 4: $[\\text{index}, \\text{reduction}]$.\n\nAll reductions must be represented as floating-point numbers in kelvin squared. The final single line should be the concatenation of these values as a flat list, for example $[\\text{case1\\_index},\\text{case1\\_reduction},\\text{case2\\_index1},\\text{case2\\_index2},\\text{case2\\_reduction},\\text{case3\\_index},\\text{case3\\_reduction},\\text{case4\\_index},\\text{case4\\_reduction}]$.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of heat transfer and Bayesian statistics, is well-posed with sufficient information, and is stated objectively. We can proceed with the derivation and solution.\n\nThe solution requires deriving the expected utility for sensor placement from first principles, which involves several steps: determining the temperature field, formulating the Bayesian linear model for measurements, deriving the posterior variance of the quantity of interest (QoI), and finally calculating the expected reduction in this variance.\n\n**1. Derivation of the Temperature Field**\n\nThe governing equation for one-dimensional, steady-state heat conduction with constant thermal conductivity $k$ and uniform volumetric heat generation $q$ is:\n$$\n\\frac{d}{dx}\\left(k \\frac{dT}{dx}\\right) + q = 0\n$$\nSince $k$ and $q$ are constant, this simplifies to:\n$$\n\\frac{d^2T}{dx^2} = -\\frac{q}{k}\n$$\nThe problem defines the uncertain parameter $s = q/k$. Thus, the equation becomes:\n$$\n\\frac{d^2T}{dx^2} = -s\n$$\nIntegrating twice with respect to the spatial coordinate $x$ yields the general solution for the temperature field $T(x)$:\n$$\n\\frac{dT}{dx} = -sx + C_1\n$$\n$$\nT(x) = -\\frac{s}{2}x^2 + C_1 x + C_2\n$$\nThe integration constants $C_1$ and $C_2$ are determined by the boundary conditions $T(0) = T_0$ and $T(L) = T_L$.\nApplying $T(0) = T_0$:\n$$\nT(0) = -\\frac{s}{2}(0)^2 + C_1(0) + C_2 = T_0 \\implies C_2 = T_0\n$$\nApplying $T(L) = T_L$:\n$$\nT(L) = -\\frac{s}{2}L^2 + C_1 L + T_0 = T_L\n$$\nSolving for $C_1$:\n$$\nC_1 L = T_L - T_0 + \\frac{s}{2}L^2 \\implies C_1 = \\frac{T_L - T_0}{L} + \\frac{sL}{2}\n$$\nSubstituting $C_1$ and $C_2$ back into the expression for $T(x)$, we obtain the full temperature profile:\n$$\nT(x; s) = -\\frac{s}{2}x^2 + \\left(\\frac{T_L - T_0}{L} + \\frac{sL}{2}\\right)x + T_0\n$$\nAs stipulated in the problem, we can separate this into a deterministic term and a term that depends linearly on the uncertain parameter $s$:\n$$\nT(x; s) = \\left( T_0 + \\frac{T_L - T_0}{L}x \\right) + s \\left( \\frac{L}{2}x - \\frac{1}{2}x^2 \\right)\n$$\nThis is of the form $T(x; s) = T_{det}(x) + s \\cdot g(x)$, where the sensitivity function $g(x)$ is:\n$$\ng(x) = \\frac{1}{2}(Lx - x^2)\n$$\n\n**2. Bayesian Linear Model Formulation**\n\nThe measurement model for a set of $N$ sensors at locations $\\mathbf{x} = \\{x_1, \\dots, x_N\\}$ is given by:\n$$\ny_i = T(x_i; s) + \\varepsilon_i \\quad \\text{for } i=1, \\dots, N\n$$\nwhere $\\varepsilon_i$ is i.i.d. Gaussian measurement noise with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim N(0, \\sigma^2)$. Substituting the linear form of $T(x; s)$:\n$$\ny_i = T_{det}(x_i) + s \\cdot g(x_i) + \\varepsilon_i\n$$\nWe can define a modified measurement $\\tilde{y}_i = y_i - T_{det}(x_i)$, which isolates the uncertain part:\n$$\n\\tilde{y}_i = s \\cdot g(x_i) + \\varepsilon_i\n$$\nIn vector form for all $N$ measurements, this is a standard Bayesian linear regression model:\n$$\n\\tilde{\\mathbf{y}} = \\mathbf{g}s + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\tilde{\\mathbf{y}} = [\\tilde{y}_1, \\dots, \\tilde{y}_N]^T$, $\\mathbf{g} = [g(x_1), \\dots, g(x_N)]^T$, and $\\boldsymbol{\\varepsilon} \\sim N(\\mathbf{0}, \\sigma^2 \\mathbf{I})$. The likelihood function for $s$ given the data $\\tilde{\\mathbf{y}}$ (and sensor locations $\\mathbf{x}$) is:\n$$\np(\\tilde{\\mathbf{y}}|s, \\mathbf{x}, \\sigma^2) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2}(\\tilde{\\mathbf{y}} - \\mathbf{g}s)^T(\\tilde{\\mathbf{y}} - \\mathbf{g}s) \\right) = \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (\\tilde{y}_i - s g(x_i))^2 \\right)\n$$\nThe prior distribution for $s$ is Gaussian: $s \\sim N(\\mu_s, \\tau^2)$. The posterior distribution $p(s|\\tilde{\\mathbf{y}})$ is also Gaussian due to the conjugacy of the Gaussian prior and likelihood. The posterior precision (inverse variance) is the sum of the prior precision and the precision from the likelihood.\n\nPosterior precision: $1/\\tau_{post}^2 = 1/\\tau^2 + \\mathbf{g}^T\\mathbf{g}/\\sigma^2$.\nThe term $\\mathbf{g}^T\\mathbf{g} = \\sum_{i=1}^N g(x_i)^2$.\nThe posterior variance $\\tau_{post}^2$ is therefore:\n$$\n\\tau_{post}^2 = \\left( \\frac{1}{\\tau^2} + \\frac{\\sum_{i=1}^N g(x_i)^2}{\\sigma^2} \\right)^{-1} = \\frac{\\tau^2\\sigma^2}{\\sigma^2 + \\tau^2 \\sum_{i=1}^N g(x_i)^2}\n$$\n\n**3. Prior and Posterior Variance of the Quantity of Interest (QoI)**\n\nThe QoI is the temperature at the midpoint, $Q = T(L/2)$. Using the linear model for $T(x; s)$:\n$$\nQ(s) = T(L/2; s) = T_{det}(L/2) + s \\cdot g(L/2)\n$$\n$Q$ is a linear function of $s$. The sensitivity of the QoI is $g_Q = g(L/2)$.\nThe prior variance of $Q$ is:\n$$\n\\text{Var}_{prior}[Q] = \\text{Var}[T_{det}(L/2) + s \\cdot g(L/2)] = g(L/2)^2 \\cdot \\text{Var}_{prior}[s] = g(L/2)^2 \\tau^2\n$$\nThe posterior variance of $Q$, after observing data, is:\n$$\n\\text{Var}_{post}[Q] = \\text{Var}[T_{det}(L/2) + s \\cdot g(L/2) | \\text{data}] = g(L/2)^2 \\cdot \\text{Var}_{post}[s] = g(L/2)^2 \\tau_{post}^2\n$$\nSubstituting the expression for $\\tau_{post}^2$:\n$$\n\\text{Var}_{post}[Q] = g(L/2)^2 \\frac{\\tau^2\\sigma^2}{\\sigma^2 + \\tau^2 \\sum_{i=1}^N g(x_i)^2}\n$$\n\n**4. Expected Utility (Expected Reduction in Variance)**\n\nThe utility is defined as the reduction in the variance of $Q$:\n$$\nU(\\mathbf{y}) = \\text{Var}_{prior}[Q] - \\text{Var}_{post}[Q|\\mathbf{y}]\n$$\nA key feature of the linear-Gaussian model is that the posterior variance $\\text{Var}_{post}[Q]$ does not depend on the specific measurement values $\\mathbf{y}$, only on the sensor locations $\\mathbf{x}$ (which define $\\mathbf{g}$) and noise variance $\\sigma^2$. Therefore, the reduction in variance is deterministic given the experimental design, and the expected utility $E[U]$ is simply this reduction:\n$$\nE[U] = \\text{Var}_{prior}[Q] - \\text{Var}_{post}[Q] = g(L/2)^2 \\tau^2 - g(L/2)^2 \\frac{\\tau^2\\sigma^2}{\\sigma^2 + \\tau^2 \\sum_{i=1}^N g(x_i)^2}\n$$\nFactoring out common terms:\n$$\nE[U] = g(L/2)^2 \\tau^2 \\left( 1 - \\frac{\\sigma^2}{\\sigma^2 + \\tau^2 \\sum_{i=1}^N g(x_i)^2} \\right)\n$$\n$$\nE[U] = g(L/2)^2 \\tau^2 \\left( \\frac{\\sigma^2 + \\tau^2 \\sum_{i=1}^N g(x_i)^2 - \\sigma^2}{\\sigma^2 + \\tau^2 \\sum_{i=1}^N g(x_i)^2} \\right)\n$$\nThis simplifies to the final expression for the expected reduction in the variance of $Q$:\n$$\nE[U] = \\frac{g(L/2)^2 \\cdot (\\tau^2)^2 \\cdot \\sum_{i=1}^N g(x_i)^2}{\\sigma^2 + \\tau^2 \\sum_{i=1}^N g(x_i)^2}\n$$\n\n**5. Optimization Problem**\n\nTo select the optimal sensor locations, we must maximize $E[U]$ with respect to the choice of $\\{x_1, \\dots, x_N\\}$ from a set of candidate locations. Let $G^2(\\mathbf{x}) = \\sum_{i=1}^N g(x_i)^2$. The expression for $E[U]$ is of the form $f(z) = \\frac{A z}{B + C z}$, where $z = G^2(\\mathbf{x})$ and $A, B, C$ are positive constants. This function is monotonically increasing with $z$ for $z0$. Therefore, maximizing the expected utility is equivalent to maximizing the term $G^2(\\mathbf{x}) = \\sum_{i=1}^N g(x_i)^2$.\n\nThe optimization strategy is to find the set of $N$ sensor locations from the candidates that maximizes the sum of their squared sensitivities.\n\n**6. Application to Test Cases**\n\nWe use the following constants:\n- $L = 0.1$ m\n- $\\tau^2 = 40000$ (K/m²)$^2$\nThe sensitivity function is $g(x) = \\frac{1}{2}(0.1x - x^2)$.\nThe QoI sensitivity is $g_Q = g(L/2) = g(0.05) = \\frac{1}{8}L^2 = \\frac{1}{8}(0.1)^2 = 0.00125$ m².\n\nCandidate locations: $\\{0, L/4, L/2, 3L/4, L\\} = \\{0, 0.025, 0.05, 0.075, 0.1\\}$ m.\nSensitivities $g(x_i)$:\n- $g(0) = 0$\n- $g(0.025) = \\frac{1}{2}(0.1 \\cdot 0.025 - 0.025^2) = 0.0009375$\n- $g(0.05) = 0.00125$\n- $g(0.075) = 0.0009375$\n- $g(0.1) = 0$\n\nSquared sensitivities $g(x_i)^2$:\n- $g(0)^2 = 0$\n- $g(0.025)^2 \\approx 8.789 \\times 10^{-7}$\n- $g(0.05)^2 \\approx 1.5625 \\times 10^{-6}$\n- $g(0.075)^2 \\approx 8.789 \\times 10^{-7}$\n- $g(0.1)^2 = 0$\n\nThe maximum value of $g(x)^2$ occurs at $x = L/2 = 0.05$ m, which is at index $2$ in the candidate list.\n\n- **Test Case 1 (1 sensor, $\\sigma^2 = 0.04$):** To maximize $\\sum g(x_i)^2$ with one sensor, we choose the location with the maximum $g(x_i)^2$. This is $x = L/2$ (index $2$).\n  - $G^2 = g(L/2)^2 = (0.00125)^2 = 1.5625 \\times 10^{-6}$.\n  - $E[U] = \\frac{(0.00125^2) \\cdot (40000^2) \\cdot (1.5625 \\times 10^{-6})}{0.04 + 40000 \\cdot (1.5625 \\times 10^{-6})} = \\frac{3.90625}{0.1025} \\approx 38.109756$ K².\n\n- **Test Case 2 (2 sensors, $\\sigma^2 = 0.04$):** To maximize $g(x_1)^2 + g(x_2)^2$ with replacement, we should pick the location with the maximum $g(x)^2$ twice. This is $x = L/2$ for both sensors.\n  - Optimal indices: $(2, 2)$.\n  - $G^2 = g(L/2)^2 + g(L/2)^2 = 2 \\cdot (1.5625 \\times 10^{-6}) = 3.125 \\times 10^{-6}$.\n  - $E[U] = \\frac{(0.00125^2) \\cdot (40000^2) \\cdot (3.125 \\times 10^{-6})}{0.04 + 40000 \\cdot (3.125 \\times 10^{-6})} = \\frac{7.8125}{0.165} \\approx 47.348485$ K².\n\n- **Test Case 3 (1 sensor, $\\sigma^2 = 4.0$):** The optimal sensor location is independent of $\\sigma^2$, so we again choose $x=L/2$ (index $2$).\n  - $G^2 = g(L/2)^2 = 1.5625 \\times 10^{-6}$.\n  - $E[U] = \\frac{(0.00125^2) \\cdot (40000^2) \\cdot (1.5625 \\times 10^{-6})}{4.0 + 40000 \\cdot (1.5625 \\times 10^{-6})} = \\frac{3.90625}{4.0625} \\approx 0.961538$ K².\n\n- **Test Case 4 (1 sensor, $\\sigma^2 = 0.04$, boundary candidates):** Candidates are $\\{0, L\\}$.\n  - $g(0)=0$ and $g(L)=0$. Thus, $G^2=0$ for either choice. The expected utility is $E[U]=0$.\n  - Both choices give the same maximum utility ($0$). By the tie-breaking rule, we select the set with the lexicographically smallest indices. The index for $x=0$ is $0$, and for $x=L$ is $1$. The smallest is $0$.\n  - Optimal index: $0$. Reduction: $0.0$ K².",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef solve():\n    \"\"\"\n    Solves a Bayesian optimal experimental design problem for sensor placement\n    in a 1D steady-state heat conduction scenario.\n    \"\"\"\n    # Define problem parameters\n    L = 0.1  # m\n    tau_sq = 40000.0  # (K/m^2)^2\n\n    # Define the sensitivity function g(x) = 0.5 * (L*x - x^2)\n    def g(x, L_val):\n        return 0.5 * (L_val * x - x**2)\n\n    # Sensitivity of the Quantity of Interest Q = T(L/2)\n    g_Q = g(L / 2.0, L)\n\n    def calculate_expected_reduction(G_sq, sigma_sq, g_Q_val, tau_sq_val):\n        \"\"\"\n        Calculates the expected reduction in posterior variance of Q.\n        E[U] = (g_Q^2 * (tau^2)^2 * G^2) / (sigma^2 + tau^2 * G^2)\n        where G^2 = sum(g(x_i)^2)\n        \"\"\"\n        if sigma_sq + tau_sq_val * G_sq == 0:\n            # This case occurs if G_sq is zero and sigma_sq is zero, which is not expected here\n            # but is a good practice to avoid division by zero.\n            # Numerator would also be zero, so the result is 0.\n            return 0.0\n            \n        numerator = (g_Q_val**2) * (tau_sq_val**2) * G_sq\n        denominator = sigma_sq + tau_sq_val * G_sq\n        if denominator == 0:\n             # If denominator is zero, numerator must be too\n             return 0.0\n        return numerator / denominator\n\n\n    # Test Suite\n    test_cases = [\n        # (num_sensors, candidate_locations, sigma_sq)\n        (1, np.array([0, L/4, L/2, 3*L/4, L]), 0.04),\n        (2, np.array([0, L/4, L/2, 3*L/4, L]), 0.04),\n        (1, np.array([0, L/4, L/2, 3*L/4, L]), 4.0),\n        (1, np.array([0, L]), 0.04),\n    ]\n\n    final_results = []\n\n    for num_sensors, candidates, sigma_sq in test_cases:\n        candidate_indices = range(len(candidates))\n        \n        # Pre-compute squared sensitivities for all candidate locations\n        g_sq_values = [g(x, L)**2 for x in candidates]\n\n        best_indices = None\n        max_utility = -1.0\n\n        # Generate all combinations of sensor placements (with replacement)\n        for index_combo in combinations_with_replacement(candidate_indices, num_sensors):\n            # Calculate G^2 = sum of squared sensitivities for this combination\n            current_G_sq = sum(g_sq_values[i] for i in index_combo)\n            \n            # Calculate expected utility (reduction in variance)\n            current_utility = calculate_expected_reduction(current_G_sq, sigma_sq, g_Q, tau_sq)\n\n            if current_utility > max_utility:\n                max_utility = current_utility\n                best_indices = index_combo\n            elif current_utility == max_utility:\n                # Tie-breaking rule: select lexicographically smallest index set\n                if best_indices is None or index_combo  best_indices:\n                    best_indices = index_combo\n\n        # Store results for this test case\n        final_results.extend(list(best_indices))\n        final_results.append(max_utility)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}