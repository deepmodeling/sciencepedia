{
    "hands_on_practices": [
        {
            "introduction": "Before building complex samplers, it's useful to learn how to create a simple, deterministic approximation of the posterior distribution. This exercise introduces the Laplace approximation, which uses a Gaussian distribution centered at the posterior's peak—the Maximum A Posteriori (MAP) estimate—to summarize its shape. This practice will solidify your understanding of the posterior landscape and its local properties by analyzing a nonlinear heat transfer model .",
            "id": "3938004",
            "problem": "A calibration problem in steady convective heat transfer is considered. A flat isothermal plate at surface temperature experiences a convective heat flux modeled as $q = h(\\theta)\\,\\Delta T$, where $\\Delta T$ is a known temperature difference and $h(\\theta)$ is a convective heat transfer coefficient parameterized by a single scalar parameter $\\theta$. Motivated by near-exponential sensitivity of some convective correlations to an exponent-like parameter, assume $h(\\theta) = h_{\\mathrm{ref}} \\exp(\\theta)$, with $h_{\\mathrm{ref}}$ known. Noisy measurements $\\{y_i\\}_{i=1}^N$ of the heat flux under known temperature differences $\\{\\Delta T_i\\}_{i=1}^N$ satisfy\n$$\ny_i \\;=\\; h(\\theta)\\,\\Delta T_i \\;+\\; \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\\quad i=1,\\dots,N,\n$$\nwith $\\sigma^2$ known, and the $\\varepsilon_i$ statistically independent. A Gaussian prior is placed on $\\theta$: $\\theta \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$, with $\\mu_0$ and $\\sigma_0^2$ known.\n\nYou are given $N=3$ experiments with $h_{\\mathrm{ref}}=100\\,\\text{W m}^{-2}\\text{K}^{-1}$, $\\mu_0=0$, $\\sigma_0=0.5$, $\\sigma=100\\,\\text{W m}^{-2}$, and\n$$\n\\Delta T_1=10\\,\\text{K},\\quad \\Delta T_2=20\\,\\text{K},\\quad \\Delta T_3=30\\,\\text{K},\n$$\nwith measured fluxes\n$$\ny_1=1000\\,\\text{W m}^{-2},\\quad y_2=2000\\,\\text{W m}^{-2},\\quad y_3=3000\\,\\text{W m}^{-2}.\n$$\n\nUsing Bayes’ theorem and the Gaussian noise and prior models:\n\n1) Derive the negative log posterior $U(\\theta)$ up to an additive constant in terms of $\\{y_i\\}$, $\\{\\Delta T_i\\}$, $h_{\\mathrm{ref}}$, $\\sigma^2$, $\\mu_0$, and $\\sigma_0^2$.\n\n2) Identify the maximum a posteriori estimate $\\hat{\\theta}$ by solving the stationarity condition $U'(\\hat{\\theta})=0$.\n\n3) Derive the Hessian $U''(\\theta)$ of the negative log posterior and the Laplace (second-order) Gaussian approximation of the posterior around $\\hat{\\theta}$, i.e., the Gaussian with mean $\\hat{\\theta}$ and variance $[U''(\\hat{\\theta})]^{-1}$.\n\n4) To assess accuracy for a mildly nonlinear model, compute the standardized non-Gaussianity indicator\n$$\n\\eta \\;=\\; \\frac{|U^{(3)}(\\hat{\\theta})|}{\\big(U''(\\hat{\\theta})\\big)^{3/2}},\n$$\nwhere $U^{(3)}(\\theta)$ is the third derivative of $U(\\theta)$. Evaluate $\\eta$ numerically for the given data.\n\nExpress your final result for $\\eta$ as a single real number, rounded to four significant figures. No units are required for $\\eta$ since it is dimensionless.",
            "solution": "The physical model for the heat flux is $q_i = h(\\theta) \\Delta T_i$, with the parameterization $h(\\theta) = h_{\\mathrm{ref}} \\exp(\\theta)$. The measurement model is $y_i = h(\\theta) \\Delta T_i + \\varepsilon_i$, where the noise terms $\\varepsilon_i$ are independent and identically distributed (i.i.d.) as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. A Gaussian prior is specified for the parameter $\\theta$, $\\theta \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n\n**Part 1: Derivation of the Negative Log Posterior $U(\\theta)$**\n\nAccording to Bayes' theorem, the posterior probability density function (PDF) $p(\\theta|\\{y_i\\}_{i=1}^N)$ is proportional to the product of the likelihood function $p(\\{y_i\\}_{i=1}^N|\\theta)$ and the prior PDF $p(\\theta)$.\n$$\np(\\theta|\\{y_i\\}) \\propto p(\\{y_i\\}|\\theta) p(\\theta)\n$$\nThe likelihood for a single measurement $y_i$ is given by the Gaussian noise model:\n$$\np(y_i|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - h(\\theta)\\Delta T_i)^2}{2\\sigma^2}\\right)\n$$\nSince the measurements are statistically independent, the joint likelihood is the product of individual likelihoods:\n$$\np(\\{y_i\\}|\\theta) = \\prod_{i=1}^{N} p(y_i|\\theta) = \\left(2\\pi\\sigma^2\\right)^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h(\\theta)\\Delta T_i)^2\\right)\n$$\nThe prior PDF for $\\theta$ is given as:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\n$$\nThe posterior PDF is therefore proportional to:\n$$\np(\\theta|\\{y_i\\}) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)^2 - \\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\n$$\nThe negative log posterior, $U(\\theta) = -\\ln(p(\\theta|\\{y_i\\}))$, up to an additive constant, is:\n$$\nU(\\theta) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)^2 + \\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\n$$\n\n**Part 2: Maximum a Posteriori (MAP) Estimate $\\hat{\\theta}$**\n\nThe MAP estimate $\\hat{\\theta}$ is the value of $\\theta$ that minimizes $U(\\theta)$. We find it by solving the stationarity condition $U'(\\hat{\\theta}) = \\frac{dU}{d\\theta}\\big|_{\\theta=\\hat{\\theta}} = 0$.\nThe first derivative of $U(\\theta)$ is:\n$$\nU'(\\theta) = \\frac{d}{d\\theta} \\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)^2 + \\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2} \\right]\n$$\n$$\nU'(\\theta) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} 2(y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)(-h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i) + \\frac{2(\\theta - \\mu_0)}{2\\sigma_0^2}\n$$\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}\\exp(\\theta)}{\\sigma^2} \\sum_{i=1}^{N} (y_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)\\Delta T_i + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\nThe data provided are $h_{\\mathrm{ref}}=100\\,\\text{W m}^{-2}\\text{K}^{-1}$, $\\Delta T_1=10\\,\\text{K}$, $\\Delta T_2=20\\,\\text{K}$, $\\Delta T_3=30\\,\\text{K}$, and $y_1=1000\\,\\text{W m}^{-2}$, $y_2=2000\\,\\text{W m}^{-2}$, $y_3=3000\\,\\text{W m}^{-2}$. We observe that for all $i$, $y_i = 100 \\Delta T_i = h_{\\mathrm{ref}} \\Delta T_i$.\nSubstituting $y_i = h_{\\mathrm{ref}} \\Delta T_i$ into the expression for $U'(\\theta)$:\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}\\exp(\\theta)}{\\sigma^2} \\sum_{i=1}^{N} (h_{\\mathrm{ref}}\\Delta T_i - h_{\\mathrm{ref}}\\exp(\\theta)\\Delta T_i)\\Delta T_i + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}^2\\exp(\\theta)(1 - \\exp(\\theta))}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\nWe need to solve $U'(\\hat{\\theta})=0$. The prior parameters are $\\mu_0=0$ and $\\sigma_0=0.5$. Let's test the solution $\\hat{\\theta}=0$. At $\\theta=0$, $\\exp(\\theta)=1$, which makes the term $(1 - \\exp(\\theta))$ equal to $0$. The first term of $U'(\\theta)$ vanishes. The second term becomes $(\\hat{\\theta} - \\mu_0)/\\sigma_0^2 = (0 - 0)/\\sigma_0^2 = 0$. Thus, $U'(0) = 0$.\nThe MAP estimate is $\\hat{\\theta} = 0$. This result arises because the data perfectly match the model prediction at the peak of the prior distribution.\n\n**Part 3: Hessian $U''(\\theta)$ and Laplace Approximation**\n\nWe compute the second derivative of $U(\\theta)$, which is the Hessian $U''(\\theta)$, by differentiating $U'(\\theta)$:\n$$\nU'(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - \\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{\\theta - \\mu_0}{\\sigma_0^2}\n$$\nDifferentiating with respect to $\\theta$:\n$$\nU''(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - 2\\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\nThe Laplace approximation is a Gaussian distribution centered at $\\hat{\\theta}$ with variance $[U''(\\hat{\\theta})]^{-1}$. We evaluate $U''(\\theta)$ at $\\hat{\\theta}=0$:\n$$\nU''(\\hat{\\theta}) = U''(0) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(0) - 2\\exp(0)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\n$$\nU''(\\hat{\\theta}) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(1 - 2) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\n$$\nU''(\\hat{\\theta}) = \\frac{h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\nThe Laplace approximation is the Gaussian PDF $p_{L}(\\theta) = \\mathcal{N}(\\hat{\\theta}, [U''(\\hat{\\theta})]^{-1})$, with $\\hat{\\theta}=0$ and the variance given by the inverse of the expression above.\n\n**Part 4: Non-Gaussianity Indicator $\\eta$**\n\nThe non-Gaussianity indicator is defined as $\\eta = |U^{(3)}(\\hat{\\theta})| / (U''(\\hat{\\theta}))^{3/2}$. We first compute the third derivative, $U^{(3)}(\\theta)$, by differentiating $U''(\\theta)$:\n$$\nU''(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - 2\\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2}\n$$\n$$\nU^{(3)}(\\theta) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(\\theta) - 4\\exp(2\\theta)) \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\nEvaluating at $\\hat{\\theta}=0$:\n$$\nU^{(3)}(\\hat{\\theta}) = U^{(3)}(0) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(\\exp(0) - 4\\exp(0)) \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\n$$\nU^{(3)}(\\hat{\\theta}) = -\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2}(1 - 4) \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\n$$\nU^{(3)}(\\hat{\\theta}) = \\frac{3h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2\n$$\nNow we compute the numerical values.\nThe sum of squares of the temperature differences is:\n$$\n\\sum_{i=1}^{3} (\\Delta T_i)^2 = (10\\,\\text{K})^2 + (20\\,\\text{K})^2 + (30\\,\\text{K})^2 = 100 + 400 + 900 = 1400\\,\\text{K}^2\n$$\nThe given parameters are $h_{\\mathrm{ref}}=100\\,\\text{W m}^{-2}\\text{K}^{-1}$, $\\sigma=100\\,\\text{W m}^{-2}$, and $\\sigma_0=0.5$.\nSo, $\\frac{h_{\\mathrm{ref}}^2}{\\sigma^2} = \\frac{100^2}{100^2} = 1\\,\\text{K}^{-2}$ and $\\sigma_0^2 = 0.5^2 = 0.25$.\n\nLet's compute $U''(\\hat{\\theta})$ and $U^{(3)}(\\hat{\\theta})$ numerically:\n$$\nU''(\\hat{\\theta}) = \\frac{h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 + \\frac{1}{\\sigma_0^2} = (1) \\cdot (1400) + \\frac{1}{0.25} = 1400 + 4 = 1404\n$$\n$$\nU^{(3)}(\\hat{\\theta}) = \\frac{3h_{\\mathrm{ref}}^2}{\\sigma^2} \\sum_{i=1}^{N} (\\Delta T_i)^2 = 3 \\cdot (1) \\cdot (1400) = 4200\n$$\nThe non-Gaussianity indicator $\\eta$ is:\n$$\n\\eta = \\frac{|U^{(3)}(\\hat{\\theta})|}{(U''(\\hat{\\theta}))^{3/2}} = \\frac{|4200|}{(1404)^{3/2}} = \\frac{4200}{1404\\sqrt{1404}}\n$$\nCalculating the numerical value:\n$$\n\\eta = \\frac{4200}{52607.889...} \\approx 0.0798364...\n$$\nRounding to four significant figures, we get $\\eta \\approx 0.07984$.",
            "answer": "$$\\boxed{0.07984}$$"
        },
        {
            "introduction": "While approximations are useful, the gold standard for exploring complex posterior distributions is Markov Chain Monte Carlo (MCMC) sampling. This exercise walks you through the core logic of the Metropolis-Hastings algorithm, one of the foundational MCMC methods. You will derive the crucial acceptance probability for a heat conduction problem, gaining insight into how the algorithm navigates the parameter space to generate samples from the target posterior distribution .",
            "id": "3938038",
            "problem": "Consider a one-dimensional steady heat conduction problem in a slab of thickness $L$ with thermal conductivity $k$ and a convective boundary at $x=L$ with heat transfer coefficient $h$ and ambient temperature $T_{\\infty}$. The left boundary at $x=0$ is held at a known temperature $T_{L}$. Under the assumptions of Fourier's law of heat conduction and steady state, the governing equation is $-\\frac{d}{dx}\\left(k\\,\\frac{dT}{dx}\\right)=0$, and the convective boundary condition is $-k\\,\\frac{dT}{dx}(L)=h\\left(T(L)-T_{\\infty}\\right)$. The solution is linear in $x$, $T(x)=T_{L}+a\\,x$, with slope determined by the boundary condition.\n\nSuppose we collect $M$ temperature measurements $\\{y_{i}\\}_{i=1}^{M}$ at locations $\\{x_{i}\\}_{i=1}^{M}$ within the slab. The measurement model is assumed to be independent Gaussian with known variance $\\sigma^{2}$,\n$$\ny_{i} = T(x_{i};k,h) + \\varepsilon_{i},\\quad \\varepsilon_{i}\\stackrel{\\text{i.i.d.}}{\\sim}\\mathcal{N}(0,\\sigma^{2}),\\quad i=1,\\dots,M.\n$$\nTo enforce physical positivity of $k$ and $h$, we introduce the unconstrained parameter vector $\\theta=\\begin{bmatrix}\\theta_{1}\\\\ \\theta_{2}\\end{bmatrix}$ with $k=\\exp(\\theta_{1})$ and $h=\\exp(\\theta_{2})$. Define the forward map $g(\\theta)$ whose $i$-th component is $T(x_{i};\\exp(\\theta_{1}),\\exp(\\theta_{2}))$ computed from the above physics. Assume a Gaussian prior on $\\theta$,\n$$\np(\\theta)=\\mathcal{N}(\\theta;\\mu,\\Lambda)=\\frac{1}{(2\\pi)^{d/2}\\sqrt{\\det\\Lambda}}\\,\\exp\\left(-\\tfrac{1}{2}(\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu)\\right),\n$$\nwith $d=2$, mean vector $\\mu\\in\\mathbb{R}^{2}$, and positive definite covariance matrix $\\Lambda\\in\\mathbb{R}^{2\\times 2}$. The likelihood follows from the Gaussian error model,\n$$\np(y\\mid \\theta)=\\prod_{i=1}^{M}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\,\\exp\\left(-\\frac{(y_{i}-g_{i}(\\theta))^{2}}{2\\sigma^{2}}\\right)=\\frac{1}{(2\\pi\\sigma^{2})^{M/2}}\\,\\exp\\left(-\\frac{\\|y-g(\\theta)\\|_{2}^{2}}{2\\sigma^{2}}\\right),\n$$\nwhere $y=\\begin{bmatrix}y_{1}&\\cdots&y_{M}\\end{bmatrix}^{\\top}$ and $g(\\theta)=\\begin{bmatrix}g_{1}(\\theta)&\\cdots&g_{M}(\\theta)\\end{bmatrix}^{\\top}$.\n\nWe aim to sample from the posterior $p(\\theta\\mid y)\\propto p(y\\mid \\theta)\\,p(\\theta)$ to calibrate the model and quantify uncertainty. Consider a Metropolis–Hastings algorithm with a Gaussian random-walk proposal\n$$\nq(\\theta'\\mid \\theta)=\\mathcal{N}(\\theta';\\theta,\\Sigma),\n$$\nwhere $\\Sigma$ is a symmetric positive definite proposal covariance. Starting from an initial state $\\theta^{(0)}$, a single Metropolis–Hastings iteration proposes $\\theta'$ from $q(\\theta'\\mid \\theta)$, evaluates an acceptance probability $\\alpha(\\theta,\\theta')$, and accepts the proposal with probability $\\alpha$, otherwise retains the current state.\n\nWhich of the following expressions correctly specifies the acceptance probability for this symmetric Gaussian random-walk, written in a numerically stable log form and its equivalent exponential form, in terms of the forward map $g(\\theta)$, the prior $p(\\theta)=\\mathcal{N}(\\theta;\\mu,\\Lambda)$, and the likelihood $p(y\\mid \\theta)$ defined above?\n\nA. \n$$\n\\log r(\\theta,\\theta')=-\\frac{\\|y-g(\\theta')\\|_{2}^{2}-\\|y-g(\\theta)\\|_{2}^{2}}{2\\sigma^{2}}-\\frac{1}{2}\\left[(\\theta'-\\mu)^{\\top}\\Lambda^{-1}(\\theta'-\\mu)-(\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu)\\right],\n$$\n$$\n\\alpha(\\theta,\\theta')=\\min\\left\\{1,\\exp\\left(\\log r(\\theta,\\theta')\\right)\\right\\}.\n$$\n\nB.\n$$\n\\log r(\\theta,\\theta')=-\\frac{\\|y-g(\\theta')\\|_{2}^{2}-\\|y-g(\\theta)\\|_{2}^{2}}{2\\sigma^{2}}+\\left(\\theta'_{1}+\\theta'_{2}-\\theta_{1}-\\theta_{2}\\right)-\\frac{1}{2}\\left[(\\theta'-\\mu)^{\\top}\\Lambda^{-1}(\\theta'-\\mu)-(\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu)\\right],\n$$\n$$\n\\alpha(\\theta,\\theta')=\\min\\left\\{1,\\exp\\left(\\log r(\\theta,\\theta')\\right)\\right\\}.\n$$\n\nC.\n$$\n\\alpha(\\theta,\\theta')=\\min\\left\\{1,\\frac{p(y\\mid \\theta')\\,p(\\theta')\\,q(\\theta\\mid \\theta')}{p(y\\mid \\theta)\\,p(\\theta)\\,q(\\theta'\\mid \\theta)}\\right\\},\n$$\nwith $q(\\theta'\\mid \\theta)=\\mathcal{N}(\\theta';\\theta,\\Sigma)$ explicitly retained.\n\nD.\n$$\n\\log r(\\theta,\\theta')=+\\frac{\\|y-g(\\theta')\\|_{2}^{2}-\\|y-g(\\theta)\\|_{2}^{2}}{2\\sigma^{2}}-\\frac{1}{2}\\left[(\\theta'-\\mu)^{\\top}\\Lambda^{-1}(\\theta'-\\mu)-(\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu)\\right],\n$$\n$$\n\\alpha(\\theta,\\theta')=\\min\\left\\{1,\\exp\\left(\\log r(\\theta,\\theta')\\right)\\right\\}.\n$$\n\nSelect the single correct option.",
            "solution": "The Metropolis-Hastings algorithm generates samples from a target distribution, in this case, the posterior $p(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)$. The general acceptance probability $\\alpha(\\theta, \\theta')$ for a move from a current state $\\theta$ to a proposed state $\\theta'$ is:\n$$\n\\alpha(\\theta, \\theta') = \\min\\left(1, \\frac{p(\\theta' \\mid y) q(\\theta \\mid \\theta')}{p(\\theta \\mid y) q(\\theta' \\mid \\theta)}\\right) = \\min\\left(1, \\frac{p(y \\mid \\theta')p(\\theta') q(\\theta \\mid \\theta')}{p(y \\mid \\theta)p(\\theta) q(\\theta' \\mid \\theta)}\\right)\n$$\nwhere $q(\\theta' \\mid \\theta)$ is the proposal distribution.\n\nThe problem specifies a symmetric Gaussian random-walk proposal, $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta'; \\theta, \\Sigma)$. Because a Gaussian density centered at $\\theta$ evaluated at $\\theta'$ is equal to a Gaussian density centered at $\\theta'$ evaluated at $\\theta$ for a symmetric covariance $\\Sigma$, we have $q(\\theta \\mid \\theta') = q(\\theta' \\mid \\theta)$. The proposal density terms cancel out, simplifying the acceptance probability to the Metropolis criterion:\n$$\n\\alpha(\\theta, \\theta') = \\min\\left(1, \\frac{p(y \\mid \\theta') p(\\theta')}{p(y \\mid \\theta) p(\\theta)}\\right)\n$$\nFor numerical stability, we work with the log of the ratio $r(\\theta, \\theta') = \\frac{p(y \\mid \\theta') p(\\theta')}{p(y \\mid \\theta) p(\\theta)}$:\n$$\n\\log r(\\theta, \\theta') = \\left[\\log p(y \\mid \\theta') - \\log p(y \\mid \\theta)\\right] + \\left[\\log p(\\theta') - \\log p(\\theta)\\right]\n$$\nLet's compute each term:\n\n1.  **Log-Likelihood Difference**: Given $p(y\\mid \\theta) \\propto \\exp\\left(-\\frac{\\|y-g(\\theta)\\|_{2}^{2}}{2\\sigma^{2}}\\right)$, the log-likelihood is $\\log p(y \\mid \\theta) = \\text{const} - \\frac{1}{2\\sigma^2}\\|y-g(\\theta)\\|_2^2$. The difference becomes:\n    $$\n    \\log p(y \\mid \\theta') - \\log p(y \\mid \\theta) = -\\frac{\\|y-g(\\theta')\\|_2^2 - \\|y-g(\\theta)\\|_2^2}{2\\sigma^2}\n    $$\n\n2.  **Log-Prior Difference**: Given $p(\\theta) \\propto \\exp\\left(-\\tfrac{1}{2}(\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu)\\right)$, the log-prior is $\\log p(\\theta) = \\text{const} - \\frac{1}{2}(\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu)$. The difference becomes:\n    $$\n    \\log p(\\theta') - \\log p(\\theta) = -\\frac{1}{2}\\left( (\\theta'-\\mu)^{\\top}\\Lambda^{-1}(\\theta'-\\mu) - (\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu) \\right)\n    $$\n\nCombining these two results gives the final expression for $\\log r(\\theta, \\theta')$:\n$$\n\\log r(\\theta,\\theta')=-\\frac{\\|y-g(\\theta')\\|_{2}^{2}-\\|y-g(\\theta)\\|_{2}^{2}}{2\\sigma^{2}}-\\frac{1}{2}\\left[(\\theta'-\\mu)^{\\top}\\Lambda^{-1}(\\theta'-\\mu)-(\\theta-\\mu)^{\\top}\\Lambda^{-1}(\\theta-\\mu)\\right]\n$$\nThis exactly matches Option A. Option B incorrectly includes a Jacobian term. Option C is the general, non-simplified formula and is not in the requested log form. Option D has an incorrect sign on the likelihood term, which would penalize proposals that fit the data better. Therefore, Option A is the only correct choice.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Running an MCMC sampler is only half the battle; ensuring the generated samples provide a reliable picture of the posterior is equally critical. This practice moves into the essential skill of MCMC diagnostics, teaching you to interpret key metrics like autocorrelation and the Gelman-Rubin statistic ($\\hat{R}$). By analyzing a challenging scenario rooted in a physical model with strong parameter correlations, you will learn to diagnose poor sampler performance and identify effective remedies .",
            "id": "3938052",
            "problem": "A one-dimensional, steady-state conduction problem is used to calibrate the thermal conductivity $k$ (units $\\mathrm{W\\,m^{-1}\\,K^{-1}}$) and the convective heat transfer coefficient $h$ (units $\\mathrm{W\\,m^{-2}\\,K^{-1}}$) of a homogeneous slab of thickness $L$ under a known, uniform applied heat flux $q_{\\mathrm{in}}$ at $x=0$ and convective cooling to an ambient temperature $T_{\\infty}$ at $x=L$. Measurements of the temperature field $T(x)$ are collected at locations $\\{x_i\\}_{i=1}^{m}$ with independent Gaussian sensor noise of unknown variance bounded by engineering specification (assume it is known and equal to a constant, denoted $\\sigma^2$). The posterior distribution for $(k,h)$ is explored with three independent Markov Chain Monte Carlo (MCMC) chains of equal length, producing the following pooled diagnostics for the stationary segments after warmup:\n\n- There are $3$ chains with post-warmup length $n=3000$ each. The pooled estimates of the first-lag autocorrelation are $\\rho_1(k)=0.95$ and $\\rho_1(h)=0.93$. The estimated integrated autocorrelation times are $\\hat{\\tau}_{\\mathrm{int}}(k)=120$ and $\\hat{\\tau}_{\\mathrm{int}}(h)=90$.\n- The estimated correlation in the log-parameterization is $\\operatorname{corr}(\\log k,\\log h)=-0.98$.\n- The chain means for $k$ are $(170,200,185)$ and the corresponding within-chain sample variances are $(1200,1500,1100)$; for $h$ the chain means are $(70,85,75)$ and the corresponding within-chain sample variances are $(400,500,350)$.\n- The estimated Gelman–Rubin potential scale reduction factors $\\hat{R}$ (computed in the classical unsplit form) are reported as $\\hat{R}(k)\\approx 1.11$ and $\\hat{R}(h)\\approx 1.09$.\n\nAssume the deterministic forward model follows the physical laws of conduction and convection with constant properties and steady-state conditions, and that the likelihood of the data under $(k,h)$ is Gaussian with mean given by the model prediction at each $x_i$ and variance $\\sigma^2$. Based on the foundational physical relations for conduction and convection, and the core definitions of effective sample size, autocorrelation, and $\\hat{R}$, which of the following options most appropriately diagnoses the mixing and convergence behavior of the chains and proposes an effective remedy that targets the root cause?\n\nA. The posterior exhibits a narrow, strongly correlated ridge in $(k,h)$ induced by the additive thermal resistances $L/k$ and $1/h$, consistent with $\\operatorname{corr}(\\log k,\\log h)=-0.98$, small effective sample sizes, and $\\hat{R}>1.05$. Remedy: sample $(k,h)$ jointly using a covariance-adapted blocked proposal aligned with the posterior covariance, or reparameterize to $(r_{\\mathrm{tot}},\\mathrm{Bi})$ where $r_{\\mathrm{tot}}=L/k+1/h$ and $\\mathrm{Bi}=hL/k$, with a log-transform to enforce positivity; alternatively, use geometry-aware MCMC such as Hamiltonian Monte Carlo with mass matrix adaptation. These actions should increase effective sample size and reduce $\\hat{R}$ towards $1$.\n\nB. The high autocorrelation is best addressed by thinning each chain by a factor of $10$, which will increase the effective sample size by roughly $10\\times$ and reduce $\\hat{R}$ below $1.05$ without any need for reparameterization or blocking.\n\nC. Increasing the number of chains from $3$ to $10$ while keeping the same sampler and per-chain length will reduce $\\hat{R}$ towards $1$ and resolve the mixing pathologies even if the autocorrelation and parameter correlation remain high.\n\nD. Decreasing the random-walk proposal step size will reduce autocorrelation and increase effective sample size; keeping single-parameter updates of $k$ and $h$ is sufficient, so no blocking or reparameterization is necessary.\n\nSelect the single best option.",
            "solution": "This problem presents a classic MCMC diagnostic challenge. Let's break down the provided information to diagnose the issue and identify the correct remedy.\n\n1.  **Diagnosis of MCMC Performance:**\n    *   **Convergence:** The Gelman-Rubin diagnostics $\\hat{R}(k)\\approx 1.11$ and $\\hat{R}(h)\\approx 1.09$ are significantly greater than the conventional threshold of 1.05, indicating that the independent chains have not converged to the same distribution. The different means for each chain confirm this.\n    *   **Mixing and Efficiency:** The first-lag autocorrelations ($\\rho_1 \\approx 0.95$) and integrated autocorrelation times ($\\hat{\\tau}_{\\mathrm{int}} \\ge 90$) are extremely high. This signifies poor mixing: consecutive samples are highly dependent, and the chains explore the parameter space very slowly. This leads to a very low effective sample size (ESS), calculated as $N_{total} / \\hat{\\tau}_{\\mathrm{int}}$, meaning the samples provide little information about the posterior.\n\n2.  **Root Cause Analysis:**\n    The key clue is the extreme parameter correlation: $\\operatorname{corr}(\\log k,\\log h)=-0.98$. This is not a numerical artifact but is caused by the physics. The temperature in the slab is governed by the total thermal resistance, which is a function of both $L/k$ (conductive resistance) and $1/h$ (convective resistance). Many different pairs of $(k,h)$ can yield a similar total resistance and thus produce similar temperature predictions, creating a long, narrow, high-probability \"ridge\" in the posterior distribution. Standard MCMC samplers that update one parameter at a time struggle to navigate such ridges, leading to the observed poor mixing and convergence.\n\n3.  **Evaluating Proposed Remedies:**\n    An effective remedy must directly address the strong parameter correlation.\n\n*   **Option A:** This option correctly identifies the physical cause (additive thermal resistances $L/k$ and $1/h$), links it to the observed statistical pathologies (correlated ridge, high $\\hat{R}$, low ESS), and proposes state-of-the-art remedies that target the root cause. Covariance-adapted blocking, reparameterization (e.g., to total resistance and Biot number), and geometry-aware methods like Hamiltonian Monte Carlo are all designed to efficiently explore correlated posteriors. This is the correct, expert-level analysis.\n\n*   **Option B:** This is incorrect. Thinning (discarding samples) does not improve the efficiency of the sampler or increase the effective sample size. It merely reduces storage size and the autocorrelation of the *stored* chain, but the information content of the original, inefficiently-generated chain remains the same. It does not fix the convergence problem.\n\n*   **Option C:** This is incorrect. Running more chains is useful for diagnosing convergence problems, but it does not fix them. If the sampler itself is inefficient, all 10 chains will mix just as poorly as the original 3. It fails to address the root cause.\n\n*   **Option D:** This is incorrect. For a random-walk sampler, decreasing the step size typically *increases* autocorrelation by making the chain move more slowly, although it may increase the acceptance rate. More importantly, it asserts that single-parameter updates are sufficient, which is the exact opposite of the correct conclusion. The problem's core difficulty lies in the correlation, which requires joint updates (blocking) or reparameterization.\n\nTherefore, Option A provides the only accurate diagnosis and effective set of remedies.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}