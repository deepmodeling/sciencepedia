{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of generalized Polynomial Chaos (gPC) hinges on choosing a polynomial basis that is orthogonal with respect to the probability distributions of the uncertain inputs. This practice guides you through the foundational process of constructing such a basis for a common scenario involving mixed input types—a uniform and a normal random variable. By completing this exercise , you will solidify your understanding of how to properly normalize univariate polynomials and combine them into a multivariate basis, and you will learn how to quantify the size of the basis under the widely used total-degree truncation scheme.",
            "id": "4001453",
            "problem": "A one-dimensional, transient heat conduction model in a homogeneous slab is solved numerically to predict a spatially averaged temperature response, with parametric uncertainty modeled by two independent random inputs. The first input, denoted by $\\xi_{1}$, represents a scaled uncertainty in the thermal conductivity and is modeled as a uniform random variable on the interval $[-1,1]$. The second input, denoted by $\\xi_{2}$, represents an uncertainty in the volumetric heat generation rate and is modeled as a standard normal random variable with zero mean and unit variance. To perform uncertainty quantification, the response is approximated using generalized Polynomial Chaos (gPC), which requires constructing an orthonormal polynomial basis consistent with the input probability measures.\n\nStarting from the definitions of orthogonal polynomials with respect to given weight functions, and using the independence of $\\xi_{1}$ and $\\xi_{2}$, construct the multivariate orthonormal gPC basis up to total polynomial degree $p$ for the input pair $(\\xi_{1},\\xi_{2})$, where Legendre polynomials are used for $\\xi_{1}$ and Hermite polynomials are used for $\\xi_{2}$. Explicitly specify the normalization of the univariate polynomials so that the resulting multivariate basis is orthonormal under the joint probability measure of $(\\xi_{1},\\xi_{2})$.\n\nThen, derive a closed-form expression for the number of multivariate basis functions retained under the total-degree truncation $i+j\\leq p$, where $i$ is the degree in $\\xi_{1}$ and $j$ is the degree in $\\xi_{2}$. Express your final answer as a single analytic expression in terms of $p$. No rounding is required and no units are needed. The final answer should be the expression for the basis size as a function of $p$.",
            "solution": "The problem statement is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- **Model**: One-dimensional, transient heat conduction.\n- **Quantity of Interest**: Spatially averaged temperature response.\n- **Input Uncertainty**: Modeled by two independent random inputs, $\\xi_{1}$ and $\\xi_{2}$.\n- **Input $\\xi_{1}$**: Represents scaled uncertainty in thermal conductivity.\n- **Distribution of $\\xi_{1}$**: Uniform random variable on the interval $[-1, 1]$.\n- **Input $\\xi_{2}$**: Represents uncertainty in the volumetric heat generation rate.\n- **Distribution of $\\xi_{2}$**: Standard normal random variable (zero mean, unit variance).\n- **Uncertainty Quantification Method**: Generalized Polynomial Chaos (gPC).\n- **Task 1**: Construct the multivariate orthonormal gPC basis up to total polynomial degree $p$ for the input pair $(\\xi_{1}, \\xi_{2})$.\n- **Polynomial for $\\xi_{1}$**: Legendre polynomials.\n- **Polynomial for $\\xi_{2}$**: Hermite polynomials.\n- **Requirement**: Explicitly specify the normalization of the univariate polynomials for orthonormality of the multivariate basis under the joint probability measure.\n- **Task 2**: Derive a closed-form expression for the number of multivariate basis functions retained under the total-degree truncation $i+j \\leq p$, where $i$ is the degree in $\\xi_{1}$ and $j$ is the degree in $\\xi_{2}$.\n- **Final Answer Format**: A single analytic expression for the basis size as a function of $p$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is well-grounded in computational science and engineering, specifically in the field of uncertainty quantification. The use of generalized Polynomial Chaos (gPC) with Legendre and Hermite polynomials for uniform and normal random variables, respectively, is a standard and correct application of the Wiener-Askey scheme.\n- **Well-Posed**: The problem is well-posed. It requests the construction of a mathematical object (an orthonormal basis) and the derivation of a combinatorial formula, for which sufficient information is provided. The solution is unique and meaningful.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from any subjectivity or bias.\n- **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, realistic, well-posed, and non-trivial.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in two parts as requested: first, the construction of the orthonormal multivariate basis, and second, the derivation of the formula for the number of basis functions under a total-degree truncation.\n\n#### Part 1: Construction of the Orthonormal Basis\n\nThe gPC methodology requires constructing a set of polynomials that are orthonormal with respect to the probability measure of the random inputs. For a set of independent random variables $\\boldsymbol{\\xi} = (\\xi_{1}, \\xi_{2}, \\dots, \\xi_{d})$, the multivariate orthonormal basis is formed by the tensor product of the univariate orthonormal polynomials.\n\n**Univariate Basis for $\\xi_{1}$ (Uniform Distribution)**\n\nThe random variable $\\xi_{1}$ follows a uniform distribution on $[-1, 1]$. Its probability density function (PDF) is:\n$$ f_{\\xi_{1}}(x) = \\begin{cases} \\frac{1}{2} & \\text{if } x \\in [-1, 1] \\\\ 0 & \\text{otherwise} \\end{cases} $$\nThe associated weight function for the inner product is $w_{1}(x) = f_{\\xi_{1}}(x) = \\frac{1}{2}$ on the support $[-1, 1]$. The problem specifies using Legendre polynomials, $P_{i}(x)$, for this distribution. The standard orthogonality relation for Legendre polynomials is defined with respect to a weight of $1$:\n$$ \\int_{-1}^{1} P_{i}(x) P_{k}(x) dx = \\frac{2}{2i+1} \\delta_{ik} $$\nwhere $\\delta_{ik}$ is the Kronecker delta.\n\nTo obtain polynomials $\\Phi_{i}^{(1)}(\\xi_{1})$ that are orthonormal with respect to the probability measure of $\\xi_{1}$, we must satisfy the condition:\n$$ \\langle \\Phi_{i}^{(1)}, \\Phi_{k}^{(1)} \\rangle \\equiv \\int_{-1}^{1} \\Phi_{i}^{(1)}(x) \\Phi_{k}^{(1)}(x) f_{\\xi_{1}}(x) dx = \\delta_{ik} $$\nLet $\\Phi_{i}^{(1)}(x) = c_{i} P_{i}(x)$ for some normalization constant $c_{i}$. Substituting this into the orthonormality condition:\n$$ \\int_{-1}^{1} \\left( c_{i} P_{i}(x) \\right) \\left( c_{k} P_{k}(x) \\right) \\frac{1}{2} dx = \\delta_{ik} $$\nFor $i=k$, this becomes:\n$$ c_{i}^{2} \\frac{1}{2} \\int_{-1}^{1} P_{i}(x)^{2} dx = 1 $$\n$$ c_{i}^{2} \\frac{1}{2} \\left( \\frac{2}{2i+1} \\right) = 1 $$\n$$ c_{i}^{2} \\frac{1}{2i+1} = 1 \\implies c_{i} = \\sqrt{2i+1} $$\nThus, the univariate orthonormal polynomials for $\\xi_{1}$ are:\n$$ \\Phi_{i}^{(1)}(\\xi_{1}) = \\sqrt{2i+1} P_{i}(\\xi_{1}) $$\n\n**Univariate Basis for $\\xi_{2}$ (Standard Normal Distribution)**\n\nThe random variable $\\xi_{2}$ follows a standard normal distribution, $\\mathcal{N}(0, 1)$. Its PDF is:\n$$ f_{\\xi_{2}}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right) $$\nThis PDF itself is the weight function for the probabilists' Hermite polynomials, commonly denoted $He_{j}(x)$. The standard orthogonality relation for these polynomials is:\n$$ \\int_{-\\infty}^{\\infty} He_{j}(x) He_{k}(x) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right) dx = j! \\delta_{jk} $$\nThe integral is already defined with respect to the probability measure of $\\xi_{2}$. Let the orthonormal polynomials be $\\Phi_{j}^{(2)}(\\xi_{2}) = d_{j} He_{j}(\\xi_{2})$. The orthonormality condition is:\n$$ \\langle \\Phi_{j}^{(2)}, \\Phi_{k}^{(2)} \\rangle \\equiv \\int_{-\\infty}^{\\infty} \\Phi_{j}^{(2)}(x) \\Phi_{k}^{(2)}(x) f_{\\xi_{2}}(x) dx = \\delta_{jk} $$\nFor $j=k$, this becomes:\n$$ d_{j}^{2} \\int_{-\\infty}^{\\infty} He_{j}(x)^{2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right) dx = 1 $$\n$$ d_{j}^{2} (j!) = 1 \\implies d_{j} = \\frac{1}{\\sqrt{j!}} $$\nThus, the univariate orthonormal polynomials for $\\xi_{2}$ are:\n$$ \\Phi_{j}^{(2)}(\\xi_{2}) = \\frac{1}{\\sqrt{j!}} He_{j}(\\xi_{2}) $$\n\n**Multivariate Orthonormal Basis**\n\nSince $\\xi_{1}$ and $\\xi_{2}$ are independent, the joint PDF is $f_{(\\xi_{1}, \\xi_{2})}(x_{1}, x_{2}) = f_{\\xi_{1}}(x_{1}) f_{\\xi_{2}}(x_{2})$. The multivariate gPC basis functions, denoted $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$ with $\\boldsymbol{\\xi}=(\\xi_{1}, \\xi_{2})$ and multi-index $\\boldsymbol{\\alpha}=(i, j)$, are formed by the tensor product of the univariate orthonormal polynomials:\n$$ \\Psi_{(i,j)}(\\xi_{1}, \\xi_{2}) = \\Phi_{i}^{(1)}(\\xi_{1}) \\Phi_{j}^{(2)}(\\xi_{2}) = \\left(\\sqrt{2i+1} P_{i}(\\xi_{1})\\right) \\left(\\frac{1}{\\sqrt{j!}} He_{j}(\\xi_{2})\\right) $$\nThese basis functions are orthonormal with respect to the joint probability measure:\n$$ \\langle \\Psi_{(i,j)}, \\Psi_{(k,l)} \\rangle = \\int_{-\\infty}^{\\infty} \\int_{-1}^{1} \\Psi_{(i,j)}(x_{1}, x_{2}) \\Psi_{(k,l)}(x_{1}, x_{2}) f_{\\xi_{1}}(x_{1}) f_{\\xi_{2}}(x_{2}) dx_{1} dx_{2} $$\n$$ = \\langle \\Phi_{i}^{(1)}, \\Phi_{k}^{(1)} \\rangle \\langle \\Phi_{j}^{(2)}, \\Phi_{l}^{(2)} \\rangle = \\delta_{ik} \\delta_{jl} = \\delta_{(i,j),(k,l)} $$\n\n#### Part 2: Number of Basis Functions for Total-Degree Truncation\n\nThe problem specifies a total-degree truncation scheme. The set of basis functions is defined by the multi-indices $(i,j)$ such that the total polynomial degree $i+j$ is less than or equal to a prescribed maximum degree $p$.\n$$ \\mathcal{J}_{p} = \\{ (i,j) \\in \\mathbb{N}_{0}^{2} \\, | \\, i+j \\leq p \\} $$\nwhere $\\mathbb{N}_{0} = \\{0, 1, 2, \\dots \\}$. We need to find the cardinality of this set, denoted by $N_{p} = |\\mathcal{J}_{p}|$.\n\nWe can compute this by summing over all possible total degrees $k$ from $0$ to $p$. For a fixed total degree $k$, we need to find the number of non-negative integer pairs $(i,j)$ such that $i+j=k$. The possible values for $j$ are $0, 1, \\dots, k$. For each such $j$, the value of $i$ is uniquely determined as $i=k-j$. This gives $k+1$ possible pairs for a fixed total degree $k$.\n\nThe total number of basis functions is the sum of these counts for $k=0, 1, \\dots, p$:\n$$ N_{p} = \\sum_{k=0}^{p} (\\text{number of pairs with total degree } k) = \\sum_{k=0}^{p} (k+1) $$\nThis is the sum of the first $p+1$ integers:\n$$ N_{p} = 1 + 2 + 3 + \\dots + (p+1) $$\nUsing the formula for the sum of an arithmetic series, $\\sum_{m=1}^{n} m = \\frac{n(n+1)}{2}$, with $n = p+1$:\n$$ N_{p} = \\frac{(p+1)((p+1)+1)}{2} = \\frac{(p+1)(p+2)}{2} $$\nThis expression can also be written using a binomial coefficient. The problem is equivalent to finding the number of non-negative integer solutions to $i+j \\le p$, which is equivalent to finding the number of non-negative integer solutions to $i+j+s=p$ where $s$ is a slack variable. This is a classic stars and bars problem for distributing $p$ items into $3$ bins, the number of solutions for which is:\n$$ N_{p} = \\binom{p+3-1}{3-1} = \\binom{p+2}{2} = \\frac{(p+2)!}{2!(p)!} = \\frac{(p+2)(p+1)}{2} $$\nThe final closed-form expression for the number of multivariate basis functions is a function of $p$.",
            "answer": "$$\\boxed{\\frac{(p+1)(p+2)}{2}}$$"
        },
        {
            "introduction": "Once a Polynomial Chaos basis is established, the next critical step is to compute the expansion coefficients. This is an estimation problem that involves a trade-off between computational cost, bias, and variance, especially when solver noise is present. This exercise  places you in a realistic scenario where you must compare two competing strategies—deterministic quadrature and least-squares regression—to understand how to best allocate a finite computational budget to minimize estimation error in the presence of noise.",
            "id": "4001483",
            "problem": "A transient heat conduction model in a two-dimensional slab computes the temperature response $T(\\boldsymbol{\\xi})$ at a fixed sensor location, where the uncertain inputs are the thermal conductivity and surface heat flux, modeled as independent standard normal random variables $\\boldsymbol{\\xi} = (\\xi_1,\\xi_2) \\sim \\mathcal{N}(0,1) \\times \\mathcal{N}(0,1)$. You build a Polynomial Chaos Expansion (PCE) using multivariate Hermite polynomials orthonormal with respect to the joint Gaussian measure, truncated at total degree $p=3$, yielding $P=\\binom{2+3}{3}=10$ coefficients. Each model evaluation returns\n$Y = f(\\boldsymbol{\\xi}) + \\varepsilon$,\nwhere $f(\\boldsymbol{\\xi})$ is the noiseless model response and $\\varepsilon$ is independent, zero-mean additive solver noise with variance $\\sigma^2 = 10^{-4}$, independent across calls. You have a total budget of $M=120$ model evaluations.\n\nTwo strategies for estimating the $P$ PCE coefficients are under consideration:\n\n- Deterministic projection by tensorized Gauss–Hermite quadrature exact for polynomials up to degree $2p$ in each coordinate. This uses $n=p+1=4$ nodes per dimension, so $N_q = 4^2 = 16$ nodes in total. To utilize the full budget, you may replicate $R = \\lfloor M/N_q \\rfloor$ evaluations at each quadrature node and average them at each node before projection.\n\n- Random sampling with independent draws $\\{\\boldsymbol{\\xi}^{(i)}\\}_{i=1}^N$ from the joint Gaussian distribution and linear least-squares regression onto the $P$-term PCE basis, using $N$ unique samples (no replication), with $N \\le M$.\n\nAssume $f(\\boldsymbol{\\xi})$ is analytic in each variable so that when there is no solver noise, deterministic quadrature integration error for smooth integrands decays rapidly with increasing quadrature order, but at fixed order $n=4$ it is small yet not exactly zero for nonpolynomial integrands. The goal is to minimize the mean squared error of the estimated coefficients relative to their exact projection values.\n\nSelect all statements that are correct about the trade-off between deterministic quadrature and random sampling under the given budget and noise model.\n\nA. With $\\sigma^2>0$ and $M \\gg P$, using all $M$ unique random samples in least-squares yields an asymptotic per-coefficient variance that scales like $O(\\sigma^2/M)$ without incurring quadrature integration bias, whereas allocating $M$ across $R$ replicas at $N_q=16$ fixed quadrature nodes cannot reduce its integration bias and achieves noise variance of the same order but with a larger constant; thus, least-squares is favored under the stated budget.\n\nB. For zero-mean noise $\\varepsilon$, deterministic quadrature without replication remains unbiased for the coefficients because the quadrature rule integrates the noise term to zero exactly.\n\nC. Replicating $R$ times at each quadrature node reduces the noise-induced variance of each coefficient estimate by a factor $1/R$ but does not alter the quadrature integration error of the noiseless term; therefore, there exists a noise-dominated regime in which replication at fixed nodes is beneficial up to the point where $\\sigma^2/R$ matches the squared quadrature bias, beyond which allocating evaluations to new, independent random samples is preferable.\n\nD. Latin hypercube sampling is necessary to preserve the orthonormality of the Hermite basis under sampling; consequently, it strictly dominates independent Gaussian sampling for least-squares coefficient estimation with solver noise.\n\nE. If $P$ is the number of retained coefficients, choosing $N \\approx c\\, P \\log P$ independent Gaussian samples for some moderate constant $c>1$ yields, with high probability, a well-conditioned design matrix and coefficient estimator variance scaling as $O(\\sigma^2/N)$; this makes random least-squares robust to solver noise relative to fixed-order deterministic quadrature as $P$ and the input dimension $d$ grow.",
            "solution": "The user wants me to critically analyze a problem concerning the estimation of Polynomial Chaos Expansion (PCE) coefficients for a computational model with solver noise, comparing two strategies: deterministic quadrature with replication and random sampling with least-squares.\n\n### Validation of the Problem Statement\n\n**Step 1: Extract Givens**\n- Model Output: Temperature response $T(\\boldsymbol{\\xi})$, denoted as a function $f(\\boldsymbol{\\xi})$.\n- Uncertain Inputs: $\\boldsymbol{\\xi} = (\\xi_1, \\xi_2)$ with $\\xi_1, \\xi_2 \\sim \\mathcal{N}(0,1)$ being independent standard normal random variables. Input dimension is $d=2$.\n- PCE Basis: Multivariate Hermite polynomials, orthonormal with respect to the standard Gaussian measure.\n- PCE Truncation: Total degree $p=3$.\n- Number of PCE coefficients: $P = \\binom{d+p}{d} = \\binom{2+3}{2} = 10$.\n- Model Evaluation with Noise: $Y = f(\\boldsymbol{\\xi}) + \\varepsilon$, where $f(\\boldsymbol{\\xi})$ is the noiseless response.\n- Solver Noise: $\\varepsilon$ is an independent, zero-mean additive noise, with variance $\\sigma^2 = 10^{-4}$.\n- Total Evaluation Budget: $M=120$.\n- Strategy 1 (Deterministic Quadrature):\n    - Tensorized Gauss-Hermite quadrature.\n    - Nodes per dimension: $n=p+1=4$. The rule is exact for polynomials of degree up to $2n-1 = 7$.\n    - Total quadrature nodes: $N_q = n^d = 4^2 = 16$.\n    - Replications per node: $R = \\lfloor M/N_q \\rfloor = \\lfloor 120/16 \\rfloor = \\lfloor 7.5 \\rfloor = 7$. Total evaluations used: $N_q \\times R = 16 \\times 7 = 112$.\n    - Procedure: Average $R$ evaluations at each node, then project onto the PCE basis.\n- Strategy 2 (Random Sampling):\n    - Independent draws $\\{\\boldsymbol{\\xi}^{(i)}\\}_{i=1}^N$ from the joint Gaussian distribution.\n    - Estimation via linear least-squares regression.\n    - Number of unique samples: $N \\le M$.\n- Assumptions:\n    - $f(\\boldsymbol{\\xi})$ is analytic, implying it's smooth but not necessarily a polynomial.\n    - The quadrature rule has a non-zero integration error for the noiseless function $f(\\boldsymbol{\\xi})$.\n- Goal: Minimize the mean squared error (MSE) of the estimated coefficients relative to their exact projection values.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound and well-posed. It describes a standard scenario in Uncertainty Quantification (UQ). The use of PCE, Gauss-Hermite quadrature for Gaussian random variables, and least-squares regression are all canonical methods. The parameters ($d=2$, $p=3$, $P=10$, $M=120$, $\\sigma^2=10^{-4}$) are well-defined and consistent. The problem asks for a comparison of the trade-offs between two competing methods, which is a core topic in numerical UQ. The language is precise and objective. There are no violations of scientific principles, logical inconsistencies, or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem statement is VALID. I will proceed with the detailed analysis and solution.\n\n### Derivation of Solution\n\nThe goal is to estimate the coefficients $c_j$ of the PCE approximation $f(\\boldsymbol{\\xi}) \\approx \\sum_{j=0}^{P-1} c_j \\Psi_j(\\boldsymbol{\\xi})$, where $\\{\\Psi_j\\}$ are the orthonormal Hermite polynomials. The exact coefficients are given by the projection $c_j = \\mathbb{E}[f(\\boldsymbol{\\xi})\\Psi_j(\\boldsymbol{\\xi})]$. An estimator $\\hat{c}_j$ has a Mean Squared Error (MSE) given by $\\text{MSE}(\\hat{c}_j) = \\mathbb{E}[(\\hat{c}_j - c_j)^2] = (\\mathbb{E}[\\hat{c}_j] - c_j)^2 + \\text{Var}(\\hat{c}_j) = \\text{Bias}^2 + \\text{Variance}$.\n\n**Strategy 1: Deterministic Quadrature with Replication**\nThe quadrature-based estimator for $c_j$ is given by:\n$$ \\hat{c}_j^{quad} = \\sum_{k=1}^{N_q} \\bar{Y}_k \\Psi_j(\\boldsymbol{\\xi}^{(k)}) w_k $$\nwhere $\\boldsymbol{\\xi}^{(k)}$ are the $N_q=16$ quadrature nodes, $w_k$ are the corresponding weights, and $\\bar{Y}_k$ is the average of $R=7$ model evaluations at node $k$:\n$$ \\bar{Y}_k = \\frac{1}{R} \\sum_{r=1}^{R} (f(\\boldsymbol{\\xi}^{(k)}) + \\varepsilon_{k,r}) = f(\\boldsymbol{\\xi}^{(k)}) + \\bar{\\varepsilon}_k $$\nwhere $\\bar{\\varepsilon}_k = \\frac{1}{R} \\sum_{r=1}^{R} \\varepsilon_{k,r}$. Since the individual noise terms $\\varepsilon_{k,r}$ are independent and have variance $\\sigma^2$, the averaged noise $\\bar{\\varepsilon}_k$ has mean $\\mathbb{E}[\\bar{\\varepsilon}_k]=0$ and variance $\\text{Var}(\\bar{\\varepsilon}_k) = \\sigma^2/R$.\n\nThe bias of the estimator is:\n$$ \\text{Bias}(\\hat{c}_j^{quad}) = \\mathbb{E}[\\hat{c}_j^{quad}] - c_j = \\left( \\sum_{k=1}^{N_q} f(\\boldsymbol{\\xi}^{(k)}) \\Psi_j(\\boldsymbol{\\xi}^{(k)}) w_k \\right) - c_j = c_j^q - c_j $$\nThis bias is the quadrature integration error, which is non-zero (as stated in the problem) and is independent of the noise level $\\sigma^2$ and the number of replications $R$.\n\nThe variance of the estimator is due to the noise term:\n$$ \\text{Var}(\\hat{c}_j^{quad}) = \\text{Var}\\left( \\sum_{k=1}^{N_q} \\bar{\\varepsilon}_k \\Psi_j(\\boldsymbol{\\xi}^{(k)}) w_k \\right) = \\sum_{k=1}^{N_q} (\\Psi_j(\\boldsymbol{\\xi}^{(k)}) w_k)^2 \\text{Var}(\\bar{\\varepsilon}_k) = \\frac{\\sigma^2}{R} \\left( \\sum_{k=1}^{N_q} (\\Psi_j(\\boldsymbol{\\xi}^{(k)}) w_k)^2 \\right) $$\nThe total MSE is $\\text{MSE}(\\hat{c}_j^{quad}) = (c_j^q - c_j)^2 + \\frac{\\sigma^2}{R} C_j$, where $C_j = \\sum_{k=1}^{N_q} (\\Psi_j(\\boldsymbol{\\xi}^{(k)}) w_k)^2$ is a constant for the given quadrature rule.\n\n**Strategy 2: Random Sampling with Least Squares**\nWe generate $N$ unique samples $\\{\\boldsymbol{\\xi}^{(i)}\\}_{i=1}^N$ and obtain outputs $\\{Y_i\\}_{i=1}^N$. We solve the linear system $\\mathbf{\\Psi c} \\approx \\mathbf{Y}$ in a least-squares sense, where $\\mathbf{\\Psi}$ is the $N \\times P$ design matrix with entries $\\Psi_{ij} = \\Psi_j(\\boldsymbol{\\xi}^{(i)})$. The estimator is $\\hat{\\mathbf{c}}^{ls} = (\\mathbf{\\Psi}^T \\mathbf{\\Psi})^{-1} \\mathbf{\\Psi}^T \\mathbf{Y}$.\n\nThe bias of this estimator has two main sources: truncation error (as $f$ is not in the span of the basis) and aliasing/sampling error from using a finite sample set $\\{ \\boldsymbol{\\xi}^{(i)} \\}$ to approximate the continuous expectation. This bias generally decreases as $N$ increases. Crucially, it does not suffer from the systematic integration error of a fixed-point quadrature rule.\n\nThe variance of the estimator, for a fixed set of samples, is:\n$$ \\text{Var}(\\hat{\\mathbf{c}}^{ls}) = \\text{Var}((\\mathbf{\\Psi}^T \\mathbf{\\Psi})^{-1} \\mathbf{\\Psi}^T \\boldsymbol{\\varepsilon}) = \\sigma^2 (\\mathbf{\\Psi}^T \\mathbf{\\Psi})^{-1} $$\nFor large $N$, due to the orthonormality of the basis polynomials, we have $\\frac{1}{N}\\mathbf{\\Psi}^T \\mathbf{\\Psi} \\to \\mathbf{I}$, the identity matrix. Thus, the variance-covariance matrix is asymptotically $\\text{Var}(\\hat{\\mathbf{c}}^{ls}) \\approx \\frac{\\sigma^2}{N} \\mathbf{I}$. The variance of a single coefficient is $\\text{Var}(\\hat{c}_j^{ls}) \\approx \\sigma^2/N$. If we use the full budget, $N=M=120$.\n\n### Evaluation of Options\n\n**A. With $\\sigma^2>0$ and $M \\gg P$, using all $M$ unique random samples in least-squares yields an asymptotic per-coefficient variance that scales like $O(\\sigma^2/M)$ without incurring quadrature integration bias, whereas allocating $M$ across $R$ replicas at $N_q=16$ fixed quadrature nodes cannot reduce its integration bias and achieves noise variance of the same order but with a larger constant; thus, least-squares is favored under the stated budget.**\n\n- The variance for least-squares with $N=M$ samples is indeed approximately $\\sigma^2/M$, so it scales as $O(\\sigma^2/M)$. Correct.\n- Least-squares does not use quadrature, so it does not incur quadrature integration bias. Correct.\n- Replication at fixed quadrature nodes reduces noise but cannot change the function values $f(\\boldsymbol{\\xi}^{(k)})$, so it cannot reduce the integration bias. Correct.\n- The variance for quadrature is $\\frac{\\sigma^2}{R} C_j = \\frac{\\sigma^2}{\\lfloor M/N_q \\rfloor} C_j \\approx \\frac{\\sigma^2 N_q}{M} C_j$. This is also of order $O(\\sigma^2/M)$, but with a constant pre-factor of $N_q C_j$. It is a known result that for a fixed total number of evaluations $M$, unique samples are more efficient at reducing variance than replicated samples, meaning the constant for LS is smaller than for replicated quadrature. Thus, the variance from quadrature has \"a larger constant\". Correct.\n- Given that $M=120 \\gg P=10$, LS is well-posed. LS has a lower variance ($\\text{Var}_{LS} \\approx \\sigma^2/120$) compared to quadrature ($\\text{Var}_{quad} \\propto \\sigma^2/7$), and it avoids the fixed quadrature bias. Therefore, a rational assessment is that least-squares is the favored strategy. The statement is a correct and complete comparison.\n**Verdict: Correct.**\n\n**B. For zero-mean noise $\\varepsilon$, deterministic quadrature without replication remains unbiased for the coefficients because the quadrature rule integrates the noise term to zero exactly.**\n\n- The estimator is \"unbiased for the coefficients\" if $\\mathbb{E}[\\hat{c}_j] = c_j$.\n- As shown above, $\\mathbb{E}[\\hat{c}_j^{quad}] = c_j^q$, the coefficient estimated via quadrature on the noiseless function. Since the problem states that quadrature has a non-zero integration error, $c_j^q \\neq c_j$. Thus, the estimator is biased.\n- The reason given, \"the quadrature rule integrates the noise term to zero exactly,\" is also problematic. The expected value of the noise contribution is zero, i.e., $\\mathbb{E}[\\sum_k \\varepsilon_k \\Psi_j(\\boldsymbol{\\xi}^{(k)}) w_k] = 0$. This means the noise does not add to the bias, but it does not remove the existing bias from the quadrature rule itself. The statement is fundamentally flawed.\n**Verdict: Incorrect.**\n\n**C. Replicating $R$ times at each quadrature node reduces the noise-induced variance of each coefficient estimate by a factor $1/R$ but does not alter the quadrature integration error of the noiseless term; therefore, there exists a noise-dominated regime in which replication at fixed nodes is beneficial up to the point where $\\sigma^2/R$ matches the squared quadrature bias, beyond which allocating evaluations to new, independent random samples is preferable.**\n\n- The variance of the coefficient is proportional to the variance of the measurements at the nodes, which is $\\sigma^2/R$. So, replication reduces the variance by $1/R$. Correct.\n- The quadrature integration error depends only on the function $f$ and the quadrature rule $(\\boldsymbol{\\xi}^{(k)}, w_k)$, not on replications. Correct.\n- The total MSE for quadrature is $\\text{MSE}_{quad} = (\\text{Bias}_{quad})^2 + \\text{Var}_{quad}$. The variance term decreases with $R$, while the bias term is fixed. If variance is initially large (noise-dominated regime), increasing $R$ effectively reduces MSE. This benefit diminishes as the variance term becomes small compared to the squared bias. The \"matching\" point is where the two error components are of similar magnitude. This is a classic bias-variance trade-off scenario. Correct.\n- Once the MSE is dominated by the fixed bias, further replications are inefficient. Using the budget to take new, independent samples for a method like least-squares would be better, as this can reduce both sampling bias and variance. The entire reasoning is sound.\n**Verdict: Correct.**\n\n**D. Latin hypercube sampling is necessary to preserve the orthonormality of the Hermite basis under sampling; consequently, it strictly dominates independent Gaussian sampling for least-squares coefficient estimation with solver noise.**\n\n- The premise \"Latin hypercube sampling is necessary to preserve the orthonormality of the Hermite basis\" is false. No sampling method, including LHS, guarantees that $\\frac{1}{N}\\mathbf{\\Psi}^T\\mathbf{\\Psi} = \\mathbf{I}$ for a finite sample size $N$. This property is a feature of specifically constructed quadrature rules (which are not sampling methods).\n- The conclusion \"consequently, it strictly dominates\" is both a non-sequitur (due to the false premise) and an overly strong claim. While LHS often performs better than independent and identically distributed (IID) sampling by providing better space-filling properties, \"strictly dominates\" implies it is never worse, which is difficult to prove and not generally true for all possible functions $f$.\n**Verdict: Incorrect.**\n\n**E. If $P$ is the number of retained coefficients, choosing $N \\approx c\\, P \\log P$ independent Gaussian samples for some moderate constant $c>1$ yields, with high probability, a well-conditioned design matrix and coefficient estimator variance scaling as $O(\\sigma^2/N)$; this makes random least-squares robust to solver noise relative to fixed-order deterministic quadrature as $P$ and the input dimension $d$ grow.**\n\n- The statement \"choosing $N \\approx c\\, P \\log P$ ... yields, with high probability, a well-conditioned design matrix\" is a key result from compressed sensing and high-dimensional approximation theory. It is correct.\n- A well-conditioned design matrix $\\mathbf{\\Psi}$ where $\\mathbf{\\Psi}^T\\mathbf{\\Psi} \\approx N\\mathbf{I}$ leads to a coefficient estimator variance $\\text{Var}(\\hat{\\mathbf{c}}^{ls}) = \\sigma^2(\\mathbf{\\Psi}^T\\mathbf{\\Psi})^{-1}$ where the variance of each component scales as $O(\\sigma^2/N)$. This is also correct.\n- The final part of the statement compares the scalability. Tensor-product quadrature cost, $N_q=n^d$, grows exponentially with dimension $d$ (the curse of dimensionality). The number of coefficients $P=\\binom{d+p}{p}$ grows polynomially in $d$ (for fixed $p$). The required number of random samples $N$ for least-squares scales with $P$ (e.g., $N \\propto P \\log P$), thus scaling polynomially with $d$. This makes random sampling methods immensely more scalable to higher dimensions than tensor-product quadrature. The statement correctly identifies this fundamental advantage.\n**Verdict: Correct.**\n\nFinal summary: Statements A, C, and E are correct.",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "While Polynomial Chaos offers one path to computational efficiency, Multilevel Monte Carlo (MLMC) provides another powerful framework for uncertainty quantification by strategically combining results from a hierarchy of model fidelities. This hands-on practice  walks you through the derivation of the MLMC estimator and its variance. You will then solve the key optimization problem at the heart of the method: how to optimally distribute a computational budget across coarse (cheap) and fine (expensive) simulations to achieve a target accuracy at minimum cost.",
            "id": "4001462",
            "problem": "Consider a steady two-dimensional heat conduction model in a square plate with uncertain thermal conductivity characterized by a random input vector $\\boldsymbol{\\xi}$. Let $Q$ denote a scalar quantity of interest, specifically the spatially averaged normal heat flux along a portion of the boundary. A finite element solver approximates $Q$ on a sequence of nested meshes with characteristic sizes $h_l = 2^{-l} h_0$, where $l \\in \\{0,1,2,3\\}$ indexes the refinement level and $h_0 = 1/8$. Levels are defined by mesh refinement, and at each level $l$ we denote by $Q_l(\\boldsymbol{\\xi})$ the computed approximation. A prior generalized polynomial chaos (gPC) analysis of the discretization error and coupling indicates the following empirically validated scalings for the inter-level correction $Y_l(\\boldsymbol{\\xi}) = Q_l(\\boldsymbol{\\xi}) - Q_{l-1}(\\boldsymbol{\\xi})$: $\\operatorname{Var}(Y_l) = a\\, h_l^{2}$ with $a = 3.2 \\times 10^{-3}$, and a per-sample computational cost $C_l = c\\, h_l^{-2}$ with $c = 50$. The gPC analysis further provides a bias bound $|\\mathbb{E}[Q - Q_L]| \\le \\gamma\\, h_L$ with $\\gamma = 2.0 \\times 10^{-2}$, where $L=3$ is the finest level.\n\nYou will construct a Multilevel Monte Carlo (MLMC) estimator for $\\mathbb{E}[Q]$, where Multilevel Monte Carlo (MLMC) uses a telescoping decomposition and level-wise sampling with matched random inputs to reduce variance. Assume independent and identically distributed samples across levels, with common random numbers used to couple $Q_l$ and $Q_{l-1}$ within each level $l \\ge 1$.\n\nTasks:\n\n1. Starting from linearity of expectation and the telescoping identity for the sequence $\\{Q_l\\}_{l=0}^{L}$, derive an unbiased estimator for $\\mathbb{E}[Q_L]$ written as a sum of level-wise sample means of $Y_l$ with $Y_0 = Q_0$ and $Y_l = Q_l - Q_{l-1}$ for $l \\ge 1$. Then state the estimator for $\\mathbb{E}[Q]$ under the assumption that the bias at level $L$ is controlled by the given bound.\n\n2. Using the variance additivity across independent levels and the given per-level cost model, pose and solve the constrained optimization problem that minimizes total expected computational cost $\\sum_{l=0}^{L} N_l C_l$ subject to a variance budget $\\sum_{l=0}^{L} \\operatorname{Var}(Y_l)/N_l \\le \\eta^{2}$ for some target variance $\\eta^{2}$. Use a Lagrange multiplier argument to obtain the optimal $N_l$ allocation in closed form as a function of $\\operatorname{Var}(Y_l)$ and $C_l$, and specify how to choose $\\eta^{2}$ from a prescribed mean-squared-error (MSE) budget $\\varepsilon^{2}$ when a bias bound is available.\n\n3. For the given data $a = 3.2 \\times 10^{-3}$, $c = 50$, $h_0=1/8$, $L=3$, and a target mean-squared-error $\\varepsilon^{2} = 10^{-6}$, compute the optimal integer sample counts $N_l$ for $l=0,1,2,3$ that minimize cost while ensuring the MSE target is met. Use the bias bound $|\\mathbb{E}[Q - Q_L]| \\le \\gamma h_L$ with $\\gamma = 2.0 \\times 10^{-2}$ and $h_3 = 2^{-3} h_0$, allocate the variance budget as $\\eta^{2} = \\varepsilon^{2} - (\\gamma h_L)^{2}$, and employ the continuous optimal allocation you derive to determine $N_l$ and then choose the smallest integers that satisfy the variance constraint.\n\nProvide the final answer as the row vector of the four optimal sample counts $\\{N_0, N_1, N_2, N_3\\}$ using the LaTeX $\\mathrm{pmatrix}$ environment. No units should be included in the final answer. If any intermediate numerical constants are needed, keep symbolic forms until the final computation. Your final answer must be a calculation.",
            "solution": "The problem is well-posed and scientifically sound, describing a standard application of Multilevel Monte Carlo (MLMC) for uncertainty quantification in computational engineering. All required data and scalings are provided and are consistent with established theory. We will proceed with the solution, addressing the three tasks in order.\n\n### Task 1: Derivation of the MLMC Estimator\n\nThe objective is to estimate the expected value of a quantity of interest, $\\mathbb{E}[Q]$, using a hierarchy of numerical models $\\{Q_l\\}_{l=0}^{L}$. The MLMC method provides an estimator for the expectation of the finest model, $\\mathbb{E}[Q_L]$, which serves as an approximation to $\\mathbb{E}[Q]$.\n\nWe begin with the telescoping identity for the approximation $Q_L$ at the finest level $L$:\n$$Q_L(\\boldsymbol{\\xi}) = Q_0(\\boldsymbol{\\xi}) + \\sum_{l=1}^{L} \\left( Q_l(\\boldsymbol{\\xi}) - Q_{l-1}(\\boldsymbol{\\xi}) \\right)$$\nLet us define the inter-level corrections $Y_l(\\boldsymbol{\\xi})$ as $Y_0(\\boldsymbol{\\xi}) = Q_0(\\boldsymbol{\\xi})$ and $Y_l(\\boldsymbol{\\xi}) = Q_l(\\boldsymbol{\\xi}) - Q_{l-1}(\\boldsymbol{\\xi})$ for $l \\in \\{1, \\dots, L\\}$. The identity can be written compactly as:\n$$Q_L(\\boldsymbol{\\xi}) = \\sum_{l=0}^{L} Y_l(\\boldsymbol{\\xi})$$\nBy the linearity of the expectation operator, the expected value of $Q_L$ is:\n$$\\mathbb{E}[Q_L] = \\mathbb{E}\\left[\\sum_{l=0}^{L} Y_l(\\boldsymbol{\\xi})\\right] = \\sum_{l=0}^{L} \\mathbb{E}[Y_l]$$\nThe MLMC method estimates each term $\\mathbb{E}[Y_l]$ on the right-hand side using an independent Monte Carlo estimator. For each level $l$, we draw $N_l$ independent and identically distributed samples of $Y_l$, denoted by $\\{Y_l^{(i)}\\}_{i=1}^{N_l}$. The sample mean estimator for $\\mathbb{E}[Y_l]$ is:\n$$\\widehat{Y}_l = \\frac{1}{N_l} \\sum_{i=1}^{N_l} Y_l^{(i)}(\\boldsymbol{\\xi}^{(i,l)})$$\nSince each $\\widehat{Y}_l$ is an unbiased estimator of $\\mathbb{E}[Y_l]$, i.e., $\\mathbb{E}[\\widehat{Y}_l] = \\mathbb{E}[Y_l]$, the MLMC estimator for $\\mathbb{E}[Q_L]$, denoted $\\widehat{Q}_{L,\\text{MLMC}}$, is constructed by summing the level-wise estimators:\n$$\\widehat{Q}_{L,\\text{MLMC}} = \\sum_{l=0}^{L} \\widehat{Y}_l = \\sum_{l=0}^{L} \\frac{1}{N_l} \\sum_{i=1}^{N_l} Y_l^{(i)}$$\nThis is an unbiased estimator for $\\mathbb{E}[Q_L]$ because $\\mathbb{E}[\\widehat{Q}_{L,\\text{MLMC}}] = \\sum_{l=0}^{L} \\mathbb{E}[\\widehat{Y}_l] = \\sum_{l=0}^{L} \\mathbb{E}[Y_l] = \\mathbb{E}[Q_L]$.\n\nThis estimator $\\widehat{Q}_{L,\\text{MLMC}}$ is used as the estimator for the true expectation $\\mathbb{E}[Q]$. This introduces a bias, known as the discretization bias, equal to $\\mathbb{E}[Q_L - Q]$. The problem provides a bound on this bias: $|\\mathbb{E}[Q_L - Q]| \\le \\gamma h_L$.\n\n### Task 2: Optimal Sample Allocation\n\nThe total computational cost, $C_{\\text{total}}$, is the sum of the costs at each level:\n$$C_{\\text{total}} = \\sum_{l=0}^{L} N_l C_l$$\nSince the samples are drawn independently across levels, the variance of the MLMC estimator is the sum of the variances of the level-wise sample means:\n$$\\operatorname{Var}(\\widehat{Q}_{L,\\text{MLMC}}) = \\operatorname{Var}\\left(\\sum_{l=0}^{L} \\widehat{Y}_l\\right) = \\sum_{l=0}^{L} \\operatorname{Var}(\\widehat{Y}_l) = \\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l}$$\nWe aim to minimize the cost $C_{\\text{total}}$ subject to the constraint that the estimator variance does not exceed a budget $\\eta^2$:\n$$\\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} \\le \\eta^2$$\nThis is a constrained optimization problem. Treating the sample counts $N_l$ as continuous variables, we use the method of Lagrange multipliers. The Lagrangian is:\n$$\\mathcal{L}(N_0, \\dots, N_L, \\lambda) = \\sum_{l=0}^{L} N_l C_l + \\lambda \\left( \\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} - \\eta^2 \\right)$$\nTo find the minimum, we set the partial derivatives with respect to each $N_l$ to zero:\n$$\\frac{\\partial \\mathcal{L}}{\\partial N_l} = C_l - \\lambda \\frac{\\operatorname{Var}(Y_l)}{N_l^2} = 0$$\nSolving for $N_l$ gives:\n$$N_l^2 = \\lambda \\frac{\\operatorname{Var}(Y_l)}{C_l} \\implies N_l = \\sqrt{\\lambda} \\sqrt{\\frac{\\operatorname{Var}(Y_l)}{C_l}}$$\nTo find the Lagrange multiplier $\\sqrt{\\lambda}$, we substitute this expression for $N_l$ into the variance constraint, assuming it is active (i.e., the equality holds):\n$$\\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{\\sqrt{\\lambda} \\sqrt{\\frac{\\operatorname{Var}(Y_l)}{C_l}}} = \\sum_{l=0}^{L} \\frac{\\sqrt{\\operatorname{Var}(Y_l) C_l}}{\\sqrt{\\lambda}} = \\eta^2$$\nSolving for $\\sqrt{\\lambda}$:\n$$\\sqrt{\\lambda} = \\frac{1}{\\eta^2} \\sum_{k=0}^{L} \\sqrt{\\operatorname{Var}(Y_k) C_k}$$\nSubstituting this back into the expression for $N_l$ yields the optimal sample allocation:\n$$N_l = \\frac{1}{\\eta^2} \\left( \\sum_{k=0}^{L} \\sqrt{\\operatorname{Var}(Y_k) C_k} \\right) \\sqrt{\\frac{\\operatorname{Var}(Y_l)}{C_l}}$$\nThe mean-squared error (MSE) of the estimator for $\\mathbb{E}[Q]$ is the sum of variance and squared bias:\n$$\\text{MSE} = \\mathbb{E}[(\\widehat{Q}_{L,\\text{MLMC}} - \\mathbb{E}[Q])^2] = \\operatorname{Var}(\\widehat{Q}_{L,\\text{MLMC}}) + (\\mathbb{E}[Q_L] - \\mathbb{E}[Q])^2$$\nGiven a total MSE budget $\\varepsilon^2$, we must satisfy $\\text{MSE} \\le \\varepsilon^2$. Using the bias bound, this becomes:\n$$\\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} + (\\gamma h_L)^2 \\le \\varepsilon^2$$\nTo ensure this, we set the variance budget $\\eta^2$ such that the variance part of the error is bounded by:\n$$\\eta^2 = \\varepsilon^2 - (\\gamma h_L)^2$$\nThis requires that the chosen finest level $L$ is such that the bias contribution $(\\gamma h_L)^2$ is smaller than the total MSE budget $\\varepsilon^2$.\n\n### Task 3: Numerical Calculation\n\nWe now apply the derived formulas using the provided data: $L=3$, $a = 3.2 \\times 10^{-3}$, $c = 50$, $h_0 = 1/8$, $\\gamma = 2.0 \\times 10^{-2}$, and $\\varepsilon^2 = 10^{-6}$.\n\nFirst, we calculate the mesh sizes $h_l = h_0 2^{-l}$:\n$h_0 = 1/8 = 0.125$\n$h_1 = 1/16 = 0.0625$\n$h_2 = 1/32 = 0.03125$\n$h_3 = 1/64 = 0.015625$\n\nNext, we compute the variance $\\operatorname{Var}(Y_l) = V_l = a h_l^2$ and cost $C_l = c h_l^{-2}$ for each level:\nFor $l=0$: $V_0 = (3.2 \\times 10^{-3}) (1/8)^2 = 5 \\times 10^{-5}$; $C_0 = 50 (1/8)^{-2} = 3200$.\nFor $l=1$: $V_1 = (3.2 \\times 10^{-3}) (1/16)^2 = 1.25 \\times 10^{-5}$; $C_1 = 50 (1/16)^{-2} = 12800$.\nFor $l=2$: $V_2 = (3.2 \\times 10^{-3}) (1/32)^2 = 3.125 \\times 10^{-6}$; $C_2 = 50 (1/32)^{-2} = 51200$.\nFor $l=3$: $V_3 = (3.2 \\times 10^{-3}) (1/64)^2 \\approx 7.8125 \\times 10^{-7}$; $C_3 = 50 (1/64)^{-2} = 204800$.\n\nNow we calculate the term $\\sqrt{V_l C_l}$. For any level $l$, $\\sqrt{V_l C_l} = \\sqrt{(a h_l^2)(c h_l^{-2})} = \\sqrt{ac}$.\n$\\sqrt{ac} = \\sqrt{(3.2 \\times 10^{-3})(50)} = \\sqrt{0.16} = 0.4$.\nThe sum over all levels is $\\sum_{k=0}^{3} \\sqrt{V_k C_k} = (3+1) \\times 0.4 = 1.6$.\n\nNext, we determine the variance budget $\\eta^2$. The squared bias at level $L=3$ is:\n$(\\gamma h_L)^2 = (\\gamma h_3)^2 = (2.0 \\times 10^{-2} \\times \\frac{1}{64})^2 = (\\frac{1}{3200})^2 \\approx 9.765625 \\times 10^{-8}$.\n$\\eta^2 = \\varepsilon^2 - (\\gamma h_3)^2 = 10^{-6} - 9.765625 \\times 10^{-8} = 9.0234375 \\times 10^{-7}$.\n\nWe now compute the continuous optimal sample counts $N_l$ using the formula from Task 2:\n$N_l = \\frac{1}{\\eta^2} \\left( \\sum_{k=0}^{3} \\sqrt{V_k C_k} \\right) \\sqrt{\\frac{V_l}{C_l}}$\nThe term $\\sqrt{V_l/C_l} = \\sqrt{(a h_l^2)/(c h_l^{-2})} = \\sqrt{a/c} \\, h_l^2$.\n$\\sqrt{a/c} = \\sqrt{(3.2 \\times 10^{-3})/50} = \\sqrt{6.4 \\times 10^{-5}} = \\sqrt{64 \\times 10^{-6}} = 8 \\times 10^{-3}$.\nSo, $N_l = \\frac{1.6}{9.0234375 \\times 10^{-7}} \\times (8 \\times 10^{-3}) h_l^2 \\approx (1.773134 \\times 10^6) \\times (8 \\times 10^{-3}) h_l^2 \\approx 14185.0734 h_l^2$.\n\nLet's compute $N_l$ for each level:\n$N_0 \\approx 14185.0734 \\times (1/8)^2 = 14185.0734 / 64 \\approx 221.64$\n$N_1 \\approx 14185.0734 \\times (1/16)^2 = 14185.0734 / 256 \\approx 55.41$\n$N_2 \\approx 14185.0734 \\times (1/32)^2 = 14185.0734 / 1024 \\approx 13.85$\n$N_3 \\approx 14185.0734 \\times (1/64)^2 = 14185.0734 / 4096 \\approx 3.46$\n\nFinally, we determine the smallest integer sample counts that satisfy the variance constraint. This is achieved by taking the ceiling of each continuous value: $N_l^{\\text{final}} = \\lceil N_l \\rceil$.\n$N_0 = \\lceil 221.64 \\rceil = 222$\n$N_1 = \\lceil 55.41 \\rceil = 56$\n$N_2 = \\lceil 13.85 \\rceil = 14$\n$N_3 = \\lceil 3.46 \\rceil = 4$\n\nThe set of optimal integer sample counts is $\\{222, 56, 14, 4\\}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n222 & 56 & 14 & 4\n\\end{pmatrix}\n}\n$$"
        }
    ]
}