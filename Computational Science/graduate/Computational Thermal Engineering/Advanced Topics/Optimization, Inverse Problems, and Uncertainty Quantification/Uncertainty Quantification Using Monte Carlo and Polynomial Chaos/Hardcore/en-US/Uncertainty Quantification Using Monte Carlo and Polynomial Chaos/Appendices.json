{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of the Polynomial Chaos Expansion (PCE) method is the construction of a basis of functions tailored to the probability distributions of the uncertain inputs. These basis polynomials must be orthogonal with respect to the input measure to ensure desirable properties for coefficient estimation and variance analysis. This exercise guides you through the fundamental process of building a multivariate orthonormal basis for a system with multiple, independent random inputs following different distributions, and then determining the size of the truncated basis, a key factor in the computational cost of PCE .",
            "id": "4001453",
            "problem": "A one-dimensional, transient heat conduction model in a homogeneous slab is solved numerically to predict a spatially averaged temperature response, with parametric uncertainty modeled by two independent random inputs. The first input, denoted by $\\xi_{1}$, represents a scaled uncertainty in the thermal conductivity and is modeled as a uniform random variable on the interval $[-1,1]$. The second input, denoted by $\\xi_{2}$, represents an uncertainty in the volumetric heat generation rate and is modeled as a standard normal random variable with zero mean and unit variance. To perform uncertainty quantification, the response is approximated using generalized Polynomial Chaos (gPC), which requires constructing an orthonormal polynomial basis consistent with the input probability measures.\n\nStarting from the definitions of orthogonal polynomials with respect to given weight functions, and using the independence of $\\xi_{1}$ and $\\xi_{2}$, construct the multivariate orthonormal gPC basis up to total polynomial degree $p$ for the input pair $(\\xi_{1},\\xi_{2})$, where Legendre polynomials are used for $\\xi_{1}$ and Hermite polynomials are used for $\\xi_{2}$. Explicitly specify the normalization of the univariate polynomials so that the resulting multivariate basis is orthonormal under the joint probability measure of $(\\xi_{1},\\xi_{2})$.\n\nThen, derive a closed-form expression for the number of multivariate basis functions retained under the total-degree truncation $i+j \\le p$, where $i$ is the degree in $\\xi_{1}$ and $j$ is the degree in $\\xi_{2}$. Express your final answer as a single analytic expression in terms of $p$. No rounding is required and no units are needed. The final answer should be the expression for the basis size as a function of $p$.",
            "solution": "The problem statement is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- **Model**: One-dimensional, transient heat conduction.\n- **Quantity of Interest**: Spatially averaged temperature response.\n- **Input Uncertainty**: Modeled by two independent random inputs, $\\xi_{1}$ and $\\xi_{2}$.\n- **Input $\\xi_{1}$**: Represents scaled uncertainty in thermal conductivity.\n- **Distribution of $\\xi_{1}$**: Uniform random variable on the interval $[-1, 1]$.\n- **Input $\\xi_{2}$**: Represents uncertainty in the volumetric heat generation rate.\n- **Distribution of $\\xi_{2}$**: Standard normal random variable (zero mean, unit variance).\n- **Uncertainty Quantification Method**: Generalized Polynomial Chaos (gPC).\n- **Task 1**: Construct the multivariate orthonormal gPC basis up to total polynomial degree $p$ for the input pair $(\\xi_{1}, \\xi_{2})$.\n- **Polynomial for $\\xi_{1}$**: Legendre polynomials.\n- **Polynomial for $\\xi_{2}$**: Hermite polynomials.\n- **Requirement**: Explicitly specify the normalization of the univariate polynomials for orthonormality of the multivariate basis under the joint probability measure.\n- **Task 2**: Derive a closed-form expression for the number of multivariate basis functions retained under the total-degree truncation $i+j \\le p$, where $i$ is the degree in $\\xi_{1}$ and $j$ is the degree in $\\xi_{2}$.\n- **Final Answer Format**: A single analytic expression for the basis size as a function of $p$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is well-grounded in computational science and engineering, specifically in the field of uncertainty quantification. The use of generalized Polynomial Chaos (gPC) with Legendre and Hermite polynomials for uniform and normal random variables, respectively, is a standard and correct application of the Wiener-Askey scheme.\n- **Well-Posed**: The problem is well-posed. It requests the construction of a mathematical object (an orthonormal basis) and the derivation of a combinatorial formula, for which sufficient information is provided. The solution is unique and meaningful.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from any subjectivity or bias.\n- **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, realistic, well-posed, and non-trivial.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in two parts as requested: first, the construction of the orthonormal multivariate basis, and second, the derivation of the formula for the number of basis functions under a total-degree truncation.\n\n#### Part 1: Construction of the Orthonormal Basis\n\nThe gPC methodology requires constructing a set of polynomials that are orthonormal with respect to the probability measure of the random inputs. For a set of independent random variables $\\boldsymbol{\\xi} = (\\xi_{1}, \\xi_{2}, \\dots, \\xi_{d})$, the multivariate orthonormal basis is formed by the tensor product of the univariate orthonormal polynomials.\n\n**Univariate Basis for $\\xi_{1}$ (Uniform Distribution)**\n\nThe random variable $\\xi_{1}$ follows a uniform distribution on $[-1, 1]$. Its probability density function (PDF) is:\n$$ f_{\\xi_{1}}(x) = \\begin{cases} \\frac{1}{2}  \\text{if } x \\in [-1, 1] \\\\ 0  \\text{otherwise} \\end{cases} $$\nThe associated weight function for the inner product is $w_{1}(x) = f_{\\xi_{1}}(x) = \\frac{1}{2}$ on the support $[-1, 1]$. The problem specifies using Legendre polynomials, $P_{i}(x)$, for this distribution. The standard orthogonality relation for Legendre polynomials is defined with respect to a weight of $1$:\n$$ \\int_{-1}^{1} P_{i}(x) P_{k}(x) dx = \\frac{2}{2i+1} \\delta_{ik} $$\nwhere $\\delta_{ik}$ is the Kronecker delta.\n\nTo obtain polynomials $\\Phi_{i}^{(1)}(\\xi_{1})$ that are orthonormal with respect to the probability measure of $\\xi_{1}$, we must satisfy the condition:\n$$ \\langle \\Phi_{i}^{(1)}, \\Phi_{k}^{(1)} \\rangle \\equiv \\int_{-1}^{1} \\Phi_{i}^{(1)}(x) \\Phi_{k}^{(1)}(x) f_{\\xi_{1}}(x) dx = \\delta_{ik} $$\nLet $\\Phi_{i}^{(1)}(x) = c_{i} P_{i}(x)$ for some normalization constant $c_{i}$. Substituting this into the orthonormality condition:\n$$ \\int_{-1}^{1} \\left( c_{i} P_{i}(x) \\right) \\left( c_{k} P_{k}(x) \\right) \\frac{1}{2} dx = \\delta_{ik} $$\nFor $i=k$, this becomes:\n$$ c_{i}^{2} \\frac{1}{2} \\int_{-1}^{1} P_{i}(x)^{2} dx = 1 $$\n$$ c_{i}^{2} \\frac{1}{2} \\left( \\frac{2}{2i+1} \\right) = 1 $$\n$$ c_{i}^{2} \\frac{1}{2i+1} = 1 \\implies c_{i} = \\sqrt{2i+1} $$\nThus, the univariate orthonormal polynomials for $\\xi_{1}$ are:\n$$ \\Phi_{i}^{(1)}(\\xi_{1}) = \\sqrt{2i+1} P_{i}(\\xi_{1}) $$\n\n**Univariate Basis for $\\xi_{2}$ (Standard Normal Distribution)**\n\nThe random variable $\\xi_{2}$ follows a standard normal distribution, $\\mathcal{N}(0, 1)$. Its PDF is:\n$$ f_{\\xi_{2}}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right) $$\nThis PDF itself is the weight function for the probabilists' Hermite polynomials, commonly denoted $He_{j}(x)$. The standard orthogonality relation for these polynomials is:\n$$ \\int_{-\\infty}^{\\infty} He_{j}(x) He_{k}(x) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right) dx = j! \\delta_{jk} $$\nThe integral is already defined with respect to the probability measure of $\\xi_{2}$. Let the orthonormal polynomials be $\\Phi_{j}^{(2)}(\\xi_{2}) = d_{j} He_{j}(\\xi_{2})$. The orthonormality condition is:\n$$ \\langle \\Phi_{j}^{(2)}, \\Phi_{k}^{(2)} \\rangle \\equiv \\int_{-\\infty}^{\\infty} \\Phi_{j}^{(2)}(x) \\Phi_{k}^{(2)}(x) f_{\\xi_{2}}(x) dx = \\delta_{jk} $$\nFor $j=k$, this becomes:\n$$ d_{j}^{2} \\int_{-\\infty}^{\\infty} He_{j}(x)^{2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right) dx = 1 $$\n$$ d_{j}^{2} (j!) = 1 \\implies d_{j} = \\frac{1}{\\sqrt{j!}} $$\nThus, the univariate orthonormal polynomials for $\\xi_{2}$ are:\n$$ \\Phi_{j}^{(2)}(\\xi_{2}) = \\frac{1}{\\sqrt{j!}} He_{j}(\\xi_{2}) $$\n\n**Multivariate Orthonormal Basis**\n\nSince $\\xi_{1}$ and $\\xi_{2}$ are independent, the joint PDF is $f_{(\\xi_{1}, \\xi_{2})}(x_{1}, x_{2}) = f_{\\xi_{1}}(x_{1}) f_{\\xi_{2}}(x_{2})$. The multivariate gPC basis functions, denoted $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$ with $\\boldsymbol{\\xi}=(\\xi_{1}, \\xi_{2})$ and multi-index $\\boldsymbol{\\alpha}=(i, j)$, are formed by the tensor product of the univariate orthonormal polynomials:\n$$ \\Psi_{(i,j)}(\\xi_{1}, \\xi_{2}) = \\Phi_{i}^{(1)}(\\xi_{1}) \\Phi_{j}^{(2)}(\\xi_{2}) = \\left(\\sqrt{2i+1} P_{i}(\\xi_{1})\\right) \\left(\\frac{1}{\\sqrt{j!}} He_{j}(\\xi_{2})\\right) $$\nThese basis functions are orthonormal with respect to the joint probability measure:\n$$ \\langle \\Psi_{(i,j)}, \\Psi_{(k,l)} \\rangle = \\int_{-\\infty}^{\\infty} \\int_{-1}^{1} \\Psi_{(i,j)}(x_{1}, x_{2}) \\Psi_{(k,l)}(x_{1}, x_{2}) f_{\\xi_{1}}(x_{1}) f_{\\xi_{2}}(x_{2}) dx_{1} dx_{2} $$\n$$ = \\langle \\Phi_{i}^{(1)}, \\Phi_{k}^{(1)} \\rangle \\langle \\Phi_{j}^{(2)}, \\Phi_{l}^{(2)} \\rangle = \\delta_{ik} \\delta_{jl} = \\delta_{(i,j),(k,l)} $$\n\n#### Part 2: Number of Basis Functions for Total-Degree Truncation\n\nThe problem specifies a total-degree truncation scheme. The set of basis functions is defined by the multi-indices $(i,j)$ such that the total polynomial degree $i+j$ is less than or equal to a prescribed maximum degree $p$.\n$$ \\mathcal{J}_{p} = \\{ (i,j) \\in \\mathbb{N}_{0}^{2} \\, | \\, i+j \\leq p \\} $$\nwhere $\\mathbb{N}_{0} = \\{0, 1, 2, \\dots \\}$. We need to find the cardinality of this set, denoted by $N_{p} = |\\mathcal{J}_{p}|$.\n\nWe can compute this by summing over all possible total degrees $k$ from $0$ to $p$. For a fixed total degree $k$, we need to find the number of non-negative integer pairs $(i,j)$ such that $i+j=k$. The possible values for $j$ are $0, 1, \\dots, k$. For each such $j$, the value of $i$ is uniquely determined as $i=k-j$. This gives $k+1$ possible pairs for a fixed total degree $k$.\n\nThe total number of basis functions is the sum of these counts for $k=0, 1, \\dots, p$:\n$$ N_{p} = \\sum_{k=0}^{p} (\\text{number of pairs with total degree } k) = \\sum_{k=0}^{p} (k+1) $$\nThis is the sum of the first $p+1$ integers:\n$$ N_{p} = 1 + 2 + 3 + \\dots + (p+1) $$\nUsing the formula for the sum of an arithmetic series, $\\sum_{m=1}^{n} m = \\frac{n(n+1)}{2}$, with $n = p+1$:\n$$ N_{p} = \\frac{(p+1)((p+1)+1)}{2} = \\frac{(p+1)(p+2)}{2} $$\nThis expression can also be written using a binomial coefficient. The problem is equivalent to finding the number of non-negative integer solutions to $i+j \\le p$, which is equivalent to finding the number of non-negative integer solutions to $i+j+s=p$ where $s$ is a slack variable. This is a classic stars and bars problem for distributing $p$ items into $3$ bins, the number of solutions for which is:\n$$ N_{p} = \\binom{p+3-1}{3-1} = \\binom{p+2}{2} = \\frac{(p+2)!}{2!(p)!} = \\frac{(p+2)(p+1)}{2} $$\nThe final closed-form expression for the number of multivariate basis functions is a function of $p$.",
            "answer": "$$\\boxed{\\frac{(p+1)(p+2)}{2}}$$"
        },
        {
            "introduction": "While standard Monte Carlo simulation is a robust and general method for propagating uncertainty, its convergence can be slow, requiring a large number of expensive model evaluations. This has motivated the development of variance reduction techniques, which improve estimation efficiency. This practice delves into the theory behind stratified sampling, demonstrating from first principles how partitioning the input space and allocating samples deliberately can systematically reduce estimator variance compared to simple random sampling .",
            "id": "4001468",
            "problem": "A convective cooling scenario in computational thermal engineering is modeled as follows. A steady thermal system with known geometry and fixed heat generation experiences convection to an ambient at temperature $T_{\\infty}$. The convective heat transfer coefficient $h$ is uncertain and modeled as $h \\sim \\mathrm{Uniform}[a,b]$ with $0ab$. Let $T=\\mathcal{T}(h)$ denote the model-predicted scalar response, for example the average wall temperature of interest, where $\\mathcal{T}$ is the mapping from the physics-based thermal model. Assume $\\mathbb{E}[\\,T^2\\,]  \\infty$. The task is to estimate the mean $\\mu=\\mathbb{E}[\\,T\\,]$ using Monte Carlo (MC) with variance reduction by stratified sampling, and to connect this to uncertainty quantification practice that also includes Polynomial Chaos (PC) methods.\n\nConsider a stratified sampling design built on the input distribution of $h$:\n- Partition the interval $[a,b]$ into $L$ disjoint strata $\\{I_{\\ell}\\}_{\\ell=1}^{L}$ of positive lengths that cover $[a,b]$. Define $p_{\\ell}=\\mathbb{P}(h\\in I_{\\ell})=(|I_{\\ell}|)/(b-a)$, where $|I_{\\ell}|$ denotes the length of $I_{\\ell}$.\n- For each stratum $I_{\\ell}$, draw $n_{\\ell}\\ge 1$ independent samples $h_{\\ell,1},\\dots,h_{\\ell,n_{\\ell}}$ from the conditional distribution of $h$ given $h\\in I_{\\ell}$ (that is, uniform on $I_{\\ell}$), evaluate $T_{\\ell,i}=\\mathcal{T}(h_{\\ell,i})$, and form the within-stratum sample mean $\\overline{T}_{\\ell}=(1/n_{\\ell})\\sum_{i=1}^{n_{\\ell}} T_{\\ell,i}$.\n- Define the stratified estimator $\\widehat{\\mu}_{\\mathrm{strat}}=\\sum_{\\ell=1}^{L} p_{\\ell}\\,\\overline{T}_{\\ell}$.\n\nLet $\\mu_{\\ell}=\\mathbb{E}[\\,T \\mid h\\in I_{\\ell}\\,]$ and $\\sigma_{\\ell}^{2}=\\mathrm{Var}(T \\mid h\\in I_{\\ell})$. Assume all samples are independent across and within strata. Using only foundational principles such as the law of total expectation, the law of total variance, and the variance properties of independent sample means, perform the following:\n\n- Explain, without resorting to any shortcut formulas, why stratifying on $h$ targets variability that is extrinsic to $h$’s location on $[a,b]$ and thereby reduces the variance of the estimator of $\\mu$ when compared at equal total budget $n=\\sum_{\\ell=1}^{L} n_{\\ell}$ to simple Monte Carlo that draws $n$ independent samples from $\\mathrm{Uniform}[a,b]$.\n- Derive, from first principles, a closed-form expression for $\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat}}\\right]$ in terms of $p_{\\ell}$, $n_{\\ell}$, and the within-stratum variances $\\sigma_{\\ell}^{2}$, valid for general $\\{p_{\\ell}\\}$ and $\\{n_{\\ell}\\}$.\n- Specialize to proportional allocation $n_{\\ell}=n\\,p_{\\ell}$ and simplify your expression. Then compare to the variance of the simple Monte Carlo estimator $\\widehat{\\mu}_{\\mathrm{mc}}=(1/n)\\sum_{i=1}^{n} T^{(i)}$, where $T^{(i)}$ are independent copies of $T$, and identify the nonnegative term that stratification removes.\n\nYour final answer must be the general closed-form expression for $\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat}}\\right]$ in terms of $p_{\\ell}$, $n_{\\ell}$, and $\\sigma_{\\ell}^{2}$. No numerical evaluation is required. Express the final answer as a single analytic expression with no units.",
            "solution": "The problem is well-defined, scientifically sound, and contains all necessary information for a complete derivation. It presents a standard, non-trivial exercise in the statistical theory of variance reduction methods as applied to computational engineering.\n\nThe core task is to analyze the variance of a stratified sampling estimator for the mean of a model response, $\\mu=\\mathbb{E}[\\,T\\,]$, where the response $T$ is a function of an uncertain input parameter $h$. The analysis will be performed from first principles.\n\nFirst, let us address the conceptual question of why stratification reduces variance. The total variance of the response $T$, denoted $\\mathrm{Var}(T)$, can be decomposed using the law of total variance with respect to the partitioning of the input domain $[a,b]$ into strata $\\{I_{\\ell}\\}_{\\ell=1}^{L}$. Let $S$ be a discrete random variable indicating the stratum index, such that $\\mathbb{P}(S=\\ell) = p_{\\ell}$. The law of total variance states:\n$$\n\\mathrm{Var}(T) = \\mathbb{E}[\\mathrm{Var}(T \\mid S)] + \\mathrm{Var}(\\mathbb{E}[T \\mid S])\n$$\nThe first term, $\\mathbb{E}[\\mathrm{Var}(T \\mid S)] = \\sum_{\\ell=1}^{L} p_{\\ell} \\sigma_{\\ell}^{2}$, is the expected within-stratum variance. It represents the average variability of $T$ that remains *after* we know which stratum $I_{\\ell}$ the input $h$ belongs to. This can be viewed as the variability \"intrinsic\" to the sub-regions of the input domain.\nThe second term, $\\mathrm{Var}(\\mathbb{E}[T \\mid S]) = \\sum_{\\ell=1}^{L} p_{\\ell} (\\mu_{\\ell} - \\mu)^2$, is the between-strata variance. It quantifies the variability that arises from the differences in the mean response $\\mu_{\\ell}$ from one stratum to another. This is the variability component that is \"extrinsic to $h$'s location\" within a stratum; it is purely due to which stratum $h$ falls into.\n\nA simple Monte Carlo (SMC) estimator, $\\widehat{\\mu}_{\\mathrm{mc}}$, is based on $n$ independent and identically distributed samples of $T$. Its variance is $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{mc}}) = \\frac{1}{n}\\mathrm{Var}(T)$. This variance is influenced by both the within-stratum and the between-strata components of variance. In SMC, the number of samples falling into any given stratum $I_{\\ell}$ is a random variable, which introduces sampling error related to the between-strata variance.\n\nIn contrast, the stratified estimator $\\widehat{\\mu}_{\\mathrm{strat}}$ is constructed by pre-allocating a fixed number of samples $n_{\\ell}$ to each stratum $I_{\\ell}$ and then combining the results with deterministic weights $p_{\\ell}$. By enforcing the number of samples taken from each stratum, stratified sampling eliminates the random fluctuations in how the strata are represented in the total sample. This effectively removes the between-strata component of variance from the final estimator's variance, thereby reducing the overall uncertainty in the estimate of $\\mu$. This is a common principle in uncertainty quantification (UQ). While stratified sampling achieves this by partitioning the input space, other UQ methods like Polynomial Chaos (PC) achieve efficiency by decomposing the output function itself into a basis that is orthogonal with respect to the input's probability measure, providing another powerful way to analyze and manage variance.\n\nNext, we derive the closed-form expression for the variance of the stratified estimator, $\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat}}\\right]$. The estimator is defined as:\n$$\n\\widehat{\\mu}_{\\mathrm{strat}}=\\sum_{\\ell=1}^{L} p_{\\ell}\\,\\overline{T}_{\\ell}\n$$\nwhere $\\overline{T}_{\\ell}$ is the sample mean for stratum $\\ell$. The samples are drawn independently across different strata. Therefore, the random variables $\\overline{T}_{1}, \\overline{T}_{2}, \\dots, \\overline{T}_{L}$ are mutually independent. The variance of a sum of independent random variables is the sum of their variances. Using this property, and the fact that $p_{\\ell}$ are constants:\n$$\n\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat}}\\right] = \\mathrm{Var}\\left(\\sum_{\\ell=1}^{L} p_{\\ell}\\,\\overline{T}_{\\ell}\\right) = \\sum_{\\ell=1}^{L} \\mathrm{Var}\\left(p_{\\ell}\\,\\overline{T}_{\\ell}\\right) = \\sum_{\\ell=1}^{L} p_{\\ell}^{2}\\,\\mathrm{Var}\\left(\\overline{T}_{\\ell}\\right)\n$$\nNow we must find the variance of the within-stratum sample mean, $\\mathrm{Var}\\!\\left(\\overline{T}_{\\ell}\\right)$. The variable $\\overline{T}_{\\ell}$ is defined as the average of $n_{\\ell}$ samples, $\\overline{T}_{\\ell} = \\frac{1}{n_{\\ell}}\\sum_{i=1}^{n_{\\ell}} T_{\\ell,i}$. These $n_{\\ell}$ samples $T_{\\ell,i}$ are independent and identically distributed draws from the conditional distribution of $T$ given $h \\in I_{\\ell}$. The variance of this conditional distribution is given as $\\sigma_{\\ell}^{2} = \\mathrm{Var}(T \\mid h\\in I_{\\ell})$.\nFor a sum of independent random variables, the variance is the sum of the variances. Therefore:\n$$\n\\mathrm{Var}\\left(\\overline{T}_{\\ell}\\right) = \\mathrm{Var}\\left(\\frac{1}{n_{\\ell}}\\sum_{i=1}^{n_{\\ell}} T_{\\ell,i}\\right) = \\frac{1}{n_{\\ell}^{2}} \\mathrm{Var}\\left(\\sum_{i=1}^{n_{\\ell}} T_{\\ell,i}\\right) = \\frac{1}{n_{\\ell}^{2}} \\sum_{i=1}^{n_{\\ell}} \\mathrm{Var}\\left(T_{\\ell,i}\\right)\n$$\nSince each $T_{\\ell,i}$ has variance $\\sigma_{\\ell}^{2}$, this sum becomes:\n$$\n\\mathrm{Var}\\left(\\overline{T}_{\\ell}\\right) = \\frac{1}{n_{\\ell}^{2}} \\left(n_{\\ell} \\sigma_{\\ell}^{2}\\right) = \\frac{\\sigma_{\\ell}^{2}}{n_{\\ell}}\n$$\nSubstituting this result back into the expression for the variance of the stratified estimator yields the general formula:\n$$\n\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat}}\\right] = \\sum_{\\ell=1}^{L} p_{\\ell}^{2} \\frac{\\sigma_{\\ell}^{2}}{n_{\\ell}}\n$$\nThis expression is valid for any allocation $\\{n_{\\ell}\\}$ where $\\sum n_{\\ell} = n$.\n\nFinally, we specialize this result to proportional allocation, where $n_{\\ell} = n p_{\\ell}$ for each stratum $\\ell$. Substituting this into the general variance formula:\n$$\n\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat, prop}}\\right] = \\sum_{\\ell=1}^{L} p_{\\ell}^{2} \\frac{\\sigma_{\\ell}^{2}}{n p_{\\ell}} = \\sum_{\\ell=1}^{L} \\frac{p_{\\ell}\\sigma_{\\ell}^{2}}{n} = \\frac{1}{n} \\sum_{\\ell=1}^{L} p_{\\ell}\\sigma_{\\ell}^{2}\n$$\nAs established earlier from the law of total variance, the term $\\sum_{\\ell=1}^{L} p_{\\ell}\\sigma_{\\ell}^{2}$ is precisely the expected within-stratum variance, $\\mathbb{E}[\\mathrm{Var}(T \\mid S)]$. So, for proportional allocation:\n$$\n\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat, prop}}\\right] = \\frac{1}{n} \\mathbb{E}[\\mathrm{Var}(T \\mid S)]\n$$\nThe variance of the simple Monte Carlo estimator $\\widehat{\\mu}_{\\mathrm{mc}}$ with the same total number of samples $n$ is:\n$$\n\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{mc}}\\right] = \\frac{1}{n}\\mathrm{Var}(T) = \\frac{1}{n} \\left(\\mathbb{E}[\\mathrm{Var}(T \\mid S)] + \\mathrm{Var}(\\mathbb{E}[T \\mid S])\\right)\n$$\nComparing the two, we see that:\n$$\n\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{mc}}\\right] - \\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat, prop}}\\right] = \\frac{1}{n} \\mathrm{Var}(\\mathbb{E}[T \\mid S])\n$$\nThe nonnegative term that stratification removes (under proportional allocation) is $\\frac{1}{n} \\mathrm{Var}(\\mathbb{E}[T \\mid S]) = \\frac{1}{n} \\sum_{\\ell=1}^{L} p_{\\ell}(\\mu_{\\ell}-\\mu)^{2}$. This term represents the contribution to variance from the differences between the mean responses of the strata.\nThe final answer required is the general closed-form expression for $\\mathrm{Var}\\!\\left[\\widehat{\\mu}_{\\mathrm{strat}}\\right]$.",
            "answer": "$$\\boxed{\\sum_{\\ell=1}^{L} p_{\\ell}^{2} \\frac{\\sigma_{\\ell}^{2}}{n_{\\ell}}}$$"
        },
        {
            "introduction": "Multilevel Monte Carlo (MLMC) represents a powerful advancement in uncertainty quantification that optimally balances the trade-off between model accuracy and computational cost. The method cleverly combines results from a hierarchy of model fidelities—from coarse and cheap to fine and expensive—to produce an accurate estimate at a fraction of the cost of traditional methods. This problem provides a comprehensive walkthrough of deriving the MLMC estimator, optimizing the allocation of computational effort across levels, and applying the method to a practical heat transfer problem to achieve a target accuracy .",
            "id": "4001462",
            "problem": "Consider a steady two-dimensional heat conduction model in a square plate with uncertain thermal conductivity characterized by a random input vector $\\boldsymbol{\\xi}$. Let $Q$ denote a scalar quantity of interest, specifically the spatially averaged normal heat flux along a portion of the boundary. A finite element solver approximates $Q$ on a sequence of nested meshes with characteristic sizes $h_l = 2^{-l} h_0$, where $l \\in \\{0,1,2,3\\}$ indexes the refinement level and $h_0 = 1/8$. Levels are defined by mesh refinement, and at each level $l$ we denote by $Q_l(\\boldsymbol{\\xi})$ the computed approximation. A prior generalized polynomial chaos (gPC) analysis of the discretization error and coupling indicates the following empirically validated scalings for the inter-level correction $Y_l(\\boldsymbol{\\xi}) = Q_l(\\boldsymbol{\\xi}) - Q_{l-1}(\\boldsymbol{\\xi})$: $\\operatorname{Var}(Y_l) = a\\, h_l^{2}$ with $a = 3.2 \\times 10^{-3}$, and a per-sample computational cost $C_l = c\\, h_l^{-2}$ with $c = 50$. The gPC analysis further provides a bias bound $|\\mathbb{E}[Q - Q_L]| \\le \\gamma h_L$ with $\\gamma = 2.0 \\times 10^{-2}$, where $L=3$ is the finest level.\n\nYou will construct a Multilevel Monte Carlo (MLMC) estimator for $\\mathbb{E}[Q]$, where Multilevel Monte Carlo (MLMC) uses a telescoping decomposition and level-wise sampling with matched random inputs to reduce variance. Assume independent and identically distributed samples across levels, with common random numbers used to couple $Q_l$ and $Q_{l-1}$ within each level $l \\ge 1$.\n\nTasks:\n\n1. Starting from linearity of expectation and the telescoping identity for the sequence $\\{Q_l\\}_{l=0}^{L}$, derive an unbiased estimator for $\\mathbb{E}[Q_L]$ written as a sum of level-wise sample means of $Y_l$ with $Y_0 = Q_0$ and $Y_l = Q_l - Q_{l-1}$ for $l \\ge 1$. Then state the estimator for $\\mathbb{E}[Q]$ under the assumption that the bias at level $L$ is controlled by the given bound.\n\n2. Using the variance additivity across independent levels and the given per-level cost model, pose and solve the constrained optimization problem that minimizes total expected computational cost $\\sum_{l=0}^{L} N_l C_l$ subject to a variance budget $\\sum_{l=0}^{L} \\operatorname{Var}(Y_l)/N_l \\le \\eta^{2}$ for some target variance $\\eta^{2}$. Use a Lagrange multiplier argument to obtain the optimal $N_l$ allocation in closed form as a function of $\\operatorname{Var}(Y_l)$ and $C_l$, and specify how to choose $\\eta^{2}$ from a prescribed mean-squared-error (MSE) budget $\\varepsilon^{2}$ when a bias bound is available.\n\n3. For the given data $a = 3.2 \\times 10^{-3}$, $c = 50$, $h_0=1/8$, $L=3$, and a target mean-squared-error $\\varepsilon^{2} = 10^{-6}$, compute the optimal integer sample counts $N_l$ for $l=0,1,2,3$ that minimize cost while ensuring the MSE target is met. Use the bias bound $|\\mathbb{E}[Q - Q_L]| \\le \\gamma h_L$ with $\\gamma = 2.0 \\times 10^{-2}$ and $h_3 = 2^{-3} h_0$, allocate the variance budget as $\\eta^{2} = \\varepsilon^{2} - (\\gamma h_L)^{2}$, and employ the continuous optimal allocation you derive to determine $N_l$ and then choose the smallest integers that satisfy the variance constraint.\n\nProvide the final answer as the row vector of the four optimal sample counts $\\{N_0, N_1, N_2, N_3\\}$ using the LaTeX $\\mathrm{pmatrix}$ environment. No units should be included in the final answer. If any intermediate numerical constants are needed, keep symbolic forms until the final computation. Your final answer must be a calculation.",
            "solution": "The problem is well-posed and scientifically sound, describing a standard application of Multilevel Monte Carlo (MLMC) for uncertainty quantification in computational engineering. All required data and scalings are provided and are consistent with established theory. We will proceed with the solution, addressing the three tasks in order.\n\n### Task 1: Derivation of the MLMC Estimator\n\nThe objective is to estimate the expected value of a quantity of interest, $\\mathbb{E}[Q]$, using a hierarchy of numerical models $\\{Q_l\\}_{l=0}^{L}$. The MLMC method provides an estimator for the expectation of the finest model, $\\mathbb{E}[Q_L]$, which serves as an approximation to $\\mathbb{E}[Q]$.\n\nWe begin with the telescoping identity for the approximation $Q_L$ at the finest level $L$:\n$$Q_L(\\boldsymbol{\\xi}) = Q_0(\\boldsymbol{\\xi}) + \\sum_{l=1}^{L} \\left( Q_l(\\boldsymbol{\\xi}) - Q_{l-1}(\\boldsymbol{\\xi}) \\right)$$\nLet us define the inter-level corrections $Y_l(\\boldsymbol{\\xi})$ as $Y_0(\\boldsymbol{\\xi}) = Q_0(\\boldsymbol{\\xi})$ and $Y_l(\\boldsymbol{\\xi}) = Q_l(\\boldsymbol{\\xi}) - Q_{l-1}(\\boldsymbol{\\xi})$ for $l \\in \\{1, \\dots, L\\}$. The identity can be written compactly as:\n$$Q_L(\\boldsymbol{\\xi}) = \\sum_{l=0}^{L} Y_l(\\boldsymbol{\\xi})$$\nBy the linearity of the expectation operator, the expected value of $Q_L$ is:\n$$\\mathbb{E}[Q_L] = \\mathbb{E}\\left[\\sum_{l=0}^{L} Y_l(\\boldsymbol{\\xi})\\right] = \\sum_{l=0}^{L} \\mathbb{E}[Y_l]$$\nThe MLMC method estimates each term $\\mathbb{E}[Y_l]$ on the right-hand side using an independent Monte Carlo estimator. For each level $l$, we draw $N_l$ independent and identically distributed samples of $Y_l$, denoted by $\\{Y_l^{(i)}\\}_{i=1}^{N_l}$. The sample mean estimator for $\\mathbb{E}[Y_l]$ is:\n$$\\widehat{Y}_l = \\frac{1}{N_l} \\sum_{i=1}^{N_l} Y_l^{(i)}(\\boldsymbol{\\xi}^{(i,l)})$$\nSince each $\\widehat{Y}_l$ is an unbiased estimator of $\\mathbb{E}[Y_l]$, i.e., $\\mathbb{E}[\\widehat{Y}_l] = \\mathbb{E}[Y_l]$, the MLMC estimator for $\\mathbb{E}[Q_L]$, denoted $\\widehat{Q}_{L,\\text{MLMC}}$, is constructed by summing the level-wise estimators:\n$$\\widehat{Q}_{L,\\text{MLMC}} = \\sum_{l=0}^{L} \\widehat{Y}_l = \\sum_{l=0}^{L} \\frac{1}{N_l} \\sum_{i=1}^{N_l} Y_l^{(i)}$$\nThis is an unbiased estimator for $\\mathbb{E}[Q_L]$ because $\\mathbb{E}[\\widehat{Q}_{L,\\text{MLMC}}] = \\sum_{l=0}^{L} \\mathbb{E}[\\widehat{Y}_l] = \\sum_{l=0}^{L} \\mathbb{E}[Y_l] = \\mathbb{E}[Q_L]$.\n\nThis estimator $\\widehat{Q}_{L,\\text{MLMC}}$ is used as the estimator for the true expectation $\\mathbb{E}[Q]$. This introduces a bias, known as the discretization bias, equal to $\\mathbb{E}[Q_L - Q]$. The problem provides a bound on this bias: $|\\mathbb{E}[Q_L - Q]| \\le \\gamma h_L$.\n\n### Task 2: Optimal Sample Allocation\n\nThe total computational cost, $C_{\\text{total}}$, is the sum of the costs at each level:\n$$C_{\\text{total}} = \\sum_{l=0}^{L} N_l C_l$$\nSince the samples are drawn independently across levels, the variance of the MLMC estimator is the sum of the variances of the level-wise sample means:\n$$\\operatorname{Var}(\\widehat{Q}_{L,\\text{MLMC}}) = \\operatorname{Var}\\left(\\sum_{l=0}^{L} \\widehat{Y}_l\\right) = \\sum_{l=0}^{L} \\operatorname{Var}(\\widehat{Y}_l) = \\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l}$$\nWe aim to minimize the cost $C_{\\text{total}}$ subject to the constraint that the estimator variance does not exceed a budget $\\eta^2$:\n$$\\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} \\le \\eta^2$$\nThis is a constrained optimization problem. Treating the sample counts $N_l$ as continuous variables, we use the method of Lagrange multipliers. The Lagrangian is:\n$$\\mathcal{L}(N_0, \\dots, N_L, \\lambda) = \\sum_{l=0}^{L} N_l C_l + \\lambda \\left( \\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} - \\eta^2 \\right)$$\nTo find the minimum, we set the partial derivatives with respect to each $N_l$ to zero:\n$$\\frac{\\partial \\mathcal{L}}{\\partial N_l} = C_l - \\lambda \\frac{\\operatorname{Var}(Y_l)}{N_l^2} = 0$$\nSolving for $N_l$ gives:\n$$N_l^2 = \\lambda \\frac{\\operatorname{Var}(Y_l)}{C_l} \\implies N_l = \\sqrt{\\lambda} \\sqrt{\\frac{\\operatorname{Var}(Y_l)}{C_l}}$$\nTo find the Lagrange multiplier $\\sqrt{\\lambda}$, we substitute this expression for $N_l$ into the variance constraint, assuming it is active (i.e., the equality holds):\n$$\\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{\\sqrt{\\lambda} \\sqrt{\\frac{\\operatorname{Var}(Y_l)}{C_l}}} = \\sum_{l=0}^{L} \\frac{\\sqrt{\\operatorname{Var}(Y_l) C_l}}{\\sqrt{\\lambda}} = \\eta^2$$\nSolving for $\\sqrt{\\lambda}$:\n$$\\sqrt{\\lambda} = \\frac{1}{\\eta^2} \\sum_{k=0}^{L} \\sqrt{\\operatorname{Var}(Y_k) C_k}$$\nSubstituting this back into the expression for $N_l$ yields the optimal sample allocation:\n$$N_l = \\frac{1}{\\eta^2} \\left( \\sum_{k=0}^{L} \\sqrt{\\operatorname{Var}(Y_k) C_k} \\right) \\sqrt{\\frac{\\operatorname{Var}(Y_l)}{C_l}}$$\nThe mean-squared error (MSE) of the estimator for $\\mathbb{E}[Q]$ is the sum of variance and squared bias:\n$$\\text{MSE} = \\mathbb{E}[(\\widehat{Q}_{L,\\text{MLMC}} - \\mathbb{E}[Q])^2] = \\operatorname{Var}(\\widehat{Q}_{L,\\text{MLMC}}) + (\\mathbb{E}[Q_L] - \\mathbb{E}[Q])^2$$\nGiven a total MSE budget $\\varepsilon^2$, we must satisfy $\\text{MSE} \\le \\varepsilon^2$. Using the bias bound, this becomes:\n$$\\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} + (\\gamma h_L)^2 \\le \\varepsilon^2$$\nTo ensure this, we set the variance budget $\\eta^2$ such that the variance part of the error is bounded by:\n$$\\eta^2 = \\varepsilon^2 - (\\gamma h_L)^2$$\nThis requires that the chosen finest level $L$ is such that the bias contribution $(\\gamma h_L)^2$ is smaller than the total MSE budget $\\varepsilon^2$.\n\n### Task 3: Numerical Calculation\n\nWe now apply the derived formulas using the provided data: $L=3$, $a = 3.2 \\times 10^{-3}$, $c = 50$, $h_0 = 1/8$, $\\gamma = 2.0 \\times 10^{-2}$, and $\\varepsilon^2 = 10^{-6}$.\n\nFirst, we calculate the mesh sizes $h_l = h_0 2^{-l}$:\n$h_0 = 1/8 = 0.125$\n$h_1 = 1/16 = 0.0625$\n$h_2 = 1/32 = 0.03125$\n$h_3 = 1/64 = 0.015625$\n\nNext, we compute the variance $\\operatorname{Var}(Y_l) = V_l = a h_l^2$ and cost $C_l = c h_l^{-2}$ for each level:\nFor $l=0$: $V_0 = (3.2 \\times 10^{-3}) (1/8)^2 = 5 \\times 10^{-5}$; $C_0 = 50 (1/8)^{-2} = 3200$.\nFor $l=1$: $V_1 = (3.2 \\times 10^{-3}) (1/16)^2 = 1.25 \\times 10^{-5}$; $C_1 = 50 (1/16)^{-2} = 12800$.\nFor $l=2$: $V_2 = (3.2 \\times 10^{-3}) (1/32)^2 = 3.125 \\times 10^{-6}$; $C_2 = 50 (1/32)^{-2} = 51200$.\nFor $l=3$: $V_3 = (3.2 \\times 10^{-3}) (1/64)^2 \\approx 7.8125 \\times 10^{-7}$; $C_3 = 50 (1/64)^{-2} = 204800$.\n\nNow we calculate the term $\\sqrt{V_l C_l}$. For any level $l$, $\\sqrt{V_l C_l} = \\sqrt{(a h_l^2)(c h_l^{-2})} = \\sqrt{ac}$.\n$\\sqrt{ac} = \\sqrt{(3.2 \\times 10^{-3})(50)} = \\sqrt{0.16} = 0.4$.\nThe sum over all levels is $\\sum_{k=0}^{3} \\sqrt{V_k C_k} = (3+1) \\times 0.4 = 1.6$.\n\nNext, we determine the variance budget $\\eta^2$. The squared bias at level $L=3$ is:\n$(\\gamma h_L)^2 = (\\gamma h_3)^2 = (2.0 \\times 10^{-2} \\times \\frac{1}{64})^2 = (\\frac{1}{3200})^2 \\approx 9.765625 \\times 10^{-8}$.\n$\\eta^2 = \\varepsilon^2 - (\\gamma h_3)^2 = 10^{-6} - 9.765625 \\times 10^{-8} = 9.0234375 \\times 10^{-7}$.\n\nWe now compute the continuous optimal sample counts $N_l$ using the formula from Task 2:\n$N_l = \\frac{1}{\\eta^2} \\left( \\sum_{k=0}^{3} \\sqrt{V_k C_k} \\right) \\sqrt{\\frac{V_l}{C_l}}$\nThe term $\\sqrt{V_l/C_l} = \\sqrt{(a h_l^2)/(c h_l^{-2})} = \\sqrt{a/c} \\, h_l^2$.\n$\\sqrt{a/c} = \\sqrt{(3.2 \\times 10^{-3})/50} = \\sqrt{6.4 \\times 10^{-5}} = \\sqrt{64 \\times 10^{-6}} = 8 \\times 10^{-3}$.\nSo, $N_l = \\frac{1.6}{9.0234375 \\times 10^{-7}} \\times (8 \\times 10^{-3}) h_l^2 \\approx (1.773134 \\times 10^6) \\times (8 \\times 10^{-3}) h_l^2 \\approx 14185.0734 h_l^2$.\n\nLet's compute $N_l$ for each level:\n$N_0 \\approx 14185.0734 \\times (1/8)^2 = 14185.0734 / 64 \\approx 221.64$\n$N_1 \\approx 14185.0734 \\times (1/16)^2 = 14185.0734 / 256 \\approx 55.41$\n$N_2 \\approx 14185.0734 \\times (1/32)^2 = 14185.0734 / 1024 \\approx 13.85$\n$N_3 \\approx 14185.0734 \\times (1/64)^2 = 14185.0734 / 4096 \\approx 3.46$\n\nFinally, we determine the smallest integer sample counts that satisfy the variance constraint. This is achieved by taking the ceiling of each continuous value: $N_l^{\\text{final}} = \\lceil N_l \\rceil$.\n$N_0 = \\lceil 221.64 \\rceil = 222$\n$N_1 = \\lceil 55.41 \\rceil = 56$\n$N_2 = \\lceil 13.85 \\rceil = 14$\n$N_3 = \\lceil 3.46 \\rceil = 4$\n\nThe set of optimal integer sample counts is $\\{222, 56, 14, 4\\}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n222  56  14  4\n\\end{pmatrix}\n}\n$$"
        }
    ]
}