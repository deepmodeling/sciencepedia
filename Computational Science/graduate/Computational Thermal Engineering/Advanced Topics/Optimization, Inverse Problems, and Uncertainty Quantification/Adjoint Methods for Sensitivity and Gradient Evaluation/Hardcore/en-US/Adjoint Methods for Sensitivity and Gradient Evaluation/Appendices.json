{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for employing adjoint methods is their remarkable computational efficiency, especially in scenarios with many design parameters. This exercise provides a quantitative comparison of the computational cost, measured in the number of large linear solves, for calculating a full gradient using finite differences, the direct sensitivity method, and the adjoint method. By analyzing these costs, you will gain a concrete understanding of why the adjoint approach is indispensable for large-scale design optimization. ",
            "id": "3935153",
            "problem": "A steady-state nonlinear heat conduction problem in a bounded domain is discretized to yield the algebraic residual equation $\\mathbf{R}(\\mathbf{u}, \\mathbf{p}) = \\mathbf{0}$, where $\\mathbf{u} \\in \\mathbb{R}^{N_u}$ is the vector of nodal temperatures and $\\mathbf{p} \\in \\mathbb{R}^{N_p}$ is a vector of design parameters that linearly parameterize the spatially varying thermal conductivity field. The quantity of interest is a single scalar functional $J(\\mathbf{u})$, such as the total normal heat flux through a portion of the boundary. Assume the following computational model:\n- The nominal parameter $\\mathbf{p}_0$ and its converged state $\\mathbf{u}_0$ have been obtained, and the corresponding Jacobian (state tangent) $\\mathbf{J}_0 \\equiv \\partial \\mathbf{R}/\\partial \\mathbf{u} \\big|_{(\\mathbf{u}_0,\\mathbf{p}_0)}$ is assembled and reusable.\n- Re-solving the nonlinear state for a different parameter vector $\\mathbf{p}$ by Newton’s method requires $S$ iterations, and each Newton iteration requires one large sparse linear solve with coefficient matrix $\\mathbf{J}(\\mathbf{u},\\mathbf{p})$.\n- Forming right-hand sides, function evaluations, and vector operations have negligible cost relative to the large sparse linear solves.\n- The goal is to evaluate the full gradient $\\nabla_{\\mathbf{p}} J(\\mathbf{u}(\\mathbf{p}))$ at $\\mathbf{p}_0$.\n\nThree approaches are considered:\n1. Central finite differences applied to each component $p_i$ using two perturbed state re-solves per parameter.\n2. Direct sensitivities obtained by linearizing the residual about $(\\mathbf{u}_0,\\mathbf{p}_0)$ and solving one linear sensitivity system per parameter.\n3. The adjoint method obtained by solving a single adjoint system associated with the scalar objective $J(\\mathbf{u})$.\n\nUnder these assumptions and cost model, derive the total number of large sparse linear solves required to compute the complete gradient with respect to all $N_p$ parameters for each approach. Express your final result as a row matrix $\\bigl[N_{\\mathrm{FD}} \\;\\; N_{\\mathrm{dir}} \\;\\; N_{\\mathrm{adj}}\\bigr]$ in terms of $N_p$ and $S$. No numerical substitution is required, and no units are needed. Provide the final expression without rounding.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- Discretized nonlinear residual equation: $\\mathbf{R}(\\mathbf{u}, \\mathbf{p}) = \\mathbf{0}$.\n- State vector of nodal temperatures: $\\mathbf{u} \\in \\mathbb{R}^{N_u}$.\n- Vector of design parameters: $\\mathbf{p} \\in \\mathbb{R}^{N_p}$.\n- Scalar quantity of interest: $J(\\mathbf{u})$.\n- Nominal point: $(\\mathbf{u}_0, \\mathbf{p}_0)$ where $\\mathbf{R}(\\mathbf{u}_0, \\mathbf{p}_0) = \\mathbf{0}$.\n- State tangent (Jacobian) at the nominal point: $\\mathbf{J}_0 \\equiv \\partial \\mathbf{R}/\\partial \\mathbf{u} \\big|_{(\\mathbf{u}_0,\\mathbf{p}_0)}$, which is assembled and reusable.\n- Cost of a nonlinear state re-solve: $S$ iterations, where each iteration requires one large sparse linear solve. The coefficient matrix for these solves is the state-dependent Jacobian $\\mathbf{J}(\\mathbf{u},\\mathbf{p})$.\n- Cost model simplification: The cost of forming right-hand sides, function evaluations, and vector operations is negligible compared to the cost of a large sparse linear solve.\n- Goal: Compute the full gradient $\\nabla_{\\mathbf{p}} J(\\mathbf{u}(\\mathbf{p}))$ at $\\mathbf{p}_0$.\n- Approach 1 (Finite Differences): \"two perturbed state re-solves per parameter\".\n- Approach 2 (Direct Sensitivities): \"solving one linear sensitivity system per parameter\".\n- Approach 3 (Adjoint Method): \"solving a single adjoint system\".\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard, well-defined exercise in computational science and engineering, specifically in the field of sensitivity analysis for systems governed by partial differential equations.\n- **Scientifically Grounded:** The problem statement is based on established principles of numerical analysis (Newton's method), finite element methods (discretized residuals), and sensitivity analysis (direct and adjoint methods). It is a canonical problem in design optimization and uncertainty quantification.\n- **Well-Posed:** The problem provides a clear objective and a complete set of assumptions and definitions required to calculate the computational cost of three distinct, standard algorithms. A unique answer exists based on these assumptions.\n- **Objective:** The language is formal, precise, and devoid of any subjectivity or ambiguity.\n- **Flaws:** The problem exhibits no flaws. It is a textbook example that is complete, consistent, relevant to the specified topic, and scientifically sound.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution follows.\n\nThe objective is to determine the total number of large sparse linear solves required to compute the complete gradient $\\nabla_{\\mathbf{p}} J$, which has $N_p$ components, for each of the three proposed methods. The unit of cost is one large sparse linear solve.\n\n### Method 1: Central Finite Differences (FD)\nThe gradient of $J$ with respect to the $i$-th parameter, $p_i$, can be approximated using a central finite difference formula:\n$$ \\frac{dJ}{dp_i} \\approx \\frac{J(\\mathbf{u}(\\mathbf{p}_0 + \\epsilon \\mathbf{e}_i)) - J(\\mathbf{u}(\\mathbf{p}_0 - \\epsilon \\mathbf{e}_i))}{2\\epsilon} $$\nwhere $\\mathbf{e}_i$ is the $i$-th standard basis vector and $\\epsilon$ is a small perturbation.\nTo evaluate this expression, we need to compute the state vector at two perturbed parameter points: $\\mathbf{u}(\\mathbf{p}_+)$ for $\\mathbf{p}_+ = \\mathbf{p}_0 + \\epsilon \\mathbf{e}_i$ and $\\mathbf{u}(\\mathbf{p}_-)$ for $\\mathbf{p}_- = \\mathbf{p}_0 - \\epsilon \\mathbf{e}_i$. This involves solving the nonlinear system $\\mathbf{R}(\\mathbf{u}, \\mathbf{p}) = \\mathbf{0}$ twice. The problem states this as requiring \"two perturbed state re-solves per parameter\".\n\nAccording to the cost model, a single nonlinear re-solve using Newton's method costs $S$ large sparse linear solves. Therefore, calculating one component of the gradient, $\\frac{dJ}{dp_i}$, costs $2 \\times S = 2S$ linear solves.\n\nTo compute the full gradient $\\nabla_{\\mathbf{p}} J$, this process must be repeated for each of the $N_p$ parameters. The total cost is:\n$$ N_{\\mathrm{FD}} = N_p \\times (2S) = 2SN_p $$\n\n### Method 2: Direct Sensitivities\nThe total derivative of the functional $J$ with respect to a parameter $p_i$ is given by the chain rule:\n$$ \\frac{dJ}{dp_i} = \\frac{\\partial J}{\\partial \\mathbf{u}} \\frac{d\\mathbf{u}}{dp_i} $$\nwhere the derivative is evaluated at the nominal point $(\\mathbf{u}_0, \\mathbf{p}_0)$. The vector $\\frac{d\\mathbf{u}}{dp_i}$ is the sensitivity of the state with respect to the parameter $p_i$.\nTo find this sensitivity vector, we differentiate the governing residual equation $\\mathbf{R}(\\mathbf{u}(\\mathbf{p}), \\mathbf{p}) = \\mathbf{0}$ with respect to $p_i$ and evaluate at the nominal point:\n$$ \\frac{d\\mathbf{R}}{dp_i} \\bigg|_{(\\mathbf{u}_0,\\mathbf{p}_0)} = \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{u}}\\bigg|_{(\\mathbf{u}_0,\\mathbf{p}_0)} \\frac{d\\mathbf{u}}{dp_i}\\bigg|_{\\mathbf{p}_0} + \\frac{\\partial \\mathbf{R}}{\\partial p_i}\\bigg|_{(\\mathbf{u}_0,\\mathbf{p}_0)} = \\mathbf{0} $$\nUsing the notation $\\mathbf{J}_0 = \\partial \\mathbf{R}/\\partial \\mathbf{u} \\big|_{(\\mathbf{u}_0,\\mathbf{p}_0)}$, we can rearrange this into a linear system for the sensitivity vector $\\frac{d\\mathbf{u}}{dp_i}$:\n$$ \\mathbf{J}_0 \\frac{d\\mathbf{u}}{dp_i} = - \\frac{\\partial \\mathbf{R}}{\\partial p_i} $$\nThe problem states that this approach involves \"solving one linear sensitivity system per parameter\". This corresponds to solving the above equation. This requires one large sparse linear solve for each parameter $p_i$. Since there are $N_p$ parameters, we must solve $N_p$ such linear systems, one for each sensitivity vector $\\frac{d\\mathbf{u}}{dp_i}$, $i=1, \\dots, N_p$. The coefficient matrix $\\mathbf{J}_0$ is the same for all systems.\n\nThe total cost for the full gradient is the number of systems to solve:\n$$ N_{\\mathrm{dir}} = N_p \\times 1 = N_p $$\n\n### Method 3: The Adjoint Method\nThe adjoint method is designed to efficiently compute the gradient of a scalar functional when the number of parameters $N_p$ is large. We start with the same expression for the gradient component as in the direct method:\n$$ \\frac{dJ}{dp_i} = \\frac{\\partial J}{\\partial \\mathbf{u}} \\frac{d\\mathbf{u}}{dp_i} $$\nFrom the direct method, we know that $\\frac{d\\mathbf{u}}{dp_i} = - \\mathbf{J}_0^{-1} \\frac{\\partial \\mathbf{R}}{\\partial p_i}$. Substituting this gives:\n$$ \\frac{dJ}{dp_i} = \\left( -\\frac{\\partial J}{\\partial \\mathbf{u}} \\mathbf{J}_0^{-1} \\right) \\frac{\\partial \\mathbf{R}}{\\partial p_i} $$\nThe adjoint method introduces a single adjoint vector, $\\boldsymbol{\\lambda} \\in \\mathbb{R}^{N_u}$, which is defined as the solution to the following linear system, known as the adjoint system:\n$$ \\mathbf{J}_0^T \\boldsymbol{\\lambda} = - \\left( \\frac{\\partial J}{\\partial \\mathbf{u}} \\right)^T $$\nSolving for the adjoint vector $\\boldsymbol{\\lambda}$ requires one large sparse linear solve, as stated in the problem: \"solving a single adjoint system\".\nBy taking the transpose of the adjoint equation, we get $\\boldsymbol{\\lambda}^T \\mathbf{J}_0 = - \\frac{\\partial J}{\\partial \\mathbf{u}}$. Post-multiplying by $\\mathbf{J}_0^{-1}$ gives $\\boldsymbol{\\lambda}^T = - \\frac{\\partial J}{\\partial \\mathbf{u}} \\mathbf{J}_0^{-1}$. This is precisely the parenthesized term in the expression for the gradient.\n\nSubstituting $\\boldsymbol{\\lambda}^T$ into the expression for $\\frac{dJ}{dp_i}$ yields:\n$$ \\frac{dJ}{dp_i} = \\boldsymbol{\\lambda}^T \\frac{\\partial \\mathbf{R}}{\\partial p_i} $$\nOnce the single adjoint vector $\\boldsymbol{\\lambda}$ is computed, all $N_p$ components of the gradient can be found by taking an inner product of $\\boldsymbol{\\lambda}$ with the vector $\\frac{\\partial \\mathbf{R}}{\\partial p_i}$ for each $p_i$. According to the cost model, vector operations (like inner products) and forming vectors (like $\\frac{\\partial \\mathbf{R}}{\\partial p_i}$) have negligible cost.\nTherefore, the total cost of the adjoint method is determined solely by the cost of solving for $\\boldsymbol{\\lambda}$.\n\nThe total cost is:\n$$ N_{\\mathrm{adj}} = 1 $$\n\n### Summary\nThe total numbers of large sparse linear solves required to compute the complete gradient for the three approaches are:\n- Finite Differences: $N_{\\mathrm{FD}} = 2SN_p$\n- Direct Sensitivities: $N_{\\mathrm{dir}} = N_p$\n- Adjoint Method: $N_{\\mathrm{adj}} = 1$\n\nThe final result, expressed as the requested row matrix, is $\\begin{bmatrix} 2SN_p & N_p & 1 \\end{bmatrix}$.",
            "answer": "$$\n\\boxed{\\begin{bmatrix} 2 S N_p & N_p & 1 \\end{bmatrix}}\n$$"
        },
        {
            "introduction": "Understanding the theoretical mechanics of the adjoint method is crucial before implementing it in code. This exercise guides you through a complete analytical derivation for a one-dimensional heat conduction problem, starting from first principles. You will derive and solve both the forward and adjoint boundary-value problems to obtain a closed-form expression for the sensitivity, providing a clear, step-by-step view of how the adjoint machinery works. ",
            "id": "3935200",
            "problem": "Consider steady heat conduction in a homogeneous, isotropic, one-dimensional rod occupying the spatial domain $\\Omega = (0,L)$, with constant thermal conductivity $k&gt;0$ and a uniform volumetric heat generation rate $s_0&gt;0$. Let $u(x)$ denote the temperature field. Start from conservation of energy and Fourier's law of heat conduction to formulate the governing partial differential equation (PDE) and boundary conditions for $u(x)$, namely homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(L)=0$. Define the objective functional as a nonlinear functional of $u$,\n$$\nJ(u) \\;=\\; \\int_{0}^{L} \\phi(u(x))\\,\\mathrm{d}x, \\qquad \\text{with} \\quad \\phi(u) \\;=\\; \\tfrac{1}{2}u^2.\n$$\nTreat the thermal conductivity $k$ as the single scalar parameter of interest. Using an adjoint-based sensitivity method:\n\n1) Derive the adjoint boundary-value problem by forming a Lagrangian that enforces the PDE constraint and requiring stationarity with respect to variations in $u$. Identify the adjoint forcing term $\\partial \\phi/\\partial u$ and the adjoint boundary conditions that are consistent with eliminating boundary variations under the imposed Dirichlet constraints on $u$.\n\n2) Solve the forward problem explicitly for $u(x)$.\n\n3) Solve the linearized adjoint problem explicitly for the adjoint field $\\lambda(x)$.\n\n4) Using your forward and adjoint solutions, derive the gradient of the objective with respect to the conductivity, $dJ/dk$, and simplify it to a single closed-form analytic expression in terms of $s_0$, $L$, and $k$.\n\nExpress your final answer for $dJ/dk$ as a single closed-form expression in terms of $s_0$, $L$, and $k$. Do not include units in your final expression.",
            "solution": "The analysis begins by establishing the governing equation for the temperature field $u(x)$ from first principles, as stipulated.\n\nFor steady-state one-dimensional heat transfer, the conservation of energy principle applied to an infinitesimal control volume $dx$ states that the net rate of heat conduction out of the volume must equal the rate of heat generation within it. This gives $\\frac{dq}{dx} = s_0$, where $q(x)$ is the heat flux and $s_0$ is the uniform volumetric heat generation rate.\nFourier's law of heat conduction relates the heat flux to the temperature gradient: $q = -k \\frac{du}{dx}$, where $k$ is the thermal conductivity.\nSubstituting Fourier's law into the energy conservation equation yields:\n$$\n\\frac{d}{dx}\\left(-k \\frac{du}{dx}\\right) = s_0\n$$\nSince the thermal conductivity $k$ is constant, this simplifies to the governing ordinary differential equation (ODE) for the forward problem:\n$$\n-k \\frac{d^2u}{dx^2} = s_0\n$$\nThe problem is defined on the domain $x \\in (0, L)$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(L)=0$. We define the state residual $R(u, k) = k \\frac{d^2u}{dx^2} + s_0 = 0$.\n\nThe task is to find the gradient of the objective functional $J(u) = \\int_{0}^{L} \\frac{1}{2}u^2(x)\\,\\mathrm{d}x$ with respect to the parameter $k$. We proceed using the adjoint method.\n\n1) Derivation of the Adjoint Boundary-Value Problem\n\nWe construct a Lagrangian functional $\\mathcal{L}$ by augmenting the objective functional $J$ with the governing PDE, enforced by a Lagrange multiplier field $\\lambda(x)$, also known as the adjoint field:\n$$\n\\mathcal{L}(u, k, \\lambda) = J(u) + \\int_{0}^{L} \\lambda(x) R(u, k)\\,\\mathrm{d}x = \\int_{0}^{L} \\frac{1}{2}u^2\\,\\mathrm{d}x + \\int_{0}^{L} \\lambda(x) \\left(k \\frac{d^2u}{dx^2} + s_0\\right) \\mathrm{d}x\n$$\nThe adjoint problem is derived by requiring the Lagrangian to be stationary with respect to arbitrary, admissible variations in the state field $u$. An admissible variation $\\delta u$ must satisfy the homogeneous form of the Dirichlet boundary conditions for $u$, i.e., $\\delta u(0)=0$ and $\\delta u(L)=0$. The stationarity condition is $\\delta_u \\mathcal{L} = 0$:\n$$\n\\delta_u \\mathcal{L} = \\frac{\\delta \\mathcal{L}}{\\delta u}[\\delta u] = \\int_{0}^{L} u\\,\\delta u\\,\\mathrm{d}x + \\int_{0}^{L} \\lambda \\left(k \\frac{d^2(\\delta u)}{dx^2}\\right) \\mathrm{d}x = 0\n$$\nWe apply integration by parts twice to the second term:\n$$\n\\int_{0}^{L} k\\lambda \\frac{d^2(\\delta u)}{dx^2}\\,\\mathrm{d}x = \\left[k\\lambda \\frac{d(\\delta u)}{dx}\\right]_0^L - \\int_{0}^{L} k\\frac{d\\lambda}{dx}\\frac{d(\\delta u)}{dx}\\,\\mathrm{d}x\n$$\n$$\n= \\left[k\\lambda \\frac{d(\\delta u)}{dx}\\right]_0^L - \\left[k\\frac{d\\lambda}{dx}\\delta u\\right]_0^L + \\int_{0}^{L} k\\frac{d^2\\lambda}{dx^2}\\delta u\\,\\mathrm{d}x\n$$\nSubstituting this back into the stationarity condition gives:\n$$\n\\int_{0}^{L} \\left(u + k\\frac{d^2\\lambda}{dx^2}\\right)\\delta u\\,\\mathrm{d}x + \\left[k\\lambda \\frac{d(\\delta u)}{dx} - k\\frac{d\\lambda}{dx}\\delta u\\right]_0^L = 0\n$$\nSince $\\delta u(0)=0$ and $\\delta u(L)=0$, the term $k\\frac{d\\lambda}{dx}\\delta u$ vanishes at the boundaries. To eliminate the remaining boundary term, $k\\lambda \\frac{d(\\delta u)}{dx}$, for arbitrary variations $\\delta u$, we must enforce boundary conditions on the adjoint field $\\lambda$. The simplest choice that ensures the boundary term is zero is to impose homogeneous Dirichlet boundary conditions on $\\lambda$: $\\lambda(0)=0$ and $\\lambda(L)=0$. These are the adjoint boundary conditions consistent with the forward problem's constraints.\n\nWith the boundary terms eliminated, the stationarity condition reduces to:\n$$\n\\int_{0}^{L} \\left(u + k\\frac{d^2\\lambda}{dx^2}\\right)\\delta u\\,\\mathrm{d}x = 0\n$$\nFor this integral to be zero for any admissible variation $\\delta u$, the integrand must be identically zero. This yields the adjoint governing PDE:\n$$\nk\\frac{d^2\\lambda}{dx^2} + u(x) = 0 \\quad \\text{or} \\quad k\\frac{d^2\\lambda}{dx^2} = -u(x)\n$$\nThe problem asks to identify the adjoint forcing term $\\partial \\phi/\\partial u$. In our problem, $\\phi(u) = \\frac{1}{2}u^2$, so its derivative with respect to $u$ is $\\frac{\\partial \\phi}{\\partial u} = u(x)$. The adjoint equation can be written as $k\\frac{d^2\\lambda}{dx^2} = -\\frac{\\partial \\phi}{\\partial u}$. Thus, the term $\\frac{\\partial \\phi}{\\partial u}=u(x)$ serves as the source for the adjoint problem.\n\nThe complete adjoint boundary-value problem is:\n-   PDE: $k\\frac{d^2\\lambda}{dx^2} = -u(x)$ for $x \\in (0,L)$.\n-   Boundary Conditions: $\\lambda(0)=0$, $\\lambda(L)=0$.\n\n2) Solution of the Forward Problem\n\nWe solve the forward problem: $k \\frac{d^2u}{dx^2} = -s_0$, with $u(0)=0$ and $u(L)=0$.\nIntegrating once with respect to $x$:\n$$\nk \\frac{du}{dx} = -s_0 x + C_1\n$$\nIntegrating a second time:\n$$\nk u(x) = -\\frac{s_0}{2}x^2 + C_1 x + C_2\n$$\nApplying the boundary conditions to find the constants $C_1$ and $C_2$:\n-   At $x=0$: $k u(0) = 0 \\implies 0 = 0 + 0 + C_2 \\implies C_2=0$.\n-   At $x=L$: $k u(L) = 0 \\implies 0 = -\\frac{s_0}{2}L^2 + C_1 L \\implies C_1 = \\frac{s_0 L}{2}$.\nSubstituting the constants, the temperature field is:\n$$\nk u(x) = -\\frac{s_0}{2}x^2 + \\frac{s_0 L}{2}x \\implies u(x) = \\frac{s_0}{2k}(Lx - x^2)\n$$\n\n3) Solution of the Adjoint Problem\n\nWe solve the adjoint problem: $k\\frac{d^2\\lambda}{dx^2} = -u(x)$, with $\\lambda(0)=0$ and $\\lambda(L)=0$.\nSubstituting the expression for $u(x)$:\n$$\nk\\frac{d^2\\lambda}{dx^2} = -\\frac{s_0}{2k}(Lx - x^2) \\implies \\frac{d^2\\lambda}{dx^2} = -\\frac{s_0}{2k^2}(Lx - x^2)\n$$\nIntegrating once with respect to $x$:\n$$\n\\frac{d\\lambda}{dx} = -\\frac{s_0}{2k^2}\\left(\\frac{L}{2}x^2 - \\frac{1}{3}x^3\\right) + D_1\n$$\nIntegrating a second time:\n$$\n\\lambda(x) = -\\frac{s_0}{2k^2}\\left(\\frac{L}{6}x^3 - \\frac{1}{12}x^4\\right) + D_1 x + D_2\n$$\nApplying the boundary conditions to find $D_1$ and $D_2$:\n-   At $x=0$: $\\lambda(0)=0 \\implies 0 = 0 + 0 + D_2 \\implies D_2=0$.\n-   At $x=L$: $\\lambda(L)=0 \\implies 0 = -\\frac{s_0}{2k^2}\\left(\\frac{L^4}{6} - \\frac{L^4}{12}\\right) + D_1 L$.\n$$\n0 = -\\frac{s_0}{2k^2}\\left(\\frac{L^4}{12}\\right) + D_1 L \\implies D_1 L = \\frac{s_0 L^4}{24k^2} \\implies D_1 = \\frac{s_0 L^3}{24k^2}\n$$\nSubstituting the constants, the adjoint field is:\n$$\n\\lambda(x) = -\\frac{s_0}{2k^2}\\left(\\frac{Lx^3}{6} - \\frac{x^4}{12}\\right) + \\frac{s_0 L^3}{24k^2}x = \\frac{s_0}{24k^2}(x^4 - 2Lx^3 + L^3x)\n$$\n\n4) Derivation of the Gradient $dJ/dk$\n\nThe core advantage of the adjoint method is that the total derivative of the objective functional with respect to a parameter can be computed as the partial derivative of the Lagrangian, holding the state and adjoint variables fixed. This is because the other term involving the sensitivity of the state variable, $\\frac{\\partial \\mathcal{L}}{\\partial u}\\frac{du}{dk}$, vanishes by the definition of the adjoint equation ($\\frac{\\partial \\mathcal{L}}{\\partial u}=0$).\n$$\n\\frac{dJ}{dk} = \\frac{d\\mathcal{L}}{dk} = \\frac{\\partial \\mathcal{L}}{\\partial k} = \\frac{\\partial}{\\partial k} \\int_{0}^{L} \\lambda(x) \\left(k \\frac{d^2u}{dx^2} + s_0\\right) \\mathrm{d}x\n$$\nDifferentiating under the integral sign with respect to $k$:\n$$\n\\frac{dJ}{dk} = \\int_{0}^{L} \\lambda(x) \\frac{\\partial}{\\partial k}\\left(k \\frac{d^2u}{dx^2} + s_0\\right) \\mathrm{d}x = \\int_{0}^{L} \\lambda(x) \\frac{d^2u}{dx^2} \\mathrm{d}x\n$$\nFrom the forward problem, we know $\\frac{d^2u}{dx^2} = -\\frac{s_0}{k}$. Substituting this into the expression for the gradient:\n$$\n\\frac{dJ}{dk} = \\int_{0}^{L} \\lambda(x) \\left(-\\frac{s_0}{k}\\right) \\mathrm{d}x = -\\frac{s_0}{k} \\int_{0}^{L} \\lambda(x) \\mathrm{d}x\n$$\nNow we compute the integral of the adjoint solution $\\lambda(x)$:\n$$\n\\int_{0}^{L} \\lambda(x) \\mathrm{d}x = \\int_{0}^{L} \\frac{s_0}{24k^2}(x^4 - 2Lx^3 + L^3x) \\mathrm{d}x\n$$\n$$\n= \\frac{s_0}{24k^2} \\left[ \\frac{x^5}{5} - \\frac{2L x^4}{4} + \\frac{L^3 x^2}{2} \\right]_0^L = \\frac{s_0}{24k^2} \\left( \\frac{L^5}{5} - \\frac{L^5}{2} + \\frac{L^5}{2} \\right) = \\frac{s_0}{24k^2} \\frac{L^5}{5} = \\frac{s_0 L^5}{120 k^2}\n$$\nFinally, we substitute this result back into the expression for the gradient:\n$$\n\\frac{dJ}{dk} = -\\frac{s_0}{k} \\left( \\frac{s_0 L^5}{120 k^2} \\right) = -\\frac{s_0^2 L^5}{120 k^3}\n$$\nThis is the final closed-form analytic expression for the sensitivity of the objective functional $J$ with respect to the thermal conductivity $k$.",
            "answer": "$$\n\\boxed{-\\frac{s_0^2 L^5}{120 k^3}}\n$$"
        },
        {
            "introduction": "A correct gradient implementation is the cornerstone of any gradient-based optimization. This practical coding exercise introduces the Taylor test, a rigorous and universally accepted method for verifying the accuracy of computed gradients. You will implement a finite-volume discretization for a heat conduction problem, develop the corresponding discrete adjoint solver, and write a verification script to confirm that your implementation is correct by checking its convergence properties. ",
            "id": "3935211",
            "problem": "Consider one-dimensional steady heat conduction on a rod of length $L$ with spatially varying thermal conductivity parameterized on cell faces by a vector $p \\in \\mathbb{R}^{M}$ where $M = N - 1$ for a uniform grid with $N$ nodal points and spacing $\\Delta x = L/(N-1)$. The governing Partial Differential Equation (PDE) is $- \\dfrac{d}{dx}\\left(k(x;p)\\,\\dfrac{du}{dx}\\right) = q(x)$ with Dirichlet boundary conditions $u(0) = T_0$ and $u(L) = T_1$, where $u(x)$ is temperature in $\\mathrm{K}$, $k(x;p)$ is thermal conductivity in $\\mathrm{W}\\cdot\\mathrm{m}^{-1}\\cdot\\mathrm{K}^{-1}$, and $q(x)$ is volumetric heat source in $\\mathrm{W}\\cdot\\mathrm{m}^{-3}$.\n\nUse a finite-volume discretization on the uniform grid, representing $k$ on faces by the components of $p$, to obtain a linear algebraic system for the interior unknown temperatures $u \\in \\mathbb{R}^{n}$ where $n = N - 2$:\n$$\nA(p)\\,u = b(p),\n$$\nwith $A(p) \\in \\mathbb{R}^{n \\times n}$ and $b(p) \\in \\mathbb{R}^{n}$ constructed from face conductivities and Dirichlet boundary values. Define a quadratic objective functional\n$$\nJ(u) = \\frac{1}{2}\\sum_{i=1}^{n}\\omega_i\\left(u_i - u^{\\mathrm{ref}}_i\\right)^2,\n$$\nwhere $\\omega_i > 0$ are weights and $u^{\\mathrm{ref}} \\in \\mathbb{R}^{n}$ is a reference interior temperature profile.\n\nTask: Starting from Fourier’s law of heat conduction and conservation of energy, derive the discrete adjoint equation and an adjoint-based expression for the gradient of $J$ with respect to the parameter vector $p$. Then implement a program that:\n- Solves the forward problem $A(p)\\,u=b(p)$ for $u$.\n- Solves the adjoint system to obtain the adjoint variable $\\lambda \\in \\mathbb{R}^{n}$.\n- Computes the gradient $\\nabla_p J(p)$ using the adjoint.\n- Performs a Taylor test to verify gradient accuracy by comparing adjoint-based directional derivatives with finite-difference directional derivatives. For a given direction $h \\in \\mathbb{R}^{M}$ and a sequence of step sizes $\\{\\varepsilon_k\\}$, compute:\n  - The first-order error $E_1(\\varepsilon_k) = \\left|J(p + \\varepsilon_k h) - J(p)\\right|$.\n  - The second-order corrected error $E_2(\\varepsilon_k) = \\left|J(p + \\varepsilon_k h) - J(p) - \\varepsilon_k\\,\\nabla_p J(p)^\\top h\\right|$.\nEstimate the convergence orders by fitting $\\log E_1$ versus $\\log \\varepsilon$ and $\\log E_2$ versus $\\log \\varepsilon$ and extracting the slopes $s_1$ and $s_2$ respectively. The Taylor test is considered passed if $s_1$ is approximately $1$ and $s_2$ is approximately $2$ within a prescribed tolerance.\n\nPhysical units must be respected for inputs: $L$ in $\\mathrm{m}$, thermal conductivity components of $p$ in $\\mathrm{W}\\cdot\\mathrm{m}^{-1}\\cdot\\mathrm{K}^{-1}$, $q$ in $\\mathrm{W}\\cdot\\mathrm{m}^{-3}$, temperatures $T_0$, $T_1$, $u$, and $u^{\\mathrm{ref}}$ in $\\mathrm{K}$. The program’s outputs are dimensionless booleans indicating pass/fail of the Taylor test for each case.\n\nTest Suite:\n- Case $1$ (general variability): $L = 1$, $N = 21$, $q(x) \\equiv q_0 = 1000$, $T_0 = 300$, $T_1 = 310$, $\\omega_i \\equiv 1$, $u^{\\mathrm{ref}}_i = 305$ for all $i$, base $p_j = 10\\left(1 + 0.2\\sin\\left(\\pi\\frac{j+0.5}{N-1}\\right)\\right)$ for $j=0,\\dots,N-2$, and direction $h$ a deterministic random unit vector.\n- Case $2$ (boundary-influenced, small system): $L = 0.1$, $N = 5$, $q(x) \\equiv 0$, $T_0 = 400$, $T_1 = 300$, $\\omega_i \\equiv 1$, $u^{\\mathrm{ref}}_i = 350$ for all $i$, base $p_j = 20$ for all faces, and direction $h$ supported only on boundary faces $j=0$ and $j=N-2$ with entries $1$ and $-1$ respectively, normalized to unit norm.\n- Case $3$ (larger system with random weights and reference): $L = 2$, $N = 31$, $q(x) \\equiv q_0 = 500$, $T_0 = 290$, $T_1 = 315$, $\\omega_i$ deterministic random positive entries in $[0.5,1.5]$, $u^{\\mathrm{ref}}$ deterministic random entries in $[300,305]$, base $p_j = 15\\left(1 + 0.1\\cos\\left(2\\pi\\frac{j+0.5}{N-1}\\right)\\right)$, direction $h$ a deterministic random unit vector.\n\nUse the step sizes $\\varepsilon_k \\in \\{10^{-1}, 5\\times10^{-2}, 2.5\\times10^{-2}, 1.25\\times10^{-2}, 6.25\\times10^{-3}\\}$. The Taylor test is considered passed for a case if the fitted slopes satisfy $|s_1 - 1| \\leq 0.15$ and $|s_2 - 2| \\leq 0.15$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$), where each $result_i$ is a boolean indicating whether the Taylor test passed for the corresponding test case.",
            "solution": "The problem statement is found to be valid. It is scientifically grounded in the principles of heat transfer, mathematically well-posed, objective, and provides a complete and consistent set of information required for a unique solution and verification. We may therefore proceed with the derivation and implementation.\n\nThe core task is to derive an adjoint-based expression for the gradient of an objective functional with respect to thermal conductivity parameters and to verify its correctness using a Taylor test. The derivation proceeds in three main stages: discretizing the governing PDE, formulating the adjoint problem, and deriving the gradient expression.\n\n**1. Finite-Volume Discretization (Forward Problem)**\n\nThe governing equation for one-dimensional steady-state heat conduction is given by the conservation of energy principle:\n$$\n- \\frac{d}{dx}\\left(k(x;p)\\,\\frac{du}{dx}\\right) = q(x)\n$$\nwhere $u(x)$ is the temperature, $k(x;p)$ is the thermal conductivity parameterized by a vector $p \\in \\mathbb{R}^{M}$, and $q(x)$ is a volumetric heat source. The domain is a rod of length $L$, with Dirichlet boundary conditions $u(0) = T_0$ and $u(L) = T_1$.\n\nWe discretize the domain using a uniform grid of $N$ nodal points $x_i = i \\Delta x$ for $i=0, 1, \\dots, N-1$, where the grid spacing is $\\Delta x = L/(N-1)$. The interior nodes where the temperature is unknown are $i=1, \\dots, n$, with $n = N-2$. The temperature at these nodes is assembled into a vector $u \\in \\mathbb{R}^{n}$. The thermal conductivity is defined on the $M=N-1$ faces between nodes, with the parameter $p_j$ representing the conductivity on the face between node $j$ and node $j+1$ for $j=0, \\dots, M-1$.\n\nApplying the finite-volume method, we integrate the governing equation over a control volume (CV) centered at each interior node $i$. The CV for node $i$ extends from face $i-\\frac{1}{2}$ to face $i+\\frac{1}{2}$.\n$$\n\\int_{x_{i-1/2}}^{x_{i+1/2}} - \\frac{d}{dx}\\left(k\\,\\frac{du}{dx}\\right) dx = \\int_{x_{i-1/2}}^{x_{i+1/2}} q(x) dx\n$$\nBy the fundamental theorem of calculus, the integral on the left becomes the difference in heat flux, $F = -k \\frac{du}{dx}$, at the CV faces:\n$$\n\\left(-k\\,\\frac{du}{dx}\\right)\\bigg|_{x_{i+1/2}} - \\left(-k\\,\\frac{du}{dx}\\right)\\bigg|_{x_{i-1/2}} = \\bar{q}_i \\Delta x\n$$\nwhere $\\bar{q}_i$ is the average heat source in the CV. Approximating the fluxes using a central difference scheme and the face conductivities $p_j$, we have:\n- Flux at face $i-\\frac{1}{2}$ (between nodes $i-1$ and $i$, conductivity $p_{i-1}$): $F_{i-1/2} \\approx -p_{i-1} \\frac{u_i - u_{i-1}}{\\Delta x}$.\n- Flux at face $i+\\frac{1}{2}$ (between nodes $i$ and $i+1$, conductivity $p_i$): $F_{i+1/2} \\approx -p_i \\frac{u_{i+1} - u_i}{\\Delta x}$.\n\nSubstituting these into the balance equation for an interior node $i \\in \\{2, \\dots, n-1\\}$ gives:\n$$\n-p_i \\frac{u_{i+1} - u_i}{\\Delta x} - \\left(-p_{i-1} \\frac{u_i - u_{i-1}}{\\Delta x}\\right) = q_i \\Delta x\n$$\nRearranging for the unknown temperatures $u_i$ yields the linear equation for row $i$:\n$$\n\\frac{1}{\\Delta x^2} \\left[ -p_{i-1} u_{i-1} + (p_{i-1} + p_i) u_i - p_i u_{i+1} \\right] = q_i\n$$\nFor the boundary-adjacent nodes, we incorporate the known temperatures $u_0=T_0$ and $u_n+1=u_{N-1}=T_1$:\n- For node $i=1$: $-p_1 \\frac{u_2 - u_1}{\\Delta x} + p_0 \\frac{u_1 - T_0}{\\Delta x} = q_1 \\Delta x \\implies \\frac{1}{\\Delta x^2} \\left[ (p_0+p_1)u_1 - p_1 u_2 \\right] = q_1 + \\frac{p_0 T_0}{\\Delta x^2}$.\n- For node $i=n=N-2$: $-p_{n} \\frac{T_1 - u_n}{\\Delta x} + p_{n-1} \\frac{u_n - u_{n-1}}{\\Delta x} = q_n \\Delta x \\implies \\frac{1}{\\Delta x^2} \\left[ -p_{n-1}u_{n-1} + (p_{n-1}+p_n)u_n \\right] = q_n + \\frac{p_n T_1}{\\Delta x^2}$.\n\nThis set of $n$ linear equations forms the system $A(p)u = b(p)$, where $A(p) \\in \\mathbb{R}^{n \\times n}$ is a symmetric, tridiagonal matrix and $b(p) \\in \\mathbb{R}^{n}$ is the source vector.\n\n**2. Adjoint Formulation**\n\nThe objective functional $J$ depends on the parameters $p$ implicitly through the state variable $u$: $J(p) = J(u(p))$. Its gradient with respect to a parameter $p_j$ is found using the chain rule:\n$$\n\\frac{dJ}{dp_j} = \\frac{\\partial J}{\\partial u}^T \\frac{\\partial u}{\\partial p_j}\n$$\nThe sensitivity term $\\frac{\\partial u}{\\partial p_j}$ is implicitly defined by differentiating the state equation, $R(u, p) = A(p)u - b(p) = 0$, with respect to $p_j$:\n$$\n\\frac{d R}{d p_j} = \\frac{\\partial R}{\\partial u}\\frac{\\partial u}{\\partial p_j} + \\frac{\\partial R}{\\partial p_j} = 0 \\implies A(p)\\frac{\\partial u}{\\partial p_j} = -\\frac{\\partial R}{\\partial p_j}\n$$\nSolving for $\\frac{\\partial u}{\\partial p_j}$ gives $\\frac{\\partial u}{\\partial p_j} = -A(p)^{-1} \\frac{\\partial R}{\\partial p_j}$. Substituting this into the gradient expression:\n$$\n\\frac{dJ}{dp_j} = - \\left(\\frac{\\partial J}{\\partial u}\\right)^T A(p)^{-1} \\frac{\\partial R}{\\partial p_j} = - \\left( (A(p)^{-1})^T \\frac{\\partial J}{\\partial u} \\right)^T \\frac{\\partial R}{\\partial p_j}\n$$\nTo avoid the computationally expensive matrix inversion $A(p)^{-1}$, we define an adjoint variable $\\lambda \\in \\mathbb{R}^n$ as the solution to the **adjoint equation**:\n$$\nA(p)^T \\lambda = -\\frac{\\partial J}{\\partial u}\n$$\nSince the matrix $A(p)$ is symmetric ($A(p)^T = A(p)$), the adjoint equation becomes $A(p)\\lambda = -\\nabla_u J$. The gradient of the objective functional $J(u) = \\frac{1}{2}\\sum_{i=1}^{n}\\omega_i(u_i - u^{\\mathrm{ref}}_i)^2$ with respect to $u$ is a vector with components $(\\nabla_u J)_i = \\omega_i(u_i - u^{\\mathrm{ref}}_i)$.\n\nWith the definition of $\\lambda$, the gradient expression simplifies to:\n$$\n\\frac{dJ}{dp_j} = \\lambda^T \\frac{\\partial R}{\\partial p_j}\n$$\n\n**3. Gradient Expression**\n\nThe final step is to compute the partial derivative of the residual vector, $\\frac{\\partial R}{\\partial p_j}$. The residual for an interior node $i$ is $R_i(u, p) = \\sum_{k=1}^n A_{ik}(p)u_k - b_i(p)$. We analyze its dependence on each parameter $p_j$ for $j \\in \\{0, \\dots, M-1\\}$.\n\nThe residual vector is defined from the physical balance equations:\n$R_1 = \\frac{1}{\\Delta x^2} \\left[ (p_0+p_1)u_1 - p_1 u_2 - p_0 T_0 \\right] - q_1$\n$R_i = \\frac{1}{\\Delta x^2} \\left[ -p_{i-1}u_{i-1} + (p_{i-1}+p_i)u_i - p_i u_{i+1} \\right] - q_i$, for $i \\in \\{2, \\dots, n-1\\}$\n$R_n = \\frac{1}{\\Delta x^2} \\left[ -p_{n-1}u_{n-1} + (p_{n-1}+p_n)u_n - p_n T_1 \\right] - q_n$\n\nWe compute the partial derivatives of the residual vector components $R_i$ with respect to a parameter $p_j$:\n\n- For $j=0$ (first face): $p_0$ only appears in $R_1$.\n  $\\frac{\\partial R_1}{\\partial p_0} = \\frac{1}{\\Delta x^2}(u_1 - T_0)$. All other $\\frac{\\partial R_i}{\\partial p_0}$ are $0$.\n  So, $\\frac{dJ}{dp_0} = \\lambda^T \\frac{\\partial R}{\\partial p_0} = \\lambda_1 \\frac{u_1 - T_0}{\\Delta x^2}$.\n\n- For $j \\in \\{1, \\dots, n-1\\}$ (interior faces): $p_j$ appears in $R_j$ and $R_{j+1}$.\n  $\\frac{\\partial R_j}{\\partial p_j} = \\frac{1}{\\Delta x^2}(u_j - u_{j+1})$\n  $\\frac{\\partial R_{j+1}}{\\partial p_j} = \\frac{1}{\\Delta x^2}(-u_j + u_{j+1})$\n  So, $\\frac{dJ}{dp_j} = \\lambda_j \\frac{\\partial R_j}{\\partial p_j} + \\lambda_{j+1} \\frac{\\partial R_{j+1}}{\\partial p_j} = \\frac{(\\lambda_j - \\lambda_{j+1})(u_j - u_{j+1})}{\\Delta x^2}$.\n\n- For $j=n=N-2$ (last face): $p_n$ only appears in $R_n$.\n  $\\frac{\\partial R_n}{\\partial p_n} = \\frac{1}{\\Delta x^2}(u_n - T_1)$. All other $\\frac{\\partial R_i}{\\partial p_n}$ are $0$.\n  So, $\\frac{dJ}{dp_n} = \\lambda_n \\frac{u_n - T_1}{\\Delta x^2}$.\n\nIn summary, the components of the gradient $\\nabla_p J$ are:\n$$\n\\frac{dJ}{dp_j} =\n\\begin{cases}\n \\lambda_1 \\frac{u_1 - T_0}{\\Delta x^2} & \\text{if } j=0 \\\\\n \\frac{(\\lambda_j - \\lambda_{j+1})(u_j - u_{j+1})}{\\Delta x^2} & \\text{if } 1 \\le j \\le n-1 \\\\\n \\lambda_n \\frac{u_n - T_1}{\\Delta x^2} & \\text{if } j=n=N-2\n\\end{cases}\n$$\nThese formulae use 1-based indexing for vectors $u$ and $\\lambda$.\n\nThe procedure for computing the gradient is:\n1.  Solve the forward problem $A(p)u = b(p)$ to find the temperature profile $u$.\n2.  Using $u$, compute the right-hand side of the adjoint system, $-\\nabla_u J$.\n3.  Solve the adjoint problem $A(p)\\lambda = -\\nabla_u J$ to find the adjoint variables $\\lambda$.\n4.  Combine $u$ and $\\lambda$ using the derived formulae to compute $\\nabla_p J$.\n\nThe Taylor test verifies this gradient by comparing the adjoint-based directional derivative $\\nabla_p J(p)^T h$ against a finite-difference approximation. For a smooth function $J(p)$, Taylor's theorem states that $J(p+\\varepsilon h) = J(p) + \\varepsilon \\nabla_p J(p)^T h + O(\\varepsilon^2)$. The errors $E_1 = |J(p+\\varepsilon h) - J(p)|$ and $E_2 = |J(p+\\varepsilon h) - J(p) - \\varepsilon \\nabla_p J(p)^T h|$ are expected to converge as $O(\\varepsilon^1)$ and $O(\\varepsilon^2)$, respectively. Plotting $\\log(E)$ vs $\\log(\\varepsilon)$ should yield lines with slopes of approximately $1$ and $2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for adjoint-based gradient verification.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general variability)\n        {\n            \"L\": 1.0, \"N\": 21, \"q0\": 1000.0, \"T0\": 300.0, \"T1\": 310.0,\n            \"omega_mode\": \"const\", \"u_ref_mode\": \"const\",\n            \"p_mode\": \"sin\", \"h_mode\": \"random\", \"seed\": 0\n        },\n        # Case 2 (boundary-influenced, small system)\n        {\n            \"L\": 0.1, \"N\": 5, \"q0\": 0.0, \"T0\": 400.0, \"T1\": 300.0,\n            \"omega_mode\": \"const\", \"u_ref_mode\": \"const\",\n            \"p_mode\": \"const\", \"h_mode\": \"boundary\", \"seed\": 1\n        },\n        # Case 3 (larger system with random weights and reference)\n        {\n            \"L\": 2.0, \"N\": 31, \"q0\": 500.0, \"T0\": 290.0, \"T1\": 315.0,\n            \"omega_mode\": \"random\", \"u_ref_mode\": \"random\",\n            \"p_mode\": \"cos\", \"h_mode\": \"random\", \"seed\": 2\n        }\n    ]\n\n    epsilons = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3])\n    slope_tolerance = 0.15\n    results = []\n\n    for case in test_cases:\n        test_passed = run_taylor_test(case, epsilons, slope_tolerance)\n        results.append(test_passed)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef assemble_system(p, L, N, q0, T0, T1):\n    \"\"\"\n    Assembles the finite-volume system matrix A and RHS vector b.\n    \"\"\"\n    n = N - 2  # Number of interior nodes\n    dx = L / (N - 1)\n    \n    A = np.zeros((n, n))\n    b = np.full(n, q0 * dx**2) # using the expression from derivation, not scaled by dx^-2\n    \n    # Python uses 0-based indexing for u_int[0...n-1]\n    # Parameter p has M=N-1 components, p[0...N-2]\n    \n    # Diagonal entries\n    for i in range(n):\n      A[i, i] = p[i] + p[i+1]\n      \n    # Off-diagonal entries\n    for i in range(n-1):\n      A[i, i+1] = -p[i+1]\n      A[i+1, i] = -p[i+1]\n\n    # Incorporate boundary conditions into RHS vector b\n    b[0] += p[0] * T0\n    b[n-1] += p[N-2] * T1 # p index is N-2, which is also n\n\n    # We derived Au/dx^2 = b_prime.\n    # The system is A_scaled u = b_scaled where A_scaled = A/dx^2.\n    # We solve A_unscaled u = b_unscaled.\n    # Here A and b are unscaled by 1/dx^2. Let's scale them.\n    A_scaled = A / (dx**2)\n    b_scaled = np.full(n, q0)\n    b_scaled[0] += p[0] * T0 / (dx**2)\n    b_scaled[n-1] += p[N-2] * T1 / (dx**2)\n\n    return A_scaled, b_scaled\n\ndef compute_J(u, omega, u_ref):\n    \"\"\"\n    Computes the objective functional J.\n    \"\"\"\n    return 0.5 * np.sum(omega * (u - u_ref)**2)\n\ndef compute_gradient(u, lamb, p, L, N, T0, T1):\n    \"\"\"\n    Computes the gradient of J w.r.t. p using the adjoint method.\n    \"\"\"\n    n = N - 2\n    M = N - 1\n    dx = L / (N - 1)\n    grad = np.zeros(M)\n    \n    # Python arrays u, lamb are 0-indexed (0 to n-1)\n    # p is 0-indexed (0 to M-1)\n    \n    # j = 0 (corresponds to p[0])\n    grad[0] = lamb[0] * (u[0] - T0) / dx**2\n    \n    # j = 1 to n-1 (corresponds to p[1] to p[n-1])\n    for j in range(1, n):\n        grad[j] = (lamb[j-1] - lamb[j]) * (u[j-1] - u[j]) / dx**2\n        \n    # j = n (corresponds to p[n] = p[N-2])\n    grad[n] = lamb[n-1] * (u[n-1] - T1) / dx**2\n        \n    return grad\n\ndef run_taylor_test(params, epsilons, tolerance):\n    \"\"\"\n    Performs a single Taylor test for a given configuration.\n    \"\"\"\n    L, N, q0, T0, T1 = params[\"L\"], params[\"N\"], params[\"q0\"], params[\"T0\"], params[\"T1\"]\n    n = N - 2\n    M = N - 1\n    \n    rng = np.random.default_rng(params[\"seed\"])\n    \n    # Setup omega (weights)\n    if params[\"omega_mode\"] == \"const\":\n        omega = np.ones(n)\n    else: # \"random\"\n        omega = rng.uniform(0.5, 1.5, size=n)\n        \n    # Setup u_ref (reference temperature)\n    if params[\"u_ref_mode\"] == \"const\":\n        u_ref_val = 305.0 if params[\"L\"] == 1.0 else 350.0\n        u_ref = np.full(n, u_ref_val)\n    else: # \"random\"\n        u_ref = rng.uniform(300.0, 305.0, size=n)\n\n    # Setup p (base parameters)\n    j = np.arange(M)\n    if params[\"p_mode\"] == \"sin\":\n        p = 10.0 * (1.0 + 0.2 * np.sin(np.pi * (j + 0.5) / M))\n    elif params[\"p_mode\"] == \"cos\":\n        p = 15.0 * (1.0 + 0.1 * np.cos(2.0 * np.pi * (j + 0.5) / M))\n    else: # \"const\"\n        p = np.full(M, 20.0)\n        \n    # Setup h (perturbation direction)\n    if params[\"h_mode\"] == \"random\":\n        h = rng.standard_normal(M)\n    else: # \"boundary\"\n        h = np.zeros(M)\n        h[0] = 1.0\n        h[M - 1] = -1.0\n    h /= np.linalg.norm(h)\n    \n    # 1. Forward solve\n    A_base, b_base = assemble_system(p, L, N, q0, T0, T1)\n    u_base = scipy.linalg.solve(A_base, b_base, assume_a='sym')\n    J_base = compute_J(u_base, omega, u_ref)\n    \n    # 2. Adjoint solve\n    adj_rhs = -omega * (u_base - u_ref)\n    lamb = scipy.linalg.solve(A_base, adj_rhs, assume_a='sym')\n\n    # 3. Gradient computation\n    grad_J = compute_gradient(u_base, lamb, p, L, N, T0, T1)\n    \n    # Directional derivative\n    grad_h = np.dot(grad_J, h)\n    \n    E1_vals = []\n    E2_vals = []\n    \n    for eps in epsilons:\n        p_eps = p + eps * h\n        \n        # Solve perturbed forward problem and compute J\n        A_eps, b_eps = assemble_system(p_eps, L, N, q0, T0, T1)\n        u_eps = scipy.linalg.solve(A_eps, b_eps, assume_a='sym')\n        J_eps = compute_J(u_eps, omega, u_ref)\n        \n        # Calculate errors\n        E1 = abs(J_eps - J_base)\n        E2 = abs(J_eps - J_base - eps * grad_h)\n        \n        E1_vals.append(E1)\n        E2_vals.append(E2)\n\n    # 4. Convergence order estimation\n    log_eps = np.log(epsilons)\n    log_E1 = np.log(np.array(E1_vals))\n    log_E2 = np.log(np.array(E2_vals))\n    \n    # Use np.polyfit for linear regression (slope is the first element)\n    s1 = np.polyfit(log_eps, log_E1, 1)[0]\n    s2 = np.polyfit(log_eps, log_E2, 1)[0]\n    \n    # 5. Verdict\n    test_passed = abs(s1 - 1.0) <= tolerance and abs(s2 - 2.0) <= tolerance\n    return test_passed\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}