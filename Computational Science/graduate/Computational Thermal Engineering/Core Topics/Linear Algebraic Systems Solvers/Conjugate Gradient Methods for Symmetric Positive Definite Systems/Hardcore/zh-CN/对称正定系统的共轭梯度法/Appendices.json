{
    "hands_on_practices": [
        {
            "introduction": "为了真正领会共轭梯度（CG）法的强大之处，最好的方法是将其与一个更直观但效率较低的前身——最速下降（SD）法进行比较。这项实践将引导您实现这两种算法，并将它们应用于一个由各向异性热传导问题产生的病态线性系统。通过观察两者性能上的巨大差异 ，您将理解为何CG方法的 $A$-正交搜索方向能够避免困扰SD方法的“之”字形停滞现象，从而获得更快的收敛速度。",
            "id": "3942625",
            "problem": "考虑单位正方形域上的二维稳态热传导问题，该问题具有齐次狄利克雷边界条件，由偏微分方程 $-\\nabla \\cdot (K \\nabla T) = s$ 控制。其中，$K = \\mathrm{diag}(k_x, k_y)$ 是一个常数热导率张量，$T$ 是温度场，$s$ 是一个均匀源项。在大小为 $n_x \\times n_y$ 的内部网格上，使用间距为 $h_x = 1/(n_x+1)$ 和 $h_y = 1/(n_y+1)$ 的二阶中心有限差分进行离散化，这将产生一个线性系统 $A x = b$，其中 $A$ 是一个稀疏对称正定 (SPD) 矩阵。未知向量 $x$ 包含内部网格点上的温度，右端项 $b$ 对应于离散化的源项。\n\n与此 SPD 系统相关的能量泛函为 $E(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$，其梯度为 $\\nabla E(x) = A x - b$。根据平衡态下最小能量的物理原理，唯一的极小值点 $x^\\star$ 满足 $A x^\\star = b$。因此，最小化 $E(x)$ 或求解 $A x = b$ 的算法利用了由 $A$ 导出的几何结构。\n\n目标是数值上证明，当矩阵 $A$ 的条件数较高时，与共轭梯度法 (CG) 相比，最速下降法 (SD) 会出现停滞现象，并将此行为与能量最小化问题的几何结构联系起来。\n\n实现以下内容：\n\n- 将 $A$ 构造为各向异性扩散算子产生的稀疏 SPD 矩阵，其对角线元素为 $2 \\left( \\frac{k_x}{h_x^2} + \\frac{k_y}{h_y^2} \\right)$，$x$ 方向的非对角线相邻元素为 $-\\frac{k_x}{h_x^2}$，$y$ 方向的非对角线相邻元素为 $-\\frac{k_y}{h_y^2}$。通过消除边界节点来使用齐次狄利克雷边界。令 $b$ 为全 1 向量，代表在内部网格点上求值的均匀源 $s=1$。\n- 实现最速下降法 (SD)，从 $x_0 = 0$ 开始，沿 $E(x)$ 的负梯度方向进行精确线搜索。当相对残差范数 $\\|r_k\\|_2 / \\|b\\|_2$ 低于指定的容差或达到指定的最大迭代次数时停止。\n- 实现共轭梯度法 (CG)，从 $x_0 = 0$ 开始，停止准则与 SD 相同。\n- 对每种算法，计算：\n  1. 达到容差所需的迭代次数（如果未达到，则为最大迭代次数）。\n  2. 连续搜索方向之间的平均绝对余弦值，定义为 $\\frac{1}{m-1} \\sum_{k=0}^{m-2} \\left| \\frac{p_k^\\top p_{k+1}}{\\|p_k\\|_2 \\|p_{k+1}\\|_2} \\right|$，其中 $p_k$ 是第 $k$ 次迭代的搜索方向，$m$ 是执行的迭代次数。对于最速下降法，取 $p_k$ 等于残差方向（负梯度）。对于共轭梯度法，取 $p_k$ 等于共轭搜索方向。\n\n使用以下参数值测试套件：\n\n- 测试 1 (一般各向同性情况): $n_x = 16$, $n_y = 16$, $k_x = 1$, $k_y = 1$, 容差 $10^{-8}$, 最大迭代次数 $2000$。\n- 测试 2 (中等各向异性情况): $n_x = 16$, $n_y = 16$, $k_x = 100$, $k_y = 1$, 容差 $10^{-8}$, 最大迭代次数 $3000$。\n- 测试 3 (强各向异性边缘情况): $n_x = 32$, $n_y = 32$, $k_x = 10000$, $k_y = 1$, 容差 $10^{-6}$, 最大迭代次数 $5000$。\n\n对于每个测试用例，计算并按顺序返回一个包含五个值的列表：\n- SD 迭代次数的整数值。\n- CG 迭代次数的整数值。\n- 浮点数比率 $\\text{SD 迭代次数} / \\text{CG 迭代次数}$。\n- 连续 SD 方向之间的平均绝对余弦值的浮点数值。\n- 连续 CG 方向之间的平均绝对余弦值的浮点数值。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表由三个子列表（每个测试用例一个）组成，每个子列表均采用上述格式，所有内容都包含在方括号中。例如：\"[[sd1,cg1,ratio1,cosSD1,cosCG1],[sd2,cg2,ratio2,cosSD2,cosCG2],[sd3,cg3,ratio3,cosSD3,cosCG3]]\"。所有值必须以 Python 列表格式打印，且仅包含基本类型（整数和浮点数），输出中不需要单位。",
            "solution": "该问题要求对求解稀疏、对称正定 (SPD) 线性系统 $A x = b$ 的最速下降法 (SD) 和共轭梯度法 (CG) 算法进行数值比较。该系统源于单位正方形上稳态各向异性热传导问题的有限差分离散化。目标是展示和解释 CG 相较于 SD 的优越性能，特别是在系统由于高热各向异性而变得病态时，并将此行为与相关能量最小化问题的几何结构联系起来。\n\n首先，我们将问题形式化。控制偏微分方程为 $-\\nabla \\cdot (K \\nabla T) = s$，定义在区域 $\\Omega = (0,1) \\times (0,1)$上，边界 $\\partial\\Omega$ 上的温度 $T=0$。热导率张量为 $K = \\mathrm{diag}(k_x, k_y)$，源项 $s$ 为常数。我们使用均匀网格对区域进行离散化，该网格在 $x$ 和 $y$ 方向上分别有 $n_x$ 和 $n_y$ 个内部点。网格间距为 $h_x = 1/(n_x+1)$ 和 $h_y = 1/(n_y+1)$。\n\n在内部网格点 $(i, j)$ 处对散度项使用二阶中心有限差分格式，可得：\n$$\n-\\left( k_x \\frac{T_{i+1,j} - 2T_{i,j} + T_{i-1,j}}{h_x^2} + k_y \\frac{T_{i,j+1} - 2T_{i,j} + T_{i,j-1}}{h_y^2} \\right) = s_{i,j}\n$$\n对未知温度 $T_{i,j}$ 重新整理各项，得到：\n$$\n\\left( \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} \\right) T_{i,j} - \\frac{k_x}{h_x^2} T_{i+1,j} - \\frac{k_x}{h_x^2} T_{i-1,j} - \\frac{k_y}{h_y^2} T_{i,j-1} - \\frac{k_y}{h_y^2} T_{i,j+1} = s_{i,j}\n$$\n齐次狄利克雷边界条件（在 $\\partial\\Omega$ 上 $T=0$）通过将任何索引在内部网格之外的 $T$ 项设为零来引入。通过将 $N = n_x n_y$ 个未知温度 $T_{i,j}$ 排列成一个单一向量 $x$，我们得到线性系统 $A x = b$。矩阵 $A$ 是一个稀疏、块三对角、对称矩阵。当 $k_x, k_y > 0$ 时，它也是正定的。向量 $b$ 由源项值 $s_{i,j}$ 构成，对于所有内部点，这些值都设为 $1$。\n\n$A x = b$ 的解也是二次能量泛函 $E(x) = \\frac{1}{2} x^\\top A x - b^\\top x$ 的唯一极小值点。像 SD 和 CG 这样的迭代方法通过生成一个近似序列 $x_0, x_1, x_2, \\dots$ 来找到这个极小值点，该序列逐步减小能量 $E(x_k)$。$E(x)$ 的水平集（即 $N$ 维椭球）的几何形状决定了收敛行为。这些椭球的离心率由 $A$ 的条件数 $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$ 决定。高的各向异性，例如 $k_x \\gg k_y$，会导致大的条件数，从而产生高度拉长的能量椭球。\n\n最速下降法 (SD) 算法在第 $k$ 次迭代时通过 $x_{k+1} = x_k + \\alpha_k p_k$ 更新解，其中搜索方向 $p_k$ 被选为能量泛函的最速下降方向。这个方向是梯度的负方向，即 $p_k = -\\nabla E(x_k) = b - A x_k = r_k$，其中 $r_k$ 是残差。步长 $\\alpha_k$ 的选择是为了最小化沿此方向的 $E(x_{k+1})$，从而得到精确线搜索公式：\n$$\n\\alpha_k = \\frac{p_k^\\top r_k}{p_k^\\top A p_k} = \\frac{r_k^\\top r_k}{r_k^\\top A r_k}\n$$\n采用精确线搜索的 SD 的一个关键特性是，连续的残差（因此也是搜索方向）是正交的：$r_{k+1}^\\top r_k = 0$。在几何上，这意味着搜索路径呈“之”字形。在每一步，算法都沿着与当前水平集正交的方向移动，直到与一个新的、能量更低的水平集相切。然后，下一步从该切点开始，与该点正交。对于病态系统（高度偏心的椭球），这些正交步骤在能量景观的长而窄的“山谷”中导航效率非常低，导致收敛极其缓慢或停滞。连续搜索方向之间的平均绝对余弦值，理论上是正交的，在实践中由于浮点运算会接近于零。\n\n共轭梯度法 (CG) 算法通过选择更智能的搜索方向，显著改进了 SD。搜索方向集 $\\{p_0, p_1, \\dots\\}$ 被构造成 A-正交（或共轭）的，即当 $i \\neq j$ 时，$p_i^\\top A p_j = 0$。第 $k$ 个搜索方向 $p_k$ 与前一个方向 $p_{k-1}$ 进行 A-正交化：\n$$\np_k = r_k + \\beta_{k-1} p_{k-1} \\quad \\text{其中} \\quad \\beta_{k-1} = \\frac{r_k^\\top r_k}{r_{k-1}^\\top r_{k-1}}\n$$\n这种构造确保了在每一步 $k$ 中，方向 $p_{k-1}$ 上的误差分量被消除，并且不会在后续步骤中重新引入。这避免了 SD 的浪费性“之”字形移动，并使 CG 能够更快地收敛，特别是对于病态系统。CG 的收敛速率取决于 $\\sqrt{\\kappa(A)}$，这比 SD 对 $\\kappa(A)$ 的依赖性有了显著的改进。连续的搜索方向在标准意义上不是正交的；平均绝对余弦值将不为零，这表明其路径比 SD 更直接，振荡更少。\n\n该实现将使用 `scipy.sparse` 构建稀疏矩阵 $A$，然后从 $x_0 = 0$ 开始实现 SD 和 CG 算法。对于每个测试用例，我们将计算达到指定相对残差容差所需的迭代次数、SD 与 CG 迭代次数的比率，以及两种方法连续搜索方向之间的平均绝对余弦值。结果将从数值上验证 CG 相对于 SD 的理论优势。",
            "answer": "```python\nimport numpy as np\nfrom scipy import sparse\n\ndef build_A(nx, ny, hx, hy, kx, ky):\n    \"\"\"\n    Constructs the sparse SPD matrix A for the 2D anisotropic diffusion problem.\n    \"\"\"\n    N = nx * ny\n    A = sparse.lil_matrix((N, N))\n    \n    diag_val = 2 * (kx / hx**2 + ky / hy**2)\n    x_off_diag = -kx / hx**2\n    y_off_diag = -ky / hy**2\n\n    for j in range(ny):\n        for i in range(nx):\n            k = i + j * nx\n            \n            # Diagonal entry\n            A[k, k] = diag_val\n            \n            # Off-diagonal entries\n            if i > 0:  # West neighbor\n                A[k, k - 1] = x_off_diag\n            if i  nx - 1:  # East neighbor\n                A[k, k + 1] = x_off_diag\n            if j > 0:  # South neighbor\n                A[k, k - nx] = y_off_diag\n            if j  ny - 1:  # North neighbor\n                A[k, k + nx] = y_off_diag\n                \n    return A.tocsr()\n\ndef calc_avg_cos(dirs):\n    \"\"\"\n    Calculates the average absolute cosine between successive vectors in a list.\n    \"\"\"\n    m = len(dirs)\n    if m  2:\n        return 0.0\n    \n    total_cos = 0.0\n    for i in range(m - 1):\n        p_curr = dirs[i]\n        p_next = dirs[i+1]\n        \n        norm_curr = np.linalg.norm(p_curr)\n        norm_next = np.linalg.norm(p_next)\n        \n        if norm_curr == 0.0 or norm_next == 0.0:\n            continue\n            \n        cos_val = (p_curr @ p_next) / (norm_curr * norm_next)\n        total_cos += abs(cos_val)\n        \n    return total_cos / (m - 1)\n\ndef steepest_descent(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Steepest Descent algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    b_norm = np.linalg.norm(b)\n    \n    if b_norm == 0.0:\n        b_norm = 1.0\n        \n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.linalg.norm(r) / b_norm  tol:\n            break\n        \n        iters += 1\n        \n        search_dirs.append(r)\n        \n        Ar = A @ r\n        alpha = (r @ r) / (r @ Ar)\n        \n        x = x + alpha * r\n        r = r - alpha * Ar\n        \n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef conjugate_gradient(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Conjugate Gradient algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0.0:\n        b_norm = 1.0\n\n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.sqrt(rs_old) / b_norm  tol:\n            break\n            \n        iters += 1\n        \n        search_dirs.append(p)\n        \n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = r @ r\n        \n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n\n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'nx': 16, 'ny': 16, 'kx': 1, 'ky': 1, 'tol': 1e-8, 'max_iter': 2000},\n        {'nx': 16, 'ny': 16, 'kx': 100, 'ky': 1, 'tol': 1e-8, 'max_iter': 3000},\n        {'nx': 32, 'ny': 32, 'kx': 10000, 'ky': 1, 'tol': 1e-6, 'max_iter': 5000}\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        nx, ny, kx, ky = params['nx'], params['ny'], params['kx'], params['ky']\n        tol, max_iter = params['tol'], params['max_iter']\n        \n        hx = 1.0 / (nx + 1)\n        hy = 1.0 / (ny + 1)\n        N = nx * ny\n        \n        A = build_A(nx, ny, hx, hy, kx, ky)\n        b = np.ones(N)\n        \n        sd_iters, sd_cos = steepest_descent(A, b, tol, max_iter)\n        cg_iters, cg_cos = conjugate_gradient(A, b, tol, max_iter)\n        \n        ratio = float('inf') if cg_iters == 0 else sd_iters / cg_iters\n        \n        all_results.append([sd_iters, cg_iters, ratio, sd_cos, cg_cos])\n\n    # Format output as a string representation of a list of lists.\n    result_str = \",\".join(map(str, all_results))\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "共轭梯度法的理论推导严格依赖于系统矩阵的对称正定（SPD）特性。本练习是一个“压力测试” ，旨在揭示当这一基本假设被违背时会发生什么。通过将标准的CG算法应用于一个轻微非对称的矩阵，您将直接观察到其收敛性质的退化甚至失效，这清晰地展示了当算法被错误地应用在其理论域之外时所产生的“模型误差”。",
            "id": "3252635",
            "problem": "考虑一个线性系统，其矩阵被建模为对称正定，但实际上是轻微非对称的。设 $n$ 为一个正整数维度，设 $A \\in \\mathbb{R}^{n \\times n}$ 是一个三对角矩阵，其主对角线元素为 $2$，每个紧邻的次对角线元素为 $-1$（即带有狄利克雷边界条件的标准一维离散拉普拉斯算子）。考虑一个由 $B \\in \\mathbb{R}^{n \\times n}$ 定义的非对称扰动，其元素为 $B_{ij} = 1$ (若 $j  i$) 且 $B_{ij} = 0$ (其他情况)。对于给定的标量 $\\epsilon \\in \\mathbb{R}$，定义扰动矩阵 $\\tilde{A} = A + \\epsilon B$。右端向量为 $b \\in \\mathbb{R}^{n}$，其定义为对所有下标 $i$ 都有 $b_i = 1$。\n\n基本原理。共轭梯度法 (Conjugate Gradient, CG) 是为求解对称正定 (Symmetric Positive Definite, SPD) 矩阵的线性系统而设计的。对于 SPD 矩阵，CG 法可以被解释为一个迭代最小化与该矩阵相关的严格凸二次泛函的过程，其搜索方向是相互 A-共轭的，残差是相互正交的。这些性质依赖于矩阵的对称性和正定性。\n\n任务。你必须实现一个假设矩阵对称的共轭梯度法 (CG)，并将其直接应用于上述矩阵序列 $\\tilde{A}$。对于每个测试用例，使用初始猜测值 $x^{(0)} = 0$、最大迭代次数等于 $n$ 来运行 CG。停止准则基于相对残差范数小于容差 $10^{-10}$，即一旦 $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$ (其中 $r^{(k)} = b - \\tilde{A} x^{(k)}$) 就停止。对于每个测试用例，还需使用直接稠密求解器求解 $\\tilde{A} x = b$ 以计算精确解 $x^{\\ast}$，并测量相对解误差 $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$。将收敛行为或解的精度中的任何偏差解释为因选择 CG 时错误地假设 $\\tilde{A}$ 具有对称性而产生的建模误差。\n\n测试套件。使用以下参数值并报告每个用例的结果：\n\n- 用例 $1$ (理想情况，精确适用)：$n = 50$, $\\epsilon = 0$。\n- 用例 $2$ (小的非对称性)：$n = 50$, $\\epsilon = 10^{-3}$。\n- 用例 $3$ (中等非对称性)：$n = 50$, $\\epsilon = 10^{-1}$。\n- 用例 $4$ (显著的非对称性)：$n = 50$, $\\epsilon = 5 \\cdot 10^{-1}$。\n\n对于每个用例，返回两个输出：\n- 一个等于相对解误差 $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$ 的浮点数。\n- 一个布尔值，指示 CG 是否在 $n$ 次迭代内满足了停止准则 (如果在 $k = n$ 或之前 $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$，则为 true，否则为 false)。\n\n最终输出格式。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表内容按测试用例排序，每个测试用例内先是浮点数，然后是布尔值。例如，格式为 $[e_1,c_1,e_2,c_2,e_3,c_3,e_4,c_4]$，其中 $e_i$ 是浮点数，$c_i$ 是布尔值。",
            "solution": "该问题要求探究一个特定建模误差所带来的后果：将为对称正定 (SPD) 系统设计的算法——共轭梯度法 (CG)，应用于一个非对称的线性系统。我们将首先阐述 CG 方法的理论基础及其对对称性的依赖，然后分析非对称扰动如何影响其性能。\n\n共轭梯度法是一种用于求解形如 $Mx = b$ 的线性系统的迭代算法，其中矩阵 $M \\in \\mathbb{R}^{n \\times n}$ 是对称且正定的。其有效性源于它与一个优化问题的联系。对于一个 SPD 矩阵 $M$，求解 $Mx = b$ 等价于寻找严格凸二次泛函 $\\phi(x) = \\frac{1}{2}x^T M x - x^T b$ 的唯一极小化子。该泛函的梯度是 $\\nabla\\phi(x) = Mx - b$，也即线性系统的残差 $r(x) = b - Mx$。CG 生成一个迭代序列 $x^{(k)}$，使得 $\\phi(x^{(k)})$ 被逐步最小化。\n\nCG 方法能够在至多 $n$ 次迭代内收敛（在精确算术下）的关键性质，是矩阵 $M$ 对称性的直接结果：\n1.  **搜索方向的 A-共轭性**：搜索方向 $\\{p^{(0)}, p^{(1)}, \\dots, p^{(n-1)}\\}$ 满足对于所有 $k \\neq j$ 都有 $p^{(k)T} M p^{(j)} = 0$。这确保了沿新方向 $p^{(k)}$ 最小化 $\\phi(x)$ 不会破坏在先前方向上已经达成的最小化效果。\n2.  **残差的正交性**：残差 $\\{r^{(0)}, r^{(1)}, \\dots, r^{(n-1)}\\}$ 是相互正交的，满足对于所有 $k \\neq j$ 都有 $r^{(k)T} r^{(j)} = 0$。\n\n当矩阵非对称时，这些基本性质就会丧失。泛函 $\\phi(x)$ 可能不再有与解对应的唯一最小值，而驱动算法效率的正交性和共轭关系也会失效。因此，将 CG 应用于非对称系统是一种建模误差，因为该算法被用在了其理论有效性范围之外。这可能导致不稳定的收敛、停滞或完全发散。\n\n在这个问题中，我们构建一个矩阵 $\\tilde{A} = A + \\epsilon B$。矩阵 $A$ 是一个 $n \\times n$ 的三对角矩阵，其主对角线元素为 $2$，次对角线和上对角线元素为 $-1$。该矩阵是 SPD 矩阵的一个著名例子，源于二阶导数算子的有限差分法离散化。矩阵 $B$ 是一个严格上三角矩阵，其对角线上方所有元素都等于 $1$，即 $B_{ij} = 1$ (若 $j  i$) 且为 $0$ (其他情况)。因此，矩阵 $\\tilde{A}$ 定义如下：\n$$\n\\tilde{A}_{ij} =\n\\begin{cases}\n2  \\text{if } i = j \\\\\n-1  \\text{if } |i - j| = 1 \\\\\n\\epsilon  \\text{if } j  i \\text{ and } |i-j|  1 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\n对于任何 $\\epsilon \\neq 0$，矩阵 $\\tilde{A}$ 都是非对称的，因为 $A$ 是对称的而 $B$ 不是，所以 $\\tilde{A}^T = A^T + \\epsilon B^T = A + \\epsilon B^T \\neq \\tilde{A}$。标量 $\\epsilon$ 的大小控制着非对称的程度。右端向量是对于所有 $i$ 都有 $b_i = 1$。\n\n将要实现的标准 CG 算法如下：\n1.  初始化 $x^{(0)} = 0$, $r^{(0)} = b - \\tilde{A}x^{(0)} = b$, $p^{(0)} = r^{(0)}$。令 $\\rho_0 = r^{(0)T} r^{(0)}$。\n2.  对于 $k = 0, 1, 2, \\dots, n-1$：\n    a. 计算 $v^{(k)} = \\tilde{A} p^{(k)}$。\n    b. 步长为 $\\alpha_k = \\frac{\\rho_k}{p^{(k)T} v^{(k)}}$。\n    c. 更新解：$x^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}$。\n    d. 更新残差：$r^{(k+1)} = r^{(k)} - \\alpha_k v^{(k)}$。\n    e. 检查收敛性：如果相对残差范数 $\\|r^{(k+1)}\\|_2 / \\|b\\|_2$ 小于容差 $10^{-10}$，则算法已收敛。\n    f. 计算 $\\rho_{k+1} = r^{(k+1)T} r^{(k+1)}$。\n    g. 更新搜索方向：$p^{(k+1)} = r^{(k+1)} + \\frac{\\rho_{k+1}}{\\rho_k} p^{(k)}$。更新 $\\rho_k \\leftarrow \\rho_{k+1}$。\n\n我们将对 $\\epsilon$ 递增的四个测试用例执行此算法，并将得到的解 $x_{\\mathrm{CG}}$ 与从直接求解器获得的精确解 $x^{\\ast}$ 进行比较。\n\n用例 1: $\\epsilon = 0$。此时，$\\tilde{A} = A$，是 SPD 矩阵。CG 方法完全适用于该系统。我们期望在 $n=50$ 次的迭代限制内快速收敛到一个高精度的解。\n\n用例 2: $\\epsilon = 10^{-3}$。此时矩阵 $\\tilde{A}$ 轻微非对称。CG 的假设被违背了，但程度很轻。该方法的性能预计会轻微下降，可能比 SPD 情况需要更多次迭代，但它仍然很可能收敛到指定的容差。最终解的误差会很小，但可能比用例 1 大。\n\n用例 3: $\\epsilon = 10^{-1}$。非对称扰动是显著的。潜在的正交性和共轭性的丧失将更加明显。残差范数的收敛可能变得非单调。该方法能否在 $n=50$ 次迭代内收敛是不确定的。如果宣告收敛，解的精度预计会差很多。\n\n用例 4: $\\epsilon = 5 \\cdot 10^{-1}$。由于有如此大的非对称部分，CG 的行为预计会高度不稳定。$\\alpha_k$ 的分母 $p^{(k)T} \\tilde{A} p^{(k)}$ 不再保证为正，并且可能变为零或负值，从而导致算法崩溃。该方法极有可能在最大迭代次数内无法满足收敛准则。最终的迭代值 $x^{(n)}$ 相对于精确解 $x^{\\ast}$ 很可能会有很大的误差。\n\n这个数值实验将展示一个建模误差——即在算法的有效理论域之外误用算法——如何表现为性能和精度上可量化的下降。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the formatted output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50, 0.0),          # Case 1: SPD matrix\n        (50, 1e-3),         # Case 2: Small nonsymmetry\n        (50, 1e-1),         # Case 3: Moderate nonsymmetry\n        (50, 5e-1),         # Case 4: Significant nonsymmetry\n    ]\n\n    results = []\n\n    for n, epsilon in test_cases:\n        # Construct the matrices A and B\n        A = np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n        B = np.triu(np.ones((n, n)), k=1)\n        \n        # Construct the perturbed matrix tilde_A\n        tilde_A = A + epsilon * B\n        \n        # Construct the right-hand side vector b\n        b = np.ones(n)\n        norm_b = np.linalg.norm(b)\n        \n        # Compute the true solution x* using a direct solver\n        try:\n            x_star = np.linalg.solve(tilde_A, b)\n        except np.linalg.LinAlgError:\n            # Handle cases where tilde_A might be singular for large epsilon\n            results.extend([float('inf'), 'false'])\n            continue\n\n        # Run the Conjugate Gradient (CG) method\n        x_cg = np.zeros(n)\n        r = b - tilde_A @ x_cg\n        p = r.copy()\n        rs_old_sq = r @ r\n        \n        max_iter = n\n        tol = 1e-10\n        converged = False\n\n        for k in range(max_iter):\n            Ap = tilde_A @ p\n            \n            p_dot_Ap = p @ Ap\n            # Breakdown condition if p_dot_Ap is zero or negative\n            if p_dot_Ap = 0:\n                break\n\n            alpha = rs_old_sq / p_dot_Ap\n            x_cg = x_cg + alpha * p\n            r = r - alpha * Ap\n            \n            rel_res_norm = np.linalg.norm(r) / norm_b\n            if rel_res_norm  tol:\n                converged = True\n                break\n            \n            rs_new_sq = r @ r\n            beta = rs_new_sq / rs_old_sq\n            p = r + beta * p\n            rs_old_sq = rs_new_sq\n\n        # Measure the relative solution error\n        norm_x_star = np.linalg.norm(x_star)\n        if norm_x_star == 0:\n            # Handle the case of a zero true solution\n            rel_err = np.linalg.norm(x_cg)\n        else:\n            rel_err = np.linalg.norm(x_cg - x_star) / norm_x_star\n            \n        results.append(rel_err)\n        # Append boolean converted to lowercase string\n        results.append(str(converged).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "共轭梯度法的用途不仅限于求解线性系统，其算法行为本身也能为我们提供关于矩阵的重要诊断信息。这项高级实践将探索CG方法的渐进收敛率与矩阵谱条件数 $\\kappa(A)$ 之间的深刻联系 。您将学习如何通过分析CG运行过程中产生的残差范数历史来估计 $\\kappa(A)$，这提供了一种无需昂贵的特征值计算就能诊断系统病态程度的实用方法。",
            "id": "3942614",
            "problem": "考虑在有界域上的稳态热传导方程，其具有齐次狄利克雷边界条件。经过二阶中心有限差分格式离散化后，该方程产生一个形如 $A x = b$ 的线性系统，其中 $A$ 是对称正定（Symmetric Positive Definite, SPD）矩阵。应用于此类系统的共轭梯度（Conjugate Gradient, CG）法会生成一个残差序列 $\\{r_k\\}$，其中 $r_k = b - A x_k$。请设计一个实验，该实验直接根据记录的CG残差历史来估计谱条件数 $\\kappa(A)$，而无需访问 $A$ 的特征值，并对该估计器进行经验性验证。\n\n您的程序必须实现以下步骤：\n\n1.  构造对热传导离散算子进行建模的SPD矩阵 $A$：\n    a) 在区间 $[0,1]$ 上导热系数恒定的一维杆，使用 $N$ 个内部点进行离散化。得到的矩阵是三对角矩阵，其主对角线元素为 $2/h^2$，次对角线元素为 $-1/h^2$，其中 $h = 1/(N+1)$。\n    b) 在 $[0,1] \\times [0,1]$ 上可能具有各向异性导热系数的二维方板，在 $x$ 方向有 $N_x$ 个点、$y$ 方向有 $N_y$ 个点的均匀网格上进行离散化。得到的矩阵使用一个五点差分格式，其主对角线上的系数为 $2 k_x/h_x^2 + 2 k_y/h_y^2$，在 $x$ 方向上最近邻点的系数为 $-k_x/h_x^2$，在 $y$ 方向上最近邻点的系数为 $-k_y/h_y^2$。其中 $h_x = 1/(N_x+1)$，$h_y = 1/(N_y+1)$，$k_x$ 和 $k_y$ 分别表示沿 $x$ 轴和 $y$ 轴的导热系数。\n\n2.  对于每个构造的SPD矩阵 $A$，将右侧向量 $b$ 设置为长度适当的全1向量，并将 $x_0$ 初始化为零向量。应用共轭梯度法求解 $A x = b$，在每次迭代 $k$ 时记录残差范数 $\\|r_k\\|_2$ 直至收敛。使用基于相对残差的停止准则，即当 $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$（其中 $\\varepsilon = 10^{-12}$）时，或达到最大迭代次数时停止。\n\n3.  从记录的残差范数 $\\{\\|r_k\\|_2\\}$ 中，仅使用残差历史构建谱条件数 $\\kappa(A)$ 的估计器。该估计器必须基于在残差范数与迭代次数的半对数图中识别一个渐近线性速率，并将此速率映射到 $\\kappa(A)$ 的估计值。该设计必须基于应用于SPD系统的CG方法的属性，具有科学依据，而不是与理论脱节的经验性曲线拟合。\n\n4.  为验证该估计器，对每个测试矩阵，使用数值方法得到的极端特征值计算参考条件数 $\\kappa_{\\text{true}}(A)$。通过相对误差 $\\left|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)\\right| / \\kappa_{\\text{true}}(A)$ 来比较估计值 $\\kappa_{\\text{est}}(A)$ 与 $\\kappa_{\\text{true}}(A)$。\n\n使用以下测试套件以确保覆盖范围：\n- 测试用例1：一维杆，$N = 50$。\n- 测试用例2：一维杆，$N = 200$。\n- 测试用例3：二维板，各向异性导热系数，$k_x = 1$，$k_y = 10$，$N_x = 20$，$N_y = 20$。\n- 测试用例4：二维各向同性板，$k_x = 1$，$k_y = 1$，$N_x = 8$，$N_y = 8$。\n\n对于所有测试，将长度和导热系数视为无量纲；无需进行物理单位转换。不涉及角度，因此无需角度单位。所有数值比较均以小数形式表示。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个测试用例，按测试套件的顺序输出一个布尔值，如果相对误差 $\\left|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)\\right| / \\kappa_{\\text{true}}(A)$ 小于或等于 $0.50$，则为 true，否则为 false。例如，输出格式应与 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$ 完全一样。",
            "solution": "该问题要求设计并验证一个实验，以仅使用共轭梯度（CG）法的收敛历史来估计对称正定（SPD）矩阵 $A$ 的谱条件数 $\\kappa(A)$。矩阵 $A$ 源于稳态热传导方程的有限差分离散化。\n\n**1. 估计器的理论基础**\n\n共轭梯度法是一种用于求解线性系统 $A x = b$ 的迭代算法，其中 $A$ 是一个 $n \\times n$ 的SPD矩阵。该方法生成一系列近似解 $x_k$，这些解在不断扩大的克雷洛夫子空间上最小化误差的A-范数，即 $\\|x - x_k\\|_A = \\sqrt{(x - x_k)^T A (x - x_k)}$。CG方法的收敛速率由谱条件数 $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$ 限定，其中 $\\lambda_{\\max}(A)$ 和 $\\lambda_{\\min}(A)$ 分别是 $A$ 的最大和最小特征值。\n\n数值线性代中的一个标准结果给出了误差 $e_k = x_* - x_k$（其中 $x_*$ 是真实解）的A-范数的以下上界：\n$$\n\\|e_k\\|_A \\le 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\|e_0\\|_A\n$$\n该不等式表明，误差预计在每次迭代中至少会渐近地减少一个因子 $\\rho$，其中 $\\rho$ 是几何收敛因子：\n$$\n\\rho = \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1}\n$$\n虽然这个界是针对误差的A-范数，但可以证明残差的 $L_2$-范数 $\\|r_k\\|_2 = \\|b - A x_k\\|_2 = \\|A e_k\\|_2$ 表现出相同的渐近收敛率。因此，对于较大的迭代次数 $k$，我们期望以下近似成立：\n$$\n\\|r_k\\|_2 \\approx C \\cdot \\rho^k\n$$\n其中 $C$ 是一个常数，取决于初始残差和误差的特征向量分解。\n\n**2. 估计器设计**\n\n关系式 $\\|r_k\\|_2 \\approx C \\cdot \\rho^k$ 构成了我们估计 $\\kappa(A)$ 的基础。通过对两边取自然对数，我们得到 $\\log(\\|r_k\\|_2)$ 与迭代次数 $k$ 之间的线性关系：\n$$\n\\log(\\|r_k\\|_2) \\approx \\log(C) + k \\log(\\rho)\n$$\n该方程表明，残差范数与迭代次数的半对数图在渐近区域内将近似呈线性关系，其斜率为 $S = \\log(\\rho)$。\n\n估计 $\\kappa(A)$ 的实验步骤如下：\n1.  将CG算法应用于系统 $A x = b$，并记录每次迭代 $k = 0, 1, 2, \\dots$ 时的残差范数序列 $\\{\\|r_k\\|_2\\}$。\n2.  选择残差历史中能代表渐近收敛行为的一部分。一个常见的启发式方法是使用达到收敛前记录的迭代次数的后半部分。设此迭代序列为 $k \\in \\{k_{\\text{start}}, \\dots, k_{\\text{end}}\\}$。\n3.  对该选定范围内的点 $(k, \\log(\\|r_k\\|_2))$ 进行线性回归（最小二乘拟合），以获得斜率的估计值 $\\hat{S}$。\n4.  在估计出斜率 $\\hat{S} \\approx \\log(\\rho)$ 后，我们可以求解 $\\rho$：\n    $$\n    \\hat{\\rho} = e^{\\hat{S}}\n    $$\n5.  最后，我们反转 $\\rho$ 和 $\\kappa(A)$ 之间的关系，以找到我们的估计值 $\\kappa_{\\text{est}}(A)$：\n    $$\n    \\hat{\\rho} = \\frac{\\sqrt{\\kappa_{\\text{est}}(A)} - 1}{\\sqrt{\\kappa_{\\text{est}}(A)} + 1} \\implies \\sqrt{\\kappa_{\\text{est}}(A)} = \\frac{1 + \\hat{\\rho}}{1 - \\hat{\\rho}}\n    $$\n    $$\n    \\kappa_{\\text{est}}(A) = \\left( \\frac{1 + \\hat{\\rho}}{1 - \\hat{\\rho}} \\right)^2 = \\left( \\frac{1 + e^{\\hat{S}}}{1 - e^{\\hat{S}}} \\right)^2\n    $$\n这提供了一种直接从CG算法的可观察输出中估计 $\\kappa(A)$ 的方法，而无需任何关于矩阵 $A$ 特征值的知识。\n\n**3. 算法与验证流程**\n\n该实验通过以下计算步骤实现：\n\n-   **矩阵构造**：使用 `scipy.sparse` 构造SPD矩阵 $A$ 作为稀疏矩阵。\n    -   对于具有 $N$ 个内部点和步长 $h = 1/(N+1)$ 的 $1$D 情况，矩阵是一个大小为 $N \\times N$ 的三对角矩阵，其对角线元素为 $2/h^2$，次对角线元素为 $-1/h^2$。\n    -   对于在 $N_x \\times N_y$ 网格上的 $2$D 情况，大小为 $(N_xN_y) \\times (N_xN_y)$ 的矩阵是通过两个 $1$D 拉普拉斯矩阵的克罗内克积之和来构造的。设 $T_M$ 是一个 $M \\times M$ 的三对角矩阵，其对角线上为 $2$，次对角线上为 $-1$，并设 $I_M$ 是大小为 $M$ 的单位矩阵。$2$D 算子矩阵为 $A = \\frac{k_x}{h_x^2} (T_{N_x} \\otimes I_{N_y}) + \\frac{k_y}{h_y^2} (I_{N_x} \\otimes T_{N_y})$。这种构造正确地模拟了在具有字典序排序的节点网格上的五点差分格式。\n\n-   **共轭梯度实现**：需要一个自定义的CG算法实现，以在每次迭代 $k$ 时记录残差 $r_k$ 的 $L_2$-范数。算法从 $x_0 = 0$（零向量）和 $b$ 为全1向量开始。当相对残差范数 $\\|r_k\\|_2 / \\|b\\|_2$ 低于容差 $\\varepsilon = 10^{-12}$ 或达到最大迭代次数时，算法终止。\n\n-   **估计器实现**：如第2节所述计算 $\\kappa_{\\text{est}}(A)$。使用 `numpy.polyfit` 对CG运行生成的迭代次数后半部分的残差范数的自然对数进行线性回归，以找到斜率 $\\hat{S}$。\n\n-   **验证**：为验证估计器，对每个测试矩阵计算“真实”条件数 $\\kappa_{\\text{true}}(A)$。这是通过使用稀疏特征值求解器 `scipy.sparse.linalg.eigs` 数值求解极端特征值来完成的。具体来说，我们找到 $\\lambda_{\\min}(A)$ 和 $\\lambda_{\\max}(A)$，并计算 $\\kappa_{\\text{true}}(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$。然后通过相对误差来量化估计器的性能：\n    $$\n    \\text{Error}_{\\text{rel}} = \\frac{|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)|}{\\kappa_{\\text{true}}(A)}\n    $$\n    每个测试用例的最终输出是一个布尔值，如果此相对误差小于或等于 $0.50$，则为 `True`，否则为 `False`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse import linalg as sla\n\ndef build_1d_matrix(N: int):\n    \"\"\"Constructs the SPD matrix for a 1D rod.\"\"\"\n    h = 1.0 / (N + 1)\n    diag_val = 2.0 / h**2\n    off_diag_val = -1.0 / h**2\n    diagonals = [[off_diag_val] * (N - 1), [diag_val] * N, [off_diag_val] * (N - 1)]\n    return sparse.diags(diagonals, [-1, 0, 1], format='csr')\n\ndef build_2d_matrix(Nx: int, Ny: int, kx: float, ky: float):\n    \"\"\"Constructs the SPD matrix for a 2D plate using Kronecker products.\"\"\"\n    hx = 1.0 / (Nx + 1)\n    hy = 1.0 / (Ny + 1)\n\n    diag_x = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Nx, Nx))\n    diag_y = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Ny, Ny))\n    \n    Ix = sparse.identity(Nx)\n    Iy = sparse.identity(Ny)\n    \n    Ax = sparse.kron(diag_x, Iy) * (kx / hx**2)\n    Ay = sparse.kron(Ix, diag_y) * (ky / hy**2)\n    \n    return (Ax + Ay).tocsr()\n\ndef conjugate_gradient(A, b, tol=1e-12, max_iter=5000):\n    \"\"\"\n    Solves Ax=b using the Conjugate Gradient method, recording residual norms.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n)\n    r = b - A.dot(x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0:\n        return x, []\n        \n    residual_norms = [np.sqrt(rs_old)]\n    \n    for k in range(max_iter):\n        Ap = A.dot(p)\n        alpha = rs_old / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = np.dot(r, r)\n        \n        norm_r = np.sqrt(rs_new)\n        residual_norms.append(norm_r)\n        \n        if norm_r / b_norm = tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return x, residual_norms\n\ndef estimate_kappa(residual_norms):\n    \"\"\"\n    Estimates the condition number from the CG residual history.\n    \"\"\"\n    # Use the second half of the iterations for linear regression, as it represents\n    # the asymptotic convergence behavior.\n    num_iterations = len(residual_norms)\n    if num_iterations  4:\n        # Not enough data for a meaningful fit\n        return np.nan\n        \n    fit_start_index = num_iterations // 2\n    \n    iters = np.arange(fit_start_index, num_iterations)\n    log_residuals = np.log(residual_norms[fit_start_index:])\n    \n    # Perform linear regression to find the slope\n    # polyfit returns [slope, intercept]\n    slope, _ = np.polyfit(iters, log_residuals, 1)\n    \n    if slope >= 0:\n        # Should be a negative slope for a converging method\n        return np.nan\n\n    # Calculate kappa from the slope\n    # S = log(rho) => rho = exp(S)\n    # kappa_est = ((1 + rho) / (1 - rho))^2\n    rho_est = np.exp(slope)\n    kappa_est = ((1 + rho_est) / (1 - rho_est))**2\n    \n    return kappa_est\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'type': '1D', 'N': 50},\n        {'type': '1D', 'N': 200},\n        {'type': '2D', 'Nx': 20, 'Ny': 20, 'kx': 1.0, 'ky': 10.0},\n        {'type': '2D', 'Nx': 8, 'Ny': 8, 'kx': 1.0, 'ky': 1.0},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        if case['type'] == '1D':\n            A = build_1d_matrix(case['N'])\n            matrix_size = case['N']\n        else: # 2D\n            A = build_2d_matrix(case['Nx'], case['Ny'], case['kx'], case['ky'])\n            matrix_size = case['Nx'] * case['Ny']\n\n        # Compute true condition number using numerical eigenvalues\n        # Since A is SPD, eigenvalues are real and positive.\n        # k=1 returns one eigenvalue, which='LM' for largest magnitude, 'SM' for smallest.\n        try:\n            # Use a small sigma for SM to avoid issues with zero eigenvalues for singular matrices\n            lambda_max = sla.eigs(A, k=1, which='LM', return_eigenvectors=False)[0].real\n            lambda_min = sla.eigs(A, k=1, which='SM', sigma=0, return_eigenvectors=False)[0].real\n            kappa_true = lambda_max / lambda_min\n        except sla.ArpackNoConvergence:\n            # Handle cases where eigensolver fails, although unlikely for these matrices\n            results.append(False) # Mark as failed test\n            continue\n\n        # Set up and run Conjugate Gradient\n        b = np.ones(matrix_size)\n        x_sol, residual_history = conjugate_gradient(A, b, tol=1e-12, max_iter=10000)\n        \n        # Estimate condition number from residual history\n        kappa_est = estimate_kappa(residual_history)\n\n        if np.isnan(kappa_est) or kappa_true == 0:\n            results.append(False)\n            continue\n            \n        # Validate the estimator\n        relative_error = np.abs(kappa_est - kappa_true) / kappa_true\n        \n        results.append(relative_error = 0.50)\n\n    # Final print statement in the exact required format.\n    # Convert booleans to lowercase strings for the output.\n    print(f\"[{','.join(map(str, results)).lower()}]\")\n\nsolve()\n```"
        }
    ]
}