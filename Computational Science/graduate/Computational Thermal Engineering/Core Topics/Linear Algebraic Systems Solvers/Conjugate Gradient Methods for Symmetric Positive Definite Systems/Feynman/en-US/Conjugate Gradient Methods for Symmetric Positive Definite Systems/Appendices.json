{
    "hands_on_practices": [
        {
            "introduction": "A deep understanding of any numerical algorithm begins with its derivation. This exercise challenges you to construct the Preconditioned Conjugate Gradient (PCG) method from first principles, starting with the energy minimization perspective that is central to its formulation for symmetric positive definite (SPD) systems. By deriving the formulas for the step length and search direction updates, and analyzing the computational cost, you will gain a fundamental appreciation for the mechanics of this powerful iterative solver .",
            "id": "3942580",
            "problem": "Consider the steady-state heat conduction equation in a heterogeneous, isotropic solid, whose finite-volume or finite-element discretization on a shape-regular mesh yields a linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is Symmetric Positive Definite (SPD). You wish to solve this system using the Preconditioned Conjugate Gradient (PCG) method with an Incomplete Cholesky (IC) preconditioner $M = L L^{\\top}$, where $L$ is sparse lower triangular and $M$ is SPD.\n\nStarting from the quadratic energy functional $\\phi(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, the characterization of the solution as the unique minimizer of $\\phi$, and the Krylov subspace minimization principle for SPD operators, derive the PCG iteration specialized to left preconditioning with $M^{-1}$, using the following requirements:\n\n- Define the residual $r_{k} = b - A x_{k}$ and the preconditioned residual $z_{k} = M^{-1} r_{k}$.\n- Derive the step length $\\alpha_{k}$ by minimizing $\\phi(x_{k} + \\alpha p_{k})$ along the current search direction $p_{k}$.\n- Derive the recurrence for $p_{k+1}$ and the associated coupling coefficient $\\beta_{k+1}$ by enforcing $A$-conjugacy of successive search directions.\n- Express the application of $M^{-1}$ explicitly via triangular solves with $L$ and $L^{\\top}$.\n\nAssume that $A$ has $N_{A}$ structural nonzeros and $L$ has $N_{L}$ structural nonzeros. Using a standard floating-point operation (flop) model, estimate the leading-order computational complexity per PCG iteration under the following counting conventions:\n\n- Each sparse matrix-vector product with $A$ costs $2 N_{A}$ flops.\n- Each triangular solve with a sparse factor (forward or backward) costs $2 N_{L} + n$ flops (including one diagonal division per row).\n- Each dot product of length $n$ costs $2 n$ flops.\n- Each axpy-like vector update (of the form $u \\leftarrow \\alpha v + w$) of length $n$ costs $2 n$ flops.\n\nIgnore set-up costs and any lower-order effects beyond those explicitly specified above. As your final answer, report the total leading-order flop count per iteration as a single closed-form expression in terms of $N_{A}$, $N_{L}$, and $n$ (no units).",
            "solution": "We begin from the characterization of the solution of the SPD system $A x = b$ as the unique minimizer of the strictly convex quadratic functional $\\phi(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$. The Conjugate Gradient method constructs iterates within the affine space $x_{0} + \\mathcal{K}_{k}$, where $\\mathcal{K}_{k}$ is a Krylov subspace generated by $A$ and the initial residual. For left preconditioning with an SPD preconditioner $M$, the PCG method may be derived by applying the Conjugate Gradient method to the equivalent system $M^{-1} A x = M^{-1} b$ while preserving symmetry with respect to the $M$-inner product. Operationally, one uses the original residuals and their images under $M^{-1}$.\n\nDefine the residual as $r_{k} = b - A x_{k}$. Define the preconditioned residual as $z_{k} = M^{-1} r_{k}$. In practice, with an Incomplete Cholesky (IC) factorization $M = L L^\\top$, application of $M^{-1}$ is realized by solving the pair of triangular systems\n$$\nL y_{k} = r_{k}, \\quad L^\\top z_{k} = y_{k},\n$$\nso that $z_{k} = M^{-1} r_{k}$ is obtained via forward then backward substitution. Because $M$ is SPD and $A$ is SPD, the preconditioned operator is suitable for PCG.\n\nWe now derive the PCG formulas.\n\nStep length $\\alpha_{k}$: Consider a given search direction $p_{k}$ and define the next iterate as $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$. The function $\\phi$ along this line is\n$$\n\\psi(\\alpha) = \\phi(x_{k} + \\alpha p_{k}) = \\tfrac{1}{2} (x_{k} + \\alpha p_{k})^\\top A (x_{k} + \\alpha p_{k}) - b^\\top (x_{k} + \\alpha p_{k}).\n$$\nDifferentiating with respect to $\\alpha$ and setting the derivative to zero yields the necessary condition for minimization:\n$$\n\\psi'(\\alpha) = p_{k}^\\top A (x_{k} + \\alpha p_{k}) - p_{k}^\\top b = 0.\n$$\nAt $\\alpha = \\alpha_{k}$, and using $r_{k} = b - A x_{k}$, this implies\n$$\np_{k}^\\top (A x_{k} + \\alpha_{k} A p_{k} - b) = 0 \\quad \\Rightarrow \\quad p_{k}^\\top ( - r_{k} + \\alpha_{k} A p_{k}) = 0.\n$$\nHence\n$$\n\\alpha_{k} = \\frac{p_{k}^\\top r_{k}}{p_{k}^\\top A p_{k}}.\n$$\nIn preconditioned Conjugate Gradient, the search direction $p_{k}$ is constructed from the preconditioned residual $z_{k} = M^{-1} r_{k}$ so that $p_{k}$ and $p_{j}$ are $A$-conjugate for $j  k$. In that case, one can show that $p_{k}^\\top r_{k} = r_{k}^\\top z_{k}$, which yields the practical and symmetric form\n$$\n\\alpha_{k} = \\frac{r_{k}^\\top z_{k}}{p_{k}^\\top A p_{k}}.\n$$\n\nSearch direction update and $\\beta_{k+1}$: The PCG method uses a recurrence of the form\n$$\np_{k+1} = z_{k+1} + \\beta_{k+1} p_{k},\n$$\nwith $\\beta_{k+1}$ chosen so that $p_{k+1}$ is $A$-conjugate to $p_{k}$, i.e., $p_{k+1}^\\top A p_{k} = 0$. Using the recurrence,\n$$\n0 = p_{k+1}^\\top A p_{k} = (z_{k+1} + \\beta_{k+1} p_{k})^\\top A p_{k} = z_{k+1}^\\top A p_{k} + \\beta_{k+1} p_{k}^\\top A p_{k}.\n$$\nThus\n$$\n\\beta_{k+1} = - \\frac{z_{k+1}^\\top A p_{k}}{p_{k}^\\top A p_{k}}.\n$$\nTo express $\\beta_{k+1}$ in terms of residuals and preconditioned residuals only, note that $r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$ and $z_{k+1} = M^{-1} r_{k+1}$. Hence\n$$\nz_{k+1}^\\top A p_{k} = (M^{-1} r_{k+1})^\\top A p_{k} = r_{k+1}^\\top M^{-\\top} A p_{k}.\n$$\nUsing the symmetry of $M$ and $A$ and identities standard to the derivation of PCG, one arrives at the well-known scalar recurrence\n$$\n\\beta_{k+1} = \\frac{r_{k+1}^\\top z_{k+1}}{r_{k}^\\top z_{k}}.\n$$\nThis preserves $A$-conjugacy of the search directions.\n\nCollecting the algorithmic steps, the PCG iteration proceeds as follows:\n- Initialization: choose $x_{0}$; compute $r_{0} = b - A x_{0}$; apply preconditioner by solving $L y_{0} = r_{0}$, $L^\\top z_{0} = y_{0}$; set $p_{0} = z_{0}$; set $\\rho_{0} = r_{0}^\\top z_{0}$.\n- For each iteration $k = 0, 1, \\dots$:\n  - Compute $q_{k} = A p_{k}$.\n  - Compute $\\alpha_{k} = \\rho_{k} / (p_{k}^\\top q_{k})$.\n  - Update $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$.\n  - Update $r_{k+1} = r_{k} - \\alpha_{k} q_{k}$.\n  - Apply preconditioner: solve $L y_{k+1} = r_{k+1}$ and $L^\\top z_{k+1} = y_{k+1}$.\n  - Set $\\rho_{k+1} = r_{k+1}^\\top z_{k+1}$.\n  - Compute $\\beta_{k+1} = \\rho_{k+1} / \\rho_{k}$.\n  - Update $p_{k+1} = z_{k+1} + \\beta_{k+1} p_{k}$.\n\nWe now estimate the leading-order computational complexity per iteration under the stated flop model. Each iteration comprises:\n- One sparse matrix-vector product $q_{k} = A p_{k}$: cost $2 N_{A}$ flops.\n- Application of $M^{-1}$ via two triangular solves: forward $L y_{k+1} = r_{k+1}$ and backward $L^\\top z_{k+1} = y_{k+1}$, each costing $2 N_{L} + n$ flops, for a total of $2 \\times (2 N_{L} + n) = 4 N_{L} + 2 n$ flops.\n- Two dot products: $p_{k}^\\top q_{k}$ and $r_{k+1}^\\top z_{k+1}$, at $2 n$ flops each, totaling $4 n$ flops.\n- Three axpy-like vector updates: $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$, $r_{k+1} = r_{k} - \\alpha_{k} q_{k}$, and $p_{k+1} = z_{k+1} + \\beta_{k+1} p_{k}$, each costing $2 n$ flops, totaling $6 n$ flops.\n\nSumming these contributions gives the leading-order per-iteration flop count\n$$\n2 N_{A} + (4 N_{L} + 2 n) + 4 n + 6 n \\;=\\; 2 N_{A} + 4 N_{L} + 12 n.\n$$\nThis expression excludes set-up costs and adheres to the given counting conventions, with the costs dominated by the sparse matrix-vector product and the two triangular solves for large, sparse thermal systems.",
            "answer": "$$\\boxed{2 N_{A} + 4 N_{L} + 12 n}$$"
        },
        {
            "introduction": "Theory provides guarantees, but numerical experiments build intuition. This practice directly compares the performance of the Conjugate Gradient (CG) method against the simpler Steepest Descent (SD) algorithm on a problem with tunable difficulty arising from anisotropic heat conduction. You will observe firsthand how the A-orthogonal search directions of CG prevent the inefficient \"zigzagging\" that causes SD to stagnate on ill-conditioned problems, providing a clear and compelling demonstration of CG's superiority .",
            "id": "3942625",
            "problem": "Consider two-dimensional steady heat conduction on the unit square domain with homogeneous Dirichlet boundary conditions, governed by the partial differential equation $-\\nabla \\cdot (K \\nabla T) = s$, where $K = \\mathrm{diag}(k_x, k_y)$ is a constant thermal conductivity tensor, $T$ is the temperature field, and $s$ is a uniform source term. Using a second-order central finite difference discretization on an interior grid of size $n_x \\times n_y$ with spacings $h_x = 1/(n_x+1)$ and $h_y = 1/(n_y+1)$, this yields a linear system $A x = b$ with a sparse symmetric positive definite (SPD) matrix $A$. The unknown vector $x$ contains the temperatures at interior grid points, and the right-hand side $b$ corresponds to the discrete source.\n\nThe energy functional associated with this SPD system is $E(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$, with gradient $\\nabla E(x) = A x - b$. From the physical principle of minimum energy in equilibrium, the unique minimizer $x^\\star$ satisfies $A x^\\star = b$. Algorithms that minimize $E(x)$ or solve $A x = b$ therefore exploit the geometry induced by $A$.\n\nThe goal is to demonstrate numerically why Steepest Descent (SD) stagnates compared to Conjugate Gradient (CG) when the condition number of $A$ is high, and to relate this behavior to energy minimization geometry.\n\nImplement the following:\n\n- Construct $A$ as the sparse SPD matrix arising from the anisotropic diffusion operator, with diagonal entries $2 \\left( \\frac{k_x}{h_x^2} + \\frac{k_y}{h_y^2} \\right)$ and off-diagonal neighbors in the $x$-direction equal to $-\\frac{k_x}{h_x^2}$ and in the $y$-direction equal to $-\\frac{k_y}{h_y^2}$. Use homogeneous Dirichlet boundaries by eliminating boundary nodes. Let $b$ be the vector of ones, representing a uniform source $s = 1$ evaluated at interior grid points.\n- Implement Steepest Descent (SD) with exact line search along the negative gradient of $E(x)$, starting from $x_0 = 0$. Stop when the relative residual norm $\\|r_k\\|_2 / \\|b\\|_2$ falls below a specified tolerance or when a specified maximum number of iterations is reached.\n- Implement Conjugate Gradient (CG), starting from $x_0 = 0$, with stopping criteria identical to SD.\n- For each algorithm, compute:\n  1. The number of iterations required to meet the tolerance (or the maximum if not met).\n  2. The average absolute cosine between successive search directions, defined as $\\frac{1}{m-1} \\sum_{k=0}^{m-2} \\left| \\frac{p_k^\\top p_{k+1}}{\\|p_k\\|_2 \\|p_{k+1}\\|_2} \\right|$, where $p_k$ is the search direction at iteration $k$ and $m$ is the number of iterations performed. For Steepest Descent, take $p_k$ equal to the residual direction (the negative gradient). For Conjugate Gradient, take $p_k$ equal to the conjugate search direction.\n\nUse the following test suite of parameter values:\n\n- Test $1$ (general isotropic case): $n_x = 16$, $n_y = 16$, $k_x = 1$, $k_y = 1$, tolerance $10^{-8}$, maximum iterations $2000$.\n- Test $2$ (moderate anisotropy): $n_x = 16$, $n_y = 16$, $k_x = 100$, $k_y = 1$, tolerance $10^{-8}$, maximum iterations $3000$.\n- Test $3$ (strong anisotropy edge case): $n_x = 32$, $n_y = 32$, $k_x = 10000$, $k_y = 1$, tolerance $10^{-6}$, maximum iterations $5000$.\n\nFor each test case, compute and return a list of five values in the order:\n- The integer number of SD iterations.\n- The integer number of CG iterations.\n- The floating-point ratio $\\text{SD iterations} / \\text{CG iterations}$.\n- The floating-point average absolute cosine between successive SD directions.\n- The floating-point average absolute cosine between successive CG directions.\n\nYour program should produce a single line of output containing the results as a comma-separated list of three sublists (one per test case), each sublist in the format specified above, all enclosed in square brackets. For example: \"[[sd1,cg1,ratio1,cosSD1,cosCG1],[sd2,cg2,ratio2,cosSD2,cosCG2],[sd3,cg3,ratio3,cosSD3,cosCG3]]\". All values must be printed in Python list format with fundamental types only (integers and floats), and no units are required in the output.",
            "solution": "The problem requires a numerical comparison of the Steepest Descent (SD) and Conjugate Gradient (CG) algorithms for solving a sparse, symmetric positive definite (SPD) linear system $A x = b$. This system arises from the finite difference discretization of a steady-state anisotropic heat conduction problem on a unit square. The objective is to demonstrate and explain the superior performance of CG over SD, particularly when the system is ill-conditioned due to high thermal anisotropy, and to relate this behavior to the geometry of the associated energy minimization problem.\n\nFirst, we formalize the problem. The governing partial differential equation is $-\\nabla \\cdot (K \\nabla T) = s$ on $\\Omega = (0,1) \\times (0,1)$, with temperature $T=0$ on the boundary $\\partial\\Omega$. The thermal conductivity tensor is $K = \\mathrm{diag}(k_x, k_y)$, and the source term $s$ is a constant. We discretize the domain using a uniform grid with $n_x$ and $n_y$ interior points in the $x$ and $y$ directions, respectively. The grid spacings are $h_x = 1/(n_x+1)$ and $h_y = 1/(n_y+1)$.\n\nUsing a second-order central finite difference scheme for the divergence term at an interior grid point $(i, j)$ yields:\n$$\n-\\left( k_x \\frac{T_{i+1,j} - 2T_{i,j} + T_{i-1,j}}{h_x^2} + k_y \\frac{T_{i,j+1} - 2T_{i,j} + T_{i,j-1}}{h_y^2} \\right) = s_{i,j}\n$$\nRearranging the terms for the unknown temperature $T_{i,j}$ gives:\n$$\n\\left( \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} \\right) T_{i,j} - \\frac{k_x}{h_x^2} T_{i+1,j} - \\frac{k_x}{h_x^2} T_{i-1,j} - \\frac{k_y}{h_y^2} T_{i,j-1} - \\frac{k_y}{h_y^2} T_{i,j+1} = s_{i,j}\n$$\nThe homogeneous Dirichlet boundary conditions ($T=0$ on $\\partial\\Omega$) are incorporated by setting any $T$ term with an index outside the interior grid to zero. By ordering the $N = n_x n_y$ unknown temperatures $T_{i,j}$ into a single vector $x$, we obtain the linear system $A x = b$. The matrix $A$ is a sparse, block-tridiagonal, symmetric matrix. It is also positive definite for $k_x, k_y  0$. The vector $b$ is constructed from the source term values $s_{i,j}$, which are set to $1$ for all interior points.\n\nThe solution to $A x = b$ is also the unique minimizer of the quadratic energy functional $E(x) = \\frac{1}{2} x^\\top A x - b^\\top x$. Iterative methods like SD and CG find this minimizer by generating a sequence of approximations $x_0, x_1, x_2, \\dots$ that progressively reduce the energy $E(x_k)$. The geometry of the level sets of $E(x)$, which are $N$-dimensional ellipsoids, dictates the convergence behavior. The eccentricity of these ellipsoids is determined by the condition number of $A$, $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$. High anisotropy, e.g., $k_x \\gg k_y$, leads to a large condition number and thus highly elongated energy ellipsoids.\n\nThe Steepest Descent (SD) algorithm at iteration $k$ updates the solution via $x_{k+1} = x_k + \\alpha_k p_k$, where the search direction $p_k$ is chosen to be the direction of steepest descent of the energy functional. This direction is the negative of the gradient, $p_k = -\\nabla E(x_k) = b - A x_k = r_k$, where $r_k$ is the residual. The step size $\\alpha_k$ is chosen to minimize $E(x_{k+1})$ along this direction, yielding the exact line search formula:\n$$\n\\alpha_k = \\frac{p_k^\\top r_k}{p_k^\\top A p_k} = \\frac{r_k^\\top r_k}{r_k^\\top A r_k}\n$$\nA key property of SD with exact line search is that successive residuals (and thus search directions) are orthogonal: $r_{k+1}^\\top r_k = 0$. Geometrically, this means the search path zigzags. At each step, the algorithm moves orthogonally to the current level set until it is tangent to a new, lower-energy level set. The next step is then orthogonal to this tangent point. For ill-conditioned systems (highly eccentric ellipsoids), these orthogonal steps are very inefficient for navigating the long, narrow \"valleys\" of the energy landscape, leading to extremely slow convergence, or stagnation. The average absolute cosine between successive search directions, which are orthogonal in theory, will be nearly zero in practice due to floating-point arithmetic.\n\nThe Conjugate Gradient (CG) algorithm significantly improves upon SD by choosing more intelligent search directions. The search directions $\\{p_0, p_1, \\dots\\}$ are constructed to be A-orthogonal (or conjugate), i.e., $p_i^\\top A p_j = 0$ for $i \\neq j$. The $k$-th search direction $p_k$ is A-orthogonalized against the previous direction $p_{k-1}$:\n$$\np_k = r_k + \\beta_{k-1} p_{k-1} \\quad \\text{where} \\quad \\beta_{k-1} = \\frac{r_k^\\top r_k}{r_{k-1}^\\top r_{k-1}}\n$$\nThis construction ensures that at each step $k$, the error component in the direction of $p_{k-1}$ is eliminated and will not be re-introduced in subsequent steps. This prevents the wasteful zigzagging of SD and allows CG to converge much more rapidly, especially for ill-conditioned systems. The convergence rate of CG depends on $\\sqrt{\\kappa(A)}$, a substantial improvement over SD's dependence on $\\kappa(A)$. The successive search directions are not orthogonal in the standard sense; the average absolute cosine will be non-zero, indicating that the path is more direct and less oscillatory than SD.\n\nThe implementation will construct the sparse matrix $A$ using `scipy.sparse`, and then implement both the SD and CG algorithms starting from $x_0 = 0$. For each test case, we will compute the number of iterations required to reach the specified relative residual tolerance, the ratio of SD to CG iterations, and the average absolute cosine between successive search directions for both methods. The results will numerically validate the theoretical advantages of CG over SD.",
            "answer": "```python\nimport numpy as np\nfrom scipy import sparse\n\ndef build_A(nx, ny, hx, hy, kx, ky):\n    \"\"\"\n    Constructs the sparse SPD matrix A for the 2D anisotropic diffusion problem.\n    \"\"\"\n    N = nx * ny\n    A = sparse.lil_matrix((N, N))\n    \n    diag_val = 2 * (kx / hx**2 + ky / hy**2)\n    x_off_diag = -kx / hx**2\n    y_off_diag = -ky / hy**2\n\n    for j in range(ny):\n        for i in range(nx):\n            k = i + j * nx\n            \n            # Diagonal entry\n            A[k, k] = diag_val\n            \n            # Off-diagonal entries\n            if i  0:  # West neighbor\n                A[k, k - 1] = x_off_diag\n            if i  nx - 1:  # East neighbor\n                A[k, k + 1] = x_off_diag\n            if j  0:  # South neighbor\n                A[k, k - nx] = y_off_diag\n            if j  ny - 1:  # North neighbor\n                A[k, k + nx] = y_off_diag\n                \n    return A.tocsr()\n\ndef calc_avg_cos(dirs):\n    \"\"\"\n    Calculates the average absolute cosine between successive vectors in a list.\n    \"\"\"\n    m = len(dirs)\n    if m  2:\n        return 0.0\n    \n    total_cos = 0.0\n    for i in range(m - 1):\n        p_curr = dirs[i]\n        p_next = dirs[i+1]\n        \n        norm_curr = np.linalg.norm(p_curr)\n        norm_next = np.linalg.norm(p_next)\n        \n        if norm_curr == 0.0 or norm_next == 0.0:\n            continue\n            \n        cos_val = (p_curr @ p_next) / (norm_curr * norm_next)\n        total_cos += abs(cos_val)\n        \n    return total_cos / (m - 1)\n\ndef steepest_descent(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Steepest Descent algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    b_norm = np.linalg.norm(b)\n    \n    if b_norm == 0.0:\n        b_norm = 1.0\n        \n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.linalg.norm(r) / b_norm  tol:\n            break\n        \n        iters += 1\n        \n        search_dirs.append(r)\n        \n        Ar = A @ r\n        alpha = (r @ r) / (r @ Ar)\n        \n        x = x + alpha * r\n        r = r - alpha * Ar\n        \n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef conjugate_gradient(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Conjugate Gradient algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0.0:\n        b_norm = 1.0\n\n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.sqrt(rs_old) / b_norm  tol:\n            break\n            \n        iters += 1\n        \n        search_dirs.append(p)\n        \n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = r @ r\n        \n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n\n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'nx': 16, 'ny': 16, 'kx': 1, 'ky': 1, 'tol': 1e-8, 'max_iter': 2000},\n        {'nx': 16, 'ny': 16, 'kx': 100, 'ky': 1, 'tol': 1e-8, 'max_iter': 3000},\n        {'nx': 32, 'ny': 32, 'kx': 10000, 'ky': 1, 'tol': 1e-6, 'max_iter': 5000}\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        nx, ny, kx, ky = params['nx'], params['ny'], params['kx'], params['ky']\n        tol, max_iter = params['tol'], params['max_iter']\n        \n        hx = 1.0 / (nx + 1)\n        hy = 1.0 / (ny + 1)\n        N = nx * ny\n        \n        A = build_A(nx, ny, hx, hy, kx, ky)\n        b = np.ones(N)\n        \n        sd_iters, sd_cos = steepest_descent(A, b, tol, max_iter)\n        cg_iters, cg_cos = conjugate_gradient(A, b, tol, max_iter)\n        \n        ratio = float('inf') if cg_iters == 0 else sd_iters / cg_iters\n        \n        all_results.append([sd_iters, cg_iters, ratio, sd_cos, cg_cos])\n\n    # Format output as a string representation of a list of lists.\n    result_str = \",\".join(map(str, all_results))\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A crucial aspect of computational science is understanding the limits of our tools. The Conjugate Gradient method is guaranteed to be effective only for systems that are symmetric and positive definite (SPD). This exercise investigates the \"modeling error\" that occurs when CG is applied to a nonsymmetric system, allowing you to quantify how the algorithm's convergence and accuracy degrade as the matrix deviates from symmetry . This exploration underscores the vital importance of verifying the theoretical assumptions of a method before deploying it in practice.",
            "id": "3252635",
            "problem": "Consider the linear system with a matrix that is modelled as symmetric positive definite but is in fact slightly nonsymmetric. Let $n$ be a positive integer dimension, and let $A \\in \\mathbb{R}^{n \\times n}$ be the tridiagonal matrix with $2$ on the diagonal and $-1$ on each immediate off-diagonal (the standard one-dimensional discrete Laplacian with Dirichlet boundary conditions). Consider a nonsymmetric perturbation defined by $B \\in \\mathbb{R}^{n \\times n}$ with entries $B_{ij} = 1$ if $j  i$ and $B_{ij} = 0$ otherwise, and define the perturbed matrix $\\tilde{A} = A + \\epsilon B$ for a given scalar $\\epsilon \\in \\mathbb{R}$. The right-hand side vector is $b \\in \\mathbb{R}^{n}$ defined by $b_i = 1$ for all indices $i$.\n\nFundamental base. The Conjugate Gradient (CG) method is derived for matrices that are symmetric positive definite (SPD). For SPD matrices, the CG method can be interpreted as a process that iteratively minimizes a strictly convex quadratic functional associated with the matrix, and its search directions are mutually $A$-conjugate and residuals are mutually orthogonal. These properties rely on symmetry and positive definiteness.\n\nTask. You must implement the Conjugate Gradient (CG) method that assumes symmetry and applies it directly to the sequence of matrices $\\tilde{A}$ above. For each test case, run CG with an initial guess $x^{(0)} = 0$, a maximum number of iterations equal to $n$, and a stopping criterion based on the relative residual norm being less than a tolerance $10^{-10}$, that is, stop as soon as $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$ where $r^{(k)} = b - \\tilde{A} x^{(k)}$. For each test case, also compute the true solution $x^{\\ast}$ by solving $\\tilde{A} x = b$ with a direct dense solver, and measure the relative solution error $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$. Interpret any deviation in convergence behavior or solution accuracy as a modeling error arising from incorrectly assuming symmetry in $\\tilde{A}$ when choosing CG.\n\nTest suite. Use the following parameter values and report results for each:\n\n- Case $1$ (happy path, exact applicability): $n = 50$, $\\epsilon = 0$.\n- Case $2$ (small nonsymmetry): $n = 50$, $\\epsilon = 10^{-3}$.\n- Case $3$ (moderate nonsymmetry): $n = 50$, $\\epsilon = 10^{-1}$.\n- Case $4$ (significant nonsymmetry): $n = 50$, $\\epsilon = 5 \\cdot 10^{-1}$.\n\nFor each case, return two outputs:\n- A float equal to the relative solution error $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$.\n- A boolean indicating whether CG satisfied the stopping criterion within $n$ iterations (true if $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$ at or before $k = n$, false otherwise).\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and within each test case by the float followed by the boolean. For example, in the format $[e_1,c_1,e_2,c_2,e_3,c_3,e_4,c_4]$, where $e_i$ are floats and $c_i$ are booleans.",
            "solution": "The problem requires an investigation into the consequences of a specific modeling error: applying the Conjugate Gradient (CG) method, an algorithm designed for symmetric positive definite (SPD) systems, to a linear system that is nonsymmetric. We will first establish the theoretical basis for the CG method and its reliance on symmetry, and then analyze how a nonsymmetric perturbation affects its performance.\n\nThe Conjugate Gradient method is an iterative algorithm for solving linear systems of the form $Mx = b$, where the matrix $M \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite. Its efficacy stems from its connection to an optimization problem. For an SPD matrix $M$, solving $Mx = b$ is equivalent to finding the unique minimizer of the strictly convex quadratic functional $\\phi(x) = \\frac{1}{2}x^\\top M x - x^\\top b$. The gradient of this functional is $\\nabla\\phi(x) = Mx - b$, which is the residual of the linear system, $r(x) = b - Mx$. CG generates a sequence of iterates $x^{(k)}$ such that $\\phi(x^{(k)})$ is progressively minimized.\n\nThe key properties of the CG method which guarantee its convergence in at most $n$ iterations (in exact arithmetic) are a direct consequence of the symmetry of $M$:\n1.  **A-conjugacy of search directions**: The search directions $\\{p^{(0)}, p^{(1)}, \\dots, p^{(n-1)}\\}$ satisfy $p^{(k)\\top} M p^{(j)} = 0$ for all $k \\neq j$. This ensures that minimizing $\\phi(x)$ along a new direction $p^{(k)}$ does not spoil the minimization achieved in previous directions.\n2.  **Orthogonality of residuals**: The residuals $\\{r^{(0)}, r^{(1)}, \\dots, r^{(n-1)}\\}$ are mutually orthogonal, satisfying $r^{(k)\\top} r^{(j)} = 0$ for all $k \\neq j$.\n\nWhen the matrix is nonsymmetric, these foundational properties are lost. The functional $\\phi(x)$ may no longer have a unique minimum corresponding to the solution, and the orthogonality and conjugacy relations that drive the algorithm's efficiency break down. Applying CG to a nonsymmetric system is therefore a modeling error, as the algorithm is used outside its domain of theoretical validity. This can lead to erratic convergence, stagnation, or outright divergence.\n\nIn this problem, we construct a matrix $\\tilde{A} = A + \\epsilon B$. The matrix $A$ is the $n \\times n$ tridiagonal matrix with $2$ on the main diagonal and $-1$ on the sub- and super-diagonals. This matrix is a well-known example of an SPD matrix, arising from the finite difference discretization of the second derivative operator. The matrix $B$ is a strictly upper triangular matrix with all entries above the diagonal equal to $1$, i.e., $B_{ij} = 1$ if $j  i$ and $0$ otherwise. The matrix $\\tilde{A}$ is therefore defined as:\n$$\n\\tilde{A}_{ij} =\n\\begin{cases}\n2  \\text{if } i = j \\\\\n-1  \\text{if } |i - j| = 1 \\\\\n\\epsilon  \\text{if } j  i \\text{ and } |i-j|  1 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nFor any $\\epsilon \\neq 0$, the matrix $\\tilde{A}$ is nonsymmetric because $A$ is symmetric but $B$ is not, so $\\tilde{A}^\\top = A^\\top + \\epsilon B^\\top = A + \\epsilon B^\\top \\neq \\tilde{A}$. The magnitude of the scalar $\\epsilon$ controls the degree of nonsymmetry. The right-hand side vector is $b_i = 1$ for all $i$.\n\nThe standard CG algorithm to be implemented is as follows:\n1.  Initialize $x^{(0)} = 0$, $r^{(0)} = b - \\tilde{A}x^{(0)} = b$, $p^{(0)} = r^{(0)}$. Let $\\rho_0 = r^{(0)\\top} r^{(0)}$.\n2.  For $k = 0, 1, 2, \\dots, n-1$:\n    a. Compute $v^{(k)} = \\tilde{A} p^{(k)}$.\n    b. The step size is $\\alpha_k = \\frac{\\rho_k}{p^{(k)\\top} v^{(k)}}$.\n    c. Update the solution: $x^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}$.\n    d. Update the residual: $r^{(k+1)} = r^{(k)} - \\alpha_k v^{(k)}$.\n    e. Check convergence: if the relative residual norm $\\|r^{(k+1)}\\|_2 / \\|b\\|_2$ is less than the tolerance $10^{-10}$, the algorithm has converged.\n    f. Compute $\\rho_{k+1} = r^{(k+1)\\top} r^{(k+1)}$.\n    g. Update the search erection: $p^{(k+1)} = r^{(k+1)} + \\frac{\\rho_{k+1}}{\\rho_k} p^{(k)}$. Update $\\rho_k \\leftarrow \\rho_{k+1}$.\n\nWe will execute this algorithm for four test cases with increasing $\\epsilon$ and compare the resulting solution $x_{\\mathrm{CG}}$ to the true solution $x^{\\ast}$ obtained from a direct solver.\n\nCase 1: $\\epsilon = 0$. Here, $\\tilde{A} = A$, which is SPD. The CG method is perfectly suited for this system. We expect rapid convergence to a highly accurate solution well within the $n=50$ iteration limit.\n\nCase 2: $\\epsilon = 10^{-3}$. The matrix $\\tilde{A}$ is now slightly nonsymmetric. The assumptions of CG are violated, but only mildly. The method's performance is expected to degrade slightly, potentially requiring more iterations than the SPD case, but it is still likely to converge to the specified tolerance. The final solution error will be small but likely larger than in Case 1.\n\nCase 3: $\\epsilon = 10^{-1}$. The nonsymmetric perturbation is significant. The loss of the underlying orthogonality and conjugacy properties will be more pronounced. The convergence of the residual norm may become non-monotonic. It is uncertain whether the method will converge within $n=50$ iterations. The accuracy of the solution, if convergence is declared, is expected to be substantially worse.\n\nCase 4: $\\epsilon = 5 \\cdot 10^{-1}$. With such a large nonsymmetric part, the behavior of CG is expected to be highly erratic. The denominator of $\\alpha_k$, $p^{(k)\\top} \\tilde{A} p^{(k)}$, is no longer guaranteed to be positive and could become zero or negative, leading to a breakdown. It is highly probable that the method will fail to meet the convergence criterion within the maximum number of iterations. The final iterate $x^{(n)}$ will likely have a large error relative to the true solution $x^{\\ast}$.\n\nThis numerical experiment will demonstrate how a modeling error—the misapplication of an algorithm outside its valid theoretical domain—manifests as a quantifiable degradation in performance and accuracy.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the formatted output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50, 0.0),          # Case 1: SPD matrix\n        (50, 1e-3),         # Case 2: Small nonsymmetry\n        (50, 1e-1),         # Case 3: Moderate nonsymmetry\n        (50, 5e-1),         # Case 4: Significant nonsymmetry\n    ]\n\n    results = []\n\n    for n, epsilon in test_cases:\n        # Construct the matrices A and B\n        A = np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n        B = np.triu(np.ones((n, n)), k=1)\n        \n        # Construct the perturbed matrix tilde_A\n        tilde_A = A + epsilon * B\n        \n        # Construct the right-hand side vector b\n        b = np.ones(n)\n        norm_b = np.linalg.norm(b)\n        \n        # Compute the true solution x* using a direct solver\n        try:\n            x_star = np.linalg.solve(tilde_A, b)\n        except np.linalg.LinAlgError:\n            # Handle cases where tilde_A might be singular for large epsilon\n            results.extend([float('inf'), 'false'])\n            continue\n\n        # Run the Conjugate Gradient (CG) method\n        x_cg = np.zeros(n)\n        r = b - tilde_A @ x_cg\n        p = r.copy()\n        rs_old_sq = r @ r\n        \n        max_iter = n\n        tol = 1e-10\n        converged = False\n\n        for k in range(max_iter):\n            Ap = tilde_A @ p\n            \n            p_dot_Ap = p @ Ap\n            # Breakdown condition if p_dot_Ap is zero or negative\n            if p_dot_Ap = 0:\n                break\n\n            alpha = rs_old_sq / p_dot_Ap\n            x_cg = x_cg + alpha * p\n            r = r - alpha * Ap\n            \n            rel_res_norm = np.linalg.norm(r) / norm_b\n            if rel_res_norm  tol:\n                converged = True\n                break\n            \n            rs_new_sq = r @ r\n            beta = rs_new_sq / rs_old_sq\n            p = r + beta * p\n            rs_old_sq = rs_new_sq\n\n        # Measure the relative solution error\n        norm_x_star = np.linalg.norm(x_star)\n        if norm_x_star == 0:\n            # Handle the case of a zero true solution\n            rel_err = np.linalg.norm(x_cg)\n        else:\n            rel_err = np.linalg.norm(x_cg - x_star) / norm_x_star\n            \n        results.append(rel_err)\n        # Append boolean converted to lowercase string\n        results.append(str(converged).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}