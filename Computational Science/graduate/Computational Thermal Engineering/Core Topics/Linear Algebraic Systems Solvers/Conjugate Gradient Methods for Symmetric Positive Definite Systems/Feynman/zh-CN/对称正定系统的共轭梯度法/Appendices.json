{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在让您切身体会到，为何共轭梯度法优于更简单的最速下降法，尤其是在求解热仿真中常见的病态问题时。通过数值模拟比较这两种方法在各向异性热传导问题上的性能 ，您将观察到共轭梯度法如何通过其巧妙的搜索方向选择，避免了困扰最速下降法的“锯齿形”停滞现象。",
            "id": "3942625",
            "problem": "考虑单位正方形域上的二维稳态热传导问题，其边界条件为齐次狄利克雷边界条件，由偏微分方程 $-\\nabla \\cdot (K \\nabla T) = s$ 控制。其中，$K = \\mathrm{diag}(k_x, k_y)$ 是一个常数导热张量，$T$ 是温度场，$s$ 是一个均匀源项。在尺寸为 $n_x \\times n_y$、网格间距为 $h_x = 1/(n_x+1)$ 和 $h_y = 1/(n_y+1)$ 的内部网格上，使用二阶中心有限差分格式进行离散化，可得到一个线性系统 $A x = b$，其中 $A$ 是一个稀疏对称正定 (SPD) 矩阵。未知向量 $x$ 包含内部网格点上的温度，右端项 $b$ 对应于离散源。\n\n与此 SPD 系统相关的能量泛函为 $E(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$，其梯度为 $\\nabla E(x) = A x - b$。根据平衡状态下的最小能量物理原理，唯一的最小化子 $x^\\star$ 满足 $A x^\\star = b$。因此，最小化 $E(x)$ 或求解 $A x = b$ 的算法利用了由 $A$ 诱导的几何结构。\n\n本题旨在数值地展示，当矩阵 $A$ 的条件数较高时，为何最速下降法 (SD) 会相比共轭梯度法 (CG) 出现停滞，并将此行为与能量最小化的几何结构联系起来。\n\n实现以下内容：\n\n- 将 $A$ 构造为由各向异性扩散算子产生的稀疏 SPD 矩阵，其对角线元素为 $2 \\left( \\frac{k_x}{h_x^2} + \\frac{k_y}{h_y^2} \\right)$，x 方向上的非对角线相邻元素为 $-\\frac{k_x}{h_x^2}$，y 方向上的为 $-\\frac{k_y}{h_y^2}$。通过消除边界节点来使用齐次狄利克雷边界。令 $b$ 为全 1 向量，表示在内部网格点上取值为 $s = 1$ 的均匀源。\n- 实现最速下降法 (SD)，从 $x_0 = 0$ 开始，沿着 $E(x)$ 的负梯度方向进行精确线搜索。当相对残差范数 $\\|r_k\\|_2 / \\|b\\|_2$ 低于指定的容差，或达到指定的最大迭代次数时停止。\n- 实现共轭梯度法 (CG)，从 $x_0 = 0$ 开始，其停止准则与 SD 相同。\n- 对每种算法，计算：\n  1. 达到容差所需的迭代次数（如果未达到则为最大迭代次数）。\n  2. 连续搜索方向之间的平均绝对余弦值，定义为 $\\frac{1}{m-1} \\sum_{k=0}^{m-2} \\left| \\frac{p_k^\\top p_{k+1}}{\\|p_k\\|_2 \\|p_{k+1}\\|_2} \\right|$，其中 $p_k$ 是第 $k$ 次迭代的搜索方向，$m$ 是执行的迭代次数。对于最速下降法，取 $p_k$ 等于残差方向（负梯度）。对于共轭梯度法，取 $p_k$ 等于共轭搜索方向。\n\n使用以下参数值测试套件：\n\n- 测试 $1$ (一般各向同性情况): $n_x = 16$，$n_y = 16$，$k_x = 1$，$k_y = 1$，容差 $10^{-8}$，最大迭代次数 $2000$。\n- 测试 $2$ (中等各向异性情况): $n_x = 16$，$n_y = 16$，$k_x = 100$，$k_y = 1$，容差 $10^{-8}$，最大迭代次数 $3000$。\n- 测试 $3$ (强各向异性边缘情况): $n_x = 32$，$n_y = 32$，$k_x = 10000$，$k_y = 1$，容差 $10^{-6}$，最大迭代次数 $5000$。\n\n对于每个测试用例，按顺序计算并返回包含五个值的列表：\n- SD 迭代的整数次数。\n- CG 迭代的整数次数。\n- 浮点数比率 $\\text{SD 迭代次数} / \\text{CG 迭代次数}$。\n- 连续 SD 方向之间的浮点数平均绝对余弦值。\n- 连续 CG 方向之间的浮点数平均绝对余弦值。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表包含三个子列表（每个测试用例一个），每个子列表的格式如上所述，所有内容都包含在方括号中。例如：\"[[sd1,cg1,ratio1,cosSD1,cosCG1],[sd2,cg2,ratio2,cosSD2,cosCG2],[sd3,cg3,ratio3,cosSD3,cosCG3]]\"。所有值必须以 Python 列表格式打印，且仅包含基本类型（整数和浮点数），输出中不需要单位。",
            "solution": "该问题要求对求解稀疏对称正定 (SPD) 线性系统 $A x = b$ 的最速下降法 (SD) 和共轭梯度法 (CG) 进行数值比较。该系统源于单位正方形上稳态各向异性热传导问题的有限差分格式离散化。目标是展示并解释 CG 相较于 SD 的优越性能，尤其是在系统因高热各向异性而变得病态时，并将此行为与相关的能量最小化问题的几何结构联系起来。\n\n首先，我们将问题形式化。控制偏微分方程为 $-\\nabla \\cdot (K \\nabla T) = s$，定义在域 $\\Omega = (0,1) \\times (0,1)$ 上，边界 $\\partial\\Omega$ 上的温度为 $T=0$。导热张量为 $K = \\mathrm{diag}(k_x, k_y)$，源项 $s$ 为一个常数。我们使用均匀网格对域进行离散化，该网格在 x 和 y 方向上分别有 $n_x$ 和 $n_y$ 个内部点。网格间距为 $h_x = 1/(n_x+1)$ 和 $h_y = 1/(n_y+1)$。\n\n在内部网格点 $(i, j)$ 处，对散度项使用二阶中心有限差分格式可得：\n$$\n-\\left( k_x \\frac{T_{i+1,j} - 2T_{i,j} + T_{i-1,j}}{h_x^2} + k_y \\frac{T_{i,j+1} - 2T_{i,j} + T_{i,j-1}}{h_y^2} \\right) = s_{i,j}\n$$\n为未知温度 $T_{i,j}$ 重新排列各项可得：\n$$\n\\left( \\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2} \\right) T_{i,j} - \\frac{k_x}{h_x^2} T_{i+1,j} - \\frac{k_x}{h_x^2} T_{i-1,j} - \\frac{k_y}{h_y^2} T_{i,j-1} - \\frac{k_y}{h_y^2} T_{i,j+1} = s_{i,j}\n$$\n通过将索引在内部网格之外的任何 $T$ 项设置为零，来引入齐次狄利克雷边界条件（在 $\\partial\\Omega$ 上 $T=0$）。通过将 $N = n_x n_y$ 个未知温度 $T_{i,j}$ 排列成一个向量 $x$，我们得到了线性系统 $A x = b$。矩阵 $A$ 是一个稀疏、块三对角、对称的矩阵。当 $k_x, k_y > 0$ 时，它也是正定的。向量 $b$ 由源项值 $s_{i,j}$ 构造而成，对于所有内部点，$s_{i,j}$ 均设为 1。\n\n$A x = b$ 的解也是二次能量泛函 $E(x) = \\frac{1}{2} x^\\top A x - b^\\top x$ 的唯一最小化子。像 SD 和 CG 这样的迭代方法通过生成一系列近似解 $x_0, x_1, x_2, \\dots$ 来找到这个最小化子，这些近似解会逐步降低能量 $E(x_k)$。$E(x)$ 的水平集是 $N$ 维椭球，其几何形状决定了收敛行为。这些椭球的偏心率由 $A$ 的条件数 $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$ 决定。高各向异性（例如 $k_x \\gg k_y$）会导致大的条件数，从而产生高度拉长的能量椭球。\n\n最速下降法 (SD) 在第 $k$ 次迭代时通过 $x_{k+1} = x_k + \\alpha_k p_k$ 更新解，其中搜索方向 $p_k$ 被选为能量泛函的最速下降方向。这个方向是梯度的负方向，即 $p_k = -\\nabla E(x_k) = b - A x_k = r_k$，其中 $r_k$ 是残差。步长 $\\alpha_k$ 的选择是为了沿此方向最小化 $E(x_{k+1})$，从而得到精确线搜索公式：\n$$\n\\alpha_k = \\frac{p_k^\\top r_k}{p_k^\\top A p_k} = \\frac{r_k^\\top r_k}{r_k^\\top A r_k}\n$$\n使用精确线搜索的 SD 的一个关键特性是连续的残差（也就是搜索方向）是正交的：$r_{k+1}^\\top r_k = 0$。从几何上看，这意味着搜索路径呈锯齿形。在每一步中，算法都沿着当前水平集的正交方向移动，直到与一个新的、能量更低的水平集相切。然后，下一步又与该切点正交。对于病态系统（高度偏心的椭球），这些正交步骤在能量景观的狭长“山谷”中导航效率非常低，导致收敛极其缓慢，甚至停滞。理论上正交的连续搜索方向之间的平均绝对余弦值，在实践中由于浮点运算将接近于零。\n\n共轭梯度法 (CG) 通过选择更智能的搜索方向，显著改进了 SD。搜索方向 $\\{p_0, p_1, \\dots\\}$ 被构造成 A-正交（或共轭）的，即当 $i \\neq j$ 时，$p_i^\\top A p_j = 0$。第 $k$ 个搜索方向 $p_k$ 是通过对前一个方向 $p_{k-1}$ 进行 A-正交化得到的：\n$$\np_k = r_k + \\beta_{k-1} p_{k-1} \\quad \\text{其中} \\quad \\beta_{k-1} = \\frac{r_k^\\top r_k}{r_{k-1}^\\top r_{k-1}}\n$$\n这种构造确保了在每一步 $k$ 中，沿 $p_{k-1}$ 方向的误差分量被消除，并且不会在后续步骤中重新引入。这避免了 SD 的浪费性锯齿形路径，并使 CG 能够更快地收敛，尤其对于病态系统。CG 的收敛速度取决于 $\\sqrt{\\kappa(A)}$，这比 SD 对 $\\kappa(A)$ 的依赖性有了实质性的改进。连续的搜索方向在标准意义上不是正交的；其平均绝对余弦值将不为零，这表明其路径比 SD 更直接，振荡更少。\n\n该实现将使用 `scipy.sparse` 构造稀疏矩阵 $A$，然后从 $x_0 = 0$ 开始实现 SD 和 CG 算法。对于每个测试用例，我们将计算达到指定的相对残差容差所需的迭代次数、SD 与 CG 迭代次数的比率，以及两种方法连续搜索方向之间的平均绝对余弦值。这些结果将数值地验证 CG 相对于 SD 的理论优势。",
            "answer": "```python\nimport numpy as np\nfrom scipy import sparse\n\ndef build_A(nx, ny, hx, hy, kx, ky):\n    \"\"\"\n    Constructs the sparse SPD matrix A for the 2D anisotropic diffusion problem.\n    \"\"\"\n    N = nx * ny\n    A = sparse.lil_matrix((N, N))\n    \n    diag_val = 2 * (kx / hx**2 + ky / hy**2)\n    x_off_diag = -kx / hx**2\n    y_off_diag = -ky / hy**2\n\n    for j in range(ny):\n        for i in range(nx):\n            k = i + j * nx\n            \n            # Diagonal entry\n            A[k, k] = diag_val\n            \n            # Off-diagonal entries\n            if i > 0:  # West neighbor\n                A[k, k - 1] = x_off_diag\n            if i  nx - 1:  # East neighbor\n                A[k, k + 1] = x_off_diag\n            if j > 0:  # South neighbor\n                A[k, k - nx] = y_off_diag\n            if j  ny - 1:  # North neighbor\n                A[k, k + nx] = y_off_diag\n                \n    return A.tocsr()\n\ndef calc_avg_cos(dirs):\n    \"\"\"\n    Calculates the average absolute cosine between successive vectors in a list.\n    \"\"\"\n    m = len(dirs)\n    if m  2:\n        return 0.0\n    \n    total_cos = 0.0\n    for i in range(m - 1):\n        p_curr = dirs[i]\n        p_next = dirs[i+1]\n        \n        norm_curr = np.linalg.norm(p_curr)\n        norm_next = np.linalg.norm(p_next)\n        \n        if norm_curr == 0.0 or norm_next == 0.0:\n            continue\n            \n        cos_val = (p_curr @ p_next) / (norm_curr * norm_next)\n        total_cos += abs(cos_val)\n        \n    return total_cos / (m - 1)\n\ndef steepest_descent(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Steepest Descent algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    b_norm = np.linalg.norm(b)\n    \n    if b_norm == 0.0:\n        b_norm = 1.0\n        \n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.linalg.norm(r) / b_norm  tol:\n            break\n        \n        iters += 1\n        \n        search_dirs.append(r)\n        \n        Ar = A @ r\n        alpha = (r @ r) / (r @ Ar)\n        \n        x = x + alpha * r\n        r = r - alpha * Ar\n        \n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef conjugate_gradient(A, b, tol, max_iter):\n    \"\"\"\n    Implements the Conjugate Gradient algorithm.\n    \"\"\"\n    x = np.zeros(A.shape[0])\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0.0:\n        b_norm = 1.0\n\n    search_dirs = []\n    \n    iters = 0\n    for k in range(max_iter):\n        if np.sqrt(rs_old) / b_norm  tol:\n            break\n            \n        iters += 1\n        \n        search_dirs.append(p)\n        \n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = r @ r\n        \n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n\n    avg_cos = calc_avg_cos(search_dirs)\n    return iters, avg_cos\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'nx': 16, 'ny': 16, 'kx': 1, 'ky': 1, 'tol': 1e-8, 'max_iter': 2000},\n        {'nx': 16, 'ny': 16, 'kx': 100, 'ky': 1, 'tol': 1e-8, 'max_iter': 3000},\n        {'nx': 32, 'ny': 32, 'kx': 10000, 'ky': 1, 'tol': 1e-6, 'max_iter': 5000}\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        nx, ny, kx, ky = params['nx'], params['ny'], params['kx'], params['ky']\n        tol, max_iter = params['tol'], params['max_iter']\n        \n        hx = 1.0 / (nx + 1)\n        hy = 1.0 / (ny + 1)\n        N = nx * ny\n        \n        A = build_A(nx, ny, hx, hy, kx, ky)\n        b = np.ones(N)\n        \n        sd_iters, sd_cos = steepest_descent(A, b, tol, max_iter)\n        cg_iters, cg_cos = conjugate_gradient(A, b, tol, max_iter)\n        \n        ratio = float('inf') if cg_iters == 0 else sd_iters / cg_iters\n        \n        all_results.append([sd_iters, cg_iters, ratio, sd_cos, cg_cos])\n\n    # Format output as a string representation of a list of lists.\n    result_str = \",\".join(map(str, all_results))\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "共轭梯度法的高效性建立在系统矩阵对称正定（SPD）这一基本假设之上。本练习将通过一个“建模误差”的例子，即故意将共轭梯度法应用于一个轻微非对称的系统，来揭示该假设的至关重要性 。您将观察到随着非对称性的增加，算法的收敛行为和求解精度会如何退化，从而学习到正确应用数值方法时的关键考量。",
            "id": "3252635",
            "problem": "考虑一个线性系统，其矩阵被建模为对称正定，但实际上略有非对称性。设 $n$ 为一个正整数维度，设 $A \\in \\mathbb{R}^{n \\times n}$ 为一个三对角矩阵，其对角线上元素为 $2$，紧邻对角线的元素为 $-1$（即标准的一维离散拉普拉斯算子，带狄利克雷边界条件）。考虑一个由 $B \\in \\mathbb{R}^{n \\times n}$ 定义的非对称扰动，其元素为 $B_{ij} = 1$（如果 $j  i$）和 $B_{ij} = 0$（其他情况），并对于一个给定的标量 $\\epsilon \\in \\mathbb{R}$，定义扰动矩阵 $\\tilde{A} = A + \\epsilon B$。右端向量 $b \\in \\mathbb{R}^{n}$ 定义为 $b_i = 1$（对于所有索引 $i$）。\n\n基本原理。共轭梯度（CG）法是为对称正定（SPD）矩阵设计的。对于SPD矩阵，CG法可以被解释为一个迭代最小化与矩阵相关的严格凸二次泛函的过程，其搜索方向是相互 $A$-共轭的，残差是相互正交的。这些性质依赖于对称性和正定性。\n\n任务。你必须实现一个假设对称性的共轭梯度（CG）法，并将其直接应用于上述矩阵序列 $\\tilde{A}$。对于每个测试用例，使用初始猜测值 $x^{(0)} = 0$、最大迭代次数等于 $n$ 来运行CG，并使用基于相对残差范数小于容差 $10^{-10}$ 的停止准则，即一旦 $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$ 就停止，其中 $r^{(k)} = b - \\tilde{A} x^{(k)}$。对于每个测试用例，还需通过使用直接稠密求解器求解 $\\tilde{A} x = b$ 来计算精确解 $x^{\\ast}$，并测量相对解误差 $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$。将收敛行为或解精度的任何偏差解释为在选择CG时错误地假设 $\\tilde{A}$ 对称性而产生的建模误差。\n\n测试套件。使用以下参数值并报告每个参数值的结果：\n\n-   案例 $1$（理想情况，精确适用）：$n = 50$，$\\epsilon = 0$。\n-   案例 $2$（小非对称性）：$n = 50$，$\\epsilon = 10^{-3}$。\n-   案例 $3$（中等非对称性）：$n = 50$，$\\epsilon = 10^{-1}$。\n-   案例 $4$（显著非对称性）：$n = 50$，$\\epsilon = 5 \\cdot 10^{-1}$。\n\n对于每个案例，返回两个输出：\n-   一个浮点数，等于相对解误差 $\\|x_{\\mathrm{CG}} - x^{\\ast}\\|_2 / \\|x^{\\ast}\\|_2$。\n-   一个布尔值，表示CG是否在 $n$ 次迭代内满足停止准则（如果在 $k = n$ 或之前 $\\|r^{(k)}\\|_2 / \\|b\\|_2  10^{-10}$ 则为true，否则为false）。\n\n最终输出格式。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按测试用例排序，并在每个测试用例内先是浮点数后是布尔值。例如，格式为 $[e_1,c_1,e_2,c_2,e_3,c_3,e_4,c_4]$，其中 $e_i$ 是浮点数，$c_i$ 是布尔值。",
            "solution": "本问题要求研究一个特定建模误差的后果：将为对称正定（SPD）系统设计的共轭梯度（CG）法应用于一个非对称的线性系统。我们将首先建立CG方法的理论基础及其对对称性的依赖，然后分析非对称扰动如何影响其性能。\n\n共轭梯度法是一种用于求解形如 $Mx = b$ 的线性系统的迭代算法，其中矩阵 $M \\in \\mathbb{R}^{n \\times n}$ 是对称且正定的。其有效性源于它与一个优化问题的联系。对于一个SPD矩阵 $M$，求解 $Mx = b$ 等价于找到严格凸二次泛函 $\\phi(x) = \\frac{1}{2}x^T M x - x^T b$ 的唯一最小化子。该泛函的梯度为 $\\nabla\\phi(x) = Mx - b$，即线性系统的残差 $r(x) = b - Mx$。CG生成一个迭代序列 $x^{(k)}$，使得 $\\phi(x^{(k)})$ 被逐步最小化。\n\n保证CG在精确算术下至多 $n$ 次迭代内收敛的关键性质是矩阵 $M$ 对称性的直接结果：\n1.  **搜索方向的A-共轭性**：搜索方向 $\\{p^{(0)}, p^{(1)}, \\dots, p^{(n-1)}\\}$ 满足 $p^{(k)T} M p^{(j)} = 0$（对于所有 $k \\neq j$）。这确保了沿新方向 $p^{(k)}$ 最小化 $\\phi(x)$ 不会破坏在先前方向上已达成的最小化。\n2.  **残差的正交性**：残差 $\\{r^{(0)}, r^{(1)}, \\dots, r^{(n-1)}\\}$ 相互正交，满足 $r^{(k)T} r^{(j)} = 0$（对于所有 $k \\neq j$）。\n\n当矩阵非对称时，这些基本性质就会丧失。泛函 $\\phi(x)$ 可能不再有与解对应的唯一最小值，并且驱动该算法效率的正交性和共轭关系也会被打破。因此，将CG应用于非对称系统是一种建模误差，因为该算法被用于其理论有效性范围之外。这可能导致不稳定的收敛、停滞或完全发散。\n\n在本问题中，我们构建一个矩阵 $\\tilde{A} = A + \\epsilon B$。矩阵 $A$ 是一个 $n \\times n$ 的三对角矩阵，主对角线上的元素为 $2$，次对角线和超对角线上的元素为 $-1$。该矩阵是SPD矩阵的一个著名例子，源于二阶导数算子的有限差分离散。矩阵 $B$ 是一个严格上三角矩阵，其对角线上方的所有元素均为 $1$，即 $B_{ij} = 1$（若 $j  i$）否则为 $0$。因此，矩阵 $\\tilde{A}$ 定义为：\n$$\n\\tilde{A}_{ij} =\n\\begin{cases}\n2  \\text{if } i = j \\\\\n-1  \\text{if } |i - j| = 1 \\\\\n\\epsilon  \\text{if } j  i \\text{ and } |i-j|  1 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\n对于任何 $\\epsilon \\neq 0$，矩阵 $\\tilde{A}$ 是非对称的，因为 $A$ 是对称的但 $B$ 不是，所以 $\\tilde{A}^T = A^T + \\epsilon B^T = A + \\epsilon B^T \\neq \\tilde{A}$。标量 $\\epsilon$ 的大小控制着非对称的程度。右端向量是 $b_i = 1$（对于所有 $i$）。\n\n待实现的标准CG算法如下：\n1.  初始化 $x^{(0)} = 0$, $r^{(0)} = b - \\tilde{A}x^{(0)} = b$, $p^{(0)} = r^{(0)}$。令 $\\rho_0 = r^{(0)T} r^{(0)}$。\n2.  对于 $k = 0, 1, 2, \\dots, n-1$：\n    a. 计算 $v^{(k)} = \\tilde{A} p^{(k)}$。\n    b. 步长为 $\\alpha_k = \\frac{\\rho_k}{p^{(k)T} v^{(k)}}$。\n    c. 更新解：$x^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}$。\n    d. 更新残差：$r^{(k+1)} = r^{(k)} - \\alpha_k v^{(k)}$。\n    e. 检查收敛性：如果相对残差范数 $\\|r^{(k+1)}\\|_2 / \\|b\\|_2$ 小于容差 $10^{-10}$，则算法已收敛。\n    f. 计算 $\\rho_{k+1} = r^{(k+1)T} r^{(k+1)}$。\n    g. 更新搜索方向：$p^{(k+1)} = r^{(k+1)} + \\frac{\\rho_{k+1}}{\\rho_k} p^{(k)}$。更新 $\\rho_k \\leftarrow \\rho_{k+1}$。\n\n我们将对四个 $\\epsilon$ 递增的测试用例执行此算法，并将得到的解 $x_{\\mathrm{CG}}$ 与通过直接求解器获得的精确解 $x^{\\ast}$ 进行比较。\n\n案例 1：$\\epsilon = 0$。此时，$\\tilde{A} = A$，是SPD矩阵。CG方法完全适用于该系统。我们期望其能快速收敛至一个高精度的解，且远在 $n=50$ 次迭代限制内。\n\n案例 2：$\\epsilon = 10^{-3}$。矩阵 $\\tilde{A}$ 现在是轻微非对称的。CG的假设被违反，但程度很轻。该方法的性能预计会略有下降，可能比SPD情况需要更多的迭代次数，但仍很可能收敛到指定的容差。最终解的误差会很小，但可能比案例1大。\n\n案例 3：$\\epsilon = 10^{-1}$。非对称扰动是显著的。潜在的正交性和共轭性质的丧失将更为明显。残差范数的收敛可能变得非单调。该方法是否能在 $n=50$ 次迭代内收敛是不确定的。如果宣告收敛，解的精度预计会差很多。\n\n案例 4：$\\epsilon = 5 \\cdot 10^{-1}$。由于非对称部分如此之大，CG的行为预计将非常不稳定。$\\alpha_k$ 的分母 $p^{(k)T} \\tilde{A} p^{(k)}$ 不再保证为正，并可能变为零或负，导致算法崩溃。该方法很可能无法在最大迭代次数内满足收敛准则。最终的迭代结果 $x^{(n)}$ 相对于精确解 $x^{\\ast}$ 很可能会有很大的误差。\n\n这个数值实验将展示一个建模误差——即在其有效的理论领域之外误用算法——如何体现为可量化的性能和精度下降。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the formatted output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50, 0.0),          # Case 1: SPD matrix\n        (50, 1e-3),         # Case 2: Small nonsymmetry\n        (50, 1e-1),         # Case 3: Moderate nonsymmetry\n        (50, 5e-1),         # Case 4: Significant nonsymmetry\n    ]\n\n    results = []\n\n    for n, epsilon in test_cases:\n        # Construct the matrices A and B\n        A = np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n        B = np.triu(np.ones((n, n)), k=1)\n        \n        # Construct the perturbed matrix tilde_A\n        tilde_A = A + epsilon * B\n        \n        # Construct the right-hand side vector b\n        b = np.ones(n)\n        norm_b = np.linalg.norm(b)\n        \n        # Compute the true solution x* using a direct solver\n        try:\n            x_star = np.linalg.solve(tilde_A, b)\n        except np.linalg.LinAlgError:\n            # Handle cases where tilde_A might be singular for large epsilon\n            results.extend([float('inf'), 'false'])\n            continue\n\n        # Run the Conjugate Gradient (CG) method\n        x_cg = np.zeros(n)\n        r = b - tilde_A @ x_cg\n        p = r.copy()\n        rs_old_sq = r @ r\n        \n        max_iter = n\n        tol = 1e-10\n        converged = False\n\n        for k in range(max_iter):\n            Ap = tilde_A @ p\n            \n            p_dot_Ap = p @ Ap\n            # Breakdown condition if p_dot_Ap is zero or negative\n            if p_dot_Ap = 0:\n                break\n\n            alpha = rs_old_sq / p_dot_Ap\n            x_cg = x_cg + alpha * p\n            r = r - alpha * Ap\n            \n            rel_res_norm = np.linalg.norm(r) / norm_b\n            if rel_res_norm  tol:\n                converged = True\n                break\n            \n            rs_new_sq = r @ r\n            beta = rs_new_sq / rs_old_sq\n            p = r + beta * p\n            rs_old_sq = rs_new_sq\n\n        # Measure the relative solution error\n        norm_x_star = np.linalg.norm(x_star)\n        if norm_x_star == 0:\n            # Handle the case of a zero true solution\n            rel_err = np.linalg.norm(x_cg)\n        else:\n            rel_err = np.linalg.norm(x_cg - x_star) / norm_x_star\n            \n        results.append(rel_err)\n        # Append boolean converted to lowercase string\n        results.append(str(converged).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "共轭梯度法不仅是一个强大的求解器，也可以作为一个诊断工具。本练习将指导您设计一个实验，仅利用共轭梯度法运行过程中的收敛历史，来估算矩阵的谱条件数 $\\kappa(A)$ 。这个实践将加深您对算法性能与其所模拟的物理系统属性之间内在联系的理解。",
            "id": "3942614",
            "problem": "考虑一个有界域上的稳态热传导方程，其带有齐次狄利克雷边界条件，经过二阶中心有限差分格式离散化后，产生一个形式为 $A x = b$ 的线性系统，其中 $A$ 是对称正定（Symmetric Positive Definite, SPD）矩阵。应用于此类系统的共轭梯度（Conjugate Gradient, CG）法会产生一个残差序列 $\\{r_k\\}$，其中 $r_k = b - A x_k$。请设计一个实验，直接从记录的 CG 残差历史记录中估计谱条件数 $\\kappa(A)$，而无需访问 $A$ 的特征值，并通过经验验证该估计量。\n\n您的程序必须实现以下步骤：\n\n1. 构造模拟热传导离散算子的 SPD 矩阵 $A$：\n   a) 在区间 $[0,1]$ 上的一维杆，具有恒定热导率，用 $N$ 个内部点进行离散化。得到的矩阵是三对角矩阵，其主对角线元素为 $2/h^2$，次对角线元素为 $-1/h^2$，其中 $h = 1/(N+1)$。\n   b) 在 $[0,1] \\times [0,1]$ 上的二维方板，可能具有各向异性热导率，在 $x$ 方向有 $N_x$ 个点、$y$ 方向有 $N_y$ 个点的均匀网格上进行离散化。得到的矩阵使用一个五点差分格式，其系数在主对角线上为 $2 k_x/h_x^2 + 2 k_y/h_y^2$，$x$ 方向最近邻居的系数为 $-k_x/h_x^2$，$y$ 方向最近邻居的系数为 $-k_y/h_y^2$，其中 $h_x = 1/(N_x+1)$，$h_y = 1/(N_y+1)$，$k_x$ 和 $k_y$ 分别表示沿 $x$ 轴和 $y$ 轴的热导率。\n\n2. 对于每个构造的 SPD 矩阵 $A$，将右端向量 $b$ 设置为适当长度的全1向量，并将 $x_0$ 初始化为零向量。应用共轭梯度法求解 $A x = b$，记录每次迭代 $k$ 的残差范数 $\\|r_k\\|_2$ 直至收敛。使用基于相对残差的停止准则，即当 $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$（其中 $\\varepsilon = 10^{-12}$）时停止，或当达到最大迭代次数时停止。\n\n3. 从记录的残差范数 $\\{\\|r_k\\|_2\\}$ 中，仅使用残差历史记录为谱条件数 $\\kappa(A)$ 构建一个估计量。该估计量必须基于在残差范数与迭代次数的半对数图中识别渐近线性速率，并将此速率映射到 $\\kappa(A)$ 的估计值。该设计必须基于应用于 SPD 系统的 CG 方法的性质，并有科学依据，而不是脱离理论的经验曲线拟合。\n\n4. 为了验证该估计量，使用为每个测试矩阵数值计算得到的极端特征值来计算参考条件数 $\\kappa_{\\text{true}}(A)$。通过相对误差 $\\left|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)\\right| / \\kappa_{\\text{true}}(A)$ 将估计值 $\\kappa_{\\text{est}}(A)$ 与 $\\kappa_{\\text{true}}(A)$ 进行比较。\n\n使用以下测试套件以确保覆盖范围：\n- 测试用例 1：一维杆，其中 $N = 50$。\n- 测试用例 2：一维杆，其中 $N = 200$。\n- 测试用例 3：二维板，具有各向异性热导率，$k_x = 1$，$k_y = 10$，$N_x = 20$，$N_y = 20$。\n- 测试用例 4：二维各向同性板，$k_x = 1$，$k_y = 1$，$N_x = 8$，$N_y = 8$。\n\n对于所有测试，将长度和热导率视为无量纲化；无需进行物理单位转换。不出现角度，因此不需要角度单位。所有数值比较均以十进制表示。\n\n您的程序应生成单行输出，其中包含一个方括号内包含的逗号分隔列表。对于每个测试用例，如果相对误差 $\\left|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)\\right| / \\kappa_{\\text{true}}(A)$ 小于或等于 $0.50$，则输出布尔值 true，否则输出 false，顺序与测试套件一致。例如，输出格式应与 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$ 完全一样。",
            "solution": "该问题要求设计并验证一个实验，用以仅通过共轭梯度（CG）法的收敛历史来估计对称正定（SPD）矩阵 $A$ 的谱条件数 $\\kappa(A)$。矩阵 $A$ 源于稳态热传导方程的有限差分格式离散化。\n\n**1. 估计量的理论基础**\n\n共轭梯度法是一种求解线性系统 $A x = b$ 的迭代算法，其中 $A$ 是一个 $n \\times n$ 的 SPD 矩阵。该方法生成一系列近似解 $x_k$，这些解在不断扩大的克雷洛夫子空间上最小化误差的 A-范数，即 $\\|x - x_k\\|_A = \\sqrt{(x - x_k)^T A (x - x_k)}$。CG 方法的收敛速率由谱条件数 $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$ 来界定，其中 $\\lambda_{\\max}(A)$ 和 $\\lambda_{\\min}(A)$ 分别是 $A$ 的最大和最小特征值。\n\n数值线性代中的一个标准结果给出了误差 $e_k = x_* - x_k$（其中 $x_*$ 是真实解）的 A-范数的以下上界：\n$$\n\\|e_k\\|_A \\le 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\|e_0\\|_A\n$$\n这个不等式表明，误差预计在每次迭代中（至少在渐近情况下）会减少一个因子 $\\rho$，其中 $\\rho$ 是几何收敛因子：\n$$\n\\rho = \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1}\n$$\n虽然这个界是关于误差的 A-范数的，但可以证明残差的 $L_2$-范数 $\\|r_k\\|_2 = \\|b - A x_k\\|_2 = \\|A e_k\\|_2$ 表现出相同的渐近收敛率。因此，对于较大的迭代次数 $k$，我们期望以下近似成立：\n$$\n\\|r_k\\|_2 \\approx C \\cdot \\rho^k\n$$\n其中 $C$ 是一个常数，取决于初始残差和误差的特征向量分解。\n\n**2. 估计量设计**\n\n关系式 $\\|r_k\\|_2 \\approx C \\cdot \\rho^k$ 是我们估计 $\\kappa(A)$ 的基础。通过对两边取自然对数，我们得到了 $\\log(\\|r_k\\|_2)$ 和迭代次数 $k$ 之间的线性关系：\n$$\n\\log(\\|r_k\\|_2) \\approx \\log(C) + k \\log(\\rho)\n$$\n这个方程表明，残差范数与迭代次数的半对数图在渐近区域将近似为线性，其斜率 $S = \\log(\\rho)$。\n\n估计 $\\kappa(A)$ 的实验步骤如下：\n1.  对系统 $A x = b$ 应用 CG 算法，并记录每次迭代 $k = 0, 1, 2, \\dots$ 的残差范数序列 $\\{\\|r_k\\|_2\\}$。\n2.  选择残差历史记录中代表渐近收敛行为的一部分。一个常见的启发式方法是使用达到收敛前记录的迭代次数的后半部分。设这个迭代序列为 $k \\in \\{k_{\\text{start}}, \\dots, k_{\\text{end}}\\}$。\n3.  对这个选定范围内的点 $(k, \\log(\\|r_k\\|_2))$ 进行线性回归（最小二乘拟合），以获得斜率的估计值 $\\hat{S}$。\n4.  在估计出斜率 $\\hat{S} \\approx \\log(\\rho)$ 后，我们可以求解 $\\rho$：\n    $$\n    \\hat{\\rho} = e^{\\hat{S}}\n    $$\n5.  最后，我们反转 $\\rho$ 和 $\\kappa(A)$ 之间的关系，以找到我们的估计值 $\\kappa_{\\text{est}}(A)$：\n    $$\n    \\hat{\\rho} = \\frac{\\sqrt{\\kappa_{\\text{est}}(A)} - 1}{\\sqrt{\\kappa_{\\text{est}}(A)} + 1} \\implies \\sqrt{\\kappa_{\\text{est}}(A)} = \\frac{1 + \\hat{\\rho}}{1 - \\hat{\\rho}}\n    $$\n    $$\n    \\kappa_{\\text{est}}(A) = \\left( \\frac{1 + \\hat{\\rho}}{1 - \\hat{\\rho}} \\right)^2 = \\left( \\frac{1 + e^{\\hat{S}}}{1 - e^{\\hat{S}}} \\right)^2\n    $$\n这提供了一种直接从 CG 算法的可观察输出中估计 $\\kappa(A)$ 的方法，而无需任何关于矩阵 $A$ 特征值的知识。\n\n**3. 算法与验证流程**\n\n该实验通过以下计算步骤实现：\n\n-   **矩阵构建**：使用 `scipy.sparse` 构建 SPD 矩阵 $A$ 为稀疏矩阵。\n    -   对于有 $N$ 个内部点和步长 $h = 1/(N+1)$ 的 $1$D 情况，矩阵是一个大小为 $N \\times N$ 的三对角矩阵，对角线元素为 $2/h^2$，次对角线元素为 $-1/h^2$。\n    -   对于 $N_x \\times N_y$ 网格上的 $2$D 情况，大小为 $(N_xN_y) \\times (N_xN_y)$ 的矩阵使用两个 $1$D 拉普拉斯矩阵的克罗内克积之和来构建。设 $T_M$ 是一个 $M \\times M$ 的三对角矩阵，对角线上为 $2$，次对角线上为 $-1$，设 $I_M$ 是大小为 $M$ 的单位矩阵。则 $2$D 算子矩阵为 $A = \\frac{k_x}{h_x^2} (T_{N_x} \\otimes I_{N_y}) + \\frac{k_y}{h_y^2} (I_{N_x} \\otimes T_{N_y})$。这种构建方式正确地模拟了在采用节点字典序排序的网格上的五点差分格式。\n\n-   **共轭梯度法实现**：需要 CG 算法的自定义实现来记录每次迭代 $k$ 的残差的 $L_2$-范数 $r_k$。算法从 $x_0 = 0$（零向量）和 $b$ 为全1向量开始。当相对残差范数 $\\|r_k\\|_2 / \\|b\\|_2$ 低于容差 $\\varepsilon = 10^{-12}$ 或达到最大迭代次数时终止。\n\n-   **估计量实现**：如第 2 节所述计算 $\\kappa_{\\text{est}}(A)$。使用 `numpy.polyfit` 对 CG 运行产生的迭代后半部分的残差范数的自然对数进行线性回归，以找到斜率 $\\hat{S}$。\n\n-   **验证**：为验证估计量，为每个测试矩阵计算“真实”条件数 $\\kappa_{\\text{true}}(A)$。这是通过使用稀疏特征求解器 `scipy.sparse.linalg.eigs` 数值求解极端特征值来完成的。具体来说，我们找到 $\\lambda_{\\min}(A)$ 和 $\\lambda_{\\max}(A)$ 并计算 $\\kappa_{\\text{true}}(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$。然后通过相对误差来量化估计量的性能：\n    $$\n    \\text{Error}_{\\text{rel}} = \\frac{|\\kappa_{\\text{est}}(A) - \\kappa_{\\text{true}}(A)|}{\\kappa_{\\text{true}}(A)}\n    $$\n    每个测试用例的最终输出是一个布尔值，如果此相对误差小于或等于 $0.50$，则为 `True`，否则为 `False`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse import linalg as sla\n\ndef build_1d_matrix(N: int):\n    \"\"\"Constructs the SPD matrix for a 1D rod.\"\"\"\n    h = 1.0 / (N + 1)\n    diag_val = 2.0 / h**2\n    off_diag_val = -1.0 / h**2\n    diagonals = [[off_diag_val] * (N - 1), [diag_val] * N, [off_diag_val] * (N - 1)]\n    return sparse.diags(diagonals, [-1, 0, 1], format='csr')\n\ndef build_2d_matrix(Nx: int, Ny: int, kx: float, ky: float):\n    \"\"\"Constructs the SPD matrix for a 2D plate using Kronecker products.\"\"\"\n    hx = 1.0 / (Nx + 1)\n    hy = 1.0 / (Ny + 1)\n\n    diag_x = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Nx, Nx))\n    diag_y = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Ny, Ny))\n    \n    Ix = sparse.identity(Nx)\n    Iy = sparse.identity(Ny)\n    \n    Ax = sparse.kron(diag_x, Iy) * (kx / hx**2)\n    Ay = sparse.kron(Ix, diag_y) * (ky / hy**2)\n    \n    return (Ax + Ay).tocsr()\n\ndef conjugate_gradient(A, b, tol=1e-12, max_iter=5000):\n    \"\"\"\n    Solves Ax=b using the Conjugate Gradient method, recording residual norms.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n)\n    r = b - A.dot(x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0:\n        return x, []\n        \n    residual_norms = [np.sqrt(rs_old)]\n    \n    for k in range(max_iter):\n        Ap = A.dot(p)\n        alpha = rs_old / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = np.dot(r, r)\n        \n        norm_r = np.sqrt(rs_new)\n        residual_norms.append(norm_r)\n        \n        if norm_r / b_norm  tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return x, residual_norms\n\ndef estimate_kappa(residual_norms):\n    \"\"\"\n    Estimates the condition number from the CG residual history.\n    \"\"\"\n    # Use the second half of the iterations for linear regression, as it represents\n    # the asymptotic convergence behavior.\n    num_iterations = len(residual_norms)\n    if num_iterations  4:\n        # Not enough data for a meaningful fit\n        return np.nan\n        \n    fit_start_index = num_iterations // 2\n    \n    iters = np.arange(fit_start_index, num_iterations)\n    log_residuals = np.log(residual_norms[fit_start_index:])\n    \n    # Perform linear regression to find the slope\n    # polyfit returns [slope, intercept]\n    slope, _ = np.polyfit(iters, log_residuals, 1)\n    \n    if slope >= 0:\n        # Should be a negative slope for a converging method\n        return np.nan\n\n    # Calculate kappa from the slope\n    # S = log(rho) => rho = exp(S)\n    # kappa_est = ((1 + rho) / (1 - rho))^2\n    rho_est = np.exp(slope)\n    kappa_est = ((1 + rho_est) / (1 - rho_est))**2\n    \n    return kappa_est\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'type': '1D', 'N': 50},\n        {'type': '1D', 'N': 200},\n        {'type': '2D', 'Nx': 20, 'Ny': 20, 'kx': 1.0, 'ky': 10.0},\n        {'type': '2D', 'Nx': 8, 'Ny': 8, 'kx': 1.0, 'ky': 1.0},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        if case['type'] == '1D':\n            A = build_1d_matrix(case['N'])\n            matrix_size = case['N']\n        else: # 2D\n            A = build_2d_matrix(case['Nx'], case['Ny'], case['kx'], case['ky'])\n            matrix_size = case['Nx'] * case['Ny']\n\n        # Compute true condition number using numerical eigenvalues\n        # Since A is SPD, eigenvalues are real and positive.\n        # k=1 returns one eigenvalue, which='LM' for largest magnitude, 'SM' for smallest.\n        try:\n            lambda_max = sla.eigs(A, k=1, which='LM', return_eigenvectors=False)[0].real\n            lambda_min = sla.eigs(A, k=1, which='SM', return_eigenvectors=False)[0].real\n            kappa_true = lambda_max / lambda_min\n        except sla.ArpackNoConvergence:\n            # Handle cases where eigensolver fails, although unlikely for these matrices\n            results.append(str(False).lower()) # Mark as failed test\n            continue\n\n        # Set up and run Conjugate Gradient\n        b = np.ones(matrix_size)\n        x_sol, residual_history = conjugate_gradient(A, b, tol=1e-12, max_iter=10000)\n        \n        # Estimate condition number from residual history\n        kappa_est = estimate_kappa(residual_history)\n\n        if np.isnan(kappa_est) or kappa_true == 0:\n            results.append(str(False).lower())\n            continue\n            \n        # Validate the estimator\n        relative_error = np.abs(kappa_est - kappa_true) / kappa_true\n        \n        results.append(str(relative_error = 0.50).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}