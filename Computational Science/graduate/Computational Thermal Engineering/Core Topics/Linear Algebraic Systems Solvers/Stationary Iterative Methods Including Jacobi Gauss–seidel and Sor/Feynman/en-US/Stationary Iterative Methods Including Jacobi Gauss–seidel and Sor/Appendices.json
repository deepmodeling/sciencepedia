{
    "hands_on_practices": [
        {
            "introduction": "To build a robust understanding of iterative methods, we must first analyze their fundamental convergence properties. This exercise guides you through a classic analytical derivation for the Jacobi method, one of the simplest stationary schemes. By applying it to the one-dimensional Poisson equation—a canonical model for steady-state heat conduction—you will explicitly compute the spectral radius of the iteration matrix and see precisely how it depends on the mesh size . This foundational analysis provides a concrete link between the problem's discretization and the method's convergence speed.",
            "id": "3986575",
            "problem": "A straight fin with constant cross-section is modeled by the steady one-dimensional heat conduction equation with homogeneous Dirichlet boundary conditions, which reduces (after nondimensionalization) to the boundary value problem\n$$\n- \\frac{d^{2} u}{dx^{2}} = f(x), \\quad x \\in (0,1), \\qquad u(0)=0,\\; u(1)=0.\n$$\nDiscretize the interior of the interval with $n$ equally spaced interior points at $x_{j} = j h$ for $j=1,2,\\dots,n$ and $h = \\frac{1}{n+1}$. Using the standard second-order central difference, obtain the linear system $A \\mathbf{u} = \\mathbf{b}$, where $A$ is the tridiagonal matrix arising from the discrete negative second derivative with Dirichlet boundary conditions. Consider the Jacobi method applied to this system with the standard splitting $A = D - (L+U)$, and the Jacobi iteration matrix $B_{J} = I - D^{-1} A$.\n\nStarting from the discrete operator and the boundary conditions, derive the eigenpairs of $B_{J}$ explicitly by constructing an appropriate orthogonal basis of eigenvectors. Then determine the spectral radius $\\rho(B_{J})$ as a closed-form function of $n$.\n\nReport only the final closed-form analytic expression for $\\rho(B_{J})$. No numerical evaluation is required.",
            "solution": "The problem is valid. It is a standard, well-posed problem in numerical analysis concerning the spectral properties of an iteration matrix derived from a finite difference discretization of a boundary value problem. All information is self-contained and scientifically sound.\n\nWe begin by discretizing the given boundary value problem:\n$$\n- \\frac{d^{2} u}{dx^{2}} = f(x), \\quad x \\in (0,1), \\qquad u(0)=0,\\; u(1)=0.\n$$\nThe domain $(0,1)$ is divided into $n+1$ subintervals of equal width $h = \\frac{1}{n+1}$. The grid points are $x_j = jh$ for $j=0, 1, \\dots, n+1$. The values of the function $u(x)$ at the interior grid points are denoted by $u_j = u(x_j)$ for $j=1, 2, \\dots, n$. The boundary conditions imply $u_0 = 0$ and $u_{n+1} = 0$.\n\nThe second derivative at an interior point $x_j$ is approximated using a second-order central difference formula:\n$$\n\\frac{d^{2} u}{dx^{2}}\\Bigg|_{x=x_j} \\approx \\frac{u(x_{j+1}) - 2u(x_j) + u(x_{j-1})}{h^2} = \\frac{u_{j+1} - 2u_j + u_{j-1}}{h^2}.\n$$\nSubstituting this into the differential equation at each interior point $x_j$ gives a system of $n$ linear equations:\n$$\n- \\frac{u_{j+1} - 2u_j + u_{j-1}}{h^2} = f(x_j), \\quad j=1, 2, \\dots, n.\n$$\nRearranging the terms, we get:\n$$\n\\frac{1}{h^2}(-u_{j-1} + 2u_j - u_{j+1}) = f_j,\n$$\nwhere $f_j = f(x_j)$. We apply the boundary conditions: for $j=1$, $u_0=0$, and for $j=n$, $u_{n+1}=0$. The system of equations can be written in matrix form $A \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} = [u_1, u_2, \\dots, u_n]^T$, $\\mathbf{b} = [f_1, f_2, \\dots, f_n]^T$, and $A$ is the $n \\times n$ matrix:\n$$\nA = \\frac{1}{h^2} \\begin{pmatrix}\n2  -1  0  \\cdots  0 \\\\\n-1  2  -1  \\ddots  \\vdots \\\\\n0  -1  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  2  -1 \\\\\n0  \\cdots  0  -1  2\n\\end{pmatrix}.\n$$\nFor the Jacobi method, the matrix $A$ is split as $A = D - (L+U)$, where $D$ is the diagonal part of $A$, and $-L$ and $-U$ are the strictly lower and upper triangular parts of $A$, respectively. The Jacobi iteration matrix is given by $B_J = D^{-1}(L+U) = I - D^{-1}A$.\n\nFrom the structure of $A$, its diagonal part is $D = \\frac{2}{h^2}I$, where $I$ is the $n \\times n$ identity matrix.\nThe inverse of $D$ is $D^{-1} = \\frac{h^2}{2}I$.\nSubstituting this into the expression for $B_J$:\n$$\nB_J = I - \\left(\\frac{h^2}{2}I\\right) A = I - \\frac{h^2}{2} \\left[ \\frac{1}{h^2} \\begin{pmatrix}\n2  -1   \\cdots \\\\\n-1  2  -1  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1  2\n\\end{pmatrix} \\right] = I - \\frac{1}{2} \\begin{pmatrix}\n2  -1   \\cdots \\\\\n-1  2  -1  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1  2\n\\end{pmatrix}.\n$$\nThis results in the Jacobi iteration matrix:\n$$\nB_J = \\begin{pmatrix}\n1    \\\\\n  1   \\\\\n   \\ddots  \\\\\n    1\n\\end{pmatrix} - \\begin{pmatrix}\n1  -1/2   \\\\\n-1/2  1  -1/2  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  -1/2  1\n\\end{pmatrix} = \\begin{pmatrix}\n0  1/2  0  \\cdots  0 \\\\\n1/2  0  1/2  \\ddots  \\vdots \\\\\n0  1/2  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  0  1/2 \\\\\n0  \\cdots  0  1/2  0\n\\end{pmatrix}.\n$$\nTo find the spectral radius $\\rho(B_J)$, we need to find the eigenvalues of $B_J$. The problem requires us to do this by first constructing an orthogonal basis of eigenvectors. The eigenvectors of $B_J$ are the same as the eigenvectors of $A$, since $B_J$ is a linear polynomial in $A$ and $D$ is a scalar multiple of the identity.\n\nLet's find the eigenpairs of the tridiagonal matrix $T$ where $A = \\frac{1}{h^2}T$ and $T$ has diagonal entries $2$ and off-diagonal entries $-1$. Let an eigenvector of $T$ be $\\mathbf{v}$ with eigenvalue $\\mu$. The $j$-th component of the eigenvalue problem $T\\mathbf{v} = \\mu \\mathbf{v}$ is:\n$$\n-v_{j-1} + 2v_j - v_{j+1} = \\mu v_j,\n$$\nwith boundary conditions $v_0 = 0$ and $v_{n+1} = 0$. This is a linear second-order difference equation. We propose a solution of the form $v_j = \\sin(j\\theta)$ for some angle $\\theta$. The boundary condition $v_0=0$ is automatically satisfied. The condition $v_{n+1}=0$ requires $\\sin((n+1)\\theta) = 0$, which implies $(n+1)\\theta = k\\pi$ for some integer $k$. Thus, we have a family of candidate solutions indexed by $k$:\n$$\n\\theta_k = \\frac{k\\pi}{n+1}, \\quad k=1, 2, \\dots, n.\n$$\nThe corresponding vectors $\\mathbf{v}^{(k)}$ have components $v_j^{(k)} = \\sin\\left(\\frac{jk\\pi}{n+1}\\right)$. These form a set of $n$ linearly independent vectors, and they constitute an orthogonal basis for $\\mathbb{R}^n$.\n\nLet's find the eigenvalue $\\mu_k$ corresponding to each eigenvector $\\mathbf{v}^{(k)}$. Substituting $v_j^{(k)}$ into the difference equation:\n$$\n-\\sin\\left(\\frac{(j-1)k\\pi}{n+1}\\right) + 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - \\sin\\left(\\frac{(j+1)k\\pi}{n+1}\\right) = \\mu_k \\sin\\left(\\frac{jk\\pi}{n+1}\\right).\n$$\nUsing the trigonometric identity $\\sin(a-b)+\\sin(a+b) = 2\\sin(a)\\cos(b)$, we can simplify the left-hand side:\n$$\n2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - \\left[ \\sin\\left(\\frac{(j-1)k\\pi}{n+1}\\right) + \\sin\\left(\\frac{(j+1)k\\pi}{n+1}\\right) \\right] = 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right)\\cos\\left(\\frac{k\\pi}{n+1}\\right).\n$$\nFactoring out $\\sin\\left(\\frac{jk\\pi}{n+1}\\right)$, we get:\n$$\n\\left(2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) \\sin\\left(\\frac{jk\\pi}{n+1}\\right) = \\mu_k \\sin\\left(\\frac{jk\\pi}{n+1}\\right).\n$$\nThus, the eigenvalues of $T$ are $\\mu_k = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)$ for $k=1, \\dots, n$.\n\nThe matrices $A$, $T$, and $B_J$ all share the same eigenvectors $\\mathbf{v}^{(k)}$. Let $\\lambda_k(A)$ be the eigenvalues of $A$ and $\\lambda_k(B_J)$ be the eigenvalues of $B_J$.\n$$\n\\lambda_k(A) = \\frac{1}{h^2}\\mu_k = \\frac{2}{h^2}\\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right).\n$$\nThe relationship between the eigenvalues of $B_J$ and $A$ is $\\lambda(B_J) = 1 - (\\frac{h^2}{2})\\lambda(A)$. Therefore,\n$$\n\\lambda_k(B_J) = 1 - \\frac{h^2}{2} \\lambda_k(A) = 1 - \\frac{h^2}{2} \\left[ \\frac{2}{h^2}\\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) \\right].\n$$\n$$\n\\lambda_k(B_J) = 1 - \\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) = \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad k=1, 2, \\dots, n.\n$$\nThe eigenpairs of $B_J$ are $\\left( \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\mathbf{v}^{(k)} \\right)$ where the $j$-th component of $\\mathbf{v}^{(k)}$ is $\\sin\\left(\\frac{jk\\pi}{n+1}\\right)$.\n\nThe spectral radius of $B_J$ is the maximum of the absolute values of its eigenvalues:\n$$\n\\rho(B_J) = \\max_{k \\in \\{1, \\dots, n\\}} |\\lambda_k(B_J)| = \\max_{k \\in \\{1, \\dots, n\\}} \\left| \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right|.\n$$\nThe argument of the cosine function, $\\frac{k\\pi}{n+1}$, ranges from $\\frac{\\pi}{n+1}$ (for $k=1$) to $\\frac{n\\pi}{n+1}$ (for $k=n$). This range is contained within the interval $(0, \\pi)$. The function $|\\cos(x)|$ on $(0, \\pi)$ is symmetric about $x=\\pi/2$ and its maximum value is attained at the endpoints of any closed interval $[a, b] \\subset (0, \\pi)$, i.e., at $x=a$ or $x=b$.\nIn our case, the set of arguments is discrete: $\\{\\frac{\\pi}{n+1}, \\frac{2\\pi}{n+1}, \\dots, \\frac{n\\pi}{n+1}\\}$. The value of $|\\cos(\\theta)|$ will be largest when $\\theta$ is closest to $0$ or $\\pi$.\nThe arguments closest to $0$ and $\\pi$ are for $k=1$ and $k=n$:\n$$\n\\text{For } k=1: \\quad \\left| \\cos\\left(\\frac{\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right) \\quad \\text{(since } \\frac{\\pi}{n+1} \\in (0, \\pi/2) \\text{ for } n \\ge 1).\n$$\n$$\n\\text{For } k=n: \\quad \\left| \\cos\\left(\\frac{n\\pi}{n+1}\\right) \\right| = \\left| \\cos\\left(\\pi - \\frac{\\pi}{n+1}\\right) \\right| = \\left| -\\cos\\left(\\frac{\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right).\n$$\nThe values of $|\\cos(\\frac{k\\pi}{n+1})|$ for $k=2, \\dots, n-1$ will be smaller because $\\frac{k\\pi}{n+1}$ will be further from $0$ and $\\pi$ than the endpoints.\nTherefore, the maximum absolute value is achieved at $k=1$ and $k=n$.\nThe spectral radius is:\n$$\n\\rho(B_J) = \\cos\\left(\\frac{\\pi}{n+1}\\right).\n$$",
            "answer": "$$\n\\boxed{\\cos\\left(\\frac{\\pi}{n+1}\\right)}\n$$"
        },
        {
            "introduction": "When moving from one to two dimensions, a new practical challenge emerges: how to order the grid of unknown temperatures into a single vector for the linear system. This ordering choice directly influences the structure of the lower and upper triangular parts of the system matrix, and thus the behavior of methods like Gauss-Seidel. This practice asks you to implement and compare two common schemes—lexicographic and red-black ordering—and numerically investigate how this choice affects the convergence rate for the 2D Laplace equation . The results will provide insight into the structure of these methods and connect your implementation to important theoretical principles.",
            "id": "3986527",
            "problem": "Consider the steady-state heat conduction in a homogeneous, isotropic medium occupying a square domain with constant thermal conductivity. The governing equation is the Laplace equation for temperature, written as $-\\nabla^2 T = 0$ with Dirichlet boundary conditions $T = 0$ on the boundary. Discretize this equation using second-order central finite differences on a uniform Cartesian grid of interior points indexed by $(i,j)$ with spacings $h_x$ and $h_y$, resulting in a linear system $A \\mathbf{u} = \\mathbf{b}$ where $\\mathbf{u}$ is the vector of temperatures at interior grid points. The matrix $A$ corresponds to the five-point discrete Laplacian, with diagonal entries $2(h_x^{-2} + h_y^{-2})$ and off-diagonals $-h_x^{-2}$ for nearest neighbors in the $x$-direction and $-h_y^{-2}$ for nearest neighbors in the $y$-direction, and where $\\mathbf{b}$ is zero due to homogeneous boundary conditions.\n\nConsider the Gauss-Seidel stationary iterative method defined by the matrix splitting $A = L + D + U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower triangular part of $A$, and $U$ is the strictly upper triangular part of $A$, leading to the iteration $\\mathbf{u}^{(m+1)} = B_{GS} \\mathbf{u}^{(m)} + (D+L)^{-1}\\mathbf{b}$, where the Gauss-Seidel iteration matrix is $B_{GS} = -(D+L)^{-1} U$. The convergence of the method is determined by the spectral radius $\\rho(B_{GS}) = \\max_i |\\lambda_i(B_{GS})|$, requiring $\\rho(B_{GS})  1$.\n\nUnknowns can be ordered in different ways, which changes the definition of $L$ and $U$ and therefore changes $B_{GS}$. Two common orderings are:\n- Lexicographic ordering: index the unknowns by scanning $i$ in increasing order for each fixed $j$, then increasing $j$.\n- Red-black ordering (two-color ordering): define a checkerboard coloring such that a node $(i,j)$ is colored red if $(i+j)$ is even and black if $(i+j)$ is odd, then list all red nodes first followed by all black nodes.\n\nYour task is to write a complete program that:\n1. Constructs the discrete Laplacian matrix $A$ for a given number of interior points $N_x$ and $N_y$ in the $x$- and $y$-directions, respectively, assuming a unit square with spacings $h_x = 1/(N_x+1)$ and $h_y = 1/(N_y+1)$.\n2. Forms the Gauss-Seidel iteration matrix $B_{GS} = -(D+L)^{-1} U$ for both lexicographic ordering and red-black ordering.\n3. Computes the spectral radii $\\rho(B_{GS}^{\\text{lex}})$ and $\\rho(B_{GS}^{\\text{rb}})$ for the two orderings and the ratio $\\rho(B_{GS}^{\\text{rb}})/\\rho(B_{GS}^{\\text{lex}})$.\n4. Outputs, for each test case, a list with three floats $[\\rho(B_{GS}^{\\text{lex}}), \\rho(B_{GS}^{\\text{rb}}), \\rho(B_{GS}^{\\text{rb}})/\\rho(B_{GS}^{\\text{lex}})]$, each rounded to six decimal places.\n\nAll computations are nondimensional and unitless. The final output must be a single line containing the results for all test cases aggregated as a comma-separated list of these three-float lists, with no spaces anywhere in the line.\n\nUse the following test suite of grid sizes to evaluate and compare the convergence behavior:\n- Case $1$: $N_x = 8$, $N_y = 8$ (a moderately sized two-dimensional grid).\n- Case $2$: $N_x = 16$, $N_y = 16$ (a larger two-dimensional grid to assess scaling).\n- Case $3$: $N_x = 1$, $N_y = 31$ (a nearly one-dimensional grid to examine anisotropy in ordering effects).\n- Case $4$: $N_x = 2$, $N_y = 2$ (a very small grid to check boundary behavior and correctness on minimal sizes).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The format must be\n\"[[a1,b1,c1],[a2,b2,c2],[a3,b3,c3],[a4,b4,c4]]\"\nwhere $a_k$, $b_k$, and $c_k$ are the floats for case $k$ rounded to six decimals, and there must be no spaces in the output line.",
            "solution": "The problem is valid. It is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution.\n\nThe problem requires a comparative analysis of the convergence rate of the Gauss–Seidel stationary iterative method for solving a linear system derived from the finite difference discretization of the Laplace equation. The comparison is to be made between two different orderings of the grid unknowns: lexicographic and red–black. The convergence rate is determined by the spectral radius of the iteration matrix.\n\n### 1. Mathematical Model and Discretization\n\nThe governing partial differential equation for steady-state heat conduction in a homogeneous, isotropic medium with no heat sources is the Laplace equation for the temperature field $T$:\n$$\n-\\nabla^2 T = -\\left( \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} \\right) = 0\n$$\nThe domain is a unit square, and the temperature on the boundary is held at $T=0$ (homogeneous Dirichlet boundary conditions).\n\nWe discretize this equation on a uniform Cartesian grid with $N_x \\times N_y$ interior points. The grid spacings in the $x$ and $y$ directions are $h_x = 1/(N_x+1)$ and $h_y = 1/(N_y+1)$, respectively. The temperature at an interior grid point $(i, j)$, where $i \\in \\{1, \\dots, N_x\\}$ and $j \\in \\{1, \\dots, N_y\\}$, is denoted by $T_{i,j}$. Using a second-order central finite difference scheme to approximate the second partial derivatives, we get:\n$$\n\\frac{\\partial^2 T}{\\partial x^2}\\bigg|_{(i,j)} \\approx \\frac{T_{i+1,j} - 2T_{i,j} + T_{i-1,j}}{h_x^2}\n$$\n$$\n\\frac{\\partial^2 T}{\\partial y^2}\\bigg|_{(i,j)} \\approx \\frac{T_{i,j+1} - 2T_{i,j} + T_{i,j-1}}{h_y^2}\n$$\nSubstituting these into the negative Laplace equation yields the discrete equation at each interior point $(i,j)$:\n$$\n-\\left( \\frac{T_{i+1,j} - 2T_{i,j} + T_{i-1,j}}{h_x^2} + \\frac{T_{i,j+1} - 2T_{i,j} + T_{i,j-1}}{h_y^2} \\right) = 0\n$$\nRearranging the terms, we obtain the five-point stencil equation:\n$$\n2\\left(\\frac{1}{h_x^2} + \\frac{1}{h_y^2}\\right)T_{i,j} - \\frac{1}{h_x^2}(T_{i+1,j} + T_{i-1,j}) - \\frac{1}{h_y^2}(T_{i,j+1} + T_{i,j-1}) = 0\n$$\nThis set of $N_x \\times N_y$ linear equations can be written in matrix form as $A\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector of the unknown temperatures $T_{i,j}$ at the $N = N_x N_y$ interior points. Due to the homogeneous Dirichlet boundary conditions, any term $T_{i,j}$ where the point $(i,j)$ lies on the boundary is zero, which means the right-hand side vector $\\mathbf{b}$ is a zero vector. The matrix $A$ is the discrete Laplacian operator. It is a sparse, symmetric, and positive definite matrix.\n\n### 2. The Gauss–Seidel Method\n\nThe Gauss–Seidel method is a stationary iterative technique for solving $A\\mathbf{u} = \\mathbf{b}$. It is based on the splitting of the matrix $A$ into its diagonal ($D$), strictly lower triangular ($L$), and strictly upper triangular ($U$) parts: $A = D + L + U$. The iteration is defined as:\n$$\n(D+L)\\mathbf{u}^{(m+1)} = -U\\mathbf{u}^{(m)} + \\mathbf{b}\n$$\nThis can be rewritten in terms of the iteration matrix $B_{GS}$:\n$$\n\\mathbf{u}^{(m+1)} = -(D+L)^{-1}U \\mathbf{u}^{(m)} + (D+L)^{-1}\\mathbf{b} = B_{GS}\\mathbf{u}^{(m)} + \\mathbf{c}\n$$\nThe method converges if and only if the spectral radius of the iteration matrix, $\\rho(B_{GS})$, is less than $1$. The spectral radius is defined as the maximum absolute value of the eigenvalues of $B_{GS}$.\n\n### 3. Node Ordering Schemes\n\nThe structure of the matrices $L$ and $U$ depends on the order in which the unknowns $T_{i,j}$ are arranged in the vector $\\mathbf{u}$.\n\n- **Lexicographic Ordering**: The grid points are ordered row-by-row (or column-by-column). For indices $i \\in \\{1, \\dots, N_x\\}$ and $j \\in \\{1, \\dots, N_y\\}$, a common mapping to a one-dimensional index $k \\in \\{0, \\dots, N-1\\}$ is $k = (j-1)N_x + (i-1)$. This ordering results in a block-tridiagonal matrix $A$.\n\n- **Red–Black (Checkerboard) Ordering**: The grid points are partitioned into two sets. A point $(i,j)$ is 'red' if $(i+j)$ is even and 'black' if $(i+j)$ is odd. The unknowns are ordered by listing all red-node temperatures first, followed by all black-node temperatures. This reordering, equivalent to applying a permutation matrix $P$ to the lexicographically ordered system, transforms the matrix $A$ into $A_{\\text{rb}} = PAP^T$. The resulting matrix has the form:\n$$\nA_{\\text{rb}} = \\begin{pmatrix} D_R  M \\\\ M^T  D_B \\end{pmatrix}\n$$\nwhere $D_R$ and $D_B$ are diagonal matrices corresponding to the red and black points, respectively, and the matrix $M$ contains the off-diagonal couplings between red and black points.\n\n### 4. Theoretical Convergence Analysis\n\nA matrix is said to be \"consistently ordered\" if its associated graph is bipartite and a specific ordering property is met. For matrices arising from the five-point finite difference stencil, both lexicographic and red-black orderings are consistent orderings.\n\nAccording to the Young-Frankel theory, if the matrix $A$ is symmetric, positive definite, and consistently ordered, the spectral radius of the Gauss-Seidel iteration matrix is related to the spectral radius of the Jacobi iteration matrix ($B_J = -D^{-1}(L+U)$) by:\n$$\n\\rho(B_{GS}) = [\\rho(B_J)]^2\n$$\nThe Jacobi matrix $B_J$ is permutation-similar under any reordering of the unknowns. That is, if $B_J^{\\text{lex}}$ and $B_J^{\\text{rb}}$ are the Jacobi matrices for the two orderings, they are related by $B_J^{\\text{rb}} = P B_J^{\\text{lex}} P^T$. A similarity transformation preserves eigenvalues. Therefore, the spectrum of the Jacobi matrix, and consequently its spectral radius $\\rho(B_J)$, is independent of the ordering.\n\nSince both lexicographic and red-black orderings are consistent for the problem at hand, it follows that:\n$$\n\\rho(B_{GS}^{\\text{lex}}) = [\\rho(B_J)]^2 \\quad \\text{and} \\quad \\rho(B_{GS}^{\\text{rb}}) = [\\rho(B_J)]^2\n$$\nThis leads to the theoretical prediction that the spectral radii for both orderings should be identical:\n$$\n\\rho(B_{GS}^{\\text{lex}}) = \\rho(B_{GS}^{\\text{rb}})\n$$\nand their ratio should be $1$. The numerical computation will verify this theoretical result.\n\n### 5. Algorithm Outline\n\nFor each test case $(N_x, N_y)$:\n1.  Calculate grid spacings $h_x = 1/(N_x+1)$ and $h_y = 1/(N_y+1)$.\n2.  **Lexicographic Analysis**:\n    a. Construct the $N \\times N$ matrix $A_{\\text{lex}}$ where $N=N_x N_y$, based on the lexicographic mapping $k = (j-1)N_x + (i-1)$.\n    b. Decompose $A_{\\text{lex}}$ into $D_{\\text{lex}}$, $L_{\\text{lex}}$, and $U_{\\text{lex}}$.\n    c. Compute the Gauss-Seidel iteration matrix $B_{GS}^{\\text{lex}} = -(D_{\\text{lex}}+L_{\\text{lex}})^{-1}U_{\\text{lex}}$.\n    d. Compute the eigenvalues of $B_{GS}^{\\text{lex}}$ and find the spectral radius $\\rho(B_{GS}^{\\text{lex}})$ by taking the maximum of their absolute values.\n3.  **Red–Black Analysis**:\n    a. Create a mapping from grid coordinates $(i,j)$ to a new one-dimensional index based on the red-black ordering. Let there be $N_R$ red points and $N_B$ black points. Red points are indexed $0, \\dots, N_R-1$ and black points are indexed $N_R, \\dots, N-1$.\n    b. Construct the $N \\times N$ matrix $A_{\\text{rb}}$ using this new mapping.\n    c. Decompose $A_{\\text{rb}}$ into $D_{\\text{rb}}$, $L_{\\text{rb}}$, and $U_{\\text{rb}}$.\n    d. Compute the iteration matrix $B_{GS}^{\\text{rb}} = -(D_{\\text{rb}}+L_{\\text{rb}})^{-1}U_{\\text{rb}}$.\n    e. Compute its spectral radius $\\rho(B_{GS}^{\\text{rb}})$.\n4.  Calculate the ratio $\\rho(B_{GS}^{\\text{rb}}) / \\rho(B_{GS}^{\\text{lex}})$.\n5.  Store the three resulting floating-point numbers, rounded to six decimal places.\nThe final output aggregates these results for all test cases into a single line of text as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef compute_spectral_radii(Nx, Ny):\n    \"\"\"\n    Constructs discrete Laplacian and computes Gauss-Seidel spectral radii\n    for lexicographic and red-black orderings.\n\n    Args:\n        Nx (int): Number of interior grid points in the x-direction.\n        Ny (int): Number of interior grid points in the y-direction.\n\n    Returns:\n        tuple: A tuple containing (rho_lex, rho_rb, ratio).\n    \"\"\"\n    N = Nx * Ny\n    if N == 0:\n        return (0.0, 0.0, 1.0)\n\n    hx = 1.0 / (Nx + 1)\n    hy = 1.0 / (Ny + 1)\n\n    hx2_inv = 1.0 / (hx * hx)\n    hy2_inv = 1.0 / (hy * hy)\n\n    # 1. Lexicographic Ordering\n    A_lex = np.zeros((N, N))\n    for j in range(Ny):\n        for i in range(Nx):\n            k = j * Nx + i\n            A_lex[k, k] = 2.0 * (hx2_inv + hy2_inv)\n            # West neighbor\n            if i > 0:\n                A_lex[k, k - 1] = -hx2_inv\n            # East neighbor\n            if i  Nx - 1:\n                A_lex[k, k + 1] = -hx2_inv\n            # South neighbor\n            if j > 0:\n                A_lex[k, k - Nx] = -hy2_inv\n            # North neighbor\n            if j  Ny - 1:\n                A_lex[k, k + Nx] = -hy2_inv\n\n    D_lex = np.diag(np.diag(A_lex))\n    L_lex = np.tril(A_lex, k=-1)\n    U_lex = np.triu(A_lex, k=1)\n\n    B_gs_lex = -inv(D_lex + L_lex) @ U_lex\n    rho_lex = np.max(np.abs(np.linalg.eigvals(B_gs_lex)))\n\n    # 2. Red-Black Ordering\n    # Using 0-based indices_ i in {0,...,Nx-1}, j in {0,...,Ny-1}.\n    # A node (i,j) is red if i+j is even, black if i+j is odd.\n    coord_map_rb = {}\n    red_nodes = []\n    black_nodes = []\n    for j in range(Ny):\n        for i in range(Nx):\n            if (i + j) % 2 == 0:\n                red_nodes.append((i, j))\n            else:\n                black_nodes.append((i, j))\n    \n    num_red = len(red_nodes)\n    for idx, (i, j) in enumerate(red_nodes):\n        coord_map_rb[(i, j)] = idx\n    for idx, (i, j) in enumerate(black_nodes):\n        coord_map_rb[(i, j)] = num_red + idx\n\n    A_rb = np.zeros((N, N))\n    for j in range(Ny):\n        for i in range(Nx):\n            k = coord_map_rb[(i, j)]\n            A_rb[k, k] = 2.0 * (hx2_inv + hy2_inv)\n            \n            neighbors = []\n            if i > 0: neighbors.append((i-1, j))\n            if i  Nx - 1: neighbors.append((i+1, j))\n            if j > 0: neighbors.append((i, j-1))\n            if j  Ny - 1: neighbors.append((i, j+1))\n\n            for neighbor_i, neighbor_j in neighbors:\n                k_neighbor = coord_map_rb[(neighbor_i, neighbor_j)]\n                if abs(i - neighbor_i) == 1: # x-neighbor\n                    A_rb[k, k_neighbor] = -hx2_inv\n                else: # y-neighbor\n                    A_rb[k, k_neighbor] = -hy2_inv\n    \n    D_rb = np.diag(np.diag(A_rb))\n    L_rb = np.tril(A_rb, k=-1)\n    U_rb = np.triu(A_rb, k=1)\n\n    B_gs_rb = -inv(D_rb + L_rb) @ U_rb\n    rho_rb = np.max(np.abs(np.linalg.eigvals(B_gs_rb)))\n\n    ratio = rho_rb / rho_lex if rho_lex != 0 else 1.0\n\n    return rho_lex, rho_rb, ratio\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (8, 8),   # Case 1\n        (16, 16), # Case 2\n        (1, 31),  # Case 3\n        (2, 2)    # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny = case\n        res_tuple = compute_spectral_radii(Nx, Ny)\n        \n        # Round each float in the result tuple to 6 decimal places\n        rounded_res = [round(val, 6) for val in res_tuple]\n        results.append(rounded_res)\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string and remove all spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Building on our understanding of ordering, we now explore a scenario where this choice is not merely a matter of structure, but is critical for convergence itself. When heat transfer problems include convection, the resulting discretized matrix is often nonsymmetric, and the convergence of the powerful Successive Over-Relaxation (SOR) method becomes highly sensitive to the ordering of unknowns. This exercise demonstrates this effect by having you implement SOR for a convection-dominated problem, showing how a \"natural\" downstream ordering can lead to divergence while a physically-informed upstream ordering ensures stability and rapid convergence . This is a vital lesson in designing robust solvers for real-world engineering simulations.",
            "id": "3986584",
            "problem": "Consider the steady, one-dimensional convection–diffusion energy balance under constant properties, leading by standard finite-volume discretization on a uniform mesh of $n$ nodes to a linear system $A \\mathbf{T} = \\mathbf{b}$ with a tridiagonal coefficient matrix $A$. For purely conductive problems (no convection), $A$ is symmetric positive definite, for example with diagonal entries $2$ and off-diagonal entries $-1$. For convection-dominated problems discretized with upwind differencing, $A$ remains strictly diagonally dominant with nonpositive off-diagonals and is generally nonsymmetric, where the magnitude of the upstream coupling (one off-diagonal) can be significantly larger than the downstream coupling (the other off-diagonal). Stationary iterative methods solve $A \\mathbf{T} = \\mathbf{b}$ by splitting $A = D - L - U$ with $D$ the diagonal, $L$ the strictly lower triangular part, and $U$ the strictly upper triangular part (all defined with respect to a given ordering of unknowns). The Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega$ uses the iteration\n$$\n\\mathbf{T}^{(k+1)} = (D - \\omega L)^{-1} \\left( (1 - \\omega) D + \\omega U \\right) \\mathbf{T}^{(k)} + (D - \\omega L)^{-1} \\omega \\mathbf{b}.\n$$\nThe associated homogeneous SOR iteration matrix is\n$$\nG_{\\omega} = (D - \\omega L)^{-1} \\left( (1 - \\omega) D + \\omega U \\right).\n$$\nConvergence of the SOR method for a given ordering and $\\omega$ is determined by the spectral radius $\\rho(G_{\\omega})$, i.e., the method converges if and only if $\\rho(G_{\\omega})  1$. The ordering of unknowns alters the decomposition of $A$ into $D$, $L$, and $U$ through permutation similarity: if $P$ is a permutation matrix representing a reordering, the reordered system is $\\tilde{A} = P A P^{\\mathsf{T}}$, and the split changes accordingly. In nonsymmetric cases, this can strongly affect $\\rho(G_{\\omega})$, potentially causing divergence of overrelaxed SOR under poor orderings.\n\nYour task is to quantify the sensitivity of SOR to ordering by computing $\\rho(G_{\\omega})$ under specified orderings and to construct a counterexample where a poor ordering leads to divergence for $\\omega  1$.\n\nStarting from the fundamental base of the discrete energy balance and the algebraic splitting $A = D - L - U$, derive how ordering maps to a permutation similarity of $A$ and thus to a modified $G_{\\omega}$. Implement a program that:\n\n- Accepts as fixed internal data the following test suite of matrices, orderings, and relaxation parameters.\n- For each test case, forms the reordered matrix $\\tilde{A} = P A P^{\\mathsf{T}}$ for the specified ordering, constructs $D$, $L$, and $U$ with respect to that ordering, builds the SOR iteration matrix $G_{\\omega}$, evaluates $\\rho(G_{\\omega})$, and returns a boolean indicating convergence ($\\rho(G_{\\omega})  1$) or divergence ($\\rho(G_{\\omega}) \\ge 1$).\n\nUse the following test suite, designed to cover different facets including a symmetric positive definite \"happy path\", a nonsymmetric convection-dominated case, and boundary behavior, with explicitly stated parameters:\n\n- Test case $1$: Symmetric positive definite conduction, $n = 20$, tridiagonal with diagonal $2$ and off-diagonals $-1$; natural ordering; $\\omega = 1.8$.\n- Test case $2$: Same as test case $1$ but reverse ordering; $\\omega = 1.8$.\n- Test case $3$: Convection-dominated nonsymmetric system, $n = 20$, tridiagonal with diagonal $4.0$, lower off-diagonal $-0.05$, and upper off-diagonal $-3.9$; natural ordering; $\\omega = 1.2$.\n- Test case $4$: Same as test case $3$ but reverse ordering; $\\omega = 1.2$.\n- Test case $5$: Same as test case $3$ but reverse ordering; $\\omega = 1.05$.\n- Test case $6$: Same as test case $3$; natural ordering; underrelaxation with $\\omega = 0.9$.\n\nDefinitions:\n\n- Natural ordering is the identity permutation $P = I$ on indices $\\{1,2,\\dots,n\\}$.\n- Reverse ordering is the permutation that maps index $i$ to $n+1-i$.\n\nYour program should produce a single line of output containing the results for the six test cases as a comma-separated list enclosed in square brackets (e.g., \"[true,true,false,...]\"). Each entry must be a boolean indicating convergence (true) or divergence (false) for the corresponding test case, in the order listed above. No physical units are needed for these algebraic evaluations. Angles are not involved. Percentages are not used; answer entries are booleans only.",
            "solution": "The central task is to determine the convergence of the Successive Overrelaxation (SOR) iterative method for several linear systems derived from computational thermal engineering problems. The convergence of the SOR method is dictated by the spectral radius of its associated iteration matrix, $G_{\\omega}$. The method converges if and only if the spectral radius $\\rho(G_{\\omega})$ is strictly less than $1$.\n\nThe problem focuses on how the ordering of the unknowns in the linear system $A \\mathbf{T} = \\mathbf{b}$ affects this convergence, particularly for nonsymmetric matrices arising from convection-dominated phenomena. An ordering of the $n$ unknowns is represented by a permutation matrix $P$ of size $n \\times n$. Applying this ordering transforms the original system into an equivalent one:\n$$\n(P A P^{\\mathsf{T}}) (P \\mathbf{T}) = P \\mathbf{b}\n$$\nLet $\\tilde{A} = P A P^{\\mathsf{T}}$, $\\tilde{\\mathbf{T}} = P \\mathbf{T}$, and $\\tilde{\\mathbf{b}} = P \\mathbf{b}$. The SOR method is then applied to the reordered system $\\tilde{A} \\tilde{\\mathbf{T}} = \\tilde{\\mathbf{b}}$.\n\nThe SOR method is based on a splitting of the coefficient matrix. For the reordered matrix $\\tilde{A}$, the splitting is $\\tilde{A} = \\tilde{D} - \\tilde{L} - \\tilde{U}$, where:\n- $\\tilde{D}$ is the diagonal part of $\\tilde{A}$.\n- $-\\tilde{L}$ is the strictly lower triangular part of $\\tilde{A}$. Thus, $\\tilde{L}$ is the negative of the strictly lower triangular part of $\\tilde{A}$.\n- $-\\tilde{U}$ is the strictly upper triangular part of $\\tilde{A}$. Thus, $\\tilde{U}$ is the negative of the strictly upper triangular part of $\\tilde{A}$.\n\nThe SOR iteration for the reordered system is given by:\n$$\n\\tilde{\\mathbf{T}}^{(k+1)} = (\\tilde{D} - \\omega \\tilde{L})^{-1} \\left( (1 - \\omega) \\tilde{D} + \\omega \\tilde{U} \\right) \\tilde{\\mathbf{T}}^{(k)} + (\\tilde{D} - \\omega \\tilde{L})^{-1} \\omega \\tilde{\\mathbf{b}}\n$$\nwhere $\\omega$ is the relaxation parameter and $k$ is the iteration index. The convergence of this process depends on the spectral radius of the SOR iteration matrix for the reordered system, $\\tilde{G}_{\\omega}$, defined as:\n$$\n\\tilde{G}_{\\omega} = (\\tilde{D} - \\omega \\tilde{L})^{-1} \\left( (1 - \\omega) \\tilde{D} + \\omega \\tilde{U} \\right)\n$$\nThe convergence condition is $\\rho(\\tilde{G}_{\\omega})  1$.\n\nTo evaluate the convergence for each test case, we follow a systematic computational procedure:\n$1$. Construct the base matrix $A$ of size $n \\times n$ as specified. For a tridiagonal matrix with main diagonal $d$, lower diagonal $l$, and upper diagonal $u$, the entries are $A_{ii} = d$, $A_{i, i-1} = l$, and $A_{i, i+1} = u$.\n$2$. Construct the permutation matrix $P$ for the specified ordering.\n    - For natural ordering, $P$ is the identity matrix $I$.\n    - For reverse ordering, $P$ is the anti-diagonal matrix with entries $P_{i, n+1-i} = 1$ for $i=1, \\dots, n$ and all other entries equal to $0$.\n$3$. Compute the reordered matrix $\\tilde{A} = P A P^{\\mathsf{T}}$.\n$4$. Decompose $\\tilde{A}$ to obtain the matrices $\\tilde{D}$, $\\tilde{L}$, and $\\tilde{U}$. Specifically:\n    - $\\tilde{D}$ is a diagonal matrix containing the diagonal entries of $\\tilde{A}$.\n    - $\\tilde{L}$ is the negative of the strictly lower triangular part of $\\tilde{A}$.\n    - $\\tilde{U}$ is the negative of the strictly upper triangular part of $\\tilde{A}$.\n$5$. Assemble the SOR iteration matrix $\\tilde{G}_{\\omega}$ using the given value of $\\omega$.\n$6$. Compute the eigenvalues, $\\lambda_i$, of $\\tilde{G}_{\\omega}$.\n$7$. Calculate the spectral radius $\\rho(\\tilde{G}_{\\omega}) = \\max_i |\\lambda_i|$.\n$8$. Determine convergence by testing if $\\rho(\\tilde{G}_{\\omega})  1$. The result is a boolean value: `true` for convergence, `false` for divergence.\n\nThis procedure is applied to each of the six test cases.\n\n- For test cases $1$ and $2$, the matrix $A$ is symmetric positive definite (SPD). For such matrices, the SOR method is known to converge for any relaxation parameter $\\omega \\in (0, 2)$. Since the permutation similarity transform $P A P^{\\mathsf{T}}$ preserves the SPD property, convergence is expected for both natural and reverse orderings with $\\omega = 1.8$.\n\n- For test cases $3-6$, the matrix $A$ is nonsymmetric and strictly diagonally dominant, corresponding to a convection-dominated problem with upwind differencing. Such matrices are typically M-matrices, for which SOR convergence is guaranteed for any ordering if $\\omega \\in (0, 1]$. This predicts convergence for test case $6$ where $\\omega = 0.9$. For overrelaxation ($\\omega  1$), convergence is not guaranteed and becomes highly sensitive to the ordering of unknowns. The \"natural\" ordering ($1, 2, \\dots, n$) constitutes a downstream sweep relative to the direction of strong coupling (from high index to low index), which is known to be unstable for SOR with $\\omega  1$. Conversely, the \"reverse\" ordering ($n, n-1, \\dots, 1$) constitutes an upstream sweep, which aligns with the flow of information and typically results in stable and rapid convergence. This reasoning suggests divergence for test case $3$ ($\\omega = 1.2$, natural ordering) and convergence for test cases $4$ and $5$ ($\\omega = 1.2$ and $\\omega=1.05$, reverse ordering).\n\nThe final program implements this complete validation procedure for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the problem by evaluating SOR convergence for a suite of test cases.\n    \"\"\"\n\n    def construct_tridiagonal_matrix(n, diag_val, lower_val, upper_val):\n        \"\"\"Constructs a tridiagonal matrix of size n x n.\"\"\"\n        diag = np.full(n, diag_val)\n        lower = np.full(n - 1, lower_val)\n        upper = np.full(n - 1, upper_val)\n        return np.diag(diag) + np.diag(lower, k=-1) + np.diag(upper, k=1)\n\n    def check_sor_convergence(A, ordering, omega):\n        \"\"\"\n        Checks the convergence of the SOR method for a given matrix, ordering, and omega.\n\n        Args:\n            A (np.ndarray): The base coefficient matrix.\n            ordering (str): The ordering of unknowns, 'natural' or 'reverse'.\n            omega (float): The relaxation parameter.\n\n        Returns:\n            bool: True if the method converges, False otherwise.\n        \"\"\"\n        n = A.shape[0]\n\n        # 1. Construct the permutation matrix P\n        if ordering == 'natural':\n            P = np.identity(n)\n        elif ordering == 'reverse':\n            P = np.fliplr(np.identity(n))\n        else:\n            raise ValueError(\"Unknown ordering type\")\n\n        # 2. Compute the reordered matrix A_tilde\n        A_tilde = P @ A @ P.T\n\n        # 3. Decompose A_tilde into D, L, U\n        # Note the convention: A = D - L - U\n        D_tilde = np.diag(np.diag(A_tilde))\n        L_tilde = -np.tril(A_tilde, k=-1)\n        U_tilde = -np.triu(A_tilde, k=1)\n\n        # 4. Construct the SOR iteration matrix G_omega\n        # G_omega = inv(D - omega*L) * ((1-omega)*D + omega*U)\n        # Check if D - omega*L is invertible\n        inv_term = D_tilde - omega * L_tilde\n        try:\n            np.linalg.inv(inv_term)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, spectral radius is effectively infinite\n            return False\n\n        G_omega = np.linalg.inv(inv_term) @ ((1 - omega) * D_tilde + omega * U_tilde)\n\n        # 5. Compute the spectral radius\n        eigenvalues = np.linalg.eigvals(G_omega)\n        spectral_radius = np.max(np.abs(eigenvalues))\n\n        # 6. Check for convergence\n        return spectral_radius  1.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 20, 'diag': 2.0, 'lower': -1.0, 'upper': -1.0, 'ordering': 'natural', 'omega': 1.8}, # Case 1\n        {'n': 20, 'diag': 2.0, 'lower': -1.0, 'upper': -1.0, 'ordering': 'reverse', 'omega': 1.8}, # Case 2\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'natural', 'omega': 1.2}, # Case 3\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'reverse', 'omega': 1.2}, # Case 4\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'reverse', 'omega': 1.05},# Case 5\n        {'n': 20, 'diag': 4.0, 'lower': -0.05, 'upper': -3.9, 'ordering': 'natural', 'omega': 0.9}  # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        # Construct the base matrix A\n        A = construct_tridiagonal_matrix(case['n'], case['diag'], case['lower'], case['upper'])\n        \n        # Check convergence and store the result\n        converges = check_sor_convergence(A, case['ordering'], case['omega'])\n        results.append(str(converges).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}