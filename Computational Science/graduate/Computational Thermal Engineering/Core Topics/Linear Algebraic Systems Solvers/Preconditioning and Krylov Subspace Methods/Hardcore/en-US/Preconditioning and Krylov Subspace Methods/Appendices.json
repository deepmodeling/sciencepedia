{
    "hands_on_practices": [
        {
            "introduction": "Our journey into preconditioning begins by understanding the root of the problem: ill-conditioning that arises from discretizing partial differential equations. This exercise guides you through deriving the discrete matrix for a 2D heat conduction problem and then analyzing the convergence of a basic iterative solver . By linking the physical grid size to the mathematical spectral radius, you will see firsthand why finer meshes lead to slower convergence, establishing the fundamental motivation for preconditioning.",
            "id": "3979820",
            "problem": "Consider the two-dimensional steady heat conduction equation with spatially varying thermal conductivity,\n$$\n-\\nabla \\cdot \\big(k(x,y)\\,\\nabla T(x,y)\\big) \\;=\\; f(x,y)\n$$\nposed on the unit square domain $\\Omega=(0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions $T=0$ on $\\partial\\Omega$. Let the domain be partitioned by a uniform Cartesian grid with $n$ interior points per coordinate direction and spacing $h=1/(n+1)$. Use a collocated arrangement where the unknowns $T_{i,j}\\approx T(x_i,y_j)$ are defined at the grid points $(x_i,y_j)=(i\\,h,j\\,h)$ for $i,j=1,\\dots,n$. \n\nTasks:\n1. Starting from conservation of energy and Fourier’s law, derive the five-point discrete operator that results from a control-volume (finite-volume-equivalent) discretization of $-\\nabla\\cdot(k\\nabla T)=f$ on each interior control volume. Your derivation must employ the divergence theorem on a rectangular control volume surrounding $(x_i,y_j)$ and centered second-order differences for normal temperature gradients at faces. To ensure physical consistency (heat flux continuity) and symmetry of the resulting linear system, evaluate the conductivity at cell faces using the harmonic average of nodal conductivities straddling that face. Explicitly write the $5$-point stencil coefficients at an interior node $(i,j)$ in terms of the face conductivities $k_{e}$, $k_{w}$, $k_{n}$, and $k_{s}$ located at the east, west, north, and south faces of the control volume, respectively, and show how $k$ enters these coefficients.\n\n2. Denote by $A\\in\\mathbb{R}^{n^2\\times n^2}$ the sparse matrix associated with the assembled system $A\\,\\mathbf{T}=\\mathbf{f}$ produced by your stencil, ordered lexicographically. Consider now the special case of constant conductivity $k(x,y)\\equiv k_{0}>0$. Let $M=\\mathrm{diag}(A)$ be the diagonal (Jacobi) preconditioner. Analyze the stationary preconditioned Richardson iteration\n$$\n\\mathbf{T}^{(m+1)} \\;=\\; \\mathbf{T}^{(m)} \\;+\\; \\omega\\,M^{-1}\\big(\\mathbf{f}-A\\,\\mathbf{T}^{(m)}\\big),\n$$\nwith relaxation parameter $\\omega>0$, used as a smoother or as a simple preconditioned fixed-point iteration. By exploiting the structure of $A$ for the constant-coefficient case, determine the value of the minimal achievable spectral radius of the iteration matrix over all $\\omega>0$, expressed in closed form as a function of $n$ only.\n\nProvide your final answer as a single closed-form expression in terms of $n$. No numerical evaluation is required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary information is provided for a unique solution. The problem is a standard exercise in numerical methods for partial differential equations and is valid. We proceed with the solution.\n\n### Part 1: Derivation of the Five-Point Discrete Operator\n\nThe governing partial differential equation is the steady-state heat conduction equation with a source term $f(x,y)$ and spatially varying thermal conductivity $k(x,y)$:\n$$-\\nabla \\cdot \\big(k(x,y)\\,\\nabla T(x,y)\\big) \\;=\\; f(x,y)$$\nTo discretize this equation using the finite volume method, we integrate it over a control volume $V_{i,j}$ surrounding a grid node $(x_i, y_j)$. The control volume is a square of side length $h$, centered at $(x_i, y_j)$, with faces located at $x = x_i \\pm h/2$ and $y = y_j \\pm h/2$. The integral form of the equation is:\n$$-\\int_{V_{i,j}} \\nabla \\cdot (k \\nabla T) \\,dV \\;=\\; \\int_{V_{i,j}} f \\,dV$$\nApplying the divergence theorem to the left-hand side converts the volume integral into a surface integral over the boundary $\\partial V_{i,j}$ of the control volume. The heat flux vector is defined by Fourier's law as $\\mathbf{q} = -k\\nabla T$.\n$$-\\int_{\\partial V_{i,j}} (k \\nabla T) \\cdot \\mathbf{n} \\,dS \\;=\\; \\int_{V_{i,j}} f \\,dV \\quad\\implies\\quad \\int_{\\partial V_{i,j}} \\mathbf{q} \\cdot \\mathbf{n} \\,dS \\;=\\; \\int_{V_{i,j}} f \\,dV$$\nThe integral on the left represents the net heat flow out of the control volume. In two dimensions, the control volume has an area $h^2$, and its boundary consists of four faces (east, west, north, south), each of length $h$. We can approximate the integral equation as:\n$$Q_e + Q_w + Q_n + Q_s \\; \\approx \\; \\overline{f}_{i,j} h^2$$\nwhere $Q_e, Q_w, Q_n, Q_s$ are the total heat flows out of the east, west, north, and south faces, respectively, and $\\overline{f}_{i,j}$ is the average value of $f$ over the control volume, which we approximate by $f_{i,j} = f(x_i, y_j)$. The total heat flow through a face is the product of the average flux normal to the face and the face area (in 2D, length).\n$$Q_e = (q_x)_e h, \\quad Q_w = -(q_x)_w h, \\quad Q_n = (q_y)_n h, \\quad Q_s = -(q_y)_s h$$\nwhere $(q_x)_e$ is the x-component of the flux at the east face, and so on. Note the sign for the west and south faces as the outward normal vectors are in the negative coordinate directions. The energy balance becomes:\n$$(q_x)_e h - (q_x)_w h + (q_y)_n h - (q_y)_s h \\; = \\; f_{i,j} h^2$$\nNext, we approximate the fluxes at the cell faces using centered second-order finite differences for the temperature gradients:\n\\begin{align*}\n(q_x)_e &= \\left(-k \\frac{\\partial T}{\\partial x}\\right)_{e} \\approx -k_e \\frac{T_{i+1,j} - T_{i,j}}{h} \\\\\n(q_x)_w &= \\left(-k \\frac{\\partial T}{\\partial x}\\right)_{w} \\approx -k_w \\frac{T_{i,j} - T_{i-1,j}}{h} \\\\\n(q_y)_n &= \\left(-k \\frac{\\partial T}{\\partial y}\\right)_{n} \\approx -k_n \\frac{T_{i,j+1} - T_{i,j}}{h} \\\\\n(q_y)_s &= \\left(-k \\frac{\\partial T}{\\partial y}\\right)_{s} \\approx -k_s \\frac{T_{i,j} - T_{i,j-1}}{h}\n\\end{align*}\nHere, $k_e, k_w, k_n, k_s$ are the thermal conductivities at the east, west, north, and south faces of the control volume. Substituting these expressions into the energy balance equation:\n$$ -k_e \\frac{T_{i+1,j} - T_{i,j}}{h}h - \\left(-k_w \\frac{T_{i,j} - T_{i-1,j}}{h}\\right)h -k_n \\frac{T_{i,j+1} - T_{i,j}}{h}h - \\left(-k_s \\frac{T_{i,j} - T_{i,j-1}}{h}\\right)h \\; = \\; f_{i,j} h^2 $$\nSimplifying and rearranging terms to group by temperature nodes:\n$$ k_w(T_{i,j} - T_{i-1,j}) + k_e(T_{i,j} - T_{i+1,j}) + k_s(T_{i,j} - T_{i,j-1}) + k_n(T_{i,j} - T_{i,j+1}) \\; = \\; f_{i,j} h^2 $$\n$$ (k_e+k_w+k_n+k_s)T_{i,j} - k_w T_{i-1,j} - k_e T_{i+1,j} - k_s T_{i,j-1} - k_n T_{i,j+1} \\; = \\; f_{i,j} h^2 $$\nThis equation defines the row of the linear system $A\\mathbf{T}=\\mathbf{f}$ corresponding to the interior node $(i,j)$. The five-point stencil coefficients for the operator at node $(i,j)$ are:\n\\begin{itemize}\n    \\item Center ($T_{i,j}$): $c_{C} = k_e+k_w+k_n+k_s$\n    \\item West ($T_{i-1,j}$): $c_{W} = -k_w$\n    \\item East ($T_{i+1,j}$): $c_{E} = -k_e$\n    \\item South ($T_{i,j-1}$): $c_{S} = -k_s$\n    \\item North ($T_{i,j+1}$): $c_{N} = -k_n$\n\\end{itemize}\nTo ensure heat flux continuity and a symmetric matrix $A$, the face conductivities are computed using the harmonic average of the nodal conductivities $k_{i,j} = k(x_i, y_j)$ straddling the face:\n\\begin{align*}\nk_e &= \\frac{2 k_{i,j} k_{i+1,j}}{k_{i,j} + k_{i+1,j}} \\\\\nk_w &= \\frac{2 k_{i-1,j} k_{i,j}}{k_{i-1,j} + k_{i,j}} \\\\\nk_n &= \\frac{2 k_{i,j} k_{i,j+1}}{k_{i,j} + k_{i,j+1}} \\\\\nk_s &= \\frac{2 k_{i,j-1} k_{i,j}}{k_{i,j-1} + k_{i,j}}\n\\end{align*}\nThis completes the derivation for the first part of the problem.\n\n### Part 2: Analysis of the Preconditioned Richardson Iteration\n\nThe stationary preconditioned Richardson iteration is given by:\n$$\\mathbf{T}^{(m+1)} \\;=\\; \\mathbf{T}^{(m)} \\;+\\; \\omega\\,M^{-1}\\big(\\mathbf{f}-A\\,\\mathbf{T}^{(m)}\\big)$$\nThe iteration matrix is $G_\\omega = I - \\omega M^{-1} A$. We aim to find $\\min_{\\omega > 0} \\rho(G_\\omega)$, where $\\rho(\\cdot)$ denotes the spectral radius.\n\nFor the special case of constant conductivity, $k(x,y) = k_0 > 0$, all nodal conductivities are $k_{i,j}=k_0$. The face conductivities become $k_e = k_w = k_n = k_s = k_0$. The discrete equation from Part 1 simplifies to:\n$$ (k_0+k_0+k_0+k_0)T_{i,j} - k_0 T_{i-1,j} - k_0 T_{i+1,j} - k_0 T_{i,j-1} - k_0 T_{i,j+1} \\; = \\; h^2 f_{i,j} $$\n$$ k_0(4T_{i,j} - T_{i-1,j} - T_{i+1,j} - T_{i,j-1} - T_{i,j+1}) \\; = \\; h^2 f_{i,j} $$\nThe system matrix $A$ in $A\\mathbf{T}=\\mathbf{f}$ (with $\\mathbf{f}$ representing the scaled source term vector with entries $h^2 f_{i,j}$) has diagonal entries of $4k_0$ and off-diagonal entries of $-k_0$ for each of the four neighbors. Thus, $A = k_0 A_{Lap}$, where $A_{Lap}$ is the standard matrix representation of the 5-point stencil for the negative discrete Laplacian on an $n \\times n$ grid, having $4$ on the diagonal and $-1$ on the off-diagonals corresponding to neighbors.\n\nThe preconditioner is the diagonal of $A$, so $M = \\mathrm{diag}(A) = 4k_0 I$, where $I$ is the $n^2 \\times n^2$ identity matrix.\nThe matrix $M^{-1}A$ is:\n$$ M^{-1}A = (4k_0 I)^{-1} (k_0 A_{Lap}) = \\frac{1}{4k_0} I k_0 A_{Lap} = \\frac{1}{4} A_{Lap} $$\nThe eigenvalues of $A_{Lap}$ for an $n \\times n$ grid with homogeneous Dirichlet boundary conditions are well-known:\n$$ \\lambda_{p,q}(A_{Lap}) = 4 - 2\\cos\\left(\\frac{p\\pi}{n+1}\\right) - 2\\cos\\left(\\frac{q\\pi}{n+1}\\right), \\quad \\text{for } p,q=1, \\dots, n. $$\nThe eigenvalues of the matrix $M^{-1}A$, denoted by $\\nu_{p,q}$, are therefore:\n$$ \\nu_{p,q} = \\frac{1}{4}\\lambda_{p,q}(A_{Lap}) = 1 - \\frac{1}{2}\\cos\\left(\\frac{p\\pi}{n+1}\\right) - \\frac{1}{2}\\cos\\left(\\frac{q\\pi}{n+1}\\right). $$\nSince the matrix $A$ is symmetric positive definite, the eigenvalues $\\nu_{p,q}$ are real and positive. The eigenvalues of the iteration matrix $G_\\omega$ are $\\mu_{p,q}(\\omega) = 1 - \\omega \\nu_{p,q}$. The spectral radius is $\\rho(G_\\omega) = \\max_{p,q} |1 - \\omega \\nu_{p,q}|$.\nTo minimize this spectral radius, we need the minimum and maximum eigenvalues of $M^{-1}A$.\nThe minimum eigenvalue $\\nu_{\\min}$ occurs when the cosine terms are maximized (i.e., for $p=1, q=1$):\n$$ \\nu_{\\min} = 1 - \\frac{1}{2}\\cos\\left(\\frac{\\pi}{n+1}\\right) - \\frac{1}{2}\\cos\\left(\\frac{\\pi}{n+1}\\right) = 1 - \\cos\\left(\\frac{\\pi}{n+1}\\right). $$\nThe maximum eigenvalue $\\nu_{\\max}$ occurs when the cosine terms are minimized (i.e., for $p=n, q=n$). Using $\\cos(\\frac{n\\pi}{n+1}) = \\cos(\\pi - \\frac{\\pi}{n+1}) = -\\cos(\\frac{\\pi}{n+1})$:\n$$ \\nu_{\\max} = 1 - \\frac{1}{2}\\cos\\left(\\frac{n\\pi}{n+1}\\right) - \\frac{1}{2}\\cos\\left(\\frac{n\\pi}{n+1}\\right) = 1 - \\cos\\left(\\frac{n\\pi}{n+1}\\right) = 1 + \\cos\\left(\\frac{\\pi}{n+1}\\right). $$\nFor Richardson iteration with a matrix whose eigenvalues $\\nu$ are in $[\\nu_{\\min}, \\nu_{\\max}]$, the optimal relaxation parameter $\\omega_{\\text{opt}}$ that minimizes $\\max_\\nu |1-\\omega\\nu|$ is $\\omega_{\\text{opt}} = \\frac{2}{\\nu_{\\min} + \\nu_{\\max}}$. The minimal spectral radius is given by:\n$$ \\rho_{\\min} = \\frac{\\nu_{\\max} - \\nu_{\\min}}{\\nu_{\\max} + \\nu_{\\min}}. $$\nLet's compute this value. The sum of the extremal eigenvalues is:\n$$ \\nu_{\\min} + \\nu_{\\max} = \\left(1 - \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right) + \\left(1 + \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right) = 2. $$\nThe difference of the extremal eigenvalues is:\n$$ \\nu_{\\max} - \\nu_{\\min} = \\left(1 + \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right) - \\left(1 - \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right) = 2\\cos\\left(\\frac{\\pi}{n+1}\\right). $$\nSubstituting these into the formula for the minimal spectral radius:\n$$ \\rho_{\\min} = \\frac{2\\cos\\left(\\frac{\\pi}{n+1}\\right)}{2} = \\cos\\left(\\frac{\\pi}{n+1}\\right). $$\nThis is the minimal achievable spectral radius of the iteration matrix, expressed as a closed-form function of $n$.",
            "answer": "$$\\boxed{\\cos\\left(\\frac{\\pi}{n+1}\\right)}$$"
        },
        {
            "introduction": "Having established why preconditioning is necessary, we now focus on how to construct a simple yet powerful preconditioner. This problem asks you to perform an Incomplete LU factorization with zero fill-in (ILU(0)) for a small matrix representing a discretized diffusion operator . This hands-on calculation demystifies the ILU process and quantifies the approximation error, illustrating the fundamental trade-off between maintaining sparsity and accurately capturing the inverse of the original matrix.",
            "id": "3979814",
            "problem": "A square plate of side length $L$ with constant thermal conductivity $k$ is held at a prescribed temperature on its boundary, so that the interior temperature field $T(x,y)$ satisfies steady conduction with no internal heat generation, governed by the conservation law $-\\nabla \\cdot (k \\nabla T) = 0$. Discretize the governing equation on a uniform Cartesian grid with spacing $h$ using the classical five-point finite difference stencil to obtain the linear system $A \\mathbf{u} = \\mathbf{b}$ for the unknown discrete interior temperatures $\\mathbf{u}$. Consider a minimal interior of $2 \\times 2$ unknowns (i.e., $4$ interior nodes) with lexicographic ordering by $x$ then $y$, and assume homogeneous Dirichlet data so that $\\mathbf{b} = \\mathbf{0}$. Up to an inconsequential uniform scaling by $k/h^{2}$, the coefficient matrix $A \\in \\mathbb{R}^{4 \\times 4}$ has the $5$-point diffusion stencil connectivity among interior nodes and is given by\n$$\nA \\;=\\; \\begin{pmatrix}\n4 & -1 & -1 & 0 \\\\\n-1 & 4 & 0 & -1 \\\\\n-1 & 0 & 4 & -1 \\\\\n0 & -1 & -1 & 4\n\\end{pmatrix}.\n$$\nConstruct the Incomplete Lower-Upper factorization with zero fill-in (ILU$(0)$) preconditioner $M = L U$, where $L$ is unit lower triangular and $U$ is upper triangular, under the constraint that the sparsity patterns of $L$ and $U$ match the strictly lower and the upper parts of $A$, respectively. Then, quantify how the sparsity constraint degrades the approximate inverse quality by computing the relative Frobenius factorization error\n$$\nr \\;=\\; \\frac{\\|A - L U\\|_{F}}{\\|A\\|_{F}}\\,.\n$$\nExpress your final answer as an exact value. Do not round. The answer must be a single real number without units.",
            "solution": "We begin from the steady energy conservation law for conduction with constant thermal conductivity, $-\\nabla \\cdot (k \\nabla T) = 0$. On a uniform Cartesian grid with spacing $h$, the standard five-point finite difference discretization at an interior node $(i,j)$ yields\n$$\n\\frac{k}{h^{2}}\\left(-T_{i-1,j} - T_{i+1,j} - T_{i,j-1} - T_{i,j+1} + 4 T_{i,j}\\right) \\;=\\; 0,\n$$\nwhich leads to a linear system $A \\mathbf{u} = \\mathbf{b}$, where, after absorbing the common factor $k/h^{2}$ into a uniform scaling of $A$ and $\\mathbf{b}$, the interior coupling among unknowns is captured by the $5$-point stencil. For a $2 \\times 2$ interior (thus $4$ unknowns) with lexicographic ordering by $x$ then $y$, the matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is\n$$\nA \\;=\\; \\begin{pmatrix}\n4 & -1 & -1 & 0 \\\\\n-1 & 4 & 0 & -1 \\\\\n-1 & 0 & 4 & -1 \\\\\n0 & -1 & -1 & 4\n\\end{pmatrix}.\n$$\nWe construct the Incomplete Lower-Upper with zero fill-in (ILU$(0)$) preconditioner $M = L U$ with the constraints:\n- $L$ is unit lower triangular and has nonzeros only where $A$ has nonzeros strictly below the diagonal, i.e., in positions $(2,1)$, $(3,1)$, $(4,2)$, $(4,3)$.\n- $U$ is upper triangular and has nonzeros only where $A$ has nonzeros on or above the diagonal, i.e., in positions $(1,1)$, $(1,2)$, $(1,3)$, $(2,2)$, $(2,4)$, $(3,3)$, $(3,4)$, $(4,4)$.\n\nWe use a Doolittle-type ILU$(0)$ process, which applies Gaussian elimination but drops all fill-in outside the prescribed sparsity of $L$ and $U$.\n\nParameterize\n$$\nL \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\ell_{21} & 1 & 0 & 0 \\\\\n\\ell_{31} & 0 & 1 & 0 \\\\\n0 & \\ell_{42} & \\ell_{43} & 1\n\\end{pmatrix}, \n\\quad\nU \\;=\\; \\begin{pmatrix}\nu_{11} & u_{12} & u_{13} & 0 \\\\\n0 & u_{22} & 0 & u_{24} \\\\\n0 & 0 & u_{33} & u_{34} \\\\\n0 & 0 & 0 & u_{44}\n\\end{pmatrix}.\n$$\nAt step $i = 1$, the upper row is\n$$\nu_{11} \\;=\\; A_{11} \\;=\\; 4,\\quad u_{12} \\;=\\; A_{12} \\;=\\; -1,\\quad u_{13} \\;=\\; A_{13} \\;=\\; -1.\n$$\nThe multipliers in column $1$ are\n$$\n\\ell_{21} \\;=\\; \\frac{A_{21}}{u_{11}} \\;=\\; \\frac{-1}{4} \\;=\\; -\\frac{1}{4}, \\qquad\n\\ell_{31} \\;=\\; \\frac{A_{31}}{u_{11}} \\;=\\; \\frac{-1}{4} \\;=\\; -\\frac{1}{4}.\n$$\nAt step $i = 2$, for the upper entries in the pattern we compute\n$$\nu_{22} \\;=\\; A_{22} - \\ell_{21} u_{12} \\;=\\; 4 - \\left(-\\frac{1}{4}\\right)(-1) \\;=\\; 4 - \\frac{1}{4} \\;=\\; \\frac{15}{4},\n$$\n$$\nu_{24} \\;=\\; A_{24} - \\ell_{21} u_{14} \\;=\\; -1 - \\left(-\\frac{1}{4}\\right)\\cdot 0 \\;=\\; -1.\n$$\nThe only allowed lower entry beneath column $2$ is $\\ell_{42}$, given by\n$$\n\\ell_{42} \\;=\\; \\frac{A_{42} - \\ell_{41} u_{12}}{u_{22}} \\;=\\; \\frac{-1 - 0 \\cdot (-1)}{15/4} \\;=\\; -\\frac{4}{15},\n$$\nwhere $\\ell_{41} = 0$ is enforced by the ILU$(0)$ sparsity constraint at position $(4,1)$.\n\nAt step $i = 3$, for the allowed upper entries we have\n$$\nu_{33} \\;=\\; A_{33} - \\left(\\ell_{31} u_{13} + \\ell_{32} u_{23}\\right) \\;=\\; 4 - \\left(\\left(-\\frac{1}{4}\\right)(-1) + 0 \\cdot 0\\right) \\;=\\; 4 - \\frac{1}{4} \\;=\\; \\frac{15}{4},\n$$\n$$\nu_{34} \\;=\\; A_{34} - \\left(\\ell_{31} u_{14} + \\ell_{32} u_{24}\\right) \\;=\\; -1 - \\left(\\left(-\\frac{1}{4}\\right)\\cdot 0 + 0 \\cdot (-1)\\right) \\;=\\; -1,\n$$\nwhere $\\ell_{32} = 0$ and $u_{23} = 0$ follow from ILU$(0)$ sparsity. The only allowed lower entry beneath column $3$ is\n$$\n\\ell_{43} \\;=\\; \\frac{A_{43} - \\left(\\ell_{41} u_{13} + \\ell_{42} u_{23}\\right)}{u_{33}} \\;=\\; \\frac{-1 - \\left(0 \\cdot (-1) + \\left(-\\frac{4}{15}\\right)\\cdot 0\\right)}{15/4} \\;=\\; -\\frac{4}{15}.\n$$\nAt step $i = 4$, the final diagonal entry is\n$$\nu_{44} \\;=\\; A_{44} - \\left(\\ell_{41} u_{14} + \\ell_{42} u_{24} + \\ell_{43} u_{34}\\right) \\;=\\; 4 - \\left(0 \\cdot 0 + \\left(-\\frac{4}{15}\\right)(-1) + \\left(-\\frac{4}{15}\\right)(-1)\\right) \\;=\\; 4 - \\frac{8}{15} \\;=\\; \\frac{52}{15}.\n$$\nThus the ILU$(0)$ factors are\n$$\nL \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n-\\tfrac{1}{4} & 1 & 0 & 0 \\\\\n-\\tfrac{1}{4} & 0 & 1 & 0 \\\\\n0 & -\\tfrac{4}{15} & -\\tfrac{4}{15} & 1\n\\end{pmatrix}, \n\\quad\nU \\;=\\; \\begin{pmatrix}\n4 & -1 & -1 & 0 \\\\\n0 & \\tfrac{15}{4} & 0 & -1 \\\\\n0 & 0 & \\tfrac{15}{4} & -1 \\\\\n0 & 0 & 0 & \\tfrac{52}{15}\n\\end{pmatrix}.\n$$\nTo show how the sparsity constraint affects the approximation quality, we examine the factorization residual $E = A - L U$. By construction of ILU$(0)$, $L U$ matches $A$ on the nonzero pattern of $A$, and any discrepancy can only appear at positions that would be fill-in under exact Gaussian elimination but are forbidden here. The only such positions are $(2,3)$ and $(3,2)$. We compute\n$$\n(LU)_{23} \\;=\\; L_{21} U_{13} + L_{22} U_{23} \\;=\\; \\left(-\\frac{1}{4}\\right)\\cdot (-1) + 1 \\cdot 0 \\;=\\; \\frac{1}{4},\n$$\nso\n$$\nE_{23} \\;=\\; A_{23} - (LU)_{23} \\;=\\; 0 - \\frac{1}{4} \\;=\\; -\\frac{1}{4}.\n$$\nBy symmetry of the construction,\n$$\n(LU)_{32} \\;=\\; L_{31} U_{12} + L_{32} U_{22} \\;=\\; \\left(-\\frac{1}{4}\\right)\\cdot (-1) + 0 \\cdot \\frac{15}{4} \\;=\\; \\frac{1}{4},\n$$\nso\n$$\nE_{32} \\;=\\; A_{32} - (LU)_{32} \\;=\\; 0 - \\frac{1}{4} \\;=\\; -\\frac{1}{4}.\n$$\nAll other entries of $E$ are zero. Therefore,\n$$\n\\|E\\|_{F}^{2} \\;=\\; \\left(-\\frac{1}{4}\\right)^{2} + \\left(-\\frac{1}{4}\\right)^{2} \\;=\\; \\frac{1}{16} + \\frac{1}{16} \\;=\\; \\frac{1}{8},\n$$\nso\n$$\n\\|E\\|_{F} \\;=\\; \\sqrt{\\frac{1}{8}} \\;=\\; \\frac{1}{2 \\sqrt{2}}.\n$$\nNext, compute the Frobenius norm of $A$:\n$$\n\\|A\\|_{F}^{2} \\;=\\; 4 \\cdot 4^2 + 8 \\cdot (-1)^2 \\;=\\; 64 + 8 \\;=\\; 72,\n$$\nso\n$$\n\\|A\\|_{F} \\;=\\; \\sqrt{72} \\;=\\; 6 \\sqrt{2}.\n$$\nTherefore, the relative Frobenius factorization error is\n$$\nr \\;=\\; \\frac{\\|A - L U\\|_{F}}{\\|A\\|_{F}} \\;=\\; \\frac{1/(2 \\sqrt{2})}{6 \\sqrt{2}} \\;=\\; \\frac{1}{24}.\n$$\nThis exact value isolates the effect of the ILU$(0)$ sparsity constraint: the entire error comes from the two dropped fill-in positions $(2,3)$ and $(3,2)$ associated with the $5$-point stencil’s natural fill pattern under this ordering. In the context of Krylov subspace methods such as the Generalized Minimal Residual (GMRES) method, smaller $r$ generally correlates with a preconditioned operator $M^{-1} A$ that is closer to the identity, which accelerates convergence; here, $r = 1/24$ quantifies the degradation due to enforcing zero fill-in at the two coupling locations.",
            "answer": "$$\\boxed{\\frac{1}{24}}$$"
        },
        {
            "introduction": "This final practice integrates the concepts of ill-conditioning and preconditioner construction into a complete algorithmic context. You will implement the Preconditioned Conjugate Gradient (PCG) method and test its performance with different preconditioners, from the simple identity (no preconditioning) to an exact inverse . This capstone exercise provides concrete evidence of how an effective preconditioner dramatically improves convergence by reducing the spectral condition number of the system, $\\kappa(M^{-1}A)$.",
            "id": "3979786",
            "problem": "Consider the steady-state heat conduction equation in a rectangular domain with homogeneous Dirichlet boundary conditions, where the thermal conductivity is constant but anisotropic. The governing partial differential equation is $$-\\nabla \\cdot \\left( \\mathbf{K} \\nabla T \\right) = q,$$ where $T$ is temperature, $\\mathbf{K} = \\operatorname{diag}(k_x,k_y)$ with $k_x>0$ and $k_y>0$, and $q$ is a volumetric heat source. Discretize this equation on a uniform Cartesian grid using a standard centered finite-difference scheme to obtain a linear system $$A x = b,$$ where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $x \\in \\mathbb{R}^n$ is the vector of nodal temperatures, and $b \\in \\mathbb{R}^n$ is the discrete source term. The discrete operator $A$ is a $5$-point stencil with off-diagonal weights proportional to $-k_x/h_x^2$ and $-k_y/h_y^2$, and diagonal weights proportional to $2(k_x/h_x^2 + k_y/h_y^2)$, where $h_x$ and $h_y$ are the mesh spacings.\n\nYou are to implement the Preconditioned Conjugate Gradient (PCG) algorithm for solving the linear system, using a general SPD preconditioner $M$. The preconditioning is to be applied in the standard manner for SPD problems, and the convergence is to be assessed using the Euclidean norm of the residual, i.e., stopping when $$\\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2} \\leq \\varepsilon,$$ where $r_k = b - A x_k$ and $\\varepsilon$ is a prescribed tolerance.\n\nStarting from first principles:\n- Derive how the SPD discretization arises from the heat equation and why $A$ is SPD for constant $k_x$ and $k_y$ with homogeneous Dirichlet boundary conditions.\n- Explain why PCG convergence for SPD $A$ depends on the spectrum of the symmetrically preconditioned operator $$S = M^{-1/2} A M^{-1/2},$$ which has the same eigenvalues as $M^{-1}A$, and not on the spectrum of $A$ alone.\n- Implement the PCG algorithm for a general SPD $M$ by using the action of $M^{-1}$ on a vector (i.e., applying the preconditioner through solves with $M$).\n- For each test case below, compute:\n  1. The number of iterations $k_{\\text{obs}}$ taken by PCG to reach the tolerance $\\varepsilon$ (with a zero initial guess).\n  2. The spectral condition number $$\\kappa(M^{-1}A) = \\frac{\\lambda_{\\max}(S)}{\\lambda_{\\min}(S)},$$ where $S = M^{-1/2} A M^{-1/2}$ and $\\lambda_{\\max}$, $\\lambda_{\\min}$ denote the largest and smallest eigenvalues respectively.\n  3. The worst-case upper bound $k_{\\text{bound}}$ on iterations needed to reach the tolerance $\\varepsilon$, given by the classical Conjugate Gradient estimate in terms of $\\kappa(M^{-1}A)$, namely choosing the smallest integer $k$ satisfying $$2 \\left( \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1} \\right)^k \\leq \\varepsilon.$$ For the special case $\\kappa=1$ (i.e., perfect preconditioning), set $k_{\\text{bound}} = 1$.\n\nUse the following test suite, which explores a general case, an anisotropic case with a simple diagonal preconditioner, and a limiting case with exact preconditioning:\n- Case $1$: $n_x = n_y = 16$, $k_x = 1$, $k_y = 1$, $M = I$ (identity preconditioner), tolerance $\\varepsilon = 10^{-8}$, maximum iterations $n$ (the dimension of $A$). This is the general \"happy path\" case.\n- Case $2$: $n_x = n_y = 16$, $k_x = 100$, $k_y = 1$, $M = \\operatorname{diag}(A)$ (Jacobi preconditioner), tolerance $\\varepsilon = 10^{-8}$, maximum iterations $n$. This case tests anisotropy and a simple SPD preconditioner.\n- Case $3$: $n_x = n_y = 16$, $k_x = 100$, $k_y = 1$, $M = A$ (exact SPD preconditioner implemented by Cholesky solves), tolerance $\\varepsilon = 10^{-8}$, maximum iterations $n$. This case is a boundary condition demonstrating ideal preconditioning.\n\nIn all cases, set $b$ to the vector with all entries equal to $1$ and the initial guess $x_0$ to the zero vector. The mesh spacings should be $h_x = h_y = 1/(n_x+1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries ordered by case and, for each case, listing $k_{\\text{obs}}$, $\\kappa(M^{-1}A)$, and $k_{\\text{bound}}$. Specifically, the output format must be\n$$[k_{\\text{obs},1},\\kappa_1,k_{\\text{bound},1},k_{\\text{obs},2},\\kappa_2,k_{\\text{bound},2},k_{\\text{obs},3},\\kappa_3,k_{\\text{bound},3}],$$\nwhere all entries are real numbers or integers and there is no additional output text. No physical units are required since the quantities are dimensionless iteration counts and spectral condition numbers.",
            "solution": "The provided problem is a well-defined exercise in computational science, requiring the application of the Preconditioned Conjugate Gradient (PCG) method to a discretized partial differential equation. It is scientifically grounded, formally specified, and internally consistent. We shall first derive the discrete system and its properties, explain the principles of PCG convergence, and then proceed to the numerical implementation and solution for the specified test cases.\n\n### 1. Discretization and Properties of the System Matrix\n\nThe governing equation is the steady-state heat conduction equation with an anisotropic thermal conductivity tensor $\\mathbf{K}$:\n$$ -\\nabla \\cdot \\left( \\mathbf{K} \\nabla T \\right) = q $$\nGiven $\\mathbf{K} = \\operatorname{diag}(k_x, k_y)$, the equation expands to:\n$$ -\\frac{\\partial}{\\partial x}\\left(k_x \\frac{\\partial T}{\\partial x}\\right) - \\frac{\\partial}{\\partial y}\\left(k_y \\frac{\\partial T}{\\partial y}\\right) = q $$\nWe discretize this equation on a uniform Cartesian grid with spacings $h_x$ and $h_y$. Let $T_{i,j}$ denote the temperature at the grid node $(x_i, y_j)$. We use a centered finite-difference scheme to approximate the derivatives at an interior node $(i,j)$.\n\nThe outer derivative $\\frac{\\partial}{\\partial x}(\\cdot)$ is approximated as:\n$$ \\left. \\frac{\\partial F}{\\partial x} \\right|_{i,j} \\approx \\frac{F_{i+1/2,j} - F_{i-1/2,j}}{h_x} $$\nwhere $F = k_x \\frac{\\partial T}{\\partial x}$. The terms at the half-grid points are then approximated as:\n$$ F_{i+1/2,j} = \\left. k_x \\frac{\\partial T}{\\partial x} \\right|_{i+1/2,j} \\approx k_x \\frac{T_{i+1,j} - T_{i,j}}{h_x} $$\n$$ F_{i-1/2,j} = \\left. k_x \\frac{\\partial T}{\\partial x} \\right|_{i-1/2,j} \\approx k_x \\frac{T_{i,j} - T_{i-1,j}}{h_x} $$\nSubstituting these back, the $x$-component of the divergence is:\n$$ -\\frac{\\partial}{\\partial x}\\left(k_x \\frac{\\partial T}{\\partial x}\\right)_{i,j} \\approx -\\frac{k_x}{h_x^2} \\left( (T_{i+1,j} - T_{i,j}) - (T_{i,j} - T_{i-1,j}) \\right) = \\frac{k_x}{h_x^2} (-T_{i+1,j} + 2T_{i,j} - T_{i-1,j}) $$\nAn analogous derivation for the $y$-component gives:\n$$ -\\frac{\\partial}{\\partial y}\\left(k_y \\frac{\\partial T}{\\partial y}\\right)_{i,j} \\approx \\frac{k_y}{h_y^2} (-T_{i,j+1} + 2T_{i,j} - T_{i,j-1}) $$\nCombining these terms and setting them equal to the source term $q_{i,j}$ at the node yields the discrete equation:\n$$ \\frac{k_x}{h_x^2} (2T_{i,j} - T_{i+1,j} - T_{i-1,j}) + \\frac{k_y}{h_y^2} (2T_{i,j} - T_{i,j+1} - T_{i,j-1}) = q_{i,j} $$\nRearranging the terms for $T_{i,j}$ gives the $5$-point stencil form:\n$$ \\left(\\frac{2k_x}{h_x^2} + \\frac{2k_y}{h_y^2}\\right)T_{i,j} - \\frac{k_x}{h_x^2}T_{i+1,j} - \\frac{k_x}{h_x^2}T_{i-1,j} - \\frac{k_y}{h_y^2}T_{i,j+1} - \\frac{k_y}{h_y^2}T_{i,j-1} = q_{i,j} $$\nHomogeneous Dirichlet boundary conditions ($T=0$ on the boundary) mean that for any node $(i,j)$ adjacent to a boundary, the corresponding neighbor term (e.g., $T_{i-1,j}$ if the node is on the left edge) is zero and is thus omitted from the equation.\n\nBy ordering the $n = n_x \\times n_y$ unknown interior temperatures into a vector $x$, these equations form a linear system $Ax=b$.\n\n**Symmetry and Positive Definiteness of $A$**:\n1.  **Symmetry**: The matrix $A$ is symmetric. The coefficient for the influence of node $l$ on the equation for node $k$ is identical to the coefficient for the influence of node $k$ on node $l$. For instance, for adjacent nodes $(i,j)$ and $(i+1,j)$, the entry in the row for $(i,j)$ corresponding to the variable for $(i+1,j)$ is $-k_x/h_x^2$. Likewise, the entry in the row for $(i+1,j)$ corresponding to the variable for $(i,j)$ is also $-k_x/h_x^2$. This holds for all neighboring pairs, thus $A=A^T$.\n\n2.  **Positive Definiteness**: Since $k_x > 0$ and $k_y > 0$, the diagonal entries of $A$, given by $A_{kk} = 2(k_x/h_x^2 + k_y/h_y^2)$, are strictly positive. The off-diagonal entries are all non-positive. The sum of the absolute values of the off-diagonal entries in any row $k$ is $\\sum_{j \\neq k} |A_{kj}|$. For a node not adjacent to any boundary, this sum equals the diagonal entry $A_{kk}$. For a node adjacent to a boundary, one or more neighbor terms are absent, so the sum is strictly less than the diagonal entry. Thus, $A$ is diagonally dominant. Furthermore, the matrix represents a connected graph of nodes, which makes it irreducible. An irreducibly diagonally dominant symmetric matrix with positive diagonal entries is strictly diagonally dominant for at least one row, and it is known to be positive definite. Therefore, $A$ is symmetric positive definite (SPD).\n\n### 2. PCG Convergence and the Role of Preconditioning\n\nThe Conjugate Gradient (CG) method is an iterative algorithm for solving SPD linear systems $Ax=b$. Its convergence rate is governed by the spectral condition number of the matrix $A$, $\\kappa(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$. The number of iterations $k$ required to reduce the error by a factor $\\varepsilon$ is roughly proportional to $\\sqrt{\\kappa(A)}$.\n\nPreconditioning aims to improve this convergence rate by transforming the original system into an equivalent one with a more favorable spectral distribution (i.e., a smaller condition number). For an SPD preconditioner $M$, we can define its Cholesky factorization $M=LL^T$. The system $Ax=b$ can be rewritten as:\n$$ L^{-1} A x = L^{-1} b \\implies (L^{-1} A (L^T)^{-1}) (L^T x) = L^{-1} b $$\nLet $S = L^{-1} A (L^T)^{-1}$, $\\hat{x} = L^T x$, and $\\hat{b} = L^{-1}b$. We now solve the transformed system $S \\hat{x} = \\hat{b}$ using the standard CG method. The matrix $S$ is SPD, so CG is applicable. The convergence of this process is governed by $\\kappa(S)$.\n\nThe eigenvalues of $S$ are the same as those of the matrix $M^{-1}A$. This can be seen via a similarity transform:\n$$ S = L^{-1} A L^{-T} \\implies L^T S L^{-T} = L^T(L^{-1} A L^{-T})L^{-T} = A (L^T L)^{-1} = A M^{-1} $$\nSince $AM^{-1}$ is similar to the symmetric matrix $M^{1/2}(AM^{-1})M^{-1/2} = M^{-1/2}AM^{-1/2}=S$, its eigenvalues are real and positive. Also, the eigenvalues of $AM^{-1}$ are the same as those of $M^{-1}A$. (For any invertible $P$, $PXP^{-1}$ has the same eigenvalues as $X$. Let $X=AM^{-1}$ and $P=M^{-1}$, then $M^{-1}(AM^{-1})M = M^{-1}A$, but this is more complex). A simpler argument is that $\\det(AB - \\lambda I) = \\det(A(B- \\lambda A^{-1})I) = \\det(A)\\det(B-\\lambda A^{-1})$ and $\\det(BA - \\lambda I) = \\det((B - \\lambda A^{-1})A) = \\det(B-\\lambda A^{-1})\\det(A)$, showing $AB$ and $BA$ have the same characteristic polynomial and thus the same eigenvalues.\n\nTherefore, the convergence of PCG depends on $\\kappa(S) = \\kappa(M^{-1}A)$. An effective preconditioner $M$ is one that approximates $A$, such that $M^{-1}A$ is close to the identity matrix $I$, making $\\kappa(M^{-1}A) \\approx 1$.\n\nThe PCG algorithm is formulated to avoid the explicit construction of $L$, $L^{-1}$, or $S$. It operates on the original vectors and matrices but includes a step to solve a system with the preconditioner, $z_k = M^{-1}r_k$, in each iteration $k$.\n\n### 3. Numerical Solution and Analysis\n\nThe implementation will consist of three main parts for each test case:\n1.  **Assembly**: Construct the matrix $A$ and vector $b$.\n2.  **PCG Execution**: Run the PCG algorithm with the specified preconditioner $M$ to find the number of iterations, $k_{\\text{obs}}$.\n3.  **Spectral Analysis**: Compute the eigenvalues of $M^{-1}A$ to determine the condition number $\\kappa(M^{-1}A)$ and the theoretical iteration bound $k_{\\text{bound}}$.\n\nThe Preconditioned Conjugate Gradient algorithm proceeds as follows ($x_0$ is the initial guess):\n$r_0 = b - Ax_0$\n$z_0 = M^{-1} r_0$\n$p_0 = z_0$\nFor $k = 0, 1, 2, \\dots$\n$\\quad \\alpha_k = (r_k^T z_k) / (p_k^T A p_k)$\n$\\quad x_{k+1} = x_k + \\alpha_k p_k$\n$\\quad r_{k+1} = r_k - \\alpha_k A p_k$\n$\\quad$ If converged: stop\n$\\quad z_{k+1} = M^{-1} r_{k+1}$\n$\\quad \\beta_k = (r_{k+1}^T z_{k+1}) / (r_k^T z_k)$\n$\\quad p_{k+1} = z_{k+1} + \\beta_k p_k$\n\nThe condition number $\\kappa(M^{-1}A)$ will be computed by finding the eigenvalues of the generalized eigenvalue problem $Av = \\lambda Mv$, which can be solved efficiently using `scipy.linalg.eigh`. The theoretical bound $k_{\\text{bound}}$ is the smallest integer $k$ satisfying $2 ((\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1))^k \\leq \\varepsilon$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to run test cases for the PCG algorithm on a discretized\n    heat equation problem.\n    \"\"\"\n\n    def build_A(nx, ny, kx, ky):\n        \"\"\"\n        Builds the sparse matrix A for the 2D heat equation using a 5-point\n        finite-difference stencil.\n        \"\"\"\n        n = nx * ny\n        hx = 1.0 / (nx + 1)\n        hy = 1.0 / (ny + 1)\n        A = np.zeros((n, n))\n\n        # Terms for the stencil\n        term_x = kx / (hx**2)\n        term_y = ky / (hy**2)\n        diag_val = 2.0 * (term_x + term_y)\n\n        for j in range(ny):\n            for i in range(nx):\n                k = j * nx + i  # Row-major mapping from 2D (i,j) to 1D k\n                \n                # Diagonal\n                A[k, k] = diag_val\n                \n                # Off-diagonals\n                # Left neighbor\n                if i > 0:\n                    k_left = j * nx + (i - 1)\n                    A[k, k_left] = -term_x\n                # Right neighbor\n                if i  nx - 1:\n                    k_right = j * nx + (i + 1)\n                    A[k, k_right] = -term_x\n                # Bottom neighbor\n                if j > 0:\n                    k_down = (j - 1) * nx + i\n                    A[k, k_down] = -term_y\n                # Top neighbor\n                if j  ny - 1:\n                    k_up = (j + 1) * nx + i\n                    A[k, k_up] = -term_y\n                    \n        return A\n\n    def pcg_solver(A, b, x0, M_inv, epsilon, max_iter):\n        \"\"\"\n        Implements the Preconditioned Conjugate Gradient (PCG) method.\n        M_inv is a function that computes z = M^{-1}r.\n        \"\"\"\n        x = x0.copy()\n        r = b - A @ x\n        norm_r0 = np.linalg.norm(r)\n\n        if norm_r0  1e-12: # If initial guess is the solution\n            return x, 0\n\n        z = M_inv(r)\n        p = z\n        rs_old = np.dot(r, z)\n\n        for k in range(max_iter):\n            Ap = A @ p\n            alpha = rs_old / np.dot(p, Ap)\n            \n            x = x + alpha * p\n            r = r - alpha * Ap\n\n            if np.linalg.norm(r) / norm_r0 = epsilon:\n                return x, k + 1\n\n            z = M_inv(r)\n            rs_new = np.dot(r, z)\n            beta = rs_new / rs_old\n            p = z + beta * p\n            rs_old = rs_new\n            \n        return x, max_iter\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (nx, ny, kx, ky, M_type, epsilon)\n        (16, 16, 1.0, 1.0, 'identity'),\n        (16, 16, 100.0, 1.0, 'jacobi'),\n        (16, 16, 100.0, 1.0, 'exact')\n    ]\n    \n    epsilon = 1e-8\n    results = []\n\n    for nx, ny, kx, ky, M_type in test_cases:\n        n = nx * ny\n        max_iter = n\n\n        # Assemble the matrix and vectors\n        A = build_A(nx, ny, kx, ky)\n        b = np.ones(n)\n        x0 = np.zeros(n)\n\n        # Define preconditioner M and its inverse action M_inv\n        if M_type == 'identity':\n            M = np.eye(n)\n            M_inv = lambda v: v\n        elif M_type == 'jacobi':\n            diag_A = np.diag(A)\n            M = np.diag(diag_A)\n            M_inv = lambda v: v / diag_A\n        elif M_type == 'exact':\n            M = A\n            # Pre-factor for efficient solves in PCG\n            L = cholesky(A, lower=True)\n            M_inv = lambda v:solve_triangular(L.T, solve_triangular(L, v, lower=True), lower=False)\n\n        # 1. Compute observed iterations k_obs\n        _, k_obs = pcg_solver(A, b, x0, M_inv, epsilon, max_iter)\n        \n        # 2. Compute spectral condition number kappa\n        # Use generalized eigenvalue solver for eigs of M^{-1}A\n        eigenvalues = eigh(A, M, eigvals_only=True)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n        \n        # Guard against division by zero and handle ideal case\n        if np.isclose(lambda_min, 0.0):\n            kappa = np.inf\n        elif np.isclose(lambda_min, lambda_max):\n            kappa = 1.0\n        else:\n            kappa = lambda_max / lambda_min\n\n        # 3. Compute worst-case iteration bound k_bound\n        if np.isclose(kappa, 1.0):\n            k_bound = 1\n        else:\n            # Formula: k >= log(eps/2) / log((sqrt(kappa)-1)/(sqrt(kappa)+1))\n            sqrt_kappa = np.sqrt(kappa)\n            term = (sqrt_kappa - 1) / (sqrt_kappa + 1)\n            k_bound = np.ceil(np.log(epsilon / 2.0) / np.log(term))\n\n        results.extend([k_obs, kappa, int(k_bound)])\n\n    # Format output according to problem specification\n    # k_obs and k_bound are integers, kappa is a float\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}