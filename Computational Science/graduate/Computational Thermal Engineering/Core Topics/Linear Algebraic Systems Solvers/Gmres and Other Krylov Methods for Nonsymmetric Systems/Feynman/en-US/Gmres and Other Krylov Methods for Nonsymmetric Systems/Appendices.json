{
    "hands_on_practices": [
        {
            "introduction": "The Generalized Minimal Residual (GMRES) method's efficiency is often enhanced by restarting, which limits memory usage. However, this practice can have unexpected consequences for convergence, as explored in this exercise . By analyzing a carefully constructed system, you will see firsthand how restarting can prevent GMRES from finding the optimal polynomial needed to eliminate error components, leading to stagnation where the full, unrestarted method would converge rapidly.",
            "id": "3957964",
            "problem": "Consider a one-dimensional steady advection–diffusion boundary value problem on a unit interval with Dirichlet boundary conditions, discretized by a second-order central finite difference scheme for diffusion and first-order upwind for advection. The resulting linear system $A x = b$ is nonsymmetric. Suppose a left preconditioner $M$ is chosen that exactly inverts the diffusive part but leaves the convective skew-symmetric part unaltered, and that, over an invariant two-dimensional subspace of the preconditioned operator $M^{-1} A$, the spectrum is tightly clustered at two real values, specifically at $\\lambda_{+} = 1$ and $\\lambda_{-} = -1$. Assume that, restricted to this two-dimensional invariant subspace, the eigenvectors $v_{+}$ and $v_{-}$ corresponding to $\\lambda_{+}$ and $\\lambda_{-}$ are orthonormal, and that the initial residual $r_{0}$ lies entirely in this subspace with equal projection onto these eigenvectors, that is $r_{0} = c \\, v_{+} + c \\, v_{-}$ for some nonzero scalar $c \\in \\mathbb{R}$. You apply the Generalized Minimal Residual (GMRES) method without restart to the preconditioned system $M^{-1} A x = M^{-1} b$, and also the restarted GMRES method with restart parameter $m = 1$ (denoted GMRES($1$)).\n\nStarting from the foundational definition that, after $k$ steps, GMRES produces a residual of the form $r_{k} = p_{k}(M^{-1} A) r_{0}$ where $p_{k}$ is a real polynomial of degree $k$ satisfying $p_{k}(0) = 1$, derive, from first principles, the exact residual-norm reduction factors for the following two cases:\n- Unrestarted GMRES after $2$ iterations.\n- GMRES($1$) after one restart cycle.\n\nExpress each reduction factor as the ratio $\\|r\\|_{2} / \\|r_{0}\\|_{2}$, where $\\|\\cdot\\|_{2}$ is the Euclidean norm, and provide the pair of results as a single $1 \\times 2$ row matrix. No rounding is required. The final quantities are dimensionless; report them without units.",
            "solution": "The problem requires the derivation of residual-norm reduction factors for two variants of the Generalized Minimal Residual (GMRES) method applied to a specific preconditioned linear system. We will address each case by starting from the foundational principles of GMRES.\n\nLet the preconditioned operator be denoted by $B = M^{-1}A$. The preconditioned system is $B x = M^{-1} b$. We start the iteration with an initial guess $x_0$ and the corresponding initial residual $r_0 = M^{-1}b - Bx_0$. After $k$ iterations, the unrestarted GMRES method finds an approximate solution $x_k$ in the affine Krylov subspace $x_0 + \\mathcal{K}_k(B, r_0)$, where $\\mathcal{K}_k(B, r_0) = \\text{span}\\{r_0, Br_0, \\dots, B^{k-1}r_0\\}$, such that the Euclidean norm of the corresponding residual $r_k = M^{-1}b - Bx_k$ is minimized.\n\nThe residual $r_k$ can be expressed in terms of the initial residual $r_0$ and a polynomial $p_k$. Since $x_k - x_0 \\in \\mathcal{K}_k(B, r_0)$, we can write $x_k = x_0 + q_{k-1}(B)r_0$ for some polynomial $q_{k-1}$ of degree at most $k-1$. The residual is then\n$$r_k = M^{-1}b - B(x_0 + q_{k-1}(B)r_0) = (M^{-1}b - Bx_0) - Bq_{k-1}(B)r_0 = r_0 - Bq_{k-1}(B)r_0$$\nDefining the residual polynomial $p_k(z) = 1 - z q_{k-1}(z)$, we see that $p_k$ is a polynomial of degree at most $k$ that satisfies the constraint $p_k(0) = 1$. The residual can be written as $r_k = p_k(B)r_0$. GMRES finds the polynomial $p_k$ that minimizes $\\|r_k\\|_2 = \\|p_k(B)r_0\\|_2$ over all real polynomials of degree at most $k$ with $p_k(0)=1$.\n\nFrom the problem statement, we are given a two-dimensional invariant subspace of $B$ spanned by two orthonormal eigenvectors, $v_+$ and $v_-$, corresponding to real eigenvalues $\\lambda_+ = 1$ and $\\lambda_- = -1$, respectively. The initial residual $r_0$ lies entirely in this subspace:\n$$r_0 = c \\, v_{+} + c \\, v_{-}$$\nfor some nonzero real scalar $c$. Since $r_0$ is in this invariant subspace, all subsequent residuals $r_k$ and Krylov vectors $B^j r_0$ will also lie within this subspace.\n\nLet's apply the operator $p_k(B)$ to $r_0$. Since $v_+$ and $v_-$ are eigenvectors of $B$:\n$$r_k = p_k(B)r_0 = p_k(B)(c v_{+} + c v_{-}) = c \\, p_k(B)v_{+} + c \\, p_k(B)v_{-} = c \\, p_k(\\lambda_{+})v_{+} + c \\, p_k(\\lambda_{-})v_{-}$$\nSubstituting the given eigenvalues:\n$$r_k = c \\, p_k(1)v_{+} + c \\, p_k(-1)v_{-}$$\nThe squared Euclidean norm of $r_k$ is found using the orthonormality of $v_+$ and $v_-$ (i.e., $\\langle v_+, v_+ \\rangle = 1$, $\\langle v_-, v_- \\rangle = 1$, and $\\langle v_+, v_- \\rangle = 0$):\n$$\\|r_k\\|_2^2 = \\langle c \\, p_k(1)v_{+} + c \\, p_k(-1)v_{-}, c \\, p_k(1)v_{+} + c \\, p_k(-1)v_{-} \\rangle$$\n$$\\|r_k\\|_2^2 = c^2 |p_k(1)|^2 \\langle v_+, v_+ \\rangle + c^2 |p_k(-1)|^2 \\langle v_-, v_- \\rangle + 2c^2 p_k(1)p_k(-1) \\langle v_+, v_- \\rangle$$\n$$\\|r_k\\|_2^2 = c^2 (p_k(1)^2 + p_k(-1)^2)$$\nThe squared norm of the initial residual $r_0$ is:\n$$\\|r_0\\|_2^2 = \\langle c v_{+} + c v_{-}, c v_{+} + c v_{-} \\rangle = c^2 \\langle v_+, v_+ \\rangle + c^2 \\langle v_-, v_- \\rangle + 2c^2 \\langle v_+, v_- \\rangle = c^2(1) + c^2(1) + 0 = 2c^2$$\nThe residual-norm reduction factor after $k$ steps is $\\frac{\\|r_k\\|_2}{\\|r_0\\|_2}$. Its square is:\n$$\\left( \\frac{\\|r_k\\|_2}{\\|r_0\\|_2} \\right)^2 = \\frac{c^2 (p_k(1)^2 + p_k(-1)^2)}{2c^2} = \\frac{p_k(1)^2 + p_k(-1)^2}{2}$$\nGMRES finds the polynomial $p_k$ that minimizes this expression.\n\n**Case 1: Unrestarted GMRES after 2 iterations ($k=2$)**\nWe are looking for a polynomial $p_2(z)$ of degree at most $2$ with $p_2(0)=1$ that minimizes $\\frac{p_2(1)^2 + p_2(-1)^2}{2}$. To minimize this sum of squares, we should ideally choose $p_2(z)$ such that $p_2(1)=0$ and $p_2(-1)=0$.\nLet's construct such a polynomial. The roots are $1$ and $-1$. The form of the polynomial must be $p_2(z) = \\gamma (z-1)(z+1)$ for some constant $\\gamma$. We use the constraint $p_2(0)=1$ to determine $\\gamma$:\n$$p_2(0) = \\gamma (0-1)(0+1) = -\\gamma = 1 \\implies \\gamma = -1$$\nSo, the optimal polynomial is $p_2(z) = -(z^2-1) = 1-z^2$. This is a polynomial of degree $2$ satisfying $p_2(0)=1$, so it is a valid candidate for GMRES(2).\nWith this polynomial, we have $p_2(1) = 0$ and $p_2(-1) = 0$. The minimum value of the squared reduction factor is:\n$$\\left( \\frac{\\|r_2\\|_2}{\\|r_0\\|_2} \\right)^2 = \\frac{0^2 + 0^2}{2} = 0$$\nThus, the reduction factor is $0$. This indicates that GMRES converges to the exact solution in two iterations for this specific problem setup.\n\n**Case 2: GMRES(1) after one restart cycle**\nGMRES(1) involves running a single GMRES step ($k=1$) and then restarting. The reduction factor \"after one restart cycle\" refers to the reduction achieved in this single step. We need to find the optimal polynomial $p_1(z)$ of degree at most $1$ with $p_1(0)=1$ that minimizes the reduction factor.\nA polynomial of degree at most $1$ with $p_1(0)=1$ has the general form $p_1(z) = 1 - \\alpha z$ for some real coefficient $\\alpha$.\nWe need to find the value of $\\alpha$ that minimizes the quantity $\\frac{p_1(1)^2 + p_1(-1)^2}{2}$.\n$$p_1(1) = 1 - \\alpha$$\n$$p_1(-1) = 1 - \\alpha(-1) = 1 + \\alpha$$\nThe expression to minimize is a function of $\\alpha$:\n$$f(\\alpha) = \\frac{(1-\\alpha)^2 + (1+\\alpha)^2}{2} = \\frac{(1 - 2\\alpha + \\alpha^2) + (1 + 2\\alpha + \\alpha^2)}{2} = \\frac{2 + 2\\alpha^2}{2} = 1 + \\alpha^2$$\nTo minimize $f(\\alpha)$, we find the critical points by taking the derivative with respect to $\\alpha$ and setting it to zero:\n$$f'(\\alpha) = 2\\alpha = 0 \\implies \\alpha = 0$$\nThis choice of $\\alpha=0$ minimizes $f(\\alpha)$. The minimal value is $f(0) = 1+0^2=1$.\nThe minimum squared reduction factor is $1$. The reduction factor is therefore $\\sqrt{1} = 1$.\nThis means that after one step of GMRES, the residual norm is not reduced at all; $\\|r_1\\|_2 = \\|r_0\\|_2$. The method stagnates. This occurs because the optimal polynomial is $p_1(z)=1$, which corresponds to making no update ($x_1=x_0$ and $r_1=r_0$).\n\nCombining the results, the reduction factor for unrestarted GMRES after $2$ iterations is $0$, and for GMRES(1) after one cycle is $1$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "For large-scale nonlinear thermal problems, explicitly forming the Jacobian matrix required by Newton's method is often computationally prohibitive. This exercise  delves into the matrix-free Newton-Krylov approach, where Jacobian-vector products are approximated using finite differences. You will derive the optimal perturbation step size $\\epsilon$ that balances the competing effects of truncation and floating-point roundoff error, a critical skill for developing robust and efficient nonlinear solvers.",
            "id": "3958015",
            "problem": "A finite volume discretization of a steady advection–diffusion heat transfer problem with radiative boundary conditions leads to a nonlinear residual map $F:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$, where $x\\in\\mathbb{R}^{n}$ collects nodal temperatures. The residual $F(x)$ is nonsymmetric due to upwinded advection terms and nonlinear radiation, and its Jacobian $J(x)\\in\\mathbb{R}^{n\\times n}$ is not formed explicitly when solving with the Generalized Minimal Residual method (GMRES) in a Newton–Krylov framework. Instead, a matrix-free Jacobian–vector product is approximated via a directional perturbation.\n\nStarting from the Fréchet differentiability of $F$ at $x$, use the definition of the Fréchet derivative to derive the forward-difference approximation of the directional derivative $J(x)v$ for an arbitrary direction $v\\in\\mathbb{R}^{n}$ of unit Euclidean norm, i.e., $\\|v\\|_{2}=1$:\n$$\nJ(x)v \\approx \\frac{F(x+\\epsilon v)-F(x)}{\\epsilon}.\n$$\nAssume $F$ is twice continuously differentiable in a neighborhood of $x$ with a bounded second derivative in operator norm in that neighborhood, quantified by a constant $M>0$ such that for all $y$ near $x$ and for all $w\\in\\mathbb{R}^{n}$,\n$$\n\\|D^{2}F(y)[w,w]\\|_{2}\\leq M\\|w\\|_{2}^{2}.\n$$\nModel floating-point evaluation of $F$ with unit roundoff $u>0$ as follows: a computed function evaluation $\\widehat{F}(y)$ satisfies $\\|\\widehat{F}(y)-F(y)\\|_{2}\\leq u\\,B$ for all $y$ in the neighborhood, for some known bound $B>0$ on the magnitude of $F(y)$ in that neighborhood. The computed finite-difference Jacobian–vector product is\n$$\n\\widehat{Jv}_{\\epsilon}=\\frac{\\widehat{F}(x+\\epsilon v)-\\widehat{F}(x)}{\\epsilon}.\n$$\nUsing these assumptions, bound the total error in $\\widehat{Jv}_{\\epsilon}$ as the sum of truncation and floating-point contributions and derive a scalar upper bound of the form\n$$\n\\|\\widehat{Jv}_{\\epsilon}-J(x)v\\|_{2}\\leq \\frac{M}{2}\\,\\epsilon + \\frac{C\\,u}{\\epsilon},\n$$\nfor some constant $C>0$ that you should express in terms of $B$ and universal inequalities. Then, by minimizing this upper bound with respect to the positive scalar $\\epsilon$, derive the analytic expression for the optimal step size $\\epsilon^{\\star}$ that balances truncation and roundoff errors in the forward-difference approximation. Your final answer must be the closed-form expression for $\\epsilon^{\\star}$, in terms of $M$, $u$, and $C$, and must be exact (no numerical evaluation or rounding). No physical units are required in the final expression.\n\nIn addition to the derivation, briefly discuss, using the assumptions stated, how the choice of $\\epsilon$ affects both accuracy and numerical stability of GMRES when used in Newton–Krylov methods for nonsymmetric thermal systems. The discussion should be qualitative but grounded in the quantitative bounds you derive. The final reported answer must be only the analytic expression for $\\epsilon^{\\star}$.",
            "solution": "The problem asks for the derivation of an optimal step size $\\epsilon^{\\star}$ for a forward-difference approximation of a Jacobian-vector product, used within a Newton-Krylov framework for solving a nonlinear system arising from a computational thermal engineering problem.\n\nThe total error in the computed Jacobian-vector product, $\\widehat{Jv}_{\\epsilon}$, is the norm of the difference between the computed approximation and the true value, $\\|\\widehat{Jv}_{\\epsilon} - J(x)v\\|_{2}$. We can decompose this error by introducing the exact finite-difference approximation as an intermediate term and applying the triangle inequality:\n$$\n\\|\\widehat{Jv}_{\\epsilon} - J(x)v\\|_{2} = \\left\\| \\left(\\frac{\\widehat{F}(x+\\epsilon v)-\\widehat{F}(x)}{\\epsilon}\\right) - J(x)v \\right\\|_{2}\n$$\n$$\n= \\left\\| \\left(\\frac{\\widehat{F}(x+\\epsilon v)-\\widehat{F}(x)}{\\epsilon} - \\frac{F(x+\\epsilon v)-F(x)}{\\epsilon}\\right) + \\left(\\frac{F(x+\\epsilon v)-F(x)}{\\epsilon} - J(x)v\\right) \\right\\|_{2}\n$$\n$$\n\\leq \\left\\| \\frac{(\\widehat{F}(x+\\epsilon v) - F(x+\\epsilon v)) - (\\widehat{F}(x) - F(x))}{\\epsilon} \\right\\|_{2} + \\left\\| \\frac{F(x+\\epsilon v)-F(x)}{\\epsilon} - J(x)v \\right\\|_{2}\n$$\nThe first term represents the contribution from floating-point roundoff error, while the second term is the truncation error inherent to the finite-difference approximation. We will bound each term separately.\n\nLet's first analyze the truncation error term:\n$$\nE_{trunc} = \\left\\| \\frac{F(x+\\epsilon v)-F(x)}{\\epsilon} - J(x)v \\right\\|_{2}\n$$\nThe problem states that $F$ is twice continuously differentiable in a neighborhood of $x$. This allows us to use a Taylor expansion for the vector-valued function $F(x+\\epsilon v)$ around $x$. Let $h = \\epsilon v$. The Taylor expansion with a remainder term is:\n$$\nF(x+h) = F(x) + DF(x)h + R_{1}(x,h)\n$$\nwhere $DF(x)$ is the Fréchet derivative of $F$ at $x$, which is precisely the Jacobian $J(x)$. The remainder term $R_{1}(x,h)$ can be bounded using the second derivative. For a twice continuously differentiable function, a standard form of Taylor's theorem for vector-valued functions gives:\n$$\nF(x+\\epsilon v) = F(x) + J(x)(\\epsilon v) + \\int_{0}^{1}(1-t)D^{2}F(x+t\\epsilon v)[\\epsilon v, \\epsilon v] dt\n$$\nSince $J(x)$ is a linear operator (matrix), $J(x)(\\epsilon v) = \\epsilon J(x)v$. The bilinear form $D^{2}F$ is also homogeneous of degree $2$, so $D^{2}F(y)[\\epsilon v, \\epsilon v] = \\epsilon^{2}D^{2}F(y)[v,v]$.\nSubstituting these into the Taylor expansion yields:\n$$\nF(x+\\epsilon v) = F(x) + \\epsilon J(x)v + \\epsilon^{2}\\int_{0}^{1}(1-t)D^{2}F(x+t\\epsilon v)[v,v] dt\n$$\nRearranging to match the form of the truncation error:\n$$\n\\frac{F(x+\\epsilon v)-F(x)}{\\epsilon} - J(x)v = \\epsilon\\int_{0}^{1}(1-t)D^{2}F(x+t\\epsilon v)[v,v] dt\n$$\nTaking the Euclidean norm on both sides and using the triangle inequality for integrals:\n$$\nE_{trunc} = \\left\\| \\epsilon\\int_{0}^{1}(1-t)D^{2}F(x+t\\epsilon v)[v,v] dt \\right\\|_{2} \\leq \\epsilon\\int_{0}^{1}(1-t)\\left\\|D^{2}F(x+t\\epsilon v)[v,v]\\right\\|_{2} dt\n$$\nWe are given that $\\|D^{2}F(y)[w,w]\\|_{2} \\leq M\\|w\\|_{2}^{2}$ for a constant $M>0$. In our case, $w=v$, and we are given $\\|v\\|_{2}=1$. Thus, $\\|D^{2}F(x+t\\epsilon v)[v,v]\\|_{2} \\leq M\\|v\\|_{2}^{2} = M$.\nThe bound on the truncation error becomes:\n$$\nE_{trunc} \\leq \\epsilon\\int_{0}^{1}(1-t)M dt = \\epsilon M \\left[t-\\frac{t^2}{2}\\right]_{0}^{1} = \\epsilon M \\left(1-\\frac{1}{2}\\right) = \\frac{M\\epsilon}{2}\n$$\n\nNext, we analyze the roundoff error term:\n$$\nE_{round} = \\left\\| \\frac{(\\widehat{F}(x+\\epsilon v) - F(x+\\epsilon v)) - (\\widehat{F}(x) - F(x))}{\\epsilon} \\right\\|_{2}\n$$\nLet the error in function evaluation be $e(y) = \\widehat{F}(y) - F(y)$. The problem provides the bound $\\|e(y)\\|_{2} \\leq uB$ for a constant $B>0$. Applying this, we have $\\|e(x+\\epsilon v)\\|_{2} \\leq uB$ and $\\|e(x)\\|_{2} \\leq uB$.\nUsing the triangle inequality on the numerator:\n$$\n\\|e(x+\\epsilon v) - e(x)\\|_{2} \\leq \\|e(x+\\epsilon v)\\|_{2} + \\|e(x)\\|_{2} \\leq uB + uB = 2uB\n$$\nTherefore, the bound on the roundoff error is:\n$$\nE_{round} \\leq \\frac{2uB}{\\epsilon}\n$$\nCombining the two error bounds, the total error is bounded by:\n$$\n\\|\\widehat{Jv}_{\\epsilon} - J(x)v\\|_{2} \\leq E_{trunc} + E_{round} \\leq \\frac{M\\epsilon}{2} + \\frac{2uB}{\\epsilon}\n$$\nThis matches the target form $\\|\\widehat{Jv}_{\\epsilon}-J(x)v\\|_{2}\\leq \\frac{M}{2}\\,\\epsilon + \\frac{C\\,u}{\\epsilon}$, with the constant $C=2B$.\n\nNow, we find the optimal step size $\\epsilon^{\\star}$ by minimizing the total error bound with respect to $\\epsilon > 0$. Let the error bound function be $E(\\epsilon)$:\n$$\nE(\\epsilon) = \\frac{M\\epsilon}{2} + \\frac{Cu}{\\epsilon}\n$$\nTo find the minimum, we compute the derivative with respect to $\\epsilon$ and set it to zero:\n$$\n\\frac{dE}{d\\epsilon} = \\frac{M}{2} - \\frac{Cu}{\\epsilon^{2}}\n$$\nSetting $\\frac{dE}{d\\epsilon} = 0$:\n$$\n\\frac{M}{2} = \\frac{Cu}{\\epsilon^{2}} \\implies \\epsilon^{2} = \\frac{2Cu}{M}\n$$\nSince $\\epsilon$ must be positive, we take the positive square root:\n$$\n\\epsilon^{\\star} = \\sqrt{\\frac{2Cu}{M}}\n$$\nTo confirm this is a minimum, we check the second derivative:\n$$\n\\frac{d^{2}E}{d\\epsilon^{2}} = \\frac{2Cu}{\\epsilon^{3}}\n$$\nSince $C>0$, $u>0$, and $\\epsilon>0$, we have $\\frac{d^{2}E}{d\\epsilon^{2}} > 0$, confirming that $\\epsilon^{\\star}$ corresponds to a local minimum.\n\nThe choice of the perturbation step size $\\epsilon$ is a critical trade-off between truncation error and roundoff error. The derived error bound $E(\\epsilon)$ illustrates this trade-off quantitatively. For large $\\epsilon$, the truncation error term, $\\frac{M\\epsilon}{2}$, dominates. This error arises because the finite difference formula is based on a linear approximation to the nonlinear function $F$, and this approximation becomes less accurate as the step size increases. For small $\\epsilon$, the roundoff error term, $\\frac{Cu}{\\epsilon}$, dominates. This error arises from subtractive cancellation in the floating-point computation of the numerator $\\widehat{F}(x+\\epsilon v) - \\widehat{F}(x)$. When $\\epsilon$ is very small, $x+\\epsilon v$ is very close to $x$, and thus $\\widehat{F}(x+\\epsilon v)$ is very close to $\\widehat{F}(x)$. The subtraction of two nearly equal numbers results in a loss of significant digits, and this loss is magnified by division by the small number $\\epsilon$. The optimal step size $\\epsilon^{\\star}$ balances these two competing error sources to achieve the maximum possible accuracy for the Jacobian-vector product approximation. The accuracy of this product directly impacts the performance of the inner GMRES solver in a Newton–Krylov method. If the Jacobian-vector products are inaccurate (i.e., if $\\epsilon$ is chosen far from $\\epsilon^{\\star}$), the operator being applied by GMRES is a significant perturbation of the true Jacobian $J(x)$. This can degrade the convergence rate of GMRES, requiring more iterations to meet the specified tolerance for the linear solve. In severe cases, it can cause the GMRES iteration to stagnate or fail, which in turn causes the outer Newton iteration to fail. Therefore, selecting an $\\epsilon$ near the optimal value is crucial for the overall robustness and efficiency of the Newton-Krylov algorithm in solving nonsymmetric thermal systems. The derived formula shows that $\\epsilon^\\star$ is proportional to $\\sqrt{u}$, a well-known result in numerical analysis confirming that the optimal choice is much larger than the machine unit roundoff $u$.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{2Cu}{M}}}\n$$"
        },
        {
            "introduction": "In engineering practice, the ultimate goal of a simulation is not just to reduce a mathematical residual, but to compute a specific physical quantity to a desired accuracy. This practice  introduces the powerful concept of adjoint-based, goal-oriented error estimation. You will learn how to relate the solver's residual to the error in a quantity of interest—such as integrated heat flux—enabling you to set a meaningful and efficient stopping tolerance for your iterative method.",
            "id": "3957987",
            "problem": "A two-dimensional steady convection–diffusion heat transfer problem in a channel with a heated wall is discretized by a conservative finite-volume method using first-order upwind for the advective fluxes and central differencing for the diffusive fluxes. The resulting linear system is nonsymmetric and can be written as $A u = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is nonsymmetric, $u \\in \\mathbb{R}^{n}$ is the vector of nodal temperatures, and $b \\in \\mathbb{R}^{n}$ is the load vector from sources and boundary conditions. The integrated wall heat flux over the heated wall $\\Gamma_{h}$ is the quantity of interest, modeled at the discrete level as the linear functional $J(u) = g^{\\mathsf{T}} u$ for a known vector $g \\in \\mathbb{R}^{n}$ constructed from face conductances and wall normals.\n\nSuppose the Generalized Minimal Residual (GMRES) method is applied without left or right preconditioning to compute an approximate solution $\\tilde{u}$ with true residual $r = b - A \\tilde{u}$. Let the error in the quantity of interest be $\\Delta J = J(u) - J(\\tilde{u})$. You are asked to relate the relative residual $\\|r\\|_{2}/\\|b\\|_{2}$ to the expected absolute error $|\\Delta J|$ in the integrated heat flux using only fundamental definitions from linear algebra (error, residual, dual problem) and norm inequalities, and then determine a stopping tolerance that guarantees an engineering-accurate result.\n\nAssume the following data are available from problem setup and auxiliary adjoint analysis on the same mesh:\n- The discrete adjoint $z \\in \\mathbb{R}^{n}$ solves $A^{\\mathsf{T}} z = g$, and its Euclidean norm is bounded as $\\|z\\|_{2} \\leq 40$.\n- The Euclidean norm of the right-hand side is measured to be $\\|b\\|_{2} = 3.0 \\times 10^{4}$.\n- A previously validated computation indicates a nominal integrated heat flux $J_{\\mathrm{ref}} = 8.0 \\times 10^{4}$ in watt. The engineering requirement is that the computed integrated heat flux be accurate to within a fractional tolerance of $0.005$ of $J_{\\mathrm{ref}}$ in absolute value.\n\nUsing only the above information and fundamental linear algebra, derive an inequality that upper bounds $|\\Delta J|$ in terms of $\\|r\\|_{2}$ and $\\|z\\|_{2}$, then express this bound in terms of the relative residual $\\tau = \\|r\\|_{2}/\\|b\\|_{2}$. Determine the largest admissible value of $\\tau$ that guarantees the engineering requirement on the absolute error in integrated heat flux. Round your final answer for $\\tau$ to three significant figures. The final answer must be reported as a dimensionless number with no units.",
            "solution": "The problem requires us to derive an inequality relating the absolute error in a quantity of interest, $|\\Delta J|$, to the relative residual, $\\tau = \\|r\\|_{2}/\\|b\\|_{2}$, for a linear system solved with an iterative method. Subsequently, we must determine the largest value of $\\tau$ that guarantees a specified engineering accuracy for $J$.\n\nFirst, we formalize the problem setup. The exact solution $u \\in \\mathbb{R}^{n}$ to the discrete system satisfies the linear equation:\n$$A u = b$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a nonsymmetric matrix, and $b \\in \\mathbb{R}^{n}$ is the right-hand side vector.\n\nThe quantity of interest is a linear functional of the solution, given by:\n$$J(u) = g^{\\mathsf{T}} u$$\nfor a specified vector $g \\in \\mathbb{R}^{n}$.\n\nAn approximate solution, denoted by $\\tilde{u} \\in \\mathbb{R}^{n}$, is computed. The error in this solution is the vector $e = u - \\tilde{u}$. The corresponding residual is $r = b - A \\tilde{u}$. We can relate the error $e$ to the residual $r$ by observing that:\n$$A e = A (u - \\tilde{u}) = A u - A \\tilde{u} = b - A \\tilde{u} = r$$\nThus, the error vector $e$ satisfies the linear system $A e = r$.\n\nThe error in the quantity of interest, $\\Delta J$, is the difference between the exact value and the approximate value:\n$$\\Delta J = J(u) - J(\\tilde{u}) = g^{\\mathsf{T}} u - g^{\\mathsf{T}} \\tilde{u} = g^{\\mathsf{T}} (u - \\tilde{u}) = g^{\\mathsf{T}} e$$\n\nTo proceed, we introduce the discrete adjoint problem, which is defined with respect to the matrix $A$ and the vector $g$ that defines the quantity of interest. The adjoint solution, $z \\in \\mathbb{R}^{n}$, solves:\n$$A^{\\mathsf{T}} z = g$$\nThis allows us to express $g$ as $g = A^{\\mathsf{T}} z$. Substituting this into the expression for $\\Delta J$:\n$$\\Delta J = (A^{\\mathsf{T}} z)^{\\mathsf{T}} e = z^{\\mathsf{T}} A e$$\nSince we have already established that $A e = r$, we can substitute this to obtain an exact expression for the error in the quantity of interest:\n$$\\Delta J = z^{\\mathsf{T}} r$$\nThis fundamental result states that the error in the linear functional output is exactly equal to the inner product of the adjoint solution and the primal residual.\n\nTo obtain an upper bound for the magnitude of this error, $|\\Delta J|$, we apply the Cauchy-Schwarz inequality to the inner product $z^{\\mathsf{T}} r$:\n$$|\\Delta J| = |z^{\\mathsf{T}} r| \\leq \\|z\\|_{2} \\|r\\|_{2}$$\nThis inequality provides an upper bound on the absolute error in the quantity of interest in terms of the Euclidean norms of the adjoint solution $z$ and the residual $r$.\n\nThe problem specifies the use of the relative residual, $\\tau = \\|r\\|_{2}/\\|b\\|_{2}$, as the stopping criterion. We can express the norm of the residual as $\\|r\\|_{2} = \\tau \\|b\\|_{2}$. Substituting this into our error bound gives:\n$$|\\Delta J| \\leq \\|z\\|_{2} (\\tau \\|b\\|_{2}) = \\tau \\|z\\|_{2} \\|b\\|_{2}$$\nThis is the desired inequality that relates the absolute error $|\\Delta J|$ to the relative residual $\\tau$.\n\nNext, we must determine the largest admissible value of $\\tau$ that guarantees the specified engineering accuracy. The reference integrated heat flux is $J_{\\mathrm{ref}} = 8.0 \\times 10^{4}$. The required fractional tolerance is $0.005$. The maximum permissible absolute error, which we denote by $|\\Delta J|_{\\mathrm{max}}$, is therefore:\n$$|\\Delta J|_{\\mathrm{max}} = 0.005 \\times J_{\\mathrm{ref}} = 0.005 \\times (8.0 \\times 10^{4}) = 400$$\n\nTo guarantee that the computed error $|\\Delta J|$ does not exceed this value, we must ensure that its upper bound is less than or equal to $|\\Delta J|_{\\mathrm{max}}$:\n$$\\tau \\|z\\|_{2} \\|b\\|_{2} \\leq |\\Delta J|_{\\mathrm{max}}$$\nWe are asked to find the largest value of $\\tau$ for which this guarantee holds. Rearranging the inequality, we get:\n$$\\tau \\leq \\frac{|\\Delta J|_{\\mathrm{max}}}{\\|z\\|_{2} \\|b\\|_{2}}$$\nTo ensure this inequality is satisfied for any possible realization of $z$ that is consistent with the given information, we must use the most conservative (largest) value for $\\|z\\|_{2}$. The problem provides the bound $\\|z\\|_{2} \\leq 40$. Therefore, we use $\\|z\\|_{2} = 40$ in our calculation.\n\nThe given values are:\n- $|\\Delta J|_{\\mathrm{max}} = 400$\n- $\\|z\\|_{2} \\leq 40$\n- $\\|b\\|_{2} = 3.0 \\times 10^{4}$\n\nSubstituting these values into the inequality to find the maximum allowed $\\tau$:\n$$\\tau \\leq \\frac{400}{40 \\times (3.0 \\times 10^{4})}$$\n$$\\tau \\leq \\frac{10}{3.0 \\times 10^{4}} = \\frac{10}{3} \\times 10^{-4}$$\nThe largest admissible value for $\\tau$ is thus $\\frac{10}{3} \\times 10^{-4}$. Evaluating this numerically:\n$$\\tau_{\\mathrm{max}} = \\frac{10}{3} \\times 10^{-4} \\approx 3.333... \\times 10^{-4}$$\nThe problem asks for the answer to be rounded to three significant figures.\n$$\\tau_{\\mathrm{max}} \\approx 3.33 \\times 10^{-4}$$\nThis is the largest value of the relative residual tolerance $\\tau$ that guarantees the absolute error in the integrated heat flux is within the required engineering specification.",
            "answer": "$$\\boxed{3.33 \\times 10^{-4}}$$"
        }
    ]
}