{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate the power of multigrid methods, it is essential to first understand the limitations of traditional iterative solvers when applied to large-scale problems arising from PDE discretizations. This practice quantitatively demonstrates this \"tyranny of the condition number\" by comparing the convergence of the standard Conjugate Gradient (CG) method with that of an ideal Multigrid-preconditioned CG (PCG) for a 3D heat conduction problem. By deriving the iteration counts from first principles, you will gain a concrete understanding of why multigrid is not just an incremental improvement, but a transformational technology for scientific computing .",
            "id": "3974261",
            "problem": "Consider the three-dimensional steady-state heat conduction equation with constant thermal conductivity, which reduces to the Laplace equation $-\\nabla^2 T = 0$ on a unit cube with homogeneous Dirichlet boundary conditions. Discretize the equation using second-order central differences on a uniform grid of $n$ interior points per coordinate direction, yielding a linear system $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$ with $\\mathbf{A}$ symmetric positive definite. The discrete operator corresponds to the standard seven-point stencil scaled by $1/h^2$ with grid spacing $h=1/(n+1)$. Assume that the linear system is solved using Conjugate Gradient (CG), optionally with a Multigrid (MG) preconditioner applied once per CG iteration (Preconditioned Conjugate Gradient, PCG).\n\nStarting from the fundamental properties of the discrete Laplacian with homogeneous Dirichlet boundary conditions, derive the extreme eigenvalues of $\\mathbf{A}$ appropriate for this setting, then obtain the condition number $\\kappa(\\mathbf{A})$ and use the standard energy-norm convergence bound for Conjugate Gradient to estimate the number of iterations needed to reduce the energy-norm of the error by a prescribed factor. For the MG-preconditioned case, assume an estimated per-cycle geometric convergence factor $\\rho \\in (0,1)$ is achieved per PCG iteration, and compute the minimum number of iterations needed to reduce the error by the same factor.\n\nFor all computations, set $n=128$ (that is, a problem size of $128^3$ interior unknowns). For the unpreconditioned Conjugate Gradient method, let the convergence estimate be based on the energy-norm error bound derived from the condition number of $\\mathbf{A}$. For the Multigrid-preconditioned case, use the simplifying assumption that each PCG iteration reduces the error by the factor $\\rho$, independent of $n$.\n\nYour program must compute, for each test case, a pair of integers $[k_{\\mathrm{CG}},k_{\\mathrm{PCG}}]$, where $k_{\\mathrm{CG}}$ is the minimum number of Conjugate Gradient iterations required to reduce the energy-norm of the error by a factor $\\tau$ (using the derived bound from first principles), and $k_{\\mathrm{PCG}}$ is the minimum number of Preconditioned Conjugate Gradient iterations required under the geometric reduction model with factor $\\rho$. The integer $k_{\\mathrm{CG}}$ must be the smallest integer $k$ for which the standard bound guarantees a reduction at least as strong as $\\tau$, and $k_{\\mathrm{PCG}}$ must be the smallest integer $k$ such that $\\rho^k \\le \\tau$.\n\nUse the following test suite, where each case provides the target reduction $\\tau$ and the estimated MG per-cycle factor $\\rho$:\n- Case $1$: $\\tau=10^{-6}$, $\\rho=0.1$.\n- Case $2$: $\\tau=10^{-8}$, $\\rho=0.3$.\n- Case $3$: $\\tau=10^{-10}$, $\\rho=0.8$.\n- Case $4$: $\\tau=10^{-4}$, $\\rho=0.99$.\n- Case $5$: $\\tau=10^{-12}$, $\\rho=0.05$.\n\nAll quantities are dimensionless. Your program should produce a single line of output containing the results for the five cases as a comma-separated list of bracketed pairs, enclosed in square brackets, with no additional text. For example, the output format must be of the form $[[k_{\\mathrm{CG},1},k_{\\mathrm{PCG},1}],[k_{\\mathrm{CG},2},k_{\\mathrm{PCG},2}],\\dots,[k_{\\mathrm{CG},5},k_{\\mathrm{PCG},5}]]$.",
            "solution": "The problem posed is a well-defined application of numerical linear algebra principles to the analysis of iterative solvers for elliptic partial differential equations. It is scientifically sound, self-contained, and objective. We proceed with the derivation and solution.\n\nThe problem centers on the steady-state heat conduction equation in three dimensions on a unit cube, which, for constant thermal conductivity, is the Laplace equation:\n$$\n-\\nabla^2 T = - \\left( \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} + \\frac{\\partial^2 T}{\\partial z^2} \\right) = 0\n$$\nwith homogeneous Dirichlet boundary conditions, $T=0$, on the boundary of the domain $\\Omega = [0,1]^3$.\n\nWe discretize this equation on a uniform grid with $n$ interior points in each coordinate direction. The grid spacing is $h = 1/(n+1)$. Using a second-order central difference approximation for the second derivatives at an interior grid point $(x_i, y_j, z_k)$, we have:\n$$\n-\\frac{\\partial^2 T}{\\partial x^2}\\bigg|_{(i,j,k)} \\approx -\\frac{T_{i-1,j,k} - 2T_{i,j,k} + T_{i+1,j,k}}{h^2}\n$$\nand similarly for the $y$ and $z$ derivatives. Summing these terms yields the discrete equation for the negative Laplacian at each interior point:\n$$\n(-\\nabla^2_h T)_{i,j,k} = \\frac{1}{h^2} \\left( 6T_{i,j,k} - T_{i-1,j,k} - T_{i+1,j,k} - T_{i,j-1,k} - T_{i,j+1,k} - T_{i,j,k-1} - T_{i,j,k+1} \\right) = 0\n$$\nThis forms a linear system $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$, where $\\mathbf{u}$ is the vector of unknown temperatures at the $n^3$ interior points. The matrix $\\mathbf{A}$ represents the standard $7$-point discrete Laplacian operator, scaled by $1/h^2$. Due to the homogeneous boundary conditions, the right-hand side vector $\\mathbf{b}$ is zero. The matrix $\\mathbf{A}$ is symmetric and positive definite.\n\nTo analyze the convergence of the Conjugate Gradient (CG) method, we must determine the condition number $\\kappa(\\mathbf{A})$, which is the ratio of its largest to its smallest eigenvalue, $\\kappa(\\mathbf{A}) = \\lambda_{\\max} / \\lambda_{\\min}$.\n\nThe eigenvalues of the $3$-dimensional discrete Laplacian operator can be found by separation of variables, building from the $1$-dimensional case. The $1$-D discrete Laplacian matrix on $n$ interior points is $L_{1D} = \\frac{1}{h^2} \\text{tridiag}(-1, 2, -1)$. Its eigenvalues are known to be:\n$$\n\\lambda_p = \\frac{2}{h^2}\\left(1 - \\cos\\left(\\frac{p\\pi}{n+1}\\right)\\right) = \\frac{4}{h^2}\\sin^2\\left(\\frac{p\\pi}{2(n+1)}\\right), \\quad p = 1, 2, \\dots, n\n$$\nThe $3$-D matrix $\\mathbf{A}$ can be expressed using Kronecker sums of the $1$-D matrix: $\\mathbf{A} = L_{1D} \\otimes I \\otimes I + I \\otimes L_{1D} \\otimes I + I \\otimes I \\otimes L_{1D}$. The eigenvalues of $\\mathbf{A}$ are therefore the sums of the eigenvalues from each dimension:\n$$\n\\lambda_{p,q,r} = \\lambda_p + \\lambda_q + \\lambda_r = \\frac{4}{h^2} \\left[ \\sin^2\\left(\\frac{p\\pi}{2(n+1)}\\right) + \\sin^2\\left(\\frac{q\\pi}{2(n+1)}\\right) + \\sin^2\\left(\\frac{r\\pi}{2(n+1)}\\right) \\right]\n$$\nwhere $p, q, r \\in \\{1, 2, \\dots, n\\}$.\n\nThe smallest eigenvalue, $\\lambda_{\\min}$, corresponds to the lowest frequency mode, where $p=q=r=1$:\n$$\n\\lambda_{\\min} = \\lambda_{1,1,1} = \\frac{4}{h^2} \\left[ 3 \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right) \\right] = \\frac{12}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)\n$$\nThe largest eigenvalue, $\\lambda_{\\max}$, corresponds to the highest frequency mode, where $p=q=r=n$:\n$$\n\\lambda_{\\max} = \\lambda_{n,n,n} = \\frac{4}{h^2} \\left[ 3 \\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right) \\right]\n$$\nUsing the identity $\\sin\\left(\\frac{n\\pi}{2(n+1)}\\right) = \\sin\\left(\\frac{\\pi}{2} - \\frac{\\pi}{2(n+1)}\\right) = \\cos\\left(\\frac{\\pi}{2(n+1)}\\right)$, we get:\n$$\n\\lambda_{\\max} = \\frac{12}{h^2} \\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right)\n$$\nThe condition number of $\\mathbf{A}$ is then:\n$$\n\\kappa(\\mathbf{A}) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\frac{12}{h^2} \\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right)}{\\frac{12}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\cot^2\\left(\\frac{\\pi}{2(n+1)}\\right)\n$$\nThe convergence of the CG method is governed by the condition number. The error $e_k = u - u_k$ after $k$ iterations, measured in the energy norm $\\|e_k\\|_{\\mathbf{A}} = \\sqrt{e_k^T \\mathbf{A} e_k}$, satisfies the standard bound:\n$$\n\\|e_k\\|_{\\mathbf{A}} \\le 2 \\left( \\frac{\\sqrt{\\kappa(\\mathbf{A})} - 1}{\\sqrt{\\kappa(\\mathbf{A})} + 1} \\right)^k \\|e_0\\|_{\\mathbf{A}}\n$$\nWe need to find the minimum integer $k$, denoted $k_{\\mathrm{CG}}$, such that the error is reduced by a factor of $\\tau$, i.e., $\\|e_k\\|_{\\mathbf{A}} / \\|e_0\\|_{\\mathbf{A}} \\le \\tau$. This requires solving for $k$ in the inequality:\n$$\n2 \\left( \\frac{\\sqrt{\\kappa(\\mathbf{A})} - 1}{\\sqrt{\\kappa(\\mathbf{A})} + 1} \\right)^k \\le \\tau\n$$\nTaking the natural logarithm of both sides:\n$$\n\\ln(2) + k \\ln\\left(\\frac{\\sqrt{\\kappa(\\mathbf{A})} - 1}{\\sqrt{\\kappa(\\mathbf{A})} + 1}\\right) \\le \\ln(\\tau)\n$$\nSince $\\sqrt{\\kappa(\\mathbf{A})} > 1$, the term inside the logarithm is less than $1$, making the logarithm negative. Thus, when solving for $k$, the inequality is reversed:\n$$\nk \\ge \\frac{\\ln(\\tau) - \\ln(2)}{\\ln\\left(\\frac{\\sqrt{\\kappa(\\mathbf{A})} - 1}{\\sqrt{\\kappa(\\mathbf{A})} + 1}\\right)} = \\frac{\\ln(\\tau/2)}{\\ln\\left(\\frac{\\sqrt{\\kappa(\\mathbf{A})} - 1}{\\sqrt{\\kappa(\\mathbf{A})} + 1}\\right)}\n$$\nThe minimum integer number of iterations is the ceiling of this expression:\n$$\nk_{\\mathrm{CG}} = \\left\\lceil \\frac{\\ln(\\tau/2)}{\\ln\\left(\\frac{\\cot(\\frac{\\pi}{2(n+1)}) - 1}{\\cot(\\frac{\\pi}{2(n+1)}) + 1}\\right)} \\right\\rceil\n$$\nFor the Multigrid-Preconditioned Conjugate Gradient (PCG) case, the problem provides a simplified model where each iteration reduces the error by a constant factor $\\rho$. We seek the minimum number of iterations $k$, denoted $k_{\\mathrm{PCG}}$, to achieve a reduction of $\\tau$. The governing inequality is:\n$$\n\\rho^k \\le \\tau\n$$\nTaking the natural logarithm:\n$$\nk \\ln(\\rho) \\le \\ln(\\tau)\n$$\nSince $\\rho \\in (0,1)$, $\\ln(\\rho)$ is negative. Solving for $k$ reverses the inequality:\n$$\nk \\ge \\frac{\\ln(\\tau)}{\\ln(\\rho)}\n$$\nThe minimum integer number of iterations is therefore:\n$$\nk_{\\mathrm{PCG}} = \\left\\lceil \\frac{\\ln(\\tau)}{\\ln(\\rho)} \\right\\rceil\n$$\nFor all computations, the problem specifies $n=128$. We will use this value along with the test case parameters $(\\tau, \\rho)$ to compute the pairs $[k_{\\mathrm{CG}}, k_{\\mathrm{PCG}}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the estimated number of iterations for Conjugate Gradient (CG)\n    and Preconditioned Conjugate Gradient (PCG) methods for solving the\n    3D discretized Laplace equation based on provided theoretical models.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple contains: (error reduction factor tau, MG convergence factor rho)\n    test_cases = [\n        (1e-6, 0.1),\n        (1e-8, 0.3),\n        (1e-10, 0.8),\n        (1e-4, 0.99),\n        (1e-12, 0.05),\n    ]\n\n    # Number of interior grid points per dimension\n    n = 128\n\n    # --- CG Iteration Count Calculation ---\n\n    # Derivation steps for k_CG:\n    # 1. Condition number kappa = cot(pi / (2*(n+1)))^2\n    # 2. sqrt(kappa) = cot(pi / (2*(n+1)))\n    # 3. CG convergence factor gamma = (sqrt(kappa) - 1) / (sqrt(kappa) + 1)\n    # 4. Error bound: 2 * gamma^k <= tau\n    # 5. k >= log(tau / 2) / log(gamma)\n    \n    n_plus_1 = n + 1\n    # Argument for the cotangent function\n    angle = np.pi / (2.0 * n_plus_1)\n    # The square root of the condition number\n    sqrt_kappa = 1.0 / np.tan(angle)\n    \n    # The convergence factor in the CG error bound\n    cg_conv_factor = (sqrt_kappa - 1.0) / (sqrt_kappa + 1.0)\n    log_cg_conv_factor = np.log(cg_conv_factor)\n\n    results = []\n    for tau, rho in test_cases:\n        # --- Calculate k_CG for the current test case ---\n        log_tau_div_2 = np.log(tau / 2.0)\n        # k_CG is the smallest integer k satisfying the bound\n        k_cg = int(math.ceil(log_tau_div_2 / log_cg_conv_factor))\n\n        # --- PCG Iteration Count Calculation ---\n\n        # Derivation steps for k_PCG:\n        # 1. Error model: rho^k <= tau\n        # 2. k >= log(tau) / log(rho)\n        log_tau = np.log(tau)\n        log_rho = np.log(rho)\n        # k_PCG is the smallest integer k satisfying the model\n        k_pcg = int(math.ceil(log_tau / log_rho))\n\n        results.append([k_cg, k_pcg])\n\n    # Final print statement in the exact required format.\n    # e.g., [[k_CG,1,k_PCG,1],[k_CG,2,k_PCG,2],...,[k_CG,5,k_PCG,5]]\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established the need for a more efficient solver, we now delve into the first of multigrid's two core components: the smoother. A smoother is an iterative method designed not to solve the system, but to rapidly damp the high-frequency components of the error. This exercise introduces Local Fourier Analysis (LFA), a fundamental tool for understanding smoother performance, to optimize the weighted Jacobi method for the 2D Poisson problem. By finding the optimal relaxation parameter $\\omega$, you will see precisely how a smoother is tuned to perform its role effectively, paving the way for the coarse-grid correction step .",
            "id": "3974232",
            "problem": "Consider steady-state heat conduction in a homogeneous, isotropic medium governed by the Poisson equation $-\\nabla^{2} T = f$ on an infinite two-dimensional uniform Cartesian grid with grid spacing $h$. Using the standard second-order five-point finite difference approximation of the Laplacian, the discrete operator $A$ has stencil $\\frac{1}{h^{2}}\\begin{pmatrix} & -1 & \\\\ -1 & 4 & -1 \\\\ & -1 & \\end{pmatrix}$, and the diagonal $D$ of $A$ is $D = \\frac{4}{h^{2}} I$. In a two-grid method with $2 \\times 2$ coarsening, full-weighting restriction, and bilinear interpolation, a common smoother is the weighted Jacobi method, defined for the iterate $u^{(k)}$ by $u^{(k+1)} = u^{(k)} + \\omega D^{-1}\\left(b - A u^{(k)}\\right)$, where $\\omega \\in (0,1)$ is the relaxation parameter.\n\nUsing Local Fourier Analysis (LFA; Local Fourier Analysis), analyze the error-propagation symbol of the weighted Jacobi smoother for this operator on the infinite grid. Define the high-frequency set as those Fourier modes with both components outside the coarse-grid representable band, specifically $|\\theta_{x}| \\in [\\pi/2,\\pi]$ and $|\\theta_{y}| \\in [\\pi/2,\\pi]$. Determine the value of the relaxation parameter $\\omega$ that minimizes the worst-case magnitude of the smoother symbol over this high-frequency set. Express your final answer as an exact number. No rounding is required.",
            "solution": "The problem requires the determination of the optimal relaxation parameter $\\omega$ for a weighted Jacobi smoother used in a multigrid method for the two-dimensional Poisson equation. The optimization is to be performed over a specified set of high-frequency modes using Local Fourier Analysis (LFA).\n\nFirst, we establish the framework of LFA. On an infinite, uniform Cartesian grid with spacing $h$, a discrete function is represented by its Fourier components. A Fourier mode is an eigenfunction of any linear, shift-invariant operator, such as the finite difference operator $A$. A mode with a dimensionless wave vector $\\theta = (\\theta_x, \\theta_y) \\in [-\\pi, \\pi]^2$ is given by $\\phi_{\\theta}(j) = \\exp(i j \\cdot \\theta)$, where $j = (j_x, j_y)$ is the grid index. The eigenvalue of an operator corresponding to this mode is called the symbol of the operator.\n\nThe discrete operator $A$ is given by the five-point stencil for the negative Laplacian:\n$$\n(A u)_j = \\frac{1}{h^2} \\left( 4u_j - u_{j+(1,0)} - u_{j-(1,0)} - u_{j+(0,1)} - u_{j-(0,1)} \\right)\n$$\nTo find the symbol $\\hat{A}(\\theta)$, we apply $A$ to the Fourier mode $\\phi_{\\theta}(j)$:\n$$\n(A \\phi_{\\theta})_j = \\frac{1}{h^2} \\left( 4e^{i j \\cdot \\theta} - e^{i (j_x+1)\\theta_x}e^{i j_y\\theta_y} - e^{i (j_x-1)\\theta_x}e^{i j_y\\theta_y} - e^{i j_x\\theta_x}e^{i (j_y+1)\\theta_y} - e^{i j_x\\theta_x}e^{i (j_y-1)\\theta_y} \\right)\n$$\nFactoring out $\\phi_{\\theta}(j) = e^{i j \\cdot \\theta}$:\n$$\n(A \\phi_{\\theta})_j = \\frac{1}{h^2} \\phi_{\\theta}(j) \\left( 4 - e^{i\\theta_x} - e^{-i\\theta_x} - e^{i\\theta_y} - e^{-i\\theta_y} \\right)\n$$\nUsing the identity $2\\cos(x) = e^{ix} + e^{-ix}$, the symbol of $A$ is:\n$$\n\\hat{A}(\\theta) = \\frac{1}{h^2} \\left( 4 - 2\\cos(\\theta_x) - 2\\cos(\\theta_y) \\right) = \\frac{2}{h^2} \\left( 2 - \\cos(\\theta_x) - \\cos(\\theta_y) \\right)\n$$\nUsing the half-angle identity $1 - \\cos(x) = 2\\sin^2(x/2)$, this can be rewritten as:\n$$\n\\hat{A}(\\theta) = \\frac{4}{h^2} \\left( \\sin^2\\left(\\frac{\\theta_x}{2}\\right) + \\sin^2\\left(\\frac{\\theta_y}{2}\\right) \\right)\n$$\nThe weighted Jacobi iteration is given by $u^{(k+1)} = u^{(k)} + \\omega D^{-1}(b - A u^{(k)})$. The error $e^{(k)} = u^{(k)} - u$ (where $Au=b$) propagates according to the smoothing operator $S = I - \\omega D^{-1} A$. We need the symbol of $S$, denoted $\\hat{S}(\\theta)$.\nThe operator $D$ is the diagonal of $A$, which is $D = \\frac{4}{h^2}I$. Its inverse is $D^{-1} = \\frac{h^2}{4}I$. The symbols of $D$ and $D^{-1}$ are simply the scalars $\\hat{D} = \\frac{4}{h^2}$ and $\\hat{D}^{-1} = \\frac{h^2}{4}$, respectively.\n\nThe symbol of the smoothing operator $S$ is then:\n$$\n\\hat{S}(\\theta) = 1 - \\omega \\hat{D}^{-1} \\hat{A}(\\theta) = 1 - \\omega \\left(\\frac{h^2}{4}\\right) \\left[ \\frac{4}{h^2} \\left( \\sin^2\\left(\\frac{\\theta_x}{2}\\right) + \\sin^2\\left(\\frac{\\theta_y}{2}\\right) \\right) \\right]\n$$\n$$\n\\hat{S}(\\theta) = 1 - \\omega \\left( \\sin^2\\left(\\frac{\\theta_x}{2}\\right) + \\sin^2\\left(\\frac{\\theta_y}{2}\\right) \\right)\n$$\nThis is the error-propagation symbol, also known as the smoothing factor for the mode $\\theta$. Let's denote it by $\\mu(\\theta, \\omega)$.\n\nThe problem defines the high-frequency set as $HF = \\{ (\\theta_x, \\theta_y) : |\\theta_x| \\in [\\pi/2, \\pi] \\text{ and } |\\theta_y| \\in [\\pi/2, \\pi] \\}$. We need to find the value of $\\omega \\in (0,1)$ that minimizes the worst-case (maximum) magnitude of $\\mu(\\theta, \\omega)$ over this set.\n$$\n\\min_{\\omega \\in (0,1)} \\max_{\\theta \\in HF} |\\mu(\\theta, \\omega)|\n$$\nLet the term depending on the frequency be $C(\\theta) = \\sin^2(\\frac{\\theta_x}{2}) + \\sin^2(\\frac{\\theta_y}{2})$. The symbol is then $\\mu(\\theta, \\omega) = 1 - \\omega C(\\theta)$.\nWe first determine the range of $C(\\theta)$ for $\\theta \\in HF$. The function $\\sin^2(x/2)$ is monotonically increasing for $x \\in [0, \\pi]$. Since $\\sin^2(x/2)$ is an even function of $x$, we only need to consider $\\theta_x, \\theta_y \\in [\\pi/2, \\pi]$.\nThe minimum value of $\\sin^2(\\alpha/2)$ for $\\alpha \\in [\\pi/2, \\pi]$ occurs at $\\alpha = \\pi/2$: $\\sin^2(\\frac{\\pi/2}{2}) = \\sin^2(\\pi/4) = (\\frac{1}{\\sqrt{2}})^2 = \\frac{1}{2}$.\nThe maximum value occurs at $\\alpha = \\pi$: $\\sin^2(\\pi/2) = 1^2 = 1$.\nThus, for $\\theta \\in HF$, the term $\\sin^2(\\theta_x/2)$ ranges over $[1/2, 1]$, and so does $\\sin^2(\\theta_y/2)$.\nThe range of their sum, $C(\\theta)$, is therefore:\n$$\nC_{min} = \\frac{1}{2} + \\frac{1}{2} = 1 \\quad (\\text{at } |\\theta_x|=|\\theta_y|=\\pi/2)\n$$\n$$\nC_{max} = 1 + 1 = 2 \\quad (\\text{at } |\\theta_x|=|\\theta_y|=\\pi)\n$$\nSo, $C(\\theta)$ for $\\theta \\in HF$ spans the interval $[1, 2]$.\n\nOur problem simplifies to finding the $\\omega$ that solves the minimax problem:\n$$\n\\min_{\\omega \\in (0,1)} \\max_{C \\in [1, 2]} |1 - \\omega C|\n$$\nThe function $g(C) = 1 - \\omega C$ is linear in $C$. For a fixed $\\omega > 0$, the maximum absolute value of a linear function over an interval $[C_{min}, C_{max}]$ must occur at the endpoints, $C=1$ or $C=2$.\nSo, we need to minimize:\n$$\n\\max(|1 - \\omega \\cdot 1|, |1 - \\omega \\cdot 2|) = \\max(|1 - \\omega|, |1 - 2\\omega|)\n$$\nThe optimal value of $\\omega$ is found where the magnitudes at the boundaries are equal, as this balances the amplification of the extreme frequency components. This is a standard result for minimizing the maximum of two functions, one decreasing and one increasing with respect to the parameter.\nWe set $|1 - \\omega| = |1 - 2\\omega|$.\nSince the problem specifies $\\omega \\in (0,1)$, the term $1-\\omega$ is always positive, so $|1-\\omega| = 1-\\omega$.\nFor the term $|1-2\\omega|$, we consider two cases for $\\omega$:\n1. If $\\omega \\in (0, 1/2]$, then $1-2\\omega \\ge 0$, so $|1-2\\omega| = 1-2\\omega$. The equation becomes $1-\\omega = 1-2\\omega$, which implies $\\omega=0$, a value outside the specified interval $(0,1)$.\n2. If $\\omega \\in (1/2, 1)$, then $1-2\\omega < 0$, so $|1-2\\omega| = -(1-2\\omega) = 2\\omega - 1$. The equation becomes:\n$$\n1 - \\omega = 2\\omega - 1\n$$\n$$\n2 = 3\\omega\n$$\n$$\n\\omega = \\frac{2}{3}\n$$\nThis value $\\omega = 2/3$ lies in the interval $(1/2, 1)$, so it is a valid candidate. At this value, the maximum magnitude is $|1 - 2/3| = |1 - 2(2/3)| = |1 - 4/3| = 1/3$.\nTo confirm this is the minimum, we can analyze the function $f(\\omega) = \\max(1-\\omega, |1-2\\omega|)$ for $\\omega \\in (0,1)$.\nFor $\\omega \\in (0, 1/2)$, $f(\\omega) = \\max(1-\\omega, 1-2\\omega) = 1-\\omega$. This is a decreasing function, so its minimum over this open interval is approached at $\\omega=1/2$, with value $1/2$.\nFor $\\omega \\in [1/2, 1)$, $f(\\omega) = \\max(1-\\omega, 2\\omega-1)$. Here, $1-\\omega$ is decreasing and $2\\omega-1$ is increasing. The minimum of their maximum occurs at their intersection, which we found to be $\\omega=2/3$, where the value is $1/3$.\nComparing the results from the two intervals, the global minimum for $\\omega \\in (0,1)$ is $1/3$, which is achieved at $\\omega = 2/3$.\n\nTherefore, the value of the relaxation parameter $\\omega$ that minimizes the worst-case magnitude of the smoother symbol over the specified high-frequency set is $2/3$.",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        },
        {
            "introduction": "After the smoother has eliminated the oscillatory, high-frequency error, the remaining smooth error must be addressed. This is the role of the coarse-grid correction. This practice focuses on the Galerkin principle, a robust and theoretically sound method for constructing the coarse-grid operator using the formulation $\\mathbf{A}_{c}=\\mathbf{R}\\mathbf{A}\\mathbf{P}$. You will derive the explicit stencil for a coarse-grid operator in a variable-coefficient problem and, importantly, prove that it preserves the symmetric positive-definite (SPD) nature of the fine-grid operator, ensuring the entire multigrid hierarchy is well-posed .",
            "id": "3974226",
            "problem": "Consider one-dimensional steady heat conduction in a heterogeneous rod with spatially varying thermal conductivity governed by Fourierâ€™s law. The governing equation is the second-order diffusion operator with variable coefficients,\n$$\n-\\frac{d}{dx}\\!\\left(k(x)\\,\\frac{du}{dx}\\right)=f(x),\n$$\non a uniform grid over the interval with homogeneous Dirichlet boundary conditions $u=0$ at both ends. The variable conductivity $k(x)$ is strictly positive and smooth. On a fine grid with spacing $h$, discretize the operator by a standard three-point conservative flux balance using mid-edge conductivities $\\{k_{i+1/2}\\}_{i}$, with the discrete linear operator $\\mathbf{A}$ acting on a fine-grid vector $\\mathbf{u}$ given at interior index $i$ by\n$$\n(\\mathbf{A}\\mathbf{u})_{i}=\\frac{1}{h^{2}}\\!\\left(-k_{i-1/2}\\,\\mathbf{u}_{i-1}+(k_{i-1/2}+k_{i+1/2})\\,\\mathbf{u}_{i}-k_{i+1/2}\\,\\mathbf{u}_{i+1}\\right).\n$$\nLet the coarse grid have spacing $H=2h$. Define the linear interpolation prolongation $\\mathbf{P}$ from coarse to fine grid by\n$$\n(\\mathbf{P}\\mathbf{v})_{2j}=\\mathbf{v}_{j},\\quad (\\mathbf{P}\\mathbf{v})_{2j+1}=\\frac{\\mathbf{v}_{j}+\\mathbf{v}_{j+1}}{2},\n$$\nfor an interior coarse index $j$. Define the full-weighting restriction $\\mathbf{R}$ from fine to coarse grid by\n$$\n(\\mathbf{R}\\mathbf{r})_{j}=\\frac{1}{4}\\,\\mathbf{r}_{2j-1}+\\frac{1}{2}\\,\\mathbf{r}_{2j}+\\frac{1}{4}\\,\\mathbf{r}_{2j+1}.\n$$\nUsing the Galerkin construction $\\mathbf{A}_{c}=\\mathbf{R}\\mathbf{A}\\mathbf{P}$, compute explicitly the three-point coarse-grid stencil of $\\mathbf{A}_{c}$ at an interior coarse index $j$ in terms of the four fine-grid mid-edge conductivities $k_{2j-3/2}$, $k_{2j-1/2}$, $k_{2j+1/2}$, and $k_{2j+3/2}$ and the coarse spacing $H$. Then, using first principles and linear-algebraic energy arguments, verify that $\\mathbf{A}_{c}$ is Symmetric Positive Definite (SPD) provided all fine-grid mid-edge conductivities are strictly positive and the boundary conditions are homogeneous Dirichlet. \n\nExpress the final coarse-grid three-point stencil coefficients at coarse node $j$, namely $\\left(a^{c}_{j,j-1},\\,a^{c}_{j,j},\\,a^{c}_{j,j+1}\\right)$, as a single row matrix using the LaTeX $\\texttt{pmatrix}$ environment. No rounding is required. No units are required in the final answer.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It presents a standard task in the analysis of multigrid methods: the derivation of the Galerkin coarse-grid operator for a variable-coefficient diffusion problem. All definitions and conditions are mathematically precise and consistent. The problem is therefore deemed valid.\n\nThe solution is divided into two parts. First, we compute the stencil of the coarse-grid operator $\\mathbf{A}_{c}=\\mathbf{R}\\mathbf{A}\\mathbf{P}$. Second, we verify that $\\mathbf{A}_{c}$ is Symmetric Positive Definite (SPD).\n\n**Part 1: Derivation of the Coarse-Grid Operator Stencil**\n\nThe goal is to compute the action of the coarse-grid operator $\\mathbf{A}_c$ on an arbitrary coarse-grid vector $\\mathbf{v}$. We will compute the $j$-th component of the resulting vector, $(\\mathbf{A}_{c}\\mathbf{v})_{j}$, which is defined as $(\\mathbf{R}(\\mathbf{A}(\\mathbf{P}\\mathbf{v})))_j$. We proceed from the innermost operation outwards.\n\n**Step 1: Action of the Prolongation Operator $\\mathbf{P}$**\nLet $\\mathbf{v}$ be a coarse-grid vector. The prolongation operator $\\mathbf{P}$ maps $\\mathbf{v}$ to a fine-grid vector $\\mathbf{u} = \\mathbf{P}\\mathbf{v}$. The components of $\\mathbf{u}$ in the neighborhood of the fine-grid point $2j$ are given by the interpolation rules:\n$$\n\\mathbf{u}_{2j-2} = \\mathbf{v}_{j-1}\n$$\n$$\n\\mathbf{u}_{2j-1} = \\frac{1}{2}(\\mathbf{v}_{j-1} + \\mathbf{v}_{j})\n$$\n$$\n\\mathbf{u}_{2j} = \\mathbf{v}_{j}\n$$\n$$\n\\mathbf{u}_{2j+1} = \\frac{1}{2}(\\mathbf{v}_{j} + \\mathbf{v}_{j+1})\n$$\n$$\n\\mathbf{u}_{2j+2} = \\mathbf{v}_{j+1}\n$$\n\n**Step 2: Action of the Fine-Grid Operator $\\mathbf{A}$**\nNext, we compute the components of the fine-grid vector $\\mathbf{r} = \\mathbf{A}\\mathbf{u}$. The restriction operator $\\mathbf{R}$ at coarse index $j$ requires the components $\\mathbf{r}_{2j-1}$, $\\mathbf{r}_{2j}$, and $\\mathbf{r}_{2j+1}$. We compute each of these using the definition of $\\mathbf{A}$:\n$$\n(\\mathbf{A}\\mathbf{u})_{i}=\\frac{1}{h^{2}}\\!\\left(-k_{i-1/2}\\,\\mathbf{u}_{i-1}+(k_{i-1/2}+k_{i+1/2})\\,\\mathbf{u}_{i}-k_{i+1/2}\\,\\mathbf{u}_{i+1}\\right)\n$$\nFor clarity, we compute $h^2 \\mathbf{r}_i$:\n\nComponent at index $2j-1$:\n$$\nh^2 \\mathbf{r}_{2j-1} = -k_{2j-3/2}\\mathbf{u}_{2j-2} + (k_{2j-3/2}+k_{2j-1/2})\\mathbf{u}_{2j-1} - k_{2j-1/2}\\mathbf{u}_{2j}\n$$\nSubstituting the expressions for $\\mathbf{u}_{i}$ from Step 1:\n$$\nh^2 \\mathbf{r}_{2j-1} = -k_{2j-3/2}\\mathbf{v}_{j-1} + (k_{2j-3/2}+k_{2j-1/2})\\frac{1}{2}(\\mathbf{v}_{j-1} + \\mathbf{v}_{j}) - k_{2j-1/2}\\mathbf{v}_{j}\n$$\nCollecting terms for $\\mathbf{v}_{j-1}$ and $\\mathbf{v}_{j}$:\n$$\nh^2 \\mathbf{r}_{2j-1} = \\left(-\\frac{1}{2}k_{2j-3/2} + \\frac{1}{2}k_{2j-1/2}\\right)\\mathbf{v}_{j-1} + \\left(\\frac{1}{2}k_{2j-3/2} - \\frac{1}{2}k_{2j-1/2}\\right)\\mathbf{v}_{j}\n$$\n\nComponent at index $2j$:\n$$\nh^2 \\mathbf{r}_{2j} = -k_{2j-1/2}\\mathbf{u}_{2j-1} + (k_{2j-1/2}+k_{2j+1/2})\\mathbf{u}_{2j} - k_{2j+1/2}\\mathbf{u}_{2j+1}\n$$\nSubstituting:\n$$\nh^2 \\mathbf{r}_{2j} = -k_{2j-1/2}\\frac{1}{2}(\\mathbf{v}_{j-1} + \\mathbf{v}_{j}) + (k_{2j-1/2}+k_{2j+1/2})\\mathbf{v}_{j} - k_{2j+1/2}\\frac{1}{2}(\\mathbf{v}_{j} + \\mathbf{v}_{j+1})\n$$\nCollecting terms for $\\mathbf{v}_{j-1}$, $\\mathbf{v}_{j}$, and $\\mathbf{v}_{j+1}$:\n$$\nh^2 \\mathbf{r}_{2j} = \\left(-\\frac{1}{2}k_{2j-1/2}\\right)\\mathbf{v}_{j-1} + \\left(\\frac{1}{2}k_{2j-1/2} + \\frac{1}{2}k_{2j+1/2}\\right)\\mathbf{v}_{j} + \\left(-\\frac{1}{2}k_{2j+1/2}\\right)\\mathbf{v}_{j+1}\n$$\n\nComponent at index $2j+1$:\n$$\nh^2 \\mathbf{r}_{2j+1} = -k_{2j+1/2}\\mathbf{u}_{2j} + (k_{2j+1/2}+k_{2j+3/2})\\mathbf{u}_{2j+1} - k_{2j+3/2}\\mathbf{u}_{2j+2}\n$$\nSubstituting:\n$$\nh^2 \\mathbf{r}_{2j+1} = -k_{2j+1/2}\\mathbf{v}_{j} + (k_{2j+1/2}+k_{2j+3/2})\\frac{1}{2}(\\mathbf{v}_{j} + \\mathbf{v}_{j+1}) - k_{2j+3/2}\\mathbf{v}_{j+1}\n$$\nCollecting terms for $\\mathbf{v}_{j}$ and $\\mathbf{v}_{j+1}$:\n$$\nh^2 \\mathbf{r}_{2j+1} = \\left(-\\frac{1}{2}k_{2j+1/2} + \\frac{1}{2}k_{2j+3/2}\\right)\\mathbf{v}_{j} + \\left(\\frac{1}{2}k_{2j+1/2} - \\frac{1}{2}k_{2j+3/2}\\right)\\mathbf{v}_{j+1}\n$$\n\n**Step 3: Action of the Restriction Operator $\\mathbf{R}$**\nFinally, we apply the full-weighting restriction operator to the vector $\\mathbf{r}$:\n$$\n(\\mathbf{A}_{c}\\mathbf{v})_{j} = (\\mathbf{R}\\mathbf{r})_j = \\frac{1}{4}\\mathbf{r}_{2j-1} + \\frac{1}{2}\\mathbf{r}_{2j} + \\frac{1}{4}\\mathbf{r}_{2j+1}\n$$\nWe substitute the expressions for $\\mathbf{r}_i$ and collect the coefficients for $\\mathbf{v}_{j-1}$, $\\mathbf{v}_{j}$, and $\\mathbf{v}_{j+1}$, which are the stencil coefficients $a^c_{j,j-1}$, $a^c_{j,j}$, and $a^c_{j,j+1}$.\n$$\nh^2(\\mathbf{A}_{c}\\mathbf{v})_{j} = \\frac{1}{4}(h^2\\mathbf{r}_{2j-1}) + \\frac{1}{2}(h^2\\mathbf{r}_{2j}) + \\frac{1}{4}(h^2\\mathbf{r}_{2j+1})\n$$\n\nCoefficient of $\\mathbf{v}_{j-1}$:\n$$\nh^2 a^c_{j,j-1} = \\frac{1}{4}\\left(-\\frac{1}{2}k_{2j-3/2} + \\frac{1}{2}k_{2j-1/2}\\right) + \\frac{1}{2}\\left(-\\frac{1}{2}k_{2j-1/2}\\right) = -\\frac{1}{8}k_{2j-3/2} - \\frac{1}{8}k_{2j-1/2} = -\\frac{1}{8}(k_{2j-3/2} + k_{2j-1/2})\n$$\n\nCoefficient of $\\mathbf{v}_{j+1}$:\nBy symmetry, we can infer this result from the coefficient of $\\mathbf{v}_{j-1}$ by shifting all indices by $+2$:\n$$\nh^2 a^c_{j,j+1} = -\\frac{1}{8}(k_{2(j+1)-3/2} + k_{2(j+1)-1/2}) = -\\frac{1}{8}(k_{2j+1/2} + k_{2j+3/2})\n$$\nLet's verify this directly:\n$$\nh^2 a^c_{j,j+1} = \\frac{1}{2}\\left(-\\frac{1}{2}k_{2j+1/2}\\right) + \\frac{1}{4}\\left(\\frac{1}{2}k_{2j+1/2} - \\frac{1}{2}k_{2j+3/2}\\right) = -\\frac{1}{4}k_{2j+1/2} + \\frac{1}{8}k_{2j+1/2} - \\frac{1}{8}k_{2j+3/2} = -\\frac{1}{8}(k_{2j+1/2} + k_{2j+3/2})\n$$\nThe result is confirmed.\n\nCoefficient of $\\mathbf{v}_{j}$:\n$$\nh^2 a^c_{j,j} = \\frac{1}{4}\\left(\\frac{1}{2}k_{2j-3/2} - \\frac{1}{2}k_{2j-1/2}\\right) + \\frac{1}{2}\\left(\\frac{1}{2}k_{2j-1/2} + \\frac{1}{2}k_{2j+1/2}\\right) + \\frac{1}{4}\\left(-\\frac{1}{2}k_{2j+1/2} + \\frac{1}{2}k_{2j+3/2}\\right)\n$$\n$$\nh^2 a^c_{j,j} = \\frac{1}{8}k_{2j-3/2} - \\frac{1}{8}k_{2j-1/2} + \\frac{1}{4}k_{2j-1/2} + \\frac{1}{4}k_{2j+1/2} - \\frac{1}{8}k_{2j+1/2} + \\frac{1}{8}k_{2j+3/2}\n$$\n$$\nh^2 a^c_{j,j} = \\frac{1}{8}k_{2j-3/2} + \\frac{1}{8}k_{2j-1/2} + \\frac{1}{8}k_{2j+1/2} + \\frac{1}{8}k_{2j+3/2} = \\frac{1}{8}(k_{2j-3/2} + k_{2j-1/2} + k_{2j+1/2} + k_{2j+3/2})\n$$\nNote that $a^c_{j,j-1} + a^c_{j,j} + a^c_{j,j+1} = 0$, which is expected for a conservative scheme.\n\n**Step 4: Express in terms of coarse spacing $H$**\nThe problem uses coarse spacing $H=2h$, so $h^2 = H^2/4$. To find the final coefficients, we divide by $h^2$:\n$$\na^c_{j,j-1} = \\frac{1}{h^2}\\left[-\\frac{1}{8}(k_{2j-3/2} + k_{2j-1/2})\\right] = \\frac{4}{H^2}\\left[-\\frac{1}{8}(k_{2j-3/2} + k_{2j-1/2})\\right] = -\\frac{k_{2j-3/2} + k_{2j-1/2}}{2H^2}\n$$\n$$\na^c_{j,j} = \\frac{1}{h^2}\\left[\\frac{1}{8}(k_{2j-3/2} + k_{2j-1/2} + k_{2j+1/2} + k_{2j+3/2})\\right] = \\frac{k_{2j-3/2} + k_{2j-1/2} + k_{2j+1/2} + k_{2j+3/2}}{2H^2}\n$$\n$$\na^c_{j,j+1} = \\frac{1}{h^2}\\left[-\\frac{1}{8}(k_{2j+1/2} + k_{2j+3/2})\\right] = -\\frac{k_{2j+1/2} + k_{2j+3/2}}{2H^2}\n$$\n\n**Part 2: Verification of Symmetric Positive Definite (SPD) Property**\n\n**Symmetry:**\nThe Galerkin coarse-grid operator is $\\mathbf{A}_{c} = \\mathbf{R}\\mathbf{A}\\mathbf{P}$. An operator is symmetric if it equals its transpose, $\\mathbf{A}_{c} = \\mathbf{A}_{c}^T$.\nThe transpose is $\\mathbf{A}_{c}^T = (\\mathbf{R}\\mathbf{A}\\mathbf{P})^T = \\mathbf{P}^T \\mathbf{A}^T \\mathbf{R}^T$.\nThe fine-grid operator $\\mathbf{A}$ is a symmetric matrix, so $\\mathbf{A}^T = \\mathbf{A}$.\nThe specified restriction and prolongation operators are related by $\\mathbf{R} = \\frac{1}{2}\\mathbf{P}^T$. To see this, we can check their actions in a dual pairing (inner product). For any fine-grid vector $\\mathbf{r}$ and coarse-grid vector $\\mathbf{v}$, $\\langle \\mathbf{r}, \\mathbf{P}\\mathbf{v} \\rangle_{f} = \\langle \\mathbf{P}^T \\mathbf{r}, \\mathbf{v} \\rangle_{c}$. Explicit calculation showed that $\\mathbf{P}^T\\mathbf{r} = 2(\\mathbf{R}\\mathbf{r})$, so $\\mathbf{R}=\\frac{1}{2}\\mathbf{P}^T$.\nSubstituting this into the definition of $\\mathbf{A}_c$ yields $\\mathbf{A}_{c} = \\frac{1}{2}\\mathbf{P}^T \\mathbf{A} \\mathbf{P}$.\nNow we check the transpose of $\\mathbf{A}_{c}$:\n$$\n\\mathbf{A}_{c}^T = \\left(\\frac{1}{2}\\mathbf{P}^T \\mathbf{A} \\mathbf{P}\\right)^T = \\frac{1}{2} \\mathbf{P}^T \\mathbf{A}^T (\\mathbf{P}^T)^T = \\frac{1}{2} \\mathbf{P}^T \\mathbf{A} \\mathbf{P} = \\mathbf{A}_{c}\n$$\nThus, $\\mathbf{A}_{c}$ is symmetric.\n\n**Positive Definiteness:**\nWe must show that for any non-zero coarse-grid vector $\\mathbf{v}$ (satisfying homogeneous boundary conditions), the quadratic form $\\mathbf{v}^T \\mathbf{A}_{c} \\mathbf{v}$ is strictly positive.\n$$\n\\mathbf{v}^T \\mathbf{A}_{c} \\mathbf{v} = \\mathbf{v}^T \\left(\\frac{1}{2}\\mathbf{P}^T \\mathbf{A} \\mathbf{P}\\right) \\mathbf{v} = \\frac{1}{2} (\\mathbf{P}\\mathbf{v})^T \\mathbf{A} (\\mathbf{P}\\mathbf{v})\n$$\nLet $\\mathbf{u} = \\mathbf{P}\\mathbf{v}$. The expression becomes $\\frac{1}{2}\\mathbf{u}^T \\mathbf{A} \\mathbf{u}$.\n\nFirst, we establish that $\\mathbf{A}$ is positive definite. The quadratic form for $\\mathbf{A}$ can be expressed as a discrete energy norm through summation by parts:\n$$\n\\mathbf{u}^T \\mathbf{A} \\mathbf{u} = \\sum_{i=1}^{N-1} \\mathbf{u}_i (\\mathbf{A}\\mathbf{u})_i = \\frac{1}{h^2} \\sum_{i=0}^{N-1} k_{i+1/2}(\\mathbf{u}_{i+1} - \\mathbf{u}_i)^2\n$$\nGiven that the conductivities $k_{i+1/2}$ are strictly positive, this sum of squares is always non-negative. It is zero if and only if $\\mathbf{u}_{i+1} - \\mathbf{u}_i = 0$ for all $i=0, \\dots, N-1$. This implies $\\mathbf{u}$ must be a constant vector. Due to the homogeneous Dirichlet boundary conditions, $\\mathbf{u}_0 = \\mathbf{u}_N = 0$, which forces the constant to be zero. Thus, $\\mathbf{u}^T \\mathbf{A} \\mathbf{u} = 0$ if and only if $\\mathbf{u} = \\mathbf{0}$. Therefore, $\\mathbf{A}$ is positive definite.\n\nSecond, we need to ensure that if $\\mathbf{v} \\neq \\mathbf{0}$, then $\\mathbf{u} = \\mathbf{P}\\mathbf{v} \\neq \\mathbf{0}$. The operator $\\mathbf{P}$ is injective (has a trivial null space). If $\\mathbf{v}$ is a non-zero coarse-grid vector, there exists at least one interior index $j$ such that $\\mathbf{v}_j \\neq 0$. According to the definition of $\\mathbf{P}$, the corresponding fine-grid component is $(\\mathbf{P}\\mathbf{v})_{2j} = \\mathbf{v}_j \\neq 0$. This means $\\mathbf{P}\\mathbf{v}$ cannot be the zero vector.\n\nCombining these two facts: for any non-zero coarse-grid vector $\\mathbf{v}$, the prolonged vector $\\mathbf{u} = \\mathbf{P}\\mathbf{v}$ is a non-zero fine-grid vector. Since $\\mathbf{A}$ is positive definite, $\\mathbf{u}^T\\mathbf{A}\\mathbf{u} > 0$. Consequently:\n$$\n\\mathbf{v}^T \\mathbf{A}_{c} \\mathbf{v} = \\frac{1}{2}\\mathbf{u}^T \\mathbf{A} \\mathbf{u} > 0\n$$\nThis proves that $\\mathbf{A}_{c}$ is positive definite. As it is also symmetric, $\\mathbf{A}_{c}$ is SPD.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{k_{2j-3/2} + k_{2j-1/2}}{2H^2} & \\frac{k_{2j-3/2} + k_{2j-1/2} + k_{2j+1/2} + k_{2j+3/2}}{2H^2} & -\\frac{k_{2j+1/2} + k_{2j+3/2}}{2H^2} \\end{pmatrix}}\n$$"
        }
    ]
}