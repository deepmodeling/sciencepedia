{"hands_on_practices": [{"introduction": "Many advanced constitutive models, particularly those based on neural networks, must respect the principle of material objectivity. A standard method to enforce this is to formulate the model using scalar invariants of deformation tensors as inputs. This practice provides a concrete, foundational exercise in calculating these crucial inputs for the case of simple shear, bridging the gap between abstract continuum mechanics theory and the practical requirements of designing machine learning architectures for mechanics [@problem_id:2656035].", "problem": "An invariant neural network for hyperelastic constitutive modeling takes as inputs the principal invariants of the right Cauchy–Green deformation tensor. Consider the simple shear deformation with deformation gradient given by the second-order tensor $\\boldsymbol{F}$,\n$$\n\\boldsymbol{F}=\\begin{bmatrix}1 & \\gamma & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1\\end{bmatrix},\n$$\nwhere $\\gamma$ is the engineering shear strain (dimensionless). Starting from the definition of the right Cauchy–Green deformation tensor $\\boldsymbol{C}$ and the principal invariants of $\\boldsymbol{C}$, derive closed-form expressions for the three principal invariants $I_{1}$, $I_{2}$, and $I_{3}$ in terms of $\\gamma$ to be used as rotationally objective inputs to the invariant neural network. Express your final answer as a single row matrix $\\big[I_{1}\\; I_{2}\\; I_{3}\\big]$. Do not approximate; provide exact analytic expressions. No units are required.", "solution": "The problem posed is subjected to rigorous scrutiny and is found to be valid. It is scientifically grounded in the principles of continuum mechanics, well-posed, and contains all necessary information for a unique solution. We will proceed with the derivation.\n\nThe objective is to find the three principal invariants, $I_{1}$, $I_{2}$, and $I_{3}$, of the right Cauchy–Green deformation tensor, $\\boldsymbol{C}$, for a simple shear deformation. These invariants serve as objective inputs for constitutive models, such as the invariant neural network mentioned in the problem statement.\n\nThe deformation gradient tensor, $\\boldsymbol{F}$, is given as:\n$$\n\\boldsymbol{F} = \\begin{bmatrix} 1 & \\gamma & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n$$\nwhere $\\gamma$ is the amount of shear.\n\nThe right Cauchy–Green deformation tensor, $\\boldsymbol{C}$, is defined as the product of the transpose of the deformation gradient, $\\boldsymbol{F}^T$, and the deformation gradient, $\\boldsymbol{F}$:\n$$\n\\boldsymbol{C} = \\boldsymbol{F}^T \\boldsymbol{F}\n$$\nFirst, we find the transpose of $\\boldsymbol{F}$:\n$$\n\\boldsymbol{F}^T = \\begin{bmatrix} 1 & 0 & 0 \\\\ \\gamma & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n$$\nNow, we perform the matrix multiplication to find $\\boldsymbol{C}$:\n$$\n\\boldsymbol{C} = \\begin{bmatrix} 1 & 0 & 0 \\\\ \\gamma & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & \\gamma & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n$$\n$$\n\\boldsymbol{C} = \\begin{bmatrix} (1)(1)+(0)(0)+(0)(0) & (1)(\\gamma)+(0)(1)+(0)(0) & (1)(0)+(0)(0)+(0)(1) \\\\ (\\gamma)(1)+(1)(0)+(0)(0) & (\\gamma)(\\gamma)+(1)(1)+(0)(0) & (\\gamma)(0)+(1)(0)+(0)(1) \\\\ (0)(1)+(0)(0)+(1)(0) & (0)(\\gamma)+(0)(1)+(1)(0) & (0)(0)+(0)(0)+(1)(1) \\end{bmatrix}\n$$\nThis calculation yields the components of $\\boldsymbol{C}$:\n$$\n\\boldsymbol{C} = \\begin{bmatrix} 1 & \\gamma & 0 \\\\ \\gamma & 1+\\gamma^2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n$$\nThe principal invariants of a second-order tensor $\\boldsymbol{C}$ in three dimensions are standard scalar quantities derived from its components. They are defined as follows:\n$I_1 = \\text{tr}(\\boldsymbol{C})$\n$I_2 = \\frac{1}{2} [(\\text{tr}(\\boldsymbol{C}))^2 - \\text{tr}(\\boldsymbol{C}^2)]$\n$I_3 = \\det(\\boldsymbol{C})$\n\nWe will now compute each invariant using the derived tensor $\\boldsymbol{C}$.\n\nThe first invariant, $I_1$, is the trace of $\\boldsymbol{C}$, which is the sum of its diagonal elements:\n$$\nI_1 = C_{11} + C_{22} + C_{33} = 1 + (1+\\gamma^2) + 1\n$$\n$$\nI_1 = 3 + \\gamma^2\n$$\nThe third invariant, $I_3$, is the determinant of $\\boldsymbol{C}$. For a physical deformation, the determinant of $\\boldsymbol{F}$ must be positive. We know that $\\det(\\boldsymbol{C}) = \\det(\\boldsymbol{F}^T \\boldsymbol{F}) = \\det(\\boldsymbol{F}^T)\\det(\\boldsymbol{F}) = (\\det(\\boldsymbol{F}))^2 = J^2$, where $J$ is the Jacobian of the deformation. For the given $\\boldsymbol{F}$, which is an upper triangular matrix, the determinant is the product of its diagonal elements:\n$$\nJ = \\det(\\boldsymbol{F}) = (1)(1)(1) = 1\n$$\nTherefore, the third invariant is:\n$$\nI_3 = J^2 = 1^2 = 1\n$$\nThis result demonstrates the isochoric (volume-preserving) nature of simple shear deformation, as $J=1$.\n\nThe second invariant, $I_2$, can be calculated using the formula involving the trace of $\\boldsymbol{C}$ and the trace of its square, but a more direct method is to compute it as the sum of the principal minors of $\\boldsymbol{C}$:\n$$\nI_2 = \\begin{vmatrix} C_{22} & C_{23} \\\\ C_{32} & C_{33} \\end{vmatrix} + \\begin{vmatrix} C_{11} & C_{13} \\\\ C_{31} & C_{33} \\end{vmatrix} + \\begin{vmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\end{vmatrix}\n$$\nSubstituting the components of $\\boldsymbol{C}$:\n$$\nI_2 = \\begin{vmatrix} 1+\\gamma^2 & 0 \\\\ 0 & 1 \\end{vmatrix} + \\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix} + \\begin{vmatrix} 1 & \\gamma \\\\ \\gamma & 1+\\gamma^2 \\end{vmatrix}\n$$\nEvaluating each determinant:\n$$\nI_2 = ((1+\\gamma^2)(1) - (0)(0)) + ((1)(1) - (0)(0)) + ((1)(1+\\gamma^2) - (\\gamma)(\\gamma))\n$$\n$$\nI_2 = (1+\\gamma^2) + 1 + (1+\\gamma^2 - \\gamma^2)\n$$\n$$\nI_2 = 1 + \\gamma^2 + 1 + 1 = 3 + \\gamma^2\n$$\nThus, for simple shear, we find that the first and second invariants are identical.\n\nThe three principal invariants of the right Cauchy–Green tensor for simple shear are:\n$$\nI_1 = 3 + \\gamma^2\n$$\n$$\nI_2 = 3 + \\gamma^2\n$$\n$$\nI_3 = 1\n$$\nThe problem requires the final answer to be presented as a single row matrix $\\begin{pmatrix} I_1 & I_2 & I_3 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 + \\gamma^2 & 3 + \\gamma^2 & 1\n\\end{pmatrix}\n}\n$$", "id": "2656035"}, {"introduction": "Physics-informed learning involves training a model to minimize the residual of a governing partial differential equation. This objective is often balanced with regularization terms that control model complexity and prevent overfitting. This hands-on exercise simplifies this complex optimization into a tangible linear algebra problem, allowing you to directly investigate the trade-off between satisfying physical equilibrium and penalizing large model parameters [@problem_id:2656054].", "problem": "You will study a one-dimensional elastostatic equilibrium in a nondimensionalized setting and quantify how the balance between an $L^2$ weight decay penalty and a physics residual penalty affects the ability of a parametric model to satisfy equilibrium. Consider the domain $[0,1]$, constant Young’s modulus $E=1$, and a uniform body force $b=1$. The strong form of equilibrium is $E\\,u''(x)+b=0$ for $x\\in(0,1)$ with Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. You will approximate the displacement field by a parametric trial function that enforces the boundary conditions exactly: $u(x;\\boldsymbol{\\theta})=x(1-x)\\,p(x;\\boldsymbol{\\theta})$, where $p(x;\\boldsymbol{\\theta})=\\sum_{k=0}^{d}\\theta_k x^k$ is a polynomial of degree $d$ with parameters $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d+1}$. The physics residual at a point $x$ is $r(x;\\boldsymbol{\\theta})=E\\,u''(x;\\boldsymbol{\\theta})+b$. The physics loss is the mean squared residual over a set of $N_c$ interior collocation points $\\{x_i\\}_{i=1}^{N_c}$, namely $L_{\\mathrm{phys}}(\\boldsymbol{\\theta})=\\frac{1}{N_c}\\sum_{i=1}^{N_c} r(x_i;\\boldsymbol{\\theta})^2$. The weight decay penalty is $L_{\\mathrm{wd}}(\\boldsymbol{\\theta})=\\|\\boldsymbol{\\theta}\\|_2^2$. The total loss is $L_{\\mathrm{tot}}(\\boldsymbol{\\theta})=\\lambda_{\\mathrm{phys}}\\,L_{\\mathrm{phys}}(\\boldsymbol{\\theta})+\\lambda_{\\mathrm{wd}}\\,L_{\\mathrm{wd}}(\\boldsymbol{\\theta})$, where $\\lambda_{\\mathrm{phys}}>0$ and $\\lambda_{\\mathrm{wd}}\\ge 0$ are user-specified hyperparameters. Your program must: (a) construct the collocation set as the $N_c$ equally spaced interior points $x_i=\\frac{i}{N_c+1}$ for $i\\in\\{1,\\dots,N_c\\}$, (b) train the parametric model by minimizing $L_{\\mathrm{tot}}(\\boldsymbol{\\theta})$ over $\\boldsymbol{\\theta}$ for each pair $(\\lambda_{\\mathrm{phys}},\\lambda_{\\mathrm{wd}})$, and (c) evaluate equilibrium satisfaction on an evaluation grid of $N_{\\mathrm{eval}}$ interior points $\\{\\hat{x}_j\\}_{j=1}^{N_{\\mathrm{eval}}}$ with $\\hat{x}_j=\\frac{j}{N_{\\mathrm{eval}}+1}$, via the root-mean-square residual $R=\\sqrt{\\frac{1}{N_{\\mathrm{eval}}}\\sum_{j=1}^{N_{\\mathrm{eval}}} r(\\hat{x}_j;\\boldsymbol{\\theta})^2}$. Define a baseline with $\\lambda_{\\mathrm{phys}}=1.0$ and $\\lambda_{\\mathrm{wd}}=10^{-12}$, which is close to pure physics minimization, and compute its baseline root-mean-square residual $R_0$. Declare that over-regularization impairs equilibrium if $R/R_0>10$. Use the following fixed values: polynomial degree $d=6$, number of collocation points $N_c=200$, number of evaluation points $N_{\\mathrm{eval}}=400$. All quantities are nondimensional, so no physical units are required; report dimensionless numbers. Your test suite consists of the following six cases, each as an ordered pair $(\\lambda_{\\mathrm{phys}},\\lambda_{\\mathrm{wd}})$:\n- Case $1$: $(1.0,10^{-12})$.\n- Case $2$: $(1.0,10^{-2})$.\n- Case $3$: $(1.0,10^{2})$.\n- Case $4$: $(10^{-6},10^{-2})$.\n- Case $5$: $(10^{6},10^{-2})$.\n- Case $6$: $(1.0,10^{-8})$.\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one boolean per case in the same order as above, where each boolean is true if and only if $R/R_0>10$ for that case. For example, your program must print a line in the form \"[true_or_false_case_1,true_or_false_case_2,...,true_or_false_case_6]\". The answer must be computed with deterministic numerical linear algebra; no randomization is allowed. The final output must strictly match this single-line format.", "solution": "The problem presented is a valid, well-posed exercise in computational mechanics that explores the intersection with machine learning principles. It requires finding an approximate solution to a one-dimensional elastostatic boundary value problem by minimizing a composite loss function. The core of the task is to analyze the trade-off between satisfying the underlying physical law and penalizing the complexity of the model, a common theme in regularized regression and scientific machine learning. All given parameters and conditions are scientifically sound, consistent, and sufficient for deriving a unique numerical solution.\n\nThe governing equation for the displacement field $u(x)$ is the strong form of elastostatic equilibrium:\n$$E u''(x) + b = 0, \\quad x \\in (0, 1)$$\nwith Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. The constants are Young's modulus $E=1$ and a uniform body force $b=1$.\n\nThe displacement field is approximated by a parametric trial function $u(x; \\boldsymbol{\\theta})$ designed to satisfy the boundary conditions exactly:\n$$u(x; \\boldsymbol{\\theta}) = x(1-x) p(x; \\boldsymbol{\\theta})$$\nwhere $p(x; \\boldsymbol{\\theta})$ is a polynomial of degree $d=6$ with parameters $\\boldsymbol{\\theta} = [\\theta_0, \\theta_1, \\dots, \\theta_d]^T \\in \\mathbb{R}^{d+1}$:\n$$p(x; \\boldsymbol{\\theta}) = \\sum_{k=0}^{d} \\theta_k x^k$$\nTo formulate the optimization problem, we first derive the physics residual $r(x; \\boldsymbol{\\theta})$. This requires the second derivative of $u(x; \\boldsymbol{\\theta})$:\n$$u(x; \\boldsymbol{\\theta}) = \\sum_{k=0}^{d} \\theta_k (x^{k+1} - x^{k+2})$$\n$$u'(x; \\boldsymbol{\\theta}) = \\sum_{k=0}^{d} \\theta_k ((k+1)x^k - (k+2)x^{k+1})$$\n$$u''(x; \\boldsymbol{\\theta}) = \\sum_{k=0}^{d} \\theta_k ((k+1)kx^{k-1} - (k+2)(k+1)x^k)$$\nThe physics residual is defined as $r(x; \\boldsymbol{\\theta}) = E u''(x; \\boldsymbol{\\theta}) + b$. With $E=1$ and $b=1$, this becomes:\n$$r(x; \\boldsymbol{\\theta}) = \\left( \\sum_{k=0}^{d} \\theta_k ((k+1)kx^{k-1} - (k+2)(k+1)x^k) \\right) + 1$$\nThis expression is linear in the parameters $\\boldsymbol{\\theta}$. We can write it as $r(x; \\boldsymbol{\\theta}) = \\mathbf{v}(x)^T \\boldsymbol{\\theta} + 1$, where the $k$-th component of the vector $\\mathbf{v}(x)$ is the coefficient of $\\theta_k$, i.e., $v_k(x) = (k+1)kx^{k-1} - (k+2)(k+1)x^k$.\n\nThe total loss function to be minimized is a weighted sum of the physics loss and a weight decay penalty:\n$$L_{\\mathrm{tot}}(\\boldsymbol{\\theta}) = \\lambda_{\\mathrm{phys}} L_{\\mathrm{phys}}(\\boldsymbol{\\theta}) + \\lambda_{\\mathrm{wd}} L_{\\mathrm{wd}}(\\boldsymbol{\\theta})$$\nThe physics loss, $L_{\\mathrm{phys}}(\\boldsymbol{\\theta})$, is the mean squared residual over $N_c=200$ collocation points $\\{x_i\\}_{i=1}^{N_c}$:\n$$L_{\\mathrm{phys}}(\\boldsymbol{\\theta}) = \\frac{1}{N_c} \\sum_{i=1}^{N_c} r(x_i; \\boldsymbol{\\theta})^2$$\nLet us define a matrix $A \\in \\mathbb{R}^{N_c \\times (d+1)}$ such that $A_{ik} = v_k(x_i)$. The vector of residuals at the collocation points can be written as $A\\boldsymbol{\\theta} + \\mathbf{1}_{N_c}$, where $\\mathbf{1}_{N_c}$ is a vector of ones of length $N_c$. The physics loss is then:\n$$L_{\\mathrm{phys}}(\\boldsymbol{\\theta}) = \\frac{1}{N_c} \\| A\\boldsymbol{\\theta} + \\mathbf{1}_{N_c} \\|_2^2$$\nThe weight decay penalty is the squared $L^2$-norm of the parameter vector:\n$$L_{\\mathrm{wd}}(\\boldsymbol{\\theta}) = \\|\\boldsymbol{\\theta}\\|_2^2 = \\boldsymbol{\\theta}^T \\boldsymbol{\\theta}$$\nCombining these, the total loss function is:\n$$L_{\\mathrm{tot}}(\\boldsymbol{\\theta}) = \\frac{\\lambda_{\\mathrm{phys}}}{N_c} \\| A\\boldsymbol{\\theta} + \\mathbf{1}_{N_c} \\|_2^2 + \\lambda_{\\mathrm{wd}} \\|\\boldsymbol{\\theta}\\|_2^2$$\nThis is a quadratic function of $\\boldsymbol{\\theta}$. To find the optimal parameters $\\boldsymbol{\\theta}^*$ that minimize this loss, we compute the gradient of $L_{\\mathrm{tot}}(\\boldsymbol{\\theta})$ with respect to $\\boldsymbol{\\theta}$ and set it to zero:\n$$\\nabla_{\\boldsymbol{\\theta}} L_{\\mathrm{tot}}(\\boldsymbol{\\theta}) = \\frac{\\lambda_{\\mathrm{phys}}}{N_c} \\nabla_{\\boldsymbol{\\theta}} \\left( (A\\boldsymbol{\\theta} + \\mathbf{1}_{N_c})^T (A\\boldsymbol{\\theta} + \\mathbf{1}_{N_c}) \\right) + \\lambda_{\\mathrm{wd}} \\nabla_{\\boldsymbol{\\theta}} (\\boldsymbol{\\theta}^T \\boldsymbol{\\theta}) = \\mathbf{0}$$\n$$\\nabla_{\\boldsymbol{\\theta}} L_{\\mathrm{tot}}(\\boldsymbol{\\theta}) = \\frac{\\lambda_{\\mathrm{phys}}}{N_c} (2A^T A \\boldsymbol{\\theta} + 2A^T \\mathbf{1}_{N_c}) + 2\\lambda_{\\mathrm{wd}} \\boldsymbol{\\theta} = \\mathbf{0}$$\nRearranging the terms yields a standard linear system of equations, also known as the normal equations for this regularized least-squares problem:\n$$\\left( \\frac{\\lambda_{\\mathrm{phys}}}{N_c} A^T A + \\lambda_{\\mathrm{wd}} I \\right) \\boldsymbol{\\theta} = -\\frac{\\lambda_{\\mathrm{phys}}}{N_c} A^T \\mathbf{1}_{N_c}$$\nwhere $I$ is the $(d+1) \\times (d+1)$ identity matrix. This system is of the form $M\\boldsymbol{\\theta} = \\mathbf{y}$, where:\n$$M = \\frac{\\lambda_{\\mathrm{phys}}}{N_c} A^T A + \\lambda_{\\mathrm{wd}} I$$\n$$\\mathbf{y} = -\\frac{\\lambda_{\\mathrm{phys}}}{N_c} A^T \\mathbf{1}_{N_c}$$\nFor each given pair of hyperparameters $(\\lambda_{\\mathrm{phys}}, \\lambda_{\\mathrm{wd}})$, we solve this $ (d+1) \\times (d+1) $ linear system for $\\boldsymbol{\\theta}$. Since $\\lambda_{\\mathrm{phys}} > 0$ and $\\lambda_{\\mathrm{wd}} \\ge 0$ (and strictly positive in the provided test cases), and $A^T A$ is positive semi-definite, the matrix $M$ is guaranteed to be positive definite and thus invertible, ensuring a unique solution.\n\nOnce the optimal parameter vector $\\boldsymbol{\\theta}^*$ is found, we evaluate the satisfaction of the equilibrium equation using the root-mean-square (RMS) residual $R$ on a separate, finer grid of $N_{\\mathrm{eval}}=400$ evaluation points $\\{\\hat{x}_j\\}_{j=1}^{N_{\\mathrm{eval}}}$.\nThe residual at each evaluation point is $r(\\hat{x}_j; \\boldsymbol{\\theta}^*) = u''(\\hat{x}_j; \\boldsymbol{\\theta}^*) + 1$. This can be computed efficiently by constructing an evaluation matrix $A_{\\mathrm{eval}}$ of size $N_{\\mathrm{eval}} \\times (d+1)$, where $(A_{\\mathrm{eval}})_{jk} = v_k(\\hat{x}_j)$. The vector of residuals on the evaluation grid is $\\mathbf{r}_{\\mathrm{eval}} = A_{\\mathrm{eval}}\\boldsymbol{\\theta}^* + \\mathbf{1}_{N_{\\mathrm{eval}}}$.\nThe RMS residual $R$ is then:\n$$R = \\sqrt{\\frac{1}{N_{\\mathrm{eval}}} \\sum_{j=1}^{N_{\\mathrm{eval}}} r(\\hat{x}_j; \\boldsymbol{\\theta}^*)^2} = \\sqrt{\\frac{1}{N_{\\mathrm{eval}}} \\|\\mathbf{r}_{\\mathrm{eval}}\\|_2^2}$$\nThis procedure is repeated for all test cases. The first case, $(\\lambda_{\\mathrm{phys}}, \\lambda_{\\mathrm{wd}}) = (1.0, 10^{-12})$, serves as a baseline, yielding $R_0$. For each subsequent case, we compute the ratio $R/R_0$ and determine if it exceeds the threshold of $10$, which indicates that over-regularization has significantly impaired the model's ability to satisfy physical equilibrium.\n\nThe computational algorithm is as follows:\n1.  Set fixed parameters: $d=6$, $N_c=200$, $N_{\\mathrm{eval}}=400$.\n2.  Define the set of test cases for $(\\lambda_{\\mathrm{phys}}, \\lambda_{\\mathrm{wd}})$.\n3.  Generate the collocation points $x_i = \\frac{i}{N_c+1}$ for $i=1, \\dots, N_c$.\n4.  Construct the matrix $A$ based on the collocation points.\n5.  Generate the evaluation points $\\hat{x}_j = \\frac{j}{N_{\\mathrm{eval}}+1}$ for $j=1, \\dots, N_{\\mathrm{eval}}$.\n6.  Construct the evaluation matrix $A_{\\mathrm{eval}}$ based on the evaluation points.\n7.  Initialize an empty list to store the computed $R$ values.\n8.  For each test case $(\\lambda_{\\mathrm{phys}}, \\lambda_{\\mathrm{wd}})$:\n    a. Form the system matrix $M$ and the right-hand side vector $\\mathbf{y}$.\n    b. Solve the linear system $M\\boldsymbol{\\theta}^* = \\mathbf{y}$ for $\\boldsymbol{\\theta}^*$.\n    c. Compute the vector of residuals on the evaluation grid $\\mathbf{r}_{\\mathrm{eval}} = A_{\\mathrm{eval}}\\boldsymbol{\\theta}^* + \\mathbf{1}_{N_{\\mathrm{eval}}}$.\n    d. Calculate the RMS residual $R$ and append it to the list.\n9.  Identify the baseline residual $R_0$ from the first test case.\n10. For each stored $R$, evaluate the boolean condition $R/R_0 > 10$.\n11. Format the list of booleans into the required output string.\nThis deterministic procedure, based on numerical linear algebra, provides a complete and rigorous solution to the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 1D elastostatic problem using a parametric polynomial model\n    and evaluates the effect of regularization on equilibrium satisfaction.\n    \"\"\"\n    # Fixed values from the problem statement\n    d = 6\n    Nc = 200\n    N_eval = 400\n    \n    # Test cases for (lambda_phys, lambda_wd)\n    test_cases = [\n        (1.0, 10**-12),\n        (1.0, 10**-2),\n        (1.0, 10**2),\n        (10**-6, 10**-2),\n        (10**6, 10**-2),\n        (1.0, 10**-8),\n    ]\n\n    def construct_A_matrix(x_points, degree):\n        \"\"\"\n        Constructs the matrix A where A_ik is the coefficient of theta_k in u''(x_i).\n        A_ik = v_k(x_i) = (k+1)k*x_i^(k-1) - (k+2)(k+1)*x_i^k\n        \"\"\"\n        n_pts = len(x_points)\n        A = np.zeros((n_pts, degree + 1))\n        \n        #\n        # The term for u'' corresponding to theta_k is the second derivative of\n        # x^(k+1) - x^(k+2), which is (k+1)*k*x^(k-1) - (k+2)*(k+1)*x^k.\n        \n        # Handle k=0 separately to avoid issues with x**(-1)\n        # For k=0: (1)*0*x^(-1) - (2)*(1)*x^0 = -2\n        A[:, 0] = -2.0\n        \n        # For k >= 1\n        for k in range(1, degree + 1):\n            term1 = k * (k + 1) * np.power(x_points, k - 1)\n            term2 = (k + 1) * (k + 2) * np.power(x_points, k)\n            A[:, k] = term1 - term2\n            \n        return A\n\n    # (a) Construct the collocation and evaluation grids\n    x_c = np.linspace(0, 1, Nc + 2)[1:-1]\n    x_eval = np.linspace(0, 1, N_eval + 2)[1:-1]\n    \n    A_c = construct_A_matrix(x_c, d)\n    A_eval = construct_A_matrix(x_eval, d)\n    \n    rms_residuals = []\n\n    # (b) Train the model for each hyperparameter pair\n    for lambda_phys, lambda_wd in test_cases:\n        # Set up the linear system M * theta = y\n        # M = (lambda_phys/Nc) * A_c.T @ A_c + lambda_wd * I\n        # y = -(lambda_phys/Nc) * A_c.T @ 1\n        \n        A_c_T_A_c = A_c.T @ A_c\n        I = np.identity(d + 1)\n        \n        M = (lambda_phys / Nc) * A_c_T_A_c + lambda_wd * I\n        \n        y = -(lambda_phys / Nc) * (A_c.T @ np.ones(Nc))\n        \n        # Solve for the optimal parameters theta\n        theta = np.linalg.solve(M, y)\n        \n        # (c) Evaluate equilibrium satisfaction on the evaluation grid\n        # Residual vector r = A_eval * theta + 1\n        r_eval = A_eval @ theta + np.ones(N_eval)\n        \n        # Root-mean-square residual R\n        R = np.sqrt(np.mean(r_eval**2))\n        rms_residuals.append(R)\n\n    # Define baseline and check for over-regularization\n    R0 = rms_residuals[0]\n    \n    results = [r / R0 > 10 for r in rms_residuals]\n\n    # Format the final output string\n    # E.g., \"[true,false,true]\"\n    output_str = f\"[{','.join(str(b).lower() for b in results)}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "2656054"}, {"introduction": "For a data-driven model to be physically trustworthy, its predictions must adhere to fundamental axioms like the second law of thermodynamics. Fitting to noisy data offers no such guarantee. This advanced practice guides you through the rigorous process of designing a penalty in the loss function to enforce thermodynamic admissibility, demonstrating how to analytically derive the necessary penalty weight from statistical principles and data characteristics [@problem_id:2656066].", "problem": "You are asked to formalize a thermodynamically admissible learning problem for a one-dimensional, isothermal, linear Kelvin–Voigt solid subjected to prescribed strain histories. Your model predicts the viscous parameter such that the viscoelastic dissipation remains nonnegative. You will implement a soft-constraint penalty in the empirical risk and determine, from first principles, how the penalty weight must scale relative to the data-fit term to control the probability of violating the second law of thermodynamics under noisy observations.\n\nContext and foundational basis:\n- The Clausius–Duhem inequality for isothermal processes imposes the nonnegative mechanical dissipation constraint, namely that the internal dissipation density satisfies $\\,\\mathcal{D} \\ge 0\\,$.\n- For a Kelvin–Voigt solid with elastic modulus $\\,E\\,$ and viscosity $\\,\\eta \\ge 0\\,$, the Cauchy stress is $\\,\\sigma = E\\,\\varepsilon + \\eta\\,\\dot{\\varepsilon}\\,$ and the Helmholtz free energy density is $\\,\\psi = \\tfrac{1}{2} E\\,\\varepsilon^2\\,$. The mechanical dissipation density simplifies to $\\,\\mathcal{D} = \\eta\\,\\dot{\\varepsilon}^2\\,$, which is thermodynamically admissible if and only if $\\,\\eta \\ge 0\\,$.\n- A data set $\\{(\\varepsilon_i,\\dot{\\varepsilon}_i,y_i)\\}_{i=1}^N$ is collected, where $\\,y_i\\,$ are noisy stress measurements obeying $\\,y_i = E\\,\\varepsilon_i + \\eta_{\\mathrm{true}}\\,\\dot{\\varepsilon}_i + \\xi_i\\,$ with independent, identically distributed zero-mean Gaussian noise $\\,\\xi_i \\sim \\mathcal{N}(0,s_\\xi^2)\\,$. The elastic modulus $\\,E\\,$ is known and the only unknown model parameter to be learned is the viscosity $\\,\\eta\\,$.\n\nLearning setup:\n- Consider the empirical mean-squared error data-fit term,\n$$\n\\mathcal{L}_{\\mathrm{data}}(\\eta) = \\frac{1}{N}\\sum_{i=1}^N\\left(E\\,\\varepsilon_i + \\eta\\,\\dot{\\varepsilon}_i - y_i\\right)^2.\n$$\n- Incorporate a soft-constraint penalty that discourages negative dissipation,\n$$\n\\mathcal{L}_{\\mathrm{pen}}(\\eta) = \\lambda\\,\\left\\langle -\\mathcal{D} \\right\\rangle_+^{\\mathrm{emp}} \\equiv \\lambda\\,\\frac{1}{N}\\sum_{i=1}^N\\left\\langle -\\eta\\,\\dot{\\varepsilon}_i^2 \\right\\rangle_+,\n$$\nwhere $\\,\\langle a\\rangle_+ = \\max(0,a)\\,$ denotes the positive-part function and $\\,\\lambda \\ge 0\\,$ is the penalty weight.\n\nObjective:\n- Define the total loss $\\,\\mathcal{L}(\\eta) = \\mathcal{L}_{\\mathrm{data}}(\\eta) + \\mathcal{L}_{\\mathrm{pen}}(\\eta)\\,$.\n- Starting only from the above fundamental and well-tested bases, analyze the optimizer of $\\,\\mathcal{L}(\\eta)\\,$ and derive a quantitative expression for the minimal $\\,\\lambda\\,$ that ensures the probability of learning a thermodynamically inadmissible viscosity $\\,\\eta_{\\ast} < 0\\,$ is upper-bounded by a prescribed tolerance $\\,\\alpha \\in (0,1)\\,$. Your analysis must reveal the explicit scaling of $\\,\\lambda\\,$ with the noise level $\\,s_\\xi\\,$, the sample size $\\,N\\,$, and the empirical second moment of the strain rates\n$$\n\\mu_2 \\equiv \\frac{1}{N}\\sum_{i=1}^N \\dot{\\varepsilon}_i^2.\n$$\n- Justify any probabilistic bound you use, and state clearly any regularity or independence assumptions.\n\nComputational task:\n- Implement a program that, for each test case in the suite below, computes the minimal penalty weight $\\,\\lambda_{\\min}\\,$ that guarantees $\\,\\mathbb{P}(\\eta_{\\ast} < 0) \\le \\alpha\\,$ under the stated Gaussian noise model and independence assumptions. Express all numeric outputs as dimensionless floating-point numbers. You may assume $\\,\\mu_2 > 0\\,$ in all test cases.\n\nTest suite:\n- Use the following parameter tuples $\\,\\left(s_\\xi, N, \\mu_2, \\eta_{\\mathrm{true}}, \\alpha\\right)\\,$:\n  - Case A: $\\,\\left(1.0,\\,20,\\,0.5,\\,0.05,\\,0.01\\right)\\,$.\n  - Case B: $\\,\\left(1.0,\\,1000,\\,2.0,\\,0.10,\\,0.01\\right)\\,$.\n  - Case C: $\\,\\left(0.5,\\,10,\\,10^{-4},\\,0.01,\\,0.05\\right)\\,$.\n  - Case D: $\\,\\left(10.0,\\,5,\\,1.0,\\,0.20,\\,0.50\\right)\\,$.\n  - Case E: $\\,\\left(0.2,\\,1,\\,0.8,\\,0.00,\\,0.005\\right)\\,$.\n\nWhat to output:\n- Your program should produce a single line of output containing a comma-separated list of the five computed $\\,\\lambda_{\\min}\\,$ values, in the order of Cases A through E, enclosed in square brackets, for example $\\,\\left[\\text{value}_A,\\text{value}_B,\\text{value}_C,\\text{value}_D,\\text{value}_E\\right]\\,$.", "solution": "The problem is to formalize and solve a thermodynamically-constrained learning problem for the viscosity parameter $\\eta$ of a one-dimensional linear Kelvin–Voigt solid. We are given a data-generating process and a corresponding empirical risk functional. The objective is to derive the minimum penalty weight $\\lambda$ that ensures the probability of obtaining a thermodynamically inadmissible solution, $\\eta_\\ast < 0$, is bounded by a specified tolerance $\\alpha$. The foundation of thermodynamic admissibility is the Clausius–Duhem inequality for isothermal processes, which for this model simplifies to the constraint that the viscosity must be non-negative, $\\eta \\ge 0$.\n\nThe total loss function to be minimized is given by $\\mathcal{L}(\\eta) = \\mathcal{L}_{\\mathrm{data}}(\\eta) + \\mathcal{L}_{\\mathrm{pen}}(\\eta)$.\nThe data-fit term is the mean-squared error:\n$$\n\\mathcal{L}_{\\mathrm{data}}(\\eta) = \\frac{1}{N}\\sum_{i=1}^N\\left(E\\,\\varepsilon_i + \\eta\\,\\dot{\\varepsilon}_i - y_i\\right)^2\n$$\nThe penalty term enforces the non-negativity of dissipation:\n$$\n\\mathcal{L}_{\\mathrm{pen}}(\\eta) = \\lambda\\,\\frac{1}{N}\\sum_{i=1}^N\\left\\langle -\\eta\\,\\dot{\\varepsilon}_i^2 \\right\\rangle_+ = \\lambda\\,\\left\\langle -\\eta \\right\\rangle_+ \\left( \\frac{1}{N}\\sum_{i=1}^N \\dot{\\varepsilon}_i^2 \\right) = \\lambda \\mu_2 \\left\\langle -\\eta \\right\\rangle_+\n$$\nwhere $\\langle a \\rangle_+ = \\max(0,a)$ and $\\mu_2 = \\frac{1}{N}\\sum_{i=1}^N \\dot{\\varepsilon}_i^2$. The full loss function is therefore:\n$$\n\\mathcal{L}(\\eta) = \\frac{1}{N}\\sum_{i=1}^N\\left(E\\,\\varepsilon_i + \\eta\\,\\dot{\\varepsilon}_i - y_i\\right)^2 + \\lambda \\mu_2 \\left\\langle -\\eta \\right\\rangle_+\n$$\nThis loss function $\\mathcal{L}(\\eta)$ is convex, as it is a sum of a convex quadratic function and a convex penalty function. Its minimizer $\\eta_\\ast$ is unique, provided $\\mu_2 > 0$. To find the minimizer $\\eta_\\ast$, we apply the subgradient optimality condition, $0 \\in \\partial \\mathcal{L}(\\eta_\\ast)$.\n\nFirst, let us analyze the unconstrained minimizer of the data-fit term, $\\mathcal{L}_{\\mathrm{data}}(\\eta)$, which we denote by $\\eta_{\\mathrm{unc}}$. Setting the derivative to zero:\n$$\n\\frac{d\\mathcal{L}_{\\mathrm{data}}}{d\\eta} = \\frac{2}{N}\\sum_{i=1}^N\\left(E\\,\\varepsilon_i + \\eta\\,\\dot{\\varepsilon}_i - y_i\\right)\\dot{\\varepsilon}_i = 0\n$$\nSolving for $\\eta$ yields:\n$$\n\\eta_{\\mathrm{unc}} = \\frac{\\sum_{i=1}^N (y_i - E\\,\\varepsilon_i)\\dot{\\varepsilon}_i}{\\sum_{i=1}^N \\dot{\\varepsilon}_i^2}\n$$\nSubstituting the data generation model $y_i = E\\,\\varepsilon_i + \\eta_{\\mathrm{true}}\\,\\dot{\\varepsilon}_i + \\xi_i$:\n$$\n\\eta_{\\mathrm{unc}} = \\frac{\\sum_{i=1}^N (\\eta_{\\mathrm{true}}\\,\\dot{\\varepsilon}_i + \\xi_i)\\dot{\\varepsilon}_i}{\\sum_{i=1}^N \\dot{\\varepsilon}_i^2} = \\eta_{\\mathrm{true}} + \\frac{\\sum_{i=1}^N \\xi_i \\dot{\\varepsilon}_i}{\\sum_{i=1}^N \\dot{\\varepsilon}_i^2} = \\eta_{\\mathrm{true}} + \\frac{\\frac{1}{N}\\sum_{i=1}^N \\xi_i \\dot{\\varepsilon}_i}{\\mu_2}\n$$\nLet us define the stochastic term $Z = \\frac{1}{N}\\sum_{i=1}^N \\xi_i \\dot{\\varepsilon}_i$. Then, $\\eta_{\\mathrm{unc}} = \\eta_{\\mathrm{true}} + Z/\\mu_2$.\n\nNow, we analyze the minimizer $\\eta_\\ast$ of the full loss $\\mathcal{L}(\\eta)$. The subdifferential of $\\mathcal{L}(\\eta)$ is:\n$$\n\\partial\\mathcal{L}(\\eta) = \\frac{d\\mathcal{L}_{\\mathrm{data}}}{d\\eta} + \\lambda \\mu_2 \\partial\\left\\langle -\\eta \\right\\rangle_+\n$$\nThe subdifferential of the penalty component is:\n$$\n\\partial\\left\\langle -\\eta \\right\\rangle_+ =\n\\begin{cases}\n    \\{0\\} & \\text{if } \\eta > 0 \\\\\n    \\{-1\\} & \\text{if } \\eta < 0 \\\\\n    [-1, 0] & \\text{if } \\eta = 0\n\\end{cases}\n$$\nThe optimality condition $0 \\in \\partial \\mathcal{L}(\\eta_\\ast)$ leads to three cases based on the value of $\\eta_\\ast$:\n1. If $\\eta_\\ast > 0$: $\\frac{d\\mathcal{L}_{\\mathrm{data}}}{d\\eta}|_{\\eta_\\ast} = 0$, which implies $\\eta_\\ast = \\eta_{\\mathrm{unc}}$. This is consistent only if $\\eta_{\\mathrm{unc}} > 0$.\n2. If $\\eta_\\ast < 0$: $\\frac{d\\mathcal{L}_{\\mathrm{data}}}{d\\eta}|_{\\eta_\\ast} + \\lambda \\mu_2 (-1) = 0$. The derivative of $\\mathcal{L}_{\\mathrm{data}}$ is $2(\\eta \\mu_2 - (\\eta_{\\mathrm{true}}\\mu_2 + Z))$. So, $2(\\eta_\\ast \\mu_2 - (\\eta_{\\mathrm{true}}\\mu_2 + Z)) - \\lambda\\mu_2 = 0$, which solves to $\\eta_\\ast = \\eta_{\\mathrm{true}} + Z/\\mu_2 + \\lambda/2 = \\eta_{\\mathrm{unc}} + \\lambda/2$. This case is consistent only if the resulting $\\eta_\\ast$ is negative, i.e., $\\eta_{\\mathrm{unc}} + \\lambda/2 < 0$, or $\\eta_{\\mathrm{unc}} < -\\lambda/2$.\n3. If $\\eta_\\ast = 0$: $0 \\in \\frac{d\\mathcal{L}_{\\mathrm{data}}}{d\\eta}|_0 + \\lambda \\mu_2 [-1,0]$. This requires that $0$ is between $-\\frac{d\\mathcal{L}_{\\mathrm{data}}}{d\\eta}|_0$ and $-\\frac{d\\mathcal{L}_{\\mathrm{data}}}{d\\eta}|_0 + \\lambda\\mu_2$. This corresponds to $-\\lambda/2 \\le \\eta_{\\mathrm{unc}} \\le 0$.\n\nIn summary, the learned parameter $\\eta_\\ast$ is thermodynamically inadmissible ($\\eta_\\ast < 0$) if and only if the unconstrained minimizer $\\eta_{\\mathrm{unc}}$ falls below a certain threshold:\n$$\n\\eta_\\ast < 0 \\iff \\eta_{\\mathrm{unc}} < -\\frac{\\lambda}{2}\n$$\nThe objective is to ensure $\\mathbb{P}(\\eta_\\ast < 0) \\le \\alpha$. This is equivalent to ensuring $\\mathbb{P}(\\eta_{\\mathrm{unc}} < -\\lambda/2) \\le \\alpha$.\n\nTo evaluate this probability, we must determine the distribution of $\\eta_{\\mathrm{unc}}$. Since the noise terms $\\xi_i \\sim \\mathcal{N}(0, s_\\xi^2)$ are independent and identically distributed Gaussian variables, and treating the strain rates $\\dot{\\varepsilon}_i$ as deterministic, the term $Z = \\frac{1}{N}\\sum_{i=1}^N \\dot{\\varepsilon}_i \\xi_i$ is a linear combination of independent Gaussian variables, and is therefore itself a Gaussian variable.\nThe mean of $Z$ is $\\mathbb{E}[Z] = \\frac{1}{N}\\sum_{i=1}^N \\dot{\\varepsilon}_i \\mathbb{E}[\\xi_i] = 0$.\nThe variance of $Z$ is:\n$$\n\\mathrm{Var}(Z) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N \\dot{\\varepsilon}_i \\xi_i\\right) = \\frac{1}{N^2}\\sum_{i=1}^N \\dot{\\varepsilon}_i^2 \\mathrm{Var}(\\xi_i) = \\frac{s_\\xi^2}{N^2}\\sum_{i=1}^N \\dot{\\varepsilon}_i^2 = \\frac{s_\\xi^2(N\\mu_2)}{N^2} = \\frac{s_\\xi^2 \\mu_2}{N}\n$$\nThus, $Z \\sim \\mathcal{N}\\left(0, \\frac{s_\\xi^2 \\mu_2}{N}\\right)$.\nThe unconstrained estimator $\\eta_{\\mathrm{unc}} = \\eta_{\\mathrm{true}} + Z/\\mu_2$ is also Gaussian, with mean $\\mathbb{E}[\\eta_{\\mathrm{unc}}] = \\eta_{\\mathrm{true}}$ and variance $\\mathrm{Var}(\\eta_{\\mathrm{unc}}) = \\mathrm{Var}(Z/\\mu_2) = (1/\\mu_2^2) \\mathrm{Var}(Z) = \\frac{s_\\xi^2}{N\\mu_2}$.\n\nThe probabilistic constraint is $\\mathbb{P}(\\eta_{\\mathrm{true}} + Z/\\mu_2 < -\\lambda/2) \\le \\alpha$, which can be rewritten as:\n$$\n\\mathbb{P}\\left(Z < -\\left(\\eta_{\\mathrm{true}} + \\frac{\\lambda}{2}\\right)\\mu_2\\right) \\le \\alpha\n$$\nLet $W = Z / \\sigma_Z$ be the standardized variable, where $\\sigma_Z = \\sqrt{\\mathrm{Var}(Z)} = s_\\xi \\sqrt{\\mu_2/N}$. Then $W \\sim \\mathcal{N}(0,1)$. The inequality becomes:\n$$\n\\mathbb{P}\\left(W < -\\frac{(\\eta_{\\mathrm{true}} + \\lambda/2)\\mu_2}{\\sigma_Z}\\right) = \\mathbb{P}\\left(W < -\\frac{(\\eta_{\\mathrm{true}} + \\lambda/2)\\sqrt{N\\mu_2}}{s_\\xi}\\right) \\le \\alpha\n$$\nLet $\\Phi$ be the cumulative distribution function (CDF) of the standard normal distribution. We have:\n$$\n\\Phi\\left(-\\frac{(\\eta_{\\mathrm{true}} + \\lambda/2)\\sqrt{N\\mu_2}}{s_\\xi}\\right) \\le \\alpha\n$$\nApplying the inverse CDF, $\\Phi^{-1}$, to both sides:\n$$\n-\\frac{(\\eta_{\\mathrm{true}} + \\lambda/2)\\sqrt{N\\mu_2}}{s_\\xi} \\le \\Phi^{-1}(\\alpha)\n$$\nIt is conventional to use the upper-tail critical value $z_{u} = \\Phi^{-1}(1-u)$, so $\\Phi^{-1}(\\alpha) = -\\Phi^{-1}(1-\\alpha) = -z_\\alpha$.\n$$\n\\frac{(\\eta_{\\mathrm{true}} + \\lambda/2)\\sqrt{N\\mu_2}}{s_\\xi} \\ge -\\Phi^{-1}(\\alpha) = \\Phi^{-1}(1-\\alpha)\n$$\nTo find the minimal required penalty weight $\\lambda$, we solve this relation for $\\lambda$ assuming equality, which gives the tightest bound.\n$$\n\\eta_{\\mathrm{true}} + \\frac{\\lambda}{2} = \\frac{s_\\xi \\Phi^{-1}(1-\\alpha)}{\\sqrt{N\\mu_2}}\n$$\n$$\n\\lambda = 2\\left(\\frac{s_\\xi \\Phi^{-1}(1-\\alpha)}{\\sqrt{N\\mu_2}} - \\eta_{\\mathrm{true}}\\right)\n$$\nSince the penalty weight must be non-negative, $\\lambda \\ge 0$, the final expression for the minimal penalty weight is:\n$$\n\\lambda_{\\min} = \\max\\left(0, 2\\left(\\frac{s_\\xi \\Phi^{-1}(1-\\alpha)}{\\sqrt{N\\mu_2}} - \\eta_{\\mathrm{true}}\\right)\\right)\n$$\nThis result demonstrates the explicit scaling of $\\lambda$ with the problem parameters. The penalty increases with noise $s_\\xi$ and stricter tolerance (smaller $\\alpha$), and decreases with more data $N$, larger signal-to-noise ratio proxy $\\mu_2$, and larger true viscosity $\\eta_{\\mathrm{true}}$. This is physically and statistically consistent.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the minimal penalty weight lambda_min for a series of test cases\n    to ensure the probability of learning a thermodynamically inadmissible\n    viscosity is bounded by a tolerance alpha.\n    \"\"\"\n\n    # Test suite: (s_xi, N, mu_2, eta_true, alpha)\n    test_cases = [\n        (1.0, 20, 0.5, 0.05, 0.01),    # Case A\n        (1.0, 1000, 2.0, 0.10, 0.01),  # Case B\n        (0.5, 10, 1e-4, 0.01, 0.05),   # Case C\n        (10.0, 5, 1.0, 0.20, 0.50),   # Case D\n        (0.2, 1, 0.8, 0.00, 0.005),   # Case E\n    ]\n\n    results = []\n    for case in test_cases:\n        s_xi, N, mu_2, eta_true, alpha = case\n\n        # The term Phi^{-1}(1-alpha) is the upper-tail critical value of the\n        # standard normal distribution, which can be computed using the\n        # percent point function (ppf) of scipy.stats.norm.\n        # norm.ppf(1 - alpha) gives the value z such that P(Z <= z) = 1 - alpha.\n        z_alpha = norm.ppf(1 - alpha)\n\n        # The term sqrt(N * mu_2) can be zero if mu_2 is zero. The problem statement\n        # assumes mu_2 > 0.\n        denominator = np.sqrt(N * mu_2)\n        \n        # Calculate the argument for the max function based on the derived formula:\n        # lambda = 2 * ( (s_xi * z_alpha / sqrt(N*mu_2)) - eta_true )\n        term = (s_xi * z_alpha / denominator) - eta_true\n        \n        # The penalty weight lambda must be non-negative.\n        lambda_min = max(0.0, 2.0 * term)\n        \n        results.append(lambda_min)\n\n    # Format the output as a comma-separated list enclosed in square brackets.\n    # The map(str, ...) ensures all numbers are converted to their string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2656066"}]}