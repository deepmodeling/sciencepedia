{"hands_on_practices": [{"introduction": "The defining challenge of any inverse heat conduction problem is its inherent ill-posedness, where small errors in temperature measurements can lead to large, non-physical oscillations in the estimated heat flux. This practice guides you through the fundamental connection between the physics of heat diffusion and the mathematical structure that causes this instability. By analyzing how the heat equation acts as a low-pass filter, you will see how the Singular Value Decomposition (SVD) of the discretized forward operator provides a clear \"spectral\" picture of this ill-posedness, motivating regularization through Truncated SVD (TSVD) [@problem_id:2497780].", "problem": "An Inverse Heat Conduction Problem (IHCP) arises when an unknown time-dependent surface heat flux $q(t)$ on the boundary $x=0$ of a homogeneous semi-infinite solid $x \\ge 0$ must be inferred from interior temperature measurements. Consider the one-dimensional heat equation $\\partial T/\\partial t = \\alpha \\, \\partial^{2} T/\\partial x^{2}$ with thermal diffusivity $\\alpha>0$, zero initial condition, and governing boundary condition $-k \\, \\partial T/\\partial x(0,t) = q(t)$, where $k>0$ is the thermal conductivity. Temperature is measured at a fixed interior location $x=x_{m}>0$ for times $t \\in (0,T]$. After discretizing $q(t)$ on a suitable temporal basis and sampling the measured temperature, the forward map from coefficients of $q$ to the data $y \\in \\mathbb{R}^{m}$ is a linear system $A q = y + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ and $\\varepsilon$ represents measurement noise.\n\nStarting from the heat equation and its Laplace-transform solution in the semi-infinite domain, argue how temporal oscillations in $q(t)$ are transmitted to $T(x_{m},t)$ and how that transmission depends on frequency. Then interpret the Singular Value Decomposition (SVD) of $A$ in this context and explain which statements correctly characterize Truncated Singular Value Decomposition (TSVD) at truncation index $k$ in terms of filtering properties relative to the SVD expansion.\n\nChoose all statements that are correct:\n\nA. In the SVD $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r} > 0$ and right singular vectors $v_{i}$, the TSVD solution at index $k$ is $\\widehat{q}_{k} = \\sum_{i=1}^{k} (u_{i}^{\\top} y/\\sigma_{i}) \\, v_{i}$. Because the heat equation attenuates high temporal frequencies, the right singular vectors $v_{i}$ tend to become more oscillatory as $i$ increases, so truncation at $k$ filters out higher-frequency components of $q$.\n\nB. TSVD keeps the $k$ smallest singular values to avoid over-smoothing and thereby emphasizes high-frequency content, which is desirable because measurement noise in IHCPs is predominantly low frequency.\n\nC. For diffusive forward operators arising from the heat equation, small singular values are associated with increasingly oscillatory right singular vectors. Truncating the SVD expansion at $k$ imposes an effective low-pass filter on $q$, with a cutoff determined by $k$.\n\nD. The TSVD filter factors in the spectral expansion are $f_{i} = \\sigma_{i}/(\\sigma_{i}^{2}+\\lambda)$ for some $\\lambda>0$, so truncating at $k$ is equivalent to choosing an appropriate $\\lambda$ in Tikhonov regularization.", "solution": "### Derivation and Analysis\n\nThe analysis begins with the continuous problem to understand the physics of the forward operator. The one-dimensional heat equation is $\\frac{\\partial T}{\\partial t} = \\alpha \\frac{\\partial^2 T}{\\partial x^2}$. We apply the Laplace transform with respect to time $t$, with $s$ as the transform variable. Let $\\bar{T}(x,s) = \\mathcal{L}\\{T(x,t)\\}$. Given the zero initial condition $T(x,0)=0$, the transformed PDE becomes an ODE in $x$:\n$$ s\\bar{T}(x,s) = \\alpha \\frac{d^2 \\bar{T}}{dx^2} \\implies \\frac{d^2 \\bar{T}}{dx^2} - \\frac{s}{\\alpha}\\bar{T} = 0 $$\nThe general solution is $\\bar{T}(x,s) = C_1 e^{\\sqrt{s/\\alpha}x} + C_2 e^{-\\sqrt{s/\\alpha}x}$. For the temperature to remain bounded as $x \\to \\infty$, we must set $C_1 = 0$, assuming $\\text{Re}(\\sqrt{s}) > 0$. Thus, $\\bar{T}(x,s) = C_2 e^{-\\sqrt{s/\\alpha}x}$.\n\nThe constant $C_2$ is found from the boundary condition at $x=0$. The transformed boundary condition is $-k \\frac{d\\bar{T}}{dx}(0,s) = \\bar{q}(s)$, where $\\bar{q}(s) = \\mathcal{L}\\{q(t)\\}$. Differentiating $\\bar{T}(x,s)$ yields $\\frac{d\\bar{T}}{dx} = -C_2 \\sqrt{s/\\alpha} e^{-\\sqrt{s/\\alpha}x}$. At $x=0$, this gives $-k(-C_2\\sqrt{s/\\alpha}) = \\bar{q}(s)$, which determines $C_2 = \\frac{\\bar{q}(s)}{k\\sqrt{s/\\alpha}}$.\n\nThe complete solution in the Laplace domain is:\n$$ \\bar{T}(x,s) = \\bar{q}(s) \\frac{e^{-\\sqrt{s/\\alpha}x}}{k\\sqrt{s/\\alpha}} $$\nThis equation describes the transfer function from the input flux $\\bar{q}(s)$ to the temperature response $\\bar{T}(x,s)$. To understand the frequency dependence, we consider a sinusoidal input by setting $s = i\\omega$, where $\\omega$ is the angular frequency. Using the identity $\\sqrt{i} = (1+i)/\\sqrt{2}$, we have $\\sqrt{i\\omega/\\alpha} = \\sqrt{\\omega/(2\\alpha)}(1+i)$. The response at a location $x$ is then:\n$$ \\bar{T}(x, i\\omega) = \\bar{q}(i\\omega) \\frac{e^{-\\sqrt{\\omega/(2\\alpha)}x \\cdot (1+i)}}{k\\sqrt{i\\omega/\\alpha}} = \\bar{q}(i\\omega) \\frac{e^{-\\sqrt{\\omega/(2\\alpha)}x} e^{-i\\sqrt{\\omega/(2\\alpha)}x}}{k\\sqrt{i\\omega/\\alpha}} $$\nThe magnitude of the transfer function at a depth $x$ behaves as $|\\bar{T}(x,i\\omega)/\\bar{q}(i\\omega)| \\propto e^{-\\sqrt{\\omega/(2\\alpha)}x}$. This demonstrates that the amplitude of temperature oscillations is exponentially attenuated with both depth $x$ and the square root of the frequency $\\omega$. High-frequency components of the heat flux $q(t)$ are severely damped and barely register at the interior sensor location $x_m$.\n\nThis physical insight directly informs the structure of the discretized forward operator $A$. The matrix $A$ models this strong damping (low-pass filtering) behavior. We consider its Singular Value Decomposition (SVD), $A = U \\Sigma V^{\\top} = \\sum_{i=1}^r \\sigma_i u_i v_i^\\top$, where $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$ are the singular values.\n- The right singular vectors $v_i$ form an orthonormal basis for the input space (discretized flux $q$).\n- The left singular vectors $u_i$ form an orthonormal basis for the output space (measured temperature $y$).\n- The singular value $\\sigma_i$ is the amplification factor for the input pattern $v_i$.\nBecause the physical system (heat conduction) strongly attenuates high-frequency inputs, an input flux $q$ that is highly oscillatory will produce a very small temperature response $y$. In the SVD framework, this means that right singular vectors $v_i$ representing increasingly oscillatory (high-frequency) flux patterns must be associated with rapidly decreasing singular values $\\sigma_i$. Thus, as the index $i$ increases, $\\sigma_i$ decreases, and $v_i$ tends to become more oscillatory.\n\nThe inverse problem is to solve for $q$ from $y = Aq + \\varepsilon$. The formal solution is $q = A^{-1}(y-\\varepsilon)$. The presence of small singular values makes $A$ nearly singular, and its inversion is unstable. The noise $\\varepsilon$ is amplified by factors of $1/\\sigma_i$. The solution is dominated by spurious, high-frequency oscillations corresponding to the vectors $v_i$ with small $\\sigma_i$. This is the essence of the ill-posedness.\n\nTruncated SVD (TSVD) is a regularization method that addresses this. The TSVD solution at a truncation index $k$ is formed by including only the first $k$ terms of the SVD expansion of the inverse:\n$$ \\widehat{q}_{k} = \\sum_{i=1}^{k} \\frac{1}{\\sigma_i} (u_i^{\\top} y) v_i = \\sum_{i=1}^{k} \\frac{u_i^{\\top} y}{\\sigma_i} v_i $$\nBy truncating the sum, we explicitly discard contributions from singular triplets $(u_i, \\sigma_i, v_i)$ for $i > k$. These are precisely the terms associated with small singular values and highly oscillatory vectors $v_i$, which are most corrupted by noise amplification. Therefore, TSVD acts as a low-pass filter, retaining the \"smoother\" components of the solution and suppressing the high-frequency components.\n\n### Evaluation of Options\n\n**A. In the SVD $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r} > 0$ and right singular vectors $v_{i}$, the TSVD solution at index $k$ is $\\widehat{q}_{k} = \\sum_{i=1}^{k} (u_{i}^{\\top} y/\\sigma_{i}) \\, v_{i}$. Because the heat equation attenuates high temporal frequencies, the right singular vectors $v_{i}$ tend to become more oscillatory as $i$ increases, so truncation at $k$ filters out higher-frequency components of $q$.**\n- The formula for the TSVD solution $\\widehat{q}_k$ is correct.\n- The reasoning that the heat equation's damping of high frequencies causes the right singular vectors $v_i$ associated with small singular values (large $i$) to be oscillatory is correct.\n- The conclusion that truncating the summation at $k$ discards these high-frequency components is correct. This is the mechanism by which TSVD regularizes the solution.\nVerdict: **Correct**.\n\n**B. TSVD keeps the $k$ smallest singular values to avoid over-smoothing and thereby emphasizes high-frequency content, which is desirable because measurement noise in IHCPs is predominantly low frequency.**\n- \"TSVD keeps the $k$ smallest singular values\": This is false. TSVD keeps the $k$ *largest* singular values, as these carry the most significant information about the solution.\n- \"emphasizes high-frequency content\": This is false. TSVD is a regularization method that *suppresses* high-frequency content to stabilize the solution.\n- \"measurement noise...is predominantly low frequency\": This is generally not assumed. Sensor noise is often modeled as white noise, which is broadband. Even if it were true, the rest of the statement is fundamentally incorrect.\nVerdict: **Incorrect**.\n\n**C. For diffusive forward operators arising from the heat equation, small singular values are associated with increasingly oscillatory right singular vectors. Truncating the SVD expansion at $k$ imposes an effective low-pass filter on $q$, with a cutoff determined by $k$.**\n- \"small singular values are associated with increasingly oscillatory right singular vectors\": This is a correct and fundamental property of SVD for diffusive (smoothing) operators, as explained in the derivation.\n- \"Truncating the SVD expansion at $k$ imposes an effective low-pass filter on $q$\": This is a correct interpretation of the TSVD mechanism.\n- \"with a cutoff determined by $k$\": This is correct. The choice of the truncation parameter $k$ determines the boundary between the kept (low-frequency) and discarded (high-frequency) components.\nVerdict: **Correct**.\n\n**D. The TSVD filter factors in the spectral expansion are $f_{i} = \\sigma_{i}/(\\sigma_{i}^{2}+\\lambda)$ for some $\\lambda>0$, so truncating at $k$ is equivalent to choosing an appropriate $\\lambda$ in Tikhonov regularization.**\n- The formula provided for the filter factors, $f_i$, is incorrect for TSVD. The filter factors for TSVD are a step function: $f_i = 1$ for $i \\le k$ and $f_i = 0$ for $i > k$. The factors given in the option, $f_i = \\sigma_i^2 / (\\sigma_i^2+\\lambda)$ (correcting for the likely typo, standard Tikhonov is $\\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda}$), belong to Tikhonov regularization. The statement confuses the two methods.\n- \"truncating at $k$ is equivalent to choosing an appropriate $\\lambda$\": This is also false. TSVD applies a sharp, \"brick-wall\" filter in the singular value spectrum. Tikhonov regularization applies a smooth, tapered filter. While they serve a similar purpose and one can find parameters $(\\lambda, k)$ that yield similar results, the filtering mechanisms are fundamentally different and not equivalent.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "2497780"}, {"introduction": "While linear models provide essential insights, most real-world materials exhibit temperature-dependent properties, rendering the inverse problem nonlinear. This practice challenges you to step from direct inversion methods to the iterative optimization frameworks required for these more complex scenarios. You will derive the classic Gauss-Newton and Levenberg-Marquardt algorithms, which tackle nonlinearity by solving a sequence of linear approximations, and learn how to ensure these powerful methods converge robustly [@problem_id:2497725].", "problem": "Consider a one-dimensional slab of thickness $L$ with spatial coordinate $x \\in [0,L]$. The thermal conductivity is temperature dependent, $k(T) = k_0 \\left(1 + \\beta \\left(T - T_{\\mathrm{ref}}\\right)\\right)$, with constant density $\\rho$ and heat capacity $c_p$. The forward heat transfer model is governed by the transient energy balance\n$$\n\\rho c_p \\frac{\\partial T}{\\partial t} = \\frac{\\partial}{\\partial x}\\!\\left(k(T)\\,\\frac{\\partial T}{\\partial x}\\right),\n$$\nsubject to a prescribed initial temperature field and boundary conditions. At the boundary $x=0$, the heat flux $q(t)$ is unknown and is parameterized by a vector of coefficients $\\mathbf{m} \\in \\mathbb{R}^M$ through a fixed basis in time, so that $q(t;\\mathbf{m}) = \\sum_{j=1}^M m_j \\,\\varphi_j(t)$. Temperature measurements are available at $x=x_m$ and discrete times $t_1,\\dots,t_N$, producing data $\\mathbf{d} \\in \\mathbb{R}^N$. Define the forward response map $\\mathbf{y}(\\mathbf{m}) \\in \\mathbb{R}^N$ as the vector of model-predicted temperatures at $(x_m,t_i)$, $i=1,\\dots,N$.\n\nBecause $k(T)$ depends on $T$, the forward map $\\mathbf{y}(\\mathbf{m})$ is nonlinear. The Inverse Heat Conduction Problem (IHCP) is posed as a Tikhonov-regularized, weighted nonlinear least-squares optimization:\n$$\n\\min_{\\mathbf{m} \\in \\mathbb{R}^M} \\; \\Phi(\\mathbf{m}) \\equiv \\frac{1}{2}\\left\\| \\mathbf{W}\\left(\\mathbf{y}(\\mathbf{m}) - \\mathbf{d}\\right) \\right\\|_2^2 + \\frac{\\gamma}{2}\\left\\|\\mathbf{L}\\left(\\mathbf{m} - \\mathbf{m}_0\\right)\\right\\|_2^2,\n$$\nwhere $\\mathbf{W} \\in \\mathbb{R}^{N \\times N}$ is a symmetric positive-definite weighting, $\\gamma>0$ is a regularization parameter, and $\\mathbf{L} \\in \\mathbb{R}^{P \\times M}$ is a regularization operator with prior $\\mathbf{m}_0$.\n\nTasks:\n- Starting from the definition of $\\Phi(\\mathbf{m})$, a first-order Taylor expansion of the model residual, and the principle of least squares, derive the Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$ as the solution of a linear system in terms of the Jacobian matrix and residual vector.\n- By introducing a damping parameter $\\lambda \\ge 0$ and a diagonal scaling $\\mathbf{D} \\succ \\mathbf{0}$, derive the Levenberg–Marquardt (LM) step $\\mathbf{s}_{\\mathrm{LM}}$ as a modified normal equation. State clearly how $\\mathbf{D}$ enters the system.\n- Briefly discuss two globalization strategies to handle nonconvexity induced by $k(T)$: a backtracking line-search using a sufficient-decrease condition and a trust-region strategy based on the ratio of actual to predicted reduction. State a sufficient-decrease condition and the expression for the predicted reduction under a Gauss–Newton quadratic model.\n- Numerical micro-instance: at a current iterate $\\mathbf{m}^{(k)}$ assume the whitened residual and Jacobian (i.e., post-multiplication by $\\mathbf{W}$) are\n$$\n\\mathbf{r} \\equiv \\mathbf{W}\\left(\\mathbf{y}(\\mathbf{m}^{(k)}) - \\mathbf{d}\\right) = \\begin{bmatrix} 0.4 \\\\ -0.2 \\end{bmatrix}, \\qquad\n\\mathbf{J} \\equiv \\mathbf{W}\\,\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{m}}(\\mathbf{m}^{(k)}) = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.2 & 0.8 \\end{bmatrix}.\n$$\nUse a Tikhonov term with $\\gamma = 0.1$, $\\mathbf{L} = \\mathbf{I}$, and assume $\\mathbf{m}^{(k)} = \\mathbf{m}_0$ so that the regularization gradient term vanishes at $\\mathbf{m}^{(k)}$. For the LM scaling take $\\mathbf{D} = \\mathrm{diag}\\!\\left(\\mathbf{J}^{\\top}\\mathbf{J}\\right)$ and damping $\\lambda = 0.5$. Compute the Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$ and the Levenberg–Marquardt step $\\mathbf{s}_{\\mathrm{LM}}$, and then compute the ratio\n$$\n\\eta \\equiv \\frac{\\|\\mathbf{s}_{\\mathrm{LM}}\\|_2}{\\|\\mathbf{s}_{\\mathrm{GN}}\\|_2}.\n$$\nExpress your final answer for $\\eta$ as a unitless real number rounded to four significant figures.", "solution": "This problem requires the derivation of optimization steps for a nonlinear least-squares problem arising from an Inverse Heat Conduction Problem (IHCP), followed by a numerical calculation. The problem statement has been validated and is deemed scientifically sound, well-posed, and complete. We proceed with a rigorous, step-by-step solution.\n\nFirst, we address the derivation of the Gauss–Newton and Levenberg–Marquardt update steps. The objective function to be minimized is\n$$\n\\Phi(\\mathbf{m}) = \\frac{1}{2}\\left\\| \\mathbf{W}\\left(\\mathbf{y}(\\mathbf{m}) - \\mathbf{d}\\right) \\right\\|_2^2 + \\frac{\\gamma}{2}\\left\\|\\mathbf{L}\\left(\\mathbf{m} - \\mathbf{m}_0\\right)\\right\\|_2^2.\n$$\nLet us define the whitened model residual $\\mathbf{r}(\\mathbf{m}) = \\mathbf{W}(\\mathbf{y}(\\mathbf{m}) - \\mathbf{d})$ and the regularization residual $\\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}) = \\mathbf{L}(\\mathbf{m} - \\mathbf{m}_0)$. The objective function is now $\\Phi(\\mathbf{m}) = \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{m})\\|_2^2 + \\frac{\\gamma}{2}\\|\\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m})\\|_2^2$.\n\nTo derive an iterative update, we consider an iterate $\\mathbf{m}^{(k)}$ and seek a step $\\mathbf{s}$ such that $\\mathbf{m}^{(k+1)} = \\mathbf{m}^{(k)} + \\mathbf{s}$ minimizes $\\Phi$. The Gauss–Newton method is based on a linear approximation of the nonlinear residual $\\mathbf{r}(\\mathbf{m})$ around $\\mathbf{m}^{(k)}$. A first-order Taylor expansion gives\n$$\n\\mathbf{r}(\\mathbf{m}^{(k)} + \\mathbf{s}) \\approx \\mathbf{r}(\\mathbf{m}^{(k)}) + \\mathbf{J}^{(k)}\\mathbf{s},\n$$\nwhere $\\mathbf{J}^{(k)} \\equiv \\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{m}}(\\mathbf{m}^{(k)}) = \\mathbf{W} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{m}}(\\mathbf{m}^{(k)})$ is the whitened Jacobian matrix at $\\mathbf{m}^{(k)}$. The regularization residual is already linear in $\\mathbf{m}$, so\n$$\n\\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)} + \\mathbf{s}) = \\mathbf{L}(\\mathbf{m}^{(k)} + \\mathbf{s} - \\mathbf{m}_0) = \\mathbf{L}(\\mathbf{m}^{(k)} - \\mathbf{m}_0) + \\mathbf{L}\\mathbf{s} = \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) + \\mathbf{L}\\mathbf{s}.\n$$\nSubstituting these approximations into the objective function yields a quadratic model for $\\Phi$ in terms of the step $\\mathbf{s}$:\n$$\n\\widehat{\\Phi}(\\mathbf{s}) = \\frac{1}{2}\\left\\| \\mathbf{r}(\\mathbf{m}^{(k)}) + \\mathbf{J}^{(k)}\\mathbf{s} \\right\\|_2^2 + \\frac{\\gamma}{2}\\left\\| \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) + \\mathbf{L}\\mathbf{s} \\right\\|_2^2.\n$$\nTo find the step $\\mathbf{s}_{\\mathrm{GN}}$ that minimizes this quadratic model, we set its gradient with respect to $\\mathbf{s}$ to zero: $\\nabla_{\\mathbf{s}}\\widehat{\\Phi}(\\mathbf{s}) = \\mathbf{0}$.\n$$\n\\nabla_{\\mathbf{s}}\\widehat{\\Phi}(\\mathbf{s}) = (\\mathbf{J}^{(k)})^\\top \\left( \\mathbf{r}(\\mathbf{m}^{(k)}) + \\mathbf{J}^{(k)}\\mathbf{s} \\right) + \\gamma \\mathbf{L}^\\top \\left( \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) + \\mathbf{L}\\mathbf{s} \\right) = \\mathbf{0}.\n$$\nRearranging the terms to solve for $\\mathbf{s}$, we obtain the Gauss–Newton normal equations for the step $\\mathbf{s}_{\\mathrm{GN}}$:\n$$\n\\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)} + \\gamma \\mathbf{L}^\\top \\mathbf{L} \\right) \\mathbf{s}_{\\mathrm{GN}} = - \\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{r}(\\mathbf{m}^{(k)}) + \\gamma \\mathbf{L}^\\top \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) \\right).\n$$\nThe matrix on the left is the regularized Gauss–Newton approximation of the Hessian of $\\Phi$. The vector on the right is the negative of the gradient of $\\Phi$ at $\\mathbf{m}^{(k)}$.\n\nThe Levenberg–Marquardt (LM) method modifies the Gauss–Newton step to improve robustness, especially when the Gauss-Newton Hessian is ill-conditioned or not positive definite. It introduces a damping parameter $\\lambda \\ge 0$ and a diagonal scaling matrix $\\mathbf{D} \\succ \\mathbf{0}$. The LM step $\\mathbf{s}_{\\mathrm{LM}}$ is the solution of the modified linear system:\n$$\n\\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)} + \\gamma \\mathbf{L}^\\top \\mathbf{L} + \\lambda \\mathbf{D} \\right) \\mathbf{s}_{\\mathrm{LM}} = - \\left( (\\mathbf{J}^{(k)})^\\top \\mathbf{r}(\\mathbf{m}^{(k)}) + \\gamma \\mathbf{L}^\\top \\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) \\right).\n$$\nThe term $\\lambda \\mathbf{D}$ acts as a trust-region-like regularization. When $\\lambda$ is large, the step is small and points towards the steepest descent direction. When $\\lambda$ is small, the step approaches the Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$. The scaling matrix $\\mathbf{D}$, often chosen as $\\mathbf{D} = \\mathrm{diag}((\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)})$, ensures that the damping is applied appropriately to parameters of different scales.\n\nDue to the nonlinearity of $k(T)$, the objective function $\\Phi(\\mathbf{m})$ can be nonconvex, and the Gauss–Newton or LM steps may not lead to a decrease in $\\Phi$. Globalization strategies are required. Two common strategies are:\n1.  **Backtracking Line Search**: A search direction $\\mathbf{s}$ (e.g., $\\mathbf{s}_{\\mathrm{GN}}$) is computed. Then, a step length $\\alpha \\in (0, 1]$ is sought to ensure sufficient decrease in the objective function. The new iterate is $\\mathbf{m}^{(k+1)} = \\mathbf{m}^{(k)} + \\alpha\\mathbf{s}$. A sufficient-decrease condition, such as the Armijo condition, must be satisfied:\n    $$\n    \\Phi(\\mathbf{m}^{(k)} + \\alpha\\mathbf{s}) \\le \\Phi(\\mathbf{m}^{(k)}) + c_1 \\alpha \\nabla\\Phi(\\mathbf{m}^{(k)})^\\top \\mathbf{s},\n    $$\n    for a small constant $c_1 \\in (0, 1)$, typically $c_1=10^{-4}$. One starts with $\\alpha=1$ and reduces it until the condition is met.\n2.  **Trust-Region Strategy**: At each iterate $\\mathbf{m}^{(k)}$, a quadratic model $q_k(\\mathbf{s})$ is built (e.g., the Gauss–Newton model $\\widehat{\\Phi}(\\mathbf{s})$) which is trusted to be a good approximation of $\\Phi(\\mathbf{m}^{(k)}+\\mathbf{s})$ only within a radius $\\Delta_k > 0$. The step $\\mathbf{s}_k$ is found by minimizing $q_k(\\mathbf{s})$ subject to $\\|\\mathbf{s}\\| \\le \\Delta_k$. The quality of the step is assessed by the ratio of actual reduction to predicted reduction, $\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}$. The actual reduction is $\\text{ared}_k = \\Phi(\\mathbf{m}^{(k)}) - \\Phi(\\mathbf{m}^{(k)} + \\mathbf{s}_k)$. Under the Gauss–Newton quadratic model, the predicted reduction is the decrease in the model function $q_k$, given by:\n    $$\n    \\text{pred}_k = q_k(\\mathbf{0}) - q_k(\\mathbf{s}_k) = - \\nabla\\Phi(\\mathbf{m}^{(k)})^\\top \\mathbf{s}_k - \\frac{1}{2}\\mathbf{s}_k^\\top \\mathbf{H}_{\\mathrm{GN}} \\mathbf{s}_k,\n    $$\n    where $\\mathbf{H}_{\\mathrm{GN}} = (\\mathbf{J}^{(k)})^\\top \\mathbf{J}^{(k)} + \\gamma \\mathbf{L}^\\top \\mathbf{L}$ is the Gauss-Newton Hessian approximation. Based on the value of $\\rho_k$, the step is accepted or rejected, and the trust-region radius $\\Delta_k$ is updated for the next iteration.\n\nFinally, we perform the numerical calculation for the micro-instance.\nThe given data are:\n$\\mathbf{r} = \\begin{bmatrix} 0.4 \\\\ -0.2 \\end{bmatrix}$, $\\mathbf{J} = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.2 & 0.8 \\end{bmatrix}$, $\\gamma = 0.1$, $\\mathbf{L} = \\mathbf{I}$, $\\lambda = 0.5$.\nThe assumption $\\mathbf{m}^{(k)} = \\mathbf{m}_0$ implies that the regularization residual $\\mathbf{r}_{\\mathrm{reg}}(\\mathbf{m}^{(k)}) = \\mathbf{L}(\\mathbf{m}^{(k)} - \\mathbf{m}_0) = \\mathbf{0}$. This simplifies the right-hand side (RHS) of the linear systems for both steps to $-\\mathbf{J}^\\top \\mathbf{r}$.\n\nFirst, compute the RHS vector:\n$$\n\\mathbf{J}^\\top \\mathbf{r} = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.5 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.4 \\\\ -0.2 \\end{bmatrix} = \\begin{bmatrix} (1.0)(0.4) + (0.2)(-0.2) \\\\ (0.5)(0.4) + (0.8)(-0.2) \\end{bmatrix} = \\begin{bmatrix} 0.4 - 0.04 \\\\ 0.2 - 0.16 \\end{bmatrix} = \\begin{bmatrix} 0.36 \\\\ 0.04 \\end{bmatrix}.\n$$\nSo, the RHS is $-\\begin{bmatrix} 0.36 \\\\ 0.04 \\end{bmatrix}$.\n\nNext, compute the matrix for the Gauss–Newton system, $\\mathbf{H}_{\\mathrm{GN}} = \\mathbf{J}^\\top\\mathbf{J} + \\gamma\\mathbf{L}^\\top\\mathbf{L}$:\n$$\n\\mathbf{J}^\\top\\mathbf{J} = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.5 & 0.8 \\end{bmatrix} \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.2 & 0.8 \\end{bmatrix} = \\begin{bmatrix} 1.0^2 + 0.2^2 & (1.0)(0.5) + (0.2)(0.8) \\\\ (0.5)(1.0) + (0.8)(0.2) & 0.5^2 + 0.8^2 \\end{bmatrix} = \\begin{bmatrix} 1.04 & 0.66 \\\\ 0.66 & 0.89 \\end{bmatrix}.\n$$\n$$\n\\mathbf{H}_{\\mathrm{GN}} = \\begin{bmatrix} 1.04 & 0.66 \\\\ 0.66 & 0.89 \\end{bmatrix} + 0.1 \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1.14 & 0.66 \\\\ 0.66 & 0.99 \\end{bmatrix}.\n$$\nThe Gauss–Newton step $\\mathbf{s}_{\\mathrm{GN}}$ is found by solving $\\mathbf{H}_{\\mathrm{GN}} \\mathbf{s}_{\\mathrm{GN}} = -\\mathbf{J}^\\top \\mathbf{r}$:\n$$\n\\begin{bmatrix} 1.14 & 0.66 \\\\ 0.66 & 0.99 \\end{bmatrix} \\mathbf{s}_{\\mathrm{GN}} = \\begin{bmatrix} -0.36 \\\\ -0.04 \\end{bmatrix}.\n$$\nThe determinant of the matrix is $\\det(\\mathbf{H}_{\\mathrm{GN}}) = (1.14)(0.99) - (0.66)^2 = 1.1286 - 0.4356 = 0.693$.\nThe solution is $\\mathbf{s}_{\\mathrm{GN}} = \\frac{1}{0.693} \\begin{bmatrix} 0.99 & -0.66 \\\\ -0.66 & 1.14 \\end{bmatrix} \\begin{bmatrix} -0.36 \\\\ -0.04 \\end{bmatrix} = \\frac{1}{0.693} \\begin{bmatrix} -0.3564 + 0.0264 \\\\ 0.2376 - 0.0456 \\end{bmatrix} = \\frac{1}{0.693} \\begin{bmatrix} -0.33 \\\\ 0.192 \\end{bmatrix}$.\nThe squared L2-norm is $\\|\\mathbf{s}_{\\mathrm{GN}}\\|_2^2 = \\frac{(-0.33)^2 + (0.192)^2}{(0.693)^2} = \\frac{0.1089 + 0.036864}{0.480249} = \\frac{0.145764}{0.480249} \\approx 0.303527$.\nThus, $\\|\\mathbf{s}_{\\mathrm{GN}}\\|_2 \\approx \\sqrt{0.303527} \\approx 0.550933$.\n\nNow, we compute the Levenberg–Marquardt step $\\mathbf{s}_{\\mathrm{LM}}$. The scaling matrix is $\\mathbf{D} = \\mathrm{diag}(\\mathbf{J}^\\top\\mathbf{J}) = \\mathrm{diag}\\left(\\begin{bmatrix} 1.04 & 0.66 \\\\ 0.66 & 0.89 \\end{bmatrix}\\right) = \\begin{bmatrix} 1.04 & 0 \\\\ 0 & 0.89 \\end{bmatrix}$.\nThe LM matrix is $\\mathbf{H}_{\\mathrm{LM}} = \\mathbf{H}_{\\mathrm{GN}} + \\lambda\\mathbf{D}$:\n$$\n\\mathbf{H}_{\\mathrm{LM}} = \\begin{bmatrix} 1.14 & 0.66 \\\\ 0.66 & 0.99 \\end{bmatrix} + 0.5 \\begin{bmatrix} 1.04 & 0 \\\\ 0 & 0.89 \\end{bmatrix} = \\begin{bmatrix} 1.14+0.52 & 0.66 \\\\ 0.66 & 0.99+0.445 \\end{bmatrix} = \\begin{bmatrix} 1.66 & 0.66 \\\\ 0.66 & 1.435 \\end{bmatrix}.\n$$\nThe LM step $\\mathbf{s}_{\\mathrm{LM}}$ solves $\\mathbf{H}_{\\mathrm{LM}} \\mathbf{s}_{\\mathrm{LM}} = -\\mathbf{J}^\\top \\mathbf{r}$:\n$$\n\\begin{bmatrix} 1.66 & 0.66 \\\\ 0.66 & 1.435 \\end{bmatrix} \\mathbf{s}_{\\mathrm{LM}} = \\begin{bmatrix} -0.36 \\\\ -0.04 \\end{bmatrix}.\n$$\nThe determinant is $\\det(\\mathbf{H}_{\\mathrm{LM}}) = (1.66)(1.435) - (0.66)^2 = 2.3821 - 0.4356 = 1.9465$.\nThe solution is $\\mathbf{s}_{\\mathrm{LM}} = \\frac{1}{1.9465} \\begin{bmatrix} 1.435 & -0.66 \\\\ -0.66 & 1.66 \\end{bmatrix} \\begin{bmatrix} -0.36 \\\\ -0.04 \\end{bmatrix} = \\frac{1}{1.9465} \\begin{bmatrix} -0.5166 + 0.0264 \\\\ 0.2376 - 0.0664 \\end{bmatrix} = \\frac{1}{1.9465} \\begin{bmatrix} -0.4902 \\\\ 0.1712 \\end{bmatrix}$.\nThe squared L2-norm is $\\|\\mathbf{s}_{\\mathrm{LM}}\\|_2^2 = \\frac{(-0.4902)^2 + (0.1712)^2}{(1.9465)^2} = \\frac{0.24029604 + 0.02930944}{3.78886225} = \\frac{0.26960548}{3.78886225} \\approx 0.0711576$.\nThus, $\\|\\mathbf{s}_{\\mathrm{LM}}\\|_2 \\approx \\sqrt{0.0711576} \\approx 0.266754$.\n\nFinally, we compute the ratio $\\eta$:\n$$\n\\eta = \\frac{\\|\\mathbf{s}_{\\mathrm{LM}}\\|_2}{\\|\\mathbf{s}_{\\mathrm{GN}}\\|_2} \\approx \\frac{0.266754}{0.550933} \\approx 0.484186.\n$$\nRounding to four significant figures gives $\\eta \\approx 0.4842$.", "answer": "$$\\boxed{0.4842}$$", "id": "2497725"}, {"introduction": "The ultimate goal of many inverse problems is not just to reconstruct a boundary condition, but to identify unknown material properties, a task that becomes challenging when multiple parameters have correlated effects on measured data. This advanced hands-on practice guides you through the process of designing a numerical experiment and using the Fisher Information Matrix to formally assess parameter identifiability. This exercise connects the choice of thermal excitation directly to the statistical confidence in your estimated parameters, a cornerstone of rigorous experimental design [@problem_id:2497727].", "problem": "Consider a one-dimensional, plane-wall, two-layer solid of total thickness $L=L_1+L_2$, composed of layer $1$ on the left with thermal conductivity $k_1$, density $\\rho_1$, specific heat $c_{p,1}$ and thickness $L_1$, and layer $2$ on the right with thermal conductivity $k_2$, density $\\rho_2$, specific heat $c_{p,2}$ and thickness $L_2$. The layers are bonded with a finite thermal contact resistance $R_{\\mathrm{int}}$ (units $\\mathrm{m^2\\,K/W}$). The left boundary at $x=0$ is subjected to a prescribed, time-varying surface heat flux $q''(t)$, and the right boundary at $x=L$ is exposed to ambient air at temperature $T_{\\infty}$ with convection coefficient $h$. The initial temperature is uniform and equal to $T_{\\infty}$. Assume unit cross-sectional area.\n\nStart from the one-dimensional transient heat equation in each layer,\n$ \\rho_i c_{p,i}\\,\\frac{\\partial T}{\\partial t} = \\frac{\\partial}{\\partial x}\\!\\left(k_i\\,\\frac{\\partial T}{\\partial x}\\right) \\quad \\text{for} \\quad x\\in\\text{layer } i,\\; i\\in\\{1,2\\}, $\nwith boundary conditions\n$ -k_1 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=0} = q''(t),\\quad -k_2 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=L} = h\\left(T(L,t)-T_{\\infty}\\right), $\nand interfacial conditions at the contact located at $x=L_1$,\n$ -k_1 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=L_1^-} = -k_2 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=L_1^+} = q_{\\mathrm{int}}(t),\\quad T(L_1^+,t) - T(L_1^-,t) = q_{\\mathrm{int}}(t) R_{\\mathrm{int}}. $\n\nYou are to pose and evaluate an inverse heat conduction identification experiment that uses multiple transient thermal excitations $q''(t)$ to decouple the unknown parameter vector $ \\boldsymbol{\\theta}=\\big[k_1,\\,k_2,\\,R_{\\mathrm{int}}\\big]^{\\top} $ from measurements of the back-face temperature $T(L,t)$ in the presence of additive, independent, identically distributed Gaussian measurement noise of known standard deviation $\\sigma_T$. The identifiability of $ \\boldsymbol{\\theta} $ is to be assessed via the Fisher information matrix.\n\nYour program must implement the following steps from first principles, without using any black-box inverse solvers:\n\n- Spatial semi-discretization: Derive a semi-discrete state-space model by dividing each layer $i$ into $N_i$ uniform control volumes of width $\\Delta x_i=L_i/N_i$, placing nodes at control-volume centers, and writing transient energy balances for each node. Between adjacent nodes $j$ and $j+1$ with conductivities $k_j$ and $k_{j+1}$ and widths $\\Delta x_j$ and $\\Delta x_{j+1}$, the inter-nodal thermal resistance must be discretized as\n$ R_{j+\\frac{1}{2}} = \\frac{\\Delta x_j}{2 k_j} + \\frac{\\Delta x_{j+1}}{2 k_{j+1}} + R_{\\mathrm{int},\\,j+\\frac{1}{2}}, $\nwhere $ R_{\\mathrm{int},\\,j+\\frac{1}{2}}=R_{\\mathrm{int}} $ only at the material interface face and $ R_{\\mathrm{int},\\,j+\\frac{1}{2}}=0 $ for all other faces. The corresponding conductance is $ G_{j+\\frac{1}{2}} = 1/R_{j+\\frac{1}{2}} $. Assemble the global capacity matrix $ \\mathbf{C} $ with diagonal entries $ C_j=\\rho_j c_{p,j}\\,\\Delta x_j $ and the symmetric conductance matrix $ \\mathbf{K} $ with tridiagonal structure defined by the $ G_{j+\\frac{1}{2}} $. Apply the left Neumann boundary by adding the heat flux $q''(t)$ as a source at node $j=1$, and the right convective boundary by adding a boundary conductance $h$ connected to $T_{\\infty}$ at the last node.\n\n- Time integration: Starting from the initial condition $ T(x,0)=T_{\\infty} $, time-march the semi-discrete system $ \\mathbf{C}\\,\\dot{\\mathbf{T}}(t) + \\mathbf{K}\\,\\mathbf{T}(t) = \\mathbf{f}(t) $ with the Crank–Nicolson method over each experiment duration to obtain the back-face temperature $ y(t)=T(L,t) $ sampled at specified times. Use the trapezoidal rule for the source term $ \\mathbf{f}(t) $.\n\n- Sensitivities and Fisher information: For a given experimental design consisting of one or more excitation signals $ q''_m(t) $ and corresponding measurement time grids, approximate the sensitivity vectors $ \\partial y/\\partial \\theta_\\ell $ by forward finite differences with a relative perturbation step $ \\epsilon_\\ell $ applied to each component $ \\theta_\\ell $ of $ \\boldsymbol{\\theta} $ independently, recomputing the forward model for each perturbation. Stack all measurements across all excitations into one vector $ \\mathbf{y} $ and stack the corresponding sensitivity vectors into the Jacobian $ \\mathbf{S} $. For independent Gaussian noise with variance $ \\sigma_T^2 $, compute the Fisher information matrix\n$ \\mathbf{F} = \\frac{1}{\\sigma_T^2}\\,\\mathbf{S}^{\\top}\\mathbf{S}. $\n\n- Identifiability decision rule: Define the Cramér–Rao lower bound covariance as $ \\mathbf{C}_{\\mathrm{CRLB}} = \\mathbf{F}^{-1} $ if $ \\mathbf{F} $ is nonsingular, or $ \\mathbf{C}_{\\mathrm{CRLB}} = \\mathbf{F}^{+} $ (the Moore–Penrose pseudoinverse) otherwise. Using the true parameter values $ \\boldsymbol{\\theta}_{\\star} $, declare the triplet $ \\boldsymbol{\\theta} $ identifiable for a given design if both of the following hold:\n    - The condition number $ \\kappa(\\mathbf{F}) $ in the $ \\ell_2 $ norm satisfies $ \\kappa(\\mathbf{F}) \\le 10^{12} $.\n    - The marginal relative standard deviations, defined as $ r_\\ell = \\sqrt{[\\mathbf{C}_{\\mathrm{CRLB}}]_{\\ell\\ell}}\\,/\\,\\max(|\\theta_{\\star,\\ell}|,\\,\\theta_{\\min}) $ with $ \\theta_{\\min}=10^{-12} $, satisfy $ r_\\ell \\le 0.2 $ for all $ \\ell\\in\\{1,2,3\\} $.\n\nUse the following constants for all test cases:\n- Ambient temperature $ T_{\\infty} = 293\\ \\mathrm{K} $.\n- Convection coefficient $ h = 15\\ \\mathrm{W\\,m^{-2}\\,K^{-1}} $.\n- Measurement noise standard deviation $ \\sigma_T = 0.02\\ \\mathrm{K} $.\n- Discretization nodes $ N_1=20 $, $ N_2=30 $.\n- Time step $ \\Delta t = 0.001\\ \\mathrm{s} $.\n- Measurement sampling interval $ \\Delta t_{\\mathrm{samp}} = 0.01\\ \\mathrm{s} $.\n- Finite difference relative steps $ \\epsilon_{k_1}=\\epsilon_{k_2}=10^{-2} $, $ \\epsilon_{R}=10^{-2} $ with an absolute floor $ \\delta_{R}=10^{-7}\\ \\mathrm{m^2\\,K/W} $.\n\nDefine the excitation signals as square pulses:\n$ q''(t) = q_0 $ for $ 0 \\le t < t_{\\mathrm{on}} $, and $ q''(t) = 0 $ for $ t \\ge t_{\\mathrm{on}} $, simulated over a total duration $ t_{\\mathrm{end}} $.\n\nYour program must evaluate the following three experimental designs (test suite), each with material properties expressed in the International System of Units:\n\n- Test case $1$ (single excitation, moderate contact resistance):\n    - Material and geometry: $ L_1=2\\times 10^{-3}\\ \\mathrm{m} $, $ L_2=3\\times 10^{-3}\\ \\mathrm{m} $, $ k_1=200\\ \\mathrm{W\\,m^{-1}\\,K^{-1}} $, $ k_2=20\\ \\mathrm{W\\,m^{-1}\\,K^{-1}} $, $ \\rho_1=8000\\ \\mathrm{kg\\,m^{-3}} $, $ c_{p,1}=500\\ \\mathrm{J\\,kg^{-1}\\,K^{-1}} $, $ \\rho_2=1200\\ \\mathrm{kg\\,m^{-3}} $, $ c_{p,2}=1400\\ \\mathrm{J\\,kg^{-1}\\,K^{-1}} $, $ R_{\\mathrm{int}}=1\\times 10^{-4}\\ \\mathrm{m^2\\,K/W} $.\n    - Excitation set: one pulse with $ q_0=8\\times 10^{4}\\ \\mathrm{W\\,m^{-2}} $, $ t_{\\mathrm{on}}=0.3\\ \\mathrm{s} $, $ t_{\\mathrm{end}}=1.2\\ \\mathrm{s} $.\n\n- Test case $2$ (multiple excitations to decouple parameters):\n    - Same material and geometry as test case $1$.\n    - Excitation set: three pulses simulated as separate experiments and aggregated:\n        - Excitation $A$: $ q_0=1.2\\times 10^{5}\\ \\mathrm{W\\,m^{-2}} $, $ t_{\\mathrm{on}}=0.05\\ \\mathrm{s} $, $ t_{\\mathrm{end}}=0.5\\ \\mathrm{s} $.\n        - Excitation $B$: $ q_0=7\\times 10^{4}\\ \\mathrm{W\\,m^{-2}} $, $ t_{\\mathrm{on}}=0.2\\ \\mathrm{s} $, $ t_{\\mathrm{end}}=1.0\\ \\mathrm{s} $.\n        - Excitation $C$: $ q_0=5\\times 10^{4}\\ \\mathrm{W\\,m^{-2}} $, $ t_{\\mathrm{on}}=0.6\\ \\mathrm{s} $, $ t_{\\mathrm{end}}=2.0\\ \\mathrm{s} $.\n\n- Test case $3$ (nearly perfect contact, challenging identifiability of $ R_{\\mathrm{int}} $):\n    - Same as test case $2$ except $ R_{\\mathrm{int}}=1\\times 10^{-6}\\ \\mathrm{m^2\\,K/W} $.\n\nProgram requirements:\n- Implement the semi-discrete model assembly and Crank–Nicolson time integrator exactly as described, with conductances $ G_{j+\\frac{1}{2}} = 1/R_{j+\\frac{1}{2}} $ using the half-cell resistances and interfacial resistance where appropriate.\n- Approximate sensitivities by forward finite differences using the specified perturbation sizes.\n- Construct the Fisher information matrix and evaluate the identifiability decision rule.\n- For each test case, compute a boolean indicating whether $ \\big[k_1,k_2,R_{\\mathrm{int}}\\big] $ is identifiable under the stated rule.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $3$, where each entry is the boolean identifiability decision for the corresponding test case (for example, $[ \\mathrm{True}, \\mathrm{False}, \\mathrm{True} ]$). No additional text should be printed.", "solution": "We begin from the one-dimensional transient heat conduction equation in each homogeneous layer $i\\in\\{1,2\\}$,\n$ \\rho_i c_{p,i} \\frac{\\partial T}{\\partial t} = \\frac{\\partial}{\\partial x}\\!\\left(k_i \\frac{\\partial T}{\\partial x}\\right), $\nwith boundary conditions at $x=0$ and $x=L$,\n$ -k_1 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=0} = q''(t), \\quad -k_2 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=L} = h \\left(T(L,t)-T_{\\infty}\\right), $\nand interfacial continuity of heat flux with a finite temperature jump proportional to heat flux across the contact at $x=L_1$,\n$ -k_1 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=L_1^-} = -k_2 \\left.\\frac{\\partial T}{\\partial x}\\right|_{x=L_1^+} = q_{\\mathrm{int}}(t), $\n$ T(L_1^+,t)-T(L_1^-,t) = q_{\\mathrm{int}}(t) R_{\\mathrm{int}}. $\nWe assume unit cross-sectional area and initial condition $ T(x,0) = T_{\\infty} $.\n\nTo construct a robust and physically faithful numerical forward model suitable for inverse analysis, we use a finite volume method. Let layer $i$ be partitioned into $N_i$ control volumes of uniform width $ \\Delta x_i=L_i/N_i $, with nodal temperatures $ T_j(t) $ located at the center of each control volume. The energy balance for node $j$ is\n$C_j \\frac{\\mathrm{d} T_j}{\\mathrm{d} t} = \\sum_{\\text{faces}} G_{j+\\frac{1}{2}} \\left( T_{j+1}-T_j \\right) + \\sum_{\\text{faces}} G_{j-\\frac{1}{2}} \\left( T_{j-1}-T_j \\right) + s_j(t), $\nwhere the nodal heat capacity is $ C_j = \\rho_j c_{p,j} \\Delta x_j $. At each interior face between adjacent control volumes $j$ and $j+1$, the thermal resistance includes two half-cell conduction resistances and, if the face coincides with the material interface, the interfacial resistance:\n$ R_{j+\\frac{1}{2}} = \\frac{\\Delta x_j}{2 k_j} + \\frac{\\Delta x_{j+1}}{2 k_{j+1}} + R_{\\mathrm{int},\\,j+\\frac{1}{2}}, $\nwith $ R_{\\mathrm{int},\\,j+\\frac{1}{2}}=R_{\\mathrm{int}} $ at $ x=L_1 $ and $ R_{\\mathrm{int},\\,j+\\frac{1}{2}}=0 $ otherwise. The corresponding conductance is $ G_{j+\\frac{1}{2}} = 1/R_{j+\\frac{1}{2}} $. The left Neumann boundary contributes a source $ s_1(t)=q''(t) $ at node $ j=1 $, and the right convective boundary at the last node $ j=N=N_1+N_2 $ is incorporated as an additional boundary conductance $G_{\\mathrm{conv}}=h$ to a fixed ambient temperature node at $ T_{\\infty} $, which modifies the last-row balance to include $+h ( T_{\\infty} - T_N )$, represented within the linear form by adding $ h $ to the diagonal of the conductance matrix and $ h T_{\\infty} $ to the source vector.\n\nCollecting all nodal temperatures into $ \\mathbf{T}(t)\\in\\mathbb{R}^N $, the semi-discrete system is\n$ \\mathbf{C} \\,\\dot{\\mathbf{T}}(t) + \\mathbf{K}\\,\\mathbf{T}(t) = \\mathbf{f}(t), $\nwith diagonal $ \\mathbf{C}=\\mathrm{diag}(C_1,\\dots,C_N) $, symmetric tridiagonal $ \\mathbf{K} $ assembled from the $ G_{j\\pm \\frac{1}{2}} $ and the convection contribution, and a source vector $ \\mathbf{f}(t) $ with $ f_1(t)=q''(t) $ and $ f_N(t) $ including $ h T_{\\infty} $.\n\nFor time integration we adopt the Crank–Nicolson scheme (the trapezoidal rule), which for a time step from $ t^n $ to $ t^{n+1}=t^n+\\Delta t $ reads\n$ \\left( \\mathbf{C} + \\frac{\\Delta t}{2}\\mathbf{K} \\right)\\mathbf{T}^{n+1} = \\left( \\mathbf{C} - \\frac{\\Delta t}{2}\\mathbf{K} \\right)\\mathbf{T}^{n} + \\frac{\\Delta t}{2}\\left( \\mathbf{f}^{n+1}+\\mathbf{f}^{n} \\right). $\nDefine $ \\mathbf{A}=\\mathbf{C} + \\frac{\\Delta t}{2}\\mathbf{K} $ and $ \\mathbf{B}=\\mathbf{C} - \\frac{\\Delta t}{2}\\mathbf{K} $. With $ \\mathbf{A} $ constant within an experiment, we factorize $ \\mathbf{A} $ once (via, for example, $\\mathrm{LU}$ factorization) and then solve at each time step. The measurement is the back-face temperature $y(t)=T_N(t)$ recorded at prescribed sampling times with noise $ \\eta(t) \\sim \\mathcal{N}(0,\\sigma_T^2) $.\n\nFor a given experimental design with $ M $ excitations $ q''_m(t) $ and corresponding sampling schedules, we simulate each excitation starting from $ \\mathbf{T}(0)=T_{\\infty}\\mathbf{1} $ to obtain a stacked measurement vector $ \\mathbf{y}\\in\\mathbb{R}^{n_y} $. To build the Jacobian $ \\mathbf{S}\\in\\mathbb{R}^{n_y\\times 3} $ with respect to $ \\boldsymbol{\\theta}=\\big[k_1,k_2,R_{\\mathrm{int}}\\big]^{\\top} $, we compute forward finite differences: for each component $ \\theta_\\ell $ we perturb by $ \\Delta \\theta_\\ell = \\epsilon_\\ell \\max(|\\theta_\\ell|,\\delta_\\ell) $, with $ \\epsilon_{k_1}=\\epsilon_{k_2}=10^{-2} $, $ \\epsilon_{R}=10^{-2} $, and $ \\delta_R=10^{-7}\\ \\mathrm{m^2\\,K/W} $; we then rerun all excitations with the perturbed parameter to obtain $ \\mathbf{y}^{(\\ell)} $ and set the $ \\ell $-th column of $ \\mathbf{S} $ to $ \\big(\\mathbf{y}^{(\\ell)}-\\mathbf{y}\\big)/\\Delta \\theta_\\ell $. Assuming independent Gaussian noise with variance $ \\sigma_T^2 $, the Fisher information matrix is\n$ \\mathbf{F} = \\frac{1}{\\sigma_T^2}\\mathbf{S}^{\\top}\\mathbf{S}. $\n\nIdentifiability is judged by two criteria. First, the Fisher matrix should not be ill-conditioned: the $ \\ell_2 $-norm condition number $ \\kappa(\\mathbf{F}) $ should satisfy $ \\kappa(\\mathbf{F}) \\le 10^{12} $. Second, the Cramér–Rao lower bound covariance $ \\mathbf{C}_{\\mathrm{CRLB}} $, taken as $ \\mathbf{F}^{-1} $ when $ \\mathbf{F} $ is invertible and as the Moore–Penrose pseudoinverse $ \\mathbf{F}^{+} $ otherwise, implies marginal standard deviations $ \\sigma_{\\theta_\\ell} = \\sqrt{[\\mathbf{C}_{\\mathrm{CRLB}}]_{\\ell\\ell}} $. We compare these to the true magnitudes via $ r_\\ell = \\sigma_{\\theta_\\ell} / \\max(|\\theta_{\\star,\\ell}|,\\theta_{\\min}) $ with $ \\theta_{\\min}=10^{-12} $ and require $ r_\\ell \\le 0.2 $ for all $ \\ell\\in\\{1,2,3\\} $. Both conditions must hold to return a boolean $ \\mathrm{True} $ for identifiability.\n\nAlgorithmic design for the program:\n- Assemble the spatial discretization once per parameter set using the specified $ N_1 $, $ N_2 $, $ L_1 $, $ L_2 $, $ k_1 $, $ k_2 $, $ R_{\\mathrm{int}} $, $ \\rho_i $, $ c_{p,i} $, $ h $, and $ T_{\\infty} $.\n- For each excitation signal $q''(t)$ defined by $ q_0 $, $ t_{\\mathrm{on}} $, and $ t_{\\mathrm{end}} $, integrate from $ t=0 $ to $ t=t_{\\mathrm{end}} $ with $ \\Delta t $, sampling $ y(t) $ at increments of $ \\Delta t_{\\mathrm{samp}} $. Repeat the forward simulation for each perturbed parameter to fill the sensitivity columns.\n- Stack all simulated measurements across the experiments and compute $ \\mathbf{S} $, $ \\mathbf{F} $, $ \\kappa(\\mathbf{F}) $, and $ \\mathbf{C}_{\\mathrm{CRLB}} $.\n- Decide identifiability using the two-condition rule and return a boolean for each test case.\n\nThe three specified test cases probe complementary facets of identifiability. Test case $1$ uses a single, moderate-duration pulse, which often entangles the effects of $ k_1 $, $ k_2 $, and $ R_{\\mathrm{int}} $, potentially resulting in a nearly singular $ \\mathbf{F} $. Test case $2 $ introduces multiple excitations with distinct time scales and magnitudes, enriching the sensitivity directions and improving the conditioning of $ \\mathbf{F} $. Test case $3 $ reduces $ R_{\\mathrm{int}} $ by two orders of magnitude, which diminishes the measurable influence of the interface and challenges the identifiability of $ R_{\\mathrm{int}} $ even under multiple excitations. The program implements this logic and outputs a single list $[b_1,b_2,b_3]$ of booleans corresponding to the identifiability outcomes for the three designs.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef assemble_matrices(L1, L2, N1, N2, k1, k2, rho1, cp1, rho2, cp2, R_int, h, T_inf):\n    \"\"\"\n    Assemble capacity matrix C (diagonal), conductance matrix K (tridiagonal),\n    and source vector template for convection contribution at the right boundary.\n    Unit area is assumed.\n    \"\"\"\n    # Grid\n    dx1 = L1 / N1\n    dx2 = L2 / N2\n    N = N1 + N2\n    dx = np.concatenate([np.full(N1, dx1), np.full(N2, dx2)])\n    k_cells = np.concatenate([np.full(N1, k1), np.full(N2, k2)])\n    rho_cells = np.concatenate([np.full(N1, rho1), np.full(N2, rho2)])\n    cp_cells = np.concatenate([np.full(N1, cp1), np.full(N2, cp2)])\n    C = (rho_cells * cp_cells * dx)\n\n    # Build tridiagonal K from face conductances\n    K = np.zeros((N, N), dtype=float)\n\n    # Faces between nodes: j between 0..N-2 for face j+1/2 between node j and j+1\n    for j in range(N - 1):\n        # Half-cell resistances on each side\n        R_face = dx[j] / (2.0 * k_cells[j]) + dx[j + 1] / (2.0 * k_cells[j + 1])\n        # Add interfacial resistance at the interface between layers\n        if j == N1 - 1:\n            R_face += R_int\n        G = 1.0 / R_face\n        K[j, j] += G\n        K[j + 1, j + 1] += G\n        K[j, j + 1] -= G\n        K[j + 1, j] -= G\n\n    # Right convective boundary at node N-1\n    K[-1, -1] += h\n    # Source template is zero except for adding h*T_inf at last node each step\n    f_conv = np.zeros(N, dtype=float)\n    f_conv[-1] = h * T_inf\n\n    return C, K, f_conv\n\ndef crank_nicolson_sim(C, K, f_conv, q_of_t, T_inf, dt, t_end, tsamp):\n    \"\"\"\n    Simulate T evolution under heat flux q_of_t(t) at left boundary using Crank-Nicolson.\n    Returns sampled back-face temperature vector y over sampling times.\n    \"\"\"\n    N = C.size\n    # Matrices\n    A = np.diag(C) + 0.5 * dt * K\n    B = np.diag(C) - 0.5 * dt * K\n    lu, piv = lu_factor(A)\n\n    # Time stepping\n    n_steps = int(np.round(t_end / dt))\n    n_samp = int(np.floor(t_end / tsamp)) + 1\n    y = np.zeros(n_samp, dtype=float)\n    T = np.full(N, T_inf, dtype=float)\n\n    # Sampling times\n    sample_times = np.linspace(0.0, t_end, n_samp)\n    next_sample_index = 0\n    next_sample_time = sample_times[next_sample_index]\n\n    t = 0.0\n    # Record initial measurement\n    y[next_sample_index] = T[-1]\n    next_sample_index += 1\n    next_sample_time = sample_times[next_sample_index] if next_sample_index < n_samp else None\n\n    for n in range(n_steps):\n        t_n = t\n        t_np1 = t + dt\n\n        # Build sources at t_n and t_np1\n        f_n = f_conv.copy()\n        f_np1 = f_conv.copy()\n        # Left Neumann flux as source at node 0\n        f_n[0] += q_of_t(t_n)\n        f_np1[0] += q_of_t(t_np1)\n\n        rhs = B @ T + 0.5 * dt * (f_n + f_np1)\n        T = lu_solve((lu, piv), rhs)\n\n        # Advance time\n        t = t_np1\n\n        # Sample as many times as needed within this step (account for dt being smaller than tsamp)\n        while next_sample_time is not None and t >= next_sample_time - 1e-12:\n            y[next_sample_index] = T[-1]\n            next_sample_index += 1\n            if next_sample_index < n_samp:\n                next_sample_time = sample_times[next_sample_index]\n            else:\n                next_sample_time = None\n\n    return y\n\ndef build_excitation(q0, t_on, t_end):\n    def q_of_t(t):\n        return q0 if (0.0 <= t < t_on) else 0.0\n    return q_of_t, t_end\n\ndef fisher_information_and_identifiability(params, excitations, constants):\n    \"\"\"\n    Compute Fisher information matrix for the given parameter set and excitation design.\n    Return identifiability boolean based on the stated rule.\n    \"\"\"\n    # Unpack parameters and constants\n    L1, L2 = params['L1'], params['L2']\n    k1, k2, Rint = params['k1'], params['k2'], params['Rint']\n    rho1, cp1 = params['rho1'], params['cp1']\n    rho2, cp2 = params['rho2'], params['cp2']\n    N1, N2 = constants['N1'], constants['N2']\n    h, Tinf = constants['h'], constants['Tinf']\n    dt, tsamp, sigma_T = constants['dt'], constants['tsamp'], constants['sigma_T']\n    # Perturbation sizes\n    eps_k = constants['eps_k']\n    eps_R = constants['eps_R']\n    delta_R = constants['delta_R']\n\n    # Assemble base matrices once per parameter set\n    C, K, f_conv = assemble_matrices(L1, L2, N1, N2, k1, k2, rho1, cp1, rho2, cp2, Rint, h, Tinf)\n\n    # Simulate base outputs and build stack of measurements y\n    y_list = []\n    # Also store sample counts to reconstruct stacking for perturbed runs\n    sample_lengths = []\n    for (q0, t_on, t_end) in excitations:\n        qfun, tend = build_excitation(q0, t_on, t_end)\n        y = crank_nicolson_sim(C, K, f_conv, qfun, Tinf, dt, tend, tsamp)\n        y_list.append(y)\n        sample_lengths.append(len(y))\n    y_stack = np.concatenate(y_list, axis=0)\n    ny = y_stack.size\n\n    # Sensitivity matrix S (ny x 3)\n    S = np.zeros((ny, 3), dtype=float)\n\n    # Parameter perturbations and simulations\n    # 1) Perturb k1\n    dk1 = eps_k * max(abs(k1), 1e-20)\n    Ck1, Kk1, fcv = assemble_matrices(L1, L2, N1, N2, k1 + dk1, k2, rho1, cp1, rho2, cp2, Rint, h, Tinf)\n    y_cols = []\n    for (q0, t_on, t_end), slen in zip(excitations, sample_lengths):\n        qfun, tend = build_excitation(q0, t_on, t_end)\n        y_pert = crank_nicolson_sim(Ck1, Kk1, fcv, qfun, Tinf, dt, tend, tsamp)\n        y_cols.append(y_pert)\n    yk1 = np.concatenate(y_cols, axis=0)\n    S[:, 0] = (yk1 - y_stack) / dk1\n\n    # 2) Perturb k2\n    dk2 = eps_k * max(abs(k2), 1e-20)\n    Ck2, Kk2, fcv = assemble_matrices(L1, L2, N1, N2, k1, k2 + dk2, rho1, cp1, rho2, cp2, Rint, h, Tinf)\n    y_cols = []\n    for (q0, t_on, t_end), slen in zip(excitations, sample_lengths):\n        qfun, tend = build_excitation(q0, t_on, t_end)\n        y_pert = crank_nicolson_sim(Ck2, Kk2, fcv, qfun, Tinf, dt, tend, tsamp)\n        y_cols.append(y_pert)\n    yk2 = np.concatenate(y_cols, axis=0)\n    S[:, 1] = (yk2 - y_stack) / dk2\n\n    # 3) Perturb R_int\n    dR = max(eps_R * max(abs(Rint), delta_R), delta_R)\n    CR, KR, fcv = assemble_matrices(L1, L2, N1, N2, k1, k2, rho1, cp1, rho2, cp2, Rint + dR, h, Tinf)\n    y_cols = []\n    for (q0, t_on, t_end), slen in zip(excitations, sample_lengths):\n        qfun, tend = build_excitation(q0, t_on, t_end)\n        y_pert = crank_nicolson_sim(CR, KR, fcv, qfun, Tinf, dt, tend, tsamp)\n        y_cols.append(y_pert)\n    yR = np.concatenate(y_cols, axis=0)\n    S[:, 2] = (yR - y_stack) / dR\n\n    # Fisher information matrix\n    F = (S.T @ S) / (sigma_T ** 2)\n\n    # Condition number and CRLB\n    # Use SVD for condition number for numerical robustness\n    try:\n        svals = np.linalg.svd(F, compute_uv=False)\n    except np.linalg.LinAlgError:\n        # If SVD fails, mark as not identifiable\n        return False\n    smax = np.max(svals)\n    smin = np.min(svals)\n    # Avoid division by zero\n    if smin <= 0.0:\n        kappa = np.inf\n    else:\n        kappa = smax / smin\n\n    # CRLB via inverse if possible, else pseudoinverse\n    if np.isfinite(kappa) and kappa < 1e15:\n        try:\n            Finv = np.linalg.inv(F)\n        except np.linalg.LinAlgError:\n            Finv = np.linalg.pinv(F, rcond=1e-12)\n    else:\n        Finv = np.linalg.pinv(F, rcond=1e-12)\n\n    stds = np.sqrt(np.clip(np.diag(Finv), a_min=0.0, a_max=np.inf))\n\n    # Relative standard deviations\n    theta = np.array([k1, k2, Rint], dtype=float)\n    denom = np.maximum(np.abs(theta), 1e-12)\n    rel_stds = stds / denom\n\n    # Identifiability rule\n    identifiable = (kappa <= 1e12) and np.all(rel_stds <= 0.2)\n    return bool(identifiable)\n\ndef solve():\n    # Constants shared across cases\n    constants = {\n        'Tinf': 293.0,        # K\n        'h': 15.0,            # W/m^2-K\n        'N1': 20,\n        'N2': 30,\n        'dt': 0.001,          # s\n        'tsamp': 0.01,        # s\n        'sigma_T': 0.02,      # K\n        'eps_k': 1e-2,\n        'eps_R': 1e-2,\n        'delta_R': 1e-7\n    }\n\n    # Test case 1: single excitation\n    params1 = {\n        'L1': 2e-3, 'L2': 3e-3,\n        'k1': 200.0, 'k2': 20.0,\n        'rho1': 8000.0, 'cp1': 500.0,\n        'rho2': 1200.0, 'cp2': 1400.0,\n        'Rint': 1e-4\n    }\n    excitations1 = [\n        (8e4, 0.3, 1.2)  # q0, t_on, t_end\n    ]\n\n    # Test case 2: multiple excitations\n    params2 = params1.copy()\n    excitations2 = [\n        (1.2e5, 0.05, 0.5),\n        (7e4, 0.2, 1.0),\n        (5e4, 0.6, 2.0)\n    ]\n\n    # Test case 3: same as 2 but very small Rint\n    params3 = params2.copy()\n    params3['Rint'] = 1e-6\n    excitations3 = excitations2\n\n    test_cases = [\n        (params1, excitations1),\n        (params2, excitations2),\n        (params3, excitations3),\n    ]\n\n    results = []\n    for params, excitations in test_cases:\n        ident = fisher_information_and_identifiability(params, excitations, constants)\n        results.append(ident)\n\n    # Final print statement in the exact required format.\n    # Booleans will be printed as True/False with capital T/F per Python string conversion.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2497727"}]}