{"hands_on_practices": [{"introduction": "Proper Orthogonal Decomposition (POD) is a powerful technique for extracting the most energetic structures from a collection of data, known as snapshots. This practice will guide you through the core mechanics of POD, specifically how to determine the optimal number of basis functions needed to capture a desired amount of system energy [@problem_id:2593061]. By computing the eigenvalues of the snapshot correlation matrix, you will gain hands-on experience with the direct link between these mathematical quantities and the physical concept of energy, a cornerstone of data-driven model reduction.", "problem": "Consider a family of linear, coercive, parameterized partial differential equations discretized by the Finite Element Method (FEM). Suppose the solution manifold is sampled at three parameter values $\\mu_{1}$, $\\mu_{2}$, and $\\mu_{3}$, yielding three snapshot vectors in $\\mathbb{R}^{3}$ collected as columns of the snapshot matrix $S \\in \\mathbb{R}^{3 \\times 3}$. Let the finite element mass matrix be the symmetric positive definite matrix $M \\in \\mathbb{R}^{3 \\times 3}$. You will compute a Proper Orthogonal Decomposition (POD) with respect to the $M$-inner product for these snapshots.\n\nUse the following data:\n- The mass matrix is \n$$\nM \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\n- The snapshot matrix is \n$$\nS \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n\nAdopt the $M$-inner product $\\langle u, v \\rangle_{M} \\equiv u^{\\top} M v$ and the corresponding $M$-norm $\\|u\\|_{M} \\equiv \\sqrt{u^{\\top} M u}$. Define the snapshot correlation matrix by the fundamental POD definition\n$$\nC \\;=\\; S^{\\top} M S \\;\\in\\; \\mathbb{R}^{3 \\times 3}.\n$$\nLet the eigenvalues of $C$ be denoted and ordered nonincreasingly as $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3} \\ge 0$. The POD energy captured by the first $r$ modes is defined as\n$$\n\\mathcal{E}(r) \\;=\\; \\frac{\\sum_{i=1}^{r} \\lambda_{i}}{\\sum_{i=1}^{3} \\lambda_{i}}.\n$$\n\nYour tasks are:\n1. Given the target energy threshold $\\eta = 0.85$, determine the minimal integer $r$ such that $\\mathcal{E}(r) \\ge \\eta$.\n2. Using the fundamental POD error identity for snapshots, estimate the total snapshot reconstruction truncation error in the $M$-norm when using $r$ modes, defined as\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=r+1}^{3} \\lambda_{i}.\n$$\n\nReport your final answer as a two-entry row matrix $\\begin{pmatrix} r & E_{\\text{trunc}} \\end{pmatrix}$. No rounding is required, and exact values are expected. No physical units are needed.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the established theory of reduced-order modeling, specifically Proper Orthogonal Decomposition (POD). It is well-posed, with all necessary data and definitions provided to ensure a unique, stable, and meaningful solution. The problem's structure is objective and logically consistent. We proceed with the solution.\n\nThe problem requires the computation of two quantities related to the Proper Orthogonal Decomposition of a given set of snapshots. The first is the minimal number of modes, $r$, required to capture a specified fraction of the total energy. The second is the corresponding truncation error. The computations are to be performed with respect to the inner product induced by the mass matrix $M$.\n\nThe core of the POD method, as described, is the eigenvalue problem for the snapshot correlation matrix $C$. We begin by computing this matrix. The definition provided is\n$$\nC \\;=\\; S^{\\top} M S\n$$\nThe snapshot matrix $S$ is given as the $3 \\times 3$ identity matrix, $S = I = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$. The transpose of the identity matrix is the identity matrix itself, $S^{\\top} = I^{\\top} = I$. Substituting these into the definition of $C$ yields:\n$$\nC \\;=\\; I^{\\top} M I \\;=\\; I M I \\;=\\; M\n$$\nTherefore, the correlation matrix $C$ is identical to the mass matrix $M$:\n$$\nC \\;=\\; M \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & \\tfrac{1}{2}\n\\end{pmatrix}\n$$\nThe next step is to determine the eigenvalues of $C$. Since $C$ is a diagonal matrix, its eigenvalues are precisely its diagonal entries. The problem requires that these eigenvalues be ordered nonincreasingly: $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3}$. From the diagonal of $C$, we identify the eigenvalues as $2$, $1$, and $\\frac{1}{2}$. Ordering them gives:\n$$\n\\lambda_{1} = 2\n$$\n$$\n\\lambda_{2} = 1\n$$\n$$\n\\lambda_{3} = \\frac{1}{2}\n$$\nThese eigenvalues represent the squared singular values of the snapshot matrix scaled by the $M$-inner product, and their sum represents the total energy, or variance, contained in the snapshots. The total energy is the trace of the correlation matrix, $\\mathrm{Tr}(C)$:\n$$\n\\sum_{i=1}^{3} \\lambda_{i} = \\lambda_{1} + \\lambda_{2} + \\lambda_{3} = 2 + 1 + \\frac{1}{2} = 3.5 = \\frac{7}{2}\n$$\nNow, we address the first task: to find the minimal integer $r$ such that the captured energy $\\mathcal{E}(r)$ meets or exceeds the threshold $\\eta = 0.85$. The captured energy is defined as:\n$$\n\\mathcal{E}(r) \\;=\\; \\frac{\\sum_{i=1}^{r} \\lambda_{i}}{\\sum_{i=1}^{3} \\lambda_{i}}\n$$\nWe evaluate $\\mathcal{E}(r)$ for $r = 1, 2, \\dots$ until the condition $\\mathcal{E}(r) \\ge 0.85$ is met. The threshold is $\\eta = 0.85 = \\frac{85}{100} = \\frac{17}{20}$.\n\nFor $r=1$:\n$$\n\\mathcal{E}(1) = \\frac{\\lambda_{1}}{\\sum_{i=1}^{3} \\lambda_{i}} = \\frac{2}{\\frac{7}{2}} = \\frac{4}{7}\n$$\nWe compare this to the threshold. Is $\\frac{4}{7} \\ge \\frac{17}{20}$? By cross-multiplication, we compare $4 \\times 20 = 80$ and $7 \\times 17 = 119$. Since $80 < 119$, we have $\\frac{4}{7} < \\frac{17}{20}$. Thus, $\\mathcal{E}(1) < 0.85$. One mode is insufficient.\n\nFor $r=2$:\n$$\n\\mathcal{E}(2) = \\frac{\\lambda_{1} + \\lambda_{2}}{\\sum_{i=1}^{3} \\lambda_{i}} = \\frac{2 + 1}{\\frac{7}{2}} = \\frac{3}{\\frac{7}{2}} = \\frac{6}{7}\n$$\nWe compare this to the threshold. Is $\\frac{6}{7} \\ge \\frac{17}{20}$? By cross-multiplication, we compare $6 \\times 20 = 120$ and $7 \\times 17 = 119$. Since $120 > 119$, we have $\\frac{6}{7} > \\frac{17}{20}$. Thus, $\\mathcal{E}(2) > 0.85$. Two modes are sufficient.\n\nSince $r=1$ is insufficient and $r=2$ is sufficient, the minimal integer is $r=2$.\n\nThe second task is to compute the total snapshot reconstruction truncation error, $E_{\\text{trunc}}$, when using $r=2$ modes. The definition provided is:\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=r+1}^{3} \\lambda_{i}\n$$\nFor $r=2$, this sum becomes:\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=2+1}^{3} \\lambda_{i} \\;=\\; \\sum_{i=3}^{3} \\lambda_{i} \\;=\\; \\lambda_{3}\n$$\nSubstituting the value for $\\lambda_{3}$:\n$$\nE_{\\text{trunc}} = \\frac{1}{2}\n$$\nThis value represents the energy content of the modes that are truncated, which is exactly the portion of total energy not captured by the first $r$ modes.\n\nThe two required values are $r=2$ and $E_{\\text{trunc}} = \\frac{1}{2}$. The final answer must be reported as a two-entry row matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "2593061"}, {"introduction": "While POD builds a basis from pre-existing data, greedy algorithms construct a basis adaptively by querying the full model where it is most needed. This exercise simulates one iteration of a greedy search, where you will use a residual-based error indicator to find the parameter that most challenges the current reduced basis [@problem_id:2593056]. This practice is key to understanding how we can automate the construction of a compact and accurate basis without needing to know the solution manifold in advance.", "problem": "Consider a parametrized, coercive linear elliptic problem discretized by the Finite Element (FE) method, yielding for each parameter value $\\,\\mu \\in \\mathbb{R}_{+}\\,$ the linear system $\\,A(\\mu) u(\\mu) = f\\,$, with an affine decomposition $\\,A(\\mu) = \\Theta_{1}(\\mu) A_{1} + \\Theta_{2}(\\mu) A_{2}\\,$. Suppose that $\\,A_{1} \\in \\mathbb{R}^{3 \\times 3}\\,$ and $\\,A_{2} \\in \\mathbb{R}^{3 \\times 3}\\,$ are symmetric positive-definite, the parameter functions are $\\,\\Theta_{1}(\\mu) = \\mu\\,$ and $\\,\\Theta_{2}(\\mu) = 1\\,$, and the right-hand side is $\\,f \\in \\mathbb{R}^{3}\\,$. The Reduced Basis (RB) greedy procedure starts from an initial seed parameter $\\,\\mu_{1}\\,$ and a training set $\\,\\Xi_{\\mathrm{train}}\\,$. In one greedy iteration, for each $\\,\\mu \\in \\Xi_{\\mathrm{train}}\\,$, one computes the reduced Galerkin solution $\\,u_{N}(\\mu)\\,$ in the current RB space and the residual-based error indicator\n$$\n\\eta(\\mu) \\;=\\; \\frac{\\|r(\\mu)\\|_{V^{*}}}{\\alpha_{\\mathrm{LB}}(\\mu)} \\,,\n$$\nwhere $\\,r(\\mu) = f - A(\\mu) u_{N}(\\mu)\\,$, $\\,\\|\\cdot\\|_{V^{*}}\\,$ is the dual norm induced by the FE trial space norm, and $\\,\\alpha_{\\mathrm{LB}}(\\mu)\\,$ is a reliable lower bound for the coercivity constant of $\\,A(\\mu)\\,$. The greedy update selects the parameter $\\,\\mu^{\\star}\\,$ that maximizes $\\,\\eta(\\mu)\\,$ over $\\,\\Xi_{\\mathrm{train}}\\,$.\n\nYou are given the following concrete data:\n- $\\,A_{1} = \\mathrm{diag}(2,3,4)\\,$, $\\,A_{2} = I_{3}\\,$, and $\\,f = [1,\\,1,\\,1]^{T}\\,$.\n- The FE inner product on the trial space is the standard Euclidean inner product on $\\,\\mathbb{R}^{3}\\,$, so that the dual norm $\\,\\|r(\\mu)\\|_{V^{*}}\\,$ equals the Euclidean norm $\\,\\|r(\\mu)\\|_{2}\\,$.\n- The initial seed is $\\,\\mu_{1} = 1\\,$ and the RB space is one-dimensional, spanned by the truth snapshot $\\,\\phi = u_{h}(\\mu_{1})\\,$, where $\\,u_{h}(\\mu_{1})\\,$ is the exact FE solution of $\\,A(\\mu_{1}) u = f\\,$. No normalization of $\\,\\phi\\,$ is performed.\n- The training set is $\\,\\Xi_{\\mathrm{train}} = \\{\\,\\tfrac{1}{2},\\,\\tfrac{3}{2},\\,2\\,\\}\\,$.\n- Use the reliable coercivity lower bound $\\,\\alpha_{\\mathrm{LB}}(\\mu) = 2\\mu + 1\\,$.\n\nUsing only the above information and starting from first principles (Galerkin projection in the current RB space, the definition of the residual, the Euclidean dual norm, and the given coercivity lower bound), carry out one greedy iteration as follows:\n1. For each $\\,\\mu \\in \\Xi_{\\mathrm{train}}\\,$, compute the reduced Galerkin approximation $\\,u_{N}(\\mu)\\,$ in the span of $\\,\\{\\phi\\}\\,$.\n2. Compute the residual $\\,r(\\mu) = f - A(\\mu) u_{N}(\\mu)\\,$ and its Euclidean norm.\n3. Compute $\\,\\eta(\\mu) = \\|r(\\mu)\\|_{2} / \\alpha_{\\mathrm{LB}}(\\mu)\\,$ for each candidate.\n4. Select the parameter $\\,\\mu^{\\star}\\,$ that maximizes $\\,\\eta(\\mu)\\,$ over $\\,\\Xi_{\\mathrm{train}}\\,$.\n\nReport the chosen parameter value $\\,\\mu^{\\star}\\,$ as a single exact rational number. Do not include units. No rounding is required.", "solution": "The problem statement is critically validated and found to be valid. It is a well-posed, scientifically grounded, and objective problem in the field of reduced-order modeling. All necessary data are provided, and there are no internal contradictions or logical flaws. The task is to perform one iteration of a standard Reduced Basis greedy algorithm. I will proceed with the solution.\n\nThe objective is to find the parameter $\\mu^{\\star}$ that maximizes the error indicator $\\eta(\\mu)$ over the training set $\\Xi_{\\mathrm{train}}$.\n$$\n\\mu^{\\star} = \\arg\\max_{\\mu \\in \\Xi_{\\mathrm{train}}} \\eta(\\mu) = \\arg\\max_{\\mu \\in \\{\\frac{1}{2}, \\frac{3}{2}, 2\\}} \\frac{\\|r(\\mu)\\|_{2}}{\\alpha_{\\mathrm{LB}}(\\mu)}\n$$\nThe problem is set in $\\mathbb{R}^{3}$, where the FE trial space inner product is the standard Euclidean inner product. The dual norm $\\| \\cdot \\|_{V^*}$ is therefore the Euclidean norm $\\| \\cdot \\|_2$.\n\nFirst, we determine the one-dimensional Reduced Basis (RB) space, $V_{N} = \\mathrm{span}\\{\\phi\\}$, where $N=1$. The basis vector $\\phi$ is the \"truth\" Finite Element (FE) solution $u_h(\\mu_1)$ for the seed parameter $\\mu_1 = 1$.\nThe parameter-dependent matrix $A(\\mu)$ is given by $A(\\mu) = \\Theta_{1}(\\mu) A_{1} + \\Theta_{2}(\\mu) A_{2} = \\mu A_{1} + A_{2}$.\nGiven $A_{1} = \\mathrm{diag}(2, 3, 4)$ and $A_{2} = I_{3} = \\mathrm{diag}(1, 1, 1)$, we have:\n$$\nA(\\mu) = \\mu \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2\\mu+1 & 0 & 0 \\\\ 0 & 3\\mu+1 & 0 \\\\ 0 & 0 & 4\\mu+1 \\end{pmatrix}\n$$\nFor $\\mu_1 = 1$, the matrix is $A(1) = \\mathrm{diag}(3, 4, 5)$. The basis vector $\\phi$ is the solution to $A(1) \\phi = f$, where $f = [1, 1, 1]^T$.\n$$\n\\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\implies \\phi = \\begin{pmatrix} 1/3 \\\\ 1/4 \\\\ 1/5 \\end{pmatrix}\n$$\nThe RB space is $V_1 = \\mathrm{span}\\{\\phi\\}$.\n\nNext, for each $\\mu \\in \\Xi_{\\mathrm{train}}$, we find the reduced Galerkin solution $u_N(\\mu) \\in V_1$. This solution can be written as $u_N(\\mu) = c(\\mu) \\phi$ for some scalar coefficient $c(\\mu)$. The Galerkin projection condition states that the residual must be orthogonal to the basis of $V_1$. The inner product is the Euclidean dot product.\n$$\n\\phi^T (A(\\mu) u_N(\\mu) - f) = 0\n$$\nSubstituting $u_N(\\mu) = c(\\mu)\\phi$:\n$$\n\\phi^T (A(\\mu) (c(\\mu)\\phi) - f) = 0 \\implies c(\\mu) (\\phi^T A(\\mu) \\phi) - \\phi^T f = 0\n$$\nThis gives the coefficient $c(\\mu)$:\n$$\nc(\\mu) = \\frac{\\phi^T f}{\\phi^T A(\\mu) \\phi}\n$$\nLet us compute the numerator and denominator.\n$$\n\\phi^T f = \\begin{pmatrix} 1/3 & 1/4 & 1/5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} = \\frac{20+15+12}{60} = \\frac{47}{60}\n$$\nThe denominator is an affine function of $\\mu$:\n$$\n\\phi^T A(\\mu) \\phi = \\phi^T (\\mu A_1 + A_2) \\phi = \\mu (\\phi^T A_1 \\phi) + (\\phi^T A_2 \\phi)\n$$\n$$\n\\phi^T A_1 \\phi = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1/3 \\\\ 1/4 \\\\ 1/5 \\end{pmatrix} = 2\\left(\\frac{1}{3}\\right)^2 + 3\\left(\\frac{1}{4}\\right)^2 + 4\\left(\\frac{1}{5}\\right)^2 = \\frac{2}{9} + \\frac{3}{16} + \\frac{4}{25} = \\frac{800+675+576}{3600} = \\frac{2051}{3600}\n$$\n$$\n\\phi^T A_2 \\phi = \\phi^T I_3 \\phi = \\|\\phi\\|_2^2 = \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 + \\left(\\frac{1}{5}\\right)^2 = \\frac{1}{9} + \\frac{1}{16} + \\frac{1}{25} = \\frac{400+225+144}{3600} = \\frac{769}{3600}\n$$\nSo, $\\phi^T A(\\mu) \\phi = \\mu \\frac{2051}{3600} + \\frac{769}{3600} = \\frac{2051\\mu + 769}{3600}$.\nAnd the coefficient is:\n$$\nc(\\mu) = \\frac{47/60}{(2051\\mu + 769)/3600} = \\frac{47 \\cdot 60}{2051\\mu + 769} = \\frac{2820}{2051\\mu + 769}\n$$\nNow we compute the residual $r(\\mu) = f - A(\\mu) u_N(\\mu) = f - c(\\mu) A(\\mu) \\phi$. A crucial insight is that the residual vector $r(\\mu)$ vanishes at $\\mu = 1$, because $u_N(1) = c(1)\\phi$ and by construction $\\phi = u_h(1)$. Let's check:\n$$\nc(1) = \\frac{47/60}{\\phi^T A(1) \\phi} = \\frac{\\phi^T f}{\\phi^T f} = 1\n$$\nSo $u_N(1) = 1 \\cdot \\phi = u_h(1)$, and $r(1) = f - A(1) u_h(1) = 0$. This implies that each component of the residual vector must have a factor of $(\\mu-1)$.\nWe can write the residual norm as $\\|r(\\mu)\\|_2 = \\| f - c(\\mu)A(\\mu)\\phi \\|_2$. Let's compute this for each $\\mu$.\nThe error indicator is $\\eta(\\mu) = \\frac{\\|r(\\mu)\\|_2}{\\alpha_{\\mathrm{LB}}(\\mu)}$, with $\\alpha_{\\mathrm{LB}}(\\mu) = 2\\mu+1$.\nTo find the maximizer, we can compare the values of $\\eta(\\mu)^2 = \\frac{\\|r(\\mu)\\|_2^2}{(2\\mu+1)^2}$ for $\\mu \\in \\{\\frac{1}{2}, \\frac{3}{2}, 2\\}$. This avoids square roots.\n\nFor $\\mu = \\frac{1}{2}$:\n$c(\\frac{1}{2}) = \\frac{2820}{2051(\\frac{1}{2})+769} = \\frac{2820}{1025.5+769} = \\frac{2820}{1794.5} = \\frac{5640}{3589}$.\n$A(\\frac{1}{2})\\phi = \\mathrm{diag}(2, \\frac{5}{2}, 3) \\begin{pmatrix} 1/3 \\\\ 1/4 \\\\ 1/5 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 5/8 \\\\ 3/5 \\end{pmatrix}$.\n$r(\\frac{1}{2}) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{5640}{3589} \\begin{pmatrix} 2/3 \\\\ 5/8 \\\\ 3/5 \\end{pmatrix} = \\begin{pmatrix} 1 - 3760/3589 \\\\ 1 - 3525/3589 \\\\ 1 - 3384/3589 \\end{pmatrix} = \\frac{1}{3589}\\begin{pmatrix} -171 \\\\ 64 \\\\ 205 \\end{pmatrix}$.\n$\\|r(\\frac{1}{2})\\|_2^2 = \\frac{1}{3589^2} ((-171)^2 + 64^2 + 205^2) = \\frac{29241+4096+42025}{3589^2} = \\frac{75362}{3589^2}$.\n$\\alpha_{\\mathrm{LB}}(\\frac{1}{2}) = 2(\\frac{1}{2})+1=2$.\n$\\eta(\\frac{1}{2})^2 = \\frac{75362}{3589^2 \\cdot 2^2} = \\frac{18840.5}{3589^2}$.\n\nFor $\\mu = \\frac{3}{2}$:\n$c(\\frac{3}{2}) = \\frac{2820}{2051(\\frac{3}{2})+769} = \\frac{2820}{3076.5+769} = \\frac{2820}{3845.5} = \\frac{5640}{7691}$.\n$A(\\frac{3}{2})\\phi = \\mathrm{diag}(4, \\frac{11}{2}, 7) \\begin{pmatrix} 1/3 \\\\ 1/4 \\\\ 1/5 \\end{pmatrix} = \\begin{pmatrix} 4/3 \\\\ 11/8 \\\\ 7/5 \\end{pmatrix}$.\n$r(\\frac{3}{2}) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{5640}{7691} \\begin{pmatrix} 4/3 \\\\ 11/8 \\\\ 7/5 \\end{pmatrix} = \\frac{1}{7691} \\begin{pmatrix} 171 \\\\ -64 \\\\ -205 \\end{pmatrix}$.\n$\\|r(\\frac{3}{2})\\|_2^2 = \\frac{1}{7691^2} (171^2 + (-64)^2 + (-205)^2) = \\frac{75362}{7691^2}$.\n$\\alpha_{\\mathrm{LB}}(\\frac{3}{2}) = 2(\\frac{3}{2})+1=4$.\n$\\eta(\\frac{3}{2})^2 = \\frac{75362}{7691^2 \\cdot 4^2} = \\frac{4710.125}{7691^2}$.\n\nFor $\\mu = 2$:\n$c(2) = \\frac{2820}{2051(2)+769} = \\frac{2820}{4102+769} = \\frac{2820}{4871}$.\n$A(2)\\phi = \\mathrm{diag}(5, 7, 9) \\begin{pmatrix} 1/3 \\\\ 1/4 \\\\ 1/5 \\end{pmatrix} = \\begin{pmatrix} 5/3 \\\\ 7/4 \\\\ 9/5 \\end{pmatrix}$.\n$r(2) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{2820}{4871} \\begin{pmatrix} 5/3 \\\\ 7/4 \\\\ 9/5 \\end{pmatrix} = \\frac{1}{4871} \\begin{pmatrix} 171 \\\\ -64 \\\\ -205 \\end{pmatrix}$.\n$\\|r(2)\\|_2^2 = \\frac{1}{4871^2} (171^2 + (-64)^2 + (-205)^2) = \\frac{75362}{4871^2}$.\n$\\alpha_{\\mathrm{LB}}(2) = 2(2)+1=5$.\n$\\eta(2)^2 = \\frac{75362}{4871^2 \\cdot 5^2} = \\frac{3014.48}{4871^2}$.\n\nNow we compare $\\eta(\\mu)^2$ for the three values of $\\mu$. Let $K = 75362$.\n$\\eta(\\frac{1}{2})^2 = \\frac{K}{(3589 \\cdot 2)^2} = \\frac{K}{7178^2}$.\n$\\eta(\\frac{3}{2})^2 = \\frac{K}{(7691 \\cdot 4)^2} = \\frac{K}{30764^2}$.\n$\\eta(2)^2 = \\frac{K}{(4871 \\cdot 5)^2} = \\frac{K}{24355^2}$.\nTo maximize $\\eta(\\mu)^2$, we must find the smallest denominator.\nWe compare $7178$, $30764$, and $24355$.\nSince $7178$ is the smallest, $\\eta(\\frac{1}{2})^2$ is the largest.\nThe selected parameter is $\\mu^{\\star} = \\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2593056"}, {"introduction": "A reduced-order model is only useful if it is significantly faster to solve than the original high-fidelity model. This practice focuses on the offline-online computational strategy, which is the key to achieving this efficiency by leveraging an affine parameter dependence in the governing equations [@problem_id:2593090]. You will derive the minimal set of pre-computed quantities needed for a rapid online evaluation and analyze the memory footprint, providing a clear understanding of the trade-offs that make reduced-order models practical for many-query applications.", "problem": "Consider a parameter-dependent finite element method (FEM) linear system arising from a coercive elliptic problem, discretized on a fixed mesh with $n_h$ degrees of freedom. For each parameter vector $\\mu$ in a compact parameter set, the assembled high-fidelity algebraic system is\n$$\nA_h(\\mu)\\,u_h(\\mu)=b_h(\\mu), \\quad A_h(\\mu)\\in\\mathbb{R}^{n_h\\times n_h},\\; b_h(\\mu)\\in\\mathbb{R}^{n_h}.\n$$\nAssume there is an exact affine parameter dependence with $Q$ terms for both the operator and the right-hand side,\n$$\nA_h(\\mu)=\\sum_{q=1}^{Q}\\theta_q(\\mu)\\,A_q,\\qquad b_h(\\mu)=\\sum_{q=1}^{Q}\\theta_q(\\mu)\\,b_q,\n$$\nwhere $A_q\\in\\mathbb{R}^{n_h\\times n_h}$ and $b_q\\in\\mathbb{R}^{n_h}$ are parameter-independent coefficient objects and $\\theta_q(\\mu)$ are known scalar functions evaluable at negligible cost compared to linear algebra operations with dimension $n_h$. Let $V_r\\in\\mathbb{R}^{n_h\\times r}$ be a reduced trial basis with $r\\ll n_h$ and $V_r^{T}V_r=I_r$. The reduced Galerkin system for the reduced coordinates $a_r(\\mu)\\in\\mathbb{R}^{r}$ is\n$$\nA_r(\\mu)\\,a_r(\\mu)=b_r(\\mu),\\qquad A_r(\\mu)=V_r^{T}A_h(\\mu)V_r,\\quad b_r(\\mu)=V_r^{T}b_h(\\mu).\n$$\nYou are tasked with many-query evaluations over $\\mu$, with the goal that the online assembly of $A_r(\\mu)$ and $b_r(\\mu)$ is independent of $n_h$. Construct the minimal set of offline, parameter-independent precomputations that enable assembling $A_r(\\mu)$ and $b_r(\\mu)$ online using only operations that scale with $Q$ and $r$. Assume that no symmetry is exploited and that you must be able to recover the high-fidelity approximation $u_h(\\mu)\\approx V_r a_r(\\mu)$ online.\n\nUnder these assumptions, determine a single expression $M(Q,n_h,r)$ giving the total number of floating-point scalars that must be stored after the offline stage to support online assembly of $A_r(\\mu)$ and $b_r(\\mu)$ and recovery of $u_h(\\mu)$, while discarding any high-dimensional objects that are no longer needed. Report $M(Q,n_h,r)$ as a closed-form symbolic expression. Do not include storage for the functions $\\theta_q(\\mu)$, and do not count storage for any quantities that can be recomputed online in $\\mathcal{O}(r^2)$ or less without dependence on $n_h$.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded, well-posed, and formulated with objective, unambiguous language. The problem addresses a fundamental aspect of the offline-online computational paradigm in reduced-order modeling for parametrized systems, which is a standard topic in computational science and engineering.\n\nThe objective is to derive an expression for the total count of floating-point scalars, $M(Q, n_h, r)$, that must be stored after the offline precomputation phase. This stored data must be sufficient to enable the online assembly of the reduced system, its solution, and the reconstruction of the high-fidelity solution approximation, with the constraints specified. The online assembly of the reduced matrix and vector must be independent of the high-fidelity dimension $n_h$.\n\nWe will analyze the storage requirements for the distinct components needed in the online phase.\n\n1.  **Assembly of the Reduced-Order Matrix $A_r(\\mu)$**:\n    The reduced-order matrix is defined by the Galerkin projection $A_r(\\mu) = V_r^{T}A_h(\\mu)V_r$. Substituting the given affine expansion for $A_h(\\mu)$:\n    $$A_r(\\mu) = V_r^{T}\\left(\\sum_{q=1}^{Q}\\theta_q(\\mu)A_q\\right)V_r$$\n    By the linearity of matrix multiplication, we can rearrange this expression as:\n    $$A_r(\\mu) = \\sum_{q=1}^{Q}\\theta_q(\\mu)\\left(V_r^{T}A_qV_r\\right)$$\n    For the online assembly to be independent of the dimension $n_h$, the parameter-independent matrices $A_{r,q} \\equiv V_r^{T}A_qV_r$ must be computed during the offline stage and stored. The inputs to this computation are the high-fidelity component matrices $A_q \\in \\mathbb{R}^{n_h \\times n_h}$ and the reduced basis $V_r \\in \\mathbb{R}^{n_h \\times r}$. Each resulting matrix $A_{r,q}$ has dimensions $r \\times r$. As the problem states that no symmetry is to be exploited, we must store all $r^2$ entries for each of the $Q$ matrices. The total number of scalars for this component is therefore $Q r^2$.\n\n2.  **Assembly of the Reduced-Order Vector $b_r(\\mu)$**:\n    Similarly, the reduced-order right-hand side is $b_r(\\mu) = V_r^{T}b_h(\\mu)$. We substitute the affine expansion for $b_h(\\mu)$:\n    $$b_r(\\mu) = V_r^{T}\\left(\\sum_{q=1}^{Q}\\theta_q(\\mu)b_q\\right) = \\sum_{q=1}^{Q}\\theta_q(\\mu)\\left(V_r^{T}b_q\\right)$$\n    To ensure an online computation that is independent of $n_h$, the parameter-independent vectors $b_{r,q} \\equiv V_r^{T}b_q$ must be precomputed and stored. Each vector $b_{r,q}$ results from the product of a matrix of size $r \\times n_h$ and a vector of size $n_h \\times 1$, yielding a vector of dimension $r$. There are $Q$ such vectors. The total storage cost for this component is $Q r$.\n\n3.  **Reconstruction of the High-Fidelity Solution $u_h(\\mu)$**:\n    The problem mandates that the high-fidelity solution approximation must be recoverable online using the formula $u_h(\\mu) \\approx V_r a_r(\\mu)$, where $a_r(\\mu) \\in \\mathbb{R}^r$ is the solution to the reduced system. This reconstruction requires the reduced basis matrix $V_r \\in \\mathbb{R}^{n_h \\times r}$. Therefore, this matrix must be stored as part of the offline-computed data. The number of scalars required to store $V_r$ is $n_h r$.\n\nThe high-dimensional objects $\\{A_q\\}_{q=1}^Q$ and $\\{b_q\\}_{q=1}^Q$ are used only during the offline stage to compute $\\{A_{r,q}\\}_{q=1}^Q$ and $\\{b_{r,q}\\}_{q=1}^Q$, and are then discarded as per the problem's instructions. The set of objects to be stored for the online phase is thus $\\{A_{r,q}\\}_{q=1}^Q$, $\\{b_{r,q}\\}_{q=1}^Q$, and $V_r$. This is the minimal set required to fulfill all specified online tasks.\n\nThe total storage, $M(Q, n_h, r)$, is the sum of the scalars required for these three components.\n$$M(Q, n_h, r) = (\\text{storage for } \\{A_{r,q}\\}) + (\\text{storage for } \\{b_{r,q}\\}) + (\\text{storage for } V_r)$$\n$$M(Q, n_h, r) = Qr^2 + Qr + n_hr$$\nThis expression can be factored as $M(Q, n_h, r) = r(Qr + Q + n_h)$, which represents the total number of floating-point scalars that must be stored to enable the efficient online execution of the reduced-order model.", "answer": "$$\n\\boxed{Qr^2 + Qr + n_hr}\n$$", "id": "2593090"}]}