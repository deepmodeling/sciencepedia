{"hands_on_practices": [{"introduction": "The primary motivation for hyper-reduction is to break the dependency of online computational cost on the size of the original full-order model ($N$). This practice provides a foundational understanding of this goal by guiding you through a flop count analysis. By constructing a simple cost model, you will quantify the computational expense of a standard nonlinear reduced-order model and compare it directly to a hyper-reduced version, deriving the theoretical speedup factor [@problem_id:2566979]. This exercise makes the abstract benefit of hyper-reduction concrete and highlights which parameters control its effectiveness.", "problem": "Consider a nonlinear finite element method (FEM) semi-discrete model with $N$ degrees of freedom (DOFs) and internal force vector $f_{\\text{int}}(u) \\in \\mathbb{R}^{N}$ defined by an element-wise integral over the spatial domain. Let a reduced-order model (ROM) with reduced basis $V \\in \\mathbb{R}^{N \\times r}$ and reduced coordinates $q \\in \\mathbb{R}^{r}$ approximate the state as $u \\approx V q$. The internal force evaluation is approximated by numerical quadrature using $n_q$ quadrature points, yielding a sum of $n_q$ local contributions. Assume:\n- Each quadrature-point contribution evaluation and its accumulation to the global internal force (including constitutive updates, strain-displacement application, and assembly) costs a constant $c_e$ floating-point operations (flops) independent of $N$, $r$, and $n_q$.\n- A dense matrix-vector multiplication of size $N \\times r$ costs $2 N r$ flops.\n- In the hyper-reduction setting, a sample-based empirical quadrature or interpolation is used with $s$ sampled quadrature points and a precomputed linear reconstruction operator $T \\in \\mathbb{R}^{N \\times s}$ such that the full internal force is approximated from the $s$ sampled contributions via a linear map. Assume evaluating the $s$ samples costs $s c_e$ flops, applying the precomputed sampled operator $(\\text{size } s \\times r)$ to $q$ costs $2 r s$ flops, and lifting the $s$ contributions to an $N$-vector via $T$ costs $2 N s$ flops.\n- All offline precomputations have been completed and incur no online cost.\n\nStarting from the definition of the internal force as a quadrature sum and the above cost model, derive the online floating-point operation counts for evaluating $f_{\\text{int}}(V q)$:\n- without hyper-reduction, and\n- with hyper-reduction.\n\nThen, provide the closed-form analytic expression for the speedup factor $S$ defined as the ratio of the non-hyper-reduced online cost to the hyper-reduced online cost, expressed in terms of $n_q$, $N$, $r$, $s$, and $c_e$. Your final answer must be a single analytic expression for $S$. Do not simplify by assuming any asymptotic regime. No rounding is required.", "solution": "The problem statement has been rigorously validated.\n\n### Step 1: Extract Givens\n- System: Nonlinear finite element method (FEM) semi-discrete model.\n- Full-order degrees of freedom (DOFs): $N$.\n- Internal force vector: $f_{\\text{int}}(u) \\in \\mathbb{R}^{N}$.\n- Reduced basis: $V \\in \\mathbb{R}^{N \\times r}$.\n- Reduced coordinates: $q \\in \\mathbb{R}^{r}$.\n- Reduced-order model (ROM) state approximation: $u \\approx V q$.\n- Number of quadrature points for full model: $n_q$.\n- Cost per quadrature point evaluation and assembly: $c_e$ flops.\n- Cost of a dense matrix-vector multiplication of size $N \\times r$: $2 N r$ flops.\n- Number of sampled quadrature points for hyper-reduction: $s$.\n- Hyper-reduction reconstruction operator: $T \\in \\mathbb{R}^{N \\times s}$.\n- Cost component 1 (hyper-reduction): Evaluating $s$ samples costs $s c_e$ flops.\n- Cost component 2 (hyper-reduction): Applying a precomputed sampled operator of size $s \\times r$ to $q$ costs $2 r s$ flops.\n- Cost component 3 (hyper-reduction): Lifting $s$ contributions to an $N$-vector via $T$ costs $2 N s$ flops.\n- Offline precomputation cost is zero online.\n- Objective: Derive the floating-point operation counts for evaluating $f_{\\text{int}}(V q)$ with and without hyper-reduction, and then the speedup factor $S$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. Its premises are consistent with the established theory of model order reduction and hyper-reduction techniques for nonlinear problems in computational mechanics. The cost models provided are standard simplifications used for complexity analysis. The problem is self-contained and provides all necessary information to derive the required expressions. There are no contradictions, ambiguities, or factual inaccuracies.\n\n### Step 3: Verdict and Action\nThe problem is valid. We will proceed with a complete derivation.\n\nThe objective is to determine the speedup factor $S$, defined as the ratio of the online computational cost without hyper-reduction to the cost with hyper-reduction. We denote these costs as $C_{\\text{full}}$ and $C_{\\text{hyper}}$, respectively.\n\nFirst, we derive the cost for the non-hyper-reduced evaluation of the internal force vector, $f_{\\text{int}}(V q)$. This evaluation involves two sequential steps:\n1.  Compute the full-order state approximation vector, $u = V q$. This is a matrix-vector product between the reduced basis matrix $V \\in \\mathbb{R}^{N \\times r}$ and the reduced coordinate vector $q \\in \\mathbb{R}^{r}$. Based on the provided cost model, the number of floating-point operations (flops) for this operation is $2 N r$.\n2.  Evaluate the nonlinear internal force vector $f_{\\text{int}}(u)$ for the state $u$. The problem states that this is performed using numerical quadrature over the entire domain, involving $n_q$ quadrature points. The cost for evaluating the contribution of each quadrature point and accumulating it into the global force vector is given as a constant $c_e$. Therefore, the total cost for this step is the number of points multiplied by the cost per point, which is $n_q c_e$.\n\nThe total online cost for the non-hyper-reduced case, $C_{\\text{full}}$, is the sum of the costs of these two steps:\n$$C_{\\text{full}} = n_q c_e + 2 N r$$\n\nNext, we derive the cost for the hyper-reduced evaluation. The problem statement explicitly partitions the online cost of the hyper-reduced evaluation into three distinct components:\n1.  Evaluating the nonlinear function at $s$ sampled quadrature points, which costs $s c_e$ flops.\n2.  Applying a precomputed operator to the reduced coordinates $q$ to obtain necessary inputs for the sampled evaluations, which costs $2 r s$ flops.\n3.  Lifting the $s$ evaluated force contributions to reconstruct the full $N$-dimensional internal force vector, which costs $2 N s$ flops.\n\nThe total online cost for the hyper-reduced case, $C_{\\text{hyper}}$, is the sum of these three defined components.\n$$C_{\\text{hyper}} = s c_e + 2 r s + 2 N s$$\nThis cost is independent of the full-system number of quadrature points $n_q$, which is the central purpose of hyper-reduction.\n\nFinally, the speedup factor $S$ is defined as the ratio of the non-hyper-reduced cost to the hyper-reduced cost.\n$$S = \\frac{C_{\\text{full}}}{C_{\\text{hyper}}}$$\nSubstituting the derived expressions for $C_{\\text{full}}$ and $C_{\\text{hyper}}$ gives the final closed-form expression for the speedup:\n$$S = \\frac{n_q c_e + 2 N r}{s c_e + 2 r s + 2 N s}$$\nThe expression can also be written by factoring out $s$ in the denominator, but this is not required.\n$$S = \\frac{n_q c_e + 2 N r}{s(c_e + 2 r + 2 N)}$$\nThis expression provides the theoretical speedup achieved by the specified hyper-reduction scheme as a function of the problem parameters $n_q$, $N$, $r$, $s$, and the unit cost $c_e$.", "answer": "$$\n\\boxed{\\frac{n_q c_e + 2 N r}{s c_e + 2 r s + 2 N s}}\n$$", "id": "2566979"}, {"introduction": "With the motivation established, we now look inside the mechanics of a hyper-reduced nonlinear solver. This exercise demystifies the Gauss–Newton with Approximated Tensors (GNAT) method by having you compute a single update step for a small, well-defined nonlinear system [@problem_id:2566919]. You will apply the core components of the method—the trial basis, residual basis, and sampling operator—to see how they collaboratively reduce the dimensionality of the update problem, providing a tangible illustration of the algorithm in action.", "problem": "Consider the nonlinear algebraic residual $R(u) \\in \\mathbb{R}^{3}$ arising from a finite element discretization, where $u \\in \\mathbb{R}^{3}$ and\n$$\nR(u) \\equiv \n\\begin{pmatrix}\nu_{1} + u_{2}^{2} - 1 \\\\\n2 u_{1} - u_{3} + \\sin(u_{2}) \\\\\nu_{1} u_{3} - \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\nLet the trial basis for the reduced-order model (ROM) be $\\Phi \\in \\mathbb{R}^{3 \\times 2}$ and the residual basis be $Z \\in \\mathbb{R}^{3 \\times 2}$, given by\n$$\n\\Phi = \n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & -1\n\\end{pmatrix},\n\\qquad\nZ =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nLet the sampling matrix $P \\in \\mathbb{R}^{3 \\times 2}$ select the first two residual entries, so that $P^{T} \\in \\mathbb{R}^{2 \\times 3}$ is\n$$\nP^{T} = \n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\nAssume the current full-state iterate is\n$$\nu^{k} = \n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n1\n\\end{pmatrix}.\n$$\nUsing the Gauss–Newton with Approximated Tensors (GNAT) hyper-reduction at $u^{k}$ with the given sampling matrix $P$ and residual basis $Z$ (you may take the reference state to be $0$ so that the reduced correction acts through $\\Phi$), form the hyper-reduced Gauss–Newton update for the reduced coordinates $\\Delta a \\in \\mathbb{R}^{2}$ by minimizing the sampled residual in the Gauss–Newton linearization at $u^{k}$, and compute the resulting reduced correction. Report the reduced correction $\\Delta a$ as a row vector. No rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**1. Givens Extraction:**\n- Nonlinear residual: $R(u) = \\begin{pmatrix} u_{1} + u_{2}^{2} - 1 \\\\ 2 u_{1} - u_{3} + \\sin(u_{2}) \\\\ u_{1} u_{3} - \\frac{1}{2} \\end{pmatrix} \\in \\mathbb{R}^{3}$ for $u \\in \\mathbb{R}^{3}$.\n- Trial basis: $\\Phi = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}$.\n- Residual basis: $Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}$.\n- Sampling matrix transpose: $P^{T} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}$.\n- Current state iterate: $u^{k} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\in \\mathbb{R}^{3}$.\n- The task is to compute the reduced correction $\\Delta a \\in \\mathbb{R}^2$ using the Gauss-Newton with Approximated Tensors (GNAT) method.\n\n**2. Validation:**\nThe problem is set within the established field of numerical analysis for model order reduction. All mathematical objects ($R(u)$, $\\Phi$, $Z$, $P$, $u^k$) are well-defined, and their dimensions are consistent for the operations involved. The problem asks for the application of a specific, albeit simplified, version of the GNAT algorithm. The terminology is standard within the discipline. There are no scientific contradictions, no missing information, and no subjective elements. The problem is therefore deemed valid.\n\n**3. Solution:**\nThe Gauss-Newton method is an iterative procedure for solving nonlinear systems of equations, such as $R(u)=0$. At each iteration $k$, it solves a linear system for a correction $\\Delta u$, based on a first-order Taylor expansion of the residual at the current iterate $u^k$:\n$$R(u^k + \\Delta u) \\approx R(u^k) + J(u^k) \\Delta u$$\nwhere $J(u^k)$ is the Jacobian of $R(u)$ evaluated at $u^k$. The update is found by setting the linearized residual to zero: $J(u^k) \\Delta u = -R(u^k)$.\n\nIn a reduced-order model (ROM), the state correction is restricted to the subspace spanned by the columns of the trial basis $\\Phi$, so $\\Delta u = \\Phi \\Delta a$, where $\\Delta a \\in \\mathbb{R}^2$ is the reduced correction vector. The linearized residual becomes:\n$$R_{lin}(\\Delta a) = R(u^k) + J(u^k) \\Phi \\Delta a$$\n\nThe GNAT hyper-reduction technique constructs a smaller, computationally inexpensive system. As specified, this involves using the sampling matrix $P$ and the residual basis $Z$. We apply a Petrov-Galerkin projection to the sampled linearized residual, $P^T R_{lin}(\\Delta a)$. The test basis for this projection is derived from the residual basis $Z$ and the sampling matrix $P$, giving the test operator $(P^T Z)^T$. The resulting hyper-reduced system is:\n$$ (P^T Z)^T \\left( P^T R(u^k) + P^T J(u^k) \\Phi \\Delta a \\right) = 0 $$\nThis can be rearranged into a linear system for the unknown reduced correction $\\Delta a$:\n$$ \\left((P^T Z)^T P^T J(u^k) \\Phi\\right) \\Delta a = - (P^T Z)^T P^T R(u^k) $$\n\nWe now compute the necessary components. First, the residual at the current iterate $u^k = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$:\n$$ R(u^k) = \\begin{pmatrix} 1 + 0^2 - 1 \\\\ 2(1) - 1 + \\sin(0) \\\\ (1)(1) - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix} $$\nNext, we compute the Jacobian matrix $J(u) = \\frac{\\partial R}{\\partial u}$:\n$$ J(u) = \\begin{pmatrix} 1 & 2u_2 & 0 \\\\ 2 & \\cos(u_2) & -1 \\\\ u_3 & 0 & u_1 \\end{pmatrix} $$\nEvaluating at $u^k$:\n$$ J(u^k) = \\begin{pmatrix} 1 & 2(0) & 0 \\\\ 2 & \\cos(0) & -1 \\\\ 1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & -1 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\n\nNow we compute the matrices for the linear system.\nThe term involving the test basis is:\n$$ P^T Z = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nSince $P^T Z$ is the $2 \\times 2$ identity matrix $I_2$, so is its transpose. The linear system for $\\Delta a$ simplifies to:\n$$ \\left(P^T J(u^k) \\Phi\\right) \\Delta a = - P^T R(u^k) $$\nThis simplification arises because the choice of sampling points and residual basis leads to a collocation condition for the projected test basis. This is also the system one would obtain by finding the exact solution to the sampled linearized system, i.e., minimizing $\\| P^T R(u^k) + P^T J(u^k) \\Phi \\Delta a \\|_2^2$, since the matrix $P^T J(u^k) \\Phi$ is square and invertible.\n\nWe compute the matrix $P^T J(u^k) \\Phi$:\n$$ J(u^k) \\Phi = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & -1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 2-1 & 1+1 \\\\ 1+1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\\\ 2 & -1 \\end{pmatrix} $$\n$$ P^T (J(u^k) \\Phi) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\\\ 2 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\end{pmatrix} $$\nAnd the right-hand side vector $-P^T R(u^k)$:\n$$ P^T R(u^k) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $$\nSo, the linear system for $\\Delta a = \\begin{pmatrix} \\Delta a_1 \\\\ \\Delta a_2 \\end{pmatrix}$ is:\n$$ \\begin{pmatrix} 1 & 0 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\Delta a_1 \\\\ \\Delta a_2 \\end{pmatrix} = - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} $$\nThis system is solved by forward substitution. The first equation is:\n$$ 1 \\cdot \\Delta a_1 + 0 \\cdot \\Delta a_2 = 0 \\implies \\Delta a_1 = 0 $$\nSubstituting this into the second equation:\n$$ 1 \\cdot \\Delta a_1 + 2 \\cdot \\Delta a_2 = -1 \\implies 1(0) + 2 \\Delta a_2 = -1 \\implies 2 \\Delta a_2 = -1 \\implies \\Delta a_2 = -\\frac{1}{2} $$\nThe reduced correction vector is $\\Delta a = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$. The problem requires the answer as a row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & -\\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "2566919"}, {"introduction": "Hyper-reduction techniques achieve speedup by introducing approximations, most notably in the evaluation of the system's tangent or Jacobian matrix. A critical question for any numerical analyst is how these approximations impact the convergence of the nonlinear solver. This advanced practice explores this very issue, asking you to analyze the local convergence of a hyper-reduced Newton method and derive how the quality of the tangent approximation affects the asymptotic convergence rate [@problem_id:2566949]. This exercise provides crucial insight into the theoretical underpinnings of stable and efficient hyper-reduced solvers.", "problem": "Consider a nonlinear finite element equilibrium problem with residual map $r:\\mathbb{R}^{N}\\to\\mathbb{R}^{N}$ arising from a Galerkin discretization, and let $u^{\\star}\\in\\mathbb{R}^{N}$ denote a solution satisfying $r(u^{\\star})=0$. Let $U_{r}\\in\\mathbb{R}^{N\\times n}$ be a full-column-rank reduced basis with $n\\ll N$, and define the Reduced Order Model (ROM) residual associated with a hyper-reduced Petrov–Galerkin projection as $R:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ given by $R(a):=\\Phi^{T}W\\,r(U_{r}a)$, where $\\Phi\\in\\mathbb{R}^{N\\times n}$ is a test basis and $W\\in\\mathbb{R}^{N\\times N}$ is a symmetric positive definite weighting induced by a hyper-reduction technique such as the Discrete Empirical Interpolation Method (DEIM) or gappy reconstruction. Let $a^{\\star}\\in\\mathbb{R}^{n}$ satisfy $R(a^{\\star})=0$ and $u^{\\star}=U_{r}a^{\\star}$.\n\nA hyper-reduced Newton iteration for solving $R(a)=0$ takes the form $a_{k+1}=a_{k}+s_{k}$ with $s_{k}$ obtained from the linear system $B(a_{k})\\,s_{k}=-R(a_{k})$, where $B(a)\\in\\mathbb{R}^{n\\times n}$ is a computable tangent approximation. In this context, define precisely what it means for the tangent to be “consistent.” Then, assume the following properties hold in a neighborhood of $a^{\\star}$:\n\n1) $R$ is continuously differentiable and its Jacobian $J(a):=\\partial R(a)/\\partial a$ is Lipschitz continuous with constant $L>0$, i.e., $\\|J(a)-J(b)\\|\\le L\\|a-b\\|$ for all $a,b$ near $a^{\\star}$, where $\\|\\cdot\\|$ is any operator norm induced by a vector norm.\n\n2) $J(a^{\\star})$ is nonsingular and there exists $\\gamma>0$ such that $\\|J(a)^{-1}\\|\\le\\gamma$ for all $a$ in a neighborhood of $a^{\\star}$.\n\n3) The hyper-reduced tangent $B(a)$ satisfies a first-order consistency error bound with respect to $J(a)$ of the form $\\|B(a)-J(a)\\|\\le c\\,\\|a-a^{\\star}\\|$ for some constant $c\\ge 0$ and all $a$ near $a^{\\star}$.\n\n4) The linear systems are solved exactly at each Newton iteration.\n\nStarting from these assumptions and standard differentiability properties only, derive the local asymptotic error recursion for the hyper-reduced Newton method and identify the smallest leading-order constant $q=q(L,\\gamma,c)$ such that $\\|a_{k+1}-a^{\\star}\\|\\le q\\,\\|a_{k}-a^{\\star}\\|^{2}+o(\\|a_{k}-a^{\\star}\\|^{2})$ as $k\\to\\infty$. Express your final result as a closed-form analytic expression in terms of $L$, $\\gamma$, and $c$. Your final answer must be a single analytic expression and contain no units.", "solution": "The problem statement is valid. It presents a well-posed question in the field of numerical analysis for model order reduction, with sufficient and consistent assumptions for a rigorous derivation.\n\nThe problem consists of two parts. First, to define what it means for the approximate tangent $B(a)$ to be \"consistent\" with the true Jacobian $J(a)$, and second, to derive the local asymptotic convergence rate for the hyper-reduced Newton method.\n\nFirst, we address the definition of consistency. In the context of iterative methods of the Newton type, an approximate Jacobian or tangent operator $B(a)$ is said to be **consistent** with the true Jacobian $J(a)$ at the solution $a^{\\star}$ if the approximation converges to the true Jacobian as the iterate approaches the solution. Precisely, this means $B(a^{\\star}) = J(a^{\\star})$. This is also known as zero-order consistency.\nThe problem provides a stronger condition in Assumption 3: $\\|B(a)-J(a)\\|\\le c\\,\\|a-a^{\\star}\\|$. This inequality implies zero-order consistency. Assuming $B(a)$ and $J(a)$ are continuous at $a^{\\star}$, taking the limit as $a \\to a^{\\star}$ yields:\n$$ \\lim_{a \\to a^{\\star}} \\|B(a)-J(a)\\| \\le \\lim_{a \\to a^{\\star}} c\\,\\|a-a^{\\star}\\| = 0 $$\nThis implies $\\|B(a^{\\star}) - J(a^{\\star})\\| = 0$, which is equivalent to $B(a^{\\star}) = J(a^{\\star})$. The condition in Assumption 3 is a statement about the rate of this convergence and is often called **first-order consistency**.\n\nNext, we derive the local asymptotic error recursion. Let $e_{k} := a_{k}-a^{\\star}$ be the error at iteration $k$. The hyper-reduced Newton iteration is given by $a_{k+1} = a_{k} - B(a_{k})^{-1} R(a_{k})$. Subtracting $a^{\\star}$ from both sides gives the error evolution equation:\n$$ e_{k+1} = a_{k+1} - a^{\\star} = (a_{k}-a^{\\star}) - B(a_{k})^{-1} R(a_{k}) = e_{k} - B(a_{k})^{-1} R(a_{k}) $$\nTo analyze $R(a_k)$, we perform a Taylor expansion of $R(a)$ around the point $a_k$ to evaluate it at $a^{\\star}$. Since $R$ is continuously differentiable (Assumption 1), we have:\n$$ R(a^{\\star}) = R(a_{k}) + J(a_{k})(a^{\\star}-a_{k}) + \\mathcal{E}_{k} $$\nwhere $\\mathcal{E}_{k}$ is the remainder term. Since $R(a^{\\star})=0$ and $a^{\\star}-a_{k} = -e_{k}$, the equation becomes:\n$$ 0 = R(a_{k}) - J(a_{k})e_{k} + \\mathcal{E}_{k} $$\nThis allows us to express $R(a_{k})$ as $R(a_{k}) = J(a_{k})e_{k} - \\mathcal{E}_{k}$. The remainder $\\mathcal{E}_{k}$ can be written in integral form:\n$$ \\mathcal{E}_{k} = \\int_{0}^{1} \\left[ J(a_{k} + t(a^{\\star}-a_{k})) - J(a_{k}) \\right](a^{\\star}-a_{k}) \\,dt $$\nWe can bound the norm of this remainder using the triangle inequality for integrals and Assumption 1, which states that $J(a)$ is Lipschitz continuous with constant $L$:\n$$ \\|\\mathcal{E}_{k}\\| \\le \\int_{0}^{1} \\| J(a_{k} - t e_{k}) - J(a_{k}) \\| \\|-e_{k}\\| \\,dt $$\n$$ \\|\\mathcal{E}_{k}\\| \\le \\int_{0}^{1} L \\|(a_{k} - t e_{k}) - a_{k}\\| \\|e_{k}\\| \\,dt = \\int_{0}^{1} L(t\\|e_{k}\\|)\\|e_{k}\\| \\,dt = L\\|e_{k}\\|^{2} \\int_{0}^{1} t \\,dt = \\frac{L}{2}\\|e_{k}\\|^{2} $$\nNow, we substitute the expression for $R(a_{k})$ back into the error evolution equation:\n$$ e_{k+1} = e_{k} - B(a_{k})^{-1} \\left[ J(a_{k})e_{k} - \\mathcal{E}_{k} \\right] $$\n$$ e_{k+1} = B(a_{k})^{-1} \\left[ B(a_{k})e_{k} - J(a_{k})e_{k} + \\mathcal{E}_{k} \\right] $$\n$$ e_{k+1} = B(a_{k})^{-1} \\left[ (B(a_{k}) - J(a_{k}))e_{k} + \\mathcal{E}_{k} \\right] $$\nTaking the norm of both sides and applying the triangle inequality:\n$$ \\|e_{k+1}\\| \\le \\|B(a_{k})^{-1}\\| \\left( \\|(B(a_{k}) - J(a_{k}))e_{k}\\| + \\|\\mathcal{E}_{k}\\| \\right) $$\n$$ \\|e_{k+1}\\| \\le \\|B(a_{k})^{-1}\\| \\left( \\|B(a_{k}) - J(a_{k})\\| \\|e_{k}\\| + \\|\\mathcal{E}_{k}\\| \\right) $$\nWe now use the bounds derived. From Assumption 3, $\\|B(a_{k}) - J(a_{k})\\| \\le c\\|a_{k}-a^{\\star}\\| = c\\|e_{k}\\|$. We have also shown that $\\|\\mathcal{E}_{k}\\| \\le \\frac{L}{2}\\|e_{k}\\|^{2}$. Substituting these gives:\n$$ \\|e_{k+1}\\| \\le \\|B(a_{k})^{-1}\\| \\left( c\\|e_{k}\\| \\cdot \\|e_{k}\\| + \\frac{L}{2}\\|e_{k}\\|^{2} \\right) = \\|B(a_{k})^{-1}\\| \\left( c + \\frac{L}{2} \\right) \\|e_{k}\\|^{2} $$\nThe final step is to analyze $\\|B(a_{k})^{-1}\\|$ for $a_k$ in a neighborhood of $a^\\star$. We use the Banach fixed-point theorem for operator inversion (or Banach perturbation lemma). Let $A = J(a_k)$ and the perturbation be $E = B(a_k) - J(a_k)$. As per Assumption 2, for $a_k$ near $a^\\star$, $J(a_k)$ is invertible and $\\|J(a_k)^{-1}\\| \\le \\gamma$. For $B(a_k) = A+E$ to be invertible, we require $\\|A^{-1}E\\| < 1$.\n$$ \\|J(a_{k})^{-1}(B(a_{k}) - J(a_{k}))\\| \\le \\|J(a_{k})^{-1}\\| \\|B(a_{k}) - J(a_{k})\\| \\le \\gamma (c\\|e_{k}\\|) $$\nFor $a_k$ sufficiently close to $a^\\star$ (i.e., $\\|e_k\\|$ small enough such that $\\gamma c \\|e_k\\| < 1$), $B(a_k)$ is invertible. The norm of its inverse is bounded by:\n$$ \\|B(a_{k})^{-1}\\| \\le \\frac{\\|J(a_{k})^{-1}\\|}{1 - \\|J(a_{k})^{-1}(B(a_{k}) - J(a_{k}))\\|} \\le \\frac{\\gamma}{1 - \\gamma c \\|e_{k}\\|} $$\nAs $k\\to\\infty$, we have $\\|e_{k}\\|\\to 0$. We can expand the bound using a geometric series:\n$$ \\|B(a_{k})^{-1}\\| \\le \\gamma(1 + \\gamma c \\|e_{k}\\| + O(\\|e_{k}\\|^{2})) = \\gamma + O(\\|e_{k}\\|) $$\nThe leading-order term for $\\|B(a_k)^{-1}\\|$ is $\\gamma$. Substituting this into our main inequality for $\\|e_{k+1}\\|$:\n$$ \\|e_{k+1}\\| \\le (\\gamma + O(\\|e_{k}\\|)) \\left( c + \\frac{L}{2} \\right) \\|e_{k}\\|^{2} $$\n$$ \\|e_{k+1}\\| \\le \\gamma \\left( c + \\frac{L}{2} \\right) \\|e_{k}\\|^{2} + O(\\|e_{k}\\|^{3}) $$\nThis expression is of the form $\\|a_{k+1}-a^{\\star}\\|\\le q\\|a_{k}-a^{\\star}\\|^{2}+o(\\|a_{k}-a^{\\star}\\|^{2})$. The leading-order constant $q$ is the coefficient of the $\\|e_{k}\\|^{2}$ term.\nThus, the smallest leading-order constant is:\n$$ q = \\gamma \\left( c + \\frac{L}{2} \\right) $$\nThis constant combines the effect of the nonlinearity of the problem (via $L$), the stability of the Jacobian (via $\\gamma$), and the first-order consistency error of the hyper-reduced tangent (via $c$). If $c=0$, we recover the quadratic convergence rate constant $\\frac{\\gamma L}{2}$ for the exact Newton method.", "answer": "$$\\boxed{\\gamma \\left( c + \\frac{L}{2} \\right)}$$", "id": "2566949"}]}