{"hands_on_practices": [{"introduction": "The theoretical guarantee of $M$-orthogonality is a cornerstone of modal analysis. This exercise makes the concept tangible by guiding you through a direct calculation on a simple 2-DOF system ([@problem_id:2578481]). By computing the Modal Assurance Criterion (MAC) with respect to both the standard Euclidean inner product and the mass-weighted $M$-inner product, you will gain a firsthand understanding of why the choice of inner product is critical and how the modes, while not orthogonal in a familiar geometric sense, are perfectly orthogonal in the space defined by the system's kinetic energy.", "problem": "Consider the undamped free-vibration generalized eigenproblem from the Finite Element Method (FEM) for a two-degree-of-freedom system,\n$$\nK\\,\\phi=\\lambda\\,M\\,\\phi,\n$$\nwith symmetric positive-definite mass and stiffness matrices\n$$\nM=\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix},\\qquad\nK=\\begin{pmatrix}3 & -1 \\\\ -1 & 1\\end{pmatrix}.\n$$\nStart from the fundamental definitions that (i) free vibrations satisfy the generalized eigenproblem above, (ii) the modes of distinct eigenvalues are orthogonal with respect to the $M$-weighted inner product, and (iii) for any inner product $\\langle\\cdot,\\cdot\\rangle$ on $\\mathbb{R}^n$, the squared cosine of the angle between vectors $a$ and $b$ is given by\n$$\n\\frac{|\\langle a,b\\rangle|^2}{\\langle a,a\\rangle\\,\\langle b,b\\rangle},\n$$\nwhich defines the Modal Assurance Criterion (MAC) once an inner product has been chosen.\n\n1) Solve the generalized eigenproblem to obtain analytic eigenpairs $(\\lambda_1,\\phi_1)$ and $(\\lambda_2,\\phi_2)$, and verify the $M$-orthogonality of the two modes by computing $\\phi_1^T M\\,\\phi_2$ and the Euclidean non-orthogonality via $\\phi_1^T \\phi_2$.\n\n2) Suppose a coarse discretization produces an approximate first-mode shape\n$$\n\\tilde{\\phi}_1=\\begin{pmatrix}1 \\\\ \\tfrac{9}{5}\\end{pmatrix}.\n$$\nCompute the Modal Assurance Criterion (MAC) between the exact $\\phi_1$ and the approximate $\\tilde{\\phi}_1$ with respect to:\n- the Euclidean inner product $\\langle a,b\\rangle_{\\mathrm{E}}=a^T b$,\n- the $M$-weighted inner product $\\langle a,b\\rangle_{M}=a^T M b$.\n\n3) Briefly interpret why the two MAC values differ in this example in light of modal orthogonality and normalization principles.\n\nProvide as your final deliverable a single exact number equal to\n$$\n\\Delta=\\mathrm{MAC}_{M}(\\phi_1,\\tilde{\\phi}_1)-\\mathrm{MAC}_{\\mathrm{E}}(\\phi_1,\\tilde{\\phi}_1),\n$$\nexpressed as a fully reduced fraction. Do not include units. Do not round; give the exact value.", "solution": "The problem statement is first subjected to validation.\n\nGivens are extracted verbatim:\n1.  The governing equation is the generalized eigenproblem $K\\,\\phi=\\lambda\\,M\\,\\phi$.\n2.  The mass matrix is $M=\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}$.\n3.  The stiffness matrix is $K=\\begin{pmatrix}3 & -1 \\\\ -1 & 1\\end{pmatrix}$.\n4.  $M$ and $K$ are stated to be symmetric positive-definite.\n5.  Modes of distinct eigenvalues are orthogonal with respect to the $M$-weighted inner product.\n6.  The squared cosine of the angle between vectors $a$ and $b$ for an inner product $\\langle\\cdot,\\cdot\\rangle$ is given by $\\frac{|\\langle a,b\\rangle|^2}{\\langle a,a\\rangle\\,\\langle b,b\\rangle}$, which defines the Modal Assurance Criterion (MAC).\n7.  An approximate first-mode shape is given as $\\tilde{\\phi}_1=\\begin{pmatrix}1 \\\\ \\tfrac{9}{5}\\end{pmatrix}$.\n8.  Two inner products are defined: Euclidean $\\langle a,b\\rangle_{\\mathrm{E}}=a^T b$ and $M$-weighted $\\langle a,b\\rangle_{M}=a^T M b$.\n9.  The final deliverable is $\\Delta=\\mathrm{MAC}_{M}(\\phi_1,\\tilde{\\phi}_1)-\\mathrm{MAC}_{\\mathrm{E}}(\\phi_1,\\tilde{\\phi}_1)$.\n\nThe problem is validated as follows:\n- **Scientifically Grounded:** The problem pertains to standard modal analysis of a linear time-invariant mechanical system, a core topic in engineering and physics. The matrices are physically plausible. We verify the positive-definiteness. For $M$, the eigenvalues are $2$ and $1$, which are positive. For $K$, the leading principal minors are $\\det(3)=3>0$ and $\\det(K)=3(1)-(-1)(-1)=2>0$. Both matrices are indeed symmetric and positive-definite.\n- **Well-Posed:** The problem is a standard generalized eigenvalue problem with symmetric positive-definite matrices, which guarantees real, positive eigenvalues and a set of $M$-orthogonal eigenvectors. The tasks are specific and lead to a unique numerical answer.\n- **Objective:** The problem is stated using precise mathematical language without subjectivity.\n\nThe verdict is that the problem is valid. We proceed to the solution.\n\nThe solution is found by following the three parts of the problem description.\n\nPart 1: Solve the generalized eigenproblem and verify orthogonality.\nThe eigenproblem is $(K - \\lambda M)\\phi = 0$. For non-trivial solutions, the determinant of the characteristic matrix must be zero:\n$$\n\\det(K - \\lambda M) = \\det\\left(\\begin{pmatrix}3 & -1 \\\\ -1 & 1\\end{pmatrix} - \\lambda \\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\right) = \\det\\begin{pmatrix}3 - 2\\lambda & -1 \\\\ -1 & 1 - \\lambda\\end{pmatrix} = 0\n$$\nThis gives the characteristic equation:\n$$\n(3 - 2\\lambda)(1 - \\lambda) - (-1)^2 = 0\n$$\n$$\n3 - 3\\lambda - 2\\lambda + 2\\lambda^2 - 1 = 0\n$$\n$$\n2\\lambda^2 - 5\\lambda + 2 = 0\n$$\nSolving this quadratic equation for $\\lambda$:\n$$\n\\lambda = \\frac{-(-5) \\pm \\sqrt{(-5)^2 - 4(2)(2)}}{2(2)} = \\frac{5 \\pm \\sqrt{25-16}}{4} = \\frac{5 \\pm 3}{4}\n$$\nThe eigenvalues are $\\lambda_1 = \\frac{5-3}{4} = \\frac{1}{2}$ and $\\lambda_2 = \\frac{5+3}{4} = 2$.\n\nFor each eigenvalue, we find the corresponding eigenvector (mode shape).\nFor $\\lambda_1 = \\frac{1}{2}$:\n$$\n(K - \\lambda_1 M)\\phi_1 = \\begin{pmatrix}3 - 2(\\frac{1}{2}) & -1 \\\\ -1 & 1 - \\frac{1}{2}\\end{pmatrix} \\phi_1 = \\begin{pmatrix}2 & -1 \\\\ -1 & \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}\\phi_{11} \\\\ \\phi_{12}\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n$$\nThis yields the equation $2\\phi_{11} - \\phi_{12} = 0$, or $\\phi_{12} = 2\\phi_{11}$. Choosing $\\phi_{11} = 1$ gives $\\phi_1 = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$.\n\nFor $\\lambda_2 = 2$:\n$$\n(K - \\lambda_2 M)\\phi_2 = \\begin{pmatrix}3 - 2(2) & -1 \\\\ -1 & 1 - 2\\end{pmatrix} \\phi_2 = \\begin{pmatrix}-1 & -1 \\\\ -1 & -1\\end{pmatrix} \\begin{pmatrix}\\phi_{21} \\\\ \\phi_{22}\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\n$$\nThis yields the equation $-\\phi_{21} - \\phi_{22} = 0$, or $\\phi_{22} = -\\phi_{21}$. Choosing $\\phi_{21} = 1$ gives $\\phi_2 = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$.\n\nThe eigenpairs are $(\\lambda_1, \\phi_1) = (\\frac{1}{2}, \\begin{pmatrix}1 \\\\ 2\\end{pmatrix})$ and $(\\lambda_2, \\phi_2) = (2, \\begin{pmatrix}1 \\\\ -1\\end{pmatrix})$.\n\nWe verify the orthogonality properties:\n$M$-orthogonality:\n$$\n\\phi_1^T M \\phi_2 = \\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}2 \\\\ -1\\end{pmatrix} = 1(2) + 2(-1) = 0\n$$\nThe modes are $M$-orthogonal, as theory dictates.\nEuclidean non-orthogonality:\n$$\n\\phi_1^T \\phi_2 = \\begin{pmatrix}1 & 2\\end{pmatrix} \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} = 1(1) + 2(-1) = -1\n$$\nThe modes are not orthogonal in the Euclidean sense.\n\nPart 2: Compute the MAC values.\nWe compare the exact first mode $\\phi_1 = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$ with the approximate mode $\\tilde{\\phi}_1 = \\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix}$.\n\nThe MAC with respect to the Euclidean inner product is:\n$$\n\\mathrm{MAC}_{\\mathrm{E}}(\\phi_1, \\tilde{\\phi}_1) = \\frac{|\\phi_1^T \\tilde{\\phi}_1|^2}{(\\phi_1^T \\phi_1)(\\tilde{\\phi}_1^T \\tilde{\\phi}_1)}\n$$\nThe terms are:\n$\\phi_1^T \\tilde{\\phi}_1 = 1(1) + 2(\\frac{9}{5}) = 1 + \\frac{18}{5} = \\frac{23}{5}$.\n$\\phi_1^T \\phi_1 = 1^2 + 2^2 = 5$.\n$\\tilde{\\phi}_1^T \\tilde{\\phi}_1 = 1^2 + (\\frac{9}{5})^2 = 1 + \\frac{81}{25} = \\frac{25+81}{25} = \\frac{106}{25}$.\n$$\n\\mathrm{MAC}_{\\mathrm{E}} = \\frac{(\\frac{23}{5})^2}{5 \\cdot \\frac{106}{25}} = \\frac{\\frac{529}{25}}{\\frac{530}{25}} = \\frac{529}{530}\n$$\n\nThe MAC with respect to the $M$-weighted inner product is:\n$$\n\\mathrm{MAC}_{M}(\\phi_1, \\tilde{\\phi}_1) = \\frac{|\\phi_1^T M \\tilde{\\phi}_1|^2}{(\\phi_1^T M \\phi_1)(\\tilde{\\phi}_1^T M \\tilde{\\phi}_1)}\n$$\nThe terms are:\n$\\phi_1^T M \\tilde{\\phi}_1 = \\begin{pmatrix}1 & 2\\end{pmatrix}\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix} = \\begin{pmatrix}2 & 2\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix} = 2(1) + 2(\\frac{9}{5}) = 2 + \\frac{18}{5} = \\frac{28}{5}$.\n$\\phi_1^T M \\phi_1 = \\begin{pmatrix}1 & 2\\end{pmatrix}\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ 2\\end{pmatrix} = 1(2)(1) + 2(1)(2) = 2+4 = 6$.\n$\\tilde{\\phi}_1^T M \\tilde{\\phi}_1 = \\begin{pmatrix}1 & \\frac{9}{5}\\end{pmatrix}\\begin{pmatrix}2 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{9}{5}\\end{pmatrix} = 1(2)(1) + \\frac{9}{5}(1)(\\frac{9}{5}) = 2 + \\frac{81}{25} = \\frac{50+81}{25} = \\frac{131}{25}$.\n$$\n\\mathrm{MAC}_{M} = \\frac{(\\frac{28}{5})^2}{6 \\cdot \\frac{131}{25}} = \\frac{\\frac{784}{25}}{\\frac{786}{25}} = \\frac{784}{786} = \\frac{392}{393}\n$$\n\nPart 3: Interpretation.\nThe Modal Assurance Criterion (MAC) is a measure of collinearity, interpreted as the squared cosine of the angle between two vectors. This \"angle\" is defined by the choice of inner product.\nThe Euclidean inner product $\\langle\\cdot,\\cdot\\rangle_{\\mathrm{E}}$ defines a standard geometric angle.\nThe $M$-weighted inner product $\\langle\\cdot,\\cdot\\rangle_{M}$ defines an angle in a space where coordinates are scaled according to the mass distribution of the physical system. This is the natural inner product for structural dynamics, as it is related to kinetic energy, and it is the one with respect to which the system's modes are orthogonal.\nThe two MAC values differ because the underlying geometric structures used to measure collinearity are different. Unless the mass matrix $M$ is a scalar multiple of the identity matrix, the Euclidean and $M$-weighted inner products will define different geometries, and thus yield different MAC values for the same pair of vectors.\n\nFinal deliverable calculation:\nWe are required to compute $\\Delta = \\mathrm{MAC}_{M} - \\mathrm{MAC}_{\\mathrm{E}}$.\n$$\n\\Delta = \\frac{392}{393} - \\frac{529}{530}\n$$\nTo subtract, we find a common denominator, which is $393 \\times 530$.\n$$\n\\Delta = \\frac{392 \\times 530 - 529 \\times 393}{393 \\times 530}\n$$\nThe numerator is:\n$392 \\times 530 = 207760$.\n$529 \\times 393 = 207897$.\nNumerator $= 207760 - 207897 = -137$.\nThe denominator is:\n$393 \\times 530 = 208290$.\nThus, the difference is:\n$$\n\\Delta = -\\frac{137}{208290}\n$$\nThe number $137$ is prime, and $208290$ is not divisible by $137$. Therefore, this fraction is in its simplest form.", "answer": "$$\n\\boxed{-\\frac{137}{208290}}\n$$", "id": "2578481"}, {"introduction": "Moving from analytical solutions to computational results, this practice addresses the essential task of verifying $M$-orthogonality for numerically computed eigenvectors ([@problem_id:2578532]). You will design a procedure that is invariant to the arbitrary scaling of eigenvectors and robust to small numerical errors, a fundamental skill for validating results from any finite element analysis software.", "problem": "Consider the generalized symmetric definite eigenvalue problem in the finite element method context: given matrices $K \\in \\mathbb{R}^{n \\times n}$ and $M \\in \\mathbb{R}^{n \\times n}$ with $K = K^T$ and $M = M^T \\succ 0$, an eigenpair $(\\lambda, \\phi)$ satisfies $K \\phi = \\lambda M \\phi$. In exact arithmetic, eigenvectors associated with distinct eigenvalues are orthogonal with respect to the $M$-inner product. For a computed set of column vectors $\\{\\hat{\\phi}_i\\}_{i=1}^m$ assembled as $\\hat{\\Phi} \\in \\mathbb{R}^{n \\times m}$, numerical verification of $M$-orthogonality requires careful handling of scaling and roundoff.\n\nTask: Design and implement a procedure that, given $K$, $M$, a matrix $\\hat{\\Phi}$ whose columns are the computed eigenvectors, and a tolerance $\\epsilon > 0$, returns a boolean indicating whether the set $\\{\\hat{\\phi}_i\\}$ is $M$-orthogonal within tolerance $\\epsilon$. The verification must be based solely on fundamental definitions: the $M$-inner product $\\langle x, y \\rangle_M := x^T M y$ and the definition of orthogonality $\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M = 0$ for $i \\neq j$. Your procedure must be invariant to nonzero rescaling of the columns of $\\hat{\\Phi}$ and must robustly detect invalid inputs such as zero columns (which violate the positive definiteness implication $\\hat{\\phi}_i^T M \\hat{\\phi}_i > 0$ for nonzero $\\hat{\\phi}_i$).\n\nSpecification of the verification procedure:\n- Inputs: $K \\in \\mathbb{R}^{n \\times n}$ with $K = K^T$, $M \\in \\mathbb{R}^{n \\times n}$ with $M = M^T \\succ 0$, $\\hat{\\Phi} \\in \\mathbb{R}^{n \\times m}$, and $\\epsilon \\in \\mathbb{R}$ with $\\epsilon > 0$.\n- Compute the Gram matrix $G := \\hat{\\Phi}^T M \\hat{\\Phi} \\in \\mathbb{R}^{m \\times m}$, with entries $g_{ij} = \\hat{\\phi}_i^T M \\hat{\\phi}_j$.\n- Let $d_i := g_{ii}$ for $i \\in \\{1,\\dots,m\\}$. If any $d_i \\le 0$, declare the set not $M$-orthogonal (return false).\n- Define the normalized correlation coefficients $c_{ij} := \\dfrac{g_{ij}}{\\sqrt{d_i d_j}}$ for $i \\ne j$. This normalization removes sensitivity to column scaling.\n- Let $\\eta := \\max_{i \\ne j} |c_{ij}|$. For the case $m = 1$, define $\\eta := 0$.\n- Return true if and only if $\\eta \\le \\epsilon$; otherwise return false.\n\nYour program must implement this verification as a pure function and apply it to the following deterministic test suite. In each test case, use the provided $M$, $\\hat{\\Phi}$, and $\\epsilon$. You may set $K$ to any symmetric matrix of compatible size; it will not be used by the verification but is included to maintain the problem’s context.\n\nTest suite:\n- Test $1$ (scale invariance, exact $M$-orthogonality):\n  - $M_1 = \\operatorname{diag}(4,3,2,5)$.\n  - $\\hat{\\Phi}_1 = [7 e_1, -2 e_2, 0.5 e_3]$, where $e_i$ denotes the $i$-th standard basis vector in $\\mathbb{R}^4$.\n  - $\\epsilon_1 = 10^{-12}$.\n  - Expected boolean: true.\n- Test $2$ (small violation, strict tolerance):\n  - $M_1$ as above.\n  - $\\hat{\\Phi}_2 = [e_1 + 10^{-3} e_2, e_2, e_3]$ in $\\mathbb{R}^4$.\n  - $\\epsilon_2 = 10^{-6}$.\n  - Expected boolean: false.\n- Test $3$ (same violation, looser tolerance):\n  - $M_1$ as above.\n  - $\\hat{\\Phi}_3 = [e_1 + 10^{-3} e_2, e_2, e_3]$ in $\\mathbb{R}^4$.\n  - $\\epsilon_3 = 10^{-2}$.\n  - Expected boolean: true.\n- Test $4$ (single vector, vacuous orthogonality):\n  - $M_2 = \\begin{bmatrix}2 & 0 & 0 \\\\ 0 & 5 & 1 \\\\ 0 & 1 & 3\\end{bmatrix}$.\n  - $\\hat{\\Phi}_4 = [[1,2,3]^T]$ (a single column in $\\mathbb{R}^3$).\n  - $\\epsilon_4 = 10^{-12}$.\n  - Expected boolean: true.\n- Test $5$ (non-orthogonal pair under $M$, degenerate subspace example):\n  - $M_3 = \\operatorname{diag}(2,2)$.\n  - $\\hat{\\Phi}_5 = [[1,0]^T, [1,1]^T]$ in $\\mathbb{R}^2$.\n  - $\\epsilon_5 = 0.5$.\n  - Expected boolean: false.\n- Test $6$ (invalid input with a zero column):\n  - $M_4 = \\operatorname{diag}(1,1)$.\n  - $\\hat{\\Phi}_6 = [[1,0]^T, [0,0]^T]$ in $\\mathbb{R}^2$.\n  - $\\epsilon_6 = 10^{-12}$.\n  - Expected boolean: false.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of Tests $1$ through $6$, for example, $[r_1,r_2,r_3,r_4,r_5,r_6]$, where each $r_i$ is a boolean value in the programming language’s canonical boolean literal format.", "solution": "The problem requires the design and implementation of a procedure to verify the numerical $M$-orthogonality of a set of vectors. This task is fundamental in the context of the finite element method, particularly in modal analysis where eigenvectors of the generalized eigenvalue problem $K \\phi = \\lambda M \\phi$ must be orthogonal with respect to the mass matrix $M$. We will first validate the problem statement and then construct the solution based on rigorous mathematical principles.\n\nThe problem is valid. It is scientifically grounded in established principles of numerical linear algebra, is well-posed with a clear and deterministic set of rules, and is expressed with objective, unambiguous language. It presents a standard, non-trivial verification task that requires careful implementation. All necessary data for the test cases are provided. We may therefore proceed with the solution.\n\nThe core of the problem lies in the definition of an inner product induced by a symmetric positive definite matrix $M \\in \\mathbb{R}^{n \\times n}$. For any two vectors $x, y \\in \\mathbb{R}^n$, the $M$-inner product is defined as:\n$$\n\\langle x, y \\rangle_M := x^T M y\n$$\nThe positive definiteness of $M$ ($M \\succ 0$) ensures that this definition satisfies all axioms of an inner product. Specifically, $\\langle x, x \\rangle_M = x^T M x > 0$ for all non-zero vectors $x$. The $M$-norm of a vector is then $\\|x\\|_M = \\sqrt{\\langle x, x \\rangle_M}$.\n\nA set of vectors $\\{\\hat{\\phi}_i\\}_{i=1}^m$ is said to be $M$-orthogonal if $\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M = 0$ for all $i \\neq j$. In numerical computation, results are subject to roundoff error, so we must verify this condition within a specified tolerance $\\epsilon$.\n\nThe specified verification procedure correctly formalizes this task. Let us analyze each step.\n\n1.  **Compute the Gram Matrix $G$**:\n    The procedure begins by computing the Gram matrix $G \\in \\mathbb{R}^{m \\times m}$ whose entries are the pairwise $M$-inner products of the columns of $\\hat{\\Phi} = [\\hat{\\phi}_1, \\dots, \\hat{\\phi}_m]$. The entry $g_{ij}$ is given by:\n    $$\n    g_{ij} = \\hat{\\phi}_i^T M \\hat{\\phi}_j = \\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M\n    $$\n    This can be expressed compactly in matrix notation as $G = \\hat{\\Phi}^T M \\hat{\\Phi}$. The diagonal entries are $g_{ii} = \\hat{\\phi}_i^T M \\hat{\\phi}_i = \\|\\hat{\\phi}_i\\|_M^2$, the squared $M$-norms of the vectors. The off-diagonal entries $g_{ij}$ ($i \\neq j$) are the quantities that should be zero for an $M$-orthogonal set.\n\n2.  **Validate Vector Norms**:\n    The next step is to examine the diagonal entries $d_i := g_{ii}$. The condition is to return `false` if any $d_i \\le 0$. This is a critical validity check. Since $M$ is positive definite, $\\hat{\\phi}_i^T M \\hat{\\phi}_i$ can only be zero if $\\hat{\\phi}_i$ is the zero vector, and it cannot be negative. The presence of a zero vector in a set intended to represent eigenvectors is an anomaly; such a vector cannot be part of an orthogonal basis. Therefore, any $\\hat{\\phi}_i$ for which $\\|\\hat{\\phi}_i\\|_M^2 \\le 0$ immediately invalidates the set for the purpose of orthogonality verification. This check also prevents division by zero in the subsequent normalization step.\n\n3.  **Normalize to Achieve Scale Invariance**:\n    Eigenvectors are defined only up to a non-zero scaling factor. A robust verification procedure must be invariant to such scaling. If we replace $\\hat{\\phi}_i$ with $\\alpha_i \\hat{\\phi}_i$ and $\\hat{\\phi}_j$ with $\\alpha_j \\hat{\\phi}_j$ (for $\\alpha_i, \\alpha_j \\neq 0$), the inner product becomes $g_{ij}' = \\alpha_i \\alpha_j g_{ij}$. The raw value of $g_{ij}$ is therefore not a reliable measure of orthogonality.\n    The procedure mandates the computation of normalized correlation coefficients:\n    $$\n    c_{ij} := \\frac{g_{ij}}{\\sqrt{d_i d_j}} = \\frac{\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M}{\\sqrt{\\|\\hat{\\phi}_i\\|_M^2 \\|\\hat{\\phi}_j\\|_M^2}} = \\frac{\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M}{\\|\\hat{\\phi}_i\\|_M \\|\\hat{\\phi}_j\\|_M}\n    $$\n    This quantity $c_{ij}$ is the cosine of the angle between vectors $\\hat{\\phi}_i$ and $\\hat{\\phi}_j$ in the Hilbert space endowed with the $M$-inner product. By the Cauchy-Schwarz inequality for this inner product, we have $|\\langle \\hat{\\phi}_i, \\hat{\\phi}_j \\rangle_M| \\le \\|\\hat{\\phi}_i\\|_M \\|\\hat{\\phi}_j\\|_M$, which implies $|c_{ij}| \\le 1$. The value of $c_{ij}$ is independent of the scaling of $\\hat{\\phi}_i$ and $\\hat{\\phi}_j$, thus satisfying the scale-invariance requirement. For perfect orthogonality, $c_{ij}=0$.\n\n4.  **Quantify Overall Deviation from Orthogonality**:\n    To assess the entire set, we must find the maximum deviation from orthogonality among all distinct pairs of vectors. This is captured by the metric $\\eta$:\n    $$\n    \\eta := \\max_{i \\neq j} |c_{ij}|\n    $$\n    This metric represents the worst-case \"orthogonality error\" in the set. For the special case of a single vector ($m \\le 1$), there are no distinct pairs, so the condition of orthogonality is vacuously satisfied. The procedure correctly defines $\\eta := 0$ in this case.\n\n5.  **Final Verdict**:\n    The final step is to compare the worst-case deviation $\\eta$ against the specified tolerance $\\epsilon > 0$. The set $\\{\\hat{\\phi}_i\\}$ is considered numerically $M$-orthogonal if $\\eta \\le \\epsilon$. This provides a clear, quantitative, and robust criterion for the verification.\n\nThis procedure is a complete and mathematically sound algorithm for the stated task. We will now implement it in code and apply it to the provided test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef is_m_orthogonal(M: np.ndarray, Phi: np.ndarray, epsilon: float) -> bool:\n    \"\"\"\n    Verifies if the columns of Phi are M-orthogonal within a given tolerance.\n\n    Args:\n        M (np.ndarray): A symmetric positive definite matrix (n x n).\n        Phi (np.ndarray): A matrix whose columns are the vectors to be checked (n x m).\n        epsilon (float): A positive tolerance for orthogonality.\n\n    Returns:\n        bool: True if the columns of Phi are M-orthogonal within tolerance, False otherwise.\n    \"\"\"\n    if not isinstance(M, np.ndarray) or not isinstance(Phi, np.ndarray):\n        raise TypeError(\"Inputs M and Phi must be numpy arrays.\")\n    \n    if M.ndim != 2 or Phi.ndim != 2:\n        raise ValueError(\"Inputs M and Phi must be 2D arrays.\")\n\n    n_m, n_m_check = M.shape\n    n_phi, m = Phi.shape\n\n    if n_m != n_m_check or n_m != n_phi:\n        raise ValueError(\"Matrix dimensions are not compatible for multiplication.\")\n\n    if epsilon <= 0:\n        raise ValueError(\"Tolerance epsilon must be positive.\")\n\n    # Per the specification, for m=1, eta is 0 if the vector is valid.\n    # We must still check for a zero vector.\n    if m <= 1:\n        if m == 0:\n            return True # An empty set of vectors is vacuously orthogonal.\n        \n        # Case m = 1\n        phi_1 = Phi[:, 0]\n        d1 = phi_1.T @ M @ phi_1\n        \n        # If d1 <= 0, the vector is a zero vector (since M is positive definite),\n        # which is invalid.\n        if d1 <= 0:\n            return False\n        \n        # For a single valid vector, orthogonality is vacuously true (eta=0).\n        # Since epsilon > 0, eta <= epsilon is always true.\n        return True\n\n    # Compute the Gram matrix: G = Phi^T * M * Phi\n    G = Phi.T @ M @ Phi\n\n    # Extract the diagonal entries d_i = g_ii\n    d = np.diagonal(G)\n\n    # Check for invalid vectors: d_i must be > 0.\n    # If any d_i <= 0, it implies a zero vector or M is not SPD.\n    if np.any(d <= 0):\n        return False\n\n    # Compute the normalized correlation coefficients c_ij = g_ij / sqrt(d_i * d_j)\n    # We do not need to compute the full matrix C. We only need the max of off-diagonals.\n    \n    # An efficient way to compute sqrt(d_i * d_j) matrix is via an outer product.\n    d_sqrt = np.sqrt(d)\n    denom_matrix = np.outer(d_sqrt, d_sqrt)\n    \n    # Avoid division by zero, though the check d <= 0 should already prevent this.\n    # Add a small machine epsilon for safety in floating point arithmetic.\n    denom_matrix[denom_matrix == 0] = np.finfo(float).eps\n    \n    C = np.abs(G / denom_matrix)\n    \n    # Set diagonal elements to 0 to only consider off-diagonal pairs (i != j)\n    np.fill_diagonal(C, 0)\n    \n    # Find the maximum absolute off-diagonal value\n    eta = np.max(C)\n    \n    return eta <= epsilon\n\ndef solve():\n    \"\"\"\n    Runs the deterministic test suite for the M-orthogonality verification procedure.\n    \"\"\"\n    # Define standard basis vectors for convenience\n    e1_4d = np.array([1., 0., 0., 0.])\n    e2_4d = np.array([0., 1., 0., 0.])\n    e3_4d = np.array([0., 0., 1., 0.])\n\n    # Test Case 1: Scale invariance, exact M-orthogonality\n    M1 = np.diag([4., 3., 2., 5.])\n    Phi1 = np.array([7. * e1_4d, -2. * e2_4d, 0.5 * e3_4d]).T\n    epsilon1 = 1e-12\n\n    # Test Case 2: Small violation, strict tolerance\n    Phi2 = np.array([e1_4d + 1e-3 * e2_4d, e2_4d, e3_4d]).T\n    epsilon2 = 1e-6\n\n    # Test Case 3: Same violation, looser tolerance\n    Phi3 = np.copy(Phi2)\n    epsilon3 = 1e-2\n\n    # Test Case 4: Single vector, vacuous orthogonality\n    M2 = np.array([[2., 0., 0.], [0., 5., 1.], [0., 1., 3.]])\n    Phi4 = np.array([[1., 2., 3.]]).T\n    epsilon4 = 1e-12\n\n    # Test Case 5: Non-orthogonal pair under M\n    M3 = np.diag([2., 2.])\n    Phi5 = np.array([[1., 0.], [1., 1.]]).T\n    epsilon5 = 0.5\n\n    # Test Case 6: Invalid input with a zero column\n    M4 = np.diag([1., 1.])\n    Phi6 = np.array([[1., 0.], [0., 0.]]).T\n    epsilon6 = 1e-12\n\n    test_cases = [\n        (M1, Phi1, epsilon1),\n        (M1, Phi2, epsilon2),\n        (M1, Phi3, epsilon3),\n        (M2, Phi4, epsilon4),\n        (M3, Phi5, epsilon5),\n        (M4, Phi6, epsilon6),\n    ]\n\n    results = []\n    for M, Phi, epsilon in test_cases:\n        # K is not needed for the verification procedure L-:)\n        result = is_m_orthogonal(M, Phi, epsilon)\n        results.append(result)\n        \n    # Format the output as a comma-separated list of lowercase boolean literals.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "2578532"}, {"introduction": "This practice advances to the important and often challenging scenario of repeated eigenvalues, where any linear combination of eigenvectors is also an eigenvector ([@problem_id:2578529]). You will develop a comprehensive verification scheme that not only checks for $M$-orthonormality but also confirms that the computed vectors correctly span the entire invariant subspace, a crucial step in validating models of symmetric structures.", "problem": "You are given a symmetric matrix pencil described by a pair of real matrices $K \\in \\mathbb{R}^{n \\times n}$ and $M \\in \\mathbb{R}^{n \\times n}$ where $K$ is symmetric and $M$ is symmetric positive definite. Consider the generalized eigenvalue problem $K x = \\lambda M x$. Suppose there is a double eigenvalue $\\lambda_0$ whose eigenspace has dimension $2$ and is invariant under the pencil. A pair of column vectors assembled as $U \\in \\mathbb{R}^{n \\times 2}$ is claimed to represent a basis of this eigenspace. In the Finite Element Method, modal orthogonality and normalization are expressed with respect to the $M$-inner product. The goal is to verify, using first principles, whether the provided $U$ both spans the correct invariant subspace associated with $\\lambda_0$ and is orthonormal with respect to the $M$-inner product.\n\nStarting from the fundamental definitions:\n- The invariant subspace associated with $\\lambda_0$ is the set of vectors $x$ satisfying $(K - \\lambda_0 M)x = 0$. For a basis matrix $U \\in \\mathbb{R}^{n \\times 2}$ to span this subspace, each column must satisfy the same relation in the ideal case. Numerically, a small residual in an appropriate relative norm is acceptable.\n- $M$-orthonormality of the columns of $U$ means $U^T M U = I_2$, where $I_2$ is the identity matrix of size $2$.\n\nYour task is to write a program that, for each provided test case, returns a boolean indicating whether both of the following numerical checks pass:\n1. $M$-orthonormality: The Frobenius norm of $U^T M U - I_2$ is less than or equal to a prescribed tolerance $\\tau_{\\mathrm{orth}}$.\n2. Invariant subspace at $\\lambda_0$: The relative Frobenius-norm residual \n$$\n\\rho = \\frac{\\lVert (K - \\lambda_0 M) U \\rVert_F}{\\left(\\lVert K \\rVert_F + |\\lambda_0| \\lVert M \\rVert_F\\right)\\lVert U \\rVert_F}\n$$\nis less than or equal to a prescribed tolerance $\\tau_{\\mathrm{inv}}$.\n\nUse the following numerical tolerances for all test cases: $\\tau_{\\mathrm{orth}} = 10^{-10}$ and $\\tau_{\\mathrm{inv}} = 10^{-8}$.\n\nThe test suite consists of five cases built from a common pencil with $n = 5$ and an exactly double eigenvalue. The pencil is constructed as follows:\n- Define $M = \\mathrm{diag}(m_1, m_2, m_3, m_4, m_5)$ with $m_1 = 1.5$, $m_2 = 2.5$, $m_3 = 3.25$, $m_4 = 4.0$, $m_5 = 6.0$.\n- Let $Q \\in \\mathbb{R}^{5 \\times 5}$ be an orthogonal matrix and set the exact eigenvalues $\\Lambda = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_5)$ with $\\lambda_1 = \\lambda_2 = 2.0$, $\\lambda_3 = 5.0$, $\\lambda_4 = 7.0$, $\\lambda_5 = 11.0$. Let $M^{1/2} = \\mathrm{diag}(\\sqrt{m_1}, \\ldots, \\sqrt{m_5})$ and define $K = M^{1/2}Q\\Lambda Q^T M^{1/2}$. Then $(K,M)$ is a symmetric pencil with the desired spectrum.\n- The exact $M$-orthonormal eigenvector matrix is $X = M^{-1/2}Q$, satisfying $X^T M X = I_5$ and $K X = M X \\Lambda$. The eigenspace of the double eigenvalue is spanned by the first two columns of $X$.\n\nFor all five tests, take $\\lambda_0 = 2.0$. Construct the five candidate bases $U$ as follows:\n- Test $1$ (happy path): $U = U_\\star R$ where $U_\\star = X[:,0:2]$ and $R$ is a $2 \\times 2$ rotation with angle $\\theta = 0.4$ radians, i.e., $R = \\begin{bmatrix}\\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta\\end{bmatrix}$. This should be $M$-orthonormal and invariant.\n- Test $2$ (correct subspace, not orthonormal): $U = U_\\star C$ with $C = \\begin{bmatrix}1.5 & 0.3 \\\\ 0 & 0.6\\end{bmatrix}$. This spans the correct subspace but violates $M$-orthonormality.\n- Test $3$ ($M$-orthonormal but wrong subspace): $U = [X[:,0], X[:,2]]$, i.e., one vector from the double eigenspace and one from a different eigenvalue. This is $M$-orthonormal but not invariant at $\\lambda_0$.\n- Test $4$ (nearby contamination that should still pass): $U$ is obtained by perturbing $U_\\star$ as $U_{\\mathrm{tmp}} = U_\\star + \\varepsilon X[:,2:4]$, with $\\varepsilon = 10^{-11}$, followed by exact $M$-orthonormalization of the two columns of $U_{\\mathrm{tmp}}$ using a Cholesky factorization of $U_{\\mathrm{tmp}}^T M U_{\\mathrm{tmp}}$; the resulting $U$ should be invariant at $\\lambda_0$ within the tolerance and $M$-orthonormal.\n- Test $5$ (rank deficiency): $U = [X[:,0], X[:,0] + 10^{-14}X[:,1]]$. This fails $M$-orthonormality.\n\nYour program must:\n- Build $M$, $Q$, $\\Lambda$, $K$, and $X$ as specified.\n- Construct each $U$ for the five tests.\n- For each $U$, evaluate the two checks above with $\\tau_{\\mathrm{orth}} = 10^{-10}$ and $\\tau_{\\mathrm{inv}} = 10^{-8}$, producing a boolean result that is $\\mathrm{True}$ if and only if both checks pass, and $\\mathrm{False}$ otherwise.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is a boolean literal. No extra text should be printed.", "solution": "The problem statement is parsed and validated. It is found to be scientifically sound, well-posed, and an unambiguous exercise in numerical linear algebra, specifically concerning the verification of eigenspaces for a generalized eigenvalue problem. The problem is valid, and a solution will be provided.\n\nThe core of the problem is to verify if a given matrix $U \\in \\mathbb{R}^{n \\times 2}$ represents an $M$-orthonormal basis for a two-dimensional invariant subspace associated with a double eigenvalue $\\lambda_0$ of the symmetric matrix pencil $(K, M)$. The governing equation is the generalized eigenvalue problem $Kx = \\lambda M x$, where $K$ is a symmetric matrix and $M$ is a symmetric positive definite matrix.\n\nThe properties of the solutions to this problem are fundamental. The eigenvalues $\\lambda_i$ are real, and the corresponding eigenvectors $x_i$ can be chosen to be $M$-orthogonal, meaning $x_i^T M x_j = 0$ for $\\lambda_i \\neq \\lambda_j$. If an eigenvector $x_i$ is normalized such that $x_i^T M x_i = 1$, the set of all such eigenvectors forms an $M$-orthonormal basis.\n\nFor a candidate basis $U$, two conditions must be met: it must span the correct invariant subspace, and it must be orthonormal with respect to the $M$-inner product. These two conditions are translated into numerical checks.\n\nFirst, the condition of $M$-orthonormality for the columns of $U$ is expressed as:\n$$\nU^T M U = I_2\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix. In numerical computation, we do not expect exact equality. We verify this condition by checking if the norm of the deviation from identity is within a specified tolerance $\\tau_{\\mathrm{orth}}$. The Frobenius norm is a suitable choice for this matrix norm. The check is:\n$$\n\\lVert U^T M U - I_2 \\rVert_F \\le \\tau_{\\mathrm{orth}}\n$$\n\nSecond, for $U$ to be a basis for the invariant subspace (eigenspace) corresponding to the eigenvalue $\\lambda_0$, every vector in the column space of $U$ must be an eigenvector for $\\lambda_0$. This is equivalent to requiring that each column of $U$ satisfies the eigenvalue equation, which can be written compactly as:\n$$\n(K - \\lambda_0 M)U = 0\n$$\nThe matrix $(K - \\lambda_0 M)U$ is the residual matrix. Again, due to finite precision arithmetic and potential small perturbations in $U$, this residual will not be exactly zero. We measure its magnitude using a relative residual to avoid issues with the overall scaling of the problem matrices $K$ and $M$. The relative Frobenius-norm residual $\\rho$ is defined as:\n$$\n\\rho = \\frac{\\lVert (K - \\lambda_0 M) U \\rVert_F}{\\left(\\lVert K \\rVert_F + |\\lambda_0| \\lVert M \\rVert_F\\right)\\lVert U \\rVert_F}\n$$\nThis relative residual must be less than or equal to a prescribed tolerance $\\tau_{\\mathrm{inv}}$. The denominator provides a natural scale for the problem. A candidate basis $U$ is considered valid if and only if both numerical checks pass.\n\nTo conduct the tests, we first construct the specified matrix pencil. The mass matrix $M$ is given as a diagonal matrix of size $n=5$:\n$$\nM = \\mathrm{diag}(1.5, 2.5, 3.25, 4.0, 6.0)\n$$\nThe eigenvalues are specified as $\\Lambda = \\mathrm{diag}(2.0, 2.0, 5.0, 7.0, 11.0)$. A fixed orthogonal matrix $Q \\in \\mathbb{R}^{5 \\times 5}$ is required. For reproducibility, we generate $Q$ by performing a QR decomposition on a fixed random matrix. With $M^{1/2} = \\mathrm{diag}(\\sqrt{m_i})$, the stiffness matrix $K$ is constructed as $K = M^{1/2}Q\\Lambda Q^T M^{1/2}$. This construction guarantees that $(K, M)$ is a symmetric pencil with the prescribed eigenvalues $\\Lambda$. The matrix of exact $M$-orthonormal eigenvectors is correspondingly given by $X = M^{-1/2}Q$. The eigenspace for the double eigenvalue $\\lambda_0 = 2.0$ is spanned by the first two columns of $X$, which we denote as $U_\\star = X[:,0:2]$.\n\nThe five test cases are constructed as follows:\n1.  **Test 1**: A rotation is applied to the true basis: $U_1 = U_\\star R$, where $R$ is a $2 \\times 2$ rotation matrix. Since rotation matrices are orthogonal, $U_1$ remains $M$-orthonormal: $U_1^T M U_1 = R^T U_\\star^T M U_\\star R = R^T I_2 R = I_2$. The subspace is unchanged. Both checks should pass.\n2.  **Test 2**: A non-orthogonal linear combination of the true basis vectors: $U_2 = U_\\star C$. This $U_2$ still spans the correct subspace, so the invariant subspace check should pass. However, $U_2^T M U_2 = C^T C \\neq I_2$, so the orthonormality check should fail.\n3.  **Test 3**: A basis constructed from eigenvectors of different eigenvalues: $U_3 = [x_1, x_3]$. By construction, this basis is $M$-orthonormal. However, the vector $x_3$ is not in the eigenspace of $\\lambda_0 = 2.0$ but of $\\lambda_3 = 5.0$. Thus, $(K - \\lambda_0 M)x_3 = (\\lambda_3 - \\lambda_0)M x_3 \\neq 0$. The invariant subspace check will fail.\n4.  **Test 4**: A slightly perturbed basis, which is then re-orthonormalized. The initial perturbation $U_{\\mathrm{tmp}} = U_\\star + \\varepsilon X[:,2:4]$ contaminates the basis with vectors from other eigenspaces. The subsequent $M$-orthonormalization, performed via a Cholesky decomposition of $U_{\\mathrm{tmp}}^T M U_{\\mathrm{tmp}}$, produces a basis $U_4$ that is perfectly $M$-orthonormal by construction. Because the initial perturbation $\\varepsilon = 10^{-11}$ is very small, the resulting subspace is extremely close to the true one, and the relative residual should be well within the tolerance $\\tau_{\\mathrm{inv}} = 10^{-8}$. Both checks should pass.\n5.  **Test 5**: A nearly rank-deficient basis, $U_5 = [x_1, x_1 + 10^{-14}x_2]$. The two columns are nearly parallel. The Gram matrix $U_5^T M U_5$ will be close to $\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$, which is far from the identity matrix. The orthonormality check is expected to fail.\n\nThe algorithm proceeds by first setting up the matrices $K$ and $M$. Then, for each of the five test cases, the candidate matrix $U$ is constructed. Finally, the two verification checks are performed using the given tolerances $\\tau_{\\mathrm{orth}} = 10^{-10}$ and $\\tau_{\\mathrm{inv}} = 10^{-8}$. A boolean result is recorded for each case, which is `True` if and only if both checks are satisfied.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates candidate bases for a generalized eigenvalue problem.\n    \"\"\"\n    # Tolerances and constants from the problem statement\n    tau_orth = 1e-10\n    tau_inv = 1e-8\n    lam0 = 2.0\n    n = 5\n\n    # --- Step 1: Construct the matrix pencil (K, M) and exact solution X ---\n\n    # Mass matrix M\n    m_diag = np.array([1.5, 2.5, 3.25, 4.0, 6.0])\n    M = np.diag(m_diag)\n\n    # Eigenvalue matrix Lambda\n    lambda_diag = np.array([2.0, 2.0, 5.0, 7.0, 11.0])\n    Lambda = np.diag(lambda_diag)\n\n    # Create a reproducible orthogonal matrix Q\n    np.random.seed(0)\n    A = np.random.rand(n, n)\n    Q, _ = np.linalg.qr(A)\n\n    # Construct K and the exact eigenvector matrix X\n    M_sqrt = np.diag(np.sqrt(m_diag))\n    M_inv_sqrt = np.diag(1.0 / np.sqrt(m_diag))\n    K = M_sqrt @ Q @ Lambda @ Q.T @ M_sqrt\n    X = M_inv_sqrt @ Q\n\n    # True basis for the eigenspace of lambda_0 = 2.0\n    U_star = X[:, 0:2]\n\n    def check_validity(U, K, M, lam0, tau_orth, tau_inv):\n        \"\"\"\n        Performs the two checks: M-orthonormality and invariant subspace.\n        \"\"\"\n        # 1. M-orthonormality check\n        I2 = np.eye(2)\n        orth_err = np.linalg.norm(U.T @ M @ U - I2, 'fro')\n        orth_pass = orth_err <= tau_orth\n\n        # 2. Invariant subspace check\n        residual_matrix = (K - lam0 * M) @ U\n        # Denominator can be zero only if K, M, and U are all zero, which is not the case here\n        denominator = (np.linalg.norm(K, 'fro') + np.abs(lam0) * np.linalg.norm(M, 'fro')) * np.linalg.norm(U, 'fro')\n        \n        # Handle potential division by zero, although highly unlikely\n        if denominator == 0:\n            inv_err = np.linalg.norm(residual_matrix, 'fro')\n        else:\n            inv_err = np.linalg.norm(residual_matrix, 'fro') / denominator\n\n        inv_pass = inv_err <= tau_inv\n\n        return orth_pass and inv_pass\n\n    # --- Step 2: Define and process the five test cases ---\n\n    test_cases = []\n\n    # Test 1: Happy path (rotated true basis)\n    theta = 0.4\n    R = np.array([[np.cos(theta), -np.sin(theta)],\n                  [np.sin(theta), np.cos(theta)]])\n    U1 = U_star @ R\n    test_cases.append(U1)\n\n    # Test 2: Correct subspace, not orthonormal\n    C = np.array([[1.5, 0.3], [0, 0.6]])\n    U2 = U_star @ C\n    test_cases.append(U2)\n    \n    # Test 3: M-orthonormal but wrong subspace\n    U3 = np.hstack((X[:, 0:1], X[:, 2:3]))\n    test_cases.append(U3)\n\n    # Test 4: Contaminated and re-orthonormalized\n    epsilon = 1e-11\n    U_tmp = U_star + epsilon * X[:, 2:4]\n    # M-orthonormalization using Cholesky factorization of the Gram matrix\n    G = U_tmp.T @ M @ U_tmp\n    L = np.linalg.cholesky(G)\n    # U = U_tmp * inv(L.T)\n    L_inv_T = np.linalg.inv(L).T\n    U4 = U_tmp @ L_inv_T\n    test_cases.append(U4)\n\n    # Test 5: Rank deficiency (nearly linearly dependent columns)\n    U5 = np.hstack((X[:, 0:1], (X[:, 0] + 1e-14 * X[:, 1]).reshape(-1, 1)))\n    test_cases.append(U5)\n\n    # --- Step 3: Run checks and collect results ---\n\n    results = []\n    for U in test_cases:\n        is_valid = check_validity(U, K, M, lam0, tau_orth, tau_inv)\n        results.append(is_valid)\n\n    # --- Step 4: Print final output in the required format ---\n    # Python's str() for a boolean gives 'True' or 'False'\n    # The problem asks for lowercase boolean literals, e.g., 'true'\n    result_str = ','.join(str(r).lower() for r in results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "2578529"}]}