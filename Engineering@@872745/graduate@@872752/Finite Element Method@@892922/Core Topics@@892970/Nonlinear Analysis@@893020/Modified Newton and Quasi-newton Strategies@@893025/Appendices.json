{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice involves a direct, step-by-step comparison between the full Newton-Raphson method and its simplest variant, the Modified Newton method. By manually performing the iterations for a small nonlinear system, you will gain a tangible understanding of their different convergence characteristics. This exercise [@problem_id:2580609] illuminates the core trade-off between the computational expense of re-evaluating the tangent matrix at every step and the slower, linear convergence that results from using a fixed, \"frozen\" tangent.", "problem": "Consider the nonlinear balance equations arising from a reduced two-degree-of-freedom finite element (FE) system, where the residual vector $\\boldsymbol{R}(\\boldsymbol{u}) \\in \\mathbb{R}^{2}$ is defined componentwise by\n$$\nR_{1}(\\boldsymbol{u}) \\equiv 2\\,u_{1} + \\tfrac{1}{2}\\,u_{1}^{2} + 0.1\\,u_{2}^{2} - 1.2, \\quad\nR_{2}(\\boldsymbol{u}) \\equiv -0.3\\,u_{1} + 1.5\\,u_{2} + 0.2\\,u_{1}\\,u_{2} + 0.1\\,u_{2}^{3} - 0.9,\n$$\nwith the initial guess $\\boldsymbol{u}_{0} = (0,0)^{\\mathsf{T}}$. The consistent tangent (Jacobian) matrix $\\boldsymbol{K}(\\boldsymbol{u}) = \\partial \\boldsymbol{R}/\\partial \\boldsymbol{u}$ is\n$$\n\\boldsymbol{K}(\\boldsymbol{u}) = \n\\begin{pmatrix}\n2 + u_{1}  0.2\\,u_{2} \\\\\n-0.3 + 0.2\\,u_{2}  1.5 + 0.2\\,u_{1} + 0.3\\,u_{2}^{2}\n\\end{pmatrix},\n$$\nand at the initial guess one has\n$$\n\\boldsymbol{K}(\\boldsymbol{u}_{0}) = \n\\begin{pmatrix}\n2  0 \\\\\n-0.3  1.5\n\\end{pmatrix}.\n$$\nStarting from $\\boldsymbol{u}_{0}$, perform two iterations for each of the following strategies:\n- Full Newton: $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} - \\boldsymbol{K}(\\boldsymbol{u}_{k})^{-1}\\,\\boldsymbol{R}(\\boldsymbol{u}_{k})$.\n- Modified Newton (a quasi-Newton strategy with frozen tangent): $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} - \\boldsymbol{K}(\\boldsymbol{u}_{0})^{-1}\\,\\boldsymbol{R}(\\boldsymbol{u}_{k})$.\n\nFor each strategy, define the Euclidean residual reduction factor at iteration $k$ as\n$$\n\\rho_{k} \\equiv \\frac{\\|\\boldsymbol{R}(\\boldsymbol{u}_{k+1})\\|_{2}}{\\|\\boldsymbol{R}(\\boldsymbol{u}_{k})\\|_{2}},\n$$\nwhere $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. Derive the iterations from the Newton linearization principle and compute all residuals and factors from first principles (no line search or damping).\n\nReport, as your final answer, the second-iteration residual reduction factor $\\rho_{1}$ for the modified Newton method. Round your final answer to four significant figures. No physical units are required.", "solution": "The problem requires the calculation of the residual reduction factor $\\rho_{1}$ for the modified Newton method after two iterations. The general principle for solving a nonlinear system of equations $\\boldsymbol{R}(\\boldsymbol{u}) = \\boldsymbol{0}$ via a Newton-type method is based on a first-order Taylor series expansion of the residual vector $\\boldsymbol{R}$ around the current iterate $\\boldsymbol{u}_{k}$:\n$$\n\\boldsymbol{R}(\\boldsymbol{u}_{k+1}) \\approx \\boldsymbol{R}(\\boldsymbol{u}_{k}) + \\frac{\\partial \\boldsymbol{R}}{\\partial \\boldsymbol{u}}\\bigg|_{\\boldsymbol{u}_{k}} (\\boldsymbol{u}_{k+1} - \\boldsymbol{u}_{k})\n$$\nSetting $\\boldsymbol{R}(\\boldsymbol{u}_{k+1})$ to $\\boldsymbol{0}$ and defining the tangent matrix $\\boldsymbol{K}(\\boldsymbol{u}_{k}) = \\frac{\\partial \\boldsymbol{R}}{\\partial \\boldsymbol{u}}\\big|_{\\boldsymbol{u}_{k}}$ and the displacement increment $\\Delta \\boldsymbol{u}_{k} = \\boldsymbol{u}_{k+1} - \\boldsymbol{u}_{k}$, we obtain the linear system for the increment:\n$$\n\\boldsymbol{K}(\\boldsymbol{u}_{k}) \\Delta \\boldsymbol{u}_{k} = -\\boldsymbol{R}(\\boldsymbol{u}_{k})\n$$\nThe next iterate is then $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} + \\Delta \\boldsymbol{u}_{k}$.\nFor the full Newton method, the tangent matrix $\\boldsymbol{K}(\\boldsymbol{u}_{k})$ is re-evaluated at each iteration $k$. For the modified Newton method, the tangent matrix is computed only once at the initial guess $\\boldsymbol{u}_{0}$ and then held constant (frozen) for all subsequent iterations. Thus, the iterative update is $\\boldsymbol{u}_{k+1} = \\boldsymbol{u}_{k} - \\boldsymbol{K}(\\boldsymbol{u}_{0})^{-1}\\boldsymbol{R}(\\boldsymbol{u}_{k})$.\n\nWe begin with the initial guess $\\boldsymbol{u}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Initial State ($k=0$)**\n\nFirst, compute the initial residual vector $\\boldsymbol{R}(\\boldsymbol{u}_{0})$:\n$$\nR_{1}(\\boldsymbol{u}_{0}) = 2(0) + \\frac{1}{2}(0)^{2} + 0.1(0)^{2} - 1.2 = -1.2\n$$\n$$\nR_{2}(\\boldsymbol{u}_{0}) = -0.3(0) + 1.5(0) + 0.2(0)(0) + 0.1(0)^{3} - 0.9 = -0.9\n$$\nSo, $\\boldsymbol{R}(\\boldsymbol{u}_{0}) = \\begin{pmatrix} -1.2 \\\\ -0.9 \\end{pmatrix}$.\nThe Euclidean norm of the initial residual is:\n$$\n\\|\\boldsymbol{R}(\\boldsymbol{u}_{0})\\|_{2} = \\sqrt{(-1.2)^{2} + (-0.9)^{2}} = \\sqrt{1.44 + 0.81} = \\sqrt{2.25} = 1.5\n$$\nThe tangent matrix at the initial guess is given as:\n$$\n\\boldsymbol{K}_{0} = \\boldsymbol{K}(\\boldsymbol{u}_{0}) = \\begin{pmatrix} 2  0 \\\\ -0.3  1.5 \\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(\\boldsymbol{K}_{0}) = (2)(1.5) - (0)(-0.3) = 3$. The inverse is:\n$$\n\\boldsymbol{K}_{0}^{-1} = \\frac{1}{3} \\begin{pmatrix} 1.5  0 \\\\ 0.3  2 \\end{pmatrix} = \\begin{pmatrix} 0.5  0 \\\\ 0.1  \\frac{2}{3} \\end{pmatrix}\n$$\n\n**First Iteration ($k=0$)**\n\nThis iteration is identical for both full and modified Newton methods. We solve $\\boldsymbol{K}_{0} \\Delta \\boldsymbol{u}_{0} = -\\boldsymbol{R}(\\boldsymbol{u}_{0})$.\n$$\n\\Delta \\boldsymbol{u}_{0} = -\\boldsymbol{K}_{0}^{-1} \\boldsymbol{R}(\\boldsymbol{u}_{0}) = -\\begin{pmatrix} 0.5  0 \\\\ 0.1  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} -1.2 \\\\ -0.9 \\end{pmatrix} = -\\begin{pmatrix} 0.5(-1.2) \\\\ 0.1(-1.2) + \\frac{2}{3}(-0.9) \\end{pmatrix} = -\\begin{pmatrix} -0.6 \\\\ -0.12 - 0.6 \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix}\n$$\nThe first updated displacement vector is:\n$$\n\\boldsymbol{u}_{1} = \\boldsymbol{u}_{0} + \\Delta \\boldsymbol{u}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix}\n$$\nNext, we compute the residual at $\\boldsymbol{u}_{1}$:\n$$\nR_{1}(\\boldsymbol{u}_{1}) = 2(0.6) + \\frac{1}{2}(0.6)^{2} + 0.1(0.72)^{2} - 1.2 = 1.2 + 0.5(0.36) + 0.1(0.5184) - 1.2 = 0.18 + 0.05184 = 0.23184\n$$\n$$\nR_{2}(\\boldsymbol{u}_{1}) = -0.3(0.6) + 1.5(0.72) + 0.2(0.6)(0.72) + 0.1(0.72)^{3} - 0.9 = -0.18 + 1.08 + 0.0864 + 0.1(0.373248) - 0.9 = 0.0864 + 0.0373248 = 0.1237248\n$$\nThus, $\\boldsymbol{R}(\\boldsymbol{u}_{1}) = \\begin{pmatrix} 0.23184 \\\\ 0.1237248 \\end{pmatrix}$. The norm of this residual is:\n$$\n\\|\\boldsymbol{R}(\\boldsymbol{u}_{1})\\|_{2} = \\sqrt{(0.23184)^{2} + (0.1237248)^{2}} = \\sqrt{0.0537498256 + 0.01530782611584} = \\sqrt{0.06905765171584} \\approx 0.2627885994\n$$\nThis value serves as the denominator for the reduction factor $\\rho_{1}$.\n\n**Second Iteration ($k=1$) - Modified Newton Method**\n\nFor the modified Newton method, we use the frozen tangent $\\boldsymbol{K}_{0}$. We solve $\\boldsymbol{K}_{0} \\Delta \\boldsymbol{u}_{1} = -\\boldsymbol{R}(\\boldsymbol{u}_{1})$.\n$$\n\\Delta \\boldsymbol{u}_{1} = -\\boldsymbol{K}_{0}^{-1} \\boldsymbol{R}(\\boldsymbol{u}_{1}) = -\\begin{pmatrix} 0.5  0 \\\\ 0.1  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 0.23184 \\\\ 0.1237248 \\end{pmatrix} = -\\begin{pmatrix} 0.5(0.23184) \\\\ 0.1(0.23184) + \\frac{2}{3}(0.1237248) \\end{pmatrix} = -\\begin{pmatrix} 0.11592 \\\\ 0.023184 + 0.0824832 \\end{pmatrix} = \\begin{pmatrix} -0.11592 \\\\ -0.1056672 \\end{pmatrix}\n$$\nThe second updated displacement vector is:\n$$\n\\boldsymbol{u}_{2} = \\boldsymbol{u}_{1} + \\Delta \\boldsymbol{u}_{1} = \\begin{pmatrix} 0.6 \\\\ 0.72 \\end{pmatrix} + \\begin{pmatrix} -0.11592 \\\\ -0.1056672 \\end{pmatrix} = \\begin{pmatrix} 0.48408 \\\\ 0.6143328 \\end{pmatrix}\n$$\nNow we compute the residual at $\\boldsymbol{u}_{2}$:\n$$\nR_{1}(\\boldsymbol{u}_{2}) = 2(0.48408) + \\frac{1}{2}(0.48408)^{2} + 0.1(0.6143328)^{2} - 1.2\n$$\n$$\nR_{1}(\\boldsymbol{u}_{2}) = 0.96816 + 0.5(0.2343334464) + 0.1(0.3774048590378304) - 1.2 = 0.96816 + 0.1171667232 + 0.0377404859 - 1.2 \\approx -0.0769327909\n$$\n$$\nR_{2}(\\boldsymbol{u}_{2}) = -0.3(0.48408) + 1.5(0.6143328) + 0.2(0.48408)(0.6143328) + 0.1(0.6143328)^{3} - 0.9\n$$\n$$\nR_{2}(\\boldsymbol{u}_{2}) = -0.145224 + 0.9214992 + 0.2(0.297440535936) + 0.1(0.231846067341235...) - 0.9 \\approx -0.145224 + 0.9214992 + 0.0594881072 + 0.0231846067 - 0.9 \\approx -0.0410520861\n$$\nThe residual vector is $\\boldsymbol{R}(\\boldsymbol{u}_{2}) \\approx \\begin{pmatrix} -0.07693279 \\\\ -0.04105209 \\end{pmatrix}$. Its norm is:\n$$\n\\|\\boldsymbol{R}(\\boldsymbol{u}_{2})\\|_{2} \\approx \\sqrt{(-0.07693279)^{2} + (-0.04105209)^{2}} = \\sqrt{0.0059186546 + 0.0016852731} = \\sqrt{0.0076039277} \\approx 0.0872005029\n$$\nFinally, the residual reduction factor at the second iteration ($k=1$) is defined as:\n$$\n\\rho_{1} = \\frac{\\|\\boldsymbol{R}(\\boldsymbol{u}_{2})\\|_{2}}{\\|\\boldsymbol{R}(\\boldsymbol{u}_{1})\\|_{2}} \\approx \\frac{0.0872005029}{0.2627885994} \\approx 0.33182046\n$$\nRounding to four significant figures, we get $0.3318$.", "answer": "$$ \\boxed{0.3318} $$", "id": "2580609"}, {"introduction": "Moving beyond the simple \"frozen\" tangent, our next exercise explores a more sophisticated and powerful quasi-Newton strategy: the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. This practice [@problem_id:2580619] requires you to manually apply the BFGS update formula, using information from previous steps to progressively build a better approximation of the system's tangent stiffness. This demystifies the \"secant update\" and provides insight into how these algorithms intelligently approximate curvature to achieve superlinear convergence without ever forming the true Hessian matrix.", "problem": "Consider a two-degree-of-freedom nonlinear finite element system derived from the stationarity of the total potential energy. Let the total potential energy be\n$$\n\\Pi(u_1,u_2) \\;=\\; u_1^2 + u_2^2 + u_1 u_2 + u_1^3 + u_2^3 + \\frac{1}{2} u_1^2 u_2 \\;-\\; f_1 u_1 \\;-\\; f_2 u_2,\n$$\nwith load vector components specified as $f_1 = 1$ and $f_2 = 1$. The residual (internal force minus external force) is the gradient of $\\Pi$, namely $r(u) = \\nabla \\Pi(u)$, and the consistent tangent (Jacobian) is the Hessian $K(u) = \\nabla^2 \\Pi(u)$.\n\nYou are given three trial displacement states\n$$\nu_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad\nu_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\nu_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nDefine $s_k = u_{k+1} - u_k$ and $y_k = r(u_{k+1}) - r(u_k)$. Starting from an initial symmetric positive definite approximation $B_0 = \\gamma I$ with $\\gamma = 2$, perform two successive Broyden-Fletcher-Goldfarb-Shanno (BFGS) updates for the Hessian approximation to obtain $B_2$ using the pairs $(s_0,y_0)$ and $(s_1,y_1)$.\n\nAt the state $u_2$, form:\n- the quasi-Newton predicted step $s_{\\mathrm{QN}} = -B_2^{-1} r(u_2)$,\n- the exact Newton step $s_{\\mathrm{N}} = -K(u_2)^{-1} r(u_2)$.\n\nCompute the Euclidean norm of the difference $\\| s_{\\mathrm{QN}} - s_{\\mathrm{N}} \\|_2$. Provide your final answer as a single real number rounded to four significant figures. No units are required.", "solution": "First, we substitute the loads $f_1 = 1$ and $f_2 = 1$ into the potential energy functional:\n$$\n\\Pi(u_1, u_2) = u_1^2 + u_2^2 + u_1 u_2 + u_1^3 + u_2^3 + \\frac{1}{2} u_1^2 u_2 - u_1 - u_2\n$$\nThe residual vector, $r(u)$, is the gradient of $\\Pi(u)$:\n$$\nr(u) = \\nabla \\Pi(u) = \\begin{pmatrix} \\frac{\\partial \\Pi}{\\partial u_1} \\\\ \\frac{\\partial \\Pi}{\\partial u_2} \\end{pmatrix} = \\begin{pmatrix} 2u_1 + u_2 + 3u_1^2 + u_1 u_2 - 1 \\\\ 2u_2 + u_1 + 3u_2^2 + \\frac{1}{2}u_1^2 - 1 \\end{pmatrix}\n$$\nThe tangent stiffness matrix, $K(u)$, is the Hessian of $\\Pi(u)$:\n$$\nK(u) = \\nabla^2 \\Pi(u) = \\begin{pmatrix} \\frac{\\partial^2 \\Pi}{\\partial u_1^2}  \\frac{\\partial^2 \\Pi}{\\partial u_1 \\partial u_2} \\\\ \\frac{\\partial^2 \\Pi}{\\partial u_2 \\partial u_1}  \\frac{\\partial^2 \\Pi}{\\partial u_2^2} \\end{pmatrix} = \\begin{pmatrix} 2 + 6u_1 + u_2  1 + u_1 \\\\ 1 + u_1  2 + 6u_2 \\end{pmatrix}\n$$\nThe BFGS update formula for the Hessian approximation $B$ is:\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}\n$$\nWe start with $B_0 = 2I = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$.\n\n**First BFGS Update:**\nWe compute the vectors $s_0 = u_1 - u_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe residuals are $r(u_0) = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$ and $r(u_1) = \\begin{pmatrix} 4 \\\\ 0.5 \\end{pmatrix}$.\nThus, $y_0 = r(u_1) - r(u_0) = \\begin{pmatrix} 5 \\\\ 1.5 \\end{pmatrix}$.\nNow we compute the terms for the update:\n$y_0^T s_0 = \\begin{pmatrix} 5  1.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 5$.\n$B_0 s_0 = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$.\n$s_0^T B_0 s_0 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = 2$.\nThe update terms are:\n$$\n\\frac{B_0 s_0 s_0^T B_0}{s_0^T B_0 s_0} = \\frac{1}{2} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 2  0 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}\n$$\n$$\n\\frac{y_0 y_0^T}{y_0^T s_0} = \\frac{1}{5} \\begin{pmatrix} 5 \\\\ 1.5 \\end{pmatrix} \\begin{pmatrix} 5  1.5 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 25  7.5 \\\\ 7.5  2.25 \\end{pmatrix} = \\begin{pmatrix} 5  1.5 \\\\ 1.5  0.45 \\end{pmatrix}\n$$\nSo, $B_1$ is:\n$$\nB_1 = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} - \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 5  1.5 \\\\ 1.5  0.45 \\end{pmatrix} = \\begin{pmatrix} 5  1.5 \\\\ 1.5  2.45 \\end{pmatrix}\n$$\n\n**Second BFGS Update:**\nWe compute $s_1 = u_2 - u_1 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe residuals are $r(u_1) = \\begin{pmatrix} 4 \\\\ 0.5 \\end{pmatrix}$ and $r(u_2) = \\begin{pmatrix} 6 \\\\ 5.5 \\end{pmatrix}$.\nThus, $y_1 = r(u_2) - r(u_1) = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$.\nNow we compute the terms for the update with $B_1$:\n$y_1^T s_1 = \\begin{pmatrix} 2  5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 5$.\n$B_1 s_1 = \\begin{pmatrix} 5  1.5 \\\\ 1.5  2.45 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 2.45 \\end{pmatrix}$.\n$s_1^T B_1 s_1 = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 1.5 \\\\ 2.45 \\end{pmatrix} = 2.45$.\nThe update terms are:\n$$\n\\frac{B_1 s_1 s_1^T B_1}{s_1^T B_1 s_1} = \\frac{1}{2.45} \\begin{pmatrix} 1.5 \\\\ 2.45 \\end{pmatrix} \\begin{pmatrix} 1.5  2.45 \\end{pmatrix} = \\frac{1}{2.45} \\begin{pmatrix} 2.25  3.675 \\\\ 3.675  6.0025 \\end{pmatrix} = \\begin{pmatrix} 45/49  1.5 \\\\ 1.5  2.45 \\end{pmatrix}\n$$\n$$\n\\frac{y_1 y_1^T}{y_1^T s_1} = \\frac{1}{5} \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\begin{pmatrix} 2  5 \\end{pmatrix} = \\begin{pmatrix} 0.8  2 \\\\ 2  5 \\end{pmatrix}\n$$\nSo, $B_2$ is:\n$$\nB_2 = \\begin{pmatrix} 5  1.5 \\\\ 1.5  2.45 \\end{pmatrix} - \\begin{pmatrix} 45/49  1.5 \\\\ 1.5  2.45 \\end{pmatrix} + \\begin{pmatrix} 0.8  2 \\\\ 2  5 \\end{pmatrix} = \\begin{pmatrix} 5 - 45/49 + 0.8  2 \\\\ 2  5 \\end{pmatrix}\n$$\nThe $(1,1)$ component is $5 + 0.8 - 45/49 = 5.8 - 45/49 = \\frac{284.2 - 45}{49} = \\frac{239.2}{49} = \\frac{1196}{245}$.\nSo, $B_2 = \\begin{pmatrix} 1196/245  2 \\\\ 2  5 \\end{pmatrix}$.\n\n**Compute Quasi-Newton Step $s_{\\mathrm{QN}}$:**\n$s_{\\mathrm{QN}} = -B_2^{-1} r(u_2)$.\n$\\det(B_2) = (\\frac{1196}{245})(5) - (2)(2) = \\frac{5980}{245} - 4 = \\frac{5980 - 980}{245} = \\frac{5000}{245} = \\frac{1000}{49}$.\n$$\nB_2^{-1} = \\frac{49}{1000} \\begin{pmatrix} 5  -2 \\\\ -2  1196/245 \\end{pmatrix} = \\frac{1}{1000} \\begin{pmatrix} 245  -98 \\\\ -98  1196/5 \\end{pmatrix} = \\begin{pmatrix} 0.245  -0.098 \\\\ -0.098  0.2392 \\end{pmatrix}\n$$\nUsing $r(u_2) = \\begin{pmatrix} 6 \\\\ 5.5 \\end{pmatrix}$:\n$$\ns_{\\mathrm{QN}} = - \\begin{pmatrix} 0.245  -0.098 \\\\ -0.098  0.2392 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 5.5 \\end{pmatrix} = - \\begin{pmatrix} 1.47 - 0.539 \\\\ -0.588 + 1.3156 \\end{pmatrix} = - \\begin{pmatrix} 0.931 \\\\ 0.7276 \\end{pmatrix} = \\begin{pmatrix} -0.931 \\\\ -0.7276 \\end{pmatrix}\n$$\n\n**Compute Exact Newton Step $s_{\\mathrm{N}}$:**\n$s_{\\mathrm{N}} = -K(u_2)^{-1} r(u_2)$.\nFirst, evaluate $K(u_2)$ at $u_2=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$:\n$$\nK(u_2) = \\begin{pmatrix} 2 + 6(1) + 1  1 + 1 \\\\ 1 + 1  2 + 6(1) \\end{pmatrix} = \\begin{pmatrix} 9  2 \\\\ 2  8 \\end{pmatrix}\n$$\n$\\det(K(u_2)) = (9)(8) - (2)(2) = 72-4 = 68$.\n$$\nK(u_2)^{-1} = \\frac{1}{68} \\begin{pmatrix} 8  -2 \\\\ -2  9 \\end{pmatrix}\n$$\n$$\ns_{\\mathrm{N}} = - \\frac{1}{68} \\begin{pmatrix} 8  -2 \\\\ -2  9 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 5.5 \\end{pmatrix} = -\\frac{1}{68} \\begin{pmatrix} 8(6) - 2(5.5) \\\\ -2(6) + 9(5.5) \\end{pmatrix} = -\\frac{1}{68} \\begin{pmatrix} 48 - 11 \\\\ -12 + 49.5 \\end{pmatrix} = -\\frac{1}{68} \\begin{pmatrix} 37 \\\\ 37.5 \\end{pmatrix} = \\begin{pmatrix} -37/68 \\\\ -75/136 \\end{pmatrix}\n$$\n\n**Compute the Norm of the Difference:**\nLet $d = s_{\\mathrm{QN}} - s_{\\mathrm{N}}$.\n$$\nd = \\begin{pmatrix} -0.931 \\\\ -0.7276 \\end{pmatrix} - \\begin{pmatrix} -37/68 \\\\ -75/136 \\end{pmatrix} = \\begin{pmatrix} -0.931 + 37/68 \\\\ -0.7276 + 75/136 \\end{pmatrix}\n$$\nCalculating the vector components:\n$$\nd_1 = -0.931 + 0.5441176... = -0.3868823...\n$$\n$$\nd_2 = -0.7276 + 0.5514705... = -0.1761294...\n$$\nNow we compute the Euclidean norm $\\|d\\|_2$:\n$$\n\\|d\\|_2 = \\sqrt{d_1^2 + d_2^2} = \\sqrt{(-0.3868823...)^2 + (-0.1761294...)^2}\n$$\n$$\n\\|d\\|_2 = \\sqrt{0.1496778... + 0.0310214...} = \\sqrt{0.1806992...} = 0.4250873...\n$$\nRounding to four significant figures, the result is $0.4251$.", "answer": "$$\\boxed{0.4251}$$", "id": "2580619"}, {"introduction": "In real-world applications, nonlinear solvers often employ iterative linear solvers, leading to \"Inexact Newton\" methods where practical issues can arise. This final practice [@problem_id:2580751] challenges you to act as a computational scientist, diagnosing a stalled simulation by analyzing its convergence history. Understanding the interplay between the nonlinear (outer) convergence and the tolerance of the linear (inner) solve is key to identifying the root cause of stagnation and selecting an effective corrective action.", "problem": "A nonlinear finite element equilibrium problem with internal variables is solved at a single load step using a globalized Newton framework with backtracking line search and an iterative linear solver. The finite element residual is denoted by $R(u)$, and the consistent tangent (Jacobian) is recomputed whenever specified below. The iterative linear solver is the Generalized Minimal Residual (GMRES) method, preconditioned by an incomplete factorization. The line search enforces a sufficient decrease condition; if a full step is accepted, the step length is recorded as $\\alpha_k = 1$. The nonlinear residual norm $\\lVert R_k \\rVert$ (measured in a consistent energy norm) and selected algorithmic diagnostics for each outer iteration $k$ are recorded as follows:\n\n- $k = 0$: $\\lVert R_0 \\rVert = 1.2 \\times 10^{1}$; tangent recomputed; GMRES terminated when $\\lVert J_0 s_0 + R_0 \\rVert / \\lVert R_0 \\rVert \\le 1.0 \\times 10^{-2}$; accepted $\\alpha_0 = 1$; obtained $\\lVert R_1 \\rVert = 2.3$.\n- $k = 1$: tangent recomputed; GMRES termination as above with relative linear residual $\\le 1.0 \\times 10^{-2}$; accepted $\\alpha_1 = 1$; obtained $\\lVert R_2 \\rVert = 4.8 \\times 10^{-1}$.\n- $k = 2$: tangent recomputed; GMRES termination as above; accepted $\\alpha_2 = 1$; obtained $\\lVert R_3 \\rVert = 1.1 \\times 10^{-1}$.\n- $k = 3$: tangent recomputed; GMRES termination as above; accepted $\\alpha_3 = 1$; obtained $\\lVert R_4 \\rVert = 2.7 \\times 10^{-2}$.\n- $k = 4$: tangent recomputed; GMRES termination as above; accepted $\\alpha_4 = 1$; obtained $\\lVert R_5 \\rVert = 2.5 \\times 10^{-2}$.\n- $k = 5$: tangent recomputed; GMRES termination as above; accepted $\\alpha_5 = 1$; obtained $\\lVert R_6 \\rVert = 2.4 \\times 10^{-2}$.\n\nNo backtracking was reported in any iteration, and the number of GMRES iterations per outer step remained nearly constant. The prescribed nonlinear convergence tolerance is $\\lVert R_k \\rVert \\le 1.0 \\times 10^{-8}$.\n\nBased on a first-principles analysis of Newton-type methods in finite element contexts, diagnose the most plausible primary cause of the observed stagnation near $\\lVert R_k \\rVert \\approx 2.4 \\times 10^{-2}$, and select the option that provides a consistent and effective corrective action.\n\nA. Stagnation is due to inexact linear solves with a forcing term bounded below by $10^{-2}$; tighten the linear solve accuracy adaptively so that the relative linear residual $\\eta_k$ decreases with $\\lVert R_k \\rVert$ (for example, using an Eisenstat–Walker forcing sequence), lower any hard floor below $10^{-6}$, and improve preconditioning to allow GMRES to meet stricter tolerances.\n\nB. Stagnation is due to poor tangents from a modified Newton strategy using a frozen Jacobian; switch to full Newton by recomputing the tangent every iteration or to a Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton update to restore superlinear convergence.\n\nC. Stagnation is due to an overly conservative line search that rejects full steps; relax the sufficient decrease parameter to accept larger $\\alpha_k$, or replace line search with a trust-region method to avoid step clipping.\n\nD. Stagnation is due to nonconvexity leading to indefinite tangents and ascent directions; add Levenberg–Marquardt damping to force positive definiteness and ensure descent, even if it increases the number of line search backtracks.", "solution": "The core of the algorithm is the Inexact Newton iteration. At each outer iteration $k$, we seek a displacement update $s_k$ by approximately solving the linear system:\n$$ J_k s_k = -R_k $$\nwhere $J_k = J(u_k)$ is the Jacobian matrix at the current displacement iterate $u_k$, and $R_k = R(u_k)$ is the residual vector. The subsequent update is $u_{k+1} = u_k + \\alpha_k s_k$. The problem states that the tangent is recomputed at every iteration, which means this is a full Inexact Newton method.\n\nThe linear system is solved iteratively using GMRES until the relative linear residual satisfies:\n$$ \\frac{\\lVert J_k s_k + R_k \\rVert}{\\lVert R_k \\rVert} \\le \\eta_k $$\nFrom the given data, the forcing term $\\eta_k$ is constant for all $k$: $\\eta_k = 1.0 \\times 10^{-2}$.\n\nThe nonlinear residual norm converges as follows: $12 \\to 2.3 \\to 0.48 \\to 0.11 \\to 0.027 \\to 0.025 \\to 0.024$. The convergence is initially linear but then stagnates around $2.4 \\times 10^{-2}$, far from the desired tolerance of $1.0 \\times 10^{-8}$.\n\nThe theory of Inexact Newton methods provides the explanation. The residual at the next step, $R_{k+1}$, can be approximated by a Taylor series expansion. For a full step ($\\alpha_k = 1$), this is: $R_{k+1} = R(u_k + s_k) \\approx R_k + J_k s_k$. The term $R_k + J_k s_k$ is the residual of the linear system solved at step $k$. The GMRES stopping criterion ensures that $\\lVert R_k + J_k s_k \\rVert \\le \\eta_k \\lVert R_k \\rVert$. This implies that the best possible reduction in the nonlinear residual is approximately $\\lVert R_{k+1} \\rVert \\lesssim \\eta_k \\lVert R_k \\rVert$.\n\nCrucially, for the nonlinear iteration to converge to high accuracy, the forcing term $\\eta_k$ must tend to zero as $k \\to \\infty$. A constant $\\eta_k$ imposes a hard limit on the achievable accuracy. The nonlinear residual $\\lVert R_k \\rVert$ cannot be reduced much further once it is on the same order of magnitude as the error introduced from the inexact linear solve. The stagnation level is therefore determined by $\\eta_k$. In this problem, stagnation occurs at $\\lVert R_k \\rVert \\approx 2.4 \\times 10^{-2}$, which is entirely consistent with the fixed linear solver tolerance $\\eta_k = 1.0 \\times 10^{-2}$. The method cannot reduce the nonlinear residual further because the search direction $s_k$ is not accurate enough.\n\nLet's evaluate the options:\n\n**A.** This option correctly identifies that the stagnation is due to the inexact linear solves with a fixed forcing term ($\\eta_k = 10^{-2}$). It proposes the standard and correct solution: tighten the linear solve accuracy adaptively as the nonlinear solve progresses. An Eisenstat–Walker forcing sequence (e.g., setting $\\eta_k = C \\lVert R_k \\rVert / \\lVert R_{k-1} \\rVert$) is a classic way to do this, ensuring that the linear system is solved more accurately as the solution is approached, which restores superlinear or quadratic convergence. This diagnosis and solution are perfectly aligned with the data and theory.\n\n**B.** This diagnosis is factually incorrect. The problem states that the tangent is \"recomputed\" at every iteration. This is a full Newton method, not a modified Newton strategy with a frozen Jacobian. The proposed correction is therefore irrelevant.\n\n**C.** This diagnosis is factually incorrect. The problem reports that the line search \"accepted $\\alpha_k = 1$\" for all iterations. This means the full Newton step was taken every time without any reduction. The line search is not being conservative; it is not the source of the problem.\n\n**D.** This diagnosis is inconsistent with the data. If the tangent were indefinite and producing ascent directions, the line search would fail to find a sufficient decrease with a full step, forcing backtracking and leading to $\\alpha_k  1$. Since $\\alpha_k=1$ is always accepted, it indicates that the computed search directions are effective descent directions. The problem is not with the direction itself being poor, but with its insufficient accuracy due to the loose linear solver tolerance.", "answer": "$$\\boxed{A}$$", "id": "2580751"}]}