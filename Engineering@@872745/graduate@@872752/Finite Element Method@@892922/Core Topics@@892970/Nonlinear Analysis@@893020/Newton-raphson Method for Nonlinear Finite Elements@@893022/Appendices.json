{"hands_on_practices": [{"introduction": "The foundation of a reliable nonlinear finite element solver is the correct implementation of its core components: the residual vector $R(u)$ and the tangent stiffness matrix $K_T(u)$. An error in either of these, particularly the tangent, can lead to poor convergence or outright failure. This practice guides you through the \"tangent consistency check,\" a gold-standard unit test that uses finite differences to verify that your analytically derived tangent matrix is indeed the true derivative of your residual vector. Mastering this verification technique is an essential first step for any developer or researcher building nonlinear simulation tools [@problem_id:2583316].", "problem": "Consider a single two-node one-dimensional bar finite element with linear shape functions and a nonlinear constitutive law. Assume all quantities are dimensionless. Let the nodal displacement vector be $u \\in \\mathbb{R}^2$, the element length be $L  0$, and the cross-sectional area be $A  0$. The Green-Lagrange strain for this element reduces to a constant engineering strain $\\varepsilon = B u$ with the constant strain-displacement matrix $B = \\left[-\\dfrac{1}{L}, \\dfrac{1}{L}\\right]$. The Cauchy stress is modeled by a smooth nonlinear law $\\sigma(\\varepsilon) = E\\,\\varepsilon + \\alpha\\,\\varepsilon^3$, where $E  0$ is a linear stiffness parameter and $\\alpha \\ge 0$ controls the nonlinearity. The internal force vector is given by the principle of virtual work as $f_{\\mathrm{int}}(u) = \\int_{0}^{L} B^\\top \\sigma(\\varepsilon)\\, A\\, \\mathrm{d}x$, and the residual is $R(u) = f_{\\mathrm{int}}(u) - f_{\\mathrm{ext}}$, where $f_{\\mathrm{ext}}\\in\\mathbb{R}^2$ is the external nodal load vector. The Newton-Raphson method for solving $R(u)=0$ uses the consistent tangent (Jacobian) $K_T(u) = \\dfrac{\\partial R}{\\partial u}(u)$.\n\nYour task is to write a program that implements a unit test verifying the consistency of the tangent $K_T(u)$ against a central finite-difference directional derivative of $R(u)$ under a random perturbation direction. The test must use a reproducible random direction and a theoretically justified step size. Specifically:\n\n- Start from the definitions above and derive $R(u)$ and $K_T(u)$ in terms of $E$, $\\alpha$, $A$, $L$, and $u$, using only the facts that $B$ is constant for a two-node linear element, $\\varepsilon = B u$, $f_{\\mathrm{int}}(u) = \\int_{0}^{L} B^\\top \\sigma(\\varepsilon)\\, A\\, \\mathrm{d}x$, and $\\sigma(\\varepsilon) = E\\,\\varepsilon + \\alpha\\,\\varepsilon^3$.\n- Implement a central finite-difference check of the form\n$$\n\\frac{R(u + h\\,p) - R(u - h\\,p)}{2\\,h} \\approx K_T(u)\\,p,\n$$\nwhere $p \\in \\mathbb{R}^2$ is a random unit vector (direction) generated with a fixed seed, and $h  0$ is a step size chosen according to floating-point error analysis. Use the relative error\n$$\ne = \\frac{\\left\\| \\frac{R(u + h\\,p) - R(u - h\\,p)}{2\\,h} - K_T(u)\\,p \\right\\|_2}{\\max\\left(1,\\;\\left\\| K_T(u)\\,p \\right\\|_2\\right)}\n$$\nand declare the test passed if $e \\le \\text{rtol}$ for a specified relative tolerance $\\text{rtol}$.\n- Choose $h$ based on the standard central-difference truncation-versus-roundoff balance, and justify your choice of $h$ and $\\text{rtol}$ in terms of the floating-point machine epsilon $\\varepsilon_{\\mathrm{mach}}$ and the expected curvature scale of the residual.\n\nAssume dimensionless quantities and radians are implied if any angles appear (no angles are used here). Use the following test suite (each case provides $(E,\\alpha,A,L,u,f_{\\mathrm{ext}})$), where all numbers are dimensionless:\n\n- Case A (linear happy path): $E = 100$, $\\alpha = 0$, $A = 1$, $L = 1$, $u = [\\,0.2,\\,0.8\\,]^\\top$, $f_{\\mathrm{ext}} = [\\,0,\\,0\\,]^\\top$.\n- Case B (moderate nonlinearity): $E = 200$, $\\alpha = 50$, $A = 2$, $L = 3$, $u = [\\,0.01,\\,0.03\\,]^\\top$, $f_{\\mathrm{ext}} = [\\,1,\\,-1\\,]^\\top$.\n- Case C (strong nonlinearity and larger strain magnitude): $E = 10$, $\\alpha = 2000$, $A = 0.8$, $L = 1.5$, $u = [\\,0.3,\\,-0.2\\,]^\\top$, $f_{\\mathrm{ext}} = [\\,-0.5,\\,0.2\\,]^\\top$.\n- Case D (zero displacement): $E = 150$, $\\alpha = 500$, $A = 1.2$, $L = 2.5$, $u = [\\,0,\\,0\\,]^\\top$, $f_{\\mathrm{ext}} = [\\,0,\\,0\\,]^\\top$.\n\nUse a fixed random seed so that the random direction $p$ is the same on every run. Use a single relative tolerance $\\text{rtol}$ for all cases, justified by analysis. Your program must output one boolean result per case indicating whether the test passes.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True]\").", "solution": "We begin from the principle of virtual work in a single two-node one-dimensional bar element with linear shape functions. The constant strain-displacement matrix is $B = \\left[-\\dfrac{1}{L}, \\dfrac{1}{L}\\right]$, so the strain is $\\varepsilon = B u$. The constitutive law is $\\sigma(\\varepsilon) = E\\,\\varepsilon + \\alpha\\,\\varepsilon^3$. The internal force follows from the virtual work statement\n$$\nf_{\\mathrm{int}}(u) \\equiv \\int_0^L B^\\top\\,\\sigma(\\varepsilon)\\,A\\,\\mathrm{d}x.\n$$\nFor a linear two-node element, $B$ and $\\varepsilon$ are constant over the element, so the integral reduces to\n$$\nf_{\\mathrm{int}}(u) = B^\\top\\,\\sigma(\\varepsilon)\\,A\\, \\int_0^L \\mathrm{d}x = B^\\top\\,\\sigma(\\varepsilon)\\,A\\,L.\n$$\nTherefore, the residual is\n$$\nR(u) = f_{\\mathrm{int}}(u) - f_{\\mathrm{ext}} = B^\\top\\,\\sigma(Bu)\\,A\\,L - f_{\\mathrm{ext}}.\n$$\nTo form the consistent tangent, we differentiate $R(u)$ with respect to $u$. Noting $R(u)$ depends on $u$ only through $\\sigma(Bu)$, we apply the chain rule:\n$$\nK_T(u) \\equiv \\frac{\\partial R}{\\partial u}(u) = B^\\top\\,\\frac{\\mathrm{d}\\sigma}{\\mathrm{d}\\varepsilon}\\bigg|_{\\varepsilon=Bu}\\,B\\,A\\,L.\n$$\nSince $\\sigma(\\varepsilon) = E\\,\\varepsilon + \\alpha\\,\\varepsilon^3$, its derivative is\n$$\n\\frac{\\mathrm{d}\\sigma}{\\mathrm{d}\\varepsilon} = E + 3\\,\\alpha\\,\\varepsilon^2.\n$$\nThus the $2\\times 2$ tangent matrix is\n$$\nK_T(u) = \\left(E + 3\\,\\alpha\\,(Bu)^2\\right)\\,A\\,L\\,\\left(B^\\top B\\right).\n$$\nWith $B = \\left[-\\dfrac{1}{L}, \\dfrac{1}{L}\\right]$, we have\n$$\nB^\\top B = \n\\begin{bmatrix}\n\\dfrac{1}{L^2}  -\\dfrac{1}{L^2}\\\n$$6pt]\n-\\dfrac{1}{L^2}  \\dfrac{1}{L^2}\n\\end{bmatrix},\n$$\nso explicitly\n$$\nK_T(u) = \\left(E + 3\\,\\alpha\\,\\varepsilon^2\\right)\\,A\\,\\frac{1}{L}\\,\n\\begin{bmatrix}\n1  -1\\\\\n-1  1\n\\end{bmatrix},\n\\quad \\text{with} \\quad \\varepsilon = \\frac{u_2 - u_1}{L}.\n$$\n\nTo verify the consistency of $K_T(u)$, we compare it to a central finite-difference directional derivative of $R(u)$. For a unit direction $p \\in \\mathbb{R}^2$ and step $h  0$, the central difference approximation is\n$$\nD_h R(u; p) \\equiv \\frac{R(u + h\\,p) - R(u - h\\,p)}{2\\,h}.\n$$\nBy Taylor expansion of $R$ about $u$ in the direction $p$,\n$$\nR(u \\pm h\\,p) = R(u) \\pm h\\,K_T(u)\\,p + \\frac{h^3}{6}\\,R^{(3)}(u)[p,p,p] + \\mathcal{O}(h^5),\n$$\nso the central difference cancels the even-order terms and yields\n$$\nD_h R(u; p) = K_T(u)\\,p + \\mathcal{O}(h^2).\n$$\nIn floating-point arithmetic, subtractive cancellation and rounding introduce an additional error term of order $\\mathcal{O}\\!\\left(\\frac{\\varepsilon_{\\mathrm{mach}}}{h}\\right)$, where $\\varepsilon_{\\mathrm{mach}}$ is the machine epsilon. Hence the total error behaves as\n$$\n\\|D_h R(u; p) - K_T(u)\\,p\\|_2 \\approx C_1\\,h^2 + C_2\\,\\frac{\\varepsilon_{\\mathrm{mach}}}{h},\n$$\nfor problem-dependent constants $C_1$ and $C_2$ set by higher derivatives and magnitudes. Balancing these terms suggests choosing $h$ on the order of $h \\sim \\varepsilon_{\\mathrm{mach}}^{1/3}$ up to a scale factor reflecting $\\|u\\|_2$. We therefore pick\n$$\nh = \\varepsilon_{\\mathrm{mach}}^{1/3}\\,\\max\\!\\left(1,\\;\\|u\\|_2\\right),\n$$\nwhich controls both truncation and roundoff across cases of different displacement magnitudes.\n\nWe measure a relative error\n$$\ne = \\frac{\\left\\| D_h R(u;p) - K_T(u)\\,p \\right\\|_2}{\\max\\left(1,\\;\\left\\| K_T(u)\\,p \\right\\|_2\\right)},\n$$\nand declare pass if $e \\le \\text{rtol}$. For double precision, $\\varepsilon_{\\mathrm{mach}} \\approx 2.22\\times 10^{-16}$, so $\\varepsilon_{\\mathrm{mach}}^{1/3} \\approx 6\\times 10^{-6}$. In purely linear cases, the central difference is exact in exact arithmetic, and the observed error is limited by roundoff, typically near $\\mathcal{O}(10^{-11})$ or smaller. In nonlinear cases, the truncation term scales with $h^2$ times a curvature magnitude set by higher derivatives. Here $\\sigma''(\\varepsilon) = 6\\,\\alpha\\,\\varepsilon$, so for moderate strains and $\\alpha$ up to values like $2000$, a conservative bound on the truncation contribution for $h \\approx 6\\times 10^{-6}$ lies around $10^{-7}$ to $10^{-8}$ in relative terms, depending on the magnitude of $K_T(u)\\,p$. To be robust across the provided test suite, including the strong nonlinearity case with larger $|\\varepsilon|$, we set a single tolerance\n$$\n\\text{rtol} = 10^{-6},\n$$\nwhich comfortably accommodates the worst-case truncation estimate while remaining stringent enough to detect inconsistencies.\n\nAlgorithmic steps implemented in the program:\n- For each test case, parse $(E,\\alpha,A,L,u,f_{\\mathrm{ext}})$.\n- Define $B = \\left[-\\dfrac{1}{L}, \\dfrac{1}{L}\\right]$, compute $\\varepsilon = B u$, compute $\\sigma(\\varepsilon) = E\\,\\varepsilon + \\alpha\\,\\varepsilon^3$, then compute $R(u) = B^\\top\\,\\sigma(\\varepsilon)\\,A\\,L - f_{\\mathrm{ext}}$.\n- Compute $K_T(u) = B^\\top\\,(E + 3\\,\\alpha\\,\\varepsilon^2)\\,B\\,A\\,L$.\n- Generate a reproducible random direction $p$ using a fixed seed and normalize it to unit length.\n- Choose $h = \\varepsilon_{\\mathrm{mach}}^{1/3}\\,\\max(1,\\|u\\|_2)$.\n- Compute $D_h R(u;p)$ and the relative error $e$, and check $e \\le \\text{rtol}$.\n- Output one boolean per case in a single bracketed list on one line.\n\nThe final output is a single line like \"[True,True,True,True]\". This unit test design validates the Newton-Raphson consistent tangent $K_T(u)$ via an accuracy-controlled finite-difference check with a justified tolerance linked to floating-point limits and truncation error analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef residual(u, E, alpha, A, L, f_ext):\n    \"\"\"\n    Compute residual R(u) = f_int(u) - f_ext for a 2-node 1D bar element\n    with nonlinear constitutive law sigma = E*eps + alpha*eps^3.\n    \"\"\"\n    # B = [-1/L, 1/L]\n    B = np.array([-1.0 / L, 1.0 / L])\n    eps = float(B @ u)\n    sigma = E * eps + alpha * (eps ** 3)\n    f_int = (A * L) * (B[:, None] * sigma)  # B^T * sigma * A * L; shape (2,1)\n    # Convert to 1D array and subtract external load\n    return f_int.ravel() - f_ext\n\ndef tangent(u, E, alpha, A, L):\n    \"\"\"\n    Compute consistent tangent K_T(u) = B^T * (d sigma/d eps) * B * A * L\n    where d sigma/d eps = E + 3*alpha*eps^2.\n    \"\"\"\n    B = np.array([-1.0 / L, 1.0 / L])\n    eps = float(B @ u)\n    dsig = E + 3.0 * alpha * (eps ** 2)\n    K = (A * L) * dsig * np.outer(B, B)\n    return K\n\ndef central_diff_check(u, params, rng, rtol):\n    \"\"\"\n    Perform central difference directional derivative check:\n    (R(u+h p) - R(u-h p))/(2h) ~ K(u) p\n    Returns True if relative error = rtol.\n    \"\"\"\n    E, alpha, A, L, f_ext = params\n    # Random direction p, normalized\n    p = rng.standard_normal(size=2)\n    nrm = np.linalg.norm(p)\n    if nrm == 0.0:\n        p = np.array([1.0, 0.0])\n    else:\n        p = p / nrm\n\n    # Step size selection: h = eps^(1/3) * max(1, ||u||)\n    eps_mach = np.finfo(float).eps\n    h_base = eps_mach ** (1.0 / 3.0)\n    scale = max(1.0, float(np.linalg.norm(u)))\n    h = h_base * scale\n\n    Ru_ph = residual(u + h * p, E, alpha, A, L, f_ext)\n    Ru_mh = residual(u - h * p, E, alpha, A, L, f_ext)\n    cd = (Ru_ph - Ru_mh) / (2.0 * h)\n\n    K = tangent(u, E, alpha, A, L)\n    Kp = K @ p\n\n    num = np.linalg.norm(cd - Kp)\n    den = max(1.0, np.linalg.norm(Kp))\n    rel_err = num / den\n    return rel_err = rtol\n\ndef solve():\n    # Define test cases as specified in the problem statement.\n    # Each case: (E, alpha, A, L, u, f_ext)\n    test_cases = [\n        (100.0, 0.0, 1.0, 1.0, np.array([0.2, 0.8]), np.array([0.0, 0.0])),            # Case A\n        (200.0, 50.0, 2.0, 3.0, np.array([0.01, 0.03]), np.array([1.0, -1.0])),         # Case B\n        (10.0, 2000.0, 0.8, 1.5, np.array([0.3, -0.2]), np.array([-0.5, 0.2])),         # Case C\n        (150.0, 500.0, 1.2, 2.5, np.array([0.0, 0.0]), np.array([0.0, 0.0])),           # Case D\n    ]\n\n    # Relative tolerance justified by truncation and roundoff analysis.\n    rtol = 1e-6\n\n    # Fixed random seed for reproducibility.\n    rng = np.random.default_rng(123456)\n\n    results = []\n    for (E, alpha, A, L, u, f_ext) in test_cases:\n        params = (E, alpha, A, L, f_ext)\n        ok = central_diff_check(u, params, rng, rtol)\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2583316"}, {"introduction": "Once the correctness of the tangent matrix is established, the next practical question is one of efficiency: how often should we compute and factorize this computationally expensive matrix? This problem explores the classic trade-off between the full Newton method, which promises rapid quadratic convergence by updating the tangent at every iteration, and the modified Newton method, which saves costs by reusing a single tangent factorization. By analyzing the implications for convergence rate and computational expense, you will develop the strategic insight needed to select the appropriate solution algorithm for different practical scenarios [@problem_id:2583323].", "problem": "Consider a quasi-static nonlinear structural problem discretized by the Finite Element Method (FEM). Let the global displacement vector be $\\,\\mathbf{u}\\,$ and the equilibrium at a given load level be stated as the root-finding problem $\\,\\mathbf{r}(\\mathbf{u})=\\mathbf{0}\\,$, where $\\,\\mathbf{r}(\\mathbf{u})=\\mathbf{f}_{\\mathrm{ext}}-\\mathbf{f}_{\\mathrm{int}}(\\mathbf{u})\\,$ is the residual vector, $\\,\\mathbf{f}_{\\mathrm{ext}}\\,$ is the external load vector, and $\\,\\mathbf{f}_{\\mathrm{int}}(\\mathbf{u})\\,$ is the internal force vector obtained from constitutive laws and strain-displacement relations. Assume $\\,\\mathbf{r}\\,$ is continuously differentiable, and its Jacobian (the consistent tangent stiffness) $\\,\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u})=\\dfrac{\\partial \\mathbf{r}}{\\partial \\mathbf{u}}(\\mathbf{u})\\,$ is Lipschitz continuous in a neighborhood of the solution $\\,\\mathbf{u}^{\\star}\\,$ with $\\,\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{\\star})\\,$ nonsingular.\n\nAt each load step, the full Newton method recomputes and refactorizes the consistent tangent stiffness $\\,\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{k})\\,$ at every nonlinear iteration $\\,k\\,$. The modified Newton method, in contrast, holds a fixed tangent stiffness $\\,\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{0})\\,$ over all iterations within the given load step, typically reusing a single factorization. Consider a large, sparse system solved by a direct method in which the factorization time of $\\,\\mathbf{K}_{\\mathrm{T}}\\,$ dominates over the times for residual assembly and forward/back substitutions.\n\nWhich of the following statements about convergence behavior and computational cost per load step are correct? Select all that apply.\n\nA. Under the stated smoothness and nonsingularity assumptions, the full Newton method exhibits local quadratic convergence when the tangent is updated at each iteration, whereas the modified Newton method with a fixed tangent within the load step is, in general, only linearly convergent.\n\nB. The modified Newton method typically requires more nonlinear iterations per load step than the full Newton method, but each iteration can be cheaper because a single factorization of the tangent stiffness can be reused across iterations.\n\nC. Fixing the tangent within a load step renders the method unconditionally stable with monotonically decreasing residual norm for arbitrary load-step sizes, so globalization techniques such as line search or trust region become unnecessary.\n\nD. For sparse large-scale problems where direct factorization dominates the per-iteration cost, performing the factorization only once per load step can make the modified Newton method faster overall than the full Newton method, even if it takes more iterations, because subsequent iterations require only back-substitutions and residual assemblies.\n\nE. Equipping the modified Newton method with a line search generally restores quadratic convergence in the neighborhood of the solution, matching the full Newton method’s local rate.", "solution": "The problem statement is first validated against scientific and logical principles.\n\n### Step 1: Extract Givens\n-   **Problem Domain**: Quasi-static nonlinear structural problem discretized by the Finite Element Method (FEM).\n-   **Unknown**: Global displacement vector, $\\mathbf{u}$.\n-   **Equilibrium Equation**: $\\mathbf{r}(\\mathbf{u})=\\mathbf{0}$.\n-   **Residual Vector**: $\\mathbf{r}(\\mathbf{u})=\\mathbf{f}_{\\mathrm{ext}}-\\mathbf{f}_{\\mathrm{int}}(\\mathbf{u})$, where $\\mathbf{f}_{\\mathrm{ext}}$ is the external load vector and $\\mathbf{f}_{\\mathrm{int}}(\\mathbf{u})$ is the internal force vector.\n-   **Smoothness Assumption**: $\\mathbf{r}$ is continuously differentiable.\n-   **Jacobian (Consistent Tangent Stiffness)**: $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u})=\\dfrac{\\partial \\mathbf{r}}{\\partial \\mathbf{u}}(\\mathbf{u})$.\n-   **Jacobian Properties**: $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u})$ is Lipschitz continuous in a neighborhood of the solution $\\mathbf{u}^{\\star}$. The matrix $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{\\star})$ is nonsingular.\n-   **Full Newton Method**: Recomputes and refactorizes $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{k})$ at every iteration $k$.\n-   **Modified Newton Method**: Uses a fixed tangent stiffness, $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{0})$, for all iterations within a load step.\n-   **Computational Context**: Large, sparse system. The linear system is solved a direct method. The factorization time of $\\mathbf{K}_{\\mathrm{T}}$ dominates over the times for residual assembly and forward/back substitutions.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Grounding**: The problem is well-grounded in the theory and practice of computational solid mechanics and numerical analysis. The setup describes the standard application of Newton-Raphson methods to nonlinear finite element analysis. All terms ($\\mathbf{f}_{\\mathrm{int}}$, $\\mathbf{f}_{\\mathrm{ext}}$, $\\mathbf{K}_{\\mathrm{T}}$) are standard. The assumptions on the function $\\mathbf{r}$ and its Jacobian $\\mathbf{K}_{\\mathrm{T}}$ are the canonical conditions required for local convergence theorems of Newton-type methods.\n-   **Well-Posedness**: The problem is well-posed. It asks for a qualitative comparison of two well-defined numerical algorithms under a clear set of assumptions and computational context. A definite set of correct statements can be identified based on established numerical theory.\n-   **Objectivity**: The problem is stated using precise, objective, and standard technical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a standard and well-formulated question about the properties of numerical methods for nonlinear systems. A solution will be derived.\n\n### Derivation and Analysis\nThe problem concerns the solution of the nonlinear system of equations $\\mathbf{r}(\\mathbf{u})=\\mathbf{0}$. The general form of a Newton-type iteration is:\n$$\n\\mathbf{u}^{k+1} = \\mathbf{u}^{k} + \\Delta \\mathbf{u}^{k}\n$$\nwhere the displacement increment $\\Delta \\mathbf{u}^{k}$ is found by solving a linear system that approximates the original nonlinear problem.\n\nFor the **full Newton method** (also known as Newton-Raphson), the iteration is defined by solving for $\\Delta \\mathbf{u}^{k}$ from:\n$$\n\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{k}) \\Delta \\mathbf{u}^{k} = -\\mathbf{r}(\\mathbf{u}^{k})\n$$\nwhere the tangent stiffness matrix $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{k}) = \\dfrac{\\partial \\mathbf{r}}{\\partial \\mathbf{u}}(\\mathbf{u}^{k})$ is the exact Jacobian of the residual vector $\\mathbf{r}$ evaluated at the current iterate $\\mathbf{u}^{k}$. This matrix must be assembled and factorized at each iteration $k$.\n\nFor the **modified Newton method**, the iteration is defined by solving:\n$$\n\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{0}) \\Delta \\mathbf{u}^{k} = -\\mathbf{r}(\\mathbf{u}^{k})\n$$\nwhere the tangent stiffness matrix $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{0})$ is computed only once at the beginning of the load step (at iteration $k=0$) and is then held constant for all subsequent iterations within that step. This means the matrix factorization is also performed only once.\n\nThe convergence properties and computational cost are analyzed based on these definitions.\n\n**Convergence Rate:**\n-   The full Newton method, under the given assumptions (Lipschitz continuous Jacobian, nonsingular Jacobian at the solution), is known to exhibit **quadratic convergence** in a sufficiently small neighborhood of the solution $\\mathbf{u}^{\\star}$. This means the error $\\mathbf{e}^{k} = \\mathbf{u}^{k} - \\mathbf{u}^{\\star}$ satisfies $||\\mathbf{e}^{k+1}|| \\le C ||\\mathbf{e}^{k}||^{2}$ for some constant $C0$.\n-   The modified Newton method is a form of a stationary iterative method. The iteration map is $g(\\mathbf{u}) = \\mathbf{u} - [\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{0})]^{-1} \\mathbf{r}(\\mathbf{u})$. Its convergence rate is determined by the spectral radius of its Jacobian at the solution, $J_g(\\mathbf{u}^{\\star})$. Since $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{\\star}) = \\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{u}}(\\mathbf{u}^{\\star})$, we have $J_g(\\mathbf{u}^{\\star}) = \\mathbf{I} - [\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{0})]^{-1} \\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{\\star})$. Unless $\\mathbf{u}^0$ is chosen such that $\\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{0}) = \\mathbf{K}_{\\mathrm{T}}(\\mathbf{u}^{\\star})$, this Jacobian is not the zero matrix. Therefore, the convergence is, in general, **linear**, with $||\\mathbf{e}^{k+1}|| \\le c ||\\mathbf{e}^{k}||$, where the convergence factor $c$ is related to the spectral radius of $J_g(\\mathbf{u}^{\\star})$.\n\n**Computational Cost:**\n-   Let $T_{\\mathrm{asm}}$, $T_{\\mathrm{fac}}$, and $T_{\\mathrm{solve}}$ be the times for matrix assembly, factorization (e.g., LU decomposition), and forward/back substitution, respectively. The problem states $T_{\\mathrm{fac}}$ is dominant for a direct solver.\n-   Cost per iteration for full Newton: $C_{\\mathrm{FN}} = T_{\\mathrm{asm}} + T_{\\mathrm{fac}} + T_{\\mathrm{solve}}$.\n-   Cost per load step for full Newton (with $N_{\\mathrm{FN}}$ iterations): $T_{\\mathrm{total, FN}} = N_{\\mathrm{FN}} \\times C_{\\mathrm{FN}}$.\n-   Cost for modified Newton (with $N_{\\mathrm{MN}}$ iterations): $T_{\\mathrm{total, MN}} = (T_{\\mathrm{asm}} + T_{\\mathrm{fac}} + T_{\\mathrm{solve}}) + (N_{\\mathrm{MN}}-1)(T'_{\\mathrm{asm}} + T_{\\mathrm{solve}})$, where $T'_{\\mathrm{asm}}$ is the time to assemble only the residual vector $\\mathbf{r}(\\mathbf{u}^k)$. Because $T_{\\mathrm{fac}}$ is dominant, the cost of the first iteration is high, but subsequent iterations are much cheaper.\n\n### Evaluation of Options\n\n**A. Under the stated smoothness and nonsingularity assumptions, the full Newton method exhibits local quadratic convergence when the tangent is updated at each iteration, whereas the modified Newton method with a fixed tangent within the load step is, in general, only linearly convergent.**\nThis statement is a direct consequence of the fundamental theory of numerical methods for solving nonlinear equations, as detailed in the analysis above. The full Newton method has a locally quadratic rate of convergence, while the modified Newton method has a locally linear rate.\n**Verdict: Correct**\n\n**B. The modified Newton method typically requires more nonlinear iterations per load step than the full Newton method, but each iteration can be cheaper because a single factorization of the tangent stiffness can be reused across iterations.**\nLinear convergence is slower than quadratic convergence. Thus, to reach the same tolerance, the modified Newton method will generally require a larger number of iterations ($N_{\\mathrm{MN}}  N_{\\mathrm{FN}}$). However, for every iteration after the first, the cost is significantly lower because the computationally expensive factorization step is skipped. Only residual assembly and forward/back substitution are needed. This statement accurately describes the trade-off.\n**Verdict: Correct**\n\n**C. Fixing the tangent within a load step renders the method unconditionally stable with monotonically decreasing residual norm for arbitrary load-step sizes, so globalization techniques such as line search or trust region become unnecessary.**\nThis statement is entirely false. Neither the full nor the modified Newton method is unconditionally stable or globally convergent. Both methods are only guaranteed to converge if the initial guess ($\\mathbf{u}^0$) is sufficiently close to the solution ($\\mathbf{u}^{\\star}$). For large load steps, $\\mathbf{u}^0$ may be far from $\\mathbf{u}^{\\star}$, leading to divergence. The residual norm is not guaranteed to decrease monotonically in either method without a globalization strategy. In fact, such strategies (line search, trust region) are often crucial for ensuring convergence, especially for the modified Newton method which uses a less accurate approximation of the system's Jacobian.\n**Verdict: Incorrect**\n\n**D. For sparse large-scale problems where direct factorization dominates the per-iteration cost, performing the factorization only once per load step can make the modified Newton method faster overall than the full Newton method, even if it takes more iterations, because subsequent iterations require only back-substitutions and residual assemblies.**\nThis statement correctly identifies the primary motivation for using the modified Newton method. The total computational time is a product of the number of iterations and the cost per iteration. For full Newton, the total time is approximately $N_{\\mathrm{FN}} \\times T_{\\mathrm{fac}}$. For modified Newton, the total time is approximately $T_{\\mathrm{fac}} + N_{\\mathrm{MN}} \\times (T_{\\mathrm{res\\_asm}} + T_{\\mathrm{solve}})$. Since $T_{\\mathrm{fac}}$ is much larger than the cost of the other operations, it is entirely possible for the total time of the modified Newton method to be less than that of the full Newton method, provided that $N_{\\mathrm{MN}}$ does not become excessively large compared to $N_{\\mathrm{FN}}$. This trade-off is central to the choice of nonlinear solver.\n**Verdict: Correct**\n\n**E. Equipping the modified Newton method with a line search generally restores quadratic convergence in the neighborhood of the solution, matching the full Newton method’s local rate.**\nA line search is a globalization technique. Its purpose is to enlarge the domain of convergence, ensuring that the algorithm makes progress toward the solution even when far away. It achieves this by adjusting the step length $\\alpha_k$ in the update $\\mathbf{u}^{k+1} = \\mathbf{u}^{k} + \\alpha_k \\Delta \\mathbf{u}^{k}$. A line search does not change the fundamental formulation of the search direction $\\Delta \\mathbf{u}^{k}$, which is what determines the asymptotic, local rate of convergence. As the iterates approach the solution, a properly implemented line search will typically accept a full step ($\\alpha_k \\rightarrow 1$), and the method's convergence rate will revert to that of the underlying scheme—which is linear for the modified Newton method. To achieve a super-linear or quadratic rate, one must use an iteration that incorporates more current information about the Jacobian, such as a quasi-Newton (e.g., BFGS) or full Newton update.\n**Verdict: Incorrect**", "answer": "$$\\boxed{ABD}$$", "id": "2583323"}, {"introduction": "The standard load-controlled Newton-Raphson method can fail when analyzing structures that exhibit instabilities like snap-through or snap-back, where the load-deflection path contains limit points. To overcome this, advanced path-following techniques are required. This exercise provides a hands-on walkthrough of a single iteration of the spherical arc-length method, a robust technique that treats both displacements and the load factor as variables. By working through the linearization of both the equilibrium and the constraint equations, you will gain practical experience with a powerful tool for tracing complex, nonlinear structural responses beyond their stability limits [@problem_id:2583322].", "problem": "A two-bar truss is modeled as two parallel, coaxial, axial springs connecting a rigid base to a single top node with one generalized displacement degree of freedom $u$ (positive downward). Each bar $i \\in \\{1,2\\}$ follows a nonlinear axial force–elongation relation $N_i(a_i) = k_i a_i + \\beta_i a_i^3$ under small displacement kinematics, so that for this one-degree-of-freedom configuration $a_1 = a_2 = u$ and the assembled internal nodal force is $f_{\\mathrm{int}}(u) = \\sum_{i=1}^{2} N_i(u)$. The external load is applied through a scalar load factor $\\lambda$ multiplying a fixed reference load $\\hat{P}$, so that $f_{\\mathrm{ext}} = \\lambda \\hat{P}$.\n\nYou will perform one Newton–Raphson corrector iteration within a Crisfield-type spherical arc-length procedure. Use the following data:\n- Constitutive parameters: $k_1 = 2$, $k_2 = 1$, $\\beta_1 = 1$, $\\beta_2 = 0$.\n- Reference load: $\\hat{P} = 5$.\n- Current iterate at load step $n \\to n{+}1$: $(u_k,\\lambda_k) = (1.0,\\,0.5)$.\n- Last converged state from the previous load step: $(u_n,\\lambda_n) = (0.7,\\,0.4)$.\n- Arc-length parameters: radius $\\Delta s = 0.6$ and load scaling $\\alpha = 2$.\n\nStarting from the principle of virtual work and its finite element residual $R(u,\\lambda) = f_{\\mathrm{int}}(u) - \\lambda \\hat{P}$, and adopting the spherical arc-length constraint\n$$\ng(\\Delta u,\\Delta \\lambda) \\equiv \\Delta u^2 + \\alpha^2 \\Delta \\lambda^2 - \\Delta s^2 = 0,\n$$\nwhere $\\Delta u = u - u_n$ and $\\Delta \\lambda = \\lambda - \\lambda_n$, do the following within this single corrector iteration about $(u_k,\\lambda_k)$:\n- Derive the Newton–Raphson linearization of the equilibrium equation to obtain a relation between the corrector $(\\delta u,\\delta \\lambda)$ and the residual $R(u_k,\\lambda_k)$.\n- Linearize the arc-length constraint about $(\\Delta u_k,\\Delta \\lambda_k) = (u_k - u_n,\\lambda_k - \\lambda_n)$ to obtain a linear relation in $(\\delta u,\\delta \\lambda)$.\n- Assemble the resulting $2 \\times 2$ linear system for $(\\delta u,\\delta \\lambda)$ and solve it.\n\nReport the scalar corrector to the load factor, $\\delta \\lambda$, as your final answer. Provide the exact value; do not round. No units are required since $\\lambda$ is dimensionless.", "solution": "The problem is subjected to validation.\n\n**Step 1: Extract Givens**\n- Model: Two parallel axial springs with a single degree of freedom $u$.\n- Constitutive law for bar $i$: $N_i(a_i) = k_i a_i + \\beta_i a_i^3$.\n- Kinematics: $a_1 = a_2 = u$.\n- Internal force: $f_{\\mathrm{int}}(u) = \\sum_{i=1}^{2} N_i(u)$.\n- External force: $f_{\\mathrm{ext}} = \\lambda \\hat{P}$.\n- Residual: $R(u,\\lambda) = f_{\\mathrm{int}}(u) - \\lambda \\hat{P}$.\n- Arc-length constraint: $g(\\Delta u,\\Delta \\lambda) \\equiv \\Delta u^2 + \\alpha^2 \\Delta \\lambda^2 - \\Delta s^2 = 0$.\n- Increment definitions: $\\Delta u = u - u_n$, $\\Delta \\lambda = \\lambda - \\lambda_n$.\n- Parameters:\n  - $k_1 = 2$, $k_2 = 1$.\n  - $\\beta_1 = 1$, $\\beta_2 = 0$.\n  - $\\hat{P} = 5$.\n  - Current iterate (iteration $k$): $(u_k,\\lambda_k) = (1.0,\\,0.5)$.\n  - Last converged state (step $n$): $(u_n,\\lambda_n) = (0.7,\\,0.4)$.\n  - Arc-length parameters: $\\Delta s = 0.6$, $\\alpha = 2$.\n- Task: Perform one Newton-Raphson corrector iteration for $(\\delta u, \\delta \\lambda)$ and report $\\delta \\lambda$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding (Critical):** The problem describes a canonical one-degree-of-freedom system with cubic material nonlinearity, analyzed using the Newton-Raphson method combined with a Crisfield-type spherical arc-length constraint. This is a standard and fundamental topic in computational nonlinear mechanics. The formulation is scientifically sound.\n- **Well-Posed:** All necessary data and constitutive relations are provided. The objective is clearly stated. The problem is self-contained and structured to yield a unique solution for the requested quantity, assuming the Jacobian of the system is non-singular.\n- **Objective (Critical):** The problem is stated in precise mathematical and engineering terms, devoid of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nThe corrector iteration seeks an update $(\\delta u, \\delta \\lambda)$ from the current iterate $(u_k, \\lambda_k)$ to a new iterate $(u_{k+1}, \\lambda_{k+1}) = (u_k + \\delta u, \\lambda_k + \\delta \\lambda)$. This update is determined by linearizing the system of governing nonlinear equations, which consists of the equilibrium residual equation and the arc-length constraint equation.\n\nFirst, we formulate the specific internal force expression for the given system.\nThe force from each bar is $N_1(u) = k_1 u + \\beta_1 u^3$ and $N_2(u) = k_2 u + \\beta_2 u^3$.\nSubstituting the given parameters:\n$N_1(u) = 2u + 1u^3 = 2u + u^3$.\n$N_2(u) = 1u + 0u^3 = u$.\nThe total internal force is the sum:\n$f_{\\mathrm{int}}(u) = N_1(u) + N_2(u) = (2u + u^3) + u = 3u + u^3$.\n\nThe equilibrium residual is $R(u,\\lambda) = f_{\\mathrm{int}}(u) - \\lambda \\hat{P}$.\nWith the given parameters, this becomes:\n$R(u,\\lambda) = 3u + u^3 - 5\\lambda$.\n\nThe Newton-Raphson method linearizes the condition $R(u_{k+1}, \\lambda_{k+1}) = 0$ about the current iterate $(u_k, \\lambda_k)$.\n$R(u_k + \\delta u, \\lambda_k + \\delta \\lambda) \\approx R(u_k, \\lambda_k) + \\frac{\\partial R}{\\partial u}\\bigg|_{(u_k, \\lambda_k)} \\delta u + \\frac{\\partial R}{\\partial \\lambda}\\bigg|_{(u_k, \\lambda_k)} \\delta \\lambda = 0$.\nThis can be written as:\n$K_{T,k} \\delta u - \\hat{P} \\delta \\lambda = -R_k$,\nwhere $K_{T,k}$ is the tangent stiffness at $u_k$, and $R_k$ is the residual at $(u_k, \\lambda_k)$.\n\nWe compute the necessary terms at $(u_k, \\lambda_k) = (1.0, 0.5)$:\nThe tangent stiffness is the derivative of the internal force:\n$K_T(u) = \\frac{d f_{\\mathrm{int}}}{du} = \\frac{d}{du}(3u + u^3) = 3 + 3u^2$.\nAt $u_k=1.0$, the stiffness is $K_{T,k} = 3 + 3(1.0)^2 = 6$.\nThe residual at the current iterate is:\n$R_k = R(1.0, 0.5) = 3(1.0) + (1.0)^3 - 5(0.5) = 3 + 1 - 2.5 = 1.5$.\nThe derivative with respect to $\\lambda$ is $-\\hat{P} = -5$.\nSubstituting these values, the linearized equilibrium equation is:\n$6 \\delta u - 5 \\delta \\lambda = -1.5$. (Equation 1)\n\nNext, we linearize the arc-length constraint equation. The constraint must be satisfied by the updated state $(u_{k+1}, \\lambda_{k+1})$. The increments from the last converged state $(u_n, \\lambda_n)$ are:\n$\\Delta u_{k+1} = u_{k+1} - u_n = (u_k - u_n) + \\delta u = \\Delta u_k + \\delta u$.\n$\\Delta \\lambda_{k+1} = \\lambda_{k+1} - \\lambda_n = (\\lambda_k - \\lambda_n) + \\delta \\lambda = \\Delta \\lambda_k + \\delta \\lambda$.\nThe constraint is $g(\\Delta u_{k+1}, \\Delta \\lambda_{k+1}) = (\\Delta u_k + \\delta u)^2 + \\alpha^2 (\\Delta \\lambda_k + \\delta \\lambda)^2 - \\Delta s^2 = 0$.\nWe linearize this equation for $(\\delta u, \\delta \\lambda)$ around $(\\delta u, \\delta \\lambda)=(0,0)$:\n$g_k + \\frac{\\partial g}{\\partial (\\delta u)}\\bigg|_k \\delta u + \\frac{\\partial g}{\\partial (\\delta \\lambda)}\\bigg|_k \\delta \\lambda = 0$.\nThe terms are:\nCurrent total increments:\n$\\Delta u_k = u_k - u_n = 1.0 - 0.7 = 0.3$.\n$\\Delta \\lambda_k = \\lambda_k - \\lambda_n = 0.5 - 0.4 = 0.1$.\nThe constraint evaluated at the current iterate:\n$g_k = (\\Delta u_k)^2 + \\alpha^2 (\\Delta \\lambda_k)^2 - \\Delta s^2 = (0.3)^2 + (2)^2 (0.1)^2 - (0.6)^2 = 0.09 + 4(0.01) - 0.36 = 0.13 - 0.36 = -0.23$.\nThe partial derivatives are:\n$\\frac{\\partial g}{\\partial (\\delta u)}\\bigg|_k = 2(\\Delta u_k + \\delta u)|_{\\delta u=0} = 2\\Delta u_k = 2(0.3) = 0.6$.\n$\\frac{\\partial g}{\\partial (\\delta \\lambda)}\\bigg|_k = 2\\alpha^2(\\Delta \\lambda_k + \\delta \\lambda)|_{\\delta \\lambda=0} = 2\\alpha^2\\Delta \\lambda_k = 2(2)^2(0.1) = 2(4)(0.1) = 0.8$.\nThe linearized constraint equation is:\n$0.6 \\delta u + 0.8 \\delta \\lambda = -g_k = -(-0.23) = 0.23$. (Equation 2)\n\nWe now have a $2 \\times 2$ linear system for $(\\delta u, \\delta \\lambda)$:\n1. $6 \\delta u - 5 \\delta \\lambda = -1.5$\n2. $0.6 \\delta u + 0.8 \\delta \\lambda = 0.23$\n\nTo solve this system, we can multiply Equation 2 by $10$ to eliminate the decimal notation:\n2'. $6 \\delta u + 8 \\delta \\lambda = 2.3$\nNow, we subtract Equation 1 from Equation 2':\n$(6 \\delta u + 8 \\delta \\lambda) - (6 \\delta u - 5 \\delta \\lambda) = 2.3 - (-1.5)$.\n$8 \\delta \\lambda + 5 \\delta \\lambda = 2.3 + 1.5$.\n$13 \\delta \\lambda = 3.8$.\nSolving for $\\delta \\lambda$:\n$\\delta \\lambda = \\frac{3.8}{13} = \\frac{38}{130} = \\frac{19}{65}$.\n\nThe requested quantity is the scalar corrector to the load factor, $\\delta \\lambda$.", "answer": "$$\\boxed{\\frac{19}{65}}$$", "id": "2583322"}]}