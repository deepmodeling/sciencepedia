{"hands_on_practices": [{"introduction": "The Kantorovich theorem provides a powerful, quantitative criterion for guaranteeing the convergence of Newton's method from a specific starting point. It connects the properties of the nonlinear function at an initial guess—specifically, the invertibility of the Jacobian, the size of the initial residual, and the smoothness of the Jacobian (Lipschitz constant)—to establish a \"ball of attraction\" within which convergence is assured. This exercise [@problem_id:2549593] translates the theorem from an abstract statement into a concrete computational tool, developing a practical feel for what makes an initial guess \"good enough\" and how these distinct mathematical quantities combine to ensure convergence.", "problem": "Consider a nonlinear finite element (FE) discretization of a quasi-static equilibrium problem written in residual form as $F(u) = 0$, where $F : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is continuously differentiable and assembled from element contributions. Let Newton–Raphson (NR) iterations be applied starting from an initial guess $u_0 \\in \\mathbb{R}^{n}$, with Jacobian (tangent) $J(u) = \\nabla F(u)$. Work in the Euclidean norm $|\\cdot|$ on vectors and the associated induced spectral norm $\\|\\cdot\\|$ on matrices.\n\nSuppose that, for a particular FE model with two degrees of freedom, the assembled quantities at $u_0$ are\n$$\nJ(u_0) = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}, \n\\qquad\nF(u_0) = \\begin{pmatrix} -\\frac{1}{10} \\\\ \\frac{1}{20} \\end{pmatrix}.\n$$\nAssume further that, on the closed ball centered at $u_0$ of radius $1$ in $\\mathbb{R}^{2}$, the Jacobian is Lipschitz continuous with a computable global Lipschitz constant $L$ obtained directly from element-wise bounds by assembly: two elements contribute Lipschitz bounds $\\ell_1 = \\frac{3}{10}$ and $\\ell_2 = \\frac{1}{5}$, and the assembled bound satisfies $L \\le \\ell_1 + \\ell_2$.\n\nTasks:\n- Starting only from core definitions (the NR update, the induced matrix norm, invertibility via the smallest singular value, and Lipschitz continuity of $J$), derive computable expressions for the Kantorovich parameters directly from the assembled matrices and vectors at $u_0$:\n  1. $B := \\|J(u_0)^{-1}\\|$,\n  2. $\\eta := |J(u_0)^{-1} F(u_0)|$,\n  3. $L$ as an assembled Lipschitz bound.\n- Using these, formulate a practical scalar inequality that decides whether to accept or reject $u_0$ as an NR initial guess under the Kantorovich framework, expressed in terms of $B$, $L$, and $\\eta$.\n- Compute the scalar quantity $h$ that enters this inequality, expressed exactly in closed form.\n\nProvide your final answer as the exact closed-form expression for $h$. No rounding is required. The final answer must be a single mathematical expression with no units.", "solution": "The objective is to compute the Kantorovich parameter $h$ for a given nonlinear problem being solved with the Newton-Raphson (NR) method. The NR iteration to solve the system $F(u)=0$ is defined by the sequence\n$$u_{k+1} = u_k - J(u_k)^{-1} F(u_k)$$\nwhere $J(u) = \\nabla F(u)$ is the Jacobian of the residual vector $F(u)$.\n\nThe Kantorovich theorem guarantees convergence from an initial guess $u_0$ if the condition $h \\le \\frac{1}{2}$ is met, where $h$ is a dimensionless quantity defined as\n$$h := B L \\eta.$$\nThe parameters are defined at the initial point $u_0$:\n1. $L$ is a Lipschitz constant for the Jacobian in a neighborhood of $u_0$, such that $\\|J(u) - J(v)\\| \\le L|u-v|$ for $u, v$ in the specified ball.\n2. $B := \\|J(u_0)^{-1}\\|$, where $\\|\\cdot\\|$ is the spectral norm (induced by the Euclidean vector norm).\n3. $\\eta := |J(u_0)^{-1} F(u_0)|$, where $|\\cdot|$ is the Euclidean vector norm.\n\nThe problem requires deriving computable expressions for these parameters and then calculating the value of $h$. We shall compute each parameter systematically.\n\nFirst, we calculate the Lipschitz constant $L$. The problem provides element-wise bounds $\\ell_1 = \\frac{3}{10}$ and $\\ell_2 = \\frac{1}{5}$, and an assembly rule $L \\le \\ell_1 + \\ell_2$. We use the upper bound as our computable value for $L$:\n$$ L = \\ell_1 + \\ell_2 = \\frac{3}{10} + \\frac{1}{5} = \\frac{3}{10} + \\frac{2}{10} = \\frac{5}{10} = \\frac{1}{2}. $$\n\nSecond, we calculate $B = \\|J(u_0)^{-1}\\|$. The initial Jacobian is given as\n$$ J(u_0) = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}. $$\nThe determinant is $\\det(J(u_0)) = (3)(2) - (1)(1) = 5$. The inverse matrix is\n$$ J(u_0)^{-1} = \\frac{1}{5} \\begin{pmatrix} 2  -1 \\\\ -1  3 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{3}{5} \\end{pmatrix}. $$\nSince $J(u_0)$ is a real symmetric matrix, its inverse $J(u_0)^{-1}$ is also symmetric. The spectral norm of a symmetric matrix is its spectral radius, i.e., the maximum of the absolute values of its eigenvalues. The eigenvalues of $J(u_0)^{-1}$ are the reciprocals of the eigenvalues of $J(u_0)$. We find the eigenvalues $\\mu$ of $J(u_0)$ using its characteristic equation $\\det(J(u_0) - \\mu I) = 0$:\n$$ (3 - \\mu)(2 - \\mu) - (1)(1) = 0 $$\n$$ \\mu^2 - 5\\mu + 6 - 1 = 0 $$\n$$ \\mu^2 - 5\\mu + 5 = 0. $$\nUsing the quadratic formula, the eigenvalues of $J(u_0)$ are $\\mu = \\frac{5 \\pm \\sqrt{(-5)^2 - 4(1)(5)}}{2} = \\frac{5 \\pm \\sqrt{5}}{2}$.\nThe smallest eigenvalue is $\\mu_{\\min} = \\frac{5 - \\sqrt{5}}{2}$. The spectral norm of $J(u_0)^{-1}$ is the reciprocal of the smallest eigenvalue of $J(u_0)$ (since $J(u_0)$ is positive definite):\n$$ B = \\|J(u_0)^{-1}\\| = \\frac{1}{\\mu_{\\min}} = \\frac{1}{\\frac{5 - \\sqrt{5}}{2}} = \\frac{2}{5 - \\sqrt{5}}. $$\nTo rationalize the denominator, we multiply the numerator and denominator by the conjugate $5 + \\sqrt{5}$:\n$$ B = \\frac{2(5 + \\sqrt{5})}{(5 - \\sqrt{5})(5 + \\sqrt{5})} = \\frac{2(5 + \\sqrt{5})}{5^2 - (\\sqrt{5})^2} = \\frac{2(5 + \\sqrt{5})}{25 - 5} = \\frac{2(5 + \\sqrt{5})}{20} = \\frac{5 + \\sqrt{5}}{10}. $$\n\nThird, we calculate $\\eta = |J(u_0)^{-1} F(u_0)|$. The initial residual vector is $F(u_0) = \\begin{pmatrix} -\\frac{1}{10} \\\\ \\frac{1}{20} \\end{pmatrix}$. We compute the product $v = J(u_0)^{-1} F(u_0)$:\n$$ v = \\begin{pmatrix} \\frac{2}{5}  -\\frac{1}{5} \\\\ -\\frac{1}{5}  \\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{10} \\\\ \\frac{1}{20} \\end{pmatrix} = \\begin{pmatrix} (\\frac{2}{5})(-\\frac{1}{10}) + (-\\frac{1}{5})(\\frac{1}{20}) \\\\ (-\\frac{1}{5})(-\\frac{1}{10}) + (\\frac{3}{5})(\\frac{1}{20}) \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{50} - \\frac{1}{100} \\\\ \\frac{1}{50} + \\frac{3}{100} \\end{pmatrix} = \\begin{pmatrix} -\\frac{4}{100} - \\frac{1}{100} \\\\ \\frac{2}{100} + \\frac{3}{100} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{100} \\\\ \\frac{5}{100} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{20} \\\\ \\frac{1}{20} \\end{pmatrix}. $$\nThe parameter $\\eta$ is the Euclidean norm of this vector:\n$$ \\eta = |v| = \\sqrt{\\left(-\\frac{1}{20}\\right)^2 + \\left(\\frac{1}{20}\\right)^2} = \\sqrt{\\frac{1}{400} + \\frac{1}{400}} = \\sqrt{\\frac{2}{400}} = \\frac{\\sqrt{2}}{20}. $$\n\nFinally, we assemble the scalar quantity $h$ from the computed parameters $B$, $L$, and $\\eta$. The practical inequality to decide on the initial guess is $h \\le \\frac{1}{2}$. We compute $h$:\n$$ h = B L \\eta = \\left(\\frac{5 + \\sqrt{5}}{10}\\right) \\left(\\frac{1}{2}\\right) \\left(\\frac{\\sqrt{2}}{20}\\right) $$\n$$ h = \\frac{(5 + \\sqrt{5})\\sqrt{2}}{(10)(2)(20)} = \\frac{(5 + \\sqrt{5})\\sqrt{2}}{400}. $$\nThis is the required closed-form expression for the scalar quantity $h$.", "answer": "$$\\boxed{\\frac{\\sqrt{2}(5 + \\sqrt{5})}{400}}$$", "id": "2549593"}, {"introduction": "The convergence of iterative solvers for systems arising from mixed finite element methods is deeply tied to the stability of the underlying discretization. Key stability conditions, such as coercivity, are not just abstract theoretical requirements; their absence in the discrete algebraic system can directly prevent iterative methods from converging. This practice [@problem_id:2549618] provides a clear counterexample demonstrating this crucial link, showing firsthand how an unstable formulation manifests as solver divergence and how to diagnose such issues by inspecting the spectral properties of the iteration matrix.", "problem": "You are to construct and analyze a minimal algebraic mixed formulation that mimics a discrete mixed finite element system and to use it to demonstrate that lack of coercivity can cause failure of nonlinear fixed-point iterations even when the residual is locally Lipschitz continuous. Your task has three parts: define the residual, implement a fixed-point iteration, and evaluate convergence and linearized contractivity across specified test cases, all in a fully reproducible program.\n\nBegin from the following foundational facts and definitions, without assuming any shortcut formulas. In a mixed formulation, the algebraic residual couples a \"primal\" variable and a \"Lagrange multiplier\" (or pressure-like) variable through a saddle-point structure. Coercivity on the kernel and the inf-sup condition are the conditions that guarantee stability of the mixed operator. A mapping is locally Lipschitz if, on a neighborhood of a point, the mapping’s increment is bounded by a constant multiple of the increment of its argument. The Banach fixed-point theorem states that a contraction mapping on a complete metric space has a unique fixed point and that iterative application converges to it. For a nonlinear residual iteration of the form $x^{k+1} = T(x^k)$, a sufficient local condition for convergence is that the derivative $DT(x^\\star)$ have spectral radius less than $1$ at a fixed point $x^\\star$. Your implementation must rely only on these principles.\n\nDefine the following $2 \\times 1$-variable residual for $x = (u,p) \\in \\mathbb{R}^2$, with parameters $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$, $\\alpha \\in \\mathbb{R}$, and $\\mu \\in \\mathbb{R}$:\n$$\nR(u,p) =\n\\begin{bmatrix}\nR_u(u,p) \\\\\nR_p(u,p)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na\\,u - p + \\mu \\tanh(u) \\\\\nb\\,u - \\alpha\\, p\n\\end{bmatrix}.\n$$\nThis residual is globally Lipschitz in $(u,p)$ for any fixed parameters $(a,b,\\alpha,\\mu)$ because the hyperbolic tangent function is globally Lipschitz. Interpret $a$ as the coefficient of a primal bilinear form (coercivity surrogate), $b$ as a coupling strength (inf-sup surrogate), and $\\alpha$ as a stabilization parameter. The case $a = 0$ and $\\alpha = 0$ represents lack of coercivity and lack of stabilization in the mixed formulation. You will apply a simple nonlinear residual fixed-point iteration\n$$\n\\begin{bmatrix}\nu^{k+1} \\\\\np^{k+1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nu^{k} \\\\\np^{k}\n\\end{bmatrix}\n- \\tau \\,\nR\\!\\left(u^{k},p^{k}\\right),\n$$\nwith a given step size $\\tau  0$, and you will also analyze the linearized iteration matrix near the origin to assess contractivity.\n\nImplementation requirements:\n- Use the iteration above with initial guess $(u^0,p^0) = (1,1)$. Use Euclidean norm for vectors. Use tolerance $\\mathrm{tol} = 10^{-8}$ on the residual norm $\\|R(u^k,p^k)\\|_2$. Declare convergence if $\\|R(u^k,p^k)\\|_2 \\le \\mathrm{tol}$ within at most $N_{\\max} = 500$ iterations. If not, declare nonconvergence.\n- For each parameter set, compute the Jacobian $J(0)$ of $R$ at $(u,p) = (0,0)$ and then compute the spectral radius $\\rho$ of the linearized fixed-point iteration matrix $I - \\tau J(0)$, where $I$ is the identity matrix and $\\tau$ is the given step size. The Jacobian must be derived from first principles and computed exactly; do not approximate it numerically.\n- The purpose is to furnish a counterexample: show at least one case where the residual is locally Lipschitz yet the nonlinear iteration fails to converge due to lack of coercivity in the mixed formulation.\n\nTest suite:\nProvide results for the following three parameter sets and step sizes, which cover a strongly noncoercive case, a nearly noncoercive case, and a stabilized/coercive case.\n\n- Case A (noncoercive, no stabilization): $a = 0$, $b = 4$, $\\alpha = 0$, $\\mu = 0.5$, $\\tau = 1$.\n- Case B (nearly noncoercive, no stabilization): $a = 0.1$, $b = 4$, $\\alpha = 0$, $\\mu = 0.5$, $\\tau = 1$.\n- Case C (stabilized/coercive): $a = 1$, $b = 1$, $\\alpha = 1$, $\\mu = 0.5$, $\\tau = 0.5$.\n\nFinal output format:\nYour program should produce a single line of output containing a comma-separated list enclosed in square brackets. For each test case (in the order A, B, C), output first a boolean indicating whether the iteration converged, followed by a floating-point number giving the spectral radius $\\rho$ of $I - \\tau J(0)$. The final output will therefore have six entries of the form\n$[ \\text{flag}_A, \\rho_A, \\text{flag}_B, \\rho_B, \\text{flag}_C, \\rho_C ]$,\nwhere $\\text{flag}_\\cdot$ are booleans and $\\rho_\\cdot$ are floats.\n\nAll quantities are nondimensional and require no physical units. Angles in any trigonometric functions are interpreted in radians by default. The only acceptance criteria for correctness are numerical: your booleans and computed spectral radii for the specified test suite will be validated.", "solution": "The objective is to analyze a simple fixed-point iteration for a nonlinear system that models a mixed formulation. The analysis comprises two parts: 1) numerical execution of the iteration to determine convergence from a specified starting point and 2) local convergence analysis at the known fixed point $(u,p) = (0,0)$ by computing the spectral radius of the linearized iteration map.\n\nThe iteration is a nonlinear map $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$ such that $x^{k+1} = T(x^k)$, where $x = (u,p)^T$ and the map is defined as:\n$$\nT(x) = x - \\tau R(x)\n$$\nA fixed point $x^*$ of this map satisfies $x^* = T(x^*)$, which implies $R(x^*) = 0$. We can verify that $x^* = (0,0)^T$ is a fixed point for any choice of parameters:\n$$\nR(0,0) =\n\\begin{bmatrix}\na(0) - 0 + \\mu \\tanh(0) \\\\\nb(0) - \\alpha(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}\n$$\nAccording to the Banach fixed-point theorem, the iteration $x^{k+1} = T(x^k)$ is guaranteed to converge to a unique fixed point $x^*$ in a complete metric space if $T$ is a contraction mapping on that space. For a differentiable map on $\\mathbb{R}^n$, a sufficient condition for local convergence in a neighborhood of $x^*$ is that the spectral radius of the Jacobian of the map $T$, evaluated at $x^*$, is less than $1$. The Jacobian of $T(x)$ is $DT(x) = I - \\tau DR(x)$, where $I$ is the $2 \\times 2$ identity matrix and $DR(x)$ is the Jacobian of the residual $R(x)$, which we denote as $J(x)$. Thus, the condition for local convergence near $x^*$ is $\\rho(I - \\tau J(x^*))  1$.\n\nWe first derive the Jacobian of the residual $R(u,p)$. The partial derivatives are computed as follows, using the fact that $\\frac{d}{dz}\\tanh(z) = \\text{sech}^2(z)$:\n$$\nJ(u,p) = DR(u,p) =\n\\begin{bmatrix}\n\\frac{\\partial R_u}{\\partial u}  \\frac{\\partial R_u}{\\partial p} \\\\\n\\frac{\\partial R_p}{\\partial u}  \\frac{\\partial R_p}{\\partial p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\na + \\mu \\, \\text{sech}^2(u)  -1 \\\\\nb  -\\alpha\n\\end{bmatrix}\n$$\nThe local convergence analysis is performed at the fixed point $(u,p) = (0,0)$. We evaluate the Jacobian at this point, noting that $\\text{sech}(0) = 1$:\n$$\nJ(0) = J(0,0) =\n\\begin{bmatrix}\na + \\mu  -1 \\\\\nb  -\\alpha\n\\end{bmatrix}\n$$\nThe linearized iteration matrix at the origin is therefore:\n$$\nM = I - \\tau J(0) =\n\\begin{bmatrix}\n1  0 \\\\\n0  1\n\\end{bmatrix}\n- \\tau\n\\begin{bmatrix}\na + \\mu  -1 \\\\\nb  -\\alpha\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 - \\tau(a + \\mu)  \\tau \\\\\n-\\tau b  1 + \\tau \\alpha\n\\end{bmatrix}\n$$\nThe spectral radius $\\rho(M)$ is the maximum absolute value of the eigenvalues of $M$. An eigenvalue $\\lambda$ of $M$ satisfies the characteristic equation $\\det(M - \\lambda I) = 0$.\n\nWe now analyze each test case.\n\n**Case A: Noncoercive, no stabilization**\nParameters: $a = 0$, $b = 4$, $\\alpha = 0$, $\\mu = 0.5$, $\\tau = 1$.\nThe iteration from $(u^0, p^0) = (1,1)$ is performed numerically. The result is non-convergence within the $N_{\\max} = 500$ iteration limit.\nFor the local analysis, we construct the matrix $M$:\n$$\nM_A =\n\\begin{bmatrix}\n1 - 1(0 + 0.5)  1 \\\\\n-1(4)  1 + 1(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.5  1 \\\\\n-4  1\n\\end{bmatrix}\n$$\nThe eigenvalues $\\lambda$ are roots of $\\lambda^2 - \\text{tr}(M_A)\\lambda + \\det(M_A) = 0$.\n$\\text{tr}(M_A) = 0.5 + 1 = 1.5$.\n$\\det(M_A) = (0.5)(1) - (1)(-4) = 0.5 + 4 = 4.5$.\nThe characteristic equation is $\\lambda^2 - 1.5\\lambda + 4.5 = 0$.\nThe eigenvalues are $\\lambda = \\frac{1.5 \\pm \\sqrt{1.5^2 - 4(4.5)}}{2} = \\frac{1.5 \\pm \\sqrt{2.25 - 18}}{2} = 0.75 \\pm i\\frac{\\sqrt{15.75}}{2}$.\nThe spectral radius is the magnitude of these complex conjugate eigenvalues:\n$\\rho(M_A) = \\sqrt{(0.75)^2 + (\\frac{\\sqrt{15.75}}{2})^2} = \\sqrt{0.5625 + \\frac{15.75}{4}} = \\sqrt{0.5625 + 3.9375} = \\sqrt{4.5}$.\n$\\rho_A = \\sqrt{4.5} \\approx 2.1213$. Since $\\rho_A > 1$, the linearized analysis predicts instability at the origin, which is consistent with the observed numerical non-convergence. This case serves as the required counterexample of an iteration failing for a noncoercive system.\n\n**Case B: Nearly noncoercive, no stabilization**\nParameters: $a = 0.1$, $b = 4$, $\\alpha = 0$, $\\mu = 0.5$, $\\tau = 1$.\nThe numerical iteration from $(1,1)$ also results in non-convergence.\nFor local analysis:\n$$\nM_B =\n\\begin{bmatrix}\n1 - 1(0.1 + 0.5)  1 \\\\\n-1(4)  1 + 1(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4  1 \\\\\n-4  1\n\\end{bmatrix}\n$$\n$\\text{tr}(M_B) = 0.4 + 1 = 1.4$.\n$\\det(M_B) = (0.4)(1) - (1)(-4) = 0.4 + 4 = 4.4$.\nThe characteristic equation is $\\lambda^2 - 1.4\\lambda + 4.4 = 0$.\nThe eigenvalues are $\\lambda = \\frac{1.4 \\pm \\sqrt{1.4^2 - 4(4.4)}}{2} = 0.7 \\pm i\\frac{\\sqrt{15.64}}{2}$.\nThe spectral radius is $\\rho(M_B) = \\sqrt{(0.7)^2 + (\\frac{\\sqrt{15.64}}{2})^2} = \\sqrt{0.49 + 3.91} = \\sqrt{4.4}$.\n$\\rho_B = \\sqrt{4.4} \\approx 2.0976$. Again, $\\rho_B > 1$, correctly predicting local instability.\n\n**Case C: Stabilized/coercive**\nParameters: $a = 1$, $b = 1$, $\\alpha = 1$, $\\mu = 0.5$, $\\tau = 0.5$.\nThe numerical iteration from $(1,1)$ is found to converge.\nFor local analysis:\n$$\nM_C =\n\\begin{bmatrix}\n1 - 0.5(1 + 0.5)  0.5 \\\\\n-0.5(1)  1 + 0.5(1)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 - 0.75  0.5 \\\\\n-0.5  1.5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.25  0.5 \\\\\n-0.5  1.5\n\\end{bmatrix}\n$$\n$\\text{tr}(M_C) = 0.25 + 1.5 = 1.75$.\n$\\det(M_C) = (0.25)(1.5) - (0.5)(-0.5) = 0.375 + 0.25 = 0.625$.\nThe characteristic equation is $\\lambda^2 - 1.75\\lambda + 0.625 = 0$.\nThe eigenvalues are $\\lambda = \\frac{1.75 \\pm \\sqrt{1.75^2 - 4(0.625)}}{2} = \\frac{1.75 \\pm \\sqrt{3.0625 - 2.5}}{2} = \\frac{1.75 \\pm \\sqrt{0.5625}}{2} = \\frac{1.75 \\pm 0.75}{2}$.\nThe eigenvalues are real: $\\lambda_1 = \\frac{2.5}{2} = 1.25$ and $\\lambda_2 = \\frac{1}{2} = 0.5$.\nThe spectral radius is the maximum of their absolute values:\n$\\rho_C = \\max(|1.25|, |0.5|) = 1.25$.\nIn this case, $\\rho_C > 1$, so the iteration is locally unstable around the origin for the given step size $\\tau=0.5$. However, the numerical simulation shows convergence. This highlights an important concept: local stability analysis only guarantees convergence within an (often unknown) neighborhood of the fixed point. The global behavior of the iteration can be different. The stability of the underlying continuous problem (provided by non-zero $a$ and $\\alpha$) allows the iteration to converge from the starting point $(1,1)$, despite the choice of $\\tau$ being too large for local contractivity at the origin. The collection of results successfully demonstrates that the lack of coercivity (Case A) can lead to iteration failure, in agreement with the local stability analysis, while a coercive system (Case C) may converge even if the local stability condition is not met for a specific step size.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the nonlinear iteration problem for three test cases and\n    computes the spectral radius of the linearized iteration matrix at the origin.\n    \"\"\"\n    # Define the test cases as per the problem statement.\n    # (a, b, alpha, mu, tau)\n    test_cases = [\n        # Case A (noncoercive, no stabilization)\n        (0.0, 4.0, 0.0, 0.5, 1.0),\n        # Case B (nearly noncoercive, no stabilization)\n        (0.1, 4.0, 0.0, 0.5, 1.0),\n        # Case C (stabilized/coercive)\n        (1.0, 1.0, 1.0, 0.5, 0.5),\n    ]\n\n    results = []\n    tol = 1e-8\n    n_max = 500\n\n    for case in test_cases:\n        a, b, alpha, mu, tau = case\n        \n        # Part 1: Perform the fixed-point iteration\n        u, p = 1.0, 1.0\n        converged = False\n        for _ in range(n_max):\n            # Calculate residual R(u, p)\n            r_u = a * u - p + mu * np.tanh(u)\n            r_p = b * u - alpha * p\n            residual_vec = np.array([r_u, r_p])\n            \n            # Check for convergence using Euclidean norm\n            if np.linalg.norm(residual_vec) = tol:\n                converged = True\n                break\n            \n            # Update variables\n            u = u - tau * r_u\n            p = p - tau * r_p\n            \n            # Check for divergence/NaN\n            if not np.isfinite(u) or not np.isfinite(p):\n                break\n        \n        # Part 2: Compute spectral radius of the linearized iteration matrix\n        # Jacobian of the residual R at (0, 0) is J(0)\n        # J(0) = [[a + mu, -1], [b, -alpha]]\n        J0 = np.array([[a + mu, -1.0], [b, -alpha]])\n        \n        # Linearized fixed-point iteration matrix is M = I - tau * J(0)\n        M = np.identity(2) - tau * J0\n        \n        # Eigenvalues of M\n        eigenvalues = np.linalg.eigvals(M)\n        \n        # Spectral radius is the maximum absolute value of the eigenvalues\n        rho = np.max(np.abs(eigenvalues))\n        \n        # Store results for this case\n        results.extend([str(converged).lower(), rho])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2549618"}, {"introduction": "In practical scientific computing, the \"exact\" Jacobian required by Newton's method is often unavailable or contaminated by numerical errors from sources like numerical quadrature. While Newton's method offers quadratic convergence in theory, its performance can degrade significantly with an inaccurate Jacobian, whereas quasi-Newton methods, which build an approximation of the Jacobian, can be more robust. This advanced exercise [@problem_id:2549625] simulates a realistic scenario where a sophisticated quasi-Newton solver with curvature averaging outperforms a noisy Newton method, providing insight into the practical trade-offs between convergence rate and robustness in modern nonlinear solver design.", "problem": "You are asked to design and compare nonlinear solvers for a one-dimensional nonlinear finite element discretization in order to investigate convergence criteria for nonlinear iterations, and to demonstrate a setting where a quasi-Newton method outperforms a nominal Newton method when the Jacobian matrix suffers from quadrature-induced noise. Start from the following foundational base: the Galerkin finite element method defines a residual operator as the weak-form balance of internal and external contributions, and Newton’s method seeks a root of a nonlinear vector equation by solving a linearization at each iteration.\n\nConsider the nonlinear diffusion-reaction boundary value problem on the domain $[0,1]$ with Dirichlet boundary conditions. Let the exact manufactured solution be $u(x)=x$. Define the nonlinear diffusion coefficient $k(u)=1+\\alpha \\sin(\\beta u)$ and a reaction term coefficient $\\gamma$. Choose the body force $g(x)$ so that $u(x)=x$ is the exact solution of the strong form $-\\dfrac{d}{dx}\\left(k(u)\\dfrac{du}{dx}\\right)+\\gamma u^3=g(x)$. Use a standard one-dimensional linear finite element discretization with a uniform mesh and nodal unknowns at interior nodes only; impose Dirichlet boundary conditions strongly at the endpoints.\n\nConstruct the discrete residual vector $R(u)\\in\\mathbb{R}^n$ as the consistent Galerkin assembly of the element contributions, where the integrals are evaluated by a sufficiently accurate Gaussian quadrature for the residual. The discrete Jacobian matrix $J(u)\\in\\mathbb{R}^{n\\times n}$ is the exact derivative of $R(u)$ with respect to the nodal unknowns. To model quadrature-induced noise in the Jacobian encountered in practical implementations, the Newton method to be implemented must assemble its Jacobian at each iteration using under-integration and multiplicative perturbations at the quadrature-point level; the residual must always be assembled with higher-order quadrature without noise. The quasi-Newton method to be implemented must not reassemble the Jacobian; instead, it must update an inverse-Jacobian approximation by a good Broyden-type secant update, in which the curvature vector differences are smoothed by an exponential moving average (curvature averaging).\n\nImplement the following two solvers for the nonlinear system $R(u)=0$.\n\n- Solver A (noisy-J Newton): At iteration $k$, compute a search direction $s_k$ by solving $J(u_k)s_k=-R(u_k)$, where $J(u_k)$ is assembled with under-integration and quadrature-point multiplicative noise, independently re-realized at each iteration. Use a backtracking line search on the merit function $\\phi(u)=\\tfrac{1}{2}\\lVert R(u)\\rVert_2^2$ to ensure monotone decrease. Apply a small Tikhonov regularization to $J(u_k)$ only if the linear solve is ill-conditioned. Use the same convergence criteria as the quasi-Newton method.\n\n- Solver B (quasi-Newton with curvature averaging): Initialize an inverse-Jacobian approximation from the initial noisy Jacobian and update it by a good Broyden secant formula using curvature differences $y_k=R(u_{k+1})-R(u_k)$ that have been smoothed by exponential averaging $y_k^{\\text{avg}}=(1-\\eta)\\,y_k+\\eta\\,y_{k-1}^{\\text{avg}}$ with a prescribed smoothing parameter $\\eta\\in(0,1)$. Use the same backtracking line search on $\\phi(u)$ as in Solver A.\n\nConvergence criteria for nonlinear iterations. Let $u_0$ be the initial guess and $R_0=R(u_0)$. Declare convergence if both of the following criteria are satisfied:\n- Residual criterion: $\\lVert R(u_k)\\rVert_2 \\le \\text{abs\\_tol} + \\text{rel\\_tol}\\,\\lVert R_0\\rVert_2$.\n- Step criterion: $\\lVert s_k\\rVert_2 \\le \\text{step\\_tol}\\,\\bigl(1+\\lVert u_k\\rVert_2\\bigr)$.\nIf either solver fails to meet the criteria within $\\text{max\\_iters}$ iterations, declare it non-convergent.\n\nBacktracking line search. For a trial step size $t\\in(0,1]$, accept $u_{k+1}=u_k+t s_k$ if $\\phi(u_{k+1})\\phi(u_k)$. Otherwise, decrease $t$ by a fixed factor until either acceptance or reaching a minimum step size. Use identical line-search parameters for both solvers.\n\nFinite element and model parameters. Use the following fixed data for all test cases:\n- Mesh: uniform partition of $[0,1]$ into $n_e=20$ linear elements, with Dirichlet values $u(0)=0$ and $u(1)=1$. Interior unknowns have size $n=n_e-1$.\n- Coefficients: $\\alpha=0.5$, $\\beta=6$, $\\gamma=1$.\n- Manufactured forcing: choose $g(x)$ so that $u(x)=x$ satisfies the strong form $-\\dfrac{d}{dx}\\left(k(u)\\dfrac{du}{dx}\\right)+\\gamma u^3=g(x)$. Explicitly, with $u(x)=x$ and $\\dfrac{du}{dx}=1$, enforce $g(x)=-k'(u)\\dfrac{du}{dx}+\\gamma u^3$ evaluated at $u=x$.\n- Quadrature for residual: Gaussian quadrature with $n_Q^R=5$ points per element.\n- Quadrature for Jacobian in Solver A: if under-integration is enabled, take $n_Q^J=1$; otherwise take $n_Q^J=5$.\n- Quadrature noise model for Solver A: at each Jacobian assembly, independently perturb each quadrature-point contribution multiplicatively by a factor $1+\\sigma\\,\\xi$, where $\\xi$ is drawn from a standard normal distribution and $\\sigma$ is a prescribed noise amplitude.\n- Quasi-Newton curvature averaging: exponential moving average with weight $\\eta=0.5$.\n\nNonlinear iteration parameters. Use the same parameters for both solvers:\n- Initial guess: $u_0$ equal to the zero vector on interior nodes.\n- Tolerances: $\\text{abs\\_tol}=10^{-10}$, $\\text{rel\\_tol}=10^{-8}$, $\\text{step\\_tol}=10^{-12}$.\n- Line search: initial step size $t=1$, backtracking factor $0.5$, minimum step size $10^{-8}$.\n- Maximum iterations: $\\text{max\\_iters}=50$.\n\nTest suite. Run the solvers on the following four cases, each defined by a pair $(\\sigma, \\text{under\\_integrate})$:\n1. $(0.0,\\text{False})$.\n2. $(10^{-2},\\text{False})$.\n3. $(5\\times 10^{-2},\\text{True})$.\n4. $(10^{-1},\\text{True})$.\n\nFor each case, run Solver A and Solver B with identical settings and initial guess. Define a boolean outcome per case indicating whether Solver B is more robust than Solver A, with the following rule: the outcome is true if Solver B converges and either Solver A does not converge or Solver B uses strictly fewer nonlinear iterations than Solver A; otherwise it is false.\n\nRequired final output format. Your program should produce a single line of output containing the four boolean outcomes as a comma-separated list enclosed in square brackets (e.g., $[{\\rm True},{\\rm False},{\\rm True},{\\rm True}]$). No other text should be printed. No physical units or angles are involved in this problem, so no unit conversion is required.\n\nYour tasks:\n- Derive the manufactured forcing $g(x)$ from the strong form for $u(x)=x)$ starting from the given definitions of $k(u)$ and $\\gamma$.\n- Specify the discrete residual assembly for linear elements and explain how the residual-based and step-based convergence criteria encode a mathematically sound stopping rule.\n- Explain why a Jacobian assembled with under-integration and multiplicative perturbations can degrade Newton’s method, and articulate the mechanism by which quasi-Newton updates with curvature averaging can outperform the noisy-J Newton method.\n- Implement the program exactly as specified, apply it to the test suite, and output the boolean list in the required format.", "solution": "The analysis proceeds in three parts. First, the manufactured source term required for the verification exercise is derived. Second, the finite element discretization is detailed, leading to the discrete residual and Jacobian, and the rationale behind the convergence criteria is explained. Third, the mechanisms of the two nonlinear solvers are contrasted, clarifying why a quasi-Newton method with curvature smoothing can be more robust than a standard Newton method when the Jacobian is subject to noise.\n\n**1. Derivation of the Manufactured Source Term**\n\nThe problem is governed by the nonlinear diffusion-reaction equation on the domain $[0,1]$:\n$$-\\dfrac{d}{dx}\\left(k(u)\\dfrac{du}{dx}\\right) + \\gamma u^3 = g(x)$$\nwith the nonlinear diffusion coefficient defined as $k(u) = 1 + \\alpha \\sin(\\beta u)$. We are tasked to find the source term $g(x)$ such that the function $u(x) = x$ is an exact solution.\n\nFirst, we evaluate the derivatives of $u(x)$ and $k(u(x))$:\n- Solution: $u(x) = x$\n- Derivative of solution: $\\dfrac{du}{dx} = 1$\n- Diffusion coefficient evaluated at the solution: $k(u(x)) = k(x) = 1 + \\alpha \\sin(\\beta x)$\n- Derivative of the diffusion coefficient with respect to its argument $u$: $k'(u) = \\dfrac{d k}{d u} = \\alpha \\beta \\cos(\\beta u)$. Evaluated at the solution, $k'(u(x)) = \\alpha \\beta \\cos(\\beta x)$.\n\nNow, we compute the divergence term in the governing equation using the chain rule:\n$$\\dfrac{d}{dx}\\left(k(u)\\dfrac{du}{dx}\\right) = \\dfrac{d}{dx}\\left(k(u(x))\\right)\\dfrac{du}{dx} + k(u(x))\\dfrac{d^2u}{dx^2}$$\nUsing the chain rule again for the first term, $\\dfrac{d}{dx}(k(u(x))) = k'(u(x)) \\dfrac{du}{dx}$.\nSubstituting the known expressions for $u(x)=x$:\n- $\\dfrac{du}{dx} = 1$\n- $\\dfrac{d^2u}{dx^2} = 0$\n- $k'(u(x)) = \\alpha \\beta \\cos(\\beta x)$\n\nThe divergence term thus simplifies to:\n$$\\dfrac{d}{dx}\\left(k(u)\\dfrac{du}{dx}\\right) = (\\alpha \\beta \\cos(\\beta x)) \\cdot (1) + (1 + \\alpha \\sin(\\beta x)) \\cdot (0) = \\alpha \\beta \\cos(\\beta x)$$\n\nSubstituting this result and $u(x)=x$ back into the strong form of the governing equation gives:\n$$-(\\alpha \\beta \\cos(\\beta x)) + \\gamma x^3 = g(x)$$\nTherefore, the manufactured source term is:\n$$g(x) = \\gamma x^3 - \\alpha \\beta \\cos(\\beta x)$$\nWith the given parameters $\\alpha=0.5$, $\\beta=6$, and $\\gamma=1$, this becomes:\n$$g(x) = x^3 - 3 \\cos(6x)$$\n\n**2. Finite Element Discretization and Convergence Criteria**\n\nThe finite element method begins with the weak form of the boundary value problem. We seek a solution $u$ satisfying the Dirichlet boundary conditions such that for all admissible test functions $v$, the following integral equation holds:\n$$\\int_0^1 k(u) \\dfrac{du}{dx} \\dfrac{dv}{dx} \\,dx + \\int_0^1 \\gamma u^3 v \\,dx = \\int_0^1 g(x) v \\,dx$$\n\nThe domain is discretized into $n_e$ linear finite elements. The solution $u(x)$ is approximated by a piecewise linear function $u_h(x) = \\sum_{j=0}^{n_e} U_j \\phi_j(x)$, where $\\phi_j(x)$ are the standard linear \"hat\" basis functions and $U_j$ are the nodal values. The boundary conditions $u(0)=0$ and $u(1)=1$ are enforced strongly, meaning $U_0=0$ and $U_{n_e}=1$. The unknown variables are the $n = n_e-1$ interior nodal values, which we denote by the vector $\\mathbf{u} \\in \\mathbb{R}^n$.\n\nThe Galerkin method sets the test functions $v$ to be the basis functions $\\phi_i(x)$ corresponding to the interior nodes ($i=1, \\dots, n$). This yields a system of $n$ nonlinear algebraic equations for the vector of unknowns $\\mathbf{u}$. The $i$-th equation, representing the residual at node $i$, is given by:\n$$R_i(\\mathbf{u}) = \\int_0^1 \\left( k(u_h) \\dfrac{du_h}{dx} \\dfrac{d\\phi_i}{dx} + \\gamma u_h^3 \\phi_i - g(x) \\phi_i \\right) \\,dx = 0$$\nThe integrals are computed numerically using Gaussian quadrature over each element. The Jacobian matrix of this system, $J(\\mathbf{u})$, has entries $J_{ij}(\\mathbf{u}) = \\dfrac{\\partial R_i}{\\partial u_j}$. Differentiating the residual expression gives:\n$$J_{ij}(\\mathbf{u}) = \\int_0^1 \\left( k'(u_h)\\phi_j \\dfrac{du_h}{dx} \\dfrac{d\\phi_i}{dx} + k(u_h) \\dfrac{d\\phi_j}{dx} \\dfrac{d\\phi_i}{dx} + 3\\gamma u_h^2 \\phi_j \\phi_i \\right) \\,dx$$\nThis matrix is generally non-symmetric due to the first term, which arises from the nonlinearity of the diffusion coefficient.\n\nThe nonlinear system $\\mathbf{R}(\\mathbf{u}) = \\mathbf{0}$ is solved iteratively. The convergence criteria are critical for terminating the iteration robustly. The specified criteria are:\n1.  Residual criterion: $\\lVert \\mathbf{R}(\\mathbf{u}_k) \\rVert_2 \\le \\text{abs\\_tol} + \\text{rel\\_tol}\\,\\lVert \\mathbf{R}_0 \\rVert_2$. This criterion checks if the current iterate $\\mathbf{u}_k$ satisfies the nonlinear equations to a desired tolerance. The term $\\lVert \\mathbf{R}(\\mathbf{u}_k) \\rVert_2$ measures the magnitude of the imbalance in the discrete equations. The use of both an absolute tolerance ($\\text{abs\\_tol}$) and a relative tolerance ($\\text{rel\\_tol}$) scaled by the initial residual norm $\\lVert \\mathbf{R}_0 \\rVert_2$ makes the criterion robust for problems of different scales and for cases where the initial guess is already close to the solution.\n2.  Step criterion: $\\lVert \\mathbf{s}_k \\rVert_2 \\le \\text{step\\_tol}\\,\\bigl(1+\\lVert \\mathbf{u}_k \\rVert_2\\bigr)$. This criterion checks if the solution is still changing significantly. The vector $\\mathbf{s}_k = \\mathbf{u}_{k+1} - \\mathbf{u}_k$ is the update step. A very small step norm suggests that the iteration has stagnated or converged to a stationary point of the merit function. Scaling the tolerance by $(1+\\lVert \\mathbf{u}_k \\rVert_2)$ makes it insensitive to the magnitude of the solution itself.\n\nRequiring both criteria to be met prevents premature termination. For example, an iteration might produce a very small step near a \"flat\" region of the residual landscape, far from an actual root. The residual criterion would prevent convergence in such a case. Conversely, if the residual norm is small but the solution is still changing, the step criterion ensures the iteration continues until the solution stabilizes.\n\n**3. Comparison of Nonlinear Solvers**\n\nThe core of this problem is the comparison between a noisy Newton method and a robust quasi-Newton method.\n\n**Solver A (Noisy-J Newton):** At each iteration $k$, Newton's method approximates the nonlinear function $\\mathbf{R}(\\mathbf{u})$ with its linearization at the current iterate $\\mathbf{u}_k$: $\\mathbf{R}(\\mathbf{u}_k + \\mathbf{s}_k) \\approx \\mathbf{R}(\\mathbf{u}_k) + J(\\mathbf{u}_k)\\mathbf{s}_k$. Setting this to zero, we find the search direction $\\mathbf{s}_k$ by solving the linear system $J(\\mathbf{u}_k)\\mathbf{s}_k = -\\mathbf{R}(\\mathbf{u}_k)$. The effectiveness of Newton's method hinges on the accuracy of the Jacobian $J(\\mathbf{u}_k)$. The problem introduces two sources of error into the Jacobian assembly:\n-   **Under-integration ($n_Q^J=1$):** Using a single quadrature point to integrate the Jacobian expression introduces a systematic quadrature error. While this reduces computational cost, it yields an inaccurate approximation of the true tangent matrix.\n-   **Multiplicative Noise:** The term $1+\\sigma\\,\\xi$ randomly perturbs the contribution of each quadrature point. This simulates noise that can arise in complex multi-physics simulations, from floating-point errors, or from using stochastic methods within the model.\n\nThese errors degrade the quality of the search direction $\\mathbf{s}_k$. A poor search direction may not be a good descent direction for the merit function $\\phi(\\mathbf{u})=\\tfrac{1}{2}\\lVert \\mathbf{R}(\\mathbf{u})\\rVert_2^2$, causing the line search to require many backtracking steps or fail altogether. Consequently, the quadratic convergence property of the ideal Newton's method is lost, and the solver may converge slowly or diverge.\n\n**Solver B (Quasi-Newton with Curvature Averaging):** Quasi-Newton methods avoid the expensive re-computation of the Jacobian at every iteration. Instead, they build an approximation of the Jacobian (or its inverse, $H_k \\approx J(\\mathbf{u}_k)^{-1}$) iteratively. The update uses information from the most recent step ($\\delta_k = \\mathbf{u}_{k+1} - \\mathbf{u}_k$) and the change in the residual ($y_k = \\mathbf{R}(\\mathbf{u}_{k+1}) - \\mathbf{R}(\\mathbf{u}_k)$), known as the curvature vector. The update formula, such as for Broyden's good method, enforces the secant condition $H_{k+1} y_k = \\delta_k$.\n\nThe key innovation here is **curvature averaging**. The raw curvature vector $y_k$ is susceptible to noise if the residual function $\\mathbf{R}(\\mathbf{u})$ itself is evaluated with some error (even though here we assume $\\mathbf{R}(\\mathbf{u})$ is exact, the principle applies to noisy functions). More importantly, the difference of two vectors from a nonlinear function can amplify underlying oscillatory behavior. The specified exponential moving average, $y_k^{\\text{avg}}=(1-\\eta)\\,y_k+\\eta\\,y_{k-1}^{\\text{avg}}$, acts as a low-pass filter. It smooths the sequence of curvature vectors, filtering out high-frequency noise or oscillations.\n\nUsing this smoothed vector $y_k^{\\text{avg}}$ in the Broyden update formula leads to a more stable and reliable approximation $H_{k+1}$ of the inverse Jacobian. This approximation effectively captures the low-frequency, essential trend of the system's tangent behavior while ignoring spurious noise. As a result, the search directions $\\mathbf{s}_k = -H_k \\mathbf{R}(\\mathbf{u}_k)$ are of higher quality and more consistently aligned with the path to the solution than those produced by the noisy-J Newton method. This makes the quasi-Newton solver more robust and often more efficient in the presence of significant Jacobian noise and approximation errors, as it is designed to build a stable model from potentially unreliable step-by-step information.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the finite element analysis and comparison of nonlinear solvers.\n    \"\"\"\n\n    # --------------------------------------------------------------------------\n    # --- FEM and Model Parameters (Fixed) ---\n    # --------------------------------------------------------------------------\n    n_e = 20  # Number of elements\n    n_nodes = n_e + 1\n    n = n_e - 1  # Number of interior nodes (degrees of freedom)\n    h = 1.0 / n_e  # Element length\n\n    # --- Dirichlet Boundary Conditions ---\n    u_left_bc = 0.0\n    u_right_bc = 1.0\n\n    # --- Material and Model Coefficients ---\n    alpha = 0.5\n    beta = 6.0\n    gamma = 1.0\n\n    # --- Quadrature Rules ---\n    # For Residual (high accuracy)\n    quad_points_R, quad_weights_R = np.polynomial.legendre.leggauss(5)\n    # For Jacobian (potentially under-integrated)\n    quad_points_J_full, quad_weights_J_full = np.polynomial.legendre.leggauss(5)\n    quad_points_J_under, quad_weights_J_under = np.polynomial.legendre.leggauss(1)\n\n    # --- Nonlinear Iteration Parameters ---\n    u_initial = np.zeros(n)\n    abs_tol = 1e-10\n    rel_tol = 1e-8\n    step_tol = 1e-12\n    max_iters = 50\n    ls_t0 = 1.0\n    ls_factor = 0.5\n    ls_min_step = 1e-8\n    eta = 0.5  # Curvature averaging parameter for Solver B\n\n    # --------------------------------------------------------------------------\n    # --- Helper Functions ---\n    # --------------------------------------------------------------------------\n\n    def k_func(u_val):\n        return 1.0 + alpha * np.sin(beta * u_val)\n\n    def k_prime_func(u_val):\n        return alpha * beta * np.cos(beta * u_val)\n\n    def g_func(x_val):\n        return gamma * x_val**3 - alpha * beta * np.cos(beta * x_val)\n\n    def get_element_dof_vals(u_dofs, e_idx):\n        \"\"\"Constructs the vector of nodal values for a given element.\"\"\"\n        if e_idx == 0:\n            return np.array([u_left_bc, u_dofs[0]])\n        elif e_idx == n_e - 1:\n            return np.array([u_dofs[n - 1], u_right_bc])\n        else:\n            return np.array([u_dofs[e_idx - 1], u_dofs[e_idx]])\n\n    def shape_functions(xi):\n        \"\"\"Linear shape functions on reference element [-1, 1].\"\"\"\n        N1 = 0.5 * (1.0 - xi)\n        N2 = 0.5 * (1.0 + xi)\n        return np.array([N1, N2])\n\n    def shape_function_derivs(xi):\n        \"\"\"Derivatives of linear shape functions on reference element.\"\"\"\n        return np.array([-0.5, 0.5])\n\n    # --------------------------------------------------------------------------\n    # --- Residual and Jacobian Assembly ---\n    # --------------------------------------------------------------------------\n\n    def assemble_residual(u_dofs):\n        R = np.zeros(n)\n        for e in range(n_e):\n            R_elem = np.zeros(2)\n            u_elem = get_element_dof_vals(u_dofs, e)\n\n            for qp, qw in zip(quad_points_R, quad_weights_R):\n                N = shape_functions(qp)\n                B = shape_function_derivs(qp) / (h / 2.0)\n                \n                x_q = (e * h) * N[0] + ((e + 1) * h) * N[1]\n                u_q = u_elem @ N\n                du_dx_q = u_elem @ B\n\n                k_val = k_func(u_q)\n                g_val = g_func(x_q)\n\n                # Weak form contributions\n                R_elem += (k_val * du_dx_q * B + gamma * u_q**3 * N - g_val * N) * (h / 2.0) * qw\n            \n            # Assembly into global residual vector\n            if e == 0:\n                R[0] += R_elem[1]\n            elif e == n_e - 1:\n                R[n-1] += R_elem[0]\n            else:\n                R[e-1:e+1] += R_elem\n\n        return R\n\n    def assemble_jacobian(u_dofs, sigma, under_integrate, rng):\n        J = np.zeros((n, n))\n        \n        if under_integrate:\n            quad_points_J, quad_weights_J = quad_points_J_under, quad_weights_J_under\n        else:\n            quad_points_J, quad_weights_J = quad_points_J_full, quad_weights_J_full\n\n        for e in range(n_e):\n            J_elem = np.zeros((2, 2))\n            u_elem = get_element_dof_vals(u_dofs, e)\n            \n            for qp, qw in zip(quad_points_J, quad_weights_J):\n                N = shape_functions(qp)\n                B = shape_function_derivs(qp) / (h / 2.0)\n                u_q = u_elem @ N\n                du_dx_q = u_elem @ B\n                \n                k_val = k_func(u_q)\n                k_prime_val = k_prime_func(u_q)\n\n                term1 = k_prime_val * du_dx_q * np.outer(N, B)\n                term2 = k_val * np.outer(B, B)\n                term3 = 3.0 * gamma * u_q**2 * np.outer(N, N)\n\n                noise_factor = 1.0 + sigma * rng.normal() if sigma > 0 else 1.0\n                \n                J_elem += (term1 + term2 + term3) * noise_factor * (h / 2.0) * qw\n\n            # Assembly into global Jacobian matrix\n            if e == 0:\n                J[0, 0] += J_elem[1, 1]\n            elif e == n_e - 1:\n                J[n-1, n-1] += J_elem[0, 0]\n            else:\n                J[e-1:e+1, e-1:e+1] += J_elem\n\n        return J\n\n    # --------------------------------------------------------------------------\n    # --- Nonlinear Solvers ---\n    # --------------------------------------------------------------------------\n\n    def solver_newton(u_init, sigma, under_integrate, rng):\n        u_k = u_init.copy()\n        \n        R0 = assemble_residual(u_k)\n        norm_R0 = np.linalg.norm(R0)\n        if norm_R0 == 0: return 0, True\n        \n        conv_tol_R = abs_tol + rel_tol * norm_R0\n\n        for k in range(max_iters):\n            R_k = assemble_residual(u_k)\n            norm_Rk = np.linalg.norm(R_k)\n            \n            if norm_Rk = conv_tol_R and k  0:\n                # Step check is done after computing the step\n                pass\n\n            J_k = assemble_jacobian(u_k, sigma, under_integrate, rng)\n            \n            try:\n                s_k = np.linalg.solve(J_k, -R_k)\n            except np.linalg.LinAlgError:\n                # Apply Tikhonov regularization if singular\n                try:\n                    s_k = np.linalg.solve(J_k + 1e-8 * np.eye(n), -R_k)\n                except np.linalg.LinAlgError:\n                    return k, False # Failed even with regularization\n\n            norm_sk = np.linalg.norm(s_k)\n            conv_tol_s = step_tol * (1.0 + np.linalg.norm(u_k))\n\n            if norm_Rk = conv_tol_R and norm_sk = conv_tol_s:\n                return k, True\n\n            # Backtracking line search\n            t = ls_t0\n            phi_k = 0.5 * norm_Rk**2\n            \n            while t  ls_min_step:\n                u_trial = u_k + t * s_k\n                R_trial = assemble_residual(u_trial)\n                phi_trial = 0.5 * np.linalg.norm(R_trial)**2\n                \n                if phi_trial  phi_k:\n                    u_k = u_trial\n                    break\n                t *= ls_factor\n            \n            if t = ls_min_step:\n                return k, False  # Line search failed\n\n        return max_iters, False\n\n    def solver_quasi_newton(u_init, sigma, under_integrate, rng):\n        u_k = u_init.copy()\n        \n        R_k = assemble_residual(u_k)\n        norm_R0 = np.linalg.norm(R_k)\n        if norm_R0 == 0: return 0, True\n        \n        conv_tol_R = abs_tol + rel_tol * norm_R0\n        \n        # Initialize inverse Jacobian\n        J0 = assemble_jacobian(u_k, sigma, under_integrate, rng)\n        try:\n            H_k = np.linalg.inv(J0)\n        except np.linalg.LinAlgError:\n            try:\n                H_k = np.linalg.inv(J0 + 1e-8 * np.eye(n))\n            except np.linalg.LinAlgError:\n                 return 0, False # Failed to initialize\n\n        y_prev_avg = np.zeros(n)\n\n        for k in range(max_iters):\n            norm_Rk = np.linalg.norm(R_k)\n            \n            if norm_Rk = conv_tol_R and k  0:\n                pass\n\n            s_k = -H_k @ R_k\n\n            norm_sk = np.linalg.norm(s_k)\n            conv_tol_s = step_tol * (1.0 + np.linalg.norm(u_k))\n            \n            if norm_Rk = conv_tol_R and norm_sk = conv_tol_s:\n                return k, True\n            \n            # Line search\n            t = ls_t0\n            phi_k = 0.5 * norm_Rk**2\n            u_k_plus_1 = u_k\n            \n            while t  ls_min_step:\n                u_trial = u_k + t * s_k\n                R_trial = assemble_residual(u_trial)\n                phi_trial = 0.5 * np.linalg.norm(R_trial)**2\n                \n                if phi_trial  phi_k:\n                    u_k_plus_1 = u_trial\n                    break\n                t *= ls_factor\n            \n            if t = ls_min_step:\n                return k, False\n            \n            # --- Update Inverse Hessian ---\n            delta_k = u_k_plus_1 - u_k\n            u_k = u_k_plus_1\n            \n            R_k_plus_1 = assemble_residual(u_k)\n            y_k = R_k_plus_1 - R_k\n            \n            # Curvature averaging\n            y_k_avg = (1.0 - eta) * y_k + eta * y_prev_avg\n            \n            # Good Broyden's method for inverse\n            Hk_yk_avg = H_k @ y_k_avg\n            denom = delta_k.T @ Hk_yk_avg\n            \n            if abs(denom)  1e-12:\n                H_k = H_k + np.outer(delta_k - Hk_yk_avg, delta_k.T @ H_k) / denom\n            # If denom is zero, H_k is not updated.\n            \n            R_k = R_k_plus_1\n            y_prev_avg = y_k_avg\n\n        return max_iters, False\n\n    # --------------------------------------------------------------------------\n    # --- Test Suite Execution ---\n    # --------------------------------------------------------------------------\n    test_cases = [\n        (0.0, False),        # Case 1: No noise, full integration\n        (1e-2, False),       # Case 2: Small noise, full integration\n        (5e-2, True),        # Case 3: Medium noise, under-integration\n        (1e-1, True),        # Case 4: High noise, under-integration\n    ]\n    \n    results = []\n    \n    for i, (sigma_val, under_int_flag) in enumerate(test_cases):\n        # Use a fixed seed for each case for reproducibility\n        rng = np.random.default_rng(seed=i) \n        \n        # Run Solver A (Noisy-J Newton)\n        iters_A, conv_A = solver_newton(u_initial, sigma_val, under_int_flag, rng)\n        \n        # Reset seed to ensure Solver B starts with the same initial noisy Jacobian\n        rng = np.random.default_rng(seed=i)\n        \n        # Run Solver B (Quasi-Newton)\n        iters_B, conv_B = solver_quasi_newton(u_initial, sigma_val, under_int_flag, rng)\n\n        # Apply comparison rule\n        # Outcome is true if Solver B converges and either Solver A does not converge \n        # or Solver B uses strictly fewer nonlinear iterations than Solver A.\n        is_B_more_robust = conv_B and (not conv_A or iters_B  iters_A)\n        results.append(is_B_more_robust)\n        \n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```", "id": "2549625"}]}