{"hands_on_practices": [{"introduction": "The cornerstone of local stability analysis is linearization, a powerful technique that approximates the behavior of a complex nonlinear system near an equilibrium point with a much simpler linear one. This approach is valid for hyperbolic equilibria, where the Hartman-Grobman theorem guarantees a topological equivalence between the nonlinear flow and its linearization. This exercise [@problem_id:2692833] offers fundamental practice in this process, guiding you to compute the Jacobian matrix and use its properties—specifically its determinant—to classify the local dynamics of a saddle point.", "problem": "Let $\\dot{x} = f(x)$ be a planar autonomous ordinary differential equation (ODE), where $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} \\in \\mathbb{R}^{2}$ and $f : \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ is a polynomial vector field of total degree at most $3$. Consider the specific choice\n$$\nf(x) \\;=\\; \\begin{pmatrix}\nx_{1} + x_{1}^{2} - x_{1} x_{2} \\\\\n-2 x_{2} + x_{1}^{3}\n\\end{pmatrix}.\n$$\nUsing only fundamental definitions from dynamical systems and control theory, proceed as follows: first, verify that the origin $x = 0$ is an equilibrium by checking $f(0) = 0$. Second, compute the Jacobian matrix $Df(0)$ by taking the matrix of first partial derivatives of $f$ and evaluating at the origin. Third, starting from the characteristic polynomial of $Df(0)$ and the relation between the determinant and the product of eigenvalues, justify why the sign of $\\det(Df(0))$ is sufficient to conclude that the origin is a saddle for the linearized system and, by hyperbolicity, for the nonlinear system in a neighborhood of the origin. Provide the determinant $\\det(Df(0))$ as your final answer. Express the final answer exactly; no rounding is required.", "solution": "The problem statement is parsed and validated. It is found to be self-contained, scientifically grounded in the principles of dynamical systems theory, and mathematically well-posed. The problem is a standard exercise in the stability analysis of a nonlinear system via linearization. We will proceed with the solution.\n\nThe system is described by the autonomous ordinary differential equation $\\dot{x} = f(x)$, where $x \\in \\mathbb{R}^{2}$ and the vector field $f$ is given by\n$$\nf(x_1, x_2) = \\begin{pmatrix} f_1(x_1, x_2) \\\\ f_2(x_1, x_2) \\end{pmatrix} = \\begin{pmatrix}\nx_{1} + x_{1}^{2} - x_{1} x_{2} \\\\\n-2 x_{2} + x_{1}^{3}\n\\end{pmatrix}.\n$$\n\nFirst, we verify that the origin, $x = 0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, is an equilibrium point. An equilibrium point $x^*$ must satisfy the condition $f(x^*) = 0$. We evaluate $f$ at the origin:\n$$\nf(0, 0) = \\begin{pmatrix}\n0 + 0^{2} - 0 \\cdot 0 \\\\\n-2 \\cdot 0 + 0^{3}\n\\end{pmatrix} = \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThe condition is satisfied, so the origin is indeed an equilibrium point of the system.\n\nSecond, we linearize the system around this equilibrium point by computing the Jacobian matrix of $f$, denoted as $Df(x)$. The Jacobian matrix is the matrix of all first-order partial derivatives:\n$$\nDf(x) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1}  \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1}  \\frac{\\partial f_2}{\\partial x_2} \\end{pmatrix}.\n$$\nWe compute the necessary partial derivatives:\n$$\n\\frac{\\partial f_1}{\\partial x_1} = 1 + 2x_1 - x_2 \\\\\n\\frac{\\partial f_1}{\\partial x_2} = -x_1 \\\\\n\\frac{\\partial f_2}{\\partial x_1} = 3x_1^2 \\\\\n\\frac{\\partial f_2}{\\partial x_2} = -2\n$$\nSubstituting these into the matrix expression gives\n$$\nDf(x) = \\begin{pmatrix}\n1 + 2x_1 - x_2  -x_1 \\\\\n3x_1^2  -2\n\\end{pmatrix}.\n$$\nTo analyze the stability at the origin, we evaluate this Jacobian matrix at $x = (0, 0)$. Let $A = Df(0)$.\n$$\nA = Df(0) = \\begin{pmatrix}\n1 + 2(0) - 0  -0 \\\\\n3(0)^2  -2\n\\end{pmatrix} = \\begin{pmatrix}\n1  0 \\\\\n0  -2\n\\end{pmatrix}.\n$$\nThis matrix $A$ defines the linearized system $\\dot{\\xi} = A\\xi$ in the neighborhood of the origin.\n\nThird, we analyze the stability based on the properties of matrix $A$. The stability of the equilibrium point is determined by the eigenvalues, $\\lambda_1$ and $\\lambda_2$, of $A$. The eigenvalues are the roots of the characteristic polynomial $p(\\lambda) = \\det(A - \\lambda I) = 0$. For a general $2 \\times 2$ matrix, this polynomial is $\\lambda^2 - \\text{tr}(A)\\lambda + \\det(A) = 0$. By Vieta's formulas, the product of the roots is equal to the constant term of the monic polynomial, thus $\\lambda_1 \\lambda_2 = \\det(A)$.\n\nWe compute the determinant of $A$:\n$$\n\\det(A) = \\det\\begin{pmatrix}\n1  0 \\\\\n0  -2\n\\end{pmatrix} = (1)(-2) - (0)(0) = -2.\n$$\nSince $\\det(A) = -2$, we have $\\lambda_1 \\lambda_2 = -2$. The product of the eigenvalues is negative. Let us consider the implications. If the eigenvalues were a complex conjugate pair, $\\lambda_{1,2} = \\alpha \\pm i\\beta$ with $\\beta \\neq 0$, their product would be $\\lambda_1 \\lambda_2 = (\\alpha + i\\beta)(\\alpha - i\\beta) = \\alpha^2 + \\beta^2$. As $\\alpha$ and $\\beta$ are real, $\\alpha^2 \\ge 0$ and $\\beta^2 > 0$, so their product would be strictly positive. This contradicts our finding that $\\det(A) = -2  0$. Therefore, the eigenvalues $\\lambda_1$ and $\\lambda_2$ must both be real.\n\nFor two real numbers, their product is negative if and only if one is positive and the other is negative. Hence, one eigenvalue is positive and one is negative. For the linearized system $\\dot{\\xi} = A\\xi$, an equilibrium point with real eigenvalues of opposite sign is, by definition, a saddle point.\n\nThe eigenvalues of $A$ are evidently $\\lambda_1 = 1$ and $\\lambda_2 = -2$. The real parts of these eigenvalues are $\\text{Re}(\\lambda_1) = 1$ and $\\text{Re}(\\lambda_2) = -2$. Since neither real part is zero, the equilibrium point is hyperbolic. The Hartman-Grobman theorem states that for a hyperbolic equilibrium point, the phase portrait of the nonlinear system in a small neighborhood of the point is topologically equivalent to the phase portrait of its linearization. Consequently, since the origin is a saddle point for the linearized system, it is also a saddle point for the original nonlinear system. The sign of $\\det(Df(0))$ is sufficient in this $2$D case because a negative determinant forces the eigenvalues to be real and of opposite sign, which is the defining characteristic of a saddle.\n\nThe value requested is the determinant of the Jacobian at the origin.", "answer": "$$\n\\boxed{-2}\n$$", "id": "2692833"}, {"introduction": "While linearization provides invaluable local insights, its conclusions do not necessarily extend globally. To determine a guaranteed domain of stability for an equilibrium, we turn to the more powerful tools of Lyapunov theory. This practice [@problem_id:2704886] moves beyond local analysis, challenging you to apply Lyapunov's direct method to estimate the region of attraction for a stable equilibrium, a crucial skill for certifying the safe operating range of any nonlinear control system.", "problem": "Consider the autonomous nonlinear system in continuous time with state $x = (x_1, x_2) \\in \\mathbb{R}^2$ given by\n$$\n\\dot{x}_1 = -x_1 + x_2^2, \\qquad \\dot{x}_2 = -x_2.\n$$\nThe origin $x = 0$ is an equilibrium point. Using the direct method of Lyapunov based on first principles, proceed as follows:\n- Start from the definitions of a Lyapunov function, negative definiteness of its derivative along trajectories, and the region of attraction.\n- Consider the Lyapunov candidate\n$$\nV(x) = x_1^2 + x_2^2,\n$$\nwhich is positive definite and radially unbounded.\n- By requiring that the derivative of $V$ along trajectories is strictly negative on the boundary of the sublevel set $\\{ x \\in \\mathbb{R}^2 : V(x) = c \\}$, determine the supremum value $c^\\star$ such that every sublevel set $\\{ x \\in \\mathbb{R}^2 : V(x) \\le c \\}$ with $0  c \\le c^\\star$ is forward invariant and contained in the region of attraction of the origin.\n\nYour final answer must be the exact value of $c^\\star$ as a single real number. Do not round.", "solution": "The problem statement has been examined and is deemed valid. It is a well-posed, scientifically grounded problem in the field of control theory, specifically concerning the application of Lyapunov's direct method to estimate the region of attraction for a nonlinear autonomous system. We may proceed with the solution.\n\nThe system is described by the state equations:\n$$\n\\begin{cases}\n\\dot{x}_1 = -x_1 + x_2^2 \\\\\n\\dot{x}_2 = -x_2\n\\end{cases}\n$$\nThe origin, $x = (x_1, x_2) = (0, 0)$, is an equilibrium point, as $\\dot{x}_1(0,0) = 0$ and $\\dot{x}_2(0,0) = 0$.\n\nThe proposed Lyapunov function candidate is $V(x) = x_1^2 + x_2^2$. This function is positive definite as $V(0) = 0$ and $V(x) > 0$ for all $x \\in \\mathbb{R}^2 \\setminus \\{0\\}$. It is also radially unbounded.\n\nAccording to Lyapunov's theorem for estimating the domain of attraction, a sublevel set $\\Omega_c = \\{ x \\in \\mathbb{R}^2 : V(x) \\le c \\}$ is a subset of the region of attraction of the origin if $c > 0$, $\\Omega_c$ is compact, and the time derivative of the Lyapunov function, $\\dot{V}(x)$, is strictly negative for all $x \\in \\Omega_c \\setminus \\{0\\}$. The sets $\\Omega_c$ for $c>0$ are closed disks centered at the origin, which are compact. We therefore need to find the largest value of $c$, which we denote $c^\\star$, such that $\\dot{V}(x)  0$ for all $x$ satisfying $0  V(x) \\le c^\\star$.\n\nThis is equivalent to finding the smallest value of $V(x)$ on the set where $\\dot{V}(x) = 0$ (excluding the origin itself). Any sublevel set defined by a constant smaller than this minimum value will be devoid of any points where the derivative is zero or positive, except for the origin.\n\nFirst, we compute the time derivative of $V(x)$ along the trajectories of the system:\n$$\n\\dot{V}(x) = \\frac{\\partial V}{\\partial x} \\dot{x} = \\begin{pmatrix} \\frac{\\partial V}{\\partial x_1}  \\frac{\\partial V}{\\partial x_2} \\end{pmatrix} \\begin{pmatrix} \\dot{x}_1 \\\\ \\dot{x}_2 \\end{pmatrix}\n$$\nThe partial derivatives are $\\frac{\\partial V}{\\partial x_1} = 2x_1$ and $\\frac{\\partial V}{\\partial x_2} = 2x_2$.\nSubstituting the system dynamics, we obtain:\n$$\n\\dot{V}(x) = (2x_1)(-x_1 + x_2^2) + (2x_2)(-x_2)\n$$\n$$\n\\dot{V}(x) = -2x_1^2 + 2x_1 x_2^2 - 2x_2^2\n$$\nFor local asymptotic stability, we require $\\dot{V}(x)$ to be negative definite in a neighborhood of the origin. However, the term $2x_1 x_2^2$ can be positive and can dominate the negative definite part $-2(x_1^2 + x_2^2)$. Therefore, the origin is not globally asymptotically stable, and we must find a region where $\\dot{V}(x)$ is strictly negative.\n\nWe seek the largest constant $c^\\star$ such that the set $\\Omega_{c^\\star} = \\{x | x_1^2 + x_2^2 \\le c^\\star \\}$ is the largest such invariant region. This value $c^\\star$ is the minimum value of $V(x)$ over all non-zero points where $\\dot{V}(x) \\ge 0$. The boundary of the region where $\\dot{V}(x)  0$ is the set of points where $\\dot{V}(x) = 0$.\nWe set $\\dot{V}(x) = 0$ for $x \\neq 0$:\n$$\n-2x_1^2 + 2x_1 x_2^2 - 2x_2^2 = 0\n$$\n$$\nx_1^2 - x_1 x_2^2 + x_2^2 = 0\n$$\nThe problem is now reduced to finding the minimum value of the function $V(x) = x_1^2 + x_2^2$ subject to the constraint $g(x_1, x_2) = x_1^2 - x_1 x_2^2 + x_2^2 = 0$, for $x \\neq 0$.\n\nFrom the constraint equation, we can write $x_1^2 + x_2^2 = x_1 x_2^2$. Therefore, we are minimizing $V(x) = x_1 x_2^2$.\nLet's express $x_2^2$ in terms of $x_1$ from the constraint:\n$$\nx_2^2(x_1 - 1) = x_1^2\n$$\nIf $x_1=1$, the equation becomes $0 = 1$, which is a contradiction. Thus, $x_1 \\neq 1$. We can write:\n$$\nx_2^2 = \\frac{x_1^2}{x_1-1}\n$$\nSince $x_2^2 \\ge 0$ and $x_1^2 \\ge 0$, and we exclude the case $x_1=0$ (which implies $x_2=0$, the origin), we must have $x_1-1 > 0$, which means $x_1 > 1$.\n\nNow we express $V$ as a function of $x_1$ alone for $x_1 > 1$:\n$$\nV(x_1) = x_1^2 + x_2^2 = x_1^2 + \\frac{x_1^2}{x_1-1}\n$$\nTo find the minimum value of $V(x_1)$, we compute its derivative with respect to $x_1$ and set it to zero:\n$$\n\\frac{dV}{dx_1} = \\frac{d}{dx_1} \\left( x_1^2 + \\frac{x_1^2}{x_1-1} \\right) = 2x_1 + \\frac{2x_1(x_1-1) - x_1^2(1)}{(x_1-1)^2}\n$$\n$$\n\\frac{dV}{dx_1} = 2x_1 + \\frac{2x_1^2 - 2x_1 - x_1^2}{(x_1-1)^2} = 2x_1 + \\frac{x_1^2 - 2x_1}{(x_1-1)^2}\n$$\n$$\n\\frac{dV}{dx_1} = \\frac{2x_1(x_1-1)^2 + x_1^2 - 2x_1}{(x_1-1)^2} = \\frac{2x_1(x_1^2 - 2x_1 + 1) + x_1^2 - 2x_1}{(x_1-1)^2}\n$$\n$$\n\\frac{dV}{dx_1} = \\frac{2x_1^3 - 4x_1^2 + 2x_1 + x_1^2 - 2x_1}{(x_1-1)^2} = \\frac{2x_1^3 - 3x_1^2}{(x_1-1)^2}\n$$\nSetting the derivative to zero to find an extremum:\n$$\n\\frac{x_1^2(2x_1 - 3)}{(x_1-1)^2} = 0\n$$\nSince $x_1>1$, the only valid solution is $2x_1 - 3 = 0$, which yields $x_1 = \\frac{3}{2}$.\nTo confirm this is a minimum, we can examine the second derivative, or observe the sign of the first derivative. For $1  x_1  \\frac{3}{2}$, $\\frac{dV}{dx_1}  0$. For $x_1 > \\frac{3}{2}$, $\\frac{dV}{dx_1} > 0$. Thus, $x_1 = \\frac{3}{2}$ corresponds to a local minimum, and since it is the only critical point in the domain $x_1 > 1$, it is the global minimum.\n\nThe minimum value of $V$ is found by substituting $x_1 = \\frac{3}{2}$ back into the expression for $V(x_1)$:\n$$\nc^\\star = V\\left(\\frac{3}{2}\\right) = \\left(\\frac{3}{2}\\right)^2 + \\frac{\\left(\\frac{3}{2}\\right)^2}{\\frac{3}{2}-1}\n$$\n$$\nc^\\star = \\frac{9}{4} + \\frac{\\frac{9}{4}}{\\frac{1}{2}} = \\frac{9}{4} + \\frac{9}{2} = \\frac{9}{4} + \\frac{18}{4} = \\frac{27}{4}\n$$\nThis value $c^\\star = \\frac{27}{4}$ is the supremum such that for any $c$ with $0  c \\le c^\\star$, the set $\\{ x \\in \\mathbb{R}^2 : V(x) \\le c \\}$ is forward invariant and contained in the region of attraction of the origin. For any $c \\in (0, \\frac{27}{4})$, $\\dot{V}(x)$ is strictly negative on the set $\\{x | 0  V(x) \\le c\\}$.", "answer": "$$\\boxed{\\frac{27}{4}}$$", "id": "2704886"}, {"introduction": "Dynamical systems in the real world are rarely static; their behavior often depends critically on system parameters that can vary. As a parameter crosses a critical threshold, an equilibrium can lose stability and give rise to entirely new dynamic behaviors, a phenomenon known as a bifurcation. This exercise [@problem_id:2704876] provides a hands-on analysis of the pivotal Hopf bifurcation, where an equilibrium point spawns a periodic orbit, by transforming the system into polar coordinates to explicitly derive the emergent limit cycle's amplitude dynamics.", "problem": "Consider the planar nonlinear system of ordinary differential equations (ODEs)\n$$\n\\dot{x}_{1}=\\lambda x_{1}-x_{2}-x_{1}\\left(x_{1}^{2}+x_{2}^{2}\\right),\\qquad \n\\dot{x}_{2}=x_{1}+\\lambda x_{2}-x_{2}\\left(x_{1}^{2}+x_{2}^{2}\\right),\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is a real-valued bifurcation parameter, and $(x_{1},x_{2}) \\in \\mathbb{R}^{2}$. An equilibrium point is a point $(x_{1}^{\\star},x_{2}^{\\star})$ such that $\\dot{x}_{1}=\\dot{x}_{2}=0$. The linearization of the system at an equilibrium is given by its Jacobian matrix evaluated at that equilibrium, and the local behavior of solutions near the equilibrium can be classified using the eigenvalues of this Jacobian. A Hopf bifurcation occurs when a pair of complex-conjugate eigenvalues of the linearization cross the imaginary axis with nonzero speed with respect to the bifurcation parameter, while all other eigenvalues have nonzero real parts.\n\nStarting from the fundamental definitions above, analyze the equilibrium at the origin $(x_{1},x_{2})=(0,0)$ as $\\lambda$ varies near $0$. Derive the exact amplitude dynamics by transforming to polar coordinates $(r,\\theta)$ with $r=\\sqrt{x_{1}^{2}+x_{2}^{2}}$ and $\\theta=\\arctan2(x_{2},x_{1})$, without introducing any state or time rescalings. Use this derivation to determine the cubic term in the radial normal form. Define the first Lyapunov coefficient $l_{1}$ at the Hopf point $\\lambda=0$ to be the coefficient multiplying $r^{3}$ in the exact radial equation written in the form\n$$\n\\dot{r}=\\mu\\, r + l_{1}\\, r^{3}+\\text{higher-order terms in } r,\n$$\nwith $\\mu$ identified with $\\lambda$ near the bifurcation. Based on its sign, one can classify the Hopf bifurcation as supercritical (if $l_{1}0$) or subcritical (if $l_{1}0$), but for the purpose of this problem you must report only the value of $l_{1}$.\n\nWhat is the exact value of the first Lyapunov coefficient $l_{1}$ at $\\lambda=0$? Provide your answer as a single exact real number. No rounding is required, and no units are involved.", "solution": "The problem requires the determination of the first Lyapunov coefficient, $l_1$, for the given planar system at the Hopf bifurcation point. The validation of the problem statement is the mandatory first step.\n\n**Step 1: Extract Givens**\nThe system of ordinary differential equations (ODEs) is:\n$$\n\\dot{x}_{1}=\\lambda x_{1}-x_{2}-x_{1}\\left(x_{1}^{2}+x_{2}^{2}\\right)\n$$\n$$\n\\dot{x}_{2}=x_{1}+\\lambda x_{2}-x_{2}\\left(x_{1}^{2}+x_{2}^{2}\\right)\n$$\n- The bifurcation parameter is $\\lambda \\in \\mathbb{R}$.\n- The state variables are $(x_{1},x_{2}) \\in \\mathbb{R}^{2}$.\n- The equilibrium point to be analyzed is the origin, $(x_{1}^{\\star},x_{2}^{\\star})=(0,0)$.\n- The analysis method is a transformation to polar coordinates $(r,\\theta)$, where $r=\\sqrt{x_{1}^{2}+x_{2}^{2}}$ and $\\theta=\\arctan2(x_{2},x_{1})$.\n- The first Lyapunov coefficient $l_1$ is defined as the coefficient of the $r^3$ term in the radial equation $\\dot{r}=\\mu r + l_1 r^3 + \\text{H.O.T.}$, with $\\mu$ identified as $\\lambda$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard model from nonlinear dynamics and bifurcation theory, specifically the normal form for a Hopf bifurcation. All definitions are standard and the required calculation is mathematically rigorous and unambiguous. There are no contradictions, missing information, or violations of scientific principles. The problem is therefore deemed **valid**.\n\n**Step 3: Solution Derivation**\nThe analysis begins by verifying the equilibrium point and its local stability properties.\n\nFirst, we confirm that $(x_1, x_2) = (0,0)$ is an equilibrium point for any value of $\\lambda$. Substituting $x_1=0$ and $x_2=0$ into the system equations yields:\n$$\n\\dot{x}_1 = \\lambda(0) - 0 - 0(0^2+0^2) = 0\n$$\n$$\n\\dot{x}_2 = 0 + \\lambda(0) - 0(0^2+0^2) = 0\n$$\nThus, the origin is indeed a fixed point.\n\nNext, we linearize the system around this equilibrium by computing the Jacobian matrix, $J$:\n$$\nJ(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial \\dot{x}_1}{\\partial x_1}  \\frac{\\partial \\dot{x}_1}{\\partial x_2} \\\\ \\frac{\\partial \\dot{x}_2}{\\partial x_1}  \\frac{\\partial \\dot{x}_2}{\\partial x_2} \\end{pmatrix}\n$$\nThe partial derivatives are:\n$$\n\\frac{\\partial \\dot{x}_1}{\\partial x_1} = \\lambda - 3x_1^2 - x_2^2\n$$\n$$\n\\frac{\\partial \\dot{x}_1}{\\partial x_2} = -1 - 2x_1x_2\n$$\n$$\n\\frac{\\partial \\dot{x}_2}{\\partial x_1} = 1 - 2x_1x_2\n$$\n$$\n\\frac{\\partial \\dot{x}_2}{\\partial x_2} = \\lambda - x_1^2 - 3x_2^2\n$$\nEvaluating the Jacobian at the origin $(0,0)$:\n$$\nJ(0,0) = \\begin{pmatrix} \\lambda  -1 \\\\ 1  \\lambda \\end{pmatrix}\n$$\nThe eigenvalues $\\sigma$ of this matrix are the roots of the characteristic equation $\\det(J(0,0) - \\sigma I) = 0$:\n$$\n(\\lambda - \\sigma)^2 - (1)(-1) = 0 \\implies (\\lambda - \\sigma)^2 = -1 \\implies \\lambda - \\sigma = \\pm i\n$$\nThe eigenvalues are $\\sigma_{1,2} = \\lambda \\pm i$.\n\nA Hopf bifurcation occurs when the real part of the complex conjugate eigenvalues crosses zero. Here, $\\text{Re}(\\sigma_{1,2})=\\lambda$. The bifurcation point is thus $\\lambda=0$. At this point, the eigenvalues are purely imaginary, $\\sigma_{1,2}=\\pm i$. The transversality condition is met, as $\\frac{d}{d\\lambda}(\\text{Re}(\\sigma))|_{\\lambda=0} = 1 \\neq 0$.\n\nTo determine the nature of the bifurcation and find the first Lyapunov coefficient, we transform the system into polar coordinates, as instructed. Let $x_1 = r\\cos\\theta$ and $x_2 = r\\sin\\theta$. The radial coordinate is $r^2 = x_1^2 + x_2^2$. Differentiating this with respect to time $t$ gives:\n$$\n2r\\dot{r} = 2x_1\\dot{x}_1 + 2x_2\\dot{x}_2 \\implies \\dot{r} = \\frac{x_1\\dot{x}_1 + x_2\\dot{x}_2}{r}\n$$\nWe substitute the given ODEs into this expression. Note that the nonlinear terms in the original equations can be written in terms of $r$:\n$$\n\\dot{x}_{1}=\\lambda x_{1}-x_{2}-x_{1}r^2\n$$\n$$\n\\dot{x}_{2}=x_{1}+\\lambda x_{2}-x_{2}r^2\n$$\nNow, we compute the numerator for the $\\dot{r}$ equation:\n$$\nx_1\\dot{x}_1 + x_2\\dot{x}_2 = x_1(\\lambda x_1 - x_2 - x_1 r^2) + x_2(x_1 + \\lambda x_2 - x_2 r^2)\n$$\n$$\n= \\lambda x_1^2 - x_1x_2 - x_1^2 r^2 + x_1x_2 + \\lambda x_2^2 - x_2^2 r^2\n$$\nThe cross-terms $-x_1x_2$ and $+x_1x_2$ cancel. We group the remaining terms:\n$$\n= \\lambda(x_1^2 + x_2^2) - (x_1^2 + x_2^2)r^2\n$$\nSubstituting $x_1^2 + x_2^2 = r^2$:\n$$\n= \\lambda r^2 - r^2 \\cdot r^2 = \\lambda r^2 - r^4\n$$\nFinally, we find the equation for $\\dot{r}$:\n$$\n\\dot{r} = \\frac{\\lambda r^2 - r^4}{r} = \\lambda r - r^3\n$$\nThis equation is the exact amplitude dynamics. It contains no higher-order terms in $r$. The problem defines the first Lyapunov coefficient, $l_1$, via the form $\\dot{r}=\\mu r + l_1 r^3 + \\dots$, where $\\mu$ is identified with the bifurcation parameter $\\lambda$.\n\nComparing our derived equation, $\\dot{r} = \\lambda r - r^3$, with the definitional form, we can directly identify the coefficients:\n- The coefficient of the linear term in $r$ is $\\lambda$, so $\\mu = \\lambda$.\n- The coefficient of the cubic term, $r^3$, is $-1$.\n\nTherefore, the first Lyapunov coefficient is $l_1 = -1$. The negative sign of $l_1$ indicates that the Hopf bifurcation is supercritical, leading to the birth of a stable limit cycle for $\\lambda > 0$. However, the problem only asks for the value of $l_1$.", "answer": "$$\\boxed{-1}$$", "id": "2704876"}]}