{"hands_on_practices": [{"introduction": "The cornerstone of solving sequential decision-making problems is understanding how to find an optimal policy when the system's model is perfectly known. This exercise [@problem_id:2738630] provides a foundational workout in dynamic programming by applying the value iteration algorithm. You will engage directly with the Bellman optimality operator to iteratively improve a value function and witness the direct link between value functions and policy improvement, a core principle that underpins many reinforcement learning methods.", "problem": "Consider a finite Markov Decision Process (MDP) with state space $\\mathcal{S}=\\{1,2,3\\}$, action set $\\mathcal{A}=\\{a,b\\}$ available at every state, discount factor $\\gamma=\\frac{4}{5}$, reward function $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$, and transition probabilities $P(s' \\mid s,a)$. The data are:\n- At state $1$:\n  - Action $a$: reward $r(1,a)=2$, transitions $P(1 \\mid 1,a)=\\frac{1}{2}$, $P(2 \\mid 1,a)=\\frac{1}{2}$.\n  - Action $b$: reward $r(1,b)=1$, transitions $P(3 \\mid 1,b)=1$.\n- At state $2$:\n  - Action $a$: reward $r(2,a)=0$, transitions $P(1 \\mid 2,a)=1$.\n  - Action $b$: reward $r(2,b)=3$, transitions $P(2 \\mid 2,b)=\\frac{7}{10}$, $P(3 \\mid 2,b)=\\frac{3}{10}$.\n- At state $3$:\n  - Action $a$: reward $r(3,a)=1$, transitions $P(3 \\mid 3,a)=1$.\n  - Action $b$: reward $r(3,b)=0$, transitions $P(1 \\mid 3,b)=1$.\n\nLet the initial value function be $V_{0}$ given by the column vector $V_{0}=\\begin{pmatrix}2\\\\2\\\\1\\end{pmatrix}$. Let the Bellman optimality operator $T$ act on any value function $V$ by\n$$\n(TV)(s) \\;=\\; \\max_{a \\in \\mathcal{A}} \\Big[\\, r(s,a) \\;+\\; \\gamma \\sum_{s'\\in\\mathcal{S}} P(s' \\mid s,a)\\, V(s') \\,\\Big].\n$$\nFor any value function $V$, define the greedy policy $\\pi=\\operatorname{Greedy}(V)$ by selecting in each state $s$ any maximizer of the right-hand side inside the maximum above. For any stationary policy $\\pi$, define its value function $V^{\\pi}$ as the unique solution of the Bellman equations $V^{\\pi} = r^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$, and define the optimal value $V^{\\star}$ as the unique fixed point of $T$, that is, $V^{\\star}=T V^{\\star}$. For any policy $\\pi$, define its greedy-policy suboptimality as\n$$\n\\Delta_{\\infty}(\\pi) \\;=\\; \\| V^{\\star} - V^{\\pi} \\|_{\\infty} \\;=\\; \\max_{s\\in\\mathcal{S}} \\big| V^{\\star}(s) - V^{\\pi}(s) \\big|.\n$$\n\nTasks:\n- Starting from $V_{0}$, compute one step of value iteration $V_{1}=T V_{0}$.\n- Compute the greedy policies $\\pi_{0}=\\operatorname{Greedy}(V_{0})$ and $\\pi_{1}=\\operatorname{Greedy}(V_{1})$.\n- Exactly evaluate $V^{\\pi_{0}}$ and $V^{\\pi_{1}}$ by solving their respective Bellman equations.\n- Exactly compute $V^{\\star}$ from first principles using the definition of $T$ and a consistent selection of maximizing actions, and verify optimality by checking action-wise optimality at each state.\n- Finally, compute the improvement\n$$\n\\Delta \\;=\\; \\Delta_{\\infty}(\\pi_{0}) \\;-\\; \\Delta_{\\infty}(\\pi_{1}),\n$$\nand report $\\Delta$ as an exact value (no rounding required).", "solution": "We begin from the standard definitions of a Markov Decision Process (MDP), the Bellman optimality operator $T$, and policy evaluation. The value iteration update is $V_{k+1} = T V_{k}$, where\n$$\n(TV)(s) \\;=\\; \\max_{a \\in \\{a,b\\}} \\left[ r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) V(s') \\right],\n$$\nwith $\\gamma=\\frac{4}{5}$.\n\nStep $1$: Compute $V_{1} = T V_{0}$ from $V_{0}=\\begin{pmatrix}2\\\\2\\\\1\\end{pmatrix}$.\n\nFor each state $s$, define the action-value for $V_{0}$:\n- At $s=1$:\n  - $Q(1,a;V_{0}) \\;=\\; 2 + \\frac{4}{5}\\left( \\frac{1}{2}\\cdot 2 + \\frac{1}{2}\\cdot 2 \\right) \\;=\\; 2 + \\frac{4}{5}\\cdot 2 \\;=\\; 2 + \\frac{8}{5} \\;=\\; \\frac{18}{5}$.\n  - $Q(1,b;V_{0}) \\;=\\; 1 + \\frac{4}{5}\\cdot 1 \\;=\\; 1 + \\frac{4}{5} \\;=\\; \\frac{9}{5}$.\n  Hence $(T V_{0})(1) \\;=\\; \\max\\left\\{ \\frac{18}{5}, \\frac{9}{5} \\right\\} \\;=\\; \\frac{18}{5}$.\n- At $s=2$:\n  - $Q(2,a;V_{0}) \\;=\\; 0 + \\frac{4}{5}\\cdot 2 \\;=\\; \\frac{8}{5}$.\n  - $Q(2,b;V_{0}) \\;=\\; 3 + \\frac{4}{5} \\left( \\frac{7}{10}\\cdot 2 + \\frac{3}{10}\\cdot 1 \\right) \\;=\\; 3 + \\frac{4}{5}\\cdot \\frac{17}{10} \\;=\\; 3 + \\frac{34}{25} \\;=\\; \\frac{109}{25}$.\n  Hence $(T V_{0})(2) \\;=\\; \\max\\left\\{ \\frac{8}{5}, \\frac{109}{25} \\right\\} \\;=\\; \\frac{109}{25}$.\n- At $s=3$:\n  - $Q(3,a;V_{0}) \\;=\\; 1 + \\frac{4}{5}\\cdot 1 \\;=\\; \\frac{9}{5}$.\n  - $Q(3,b;V_{0}) \\;=\\; 0 + \\frac{4}{5}\\cdot 2 \\;=\\; \\frac{8}{5}$.\n  Hence $(T V_{0})(3) \\;=\\; \\max\\left\\{ \\frac{9}{5}, \\frac{8}{5} \\right\\} \\;=\\; \\frac{9}{5}$.\n\nTherefore,\n$$\nV_{1} \\;=\\; T V_{0} \\;=\\; \\begin{pmatrix} \\frac{18}{5} \\\\ \\frac{109}{25} \\\\ \\frac{9}{5} \\end{pmatrix}.\n$$\n\nStep $2$: Compute $\\pi_{0}=\\operatorname{Greedy}(V_{0})$ and $\\pi_{1}=\\operatorname{Greedy}(V_{1})$.\n\nFrom the $Q$-values above for $V_{0}$:\n- At $s=1$, $\\max\\{Q(1,a;V_{0}), Q(1,b;V_{0})\\} = \\frac{18}{5}$, attained by action $a$.\n- At $s=2$, $\\max\\{Q(2,a;V_{0}), Q(2,b;V_{0})\\} = \\frac{109}{25}$, attained by action $b$.\n- At $s=3$, $\\max\\{Q(3,a;V_{0}), Q(3,b;V_{0})\\} = \\frac{9}{5}$, attained by action $a$.\nThus $\\pi_{0}=(a,b,a)$.\n\nFor $\\pi_{1}=\\operatorname{Greedy}(V_{1})$, compute $Q(\\cdot,\\cdot;V_{1})$:\n- At $s=1$:\n  $$\n  Q(1,a;V_{1}) \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{ \\frac{18}{5} + \\frac{109}{25} }{2} \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{199}{50} \\;=\\; 2 + \\frac{398}{125} \\;=\\; \\frac{648}{125},\n  $$\n  $$\n  Q(1,b;V_{1}) \\;=\\; 1 + \\frac{4}{5}\\cdot \\frac{9}{5} \\;=\\; 1 + \\frac{36}{25} \\;=\\; \\frac{61}{25}.\n  $$\n  Since $\\frac{648}{125} > \\frac{61}{25}$, choose $a$.\n- At $s=2$:\n  $$\n  Q(2,a;V_{1}) \\;=\\; 0 + \\frac{4}{5}\\cdot \\frac{18}{5} \\;=\\; \\frac{72}{25}, \\quad\n  Q(2,b;V_{1}) \\;=\\; 3 + \\frac{4}{5}\\left( \\frac{7}{10}\\cdot \\frac{109}{25} + \\frac{3}{10}\\cdot \\frac{9}{5} \\right).\n  $$\n  Compute the bracket:\n  $$\n  \\frac{7}{10}\\cdot \\frac{109}{25} + \\frac{3}{10}\\cdot \\frac{9}{5} \\;=\\; \\frac{763}{250} + \\frac{27}{50} \\;=\\; \\frac{763}{250} + \\frac{135}{250} \\;=\\; \\frac{898}{250} \\;=\\; \\frac{449}{125}.\n  $$\n  Thus\n  $$\n  Q(2,b;V_{1}) \\;=\\; 3 + \\frac{4}{5}\\cdot \\frac{449}{125} \\;=\\; 3 + \\frac{1796}{625} \\;=\\; \\frac{1875}{625} + \\frac{1796}{625} \\;=\\; \\frac{3671}{625}.\n  $$\n  Since $\\frac{3671}{625} > \\frac{72}{25}$, choose $b$.\n- At $s=3$:\n  $$\n  Q(3,a;V_{1}) \\;=\\; 1 + \\frac{4}{5}\\cdot \\frac{9}{5} \\;=\\; \\frac{61}{25}, \\quad\n  Q(3,b;V_{1}) \\;=\\; 0 + \\frac{4}{5}\\cdot \\frac{18}{5} \\;=\\; \\frac{72}{25}.\n  $$\n  Since $\\frac{72}{25} > \\frac{61}{25}$, choose $b$.\n\nHence $\\pi_{1}=(a,b,b)$.\n\nStep $3$: Exactly evaluate $V^{\\pi_{0}}$ and $V^{\\pi_{1}}$ by solving $V^{\\pi} = r^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$.\n\nWrite $V^{\\pi} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{pmatrix}$.\n\nFor $\\pi_{0}=(a,b,a)$:\n- State $1$ with action $a$: $x_{1} = 2 + \\frac{4}{5}\\left( \\frac{1}{2} x_{1} + \\frac{1}{2} x_{2} \\right) \\;\\Rightarrow\\; \\frac{3}{5} x_{1} - \\frac{2}{5} x_{2} = 2 \\;\\Rightarrow\\; 3 x_{1} - 2 x_{2} = 10$.\n- State $2$ with action $b$: $x_{2} = 3 + \\frac{4}{5}\\left( \\frac{7}{10} x_{2} + \\frac{3}{10} x_{3} \\right) \\;\\Rightarrow\\; \\frac{11}{25} x_{2} - \\frac{6}{25} x_{3} = 3 \\;\\Rightarrow\\; 11 x_{2} - 6 x_{3} = 75$.\n- State $3$ with action $a$: $x_{3} = 1 + \\frac{4}{5} x_{3} \\;\\Rightarrow\\; \\frac{1}{5} x_{3} = 1 \\;\\Rightarrow\\; x_{3}=5$.\n\nSubstitute $x_{3}=5$ into $11 x_{2} - 6 x_{3} = 75$ to get $11 x_{2} = 105$, so $x_{2} = \\frac{105}{11}$. Then $3 x_{1} - 2 x_{2} = 10$ gives $3 x_{1} = 10 + \\frac{210}{11} = \\frac{320}{11}$, hence $x_{1} = \\frac{320}{33}$. Therefore\n$$\nV^{\\pi_{0}} \\;=\\; \\begin{pmatrix} \\frac{320}{33} \\\\ \\frac{105}{11} \\\\ 5 \\end{pmatrix}.\n$$\n\nFor $\\pi_{1}=(a,b,b)$:\n- State $1$ with action $a$: $x_{1} = 2 + \\frac{4}{5}\\left( \\frac{1}{2} x_{1} + \\frac{1}{2} x_{2} \\right) \\;\\Rightarrow\\; 3 x_{1} - 2 x_{2} = 10$.\n- State $2$ with action $b$: $11 x_{2} - 6 x_{3} = 75$.\n- State $3$ with action $b$: $x_{3} = \\frac{4}{5} x_{1}$.\n\nUse $x_{3}=\\frac{4}{5} x_{1}$ in $11 x_{2} - 6 x_{3} = 75$ to get $11 x_{2} - \\frac{24}{5} x_{1} = 75 \\;\\Rightarrow\\; 55 x_{2} - 24 x_{1} = 375$. From $3 x_{1} - 2 x_{2} = 10$, express $x_{2} = \\frac{3}{2} x_{1} - 5$ and substitute:\n$$\n55\\left( \\frac{3}{2} x_{1} - 5 \\right) - 24 x_{1} = 375 \\;\\Rightarrow\\; \\frac{165}{2} x_{1} - 275 - 24 x_{1} = 375 \\;\\Rightarrow\\; \\frac{117}{2} x_{1} = 650,\n$$\nso $x_{1} = \\frac{1300}{117} = \\frac{100}{9}$, $x_{3} = \\frac{4}{5} x_{1} = \\frac{80}{9}$, and $x_{2} = \\frac{3}{2}\\cdot \\frac{100}{9} - 5 = \\frac{50}{3} - 5 = \\frac{35}{3}$. Therefore\n$$\nV^{\\pi_{1}} \\;=\\; \\begin{pmatrix} \\frac{100}{9} \\\\ \\frac{35}{3} \\\\ \\frac{80}{9} \\end{pmatrix}.\n$$\n\nStep $4$: Compute the optimal value $V^{\\star}$ and verify optimality.\n\nBy definition, $V^{\\star}$ satisfies $V^{\\star} = T V^{\\star}$. A candidate optimal stationary policy is $\\pi^{\\star}=(a,b,b)$, which we already analyzed; its value is $V^{\\pi^{\\star}}=\\begin{pmatrix} \\frac{100}{9} \\\\ \\frac{35}{3} \\\\ \\frac{80}{9} \\end{pmatrix}$. To verify optimality, check that at each state, the selected action maximizes the action-value computed with $V^{\\pi^{\\star}}$:\n- At $s=1$:\n  $$\n  Q(1,a;V^{\\pi^{\\star}}) \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{ \\frac{100}{9} + \\frac{35}{3} }{2}\n  \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{ \\frac{100}{9} + \\frac{105}{9} }{2}\n  \\;=\\; 2 + \\frac{4}{5}\\cdot \\frac{205}{18}\n  \\;=\\; \\frac{100}{9},\n  $$\n  while $Q(1,b;V^{\\pi^{\\star}}) = 1 + \\frac{4}{5}\\cdot \\frac{80}{9} = \\frac{73}{9}  \\frac{100}{9}$, so action $a$ is optimal.\n- At $s=2$:\n  $$\n  Q(2,b;V^{\\pi^{\\star}}) \\;=\\; 3 + \\frac{4}{5}\\left( \\frac{7}{10}\\cdot \\frac{35}{3} + \\frac{3}{10}\\cdot \\frac{80}{9} \\right)\n  \\;=\\; 3 + \\frac{4}{5}\\left( \\frac{49}{6} + \\frac{8}{3} \\right)\n  \\;=\\; 3 + \\frac{4}{5}\\left( \\frac{65}{6} \\right)\n  \\;=\\; 3 + \\frac{26}{3}\n  \\;=\\; \\frac{35}{3},\n  $$\n  while $Q(2,a;V^{\\pi^{\\star}}) = \\frac{4}{5}\\cdot \\frac{100}{9} = \\frac{80}{9}  \\frac{35}{3}$, so action $b$ is optimal.\n- At $s=3$:\n  $$\n  Q(3,b;V^{\\pi^{\\star}}) \\;=\\; \\frac{4}{5}\\cdot \\frac{100}{9} \\;=\\; \\frac{80}{9},\n  $$\n  while $Q(3,a;V^{\\pi^{\\star}}) = 1 + \\frac{4}{5}\\cdot \\frac{80}{9} = \\frac{73}{9}  \\frac{80}{9}$, so action $b$ is optimal.\n\nThus $\\pi^{\\star}=(a,b,b)$ is greedy with respect to its own value and hence optimal. Therefore $V^{\\star} = V^{\\pi^{\\star}} = \\begin{pmatrix} \\frac{100}{9} \\\\ \\frac{35}{3} \\\\ \\frac{80}{9} \\end{pmatrix}$.\n\nStep $5$: Compute the greedy-policy suboptimalities and the improvement $\\Delta$.\n\nBy definition,\n$$\n\\Delta_{\\infty}(\\pi_{0}) \\;=\\; \\| V^{\\star} - V^{\\pi_{0}} \\|_{\\infty}, \\quad\n\\Delta_{\\infty}(\\pi_{1}) \\;=\\; \\| V^{\\star} - V^{\\pi_{1}} \\|_{\\infty}.\n$$\nWe have $V^{\\pi_{1}}=V^{\\star}$, so $\\Delta_{\\infty}(\\pi_{1})=0$. For $\\pi_{0}$,\n$$\nV^{\\star} - V^{\\pi_{0}} \\;=\\; \\begin{pmatrix}\n\\frac{100}{9} - \\frac{320}{33} \\\\\n\\frac{35}{3} - \\frac{105}{11} \\\\\n\\frac{80}{9} - 5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{140}{99} \\\\\n\\frac{70}{33} \\\\\n\\frac{35}{9}\n\\end{pmatrix},\n$$\nso\n$$\n\\Delta_{\\infty}(\\pi_{0}) \\;=\\; \\max\\left\\{ \\frac{140}{99}, \\frac{70}{33}, \\frac{35}{9} \\right\\} \\;=\\; \\frac{35}{9}.\n$$\nTherefore, the improvement is\n$$\n\\Delta \\;=\\; \\Delta_{\\infty}(\\pi_{0}) - \\Delta_{\\infty}(\\pi_{1}) \\;=\\; \\frac{35}{9} - 0 \\;=\\; \\frac{35}{9}.\n$$\n\nThis strictly positive value verifies the improvement in greedy policy suboptimality after one step of value iteration and policy improvement.", "answer": "$$\\boxed{\\frac{35}{9}}$$", "id": "2738630"}, {"introduction": "While dynamic programming is powerful, real-world control problems often lack a precise model of the environment. This is where model-free reinforcement learning excels, and Q-learning stands as one of its most fundamental algorithms. This practice problem [@problem_id:2738645] allows you to simulate the learning process by performing step-by-step Q-learning updates, giving you a tangible feel for how an agent can learn optimal state-action values directly from a sequence of experiences.", "problem": "Consider a finite Markov Decision Process (MDP) with state set $\\mathcal{S}=\\{s_{0},s_{1},s_{2}\\}$, where $s_{2}$ is terminal, and action set $\\mathcal{A}=\\{a_{0},a_{1}\\}$. The one-step transition dynamics and rewards are deterministic and given by the following descriptions, which are consistent with the controlled Markov property and an additive reward signal:\n- From $s_{0}$:\n  - Taking $a_{0}$ leads to $s_{1}$ with reward $2$.\n  - Taking $a_{1}$ leads to $s_{2}$ with reward $0$.\n- From $s_{1}$:\n  - Taking $a_{0}$ leads to $s_{2}$ with reward $0$.\n  - Taking $a_{1}$ leads to $s_{1}$ with reward $-1$.\n- From $s_{2}$: the episode terminates immediately. For any evaluation that involves $\\max_{a'}$ in a terminal state, use the convention that the maximum over the empty action set equals $0$.\n\nYou will approximate the optimal state-action value function (the $Q$-function) using off-policy temporal-difference updates arising from the Bellman optimality operator, as in the standard Q-learning method in reinforcement learning (RL). The discount factor is $\\gamma=\\tfrac{1}{2}$. The initial action-value table over the nonterminal states is uniform: $Q_{0}(s,a)=1$ for all $(s,a)\\in\\{s_{0},s_{1}\\}\\times\\{a_{0},a_{1}\\}$. The step size (learning rate) is constant and equal to $\\alpha=\\tfrac{1}{2}$ for each update. You observe the following ordered sequence of transitions generated by some behavior policy (which need not be optimal):\n1. $(s,a,r,s')=(s_{0},a_{0},2,s_{1})$,\n2. $(s,a,r,s')=(s_{1},a_{1},-1,s_{1})$,\n3. $(s,a,r,s')=(s_{1},a_{0},0,s_{2})$.\n\nStarting from $Q_{0}$ and applying three off-policy Q-learning updates in the order of the transitions above, compute the resulting action-value table $Q_{3}$ over the nonterminal state-action pairs $(s_{0},a_{0})$, $(s_{0},a_{1})$, $(s_{1},a_{0})$, $(s_{1},a_{1})$. Report your final answer as a single row matrix in that order. No rounding is required, and exact fractional values are expected.", "solution": "The Q-learning update rule for a transition from state $s$ to state $s'$ with action $a$ and reward $r$ is given by:\n$$Q_{k+1}(s,a) = Q_{k}(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q_{k}(s', a') - Q_{k}(s,a) \\right]$$\nwhere $k$ is the update index, $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor. Given the problem parameters, we have $\\alpha=\\frac{1}{2}$ and $\\gamma=\\frac{1}{2}$. The update rule can be rewritten as:\n$$Q_{k+1}(s,a) = \\left(1-\\alpha\\right)Q_{k}(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q_{k}(s', a') \\right]$$\n$$Q_{k+1}(s,a) = \\frac{1}{2} Q_{k}(s,a) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{k}(s', a') \\right]$$\n\nThe initial action-value table for nonterminal states is given as $Q_{0}(s, a) = 1$ for all $(s, a) \\in \\{s_{0}, s_{1}\\} \\times \\{a_{0}, a_{1}\\}$. So we begin with:\n$$Q_{0}(s_{0}, a_{0}) = 1$$\n$$Q_{0}(s_{0}, a_{1}) = 1$$\n$$Q_{0}(s_{1}, a_{0}) = 1$$\n$$Q_{0}(s_{1}, a_{1}) = 1$$\nFor the terminal state $s_{2}$, we use the convention that $\\max_{a'} Q(s_{2}, a') = 0$.\n\nWe now proceed with the updates in the specified order.\n\n**Update 1:**\nThe first transition is $(s, a, r, s') = (s_{0}, a_{0}, 2, s_{1})$. We update $Q(s_{0}, a_{0})$. The other values in the table remain unchanged for this step. The next state is $s_{1}$. We must first compute the maximum Q-value from state $s_{1}$ using the current table, $Q_{0}$.\n$$\\max_{a'} Q_{0}(s_{1}, a') = \\max\\{Q_{0}(s_{1}, a_{0}), Q_{0}(s_{1}, a_{1})\\} = \\max\\{1, 1\\} = 1$$\nNow we apply the update rule for $Q_{1}(s_{0}, a_{0})$:\n$$Q_{1}(s_{0}, a_{0}) = \\frac{1}{2} Q_{0}(s_{0}, a_{0}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{0}(s_{1}, a') \\right]$$\n$$Q_{1}(s_{0}, a_{0}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ 2 + \\frac{1}{2}(1) \\right] = \\frac{1}{2} + \\frac{1}{2} \\left[ 2 + \\frac{1}{2} \\right] = \\frac{1}{2} + \\frac{1}{2} \\left( \\frac{5}{2} \\right) = \\frac{1}{2} + \\frac{5}{4} = \\frac{2}{4} + \\frac{5}{4} = \\frac{7}{4}$$\nAfter this first update, the action-value table $Q_{1}$ is:\n$$Q_{1}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{1}(s_{0}, a_{1}) = 1$$\n$$Q_{1}(s_{1}, a_{0}) = 1$$\n$$Q_{1}(s_{1}, a_{1}) = 1$$\n\n**Update 2:**\nThe second transition is $(s, a, r, s') = (s_{1}, a_{1}, -1, s_{1})$. We update $Q(s_{1}, a_{1})$ using the values from $Q_{1}$. The next state is $s_{1}$.\n$$\\max_{a'} Q_{1}(s_{1}, a') = \\max\\{Q_{1}(s_{1}, a_{0}), Q_{1}(s_{1}, a_{1})\\} = \\max\\{1, 1\\} = 1$$\nNow we apply the update rule for $Q_{2}(s_{1}, a_{1})$:\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{2} Q_{1}(s_{1}, a_{1}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{1}(s_{1}, a') \\right]$$\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ -1 + \\frac{1}{2}(1) \\right] = \\frac{1}{2} + \\frac{1}{2} \\left[ -1 + \\frac{1}{2} \\right] = \\frac{1}{2} + \\frac{1}{2} \\left( -\\frac{1}{2} \\right) = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}$$\nAfter this second update, the action-value table $Q_{2}$ is:\n$$Q_{2}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{2}(s_{0}, a_{1}) = 1$$\n$$Q_{2}(s_{1}, a_{0}) = 1$$\n$$Q_{2}(s_{1}, a_{1}) = \\frac{1}{4}$$\n\n**Update 3:**\nThe third transition is $(s, a, r, s') = (s_{1}, a_{0}, 0, s_{2})$. We update $Q(s_{1}, a_{0})$ using the values from $Q_{2}$. The next state $s_{2}$ is terminal.\n$$\\max_{a'} Q_{2}(s_{2}, a') = 0$$\nNow we apply the update rule for $Q_{3}(s_{1}, a_{0})$:\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2} Q_{2}(s_{1}, a_{0}) + \\frac{1}{2} \\left[ r + \\frac{1}{2} \\max_{a'} Q_{2}(s_{2}, a') \\right]$$\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2}(1) + \\frac{1}{2} \\left[ 0 + \\frac{1}{2}(0) \\right] = \\frac{1}{2} + 0 = \\frac{1}{2}$$\nAfter this final update, the action-value table $Q_{3}$ is:\n$$Q_{3}(s_{0}, a_{0}) = \\frac{7}{4}$$\n$$Q_{3}(s_{0}, a_{1}) = 1$$\n$$Q_{3}(s_{1}, a_{0}) = \\frac{1}{2}$$\n$$Q_{3}(s_{1}, a_{1}) = \\frac{1}{4}$$\nThe problem requires reporting these final values as a single row matrix in the order $(s_{0}, a_{0})$, $(s_{0}, a_{1})$, $(s_{1}, a_{0})$, $(s_{1}, a_{1})$. The values are $\\frac{7}{4}$, $1$, $\\frac{1}{2}$, and $\\frac{1}{4}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{7}{4}  1  \\frac{1}{2}  \\frac{1}{4} \\end{pmatrix}}$$", "id": "2738645"}, {"introduction": "Tabular methods like Q-learning are impractical for control systems with large or continuous state spaces. To overcome this, we approximate the value function using features of the state, a concept central to modern reinforcement learning and adaptive control. This exercise [@problem_id:2738612] delves into this crucial technique, guiding you through a one-step semi-gradient temporal-difference (TD) update to adjust the parameters of a linear value function approximator, which forms the critic component in many actor-critic architectures.", "problem": "Consider policy evaluation in a discounted Markov Decision Process (MDP) under a fixed, stabilizing feedback policy for a linear time-invariant stochastic control system. Let the state-value function under this policy be approximated by a linear function $V_{\\theta}(x) = \\theta^{\\top}\\phi(x)$, where $\\phi(x) \\in \\mathbb{R}^{3}$ is a known feature map and $\\theta \\in \\mathbb{R}^{3}$ is the parameter vector of the critic in an actor-critic architecture. At time step $k$, a single transition is observed with state $x_{k}$, next state $x_{k+1}$, and immediate reward $r_{k}$. Assume the following data are available:\n- Feature vectors $\\phi(x_{k}) = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$ and $\\phi(x_{k+1}) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 3 \\end{pmatrix}$,\n- Discount factor $\\gamma = \\frac{9}{10}$,\n- Stepsize $\\alpha = \\frac{1}{10}$,\n- Current parameter vector $\\theta_{k} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$,\n- Realized reward $r_{k} = 1$.\n\nStarting from the Bellman equation for the value function under a fixed policy and the projection of value functions onto the linear span of the features, derive the one-step semi-gradient Temporal-Difference (TD(0)) update for $V_{\\theta}$ specialized to a single observed transition, and apply it to compute the updated parameter vector $\\theta_{k+1}$. Express your final answer as a $3 \\times 1$ column vector exactly, without rounding.", "solution": "The objective of policy evaluation is to determine the state-value function $V_{\\pi}(x)$ for a given policy $\\pi$. For a discounted MDP, $V_{\\pi}(x)$ must satisfy the Bellman equation:\n$$V_{\\pi}(x) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V_{\\pi}(X_{t+1}) | X_t = x]$$\nwhere $R_{t+1}$ is the reward at the next time step, $X_{t+1}$ is the next state, and $\\gamma \\in [0, 1)$ is the discount factor.\n\nWhen the state space is large or continuous, we approximate $V_{\\pi}(x)$ with a parameterized function, in this case a linear combination of features: $V_{\\theta}(x) = \\theta^{\\top}\\phi(x)$. The goal is to find the parameter vector $\\theta$ such that $V_{\\theta}(x) \\approx V_{\\pi}(x)$.\n\nTemporal-Difference (TD) learning methods update the parameters based on experience. For a single observed transition $(x_k, r_k, x_{k+1})$, the TD($0$) method aims to reduce the TD error, $\\delta_k$, which is the difference between the TD target and the current value estimate:\n$$\\delta_k = r_k + \\gamma V_{\\theta_k}(x_{k+1}) - V_{\\theta_k}(x_k)$$\nThe TD target, $r_k + \\gamma V_{\\theta_k}(x_{k+1})$, is an improved estimate of the value of state $x_k$.\n\nThe update for the parameter vector $\\theta$ is derived using a stochastic semi-gradient descent approach to minimize the Mean Squared Bellman Error. The semi-gradient method ignores the dependency of the TD target on $\\theta$ during differentiation, which simplifies the update and improves convergence properties. The update rule for $\\theta_k$ is given by:\n$$\\theta_{k+1} = \\theta_k + \\alpha \\delta_k \\nabla_{\\theta} V_{\\theta_k}(x_k)$$\nFor our linear function approximator, $V_{\\theta}(x) = \\theta^{\\top}\\phi(x)$, the gradient with respect to $\\theta$ is simply the feature vector:\n$$\\nabla_{\\theta} V_{\\theta}(x) = \\phi(x)$$\nSubstituting this and the expression for $\\delta_k$ into the update rule, we obtain the one-step semi-gradient TD($0$) update for a single transition:\n$$\\theta_{k+1} = \\theta_k + \\alpha (r_k + \\gamma \\theta_k^{\\top}\\phi(x_{k+1}) - \\theta_k^{\\top}\\phi(x_k)) \\phi(x_k)$$\n\nNow, we apply this formula using the provided data.\nFirst, we compute the value estimates at states $x_k$ and $x_{k+1}$ using the current parameter vector $\\theta_k$:\n$$V_{\\theta_k}(x_k) = \\theta_k^{\\top}\\phi(x_k) = \\begin{pmatrix} 2  -1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix} = (2)(1) + (-1)(2) + (0)(-1) = 2 - 2 + 0 = 0$$\n$$V_{\\theta_k}(x_{k+1}) = \\theta_k^{\\top}\\phi(x_{k+1}) = \\begin{pmatrix} 2  -1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 3 \\end{pmatrix} = (2)(1) + (-1)(0) + (0)(3) = 2 - 0 + 0 = 2$$\n\nNext, we calculate the TD error, $\\delta_k$:\n$$\\delta_k = r_k + \\gamma V_{\\theta_k}(x_{k+1}) - V_{\\theta_k}(x_k) = 1 + \\left(\\frac{9}{10}\\right)(2) - 0 = 1 + \\frac{18}{10} = \\frac{10}{10} + \\frac{18}{10} = \\frac{28}{10}$$\n\nFinally, we compute the updated parameter vector $\\theta_{k+1}$:\n$$\\theta_{k+1} = \\theta_k + \\alpha \\delta_k \\phi(x_k)$$\nSubstituting the values:\n$$\\theta_{k+1} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\left(\\frac{1}{10}\\right) \\left(\\frac{28}{10}\\right) \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$$\n$$\\theta_{k+1} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\frac{28}{100} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$$\nSimplifying the fraction $\\frac{28}{100} = \\frac{7}{25}$:\n$$\\theta_{k+1} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\frac{7}{25} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 7/25 \\\\ 14/25 \\\\ -7/25 \\end{pmatrix}$$\nPerforming the vector addition:\n$$\\theta_{k+1} = \\begin{pmatrix} 2 + \\frac{7}{25} \\\\ -1 + \\frac{14}{25} \\\\ 0 - \\frac{7}{25} \\end{pmatrix} = \\begin{pmatrix} \\frac{50}{25} + \\frac{7}{25} \\\\ -\\frac{25}{25} + \\frac{14}{25} \\\\ -\\frac{7}{25} \\end{pmatrix} = \\begin{pmatrix} \\frac{57}{25} \\\\ -\\frac{11}{25} \\\\ -\\frac{7}{25} \\end{pmatrix}$$\n\nThis is the updated parameter vector after observing the single transition.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{57}{25} \\\\\n-\\frac{11}{25} \\\\\n-\\frac{7}{25}\n\\end{pmatrix}\n}\n$$", "id": "2738612"}]}