## Applications and Interdisciplinary Connections

The preceding sections have established the core theoretical pillars of Model Predictive Control (MPC): the formulation of a finite-horizon [optimal control](@entry_id:138479) problem and the crucial role of terminal sets and terminal costs in guaranteeing [recursive feasibility](@entry_id:167169) and closed-loop stability. While these principles were developed for nominal, disturbance-free systems, their true power lies in their extensibility. This section explores how the foundational concepts of MPC are applied, adapted, and integrated to solve complex control problems in diverse and interdisciplinary contexts. We will demonstrate that the rigorous framework of stability and feasibility is not merely a theoretical construct but a versatile toolkit for designing high-performance, provably safe controllers for systems subject to uncertainty, complex economic objectives, network interactions, and challenging structural properties.

### Robust and Adaptive Control

Real-world systems are invariably subject to uncertainty, which can manifest as external disturbances, [measurement noise](@entry_id:275238), or imprecise knowledge of the system's own parameters. A practical control design must be robust to these uncertainties, ensuring that safety constraints are respected and stability is maintained. MPC offers a systematic framework for achieving this robustness.

#### Handling Bounded Disturbances

A primary concern in control engineering is the presence of persistent, bounded disturbances. MPC provides several powerful strategies to handle such uncertainties. One of the most widespread and computationally efficient methods is **tube-based MPC**. This approach conceptually decouples the control problem into two parts: a nominal planning problem and an error-regulation problem. The actual system state $x_k$ is viewed as the sum of a nominal state $z_k$ and an error state $e_k = x_k - z_k$. The MPC controller is designed to optimize the trajectory of the nominal system, which is disturbance-free, while a separate, often linear, ancillary feedback controller $u_k = v_k + K e_k$ is designed to stabilize the error dynamics. The key challenge is to ensure that the real state $x_k$ and input $u_k$ respect their constraints, $x_k \in \mathcal{X}$ and $u_k \in \mathcal{U}$, even though the error $e_k$ is never zero. This is achieved by planning the nominal trajectory within strictly tightened constraint sets, $\mathcal{Z} \subseteq \mathcal{X} \ominus \mathcal{E}$ and $\mathcal{V} \subseteq \mathcal{U} \ominus K\mathcal{E}$, where $\mathcal{E}$ is a "tube" that is guaranteed to contain the error at all times. This guarantee is established by designing the ancillary controller $K$ such that the error dynamics are stable and constructing $\mathcal{E}$ as a Robust Positively Invariant (RPI) set. An RPI set has the property that if the error starts within it, it remains within it for all future times under all possible disturbance realizations. This elegant decomposition allows the online MPC optimization to remain a nominal (and often convex) problem, while robustness is guaranteed by the offline design of the tube and the corresponding [constraint tightening](@entry_id:174986) [@problem_id:2746566].

An alternative, and often more direct, approach to robust MPC is **min-max MPC**. Instead of relying on a tube decomposition, this method tackles the uncertainty head-on within the optimization problem itself. At each time step, the controller solves a min-max problem, seeking a control sequence that minimizes the [cost function](@entry_id:138681) under the worst-possible sequence of future disturbances. Robust [constraint satisfaction](@entry_id:275212) is enforced by requiring the state and input constraints to hold for all possible disturbance realizations over the [prediction horizon](@entry_id:261473). While conceptually straightforward, this formulation leads to a semi-infinite optimization problem, which is computationally far more demanding than the nominal optimization in a tube-based scheme. Nevertheless, for systems where the tube-based approach might be overly conservative, min-max MPC provides a powerful, albeit computationally intensive, alternative for ensuring [robust performance](@entry_id:274615) and safety [@problem_id:2746618].

The stability of systems under persistent disturbances is most appropriately characterized within the framework of **Input-to-State Stability (ISS)**. A system is ISS if its state trajectories are bounded by a function of the initial state, which decays to zero over time, and a function of the magnitude of the disturbance input. The core theoretical tool for proving ISS is an ISS-Lyapunov function. For an MPC-controlled system, one can often show that the optimal [value function](@entry_id:144750) $V_N^\star(x)$ serves as an ISS-Lyapunov function. This is established by proving a [dissipation inequality](@entry_id:188634) of the form $V(x_{k+1}) - V(x_k) \le -\alpha(\|x_k\|) + \sigma(\|w_k\|)$, where $\alpha(\cdot)$ is a class-$\mathcal{K}$ function capturing the decay due to the control action and $\sigma(\cdot)$ is a class-$\mathcal{K}$ function capturing the potential increase due to the disturbance $w_k$. This inequality elegantly demonstrates that in the absence of disturbances ($w_k = 0$), the system is asymptotically stable, while in their presence, the state is guaranteed to remain in a bounded region whose size depends on the bound of the disturbance. This provides a more nuanced and powerful characterization of stability than simple practical stability [@problem_id:2746598].

#### State Estimation and Parametric Uncertainty

The assumption of full [state feedback](@entry_id:151441) is often unrealistic. In **output-feedback MPC**, the controller is provided only with noisy measurements of the system outputs, $y_k = C x_k + v_k$. This necessitates the use of a [state estimator](@entry_id:272846), such as a Luenberger observer, to generate an estimate $\hat{x}_k$ of the true state $x_k$. The MPC controller then operates on this state estimate. This architecture introduces a new source of error: the [estimation error](@entry_id:263890) $e_k = x_k - \hat{x}_k$. The dynamics of this error are driven by both the process noise $w_k$ and the measurement noise $v_k$. To ensure robust [constraint satisfaction](@entry_id:275212) for the true system, the [estimation error](@entry_id:263890) must be treated as another bounded disturbance. This is naturally handled within the tube-based MPC framework, where a robust positively [invariant set](@entry_id:276733) $\mathcal{E}_e$ is constructed for the [estimation error](@entry_id:263890) dynamics. The nominal MPC problem is then solved for the estimated state, subject to constraints that are tightened by the radius of this error tube $\mathcal{E}_e$. This effectively separates the design of the estimator and the controller, while rigorously accounting for their interaction.

### Economic Model Predictive Control

Traditional MPC is designed to regulate a system to a known, desired steady-state, with a cost function that penalizes deviations from this setpoint. However, in many industrial applications, from chemical processing to power grid management, the primary goal is not to track a reference but to operate the system in a way that optimizes an economic objective, such as minimizing operating cost, maximizing production rate, or maximizing profit. This has given rise to **Economic MPC (eMPC)**.

#### From Tracking to Economic Optimization

The defining feature of eMPC is the replacement of the positive-definite quadratic tracking cost with a general stage cost $\ell(x,u)$ that directly represents the economic performance index. This economic cost is not necessarily minimized at a pre-specified setpoint; indeed, the most economical operating point or trajectory is typically unknown a priori and should emerge as a result of the optimization. The objective of the receding-horizon controller is to steer the system towards its most profitable mode of operation, dynamically responding to changing conditions. The ultimate goal is to optimize the long-run average performance, such as $\limsup_{T\to\infty}\frac{1}{T}\sum_{k=0}^{T-1} \ell(x_k,u_k)$, rather than to simply stabilize a fixed point [@problem_id:2701652].

#### Stability of Economic MPC and Dissipativity

This shift in [objective function](@entry_id:267263) poses a significant theoretical challenge: if the stage cost $\ell(x,u)$ is not positive-definite with respect to a target, how can closed-loop stability be guaranteed? The MPC value function no longer inherently serves as a Lyapunov function. The key to resolving this lies in the concept of **[dissipativity](@entry_id:162959)**. A system is said to be strictly dissipative with respect to an economically optimal steady-state $(x^\star, u^\star)$ if there exists a "storage function" $S(x)$ such that the economic supply rate, $\ell(x,u) - \ell(x^\star, u^\star)$, is greater than or equal to the change in storage, $S(f(x,u)) - S(x)$, plus a term that is positive-definite in the distance from the optimum. This property allows for the construction of a "rotated" stage cost, $\ell_r(x,u) = \ell(x,u) - \ell(x^\star, u^\star) + S(x) - S(f(x,u))$, which can be proven to be positive-definite with respect to the optimal steady-state. The value function of the eMPC problem formulated with this rotated cost can then serve as a valid Lyapunov function, enabling a rigorous proof of [asymptotic stability](@entry_id:149743). Dissipativity thus provides the crucial theoretical bridge between a general economic objective and the Lyapunov-based stability guarantees of MPC [@problem_id:2701669].

A fascinating consequence of this framework is the **turnpike phenomenon**. Intuitively, if one wants to drive a system from an initial state to a terminal state over a very long time, the most efficient path is to quickly move to the most economical steady-state, operate there for most of the duration (the "turnpike"), and then exit to the terminal state at the end. The theory of [dissipativity](@entry_id:162959) allows this intuition to be made precise. The strict [dissipativity](@entry_id:162959) inequality can be used to derive a rigorous upper bound on the number of time steps an optimal trajectory can spend far away from the economically optimal steady-state. This bound is directly proportional to the "suboptimality gap" of the finite-[horizon problem](@entry_id:161031), providing a clear quantitative link between the system's economic properties and its dynamic behavior under eMPC [@problem_id:2746621].

### Control of Complex and Networked Systems

The principles of MPC are not limited to single, monolithic systems. They can be extended to control large-scale, interconnected networks, and to handle systems with complex internal structures such as delays and logical switching.

#### Large-Scale and Distributed Systems

Controlling a large-scale system, such as a power grid or a manufacturing plant, with a single, centralized MPC is often infeasible due to computational burden and communication limitations. This has motivated the development of multi-agent MPC architectures. These can be broadly classified by their information structure. In **decentralized MPC**, each subsystem (or agent) makes decisions based only on its local information, treating the influence of its neighbors as a disturbance. This requires no communication but can lead to poor performance. In contrast, **distributed MPC** involves peer-to-peer negotiation, where agents iteratively exchange information (such as predicted trajectories or dual variables) to coordinate their actions and converge towards a collectively optimal solution. A third paradigm is **hierarchical MPC**, where a high-level coordinator optimizes an aggregated model of the system and communicates directives (e.g., setpoints, prices, or constraints) to local MPC controllers, which then optimize their subsystems subject to these directives. The latter is particularly well-suited for hierarchical eMPC, where a central coordinator can set prices for shared resources, and local agents optimize their economic performance in response to these prices [@problem_id:2701637] [@problem_id:2701669] [@problem_id:2701652].

Implementing distributed MPC often involves iterative optimization algorithms like the Alternating Direction Method of Multipliers (ADMM). A practical challenge is that these algorithms must be terminated after a finite number of iterations at each time step, meaning the computed solution is inexact and may violate coupling constraints. To guarantee safety and [recursive feasibility](@entry_id:167169), robust techniques are required. One effective strategy is to tighten the coupling constraints by a margin that is known to bound the "consensus error" from the inexact optimization. The controller then applies the computed input only if the run-time residual is within this margin; otherwise, it reverts to a safe, pre-computed fallback input. This combination of design-time robustification and run-time verification is critical for the safe deployment of [distributed optimization](@entry_id:170043)-based control [@problem_id:2746596].

#### Systems with Delays, Hybrid Dynamics, and Network Effects

Many physical and engineered systems exhibit challenging structural properties that can be elegantly handled within the MPC framework. A common issue is the presence of **input delays**, where a control action taken at time $k$ only affects the plant at a future time $k+d$. This can be systematically addressed using **[state augmentation](@entry_id:140869)**. By defining a new, augmented state vector that includes not only the physical state of the plant but also the sequence of past inputs that are still "in the pipeline," the delayed system can be transformed into a higher-dimensional but delay-free linear system. Standard MPC theory, including terminal sets and costs, can then be directly applied to this augmented system to guarantee stability and [recursive feasibility](@entry_id:167169) [@problem_id:2746604].

Another important class of systems are **[hybrid systems](@entry_id:271183)**, which feature an interaction between [continuous dynamics](@entry_id:268176) and discrete logic (e.g., on/off states, switching modes). Controlling such systems with MPC naturally leads to a Mixed-Integer Programming (MIP) problem at each time step, which is computationally expensive and poses significant challenges for guaranteeing [recursive feasibility](@entry_id:167169). The standard "shift-and-append" argument for feasibility can fail because the optimal sequence of discrete modes may change from one time step to the next. One strategy to recover guaranteed feasibility is to use a convex inner-approximation. This involves restricting the prediction to a single, predetermined "safe" mode for which stabilizing terminal ingredients are known to exist. This converts the MIP into a convex problem, sacrificing some optimality for the sake of computational tractability and, crucially, a provable guarantee of [recursive feasibility](@entry_id:167169) [@problem_id:2746574].

Finally, the principles of robust MPC find direct application in **Networked Control Systems (NCS)**, where control signals are transmitted over unreliable communication channels. Consider a system subject to bounded consecutive **packet dropouts**. When a packet is received, a feedback law can be applied. During a dropout, the actuator might revert to an open-loop strategy, such as applying a pre-buffered sequence of inputs. During this open-loop phase, the error between the true state and the nominal plan can grow, driven by the system's open-loop dynamics. The tube-based MPC framework allows for a precise analysis of this scenario. The maximum tolerable number of consecutive dropouts is determined by the point at which the [worst-case error](@entry_id:169595) growth would cause the state to exit its designated robust tube, violating the premises of [constraint satisfaction](@entry_id:275212) and [recursive feasibility](@entry_id:167169). This provides a direct link between the communication network's [quality of service](@entry_id:753918) and the control system's stability and safety [@problem_id:2746617].

### Practical Implementation and Suboptimality

A final, critical consideration is that the optimization problem at the core of MPC must be solved online, at each sampling instant. For complex systems, finding the globally [optimal control](@entry_id:138479) sequence within the allotted time is often impossible. This reality has led to the study of **suboptimal MPC**, where the optimizer is terminated early but provides a feasible solution whose cost is guaranteed to be within a certain bound of the true optimum. For example, the computed cost $\hat{J}_N$ might be guaranteed to be no more than a factor $\kappa \ge 1$ times the true optimal cost $V_N^\star$. This suboptimality introduces a potential "ascent" term in the standard Lyapunov decrease inequality. Stability can still be guaranteed, but only if this ascent is dominated by the guaranteed cost decrease from the control action. This leads to a relaxed decrease condition and a constraint on the allowable suboptimality factor, $\kappa$. For stability to hold, $\kappa$ must be sufficiently close to 1, with the exact bound depending on properties of the system and the cost function. This analysis is crucial for bridging the gap between control theory and real-time implementation, providing a rigorous way to reason about the stability of systems controlled by practical, inexact optimization algorithms [@problem_id:2746573].

In conclusion, the foundational principles of MPC stability and feasibility are remarkably versatile. They serve as the basis for a wide array of advanced control strategies, enabling the design of robust, adaptive, and economically optimal controllers for systems with complex dynamics, network interactions, and practical computational constraints. This demonstrates the power of MPC as a unifying framework for modern control engineering.