## Introduction
The [state-space representation](@entry_id:147149) is a cornerstone of modern control theory, providing a powerful and unified framework for modeling dynamic systems. However, representation is only the first step; the ultimate goal is to predict and analyze a system's behavior over time. This requires finding the explicit solution to the governing [non-homogeneous state equation](@entry_id:270918), which accounts for both the system's initial state and the influence of external inputs. This article bridges the gap between system modeling and dynamic analysis by systematically deriving the complete solution for [linear systems](@entry_id:147850).

This journey will be structured into three comprehensive chapters. First, in **Principles and Mechanisms**, we will delve into the mathematical heart of the problem, deriving the solution from first principles for both time-invariant and [time-varying systems](@entry_id:175653) using techniques like the [variation of constants](@entry_id:196393) and the Laplace transform. Next, **Applications and Interdisciplinary Connections** will demonstrate the immense practical utility of these solutions, showing how they provide crucial insights into phenomena in fields ranging from mechanical engineering and electrical circuits to digital control and stochastic processes. Finally, **Hands-On Practices** will provide curated problems to solidify your understanding and bridge the gap between theoretical knowledge and computational implementation.

## Principles and Mechanisms

The analysis of dynamic systems described by non-homogeneous [state equations](@entry_id:274378) is a cornerstone of control theory. Having established the [state-space representation](@entry_id:147149), we now turn to the central task of finding the explicit solution for the [state vector](@entry_id:154607) over time. This chapter will systematically derive the solution for linear systems, beginning with the foundational [principle of superposition](@entry_id:148082) and progressing from the more common time-invariant case to the general time-varying framework. We will explore multiple solution methodologies, including time-domain integration, Laplace transforms, and eigen-decomposition, to provide a comprehensive and versatile toolkit for [system analysis](@entry_id:263805).

### The Principle of Superposition: Decomposing the System Response

A fundamental property of any linear system is that it obeys the **principle of superposition**. For a system described by the state equation $\dot{x}(t) = A(t)x(t) + B(t)u(t)$ with initial condition $x(t_0) = x_0$, the total response of the state vector $x(t)$ can be thought of as the sum of two distinct components: the response generated by the initial state alone, and the response generated by the external input alone.

This decomposition is not merely a mathematical convenience; it reflects a deep physical intuition. The evolution of a system's state is influenced by two factors: its internal energy or configuration at the start (represented by $x_0$), and the external energy or forces driving it over time (represented by $u(t)$). Linearity ensures that these two influences do not interact in a complex, nonlinear fashion, and their effects can be calculated separately and then added.

Formally, we express the total solution $x(t)$ as:

$x(t) = x_{zi}(t) + x_{zs}(t)$

Here, the two components are:

1.  **Zero-Input Response (ZIR)**: The term $x_{zi}(t)$ represents the system's natural evolution from its initial state $x_0$ in the absence of any external input (i.e., with $u(t) \equiv 0$). It is the solution to the homogeneous initial value problem:
    $\dot{x}_{zi}(t) = A(t)x_{zi}(t)$, with $x_{zi}(t_0) = x_0$.

2.  **Zero-State Response (ZSR)**: The term $x_{zs}(t)$ represents the system's response to the external input $u(t)$ when starting from a state of rest (i.e., with $x(t_0) = 0$). It is the solution to the non-homogeneous initial value problem with a zero initial condition:
    $\dot{x}_{zs}(t) = A(t)x_{zs}(t) + B(t)u(t)$, with $x_{zs}(t_0) = 0$.

This decomposition is powerful because it allows us to analyze the effects of initial conditions and inputs independently. For example, the property of homogeneity, a direct consequence of linearity, states that scaling the input by a constant factor $K$ will scale the [zero-state response](@entry_id:273280) by the same factor. If a unit step input $u_1(t)=1$ produces a zero-state output $y_{zs,1}(t)$, then an input $u_2(t)=K$ will produce the output $y_{zs,2}(t) = K y_{zs,1}(t)$, assuming the system starts from rest in both cases [@problem_id:1611755]. This predictable scaling is a hallmark of linear systems.

### Solution of Continuous-Time LTI Systems

We begin our detailed analysis with the most frequently encountered class of systems: Linear Time-Invariant (LTI) systems, where the matrices $A$ and $B$ are constant. The state equation is:

$\dot{x}(t) = Ax(t) + Bu(t), \quad x(0) = x_0$

#### The Zero-Input Response and the Matrix Exponential

First, consider the zero-input case, which describes the system's natural dynamics:
$\dot{x}(t) = Ax(t), \quad x(0) = x_0$

In the scalar case ($\dot{x} = ax$), the solution is the familiar exponential function $x(t) = e^{at}x_0$. For the vector case, the solution is analogous, involving the **matrix exponential**, defined by its infinite [series expansion](@entry_id:142878):

$e^{At} = \sum_{k=0}^{\infty} \frac{(At)^k}{k!} = I + At + \frac{A^2t^2}{2!} + \frac{A^3t^3}{3!} + \cdots$

The [matrix exponential](@entry_id:139347) $e^{At}$ plays the role of a **[state-transition matrix](@entry_id:269075)** for LTI systems, often denoted $\Phi(t) = e^{At}$. It can be shown by differentiating the series term-by-term that $\frac{d}{dt}e^{At} = A e^{At}$. Therefore, the [zero-input response](@entry_id:274925) is given by:

$x_{zi}(t) = e^{At}x_0$

This expression elegantly describes how the system's state evolves from $x_0$ under its own internal dynamics.

#### The Complete Solution via Variation of Constants

To solve the full non-homogeneous equation, we employ a powerful technique known as the **[variation of parameters](@entry_id:173919)** (or [variation of constants](@entry_id:196393)). We hypothesize a solution of the form $x(t) = e^{At}c(t)$, where $c(t)$ is a time-varying vector that we need to determine, replacing the constant parameter $x_0$ from the homogeneous solution. Differentiating this [ansatz](@entry_id:184384) using the [product rule](@entry_id:144424) gives:

$\dot{x}(t) = \frac{d}{dt}(e^{At})c(t) + e^{At}\dot{c}(t) = A e^{At}c(t) + e^{At}\dot{c}(t)$

Recognizing that $e^{At}c(t) = x(t)$, this becomes $\dot{x}(t) = Ax(t) + e^{At}\dot{c}(t)$. We then compare this to the original state equation $\dot{x}(t) = Ax(t) + Bu(t)$. The terms $Ax(t)$ match, leaving us with:

$e^{At}\dot{c}(t) = Bu(t)$

To solve for $\dot{c}(t)$, we pre-multiply by the inverse of the matrix exponential, which is $(e^{At})^{-1} = e^{-At}$:

$\dot{c}(t) = e^{-At}Bu(t)$

Integrating from $0$ to $t$ yields $c(t)$:

$c(t) - c(0) = \int_{0}^{t} e^{-A\tau}B u(\tau) d\tau$

The initial condition $c(0)$ is found by evaluating our ansatz at $t=0$: $x(0) = e^{A \cdot 0}c(0) = I c(0) = c(0)$. Thus, $c(0) = x_0$. Substituting this back, we get:

$c(t) = x_0 + \int_{0}^{t} e^{-A\tau}B u(\tau) d\tau$

Finally, we find the complete solution $x(t)$ by substituting this expression for $c(t)$ back into our ansatz $x(t) = e^{At}c(t)$:

$x(t) = e^{At} \left( x_0 + \int_{0}^{t} e^{-A\tau}B u(\tau) d\tau \right)$

Distributing $e^{At}$ gives the final solution, often called the **[variation of constants](@entry_id:196393) formula**:

$x(t) = e^{At}x_0 + e^{At} \int_{0}^{t} e^{-A\tau}B u(\tau) d\tau$

Since $e^{At}$ is constant with respect to the integration variable $\tau$, we can move it inside the integral. Using the property $e^{At}e^{-A\tau} = e^{A(t-\tau)}$, we arrive at the most common form of the solution:

$x(t) = \underbrace{e^{At}x_0}_{\text{Zero-Input Response}} + \underbrace{\int_{0}^{t} e^{A(t-\tau)}B u(\tau) d\tau}_{\text{Zero-State Response}}$

This elegant formula is the complete solution to any LTI state-space system. It perfectly embodies the [superposition principle](@entry_id:144649) [@problem_id:2746286] [@problem_id:2746237]. The first term, $x_{zi}(t) = e^{At}x_0$, is the [zero-input response](@entry_id:274925). The second term, $x_{zs}(t) = \int_{0}^{t} e^{A(t-\tau)}B u(\tau) d\tau$, is the [zero-state response](@entry_id:273280). This integral is a **[convolution integral](@entry_id:155865)**. It expresses the [forced response](@entry_id:262169) as a weighted sum over the entire history of the input $u(\tau)$ from $\tau=0$ to $\tau=t$. The weighting function, $e^{A(t-\tau)}B$, is the system's **impulse [response matrix](@entry_id:754302)**; it describes the state at time $t$ due to an impulse applied at time $\tau$.

Given a system's parameters and its total response $x(t)$, one can reverse-engineer this decomposition. By calculating the [zero-input response](@entry_id:274925) $x_{zi}(t) = e^{At}x_0$ independently, the [zero-state response](@entry_id:273280) can be found simply by subtraction: $x_{zs}(t) = x(t) - x_{zi}(t)$ [@problem_id:1611722] [@problem_id:1611777].

#### An Alternative Perspective: The Laplace Transform

For LTI systems, the Laplace transform provides a powerful alternative method that converts the differential equation in the time domain into an algebraic equation in the frequency domain (or $s$-domain) [@problem_id:2746263].

We begin with the state equation $\dot{x}(t) = Ax(t) + Bu(t)$. Applying the unilateral Laplace transform $\mathcal{L}\{\cdot\}$ to both sides, and using the differentiation property $\mathcal{L}\{\dot{x}(t)\} = sX(s) - x(0)$, we get:

$sX(s) - x_0 = AX(s) + BU(s)$

where $X(s) = \mathcal{L}\{x(t)\}$ and $U(s) = \mathcal{L}\{u(t)\}$. This is now an algebraic equation for the transformed [state vector](@entry_id:154607) $X(s)$. We can solve for $X(s)$ by rearranging the terms:

$sX(s) - AX(s) = x_0 + BU(s)$
$(sI - A)X(s) = x_0 + BU(s)$

Pre-multiplying by the inverse of the matrix $(sI - A)$ gives the solution in the $s$-domain:

$X(s) = \underbrace{(sI - A)^{-1}x_0}_{\text{ZIR}} + \underbrace{(sI - A)^{-1}BU(s)}_{\text{ZSR}}$

The matrix $(sI-A)^{-1}$ is known as the **resolvent matrix**. To return to the time domain, we apply the inverse Laplace transform. This requires two key identities:
1.  The inverse Laplace transform of the resolvent is the [matrix exponential](@entry_id:139347): $\mathcal{L}^{-1}\{(sI - A)^{-1}\} = e^{At}$.
2.  The **convolution theorem**: $\mathcal{L}^{-1}\{F_1(s)F_2(s)\} = (f_1 * f_2)(t) = \int_0^t f_1(t-\tau)f_2(\tau)d\tau$.

Applying these to the two terms in $X(s)$:
- The first term gives the [zero-input response](@entry_id:274925): $\mathcal{L}^{-1}\{(sI - A)^{-1}x_0\} = (\mathcal{L}^{-1}\{(sI - A)^{-1}\})x_0 = e^{At}x_0$.
- For the second term, we apply the [convolution theorem](@entry_id:143495) with $f_1(t) = \mathcal{L}^{-1}\{(sI - A)^{-1}B\} = e^{At}B$ and $f_2(t) = \mathcal{L}^{-1}\{U(s)\} = u(t)$. This gives the [zero-state response](@entry_id:273280): $\mathcal{L}^{-1}\{(sI - A)^{-1}BU(s)\} = \int_0^t e^{A(t-\tau)}B u(\tau) d\tau$.

Summing these two parts, we recover the exact same time-domain solution derived via [variation of constants](@entry_id:196393), confirming the consistency of our methods.

#### A Computational Shortcut: Solution in the Eigenbasis

When the system matrix $A$ is **diagonalizable**, the dynamics can be greatly simplified by a [change of coordinates](@entry_id:273139) into the basis of its eigenvectors [@problem_id:2746253]. If $A$ has a full set of [linearly independent](@entry_id:148207) eigenvectors, we can form a modal matrix $V$ whose columns are these eigenvectors. This matrix diagonalizes $A$ via the similarity transformation $A = V\Lambda V^{-1}$, where $\Lambda$ is a [diagonal matrix](@entry_id:637782) of the corresponding eigenvalues.

Let's define a new [state vector](@entry_id:154607) $z(t)$ in this [eigenbasis](@entry_id:151409) such that $x(t) = Vz(t)$. Substituting this into the state equation:

$V\dot{z}(t) = A(Vz(t)) + Bu(t) = (V\Lambda V^{-1})Vz(t) + Bu(t) = V\Lambda z(t) + Bu(t)$

Pre-multiplying by $V^{-1}$ yields a new state equation for $z(t)$:

$\dot{z}(t) = \Lambda z(t) + V^{-1}Bu(t)$

The crucial advantage is that since $\Lambda$ is diagonal, this vector equation represents a set of $n$ completely decoupled scalar differential equations. Each component $z_i(t)$ evolves independently according to:

$\dot{z}_i(t) = \lambda_i z_i(t) + (V^{-1}Bu(t))_i$

where $\lambda_i$ is the $i$-th eigenvalue. These are simple first-order linear ODEs that can be solved easily. Once the solution for the transformed [state vector](@entry_id:154607) $z(t)$ is found, we can transform back to the original coordinates to find the final solution for $x(t)$ using the relation $x(t) = Vz(t)$. This method is often computationally more straightforward than calculating the full [matrix exponential](@entry_id:139347) $e^{At}$ directly, especially for [high-dimensional systems](@entry_id:750282).

### Solution of Discrete-Time LTI Systems

The principles developed for [continuous-time systems](@entry_id:276553) have direct analogs in the discrete-time domain. Consider the discrete-time LTI system described by the [linear recurrence relation](@entry_id:180172):

$x_{k+1} = Ax_k + Bu_k, \quad x_0 \text{ is given.}$

Instead of integration, the solution is found by recursive substitution [@problem_id:2746287]:

For $k=1$: $x_1 = Ax_0 + Bu_0$
For $k=2$: $x_2 = Ax_1 + Bu_1 = A(Ax_0 + Bu_0) + Bu_1 = A^2x_0 + ABu_0 + Bu_1$
For $k=3$: $x_3 = Ax_2 + Bu_2 = A(A^2x_0 + ABu_0 + Bu_1) + Bu_2 = A^3x_0 + A^2Bu_0 + ABu_1 + Bu_2$

By observing the pattern, we can deduce the general formula, which can be formally proven by induction:

$x_k = A^k x_0 + \sum_{i=0}^{k-1} A^{k-1-i} B u_i$

Again, this solution perfectly displays the principle of superposition. The state at time step $k$ is the sum of:
1.  **Zero-Input Response**: $x_{zi,k} = A^k x_0$, which describes the evolution from the initial state if the input were always zero. Here, $A^k$ is the discrete-time [state-transition matrix](@entry_id:269075).
2.  **Zero-State Response**: $x_{zs,k} = \sum_{i=0}^{k-1} A^{k-1-i} B u_i$, which is the response to the input sequence $\{u_i\}$ assuming a zero initial state.

This summation is a **[discrete convolution](@entry_id:160939)**. If we define the system's impulse response sequence as $H_j = c^{\top}A^{j-1}B$ for $j \ge 1$ and $H_0=0$ (for a system with no direct feedthrough from input to output), the forced component of the output $y_k = c^\top x_k$ can be written as $y_{f,k} = \sum_{i=0}^{k-1} u_i (c^{\top} A^{k-1-i} B) = \sum_{i=0}^{k-1} u_i H_{k-i}$, which is the standard form of a [discrete convolution](@entry_id:160939) $(u * H)_k$.

### The General Case: Linear Time-Varying (LTV) Systems

We now generalize our findings to Linear Time-Varying (LTV) systems, where the $A$ and $B$ matrices are functions of time.

$\dot{x}(t) = A(t)x(t) + B(t)u(t), \quad x(t_0) = x_0$

#### The State-Transition Matrix

For LTV systems, the simple [matrix exponential](@entry_id:139347) $e^{A(t-t_0)}$ is no longer the solution to the homogeneous equation. This is because, in general, the matrix $A(t)$ does not commute with its integral, or with itself at different points in time (i.e., $A(t_1)A(t_2) \neq A(t_2)A(t_1)$).

Instead, we must define a more general **State-Transition Matrix (STM)**, denoted $\Phi(t, t_0)$, which maps the state from an initial time $t_0$ to a future time $t$. The STM is defined as the unique solution to the matrix differential equation [@problem_id:2746252]:

$\frac{\partial}{\partial t}\Phi(t, t_0) = A(t)\Phi(t, t_0)$, with the initial condition $\Phi(t_0, t_0) = I$

With this definition, the [zero-input response](@entry_id:274925) for an LTV system is $x_{zi}(t) = \Phi(t, t_0)x_0$. The STM also satisfies other important properties, including the composition rule $\Phi(t, s)\Phi(s, t_0) = \Phi(t, t_0)$ and a differential equation with respect to its second argument: $\frac{\partial}{\partial t_0}\Phi(t, t_0) = -\Phi(t, t_0)A(t_0)$.

#### The General Solution Formula

Using the LTV [state-transition matrix](@entry_id:269075) $\Phi(t, t_0)$, we can apply the method of [variation of constants](@entry_id:196393) just as we did for the LTI case. The derivation is structurally identical, leading to the general solution for LTV systems [@problem_id:2746251]:

$x(t) = \Phi(t, t_0)x_0 + \int_{t_0}^{t} \Phi(t, \tau) B(\tau) u(\tau) d\tau$

This is the most general solution for any linear state-space system. The [zero-input response](@entry_id:274925) is propagated forward by $\Phi(t, t_0)$, and the [zero-state response](@entry_id:273280) is a convolution integral where the [propagator](@entry_id:139558) kernel is now the time-varying function $\Phi(t, \tau)$. This kernel correctly propagates the effect of an input at time $\tau$ forward to the current time $t$.

#### The Peano-Baker Series and the Time-Ordered Exponential

While we have a formal solution, the question of how to compute $\Phi(t, t_0)$ remains. For the general LTV case, there is no simple [closed-form expression](@entry_id:267458). The STM can be represented by an infinite series known as the **Peano-Baker series**, generated by Picard iteration on its defining differential equation [@problem_id:2746231]:

$\Phi(t,t_0) = I + \int_{t_0}^t A(\sigma_1)d\sigma_1 + \int_{t_0}^t \int_{t_0}^{\sigma_1} A(\sigma_1)A(\sigma_2)d\sigma_2 d\sigma_1 + \cdots$

Notice the structure of the nested integrals imposes a strict time ordering on the matrix products: $t \ge \sigma_1 \ge \sigma_2 \ge \cdots$. This series is compactly written using the **[time-ordering operator](@entry_id:148044)** $\mathcal{T}$ as a time-ordered exponential:

$\Phi(t,t_0) = \mathcal{T}\exp\left(\int_{t_0}^t A(\sigma)d\sigma\right)$

This notation makes it explicit why the ordinary exponential fails: the ordinary exponential assumes the arguments of the products commute, which would allow the nested integrals to be simplified. The [time-ordering operator](@entry_id:148044) $\mathcal{T}$ enforces the necessary [causal structure](@entry_id:159914) when $A(t_1)$ and $A(t_2)$ do not commute. This advanced concept provides the formal mathematical foundation for the solution of general [linear time-varying systems](@entry_id:203710).