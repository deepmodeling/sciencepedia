{"hands_on_practices": [{"introduction": "While the matrix exponential is defined by an infinite series, for any given matrix it can be expressed as a finite polynomial in that matrix. This exercise [@problem_id:1156702] provides hands-on practice with one of the most powerful analytical techniques for this task, which leverages the Cayley-Hamilton theorem. Mastering this method is essential for understanding how a system's eigenvalues, including their multiplicities, directly shape the structure of its state-transition matrix $e^{tA}$.", "problem": "A physical system's state, described by a vector $\\mathbf{x}(t) \\in \\mathbb{R}^3$, evolves in time according to the linear system of ordinary differential equations $\\frac{d\\mathbf{x}}{dt} = A\\mathbf{x}$, where $A$ is a constant $3 \\times 3$ matrix. The solution to this system is given by $\\mathbf{x}(t) = e^{tA}\\mathbf{x}(0)$, where $e^{tA}$ is the matrix exponential.\n\nA powerful method for computing the matrix exponential $e^{tA}$ for an $n \\times n$ matrix $A$ relies on the Cayley-Hamilton theorem, which states that any square matrix satisfies its own characteristic equation. A consequence of this theorem is that $e^{tA}$ can be expressed as a polynomial in $A$ of degree at most $n-1$:\n$$e^{tA} = \\sum_{j=0}^{n-1} \\alpha_j(t) A^j$$\nThe scalar coefficients $\\alpha_j(t)$ are determined by solving a system of linear equations. These equations are generated by replacing the matrix $A$ with its eigenvalues $\\lambda$ in the above expression, i.e., $e^{t\\lambda} = \\sum_{j=0}^{n-1} \\alpha_j(t) \\lambda^j$. If an eigenvalue $\\lambda_k$ has algebraic multiplicity $m_k  1$, then additional equations are obtained by taking derivatives with respect to $\\lambda$ up to order $m_k-1$.\n\nConsider a system governed by the matrix:\n$$A = \\begin{pmatrix} 2  1  c \\\\ 0  2  d \\\\ 0  0  1 \\end{pmatrix}$$\nwhere $c$ and $d$ are arbitrary real parameters.\n\nUsing the Cayley-Hamilton theorem approach, derive the state-transition matrix $e^{tA}$.", "solution": "The problem is to compute the matrix exponential $e^{tA}$ for the matrix $A = \\begin{pmatrix} 2  1  c \\\\ 0  2  d \\\\ 0  0  1 \\end{pmatrix}$.\n\n**Step 1: Find the eigenvalues of A**\n\nThe characteristic polynomial is $p(\\lambda) = \\det(A - \\lambda I)$.\n$$p(\\lambda) = \\det \\begin{pmatrix} 2-\\lambda  1  c \\\\ 0  2-\\lambda  d \\\\ 0  0  1-\\lambda \\end{pmatrix} = (2-\\lambda)^2 (1-\\lambda)$$\nThe eigenvalues are the roots of $p(\\lambda)=0$. Thus, we have an eigenvalue $\\lambda_1 = 2$ with algebraic multiplicity $m_1 = 2$, and a simple eigenvalue $\\lambda_2 = 1$ with algebraic multiplicity $m_2 = 1$.\n\n**Step 2: Set up the system for the coefficients $\\alpha_j(t)$**\n\nSince $A$ is a $3 \\times 3$ matrix, $e^{tA}$ can be written as a quadratic polynomial in $A$:\n$$e^{tA} = \\alpha_0(t)I + \\alpha_1(t)A + \\alpha_2(t)A^2$$\nThe corresponding scalar equation is:\n$$e^{t\\lambda} = \\alpha_0(t) + \\alpha_1(t)\\lambda + \\alpha_2(t)\\lambda^2$$\nWe use the eigenvalues to generate a system of equations for $\\alpha_0, \\alpha_1, \\alpha_2$.\n\nFor the simple eigenvalue $\\lambda_2 = 1$:\n$$e^{t\\cdot 1} = \\alpha_0 + \\alpha_1(1) + \\alpha_2(1)^2 \\implies e^t = \\alpha_0 + \\alpha_1 + \\alpha_2 \\quad (1)$$\n\nFor the repeated eigenvalue $\\lambda_1 = 2$ (multiplicity 2), we get two equations. First, using the eigenvalue itself:\n$$e^{t\\cdot 2} = \\alpha_0 + \\alpha_1(2) + \\alpha_2(2)^2 \\implies e^{2t} = \\alpha_0 + 2\\alpha_1 + 4\\alpha_2 \\quad (2)$$\nSecond, we take the derivative of the scalar equation with respect to $\\lambda$ and then evaluate at $\\lambda=2$:\n$$\\frac{d}{d\\lambda}(e^{t\\lambda}) = \\frac{d}{d\\lambda}(\\alpha_0 + \\alpha_1\\lambda + \\alpha_2\\lambda^2)$$\n$$t e^{t\\lambda} = \\alpha_1 + 2\\alpha_2\\lambda$$\nEvaluating at $\\lambda=2$:\n$$t e^{2t} = \\alpha_1 + 2\\alpha_2(2) \\implies t e^{2t} = \\alpha_1 + 4\\alpha_2 \\quad (3)$$\n\n**Step 3: Solve for $\\alpha_0(t)$, $\\alpha_1(t)$, and $\\alpha_2(t)$**\n\nWe have a system of three linear equations:\n1. $\\alpha_0 + \\alpha_1 + \\alpha_2 = e^t$\n2. $\\alpha_0 + 2\\alpha_1 + 4\\alpha_2 = e^{2t}$\n3. $\\alpha_1 + 4\\alpha_2 = t e^{2t}$\n\nFrom (3), we can express $\\alpha_1$ in terms of $\\alpha_2$:\n$$\\alpha_1 = t e^{2t} - 4\\alpha_2$$\nSubstitute this into (1) and (2).\nSubstituting into (2):\n$$\\alpha_0 + 2(t e^{2t} - 4\\alpha_2) + 4\\alpha_2 = e^{2t}$$\n$$\\alpha_0 + 2t e^{2t} - 8\\alpha_2 + 4\\alpha_2 = e^{2t} \\implies \\alpha_0 - 4\\alpha_2 = e^{2t} - 2t e^{2t} \\quad (4)$$\nSubstituting into (1):\n$$\\alpha_0 + (t e^{2t} - 4\\alpha_2) + \\alpha_2 = e^t$$\n$$\\alpha_0 - 3\\alpha_2 = e^t - t e^{2t} \\quad (5)$$\n\nNow we solve the system of two equations (4) and (5) for $\\alpha_0$ and $\\alpha_2$. Subtract (4) from (5):\n$$(\\alpha_0 - 3\\alpha_2) - (\\alpha_0 - 4\\alpha_2) = (e^t - t e^{2t}) - (e^{2t} - 2t e^{2t})$$\n$$\\alpha_2 = e^t - e^{2t} + t e^{2t} = e^t + (t-1)e^{2t}$$\nNow find $\\alpha_1$:\n$$\\alpha_1 = t e^{2t} - 4\\alpha_2 = t e^{2t} - 4(e^t + (t-1)e^{2t}) = t e^{2t} - 4e^t - 4t e^{2t} + 4e^{2t}$$\n$$\\alpha_1 = -4e^t + (4-3t)e^{2t}$$\nFinally, find $\\alpha_0$ using (1):\n$$\\alpha_0 = e^t - \\alpha_1 - \\alpha_2 = e^t - (-4e^t + (4-3t)e^{2t}) - (e^t + (t-1)e^{2t})$$\n$$\\alpha_0 = e^t + 4e^t - (4-3t)e^{2t} - e^t - (t-1)e^{2t} = 4e^t - (4-3t+t-1)e^{2t}$$\n$$\\alpha_0 = 4e^t - (3-2t)e^{2t}$$\n\n**Step 4: Compute A²**\n$$A^2 = A \\cdot A = \\begin{pmatrix} 2  1  c \\\\ 0  2  d \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 2  1  c \\\\ 0  2  d \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 4  2+2  2c+d+c \\\\ 0  4  2d+d \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 4  4  3c+d \\\\ 0  4  3d \\\\ 0  0  1 \\end{pmatrix}$$\n\n**Step 5: Assemble $e^{tA}$**\n$$e^{tA} = \\alpha_0 I + \\alpha_1 A + \\alpha_2 A^2$$\n$$e^{tA} = \\alpha_0 \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} + \\alpha_1 \\begin{pmatrix} 2  1  c \\\\ 0  2  d \\\\ 0  0  1 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 4  4  3c+d \\\\ 0  4  3d \\\\ 0  0  1 \\end{pmatrix}$$\n$$e^{tA} = \\begin{pmatrix} \\alpha_0 + 2\\alpha_1 + 4\\alpha_2  \\alpha_1 + 4\\alpha_2  c\\alpha_1 + (3c+d)\\alpha_2 \\\\ 0  \\alpha_0 + 2\\alpha_1 + 4\\alpha_2  d\\alpha_1 + 3d\\alpha_2 \\\\ 0  0  \\alpha_0 + \\alpha_1 + \\alpha_2 \\end{pmatrix}$$\n\nLet's compute the matrix elements using the combinations of $\\alpha_j$ we already used to define our system:\n- $(1,1)$ and $(2,2)$ elements: $\\alpha_0 + 2\\alpha_1 + 4\\alpha_2 = e^{2t}$ from (2).\n- $(3,3)$ element: $\\alpha_0 + \\alpha_1 + \\alpha_2 = e^t$ from (1).\n- $(1,2)$ element: $\\alpha_1 + 4\\alpha_2 = te^{2t}$ from (3).\n\nNow compute the remaining off-diagonal elements:\n- $(2,3)$ element: $d\\alpha_1 + 3d\\alpha_2 = d(\\alpha_1 + 3\\alpha_2)$.\n  $\\alpha_1 + 3\\alpha_2 = (-4e^t + (4-3t)e^{2t}) + 3(e^t + (t-1)e^{2t}) = -e^t + (4-3t+3t-3)e^{2t} = e^{2t}-e^t$.\n  So the $(2,3)$ element is $d(e^{2t}-e^t)$.\n- $(1,3)$ element: $c\\alpha_1 + (3c+d)\\alpha_2 = c(\\alpha_1 + 3\\alpha_2) + d\\alpha_2$.\n  Using the expressions for $\\alpha_1+3\\alpha_2$ and $\\alpha_2$:\n  $c(e^{2t}-e^t) + d(e^t + (t-1)e^{2t}) = c e^{2t} - c e^t + d e^t + dt e^{2t} - d e^{2t} = (c-d+dt)e^{2t} + (d-c)e^t$.\n\nFinally, assembling the matrix $e^{tA}$:\n$$e^{tA} = \\begin{pmatrix} e^{2t}  t e^{2t}  (c-d+dt)e^{2t} + (d-c)e^t \\\\ 0  e^{2t}  d(e^{2t} - e^t) \\\\ 0  0  e^t \\end{pmatrix}$$", "answer": "$$\n\\boxed{\\begin{pmatrix} e^{2t}  t e^{2t}  (c-d+dt)e^{2t} + (d-c)e^t \\\\ 0  e^{2t}  d(e^{2t} - e^t) \\\\ 0  0  e^t \\end{pmatrix}}\n$$", "id": "1156702"}, {"introduction": "The eigenvalues of a system matrix $A$ determine its long-term stability, but what about its short-term behavior? This practice [@problem_id:2753706] delves into the crucial phenomenon of transient growth in non-normal systems, where the norm of the state-transition matrix $\\lVert e^{tA} \\rVert$ can increase significantly before decaying. By working through a concrete near-defective example, you will see how a system can be asymptotically stable yet exhibit large, and potentially undesirable, transient responses.", "problem": "Consider the family of real upper-triangular matrices parameterized by $0\\varepsilon\\leq 1$,\n$$\nA_{\\varepsilon} \\;=\\; \\begin{pmatrix}\n-1  \\dfrac{1}{\\varepsilon} \\\\\n0  -(1+\\varepsilon)\n\\end{pmatrix}.\n$$\nThis family is asymptotically stable for every fixed $\\varepsilon0$ and becomes nearly defective as $\\varepsilon\\to 0^{+}$ because $A_{\\varepsilon}\\to \\begin{pmatrix}-1  \\infty\\\\ 0-1\\end{pmatrix}$ in the sense that its eigenvalues coalesce at $-1$ while its eigenvectors become ill-conditioned. In state-space terms, the state-transition matrix is the matrix exponential $\\exp(t A_{\\varepsilon})$, which solves the homogeneous linear time-invariant system $\\dot{x}(t)=A_{\\varepsilon} x(t)$ with $x(0)=I$.\n\nStarting only from the defining properties of the matrix exponential as the unique solution $\\Phi(t)=\\exp(tA_{\\varepsilon})$ of $\\dot{\\Phi}(t)=A_{\\varepsilon}\\Phi(t)$ with $\\Phi(0)=I$, carry out the following:\n\n1. Derive an explicit expression for $\\exp(t A_{\\varepsilon})$.\n2. Using the induced Euclidean operator norm (that is, the spectral norm), define\n$$\nM(\\varepsilon) \\;=\\; \\sup_{t\\geq 0} \\left\\| \\exp\\!\\big(t A_{\\varepsilon}\\big) \\right\\|_{2}.\n$$\n3. Prove that $M(\\varepsilon)$ exhibits large transient growth as $\\varepsilon\\to 0^{+}$ despite asymptotic stability, and compute the exact constant governing the leading-order growth by evaluating the limit\n$$\nL \\;=\\; \\lim_{\\varepsilon\\to 0^{+}} \\; \\varepsilon \\, M(\\varepsilon).\n$$\n\nProvide your final answer for $L$ as an exact symbolic expression (no numerical rounding). No units are required.", "solution": "The user has provided a problem statement that must first be validated.\n\n### Step 1: Extract Givens\n-   The matrix family is given by $A_{\\varepsilon} = \\begin{pmatrix} -1  \\frac{1}{\\varepsilon} \\\\ 0  -(1+\\varepsilon) \\end{pmatrix}$ for $0  \\varepsilon \\leq 1$.\n-   The associated linear time-invariant system is $\\dot{x}(t) = A_{\\varepsilon} x(t)$.\n-   The state-transition matrix is $\\Phi(t) = \\exp(t A_{\\varepsilon})$, which is defined as the unique solution to $\\dot{\\Phi}(t) = A_{\\varepsilon} \\Phi(t)$ with the initial condition $\\Phi(0) = I$, where $I$ is the identity matrix.\n-   A quantity $M(\\varepsilon)$ is defined as the supremum over non-negative time of the spectral norm of the state-transition matrix: $M(\\varepsilon) = \\sup_{t \\geq 0} \\| \\exp(t A_{\\varepsilon}) \\|_{2}$.\n-   The task is to compute the limit $L = \\lim_{\\varepsilon\\to 0^{+}} \\varepsilon M(\\varepsilon)$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is firmly rooted in the field of control theory and linear algebra, specifically concerning the stability and transient behavior of linear systems. The matrix $A_{\\varepsilon}$ is a canonical example of a non-normal matrix whose norm of the exponential exhibits transient growth, a well-studied phenomenon related to pseudospectra. The concepts of matrix exponential, spectral norm, and stability are all standard and rigorously defined.\n-   **Well-Posed:** The problem is well-posed. Each task is a clear mathematical directive. The existence and uniqueness of the matrix exponential are guaranteed. The definitions of the norm, supremum, and limit are unambiguous. A unique, meaningful solution is expected to exist.\n-   **Objective:** The problem statement is formulated with precise, objective mathematical language, free from any subjective or speculative content.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, objective, and complete. It is a valid and non-trivial problem in applied mathematics. I will now proceed with the solution.\n\nThe solution will be constructed in three main parts as requested by the problem.\n\n**Part 1: Derivation of the matrix exponential $\\exp(t A_{\\varepsilon})$**\n\nWe must find the matrix $\\Phi(t) = \\exp(t A_{\\varepsilon})$ by solving the matrix differential equation $\\dot{\\Phi}(t) = A_{\\varepsilon} \\Phi(t)$ with $\\Phi(0) = I$. Let $\\Phi(t) = \\begin{pmatrix} \\phi_{11}(t)  \\phi_{12}(t) \\\\ \\phi_{21}(t)  \\phi_{22}(t) \\end{pmatrix}$. The equation becomes:\n$$\n\\begin{pmatrix} \\dot{\\phi}_{11}(t)  \\dot{\\phi}_{12}(t) \\\\ \\dot{\\phi}_{21}(t)  \\dot{\\phi}_{22}(t) \\end{pmatrix} = \\begin{pmatrix} -1  \\frac{1}{\\varepsilon} \\\\ 0  -(1+\\varepsilon) \\end{pmatrix} \\begin{pmatrix} \\phi_{11}(t)  \\phi_{12}(t) \\\\ \\phi_{21}(t)  \\phi_{22}(t) \\end{pmatrix}\n$$\nThe initial conditions from $\\Phi(0) = I$ are $\\phi_{11}(0) = 1$, $\\phi_{12}(0) = 0$, $\\phi_{21}(0) = 0$, and $\\phi_{22}(0) = 1$.\n\nFor the second row, the equations are decoupled:\n-   $\\dot{\\phi}_{21}(t) = -(1+\\varepsilon) \\phi_{21}(t)$, with $\\phi_{21}(0) = 0$. The unique solution is $\\phi_{21}(t) = 0$.\n-   $\\dot{\\phi}_{22}(t) = -(1+\\varepsilon) \\phi_{22}(t)$, with $\\phi_{22}(0) = 1$. The solution is $\\phi_{22}(t) = \\exp(-(1+\\varepsilon)t)$.\n\nFor the first row, we have:\n-   $\\dot{\\phi}_{11}(t) = -\\phi_{11}(t) + \\frac{1}{\\varepsilon}\\phi_{21}(t)$. Since $\\phi_{21}(t) = 0$, this simplifies to $\\dot{\\phi}_{11}(t) = -\\phi_{11}(t)$. With $\\phi_{11}(0) = 1$, the solution is $\\phi_{11}(t) = \\exp(-t)$.\n-   $\\dot{\\phi}_{12}(t) = -\\phi_{12}(t) + \\frac{1}{\\varepsilon}\\phi_{22}(t)$. Substituting $\\phi_{22}(t)$, we get the first-order linear ordinary differential equation:\n    $$\n    \\dot{\\phi}_{12}(t) + \\phi_{12}(t) = \\frac{1}{\\varepsilon} \\exp(-(1+\\varepsilon)t)\n    $$\n    We solve this using an integrating factor, which is $\\exp(\\int 1 \\, dt) = \\exp(t)$.\n    $$\n    \\frac{d}{dt} \\left(\\exp(t) \\phi_{12}(t)\\right) = \\exp(t) \\left( \\frac{1}{\\varepsilon} \\exp(-(1+\\varepsilon)t) \\right) = \\frac{1}{\\varepsilon} \\exp(-\\varepsilon t)\n    $$\n    Integrating with respect to $t$:\n    $$\n    \\exp(t) \\phi_{12}(t) = \\int \\frac{1}{\\varepsilon} \\exp(-\\varepsilon t) \\, dt = \\frac{1}{\\varepsilon} \\left(-\\frac{1}{\\varepsilon} \\exp(-\\varepsilon t)\\right) + C = -\\frac{1}{\\varepsilon^2} \\exp(-\\varepsilon t) + C\n    $$\n    Using the initial condition $\\phi_{12}(0) = 0$:\n    $$\n    \\exp(0) \\phi_{12}(0) = 0 = -\\frac{1}{\\varepsilon^2}\\exp(0) + C \\implies C = \\frac{1}{\\varepsilon^2}\n    $$\n    Thus, $\\exp(t) \\phi_{12}(t) = \\frac{1}{\\varepsilon^2}(1 - \\exp(-\\varepsilon t))$, which gives:\n    $$\n    \\phi_{12}(t) = \\frac{1}{\\varepsilon^2}(\\exp(-t) - \\exp(-(1+\\varepsilon)t))\n    $$\nCollecting the components, the matrix exponential is:\n$$\n\\exp(t A_{\\varepsilon}) = \\begin{pmatrix} \\exp(-t)  \\frac{1}{\\varepsilon^2} (\\exp(-t) - \\exp(-(1+\\varepsilon)t)) \\\\ 0  \\exp(-(1+\\varepsilon)t) \\end{pmatrix}\n$$\n\n**Part 2  3: Analysis of Transient Growth and Evaluation of the Limit**\n\nWe are asked to compute $L = \\lim_{\\varepsilon\\to 0^{+}} \\varepsilon M(\\varepsilon) = \\lim_{\\varepsilon\\to 0^{+}} \\varepsilon \\sup_{t\\geq 0} \\| \\exp(t A_{\\varepsilon}) \\|_{2}$.\nThis is equivalent to $L = \\lim_{\\varepsilon\\to 0^{+}} \\sup_{t\\geq 0} (\\varepsilon \\| \\exp(t A_{\\varepsilon}) \\|_{2})$. Let $h_{\\varepsilon}(t) = \\varepsilon \\| \\exp(t A_{\\varepsilon}) \\|_{2}$. We wish to compute $\\lim_{\\varepsilon\\to 0^{+}} \\sup_{t\\geq 0} h_{\\varepsilon}(t)$.\n\nIt is a known result in analysis that if a sequence of functions converges uniformly, the limit and supremum operators can be interchanged. We will show that $h_{\\varepsilon}(t)$ converges uniformly to a function $h(t)$ for $t \\in [0, \\infty)$, which justifies writing $L = \\sup_{t\\geq 0} \\lim_{\\varepsilon\\to 0^{+}} h_{\\varepsilon}(t)$.\n\nLet us first find the pointwise limit of $h_{\\varepsilon}(t)$. We need an asymptotic expression for $\\| \\exp(t A_{\\varepsilon}) \\|_{2}$ for small $\\varepsilon$. The squared spectral norm $\\| \\cdot \\|_{2}^2$ is the largest eigenvalue of $\\Phi(t)^T \\Phi(t)$.\nLet $\\Phi(t) = \\exp(t A_{\\varepsilon})$. Then\n$$\n\\Phi(t)^T \\Phi(t) = \\begin{pmatrix} \\exp(-2t)  \\exp(-t)f(t) \\\\ \\exp(-t)f(t)  f(t)^2 + \\exp(-2(1+\\varepsilon)t) \\end{pmatrix}\n$$\nwhere $f(t) = \\phi_{12}(t) = \\frac{1}{\\varepsilon^2}(\\exp(-t) - \\exp(-(1+\\varepsilon)t))$. For small $\\varepsilon$, we expand $f(t)$:\n$$\nf(t) = \\frac{\\exp(-t)}{\\varepsilon^2}(1 - \\exp(-\\varepsilon t)) = \\frac{\\exp(-t)}{\\varepsilon^2}\\left(1 - \\left(1 - \\varepsilon t + \\frac{(\\varepsilon t)^2}{2} - \\dots\\right)\\right) = \\frac{t\\exp(-t)}{\\varepsilon} - \\frac{t^2\\exp(-t)}{2} + O(\\varepsilon)\n$$\nThe dominant term is of order $1/\\varepsilon$, so $f(t)^2$ is of order $1/\\varepsilon^2$.\nThe eigenvalues $\\lambda$ of $\\Phi(t)^T \\Phi(t)$ are given by the solution to the characteristic equation $\\lambda^2 - \\text{Tr}(\\Phi(t)^T \\Phi(t))\\lambda + \\det(\\Phi(t)^T \\Phi(t)) = 0$.\nThe squared norm is the larger eigenvalue, $\\lambda_{\\max} = \\frac{1}{2}\\left(\\text{Tr} + \\sqrt{\\text{Tr}^2 - 4\\det}\\right)$. Let's analyze the terms for small $\\varepsilon$:\n$$\n\\text{Tr}(\\Phi(t)^T \\Phi(t)) = f(t)^2 + \\exp(-2t) + \\exp(-2(1+\\varepsilon)t) = \\frac{t^2\\exp(-2t)}{\\varepsilon^2} - \\frac{t^3\\exp(-2t)}{\\varepsilon} + O(1)\n$$\n$$\n\\det(\\Phi(t)^T \\Phi(t)) = (\\det \\Phi(t))^2 = (\\exp(-t)\\exp(-(1+\\varepsilon)t))^2 = \\exp(-2(2+\\varepsilon)t) = O(1)\n$$\nThe term under the square root is $\\text{Tr}^2 - 4\\det$. Asymptotically, this is dominated by $\\text{Tr}^2$:\n$$\n\\sqrt{\\text{Tr}^2 - 4\\det} = \\text{Tr} \\sqrt{1 - \\frac{4\\det}{\\text{Tr}^2}} = \\text{Tr} \\left(1 - \\frac{2\\det}{\\text{Tr}^2} + \\dots\\right) \\approx \\text{Tr}\n$$\nMore precisely, a careful expansion similar to that for $\\text{Tr}$ yields:\n$$\n\\sqrt{\\text{Tr}^2 - 4\\det} = \\frac{t^2\\exp(-2t)}{\\varepsilon^2} - \\frac{t^3\\exp(-2t)}{\\varepsilon} + O(1)\n$$\nTherefore,\n$$\n\\|\\exp(t A_{\\varepsilon})\\|_{2}^2 = \\lambda_{\\max} \\approx \\frac{1}{2}(\\text{Tr} + \\text{Tr}) = \\text{Tr} = \\frac{t^2\\exp(-2t)}{\\varepsilon^2} - \\frac{t^3\\exp(-2t)}{\\varepsilon} + O(1)\n$$\nNow we can analyze $h_{\\varepsilon}(t)^2$:\n$$\nh_{\\varepsilon}(t)^2 = \\varepsilon^2 \\|\\exp(t A_{\\varepsilon})\\|_{2}^2 = t^2\\exp(-2t) - \\varepsilon t^3\\exp(-2t) + O(\\varepsilon^2)\n$$\nTaking the square root:\n$$\nh_{\\varepsilon}(t) = \\sqrt{t^2\\exp(-2t) - \\varepsilon t^3\\exp(-2t) + O(\\varepsilon^2)} = t\\exp(-t)\\sqrt{1 - \\varepsilon t + O(\\varepsilon^2)}\n$$\nUsing the Taylor expansion $\\sqrt{1-x} = 1 - \\frac{x}{2} + O(x^2)$:\n$$\nh_{\\varepsilon}(t) = t\\exp(-t)\\left(1 - \\frac{1}{2}\\varepsilon t + O(\\varepsilon^2)\\right) = t\\exp(-t) - \\frac{1}{2}\\varepsilon t^2\\exp(-t) + O(\\varepsilon^2)\n$$\nThe pointwise limit is clear: $h(t) = \\lim_{\\varepsilon\\to 0^{+}} h_{\\varepsilon}(t) = t\\exp(-t)$.\nTo justify interchanging the limit and supremum, we examine the convergence. The error is\n$$\n|h_{\\varepsilon}(t) - h(t)| = \\left| -\\frac{1}{2}\\varepsilon t^2\\exp(-t) + O(\\varepsilon^2) \\right|\n$$\nFor a given small $\\varepsilon  0$, we must find the supremum of this error over $t \\geq 0$. The function $g(t) = t^2\\exp(-t)$ is bounded for $t \\ge 0$. Its derivative $g'(t) = (2t-t^2)\\exp(-t)$ is zero at $t=0$ and $t=2$. The maximum value is $g(2) = 4\\exp(-2)$. Therefore,\n$$\n\\sup_{t\\ge 0} |h_{\\varepsilon}(t) - h(t)| \\le \\frac{1}{2}\\varepsilon (4\\exp(-2)) + O(\\varepsilon^2)\n$$\nAs $\\varepsilon \\to 0^{+}$, this supremum tends to $0$, which proves that the convergence of $h_{\\varepsilon}(t)$ to $h(t)$ is uniform on $[0, \\infty)$. The interchange of limit and supremum is thus justified.\n$$\nL = \\lim_{\\varepsilon\\to 0^{+}} \\sup_{t\\geq 0} h_{\\varepsilon}(t) = \\sup_{t\\geq 0} \\lim_{\\varepsilon\\to 0^{+}} h_{\\varepsilon}(t) = \\sup_{t\\geq 0} (t\\exp(-t))\n$$\nTo find this supremum, we differentiate $h(t) = t\\exp(-t)$ with respect to $t$ and set the derivative to zero:\n$$\n\\frac{dh}{dt} = \\exp(-t) - t\\exp(-t) = (1-t)\\exp(-t)\n$$\nThe derivative is zero for $t=1$. At this point, the function attains its maximum value:\n$$\nh(1) = 1 \\cdot \\exp(-1) = \\frac{1}{e}\n$$\nThus, the value of the limit is $L = \\exp(-1)$.", "answer": "$$\\boxed{\\exp(-1)}$$", "id": "2753706"}, {"introduction": "Analytical methods become impractical for large-scale systems, where numerical algorithms are indispensable. This exercise [@problem_id:2753724] moves into the practical realm of numerical computation, focusing on the widely used scaling-and-squaring method with Padé approximants. You will analyze the subtle but critical risks of rounding errors and catastrophic cancellation, learning to think like a numerical analyst to ensure the accuracy and reliability of your computations.", "problem": "Consider computing the matrix exponential $\\exp(A)$ for a square matrix $A \\in \\mathbb{C}^{n \\times n}$ using the scaling-and-squaring framework together with a Padé approximant. The matrix exponential is defined by the absolutely convergent power series $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}$. The scaling-and-squaring framework exploits the identity $\\exp(A) = \\left(\\exp\\left(A / 2^{s}\\right)\\right)^{2^{s}}$ for an integer $s \\ge 0$. A rational Padé approximant of type $[m/m]$ to $\\exp(\\cdot)$ is used to approximate $\\exp\\left(A / 2^{s}\\right)$.\n\nThere are multiple algebraically equivalent ways to evaluate the $[m/m]$ Padé approximant. Two commonly discussed forms are: (i) evaluating a numerator polynomial $p_{m}(\\cdot)$ and a denominator polynomial $q_{m}(\\cdot)$ and forming $q_{m}(X)^{-1} p_{m}(X)$, and (ii) the even–odd split that writes the approximant in the form $\\left(V_{m}(X)+U_{m}(X)\\right)\\left(V_{m}(X)-U_{m}(X)\\right)^{-1}$, where $V_{m}$ gathers even powers and $U_{m}$ gathers odd powers (equivalently, $U_{m}(X) = X \\,\\widehat{U}_{m}(X^{2})$ and $V_{m}(X) = \\widehat{V}_{m}(X^{2})$ for some polynomials $\\widehat{U}_{m}$ and $\\widehat{V}_{m}$ with positive coefficients). Assume floating-point arithmetic satisfying the standard model $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$ with $|\\delta| \\le u$ for $\\circ \\in \\{+, -, \\times\\}$, where $u$ is the unit roundoff (for example, $u \\approx 2^{-53}$ in binary64).\n\nSuppose $\\lVert A \\rVert$ (in some consistent submultiplicative matrix norm) is small, say $\\lVert A \\rVert \\le 10^{-8}$, and you wish to analyze the risk of catastrophic cancellation when evaluating the Padé polynomials and to propose parameter choices to avoid it. In particular, consider how the choice of the scaling parameter $s$, the approximant degree $m$, and the evaluation form (numerator/denominator versus even–odd split) impact cancellation and rounding error amplification.\n\nWhich of the following strategies are well-justified to minimize cancellation and maintain high accuracy for such small $\\lVert A \\rVert$?\n\nA. Set $s = 0$ (no scaling) and evaluate the $[m/m]$ Padé approximant in the even–odd split form $\\left(V_{m}(A)+U_{m}(A)\\right)\\left(V_{m}(A)-U_{m}(A)\\right)^{-1}$, computing $U_{m}$ and $V_{m}$ by Horner’s rule in $A^{2}$ (or a Paterson–Stockmeyer scheme in $A^{2}$). This avoids large subtractive cancellation because $V_{m}(A) \\approx I$ and $U_{m}(A)$ is small when $\\lVert A \\rVert$ is small.\n\nB. Choose $s$ so large that $\\lVert A/2^{s} \\rVert \\lesssim u$. Then $U_{m}(A/2^{s})$ is negligible, so $\\left(V_{m} \\pm U_{m}\\right)$ can be formed without cancellation, and repeated squaring exactly recovers the missing terms.\n\nC. For sufficiently small $\\lVert A \\rVert$, switch to a truncated Taylor polynomial $\\sum_{k=0}^{K} \\frac{A^{k}}{k!}$ with $K$ chosen by the inequality $\\lVert A \\rVert^{K+1}/(K+1)! \\le \\text{tol}$ (for a prescribed tolerance). This avoids forming near-cancelling numerator/denominator polynomials and exploits that all Taylor coefficients are positive.\n\nD. Evaluate the Padé approximant by separately forming $p_{m}(A)$ and $q_{m}(A)$ using Paterson–Stockmeyer and then computing $q_{m}(A)^{-1} p_{m}(A)$. Because $\\lVert A \\rVert$ is small, the alternating signs in $q_{m}$ do not cause cancellation in floating point, so this is preferable to the even–odd split.\n\nE. To reduce cancellation in $\\left(V_{m} \\pm U_{m}\\right)$, choose $m$ as large as possible (for example, $m = 13$) regardless of $\\lVert A \\rVert$, since higher degree always reduces cancellation in practice for small $\\lVert A \\rVert$.\n\nSelect all that apply.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- The object of computation is the matrix exponential $\\exp(A)$ for a square matrix $A \\in \\mathbb{C}^{n \\times n}$.\n- The definition of the matrix exponential is the power series $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}$.\n- The computational framework is scaling-and-squaring, based on the identity $\\exp(A) = \\left(\\exp\\left(A / 2^{s}\\right)\\right)^{2^{s}}$ for an integer $s \\ge 0$.\n- The approximation method for $\\exp\\left(A / 2^{s}\\right)$ is a $[m/m]$ Padé approximant, denoted $R_{m,m}(\\cdot)$.\n- Two evaluation forms for the Padé approximant are given:\n    1. Numerator/denominator form: $q_{m}(X)^{-1} p_{m}(X)$.\n    2. Even–odd split form: $\\left(V_{m}(X)+U_{m}(X)\\right)\\left(V_{m}(X)-U_{m}(X)\\right)^{-1}$, where $V_{m}$ contains even powers and $U_{m}$ contains odd powers. The polynomials $\\widehat{U}_{m}$ and $\\widehat{V}_{m}$ defined by $U_{m}(X) = X \\,\\widehat{U}_{m}(X^{2})$ and $V_{m}(X) = \\widehat{V}_{m}(X^{2})$ have positive coefficients.\n- The floating-point arithmetic model is $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$ with $|\\delta| \\le u$ for $\\circ \\in \\{+, -, \\times\\}$.\n- A specific condition is given: the matrix norm $\\lVert A \\rVert$ is small, specifically $\\lVert A \\rVert \\le 10^{-8}$.\n- The goal is to identify well-justified strategies to minimize cancellation and maintain high accuracy under this condition.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is firmly based on standard, well-established concepts in numerical linear algebra and the theory of matrix functions. The definitions of the matrix exponential, scaling-and-squaring, and Padé approximants are all correct. The numerical stability issues described, such as cancellation, are central topics in this field.\n- **Well-Posed**: The problem is well-posed. It asks for an evaluation of specific numerical strategies under a clearly defined condition ($\\lVert A \\rVert \\le 10^{-8}$). A unique set of correct strategies can be identified based on principles of numerical analysis.\n- **Objective**: The problem is stated in precise, objective, technical language. There are no subjective or ambiguous terms.\n- **Other Flaws**: The problem statement is self-contained, consistent, and does not contain any of the invalidating flaws listed in the instructions. It represents a realistic and non-trivial question in the design of robust software for computing the matrix exponential.\n\nStep 3: Verdict and Action\nThe problem statement is valid. A full solution and evaluation of options will be provided.\n\nThe core of the problem lies in analyzing rounding errors, particularly catastrophic cancellation, when computing $\\exp(A)$ for a matrix $A$ with a small norm, $\\lVert A \\rVert \\le 10^{-8}$. Catastrophic cancellation occurs when two nearly equal numbers are subtracted, leading to a potential loss of relative accuracy.\n\nThe identity relating the Padé forms is $p_m(X) = V_m(X) + U_m(X)$ and $q_m(X) = p_m(-X) = V_m(-X) + U_m(-X) = V_m(X) - U_m(X)$, since $V_m$ is an even function and $U_m$ is an odd function. For small $\\lVert A \\rVert$, we have the Taylor series $\\exp(A) = I + A + \\frac{A^2}{2} + O(\\lVert A \\rVert^3)$. Any good approximant must be close to this. For the Padé approximant $R_{m,m}(A)$, the leading terms are $V_m(A) \\approx I$ and $U_m(A) \\approx \\frac{1}{2}A$. Therefore, $\\lVert V_m(A) \\rVert \\approx 1$ and $\\lVert U_m(A) \\rVert \\approx \\frac{1}{2}\\lVert A \\rVert \\le 5 \\times 10^{-9}$.\n\nA. **Set $s = 0$ (no scaling) and evaluate the $[m/m]$ Padé approximant in the even–odd split form $\\left(V_{m}(A)+U_{m}(A)\\right)\\left(V_{m}(A)-U_{m}(A)\\right)^{-1}$, computing $U_{m}$ and $V_{m}$ by Horner’s rule in $A^{2}$ (or a Paterson–Stockmeyer scheme in $A^{2}$). This avoids large subtractive cancellation because $V_{m}(A) \\approx I$ and $U_{m}(A)$ is small when $\\lVert A \\rVert$ is small.**\n\nSince $\\lVert A \\rVert \\le 10^{-8}$ is already small, further scaling (i.e., choosing $s  0$) is generally unnecessary and can introduce its own rounding errors during the squaring phase. Thus, setting $s=0$ is a reasonable starting point. The even–odd split is a computationally efficient method to evaluate the Padé approximant, as it involves polynomials in $A^2$. The polynomials $\\widehat{V}_m(Y)$ and $\\widehat{U}_m(Y)$ (where $Y=A^2$) have positive coefficients. Evaluation of such polynomials is numerically very stable, as it involves primarily additions of terms with decreasing magnitudes.\nThe critical step concerning cancellation is the subtraction $V_m(A) - U_m(A)$. As established, for small $\\lVert A \\rVert$, $V_m(A) \\approx I$ and $U_m(A) \\approx \\frac{1}{2}A$. Since $\\lVert U_m(A) \\rVert \\ll \\lVert V_m(A) \\rVert$, these two matrices are not nearly equal. Subtracting a small matrix from one close to the identity does not cause catastrophic cancellation. The reasoning provided in this option is sound. This strategy is numerically stable and efficient for small $\\lVert A \\rVert$.\n\nVerdict: **Correct**.\n\nB. **Choose $s$ so large that $\\lVert A/2^{s} \\rVert \\lesssim u$. Then $U_{m}(A/2^{s})$ is negligible, so $\\left(V_{m} \\pm U_{m}\\right)$ can be formed without cancellation, and repeated squaring exactly recovers the missing terms.**\n\nThis strategy is known as \"overscaling\". Let $X = A/2^s$. If $\\lVert X \\rVert \\lesssim u$, then $X$ is on the order of the unit roundoff. The Padé approximant will be very accurate: $R_{m,m}(X) \\approx \\exp(X) \\approx I + X$. Due to floating-point arithmetic, higher-order terms like $X^2$ will be lost when added to $I$. For example, $\\operatorname{fl}((I+X)^2) = \\operatorname{fl}(I+2X+X^2) \\approx I+2X$ because $\\lVert X^2 \\rVert \\lesssim u^2 \\ll u$. After $s$ squarings, the computed result will be approximately $I + 2^s X = I + A$. The true value is $\\exp(A) = I + A + A^2/2! + A^3/3! + \\dots$. The error of this approximation is $A^2/2! + \\dots$, which was lost.\nThe claim that \"repeated squaring exactly recovers the missing terms\" is fundamentally false. On the contrary, repeated squaring in this regime *loses* higher-order information. This is a well-known pitfall in implementing the scaling-and-squaring method. While there is no cancellation in forming $V_m \\pm U_m$ for such a small argument, the overall method introduces a significant truncation error due to the squarings.\n\nVerdict: **Incorrect**.\n\nC. **For sufficiently small $\\lVert A \\rVert$, switch to a truncated Taylor polynomial $\\sum_{k=0}^{K} \\frac{A^{k}}{k!}$ with $K$ chosen by the inequality $\\lVert A \\rVert^{K+1}/(K+1)! \\le \\text{tol}$ (for a prescribed tolerance). This avoids forming near-cancelling numerator/denominator polynomials and exploits that all Taylor coefficients are positive.**\n\nFor small matrix norms, using a truncated Taylor series is a standard and highly effective strategy. State-of-the-art algorithms for `expm` switch from Padé approximants to Taylor series below a certain norm threshold. The primary reasons are:\n1.  **Numerical Stability**: The coefficients $1/k!$ of the Taylor series are all positive. The evaluation of $T_K(A) = \\sum_{k=0}^K \\frac{A^k}{k!}$ involves only additions of matrix terms (assuming non-negative entries for simplicity, but the principle holds more generally), which is numerically favorable and avoids the subtractions present in evaluating $q_m(A)$ or $V_m(A) - U_m(A)$, however benign those subtractions may be for small norm.\n2.  **Computational Cost**: For a given accuracy requirement, evaluating a Taylor polynomial can be cheaper than evaluating a Padé approximant, which requires solving a system of linear equations ($q_m(A) Y = p_m(A)$).\nThe condition $\\lVert A \\rVert \\le 10^{-8}$ is well within the region where this switch is beneficial. The reasoning provided is entirely correct. This strategy is a cornerstone of modern `expm` implementations.\n\nVerdict: **Correct**.\n\nD. **Evaluate the Padé approximant by separately forming $p_{m}(A)$ and $q_{m}(A)$ using Paterson–Stockmeyer and then computing $q_{m}(A)^{-1} p_{m}(A)$. Because $\\lVert A \\rVert$ is small, the alternating signs in $q_{m}$ do not cause cancellation in floating point, so this is preferable to the even–odd split.**\n\nThe polynomial $q_m(A) = \\sum_{j=0}^m c_j (-1)^j A^j$ has terms with alternating signs. For small $\\lVert A \\rVert$, we have $q_m(A) \\approx I - \\frac{1}{2}A$. As noted before, computing this does not involve subtracting nearly equal quantities, so catastrophic cancellation is not an issue. The first part of the reasoning is correct.\nHowever, the conclusion that this method is \"preferable to the even–odd split\" is incorrect.\nFrom a computational cost perspective, evaluating $p_m(A)$ and $q_m(A)$ is best done by first computing $U_m(A)$ and $V_m(A)$ (which are polynomials in $A^2$) and then forming the sum and difference. This is precisely the even-odd split approach. Evaluating $p_m(A)$ and $q_m(A)$ from scratch without leveraging the even-odd structure would be less efficient.\nFrom a numerical stability perspective, the even-odd split evaluates polynomials ($\\widehat{V}_m$ and $\\widehat{U}_m$) with positive coefficients, which is maximally stable, and then performs a single sum and a single subtraction. The direct evaluation of $q_m(A)$ involves a sum with alternating signs throughout, which is generally considered slightly less stable, even if it does not cause catastrophic cancellation in this particular case. Thus, the even-odd split is generally considered superior in both efficiency and stability.\n\nVerdict: **Incorrect**.\n\nE. **To reduce cancellation in $\\left(V_{m} \\pm U_{m}\\right)$, choose $m$ as large as possible (for example, $m = 13$) regardless of $\\lVert A \\rVert$, since higher degree always reduces cancellation in practice for small $\\lVert A \\rVert$.**\n\nThis strategy is ill-advised. The choice of the degree $m$ and the scaling parameter $s$ are coupled. For a given desired accuracy, they should be chosen to minimize computational cost. For a very small norm like $\\lVert A \\rVert \\le 10^{-8}$, a very small degree $m$ is sufficient to achieve full machine precision. For instance, for $m=1$, the approximation error is of order $\\lVert A^3 \\rVert \\approx 10^{-24}$, which is already negligible. Using a large degree like $m=13$ is computationally wasteful, requiring many more matrix multiplications.\nFurthermore, the premise that \"higher degree always reduces cancellation\" is not a sound general principle. In this specific case, cancellation is not a problem to begin with for small $\\lVert A \\rVert$. The choice of $m$ is driven by the truncation error of the approximant, not by a need to mitigate nonexistent cancellation. Using an unnecessarily high degree does not provide any practical benefit in accuracy, as the error is already dominated by other floating-point rounding errors.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "2753724"}]}