{"hands_on_practices": [{"introduction": "The Routh-Hurwitz criterion is a powerful algebraic method for assessing the internal stability of a linear time-invariant (LTI) system without computing the roots of its characteristic polynomial. This practice solidifies your understanding of the criterion by guiding you through the systematic construction of a Routh array. By analyzing the signs in the first column, you can directly determine the number of unstable internal modes, providing a definitive conclusion about the system's stability [@problem_id:2713212].", "problem": "Consider a single-input single-output linear time-invariant (LTI) feedback system whose internal (asymptotic) stability is determined entirely by the roots of its characteristic polynomial. The characteristic polynomial associated with the closed-loop dynamics is\n$$\np(s) \\;=\\; s^{4} \\;+\\; 2 s^{3} \\;+\\; 3 s^{2} \\;+\\; 4 s \\;+\\; 5.\n$$\nUsing only first principles and the Routh–Hurwitz stability criterion, construct the complete Routh array for $p(s)$, and justify each row construction from the fundamental definition that internal asymptotic stability requires all roots of $p(s)$ to lie strictly in the open left half-plane $\\{ s \\in \\mathbb{C} : \\Re(s)  0 \\}$. Explain how each computed entry contributes to determining the number of roots in the right half-plane via sign changes in the first column, and explicitly connect these sign changes to the count of left half-plane roots. Conclude by determining the number of roots of $p(s)$ that lie strictly in the open right half-plane and state this number as your final answer.\n\nYour final answer must be a single real-valued number. No rounding is required.", "solution": "The problem presented is a standard exercise in the stability analysis of linear time-invariant (LTI) systems and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution via the specified methodology. We shall proceed with the solution.\n\nThe problem requires an analysis of the internal asymptotic stability of a system characterized by the polynomial $p(s) = s^{4} + 2 s^{3} + 3 s^{2} + 4 s + 5$. According to fundamental principles of control theory, a system is asymptotically stable if and only if all roots of its characteristic polynomial lie strictly in the open left half of the complex plane, defined as $\\{ s \\in \\mathbb{C} : \\Re(s)  0 \\}$. The Routh-Hurwitz stability criterion provides a systematic procedure for determining the number of roots in the open right half-plane ($\\Re(s)  0$) without explicitly solving for the roots.\n\nThe criterion proceeds by constructing a Routh array from the coefficients of the polynomial $p(s) = a_n s^n + a_{n-1} s^{n-1} + \\dots + a_1 s + a_0$. For the given fourth-order polynomial, we have $n=4$ and the coefficients are $a_4=1$, $a_3=2$, $a_2=3$, $a_1=4$, and $a_0=5$. A necessary, but not sufficient, condition for stability is that all coefficients $a_i$ must be positive. This condition is met, so we must proceed with the full Routh array construction.\n\nThe Routh array is constructed as follows:\n\nThe first two rows are formed directly from the polynomial coefficients. The first row contains the coefficients of the even powers of $s$, starting from the highest power. The second row contains the coefficients of the odd powers of $s$.\n\nRow $s^4$: The coefficients are $a_4=1$, $a_2=3$, $a_0=5$.\nRow $s^3$: The coefficients are $a_3=2$, $a_1=4$. We append a zero for subsequent calculations.\n\nThe initial state of the array is:\n$$\n\\begin{array}{c|ccc}\ns^4  1  3  5 \\\\\ns^3  2  4  0\n\\end{array}\n$$\n\nSubsequent rows are computed using the elements of the two preceding rows. Let the elements of row $s^k$ be denoted by $r_{k,j}$. Then the elements of row $s^{k-2}$ are computed as $r_{k-2,j} = -\\frac{1}{r_{k-1,1}} \\det \\begin{pmatrix} r_{k,1}  r_{k, j+1} \\\\ r_{k-1,1}  r_{k-1, j+1} \\end{pmatrix}$.\n\nRow $s^2$: The elements, let us call them $b_1, b_2, \\dots$, are computed from the $s^4$ and $s^3$ rows.\nThe first element is $b_1 = -\\frac{\\det\\begin{pmatrix} 1  3 \\\\ 2  4 \\end{pmatrix}}{2} = -\\frac{(1)(4) - (3)(2)}{2} = -\\frac{4-6}{2} = 1$.\nThe second element is $b_2 = -\\frac{\\det\\begin{pmatrix} 1  5 \\\\ 2  0 \\end{pmatrix}}{2} = -\\frac{(1)(0) - (5)(2)}{2} = -\\frac{-10}{2} = 5$.\nThe third element is $b_3 = -\\frac{\\det\\begin{pmatrix} 1  0 \\\\ 2  0 \\end{pmatrix}}{2} = 0$.\n\nThe array now stands as:\n$$\n\\begin{array}{c|ccc}\ns^4  1  3  5 \\\\\ns^3  2  4  0 \\\\\ns^2  1  5  0\n\\end{array}\n$$\nEach of these computed entries—$b_1=1$ and $b_2=5$—forms the coefficients of an auxiliary polynomial that plays an intermediate role in the stability test. The fact that $b_1=1$ is a positive number means that, up to this stage, no instability has been conclusively detected.\n\nRow $s^1$: The elements, let us call them $c_1, c_2, \\dots$, are computed from the $s^3$ and $s^2$ rows.\nThe first element is $c_1 = -\\frac{\\det\\begin{pmatrix} 2  4 \\\\ 1  5 \\end{pmatrix}}{1} = -\\frac{(2)(5) - (4)(1)}{1} = -\\frac{10-4}{1} = -6$.\nThe second element is $c_2 = -\\frac{\\det\\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix}}{1} = -\\frac{(2)(0) - (0)(1)}{1} = 0$.\n\nThe array becomes:\n$$\n\\begin{array}{c|ccc}\ns^4  1  3  5 \\\\\ns^3  2  4  0 \\\\\ns^2  1  5  0 \\\\\ns^1  -6  0  0\n\\end{array}\n$$\nThe computation of $c_1 = -6$ is a critical step. A negative value in the first column of the Routh array signifies that the polynomial has roots in the right half-plane, and thus the system is unstable.\n\nRow $s^0$: The element, let us call it $d_1$, is computed from the $s^2$ and $s^1$ rows.\nThe first element is $d_1 = -\\frac{\\det\\begin{pmatrix} 1  5 \\\\ -6  0 \\end{pmatrix}}{-6} = -\\frac{(1)(0) - (5)(-6)}{-6} = -\\frac{30}{-6} = 5$.\n\nThe complete Routh array is:\n$$\n\\begin{array}{c|ccc}\ns^4  1  3  5 \\\\\ns^3  2  4  0 \\\\\ns^2  1  5  0 \\\\\ns^1  -6  0  0 \\\\\ns^0  5  0  0\n\\end{array}\n$$\n\nTo determine the number of roots in the open right half-plane ($\\Re(s)  0$), we inspect the signs of the elements in the first column of the array: $[1, 2, 1, -6, 5]$.\nThe sequence of signs is $(+, +, +, -, +)$.\nThere is a sign change from the $s^2$ row ($1$) to the $s^1$ row ($-6$). This is the first sign change.\nThere is a second sign change from the $s^1$ row ($-6$) to the $s^0$ row ($5$).\n\nThe total number of sign changes in the first column is $2$.\nAccording to the Routh-Hurwitz criterion, the number of sign changes in the first column is equal to the number of roots of the polynomial with positive real parts. Therefore, the characteristic polynomial $p(s)$ has exactly $2$ roots in the open right half-plane.\n\nThe degree of the polynomial is $n=4$, so it has a total of $4$ roots. Let $N_{RHP}$, $N_{LHP}$, and $N_{j\\omega}$ be the number of roots in the right half-plane, left half-plane, and on the imaginary axis, respectively. We have $N_{RHP} + N_{LHP} + N_{j\\omega} = 4$. From our analysis, $N_{RHP} = 2$. Since no row in the Routh array became entirely zero, there are no roots on the imaginary axis ($N_{j\\omega}=0$). Consequently, the number of roots in the open left half-plane is $N_{LHP} = 4 - N_{RHP} - N_{j\\omega} = 4 - 2 - 0 = 2$.\n\nThe presence of $2$ roots in the right half-plane means the system is unstable. The problem asks for the number of roots that lie strictly in the open right half-plane. This number is precisely the number of sign changes found in the first column of the Routh array.", "answer": "$$\n\\boxed{2}\n$$", "id": "2713212"}, {"introduction": "A critical concept in control theory is the distinction between internal stability and Bounded-Input Bounded-Output (BIBO) stability. This exercise explores a classic scenario where an unstable internal mode is \"hidden\" from the input-output response due to a lack of controllability. By working through this state-space example, you will discover how a system can be deceptively stable from an external perspective while harboring internal dynamics that grow without bound, reinforcing why internal stability is the more stringent and essential requirement for robust system design [@problem_id:2713315].", "problem": "Consider the continuous-time, single-input single-output linear time-invariant system described by the state-space equations\n$$\n\\dot{x}(t)=A\\,x(t)+B\\,u(t),\\quad y(t)=C\\,x(t)+D\\,u(t),\n$$\nwith\n$$\nA=\\begin{pmatrix}1  0 \\\\ 0  -2\\end{pmatrix},\\quad\nB=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix},\\quad\nC=\\begin{pmatrix}0  1\\end{pmatrix},\\quad\nD=0.\n$$\nStarting from core definitions in linear systems and control (definitions of internal asymptotic stability for autonomous dynamics, controllability for linear time-invariant pairs, and bounded-input bounded-output stability in terms of the transfer function poles), do the following:\n- Determine whether the realization is internally asymptotically stable by analyzing the autonomous dynamics and explaining your reasoning from first principles.\n- Determine whether the realization is controllable and identify any unstable mode that is uncontrollable.\n- Derive the transfer function associated with the given realization directly from the state-space representation, simplify it to lowest terms, and decide whether it is strictly proper and bounded-input bounded-output stable.\n\nYour final reported answer must be the transfer function in simplest rational form as a single analytic expression in the complex Laplace variable $s$. Do not include units. Do not provide inequalities or sets as your final answer. No rounding is required.", "solution": "The posed problem is a standard exercise in the analysis of linear time-invariant (LTI) systems. It requires the evaluation of internal stability, controllability, and input-output properties based on a given state-space realization. The problem is well-defined, scientifically grounded, and provides all necessary information for a complete analysis. It is therefore a valid problem.\n\nWe are given the continuous-time LTI system:\n$$\n\\dot{x}(t) = A x(t) + B u(t)\n$$\n$$\ny(t) = C x(t) + D u(t)\n$$\nwith the matrices:\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  -2 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 0  1 \\end{pmatrix}, \\quad D = 0\n$$\n\nThe analysis will proceed in three steps as requested: internal stability, controllability, and transfer function analysis.\n\n**1. Internal Asymptotic Stability**\n\nInternal (or state) stability concerns the behavior of the autonomous system, which is described by setting the external input $u(t)$ to zero:\n$$\n\\dot{x}(t) = A x(t)\n$$\nA linear time-invariant system is defined as internally asymptotically stable if, for any initial condition $x(0)$, the state vector $x(t)$ converges to the zero vector as time $t$ approaches infinity. That is, $\\lim_{t \\to \\infty} x(t) = 0$.\n\nFrom the theory of linear ordinary differential equations, the solution to this autonomous system is of the form $x(t) = \\exp(At) x(0)$. The convergence of $x(t)$ to $0$ is determined entirely by the eigenvalues of the state matrix $A$. The system is asymptotically stable if and only if all eigenvalues of $A$ have strictly negative real parts.\n\nThe given state matrix is:\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  -2 \\end{pmatrix}\n$$\nSince $A$ is a diagonal matrix, its eigenvalues are simply its diagonal entries. The eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = -2$.\n\nThe real parts of the eigenvalues are $\\text{Re}(\\lambda_1) = 1$ and $\\text{Re}(\\lambda_2) = -2$. The eigenvalue $\\lambda_1 = 1$ has a positive real part. The presence of an eigenvalue with a positive real part implies that there is a mode of the system that grows exponentially with time ($\\exp(1t)$). Therefore, the system is not asymptotically stable; it is unstable.\n\n**2. Controllability Analysis**\n\nControllability of the pair $(A, B)$ determines whether it is possible to steer the state of the system from any initial state to any final state within a finite time interval using some control input $u(t)$. For an LTI system of state dimension $n$, a necessary and sufficient condition for controllability is that the controllability matrix, $\\mathcal{C}$, has full rank, i.e., $\\text{rank}(\\mathcal{C}) = n$.\n\nThe state dimension is $n=2$. The controllability matrix is constructed as:\n$$\n\\mathcal{C} = \\begin{pmatrix} B  AB \\end{pmatrix}\n$$\nWe have $B = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. We compute the product $AB$:\n$$\nAB = \\begin{pmatrix} 1  0 \\\\ 0  -2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(1) \\\\ (0)(0) + (-2)(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}\n$$\nNow, we form the controllability matrix $\\mathcal{C}$:\n$$\n\\mathcal{C} = \\begin{pmatrix} 0  0 \\\\ 1  -2 \\end{pmatrix}\n$$\nTo check if $\\mathcal{C}$ has full rank, we can compute its determinant.\n$$\n\\det(\\mathcal{C}) = (0)(-2) - (0)(1) = 0\n$$\nSince the determinant is zero, the matrix $\\mathcal{C}$ is rank-deficient (its rank is $1$, not $2$). Therefore, the realization is **not controllable**.\n\nTo identify the uncontrollable mode, we use the Popov-Hautus-Belevitch (PBH) test. A mode associated with an eigenvalue $\\lambda$ of $A$ is uncontrollable if and only if $\\text{rank}\\begin{pmatrix} A - \\lambda I  B \\end{pmatrix}  n$.\n\nFor the eigenvalue $\\lambda_1 = 1$:\n$$\n\\begin{pmatrix} A - 1 \\cdot I  B \\end{pmatrix} = \\begin{pmatrix} 1-1  0  0 \\\\ 0  -2-1  1 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 0  -3  1 \\end{pmatrix}\n$$\nThe rank of this $2 \\times 3$ matrix is $1$, which is less than $n=2$. Thus, the mode associated with the unstable eigenvalue $\\lambda_1 = 1$ is uncontrollable.\n\nFor the eigenvalue $\\lambda_2 = -2$:\n$$\n\\begin{pmatrix} A - (-2) \\cdot I  B \\end{pmatrix} = \\begin{pmatrix} 1-(-2)  0  0 \\\\ 0  -2-(-2)  1 \\end{pmatrix} = \\begin{pmatrix} 3  0  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\nThe rank of this matrix is $2$, which is equal to $n$. Thus, the mode associated with the stable eigenvalue $\\lambda_2 = -2$ is controllable.\n\nThe analysis concludes that the unstable mode of the system is uncontrollable.\n\n**3. Transfer Function and Bounded-Input Bounded-Output (BIBO) Stability**\n\nThe transfer function $G(s)$ of an LTI system relates the Laplace transform of the output, $Y(s)$, to the Laplace transform of the input, $U(s)$, under the assumption of zero initial conditions. It is given by the formula:\n$$\nG(s) = C (sI - A)^{-1} B + D\n$$\nFirst, we compute the matrix $(sI - A)$:\n$$\nsI - A = s \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  -2 \\end{pmatrix} = \\begin{pmatrix} s-1  0 \\\\ 0  s+2 \\end{pmatrix}\n$$\nNext, we find its inverse, $(sI - A)^{-1}$:\n$$\n(sI - A)^{-1} = \\frac{1}{\\det(sI-A)} \\text{adj}(sI-A) = \\frac{1}{(s-1)(s+2)} \\begin{pmatrix} s+2  0 \\\\ 0  s-1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{s-1}  0 \\\\ 0  \\frac{1}{s+2} \\end{pmatrix}\n$$\nNow, we can compute the transfer function $G(s)$, noting that $D=0$:\n$$\nG(s) = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{s-1}  0 \\\\ 0  \\frac{1}{s+2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nG(s) = \\begin{pmatrix} (0)\\frac{1}{s-1} + (1)(0)  (0)(0) + (1)\\frac{1}{s+2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\nG(s) = \\begin{pmatrix} 0  \\frac{1}{s+2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\nG(s) = (0)(0) + \\left(\\frac{1}{s+2}\\right)(1) = \\frac{1}{s+2}\n$$\nThe transfer function, simplified to its lowest terms, is $G(s) = \\frac{1}{s+2}$.\n\nA rational transfer function is strictly proper if the degree of the denominator polynomial is strictly greater than the degree of the numerator polynomial. Here, the denominator degree is $1$ and the numerator degree is $0$. Since $1  0$, the transfer function is strictly proper.\n\nBounded-input bounded-output (BIBO) stability requires that for every bounded input $u(t)$, the output $y(t)$ is also bounded. For an LTI system, this is equivalent to the condition that all poles of the transfer function $G(s)$ lie in the open left-half of the complex plane (i.e., have strictly negative real parts).\n\nThe transfer function is $G(s) = \\frac{1}{s+2}$. It has a single pole at $s = -2$. Since the real part of this pole is $-2$, which is negative, the system is BIBO stable.\n\nIt is crucial to note that the unstable pole of the system, located at $s=1$, does not appear in the transfer function. This is a direct consequence of the fact that this mode is uncontrollable. The input $u(t)$ has no influence on the state component associated with the eigenvalue $\\lambda_1 = 1$, and thus this unstable internal dynamic is \"hidden\" from the input-output relationship. This system is a classic example of one that is internally unstable but externally (BIBO) stable.", "answer": "$$\n\\boxed{\\frac{1}{s+2}}\n$$", "id": "2713315"}, {"introduction": "While many classical techniques are restricted to linear systems, Lyapunov's direct method offers a powerful framework for analyzing the internal stability of nonlinear systems. This practice challenges you to apply this fundamental concept by constructing a suitable energy-like Lyapunov function for a given nonlinear system. By analyzing the properties of this function and its derivative, you can rigorously prove the asymptotic stability of an equilibrium point, gaining invaluable insight into a cornerstone of modern stability theory [@problem_id:2713224].", "problem": "Consider the autonomous nonlinear planar system given by the ordinary differential equation\n$$\n\\dot{x} \\,=\\, f(x), \\quad \\text{with} \\quad x \\,=\\, \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}, \\quad f(x) \\,=\\, \\begin{pmatrix} -x_{1} + x_{2} - x_{1}^{3} \\\\ -x_{1} - x_{2} \\end{pmatrix}.\n$$\nYou are asked to reason from first principles of Lyapunov stability theory for autonomous systems. Begin from the definitions of positive definiteness, radial unboundedness, and Lyapunov’s direct method for internal (asymptotic) stability of an equilibrium of a zero-input system. Do not invoke any high-level theorems without establishing the required conditions from these definitions.\n\nTasks:\n1) Construct a continuously differentiable candidate Lyapunov function $V(x)$ that is positive definite and radially unbounded, and compute its derivative $\\dot{V}(x)$ along trajectories $\\dot{x} = f(x)$. Justify both properties from first principles.\n2) Use your $V(x)$ and $\\dot{V}(x)$ to establish that the origin $x = 0$ is an internally asymptotically stable equilibrium. Provide an explicit estimate of the region of attraction based on the sign of $\\dot{V}(x)$ and the growth of $V(x)$.\n3) Among all scalars $\\lambda  0$ for which the differential inequality\n$$\n\\dot{V}(x) \\,\\leq\\, -\\lambda \\, V(x) \\quad \\text{holds for all } x \\in \\mathbb{R}^{2},\n$$\ndetermine the largest admissible constant $\\lambda$ consistent with your chosen $V(x)$ and the exact expression for $\\dot{V}(x)$.\n\nAnswer specification:\n- Report only the largest admissible $\\lambda$ from Task 3 as your final answer.\n- The final answer must be a single exact number. No rounding is required.", "solution": "The problem requires an analysis of the stability of an equilibrium point of a given autonomous nonlinear system using Lyapunov's direct method. We must proceed from first principles.\n\nThe system is described by the ordinary differential equation $\\dot{x} = f(x)$, with $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ and $f(x) = \\begin{pmatrix} -x_1 + x_2 - x_1^3 \\\\ -x_1 - x_2 \\end{pmatrix}$. First, we verify that the origin, $x=0$, is an equilibrium point. Substituting $x_1=0$ and $x_2=0$ into the expression for $f(x)$ yields $f(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, confirming that the origin is an equilibrium point.\n\nTask 1: We must construct a suitable candidate Lyapunov function $V(x)$, demonstrate its properties, and compute its time derivative along the system's trajectories. A common choice for systems with a stable linearization is a simple quadratic form. Let us propose the function:\n$$\nV(x) = \\frac{1}{2}x_1^2 + \\frac{1}{2}x_2^2\n$$\nThis function is continuously differentiable, as it is a polynomial in $x_1$ and $x_2$. We must now verify its properties from first principles.\n\nA function $V(x)$ is defined as positive definite if $V(0)=0$ and $V(x)0$ for all $x \\neq 0$.\n1.  For $x=0$, we have $x_1=0$ and $x_2=0$, so $V(0) = \\frac{1}{2}(0)^2 + \\frac{1}{2}(0)^2 = 0$.\n2.  For any $x \\neq 0$, at least one of $x_1$ or $x_2$ is non-zero. Since $x_1^2 \\geq 0$ and $x_2^2 \\geq 0$, their sum $x_1^2 + x_2^2$ is strictly positive. Thus, $V(x) = \\frac{1}{2}(x_1^2 + x_2^2)  0$.\nThese two conditions confirm that $V(x)$ is positive definite.\n\nA function $V(x)$ is radially unbounded if $V(x) \\to \\infty$ as $\\|x\\| \\to \\infty$. Our chosen function can be written in terms of the Euclidean norm $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$ as $V(x) = \\frac{1}{2}\\|x\\|_2^2$. As $\\|x\\|_2 \\to \\infty$, it is clear that $\\|x\\|_2^2 \\to \\infty$, and therefore $V(x) \\to \\infty$. The function is radially unbounded.\n\nNext, we compute the time derivative of $V(x)$ along the trajectories of the system, $\\dot{V}(x) = \\frac{d}{dt}V(x(t))$. Using the chain rule, this is given by $\\dot{V}(x) = \\nabla V(x)^T f(x)$. The gradient of $V(x)$ is $\\nabla V(x) = \\begin{pmatrix} \\frac{\\partial V}{\\partial x_1} \\\\ \\frac{\\partial V}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\nThe calculation proceeds as follows:\n$$\n\\dot{V}(x) = \\begin{pmatrix} x_1  x_2 \\end{pmatrix} \\begin{pmatrix} -x_1 + x_2 - x_1^3 \\\\ -x_1 - x_2 \\end{pmatrix}\n$$\n$$\n\\dot{V}(x) = x_1(-x_1 + x_2 - x_1^3) + x_2(-x_1 - x_2)\n$$\n$$\n\\dot{V}(x) = -x_1^2 + x_1x_2 - x_1^4 - x_1x_2 - x_2^2\n$$\nThe cross-terms $x_1x_2$ cancel, yielding the simplified expression:\n$$\n\\dot{V}(x) = -x_1^2 - x_2^2 - x_1^4\n$$\n\nTask 2: We use $V(x)$ and $\\dot{V}(x)$ to establish the stability of the origin. For asymptotic stability, we must analyze the sign of $\\dot{V}(x)$.\nA function $\\dot{V}(x)$ is negative definite if $\\dot{V}(0)=0$ and $\\dot{V}(x)0$ for all $x \\neq 0$.\n1.  At the origin, $\\dot{V}(0) = -0^2 - 0^2 - 0^4 = 0$.\n2.  For any $x \\neq 0$, the term $x_1^2 + x_2^2  0$. The term $x_1^4 \\geq 0$. Therefore, $\\dot{V}(x) = -(x_1^2 + x_2^2) - x_1^4  0$.\nThus, $\\dot{V}(x)$ is negative definite.\n\nAccording to Lyapunov's direct method, if for an equilibrium point at the origin there exists a continuously differentiable, positive definite, and radially unbounded function $V(x)$ such that its time derivative $\\dot{V}(x)$ is negative definite, then the origin is globally asymptotically stable. Our constructed $V(x)$ and its derivative $\\dot{V}(x)$ satisfy all these conditions. Therefore, the equilibrium point $x=0$ is globally asymptotically stable. The region of attraction is the entire state space, $\\mathbb{R}^2$.\n\nTask 3: We are asked to find the largest scalar $\\lambda  0$ for which the inequality $\\dot{V}(x) \\leq -\\lambda V(x)$ holds for all $x \\in \\mathbb{R}^2$.\nSubstituting our expressions for $V(x)$ and $\\dot{V}(x)$:\n$$\n-x_1^2 - x_2^2 - x_1^4 \\leq -\\lambda \\left( \\frac{1}{2}x_1^2 + \\frac{1}{2}x_2^2 \\right)\n$$\nMultiplying by $-1$ reverses the inequality:\n$$\nx_1^2 + x_2^2 + x_1^4 \\geq \\lambda \\left( \\frac{1}{2}x_1^2 + \\frac{1}{2}x_2^2 \\right)\n$$\nFor any $x \\neq 0$, we can rearrange to find an upper bound on $\\lambda$:\n$$\n\\lambda \\leq \\frac{x_1^2 + x_2^2 + x_1^4}{\\frac{1}{2}(x_1^2 + x_2^2)} = 2\\frac{x_1^2 + x_2^2 + x_1^4}{x_1^2 + x_2^2}\n$$\n$$\n\\lambda \\leq 2\\left(1 + \\frac{x_1^4}{x_1^2 + x_2^2}\\right)\n$$\nFor this inequality to hold for all $x \\in \\mathbb{R}^2$, $\\lambda$ must be less than or equal to the minimum value (infimum) of the right-hand side expression over all $x \\neq 0$.\n$$\n\\lambda_{max} = \\inf_{x \\neq 0} \\left( 2\\left(1 + \\frac{x_1^4}{x_1^2 + x_2^2}\\right) \\right)\n$$\nWe analyze the term $F(x_1, x_2) = \\frac{x_1^4}{x_1^2 + x_2^2}$. Since $x_1^4 \\ge 0$ and $x_1^2+x_2^2  0$ for $x \\ne 0$, we have $F(x_1, x_2) \\ge 0$. The minimum value of $F(x_1, x_2)$ is attained when the numerator is zero, i.e., when $x_1=0$. If we choose $x_1=0$ and any $x_2 \\neq 0$, we get $F(0, x_2) = \\frac{0}{x_2^2} = 0$.\nThus, the infimum of the expression is:\n$$\n\\lambda_{max} = 2(1 + 0) = 2\n$$\nThe largest admissible value is $\\lambda = 2$. We verify this solution by substituting $\\lambda=2$ into the original inequality:\n$$\n-x_1^2 - x_2^2 - x_1^4 \\leq -2 \\left( \\frac{1}{2}x_1^2 + \\frac{1}{2}x_2^2 \\right)\n$$\n$$\n-x_1^2 - x_2^2 - x_1^4 \\leq -x_1^2 - x_2^2\n$$\nThis simplifies to:\n$$\n-x_1^4 \\leq 0\n$$\nThis inequality is true for all $x_1 \\in \\mathbb{R}$. Therefore, the condition holds for all $x \\in \\mathbb{R}^2$, and the largest such constant is indeed $\\lambda = 2$.", "answer": "$$\\boxed{2}$$", "id": "2713224"}]}