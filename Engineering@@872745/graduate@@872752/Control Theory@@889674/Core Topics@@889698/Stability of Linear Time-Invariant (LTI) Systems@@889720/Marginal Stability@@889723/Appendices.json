{"hands_on_practices": [{"introduction": "We begin with a foundational exercise that connects the definition of marginal stability directly to the behavior of a system's state-transition matrix. This practice challenges you to construct a continuous-time linear system with eigenvalues on the imaginary axis and prove from first principles that its trajectories are bounded but not asymptotically stable. By explicitly computing the matrix exponential and analyzing its norm, you will gain a concrete understanding of the oscillatory, non-decaying nature of marginally stable systems [@problem_id:2723355].", "problem": "Consider the continuous-time linear time-invariant (LTI) system $\\dot{x}(t)=A x(t)$ where $A \\in \\mathbb{R}^{3 \\times 3}$. Starting from the definitions of the matrix exponential and marginal stability for LTI systems, do the following:\n\n1) Construct explicitly a real matrix $A$ whose spectrum is $\\{0,\\pm j\\}$, where $j^{2}=-1$, with each eigenvalue having geometric multiplicity equal to its algebraic multiplicity.\n\n2) Using only fundamental definitions such as the power-series definition of the matrix exponential and basic linear algebra, obtain a closed-form expression for $e^{A t}$.\n\n3) Prove rigorously that all trajectories $x(t)=e^{A t} x(0)$ are bounded for all $t \\ge 0$ while the system is not asymptotically stable, thus establishing marginal stability. Your argument must not rely on unproved “shortcut” criteria; base it on the spectral properties of $A$ and the explicit form of $e^{A t}$.\n\n4) Compute the induced spectral norm $\\|e^{A t}\\|_{2}$ as an explicit function of $t \\ge 0$, and then determine the exact value of $\\sup_{t \\ge 0} \\|e^{A t}\\|_{2}$.\n\nProvide as your final answer only the single real number equal to $\\sup_{t \\ge 0} \\|e^{A t}\\|_{2}$ (no units and no rounding are required).", "solution": "The problem statement is evaluated and found to be valid. It is self-contained, scientifically grounded in the principles of linear systems theory, and mathematically well-posed. No contradictions, ambiguities, or factual unsoundness are present. We may therefore proceed with a rigorous solution.\n\nThe problem requires a four-part solution, which we address in sequence.\n\n1) Construction of the matrix $A$.\nWe are tasked with constructing a real matrix $A \\in \\mathbb{R}^{3 \\times 3}$ with the spectrum $\\sigma(A) = \\{0, j, -j\\}$, where $j^{2}=-1$. The algebraic multiplicity of each eigenvalue is $1$. The geometric multiplicity of any eigenvalue must be at least $1$ and cannot exceed its algebraic multiplicity. Therefore, the condition that geometric multiplicity equals algebraic multiplicity is automatically satisfied, as it must be $1$ for all three distinct eigenvalues. This implies that the matrix $A$ is diagonalizable over the field of complex numbers $\\mathbb{C}$.\n\nFor a real matrix to have complex eigenvalues, they must appear in conjugate pairs, which is the case for $\\pm j$. The standard approach for constructing such a real matrix is to use the real Jordan normal form. The real eigenvalue $\\lambda=0$ corresponds to a $1 \\times 1$ block $[0]$. A complex conjugate pair of eigenvalues $\\alpha \\pm j\\beta$ (here, $\\alpha=0, \\beta=1$) corresponds to a $2 \\times 2$ block of the form $\\begin{pmatrix} \\alpha & -\\beta \\\\ \\beta & \\alpha \\end{pmatrix}$.\n\nThus, we construct $A$ as the following block-diagonal matrix:\n$$A = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -1 \\\\ 0 & 1 & 0 \\end{pmatrix}$$\nThis matrix is clearly real. Its characteristic polynomial is $\\det(\\lambda I - A) = \\det \\begin{pmatrix} \\lambda & 0 & 0 \\\\ 0 & \\lambda & 1 \\\\ 0 & -1 & \\lambda \\end{pmatrix} = \\lambda(\\lambda^2 + 1)$, which has roots $\\lambda=0$, $\\lambda=j$, and $\\lambda=-j$. The construction is correct.\n\n2) Computation of the matrix exponential $e^{At}$.\nThe problem requires using the power-series definition: $e^{M} = \\sum_{k=0}^{\\infty} \\frac{M^k}{k!}$.\nSince $A$ is a block-diagonal matrix of the form $A = \\mathrm{diag}(A_1, A_2)$, its exponential is $e^{At} = \\mathrm{diag}(e^{A_1 t}, e^{A_2 t})$.\n\nFor the first block, $A_1 = [0]$, a $1 \\times 1$ zero matrix.\n$e^{A_1 t} = e^{[0]t} = [e^0] = [1]$\n\nFor the second block, $A_2 = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$. We compute its powers:\n$A_2^0 = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$A_2^1 = A_2 = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$\n$A_2^2 = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} = -I_2$\n$A_2^3 = A_2 \\cdot A_2^2 = A_2(-I_2) = -A_2$\n$A_2^4 = (A_2^2)^2 = (-I_2)^2 = I_2$\nThe powers of $A_2$ cycle with a period of $4$: $\\{I_2, A_2, -I_2, -A_2, \\dots \\}$.\n\nNow we substitute these into the power series for $e^{A_2 t}$:\n$$ e^{A_2 t} = \\sum_{k=0}^{\\infty} \\frac{(A_2 t)^k}{k!} = I_2 + t A_2 + \\frac{t^2}{2!} A_2^2 + \\frac{t^3}{3!} A_2^3 + \\frac{t^4}{4!} A_2^4 + \\dots $$\n$$ e^{A_2 t} = I_2 + t A_2 - \\frac{t^2}{2!} I_2 - \\frac{t^3}{3!} A_2 + \\frac{t^4}{4!} I_2 + \\dots $$\nWe group terms multiplying $I_2$ and $A_2$:\n$$ e^{A_2 t} = \\left( 1 - \\frac{t^2}{2!} + \\frac{t^4}{4!} - \\dots \\right) I_2 + \\left( t - \\frac{t^3}{3!} + \\frac{t^5}{5!} - \\dots \\right) A_2 $$\nThese are precisely the Taylor series expansions for $\\cos(t)$ and $\\sin(t)$.\n$$ e^{A_2 t} = \\cos(t) I_2 + \\sin(t) A_2 = \\cos(t)\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\sin(t)\\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(t) & -\\sin(t) \\\\ \\sin(t) & \\cos(t) \\end{pmatrix} $$\nThis matrix is a standard rotation matrix.\n\nCombining the blocks, we obtain the closed-form expression for $e^{At}$:\n$$ e^{At} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(t) & -\\sin(t) \\\\ 0 & \\sin(t) & \\cos(t) \\end{pmatrix} $$\n\n3) Proof of marginal stability.\nA continuous-time LTI system is defined as marginally stable if its trajectories $x(t)$ are bounded for all $t \\ge 0$ for any initial condition $x(0)$, but the system is not asymptotically stable.\n\nFirst, we prove that all trajectories are bounded. The state trajectory is given by $x(t) = e^{At}x(0)$. We examine its squared Euclidean norm, $\\|x(t)\\|_2^2$.\n$$ \\|x(t)\\|_2^2 = \\|e^{At}x(0)\\|_2^2 = (e^{At}x(0))^T(e^{At}x(0)) = x(0)^T (e^{At})^T e^{At} x(0) $$\nLet us examine the matrix product $(e^{At})^T e^{At}$.\n$$ (e^{At})^T = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(t) & \\sin(t) \\\\ 0 & -\\sin(t) & \\cos(t) \\end{pmatrix} $$\n$$ (e^{At})^T e^{At} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(t) & \\sin(t) \\\\ 0 & -\\sin(t) & \\cos(t) \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(t) & -\\sin(t) \\\\ 0 & \\sin(t) & \\cos(t) \\end{pmatrix} $$\n$$ = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos^2(t) + \\sin^2(t) & -\\cos(t)\\sin(t) + \\sin(t)\\cos(t) \\\\ 0 & -\\sin(t)\\cos(t) + \\cos(t)\\sin(t) & \\sin^2(t) + \\cos^2(t) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I_3 $$\nSince $(e^{At})^T e^{At} = I_3$, the matrix $e^{At}$ is an orthogonal matrix for all $t \\in \\mathbb{R}$. Orthogonal transformations preserve the Euclidean norm. Therefore,\n$$ \\|x(t)\\|_2^2 = x(0)^T I_3 x(0) = \\|x(0)\\|_2^2 $$\nThis implies $\\|x(t)\\|_2 = \\|x(0)\\|_2$ for all $t \\ge 0$. For any finite initial state $x(0)$, the norm of the trajectory is constant, and thus bounded.\n\nNext, we show the system is not asymptotically stable. Asymptotic stability requires that $\\lim_{t \\to \\infty} x(t) = 0$ for all $x(0)$. We can find a counterexample. Let $x(0) = (1, 0, 0)^T$. Then\n$$ x(t) = e^{At}x(0) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(t) & -\\sin(t) \\\\ 0 & \\sin(t) & \\cos(t) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nIn this case, $\\lim_{t \\to \\infty} x(t) = (1, 0, 0)^T \\neq 0$. Thus, the system is not asymptotically stable.\nSince all trajectories are bounded and the system is not asymptotically stable, it is marginally stable by definition. This argument is based on the explicit form of $e^{At}$ as required.\n\n4) Computation of $\\|e^{At}\\|_2$ and its supremum.\nThe induced $2$-norm (or spectral norm) of a matrix $M$, denoted $\\|M\\|_2$, is its largest singular value, $\\sigma_{\\max}(M)$. The singular values are the square roots of the eigenvalues of $M^T M$.\n\nIn the previous step, we proved that for our matrix $E(t) = e^{At}$, we have $E(t)^T E(t) = I_3$.\nThe eigenvalues of the identity matrix $I_3$ are all equal to $1$.\nThe singular values of $e^{At}$ are the square roots of these eigenvalues, so $\\sigma_1 = \\sigma_2 = \\sigma_3 = \\sqrt{1} = 1$.\nThe largest singular value is $\\sigma_{\\max}(e^{At}) = 1$.\nTherefore, the induced spectral norm is an explicit function of $t \\ge 0$:\n$$ \\|e^{At}\\|_2 = 1 $$\nThis is a constant function.\n\nFinally, we must determine the exact value of $\\sup_{t \\ge 0} \\|e^{At}\\|_2$.\n$$ \\sup_{t \\ge 0} \\|e^{At}\\|_2 = \\sup_{t \\ge 0} (1) = 1 $$\nThe supremum of a constant function is the constant itself.", "answer": "$$\\boxed{1}$$", "id": "2723355"}, {"introduction": "The location of eigenvalues on the stability boundary is not the full story; their multiplicity plays a critical role. This discrete-time exercise explores the crucial distinction between a simple eigenvalue and a repeated eigenvalue on the unit circle. By analyzing a system with a Jordan block of size greater than one for an eigenvalue at $z=1$, you will see how this structure leads to unbounded, polynomially growing trajectories, demonstrating why such a system is unstable, not marginally stable [@problem_id:2723345].", "problem": "Consider the discrete-time linear time-invariant (LTI) system defined by $x_{k+1} = A x_k$, where $A \\in \\mathbb{R}^{3 \\times 3}$ is the controllable companion matrix associated with the monic characteristic polynomial $(z-1)^2(z-0.5)$. Expand the polynomial to determine the explicit $A$. Starting from the foundational definitions of the spectral radius, minimal polynomial, Jordan normal form, and the behavior of powers of Jordan blocks, analyze the asymptotic behavior of $A^k$ as $k \\to \\infty$.\n\nDefine the polynomial growth degree $d$ of $\\lVert A^k \\rVert$ (for any induced matrix norm) to be the unique nonnegative integer such that there exist positive constants $c_1$ and $c_2$ for which $c_1 k^d \\le \\lVert A^k \\rVert \\le c_2 k^d$ holds for all sufficiently large $k$, in the case where the spectral radius is $1$ and all eigenvalues satisfy $|\\lambda| \\le 1$. Using only first principles and well-tested facts (not shortcut formulas), derive $d$ for this $A$ and use your result to explain, in terms of the eigenstructure, why the system is not marginally stable.\n\nReport as your final answer the integer value of $d$. No rounding is required. The final answer must be given as a single integer without units.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n- System: Discrete-time linear time-invariant (LTI) system $x_{k+1} = A x_k$.\n- Matrix $A$: $A \\in \\mathbb{R}^{3 \\times 3}$.\n- Matrix Type: Controllable companion matrix.\n- Characteristic Polynomial: $p(z) = (z-1)^2(z-0.5)$.\n- Definition: The polynomial growth degree $d$ of $\\lVert A^k \\rVert$ is the unique nonnegative integer such that $c_1 k^d \\le \\lVert A^k \\rVert \\le c_2 k^d$ for positive constants $c_1, c_2$ and sufficiently large $k$, under specified conditions.\n- Task: Derive $d$ from first principles and explain why the system is not marginally stable.\n- Final Answer: The integer value of $d$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded within the fields of linear algebra and control theory. The concepts of companion matrices, characteristic polynomials, Jordan normal forms, and system stability are standard and well-defined. The problem is self-contained, with the characteristic polynomial providing sufficient information to determine the eigenstructure of matrix $A$. The tasks are logically structured and lead to a unique, well-defined integer solution for $d$. The problem is objective and free of ambiguity. It is not trivial, as it requires a step-by-step derivation from foundational principles rather than the application of a memorized formula.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete, reasoned solution will be provided.\n\nWe begin by determining the explicit form of the matrix $A$. The characteristic polynomial is given as $p(z) = (z-1)^2(z-0.5)$. We expand this polynomial to find its coefficients:\n$$p(z) = (z^2 - 2z + 1)(z - 0.5)$$\n$$p(z) = z^3 - 0.5z^2 - 2z^2 + z + z - 0.5$$\n$$p(z) = z^3 - 2.5z^2 + 2z - 0.5$$\nThis is a monic polynomial of the form $p(z) = z^3 + a_2 z^2 + a_1 z + a_0$, with coefficients $a_2 = -2.5$, $a_1 = 2$, and $a_0 = -0.5$. The controllable companion matrix associated with this polynomial is:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-a_0 & -a_1 & -a_2\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0.5 & -2 & 2.5\n\\end{pmatrix}\n$$\nThe eigenvalues of $A$ are the roots of its characteristic polynomial, which are $\\lambda_1 = 1$ with algebraic multiplicity $2$, and $\\lambda_2 = 0.5$ with algebraic multiplicity $1$. The spectral radius is $\\rho(A) = \\max\\{|1|, |0.5|\\} = 1$.\n\nThe structure of the Jordan normal form $J$ of $A$ is determined by the minimal polynomial $m(z)$ of $A$. A fundamental property of a controllable companion matrix is that its minimal polynomial is identical to its characteristic polynomial. Therefore, $m(z) = p(z) = (z-1)^2(z-0.5)$.\n\nThe factors of the minimal polynomial determine the sizes of the Jordan blocks.\n1. The factor $(z-1)^2$ implies that the largest Jordan block associated with the eigenvalue $\\lambda=1$ is of size $2 \\times 2$. Since the algebraic multiplicity of $\\lambda=1$ is $2$, there must be a single Jordan block of this size:\n$$J_1 = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}$$\n2. The factor $(z-0.5)^1$ implies that the Jordan block associated with the eigenvalue $\\lambda=0.5$ is of size $1 \\times 1$:\n$$J_{0.5} = \\begin{pmatrix} 0.5 \\end{pmatrix}$$\nThus, the Jordan normal form of $A$ is $J = \\mathrm{diag}(J_1, J_{0.5})$, up to permutation of blocks. There exists an invertible matrix $P$ such that $A = PJP^{-1}$.\n\nTo analyze the asymptotic behavior of $A^k$, we examine $A^k = (PJP^{-1})^k = P J^k P^{-1}$. The matrix $J^k$ is also block diagonal: $J^k = \\mathrm{diag}(J_1^k, J_{0.5}^k)$.\n- For the block $J_{0.5}$, the power is simply $J_{0.5}^k = \\begin{pmatrix} 0.5^k \\end{pmatrix}$. As $k \\to \\infty$, this term decays to $0$.\n- For the block $J_1$, we compute its $k$-th power. Let $J_1 = I + N$, where $I$ is the $2 \\times 2$ identity matrix and $N = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$. The matrix $N$ is nilpotent with $N^2 = 0$. Using the binomial theorem, which is applicable since $I$ and $N$ commute:\n$$J_1^k = (I+N)^k = \\binom{k}{0}I^k N^0 + \\binom{k}{1}I^{k-1}N^1 + \\binom{k}{2}I^{k-2}N^2 + \\dots$$\nSince $N^m = 0$ for $m \\ge 2$, all terms from the third onward are zero.\n$$J_1^k = 1 \\cdot I \\cdot I + k \\cdot I \\cdot N = I + kN = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + k \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & k \\\\ 0 & 1 \\end{pmatrix}$$\nCombining the blocks, we have:\n$$J^k = \\begin{pmatrix} 1 & k & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0.5^k \\end{pmatrix}$$\nThe behavior of $\\lVert A^k \\rVert$ is tied to the behavior of $\\lVert J^k \\rVert$. For any induced matrix norm, the property $\\lVert XY \\rVert \\le \\lVert X \\rVert \\lVert Y \\rVert$ and the fact that all norms on a finite-dimensional space are equivalent imply that there exist positive constants $\\tilde{c}_1, \\tilde{c}_2$ such that for all $k$:\n$$\\tilde{c}_1 \\lVert J^k \\rVert \\le \\lVert A^k \\rVert = \\lVert P J^k P^{-1} \\rVert \\le \\tilde{c}_2 \\lVert J^k \\rVert$$\nTherefore, $\\lVert A^k \\rVert$ and $\\lVert J^k \\rVert$ have the same order of polynomial growth. We examine the growth of $\\lVert J^k \\rVert$. For large $k$, the term $k$ is the fastest-growing element in the matrix $J^k$. The other elements are either constant ($1$) or decay to zero ($0.5^k$). For any matrix norm, the norm will be dominated by this linear term in $k$. For example, the infinity norm is $\\lVert J^k \\rVert_\\infty = \\max(|1|+|k|, |1|, |0.5^k|) = 1+k$. The Frobenius norm is $\\lVert J^k \\rVert_F = \\sqrt{1^2 + k^2 + 1^2 + (0.5^k)^2} = \\sqrt{k^2+2+0.25^k}$. For large $k$, both norms are asymptotically proportional to $k$. Thus, we can find positive constants $c'_1, c'_2$ such that for sufficiently large $k$:\n$$c'_1 k^1 \\le \\lVert J^k \\rVert \\le c'_2 k^1$$\nCombining this with our earlier inequality, we conclude there exist positive constants $c_1 = \\tilde{c}_1 c'_1$ and $c_2 = \\tilde{c}_2 c'_2$ such that for sufficiently large $k$:\n$$c_1 k^1 \\le \\lVert A^k \\rVert \\le c_2 k^1$$\nBy comparison with the definition of the polynomial growth degree $d$, we identify $d=1$. The growth is linear. This value $d=m_{\\max}-1$, where $m_{\\max}=2$ is the size of the largest Jordan block for an eigenvalue with magnitude $1$.\n\nFinally, we analyze the system's stability. A discrete-time LTI system is marginally stable if and only if $\\sup_{k \\ge 0} \\lVert A^k \\rVert < \\infty$. In our case, $\\lVert A^k \\rVert$ grows approximately linearly with $k$, so $\\lim_{k \\to \\infty} \\lVert A^k \\rVert = \\infty$. The norm is unbounded. Therefore, the system is not marginally stable; it is unstable. The fundamental reason for this instability, rooted in the eigenstructure of $A$, is the presence of a Jordan block of size greater than $1$ (specifically, size $2$) for an eigenvalue on the unit circle ($\\lambda=1$). For marginal stability, any eigenvalue $\\lambda$ on the unit circle ($|\\lambda|=1$) must be semi-simple, meaning its algebraic and geometric multiplicities must be equal, which is equivalent to stating that all its Jordan blocks must be of size $1 \\times 1$. This condition is violated here for $\\lambda=1$.\nThe final answer is the integer value of $d$.", "answer": "$$\\boxed{1}$$", "id": "2723345"}, {"introduction": "Linearization is a powerful tool, but its predictions are inconclusive when eigenvalues lie on the imaginary axis. This practice takes you to the frontier of nonlinear analysis, where the stability of such an equilibrium is determined by higher-order nonlinear terms. By analyzing a system near a Hopf bifurcation and computing its first Lyapunov coefficient, you will determine how the nonlinearities can either stabilize, destabilize, or preserve the marginal stability of the origin [@problem_id:2723364].", "problem": "Consider the planar polynomial vector field parameterized by real constants $\\omega>0$, $\\sigma \\in \\mathbb{R}$, and $\\kappa \\in \\mathbb{R}$:\n$$\n\\begin{aligned}\n\\dot{x} &= -\\omega y + \\sigma x \\left(x^{2}+y^{2}\\right) - \\kappa y \\left(x^{2}+y^{2}\\right),\\\\\n\\dot{y} &= \\ \\ \\ \\omega x + \\sigma y \\left(x^{2}+y^{2}\\right) + \\kappa x \\left(x^{2}+y^{2}\\right).\n\\end{aligned}\n$$\nThe origin is an equilibrium. The Jacobian at the origin has eigenvalues $\\pm j \\omega$, a simple purely imaginary pair.\n\nUsing only fundamental concepts and well-tested facts of nonlinear systems analysis in control theory (for example, linearization, Lyapunov stability, and the reduction to polar coordinates or to the center manifold when the linearization has eigenvalues on the imaginary axis), do the following:\n\n1. Starting from the definitions of stability for an equilibrium and from the given vector field, derive the reduced scalar amplitude dynamics near the origin by transforming to polar coordinates $x = r\\cos\\theta$, $y = r\\sin\\theta$. Explain why the linearization alone is inconclusive and justify the reduction step based on the structure of the system.\n\n2. From the reduced amplitude dynamics, identify the first Lyapunov coefficient at the Hopf point (that is, at the parameter value where the real part of the linearization’s eigenvalues is zero). Then, rigorously argue how the sign of this coefficient determines whether the origin is locally asymptotically stable, marginally stable (Lyapunov stable but not asymptotically stable), or unstable.\n\n3. Compute the exact closed-form analytic expression of the first Lyapunov coefficient for this system in terms of $\\omega$, $\\sigma$, and $\\kappa$.\n\nProvide your final answer as a single analytic expression. No numerical rounding is required, and no units are involved.", "solution": "The problem as stated is subjected to validation.\n\nGivens extracted verbatim from the problem statement:\n- Planar polynomial vector field:\n$$\n\\begin{aligned}\n\\dot{x} &= -\\omega y + \\sigma x \\left(x^{2}+y^{2}\\right) - \\kappa y \\left(x^{2}+y^{2}\\right),\\\\\n\\dot{y} &= \\ \\ \\ \\omega x + \\sigma y \\left(x^{2}+y^{2}\\right) + \\kappa x \\left(x^{2}+y^{2}\\right).\n\\end{aligned}\n$$\n- Parameters: $\\omega>0$, $\\sigma \\in \\mathbb{R}$, $\\kappa \\in \\mathbb{R}$.\n- Equilibrium: The origin $(0,0)$.\n- Jacobian eigenvalues at the origin: $\\pm j \\omega$.\n\nProblem Validation:\n1.  **Scientific or Factual Unsoundness**: The problem describes a canonical system near a Hopf bifurcation, a fundamental topic in nonlinear dynamics and control theory. The mathematical structure is standard and sound. No flaws.\n2.  **Non-Formalizable or Irrelevant**: The problem is precisely stated in mathematical terms and is directly formalizable. It is relevant to the stability analysis of nonlinear systems. No flaws.\n3.  **Incomplete or Contradictory Setup**: The system and its parameters are fully defined. The given eigenvalues are consistent with the linearization of the vector field at the origin. The problem is self-contained and consistent. No flaws.\n4.  **Unrealistic or Infeasible**: The problem is a purely mathematical exercise. No physical constraints are violated. No flaws.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The tasks requested lead to a unique, well-defined analytical expression for the first Lyapunov coefficient. No flaws.\n6.  **Pseudo-Profound, Trivial, Tautological**: The problem requires a standard, but non-trivial, application of coordinate transformation and stability analysis techniques for nonlinear systems. It is not trivial or tautological. No flaws.\n7.  **Outside Scientific Verifiability**: The calculations and the final result are fully verifiable through standard mathematical procedures. No flaws.\n\nVerdict: The problem is valid. A solution will be provided.\n\nThe analysis proceeds in three parts as requested.\n\n1. The starting point is the given system of ordinary differential equations:\n$$\n\\begin{aligned}\n\\dot{x} &= -\\omega y + \\sigma x \\left(x^{2}+y^{2}\\right) - \\kappa y \\left(x^{2}+y^{2}\\right) \\\\\n\\dot{y} &= \\omega x + \\sigma y \\left(x^{2}+y^{2}\\right) + \\kappa x \\left(x^{2}+y^{2}\\right)\n\\end{aligned}\n$$\nFirst, we analyze the linearization at the equilibrium point $(x,y) = (0,0)$. The Jacobian matrix $J$ of the vector field is:\n$$\nJ(x,y) = \\begin{pmatrix}\n\\frac{\\partial \\dot{x}}{\\partial x} & \\frac{\\partial \\dot{x}}{\\partial y} \\\\\n\\frac{\\partial \\dot{y}}{\\partial x} & \\frac{\\partial \\dot{y}}{\\partial y}\n\\end{pmatrix}\n$$\nThe partial derivatives are:\n$\\frac{\\partial \\dot{x}}{\\partial x} = \\sigma \\left(x^{2}+y^{2}\\right) + 2\\sigma x^{2} - 2\\kappa xy$\n$\\frac{\\partial \\dot{x}}{\\partial y} = -\\omega + 2\\sigma xy - \\kappa \\left(x^{2}+y^{2}\\right) - 2\\kappa y^{2}$\n$\\frac{\\partial \\dot{y}}{\\partial x} = \\omega + 2\\sigma yx + \\kappa \\left(x^{2}+y^{2}\\right) + 2\\kappa x^{2}$\n$\\frac{\\partial \\dot{y}}{\\partial y} = \\sigma \\left(x^{2}+y^{2}\\right) + 2\\sigma y^{2} + 2\\kappa xy$\n\nEvaluating at the origin $(0,0)$:\n$$\nJ(0,0) = \\begin{pmatrix}\n0 & -\\omega \\\\\n\\omega & 0\n\\end{pmatrix}\n$$\nThe characteristic equation is $\\det(J(0,0) - \\lambda I) = \\lambda^{2} + \\omega^{2} = 0$, which yields the eigenvalues $\\lambda_{1,2} = \\pm j\\omega$. Since the eigenvalues are a simple, purely imaginary pair (zero real part), the Hartman-Grobman theorem (or Lyapunov's first method) is inconclusive. The stability of the equilibrium depends on the nonlinear terms, which are neglected in the linearization. This necessitates a nonlinear analysis, such as a center manifold reduction or, more directly for this system, a transformation to polar coordinates.\n\nThis reduction is justified because the linear part of the system describes a center (rotation), and we must investigate if the nonlinear terms introduce radial drift. The specific structure of the nonlinearities, involving the term $(x^2+y^2)$, is a strong indicator that a polar coordinate transformation will simplify the analysis. We introduce polar coordinates $x = r \\cos\\theta$ and $y = r \\sin\\theta$, where $r^{2} = x^{2}+y^{2}$.\n\nTo find the dynamics for the amplitude $r$, we differentiate $r^{2} = x^{2}+y^{2}$ with respect to time:\n$2r\\dot{r} = 2x\\dot{x} + 2y\\dot{y} \\implies \\dot{r} = \\frac{x\\dot{x} + y\\dot{y}}{r}$.\nSubstituting the expressions for $\\dot{x}$ and $\\dot{y}$:\n$r\\dot{r} = x\\left[-\\omega y + \\sigma x \\left(x^{2}+y^{2}\\right) - \\kappa y \\left(x^{2}+y^{2}\\right)\\right] + y\\left[\\omega x + \\sigma y \\left(x^{2}+y^{2}\\right) + \\kappa x \\left(x^{2}+y^{2}\\right)\\right]$\n$r\\dot{r} = -\\omega xy + \\sigma x^{2}r^{2} - \\kappa xyr^{2} + \\omega xy + \\sigma y^{2}r^{2} + \\kappa xyr^{2}$\nMany terms cancel:\n$r\\dot{r} = \\sigma x^{2}r^{2} + \\sigma y^{2}r^{2} = \\sigma r^{2}(x^{2}+y^{2}) = \\sigma r^{2}(r^{2}) = \\sigma r^{4}$\nFor $r \\neq 0$, we divide by $r$ to obtain the reduced scalar amplitude dynamics:\n$$\n\\dot{r} = \\sigma r^{3}\n$$\nFor completeness, the phase dynamics are found via $\\dot{\\theta} = \\frac{x\\dot{y} - y\\dot{x}}{r^{2}}$:\n$r^{2}\\dot{\\theta} = x\\left[\\omega x + \\sigma yr^{2} + \\kappa xr^{2}\\right] - y\\left[-\\omega y + \\sigma xr^{2} - \\kappa yr^{2}\\right]$\n$r^{2}\\dot{\\theta} = \\omega x^{2} + \\sigma xyr^{2} + \\kappa x^{2}r^{2} + \\omega y^{2} - \\sigma xyr^{2} + \\kappa y^{2}r^{2}$\n$r^{2}\\dot{\\theta} = \\omega (x^{2}+y^{2}) + \\kappa r^{2}(x^{2}+y^{2}) = \\omega r^{2} + \\kappa r^{4}$\nThus, $\\dot{\\theta} = \\omega + \\kappa r^{2}$. The full system in polar coordinates is decoupled:\n$$\n\\begin{aligned}\n\\dot{r} &= \\sigma r^{3} \\\\\n\\dot{\\theta} &= \\omega + \\kappa r^{2}\n\\end{aligned}\n$$\n\n2. The stability of the origin $(r=0)$ is determined by the amplitude equation $\\dot{r} = \\sigma r^{3}$. Near a Hopf bifurcation point, the amplitude dynamics are generally expressed as a series expansion in $r$, of the form $\\dot{r} = \\mu r + a_{1}r^{3} + a_{2}r^{5} + \\dots$. The parameter $\\mu$ is the real part of the critical pair of eigenvalues, which is zero in this problem. The coefficient $a_{1}$ is known as the first Lyapunov coefficient, conventionally denoted $L_1$. In our derived equation, $\\dot{r} = \\sigma r^{3}$, we see that the linear term is absent and the first non-vanishing term is cubic. By direct comparison, the first Lyapunov coefficient for this system is $L_1 = \\sigma$.\n\nThe sign of $L_1 = \\sigma$ determines the stability of the equilibrium at $r=0$. We analyze the behavior of $\\dot{r}$ for small $r > 0$.\n- If $\\sigma < 0$: $\\dot{r} < 0$. The radius $r$ decreases for any initial perturbation $r(0) > 0$. Trajectories starting near the origin converge to the origin. Thus, the origin is locally asymptotically stable.\n- If $\\sigma > 0$: $\\dot{r} > 0$. The radius $r$ increases for any initial perturbation $r(0) > 0$. Trajectories starting near the origin move away from it. Thus, the origin is unstable.\n- If $\\sigma = 0$: $\\dot{r} = 0$. The radius $r$ remains constant, $r(t) = r(0)$. Trajectories are circles around the origin. An orbit starting in a neighborhood of the origin remains in a (possibly larger) neighborhood, but does not converge to the origin. The origin is stable in the sense of Lyapunov, but not asymptotically stable. This is a case of marginal stability (a center).\n\n3. As established from the analysis in polar coordinates, the amplitude dynamics are given by $\\dot{r} = \\sigma r^{3}$. The first Lyapunov coefficient, $L_1$, is by definition the coefficient of the $r^{3}$ term in the normal form of the amplitude equation. Therefore, for this system, the first Lyapunov coefficient is exactly $\\sigma$. The parameters $\\omega$ and $\\kappa$ influence the angular velocity, but not the growth or decay of the amplitude at the cubic order.\n\nTo confirm this result rigorously, one can use the general formula for the first Lyapunov coefficient for a planar system $\\dot{x}=f(x,y)$, $\\dot{y}=g(x,y)$ with eigenvalues $\\pm j\\omega$ at the origin:\n$$\nL_1 = \\frac{1}{16\\omega} \\left[f_{xy}(f_{xx}+f_{yy}) - g_{xy}(g_{xx}+g_{yy}) - f_{xx}g_{xx} + f_{yy}g_{yy} \\right] + \\frac{1}{16} \\left[f_{xxx}+f_{xyy}+g_{xxy}+g_{yyy} \\right]\n$$\nwhere all partial derivatives are evaluated at the equilibrium $(0,0)$.\nIn our system, the nonlinear terms are exclusively cubic. Consequently, all second-order partial derivatives ($f_{xx}, f_{xy}, f_{yy}, g_{xx}, g_{xy}, g_{yy}$) are zero at the origin. The formula simplifies to:\n$$\nL_1 = \\frac{1}{16} \\left[f_{xxx}+f_{xyy}+g_{xxy}+g_{yyy} \\right]_{(0,0)}\n$$\nWe compute the necessary third-order derivatives from $f(x,y) = -\\omega y + \\sigma x^{3} + \\sigma xy^{2} - \\kappa yx^{2} - \\kappa y^{3}$ and $g(x,y) = \\omega x + \\sigma yx^{2} + \\sigma y^{3} + \\kappa x^{3} + \\kappa xy^{2}$.\n- $f_{xxx} = 6\\sigma$\n- $f_{xyy} = 2\\sigma$\n- $g_{xxy} = 2\\sigma$\n- $g_{yyy} = 6\\sigma$\nSubstituting these values back into the simplified formula:\n$$\nL_1 = \\frac{1}{16} \\left( 6\\sigma + 2\\sigma + 2\\sigma + 6\\sigma \\right) = \\frac{16\\sigma}{16} = \\sigma\n$$\nThis confirms the result obtained from the polar-coordinate transformation. The closed-form analytic expression for the first Lyapunov coefficient is $\\sigma$.", "answer": "$$\\boxed{\\sigma}$$", "id": "2723364"}]}