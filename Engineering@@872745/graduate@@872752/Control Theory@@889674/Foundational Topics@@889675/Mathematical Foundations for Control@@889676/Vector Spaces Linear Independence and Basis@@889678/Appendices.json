{"hands_on_practices": [{"introduction": "The concept of controllability is central to modern control theory, defining whether a system's state can be driven to any desired value. This practice [@problem_id:2757663] directly connects this system property to the vector space concept of a basis. By constructing the controllability matrix, you will explore how its columns form a spanning set for the state space and then practice the essential skill of selecting a linearly independent subset to form a valid basis for $\\mathbb{R}^n$.", "problem": "Let $\\dot{x}(t) = A x(t) + B u(t)$ be a finite-dimensional linear time-invariant (LTI) system with $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$. The reachable subspace from the origin over any nontrivial time interval equals the linear span of vectors of the form $A^{k} b_{j}$ where $k \\in \\{0,1,\\dots,n-1\\}$ and $b_{j}$ is the $j$-th column of $B$. The pair $(A,B)$ is called controllable if the reachable subspace equals $\\mathbb{R}^{n}$.\n\n1) Starting from the definitions of reachability and linear span in vector spaces, and using only fundamental properties of linearity, show that when $(A,B)$ is controllable, the set of columns of the controllability matrix\n$$\n\\mathcal{C}(A,B) \\coloneqq \\big[\\, B \\;\\; A B \\;\\; A^{2} B \\;\\; \\dots \\;\\; A^{n-1} B \\,\\big]\n$$\nspans $\\mathbb{R}^{n}$, and therefore any choice of $n$ linearly independent columns from $\\mathcal{C}(A,B)$ forms a basis of $\\mathbb{R}^{n}$.\n\n2) Describe, in terms of linear independence and the graded sequence of subspaces $\\mathcal{R}_{k} \\coloneqq \\operatorname{span}\\{B,AB,\\dots,A^{k}B\\}$, conditions under which a basis selected from columns of $\\mathcal{C}(A,B)$ is unique. Your discussion should identify structural features (e.g., the number of inputs and the pattern of rank increments $\\operatorname{rank}(\\mathcal{R}_{k}) - \\operatorname{rank}(\\mathcal{R}_{k-1})$) that either enforce uniqueness or create non-uniqueness, explicitly citing how ordering and normalization choices affect the outcome.\n\n3) Consider the concrete $4$-state, $2$-input system with\n$$\nA \\,=\\, \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix},\n\\qquad\nB \\,=\\, \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \n\\end{bmatrix}.\n$$\nForm the controllability matrix $\\mathcal{C}(A,B)$, and, proceeding left-to-right, select the first $n=4$ columns that are linearly independent to assemble a basis matrix $T \\in \\mathbb{R}^{4 \\times 4}$ (i.e., $T$ has those four columns, in the same order, as its columns). Compute the determinant $\\det(T)$. Express the final answer as an exact integer (no rounding is needed).", "solution": "The problem as stated is scientifically grounded, well-posed, and complete. It presents a standard set of questions in linear control theory. I shall proceed to solve it.\n\n### Part 1: Proof of the Controllability Rank Condition\n\nThe problem defines a linear time-invariant (LTI) system $\\dot{x}(t) = A x(t) + B u(t)$, with $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{n \\times n}$, and $B \\in \\mathbb{R}^{n \\times m}$. Let the columns of the input matrix $B$ be denoted by $b_j$ for $j \\in \\{1, 2, \\dots, m\\}$.\n\nThe problem provides two key definitions as premises:\n1.  The reachable subspace from the origin, denoted $\\mathcal{R}$, is defined as the linear span of the set of vectors $S = \\{A^k b_j \\mid k \\in \\{0, 1, \\dots, n-1\\}, j \\in \\{1, \\dots, m\\}\\}$. In mathematical notation, $\\mathcal{R} \\coloneqq \\operatorname{span}(S)$.\n2.  The pair $(A, B)$ is controllable if and only if the reachable subspace is the entire state space, i.e., $\\mathcal{R} = \\mathbb{R}^n$.\n\nThe controllability matrix is defined as\n$$\n\\mathcal{C}(A,B) \\coloneqq \\big[\\, B \\;\\; A B \\;\\; A^{2} B \\;\\; \\dots \\;\\; A^{n-1} B \\,\\big]\n$$\nThis matrix is formed by concatenating the blocks $A^k B$ for $k = 0, \\dots, n-1$. Each block $A^k B$ consists of the columns $A^k b_1, A^k b_2, \\dots, A^k b_m$.\n\nThe set of all columns of the controllability matrix $\\mathcal{C}(A,B)$ is precisely the set $S$ defined in the premise. That is, the columns of $\\mathcal{C}(A,B)$ are the vectors $\\{A^k b_j\\}$ for all $k \\in \\{0, \\dots, n-1\\}$ and $j \\in \\{1, \\dots, m\\}$.\n\nBy the definition of linear span, the space spanned by the columns of $\\mathcal{C}(A,B)$ is $\\operatorname{span}(S)$. From the problem's first premise, this is the reachable subspace $\\mathcal{R}$.\n$$\n\\operatorname{span}(\\text{columns of } \\mathcal{C}(A,B)) = \\operatorname{span}(S) = \\mathcal{R}\n$$\n\nThe problem states that we are to assume the pair $(A,B)$ is controllable. By the second premise, this means $\\mathcal{R} = \\mathbb{R}^n$.\nSubstituting this into our previous finding, we arrive at the conclusion:\n$$\n\\operatorname{span}(\\text{columns of } \\mathcal{C}(A,B)) = \\mathbb{R}^n\n$$\nThis demonstrates that when $(A,B)$ is controllable, the columns of the controllability matrix $\\mathcal{C}(A,B)$ span the state space $\\mathbb{R}^n$.\n\nA basis for an $n$-dimensional vector space like $\\mathbb{R}^n$ is defined as any set of $n$ linearly independent vectors that span the space. Since the columns of $\\mathcal{C}(A,B)$ form a spanning set for $\\mathbb{R}^n$, it follows from the definition of a basis that any subset of these columns containing exactly $n$ linearly independent vectors constitutes a basis for $\\mathbb{R}^n$.\n\n### Part 2: Uniqueness of Basis from Controllability Matrix\n\nThe uniqueness of a basis selected from the columns of the controllability matrix $\\mathcal{C}(A,B)$ depends critically on the number of inputs, $m$.\n\nFirst, consider the statement \"a basis selected from columns of $\\mathcal{C}(A,B)$\". This implies we choose a subset of the column vectors of $\\mathcal{C}(A,B)$ as they are given, without allowing for arbitrary scaling (normalization) or linear combinations. We are discussing the uniqueness of this chosen *set* of vectors.\n\nCase 1: Single-Input System ($m=1$)\nIf the system has a single input, $B$ is an $n \\times 1$ column vector, which we denote as $b$. The controllability matrix is\n$$\n\\mathcal{C}(A,b) = \\big[\\, b \\;\\; Ab \\;\\; A^2 b \\;\\; \\dots \\;\\; A^{n-1}b \\,\\big]\n$$\nThis is an $n \\times n$ matrix. For the pair $(A,b)$ to be controllable, the columns of $\\mathcal{C}(A,b)$ must span $\\mathbb{R}^n$. Since there are exactly $n$ columns, for them to span the $n$-dimensional space $\\mathbbR^n$, they must be linearly independent.\nIn this situation, there are precisely $n$ columns in $\\mathcal{C}(A,b)$, and all $n$ of them are required to form a basis. Therefore, there is only one possible set of vectors to choose: the entire set of columns of $\\mathcal{C}(A,b)$. The choice of basis vectors is unique.\nOrdering and normalization: If the order of vectors in the basis matters, then the basis $[b, Ab, \\dots, A^{n-1}b]$ is also unique. If permutations are allowed, there are $n!$ possible ordered bases. Since the problem phrasing does not mention normalization, we assume the columns are taken as is.\n\nCase 2: Multi-Input System ($m>1$)\nIf the system has multiple inputs, the controllability matrix $\\mathcal{C}(A,B)$ is an $n \\times (nm)$ matrix. Since $m>1$, the number of columns $nm$ is strictly greater than the dimension of the state space $n$.\nFor the pair $(A,B)$ to be controllable, the rank of this $n \\times (nm)$ matrix must be $n$. This means that the set of $nm$ columns spans $\\mathbb{R}^n$. Since there are more than $n$ vectors in this spanning set, the set must be linearly dependent.\nThe existence of linear dependencies among the columns of $\\mathcal{C}(A,B)$ implies that there are multiple distinct ways to select a subset of $n$ linearly independent columns. For a simple example, if columns $c_1, c_2, \\dots, c_n$ form a basis, and another column $c_{n+1}$ can be written as a linear combination of some of these, say $c_{n+1} = \\alpha_1 c_1 + \\dots + \\alpha_n c_n$ with $\\alpha_1 \\neq 0$, then we can replace $c_1$ with $c_{n+1}$ to form a new basis $\\{c_{n+1}, c_2, \\dots, c_n\\}$. As long as $c_{n+1}$ is not a scalar multiple of $c_1$ and is a distinct column in $\\mathcal{C}(A,B)$, this new set of basis vectors is different from the original one. Such dependencies are guaranteed to exist when $nm > n$. Therefore, for any controllable multi-input system, the basis selected from the columns of $\\mathcal{C}(A,B)$ is **never** unique.\n\nThe graded sequence of subspaces $\\mathcal{R}_k = \\operatorname{span}\\{B, AB, \\dots, A^k B\\}$ and the rank increments $\\nu_k = \\operatorname{rank}(\\mathcal{R}_k) - \\operatorname{rank}(\\mathcal{R}_{k-1})$ (with $\\mathcal{R}_{-1}=\\{0\\}$) illuminate this non-uniqueness. At each step $k$, we must find $\\nu_k$ new basis vectors from the columns of $A^k B$ that are linearly independent of $\\mathcal{R}_{k-1}$. For $m>1$, it is generally possible that the number of columns in $A^k B$ that are candidates for providing these new directions is greater than $\\nu_k$, creating a choice. For instance, at step $k=0$, we must choose $\\nu_0 = \\operatorname{rank}(B)$ vectors. If $m > \\operatorname{rank}(B)$, there are multiple ways to choose a basis for the image of $B$ from its columns.\n\nIn summary, the structural feature determining uniqueness is the number of inputs $m$:\n-   Uniqueness of the set of basis vectors is guaranteed if and only if $m=1$.\n-   Non-uniqueness is guaranteed if $m>1$.\n\n### Part 3: Concrete Example and Determinant Calculation\n\nWe are given the system with $n=4$ states and $m=2$ inputs:\n$$\nA \\,=\\, \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix},\n\\qquad\nB \\,=\\, \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \n\\end{bmatrix} \\,=\\, [b_1, b_2]\n$$\nThe controllability matrix is $\\mathcal{C}(A,B) = [B, AB, A^2 B, A^3 B]$. We need to find the first $4$ linearly independent columns by searching from left to right.\n\nThe first two columns are the columns of $B$:\n$c_1 = b_1 = [0, 0, 1, 0]^T$\n$c_2 = b_2 = [0, 0, 0, 1]^T$\nThese two vectors are linearly independent. Our basis so far is $\\{c_1, c_2\\}$.\n\nNext, we compute the columns of $AB$:\n$$\nAB = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \n\\end{bmatrix} = \\begin{bmatrix}\n0 & 0 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n-3 & -4\n\\end{bmatrix}\n$$\nThe next two columns of $\\mathcal{C}(A,B)$ are:\n$c_3 = Ab_1 = [0, 1, 0, -3]^T$\n$c_4 = Ab_2 = [0, 0, 1, -4]^T$\n\nWe check if $c_3$ is in the span of $\\{c_1, c_2\\}$. Any vector in $\\operatorname{span}\\{c_1, c_2\\}$ has the form $[\\alpha \\cdot 0 + \\beta \\cdot 0, \\alpha \\cdot 0 + \\beta \\cdot 0, \\alpha \\cdot 1, \\beta \\cdot 1]^T = [0, 0, \\alpha, \\beta]^T$. Since $c_3$ has a non-zero second component, it is linearly independent of $c_1$ and $c_2$. We add $c_3$ to our basis, which is now $\\{c_1, c_2, c_3\\}$.\n\nNext, we check if $c_4$ is in the span of $\\{c_1, c_2, c_3\\}$. We test if there exist scalars $\\alpha, \\beta, \\gamma$ such that $c_4 = \\alpha c_1 + \\beta c_2 + \\gamma c_3$.\n$$\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -4 \\end{bmatrix} = \\alpha \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + \\beta \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} + \\gamma \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\gamma \\\\ \\alpha \\\\ \\beta - 3\\gamma \\end{bmatrix}\n$$\nComparing components gives $\\gamma=0$, $\\alpha=1$, and $\\beta-3\\gamma = -4 \\Rightarrow \\beta = -4$. This is consistent. We find $c_4 = 1 \\cdot c_1 - 4 \\cdot c_2$. So, $c_4$ is linearly dependent on the preceding columns. We must skip it.\n\nWe need a fourth linearly independent vector. We proceed to the next column of $\\mathcal{C}(A,B)$, which is the first column of $A^2 B$, i.e., $A^2 b_1 = A(Ab_1) = Ac_3$.\n$c_5 = A c_3 = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n-1 & -2 & -3 & -4\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\\n1 \\\\\n0 \\\\\n-3\n\\end{bmatrix} = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n-3 \\\\\n-2+12\n\\end{bmatrix} = \\begin{bmatrix}\n1 \\\\\n0 \\\\\n-3 \\\\\n10\n\\end{bmatrix}$\n\nWe check if $c_5$ is in the span of $\\{c_1, c_2, c_3\\}$.\n$$\n\\begin{bmatrix} 1 \\\\ 0 \\\\ -3 \\\\ 10 \\end{bmatrix} = \\alpha \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} + \\beta \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} + \\gamma \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\gamma \\\\ \\alpha \\\\ \\beta - 3\\gamma \\end{bmatrix}\n$$\nComparing the first component yields $1=0$, a contradiction. Thus, $c_5$ is linearly independent of $\\{c_1, c_2, c_3\\}$.\n\nWe have found our four linearly independent columns, selected in order by searching left-to-right: $\\{c_1, c_2, c_3, c_5\\}$. The basis matrix $T$ is formed by these columns:\n$$\nT = [c_1, c_2, c_3, c_5] = \\begin{bmatrix}\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & -3 \\\\\n0 & 1 & -3 & 10\n\\end{bmatrix}\n$$\nWe compute the determinant of $T$. We use cofactor expansion along the first row:\n$$\n\\det(T) = (-1)^{1+1} \\cdot 0 \\cdot M_{11} + (-1)^{1+2} \\cdot 0 \\cdot M_{12} + (-1)^{1+3} \\cdot 0 \\cdot M_{13} + (-1)^{1+4} \\cdot 1 \\cdot \\det \\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & -3\n\\end{pmatrix}\n$$\n$$\n\\det(T) = -1 \\cdot \\det \\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & -3\n\\end{pmatrix}\n$$\nTo compute the determinant of the $3 \\times 3$ matrix, we expand along its first row:\n$$\n\\det \\begin{pmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & -3\n\\end{pmatrix} = (-1)^{1+3} \\cdot 1 \\cdot \\det \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = 1 \\cdot (1 \\cdot 1 - 0 \\cdot 0) = 1\n$$\nSubstituting this back, we find the determinant of $T$:\n$$\n\\det(T) = -1 \\cdot 1 = -1\n$$\nThe determinant is an exact integer.", "answer": "$$\n\\boxed{-1}\n$$", "id": "2757663"}, {"introduction": "Vector space principles extend far beyond columns of numbers; they provide a powerful framework for understanding spaces of functions. This exercise [@problem_id:2757652] treats a class of rational transfer functions as a finite-dimensional vector space, a perspective crucial for advanced system analysis. You will practice finding the coordinates of a transfer function in different bases, revealing that a familiar engineering tool—partial fraction expansion—is fundamentally a change-of-basis operation.", "problem": "Consider the real vector space of single-input single-output transfer functions in the Laplace variable $s$, defined as\n$$\n\\mathcal{V} \\;=\\; \\left\\{\\, \\frac{P(s)}{Q(s)} \\;:\\; Q(s) \\,=\\, (s+1)^{2}(s+2),\\; P(s)\\in\\mathbb{R}[s],\\; \\deg P \\leq 2 \\,\\right\\}.\n$$\nBy construction, $\\mathcal{V}$ is a finite-dimensional subspace of the space of real-rational transfer functions commonly used in Linear Time-Invariant (LTI) control theory. Let $Q(s)=(s+1)^{2}(s+2)$.\n\nDefine the two ordered bases of $\\mathcal{V}$:\n- The monomial-denominator basis $\\mathcal{B}_{\\mathrm{mon}}=\\{\\, b_{1}(s),\\, b_{2}(s),\\, b_{3}(s)\\,\\}$ where\n$$\nb_{1}(s)=\\frac{1}{Q(s)},\\quad b_{2}(s)=\\frac{s}{Q(s)},\\quad b_{3}(s)=\\frac{s^{2}}{Q(s)}.\n$$\n- The partial-fraction basis $\\mathcal{B}_{\\mathrm{pf}}=\\{\\, c_{1}(s),\\, c_{2}(s),\\, c_{3}(s)\\,\\}$ where\n$$\nc_{1}(s)=\\frac{1}{s+1},\\quad c_{2}(s)=\\frac{1}{(s+1)^{2}},\\quad c_{3}(s)=\\frac{1}{s+2}.\n$$\n\nLet the transfer function $G(s)\\in\\mathcal{V}$ be\n$$\nG(s)\\;=\\;\\frac{5s^{2}-s+4}{(s+1)^{2}(s+2)}.\n$$\n\nUsing only the linear-algebraic definitions of vector space, linear independence, basis, coordinates, and change of basis:\n1) Compute the coordinate vector of $G(s)$ with respect to $\\mathcal{B}_{\\mathrm{mon}}$.\n2) Construct explicitly, from first principles, the change-of-basis map that sends coordinates in $\\mathcal{B}_{\\mathrm{mon}}$ to coordinates in $\\mathcal{B}_{\\mathrm{pf}}$, by expressing the basis elements of $\\mathcal{B}_{\\mathrm{pf}}$ in the basis $\\mathcal{B}_{\\mathrm{mon}}$ and inverting the resulting linear relation.\n3) Apply this change of basis to obtain the coordinate vector of $G(s)$ in the basis $\\mathcal{B}_{\\mathrm{pf}}$.\n\nReport only the final coordinate vector of $G(s)$ in the basis $\\mathcal{B}_{\\mathrm{pf}}$ as your answer, formatted as a row vector. No rounding is required; give exact values.", "solution": "The problem presented is a standard exercise in linear algebra, applied to a vector space of rational functions relevant to control theory. The problem is well-posed, self-contained, and scientifically sound. We shall proceed with its resolution.\n\nThe vector space is $\\mathcal{V} = \\left\\{\\, \\frac{P(s)}{Q(s)} \\;:\\; Q(s) = (s+1)^{2}(s+2),\\; P(s)\\in\\mathbb{R}[s],\\; \\deg P \\leq 2 \\,\\right\\}$. The dimension of this space is $3$, as the numerator polynomial is determined by $3$ coefficients. We are given two bases for this space:\nThe monomial-denominator basis $\\mathcal{B}_{\\mathrm{mon}}=\\{\\, b_{1}(s),\\, b_{2}(s),\\, b_{3}(s)\\,\\}$ with $b_{1}(s)=\\frac{1}{Q(s)}$, $b_{2}(s)=\\frac{s}{Q(s)}$, $b_{3}(s)=\\frac{s^{2}}{Q(s)}$.\nThe partial-fraction basis $\\mathcal{B}_{\\mathrm{pf}}=\\{\\, c_{1}(s),\\, c_{2}(s),\\, c_{3}(s)\\,\\}$ with $c_{1}(s)=\\frac{1}{s+1}$, $c_{2}(s)=\\frac{1}{(s+1)^{2}}$, $c_{3}(s)=\\frac{1}{s+2}$.\nThe vector of interest is $G(s) = \\frac{5s^{2}-s+4}{(s+1)^{2}(s+2)}$.\n\nOur objective is to find the coordinate vector of $G(s)$ with respect to the basis $\\mathcal{B}_{\\mathrm{pf}}$. We will follow the three prescribed steps.\n\nFirst, we compute the coordinate vector of $G(s)$ with respect to $\\mathcal{B}_{\\mathrm{mon}}$, denoted by $[G]_{\\mathcal{B}_{\\mathrm{mon}}}$. By definition, we must find scalars $k_{1}$, $k_{2}$, $k_{3}$ such that $G(s) = k_{1}b_{1}(s) + k_{2}b_{2}(s) + k_{3}b_{3}(s)$.\nSubstituting the definitions of $G(s)$ and the basis vectors $b_{i}(s)$:\n$$\n\\frac{5s^{2}-s+4}{Q(s)} = k_{1}\\frac{1}{Q(s)} + k_{2}\\frac{s}{Q(s)} + k_{3}\\frac{s^{2}}{Q(s)} = \\frac{k_{1} + k_{2}s + k_{3}s^{2}}{Q(s)}.\n$$\nFor these two rational functions to be equal, their numerators must be equal. By comparing the coefficients of the polynomials in the numerators, we have:\n$$\nk_{3}s^{2} + k_{2}s + k_{1} \\;=\\; 5s^{2} - 1s + 4.\n$$\nThis equality must hold for all $s$, so we identify the coefficients of like powers of $s$:\n$k_{1} = 4$, $k_{2} = -1$, $k_{3} = 5$.\nThe coordinate vector of $G(s)$ in the basis $\\mathcal{B}_{\\mathrm{mon}}$ is therefore\n$$\n[G]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 4 \\\\ -1 \\\\ 5 \\end{pmatrix}.\n$$\n\nSecond, we construct the change-of-basis matrix from $\\mathcal{B}_{\\mathrm{mon}}$ to $\\mathcal{B}_{\\mathrm{pf}}$, which we denote as $P_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}}$. The problem specifies a method: express the basis vectors of $\\mathcal{B}_{\\mathrm{pf}}$ in terms of $\\mathcal{B}_{\\mathrm{mon}}$, which gives the matrix $P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}}$, and then invert it.\nLet us write each vector $c_{i}(s)$ from $\\mathcal{B}_{\\mathrm{pf}}$ as a linear combination of the vectors from $\\mathcal{B}_{\\mathrm{mon}}$:\nFor $c_{1}(s)$:\n$$\nc_{1}(s) = \\frac{1}{s+1} = \\frac{(s+1)(s+2)}{(s+1)^{2}(s+2)} = \\frac{s^{2}+3s+2}{Q(s)} = 2\\frac{1}{Q(s)} + 3\\frac{s}{Q(s)} + 1\\frac{s^{2}}{Q(s)} = 2b_{1}(s) + 3b_{2}(s) + 1b_{3}(s).\n$$\nThe coordinate vector is $[c_{1}]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix}$.\nFor $c_{2}(s)$:\n$$\nc_{2}(s) = \\frac{1}{(s+1)^{2}} = \\frac{s+2}{(s+1)^{2}(s+2)} = \\frac{s+2}{Q(s)} = 2\\frac{1}{Q(s)} + 1\\frac{s}{Q(s)} + 0\\frac{s^{2}}{Q(s)} = 2b_{1}(s) + 1b_{2}(s) + 0b_{3}(s).\n$$\nThe coordinate vector is $[c_{2}]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nFor $c_{3}(s)$:\n$$\nc_{3}(s) = \\frac{1}{s+2} = \\frac{(s+1)^{2}}{(s+1)^{2}(s+2)} = \\frac{s^{2}+2s+1}{Q(s)} = 1\\frac{1}{Q(s)} + 2\\frac{s}{Q(s)} + 1\\frac{s^{2}}{Q(s)} = 1b_{1}(s) + 2b_{2}(s) + 1b_{3}(s).\n$$\nThe coordinate vector is $[c_{3}]_{\\mathcal{B}_{\\mathrm{mon}}} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$.\nThe change-of-basis matrix from $\\mathcal{B}_{\\mathrm{pf}}$ to $\\mathcal{B}_{\\mathrm{mon}}$ is formed by taking these coordinate vectors as its columns:\n$$\nP_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}} = \\begin{pmatrix} [c_{1}]_{\\mathcal{B}_{\\mathrm{mon}}} & [c_{2}]_{\\mathcal{B}_{\\mathrm{mon}}} & [c_{3}]_{\\mathcal{B}_{\\mathrm{mon}}} \\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 1 \\\\ 3 & 1 & 2 \\\\ 1 & 0 & 1 \\end{pmatrix}.\n$$\nThe mapping of coordinates is given by $[v]_{\\mathcal{B}_{\\mathrm{mon}}} = P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}} [v]_{\\mathcal{B}_{\\mathrm{pf}}}$. To find the map from $\\mathcal{B}_{\\mathrm{mon}}$-coordinates to $\\mathcal{B}_{\\mathrm{pf}}$-coordinates, we must compute the inverse of this matrix:\n$$\nP_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}} = (P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}})^{-1}.\n$$\nLet $A = P_{\\mathcal{B}_{\\mathrm{mon}} \\leftarrow \\mathcal{B}_{\\mathrm{pf}}}$. The determinant of $A$ is:\n$$\n\\det(A) = 2(1\\cdot 1 - 2\\cdot 0) - 2(3\\cdot 1 - 2\\cdot 1) + 1(3\\cdot 0 - 1\\cdot 1) = 2(1) - 2(1) - 1 = -1.\n$$\nSince the determinant is non-zero, the matrix is invertible. The inverse is $A^{-1} = \\frac{1}{\\det(A)}\\mathrm{adj}(A)$. The adjugate matrix, $\\mathrm{adj}(A)$, is the transpose of the cofactor matrix.\nThe cofactor matrix is:\n$$\nC = \\begin{pmatrix} +\\begin{vmatrix} 1 & 2 \\\\ 0 & 1 \\end{vmatrix} & -\\begin{vmatrix} 3 & 2 \\\\ 1 & 1 \\end{vmatrix} & +\\begin{vmatrix} 3 & 1 \\\\ 1 & 0 \\end{vmatrix} \\\\ -\\begin{vmatrix} 2 & 1 \\\\ 0 & 1 \\end{vmatrix} & +\\begin{vmatrix} 2 & 1 \\\\ 1 & 1 \\end{vmatrix} & -\\begin{vmatrix} 2 & 2 \\\\ 1 & 0 \\end{vmatrix} \\\\ +\\begin{vmatrix} 2 & 1 \\\\ 1 & 2 \\end{vmatrix} & -\\begin{vmatrix} 2 & 1 \\\\ 3 & 2 \\end{vmatrix} & +\\begin{vmatrix} 2 & 2 \\\\ 3 & 1 \\end{vmatrix} \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & -1 \\\\ -2 & 1 & 2 \\\\ 3 & -1 & -4 \\end{pmatrix}.\n$$\nThe adjugate matrix is $\\mathrm{adj}(A) = C^{T} = \\begin{pmatrix} 1 & -2 & 3 \\\\ -1 & 1 & -1 \\\\ -1 & 2 & -4 \\end{pmatrix}$.\nThe inverse matrix is therefore:\n$$\nP_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}} = \\frac{1}{-1} \\begin{pmatrix} 1 & -2 & 3 \\\\ -1 & 1 & -1 \\\\ -1 & 2 & -4 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 & -3 \\\\ 1 & -1 & 1 \\\\ 1 & -2 & 4 \\end{pmatrix}.\n$$\n\nThird, we apply this change-of-basis matrix to the coordinate vector $[G]_{\\mathcal{B}_{\\mathrm{mon}}}$ to obtain $[G]_{\\mathcal{B}_{\\mathrm{pf}}}$.\nThe transformation rule is $[G]_{\\mathcal{B}_{\\mathrm{pf}}} = P_{\\mathcal{B}_{\\mathrm{pf}} \\leftarrow \\mathcal{B}_{\\mathrm{mon}}} [G]_{\\mathcal{B}_{\\mathrm{mon}}}$.\n$$\n[G]_{\\mathcal{B}_{\\mathrm{pf}}} = \\begin{pmatrix} -1 & 2 & -3 \\\\ 1 & -1 & 1 \\\\ 1 & -2 & 4 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -1 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} (-1)(4) + (2)(-1) + (-3)(5) \\\\ (1)(4) + (-1)(-1) + (1)(5) \\\\ (1)(4) + (-2)(-1) + (4)(5) \\end{pmatrix} = \\begin{pmatrix} -4 - 2 - 15 \\\\ 4 + 1 + 5 \\\\ 4 + 2 + 20 \\end{pmatrix} = \\begin{pmatrix} -21 \\\\ 10 \\\\ 26 \\end{pmatrix}.\n$$\nThus, the coordinate vector of $G(s)$ with respect to the basis $\\mathcal{B}_{\\mathrm{pf}}$ is $\\begin{pmatrix} -21 \\\\ 10 \\\\ 26 \\end{pmatrix}$. This corresponds to the partial fraction expansion $G(s) = \\frac{-21}{s+1} + \\frac{10}{(s+1)^{2}} + \\frac{26}{s+2}$.\n\nThe final answer, formatted as a row vector as requested, is the transpose of this column vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} -21 & 10 & 26 \\end{pmatrix}}\n$$", "id": "2757652"}, {"introduction": "While any basis can define a coordinate system, orthonormal bases offer significant computational and analytical advantages. This practice [@problem_id:2757650] focuses on the constructive process of building such a basis using the Gram-Schmidt algorithm. By applying this procedure to the columns of a controllability matrix—vectors that are often nearly collinear—you will gain first-hand insight into the numerical challenges of orthogonalization and the importance of numerical stability in real-world applications.", "problem": "Consider the single-input linear time-invariant state-space model in controllable canonical form with state matrix $A \\in \\mathbb{R}^{3 \\times 3}$ and input matrix $B \\in \\mathbb{R}^{3 \\times 1}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-6 & -11 & -6\n\\end{pmatrix}, \n\\qquad\nB \\;=\\; \\begin{pmatrix}\n0 \\\\\n0 \\\\\n1\n\\end{pmatrix}.\n$$\nUse the standard Euclidean inner product on $\\mathbb{R}^{3}$, defined by $\\langle x,y\\rangle \\,=\\, x^{\\top} y$ for $x,y \\in \\mathbb{R}^{3}$. Form the controllability matrix $C \\in \\mathbb{R}^{3 \\times 3}$ as the ordered Krylov sequence\n$$\nC \\;=\\; \\big[\\, B,\\, AB,\\, A^{2}B \\,\\big].\n$$\nApply the classical Gram–Schmidt orthonormalization procedure to the columns of $C$ in this given order with respect to the standard inner product, producing an orthonormal basis $\\{q_{1},q_{2},q_{3}\\}$ (assembled as the matrix $Q \\in \\mathbb{R}^{3 \\times 3}$ with columns $q_{1},q_{2},q_{3}$) and an upper triangular matrix $R \\in \\mathbb{R}^{3 \\times 3}$ with positive diagonal entries such that $C \\,=\\, Q R$. In your solution, clearly identify each projection, each intermediate vector prior to normalization, and the norms used at every step, starting from the definitions of the controllability matrix and the Gram–Schmidt algorithm. Then, discuss the numerical conditioning of the classical Gram–Schmidt process in this controllability/Krylov setting, including how orthogonality loss can arise and when modified Gram–Schmidt or Householder reflections are preferable.\n\nWhat is the exact value of the scalar $r_{33}$, the $(3,3)$-entry of $R$? Provide your answer as an exact value; no rounding is required.", "solution": "The problem posed is valid. It is a well-defined exercise in linear algebra and control theory, grounded in established principles. It provides all necessary data—the state matrix $A$, the input matrix $B$, the definition of the controllability matrix $C$, and the algorithm to be used (classical Gram–Schmidt)—to arrive at a unique, verifiable solution. The problem is objective, free of ambiguity, and does not violate any scientific or mathematical laws. Therefore, a solution will be furnished.\n\nThe first step is to construct the controllability matrix $C$ from the given state-space model. The columns of $C$, denoted $c_1, c_2, c_3$, form a Krylov sequence.\nThe matrices are given as:\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n-6 & -11 & -6\n\\end{pmatrix}, \n\\qquad\nB = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 1\n\\end{pmatrix}.\n$$\nThe columns of the controllability matrix $C = [\\ c_1 \\ c_2 \\ c_3 \\ ]$ are defined as $c_1 = B$, $c_2 = AB$, and $c_3 = A^2B$.\n\nFirst column:\n$$\nc_1 = B = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nSecond column:\n$$\nc_2 = AB = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -6 & -11 & -6 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -6 \\end{pmatrix}.\n$$\nThird column:\n$$\nc_3 = A^2B = A(AB) = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -6 & -11 & -6 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -6 \\\\ 0 \\cdot (-6) + 1 \\cdot (-11) + (-6) \\cdot (-6) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -6 \\\\ -11 + 36 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -6 \\\\ 25 \\end{pmatrix}.\n$$\nThus, the controllability matrix is:\n$$\nC = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & -6 \\\\ 1 & -6 & 25 \\end{pmatrix}.\n$$\nNext, we apply the classical Gram–Schmidt orthonormalization procedure to the columns $c_1, c_2, c_3$ of $C$. The process generates an orthonormal basis $\\{q_1, q_2, q_3\\}$ and an upper triangular matrix $R = (r_{ij})$ such that $C=QR$. The diagonal entries $r_{kk}$ are required to be positive.\n\nStep 1: Orthonormalize $c_1$.\nThe first intermediate vector before normalization, $v_1$, is simply $c_1$:\n$$\nv_1 = c_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nThe corresponding diagonal entry of $R$ is the Euclidean norm of $v_1$:\n$$\nr_{11} = \\|v_1\\| = \\sqrt{0^2 + 0^2 + 1^2} = 1.\n$$\nSince $r_{11}>0$ is required, this is correct. The first orthonormal vector $q_1$ is:\n$$\nq_1 = \\frac{v_1}{\\|v_1\\|} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nStep 2: Orthonormalize $c_2$.\nWe find the component of $c_2$ that is orthogonal to the subspace spanned by $\\{q_1\\}$. The projection of $c_2$ onto $q_1$ is given by $\\text{proj}_{q_1}(c_2) = \\langle q_1, c_2 \\rangle q_1$. The coefficient $\\langle q_1, c_2 \\rangle$ is the entry $r_{12}$ of the matrix $R$:\n$$\nr_{12} = \\langle q_1, c_2 \\rangle = q_1^\\top c_2 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -6 \\end{pmatrix} = -6.\n$$\nThe intermediate vector $v_2$ is obtained by subtracting this projection from $c_2$:\n$$\nv_2 = c_2 - r_{12} q_1 = \\begin{pmatrix} 0 \\\\ 1 \\\\ -6 \\end{pmatrix} - (-6) \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -6 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nThe diagonal entry $r_{22}$ is the norm of $v_2$:\n$$\nr_{22} = \\|v_2\\| = \\sqrt{0^2 + 1^2 + 0^2} = 1.\n$$\nThe second orthonormal vector $q_2$ is:\n$$\nq_2 = \\frac{v_2}{\\|v_2\\|} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\n\nStep 3: Orthonormalize $c_3$.\nWe seek the component of $c_3$ that is orthogonal to the subspace spanned by $\\{q_1, q_2\\}$. This requires subtracting the projections of $c_3$ onto both $q_1$ and $q_2$.\nThe projection of $c_3$ onto $q_1$ is $\\text{proj}_{q_1}(c_3) = \\langle q_1, c_3 \\rangle q_1$. The coefficient is $r_{13}$:\n$$\nr_{13} = \\langle q_1, c_3 \\rangle = q_1^\\top c_3 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -6 \\\\ 25 \\end{pmatrix} = 25.\n$$\nThe projection of $c_3$ onto $q_2$ is $\\text{proj}_{q_2}(c_3) = \\langle q_2, c_3 \\rangle q_2$. The coefficient is $r_{23}$:\n$$\nr_{23} = \\langle q_2, c_3 \\rangle = q_2^\\top c_3 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -6 \\\\ 25 \\end{pmatrix} = -6.\n$$\nThe intermediate vector $v_3$ is obtained by subtracting these projections from $c_3$:\n$$\nv_3 = c_3 - r_{13} q_1 - r_{23} q_2 = \\begin{pmatrix} 1 \\\\ -6 \\\\ 25 \\end{pmatrix} - (25) \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} - (-6) \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\nv_3 = \\begin{pmatrix} 1 \\\\ -6 \\\\ 25 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 25 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 6 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe final value we seek is $r_{33}$, which is the norm of this intermediate vector $v_3$:\n$$\nr_{33} = \\|v_3\\| = \\sqrt{1^2 + 0^2 + 0^2} = 1.\n$$\n\nThe question is answered. However, for completeness, we can determine $q_3$:\n$$\nq_3 = \\frac{v_3}{\\|v_3\\|} = \\frac{1}{1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n\nNow for the required discussion on numerical conditioning. The columns of the controllability matrix, forming a Krylov sequence $[B, AB, \\dots, A^{n-1}B]$, often become nearly linearly dependent as the dimension increases. This is because successive applications of the matrix $A$ tend to align the vectors with the direction of the dominant eigenvector of $A$, a principle exploited by the power iteration method. Consequently, the controllability matrix $C$ is frequently ill-conditioned, meaning its columns form a basis that is \"almost\" linearly dependent.\n\nWhen an ill-conditioned set of vectors is subjected to the classical Gram–Schmidt (CGS) procedure, severe numerical instability arises. The core operation in CGS is the subtraction of projections: $v_k = c_k - \\sum_{j=1}^{k-1} \\langle q_j, c_k \\rangle q_j$. If $c_k$ is almost a linear combination of the preceding vectors $c_1, \\dots, c_{k-1}$ (and thus of $q_1, \\dots, q_{k-1}$), then $v_k$ will be the result of subtracting a vector that is very close to $c_k$ itself. This leads to catastrophic cancellation, where the most significant digits are lost, resulting in a large relative error in the computed $v_k$. This error propagation leads to a severe loss of orthogonality in the computed basis $\\{q_j\\}$; that is, $q_i^\\top q_j$ for $i \\neq j$ will be far from machine zero.\n\nIn contrast, the Modified Gram–Schmidt (MGS) algorithm is numerically more stable, although mathematically equivalent in exact arithmetic. MGS reorders the subtractions. At each step $k$, it orthogonalizes the current vector against all previously computed basis vectors. Then, critically, it applies the same orthogonalization to all *remaining* vectors in the set. This repeated removal of components prevents the reintroduction of errors and maintains orthogonality much more effectively.\n\nFor maximum numerical stability, especially for ill-conditioned matrices like Krylov matrices, Householder reflections are superior. This method computes the QR factorization by applying a sequence of orthogonal reflection matrices to zero out the subdiagonal entries of the matrix $C$. Since the product of orthogonal matrices is perfectly orthogonal (up to machine precision), the resulting matrix $Q$ maintains orthogonality far better than either CGS or MGS. For constructing orthonormal bases for Krylov subspaces, robust algorithms like the Arnoldi iteration, which is based on the MGS procedure, are standard practice in numerical linear algebra precisely to avoid the instability of CGS.", "answer": "$$\n\\boxed{1}\n$$", "id": "2757650"}]}