{"hands_on_practices": [{"introduction": "Understanding a principle often begins with verifying it from its most fundamental definition. This practice guides you through a direct, hands-on verification of superposition for a discrete-time Linear Time-Invariant (LTI) system. By first calculating the system's response to a composite input using the convolution sum, and then comparing it to the sum of responses from individual inputs, you will solidify your grasp of how linearity guarantees this powerful decomposition property [@problem_id:2733502].", "problem": "Consider a causal discrete-time linear time-invariant (LTI) system with impulse response specified by $h[0]=1$, $h[1]=\\alpha$, $h[2]=\\beta$, and $h[k]=0$ for all $k\\ge 3$, where $\\alpha\\in\\mathbb{R}$ and $\\beta\\in\\mathbb{R}$ are fixed constants. Let the input be $u[k]=a^{k}+b^{k}$ for $k\\ge 0$ and $u[k]=0$ for $k0$, where $a\\in\\mathbb{R}$ and $b\\in\\mathbb{R}$ are arbitrary parameters. Assume zero initial conditions and adopt the convention that $a^{0}=1$ and $b^{0}=1$.\n\nUsing only the fundamental definitions of linearity and the convolution sum for discrete-time LTI systems, derive from first principles a closed-form expression for the zero-state output $y[k]$ for all $k\\in\\mathbb{Z}$. Then, by computing the individual outputs to $a^{k}$ and $b^{k}$ separately and invoking the superposition principle, verify that the total output equals the sum of the two individual responses. Your final result should be a single explicit expression for $y[k]$ valid for all integer $k$ (you may present it as a piecewise function of $k$). No numerical evaluation or rounding is required.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extracted Givens**\n- System description: Causal discrete-time linear time-invariant (LTI) system.\n- Impulse response: $h[0]=1$, $h[1]=\\alpha$, $h[2]=\\beta$. $h[k]=0$ for all $k \\ge 3$ and $k  0$.\n- Input signal: $u[k]=a^{k}+b^{k}$ for $k\\ge 0$ and $u[k]=0$ for $k0$.\n- Parameters: $\\alpha, \\beta, a, b$ are real constants ($\\mathbb{R}$).\n- Conditions: Zero initial conditions (zero-state response is required). Convention: $a^0=1$ and $b^0=1$.\n- Task:\n    1.  Derive a closed-form expression for the output $y[k]$ for all $k \\in \\mathbb{Z}$ using the convolution sum.\n    2.  Verify the result by applying the superposition principle to the individual inputs $a^k$ and $b^k$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is analyzed against the required criteria.\n- **Scientifically Grounded and Objective**: The problem is formulated using standard, universally accepted concepts from discrete-time linear systems theory, namely convolution and superposition. All terms are defined unambiguously within this context. The premises are mathematically and scientifically sound.\n- **Well-Posed and Complete**: The problem is self-contained. The system's impulse response and the input signal are fully specified. The request for a zero-state response given zero initial conditions is standard. The provided information is necessary and sufficient to determine a unique solution for the output $y[k]$. The convention for $a^0$ and $b^0$ removes any potential ambiguity for the case where $a=0$ or $b=0$.\n- **Not Trivial or Ill-Posed**: The problem requires a methodical application of the convolution definition and careful handling of indices for different time intervals. It is a standard exercise designed to test fundamental understanding, not a triviality or a tautology.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. It is a well-defined exercise in signals and systems analysis. We will proceed with a rigorous derivation.\n\n**Derivation of the System Output**\n\nThe output $y[k]$ of a discrete-time LTI system is given by the convolution sum of the input $u[k]$ and the impulse response $h[k]$:\n$$y[k] = u[k] * h[k] = \\sum_{n=-\\infty}^{\\infty} h[n] u[k-n]$$\nThe impulse response $h[k]$ is non-zero only for $n \\in \\{0, 1, 2\\}$. Therefore, the sum simplifies to:\n$$y[k] = h[0]u[k-0] + h[1]u[k-1] + h[2]u[k-2]$$\nSubstituting the given values $h[0]=1$, $h[1]=\\alpha$, and $h[2]=\\beta$:\n$$y[k] = u[k] + \\alpha u[k-1] + \\beta u[k-2]$$\nThe input signal is given by $u[k] = (a^k + b^k)\\sigma[k]$, where $\\sigma[k]$ is the discrete-time unit step function ($\\sigma[k]=1$ for $k \\ge 0$ and $\\sigma[k]=0$ for $k  0$). We evaluate $y[k]$ for different ranges of the integer index $k$.\n\nCase 1: $k  0$\nFor $k0$, the indices $k$, $k-1$, and $k-2$ are all negative. Thus, $u[k]=0$, $u[k-1]=0$, and $u[k-2]=0$.\n$$y[k] = 0 + \\alpha(0) + \\beta(0) = 0$$\n\nCase 2: $k=0$\nFor $k=0$, the indices are $0$, $-1$, and $-2$.\n$u[0] = a^0 + b^0 = 1 + 1 = 2$.\n$u[-1] = 0$ and $u[-2] = 0$.\n$$y[0] = u[0] + \\alpha u[-1] + \\beta u[-2] = 2 + \\alpha(0) + \\beta(0) = 2$$\n\nCase 3: $k=1$\nFor $k=1$, the indices are $1$, $0$, and $-1$.\n$u[1] = a^1 + b^1 = a+b$.\n$u[0] = 2$.\n$u[-1] = 0$.\n$$y[1] = u[1] + \\alpha u[0] + \\beta u[-1] = (a+b) + \\alpha(2) + \\beta(0) = a+b+2\\alpha$$\n\nCase 4: $k \\ge 2$\nFor $k \\ge 2$, the indices $k$, $k-1$, and $k-2$ are all non-negative. Therefore, $\\sigma[k]=\\sigma[k-1]=\\sigma[k-2]=1$.\n$u[k] = a^k+b^k$\n$u[k-1] = a^{k-1}+b^{k-1}$\n$u[k-2] = a^{k-2}+b^{k-2}$\n$$y[k] = (a^k+b^k) + \\alpha(a^{k-1}+b^{k-1}) + \\beta(a^{k-2}+b^{k-2})$$\nThis expression can be rearranged by grouping terms with $a$ and $b$:\n$$y[k] = (a^k + \\alpha a^{k-1} + \\beta a^{k-2}) + (b^k + \\alpha b^{k-1} + \\beta b^{k-2})$$\n$$y[k] = a^{k-2}(a^2+\\alpha a+\\beta) + b^{k-2}(b^2+\\alpha b+\\beta)$$\n\nThis concludes the direct calculation of the output $y[k]$.\n\n**Verification via Superposition Principle**\n\nThe principle of superposition states that for an LTI system, if the input is $u[k] = c_1 u_1[k] + c_2 u_2[k]$, the output is $y[k] = c_1 y_1[k] + c_2 y_2[k]$. In our case, the input is $u[k] = u_a[k] + u_b[k]$, where $u_a[k] = a^k\\sigma[k]$ and $u_b[k] = b^k\\sigma[k]$. We calculate the responses $y_a[k]$ and $y_b[k]$ separately.\n\nResponse to $u_a[k] = a^k\\sigma[k]$:\nLet $y_a[k]$ be the output for the input $u_a[k]$.\n$$y_a[k] = u_a[k] + \\alpha u_a[k-1] + \\beta u_a[k-2]$$\n- For $k0$: $y_a[k]=0$.\n- For $k=0$: $y_a[0] = u_a[0] = a^0 = 1$.\n- For $k=1$: $y_a[1] = u_a[1] + \\alpha u_a[0] = a + \\alpha(1) = a+\\alpha$.\n- For $k\\ge2$: $y_a[k] = a^k + \\alpha a^{k-1} + \\beta a^{k-2} = a^{k-2}(a^2+\\alpha a+\\beta)$.\n\nResponse to $u_b[k] = b^k\\sigma[k]$:\nBy symmetry, the calculation for $y_b[k]$ is identical to that for $y_a[k]$, with $a$ replaced by $b$.\n- For $k0$: $y_b[k]=0$.\n- For $k=0$: $y_b[0] = b^0 = 1$.\n- For $k=1$: $y_b[1] = b + \\alpha(1) = b+\\alpha$.\n- For $k\\ge2$: $y_b[k] = b^k + \\alpha b^{k-1} + \\beta b^{k-2} = b^{k-2}(b^2+\\alpha b+\\beta)$.\n\nNow, we sum the individual responses to verify that $y_a[k] + y_b[k] = y[k]$.\n- For $k0$: $y_a[k]+y_b[k] = 0+0=0$, which matches $y[k]$.\n- For $k=0$: $y_a[0]+y_b[0] = 1+1=2$, which matches $y[0]$.\n- For $k=1$: $y_a[1]+y_b[1] = (a+\\alpha)+(b+\\alpha) = a+b+2\\alpha$, which matches $y[1]$.\n- For $k\\ge2$: $y_a[k]+y_b[k] = a^{k-2}(a^2+\\alpha a+\\beta) + b^{k-2}(b^2+\\alpha b+\\beta)$, which matches the expression for $y[k]$ derived previously.\n\nThe verification is complete and successful. The superposition principle is shown to hold. The total output is the sum of the responses to the individual exponential components of the input.\n\nThe final expression for the output $y[k]$ is a piecewise function.", "answer": "$$\n\\boxed{\ny[k] = \n\\begin{cases}\n0  \\text{for } k  0 \\\\\n2  \\text{for } k=0 \\\\\na+b+2\\alpha  \\text{for } k=1 \\\\\na^{k-2}(a^2+\\alpha a+\\beta) + b^{k-2}(b^2+\\alpha b+\\beta)  \\text{for } k \\ge 2\n\\end{cases}\n}\n$$", "id": "2733502"}, {"introduction": "Moving from direct time-domain convolution to state-space representations provides a powerful, structured way to analyze complex systems. This exercise demonstrates how the principle of superposition applies within the matrix framework of a multiple-input LTI system. You will derive how the total output can be seen as a weighted sum of transfer functions corresponding to each input channel, a cornerstone concept for designing and analyzing multiple-input, multiple-output (MIMO) control systems [@problem_id:2733532].", "problem": "A Linear Time-Invariant (LTI) state-space system with two inputs and one output is described by the equations\n$$\\dot{x}(t)=A\\,x(t)+B\\,u(t), \\quad y(t)=C\\,x(t)+D\\,u(t),$$\nwith zero initial condition $x(0)=0$. Here $x(t)\\in\\mathbb{R}^{n}$ is the state, $u(t)\\in\\mathbb{R}^{m}$ is the input, and $y(t)\\in\\mathbb{R}$ is the output. Let $B=[\\,b_{1}\\ \\cdots\\ b_{m}\\,]$ and $D=[\\,d_{1}\\ \\cdots\\ d_{m}\\,]$ denote the columns of the input and feedthrough matrices. Using only the linearity of the state-space equations and the linearity of the unilateral Laplace transform, derive an expression that shows the output in the Laplace domain as a superposition of contributions associated with the columns of $B$ (and $D$), when the input is decomposed as $u(t)=\\sum_{j=1}^{m}e_{j}\\,u_{j}(t)$ where $e_{j}$ is the $j$-th standard basis vector and $u_{j}(t)$ are scalar input components.\n\nThen, specialize your result to the strictly proper, two-input, one-output case with\n$$A=\\begin{pmatrix}-1  2\\\\ -3  -4\\end{pmatrix},\\quad B=\\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix},\\quad C=\\begin{pmatrix}1  1\\end{pmatrix},\\quad D=\\begin{pmatrix}0  0\\end{pmatrix},$$\nunder zero initial condition. Suppose the inputs are\n$$u_{1}(t)=3\\,\\exp(-t)\\,u(t),\\qquad u_{2}(t)=2\\,u(t),$$\nwhere $u(t)$ is the unit step. Using your superposition expression, compute the Laplace-domain output $Y(s)$ and express your final answer as a single rational function of $s$. No rounding is required, and no units are needed. The final answer must be a single closed-form analytic expression.", "solution": "The problem statement is scientifically grounded, well-posed, and contains all necessary information to derive a unique and meaningful solution. It is therefore deemed valid. We shall proceed with the derivation and subsequent calculation.\n\nFirst, we derive the general expression for the output in the Laplace domain. The system is described by the linear state-space equations:\n$$ \\dot{x}(t) = A x(t) + B u(t) $$\n$$ y(t) = C x(t) + D u(t) $$\nThe initial condition is specified as $x(0)=0$. The unilateral Laplace transform is a linear operator. Applying it to the state and output equations yields:\n$$ \\mathcal{L}\\{\\dot{x}(t)\\} = sX(s) - x(0) = sX(s) = \\mathcal{L}\\{A x(t) + B u(t)\\} = A X(s) + B U(s) $$\n$$ Y(s) = \\mathcal{L}\\{y(t)\\} = \\mathcal{L}\\{C x(t) + D u(t)\\} = C X(s) + D U(s) $$\nFrom the transformed state equation, we solve for the state vector $X(s)$:\n$$ sX(s) - A X(s) = B U(s) $$\n$$ (sI - A) X(s) = B U(s) $$\n$$ X(s) = (sI - A)^{-1} B U(s) $$\nwhere $I$ is the identity matrix of appropriate dimension, and we assume that $s$ is not an eigenvalue of $A$ such that $(sI - A)^{-1}$ exists. Substituting this expression for $X(s)$ into the transformed output equation gives:\n$$ Y(s) = C(sI - A)^{-1} B U(s) + D U(s) $$\n$$ Y(s) = [C(sI - A)^{-1} B + D] U(s) $$\nThe input vector $u(t)$ is decomposed as $u(t) = \\sum_{j=1}^{m} e_{j} u_{j}(t)$, where $e_{j}$ is the $j$-th standard basis vector in $\\mathbb{R}^{m}$ and $u_{j}(t)$ are the scalar input components. Due to the linearity of the Laplace transform:\n$$ U(s) = \\mathcal{L}\\left\\{\\sum_{j=1}^{m} e_{j} u_{j}(t)\\right\\} = \\sum_{j=1}^{m} e_{j} \\mathcal{L}\\{u_{j}(t)\\} = \\sum_{j=1}^{m} e_{j} U_{j}(s) $$\nwhere $U_{j}(s) = \\mathcal{L}\\{u_{j}(t)\\}$. Substituting this into the expression for $Y(s)$:\n$$ Y(s) = [C(sI - A)^{-1} B + D] \\left( \\sum_{j=1}^{m} e_{j} U_{j}(s) \\right) $$\nBy the linearity of matrix multiplication, we can distribute the sum:\n$$ Y(s) = \\sum_{j=1}^{m} [C(sI - A)^{-1} B + D] e_{j} U_{j}(s) $$\nLet the input matrix $B$ be represented by its columns $b_j$, so $B = [b_1 \\ b_2 \\ \\cdots \\ b_m]$, and similarly for the feedthrough matrix $D = [d_1 \\ d_2 \\ \\cdots \\ d_m]$. The product $B e_{j}$ isolates the $j$-th column of $B$, i.e., $B e_{j} = b_{j}$. Likewise, $D e_{j} = d_{j}$. Therefore, the expression becomes:\n$$ Y(s) = \\sum_{j=1}^{m} \\left( C(sI - A)^{-1} (B e_{j}) + (D e_{j}) \\right) U_{j}(s) $$\n$$ Y(s) = \\sum_{j=1}^{m} \\left( C(sI - A)^{-1} b_{j} + d_{j} \\right) U_{j}(s) $$\nThis expression demonstrates that the total output $Y(s)$ is a linear superposition of the responses to each scalar input component $U_{j}(s)$. The term $G_j(s) = C(sI - A)^{-1} b_{j} + d_{j}$ is the transfer function from the $j$-th input $u_j(t)$ to the output $y(t)$. This completes the general derivation.\n\nNext, we specialize this result to the given two-input, one-output system. The matrices are:\n$$ A=\\begin{pmatrix}-1  2\\\\ -3  -4\\end{pmatrix},\\quad B=\\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix},\\quad C=\\begin{pmatrix}1  1\\end{pmatrix},\\quad D=\\begin{pmatrix}0  0\\end{pmatrix} $$\nThe system is strictly proper as $D$ is the zero matrix. The columns of $B$ are $b_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $b_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The columns of $D$ are $d_1 = 0$ and $d_2 = 0$.\nThe superposition formula for this $m=2$ case is:\n$$ Y(s) = \\left( C(sI - A)^{-1} b_{1} \\right) U_{1}(s) + \\left( C(sI - A)^{-1} b_{2} \\right) U_{2}(s) $$\nFirst, we must compute the inverse of $(sI - A)$:\n$$ sI - A = s\\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\begin{pmatrix}-1  2\\\\ -3  -4\\end{pmatrix} = \\begin{pmatrix}s+1  -2\\\\ 3  s+4\\end{pmatrix} $$\nThe determinant is $\\det(sI - A) = (s+1)(s+4) - (-2)(3) = s^2 + 5s + 4 + 6 = s^2 + 5s + 10$.\nThe inverse is:\n$$ (sI - A)^{-1} = \\frac{1}{s^2 + 5s + 10} \\begin{pmatrix}s+4  2\\\\ -3  s+1\\end{pmatrix} $$\nNow we find the individual transfer functions $G_1(s) = C(sI-A)^{-1}b_1$ and $G_2(s) = C(sI-A)^{-1}b_2$:\n$$ G_1(s) = \\frac{1}{s^2 + 5s + 10} \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}s+4  2\\\\ -3  s+1\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = \\frac{1}{s^2 + 5s + 10} \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}s+4\\\\-3\\end{pmatrix} = \\frac{s+4-3}{s^2 + 5s + 10} = \\frac{s+1}{s^2 + 5s + 10} $$\n$$ G_2(s) = \\frac{1}{s^2 + 5s + 10} \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}s+4  2\\\\ -3  s+1\\end{pmatrix} \\begin{pmatrix}0\\\\1\\end{pmatrix} = \\frac{1}{s^2 + 5s + 10} \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}2\\\\s+1\\end{pmatrix} = \\frac{2+s+1}{s^2 + 5s + 10} = \\frac{s+3}{s^2 + 5s + 10} $$\nThe inputs are given as $u_{1}(t) = 3 \\exp(-t) u(t)$ and $u_{2}(t) = 2 u(t)$, where $u(t)$ is the unit step function. Their Laplace transforms are:\n$$ U_{1}(s) = \\mathcal{L}\\{3 \\exp(-t) u(t)\\} = 3 \\frac{1}{s+1} $$\n$$ U_{2}(s) = \\mathcal{L}\\{2 u(t)\\} = 2 \\frac{1}{s} = \\frac{2}{s} $$\nWe now assemble the expression for $Y(s)$:\n$$ Y(s) = G_1(s) U_1(s) + G_2(s) U_2(s) = \\left(\\frac{s+1}{s^2 + 5s + 10}\\right) \\left(\\frac{3}{s+1}\\right) + \\left(\\frac{s+3}{s^2 + 5s + 10}\\right) \\left(\\frac{2}{s}\\right) $$\nThe term $(s+1)$ cancels in the first part:\n$$ Y(s) = \\frac{3}{s^2 + 5s + 10} + \\frac{2(s+3)}{s(s^2 + 5s + 10)} $$\nTo express this as a single rational function, we find a common denominator, which is $s(s^2 + 5s + 10)$:\n$$ Y(s) = \\frac{3s}{s(s^2 + 5s + 10)} + \\frac{2(s+3)}{s(s^2 + 5s + 10)} $$\n$$ Y(s) = \\frac{3s + 2(s+3)}{s(s^2 + 5s + 10)} = \\frac{3s + 2s + 6}{s(s^2 + 5s + 10)} $$\n$$ Y(s) = \\frac{5s + 6}{s(s^2 + 5s + 10)} $$\nThis is the final expression for the output $Y(s)$ as a single rational function.", "answer": "$$\n\\boxed{\\frac{5s+6}{s(s^2 + 5s + 10)}}\n$$", "id": "2733532"}, {"introduction": "A deep understanding of any scientific principle requires knowing its limits. This practice challenges you to dissect the relationship between linearity and time-invariance, proving that superposition is a direct consequence of linearity alone. By constructing counterexamples and calculating a 'superposition residual' for a nonlinear system, you will gain a concrete, quantitative insight into why and how the principle fails when its core requirement is not met [@problem_id:2733503].", "problem": "Let $\\mathcal{X}$ denote the set of all real-valued signals $x:\\mathbb{R}\\to\\mathbb{R}$ for which the expressions below are well-defined pointwise. A system is a mapping $\\mathcal{S}:\\mathcal{X}\\to\\mathcal{X}$. The following foundational operator properties will be used:\n- Linearity: $\\mathcal{S}$ is linear if for all $x_{1},x_{2}\\in\\mathcal{X}$ and all scalars $\\alpha,\\beta\\in\\mathbb{R}$, $\\mathcal{S}[\\alpha x_{1}+\\beta x_{2}]=\\alpha \\mathcal{S}[x_{1}]+\\beta \\mathcal{S}[x_{2}]$.\n- Time invariance: $\\mathcal{S}$ is time-invariant if for all $x\\in\\mathcal{X}$, all shifts $\\tau\\in\\mathbb{R}$, and all $t\\in\\mathbb{R}$, $\\mathcal{S}[x(\\cdot-\\tau)](t)=\\mathcal{S}[x](t-\\tau)$.\n- Superposition principle: the response of a linear system to a linear combination of inputs is the same linear combination of the corresponding individual responses.\n\nConsider the following two system operators on $\\mathcal{X}$:\n- $\\mathcal{S}_{1}[x](t) = (2+\\cos t)\\,x(t)$.\n- $\\mathcal{S}_{2}[x](t) = \\big(x(t)\\big)^{2}$.\n\nUsing only the above foundational definitions, complete the following tasks:\n1. Prove that $\\mathcal{S}_{1}$ is linear but time-varying, and therefore satisfies the superposition principle while not being time-invariant.\n2. Prove that $\\mathcal{S}_{2}$ is time-invariant but nonlinear, and therefore violates the superposition principle despite being time-invariant.\n3. Let $u(t)$ denote the unit step function with $u(t)=1$ for $t0$ and $u(t)=0$ for $t0$. Define the test signals $x_{1}(t)=\\exp(-t)\\,u(t)$ and $x_{2}(t)=\\cos t$, and the scalars $\\alpha=3$ and $\\beta=-2$. Define the superposition residuals\n$$\nr_{1}(t)\\coloneqq \\mathcal{S}_{1}[\\alpha x_{1}+\\beta x_{2}](t)-\\big(\\alpha\\,\\mathcal{S}_{1}[x_{1}](t)+\\beta\\,\\mathcal{S}_{1}[x_{2}](t)\\big),\n$$\n$$\nr_{2}(t)\\coloneqq \\mathcal{S}_{2}[\\alpha x_{1}+\\beta x_{2}](t)-\\big(\\alpha\\,\\mathcal{S}_{2}[x_{1}](t)+\\beta\\,\\mathcal{S}_{2}[x_{2}](t)\\big).\n$$\nCompute $r_{1}(1)$ and $r_{2}(1)$ explicitly. Use radian measure for trigonometric arguments. Report the two values in the order $\\big(r_{1}(1),\\,r_{2}(1)\\big)$. No rounding is required; give an exact expression.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It consists of standard exercises in the analysis of linear and nonlinear systems, based on fundamental, universally accepted definitions in control theory and signal processing. All conditions and parameters are provided, and the tasks are unambiguous. Thus, the problem is valid, and a solution will be provided.\n\nThe analysis will proceed according to the three tasks outlined.\n\nTask $1$: Analyze the properties of the system $\\mathcal{S}_{1}[x](t) = (2+\\cos t)\\,x(t)$.\n\nTo prove linearity, we must verify that for any scalars $\\alpha, \\beta \\in \\mathbb{R}$ and any signals $x_{1}, x_{2} \\in \\mathcal{X}$, the condition $\\mathcal{S}_{1}[\\alpha x_{1}+\\beta x_{2}]=\\alpha \\mathcal{S}_{1}[x_{1}]+\\beta \\mathcal{S}_{1}[x_{2}]$ holds.\nLet us evaluate the left-hand side of the linearity equation:\n$$\n\\mathcal{S}_{1}[\\alpha x_{1}+\\beta x_{2}](t) = (2+\\cos t)[\\alpha x_{1}(t)+\\beta x_{2}(t)]\n$$\nBy the distributive property of multiplication over addition in $\\mathbb{R}$:\n$$\n\\mathcal{S}_{1}[\\alpha x_{1}+\\beta x_{2}](t) = \\alpha(2+\\cos t)x_{1}(t) + \\beta(2+\\cos t)x_{2}(t)\n$$\nRecognizing the definition of $\\mathcal{S}_{1}$, this becomes:\n$$\n\\mathcal{S}_{1}[\\alpha x_{1}+\\beta x_{2}](t) = \\alpha\\,\\mathcal{S}_{1}[x_{1}](t) + \\beta\\,\\mathcal{S}_{1}[x_{2}](t)\n$$\nThis equality holds for all $t \\in \\mathbb{R}$, $x_{1}, x_{2} \\in \\mathcal{X}$, and $\\alpha, \\beta \\in \\mathbb{R}$. Therefore, the system $\\mathcal{S}_{1}$ is linear. The superposition principle is defined as the property of a linear system, so $\\mathcal{S}_{1}$ satisfies the superposition principle.\n\nTo test for time invariance, we must compare the system's response to a shifted input, $\\mathcal{S}_{1}[x(\\cdot-\\tau)](t)$, with the shifted response of the system, $\\mathcal{S}_{1}[x](t-\\tau)$, for an arbitrary time shift $\\tau \\in \\mathbb{R}$.\nThe response to a shifted input is:\n$$\n\\mathcal{S}_{1}[x(\\cdot-\\tau)](t) = (2+\\cos t)x(t-\\tau)\n$$\nThe shifted response is found by replacing $t$ with $t-\\tau$ in the original system output expression:\n$$\n\\mathcal{S}_{1}[x](t-\\tau) = (2+\\cos(t-\\tau))x(t-\\tau)\n$$\nFor the system to be time-invariant, we must have $\\mathcal{S}_{1}[x(\\cdot-\\tau)](t) = \\mathcal{S}_{1}[x](t-\\tau)$ for all $t$ and $\\tau$. This requires:\n$$\n(2+\\cos t)x(t-\\tau) = (2+\\cos(t-\\tau))x(t-\\tau)\n$$\nThis equality is not guaranteed. For any non-zero input signal $x$, the equality holds only if $2+\\cos t = 2+\\cos(t-\\tau)$, or $\\cos t = \\cos(t-\\tau)$. This is not true for a general $\\tau \\in \\mathbb{R}$. For instance, let $\\tau = \\frac{\\pi}{2}$. Then the condition is $\\cos t = \\cos(t-\\frac{\\pi}{2}) = \\sin t$, which is not true for all $t$. Thus, the system $\\mathcal{S}_{1}$ is not time-invariant; it is time-varying. This completes the proof for Task $1$.\n\nTask $2$: Analyze the properties of the system $\\mathcal{S}_{2}[x](t) = \\big(x(t)\\big)^{2}$.\n\nTo test for time invariance, we again compare the response to a shifted input with the shifted response.\nThe response to a shifted input $x(\\cdot-\\tau)$ is:\n$$\n\\mathcal{S}_{2}[x(\\cdot-\\tau)](t) = \\big(x(t-\\tau)\\big)^{2}\n$$\nThe shifted response is found by replacing $t$ with $t-\\tau$ in the output expression:\n$$\n\\mathcal{S}_{2}[x](t-\\tau) = \\big(x(t-\\tau)\\big)^{2}\n$$\nSince $\\mathcal{S}_{2}[x(\\cdot-\\tau)](t) = \\mathcal{S}_{2}[x](t-\\tau)$ for all $t$ and $\\tau$, the system $\\mathcal{S}_{2}$ is time-invariant.\n\nTo prove nonlinearity, we test the superposition condition. We evaluate the system's response to a linear combination of inputs, $\\mathcal{S}_{2}[\\alpha x_{1}+\\beta x_{2}](t)$:\n$$\n\\mathcal{S}_{2}[\\alpha x_{1}+\\beta x_{2}](t) = \\big(\\alpha x_{1}(t) + \\beta x_{2}(t)\\big)^{2} = \\alpha^{2}\\big(x_{1}(t)\\big)^{2} + 2\\alpha\\beta x_{1}(t)x_{2}(t) + \\beta^{2}\\big(x_{2}(t)\\big)^{2}\n$$\nNow, we evaluate the linear combination of the individual responses:\n$$\n\\alpha \\mathcal{S}_{2}[x_{1}](t) + \\beta \\mathcal{S}_{2}[x_{2}](t) = \\alpha\\big(x_{1}(t)\\big)^{2} + \\beta\\big(x_{2}(t)\\big)^{2}\n$$\nIn general, for non-trivial choices of inputs and scalars,\n$$\n\\alpha^{2}\\big(x_{1}(t)\\big)^{2} + 2\\alpha\\beta x_{1}(t)x_{2}(t) + \\beta^{2}\\big(x_{2}(t)\\big)^{2} \\neq \\alpha\\big(x_{1}(t)\\big)^{2} + \\beta\\big(x_{2}(t)\\big)^{2}\n$$\nFor instance, if we let $\\alpha=2$, $\\beta=0$, and $x_{1}(t)=1$, the left side is $\\mathcal{S}_{2}[2\\cdot 1](t) = (2)^{2} = 4$, while the right side is $2\\cdot\\mathcal{S}_{2}[1](t) = 2(1)^{2} = 2$. Since $4 \\neq 2$, the system $\\mathcal{S}_{2}$ is not linear. It therefore violates the superposition principle. This completes the proof for Task $2$.\n\nTask $3$: Compute the superposition residuals $r_{1}(1)$ and $r_{2}(1)$.\n\nThe first residual is defined as $r_{1}(t) = \\mathcal{S}_{1}[\\alpha x_{1}+\\beta x_{2}](t)-\\big(\\alpha\\,\\mathcal{S}_{1}[x_{1}](t)+\\beta\\,\\mathcal{S}_{1}[x_{2}](t)\\big)$. As proven in Task $1$, the system $\\mathcal{S}_{1}$ is linear. By definition of linearity, the two terms in the expression for $r_{1}(t)$ are identical. Therefore, $r_{1}(t) = 0$ for all $t \\in \\mathbb{R}$. Consequently,\n$$\nr_{1}(1) = 0\n$$\n\nThe second residual is defined as $r_{2}(t) = \\mathcal{S}_{2}[\\alpha x_{1}+\\beta x_{2}](t)-\\big(\\alpha\\,\\mathcal{S}_{2}[x_{1}](t)+\\beta\\,\\mathcal{S}_{2}[x_{2}](t)\\big)$. We must compute this at $t=1$ using $\\alpha=3$, $\\beta=-2$, $x_{1}(t)=\\exp(-t)\\,u(t)$, and $x_{2}(t)=\\cos t$. First, we evaluate the inputs at $t=1$:\n$$\nx_{1}(1) = \\exp(-1)\\,u(1) = \\exp(-1) \\cdot 1 = \\exp(-1)\n$$\n$$\nx_{2}(1) = \\cos(1)\n$$\nNow we compute the two parts of the expression for $r_{2}(1)$.\nThe first part is $\\mathcal{S}_{2}[\\alpha x_{1}+\\beta x_{2}](1)$:\n$$\n\\mathcal{S}_{2}[3x_{1} - 2x_{2}](1) = \\big(3x_{1}(1) - 2x_{2}(1)\\big)^{2} = \\big(3\\exp(-1) - 2\\cos(1)\\big)^{2}\n$$\n$$\n= 9\\big(\\exp(-1)\\big)^{2} - 2(3\\exp(-1))(2\\cos(1)) + 4\\big(\\cos(1)\\big)^{2} = 9\\exp(-2) - 12\\exp(-1)\\cos(1) + 4\\cos^{2}(1)\n$$\nThe second part is $\\alpha\\mathcal{S}_{2}[x_{1}](1) + \\beta\\mathcal{S}_{2}[x_{2}](1)$:\n$$\n3\\mathcal{S}_{2}[x_{1}](1) - 2\\mathcal{S}_{2}[x_{2}](1) = 3\\big(x_{1}(1)\\big)^{2} - 2\\big(x_{2}(1)\\big)^{2} = 3\\big(\\exp(-1)\\big)^{2} - 2\\big(\\cos(1)\\big)^{2}\n$$\n$$\n= 3\\exp(-2) - 2\\cos^{2}(1)\n$$\nFinally, we compute the difference to find $r_{2}(1)$:\n$$\nr_{2}(1) = \\big(9\\exp(-2) - 12\\exp(-1)\\cos(1) + 4\\cos^{2}(1)\\big) - \\big(3\\exp(-2) - 2\\cos^{2}(1)\\big)\n$$\n$$\nr_{2}(1) = (9-3)\\exp(-2) - 12\\exp(-1)\\cos(1) + (4 - (-2))\\cos^{2}(1)\n$$\n$$\nr_{2}(1) = 6\\exp(-2) - 12\\exp(-1)\\cos(1) + 6\\cos^{2}(1)\n$$\nThe requested values are $(r_{1}(1), r_{2}(1))$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  6\\exp(-2) - 12\\exp(-1)\\cos(1) + 6\\cos^{2}(1) \\end{pmatrix}}\n$$", "id": "2733503"}]}