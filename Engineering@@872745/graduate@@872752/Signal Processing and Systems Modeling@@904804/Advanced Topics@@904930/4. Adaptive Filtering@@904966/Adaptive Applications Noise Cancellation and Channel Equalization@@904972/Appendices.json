{"hands_on_practices": [{"introduction": "Before diving into adaptive methods, it is crucial to understand the theoretical gold standard: the Wiener filter. This optimal linear filter provides the best possible performance in a mean-square error sense when we have complete knowledge of the signal statistics. This exercise [@problem_id:2850046] challenges you to compute this ideal solution directly from the signal's autocorrelation and cross-correlation, providing a benchmark against which all adaptive filters are measured.", "problem": "A two-tap linear equalizer is designed to minimize the mean-square error between a zero-mean desired scalar signal $d(n)$ and a linear estimate $\\hat{d}(n)=\\mathbf{w}^{\\top}\\mathbf{x}(n)$ formed from a zero-mean, jointly wide-sense stationary two-dimensional input $\\mathbf{x}(n)\\in\\mathbb{R}^{2}$. Let the input autocorrelation matrix be $R=\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}$ and the cross-correlation vector be $\\mathbf{p}=\\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$. Suppose that $R=\\begin{bmatrix}21\\\\12\\end{bmatrix}$ and $\\mathbf{p}=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ have been obtained from second-order statistics of the data, and the desired signal variance is $\\sigma_{d}^{2}=\\mathbb{E}\\{d^{2}(n)\\}=1$.\n\nStarting from the minimum mean-square error formulation based on second-order moments and the orthogonality principle, determine:\n\n1. The optimal Wiener solution $\\mathbf{w}_{o}\\in\\mathbb{R}^{2}$ that minimizes $\\mathbb{E}\\{(d(n)-\\mathbf{w}^{\\top}\\mathbf{x}(n))^{2}\\}$.\n\n2. The minimum mean-square error $J_{\\min}$ achieved at $\\mathbf{w}_{o}$.\n\nExpress the final numerical values exactly. No rounding is required. The answer must be provided as exact rational numbers.", "solution": "The problem presented is a standard exercise in statistical signal processing concerning the derivation of the Wiener filter. It is scientifically grounded, well-posed, and contains all necessary information for a complete solution. Therefore, it is valid. We proceed with the derivation.\n\nThe objective is to find the weight vector $\\mathbf{w}$ that minimizes the mean-square error (MSE) $J(\\mathbf{w})$. The MSE is defined as the expected value of the squared error between the desired signal $d(n)$ and its estimate $\\hat{d}(n)$.\nThe estimate is a linear combination of the input vector $\\mathbf{x}(n)$, given by $\\hat{d}(n) = \\mathbf{w}^{\\top}\\mathbf{x}(n)$. The error signal is $e(n) = d(n) - \\hat{d}(n) = d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n)$.\n\nThe MSE cost function $J(\\mathbf{w})$ is:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{e^2(n)\\} = \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^2\\}$$\nExpanding the squared term, we obtain:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n) - 2d(n)\\mathbf{w}^{\\top}\\mathbf{x}(n) + (\\mathbf{w}^{\\top}\\mathbf{x}(n))(\\mathbf{x}^{\\top}(n)\\mathbf{w})\\}$$\nUsing the linearity of the expectation operator, this becomes:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n)\\} - 2\\mathbb{E}\\{d(n)\\mathbf{w}^{\\top}\\mathbf{x}(n)\\} + \\mathbb{E}\\{\\mathbf{w}^{\\top}\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\mathbf{w}\\}$$\nSince $\\mathbf{w}$ is a deterministic vector of coefficients, it can be moved outside the expectation:\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n)\\} - 2\\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)d(n)\\} + \\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}\\mathbf{w}$$\nWe are given the following second-order statistics:\nThe variance of the desired signal: $\\sigma_d^2 = \\mathbb{E}\\{d^2(n)\\} = 1$.\nThe cross-correlation vector between the input and the desired signal: $\\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe autocorrelation matrix of the input vector: $R = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$.\n\nSubstituting these quantities into the expression for $J(\\mathbf{w})$ yields the performance surface:\n$$J(\\mathbf{w}) = \\sigma_d^2 - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}R\\mathbf{w}$$\nTo find the optimal weight vector $\\mathbf{w}_o$ that minimizes this quadratic function, we must compute the gradient of $J(\\mathbf{w})$ with respect to $\\mathbf{w}$ and set it to the zero vector.\n$$\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\frac{\\mathrm{d}}{\\mathrm{d}\\mathbf{w}} (\\sigma_d^2 - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}R\\mathbf{w}) = -2\\mathbf{p} + 2R\\mathbf{w}$$\nSetting the gradient to zero for the optimal vector $\\mathbf{w}_o$:\n$$-2\\mathbf{p} + 2R\\mathbf{w}_o = \\mathbf{0}$$\nThis leads to the Wiener-Hopf equation:\n$$R\\mathbf{w}_o = \\mathbf{p}$$\nThe optimal Wiener solution $\\mathbf{w}_o$ is found by solving this system of linear equations:\n$$\\mathbf{w}_o = R^{-1}\\mathbf{p}$$\nFirst, we compute the inverse of the autocorrelation matrix $R$. The determinant of $R$ is:\n$$\\det(R) = (2)(2) - (1)(1) = 4 - 1 = 3$$\nSince $\\det(R) \\neq 0$, the inverse exists. For a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\n$$R^{-1} = \\frac{1}{3}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$$\nNow, we can calculate $\\mathbf{w}_o$:\n$$\\mathbf{w}_o = R^{-1}\\mathbf{p} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (\\frac{2}{3})(1) + (-\\frac{1}{3})(0) \\\\ (-\\frac{1}{3})(1) + (\\frac{2}{3})(0) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\nThis is the optimal Wiener solution for the filter taps.\n\nNext, we determine the minimum mean-square error, $J_{\\min}$, which is the value of the cost function evaluated at $\\mathbf{w} = \\mathbf{w}_o$.\n$$J_{\\min} = J(\\mathbf{w}_o) = \\sigma_d^2 - 2\\mathbf{w}_o^{\\top}\\mathbf{p} + \\mathbf{w}_o^{\\top}R\\mathbf{w}_o$$\nUsing the Wiener-Hopf equation, $R\\mathbf{w}_o = \\mathbf{p}$, the expression for $J_{\\min}$ can be simplified. The most common form is derived by substituting $\\mathbf{w}_o^{\\top}R\\mathbf{w}_o = \\mathbf{w}_o^{\\top}\\mathbf{p}$ into the expanded MSE equation:\n$$J_{\\min} = \\sigma_d^2 - 2\\mathbf{w}_o^{\\top}\\mathbf{p} + \\mathbf{w}_o^{\\top}\\mathbf{p} = \\sigma_d^2 - \\mathbf{w}_o^{\\top}\\mathbf{p}$$\nThis simplified expression is used for the calculation. Using the values we have:\n$\\sigma_d^2 = 1$, $\\mathbf{w}_o = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$, and $\\mathbf{p} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n$$\\mathbf{w}_o^{\\top}\\mathbf{p} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (\\frac{2}{3})(1) + (-\\frac{1}{3})(0) = \\frac{2}{3}$$\nTherefore, the minimum mean-square error is:\n$$J_{\\min} = \\sigma_d^2 - \\mathbf{w}_o^{\\top}\\mathbf{p} = 1 - \\frac{2}{3} = \\frac{1}{3}$$\nThe optimal Wiener filter is $\\mathbf{w}_o = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$ and the minimum MSE is $J_{\\min} = \\frac{1}{3}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3}  -\\frac{1}{3}  \\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "2850046"}, {"introduction": "In most practical scenarios, signal statistics are unknown or time-varying, making the direct computation of a Wiener filter impossible. The Least Mean Squares (LMS) algorithm provides an elegant solution by iteratively updating filter weights using only the current input data and estimation error. This practice [@problem_id:2850025] walks you through the core mechanics of the LMS update rule over multiple time steps, offering a hands-on feel for how an adaptive filter learns from experience.", "problem": "Consider a baseband discrete-time linear equalizer of order $3$ with weight vector $\\mathbf{w}[n] \\in \\mathbb{R}^{3}$ operating on the input regressor $\\mathbf{u}[n] \\in \\mathbb{R}^{3}$ constructed from the present and two past input samples, namely $\\mathbf{u}[n] \\triangleq \\begin{pmatrix} x[n] \\\\ x[n-1] \\\\ x[n-2] \\end{pmatrix}$. The equalizer output is the linear form $y[n] \\triangleq \\mathbf{w}^{\\top}[n]\\mathbf{u}[n]$, and the instantaneous error is $e[n] \\triangleq d[n] - y[n]$, where $d[n]$ is the desired response. The adaptation is driven by minimizing the instantaneous squared error cost $J[n] \\triangleq \\tfrac{1}{2} e^{2}[n]$ using stochastic gradient descent with stepsize $\\mu$.\n\nStarting from these definitions and the gradient-descent principle, derive the Least Mean Squares (LMS) update rule for $\\mathbf{w}[n]$ and then apply it to the following short data record. Use zero padding for unavailable past inputs so that $x[n] = 0$ for $n  0$.\n\nGiven:\n- Stepsize $\\mu = \\tfrac{1}{4}$.\n- Input samples $x[-1] = 0$, $x[0] = 2$, $x[1] = -1$, $x[2] = 3$.\n- Desired responses $d[1] = 5$, $d[2] = -2$.\n- Initial condition $\\mathbf{w}[1] = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nPerform two consecutive LMS updates at $n = 1$ and $n = 2$. Compute the instantaneous errors $e[1]$ and $e[2]$. Provide your final answer as a $1 \\times 2$ row matrix $\\begin{pmatrix} e[1]  e[2] \\end{pmatrix}$ with exact values (no rounding or decimal approximations). No physical units are required.", "solution": "The problem is first validated.\n\n**Step 1: Extract Givens**\n- Equalizer order: $3$.\n- Weight vector: $\\mathbf{w}[n] \\in \\mathbb{R}^{3}$.\n- Input regressor: $\\mathbf{u}[n] \\triangleq \\begin{pmatrix} x[n] \\\\ x[n-1] \\\\ x[n-2] \\end{pmatrix}$.\n- Equalizer output: $y[n] \\triangleq \\mathbf{w}^{\\top}[n]\\mathbf{u}[n]$.\n- Instantaneous error: $e[n] \\triangleq d[n] - y[n]$.\n- Cost function: $J[n] \\triangleq \\frac{1}{2} e^{2}[n]$.\n- Adaptation algorithm: Stochastic gradient descent (SGD).\n- Stepsize: $\\mu = \\frac{1}{4}$.\n- Input samples: $x[-1] = 0$, $x[0] = 2$, $x[1] = -1$, $x[2] = 3$.\n- Padding rule: $x[n] = 0$ for $n  0$.\n- Desired responses: $d[1] = 5$, $d[2] = -2$.\n- Initial condition: $\\mathbf{w}[1] = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n- Objective: Derive the Least Mean Squares (LMS) update rule and compute the errors $e[1]$ and $e[2]$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in adaptive digital filtering, a core topic in signal processing. All definitions, principles, and data are consistent with established theory. The problem is scientifically grounded, well-posed, objective, and fully specified. There are no contradictions, ambiguities, or factual unsoundness. It requires a direct application of fundamental principles.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will now be provided.\n\nThe task is twofold: first, to derive the general update rule for the Least Mean Squares (LMS) algorithm, and second, to apply this rule for two consecutive time steps using the provided data.\n\nThe LMS algorithm is a stochastic gradient descent method aimed at minimizing the instantaneous squared error, $J[n]$. The weight vector update rule is given by:\n$$\n\\mathbf{w}[n+1] = \\mathbf{w}[n] - \\mu \\nabla_{\\mathbf{w}[n]} J[n]\n$$\nwhere $\\mathbf{w}[n]$ is the weight vector at time step $n$, $\\mu$ is the stepsize parameter, and $\\nabla_{\\mathbf{w}[n]} J[n]$ is the instantaneous gradient of the cost function with respect to the weight vector.\n\nThe cost function is defined as:\n$$\nJ[n] = \\frac{1}{2} e^{2}[n]\n$$\nTo find the gradient, we use the chain rule:\n$$\n\\nabla_{\\mathbf{w}[n]} J[n] = \\frac{\\partial J[n]}{\\partial e[n]} \\frac{\\partial e[n]}{\\partial \\mathbf{w}[n]}\n$$\nThe first term is:\n$$\n\\frac{\\partial J[n]}{\\partial e[n]} = \\frac{\\partial}{\\partial e[n]} \\left(\\frac{1}{2} e^{2}[n]\\right) = e[n]\n$$\nThe second term requires the definition of the error $e[n]$:\n$$\ne[n] = d[n] - y[n] = d[n] - \\mathbf{w}^{\\top}[n]\\mathbf{u}[n]\n$$\nTaking the gradient with respect to $\\mathbf{w}[n]$ gives:\n$$\n\\frac{\\partial e[n]}{\\partial \\mathbf{w}[n]} = \\nabla_{\\mathbf{w}[n]} (d[n] - \\mathbf{w}^{\\top}[n]\\mathbf{u}[n]) = -\\mathbf{u}[n]\n$$\nCombining these results, the instantaneous gradient is:\n$$\n\\nabla_{\\mathbf{w}[n]} J[n] = e[n] (-\\mathbf{u}[n]) = -e[n]\\mathbf{u}[n]\n$$\nSubstituting this gradient back into the SGD update equation yields the LMS algorithm update rule:\n$$\n\\mathbf{w}[n+1] = \\mathbf{w}[n] - \\mu(-e[n]\\mathbf{u}[n]) = \\mathbf{w}[n] + \\mu e[n]\\mathbf{u}[n]\n$$\nThis is the equation we must now apply. The process at each time step $n$ is:\n$1.$ Compute the filter output: $y[n] = \\mathbf{w}^{\\top}[n]\\mathbf{u}[n]$.\n$2.$ Compute the error: $e[n] = d[n] - y[n]$.\n$3.$ Update the weights: $\\mathbf{w}[n+1] = \\mathbf{w}[n] + \\mu e[n]\\mathbf{u}[n]$.\n\nWe are given the stepsize $\\mu = \\frac{1}{4}$ and initial weights $\\mathbf{w}[1] = \\begin{pmatrix} 0  0  0 \\end{pmatrix}^{\\top}$. We perform the two updates for $n=1$ and $n=2$.\n\n**Iteration at $n=1$**:\nFirst, we construct the input regressor $\\mathbf{u}[1]$ using the given input samples $x[1]=-1$, $x[0]=2$, and $x[-1]=0$.\n$$\n\\mathbf{u}[1] = \\begin{pmatrix} x[1] \\\\ x[0] \\\\ x[-1] \\end{{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\nThe filter output $y[1]$ is computed using the weight vector $\\mathbf{w}[1]$:\n$$\ny[1] = \\mathbf{w}^{\\top}[1]\\mathbf{u}[1] = \\begin{pmatrix} 0  0  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix} = 0\n$$\nThe instantaneous error $e[1]$ is calculated using the desired response $d[1]=5$:\n$$\ne[1] = d[1] - y[1] = 5 - 0 = 5\n$$\nThis is the first required value. Next, we update the weight vector to find $\\mathbf{w}[2]$:\n$$\n\\mathbf{w}[2] = \\mathbf{w}[1] + \\mu e[1]\\mathbf{u}[1] = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4}(5)\\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\frac{5}{4}\\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{4} \\\\ \\frac{10}{4} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{4} \\\\ \\frac{5}{2} \\\\ 0 \\end{pmatrix}\n$$\n\n**Iteration at $n=2$**:\nNow we use the updated weight vector $\\mathbf{w}[2]$ for the calculations at time $n=2$. First, construct the input regressor $\\mathbf{u}[2]$ from $x[2]=3$, $x[1]=-1$, and $x[0]=2$.\n$$\n\\mathbf{u}[2] = \\begin{pmatrix} x[2] \\\\ x[1] \\\\ x[0] \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\nThe filter output $y[2]$ is computed using $\\mathbf{w}[2]$:\n$$\ny[2] = \\mathbf{w}^{\\top}[2]\\mathbf{u}[2] = \\begin{pmatrix} -\\frac{5}{4}  \\frac{5}{2}  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1 \\\\ 2 \\end{pmatrix} = \\left(-\\frac{5}{4}\\right)(3) + \\left(\\frac{5}{2}\\right)(-1) + (0)(2) = -\\frac{15}{4} - \\frac{5}{2}\n$$\nTo sum these fractions, we use a common denominator of $4$:\n$$\ny[2] = -\\frac{15}{4} - \\frac{10}{4} = -\\frac{25}{4}\n$$\nThe instantaneous error $e[2]$ is calculated using the desired response $d[2]=-2$:\n$$\ne[2] = d[2] - y[2] = -2 - \\left(-\\frac{25}{4}\\right) = -2 + \\frac{25}{4} = -\\frac{8}{4} + \\frac{25}{4} = \\frac{17}{4}\n$$\nThis is the second required value.\n\nThe problem asks for the instantaneous errors $e[1]$ and $e[2]$ in the form of a $1 \\times 2$ row matrix. We have found $e[1]=5$ and $e[2]=\\frac{17}{4}$.\nThe final result is $\\begin{pmatrix} 5  \\frac{17}{4} \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 5  \\frac{17}{4} \\end{pmatrix}}\n$$", "id": "2850025"}, {"introduction": "While the LMS algorithm is powerful, its convergence speed is sensitive to the magnitude of the input signal. The Normalized Least Mean Squares (NLMS) algorithm addresses this by introducing a data-dependent step size, leading to more robust and often faster convergence. This exercise [@problem_id:2850035] demonstrates the implementation of this important variant, highlighting how a simple normalization can significantly improve the filter's practical performance.", "problem": "A two-tap adaptive filter is used as a simplified model for noise cancellation in a single-channel sensing scenario. The filter weight vector at time index $n$ is $w(n) \\in \\mathbb{R}^{2}$, the input regressor is $x(n) \\in \\mathbb{R}^{2}$, and the desired signal is $d(n) \\in \\mathbb{R}$. The objective at time $n$ is to minimize the instantaneous squared error, defined as $J(n) = \\tfrac{1}{2} e^{2}(n)$, where $e(n) = d(n) - w^{\\top}(n) x(n)$. Starting from this objective and the gradient $\\nabla_{w} J(n)$ with respect to $w$, the Least Mean Squares (LMS) method performs a gradient step. The Normalized Least Mean Squares (NLMS) method rescales this step by the instantaneous input power to make it invariant to input amplitude, with a small positive regularization to avoid division by zero.\n\nGiven the initial weight vector $w(0) = [0, 0]^{\\top}$, step size $\\mu = 1$, regularization $\\delta = 10^{-3}$, input $x(0) = [3, 4]^{\\top}$, and desired response $d(0) = 5$, perform one update of the Normalized Least Mean Squares (NLMS) algorithm starting from the definition of $J(n)$ and its gradient, and compute:\n- the updated coefficient vector $w(1)$, and\n- the resulting a posteriori instantaneous error $e^{+}(0) \\equiv d(0) - w^{\\top}(1)\\,x(0)$.\n\nExpress your final answer as a single row matrix containing, in order, the two components of $w(1)$ followed by $e^{+}(0)$. No rounding is required; provide the exact expression.", "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- **Model:** Two-tap adaptive filter\n- **Weight vector:** $w(n) \\in \\mathbb{R}^{2}$\n- **Input regressor:** $x(n) \\in \\mathbb{R}^{2}$\n- **Desired signal:** $d(n) \\in \\mathbb{R}$\n- **Objective function:** $J(n) = \\frac{1}{2} e^{2}(n)$\n- **Error signal:** $e(n) = d(n) - w^{\\top}(n) x(n)$\n- **Algorithm:** Normalized Least Mean Squares (NLMS)\n- **Initial weight vector:** $w(0) = [0, 0]^{\\top}$\n- **Step size:** $\\mu = 1$\n- **Regularization:** $\\delta = 10^{-3}$\n- **Input at $n=0$:** $x(0) = [3, 4]^{\\top}$\n- **Desired response at $n=0$:** $d(0) = 5$\n- **Required outputs:** $w(1)$ and $e^{+}(0) \\equiv d(0) - w^{\\top}(1)\\,x(0)$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it concerns the standard NLMS algorithm, a fundamental topic in adaptive signal processing. It is well-posed, providing all necessary parameters and initial conditions for a unique solution. The language is objective and precise. The data are consistent and complete. Therefore, the problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\nThe objective is to perform one update of the Normalized Least Mean Squares (NLMS) algorithm to find the updated weight vector $w(1)$ and the a posteriori error $e^{+}(0)$.\n\nThe instantaneous cost function at time $n$ is the squared error:\n$$J(n) = \\frac{1}{2} e^{2}(n) = \\frac{1}{2} (d(n) - w^{\\top}(n) x(n))^{2}$$\nThe NLMS algorithm is a form of stochastic gradient descent. The first step is to compute the gradient of the cost function $J(n)$ with respect to the weight vector $w(n)$. Using the chain rule:\n$$\\nabla_{w} J(n) = \\frac{\\partial J(n)}{\\partial w(n)} = \\frac{\\partial J(n)}{\\partial e(n)} \\frac{\\partial e(n)}{\\partial w(n)}$$\nThe derivatives are:\n$$\\frac{\\partial J(n)}{\\partial e(n)} = e(n)$$\n$$\\frac{\\partial e(n)}{\\partial w(n)} = \\frac{\\partial}{\\partial w(n)} (d(n) - w^{\\top}(n) x(n)) = -x(n)$$\nThus, the gradient is:\n$$\\nabla_{w} J(n) = -e(n) x(n)$$\nThe general stochastic gradient descent update rule is $w(n+1) = w(n) - \\alpha \\nabla_{w} J(n)$, where $\\alpha$ is a step-size parameter. For the NLMS algorithm, the update rule is specifically defined as:\n$$w(n+1) = w(n) - \\left( \\frac{\\mu}{\\delta + \\|x(n)\\|^{2}} \\right) \\nabla_{w} J(n)$$\nSubstituting the expression for the gradient, we obtain the NLMS update equation:\n$$w(n+1) = w(n) + \\frac{\\mu}{\\delta + \\|x(n)\\|^{2}} e(n) x(n)$$\nwhere $\\|x(n)\\|^{2} = x^{\\top}(n) x(n)$ is the squared Euclidean norm (instantaneous power) of the input vector.\n\nWe are asked to compute the update for $n=0$. The provided values are:\n$w(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x(0) = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, $d(0) = 5$, $\\mu = 1$, and $\\delta = 10^{-3}$.\n\nFirst, we calculate the a priori error at time $n=0$:\n$$e(0) = d(0) - w^{\\top}(0) x(0) = 5 - \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = 5 - 0 = 5$$\nNext, we calculate the squared norm of the input vector $x(0)$:\n$$\\|x(0)\\|^{2} = x^{\\top}(0) x(0) = 3^{2} + 4^{2} = 9 + 16 = 25$$\nNow we can compute the updated weight vector $w(1)$:\n$$w(1) = w(0) + \\frac{\\mu}{\\delta + \\|x(0)\\|^{2}} e(0) x(0)$$\n$$w(1) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{10^{-3} + 25} (5) \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\nThe term $\\delta + \\|x(0)\\|^2$ is $25.001$, which is $\\frac{25001}{1000}$. The coefficient for the update is:\n$$\\frac{5}{25.001} = \\frac{5}{25001/1000} = \\frac{5000}{25001}$$\nSo, the updated weight vector is:\n$$w(1) = \\frac{5000}{25001} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{15000}{25001} \\\\ \\frac{20000}{25001} \\end{pmatrix}$$\nThe components of the updated weight vector are $w_{1}(1) = \\frac{15000}{25001}$ and $w_{2}(1) = \\frac{20000}{25001}$.\n\nFinally, we compute the a posteriori error, $e^{+}(0)$:\n$$e^{+}(0) = d(0) - w^{\\top}(1) x(0)$$\n$$e^{+}(0) = 5 - \\begin{pmatrix} \\frac{15000}{25001}  \\frac{20000}{25001} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\n$$e^{+}(0) = 5 - \\left( \\frac{15000 \\cdot 3 + 20000 \\cdot 4}{25001} \\right)$$\n$$e^{+}(0) = 5 - \\left( \\frac{45000 + 80000}{25001} \\right) = 5 - \\frac{125000}{25001}$$\n$$e^{+}(0) = \\frac{5 \\cdot 25001 - 125000}{25001} = \\frac{125005 - 125000}{25001} = \\frac{5}{25001}$$\n\nThe required outputs are the two components of $w(1)$ and the value of $e^{+}(0)$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{15000}{25001}  \\frac{20000}{25001}  \\frac{5}{25001} \\end{pmatrix}}$$", "id": "2850035"}]}