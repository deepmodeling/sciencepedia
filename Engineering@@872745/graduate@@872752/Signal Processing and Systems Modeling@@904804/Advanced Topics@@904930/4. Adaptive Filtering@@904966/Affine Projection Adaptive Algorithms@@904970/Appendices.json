{"hands_on_practices": [{"introduction": "To truly grasp the Affine Projection Algorithm (APA), we begin at its source: the optimization problem it aims to solve. This first practice challenges you to derive the APA update from first principles, giving you a deep appreciation for its geometric interpretation as a minimal-norm solution constrained by recent data [@problem_id:2850728]. By then applying this derived formula to a concrete numerical example, you will bridge the gap between abstract theory and practical calculation.", "problem": "Consider a real-valued single-input single-output linear finite impulse response system identification problem with an adaptive filter of length $M$. Let the current weight vector be $\\mathbf{w}(n) \\in \\mathbb{R}^{M}$, and define the regressor vectors $\\mathbf{x}(n), \\mathbf{x}(n-1) \\in \\mathbb{R}^{M}$ with corresponding desired outputs $d(n), d(n-1) \\in \\mathbb{R}$. The Affine Projection Algorithm (APA) of order $P=2$ seeks an update increment $\\Delta \\mathbf{w}(n) \\in \\mathbb{R}^{M}$ that minimally disturbs the current estimate while reconciling the most recent two error equations. The principle of minimal disturbance enforces that the updated estimate $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\,\\Delta \\mathbf{w}(n)$ lies as close as possible to $\\mathbf{w}(n)$ in Euclidean norm, while driving the pair of a posteriori equations at times $n$ and $n-1$ to the corresponding desired values in a regularized sense governed by a nonnegative regularization parameter $\\delta \\ge 0$ and a positive stepsize $\\mu > 0$.\n\n1) Starting from the fundamental optimization principle that the update increment $\\Delta \\mathbf{w}(n)$ should have minimum Euclidean norm subject to the affine constraints implied by the two most recent data equations, formulate a regularized constrained optimization problem whose solution produces the $P=2$ Affine Projection Algorithm (APA) update. Introduce Lagrange multipliers to enforce the two affine constraints and include a Tikhonov regularization term with parameter $\\delta$ on the multipliers to guarantee numerical stability. Derive, from first principles, a closed-form expression for $\\Delta \\mathbf{w}(n)$ expressed in terms of the $M \\times 2$ data matrix $\\mathbf{X}(n) \\triangleq [\\,\\mathbf{x}(n)\\ \\ \\mathbf{x}(n-1)\\,]$, the two-sample a priori error vector $\\mathbf{e}(n) \\triangleq \\begin{bmatrix} d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) \\\\ d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) \\end{bmatrix}$, and the regularization parameter $\\delta$. Show explicitly how the update increment is a linear combination of the two regressors, thereby combining the information in the two error equations.\n\n2) Use your derived expression to compute the explicit numerical value of the updated weight vector $\\mathbf{w}(n+1)$ for the following data: let $M=3$, $\\mathbf{w}(n) = \\mathbf{0}_{3\\times 1}$, $\\mathbf{x}(n) = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$, $\\mathbf{x}(n-1) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $d(n) = 2$, $d(n-1)=1$, stepsize $\\mu = 1$, and regularization parameter $\\delta = 1$. Express your final answer for $\\mathbf{w}(n+1)$ as a single row vector.\n\nYour final answer must be the single updated weight vector as a row matrix. No rounding is required, and no units are involved.", "solution": "The problem is divided into two parts. The first part requires the derivation of the update increment $\\Delta \\mathbf{w}(n)$ for the order $P=2$ Affine Projection Algorithm. The second part requires the numerical computation of the updated weight vector $\\mathbf{w}(n+1)$ for a specific set of data.\n\n*Editorial Note: The derivation below follows the specific optimization problem posed, which leads to a slightly non-standard form of the APA update: $\\Delta\\mathbf{w}(n) = \\mu \\mathbf{X}(n) (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})^{-1} \\mathbf{e}(n)$. This differs from the more common form presented in the main text of the article. For the specific parameters given in Part 2, where $\\mu=1$, both forms coincide.*\n\n**Part 1: Derivation of the APA Update Increment**\n\nThe core principle of the APA is to find an updated weight vector $\\mathbf{w}(n+1)$ that is minimally different from the current vector $\\mathbf{w}(n)$, while satisfying a set of constraints. The update is given by $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)$, where $\\mu > 0$ is a stepsize and $\\Delta\\mathbf{w}(n)$ is the update increment. Minimizing the disturbance means minimizing the Euclidean norm of the change in the weight vector, $\\|\\mathbf{w}(n+1) - \\mathbf{w}(n)\\|^2 = \\|\\mu \\Delta\\mathbf{w}(n)\\|^2$. Since $\\mu$ is a positive constant, this is equivalent to minimizing $\\|\\Delta\\mathbf{w}(n)\\|^2$.\n\nThe constraints are that the updated filter should satisfy the two most recent desired output relations. That is, for times $n$ and $n-1$:\n$$ \\mathbf{x}^{\\top}(n)\\mathbf{w}(n+1) = d(n) $$\n$$ \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n+1) = d(n-1) $$\nSubstituting $\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)$, we can express the constraints in terms of the update increment $\\Delta\\mathbf{w}(n)$:\n$$ \\mathbf{x}^{\\top}(n)(\\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)) = d(n) \\implies \\mu \\mathbf{x}^{\\top}(n) \\Delta\\mathbf{w}(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) $$\n$$ \\mathbf{x}^{\\top}(n-1)(\\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n)) = d(n-1) \\implies \\mu \\mathbf{x}^{\\top}(n-1) \\Delta\\mathbf{w}(n) = d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) $$\nUsing the provided definitions for the data matrix $\\mathbf{X}(n) \\triangleq [\\,\\mathbf{x}(n)\\ \\ \\mathbf{x}(n-1)\\,]$ and the a priori error vector $\\mathbf{e}(n) \\triangleq \\begin{bmatrix} d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) \\\\ d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) \\end{bmatrix}$, these two scalar constraints can be written compactly as a single vector equation:\n$$ \\mu \\mathbf{X}(n)^{\\top} \\Delta\\mathbf{w}(n) = \\mathbf{e}(n) $$\nThe optimization problem is thus to find the $\\Delta\\mathbf{w}(n)$ that minimizes the cost function $J(\\Delta\\mathbf{w}(n)) = \\frac{1}{2}\\|\\Delta\\mathbf{w}(n)\\|^2$ subject to the linear constraint $\\mu \\mathbf{X}(n)^{\\top} \\Delta\\mathbf{w}(n) = \\mathbf{e}(n)$. The factor of $\\frac{1}{2}$ is included for algebraic convenience.\n\nWe solve this constrained optimization problem using the method of Lagrange multipliers. We introduce a vector of Lagrange multipliers $\\boldsymbol{\\lambda} \\in \\mathbb{R}^2$ and form the Lagrangian:\n$$ L(\\Delta\\mathbf{w}(n), \\boldsymbol{\\lambda}) = \\frac{1}{2} \\Delta\\mathbf{w}(n)^{\\top}\\Delta\\mathbf{w}(n) + \\boldsymbol{\\lambda}^{\\top}(\\mathbf{e}(n) - \\mu \\mathbf{X}(n)^{\\top}\\Delta\\mathbf{w}(n)) $$\nTo find the minimum, we first find the stationary point of $L$ with respect to $\\Delta\\mathbf{w}(n)$ by setting the gradient to zero:\n$$ \\nabla_{\\Delta\\mathbf{w}(n)} L = \\Delta\\mathbf{w}(n) - \\mu \\mathbf{X}(n)\\boldsymbol{\\lambda} = \\mathbf{0} $$\nThis gives the structure of the optimal update increment:\n$$ \\Delta\\mathbf{w}(n) = \\mu \\mathbf{X}(n)\\boldsymbol{\\lambda} $$\nThis equation shows that the update increment $\\Delta\\mathbf{w}(n)$ is a linear combination of the regressor vectors $\\mathbf{x}(n)$ and $\\mathbf{x}(n-1)$, the columns of $\\mathbf{X}(n)$.\n\nThe problem states that regularization should be introduced on the multipliers. This is accomplished by regularizing the dual problem. Substituting the expression for $\\Delta\\mathbf{w}(n)$ back into the Lagrangian gives the dual objective function $L_D(\\boldsymbol{\\lambda})$:\n$$ L_D(\\boldsymbol{\\lambda}) = \\frac{1}{2}(\\mu \\mathbf{X}(n)\\boldsymbol{\\lambda})^{\\top}(\\mu \\mathbf{X}(n)\\boldsymbol{\\lambda}) + \\boldsymbol{\\lambda}^{\\top}(\\mathbf{e}(n) - \\mu \\mathbf{X}(n)^{\\top}(\\mu \\mathbf{X}(n)\\boldsymbol{\\lambda})) $$\n$$ L_D(\\boldsymbol{\\lambda}) = \\frac{\\mu^2}{2} \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} + \\boldsymbol{\\lambda}^{\\top}\\mathbf{e}(n) - \\mu^2 \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} $$\n$$ L_D(\\boldsymbol{\\lambda}) = \\boldsymbol{\\lambda}^{\\top}\\mathbf{e}(n) - \\frac{\\mu^2}{2} \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} $$\nMaximizing this dual function yields the Lagrange multipliers. To incorporate Tikhonov regularization on the multipliers as specified, we subtract a quadratic penalty term $\\frac{\\delta}{2}\\|\\boldsymbol{\\lambda}\\|^2$ from the dual objective, where $\\delta \\ge 0$ is the regularization parameter. The regularized dual objective is:\n$$ L_{D,reg}(\\boldsymbol{\\lambda}) = \\boldsymbol{\\lambda}^{\\top}\\mathbf{e}(n) - \\frac{\\mu^2}{2} \\boldsymbol{\\lambda}^{\\top}\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} - \\frac{\\delta}{2} \\boldsymbol{\\lambda}^{\\top}\\boldsymbol{\\lambda} $$\nTo maximize $L_{D,reg}(\\boldsymbol{\\lambda})$, we set its gradient with respect to $\\boldsymbol{\\lambda}$ to zero:\n$$ \\nabla_{\\boldsymbol{\\lambda}} L_{D,reg} = \\mathbf{e}(n) - \\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n)\\boldsymbol{\\lambda} - \\delta \\boldsymbol{\\lambda} = \\mathbf{0} $$\nRearranging to solve for $\\boldsymbol{\\lambda}$:\n$$ (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I}) \\boldsymbol{\\lambda} = \\mathbf{e}(n) $$\nwhere $\\mathbf{I}$ is the $2 \\times 2$ identity matrix. For $\\delta > 0$, the matrix $(\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})$ is guaranteed to be positive definite and thus invertible. The Lagrange multipliers are:\n$$ \\boldsymbol{\\lambda} = (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})^{-1} \\mathbf{e}(n) $$\nFinally, we substitute this expression for $\\boldsymbol{\\lambda}$ back into the equation for $\\Delta\\mathbf{w}(n)$:\n$$ \\Delta\\mathbf{w}(n) = \\mu \\mathbf{X}(n) (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})^{-1} \\mathbf{e}(n) $$\nThis is the closed-form expression for the update increment, showing it is a linear combination of the columns of $\\mathbf{X}(n)$.\n\n**Part 2: Numerical Calculation**\n\nWe are given the following data:\n- Filter length $M=3$.\n- Initial weights $\\mathbf{w}(n) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Regressor vectors $\\mathbf{x}(n) = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$ and $\\mathbf{x}(n-1) = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- Desired outputs $d(n) = 2$ and $d(n-1)=1$.\n- Stepsize $\\mu = 1$.\n- Regularization parameter $\\delta = 1$.\n\nFirst, we construct the data matrix $\\mathbf{X}(n)$ and the a priori error vector $\\mathbf{e}(n)$.\n$$ \\mathbf{X}(n) = [\\,\\mathbf{x}(n)\\ \\ \\mathbf{x}(n-1)\\,] = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} $$\nThe components of the a priori error vector are:\n$$ e(n) = d(n) - \\mathbf{x}^{\\top}(n)\\mathbf{w}(n) = 2 - \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = 2 $$\n$$ e(n-1) = d(n-1) - \\mathbf{x}^{\\top}(n-1)\\mathbf{w}(n) = 1 - \\begin{bmatrix} 0 & 1 & 1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = 1 $$\nSo, the error vector is $\\mathbf{e}(n) = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n\nNext, we compute the matrix product $\\mathbf{X}(n)^{\\top}\\mathbf{X}(n)$:\n$$ \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} (1)(1)+(0)(0)+(1)(1) & (1)(0)+(0)(1)+(1)(1) \\\\ (0)(1)+(1)(0)+(1)(1) & (0)(0)+(1)(1)+(1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $$\nNow we form the matrix to be inverted, using $\\mu=1$ and $\\delta=1$:\n$$ \\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I} = (1)^2 \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} + (1) \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} + \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} $$\nThe inverse of this $2 \\times 2$ matrix is:\n$$ \\left(\\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\\right)^{-1} = \\frac{1}{(3)(3)-(1)(1)} \\begin{bmatrix} 3 & -1 \\\\ -1 & 3 \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 3 & -1 \\\\ -1 & 3 \\end{bmatrix} $$\nNow we can calculate the update increment $\\Delta\\mathbf{w}(n)$:\n$$ \\Delta\\mathbf{w}(n) = \\mu \\mathbf{X}(n) (\\mu^2 \\mathbf{X}(n)^{\\top}\\mathbf{X}(n) + \\delta \\mathbf{I})^{-1} \\mathbf{e}(n) $$\n$$ \\Delta\\mathbf{w}(n) = (1) \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\left( \\frac{1}{8} \\begin{bmatrix} 3 & -1 \\\\ -1 & 3 \\end{bmatrix} \\right) \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} $$\n$$ \\Delta\\mathbf{w}(n) = \\frac{1}{8} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} (3)(2)+(-1)(1) \\\\ (-1)(2)+(3)(1) \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix} $$\n$$ \\Delta\\mathbf{w}(n) = \\frac{1}{8} \\begin{bmatrix} (1)(5)+(0)(1) \\\\ (0)(5)+(1)(1) \\\\ (1)(5)+(1)(1) \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 5 \\\\ 1 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 6/8 \\end{bmatrix} = \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 3/4 \\end{bmatrix} $$\nFinally, we compute the updated weight vector $\\mathbf{w}(n+1)$:\n$$ \\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\Delta\\mathbf{w}(n) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} + (1) \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 3/4 \\end{bmatrix} = \\begin{bmatrix} 5/8 \\\\ 1/8 \\\\ 3/4 \\end{bmatrix} $$\nThe final answer is requested as a single row vector.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{5}{8} & \\frac{1}{8} & \\frac{3}{4} \\end{pmatrix} } $$", "id": "2850728"}, {"introduction": "Moving from exact arithmetic to the realities of floating-point computation reveals hidden challenges in implementing adaptive algorithms. The direct, \"textbook\" implementation of APA involves forming a Gram matrix, a step fraught with numerical peril [@problem_id:2850768]. This exercise will guide you through diagnosing how finite precision can lead to a loss of the essential mathematical properties of symmetry and positive definiteness, and how to apply standard, robust remedies.", "problem": "In the Affine Projection Algorithm (APA) for adaptive filtering, at time index $n$ one constructs the regressor matrix $\\mathbf{X}_n \\in \\mathbb{R}^{M \\times P}$ whose columns contain the most recent $P$ regressors, and solves a small normal-equations system involving the Gram matrix $\\mathbf{G}_n = \\mathbf{X}_n^{\\mathsf{T}} \\mathbf{X}_n$ as part of the update. In exact arithmetic, $\\mathbf{G}_n$ is symmetric and positive semidefinite, and it is symmetric positive definite when $\\mathbf{X}_n$ has full column rank. However, in floating-point arithmetic with unit roundoff $u$, directly forming and factorizing $\\mathbf{G}_n$ can lead to numerical issues: loss of exact symmetry, and failure of positive definiteness checks required by Cholesky factorization.\n\nUsing only fundamental properties of Gram matrices and the standard floating-point model $\\mathrm{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)\\,(1+\\delta)$ with $|\\delta| \\le u$ for a basic operation $\\circ$, reason about how these issues arise and how to mitigate them. In particular, consider remedies based on symmetrization and diagonal loading (also called “jitter”), and the use of Cholesky factorization under such remedies.\n\nWhich of the following statements correctly explain the mechanisms by which finite precision can destroy symmetry and positive definiteness of $\\mathbf{X}_n^{\\mathsf{T}}\\mathbf{X}_n$, and propose robust remedies that are compatible with performing a Cholesky factorization with a small diagonal jitter? Select all that apply.\n\nA. Even though $\\mathbf{G}_n = \\mathbf{X}_n^{\\mathsf{T}}\\mathbf{X}_n$ is symmetric in exact arithmetic, separately accumulated inner products for $[\\mathbf{G}_n]_{ij}$ and $[\\mathbf{G}_n]_{ji}$ in floating point can round differently due to nonassociativity and distinct summation paths, yielding a computed $\\widehat{\\mathbf{G}}_n$ with $[\\widehat{\\mathbf{G}}_n]_{ij} \\ne [\\widehat{\\mathbf{G}}_n]_{ji}$. A robust mitigation is to enforce symmetry before factorization by replacing $\\widehat{\\mathbf{G}}_n$ with $\\tfrac{1}{2}(\\widehat{\\mathbf{G}}_n + \\widehat{\\mathbf{G}}_n^{\\mathsf{T}})$.\n\nB. If $\\mathbf{X}_n$ is ill-conditioned so that $\\lambda_{\\min}(\\mathbf{G}_n)$ is small, the perturbation $\\widehat{\\mathbf{G}}_n = \\mathbf{G}_n + \\mathbf{E}_n$ from rounding can shift the smallest eigenvalue negative. Adding a small diagonal “jitter” $\\alpha \\mathbf{I}$ with $\\alpha > 0$ to the symmetrized matrix shifts all eigenvalues by $\\alpha$ and restores strict positive definiteness, enabling a stable Cholesky factorization.\n\nC. To avoid bias, one must set $\\alpha = 0$; if Cholesky fails on the unsymmetrized $\\widehat{\\mathbf{G}}_n$, the only reliable remedy is to abandon the update, because any positive $\\alpha$ fundamentally changes the projection order and invalidates the APA.\n\nD. Rounding errors in inner products can be reduced by computing them with compensated summation or in a higher precision so that the perturbation $\\mathbf{E}_n$ is smaller; nonetheless, performing a final symmetrization and adding a minimal jitter $\\alpha \\mathbf{I}$ as needed serve as a robust safety net for Cholesky without materially altering the solution when $\\alpha$ is chosen proportional to $u \\|\\widehat{\\mathbf{G}}_n\\|$.\n\nE. If the exact $\\mathbf{G}_n$ is only positive semidefinite (rank deficient), then adding any $\\alpha > 0$ produces the same solution as the least-norm solution of the original normal equations, so diagonal loading does not introduce bias relative to the ideal APA solution.", "solution": "The core of the APA update involves solving a linear system of the form $\\mathbf{G}_n \\mathbf{p}_n = \\mathbf{e}_n$, where $\\mathbf{G}_n = \\mathbf{X}_n^{\\mathsf{T}} \\mathbf{X}_n$. This is often done via Cholesky factorization, which requires the matrix to be symmetric and positive definite. We analyze the challenges and remedies described.\n\n**Analysis of Numerical Issues**\n\n1.  **Loss of Symmetry:** The entry at row $i$ and column $j$ of the Gram matrix $\\mathbf{G}_n$ is the inner product of the $i$-th and $j$-th columns of $\\mathbf{X}_n$. Let these columns be denoted by $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n    $$[\\mathbf{G}_n]_{ij} = \\mathbf{x}_i^{\\mathsf{T}} \\mathbf{x}_j = \\sum_{k=1}^M [\\mathbf{X}_n]_{ki} [\\mathbf{X}_n]_{kj}$$\n    In exact arithmetic, $[\\mathbf{G}_n]_{ij} = \\mathbf{x}_i^{\\mathsf{T}} \\mathbf{x}_j = \\mathbf{x}_j^{\\mathsf{T}} \\mathbf{x}_i = [\\mathbf{G}_n]_{ji}$. However, in floating-point arithmetic, the summation is performed sequentially. Let $\\widehat{\\mathbf{G}}_n$ be the computed matrix. The computation $\\mathrm{fl}(\\sum_{k=1}^M [\\mathbf{X}_n]_{ki} [\\mathbf{X}_n]_{kj})$ involves a sequence of additions, which are not associative. The exact order of operations and the specific values being summed influence the final accumulated rounding error. Unless the code explicitly computes only the upper (or lower) triangle and copies the values to the other, the independent computations of $[\\widehat{\\mathbf{G}}_n]_{ij}$ and $[\\widehat{\\mathbf{G}}_n]_{ji}$ are not guaranteed to be identical. Thus, it is possible that $[\\widehat{\\mathbf{G}}_n]_{ij} \\ne [\\widehat{\\mathbf{G}}_n]_{ji}$.\n\n2.  **Loss of Positive Definiteness:** In exact arithmetic, $\\mathbf{G}_n$ is positive semidefinite. For any vector $\\mathbf{z} \\ne \\mathbf{0}$, we have $\\mathbf{z}^{\\mathsf{T}}\\mathbf{G}_n\\mathbf{z} = \\mathbf{z}^{\\mathsf{T}}\\mathbf{X}_n^{\\mathsf{T}}\\mathbf{X}_n\\mathbf{z} = (\\mathbf{X}_n\\mathbf{z})^{\\mathsf{T}}(\\mathbf{X}_n\\mathbf{z}) = \\|\\mathbf{X}_n\\mathbf{z}\\|_2^2 \\ge 0$. The matrix is positive definite if $\\mathbf{X}_n$ has full column rank, which means $\\mathbf{X}_n\\mathbf{z} \\ne \\mathbf{0}$ for any $\\mathbf{z} \\ne \\mathbf{0}$.\n    The computed matrix is $\\widehat{\\mathbf{G}}_n = \\mathbf{G}_n + \\mathbf{E}_n$, where $\\mathbf{E}_n$ is the matrix of rounding errors. Even if we enforce symmetry, creating $\\tilde{\\mathbf{G}}_n = \\frac{1}{2}(\\widehat{\\mathbf{G}}_n + \\widehat{\\mathbf{G}}_n^{\\mathsf{T}})$, this matrix is a perturbed version of the true $\\mathbf{G}_n$. The eigenvalues of $\\tilde{\\mathbf{G}}_n$ are related to those of $\\mathbf{G}_n$ by Weyl's inequality. If the smallest eigenvalue of $\\mathbf{G}_n$, $\\lambda_{\\min}(\\mathbf{G}_n)$, is very close to zero (i.e., $\\mathbf{G}_n$ is nearly singular or ill-conditioned), the negative eigenvalues of the symmetric part of the error matrix can cause the smallest eigenvalue of $\\tilde{\\mathbf{G}}_n$ to become negative. Cholesky factorization $\\tilde{\\mathbf{G}}_n = \\mathbf{L}\\mathbf{L}^{\\mathsf{T}}$ requires taking square roots of diagonal elements during its computation. A negative eigenvalue implies the matrix is not positive definite, and the algorithm will fail when it attempts to compute the square root of a negative number.\n\n**Evaluation of Options**\n\n**A. Even though $\\mathbf{G}_n = \\mathbf{X}_n^{\\mathsf{T}}\\mathbf{X}_n$ is symmetric in exact arithmetic, separately accumulated inner products for $[\\mathbf{G}_n]_{ij}$ and $[\\mathbf{G}_n]_{ji}$ in floating point can round differently due to nonassociativity and distinct summation paths, yielding a computed $\\widehat{\\mathbf{G}}_n$ with $[\\widehat{\\mathbf{G}}_n]_{ij} \\ne [\\widehat{\\mathbf{G}}_n]_{ji}$. A robust mitigation is to enforce symmetry before factorization by replacing $\\widehat{\\mathbf{G}}_n$ with $\\tfrac{1}{2}(\\widehat{\\mathbf{G}}_n + \\widehat{\\mathbf{G}}_n^{\\mathsf{T}})$.**\n\nThis statement is correct. It accurately identifies the non-associativity of floating-point addition as the cause for potential loss of symmetry in the computed Gram matrix. The proposed remedy, averaging the computed matrix with its transpose, i.e., forming $\\frac{1}{2}(\\widehat{\\mathbf{G}}_n + \\widehat{\\mathbf{G}}_n^{\\mathsf{T}})$, is a standard and effective procedure to restore symmetry. This new matrix is the closest symmetric matrix to $\\widehat{\\mathbf{G}}_n$ in the Frobenius norm.\n**Verdict: Correct.**\n\n**B. If $\\mathbf{X}_n$ is ill-conditioned so that $\\lambda_{\\min}(\\mathbf{G}_n)$ is small, the perturbation $\\widehat{\\mathbf{G}}_n = \\mathbf{G}_n + \\mathbf{E}_n$ from rounding can shift the smallest eigenvalue negative. Adding a small diagonal “jitter” $\\alpha \\mathbf{I}$ with $\\alpha > 0$ to the symmetrized matrix shifts all eigenvalues by $\\alpha$ and restores strict positive definiteness, enabling a stable Cholesky factorization.**\n\nThis statement is correct. It precisely describes how ill-conditioning makes the problem susceptible to floating-point errors, potentially leading to a computed matrix that is not positive definite. The remedy of adding a diagonal matrix $\\alpha \\mathbf{I}$ (diagonal loading or Tikhonov regularization) is a cornerstone of robust numerical linear algebra. For a symmetric matrix $\\mathbf{A}$, the eigenvalues of $\\mathbf{A} + \\alpha \\mathbf{I}$ are $\\lambda_i(\\mathbf{A}) + \\alpha$. By choosing $\\alpha$ large enough to overcome the most negative eigenvalue of the computed (and symmetrized) matrix, one guarantees strict positive definiteness, which in turn guarantees the success of the Cholesky factorization.\n**Verdict: Correct.**\n\n**C. To avoid bias, one must set $\\alpha = 0$; if Cholesky fails on the unsymmetrized $\\widehat{\\mathbf{G}}_n$, the only reliable remedy is to abandon the update, because any positive $\\alpha$ fundamentally changes the projection order and invalidates the APA.**\n\nThis statement is incorrect and demonstrates poor engineering judgment. While diagonal loading with $\\alpha > 0$ does introduce bias (the solution of the regularized system is not the solution of the original system), this is often a necessary trade-off for obtaining a stable and meaningful result. Insisting on $\\alpha = 0$ in the face of numerical indefiniteness can lead to division by zero, floating-point exceptions, or a numerically catastrophic update that corrupts the filter state. Abandoning the update is one possible, but often suboptimal, strategy. Regularization provides a stable, albeit slightly biased, update. Claiming it \"invalidates\" the APA is too strong; it produces a regularized APA, which is a widely used and valid variant. Therefore, this is not the \"only reliable remedy.\"\n**Verdict: Incorrect.**\n\n**D. Rounding errors in inner products can be reduced by computing them with compensated summation or in a higher precision so that the perturbation $\\mathbf{E}_n$ is smaller; nonetheless, performing a final symmetrization and adding a minimal jitter $\\alpha \\mathbf{I}$ as needed serve as a robust safety net for Cholesky without materially altering the solution when $\\alpha$ is chosen proportional to $u \\|\\widehat{\\mathbf{G}}_n\\|$.**\n\nThis statement is correct. It presents a comprehensive and practical strategy for robust implementation. Using higher-precision accumulators or compensated summation (e.g., Kahan summation) directly attacks the problem by reducing the magnitude of the error matrix $\\mathbf{E}_n$. However, these methods may not be sufficient for extremely ill-conditioned problems and add computational cost. The statement correctly identifies symmetrization and diagonal loading as a \"safety net\" that provides robustness regardless. Choosing a small $\\alpha$, such as a small multiple of the machine precision $u$ times the matrix norm, is a standard heuristic. This ensures that the regularization is minimal—just enough to counteract the numerical uncertainty without significantly perturbing the original problem. This represents a multi-layered, robust approach to the problem.\n**Verdict: Correct.**\n\n**E. If the exact $\\mathbf{G}_n$ is only positive semidefinite (rank deficient), then adding any $\\alpha > 0$ produces the same solution as the least-norm solution of the original normal equations, so diagonal loading does not introduce bias relative to the ideal APA solution.**\n\nThis statement is incorrect. The ideal APA solution for a rank-deficient $\\mathbf{G}_n$ is the minimum norm solution, which is given by using the Moore-Penrose pseudoinverse: $\\mathbf{p}_n = \\mathbf{G}_n^\\dagger \\mathbf{e}_n$. The regularized solution is $\\mathbf{p}_{n, \\alpha} = (\\mathbf{G}_n + \\alpha \\mathbf{I})^{-1} \\mathbf{e}_n$. It is a well-known result from regularization theory that $\\lim_{\\alpha \\to 0^+} \\mathbf{p}_{n, \\alpha} = \\mathbf{p}_n$. However, for any finite, non-zero $\\alpha$, the solutions are not identical: $\\mathbf{p}_{n, \\alpha} \\ne \\mathbf{p}_n$. The regularization introduces a bias that shrinks the solution vector components, a phenomenon central to ridge regression and Tikhonov regularization. The claim that the solutions are the same for *any* $\\alpha > 0$ is false.\n**Verdict: Incorrect.**\n\nIn summary, statements A, B, and D correctly describe the numerical pathologies and their standard, robust mitigation techniques.", "answer": "$$\\boxed{ABD}$$", "id": "2850768"}, {"introduction": "Having identified the numerical weaknesses of the Gram-matrix approach, we now explore a superior, more robust implementation strategy. This practice introduces a method based on QR factorization, which avoids explicitly forming the potentially ill-conditioned Gram matrix, thereby preserving numerical accuracy [@problem_id:2850737]. By analyzing the computational steps and costs, you will understand the trade-offs involved in designing a stable and efficient APA for real-world applications.", "problem": "Consider an Affine Projection Algorithm (APA) adaptive filter of length $M$ and projection order $P$ that updates a weight vector $w(n) \\in \\mathbb{R}^{M}$ using the most recent $P$ input regressors. At each iteration, the direct form of the update requires solving a symmetric positive definite linear system of dimension $P$ whose coefficient matrix is constructed from inner products of the most recent input regressors together with a small diagonal loading for regularization, and whose right-hand side is the vector of $P$ a priori errors. You are asked to choose an implementation that avoids computing any explicit matrix inverse and that leverages an orthogonal-triangular (QR) factorization to enhance numerical robustness.\n\nStarting only from the following fundamentals:\n- The least-squares normal equations for a tall matrix $A \\in \\mathbb{R}^{m \\times n}$, $m \\ge n$, are $A^{\\top} A x = A^{\\top} b$, and forming $A^{\\top} A$ squares the condition number of $A$.\n- A thin QR factorization of a tall matrix $B \\in \\mathbb{R}^{m \\times n}$, $m \\ge n$, writes $B = Q R$ with $Q \\in \\mathbb{R}^{m \\times n}$ having orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ upper-triangular, and satisfies $B^{\\top} B = R^{\\top} R$.\n- Solving a triangular system of size $P$ by back-substitution costs on the order of $\\mathcal{O}(P^{2})$ operations, while a Householder QR of an $m \\times n$ matrix with $m \\ge n$ costs approximately $2 m n^{2} - \\tfrac{2}{3} n^{3}$ floating-point operations.\n\nWhich option best describes a QR-based per-iteration implementation of the APA that avoids explicit matrix inversion, together with its main numerical advantages and computational costs?\n\nA. Build the $M \\times P$ regressor matrix $X(n)$ and the $P \\times 1$ a priori error vector $e(n)$. Form the $(M+P) \\times P$ augmented matrix $B(n) \\triangleq \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix}$ with a small diagonal loading $\\epsilon > 0$. Compute the thin QR factorization $B(n) = Q(n) R(n)$ using orthogonal transformations (e.g., Householder reflectors or Givens rotations), so that $R(n)^{\\top} R(n) = B(n)^{\\top} B(n) = X(n)^{\\top} X(n) + \\epsilon I_{P}$. Solve the two triangular systems $R(n)^{\\top} z(n) = e(n)$ and then $R(n) g(n) = z(n)$, and update $w(n+1) = w(n) + \\mu X(n) g(n)$ with stepsize $\\mu \\in (0,1]$. From-scratch per-iteration cost is approximately $2 (M+P) P^{2} - \\tfrac{2}{3} P^{3}$ for the QR, plus $\\mathcal{O}(P^{2})$ for the triangular solves and $\\mathcal{O}(M P)$ for the projection, i.e., $\\mathcal{O}(M P^{2} + P^{3})$, but with sliding-window QR updates/downdates via Givens rotations the amortized cost can be reduced toward $\\mathcal{O}(M P)$. Advantages: no explicit inverse, no explicit forming of $X(n)^{\\top} X(n)$, improved numerical stability under collinearity, and graceful handling of near-rank deficiency due to regularization; trade-offs: higher constants than naive methods and extra storage for $R(n)$ and update operators.\n\nB. At each iteration, form the $P \\times P$ Gram matrix $G(n) \\triangleq X(n)^{\\top} X(n) + \\epsilon I_{P}$, compute its eigenvalue decomposition $G(n) = U(n) \\Lambda(n) U(n)^{\\top}$, and then set $g(n) = U(n) \\Lambda(n)^{-1} U(n)^{\\top} e(n)$ to avoid an explicit inverse. This is as stable as QR while being cheaper per iteration, because diagonal inversion dominates and costs only $\\mathcal{O}(P)$; the total per-iteration cost is $\\mathcal{O}(P^{2})$ independent of $M$.\n\nC. Form the $P \\times P$ Gram matrix $G(n) \\triangleq X(n)^{\\top} X(n) + \\epsilon I_{P}$ and compute its Cholesky factorization $G(n) = L(n) L(n)^{\\top}$. Compute the explicit inverse $G(n)^{-1}$ by inverting $L(n)$ and then compute $g(n) = G(n)^{-1} e(n)$. This yields per-iteration complexity $\\mathcal{O}(P^{2})$ because triangular inversion is cheap, and it is numerically equivalent to QR without the overhead of orthogonal transformations.\n\nD. Maintain a thin QR factorization $X(n) = \\tilde{Q}(n) \\tilde{R}(n)$ and, at each iteration, update $\\tilde{R}(n)$ by appending the newest column and removing the oldest via Givens rotations. Then solve $\\tilde{R}(n) g(n) = e(n)$ by a single back-substitution and update $w(n+1) = w(n) + \\mu X(n) g(n)$. The regularization is unnecessary if the QR is maintained, and the cost is $\\mathcal{O}(P^{2})$ per iteration independent of $M$ due to the triangular solve.", "solution": "### Derivation of the QR-based APA Implementation\n\nThe APA weight update is given by $w(n+1) = w(n) + \\mu X(n) g(n)$, where $X(n) \\in \\mathbb{R}^{M \\times P}$ is the matrix of the $P$ most recent input regressors. The vector $g(n) \\in \\mathbb{R}^{P}$ is the solution to the regularized normal equations:\n$$ (X(n)^{\\top} X(n) + \\epsilon I_{P}) g(n) = e(n) $$\nwhere $e(n) \\in \\mathbb{R}^{P}$ is the a priori error vector, $\\epsilon > 0$ is the regularization parameter, and $I_{P}$ is the $P \\times P$ identity matrix.\n\nThe primary objective is to solve this system for $g(n)$ without explicitly forming the matrix $X(n)^{\\top} X(n)$, as this operation squares the condition number, leading to potential numerical instability (as stated in Principle 1). We must use a QR-based method.\n\nLet us construct an augmented matrix $B(n)$ such that its Gram matrix $B(n)^{\\top} B(n)$ is precisely the coefficient matrix of our linear system. Consider the $(M+P) \\times P$ matrix:\n$$ B(n) = \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix} $$\nComputing its Gram matrix gives:\n$$ B(n)^{\\top} B(n) = \\begin{bmatrix} X(n)^{\\top} & \\sqrt{\\epsilon} I_{P} \\end{bmatrix} \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix} = X(n)^{\\top} X(n) + (\\sqrt{\\epsilon} I_{P})(\\sqrt{\\epsilon} I_{P}) = X(n)^{\\top} X(n) + \\epsilon I_{P} $$\nThis successfully maps our problem onto the structure of normal equations without requiring the explicit formation of the product. The original system can now be written as:\n$$ B(n)^{\\top} B(n) g(n) = e(n) $$\nTo solve this using QR factorization (Principle 2), we compute the thin QR factorization of $B(n)$:\n$$ B(n) = Q(n) R(n) $$\nwhere $Q(n)$ is an $(M+P) \\times P$ matrix with orthonormal columns ($Q(n)^{\\top} Q(n) = I_{P}$) and $R(n)$ is a $P \\times P$ upper-triangular matrix.\n\nSubstituting this into our system:\n$$ (Q(n) R(n))^{\\top} (Q(n) R(n)) g(n) = e(n) $$\n$$ R(n)^{\\top} Q(n)^{\\top} Q(n) R(n) g(n) = e(n) $$\n$$ R(n)^{\\top} R(n) g(n) = e(n) $$\nThis system can be solved efficiently without any matrix inversions by using a two-step procedure involving triangular solves:\n1. Define an intermediate vector $z(n) = R(n) g(n)$. The system becomes $R(n)^{\\top} z(n) = e(n)$. Since $R(n)^{\\top}$ is lower-triangular, we can solve for $z(n)$ using forward substitution.\n2. With $z(n)$ known, we solve $R(n) g(n) = z(n)$ for $g(n)$. Since $R(n)$ is upper-triangular, we can use back substitution.\n\nThe computational cost of this procedure is dominated by the QR factorization of the $(M+P) \\times P$ matrix $B(n)$. Using the provided formula (Principle 4) with $m = M+P$ and $n = P$, the cost is approximately $2(M+P)P^2 - \\frac{2}{3}P^3$, which is of order $\\mathcal{O}(MP^2 + P^3)$. The two triangular solves cost $\\mathcal{O}(P^2)$ each (Principle 3). The final weight update $w(n+1) = w(n) + \\mu X(n) g(n)$ involves a matrix-vector product costing $\\mathcal{O}(MP)$. Thus, the total per-iteration cost if computed from scratch is $\\mathcal{O}(MP^2 + P^3)$. For a sliding-window regressor matrix $X(n)$, efficient update/downdate procedures for the QR factorization (e.g., using Givens rotations) can reduce the amortized cost to $\\mathcal{O}(MP)$.\n\n### Evaluation of Options\n\n**Option A:**\nThis option describes the exact procedure derived above. It correctly forms the augmented matrix $B(n) = \\begin{bmatrix} X(n) \\\\ \\sqrt{\\epsilon} I_{P} \\end{bmatrix}$, computes its thin QR factorization $B(n) = Q(n) R(n)$, and solves the system $R(n)^{\\top} R(n) g(n) = e(n)$ via the two-step triangular solve: first $R(n)^{\\top} z(n) = e(n)$ and then $R(n) g(n) = z(n)$. The analysis of numerical advantages (avoiding explicit inverse and Gram matrix formation, improved stability) is correct. The computational cost analysis, both for the from-scratch version ($ \\mathcal{O}(M P^{2} + P^{3})$) and the fast-update version ($\\mathcal{O}(M P)$), is also correct. The trade-offs are accurately stated.\n**Verdict: Correct**\n\n**Option B:**\nThis option proposes to form the Gram matrix $G(n) = X(n)^{\\top} X(n) + \\epsilon I_{P}$ explicitly. This contradicts the goal of avoiding the numerically problematic squaring of the condition number. It uses eigenvalue decomposition (EVD) instead of the required QR factorization. The cost analysis is incorrect: forming $G(n)$ costs $\\mathcal{O}(MP^2)$, so the total cost is not independent of $M$. EVD of a $P \\times P$ matrix costs $\\mathcal{O}(P^3)$, not $\\mathcal{O}(P^2)$.\n**Verdict: Incorrect**\n\n**Option C:**\nThis option also proposes forming the Gram matrix $G(n)$ explicitly, which is undesirable. It suggests using Cholesky factorization, not QR factorization. Most importantly, it proposes to \"compute the explicit inverse $G(n)^{-1}$\", which directly violates a core constraint of the problem. Further, its claim of being \"numerically equivalent to QR\" is false; QR on the data matrix $B(n)$ is more stable than Cholesky on the Gram matrix $B(n)^{\\top}B(n)$. The complexity is also misstated as $\\mathcal{O}(P^2)$, ignoring the $\\mathcal{O}(MP^2)$ cost of forming $G(n)$.\n**Verdict: Incorrect**\n\n**Option D:**\nThis option correctly suggests maintaining a QR factorization of $X(n)$ and using fast updates. However, it makes a critical error in formulating the system to be solved. It claims the system is $\\tilde{R}(n) g(n) = e(n)$, but the correct system, derived from $(X(n)^{\\top} X(n) + \\epsilon I_{P}) g(n) = e(n)$, is $(\\tilde{R}(n)^{\\top} \\tilde{R}(n) + \\epsilon I_{P}) g(n) = e(n)$. Option D completely ignores the regularization term $\\epsilon I_P$ in the solution step and makes the false assertion that \"regularization is unnecessary if the QR is maintained\". Regularization is crucial for handling collinearity in regressors, which QR factorization diagnoses but does not by itself resolve without modifying the problem. The complexity analysis is also flawed; updates cost $\\mathcal{O}(MP)$, not a cost independent of $M$.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "2850737"}]}