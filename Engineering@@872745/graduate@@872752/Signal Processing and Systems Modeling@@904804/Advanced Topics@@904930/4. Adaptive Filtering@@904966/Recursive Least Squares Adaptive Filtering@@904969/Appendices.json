{"hands_on_practices": [{"introduction": "The power of the Recursive Least Squares algorithm lies in its efficient, step-by-step update mechanism. Before implementing the full filter, it is essential to master the mechanics of a single iteration. This exercise ([@problem_id:2899717]) provides a hands-on opportunity to walk through the derivation and perform a concrete numerical calculation of one RLS update, solidifying your understanding of the core formulas at play.", "problem": "Consider a linear-in-parameters model where the measured scalar output at discrete time index $k$ is modeled as $y_k \\approx \\varphi_k^{\\top} \\theta$, with a time-varying regressor vector $\\varphi_k \\in \\mathbb{R}^n$ and an unknown parameter vector $\\theta \\in \\mathbb{R}^n$. Define the exponentially weighted least squares cost at time $k$ by\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi_i^{\\top}\\theta\\big)^2,\n$$\nwhere the forgetting factor satisfies $0\\lambda\\leq 1$. Let the weighted normal-equation matrix and cross-correlation vector be\n$$\nR_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i \\varphi_i^{\\top}, \\qquad r_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i y_i,\n$$\nso that the minimizer satisfies $R_k \\hat{\\theta}_k = r_k$. Let $P_k \\triangleq R_k^{-1}$, and assume $R_k$ is nonsingular.\n\nStarting from the cost definition $J_k(\\theta)$ and the normal equations, derive the one-step recursive least squares update for the parameter estimate $\\hat{\\theta}_k$ in terms of $P_{k-1}$, $\\hat{\\theta}_{k-1}$, $\\varphi_k$, $y_k$, and $\\lambda$. Then evaluate the update numerically for the following data (all quantities are exact):\n- Dimension $n=2$,\n- Forgetting factor $\\lambda = \\tfrac{1}{2}$,\n- Previous inverse normal-equation matrix\n$$\nP_{k-1} = \\begin{pmatrix} 2  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  1 \\end{pmatrix},\n$$\n- Previous parameter estimate\n$$\n\\hat{\\theta}_{k-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\n$$\n- Current regressor and output\n$$\n\\varphi_k = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\qquad y_k = 5.\n$$\n\nReport the updated parameter estimate vector $\\hat{\\theta}_k$ in its simplest exact form. Do not round. Your final answer must be written as a single row matrix.", "solution": "The problem presented is a standard exercise in adaptive filtering, specifically concerning the derivation and application of the recursive least squares (RLS) algorithm. It is scientifically sound, well-posed, and contains all necessary information. I shall proceed to solve it.\n\nThe problem requires two parts: first, a formal derivation of the one-step update for the parameter estimate $\\hat{\\theta}_k$, and second, a numerical evaluation using the provided data.\n\nFirst, we derive the recursive update for $\\hat{\\theta}_k$. The foundation of the derivation lies in the recursive nature of the normal-equation matrix $R_k$ and the cross-correlation vector $r_k$.\n\nFrom their definitions:\n$$\nR_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i \\varphi_i^{\\top}\n$$\n$$\nr_k \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\varphi_i y_i\n$$\nWe can separate the term for index $k$ from the sum:\n$$\nR_k = \\lambda \\left( \\sum_{i=1}^{k-1} \\lambda^{k-1-i} \\varphi_i \\varphi_i^{\\top} \\right) + \\varphi_k \\varphi_k^{\\top}\n$$\n$$\nr_k = \\lambda \\left( \\sum_{i=1}^{k-1} \\lambda^{k-1-i} \\varphi_i y_i \\right) + \\varphi_k y_k\n$$\nThis reveals the recursive relationships:\n$$\nR_k = \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top}\n$$\n$$\nr_k = \\lambda r_{k-1} + \\varphi_k y_k\n$$\nThe least-squares estimate $\\hat{\\theta}_k$ is the solution to the normal equation $R_k \\hat{\\theta}_k = r_k$. Substituting the recursive expressions, we have:\n$$\n( \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top} ) \\hat{\\theta}_k = \\lambda r_{k-1} + \\varphi_k y_k\n$$\nUsing the previous estimate's normal equation, $r_{k-1} = R_{k-1} \\hat{\\theta}_{k-1}$, we substitute for $r_{k-1}$:\n$$\n( \\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top} ) \\hat{\\theta}_k = \\lambda R_{k-1} \\hat{\\theta}_{k-1} + \\varphi_k y_k\n$$\nWe now rearrange this equation to express $\\hat{\\theta}_k$ as a correction to $\\hat{\\theta}_{k-1}$. We can substitute $R_k$ on the left side and rewrite $\\lambda R_{k-1}$ on the right side as $R_k - \\varphi_k \\varphi_k^{\\top}$:\n$$\nR_k \\hat{\\theta}_k = (R_k - \\varphi_k \\varphi_k^{\\top}) \\hat{\\theta}_{k-1} + \\varphi_k y_k\n$$\nRearranging terms yields:\n$$\nR_k \\hat{\\theta}_k - R_k \\hat{\\theta}_{k-1} = \\varphi_k y_k - \\varphi_k \\varphi_k^{\\top} \\hat{\\theta}_{k-1}\n$$\n$$\nR_k (\\hat{\\theta}_k - \\hat{\\theta}_{k-1}) = \\varphi_k (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\nLeft-multiplying by $P_k = R_k^{-1}$ gives the update equation:\n$$\n\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + P_k \\varphi_k (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\nThis equation depends on $P_k$. To express the update in terms of $P_{k-1}$, we must find a recursive formula for $P_k$. Using the recursive formula for $R_k$ and the Sherman-Morrison-Woodbury matrix inversion lemma, we have:\n$$\nP_k = R_k^{-1} = (\\lambda R_{k-1} + \\varphi_k \\varphi_k^{\\top})^{-1} = \\frac{1}{\\lambda} \\left( P_{k-1} - \\frac{P_{k-1} \\varphi_k \\varphi_k^{\\top} P_{k-1}}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} \\right)\n$$\nThe gain vector in the update for $\\hat{\\theta}_k$ is $K_k \\triangleq P_k \\varphi_k$. Let's derive its expression in terms of $P_{k-1}$:\n$$\nK_k = P_k \\varphi_k = \\frac{1}{\\lambda} \\left( P_{k-1} - \\frac{P_{k-1} \\varphi_k \\varphi_k^{\\top} P_{k-1}}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} \\right) \\varphi_k\n$$\nLet the scalar term be $s_k = \\varphi_k^{\\top} P_{k-1} \\varphi_k$.\n$$\nK_k = \\frac{1}{\\lambda} \\left( P_{k-1} \\varphi_k - \\frac{(P_{k-1} \\varphi_k) (\\varphi_k^{\\top} P_{k-1} \\varphi_k)}{\\lambda + s_k} \\right) = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( 1 - \\frac{s_k}{\\lambda + s_k} \\right)\n$$\n$$\nK_k = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( \\frac{\\lambda + s_k - s_k}{\\lambda + s_k} \\right) = \\frac{1}{\\lambda} P_{k-1} \\varphi_k \\left( \\frac{\\lambda}{\\lambda + s_k} \\right) = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k}\n$$\nSubstituting this result for the gain vector into the update for $\\hat{\\theta}_k$ yields the complete RLS update algorithm based on quantities from time $k-1$:\n$$\n\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} (y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1})\n$$\nThis completes the derivation.\n\nSecond, we perform the numerical calculation. The given data are:\n$$\n\\lambda = \\frac{1}{2}, \\quad P_{k-1} = \\begin{pmatrix} 2  \\frac{1}{2} \\\\ \\frac{1}{2}  1 \\end{pmatrix}, \\quad \\hat{\\theta}_{k-1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad \\varphi_k = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad y_k = 5\n$$\nWe proceed step by step.\n\n1.  Calculate the *a priori* estimation error, $e_k = y_k - \\varphi_k^{\\top} \\hat{\\theta}_{k-1}$:\n    $$\n    \\varphi_k^{\\top} \\hat{\\theta}_{k-1} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (1)(0) + (2)(1) = 2\n    $$\n    $$\n    e_k = 5 - 2 = 3\n    $$\n\n2.  Calculate the numerator of the gain vector, $P_{k-1} \\varphi_k$:\n    $$\n    P_{k-1} \\varphi_k = \\begin{pmatrix} 2  \\frac{1}{2} \\\\ \\frac{1}{2}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2(1) + \\frac{1}{2}(2) \\\\ \\frac{1}{2}(1) + 1(2) \\end{pmatrix} = \\begin{pmatrix} 2 + 1 \\\\ \\frac{1}{2} + 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix}\n    $$\n\n3.  Calculate the denominator of the gain vector, $\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k$:\n    $$\n    \\varphi_k^{\\top} P_{k-1} \\varphi_k = \\varphi_k^{\\top} (P_{k-1} \\varphi_k) = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix} = (1)(3) + (2)\\left(\\frac{5}{2}\\right) = 3 + 5 = 8\n    $$\n    $$\n    \\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k = \\frac{1}{2} + 8 = \\frac{17}{2}\n    $$\n\n4.  Calculate the gain vector, $K_k$:\n    $$\n    K_k = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k} = \\frac{\\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix}}{\\frac{17}{2}} = \\frac{2}{17} \\begin{pmatrix} 3 \\\\ \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{5}{17} \\end{pmatrix}\n    $$\n\n5.  Finally, compute the updated parameter estimate, $\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + K_k e_k$:\n    $$\n    \\hat{\\theta}_k = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{5}{17} \\end{pmatrix} (3) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{15}{17} \\end{pmatrix}\n    $$\n    $$\n    \\hat{\\theta}_k = \\begin{pmatrix} \\frac{18}{17} \\\\ 1 + \\frac{15}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{17}{17} + \\frac{15}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{17} \\\\ \\frac{32}{17} \\end{pmatrix}\n    $$\nThe updated parameter estimate vector is $\\hat{\\theta}_k$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{18}{17}  \\frac{32}{17} \\end{pmatrix}\n}\n$$", "id": "2899717"}, {"introduction": "With the single-step update understood, we can now apply the RLS algorithm to a practical task: online system identification. This practice ([@problem_id:2899673]) challenges you to implement the full RLS recursion to estimate the parameters of an Autoregressive with Exogenous input (ARX) model from synthetic data. By coding the algorithm and observing its convergence, you will gain a practical appreciation for how RLS is used to model dynamic systems.", "problem": "You are given an Autoregressive with Exogenous input (ARX) model driven by a known input. The data-generating model is\n$$\ny_k \\;=\\; -\\sum_{i=1}^{p} a_i\\,y_{k-i} \\;+\\; \\sum_{j=1}^{q} b_j\\,u_{k-j} \\;+\\; e_k,\n$$\nwhere $y_k$ is the output, $u_k$ is the known input, $e_k$ is white noise, and the unknown parameter vector to estimate is\n$$\n\\theta \\;=\\; [\\,a_1,\\dots,a_p,\\,b_1,\\dots,b_q\\,]^{\\top}.\n$$\nAssume $y_k=0$ and $u_k=0$ for all negative indices. The white noise sequence is independent and identically distributed Gaussian with zero mean and variance $\\sigma^2$, that is $e_k \\sim \\mathcal{N}(0,\\sigma^2)$, and is independent of the input process.\n\nYour task is to implement a Recursive Least Squares (RLS) estimator with forgetting factor to estimate $\\theta$ from synthetic data generated by the model. Construct the regressor vector at each time $k$ as\n$$\n\\varphi_k \\;=\\; \\big[\\, -y_{k-1},\\,-y_{k-2},\\,\\dots,\\,-y_{k-p},\\; u_{k-1},\\,u_{k-2},\\,\\dots,\\,u_{k-q}\\,\\big]^{\\top},\n$$\nso that the one-step linear predictor is $\\widehat{y}_k = \\varphi_k^{\\top}\\,\\widehat{\\theta}_{k-1}$. Initialize the RLS recursion with $\\widehat{\\theta}_0 = 0 \\in \\mathbb{R}^{p+q}$ and $P_0 = \\delta I_{p+q}$. Use a forgetting factor $\\lambda \\in (0,1]$.\n\nYou must generate the synthetic datasets as follows in each test case:\n- Generate the input sequence $u_k$ as independent Gaussian samples with zero mean and unit variance, using the specified random seed.\n- Generate the noise sequence $e_k$ as independent Gaussian samples with zero mean and variance $\\sigma^2$, using the specified random seed (if $\\sigma=0$, set $e_k=0$ deterministically).\n- Form the output $y_k$ sequentially using the ARX recursion above and the stated initial conditions.\n- Run the RLS recursion for $N$ iterations, processing samples $k=0,1,\\dots,N-1$, and report the final estimate $\\widehat{\\theta}_N$.\n\nImportant requirements:\n- For each test case, output the final estimated parameter vector $\\widehat{\\theta}_N$ ordered as $[a_1,\\dots,a_p,b_1,\\dots,b_q]$.\n- Round each element of each estimated parameter vector to $6$ decimal places.\n- There are no physical units in this problem.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the rounded list of estimates for the corresponding test case. For example, a valid output format is\n$[ [\\dots], [\\dots], [\\dots], [\\dots] ]$,\nwith no additional text printed.\n\nTest suite:\n- Test case $1$ (happy path, moderate noise, strong prior):\n  - $p=1$, $q=1$,\n  - true $a_1 = 0.7$, true $b_1 = 0.9$,\n  - forgetting factor $\\lambda = 0.99$,\n  - initial covariance scale $\\delta = 1000$,\n  - noise standard deviation $\\sigma = 0.05$,\n  - number of samples $N = 80$,\n  - input seed $= 12345$, noise seed $= 54321$.\n- Test case $2$ (no forgetting, noise-free):\n  - $p=2$, $q=1$,\n  - true $[a_1,a_2] = [0.4,-0.15]$, true $[b_1] = [1.2]$,\n  - forgetting factor $\\lambda = 1.0$,\n  - initial covariance scale $\\delta = 1000$,\n  - noise standard deviation $\\sigma = 0.0$,\n  - number of samples $N = 120$,\n  - input seed $= 111$, noise seed $= 222$.\n- Test case $3$ (pure FIR, $p=0$ edge case, mild noise, strong forgetting):\n  - $p=0$, $q=2$,\n  - true $[b_1,b_2] = [0.3,-0.1]$,\n  - forgetting factor $\\lambda = 0.98$,\n  - initial covariance scale $\\delta = 1000$,\n  - noise standard deviation $\\sigma = 0.1$,\n  - number of samples $N = 60$,\n  - input seed $= 7$, noise seed $= 8$.\n- Test case $4$ (higher noise, near-unity forgetting, smaller prior scale):\n  - $p=1$, $q=2$,\n  - true $[a_1] = [0.3]$, true $[b_1,b_2] = [0.5,0.2]$,\n  - forgetting factor $\\lambda = 0.995$,\n  - initial covariance scale $\\delta = 10$,\n  - noise standard deviation $\\sigma = 0.5$,\n  - number of samples $N = 150$,\n  - input seed $= 999$, noise seed $= 1001$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the rounded list of estimated parameters for one test case, in the same order as listed above, for example\n$[[\\text{case1\\_estimates}],[\\text{case2\\_estimates}],[\\text{case3\\_estimates}],[\\text{case4\\_estimates}]]$.", "solution": "The problem presented is a standard exercise in system identification, specifically, the estimation of parameters for an Autoregressive with Exogenous input (ARX) model using a Recursive Least Squares (RLS) adaptive filter. The problem is well-defined, scientifically sound, and internally consistent. It provides all necessary parameters, initial conditions, and procedural steps for both data generation and estimation. Therefore, the problem is valid, and a solution will be provided.\n\nThe core of the problem lies in applying the RLS algorithm to identify the parameters of a linear system from its input-output data. The system is described by the ARX model:\n$$\ny_k = -\\sum_{i=1}^{p} a_i y_{k-i} + \\sum_{j=1}^{q} b_j u_{k-j} + e_k\n$$\nwhere $k$ is the discrete time index, $y_k$ is the system output, $u_k$ is the exogenous input, and $e_k$ is a zero-mean white noise process. The goal is to estimate the unknown parameter vector $\\theta$, which contains the model coefficients:\n$$\n\\theta = [a_1, \\dots, a_p, b_1, \\dots, b_q]^{\\top} \\in \\mathbb{R}^{p+q}\n$$\nTo facilitate a linear regression framework, we define a regressor vector $\\varphi_k$ that contains the past output and input values:\n$$\n\\varphi_k = [-y_{k-1}, -y_{k-2}, \\dots, -y_{k-p}, u_{k-1}, u_{k-2}, \\dots, u_{k-q}]^{\\top}\n$$\nWith this definition, the ARX model can be expressed in a compact linear form:\n$$\ny_k = \\varphi_k^{\\top} \\theta + e_k\n$$\nThis formulation states that the current output $y_k$ is a linear function of the regressor vector $\\varphi_k$ and the true parameter vector $\\theta$, corrupted by noise $e_k$. Our task is to find an estimate $\\widehat{\\theta}_k$ of $\\theta$ at each time step $k$.\n\nThe Recursive Least Squares algorithm provides an efficient method to solve the exponentially weighted least-squares problem, which aims to find the parameter estimate $\\widehat{\\theta}_k$ that minimizes the cost function:\n$$\nJ_k(\\theta) = \\sum_{i=1}^{k} \\lambda^{k-i} (y_i - \\varphi_i^{\\top} \\theta)^2 + \\lambda^k (\\theta - \\widehat{\\theta}_0)^{\\top} P_0^{-1} (\\theta - \\widehat{\\theta}_0)\n$$\nHere, $\\lambda \\in (0, 1]$ is the forgetting factor, which gives exponentially less weight to older data, allowing the algorithm to track time-varying parameters or discard the influence of initial conditions. The second term incorporates the prior knowledge about the parameters, where $\\widehat{\\theta}_0$ is the initial estimate and $P_0$ is the initial inverse covariance matrix, typically chosen as $P_0 = \\delta I$ for a large scalar $\\delta$.\n\nInstead of solving this minimization problem by batch processing at each time step, RLS provides a recursive update rule. Given the estimate $\\widehat{\\theta}_{k-1}$ and the inverse covariance matrix $P_{k-1}$ at time $k-1$, and a new data pair $(y_k, u_k)$, the algorithm computes the new estimate $\\widehat{\\theta}_k$ and matrix $P_k$.\n\nThe standard RLS algorithm proceeds as follows:\n\n1.  **Initialization** (at time $k=0$):\n    -   Parameter estimate: $\\widehat{\\theta}_0 = 0 \\in \\mathbb{R}^{p+q}$\n    -   Inverse covariance matrix: $P_0 = \\delta I_{p+q}$, where $I$ is the identity matrix and $\\delta$ is a large positive scalar.\n\n2.  **Recursion** (for each time step $k=1, 2, \\dots, N$ or, following the problem's indexing, $k=0, 1, \\dots, N-1$ for $N$ samples):\n    a. Form the regressor vector $\\varphi_k$ using past data $\\{y_{k-i}\\}_{i=1}^p$ and $\\{u_{k-j}\\}_{j=1}^q$. Per the problem statement, values for negative time indices are zero.\n    b. Calculate the *a priori* prediction error, $\\epsilon_k$, which is the difference between the measured output and the output predicted using the previous estimate $\\widehat{\\theta}_{k-1}$:\n    $$\n    \\epsilon_k = y_k - \\varphi_k^{\\top} \\widehat{\\theta}_{k-1}\n    $$\n    c. Compute the Kalman gain vector, $K_k$:\n    $$\n    K_k = \\frac{P_{k-1} \\varphi_k}{\\lambda + \\varphi_k^{\\top} P_{k-1} \\varphi_k}\n    $$\n    d. Update the parameter estimate vector. The new estimate is the old estimate plus a correction term proportional to the gain and the prediction error:\n    $$\n    \\widehat{\\theta}_k = \\widehat{\\theta}_{k-1} + K_k \\epsilon_k\n    $$\n    e. Update the inverse covariance matrix $P_k$. This step is derived from the matrix inversion lemma and simplifies the update of the inverse of the correlation matrix:\n    $$\n    P_k = \\frac{1}{\\lambda} (I - K_k \\varphi_k^{\\top}) P_{k-1}\n    $$\n\nThe implementation will first generate the synthetic data $(u_k, y_k)$ for $k=0, \\dots, N-1$ according to the specified ARX model, true parameters, and noise characteristics. The output $y_k$ must be generated sequentially as it depends on its own past values. Then, the RLS recursion is executed for $N$ iterations, processing one sample at each step, to obtain the final parameter estimate $\\widehat{\\theta}_N$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the RLS estimation problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 1, \"q\": 1, \"a\": [0.7], \"b\": [0.9],\n            \"lambda\": 0.99, \"delta\": 1000, \"sigma\": 0.05, \"N\": 80,\n            \"input_seed\": 12345, \"noise_seed\": 54321,\n        },\n        {\n            \"p\": 2, \"q\": 1, \"a\": [0.4, -0.15], \"b\": [1.2],\n            \"lambda\": 1.0, \"delta\": 1000, \"sigma\": 0.0, \"N\": 120,\n            \"input_seed\": 111, \"noise_seed\": 222,\n        },\n        {\n            \"p\": 0, \"q\": 2, \"a\": [], \"b\": [0.3, -0.1],\n            \"lambda\": 0.98, \"delta\": 1000, \"sigma\": 0.1, \"N\": 60,\n            \"input_seed\": 7, \"noise_seed\": 8,\n        },\n        {\n            \"p\": 1, \"q\": 2, \"a\": [0.3], \"b\": [0.5, 0.2],\n            \"lambda\": 0.995, \"delta\": 10, \"sigma\": 0.5, \"N\": 150,\n            \"input_seed\": 999, \"noise_seed\": 1001,\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p, q = case[\"p\"], case[\"q\"]\n        true_a = np.array(case[\"a\"])\n        true_b = np.array(case[\"b\"])\n        lam, delta, sigma, N = case[\"lambda\"], case[\"delta\"], case[\"sigma\"], case[\"N\"]\n        input_seed, noise_seed = case[\"input_seed\"], case[\"noise_seed\"]\n\n        # 1. Generate synthetic data\n        rng_u = np.random.default_rng(input_seed)\n        rng_e = np.random.default_rng(noise_seed)\n\n        u = rng_u.normal(0, 1, size=N)\n        if sigma == 0.0:\n            e = np.zeros(N)\n        else:\n            e = rng_e.normal(0, sigma, size=N)\n\n        y = np.zeros(N)\n        for k in range(N):\n            ar_term = 0.0\n            if p > 0:\n                for i in range(1, p + 1):\n                    if k - i >= 0:\n                        ar_term += true_a[i - 1] * y[k - i]\n            \n            x_term = 0.0\n            if q > 0:\n                for j in range(1, q + 1):\n                    if k - j >= 0:\n                        x_term += true_b[j - 1] * u[k - j]\n            \n            y[k] = -ar_term + x_term + e[k]\n\n        # 2. Run RLS estimation\n        dim = p + q\n        if dim == 0:\n            results.append([])\n            continue\n\n        theta_hat = np.zeros(dim)\n        P = delta * np.eye(dim)\n\n        for k in range(N):\n            # Construct regressor vector phi_k\n            phi = np.zeros(dim)\n            if p > 0:\n                for i in range(1, p + 1):\n                    if k - i >= 0:\n                        phi[i - 1] = -y[k - i]\n            if q > 0:\n                for j in range(1, q + 1):\n                    if k - j >= 0:\n                        phi[p + j - 1] = u[k - j]\n\n            # RLS update equations\n            phi = phi.reshape(-1, 1) # Ensure phi is a column vector\n            \n            # Gain vector K_k\n            P_phi = P @ phi\n            denominator = lam + (phi.T @ P_phi)\n            K = P_phi / denominator\n\n            # A priori prediction error\n            prediction_error = y[k] - (phi.T @ theta_hat.reshape(-1, 1))\n\n            # Update parameter estimate\n            theta_hat = theta_hat + (K * prediction_error).flatten()\n\n            # Update inverse covariance matrix\n            P = (1 / lam) * (P - K @ phi.T @ P)\n        \n        # 3. Format result\n        final_estimate = np.round(theta_hat, 6).tolist()\n        results.append(final_estimate)\n        \n    # Final output formatting\n    # Using map(str, results) correctly stringifies each inner list.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2899673"}, {"introduction": "While theoretically elegant, the standard RLS algorithm can suffer from numerical instability in finite-precision implementations, where the inverse correlation matrix $P_k$ may lose its required positive-definiteness. This advanced practice ([@problem_id:2899744]) introduces the square-root RLS algorithm, a more robust formulation that guarantees the positive-definiteness of $P_k$ by propagating its Cholesky factor. Mastering this technique is crucial for developing reliable, real-world adaptive filters.", "problem": "Consider a discrete-time linear regression model with regressor vector $x_k \\in \\mathbb{R}^n$ and parameter vector $\\theta_k \\in \\mathbb{R}^n$. At each time $k$, the exponentially weighted least squares problem is defined by the cost\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{\\,k-i} \\left(d_i - x_i^{\\top}\\theta\\right)^2,\n$$\nwhere $d_i \\in \\mathbb{R}$ is the observed scalar output and $\\lambda \\in (0,1]$ is the forgetting factor. Let $R_k \\triangleq \\sum_{i=1}^{k} \\lambda^{\\,k-i} x_i x_i^{\\top}$ denote the weighted correlation matrix and $P_k \\triangleq R_k^{-1}$ its inverse. You will maintain a lower-triangular Cholesky factor $S_k \\in \\mathbb{R}^{n \\times n}$ such that $P_k = S_k S_k^{\\top}$.\n\nTask 1 (Derivation): Starting from the definition of $J_k(\\theta)$ and the associated normal equations, derive a recursion for $P_k$ in terms of $P_{k-1}$, $\\lambda$, and $x_k$ without inverting any full matrix at time $k$. Then, by introducing a Cholesky factor $S_{k-1}$ such that $P_{k-1} = S_{k-1} S_{k-1}^{\\top}$ and invoking the existence of orthogonal transformations (for example, Givens rotations or Householder reflections) that preserve the Euclidean norm, show that the update from $S_{k-1}$ to $S_k$ can be realized as a single rank-one downdate of the scaled factor $\\lambda^{-1/2} S_{k-1}$. Your derivation must make clear why the new information enters as a norm-preserving annihilation of a single vector and why this preserves positive definiteness.\n\nTask 2 (Application): Specialize to $n=2$ with forgetting factor $\\lambda = 1$. Let the initial factor be $S_0 = 10 I_2$, so that $P_0 = S_0 S_0^{\\top} = 100 I_2$. Process the two regressors\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix},\\qquad x_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\nsequentially using your square-root update, obtaining $S_1$ and then $S_2$. You do not need the outputs $d_1$ or $d_2$ for this computation. Compute the $(1,1)$ entry of $S_2$, denoted $[S_2]_{11}$. Express your final answer as an exact closed-form expression. No rounding is required.", "solution": "This problem consists of two tasks: first, to derive the square-root RLS update, and second, to apply it to a specific numerical example.\n\n### Task 1: Derivation\n\nThe solution to the exponentially weighted least squares problem is found by setting the gradient of the cost function $J_k(\\theta)$ with respect to $\\theta$ to zero. This yields the normal equations, whose matrix part is the weighted correlation matrix $R_k$:\n$$\nR_k = \\sum_{i=1}^{k} \\lambda^{\\,k-i} x_i x_i^{\\top}\n$$\nThis matrix can be written recursively as:\n$$\nR_k = \\lambda R_{k-1} + x_k x_k^{\\top}\n$$\nTo derive a recursion for the inverse matrix $P_k = R_k^{-1}$, we apply the Sherman-Morrison-Woodbury matrix identity to the expression for $R_k$. This yields the standard RLS update for the inverse correlation matrix:\n$$\nP_k = \\frac{1}{\\lambda} \\left[ P_{k-1} - \\frac{P_{k-1} x_k x_k^{\\top} P_{k-1}}{\\lambda + x_k^{\\top} P_{k-1} x_k} \\right]\n$$\nThis update involves a subtraction, which can be numerically unstable. The square-root algorithm avoids this by propagating a Cholesky factor $S_k$ of $P_k$, where $P_k = S_k S_k^{\\top}$. Substituting $P_{k-1} = S_{k-1} S_{k-1}^{\\top}$ into the recursion gives:\n$$\nS_k S_k^{\\top} = \\frac{1}{\\lambda} \\left[ S_{k-1} S_{k-1}^{\\top} - \\frac{S_{k-1} S_{k-1}^{\\top} x_k x_k^{\\top} S_{k-1} S_{k-1}^{\\top}}{\\lambda + x_k^{\\top} S_{k-1} S_{k-1}^{\\top} x_k} \\right]\n$$\nBy defining an auxiliary vector $v_k = S_{k-1}^{\\top} x_k$ and a scaled factor $F_{k-1} \\triangleq \\lambda^{-1/2} S_{k-1}$, we can rewrite this as a rank-one downdate:\n$$\nS_k S_k^{\\top} = F_{k-1} \\left( I - \\frac{v_k v_k^{\\top}}{\\lambda + \\|v_k\\|^2} \\right) F_{k-1}^{\\top}\n$$\nThis shows that updating from the scaled prior factor $F_{k-1}$ to the posterior factor $S_k$ is equivalent to performing a rank-one downdate. Numerically stable algorithms, such as those using Givens rotations, can compute $S_k$ from $F_{k-1}$ and $v_k$ without explicit subtraction. These methods rely on finding an orthogonal transformation that triangularizes a specific augmented matrix. Since orthogonal transformations preserve Euclidean norms, the process is numerically robust. The downdated matrix remains positive definite because the norm of the vector being subtracted is less than one (since $\\lambda > 0$), ensuring that the new Cholesky factor $S_k$ is always real and well-defined.\n\n### Task 2: Application\n\nWe are given $n=2$, $\\lambda=1$, $S_0 = 10 I_2$, and regressors $x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $x_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$. We need to find $[S_2]_{11}$.\n\nInstead of performing the complex square-root downdating steps, we can solve this specific problem more directly by computing the final correlation matrix $R_2$ and then finding its inverse's Cholesky factor.\n\nThe initial inverse correlation matrix is $P_0 = S_0 S_0^{\\top} = (10 I_2)(10 I_2)^{\\top} = 100 I_2$.\nThe initial correlation matrix is therefore $R_0 = P_0^{-1} = (100 I_2)^{-1} = 0.01 I_2$.\n\nWe process the data points sequentially using the recursion $R_k = \\lambda R_{k-1} + x_k x_k^{\\top}$. With $\\lambda=1$:\n\nFor $k=1$:\n$$\nR_1 = R_0 + x_1 x_1^{\\top} = \\begin{pmatrix} 0.01  0 \\\\ 0  0.01 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1  2 \\end{pmatrix} = \\begin{pmatrix} 0.01  0 \\\\ 0  0.01 \\end{pmatrix} + \\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix} = \\begin{pmatrix} 1.01  2 \\\\ 2  4.01 \\end{pmatrix}\n$$\nFor $k=2$:\n$$\nR_2 = R_1 + x_2 x_2^{\\top} = \\begin{pmatrix} 1.01  2 \\\\ 2  4.01 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 2  -1 \\end{pmatrix} = \\begin{pmatrix} 1.01  2 \\\\ 2  4.01 \\end{pmatrix} + \\begin{pmatrix} 4  -2 \\\\ -2  1 \\end{pmatrix}\n$$\nThe calculation simplifies to:\n$$\nR_2 = \\begin{pmatrix} 5.01  0 \\\\ 0  5.01 \\end{pmatrix} = 5.01 I_2\n$$\nNow we find $P_2 = R_2^{-1}$:\n$$\nP_2 = (5.01 I_2)^{-1} = \\frac{1}{5.01} I_2\n$$\nThe problem requires the lower-triangular Cholesky factor $S_2$ such that $S_2 S_2^{\\top} = P_2$. Since $P_2$ is a diagonal matrix, its unique lower-triangular Cholesky factor is also a diagonal matrix with the square roots of the diagonal entries.\n$$\nS_2 = \\begin{pmatrix} \\sqrt{\\frac{1}{5.01}}  0 \\\\ 0  \\sqrt{\\frac{1}{5.01}} \\end{pmatrix}\n$$\nThe $(1,1)$ entry of $S_2$ is $[S_2]_{11} = \\sqrt{\\frac{1}{5.01}}$.\nAs an exact closed-form expression:\n$$\n[S_2]_{11} = \\sqrt{\\frac{1}{501/100}} = \\sqrt{\\frac{100}{501}} = \\frac{10}{\\sqrt{501}}\n$$", "answer": "$$\\boxed{\\frac{10}{\\sqrt{501}}}$$", "id": "2899744"}]}