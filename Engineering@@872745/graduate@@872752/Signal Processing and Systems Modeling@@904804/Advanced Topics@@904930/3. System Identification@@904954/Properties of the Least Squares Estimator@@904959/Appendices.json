{"hands_on_practices": [{"introduction": "The least squares estimator can be understood geometrically as an orthogonal projection of the data vector onto the subspace spanned by the model's regressors. This projection is represented by a linear operator known as the \"hat matrix,\" $P$. This exercise [@problem_id:2897104] invites you to derive the form of this matrix and prove its fundamental properties, culminating in calculating its trace, which provides a precise measure of the model's complexity, or the \"degrees of freedom\" it consumes.", "problem": "Consider a linear signal model in which an observed deterministic signal vector $y \\in \\mathbb{R}^{n}$ is modeled as a linear combination of $p$ known regressors collected as the columns of a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with full column rank $p$ and $p \\leq n$. The least squares estimator (LS) chooses coefficient vector $\\hat{\\beta} \\in \\mathbb{R}^{p}$ to minimize the squared error $\\|y - X \\beta\\|_{2}^{2}$. Let the fitted signal be $\\hat{y} = X \\hat{\\beta}$, and define the linear mapping $P \\in \\mathbb{R}^{n \\times n}$ via $\\hat{y} = P y$ for every $y \\in \\mathbb{R}^{n}$.\n\nStarting from the optimality condition that characterizes the least squares solution and the orthogonality principle stating that the residual is orthogonal to the model subspace, derive an explicit formula for $P$, and prove from first principles that $P$ is symmetric and idempotent. Then, using only structural properties of orthogonal projectors and without assuming any particular basis for $\\mathbb{R}^{n}$, determine $\\operatorname{tr}(P)$ in terms of intrinsic properties of $X$. Interpret this trace as the effective degrees of freedom used by least squares in this model.\n\nProvide as your final answer the analytical value of $\\operatorname{tr}(P)$ expressed in terms of $p$. Do not include any units. If you introduce any additional notation, define it clearly. Your final answer must be a single closed-form expression.", "solution": "We begin from the definition of the least squares estimator: $\\hat{\\beta}$ minimizes $\\|y - X \\beta\\|_{2}^{2}$ over $\\beta \\in \\mathbb{R}^{p}$. The fundamental optimality condition is the normal equations, derived by differentiating the objective with respect to $\\beta$ and setting the derivative to zero:\n$$\nX^{\\top}\\bigl(y - X \\hat{\\beta}\\bigr) = 0.\n$$\nUnder the assumption that $X$ has full column rank $p$, the Gram matrix $X^{\\top} X \\in \\mathbb{R}^{p \\times p}$ is symmetric positive definite and hence invertible. Therefore,\n$$\n\\hat{\\beta} = \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} y.\n$$\nThe fitted vector is then\n$$\n\\hat{y} = X \\hat{\\beta} = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} y.\n$$\nBy definition of the linear mapping $P$ with $\\hat{y} = P y$ for all $y \\in \\mathbb{R}^{n}$, we identify\n$$\nP = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}.\n$$\n\nWe now establish that $P$ is symmetric. Using the properties of transpose and symmetry of $X^{\\top} X$,\n$$\nP^{\\top} = \\bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\bigr)^{\\top} = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top} = P.\n$$\nNext, we verify idempotence, i.e., $P^{2} = P$. Compute\n$$\nP^{2} = \\Bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\Bigr)\\Bigl(X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\\Bigr)\n= X \\bigl(X^{\\top} X\\bigr)^{-1} \\underbrace{X^{\\top} X}_{\\text{invertible}} \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= P.\n$$\nThus $P$ is a symmetric idempotent matrix, i.e., an orthogonal projector onto the model subspace $\\mathcal{R}(X)$, the range (column space) of $X$. This also follows from the orthogonality principle: the residual $r = y - \\hat{y}$ satisfies $X^{\\top} r = 0$, which states $r \\in \\mathcal{R}(X)^{\\perp}$ and $\\hat{y} \\in \\mathcal{R}(X)$, hence $P$ is the orthogonal projector onto $\\mathcal{R}(X)$.\n\nTo determine $\\operatorname{tr}(P)$ from intrinsic properties of $X$, observe that any real symmetric idempotent matrix has eigenvalues in $\\{0, 1\\}$. Indeed, if $P v = \\lambda v$ for some nonzero $v$, then $P^{2} v = \\lambda^{2} v$ but also $P^{2} v = P v = \\lambda v$, hence $\\lambda^{2} = \\lambda$, so $\\lambda \\in \\{0, 1\\}$. Moreover, the rank of $P$ equals the number of eigenvalues equal to $1$ and equals $\\dim\\bigl(\\mathcal{R}(P)\\bigr)$. Since $P$ is the orthogonal projector onto $\\mathcal{R}(X)$, we have $\\mathcal{R}(P) = \\mathcal{R}(X)$ and therefore\n$$\n\\operatorname{rank}(P) = \\dim\\bigl(\\mathcal{R}(X)\\bigr) = \\operatorname{rank}(X).\n$$\nBecause the trace of a matrix equals the sum of its eigenvalues, for a symmetric idempotent matrix $P$ this trace equals the number of ones among its eigenvalues, which is the rank of $P$. Therefore,\n$$\n\\operatorname{tr}(P) = \\operatorname{rank}(P) = \\operatorname{rank}(X).\n$$\nUnder the given assumption that $X$ has full column rank $p$, we have $\\operatorname{rank}(X) = p$, and hence\n$$\n\\operatorname{tr}(P) = p.\n$$\n\nAn alternative derivation that makes the structure explicit uses the Singular Value Decomposition (SVD). Let $X = U \\Sigma V^{\\top}$ be a compact Singular Value Decomposition (SVD) with $U \\in \\mathbb{R}^{n \\times n}$ orthogonal, $V \\in \\mathbb{R}^{p \\times p}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{n \\times p}$ having the form $\\Sigma = \\begin{bmatrix} D \\\\ 0 \\end{bmatrix}$ where $D \\in \\mathbb{R}^{p \\times p}$ is diagonal with strictly positive diagonal entries because $X$ has full column rank $p$. Then\n$$\nP = X \\bigl(X^{\\top} X\\bigr)^{-1} X^{\\top}\n= U \\Sigma V^{\\top} \\bigl(V \\Sigma^{\\top} \\Sigma V^{\\top}\\bigr)^{-1} V \\Sigma^{\\top} U^{\\top}\n= U \\Sigma \\bigl(\\Sigma^{\\top} \\Sigma\\bigr)^{-1} \\Sigma^{\\top} U^{\\top}.\n$$\nSince $\\Sigma^{\\top} \\Sigma = D^{2}$, we obtain\n$$\n\\Sigma \\bigl(\\Sigma^{\\top} \\Sigma\\bigr)^{-1} \\Sigma^{\\top}\n= \\begin{bmatrix} D \\\\ 0 \\end{bmatrix} D^{-2} \\begin{bmatrix} D  0 \\end{bmatrix}\n= \\begin{bmatrix} I_{p}  0 \\\\ 0  0 \\end{bmatrix},\n$$\nso\n$$\nP = U \\begin{bmatrix} I_{p}  0 \\\\ 0  0 \\end{bmatrix} U^{\\top}.\n$$\nThe trace is invariant under similarity by an orthogonal matrix $U$, hence\n$$\n\\operatorname{tr}(P) = \\operatorname{tr}\\!\\left(\\begin{bmatrix} I_{p}  0 \\\\ 0  0 \\end{bmatrix}\\right) = p.\n$$\n\nInterpretation as effective degrees of freedom: In least squares fitting, $P$ maps any data vector $y$ to its orthogonal projection onto the $p$-dimensional subspace $\\mathcal{R}(X)$. The trace $\\operatorname{tr}(P)$ equals $p$, which represents the effective number of parameters or linear constraints that the fit adapts to the data. Consequently, the residual subspace has dimension $n - p$, often interpreted as the remaining degrees of freedom available for assessing the discrepancy between the model and the data (e.g., for variance estimation in the presence of additive noise). Thus, $\\operatorname{tr}(P) = p$ quantifies the effective degrees of freedom used by least squares.", "answer": "$$\\boxed{p}$$", "id": "2897104"}, {"introduction": "A core assumption for the desirable properties of the least squares estimator is that the model is correctly specified. In this practice [@problem_id:1948135], we investigate the consequences of violating this assumption, a common scenario in applied work. By deriving the expected value of an estimated coefficient in the presence of an omitted variable, you will uncover the precise mathematical form of omitted-variable bias, a critical concept for any practitioner of regression analysis.", "problem": "An economist is investigating the factors that determine hourly wages. The true underlying relationship for an individual $i$ in a sample of size $n$ is believed to be a multiple linear regression model:\n$$Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\epsilon_i$$\nHere, $Y_i$ is the hourly wage, $x_i$ is the number of years of education, and $z_i$ is the number of years of work experience. The parameters $\\beta_0$, $\\beta_1$, and $\\beta_2$ are unknown constants. The error terms $\\epsilon_i$ are assumed to be independent and identically distributed with an expected value of zero, i.e., $E[\\epsilon_i] = 0$. Both $x_i$ and $z_i$ are treated as non-stochastic.\n\nUnfortunately, the economist misplaces the data for work experience, $z_i$. They proceed to estimate a misspecified simple linear regression model using only the education data:\n$$Y_i = \\alpha_0 + \\alpha_1 x_i + \\nu_i$$\nThe economist uses the method of Ordinary Least Squares (OLS) to obtain an estimator for the slope parameter $\\alpha_1$, denoted as $\\hat{\\alpha}_1$.\n\nDerive the expected value of this estimator, $E[\\hat{\\alpha}_1]$. Express your answer in terms of the true model parameters ($\\beta_1$, $\\beta_2$) and sums involving the sample data ($x_i$, $z_i$) and their respective sample means, $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ and $\\bar{z} = \\frac{1}{n} \\sum_{i=1}^n z_i$.", "solution": "The OLS slope estimator from the misspecified simple regression of $Y_{i}$ on $x_{i}$ is\n$$\n\\hat{\\alpha}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}.\n$$\nUnder the true model $Y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\beta_{2}z_{i}+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$ and non-stochastic regressors, the sample mean of $Y_{i}$ is\n$$\n\\bar{Y}=\\beta_{0}+\\beta_{1}\\bar{x}+\\beta_{2}\\bar{z}+\\bar{\\epsilon},\\quad \\text{where } \\bar{\\epsilon}=\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}.\n$$\nHence\n$$\nY_{i}-\\bar{Y}=\\beta_{1}(x_{i}-\\bar{x})+\\beta_{2}(z_{i}-\\bar{z})+(\\epsilon_{i}-\\bar{\\epsilon}).\n$$\nSubstituting into the numerator,\n$$\n\\sum_{i=1}^{n}(x_{i}-\\bar{x})(Y_{i}-\\bar{Y})=\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}+\\beta_{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})+\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\epsilon_{i}-\\bar{\\epsilon}).\n$$\nTaking expectations and using $E[\\epsilon_{i}]=0$ implies $E[\\bar{\\epsilon}]=0$ and therefore\n$$\nE\\!\\left[\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\epsilon_{i}-\\bar{\\epsilon})\\right]=0.\n$$\nWith non-stochastic $x_{i}$ and $z_{i}$, the denominator $\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$ is fixed, so\n$$\nE[\\hat{\\alpha}_{1}]=\\frac{\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}+\\beta_{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}=\\beta_{1}+\\beta_{2}\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}.\n$$\nThis shows the omitted-variable bias term is $\\beta_{2}\\,\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}$, which vanishes if and only if $\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})=0$.", "answer": "$$\\boxed{\\beta_{1}+\\beta_{2}\\,\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(z_{i}-\\bar{z})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}}$$", "id": "1948135"}, {"introduction": "The ultimate test of a model is its ability to predict new, unseen data. This exercise [@problem_id:2897085] connects the statistical properties of the least squares estimator to its out-of-sample prediction performance. You will derive the expected prediction risk and decompose it into its constituent parts—squared bias and variance—to understand how estimation uncertainty, driven by measurement noise, translates into prediction error.", "problem": "A wide-sense stationary discrete-time linear time-invariant system with unknown finite impulse response vector $\\beta_{0} \\in \\mathbb{R}^{p}$ is identified from $n$ measurements using ordinary least squares (OLS). The training data are modeled as $y = X \\beta_{0} + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$ is a fixed full-column-rank design matrix constructed from the input sequence, $\\varepsilon \\in \\mathbb{R}^{n}$ is zero-mean noise with covariance $\\sigma^{2} I_{n}$, and $I_{n}$ is the $n \\times n$ identity matrix. The OLS estimator $\\hat{\\beta}$ is defined as the minimizer of the empirical sum of squared residuals, and is used to predict the noiseless output for a new input feature vector $x \\in \\mathbb{R}^{p}$ via the predictor $x^{\\top} \\hat{\\beta}$. Assume that, at deployment, new inputs $x$ are independent of the training noise and are drawn from a distribution with zero mean and covariance $\\Sigma_{x} \\in \\mathbb{R}^{p \\times p}$.\n\nUsing only the stated modeling assumptions and first principles, derive the expected out-of-sample squared prediction risk for the noiseless target $x^{\\top} \\beta_{0}$, defined by\n$$\nR_{\\mathrm{out}} \\triangleq \\mathbb{E}\\!\\left[\\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X \\right],\n$$\nwhere the expectation is over the training noise and the new input distribution for $x$. Express $R_{\\mathrm{out}}$ explicitly in terms of $X$, $\\Sigma_{x}$, and $\\sigma^{2}$, and separate it into its squared bias and variance contributions with respect to the distribution of $\\hat{\\beta}$. Provide the final answer as analytic expressions; no numerical approximation is required.", "solution": "The OLS estimator $\\hat{\\beta}$ that minimizes the sum of squared residuals $\\|y - X\\beta\\|_{2}^{2}$ is given by the normal equations, yielding:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\nThe existence of $(X^{\\top}X)^{-1}$ is guaranteed because $X$ is full column rank. We substitute the model $y = X \\beta_{0} + \\varepsilon$ into this expression:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}(X \\beta_{0} + \\varepsilon) = (X^{\\top}X)^{-1}X^{\\top}X \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon\n$$\nThis equation expresses the estimator $\\hat{\\beta}$ in terms of the true parameter vector $\\beta_{0}$ and the noise $\\varepsilon$.\n\nThe risk $R_{\\mathrm{out}}$ is defined as the expectation of the squared prediction error, $(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0})^{2}$, over the distributions of the training noise $\\varepsilon$ and the new input $x$. Since $x$ and $\\varepsilon$ are independent, and the expectation is conditioned on $X$, we can write:\n$$\nR_{\\mathrm{out}} = \\mathbb{E}_{x, \\varepsilon} \\left[ \\left(x^{\\top} (\\hat{\\beta} - \\beta_{0})\\right)^{2} \\,\\middle|\\, X \\right] = \\mathbb{E}_{x} \\left[ \\mathbb{E}_{\\varepsilon} \\left[ \\left(x^{\\top} \\hat{\\beta} - x^{\\top} \\beta_{0}\\right)^{2} \\,\\middle|\\, X, x \\right] \\right]\n$$\nThe inner expectation is the conditional Mean Squared Error (MSE) of the predictor $x^{\\top}\\hat{\\beta}$ for a fixed new input $x$. We can decompose this conditional MSE into its squared bias and variance components.\n\nFirst, the bias term. The expectation of the estimator $\\hat{\\beta}$ over $\\varepsilon$ is:\n$$\n\\mathbb{E}_{\\varepsilon} \\left[ \\hat{\\beta} \\,\\middle|\\, X \\right] = \\mathbb{E}_{\\varepsilon} \\left[ \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\,\\middle|\\, X \\right] = \\beta_{0} + (X^{\\top}X)^{-1}X^{\\top}\\mathbb{E}_{\\varepsilon}[\\varepsilon] = \\beta_{0}\n$$\nSince $\\hat{\\beta}$ is an unbiased estimator for $\\beta_0$, the predictor $x^\\top\\hat{\\beta}$ is also unbiased for $x^\\top\\beta_0$. The squared bias contribution to the risk is therefore zero.\n\nNext, the variance term. For a fixed $x$, the variance of the predictor $x^{\\top}\\hat{\\beta}$ over $\\varepsilon$ is:\n$$\n\\mathrm{Var}_{\\varepsilon} \\left( x^{\\top} \\hat{\\beta} \\,\\middle|\\, X, x \\right) = \\mathrm{Var}_{\\varepsilon} \\left( x^{\\top}\\beta_{0} + x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\right) = \\mathrm{Var}_{\\varepsilon} \\left( x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\varepsilon \\right)\n$$\nUsing the property $\\mathrm{Var}(c^{\\top}\\varepsilon) = c^{\\top}\\mathrm{Cov}(\\varepsilon)c$, with $\\mathrm{Cov}(\\varepsilon) = \\sigma^{2}I_{n}$, this becomes:\n$$\n\\sigma^{2} \\left(x^{\\top}(X^{\\top}X)^{-1}X^{\\top}\\right) \\left(X(X^{\\top}X)^{-1}x\\right) = \\sigma^{2} x^{\\top}(X^{\\top}X)^{-1}x\n$$\nTo find the total risk, we must average this quantity over the distribution of $x$:\n$$\nR_{\\mathrm{out}} = \\mathbb{E}_{x} \\left[ \\sigma^{2} x^{\\top}(X^{\\top}X)^{-1}x \\right] = \\sigma^{2} \\mathbb{E}_{x} \\left[ x^{\\top}(X^{\\top}X)^{-1}x \\right]\n$$\nThe term inside the expectation is a quadratic form in the random vector $x$. For a random vector $x$ with mean $\\mu_{x}$ and covariance $\\Sigma_{x}$, the expectation of the quadratic form $x^{\\top}Mx$ is given by $\\mathbb{E}[x^{\\top}Mx] = \\mathrm{Tr}(M\\Sigma_{x}) + \\mu_{x}^{\\top}M\\mu_{x}$. In our case, $M=(X^{\\top}X)^{-1}$, $\\mu_{x}=0$, and the covariance is $\\Sigma_{x}$. Therefore:\n$$\n\\mathbb{E}_{x} \\left[ x^{\\top}(X^{\\top}X)^{-1}x \\right] = \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right)\n$$\nThe total risk is the sum of the squared bias (0) and the average variance:\n$$\nR_{\\mathrm{out}} = \\sigma^{2} \\mathrm{Tr}\\left((X^{\\top}X)^{-1}\\Sigma_{x}\\right)\n$$", "answer": "$$\n\\boxed{\\sigma^{2} \\mathrm{Tr}((X^{\\top}X)^{-1} \\Sigma_{x})}\n$$", "id": "2897085"}]}