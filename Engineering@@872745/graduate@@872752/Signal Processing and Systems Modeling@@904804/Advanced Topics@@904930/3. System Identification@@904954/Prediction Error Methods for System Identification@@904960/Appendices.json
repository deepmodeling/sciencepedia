{"hands_on_practices": [{"introduction": "The journey into Prediction Error Methods (PEM) begins with a solid understanding of its core components: the predictor and the cost function. This first exercise grounds your understanding by applying these concepts to the simplest class of linear time-invariant systems, the Finite Impulse Response (FIR) model. By deriving the one-step-ahead predictor and the PEM cost function from first principles, you will build a foundational intuition for how prediction errors are defined and subsequently used for parameter estimation [@problem_id:2892835].", "problem": "Consider a single-input single-output finite-impulse-response model with order $m \\in \\mathbb{N}$ given by $y_{t}=\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t}$, where $\\{u_{t}\\}_{t \\in \\mathbb{Z}}$ and $\\{y_{t}\\}_{t \\in \\mathbb{Z}}$ are the input and output sequences, and $\\{e_{t}\\}_{t \\in \\mathbb{Z}}$ is a zero-mean, white sequence with finite variance, independent of the input. Assume that a batch of $N \\in \\mathbb{N}$ samples $\\{(u_{t},y_{t})\\}_{t=1}^{N}$ is available. The unknown parameter vector is $\\theta \\triangleq [\\,b_{1}\\ \\cdots\\ b_{m}\\,]^{\\top}$. Let $\\mathcal{F}_{t-1}$ denote the $\\sigma$-algebra generated by the past input-output data up to time $t-1$. Starting from the definition of the one-step-ahead predictor as the conditional expectation of $y_{t}$ given $\\mathcal{F}_{t-1}$ under the stated model and assumptions, derive an explicit expression for the predictor $\\hat{y}_{t}(\\theta) \\triangleq \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]$ for all time indices $t$ such that all regressors are available from the observed data. Then, using the definition of the Prediction Error Method (PEM) cost as the sample average of squared one-step-ahead prediction errors for white innovations, write the explicit finite-sample PEM cost $V_{N}(\\theta)$ to be minimized over $b_{1},\\dots,b_{m}$, clearly specifying the index range used to avoid unknown pre-sample quantities. Express your final answer as explicit formulas for $\\hat{y}_{t}(\\theta)$ and $V_{N}(\\theta)$. No numerical evaluation is required, and no rounding is needed.", "solution": "The problem requires the derivation of the one-step-ahead predictor and the associated Prediction Error Method (PEM) cost function for a specified Finite Impulse Response (FIR) model. A rigorous validation of the problem statement is a mandatory prerequisite.\n\nFirst, we must extract the given information verbatim.\nThe model is given by $y_{t}=\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t}$, where the order is $m \\in \\mathbb{N}$.\nThe parameter vector is $\\theta \\triangleq [\\,b_{1}\\ \\cdots\\ b_{m}\\,]^{\\top}$.\nThe signals involved are the input sequence $\\{u_{t}\\}_{t \\in \\mathbb{Z}}$ and the output sequence $\\{y_{t}\\}_{t \\in \\mathbb{Z}}$.\nThe noise sequence $\\{e_{t}\\}_{t \\in \\mathbb{Z}}$ is stipulated to be a zero-mean, white sequence with finite variance, which is also independent of the input sequence.\nThe available data consists of a batch of $N \\in \\mathbb{N}$ samples, denoted as $\\{(u_{t},y_{t})\\}_{t=1}^{N}$.\nThe information set $\\mathcal{F}_{t-1}$ is defined as the $\\sigma$-algebra generated by the past input-output data up to time $t-1$.\nThe one-step-ahead predictor is defined as $\\hat{y}_{t}(\\theta) \\triangleq \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]$.\nThe PEM cost function, $V_{N}(\\theta)$, is defined as the sample average of squared one-step-ahead prediction errors.\n\nThe problem statement is scrutinized for validity. It is scientifically grounded, situated firmly within the standard theory of system identification. The model and assumptions are canonical. The problem is well-posed, providing all necessary definitions and data to arrive at a unique, meaningful solution. The requirement to specify the index range to handle the finite data set is an integral part of the problem, not an indicator of incompleteness. The problem is a fundamental exercise in applying first principles of prediction theory and statistical signal processing. Therefore, the problem is deemed valid and a solution will be constructed.\n\nWe proceed with the derivation of the one-step-ahead predictor, $\\hat{y}_{t}(\\theta)$. By its definition:\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}[\\,y_{t}\\mid \\mathcal{F}_{t-1}\\,]\n$$\nSubstituting the given model for $y_{t}$:\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k}+e_{t} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right]\n$$\nUsing the linearity of conditional expectation, this expression is split into two terms:\n$$\n\\hat{y}_{t}(\\theta) = \\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right] + \\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,]\n$$\nLet us analyze the first term. The parameters $b_{k}$ are deterministic components of the vector $\\theta$. The inputs $u_{t-k}$ for $k \\in \\{1, 2, \\dots, m\\}$ are, by definition, events that occurred at or before time $t-1$. Thus, they are measurable with respect to the $\\sigma$-algebra $\\mathcal{F}_{t-1}$. Consequently, the entire sum $\\sum_{k=1}^{m} b_{k}\\,u_{t-k}$ is an $\\mathcal{F}_{t-1}$-measurable quantity. The conditional expectation of an $\\mathcal{F}_{t-1}$-measurable variable given $\\mathcal{F}_{t-1}$ is the variable itself.\n$$\n\\mathbb{E}\\left[\\,\\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\,\\middle|\\, \\mathcal{F}_{t-1}\\,\\right] = \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\nNow, consider the second term, $\\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,]$. The noise $e_{t}$ is assumed to be a white sequence, meaning it is uncorrelated with its past values $\\{e_{s}\\}_{st}$. It is also independent of the entire input sequence $\\{u_{s}\\}_{s \\in \\mathbb{Z}}$. The information set $\\mathcal{F}_{t-1}$ is generated by past inputs $\\{u_{s}\\}_{s \\le t-1}$ and past outputs $\\{y_{s}\\}_{s \\le t-1}$. Since each past output $y_{s}$ is a function of past inputs and the past noise term $e_{s}$, the $\\sigma$-algebra $\\mathcal{F}_{t-1}$ contains information only about $\\{u_{s}\\}_{s \\le t-1}$ and $\\{e_{s}\\}_{s \\le t-1}$. Because $e_{t}$ is independent of both of these sets of random variables, it is independent of the $\\sigma$-algebra $\\mathcal{F}_{t-1}$. The conditional expectation of a random variable given an independent $\\sigma$-algebra is its unconditional expectation.\n$$\n\\mathbb{E}[\\,e_{t} \\mid \\mathcal{F}_{t-1}\\,] = \\mathbb{E}[\\,e_{t}\\,]\n$$\nAs stated in the problem, $\\{e_{t}\\}$ is a zero-mean sequence, so $\\mathbb{E}[\\,e_{t}\\,] = 0$.\nCombining these results gives the final expression for the predictor:\n$$\n\\hat{y}_{t}(\\theta) = \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\nNext, we derive the PEM cost function $V_{N}(\\theta)$. The one-step-ahead prediction error is defined as $\\varepsilon_{t}(\\theta) = y_{t} - \\hat{y}_{t}(\\theta)$. Using our derived predictor, we have:\n$$\n\\varepsilon_{t}(\\theta) = y_{t} - \\sum_{k=1}^{m} b_{k}\\,u_{t-k}\n$$\nThe problem defines the cost function as the sample average of the squared prediction errors. The summation must be performed over the time indices $t$ for which all quantities ($y_{t}$ and the regressors $u_{t-1}, \\dots, u_{t-m}$) are available within the given data set $\\{(u_{t},y_{t})\\}_{t=1}^{N}$. The regressor with the lowest time index is $u_{t-m}$. For this to be within our data record, we must have $t-m \\ge 1$, which implies $t \\ge m+1$. The latest time for which we have an output $y_{t}$ is $t=N$. Thus, the summation must range from $t=m+1$ to $t=N$. This is valid only if $N \\ge m+1$. The number of terms in this sum is $N - (m+1) + 1 = N-m$.\nThe cost function $V_N(\\theta)$ is therefore:\n$$\nV_{N}(\\theta) = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( \\varepsilon_{t}(\\theta) \\right)^2 = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( y_{t} - \\sum_{k=1}^{m} b_{k}\\,u_{t-k} \\right)^2\n$$\nThis completes the derivation. The results provide the explicit formulas requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\hat{y}_{t}(\\theta) = \\sum_{k=1}^{m} b_{k}u_{t-k}  V_{N}(\\theta) = \\frac{1}{N-m} \\sum_{t=m+1}^{N} \\left( y_{t} - \\sum_{k=1}^{m} b_{k}u_{t-k} \\right)^{2}\n\\end{pmatrix}\n}\n$$", "id": "2892835"}, {"introduction": "While FIR models are simple, many real-world processes exhibit more complex dynamics, where the process noise itself is structured. The AutoRegressive Moving Average with eXogenous input (ARMAX) model captures such scenarios. This practice challenges you to derive the optimal one-step-ahead predictor for an ARMAX system, a task that requires a powerful algebraic tool: the polynomial Diophantine equation. Completing this exercise [@problem_id:2892827] will not only deepen your theoretical knowledge but also provide hands-on experience in implementing the recursive filter needed to compute the prediction errors, a critical step in any practical PEM application.", "problem": "You are given a single-input single-output AutoRegressive Moving Average with eXogenous input (ARMAX) model written in the backward-shift operator as\n$$\nA(q,\\theta)\\,y_t \\;=\\; B(q,\\theta)\\,u_t \\;+\\; C(q,\\theta)\\,e_t,\n$$\nwhere $q^{-1}$ denotes the unit-delay operator, $A(q,\\theta)$ and $C(q,\\theta)$ are monic polynomials in $q^{-1}$ with all zeros strictly inside the unit disk, $B(q,\\theta)$ is a finite polynomial in $q^{-1}$, and $\\{e_t\\}$ is a zero-mean white-noise sequence of finite variance that is independent of $\\{u_t\\}$. Assume $A(q,\\theta)$ and $C(q,\\theta)$ are coprime. The one-step-ahead predictor $\\hat{y}_{t|t-1}(\\theta)$ is defined as the conditional expectation $\\mathbb{E}\\!\\left[y_t \\mid \\mathcal{F}_{t-1}\\right]$, where $\\mathcal{F}_{t-1}$ is the $\\sigma$-algebra generated by $\\{y_{t-k},u_{t-k}\\}_{k\\ge 1}$.\n\nTasks:\n1) Starting from the above model and the definition of conditional expectation, use a polynomial Diophantine equation to derive a strictly causal one-step-ahead predictor $\\hat{y}_{t|t-1}(\\theta)$ that depends only on past measured data and previously computed prediction errors. In particular, find polynomials $\\Gamma(q,\\theta)$ and $\\Delta(q,\\theta)$ in $q^{-1}$ satisfying a suitable Diophantine relation between $A(q,\\theta)$ and $C(q,\\theta)$, and use it to express $\\hat{y}_{t|t-1}(\\theta)$ in terms of $u_t$, $y_t$, and past innovations. Clearly state the degree conditions you impose to ensure causality and uniqueness.\n2) From your predictor, show how to compute the prediction error $\\epsilon_t(\\theta) \\equiv y_t - \\hat{y}_{t|t-1}(\\theta)$ recursively using only past $\\epsilon$-values together with current and past inputs and outputs; explicitly write this recursion in terms of the coefficients of $A(q,\\theta)$, $B(q,\\theta)$, and $C(q,\\theta)$, assuming they are parameterized as $A(q,\\theta)=1+\\sum_{k=1}^{n_a} a_k q^{-k}$, $B(q,\\theta)=\\sum_{k=0}^{n_b} b_k q^{-k}$, and $C(q,\\theta)=1+\\sum_{k=1}^{n_c} c_k q^{-k}$.\n3) Consider the specific parameter vector $\\theta$ and short data record given by\n- Model: $A(q,\\theta)=1-1.2\\,q^{-1}+0.32\\,q^{-2}$, $B(q,\\theta)=0.5+0.1\\,q^{-1}$, $C(q,\\theta)=1+0.5\\,q^{-1}$.\n- Inputs: $u_{-1}=0$, $u_0=1.0$, $u_1=-0.5$, $u_2=0.0$, $u_3=0.25$.\n- Outputs: $y_{-2}=0$, $y_{-1}=0$, $y_0=0.8$, $y_1=-0.4$, $y_2=0.5$, $y_3=0.1$.\nAssume initial rest for the predictor recursion, i.e., $\\epsilon_t(\\theta)=0$ for all $t0$. Using your recursion from part 2), compute the numerical value of $\\epsilon_3(\\theta)$. Round your answer to four significant figures.", "solution": "This problem demands a rigorous derivation of the one-step-ahead predictor for an ARMAX model, followed by a numerical calculation. The problem is well-posed and scientifically sound. I will address each part in sequence. For clarity, the dependence of polynomials on the parameter vector $\\theta$ will be suppressed in the notation, e.g., $A(q)$ instead of $A(q,\\theta)$, unless ambiguity arises.\n\n**Part 1: Derivation of the One-Step-Ahead Predictor**\n\nThe ARMAX model is given by the stochastic difference equation:\n$$\nA(q) y_t = B(q) u_t + C(q) e_t\n$$\nwhere $A(q)$ and $C(q)$ are monic polynomials in the backward-shift operator $q^{-1}$ with zeros inside the unit disk, and $\\{e_t\\}$ is a zero-mean white noise process. We seek the one-step-ahead predictor $\\hat{y}_{t|t-1}$, defined as the conditional expectation $\\mathbb{E}[y_t | \\mathcal{F}_{t-1}]$, where $\\mathcal{F}_{t-1}$ is the $\\sigma$-algebra generated by past inputs and outputs $\\{y_{t-k}, u_{t-k}\\}_{k \\ge 1}$. We assume the input sequence $\\{u_t\\}$ is known, which is standard practice.\n\nFirst, we express $y_t$ in terms of the driving signals $u_t$ and $e_t$. Since $A(q)$ has its zeros inside the unit disk, its inverse $A(q)^{-1}$ corresponds to a stable and causal filter.\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + \\frac{C(q)}{A(q)} e_t\n$$\nTo separate the predictable part of $y_t$ from the unpredictable innovation $e_t$, we decompose the rational function $C(q)/A(q)$ using a polynomial Diophantine equation. For a one-step-ahead predictor, this equation is:\n$$\nC(q) = A(q) \\Gamma(q) + q^{-1} \\Delta(q)\n$$\nHere, $\\Gamma(q)$ and $\\Delta(q)$ are polynomials in $q^{-1}$. To ensure a unique solution, we must impose degree constraints. For a one-step prediction horizon ($d=1$), we require the degree of $\\Gamma(q)$ to be $\\deg(\\Gamma) = 1-1=0$. Since both $A(q)$ and $C(q)$ are monic (having a leading coefficient of $1$), it is necessary that $\\Gamma(q) = 1$. This implies the Diophantine equation simplifies to:\n$$\nC(q) = A(q) \\cdot 1 + q^{-1} \\Delta(q)\n$$\nFrom this, we solve for $\\Delta(q)$:\n$$\n\\Delta(q) = q(C(q) - A(q))\n$$\nSince $A(q)$ and $C(q)$ are monic, the polynomial $C(q)-A(q)$ has a constant term of $1-1=0$, meaning it is of the form $(c_1-a_1)q^{-1} + (c_2-a_2)q^{-2} + \\dots$. Consequently, $\\Delta(q) = q(C(q)-A(q))$ is a finite polynomial in $q^{-1}$. The degree of $\\Delta(q)$ is $\\max(\\deg A, \\deg C) - 1$. This confirms the causality and uniqueness of our decomposition.\n\nNow, we substitute the Diophantine identity back into the expression for $y_t$:\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + \\frac{A(q)\\Gamma(q) + q^{-1}\\Delta(q)}{A(q)} e_t\n$$\nWith $\\Gamma(q)=1$, this becomes:\n$$\ny_t = \\frac{B(q)}{A(q)} u_t + e_t + \\frac{q^{-1}\\Delta(q)}{A(q)} e_t\n$$\nThe term $e_t$ is the innovation at time $t$. By definition, it is unpredictable given the past information $\\mathcal{F}_{t-1}$, so $\\mathbb{E}[e_t | \\mathcal{F}_{t-1}] = 0$. The other terms are functions of past values of $e_t$ (i.e., $\\{e_{t-k}\\}_{k \\ge 1}$) and current and past values of $u_t$. Since the sequence $\\{u_t\\}$ is assumed known, and past innovations $\\{e_{t-k}\\}_{k \\ge 1}$ are measurable with respect to $\\mathcal{F}_{t-1}$ (as they can be constructed from past inputs and outputs, assuming invertibility of $C(q)$), these terms are predictable.\n\nTherefore, taking the conditional expectation gives the predictor:\n$$\n\\hat{y}_{t|t-1} = \\mathbb{E}[y_t | \\mathcal{F}_{t-1}] = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{A(q)} e_t\n$$\nThis expression depends on the unmeasurable noise sequence $\\{e_t\\}$. To obtain a practical form, we must express $e_t$ in terms of measurable quantities. From the original ARMAX model, $C(q)e_t = A(q)y_t - B(q)u_t$. Since $C(q)$ is stable and invertible, we have $e_t = C(q)^{-1}(A(q)y_t - B(q)u_t)$. Substituting this into the predictor equation yields:\n$$\n\\hat{y}_{t|t-1} = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{A(q)} \\left( \\frac{A(q)}{C(q)}y_t - \\frac{B(q)}{C(q)}u_t \\right) = \\frac{B(q)}{A(q)} u_t + \\frac{q^{-1}\\Delta(q)}{C(q)} y_t - \\frac{q^{-1}\\Delta(q)B(q)}{A(q)C(q)} u_t\n$$\nCombining the terms involving $u_t$:\n$$\n\\hat{y}_{t|t-1} = \\left( \\frac{B(q)}{A(q)} - \\frac{q^{-1}\\Delta(q)B(q)}{A(q)C(q)} \\right) u_t + \\frac{q^{-1}\\Delta(q)}{C(q)} y_t\n$$\nSubstituting $q^{-1}\\Delta(q) = C(q) - A(q)$:\n$$\n\\hat{y}_{t|t-1} = \\left( \\frac{B(q)C(q) - (C(q)-A(q))B(q)}{A(q)C(q)} \\right) u_t + \\frac{C(q)-A(q)}{C(q)} y_t\n$$\nThe term in the parenthesis simplifies: $B(q)C(q) - C(q)B(q) + A(q)B(q) = A(q)B(q)$.\n$$\n\\hat{y}_{t|t-1} = \\frac{A(q)B(q)}{A(q)C(q)} u_t + \\frac{C(q)-A(q)}{C(q)} y_t = \\frac{B(q)}{C(q)} u_t + \\left(1 - \\frac{A(q)}{C(q)}\\right) y_t\n$$\nThis is the final expression for the one-step-ahead predictor. It can be implemented recursively as:\n$$\nC(q)\\hat{y}_{t|t-1} = B(q)u_t + (C(q)-A(q))y_t\n$$\nThis form depends only on past outputs, current and past inputs, and past predictor values, making it strictly causal in its use of measured outputs.\n\n**Part 2: Recursive Formula for the Prediction Error**\n\nThe prediction error is defined as $\\epsilon_t(\\theta) = y_t - \\hat{y}_{t|t-1}(\\theta)$. From the predictor derived in Part 1, we have:\n$$\nC(q)\\hat{y}_{t|t-1} = B(q)u_t + (C(q)-A(q))y_t\n$$\nSubstitute $\\hat{y}_{t|t-1} = y_t - \\epsilon_t$:\n$$\nC(q)(y_t - \\epsilon_t) = B(q)u_t + C(q)y_t - A(q)y_t\n$$\nExpanding the left side:\n$$\nC(q)y_t - C(q)\\epsilon_t = B(q)u_t + C(q)y_t - A(q)y_t\n$$\nThe term $C(q)y_t$ cancels from both sides, leaving:\n$$\n-C(q)\\epsilon_t = B(q)u_t - A(q)y_t\n$$\nor\n$$\nC(q)\\epsilon_t = A(q)y_t - B(q)u_t\n$$\nThis demonstrates that the prediction error $\\epsilon_t$ can be obtained by filtering the input $u_t$ and output $y_t$. Note that if parameters $\\theta$ are the true ones, $\\epsilon_t(\\theta)$ becomes the true innovation $e_t$, and this equation is simply a rearrangement of the original ARMAX model.\n\nTo obtain an explicit recursive formula, we write out the polynomials:\n$A(q) = 1+\\sum_{k=1}^{n_a} a_k q^{-k}$, $B(q) = \\sum_{k=0}^{n_b} b_k q^{-k}$, and $C(q) = 1+\\sum_{k=1}^{n_c} c_k q^{-k}$.\nThe equation $C(q)\\epsilon_t = A(q)y_t - B(q)u_t$ becomes:\n$$\n\\left(1+\\sum_{k=1}^{n_c} c_k q^{-k}\\right)\\epsilon_t = \\left(1+\\sum_{k=1}^{n_a} a_k q^{-k}\\right)y_t - \\left(\\sum_{k=0}^{n_b} b_k q^{-k}\\right)u_t\n$$\nApplying the shift operator:\n$$\n\\epsilon_t + \\sum_{k=1}^{n_c} c_k \\epsilon_{t-k} = y_t + \\sum_{k=1}^{n_a} a_k y_{t-k} - \\sum_{k=0}^{n_b} b_k u_{t-k}\n$$\nIsolating $\\epsilon_t$ yields the desired recursion:\n$$\n\\epsilon_t = y_t + \\sum_{k=1}^{n_a} a_k y_{t-k} - \\sum_{k=0}^{n_b} b_k u_{t-k} - \\sum_{k=1}^{n_c} c_k \\epsilon_{t-k}\n$$\nThis formula allows for the computation of $\\epsilon_t$ using the current output $y_t$, current and past inputs $u_t, u_{t-1}, \\dots$, past outputs $y_{t-1}, \\dots$, and past prediction errors $\\epsilon_{t-1}, \\dots$.\n\n**Part 3: Numerical Computation**\n\nWe are given the specific model parameters:\n$A(q) = 1-1.2\\,q^{-1}+0.32\\,q^{-2} \\implies n_a=2, a_1 = -1.2, a_2 = 0.32$\n$B(q) = 0.5+0.1\\,q^{-1} \\implies n_b=1, b_0 = 0.5, b_1 = 0.1$\n$C(q) = 1+0.5\\,q^{-1} \\implies n_c=1, c_1 = 0.5$\n\nThe recursion for $\\epsilon_t$ is:\n$$\n\\epsilon_t = y_t + a_1 y_{t-1} + a_2 y_{t-2} - (b_0 u_t + b_1 u_{t-1}) - c_1 \\epsilon_{t-1}\n$$\n$$\n\\epsilon_t = y_t - 1.2 y_{t-1} + 0.32 y_{t-2} - (0.5 u_t + 0.1 u_{t-1}) - 0.5 \\epsilon_{t-1}\n$$\nWe are provided with data and the initial condition $\\epsilon_t=0$ for $t0$. We must compute $\\epsilon_3$. This requires computing $\\epsilon_0, \\epsilon_1, \\epsilon_2$ sequentially.\n\nFor $t=0$:\n$\\epsilon_0 = y_0 - 1.2 y_{-1} + 0.32 y_{-2} - (0.5 u_0 + 0.1 u_{-1}) - 0.5 \\epsilon_{-1}$\n$\\epsilon_0 = 0.8 - 1.2(0) + 0.32(0) - (0.5(1.0) + 0.1(0)) - 0.5(0)$\n$\\epsilon_0 = 0.8 - 0.5 = 0.3$\n\nFor $t=1$:\n$\\epsilon_1 = y_1 - 1.2 y_{0} + 0.32 y_{-1} - (0.5 u_1 + 0.1 u_{0}) - 0.5 \\epsilon_{0}$\n$\\epsilon_1 = -0.4 - 1.2(0.8) + 0.32(0) - (0.5(-0.5) + 0.1(1.0)) - 0.5(0.3)$\n$\\epsilon_1 = -0.4 - 0.96 - (-0.25 + 0.1) - 0.15$\n$\\epsilon_1 = -1.36 - (-0.15) - 0.15 = -1.36$\n\nFor $t=2$:\n$\\epsilon_2 = y_2 - 1.2 y_{1} + 0.32 y_{0} - (0.5 u_2 + 0.1 u_{1}) - 0.5 \\epsilon_{1}$\n$\\epsilon_2 = 0.5 - 1.2(-0.4) + 0.32(0.8) - (0.5(0.0) + 0.1(-0.5)) - 0.5(-1.36)$\n$\\epsilon_2 = 0.5 + 0.48 + 0.256 - (-0.05) + 0.68$\n$\\epsilon_2 = 0.5 + 0.48 + 0.256 + 0.05 + 0.68 = 1.966$\n\nFinally, for $t=3$:\n$\\epsilon_3 = y_3 - 1.2 y_{2} + 0.32 y_{1} - (0.5 u_3 + 0.1 u_{2}) - 0.5 \\epsilon_{2}$\n$\\epsilon_3 = 0.1 - 1.2(0.5) + 0.32(-0.4) - (0.5(0.25) + 0.1(0.0)) - 0.5(1.966)$\n$\\epsilon_3 = 0.1 - 0.6 - 0.128 - (0.125) - 0.983$\n$\\epsilon_3 = -0.5 - 0.128 - 0.125 - 0.983$\n$\\epsilon_3 = -0.628 - 0.125 - 0.983$\n$\\epsilon_3 = -0.753 - 0.983 = -1.736$\n\nThe computed value is $\\epsilon_3 = -1.736$. This result has four significant figures as requested.", "answer": "$$\n\\boxed{-1.736}\n$$", "id": "2892827"}, {"introduction": "Defining the PEM cost function is only half the battle; the ultimate goal is to find the parameters that minimize it. This final practice bridges the gap between theory and numerical implementation by focusing on the Gauss-Newton method, a workhorse algorithm for solving the nonlinear least-squares problem at the heart of PEM. You will derive the Gauss-Newton update rule and then apply it to a concrete problem, including the use of line search techniques like the strong Wolfe conditions [@problem_id:2892776]. This exercise provides invaluable insight into how optimization algorithms ensure robust and efficient convergence to a solution in practice.", "problem": "Consider a Single-Input Single-Output (SISO) output-error model identified by the prediction error method (PEM). Let the prediction error at sample index $t$ be defined as $\\epsilon(t,\\theta) = y(t) - \\hat{y}(t,\\theta)$, where $\\hat{y}(t,\\theta)$ is the model output given input $u(t)$ and parameter vector $\\theta \\in \\mathbb{R}^{n_{\\theta}}$. The PEM cost over $N$ samples is $V_{N}(\\theta) = \\frac{1}{2N} \\sum_{t=1}^{N} \\epsilon(t,\\theta)^{2}$. Starting from the least-squares structure of $V_{N}(\\theta)$ and a first-order Taylor expansion of the stacked residual vector around the current iterate, derive the search direction used by the Gauss-Newton method and the associated parameter update rule. Then, explain how line-search conditions of the strong Wolfe type enforce both sufficient decrease and acceptable curvature to ensure descent for a descent direction.\n\nNext, specialize to a linear-in-parameters static model $\\hat{y}(t,\\theta) = \\theta_{1} u(t) + \\theta_{2}$ with $\\theta = \\begin{pmatrix}\\theta_{1}  \\theta_{2}\\end{pmatrix}^{\\top}$. You are given $N=3$ samples\n$$(u(1),y(1)) = (0,\\,-0.45), \\quad (u(2),y(2)) = (1,\\,1.48), \\quad (u(3),y(3)) = (2,\\,3.51),$$\nand the initial parameter $\\theta^{0} = \\begin{pmatrix}1.0  0.0\\end{pmatrix}^{\\top}$. Compute the Gauss-Newton direction $p$ at $\\theta^{0}$, define $\\phi(\\alpha) = V_{N}(\\theta^{0} + \\alpha p)$, and use the strong Wolfe conditions with $c_{1} = 10^{-4}$ and $c_{2} = 0.9$ to select a step length $\\alpha$ from the candidate set $\\{\\;1,\\; \\tfrac{1}{2},\\; \\tfrac{1}{4}\\;\\}$. Report the largest $\\alpha$ in this set that satisfies both the sufficient decrease and curvature conditions. Round your final numerical answer to four significant figures.", "solution": "The problem presented is a standard exercise in nonlinear optimization applied to system identification and is well-posed and scientifically sound. I will first provide the requested general derivations and explanations, followed by the specific numerical calculation.\n\nThe Prediction Error Method (PEM) aims to find the parameter vector $\\theta$ that minimizes a cost function based on the prediction error, $\\epsilon(t,\\theta) = y(t) - \\hat{y}(t,\\theta)$. The cost function is given as a sum of squared errors:\n$$V_{N}(\\theta) = \\frac{1}{2N} \\sum_{t=1}^{N} \\epsilon(t,\\theta)^{2}$$\nThis can be written in vector form by defining the stacked residual vector $\\mathbf{\\epsilon}(\\theta) \\in \\mathbb{R}^{N}$ with components $\\epsilon(t, \\theta)$ for $t=1, \\dots, N$. The cost function becomes:\n$$V_{N}(\\theta) = \\frac{1}{2N} \\mathbf{\\epsilon}(\\theta)^{\\top} \\mathbf{\\epsilon}(\\theta)$$\n\nThe Gauss-Newton method is an iterative algorithm for solving nonlinear least-squares problems. At each iteration $k$, we seek a search direction $p^k$ to update the current parameter estimate $\\theta^k$ as $\\theta^{k+1} = \\theta^k + \\alpha_k p^k$, where $\\alpha_k$ is a step length. The direction $p^k$ is found by linearizing the residual vector $\\mathbf{\\epsilon}(\\theta)$ around $\\theta^k$ and minimizing the resulting quadratic approximation of the cost function.\n\nA first-order Taylor expansion of $\\mathbf{\\epsilon}(\\theta)$ around $\\theta^k$ gives:\n$$\\mathbf{\\epsilon}(\\theta^k + p) \\approx \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p$$\nwhere $p = \\theta - \\theta^k$ is the step, and $J(\\theta^k)$ is the Jacobian matrix of the residual vector evaluated at $\\theta^k$. Its elements are $[J(\\theta^k)]_{ti} = \\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta_i} \\Big|_{\\theta=\\theta^k}$.\n\nSubstituting this linearization into the cost function yields an approximate cost function, $\\tilde{V}_{N}(p)$:\n$$\\tilde{V}_{N}(p) = \\frac{1}{2N} \\left( \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p \\right)^{\\top} \\left( \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p \\right)$$\nTo find the optimal step $p$ that minimizes this quadratic function, we set its gradient with respect to $p$ to zero:\n$$\\nabla_{p} \\tilde{V}_{N}(p) = \\frac{1}{N} J(\\theta^k)^{\\top} \\left( \\mathbf{\\epsilon}(\\theta^k) + J(\\theta^k) p \\right) = 0$$\nRearranging this equation gives the Gauss-Newton normal equations:\n$$ \\left( J(\\theta^k)^{\\top} J(\\theta^k) \\right) p^k = - J(\\theta^k)^{\\top} \\mathbf{\\epsilon}(\\theta^k)$$\nThe Gauss-Newton search direction $p^k$ is the solution to this linear system:\n$$p^k = - \\left( J(\\theta^k)^{\\top} J(\\theta^k) \\right)^{-1} J(\\theta^k)^{\\top} \\mathbf{\\epsilon}(\\theta^k)$$\nThe associated parameter update rule is $\\theta^{k+1} = \\theta^k + \\alpha_k p^k$, where $\\alpha_k  0$ is the step length determined by a line search procedure.\n\nThe line search aims to find a suitable $\\alpha_k$ that ensures progress towards a minimum. The strong Wolfe conditions provide a standard set of criteria for an acceptable step length $\\alpha$. For a given descent direction $p$, we define a one-dimensional function $\\phi(\\alpha) = V_N(\\theta^k + \\alpha p)$. The strong Wolfe conditions are:\n1.  **Sufficient Decrease (Armijo) Condition**:\n    $$V_N(\\theta^k + \\alpha p^k) \\le V_N(\\theta^k) + c_1 \\alpha \\nabla V_N(\\theta^k)^{\\top} p^k$$\n    with a constant $c_1 \\in (0,1)$. Since $p^k$ is a descent direction, $\\nabla V_N(\\theta^k)^{\\top} p^k  0$. This condition requires that the actual reduction in the cost function is at least a fraction $c_1$ of the decrease predicted by the first-order approximation at $\\alpha=0$. It prevents the step from being unacceptably large while offering only minimal improvement.\n\n2.  **Strong Curvature Condition**:\n    $$|\\nabla V_N(\\theta^k + \\alpha p^k)^{\\top} p^k| \\le c_2 |\\nabla V_N(\\theta^k)^{\\top} p^k|$$\n    with a constant $c_2 \\in (c_1, 1)$. This condition ensures that the magnitude of the directional derivative at the new point $\\theta^k + \\alpha p^k$ is significantly smaller than at the starting point $\\theta^k$. It prevents the step from being too small, as very small steps tend to land in regions where the slope has not flattened out sufficiently. The absolute value also prevents steps into regions where the directional derivative has become large and positive, which would indicate that a minimum has been overshot.\n\nNow, we specialize to the given linear-in-parameters static model: $\\hat{y}(t,\\theta) = \\theta_{1} u(t) + \\theta_{2}$.\nThe parameter vector is $\\theta = \\begin{pmatrix}\\theta_1  \\theta_2\\end{pmatrix}^\\top$.\nThe prediction error is $\\epsilon(t, \\theta) = y(t) - (\\theta_1 u(t) + \\theta_2)$.\nThe data consists of $N=3$ samples: $(u(1),y(1)) = (0,\\,-0.45)$, $(u(2),y(2)) = (1,\\,1.48)$, $(u(3),y(3)) = (2,\\,3.51)$.\nThe initial parameter estimate is $\\theta^0 = \\begin{pmatrix}1.0  0.0\\end{pmatrix}^\\top$.\n\nFirst, we compute the Jacobian matrix $J$. The partial derivatives of the error are:\n$$\\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta_1} = -u(t), \\quad \\frac{\\partial \\epsilon(t,\\theta)}{\\partial \\theta_2} = -1$$\nThe Jacobian $J$ is constant for this linear model:\n$$J = \\begin{pmatrix} -u(1)  -1 \\\\ -u(2)  -1 \\\\ -u(3)  -1 \\end{pmatrix} = \\begin{pmatrix} 0  -1 \\\\ -1  -1 \\\\ -2  -1 \\end{pmatrix}$$\nNext, we compute the residual vector $\\mathbf{\\epsilon}(\\theta^0)$ at the initial point $\\theta^0$:\n$$\\epsilon(1,\\theta^0) = -0.45 - (1.0 \\cdot 0 + 0.0) = -0.45$$\n$$\\epsilon(2,\\theta^0) = 1.48 - (1.0 \\cdot 1 + 0.0) = 0.48$$\n$$\\epsilon(3,\\theta^0) = 3.51 - (1.0 \\cdot 2 + 0.0) = 1.51$$\nSo, $\\mathbf{\\epsilon}(\\theta^0) = \\begin{pmatrix}-0.45  0.48  1.51\\end{pmatrix}^\\top$.\n\nTo find the Gauss-Newton direction $p$, we compute $J^\\top J$ and $J^\\top \\mathbf{\\epsilon}(\\theta^0)$:\n$$J^\\top J = \\begin{pmatrix} 0  -1  -2 \\\\ -1  -1  -1 \\end{pmatrix} \\begin{pmatrix} 0  -1 \\\\ -1  -1 \\\\ -2  -1 \\end{pmatrix} = \\begin{pmatrix} 5  3 \\\\ 3  3 \\end{pmatrix}$$\n$$(J^\\top J)^{-1} = \\frac{1}{5 \\cdot 3 - 3 \\cdot 3} \\begin{pmatrix} 3  -3 \\\\ -3  5 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 3  -3 \\\\ -3  5 \\end{pmatrix}$$\n$$J^\\top \\mathbf{\\epsilon}(\\theta^0) = \\begin{pmatrix} 0  -1  -2 \\\\ -1  -1  -1 \\end{pmatrix} \\begin{pmatrix} -0.45 \\\\ 0.48 \\\\ 1.51 \\end{pmatrix} = \\begin{pmatrix} 0 - 0.48 - 3.02 \\\\ 0.45 - 0.48 - 1.51 \\end{pmatrix} = \\begin{pmatrix} -3.50 \\\\ -1.54 \\end{pmatrix}$$\nThe search direction $p$ is then:\n$$p = - (J^\\top J)^{-1} (J^\\top \\mathbf{\\epsilon}(\\theta^0)) = -\\frac{1}{6} \\begin{pmatrix} 3  -3 \\\\ -3  5 \\end{pmatrix} \\begin{pmatrix} -3.50 \\\\ -1.54 \\end{pmatrix} = -\\frac{1}{6} \\begin{pmatrix} -10.50 + 4.62 \\\\ 10.50 - 7.70 \\end{pmatrix} = -\\frac{1}{6} \\begin{pmatrix} -5.88 \\\\ 2.80 \\end{pmatrix} = \\begin{pmatrix} 0.98 \\\\ -\\frac{2.80}{6} \\end{pmatrix} = \\begin{pmatrix} 0.98 \\\\ -\\frac{7}{15} \\end{pmatrix}$$\n\nNow we check the strong Wolfe conditions for $\\alpha$ in the set $\\{\\;1,\\; \\tfrac{1}{2},\\; \\tfrac{1}{4}\\;\\}$, with $c_1=10^{-4}$ and $c_2=0.9$. We must first evaluate the terms at $\\alpha=0$:\n$$V_N(\\theta^0) = \\frac{1}{2(3)} \\left( (-0.45)^2 + (0.48)^2 + (1.51)^2 \\right) = \\frac{1}{6} (0.2025 + 0.2304 + 2.2801) = \\frac{2.713}{6} \\approx 0.452167$$\nThe gradient of the cost function is $\\nabla V_N(\\theta) = \\frac{1}{N} J^\\top \\mathbf{\\epsilon}(\\theta)$.\n$$\\nabla V_N(\\theta^0) = \\frac{1}{3} \\begin{pmatrix} -3.50 \\\\ -1.54 \\end{pmatrix}$$\nThe directional derivative at $\\alpha=0$ is $\\phi'(0) = \\nabla V_N(\\theta^0)^{\\top} p$:\n$$\\phi'(0) = \\frac{1}{3} \\begin{pmatrix} -3.50  -1.54 \\end{pmatrix} \\begin{pmatrix} 0.98 \\\\ -7/15 \\end{pmatrix} = \\frac{1}{3} \\left( -3.43 + \\frac{10.78}{15} \\right) = \\frac{1}{3} \\left( -3.43 + 0.71866... \\right) \\approx -0.903778$$\n\nWe start by testing the largest candidate step length, $\\alpha=1$:\nThe new parameter estimate is $\\theta(1) = \\theta^0 + 1 \\cdot p = \\begin{pmatrix} 1.0 \\\\ 0.0 \\end{pmatrix} + \\begin{pmatrix} 0.98 \\\\ -7/15 \\end{pmatrix} = \\begin{pmatrix} 1.98 \\\\ -7/15 \\end{pmatrix}$.\nLet us evaluate the residual vector at this new point, $\\mathbf{\\epsilon}(1)$:\n$$\\epsilon(1, \\theta(1)) = -0.45 - (1.98 \\cdot 0 - 7/15) = -0.45 + 7/15 = \\frac{-27+28}{60} = \\frac{1}{60}$$\n$$\\epsilon(2, \\theta(1)) = 1.48 - (1.98 \\cdot 1 - 7/15) = 1.48 - (1.98 - 0.4666...) = 1.48 - 1.5133... = \\frac{37}{25} - \\frac{227}{150} = \\frac{222-227}{150} = -\\frac{5}{150}=-\\frac{1}{30}$$\n$$\\epsilon(3, \\theta(1)) = 3.51 - (1.98 \\cdot 2 - 7/15) = 3.51 - (3.96 - 0.4666...) = 3.51 - 3.4933... = \\frac{351}{100} - \\frac{524}{150} = \\frac{1053-1048}{300} = \\frac{5}{300}=\\frac{1}{60}$$\nThe new cost is $V_N(\\theta(1)) = \\frac{1}{6} \\left( (\\frac{1}{60})^2 + (-\\frac{1}{30})^2 + (\\frac{1}{60})^2 \\right) = \\frac{1}{6 \\cdot 3600} (1+4+1) = \\frac{6}{6 \\cdot 3600} = \\frac{1}{3600} \\approx 0.0002778$.\n\nCheck Condition 1 (Sufficient Decrease) for $\\alpha=1$:\n$$V_N(\\theta(1)) \\le V_N(\\theta^0) + c_1 \\alpha \\phi'(0)$$\n$$0.0002778 \\le 0.452167 + 10^{-4} (1) (-0.903778)$$\n$$0.0002778 \\le 0.452167 - 0.00009038 = 0.452076...$$\nThis inequality is clearly true. The first condition is satisfied.\n\nCheck Condition 2 (Strong Curvature) for $\\alpha=1$:\n$$|\\nabla V_N(\\theta(1))^{\\top} p | \\le c_2 |\\phi'(0)|$$\nFirst, we compute the gradient at $\\theta(1)$:\n$$\\nabla V_N(\\theta(1)) = \\frac{1}{3} J^\\top \\mathbf{\\epsilon}(1) = \\frac{1}{3} \\begin{pmatrix} 0  -1  -2 \\\\ -1  -1  -1 \\end{pmatrix} \\begin{pmatrix} 1/60 \\\\ -1/30 \\\\ 1/60 \\end{pmatrix} = \\frac{1}{180} \\begin{pmatrix} 0  -1  -2 \\\\ -1  -1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\frac{1}{180} \\begin{pmatrix} 0+2-2 \\\\ -1+2-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe gradient is zero. This occurs because for a linear least-squares problem, the Gauss-Newton method is equivalent to Newton's method, and it finds the exact minimizer in a single full step $(\\alpha=1)$.\nThe directional derivative is $\\phi'(1) = \\nabla V_N(\\theta(1))^{\\top} p = \\begin{pmatrix} 0  0 \\end{pmatrix} p = 0$.\nThe condition becomes:\n$$|0| \\le 0.9 \\cdot |-0.903778| \\approx 0.8134$$\nThis inequality is also true. Both strong Wolfe conditions are satisfied for $\\alpha=1$.\n\nSince the question asks for the largest $\\alpha$ in the set $\\{\\;1,\\; \\tfrac{1}{2},\\; \\tfrac{1}{4}\\;\\}$ that satisfies the conditions, and $\\alpha=1$ satisfies them, it is the correct answer. There is no need to test the smaller values. The calculated step length, rounded to four significant figures, is $1.000$.", "answer": "$$\\boxed{1.000}$$", "id": "2892776"}]}