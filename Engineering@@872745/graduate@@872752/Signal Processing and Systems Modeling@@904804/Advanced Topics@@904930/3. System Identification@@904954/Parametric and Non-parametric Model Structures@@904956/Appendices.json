{"hands_on_practices": [{"introduction": "Parametric modeling forms the bedrock of system identification, where we assume the system follows a specific structure defined by a small set of parameters. This exercise [@problem_id:2889301] provides a concrete application of this principle by asking you to estimate the parameters of an AutoRegressive with eXogenous input (ARX) model. You will engage directly with the least-squares method, translating a set of input-output data into the famous normal equations to find the optimal parameter estimate.", "problem": "Consider the linear time-invariant AutoRegressive with eXogenous input (ARX) model specified in the prediction-error form\n$$\ny(k) \\;=\\; -a_{1}\\,y(k-1)\\;-\\;a_{2}\\,y(k-2)\\;+\\;b_{1}\\,u(k-1)\\;+\\;b_{2}\\,u(k-2)\\;+\\;e(k),\n$$\nwhere $y(k)$ is the system output, $u(k)$ is a known input, $e(k)$ is a zero-mean disturbance, and the unknown parameter vector is $\\theta \\equiv \\begin{pmatrix} a_{1} & a_{2} & b_{1} & b_{2} \\end{pmatrix}^{\\top}$. You are given measured input-output data and initial conditions\n$$\nu(-1)=0,\\quad u(0)=0,\\quad y(-1)=0,\\quad y(0)=0,\n$$\nand the following finite sequences:\n$$\n\\begin{aligned}\n&u(1)=1,\\; u(2)=2,\\; u(3)=0,\\; u(4)=-1,\\; u(5)=1,\\; u(6)=0,\\\\\n&y(1)=0,\\; y(2)=1,\\; y(3)=2.5,\\; y(4)=1.05,\\; y(5)=-0.975,\\; y(6)=0.3025,\\; y(7)=0.34625.\n\\end{aligned}\n$$\nAssume that for this dataset the disturbance is identically zero, i.e., $e(k)=0$ for all $k$ used below. Formulate the batch least-squares estimation problem that minimizes the sum of squared one-step-ahead prediction errors over the data indices $k=2,3,4,5,6,7$ and construct the corresponding linear normal equations in terms of the data above. Then compute the least-squares estimate of the parameter $b_{1}$.\n\nExpress your final reported value for the scalar $b_{1}$ as an exact number. No rounding is required. The final answer must be a single real number.", "solution": "We begin from the model definition and the least-squares principle. The one-step-ahead prediction error at time $k$ is\n$$\n\\varepsilon(k;\\theta)\\;=\\;y(k)\\;-\\;\\big(-a_{1}\\,y(k-1)-a_{2}\\,y(k-2)+b_{1}\\,u(k-1)+b_{2}\\,u(k-2)\\big).\n$$\nDefine the stacked regression vector\n$$\n\\varphi(k)\\;\\equiv\\;\\begin{pmatrix}-y(k-1)\\\\ -y(k-2)\\\\ u(k-1)\\\\ u(k-2)\\end{pmatrix},\\qquad \\theta\\;\\equiv\\;\\begin{pmatrix}a_{1}\\\\ a_{2}\\\\ b_{1}\\\\ b_{2}\\end{pmatrix},\n$$\nso that the error is $\\varepsilon(k;\\theta)=y(k)-\\varphi(k)^{\\top}\\theta$. The batch Least Squares (LS) criterion over the index set $\\mathcal{K}=\\{2,3,4,5,6,7\\}$ is\n$$\nJ(\\theta)\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varepsilon(k;\\theta)^{2}\\;=\\;\\sum_{k\\in\\mathcal{K}}\\big(y(k)-\\varphi(k)^{\\top}\\theta\\big)^{2}.\n$$\nMinimizing $J(\\theta)$ with respect to $\\theta$ by setting its gradient to zero yields the normal equations\n$$\n\\left(\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,\\varphi(k)^{\\top}\\right)\\theta\\;=\\;\\sum_{k\\in\\mathcal{K}}\\varphi(k)\\,y(k).\n$$\nEquivalently, if we define the data matrix $\\Phi\\in\\mathbb{R}^{6\\times 4}$ with rows $\\varphi(k)^{\\top}$ for $k=2,\\dots,7$ and the data vector $Y\\in\\mathbb{R}^{6}$ with entries $y(k)$ for $k=2,\\dots,7$, we have the compact form\n$$\n\\Phi^{\\top}\\Phi\\,\\theta\\;=\\;\\Phi^{\\top}Y.\n$$\n\nWe now build $\\Phi$ and $Y$ explicitly from the provided data. For each $k\\in\\{2,3,4,5,6,7\\}$ we compute $\\varphi(k)$:\n$$\n\\begin{aligned}\n&k=2:\\;\\; \\varphi(2)^{\\top}=\\begin{pmatrix}-y(1)&-y(0)&u(1)&u(0)\\end{pmatrix}=\\begin{pmatrix}0&0&1&0\\end{pmatrix},\\;\\; y(2)=1,\\\\\n&k=3:\\;\\; \\varphi(3)^{\\top}=\\begin{pmatrix}-y(2)&-y(1)&u(2)&u(1)\\end{pmatrix}=\\begin{pmatrix}-1&0&2&1\\end{pmatrix},\\;\\; y(3)=2.5,\\\\\n&k=4:\\;\\; \\varphi(4)^{\\top}=\\begin{pmatrix}-y(3)&-y(2)&u(3)&u(2)\\end{pmatrix}=\\begin{pmatrix}-2.5&-1&0&2\\end{pmatrix},\\;\\; y(4)=1.05,\\\\\n&k=5:\\;\\; \\varphi(5)^{\\top}=\\begin{pmatrix}-y(4)&-y(3)&u(4)&u(3)\\end{pmatrix}=\\begin{pmatrix}-1.05&-2.5&-1&0\\end{pmatrix},\\;\\; y(5)=-0.975,\\\\\n&k=6:\\;\\; \\varphi(6)^{\\top}=\\begin{pmatrix}-y(5)&-y(4)&u(5)&u(4)\\end{pmatrix}=\\begin{pmatrix}0.975&-1.05&1&-1\\end{pmatrix},\\;\\; y(6)=0.3025,\\\\\n&k=7:\\;\\; \\varphi(7)^{\\top}=\\begin{pmatrix}-y(6)&-y(5)&u(6)&u(5)\\end{pmatrix}=\\begin{pmatrix}-0.3025&0.975&0&1\\end{pmatrix},\\;\\; y(7)=0.34625.\n\\end{aligned}\n$$\nThus\n$$\n\\Phi=\\begin{pmatrix}\n0 & 0 & 1 & 0\\\\\n-1 & 0 & 2 & 1\\\\\n-2.5 & -1 & 0 & 2\\\\\n-1.05 & -2.5 & -1 & 0\\\\\n0.975 & -1.05 & 1 & -1\\\\\n-0.3025 & 0.975 & 0 & 1\n\\end{pmatrix},\\qquad\nY=\\begin{pmatrix}\n1\\\\\n2.5\\\\\n1.05\\\\\n-0.975\\\\\n0.3025\\\\\n0.34625\n\\end{pmatrix}.\n$$\nThe normal equations take the explicit form\n$$\n\\underbrace{\\Phi^{\\top}\\Phi}_{G}\\,\\theta\\;=\\;\\underbrace{\\Phi^{\\top}Y}_{g},\n$$\nwith\n$$\nG=\\begin{pmatrix}\n9.39463125 & 3.8063125 & 0.025 & -7.2775\\\\\n3.8063125 & 9.303125 & 1.45 & 0.025\\\\\n0.025 & 1.45 & 7 & 1\\\\\n-7.2775 & 0.025 & 1 & 7\n\\end{pmatrix},\\qquad\ng=\\begin{pmatrix}\n-3.911053125\\\\\n1.40746875\\\\\n7.2775\\\\\n4.64375\n\\end{pmatrix}.\n$$\nThis completes the construction of the normal equations.\n\nTo compute the least-squares estimate, we note that the LS solution satisfies $\\Phi\\,\\hat{\\theta}=Y$ exactly if the data are noise-free and $\\Phi$ has full column rank. From the first regression row (corresponding to $k=2$),\n$$\ny(2)\\;=\\;-a_{1}\\,y(1)\\;-\\;a_{2}\\,y(0)\\;+\\;b_{1}\\,u(1)\\;+\\;b_{2}\\,u(0)\\;=\\;0\\;+\\;0\\;+\\;b_{1}\\cdot 1\\;+\\;0,\n$$\nwhich implies immediately\n$$\nb_{1}\\;=\\;y(2)\\;=\\;1.\n$$\nFor completeness, we can verify that there exist $(a_{1},a_{2},b_{2})$ making all residuals zero with this $b_{1}$. Using three subsequent equations (for $k=3,4,5$) with $b_{1}=1$,\n$$\n\\begin{aligned}\n&k=3:\\;\\;2.5=-a_{1}\\cdot 1-a_{2}\\cdot 0+1\\cdot 2+b_{2}\\cdot 1\\;\\;\\Rightarrow\\;\\;-a_{1}+b_{2}=0.5,\\\\\n&k=4:\\;\\;1.05=-a_{1}\\cdot 2.5-a_{2}\\cdot 1+1\\cdot 0+b_{2}\\cdot 2\\;\\;\\Rightarrow\\;\\;-2.5a_{1}-a_{2}+2b_{2}=1.05,\\\\\n&k=5:\\;\\;-0.975=-a_{1}\\cdot 1.05-a_{2}\\cdot 2.5+1\\cdot(-1)+b_{2}\\cdot 0\\;\\;\\Rightarrow\\;\\;-1.05a_{1}-2.5a_{2}=-0.975+1=0.025,\n\\end{aligned}\n$$\nwhich solve to $a_{1}=-0.5$, $a_{2}=0.2$, and $b_{2}=0$. With these values, all six equations are satisfied exactly, and thus the residual vector is identically zero. Consequently the LS estimator coincides with this solution, and in particular the least-squares estimate of $b_{1}$ is\n$$\n\\hat{b}_{1}\\;=\\;1.\n$$\nThis value also satisfies the third component of the normal equations, since substituting $\\theta=\\begin{pmatrix}-0.5&0.2&1&0\\end{pmatrix}^{\\top}$ gives\n$$\n\\begin{pmatrix}0.025 & 1.45 & 7 & 1\\end{pmatrix}\\theta\\;=\\;0.025(-0.5)+1.45(0.2)+7(1)+1(0)\\;=\\;-0.0125+0.29+7\\;=\\;7.2775\\;=\\;g_{3}.\n$$\nTherefore, the least-squares estimate of $b_{1}$ for the given dataset is exactly $1$.", "answer": "$$\\boxed{1}$$", "id": "2889301"}, {"introduction": "Moving from the rigid structure of parametric models, we now explore the flexibility of non-parametric spectral estimation with Welch's method. This powerful technique estimates the power spectral density without assuming an underlying generative model, making it a robust tool for exploratory data analysis. This practice problem [@problem_id:2889322] focuses on a critical aspect of the method: understanding and calculating the frequency resolution, which is directly controlled by the choice of window length.", "problem": "A real, wide-sense stationary, zero-mean discrete-time process $x[n]$ is sampled at a sampling frequency $f_{s} = 48\\,\\text{kHz}$. You are to estimate its power spectral density (PSD) using Welch’s method, which is a non-parametric spectral estimator, and contrast its notion of frequency resolution with that of parametric approaches, without computing any parametric estimate. The total record length is $N_{\\text{tot}} = 1{,}228{,}800$ samples. You choose the following Welch parameters: segment length $L = 4096$ samples, $50\\%$ overlap between adjacent segments, and the periodic Hann window $w[n]$ defined for $n=0,1,\\dots,L-1$ as\n$$\nw[n] = \\tfrac{1}{2}\\Big(1 - \\cos\\!\\big(\\tfrac{2\\pi n}{L}\\big)\\Big).\n$$\nAssume the Discrete Fourier Transform (DFT) length is $K=L$ for each segment (no zero-padding).\n\nIn this setting, define the frequency resolution of the Welch PSD estimate as the Equivalent Noise Bandwidth (ENBW) of the spectral window, expressed in hertz. Using only fundamental definitions of Welch’s method and ENBW, and properties of discrete-time sums of cosine sequences, compute the resulting frequency resolution in hertz. Provide your final answer as an exact value with no rounding, and express the result in $\\text{Hz}$ (do not include units in the final boxed answer).", "solution": "The frequency resolution for a non-parametric spectral estimator like Welch's method is determined by the spectral characteristics of the window function applied to each data segment. The problem defines this resolution as the Equivalent Noise Bandwidth (ENBW) of the window, expressed in hertz.\n\nThe ENBW of a discrete-time window $w[n]$ of length $L$ is defined in normalized frequency (cycles/sample) as the ratio of the sum of the squared window coefficients to the square of the sum of the window coefficients:\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{\\sum_{n=0}^{L-1} w^2[n]}{\\left(\\sum_{n=0}^{L-1} w[n]\\right)^2}\n$$\nTo convert this normalized bandwidth to a physical frequency in hertz, one must multiply by the sampling frequency, $f_s$:\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\text{ENBW}_{\\text{norm}} \\times f_s\n$$\nThe task reduces to calculating the two sums for the given periodic Hann window, $w[n] = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)$, for $n=0, 1, \\dots, L-1$.\n\nFirst, we compute the sum of the window coefficients, which we denote $S_1$:\n$$\nS_1 = \\sum_{n=0}^{L-1} w[n] = \\sum_{n=0}^{L-1} \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\nBy linearity of summation:\n$$\nS_1 = \\frac{1}{2} \\left[ \\left(\\sum_{n=0}^{L-1} 1\\right) - \\left(\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]\n$$\nThe first term is $\\sum_{n=0}^{L-1} 1 = L$. The second term is the sum of a cosine over one full cycle ($L$ samples). This sum is zero for $L > 1$. This can be shown by considering the sum of the complex exponential $\\sum_{n=0}^{L-1} \\exp\\left(j\\frac{2\\pi k n}{L}\\right)$, which equals $0$ for any integer $k$ not a multiple of $L$. Here, $k=1$. Taking the real part confirms $\\sum_{n=0}^{L-1} \\cos(\\frac{2\\pi n}{L}) = 0$. Given $L = 4096$, this condition holds.\nThus,\n$$\nS_1 = \\frac{1}{2} (L - 0) = \\frac{L}{2}\n$$\n\nNext, we compute the sum of the squared window coefficients, which we denote $S_2$:\n$$\nS_2 = \\sum_{n=0}^{L-1} w^2[n] = \\sum_{n=0}^{L-1} \\left[ \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right) \\right]^2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - \\cos\\left(\\frac{2\\pi n}{L}\\right)\\right)^2\n$$\nExpanding the square:\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\cos^2\\left(\\frac{2\\pi n}{L}\\right)\\right)\n$$\nWe use the power-reduction identity $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$:\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(1 - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\\right)\n$$\n$$\nS_2 = \\frac{1}{4} \\sum_{n=0}^{L-1} \\left(\\frac{3}{2} - 2\\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\cos\\left(\\frac{4\\pi n}{L}\\right)\\right)\n$$\nDistributing the summation:\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3}{2}\\sum_{n=0}^{L-1} 1 - 2\\sum_{n=0}^{L-1} \\cos\\left(\\frac{2\\pi n}{L}\\right) + \\frac{1}{2}\\sum_{n=0}^{L-1} \\cos\\left(\\frac{4\\pi n}{L}\\right) \\right]\n$$\nAs established, $\\sum \\cos(\\frac{2\\pi n}{L}) = 0$. Similarly, $\\sum \\cos(\\frac{4\\pi n}{L}) = 0$ for $L>2$, which is true for $L=4096$.\nThis leaves:\n$$\nS_2 = \\frac{1}{4} \\left[ \\frac{3L}{2} - 2(0) + \\frac{1}{2}(0) \\right] = \\frac{3L}{8}\n$$\n\nNow, we can compute the normalized ENBW:\n$$\n\\text{ENBW}_{\\text{norm}} = \\frac{S_2}{S_1^2} = \\frac{\\frac{3L}{8}}{\\left(\\frac{L}{2}\\right)^2} = \\frac{\\frac{3L}{8}}{\\frac{L^2}{4}} = \\frac{3L \\cdot 4}{8 \\cdot L^2} = \\frac{12L}{8L^2} = \\frac{3}{2L}\n$$\nThis is the well-known ENBW for the Hann window. The frequency resolution depends inversely on the window length $L$.\n\nFinally, we compute the resolution in hertz using the given values $L=4096$ and $f_s = 48000\\,\\text{Hz}$:\n$$\n\\text{Resolution} = \\text{ENBW}_{\\text{Hz}} = \\frac{3}{2L} \\times f_s = \\frac{3}{2 \\times 4096} \\times 48000 = \\frac{3 \\times 48000}{8192} = \\frac{144000}{8192}\n$$\nTo simplify the fraction:\n$$\n\\frac{144000}{8192} = \\frac{144 \\times 1000}{8 \\times 1024} = \\frac{18 \\times 1000}{1024} = \\frac{18000}{1024} = \\frac{9000}{512} = \\frac{4500}{256} = \\frac{2250}{128} = \\frac{1125}{64}\n$$\nThe exact value is $1125/64$, which in decimal form is $17.578125$.\n\nThe problem also requires a contrast with parametric approaches. In non-parametric methods like Welch's, the frequency resolution, here defined as $1.5 \\frac{f_s}{L}$, is fixed by the analyst's choice of the window length $L$. A longer window provides better resolution (narrower main lobe) but at the cost of increased estimator variance for a fixed total data record, as fewer segments can be averaged. This resolution is independent of the signal content. In contrast, parametric methods (e.g., Autoregressive (AR), Moving Average (MA), or ARMA models) postulate that the signal is generated by a linear time-invariant system driven by white noise. The PSD is then a function of the model parameters. The \"resolution\" in this context is not a fixed bandwidth but rather the ability to represent sharp spectral features. If the underlying data conforms to the chosen model structure, parametric methods can achieve superior resolution, resolving closely spaced sinusoids even with very short data records, far exceeding the $1/L$ limitation of non-parametric methods. However, this performance is critically dependent on the correctness of the model assumption; a model mismatch can lead to a spectral estimate that is grossly inaccurate and misleading.", "answer": "$$\\boxed{17.578125}$$", "id": "2889322"}, {"introduction": "The design choices in non-parametric estimation, such as the window length in Welch's method, are governed by the fundamental bias-variance trade-off. This advanced exercise [@problem_id:2889340] moves from computation to theory, guiding you through the derivation of the asymptotic bias and variance for a general class of spectral estimators. By minimizing the resulting mean-squared error, you will determine the optimal smoothing bandwidth, providing a rigorous justification for the trade-offs encountered in practices like [@problem_id:2889322].", "problem": "Consider a zero-mean, Gaussian, wide-sense stationary (WSS) discrete-time process with power spectral density (PSD) denoted by $S(\\omega)$, observed over $N$ consecutive samples. Let $I_{N}(\\omega)$ denote the raw periodogram formed from these $N$ samples. A non-parametric spectral estimator is constructed by convolving $I_{N}(\\omega)$ with the squared magnitude of a data-taper Fourier transform,\n$$\n\\hat{S}_{B}(\\omega_{0}) \\;\\triangleq\\; \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} \\big|W_{B}(\\omega_{0}-\\lambda)\\big|^{2}\\, I_{N}(\\lambda)\\, d\\lambda,\n$$\nwhere the taper family is parameterized by a mainlobe half-width parameter $B>0$ via the scaling\n$$\n\\big|W_{B}(\\omega)\\big|^{2} \\;=\\; \\frac{2\\pi}{B}\\,K\\!\\left(\\frac{\\omega}{B}\\right),\n$$\nwith $K(\\cdot)$ an even, nonnegative kernel satisfying $\\int_{-\\infty}^{\\infty} K(u)\\,du=1$, finite second moment $\\mu_{2}(K)\\triangleq \\int_{-\\infty}^{\\infty} u^{2}K(u)\\,du<\\infty$, and finite energy $R(K)\\triangleq \\int_{-\\infty}^{\\infty} K(u)^{2}\\,du<\\infty$. This normalization ensures $\\frac{1}{2\\pi}\\int |W_{B}(\\omega)|^{2}\\,d\\omega=1$, so that white spectra are preserved in expectation.\n\nAssume that the true spectrum is twice continuously differentiable in a neighborhood of a target frequency $\\omega_{0}\\in(-\\pi,\\pi)$ and admits the local quadratic approximation\n$$\nS(\\omega) \\;=\\; S(\\omega_{0}) \\;+\\; \\frac{1}{2}S''(\\omega_{0})\\,(\\omega-\\omega_{0})^{2} \\;+\\; o\\!\\left((\\omega-\\omega_{0})^{2}\\right).\n$$\nUsing only the foundational definitions above (WSS Gaussian process, raw periodogram, and the spectral window convolution estimator), derive the leading-order bias and variance of $\\hat{S}_{B}(\\omega_{0})$ as functions of $B$ and $N$, and then minimize the asymptotic mean-squared error with respect to $B$ to obtain the mainlobe half-width $B^{\\star}$ that balances bias and variance.\n\nProvide your final result as a single closed-form analytic expression for $B^{\\star}$ in terms of $N$, $S(\\omega_{0})$, $S''(\\omega_{0})$, $\\mu_{2}(K)$, and $R(K)$. Express $B^{\\star}$ in radians. No numerical evaluation is required.", "solution": "The objective is to find the optimal mainlobe half-width $B^{\\star}$ that minimizes the asymptotic Mean-Squared Error (MSE) of the spectral estimator $\\hat{S}_{B}(\\omega_{0})$. The MSE is defined as the sum of the squared bias and the variance:\n$$\n\\text{MSE}[\\hat{S}_{B}(\\omega_{0})] = \\left(\\text{Bias}[\\hat{S}_{B}(\\omega_{0})]\\right)^{2} + \\text{Var}[\\hat{S}_{B}(\\omega_{0})]\n$$\nWe will derive the leading-order terms for the bias and the variance as functions of the bandwidth parameter $B$ and the number of samples $N$.\n\nFirst, we compute the bias. The bias is defined as $\\text{Bias}[\\hat{S}_{B}(\\omega_{0})] = E[\\hat{S}_{B}(\\omega_{0})] - S(\\omega_{0})$. The expectation of the estimator $\\hat{S}_{B}(\\omega_{0})$ is:\n$$\nE[\\hat{S}_{B}(\\omega_{0})] = E\\left[ \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} |W_{B}(\\omega_{0}-\\lambda)|^{2} I_{N}(\\lambda) d\\lambda \\right] = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} |W_{B}(\\omega_{0}-\\lambda)|^{2} E[I_{N}(\\lambda)] d\\lambda\n$$\nFor large $N$, the expected value of the periodogram, $E[I_{N}(\\lambda)]$, approaches the true power spectral density $S(\\lambda)$. Using this asymptotic approximation, we have:\n$$\nE[\\hat{S}_{B}(\\omega_{0})] \\approx \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} |W_{B}(\\omega_{0}-\\lambda)|^{2} S(\\lambda) d\\lambda\n$$\nThis is a convolution of the true spectrum $S(\\lambda)$ with the smoothing kernel $|W_{B}(\\cdot)|^{2}$. Let's perform a change of variables $\\omega = \\omega_{0}-\\lambda$.\n$$\nE[\\hat{S}_{B}(\\omega_{0})] \\approx \\frac{1}{2\\pi}\\int_{\\omega_{0}-\\pi}^{\\omega_{0}+\\pi} |W_{B}(\\omega)|^{2} S(\\omega_{0}-\\omega) d\\omega\n$$\nSince all functions are periodic with period $2\\pi$, we can integrate over $[-\\pi, \\pi]$. The kernel $|W_{B}(\\omega)|^{2}$ is concentrated around $\\omega=0$ for small $B$. We use the given Taylor expansion for $S(\\lambda)$ around $\\lambda=\\omega_{0}$, which implies an expansion for $S(\\omega_{0}-\\omega)$ around $\\omega=0$:\n$$\nS(\\omega_{0}-\\omega) = S(\\omega_{0}) + \\frac{1}{2}S''(\\omega_{0})(-\\omega)^{2} + o(\\omega^{2}) = S(\\omega_{0}) + \\frac{1}{2}S''(\\omega_{0})\\omega^{2} + o(\\omega^{2})\n$$\nSubstituting this into the integral for the expected value:\n$$\nE[\\hat{S}_{B}(\\omega_{0})] \\approx \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} |W_{B}(\\omega)|^{2} \\left[ S(\\omega_{0}) + \\frac{1}{2}S''(\\omega_{0})\\omega^{2} \\right] d\\omega\n$$\n$$\nE[\\hat{S}_{B}(\\omega_{0})] \\approx S(\\omega_{0}) \\left(\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} |W_{B}(\\omega)|^{2} d\\omega\\right) + \\frac{1}{2}S''(\\omega_{0}) \\left(\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} \\omega^{2}|W_{B}(\\omega)|^{2} d\\omega \\right)\n$$\nThe first integral is equal to $1$ due to the normalization of the kernel. For the second integral, we substitute the definition of $|W_{B}(\\omega)|^{2}$ and extend the integration limits from $[-\\pi, \\pi]$ to $(-\\infty, \\infty)$ as $B \\to 0$:\n$$\n\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\omega^{2} |W_{B}(\\omega)|^{2} d\\omega = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\omega^{2} \\frac{2\\pi}{B} K\\left(\\frac{\\omega}{B}\\right) d\\omega = \\frac{1}{B}\\int_{-\\infty}^{\\infty} \\omega^{2} K\\left(\\frac{\\omega}{B}\\right) d\\omega\n$$\nUsing the substitution $u = \\omega/B$, which gives $\\omega = Bu$ and $d\\omega = B du$:\n$$\n\\frac{1}{B}\\int_{-\\infty}^{\\infty} (Bu)^{2} K(u) (B du) = B^{2} \\int_{-\\infty}^{\\infty} u^{2} K(u) du = B^{2} \\mu_{2}(K)\n$$\nThus, the expected value is $E[\\hat{S}_{B}(\\omega_{0})] \\approx S(\\omega_{0}) + \\frac{1}{2} B^{2} \\mu_{2}(K) S''(\\omega_{0})$. The leading-order bias is:\n$$\n\\text{Bias}[\\hat{S}_{B}(\\omega_{0})] \\approx \\frac{1}{2} B^{2} \\mu_{2}(K) S''(\\omega_{0})\n$$\nThe squared bias is therefore $(\\text{Bias})^2 \\approx \\frac{1}{4} B^{4} [\\mu_{2}(K)]^{2} [S''(\\omega_{0})]^{2}$.\n\nNext, we compute the variance. For a Gaussian process and large $N$, the periodogram ordinates are approximately uncorrelated. The variance of the estimator is:\n$$\n\\text{Var}[\\hat{S}_{B}(\\omega_{0})] = \\left(\\frac{1}{2\\pi}\\right)^{2} \\iint |W_{B}(\\omega_{0}-\\lambda_{1})|^{2} |W_{B}(\\omega_{0}-\\lambda_{2})|^{2} \\text{Cov}[I_{N}(\\lambda_{1}), I_{N}(\\lambda_{2})] d\\lambda_{1}d\\lambda_{2}\n$$\nThe asymptotic covariance of the periodogram for a Gaussian process is approximately (for $\\lambda_{1}, \\lambda_{2} \\neq 0, \\pi$):\n$$\n\\text{Cov}[I_{N}(\\lambda_{1}), I_{N}(\\lambda_{2})] \\approx \\frac{2\\pi}{N} S(\\lambda_{1})^{2} \\left[ \\delta(\\lambda_{1}-\\lambda_{2}) + \\delta(\\lambda_{1}+\\lambda_{2}) \\right]\n$$\nSince $\\omega_{0} \\in (-\\pi, \\pi)$ and for small $B$ the window concentrates mass around $\\omega_0$, both $\\lambda_{1}$ and $\\lambda_{2}$ are close to $\\omega_{0}$. If $\\omega_{0} \\neq 0$, then $\\lambda_{1}+\\lambda_{2} \\approx 2\\omega_{0} \\neq 0$, so the term $\\delta(\\lambda_{1}+\\lambda_{2})$ does not contribute.\n$$\n\\text{Var}[\\hat{S}_{B}(\\omega_{0})] \\approx \\left(\\frac{1}{2\\pi}\\right)^{2} \\iint |W_{B}(\\omega_{0}-\\lambda_{1})|^{2} |W_{B}(\\omega_{0}-\\lambda_{2})|^{2} \\frac{2\\pi}{N} S(\\lambda_{1})^{2} \\delta(\\lambda_{1}-\\lambda_{2}) d\\lambda_{1}d\\lambda_{2}\n$$\nIntegrating with respect to $\\lambda_{2}$:\n$$\n\\text{Var}[\\hat{S}_{B}(\\omega_{0})] \\approx \\frac{1}{2\\pi N} \\int |W_{B}(\\omega_{0}-\\lambda_{1})|^{4} S(\\lambda_{1})^{2} d\\lambda_{1}\n$$\nChange variables to $\\omega = \\omega_{0}-\\lambda_{1}$. Since the window is narrow, we approximate $S(\\omega_{0}-\\omega) \\approx S(\\omega_{0})$:\n$$\n\\text{Var}[\\hat{S}_{B}(\\omega_{0})] \\approx \\frac{S(\\omega_{0})^{2}}{2\\pi N} \\int_{-\\pi}^{\\pi} |W_{B}(\\omega)|^{4} d\\omega\n$$\nWe evaluate the integral by substituting the definition of $|W_{B}(\\omega)|^{2}$:\n$$\n\\int_{-\\infty}^{\\infty} |W_{B}(\\omega)|^{4} d\\omega = \\int_{-\\infty}^{\\infty} \\left(\\frac{2\\pi}{B} K\\left(\\frac{\\omega}{B}\\right)\\right)^{2} d\\omega = \\frac{4\\pi^{2}}{B^{2}} \\int_{-\\infty}^{\\infty} K\\left(\\frac{\\omega}{B}\\right)^{2} d\\omega\n$$\nLet $u=\\omega/B$, so $d\\omega=Bdu$:\n$$\n\\frac{4\\pi^{2}}{B^{2}} \\int_{-\\infty}^{\\infty} K(u)^{2} (Bdu) = \\frac{4\\pi^{2}}{B} \\int_{-\\infty}^{\\infty} K(u)^{2} du = \\frac{4\\pi^{2}}{B} R(K)\n$$\nThe variance is then:\n$$\n\\text{Var}[\\hat{S}_{B}(\\omega_{0})] \\approx \\frac{S(\\omega_{0})^{2}}{2\\pi N} \\frac{4\\pi^{2} R(K)}{B} = \\frac{2\\pi R(K) S(\\omega_{0})^{2}}{NB}\n$$\n\nThe asymptotic MSE is the sum of the squared bias and the variance:\n$$\n\\text{MSE}(B) \\approx \\frac{1}{4} B^{4} [\\mu_{2}(K) S''(\\omega_{0})]^{2} + \\frac{2\\pi R(K) S(\\omega_{0})^{2}}{N B}\n$$\nTo find the optimal bandwidth $B^{\\star}$ that minimizes this MSE, we differentiate with respect to $B$ and set the result to zero:\n$$\n\\frac{d}{dB}\\text{MSE}(B) = B^{3} [\\mu_{2}(K) S''(\\omega_{0})]^{2} - \\frac{2\\pi R(K) S(\\omega_{0})^{2}}{N B^{2}} = 0\n$$\n$$\nB^{3} [\\mu_{2}(K) S''(\\omega_{0})]^{2} = \\frac{2\\pi R(K) S(\\omega_{0})^{2}}{N B^{2}}\n$$\n$$\nB^{5} = \\frac{2\\pi R(K) S(\\omega_{0})^{2}}{N [\\mu_{2}(K)]^{2} [S''(\\omega_{0})]^{2}}\n$$\nSolving for $B$ yields the optimal bandwidth $B^{\\star}$:\n$$\nB^{\\star} = \\left( \\frac{2\\pi R(K) S(\\omega_{0})^{2}}{N [\\mu_{2}(K)]^{2} [S''(\\omega_{0})]^{2}} \\right)^{1/5}\n$$\nThe second derivative is positive for $B>0$, confirming this is a minimum. This expression provides the optimal balance between the squared bias, which increases as $B^{4}$, and the variance, which decreases as $(NB)^{-1}$.", "answer": "$$\n\\boxed{\\left(\\frac{2\\pi R(K) S(\\omega_{0})^{2}}{N [\\mu_{2}(K)]^{2} [S''(\\omega_{0})]^{2}}\\right)^{1/5}}\n$$", "id": "2889340"}]}