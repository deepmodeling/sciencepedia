{"hands_on_practices": [{"introduction": "A cornerstone of model validation is assessing whether the residuals are indistinguishable from a white noise process. The Ljung-Box test provides a powerful portmanteau statistic for this purpose, jointly testing the significance of a set of residual autocorrelations. However, a critical subtlety arises when applying this test not to raw data, but to residuals from an estimated model. This exercise [@problem_id:2885037] will guide you through the reasoning for why the test's reference distribution must be adjusted, a crucial step for correctly interpreting its results and avoiding erroneous conclusions about your model's adequacy.", "problem": "A discrete-time, wide-sense stationary stochastic process is modeled in the linear time series framework and fitted by an Autoregressive Moving-Average (ARMA) model. You estimate an Autoregressive Moving-Average (ARMA) model of order $(2,1)$ to a scalar time series of length $N=500$ using standard maximum likelihood techniques. Let $\\{e_t\\}_{t=1}^{N}$ denote the resulting one-step-ahead prediction residuals. To assess residual whiteness, you plan to use the Ljung–Box (LB) portmanteau statistic $Q_{\\mathrm{LB}}(m)$ based on autocorrelations up to lag $m=20$.\n\nStarting from first principles about residual whiteness and large-sample behavior of sample autocorrelations under the null hypothesis that $\\{e_t\\}$ are independent and identically distributed (i.i.d.) with zero mean and finite variance, and using the well-tested asymptotic fact that, when no dynamic parameters are estimated, the sum of $m$ squared, appropriately scaled residual autocorrelations is approximately chi-squared distributed with $m$ degrees of freedom, derive the appropriate degrees of freedom to be used for the reference chi-squared distribution of $Q_{\\mathrm{LB}}(m)$ when an ARMA$(2,1)$ model is estimated from the same data.\n\nThen, using this reference, state a decision rule at significance level $\\alpha=0.05$ for testing the null hypothesis that the residuals are i.i.d. (that is, white noise), in terms of the upper quantile of the chi-squared distribution. Do not compute any numerical quantiles.\n\nReport the degrees of freedom as your final answer. No rounding is required, and no units should be included in the final answer.", "solution": "The problem requires the derivation of the degrees of freedom for a Ljung-Box test applied to the residuals of an estimated Autoregressive Moving-Average (ARMA) model. The derivation must proceed from first principles.\n\nLet the observed time series be $\\{y_t\\}$. We have fitted an ARMA model of order $(p,q)$, which is specified as:\n$$ y_t - \\sum_{i=1}^p \\phi_i y_{t-i} = \\epsilon_t + \\sum_{j=1}^q \\theta_j \\epsilon_{t-j} $$\nwhere $\\{\\epsilon_t\\}$ is assumed to be a white noise process with zero mean and constant variance $\\sigma^2$. A white noise process is a sequence of independent and identically distributed (i.i.d.) random variables. In this problem, the model order is specified as ARMA($2,1$), so we have $p=2$ and $q=1$.\n\nThe problem states to begin with the asymptotic behavior of sample autocorrelations for a true white noise process. If we have a sequence of $N$ observations from a white noise process, $\\{\\epsilon_t\\}$, the sample autocorrelation at lag $k$, denoted $\\hat{\\rho}_k$, is for large $N$ approximately normally distributed with mean $0$ and variance $1/N$. Furthermore, the sample autocorrelations at different lags are approximately independent.\n$$ \\hat{\\rho}_k \\stackrel{\\text{approx}}{\\sim} N(0, 1/N) \\quad \\text{for } k>0 $$\nFrom this, it follows that the quantity $N\\hat{\\rho}_k^2$ is approximately distributed as a chi-squared random variable with $1$ degree of freedom, $\\chi^2(1)$. The sum of $m$ such independent quantities, a test statistic known as the Box-Pierce statistic, is given by $Q_{\\mathrm{BP}}(m) = N \\sum_{k=1}^m \\hat{\\rho}_k^2$. This statistic is asymptotically distributed as a chi-squared random variable with $m$ degrees of freedom, $\\chi^2(m)$.\n\nThe Ljung-Box statistic, $Q_{\\mathrm{LB}}(m)$, is a modification of the Box-Pierce statistic that provides a better approximation in finite samples:\n$$ Q_{\\mathrm{LB}}(m) = N(N+2) \\sum_{k=1}^m \\frac{\\hat{\\rho}_k^2}{N-k} $$\nUnder the null hypothesis that the series is white noise, $Q_{\\mathrm{LB}}(m)$ also converges to a $\\chi^2(m)$ distribution as $N \\to \\infty$. This is the baseline case where no model parameters are estimated from the data.\n\nHowever, the problem concerns the residuals, $\\{e_t\\}$, obtained after estimating the parameters of the ARMA($2,1$) model. The parameters are the $p=2$ autoregressive coefficients, $\\{\\phi_1, \\phi_2\\}$, and the $q=1$ moving-average coefficient, $\\{\\theta_1\\}$. The total number of estimated dynamic parameters is $p+q$. Let the estimates be $\\{\\hat{\\phi}_1, \\hat{\\phi}_2\\}$ and $\\{\\hat{\\theta}_1\\}$. The residuals are computed as:\n$$ e_t = y_t - \\hat{\\phi}_1 y_{t-1} - \\hat{\\phi}_2 y_{t-2} - \\hat{\\theta}_1 e_{t-1} $$\nThe estimation procedure, such as maximum likelihood, is designed to find parameter values that make the residuals $\\{e_t\\}$ behave as closely as possible to a white noise sequence. This process imposes constraints on the residuals. Essentially, the estimation process forces the sum of squared residuals to be minimized, which in turn reduces the magnitude of the sample autocorrelations of the residuals, which we denote by $\\hat{r}_k$. This effect is most pronounced for small lags.\n\nBecause the estimation of $p+q$ parameters introduces dependencies and constraints, the resulting residual autocorrelations $\\{\\hat{r}_k\\}_{k=1}^m$ are no longer asymptotically independent. The theory developed by Box and Pierce (1970) demonstrates that the estimation of $p+q$ parameters reduces the degrees of freedom of the chi-squared distribution by exactly $p+q$. Each estimated parameter can be thought of as \"explaining\" one degree of freedom in the data's correlation structure, leaving fewer degrees of freedom for the test statistic.\n\nTherefore, the asymptotic distribution of the Ljung-Box statistic when calculated on the residuals of a correctly specified ARMA($p,q$) model is not $\\chi^2(m)$, but rather a chi-squared distribution with degrees of freedom ($df$) given by:\n$$ df = m - (p+q) $$\nThis result holds provided that the number of lags, $m$, is sufficiently large compared to $p$ and $q$.\n\nFor the specific problem given:\n- The model is ARMA($2,1$), so $p=2$ and $q=1$.\n- The number of estimated parameters is $p+q = 2+1 = 3$.\n- The Ljung-Box test is performed up to lag $m=20$.\n\nSubstituting these values into the formula for degrees of freedom:\n$$ df = 20 - (2+1) = 20 - 3 = 17 $$\nThe reference distribution for the test statistic $Q_{\\mathrm{LB}}(20)$ is therefore a chi-squared distribution with $17$ degrees of freedom.\n\nThe problem also requests a decision rule for testing the null hypothesis $H_0$ that the residuals are i.i.d. (white noise) at a significance level of $\\alpha=0.05$. The alternative hypothesis $H_1$ is that the residuals are not white noise, implying serial correlation. High values of the $Q_{\\mathrm{LB}}(m)$ statistic suggest the presence of significant autocorrelation, providing evidence against $H_0$. Thus, a one-sided, upper-tail test is appropriate.\n\nThe decision rule is to reject $H_0$ if the computed test statistic $Q_{\\mathrm{LB}}(20)$ is greater than the critical value from the $\\chi^2(17)$ distribution. The critical value is the upper $(1-\\alpha)$ quantile of this distribution, denoted $\\chi^2_{1-\\alpha, df}$.\nWith $\\alpha=0.05$ and $df=17$, the decision rule is formally stated as:\nReject $H_0$ if $Q_{\\mathrm{LB}}(20) > \\chi^2_{0.95, 17}$.\nIf the calculated statistic does not exceed this value, we fail to reject the null hypothesis, and the model is considered adequate with respect to residual whiteness. The final answer is the number of degrees of freedom derived.", "answer": "$$\n\\boxed{17}\n$$", "id": "2885037"}, {"introduction": "When a portmanteau test like Ljung-Box signals that residuals are not white noise, the analyst's job shifts from detection to diagnosis. A common culprit is an under-parameterized model, which fails to capture the full dynamics of the process and leaves a remnant autoregressive (AR) structure in the residuals. This practice [@problem_id:2884970] delves into this exact scenario, showing how to use the partial autocorrelation function (PACF), computed via the elegant Durbin-Levinson recursion, to identify the order of this leftover AR component and diagnose the specific nature of the model's deficiency.", "problem": "A discrete-time, zero-mean, second-order stationary residual process $\\{e_t\\}$ arises from fitting a model to input-output data of a stable linear time-invariant system. Suppose the fitted model is under-parameterized so that the residuals retain linear dependence that can be represented by a finite-order autoregressive (AR) process of unknown order $m$. Consider the one-step linear minimum mean-square error (MMSE) predictor of $e_t$ from its $m$ most recent past values, $\\hat{e}_t = \\sum_{k=1}^{m} a_k e_{t-k}$, and denote the prediction error by $\\varepsilon_t = e_t - \\hat{e}_t$. \n\n1) Starting from the orthogonality principle for MMSE prediction, namely that $\\varepsilon_t$ is orthogonal to the past regressors $\\{e_{t-i}\\}_{i=1}^{m}$, derive the normal equations that link the autocovariance function $\\gamma(i) = \\mathbb{E}[e_t e_{t-i}]$ to the AR coefficients $\\{a_k\\}_{k=1}^{m}$. Express your result in the form of a Toeplitz linear system for $\\mathbf{a} = [a_1,\\dots,a_m]^{\\top}$ and obtain the residual innovation variance $\\sigma_{\\varepsilon}^{2} = \\mathbb{E}[\\varepsilon_t^2]$ in terms of $\\{\\gamma(k)\\}_{k=0}^{m}$ and $\\{a_k\\}_{k=1}^{m}$.\n\n2) Now use these equations for residual analysis and model validation. You are given $N = 400$ residual samples whose (biased) sample autocovariances are\n$$\\hat{\\gamma}(0) = 1.0000,\\quad \\hat{\\gamma}(1) = 0.5000,\\quad \\hat{\\gamma}(2) = 0.1000,\\quad \\hat{\\gamma}(3) = -0.0400,\\quad \\hat{\\gamma}(4) = -0.0440.$$\nCompute the sample autocorrelations $\\hat{r}(k) = \\hat{\\gamma}(k)/\\hat{\\gamma}(0)$ for $k=1,2,3,4$. Using the Durbin–Levinson recursion implied by the equations you derived, compute the first four sample partial autocorrelation coefficients (partial autocorrelation function (PACF)) $\\{\\hat{\\alpha}(k)\\}_{k=1}^{4}$.\n\n3) Under the large-sample approximation that, for an autoregressive process of true order $m$, the sampling distribution of $\\hat{\\alpha}(k)$ for $k>m$ is approximately normal with mean $0$ and standard deviation $1/\\sqrt{N}$, use a two-sided significance level $0.05$ (i.e., compare $|\\hat{\\alpha}(k)|$ to the threshold $1.96/\\sqrt{N}$) to choose the smallest order $m$ such that all lags beyond $m$ up to lag $4$ are not statistically significant. Interpret this $m$ as the estimated missing order due to underfitting.\n\nReport only that estimated missing order as a single integer. No units are required.", "solution": "Part 1: Derivation of the Normal Equations and Innovation Variance\n\nThe one-step linear minimum mean-square error (MMSE) predictor of the stationary process $\\{e_t\\}$ is given by $\\hat{e}_t = \\sum_{k=1}^{m} a_k e_{t-k}$. The prediction error is $\\varepsilon_t = e_t - \\hat{e}_t$. The orthogonality principle dictates that for an MMSE predictor, the error vector is orthogonal to the subspace of observations. This implies that the expectation of the product of the error and any of the regressors is zero:\n$$\n\\mathbb{E}[\\varepsilon_t e_{t-i}] = 0 \\quad \\text{for } i \\in \\{1, 2, \\dots, m\\}\n$$\nSubstituting the expression for $\\varepsilon_t$:\n$$\n\\mathbb{E}\\left[\\left(e_t - \\sum_{k=1}^{m} a_k e_{t-k}\\right) e_{t-i}\\right] = 0\n$$\nBy the linearity of the expectation operator:\n$$\n\\mathbb{E}[e_t e_{t-i}] - \\sum_{k=1}^{m} a_k \\mathbb{E}[e_{t-k} e_{t-i}] = 0\n$$\nLet the autocovariance function of the process be $\\gamma(j) = \\mathbb{E}[e_t e_{t-j}]$. For a stationary process, $\\mathbb{E}[e_{t-k} e_{t-i}] = \\gamma((t-i)-(t-k)) = \\gamma(k-i)$. The equations become:\n$$\n\\gamma(i) - \\sum_{k=1}^{m} a_k \\gamma(k-i) = 0 \\quad \\text{for } i \\in \\{1, 2, \\dots, m\\}\n$$\nSince $\\gamma(j) = \\gamma(-j)$ due to stationarity, we can write $\\gamma(k-i) = \\gamma(i-k)$. This yields the Yule-Walker equations:\n$$\n\\sum_{k=1}^{m} a_k \\gamma(i-k) = \\gamma(i)\n$$\nThis is a system of $m$ linear equations for the $m$ unknown coefficients $\\{a_k\\}_{k=1}^{m}$. In matrix form, this is a Toeplitz system $\\boldsymbol{\\Gamma}_m \\mathbf{a} = \\boldsymbol{\\gamma}_m$, where $\\mathbf{a} = [a_1, \\dots, a_m]^\\top$, $\\boldsymbol{\\gamma}_m = [\\gamma(1), \\dots, \\gamma(m)]^\\top$, and $\\boldsymbol{\\Gamma}_m$ is the $m \\times m$ symmetric Toeplitz autocovariance matrix with elements $(\\boldsymbol{\\Gamma}_m)_{ij} = \\gamma(i-j)$:\n$$\n\\begin{pmatrix}\n\\gamma(0) & \\gamma(1) & \\cdots & \\gamma(m-1) \\\\\n\\gamma(1) & \\gamma(0) & \\cdots & \\gamma(m-2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\gamma(m-1) & \\gamma(m-2) & \\cdots & \\gamma(0)\n\\end{pmatrix}\n\\begin{pmatrix}\na_1 \\\\\na_2 \\\\\n\\vdots \\\\\na_m\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\gamma(1) \\\\\n\\gamma(2) \\\\\n\\vdots \\\\\n\\gamma(m)\n\\end{pmatrix}\n$$\nNext, the residual innovation variance $\\sigma_{\\varepsilon}^{2} = \\mathbb{E}[\\varepsilon_t^2]$ is derived.\n$$\n\\sigma_{\\varepsilon}^{2} = \\mathbb{E}[\\varepsilon_t \\cdot \\varepsilon_t] = \\mathbb{E}\\left[\\varepsilon_t \\left(e_t - \\sum_{k=1}^{m} a_k e_{t-k}\\right)\\right] = \\mathbb{E}[\\varepsilon_t e_t] - \\sum_{k=1}^{m} a_k \\mathbb{E}[\\varepsilon_t e_{t-k}]\n$$\nFrom the orthogonality principle, the second term is zero. Thus:\n$$\n\\sigma_{\\varepsilon}^{2} = \\mathbb{E}[\\varepsilon_t e_t] = \\mathbb{E}\\left[\\left(e_t - \\sum_{k=1}^{m} a_k e_{t-k}\\right) e_t\\right] = \\mathbb{E}[e_t^2] - \\sum_{k=1}^{m} a_k \\mathbb{E}[e_{t-k} e_t]\n$$\nThis gives the final expression for the innovation variance:\n$$\n\\sigma_{\\varepsilon}^{2} = \\gamma(0) - \\sum_{k=1}^{m} a_k \\gamma(k)\n$$\n\nPart 2: Computation of Sample Autocorrelations and Partial Autocorrelations\n\nGiven the sample autocovariances: $\\hat{\\gamma}(0) = 1.0000$, $\\hat{\\gamma}(1) = 0.5000$, $\\hat{\\gamma}(2) = 0.1000$, $\\hat{\\gamma}(3) = -0.0400$, $\\hat{\\gamma}(4) = -0.0440$.\nThe sample autocorrelations $\\hat{r}(k) = \\hat{\\gamma}(k)/\\hat{\\gamma}(0)$ are:\n$$\n\\hat{r}(1) = \\frac{0.5000}{1.0000} = 0.5, \\quad \\hat{r}(2) = \\frac{0.1000}{1.0000} = 0.1, \\quad \\hat{r}(3) = \\frac{-0.0400}{1.0000} = -0.04, \\quad \\hat{r}(4) = \\frac{-0.0440}{1.0000} = -0.044\n$$\nThe sample partial autocorrelation function (PACF), $\\{\\hat{\\alpha}(k)\\}$, is computed using the Durbin-Levinson recursion. The coefficient $\\hat{\\alpha}(p)$ is the last AR coefficient, $\\hat{a}_{p,p}$, in a model of order $p$.\nInitialize with prediction error variance $\\hat{v}_0 = \\hat{\\gamma}(0) = 1.0$.\n\nFor order $p=1$:\n$$\n\\hat{\\alpha}(1) = \\hat{a}_{1,1} = \\hat{r}(1) = 0.5\n$$\n$$\n\\hat{v}_1 = \\hat{v}_0(1 - \\hat{\\alpha}(1)^2) = 1.0(1 - 0.5^2) = 0.75\n$$\n\nFor order $p=2$:\n$$\n\\hat{\\alpha}(2) = \\hat{a}_{2,2} = \\frac{\\hat{\\gamma}(2) - \\hat{a}_{1,1} \\hat{\\gamma}(1)}{\\hat{v}_1} = \\frac{0.1 - (0.5)(0.5)}{0.75} = \\frac{-0.15}{0.75} = -0.2\n$$\nThe AR(2) coefficients are $\\hat{a}_{2,1} = \\hat{a}_{1,1} - \\hat{\\alpha}(2)\\hat{a}_{1,1} = 0.5 - (-0.2)(0.5) = 0.6$ and $\\hat{a}_{2,2} = -0.2$.\nThe variance is $\\hat{v}_2 = \\hat{v}_1(1 - \\hat{\\alpha}(2)^2) = 0.75(1 - (-0.2)^2) = 0.75(0.96) = 0.72$.\n\nFor order $p=3$:\nThe coefficients for the AR(2) model are $\\{\\hat{a}_{2,1}, \\hat{a}_{2,2}\\} = \\{0.6, -0.2\\}$.\n$$\n\\hat{\\alpha}(3) = \\hat{a}_{3,3} = \\frac{\\hat{\\gamma}(3) - (\\hat{a}_{2,1}\\hat{\\gamma}(2) + \\hat{a}_{2,2}\\hat{\\gamma}(1))}{\\hat{v}_2} = \\frac{-0.04 - (0.6(0.1) + (-0.2)(0.5))}{0.72} = \\frac{-0.04 - (0.06 - 0.1)}{0.72} = \\frac{0}{0.72} = 0\n$$\n\nFor order $p=4$:\nSince $\\hat{\\alpha}(3)=0$, the AR(3) coefficients are $\\{\\hat{a}_{3,1}, \\hat{a}_{3,2}, \\hat{a}_{3,3}\\} = \\{0.6, -0.2, 0\\}$ and $\\hat{v}_3 = \\hat{v}_2(1 - 0^2) = 0.72$.\n$$\n\\hat{\\alpha}(4) = \\hat{a}_{4,4} = \\frac{\\hat{\\gamma}(4) - (\\hat{a}_{3,1}\\hat{\\gamma}(3) + \\hat{a}_{3,2}\\hat{\\gamma}(2) + \\hat{a}_{3,3}\\hat{\\gamma}(1))}{\\hat{v}_3} = \\frac{-0.044 - (0.6(-0.04) + (-0.2)(0.1) + 0(0.5))}{0.72} = \\frac{-0.044 - (-0.024 - 0.02)}{0.72} = \\frac{0}{0.72} = 0\n$$\nThe computed sample PACF values up to lag $4$ are $\\{\\hat{\\alpha}(1), \\hat{\\alpha}(2), \\hat{\\alpha}(3), \\hat{\\alpha}(4)\\} = \\{0.5, -0.2, 0, 0\\}$.\n\nPart 3: Model Order Selection\n\nWe test the null hypothesis $H_0: \\alpha(k) = 0$ for lags $k=1, 2, 3, 4$. For an AR process of true order $m$, the sample PACF coefficient $\\hat{\\alpha}(k)$ for $k>m$ follows an approximate normal distribution with mean $0$ and variance $1/N$. With $N=400$, the standard deviation is $1/\\sqrt{400} = 0.05$.\nFor a significance level of $0.05$, the critical value is $z_{0.025} \\approx 1.96$. The threshold for statistical significance is $1.96/\\sqrt{N} = 1.96 \\times 0.05 = 0.098$.\nWe test the absolute value of each sample PACF coefficient against this threshold:\n- For $k=1$: $|\\hat{\\alpha}(1)| = |0.5| = 0.5$. Since $0.5 > 0.098$, the coefficient is statistically significant. The order $m$ is at least $1$.\n- For $k=2$: $|\\hat{\\alpha}(2)| = |-0.2| = 0.2$. Since $0.2 > 0.098$, the coefficient is statistically significant. The order $m$ is at least $2$.\n- For $k=3$: $|\\hat{\\alpha}(3)| = |0| = 0$. Since $0 < 0.098$, the coefficient is not statistically significant.\n- For $k=4$: $|\\hat{\\alpha}(4)| = |0| = 0$. Since $0 < 0.098$, the coefficient is not statistically significant.\n\nThe PACF is said to \"cut off\" after the last significant lag. In this case, the last significant lag is $k=2$. All subsequent lags tested (up to $4$) are statistically insignificant. This behavior is characteristic of an autoregressive process of order $2$. Therefore, the estimated missing order due to underfitting is $m=2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "2884970"}, {"introduction": "Just as under-fitting can invalidate a model, so too can over-zealous processing, a common example of which is over-differencing in ARIMA modeling. This error also leaves a distinct, non-random pattern in the residuals, but one that is characteristically different from that of under-fitting. This hands-on exercise [@problem_id:2885077] will demonstrate how applying one too many differencing operators induces a specific moving-average (MA) structure in the residuals, and you will derive its signature in both the time domain (via the autocorrelation function) and the frequency domain (via the power spectral density).", "problem": "An engineer is validating a time-series model for a scalar discrete-time process. The true data-generating mechanism is an Autoregressive Integrated Moving Average (ARIMA) process of order $(0,1,0)$, defined by the fundamental difference equation $(1 - B) x_t = w_t$, where $B$ is the backshift operator and $w_t$ is a zero-mean white-noise process with variance $\\sigma^{2}$. Unaware of the true differencing order, the engineer fits an ARIMA$(0,2,0)$ model without a deterministic constant and computes residuals $\\hat{e}_t$ equal to the twice-differenced series $\\hat{e}_t = (1 - B)^{2} x_t$.\n\nStarting from the fundamental definitions of the differencing operator, white noise, linear time-invariant filtering, and the Wiener–Khinchin relation between autocovariance and power spectral density, derive the expected residual process implied by this over-differenced specification. In particular:\n- Express $\\hat{e}_t$ in terms of $w_t$ and identify the corresponding linear filter.\n- Derive the autocovariance function $\\gamma_{\\hat{e}}(k)$ and the power spectral density $S_{\\hat{e}}(\\omega)$ of the residuals $\\hat{e}_t$ in closed form, in terms of $\\sigma^{2}$.\n- Explain the expected residual pattern in both the time and frequency domains, and propose rigorous diagnostics that would detect over-differencing, including how the residual autocorrelation at lag $1$ and the behavior of the spectrum near zero frequency would be expected to appear under over-differencing.\n\nProvide as your final answer the closed-form expression for the residual power spectral density $S_{\\hat{e}}(\\omega)$ as a function of $\\omega \\in [-\\pi,\\pi]$ and $\\sigma^{2}$. No rounding is required, and you should not include any units in your final answer.", "solution": "First, we express the residual process $\\hat{e}_t$ in terms of the underlying white-noise process $w_t$.\nThe definition of the residual is given as:\n$$\n\\hat{e}_t = (1 - B)^{2} x_t\n$$\nWe can rewrite this expression by factoring the operator:\n$$\n\\hat{e}_t = (1 - B) \\left[ (1 - B) x_t \\right]\n$$\nThe true data-generating process is given by the equation $(1 - B) x_t = w_t$. Substituting this into the expression for $\\hat{e}_t$, we obtain:\n$$\n\\hat{e}_t = (1 - B) w_t\n$$\nExpanding this expression gives the explicit form of the residual process:\n$$\n\\hat{e}_t = w_t - w_{t-1}\n$$\nThis demonstrates that the residual process $\\hat{e}_t$ is not white noise. Instead, it is a Moving Average process of order $1$, or MA($1$). The linear time-invariant filter that transforms the input white noise $w_t$ into the output residual $\\hat{e}_t$ is represented by the polynomial in the backshift operator $H(B) = 1 - B$.\n\nSecond, we derive the autocovariance function $\\gamma_{\\hat{e}}(k)$ for the residual process $\\hat{e}_t$. The autocovariance at lag $k$ is defined as $\\gamma_{\\hat{e}}(k) = E[\\hat{e}_t \\hat{e}_{t-k}]$. We use the expression $\\hat{e}_t = w_t - w_{t-1}$.\n\nFor lag $k=0$, we compute the variance:\n$$\n\\gamma_{\\hat{e}}(0) = E[\\hat{e}_t^2] = E[(w_t - w_{t-1})^2] = E[w_t^2 - 2w_t w_{t-1} + w_{t-1}^2]\n$$\nBy linearity of expectation and the properties of white noise ($E[w_t^2] = \\sigma^2$ and $E[w_t w_s] = 0$ for $t \\neq s$):\n$$\n\\gamma_{\\hat{e}}(0) = E[w_t^2] - 2E[w_t w_{t-1}] + E[w_{t-1}^2] = \\sigma^2 - 2(0) + \\sigma^2 = 2\\sigma^2\n$$\n\nFor lag $k=1$:\n$$\n\\gamma_{\\hat{e}}(1) = E[\\hat{e}_t \\hat{e}_{t-1}] = E[(w_t - w_{t-1})(w_{t-1} - w_{t-2})] = E[w_t w_{t-1} - w_t w_{t-2} - w_{t-1}^2 + w_{t-1} w_{t-2}]\n$$\nAgain, applying the properties of white noise:\n$$\n\\gamma_{\\hat{e}}(1) = E[w_t w_{t-1}] - E[w_t w_{t-2}] - E[w_{t-1}^2] + E[w_{t-1} w_{t-2}] = 0 - 0 - \\sigma^2 + 0 = -\\sigma^2\n$$\nDue to the symmetry of the autocovariance function, $\\gamma_{\\hat{e}}(-1) = \\gamma_{\\hat{e}}(1) = -\\sigma^2$.\n\nFor lags $|k| \\geq 2$:\n$$\n\\gamma_{\\hat{e}}(k) = E[\\hat{e}_t \\hat{e}_{t-k}] = E[(w_t - w_{t-1})(w_{t-k} - w_{t-k-1})]\n$$\nFor $|k| \\geq 2$, the indices of the white-noise terms in the first factor ($t$, $t-1$) are distinct from the indices in the second factor ($t-k$, $t-k-1$). Therefore, the expectation of any cross-product is zero.\n$$\n\\gamma_{\\hat{e}}(k) = 0 \\quad \\text{for } |k| \\geq 2\n$$\nIn summary, the autocovariance function of the residual process is:\n$$\n\\gamma_{\\hat{e}}(k) =\n\\begin{cases}\n2\\sigma^2 & k=0 \\\\\n-\\sigma^2 & |k|=1 \\\\\n0 & |k| \\ge 2\n\\end{cases}\n$$\n\nThird, we derive the power spectral density (PSD) $S_{\\hat{e}}(\\omega)$ of the residuals. We use the property that for a process $y_t = H(B)w_t$, where $w_t$ is white noise with PSD $S_w(\\omega)=\\sigma^2$, the PSD of $y_t$ is $S_y(\\omega) = |H(e^{-i\\omega})|^2 S_w(\\omega)$.\nHere, $\\hat{e}_t = (1-B)w_t$, so the filter is $H(B) = 1 - B$. The frequency response is found by substituting $B = e^{-i\\omega}$:\n$$\nH(e^{-i\\omega}) = 1 - e^{-i\\omega}\n$$\nThe squared magnitude of the frequency response is:\n$$\n|H(e^{-i\\omega})|^2 = |1 - e^{-i\\omega}|^2 = (1 - \\cos(\\omega))^2 + (\\sin(\\omega))^2 = 1 - 2\\cos(\\omega) + \\cos^2(\\omega) + \\sin^2(\\omega)\n$$\nUsing the identity $\\cos^2(\\omega) + \\sin^2(\\omega) = 1$:\n$$\n|H(e^{-i\\omega})|^2 = 1 - 2\\cos(\\omega) + 1 = 2 - 2\\cos(\\omega) = 2(1 - \\cos(\\omega))\n$$\nThe PSD of the input white noise $w_t$ is constant, $S_w(\\omega) = \\sigma^2$. Therefore, the PSD of the residual process $\\hat{e}_t$ is:\n$$\nS_{\\hat{e}}(\\omega) = |H(e^{-i\\omega})|^2 S_w(\\omega) = 2\\sigma^2(1 - \\cos(\\omega))\n$$\nThis expression is valid for the frequency interval $\\omega \\in [-\\pi, \\pi]$.\n\nFinally, we explain the diagnostic implications. A correctly specified time-series model should produce residuals that are indistinguishable from white noise.\n- **Time-Domain Diagnostics**: The autocovariance function of white noise is zero for all non-zero lags. The derived autocovariance for $\\hat{e}_t$ is non-zero at lag $k=1$. The theoretical autocorrelation function (ACF) of the residuals is:\n$$\n\\rho_{\\hat{e}}(k) = \\frac{\\gamma_{\\hat{e}}(k)}{\\gamma_{\\hat{e}}(0)}\n$$\nSpecifically, for lag $1$:\n$$\n\\rho_{\\hat{e}}(1) = \\frac{-\\sigma^2}{2\\sigma^2} = -0.5\n$$\nFor a model diagnostic, the engineer would inspect the sample ACF of the residuals $\\hat{e}_t$. In this case of over-differencing, the ACF plot will exhibit a single, large, statistically significant negative spike at lag $1$, with a value near $-0.5$. The ACF for all lags greater than $1$ will be insignificant. This is the characteristic signature of superfluous differencing.\n\n- **Frequency-Domain Diagnostics**: The power spectral density of white noise is constant (flat) for all frequencies, $S(\\omega) = \\sigma^2$. The derived PSD for $\\hat{e}_t$ is $S_{\\hat{e}}(\\omega) = 2\\sigma^2(1 - \\cos(\\omega))$. This function is not flat. Crucially, let us evaluate the spectrum at zero frequency:\n$$\nS_{\\hat{e}}(0) = 2\\sigma^2(1 - \\cos(0)) = 2\\sigma^2(1 - 1) = 0\n$$\nThe differencing operator $(1-B)$ acts as a high-pass filter, attenuating low frequencies. Applying it twice results in excessive attenuation of power near zero frequency, creating a \"spectral hole\" or dip at $\\omega=0$. For model diagnostics, an engineer would compute the periodogram or a smoothed spectral estimate of the residuals. A spectrum that shows a significant dip toward zero at the origin is a clear indication of over-differencing.", "answer": "$$\n\\boxed{2\\sigma^{2}(1 - \\cos(\\omega))}\n$$", "id": "2885077"}]}