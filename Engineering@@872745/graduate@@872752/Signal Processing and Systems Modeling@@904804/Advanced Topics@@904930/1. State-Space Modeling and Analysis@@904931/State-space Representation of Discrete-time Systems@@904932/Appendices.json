{"hands_on_practices": [{"introduction": "The most fundamental property of a system is its stability, which dictates its long-term behavior. This exercise explores marginal stability, a critical concept for systems that exhibit sustained oscillations without growing unstable. By analyzing a canonical system—a 2D rotation—you will connect the eigenvalues of the state matrix $A$ directly to the system's time-domain response, reinforcing the foundational definitions of stability from first principles [@problem_id:2908001].", "problem": "Consider the autonomous discrete-time linear time-invariant (LTI) state-space system\n$$\nx[k+1] = A\\,x[k],\\qquad y[k] = C\\,x[k],\n$$\nwhere $x[k] \\in \\mathbb{R}^{2}$, $y[k] \\in \\mathbb{R}$, and\n$$\nA = \\begin{bmatrix}\n\\cos(\\theta)  -\\sin(\\theta)\\\\\n\\sin(\\theta)  \\cos(\\theta)\n\\end{bmatrix},\\qquad C = \\begin{bmatrix}1  0\\end{bmatrix},\\qquad \\theta = \\frac{\\pi}{3},\n$$\nwith initial condition $x[0] = \\begin{bmatrix}2\\\\0\\end{bmatrix}$. Angles must be interpreted in radians.\n\na) Provide a precise definition of marginal stability for such an autonomous discrete-time LTI system in terms of the boundedness of state trajectories and asymptotic behavior, grounded in fundamental definitions of stability and the behavior of matrix powers.\n\nb) Starting only from core definitions for discrete-time systems (that is, repeated substitution of the state equation and the algebraic properties of plane rotations), determine whether the given system is marginally stable and derive a closed-form analytic expression for $y[k]$ as a function of $k \\in \\mathbb{Z}_{\\ge 0}$. Express your final answer for $y[k]$ as a single closed-form expression in $k$ with angles in radians. No numerical rounding is required, and no transform-domain shortcuts are permitted.", "solution": "The problem as stated is subjected to validation.\n\nStep 1: Extract Givens.\nThe autonomous discrete-time linear time-invariant system is described by the equations:\n$x[k+1] = A\\,x[k]$\n$y[k] = C\\,x[k]$\nThe state vector is $x[k] \\in \\mathbb{R}^{2}$, and the output is $y[k] \\in \\mathbb{R}$.\nThe system matrices are:\n$A = \\begin{bmatrix} \\cos(\\theta)  -\\sin(\\theta)\\\\ \\sin(\\theta)  \\cos(\\theta) \\end{bmatrix}$\n$C = \\begin{bmatrix}1  0\\end{bmatrix}$\nThe parameter $\\theta$ is given as $\\theta = \\frac{\\pi}{3}$.\nThe initial condition is $x[0] = \\begin{bmatrix}2\\\\0\\end{bmatrix}$.\nThe time index $k$ belongs to the set of non-negative integers, $k \\in \\mathbb{Z}_{\\ge 0}$.\nThe problem requires:\na) A precise definition of marginal stability for such a system.\nb) A determination of the system's stability and the derivation of a closed-form expression for $y[k]$ using only repeated substitution and algebraic properties.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, situated squarely within the fundamental theory of linear discrete-time systems. It is a canonical example of analyzing system dynamics. The problem is well-posed, completely specified, and contains no contradictions. All data ($A$, $C$, $x[0]$) are provided and are dimensionally consistent. The language is objective and unambiguous. The constraints on the solution method (no transform-domain shortcuts) are clear and serve to test foundational understanding.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A solution will be provided.\n\nThe response is structured to address parts (a) and (b) in sequence.\n\na) Definition of Marginal Stability\nFor an autonomous discrete-time LTI system described by $x[k+1] = A\\,x[k]$, stability concerns the behavior of the state trajectory $x[k]$ as $k \\to \\infty$. The system's stability is completely determined by the eigenvalues of the state matrix $A$. Let the eigenvalues of $A$ be $\\lambda_i$.\n\nA system is defined as **marginally stable** if, for any bounded initial condition $x[0]$, the state trajectory $x[k]$ remains bounded for all $k \\ge 0$, but the system is not asymptotically stable.\nThis behavior corresponds to specific conditions on the eigenvalues of the matrix $A$:\n$1$. All eigenvalues must have a magnitude less than or equal to one: $|\\lambda_i| \\le 1$ for all $i$.\n$2$. At least one eigenvalue must have a magnitude exactly equal to one: $|\\lambda_j| = 1$ for some $j$.\n$3$. Any eigenvalue $\\lambda_m$ with magnitude $|\\lambda_m| = 1$ must be a simple root of the minimal polynomial of $A$. This condition is equivalent to stating that the geometric multiplicity of such an eigenvalue must equal its algebraic multiplicity. If this condition is not met (i.e., if there is a Jordan block of size greater than $1$ associated with an eigenvalue on the unit circle), the state trajectory will contain terms of the form $k^p |\\lambda_m|^k = k^p$, which are unbounded, and the system would be unstable.\n\nIn summary, a marginally stable system's state does not decay to the origin for all initial conditions (unlike an asymptotically stable system), but it also does not grow without bound (unlike an unstable system). It typically exhibits sustained oscillations or constant-magnitude behavior.\n\nb) Stability Analysis and Derivation of $y[k]$\nFirst, we analyze the stability of the given system. The state matrix is $A = \\begin{bmatrix} \\cos(\\theta)  -\\sin(\\theta)\\\\ \\sin(\\theta)  \\cos(\\theta) \\end{bmatrix}$ with $\\theta = \\frac{\\pi}{3}$.\nTo determine stability, we find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} \\cos(\\theta) - \\lambda  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) - \\lambda \\end{pmatrix} = (\\cos(\\theta) - \\lambda)^2 + \\sin^2(\\theta) = 0\n$$\n$$\n\\lambda^2 - 2\\lambda\\cos(\\theta) + \\cos^2(\\theta) + \\sin^2(\\theta) = 0\n$$\n$$\n\\lambda^2 - 2\\lambda\\cos(\\theta) + 1 = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{2\\cos(\\theta) \\pm \\sqrt{4\\cos^2(\\theta) - 4}}{2} = \\cos(\\theta) \\pm \\sqrt{\\cos^2(\\theta) - 1} = \\cos(\\theta) \\pm \\sqrt{- \\sin^2(\\theta)}\n$$\n$$\n\\lambda = \\cos(\\theta) \\pm i\\sin(\\theta)\n$$\nThese are the complex conjugates $e^{i\\theta}$ and $e^{-i\\theta}$.\nFor $\\theta = \\frac{\\pi}{3}$, the eigenvalues are $\\lambda_1 = e^{i\\pi/3}$ and $\\lambda_2 = e^{-i\\pi/3}$.\nThe magnitudes of these eigenvalues are $|\\lambda_1| = |e^{i\\pi/3}| = 1$ and $|\\lambda_2| = |e^{-i\\pi/3}| = 1$.\nSince $\\theta = \\frac{\\pi}{3} \\ne n\\pi$ for an integer $n$, the eigenvalues are distinct.\nThe conditions for marginal stability are met: all eigenvalues have magnitude equal to $1$, and they are distinct (hence, they are simple roots of the minimal polynomial). Therefore, the system is **marginally stable**.\n\nNext, we derive the closed-form expression for $y[k]$. The problem requires using repeated substitution. The solution to the state equation $x[k+1] = A x[k]$ is found by recursion:\n$x[1] = A x[0]$\n$x[2] = A x[1] = A(A x[0]) = A^2 x[0]$\n...\n$x[k] = A^k x[0]$\nThe matrix $A$ is a rotation matrix $R(\\theta)$. The product of two rotation matrices $R(\\theta_1)R(\\theta_2)$ is $R(\\theta_1 + \\theta_2)$. By induction, one can rigorously prove that $(R(\\theta))^k = R(k\\theta)$.\nLet us formally demonstrate this.\nBase case ($k=1$): $A^1 = A = R(\\theta) = R(1 \\cdot \\theta)$. The property holds.\nInductive step: Assume $A^n = R(n\\theta)$ for some integer $n \\ge 1$.\nThen $A^{n+1} = A^n A = R(n\\theta) R(\\theta)$.\n$$\nA^{n+1} = \\begin{pmatrix} \\cos(n\\theta)  -\\sin(n\\theta) \\\\ \\sin(n\\theta)  \\cos(n\\theta) \\end{pmatrix} \\begin{pmatrix} \\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} \\cos(n\\theta)\\cos(\\theta) - \\sin(n\\theta)\\sin(\\theta)  -\\cos(n\\theta)\\sin(\\theta) - \\sin(n\\theta)\\cos(\\theta) \\\\ \\sin(n\\theta)\\cos(\\theta) + \\cos(n\\theta)\\sin(\\theta)  -\\sin(n\\theta)\\sin(\\theta) + \\cos(n\\theta)\\cos(\\theta) \\end{pmatrix}\n$$\nUsing the trigonometric angle addition identities:\n$$\nA^{n+1} = \\begin{pmatrix} \\cos(n\\theta+\\theta)  -\\sin(n\\theta+\\theta) \\\\ \\sin(n\\theta+\\theta)  \\cos(n\\theta+\\theta) \\end{pmatrix} = \\begin{pmatrix} \\cos((n+1)\\theta)  -\\sin((n+1)\\theta) \\\\ \\sin((n+1)\\theta)  \\cos((n+1)\\theta) \\end{pmatrix} = R((n+1)\\theta)\n$$\nThe induction is complete. Thus, for any $k \\in \\mathbb{Z}_{\\ge 0}$, we have:\n$$\nA^k = \\begin{bmatrix} \\cos(k\\theta)  -\\sin(k\\theta) \\\\ \\sin(k\\theta)  \\cos(k\\theta) \\end{bmatrix}\n$$\nNow we compute the state vector $x[k]$ with the given initial condition $x[0] = \\begin{bmatrix}2\\\\0\\end{bmatrix}$:\n$$\nx[k] = A^k x[0] = \\begin{bmatrix} \\cos(k\\theta)  -\\sin(k\\theta) \\\\ \\sin(k\\theta)  \\cos(k\\theta) \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2\\cos(k\\theta) \\\\ 2\\sin(k\\theta) \\end{bmatrix}\n$$\nFinally, we compute the output $y[k]$ using the output equation $y[k] = C x[k]$:\n$$\ny[k] = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} 2\\cos(k\\theta) \\\\ 2\\sin(k\\theta) \\end{bmatrix} = 1 \\cdot (2\\cos(k\\theta)) + 0 \\cdot (2\\sin(k\\theta)) = 2\\cos(k\\theta)\n$$\nSubstituting the value $\\theta = \\frac{\\pi}{3}$, we obtain the final closed-form expression for the output:\n$$\ny[k] = 2\\cos\\left(\\frac{k\\pi}{3}\\right)\n$$\nThis expression describes a discrete-time sinusoid with amplitude $2$ and discrete frequency $\\frac{\\pi}{3}$ radians per sample, consistent with the behavior of a marginally stable system representing pure rotation.", "answer": "$$\n\\boxed{2\\cos\\left(\\frac{k\\pi}{3}\\right)}\n$$", "id": "2908001"}, {"introduction": "The transfer function $G(z)$, which describes a system's input-output relationship, is fundamentally determined by the state matrix $A$ through the resolvent matrix $(zI-A)^{-1}$. This practice guides you through a derivation of the spectral representation of the resolvent for diagonalizable systems. By performing a partial fraction expansion, you will uncover a deep connection between the resolvent, the system's eigenvalues, and a set of matrices called eigenprojectors, providing a direct analytical formula for the system's response characteristics [@problem_id:2908035].", "problem": "Consider a discrete-time Linear Time-Invariant (LTI) state-space system with dynamics $x[k+1] = A x[k] + B u[k]$ and $y[k] = C x[k] + D u[k]$, where $A \\in \\mathbb{C}^{n \\times n}$ is diagonalizable over $\\mathbb{C}$. Let the set of distinct eigenvalues of $A$ be $\\{\\lambda_{1},\\ldots,\\lambda_{r}\\}$ with $1 \\le r \\le n$, and let $I$ denote the identity matrix of appropriate size. The transfer function under the $Z$-transform (ZT) is $G(z) = C\\,(zI - A)^{-1} B + D$, so the analytic structure of the resolvent $(zI - A)^{-1}$ controls the pole structure of $G(z)$.\n\nStarting from first principles in linear algebra and system theory, use only the following foundational facts:\n- The definition of the resolvent $(zI - A)^{-1}$ for $z \\in \\mathbb{C}$ with $z \\notin \\{\\lambda_{1},\\ldots,\\lambda_{r}\\}$.\n- The spectral decomposition for diagonalizable matrices, that is, the existence of an invertible matrix $V$ and a diagonal matrix $\\Lambda$ with $A = V \\Lambda V^{-1}$, where the diagonal of $\\Lambda$ lists the eigenvalues of $A$ (repeated according to their algebraic multiplicities).\n- Polynomial functional calculus for matrices: if $p$ is a polynomial and $A$ is a matrix, then $p(A)$ is defined, and if $A = V \\Lambda V^{-1}$, then $p(A) = V p(\\Lambda) V^{-1}$ where $p(\\Lambda)$ is obtained by applying $p$ entrywise to the diagonal of $\\Lambda$.\n- The Lagrange interpolation construction of polynomials $p_{i}(\\xi)$ satisfying $p_{i}(\\lambda_{j}) = \\delta_{ij}$ for $i,j \\in \\{1,\\ldots,r\\}$.\n\nDerive the partial fraction expansion of $(zI - A)^{-1}$ as a sum over its simple poles at the distinct eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{r}$ and prove that the residues at these poles are the eigenprojectors onto the corresponding eigenspaces. Then, express each eigenprojector as a polynomial in $A$ using only $\\{\\lambda_{i}\\}_{i=1}^{r}$ and $I$ (that is, without reference to any eigenvector basis or modal matrix).\n\nWhat is the resulting closed-form analytic expression for $(zI - A)^{-1}$ written entirely in terms of $z$, $A$, $I$, and the distinct eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{r}$? Your final answer must be a single analytic expression. No numerical approximation or rounding is required.", "solution": "The problem is well-posed and scientifically grounded within the established principles of linear algebra and system theory. All provided information is self-contained and consistent. We shall proceed with the derivation.\n\nThe objective is to derive the partial fraction expansion of the resolvent matrix $(zI - A)^{-1}$ for a diagonalizable matrix $A$, and to express the resulting residues, which are the eigenprojectors, as polynomials in $A$.\n\nThe derivation is structured in three parts. First, we use the spectral decomposition of $A$ to obtain the partial fraction form of the resolvent. Second, we use polynomial functional calculus and Lagrange interpolation to find a polynomial representation of the eigenprojectors. Third, we combine these results to form the final expression.\n\nLet $A \\in \\mathbb{C}^{n \\times n}$ be a diagonalizable matrix. By the spectral theorem, there exists an invertible matrix $V \\in \\mathbb{C}^{n \\times n}$ and a diagonal matrix $\\Lambda \\in \\mathbb{C}^{n \\times n}$ such that $A = V \\Lambda V^{-1}$. The diagonal entries of $\\Lambda$, denoted $\\mu_1, \\mu_2, \\ldots, \\mu_n$, are the eigenvalues of $A$, listed according to their algebraic multiplicities. The columns of $V$ are the corresponding eigenvectors. The set of distinct eigenvalues is given as $\\{\\lambda_1, \\ldots, \\lambda_r\\}$ where $1 \\le r \\le n$.\n\nWe begin by substituting the spectral decomposition of $A$ into the resolvent expression:\n$$ (zI - A)^{-1} = (zVIV^{-1} - V \\Lambda V^{-1})^{-1} = (V(zI - \\Lambda)V^{-1})^{-1} $$\nUsing the property $(XYZ)^{-1} = Z^{-1}Y^{-1}X^{-1}$, we obtain:\n$$ (zI - A)^{-1} = V (zI - \\Lambda)^{-1} V^{-1} $$\nThe matrix $(zI - \\Lambda)$ is a diagonal matrix with diagonal entries $(z - \\mu_k)$ for $k=1, \\ldots, n$. Its inverse, $(zI - \\Lambda)^{-1}$, is also a diagonal matrix, with diagonal entries $\\frac{1}{z - \\mu_k}$, provided $z$ is not an eigenvalue of $A$.\n\nWe can express the diagonal matrix $(zI - \\Lambda)^{-1}$ as a sum. Let $E_i$ be a diagonal matrix with $1$s on the diagonal at positions corresponding to the eigenvalue $\\lambda_i$, and $0$s elsewhere. These matrices are orthogonal projectors in the sense that $E_i E_j = \\delta_{ij} E_i$, and they form a resolution of the identity, $\\sum_{i=1}^{r} E_i = I$. With these matrices, we can write:\n$$ (zI - \\Lambda)^{-1} = \\sum_{i=1}^{r} \\frac{1}{z - \\lambda_i} E_i $$\nSubstituting this back into the expression for the resolvent of $A$:\n$$ (zI - A)^{-1} = V \\left( \\sum_{i=1}^{r} \\frac{1}{z - \\lambda_i} E_i \\right) V^{-1} = \\sum_{i=1}^{r} \\frac{1}{z - \\lambda_i} (V E_i V^{-1}) $$\nLet us define the matrices $P_i = V E_i V^{-1}$. The expression for the resolvent becomes a partial fraction expansion with respect to the distinct eigenvalues:\n$$ (zI - A)^{-1} = \\sum_{i=1}^{r} \\frac{P_i}{z - \\lambda_i} $$\nThe residue of $(zI - A)^{-1}$ at the simple pole $z = \\lambda_i$ is $\\lim_{z \\to \\lambda_i} (z - \\lambda_i)(zI - A)^{-1} = P_i$. We now demonstrate that these matrices $P_i$ are the eigenprojectors onto the eigenspaces of $A$.\nFirst, they are projectors:\n$$ P_i P_j = (V E_i V^{-1})(V E_j V^{-1}) = V E_i (V^{-1}V) E_j V^{-1} = V (E_i E_j) V^{-1} = V (\\delta_{ij} E_i) V^{-1} = \\delta_{ij} P_i $$\nSecond, they sum to the identity matrix:\n$$ \\sum_{i=1}^{r} P_i = \\sum_{i=1}^{r} V E_i V^{-1} = V \\left( \\sum_{i=1}^{r} E_i \\right) V^{-1} = V I V^{-1} = I $$\nThird, they project onto the eigenspace corresponding to $\\lambda_i$. We verify this by computing $A P_i$:\n$$ A P_i = (V \\Lambda V^{-1})(V E_i V^{-1}) = V (\\Lambda E_i) V^{-1} $$\nThe product $\\Lambda E_i$ results in a diagonal matrix where the only non-zero entries are those of $\\Lambda$ at positions where $E_i$ has a $1$. By definition of $E_i$, these are the positions corresponding to the eigenvalue $\\lambda_i$. Thus, $\\Lambda E_i = \\lambda_i E_i$.\n$$ A P_i = V (\\lambda_i E_i) V^{-1} = \\lambda_i (V E_i V^{-1}) = \\lambda_i P_i $$\nThis shows that the range of $P_i$ is a subspace of the eigenspace of $A$ for eigenvalue $\\lambda_i$. Because $\\sum P_i = I$, the $P_i$ form a complete set of projectors, and thus the range of $P_i$ is precisely the eigenspace for $\\lambda_i$. This completes the first part of the proof.\n\nNext, we express each eigenprojector $P_i$ as a polynomial in $A$. We seek a set of polynomials $p_i(\\xi)$ such that $P_i = p_i(A)$ for each $i=1, \\ldots, r$. Using the given fact about polynomial functional calculus for diagonalizable matrices, we have $p_i(A) = V p_i(\\Lambda) V^{-1}$. For this to be equal to $P_i = V E_i V^{-1}$, we must satisfy the condition $p_i(\\Lambda) = E_i$.\nThe matrix $p_i(\\Lambda)$ is a diagonal matrix with entries $p_i(\\mu_k)$ on its diagonal. The matrix $E_i$ is a diagonal matrix with a $1$ on its diagonal if the corresponding eigenvalue $\\mu_k$ is equal to $\\lambda_i$, and a $0$ otherwise. This means the polynomial $p_i(\\xi)$ must satisfy the conditions:\n$$ p_i(\\mu_k) = \\begin{cases} 1  \\text{if } \\mu_k = \\lambda_i \\\\ 0  \\text{if } \\mu_k \\neq \\lambda_i \\end{cases} $$\nThese conditions can be summarized over the set of distinct eigenvalues as $p_i(\\lambda_j) = \\delta_{ij}$ for $j=1, \\ldots, r$. This is the defining property of the Lagrange basis polynomials for interpolation at the points $\\{\\lambda_1, \\ldots, \\lambda_r\\}$. The unique polynomial of degree $r-1$ satisfying these conditions is given by the Lagrange interpolation formula:\n$$ p_i(\\xi) = \\prod_{j=1, j \\neq i}^{r} \\frac{\\xi - \\lambda_j}{\\lambda_i - \\lambda_j} $$\nApplying the functional calculus, we substitute the matrix $A$ for the variable $\\xi$:\n$$ P_i = p_i(A) = \\prod_{j=1, j \\neq i}^{r} \\frac{A - \\lambda_j I}{\\lambda_i - \\lambda_j} $$\nThis expression represents the eigenprojector $P_i$ as a polynomial in $A$ of degree $r-1$, using only $A$, $I$, and the distinct eigenvalues $\\{\\lambda_i\\}$, as required.\n\nFinally, we substitute this polynomial representation of $P_i$ into the partial fraction expansion of the resolvent:\n$$ (zI - A)^{-1} = \\sum_{i=1}^{r} \\frac{1}{z - \\lambda_i} P_i = \\sum_{i=1}^{r} \\frac{1}{z - \\lambda_i} \\left( \\prod_{j=1, j \\neq i}^{r} \\frac{A - \\lambda_j I}{\\lambda_i - \\lambda_j} \\right) $$\nThis is the sought-after closed-form expression for the resolvent, known as the Sylvester formula or spectral representation of the resolvent. It is expressed solely in terms of $z$, $A$, $I$, and the distinct eigenvalues of $A$.", "answer": "$$\n\\boxed{\\sum_{i=1}^{r} \\frac{1}{z - \\lambda_i} \\prod_{j=1, j \\neq i}^{r} \\frac{A - \\lambda_j I}{\\lambda_i - \\lambda_j}}\n$$", "id": "2908035"}, {"introduction": "A single transfer function can be represented by infinitely many state-space models, which begs a crucial question: what is the simplest possible internal description? This exercise tackles the concept of a minimal realization by having you analyze a transfer function with pole-zero cancellations. You will learn how common factors in a transfer function correspond to 'hidden' dynamics—unreachable or unobservable modes—and how simplifying the transfer function reveals the dimension of the minimal model, an essential step for efficient system design [@problem_id:2908024].", "problem": "Consider a single-input single-output, linear time-invariant, discrete-time system with complex variable $z$ and transfer function\n$$\nH(z) = \\frac{\\left(z - 1\\right)\\left(z - \\frac{1}{2}\\right)\\left(z + \\frac{1}{3}\\right)}{\\left(z - 1\\right)^{2}\\left(z - \\frac{1}{2}\\right)\\left(z + \\frac{1}{4}\\right)\\left(z - \\frac{2}{3}\\right)} \\, .\n$$\nYou are tasked with deriving a state-space realization starting from the foundational definitions for discrete-time linear systems. Use only the following base concepts: the definition of a state-space realization, the definition of the transfer function $H(z)$ as $H(z) = C\\left(zI - A\\right)^{-1}B + D$, and the definition of minimality in terms of reachability and observability.\n\nExplain, from these principles, how non-coprime numerator and denominator polynomials are handled when deriving a realization, and why common factors impact minimality. Your explanation should make precise how common factors in the numerator and denominator manifest as uncontrollable or unobservable modes in a non-minimal realization and how they are eliminated to obtain a minimal realization.\n\nCompute the minimal achievable state dimension $n_{\\min}$ for any realization that exactly reproduces $H(z)$. Express your final answer as a single integer. No rounding is needed.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard task in linear systems theory, requiring an understanding of the relationship between a system's external description (transfer function) and its internal description (state-space realization), with a specific focus on the concept of minimality. The problem is valid and permits a rigorous, unique solution.\n\nWe begin from first principles. A single-input single-output (SISO) linear time-invariant (LTI) discrete-time system can be described by the state-space equations:\n$$\n\\mathbf{x}[k+1] = A\\mathbf{x}[k] + B u[k]\n$$\n$$\ny[k] = C\\mathbf{x}[k] + D u[k]\n$$\nwhere $\\mathbf{x}[k] \\in \\mathbb{R}^n$ is the state vector at time step $k$, $u[k] \\in \\mathbb{R}$ is the input, and $y[k] \\in \\mathbb{R}$ is the output. The matrices $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times 1}$, $C \\in \\mathbb{R}^{1 \\times n}$, and $D \\in \\mathbb{R}$ constitute the state-space realization. The dimension of the realization is $n$.\n\nThe transfer function $H(z)$ of the system is defined as the Z-transform of the output divided by the Z-transform of the input, under zero initial conditions. Applying the Z-transform to the state-space equations yields:\n$$\nz\\mathbf{X}(z) = A\\mathbf{X}(z) + B U(z)\n$$\n$$\nY(z) = C\\mathbf{X}(z) + D U(z)\n$$\nFrom the first equation, we solve for $\\mathbf{X}(z)$:\n$$\n(zI - A)\\mathbf{X}(z) = B U(z) \\implies \\mathbf{X}(z) = (zI - A)^{-1} B U(z)\n$$\nSubstituting this into the second equation gives:\n$$\nY(z) = C(zI - A)^{-1} B U(z) + D U(z) = \\left( C(zI - A)^{-1}B + D \\right) U(z)\n$$\nThus, the transfer function is given by the expression $H(z) = \\frac{Y(z)}{U(z)}$:\n$$\nH(z) = C(zI - A)^{-1}B + D\n$$\nUsing the identity for the inverse of a matrix, $(zI-A)^{-1} = \\frac{\\text{adj}(zI-A)}{\\det(zI-A)}$, we can write:\n$$\nH(z) = \\frac{C \\, \\text{adj}(zI-A) \\, B}{\\det(zI-A)} + D\n$$\nThe characteristic polynomial of the matrix $A$ is $p_A(z) = \\det(zI-A)$. Its roots are the eigenvalues of $A$, which are the modes of the system. The poles of the transfer function $H(z)$ are a subset of the eigenvalues of $A$.\n\nA state-space realization is said to be **minimal** if its dimension $n$ is the smallest possible among all realizations that produce the given transfer function $H(z)$. A fundamental theorem of linear systems theory states that a realization is minimal if and only if it is both **completely reachable** and **completely observable**.\n- **Reachability** means that any state $\\mathbf{x} \\in \\mathbb{R}^n$ can be reached from the zero state in a finite number of steps by applying an appropriate input sequence.\n- **Observability** means that the initial state $\\mathbf{x}[0]$ can be uniquely determined from a finite sequence of future outputs $y[k]$ and inputs $u[k]$.\n\nThe transfer function $H(z)$ only describes the input-output behavior of a system. This behavior is determined exclusively by the subsystem that is both reachable and observable. Any modes of the system (eigenvalues of $A$) that are either unreachable or unobservable are \"hidden\" from the input-output perspective and do not appear in the poles of the simplified transfer function.\n\nThe given transfer function is:\n$$\nH(z) = \\frac{\\left(z - 1\\right)\\left(z - \\frac{1}{2}\\right)\\left(z + \\frac{1}{3}\\right)}{\\left(z - 1\\right)^{2}\\left(z - \\frac{1}{2}\\right)\\left(z + \\frac{1}{4}\\right)\\left(z - \\frac{2}{3}\\right)}\n$$\nThe numerator polynomial $N(z)$ and denominator polynomial $D(z)$ are not coprime. We can identify common factors by inspection. The zeros are at $z=1$, $z=\\frac{1}{2}$, and $z=-\\frac{1}{3}$. The poles are at $z=1$ (with multiplicity $2$), $z=\\frac{1}{2}$, $z=-\\frac{1}{4}$, and $z=\\frac{2}{3}$.\n\nIf one were to construct a realization directly from this un-simplified form, for example using a controllable or observable canonical form, the dimension of the resulting state-space model would be equal to the degree of the denominator polynomial, which is $2+1+1+1 = 5$. Let this non-minimal realization be $(A_5, B_5, C_5, D_5)$. The characteristic polynomial of $A_5$ would be $\\det(zI-A_5) = \\left(z - 1\\right)^{2}\\left(z - \\frac{1}{2}\\right)\\left(z + \\frac{1}{4}\\right)\\left(z - \\frac{2}{3}\\right)$, and the eigenvalues of $A_5$ would be $\\{1, 1, \\frac{1}{2}, -\\frac{1}{4}, \\frac{2}{3}\\}$.\n\nHowever, the transfer function has pole-zero cancellations. The factors $(z-1)$ and $(z-\\frac{1}{2})$ appear in both the numerator and the denominator. A pole-zero cancellation in the transfer function $H(z) = C(zI-A)^{-1}B+D$ implies that the corresponding mode (eigenvalue of $A$) is either unreachable or unobservable. For a mode at $z=\\lambda$ to be cancelled, the numerator term $C \\, \\text{adj}(zI-A) \\, B$ must also be zero at $z=\\lambda$, effectively masking the pole that arises from $\\det(zI-A)=0$. This masking signifies a loss of either reachability or observability for that specific mode.\n\nTo obtain a minimal realization, we must first simplify the transfer function by canceling all common factors. This process isolates the part of the system dynamics that is both reachable and observable.\nThe greatest common divisor of the numerator and denominator is $\\gcd(N(z), D(z)) = \\left(z-1\\right)\\left(z-\\frac{1}{2}\\right)$.\nDividing the numerator and denominator by this GCD yields the simplified transfer function, $H_{min}(z)$:\n$$\nH_{min}(z) = \\frac{\\frac{\\left(z - 1\\right)\\left(z - \\frac{1}{2}\\right)\\left(z + \\frac{1}{3}\\right)}{\\left(z - 1\\right)\\left(z - \\frac{1}{2}\\right)}}{\\frac{\\left(z - 1\\right)^{2}\\left(z - \\frac{1}{2}\\right)\\left(z + \\frac{1}{4}\\right)\\left(z - \\frac{2}{3}\\right)}{\\left(z - 1\\right)\\left(z - \\frac{1}{2}\\right)}} = \\frac{z + \\frac{1}{3}}{\\left(z - 1\\right)\\left(z + \\frac{1}{4}\\right)\\left(z - \\frac{2}{3}\\right)}\n$$\nThis simplified transfer function $H_{min}(z)$ represents the purely reachable and observable part of any system having the original transfer function $H(z)$. A minimal state-space realization is one that realizes $H_{min}(z)$.\n\nThe minimal achievable state dimension, $n_{min}$, for any realization of a given rational transfer function is equal to the degree of the denominator of the transfer function after it has been reduced to a coprime fraction.\nThe denominator of $H_{min}(z)$ is:\n$$\nD_{min}(z) = \\left(z - 1\\right)\\left(z + \\frac{1}{4}\\right)\\left(z - \\frac{2}{3}\\right)\n$$\nThe degree of this polynomial is $3$. Therefore, the minimal dimension of any state-space realization that exactly reproduces the input-output behavior described by $H(z)$ is $3$. The modes corresponding to the canceled poles (one at $z=1$ and one at $z=\\frac{1}{2}$) would be present as either unreachable or unobservable states in any non-minimal realization of dimension greater than $3$.\n\nThe minimal achievable state dimension $n_{\\min}$ is therefore $3$.", "answer": "$$\n\\boxed{3}\n$$", "id": "2908024"}]}