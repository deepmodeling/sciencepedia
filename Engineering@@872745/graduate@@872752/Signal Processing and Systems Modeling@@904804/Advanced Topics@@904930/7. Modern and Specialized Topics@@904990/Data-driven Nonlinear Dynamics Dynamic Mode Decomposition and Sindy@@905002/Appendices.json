{"hands_on_practices": [{"introduction": "A core tenet of data-driven dynamics is that a time series can reveal the structure of the underlying system that generated it. While Dynamic Mode Decomposition (DMD) is a linear technique, it can perfectly capture nonlinear oscillatory behavior by \"lifting\" the data into a higher-dimensional space using time delays. This practice [@problem_id:2862877] explores this fundamental concept, asking you to determine the minimum number of time delays required to uniquely identify an oscillation, thereby connecting the signal's complexity to the rank of its corresponding Hankel matrix.", "problem": "You observe a real-valued, uniformly sampled time series generated by a single sinusoid with additive noise, \n$$y_k = A \\cos(\\omega k \\Delta t + \\phi) + \\eta_k,$$\nwhere $A \\neq 0$, $\\omega \\in \\mathbb{R}$, $\\Delta t > 0$, $\\phi \\in \\mathbb{R}$, and $\\{\\eta_k\\}$ is a zero-mean disturbance with finite variance. Assume the nondegenerate oscillatory case where the discrete-time radian frequency $\\theta := \\omega \\Delta t$ satisfies $\\theta \\not\\equiv 0 \\ (\\mathrm{mod}\\ 2\\pi)$ and $\\theta \\not\\equiv \\pi \\ (\\mathrm{mod}\\ 2\\pi)$. You build a Hankel matrix with $m$ delays from $N$ samples,\n$$\\mathbf{H} = \n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_{N-m} \\\\\ny_1 & y_2 & \\cdots & y_{N-m+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{m-1} & y_m & \\cdots & y_{N-1}\n\\end{bmatrix} \\in \\mathbb{R}^{m \\times (N-m+1)},$$\nand apply Hankel Dynamic Mode Decomposition (delay-embedded Dynamic Mode Decomposition), which seeks a linear map that advances the delay-embedded state by one time step. Starting only from the fundamental representation of sinusoids in terms of complex exponentials and the definition of a Hankel matrix, derive the minimal linear recurrence order satisfied by the noiseless signal and use it to determine the smallest number of delays $m$ required so that Hankel Dynamic Mode Decomposition can uniquely identify the oscillation frequency $\\omega$ in the limit of vanishing noise. Explicitly relate this minimal $m$ to the rank of the noiseless Hankel matrix. State any assumptions you use about distinctness of the underlying modes and sampling. Provide your final answer as the minimal integer $m$; no rounding is necessary and no units are required.", "solution": "The problem asks for the behavior in the limit of vanishing noise, so we analyze the noiseless signal $x_k = A \\cos(\\omega k \\Delta t + \\phi)$. First, we express this real-valued sinusoid as a sum of complex exponentials using Euler's formula, as instructed.\nLet $\\theta = \\omega \\Delta t$. The signal is:\n$$ x_k = A \\left( \\frac{\\exp(i(k \\theta + \\phi)) + \\exp(-i(k \\theta + \\phi))}{2} \\right) $$\n$$ x_k = \\left( \\frac{A \\exp(i\\phi)}{2} \\right) \\exp(ik\\theta) + \\left( \\frac{A \\exp(-i\\phi)}{2} \\right) \\exp(-ik\\theta) $$\nThis is a signal of the form $x_k = c_1 \\lambda_1^k + c_2 \\lambda_2^k$, where the modes are $\\lambda_1 = \\exp(i\\theta)$ and $\\lambda_2 = \\exp(-i\\theta)$, and the coefficients are $c_1 = \\frac{A}{2}\\exp(i\\phi)$ and $c_2 = \\overline{c_1}$. Since $A \\neq 0$, both $c_1$ and $c_2$ are nonzero.\n\nA signal that is a linear combination of $p$ distinct exponential terms satisfies a unique minimal-order linear constant-coefficient recurrence relation of order $p$. We must determine if the modes $\\lambda_1$ and $\\lambda_2$ are distinct. The modes are equal if $\\exp(i\\theta) = \\exp(-i\\theta)$, which implies $\\exp(i2\\theta) = 1$. This occurs if $2\\theta$ is an integer multiple of $2\\pi$, i.e., $2\\theta = 2n\\pi$ for $n \\in \\mathbb{Z}$, which simplifies to $\\theta = n\\pi$. The problem statement explicitly provides the nondegeneracy condition that $\\theta \\not\\equiv 0 \\ (\\mathrm{mod}\\ 2\\pi)$ and $\\theta \\not\\equiv \\pi \\ (\\mathrm{mod}\\ 2\\pi)$. This is equivalent to stating that $\\theta$ is not an integer multiple of $\\pi$. Therefore, $\\lambda_1 \\neq \\lambda_2$, and the modes are distinct.\n\nSince the signal is composed of two distinct modes, the minimal linear recurrence order is $p=2$. The characteristic polynomial for this recurrence has roots $\\lambda_1$ and $\\lambda_2$.\n$$ P(z) = (z - \\lambda_1)(z - \\lambda_2) = z^2 - (\\lambda_1 + \\lambda_2)z + \\lambda_1\\lambda_2 = 0 $$\nThe coefficients are:\n$$ \\lambda_1 + \\lambda_2 = \\exp(i\\theta) + \\exp(-i\\theta) = 2\\cos(\\theta) $$\n$$ \\lambda_1 \\lambda_2 = \\exp(i\\theta)\\exp(-i\\theta) = 1 $$\nThe characteristic polynomial is $z^2 - 2\\cos(\\theta)z + 1 = 0$. This corresponds to the linear recurrence relation:\n$$ x_{k+2} - 2\\cos(\\theta)x_{k+1} + x_k = 0 $$\nThis relation shows that any three consecutive samples of the noiseless signal are linearly dependent.\n\nNow, we relate this to the Hankel matrix. Let $\\mathbf{X}$ be the Hankel matrix for the noiseless signal $x_k$. Its columns are the delay vectors $\\mathbf{x}_j = [x_j, x_{j+1}, \\dots, x_{j+m-1}]^T$. Each column vector can be expressed as a linear combination of two fixed Vandermonde vectors associated with the modes:\n$$ \\mathbf{x}_j = c_1 \\lambda_1^j \\begin{pmatrix} 1 \\\\ \\lambda_1 \\\\ \\vdots \\\\ \\lambda_1^{m-1} \\end{pmatrix} + c_2 \\lambda_2^j \\begin{pmatrix} 1 \\\\ \\lambda_2 \\\\ \\vdots \\\\ \\lambda_2^{m-1} \\end{pmatrix} $$\nThe entire column space of $\\mathbf{X}$ is spanned by these two Vandermonde vectors. Since $\\lambda_1 \\neq \\lambda_2$, these two vectors are linearly independent provided $m \\ge 2$. According to Kronecker's theorem, the rank of an infinite Hankel matrix is equal to the number of distinct exponential modes in the generating signal. For a finite Hankel matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times (N-m+1)}$, the rank will be $2$ provided the dimensions are sufficient, i.e., $m \\ge 2$ and the number of columns $N-m+1 \\ge 2$.\n\nHankel DMD aims to find a linear operator that models the evolution of the delay-embedded state $\\mathbf{x}_k \\in \\mathbb{R}^{m}$. The dimension of this delay vector, $m$, must be large enough to embed the dynamics. To capture a system whose underlying state space has dimension $r$, the embedding dimension $m$ must be at least $r$. Here, the rank of the noiseless data matrix is $r=2$, representing a two-dimensional system (one complex-conjugate pair of modes). Therefore, the number of delays $m$ must be at least $2$.\n\nLet's check the sufficiency.\nIf $m=1$, the state is a scalar $x_k$. The model is $x_{k+1} = a x_k$. This can only model a single real mode, not a complex-conjugate pair that generates oscillations. So, $m=1$ is insufficient.\n\nIf $m=2$, the state is $\\mathbf{x}_k = [x_k, x_{k+1}]^T$. The one-step evolution is $\\mathbf{x}_{k+1} = [x_{k+1}, x_{k+2}]^T$. We seek an operator $\\mathbf{A}$ such that $\\mathbf{x}_{k+1} = \\mathbf{A}\\mathbf{x}_k$.\n$$ \\begin{pmatrix} x_{k+1} \\\\ x_{k+2} \\end{pmatrix} = \\mathbf{A} \\begin{pmatrix} x_k \\\\ x_{k+1} \\end{pmatrix} $$\nThe first row implies $x_{k+1} = A_{11}x_k + A_{12}x_{k+1}$, which requires $A_{11}=0$ and $A_{12}=1$.\nThe second row is $x_{k+2} = A_{21}x_k + A_{22}x_{k+1}$. From our recurrence relation, we know $x_{k+2} = -x_k + 2\\cos(\\theta)x_{k+1}$.\nThus, the unique linear operator is the companion matrix:\n$$ \\mathbf{A} = \\begin{pmatrix} 0 & 1 \\\\ -1 & 2\\cos(\\theta) \\end{pmatrix} $$\nThe eigenvalues of this operator $\\mathbf{A}$ are found from the characteristic equation $\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = \\lambda^2 - 2\\cos(\\theta)\\lambda + 1 = 0$, which yields the solutions $\\lambda = \\exp(\\pm i\\theta)$. These are precisely the modes of the underlying system.\nHankel DMD, operating on data matrices formed from $\\mathbf{x}_k$, will compute an approximation of this matrix $\\mathbf{A}$. The eigenvalues of the computed operator will approximate $\\exp(\\pm i\\theta)$, from which $\\theta = \\omega\\Delta t$ and consequently the frequency $\\omega$ can be uniquely determined (up to a sign, which is physically ambiguous without further information).\n\nTherefore, $m=2$ is sufficient to capture the dynamics. Since $m=1$ is insufficient, the minimal required number of delays is $m=2$. This minimal value for $m$ is equal to the minimal linear recurrence order of the signal and also to the rank of the noiseless Hankel matrix, assuming sufficient data. The assumption on sampling is that the number of samples $N$ is large enough for the matrices used in DMD to have the correct rank, which minimally requires $N-m+1 \\geq 2$. For $m=2$, this means $N \\geq 3$, a trivial requirement.", "answer": "$$\n\\boxed{2}\n$$", "id": "2862877"}, {"introduction": "Moving from theory to practice requires confronting the reality of measurement noise. The precision of any data-driven model is fundamentally limited by the quality of the input data and the amount of it available. This exercise [@problem_id:2862855] tackles this challenge head-on by quantifying how noise propagates through the DMD algorithm and affects the accuracy of the identified frequencies. By deriving and implementing a formula for the variance of the frequency error, you will develop a practical rule of thumb for choosing the data window length to achieve a desired level of precision.", "problem": "You are given a single-mode discrete-time linear dynamical system observed in additive noise. The underlying clean state sequence is modeled as a complex exponential with constant magnitude and a single angular frequency. The data matrices used for Dynamic Mode Decomposition (DMD) are constructed from equispaced time samples of this state as follows.\n\nAssume the clean signal is given by $x_k = A \\, e^{i (\\omega k \\Delta t + \\phi_0)}$ for $k \\in \\{0,1,2,\\dots\\}$, where $A > 0$ is an unknown constant amplitude, $\\omega$ is the true angular frequency in radians per second, $\\Delta t$ is the sampling interval in seconds, and $\\phi_0$ is an arbitrary initial phase. The discrete-time eigenvalue is $\\lambda = e^{i \\theta}$ with $\\theta = \\omega \\Delta t$. The observed data are generated by additive complex-valued white Gaussian noise $\\eta_k$ with zero mean and variance $\\sigma^2$, so that the measured snapshots are $\\tilde{x}_k = x_k + \\eta_k$. Signal-to-noise ratio (SNR) is defined as $\\mathrm{SNR} = A^2 / \\sigma^2$, a dimensionless, linear-scale quantity. Consider the rank-$1$ DMD estimator that uses a window of $m$ consecutive snapshot pairs to form\n$$\nX = \\begin{bmatrix} \\tilde{x}_0 & \\tilde{x}_1 & \\dots & \\tilde{x}_{m-1} \\end{bmatrix}, \\quad\nY = \\begin{bmatrix} \\tilde{x}_1 & \\tilde{x}_2 & \\dots & \\tilde{x}_m \\end{bmatrix},\n$$\nand computes the least-squares DMD eigenvalue estimate $\\hat{\\lambda}$ by $\\hat{\\lambda} = (X^* Y)/(X^* X)$, where $X^*$ denotes conjugate transpose.\n\nYour tasks are:\n\n- Starting from the above definitions, small-noise linearization, and properties of additive white Gaussian noise, derive the leading-order dependence of the variance of the DMD eigenvalue phase error $\\delta \\theta = \\arg(\\hat{\\lambda}) - \\theta$ on the window length $m$ and the signal-to-noise ratio $\\mathrm{SNR}$. Your derivation must be principle-based, beginning from the least-squares estimator and noise perturbation without invoking any pre-quoted formula for the variance. Then express the implied standard deviation of the continuous-time frequency estimate error $\\delta \\omega$ in radians per second, where $\\omega = \\theta / \\Delta t$, in terms of $m$, $\\mathrm{SNR}$, and $\\Delta t$. Finally, propose a rule of thumb that returns the minimal window length $m_{\\min}$ necessary to achieve a target standard deviation $\\varepsilon_\\omega$ (in radians per second) for the frequency estimate error $\\delta \\omega$, as a function of $\\mathrm{SNR}$ and $\\Delta t$. The rule of thumb must be an explicit algebraic expression $m_{\\min} = f(\\mathrm{SNR}, \\Delta t, \\varepsilon_\\omega)$ with a clearly identified constant factor.\n\n- Implement a complete, runnable program that evaluates your derived expressions. The program must compute two types of outputs over a fixed test suite:\n  1. Predicted standard deviation of the frequency error $\\delta \\omega$ (in radians per second) for given $(m, \\mathrm{SNR}, \\Delta t)$.\n  2. Minimal integer window length $m_{\\min}$ to achieve a target frequency standard deviation $\\varepsilon_\\omega$ (in radians per second) given $(\\mathrm{SNR}, \\Delta t, \\varepsilon_\\omega)$. Enforce a lower bound $m_{\\min} \\geq 3$ to ensure a meaningful DMD window.\n\nUse the following test suite:\n\n- Prediction cases (compute the frequency standard deviation in radians per second):\n  - Case A$1$: $m = 50$, $\\mathrm{SNR} = 25$, $\\Delta t = 0.01$ seconds.\n  - Case A$2$: $m = 10$, $\\mathrm{SNR} = 4$, $\\Delta t = 0.02$ seconds.\n  - Case A$3$: $m = 3$, $\\mathrm{SNR} = 100$, $\\Delta t = 0.001$ seconds.\n- Design cases (compute the minimal window length $m_{\\min}$ as an integer):\n  - Case B$1$: $\\mathrm{SNR} = 25$, $\\Delta t = 0.01$ seconds, $\\varepsilon_\\omega = 0.5$ radians per second.\n  - Case B$2$: $\\mathrm{SNR} = 4$, $\\Delta t = 0.02$ seconds, $\\varepsilon_\\omega = 2.0$ radians per second.\n  - Case B$3$: $\\mathrm{SNR} = 0.5$, $\\Delta t = 0.01$ seconds, $\\varepsilon_\\omega = 1.0$ radians per second.\n  - Case B$4$: $\\mathrm{SNR} = 100$, $\\Delta t = 0.001$ seconds, $\\varepsilon_\\omega = 5.0$ radians per second.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the three predicted frequency standard deviations for the prediction cases (in radians per second, as floating-point numbers), followed by the four minimal window lengths for the design cases (as integers): $[\\text{A1}, \\text{A2}, \\text{A3}, \\text{B1}, \\text{B2}, \\text{B3}, \\text{B4}]$.\n\nAngle units must be radians, and frequency units must be radians per second. All outputs must be in these units. No external input is required; all parameters are as specified above and embedded in the program. The final outputs for each test case must be floating-point numbers or integers, as specified.", "solution": "We begin with the least-squares estimator for the discrete-time eigenvalue $\\lambda$:\n$$\n\\hat{\\lambda} = \\frac{\\sum_{k=0}^{m-1} \\tilde{x}_k^* \\tilde{x}_{k+1}}{\\sum_{k=0}^{m-1} |\\tilde{x}_k|^2}\n$$\nSubstitute the observation model $\\tilde{x}_k = x_k + \\eta_k$. The numerator $N$ and denominator $D$ become:\n$$\nN = \\sum_{k=0}^{m-1} (x_k^* + \\eta_k^*) (x_{k+1} + \\eta_{k+1}) = \\sum_{k=0}^{m-1} ( x_k^* x_{k+1} + x_k^* \\eta_{k+1} + \\eta_k^* x_{k+1} + \\eta_k^* \\eta_{k+1} )\n$$\n$$\nD = \\sum_{k=0}^{m-1} |x_k + \\eta_k|^2 = \\sum_{k=0}^{m-1} ( |x_k|^2 + x_k^* \\eta_k + \\eta_k^* x_k + |\\eta_k|^2 )\n$$\nFor the clean signal, $x_{k+1} = \\lambda x_k$. The clean parts of $N$ and $D$ are $N_0 = \\sum x_k^* x_{k+1} = \\lambda \\sum |x_k|^2$ and $D_0 = \\sum |x_k|^2$. Thus, the noiseless estimate is exact: $\\hat{\\lambda}_0 = N_0/D_0 = \\lambda$.\n\nWe perform a first-order perturbation analysis under the small-noise assumption ($\\sigma^2 \\ll A^2$, or $\\mathrm{SNR} \\gg 1$), keeping only terms linear in the noise $\\eta_k$.\n$$\n\\hat{\\lambda} = \\frac{N_0 + \\delta N}{D_0 + \\delta D} \\approx \\frac{N_0}{D_0} \\left(1 + \\frac{\\delta N}{N_0} - \\frac{\\delta D}{D_0}\\right) = \\lambda + \\frac{\\delta N - \\lambda \\delta D}{D_0}\n$$\nThe first-order perturbations are:\n$$\n\\delta N \\approx \\sum_{k=0}^{m-1} (x_k^* \\eta_{k+1} + \\eta_k^* x_{k+1})\n$$\n$$\n\\delta D \\approx \\sum_{k=0}^{m-1} (x_k^* \\eta_k + \\eta_k^* x_k)\n$$\nThe perturbation to the eigenvalue, $\\delta \\lambda = \\hat{\\lambda} - \\lambda$, is:\n$$\n\\delta \\lambda \\approx \\frac{1}{D_0} \\sum_{k=0}^{m-1} [ (x_k^* \\eta_{k+1} + \\eta_k^* x_{k+1}) - \\lambda (x_k^* \\eta_k + \\eta_k^* x_k) ]\n$$\nUsing $x_{k+1} = \\lambda x_k$, this simplifies. The term $\\eta_k^* x_{k+1} = \\eta_k^* \\lambda x_k = \\lambda \\eta_k^* x_k$ cancels with $-\\lambda \\eta_k^* x_k$.\n$$\n\\delta \\lambda \\approx \\frac{1}{D_0} \\sum_{k=0}^{m-1} (x_k^* \\eta_{k+1} - \\lambda x_k^* \\eta_k)\n$$\nThis sum has a telescoping structure. Let's examine the coefficient of each noise term $\\eta_k$. For $k \\in \\{1, \\dots, m-1\\}$, the coefficient is $x_{k-1}^* - \\lambda x_k^*$. From the signal model, $x_k = \\lambda x_{k-1}$, so $x_k^* = \\lambda^* x_{k-1}^* = \\lambda^{-1} x_{k-1}^*$ (since $|\\lambda|=1$). Thus, $x_{k-1}^* = \\lambda x_k^*$, and the coefficient vanishes. The only non-canceling terms are at the boundaries of the sum:\n$$\n\\sum_{k=0}^{m-1} (x_k^* \\eta_{k+1} - \\lambda x_k^* \\eta_k) = x_{m-1}^* \\eta_m - \\lambda x_0^* \\eta_0\n$$\nThe clean-signal denominator is $D_0 = \\sum_{k=0}^{m-1} |x_k|^2 = \\sum_{k=0}^{m-1} A^2 = m A^2$.\nSo, the eigenvalue perturbation is:\n$$\n\\delta \\lambda \\approx \\frac{1}{m A^2} (x_{m-1}^* \\eta_m - \\lambda x_0^* \\eta_0)\n$$\nThe phase error is $\\delta \\theta = \\arg(\\hat{\\lambda}) - \\theta = \\arg(\\lambda + \\delta\\lambda) - \\arg(\\lambda) = \\arg(1 + \\lambda^{-1}\\delta\\lambda)$. For small perturbations, $\\arg(1+z) \\approx \\mathrm{Im}(z)$.\n$$\n\\delta \\theta \\approx \\mathrm{Im}(\\lambda^{-1} \\delta \\lambda) \\approx \\mathrm{Im}\\left( \\frac{\\lambda^{-1}}{m A^2} (x_{m-1}^* \\eta_m - \\lambda x_0^* \\eta_0) \\right) = \\mathrm{Im}\\left( \\frac{1}{m A^2} (\\lambda^{-1} x_{m-1}^* \\eta_m - x_0^* \\eta_0) \\right)\n$$\nTo find the variance, $\\mathrm{Var}(\\delta\\theta) = \\mathbb{E}[(\\delta\\theta)^2]$ (since $\\mathbb{E}[\\delta\\theta] = 0$), we use the property that for a zero-mean complex random variable $Z$ with $\\mathbb{E}[Z^2]=0$ (true for circular complex Gaussian noise), $\\mathbb{E}[(\\mathrm{Im}(Z))^2] = \\frac{1}{2}\\mathbb{E}[|Z|^2]$.\n$$\n\\mathrm{Var}(\\delta\\theta) \\approx \\frac{1}{2} \\mathbb{E}[|\\lambda^{-1} \\delta \\lambda|^2] = \\frac{1}{2} \\mathbb{E}[|\\delta \\lambda|^2]\n$$\nWe compute the variance of $\\delta \\lambda$. Since $\\eta_0$ and $\\eta_m$ are independent (for $m>0$) and mean-zero:\n$$\n\\mathbb{E}[|\\delta \\lambda|^2] \\approx \\frac{1}{(m A^2)^2} \\mathbb{E}[|x_{m-1}^* \\eta_m - \\lambda x_0^* \\eta_0|^2] = \\frac{1}{m^2 A^4} \\left( |x_{m-1}|^2 \\mathbb{E}[|\\eta_m|^2] + |\\lambda|^2 |x_0|^2 \\mathbb{E}[|\\eta_0|^2] \\right)\n$$\nWith $|x_k|^2 = A^2$, $|\\lambda|^2=1$, and $\\mathbb{E}[|\\eta_k|^2] = \\sigma^2$:\n$$\n\\mathbb{E}[|\\delta \\lambda|^2] \\approx \\frac{1}{m^2 A^4} (A^2 \\sigma^2 + A^2 \\sigma^2) = \\frac{2 A^2 \\sigma^2}{m^2 A^4} = \\frac{2 \\sigma^2}{m^2 A^2} = \\frac{2}{m^2 \\mathrm{SNR}}\n$$\nThus, the variance of the phase error is:\n$$\n\\mathrm{Var}(\\delta\\theta) \\approx \\frac{1}{2} \\mathbb{E}[|\\delta\\lambda|^2] = \\frac{1}{m^2 \\mathrm{SNR}}\n$$\nThe analysis shows the variance is inversely proportional to $\\mathrm{SNR}$ and to the square of the window length $m$.\n\nNext, we find the standard deviation of the continuous-time frequency error, $\\delta \\omega$. The frequency estimate is $\\hat{\\omega} = \\arg(\\hat{\\lambda})/\\Delta t = (\\theta+\\delta\\theta)/\\Delta t = \\omega + \\delta\\theta/\\Delta t$. So, $\\delta\\omega = \\delta\\theta / \\Delta t$. The standard deviation $\\sigma_{\\delta\\omega}$ is:\n$$\n\\sigma_{\\delta\\omega} = \\sqrt{\\mathrm{Var}(\\delta\\omega)} = \\frac{\\sqrt{\\mathrm{Var}(\\delta\\theta)}}{\\Delta t} \\approx \\frac{1}{\\Delta t} \\sqrt{\\frac{1}{m^2 \\mathrm{SNR}}} = \\frac{1}{m \\Delta t \\sqrt{\\mathrm{SNR}}}\n$$\nThis is the required expression for the standard deviation of the frequency error.\n\nFinally, we propose a rule of thumb for the minimal window length $m_{\\min}$ to achieve a target frequency standard deviation $\\varepsilon_\\omega$. We set $\\sigma_{\\delta\\omega} \\le \\varepsilon_\\omega$:\n$$\n\\frac{1}{m_{\\min} \\Delta t \\sqrt{\\mathrm{SNR}}} \\le \\varepsilon_\\omega \\implies m_{\\min} \\ge \\frac{1}{\\varepsilon_\\omega \\Delta t \\sqrt{\\mathrm{SNR}}}\n$$\nThe minimal integer length is the ceiling of this value. Including the problem's constraint $m_{\\min} \\geq 3$:\n$$\nm_{\\min} = \\max\\left(3, \\left\\lceil \\frac{1}{\\varepsilon_\\omega \\Delta t \\sqrt{\\mathrm{SNR}}} \\right\\rceil\\right)\n$$\nThis rule of thumb has a constant factor of $1$.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the problem by first implementing the derived formulas for frequency error\n    standard deviation and minimal window length, then applying them to the given test cases.\n    \"\"\"\n\n    # --- Part 1: Prediction Cases ---\n    \n    # Test cases for predicting the standard deviation of the frequency error.\n    # Format: (m, SNR, dt)\n    prediction_cases = [\n        (50, 25, 0.01),  # Case A1\n        (10, 4, 0.02),   # Case A2\n        (3, 100, 0.001)  # Case A3\n    ]\n\n    predicted_std_devs = []\n    for m, snr, dt in prediction_cases:\n        # Formula for standard deviation of frequency error (sigma_delta_omega)\n        # sigma_delta_omega = 1 / (m * dt * sqrt(SNR))\n        std_dev = 1.0 / (m * dt * np.sqrt(snr))\n        predicted_std_devs.append(std_dev)\n\n    # --- Part 2: Design Cases ---\n\n    # Test cases for designing the minimal window length m_min.\n    # Format: (SNR, dt, epsilon_omega)\n    design_cases = [\n        (25, 0.01, 0.5),    # Case B1\n        (4, 0.02, 2.0),     # Case B2\n        (0.5, 0.01, 1.0),   # Case B3\n        (100, 0.001, 5.0)   # Case B4\n    ]\n\n    minimal_lengths = []\n    for snr, dt, epsilon_omega in design_cases:\n        # The small-noise approximation might be less accurate for low SNR (e.g., Case B3).\n        # However, we apply the formula as derived.\n        \n        # Formula for minimal required window length (m_min)\n        # m_min_raw = 1 / (epsilon_omega * dt * sqrt(SNR))\n        m_min_raw = 1.0 / (epsilon_omega * dt * np.sqrt(snr))\n        \n        # The minimal length is the ceiling of this value, with a lower bound of 3.\n        m_min = max(3, math.ceil(m_min_raw))\n        minimal_lengths.append(m_min)\n\n    # --- Final Output Formatting ---\n\n    # Combine all results into a single list\n    all_results = predicted_std_devs + minimal_lengths\n    \n    # Format the output string as required by the problem statement\n    result_str = \"[\" + \",\".join(map(str, all_results)) + \"]\"\n    \n    print(result_str)\n\nsolve()\n```", "id": "2862855"}, {"introduction": "The Sparse Identification of Nonlinear Dynamics (SINDy) algorithm excels at discovering parsimonious models from a library of candidate functions. However, this library may contain terms with vastly different physical units and magnitudes, such as a position $x$ and a nonlinear term $x^2$. This poses a challenge for the thresholding step at the core of SINDy: how can a single threshold value be applied fairly? This exercise [@problem_id:2862870] demonstrates how a proper normalization scheme creates a level playing field for all candidate functions, making the sparse identification process robust and physically meaningful.", "problem": "You are modeling a single-state continuous-time system with state $x(t)$ using Sparse Identification of Nonlinear Dynamics (SINDy). You collect snapshot pairs $\\{x(t_k), \\dot{x}(t_k)\\}_{k=1}^{m}$ and assemble a library matrix $\\Theta(X) \\in \\mathbb{R}^{m \\times p}$ with columns comprising candidate functions of $x$, and a target vector $y \\in \\mathbb{R}^{m}$ containing $\\dot{x}(t_k)$. For each state, SINDy solves a linear regression problem of the form $y \\approx \\Theta(X)\\,\\xi$ and uses Sequentially Thresholded Least Squares (STLSQ), which alternates between least-squares estimation and hard-thresholding of entries of $\\xi$ based on a threshold.\n\nTo make the thresholding in STLSQ physically meaningful and robust across features with different physical units and magnitudes, you decide to rescale the columns of $\\Theta(X)$ by a diagonal matrix $S = \\mathrm{diag}(s_1,\\dots,s_p)$ to form a normalized library $Z = \\Theta(X)\\,S^{-1}$. Your design requirement is that the normalization should (i) remove units from each column of $Z$ by dividing by appropriate characteristic unit scales based on the dimensions of each feature and chosen characteristic base-unit scales for the dataset, and (ii) balance magnitudes by dividing each now-dimensionless column by its sample standard deviation over the $m$ snapshots. After this normalization, you apply a single hard-threshold value $\\Lambda$ inside STLSQ to the coefficients associated with the normalized library.\n\nStarting only from the definitions of least squares regression and hard-thresholding, do the following:\n\n1) Derive, in closed form, how the coefficient vector changes under the column scaling by $S$ and how a single hard threshold $\\Lambda$ applied to the normalized coefficients translates into a feature-dependent effective hard threshold on the original, unscaled coefficients. Your derivation must start from the normal equations for least squares, the definition of hard-thresholding, and linear algebraic invariance of the prediction under column rescaling, without assuming any pre-known formula connecting rescaling and thresholds.\n\n2) Consider the concrete case of modeling $\\dot{x}(t)$ from a library with two columns $[x, x^2]$. The state $x$ is measured in meters. You choose a characteristic length scale $L_{\\mathrm{c}} = 2\\,\\mathrm{m}$ to nondimensionalize $x$ and estimate the sample standard deviations of the dimensionless columns over the $m$ snapshots as follows: $\\mathrm{std}(x/L_{\\mathrm{c}}) = 0.80$ and $\\mathrm{std}((x/L_{\\mathrm{c}})^2) = 0.70$. You decide to use a single hard threshold of $\\Lambda = 0.15\\,\\mathrm{m/s}$ when thresholding the coefficients associated with the normalized library $Z$.\n\nAccording to your derived expression in part 1), what is the resulting effective hard threshold on the unscaled coefficient multiplying the $x^2$ column? Express the final numerical value in $\\mathrm{s}^{-1}\\,\\mathrm{m}^{-1}$ and round your answer to four significant figures.", "solution": "The core of this problem lies in understanding how a linear transformation of the feature space (the columns of the library matrix) affects the coefficients in a linear regression model and, consequently, the application of a nonlinear operation like hard-thresholding on those coefficients.\n\nLet the unscaled regression problem be\n$$ y \\approx \\Theta \\xi $$\nwhere $y \\in \\mathbb{R}^{m}$ is the vector of time derivatives, $\\Theta \\in \\mathbb{R}^{m \\times p}$ is the library of candidate functions, and $\\xi \\in \\mathbb{R}^{p}$ is the vector of coefficients we seek to identify. The ordinary least-squares estimate, $\\xi_{LS}$, minimizes the squared Euclidean norm of the residual, $\\|y - \\Theta \\xi\\|_{2}^{2}$, and is found by solving the normal equations:\n$$ \\Theta^{T} \\Theta \\xi_{LS} = \\Theta^{T} y $$\n\nThe problem introduces a scaled library $Z = \\Theta S^{-1}$, where $S = \\mathrm{diag}(s_1, \\dots, s_p)$ is a diagonal matrix of scaling factors. The regression is now performed with respect to a new coefficient vector, $\\zeta \\in \\mathbb{R}^{p}$, for the scaled library:\n$$ y \\approx Z \\zeta $$\nThe least-squares solution for $\\zeta$, which we denote $\\zeta_{LS}$, is given by the corresponding normal equations:\n$$ Z^{T} Z \\zeta_{LS} = Z^{T} y $$\n\nThe physical model being identified is independent of the basis chosen for the library. Therefore, the prediction must be invariant under this rescaling:\n$$ \\Theta \\xi = Z \\zeta $$\nSubstituting the definition $Z = \\Theta S^{-1}$, we have:\n$$ \\Theta \\xi = (\\Theta S^{-1}) \\zeta $$\nAssuming the columns of $\\Theta$ are linearly independent (a standard requirement for a well-posed least-squares problem, implying that $\\Theta^{T} \\Theta$ is invertible), we can left-multiply by $(\\Theta^{T} \\Theta)^{-1} \\Theta^{T}$ to find the unique relationship between the coefficient vectors:\n$$ \\xi = S^{-1} \\zeta $$\nOr, equivalently,\n$$ \\zeta = S \\xi $$\nThis directly shows how the coefficient vector transforms under the column scaling of the library. The $j$-th component of the scaled coefficient vector, $\\zeta_j$, is related to the $j$-th component of the unscaled vector, $\\xi_j$, by $\\zeta_j = s_j \\xi_j$.\n\nNow, we consider the hard-thresholding step as used in the Sequentially Thresholded Least Squares (STLSQ) algorithm. A single threshold, $\\Lambda$, is applied to the components of the coefficient vector $\\zeta$ associated with the normalized library $Z$. A coefficient $\\zeta_j$ is set to zero if its magnitude is less than the threshold:\n$$ \\text{Set } \\zeta_j \\to 0 \\quad \\text{if} \\quad |\\zeta_j| < \\Lambda $$\nTo find the equivalent, feature-dependent threshold on the original, unscaled coefficients $\\xi_j$, we substitute the relationship $\\zeta_j = s_j \\xi_j$ into the thresholding condition:\n$$ |s_j \\xi_j| < \\Lambda $$\nSince the scaling factors $s_j$ are positive real numbers (representing physical scales and standard deviations), we can write:\n$$ |\\xi_j| < \\frac{\\Lambda}{s_j} $$\nThis demonstrates that applying a uniform threshold $\\Lambda$ in the scaled domain is equivalent to applying a feature-dependent threshold, $\\lambda_j = \\Lambda/s_j$, in the original, unscaled domain. This completes the first part of the problem.\n\nFor the second part, we must apply this result to the specific case given. The unscaled library is $\\Theta = [x, x^2]$, so the feature columns are $\\theta_1(x) = x$ and $\\theta_2(x) = x^2$. We are interested in the effective threshold on the coefficient $\\xi_2$, which multiplies the $x^2$ column. Based on our derivation, this effective threshold is $\\lambda_2 = \\Lambda/s_2$.\n\nWe must determine the scaling factor $s_2$. The problem states that the normalization consists of two steps. First, the feature is made dimensionless using a characteristic scale. Second, the resulting dimensionless feature is divided by its sample standard deviation.\n\nThe feature in the second column is $\\theta_2 = x^2$. The state $x$ has units of meters ($\\mathrm{m}$). Therefore, $\\theta_2$ has units of $\\mathrm{m}^2$.\n1.  **Nondimensionalization**: The characteristic length scale is $L_{\\mathrm{c}} = 2\\,\\mathrm{m}$. To make $x^2$ dimensionless, we must divide by a quantity with units of $\\mathrm{m}^2$. Using $L_c$, this quantity is $L_c^2$. The dimensionless feature is $\\left(\\frac{x}{L_{\\mathrm{c}}}\\right)^2$.\n2.  **Magnitude Balancing**: The resulting dimensionless column is then divided by its sample standard deviation. The problem provides this value: $\\mathrm{std}((x/L_{\\mathrm{c}})^2) = 0.70$.\n\nCombining these two steps, the fully normalized second column, $z_2$, is:\n$$ z_2 = \\frac{\\left(\\frac{x}{L_{\\mathrm{c}}}\\right)^2}{\\mathrm{std}\\left(\\left(\\frac{x}{L_{\\mathrm{c}}}\\right)^2\\right)} = \\frac{\\theta_2}{L_{\\mathrm{c}}^2 \\cdot \\mathrm{std}\\left(\\left(\\frac{x}{L_{\\mathrm{c}}}\\right)^2\\right)} $$\nFrom the definition of the scaled library, we have $z_2 = \\theta_2 / s_2$. By comparing the two expressions for $z_2$, we identify the scaling factor $s_2$:\n$$ s_2 = L_{\\mathrm{c}}^2 \\cdot \\mathrm{std}\\left(\\left(\\frac{x}{L_{\\mathrm{c}}}\\right)^2\\right) $$\nWe are given the values $L_{\\mathrm{c}} = 2\\,\\mathrm{m}$ and $\\mathrm{std}((x/L_{\\mathrm{c}})^2) = 0.70$. We can now compute the numerical value of $s_2$:\n$$ s_2 = (2\\,\\mathrm{m})^2 \\cdot 0.70 = 4\\,\\mathrm{m}^2 \\cdot 0.70 = 2.8\\,\\mathrm{m}^2 $$\nNow we find the effective threshold $\\lambda_2$. We are given the threshold in the scaled domain as $\\Lambda = 0.15\\,\\mathrm{m/s}$.\n$$ \\lambda_2 = \\frac{\\Lambda}{s_2} = \\frac{0.15\\,\\mathrm{m/s}}{2.8\\,\\mathrm{m}^2} $$\nThe numerical value is:\n$$ \\lambda_2 = \\frac{0.15}{2.8} \\,\\mathrm{s}^{-1}\\,\\mathrm{m}^{-1} \\approx 0.0535714... \\,\\mathrm{s}^{-1}\\,\\mathrm{m}^{-1} $$\nThe units $\\mathrm{s}^{-1}\\,\\mathrm{m}^{-1}$ are consistent with the units of the coefficient $\\xi_2$ in the model equation $\\dot{x} = \\dots + \\xi_2 x^2$, where the units of $\\dot{x}$ are $\\mathrm{m/s}$ and the units of $x^2$ are $\\mathrm{m}^2$.\nRounding the result to four significant figures as required gives $0.05357$.", "answer": "$$\n\\boxed{0.05357}\n$$", "id": "2862870"}]}