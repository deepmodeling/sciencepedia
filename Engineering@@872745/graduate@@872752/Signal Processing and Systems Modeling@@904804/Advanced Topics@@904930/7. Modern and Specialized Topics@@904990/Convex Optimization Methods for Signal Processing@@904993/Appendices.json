{"hands_on_practices": [{"introduction": "The Least Absolute Shrinkage and Selection Operator (LASSO) is a cornerstone of modern signal processing, prized for its ability to recover sparse solutions to linear inverse problems by using an $\\ell_1$-norm penalty. While its formulation is elegant, its practical power comes from efficient algorithms. This exercise guides you through building one of its most popular solvers, cyclic coordinate descent, from first principles. By deriving the update rule from subgradient calculus and implementing it in code, you will bridge the gap between abstract theory and a tangible, high-performance algorithm.", "problem": "You are tasked with deriving and implementing a cyclic coordinate descent algorithm for the Least Absolute Shrinkage and Selection Operator (LASSO) problem using principles from convex optimization and signal processing.\n\nConsider the LASSO objective for a design matrix $A \\in \\mathbb{R}^{m \\times n}$ and observation vector $b \\in \\mathbb{R}^{m}$:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is a given regularization parameter and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm.\n\nYour tasks are:\n1. Derive, from first principles, the single-coordinate minimization update rule used in a cyclic coordinate descent method for $f(x)$. Start from the definition of $f(x)$ and the subgradient optimality condition for the $\\ell_1$ norm, and reason about the minimization of $f(x)$ with respect to a single coordinate $x_i$ while holding all other coordinates fixed. Use only foundational facts including properties of convex functions, subgradients of the absolute value, and basic linear algebra. Do not assume any specific closed-form update a priori.\n2. Show that the coordinate-wise minimizer is obtained by applying the soft-thresholding operator to an affine function of the current iterate and the residual. Clearly define all quantities introduced in your derivation.\n3. Implement a cyclic coordinate descent algorithm that uses the derived update. Your implementation must:\n   - Maintain the residual $r \\triangleq b - A x$ and update it incrementally after each coordinate update to achieve $\\mathcal{O}(m)$ cost per coordinate update.\n   - Use the soft-thresholding operator defined by $S_{\\tau}(z) \\triangleq \\mathrm{sign}(z)\\max(|z| - \\tau, 0)$.\n   - Terminate when either the maximum absolute change in any coordinate during a full cycle is less than a tolerance $\\varepsilon$ or a maximum number of epochs is reached.\n   - Return the final iterate $x$ and, when requested, the sequence of objective values at the end of each epoch to assess monotonicity.\n\nFoundational base you may use:\n- Convexity of $\\|\\cdot\\|_2^2$ and $\\|\\cdot\\|_1$ and properties of their subgradients.\n- Subgradient optimality condition: $0 \\in \\partial f(x^\\star)$ at an optimum $x^\\star$ of a convex function $f$.\n- The subdifferential of the absolute value: for $t \\in \\mathbb{R}$, $\\partial |t| = \\{\\mathrm{sign}(t)\\}$ if $t \\ne 0$, and $\\partial |t| = [-1,1]$ if $t = 0$.\n- Linear algebra identities for residual updates.\n\nDefine the objective value as:\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n\nTest suite:\nImplement your program to run the following five test cases and aggregate the results into a single output line.\n\n- Test 1 (orthonormal columns, analytical check): Set $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, $\\lambda = 0.7$. Run your coordinate descent to obtain $x_{\\mathrm{cd}}$. The known analytical solution for orthonormal columns is $x^\\star = S_{\\lambda}(A^\\top b) = S_{\\lambda}(b)$. Output the scalar\n  $$\n  e_1 \\triangleq \\|x_{\\mathrm{cd}} - x^\\star\\|_{\\infty}.\n  $$\n\n- Test 2 (general tall system, Karush–Kuhn–Tucker (KKT) check): Generate $A \\in \\mathbb{R}^{60 \\times 30}$ with independent standard normal entries and then normalize each column to have unit $\\ell_2$ norm. Use a fixed pseudorandom seed $0$ to make the instance deterministic. Define $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$ with nonzero entries at indices $0,5,10,15,20$ with values $[2.5,-1.7,1.2,-0.9,1.8]$ respectively, and zeros elsewhere. Set $b = A x_{\\mathrm{true}} + \\eta$, where $\\eta \\in \\mathbb{R}^{60}$ has independent normal entries with standard deviation $0.01$ generated with the same seed $0$. Let $\\lambda = 0.05$. Run coordinate descent to obtain $x_{\\mathrm{cd}}$. Verify the KKT conditions for the LASSO: letting $g \\triangleq A^\\top(A x_{\\mathrm{cd}} - b)$,\n  - If $x_{\\mathrm{cd},i} \\ne 0$, then $g_i + \\lambda \\,\\mathrm{sign}(x_{\\mathrm{cd},i}) = 0$.\n  - If $x_{\\mathrm{cd},i} = 0$, then $|g_i| \\le \\lambda$.\n  Because of numerical error, implement a tolerance of $10^{-4}$ in these checks. Output the boolean $b_2$ indicating whether all coordinates satisfy the KKT conditions within tolerance.\n\n- Test 3 (large regularization drives solution to zero): Use $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, and $\\lambda = 10^6$. Output the boolean $b_3$ indicating whether the returned solution is the zero vector within an absolute tolerance of $10^{-12}$.\n\n- Test 4 (zero regularization reduces to least squares): Generate $A \\in \\mathbb{R}^{40 \\times 10}$ with independent standard normal entries using pseudorandom seed $1$. Generate $b \\in \\mathbb{R}^{40}$ with independent standard normal entries using seed $2$. Let $\\lambda = 0$. Let $x_{\\mathrm{ls}}$ denote the least-squares solution minimizing $\\frac{1}{2}\\|A x - b\\|_2^2$, computed by the standard linear least-squares method. Run coordinate descent to obtain $x_{\\mathrm{cd}}$. Output the scalar\n  $$\n  e_4 \\triangleq \\frac{\\|x_{\\mathrm{cd}} - x_{\\mathrm{ls}}\\|_2}{\\max(\\|x_{\\mathrm{ls}}\\|_2, 10^{-12})}.\n  $$\n\n- Test 5 (monotone descent of objective across epochs): Generate $A \\in \\mathbb{R}^{30 \\times 15}$ and $b \\in \\mathbb{R}^{30}$ with independent standard normal entries using pseudorandom seed $3$. Let $\\lambda = 0.1$. Record the objective after each full pass over all coordinates and verify that the sequence is nonincreasing up to numerical tolerance $10^{-10}$. Output the boolean $b_5$ indicating whether monotonicity holds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[e_1, b_2, b_3, e_4, b_5]$. No physical units are involved in this problem, and no angle units are relevant. All numeric outputs should be real numbers or booleans as specified, with no percentage signs. Your implementation must be robust to the given instances and should not require any user input.", "solution": "We start from the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda \\ge 0$. The function $f$ is convex because it is the sum of the convex function $\\frac{1}{2}\\|A x - b\\|_2^2$ and the convex function $\\lambda \\|x\\|_1$.\n\nCyclic coordinate descent minimizes $f$ with respect to one coordinate at a time while holding the others fixed. Fix an index $i \\in \\{1,\\dots,n\\}$ and write $a_i \\in \\mathbb{R}^m$ for the $i$-th column of $A$. Let $x \\in \\mathbb{R}^n$ be the current iterate, and define the residual\n$$\nr \\triangleq b - A x.\n$$\nBecause $A x = \\sum_{j=1}^n a_j x_j$, changing only $x_i$ to a new value $t \\in \\mathbb{R}$ leads to a new vector $x^{(i \\leftarrow t)}$ and residual\n$$\nr^{(i \\leftarrow t)} = b - A x^{(i \\leftarrow t)} = b - \\left(A x + a_i (t - x_i)\\right) = r - a_i (t - x_i).\n$$\nThe objective as a function of $t$ (with other coordinates fixed) becomes\n\\begin{align*}\n\\phi_i(t) &\\triangleq \\frac{1}{2}\\|A x^{(i \\leftarrow t)} - b\\|_2^2 + \\lambda \\left(\\sum_{j \\ne i} |x_j| + |t|\\right) \\\\\n&= \\frac{1}{2}\\|r^{(i \\leftarrow t)}\\|_2^2 + \\lambda |t| + \\text{constant independent of } t \\\\\n&= \\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2 + \\lambda |t| + \\text{constant}.\n\\end{align*}\nExpanding the squared norm using $\\|u - v\\|_2^2 = \\|u\\|_2^2 - 2 u^\\top v + \\|v\\|_2^2$, we get\n\\begin{align*}\n\\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2\n&= \\frac{1}{2}\\|r\\|_2^2 - (t - x_i) a_i^\\top r + \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2.\n\\end{align*}\nDiscarding the terms independent of $t$, the coordinate-wise objective reduces to the univariate convex function\n$$\n\\tilde{\\phi}_i(t) \\triangleq \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t|.\n$$\nCompleting the square, define $d_i \\triangleq \\|a_i\\|_2^2$ and\n$$\nc_i \\triangleq x_i + \\frac{a_i^\\top r}{d_i} \\quad \\text{when } d_i > 0.\n$$\nThen\n\\begin{align*}\n\\tilde{\\phi}_i(t)\n&= \\frac{1}{2} d_i (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t| \\\\\n&= \\frac{1}{2} d_i \\left(t - x_i - \\frac{a_i^\\top r}{d_i}\\right)^2 - \\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i} + \\lambda |t|.\n\\end{align*}\nIgnoring the constant $-\\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i}$, the minimization of $\\tilde{\\phi}_i(t)$ over $t$ is equivalent to minimizing\n$$\n\\psi_i(t) \\triangleq \\frac{1}{2} d_i (t - c_i)^2 + \\lambda |t|.\n$$\nThe subgradient optimality condition for this one-dimensional convex problem is\n$$\n0 \\in \\partial \\psi_i(t^\\star) = d_i (t^\\star - c_i) + \\lambda \\,\\partial |t^\\star|,\n$$\nwhere the subdifferential of the absolute value is $\\partial |t^\\star| = \\{\\mathrm{sign}(t^\\star)\\}$ if $t^\\star \\ne 0$, and $\\partial |t^\\star| = [-1, 1]$ if $t^\\star = 0$.\n\nConsider two cases.\n\nCase 1: $t^\\star \\ne 0$. Then the subgradient condition is\n$$\nd_i (t^\\star - c_i) + \\lambda \\,\\mathrm{sign}(t^\\star) = 0 \\;\\;\\Longleftrightarrow\\;\\; t^\\star = c_i - \\frac{\\lambda}{d_i} \\,\\mathrm{sign}(t^\\star).\n$$\nThis implies $|c_i| > \\lambda/d_i$, and the solution is obtained by shrinking $c_i$ towards zero by $\\lambda/d_i$ while preserving sign:\n$$\nt^\\star = \\mathrm{sign}(c_i)\\left(|c_i| - \\frac{\\lambda}{d_i}\\right).\n$$\n\nCase 2: $t^\\star = 0$. Then the subgradient condition becomes\n$$\n0 \\in - d_i c_i + \\lambda [-1,1] \\;\\;\\Longleftrightarrow\\;\\; |d_i c_i| \\le \\lambda \\;\\;\\Longleftrightarrow\\;\\; |c_i| \\le \\frac{\\lambda}{d_i}.\n$$\nCombining both cases yields the soft-thresholding form\n$$\nt^\\star = S_{\\lambda/d_i}(c_i) \\triangleq \\mathrm{sign}(c_i)\\max\\left(|c_i| - \\frac{\\lambda}{d_i}, \\, 0 \\right).\n$$\nEquivalently, using the residual definition $r = b - A x$, we have\n$$\nc_i = x_i + \\frac{a_i^\\top r}{d_i} = \\frac{a_i^\\top r + d_i x_i}{d_i},\n$$\nso the coordinate-wise minimizer is\n$$\nx_i \\leftarrow S_{\\lambda/\\|a_i\\|_2^2}\\!\\left(\\frac{a_i^\\top r + \\|a_i\\|_2^2 x_i}{\\|a_i\\|_2^2}\\right).\n$$\nIf $d_i = \\|a_i\\|_2^2 = 0$ (a zero column), any change in $x_i$ does not affect the quadratic term; the minimizer of $\\lambda |t|$ is $t^\\star = 0$ for $\\lambda > 0$. In our implementation, we set $x_i \\leftarrow 0$ if $d_i = 0$ and $\\lambda > 0$; if $\\lambda = 0$ and $d_i = 0$, the coordinate is irrelevant and can be left unchanged.\n\nEfficient residual update: If $\\Delta_i \\triangleq x_i^{\\text{new}} - x_i^{\\text{old}}$, then\n$$\nr^{\\text{new}} = b - A x^{\\text{new}} = b - \\left(A x^{\\text{old}} + a_i \\Delta_i\\right) = r^{\\text{old}} - a_i \\Delta_i,\n$$\nwhich costs $\\mathcal{O}(m)$ operations.\n\nConvergence and monotonicity: Each coordinate update exactly minimizes $f$ over that coordinate, so $f$ is nonincreasing after each coordinate update, and hence after each epoch (full pass over all coordinates). The algorithm terminates when the maximum absolute coordinate change in an epoch is below a tolerance or when a maximum number of epochs is reached.\n\nOptimality verification via Karush–Kuhn–Tucker (KKT) conditions: Let $g(x) \\triangleq A^\\top (A x - b)$ be the gradient of the smooth part. The KKT condition for optimality of $x^\\star$ in the LASSO is\n$$\n0 \\in g(x^\\star) + \\lambda \\,\\partial \\|x^\\star\\|_1,\n$$\nwhich is equivalent to the component-wise conditions\n$$\n\\begin{cases}\ng_i(x^\\star) + \\lambda \\,\\mathrm{sign}(x_i^\\star) = 0, & \\text{if } x_i^\\star \\ne 0, \\\\\n|g_i(x^\\star)| \\le \\lambda, & \\text{if } x_i^\\star = 0.\n\\end{cases}\n$$\nIn practice, we check these equalities and inequalities within a small numerical tolerance.\n\nTest cases and outputs: We implement the five test cases specified and compute\n- $e_1 = \\|x_{\\mathrm{cd}} - S_{\\lambda}(b)\\|_\\infty$ for orthonormal columns,\n- $b_2$ indicating KKT satisfaction within tolerance for the tall system,\n- $b_3$ indicating that the solution is zero for very large $\\lambda$,\n- $e_4$ the relative error to the least-squares solution when $\\lambda = 0$,\n- $b_5$ indicating monotone nonincreasing objective values across epochs.\n\nThe final program outputs the results as a single list $[e_1, b_2, b_3, e_4, b_5]$ on one line.", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S_tau(z) = sign(z) * max(|z| - tau, 0).\"\"\"\n    if tau <= 0:\n        return z\n    abs_z = abs(z)\n    if abs_z <= tau:\n        return 0.0\n    return np.copysign(abs_z - tau, z)\n\ndef objective_value(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float) -> float:\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lambd * float(np.linalg.norm(x, 1))\n\ndef coordinate_descent_lasso(\n    A: np.ndarray,\n    b: np.ndarray,\n    lambd: float,\n    max_epochs: int = 2000,\n    tol: float = 1e-8,\n    record_objective: bool = False\n):\n    \"\"\"\n    Cyclic coordinate descent for LASSO:\n        minimize 0.5 * ||A x - b||_2^2 + lambd * ||x||_1.\n    Uses residual updates for O(m) per coordinate.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n, dtype=float)\n    r = b.copy()  # r = b - A x, initially x=0\n    obj_hist = []\n\n    # Precompute squared column norms d_i = ||a_i||_2^2\n    col_sq_norms = np.sum(A * A, axis=0)\n\n    for epoch in range(max_epochs):\n        max_delta = 0.0\n        for i in range(n):\n            ai = A[:, i]\n            di = col_sq_norms[i]\n            xi_old = x[i]\n\n            if di == 0.0:\n                # If the column is zero, the quadratic does not depend on x_i.\n                # Minimizer of lambd * |t| is t=0 for lambd>0; do nothing if lambd==0.\n                xi_new = 0.0 if lambd > 0 else xi_old\n            else:\n                # c_i = x_i + (a_i^T r) / d_i = (a_i^T r + d_i x_i) / d_i\n                ci = xi_old + float(ai.T @ r) / di\n                # Update via soft-thresholding with threshold lambd / d_i\n                xi_new = soft_threshold(ci, lambd / di)\n\n            delta = xi_new - xi_old\n            if delta != 0.0:\n                # Update residual: r_new = r_old - a_i * delta\n                r -= ai * delta\n                x[i] = xi_new\n                max_delta = max(max_delta, abs(delta))\n\n        if record_objective:\n            obj_hist.append(objective_value(A, b, x, lambd))\n\n        if max_delta < tol:\n            break\n\n    if record_objective:\n        return x, obj_hist\n    return x\n\ndef kkt_satisfied(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float, tol: float = 1e-4) -> bool:\n    \"\"\"\n    Check KKT conditions for LASSO:\n      g = A^T (A x - b)\n      If x_i != 0: g_i + lambd * sign(x_i) = 0\n      If x_i == 0: |g_i| <= lambd\n    with numerical tolerance tol.\n    \"\"\"\n    g = A.T @ (A @ x - b)\n    for i in range(x.size):\n        xi = x[i]\n        gi = g[i]\n        if abs(xi) > 1e-12:\n            if abs(gi + lambd * np.sign(xi)) > tol:\n                return False\n        else:\n            if abs(gi) - lambd > tol:\n                return False\n    return True\n\ndef test_suite():\n    results = []\n\n    # Test 1: Orthonormal columns (A = I), analytical solution S_lambda(b)\n    A1 = np.eye(4)\n    b1 = np.array([3.0, -1.0, 0.2, -0.5])\n    lambd1 = 0.7\n    x1 = coordinate_descent_lasso(A1, b1, lambd1, max_epochs=100, tol=1e-12)\n    # Analytical solution: S_lambda(b)\n    x1_star = np.array([soft_threshold(b1[i], lambd1) for i in range(4)])\n    e1 = float(np.max(np.abs(x1 - x1_star)))\n    results.append(e1)\n\n    # Test 2: General tall system, KKT check\n    rng = np.random.default_rng(0)\n    A2 = rng.standard_normal((60, 30))\n    # Normalize columns to unit norm\n    col_norms = np.linalg.norm(A2, axis=0)\n    # Avoid division by zero in extremely unlikely all-zero columns\n    col_norms[col_norms == 0.0] = 1.0\n    A2 = A2 / col_norms\n    x_true = np.zeros(30)\n    nz_idx = [0, 5, 10, 15, 20]\n    nz_vals = [2.5, -1.7, 1.2, -0.9, 1.8]\n    for idx, val in zip(nz_idx, nz_vals):\n        x_true[idx] = val\n    noise = rng.normal(0.0, 0.01, size=60)\n    b2 = A2 @ x_true + noise\n    lambd2 = 0.05\n    x2 = coordinate_descent_lasso(A2, b2, lambd2, max_epochs=2000, tol=1e-10)\n    b2_ok = kkt_satisfied(A2, b2, x2, lambd2, tol=1e-4)\n    results.append(bool(b2_ok))\n\n    # Test 3: Large lambda -> zero solution\n    lambd3 = 1e6\n    x3 = coordinate_descent_lasso(A1, b1, lambd3, max_epochs=50, tol=1e-14)\n    b3_ok = bool(np.allclose(x3, 0.0, atol=1e-12))\n    results.append(b3_ok)\n\n    # Test 4: Zero lambda -> least squares\n    rng1 = np.random.default_rng(1)\n    A4 = rng1.standard_normal((40, 10))\n    rng2 = np.random.default_rng(2)\n    b4 = rng2.standard_normal(40)\n    lambd4 = 0.0\n    x4_cd = coordinate_descent_lasso(A4, b4, lambd4, max_epochs=5000, tol=1e-12)\n    # Least squares solution via numpy\n    x4_ls, *_ = np.linalg.lstsq(A4, b4, rcond=None)\n    denom = max(np.linalg.norm(x4_ls), 1e-12)\n    e4 = float(np.linalg.norm(x4_cd - x4_ls) / denom)\n    results.append(e4)\n\n    # Test 5: Monotone objective decrease across epochs\n    rng3 = np.random.default_rng(3)\n    A5 = rng3.standard_normal((30, 15))\n    b5 = rng3.standard_normal(30)\n    lambd5 = 0.1\n    x5, obj_hist = coordinate_descent_lasso(A5, b5, lambd5, max_epochs=200, tol=1e-10, record_objective=True)\n    # Check nonincreasing sequence within small tolerance\n    diffs = np.diff(obj_hist)\n    b5_ok = bool(np.all(diffs <= 1e-10))\n    results.append(b5_ok)\n\n    return results\n\ndef solve():\n    results = test_suite()\n    # Format booleans and floats in a single list\n    # Convert to string with Python default formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2861565"}, {"introduction": "Many real-world signals exhibit structured sparsity, where non-zero coefficients appear in predefined groups. The group LASSO model captures this structure by penalizing the $\\ell_2$-norm of each group of variables, but solving it requires tools beyond simple soft-thresholding. This practice introduces the proximal operator, a central concept in modern convex optimization, by tasking you with deriving the 'block soft-thresholding' operator. This exercise demonstrates how sparsity-inducing ideas can be generalized to handle more complex, structured regularizers, a key skill for advanced signal modeling.", "problem": "Consider a linear inverse problem in signal processing where one seeks a structured sparse estimate $x \\in \\mathbb{R}^{n}$ partitioned into $G$ nonoverlapping groups $\\{x_{g}\\}_{g=1}^{G}$ according to a fixed index partition of $\\{1,\\dots,n\\}$. Let the regularizer be the weighted group-lasso penalty $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$ with strictly positive weights $w_{g} > 0$. In many first-order convex optimization methods based on proximal splitting, one repeatedly evaluates the proximal operator (prox) of $R$, defined for any $\\lambda > 0$ and any $y \\in \\mathbb{R}^{n}$ by\n$$\n\\operatorname{prox}_{\\lambda R}(y) \\triangleq \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}.\n$$\nStarting from the definition of the proximal operator and the subdifferential optimality condition for convex functions, derive the block soft-thresholding operator associated with $R$, and give its closed-form expression for each group $g \\in \\{1,\\dots,G\\}$ in terms of $y_{g}$, $\\lambda$, and $w_{g}$. Your final answer must be a single closed-form analytic expression for $[\\operatorname{prox}_{\\lambda R}(y)]_{g}$ that is valid for all $y_{g} \\in \\mathbb{R}^{|g|}$ and all $g \\in \\{1,\\dots,G\\}$. Do not report an inequality or an equation to be solved; provide the explicit expression. No numerical rounding is required.", "solution": "The problem statement is parsed and validated. It is found to be scientifically grounded, well-posed, objective, and self-contained. It is a standard problem in convex optimization for signal processing. No flaws are identified. The derivation of the solution can proceed.\n\nThe objective is to find the closed-form expression for the proximal operator of the weighted group-lasso penalty $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$. The proximal operator is defined as the solution $z^*$ to the following minimization problem:\n$$\nz^{*} = \\operatorname{prox}_{\\lambda R}(y) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\nLet the objective function be $F(z) = \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda R(z)$. The squared Euclidean norm can be decomposed over the non-overlapping groups:\n$$\n\\|z - y\\|_{2}^{2} = \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2\n$$\nSubstituting this into the objective function, we obtain:\n$$\nF(z) = \\frac{1}{2} \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2 + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} = \\sum_{g=1}^{G} \\left( \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right)\n$$\nThe total objective function $F(z)$ is a sum of functions, where each term only depends on a single group of variables $z_g$. This property is known as separability. Consequently, the minimization of $F(z)$ over $z \\in \\mathbb{R}^n$ can be performed by minimizing each term of the sum independently over the corresponding group variables $z_g \\in \\mathbb{R}^{|g|}$.\n\nFor each group $g \\in \\{1,\\dots,G\\}$, the solution for the corresponding block, denoted $z_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g$, is given by:\n$$\nz_g^* = \\arg\\min_{z_g \\in \\mathbb{R}^{|g|}} \\left\\{ J(z_g) \\triangleq \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\nThe function $J(z_g)$ is convex, being the sum of a strictly convex function (the quadratic term) and a convex function (the $\\ell_2$-norm term). Thus, a unique minimizer $z_g^*$ exists. The optimality condition for a convex function states that $z_g^*$ is a minimizer if and only if the zero vector is an element of the subdifferential of $J$ at $z_g^*$, i.e., $0 \\in \\partial J(z_g^*)$.\n\nThe function $J(z_g)$ is composed of two terms. The first term, $\\frac{1}{2}\\|z_g - y_g\\|_2^2$, is differentiable, and its gradient is $z_g - y_g$. The second term is $\\lambda w_g \\|z_g\\|_2$. The subdifferential of the $\\ell_2$-norm, $\\|z_g\\|_2$, is:\n$$\n\\partial \\|z_g\\|_2 =\n\\begin{cases}\n\\{ \\frac{z_g}{\\|z_g\\|_2} \\} & \\text{if } z_g \\neq 0 \\\\\n\\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\} & \\text{if } z_g = 0\n\\end{cases}\n$$\nUsing the sum rule for subdifferentials (as one function is differentiable), we have $\\partial J(z_g) = (z_g - y_g) + \\lambda w_g \\partial \\|z_g\\|_2$. The optimality condition $0 \\in \\partial J(z_g^*)$ becomes:\n$$\n0 \\in (z_g^* - y_g) + \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\nThis can be rewritten as:\n$$\ny_g - z_g^* \\in \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\nWe now analyze two cases based on the value of $z_g^*$.\n\nCase 1: $z_g^* \\neq 0$.\nIn this case, the subdifferential $\\partial \\|z_g^*\\|_2$ is the singleton set $\\{ \\frac{z_g^*}{\\|z_g^*\\|_2} \\}$. The optimality condition becomes an equality:\n$$\ny_g - z_g^* = \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2}\n$$\nRearranging the terms to solve for $y_g$:\n$$\ny_g = z_g^* + \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2} = z_g^* \\left( 1 + \\frac{\\lambda w_g}{\\|z_g^*\\|_2} \\right)\n$$\nFrom this equation, we see that $y_g$ is a positive scaling of $z_g^*$. This implies that $z_g^*$ must be collinear with $y_g$ and point in the same direction. Therefore, we have $\\frac{z_g^*}{\\|z_g^*\\|_2} = \\frac{y_g}{\\|y_g\\|_2}$. Note that this requires $y_g \\neq 0$. Substituting this back into the equation for $y_g - z_g^*$:\n$$\nz_g^* = y_g - \\lambda w_g \\frac{y_g}{\\|y_g\\|_2} = \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g\n$$\nSince we assumed $z_g^* \\neq 0$, its norm must be positive, $\\|z_g^*\\|_2 > 0$. Taking the norm of the expression for $z_g^*$:\n$$\n\\|z_g^*\\|_2 = \\left\\| \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g \\right\\|_2 = \\left| 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right| \\|y_g\\|_2\n$$\nAs $z_g^*$ and $y_g$ are in the same direction, the scalar factor must be positive: $1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} > 0$. This implies $\\|y_g\\|_2 > \\lambda w_g$. If this condition holds, our assumption $z_g^* \\neq 0$ is consistent, and the solution is as derived.\n\nCase 2: $z_g^* = 0$.\nIn this case, the subdifferential is the closed unit ball: $\\partial \\|z_g^*\\|_2 = \\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\}$. The optimality condition becomes:\n$$\ny_g - 0 \\in \\lambda w_g \\{ v \\mid \\|v\\|_2 \\le 1 \\}\n$$\nThis is equivalent to stating that $y_g$ is in the ball of radius $\\lambda w_g$:\n$$\n\\|y_g\\|_2 \\le \\lambda w_g\n$$\nIf this condition is met, the minimizer is $z_g^* = 0$.\n\nCombining the two cases, we have:\n$$\nz_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g =\n\\begin{cases}\n\\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g & \\text{if } \\|y_g\\|_2 > \\lambda w_g \\\\\n0 & \\text{if } \\|y_g\\|_2 \\le \\lambda w_g\n\\end{cases}\n$$\nThis piecewise expression can be written compactly as a single formula using the maximum function. The shrinkage factor $\\left(1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right)$ is thresholded at $0$.\n$$\nz_g^* = \\max\\left\\{0, 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right\\} y_g\n$$\nThis expression is the closed-form representation of the block soft-thresholding operator. It is well-defined for all $y_g \\in \\mathbb{R}^{|g|}$. Specifically, if $y_g = 0$, then $\\|y_g\\|_2 = 0$. The term $\\frac{\\lambda w_g}{\\|y_g\\|_2}$ becomes infinite since $\\lambda w_g > 0$. The argument of the max function, $1 - \\infty$, evaluates to $-\\infty$. Thus, $\\max\\{0, -\\infty\\} = 0$, and the result is $z_g^* = 0 \\cdot y_g = 0$, which is consistent with our derivation.\nFor any non-zero $y_g$ such that $\\|y_g\\|_2 \\le \\lambda w_g$, the term $1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}$ is non-positive, so the max function returns $0$, resulting in $z_g^*=0$. For $\\|y_g\\|_2 > \\lambda w_g$, the scaling factor is positive, and the expression yields the correct shrinkage.", "answer": "$$\n\\boxed{\\max\\left\\{0, 1 - \\frac{\\lambda w_{g}}{\\|y_{g}\\|_{2}}\\right\\} y_{g}}\n$$", "id": "2861514"}, {"introduction": "Once an optimization algorithm returns a candidate solution, a critical question remains: how close is it to the true optimum? This practice moves from finding solutions to certifying their quality using the powerful framework of convex duality. You will derive the dual of the LASSO problem and use it to compute the primal-dual gap, providing a rigorous and computable upper bound on the suboptimality of any proposed solution. Mastering this concept provides a crucial tool for designing reliable stopping criteria and analyzing the performance of optimization algorithms.", "problem": "A common sparse linear inverse problem in signal processing is the Least Absolute Shrinkage and Selection Operator (LASSO), which seeks a coefficient vector that balances data fidelity and sparsity. Consider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\,\\|A x - b\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1},\n$$\nwith data matrix\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1,\n$$\nand a candidate primal point\n$$\nx = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nYou are also given a candidate dual vector\n$$\ny = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix},\n$$\nand the infinity norm feasibility condition for dual candidates\n$$\n\\|A^{\\top} y\\|_{\\infty} \\le \\lambda.\n$$\nStarting from the foundational definitions of the Lagrangian, the Lagrangian dual function, and convex conjugacy for indicator and norm functions, perform the following tasks without invoking any pre-memorized final formulas.\n\n1. Derive the dual function $g(y)$ of the LASSO from the definition $g(y) = \\inf_{x,u} \\mathcal{L}(x,u,y)$ after introducing an auxiliary variable $u$ to represent the measurement fit, and identify the corresponding dual feasibility set in terms of $\\|A^{\\top} y\\|_{\\infty}$.\n\n2. Define the primal-dual gap\n$$\nG(x,y) \\triangleq \\left(\\frac{1}{2}\\,\\|A x - b\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1}\\right) - g(y),\n$$\nand explain, from first principles of weak duality, why $G(x,y)$ is an upper bound on the primal suboptimality $\\left(\\frac{1}{2}\\,\\|A x - b\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1}\\right) - \\left(\\frac{1}{2}\\,\\|A x^{\\star} - b\\|_{2}^{2} + \\lambda \\,\\|x^{\\star}\\|_{1}\\right)$, where $x^{\\star}$ is an optimal primal solution.\n\n3. Verify that the provided $y$ satisfies the dual feasibility condition $\\|A^{\\top} y\\|_{\\infty} \\le \\lambda$.\n\n4. Compute the exact value of the primal-dual gap $G(x,y)$ for the given $A$, $b$, $\\lambda$, $x$, and $y$. Provide the exact value as a real number. No rounding is required.", "solution": "The problem presented is a standard instance of the LASSO optimization problem, which is a fundamental and well-posed problem in signal processing and convex optimization. It is scientifically grounded, self-contained, and objective. There are no flaws; therefore, a rigorous solution can be constructed. We will proceed by addressing each of the four specified tasks in sequence.\n\nFirst, we address the derivation of the Lagrangian dual function. The primal problem is\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\,\\|A x - b\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1}.\n$$\nAs instructed, we introduce an auxiliary variable $u \\in \\mathbb{R}^{3}$ such that $u = Ax - b$. The problem can be rewritten as an equivalent constrained optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{2}, u \\in \\mathbb{R}^{3}} \\; \\frac{1}{2}\\,\\|u\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1} \\quad \\text{subject to} \\quad Ax - u - b = 0.\n$$\nThe Lagrangian $\\mathcal{L}(x, u, y)$ for this problem is formed by introducing a dual variable (Lagrange multiplier) $y \\in \\mathbb{R}^{3}$ for the equality constraint:\n$$\n\\mathcal{L}(x, u, y) = \\frac{1}{2}\\,\\|u\\|_{2}^{2} + \\lambda \\,\\|x\\|_{1} + y^{\\top}(Ax - u - b).\n$$\nThe Lagrangian dual function $g(y)$ is defined as the infimum of the Lagrangian over the primal variables $x$ and $u$:\n$$\ng(y) = \\inf_{x, u} \\mathcal{L}(x, u, y).\n$$\nTo compute this infimum, we can rearrange the terms in the Lagrangian to separate the variables $x$ and $u$:\n$$\n\\mathcal{L}(x, u, y) = (\\lambda \\,\\|x\\|_{1} + y^{\\top}Ax) + \\left(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u\\right) - y^{\\top}b.\n$$\nThe infimum can be taken independently for the terms involving $x$ and $u$:\n$$\ng(y) = \\inf_{x} \\left(\\lambda \\,\\|x\\|_{1} + (A^{\\top}y)^{\\top}x\\right) + \\inf_{u} \\left(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u\\right) - y^{\\top}b.\n$$\nThe first term, $\\inf_{x} \\left(\\lambda \\,\\|x\\|_{1} + (A^{\\top}y)^{\\top}x\\right)$, is related to the convex conjugate of the function $f(x) = \\lambda \\|x\\|_{1}$. The conjugate is defined as $f^{*}(z) = \\sup_{x}(z^{\\top}x - f(x))$. The term in our expression is $-\\sup_{x}(- (A^{\\top}y)^{\\top}x - \\lambda \\|x\\|_1) = -f^{*}(-A^{\\top}y)$. The conjugate of $\\lambda \\|x\\|_{1}$ is the indicator function of the dual norm ball, scaled by $\\lambda$. Specifically, $f^{*}(z)$ is $0$ if $\\|z\\|_{\\infty} \\le \\lambda$ and $+\\infty$ otherwise. Therefore, the infimum over $x$ is $0$ if $\\|-A^{\\top}y\\|_{\\infty} \\le \\lambda$, which is equivalent to $\\|A^{\\top}y\\|_{\\infty} \\le \\lambda$, and $-\\infty$ otherwise. For the dual function to be non-trivial (i.e., not $-\\infty$), the dual feasibility condition $\\|A^{\\top}y\\|_{\\infty} \\le \\lambda$ must hold.\n\nThe second term, $\\inf_{u} \\left(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u\\right)$, is the minimum of a convex quadratic function of $u$. The minimum is achieved when the gradient with respect to $u$ is zero: $\\nabla_{u}(\\frac{1}{2}\\,\\|u\\|_{2}^{2} - y^{\\top}u) = u - y = 0$, which implies $u = y$. Substituting this back gives the minimum value: $\\frac{1}{2}\\|y\\|_{2}^{2} - y^{\\top}y = -\\frac{1}{2}\\|y\\|_{2}^{2}$.\n\nCombining these results, the dual function $g(y)$ is:\n$$\ng(y) = \\begin{cases} -b^{\\top}y - \\frac{1}{2}\\|y\\|_{2}^{2} & \\text{if } \\|A^{\\top}y\\|_{\\infty} \\le \\lambda \\\\ -\\infty & \\text{otherwise} \\end{cases}\n$$\nThe dual problem is to maximize $g(y)$, which is equivalent to maximizing $-b^{\\top}y - \\frac{1}{2}\\|y\\|_{2}^{2}$ over the dual feasibility set $\\{y \\in \\mathbb{R}^{3} \\mid \\|A^{\\top}y\\|_{\\infty} \\le \\lambda \\}$.\n\nSecond, we explain the role of the primal-dual gap. Let the primal objective function be $p(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, and let the optimal value of the primal problem be $p^{\\star} = \\inf_{x} p(x) = p(x^{\\star})$, where $x^{\\star}$ is an optimal solution. By the definition of the dual function, for any $y$ and any $x$, we have $g(y) \\le \\mathcal{L}(x, Ax-b, y)$. For a primal feasible pair $(x, u=Ax-b)$, the term $y^{\\top}(Ax - u - b)$ in the Lagrangian is zero. Thus, $\\mathcal{L}(x, Ax-b, y) = p(x)$. This leads to the principle of weak duality: for any $x$ and any $y$, $g(y) \\le p(x)$. Since this holds for any $x$, it must also hold for the optimal solution $x^{\\star}$. Therefore, $g(y) \\le p(x^{\\star}) = p^{\\star}$ for any $y$. The primal suboptimality of a candidate solution $x$ is the non-negative quantity $p(x) - p^{\\star}$. The primal-dual gap is defined as $G(x,y) = p(x) - g(y)$. From the weak duality inequality $g(y) \\le p^{\\star}$, we can write $-p^{\\star} \\le -g(y)$. Adding $p(x)$ to both sides of this inequality yields $p(x) - p^{\\star} \\le p(x) - g(y)$. This demonstrates that the primal suboptimality is bounded above by the primal-dual gap:\n$$\np(x) - p^{\\star} \\le G(x,y).\n$$\nThus, the gap $G(x,y)$ provides a computable upper bound on how far the current primal objective value $p(x)$ is from the true optimal value $p^{\\star}$.\n\nThird, we verify that the given candidate dual vector $y$ is dual feasible. The condition for dual feasibility is $\\|A^{\\top}y\\|_{\\infty} \\le \\lambda$. The given data are:\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1.\n$$\nFirst, we compute the transpose of $A$, which is $A^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$.\nThen, we compute the product $A^{\\top}y$:\n$$\nA^{\\top}y = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(-\\frac{1}{2}) + (0)(\\frac{1}{2}) + (1)(0) \\\\ (0)(-\\frac{1}{2}) + (1)(\\frac{1}{2}) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nNow we compute the infinity norm of this resulting vector:\n$$\n\\|A^{\\top}y\\|_{\\infty} = \\max\\left(\\left|-\\frac{1}{2}\\right|, \\left|\\frac{1}{2}\\right|\\right) = \\frac{1}{2}.\n$$\nComparing this to $\\lambda = 1$, we find that $\\frac{1}{2} \\le 1$. The condition is satisfied, so the given vector $y$ is dual feasible.\n\nFourth, we compute the exact value of the primal-dual gap $G(x,y) = p(x) - g(y)$ for the given primal and dual points.\nThe primal point is $x = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The primal objective value is $p(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$.\nWe first compute the residual term $Ax - b$:\n$$\nAx = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n$$\nAx - b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe squared $\\ell_2$-norm of the residual is $\\|Ax - b\\|_{2}^{2} = 0^{2} + 1^{2} + 1^{2} = 2$.\nThe $\\ell_1$-norm of $x$ is $\\|x\\|_{1} = |1| + |0| = 1$.\nWith $\\lambda = 1$, the primal objective value is:\n$$\np(x) = \\frac{1}{2}(2) + (1)(1) = 1 + 1 = 2.\n$$\nNext, we compute the dual objective value $g(y) = -b^{\\top}y - \\frac{1}{2}\\|y\\|_{2}^{2}$. The required vectors are $b = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$ and $y = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}$.\nThe term $-b^{\\top}y$ is:\n$$\n-b^{\\top}y = - \\left( (1)\\left(-\\frac{1}{2}\\right) + (-1)\\left(\\frac{1}{2}\\right) + (0)(0) \\right) = - \\left(-\\frac{1}{2} - \\frac{1}{2}\\right) = -(-1) = 1.\n$$\nThe term $-\\frac{1}{2}\\|y\\|_{2}^{2}$ is:\n$$\n-\\frac{1}{2}\\|y\\|_{2}^{2} = -\\frac{1}{2} \\left( \\left(-\\frac{1}{2}\\right)^{2} + \\left(\\frac{1}{2}\\right)^{2} + 0^{2} \\right) = -\\frac{1}{2} \\left( \\frac{1}{4} + \\frac{1}{4} \\right) = -\\frac{1}{2} \\left( \\frac{1}{2} \\right) = -\\frac{1}{4}.\n$$\nThe dual objective value is:\n$$\ng(y) = 1 - \\frac{1}{4} = \\frac{3}{4}.\n$$\nFinally, the primal-dual gap $G(x,y)$ is the difference between the primal and dual objective values:\n$$\nG(x,y) = p(x) - g(y) = 2 - \\frac{3}{4} = \\frac{8}{4} - \\frac{3}{4} = \\frac{5}{4}.\n$$\nThe exact value of the primal-dual gap is $\\frac{5}{4}$.", "answer": "$$\n\\boxed{\\frac{5}{4}}\n$$", "id": "2861525"}]}