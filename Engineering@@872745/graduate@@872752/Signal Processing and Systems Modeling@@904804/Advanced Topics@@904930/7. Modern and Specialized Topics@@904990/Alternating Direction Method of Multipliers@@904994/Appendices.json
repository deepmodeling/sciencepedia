{"hands_on_practices": [{"introduction": "To effectively apply the Alternating Direction Method of Multipliers (ADMM), one must first master its core mechanics. This initial practice focuses on deriving a closed-form solution for one of the algorithm's fundamental subproblems. By working through the minimization of the augmented Lagrangian for a simple quadratic function, you will develop a concrete understanding of how the individual update steps are derived, a foundational skill for tackling more complex ADMM applications.", "problem": "The Alternating Direction Method of Multipliers (ADMM) is an algorithm that solves optimization problems of the form:\n$$ \\min_{x, z} f(x) + g(z) $$\n$$ \\text{subject to } Ax + Bz = b $$\nwhere variables are $x \\in \\mathbb{R}^n$ and $z \\in \\mathbb{R}^m$, and the problem data are given by matrices $A \\in \\mathbb{R}^{p \\times n}$, $B \\in \\mathbb{R}^{p \\times m}$, a vector $b \\in \\mathbb{R}^p$, and convex functions $f: \\mathbb{R}^n \\to \\mathbb{R}$ and $g: \\mathbb{R}^m \\to \\mathbb{R}$.\n\nThe algorithm is based on the augmented Lagrangian:\n$$ L_\\rho(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - b) + \\frac{\\rho}{2}\\|Ax + Bz - b\\|_2^2 $$\nwhere $y \\in \\mathbb{R}^p$ is the dual variable (or Lagrange multiplier) and $\\rho > 0$ is a penalty parameter. At each iteration $k$, ADMM performs the following updates sequentially:\n1.  $x^{k+1} := \\arg\\min_x L_\\rho(x, z^k, y^k)$\n2.  $z^{k+1} := \\arg\\min_z L_\\rho(x^{k+1}, z, y^k)$\n3.  $y^{k+1} := y^k + \\rho(Ax^{k+1} + Bz^{k+1} - b)$\n\nConsider a specific instance of this problem where the function $f(x)$ is defined as a quadratic function:\n$$ f(x) = \\frac{1}{2}\\|x - c\\|_2^2 $$\nfor a given constant vector $c \\in \\mathbb{R}^n$.\n\nDerive a closed-form analytical expression for the $x$-update step, $x^{k+1}$. Your expression should be in terms of the problem data $A, B, b, c$, the penalty parameter $\\rho$, and the values from the previous iteration, $z^k$ and $y^k$. In your derivation, let $I$ denote the $n \\times n$ identity matrix and assume the matrix $(I + \\rho A^T A)$ is invertible.", "solution": "We derive the $x$-update by minimizing the augmented Lagrangian with respect to $x$ while holding $z^{k}$ and $y^{k}$ fixed. The $x$-subproblem is\n$$\nx^{k+1} := \\arg\\min_{x}\\left\\{\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{T}(Ax + B z^{k} - b) + \\frac{\\rho}{2}\\|Ax + B z^{k} - b\\|_{2}^{2}\\right\\}.\n$$\nDefine $d := B z^{k} - b$, so the objective becomes\n$$\n\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{T}(A x + d) + \\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}.\n$$\nTerms independent of $x$ do not affect the minimizer, so we focus on the $x$-dependent part. Taking the gradient with respect to $x$ and setting it to zero gives\n$$\n\\nabla_{x}\\left(\\frac{1}{2}\\|x-c\\|_{2}^{2}\\right) + \\nabla_{x}\\left((y^{k})^{T}A x\\right) + \\nabla_{x}\\left(\\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}\\right) = 0,\n$$\nwhich simplifies to\n$$\n(x - c) + A^{T} y^{k} + \\rho A^{T}(A x + d) = 0.\n$$\nCollecting the terms in $x$ yields\n$$\n\\left(I + \\rho A^{T}A\\right) x = c - A^{T} y^{k} - \\rho A^{T} d.\n$$\nSubstituting $d = B z^{k} - b$, we have\n$$\n\\left(I + \\rho A^{T}A\\right) x = c - A^{T} y^{k} - \\rho A^{T}(B z^{k} - b).\n$$\nUnder the assumption that $\\left(I + \\rho A^{T}A\\right)$ is invertible, the unique minimizer is\n$$\nx^{k+1} = \\left(I + \\rho A^{T}A\\right)^{-1}\\left(c - A^{T} y^{k} - \\rho A^{T}(B z^{k} - b)\\right).\n$$", "answer": "$$\\boxed{\\left(I+\\rho A^{T}A\\right)^{-1}\\left(c-A^{T}y^{k}-\\rho A^{T}\\left(Bz^{k}-b\\right)\\right)}$$", "id": "2153727"}, {"introduction": "ADMM's power extends beyond simple objective functions; it elegantly handles complex constraints. This exercise explores how ADMM can be used to solve problems where a variable must belong to a specific convex set, a common requirement in signal processing and machine learning. You will discover that by representing the constraint with an indicator function, the corresponding ADMM update step simplifies to a Euclidean projection, revealing a deep and practical connection between optimization and geometry.", "problem": "Consider a discrete-time signal reconstruction problem in which the estimate $x \\in \\mathbb{R}^{n}$ must satisfy known convex constraints modeling prior knowledge about feasible signals. Formulate the reconstruction as the splitting problem\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} \\; f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0,\n$$\nwhere $f:\\mathbb{R}^{n} \\to \\mathbb{R}$ is a proper, closed, convex function encoding the data fidelity and regularization of the signal model, and $g:\\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ is the indicator of a nonempty, closed, convex set $C \\subset \\mathbb{R}^{n}$, defined by\n$$\ng(z) = \\begin{cases}\n0, & \\text{if } z \\in C,\\\\\n+\\infty, & \\text{if } z \\notin C.\n\\end{cases}\n$$\nThe Alternating Direction Method of Multipliers (ADMM) in its scaled form is to be applied to this problem. Let $\\rho > 0$ be the penalty parameter and $u \\in \\mathbb{R}^{n}$ denote the scaled dual variable. The scaled augmented Lagrangian is\n$$\n\\mathcal{L}_{\\rho}(x,z,u) = f(x) + g(z) + \\frac{\\rho}{2}\\,\\|x - z + u\\|_{2}^{2} - \\frac{\\rho}{2}\\,\\|u\\|_{2}^{2}.\n$$\nThe Euclidean projection of a point $v \\in \\mathbb{R}^{n}$ onto $C$ is defined by\n$$\n\\Pi_{C}(v) := \\arg\\min_{z \\in C} \\;\\|z - v\\|_{2}.\n$$\nStarting from these definitions alone, derive the closed-form expression for the $z$-update in the ADMM iteration,\n$$\nz^{k+1} \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ g(z) + \\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right\\}.\n$$\nExpress your final result explicitly as the Euclidean projection of an affine argument onto $C$. Your final answer must be a single symbolic expression containing only $\\Pi_{C}$, $x^{k+1}$, and $u^{k}$. Do not include any equality sign in the final answer. No numerical approximation is required, and no units are involved.", "solution": "We begin from the problem structure and the scaled augmented Lagrangian. The problem is\n$$\n\\min_{x,z} \\; f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0,\n$$\nwith $g$ as the indicator of a nonempty, closed, convex set $C \\subset \\mathbb{R}^{n}$. In the scaled Alternating Direction Method of Multipliers (ADMM), one alternately minimizes the scaled augmented Lagrangian\n$$\n\\mathcal{L}_{\\rho}(x,z,u) = f(x) + g(z) + \\frac{\\rho}{2}\\,\\|x - z + u\\|_{2}^{2} - \\frac{\\rho}{2}\\,\\|u\\|_{2}^{2}\n$$\nwith respect to $x$ and $z$, and then updates $u$. We focus on the $z$-update, which for fixed $x^{k+1}$ and $u^{k}$ is given by\n$$\nz^{k+1} \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ g(z) + \\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right\\}.\n$$\nBy the definition of the indicator function $g$, minimizing $g(z)$ plus any other function over $z \\in \\mathbb{R}^{n}$ is equivalent to minimizing the other function subject to $z \\in C$. Therefore, the $z$-update reduces to the constrained quadratic minimization\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nSince $\\rho > 0$ is a positive constant, it does not change the location of the minimizer. Thus, equivalently,\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nWe rewrite the squared norm to exhibit it as a projection objective:\n$$\n\\|x^{k+1} - z + u^{k}\\|_{2}^{2} = \\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nTherefore,\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nBy the definition of Euclidean projection onto a nonempty, closed, convex set $C$, the minimizer is the projection of the point $x^{k+1} + u^{k}$ onto $C$. In particular, since $C$ is nonempty, closed, and convex, the projection is uniquely defined, and we have\n$$\nz^{k+1} = \\Pi_{C}\\!\\big(x^{k+1} + u^{k}\\big).\n$$\nThis expresses the $z$-update in closed form as the Euclidean projection of the affine argument $x^{k+1} + u^{k}$ onto the convex set $C$, derived directly from the definitions of the indicator function, the scaled augmented Lagrangian, and Euclidean projection.", "answer": "$$\\boxed{\\Pi_{C}\\!\\left(x^{k+1} + u^{k}\\right)}$$", "id": "2852062"}, {"introduction": "The theoretical elegance of ADMM meets practical reality in the tuning of its penalty parameter, $\\rho$. This advanced exercise bridges the gap between analysis and implementation, asking you to both derive the theoretical convergence rate for a quadratic problem and verify it through code. By comparing a fixed $\\rho$ strategy with a popular adaptive heuristic, you will gain hands-on experience with the critical task of performance tuning and appreciate the interplay between primal and dual residuals that governs the algorithm's speed.", "problem": "Consider the Alternating Direction Method of Multipliers (ADMM) applied to a strongly convex quadratic consensus problem of the form: minimize $f(x) + g(z)$ subject to $x - z = 0$, where $f(x) = \\tfrac{1}{2} x^{\\mathsf{T}} P x$ and $g(z) = \\tfrac{1}{2} z^{\\mathsf{T}} R z$. Assume $P$ and $R$ are symmetric positive definite and diagonal, with strictly positive diagonal entries. Work in the scaled ADMM form with penalty parameter $\\rho > 0$ and scaled dual variable $u$. Use the standard update rules for ADMM on this problem class, and define the primal residual $r^{k} = x^{k} - z^{k}$ and the dual residual $s^{k} = \\rho (z^{k} - z^{k-1})$. Take the initial conditions $z^{0}$ equal to the all-ones vector of appropriate dimension and $u^{0}$ equal to the all-zeros vector; $x^{0}$ is implicitly defined by the algorithmic update. Use the Euclidean norm for all vector norms.\n\nYour task is to derive, implement, and compare fixed $\\rho$ and adaptive $\\rho$ strategies on synthetic diagonal quadratic instances by:\n- Deriving, from first principles, the linear iteration for ADMM on a single scalar coordinate with curvatures $p > 0$ and $r > 0$ and a fixed $\\rho > 0$, and then generalizing to the diagonal multi-dimensional case. From this, compute the predicted asymptotic linear convergence factor for fixed $\\rho$ as the spectral radius of the per-iteration linear map induced on the error dynamics, expressed in terms of $p$, $r$, and $\\rho$.\n- Implementing ADMM with fixed $\\rho$ and estimating the observed asymptotic linear convergence factor from the sequence of residual norms, where a reasonable estimate is the median of ratios $\\|e^{k+1}\\|/\\|e^{k}\\|$ over the last portion of the iterations, with $e^{k}$ defined as the concatenated residual vector containing $r^{k}$ and $s^{k}$.\n- Implementing an adaptive-$\\rho$ heuristic using the residual balancing rule with parameters $\\mu > 1$, $\\tau_{\\mathrm{incr}} > 1$, and $\\tau_{\\mathrm{decr}} \\in (0,1)$: if $\\|r^{k}\\| > \\mu \\|s^{k}\\|$, set $\\rho \\leftarrow \\tau_{\\mathrm{incr}} \\rho$; if $\\|s^{k}\\| > \\mu \\|r^{k}\\|$, set $\\rho \\leftarrow \\tau_{\\mathrm{decr}} \\rho$; otherwise leave $\\rho$ unchanged. When $\\rho$ changes, scale the dual variable to maintain consistency by setting $u \\leftarrow (\\rho_{\\mathrm{old}}/\\rho_{\\mathrm{new}}) u$. For the adaptive run, compute a predicted factor using the final value of $\\rho$ in the same manner as the fixed case, and compute the observed factor analogously to the fixed case.\n\nFundamental base for the derivation includes the definitions of proximal operators for strongly convex quadratics, the scaled form of ADMM, and linearization of the induced iteration for quadratic mappings.\n\nImplement a single program that:\n- For each test case below, runs both fixed-$\\rho$ and adaptive-$\\rho$ ADMM for exactly $K$ iterations, estimates the observed asymptotic factors from the final segment of the iteration, and computes the predicted factors as described.\n- Uses the following constants across all cases: number of iterations $K = 400$, residual-balancing parameters $\\mu = 10$, $\\tau_{\\mathrm{incr}} = 2$, and $\\tau_{\\mathrm{decr}} = 0.5$.\n- Uses the error measure $\\|e^{k}\\| = \\sqrt{ \\|r^{k}\\|^{2} + \\|s^{k}\\|^{2} }$.\n- Estimates the observed factor as the median of $\\|e^{k+1}\\|/\\|e^{k}\\|$ over the last $100$ iterations, excluding ratios where the denominator is $0$.\n- Computes the fixed-$\\rho$ predicted factor for diagonal $P$ and $R$ by taking the maximum over coordinates of the per-coordinate predicted factor derived in your solution.\n\nTest suite:\n- Case $1$: dimension $3$, $P$ diagonal entries $1, 2, 3$, $R$ diagonal entries $1, 2, 4$, fixed $\\rho = 1$.\n- Case $2$: dimension $5$, $P$ diagonal entries $10^{-2}, 10^{-1}, 1, 10, 100$, $R$ diagonal entries $100, 10, 1, 10^{-1}, 10^{-2}$, fixed $\\rho = 1$.\n- Case $3$: dimension $3$, $P$ diagonal entries $0.5, 5, 50$, $R$ diagonal entries $0.2, 2, 200$, fixed $\\rho = 5$.\n- Case $4$: dimension $3$, $P$ diagonal entries $10^{-4}, 2\\cdot 10^{-4}, 5\\cdot 10^{-4}$, $R$ diagonal entries $10^{-4}, 3\\cdot 10^{-4}, 7\\cdot 10^{-4}$, fixed $\\rho = 10^{-3}$.\n\nAngle units are not applicable. Physical units are not involved.\n\nRequired final output format:\n- Your program should produce a single line of output containing a flat list with, for each test case in order, four floating-point numbers in the following order: predicted-fixed, observed-fixed, predicted-adaptive-final, observed-adaptive. That is, the output should be a comma-separated list enclosed in square brackets, containing $4N$ numbers for $N$ test cases, with no additional text. For example, for two cases the format would be $[p_{1},o_{1},p'_{1},o'_{1},p_{2},o_{2},p'_{2},o'_{2}]$ where $p_{i}$ is predicted-fixed, $o_{i}$ is observed-fixed, $p'_{i}$ is predicted-adaptive-final, and $o'_{i}$ is observed-adaptive for case $i$.", "solution": "The problem requires the analysis and implementation of the Alternating Direction Method of Multipliers (ADMM) for a specific strongly convex quadratic consensus problem. The first step, as is mandatory in any rigorous scientific inquiry, is to validate the problem statement.\n\nStep 1: Extract Givens.\nThe problem is to solve:\n$$ \\min_{x,z} f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0 $$\nwhere $f(x) = \\frac{1}{2} x^{\\mathsf{T}} P x$ and $g(z) = \\frac{1}{2} z^{\\mathsf{T}} R z$.\n-   $P$ and $R$ are symmetric, positive definite, and diagonal matrices with strictly positive diagonal entries.\n-   The algorithm is the scaled ADMM with penalty parameter $\\rho > 0$ and scaled dual variable $u$.\n-   Initial conditions: $z^{0}$ is the all-ones vector, $u^{0}$ is the all-zeros vector.\n-   Primal residual: $r^{k} = x^{k} - z^{k}$.\n-   Dual residual: $s^{k} = \\rho (z^{k} - z^{k-1})$.\n-   Error measure: $\\|e^{k}\\| = \\sqrt{ \\|r^{k}\\|^{2} + \\|s^{k}\\|^{2} }$, using the Euclidean norm.\n-   Number of iterations: $K = 400$.\n-   Adaptive $\\rho$ parameters: $\\mu = 10$, $\\tau_{\\mathrm{incr}} = 2$, $\\tau_{\\mathrm{decr}} = 0.5$.\n-   Adaptive $\\rho$ rule: if $\\|r^{k}\\| > \\mu \\|s^{k}\\|$, set $\\rho \\leftarrow \\tau_{\\mathrm{incr}} \\rho$; if $\\|s^{k}\\| > \\mu \\|r^{k}\\|$, set $\\rho \\leftarrow \\tau_{\\mathrm{decr}} \\rho$.\n-   Dual variable scaling: when $\\rho$ changes, $u \\leftarrow (\\rho_{\\mathrm{old}}/\\rho_{\\mathrm{new}}) u$.\n-   Observed factor estimation: median of $\\|e^{k+1}\\|/\\|e^{k}\\|$ over the last $100$ iterations.\n-   The test cases are specified with dimensions, diagonal entries for $P$ and $R$, and initial fixed $\\rho$.\n\nStep 2: Validate Using Extracted Givens.\n-   **Scientifically Grounded**: The problem is rooted in the established theory of convex optimization and numerical methods. ADMM is a standard algorithm, and its application to quadratic programs is a classic topic. The premises are factually sound.\n-   **Well-Posed**: The objective function is strongly convex because $P$ and $R$ are positive definite. The constraint is linear. This guarantees the existence of a unique solution. The algorithmic structure is well-defined.\n-   **Objective**: The problem is stated in precise mathematical language, free of subjective or ambiguous terminology.\n-   The problem is self-contained, with all necessary data and parameters provided for each test case. The constraints and conditions are consistent. The setup is not trivial and requires a proper derivation and implementation.\n\nStep 3: Verdict and Action.\nThe problem is valid. I will now proceed with the derivation and solution.\n\nThe scaled augmented Lagrangian for this problem is:\n$$ L_{\\rho}(x, z, u) = \\frac{1}{2} x^{\\mathsf{T}} P x + \\frac{1}{2} z^{\\mathsf{T}} R z + \\frac{\\rho}{2} \\|x - z + u\\|^2 - \\frac{\\rho}{2} \\|u\\|^2 $$\nThe ADMM iterations consist of three steps:\n1.  $x$-minimization: $x^{k+1} = \\arg\\min_x L_{\\rho}(x, z^k, u^k)$\n2.  $z$-minimization: $z^{k+1} = \\arg\\min_z L_{\\rho}(x^{k+1}, z, u^k)$\n3.  Dual update: $u^{k+1} = u^k + x^{k+1} - z^{k+1}$\n\nWe derive the explicit forms for the update steps.\nFor the $x$-update, we solve the minimization problem:\n$$ x^{k+1} = \\arg\\min_x \\left( \\frac{1}{2} x^{\\mathsf{T}} P x + \\frac{\\rho}{2} \\|x - z^k + u^k\\|^2 \\right) $$\nThe objective is a strictly convex quadratic function of $x$. The unique minimum is found by setting the gradient with respect to $x$ to zero:\n$$ Px + \\rho(x - (z^k - u^k)) = 0 $$\n$$ (P + \\rho I)x = \\rho(z^k - u^k) $$\n$$ x^{k+1} = \\rho (P + \\rho I)^{-1} (z^k - u^k) $$\nFor the $z$-update, we solve:\n$$ z^{k+1} = \\arg\\min_z \\left( \\frac{1}{2} z^{\\mathsf{T}} R z + \\frac{\\rho}{2} \\|x^{k+1} - z + u^k\\|^2 \\right) $$\nSetting the gradient with respect to $z$ to zero:\n$$ Rz - \\rho(x^{k+1} - z + u^k) = 0 $$\n$$ (R + \\rho I)z = \\rho(x^{k+1} + u^k) $$\n$$ z^{k+1} = \\rho (R + \\rho I)^{-1} (x^{k+1} + u^k) $$\n\nSince $P$ and $R$ are diagonal, the matrices $(P + \\rho I)$ and $(R + \\rho I)$ are also diagonal. The problem decouples completely across the coordinates. We can analyze the convergence by examining a single scalar coordinate. Let $p>0$ and $r>0$ be the diagonal entries of $P$ and $R$ for a given coordinate. The scalar updates are:\n1. $x_i^{k+1} = \\frac{\\rho}{p_i+\\rho} (z_i^k - u_i^k)$\n2. $z_i^{k+1} = \\frac{\\rho}{r_i+\\rho} (x_i^{k+1} + u_i^k)$\n3. $u_i^{k+1} = u_i^k + x_i^{k+1} - z_i^{k+1}$\n\nThe unique solution to the problem is $x=z=0$, which implies the optimal dual variable $u$ is also $0$. Thus, the iterates $(z_i^k, u_i^k)$ represent the error from the optimal point $(0,0)$. We can express the evolution of this error as a linear system.\nSubstituting the expression for $x_i^{k+1}$ into the update for $z_i^{k+1}$:\n$$ z_i^{k+1} = \\frac{\\rho}{r_i+\\rho} \\left( \\frac{\\rho}{p_i+\\rho} (z_i^k - u_i^k) + u_i^k \\right) = \\frac{\\rho}{r_i+\\rho} \\left( \\frac{\\rho}{p_i+\\rho} z_i^k + \\left(1 - \\frac{\\rho}{p_i+\\rho}\\right) u_i^k \\right) $$\n$$ z_i^{k+1} = \\frac{\\rho}{(r_i+\\rho)(p_i+\\rho)} (\\rho z_i^k + p_i u_i^k) $$\nNow, substituting the expressions for $x_i^{k+1}$ and $z_i^{k+1}$ into the update for $u_i^{k+1}$:\n$$ u_i^{k+1} = u_i^k + \\frac{\\rho}{p_i+\\rho} (z_i^k - u_i^k) - z_i^{k+1} = \\frac{p_i}{p_i+\\rho} u_i^k + \\frac{\\rho}{p_i+\\rho} z_i^k - z_i^{k+1} $$\n$$ u_i^{k+1} = \\frac{p_i}{p_i+\\rho} u_i^k + \\frac{\\rho}{p_i+\\rho} z_i^k - \\frac{\\rho}{(r_i+\\rho)(p_i+\\rho)} (\\rho z_i^k + p_i u_i^k) $$\n$$ u_i^{k+1} = \\left( \\frac{p_i}{p_i+\\rho} - \\frac{\\rho p_i}{(r_i+\\rho)(p_i+\\rho)} \\right) u_i^k + \\left( \\frac{\\rho}{p_i+\\rho} - \\frac{\\rho^2}{(r_i+\\rho)(p_i+\\rho)} \\right) z_i^k $$\n$$ u_i^{k+1} = \\frac{p_i(r_i+\\rho) - \\rho p_i}{(r_i+\\rho)(p_i+\\rho)} u_i^k + \\frac{\\rho(r_i+\\rho) - \\rho^2}{(r_i+\\rho)(p_i+\\rho)} z_i^k $$\n$$ u_i^{k+1} = \\frac{p_i r_i}{(r_i+\\rho)(p_i+\\rho)} u_i^k + \\frac{\\rho r_i}{(r_i+\\rho)(p_i+\\rho)} z_i^k $$\nThe per-coordinate iteration can be written in matrix form:\n$$\n\\begin{pmatrix} z_i^{k+1} \\\\ u_i^{k+1} \\end{pmatrix} =\n\\frac{1}{(p_i+\\rho)(r_i+\\rho)}\n\\begin{pmatrix} \\rho^2 & \\rho p_i \\\\ \\rho r_i & p_i r_i \\end{pmatrix}\n\\begin{pmatrix} z_i^k \\\\ u_i^k \\end{pmatrix}\n$$\nThe asymptotic linear convergence factor is determined by the spectral radius of this iteration matrix, which we denote as $M_i$. The matrix $M_i$ has rank $1$, as it can be expressed as an outer product:\n$$ M_i = \\frac{1}{(p_i+\\rho)(r_i+\\rho)} \\begin{pmatrix} \\rho \\\\ r_i \\end{pmatrix} \\begin{pmatrix} \\rho & p_i \\end{pmatrix} $$\nThe eigenvalues of a rank-$1$ matrix $ab^{\\mathsf{T}}$ are $b^{\\mathsf{T}}a$ and $0$ with multiplicity $n-1$. Here, $n=2$. The non-zero eigenvalue is:\n$$ \\lambda_i = \\frac{1}{(p_i+\\rho)(r_i+\\rho)} \\begin{pmatrix} \\rho & p_i \\end{pmatrix} \\begin{pmatrix} \\rho \\\\ r_i \\end{pmatrix} = \\frac{\\rho^2 + p_i r_i}{(p_i+\\rho)(r_i+\\rho)} $$\nSince $p_i, r_i, \\rho > 0$, this eigenvalue is positive and less than $1$. The spectral radius is simply this value. The asymptotic convergence factor for the entire system is the maximum of the per-coordinate factors, as the slowest-converging coordinate dictates the overall rate:\n$$ \\kappa_{\\text{predicted}} = \\max_i \\left\\{ \\frac{p_i r_i + \\rho^2}{(p_i+\\rho)(r_i+\\rho)} \\right\\} $$\nThis formula will be used to compute the predicted convergence factors. The observed factor is estimated empirically from the sequence of error norms $\\|e^k\\|$. The adaptive scheme adjusts $\\rho$ to balance the primal and dual residuals, aiming for faster convergence, and its predicted performance is evaluated using the final value of $\\rho$.", "answer": "```python\nimport numpy as np\n\ndef run_admm(p_diag, r_diag, rho_init, n, k_max, adaptive, mu, tau_incr, tau_decr):\n    \"\"\"\n    Runs fixed or adaptive ADMM for the given quadratic problem.\n    \"\"\"\n    # Initial conditions\n    z = np.ones(n)\n    u = np.zeros(n)\n    rho = rho_init\n\n    errors = []\n    \n    # Precompute inverse matrices for speed (they are diagonal)\n    # The solver will update these within the loop if rho is adaptive\n    \n    # Main ADMM loop\n    for k in range(k_max):\n        # Store z_k for dual residual calculation\n        z_prev = z.copy()\n\n        # Precompute denominators for element-wise updates\n        p_plus_rho = p_diag + rho\n        r_plus_rho = r_diag + rho\n\n        # x-update\n        # x_k+1 = rho * (P + rho*I)^-1 * (z_k - u_k)\n        x = (rho / p_plus_rho) * (z - u)\n\n        # z-update\n        # z_k+1 = rho * (R + rho*I)^-1 * (x_k+1 + u_k)\n        z = (rho / r_plus_rho) * (x + u)\n\n        # u-update\n        # u_k+1 = u_k + x_k+1 - z_k+1\n        u = u + x - z\n\n        # Calculate residuals\n        r_k = x - z\n        s_k = rho * (z - z_prev)\n\n        # Calculate error norm\n        norm_r_k = np.linalg.norm(r_k)\n        norm_s_k = np.linalg.norm(s_k)\n        error_norm = np.sqrt(norm_r_k**2 + norm_s_k**2)\n        errors.append(error_norm)\n\n        # Adaptive rho update\n        if adaptive and k  k_max -1: # Don't update on the last iteration\n            rho_old = rho\n            if norm_r_k > mu * norm_s_k:\n                rho = tau_incr * rho\n            elif norm_s_k > mu * norm_r_k:\n                rho = tau_decr * rho\n            \n            if rho != rho_old:\n                # Scale dual variable u\n                u = u * (rho_old / rho)\n\n    return errors, rho\n\ndef calculate_predicted_factor(p_diag, r_diag, rho):\n    \"\"\"\n    Calculates the predicted asymptotic convergence factor.\n    \"\"\"\n    factors = (p_diag * r_diag + rho**2) / ((p_diag + rho) * (r_diag + rho))\n    return np.max(factors)\n\ndef estimate_observed_factor(errors, k_max):\n    \"\"\"\n    Estimates the observed asymptotic convergence factor from error norms.\n    \"\"\"\n    if len(errors)  k_max:\n        return np.nan\n\n    # Use the last 100 iterations for ratios.\n    start_index = k_max - 100\n    \n    ratios = []\n    for i in range(start_index, k_max):\n        if i > 0 and errors[i-1] > 1e-12: # Avoid division by zero\n            ratios.append(errors[i] / errors[i-1])\n\n    if not ratios:\n        return 0.0 # Converged to zero\n        \n    return np.median(ratios)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and format output.\n    \"\"\"\n    # Global constants\n    K_MAX = 400\n    MU = 10.0\n    TAU_INCR = 2.0\n    TAU_DECR = 0.5\n\n    # Test suite\n    test_cases = [\n        {'n': 3, 'p': [1., 2., 3.], 'r': [1., 2., 4.], 'rho': 1.},\n        {'n': 5, 'p': [1e-2, 1e-1, 1., 10., 100.], 'r': [100., 10., 1., 1e-1, 1e-2], 'rho': 1.},\n        {'n': 3, 'p': [0.5, 5., 50.], 'r': [0.2, 2., 200.], 'rho': 5.},\n        {'n': 3, 'p': [1e-4, 2e-4, 5e-4], 'r': [1e-4, 3e-4, 7e-4], 'rho': 1e-3}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        p_diag = np.array(case['p'])\n        r_diag = np.array(case['r'])\n        rho_init = case['rho']\n        n = case['n']\n\n        # --- Fixed rho run ---\n        errors_fixed, _ = run_admm(p_diag, r_diag, rho_init, n, K_MAX, False, MU, TAU_INCR, TAU_DECR)\n        \n        predicted_fixed = calculate_predicted_factor(p_diag, r_diag, rho_init)\n        observed_fixed = estimate_observed_factor(errors_fixed, K_MAX)\n        \n        # --- Adaptive rho run ---\n        errors_adaptive, rho_final = run_admm(p_diag, r_diag, rho_init, n, K_MAX, True, MU, TAU_INCR, TAU_DECR)\n        \n        predicted_adaptive_final = calculate_predicted_factor(p_diag, r_diag, rho_final)\n        observed_adaptive = estimate_observed_factor(errors_adaptive, K_MAX)\n        \n        results.extend([predicted_fixed, observed_fixed, predicted_adaptive_final, observed_adaptive])\n\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2852079"}]}