{"hands_on_practices": [{"introduction": "The most direct way to transform white noise into colored noise is by applying a filter. In this first practice [@problem_id:2916640], we examine one of the simplest yet most illustrative examples: a first-order difference filter. By analyzing the process $v[n] = w[n] - w[n-1]$, you will see firsthand how a finite impulse response (FIR) filter introduces correlation between adjacent samples, turning an uncorrelated white noise source into a colored, predictable process.", "problem": "Consider a discrete-time stochastic process defined by $w[n]$, where $\\{w[n]\\}$ are independent and identically distributed (i.i.d.) Gaussian random variables with zero mean and variance $\\sigma^{2}$. Define a new process by differencing as $v[n] = w[n] - w[n-1]$.\n\nUsing only core definitions from stochastic process theory and linear time-invariant systems, do the following:\n\n1. Determine the marginal distribution of $v[n]$ at any fixed time index $n$.\n2. Determine whether $v[n]$ is wide-sense stationary (WSS). If it is WSS, derive its autocorrelation function $r_{v}[k] = \\mathbb{E}\\{v[n]\\,v[n+k]\\}$ for all integer lags $k$.\n3. From first principles, derive the power spectral density (PSD) $S_{v}(e^{j\\omega})$ of $v[n]$ using its definition as the discrete-time Fourier transform of the autocorrelation function.\n4. Determine whether $v[n]$ is a white process under the standard definition of a white process in discrete time.\n\nProvide clear reasoning grounded in the definitions of independence, Gaussianity, wide-sense stationarity, autocorrelation, and power spectral density. Do not invoke any result without justification from these definitions.\n\nProvide, as your final reported answer, the closed-form analytic expression for the power spectral density $S_{v}(e^{j\\omega})$. No numerical approximation is required and no units are involved. The final answer must be a single closed-form expression.", "solution": "The problem statement will first be validated for scientific soundness, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n- A discrete-time stochastic process is defined by $w[n]$.\n- The random variables $\\{w[n]\\}$ are independent and identically distributed (i.i.d.) Gaussian.\n- The mean of $w[n]$ is zero: $\\mathbb{E}\\{w[n]\\} = 0$.\n- The variance of $w[n]$ is $\\sigma^{2}$: $\\mathbb{E}\\{w[n]^2\\} = \\sigma^{2}$.\n- A new process is defined by $v[n] = w[n] - w[n-1]$.\n- The task requires:\n    1.  Determination of the marginal distribution of $v[n]$.\n    2.  Determination of whether $v[n]$ is wide-sense stationary (WSS) and, if so, derivation of its autocorrelation function $r_{v}[k]$.\n    3.  Derivation of the power spectral density (PSD) $S_{v}(e^{j\\omega})$ from the autocorrelation function.\n    4.  Determination of whether $v[n]$ is a white process.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in the analysis of discrete-time stochastic processes and linear systems.\n- **Scientifically Grounded**: The problem is based on fundamental principles of probability theory and signal processing, specifically the properties of Gaussian processes, i.i.d. sequences (white noise), and linear time-invariant (LTI) filtering. The concepts are standard and mathematically rigorous.\n- **Well-Posed**: The problem is well-defined. The properties of the input process $w[n]$ are fully specified, and the linear operation defining $v[n]$ is explicit. A unique, stable, and meaningful solution exists for all parts of the problem.\n- **Objective**: The problem is stated in precise, objective mathematical language, free of any subjective or ambiguous claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-posed, scientifically sound problem in stochastic process theory. I will proceed with a complete solution.\n\n**Solution**\n\nThe process $w[n]$ is a discrete-time Gaussian white noise process, as it is i.i.d. with zero mean. Its autocorrelation function $r_{w}[k]$ is given by definition as $r_{w}[k] = \\mathbb{E}\\{w[n] w[n+k]\\}$. Due to the i.i.d. property and zero mean, $w[n]$ and $w[n+k]$ are independent for $k \\neq 0$, making $\\mathbb{E}\\{w[n] w[n+k]\\} = \\mathbb{E}\\{w[n]\\}\\mathbb{E}\\{w[n+k]\\} = 0 \\cdot 0 = 0$. For $k=0$, $\\mathbb{E}\\{w[n]^2\\}$ is the variance, $\\sigma^2$. Thus, the autocorrelation of $w[n]$ is $r_{w}[k] = \\sigma^2 \\delta[k]$, where $\\delta[k]$ is the Kronecker delta function.\n\n**1. Marginal Distribution of $v[n]$**\nThe process $v[n]$ is defined as $v[n] = w[n] - w[n-1]$. This is a linear combination of two Gaussian random variables, $w[n]$ and $w[n-1]$. A fundamental property of Gaussian distributions is that any linear combination of Gaussian random variables results in another Gaussian random variable. Therefore, $v[n]$ is a Gaussian random variable for any fixed $n$.\nTo specify its distribution, we must find its mean and variance.\n\nThe mean of $v[n]$ is:\n$$\n\\mathbb{E}\\{v[n]\\} = \\mathbb{E}\\{w[n] - w[n-1]\\}\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}\\{v[n]\\} = \\mathbb{E}\\{w[n]\\} - \\mathbb{E}\\{w[n-1]\\} = 0 - 0 = 0\n$$\nThe mean of $v[n]$ is zero.\n\nThe variance of $v[n]$ is $\\text{Var}(v[n]) = \\mathbb{E}\\{(v[n] - \\mathbb{E}\\{v[n]\\})^2\\} = \\mathbb{E}\\{v[n]^2\\}$, as the mean is zero.\n$$\n\\text{Var}(v[n]) = \\text{Var}(w[n] - w[n-1])\n$$\nBecause $w[n]$ and $w[n-1]$ are independent (since the process $w[n]$ is i.i.d.), the variance of their difference is the sum of their variances:\n$$\n\\text{Var}(v[n]) = \\text{Var}(w[n]) + \\text{Var}(w[n-1]) = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nThus, for any time index $n$, $v[n]$ follows a Gaussian distribution with mean $0$ and variance $2\\sigma^2$. This is denoted as $v[n] \\sim \\mathcal{N}(0, 2\\sigma^2)$.\n\n**2. Wide-Sense Stationarity and Autocorrelation of $v[n]$**\nA process is wide-sense stationary (WSS) if its mean is constant and its autocorrelation function depends only on the time lag $k$.\nThe mean of $v[n]$ is $\\mathbb{E}\\{v[n]\\} = 0$, which is a constant for all $n$. The first condition for WSS is satisfied.\n\nNext, we derive the autocorrelation function $r_{v}[k] = \\mathbb{E}\\{v[n] v[n+k]\\}$.\n$$\nr_{v}[k] = \\mathbb{E}\\{(w[n] - w[n-1])(w[n+k] - w[n+k-1])\\}\n$$\nExpanding the product:\n$$\nr_{v}[k] = \\mathbb{E}\\{w[n]w[n+k] - w[n]w[n+k-1] - w[n-1]w[n+k] + w[n-1]w[n+k-1]\\}\n$$\nBy linearity of expectation:\n$$\nr_{v}[k] = \\mathbb{E}\\{w[n]w[n+k]\\} - \\mathbb{E}\\{w[n]w[n+k-1]\\} - \\mathbb{E}\\{w[n-1]w[n+k]\\} + \\mathbb{E}\\{w[n-1]w[n+k-1]\\}\n$$\nEach term is an autocorrelation of the process $w[n]$ at a specific lag. Using the notation $r_{w}[m] = \\mathbb{E}\\{w[j]w[j+m]\\}$:\n$$\nr_{v}[k] = r_{w}[k] - r_{w}[k-1] - r_{w}[k+1] + r_{w}[k] = 2r_{w}[k] - r_{w}[k-1] - r_{w}[k+1]\n$$\nNow, substitute $r_{w}[m] = \\sigma^2 \\delta[m]$:\n$$\nr_{v}[k] = 2\\sigma^2\\delta[k] - \\sigma^2\\delta[k-1] - \\sigma^2\\delta[k+1]\n$$\nWe evaluate this for different integer values of $k$:\n- For $k=0$: $r_{v}[0] = 2\\sigma^2\\delta[0] - \\sigma^2\\delta[-1] - \\sigma^2\\delta[1] = 2\\sigma^2(1) - \\sigma^2(0) - \\sigma^2(0) = 2\\sigma^2$. This correctly matches the variance of $v[n]$ computed earlier.\n- For $k=1$: $r_{v}[1] = 2\\sigma^2\\delta[1] - \\sigma^2\\delta[0] - \\sigma^2\\delta[2] = 2\\sigma^2(0) - \\sigma^2(1) - \\sigma^2(0) = -\\sigma^2$.\n- For $k=-1$: $r_{v}[-1] = 2\\sigma^2\\delta[-1] - \\sigma^2\\delta[-2] - \\sigma^2\\delta[0] = 2\\sigma^2(0) - \\sigma^2(0) - \\sigma^2(1) = -\\sigma^2$.\n- For $|k| \\geq 2$: All delta functions $\\delta[k]$, $\\delta[k-1]$, and $\\delta[k+1]$ are zero. Thus, $r_{v}[k] = 0$.\n\nThe autocorrelation function is:\n$$\nr_{v}[k] = \\begin{cases} 2\\sigma^2  \\text{if } k=0 \\\\ -\\sigma^2  \\text{if } |k|=1 \\\\ 0  \\text{if } |k|\\geq 2 \\end{cases}\n$$\nSince $r_{v}[k]$ depends only on the lag $k$ and not on the time index $n$, the second condition for WSS is satisfied. Therefore, the process $v[n]$ is wide-sense stationary.\n\n**3. Power Spectral Density of $v[n]$**\nThe power spectral density (PSD) $S_{v}(e^{j\\omega})$ is defined as the discrete-time Fourier transform (DTFT) of the autocorrelation function $r_{v}[k]$.\n$$\nS_{v}(e^{j\\omega}) = \\sum_{k=-\\infty}^{\\infty} r_{v}[k] e^{-j\\omega k}\n$$\nSince $r_{v}[k]$ is non-zero only for $k \\in \\{-1, 0, 1\\}$, the infinite sum reduces to three terms:\n$$\nS_{v}(e^{j\\omega}) = r_{v}[-1]e^{-j\\omega(-1)} + r_{v}[0]e^{-j\\omega(0)} + r_{v}[1]e^{-j\\omega(1)}\n$$\nSubstituting the values of $r_{v}[k]$:\n$$\nS_{v}(e^{j\\omega}) = (-\\sigma^2)e^{j\\omega} + (2\\sigma^2)(1) + (-\\sigma^2)e^{-j\\omega}\n$$\nFactor out $2\\sigma^2$:\n$$\nS_{v}(e^{j\\omega}) = 2\\sigma^2 - \\sigma^2(e^{j\\omega} + e^{-j\\omega})\n$$\nUsing Euler's formula, $\\cos(\\omega) = \\frac{e^{j\\omega} + e^{-j\\omega}}{2}$, which implies $e^{j\\omega} + e^{-j\\omega} = 2\\cos(\\omega)$.\n$$\nS_{v}(e^{j\\omega}) = 2\\sigma^2 - \\sigma^2(2\\cos(\\omega)) = 2\\sigma^2(1 - \\cos(\\omega))\n$$\nThis expression can be further simplified using the half-angle trigonometric identity $1 - \\cos(\\omega) = 2\\sin^2(\\frac{\\omega}{2})$.\n$$\nS_{v}(e^{j\\omega}) = 2\\sigma^2 \\left( 2\\sin^2\\left(\\frac{\\omega}{2}\\right) \\right) = 4\\sigma^2\\sin^2\\left(\\frac{\\omega}{2}\\right)\n$$\nThis is the closed-form expression for the power spectral density of $v[n]$.\n\n**4. White Process Determination**\nA discrete-time stochastic process is defined as white if its samples are uncorrelated. This is equivalent to its autocorrelation function being non-zero only at lag $k=0$. That is, $r_v[k]$ must be of the form $C\\delta[k]$ for some constant $C$.\n\nFrom our derivation in part 2, the autocorrelation function of $v[n]$ is $r_{v}[k] = 2\\sigma^2\\delta[k] - \\sigma^2\\delta[k-1] - \\sigma^2\\delta[k+1]$.\nThis function has non-zero values at lags $k=1$ and $k=-1$, specifically $r_{v}[1] = r_{v}[-1] = -\\sigma^2$. Because the autocorrelation is non-zero for non-zero lags, the samples of $v[n]$ are not uncorrelated. For example, $v[n]$ and $v[n+1]$ are correlated.\n\nAlternatively, a process is white if its power spectral density is a constant for all frequencies, i.e., $S_v(e^{j\\omega}) = \\text{constant}$. From part 3, we found $S_v(e^{j\\omega}) = 4\\sigma^2\\sin^2(\\frac{\\omega}{2})$. This function is dependent on the frequency $\\omega$ and is not constant. It varies from $0$ at $\\omega=0$ to $4\\sigma^2$ at $\\omega=\\pi$.\n\nTherefore, the process $v[n]$ is not a white process. It is a colored noise process.", "answer": "$$\n\\boxed{4\\sigma^{2}\\sin^{2}\\left(\\frac{\\omega}{2}\\right)}\n$$", "id": "2916640"}, {"introduction": "While finite impulse response (FIR) filters introduce local correlations, many real-world processes exhibit dependencies that extend over longer periods. This practice [@problem_id:2916615] investigates the output of a first-order autoregressive (AR) filter, a fundamental model with an infinite impulse response. By deriving the autocorrelation of the output, you will uncover its characteristic exponential decay, a signature of processes with \"memory\".", "problem": "Consider a discrete-time, causal, linear time-invariant (LTI) filter with transfer function $H(z)=\\frac{1}{1-a z^{-1}}$, where $|a|1$. The input $w[n]$ is zero-mean white noise that is wide-sense stationary (WSS), with autocorrelation $R_{w}[k]=\\sigma_{w}^{2}\\,\\delta[k]$, where $\\sigma_{w}^{2}0$ and $\\delta[k]$ is the Kronecker delta. Let the output be $y[n]$. Assume the unique WSS solution exists.\n\nStarting only from the core definitions of autocorrelation for WSS processes, the whiteness property of $w[n]$, and linearity and causality of the system, derive an explicit closed-form expression for the output autocorrelation $R_{y}[k]=\\mathbb{E}\\{y[n]\\,y[n-k]\\}$ for all integer lags $k$. Then, using your expression, verify that $|R_{y}[k]|$ decays exponentially as $|k|\\to\\infty$ and identify the base of this exponential. Express your final result for $R_{y}[k]$ in terms of $a$, $\\sigma_{w}^{2}$, and $k$. No numerical rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It represents a fundamental exercise in the analysis of linear time-invariant systems driven by stochastic processes. All provided information is self-contained and consistent, permitting a unique and stable solution. We will therefore proceed with a rigorous derivation.\n\nThe task is to find the autocorrelation function $R_{y}[k]$ of the output process $y[n]$ of a discrete-time LTI system. The system is defined by its transfer function $H(z) = \\frac{1}{1 - a z^{-1}}$, with the stability condition $|a|  1$. The input $w[n]$ is a zero-mean wide-sense stationary (WSS) white noise process with autocorrelation $R_{w}[k] = \\sigma_{w}^{2} \\delta[k]$.\n\nWe begin from first principles. The output $y[n]$ is given by the convolution of the input $w[n]$ with the system's impulse response $h[n]$:\n$$y[n] = \\sum_{m=-\\infty}^{\\infty} h[m] w[n-m]$$\nFirst, we must determine the impulse response $h[n]$ from the transfer function $H(z)$. Given that the system is causal and $|a|1$, the transfer function $H(z) = \\frac{1}{1 - a z^{-1}}$ corresponds to the one-sided Z-transform of the sequence $h[n] = a^{n} u[n]$, where $u[n]$ is the Heaviside unit step function. The region of convergence is $|z|  |a|$, which includes the unit circle, confirming stability.\n\nSince $h[m] = 0$ for $m  0$, the convolution sum becomes:\n$$y[n] = \\sum_{m=0}^{\\infty} h[m] w[n-m] = \\sum_{m=0}^{\\infty} a^{m} w[n-m]$$\nThe output autocorrelation $R_{y}[k]$ is defined for a WSS process as $R_{y}[k] = \\mathbb{E}\\{y[n] y[n-k]\\}$. Substituting the expression for $y[n]$ and $y[n-k]$:\n$$y[n-k] = \\sum_{l=0}^{\\infty} a^{l} w[n-k-l]$$\n$$R_{y}[k] = \\mathbb{E}\\left\\{ \\left( \\sum_{m=0}^{\\infty} a^{m} w[n-m] \\right) \\left( \\sum_{l=0}^{\\infty} a^{l} w[n-k-l] \\right) \\right\\}$$\nBy linearity of the expectation operator, we can interchange the expectation and the summations:\n$$R_{y}[k] = \\sum_{m=0}^{\\infty} \\sum_{l=0}^{\\infty} a^{m} a^{l} \\mathbb{E}\\{w[n-m] w[n-k-l]\\}$$\nThe expectation term is the autocorrelation of the input process $w[n]$. Since $w[n]$ is WSS, its autocorrelation depends only on the time difference:\n$$\\mathbb{E}\\{w[n-m] w[n-k-l]\\} = R_{w}[(n-m) - (n-k-l)] = R_{w}[k+l-m]$$\nWe are given that the input is white noise, so $R_{w}[\\tau] = \\sigma_{w}^{2} \\delta[\\tau]$, where $\\delta[\\tau]$ is the Kronecker delta function. Therefore:\n$$\\mathbb{E}\\{w[n-m] w[n-k-l]\\} = \\sigma_{w}^{2} \\delta[k+l-m]$$\nSubstituting this back into the expression for $R_{y}[k]$:\n$$R_{y}[k] = \\sum_{m=0}^{\\infty} \\sum_{l=0}^{\\infty} a^{m+l} \\sigma_{w}^{2} \\delta[m - (k+l)]$$\nWe can evaluate the inner sum over $m$ using the sifting property of the Kronecker delta. The term $\\delta[m - (k+l)]$ is non-zero only when $m = k+l$. This substitution collapses the sum over $m$:\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{l=0}^{\\infty} a^{(k+l)+l} \\quad\\text{subject to } m = k+l \\ge 0$$\nThe condition $k+l \\ge 0$ implies $l \\ge -k$. The summation index $l$ must satisfy both $l \\ge 0$ and $l \\ge -k$. This leads to two cases depending on the sign of $k$.\n\nCase 1: $k \\ge 0$.\nIn this case, $-k \\le 0$. The condition $l \\ge -k$ is automatically satisfied for all $l \\ge 0$. Thus, the summation is from $l=0$ to $\\infty$.\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{l=0}^{\\infty} a^{k+2l} = \\sigma_{w}^{2} a^{k} \\sum_{l=0}^{\\infty} (a^{2})^{l}$$\nThis is a geometric series with ratio $a^{2}$. Since $|a|1$, we have $|a^{2}|1$, and the series converges to $\\frac{1}{1-a^{2}}$.\nFor $k \\ge 0$:\n$$R_{y}[k] = \\sigma_{w}^{2} a^{k} \\frac{1}{1-a^{2}}$$\n\nCase 2: $k  0$.\nIn this case, $-k  0$. The condition $l \\ge -k$ is the more restrictive one. Thus, the summation starts from $l=-k$.\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{l=-k}^{\\infty} a^{k+2l}$$\nLet us perform a change of index. Let $j = l+k$. When $l = -k$, $j=0$. The sum becomes:\n$$R_{y}[k] = \\sigma_{w}^{2} \\sum_{j=0}^{\\infty} a^{k+2(j-k)} = \\sigma_{w}^{2} \\sum_{j=0}^{\\infty} a^{2j-k} = \\sigma_{w}^{2} a^{-k} \\sum_{j=0}^{\\infty} (a^{2})^{j}$$\nAgain, the geometric series sums to $\\frac{1}{1-a^{2}}$.\nFor $k  0$:\n$$R_{y}[k] = \\sigma_{w}^{2} a^{-k} \\frac{1}{1-a^{2}}$$\n\nWe can combine these two cases into a single closed-form expression using the absolute value function. For $k \\ge 0$, $|k|=k$. For $k  0$, $|k|=-k$. Both results are captured by:\n$$R_{y}[k] = \\frac{\\sigma_{w}^{2}}{1-a^{2}} a^{|k|}$$\nThis is the explicit closed-form expression for the output autocorrelation for all integer lags $k$.\n\nFinally, we must verify that $|R_{y}[k]|$ decays exponentially as $|k|\\to\\infty$ and identify the base of this exponential. Taking the absolute value of our result:\n$$|R_{y}[k]| = \\left| \\frac{\\sigma_{w}^{2}}{1-a^{2}} a^{|k|} \\right|$$\nSince $\\sigma_{w}^{2}  0$ and $|a|1$ (which implies $1-a^{2}  0$), the term $\\frac{\\sigma_{w}^{2}}{1-a^{2}}$ is a positive constant.\n$$|R_{y}[k]| = \\frac{\\sigma_{w}^{2}}{1-a^{2}} |a^{|k|}| = \\frac{\\sigma_{w}^{2}}{1-a^{2}} |a|^{|k|}$$\nThis expression has the form $C \\cdot b^{|k|}$, where $C = \\frac{\\sigma_{w}^{2}}{1-a^{2}}$ is the constant amplitude and $b = |a|$ is the base. As it is given that $|a|1$, the base $b$ is strictly between $0$ and $1$ (assuming $a \\neq 0$). Consequently, as $|k| \\to \\infty$, the term $|a|^{|k|}$ decays to zero exponentially.\nThe base of this exponential decay is indeed $|a|$. This completes the verification. The problem is solved.", "answer": "$$\n\\boxed{\\frac{\\sigma_{w}^{2}}{1-a^{2}} a^{|k|}}\n$$", "id": "2916615"}, {"introduction": "The principles of noise shaping are not confined to discrete-time models; they are fundamental to the analysis of continuous-time physical systems. This exercise [@problem_id:2916688] applies these concepts to a classic RC low-pass filter, a ubiquitous component in electronics. Here, you will work in the frequency domain to see how the filter's response shapes the flat power spectral density of white noise and calculate the resulting total output noise power, a key metric in circuit design.", "problem": "A continuous-time resistive–capacitive low-pass network is modeled as a linear time-invariant system with frequency response $H(j\\omega)=\\dfrac{1}{1+j\\omega RC}$, where $R$ and $C$ are strictly positive real parameters. The input $x(t)$ is a zero-mean, wide-sense stationary white-noise process whose two-sided power spectral density is constant, $S_{in}(\\omega)=N_0$ for all real $\\omega$. Let $y(t)$ denote the output.\n\nStarting only from foundational definitions that connect wide-sense stationarity, power spectral density, and the frequency-domain input–output relation for linear time-invariant systems, derive an exact closed-form expression for the output noise variance $\\sigma_{out}^{2}=\\mathbb{E}\\{y^{2}(t)\\}$. Express your final answer as a simplified symbolic expression in terms of $N_0$, $R$, and $C$. Do not include units. No numerical rounding is required.", "solution": "The problem as stated is subjected to validation.\n\nGivens are extracted verbatim:\n- System: Continuous-time resistive–capacitive low-pass network.\n- Model: Linear time-invariant (LTI) system.\n- Frequency Response: $H(j\\omega)=\\dfrac{1}{1+j\\omega RC}$.\n- Parameters: $R$ and $C$ are strictly positive real parameters, thus $R0$ and $C0$.\n- Input: $x(t)$ is a zero-mean, wide-sense stationary (WSS) white-noise process.\n- Input Power Spectral Density (PSD): Two-sided, $S_{in}(\\omega)=N_0$ for all real $\\omega$.\n- Output: $y(t)$.\n- Goal: Derive an exact closed-form expression for the output noise variance $\\sigma_{out}^{2}=\\mathbb{E}\\{y^{2}(t)\\}$.\n- Constraint: The derivation must start from foundational definitions.\n\nValidation:\nThe problem is scientifically grounded, describing a standard RC filter subjected to white noise, a canonical problem in signal processing and communications theory. It is well-posed, providing all necessary information: the system's transfer function, the input's statistical properties (zero-mean and PSD), and a clear definition of the quantity to be derived. The problem statement is objective and uses precise, unambiguous terminology. All conditions for a valid problem are met. The problem is therefore deemed valid and a solution will be provided.\n\nThe objective is to compute the variance of the output process, $\\sigma_{out}^{2}$. The variance of a random process $y(t)$ is defined as $\\sigma_{out}^{2}(t) = \\mathbb{E}\\{[y(t) - \\mu_y(t)]^2\\}$, where $\\mu_y(t) = \\mathbb{E}\\{y(t)\\}$ is the mean of the output process.\n\nFirst, we determine the mean of the output process, $\\mu_y(t)$. For a linear time-invariant (LTI) system, the output mean is related to the input mean, $\\mu_x(t) = \\mathbb{E}\\{x(t)\\}$, by the convolution with the system's impulse response, $h(t)$. In the frequency domain, this relationship is simpler: $\\mu_y(t)$ is constant for a WSS input, and its value is given by $\\mu_y = \\mu_x H(j0)$, where $H(j0)$ is the DC gain of the system.\nThe problem states that the input process $x(t)$ is zero-mean, so $\\mu_x = 0$.\nThe DC gain of the system is $H(j0) = \\frac{1}{1+j(0)RC} = 1$.\nTherefore, the output mean is $\\mu_y = (0)(1) = 0$.\n\nSince the output process $y(t)$ is zero-mean, the expression for the variance simplifies to the mean-square value of the output:\n$$\n\\sigma_{out}^{2} = \\mathbb{E}\\{[y(t) - 0]^2\\} = \\mathbb{E}\\{y^2(t)\\}\n$$\nThis confirms the expression given in the problem statement. Because the input is WSS and the system is LTI, the output $y(t)$ is also WSS. Consequently, its variance $\\sigma_{out}^2$ is constant and not a function of time $t$.\n\nThe mean-square value of a WSS process is equal to its autocorrelation function $R_{yy}(\\tau) = \\mathbb{E}\\{y(t+\\tau)y^{*}(t)\\}$ evaluated at $\\tau=0$.\n$$\n\\sigma_{out}^{2} = \\mathbb{E}\\{y^2(t)\\} = R_{yy}(0)\n$$\nThe Wiener-Khinchin theorem establishes the relationship between the autocorrelation function $R_{yy}(\\tau)$ and the power spectral density (PSD) $S_{out}(\\omega)$ of the output process. They form a Fourier transform pair. The inverse Fourier transform relationship is:\n$$\nR_{yy}(\\tau) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_{out}(\\omega) e^{j\\omega\\tau} d\\omega\n$$\nTo find the variance, we evaluate this integral at $\\tau=0$:\n$$\n\\sigma_{out}^{2} = R_{yy}(0) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_{out}(\\omega) e^{j\\omega(0)} d\\omega = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_{out}(\\omega) d\\omega\n$$\nThis shows that the output variance is the total power of the output signal, which is obtained by integrating its two-sided PSD over all frequencies.\n\nThe next step is to determine the output PSD, $S_{out}(\\omega)$. For an LTI system, the output PSD is related to the input PSD, $S_{in}(\\omega)$, by the squared magnitude of the system's frequency response, $|H(j\\omega)|^2$.\n$$\nS_{out}(\\omega) = |H(j\\omega)|^2 S_{in}(\\omega)\n$$\nWe are given $S_{in}(\\omega) = N_0$ and $H(j\\omega) = \\frac{1}{1+j\\omega RC}$. We calculate the squared magnitude of the frequency response:\n$$\n|H(j\\omega)|^2 = \\left| \\frac{1}{1+j\\omega RC} \\right|^2 = \\frac{|1|^2}{|1+j\\omega RC|^2} = \\frac{1}{1^2+(\\omega RC)^2} = \\frac{1}{1+(\\omega RC)^2}\n$$\nSubstituting this and the input PSD into the equation for $S_{out}(\\omega)$ gives:\n$$\nS_{out}(\\omega) = \\left( \\frac{1}{1+(\\omega RC)^2} \\right) N_0 = \\frac{N_0}{1+(\\omega RC)^2}\n$$\nNow, we substitute this expression for $S_{out}(\\omega)$ into the integral for the output variance:\n$$\n\\sigma_{out}^{2} = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{N_0}{1+(\\omega RC)^2} d\\omega\n$$\nSince $N_0$ is a constant, it can be moved outside the integral:\n$$\n\\sigma_{out}^{2} = \\frac{N_0}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{1+(\\omega RC)^2} d\\omega\n$$\nTo solve this integral, we perform a substitution. Let $u = \\omega RC$. Then, the differential is $du = RC d\\omega$, or $d\\omega = \\frac{1}{RC} du$. The limits of integration for $u$ as $\\omega$ goes from $-\\infty$ to $\\infty$ remain from $-\\infty$ to $\\infty$, since $R$ and $C$ are positive constants.\nSubstituting $u$ into the integral:\n$$\n\\sigma_{out}^{2} = \\frac{N_0}{2\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{1+u^2} \\left( \\frac{1}{RC} \\right) du\n$$\nThe term $\\frac{1}{RC}$ is also a constant and can be moved outside the integral:\n$$\n\\sigma_{out}^{2} = \\frac{N_0}{2\\pi RC} \\int_{-\\infty}^{\\infty} \\frac{1}{1+u^2} du\n$$\nThe integral of $\\frac{1}{1+u^2}$ is a standard result:\n$$\n\\int \\frac{1}{1+u^2} du = \\arctan(u) + C_{const}\n$$\nEvaluating the definite integral from $-\\infty$ to $\\infty$:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{1}{1+u^2} du = [\\arctan(u)]_{-\\infty}^{\\infty} = \\lim_{b \\to \\infty} \\arctan(b) - \\lim_{a \\to -\\infty} \\arctan(a) = \\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right) = \\pi\n$$\nFinally, we substitute this result back into the expression for $\\sigma_{out}^{2}$:\n$$\n\\sigma_{out}^{2} = \\frac{N_0}{2\\pi RC} (\\pi) = \\frac{N_0 \\pi}{2\\pi RC}\n$$\nSimplifying the expression by canceling the $\\pi$ terms yields the final answer.\n$$\n\\sigma_{out}^{2} = \\frac{N_0}{2RC}\n$$", "answer": "$$\\boxed{\\frac{N_0}{2RC}}$$", "id": "2916688"}]}