{"hands_on_practices": [{"introduction": "For an ergodic process, the time average is a reliable estimator of the ensemble average. This exercise makes that principle concrete by tasking you with a direct calculation of the estimator's variance for a moving-average (MA) process [@problem_id:2869728]. By deriving the exact variance of the time average $\\overline{x}_{N}$ from first principles, you will explicitly see how it diminishes as the number of samples $N$ increases, solidifying your understanding of convergence.", "problem": "Consider the discrete-time process defined by the finite-support moving-average filter of order $q = 3$ acting on a zero-mean, independent and identically distributed (i.i.d.) innovation sequence. Specifically, let $\\{w[n]\\}$ be zero-mean i.i.d. with variance $\\sigma_{w}^{2} = 2$, and define\n$$\nx[n] \\triangleq \\sum_{k=0}^{3} b_{k}\\, w[n-k], \\quad b_{0} = 1,\\; b_{1} = -2,\\; b_{2} = \\tfrac{1}{2},\\; b_{3} = 3.\n$$\nThe time average over $N$ samples is\n$$\n\\overline{x}_{N} \\triangleq \\frac{1}{N}\\sum_{n=1}^{N} x[n].\n$$\nAssume $N \\geq 4$ so that end effects do not truncate the nonzero lags in the autocovariance summations. Starting only from the core definitions of wide-sense stationarity (WSS) and second-order moments, compute the exact variance $\\operatorname{Var}(\\overline{x}_{N})$ and then derive its asymptotic rate in $N$, identifying the explicit leading constant in the $1/N$ term and the next-order constant. Your derivation must proceed from first principles: use linearity of expectation, the definition of autocovariance for a WSS process, and the independence and variance of the innovations to construct the necessary expressions; do not assume any pre-packaged formulas for the variance of time averages.\n\nExpress your final answer as a single closed-form analytic expression in terms of $N$ only. Do not round your answer.", "solution": "The problem statement has been evaluated and is deemed valid. It is a well-posed problem in the analysis of stochastic processes, grounded in established principles of signal processing and free of scientific or logical flaws. We will now proceed with a complete solution derived from first principles.\n\nThe objective is to compute the variance of the time average, $\\operatorname{Var}(\\overline{x}_{N})$, for the given discrete-time process. The process is defined as\n$$\nx[n] = \\sum_{k=0}^{3} b_{k} w[n-k]\n$$\nwhere $\\{w[n]\\}$ is an independent and identically distributed (i.i.d.) sequence with $E[w[n]] = 0$ and $E[w[n]^2] = \\sigma_{w}^{2} = 2$. The coefficients are $b_{0} = 1$, $b_{1} = -2$, $b_{2} = \\tfrac{1}{2}$, and $b_{3} = 3$.\n\nFirst, we establish that the process $\\{x[n]\\}$ is wide-sense stationary (WSS). We compute its mean and autocovariance.\nThe mean of the process is\n$$\nE[x[n]] = E\\left[\\sum_{k=0}^{3} b_{k} w[n-k]\\right] = \\sum_{k=0}^{3} b_{k} E[w[n-k]]\n$$\nSince $E[w[m]] = 0$ for all $m$, it follows that $E[x[n]] = 0$. The mean is constant for all $n$.\n\nThe autocovariance function is $C_x[m] = E[(x[n] - E[x[n]])(x[n-m] - E[x[n-m]])]$. Since the mean is zero, the autocovariance is identical to the autocorrelation function, $R_x[m] = E[x[n]x[n-m]]$.\n$$\nR_x[m] = E\\left[ \\left(\\sum_{k=0}^{3} b_{k} w[n-k]\\right) \\left(\\sum_{j=0}^{3} b_{j} w[n-m-j]\\right) \\right]\n$$\nBy linearity of expectation,\n$$\nR_x[m] = \\sum_{k=0}^{3} \\sum_{j=0}^{3} b_{k} b_{j} E[w[n-k]w[n-m-j]]\n$$\nThe sequence $\\{w[n]\\}$ is i.i.d. with zero mean, so $E[w[i]w[l]] = \\sigma_{w}^{2} \\delta[i-l]$, where $\\delta[\\cdot]$ is the Kronecker delta. Thus,\n$$\nE[w[n-k]w[n-m-j]] = \\sigma_{w}^{2} \\delta[(n-k) - (n-m-j)] = \\sigma_{w}^{2} \\delta[m - k + j]\n$$\nThe expectation is non-zero only if $j = k-m$. Substituting this into the double summation,\n$$\nR_x[m] = \\sigma_{w}^{2} \\sum_{k=0}^{3} b_{k} b_{k-m}\n$$\nThis expression depends only on the lag $m$, not on $n$. Since the mean is constant and the autocovariance depends only on the time lag, the process $\\{x[n]\\}$ is WSS. The filter length is $4$ (order $q=3$), so the autocovariance $R_x[m]$ is non-zero only for $|m| \\le 3$.\n\nWe now calculate the specific values of $R_x[m]$ for $m \\ge 0$, given $\\sigma_{w}^{2} = 2$ and the coefficients $b_k$.\nFor $m=0$:\n$$\nR_x[0] = \\sigma_{w}^{2} \\sum_{k=0}^{3} b_{k}^{2} = 2 \\left( 1^2 + (-2)^2 + \\left(\\frac{1}{2}\\right)^2 + 3^2 \\right) = 2 \\left( 1 + 4 + \\frac{1}{4} + 9 \\right) = 2 \\left( \\frac{57}{4} \\right) = \\frac{57}{2}\n$$\nFor $m=1$:\n$$\nR_x[1] = \\sigma_{w}^{2} (b_1 b_0 + b_2 b_1 + b_3 b_2) = 2 \\left( (-2)(1) + \\left(\\frac{1}{2}\\right)(-2) + (3)\\left(\\frac{1}{2}\\right) \\right) = 2 \\left( -2 - 1 + \\frac{3}{2} \\right) = 2 \\left( -\\frac{3}{2} \\right) = -3\n$$\nFor $m=2$:\n$$\nR_x[2] = \\sigma_{w}^{2} (b_2 b_0 + b_3 b_1) = 2 \\left( \\left(\\frac{1}{2}\\right)(1) + (3)(-2) \\right) = 2 \\left( \\frac{1}{2} - 6 \\right) = 2 \\left( -\\frac{11}{2} \\right) = -11\n$$\nFor $m=3$:\n$$\nR_x[3] = \\sigma_{w}^{2} (b_3 b_0) = 2 \\left( (3)(1) \\right) = 6\n$$\nFor $|m|  3$, $R_x[m]=0$. The autocovariance function is even, so $R_x[-m] = R_x[m]$.\n\nNext, we compute the variance of the time average $\\overline{x}_{N} = \\frac{1}{N}\\sum_{n=1}^{N} x[n]$.\nThe mean of the time average is $E[\\overline{x}_{N}] = \\frac{1}{N}\\sum_{n=1}^{N} E[x[n]] = 0$.\nTherefore, its variance is\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = E[\\overline{x}_{N}^2] = E\\left[ \\left(\\frac{1}{N}\\sum_{n=1}^{N} x[n]\\right)^2 \\right] = \\frac{1}{N^2} E\\left[ \\left(\\sum_{n=1}^{N} x[n]\\right) \\left(\\sum_{l=1}^{N} x[l]\\right) \\right]\n$$\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2} \\sum_{n=1}^{N} \\sum_{l=1}^{N} E[x[n]x[l]] = \\frac{1}{N^2} \\sum_{n=1}^{N} \\sum_{l=1}^{N} R_x[n-l]\n$$\nTo evaluate the double summation, let $m = n-l$. For a fixed $m$, the number of pairs $(n, l)$ in the summation range $1 \\le n,l \\le N$ such that $n-l = m$ is $N - |m|$. The lag $m$ ranges from $-(N-1)$ to $N-1$.\n$$\n\\sum_{n=1}^{N} \\sum_{l=1}^{N} R_x[n-l] = \\sum_{m=-(N-1)}^{N-1} (N - |m|) R_x[m]\n$$\nThe problem specifies $N \\ge 4$, so $N-1 \\ge 3$. Since $R_x[m]=0$ for $|m|3$, the sum simplifies to\n$$\n\\sum_{m=-3}^{3} (N - |m|) R_x[m]\n$$\nUsing the even property $R_x[-m]=R_x[m]$, we can write this as\n$$\n(N-0)R_x[0] + \\sum_{m=1}^{3} (N-m)R_x[-m] + \\sum_{m=1}^{3} (N-m)R_x[m] = N R_x[0] + 2 \\sum_{m=1}^{3} (N-m)R_x[m]\n$$\nThe sum becomes\n$$\nN R_x[0] + 2(N-1)R_x[1] + 2(N-2)R_x[2] + 2(N-3)R_x[3]\n$$\nSubstituting this back into the expression for the variance:\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2} \\left[ N R_x[0] + 2(N-1)R_x[1] + 2(N-2)R_x[2] + 2(N-3)R_x[3] \\right]\n$$\nNow, we substitute the calculated values of $R_x[m]$:\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2} \\left[ N \\left(\\frac{57}{2}\\right) + 2(N-1)(-3) + 2(N-2)(-11) + 2(N-3)(6) \\right]\n$$\nExpand the terms inside the brackets:\n$$\n\\frac{57}{2}N - 6(N-1) - 22(N-2) + 12(N-3) = \\frac{57}{2}N - 6N + 6 - 22N + 44 + 12N - 36\n$$\nGroup terms by powers of $N$:\n$$\n\\left(\\frac{57}{2} - 6 - 22 + 12\\right)N + (6 + 44 - 36) = \\left(\\frac{57}{2} - 16\\right)N + 14 = \\left(\\frac{57 - 32}{2}\\right)N + 14 = \\frac{25}{2}N + 14\n$$\nTherefore, the exact variance is\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{1}{N^2}\\left(\\frac{25}{2}N + 14\\right) = \\frac{25}{2N} + \\frac{14}{N^2}\n$$\nThis expression provides the exact variance for any $N \\ge 4$. It also explicitly shows the asymptotic behavior. The leading term, which dictates the rate of convergence, is $\\frac{25}{2N}$. The next-order term is $\\frac{14}{N^2}$.\n\nTo present the result as a single closed-form expression, we combine the terms over a common denominator:\n$$\n\\operatorname{Var}(\\overline{x}_{N}) = \\frac{25N}{2N^2} + \\frac{28}{2N^2} = \\frac{25N + 28}{2N^2}\n$$\nThis is the final analytical expression for the variance of the time average.", "answer": "$$\n\\boxed{\\frac{25N + 28}{2N^2}}\n$$", "id": "2869728"}, {"introduction": "A process can be wide-sense stationary (WSS) without being ergodic, a crucial distinction in practice. This problem explores a canonical example of non-ergodicity: a WSS process constructed with a random DC offset [@problem_id:2869742]. Your task is to compute the limit of the time average and show that it converges to a random variable rather than a constant, directly demonstrating the failure of ergodicity in the mean.", "problem": "Consider the following explicit construction. Let the stochastic process $v[n]$ be a finite linear combination of sinusoids with random phases:\n$$\nv[n] \\triangleq 2\\cos\\!\\bigg(\\frac{\\pi}{4} n + \\Theta_1\\bigg) + 3\\cos\\!\\big(\\sqrt{2}\\, n + \\Theta_2\\big) + \\cos\\!\\bigg(\\frac{5\\pi}{7} n + \\Theta_3\\bigg),\n$$\nwhere $\\Theta_1$, $\\Theta_2$, and $\\Theta_3$ are independent and identically distributed, each uniform on $[0,2\\pi)$, and independent of all other sources of randomness. Define a random variable $U$ with distribution $U \\sim \\mathcal{N}(0,\\sigma_U^2)$ for some fixed $\\sigma_U^20$, independent of $(\\Theta_1,\\Theta_2,\\Theta_3)$. Define the process\n$$\nx[n] \\triangleq U + v[n], \\quad n \\in \\mathbb{Z}.\n$$\nStarting only from the definitions of wide-sense stationarity and ergodicity in the mean, and standard properties of trigonometric sums derived from first principles, do the following:\n\n- Justify from definitions that $x[n]$ is wide-sense stationary (that is, its mean is constant and its autocorrelation depends only on the lag).\n- Compute the almost sure limit of the time average\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n].\n$$\n\nYour final answer must be a single, closed-form analytic expression for this limit in terms of the primitive random variable(s) introduced above. No numerical rounding is required. Express the final answer without any units. Use radians for any angles that may appear in intermediate steps.", "solution": "The problem statement must first be validated for scientific and logical integrity.\n\nStep 1: Extract Givens.\n- The stochastic process $v[n]$ is defined as $v[n] \\triangleq 2\\cos(\\frac{\\pi}{4} n + \\Theta_1) + 3\\cos(\\sqrt{2}\\, n + \\Theta_2) + \\cos(\\frac{5\\pi}{7} n + \\Theta_3)$.\n- The random variables $\\Theta_1$, $\\Theta_2$, $\\Theta_3$ are independent and identically distributed (i.i.d.) with a uniform distribution on the interval $[0, 2\\pi)$.\n- A random variable $U$ is defined with a normal distribution $U \\sim \\mathcal{N}(0,\\sigma_U^2)$ for some constant $\\sigma_U^2  0$.\n- The random variable $U$ is independent of the random vector $(\\Theta_1, \\Theta_2, \\Theta_3)$.\n- The stochastic process $x[n]$ is defined as $x[n] \\triangleq U + v[n]$ for all integers $n \\in \\mathbb{Z}$.\n- The task is twofold: first, to justify from formal definitions that $x[n]$ is wide-sense stationary (WSS), and second, to compute the almost sure limit of the time average $\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n]$.\n- The solution must be derived from first principles.\n\nStep 2: Validate Using Extracted Givens.\n- The problem is scientifically grounded. It presents a standard construction of a stochastic process and asks for analysis of its fundamental properties, WSS and ergodicity, which are core concepts in signal processing and probability theory.\n- The problem is well-posed. The definitions of the processes and random variables are explicit and self-consistent. The questions posed have unique, derivable answers based on established mathematical theory.\n- The problem is objective and uses precise mathematical language. There are no subjective or ambiguous statements.\n- The problem is complete. All necessary information regarding the distributions and independence of random variables is provided. No essential data is missing.\n- The problem is not trivial. It requires a careful application of definitions and an understanding of how a random DC offset affects ergodicity, as well as the properties of time averages of sinusoids with both rational and irrational frequencies relative to $\\pi$.\n\nStep 3: Verdict and Action.\nThe problem is valid. A rigorous solution will be constructed.\n\nA discrete-time stochastic process $x[n]$ is defined as wide-sense stationary (WSS) if it satisfies two conditions:\n1.  The mean function $m_x[n] = E[x[n]]$ is a constant, independent of the time index $n$.\n2.  The autocorrelation function $R_{xx}[n_1, n_2] = E[x[n_1]x[n_2]]$ depends only on the time lag $\\tau = n_1 - n_2$.\n\nWe will now verify these two conditions for the process $x[n] = U + v[n]$.\n\nFirst, we compute the mean function $m_x[n]$. By the linearity of the expectation operator:\n$$\nm_x[n] = E[x[n]] = E[U + v[n]] = E[U] + E[v[n]].\n$$\nThe random variable $U$ is given to follow a normal distribution $\\mathcal{N}(0, \\sigma_U^2)$, so its expectation is $E[U] = 0$.\nNext, we compute the expectation of $v[n]$:\n$$\nE[v[n]] = E\\bigg[2\\cos\\bigg(\\frac{\\pi}{4} n + \\Theta_1\\bigg) + 3\\cos\\big(\\sqrt{2}\\, n + \\Theta_2\\big) + \\cos\\bigg(\\frac{5\\pi}{7} n + \\Theta_3\\bigg)\\bigg].\n$$\nUsing the linearity of expectation again:\n$$\nE[v[n]] = 2E\\bigg[\\cos\\bigg(\\frac{\\pi}{4} n + \\Theta_1\\bigg)\\bigg] + 3E\\bigg[\\cos\\big(\\sqrt{2}\\, n + \\Theta_2\\big)\\bigg] + E\\bigg[\\cos\\bigg(\\frac{5\\pi}{7} n + \\Theta_3\\bigg)\\bigg].\n$$\nEach term involves the expectation of a cosine with a random phase $\\Theta_i \\sim U[0, 2\\pi)$. For a generic term with frequency $\\omega$ and phase $\\Theta \\sim U[0, 2\\pi)$, the expectation is calculated by definition:\n$$\nE[\\cos(\\omega n + \\Theta)] = \\int_{0}^{2\\pi} \\cos(\\omega n + \\theta) \\frac{1}{2\\pi} d\\theta = \\frac{1}{2\\pi} \\bigg[\\sin(\\omega n + \\theta)\\bigg]_{\\theta=0}^{\\theta=2\\pi}.\n$$\n$$\n= \\frac{1}{2\\pi} \\big(\\sin(\\omega n + 2\\pi) - \\sin(\\omega n)\\big) = \\frac{1}{2\\pi} \\big(\\sin(\\omega n) - \\sin(\\omega n)\\big) = 0.\n$$\nSince this result is $0$ for any $\\omega$ and $n$, each of the three terms in the expression for $E[v[n]]$ is zero. Thus, $E[v[n]] = 0$ for all $n \\in \\mathbb{Z}$.\nCombining these results, the mean of $x[n]$ is:\n$$\nm_x[n] = E[U] + E[v[n]] = 0 + 0 = 0.\n$$\nThe mean is $0$, a constant, so the first condition for WSS is satisfied.\n\nSecond, we compute the autocorrelation function $R_{xx}[n_1, n_2]$.\n$$\nR_{xx}[n_1, n_2] = E[x[n_1]x[n_2]] = E\\big[(U + v[n_1])(U + v[n_2])\\big].\n$$\nExpanding the product:\n$$\nR_{xx}[n_1, n_2] = E[U^2 + U v[n_2] + v[n_1] U + v[n_1]v[n_2]].\n$$\nBy linearity of expectation:\n$$\nR_{xx}[n_1, n_2] = E[U^2] + E[U v[n_2]] + E[v[n_1]U] + E[v[n_1]v[n_2]].\n$$\nThe random variable $U$ is independent of $(\\Theta_1, \\Theta_2, \\Theta_3)$, and thus independent of $v[n_1]$ and $v[n_2]$. Therefore, the expectation of their products separates:\n$E[U v[n_2]] = E[U]E[v[n_2]] = 0 \\cdot 0 = 0$.\n$E[v[n_1]U] = E[v[n_1]]E[U] = 0 \\cdot 0 = 0$.\nThe term $E[U^2]$ is the second moment of $U$. Since $U \\sim \\mathcal{N}(0, \\sigma_U^2)$, its variance is $\\text{Var}(U) = E[U^2] - (E[U])^2$. Thus, $E[U^2] = \\text{Var}(U) + (E[U])^2 = \\sigma_U^2 + 0^2 = \\sigma_U^2$.\nThe autocorrelation function simplifies to:\n$$\nR_{xx}[n_1, n_2] = \\sigma_U^2 + E[v[n_1]v[n_2]].\n$$\nWe now compute the autocorrelation of $v[n]$, denoted $R_{vv}[n_1, n_2]$. Let $v_k[n] = A_k \\cos(\\omega_k n + \\Theta_k)$ for $k \\in \\{1, 2, 3\\}$, where $(A_1, \\omega_1) = (2, \\frac{\\pi}{4})$, $(A_2, \\omega_2) = (3, \\sqrt{2})$, and $(A_3, \\omega_3) = (1, \\frac{5\\pi}{7})$.\n$$\nR_{vv}[n_1, n_2] = E\\bigg[\\bigg(\\sum_{k=1}^3 v_k[n_1]\\bigg)\\bigg(\\sum_{j=1}^3 v_j[n_2]\\bigg)\\bigg] = \\sum_{k=1}^3 \\sum_{j=1}^3 E[v_k[n_1]v_j[n_2]].\n$$\nThe random phases $\\Theta_k$ are independent for different $k$. For any cross-term where $k \\neq j$:\n$$\nE[v_k[n_1]v_j[n_2]] = E[A_k \\cos(\\omega_k n_1 + \\Theta_k) A_j \\cos(\\omega_j n_2 + \\Theta_j)].\n$$\nDue to independence of $\\Theta_k$ and $\\Theta_j$:\n$$\nE[v_k[n_1]v_j[n_2]] = A_k A_j E[\\cos(\\omega_k n_1 + \\Theta_k)] E[\\cos(\\omega_j n_2 + \\Theta_j)] = A_k A_j \\cdot 0 \\cdot 0 = 0.\n$$\nThus, only the terms with $k=j$ survive:\n$$\nR_{vv}[n_1, n_2] = \\sum_{k=1}^3 E[v_k[n_1]v_k[n_2]] = \\sum_{k=1}^3 E[A_k^2 \\cos(\\omega_k n_1 + \\Theta_k) \\cos(\\omega_k n_2 + \\Theta_k)].\n$$\nWe use the trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}(\\cos(A-B) + \\cos(A+B))$.\n$$\nE[A_k^2 \\cos(\\omega_k n_1 + \\Theta_k) \\cos(\\omega_k n_2 + \\Theta_k)] = \\frac{A_k^2}{2} E[\\cos(\\omega_k(n_1 - n_2)) + \\cos(\\omega_k(n_1+n_2) + 2\\Theta_k)].\n$$\nThe term $\\cos(\\omega_k(n_1 - n_2))$ is deterministic, so its expectation is itself. The expectation of the second term is:\n$$\nE[\\cos(\\omega_k(n_1+n_2) + 2\\Theta_k)] = \\int_0^{2\\pi} \\cos(\\omega_k(n_1+n_2) + 2\\theta) \\frac{1}{2\\pi} d\\theta.\n$$\nThis integral evaluates to $0$ because it is an integral of a sinusoid over two full periods of its argument's phase component ($2\\theta$ spans $[0, 4\\pi)$). Formally, $\\frac{1}{2\\pi}[\\frac{1}{2}\\sin(\\omega_k(n_1+n_2) + 2\\theta)]_0^{2\\pi} = 0$.\nTherefore, for each $k$:\n$$\nE[v_k[n_1]v_k[n_2]] = \\frac{A_k^2}{2} \\cos(\\omega_k(n_1 - n_2)).\n$$\nThis depends only on the lag $\\tau = n_1 - n_2$. Summing over $k$:\n$$\nR_{vv}[n_1, n_2] = \\frac{2^2}{2}\\cos\\bigg(\\frac{\\pi}{4}(n_1-n_2)\\bigg) + \\frac{3^2}{2}\\cos\\big(\\sqrt{2}(n_1-n_2)\\big) + \\frac{1^2}{2}\\cos\\bigg(\\frac{5\\pi}{7}(n_1-n_2)\\bigg).\n$$\nLet $\\tau=n_1-n_2$. Then $R_{vv}(\\tau) = 2\\cos(\\frac{\\pi}{4}\\tau) + \\frac{9}{2}\\cos(\\sqrt{2}\\tau) + \\frac{1}{2}\\cos(\\frac{5\\pi}{7}\\tau)$.\nFinally, the full autocorrelation function for $x[n]$ is:\n$$\nR_{xx}[n_1, n_2] = \\sigma_U^2 + 2\\cos\\bigg(\\frac{\\pi}{4}(n_1-n_2)\\bigg) + \\frac{9}{2}\\cos\\big(\\sqrt{2}(n_1-n_2)\\big) + \\frac{1}{2}\\cos\\bigg(\\frac{5\\pi}{7}(n_1-n_2)\\bigg).\n$$\nThis function depends only on the time lag $\\tau = n_1 - n_2$. Since both conditions are met, the process $x[n]$ is wide-sense stationary.\n\nNow, we proceed to the second part of the problem: computing the almost sure limit of the time average of $x[n]$.\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n] = \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} (U + v[n]).\n$$\nBy the linearity of summation and limits:\n$$\n= \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} U + \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} v[n].\n$$\nFor the first term, $U$ is a random variable that does not depend on the time index $n$. For any particular realization of the process, $U$ is a constant.\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} U = \\lim_{N \\to \\infty} \\frac{N \\cdot U}{N} = \\lim_{N \\to \\infty} U = U.\n$$\nFor the second term, we analyze the time average of $v[n]$ by analyzing each of its sinusoidal components. We need to evaluate the time average of a generic sinusoid $\\cos(\\omega n + \\Theta)$. This is derived from the time average of a complex exponential $e^{j\\omega_0 n}$:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{j\\omega_0 n}.\n$$\nIf $\\omega_0 = 2\\pi k$ for some integer $k$, then $e^{j\\omega_0 n} = e^{j2\\pi k n} = (e^{j2\\pi k})^n = 1^n=1$. The sum is $N$, and the limit of the average is $1$.\nIf $\\omega_0 \\neq 2\\pi k$ for any integer $k$, the sum is a finite geometric series:\n$$\n\\sum_{n=0}^{N-1} (e^{j\\omega_0})^n = \\frac{1 - (e^{j\\omega_0})^N}{1 - e^{j\\omega_0}}.\n$$\nThe magnitude of the numerator is bounded: $|1 - e^{j\\omega_0 N}| \\leq |1| + |e^{j\\omega_0 N}| = 1+1=2$. The denominator $1 - e^{j\\omega_0}$ is a non-zero constant. Thus, the sum is bounded in magnitude. The time average is this bounded quantity divided by $N$, which converges to $0$ as $N \\to \\infty$.\nIn summary, $\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{j\\omega_0 n} = \\delta(\\omega_0 \\text{ mod } 2\\pi)$, where $\\delta$ is $1$ if the argument is $0$ and $0$ otherwise.\n\nThe time average of $\\cos(\\omega n + \\Theta)$ is:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} \\cos(\\omega n + \\Theta) = \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} \\frac{e^{j(\\omega n + \\Theta)} + e^{-j(\\omega n + \\Theta)}}{2}\n$$\n$$\n= \\frac{e^{j\\Theta}}{2} \\bigg(\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{j\\omega n}\\bigg) + \\frac{e^{-j\\Theta}}{2} \\bigg(\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} e^{-j\\omega n}\\bigg).\n$$\nThis limit is non-zero only if $\\omega = 2\\pi k$ or $-\\omega = 2\\pi k$ for some integer $k$. In other words, $\\omega$ must be an integer multiple of $2\\pi$.\nLet us examine the frequencies in $v[n]$:\n1.  $\\omega_1 = \\frac{\\pi}{4}$. This is not an integer multiple of $2\\pi$. Thus, the time average of $2\\cos(\\frac{\\pi}{4} n + \\Theta_1)$ is $0$.\n2.  $\\omega_2 = \\sqrt{2}$. As $\\sqrt{2}$ is irrational, $\\sqrt{2}/(2\\pi)$ is also irrational, so $\\sqrt{2}$ cannot be an integer multiple of $2\\pi$. Thus, the time average of $3\\cos(\\sqrt{2} n + \\Theta_2)$ is $0$.\n3.  $\\omega_3 = \\frac{5\\pi}{7}$. This is not an integer multiple of $2\\pi$. Thus, the time average of $\\cos(\\frac{5\\pi}{7} n + \\Theta_3)$ is $0$.\n\nSince the time average of each sinusoidal component of $v[n]$ is $0$, the time average of their sum is also $0$:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} v[n] = 0.\n$$\nCombining the results for the two parts of $x[n]$:\n$$\n\\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{n=0}^{N-1} x[n] = U + 0 = U.\n$$\nThe almost sure limit of the time average is the random variable $U$. This implies that the process $x[n]$ is not ergodic in the mean, because its time average converges to a random variable $U$, which is almost surely not equal to the constant ensemble mean $m_x=0$.", "answer": "$$\n\\boxed{U}\n$$", "id": "2869742"}, {"introduction": "The behavior of a stochastic process in the time domain is deeply connected to its properties in the frequency domain, as described by the Wiener-Khinchin theorem. This exercise bridges this gap by examining a process whose power spectral density (PSD) contains a Dirac delta function at zero frequency, $S_x(\\omega) = 2\\pi\\sigma_0^2\\delta(\\omega) + S_c(\\omega)$ [@problem_id:2869754]. You will demonstrate that this spectral feature is the signature of a non-ergodic component, leading to a non-vanishing variance in the time average as $T \\to \\infty$.", "problem": "Consider a real, zero-mean, wide-sense stationary (WSS) process with power spectral density (PSD) given by\n$$\nS_{x}(\\omega)=2\\pi\\sigma_{0}^{2}\\,\\delta(\\omega)+S_{c}(\\omega),\n$$\nwhere $S_{c}(\\omega)$ is an even, nonnegative function that is integrable on $\\mathbb{R}$, continuous at $\\omega=0$, and satisfies $\\int_{-\\infty}^{\\infty} S_{c}(\\omega)\\,d\\omega\\infty$. \n\n1. Construct explicitly, in the time domain, a zero-mean WSS process $x(t)$ whose PSD is $S_{x}(\\omega)$ in the sense of the Wiener–Khinchin theorem. Your construction must start from first principles by combining basic building blocks that have well-defined autocorrelation functions and PSDs.\n\n2. For the time average\n$$\n\\overline{x}_{T}=\\frac{1}{T}\\int_{0}^{T} x(t)\\,dt,\n$$\ncompute the non-vanishing limit of $\\mathrm{var}(\\overline{x}_{T})$ as $T\\to\\infty$ using only foundational results connecting linear time-invariant filtering, autocorrelation functions, and power spectral densities. Give your final answer as a single, closed-form analytic expression in terms of $\\sigma_{0}^{2}$. No numerical approximation is required. Express your final answer without units.", "solution": "The problem statement is subjected to critical validation.\n\n**Step 1: Extract Givens**\n- Process $x(t)$ is a real, zero-mean, wide-sense stationary (WSS) random process.\n- The Power Spectral Density (PSD) is $S_{x}(\\omega)=2\\pi\\sigma_{0}^{2}\\,\\delta(\\omega)+S_{c}(\\omega)$.\n- $S_{c}(\\omega)$ is an even, nonnegative function.\n- $S_{c}(\\omega)$ is integrable on $\\mathbb{R}$.\n- $S_{c}(\\omega)$ is continuous at $\\omega=0$.\n- $\\int_{-\\infty}^{\\infty} S_{c}(\\omega)\\,d\\omega\\infty$.\n- The time average is defined as $\\overline{x}_{T}=\\frac{1}{T}\\int_{0}^{T} x(t)\\,dt$.\n- The objective is to compute the limit $\\lim_{T\\to\\infty} \\mathrm{var}(\\overline{x}_{T})$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined within the mathematical framework of stochastic processes. The provided PSD, consisting of a Dirac delta function at the origin and a continuous component, is a standard model for a WSS process that is not ergodic in the mean. The properties ascribed to $S_c(\\omega)$ are consistent with it being a valid PSD component. The process is stipulated to be zero-mean, $E[x(t)]=0$. This requirement might appear to contradict the presence of a delta function at $\\omega=0$ in the PSD, which corresponds to a non-zero mean square value of the time average. However, a process can be constructed as $x(t) = A + c(t)$, where $A$ is a random variable with $E[A]=0$ and $E[A^2]=\\sigma_0^2$, and $c(t)$ is a zero-mean WSS process. Such a process $x(t)$ is indeed zero-mean, $E[x(t)]=E[A]+E[c(t)]=0$, and its autocorrelation and PSD match the given forms. Thus, there is no internal contradiction. The problem is scientifically grounded, objective, complete, and well-posed.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A complete solution will be developed.\n\n### Part 1: Construction of the Process\nA process $x(t)$ with the specified PSD can be constructed as the sum of two independent, real, zero-mean WSS processes. Let $x(t) = A(t) + c(t)$, where:\n1.  $A(t)$ is a process whose PSD is $S_A(\\omega) = 2\\pi\\sigma_{0}^{2}\\delta(\\omega)$.\n2.  $c(t)$ is a process whose PSD is $S_c(\\omega)$.\n\nLet us construct these building blocks.\nFor the process $c(t)$, its PSD $S_c(\\omega)$ is given to be even, non-negative, and integrable. By Wiener's theorem on the characterization of autocorrelation functions, such a function is a valid PSD for a zero-mean WSS process $c(t)$. The property that $S_c(\\omega)$ is continuous at $\\omega=0$ is crucial, as will be shown.\n\nFor the process $A(t)$, its autocorrelation function $R_A(\\tau)$ is the inverse Fourier transform of its PSD:\n$$R_A(\\tau) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} S_A(\\omega)e^{i\\omega\\tau}d\\omega = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} 2\\pi\\sigma_{0}^{2}\\delta(\\omega)e^{i\\omega\\tau}d\\omega = \\sigma_{0}^{2}$$\nThis means $R_A(\\tau) = E[A(t+\\tau)A(t)] = \\sigma_{0}^{2}$ for all $t$ and $\\tau$. This is characteristic of a random constant process, i.e., $A(t) = A$, where $A$ is a random variable. The autocorrelation is then $E[A^2] = \\sigma_{0}^{2}$. To satisfy the zero-mean condition for $x(t)$, we must have $E[x(t)]=0$. If we assume $A$ and $c(t)$ are independent, then $E[x(t)] = E[A + c(t)] = E[A] + E[c(t)]$. Since $c(t)$ corresponds to the continuous part of the spectrum with no delta at the origin, we can define it to be zero-mean, $E[c(t)]=0$. This forces $E[A]=0$.\nTherefore, the first component is a random variable $A$ with mean $E[A]=0$ and variance $\\mathrm{var}(A) = E[A^2] - (E[A])^2 = \\sigma_0^2 - 0 = \\sigma_0^2$.\n\nThus, the explicit construction for $x(t)$ is:\n$$x(t) = A + c(t)$$\nwhere $A$ is a random variable, independent of the process $c(t)$, with $E[A]=0$ and $E[A^2]=\\sigma_0^2$, and $c(t)$ is a zero-mean WSS process with PSD $S_c(\\omega)$.\n\nLet us verify that this construction yields the specified properties for $x(t)$:\n- **Mean**: $E[x(t)] = E[A+c(t)] = E[A] + E[c(t)] = 0 + 0 = 0$.\n- **Autocorrelation**: Since $A$ and $c(t)$ are independent and both are zero-mean:\n$$R_x(\\tau) = E[x(t+\\tau)x(t)] = E[(A+c(t+\\tau))(A+c(t))]$$\n$$R_x(\\tau) = E[A^2] + E[A]E[c(t)] + E[A]E[c(t+\\tau)] + E[c(t+\\tau)c(t)]$$\n$$R_x(\\tau) = \\sigma_0^2 + 0 + 0 + R_c(\\tau) = \\sigma_0^2 + R_c(\\tau)$$\nThe process is WSS.\n- **PSD**: The PSD is the Fourier transform of the autocorrelation function:\n$$S_x(\\omega) = \\mathcal{F}\\{R_x(\\tau)\\} = \\mathcal{F}\\{\\sigma_0^2\\} + \\mathcal{F}\\{R_c(\\tau)\\} = 2\\pi\\sigma_0^2\\delta(\\omega) + S_c(\\omega)$$\nThe construction is validated.\n\n### Part 2: Computation of the Limit of the Variance\nThe time average of the process $x(t)$ is given by $\\overline{x}_{T}=\\frac{1}{T}\\int_{0}^{T} x(t)\\,dt$.\nUsing the constructed form of $x(t)$:\n$$\\overline{x}_{T} = \\frac{1}{T}\\int_{0}^{T} (A + c(t))\\,dt = \\frac{1}{T}\\int_{0}^{T} A\\,dt + \\frac{1}{T}\\int_{0}^{T} c(t)\\,dt = A + \\overline{c}_{T}$$\nwhere $\\overline{c}_{T}$ is the time average of the process $c(t)$.\n\nThe variance of $\\overline{x}_T$ is $\\mathrm{var}(\\overline{x}_{T}) = E[\\overline{x}_{T}^2] - (E[\\overline{x}_{T}])^2$.\nFirst, we compute the mean of $\\overline{x}_{T}$:\n$$E[\\overline{x}_{T}] = E[A + \\overline{c}_{T}] = E[A] + E[\\overline{c}_{T}]$$\nWe know $E[A]=0$. For $\\overline{c}_T$:\n$$E[\\overline{c}_{T}] = E\\left[\\frac{1}{T}\\int_{0}^{T} c(t)\\,dt\\right] = \\frac{1}{T}\\int_{0}^{T} E[c(t)]\\,dt = \\frac{1}{T}\\int_{0}^{T} 0\\,dt = 0$$\nThus, $E[\\overline{x}_{T}]=0$, and the variance is $\\mathrm{var}(\\overline{x}_{T}) = E[\\overline{x}_{T}^2]$.\n$$E[\\overline{x}_{T}^2] = E[(A+\\overline{c}_{T})^2] = E[A^2 + 2A\\overline{c}_{T} + \\overline{c}_{T}^2] = E[A^2] + 2E[A\\overline{c}_{T}] + E[\\overline{c}_{T}^2]$$\nWe have $E[A^2] = \\sigma_0^2$.\nThe cross-term is:\n$$E[A\\overline{c}_{T}] = E\\left[A \\cdot \\frac{1}{T}\\int_{0}^{T} c(t)\\,dt\\right] = \\frac{1}{T}\\int_{0}^{T} E[A c(t)]\\,dt$$\nSince $A$ and $c(t)$ are independent and both are zero-mean, $E[A c(t)] = E[A]E[c(t)] = 0 \\cdot 0 = 0$. So, $E[A\\overline{c}_{T}]=0$.\nThe last term is $E[\\overline{c}_{T}^2] = \\mathrm{var}(\\overline{c}_{T})$ since $E[\\overline{c}_{T}]=0$.\nPutting everything together:\n$$\\mathrm{var}(\\overline{x}_{T}) = \\sigma_0^2 + \\mathrm{var}(\\overline{c}_{T})$$\nNow, we must compute the limit as $T\\to\\infty$:\n$$\\lim_{T\\to\\infty} \\mathrm{var}(\\overline{x}_{T}) = \\sigma_0^2 + \\lim_{T\\to\\infty} \\mathrm{var}(\\overline{c}_{T})$$\nThe term $\\lim_{T\\to\\infty} \\mathrm{var}(\\overline{c}_{T})$ represents the condition for the process $c(t)$ to be ergodic in the mean. A zero-mean WSS process is ergodic in the mean if and only if the limit of the variance of its time average is zero. A necessary and sufficient condition for this is that its PSD has no Dirac delta component at $\\omega=0$.\n\nThe variance of the time average $\\overline{c}_{T}$ can be expressed in terms of the PSD $S_c(\\omega)$ as:\n$$\\mathrm{var}(\\overline{c}_T) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_c(\\omega) \\left| H_T(\\omega) \\right|^2 d\\omega$$\nwhere $H_T(\\omega)$ is the Fourier transform of the impulse response $h_T(t) = \\frac{1}{T}$ for $t \\in [0, T]$ and $0$ otherwise.\n$$H_T(\\omega) = \\int_0^T \\frac{1}{T} e^{-i\\omega t} dt = \\frac{1-e^{-i\\omega T}}{i\\omega T}$$\nIts squared magnitude is:\n$$\\left| H_T(\\omega) \\right|^2 = \\frac{|1 - \\cos(\\omega T) + i\\sin(\\omega T)|^2}{(\\omega T)^2} = \\frac{(1-\\cos(\\omega T))^2 + \\sin^2(\\omega T)}{(\\omega T)^2} = \\frac{2(1-\\cos(\\omega T))}{(\\omega T)^2} = \\frac{4\\sin^2(\\omega T/2)}{(\\omega T)^2} = \\left(\\frac{\\sin(\\omega T/2)}{\\omega T/2}\\right)^2$$\nSo,\n$$\\mathrm{var}(\\overline{c}_T) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_c(\\omega) \\left(\\frac{\\sin(\\omega T/2)}{\\omega T/2}\\right)^2 d\\omega$$\nTo evaluate the limit as $T \\to \\infty$, we recognize the function $\\frac{T}{2\\pi}\\left(\\frac{\\sin(\\omega T/2)}{\\omega T/2}\\right)^2$ as the Fejér kernel, which forms an approximation to the Dirac delta function $\\delta(\\omega)$.\n$$\\lim_{T\\to\\infty} \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S_c(\\omega) \\left(\\frac{\\sin(\\omega T/2)}{\\omega T/2}\\right)^2 d\\omega = \\lim_{T\\to\\infty} \\frac{1}{T} \\int_{-\\infty}^{\\infty} S_c(\\omega) \\frac{T}{2\\pi}\\left(\\frac{\\sin(\\omega T/2)}{\\omega T/2}\\right)^2 d\\omega$$\nSince $S_c(\\omega)$ is continuous at $\\omega=0$ by hypothesis, the integral converges to $S_c(0)$.\n$$\\lim_{T\\to\\infty} \\mathrm{var}(\\overline{c}_T) = \\lim_{T\\to\\infty} \\frac{1}{T} \\cdot S_c(0) = 0$$\nThis formally proves that the process $c(t)$ is ergodic in the mean.\n\nSubstituting this result back into the expression for the limit of the variance of $\\overline{x}_T$:\n$$\\lim_{T\\to\\infty} \\mathrm{var}(\\overline{x}_{T}) = \\sigma_0^2 + 0 = \\sigma_0^2$$\nThe non-vanishing part of the variance of the time average is due entirely to the random DC component, represented by the delta function in the PSD.", "answer": "$$\\boxed{\\sigma_{0}^{2}}$$", "id": "2869754"}]}