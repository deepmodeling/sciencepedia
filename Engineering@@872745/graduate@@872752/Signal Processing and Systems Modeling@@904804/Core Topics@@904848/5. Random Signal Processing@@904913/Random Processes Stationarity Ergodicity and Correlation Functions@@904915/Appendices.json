{"hands_on_practices": [{"introduction": "The connection between a random process's time-domain characteristics and its frequency-domain representation is fundamental to signal processing. This exercise explores the most basic of these connections: the effect of a non-zero mean on the power spectral density (PSD). You will derive how a constant DC offset in the time domain manifests as a distinct impulse at zero frequency in the spectrum, a direct consequence of the Wiener-Khinchin theorem. [@problem_id:2899117]", "problem": "Consider a continuous-time wide-sense stationary (WSS) random process $X(t)$ with possibly complex values, having constant (time-invariant) mean $m_{X} \\triangleq \\mathbb{E}[X(t)] \\neq 0$. Let $Y(t) \\triangleq X(t) - m_{X}$ denote the zero-mean fluctuation about the mean. Denote by $R_{X}(\\tau) \\triangleq \\mathbb{E}[X(t) X^*(t+\\tau)]$ the autocorrelation function and by $S_{X}(f)$ the power spectral density (PSD), related for WSS processes by the Fourier transform pair\n$$\nS_{X}(f) = \\int_{-\\infty}^{\\infty} R_{X}(\\tau)\\, \\exp(-j 2\\pi f \\tau)\\, d\\tau, \n\\quad\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} S_{X}(f)\\, \\exp(j 2\\pi f \\tau)\\, df,\n$$\nwhere $\\delta(\\cdot)$ denotes the Dirac delta distribution.\n\n1) Starting strictly from the definitions above, and using only linearity of expectation and stationarity, derive how a nonzero mean $m_{X}$ contributes a discrete spectral line to $S_{X}(f)$. State precisely the frequency at which this line occurs and its amplitude.\n\n2) Suppose the zero-mean component $Y(t)$ has a continuous PSD given by\n$$\nS_{Y}(f) = \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}},\n$$\nwith constants $a  0$ and $\\sigma^{2}  0$. Using only the inverse Fourier transform definition above, integrate the total PSD\n$$\nS_{X}(f) = |m_{X}|^{2}\\, \\delta(f) + \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}}\n$$\nto recover the autocorrelation $R_{X}(\\tau)$ as a closed-form analytic expression in terms of $m_{X}$, $\\sigma^{2}$, $a$, and $\\tau$. Provide your final answer as a single expression for $R_{X}(\\tau)$. No numerical approximation is required, and no units are needed.", "solution": "The problem statement has been analyzed and is determined to be valid. It is a well-posed, self-contained, and scientifically sound problem in the field of signal processing and random processes. The problem is formalizable and free from contradiction, ambiguity, or factual error. We may therefore proceed with the solution.\n\nThe problem is divided into two parts.\n\nPart 1: Derivation of the spectral line from a non-zero mean.\n\nWe are given a wide-sense stationary (WSS) random process $X(t)$ with a constant, non-zero mean $m_{X} = \\mathbb{E}[X(t)] \\neq 0$. The zero-mean fluctuation is defined as $Y(t) = X(t) - m_{X}$, from which it follows that $\\mathbb{E}[Y(t)] = \\mathbb{E}[X(t) - m_{X}] = \\mathbb{E}[X(t)] - m_{X} = m_{X} - m_{X} = 0$.\n\nThe autocorrelation function of $X(t)$ is defined as $R_{X}(\\tau) \\triangleq \\mathbb{E}[X(t) X^*(t+\\tau)]$. We substitute $X(t) = Y(t) + m_{X}$ into this definition:\n$$\nR_{X}(\\tau) = \\mathbb{E}\\left[ \\left( Y(t) + m_{X} \\right) \\left( Y(t+\\tau) + m_{X} \\right)^* \\right]\n$$\nExpanding the product, we get:\n$$\nR_{X}(\\tau) = \\mathbb{E}\\left[ \\left( Y(t) + m_{X} \\right) \\left( Y^*(t+\\tau) + m_{X}^* \\right) \\right] = \\mathbb{E}\\left[ Y(t)Y^*(t+\\tau) + Y(t)m_{X}^* + m_{X}Y^*(t+\\tau) + m_{X}m_{X}^* \\right]\n$$\nBy linearity of the expectation operator, we can write this as four separate terms:\n$$\nR_{X}(\\tau) = \\mathbb{E}[Y(t)Y^*(t+\\tau)] + \\mathbb{E}[Y(t)m_{X}^*] + \\mathbb{E}[m_{X}Y^*(t+\\tau)] + \\mathbb{E}[m_{X}m_{X}^*]\n$$\nSince $m_{X}$ is a constant, we can factor it out of the expectations:\n$$\nR_{X}(\\tau) = \\mathbb{E}[Y(t)Y^*(t+\\tau)] + m_{X}^*\\mathbb{E}[Y(t)] + m_{X}\\mathbb{E}[Y^*(t+\\tau)] + |m_{X}|^2\n$$\nWe have already established that $Y(t)$ is a zero-mean process, so $\\mathbb{E}[Y(t)] = 0$ for any time $t$. By WSS properties, this holds for all $t$. This also implies $\\mathbb{E}[Y^*(t+\\tau)] = (\\mathbb{E}[Y(t+\\tau)])^* = 0^* = 0$. The middle two terms are therefore zero. The first term is, by definition, the autocorrelation function of the zero-mean process $Y(t)$, which we denote $R_{Y}(\\tau)$.\nThe expression for $R_{X}(\\tau)$ simplifies to:\n$$\nR_{X}(\\tau) = R_{Y}(\\tau) + |m_{X}|^2\n$$\nNow, we find the power spectral density (PSD) $S_{X}(f)$ by taking the Fourier transform of $R_{X}(\\tau)$, as per the given Wiener-Khinchin relation:\n$$\nS_{X}(f) = \\int_{-\\infty}^{\\infty} R_{X}(\\tau) \\exp(-j 2\\pi f \\tau) d\\tau = \\int_{-\\infty}^{\\infty} (R_{Y}(\\tau) + |m_{X}|^2) \\exp(-j 2\\pi f \\tau) d\\tau\n$$\nUsing the linearity of the Fourier transform, we separate the integral:\n$$\nS_{X}(f) = \\int_{-\\infty}^{\\infty} R_{Y}(\\tau) \\exp(-j 2\\pi f \\tau) d\\tau + \\int_{-\\infty}^{\\infty} |m_{X}|^2 \\exp(-j 2\\pi f \\tau) d\\tau\n$$\nThe first integral is the definition of the PSD of $Y(t)$, which is $S_{Y}(f)$. For the second integral, we recognize the integral representation of the Dirac delta distribution, $\\delta(f) = \\int_{-\\infty}^{\\infty} \\exp(-j 2\\pi f \\tau) d\\tau$.\n$$\n\\int_{-\\infty}^{\\infty} |m_{X}|^2 \\exp(-j 2\\pi f \\tau) d\\tau = |m_{X}|^2 \\int_{-\\infty}^{\\infty} \\exp(-j 2\\pi f \\tau) d\\tau = |m_{X}|^2 \\delta(f)\n$$\nCombining these results, we obtain the total PSD of $X(t)$:\n$$\nS_{X}(f) = S_{Y}(f) + |m_{X}|^2 \\delta(f)\n$$\nThis result demonstrates that the non-zero mean $m_{X}$ contributes a discrete spectral line to the power spectral density. This line is precisely described as follows:\n-   **Frequency:** It occurs at frequency $f=0$, which is the DC component.\n-   **Amplitude:** The term \"amplitude\" for a Dirac delta function refers to its weight or integrated strength. The line is given by the term $|m_{X}|^2 \\delta(f)$, so its amplitude (strength) is $|m_{X}|^2$.\n\nPart 2: Recovery of the autocorrelation function $R_{X}(\\tau)$.\n\nWe are given the total power spectral density of the process $X(t)$:\n$$\nS_{X}(f) = |m_{X}|^{2}\\, \\delta(f) + \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}}\n$$\nwhere the second term is the PSD of the zero-mean component, $S_{Y}(f)$. We are to find the autocorrelation function $R_{X}(\\tau)$ by applying the inverse Fourier transform as defined in the problem:\n$$\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} S_{X}(f) \\exp(j 2\\pi f \\tau) df\n$$\nSubstituting the expression for $S_{X}(f)$:\n$$\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} \\left( |m_{X}|^2\\, \\delta(f) + \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}} \\right) \\exp(j 2\\pi f \\tau) df\n$$\nBy linearity, we can split this into two integrals:\n$$\nR_{X}(\\tau) = \\int_{-\\infty}^{\\infty} |m_{X}|^2\\, \\delta(f) \\exp(j 2\\pi f \\tau) df + \\int_{-\\infty}^{\\infty} \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}} \\exp(j 2\\pi f \\tau) df\n$$\nLet us evaluate each integral separately. The first integral is evaluated using the sifting property of the Dirac delta function, $\\int_{-\\infty}^{\\infty} g(f) \\delta(f-f_{0}) df = g(f_{0})$. Here, $g(f) = |m_{X}|^2 \\exp(j 2\\pi f \\tau)$ and $f_{0}=0$.\n$$\n\\int_{-\\infty}^{\\infty} |m_{X}|^2\\, \\delta(f) \\exp(j 2\\pi f \\tau) df = |m_{X}|^2 \\exp(j 2\\pi (0) \\tau) = |m_{X}|^2\n$$\nThis is the constant component of the autocorrelation function, corresponding to the mean.\n\nThe second integral is the inverse Fourier transform of $S_{Y}(f)$, which yields $R_{Y}(\\tau)$:\n$$\nR_{Y}(\\tau) = \\int_{-\\infty}^{\\infty} \\frac{2 \\sigma^{2} a}{a^{2} + (2\\pi f)^{2}} \\exp(j 2\\pi f \\tau) df\n$$\nThis is a standard Fourier transform pair. We recognize that the function $\\sigma^{2} \\exp(-a|\\tau|)$ for $a0$ has the Fourier transform:\n$$\n\\mathcal{F}\\{\\sigma^2 \\exp(-a|\\tau|)\\} = \\sigma^{2} \\int_{-\\infty}^{\\infty} \\exp(-a|\\tau|) \\exp(-j 2\\pi f \\tau) d\\tau = \\sigma^{2} \\frac{2a}{a^{2} + (2\\pi f)^{2}} = S_Y(f)\n$$\nTherefore, by the definition of the inverse Fourier transform, we have:\n$$\nR_{Y}(\\tau) = \\mathcal{F}^{-1}\\{S_{Y}(f)\\} = \\sigma^{2} \\exp(-a|\\tau|)\n$$\nCombining the results for the two parts of the integral, we obtain the complete autocorrelation function for $X(t)$:\n$$\nR_{X}(\\tau) = |m_{X}|^2 + R_{Y}(\\tau) = |m_{X}|^2 + \\sigma^{2} \\exp(-a|\\tau|)\n$$\nThis is the final closed-form analytic expression for $R_{X}(\\tau)$.", "answer": "$$\n\\boxed{|m_{X}|^{2} + \\sigma^{2} \\exp(-a |\\tau|)}\n$$", "id": "2899117"}, {"introduction": "Beyond abstract definitions, random processes are powerful tools for modeling real-world physical phenomena, from noisy electronic circuits to the diffusion of particles. This practice delves into the Ornstein-Uhlenbeck process, a cornerstone model for systems exhibiting mean-reversion and subject to random fluctuations. By solving the governing stochastic differential equation, you will derive its autocorrelation function and power spectral density from first principles, gaining insight into how a system's physical parameters shape its statistical signature. [@problem_id:2899164]", "problem": "Consider the Ornstein–Uhlenbeck process $X(t)$, $t \\in \\mathbb{R}$, defined as the unique strictly stationary solution of the linear stochastic differential equation (SDE)\n$$\ndX(t) \\;=\\; -\\,\\alpha\\,X(t)\\,dt \\;+\\; \\sigma\\,dB_t,\n$$\nwhere $\\alpha0$ and $\\sigma0$ are constants and $\\{B_t\\}_{t \\in \\mathbb{R}}$ is a standard Wiener process (also called Brownian motion) with independent increments. Work from first principles appropriate to signal processing and systems modeling: solve the linear SDE using an integrating factor, use properties of the Wiener process and Itô integrals, and start from the definition of autocorrelation for a wide-sense stationary process. Then, from the Wiener–Khintchine theorem (power spectral density (PSD) is the Fourier transform of the autocorrelation), adopt the convention\n$$\nS_X(f) \\;=\\; \\int_{-\\infty}^{\\infty} R_X(\\tau)\\,\\exp\\!\\big(-\\mathrm{j}\\,2\\pi f\\,\\tau\\big)\\,d\\tau,\\qquad\nR_X(\\tau) \\;=\\; \\int_{-\\infty}^{\\infty} S_X(f)\\,\\exp\\!\\big(\\mathrm{j}\\,2\\pi f\\,\\tau\\big)\\,df.\n$$\nDerive the closed-form autocorrelation function $R_X(\\tau)$ of $X(t)$ and then compute the corresponding PSD $S_X(f)$.\n\nProvide as your final answer the single closed-form analytic expression for $S_X(f)$ in terms of $\\alpha$, $\\sigma$, and $f$. No rounding is required, and no units are to be reported.", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n**Step 1: Extract Givens**\n- The process is the Ornstein-Uhlenbeck process, denoted as $X(t)$, for $t \\in \\mathbb{R}$.\n- It is the unique strictly stationary solution to the stochastic differential equation (SDE): $dX(t) = -\\alpha X(t) dt + \\sigma dB_t$.\n- Parameters $\\alpha$ and $\\sigma$ are constants, with $\\alpha  0$ and $\\sigma  0$.\n- $\\{B_t\\}_{t \\in \\mathbb{R}}$ is a standard Wiener process with independent increments.\n- The required derivation method is specified: solve the SDE using an integrating factor, use properties of the Wiener process and Itô integrals, and start from the definition of the autocorrelation function.\n- The Wiener-Khintchine theorem is given with the Fourier transform convention: $S_X(f) = \\int_{-\\infty}^{\\infty} R_X(\\tau) \\exp(-\\mathrm{j} 2\\pi f \\tau) d\\tau$.\n- The task is to derive the autocorrelation function $R_X(\\tau)$ and then compute the power spectral density (PSD) $S_X(f)$.\n- The final answer is to be a single closed-form expression for $S_X(f)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on the Ornstein-Uhlenbeck process, a fundamental model in stochastic processes and statistical physics. The SDE, the Wiener process, and the Wiener-Khintchine theorem are all standard, well-established concepts. The problem is scientifically valid.\n- **Well-Posed:** The problem is clearly stated. The condition $\\alpha  0$ guarantees the existence of a unique stationary solution. The objective and the required methodology are specified unambiguously.\n- **Objective:** The problem is formulated in precise, objective mathematical language, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard, well-posed problem in the theory of random processes. A solution will be provided.\n\nThe Ornstein-Uhlenbeck SDE is given by $dX(t) = -\\alpha X(t) dt + \\sigma dB_t$. We can rewrite this as a linear first-order differential equation:\n$$\ndX(t) + \\alpha X(t) dt = \\sigma dB_t\n$$\nTo solve this, we use the integrating factor method. The integrating factor is $I(t) = \\exp(\\int \\alpha dt) = \\exp(\\alpha t)$. Multiplying the SDE by $I(t)$ gives:\n$$\n\\exp(\\alpha t) dX(t) + \\alpha \\exp(\\alpha t) X(t) dt = \\sigma \\exp(\\alpha t) dB_t\n$$\nThe left-hand side is the Itô differential of the product $\\exp(\\alpha t) X(t)$. For a general function $f(t,x)$, Itô's lemma states $df(t, X(t)) = \\frac{\\partial f}{\\partial t} dt + \\frac{\\partial f}{\\partial x} dX(t) + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial x^2} (dX(t))^2$. For $f(t,X(t)) = \\exp(\\alpha t) X(t)$, we have $\\frac{\\partial f}{\\partial t} = \\alpha \\exp(\\alpha t) X(t)$, $\\frac{\\partial f}{\\partial x} = \\exp(\\alpha t)$, and $\\frac{\\partial^2 f}{\\partial x^2} = 0$. The term with $(dX(t))^2$ vanishes, so the standard product rule from calculus applies:\n$$\nd(\\exp(\\alpha t) X(t)) = \\sigma \\exp(\\alpha t) dB_t\n$$\nIntegrating both sides from a past time $t_0$ to $t$:\n$$\n\\int_{t_0}^t d(\\exp(\\alpha s) X(s)) = \\int_{t_0}^t \\sigma \\exp(\\alpha s) dB_s\n$$\n$$\n\\exp(\\alpha t) X(t) - \\exp(\\alpha t_0) X(t_0) = \\sigma \\int_{t_0}^t \\exp(\\alpha s) dB_s\n$$\nSolving for $X(t)$:\n$$\nX(t) = \\exp(-\\alpha(t-t_0))X(t_0) + \\sigma \\int_{t_0}^t \\exp(-\\alpha(t-s)) dB_s\n$$\nThe problem specifies that $X(t)$ is a stationary solution. This corresponds to the case where the process has been running for an infinite time, which is achieved by taking the limit $t_0 \\to -\\infty$. Since $\\alpha  0$, the term $\\exp(-\\alpha(t-t_0))$ decays to zero as $t_0 \\to -\\infty$. The stationary solution is therefore given by the Itô integral:\n$$\nX(t) = \\sigma \\int_{-\\infty}^t \\exp(-\\alpha(t-s)) dB_s\n$$\nThe mean of the process is $E[X(t)] = E[\\sigma \\int_{-\\infty}^t \\exp(-\\alpha(t-s)) dB_s]$. Due to the zero-mean property of Itô integrals with respect to a standard Wiener process, $E[X(t)] = 0$.\n\nNext, we derive the autocorrelation function, defined for a wide-sense stationary process as $R_X(\\tau) = E[X(t) X(t+\\tau)]$. Without loss of generality, let $\\tau \\geq 0$.\n$$\nR_X(\\tau) = E\\left[ \\left(\\sigma \\int_{-\\infty}^t \\exp(-\\alpha(t-s_1)) dB_{s_1}\\right) \\left(\\sigma \\int_{-\\infty}^{t+\\tau} \\exp(-\\alpha(t+\\tau-s_2)) dB_{s_2}\\right) \\right]\n$$\nUsing the Itô isometry property, $E[\\int f(s) dB_s \\int g(s) dB_s] = \\int f(s) g(s) ds$, where the integral is over the common domain of integration. For our case, the domain is $(-\\infty, t] \\cap (-\\infty, t+\\tau] = (-\\infty, t]$.\n$$\nR_X(\\tau) = \\sigma^2 \\int_{-\\infty}^t \\exp(-\\alpha(t-s)) \\exp(-\\alpha(t+\\tau-s)) ds\n$$\nWe combine the exponents in the integrand:\n$$\nR_X(\\tau) = \\sigma^2 \\int_{-\\infty}^t \\exp(-\\alpha t + \\alpha s - \\alpha t - \\alpha \\tau + \\alpha s) ds = \\sigma^2 \\int_{-\\infty}^t \\exp(-2\\alpha t - \\alpha\\tau + 2\\alpha s) ds\n$$\nThe terms independent of the integration variable $s$ can be moved outside the integral:\n$$\nR_X(\\tau) = \\sigma^2 \\exp(-2\\alpha t - \\alpha\\tau) \\int_{-\\infty}^t \\exp(2\\alpha s) ds\n$$\nEvaluating the integral:\n$$\n\\int_{-\\infty}^t \\exp(2\\alpha s) ds = \\left[ \\frac{1}{2\\alpha} \\exp(2\\alpha s) \\right]_{-\\infty}^t = \\frac{1}{2\\alpha} \\exp(2\\alpha t) - 0 = \\frac{1}{2\\alpha} \\exp(2\\alpha t)\n$$\nSubstituting this result back into the expression for $R_X(\\tau)$:\n$$\nR_X(\\tau) = \\sigma^2 \\exp(-2\\alpha t - \\alpha\\tau) \\left( \\frac{1}{2\\alpha} \\exp(2\\alpha t) \\right) = \\frac{\\sigma^2}{2\\alpha} \\exp(-2\\alpha t - \\alpha\\tau + 2\\alpha t) = \\frac{\\sigma^2}{2\\alpha} \\exp(-\\alpha\\tau)\n$$\nThis result was derived for $\\tau \\geq 0$. The autocorrelation function must be an even function, $R_X(\\tau) = R_X(-\\tau)$. Therefore, for any real $\\tau$, the function is:\n$$\nR_X(\\tau) = \\frac{\\sigma^2}{2\\alpha} \\exp(-\\alpha|\\tau|)\n$$\nThe variance of the process is $R_X(0) = \\frac{\\sigma^2}{2\\alpha}$.\n\nFinally, we compute the power spectral density $S_X(f)$ using the Wiener-Khintchine theorem, which states that the PSD is the Fourier transform of the autocorrelation function.\n$$\nS_X(f) = \\int_{-\\infty}^{\\infty} R_X(\\tau) \\exp(-\\mathrm{j} 2\\pi f \\tau) d\\tau = \\int_{-\\infty}^{\\infty} \\frac{\\sigma^2}{2\\alpha} \\exp(-\\alpha|\\tau|) \\exp(-\\mathrm{j} 2\\pi f \\tau) d\\tau\n$$\n$$\nS_X(f) = \\frac{\\sigma^2}{2\\alpha} \\int_{-\\infty}^{\\infty} \\exp(-\\alpha|\\tau|) \\exp(-\\mathrm{j} 2\\pi f \\tau) d\\tau\n$$\nWe split the integral at $\\tau=0$ to handle the absolute value:\n$$\nS_X(f) = \\frac{\\sigma^2}{2\\alpha} \\left[ \\int_{-\\infty}^{0} \\exp(\\alpha\\tau) \\exp(-\\mathrm{j} 2\\pi f \\tau) d\\tau + \\int_{0}^{\\infty} \\exp(-\\alpha\\tau) \\exp(-\\mathrm{j} 2\\pi f \\tau) d\\tau \\right]\n$$\n$$\nS_X(f) = \\frac{\\sigma^2}{2\\alpha} \\left[ \\int_{-\\infty}^{0} \\exp((\\alpha - \\mathrm{j} 2\\pi f)\\tau) d\\tau + \\int_{0}^{\\infty} \\exp(-(\\alpha + \\mathrm{j} 2\\pi f)\\tau) d\\tau \\right]\n$$\nEvaluating each integral:\n$$\n\\int_{-\\infty}^{0} \\exp((\\alpha - \\mathrm{j} 2\\pi f)\\tau) d\\tau = \\left[ \\frac{\\exp((\\alpha - \\mathrm{j} 2\\pi f)\\tau)}{\\alpha - \\mathrm{j} 2\\pi f} \\right]_{-\\infty}^0 = \\frac{1}{\\alpha - \\mathrm{j} 2\\pi f}\n$$\n$$\n\\int_{0}^{\\infty} \\exp(-(\\alpha + \\mathrm{j} 2\\pi f)\\tau) d\\tau = \\left[ \\frac{\\exp(-(\\alpha + \\mathrm{j} 2\\pi f)\\tau)}{-(\\alpha + \\mathrm{j} 2\\pi f)} \\right]_{0}^{\\infty} = \\frac{1}{\\alpha + \\mathrm{j} 2\\pi f}\n$$\nSubstituting these into the expression for $S_X(f)$:\n$$\nS_X(f) = \\frac{\\sigma^2}{2\\alpha} \\left[ \\frac{1}{\\alpha - \\mathrm{j} 2\\pi f} + \\frac{1}{\\alpha + \\mathrm{j} 2\\pi f} \\right]\n$$\nWe combine the fractions over a common denominator:\n$$\nS_X(f) = \\frac{\\sigma^2}{2\\alpha} \\left[ \\frac{(\\alpha + \\mathrm{j} 2\\pi f) + (\\alpha - \\mathrm{j} 2\\pi f)}{(\\alpha - \\mathrm{j} 2\\pi f)(\\alpha + \\mathrm{j} 2\\pi f)} \\right] = \\frac{\\sigma^2}{2\\alpha} \\left[ \\frac{2\\alpha}{\\alpha^2 - (\\mathrm{j} 2\\pi f)^2} \\right]\n$$\nSince $\\mathrm{j}^2 = -1$, the denominator becomes $\\alpha^2 + (2\\pi f)^2$.\n$$\nS_X(f) = \\frac{\\sigma^2}{2\\alpha} \\left( \\frac{2\\alpha}{\\alpha^2 + (2\\pi f)^2} \\right)\n$$\nThe term $2\\alpha$ cancels, yielding the final expression for the power spectral density.\n$$\nS_X(f) = \\frac{\\sigma^2}{\\alpha^2 + (2\\pi f)^2}\n$$\nThis result is real-valued and non-negative for all $f$, as expected for a PSD. It has the characteristic Lorentzian shape.", "answer": "$$\n\\boxed{\\frac{\\sigma^2}{\\alpha^2 + (2\\pi f)^2}}\n$$", "id": "2899164"}, {"introduction": "The concepts of stationarity and ergodicity are often discussed together, but they are not equivalent, and this distinction is critical for practical signal analysis. This exercise clarifies the difference by examining a process where time averages do not necessarily converge to their corresponding ensemble averages. By analyzing a simple sinusoidal process with added noise under two different assumptions for its phase, you will explore the conditions required for mean-ergodicity, a property that justifies estimating statistical parameters from a single, finite-duration measurement. [@problem_id:2899156]", "problem": "Consider a real-valued random process defined by $X(t) = A \\cos(2\\pi f_0 t + \\Phi) + W(t)$, where $A  0$ and $f_0  0$ are fixed constants, $W(t)$ is a zero-mean wide-sense stationary process with autocorrelation function $R_W(\\tau)$ that is absolutely integrable (i.e., $\\int_{-\\infty}^{\\infty} |R_W(\\tau)| \\, d\\tau  \\infty$), and $W(t)$ is independent of $\\Phi$. Define the finite-time average $\\overline{X}_T$ by\n$$\n\\overline{X}_T \\triangleq \\frac{1}{T} \\int_{0}^{T} X(t) \\, dt.\n$$\nWork in the mean-square sense, that is, limits of random quantities are to be interpreted as limits in mean-square. Use only foundational definitions, specifically: the ensemble mean $m_X(t) \\triangleq \\mathbb{E}\\{X(t)\\}$, the time average $\\overline{X}_T$, and the definition of mean-square convergence.\n\nAnalyze the following two cases for the phase $\\Phi$:\n\n(i) $\\Phi = \\phi_0$ is a deterministic constant with $\\phi_0 \\in \\mathbb{R}$.\n\n(ii) $\\Phi$ is uniformly distributed over $[0,2\\pi)$ and independent of $W(t)$.\n\nFor each case, compute the mean-square limit\n$$\n\\Delta \\triangleq \\lim_{T \\to \\infty} \\left( \\overline{X}_T - \\mathbb{E}\\{X(0)\\} \\right),\n$$\nas a closed-form expression in terms of $A$, $f_0$, and $\\phi_0$, as appropriate. Report your final result as a single row vector using the LaTeX $\\texttt{pmatrix}$ format whose first entry is the value of $\\Delta$ in case (i) and whose second entry is the value of $\\Delta$ in case (ii). No rounding is required. No units are involved. The angle $\\phi_0$ is in radians.", "solution": "The problem requires the computation of the mean-square limit\n$$\n\\Delta \\triangleq \\lim_{T \\to \\infty} \\left( \\overline{X}_T - \\mathbb{E}\\{X(0)\\} \\right)\n$$\nfor the real-valued random process $X(t) = A \\cos(2\\pi f_0 t + \\Phi) + W(t)$. The limit is to be evaluated for two distinct cases concerning the phase $\\Phi$. The analysis will be performed in the mean-square sense, which requires that for a sequence of random variables $Y_T$, its limit $Y$ satisfies $\\lim_{T \\to \\infty} \\mathbb{E}\\left\\{(Y_T - Y)^2\\right\\} = 0$.\n\nLet us first decompose the process $X(t)$ into a sinusoidal component $S(t) = A \\cos(2\\pi f_0 t + \\Phi)$ and a noise component $W(t)$. By the linearity of both the time-averaging and expectation operators, we can write:\n$$\n\\overline{X}_T = \\frac{1}{T} \\int_{0}^{T} X(t) \\, dt = \\frac{1}{T} \\int_{0}^{T} S(t) \\, dt + \\frac{1}{T} \\int_{0}^{T} W(t) \\, dt = \\overline{S}_T + \\overline{W}_T\n$$\nAnd\n$$\n\\mathbb{E}\\{X(0)\\} = \\mathbb{E}\\{S(0) + W(0)\\} = \\mathbb{E}\\{S(0)\\} + \\mathbb{E}\\{W(0)\\}\n$$\nThe quantity to be evaluated is $Z_T \\triangleq \\overline{X}_T - \\mathbb{E}\\{X(0)\\} = (\\overline{S}_T - \\mathbb{E}\\{S(0)\\}) + (\\overline{W}_T - \\mathbb{E}\\{W(0)\\})$.\nThe process $W(t)$ is given as zero-mean, so $\\mathbb{E}\\{W(t)\\} = 0$ for all $t$, which implies $\\mathbb{E}\\{W(0)\\} = 0$. The expression simplifies to:\n$$\nZ_T = (\\overline{S}_T - \\mathbb{E}\\{S(0)\\}) + \\overline{W}_T\n$$\n\nLet us analyze the behavior of the noise term $\\overline{W}_T$ as $T \\to \\infty$. We will show that it converges to $0$ in the mean-square sense. The mean of $\\overline{W}_T$ is $\\mathbb{E}\\{\\overline{W}_T\\} = \\frac{1}{T}\\int_0^T \\mathbb{E}\\{W(t)\\} dt = 0$. The mean-square value of $\\overline{W}_T$ is its variance:\n$$\n\\mathbb{E}\\{\\overline{W}_T^2\\} = \\mathbb{E}\\left\\{ \\left( \\frac{1}{T} \\int_0^T W(t) \\, dt \\right)^2 \\right\\} = \\frac{1}{T^2} \\mathbb{E}\\left\\{ \\int_0^T \\int_0^T W(t_1)W(t_2) \\, dt_1 dt_2 \\right\\}\n$$\nUsing Fubini's theorem and the fact that $W(t)$ is wide-sense stationary (WSS) with autocorrelation $R_W(\\tau) = \\mathbb{E}\\{W(t)W(t+\\tau)\\}$, we have:\n$$\n\\mathbb{E}\\{\\overline{W}_T^2\\} = \\frac{1}{T^2} \\int_0^T \\int_0^T R_W(t_1 - t_2) \\, dt_1 dt_2\n$$\nThis double integral is a standard form that can be reduced to a single integral:\n$$\n\\mathbb{E}\\{\\overline{W}_T^2\\} = \\frac{1}{T} \\int_{-T}^{T} \\left(1 - \\frac{|\\tau|}{T}\\right) R_W(\\tau) \\, d\\tau\n$$\nWe are given that $\\int_{-\\infty}^{\\infty} |R_W(\\tau)| \\, d\\tau  \\infty$. Let this finite value be $C$. We can bound the mean-square value:\n$$\n\\left| \\mathbb{E}\\{\\overline{W}_T^2\\} \\right| \\le \\frac{1}{T} \\int_{-T}^{T} \\left|1 - \\frac{|\\tau|}{T}\\right| |R_W(\\tau)| \\, d\\tau\n$$\nFor $\\tau \\in [-T, T]$, the term $0 \\le (1 - |\\tau|/T) \\le 1$. Thus,\n$$\n0 \\le \\mathbb{E}\\{\\overline{W}_T^2\\} \\le \\frac{1}{T} \\int_{-T}^{T} |R_W(\\tau)| \\, d\\tau \\le \\frac{1}{T} \\int_{-\\infty}^{\\infty} |R_W(\\tau)| \\, d\\tau = \\frac{C}{T}\n$$\nAs $T \\to \\infty$, we have $\\frac{C}{T} \\to 0$. By the Squeeze Theorem, $\\lim_{T \\to \\infty} \\mathbb{E}\\{\\overline{W}_T^2\\} = 0$. This proves that $\\overline{W}_T$ converges to $0$ in the mean-square sense.\n\nTherefore, the limit can be determined by analyzing the signal component alone:\n$$\n\\Delta = \\lim_{T \\to \\infty} (\\overline{S}_T - \\mathbb{E}\\{S(0)\\}) \\quad (\\text{in mean-square})\n$$\nWe now proceed with the two specified cases.\n\n(i) $\\Phi = \\phi_0$ is a deterministic constant.\nIn this case, $S(t) = A \\cos(2\\pi f_0 t + \\phi_0)$ is a deterministic signal. Its time average $\\overline{S}_T$ is also deterministic.\n$$\n\\overline{S}_T = \\frac{1}{T} \\int_0^T A \\cos(2\\pi f_0 t + \\phi_0) \\, dt = \\frac{A}{T} \\left[ \\frac{\\sin(2\\pi f_0 t + \\phi_0)}{2\\pi f_0} \\right]_0^T\n$$\n$$\n\\overline{S}_T = \\frac{A}{2\\pi f_0 T} \\left( \\sin(2\\pi f_0 T + \\phi_0) - \\sin(\\phi_0) \\right)\n$$\nSince the sine functions are bounded by $1$, the term in the parenthesis is bounded by $2$. As $T \\to \\infty$, the factor $\\frac{1}{T}$ causes the expression to go to zero. So, the deterministic limit is $\\lim_{T \\to \\infty} \\overline{S}_T = 0$. For a deterministic sequence, this is also its mean-square limit.\nThe expectation term is $\\mathbb{E}\\{S(0)\\} = \\mathbb{E}\\{A \\cos(\\phi_0)\\} = A \\cos(\\phi_0)$, since it is a constant.\nThe total quantity $Z_T = (\\overline{S}_T - A \\cos(\\phi_0)) + \\overline{W}_T$. We seek its mean-square limit $\\Delta_{(i)}$. Based on our analysis, we propose the limit is the constant $\\Delta_{(i)} = 0 - A \\cos(\\phi_0) = -A \\cos(\\phi_0)$.\nLet us verify this:\n$$\n\\lim_{T\\to\\infty} \\mathbb{E}\\left\\{ (Z_T - \\Delta_{(i)})^2 \\right\\} = \\lim_{T\\to\\infty} \\mathbb{E}\\left\\{ ((\\overline{S}_T - A \\cos(\\phi_0)) + \\overline{W}_T - (-A \\cos(\\phi_0)))^2 \\right\\}\n$$\n$$\n= \\lim_{T\\to\\infty} \\mathbb{E}\\left\\{ (\\overline{S}_T + \\overline{W}_T)^2 \\right\\} = \\lim_{T\\to\\infty} \\left( \\overline{S}_T^2 + 2\\overline{S}_T \\mathbb{E}\\{\\overline{W}_T\\} + \\mathbb{E}\\{\\overline{W}_T^2\\} \\right)\n$$\nSince $\\mathbb{E}\\{\\overline{W}_T\\}=0$, this simplifies to $\\lim_{T\\to\\infty} (\\overline{S}_T^2 + \\mathbb{E}\\{\\overline{W}_T^2\\})$.\nAs $T \\to \\infty$, $\\overline{S}_T \\to 0$ and $\\mathbb{E}\\{\\overline{W}_T^2\\} \\to 0$. Thus, the limit is $0$.\nThe proposed limit is correct. For case (i), $\\Delta_{(i)} = -A \\cos(\\phi_0)$.\n\n(ii) $\\Phi$ is uniformly distributed over $[0, 2\\pi)$.\nIn this case, $S(t)$ is a random process.\nFirst, we compute $\\mathbb{E}\\{S(0)\\} = \\mathbb{E}\\{A \\cos(\\Phi)\\}$.\n$$\n\\mathbb{E}\\{S(0)\\} = A \\int_0^{2\\pi} \\cos(\\phi) \\frac{1}{2\\pi} \\, d\\phi = \\frac{A}{2\\pi} [\\sin(\\phi)]_0^{2\\pi} = 0\n$$\nSo $\\Delta_{(ii)} = \\lim_{T \\to \\infty} \\overline{S}_T$ in mean-square. We propose the limit is $0$. To verify, we must show that $\\lim_{T \\to \\infty} \\mathbb{E}\\{\\overline{S}_T^2\\} = 0$.\nThe random variable $\\overline{S}_T$ is given by:\n$$\n\\overline{S}_T = \\frac{A}{2\\pi f_0 T} \\left( \\sin(2\\pi f_0 T + \\Phi) - \\sin(\\Phi) \\right)\n$$\nIts mean-square value is:\n$$\n\\mathbb{E}\\{\\overline{S}_T^2\\} = \\left( \\frac{A}{2\\pi f_0 T} \\right)^2 \\mathbb{E}\\left\\{ (\\sin(2\\pi f_0 T + \\Phi) - \\sin(\\Phi))^2 \\right\\}\n$$\nLet us evaluate the expectation term. Let $\\theta = 2\\pi f_0 T$.\n$$\n\\mathbb{E}\\{ (\\sin(\\theta + \\Phi) - \\sin(\\Phi))^2 \\} = \\mathbb{E}\\{\\sin^2(\\theta+\\Phi) - 2\\sin(\\theta+\\Phi)\\sin(\\Phi) + \\sin^2(\\Phi)\\}\n$$\nUsing the identities $\\sin^2(x) = \\frac{1-\\cos(2x)}{2}$ and $2\\sin(x)\\sin(y) = \\cos(x-y)-\\cos(x+y)$:\n$$\n\\mathbb{E}\\{\\sin^2(\\alpha)\\} = \\int_0^{2\\pi} \\frac{1-\\cos(2\\alpha)}{2} \\frac{1}{2\\pi} d\\phi = \\frac{1}{2} - \\frac{1}{4\\pi}\\int_0^{2\\pi}\\cos(2\\alpha)d\\phi\n$$\nFor $\\mathbb{E}\\{\\sin^2(\\theta+\\Phi)\\}$, $\\alpha=\\theta+\\phi$. For $\\mathbb{E}\\{\\sin^2(\\Phi)\\}$, $\\alpha=\\phi$. In both cases, the integral of the cosine term over a multiple of its period is zero. So, $\\mathbb{E}\\{\\sin^2(\\theta+\\Phi)\\} = \\frac{1}{2}$ and $\\mathbb{E}\\{\\sin^2(\\Phi)\\} = \\frac{1}{2}$.\nThe cross-term is $\\mathbb{E}\\{-2\\sin(\\theta+\\Phi)\\sin(\\Phi)\\} = \\mathbb{E}\\{-(\\cos(\\theta) - \\cos(\\theta+2\\Phi))\\} = -\\cos(\\theta) + \\mathbb{E}\\{\\cos(\\theta+2\\Phi)\\}$.\nThe expectation $\\mathbb{E}\\{\\cos(\\theta+2\\Phi)\\} = \\int_0^{2\\pi} \\cos(\\theta+2\\phi) \\frac{1}{2\\pi} d\\phi = 0$.\nSo the expectation evaluates to $\\frac{1}{2} - \\cos(\\theta) + \\frac{1}{2} = 1 - \\cos(2\\pi f_0 T)$.\nPutting it all back:\n$$\n\\mathbb{E}\\{\\overline{S}_T^2\\} = \\left( \\frac{A}{2\\pi f_0 T} \\right)^2 (1 - \\cos(2\\pi f_0 T))\n$$\nSince $0 \\le 1 - \\cos(2\\pi f_0 T) \\le 2$, we have:\n$$\n0 \\le \\mathbb{E}\\{\\overline{S}_T^2\\} \\le 2 \\left( \\frac{A}{2\\pi f_0 T} \\right)^2\n$$\nAs $T \\to \\infty$, the right-hand side goes to $0$. By the Squeeze Theorem, $\\lim_{T \\to \\infty} \\mathbb{E}\\{\\overline{S}_T^2\\} = 0$.\nThis confirms that $\\overline{S}_T \\to 0$ in mean-square.\nTherefore, $\\Delta_{(ii)} = \\lim_{T \\to \\infty} \\overline{S}_T - \\mathbb{E}\\{S(0)\\} = 0 - 0 = 0$.\n\nIn summary, the result for case (i) is $-A \\cos(\\phi_0)$ and for case (ii) is $0$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -A \\cos(\\phi_0)  0 \\end{pmatrix}}\n$$", "id": "2899156"}]}