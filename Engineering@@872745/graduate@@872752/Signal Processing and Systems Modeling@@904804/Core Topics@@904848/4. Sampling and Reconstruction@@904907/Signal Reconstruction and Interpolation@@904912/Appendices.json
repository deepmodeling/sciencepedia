{"hands_on_practices": [{"introduction": "The cornerstone of digital signal processing is the Shannon Sampling Theorem, which provides the theoretical guarantee for perfect reconstruction of a bandlimited signal. This exercise provides a direct hands-on experience with this fundamental principle, bridging the gap between abstract theory and practical implementation [@problem_id:2904292]. You will first derive the ideal interpolation kernel from first principles and then apply it numerically, building a tangible understanding of how a continuous signal can be perfectly recreated from its discrete samples.", "problem": "You are given a continuous-time signal $x(t)$ that is strictly bandlimited to angular frequency $|\\omega| \\le \\Omega_{\\mathrm{b}}$ with $\\Omega_{\\mathrm{b}}  \\pi/T$, where $T  0$ is a fixed sampling period. Ideal impulse sampling at rate $1/T$ produces the discrete-time samples $x[n] = x(nT)$ for all integers $n$, and the impulse train $s(t) = \\sum_{n\\in\\mathbb{Z}} x[n] \\,\\delta(t - nT)$. You feed $s(t)$ into a stable linear time-invariant (LTI) reconstruction system with impulse response $h(t)$ to produce an output $y(t)$.\n\nTask 1 (derivation from fundamentals). Starting from the following foundational bases:\n- the definition of the continuous-time Fourier transform (CTFT): $X(\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t}\\, dt$ and its inverse $x(t) = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} X(\\omega) e^{j\\omega t}\\, d\\omega$,\n- the spectrum of an impulse-sampled signal: $S(\\omega) = \\frac{1}{T}\\sum_{k\\in\\mathbb{Z}} X(\\omega - \\frac{2\\pi k}{T})$,\n- the LTI input-output relation in the frequency domain: $Y(\\omega) = H(\\omega) S(\\omega)$,\n\nderive, without assuming any interpolation shortcut formulas, the unique ideal lowpass $H(\\omega)$ that yields perfect reconstruction $Y(\\omega) = X(\\omega)$ for all $|\\omega| \\le \\Omega_{\\mathrm{b}}$ under the no-aliasing condition $\\Omega_{\\mathrm{b}}  \\pi/T$. Then, derive the corresponding time-domain interpolation kernel $h(t)$ and show that the reconstructed signal can be written as a convolutional superposition\n$$\ny(t) = \\sum_{n\\in\\mathbb{Z}} x[n]\\, h(t - nT).\n$$\nYou must arrive at a closed-form $h(t)$ in terms of a normalized sinc function $\\operatorname{sinc}(u) = \\frac{\\sin(\\pi u)}{\\pi u}$.\n\nTask 2 (numerical evaluation of the reconstruction). Using the result of Task 1, numerically evaluate $y(t)$ at specified time instants by replacing the infinite sum with a symmetric truncation $|n| \\le N$ for a sufficiently large integer $N$. Use the normalized sinc definition in Task 1. For all computations below, set $N = 2000$ and round every reported $y(t)$ value to $9$ decimal places using standard rounding to nearest, breaking ties to even.\n\nAngle quantities must be interpreted in radians. There are no physical units in this problem.\n\nTest suite. For each test case, the discrete-time sequence $x[n]$ is specified directly in terms of $n$, $T$, and fixed real parameters. All frequency parameters are strictly within the Nyquist limit $1/(2T)$, ensuring $\\Omega_{\\mathrm{b}}  \\pi/T$. For each test case, compute and report the list of reconstructed values $[y(t_1),\\dots,y(t_m)]$ at the given times using the truncated interpolation sum with $N = 2000$.\n\n- Test case A (multi-tone, unit sampling period):\n  - Parameters: $T = 1$, $\\phi = \\pi/7$.\n  - Sequence: $x[n] = \\cos(2\\pi \\cdot 0.2 \\cdot nT) + 0.5 \\,\\sin(2\\pi \\cdot 0.3 \\cdot nT + \\phi)$.\n  - Evaluation times: $t \\in \\{0, 0.25, 0.5, 1.25, 2\\}$.\n\n- Test case B (near-Nyquist single tone, unit sampling period):\n  - Parameters: $T = 1$.\n  - Sequence: $x[n] = \\cos(2\\pi \\cdot 0.49 \\cdot nT)$.\n  - Evaluation times: $t \\in \\{0, 0.5, 1, 1.5, 2\\}$.\n\n- Test case C (non-unit sampling period, composite with direct current (DC) component and phase):\n  - Parameters: $T = 0.8$, $\\theta = -\\pi/5$.\n  - Sequence: $x[n] = 0.1 + 0.7 \\cos\\!\\big(2\\pi \\cdot 0.25 \\cdot (nT)\\big) + 0.4 \\sin\\!\\big(2\\pi \\cdot 0.6 \\cdot (nT) + \\theta\\big)$.\n  - Evaluation times: $t \\in \\{0, 0.8, 1.6, 2.4, 4.0\\}$.\n\nComputational prescription. Implement\n$$\ny(t) \\approx \\sum_{n=-N}^{N} x[n]\\, h(t - nT), \\quad h(t) = \\operatorname{sinc}\\!\\left(\\frac{t}{T}\\right),\n$$\nwith $N = 2000$. Use vectorized computation where possible to avoid unnecessary numerical error and to ensure computational efficiency. Ensure stable evaluation of $\\operatorname{sinc}(0)$ using the continuous extension $\\operatorname{sinc}(0) = 1$.\n\nAnswer specification and output format. Your program should produce a single line of output containing the results as a comma-separated nested list with no spaces, of the form\n$[[y\\_A(t\\_1),\\dots,y\\_A(t\\_5)],[y\\_B(t\\_1),\\dots,y\\_B(t\\_5)],[y\\_C(t\\_1),\\dots,y\\_C(t\\_5)]]$,\nwhere each inner list corresponds to the test cases A, B, and C respectively, and each numeric entry is a decimal string rounded to $9$ digits after the decimal point. For example, an output line of the correct shape would look like $[[0.000000000,1.234000000],[...],[...]]$ but with the actual computed values for the three test cases.", "solution": "The problem is valid as it is scientifically grounded in the principles of signal processing, well-posed, and objective. It provides all necessary information for both the theoretical derivation and the numerical computation. We will proceed with the solution.\n\n**Task 1: Derivation of the Ideal Reconstruction Filter and Interpolation Formula**\n\nThe objective is to find a linear time-invariant (LTI) system with impulse response $h(t)$ that perfectly reconstructs a continuous-time signal $x(t)$ from its samples $x[n] = x(nT)$. The input to this reconstruction system is an impulse train $s(t) = \\sum_{n\\in\\mathbb{Z}} x[n] \\delta(t-nT)$.\n\nThe foundation for this derivation rests upon the given frequency-domain relationships:\n1.  The output spectrum $Y(\\omega)$ is related to the input spectrum $S(\\omega)$ and the system's frequency response $H(\\omega)$ by $Y(\\omega) = H(\\omega) S(\\omega)$.\n2.  The spectrum of the impulse train, $S(\\omega)$, is a periodic replication of the original signal's spectrum $X(\\omega)$:\n    $$S(\\omega) = \\frac{1}{T}\\sum_{k\\in\\mathbb{Z}} X\\left(\\omega - \\frac{2\\pi k}{T}\\right)$$\n    where $T$ is the sampling period. Let $\\omega_s = 2\\pi/T$ be the sampling angular frequency. The relation is thus $S(\\omega) = \\frac{1}{T}\\sum_{k\\in\\mathbb{Z}} X(\\omega - k\\omega_s)$.\n\nThe signal $x(t)$ is strictly bandlimited to $|\\omega| \\le \\Omega_{\\mathrm{b}}$, meaning its continuous-time Fourier transform (CTFT), $X(\\omega)$, is zero for all $|\\omega| > \\Omega_{\\mathrm{b}}$. The problem states the no-aliasing condition $\\Omega_{\\mathrm{b}}  \\pi/T$, which is equivalent to $\\Omega_{\\mathrm{b}}  \\omega_s/2$. This condition ensures that the spectral replicas in $S(\\omega)$ do not overlap. The spectral support of the $k$-th replica, $X(\\omega - k\\omega_s)$, is the interval $[k\\omega_s - \\Omega_{\\mathrm{b}}, k\\omega_s + \\Omega_{\\mathrm{b}}]$. Due to the no-aliasing condition, the baseband spectrum (for $k=0$, centered at $\\omega=0$) and the first harmonic replicas (for $k=\\pm 1$, centered at $\\omega=\\pm\\omega_s$) are disjoint. Specifically, the baseband spectrum ends at $\\Omega_{\\mathrm{b}}$, and the first replica begins at $\\omega_s - \\Omega_{\\mathrm{b}}$. Since $\\Omega_{\\mathrm{b}}  \\omega_s/2$, we have $\\omega_s - \\Omega_{\\mathrm{b}}  \\omega_s - \\omega_s/2 = \\omega_s/2 > \\Omega_{\\mathrm{b}}$.\n\nPerfect reconstruction requires that the output signal $y(t)$ is identical to the original signal $x(t)$, which implies their spectra must be equal: $Y(\\omega) = X(\\omega)$ for all $\\omega$.\n\nCombining the given relations, we require:\n$$X(\\omega) = H(\\omega) S(\\omega) = H(\\omega) \\left[ \\frac{1}{T}\\sum_{k\\in\\mathbb{Z}} X(\\omega - k\\omega_s) \\right]$$\n\nTo satisfy this equality, we design the filter $H(\\omega)$ to perform two functions:\n1.  Isolate the baseband component of $S(\\omega)$ (the $k=0$ term).\n2.  Scale this component to recover the original spectrum $X(\\omega)$.\n\nLet us define an ideal low-pass filter with a cutoff frequency $\\omega_c$. To isolate the baseband component, which is non-zero only for $|\\omega| \\le \\Omega_{\\mathrm{b}}$, while completely rejecting all higher-frequency replicas (the first of which starts at $\\omega_s - \\Omega_{\\mathrm{b}}$), we must choose a cutoff frequency $\\omega_c$ such that $\\Omega_{\\mathrm{b}} \\le \\omega_c \\le \\omega_s - \\Omega_{\\mathrm{b}}$. A canonical choice that is independent of the specific $\\Omega_{\\mathrm{b}}$ is the Nyquist frequency, $\\omega_c = \\omega_s/2 = \\pi/T$.\n\nFor frequencies within the passband, $|\\omega| \\le \\omega_c$, the sum $\\sum_{k\\in\\mathbb{Z}} X(\\omega - k\\omega_s)$ reduces to a single term, $X(\\omega)$, because all other terms with $k \\neq 0$ are zero in this range due to the no-aliasing condition. Thus, for $|\\omega| \\le \\pi/T$, the reconstruction equation becomes:\n$$X(\\omega) = H(\\omega) \\left( \\frac{1}{T} X(\\omega) \\right)$$\nFor this to hold for any non-trivial $X(\\omega)$, the filter's response in this band must be $H(\\omega) = T$.\n\nFor all frequencies outside this passband, $|\\omega| > \\pi/T$, the filter must have zero gain to eliminate the unwanted spectral replicas. Therefore, the unique ideal low-pass filter frequency response is:\n$$H(\\omega) = \\begin{cases} T  \\text{if } |\\omega| \\le \\pi/T \\\\ 0  \\text{if } |\\omega|  \\pi/T \\end{cases}$$\nThis is a rectangular function of width $2\\pi/T$ and height $T$.\n\nNext, we derive the corresponding impulse response $h(t)$ by taking the inverse CTFT of $H(\\omega)$:\n$$h(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} H(\\omega) e^{j\\omega t} \\,d\\omega$$\nSubstituting the expression for $H(\\omega)$:\n$$h(t) = \\frac{1}{2\\pi} \\int_{-\\pi/T}^{\\pi/T} T e^{j\\omega t} \\,d\\omega = \\frac{T}{2\\pi} \\left[ \\frac{e^{j\\omega t}}{jt} \\right]_{-\\pi/T}^{\\pi/T}$$\nFor $t \\neq 0$:\n$$h(t) = \\frac{T}{2\\pi j t} \\left( e^{j\\pi t/T} - e^{-j\\pi t/T} \\right)$$\nUsing Euler's formula, $e^{j\\theta} - e^{-j\\theta} = 2j\\sin(\\theta)$:\n$$h(t) = \\frac{T}{2\\pi j t} \\left( 2j \\sin\\left(\\frac{\\pi t}{T}\\right) \\right) = \\frac{T}{\\pi t} \\sin\\left(\\frac{\\pi t}{T}\\right)$$\nTo express this in terms of the normalized sinc function, $\\operatorname{sinc}(u) = \\frac{\\sin(\\pi u)}{\\pi u}$, we rearrange the expression for $h(t)$:\n$$h(t) = \\frac{\\sin\\left(\\pi \\frac{t}{T}\\right)}{\\frac{\\pi t}{T}} = \\operatorname{sinc}\\left(\\frac{t}{T}\\right)$$\nFor $t=0$, the value is found by taking the limit or from the integral: $h(0) = \\frac{1}{2\\pi}\\int_{-\\pi/T}^{\\pi/T} T \\,d\\omega = \\frac{T}{2\\pi} \\left( \\frac{2\\pi}{T} \\right) = 1$. This corresponds to $\\operatorname{sinc}(0)=1$.\n\nFinally, we show that the reconstructed signal $y(t)$ can be expressed as a convolutional superposition. The output $y(t)$ is the convolution of the input impulse train $s(t)$ and the filter's impulse response $h(t)$:\n$$y(t) = s(t) * h(t) = \\int_{-\\infty}^{\\infty} s(\\tau) h(t - \\tau) \\,d\\tau$$\nSubstituting the definition of $s(t) = \\sum_{n\\in\\mathbb{Z}} x[n] \\delta(t - nT)$:\n$$y(t) = \\int_{-\\infty}^{\\infty} \\left( \\sum_{n\\in\\mathbb{Z}} x[n] \\delta(\\tau - nT) \\right) h(t - \\tau) \\,d\\tau$$\nBy linearity, we can interchange the summation and integration:\n$$y(t) = \\sum_{n\\in\\mathbb{Z}} x[n] \\int_{-\\infty}^{\\infty} \\delta(\\tau - nT) h(t - \\tau) \\,d\\tau$$\nUsing the sifting property of the Dirac delta function, $\\int_{-\\infty}^{\\infty} f(\\tau) \\delta(\\tau - a) \\,d\\tau = f(a)$, with $f(\\tau) = h(t-\\tau)$ and $a=nT$:\n$$y(t) = \\sum_{n\\in\\mathbb{Z}} x[n] h(t - nT)$$\nThis completes the derivation of the Whittaker-Shannon interpolation formula.\n\n**Task 2: Numerical Evaluation**\n\nThe formula derived above is implemented by truncating the infinite sum to a symmetric window of $|n| \\le N$, where $N=2000$. The reconstructed signal at time $t$ is approximated by:\n$$y(t) \\approx \\sum_{n=-N}^{N} x[n] \\operatorname{sinc}\\left(\\frac{t - nT}{T}\\right)$$\nThe provided test cases are evaluated using this formula. The implementation utilizes vectorized operations for efficiency and `numpy.sinc` for a stable and accurate computation of the normalized sinc function.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the signal reconstruction problem for the three given test cases.\n    \"\"\"\n    N = 2000\n    n_range = np.arange(-N, N + 1)\n\n    # --- Test Case A ---\n    T_A = 1.0\n    phi_A = np.pi / 7.0\n    times_A = [0.0, 0.25, 0.5, 1.25, 2.0]\n    \n    # Generate the samples x[n] for case A\n    nT_A = n_range * T_A\n    x_n_A = np.cos(2 * np.pi * 0.2 * nT_A) + 0.5 * np.sin(2 * np.pi * 0.3 * nT_A + phi_A)\n    \n    results_A = []\n    for t in times_A:\n        # Calculate kernel values h(t - nT)\n        sinc_arg_A = (t - nT_A) / T_A\n        h_vals_A = np.sinc(sinc_arg_A)\n        # Compute y(t) as the dot product (sum of products)\n        y_t = np.dot(x_n_A, h_vals_A)\n        results_A.append(y_t)\n\n    # --- Test Case B ---\n    T_B = 1.0\n    times_B = [0.0, 0.5, 1.0, 1.5, 2.0]\n    \n    # Generate the samples x[n] for case B\n    nT_B = n_range * T_B\n    x_n_B = np.cos(2 * np.pi * 0.49 * nT_B)\n    \n    results_B = []\n    for t in times_B:\n        # Calculate kernel values h(t - nT)\n        sinc_arg_B = (t - nT_B) / T_B\n        h_vals_B = np.sinc(sinc_arg_B)\n        # Compute y(t)\n        y_t = np.dot(x_n_B, h_vals_B)\n        results_B.append(y_t)\n\n    # --- Test Case C ---\n    T_C = 0.8\n    theta_C = -np.pi / 5.0\n    times_C = [0.0, 0.8, 1.6, 2.4, 4.0]\n\n    # Generate the samples x[n] for case C\n    nT_C = n_range * T_C\n    x_n_C = 0.1 + 0.7 * np.cos(2 * np.pi * 0.25 * nT_C) + 0.4 * np.sin(2 * np.pi * 0.6 * nT_C + theta_C)\n\n    results_C = []\n    for t in times_C:\n        # Calculate kernel values h(t - nT)\n        sinc_arg_C = (t - nT_C) / T_C\n        h_vals_C = np.sinc(sinc_arg_C)\n        # Compute y(t)\n        y_t = np.dot(x_n_C, h_vals_C)\n        results_C.append(y_t)\n\n    all_results = [results_A, results_B, results_C]\n\n    # Format the output string according to the specification.\n    # The f-string format specifier .9f rounds to 9 decimal places using\n    # standard rounding (round half to even), as required.\n    output_str = f\"[{','.join([f'[{\",\".join([f\"{val:.9f}\" for val in res_list])}]' for res_list in all_results])}]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "2904292"}, {"introduction": "While the ideal 'brick-wall' filter is a powerful theoretical construct, it is not physically realizable. Real-world reconstruction systems must use practical, non-ideal filters, which introduces errors. This practice [@problem_id:1728147] explores this crucial aspect by tasking you with analyzing the performance of a simple first-order RC filter for reconstruction. By calculating the mean-squared error, you will learn to distinguish and quantify two primary sources of imperfection: in-band signal distortion and out-of-band noise from spectral replicas, providing vital insights into practical system design.", "problem": "A real-valued continuous-time, wide-sense stationary (WSS) random signal, $x_c(t)$, is known to be band-limited to a maximum angular frequency of $\\omega_M$. Its power spectral density is constant, $\\Phi_{xx}(\\omega) = N_0$, for $|\\omega| \\leq \\omega_M$ and is zero otherwise. This signal is sampled at the Nyquist rate, $T_s = \\pi/\\omega_M$, to produce the discrete-time sequence $x_d[n] = x_c(nT_s)$.\n\nAn attempt is made to reconstruct the original signal from its samples. The reconstruction filter is a scaled first-order Resistor-Capacitor (RC) low-pass filter, instead of an ideal brick-wall filter. The transfer function of the basic RC filter is $H(j\\omega) = \\frac{1}{1+j\\omega\\tau}$, where $\\tau$ is the time constant. For this specific application, two modifications are made:\n1. The filter's 3-dB cutoff frequency, $\\omega_c = 1/\\tau$, is set to be a fraction of the signal's maximum frequency, such that $\\omega_c = \\frac{1}{\\sqrt{3}} \\omega_M$.\n2. The filter is scaled by a gain constant $K$ so that its DC gain, $K H(0)$, matches the required DC gain of an ideal reconstruction filter, which is the sampling period $T_s$. The resulting reconstruction filter is $G(j\\omega) = K H(j\\omega)$.\n\nThe reconstructed signal, $x_r(t)$, is the output of this filter when its input is an impulse train weighted by the sample values, i.e., $x_s(t) = \\sum_{n=-\\infty}^{\\infty} x_d[n] \\delta(t - nT_s)$.\n\nDetermine the Normalized Mean-Squared Error (NMSE) of the reconstruction. The NMSE is defined as the ratio of the mean power of the error signal, $e(t) = x_r(t) - x_c(t)$, to the mean power of the original signal, $x_c(t)$. Your answer should be a single, closed-form analytic expression.", "solution": "The continuous-time WSS input has power spectral density (PSD) $\\Phi_{xx}(\\omega)=N_{0}$ for $|\\omega|\\leq \\omega_{M}$ and zero otherwise. The sampling period is $T_{s}=\\pi/\\omega_{M}$, so the sampling angular frequency is $\\omega_{s}=2\\pi/T_{s}=2\\omega_{M}$. The PSD of the impulse-sampled signal is\n$$\n\\Phi_{ss}(\\omega)=\\frac{1}{T_s^2}\\sum_{k=-\\infty}^{\\infty}\\Phi_{xx}(\\omega-k\\omega_{s}).\n$$\nThe reconstruction filter is $G(\\omega)=K H(\\omega)$ with $H(\\omega)=\\frac{1}{1+j\\omega\\tau}$. The parameter $\\omega_{c}=\\frac{1}{\\tau}=\\frac{1}{\\sqrt{3}}\\omega_{M}$ implies $\\tau=\\frac{\\sqrt{3}}{\\omega_{M}}$. The gain $K$ is set so that the DC gain matches the sampling period, $G(0)=T_s$. Since $H(0)=1$, this gives $K=T_s$. Therefore, the filter is:\n$$\nG(\\omega)=\\frac{T_{s}}{1+j\\omega\\tau}.\n$$\nThe error is $e(t) = x_r(t) - x_c(t)$. The error PSD $\\Phi_{ee}(\\omega)$ can be split into in-band and out-of-band contributions.\n\n**In-band error ($|\\omega| \\leq \\omega_M$):**\nIn this band, there is no aliasing, so the sampled signal's PSD is $\\Phi_{ss}(\\omega) = \\frac{1}{T_s^2}\\Phi_{xx}(\\omega)$. The effective transfer function from the original signal to the reconstructed signal is $G(\\omega)/T_s$. The error transfer function is $H_e(\\omega) = \\frac{G(\\omega)}{T_s} - 1 = \\frac{1}{1+j\\omega\\tau} - 1 = -\\frac{j\\omega\\tau}{1+j\\omega\\tau}$.\nThe in-band error PSD is:\n$$\n\\Phi_{ee}^{\\text{in}}(\\omega)=|H_e(\\omega)|^2 \\Phi_{xx}(\\omega)=\\frac{\\omega^{2}\\tau^{2}}{1+\\omega^{2}\\tau^{2}}\\,N_{0}, \\quad |\\omega|\\leq \\omega_{M}.\n$$\n\n**Out-of-band error ($|\\omega| > \\omega_M$):**\nIn this region, the original signal PSD $\\Phi_{xx}(\\omega)$ is zero. The error is the reconstructed signal itself, $e(t) = x_r(t)$. The error PSD is $\\Phi_{ee}^{\\text{out}}(\\omega) = |G(\\omega)|^2 \\Phi_{ss}(\\omega)$. Because the sampling is at the Nyquist rate, the spectral replicas of $\\Phi_{xx}$ tile the frequency axis without overlap. For any $\\omega$ where $|\\omega| > \\omega_M$, there is exactly one non-zero term in the sum for $\\Phi_{ss}(\\omega)$, and that term has a value of $N_0$. So, $\\Phi_{ss}(\\omega) = N_0/T_s^2$ in these regions.\n$$\n\\Phi_{ee}^{\\text{out}}(\\omega)=|G(\\omega)|^2 \\frac{N_0}{T_s^2} = \\left|\\frac{T_s}{1+j\\omega\\tau}\\right|^{2}\\frac{N_{0}}{T_s^2}=\\frac{1}{1+\\omega^{2}\\tau^{2}}\\,N_{0},\\quad \\text{for regions where replicas exist, i.e., } |\\omega| \\ge \\omega_M.\n$$\n\n**Total Error Power:**\nThe total mean-squared error power is $P_e = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\Phi_{ee}(\\omega) \\, d\\omega$.\n$$\nP_{e}=\\frac{N_0}{2\\pi}\\left[\\int_{-\\omega_{M}}^{\\omega_{M}}\\frac{\\omega^{2}\\tau^{2}}{1+\\omega^{2}\\tau^{2}}\\,d\\omega+\\int_{|\\omega|\\geq \\omega_{M}}\\frac{1}{1+\\omega^{2}\\tau^{2}}\\,d\\omega\\right].\n$$\nUsing the identity $\\frac{\\omega^{2}\\tau^{2}}{1+\\omega^{2}\\tau^{2}}=1-\\frac{1}{1+\\omega^{2}\\tau^{2}}$, and the standard integral $\\int\\frac{du}{1+u^2} = \\arctan(u)$, we have:\n$$\n\\int_{-\\omega_{M}}^{\\omega_{M}}\\frac{\\omega^{2}\\tau^{2}}{1+\\omega^{2}\\tau^{2}}\\,d\\omega = \\int_{-\\omega_{M}}^{\\omega_{M}} 1 \\,d\\omega - \\int_{-\\omega_{M}}^{\\omega_{M}} \\frac{1}{1+\\omega^{2}\\tau^{2}}\\,d\\omega = 2\\omega_{M}-\\frac{2}{\\tau}\\arctan(\\omega_{M}\\tau).\n$$\n$$\n\\int_{|\\omega|\\geq \\omega_{M}}\\frac{d\\omega}{1+\\omega^{2}\\tau^{2}} = \\int_{-\\infty}^{\\infty}\\frac{d\\omega}{1+\\omega^{2}\\tau^{2}} - \\int_{-\\omega_{M}}^{\\omega_{M}}\\frac{d\\omega}{1+\\omega^{2}\\tau^{2}} = \\frac{\\pi}{\\tau}-\\frac{2}{\\tau}\\arctan(\\omega_{M}\\tau).\n$$\nSumming the terms for the total power calculation:\n$$\nP_{e}=\\frac{N_{0}}{2\\pi}\\left[\\left(2\\omega_{M}-\\frac{2}{\\tau}\\arctan(\\omega_{M}\\tau)\\right)+\\left(\\frac{\\pi}{\\tau}-\\frac{2}{\\tau}\\arctan(\\omega_{M}\\tau)\\right)\\right] = \\frac{N_{0}}{2\\pi}\\left[2\\omega_{M}+\\frac{\\pi}{\\tau}-\\frac{4}{\\tau}\\arctan(\\omega_{M}\\tau)\\right].\n$$\n\n**NMSE Calculation:**\nThe original signal power is $P_{x}=\\frac{1}{2\\pi}\\int_{-\\omega_{M}}^{\\omega_{M}}N_{0}\\,d\\omega=\\frac{N_{0}\\omega_{M}}{\\pi}$.\nThe Normalized Mean-Squared Error (NMSE) is:\n$$\n\\text{NMSE}=\\frac{P_{e}}{P_{x}}=\\frac{\\pi}{N_0\\omega_M} \\cdot \\frac{N_{0}}{2\\pi}\\left[2\\omega_{M}+\\frac{\\pi}{\\tau}-\\frac{4}{\\tau}\\arctan(\\omega_{M}\\tau)\\right] = \\frac{1}{2\\omega_{M}}\\left[2\\omega_{M}+\\frac{\\pi}{\\tau}-\\frac{4}{\\tau}\\arctan(\\omega_{M}\\tau)\\right].\n$$\n$$\n\\text{NMSE}=1+\\frac{1}{2\\omega_{M}\\tau}\\left[\\pi-4\\arctan(\\omega_{M}\\tau)\\right].\n$$\nSubstituting $\\tau=\\sqrt{3}/\\omega_{M}$, we have $\\omega_{M}\\tau=\\sqrt{3}$ and $\\arctan(\\sqrt{3})=\\pi/3$:\n$$\n\\text{NMSE}=1+\\frac{1}{2\\sqrt{3}}\\left(\\pi-\\frac{4\\pi}{3}\\right)=1+\\frac{1}{2\\sqrt{3}}\\left(-\\frac{\\pi}{3}\\right) = 1-\\frac{\\pi}{6\\sqrt{3}}.\n$$", "answer": "$$\\boxed{1-\\frac{\\pi}{6\\sqrt{3}}}$$", "id": "1728147"}, {"introduction": "Beyond the realm of strictly bandlimited signals, many applications require interpolating data where the assumptions of the Shannon theorem do not hold, or where the global support of the sinc kernel is impractical. Cubic spline interpolation offers a powerful and flexible alternative, prized for its smoothness and local control. This exercise [@problem_id:2904294] guides you through implementing a natural cubic spline and numerically investigating its error characteristics as a function of signal frequency. This will deepen your understanding of how different interpolators perform and why splines are a cornerstone of modern numerical methods and computer graphics.", "problem": "You are given the task of assessing the interpolation error of a natural cubic spline interpolant applied to a sinusoidal signal. Consider the function $x(t) = \\sin(\\omega t)$ on the interval $[0,1]$, where angles are in radians. Let the sampling grid be $t_k = k h$ for $k \\in \\{0,1,\\dots,N\\}$ with $h = 1/N$. The sample sequence is $x_k = x(t_k) = \\sin(\\omega t_k)$. Using only the samples $\\{(t_k, x_k)\\}_{k=0}^{N}$, construct the natural cubic spline interpolant $S(t)$, which is the unique twice continuously differentiable piecewise cubic function on $[0,1]$ that interpolates the samples and satisfies the natural boundary conditions $S''(0) = 0$ and $S''(1) = 0$.\n\nYour program must:\n- Implement the natural cubic spline interpolation to obtain $S(t)$ from $\\{(t_k, x_k)\\}$.\n- For a dense evaluation grid of $M$ equally spaced points in $[0,1]$, compute the maximum absolute interpolation error\n$$\ne(\\omega) = \\max_{t \\in \\{0, \\frac{1}{M-1}, \\dots, 1\\}} \\left| \\sin(\\omega t) - S(t) \\right|.\n$$\n\nFundamental basis for reasoning: Use the definition of interpolation and standard smoothness properties of cubic splines, along with Taylorâ€™s theorem for functions with continuous derivatives, to justify the behavior and bounds of the interpolation error for smooth functions.\n\nTest suite and parameters:\n- Use $N = 64$ (so $h = 1/64$).\n- Use $M = 10001$ dense evaluation points on $[0,1]$.\n- Evaluate the error for the following values of $\\omega$ (in radians per unit time): $\\omega \\in \\{0, \\frac{\\pi}{4}, \\pi, 4\\pi, 12\\pi\\}$.\n\nOutput specification:\n- For each $\\omega$ in the order given, compute $e(\\omega)$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[e(\\omega_1),e(\\omega_2),\\dots]$.\n- Round each $e(\\omega)$ to $10$ decimal places before printing.\n\nAll computations are purely mathematical with angles in radians. No physical units are involved. The final output must be exactly one line in the specified format.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- Function to interpolate: $x(t) = \\sin(\\omega t)$\n- Interpolation interval: $[0, 1]$\n- Number of intervals: $N = 64$\n- Sampling step size: $h = 1/N = 1/64$\n- Sampling grid (nodes): $t_k = k h$ for $k \\in \\{0, 1, \\dots, N\\}$\n- Sample values: $x_k = x(t_k) = \\sin(\\omega t_k)$\n- Interpolant: Natural cubic spline $S(t)$\n- Boundary conditions: $S''(0) = 0$ and $S''(1) = 0$\n- Dense evaluation grid: $M = 10001$ equally spaced points in $[0, 1]$\n- Error metric: $e(\\omega) = \\max_{t \\in \\{0, \\frac{1}{M-1}, \\dots, 1\\}} \\left| \\sin(\\omega t) - S(t) \\right|$\n- Test cases: $\\omega \\in \\{0, \\frac{\\pi}{4}, \\pi, 4\\pi, 12\\pi\\}$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem concerns natural cubic spline interpolation of a sinusoidal function, a standard and well-understood topic in numerical analysis and signal processing. The function $x(t) = \\sin(\\omega t)$ is infinitely differentiable and poses no mathematical irregularities. This is scientifically sound.\n- **Well-Posed**: The construction of a unique natural cubic spline for a given set of distinct nodes is a classic well-posed problem. The parameters are fully specified, leading to a unique and computable solution for the interpolation error.\n- **Objective**: The problem is stated in precise, mathematical language, free from subjectivity or ambiguity.\n- **Completeness**: All necessary data ($N$, $M$, $\\omega$ values, function definition, boundary conditions) are provided. The problem is self-contained.\n- **Unrealistic or Infeasible**: The computational task is standard and feasible with modern numerical libraries. The parameters are reasonable.\n- **Ill-Posed or Poorly Structured**: The problem is clearly structured and leads to a unique, stable solution.\n- **Triviality**: The problem is non-trivial, requiring the implementation or use of a standard numerical algorithm for spline construction and error evaluation.\n- **Outside Scientific Verifiability**: The results are numerically computable and verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nA natural cubic spline $S(t)$ is a piecewise cubic function that interpolates a given set of data points $\\{(t_k, x_k)\\}_{k=0}^N$ and is twice continuously differentiable ($C^2$). For each subinterval $[t_k, t_{k+1}]$, where $k \\in \\{0, 1, \\dots, N-1\\}$, the spline $S(t)$ is a cubic polynomial, which we denote by $S_k(t)$.\n\nThe construction of $S(t)$ requires determining the coefficients of these $N$ cubic polynomials. This is achieved by imposing a set of conditions:\n1.  **Interpolation**: $S_k(t_k) = x_k$ and $S_k(t_{k+1}) = x_{k+1}$ for $k=0, \\dots, N-1$.\n2.  **Continuity of derivatives**: $S'_{k-1}(t_k) = S'_{k}(t_k)$ and $S''_{k-1}(t_k) = S''_{k}(t_k)$ for $k=1, \\dots, N-1$.\n\nLet $h = t_{k+1} - t_k$ be the uniform step size, and let $M_k = S''(t_k)$ be the second derivative of the spline at node $t_k$. Since $S_k(t)$ is cubic, $S''_k(t)$ is linear on $[t_k, t_{k+1}]$ and can be written as:\n$$ S''_k(t) = M_k \\frac{t_{k+1} - t}{h} + M_{k+1} \\frac{t - t_k}{h} $$\nIntegrating twice and applying the interpolation conditions $S_k(t_k) = x_k$ and $S_k(t_{k+1}) = x_{k+1}$ yields the expression for the spline on the $k$-th interval:\n$$ S_k(t) = M_k \\frac{(t_{k+1}-t)^3}{6h} + M_{k+1} \\frac{(t-t_k)^3}{6h} + \\left(\\frac{x_k}{h} - M_k\\frac{h}{6}\\right)(t_{k+1}-t) + \\left(\\frac{x_{k+1}}{h} - M_{k+1}\\frac{h}{6}\\right)(t-t_k) $$\nThis expression depends on the unknown second derivatives $M_0, M_1, \\dots, M_N$. To find these values, we enforce the continuity of the first derivative, $S'_{k-1}(t_k) = S'_{k}(t_k)$, for $k=1, \\dots, N-1$. This leads to a system of linear equations:\n$$ h M_{k-1} + 4h M_k + h M_{k+1} = \\frac{6}{h}(x_{k+1} - 2x_k + x_{k-1}) $$\nThis gives $N-1$ equations for $N+1$ unknowns ($M_0, \\dots, M_N$). Two additional constraints are required. The problem specifies *natural* boundary conditions, which are:\n$$ S''(0) = M_0 = 0 \\quad \\text{and} \\quad S''(1) = M_N = 0 $$\nSubstituting these into the system leaves $N-1$ equations for the interior unknowns $M_1, \\dots, M_{N-1}$. The resulting $(N-1) \\times (N-1)$ system has a tridiagonal matrix that is strictly diagonally dominant, guaranteeing a unique solution for the $M_k$ values. This system can be solved efficiently.\n\nOnce the values of $M_k$ are known, the spline $S(t)$ is fully determined. The program will evaluate $S(t)$ on a dense grid of $M$ points and compute the maximum absolute error against the true function $x(t) = \\sin(\\omega t)$.\n\nThe interpolation error $e(t) = x(t) - S(t)$ for a four-times continuously differentiable function $x(t)$ is known to be bounded. For clamped boundary conditions ($S'(t_0)=x'(t_0)$, $S'(t_N)=x'(t_N)$), the error is of order $O(h^4)$: $\\max|e(t)| \\le C h^4 \\|x^{(4)}\\|_\\infty$. For natural boundary conditions, the error is generally of order $O(h^2)$ due to potential mismatches between $S''(t_0)$, $S''(t_N)$ and $x''(t_0)$, $x''(t_N)$.\nIn our problem, $x(t) = \\sin(\\omega t)$. The second derivative is $x''(t) = -\\omega^2 \\sin(\\omega t)$.\nAt the boundaries:\n-   $x''(0) = -\\omega^2 \\sin(0) = 0$. The natural condition $S''(0)=0$ matches the true function's second derivative.\n-   $x''(1) = -\\omega^2 \\sin(\\omega)$. This is zero only if $\\omega = k\\pi$ for some integer $k$.\n\nFor the test cases $\\omega \\in \\{0, \\pi, 4\\pi, 12\\pi\\}$, we have $\\sin(\\omega) = 0$, so $x''(1)=0$. In these cases, the natural boundary conditions are perfectly compatible with the function itself, and we expect a smaller error, consistent with an $O(h^4)$ convergence behavior. The error magnitude is still expected to scale with $\\omega$, as $x^{(4)}(t) = \\omega^4 \\sin(\\omega t)$, so $\\|x^{(4)}\\|_\\infty = \\omega^4$.\nFor $\\omega = \\frac{\\pi}{4}$, $x''(1) \\ne 0$. The mismatch at $t=1$ where $S''(1)=0$ will introduce a larger boundary error, and we anticipate the overall error to be larger than for nearby cases where the boundary conditions match.\n\nThe numerical implementation will leverage `scipy.interpolate.CubicSpline`, which provides a robust and efficient implementation for constructing a natural cubic spline by specifying `bc_type='natural'`.", "answer": "```python\nimport numpy as np\nfrom scipy.interpolate import CubicSpline\n\ndef solve():\n    \"\"\"\n    Computes the maximum absolute interpolation error of a natural cubic spline\n    for the function x(t) = sin(omega*t) for several values of omega.\n    \"\"\"\n    # Test suite and parameters from the problem statement\n    N = 64\n    M = 10001\n    omega_values = [0.0, np.pi/4, np.pi, 4*np.pi, 12*np.pi]\n\n    results = []\n\n    # Coarse grid for interpolation nodes\n    t_k = np.linspace(0, 1, N + 1)\n    \n    # Dense grid for error evaluation\n    t_eval = np.linspace(0, 1, M)\n\n    for omega in omega_values:\n        # Trivial case: sin(0*t) = 0. The natural spline for zero data is\n        # the zero function, so the error is exactly 0.\n        if omega == 0.0:\n            results.append(0.0)\n            continue\n\n        # Generate sample points from the true function\n        x_k = np.sin(omega * t_k)\n\n        # Construct the natural cubic spline interpolant.\n        # The 'natural' boundary condition sets the second derivatives\n        # at the endpoints to zero, as required by the problem.\n        cs = CubicSpline(t_k, x_k, bc_type='natural')\n\n        # Evaluate the spline and the true function on the dense grid\n        s_eval = cs(t_eval)\n        x_eval = np.sin(omega * t_eval)\n\n        # Compute the maximum absolute error\n        max_abs_error = np.max(np.abs(x_eval - s_eval))\n        \n        results.append(max_abs_error)\n\n    # Format the output as specified: a list of errors rounded to 10 decimal places.\n    # The f-string formatting ensures each number has 10 digits after the decimal point.\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2904294"}]}