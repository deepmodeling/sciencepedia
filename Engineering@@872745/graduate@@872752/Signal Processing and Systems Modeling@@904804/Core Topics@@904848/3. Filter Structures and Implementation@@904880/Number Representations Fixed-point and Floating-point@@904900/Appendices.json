{"hands_on_practices": [{"introduction": "Fixed-point arithmetic is the bedrock of many high-performance and resource-constrained digital signal processing systems. A critical design decision involves choosing the number of integer bits, $m$, and fractional bits, $n$, for a given word length. This exercise ([@problem_id:2887760]) guides you through a first-principles derivation of a fixed-point format's dynamic range and resolution, forcing you to confront the fundamental trade-off between accommodating large signal values to prevent overflow and maintaining high precision to minimize quantization error.", "problem": "A uniformly scaled, signed fixed-point representation is used in a signal processing chain. The representation is two’s complement with $m$ integer bits (including the sign bit) and $n$ fractional bits, for a total word length $W=m+n$. Each bit position has binary weight consistent with two’s complement: the sign bit has weight $-2^{m-1}$, the remaining integer bits have weights $2^{m-2},\\dots,2^{0}$, and the fractional bits have weights $2^{-1},2^{-2},\\dots,2^{-n}$. Quantization is by rounding to the nearest representable value (mid-tread), and, when not explicitly stated otherwise, arithmetic is assumed not to saturate nor wrap; rather, one is concerned with whether the real-valued signal falls outside the representable set (overflow event). \n\nStarting from these definitions:\n1) Derive the resolution $\\Delta$, defined as the smallest positive difference between two adjacent representable numbers.\n2) Derive the maximal absolute representable value $V_{\\text{abs,max}}=\\max\\{|x|:x\\text{ is representable}\\}$, and state the closed interval of representable values.\n\nNow suppose the input signal is bounded in magnitude by $|x(t)|\\leq X_{\\max}$ and the total word length $W$ is fixed. Using only first principles of binary weighting and the high-resolution model for rounding quantization, reason about the trade-off between overflow and quantization error as the integer–fractional bit split $(m,n)$ varies under the constraint $m+n=W$. In particular, derive the condition on $(m,n)$ under which overflow cannot occur for the bounded signal, and explain how the mean-squared quantization error scales with $(m,n)$ through $\\Delta$.\n\nExpress your final answer as the ordered pair $\\left(\\Delta,\\,V_{\\text{abs,max}}\\right)$ in exact symbolic form. Do not round.", "solution": "The problem statement must first undergo a rigorous validation process.\n\nStep 1: Extract Givens.\n- Representation: Uniformly scaled, signed fixed-point, two’s complement.\n- Word length: $W=m+n$.\n- Integer bits: $m$ (including a sign bit).\n- Fractional bits: $n$.\n- Bit weights: Sign bit has weight $-2^{m-1}$, remaining integer bits have weights $2^{m-2}, \\dots, 2^{0}$, and fractional bits have weights $2^{-1}, 2^{-2}, \\dots, 2^{-n}$.\n- Quantization: Rounding to the nearest representable value (mid-tread).\n- Overflow: Occurs when a real-valued signal falls outside the representable set. Saturation and wrap-around are not considered.\n- Input signal: Bounded by $|x(t)| \\le X_{\\max}$.\n- Constraint: $W$ is a fixed total word length.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded and well-posed. It presents a standard, fundamental scenario in digital signal processing concerning the properties of fixed-point number systems. The definitions provided for two's complement fixed-point representation are standard and internally consistent. The problem is objective, using precise technical language. It does not violate any scientific principles, is not based on false premises, and contains all necessary information for a rigorous derivation. The problem is directly relevant to the topic of number representations in signal processing. The questions posed are structured to have unique, derivable answers based on first principles. The problem does not contain any of the flaws listed in the validation criteria.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A complete solution will be provided.\n\nA number $x$ in this fixed-point format is represented by a set of $W=m+n$ bits, denoted $(b_{m-1}, b_{m-2}, \\dots, b_0, b_{-1}, \\dots, b_{-n})$, where $b_i \\in \\{0, 1\\}$. The value of the number is given by the weighted sum of its bits:\n$$ x = -b_{m-1}2^{m-1} + \\sum_{i=0}^{m-2} b_i 2^i + \\sum_{j=1}^{n} b_{-j} 2^{-j} $$\n\n1) Derivation of Resolution $\\Delta$.\nThe resolution, $\\Delta$, is defined as the smallest positive difference between two adjacent representable numbers. This difference corresponds to a change in the least significant bit (LSB). In this representation, the LSB is the bit with the smallest weight, which is $b_{-n}$ with a weight of $2^{-n}$. Changing this bit from $0$ to $1$ while all other bits remain constant increases the value of the number by $2^{-n}$. Any other change in bits would result in a change of at least this magnitude. Therefore, the resolution is the weight of the LSB.\n$$ \\Delta = 2^{-n} $$\n\n2) Derivation of $V_{\\text{abs,max}}$ and the Representable Range.\nTo determine the range of representable values, we must find the minimum and maximum values that can be formed.\n\nThe maximum value, $x_{\\max}$, occurs when the sign bit $b_{m-1}$ is $0$ and all other bits are $1$:\n$$ x_{\\max} = -0 \\cdot 2^{m-1} + \\sum_{i=0}^{m-2} 1 \\cdot 2^i + \\sum_{j=1}^{n} 1 \\cdot 2^{-j} $$\nThe first sum is a geometric series for the integer part: $\\sum_{i=0}^{m-2} 2^i = \\frac{2^{m-1}-1}{2-1} = 2^{m-1}-1$.\nThe second sum is a geometric series for the fractional part: $\\sum_{j=1}^{n} 2^{-j} = \\frac{2^{-1}(1-(2^{-1})^n)}{1-2^{-1}} = 1-2^{-n}$.\nCombining these results:\n$$ x_{\\max} = (2^{m-1}-1) + (1-2^{-n}) = 2^{m-1} - 2^{-n} $$\n\nThe minimum value, $x_{\\min}$, occurs in two's complement representation when the sign bit $b_{m-1}$ is $1$ and all other bits are $0$:\n$$ x_{\\min} = -1 \\cdot 2^{m-1} + \\sum_{i=0}^{m-2} 0 \\cdot 2^i + \\sum_{j=1}^{n} 0 \\cdot 2^{-j} = -2^{m-1} $$\n\nThe closed interval of representable values is therefore $[x_{\\min}, x_{\\max}] = [-2^{m-1}, 2^{m-1} - 2^{-n}]$.\n\nThe maximal absolute representable value, $V_{\\text{abs,max}}$, is the maximum of the absolute values of the numbers in this interval. We compare the magnitudes of $x_{\\min}$ and $x_{\\max}$:\n$$ |x_{\\min}| = |-2^{m-1}| = 2^{m-1} $$\n$$ |x_{\\max}| = |2^{m-1} - 2^{-n}| = 2^{m-1} - 2^{-n} $$\nSince $n \\ge 0$, the term $2^{-n}$ is positive (assuming $n$ is finite), which means $|x_{\\max}|  |x_{\\min}|$.\nTherefore, the maximal absolute representable value is the magnitude of the most negative number:\n$$ V_{\\text{abs,max}} = 2^{m-1} $$\n\n3) Trade-off between Overflow and Quantization Error.\nThe total word length $W = m+n$ is fixed.\n\nOverflow: An overflow event occurs if the input signal $x(t)$ falls outside the representable range $[-2^{m-1}, 2^{m-1}-2^{-n}]$. Given that the input signal is bounded by $|x(t)| \\le X_{\\max}$, this means its values lie in the interval $[-X_{\\max}, X_{\\max}]$. To prevent overflow, this entire signal interval must be contained within the representable range:\n$$ [-X_{\\max}, X_{\\max}] \\subseteq [-2^{m-1}, 2^{m-1}-2^{-n}] $$\nThis requires satisfaction of two conditions:\n$$ X_{\\max} \\le 2^{m-1}-2^{-n} \\quad \\text{and} \\quad -X_{\\max} \\ge -2^{m-1} $$\nThe second condition is equivalent to $X_{\\max} \\le 2^{m-1}$. The first condition, $X_{\\max} \\le 2^{m-1}-2^{-n}$, is more restrictive. Thus, the rigorous condition on $(m,n)$ to guarantee that overflow cannot occur is:\n$$ X_{\\max} \\le 2^{m-1} - 2^{-n} $$\nTo accommodate a signal with a large maximum amplitude $X_{\\max}$, one must choose a sufficiently large number of integer bits, $m$. A larger $m$ expands the dynamic range of the representation.\n\nQuantization Error: Quantization by rounding to the nearest representable value introduces an error $e_q$ bounded by $|e_q| \\le \\frac{\\Delta}{2}$. The high-resolution quantization model assumes $e_q$ is a uniformly distributed random variable on the interval $[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$. The mean-squared error (MSE), or quantization noise power, is the variance of this distribution:\n$$ \\sigma_q^2 = E[e_q^2] = \\int_{-\\Delta/2}^{\\Delta/2} e^2 \\frac{1}{\\Delta} de = \\frac{1}{\\Delta} \\left[ \\frac{e^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{\\Delta^2}{12} $$\nSubstituting $\\Delta = 2^{-n}$, the MSE is:\n$$ \\sigma_q^2 = \\frac{(2^{-n})^2}{12} = \\frac{2^{-2n}}{12} $$\nTo minimize quantization error, $\\sigma_q^2$ must be small, which requires $\\Delta$ to be small. This is achieved by increasing the number of fractional bits, $n$.\n\nTrade-off: The constraint is $m+n=W$, where $W$ is constant.\n- To prevent overflow for a signal of a given amplitude $X_{\\max}$, one must allocate a sufficient number of bits to $m$. Increasing $m$ increases the representable dynamic range, reducing the risk of overflow.\n- To improve precision (i.e., reduce quantization noise), one must allocate more bits to $n$. Increasing $n$ reduces the step size $\\Delta$, thereby lowering the quantization error power $\\sigma_q^2$.\nBecause $m$ and $n$ are coupled by a fixed sum $W$, these two objectives are in direct conflict. Increasing $m$ to expand the dynamic range forces a decrease in $n$, which degrades precision by increasing $\\Delta$ and thus $\\sigma_q^2$. Conversely, increasing $n$ to improve precision forces a decrease in $m$, which shrinks the dynamic range and increases the likelihood of overflow. This represents the fundamental trade-off between dynamic range and precision in fixed-point system design. For a given total word length $W$, the engineer must select an $(m,n)$ split that provides sufficient dynamic range to avoid overflow for the expected signal levels, while providing acceptable precision for the application.", "answer": "$$\n\\boxed{(2^{-n}, 2^{m-1})}\n$$", "id": "2887760"}, {"introduction": "While floating-point formats like IEEE 754 offer a vast dynamic range, they are not immune to numerical pitfalls. This practice ([@problem_id:2887738]) explores one of the most classic and insidious issues: catastrophic cancellation, where the subtraction of two nearly equal numbers results in a dramatic loss of relative precision. You will analyze a specific case where a seemingly benign calculation fails, quantify the information loss, and derive a mathematically equivalent, yet numerically robust, alternative.", "problem": "A computation is performed in the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) binary64 format with round-to-nearest, ties-to-even, and correctly rounded square root. Recall that binary64 has a fixed precision of $p=53$ bits in the significand (including the implicit leading bit). The unit in the last place (ULP) at magnitude approximately $2^{k}$ is $2^{k-52}$. Let $\\operatorname{fl}(\\cdot)$ denote the correctly rounded binary64 result of a real operation.\n\nConsider the real quantities $r_{1}=\\sqrt{N+1}$ and $r_{2}=\\sqrt{N}$ with $N=2^{104}$. Define the binary64 numbers $a=\\operatorname{fl}(r_{1})$ and $b=\\operatorname{fl}(r_{2})$.\n\n(i) Using only the definitions above and general properties of ULP spacing and rounding to nearest, construct the explicit pair of binary64 numbers $a$ and $b$ and show that their subtraction exhibits catastrophic cancellation in the sense that $\\operatorname{fl}(a-b)$ does not resolve the true real difference $r_{1}-r_{2}$.\n\n(ii) To quantify the loss of significant bits in the subtraction, define the loss index\n$$\nK \\triangleq \\left\\lfloor \\log_{2}\\!\\left(\\frac{M}{D}\\right)\\right\\rfloor,\n$$\nwhere $M=\\max\\{|r_{1}|,|r_{2}|\\}$ and $D=|r_{1}-r_{2}|$. Compute $K$ exactly for the $N$ specified above.\n\n(iii) Recommend and justify a mathematically equivalent reformulation to compute $r_{1}-r_{2}$ that avoids catastrophic cancellation in binary64, deriving it from first principles rather than quoting a known formula, and explain (qualitatively, in terms of ULPs and rounding to nearest) why the reformulated computation is numerically stable.\n\nProvide as your final answer only the value of $K$ from part (ii). No rounding instruction is needed because the result is an exact integer with no physical units.", "solution": "The problem will be validated against the specified criteria before a solution is attempted.\n\nStep 1: Extract Givens\n- Computation format: Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) binary64.\n- Rounding mode: round-to-nearest, ties-to-even.\n- Square root: correctly rounded, denoted $\\operatorname{fl}(\\sqrt{\\cdot})$.\n- Significand precision: $p=53$ bits (including the implicit leading bit).\n- Unit in the last place (ULP): For a magnitude of approximately $2^{k}$, the ULP is $2^{k-52}$.\n- Floating-point rounding function: $\\operatorname{fl}(\\cdot)$.\n- Real quantities: $r_{1}=\\sqrt{N+1}$ and $r_{2}=\\sqrt{N}$.\n- Constant: $N=2^{104}$.\n- Floating-point numbers: $a=\\operatorname{fl}(r_{1})$ and $b=\\operatorname{fl}(r_{2})$.\n- Loss index definition: $K \\triangleq \\left\\lfloor \\log_{2}\\!\\left(\\frac{M}{D}\\right)\\right\\rfloor$, with $M=\\max\\{|r_{1}|,|r_{2}|\\}$ and $D=|r_{1}-r_{2}|$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the standard principles of numerical analysis and floating-point arithmetic as defined by IEEE 754. It is well-posed, providing all necessary parameters ($N$, $p$, rounding mode) to uniquely determine the quantities in question. The problem statement is objective and uses precise, formal language. It is a standard, non-trivial problem designed to illustrate the phenomenon of catastrophic cancellation and its mitigation, directly relevant to the topic of number representations. No flaws from the checklist are present.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\nThe solution proceeds by addressing each of the three parts of the problem.\n\n(i) Explicit construction of $a$ and $b$ and demonstration of catastrophic cancellation.\n\nFirst, we analyze the quantity $r_{2} = \\sqrt{N} = \\sqrt{2^{104}} = 2^{52}$. In the binary64 format, a number is represented as $\\pm (1.f)_{2} \\times 2^{e_{\\text{biased}}-1023}$. The value $2^{52}$ can be written as $1.0 \\times 2^{52}$. This representation has a significand of $1$ followed by all zeros and an exponent of $52$. It is an exact binary floating-point number. Therefore, its representation in binary64 is exact: $b = \\operatorname{fl}(r_{2}) = \\operatorname{fl}(2^{52}) = 2^{52}$.\n\nNext, we analyze the quantity $r_{1} = \\sqrt{N+1} = \\sqrt{2^{104}+1}$. We use the binomial expansion for $\\sqrt{1+x}$ where $x$ is small: $\\sqrt{1+x} = 1 + \\frac{1}{2}x - \\frac{1}{8}x^2 + O(x^3)$.\n$r_{1} = \\sqrt{2^{104}(1 + 2^{-104})} = 2^{52}\\sqrt{1 + 2^{-104}}$.\nLetting $x = 2^{-104}$, we have:\n$r_{1} = 2^{52} \\left(1 + \\frac{1}{2}(2^{-104}) - \\frac{1}{8}(2^{-104})^2 + \\dots \\right) = 2^{52} \\left(1 + 2^{-105} - 2^{-3} \\cdot 2^{-208} + \\dots \\right) = 2^{52} + 2^{-53} - 2^{-159} + \\dots$.\nThe value of $r_{1}$ is slightly greater than $2^{52}$. We must determine how it rounds to a binary64 number. The floating-point number $b$ is $2^{52}$. We need to find the next representable binary64 number greater than $b$.\nThe unit in the last place for numbers with magnitude around $2^{52}$ is $\\operatorname{ulp}(2^{52})$. The exponent is $E=52$. The precision is $p=53$. The ULP is given by $2^{E-(p-1)} = 2^{52-(53-1)} = 2^{0} = 1$.\nThe representable numbers around $2^{52}$ are therefore spaced by $1$. The two binary64 numbers that bracket $r_{1}$ are $y_{\\text{low}} = 2^{52}$ and $y_{\\text{high}} = 2^{52}+1$.\nThe rounding mode is round-to-nearest. The midpoint between $y_{\\text{low}}$ and $y_{\\text{high}}$ is $m = \\frac{y_{\\text{low}}+y_{\\text{high}}}{2} = \\frac{2^{52} + (2^{52}+1)}{2} = 2^{52} + \\frac{1}{2} = 2^{52} + 2^{-1}$.\nOur value is $r_{1} = 2^{52} + 2^{-53} - 2^{-159} + \\dots$. Since $2^{-53}  2^{-1}$, it is clear that $r_{1}  m$.\nTherefore, $r_{1}$ rounds down to the nearest representable number, which is $y_{\\text{low}}$.\nSo, $a = \\operatorname{fl}(r_{1}) = 2^{52}$.\nWe have thus constructed the explicit pair: $a = 2^{52}$ and $b = 2^{52}$.\nThe computed subtraction is $\\operatorname{fl}(a-b) = \\operatorname{fl}(2^{52}-2^{52}) = \\operatorname{fl}(0) = 0$.\nThe true real difference is $r_{1}-r_{2} = \\sqrt{2^{104}+1} - 2^{52}$. From our expansion, this is $r_{1}-r_{2} = 2^{-53} - 2^{-159} + \\dots$, which is a small positive number approximately equal to $2^{-53}$.\nThe computed result is $0$, while the true result is non-zero. The relative error is essentially infinite. This demonstrates catastrophic cancellation: the initial rounding of $r_1$ and $r_2$ into $a$ and $b$ loses all information about their small difference. The subtraction $\\operatorname{fl}(a-b)$ fails to resolve the true difference $r_1-r_2$.\n\n(ii) Computation of the loss index $K$.\n\nThe loss index is defined as $K = \\left\\lfloor \\log_{2}\\!\\left(\\frac{M}{D}\\right)\\right\\rfloor$.\nWe have $M = \\max\\{|r_{1}|,|r_{2}|\\} = \\max\\{\\sqrt{N+1}, \\sqrt{N}\\} = \\sqrt{N+1}$.\nAnd $D = |r_{1}-r_{2}| = \\sqrt{N+1} - \\sqrt{N}$.\nThe ratio is $\\frac{M}{D} = \\frac{\\sqrt{N+1}}{\\sqrt{N+1} - \\sqrt{N}}$.\nTo simplify this expression, we multiply the numerator and denominator of the fraction $\\frac{1}{D}$ by the conjugate of its denominator:\n$D = \\frac{(\\sqrt{N+1} - \\sqrt{N})(\\sqrt{N+1} + \\sqrt{N})}{\\sqrt{N+1} + \\sqrt{N}} = \\frac{(N+1) - N}{\\sqrt{N+1} + \\sqrt{N}} = \\frac{1}{\\sqrt{N+1} + \\sqrt{N}}$.\nThus, $\\frac{M}{D} = M \\cdot (\\sqrt{N+1} + \\sqrt{N}) = \\sqrt{N+1}(\\sqrt{N+1} + \\sqrt{N}) = (N+1) + \\sqrt{N(N+1)}$.\nSubstituting $N=2^{104}$:\n$\\frac{M}{D} = (2^{104}+1) + \\sqrt{2^{104}(2^{104}+1)} = 2^{104}+1 + \\sqrt{2^{208}+2^{104}}$.\nWe need to find the integer part of the base-$2$ logarithm of this quantity. Let $Y = \\frac{M}{D}$.\nWe establish bounds for $Y$.\nFor a lower bound:\n$\\sqrt{2^{208}+2^{104}} > \\sqrt{2^{208}} = 2^{104}$.\nSo, $Y > (2^{104}+1) + 2^{104} = 2 \\cdot 2^{104} + 1 = 2^{105} + 1$.\nFor an upper bound, consider the identity $(x+y)^{2} = x^2+2xy+y^2$. Let $x=2^{104}$ and $y=1/2=2^{-1}$.\n$(2^{104} + 2^{-1})^2 = (2^{104})^2 + 2(2^{104})(2^{-1}) + (2^{-1})^2 = 2^{208} + 2^{104} + \\frac{1}{4}$.\nSince $2^{208}+2^{104}  2^{208}+2^{104}+1/4$, we have $\\sqrt{2^{208}+2^{104}}  \\sqrt{(2^{104}+2^{-1})^2} = 2^{104}+2^{-1}$.\nSo, $Y  (2^{104}+1) + (2^{104}+2^{-1}) = 2 \\cdot 2^{104} + 1.5 = 2^{105} + 1.5$.\nWe have established the strict bounds $2^{105} + 1  Y  2^{105} + 1.5$.\nTaking the base-$2$ logarithm:\n$\\log_{2}(2^{105}+1)  \\log_{2}(Y)  \\log_{2}(2^{105}+1.5)$.\nSince $2^{105}  2^{105}+1$, we have $\\log_{2}(2^{105})  \\log_{2}(2^{105}+1)$, which means $105  \\log_{2}(Y)$.\nAlso, since $2^{105}+1.5  2^{106}$, we have $\\log_{2}(Y)  \\log_{2}(2^{106}) = 106$.\nSo, $105  \\log_{2}(Y)  106$.\nThe floor of this value is therefore $\\lfloor \\log_{2}(Y) \\rfloor = 105$.\n$K=105$.\n\n(iii) Reformulation and justification.\n\nThe catastrophic cancellation in $r_{1}-r_{2} = \\sqrt{N+1} - \\sqrt{N}$ arises from the subtraction of two nearly identical large numbers. To avoid this, we must reformulate the expression to eliminate the subtraction. This can be derived by multiplying and dividing by the conjugate expression, $\\sqrt{N+1}+\\sqrt{N}$:\n$r_{1}-r_{2} = (\\sqrt{N+1} - \\sqrt{N}) \\times \\frac{\\sqrt{N+1}+\\sqrt{N}}{\\sqrt{N+1}+\\sqrt{N}} = \\frac{(N+1)-N}{\\sqrt{N+1}+\\sqrt{N}} = \\frac{1}{\\sqrt{N+1}+\\sqrt{N}}$.\nThis is the recommended reformulation.\n\nJustification: The reformulated expression is numerically stable because it replaces the problematic subtraction with a benign addition.\nLet us analyze the computation of $y = \\frac{1}{\\sqrt{N+1}+\\sqrt{N}}$ in binary64.\n1. Compute $a = \\operatorname{fl}(\\sqrt{N+1})$ and $b = \\operatorname{fl}(\\sqrt{N})$. As shown in part (i), for $N=2^{104}$, both of these round to the same value, $2^{52}$.\n2. Compute the sum $S = \\operatorname{fl}(a+b) = \\operatorname{fl}(2^{52}+2^{52}) = \\operatorname{fl}(2 \\cdot 2^{52}) = \\operatorname{fl}(2^{53})$. This operation is exact, so $S=2^{53}$.\n3. Compute the final result $R = \\operatorname{fl}(1/S) = \\operatorname{fl}(1/2^{53}) = \\operatorname{fl}(2^{-53})$. The value $2^{-53}$ is a power of two and is exactly representable as a normal binary64 number. So the final computed result is exactly $R=2^{-53}$.\n\nThe true value is $D = \\frac{1}{\\sqrt{2^{104}+1}+2^{52}}$. As shown in part (i), $\\sqrt{2^{104}+1} = 2^{52}+2^{-53}-\\dots$.\nSo, $D = \\frac{1}{(2^{52}+2^{-53}-\\dots)+2^{52}} = \\frac{1}{2^{53}+2^{-53}-\\dots}$.\nThis is extremely close to $\\frac{1}{2^{53}} = 2^{-53}$.\nThe relative error of the reformulated computation is approximately $\\left| \\frac{D-R}{D} \\right| \\approx \\left| \\frac{(2^{-53}(1-2^{-106})) - 2^{-53}}{2^{-53}(1-2^{-106})} \\right| \\approx 2^{-106}$, which is extremely small.\nQualitatively, the original form $\\operatorname{fl}(\\operatorname{fl}(\\sqrt{N+1}) - \\operatorname{fl}(\\sqrt{N}))$ fails because the true difference $|r_1-r_2| \\approx 2^{-53}$ is smaller than the rounding error in the computation of the individual roots, which is on the order of $\\operatorname{ulp}(\\sqrt{N}) = \\operatorname{ulp}(2^{52}) = 1$. All significant bits of the true difference are lost.\nIn contrast, the reformulated expression calculates $\\operatorname{fl}(1 / (\\operatorname{fl}(\\sqrt{N+1}) + \\operatorname{fl}(\\sqrt{N})))$. The addition $\\sqrt{N+1}+\\sqrt{N}$ is numerically stable as it adds two large, positive, nearly equal numbers. The relative error of the sum is small. The subsequent division is also a stable operation. The new algorithm preserves the information and yields a highly accurate result.", "answer": "$$\n\\boxed{105}\n$$", "id": "2887738"}, {"introduction": "The ultimate test of understanding is the ability to build. This capstone practice ([@problem_id:2887709]) moves beyond analysis and challenges you to implement a high-fidelity simulation framework for a fixed-point digital filter from the ground up. You will translate the abstract models of rounding and saturation into concrete code, gaining practical experience in modeling the complex, nonlinear behaviors that arise from finite-precision effects at every stage of a recursive algorithm.", "problem": "Design and implement a complete, runnable program that simulates fixed-point quantization at each operation inside a discrete-time linear time-invariant algorithm. The algorithm to be simulated is a first-order direct-form difference equation with one feedforward delay and one feedback delay. The framework must let you insert quantizers at well-defined operation sites and must model both rounding and saturation exactly.\n\nThe fundamental base you must start from consists of the following core definitions.\n\n1. A signed fixed-point format is specified by a word length $W \\in \\mathbb{Z}_{\\ge 2}$ and a fractional bit count $F \\in \\mathbb{Z}_{\\ge 0}$, commonly denoted by $Q(W,F)$. Real numbers are represented as integers scaled by $2^{-F}$ with two's-complement saturation bounds given by\n$$\nx_{\\min} \\,=\\, -\\frac{2^{W-1}}{2^F}, \\quad x_{\\max} \\,=\\, \\frac{2^{W-1}-1}{2^F}.\n$$\nA quantizer $Q_{W,F}(\\cdot)$ maps any real input $x \\in \\mathbb{R}$ to the nearest representable value in $Q(W,F)$ with rounding to nearest, ties to even, followed by saturation to $[x_{\\min}, x_{\\max}]$.\n\n2. To implement rounding to nearest, ties to even, apply the rule in the scaled integer domain: let $s = x \\cdot 2^F$. Map $s$ to $r = \\mathrm{round\\_to\\_nearest\\_even}(s)$, then clip $r$ to the integer interval $[ -2^{W-1},\\, 2^{W-1}-1 ]$, and finally return $r / 2^F$. Saturation occurs exactly when $r$ falls outside the integer interval before clipping.\n\n3. A first-order direct-form difference equation with normalized leading coefficient is defined by real coefficients $b_0, b_1, a_1 \\in \\mathbb{R}$ and the recursion\n$$\ny[n] \\,=\\, b_0\\,x[n] \\,+\\, b_1\\,x[n-1] \\,-\\, a_1\\,y[n-1],\n$$\nfor $n \\in \\{0,1,2,\\dots\\}$ with initial conditions $x[-1]=0$ and $y[-1]=0$. All signals are dimensionless real-valued sequences. A real-valued reference output is the sequence produced by evaluating the recursion in $\\mathbb{R}$ with no quantization anywhere.\n\nYour task is to design a simulation framework that inserts a fixed-point quantizer at specific operation sites in the algorithm. A site-selection vector of five booleans determines whether a quantizer is applied at each of the following sites, in this order:\n- input sample site: quantize $x[n]$ before it is used at time $n$,\n- multiply site: quantize the result of each scalar multiplication,\n- add site: quantize the result of each addition/subtraction,\n- state update site: quantize the internal state $y[n]$ that will be stored for the next time step,\n- output site: quantize the final $y[n]$ returned as output.\n\nIf a site is disabled, the corresponding intermediate passes through without quantization. Saturation events must be counted across all enabled quantizer sites; each time a clip to $x_{\\min}$ or $x_{\\max}$ occurs, increment a global saturation counter by $1$.\n\nImplement this framework and apply it to the following test suite. For each case, you must run both the quantized simulation (with the specified sites enabled) and the real-valued reference recursion (with no quantization) when required, and then compute the specified metric.\n\nTest case $1$ (happy path with all sites enabled, moderate precision):\n- Fixed-point format: $Q(W,F)$ with $W=8$, $F=6$.\n- Rounding mode: nearest, ties to even; saturation: two's-complement bounds as above.\n- Enabled quantizer sites: input, multiply, add, state update, output.\n- Coefficients: $b_0=0.625$, $b_1=0.375$, $a_1=-0.3$.\n- Input sequence $x[n]$ for $n=0,\\dots,19$: $[\\,0.5,\\,-0.5,\\,0.25,\\,-0.25,\\,1.0,\\,-1.0,\\,0.75,\\,-0.75,\\,0.125,\\,-0.125,\\,0.0,\\,0.6,\\,-0.6,\\,0.9,\\,-0.9,\\,1.5,\\,-1.5,\\,0.03125,\\,-0.03125,\\,0.0\\,]$.\n- Metric to report: the maximum absolute error between the quantized output and the real-valued reference output over $n=0,\\dots,19$, expressed as a real number.\n\nTest case $2$ (saturation stress with all sites enabled, coarse precision):\n- Fixed-point format: $Q(W,F)$ with $W=4$, $F=2$.\n- Rounding mode: nearest, ties to even; saturation: two's-complement bounds as above.\n- Enabled quantizer sites: input, multiply, add, state update, output.\n- Coefficients: $b_0=1.0$, $b_1=1.0$, $a_1=-0.5$.\n- Input sequence $x[n]$ for $n=0,\\dots,4$: $[\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,]$.\n- Metric to report: the total number of saturation events encountered across all enabled sites during the simulation, expressed as a nonnegative integer.\n\nTest case $3$ (rounding tie behavior at output only, identity algorithm):\n- Fixed-point format: $Q(W,F)$ with $W=5$, $F=2$.\n- Rounding mode: nearest, ties to even; saturation: two's-complement bounds as above.\n- Enabled quantizer sites: output only; the algorithm is the identity $y[n]=x[n]$ with no other operations.\n- Input sequence $x[n]$ for $n=0,\\dots,4$: $[\\,(-2+0.5)/4,\\,(-1+0.5)/4,\\,(0+0.5)/4,\\,(1+0.5)/4,\\,(2+0.5)/4\\,]$.\n- Metric to report: the list of integer-scaled outputs $r[n]$ such that $r[n] = y_q[n]\\cdot 2^F$, where $y_q[n]$ is the quantized output. Each $r[n]$ must be reported as an integer, leveraging that $y_q[n]$ is exactly representable on the $Q(W,F)$ grid.\n\nFinal output format specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the three test-case metrics: for test case $1$ a real number, for test case $2$ a nonnegative integer, and for test case $3$ a list of integers. For example, a syntactically valid output would look like $[\\,0.001234,\\,7,\\,[\\,\\dots\\,]\\,]$ with the actual values determined by your computation. No extra text or lines should be printed. Angles do not occur in this problem, and no physical units are involved; all quantities are dimensionless real numbers or integers as specified. Ensure that your implementation is self-contained and uses only the specified language and libraries.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It is scientifically grounded in digital signal processing principles, well-posed with unambiguous definitions and sufficient data, and objective in its formulation. The problem is a standard exercise in modeling finite-precision effects in discrete-time systems and is free of any logical contradictions, factual errors, or infeasible requirements. We may therefore proceed with the solution.\n\nThe core of the task is to construct a simulation framework for a first-order linear time-invariant (LTI) system, accounting for fixed-point quantization effects at various computational stages. The solution is structured into two main components: a generalized fixed-point quantizer and a simulation loop that implements the specified difference equation dataflow.\n\n**1. Fixed-Point Quantizer Model**\n\nA signed fixed-point number format, denoted $Q(W,F)$, is defined by a word length $W$ and a fractional bit count $F$. A real number $x \\in \\mathbb{R}$ is represented by an integer $r$ scaled by a factor of $2^{-F}$. The representable range is determined by the two's complement integer range $[I_{\\min}, I_{\\max}]$, where $I_{\\min} = -2^{W-1}$ and $I_{\\max} = 2^{W-1}-1$. The corresponding real-valued range is $[x_{\\min}, x_{\\max}]$, with $x_{\\min} = I_{\\min} \\cdot 2^{-F}$ and $x_{\\max} = I_{\\max} \\cdot 2^{-F}$.\n\nThe quantization function $Q_{W,F}(x)$ maps a real input $x$ to this format. The process is defined as follows:\n1.  **Scaling**: The real number $x$ is scaled to the integer domain: $s = x \\cdot 2^F$.\n2.  **Rounding**: The scaled value $s$ is rounded to the nearest integer. In cases where $s$ is exactly halfway between two integers (a tie), it is rounded to the nearest even integer. This is the \"round half to even\" rule. Let the result be $r = \\mathrm{round\\_half\\_to\\_even}(s)$.\n3.  **Saturation (Clipping)**: The integer $r$ is clipped to the representable range $[I_{\\min}, I_{\\max}]$. Let $r_{clipped} = \\max(I_{\\min}, \\min(I_{\\max}, r))$. A saturation event is registered if and only if $r_{clipped} \\neq r$.\n4.  **Unscaling**: The final quantized value is obtained by scaling the clipped integer back to the real domain: $x_q = r_{clipped} \\cdot 2^{-F}$.\n\nThis entire procedure is encapsulated within a reusable software component (a class in the implementation) that is initialized with $W$ and $F$ and provides a method to perform quantization, while also tracking saturation events via an external counter.\n\n**2. Difference Equation Simulation Dataflow**\n\nThe system to be simulated is the first-order difference equation:\n$$\ny[n] \\,=\\, b_0\\,x[n] \\,+\\, b_1\\,x[n-1] \\,-\\, a_1\\,y[n-1]\n$$\nwith initial conditions $x[-1]=0$ and $y[-1]=0$. The simulation must model quantization at five specific sites, controlled by a boolean vector. The dataflow at each time step $n$ is as follows, where $Q(\\cdot)$ denotes a call to the quantizer if the corresponding site is enabled. The state from the previous step is maintained in memory variables $x_{mem}$ (representing $x[n-1]$) and $y_{mem}$ (representing $y[n-1]$).\n\n1.  **Input Quantization (Site 1)**: The current input sample $x[n]$ is quantized.\n    $$\n    x_q[n] = Q_{input}(x[n])\n    $$\n2.  **Multiplication Quantization (Site 2)**: The three products in the equation are computed and immediately quantized.\n    $$\n    t_0[n] = Q_{mul}(b_0 \\cdot x_q[n])\n    $$\n    $$\n    t_1[n] = Q_{mul}(b_1 \\cdot x_{mem})\n    $$\n    $$\n    t_2[n] = Q_{mul}(-a_1 \\cdot y_{mem})\n    $$\n3.  **Addition Quantization (Site 3)**: The terms are summed, with quantization after each of the two additions.\n    $$\n    s_1[n] = Q_{add}(t_0[n] + t_1[n])\n    $$\n    $$\n    y_{pre}[n] = Q_{add}(s_1[n] + t_2[n])\n    $$\n    The value $y_{pre}[n]$ is the temporary result before it is directed to the output and the feedback path.\n\n4.  **State Update and Output Quantization (Sites 4 and 5)**: The value $y_{pre}[n]$ is the source for both the next state and the current output. These paths have their own, independent quantization sites.\n    -   The new state for the feedback delay is calculated by quantizing $y_{pre}[n]$ at the State Update site:\n        $$\n        y_{mem, new} = Q_{state}(y_{pre}[n])\n        $$\n    -   The final output for the current step is calculated by quantizing $y_{pre}[n]$ at the Output site:\n        $$\n        y_{out}[n] = Q_{output}(y_{pre}[n])\n        $$\n5.  **State Register Update**: The memory registers for the next time step $(n+1)$ are updated.\n    $$\n    x_{mem} \\leftarrow x_q[n]\n    $$\n    $$\n    y_{mem} \\leftarrow y_{mem, new}\n    $$\nThis sequence is repeated for all samples in the input signal.\n\n**3. Application to Test Cases**\n\nThe described framework is applied to each test case.\n\n-   **Test Case 1**: We require the maximum absolute error between the quantized and ideal outputs. Two simulations are run for $n=0, \\dots, 19$: one with all quantization sites enabled ($W=8, F=6$) to generate $y_q[n]$, and a reference simulation with all sites disabled (equivalent to infinite precision floating-point arithmetic) to generate $y_{ref}[n]$. The metric is $\\max_{n} |y_q[n] - y_{ref}[n]|$.\n\n-   **Test Case 2**: We simulate the system with a coarse format ($W=4, F=2$) and inputs designed to cause overflow. All quantization sites are enabled. The simulation loop proceeds as described above, and the single metric reported is the total count of saturation events accumulated across all operations and time steps.\n\n-   **Test Case 3**: This case is designed to test the \"round half to even\" rule. The algorithm is a simple identity $y[n]=x[n]$, with only the output site quantization enabled. For each input sample $x[n]$, we compute $y_q[n] = Q_{5,2}(x[n])$. The required metric is the list of integer-scaled outputs $r[n] = y_q[n] \\cdot 2^F$. This is equivalent to the integer value $r_{clipped}$ from the quantization procedure, which is obtained and reported for each input sample.\n\nThe implementation will strictly follow this design to produce the required results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    class Quantizer:\n        \"\"\"\n        Implements a signed fixed-point quantizer with saturation.\n        \"\"\"\n        def __init__(self, W, F):\n            if not (isinstance(W, int) and W >= 2):\n                raise ValueError(\"W must be an integer >= 2.\")\n            if not (isinstance(F, int) and F >= 0):\n                raise ValueError(\"F must be an integer >= 0.\")\n            \n            self.W = W\n            self.F = F\n            self.scale = 2.0**F\n            \n            self.int_min = -(2**(W - 1))\n            self.int_max = 2**(W - 1) - 1\n\n        def quantize(self, x, sat_counter):\n            \"\"\"\n            Quantizes a real value x to the Q(W,F) format.\n            - Rounds to nearest, ties to even.\n            - Clips to the representable range (saturation).\n            - Increments sat_counter[0] if saturation occurs.\n            \"\"\"\n            s = x * self.scale\n            # numpy.round implements round-half-to-even\n            r = int(np.round(s))\n            \n            r_clipped = max(self.int_min, min(self.int_max, r))\n            \n            if r != r_clipped:\n                sat_counter[0] += 1\n            \n            return float(r_clipped) / self.scale\n\n    def run_simulation(W, F, coeffs, x_in, sites):\n        \"\"\"\n        Simulates the first-order difference equation with fixed-point quantization.\n        \n        Args:\n            W (int): Word length.\n            F (int): Fractional bits.\n            coeffs (dict): Filter coefficients {b0, b1, a1}.\n            x_in (list): Input signal sequence.\n            sites (list[bool]): 5-element boolean list for enabling quantizer sites.\n            \n        Returns:\n            tuple: (list of output samples, final saturation count).\n        \"\"\"\n        quantizer = Quantizer(W, F) if any(sites) else None\n        sat_counter = [0]\n        \n        # Unpack site booleans for clarity\n        site_in, site_mul, site_add, site_state, site_out = sites\n\n        # Helper to conditionally quantize\n        def _quantize(val, site_enabled):\n            if site_enabled and quantizer:\n                return quantizer.quantize(val, sat_counter)\n            return val\n\n        x_mem = 0.0\n        y_mem = 0.0\n        y_out_list = []\n\n        for x_n in x_in:\n            # 1. Input Quantization\n            x_q = _quantize(x_n, site_in)\n\n            # 2. Multiplication Quantization\n            t0 = _quantize(coeffs['b0'] * x_q, site_mul)\n            t1 = _quantize(coeffs['b1'] * x_mem, site_mul)\n            t2 = _quantize(-coeffs['a1'] * y_mem, site_mul)\n            \n            # 3. Addition Quantization\n            s1 = _quantize(t0 + t1, site_add)\n            y_pre = _quantize(s1 + t2, site_add)\n            \n            # 5. Output Quantization (before state update)\n            y_out = _quantize(y_pre, site_out)\n            y_out_list.append(y_out)\n\n            # 4. State Update Quantization\n            y_mem_new = _quantize(y_pre, site_state)\n            \n            # Update state registers for next iteration\n            x_mem = x_q\n            y_mem = y_mem_new\n            \n        return y_out_list, sat_counter[0]\n\n    # --- Test Case 1 ---\n    W1, F1 = 8, 6\n    coeffs1 = {'b0': 0.625, 'b1': 0.375, 'a1': -0.3}\n    x_in1 = [0.5, -0.5, 0.25, -0.25, 1.0, -1.0, 0.75, -0.75, 0.125, -0.125,\n             0.0, 0.6, -0.6, 0.9, -0.9, 1.5, -1.5, 0.03125, -0.03125, 0.0]\n    sites1_quantized = [True] * 5\n    sites1_real = [False] * 5\n\n    y_q, _ = run_simulation(W1, F1, coeffs1, x_in1, sites1_quantized)\n    y_ref, _ = run_simulation(W1, F1, coeffs1, x_in1, sites1_real)\n    \n    max_abs_error = np.max(np.abs(np.array(y_q) - np.array(y_ref)))\n    \n    # --- Test Case 2 ---\n    W2, F2 = 4, 2\n    coeffs2 = {'b0': 1.0, 'b1': 1.0, 'a1': -0.5}\n    x_in2 = [2.0, 2.0, 2.0, 2.0, 2.0]\n    sites2 = [True] * 5\n    \n    _, total_saturations = run_simulation(W2, F2, coeffs2, x_in2, sites2)\n\n    # --- Test Case 3 ---\n    W3, F3 = 5, 2\n    x_in3 = [(-2 + 0.5) / 4, (-1 + 0.5) / 4, (0 + 0.5) / 4, (1 + 0.5) / 4, (2 + 0.5) / 4]\n    \n    quantizer3 = Quantizer(W3, F3)\n    sat_counter3 = [0]\n    y_q3 = [quantizer3.quantize(x, sat_counter3) for x in x_in3]\n    \n    scale3 = 2.0**F3\n    # Multiply by scale and round to correct for potential float inaccuracies\n    scaled_output_integers = [int(round(y * scale3)) for y in y_q3]\n\n    # --- Final Output ---\n    results = [max_abs_error, total_saturations, scaled_output_integers]\n    print(f\"[{results[0]},{results[1]},{results[2]}]\")\n\n\nsolve()\n\n```", "id": "2887709"}]}