{"hands_on_practices": [{"introduction": "This exercise provides a foundational entry point into parallel form realization. By starting with a system's impulse response, we can directly identify its constituent parts and understand how they sum together to form the overall system. This practice [@problem_id:1701264] builds intuition by connecting the time-domain behavior ($h[n]$) directly to the parameters of a simple parallel structure in the $z$-domain.", "problem": "A discrete-time Linear Time-Invariant (LTI) system is characterized by its impulse response, given by the expression:\n$$h[n] = (0.5)^{n} u[n] + (-0.5)^{n} u[n]$$\nwhere $u[n]$ is the causal unit step function, defined as $u[n] = 1$ for $n \\ge 0$ and $u[n] = 0$ for $n < 0$.\n\nThis system is to be implemented using a parallel structure comprising two stable first-order subsystems. The overall transfer function $H(z)$ is the sum of the transfer functions of these two subsystems, $H(z) = H_1(z) + H_2(z)$. The transfer function for each first-order subsystem $k \\in \\{1, 2\\}$ is defined as:\n$$H_k(z) = \\frac{b_k}{1 - a_k z^{-1}}$$\nwhere $a_k$ and $b_k$ are real-valued coefficients.\n\nTo ensure a unique solution, the coefficients are ordered such that $a_1 > a_2$. Determine the values of the coefficients $a_1, b_1, a_2, b_2$ that realize the given system. Present your four answers as a single row matrix in the order $\\begin{pmatrix} a_1 & b_1 & a_2 & b_2 \\end{pmatrix}$.", "solution": "The given impulse response is\n$$\nh[n] = \\left(\\frac{1}{2}\\right)^{n} u[n] + \\left(-\\frac{1}{2}\\right)^{n} u[n]\n$$.\nFor a first-order subsystem with transfer function\n$$\nH_{k}(z) = \\frac{b_{k}}{1 - a_{k} z^{-1}}\n$$,\nthe corresponding impulse response is obtained by inverse Z-transform. Using the standard pair $\\mathcal{Z}\\{a^{n} u[n]\\} = \\frac{1}{1 - a z^{-1}}$ for $|z| > |a|$, we have\n$h_{k}[n] = b_{k} a_{k}^{n} u[n]$.\nSince the overall system is a parallel sum of two such subsystems, the total impulse response must satisfy\n$h[n] = h_{1}[n] + h_{2}[n] = b_{1} a_{1}^{n} u[n] + b_{2} a_{2}^{n} u[n]$.\nComparing with the given $h[n]$, the two distinct exponential terms identify directly with\n$a_{1} = \\frac{1}{2}$, $\\quad b_{1} = 1$, $\\quad a_{2} = -\\frac{1}{2}$, $\\quad b_{2} = 1$,\nwhich also satisfy the ordering constraint $a_{1} > a_{2}$ since $\\frac{1}{2} > -\\frac{1}{2}$. As a consistency check in the $z$-domain,\n$H(z) = \\mathcal{Z}\\{h[n]\\} = \\frac{1}{1 - \\frac{1}{2} z^{-1}} + \\frac{1}{1 + \\frac{1}{2} z^{-1}} = \\frac{1}{1 - a_{1} z^{-1}} b_{1} + \\frac{1}{1 - a_{2} z^{-1}} b_{2}$,\nconfirming the identified coefficients. Both subsystems are stable because $|a_{1}| < 1$ and $|a_{2}| < 1$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & 1 & -\\frac{1}{2} & 1 \\end{pmatrix}}$$", "id": "1701264"}, {"introduction": "Building on the basic concept of parallel decomposition, this practice explores the more complex scenario of repeated poles in a system's transfer function. Using partial fraction expansion in the continuous-time domain, you will see how repeated poles give rise to parallel branches that are themselves cascades of first-order systems. This exercise [@problem_id:2856865] is crucial for understanding how to realize higher-order dynamics using a standardized set of simpler building blocks.", "problem": "A linear time-invariant (LTI) single-input single-output system with transfer function in the Laplace domain is defined by the rational function $H(s)$. A parallel form structure represents $H(s)$ as a sum of subsystem transfer functions connected in parallel, and when the poles are repeated, the partial fraction expansion includes higher-order terms associated with those repeated poles. Consider the transfer function\n$$\nH(s) = \\frac{2s+3}{(s+1)^{3}}.\n$$\nStarting from the foundational definitions of the Laplace transform, the definition of a transfer function as $H(s)=Y(s)/X(s)$, and the principle that any proper rational transfer function can be expressed as a sum of simpler rational functions through partial fraction expansion, perform the following:\n- Derive the partial fraction expansion of $H(s)$ that explicitly accounts for the repeated pole at $s=-1$ up to its multiplicity.\n- Justify how each term in this expansion corresponds to a branch in a parallel form realization built from first-order building blocks, and explain the role of repeated poles in this mapping based on first principles of LTI system decomposition.\n\nYour final answer must be the explicit partial fraction expansion of $H(s)$ expressed as a single closed-form analytic expression. No numerical rounding is required. Do not include units.", "solution": "The problem presented is a standard exercise in the analysis of linear time-invariant (LTI) systems and is found to be valid. It is scientifically grounded, well-posed, objective, and contains no logical or factual flaws. We may therefore proceed with a rigorous solution.\n\nThe core principle is the decomposition of a complex system into a set of simpler subsystems. For an LTI system described by a proper rational transfer function $H(s)$, this decomposition is realized through partial fraction expansion. The resulting structure, where the total output is the sum of the outputs of subsystems driven by the same input, is known as a parallel form realization. The transfer function is given as\n$$\nH(s) = \\frac{2s+3}{(s+1)^{3}}\n$$\nThis function has a single pole at $s = -1$ with multiplicity $m=3$. For a pole at $s=p$ with multiplicity $m$, the partial fraction expansion must include terms for all powers of $(s-p)^{-k}$ from $k=1$ to $k=m$. Therefore, the expansion for the given $H(s)$ must take the form:\n$$\nH(s) = \\frac{A_1}{s+1} + \\frac{A_2}{(s+1)^2} + \\frac{A_3}{(s+1)^3}\n$$\nThe coefficients $A_k$ for a repeated pole at $s=p$ of multiplicity $m$ are determined by the general formula:\n$$\nA_{m-k} = \\frac{1}{k!} \\frac{d^k}{ds^k} \\left[ (s-p)^m H(s) \\right] \\bigg|_{s=p}\n$$\nIn this specific problem, $p=-1$ and $m=3$. Let us define the function $\\Phi(s)$ as:\n$$\n\\Phi(s) = (s+1)^3 H(s) = (s+1)^3 \\left( \\frac{2s+3}{(s+1)^3} \\right) = 2s+3\n$$\nWe can now calculate the coefficients $A_3$, $A_2$, and $A_1$.\n\nFor $A_3$, we set $k=m-3=0$:\n$$\nA_3 = A_{3-0} = \\frac{1}{0!} \\frac{d^0}{ds^0} [\\Phi(s)] \\bigg|_{s=-1} = \\Phi(s) \\bigg|_{s=-1} = (2s+3) \\bigg|_{s=-1} = 2(-1) + 3 = 1\n$$\nFor $A_2$, we set $k=m-2=1$:\n$$\nA_2 = A_{3-1} = \\frac{1}{1!} \\frac{d}{ds} [\\Phi(s)] \\bigg|_{s=-1} = \\frac{d}{ds} (2s+3) \\bigg|_{s=-1} = 2 \\bigg|_{s=-1} = 2\n$$\nFor $A_1$, we set $k=m-1=2$:\n$$\nA_1 = A_{3-2} = \\frac{1}{2!} \\frac{d^2}{ds^2} [\\Phi(s)] \\bigg|_{s=-1} = \\frac{1}{2} \\frac{d^2}{ds^2} (2s+3) \\bigg|_{s=-1} = \\frac{1}{2} \\frac{d}{ds}(2) \\bigg|_{s=-1} = \\frac{1}{2}(0) = 0\n$$\nSubstituting these coefficients back into the expansion gives the complete partial fraction form for $H(s)$:\n$$\nH(s) = \\frac{0}{s+1} + \\frac{2}{(s+1)^2} + \\frac{1}{(s+1)^3}\n$$\nThis expression is the explicit expansion that accounts for the repeated pole up to its full multiplicity of $3$.\n\nNow, we justify the parallel form realization based on first principles. The transfer function $H(s)$ relates the Laplace-transformed output $Y(s)$ to the input $X(s)$ via the relation $Y(s) = H(s)X(s)$. Using our derived expansion:\n$$\nY(s) = \\left( \\frac{0}{s+1} + \\frac{2}{(s+1)^2} + \\frac{1}{(s+1)^3} \\right) X(s)\n$$\nBy the property of linearity, this is equivalent to:\n$$\nY(s) = \\left(\\frac{0}{s+1}\\right)X(s) + \\left(\\frac{2}{(s+1)^2}\\right)X(s) + \\left(\\frac{1}{(s+1)^3}\\right)X(s)\n$$\nLet us define the subsystem transfer functions $H_1(s)$, $H_2(s)$, and $H_3(s)$ as the individual terms in the expansion:\n$$\nH_1(s) = \\frac{0}{s+1}, \\quad H_2(s) = \\frac{2}{(s+1)^2}, \\quad H_3(s) = \\frac{1}{(s+1)^3}\n$$\nThe total system transfer function is the sum $H(s) = H_1(s) + H_2(s) + H_3(s)$. Correspondingly, the output is the sum of the outputs from each subsystem when driven by the common input $X(s)$:\n$$\nY(s) = Y_1(s) + Y_2(s) + Y_3(s), \\quad \\text{where } Y_k(s) = H_k(s)X(s)\n$$\nThis structure, where a single input drives multiple subsystems and their outputs are summed, is precisely the definition of a parallel form realization.\n\nThe role of the repeated pole is critical in defining the nature of these parallel branches.\n- For a simple, non-repeated pole at $s=p$, the corresponding branch is a first-order system with transfer function $\\frac{A}{s-p}$.\n- For a repeated pole, terms of the form $\\frac{A}{(s-p)^k}$ with $k>1$ appear. A subsystem with transfer function $H_k(s) = \\frac{1}{(s-p)^k}$ is not a first-order system. Instead, it is equivalent to a cascade of $k$ identical first-order systems, each having the transfer function $\\frac{1}{s-p}$. This is evident from the relationship:\n$$\n\\frac{1}{(s-p)^k} = \\left(\\frac{1}{s-p}\\right) \\times \\left(\\frac{1}{s-p}\\right) \\times \\cdots \\times \\left(\\frac{1}{s-p}\\right) \\quad (k \\text{ times})\n$$\nIn our case, the parallel branches are:\n1. $H_1(s) = \\frac{0}{s+1}$: A first-order system with zero gain, contributing nothing to the output.\n2. $H_2(s) = \\frac{2}{(s+1)^2}$: A subsystem equivalent to a cascade of two first-order building blocks $\\frac{1}{s+1}$, followed by a gain of $2$.\n3. $H_3(s) = \\frac{1}{(s+1)^3}$: A subsystem equivalent to a cascade of three first-order building blocks $\\frac{1}{s+1}$.\n\nThus, the presence of a repeated pole of multiplicity $m$ necessitates the inclusion of $m$ parallel branches, where each branch $k$ (for $k=1, \\dots, m$) is itself a cascade of $k$ basic first-order integrators associated with that pole location, scaled by the coefficient $A_k$. This demonstrates how the algebraic structure of the partial fraction expansion for repeated poles maps directly to a physical or block-diagram realization consisting of parallel combinations of cascaded first-order elements.", "answer": "$$\n\\boxed{\\frac{0}{s+1} + \\frac{2}{(s+1)^{2}} + \\frac{1}{(s+1)^{3}}}\n$$", "id": "2856865"}, {"introduction": "Shifting our focus to cascade structures, this problem delves into a critical aspect of practical digital filter implementation: dynamic range scaling. In fixed-point arithmetic, it is essential to prevent internal signals from exceeding the maximum representable value (overflow). This hands-on practice [@problem_id:2856870] demonstrates how to strategically insert scaling factors within a cascade of second-order sections to manage internal signal levels, ensuring the filter operates correctly without distortion.", "problem": "Consider a discrete-time linear time-invariant (LTI) digital filter implemented as a cascade of three second-order sections (biquads) in Direct Form II Transposed. Let the input signal be bounded in amplitude by $|x[n]| \\leq X_{\\max}$ with $X_{\\max} = 0.2$ in normalized full-scale units, where full-scale saturation occurs at magnitude $1$. For each biquad $k \\in \\{1,2,3\\}$, you are given two induced $\\ell_{\\infty}$-norms (also known as the $H_{\\infty}$ norm): \n- $\\,\\beta_k\\,$, the worst-case input-to-output gain bound for the section,\n- $\\,\\alpha_k\\,$, the worst-case input-to-largest-internal-state gain bound for the section,\nso that for any bounded input amplitude $U$, the section output amplitude is bounded by $\\beta_k U$, and the largest internal state magnitude inside the section is bounded by $\\alpha_k U$.\n\nThe sections have the following parameters:\n- Section $1$: $\\alpha_1 = 2.7$, $\\beta_1 = 1.6$,\n- Section $2$: $\\alpha_2 = 1.8$, $\\beta_2 = 1.2$,\n- Section $3$: $\\alpha_3 = 3.0$, $\\beta_3 = 0.9$.\n\nYou are allowed to insert, around each section $k$, a symmetric power-of-two scaling of the form: a pre-scale factor $c_k = 2^{m_k}$ ($m_k \\in \\mathbb{Z}$) at the section input and a post-scale factor $1/c_k$ at the section output. This preserves the overall end-to-end transfer function magnitude but changes the internal state magnitudes within the section. The wordlength is such that any internal node (including all interstage junctions and internal states) must remain strictly below full-scale magnitude $1$ to avoid overflow.\n\nDesign the per-section scaling factors $c_k$ to prevent overflow at all internal nodes. Then, under your design, determine the maximum worst-case magnitude attained by any internal node in the entire cascade (including interstage junctions and internal states), assuming the input bound $|x[n]| \\leq 0.2$. Round your final answer to four significant figures and express it in normalized full-scale units (no unit symbol).", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It is a standard problem in the analysis of fixed-point digital filter implementations. We shall proceed with the solution.\n\nLet the discrete-time LTI system be a cascade of $N=3$ second-order sections. Let $u_k[n]$ be the input to the $k$-th scaled section for $k \\in \\{1, 2, 3\\}$, and let $y_k[n]$ be its output. The input to the entire filter is $x[n]$, so $u_1[n] = x[n]$. The sections are in cascade, so $u_{k+1}[n] = y_k[n]$. The input signal is bounded by $|x[n]| \\le X_{\\max}$, where $X_{\\max} = 0.2$. Let $U_k$ be the worst-case amplitude (the $\\ell_{\\infty}$-norm of the signal) at the input to the $k$-th scaled section, $U_k = \\sup_{n} |u_k[n]|$. Thus, $U_1 = X_{\\max} = 0.2$.\n\nWithin each scaled section $k$, the input signal $u_k[n]$ is first multiplied by a scaling factor $c_k = 2^{m_k}$ where $m_k \\in \\mathbb{Z}$. The input to the biquad itself is therefore $c_k u_k[n]$. The problem provides two gain parameters for each biquad:\n1. $\\alpha_k$: the worst-case gain from the biquad's input to its largest internal state magnitude.\n2. $\\beta_k$: the worst-case gain from the biquad's input to its output.\n\nThe worst-case magnitude of the internal states in biquad $k$, denoted $S_k$, is bounded by the amplitude of the biquad's input multiplied by $\\alpha_k$.\n$$\nS_k = \\alpha_k \\sup_{n} |c_k u_k[n]| = \\alpha_k c_k U_k\n$$\nThe worst-case magnitude of the biquad's output is $\\beta_k c_k U_k$. This signal is then scaled by $1/c_k$ to produce the output of the scaled section, $y_k[n]$. The worst-case amplitude of $y_k[n]$ is therefore:\n$$\nU_{k+1} = \\sup_{n} |y_k[n]| = \\frac{1}{c_k} (\\beta_k c_k U_k) = \\beta_k U_k\n$$\nThis establishes a recurrence relation for the worst-case amplitudes at the interstage junctions. Using the given values $\\beta_1 = 1.6$, $\\beta_2 = 1.2$, and $\\beta_3 = 0.9$:\n$$\nU_1 = X_{\\max} = 0.2\n$$\n$$\nU_2 = \\beta_1 U_1 = 1.6 \\times 0.2 = 0.32\n$$\n$$\nU_3 = \\beta_2 U_2 = 1.2 \\times 0.32 = 0.384\n$$\n$$\nU_4 = \\beta_3 U_3 = 0.9 \\times 0.384 = 0.3456\n$$\n$U_4$ is the worst-case amplitude of the final output signal. The internal nodes of the filter consist of the internal states within each biquad and the interstage junctions. To prevent overflow, the magnitude at every internal node must be strictly less than $1$.\n\nThe constraints are:\n1. For internal states: $S_k < 1 \\implies \\alpha_k c_k U_k < 1$ for $k \\in \\{1, 2, 3\\}$.\n2. For interstage junctions: $U_k < 1$ for $k \\in \\{2, 3, 4\\}$.\n\nThe interstage junction magnitudes are $U_2=0.32$, $U_3=0.384$, and $U_4=0.3456$. All are less than $1$, so these nodes are safe from overflow regardless of scaling.\n\nWe must choose the scaling factors $c_k$ to satisfy the internal state constraints. The optimal choice for each $c_k$ is the largest power of two that satisfies the inequality, as this maximizes the use of the available dynamic range. The constraint for each $c_k$ is:\n$$\nc_k < \\frac{1}{\\alpha_k U_k}\n$$\n\nFor section $k=1$:\nGiven $\\alpha_1 = 2.7$.\n$$\nc_1 < \\frac{1}{\\alpha_1 U_1} = \\frac{1}{2.7 \\times 0.2} = \\frac{1}{0.54} \\approx 1.85185\n$$\nThe largest power of two, $2^{m_1}$, less than $1.85185$ is $2^0 = 1$. So, we choose $c_1 = 1$.\n\nFor section $k=2$:\nGiven $\\alpha_2 = 1.8$.\n$$\nc_2 < \\frac{1}{\\alpha_2 U_2} = \\frac{1}{1.8 \\times 0.32} = \\frac{1}{0.576} \\approx 1.73611\n$$\nThe largest power of two, $2^{m_2}$, less than $1.73611$ is $2^0 = 1$. So, we choose $c_2 = 1$.\n\nFor section $k=3$:\nGiven $\\alpha_3 = 3.0$.\n$$\nc_3 < \\frac{1}{\\alpha_3 U_3} = \\frac{1}{3.0 \\times 0.384} = \\frac{1}{1.152} \\approx 0.86805\n$$\nThe largest power of two, $2^{m_3}$, less than $0.86805$ is $2^{-1} = 0.5$. So, we choose $c_3 = 0.5$.\n\nWith the scaling factors $c_1 = 1$, $c_2 = 1$, and $c_3 = 0.5$, we now determine the maximum worst-case magnitude across all internal nodes. We must evaluate all $S_k$ and $U_k$ (for $k>1$).\nThe worst-case magnitudes are:\n- Internal states of section $1$: $S_1 = \\alpha_1 c_1 U_1 = 2.7 \\times 1 \\times 0.2 = 0.54$.\n- Interstage junction after section $1$: $U_2 = 0.32$.\n- Internal states of section $2$: $S_2 = \\alpha_2 c_2 U_2 = 1.8 \\times 1 \\times 0.32 = 0.576$.\n- Interstage junction after section $2$: $U_3 = 0.384$.\n- Internal states of section $3$: $S_3 = \\alpha_3 c_3 U_3 = 3.0 \\times 0.5 \\times 0.384 = 1.5 \\times 0.384 = 0.576$.\n- Final output node: $U_4 = 0.3456$.\n\nWe must find the maximum of these values:\n$$\n\\max\\{S_1, U_2, S_2, U_3, S_3, U_4\\} = \\max\\{0.54, 0.32, 0.576, 0.384, 0.576, 0.3456\\} = 0.576\n$$\nThe maximum worst-case magnitude attained by any internal node is $0.576$. The problem requires rounding the final answer to four significant figures, which gives $0.5760$.", "answer": "$$\n\\boxed{0.5760}\n$$", "id": "2856870"}]}