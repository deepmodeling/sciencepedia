{"hands_on_practices": [{"introduction": "Mastering quantizer design begins with understanding the fundamental principles of optimality in a clear, tractable scenario. This first practice exercise challenges you to derive the parameters of an optimal 3-level scalar quantizer from scratch for a uniformly distributed signal, using only the definition of Mean Squared Error (MSE). By working through this foundational problem [@problem_id:2915963], you will gain direct experience with the two cornerstone conditions of optimal quantization—the centroid condition and the nearest-neighbor condition—and discover a classic result connecting optimality with uniformity.", "problem": "Consider a zero-mean random variable $X$ uniformly distributed on the bounded interval $[-1,1]$ with probability density function $f_{X}(x)=\\frac{1}{2}$ for $x \\in [-1,1]$ and $f_{X}(x)=0$ otherwise. A scalar $3$-level quantizer $q(\\cdot)$ is specified by two decision thresholds $t_{1}t_{2}$ in $(-1,1)$ and three representation levels (also called codepoints or centroids) $m_{1},m_{2},m_{3}$, with decision regions $[-1,t_{1})$, $[t_{1},t_{2})$, and $[t_{2},1]$, respectively. The quantizer output is $q(x)=m_{i}$ if $x$ lies in the $i$-th region. The quality of the quantizer is measured by the mean squared error $J=\\mathbb{E}\\{(X-q(X))^{2}\\}$.\n\nStarting only from the definition of $J$ as an integral over the decision regions and the uniform density of $X$, and invoking first principles of optimality by minimizing $J$ with respect to both the thresholds and the representation levels, derive the exact optimal thresholds and centroids for the $3$-level quantizer that minimizes $J$. Exploit any symmetry that follows from the problem data, but do not assume any formulaic optimality conditions without derivation.\n\nThen, for comparison, construct the uniform-spacing $3$-level quantizer on $[-1,1]$ whose decision regions have equal width and whose representation levels are the midpoints of the regions, and compute its thresholds and centroids exactly.\n\nReport your final results as exact rational numbers. Provide your final answer as a single row matrix containing, in order, the optimal tuple $(m_{1},m_{2},m_{3},t_{1},t_{2})$ followed by the uniform-spacing tuple $(\\tilde{m}_{1},\\tilde{m}_{2},\\tilde{m}_{3},\\tilde{t}_{1},\\tilde{t}_{2})$. No rounding is required and no units are needed.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Random variable $X$ is zero-mean.\n- The probability density function (PDF) of $X$ is $f_{X}(x)=\\frac{1}{2}$ for $x \\in [-1,1]$ and $f_{X}(x)=0$ otherwise.\n- A scalar $3$-level quantizer $q(\\cdot)$ is defined by two decision thresholds $t_{1}t_{2}$ in $(-1,1)$ and three representation levels $m_{1},m_{2},m_{3}$.\n- The decision regions are $R_1 = [-1,t_{1})$, $R_2 = [t_{1},t_{2})$, and $R_3 = [t_{2},1]$.\n- The quantizer output is $q(x)=m_{i}$ for $x \\in R_i$.\n- The performance metric is the mean squared error (MSE), $J=\\mathbb{E}\\{(X-q(X))^{2}\\}$.\n- The task is to derive the optimal set of parameters $(m_1, m_2, m_3, t_1, t_2)$ that minimize $J$ from first principles, and also to determine the parameters $(\\tilde{m}_1, \\tilde{m}_2, \\tilde{m}_3, \\tilde{t}_1, \\tilde{t}_2)$ for a uniform quantizer on the same interval.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard, fundamental problem in quantization theory, a subfield of signal processing. All necessary definitions and constraints are provided, and there are no contradictions. The problem is complete and solvable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A rigorous derivation will now be provided.\n\nThe mean squared error $J$ is defined as the expectation of the squared error:\n$$J = \\mathbb{E}\\{(X - q(X))^2\\} = \\int_{-\\infty}^{\\infty} (x-q(x))^2 f_{X}(x) dx$$\nSubstituting the given PDF and the structure of the quantizer, the expression for $J$ becomes a function of the parameters $\\{m_1, m_2, m_3, t_1, t_2\\}$:\n$$J(m_1, m_2, m_3, t_1, t_2) = \\int_{-1}^{t_1} (x - m_1)^2 \\frac{1}{2} dx + \\int_{t_1}^{t_2} (x - m_2)^2 \\frac{1}{2} dx + \\int_{t_2}^{1} (x - m_3)^2 \\frac{1}{2} dx$$\nTo minimize $J$, we must find the stationary points by setting the partial derivatives of $J$ with respect to each parameter to zero.\n\nFirst, we derive the necessary conditions for the optimal representation levels $\\{m_1, m_2, m_3\\}$. We differentiate $J$ with respect to each $m_i$ and set the result to zero:\n$$\\frac{\\partial J}{\\partial m_1} = \\frac{\\partial}{\\partial m_1} \\left[ \\frac{1}{2} \\int_{-1}^{t_1} (x - m_1)^2 dx \\right] = \\frac{1}{2} \\int_{-1}^{t_1} 2(x - m_1)(-1) dx = -\\int_{-1}^{t_1} (x - m_1) dx = 0$$\nThis implies $\\int_{-1}^{t_1} x dx = m_1 \\int_{-1}^{t_1} dx$, which gives the optimal $m_1$ as the conditional mean of $X$ given $X \\in R_1$:\n$$m_1 = \\frac{\\int_{-1}^{t_1} x dx}{\\int_{-1}^{t_1} dx} = \\frac{\\frac{1}{2}[x^2]_{-1}^{t_1}}{[x]_{-1}^{t_1}} = \\frac{\\frac{1}{2}(t_1^2 - 1)}{t_1 + 1} = \\frac{t_1 - 1}{2}$$\nBy identical reasoning for $m_2$ and $m_3$:\n$$\\frac{\\partial J}{\\partial m_2} = -\\int_{t_1}^{t_2} (x - m_2) dx = 0 \\implies m_2 = \\frac{\\int_{t_1}^{t_2} x dx}{\\int_{t_1}^{t_2} dx} = \\frac{\\frac{1}{2}(t_2^2 - t_1^2)}{t_2 - t_1} = \\frac{t_1 + t_2}{2}$$\n$$\\frac{\\partial J}{\\partial m_3} = -\\int_{t_2}^{1} (x - m_3) dx = 0 \\implies m_3 = \\frac{\\int_{t_2}^{1} x dx}{\\int_{t_2}^{1} dx} = \\frac{\\frac{1}{2}(1 - t_2^2)}{1 - t_2} = \\frac{1 + t_2}{2}$$\nThese are the centroid conditions for optimality.\n\nNext, we derive the necessary conditions for the optimal decision thresholds $\\{t_1, t_2\\}$. We differentiate $J$ with respect to each $t_j$. According to the Leibniz integral rule, and given that the conditions $\\frac{\\partial J}{\\partial m_i}=0$ are already satisfied, the derivatives simplify significantly:\n$$\\frac{\\partial J}{\\partial t_1} = \\frac{1}{2} (t_1 - m_1)^2 - \\frac{1}{2} (t_1 - m_2)^2 = 0$$\nThis implies $(t_1 - m_1)^2 = (t_1 - m_2)^2$. For an optimal quantizer, the representation levels must be ordered such that $m_1  t_1  m_2$, so we must have $t_1 - m_1 = -(t_1 - m_2) = m_2 - t_1$. This yields:\n$$t_1 = \\frac{m_1 + m_2}{2}$$\nSimilarly, for $t_2$:\n$$\\frac{\\partial J}{\\partial t_2} = \\frac{1}{2} (t_2 - m_2)^2 - \\frac{1}{2} (t_2 - m_3)^2 = 0$$\nWith $m_2  t_2  m_3$, we find $t_2 - m_2 = -(t_2 - m_3)$, which gives:\n$$t_2 = \\frac{m_2 + m_3}{2}$$\nThese are the nearest-neighbor conditions for optimality.\n\nThe problem possesses symmetry. The PDF $f_X(x)$ is an even function on a symmetric interval $[-1, 1]$. For such a distribution, the optimal quantizer must be symmetric about the origin. This means its structure must be anti-symmetric, $q(x) = -q(-x)$. This symmetry implies $t_2 = -t_1$, $m_3 = -m_1$, and $m_2 = -m_2$, which forces $m_2=0$. This greatly simplifies the system of equations.\n\nLet us apply the symmetry conditions to the optimality equations.\n$m_2=0$.\nThe condition $t_1 = \\frac{m_1+m_2}{2}$ becomes $t_1 = \\frac{m_1}{2}$.\nThe condition $m_1 = \\frac{t_1-1}{2}$ becomes $m_1 = \\frac{(m_1/2) - 1}{2}$.\nSolving for $m_1$:\n$$2m_1 = \\frac{m_1}{2} - 1 \\implies \\frac{3}{2}m_1 = -1 \\implies m_1 = -\\frac{2}{3}$$\nFrom this, we find the other parameters for the optimal quantizer:\n$$t_1 = \\frac{m_1}{2} = \\frac{-2/3}{2} = -\\frac{1}{3}$$\n$$m_2 = 0$$\n$$m_3 = -m_1 = \\frac{2}{3}$$\n$$t_2 = -t_1 = \\frac{1}{3}$$\nThe optimal parameters are $(m_{1},m_{2},m_{3},t_{1},t_{2}) = (-\\frac{2}{3}, 0, \\frac{2}{3}, -\\frac{1}{3}, \\frac{1}{3})$.\n\nNext, we construct the uniform-spacing $3$-level quantizer on $[-1,1]$.\nThe total length of the interval is $1 - (-1) = 2$.\nFor $3$ regions of equal width, each region has a width of $\\Delta = \\frac{2}{3}$.\nThe decision thresholds, denoted by $\\tilde{t}_i$, are the boundaries of these regions:\n$$\\tilde{t}_1 = -1 + \\Delta = -1 + \\frac{2}{3} = -\\frac{1}{3}$$\n$$\\tilde{t}_2 = -1 + 2\\Delta = -1 + \\frac{4}{3} = \\frac{1}{3}$$\nThe decision regions are therefore $R_1 = [-1, -1/3)$, $R_2 = [-1/3, 1/3)$, and $R_3 = [1/3, 1]$.\nThe representation levels, denoted by $\\tilde{m}_i$, are the midpoints of these regions:\n$$\\tilde{m}_1 = \\frac{-1 + (-\\frac{1}{3})}{2} = \\frac{-4/3}{2} = -\\frac{2}{3}$$\n$$\\tilde{m}_2 = \\frac{-\\frac{1}{3} + \\frac{1}{3}}{2} = 0$$\n$$\\tilde{m}_3 = \\frac{\\frac{1}{3} + 1}{2} = \\frac{4/3}{2} = \\frac{2}{3}$$\nThe parameters for the uniform-spacing quantizer are $(\\tilde{m}_{1},\\tilde{m}_{2},\\tilde{m}_{3},\\tilde{t}_{1},\\tilde{t}_{2}) = (-\\frac{2}{3}, 0, \\frac{2}{3}, -\\frac{1}{3}, \\frac{1}{3})$.\nIt is a known result that for a uniformly distributed source, the optimal scalar quantizer is a uniform quantizer. Our derivation confirms this, as both sets of parameters are identical.\n\nThe final result is reported as a single row matrix containing the ten values in the specified order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{2}{3}  0  \\frac{2}{3}  -\\frac{1}{3}  \\frac{1}{3}  -\\frac{2}{3}  0  \\frac{2}{3}  -\\frac{1}{3}  \\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "2915963"}, {"introduction": "While deriving a closed-form optimal quantizer is possible for simple distributions, most real-world signals require an iterative design approach. This practice problem [@problem_id:2915969] brings the celebrated Lloyd algorithm to life by guiding you through one complete analytical iteration for a signal with a Laplacian distribution. You will apply the centroid and nearest-neighbor update rules sequentially and verify a key property of the algorithm: the guaranteed monotonic decrease in mean-squared error with each step.", "problem": "Consider a memoryless scalar source with zero-mean Laplace distribution having probability density function $f_{X}(x) = \\frac{1}{2}\\exp(-|x|)$, which corresponds to a Laplace scale parameter $b = 1$. A symmetric $3$-level quantizer is initialized with decision thresholds at $-1$ and $+1$, and with initial reconstruction levels at $-1$, $0$, and $+1$. One iteration of the Lloyd algorithm (also known as the Lloyd–Max algorithm for squared-error distortion) consists of the following two substeps applied in sequence: first, for fixed thresholds, update each reconstruction level to the conditional mean of $X$ over its corresponding decision region; second, for fixed reconstruction levels, update each threshold to the midpoint between its two neighboring reconstruction levels.\n\nUsing only core definitions of mean-squared error, conditional expectation, and decision region partitions for quantizers, carry out precisely one Lloyd iteration starting from the specified initialization. Then, compute the mean-squared error before the iteration and after the iteration, and verify analytically that the latter is smaller than the former.\n\nProvide as your final answer the exact closed-form analytic expression for the mean-squared error after the single Lloyd iteration. Do not round. Do not include units.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains all necessary information to proceed. It is a standard problem in scalar quantizer design. Thus, the problem is valid.\n\nThe memoryless scalar source $X$ has a zero-mean Laplace distribution with a probability density function (PDF) given by $f_{X}(x) = \\frac{1}{2}\\exp(-|x|)$. The quantizer is a symmetric $3$-level quantizer.\n\nThe initial state of the quantizer is defined by:\nDecision thresholds: $t_1 = -1$, $t_2 = 1$. These define three decision regions: $R_1 = (-\\infty, -1]$, $R_2 = (-1, 1]$, and $R_3 = (1, \\infty)$.\nReconstruction levels (codepoints): $c_1 = -1$, $c_2 = 0$, $c_3 = 1$.\n\nThe Mean-Squared Error (MSE) of a quantizer $Q$ is given by the formula $D = E[(X - Q(X))^2]$. For this specific quantizer, it is:\n$$D = \\sum_{i=1}^{3} \\int_{R_i} (x - c_i)^2 f_X(x) \\, dx$$\n\nFirst, we calculate the initial MSE, denoted as $D_{initial}$, using the initial quantizer parameters.\n$$D_{initial} = \\int_{-\\infty}^{-1} (x - (-1))^2 \\frac{1}{2}\\exp(-|x|) \\, dx + \\int_{-1}^{1} (x - 0)^2 \\frac{1}{2}\\exp(-|x|) \\, dx + \\int_{1}^{\\infty} (x - 1)^2 \\frac{1}{2}\\exp(-|x|) \\, dx$$\nFor $x  0$, $|x| = -x$, so $\\exp(-|x|) = \\exp(x)$. For $x  0$, $|x| = x$, so $\\exp(-|x|) = \\exp(-x)$.\n$$D_{initial} = \\frac{1}{2} \\int_{-\\infty}^{-1} (x+1)^2 \\exp(x) \\, dx + \\frac{1}{2} \\int_{-1}^{1} x^2 \\exp(-|x|) \\, dx + \\frac{1}{2} \\int_{1}^{\\infty} (x-1)^2 \\exp(-x) \\, dx$$\nDue to the symmetry of the PDF and the quantizer, the first and third integrals are equal. The middle integral can be split due to the evenness of its integrand $x^2 \\exp(-|x|)$.\n$$D_{initial} = \\int_{1}^{\\infty} (x-1)^2 \\exp(-x) \\, dx + \\int_{0}^{1} x^2 \\exp(-x) \\, dx$$\nWe require the following indefinite integrals, which are found by repeated integration by parts:\n$\\int x^2 \\exp(-x) \\, dx = -(x^2 + 2x + 2)\\exp(-x) + C$\n$\\int (x-1)^2 \\exp(-x) \\, dx = \\int (x^2 - 2x + 1)\\exp(-x) \\, dx = -(x^2+1)\\exp(-x) + C$\n\nUsing these, we evaluate the definite integrals:\n$\\int_{1}^{\\infty} (x-1)^2 \\exp(-x) \\, dx = [-(x^2+1)\\exp(-x)]_{1}^{\\infty} = 0 - (-(1^2+1)\\exp(-1)) = 2\\exp(-1)$.\n$\\int_{0}^{1} x^2 \\exp(-x) \\, dx = [-(x^2+2x+2)\\exp(-x)]_{0}^{1} = -(1^2+2(1)+2)\\exp(-1) - (-(0+0+2)\\exp(0)) = -5\\exp(-1) + 2$.\nTherefore, the initial MSE is:\n$$D_{initial} = 2\\exp(-1) + (2 - 5\\exp(-1)) = 2 - 3\\exp(-1)$$\n\nNext, we perform one iteration of the Lloyd algorithm.\nStep 1: Update reconstruction levels to be the centroids of their corresponding decision regions. The new reconstruction level $c'_i$ is the conditional mean $E[X | X \\in R_i]$.\n$$c'_i = \\frac{\\int_{R_i} x f_X(x) \\, dx}{\\int_{R_i} f_X(x) \\, dx}$$\nFirst, we find the probabilities of the regions, $P(R_i) = \\int_{R_i} f_X(x) \\, dx$.\n$P(R_3) = \\int_{1}^{\\infty} \\frac{1}{2}\\exp(-x) \\, dx = \\frac{1}{2}[-\\exp(-x)]_{1}^{\\infty} = \\frac{1}{2}\\exp(-1)$.\nBy symmetry, $P(R_1) = P(R_3) = \\frac{1}{2}\\exp(-1)$.\n$P(R_2) = 1 - P(R_1) - P(R_3) = 1 - \\exp(-1)$.\n\nNow, we compute the numerators for the centroids. For $c'_3$:\n$\\int_{1}^{\\infty} x f_X(x) \\, dx = \\frac{1}{2}\\int_{1}^{\\infty} x \\exp(-x) \\, dx = \\frac{1}{2}[-(x+1)\\exp(-x)]_{1}^{\\infty} = \\frac{1}{2}(0 - (-(1+1)\\exp(-1))) = \\exp(-1)$.\nSo, $c'_3 = \\frac{\\exp(-1)}{P(R_3)} = \\frac{\\exp(-1)}{\\frac{1}{2}\\exp(-1)} = 2$.\nBy symmetry, the centroid for $R_1$ is $c'_1 = -2$.\nFor $c'_2$, the region is $R_2 = (-1, 1)$, which is symmetric about the origin. The integrand $x f_X(x) = \\frac{1}{2}x\\exp(-|x|)$ is an odd function. Therefore, the integral over this symmetric interval is zero: $\\int_{-1}^{1} x f_X(x) \\, dx = 0$.\nSo, $c'_2 = 0$.\nThe updated reconstruction levels are $c'_1 = -2$, $c'_2 = 0$, $c'_3 = 2$.\n\nStep 2: Update decision thresholds to be the midpoints of adjacent new reconstruction levels.\nThe new threshold $t'_1$ is the midpoint of $c'_1$ and $c'_2$:\n$t'_1 = \\frac{c'_1 + c'_2}{2} = \\frac{-2 + 0}{2} = -1$.\nThe new threshold $t'_2$ is the midpoint of $c'_2$ and $c'_3$:\n$t'_2 = \\frac{c'_2 + c'_3}{2} = \\frac{0 + 2}{2} = 1$.\nThe decision thresholds remain unchanged: $t'_1 = -1$ and $t'_2 = 1$. Consequently, the decision regions also remain unchanged.\n\nNow we compute the final MSE, denoted as $D_{final}$, after one full iteration. The quantizer is defined by regions $R_1, R_2, R_3$ and new reconstruction levels $c'_1 = -2, c'_2 = 0, c'_3 = 2$.\n$$D_{final} = \\int_{-\\infty}^{-1} (x - (-2))^2 \\frac{1}{2}\\exp(x) \\, dx + \\int_{-1}^{1} (x - 0)^2 \\frac{1}{2}\\exp(-|x|) \\, dx + \\int_{1}^{\\infty} (x - 2)^2 \\frac{1}{2}\\exp(-x) \\, dx$$\nAgain, by symmetry:\n$$D_{final} = \\int_{1}^{\\infty} (x-2)^2 \\exp(-x) \\, dx + \\int_{0}^{1} x^2 \\exp(-x) \\, dx$$\nThe second term was already computed as $2 - 5\\exp(-1)$. We compute the first term.\nThe indefinite integral is $\\int (x-2)^2 \\exp(-x) \\, dx = \\int(x^2 - 4x + 4)\\exp(-x) \\, dx = (-x^2+2x-2)\\exp(-x) + C$.\n$\\int_{1}^{\\infty} (x-2)^2 \\exp(-x) \\, dx = [(-x^2+2x-2)\\exp(-x)]_{1}^{\\infty} = 0 - ((-1^2+2(1)-2)\\exp(-1)) = -(-1)\\exp(-1) = \\exp(-1)$.\nThus, the final MSE is:\n$$D_{final} = \\exp(-1) + (2 - 5\\exp(-1)) = 2 - 4\\exp(-1)$$\n\nFinally, we verify that the MSE has decreased, i.e., $D_{final}  D_{initial}$.\nWe must check if $2 - 4\\exp(-1)  2 - 3\\exp(-1)$.\nSubtracting $2$ from both sides gives $-4\\exp(-1)  -3\\exp(-1)$.\nMultiplying by $-1$ and reversing the inequality sign yields $4\\exp(-1)  3\\exp(-1)$.\nSince $\\exp(-1)  0$, we can divide by it, resulting in $4  3$. This is a true statement, which confirms that the MSE decreased, as expected from the Lloyd algorithm.\n\nThe problem asks for the exact closed-form analytic expression for the mean-squared error after the single Lloyd iteration. This is $D_{final}$.", "answer": "$$\\boxed{2 - 4\\exp(-1)}$$", "id": "2915969"}, {"introduction": "The ultimate goal of designing an optimal non-uniform quantizer is to achieve superior performance for a given signal distribution. This exercise focuses on quantifying that performance gain by comparing an optimal 2-level Lloyd-Max quantizer against a simple uniform quantizer for a Gaussian-distributed signal [@problem_id:2915970]. By deriving the Mean Squared Error (MSE) for both systems, you will not only practice handling integrals with the Gaussian PDF but also produce a final expression for their performance ratio, making the advantage of non-uniform quantization precise and tangible.", "problem": "Consider a real-valued random input signal modeled as a zero-mean Gaussian random variable $X \\sim \\mathcal{N}(0,\\sigma^{2})$ with probability density function (PDF) $f_{X}(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)$. Two memoryless scalar quantizers are applied to $X$:\n\n1. An optimal two-level Lloyd–Max quantizer $Q_{\\mathrm{LM}}$ with decision threshold $t \\in \\mathbb{R}$ and reproduction levels $y_{1}  y_{2}$ chosen to minimize the Mean Squared Error (MSE), where MSE is defined as $\\operatorname{MSE} = \\mathbb{E}\\!\\left[(X - Q(X))^{2}\\right]$.\n\n2. A two-level uniform quantizer with a mid-threshold at zero (often termed a “mid-tread” two-level setting in some engineering sources; here, it is defined unambiguously as having a single decision threshold at $0$ and symmetric reproduction levels), denoted $Q_{\\mathrm{U}}$, with decision rule\n$Q_{\\mathrm{U}}(x) = \\begin{cases}\n-\\beta \\sigma,  x \\le 0,\\\\\n+\\beta \\sigma,  x  0,\n\\end{cases}$\nwhere $\\beta  0$ is a fixed, hardware-imposed constant that does not depend on the input statistics.\n\nStarting only from the definition of MSE and standard properties of conditional expectation and differentiation under the integral sign as needed for optimization, do the following:\n\n- Derive the optimal parameters $(t^{\\star},y_{1}^{\\star},y_{2}^{\\star})$ for $Q_{\\mathrm{LM}}$ and compute $\\operatorname{MSE}_{\\mathrm{LM}}$ in closed form as a function of $\\sigma$.\n- Compute $\\operatorname{MSE}_{\\mathrm{U}}$ in closed form as a function of $\\sigma$ and $\\beta$.\n- As the final reported quantity, provide the exact closed-form expression for the ratio\n$$\nR(\\beta) \\equiv \\frac{\\operatorname{MSE}_{\\mathrm{U}}}{\\operatorname{MSE}_{\\mathrm{LM}}}.\n$$\n\nYour final answer must be a single closed-form analytic expression for $R(\\beta)$; do not include units. No numerical rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is a standard problem in signal processing theory concerning the performance analysis of scalar quantizers. All required information is provided, and there are no contradictions or ambiguities. The problem is valid. We proceed to the solution.\n\nThe problem requires the derivation of performance for two different two-level scalar quantizers for a zero-mean Gaussian input signal $X \\sim \\mathcal{N}(0,\\sigma^{2})$ with probability density function (PDF) $f_{X}(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp(-\\frac{x^{2}}{2\\sigma^{2}})$. The performance metric is the Mean Squared Error (MSE), defined as $\\operatorname{MSE} = \\mathbb{E}[(X - Q(X))^{2}]$.\n\nFirst, we analyze the optimal two-level Lloyd–Max quantizer, $Q_{\\mathrm{LM}}$. A two-level quantizer is defined by a single decision threshold $t$ and two reproduction levels, $y_{1}$ for inputs $x \\le t$ and $y_{2}$ for inputs $x  t$. The MSE is given by:\n$$ \\operatorname{MSE}(t, y_1, y_2) = \\int_{-\\infty}^{t} (x - y_1)^2 f_X(x) \\,dx + \\int_{t}^{\\infty} (x - y_2)^2 f_X(x) \\,dx $$\nTo minimize this expression with respect to $y_1$ and $y_2$, we take the partial derivatives and set them to zero:\n$$ \\frac{\\partial \\operatorname{MSE}}{\\partial y_1} = -2\\int_{-\\infty}^{t} (x - y_1) f_X(x) \\,dx = 0 \\implies y_1 \\int_{-\\infty}^{t} f_X(x) \\,dx = \\int_{-\\infty}^{t} x f_X(x) \\,dx $$\n$$ y_1^{\\star} = \\frac{\\int_{-\\infty}^{t} x f_X(x) \\,dx}{\\int_{-\\infty}^{t} f_X(x) \\,dx} = \\mathbb{E}[X \\mid X \\le t] $$\nSimilarly, for $y_2$:\n$$ y_2^{\\star} = \\frac{\\int_{t}^{\\infty} x f_X(x) \\,dx}{\\int_{t}^{\\infty} f_X(x) \\,dx} = \\mathbb{E}[X \\mid X  t] $$\nThese are the centroid conditions for the reproduction levels. To find the optimal threshold $t$, we substitute $y_1^{\\star}$ and $y_2^{\\star}$ back into the MSE expression and differentiate with respect to $t$ using the Leibniz integral rule:\n$$ \\frac{d \\operatorname{MSE}}{dt} = (t - y_1^{\\star})^2 f_X(t) - (t - y_2^{\\star})^2 f_X(t) = 0 $$\nAssuming $f_X(t) \\neq 0$, this implies $(t - y_1^{\\star})^2 = (t - y_2^{\\star})^2$. Since we require $y_1  y_2$, we must have $y_1^{\\star} \\neq y_2^{\\star}$, which leads to $t - y_1^{\\star} = -(t - y_2^{\\star})$. This gives the optimal threshold as the midpoint of the optimal reproduction levels:\n$$ t^{\\star} = \\frac{y_1^{\\star} + y_2^{\\star}}{2} $$\nFor the given zero-mean Gaussian PDF, which is symmetric about $0$, i.e., $f_X(x) = f_X(-x)$, the optimal quantizer must also be symmetric. This implies $t^{\\star} = 0$, and consequently $y_1^{\\star} = -y_2^{\\star}$. This is consistent with the condition $t^{\\star} = (y_1^{\\star} + y_2^{\\star})/2$.\n\nWith $t^{\\star}=0$, the optimal reproduction level $y_2^{\\star}$ is:\n$$ y_2^{\\star} = \\mathbb{E}[X \\mid X  0] = \\frac{\\int_{0}^{\\infty} x f_X(x) \\,dx}{\\int_{0}^{\\infty} f_X(x) \\,dx} $$\nThe denominator is $P(X0) = 1/2$. The numerator is:\n$$ \\int_{0}^{\\infty} x \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) dx = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\left[ -\\sigma^2 \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\right]_{0}^{\\infty} = \\frac{-\\sigma^2}{\\sigma\\sqrt{2\\pi}} (0 - 1) = \\frac{\\sigma}{\\sqrt{2\\pi}} $$\nTherefore, $y_2^{\\star} = \\frac{\\sigma/\\sqrt{2\\pi}}{1/2} = \\sigma \\sqrt{\\frac{2}{\\pi}}$. The optimal parameters for $Q_{\\mathrm{LM}}$ are:\n$$ t^{\\star} = 0, \\quad y_1^{\\star} = -\\sigma\\sqrt{\\frac{2}{\\pi}}, \\quad y_2^{\\star} = \\sigma\\sqrt{\\frac{2}{\\pi}} $$\nTo compute $\\operatorname{MSE}_{\\mathrm{LM}}$, we use the property that for an optimal quantizer, the quantization error is orthogonal to the quantizer output, $\\mathbb{E}[(X - Q(X))Q(X)] = 0$. This gives $\\operatorname{MSE} = \\mathbb{E}[X^2] - \\mathbb{E}[XQ(X)]$. The variance $\\mathbb{E}[X^2]$ is $\\sigma^2$. We compute $\\mathbb{E}[XQ_{\\mathrm{LM}}(X)]$:\n$$ \\mathbb{E}[XQ_{\\mathrm{LM}}(X)] = \\int_{-\\infty}^{\\infty} x Q_{\\mathrm{LM}}(x) f_X(x) \\,dx = y_1^{\\star} \\int_{-\\infty}^{0} x f_X(x) \\,dx + y_2^{\\star} \\int_{0}^{\\infty} x f_X(x) \\,dx $$\nBy symmetry, $\\int_{-\\infty}^{0} x f_X(x) \\,dx = -\\int_{0}^{\\infty} x f_X(x) \\,dx = -\\frac{\\sigma}{\\sqrt{2\\pi}}$.\n$$ \\mathbb{E}[XQ_{\\mathrm{LM}}(X)] = \\left(-\\sigma\\sqrt{\\frac{2}{\\pi}}\\right)\\left(-\\frac{\\sigma}{\\sqrt{2\\pi}}\\right) + \\left(\\sigma\\sqrt{\\frac{2}{\\pi}}\\right)\\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right) = 2 \\sigma^2 \\frac{\\sqrt{2}}{\\pi\\sqrt{2}} = \\frac{2\\sigma^2}{\\pi} $$\nThus, the minimum MSE is:\n$$ \\operatorname{MSE}_{\\mathrm{LM}} = \\sigma^2 - \\frac{2\\sigma^2}{\\pi} = \\sigma^2\\left(1 - \\frac{2}{\\pi}\\right) $$\n\nNext, we analyze the uniform quantizer $Q_{\\mathrm{U}}$, defined by $Q_{\\mathrm{U}}(x) = -\\beta\\sigma$ for $x \\le 0$ and $Q_{\\mathrm{U}}(x) = +\\beta\\sigma$ for $x  0$. The MSE is:\n$$ \\operatorname{MSE}_{\\mathrm{U}} = \\int_{-\\infty}^{0} (x + \\beta\\sigma)^2 f_X(x) \\,dx + \\int_{0}^{\\infty} (x - \\beta\\sigma)^2 f_X(x) \\,dx $$\nDue to the symmetry of $f_X(x)$, the two terms are identical. We compute the second term and multiply by $2$:\n$$ \\operatorname{MSE}_{\\mathrm{U}} = 2 \\int_{0}^{\\infty} (x^2 - 2x\\beta\\sigma + \\beta^2\\sigma^2) f_X(x) \\,dx $$\n$$ = 2\\left[ \\int_{0}^{\\infty} x^2 f_X(x) \\,dx - 2\\beta\\sigma \\int_{0}^{\\infty} x f_X(x) \\,dx + \\beta^2\\sigma^2 \\int_{0}^{\\infty} f_X(x) \\,dx \\right] $$\nWe use the previously derived integrals: $\\int_{0}^{\\infty} f_X(x) \\,dx = 1/2$, $\\int_{0}^{\\infty} x f_X(x) \\,dx = \\sigma/\\sqrt{2\\pi}$, and $\\int_{0}^{\\infty} x^2 f_X(x) \\,dx = \\frac{1}{2}\\mathbb{E}[X^2] = \\sigma^2/2$.\n$$ \\operatorname{MSE}_{\\mathrm{U}} = 2\\left[ \\frac{\\sigma^2}{2} - 2\\beta\\sigma \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right) + \\beta^2\\sigma^2 \\left(\\frac{1}{2}\\right) \\right] $$\n$$ = \\sigma^2 - \\frac{4\\beta\\sigma^2}{\\sqrt{2\\pi}} + \\beta^2\\sigma^2 = \\sigma^2\\left(1 - 2\\beta\\sqrt{\\frac{2}{\\pi}} + \\beta^2\\right) $$\n\nFinally, we compute the ratio $R(\\beta) = \\operatorname{MSE}_{\\mathrm{U}} / \\operatorname{MSE}_{\\mathrm{LM}}$:\n$$ R(\\beta) = \\frac{\\sigma^2\\left(1 + \\beta^2 - 2\\beta\\sqrt{\\frac{2}{\\pi}}\\right)}{\\sigma^2\\left(1 - \\frac{2}{\\pi}\\right)} = \\frac{1 + \\beta^2 - 2\\beta\\sqrt{\\frac{2}{\\pi}}}{1 - \\frac{2}{\\pi}} $$\nThis expression can be rewritten to be more illuminating. Let $\\beta_{opt} = \\sqrt{2/\\pi}$. The numerator is $1 + \\beta^2 - 2\\beta\\beta_{opt}$. We can express this relative to the optimal case by completing the square:\n$$ 1 + \\beta^2 - 2\\beta\\beta_{opt} = (\\beta - \\beta_{opt})^2 - \\beta_{opt}^2 + 1 = \\left(\\beta - \\sqrt{\\frac{2}{\\pi}}\\right)^2 - \\frac{2}{\\pi} + 1 $$\nSo, the ratio becomes:\n$$ R(\\beta) = \\frac{\\left(1 - \\frac{2}{\\pi}\\right) + \\left(\\beta - \\sqrt{\\frac{2}{\\pi}}\\right)^2}{1 - \\frac{2}{\\pi}} = 1 + \\frac{\\left(\\beta - \\sqrt{\\frac{2}{\\pi}}\\right)^2}{1 - \\frac{2}{\\pi}} $$\nTo present this as a single fraction with integer coefficients where possible, we multiply the numerator and denominator by $\\pi$:\n$$ R(\\beta) = 1 + \\frac{\\left(\\beta - \\sqrt{\\frac{2}{\\pi}}\\right)^2}{\\frac{\\pi-2}{\\pi}} = 1 + \\frac{\\pi\\left(\\beta - \\sqrt{\\frac{2}{\\pi}}\\right)^2}{\\pi-2} = 1 + \\frac{\\pi\\left(\\beta^2 - 2\\beta\\sqrt{\\frac{2}{\\pi}} + \\frac{2}{\\pi}\\right)}{\\pi-2} $$\n$$ R(\\beta) = 1 + \\frac{\\pi\\beta^2 - 2\\beta\\sqrt{2\\pi} + 2}{\\pi-2} = \\frac{(\\pi-2) + (\\pi\\beta^2 - 2\\beta\\sqrt{2\\pi} + 2)}{\\pi-2} = \\frac{\\pi + \\pi\\beta^2 - 2\\beta\\sqrt{2\\pi}}{\\pi-2} $$\n$$ R(\\beta) = \\frac{\\pi(1+\\beta^2) - 2\\beta\\sqrt{2\\pi}}{\\pi-2} $$\nThis is the final expression for the ratio.", "answer": "$$\n\\boxed{\\frac{\\pi(1+\\beta^2) - 2\\beta\\sqrt{2\\pi}}{\\pi-2}}\n$$", "id": "2915970"}]}