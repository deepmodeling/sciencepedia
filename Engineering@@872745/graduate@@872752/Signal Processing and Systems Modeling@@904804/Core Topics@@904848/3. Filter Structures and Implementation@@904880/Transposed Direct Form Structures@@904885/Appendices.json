{"hands_on_practices": [{"introduction": "A primary motivation for using canonical forms is implementation efficiency. This exercise guides you through a fundamental accounting of the components required for a general LTI system, confirming that the Transposed Direct Form II structure is just as resource-efficient as the Direct Form II. By rigorously counting multipliers, adders, and delay elements, you will see a direct consequence of the transposition theorem: the inventory of components is preserved even as their interconnections are rearranged [@problem_id:2915276].", "problem": "Consider a causal, linear time-invariant (LTI) discrete-time system with transfer function\n$$\nH(z)=\\frac{B(z)}{A(z)}=\\frac{b_{0}+b_{1}z^{-1}+\\cdots+b_{M}z^{-M}}{1+a_{1}z^{-1}+\\cdots+a_{N}z^{-N}},\n$$\nwhere $A(z)$ is monic, $M$ and $N$ are nonnegative integers, and all coefficients $b_{k}$ for $0\\leq k\\leq M$ and $a_{k}$ for $1\\leq k\\leq N$ are generic nonzero real numbers (no structural cancellations). Assume ideal signal splitters (no cost), each summation is implemented using only $2$-input adders, and unary sign changes are absorbed into multipliers. No structural optimizations beyond the canonical forms are allowed.\n\nUsing only the core facts that (i) a causal LTI system described by a linear constant-coefficient difference equation can be realized by interconnecting multipliers, adders, and unit delays, (ii) a sum of $K$ signals requires $K-1$ two-input adders, and (iii) the transposition theorem for linear signal-flow graphs preserves the transfer function and the counts of linear elements while reversing interconnections, do the following:\n\n1. Realize $H(z)$ conceptually in Direct Form II (DF-II) canonical structure, and count the total number of real multipliers, two-input adders, and unit delays required, expressed in terms of $M$ and $N$.\n\n2. Apply the transposition theorem to obtain the Transposed Direct Form II (TDF-II) structure, and again count the total number of real multipliers, two-input adders, and unit delays required, expressed in terms of $M$ and $N$.\n\n3. Explain whether these totals are equal between DF-II and TDF-II and why this is the case, even though the placements of elements differ.\n\nProvide your final answer as the common totals, in the form of a single row matrix $[n_{\\text{mult}},\\,n_{\\text{add}},\\,n_{\\text{delay}}]$ whose entries are symbolic expressions in $M$ and $N$. No numerical evaluation is required, and no units are involved.", "solution": "The problem statement is a well-posed exercise in the analysis of canonical digital filter structures. It is scientifically grounded in established principles of linear systems and digital signal processing. All premises and constraints are clearly defined and consistent. The problem is valid. We proceed with the solution.\n\nThe system is described by the transfer function $H(z)$:\n$$\nH(z) = \\frac{Y(z)}{X(z)} = \\frac{B(z)}{A(z)} = \\frac{\\sum_{k=0}^{M} b_k z^{-k}}{1 + \\sum_{k=1}^{N} a_k z^{-k}}\n$$\nwhere $Y(z)$ and $X(z)$ are the Z-transforms of the output and input signals, respectively. This transfer function can be decomposed into an all-pole section $H_1(z) = 1/A(z)$ and an all-zero section $H_2(z) = B(z)$.\n\n1.  **Direct Form II (DF-II) Realization**\n\nThe Direct Form II structure realizes the system by cascading the all-pole section followed by the all-zero section, $H(z) = H_2(z)H_1(z)$, and sharing the delay elements between the two sections to achieve a canonical implementation. Let $w[n]$ be the intermediate signal at the output of the all-pole section, such that its Z-transform is $W(z) = X(z)/A(z)$, and the system output is $Y(z) = B(z)W(z)$.\n\nThe corresponding difference equations are:\n$$\nA(z)W(z) = X(z) \\implies \\left(1 + \\sum_{k=1}^{N} a_k z^{-k}\\right)W(z) = X(z)\n$$\nIn the time domain, this gives the recursive equation for the intermediate signal $w[n]$:\n$$\nw[n] = x[n] - \\sum_{k=1}^{N} a_k w[n-k]\n$$\nAnd for the output signal $y[n]$:\n$$\nY(z) = B(z)W(z) \\implies Y(z) = \\left(\\sum_{k=0}^{M} b_k z^{-k}\\right)W(z)\n$$\nIn the time domain, this gives the non-recursive equation for the output:\n$$\ny[n] = \\sum_{k=0}^{M} b_k w[n-k]\n$$\nWe now count the required components based on these two equations.\n\n*   **Multipliers:** The recursive equation for $w[n]$ involves multiplying the $N$ delayed signals $w[n-1], \\dots, w[n-N]$ by the coefficients $a_1, \\dots, a_N$. Since the problem states that unary sign changes are absorbed into the multiplier, this requires $N$ real multipliers. The non-recursive equation for $y[n]$ involves multiplying the signals $w[n], w[n-1], \\dots, w[n-M]$ by the coefficients $b_0, \\dots, b_M$. This requires $M+1$ real multipliers. The total number of multipliers is $n_{\\text{mult}} = N + (M+1) = M+N+1$.\n\n*   **Two-Input Adders:** The calculation of $w[n]$ is a summation of $N+1$ terms: $x[n]$ and the $N$ products $-a_k w[n-k]$. Based on the given rule that a sum of $K$ signals requires $K-1$ two-input adders, this computation requires $(N+1)-1 = N$ adders. The calculation of $y[n]$ is a summation of $M+1$ terms. This requires $(M+1)-1 = M$ adders. The total number of adders is $n_{\\text{add}} = N+M = M+N$.\n\n*   **Unit Delays:** The DF-II structure uses a single shared delay line for the intermediate signal $w[n]$. The recursive part requires delays up to $w[n-N]$ and the non-recursive part requires delays up to $w[n-M]$. To satisfy both, the delay line must have a length equal to the greater of the two orders. The total number of unit delay elements is $n_{\\text{delay}} = \\max(M, N)$.\n\nIn summary, for the Direct Form II realization, the component counts are:\n$n_{\\text{mult}} = M+N+1$\n$n_{\\text{add}} = M+N$\n$n_{\\text{delay}} = \\max(M, N)$\n\n2.  **Transposed Direct Form II (TDF-II) Realization**\n\nThe Transposed Direct Form II structure is derived by applying the transposition theorem to the signal-flow graph of the Direct Form II structure. The problem states as a given fact that this theorem \"(iii) preserves the transfer function and the counts of linear elements while reversing interconnections.\" The linear elements are the multipliers, adders, and delay elements.\n\nTherefore, the transposition operation, which involves reversing the direction of all signal paths and interchanging summation nodes with pick-off nodes, does not alter the total number of each type of component. It only reconfigures their connectivity.\n\nBased on this principle, the component counts for the TDF-II structure must be identical to those of the DF-II structure.\n\n*   **Multipliers:** $n_{\\text{mult}} = M+N+1$\n*   **Two-Input Adders:** $n_{\\text{add}} = M+N$\n*   **Unit Delays:** $n_{\\text{delay}} = \\max(M, N)$\n\n3.  **Equality of Totals and Explanation**\n\nThe total counts for real multipliers, two-input adders, and unit delays are indeed equal for the Direct Form II and Transposed Direct Form II structures.\n\nThe fundamental reason for this equality is the transposition theorem for linear signal-flow graphs. A signal-flow graph is a collection of nodes interconnected by directed branches. The branches represent linear operations, which in this context are multiplication by a constant (multiplier) or multiplication by $z^{-1}$ (unit delay). The nodes represent either the splitting of a signal (pick-off node) or the summation of multiple signals (summer node).\n\nWhen a graph is transposed, all branch directions are reversed. A summer node with $K$ inputs and $1$ output becomes a pick-off node with $1$ input and $K$ outputs. Conversely, a pick-off node becomes a summer. The branches themselves, representing multipliers and delays, remain in place with their inputs and outputs swapped.\n\nCrucially, as stated in the problem's premises, the total number of each type of linear element is invariant under this transformation. Since the TDF-II structure is by definition the transpose of the DF-II structure, they are guaranteed to have the same inventory of components. The difference lies not in the *what* or *how many*, but in the *where*â€”the topological arrangement of these components, which alters the flow of signals and the numerical properties of the filter implementation (such as its sensitivity to coefficient quantization and round-off noise), but not the fundamental component count required for the realization.\n\nThe common totals for both structures are therefore $[n_{\\text{mult}},\\,n_{\\text{add}},\\,n_{\\text{delay}}] = [M+N+1,\\, M+N,\\, \\max(M,N)]$.", "answer": "$$\n\\boxed{\\begin{bmatrix} M+N+1 & M+N & \\max(M,N) \\end{bmatrix}}\n$$", "id": "2915276"}, {"introduction": "After establishing that transposed structures are equivalent in component cost, we must verify that they are also functionally equivalent. This hands-on coding practice tasks you with numerically proving that the transposition operation preserves a system's input-output behavior by showing that the impulse response of a state-space realization and its transpose are identical. This exercise bridges the gap between the algebraic proof of transfer function invariance and its concrete manifestation in a simulated system [@problem_id:2915280].", "problem": "You are given discrete-time Linear Time-Invariant (LTI) state-space realizations of order $2$, each specified by a quadruple $(A,B,C,D)$ with $A \\in \\mathbb{R}^{2 \\times 2}$, $B \\in \\mathbb{R}^{2 \\times 1}$, $C \\in \\mathbb{R}^{1 \\times 2}$, and $D \\in \\mathbb{R}$. The system dynamics are defined by the state and output equations\n$$\nx[k+1] = A\\,x[k] + B\\,u[k], \\quad y[k] = C\\,x[k] + D\\,u[k],\n$$\nwith zero initial state $x[0] = 0$. For the transposed direct form structure, define the transposed realization by $(\\tilde{A},\\tilde{B},\\tilde{C},\\tilde{D}) = (A^{T},C^{T},B^{T},D^{T})$, where the superscript $T$ denotes the matrix transpose. Consider the unit impulse input $u[k]$ defined by $u[0] = 1$ and $u[k] = 0$ for all $k \\ge 1$.\n\nTask:\n1) For each given test case $(A,B,C,D)$, compute $(\\tilde{A},\\tilde{B},\\tilde{C},\\tilde{D}) = (A^{T},C^{T},B^{T},D^{T})$.\n2) For both the original and the transposed realizations, simulate the response to the unit impulse input with zero initial state $x[0]=0$ and compute the output sequences $\\{y[k]\\}_{k=0}^{N-1}$ and $\\{\\tilde{y}[k]\\}_{k=0}^{N-1}$ for $N$ time steps.\n3) Verify numerically that the impulse responses are identical by checking whether\n$$\n\\max_{0 \\le k \\le N-1} \\left| y[k] - \\tilde{y}[k] \\right| \\le \\tau,\n$$\nwhere $\\tau$ is a small nonnegative tolerance. The verification result for each test case should be a boolean value that is `True` if the inequality holds and `False` otherwise.\n\nUse the following test suite of four second-order Single-Input Single-Output (SISO) realizations, with $N=12$ and tolerance $\\tau = 10^{-11}$:\n- Test case $1$ (strictly proper, stable): \n$$\nA = \\begin{bmatrix} 0.7 & 0.2 \\\\ -0.1 & 0.9 \\end{bmatrix},\\;\nB = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix},\\;\nC = \\begin{bmatrix} 0.3 & -0.4 \\end{bmatrix},\\;\nD = 0.0.\n$$\n- Test case $2$ (nonzero direct feedthrough $D$):\n$$\nA = \\begin{bmatrix} 0.5 & -0.3 \\\\ 0.1 & 0.4 \\end{bmatrix},\\;\nB = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix},\\;\nC = \\begin{bmatrix} 1.0 & 0.2 \\end{bmatrix},\\;\nD = 0.25.\n$$\n- Test case $3$ (repeated eigenvalue, Jordan form):\n$$\nA = \\begin{bmatrix} 0.6 & 1.0 \\\\ 0.0 & 0.6 \\end{bmatrix},\\;\nB = \\begin{bmatrix} 0.2 \\\\ 0.3 \\end{bmatrix},\\;\nC = \\begin{bmatrix} -0.5 & 0.7 \\end{bmatrix},\\;\nD = 0.0.\n$$\n- Test case $4$ (edge case with zero input-to-state coupling):\n$$\nA = \\begin{bmatrix} 0.8 & 0.0 \\\\ 0.0 & 0.8 \\end{bmatrix},\\;\nB = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\\;\nC = \\begin{bmatrix} 0.9 & -0.1 \\end{bmatrix},\\;\nD = -0.3.\n$$\n\nYour program must:\n- Internally construct these four test cases.\n- For each test case, compute the transposed realization $(A^{T},C^{T},B^{T},D^{T})$ and simulate the impulse responses for $N=12$ samples.\n- Compare the two output sequences elementwise and compute the maximum absolute difference.\n- Return a boolean result for each test case indicating whether the maximum difference is at most $\\tau = 10^{-11}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of linear systems theory, is well-posed with a complete and consistent set of givens, and is expressed in objective mathematical language. We shall proceed with a solution.\n\nThe fundamental principle to be verified is the invariance of the input-output behavior of a Single-Input Single-Output (SISO) Linear Time-Invariant (LTI) system under a specific state-space transformation known as transposition. The impulse response of a system is a unique characterization of its input-output properties. If two systems have identical impulse responses, they are externally equivalent.\n\nA discrete-time LTI system is given by the state-space realization $(A, B, C, D)$:\n$$\nx[k+1] = A x[k] + B u[k]\n$$\n$$\ny[k] = C x[k] + D u[k]\n$$\nwhere $x[k] \\in \\mathbb{R}^n$ is the state vector at time $k$, $u[k] \\in \\mathbb{R}$ is the input, $y[k] \\in \\mathbb{R}$ is the output, and the matrices have dimensions $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times 1}$, $C \\in \\mathbb{R}^{1 \\times n}$, and $D \\in \\mathbb{R}$.\n\nThe transfer function $H(z)$ from the input $U(z)$ to the output $Y(z)$ in the Z-domain is given by:\n$$\nH(z) = C(zI - A)^{-1}B + D\n$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nThe problem defines the transposed realization as $(\\tilde{A}, \\tilde{B}, \\tilde{C}, \\tilde{D}) = (A^T, C^T, B^T, D^T)$. This is a standard definition in systems theory, where $D^T=D$ since $D$ is a scalar. The state equations for the transposed system are:\n$$\n\\tilde{x}[k+1] = \\tilde{A} \\tilde{x}[k] + \\tilde{B} u[k] = A^T \\tilde{x}[k] + C^T u[k]\n$$\n$$\n\\tilde{y}[k] = \\tilde{C} \\tilde{x}[k] + \\tilde{D} u[k] = B^T \\tilde{x}[k] + D u[k]\n$$\nThe transfer function $\\tilde{H}(z)$ of the transposed system is:\n$$\n\\tilde{H}(z) = \\tilde{C}(zI - \\tilde{A})^{-1}\\tilde{B} + \\tilde{D} = B^T(zI - A^T)^{-1}C^T + D\n$$\nUsing the matrix identity $(M^{-1})^T = (M^T)^{-1}$, we can write $(zI - A^T)^{-1} = ((zI - A)^T)^{-1} = ((zI - A)^{-1})^T$. Substituting this into the expression for $\\tilde{H}(z)$:\n$$\n\\tilde{H}(z) = B^T((zI - A)^{-1})^T C^T + D\n$$\nUsing the identity $(MNP)^T = P^T N^T M^T$, we can recognize the term $B^T((zI - A)^{-1})^T C^T$ as the transpose of a product:\n$$\nB^T((zI - A)^{-1})^T C^T = (C(zI - A)^{-1}B)^T\n$$\nTherefore, the transfer function of the transposed system is:\n$$\n\\tilde{H}(z) = (C(zI - A)^{-1}B)^T + D\n$$\nFor a SISO system, the term $C(zI - A)^{-1}B$ is a $1 \\times 1$ matrix, i.e., a scalar. A scalar is equal to its own transpose. Thus, $(C(zI - A)^{-1}B)^T = C(zI - A)^{-1}B$. This leads to the conclusion:\n$$\n\\tilde{H}(z) = H(z)\n$$\nSince the transfer functions of the original and transposed systems are identical, their impulse responses must also be identical. The impulse response, $\\{h[k]\\}_{k=0}^{\\infty}$, is the inverse Z-transform of the transfer function $H(z)$. The problem requires a numerical verification of this theoretical fact, $y[k] = \\tilde{y}[k]$ for a unit impulse input, by direct simulation over a finite time horizon $N=12$.\n\nThe simulation procedure is as follows for each test case $(A, B, C, D)$:\n$1$. Define the simulation parameters: number of steps $N=12$ and tolerance $\\tau=10^{-11}$.\n$2$. Construct the original system matrices $(A, B, C, D)$ and the transposed system matrices $(\\tilde{A}, \\tilde{B}, \\tilde{C}, \\tilde{D}) = (A^T, C^T, B^T, D)$.\n$3$. Initialize the state vectors for both systems to zero: $x[0] = \\mathbf{0}$ and $\\tilde{x}[0] = \\mathbf{0}$. These are vectors in $\\mathbb{R}^2$.\n$4$. Define the unit impulse input: $u[0] = 1$ and $u[k] = 0$ for $k \\in \\{1, 2, \\dots, N-1\\}$.\n$5$. Simulate both systems iteratively for $k$ from $0$ to $N-1$:\n   a. Compute the outputs at step $k$:\n      - $y[k] = C x[k] + D u[k]$\n      - $\\tilde{y}[k] = \\tilde{C} \\tilde{x}[k] + \\tilde{D} u[k]$\n   b. Store $y[k]$ and $\\tilde{y}[k]$ in their respective output sequences.\n   c. Compute the next states for step $k+1$:\n      - $x[k+1] = A x[k] + B u[k]$\n      - $\\tilde{x}[k+1] = \\tilde{A} \\tilde{x}[k] + \\tilde{B} u[k]$\n$6$. After completing the simulation for $N$ steps, compute the maximum absolute difference between the two output sequences:\n$$\n\\Delta_{\\max} = \\max_{0 \\le k \\le N-1} |y[k] - \\tilde{y}[k]|\n$$\n$7$. The verification is successful if $\\Delta_{\\max} \\le \\tau$. The result for the test case is $\\mathrm{True}$ if this condition holds, and $\\mathrm{False}$ otherwise. Any observed non-zero difference is attributable to floating-point numerical errors, which are expected to be well below the specified tolerance $\\tau$.\n\nThis procedure is applied to all four specified test cases. Based on the theory, the expected result for all four cases is $\\mathrm{True}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate(A, B, C, D, N, u_seq):\n    \"\"\"\n    Simulates the output of a discrete-time LTI state-space system.\n\n    Args:\n        A (np.ndarray): State matrix (n x n).\n        B (np.ndarray): Input matrix (n x 1).\n        C (np.ndarray): Output matrix (1 x n).\n        D (float): Direct feedthrough scalar.\n        N (int): Number of time steps to simulate.\n        u_seq (np.ndarray): Input signal sequence of length N.\n\n    Returns:\n        np.ndarray: The output sequence y[k] for k=0 to N-1.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros((n, 1))\n    y_seq = np.zeros(N)\n\n    for k in range(N):\n        # Calculate output y[k]\n        y_k = C @ x + D * u_seq[k]\n        y_seq[k] = y_k.item()\n        \n        # Update state to x[k+1]\n        x = A @ x + B * u_seq[k]\n        \n    return y_seq\n\n\ndef solve():\n    \"\"\"\n    Solves the problem by verifying the equivalence of impulse responses for\n    four LTI systems and their transpositions.\n    \"\"\"\n    # Define problem parameters\n    N = 12\n    tau = 1e-11\n\n    # Define the unit impulse input sequence\n    u_impulse = np.zeros(N)\n    u_impulse[0] = 1.0\n\n    # Define the four test cases\n    test_cases = [\n        # Test case 1 (strictly proper, stable)\n        {\n            \"A\": np.array([[0.7, 0.2], [-0.1, 0.9]]),\n            \"B\": np.array([[1.0], [0.5]]),\n            \"C\": np.array([[0.3, -0.4]]),\n            \"D\": 0.0,\n        },\n        # Test case 2 (nonzero direct feedthrough D)\n        {\n            \"A\": np.array([[0.5, -0.3], [0.1, 0.4]]),\n            \"B\": np.array([[0.0], [1.0]]),\n            \"C\": np.array([[1.0, 0.2]]),\n            \"D\": 0.25,\n        },\n        # Test case 3 (repeated eigenvalue, Jordan form)\n        {\n            \"A\": np.array([[0.6, 1.0], [0.0, 0.6]]),\n            \"B\": np.array([[0.2], [0.3]]),\n            \"C\": np.array([[-0.5, 0.7]]),\n            \"D\": 0.0,\n        },\n        # Test case 4 (edge case with zero input-to-state coupling)\n        {\n            \"A\": np.array([[0.8, 0.0], [0.0, 0.8]]),\n            \"B\": np.array([[0.0], [0.0]]),\n            \"C\": np.array([[0.9, -0.1]]),\n            \"D\": -0.3,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, B, C, D = case[\"A\"], case[\"B\"], case[\"C\"], case[\"D\"]\n\n        # 1. Compute the transposed realization\n        At = A.T\n        Bt = C.T\n        Ct = B.T\n        Dt = D  # D is a scalar, so D.T = D\n\n        # 2. Simulate impulse response for both systems\n        y_original = simulate(A, B, C, D, N, u_impulse)\n        y_transposed = simulate(At, Bt, Ct, Dt, N, u_impulse)\n\n        # 3. Verify that the impulse responses are identical\n        max_diff = np.max(np.abs(y_original - y_transposed))\n        verification_result = max_diff <= tau\n        results.append(verification_result)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) correctly converts Python's True/False to strings \"True\"/\"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2915280"}, {"introduction": "If both DF-II and TDF-II structures use the same number of components and produce the same ideal output, why would we prefer one over the other? This advanced problem explores the crucial differences that arise in practical fixed-point implementations, where quantization and overflow are unavoidable. By analyzing the conditions for zero-input limit cycles, you will discover how the internal topology of the transposed form impacts its stability and performance under finite-precision arithmetic, a critical consideration for any hardware or embedded DSP designer [@problem_id:2915283].", "problem": "Consider a causal, stable, first-order Infinite Impulse Response (IIR) digital filter implemented in the Transposed Direct Form II (TDF-II) structure. Let the internal state be denoted by $v[n]$. The zero-input ($x[n] = 0$ for all $n$) state recursion of the ideal linear filter is $v[n] = a_{1}v[n-1]$ with $|a_{1}| < 1$. In fixed-point implementation, every adder output in the TDF-II structure is quantized to a uniform, mid-tread, round-to-nearest quantizer $Q(\\cdot)$ with step size $\\Delta > 0$. Overflow is handled by either saturation arithmetic (clipping to the representable range) or wrap-around arithmetic (modulo arithmetic on the representable set). Assume that the coefficient $a_{1}$ is exactly representable and that quantization occurs only at the adder outputs (i.e., multipliers are exact but their results are quantized at the following adder). The zero-input nonlinear recursion for the internal state can be modeled as\n$$\nv[n] = \\mathcal{O}\\!\\left(Q\\!\\big(a_{1}v[n-1]\\big)\\right),\n$$\nwhere $\\mathcal{O}(\\cdot)$ is the overflow operator that equals the identity under saturation arithmetic as long as no clipping occurs and equals reduction modulo the word range under wrap-around arithmetic.\n\nYou are asked to compare saturation arithmetic and wrap-around arithmetic in terms of zero-input limit cycles (nonzero, bounded, periodic solutions of the recursion with $x[n] = 0$) by deriving conditions from first principles. Use only the following foundational facts:\n- Uniform mid-tread quantization by round-to-nearest satisfies $Q(u) = u + e$ with $e \\in \\left[-\\frac{\\Delta}{2},\\frac{\\Delta}{2}\\right]$ for any real $u$.\n- The linear recursion $v[n] = a_{1}v[n-1]$ with $|a_{1}|<1$ is exponentially stable in the absence of quantization.\n- In finite-word arithmetic, the representable set is finite, so any deterministic zero-input recursion on this set yields ultimately periodic state trajectories.\n\nTasks:\n- Starting from the above base, establish for saturation arithmetic a sharp threshold on $|a_{1}|$ that separates guaranteed extinction of the zero-input state (i.e., convergence to $v[n]=0$ for every initial state and no zero-input limit cycles) from the possibility of nontrivial zero-input limit cycles. Your derivation must not assume or quote any specialized finite-word-length theorems beyond the stated foundational facts; proceed by bounding trajectories and reasoning about invariant sets.\n- Briefly contrast this result with wrap-around arithmetic by indicating how the absence of clipping affects the existence of zero-input limit cycles.\n\nAnswer specification:\n- Your final answer must be the critical value of $|a_{1}|$ that separates the two behaviors under saturation arithmetic, expressed as an exact number. Do not include units.\n- No numerical rounding is required.", "solution": "The problem requires an analysis of zero-input limit cycles in a first-order digital filter implemented in Transposed Direct Form II, comparing saturation and wrap-around arithmetic.\n\nFirst, the problem must be validated.\nGivens:\n- System: Causal, stable, first-order IIR digital filter, Transposed Direct Form II (TDF-II).\n- State: $v[n]$.\n- Ideal zero-input recursion: $v[n] = a_{1}v[n-1]$, with $|a_{1}| < 1$.\n- Nonlinear zero-input recursion: $v[n] = \\mathcal{O}\\!\\left(Q\\!\\big(a_{1}v[n-1]\\big)\\right)$.\n- Quantizer $Q(\\cdot)$: Uniform, mid-tread, round-to-nearest, with step size $\\Delta > 0$.\n- Overflow operator $\\mathcal{O}(\\cdot)$: Saturation or wrap-around arithmetic.\n- Coefficient $a_1$: Exactly representable.\n- Quantization location: At adder outputs.\n- Foundational fact 1: $Q(u) = u + e$ with $e \\in \\left[-\\frac{\\Delta}{2},\\frac{\\Delta}{2}\\right]$.\n- Foundational fact 2: The linear system is exponentially stable.\n- Foundational fact 3: Deterministic recursion on a finite set is ultimately periodic.\n\nValidation:\nThe problem is scientifically grounded, rooted in the standard analysis of finite-word-length effects in digital signal processing. The model provided is a well-established simplification used to study quantization nonlinearities. The problem is well-posed, objective, and internally consistent, asking for a derivation of a known result from first principles. It contains no scientific flaws, ambiguities, or contradictions. The problem is therefore valid.\n\n**Part 1: Derivation for Saturation Arithmetic**\n\nUnder saturation arithmetic, the overflow operator $\\mathcal{O}(\\cdot)$ clips the result to the representable range. We are interested in the conditions that guarantee the state $v[n]$ converges to $0$ for any initial state $v[0]$. A non-trivial limit cycle is a bounded, periodic sequence. For such a cycle to exist, its states must lie within the non-overflow range of the arithmetic. Thus, for the analysis of limit cycle existence, we can disregard the overflow operator $\\mathcal{O}(\\cdot)$ and analyze the recursion:\n$$\nv[n] = Q(a_{1}v[n-1])\n$$\nA zero-input limit cycle can only be sustained if the state does not enter the \"deadband\" around zero. The deadband is the set of values $x$ for which $Q(x) = 0$. For a mid-tread quantizer with step size $\\Delta$, this occurs when its argument is in the interval $(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2})$. Therefore, if at some time step $N$, we have $|a_{1}v[N-1]| < \\frac{\\Delta}{2}$, then $v[N] = Q(a_{1}v[N-1]) = 0$, and the state will remain zero for all subsequent time, $v[n]=0$ for $n \\ge N$.\n\nTo guarantee that no non-trivial limit cycles exist, we must find the condition on $a_{1}$ that ensures the magnitude of the state, $|v[n]|$, strictly decreases at every step, as long as the state is outside the deadband. This will force any trajectory to eventually enter the deadband and collapse to zero.\nLet $v[n-1]$ be a state value such that it does not fall into the deadband in the next step, i.e., $|a_{1}v[n-1]| \\ge \\frac{\\Delta}{2}$. We require that $|v[n]| < |v[n-1]|$.\n\nUsing the given property of the quantizer, $Q(u) = u + e$ where $|e| \\le \\frac{\\Delta}{2}$, we can write:\n$$\nv[n] = a_{1}v[n-1] + e[n], \\quad \\text{where } |e[n]| \\le \\frac{\\Delta}{2}\n$$\nThe magnitude of the next state is $|v[n]| = |a_{1}v[n-1] + e[n]|$. By the triangle inequality:\n$$\n|v[n]| \\le |a_{1}v[n-1]| + |e[n]|\n$$\nSubstituting the bound for the error magnitude, we get:\n$$\n|v[n]| \\le |a_{1}| |v[n-1]| + \\frac{\\Delta}{2}\n$$\nFor the state magnitude to be guaranteed to decrease, we require $|v[n]| < |v[n-1]|$. A sufficient condition for this is:\n$$\n|a_{1}| |v[n-1]| + \\frac{\\Delta}{2} < |v[n-1]|\n$$\nRearranging this inequality to solve for $|v[n-1]|$ gives:\n$$\n\\frac{\\Delta}{2} < |v[n-1]|(1 - |a_{1}|)\n$$\nSince $|a_{1}| < 1$ for stability, $(1 - |a_{1}|) > 0$. Thus, we can write:\n$$\n|v[n-1]| > \\frac{\\Delta}{2(1 - |a_{1}|)}\n$$\nThis inequality shows that if the magnitude of the state $|v[n-1]|$ is sufficiently large, it is guaranteed to decrease. However, for a limit cycle to be impossible, this decrease must be guaranteed for *all* states outside the deadband. The deadband is defined by $|a_{1}v[n-1]| < \\frac{\\Delta}{2}$, which is equivalent to $|v[n-1]| < \\frac{\\Delta}{2|a_{1}|}$.\nA limit cycle could potentially exist in the region where decay is not guaranteed, i.e., where $|v[n-1]| \\le \\frac{\\Delta}{2(1 - |a_{1}|)}$, but the state is not in the deadband, i.e., $|v[n-1]| \\ge \\frac{\\Delta}{2|a_{1}|}$.\n\nFor non-trivial limit cycles to be impossible, the region of guaranteed decay must cover the entire state space outside the deadband. In other words, the lower bound for guaranteed decay must be less than or equal to the lower bound of the region where limit cycles can exist. This means the condition for decay, $|v| > \\frac{\\Delta}{2(1 - |a_{1}|)}$, must be satisfied for any $v$ that is outside the deadband, i.e., for any $v$ such that $|v| \\ge \\frac{\\Delta}{2|a_{1}|}$.\nThis requires:\n$$\n\\frac{\\Delta}{2|a_{1}|} > \\frac{\\Delta}{2(1 - |a_{1}|)}\n$$\nSince $\\Delta > 0$, we can simplify this to:\n$$\n\\frac{1}{|a_{1}|} > \\frac{1}{1 - |a_{1}|}\n$$\nBecause both $|a_{1}|$ and $1 - |a_{1}|$ are positive, we can cross-multiply:\n$$\n1 - |a_{1}| > |a_{1}|\n$$\n$$\n1 > 2|a_{1}|\n$$\n$$\n|a_{1}| < \\frac{1}{2}\n$$\nIf this condition holds, then for any state $v[n-1]$ not in the deadband, its magnitude is guaranteed to strictly decrease. Since the state space in a fixed-point implementation is finite, this strictly decreasing sequence of magnitudes must eventually enter the deadband, at which point the state becomes $0$. Therefore, if $|a_{1}| < \\frac{1}{2}$, no non-trivial zero-input limit cycles can exist.\n\nConversely, if $|a_{1}| \\ge \\frac{1}{2}$, there exists a region of state values where decay is not guaranteed. For example, consider a constant limit cycle at $v[n] = V \\neq 0$. This requires $V = Q(a_{1}V)$. For a round-to-nearest quantizer, this is possible if $a_{1}V$ lies in the quantization interval corresponding to $V$. If we test $V = \\Delta$, this requires $a_{1}\\Delta$ to be rounded to $\\Delta$. The input interval that rounds to $\\Delta$ is $[\\frac{\\Delta}{2}, \\frac{3\\Delta}{2})$. Thus, we need $\\frac{\\Delta}{2} \\le a_{1}\\Delta < \\frac{3\\Delta}{2}$, which simplifies to $\\frac{1}{2} \\le a_{1} < \\frac{3}{2}$. Given $|a_{1}|<1$, this implies that if $a_{1} \\in [\\frac{1}{2}, 1)$, a limit cycle at $V = \\Delta$ is possible.\nThe sharp threshold separating guaranteed extinction from the possibility of limit cycles is therefore $|a_{1}| = \\frac{1}{2}$.\n\n**Part 2: Contrast with Wrap-Around Arithmetic**\n\nWrap-around arithmetic handles overflow fundamentally differently from saturation. Saturation is a dissipative process: when the state magnitude exceeds the representable range, it is clipped, effectively reducing the magnitude and removing \"energy\" from the system. This property is what prevents large-amplitude limit cycles.\n\nIn contrast, wrap-around (or modulo) arithmetic is non-dissipative in this sense. When a value $u$ exceeds the representable range, say $[-M, M)$, it is mapped back into the range by adding or subtracting multiples of $2M$. For example, a large positive value $M+\\epsilon$ becomes a large negative value $-M+\\epsilon$. This means a state with large magnitude is mapped to another state with large magnitude, often with a sign flip.\n\nWhile the analysis for small-amplitude \"granularity\" limit cycles (which occur in the non-overflow region) remains the same as for the saturation case, leading to the same threshold of $|a_{1}| = \\frac{1}{2}$, wrap-around arithmetic introduces a new possibility: large-amplitude \"overflow oscillations.\" In higher-order systems, this mapping of large values to other large values can sustain oscillations where the state repeatedly overflows and wraps around the number range. These overflow oscillations are a form of zero-input limit cycle that cannot occur with saturation arithmetic, which would simply clip the state at the boundaries and cause it to decay. Thus, while saturation guarantees bounded-input, bounded-output (BIBO) stability for the implemented filter if the linear filter is stable, wrap-around arithmetic can lead to large, sustained oscillations from zero input alone, representing a form of instability. The absence of clipping removes the inherent damping mechanism at the boundaries of the representable range.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "2915283"}]}