{"hands_on_practices": [{"introduction": "The choice of where to place a quantizer in a computational chain—before or after an operation—has a significant impact on the resulting error. This exercise explores the trade-offs between quantizing operands before multiplication versus quantizing the final product [@problem_id:2893694]. By deriving the Mean-Squared Error ($MSE$) for both architectures, you will gain a quantitative understanding of how signal characteristics and quantization parameters interact, a skill fundamental to designing efficient fixed-point hardware and software.", "problem": "Consider two independent, zero-mean, dimensionless, wide-sense stationary real-valued random variables $a$ and $b$ with finite variances $\\sigma_{a}^{2} = \\mathbb{E}[a^{2}]$ and $\\sigma_{b}^{2} = \\mathbb{E}[b^{2}]$. You are to compare two architectures for producing an approximation to the exact product $z = a b$:\n\n1. Operand quantization before multiplication (product quantization): The operands are passed through mid-tread uniform rounding quantizers $Q_{a}$ and $Q_{b}$ with steps $\\Delta_{a}$ and $\\Delta_{b}$, respectively, yielding $a_{q} = Q_{a}(a)$ and $b_{q} = Q_{b}(b)$, and the output is $y_{\\mathrm{pre}} = a_{q} b_{q}$.\n\n2. Product quantization after exact multiplication: The exact product $z = a b$ is computed and then quantized by a mid-tread uniform rounding quantizer $Q_{z}$ with step $\\Delta_{z}$, giving $y_{\\mathrm{post}} = Q_{z}(z)$.\n\nAssume the additive noise model for each quantizer under the high-resolution regime: $Q_{x}(x) = x + e_{x}$, where $e_{x}$ is the quantization error for signal $x \\in \\{a,b,z\\}$. For each $x$, model $e_{x}$ as independent of $x$ and of the other error processes, zero mean, and uniformly distributed on $[-\\Delta_{x}/2, \\Delta_{x}/2]$. In particular, $\\mathbb{E}[e_{x}] = 0$ and $\\mathbb{E}[e_{x}^{2}] = \\Delta_{x}^{2}/12$. \n\nDefine the total output mean-squared error (MSE) for each architecture with respect to the exact product $z = a b$ as $J_{\\mathrm{pre}} = \\mathbb{E}\\big[(y_{\\mathrm{pre}} - z)^{2}\\big]$ and $J_{\\mathrm{post}} = \\mathbb{E}\\big[(y_{\\mathrm{post}} - z)^{2}\\big]$. Work to second order in the quantization step sizes, meaning you must retain terms up to products of two quantization variances (for example, terms proportional to $\\Delta_{a}^{2} \\Delta_{b}^{2}$), and may neglect any higher-order terms beyond that order.\n\nDerive, from first principles and the stated assumptions, a closed-form analytic expression for the difference in output MSE between the two architectures,\n$$\n\\Delta J \\triangleq J_{\\mathrm{pre}} - J_{\\mathrm{post}},\n$$\nas a function of $\\sigma_{a}^{2}$, $\\sigma_{b}^{2}$, $\\Delta_{a}$, $\\Delta_{b}$, and $\\Delta_{z}$. Express your final answer as a single simplified symbolic expression. No units are required.", "solution": "The problem requires a comparative analysis of the mean-squared error (MSE) for two distinct quantization architectures in a multiplication operation. We are given two independent, zero-mean, wide-sense stationary random variables, $a$ and $b$, with variances $\\sigma_{a}^{2}$ and $\\sigma_{b}^{2}$, respectively. The objective is to derive an expression for $\\Delta J = J_{\\mathrm{pre}} - J_{\\mathrm{post}}$, where $J_{\\mathrm{pre}}$ and $J_{\\mathrm{post}}$ are the MSEs for pre-multiplication and post-multiplication quantization, respectively. We shall proceed by deriving expressions for each MSE individually, adhering to the provided additive noise model for quantization and retaining terms up to the second order in quantization variances.\n\nFirst, let us analyze the post-multiplication quantization architecture. The exact product is $z = ab$. This product is quantized to yield the output $y_{\\mathrm{post}} = Q_{z}(z)$. According to the additive noise model, we can write $y_{\\mathrm{post}} = z + e_{z}$, where $e_{z}$ is the quantization error associated with the quantizer $Q_{z}$. The properties of this error are given as $\\mathbb{E}[e_{z}] = 0$ and $\\mathbb{E}[e_{z}^{2}] = \\frac{\\Delta_{z}^{2}}{12}$.\n\nThe MSE for this architecture, $J_{\\mathrm{post}}$, is defined with respect to the exact product $z$:\n$$\nJ_{\\mathrm{post}} = \\mathbb{E}\\big[(y_{\\mathrm{post}} - z)^{2}\\big]\n$$\nSubstituting the expression for $y_{\\mathrm{post}}$, we have:\n$$\nJ_{\\mathrm{post}} = \\mathbb{E}\\big[((z + e_{z}) - z)^{2}\\big] = \\mathbb{E}[e_{z}^{2}]\n$$\nUsing the given variance of the quantization error, we find the MSE for the post-quantization case:\n$$\nJ_{\\mathrm{post}} = \\frac{\\Delta_{z}^{2}}{12}\n$$\nThis result is exact under the stated model assumptions.\n\nNext, we analyze the pre-multiplication quantization architecture. The operands $a$ and $b$ are first quantized, yielding $a_{q} = Q_{a}(a)$ and $b_{q} = Q_{b}(b)$. Using the additive noise model, we have:\n$$\na_{q} = a + e_{a}\n$$\n$$\nb_{q} = b + e_{b}\n$$\nwhere $e_{a}$ and $e_{b}$ are the respective quantization errors. Their properties are given as $\\mathbb{E}[e_{a}] = 0$, $\\mathbb{E}[e_{a}^{2}] = \\frac{\\Delta_{a}^{2}}{12}$, $\\mathbb{E}[e_{b}] = 0$, and $\\mathbb{E}[e_{b}^{2}] = \\frac{\\Delta_{b}^{2}}{12}$. The problem states that the error processes are independent of each other and of the signals.\n\nThe output for this architecture is the product of the quantized operands:\n$$\ny_{\\mathrm{pre}} = a_{q} b_{q} = (a + e_{a})(b + e_{b}) = ab + a e_{b} + b e_{a} + e_{a} e_{b}\n$$\nThe MSE, $J_{\\mathrm{pre}}$, is the expectation of the squared difference between this output and the exact product $z = ab$:\n$$\ny_{\\mathrm{pre}} - z = (ab + a e_{b} + b e_{a} + e_{a} e_{b}) - ab = a e_{b} + b e_{a} + e_{a} e_{b}\n$$\n$$\nJ_{\\mathrm{pre}} = \\mathbb{E}\\big[(y_{\\mathrm{pre}} - z)^{2}\\big] = \\mathbb{E}\\big[(a e_{b} + b e_{a} + e_{a} e_{b})^{2}\\big]\n$$\nTo evaluate this expectation, we expand the squared term:\n$$\n(a e_{b} + b e_{a} + e_{a} e_{b})^{2} = a^{2}e_{b}^{2} + b^{2}e_{a}^{2} + e_{a}^{2}e_{b}^{2} + 2ab e_{a} e_{b} + 2a e_{a} e_{b}^{2} + 2b e_{a}^{2} e_{b}\n$$\nNow, we take the expectation of this expression term by term, leveraging the mutual independence of $a, b, e_{a}, e_{b}$, and the fact that they are all zero-mean signals (except for their variances).\n\nThe expectation of the diagonal terms:\n$$\n\\mathbb{E}[a^{2}e_{b}^{2}] = \\mathbb{E}[a^{2}]\\mathbb{E}[e_{b}^{2}] = \\sigma_{a}^{2} \\frac{\\Delta_{b}^{2}}{12}\n$$\n$$\n\\mathbb{E}[b^{2}e_{a}^{2}] = \\mathbb{E}[b^{2}]\\mathbb{E}[e_{a}^{2}] = \\sigma_{b}^{2} \\frac{\\Delta_{a}^{2}}{12}\n$$\n$$\n\\mathbb{E}[e_{a}^{2}e_{b}^{2}] = \\mathbb{E}[e_{a}^{2}]\\mathbb{E}[e_{b}^{2}] = \\left(\\frac{\\Delta_{a}^{2}}{12}\\right)\\left(\\frac{\\Delta_{b}^{2}}{12}\\right) = \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144}\n$$\nThe expectation of the cross-product terms, which all contain at least one first-order moment of a zero-mean random variable:\n$$\n\\mathbb{E}[2ab e_{a} e_{b}] = 2\\mathbb{E}[a]\\mathbb{E}[b]\\mathbb{E}[e_{a}]\\mathbb{E}[e_{b}] = 2(0)(0)(0)(0) = 0\n$$\n$$\n\\mathbb{E}[2a e_{a} e_{b}^{2}] = 2\\mathbb{E}[a]\\mathbb{E}[e_{a}]\\mathbb{E}[e_{b}^{2}] = 2(0)(0)\\left(\\frac{\\Delta_{b}^{2}}{12}\\right) = 0\n$$\n$$\n\\mathbb{E}[2b e_{a}^{2} e_{b}] = 2\\mathbb{E}[b]\\mathbb{E}[e_{a}^{2}]\\mathbb{E}[e_{b}] = 2(0)\\left(\\frac{\\Delta_{a}^{2}}{12}\\right)(0) = 0\n$$\nSumming the expectations of all terms gives the total MSE for the pre-quantization case:\n$$\nJ_{\\mathrm{pre}} = \\sigma_{a}^{2} \\frac{\\Delta_{b}^{2}}{12} + \\sigma_{b}^{2} \\frac{\\Delta_{a}^{2}}{12} + \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144}\n$$\nThis expression is consistent with the requirement to retain terms up to the second order in quantization variances (i.e., products like $\\Delta_{a}^{2}\\Delta_{b}^{2}$).\n\nFinally, we compute the difference in MSE between the two architectures, $\\Delta J = J_{\\mathrm{pre}} - J_{\\mathrm{post}}$:\n$$\n\\Delta J = \\left( \\sigma_{a}^{2} \\frac{\\Delta_{b}^{2}}{12} + \\sigma_{b}^{2} \\frac{\\Delta_{a}^{2}}{12} + \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144} \\right) - \\left( \\frac{\\Delta_{z}^{2}}{12} \\right)\n$$\nTo present this as a single simplified expression, we find a common denominator of $144$:\n$$\n\\Delta J = \\frac{12\\sigma_{a}^{2} \\Delta_{b}^{2}}{144} + \\frac{12\\sigma_{b}^{2} \\Delta_{a}^{2}}{144} + \\frac{\\Delta_{a}^{2}\\Delta_{b}^{2}}{144} - \\frac{12\\Delta_{z}^{2}}{144}\n$$\n$$\n\\Delta J = \\frac{12(\\sigma_{a}^{2} \\Delta_{b}^{2} + \\sigma_{b}^{2} \\Delta_{a}^{2} - \\Delta_{z}^{2}) + \\Delta_{a}^{2}\\Delta_{b}^{2}}{144}\n$$\nThis expression represents the difference in mean-squared error between the pre-quantization and post-quantization schemes under the specified assumptions.", "answer": "$$\n\\boxed{\\frac{12(\\sigma_{a}^{2} \\Delta_{b}^{2} + \\sigma_{b}^{2} \\Delta_{a}^{2} - \\Delta_{z}^{2}) + \\Delta_{a}^{2}\\Delta_{b}^{2}}{144}}\n$$", "id": "2893694"}, {"introduction": "This practice moves from a single operation to a complete system: a second-order Infinite Impulse Response (IIR) filter, which is a core building block in digital signal processing. It requires you to integrate two critical aspects of fixed-point design: scaling to prevent overflow and analyzing the impact of round-off noise on the output Signal-to-Noise Ratio ($SNR$) [@problem_id:2893769]. Solving this problem will provide you with a concrete methodology for designing and evaluating the performance of fixed-point digital filters, ensuring both stability and the desired signal fidelity.", "problem": "Consider the causal, stable all-pole second-order section with complex-conjugate poles at radius $r$ and angle $\\theta$, described by the transfer function\n$$\nH(z) \\;=\\; \\frac{1}{1 - 2 r \\cos\\theta \\, z^{-1} + r^{2} z^{-2}} \\;=\\; \\frac{1}{\\bigl(1 - r e^{j\\theta} z^{-1}\\bigr)\\bigl(1 - r e^{-j\\theta} z^{-1}\\bigr)}.\n$$\nThe section is realized in Direct Form II Transposed with feedforward coefficients $b_{0}=1$, $b_{1}=0$, $b_{2}=0$ (so the output equals the first state), and feedback coefficients $a_{1}=2 r \\cos\\theta$ and $a_{2}=-r^{2}$. The internal arithmetic is signed fixed-point with $B=16$ total bits (one sign bit and $B-1=15$ fractional bits, that is, format $Q1.15$), dynamic range $[-1, 1)$, and rounding to nearest at every product. Specifically, the two feedback products $a_{1} y[n-1]$ and $a_{2} y[n-2]$ are each quantized to $Q1.15$ with step size $\\Delta = 2^{-(B-1)}$; denote their round-off errors by $e_{1}[n]$ and $e_{2}[n]$. Assume the standard round-off noise model in which each $e_{k}[n]$ is a zero-mean, white, uniformly distributed process over $\\bigl[-\\tfrac{\\Delta}{2},\\,\\tfrac{\\Delta}{2}\\bigr)$, independent of the signals and of each other. The input is a deterministic sinusoid at the pole angle,\n$$\nx[n] \\;=\\; A \\cos(\\theta n),\n$$\nwith unknown amplitude $A0$.\n\nUsing only fundamental properties of linear time-invariant systems, fixed-point quantization, and sinusoidal steady-state analysis, answer the following for the numerical parameters $r=0.95$ and $\\theta=\\pi/3$:\n\n1) Determine the largest input amplitude $A_{\\max}$ such that no overflow can occur anywhere in the implementation for any $n$ (recall that in this realization the internal states are $y[n]$ and $y[n-1]$). Justify why this condition is sufficient under the stated realization.\n\n2) For operation at $A=A_{\\max}$, compute the output Signal-to-Noise Ratio (SNR), where Signal-to-Noise Ratio (SNR) is defined as the ratio of the steady-state output signal power (due solely to $x[n]$) to the steady-state output noise power (due solely to $e_{1}[n]$ and $e_{2}[n]$), expressed in decibels. You must derive, from first principles, the closed-form impulse response $h[n]$ and its energy $\\sum_{n=0}^{\\infty} h^{2}[n]$ for the given $H(z)$ to evaluate the noise power.\n\nGive your final numerical results as follows:\n- Express $A_{\\max}$ as a pure number (unitless).\n- Express the SNR in decibels (dB).\n- Round both reported values to four significant figures.\n\nReport the two numbers in the final answer as a single row vector in the order $\\bigl[A_{\\max},\\,\\mathrm{SNR}_{\\mathrm{dB}}\\bigr]$.", "solution": "The problem will be validated against the required criteria before a solution is attempted.\n\n**Step 1: Extract Givens**\n- System Transfer Function: $H(z) = \\frac{1}{1 - 2 r \\cos\\theta \\, z^{-1} + r^{2} z^{-2}} = \\frac{1}{\\bigl(1 - r e^{j\\theta} z^{-1}\\bigr)\\bigl(1 - r e^{-j\\theta} z^{-1}\\bigr)}$\n- System Properties: Causal, stable, all-pole, second-order section.\n- Pole locations: Radius $r$, angle $\\theta$.\n- Realization: Direct Form II Transposed.\n- Feedforward coefficients: $b_{0}=1$, $b_{1}=0$, $b_{2}=0$.\n- Feedback coefficients: $a_{1}=2 r \\cos\\theta$, $a_{2}=-r^{2}$.\n- Arithmetic: Signed fixed-point, $B=16$ total bits ($1$ sign, $B-1=15$ fractional bits, i.e., Q$1.15$).\n- Dynamic Range: $[-1, 1)$.\n- Quantization: Rounding to nearest at every product for $a_{1} y[n-1]$ and $a_{2} y[n-2]$.\n- Round-off errors: $e_{1}[n]$ and $e_{2}[n]$ for the two products, each modeled as a zero-mean, white, uniformly distributed random process over $\\bigl[-\\frac{\\Delta}{2},\\,\\frac{\\Delta}{2}\\bigr)$, where $\\Delta = 2^{-(B-1)}$. The errors are independent of signals and of each other.\n- Input signal: $x[n] = A \\cos(\\theta n)$, with $A0$.\n- Numerical parameters: $r=0.95$, $\\theta=\\pi/3$.\n- Tasks: 1) Find the maximum input amplitude $A_{\\max}$ to prevent overflow. 2) For $A=A_{\\max}$, calculate the output Signal-to-Noise Ratio (SNR) in decibels.\n- Derivation requirements: The impulse response $h[n]$ and its energy $\\sum_{n=0}^{\\infty} h^{2}[n]$ must be derived from first principles.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard scenario in digital signal processing involving filter implementation, scaling, and noise analysis. However, a contradiction exists in the problem statement. It specifies a \"Direct Form II Transposed\" realization, but describes the quantized products as \"$a_{1} y[n-1]$ and $a_{2} y[n-2]$\". These product terms correspond to a Direct Form I realization, governed by the difference equation $y[n] = x[n] + a_1 y[n-1] + a_2 y[n-2]$. In a standard Direct Form II Transposed realization, the feedback products would involve $y[n]$ and $y[n-1]$.\n\nDespite this contradiction, the problem remains solvable. The overall transfer function $H(z)$, the coefficients $a_1$ and $a_2$, and the resulting difference equation $y[n] = x[n] + a_1 y[n-1] + a_2 y[n-2]$ are all consistent with each other. The noise model, while described with terms from a different structure, is explicit. We can show that the total output noise power is independent of whether the noise is injected as per a Direct Form I or a Direct Form II Transposed structure. Therefore, the ambiguity does not prevent a unique solution. The problem is deemed valid under the interpretation that the explicit definitions of the transfer function and quantized products take precedence over the name of the realization structure.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Part 1: Determination of Maximum Input Amplitude $A_{\\max}$**\n\nTo prevent overflow, all internal signal values must remain within the dynamic range $[-1, 1)$. The problem states to consider the internal states as $y[n]$ and $y[n-1]$, which simplifies the overflow constraint to ensuring that the output signal $y[n]$ does not exceed the range. We must enforce $|y[n]|  1$ for all $n$.\n\nThe input is a sinusoid $x[n] = A \\cos(\\theta n)$. The steady-state output $y_{\\mathrm{ss}}[n]$ is given by\n$$y_{\\mathrm{ss}}[n] = A |H(e^{j\\theta})| \\cos\\bigl(\\theta n + \\arg(H(e^{j\\theta}))\\bigr)$$\nThe amplitude of the output is $A_{\\mathrm{out}} = A |H(e^{j\\theta})|$. The overflow condition is $A_{\\mathrm{out}}  1$. The maximum amplitude $A_{\\max}$ is therefore determined by the limit $A_{\\max} |H(e^{j\\theta})| = 1$, which gives $A_{\\max} = 1/|H(e^{j\\theta})|$.\n\nWe must evaluate the frequency response $H(e^{j\\omega})$ at the input frequency $\\omega_0=\\theta$.\n$$H(e^{j\\theta}) = \\frac{1}{\\bigl(1 - r e^{j\\theta} e^{-j\\theta}\\bigr)\\bigl(1 - r e^{-j\\theta} e^{-j\\theta}\\bigr)} = \\frac{1}{(1-r)(1 - r e^{-j2\\theta})}$$\nThe magnitude is:\n$$|H(e^{j\\theta})| = \\frac{1}{|1-r| \\cdot |1 - r e^{-j2\\theta}|}$$\nSince $r=0.95  1$, $|1-r| = 1-r$. The second term's magnitude is:\n$$|1 - r e^{-j2\\theta}| = \\sqrt{(1 - r\\cos(2\\theta))^2 + (-r\\sin(2\\theta))^2} = \\sqrt{1 - 2r\\cos(2\\theta) + r^2\\cos^2(2\\theta) + r^2\\sin^2(2\\theta)} = \\sqrt{1 - 2r\\cos(2\\theta) + r^2}$$\nSubstituting the numerical values $r=0.95$ and $\\theta=\\pi/3$:\n$\\cos(2\\theta) = \\cos(2\\pi/3) = -1/2$.\n$$|H(e^{j\\pi/3})| = \\frac{1}{(1-0.95)\\sqrt{1 - 2(0.95)(-1/2) + (0.95)^2}} = \\frac{1}{0.05\\sqrt{1 + 0.95 + 0.9025}} = \\frac{1}{0.05\\sqrt{2.8525}} \\approx 11.8418$$\nThus, the maximum input amplitude is:\n$$A_{\\max} = \\frac{1}{|H(e^{j\\pi/3})|} = 0.05\\sqrt{2.8525} \\approx 0.0844467$$\nJustification for sufficiency: We must check that no other internal node overflows. As noted, the problem description contains contradictions. Assuming the Direct Form II Transposed structure, the other summer output is $g[n]=a_1 y[n] + a_2 y[n-1]$ in the steady-state. For $|y[n]| \\le 1$, the amplitude of this signal is $\\sqrt{a_1^2 + a_2^2 + 2 a_1 a_2 \\cos \\theta} = \\sqrt{(0.95)^2 + (-0.9025)^2 + 2(0.95)(-0.9025)(0.5)} \\approx 0.927  1$. Thus, scaling the output node is sufficient.\n\n**Part 2: Signal-to-Noise Ratio (SNR) Calculation**\n\nThe SNR is defined as $\\mathrm{SNR} = 10 \\log_{10}(P_{\\mathrm{signal}} / P_{\\mathrm{noise}})$.\n\nSignal Power, $P_{\\mathrm{signal}}$:\nWhen the input amplitude is $A=A_{\\max}$, the output amplitude is $A_{\\max} |H(e^{j\\theta})| = 1$. The output signal is a sinusoid with amplitude $1$. The power of a sinusoid with amplitude $C$ is $C^2/2$.\n$$P_{\\mathrm{signal}} = \\frac{1^2}{2} = \\frac{1}{2}$$\n\nNoise Power, $P_{\\mathrm{noise}}$:\nThe two round-off errors $e_1[n]$ and $e_2[n]$ are modeled as independent, white noise sources. The variance of each source, being uniformly distributed over $[-\\Delta/2, \\Delta/2]$, is:\n$$\\sigma_e^2 = \\frac{(\\Delta/2 - (-\\Delta/2))^2}{12} = \\frac{\\Delta^2}{12}$$\nWith $B=16$, the quantization step is $\\Delta = 2^{-(B-1)} = 2^{-15}$.\n$$\\sigma_e^2 = \\frac{(2^{-15})^2}{12} = \\frac{2^{-30}}{12}$$\nThe noise sources are added to the system. The total output noise power $P_{\\mathrm{noise}}$ is the sum of the powers from each source. Since the system is linear and the noise sources are independent, we can add their contributions at the output. The transfer function from the noise injection point to the output is $H(z)$. The output power due to a single white noise source with variance $\\sigma_e^2$ is $\\sigma_e^2 \\sum_{n=0}^{\\infty} h^2[n]$. Since there are two such sources, the total output noise power is:\n$$P_{\\mathrm{noise}} = (\\sigma_e^2 + \\sigma_e^2) \\sum_{n=0}^\\infty h^2[n] = 2\\sigma_e^2 \\sum_{n=0}^\\infty h^2[n] = \\frac{\\Delta^2}{6} \\sum_{n=0}^\\infty h^2[n]$$\nWe must derive the impulse response $h[n]$ and its energy $\\sum h^2[n]$. The Z-transform is $H(z) = 1/((1-pz^{-1})(1-p^*z^{-1}))$ with $p=re^{j\\theta}$. Via partial fraction expansion, $H(z)=\\frac{C}{1-pz^{-1}} + \\frac{C^*}{1-p^*z^{-1}}$, where $C=1/(1-p^*/p) = e^{j\\theta}/(2j\\sin\\theta)$. The inverse Z-transform gives:\n$$h[n] = (Cp^n + C^*(p^*)^n)u[n] = 2\\Re\\{Cp^n\\}u[n] = \\frac{r^n}{\\sin\\theta}\\sin((n+1)\\theta)u[n]$$\nThe energy of the impulse response, $\\sum_{n=0}^\\infty h^2[n]$, can be found using the complex contour integral from Parseval's theorem, or by direct summation, which is algebraically intensive. A standard result for a second-order system $H(z)=1/(1-\\alpha_1 z^{-1}-\\alpha_2 z^{-2})$ is:\n$$\\sum_{n=0}^\\infty h^2[n] = \\frac{1+\\alpha_2}{(1-\\alpha_2)((1+\\alpha_2)^2-\\alpha_1^2)}$$\nIn our system, $\\alpha_1=2r\\cos\\theta$ and $\\alpha_2=-r^2$. Substituting these yields:\n$$\\sum_{n=0}^\\infty h^2[n] = \\frac{1-r^2}{(1+r^2)((1-r^2)^2-(2r\\cos\\theta)^2)} = \\frac{1-r^2}{(1+r^2)(1-2r^2+r^4-4r^2\\cos^2\\theta)}$$\nThis expression simplifies. It is known that the result can be written as:\n$$\\sum_{n=0}^\\infty h^2[n] = \\frac{1+r^2}{(1-r^2)(1-2r^2\\cos(2\\theta)+r^4)}$$\nUsing $r=0.95$ and $\\theta=\\pi/3$, we have $r^2=0.9025$, $r^4\\approx 0.814506$, and $\\cos(2\\theta)=-1/2$.\n$$\\sum_{n=0}^\\infty h^2[n] = \\frac{1+0.9025}{(1-0.9025)(1-2(0.9025)(-0.5)+0.814506)} = \\frac{1.9025}{(0.0975)(1+0.9025+0.814506)} = \\frac{1.9025}{(0.0975)(2.717006)} \\approx 7.18170$$\nNow we compute the noise power:\n$$P_{\\mathrm{noise}} = \\frac{(2^{-15})^2}{6} \\times 7.18170 = \\frac{2^{-30}}{6} \\times 7.18170 \\approx 1.19695 \\times 2^{-30}$$\nFinally, we compute the SNR in decibels:\n$$\\mathrm{SNR} = \\frac{P_{\\mathrm{signal}}}{P_{\\mathrm{noise}}} = \\frac{0.5}{1.19695 \\times 2^{-30}} \\approx \\frac{0.5}{1.19695 \\times 9.313 \\times 10^{-10}} \\approx 4.4855 \\times 10^8$$\n$$\\mathrm{SNR}_{\\mathrm{dB}} = 10 \\log_{10}(\\mathrm{SNR}) = 10 \\log_{10}(4.4855 \\times 10^8) \\approx 86.5186$$\n\n**Final Numerical Results**\nRounding to four significant figures:\n- $A_{\\max} \\approx 0.08445$\n- $\\mathrm{SNR}_{\\mathrm{dB}} \\approx 86.52 \\, \\mathrm{dB}$\nThese are reported as a row vector $[A_{\\max}, \\mathrm{SNR}_{\\mathrm{dB}}]$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.08445  86.52\n\\end{pmatrix}\n}\n$$", "id": "2893769"}, {"introduction": "Analytical models provide powerful predictions, but their assumptions must be validated against practical performance. This exercise bridges the gap between theoretical analysis and computational implementation by tasking you with a two-part challenge [@problem_id:2893697]. First, you will use the standard noise model to predict the minimum word length required for a target Signal-to-Noise Ratio ($SNR$), and second, you will use Monte Carlo simulation to verify this prediction under randomized signal conditions. This workflow is essential for building robust systems where theoretical guarantees meet practical realities.", "problem": "You are given a finite impulse response implementation scenario for a discrete-time Linear Time-Invariant (LTI) system under product quantization, and you must determine the minimum word length that guarantees a target Signal-to-Noise Ratio (SNR) by analysis, then verify by Monte Carlo simulation the fraction of random trials that meet the target. Use the following fundamental base and definitions only.\n\nAssumptions and definitions:\n- The system is a real-valued finite impulse response filter with impulse response coefficients $\\{h[k]\\}_{k=0}^{N-1}$, producing the ideal output $s[n] = \\sum_{k=0}^{N-1} h[k]\\,x[n-k]$.\n- The system is implemented with product quantization: each product $h[k]\\,x[n-k]$ is individually quantized to $b$ bits by a uniform mid-tread quantizer with range $[-1,1)$ and step size $\\Delta = 2^{1-b}$. The quantized product is $q(h[k]\\,x[n-k])$ and the realized output is $y[n] = \\sum_{k=0}^{N-1} q(h[k]\\,x[n-k])$.\n- Quantization round-off noise model: for each product, the quantization error $e_k[n] = q(h[k]\\,x[n-k]) - h[k]\\,x[n-k]$ is modeled as a white, zero-mean random process independent of the signal and independent across $k$ and $n$, with variance $\\sigma_q^2 = \\Delta^2/12$.\n- Input signal is a real sinusoid $x[n] = A \\sin(\\omega_0 n + \\phi)$ with amplitude $A$ drawn independently and uniformly from $[A_{\\min},A_{\\max}]$ and phase $\\phi$ drawn independently and uniformly from $[0,2\\pi)$. Angles are in radians.\n- The ideal output $s[n]$ is a sinusoid at the same frequency $\\omega_0$ with amplitude $A\\,|H(e^{j\\omega_0})|$, where $H(e^{j\\omega}) = \\sum_{k=0}^{N-1} h[k]\\,e^{-j\\omega k}$ is the Discrete-Time Fourier Transform of the impulse response. The time-average power of a real sinusoid with amplitude $\\alpha$ is $\\alpha^2/2$.\n- The decibel (dB) SNR is defined as $\\mathrm{SNR_{dB}} = 10 \\log_{10}\\left(\\frac{P_{\\text{signal}}}{P_{\\text{noise}}}\\right)$, where $P_{\\text{signal}}$ and $P_{\\text{noise}}$ are the signal and noise powers, respectively.\n\nTasks:\n1) Analytical minimum word length. For each test case, derive from first principles the minimum integer word length $b_{\\min}$ that guarantees the target SNR for a reference amplitude equal to the $p_{\\mathrm{ref}}$-quantile of the amplitude distribution, that is $A_{\\mathrm{ref}} = A_{\\min} + p_{\\mathrm{ref}}(A_{\\max} - A_{\\min})$. Use only the above assumptions and definitions to reason from the base.\n2) Monte Carlo verification. For each test case, run a simulation with $T$ trials. In each trial, draw $A \\sim \\mathcal{U}[A_{\\min},A_{\\max}]$ and $\\phi \\sim \\mathcal{U}[0,2\\pi)$ independently, generate $x[n] = A \\sin(\\omega_0 n + \\phi)$ for $n=0,1,\\dots, M+N-2$, and form both $s[n]$ and $y[n]$ for the $M$ valid output samples according to the described implementation. Estimate the SNR for the trial by the sample-variance ratio $\\widehat{\\mathrm{SNR}} = \\frac{\\mathrm{var}(s)}{\\mathrm{var}(y-s)}$ over the $M$ valid samples. A trial is counted as meeting the target if $10 \\log_{10}(\\widehat{\\mathrm{SNR}}) \\ge \\mathrm{SNR_{dB}^{target}}$. If $\\mathrm{var}(s)=0$ or $\\mathrm{var}(y-s)=0$, count the trial as not meeting the target.\n3) For each test case, report the simulated fraction (a decimal in $[0,1]$) of trials meeting the target when the quantizer uses $b_{\\min}$ bits computed in Task $1$.\n\nUniform quantizer details:\n- Use a mid-tread uniform quantizer with $2^b$ levels over $[-1,1)$ and step size $\\Delta = 2^{1-b}$. Quantization is by rounding to the nearest grid point followed by saturation to the nearest representable level if necessary, i.e., $q(v) = \\mathrm{clip}\\!\\left(\\Delta \\cdot \\mathrm{round}\\!\\left(\\frac{v}{\\Delta}\\right), -1+\\frac{\\Delta}{2}, 1-\\frac{\\Delta}{2}\\right)$.\n\nImportant constraints:\n- Assume $|h[k]| \\le 1$ and $A_{\\max} \\le 1$ so that each unquantized product $h[k]\\,x[n-k]$ lies in $[-1,1]$ and the quantizer does not saturate under ideal conditions.\n- Angles must be treated in radians.\n- All outputs that are percentages must be expressed as decimals in $[0,1]$.\n\nTest suite:\nProvide results for the following three test cases. Each case specifies the impulse response coefficients $\\{h[k]\\}$, the sinusoid frequency $\\omega_0$, the target SNR in decibels, the amplitude range $[A_{\\min},A_{\\max}]$, the amplitude reference quantile $p_{\\mathrm{ref}}$, the number of trials $T$, and the number of valid output samples $M$ to use in each trial.\n\n- Case 1 (happy path):\n  - $h = [0.25,\\,0.25,\\,0.25,\\,0.25]$\n  - $\\omega_0 = 0.2\\pi$\n  - $\\mathrm{SNR_{dB}^{target}} = 40$\n  - $A_{\\min} = 0.5$, $A_{\\max} = 1.0$\n  - $p_{\\mathrm{ref}} = 0.5$\n  - $T = 600$\n  - $M = 4096$\n\n- Case 2 (frequency response dip coverage):\n  - $h = [1,\\,-1,\\,1,\\,-1,\\,1,\\,-1,\\,1,\\,-1]$\n  - $\\omega_0 = 0.2\\pi$\n  - $\\mathrm{SNR_{dB}^{target}} = 30$\n  - $A_{\\min} = 0.2$, $A_{\\max} = 0.8$\n  - $p_{\\mathrm{ref}} = 0.5$\n  - $T = 400$\n  - $M = 4096$\n\n- Case 3 (amplitude edge case with zero lower bound):\n  - $h = [0.8,\\,0.6]$\n  - $\\omega_0 = 0.1\\pi$\n  - $\\mathrm{SNR_{dB}^{target}} = 20$\n  - $A_{\\min} = 0.0$, $A_{\\max} = 1.0$\n  - $p_{\\mathrm{ref}} = 0.9$\n  - $T = 800$\n  - $M = 4096$\n\nDeterminism requirement:\n- Use a fixed pseudorandom generator seed equal to $12345$ to ensure reproducibility.\n\nRequired final output format:\n- Your program should produce a single line of output containing the three simulated fractions, in the order of the test cases, as a comma-separated list enclosed in square brackets, with each fraction rounded to three decimal places. For example, the format must be like $[0.812,0.945,0.903]$.", "solution": "The problem requires the determination of the minimum quantizer word length for a Finite Impulse Response (FIR) filter under product quantization to meet a specific Signal-to-Noise Ratio (SNR), and subsequent verification of this result via Monte Carlo simulation. The problem is valid as it is scientifically grounded in established principles of digital signal processing, is well-posed with all necessary information provided, and is stated objectively.\n\nThe solution is presented in two main parts: first, an analytical derivation of the minimum word length $b_{\\min}$, and second, a description of the Monte Carlo simulation methodology used for verification.\n\n**1. Analytical Derivation of Minimum Word Length ($b_{\\min}$)**\n\nThe goal is to find the minimum integer word length $b$ that satisfies a target SNR, $\\mathrm{SNR_{dB}^{target}}$, for a reference input amplitude $A_{\\mathrm{ref}}$. This involves deriving expressions for the output signal power and the quantization noise power.\n\n**1.1. Output Signal Power ($P_s$)**\nThe input signal is a sinusoid $x[n] = A \\sin(\\omega_0 n + \\phi)$. The LTI system is an FIR filter with impulse response $\\{h[k]\\}_{k=0}^{N-1}$. The ideal output $s[n]$ is the convolution of the input with the impulse response. Due to the properties of LTI systems, a sinusoidal input produces a sinusoidal output at the same frequency, but with modified amplitude and phase. The output amplitude is the input amplitude $A$ multiplied by the magnitude of the filter's frequency response $|H(e^{j\\omega_0})|$ evaluated at the input frequency $\\omega_0$. The frequency response $H(e^{j\\omega})$ is the Discrete-Time Fourier Transform (DTFT) of $h[k]$:\n$$H(e^{j\\omega}) = \\sum_{k=0}^{N-1} h[k] e^{-j\\omega k}$$\nThe output signal is thus $s[n] = A |H(e^{j\\omega_0})| \\sin(\\omega_0 n + \\phi')$, where $\\phi'$ is the new phase. The time-average power of a real sinusoid with amplitude $\\alpha$ is given as $P = \\alpha^2/2$. Therefore, the output signal power is:\n$$P_s = \\frac{\\left( A |H(e^{j\\omega_0})| \\right)^2}{2} = \\frac{A^2 |H(e^{j\\omega_0})|^2}{2}$$\n\n**1.2. Quantization Noise Power ($P_n$)**\nThe filter implementation uses product quantization, where each product term $h[k]x[n-k]$ is quantized before summation. The realized output is $y[n] = \\sum_{k=0}^{N-1} q(h[k]x[n-k])$. The total output error is the difference between the realized and ideal outputs:\n$$e[n] = y[n] - s[n] = \\sum_{k=0}^{N-1} \\left[ q(h[k]x[n-k]) - h[k]x[n-k] \\right] = \\sum_{k=0}^{N-1} e_k[n]$$\nHere, $e_k[n]$ is the quantization error for the $k$-th product at time $n$. According to the problem's statistical model, each $e_k[n]$ is a zero-mean, white random process with variance $\\sigma_q^2 = \\Delta^2/12$, where $\\Delta = 2^{1-b}$ is the quantizer step size. The errors $e_k[n]$ are assumed to be independent for different $k$. The variance of a sum of independent random variables is the sum of their variances. Therefore, the total noise power $P_n$ (which is the variance of $e[n]$) is:\n$$P_n = \\sigma_e^2 = \\mathrm{Var}\\left(\\sum_{k=0}^{N-1} e_k[n]\\right) = \\sum_{k=0}^{N-1} \\mathrm{Var}(e_k[n]) = N \\sigma_q^2$$\nSubstituting the expressions for $\\sigma_q^2$ and $\\Delta$:\n$$P_n = N \\frac{\\Delta^2}{12} = N \\frac{(2^{1-b})^2}{12} = N \\frac{2^{2-2b}}{12} = \\frac{N}{3} 2^{-2b}$$\n\n**1.3. Signal-to-Noise Ratio (SNR) and Minimum Word Length**\nThe SNR in linear scale is the ratio of signal power to noise power:\n$$\\mathrm{SNR} = \\frac{P_s}{P_n} = \\frac{A^2 |H(e^{j\\omega_0})|^2 / 2}{N 2^{-2b} / 3} = \\frac{3 A^2 |H(e^{j\\omega_0})|^2}{2N} 2^{2b}$$\nThe SNR in decibels is $\\mathrm{SNR_{dB}} = 10 \\log_{10}(\\mathrm{SNR})$. We require this to be greater than or equal to the target SNR, $\\mathrm{SNR_{dB}^{target}}$, for the reference amplitude $A = A_{\\mathrm{ref}}$.\n$$10 \\log_{10}(\\mathrm{SNR}) \\ge \\mathrm{SNR_{dB}^{target}} \\implies \\mathrm{SNR} \\ge 10^{\\mathrm{SNR_{dB}^{target}}/10}$$\nSubstituting the expression for SNR and solving for $b$:\n$$\\frac{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2}{2N} 2^{2b} \\ge 10^{\\mathrm{SNR_{dB}^{target}}/10}$$\n$$2^{2b} \\ge \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10}$$\nTaking the base-2 logarithm of both sides gives:\n$$2b \\ge \\log_2\\left( \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10} \\right)$$\n$$b \\ge \\frac{1}{2} \\log_2\\left( \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10} \\right)$$\nSince the word length $b$ must be an integer, the minimum required word length $b_{\\min}$ is the smallest integer that satisfies this inequality:\n$$b_{\\min} = \\left\\lceil \\frac{1}{2} \\log_2\\left( \\frac{2N}{3 A_{\\mathrm{ref}}^2 |H(e^{j\\omega_0})|^2} \\cdot 10^{\\mathrm{SNR_{dB}^{target}}/10} \\right) \\right\\rceil$$\nFor each test case, we compute $A_{\\mathrm{ref}} = A_{\\min} + p_{\\mathrm{ref}}(A_{\\max} - A_{\\min})$, evaluate $|H(e^{j\\omega_0})|^2$, and substitute the given parameters to find $b_{\\min}$.\n\n**2. Monte Carlo Simulation Methodology**\n\nThe simulation aims to verify the analytically derived $b_{\\min}$ by estimating the fraction of random trials that meet the target SNR. The procedure is as follows, repeated for $T$ trials for each test case.\n\n**2.1. Trial Setup**\nIn each trial, an input signal is generated with random parameters. The amplitude $A$ is drawn from a uniform distribution $\\mathcal{U}[A_{\\min},A_{\\max}]$, and the phase $\\phi$ from $\\mathcal{U}[0, 2\\pi)$. A fixed pseudorandom number generator seed ($12345$) is used to ensure reproducibility.\n\n**2.2. Signal Generation**\nFor each trial, the following signals of length $M$ are generated:\n- **Input Signal**: An input sequence $x[n] = A \\sin(\\omega_0 n + \\phi)$ is generated for $n=0, 1, \\dots, M+N-2$. This provides sufficient data for $M$ valid output samples.\n- **Ideal Output**: The ideal output $s[n]$ is computed by convolving the input $x[n]$ with the filter's impulse response $h[k]$. We use the 'valid' part of the convolution, resulting in $M$ samples: $s[j] = \\sum_{k=0}^{N-1} h[k]x[j+k]$ for $j=0, \\dots, M-1$ (after reversing one sequence for standard convolution definition).\n- **Quantized Output**: The quantized output $y[n]$ is generated by simulating the product quantization process. For each output sample $j=0, \\dots, M-1$, the corresponding sum of quantized products is calculated:\n$$y[j+N-1] = \\sum_{k=0}^{N-1} q(h[k]x[j+N-1-k])$$\nThe quantizer function $q(v)$ for a value $v$ is implemented exactly as specified, using the word length $b=b_{\\min}$ derived analytically:\n$$q(v) = \\mathrm{clip}\\!\\left(\\Delta \\cdot \\mathrm{round}\\!\\left(\\frac{v}{\\Delta}\\right), -1+\\frac{\\Delta}{2}, 1-\\frac{\\Delta}{2}\\right) \\quad \\text{with} \\quad \\Delta = 2^{1-b_{\\min}}$$\n\n**2.3. SNR Estimation and Verification**\nFor each trial, the output error sequence is computed as $e[n] = y[n] - s[n]$. The sample variances of the ideal output, $\\mathrm{var}(s)$, and the error, $\\mathrm{var}(y-s)$, are calculated over the $M$ samples. The estimated SNR for the trial is:\n$$\\widehat{\\mathrm{SNR}} = \\frac{\\mathrm{var}(s)}{\\mathrm{var}(y-s)}$$\nA trial is counted as successful if its SNR in decibels meets or exceeds the target, i.e., $10 \\log_{10}(\\widehat{\\mathrm{SNR}}) \\ge \\mathrm{SNR_{dB}^{target}}$. Trials where $\\mathrm{var}(s)=0$ or $\\mathrm{var}(y-s)=0$ are counted as failures.\n\n**2.4. Final Output**\nAfter all $T$ trials for a given test case are complete, the final reported value is the fraction of successful trials, calculated as (number of successes) / $T$. This process is repeated for all three test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It calculates the analytical minimum word length and then runs a Monte Carlo\n    simulation to find the fraction of trials meeting the SNR target.\n    \"\"\"\n    # Use a single random number generator for all test cases for reproducibility.\n    rng = np.random.default_rng(12345)\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"h\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"w0\": 0.2 * np.pi,\n            \"snr_db_target\": 40.0,\n            \"A_min\": 0.5,\n            \"A_max\": 1.0,\n            \"p_ref\": 0.5,\n            \"T\": 600,\n            \"M\": 4096,\n        },\n        # Case 2 (frequency response dip coverage)\n        {\n            \"h\": np.array([1, -1, 1, -1, 1, -1, 1, -1]),\n            \"w0\": 0.2 * np.pi,\n            \"snr_db_target\": 30.0,\n            \"A_min\": 0.2,\n            \"A_max\": 0.8,\n            \"p_ref\": 0.5,\n            \"T\": 400,\n            \"M\": 4096,\n        },\n        # Case 3 (amplitude edge case with zero lower bound)\n        {\n            \"h\": np.array([0.8, 0.6]),\n            \"w0\": 0.1 * np.pi,\n            \"snr_db_target\": 20.0,\n            \"A_min\": 0.0,\n            \"A_max\": 1.0,\n            \"p_ref\": 0.9,\n            \"T\": 800,\n            \"M\": 4096,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        h = case[\"h\"]\n        w0 = case[\"w0\"]\n        snr_db_target = case[\"snr_db_target\"]\n        A_min = case[\"A_min\"]\n        A_max = case[\"A_max\"]\n        p_ref = case[\"p_ref\"]\n        T = case[\"T\"]\n        M = case[\"M\"]\n        \n        # --- Task 1: Analytical minimum word length ---\n        A_ref = A_min + p_ref * (A_max - A_min)\n        N = len(h)\n        \n        k_indices = np.arange(N)\n        H_w0 = np.sum(h * np.exp(-1j * w0 * k_indices))\n        H_w0_mag_sq = np.abs(H_w0)**2\n\n        # Handle case where gain is zero. Set b_min to a high value as SNR is -inf.\n        if H_w0_mag_sq == 0 or A_ref == 0:\n            b_min = 32 # A practical large number, problem won't be solvable\n        else:\n            snr_linear_target = 10**(snr_db_target / 10.0)\n            # Argument for log2\n            log_arg = ( (2 * N * snr_linear_target) / \n                        (3 * A_ref**2 * H_w0_mag_sq) )\n            \n            b_min = math.ceil(0.5 * np.log2(log_arg)) if log_arg > 0 else 1\n\n        # --- Task 2: Monte Carlo verification ---\n        \n        # Define the quantizer function based on problem specification\n        delta = 2**(1 - b_min)\n        clip_min = -1 + delta / 2\n        clip_max = 1 - delta / 2\n\n        def quantizer(v):\n            quantized_v = delta * np.round(v / delta)\n            return np.clip(quantized_v, clip_min, clip_max)\n\n        success_count = 0\n        for _ in range(T):\n            A = rng.uniform(A_min, A_max)\n            phi = rng.uniform(0, 2 * np.pi)\n            \n            # Generate signals\n            x_len = M + N - 1\n            n_x = np.arange(x_len)\n            x = A * np.sin(w0 * n_x + phi)\n            \n            # Ideal output\n            s = np.convolve(x, h, mode='valid')\n            \n            # Quantized output (product quantization)\n            y = np.zeros(M)\n            n_out_start = N - 1\n            for k in range(N):\n                n_for_x = np.arange(n_out_start, n_out_start + M) - k\n                x_slice = x[n_for_x]\n                products = h[k] * x_slice\n                quantized_products = quantizer(products)\n                y += quantized_products\n\n            # Calculate estimated SNR\n            error_signal = y - s\n            var_s = np.var(s)\n            var_e = np.var(error_signal)\n            \n            if var_s > 0 and var_e > 0:\n                snr_est = var_s / var_e\n                snr_est_db = 10 * np.log10(snr_est)\n                if snr_est_db >= snr_db_target:\n                    success_count += 1\n            # else: trial fails if variance is zero (treated as not meeting target)\n\n        # --- Task 3: Report simulated fraction ---\n        fraction_success = success_count / T\n        results.append(fraction_success)\n        \n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2893697"}]}