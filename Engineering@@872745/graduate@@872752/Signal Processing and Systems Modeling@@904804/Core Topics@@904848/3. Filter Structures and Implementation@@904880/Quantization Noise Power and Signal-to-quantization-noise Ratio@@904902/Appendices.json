{"hands_on_practices": [{"introduction": "A foundational skill in analyzing analog-to-digital converters is the ability to calculate their theoretical performance limits. This first practice invites you to derive the Signal-to-Quantization-Noise Ratio ($ \\mathrm{SQNR} $) from first principles for a sinusoidal input, the standard test signal for characterizing ADCs. By analyzing how the $ \\mathrm{SQNR} $ changes when the input signal amplitude is reduced from the full-scale range, you will quantify a critical performance trade-off encountered in all practical applications. [@problem_id:2898438]", "problem": "An analog-to-digital converter (ADC) employs a uniform mid-tread quantizer with resolution $B$ bits and symmetric full-scale range $\\left[-A_{\\mathrm{FS}},\\,A_{\\mathrm{FS}}\\right]$. The quantizer uses rounding and is not overloaded. Assume the classical additive, white, signal-independent quantization-noise model with quantization error uniformly distributed over a single quantization step. A single-tone sinusoid $x(t)=A\\cos(\\omega_{0}t)$ is applied at the input.\n\nDefine the Signal-to-Quantization-Noise Ratio (SQNR) as the ratio of the input signal’s average power to the quantization error power, and define its decibel (dB) measure via $10\\log_{10}(\\cdot)$. Let $\\mathrm{SQNR}_{\\mathrm{dB,ref}}$ be the decibel-valued SQNR when the sinusoid uses the full available peak amplitude, $A=A_{\\mathrm{FS}}$, and let $\\mathrm{SQNR}_{\\mathrm{dB}}(\\gamma)$ be the decibel-valued SQNR when the sinusoid amplitude is backed off to $A=\\gamma A_{\\mathrm{FS}}$ to provide a crest factor margin, where $\\gamma\\in(0,1)$.\n\nWorking from first principles of uniform quantization, sinusoidal average power, and the stated noise model, derive the decibel offset\n$$\\Delta_{\\mathrm{dB}}=\\mathrm{SQNR}_{\\mathrm{dB}}(\\gamma)-\\mathrm{SQNR}_{\\mathrm{dB,ref}}$$\nas a closed-form expression that depends only on $\\gamma$. Express the final answer in decibels. No numerical evaluation is required, and no rounding is needed. The final answer must be a single closed-form analytic expression.", "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a rigorous derivation. It is a standard problem in the analysis of analog-to-digital conversion. I will proceed with the solution from first principles as required.\n\nThe problem asks for the decibel offset $\\Delta_{\\mathrm{dB}} = \\mathrm{SQNR}_{\\mathrm{dB}}(\\gamma) - \\mathrm{SQNR}_{\\mathrm{dB,ref}}$. We must first derive an expression for the Signal-to-Quantization-Noise Ratio (SQNR).\n\nFirst, we determine the quantization step size, $\\Delta$. The quantizer has a resolution of $B$ bits, which corresponds to $L = 2^B$ quantization levels. The full-scale range is specified as $[-A_{\\mathrm{FS}}, A_{\\mathrm{FS}}]$, so the total span is $A_{\\mathrm{FS}} - (-A_{\\mathrm{FS}}) = 2A_{\\mathrm{FS}}$. For a uniform quantizer, the step size $\\Delta$ is the total range divided by the number of levels:\n$$\n\\Delta = \\frac{2A_{\\mathrm{FS}}}{L} = \\frac{2A_{\\mathrm{FS}}}{2^B}\n$$\n\nNext, we calculate the quantization noise power, $P_q$. The problem states that the quantization error, which we denote by $e$, is modeled as a random variable uniformly distributed over a single quantization step. For a mid-tread quantizer using rounding, the error for any given sample lies in the interval $[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$. The probability density function (PDF) of the error $e$ is therefore:\n$$\np_e(x) = \\begin{cases} \\frac{1}{\\Delta} & \\text{for } -\\frac{\\Delta}{2} \\le x \\le \\frac{\\Delta}{2} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe noise power $P_q$ is the variance of the quantization error, $\\sigma_e^2 = E[e^2] - (E[e])^2$. Due to the symmetry of the PDF about zero, the mean error is $E[e] = 0$. Thus, the power is equal to the mean-square value $E[e^2]$:\n$$\nP_q = E[e^2] = \\int_{-\\infty}^{\\infty} x^2 p_e(x) \\,dx = \\int_{-\\Delta/2}^{\\Delta/2} x^2 \\frac{1}{\\Delta} \\,dx\n$$\nEvaluating the integral:\n$$\nP_q = \\frac{1}{\\Delta} \\left[ \\frac{x^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{1}{3\\Delta} \\left( \\left(\\frac{\\Delta}{2}\\right)^3 - \\left(-\\frac{\\Delta}{2}\\right)^3 \\right) = \\frac{1}{3\\Delta} \\left( \\frac{\\Delta^3}{8} + \\frac{\\Delta^3}{8} \\right) = \\frac{1}{3\\Delta} \\left( \\frac{2\\Delta^3}{8} \\right) = \\frac{\\Delta^2}{12}\n$$\nThis is the well-known result for the power of uniformly distributed quantization noise.\n\nNow, we determine the average power of the input signal, $P_x$. The signal is a single-tone sinusoid $x(t) = A\\cos(\\omega_0 t)$. The average power is the mean-square value of the signal, calculated over one period $T_0 = 2\\pi/\\omega_0$:\n$$\nP_x = \\frac{1}{T_0} \\int_{0}^{T_0} x^2(t) \\,dt = \\frac{1}{T_0} \\int_{0}^{T_0} \\left( A\\cos(\\omega_0 t) \\right)^2 \\,dt = \\frac{A^2}{T_0} \\int_{0}^{T_0} \\cos^2(\\omega_0 t) \\,dt\n$$\nUsing the trigonometric identity $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$:\n$$\nP_x = \\frac{A^2}{T_0} \\int_{0}^{T_0} \\frac{1}{2}(1 + \\cos(2\\omega_0 t)) \\,dt = \\frac{A^2}{2T_0} \\left[ t + \\frac{\\sin(2\\omega_0 t)}{2\\omega_0} \\right]_{0}^{T_0}\n$$\nThe sine term evaluates to zero at both $t=0$ and $t=T_0$. Therefore, the expression simplifies to:\n$$\nP_x = \\frac{A^2}{2T_0} (T_0) = \\frac{A^2}{2}\n$$\n\nThe SQNR is defined as the ratio of signal power to quantization noise power:\n$$\n\\mathrm{SQNR} = \\frac{P_x}{P_q} = \\frac{A^2 / 2}{\\Delta^2 / 12} = \\frac{6A^2}{\\Delta^2}\n$$\nWe substitute the expression for $\\Delta$:\n$$\n\\mathrm{SQNR} = \\frac{6A^2}{\\left( \\frac{2A_{\\mathrm{FS}}}{2^B} \\right)^2} = \\frac{6A^2 \\cdot (2^B)^2}{4A_{\\mathrm{FS}}^2} = \\frac{3}{2} \\left( \\frac{A}{A_{\\mathrm{FS}}} \\right)^2 2^{2B}\n$$\nThis is the general expression for the SQNR of a sinusoidal signal in a uniform quantizer.\n\nNow we convert to decibels (dB) and evaluate for the two specified cases. The dB measure is given by $10\\log_{10}(\\cdot)$.\n$$\n\\mathrm{SQNR}_{\\mathrm{dB}} = 10\\log_{10}(\\mathrm{SQNR}) = 10\\log_{10}\\left( \\frac{3}{2} \\left( \\frac{A}{A_{\\mathrm{FS}}} \\right)^2 2^{2B} \\right)\n$$\nUsing properties of logarithms:\n$$\n\\mathrm{SQNR}_{\\mathrm{dB}} = 10\\log_{10}\\left(\\frac{3}{2}\\right) + 10\\log_{10}\\left(\\left(\\frac{A}{A_{\\mathrm{FS}}}\\right)^2\\right) + 10\\log_{10}\\left(2^{2B}\\right)\n$$\n$$\n\\mathrm{SQNR}_{\\mathrm{dB}} = 10\\log_{10}\\left(\\frac{3}{2}\\right) + 20\\log_{10}\\left(\\frac{A}{A_{\\mathrm{FS}}}\\right) + 20B\\log_{10}(2)\n$$\n\nFor the reference case, the sinusoid uses the full peak amplitude, $A = A_{\\mathrm{FS}}$.\n$$\n\\mathrm{SQNR}_{\\mathrm{dB,ref}} = 10\\log_{10}\\left(\\frac{3}{2}\\right) + 20\\log_{10}\\left(\\frac{A_{\\mathrm{FS}}}{A_{\\mathrm{FS}}}\\right) + 20B\\log_{10}(2)\n$$\nSince $\\log_{10}(1)=0$, this becomes:\n$$\n\\mathrm{SQNR}_{\\mathrm{dB,ref}} = 10\\log_{10}\\left(\\frac{3}{2}\\right) + 20B\\log_{10}(2)\n$$\n\nFor the backed-off case, the amplitude is $A = \\gamma A_{\\mathrm{FS}}$, where $\\gamma \\in (0,1)$.\n$$\n\\mathrm{SQNR}_{\\mathrm{dB}}(\\gamma) = 10\\log_{10}\\left(\\frac{3}{2}\\right) + 20\\log_{10}\\left(\\frac{\\gamma A_{\\mathrm{FS}}}{A_{\\mathrm{FS}}}\\right) + 20B\\log_{10}(2)\n$$\n$$\n\\mathrm{SQNR}_{\\mathrm{dB}}(\\gamma) = 10\\log_{10}\\left(\\frac{3}{2}\\right) + 20\\log_{10}(\\gamma) + 20B\\log_{10}(2)\n$$\n\nFinally, we compute the required decibel offset, $\\Delta_{\\mathrm{dB}}$:\n$$\n\\Delta_{\\mathrm{dB}} = \\mathrm{SQNR}_{\\mathrm{dB}}(\\gamma) - \\mathrm{SQNR}_{\\mathrm{dB,ref}}\n$$\n$$\n\\Delta_{\\mathrm{dB}} = \\left( 10\\log_{10}\\left(\\frac{3}{2}\\right) + 20\\log_{10}(\\gamma) + 20B\\log_{10}(2) \\right) - \\left( 10\\log_{10}\\left(\\frac{3}{2}\\right) + 20B\\log_{10}(2) \\right)\n$$\nAll terms except the one involving $\\gamma$ cancel out.\n$$\n\\Delta_{\\mathrm{dB}} = 20\\log_{10}(\\gamma)\n$$\nThis expression depends only on $\\gamma$, as required. Since $\\gamma \\in (0,1)$, $\\log_{10}(\\gamma)$ is negative, correctly indicating that backing off the signal amplitude reduces the SQNR.", "answer": "$$\n\\boxed{20\\log_{10}(\\gamma)}\n$$", "id": "2898438"}, {"introduction": "Moving from analysis to design, real-world systems require careful optimization to balance competing objectives. This exercise tackles the fundamental trade-off between maximizing granular $ \\mathrm{SQNR} $ and minimizing overload distortion by adjusting the input signal gain. You will formulate and solve a constrained optimization problem to find the ideal pre-gain factor for a Gaussian signal, subject to a strict limit on the probability of clipping. [@problem_id:2898405]", "problem": "A zero-mean, wide-sense stationary Gaussian random process $x[n]$ with variance $\\sigma_{x}^{2}$ is passed through a real scalar pre-gain $g>0$ to produce $u[n]=g\\,x[n]$. The sequence $u[n]$ is fed to a uniform, symmetric, mid-tread, saturating quantizer with $B$ bits and full-scale range $[-A, A]$. The quantizer has $2^{B}$ decision intervals spanning $[-A, A]$, so the step size is $\\Delta = \\dfrac{2A}{2^{B}}$. The quantizer saturates outside $[-A, A]$. After quantization, the output is de-scaled by the factor $1/g$ to produce an estimate $\\hat{x}[n]=\\dfrac{1}{g}\\,Q(g\\,x[n])$.\n\nAdopt the high-resolution granular-noise model inside the no-overload region: on samples that do not saturate, the granular quantization error at the quantizer input is modeled as independent, zero-mean, and uniformly distributed on $[-\\Delta/2,\\Delta/2]$ with variance $\\Delta^{2}/12$. The system designer defines the signal-to-quantization-noise ratio (SQNR) as the ratio of the signal variance $\\sigma_{x}^{2}$ to the output granular-noise variance only (i.e., overload distortion is not counted in the SQNR), while imposing a hard constraint that the clipping probability (overload probability) must not exceed a target $\\varepsilon \\in (0,1)$:\n$$\n\\mathbb{P}\\!\\left(|g\\,x[n]|>A\\right)\\le \\varepsilon.\n$$\n\nUsing only first principles and the stated modeling assumptions (do not use any pre-derived “shortcut” formulas for the answer), perform the following:\n\n- Formulate the constrained optimization problem whose decision variable is $g$ and whose objective is to maximize the SQNR at the de-scaled output.\n- Argue from basic monotonicity and modeling assumptions where the optimizer must lie relative to the clipping-probability constraint.\n- Specialize to the Gaussian input $x[n]\\sim\\mathcal{N}(0,\\sigma_{x}^{2})$ and solve the optimization in closed form for the optimal pre-gain $g^{\\star}$ in terms of $A$, $\\sigma_{x}$, and $\\varepsilon$. You may use the Gaussian tail probability function (the $Q$-function) defined by\n$$\nQ(z)=\\frac{1}{\\sqrt{2\\pi}}\\int_{z}^{\\infty}\\exp\\!\\left(-\\frac{t^{2}}{2}\\right)\\,\\mathrm{d}t,\n$$\nand its functional inverse $Q^{-1}(\\cdot)$.\n\nProvide your final result as a single symbolic expression for $g^{\\star}$. No numerical evaluation is required, and no units are needed for the final answer.", "solution": "The problem as stated constitutes a valid exercise in constrained optimization within the domain of signal processing. We shall proceed with its formal analysis.\n\nThe objective is to maximize the signal-to-quantization-noise ratio (SQNR). The signal power is given as $\\sigma_{x}^{2}$. The noise to be considered is the output granular noise only. The quantization error at the input to the quantizer, which we shall denote as $e_{u}[n]$, is given by $e_{u}[n] = Q(u[n]) - u[n]$, where $u[n] = g\\,x[n]$. In the non-saturating (granular) region, i.e., for $|u[n]| \\le A$, this error is modeled as an independent, zero-mean random process, uniformly distributed on $[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$. The variance of this granular error is therefore $\\sigma_{e_{u}}^{2} = \\frac{(\\Delta/2 - (-\\Delta/2))^{2}}{12} = \\frac{\\Delta^{2}}{12}$.\n\nThe final output is $\\hat{x}[n] = \\frac{1}{g}Q(g\\,x[n])$. The error at the output is $\\hat{e}[n] = \\hat{x}[n] - x[n] = \\frac{1}{g}Q(g\\,x[n]) - x[n] = \\frac{1}{g} \\left( Q(g\\,x[n]) - g\\,x[n] \\right) = \\frac{1}{g}e_{u}[n]$. The variance of the output granular noise, $\\sigma_{n_{q}}^{2}$, is thus related to the variance of the input granular error by a factor of $\\frac{1}{g^2}$:\n$$\n\\sigma_{n_{q}}^{2} = \\text{Var}\\left( \\frac{1}{g} e_{u}[n] \\right) = \\frac{1}{g^{2}} \\sigma_{e_{u}}^{2} = \\frac{\\Delta^{2}}{12g^{2}}.\n$$\nThe quantizer step size is given as $\\Delta = \\frac{2A}{2^{B}}$. Substituting this into the expression for the noise variance gives:\n$$\n\\sigma_{n_{q}}^{2} = \\frac{1}{12g^{2}} \\left( \\frac{2A}{2^{B}} \\right)^{2} = \\frac{4A^{2}}{12g^{2}(2^{B})^{2}} = \\frac{A^{2}}{3g^{2}2^{2B}}.\n$$\nThe SQNR is defined as the ratio of the signal variance to this output granular noise variance.\n$$\n\\text{SQNR}(g) = \\frac{\\sigma_{x}^{2}}{\\sigma_{n_{q}}^{2}} = \\frac{\\sigma_{x}^{2}}{A^{2}/(3g^{2}2^{2B})} = \\frac{3 \\cdot 2^{2B}\\sigma_{x}^{2}}{A^{2}} g^{2}.\n$$\nNow, we formulate the constrained optimization problem. The decision variable is the pre-gain $g$. The objective is to maximize $\\text{SQNR}(g)$ subject to the constraint on the clipping probability.\n$$\n\\underset{g>0}{\\text{maximize}} \\quad \\text{SQNR}(g) = \\left( \\frac{3 \\cdot 2^{2B}\\sigma_{x}^{2}}{A^{2}} \\right) g^{2}\n$$\n$$\n\\text{subject to} \\quad \\mathbb{P}\\left(|g\\,x[n]|>A\\right)\\le \\varepsilon.\n$$\nThe term $\\frac{3 \\cdot 2^{2B}\\sigma_{x}^{2}}{A^{2}}$ is a positive constant. Therefore, maximizing $\\text{SQNR}(g)$ is equivalent to maximizing $g^{2}$, or simply $g$ since $g > 0$. The objective function is strictly monotonically increasing with respect to $g$.\n\nLet us examine the constraint function, $C(g) = \\mathbb{P}(|g\\,x[n]|>A) = \\mathbb{P}(|x[n]|>A/g)$. As the gain $g$ increases, the threshold $A/g$ decreases. For a continuous random variable $x[n]$ with zero mean, the probability of its magnitude exceeding a positive threshold, $\\mathbb{P}(|x[n]|>c)$, is a monotonically decreasing function of that threshold $c$. Consequently, as $g$ increases, $A/g$ decreases, which in turn causes $C(g)$ to increase. The constraint function $C(g)$ is therefore a monotonically increasing function of $g$.\n\nWe are tasked with maximizing a monotonically increasing objective function subject to an upper-bound constraint defined by a monotonically increasing function. The optimal solution must therefore lie on the boundary of the feasible region, where the inequality constraint is met with equality. Any value of $g$ smaller than this boundary value would yield a smaller SQNR, and any value larger would violate the constraint. Thus, the optimal gain $g^{\\star}$ must satisfy:\n$$\n\\mathbb{P}\\left(|g^{\\star}\\,x[n]|>A\\right) = \\varepsilon.\n$$\nThe input signal is specified as a zero-mean Gaussian process, $x[n]\\sim\\mathcal{N}(0,\\sigma_{x}^{2})$. To analyze the probability, we standardize the random variable $x[n]$ by defining $Z = x[n]/\\sigma_{x}$, where $Z \\sim \\mathcal{N}(0,1)$ is a standard normal variable. The constraint equation becomes:\n$$\n\\mathbb{P}\\left(\\left|\\frac{x[n]}{\\sigma_{x}}\\right| > \\frac{A}{g^{\\star}\\sigma_{x}}\\right) = \\varepsilon \\implies \\mathbb{P}\\left(|Z| > \\frac{A}{g^{\\star}\\sigma_{x}}\\right) = \\varepsilon.\n$$\nFor a standard normal distribution, which is symmetric about zero, the probability in the two tails is given by $\\mathbb{P}(|Z|>z) = \\mathbb{P}(Z>z) + \\mathbb{P}(Z<-z) = 2\\mathbb{P}(Z>z)$ for any $z>0$. Using the provided Gaussian tail probability function, $Q(z) = \\mathbb{P}(Z>z)$, the equation becomes:\n$$\n2Q\\left(\\frac{A}{g^{\\star}\\sigma_{x}}\\right) = \\varepsilon.\n$$\nSolving for the $Q$-function term, we have:\n$$\nQ\\left(\\frac{A}{g^{\\star}\\sigma_{x}}\\right) = \\frac{\\varepsilon}{2}.\n$$\nTo find the argument of the $Q$-function, we apply its functional inverse, $Q^{-1}(\\cdot)$:\n$$\n\\frac{A}{g^{\\star}\\sigma_{x}} = Q^{-1}\\left(\\frac{\\varepsilon}{2}\\right).\n$$\nFinally, we solve for the optimal pre-gain $g^{\\star}$:\n$$\ng^{\\star} = \\frac{A}{\\sigma_{x}Q^{-1}\\left(\\frac{\\varepsilon}{2}\\right)}.\n$$\nThis expression provides the optimal gain in terms of the given parameters $A$, $\\sigma_{x}$, and $\\varepsilon$, completing the derivation.", "answer": "$$\\boxed{\\frac{A}{\\sigma_{x} Q^{-1}\\left(\\frac{\\varepsilon}{2}\\right)}}$$", "id": "2898405"}, {"introduction": "Ideal models provide essential intuition, but practical hardware introduces non-idealities that must be understood. This final practice explores the impact of a systematic offset in a quantizer's decision thresholds, a common type of hardware imperfection. By deriving the mean and variance of the resulting quantization error, you will gain insight into how such offsets introduce a deterministic bias, a crucial consideration in high-precision measurement and DC-sensitive signal chains. [@problem_id:2898425]", "problem": "A real-valued, discrete-time, stationary input process $x[n]$ with finite variance $\\sigma_{x}^{2}$ is processed by an infinite, uniform, mid-tread quantizer of step size $\\Delta \\gt 0$ and reconstruction levels $\\{k\\Delta : k \\in \\mathbb{Z}\\}$ that has all of its decision thresholds shifted by a fixed offset $\\theta \\in \\mathbb{R}$ relative to the origin. Concretely, the decision threshold between the reconstruction levels $k\\Delta$ and $(k+1)\\Delta$ is located at $(k+\\tfrac{1}{2})\\Delta + \\theta$ for every integer $k$, while the reconstruction levels remain at $\\{k\\Delta\\}$. Assume there is no overload.\n\nLet $q[n]$ denote the quantizer output and define the quantization error $e[n] \\triangleq q[n] - x[n]$.\n\nAdopt the high-resolution model (also known as Bennett’s approximation): for a uniform mid-tread quantizer with zero offset (i.e., thresholds at $(k+\\tfrac{1}{2})\\Delta$) applied to a sufficiently smooth stationary input with no overload, the resulting quantization error $e_{0}[n]$ is approximately independent of the input and approximately uniformly distributed on the interval $[-\\Delta/2,\\,+\\Delta/2]$.\n\nUsing only the foregoing definitions and assumptions, derive closed-form expressions for the following quantities as functions of $\\Delta$, $\\theta$, and $\\sigma_{x}^{2}$:\n- the mean $\\mathbb{E}\\{e[n]\\}$,\n- the variance $\\operatorname{Var}\\{e[n]\\}$,\n- and the signal-to-quantization-noise ratio (SQNR), defined here in linear scale as $\\mathrm{SQNR} \\triangleq \\sigma_{x}^{2} / \\operatorname{Var}\\{e[n]\\}$.\n\nProvide your final answer as a single row vector containing these three expressions in the order listed above. No numerical evaluation or rounding is required.", "solution": "The problem is found to be valid as it is scientifically grounded, well-posed, and objective. It presents a standard problem in signal processing theory with a specific variation (threshold offset) that is solvable using the provided assumptions.\n\nLet the quantizer function with offset $\\theta$ be denoted by $Q_{\\theta}(x)$. The input $x$ is mapped to the reconstruction level $k\\Delta$ if it falls into the quantization interval corresponding to that level. The decision thresholds are given at $(k+\\frac{1}{2})\\Delta + \\theta$. Therefore, the quantizer mapping is defined as:\n$$Q_{\\theta}(x) = k\\Delta \\quad \\text{for} \\quad (k-\\frac{1}{2})\\Delta + \\theta < x \\le (k+\\frac{1}{2})\\Delta + \\theta$$\nwhere $k$ is any integer from $\\mathbb{Z}$.\n\nLet us compare this to a standard uniform mid-tread quantizer with zero offset, denoted by $Q_{0}(y)$. Its mapping is:\n$$Q_{0}(y) = k\\Delta \\quad \\text{for} \\quad (k-\\frac{1}{2})\\Delta < y \\le (k+\\frac{1}{2})\\Delta$$\nBy examining the inequalities, we can establish a relationship between the two quantizers. The condition for $Q_{\\theta}(x)$ can be rewritten by subtracting $\\theta$ from all parts of the inequality:\n$$(k-\\frac{1}{2})\\Delta < x - \\theta \\le (k+\\frac{1}{2})\\Delta$$\nThis is precisely the condition for applying the zero-offset quantizer $Q_{0}$ to the input $y = x - \\theta$. This leads to the fundamental identity:\n$$Q_{\\theta}(x) = Q_{0}(x - \\theta)$$\n\nThe quantization error for the given system is defined as $e[n] \\triangleq q[n] - x[n]$, where $q[n] = Q_{\\theta}(x[n])$. Using the derived identity, we can express the error as:\n$$e[n] = Q_{0}(x[n] - \\theta) - x[n]$$\nTo relate this to the error of the standard quantizer, we add and subtract $\\theta$:\n$$e[n] = \\left( Q_{0}(x[n] - \\theta) - (x[n] - \\theta) \\right) - \\theta$$\nLet us define $e'[n]$ as the quantization error that would result from applying the standard zero-offset quantizer $Q_{0}$ to a shifted input signal $x[n] - \\theta$:\n$$e'[n] \\triangleq Q_{0}(x[n] - \\theta) - (x[n] - \\theta)$$\nThis allows us to write the error of the offset quantizer in a simple form:\n$$e[n] = e'[n] - \\theta$$\nThe problem states that under the high-resolution approximation, the error $e_{0}[n]$ from a zero-offset quantizer is approximately uniformly distributed on the interval $[-\\frac{\\Delta}{2}, +\\frac{\\Delta}{2}]$. The input to our standard quantizer component is $x[n] - \\theta$. Since $x[n]$ is a stationary process, the shifted process $x[n] - \\theta$ is also stationary and satisfies the conditions for the approximation (e.g., smoothness, no overload). Therefore, the error term $e'[n]$ follows this model. The random variable representing $e'[n]$ is uniformly distributed on $[-\\frac{\\Delta}{2}, +\\frac{\\Delta}{2}]$.\n\nThe probability density function (PDF) of $e'[n]$, denoted $f_{e'}(u)$, is:\n$$f_{e'}(u) = \\begin{cases} \\frac{1}{\\Delta} & \\text{if } u \\in [-\\frac{\\Delta}{2}, +\\frac{\\Delta}{2}] \\\\ 0 & \\text{otherwise} \\end{cases}$$\nThe error of our system, $e[n]$, is simply a shifted version of $e'[n]$. Thus, $e[n]$ is uniformly distributed on the interval $[-\\frac{\\Delta}{2} - \\theta, +\\frac{\\Delta}{2} - \\theta]$.\n\nWe can now compute the required statistical properties.\n\n1.  Mean of the quantization error, $\\mathbb{E}\\{e[n]\\}$:\n    Using the linearity of the expectation operator and the relation $e[n] = e'[n] - \\theta$:\n    $$\\mathbb{E}\\{e[n]\\} = \\mathbb{E}\\{e'[n] - \\theta\\} = \\mathbb{E}\\{e'[n]\\} - \\mathbb{E}\\{\\theta\\}$$\n    The mean of $e'[n]$, which is uniformly distributed on a symmetric interval about zero, is $\\mathbb{E}\\{e'[n]\\} = 0$. The expected value of the constant $\\theta$ is $\\theta$.\n    Therefore, the mean of the error is:\n    $$\\mathbb{E}\\{e[n]\\} = 0 - \\theta = -\\theta$$\n\n2.  Variance of the quantization error, $\\operatorname{Var}\\{e[n]\\}$:\n    The variance of a random variable is not affected by adding a constant.\n    $$\\operatorname{Var}\\{e[n]\\} = \\operatorname{Var}\\{e'[n] - \\theta\\} = \\operatorname{Var}\\{e'[n]\\}$$\n    The variance of a random variable uniformly distributed on an interval $[a, b]$ is given by the formula $\\frac{(b-a)^2}{12}$. For $e'[n]$, the interval is $[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}]$, so the length is $b-a = \\frac{\\Delta}{2} - (-\\frac{\\Delta}{2}) = \\Delta$.\n    Thus, the variance of the error is:\n    $$\\operatorname{Var}\\{e[n]\\} = \\frac{\\Delta^2}{12}$$\n    Notably, the variance of the error is independent of the offset $\\theta$.\n\n3.  Signal-to-Quantization-Noise Ratio (SQNR):\n    The problem provides an explicit definition for the SQNR in linear scale:\n    $$\\mathrm{SQNR} \\triangleq \\frac{\\sigma_{x}^{2}}{\\operatorname{Var}\\{e[n]\\}}$$\n    where $\\sigma_{x}^{2}$ is the variance of the input signal, and $\\operatorname{Var}\\{e[n]\\}$ is the variance of the quantization error, often termed the quantization noise power. Substituting the expressions for the signal power and the noise power:\n    $$\\mathrm{SQNR} = \\frac{\\sigma_{x}^{2}}{\\frac{\\Delta^2}{12}} = \\frac{12\\sigma_{x}^{2}}{\\Delta^2}$$\n    The SQNR, as defined, is also independent of the offset $\\theta$.\n\nThe three derived quantities are $\\mathbb{E}\\{e[n]\\} = -\\theta$, $\\operatorname{Var}\\{e[n]\\} = \\frac{\\Delta^2}{12}$, and $\\mathrm{SQNR} = \\frac{12\\sigma_{x}^{2}}{\\Delta^2}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\theta & \\frac{\\Delta^2}{12} & \\frac{12\\sigma_{x}^{2}}{\\Delta^2}\n\\end{pmatrix}\n}\n$$", "id": "2898425"}]}