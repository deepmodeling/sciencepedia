## Introduction
In the landscape of modern control theory, there is a growing need for strategies that can manage complex, [multivariable systems](@entry_id:169616) while adhering to strict operational constraints. Classical controllers often fall short in this regard, struggling to balance performance with the physical and safety limits inherent in real-world applications. Receding Horizon Control (RHC), also known as Model Predictive Control (MPC), emerges as a powerful solution to this challenge. It is a forward-looking control methodology that leverages a system model and [computational optimization](@entry_id:636888) to determine the best course of action at every moment. This article provides a foundational understanding of RHC, designed to take you from core concepts to practical application. We will begin by deconstructing its fundamental *Principles and Mechanisms*, exploring how predictive optimization and the [receding horizon](@entry_id:181425) strategy create a robust feedback system. Next, we will survey its broad impact across various fields in *Applications and Interdisciplinary Connections*, from industrial [process control](@entry_id:271184) to cutting-edge biomedical devices. Finally, you will have the opportunity to apply these concepts through a series of *Hands-On Practices* that reinforce the key design and tuning considerations of RHC.

## Principles and Mechanisms

Receding Horizon Control (RHC), also widely known as Model Predictive Control (MPC), is a modern control strategy that leverages computational power to handle complex systems, particularly those with multiple inputs and outputs and operational constraints. Unlike classical controllers that rely on a fixed, pre-computed feedback law, RHC solves an optimization problem at each time step to determine the optimal control action. This chapter elucidates the fundamental principles and mechanisms that govern the operation of RHC.

### The Core Principle: Predictive Optimization

At its heart, RHC is a control methodology based on the repeated optimization of a system's predicted future behavior. The controller's actions are not based on a static mapping from error to output, but on a forward-looking plan that is continuously updated. This process requires three essential components: a predictive model, a finite [prediction horizon](@entry_id:261473), and a cost function.

#### The Predictive Model

The ability to predict how a system will respond to future control inputs is the bedrock of RHC. To design an RHC controller, one must first possess a **dynamic model** of the process. This model, which can be linear or nonlinear, mathematically describes the evolution of the system's state over time.

For instance, consider the challenge of managing the Heating, Ventilation, and Air Conditioning (HVAC) system in a large office building. The goal is to maintain a comfortable indoor temperature while minimizing energy use. The building's thermal dynamics can be captured by a model of the form:

$T_{k+1} = f(T_k, u_k, d_k)$

where $T_k$ is the indoor temperature at time $k$, $u_k$ is the power supplied to the HVAC system (the control input), and $d_k$ represents external disturbances like the outside temperature or heat generated by occupants. This predictive model is indispensable because it allows the controller to simulate, or "look ahead" at, how different sequences of future power settings will influence the indoor temperature over a defined time window. By simulating these potential futures, an [optimization algorithm](@entry_id:142787) can then search for the specific sequence of control actions that minimizes a cost (e.g., total energy consumption) while ensuring predicted temperatures remain within a specified comfort range. Without the model, the controller would be blind to the consequences of its actions beyond the immediate present, making such predictive optimization impossible. [@problem_id:1603985]

#### Decision Variables and the Optimization Problem

Given a model and a starting state, the RHC controller formulates an optimization problem. The variables that the optimizer is free to choose are known as **decision variables**. In the context of RHC, the decision variables are the sequence of future control inputs over the [prediction horizon](@entry_id:261473).

Let us consider a general discrete-time Linear Time-Invariant (LTI) system described by:

$x(k+1) = Ax(k) + Bu(k)$

where $x(k)$ is the state vector and $u(k)$ is the control input vector at time step $k$. At the current time $k$, the state $x(k)$ is measured or estimated; it is a known parameter for the optimization problem. The RHC controller plans its actions over a **[prediction horizon](@entry_id:261473)** of $N_p$ steps. The set of decision variables is the sequence of future control inputs:

$U_k = \{u(k|k), u(k+1|k), \dots, u(k+N_p-1|k)\}$

Here, the notation $u(k+i|k)$ denotes the planned control input for the future time step $k+i$, as computed at the current time step $k$. The predicted future states, $\{x(k+1|k), \dots, x(k+N_p|k)\}$, are not independent decision variables. Instead, they are direct consequences of the initial state $x(k)$ and the chosen input sequence $U_k$, as determined by the system model. For example, the state at the next step is predicted as $x(k+1|k) = Ax(k) + Bu(k|k)$, and subsequent states are found by recursively applying the model. Therefore, the optimization algorithm's task is to find the optimal input sequence $U_k^*$ that minimizes a predefined **[cost function](@entry_id:138681)**, which typically penalizes deviations of the predicted states from a desired trajectory and the magnitude of the control inputs. [@problem_id:1603941]

### The Receding Horizon and Feedback Mechanism

The name "Receding Horizon Control" derives from the unique way the controller executes its plan and updates its planning window over time. This process is the source of RHC's inherent feedback nature. The control loop consists of three key steps performed at every control interval:

1.  **Optimize:** At the current time $k$, measure the system's state $x_k$. Then, solve a finite-horizon optimal control problem to generate an entire sequence of optimal future inputs, $U_k^* = \{u_{k|k}^*, u_{k+1|k}^*, \dots, u_{k+N_p-1|k}^*\}$.

2.  **Apply:** Implement *only the first* element of the optimal sequence. The control action applied to the plant at time $k$ is $u_k = u_{k|k}^*$.

3.  **Recede:** Discard the remainder of the optimal sequence $\{u_{k+1|k}^*, \dots, u_{k+N_p-1|k}^*\}$. At the next time step, $k+1$, the entire process is repeated. The controller measures the new state $x_{k+1}$, and the [prediction horizon](@entry_id:261473) "recedes," or slides forward, by one time step. The new optimization problem is solved over the interval from $k+1$ to $k+1+N_p$. [@problem_id:1603993] [@problem_id:1603955]

This cycle of re-measurement and re-optimization at every step creates a powerful closed-loop [feedback system](@entry_id:262081). By constantly updating its plan based on the most recent state measurement, the controller can automatically compensate for unforeseen disturbances and mismatches between the predictive model and the real system.

To illustrate this, consider a robotic joint whose angular error $x[k]$ is governed by $x[k+1] = x[k] + \alpha u[k]$. Suppose at time $k=0$, the controller calculates an optimal two-step plan $\{u[0]^*, u[1]^*\}$ based on the initial error $x[0]$. It applies the first action, $u[0]^*$. Now, imagine an unexpected mechanical shock hits the joint before the next measurement, adding a disturbance $d$ to the state. At time $k=1$, the controller measures the new state $x[1]$, which is now different from what the controller had predicted. Because the optimization process is repeated at $k=1$, the controller solves a new problem starting from this actual, disturbed state. The resulting control action, $u[1]^*$, is calculated to correct for the new reality, not the one that was anticipated at $k=0$. This inherent re-planning mechanism allows RHC to robustly reject disturbances without needing an explicit disturbance model, showcasing its intrinsic feedback nature. [@problem_id:1603951]

### Key Design Elements and Their Trade-offs

The performance of an RHC controller is critically dependent on several design choices, most notably the length of the [prediction horizon](@entry_id:261473) and the structure of the [cost function](@entry_id:138681).

#### Prediction Horizon Length

The choice of the [prediction horizon](@entry_id:261473) length, $N_p$, involves a fundamental trade-off between performance and computational complexity. A longer horizon allows the controller to be more proactive and far-sighted, while a shorter horizon is computationally less demanding but can lead to myopic and suboptimal behavior.

Consider an inventory management system where $x_k$ is the inventory level, $u_k$ is the quantity ordered, and $d_k$ is the customer demand. The goal is to keep the inventory close to a target level $x_{ref}$. Suppose the system faces a small demand today ($d_0$) but a very large demand tomorrow ($d_1$). An RHC controller with a short horizon of $N_p=1$ will only see the small demand $d_0$. It will place a small order, bringing the inventory back towards the target but leaving the system unprepared for the large demand on the next day, which will cause a significant stock depletion. In contrast, a controller with a longer horizon of $N_p=2$ can see both demands. The optimization will anticipate the large future withdrawal $d_1$ and will proactively place a larger order at time $k=0$, even if the current demand is small. This foresight leads to a smoother inventory trajectory and avoids a large future deficit. The cost of this improved performance is the increased computational burden of solving a larger optimization problem at each step. [@problem_id:1603956]

#### Cost Function and Terminal Cost

For systems that are open-loop unstable, a finite [prediction horizon](@entry_id:261473) can be dangerously myopic. The controller might choose actions that look optimal within its limited window but lead the system towards instability just beyond the horizon. To mitigate this, a **terminal cost** is often added to the cost function. The [cost function](@entry_id:138681) then takes the general form:

$J_k = \left( \sum_{i=0}^{N_p-1} \ell(x_{k+i|k}, u_{k+i|k}) \right) + J_f(x_{k+N_p|k})$

Here, $\ell$ is the stage cost (e.g., a [quadratic penalty](@entry_id:637777) on state error and control effort), and $J_f$ is the terminal cost, which penalizes the final predicted state of the horizon, $x_{k+N_p|k}$.

A powerful choice for the terminal cost is one that approximates the cost-to-go of an infinite-[horizon problem](@entry_id:161031) starting from the terminal state. For linear systems with a quadratic cost, this is achieved by setting $J_f(x) = x^T P x$, where the matrix $P$ is the solution to the **discrete-time Algebraic Riccati Equation (DARE)** associated with the infinite-horizon version of the problem. By including this term, the controller is incentivized not just to perform well over the $N_p$ steps, but also to steer the state to a region from which future costs will be low. This forces the controller to account for the long-term stability of the system, effectively providing it with a "summary" of the consequences beyond its finite horizon. For an unstable system, an RHC with an appropriate terminal cost will apply more aggressive, stabilizing control actions from the very first step compared to a controller with no terminal cost, which might dangerously underestimate the long-term consequences of its myopic plan. [@problem_id:1603979]

### Advantages and Comparisons with Classical Control

RHC offers several significant advantages that have made it the controller of choice for a wide range of industrial applications. Its relationship to classical optimal control also provides deep theoretical insights.

#### Systematic Handling of Constraints

Perhaps the most celebrated advantage of RHC is its ability to handle constraints on states and inputs in a systematic and explicit manner. Physical actuators have saturation limits (e.g., maximum valve opening, maximum motor torque), and many processes have [state constraints](@entry_id:271616) for reasons of safety or quality (e.g., temperature limits, pressure limits). In RHC, these constraints are simply added as inequalities to the optimization problem solved at each time step. The optimization algorithm will then find the best possible control sequence that respects all specified limits. This is a profound advantage over classical methods like PID or LQR, where handling constraints is often difficult and typically requires ad-hoc modifications like [anti-windup schemes](@entry_id:267727).

#### Coordinated Control of Multivariable Systems

RHC is exceptionally well-suited for controlling **multi-input, multi-output (MIMO)** systems, especially those with significant cross-coupling between inputs and outputs. Consider a [hydroponics](@entry_id:141599) chamber where one must control both nutrient concentration ($y_1$) and air temperature ($y_2$) using a nutrient pump ($u_1$) and a heater ($u_2$). If an increase in heater power ($u_2$) also inadvertently affects the nutrient concentration ($y_1$), the system has [strong coupling](@entry_id:136791). A decentralized approach using two separate single-input, single-output (SISO) controllers would struggle, as the action of one controller would act as an unmeasured disturbance to the other, leading to poor performance or oscillations.

A multivariable RHC controller, however, uses a model that explicitly includes these cross-coupling terms. When it computes the optimal inputs $\{u_1, u_2\}$, it does so simultaneously, anticipating that a change in $u_2$ will affect both $y_1$ and $y_2$. It can therefore proactively adjust $u_1$ in the same control step to compensate for the known effect of $u_2$ on $y_1$. This coordinated, anticipatory action is the key to high performance in strongly coupled [multivariable systems](@entry_id:169616). [@problem_id:1583601]

#### Relationship to LQR and Computational Cost

RHC has a deep connection to the classical Linear Quadratic Regulator (LQR). For an unconstrained LTI system with a quadratic [cost function](@entry_id:138681), if the RHC [prediction horizon](@entry_id:261473) is extended to infinity ($N_p \to \infty$), the resulting control law becomes identical to the time-invariant LQR controller. That is, the solution to the [online optimization](@entry_id:636729) at every step yields the static state-feedback law $u_k = -Kx_k$, where $K$ is the constant LQR gain matrix derived from the offline solution of the Riccati equation. This establishes RHC as a powerful generalization of LQRâ€”one that converges to the well-known [optimal solution](@entry_id:171456) in the unconstrained case but extends its power to handle constraints. [@problem_id:1603973]

This power comes at a price: **computational complexity**. An LQR controller's online implementation is trivial, requiring only a single [matrix-vector multiplication](@entry_id:140544) to compute the control action. In contrast, an RHC controller must solve a constrained optimization problem (typically a [quadratic program](@entry_id:164217), or QP) at every single time step. This is a significantly more demanding task, which can limit the applicability of RHC to systems with very fast dynamics where the time available to compute the solution is short. [@problem_id:1603977]

### Practical Implementation: The Role of State Estimation

The entire RHC framework is predicated on the availability of the current [state vector](@entry_id:154607) $x_k$ at the beginning of each time step. In many real-world applications, however, it is not possible or practical to measure all state variables directly. Often, only a subset of the states or a linear combination of them can be measured, represented by an output vector $y_k = Cx_k$.

To bridge this gap, RHC is almost always implemented in conjunction with a **[state estimator](@entry_id:272846)**, also known as an observer. The role of the [state estimator](@entry_id:272846) (such as a Luenberger observer or a Kalman filter) is to use the known system model, the past control inputs $u_k$, and the current measurements $y_k$ to compute an estimate of the full [state vector](@entry_id:154607), denoted $\hat{x}_k$.

The primary role of this state estimate $\hat{x}_k$ within the RHC framework is to serve as the **initial state for the prediction model** in the optimization problem. At time $k$, the controller takes the estimate $\hat{x}_k$ and treats it as the true starting point for its forward prediction. The optimization then proceeds as previously described, generating an [optimal control](@entry_id:138479) sequence based on the predicted evolution from this estimated state. This combination of an estimator and an RHC controller is a practical application of the separation principle, allowing for the powerful methods of RHC to be applied even when the full state is not available for direct measurement. [@problem_id:1603989]