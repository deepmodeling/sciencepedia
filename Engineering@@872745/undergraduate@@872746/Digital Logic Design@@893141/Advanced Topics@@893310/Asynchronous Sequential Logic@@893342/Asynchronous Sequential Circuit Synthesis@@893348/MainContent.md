## Introduction
Asynchronous [sequential circuits](@entry_id:174704) are a fundamental component of digital systems, operating not by the rhythm of a central clock but by the natural flow of events. This event-driven nature makes them essential for interfacing with the unpredictable real world, yet it also introduces significant design challenges not present in their synchronous counterparts. The absence of a global clock means designers must meticulously manage complex timing dependencies, races, and hazards to ensure correct and reliable operation. This article provides a comprehensive guide to mastering the synthesis of these circuits. In **Principles and Mechanisms**, you will learn the systematic design process, from creating flow tables to achieving race-free state assignments and eliminating [logic hazards](@entry_id:174770). Following this, **Applications and Interdisciplinary Connections** explores the practical utility of these circuits in areas like user interfaces, resource arbitration, and even [biological modeling](@entry_id:268911). Finally, **Hands-On Practices** offers targeted exercises to solidify your understanding of these core concepts. We begin our journey by delving into the foundational principles that transform a behavioral description into a working asynchronous circuit.

## Principles and Mechanisms

The synthesis of [asynchronous sequential circuits](@entry_id:170735) is a systematic process that transforms a high-level behavioral description into a physical gate-level implementation. Unlike their synchronous counterparts, which are governed by the discrete ticks of a global clock, [asynchronous circuits](@entry_id:169162) are event-driven. Their state transitions are initiated directly by changes in their inputs. This fundamental difference necessitates a unique design methodology focused on managing the complex timing relationships that arise from [signal propagation](@entry_id:165148) delays through logic gates and feedback paths. This chapter elucidates the core principles and mechanisms of this synthesis process, from abstract specification to a robust and reliable physical circuit.

### From Specification to Flow Table

The design process begins with a precise, unambiguous description of the desired circuit behavior. This specification is then translated into a formal structure known as a **[primitive flow table](@entry_id:168105)**. This table is the foundational representation of an [asynchronous state machine](@entry_id:165678), capturing all required states, transitions, and outputs.

A [primitive flow table](@entry_id:168105) has one row for each state of the machine and one column for each possible combination of input values. Each cell in the table specifies the **next state** and the corresponding **output** for a given present state and input combination. A key concept is the **stable state**, which represents a condition where the circuit is at rest, waiting for an input to change. In the [flow table](@entry_id:175022), a stable state is indicated when the next-state entry is the same as the present-state row. All other entries represent **unstable states**, which are transient conditions that direct the circuit to a new stable state.

A fundamental assumption in this initial design phase is that the circuit operates in **fundamental mode**. This mode assumes that only one input variable is allowed to change at any given time, and the circuit must be allowed to reach a stable state before the next input change occurs. This simplifies the design process by excluding the complexities of simultaneous input changes, which we will address later.

To illustrate the construction of a [primitive flow table](@entry_id:168105), consider the design of a safety interlock system for an industrial press [@problem_id:1911362]. The system has two input buttons, $x_1$ and $x_2$, and one output $Z$ that enables the press ($Z=1$). The press should only be enabled if $x_1$ is pressed first, followed by $x_2$. Releasing any button must immediately disable the press.

We can derive the [flow table](@entry_id:175022) by tracing the possible input sequences:

1.  **Initial State (A):** The system starts with both buttons released ($x_1x_2=00$). This is a stable state with the press disabled ($Z=0$). We denote this as state A, with the entry for column $00$ being $(\text{\textcircled{A}}, 0)$.

2.  **First Input Change:** From state A, if $x_1$ is pressed ($x_1x_2$ becomes $10$), the system moves to a new state, B, which remembers that the first step of the correct sequence has occurred. This is a stable state with $Z=0$. So, from state A, input $10$ leads to state B. Symmetrically, if $x_2$ is pressed first ($x_1x_2$ becomes $01$), the system moves to a different state, C, representing an incorrect sequence, also with $Z=0$.

3.  **Second Input Change:**
    *   If the system is in state B ($x_1x_2=10$) and $x_2$ is pressed, the input becomes $11$. The correct sequence ($x_1$ then $x_2$) is complete, so the system must transition to a new stable state, D, where the output is $Z=1$.
    *   If the system is in state C ($x_1x_2=01$) and $x_1$ is pressed, the input becomes $11$. However, this is the incorrect sequence ($x_2$ then $x_1$). The system must remember this and transition to a stable state, E, where the output remains $Z=0$. This demonstrates a crucial feature: the circuit must have two distinct stable states for the input combination $x_1x_2=11$, one for the correct sequence (output 1) and one for the incorrect sequence (output 0).

4.  **Releasing Buttons:** From any state, if a button is released, the output $Z$ must become $0$. For example, if in state D ($x_1x_2=11$, $Z=1$), releasing $x_2$ changes the input to $10$. The circuit should transition to the stable state corresponding to this input, which is state B, with $Z=0$. Similarly, releasing $x_1$ would cause a transition to state C with $Z=0$.

By systematically considering all [allowed transitions](@entry_id:160018), we construct the complete [primitive flow table](@entry_id:168105). Unspecified entries, marked with a dash, correspond to input changes where more than one variable changes simultaneously, which are disallowed under the [fundamental mode](@entry_id:165201) assumption.

### State Minimization: Creating an Efficient Representation

The [primitive flow table](@entry_id:168105) often contains redundant states. A **[state minimization](@entry_id:273227)** procedure is employed to merge equivalent states, resulting in a **merged [flow table](@entry_id:175022)** with the minimum number of states required to implement the specified behavior. This reduction simplifies the final circuit, typically leading to less hardware.

Two states, $S_i$ and $S_j$, are considered **equivalent** if they satisfy two conditions for every possible input combination:
1.  Their specified outputs are identical.
2.  Their next states are equivalent.

This [recursive definition](@entry_id:265514) is the basis for the merging process. Don't-care entries for outputs or next states are compatible with any value, providing flexibility in the merging process.

The procedure begins by partitioning all states into groups based on their specified outputs. States with conflicting outputs for any input combination cannot be equivalent. For example, in the safety interlock problem, states D ($Z=1$) and E ($Z=0$) for input $11$ can never be merged. Then, within each group, we iteratively check pairs of states for next-[state equivalence](@entry_id:261329). If merging states $(S_i, S_j)$ requires that another pair of states $(S_k, S_l)$ also be equivalent, this forms an **implication**. By resolving these implications, we can identify sets of mutually compatible states, known as **compatibles**. The goal is to find a minimal set of these compatibles that covers all original states and is **closed**, meaning that for any compatible in the set, all its next-state transitions lead to states that are contained within a single compatible in the set.

As an example [@problem_id:1911376], consider a [primitive flow table](@entry_id:168105) with six states {a, b, c, d, e, f}. States {c, f} both have an output of 1, while states {a, b, d, e} have an output of 0. This forms our initial partition. An analysis of the next-state transitions reveals that states 'b' and 'e' are compatible, and states 'c' and 'f' are compatible only if 'b' and 'e' are. Further analysis shows that all states in the set {a, b, d, e} are mutually compatible. This leads to two **maximal compatibles**: {a, b, d, e} and {c, f}. This two-set collection covers all original states and satisfies the [closure property](@entry_id:136899). Therefore, the six-state primitive table can be reduced to a two-state merged [flow table](@entry_id:175022).

### The Challenge of Timing: Race Conditions

After obtaining a minimized [flow table](@entry_id:175022), the next step is to assign binary codes to the abstract states. This is where the physical reality of gate delays introduces a major challenge: **race conditions**. In a physical implementation, state is stored using feedback loops. A change in state is achieved by changing the values of one or more binary [state variables](@entry_id:138790). A race condition occurs when a state transition requires two or more [state variables](@entry_id:138790) to change their values simultaneously. Due to minute, unavoidable differences in the propagation delays of the [logic gates](@entry_id:142135) in the feedback paths, the state variables will not change at exactly the same time. The order in which they change becomes unpredictable.

This phenomenon can be clearly observed in a basic SR latch built from cross-coupled NOR gates [@problem_id:1911320]. If both inputs S and R are initially 1, both outputs $Q$ and $\bar{Q}$ are forced to 0. If S and R then transition to 0 "simultaneously", each NOR gate sees its inputs change from having a '1' to having only '0's. Both gates will attempt to drive their output to 1. Which gate "wins" this race depends on which has a slightly shorter [propagation delay](@entry_id:170242). If the 'S' gate is faster, $\bar{Q}$ will go to 1 first, keeping $Q$ at 0. If the 'R' gate is faster, $Q$ will go to 1 first, keeping $\bar{Q}$ at 0. The final state is thus unpredictable.

Races are categorized based on their outcome:
*   **Non-critical Race:** All possible sequences of state variable changes ultimately lead to the intended final stable state. Non-critical races are generally acceptable.
*   **Critical Race:** Different sequences of state variable changes can lead to different final stable states. At least one of these final states is incorrect, causing the circuit to malfunction. Critical races must be eliminated.

To identify a [critical race](@entry_id:173597) from a transition table [@problem_id:1911351], one must analyze the intermediate states. Consider a transition from state `00` to `11`. The [state variables](@entry_id:138790) $y_1$ and $y_2$ must both change.
1.  If $y_1$ changes first, the circuit passes through the transient state `10`.
2.  If $y_2$ changes first, it passes through `01`.

We must then look up the behavior of the circuit from these intermediate states (`10` and `01`) for the *same input condition* that caused the original transition. If the next-state entry for state `10` directs the circuit to a different final stable state than the next-state entry for state `01`, the race is critical. For instance, if the circuit stabilizes at `10` in the first case and at `01` in the second, the behavior is unpredictable and erroneous.

### Achieving Stability: Race-Free State Assignment

The primary method for eliminating critical races is to devise a **race-free [state assignment](@entry_id:172668)**. The goal is to assign binary codes (e.g., $y_1y_0$) to the abstract states (e.g., A, B, C, D) in such a way that any transition between two states in the merged [flow table](@entry_id:175022) involves the change of only a single state variable. In other words, the binary codes for any two adjacent states must have a Hamming distance of 1.

A systematic way to achieve this is by using a **state adjacency diagram**. This is a graph where each node represents a state from the merged [flow table](@entry_id:175022). An edge is drawn between any two nodes if there is a direct transition between those two states in the table. The state [assignment problem](@entry_id:174209) then becomes a graph-embedding problem: we must assign binary codes to the nodes such that any two connected nodes have codes with a Hamming distance of 1. This is equivalent to mapping the nodes of the adjacency diagram onto the vertices of a Boolean hypercube of the appropriate dimension.

For a four-state circuit with states A, B, C, D, we need two [state variables](@entry_id:138790), $y_1$ and $y_2$. The vertices of the corresponding 2-cube are 00, 01, 11, 10. If the adjacency diagram requires the pairs (A,B), (B,C), (C,D), and (D,A) to be adjacent, we can map this cycle onto the 2-cube. For example, the assignment A=00, B=01, C=11, D=10 satisfies all adjacencies and is therefore race-free [@problem_id:1911377].

Sometimes, a perfect assignment where all transitions are single-bit changes is not possible with the minimum number of state variables. In these cases, one might need to add extra [state variables](@entry_id:138790) or carefully design the transitions involving multiple bit changes to ensure they are non-critical races. A two-bit change can be non-critical if both possible intermediate states lead to the same correct final destination [@problem_id:1911309]. For example, in a transition from state D=10 to state B=01, the intermediate states are $00$ and $11$. If, under the input condition causing the transition, both state $00$ and state $11$ are specified to transition to state B, then the race is non-critical and the assignment is valid.

### The Challenge of Logic: Hazards in Combinational Circuits

Even with a perfectly race-free [state assignment](@entry_id:172668), the circuit is not yet guaranteed to be reliable. The [combinational logic](@entry_id:170600) that computes the next-state variables and the outputs can itself produce transient, unwanted glitches known as **hazards**. These hazards arise from unequal propagation delays along different paths in the logic.

A **[static hazard](@entry_id:163586)** occurs when a single input variable changes, and the output is meant to remain at a constant value (0 or 1), but it momentarily glitches to the opposite value.
*   A **[static-1 hazard](@entry_id:261002)** is a $1 \rightarrow 0 \rightarrow 1$ glitch.
*   A **[static-0 hazard](@entry_id:172764)** is a $0 \rightarrow 1 \rightarrow 0$ glitch.

In [asynchronous circuits](@entry_id:169162), a [static-1 hazard](@entry_id:261002) in the [next-state logic](@entry_id:164866) can be catastrophic. If a next-state variable $Y_i$ is supposed to remain 1 during a transition but briefly glitches to 0, this can cause the feedback latch for that state variable to be incorrectly reset, throwing the entire machine into a wrong state.

Static-1 hazards in Sum-of-Products (SOP) logic occur when a single input change causes the active product term to change. For instance, consider the expression $Y = x'y' + xz$ [@problem_id:1911315]. If the input changes from $(x,y,z)=(0,0,1)$ to $(1,0,1)$, the output $Y$ should remain 1. Initially, the term $x'y'$ is 1. After the transition, the term $xz$ is 1. During the transition, as $x$ changes from 0 to 1, there might be a moment when neither term is asserted, causing $Y$ to glitch to 0.

The solution is to add redundant product terms to the SOP expression to ensure coverage during these transitions. The required term is the **consensus term**. According to the [consensus theorem](@entry_id:177696), for an expression of the form $AB + A'C$, the consensus term is $BC$. Adding this redundant term ($AB + A'C + BC$) ensures that when the transition between term $AB$ and $A'C$ occurs, the term $BC$ remains asserted, holding the output high. In our example $Y = x'y' + xz$, we can identify $A'=y'$, $C=x'$, $A=z$, $B=x$. This is incorrect. Let's re-analyze. The terms are $P_1 = x'y'$ and $P_2 = xz$. The variable that changes is $x$. Let the expression be of the form $x'A + xB$. Here, $A=y'$ and $B=z$. The consensus term is $AB = y'z$. Adding this to the expression gives the hazard-free form $Y = x'y' + xz + y'z$. Now, during the transition from $(0,0,1)$ to $(1,0,1)$, the new term $y'z$ remains 1 throughout, preventing the glitch. This procedure must be applied to all next-state expressions for all pairs of product terms that could lead to a hazard [@problem_id:1911350].

It is crucial to recognize that not all hazards can be fixed by adding logic. A **[function hazard](@entry_id:164428)** is a hazard inherent to the functional specification itself. This can occur during a multi-input change. If the output is 1 at the initial and final inputs, but the function is specified to be 0 for *all* possible intermediate inputs, then a glitch is unavoidable, regardless of the implementation. No amount of [redundant logic](@entry_id:163017) can prevent this, because doing so would require changing the function's definition [@problem_id:1911310]. Such transitions must typically be forbidden by the system's operational constraints.

### Metastability: The Unavoidable Reality of Asynchronous Timing

The final and most fundamental timing challenge is **[metastability](@entry_id:141485)**. This is an unavoidable phenomenon in any system with feedback that is subjected to [asynchronous inputs](@entry_id:163723). A bistable element, such as a latch, has two stable states (logic 0 and 1) and an unstable equilibrium point between them. If the inputs to the latch change in such a way that violates its internal timing requirements, the latch can enter this balanced, **metastable state**. In this state, the output is at an indeterminate voltage level, neither a valid 0 nor a valid 1, and may oscillate before eventually, and unpredictably, resolving to one of the stable states. The time it takes to resolve is unbounded.

Revisiting the SR latch, metastability is not just a theoretical concern for perfectly simultaneous input changes. A risk exists whenever the competing input signals arrive "too close" in time. Let's consider the transition from $S=R=1$ to $S=R=0$, and define $\Delta t = |t_S - t_R|$ as the time difference between the de-assertion of the S and R inputs. Let the [propagation delay](@entry_id:170242) of the NOR gates be $t_{pd}$ [@problem_id:1911371].

If $\Delta t > t_{pd}$, the first input change has enough time to propagate through a gate and stabilize the latch before the second input change arrives to contest it. The outcome is deterministic. However, if $\Delta t \lt t_{pd}$, the second input change occurs before the effect of the first change has had time to resolve. Both gates are thus set into a race, creating a high risk of driving the latch into a metastable state. The smaller the $\Delta t$, the higher the probability of a long resolution time. While designers cannot eliminate [metastability](@entry_id:141485), they can design **synchronizers** and other circuits that manage its effects and minimize the probability of system failure. Understanding these fundamental principles—from flow tables to race conditions and hazards—is the key to mastering the art of asynchronous design.