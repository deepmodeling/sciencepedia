{"hands_on_practices": [{"introduction": "Understanding stationarity begins with verifying its two core conditions: a constant mean and an autocorrelation function dependent only on time lag. This first exercise provides a practical scenario—the random startup time of a device—to test the most fundamental of these conditions. By calculating the ensemble average, or mean function, of the process, you will directly determine if the signal's average behavior is consistent over time, a crucial first step in classifying any random signal. [@problem_id:1755455]", "problem": "Consider a random signal model that could represent the startup of a device at a random time. The process, denoted by $X(t)$, is defined as:\n$$\nX(t) = A u(t-T)\n$$\nwhere $A$ is a positive real constant, $u(t)$ is the Heaviside unit step function, and $T$ is a continuous random variable representing a time delay. The probability density function (PDF) for the random variable $T$ is given by a one-sided exponential distribution:\n$$\np_T(\\tau) =\n\\begin{cases}\n\\lambda \\exp(-\\lambda \\tau) & \\text{for } \\tau \\geq 0 \\\\\n0 & \\text{for } \\tau < 0\n\\end{cases}\n$$\nwhere $\\lambda$ is a positive real constant.\n\nYour task is to find the ensemble average, or mean function, $\\mu_X(t) = E[X(t)]$, for this random process and determine if the process is wide-sense stationary (WSS). Select the option that correctly provides both the mean function and the conclusion about its stationarity.\n\nA. The mean is $\\mu_X(t) = A(1 - \\exp(-\\lambda t))u(t)$, and the process is not WSS.\n\nB. The mean is $\\mu_X(t) = A(1 - \\exp(-\\lambda t))u(t)$, and the process is WSS.\n\nC. The mean is $\\mu_X(t) = A \\exp(-\\lambda t)u(t)$, and the process is not WSS.\n\nD. The mean is $\\mu_X(t) = A \\lambda \\exp(-\\lambda t)u(t)$, and the process is not WSS.\n\nE. The mean is $\\mu_X(t) = A/2$, and the process is WSS.", "solution": "We are given the random process $X(t) = A\\,u(t-T)$, where $A$ is a positive constant, $u(\\cdot)$ is the Heaviside unit step function, and $T$ is an exponential random variable with PDF $p_{T}(\\tau) = \\lambda \\exp(-\\lambda \\tau)$ for $\\tau \\geq 0$ and $0$ otherwise. The ensemble average is defined by\n$$\n\\mu_{X}(t) = E[X(t)] = E[A\\,u(t-T)] = A\\,E[u(t-T)] = A \\int_{-\\infty}^{\\infty} u(t-\\tau)\\,p_{T}(\\tau)\\,d\\tau.\n$$\nSubstituting the support of $p_{T}(\\tau)$, this becomes\n$$\n\\mu_{X}(t) = A \\int_{0}^{\\infty} u(t-\\tau)\\,\\lambda \\exp(-\\lambda \\tau)\\,d\\tau.\n$$\nThe step $u(t-\\tau)$ equals $1$ when $\\tau \\leq t$ and $0$ otherwise. Therefore:\n- If $t < 0$, there is no $\\tau \\geq 0$ such that $\\tau \\leq t$, so the integral is zero and $\\mu_{X}(t) = 0$.\n- If $t \\geq 0$, the integral runs from $\\tau = 0$ to $\\tau = t$, yielding\n$$\n\\mu_{X}(t) = A \\int_{0}^{t} \\lambda \\exp(-\\lambda \\tau)\\,d\\tau = A\\left[ -\\exp(-\\lambda \\tau) \\right]_{0}^{t} = A\\left(1 - \\exp(-\\lambda t)\\right).\n$$\nCombining both cases,\n$$\n\\mu_{X}(t) = A\\left(1 - \\exp(-\\lambda t)\\right) u(t).\n$$\nTo assess wide-sense stationarity (WSS), recall that a WSS process must have a mean that is constant in time and an autocorrelation that depends only on time differences. Since $\\mu_{X}(t)$ explicitly depends on $t$ (it rises from $0$ and approaches $A$ as $t \\to \\infty$), the mean is not constant. Therefore, the process is not WSS.\n\nThe correct option is the one giving $\\mu_{X}(t) = A\\left(1 - \\exp(-\\lambda t)\\right)u(t)$ and stating that the process is not WSS.", "answer": "$$\\boxed{A}$$", "id": "1755455"}, {"introduction": "While some processes are clearly non-stationary, others may be stationary only under specific conditions. This practice moves beyond simple verification to a more design-oriented problem common in communication systems engineering. You will investigate a modulated signal and determine the precise relationship between its components that ensures wide-sense stationarity, focusing on the properties of the autocorrelation function. [@problem_id:1755509]", "problem": "A random process $X(t)$ is generated as a model for a specific type of modulated signal in a communication system. The process is defined as:\n$$\nX(t) = 3 A_I \\cos(\\omega_0 t) + 5 A_Q \\sin(\\omega_0 t)\n$$\nwhere $t$ represents time, and $\\omega_0$ is a positive constant angular frequency. The amplitudes $A_I$ and $A_Q$ are statistically independent random variables. Both variables have a mean of zero. The variance of $A_I$ is denoted by $\\sigma_I^2 = \\text{Var}(A_I)$, and the variance of $A_Q$ is denoted by $\\sigma_Q^2 = \\text{Var}(A_Q)$. Assume both variances are non-zero.\n\nFor a random process to be classified as wide-sense stationary (WSS), two conditions must be satisfied:\n1.  The mean function of the process, $\\mu_X(t) = E[X(t)]$, must be a constant for all time $t$.\n2.  The autocorrelation function of the process, $R_X(t_1, t_2) = E[X(t_1)X(t_2)]$, must depend only on the time lag $\\tau = t_1 - t_2$.\n\nDetermine the numerical value of the ratio $r = \\frac{\\sigma_I^2}{\\sigma_Q^2}$ that is required for the process $X(t)$ to be wide-sense stationary. Express your answer as a fraction.", "solution": "We are given the random process\n$$\nX(t) = 3 A_I \\cos(\\omega_0 t) + 5 A_Q \\sin(\\omega_0 t),\n$$\nwith $A_I$ and $A_Q$ independent, zero-mean random variables with variances $\\sigma_I^2 = \\text{Var}(A_I)$ and $\\sigma_Q^2 = \\text{Var}(A_Q)$, both non-zero. For wide-sense stationarity (WSS), we require:\n1) The mean $\\mu_X(t) = E[X(t)]$ is constant in $t$.\n2) The autocorrelation $R_X(t_1, t_2) = E[X(t_1) X(t_2)]$ depends only on $\\tau = t_1 - t_2$.\n\nFirst, compute the mean. Using linearity of expectation and the given zero means,\n$$\n\\mu_X(t) = E[3 A_I \\cos(\\omega_0 t) + 5 A_Q \\sin(\\omega_0 t)]\n= 3 \\cos(\\omega_0 t) E[A_I] + 5 \\sin(\\omega_0 t) E[A_Q] = 0,\n$$\nwhich is a constant (zero) for all $t$. Thus, condition 1 is satisfied.\n\nNext, compute the autocorrelation. Expand the product and take expectation:\n$$\n\\begin{aligned}\nR_X(t_1, t_2) &= E[X(t_1) X(t_2)] \\\\\n&= E\\left[\\left(3 A_I \\cos(\\omega_0 t_1) + 5 A_Q \\sin(\\omega_0 t_1)\\right)\n\\left(3 A_I \\cos(\\omega_0 t_2) + 5 A_Q \\sin(\\omega_0 t_2)\\right)\\right] \\\\\n&= 9 E[A_I^2] \\cos(\\omega_0 t_1) \\cos(\\omega_0 t_2)\n+ 15 E[A_I A_Q] \\left(\\cos(\\omega_0 t_1) \\sin(\\omega_0 t_2) + \\sin(\\omega_0 t_1) \\cos(\\omega_0 t_2)\\right) \\\\\n&\\quad + 25 E[A_Q^2] \\sin(\\omega_0 t_1) \\sin(\\omega_0 t_2).\n\\end{aligned}\n$$\nIndependence and zero means imply $E[A_I A_Q] = E[A_I] E[A_Q] = 0$. Using $E[A_I^2] = \\sigma_I^2$ and $E[A_Q^2] = \\sigma_Q^2$, we obtain\n$$\nR_X(t_1, t_2) = 9 \\sigma_I^2 \\cos(\\omega_0 t_1) \\cos(\\omega_0 t_2)\n+ 25 \\sigma_Q^2 \\sin(\\omega_0 t_1) \\sin(\\omega_0 t_2).\n$$\nApply the product-to-sum identities\n$$\n\\cos a \\cos b = \\frac{1}{2}\\left[\\cos(a-b) + \\cos(a+b)\\right], \\quad\n\\sin a \\sin b = \\frac{1}{2}\\left[\\cos(a-b) - \\cos(a+b)\\right],\n$$\nwith $a = \\omega_0 t_1$ and $b = \\omega_0 t_2$. This yields\n$$\n\\begin{aligned}\nR_X(t_1, t_2)\n&= \\frac{9 \\sigma_I^2}{2}\\left[\\cos(\\omega_0(t_1-t_2)) + \\cos(\\omega_0(t_1+t_2))\\right]\n+ \\frac{25 \\sigma_Q^2}{2}\\left[\\cos(\\omega_0(t_1-t_2)) - \\cos(\\omega_0(t_1+t_2))\\right] \\\\\n&= \\frac{1}{2}\\left(9 \\sigma_I^2 + 25 \\sigma_Q^2\\right) \\cos(\\omega_0(t_1-t_2))\n+ \\frac{1}{2}\\left(9 \\sigma_I^2 - 25 \\sigma_Q^2\\right) \\cos(\\omega_0(t_1+t_2)).\n\\end{aligned}\n$$\nFor WSS, $R_X(t_1, t_2)$ must depend only on $\\tau = t_1 - t_2$, so the coefficient of $\\cos(\\omega_0(t_1+t_2))$ must be zero:\n$$\n9 \\sigma_I^2 - 25 \\sigma_Q^2 = 0 \\quad \\Longrightarrow \\quad \\frac{\\sigma_I^2}{\\sigma_Q^2} = \\frac{25}{9}.\n$$\nWith this ratio, the autocorrelation simplifies to\n$$\nR_X(t_1, t_2) = \\frac{1}{2}\\left(9 \\sigma_I^2 + 25 \\sigma_Q^2\\right) \\cos(\\omega_0(t_1-t_2)),\n$$\nwhich depends only on $\\tau$, and the mean is constant, so $X(t)$ is WSS.\n\nTherefore, the required ratio is $\\sigma_I^2/\\sigma_Q^2 = \\frac{25}{9}$.", "answer": "$$\\boxed{\\frac{25}{9}}$$", "id": "1755509"}, {"introduction": "A process can have statistical properties that are constant over time (stationarity), but this does not guarantee that a single, long observation can reveal those properties. This is the crucial distinction between stationarity and ergodicity. In this problem, we analyze a signal model with a random DC offset to explore the concept of mean-ergodicity, investigating when the time average of a single signal realization can reliably estimate the average of the entire ensemble of possible signals. [@problem_id:1755454]", "problem": "In the quality control process for a new type of digital sensor, the output signal is modeled as a discrete-time random process $S[n]$. Each sensor manufactured has a slight, unpredictable imperfection, resulting in a random but constant Direct Current (DC) offset unique to that sensor. This offset is represented by a random variable $A$. Superimposed on this offset is a noise signal, $X[n]$, which is inherent to the sensor's operation. Thus, the signal from a randomly selected sensor is described by the process $S[n] = A + X[n]$.\n\nThe random DC offset $A$ for any given sensor is constant over time. For the entire batch of sensors, its value is governed by the probability distribution $P(A = V_0) = 0.5$ and $P(A = -V_0) = 0.5$, where $V_0$ is a non-negative real constant representing the magnitude of the offset.\n\nThe noise process $X[n]$ is a sequence of independent and identically distributed (i.i.d.) Bernoulli random variables. At each time step $n$, the noise follows the distribution $P(X[n] = 1) = p$ and $P(X[n] = 0) = 1-p$, where $0 < p < 1$. The random variable $A$ is statistically independent of the entire noise process $X[n]$.\n\nA key desirable property for this sensor model is mean-ergodicity, which implies that the ensemble mean (the average behavior over the entire batch of sensors) can be reliably estimated from a long-term time average of the signal from a single sensor. Under what condition on the parameters $p$ and $V_0$ is the process $S[n]$ mean-ergodic?\n\nA. The process is mean-ergodic for all values of $p$ and $V_0$.\n\nB. The process is mean-ergodic only if $p = 0.5$.\n\nC. The process is mean-ergodic only if $V_0 = 0$.\n\nD. The process is mean-ergodic only if $p = 0.5$ and $V_0 = 0$.\n\nE. The process is never mean-ergodic.", "solution": "We model the process as $S[n]=A+X[n]$ with $A$ independent of $\\{X[n]\\}$, where $A$ is constant over time for a given realization and takes values $\\pm V_0$ with equal probabilities, and $X[n]$ are i.i.d. Bernoulli with $P(X[n]=1)=p$ and $P(X[n]=0)=1-p$, with $0<p<1$.\n\nFirst compute the ensemble mean $m_{S}$:\n$$\nm_{S} \\triangleq \\mathbb{E}[S[n]]=\\mathbb{E}[A]+\\mathbb{E}[X[n]].\n$$\nUsing $P(A=V_0)=P(A=-V_0)=\\frac{1}{2}$,\n$$\n\\mathbb{E}[A]=\\frac{1}{2}V_0+\\frac{1}{2}(-V_0)=0,\n$$\nand\n$$\n\\mathbb{E}[X[n]]=p.\n$$\nHence,\n$$\nm_{S}=p.\n$$\n\nConsider the time average over $N$ samples of one realization:\n$$\n\\overline{S}_{N}\\triangleq \\frac{1}{N}\\sum_{n=1}^{N}S[n]=A+\\frac{1}{N}\\sum_{n=1}^{N}X[n]\\equiv A+\\overline{X}_{N}.\n$$\nSince $\\{X[n]\\}$ are i.i.d. with variance $\\sigma_{X}^{2}=p(1-p)$, we have\n$$\n\\mathbb{E}\\big[(\\overline{X}_{N}-p)^{2}\\big]=\\operatorname{Var}(\\overline{X}_{N})=\\frac{1}{N^{2}}\\sum_{n=1}^{N}\\operatorname{Var}(X[n])=\\frac{N\\,\\sigma_{X}^{2}}{N^{2}}=\\frac{p(1-p)}{N}\\to 0 \\quad \\text{as } N\\to\\infty.\n$$\nTherefore,\n$$\n\\overline{X}_{N}\\xrightarrow{m.s.} p,\n$$\nand consequently\n$$\n\\overline{S}_{N}\\xrightarrow{m.s.} A+p.\n$$\n\nMean-ergodicity in the mean-square sense requires\n$$\n\\overline{S}_{N}\\xrightarrow{m.s.} m_{S}=p.\n$$\nFrom the limit above, this occurs if and only if $A=0$ almost surely. Under the given distribution $P(A=V_0)=P(A=-V_0)=\\frac{1}{2}$, this happens if and only if $V_0=0$. The value of $p$ does not affect this condition.\n\nTherefore, $S[n]$ is mean-ergodic only if $V_0=0$, which corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1755454"}]}