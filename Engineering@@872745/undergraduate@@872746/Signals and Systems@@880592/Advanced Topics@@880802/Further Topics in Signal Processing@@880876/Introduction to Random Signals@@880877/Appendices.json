{"hands_on_practices": [{"introduction": "In the world of digital signal processing, the conversion from a continuous analog signal to a discrete digital one is a fundamental first step. This process, known as quantization, inherently introduces a small amount of error. This exercise provides a hands-on look at how we can mathematically model this quantization error as a random variable and calculate one of its most important statistical properties: its mean value. Understanding this is crucial for analyzing the performance and limitations of any digital system, from audio recording to scientific measurement. [@problem_id:1730075]", "problem": "A digital measurement system employs an analog-to-digital converter (ADC) to process a continuous input signal. The ADC operates by rounding the incoming analog voltage to the nearest discrete quantization level. The voltage difference between any two adjacent quantization levels is a constant, denoted by $\\Delta$.\n\nThe quantization error, $e$, is defined as the difference between the quantized level and the true analog input voltage. It is a standard practice in signal processing to model this error as a continuous random variable that is uniformly distributed over the interval $\\left[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right]$.\n\nBased on this model, determine the mean value (or expected value) of the quantization error, $E[e]$. Express your answer as a symbolic expression which may involve $\\Delta$.", "solution": "The quantization error $e$ is modeled as a continuous random variable uniformly distributed over the interval $\\left[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right]$. The probability density function (pdf) for a continuous uniform distribution over $[a,b]$ is $f(x)=\\frac{1}{b-a}$ for $x \\in [a,b]$ and $0$ otherwise. Therefore, for $e$ we have\n$$\nf_{e}(x)=\\frac{1}{\\Delta} \\quad \\text{for } x \\in \\left[-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right], \\quad \\text{and } 0 \\text{ otherwise}.\n$$\nThe expected value of a continuous random variable with pdf $f_{e}(x)$ is defined by\n$$\nE[e]=\\int_{-\\infty}^{\\infty} x f_{e}(x)\\,dx.\n$$\nSubstituting the uniform pdf and restricting the integral to the support,\n$$\nE[e]=\\int_{-\\frac{\\Delta}{2}}^{\\frac{\\Delta}{2}} x \\left(\\frac{1}{\\Delta}\\right)\\,dx=\\frac{1}{\\Delta}\\int_{-\\frac{\\Delta}{2}}^{\\frac{\\Delta}{2}} x\\,dx.\n$$\nEvaluating the integral,\n$$\n\\int x\\,dx=\\frac{x^{2}}{2} \\quad \\Rightarrow \\quad \\frac{1}{\\Delta}\\left[\\frac{x^{2}}{2}\\right]_{-\\frac{\\Delta}{2}}^{\\frac{\\Delta}{2}}=\\frac{1}{\\Delta}\\left(\\frac{\\left(\\frac{\\Delta}{2}\\right)^{2}}{2}-\\frac{\\left(-\\frac{\\Delta}{2}\\right)^{2}}{2}\\right)=\\frac{1}{\\Delta}\\left(\\frac{\\Delta^{2}}{8}-\\frac{\\Delta^{2}}{8}\\right)=0.\n$$\nAlternatively, by symmetry of the uniform distribution about zero, the mean is zero. Hence,\n$$\nE[e]=0.\n$$", "answer": "$$\\boxed{0}$$", "id": "1730075"}, {"introduction": "When analyzing a random signal, we often have access only to a finite set of measurements. From this limited data, we must estimate key statistical parameters like the signal's variance, which quantifies its power or variability. This practice explores a subtle but critical aspect of statistical estimation: bias. You will investigate how using a pre-supposed target mean, rather than the true (but unknown) mean of the signal, affects the accuracy of a variance estimator. This problem highlights the importance of carefully considering your assumptions when performing statistical analysis on real-world data. [@problem_id:1730065]", "problem": "In a quality control process for manufacturing high-precision resistors, the resistance of each component is a random variable. The process is designed to produce resistors with a target mean resistance of $\\mu_0$. However, due to slight variations in manufacturing conditions, the true mean resistance of a production batch can drift to a value $\\mu$, which might not be equal to $\\mu_0$. The intrinsic random fluctuation of the process results in a true variance of $\\sigma^2$ around the true mean $\\mu$.\n\nTo monitor the process variability, a simplified estimation procedure is implemented. An inspector measures the resistance of $N$ randomly selected resistors, obtaining the values $r_1, r_2, \\ldots, r_N$. These measurements are assumed to be independent and identically distributed random variables, each with mean $\\mu$ and variance $\\sigma^2$. The proposed estimator for the process variance, denoted $V_{est}$, uses the known target mean $\\mu_0$ instead of the unknown true mean $\\mu$:\n\n$$V_{est} = \\frac{1}{N} \\sum_{i=1}^{N} (r_i - \\mu_0)^2$$\n\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$, where $E[\\cdot]$ denotes the expectation operator. Your task is to determine the bias of $V_{est}$ as an estimator for the true variance $\\sigma^2$. Express your answer as an analytic expression in terms of $\\mu$ and $\\mu_0$.", "solution": "We are given independent and identically distributed measurements $r_{1},\\ldots,r_{N}$ with $E[r_{i}]=\\mu$ and $\\operatorname{Var}(r_{i})=\\sigma^{2}$. The estimator is\n$$\nV_{est}=\\frac{1}{N}\\sum_{i=1}^{N}(r_{i}-\\mu_{0})^{2}.\n$$\nCompute its expectation using linearity of expectation:\n$$\nE[V_{est}]=\\frac{1}{N}\\sum_{i=1}^{N}E\\big[(r_{i}-\\mu_{0})^{2}\\big]=E\\big[(r_{1}-\\mu_{0})^{2}\\big].\n$$\nFor any random variable $X$ with mean $\\mu$ and variance $\\sigma^{2}$, and any constant $a$, the identity\n$$\nE[(X-a)^{2}]=\\operatorname{Var}(X)+(E[X]-a)^{2}\n$$\nholds. Apply this with $X=r_{1}$ and $a=\\mu_{0}$:\n$$\nE\\big[(r_{1}-\\mu_{0})^{2}\\big]=\\sigma^{2}+(\\mu-\\mu_{0})^{2}.\n$$\nTherefore,\n$$\nE[V_{est}]=\\sigma^{2}+(\\mu-\\mu_{0})^{2}.\n$$\nBy definition, the bias of $V_{est}$ as an estimator of $\\sigma^{2}$ is\n$$\nB(V_{est})=E[V_{est}]-\\sigma^{2}=(\\mu-\\mu_{0})^{2}.\n$$\nThis bias is nonnegative and equals zero if and only if $\\mu=\\mu_{0}$.", "answer": "$$\\boxed{(\\mu-\\mu_{0})^{2}}$$", "id": "1730065"}, {"introduction": "One of the most powerful ideas in signal processing is that linear systems transform signals in predictable ways. But what happens when the input signal is random? This exercise tackles that question by examining how a simple yet widely used digital filter—a moving average filter—alters the properties of a random noise input. Specifically, you will determine the autocorrelation function of the output signal, revealing how the filter introduces correlation into a previously uncorrelated noise process. This practice is key to understanding filter design and the analysis of random signals in communication and control systems. [@problem_id:1730025]", "problem": "A digital signal processing system is designed to smooth a noisy data stream. The input to the system, a discrete-time random process denoted by $X[n]$, represents the noise component of a sequence of measurements. This noise process is stationary and consists of uncorrelated random variables, where each variable has a mean of zero and a variance of $\\sigma_X^2$. The system implements a simple 2-tap Finite Impulse Response (FIR) filter, described by the difference equation $Y[n] = c_0 X[n] + c_1 X[n-1]$, where $Y[n]$ is the smoothed output signal and the filter coefficients are $c_0 = c_1 = \\frac{1}{\\sqrt{2}}$.\n\nDetermine the autocorrelation function, $R_Y[k] = E[Y[n]Y[n-k]]$, of the output process $Y[n]$ as a function of the integer time lag $k$ and the input variance $\\sigma_X^2$. Express your answer using the Kronecker delta function, $\\delta[k]$.", "solution": "The output process is defined by the 2-tap FIR filter $Y[n]=c_{0}X[n]+c_{1}X[n-1]$ with $c_{0}=c_{1}=1/\\sqrt{2}$. The autocorrelation function is\n$$\nR_{Y}[k]=E\\!\\left[Y[n]\\,Y[n-k]\\right]=E\\!\\left[\\left(c_{0}X[n]+c_{1}X[n-1]\\right)\\left(c_{0}X[n-k]+c_{1}X[n-k-1]\\right)\\right].\n$$\nUsing linearity of expectation and $c_{0}c_{0}=c_{0}c_{1}=c_{1}c_{0}=c_{1}c_{1}=1/2$, we expand:\n$$\nR_{Y}[k]=\\frac{1}{2}E\\!\\left[X[n]X[n-k]\\right]+\\frac{1}{2}E\\!\\left[X[n]X[n-k-1]\\right]+\\frac{1}{2}E\\!\\left[X[n-1]X[n-k]\\right]+\\frac{1}{2}E\\!\\left[X[n-1]X[n-k-1]\\right].\n$$\nSince $\\{X[n]\\}$ is white, zero-mean, and stationary with variance $\\sigma_{X}^{2}$, its autocorrelation is $R_{X}[k]=E[X[n]X[n-k]]=\\sigma_{X}^{2}\\delta[k]$. Therefore,\n$$\nE[X[n]X[n-k]]=\\sigma_{X}^{2}\\delta[k],\\quad E[X[n]X[n-k-1]]=\\sigma_{X}^{2}\\delta[k+1],\n$$\n$$\nE[X[n-1]X[n-k]]=\\sigma_{X}^{2}\\delta[k-1],\\quad E[X[n-1]X[n-k-1]]=\\sigma_{X}^{2}\\delta[k].\n$$\nSubstituting these into the expansion and combining terms yields\n$$\nR_{Y}[k]=\\frac{1}{2}\\sigma_{X}^{2}\\left(\\delta[k]+\\delta[k+1]+\\delta[k-1]+\\delta[k]\\right)=\\sigma_{X}^{2}\\delta[k]+\\frac{\\sigma_{X}^{2}}{2}\\left(\\delta[k-1]+\\delta[k+1]\\right).\n$$\nEquivalently, this can be seen from the general result for filtering white noise: with impulse response $h[0]=h[1]=1/\\sqrt{2}$ and $h[m]=0$ otherwise, $R_{Y}[k]=\\sigma_{X}^{2}\\sum_{m}h[m]h[m-k]$, which gives the same three-point sequence.", "answer": "$$\\boxed{\\sigma_{X}^{2}\\left(\\delta[k]+\\frac{1}{2}\\delta[k-1]+\\frac{1}{2}\\delta[k+1]\\right)}$$", "id": "1730025"}]}