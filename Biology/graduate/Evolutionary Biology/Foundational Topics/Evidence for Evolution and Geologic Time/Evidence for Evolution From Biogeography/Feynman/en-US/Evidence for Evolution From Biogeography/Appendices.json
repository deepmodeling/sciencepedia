{
    "hands_on_practices": [
        {
            "introduction": "A classic pattern in island biogeography is the \"progression rule,\" often observed in volcanic archipelagos formed by geological hotspots. This exercise allows you to test this rule by assessing the correlation between island age and the age of colonization by a species clade. You will use Kendall's $\\tau$ rank correlation, a robust non-parametric method, to quantitatively evaluate whether evolutionary history mirrors geological history, providing a tangible way to test a core biogeographic hypothesis with statistical rigor .",
            "id": "2705037",
            "problem": "A volcanic hotspot archipelago has formed a linear chain of islands with well-constrained emergence ages. Under the progression rule of hotspot archipelagos in island biogeography, colonization or in situ divergence within a clade should proceed from older to younger islands, producing a monotonic association between island geological ages and colonization ages inferred from phylogenetic divergence times. Consider a focal endemic plant clade for which the colonization ages on each island have been estimated using a relaxed-clock phylogeny calibrated with fossils external to the archipelago.\n\nYou are given the following paired data for $n=8$ islands, where each pair lists the island’s emergence age in million years ago (Ma) and the estimated colonization age in Ma for the clade on that island. Each colonization event post-dates island emergence.\n\n- Island $1$: $(5.6,\\ 4.7)$\n- Island $2$: $(4.9,\\ 4.3)$\n- Island $3$: $(4.1,\\ 3.0)$\n- Island $4$: $(3.4,\\ 3.1)$\n- Island $5$: $(2.6,\\ 1.5)$\n- Island $6$: $(1.9,\\ 1.7)$\n- Island $7$: $(1.1,\\ 0.9)$\n- Island $8$: $(0.6,\\ 0.4)$\n\nAssume there are no ties in either variable and treat each island as an independent observational unit for the purposes of rank correlation. Using only rank information, compute Kendall’s $\\tau$ rank correlation between island ages and colonization ages to assess the progression rule. Then perform a two-sided significance test for Kendall’s $\\tau$ at significance level $\\alpha=0.05$ under the null hypothesis of no association, using the standard null variance for Kendall’s statistic without continuity correction.\n\nReport the value of Kendall’s $\\tau$ as your final answer, rounded to four significant figures. No units are required for the final answer.",
            "solution": "The problem presents a set of paired data for island emergence ages and clade colonization ages and asks for a statistical evaluation of the \"progression rule\" hypothesis using Kendall's rank correlation. The problem is scientifically grounded in island biogeography and presents a well-posed statistical question. It is free of contradictions, ambiguities, and factual errors. Therefore, it is a valid problem, and a solution will be furnished.\n\nThe objective is to compute Kendall's rank correlation coefficient, $\\tau$, and to test its statistical significance. Kendall's $\\tau$ measures the ordinal association between two measured quantities. For a set of $n$ pairs of observations $(x_i, y_i)$, Kendall's $\\tau$ is defined as:\n$$\n\\tau = \\frac{N_c - N_d}{\\frac{1}{2} n(n-1)}\n$$\nwhere $N_c$ is the number of concordant pairs and $N_d$ is the number of discordant pairs. The denominator is the total number of pairs of observations. A pair of observations, $(x_i, y_i)$ and $(x_j, y_j)$, is concordant if the ranks of both elements agree (i.e., if $x_i  x_j$ and $y_i  y_j$, or if $x_i  x_j$ and $y_i  y_j$). It is discordant if the ranks disagree (i.e., if $x_i  x_j$ and $y_i  y_j$, or if $x_i  x_j$ and $y_i  y_j$). The problem states there are no ties, simplifying the calculation.\n\nThe given data consists of $n=8$ pairs, with island emergence age as variable $X$ and colonization age as variable $Y$, both in millions of years ago (Ma). The pairs are: $(5.6, 4.7)$, $(4.9, 4.3)$, $(4.1, 3.0)$, $(3.4, 3.1)$, $(2.6, 1.5)$, $(1.9, 1.7)$, $(1.1, 0.9)$, and $(0.6, 0.4)$.\n\nTo compute $N_c$ and $N_d$, we first order the data according to one variable, say $X$. The data are already provided in descending order of island age $X$.\n$X: 5.6  4.9  4.1  3.4  2.6  1.9  1.1  0.6$\nThe corresponding sequence of $Y$ values is:\n$Y: 4.7, 4.3, 3.0, 3.1, 1.5, 1.7, 0.9, 0.4$\n\nNow, we count for each $y_i$, the number of subsequent values $y_j$ (where $ji$) that are smaller (concordant) or larger (discordant).\n\n1.  For $y_1 = 4.7$: The subsequent values are $\\{4.3, 3.0, 3.1, 1.5, 1.7, 0.9, 0.4\\}$. All $7$ values are smaller.\n    Concordant count = $7$. Discordant count = $0$.\n\n2.  For $y_2 = 4.3$: The subsequent values are $\\{3.0, 3.1, 1.5, 1.7, 0.9, 0.4\\}$. All $6$ values are smaller.\n    Concordant count = $6$. Discordant count = $0$.\n\n3.  For $y_3 = 3.0$: The subsequent values are $\\{3.1, 1.5, 1.7, 0.9, 0.4\\}$. Values smaller are $\\{1.5, 1.7, 0.9, 0.4\\}$ ($4$ values). Value larger is $\\{3.1\\}$ ($1$ value).\n    Concordant count = $4$. Discordant count = $1$.\n\n4.  For $y_4 = 3.1$: The subsequent values are $\\{1.5, 1.7, 0.9, 0.4\\}$. All $4$ values are smaller.\n    Concordant count = $4$. Discordant count = $0$.\n\n5.  For $y_5 = 1.5$: The subsequent values are $\\{1.7, 0.9, 0.4\\}$. Values smaller are $\\{0.9, 0.4\\}$ ($2$ values). Value larger is $\\{1.7\\}$ ($1$ value).\n    Concordant count = $2$. Discordant count = $1$.\n\n6.  For $y_6 = 1.7$: The subsequent values are $\\{0.9, 0.4\\}$. Both $2$ values are smaller.\n    Concordant count = $2$. Discordant count = $0$.\n\n7.  For $y_7 = 0.9$: The subsequent value is $\\{0.4\\}$. This $1$ value is smaller.\n    Concordant count = $1$. Discordant count = $0$.\n\nThe total number of concordant pairs is the sum of the individual counts:\n$$\nN_c = 7 + 6 + 4 + 4 + 2 + 2 + 1 = 26\n$$\nThe total number of discordant pairs is:\n$$\nN_d = 0 + 0 + 1 + 0 + 1 + 0 + 0 = 2\n$$\nThe total number of pairs is $\\binom{n}{2} = \\binom{8}{2} = \\frac{8 \\times 7}{2} = 28$.\nAs a check, $N_c + N_d = 26 + 2 = 28$, which is correct.\n\nNow, we compute Kendall's $\\tau$:\n$$\n\\tau = \\frac{N_c - N_d}{N_c + N_d} = \\frac{26 - 2}{26 + 2} = \\frac{24}{28} = \\frac{6}{7}\n$$\nAs a decimal, $\\tau \\approx 0.8571428...$. Rounded to four significant figures, $\\tau = 0.8571$.\n\nNext, we perform the two-sided significance test.\nThe null hypothesis $H_0$ is that there is no association between the variables, i.e., $\\tau = 0$.\nThe alternative hypothesis $H_1$ is that there is an association, i.e., $\\tau \\neq 0$.\nThe significance level is $\\alpha = 0.05$.\n\nFor sample sizes $n  10$, the distribution of the statistic $S = N_c - N_d$ under $H_0$ is approximately normal. We will use this approximation as implicitly instructed.\nUnder $H_0$, the expected value of $S$ is $E[S] = 0$.\nThe variance of $S$ (for the case with no ties) is given by:\n$$\n\\sigma_S^2 = \\text{Var}(S) = \\frac{n(n-1)(2n+5)}{18}\n$$\nFor $n=8$, we calculate the variance:\n$$\n\\sigma_S^2 = \\frac{8(8-1)(2(8)+5)}{18} = \\frac{8 \\times 7 \\times (16+5)}{18} = \\frac{8 \\times 7 \\times 21}{18} = \\frac{1176}{18} = \\frac{196}{3}\n$$\nThe standard deviation is:\n$$\n\\sigma_S = \\sqrt{\\frac{196}{3}} = \\frac{14}{\\sqrt{3}} \\approx 8.083\n$$\nThe test statistic $Z$ is calculated without continuity correction, as specified:\n$$\nZ = \\frac{S - E[S]}{\\sigma_S} = \\frac{S}{\\sigma_S}\n$$\nOur observed value of $S$ is $N_c - N_d = 24$.\n$$\nZ = \\frac{24}{14/\\sqrt{3}} = \\frac{24\\sqrt{3}}{14} = \\frac{12\\sqrt{3}}{7} \\approx 2.969\n$$\nFor a two-sided test at $\\alpha = 0.05$, the critical values are $Z_{\\alpha/2} = \\pm 1.96$.\nSince our observed test statistic $|Z| \\approx 2.969$ is greater than the critical value $1.96$, we reject the null hypothesis $H_0$. The p-value for this test is $p = 2 \\times P(Z \\ge 2.969) \\approx 0.003$, which is much less than $\\alpha=0.05$.\nThe conclusion is that there is a statistically significant positive association between island age and colonization age, which is consistent with the progression rule.\n\nThe problem asks only for the value of Kendall's $\\tau$.\n$$\n\\tau = \\frac{6}{7} \\approx 0.8571\n$$",
            "answer": "$$\n\\boxed{0.8571}\n$$"
        },
        {
            "introduction": "Movement and colonization are not merely functions of straight-line distance but are shaped by the landscape's features. This practice introduces the concept of \"effective distance,\" which quantifies the path of least resistance across a heterogeneous environment. You will first construct a resistance landscape and then use Dijkstra's algorithm to calculate these effective distances, a core task in landscape genetics, before fitting a colonization probability model using maximum likelihood estimation . This powerful combination of algorithmic pathfinding and statistical inference allows you to model how geography mechanistically influences dispersal.",
            "id": "2705035",
            "problem": "You are given discrete resistance landscapes and observed colonization outcomes between a single source site and multiple destination sites. The scientific premise is that colonization probability decays with effective distance, a biogeographic quantity that accounts for the cumulative cost of movement across a heterogeneous landscape. Assume the following generative model: for each source-destination pair indexed by $i$, the colonization outcome $Y_i \\in \\{0,1\\}$ is a Bernoulli trial with success probability $p_i$, and the probability decays with effective distance as $p_i = \\exp(-\\beta \\,\\mathrm{ED}_i)$, where $\\mathrm{ED}_i \\ge 0$ is the effective distance and $\\beta \\ge 0$ is an unknown parameter. Your task is to estimate $\\beta$ by maximum likelihood given the data below.\n\nFundamental basis to use:\n- A colonization outcome $Y_i$ is modeled as a Bernoulli random variable with parameter $p_i$, and independent across $i$.\n- The joint likelihood of independent Bernoulli observations is the product of individual likelihoods.\n- The effective distance $\\mathrm{ED}$ between two cells on a raster is defined as the minimum cumulative movement cost over an $8$-connected grid, where the movement cost of stepping from one cell to a neighboring cell is the average of their cell costs multiplied by the Euclidean step length.\n- A cell’s cost is a nonnegative linear combination of resistance layers: if there are $K$ layers with cell values $L_{k}(r,c)$ and nonnegative weights $w_k$, then the total cell cost is $C(r,c) = \\sum_{k=1}^{K} w_k \\, L_k(r,c)$.\n\nEffective distance computation details:\n- The grid is indexed by $(r,c)$ with $r$ as zero-based row and $c$ as zero-based column.\n- Movement is allowed to the $8$ immediate neighbors. For orthogonal moves the step length is $1$, for diagonal moves the step length is $\\sqrt{2}$.\n- The step cost from cell $(r,c)$ to neighbor $(r',c')$ is $\\frac{C(r,c)+C(r',c')}{2} \\times d$, where $d$ is the Euclidean distance between the centers of the two cells, which is either $1$ or $\\sqrt{2}$.\n\nEstimation details:\n- Constrain $\\beta$ to the closed interval $[0,\\beta_{\\max}]$, with $\\beta_{\\max} = 10$. If the unconstrained maximum likelihood estimate lies outside, return the boundary value.\n- For degenerate datasets, if all $Y_i = 1$, return $\\beta = 0$. If all $Y_i = 0$, return $\\beta = \\beta_{\\max}$.\n- Your program must compute $\\mathrm{ED}_i$ values from the provided resistance layers using the rule above and then estimate $\\beta$ by maximizing the likelihood under the model $p_i = \\exp(-\\beta \\,\\mathrm{ED}_i)$.\n\nTest suite:\nImplement your solution on the following three test cases. In all cases, answer by reporting the maximum likelihood estimate of $\\beta$ as a float rounded to $6$ decimal places.\n\n- Test case A:\n  - Grid size: $5 \\times 5$.\n  - Layers ($L_1$, $L_2$) given as matrices with entries:\n    - $L_1 = \\begin{bmatrix}\n    1  1  2  3  4\\\\\n    1  2  2  3  4\\\\\n    1  2  3  3  4\\\\\n    2  2  3  4  5\\\\\n    3  3  4  5  5\n    \\end{bmatrix}$,\n    $L_2 = \\begin{bmatrix}\n    3  3  3  3  3\\\\\n    3  2  2  2  3\\\\\n    3  2  1  2  3\\\\\n    3  2  2  2  3\\\\\n    3  3  3  3  3\n    \\end{bmatrix}$.\n  - Weights: $w_1 = 0.6$, $w_2 = 0.4$.\n  - Source: $(0,0)$.\n  - Destinations: $(0,4)$, $(2,2)$, $(4,4)$, $(4,0)$, $(1,3)$.\n  - Observed outcomes $Y$: $[0,1,0,0,1]$.\n\n- Test case B:\n  - Grid size: $4 \\times 4$.\n  - Layers ($L_1$, $L_2$):\n    - $L_1 = \\begin{bmatrix}\n    2  2  3  4\\\\\n    2  3  4  5\\\\\n    3  4  5  6\\\\\n    4  5  6  7\n    \\end{bmatrix}$,\n    $L_2 = \\begin{bmatrix}\n    1  1  1  1\\\\\n    1  2  2  1\\\\\n    1  2  3  1\\\\\n    1  1  1  1\n    \\end{bmatrix}$.\n  - Weights: $w_1 = 0.5$, $w_2 = 0.5$.\n  - Source: $(0,0)$.\n  - Destinations: $(3,3)$, $(2,1)$, $(1,2)$.\n  - Observed outcomes $Y$: $[0,0,0]$.\n\n- Test case C:\n  - Grid size: $3 \\times 3$.\n  - Layers ($L_1$, $L_2$):\n    - $L_1 = \\begin{bmatrix}\n    1  1  1\\\\\n    1  1  1\\\\\n    1  1  1\n    \\end{bmatrix}$,\n    $L_2 = \\begin{bmatrix}\n    1  2  1\\\\\n    2  1  2\\\\\n    1  2  1\n    \\end{bmatrix}$.\n  - Weights: $w_1 = 1.0$, $w_2 = 0.0$.\n  - Source: $(1,1)$.\n  - Destinations: $(0,0)$, $(0,2)$, $(2,0)$, $(2,2)$.\n  - Observed outcomes $Y$: $[1,1,1,1]$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the three estimates $[\\hat{\\beta}_A,\\hat{\\beta}_B,\\hat{\\beta}_C]$ as a comma-separated list enclosed in square brackets, with each estimate rounded to $6$ decimal places. For example, an output line could look like $[0.731000,10.000000,0.000000]$.\n- No additional text should be printed.",
            "solution": "The problem statement is parsed and validated. It is found to be scientifically grounded, well-posed, and objective. The problem is a standard application of maximum likelihood estimation in landscape genetics, a subfield of evolutionary biology. All necessary definitions, data, and constraints are provided, and no contradictions are present. The problem is therefore deemed **valid**.\n\nThe solution to this problem requires two main computational steps: first, the calculation of effective distances ($ED$) from a source to multiple destinations on a resistance landscape; second, the estimation of the decay parameter $\\beta$ using maximum likelihood.\n\n**1. Effective Distance Calculation**\n\nThe effective distance is defined as the minimum cumulative cost of movement between two locations. This is a classic shortest path problem on a graph. The grid cells $(r,c)$ represent the nodes of the graph. An edge exists between any two cells that are adjacent in an $8$-connected sense (including diagonals).\n\nThe cost of traversing an edge from cell $u$ with coordinates $(r,c)$ to a neighboring cell $v$ with coordinates $(r',c')$ is given by the formula:\n$$ \\text{cost}(u,v) = \\frac{C(r,c) + C(r',c')}{2} \\times d_{uv} $$\nwhere $d_{uv}$ is the Euclidean distance between the cell centers. For orthogonal neighbors, $d_{uv}=1$. For diagonal neighbors, $d_{uv}=\\sqrt{2}$. The term $C(r,c)$ represents the total cost or resistance of a cell, which must be computed first. It is a weighted sum of $K$ different resistance layers $L_k$:\n$$ C(r,c) = \\sum_{k=1}^{K} w_k L_k(r,c) $$\nwhere $w_k$ are non-negative weights.\n\nTo find the minimum cumulative cost from the single source cell to all other cells in the grid, we employ Dijkstra's algorithm. The algorithm starts with the source cell having a distance of $0$ and all other cells having an infinite distance. It iteratively explores the grid, always extending the path from the unvisited cell with the smallest known distance, and updates the distances to its neighbors if a shorter path is found. A priority queue is used to efficiently manage the set of unvisited cells. After the algorithm terminates, we have the effective distance from the source to every other cell, from which we extract the values $\\mathrm{ED}_i$ for the specified destination sites.\n\n**2. Maximum Likelihood Estimation of $\\beta$**\n\nThe colonization outcome for each destination site $i$, denoted by $Y_i \\in \\{0, 1\\}$, is modeled as an independent Bernoulli trial with success probability $p_i$. This probability is assumed to decay exponentially with effective distance $\\mathrm{ED}_i$:\n$$ p_i = \\exp(-\\beta \\,\\mathrm{ED}_i) $$\nwhere $\\beta \\ge 0$ is the parameter to be estimated.\n\nThe likelihood of a single observation $(Y_i, \\mathrm{ED}_i)$ is given by the Bernoulli probability mass function:\n$$ L_i(\\beta | Y_i, \\mathrm{ED}_i) = p_i^{Y_i} (1 - p_i)^{1 - Y_i} $$\nDue to the independence of observations, the total likelihood for the entire dataset is the product of individual likelihoods, $L(\\beta) = \\prod_{i} L_i(\\beta)$. It is computationally more convenient to work with the log-likelihood function, $\\ell(\\beta) = \\log L(\\beta)$:\n$$ \\ell(\\beta) = \\sum_{i=1}^{N} \\left[ Y_i \\log(p_i) + (1-Y_i) \\log(1-p_i) \\right] $$\nSubstituting the model for $p_i$, we get:\n$$ \\ell(\\beta) = \\sum_{i=1}^{N} \\left[ Y_i \\log(\\exp(-\\beta \\,\\mathrm{ED}_i)) + (1-Y_i) \\log(1 - \\exp(-\\beta \\,\\mathrm{ED}_i)) \\right] $$\n$$ \\ell(\\beta) = -\\beta \\sum_{i=1}^{N} Y_i \\mathrm{ED}_i + \\sum_{i=1}^{N} (1-Y_i) \\log(1 - \\exp(-\\beta \\,\\mathrm{ED}_i)) $$\nThe maximum likelihood estimate (MLE) $\\hat{\\beta}$ is the value of $\\beta$ that maximizes this function. The second derivative of $\\ell(\\beta)$ with respect to $\\beta$ is non-positive, which ensures that the log-likelihood function is concave and has a unique maximum.\n\nA closed-form solution for $\\hat{\\beta}$ does not exist, so a numerical optimization method must be used. We seek to find the maximum of $\\ell(\\beta)$ within the constrained interval $\\beta \\in [0, \\beta_{\\max}]$, where $\\beta_{\\max}=10$. This is equivalent to minimizing the negative log-likelihood, $-\\ell(\\beta)$. For numerical stability, especially for small values of the argument $x = \\beta \\mathrm{ED}_i$, the term $\\log(1 - \\exp(-x))$ is computed as $\\log(\\text{expm1}(x)) - x$.\n\nThe problem specifies handling of degenerate cases:\n- If all observed outcomes $Y_i$ are $1$ (colonization succeeded everywhere), the MLE is $\\hat{\\beta}=0$. This corresponds to a situation where distance poses no barrier.\n- If all $Y_i$ are $0$ (colonization failed everywhere), the MLE is taken as the upper bound, $\\hat{\\beta}=\\beta_{\\max}=10$. This reflects the strongest possible distance effect allowed by the model constraints.\n\nFor the general case with mixed outcomes, a numerical solver, such as `scipy.optimize.minimize_scalar` with the `bounded` method, is employed to find the value of $\\beta$ that minimizes the negative log-likelihood within the specified interval $[0, 10]$.",
            "answer": "```python\nimport numpy as np\nimport heapq\nfrom scipy.optimize import minimize_scalar\n\ndef solve_case(grid_size, L1, L2, w1, w2, source, destinations, outcomes):\n    \"\"\"\n    Computes the MLE for beta for a single test case.\n    \"\"\"\n    outcomes = np.array(outcomes)\n    \n    # Handle degenerate cases as per the problem statement\n    if np.all(outcomes == 1):\n        return 0.0\n    if np.all(outcomes == 0):\n        return 10.0\n\n    # 1. Compute the cell cost matrix C\n    L1, L2 = np.array(L1), np.array(L2)\n    C = w1 * L1 + w2 * L2\n    rows, cols = grid_size\n\n    # 2. Run Dijkstra's algorithm to find effective distances\n    dists = np.full((rows, cols), np.inf)\n    dists[source] = 0\n    pq = [(0, source)]  # (distance, (r, c))\n\n    while pq:\n        d, (r, c) = heapq.heappop(pq)\n\n        if d > dists[r, c]:\n            continue\n\n        for dr in [-1, 0, 1]:\n            for dc in [-1, 0, 1]:\n                if dr == 0 and dc == 0:\n                    continue\n\n                nr, nc = r + dr, c + dc\n\n                if 0 = nr  rows and 0 = nc  cols:\n                    step_len = np.sqrt(dr**2 + dc**2)\n                    avg_cost = (C[r, c] + C[nr, nc]) / 2\n                    step_cost = avg_cost * step_len\n                    \n                    new_dist = dists[r, c] + step_cost\n                    if new_dist  dists[nr, nc]:\n                        dists[nr, nc] = new_dist\n                        heapq.heappush(pq, (new_dist, (nr, nc)))\n\n    # 3. Extract effective distances for specified destinations\n    EDs = np.array([dists[dest] for dest in destinations])\n\n    # 4. Define the negative log-likelihood function\n    def neg_log_likelihood(beta, ed_vec, y_vec):\n        # A very small beta is numerically equivalent to beta=0 for this model\n        if beta  1e-12:\n            # If beta=0, p=1. Likelihood is 0 if any y=0. Log-L is -inf.\n            if np.any(y_vec == 0):\n                return np.inf\n            # If all y=1, Log-L = -beta * sum(ed). -Log-L is beta*sum(ed)\n            else:\n                return beta * np.sum(ed_vec)\n        \n        # We want to minimize the negative log-likelihood.\n        # -ll(beta) = -sum( Y*log(p) + (1-Y)*log(1-p) )\n        # p = exp(-beta*ED) -> log(p) = -beta*ED\n        # log(1-p) = log(1 - exp(-beta*ED))\n        # -ll(beta) = sum( Y*beta*ED - (1-Y)*log(1-exp(-beta*ED)) )\n        # Using identity log(1-exp(-x)) = log(expm1(x)) - x, where x = beta*ED\n        # -ll(beta) = sum( Y*beta*ED - (1-Y)*(log(expm1(beta*ED)) - beta*ED) )\n        # -ll(beta) = sum( (Y + (1-Y))*beta*ED - (1-Y)*log(expm1(beta*ED)) )\n        # -ll(beta) = sum( beta*ED ) - sum( (1-Y)*log(expm1(beta*ED)) )\n        z = beta * ed_vec\n        \n        # term1 = beta * np.sum(ed_vec)\n        # term2 = np.sum((1 - y_vec) * np.log(np.expm1(z)))\n        \n        logL = 0\n        successes = ed_vec[y_vec == 1]\n        failures = ed_vec[y_vec == 0]\n\n        logL -= beta * np.sum(successes)\n        \n        # log(1 - exp(-beta*ED))\n        # Use log1p for numerical stability\n        z_fail = -beta * failures\n        logL += np.sum(np.log1p(-np.exp(z_fail)))\n        \n        return -logL\n\n    # 5. Minimize the negative log-likelihood to find beta\n    result = minimize_scalar(\n        neg_log_likelihood,\n        args=(EDs, outcomes),\n        bounds=(0, 10),\n        method='bounded'\n    )\n    \n    return result.x\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {\n            \"grid_size\": (5, 5),\n            \"L1\": [\n                [1, 1, 2, 3, 4], [1, 2, 2, 3, 4], [1, 2, 3, 3, 4],\n                [2, 2, 3, 4, 5], [3, 3, 4, 5, 5]\n            ],\n            \"L2\": [\n                [3, 3, 3, 3, 3], [3, 2, 2, 2, 3], [3, 2, 1, 2, 3],\n                [3, 2, 2, 2, 3], [3, 3, 3, 3, 3]\n            ],\n            \"w1\": 0.6, \"w2\": 0.4,\n            \"source\": (0, 0),\n            \"destinations\": [(0, 4), (2, 2), (4, 4), (4, 0), (1, 3)],\n            \"outcomes\": [0, 1, 0, 0, 1]\n        },\n        # Test case B\n        {\n            \"grid_size\": (4, 4),\n            \"L1\": [\n                [2, 2, 3, 4], [2, 3, 4, 5], \n                [3, 4, 5, 6], [4, 5, 6, 7]\n            ],\n            \"L2\": [\n                [1, 1, 1, 1], [1, 2, 2, 1], \n                [1, 2, 3, 1], [1, 1, 1, 1]\n            ],\n            \"w1\": 0.5, \"w2\": 0.5,\n            \"source\": (0, 0),\n            \"destinations\": [(3, 3), (2, 1), (1, 2)],\n            \"outcomes\": [0, 0, 0]\n        },\n        # Test case C\n        {\n            \"grid_size\": (3, 3),\n            \"L1\": [\n                [1, 1, 1], [1, 1, 1], [1, 1, 1]\n            ],\n            \"L2\": [\n                [1, 2, 1], [2, 1, 2], [1, 2, 1]\n            ],\n            \"w1\": 1.0, \"w2\": 0.0,\n            \"source\": (1, 1),\n            \"destinations\": [(0, 0), (0, 2), (2, 0), (2, 2)],\n            \"outcomes\": [1, 1, 1, 1]\n        }\n    ]\n\n    results = []\n    for case_data in test_cases:\n        beta_hat = solve_case(**case_data)\n        results.append(f\"{beta_hat:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\n# Note: The original python code in the prompt for this problem had a bug in the\n# log-likelihood calculation. The corrected, numerically stable implementation using\n# log1p is provided here. The logic in the solution text, however, remains correct\n# in its high-level description.\nsolve()\n```"
        },
        {
            "introduction": "Biogeographic barriers, such as the formation of a mountain range or a seaway, can simultaneously fragment the populations of many co-distributed species, leading to synchronous divergence. This exercise provides a hands-on introduction to a core problem in comparative phylogeography: testing for such temporal congruence in speciation events. Using a Bayesian framework, you will analyze multilocus genetic data to compute the Bayes factor comparing a model of shared, synchronous divergence ($M_S$) against a model of independent, lineage-specific divergence times ($M_I$) . This practice offers a rigorous, model-based approach to weigh evidence for a single, shared historical event shaping an entire biological community.",
            "id": "2705269",
            "problem": "You are given multilocus sequence-divergence summaries for multiple pairs of closely related taxa that occur on different sides of a putative biogeographic barrier. Under a strict molecular clock and low divergence, the accumulation of substitutions along lineages can be modeled as a Poisson process. Assume the following generative model: For each taxon pair index $k \\in \\{1,\\dots,K\\}$ and locus index $\\ell \\in \\{1,\\dots,L\\}$, the observed number of site differences $x_{k,\\ell}$ across sequences of length $n_{k,\\ell}$ sites is modeled as\n$$\nx_{k,\\ell} \\,\\sim\\, \\mathrm{Poisson}\\!\\left(\\lambda_{k,\\ell}\\right), \\quad \\lambda_{k,\\ell} \\,=\\, a_{k,\\ell}\\, T_k, \\quad a_{k,\\ell} \\,=\\, 2\\, r_{\\ell}\\, n_{k,\\ell},\n$$\nwhere $r_{\\ell}$ is the per-site substitution rate (in Myr$^{-1}$ per site), $T_k$ is the divergence time for taxon pair $k$ (in Myr), and the factor $2$ reflects two independent lineages since the split under a strict clock. The loci are independent given parameters. Place a conjugate Gamma prior on each divergence time. Specifically, under the independent-times model, $M_I$, assume $T_k \\stackrel{\\mathrm{iid}}{\\sim} \\mathrm{Gamma}(\\alpha,\\beta)$ with shape $\\alpha$ and rate $\\beta$ (so that $\\mathbb{E}[T_k] = \\alpha/\\beta$). Under the shared-time (synchrony) model, $M_S$, assume $T_1=\\dots=T_K = T$ and place the same prior $T \\sim \\mathrm{Gamma}(\\alpha,\\beta)$. All priors are independent of $r_{\\ell}$ and $n_{k,\\ell}$, which are treated as known.\n\nYour task is to, for each test case below, compute the following quantities:\n- The posterior mean of the shared divergence time under $M_S$, namely $\\mathbb{E}[T \\mid \\text{data}, M_S]$, expressed in Myr.\n- The Bayes factor $BF_{S,I} = \\dfrac{p(\\text{data}\\mid M_S)}{p(\\text{data}\\mid M_I)}$ comparing the shared-time model $M_S$ to the independent-times model $M_I$.\n- The posterior probability of $M_S$ given equal prior model probabilities, namely $\\mathbb{P}(M_S \\mid \\text{data}) = \\dfrac{BF_{S,I}}{BF_{S,I} + 1}$.\n\nBase your derivation on the following fundamental and well-tested facts:\n- Under a strict molecular clock with low divergence, the number of substitutions along a lineage over time $t$ at rate $r$ per site per unit time is modeled by a Poisson process, and independent lineages yield additive rates.\n- The Poisson likelihood for counts with a Gamma prior on the Poisson mean parameter yields Gamma–Poisson conjugacy and closed-form posterior and marginal likelihoods.\n- Bayes factors are ratios of marginal likelihoods, and posterior model probabilities with equal prior odds satisfy $\\mathbb{P}(M_S \\mid \\text{data}) = \\dfrac{BF_{S,I}}{BF_{S,I} + 1}$.\n\nYour program must implement the computation in a numerically stable way (use logarithms for products of gamma functions) and must produce results for the following test suite. Use $\\alpha = 1.0$ (dimensionless) and $\\beta = 1.0$ Myr$^{-1}$ in all test cases. All time quantities must be expressed in Myr, substitution rates $r_{\\ell}$ are in Myr$^{-1}$ per site, sequence lengths $n_{k,\\ell}$ are in sites, and counts $x_{k,\\ell}$ are dimensionless. Round all final reported floats to six decimal places.\n\nTest Suite:\n- Test Case $1$ (general happy path; three taxa with three loci; moderate counts consistent with a shared divergence):\n  - Number of taxa $K = 3$, number of loci $L = 3$.\n  - Locus-specific rates $[r_1, r_2, r_3] = [1.0\\times 10^{-3}, 5.0\\times 10^{-4}, 2.0\\times 10^{-3}]$ Myr$^{-1}$ per site.\n  - Sequence lengths matrix $[n_{k,\\ell}]$ (rows are taxa, columns are loci, in sites):\n    - Taxon $1$: $[1000, 1200, 800]$.\n    - Taxon $2$: $[1000, 1200, 800]$.\n    - Taxon $3$: $[1000, 1200, 800]$.\n  - Observed counts matrix $[x_{k,\\ell}]$:\n    - Taxon $1$: $[2, 1, 4]$.\n    - Taxon $2$: $[3, 2, 3]$.\n    - Taxon $3$: $[2, 1, 4]$.\n\n- Test Case $2$ (asynchrony; two taxa with distinct implied divergence scales using the same three loci):\n  - Number of taxa $K = 2$, number of loci $L = 3$.\n  - Locus-specific rates $[r_1, r_2, r_3] = [1.0\\times 10^{-3}, 5.0\\times 10^{-4}, 2.0\\times 10^{-3}]$ Myr$^{-1}$ per site.\n  - Sequence lengths matrix $[n_{k,\\ell}]$ (in sites):\n    - Taxon $1$: $[1000, 1200, 800]$.\n    - Taxon $2$: $[1000, 1200, 800]$.\n  - Observed counts matrix $[x_{k,\\ell}]$:\n    - Taxon $1$: $[1, 0, 2]$.\n    - Taxon $2$: $[6, 4, 10]$.\n\n- Test Case $3$ (edge case; a single taxon with two loci, including a zero count; this exercises the boundary where $K=1$ so models $M_S$ and $M_I$ coincide in evidence):\n  - Number of taxa $K = 1$, number of loci $L = 2$.\n  - Locus-specific rates $[r_1, r_2] = [1.0\\times 10^{-3}, 1.0\\times 10^{-3}]$ Myr$^{-1}$ per site.\n  - Sequence lengths matrix $[n_{1,\\ell}]$ (in sites): $[500, 1500]$.\n  - Observed counts matrix $[x_{1,\\ell}]$: $[0, 3]$.\n\nFinal Output Format:\n- For each test case, output a list of three floats in the order: $[\\mathbb{E}[T \\mid \\text{data}, M_S]\\ \\text{in Myr},\\ \\mathbb{P}(M_S \\mid \\text{data}),\\ \\log_{10} BF_{S,I}]$, each rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of these three-element lists enclosed in square brackets. For example: \n  - $[[t_1,p_1,b_1],[t_2,p_2,b_2],[t_3,p_3,b_3]]$",
            "solution": "The problem as stated is subjected to validation.\n\nGivens are extracted verbatim as follows:\n- **Generative model**: For each taxon pair $k \\in \\{1,\\dots,K\\}$ and locus $\\ell \\in \\{1,\\dots,L\\}$, the observed number of site differences $x_{k,\\ell}$ for a sequence of length $n_{k,\\ell}$ is $x_{k,\\ell} \\sim \\mathrm{Poisson}(\\lambda_{k,\\ell})$, with rate $\\lambda_{k,\\ell} = a_{k,\\ell} T_k$, where $a_{k,\\ell} = 2 r_{\\ell} n_{k,\\ell}$. The per-site substitution rate is $r_{\\ell}$ and divergence time is $T_k$. Loci are independent given parameters.\n- **Model $M_I$ (Independent Times)**: $T_k \\stackrel{\\mathrm{iid}}{\\sim} \\mathrm{Gamma}(\\alpha, \\beta)$.\n- **Model $M_S$ (Shared Time)**: $T_1=\\dots=T_K = T$, with $T \\sim \\mathrm{Gamma}(\\alpha, \\beta)$.\n- **Known quantities**: $r_{\\ell}$, $n_{k,\\ell}$, and prior hyperparameters $\\alpha = 1.0$, $\\beta = 1.0 \\text{ Myr}^{-1}$.\n- **Tasks**: Compute $\\mathbb{E}[T \\mid \\text{data}, M_S]$, the Bayes factor $BF_{S,I} = p(\\text{data}\\mid M_S) / p(\\text{data}\\mid M_I)$, and the posterior model probability $\\mathbb{P}(M_S \\mid \\text{data})$.\n- **Data**: Three test cases are specified with values for $K$, $L$, $[r_\\ell]$, $[n_{k,\\ell}]$, and $[x_{k,\\ell}]$.\n\nValidation of the problem proceeds.\n1.  **Scientific Grounding**: The problem is well-grounded in molecular evolution theory. The use of a Poisson process for substitutions under a strict molecular clock is a foundational concept. The application of a Gamma-Poisson conjugate model for Bayesian inference is a standard and robust statistical method. The entire setup represents a simplified but scientifically valid problem in comparative phylogeography.\n2.  **Well-Posedness**: The problem is mathematically well-posed. It provides two fully specified Bayesian models with proper priors. The quantities to be computed (posterior mean, Bayes factor, posterior probability) are standard and uniquely defined.\n3.  **Objectivity**: The statement is objective, using precise mathematical and scientific language, free of ambiguity or subjective claims.\n4.  **Completeness**: All necessary data and parameters for the computations are provided for each test case.\n5.  **Consistency and Feasibility**: The data values are consistent and fall within a realistic range for molecular sequence data. No scientific or logical contradictions are present.\n\nThe verdict is that the problem is **valid**. It is a rigorous, self-contained exercise in Bayesian modeling applied to evolutionary biology. I will now proceed with the solution.\n\nThe solution requires derivation of the posterior mean under model $M_S$ and the Bayes factor $BF_{S,I}$. This relies on the properties of the Gamma-Poisson conjugate pairing.\n\nLet the complete dataset be $X = \\{x_{k,\\ell}\\}_{k=1, \\ell=1}^{K,L}$. The likelihood of a single observation $x_{k,\\ell}$ given divergence time $T_k$ is given by the Poisson probability mass function:\n$$p(x_{k,\\ell} \\mid T_k) = \\frac{(a_{k, \\ell} T_k)^{x_{k,\\ell}} e^{-a_{k, \\ell} T_k}}{x_{k,\\ell}!}$$\nwhere $a_{k,\\ell} = 2 r_\\ell n_{k,\\ell}$.\n\nFirst, we analyze the shared-time model, $M_S$. Under this model, $T_1 = \\dots = T_K = T$, and $T \\sim \\mathrm{Gamma}(\\alpha, \\beta)$. The prior density is $p(T) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} T^{\\alpha-1} e^{-\\beta T}$. The likelihood for the entire dataset $X$ given $T$ is the product of individual Poisson probabilities due to independence of loci and taxon pairs, conditional on $T$:\n$$p(X \\mid T, M_S) = \\prod_{k=1}^K \\prod_{\\ell=1}^L p(x_{k,\\ell} \\mid T) = \\left(\\prod_{k,\\ell} \\frac{a_{k,\\ell}^{x_{k,\\ell}}}{x_{k,\\ell}!}\\right) T^{\\sum_{k,\\ell} x_{k,\\ell}} e^{-T \\sum_{k,\\ell} a_{k,\\ell}}$$\nThe posterior distribution for $T$ is found via Bayes' theorem: $p(T \\mid X, M_S) \\propto p(X \\mid T, M_S)p(T)$.\n$$p(T \\mid X, M_S) \\propto \\left( T^{\\sum_{k,\\ell} x_{k,\\ell}} e^{-T \\sum_{k,\\ell} a_{k,\\ell}} \\right) \\left( T^{\\alpha-1} e^{-\\beta T} \\right)$$\n$$p(T \\mid X, M_S) \\propto T^{(\\alpha + \\sum_{k,\\ell} x_{k,\\ell}) - 1} e^{-(\\beta + \\sum_{k,\\ell} a_{k,\\ell}) T}$$\nThis is the kernel of a Gamma distribution. Thus, the posterior is $T \\mid X, M_S \\sim \\mathrm{Gamma}(\\alpha_S, \\beta_S)$, with parameters:\n$$\\alpha_S = \\alpha + \\sum_{k=1}^K \\sum_{\\ell=1}^L x_{k,\\ell}$$\n$$\\beta_S = \\beta + \\sum_{k=1}^K \\sum_{\\ell=1}^L a_{k,\\ell}$$\nThe first required quantity is the posterior mean of $T$ under $M_S$. The mean of a $\\mathrm{Gamma}(\\alpha, \\beta)$ distribution is $\\alpha/\\beta$. Therefore:\n$$\\mathbb{E}[T \\mid X, M_S] = \\frac{\\alpha_S}{\\beta_S} = \\frac{\\alpha + \\sum_{k,\\ell} x_{k,\\ell}}{\\beta + \\sum_{k,\\ell} a_{k,\\ell}}$$\n\nNext, we derive the Bayes factor $BF_{S,I} = p(X \\mid M_S) / p(X \\mid M_I)$. This requires calculating the marginal likelihood (evidence) for each model. The marginal likelihood is $p(X \\mid M) = \\int p(X \\mid \\theta, M) p(\\theta \\mid M) d\\theta$.\nFor model $M_S$, the evidence is:\n$$p(X \\mid M_S) = \\int_0^\\infty p(X \\mid T, M_S) p(T) dT$$\nSubstituting the expressions for the likelihood and prior:\n$$p(X \\mid M_S) = \\int_0^\\infty \\left[ \\left(\\prod_{k,\\ell} \\frac{a_{k,\\ell}^{x_{k,\\ell}}}{x_{k,\\ell}!}\\right) T^{\\sum_{k,\\ell} x_{k,\\ell}} e^{-T \\sum_{k,\\ell} a_{k,\\ell}} \\right] \\left[ \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} T^{\\alpha-1} e^{-\\beta T} \\right] dT$$\n$$p(X \\mid M_S) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\prod_{k,\\ell} \\frac{a_{k,\\ell}^{x_{k,\\ell}}}{x_{k,\\ell}!}\\right) \\int_0^\\infty T^{\\alpha_S - 1} e^{-\\beta_S T} dT$$\nThe integral is the unnormalized Gamma function, which evaluates to $\\Gamma(\\alpha_S) / \\beta_S^{\\alpha_S}$.\n$$p(X \\mid M_S) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\prod_{k,\\ell} \\frac{a_{k,\\ell}^{x_{k,\\ell}}}{x_{k,\\ell}!}\\right) \\frac{\\Gamma(\\alpha_S)}{\\beta_S^{\\alpha_S}}$$\n\nFor model $M_I$, there are $K$ independent divergence times $T_1, \\dots, T_K$, each with prior $T_k \\sim \\mathrm{Gamma}(\\alpha, \\beta)$. The data for each taxon pair $k$, denoted $X_k = \\{x_{k,\\ell}\\}_{\\ell=1}^L$, depend only on $T_k$. The total evidence is the product of the evidences for each taxon pair:\n$$p(X \\mid M_I) = \\prod_{k=1}^K p(X_k \\mid M_I)$$\nFor each $k$, we compute the evidence $p(X_k \\mid M_I)$ by integrating over $T_k$. The calculation is analogous to the one for $M_S$, but restricted to the data for taxon $k$. Let $\\alpha'_k = \\alpha + \\sum_{\\ell=1}^L x_{k,\\ell}$ and $\\beta'_k = \\beta + \\sum_{\\ell=1}^L a_{k,\\ell}$.\n$$p(X_k \\mid M_I) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\prod_{\\ell=1}^L \\frac{a_{k,\\ell}^{x_{k,\\ell}}}{x_{k,\\ell}!}\\right) \\frac{\\Gamma(\\alpha'_k)}{(\\beta'_k)^{\\alpha'_k}}$$\nThe total evidence for $M_I$ is:\n$$p(X \\mid M_I) = \\prod_{k=1}^K \\left[ \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\prod_{\\ell=1}^L \\frac{a_{k,\\ell}^{x_{k,\\ell}}}{x_{k,\\ell}!}\\right) \\frac{\\Gamma(\\alpha'_k)}{(\\beta'_k)^{\\alpha'_k}} \\right] = \\left(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\right)^K \\left(\\prod_{k,\\ell} \\frac{a_{k,\\ell}^{x_{k,\\ell}}}{x_{k,\\ell}!}\\right) \\prod_{k=1}^K \\frac{\\Gamma(\\alpha'_k)}{(\\beta'_k)^{\\alpha'_k}}$$\nThe Bayes factor $BF_{S,I}$ is the ratio $p(X \\mid M_S) / p(X \\mid M_I)$. Several terms cancel:\n$$BF_{S,I} = \\frac{\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha_S)}{\\beta_S^{\\alpha_S}}}{(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)})^K \\prod_{k=1}^K \\frac{\\Gamma(\\alpha'_k)}{(\\beta'_k)^{\\alpha'_k}}} = \\left(\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\right)^{K-1} \\frac{\\Gamma(\\alpha_S)}{\\beta_S^{\\alpha_S}} \\prod_{k=1}^K \\frac{(\\beta'_k)^{\\alpha'_k}}{\\Gamma(\\alpha'_k)}$$\nFor numerical stability, we compute the natural logarithm of the Bayes factor, $\\log BF_{S,I}$. Using the log-gamma function, denoted $\\mathrm{gammaln}(z) = \\log\\Gamma(z)$:\n$$\\log BF_{S,I} = (K-1)[\\mathrm{gammaln}(\\alpha) - \\alpha\\log\\beta] + [\\mathrm{gammaln}(\\alpha_S) - \\alpha_S\\log\\beta_S] - \\sum_{k=1}^K [\\mathrm{gammaln}(\\alpha'_k) - \\alpha'_k\\log\\beta'_k]$$\nThe final requested value is $\\log_{10} BF_{S,I} = (\\log BF_{S,I}) / (\\log 10)$.\n\nFinally, the posterior probability of model $M_S$, assuming equal prior probabilities $P(M_S)=P(M_I)=0.5$, is given by:\n$$\\mathbb{P}(M_S \\mid X) = \\frac{BF_{S,I}}{BF_{S,I} + 1} = \\frac{1}{1 + 1/BF_{S,I}} = \\frac{1}{1 + e^{-\\log BF_{S,I}}}$$\nThese derived formulas are implemented to solve the given test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian phylogeographic model comparison problem for the given test suite.\n    \"\"\"\n    \n    # Define prior hyperparameters. Alpha is dimensionless, Beta is in Myr^-1.\n    alpha = 1.0\n    beta = 1.0\n\n    test_cases = [\n        # Test Case 1: General happy path\n        {\n            \"K\": 3, \"L\": 3,\n            \"r\": np.array([1.0e-3, 5.0e-4, 2.0e-3]),\n            \"n\": np.array([[1000, 1200, 800], [1000, 1200, 800], [1000, 1200, 800]]),\n            \"x\": np.array([[2, 1, 4], [3, 2, 3], [2, 1, 4]]),\n        },\n        # Test Case 2: Asynchrony\n        {\n            \"K\": 2, \"L\": 3,\n            \"r\": np.array([1.0e-3, 5.0e-4, 2.0e-3]),\n            \"n\": np.array([[1000, 1200, 800], [1000, 1200, 800]]),\n            \"x\": np.array([[1, 0, 2], [6, 4, 10]]),\n        },\n        # Test Case 3: Edge case, K=1\n        {\n            \"K\": 1, \"L\": 2,\n            \"r\": np.array([1.0e-3, 1.0e-3]),\n            \"n\": np.array([[500, 1500]]),\n            \"x\": np.array([[0, 3]]),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        K = case[\"K\"]\n        L = case[\"L\"]\n        r = case[\"r\"] # L-dimensional vector\n        n = case[\"n\"] # KxL matrix\n        x = case[\"x\"] # KxL matrix\n\n        # Calculate a_kl = 2 * r_l * n_kl.\n        # r is broadcasted across the rows of n.\n        a = 2 * r * n\n\n        # --- Calculations for Shared-Time Model (M_S) ---\n        \n        # Calculate posterior parameters for T under M_S\n        sum_x_S = np.sum(x)\n        sum_a_S = np.sum(a)\n        \n        alpha_S = alpha + sum_x_S\n        beta_S = beta + sum_a_S\n\n        # 1. Posterior mean of shared divergence time T under M_S (in Myr)\n        E_T_S = alpha_S / beta_S\n\n        # --- Calculations for Independent-Times Model (M_I) ---\n        \n        # Calculate posterior parameters for each T_k under M_I\n        # Sums are taken across loci (axis=1) for each taxon k\n        sum_x_I_k = np.sum(x, axis=1)\n        sum_a_I_k = np.sum(a, axis=1)\n\n        alpha_prime_k = alpha + sum_x_I_k\n        beta_prime_k = beta + sum_a_I_k\n\n        # --- Bayes Factor Calculation (in log space for stability) ---\n\n        # Log marginal likelihood components for M_S\n        log_evidence_S = (\n            gammaln(alpha_S) - alpha_S * np.log(beta_S)\n        )\n\n        # Sum of log marginal likelihood components for M_I\n        log_evidence_I = np.sum(\n            gammaln(alpha_prime_k) - alpha_prime_k * np.log(beta_prime_k)\n        )\n\n        # Log Bayes Factor log(P(data|M_S) / P(data|M_I))\n        # The full expression for log BF simplifies terms.\n        log_prior_norm_term = (K - 1) * (gammaln(alpha) - alpha * np.log(beta))\n        \n        log_BF_SI = log_prior_norm_term + log_evidence_S - log_evidence_I\n\n        # 2. Bayes Factor BF_SI in log10 scale\n        log10_BF_SI = log_BF_SI / np.log(10)\n        \n        # 3. Posterior Probability of M_S given equal prior model probabilities\n        # Use log_BF_SI for numerical stability: P(M_S|data) = 1 / (1 + exp(-log_BF_SI))\n        prob_MS = 1.0 / (1.0 + np.exp(-log_BF_SI))\n\n        # Append rounded results for the current test case\n        results.append([\n            round(E_T_S, 6),\n            round(prob_MS, 6),\n            round(log10_BF_SI, 6)\n        ])\n\n    # Print results in the exact specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}