{
    "hands_on_practices": [
        {
            "introduction": "Mastering the handicap principle begins with understanding its mathematical underpinnings. This first exercise walks you through the derivation of a separating equilibrium in a classic Grafen-style model, which is a cornerstone of modern signaling theory. By working through this problem , you will apply the logic of incentive compatibility to derive the specific signal level that a high-quality individual should honestly display, starting from the fundamental fitness costs and benefits of signaling.",
            "id": "2726702",
            "problem": "In the Grafen-style indicator model of signaling in evolutionary biology, individual quality is represented by a continuous trait $q \\in [\\underline{q}, \\overline{q}]$ with $\\underline{q} > 0$. An individual of quality $q$ chooses a nonnegative signal $s \\in \\mathbb{R}_{+}$ to influence a receiver’s response. The individual’s fitness payoff is $U(s,q) = B(s) - c(s,q)$, where the benefit is linear in the signal, $B(s) = a s$ with $a > 0$, and the signaling cost decreases with quality and is convex in the signal, $c(s,q) = k s^{2}/q$ with $k > 0$. Assume differentiability and interior choices of $s$ for all $q \\in [\\underline{q}, \\overline{q}]$.\n\nConsider a separating equilibrium in which types choose distinct signals according to a differentiable function $s^{*}(q)$ that is strictly increasing in $q$ and satisfies incentive compatibility: for all $q$ and all $\\tilde{q}$ in $[\\underline{q}, \\overline{q}]$, $U(s^{*}(q), q) \\geq U(s^{*}(\\tilde{q}), q)$. Starting only from these primitives and the definitions of incentive compatibility and separating equilibrium in signaling games, derive the closed-form expression for the separating equilibrium signal $s^{*}(q)$ that maximizes $U(s,q)$ subject to incentive compatibility. Express your final answer as a function of $a$, $k$, and $q$. No rounding is required, and no units are involved.",
            "solution": "The problem is valid. It presents a well-defined mathematical problem in the context of evolutionary signaling theory, based on established models and principles. All necessary information is provided, the terms are clear, and the setup is scientifically and mathematically consistent.\n\nThe objective is to derive the separating equilibrium signal function, $s^{*}(q)$, for an individual of quality $q \\in [\\underline{q}, \\overline{q}]$. The individual's fitness payoff is given by $U(s,q) = B(s) - c(s,q)$, where the benefit is $B(s) = as$ and the cost is $c(s,q) = \\frac{k s^{2}}{q}$. The parameters $a$ and $k$ are positive constants. The signal choice is $s \\in \\mathbb{R}_{+}$.\n\nA separating equilibrium is characterized by an injective signal function $s^{*}(q)$ where different types $q$ choose different signals. The problem states that $s^{*}(q)$ is differentiable and strictly increasing, which means $\\frac{ds^{*}}{dq} > 0$. The fundamental condition for such an equilibrium is incentive compatibility (IC), which states that an individual of any type $q$ must achieve a higher (or at least equal) fitness by sending its true signal $s^{*}(q)$ compared to sending any other signal $s^{*}(\\tilde{q})$ corresponding to a different type $\\tilde{q}$. Mathematically:\n$$ U(s^{*}(q), q) \\geq U(s^{*}(\\tilde{q}), q) \\quad \\forall q, \\tilde{q} \\in [\\underline{q}, \\overline{q}] $$\nThis condition implies that for any given type $q$, the function $f(\\tilde{q}) = U(s^{*}(\\tilde{q}), q)$ must have a global maximum at $\\tilde{q} = q$. Since we are given that $s^{*}(q)$ is differentiable and choices are interior, we can analyze the local behavior of this function using calculus. The first-order necessary condition for a maximum at $\\tilde{q} = q$ is that the derivative of $f(\\tilde{q})$ with respect to $\\tilde{q}$, evaluated at $\\tilde{q} = q$, must be zero.\n$$ \\frac{d}{d\\tilde{q}} U(s^{*}(\\tilde{q}), q) \\Big|_{\\tilde{q}=q} = 0 $$\nWe apply the chain rule to differentiate $U(s^{*}(\\tilde{q}), q)$ with respect to $\\tilde{q}$:\n$$ \\frac{d}{d\\tilde{q}} U(s^{*}(\\tilde{q}), q) = \\frac{\\partial U(s,q)}{\\partial s}\\Bigg|_{s=s^{*}(\\tilde{q})} \\cdot \\frac{ds^{*}(\\tilde{q})}{d\\tilde{q}} $$\nEvaluating this derivative at $\\tilde{q} = q$ gives the first-order condition:\n$$ \\frac{\\partial U(s,q)}{\\partial s}\\Bigg|_{s=s^{*}(q)} \\cdot \\frac{ds^{*}(q)}{dq} = 0 $$\nThe problem specifies a separating equilibrium where $s^{*}(q)$ is strictly increasing. This implies that the derivative $\\frac{ds^{*}(q)}{dq}$ is strictly positive. Since the product of two terms is zero and one term is strictly positive, the other term must be zero. This gives the crucial local incentive compatibility condition:\n$$ \\frac{\\partial U(s,q)}{\\partial s}\\Bigg|_{s=s^{*}(q)} = 0 $$\nThis condition states that in equilibrium, each type $q$ must choose a signal level $s^*(q)$ that locally maximizes its own fitness function. We now compute this partial derivative for the given fitness function $U(s,q) = as - \\frac{ks^{2}}{q}$:\n$$ \\frac{\\partial U}{\\partial s} = \\frac{\\partial}{\\partial s} \\left( as - \\frac{ks^{2}}{q} \\right) = a - \\frac{2ks}{q} $$\nSetting this derivative to zero at the equilibrium signal $s = s^{*}(q)$:\n$$ a - \\frac{2ks^{*}(q)}{q} = 0 $$\nThis is an algebraic equation for the function $s^{*}(q)$. We solve for $s^{*}(q)$:\n$$ a = \\frac{2ks^{*}(q)}{q} $$\n$$ s^{*}(q) = \\frac{aq}{2k} $$\nThis expression gives the candidate separating equilibrium signal as a function of quality $q$ and the a priori parameters $a$ and $k$. We must verify that this solution is consistent with the initial assumptions. The derivative with respect to $q$ is $\\frac{ds^{*}}{dq} = \\frac{a}{2k}$. Since $a > 0$ and $k > 0$, this derivative is positive, confirming that the signal function is strictly increasing, which is required for a separating equilibrium. Furthermore, the global incentive compatibility is also satisfied. The utility function $U(s,q)$ for a fixed $q$ is a downward-opening parabola in $s$, with its unique maximum at $s = \\frac{aq}{2k}$. Therefore, $U\\left(\\frac{aq}{2k}, q\\right) \\ge U(s, q)$ for any other signal $s$, including any $s = s^{*}(\\tilde{q}) = \\frac{a\\tilde{q}}{2k}$. Thus, the incentive compatibility condition holds globally.\n\nThe derived function $s^{*}(q) = \\frac{aq}{2k}$ is the unique differentiable separating equilibrium for this model.",
            "answer": "$$\\boxed{\\frac{aq}{2k}}$$"
        },
        {
            "introduction": "Building on the foundational derivation of a specific equilibrium, this next practice challenges you to think more generally about model structure. A key skill in theoretical biology is to understand which components of a model drive its outcomes. In this problem , you will analyze a generalized cost function, $c(s,q)=s^{\\gamma}/q^{\\delta}$, to determine the precise conditions on the parameters that lead to different signaling regimes, such as separating versus pooling equilibria. This exercise illuminates how the relationship between signal cost, signal intensity, and signaler quality shapes the evolution of honest communication.",
            "id": "2726653",
            "problem": "Consider an indicator signaling model of the handicap type. Individuals have a continuous quality type $q \\in (0,\\infty)$, which is private information. Each individual chooses a nonnegative signal intensity $s \\ge 0$ that affects mating success. The fitness of an individual of type $q$ who chooses signal $s$ is the difference between a mating benefit and a viability cost, given by\n$$\nW(s,q) \\equiv B(s) - c(s,q) = b\\, s - \\frac{s^{\\gamma}}{q^{\\delta}},\n$$\nwhere $b>0$ is a constant parameter capturing how strongly receivers reward the signal, and the signal cost is $c(s,q)=s^{\\gamma}/q^{\\delta}$ with $\\gamma>0$ and $\\delta \\in \\mathbb{R}$. Assume that the ecological and demographic conditions make $b$ and the functional forms stable, and that in equilibrium each type $q$ best responds myopically to maximize $W(s,q)$, independently of other types. Assume full support of $q$ on any compact interval of positive $q$ values, so that monotonicity considerations are meaningful.\n\nYou are to study when the resulting equilibrium signaling schedule $s^{\\ast}(q)$ is separating versus pooling in the sense of indicator mechanisms. Use only definitions and principles from optimization under natural selection, namely that at an interior optimum the first-order condition holds and the second-order condition determines local maximality, and that a separating handicap mechanism requires that higher-quality types face lower marginal cost of signaling at any given $s$ (single-crossing), i.e., $\\partial^{2} c / \\partial s \\partial q  0$.\n\nProceed as follows:\n1. Derive the interior best response $s^{\\ast}(q)$ when it exists and determine the parameter restrictions on $(\\gamma,\\delta)$ under which this interior solution is locally stable and finite for all $q0$.\n2. Define the elasticity of the equilibrium signal with respect to quality, $E \\equiv \\frac{d \\ln s^{\\ast}(q)}{d \\ln q}$, and show that it is constant in $q$ whenever an interior solution exists.\n3. Using your expression for $E$, provide the parameter regions for which the equilibrium is separating in the indicator sense (monotone increasing in $q$) versus pooling (independent of $q$). You may briefly note what happens in any remaining parameter regions, but your classification must be justified from first principles.\n\nReport, as your final answer, the analytic expression for the elasticity $E$ in terms of $\\gamma$ and $\\delta$. No numerical evaluation is required and no rounding applies. Do not include units.",
            "solution": "The problem proposed is a standard exercise in signaling theory within mathematical evolutionary biology. It is scientifically grounded, well-posed, objective, and contains all necessary information for its resolution. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe objective is to analyze the equilibrium signaling strategy $s^{\\ast}(q)$ for an individual of quality $q$ with the fitness function $W(s,q) = b s - s^{\\gamma}/q^{\\delta}$. We assume individuals choose their signal level $s \\ge 0$ to maximize this fitness function. The parameters are given as $b  0$, $\\gamma  0$, and $\\delta \\in \\mathbb{R}$.\n\n**1. Derivation of the Interior Best Response and Stability Conditions**\n\nTo find the optimal signaling level $s^{\\ast}(q)$, we differentiate the fitness function $W(s,q)$ with respect to $s$ and set the result to zero. This is the first-order condition (FOC) for an interior maximum.\n$$\n\\frac{\\partial W}{\\partial s} = \\frac{\\partial}{\\partial s} \\left( b s - \\frac{s^{\\gamma}}{q^{\\delta}} \\right) = b - \\frac{\\gamma s^{\\gamma-1}}{q^{\\delta}}.\n$$\nSetting the FOC to zero gives:\n$$\nb - \\frac{\\gamma s^{\\gamma-1}}{q^{\\delta}} = 0 \\implies b = \\frac{\\gamma s^{\\gamma-1}}{q^{\\delta}}.\n$$\nWe now solve for the candidate signaling level, which we denote $s^{\\ast}(q)$:\n$$\ns^{\\gamma-1} = \\frac{b q^{\\delta}}{\\gamma}.\n$$\nFor this equation to yield a unique, positive solution for $s$, the exponent $\\gamma-1$ must be non-zero. If $\\gamma = 1$, the fitness becomes $W(s,q) = s(b - q^{-\\delta})$, which is linear in $s$. The optimal strategy would be $s=0$ if $b  q^{-\\delta}$ or $s \\to \\infty$ if $b  q^{-\\delta}$. No stable interior optimum exists for $\\gamma=1$. Thus, we must have $\\gamma \\neq 1$.\n\nAssuming $\\gamma \\neq 1$, we can write:\n$$\ns^{\\ast}(q) = \\left( \\frac{b q^{\\delta}}{\\gamma} \\right)^{\\frac{1}{\\gamma-1}}.\n$$\nFor this to be a local maximum, the second-order condition (SOC) must be satisfied, namely $\\frac{\\partial^2 W}{\\partial s^2}  0$. We compute the second derivative:\n$$\n\\frac{\\partial^2 W}{\\partial s^2} = -\\frac{\\gamma (\\gamma-1) s^{\\gamma-2}}{q^{\\delta}}.\n$$\nFor an interior solution $s^{\\ast}(q)  0$, the signs of $b$, $\\gamma$, and $q$ are positive. The term $s^{\\gamma-2}$ is positive. The term $q^{\\delta}$ is positive. Therefore, the sign of the second derivative is determined by the sign of $-\\gamma(\\gamma-1)$. Since $\\gamma  0$ is given, the condition $\\frac{\\partial^2 W}{\\partial s^2}  0$ simplifies to $\\gamma-1  0$, or $\\gamma  1$.\n\nIf $0  \\gamma  1$, then $\\gamma-1  0$, making $\\frac{\\partial^2 W}{\\partial s^2}  0$. This corresponds to a local minimum, not a maximum. In this case, since the exponent $\\gamma$ on the cost term is less than $1$, the linear benefit term $bs$ dominates for large $s$, and fitness $W(s,q)$ would increase without bound as $s \\to \\infty$. This does not yield a finite solution.\n\nTherefore, for a finite, interior, locally stable signaling equilibrium to exist for all $q  0$, we require the parameter restriction $\\gamma  1$. The parameter $\\delta$ does not influence the existence or stability of the optimum, only its value as a function of $q$.\n\n**2. Elasticity of the Equilibrium Signal**\n\nThe elasticity of the equilibrium signal $s^{\\ast}(q)$ with respect to quality $q$ is defined as $E \\equiv \\frac{d \\ln s^{\\ast}(q)}{d \\ln q}$. We first express $\\ln s^{\\ast}(q)$ using the result from part 1, under the condition $\\gamma  1$:\n$$\ns^{\\ast}(q) = \\left( \\frac{b}{\\gamma} \\right)^{\\frac{1}{\\gamma-1}} q^{\\frac{\\delta}{\\gamma-1}}.\n$$\nTaking the natural logarithm of both sides:\n$$\n\\ln s^{\\ast}(q) = \\ln \\left[ \\left( \\frac{b}{\\gamma} \\right)^{\\frac{1}{\\gamma-1}} q^{\\frac{\\delta}{\\gamma-1}} \\right] = \\frac{1}{\\gamma-1} \\ln\\left(\\frac{b}{\\gamma}\\right) + \\frac{\\delta}{\\gamma-1} \\ln q.\n$$\nThe elasticity $E$ is the derivative of $\\ln s^{\\ast}(q)$ with respect to $\\ln q$:\n$$\nE = \\frac{d}{d \\ln q} \\left[ \\frac{1}{\\gamma-1} \\ln\\left(\\frac{b}{\\gamma}\\right) + \\frac{\\delta}{\\gamma-1} \\ln q \\right].\n$$\nSince the first term is a constant with respect to $q$, the derivative is:\n$$\nE = \\frac{\\delta}{\\gamma-1}.\n$$\nThis expression depends only on the parameters $\\gamma$ and $\\delta$, and is therefore constant with respect to quality $q$.\n\n**3. Classification of Equilibrium**\n\nThe nature of the signaling equilibrium (separating versus pooling) depends on how the equilibrium signal $s^{\\ast}(q)$ changes with quality $q$. This is determined by the sign of the derivative $\\frac{d s^{\\ast}(q)}{d q}$, which has the same sign as the elasticity $E$, since $E = \\frac{q}{s^{\\ast}(q)} \\frac{d s^{\\ast}(q)}{d q}$ and both $q$ and $s^{\\ast}(q)$ are positive. We require $\\gamma  1$, so the denominator $\\gamma-1$ is positive. The sign of $E$ is therefore determined solely by the sign of $\\delta$.\n\n- **Separating Equilibrium**: This occurs when different types $q$ choose different signals. In particular, an indicator mechanism where higher quality is associated with a stronger signal requires $s^{\\ast}(q)$ to be a strictly increasing function of $q$. This means $\\frac{d s^{\\ast}}{d q}  0$, which implies $E  0$.\n  $$\n  E = \\frac{\\delta}{\\gamma-1}  0 \\implies \\delta  0.\n  $$\n  This condition corresponds to the classic handicap principle. The single-crossing property, stating that the marginal cost of signaling decreases with quality, is $\\frac{\\partial^2 c}{\\partial s \\partial q}  0$. For the cost function $c(s,q) = s^{\\gamma}q^{-\\delta}$, we have $\\frac{\\partial c}{\\partial s} = \\gamma s^{\\gamma-1}q^{-\\delta}$ and $\\frac{\\partial^2 c}{\\partial s \\partial q} = -\\delta \\gamma s^{\\gamma-1}q^{-\\delta-1}$. For this to be negative, we need $\\delta  0$. Thus, a separating equilibrium consistent with the handicap principle exists if and only if $\\gamma  1$ and $\\delta  0$.\n\n- **Pooling Equilibrium**: This occurs when all types $q$ choose the same signal. This requires $s^{\\ast}(q)$ to be a constant, independent of $q$. This means $\\frac{d s^{\\ast}}{d q} = 0$, which implies $E=0$.\n  $$\n  E = \\frac{\\delta}{\\gamma-1} = 0 \\implies \\delta = 0.\n  $$\n  If $\\delta = 0$, the cost function $c(s,q) = s^{\\gamma}$ is independent of quality. All types face identical optimization problems and thus choose the same signal level $s^{\\ast} = (b/\\gamma)^{1/(\\gamma-1)}$.\n\n- **Other Behavior**: If $\\delta  0$, then $E = \\frac{\\delta}{\\gamma-1}  0$. This implies $\\frac{d s^{\\ast}}{d q}  0$, meaning that higher-quality individuals signal *less* intensively. This is a form of separating equilibrium, but it violates the standard handicap principle as the marginal cost of signaling *increases* with quality ($\\frac{\\partial^2 c}{\\partial s \\partial q}  0$). This scenario is sometimes referred to as \"counter-signaling\".\n\nThe final answer requested is the analytical expression for the elasticity $E$. Based on the derivation above, this is a constant function of the model's cost parameters.",
            "answer": "$$\n\\boxed{\\frac{\\delta}{\\gamma-1}}\n$$"
        },
        {
            "introduction": "Theoretical models are powerful, but they must ultimately be tested against empirical data. This final practice bridges the gap between abstract theory and data analysis, a critical skill for any evolutionary biologist. Using the Akaike Information Criterion ($AIC$), a standard tool for model selection, you will analyze simulated datasets to determine whether a \"separating\" or a \"pooling\" model best explains the observed relationship between signal and quality . This hands-on computational exercise demonstrates how to statistically evaluate the predictions of signaling theory.",
            "id": "2726630",
            "problem": "Consider a population in which individuals of latent genetic quality emit a costly signal. Under the indicator-mechanism perspective relevant to the handicap principle, high-quality individuals can afford higher signals. We observe data pairs $(s_i, q_i)$ for $i \\in \\{1,\\dots,n\\}$, where $s_i \\in \\mathbb{R}$ is a scalar signal magnitude and $q_i \\in \\{0,1\\}$ is an observed quality label ($q_i = 1$ for high quality, $q_i = 0$ for low quality). You are to perform likelihood-based model comparison between two signaling models:\n\n- Separating model: signals are informative of quality, modeled as conditionally Gaussian with quality-specific means and a shared variance. Formally, $s_i \\mid q_i = j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$ with $j \\in \\{0,1\\}$.\n- Pooling model: signals are uninformative of quality, modeled as a single Gaussian distribution irrespective of quality. Formally, $s_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ independently of $q_i$.\n\nYour task is to implement maximum likelihood estimation and compare the models using Akaike Information Criterion (AIC).\n\nFundamental definitions to be used:\n- The Gaussian (normal) density for $x \\in \\mathbb{R}$ is $f(x \\mid \\mu, \\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\!\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right)$ for $\\sigma^2  0$.\n- The log-likelihood for independent observations is the sum of the log-densities.\n- The maximum likelihood estimators (derived by setting derivatives of the log-likelihood to zero) under the pooling model are the sample mean $\\hat{\\mu} = \\dfrac{1}{n} \\sum_{i=1}^n s_i$ and the variance $\\hat{\\sigma}^2 = \\dfrac{1}{n} \\sum_{i=1}^n (s_i - \\hat{\\mu})^2$. Under the separating model, the means are group-specific sample means $\\hat{\\mu}_j = \\dfrac{1}{n_j} \\sum_{i: q_i = j} s_i$ for $j \\in \\{0,1\\}$, where $n_j = \\sum_{i=1}^n \\mathbb{I}[q_i = j]$, and the shared variance is $\\hat{\\sigma}^2 = \\dfrac{1}{n} \\sum_{j \\in \\{0,1\\}} \\sum_{i: q_i = j} (s_i - \\hat{\\mu}_j)^2$.\n- The maximized log-likelihood for a Gaussian model with unknown mean(s) and variance (evaluated at the maximum likelihood estimates) can be expressed as\n$$\n\\log L = -\\dfrac{n}{2}\\left( \\log(2\\pi) + 1 + \\log(\\hat{\\sigma}^2) \\right),\n$$\nwhere $\\hat{\\sigma}^2$ is the corresponding maximum likelihood variance ($\\dfrac{1}{n}$ times the sum of squared residuals).\n- The Akaike Information Criterion is $\\mathrm{AIC} = 2k - 2 \\log L$, where $k$ is the number of free parameters. For the pooling model, $k = 2$ (one mean and one variance). For the separating model with two means and one shared variance, $k = 3$.\n\nImportant implementation details:\n- If one quality class is absent (i.e., $n_j = 0$ for some $j$), treat the separating model as identical to the pooling model for that dataset with $k = 2$ and $\\hat{\\mu}$ equal to the overall sample mean, because the absent-class mean is not identifiable.\n- To ensure numerical stability, if a computed $\\hat{\\sigma}^2$ equals $0$, replace it by $\\varepsilon = 10^{-12}$ before evaluating the log-likelihood.\n- When comparing AIC values, if they are equal within an absolute tolerance $\\tau = 10^{-9}$, break ties in favor of the pooling model.\n\nInput specification for testing:\nYou will hard-code the following three datasets (each is a pair of aligned arrays $(\\mathbf{s}, \\mathbf{q})$) directly into your program.\n\n- Dataset $\\#1$ (a case favoring separating): $n = 10$, with aligned observations\n  - $\\mathbf{q} = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$,\n  - $\\mathbf{s} = [\\,9.8,\\,10.5,\\,10.2,\\,11.0,\\,9.7,\\,4.8,\\,5.1,\\,5.5,\\,6.0,\\,4.9\\,]$.\n- Dataset $\\#2$ (a case favoring pooling): $n = 10$, with aligned observations\n  - $\\mathbf{q} = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]$,\n  - $\\mathbf{s} = [\\,6.8,\\,7.1,\\,7.0,\\,7.3,\\,6.9,\\,7.2,\\,6.7,\\,7.4,\\,6.8,\\,7.0\\,]$.\n- Dataset $\\#3$ (an imbalanced edge case): $n = 10$, with aligned observations\n  - $\\mathbf{q} = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,0\\,]$,\n  - $\\mathbf{s} = [\\,7.9,\\,8.1,\\,8.2,\\,8.0,\\,7.8,\\,8.3,\\,8.1,\\,8.0,\\,7.7,\\,8.05\\,]$.\n\nTask:\n- For each dataset, compute the maximum likelihood estimates under both models, compute the corresponding maximized log-likelihoods and AIC values, and decide which model better fits the data by the AIC rule (lower AIC is better; break ties within tolerance $\\tau$ in favor of pooling).\n- Express the decision for each dataset as an integer: output $1$ if the separating model is better and $0$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the decisions for the three datasets as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$. Each result must be an integer ($0$ or $1$).",
            "solution": "The problem requires a quantitative comparison of two statistical models for signaling behavior in a biological population, based on the principle of maximum likelihood and the Akaike Information Criterion (AIC). The problem is self-contained, scientifically grounded in statistical theory, and well-posed. We shall proceed with the solution.\n\nThe core of the task is to evaluate, for each provided dataset, which of two models provides a more parsimonious fit to the observed data pairs $(s_i, q_i)$, where $s_i$ is a continuous signal and $q_i$ is a binary quality label.\n\nThe two competing models are:\n1.  **Pooling Model**: This model assumes the signal $s_i$ is uninformative of quality $q_i$. The signals are modeled as independent draws from a single Gaussian distribution, $s_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$. This model has $k_{\\text{pool}} = 2$ free parameters: the mean $\\mu$ and the variance $\\sigma^2$.\n\n2.  **Separating Model**: This model, consistent with the handicap principle, assumes the signal is informative. Signals are drawn from one of two Gaussian distributions, conditional on quality, $s_i \\mid q_i=j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$ for $j \\in \\{0, 1\\}$. Critically, the variance $\\sigma^2$ is assumed to be shared across the two quality groups. This model has $k_{\\text{sep}} = 3$ free parameters: the two means $\\mu_0$ and $\\mu_1$, and the shared variance $\\sigma^2$.\n\nFor each model and dataset, we first find the maximum likelihood estimates (MLEs) for its parameters. The problem provides the standard closed-form expressions for these MLEs.\n\nFor the **pooling model**, given a sample of $n$ signals $\\{s_1, \\dots, s_n\\}$:\n-   The MLE for the mean is the overall sample mean:\n    $$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} s_i $$\n-   The MLE for the variance is the sample variance (with $n$ in the denominator):\n    $$ \\hat{\\sigma}^2_{\\text{pool}} = \\frac{1}{n} \\sum_{i=1}^{n} (s_i - \\hat{\\mu})^2 $$\n\nFor the **separating model**, we first partition the data into two subsamples based on the quality label $q_i$. Let $n_j = \\sum_{i=1}^{n} \\mathbb{I}[q_i=j]$ be the number of individuals in group $j \\in \\{0, 1\\}$.\n-   The MLEs for the group-specific means are the subsample means:\n    $$ \\hat{\\mu}_j = \\frac{1}{n_j} \\sum_{i: q_i=j} s_i \\quad \\text{for } j \\in \\{0, 1\\} $$\n-   The MLE for the shared variance is the pooled variance, calculated as the total sum of squared residuals divided by the total sample size $n$:\n    $$ \\hat{\\sigma}^2_{\\text{sep}} = \\frac{1}{n} \\left( \\sum_{i: q_i=0} (s_i - \\hat{\\mu}_0)^2 + \\sum_{i: q_i=1} (s_i - \\hat{\\mu}_1)^2 \\right) $$\n\nOnce the MLE for the variance, $\\hat{\\sigma}^2$, is computed for a given model, the maximized log-likelihood, $\\log L$, is calculated using the provided formula, which applies to Gaussian models where both mean(s) and variance are estimated:\n$$ \\log L = -\\frac{n}{2} \\left( \\log(2\\pi) + 1 + \\log(\\hat{\\sigma}^2) \\right) $$\nA numerical stability precaution is taken: if any calculated $\\hat{\\sigma}^2$ is zero, it is replaced with $\\varepsilon = 10^{-12}$ before taking the logarithm.\n\nWith the maximized log-likelihood $\\log L$ and the number of parameters $k$ for each model, we compute the AIC:\n$$ \\mathrm{AIC} = 2k - 2 \\log L $$\nThe model with the lower AIC value is considered a better fit to the data, as it provides a better balance between goodness-of-fit (high $\\log L$) and model complexity (low $k$).\n\nA crucial edge case arises if a dataset contains observations from only one quality class (i.e., $n_0 = 0$ or $n_1 = 0$). In this situation, the parameters for the absent class ($\\mu_j$) are not identifiable. The separating model cannot be distinguished from the pooling model. As per the problem specification, we treat the separating model as identical to the pooling model: we assign it the same log-likelihood, and notably, the same number of parameters, $k_{\\text{sep}} = 2$.\n\nThe final decision rule is as follows: the separating model is chosen (output $1$) if its AIC is strictly smaller than the pooling model's AIC, considering a numerical tolerance $\\tau = 10^{-9}$. That is, if $\\mathrm{AIC}_{\\text{sep}}  \\mathrm{AIC}_{\\text{pool}} - \\tau$. Otherwise, the pooling model is chosen (output $0$), which includes the case of near-equal AIC values.\n\nThe algorithm proceeds as follows for each dataset:\n1.  Compute the parameters $k_{\\text{pool}}$, $\\hat{\\sigma}^2_{\\text{pool}}$, $\\log L_{\\text{pool}}$, and $\\mathrm{AIC}_{\\text{pool}}$ for the pooling model.\n2.  Check for the class imbalance edge case. If one class is absent, set $k_{\\text{sep}} = k_{\\text{pool}}$ and $\\mathrm{AIC}_{\\text{sep}} = \\mathrm{AIC}_{\\text{pool}}$.\n3.  If both classes are present, compute the parameters $k_{\\text{sep}}$, $\\hat{\\sigma}^2_{\\text{sep}}$, $\\log L_{\\text{sep}}$, and $\\mathrm{AIC}_{\\text{sep}}$ for the separating model.\n4.  Compare $\\mathrm{AIC}_{\\text{sep}}$ and $\\mathrm{AIC}_{\\text{pool}}$ using the specified tolerance and decision rule to determine the result ($0$ or $1$).\nThis procedure is applied to all three datasets to produce the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for the three specified datasets.\n    \"\"\"\n    \n    # Define the datasets as per the problem statement.\n    test_cases = [\n        # Dataset #1: Expected to favor the separating model.\n        {\n            \"s\": np.array([9.8, 10.5, 10.2, 11.0, 9.7, 4.8, 5.1, 5.5, 6.0, 4.9]),\n            \"q\": np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n        },\n        # Dataset #2: Expected to favor the pooling model.\n        {\n            \"s\": np.array([6.8, 7.1, 7.0, 7.3, 6.9, 7.2, 6.7, 7.4, 6.8, 7.0]),\n            \"q\": np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n        },\n        # Dataset #3: Imbalanced edge case.\n        {\n            \"s\": np.array([7.9, 8.1, 8.2, 8.0, 7.8, 8.3, 8.1, 8.0, 7.7, 8.05]),\n            \"q\": np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        s_data = case[\"s\"]\n        q_data = case[\"q\"]\n        result = _compare_models(s_data, q_data)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _calculate_aic(n, k, sigma2_hat):\n    \"\"\"\n    Calculates the Akaike Information Criterion (AIC).\n\n    Args:\n        n (int): Number of observations.\n        k (int): Number of free parameters in the model.\n        sigma2_hat (float): Maximum likelihood estimate of the variance.\n\n    Returns:\n        float: The calculated AIC value.\n    \"\"\"\n    # Numerical stability constant as specified.\n    epsilon = 1e-12\n    if sigma2_hat = 0.0:\n        sigma2_hat = epsilon\n\n    # Maximized log-likelihood for a Gaussian model.\n    # logL = -n/2 * (log(2*pi) + 1 + log(sigma^2_hat))\n    logL = -n / 2.0 * (np.log(2 * np.pi) + 1.0 + np.log(sigma2_hat))\n    \n    # AIC = 2k - 2*logL\n    aic = 2 * k - 2 * logL\n    return aic\n\ndef _compare_models(s, q):\n    \"\"\"\n    Performs AIC-based model comparison between a pooling and a separating model.\n\n    Args:\n        s (np.ndarray): Array of signal magnitudes.\n        q (np.ndarray): Array of quality labels (0 or 1).\n\n    Returns:\n        int: 1 if the separating model is better, 0 otherwise.\n    \"\"\"\n    n = len(s)\n    tau = 1e-9  # Tie-breaking tolerance\n\n    # --- Pooling Model ---\n    # Parameters: mu, sigma^2 (k=2)\n    k_pool = 2\n    # MLE for variance is the population variance (ddof=0).\n    sigma2_hat_pool = np.var(s)\n    aic_pool = _calculate_aic(n, k_pool, sigma2_hat_pool)\n\n    # --- Separating Model ---\n    # Parameters: mu_0, mu_1, sigma^2 (k=3)\n    s_group0 = s[q == 0]\n    s_group1 = s[q == 1]\n    \n    n0 = len(s_group0)\n    n1 = len(s_group1)\n\n    # Handle edge case where one class is absent.\n    if n0 == 0 or n1 == 0:\n        # Separating model is identical to pooling model.\n        k_sep = 2\n        aic_sep = _calculate_aic(n, k_sep, sigma2_hat_pool)\n    else:\n        # Standard case with both classes present.\n        k_sep = 3\n        \n        # MLEs for group means.\n        mu0_hat = np.mean(s_group0)\n        mu1_hat = np.mean(s_group1)\n        \n        # Sum of squared residuals for each group.\n        ssr0 = np.sum((s_group0 - mu0_hat)**2)\n        ssr1 = np.sum((s_group1 - mu1_hat)**2)\n        \n        # MLE for shared variance.\n        sigma2_hat_sep = (ssr0 + ssr1) / n\n        aic_sep = _calculate_aic(n, k_sep, sigma2_hat_sep)\n\n    # --- Comparison ---\n    # Choose separating model if its AIC is strictly lower, considering tolerance.\n    if aic_sep  aic_pool - tau:\n        return 1\n    else: # Favor pooling model in case of tie or if it's better.\n        return 0\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}