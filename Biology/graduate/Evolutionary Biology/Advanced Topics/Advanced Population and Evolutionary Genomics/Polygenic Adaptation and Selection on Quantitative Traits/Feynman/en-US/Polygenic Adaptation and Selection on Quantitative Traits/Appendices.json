{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of quantitative genetics is predicting how the mean of a polygenic trait responds to selection over time. This exercise leverages the foundational Lande-Arnold framework, where the evolutionary response vector $\\Delta \\overline{\\mathbf{z}}$ is the product of the additive genetic variance-covariance matrix $\\mathbf{G}$ and the selection gradient vector $\\boldsymbol{\\beta}$ . By implementing a simulation of a trait evolving under stabilizing selection, you will gain a dynamic understanding of how factors like genetic correlations and a moving environmental optimum shape the trajectory of adaptation.",
            "id": "2744372",
            "problem": "You are to write a complete program that computes the deterministic evolutionary response of the mean of a quantitative polygenic trait vector under stabilizing selection with a potentially moving optimum, using the principle that directional selection arises from the gradient of log fitness with respect to the mean trait and the standard quantitative-genetic response to selection under linkage equilibrium.\n\nStarting base:\n- Use the following foundational elements only:\n  1. The definition of Malthusian fitness as the natural logarithm of absolute fitness, i.e., if absolute fitness is $W(\\mathbf{z})$, then Malthusian fitness is $m(\\mathbf{z}) = \\ln W(\\mathbf{z})$.\n  2. The definition of the selection gradient as the gradient of Malthusian fitness with respect to the trait vector, i.e., $\\boldsymbol{\\beta}(\\mathbf{z}) = \\nabla_{\\mathbf{z}} m(\\mathbf{z})$.\n  3. The statement that the change in trait mean over one generation under standard quantitative-genetic assumptions (weak selection, additivity, linkage equilibrium (LE), and approximately constant additive genetic variance-covariance) is proportional to the selection gradient via the additive genetic variance-covariance matrix. You must derive the explicit proportionality from these foundations.\n  4. A widely used and empirically grounded form of stabilizing selection on a multivariate quantitative trait with trait vector $\\mathbf{z} \\in \\mathbb{R}^n$ given by an absolute fitness function of the form\n     $$W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)\\right),$$\n     where $\\boldsymbol{\\theta}_t \\in \\mathbb{R}^n$ is the optimum at generation $t$ and $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive-definite with entries specifying the strength and correlational structure of stabilizing selection.\n- You must use the above elements as the fundamental base to derive a deterministic, discrete-time update for the mean trait vector $\\overline{\\mathbf{z}}_t$ over $T$ generations, given a constant additive genetic variance-covariance matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times n}$, a linear optimum path $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$ for a fixed velocity $\\mathbf{v} \\in \\mathbb{R}^n$, and initial mean $\\overline{\\mathbf{z}}_0$.\n\nTask:\n- Derive the explicit deterministic update for $\\overline{\\mathbf{z}}_{t+1}$ in terms of $\\overline{\\mathbf{z}}_t$, $\\boldsymbol{\\theta}_t$, $\\boldsymbol{\\Omega}$, and $\\mathbf{G}$ by combining the above base elements. Implement that update and iterate it for $T$ generations to obtain $\\overline{\\mathbf{z}}_T$.\n- You may assume $\\boldsymbol{\\Omega}$ is symmetric positive-definite for all test cases, so $\\boldsymbol{\\Omega}^{-1}$ exists. You may not assume $\\mathbf{G}$ is invertible.\n\nInput and test suite specification:\n- There is no external input; hard-code the following test suite in your program. For each test case $i$, you are given trait dimension $n$, initial mean $\\overline{\\mathbf{z}}_0^{(i)}$, additive genetic variance-covariance $\\mathbf{G}^{(i)}$, stabilizing selection matrix $\\boldsymbol{\\Omega}^{(i)}$, initial optimum $\\boldsymbol{\\theta}_0^{(i)}$, optimum velocity $\\mathbf{v}^{(i)}$, and number of generations $T^{(i)}$. For each test case, compute $\\overline{\\mathbf{z}}_{T^{(i)}}^{(i)}$.\n\nTest cases:\n1. One-dimensional, stationary optimum:\n   - $n = 1$\n   - $\\overline{\\mathbf{z}}_0^{(1)} = (0.0)$\n   - $\\mathbf{G}^{(1)} = \\begin{bmatrix} 0.2 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(1)} = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(1)} = (1.5)$\n   - $\\mathbf{v}^{(1)} = (0.0)$\n   - $T^{(1)} = 5$\n\n2. Two-dimensional, correlated $\\mathbf{G}$, stationary optimum:\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(2)} = (0.0, 0.0)$\n   - $\\mathbf{G}^{(2)} = \\begin{bmatrix} 0.1 & 0.05 \\\\ 0.05 & 0.2 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(2)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.5 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(2)} = (1.0, -0.5)$\n   - $\\mathbf{v}^{(2)} = (0.0, 0.0)$\n   - $T^{(2)} = 10$\n\n3. One-dimensional, zero additive variance (no response):\n   - $n = 1$\n   - $\\overline{\\mathbf{z}}_0^{(3)} = (0.0)$\n   - $\\mathbf{G}^{(3)} = \\begin{bmatrix} 0.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(3)} = \\begin{bmatrix} 0.5 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(3)} = (1.0)$\n   - $\\mathbf{v}^{(3)} = (0.0)$\n   - $T^{(3)} = 7$\n\n4. Two-dimensional, weak stabilizing selection:\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(4)} = (0.2, -0.2)$\n   - $\\mathbf{G}^{(4)} = \\begin{bmatrix} 0.05 & 0.0 \\\\ 0.0 & 0.05 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(4)} = \\begin{bmatrix} 100.0 & 0.0 \\\\ 0.0 & 100.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(4)} = (1.0, 1.0)$\n   - $\\mathbf{v}^{(4)} = (0.0, 0.0)$\n   - $T^{(4)} = 10$\n\n5. Two-dimensional, moving optimum with correlational stabilizing selection:\n   - $n = 2$\n   - $\\overline{\\mathbf{z}}_0^{(5)} = (0.0, 0.0)$\n   - $\\mathbf{G}^{(5)} = \\begin{bmatrix} 0.05 & 0.0 \\\\ 0.0 & 0.05 \\end{bmatrix}$\n   - $\\boldsymbol{\\Omega}^{(5)} = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 1.0 \\end{bmatrix}$\n   - $\\boldsymbol{\\theta}_0^{(5)} = (0.0, 0.0)$\n   - $\\mathbf{v}^{(5)} = (0.1, -0.05)$\n   - $T^{(5)} = 20$\n\nOutput requirements:\n- For each test case $i$, compute $\\overline{\\mathbf{z}}_{T^{(i)}}^{(i)}$.\n- Round every component of every resulting vector to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of vectors, each vector enclosed in square brackets, and the whole collection enclosed in square brackets. For example, the formatting should be like $[\\,[a_{1,1},a_{1,2}],\\,[a_{2,1},a_{2,2}]\\,]$ but with the actual numbers instead of symbols, with no spaces.\n- There are no physical units involved.\n\nScientific realism constraints:\n- Assume weak selection and additivity with approximately constant $\\mathbf{G}$ over the simulated time, and linkage equilibrium (LE). Ensure that $\\boldsymbol{\\Omega}$ is symmetric positive-definite in all computations so that $\\boldsymbol{\\Omega}^{-1}$ exists.\n\nYour program must be self-contained and must not read any external input.",
            "solution": "The problem presented is a well-posed and scientifically grounded exercise in evolutionary quantitative genetics. It asks for the derivation and implementation of a discrete-time model for the evolution of a multivariate polygenic trait under stabilizing selection with a linearly moving optimum. The problem statement provides all necessary components and adheres to established principles of the field. It is therefore deemed valid.\n\nThe objective is to derive the deterministic, discrete-time update equation for the mean trait vector, $\\overline{\\mathbf{z}}_t$, and to iterate this equation for a specified number of generations, $T$, to find the final mean trait vector, $\\overline{\\mathbf{z}}_T$. The derivation will proceed from the foundational elements provided.\n\nFirst, we are given the absolute fitness function for an individual with trait vector $\\mathbf{z} \\in \\mathbb{R}^n$ at generation $t$:\n$$W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\exp\\!\\left(-\\tfrac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)\\right)$$\nHere, $\\boldsymbol{\\theta}_t$ is the optimal trait vector at generation $t$, and $\\boldsymbol{\\Omega}$ is a symmetric positive-definite matrix that describes the strength of stabilizing selection. A larger $\\boldsymbol{\\Omega}$ corresponds to weaker selection.\n\nThe Malthusian fitness, $m(\\mathbf{z})$, is the natural logarithm of the absolute fitness, $W(\\mathbf{z})$.\n$$m(\\mathbf{z}; \\boldsymbol{\\theta}_t) = \\ln W(\\mathbf{z}; \\boldsymbol{\\theta}_t) = -\\frac{1}{2}(\\mathbf{z}-\\boldsymbol{\\theta}_t)^{\\mathsf{T}} \\boldsymbol{\\Omega}^{-1} (\\mathbf{z}-\\boldsymbol{\\theta}_t)$$\nThis function is a quadratic surface, which simplifies subsequent analysis.\n\nNext, we define the selection gradient, which measures the directional selective force on a trait. The gradient of Malthusian fitness with respect to the trait vector $\\mathbf{z}$ is:\n$$\\boldsymbol{\\beta}(\\mathbf{z}) = \\nabla_{\\mathbf{z}} m(\\mathbf{z}; \\boldsymbol{\\theta}_t)$$\nUsing the standard rule for differentiating a quadratic form, $\\nabla_{\\mathbf{x}} (\\mathbf{x}-\\mathbf{c})^{\\mathsf{T}}\\mathbf{A}(\\mathbf{x}-\\mathbf{c}) = 2\\mathbf{A}(\\mathbf{x}-\\mathbf{c})$ for a symmetric matrix $\\mathbf{A}$, and noting that $\\boldsymbol{\\Omega}^{-1}$ is symmetric as $\\boldsymbol{\\Omega}$ is symmetric, we obtain:\n$$\\boldsymbol{\\beta}(\\mathbf{z}) = -\\frac{1}{2} \\cdot 2\\boldsymbol{\\Omega}^{-1}(\\mathbf{z}-\\boldsymbol{\\theta}_t) = -\\boldsymbol{\\Omega}^{-1}(\\mathbf{z}-\\boldsymbol{\\theta}_t)$$\n\nThe evolutionary response of the population mean trait, $\\Delta \\overline{\\mathbf{z}}_t = \\overline{\\mathbf{z}}_{t+1} - \\overline{\\mathbf{z}}_t$, is governed by the multivariate breeder's equation. This fundamental result of quantitative genetics states that the change in the mean is the product of the additive genetic variance-covariance matrix, $\\mathbf{G}$, and the selection gradient acting on the population mean, $\\boldsymbol{\\beta}_t$:\n$$\\Delta \\overline{\\mathbf{z}}_t = \\mathbf{G} \\boldsymbol{\\beta}_t$$\nThe selection gradient on the mean, $\\boldsymbol{\\beta}_t$, is formally the gradient of the mean log fitness with respect to the mean trait, $\\boldsymbol{\\beta}_t = \\nabla_{\\overline{\\mathbf{z}}} \\overline{m(\\mathbf{z})}$. Under the standard assumption of a symmetric (e.g., multivariate normal) distribution of phenotypes around the mean $\\overline{\\mathbf{z}}_t$, and for the quadratic fitness surface given, this population-level gradient is precisely the individual-level gradient evaluated at the population mean trait value:\n$$\\boldsymbol{\\beta}_t = \\boldsymbol{\\beta}(\\overline{\\mathbf{z}}_t) = -\\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\nThis approximation is central to the Lande-Arnold framework and is implicitly required by the problem's structure, which omits the full phenotypic covariance matrix.\n\nBy substituting this expression for $\\boldsymbol{\\beta}_t$ into the breeder's equation, we find the change in the mean trait vector over one generation:\n$$\\Delta \\overline{\\mathbf{z}}_t = \\mathbf{G} \\left( -\\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t) \\right) = -\\mathbf{G} \\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\n\nFrom this, we derive the explicit, deterministic update rule for the mean trait vector from generation $t$ to $t+1$:\n$$\\overline{\\mathbf{z}}_{t+1} = \\overline{\\mathbf{z}}_t + \\Delta \\overline{\\mathbf{z}}_t = \\overline{\\mathbf{z}}_t - \\mathbf{G} \\boldsymbol{\\Omega}^{-1}(\\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t)$$\n\nThe problem specifies a linearly changing optimum:\n$$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$$\nwhere $\\boldsymbol{\\theta}_0$ is the initial optimum, $\\mathbf{v}$ is the constant velocity of the optimum's movement, and $t$ is the generation number, starting from $t=0$.\n\nThe complete iterative algorithm is as follows:\n1. Initialize the mean trait vector at generation $t=0$: $\\overline{\\mathbf{z}} \\leftarrow \\overline{\\mathbf{z}}_0$.\n2. For each generation $t$ from $0$ to $T-1$:\n   a. Calculate the current optimum: $\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_0 + t\\,\\mathbf{v}$.\n   b. Calculate the difference vector between the mean and the optimum: $\\mathbf{d}_t = \\overline{\\mathbf{z}}_t - \\boldsymbol{\\theta}_t$.\n   c. Calculate the evolutionary response: $\\Delta \\overline{\\mathbf{z}}_t = - \\mathbf{G} \\boldsymbol{\\Omega}^{-1} \\mathbf{d}_t$.\n   d. Update the mean trait vector: $\\overline{\\mathbf{z}}_{t+1} = \\overline{\\mathbf{z}}_t + \\Delta \\overline{\\mathbf{z}}_t$.\n3. The final result after $T$ generations is the vector $\\overline{\\mathbf{z}}_T$.\n\nThis procedure will be implemented for each test case provided. The implementation involves basic matrix-vector operations: matrix inversion for $\\boldsymbol{\\Omega}$, and matrix-vector multiplication. The problem guarantees that $\\boldsymbol{\\Omega}$ is invertible. The additive genetic matrix $\\mathbf{G}$ may be singular, which is handled correctly by the derived equation, as no inversion of $\\mathbf{G}$ is required. For example, if $\\mathbf{G}$ is the zero matrix, $\\Delta \\overline{\\mathbf{z}}_t$ is zero, and the population mean does not evolve, as expected.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the deterministic evolutionary response of a quantitative polygenic\n    trait vector under stabilizing selection with a potentially moving optimum.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: 1D, stationary optimum\n        {\n            \"n\": 1,\n            \"z0\": np.array([0.0]),\n            \"G\": np.array([[0.2]]),\n            \"Omega\": np.array([[1.0]]),\n            \"theta0\": np.array([1.5]),\n            \"v\": np.array([0.0]),\n            \"T\": 5\n        },\n        # Case 2: 2D, correlated G, stationary optimum\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.0, 0.0]),\n            \"G\": np.array([[0.1, 0.05], [0.05, 0.2]]),\n            \"Omega\": np.array([[1.0, 0.0], [0.0, 1.5]]),\n            \"theta0\": np.array([1.0, -0.5]),\n            \"v\": np.array([0.0, 0.0]),\n            \"T\": 10\n        },\n        # Case 3: 1D, zero additive variance\n        {\n            \"n\": 1,\n            \"z0\": np.array([0.0]),\n            \"G\": np.array([[0.0]]),\n            \"Omega\": np.array([[0.5]]),\n            \"theta0\": np.array([1.0]),\n            \"v\": np.array([0.0]),\n            \"T\": 7\n        },\n        # Case 4: 2D, weak stabilizing selection\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.2, -0.2]),\n            \"G\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"Omega\": np.array([[100.0, 0.0], [0.0, 100.0]]),\n            \"theta0\": np.array([1.0, 1.0]),\n            \"v\": np.array([0.0, 0.0]),\n            \"T\": 10\n        },\n        # Case 5: 2D, moving optimum, correlational selection\n        {\n            \"n\": 2,\n            \"z0\": np.array([0.0, 0.0]),\n            \"G\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"Omega\": np.array([[1.0, 0.2], [0.2, 1.0]]),\n            \"theta0\": np.array([0.0, 0.0]),\n            \"v\": np.array([0.1, -0.05]),\n            \"T\": 20\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract parameters for the current test case\n        z_mean = case[\"z0\"]\n        G = case[\"G\"]\n        Omega = case[\"Omega\"]\n        theta0 = case[\"theta0\"]\n        v = case[\"v\"]\n        T = case[\"T\"]\n        \n        # Pre-compute the inverse of Omega and the product G * Omega_inv\n        # The problem statement guarantees Omega is invertible.\n        Omega_inv = np.linalg.inv(Omega)\n        G_Omega_inv = G @ Omega_inv\n\n        # Iterate the discrete-time update equation for T generations\n        for t in range(T):\n            # Calculate the optimum at the current generation t\n            theta_t = theta0 + t * v\n            \n            # Calculate the evolutionary response delta_z\n            # delta_z = -G * Omega_inv * (z_mean - theta_t)\n            delta_z = -G_Omega_inv @ (z_mean - theta_t)\n            \n            # Update the mean trait vector\n            z_mean = z_mean + delta_z\n            \n        # Round the final vector components to 6 decimal places\n        z_T = np.round(z_mean, 6)\n        \n        # Format the result vector as a string '[c1,c2,...]' with exactly 6 decimal places.\n        result_str = f\"[{','.join(f'{x:.6f}' for x in z_T)}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While many evolutionary models assume the additive genetic variance ($V_A$) is constant, selection itself can alter the very parameters that govern the response to selection. This practice delves into the Bulmer effect, a classic phenomenon where selection generates negative linkage disequilibrium among causal loci, causing a temporary reduction in $V_A$ . By deriving and iterating the recursion for $V_A$, you will explore the dynamic interplay between selection, which depletes heritable variance, and recombination, which partially restores it, providing a more nuanced view of evolutionary potential.",
            "id": "2744369",
            "problem": "You are to formalize and compute the reduction of additive genetic variance due to selection-induced linkage disequilibrium (linkage disequilibrium (LD))—the Bulmer effect—under the infinitesimal model for a quantitative trait. Work in discrete generations with the following assumptions, which you must use as the fundamental base for the derivation and algorithm:\n\n- The phenotype $P$ is the sum $P = G + E$ of an additive breeding value $G$ and an independent environmental deviation $E$, with $\\mathrm{Var}(G) = V_A$ and $\\mathrm{Var}(E) = V_E$. Assume the population mean of $P$ is initially $0$ by centering.\n- The population is infinitely large, mating is random, loci are unlinked (free recombination), there is no dominance or epistasis, and there is no mutation. The infinitesimal model holds: $G$ is approximately normally distributed, and allele-frequency changes per locus are negligible so that the genic variance $V_g$ (the additive variance that would be present if LD were $0$) can be treated as constant over generations.\n- Selection is truncation selection on $P$, retaining the top $\\pi$ fraction each generation. For a normally distributed trait, conditional moments of a truncated normal random variable are well tested: if $Z \\sim \\mathcal{N}(0,1)$ and we condition on $Z > z_\\pi$ with $\\pi = 1 - \\Phi(z_\\pi)$, then $\\mathbb{E}[Z \\mid Z > z_\\pi] = \\lambda$ and $\\mathrm{Var}(Z \\mid Z > z_\\pi) = 1 + z_\\pi \\lambda - \\lambda^2$, where $\\lambda = \\phi(z_\\pi)/\\pi$, with $\\phi$ and $\\Phi$ the standard normal probability density function and cumulative distribution function, respectively.\n\nYour goals:\n\n- Derive, from first principles and the laws of total expectation and variance, an expression for the additive variance among selected parents, $V_A^{(S)}$, as a function of $V_A$, $V_E$, and $\\pi$. Your derivation must start from the bivariate normal relation between $G$ and $P$ and must not assume any target formulas beyond the stated statistical facts about truncation of a normal distribution.\n- Using random mating and free recombination in the unlinked limit, express the additive variance in the offspring generation, $V_A^{\\text{next}}$, in terms of the genic variance $V_g$ and the selected-parent additive variance $V_A^{(S)}$. Justify, from the decomposition of additive variance into genic variance plus contributions from LD and the effect of recombination on LD, why $V_A^{\\text{next}}$ lies strictly between $V_g$ and $V_A^{(S)}$ when selection induces nonzero LD.\n- Design and implement an algorithm that, given $V_A(0)$, $V_E$, $\\pi$, and an integer number of generations $T$, iterates the variance recursion to compute the ratio $\\rho(T) = V_A(T)/V_g$.\n\nScientific realism constraints:\n\n- Treat $V_g$ as constant and equal to $V_A(0)$ under the infinitesimal model.\n- Use the exact truncated normal moments stated above to compute the selected-phenotype variance each generation.\n- Ensure that your implementation handles the boundary case $\\pi = 1$ (no selection). In this case, the correct limit is $\\rho(T) = 1$ for all $T$.\n\nInput is not read from the user. Instead, your program must run the following test suite hard-coded within the program and output the results as specified:\n\nTest suite (each test case is a tuple $(V_A(0), V_E, \\pi, T)$):\n\n- Case A (moderate selection, intermediate heritability): $(1.0, 1.0, 0.1, 5)$.\n- Case B (no environmental variance, strong heritability): $(1.0, 0.0, 0.2, 10)$.\n- Case C (no selection boundary): $(0.5, 1.5, 1.0, 50)$.\n- Case D (very strong selection, low heritability, near-equilibrium): $(1.0, 3.0, 0.01, 100)$.\n\nAll real numbers must be handled in dimensionless form; no physical units are involved. Your program should compute, for each case, the single floating-point value $\\rho(T) = V_A(T)/V_g$, and round it to $6$ decimal places. Angles are not used. Percentages must be represented as decimal fractions; the parameter $\\pi$ is already provided in this form.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example, $[\\rho_A,\\rho_B,\\rho_C,\\rho_D]$, with each entry rounded to $6$ decimal places.",
            "solution": "The problem statement is a well-posed and scientifically grounded formulation of the Bulmer effect under the infinitesimal model of quantitative genetics. It provides a complete set of assumptions and parameters required for a unique solution. The problem is valid, and a solution is derived and implemented as follows.\n\nThe core of the problem lies in iterating a recursion for the additive genetic variance $V_A$ over discrete generations. This requires deriving the change in $V_A$ within one generation, which involves two steps: the effect of selection on the parental generation, and the effect of random mating and recombination on the offspring generation.\n\nFirst, we derive the additive genetic variance among selected parents, $V_A^{(S)}$.\nThe phenotype $P$ is the sum of the breeding value $G$ and an independent environmental deviation $E$, so $P = G + E$. We are given that $G \\sim \\mathcal{N}(0, V_A)$ and $E \\sim \\mathcal{N}(0, V_E)$. Their independence implies that the joint distribution of $(G, P)$ is a bivariate normal distribution with mean vector $(0, 0)$ and covariance matrix:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathrm{Var}(G) & \\mathrm{Cov}(G, P) \\\\ \\mathrm{Cov(G, P)} & \\mathrm{Var}(P) \\end{pmatrix} = \\begin{pmatrix} V_A & V_A \\\\ V_A & V_A + V_E \\end{pmatrix}\n$$\nwhere $\\mathrm{Cov}(G, P) = \\mathrm{Cov}(G, G+E) = \\mathrm{Var}(G) + \\mathrm{Cov}(G, E) = V_A$. The total phenotypic variance is $V_P = V_A + V_E$.\n\nThe conditional distribution of $G$ given a specific phenotype value $P=p$ is normal. Standard results for a bivariate normal distribution give the conditional mean and variance:\n$$\n\\mathbb{E}[G \\mid P=p] = \\frac{\\mathrm{Cov}(G, P)}{\\mathrm{Var}(P)} p = \\frac{V_A}{V_P} p = h^2 p\n$$\n$$\n\\mathrm{Var}(G \\mid P=p) = \\mathrm{Var}(G) - \\frac{\\mathrm{Cov}(G, P)^2}{\\mathrm{Var}(P)} = V_A - \\frac{V_A^2}{V_P} = V_A \\left(1 - \\frac{V_A}{V_P}\\right) = V_A (1 - h^2)\n$$\nwhere $h^2 = V_A / V_P$ is the narrow-sense heritability. Note that this conditional variance is independent of the value of $p$.\n\nWe seek the variance of $G$ among selected individuals, which are those with phenotype $P$ exceeding a truncation threshold $P_t$, i.e., $V_A^{(S)} = \\mathrm{Var}(G \\mid P > P_t)$. We apply the law of total variance:\n$$\nV_A^{(S)} = \\mathrm{Var}(G \\mid P > P_t) = \\mathbb{E}[\\mathrm{Var}(G \\mid P) \\mid P > P_t] + \\mathrm{Var}(\\mathbb{E}[G \\mid P] \\mid P > P_t)\n$$\nThe first term is the expectation of a constant:\n$$\n\\mathbb{E}[\\mathrm{Var}(G \\mid P) \\mid P > P_t] = \\mathbb{E}[V_A(1 - h^2) \\mid P > P_t] = V_A(1 - h^2)\n$$\nThe second term is the variance of the conditional expectation:\n$$\n\\mathrm{Var}(\\mathbb{E}[G \\mid P] \\mid P > P_t) = \\mathrm{Var}(h^2 P \\mid P > P_t) = (h^2)^2 \\mathrm{Var}(P \\mid P > P_t) = (h^2)^2 V_P^{(S)}\n$$\nwhere $V_P^{(S)}$ is the phenotypic variance in the selected group. To find $V_P^{(S)}$, we standardize $P$ to $Z = P / \\sqrt{V_P}$, where $Z \\sim \\mathcal{N}(0, 1)$. The selection condition $P > P_t$ becomes $Z > z_\\pi$, where $z_\\pi = P_t / \\sqrt{V_P}$. The problem provides the variance of a truncated standard normal variable: $\\mathrm{Var}(Z \\mid Z > z_\\pi) = 1 + z_\\pi \\lambda - \\lambda^2$, with $\\lambda = \\phi(z_\\pi)/\\pi$.\nThus, the variance of the truncated phenotype is $V_P^{(S)} = \\mathrm{Var}(\\sqrt{V_P} Z \\mid Z > z_\\pi) = V_P \\mathrm{Var}(Z \\mid Z > z_\\pi) = V_P (1 + z_\\pi \\lambda - \\lambda^2)$.\nSubstituting back, we get:\n$$\nV_A^{(S)} = V_A(1 - h^2) + (h^2)^2 V_P (1 + z_\\pi \\lambda - \\lambda^2)\n$$\nReplacing $h^2 = V_A/V_P$ and simplifying:\n$$\nV_A^{(S)} = V_A\\left(1 - \\frac{V_A}{V_P}\\right) + \\left(\\frac{V_A}{V_P}\\right)^2 V_P (1 + z_\\pi \\lambda - \\lambda^2) = V_A - \\frac{V_A^2}{V_P} + \\frac{V_A^2}{V_P}(1 + z_\\pi \\lambda - \\lambda^2)\n$$\n$$\nV_A^{(S)} = V_A + \\frac{V_A^2}{V_P}(z_\\pi \\lambda - \\lambda^2) = V_A\\left(1 - \\frac{V_A}{V_P}(\\lambda^2 - z_\\pi \\lambda)\\right)\n$$\nLet us define the selection intensity coefficient $k = \\lambda(\\lambda - z_\\pi)$. The expression simplifies to the well-known result:\n$$\nV_A^{(S)} = V_A(1 - k h^2)\n$$\nFor any non-trivial selection ($0 < \\pi < 1$), one can show $k>0$, implying $V_A^{(S)} < V_A$. Selection reduces additive genetic variance by inducing negative linkage disequilibrium.\n\nSecond, we derive the additive variance in the next generation, $V_A^{\\text{next}}$. The selected parents mate randomly. The breeding value of an offspring is the average of its parents' breeding values, plus a Mendelian sampling term $\\delta$: $G_{\\text{offspring}} = \\frac{1}{2}(G_{\\text{father}} + G_{\\text{mother}}) + \\delta$.\nThe variance is $V_A^{\\text{next}} = \\mathrm{Var}(G_{\\text{offspring}}) = \\mathrm{Var}(\\frac{1}{2}(G_{\\text{father}} + G_{\\text{mother}})) + \\mathrm{Var}(\\delta)$. Since parents are chosen randomly from the selected group, $\\mathrm{Var}(G_{\\text{father}}) = \\mathrm{Var}(G_{\\text{mother}}) = V_A^{(S)}$. Their covariance is $0$.\n$$\nV_A^{\\text{next}} = \\frac{1}{4}(\\mathrm{Var}(G_{\\text{father}}) + \\mathrm{Var}(G_{\\text{mother}})) + \\mathrm{Var}(\\delta) = \\frac{1}{2}V_A^{(S)} + \\mathrm{Var}(\\delta)\n$$\nThe Mendelian sampling variance, $\\mathrm{Var}(\\delta)$, arises from segregation of alleles. With free recombination (unlinked loci), all linkage disequilibrium induced by selection is dissipated in one generation. The variance from segregation depends on the underlying allele frequencies, which under the infinitesimal model do not change. This variance is equal to half the genic variance, $\\mathrm{Var}(\\delta) = \\frac{1}{2}V_g$. The genic variance $V_g$ is the additive variance that would exist if the population were in linkage equilibrium; under the problem's assumptions, it is constant and equal to $V_A(0)$.\nThis yields the final recurrence relation:\n$$\nV_A^{\\text{next}} = \\frac{1}{2}V_A^{(S)} + \\frac{1}{2}V_g\n$$\nThis expression shows that $V_A^{\\text{next}}$ is the arithmetic mean of the post-selection variance $V_A^{(S)}$ and the constant genic variance $V_g$. Consequently, for any case where $V_A^{(S)} \\neq V_g$ (which is true for any non-trivial selection), $V_A^{\\text{next}}$ must lie strictly between $V_A^{(S)}$ and $V_g$. Selection drives $V_A$ down towards an equilibrium value below $V_g$, while recombination partially restores it by breaking down the negative linkage disequilibrium.\n\nThe computational algorithm proceeds as follows:\n1.  Initialize $V_A = V_A(0)$ and $V_g = V_A(0)$.\n2.  If $\\pi=1$, there is no selection, $k=0$, $V_A^{(S)}=V_A$. The recursion becomes $V_A^{\\text{next}} = \\frac{1}{2}V_A + \\frac{1}{2}V_g$. Since $V_A(0)=V_g$, $V_A$ remains equal to $V_g$ for all generations. Thus, $\\rho(T)=1$.\n3.  If $\\pi<1$, calculate the constants $z_\\pi$, $\\lambda$, and $k$.\n4.  Iterate for $T$ generations. In each generation $t$:\n    a. Calculate $V_P(t) = V_A(t) + V_E$ and $h^2(t) = V_A(t) / V_P(t)$.\n    b. Calculate $V_A^{(S)}(t) = V_A(t)(1 - k \\cdot h^2(t))$.\n    c. Calculate $V_A(t+1) = \\frac{1}{2}V_A^{(S)}(t) + \\frac{1}{2}V_g$.\n5.  After $T$ iterations, compute the final ratio $\\rho(T) = V_A(T) / V_g$.\nThis procedure is implemented to solve the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the reduction of additive genetic variance due to the Bulmer effect.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate selection, intermediate heritability)\n        (1.0, 1.0, 0.1, 5),\n        # Case B (no environmental variance, strong heritability)\n        (1.0, 0.0, 0.2, 10),\n        # Case C (no selection boundary)\n        (0.5, 1.5, 1.0, 50),\n        # Case D (very strong selection, low heritability, near-equilibrium)\n        (1.0, 3.0, 0.01, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        Va0, Ve, pi, T = case\n        \n        # Under the infinitesimal model, genic variance is constant and equals\n        # the initial additive variance before selection begins.\n        Vg = Va0\n        \n        # Handle the boundary case of no selection (pi = 1).\n        if pi == 1.0:\n            # With no selection, V_A remains at V_g.\n            # The recursion V_A(t+1) = 0.5 * V_A(t) + 0.5 * V_g with V_A(0) = V_g\n            # yields V_A(t) = V_g for all t.\n            # Therefore, the ratio V_A(T) / V_g is always 1.\n            rho_T = 1.0\n            results.append(f\"{rho_T:.6f}\")\n            continue\n\n        # Pre-calculate selection-related constants that do not change over generations.\n        # z_pi is the truncation point on a standard normal distribution.\n        z_pi = norm.ppf(1.0 - pi)\n        # lambda_val is the mean of the truncated standard normal distribution.\n        # Note: 'lambda' is a reserved keyword in Python.\n        lambda_val = norm.pdf(z_pi) / pi\n        # k is the selection intensity coefficient related to variance reduction.\n        k = lambda_val * (lambda_val - z_pi)\n        \n        # Initialize the additive genetic variance for the first generation.\n        Va_t = Va0\n        \n        # Iterate the recursion for T generations.\n        for _ in range(T):\n            # Phenotypic variance in the current generation.\n            Vp_t = Va_t + Ve\n            \n            # Heritability in the current generation.\n            # We assume Vp_t > 0, which is true for the given test cases.\n            h2_t = Va_t / Vp_t if Vp_t > 0 else 0.0\n            \n            # Additive variance among selected parents (after selection).\n            Va_s = Va_t * (1.0 - k * h2_t)\n            \n            # Additive variance in the next generation (Bulmer's recursion).\n            # This accounts for recombination breaking down linkage disequilibrium.\n            Va_next = 0.5 * Va_s + 0.5 * Vg\n            \n            # Update the variance for the next iteration.\n            Va_t = Va_next\n            \n        # The final ratio is rho(T) = V_A(T) / V_g.\n        rho_T = Va_t / Vg\n        results.append(f\"{rho_T:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The predictive models of quantitative genetics rely on key parameters like the additive genetic variance, but how are these estimated from empirical data? This exercise introduces the \"animal model,\" a linear mixed model (LMM) that partitions phenotypic variance into genetic ($\\sigma_g^2$) and environmental ($\\sigma_e^2$) components using kinship data . You will implement variance component estimation using Restricted Maximum Likelihood (REML), a robust statistical method that provides unbiased estimates by accounting for fixed effects in the model. Completing this task will give you practical experience with one of the most fundamental computational methods in modern statistical genetics.",
            "id": "2744356",
            "problem": "You are given independent populations of unrelated sizes expressing a quantitative trait shaped by many loci of small effect (polygenic). Assume the following fundamental base: quantitative trait values across individuals follow a linear mixed model with an additive genetic random effect capturing shared ancestry through a kinship matrix and an independent environmental residual. Under this base, for a sample of size $n$ with an intercept in the fixed-effect design, the distribution of the trait vector is multivariate normal, and the variance of the trait decomposes into an additive genetic component proportional to the kinship matrix and an environmental component proportional to the identity matrix. The goal is to estimate the additive genetic variance and the environmental variance jointly by maximizing the restricted likelihood and then compute the narrow-sense heritability, defined as the proportion of phenotypic variance attributable to additive genetic effects. Your task is to implement a program that performs this estimation via numerical optimization of the restricted likelihood, using stable linear algebra.\n\nUse the following modeling definitions and constraints:\n- Let the trait vector be denoted by $y \\in \\mathbb{R}^{n}$ and the fixed-effect design matrix be $X \\in \\mathbb{R}^{n \\times p}$, with an intercept only (so $p = 1$ and $X$ is a column of ones). Let the kinship matrix be $K \\in \\mathbb{R}^{n \\times n}$, symmetric positive semidefinite.\n- The variance components are the additive genetic variance $\\sigma_{g}^{2}$ and the environmental variance $\\sigma_{e}^{2}$, both strictly positive real numbers.\n- The covariance of $y$ is the sum of the additive genetic covariance and the environmental covariance. You must maximize the restricted likelihood with respect to $\\sigma_{g}^{2}$ and $\\sigma_{e}^{2}$ using a numerically stable approach that guarantees positivity of the variance components.\n- After estimating $\\widehat{\\sigma}_{g}^{2}$ and $\\widehat{\\sigma}_{e}^{2}$, compute the narrow-sense heritability $h^{2}$ as the proportion of total phenotypic variance attributable to additive genetic effects. Report $h^{2}$ for each test case.\n- All computations must be done in floating-point arithmetic. Angles are not involved. No physical units are required. Express final numerical answers rounded to four decimal places as decimals (not fractions).\n\nInput data for the test suite are provided below. For each case, you are given a factor matrix $B$ from which the kinship matrix must be constructed as $K = B B^{\\top}$ to ensure positive semidefiniteness, and an observed trait vector $y$. In all cases, use an intercept-only design $X$ with entries equal to $1$. Implement a numerical optimization of the restricted likelihood that is robust to conditioning, using a reparameterization that enforces $\\sigma_{g}^{2} > 0$ and $\\sigma_{e}^{2} > 0$, and stable solvers for linear systems and log-determinants.\n\nTest suite:\n- Case A (moderate heritability):\n  - $B = \\begin{bmatrix}\n  1.2 & 0.0 & 0.0 \\\\\n  0.6 & 0.3 & 0.0 \\\\\n  0.4 & 0.8 & 0.2 \\\\\n  0.0 & 0.5 & 0.7 \\\\\n  0.0 & 0.2 & 1.0 \\\\\n  0.0 & 0.0 & 0.9\n  \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1.04 & 0.61 & 0.82 & 0.25 & -0.04 & -0.20 \\end{bmatrix}^{\\top}$\n- Case B (near-zero heritability):\n  - $B$ is identical to Case A.\n  - $y = \\begin{bmatrix} 0.30 & -0.70 & 0.20 & -0.10 & 0.05 & -0.30 \\end{bmatrix}^{\\top}$\n- Case C (high heritability):\n  - $B = \\begin{bmatrix}\n  1.0 & 0.0 \\\\\n  0.8 & 0.2 \\\\\n  0.6 & 0.4 \\\\\n  0.4 & 0.6 \\\\\n  0.2 & 0.8\n  \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1.01 & 0.79 & 0.605 & 0.395 & 0.20 \\end{bmatrix}^{\\top}$\n- Case D (moderate heritability, different structure and size):\n  - $B = \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 \\\\\n  0.7 & 0.2 & 0.0 \\\\\n  0.5 & 0.4 & 0.1 \\\\\n  0.3 & 0.6 & 0.2 \\\\\n  0.1 & 0.8 & 0.3 \\\\\n  0.0 & 0.6 & 0.5 \\\\\n  0.0 & 0.3 & 0.9\n  \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 0.55 & 0.40 & 0.45 & 0.42 & 0.47 & 0.32 & 0.33 \\end{bmatrix}^{\\top}$\n\nRequirements and output format:\n- Construct $K$ for each case as $K = B B^{\\top}$.\n- Estimate $\\widehat{\\sigma}_{g}^{2}$ and $\\widehat{\\sigma}_{e}^{2}$ by maximizing the restricted likelihood with respect to these parameters given $y$, $X$, and $K$. Use numerically stable linear algebra (for example, factorizations that avoid explicit matrix inversion) and an unconstrained reparameterization that ensures positivity of the variance components.\n- For each case, compute the narrow-sense heritability $h^{2}$ from the estimated variance components.\n- Your program should produce a single line of output containing the results as a comma-separated list of the four heritability estimates, each rounded to four decimal places, enclosed in square brackets, in the order Case A, Case B, Case C, Case D. For example, an output of the correct format is $[\\;0.5023,0.0311,0.9120,0.4722\\;]$ (these are illustrative values, not the required answers).",
            "solution": "The problem presented is a standard and well-posed task in statistical genetics: the estimation of variance components in a linear mixed model (LMM) using Restricted Maximum Likelihood (REML). The problem is scientifically grounded, free of contradictions, and contains all necessary information for its resolution. I will, therefore, provide a complete solution.\n\nThe fundamental model for the quantitative trait vector $y \\in \\mathbb{R}^n$ is the LMM:\n$$ y = X\\beta + u + \\epsilon $$\nwhere:\n- $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix for fixed effects, which in this problem is a single column of ones ($p=1$), representing a global intercept.\n- $\\beta \\in \\mathbb{R}^p$ is the vector of fixed effects.\n- $u \\in \\mathbb{R}^n$ is the vector of random genetic effects, assumed to follow a multivariate normal distribution $u \\sim \\mathcal{N}(0, \\sigma_g^2 K)$, where $\\sigma_g^2$ is the additive genetic variance and $K$ is the kinship matrix derived from ancestry.\n- $\\epsilon \\in \\mathbb{R}^n$ is the vector of random environmental and non-genetic effects, assumed to follow $\\epsilon \\sim \\mathcal{N}(0, \\sigma_e^2 I)$, where $\\sigma_e^2$ is the environmental variance and $I$ is the $n \\times n$ identity matrix.\n\nThe random effects $u$ and $\\epsilon$ are assumed to be independent. Consequently, the trait vector $y$ follows a multivariate normal distribution:\n$$ y \\sim \\mathcal{N}(X\\beta, V) $$\nwhere the total covariance matrix $V$ is given by:\n$$ V(\\sigma_g^2, \\sigma_e^2) = \\sigma_g^2 K + \\sigma_e^2 I $$\nThe objective is to estimate the variance components $\\sigma_g^2$ and $\\sigma_e^2$. A simple maximum likelihood estimation would yield biased estimates because it does not account for the degrees of freedom lost in estimating the fixed effects $\\beta$. REML corrects for this by maximizing the likelihood of a set of $n-p$ linearly independent contrasts of $y$ that do not depend on $\\beta$.\n\nThe REML log-likelihood function, ignoring constant terms, which is to be maximized with respect to $\\sigma_g^2$ and $\\sigma_e^2$, is:\n$$ \\ell_{\\text{REML}}(\\sigma_g^2, \\sigma_e^2 | y) = -\\frac{1}{2} \\log\\det(V) - \\frac{1}{2} \\log\\det(X^\\top V^{-1} X) - \\frac{1}{2} y^\\top P y $$\nwhere $P$ is the projection matrix $P = V^{-1} - V^{-1}X(X^\\top V^{-1}X)^{-1}X^\\top V^{-1}$.\n\nMaximizing this function directly is numerically problematic due to the explicit computation of matrix inverses ($V^{-1}$) and determinants. A numerically stable approach is mandatory. Furthermore, the variance components must be strictly positive, $\\sigma_g^2 > 0$ and $\\sigma_e^2 > 0$. This constrained optimization is best handled by reparameterizing the problem into an unconstrained one. We define new parameters:\n$$ \\theta_g = \\log(\\sigma_g^2) \\quad \\text{and} \\quad \\theta_e = \\log(\\sigma_e^2) $$\nThe optimization is then performed over $(\\theta_g, \\theta_e) \\in \\mathbb{R}^2$. The variance components are recovered as $\\sigma_g^2 = \\exp(\\theta_g)$ and $\\sigma_e^2 = \\exp(\\theta_e)$, which are guaranteed to be positive.\n\nThe components of the REML likelihood will be computed using stable matrix factorizations. Since $K$ is positive semidefinite and $\\sigma_e^2 > 0$, the matrix $V = \\sigma_g^2 K + \\sigma_e^2 I$ is guaranteed to be symmetric and positive definite. Thus, it admits a Cholesky decomposition $V = LL^\\top$, where $L$ is a lower triangular matrix. This decomposition is the cornerstone of a stable implementation.\n\nThe terms in the negative log-likelihood function (which we will minimize) are computed as follows:\n1.  **Log-determinant of $V$**: $\\log\\det(V) = \\log\\det(LL^\\top) = 2 \\log\\det(L) = 2 \\sum_{i=1}^{n} \\log(L_{ii})$.\n2.  **Solving linear systems**: Instead of computing $V^{-1}$, we solve linear systems. For a vector $z$, the product $V^{-1}z$ is found by solving $Vw = z$ for $w$. Using the Cholesky factor, this is done via two triangular solves: first solve $Lq = z$ for $q$ (forward substitution), then solve $L^\\top w = q$ for $w$ (backward substitution).\n3.  **Log-determinant of $X^\\top V^{-1} X$**: Let's compute the matrix $A = X^\\top V^{-1} X$. Its $j$-th column is $X^\\top V^{-1} X_j$, where $X_j$ is the $j$-th column of $X$. We first solve for $W_j = V^{-1}X_j$ using the Cholesky method described above. Then we form $X^\\top W_j$. For this problem, $p=1$ and $X$ is a column of ones, so $X^\\top V^{-1} X$ is a scalar. We solve $Vw_x = X$ for $w_x$, and then compute the scalar $s_{xx} = X^\\top w_x$. The log-determinant term is simply $\\log(s_{xx})$.\n4.  **Quadratic form $y^\\top P y$**:\n    $$ y^\\top P y = y^\\top V^{-1} y - y^\\top V^{-1} X (X^\\top V^{-1} X)^{-1} X^\\top V^{-1} y $$\n    We solve $Vw_y = y$ for $w_y$. Then $y^\\top V^{-1} y = y^\\top w_y$. The term $X^\\top V^{-1} y$ is $X^\\top w_y$. Let's call this scalar $s_{xy}$. The quadratic form becomes $y^\\top w_y - s_{xy}^2 / s_{xx}$.\n\nThe objective function to be minimized by a numerical optimization algorithm (e.g., Nelder-Mead or L-BFGS) is the negative REML log-likelihood:\n$$ f(\\theta_g, \\theta_e) = \\frac{1}{2} \\left[ \\log\\det(V) + \\log(s_{xx}) + (y^\\top w_y - s_{xy}^2 / s_{xx}) \\right] $$\nwhere $V$, $s_{xx}$, $s_{xy}$, and $w_y$ all depend on $\\theta_g$ and $\\theta_e$.\n\nAfter finding the optimal parameters $(\\widehat{\\theta}_g, \\widehat{\\theta}_e)$ that minimize $f$, the estimated variance components are:\n$$ \\widehat{\\sigma}_g^2 = \\exp(\\widehat{\\theta}_g) \\quad \\text{and} \\quad \\widehat{\\sigma}_e^2 = \\exp(\\widehat{\\theta}_e) $$\nFinally, the narrow-sense heritability, $h^2$, is computed as the proportion of the total variance attributable to additive genetic effects:\n$$ h^2 = \\frac{\\widehat{\\sigma}_g^2}{\\widehat{\\sigma}_g^2 + \\widehat{\\sigma}_e^2} $$\n\nThe implementation will proceed by defining a Python function that computes $f(\\theta_g, \\theta_e)$ for a given dataset $(y, K)$. This function will then be passed to an optimizer from the `scipy.optimize` library. The process is repeated for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the heritability estimation problem for all test cases.\n    \"\"\"\n\n    # Test suite data\n    B_A = np.array([\n        [1.2, 0.0, 0.0],\n        [0.6, 0.3, 0.0],\n        [0.4, 0.8, 0.2],\n        [0.0, 0.5, 0.7],\n        [0.0, 0.2, 1.0],\n        [0.0, 0.0, 0.9]\n    ])\n    y_A = np.array([1.04, 0.61, 0.82, 0.25, -0.04, -0.20])\n\n    B_B = B_A\n    y_B = np.array([0.30, -0.70, 0.20, -0.10, 0.05, -0.30])\n    \n    B_C = np.array([\n        [1.0, 0.0],\n        [0.8, 0.2],\n        [0.6, 0.4],\n        [0.4, 0.6],\n        [0.2, 0.8]\n    ])\n    y_C = np.array([1.01, 0.79, 0.605, 0.395, 0.20])\n\n    B_D = np.array([\n        [1.0, 0.0, 0.0],\n        [0.7, 0.2, 0.0],\n        [0.5, 0.4, 0.1],\n        [0.3, 0.6, 0.2],\n        [0.1, 0.8, 0.3],\n        [0.0, 0.6, 0.5],\n        [0.0, 0.3, 0.9]\n    ])\n    y_D = np.array([0.55, 0.40, 0.45, 0.42, 0.47, 0.32, 0.33])\n\n    test_cases = [\n        (B_A, y_A),\n        (B_B, y_B),\n        (B_C, y_C),\n        (B_D, y_D),\n    ]\n\n    results = []\n    \n    for B, y in test_cases:\n        n = y.shape[0]\n        K = B @ B.T\n        X = np.ones((n, 1))\n\n        def neg_reml_log_likelihood(log_vars):\n            \"\"\"\n            Computes the negative REML log-likelihood for given log-variances.\n            Parameters are log(sigma_g^2) and log(sigma_e^2).\n            \"\"\"\n            sigma2_g = np.exp(log_vars[0])\n            sigma2_e = np.exp(log_vars[1])\n\n            # Form the covariance matrix V = sigma_g^2 * K + sigma_e^2 * I\n            V = sigma2_g * K + sigma2_e * np.identity(n)\n\n            try:\n                # Cholesky decomposition of V\n                # This is numerically stable and efficient for SPD matrices.\n                L, lower = cho_factor(V, lower=True)\n            except np.linalg.LinAlgError:\n                # V is not positive definite, return a large value\n                # to guide the optimizer away.\n                return 1e10\n            \n            # log(det(V)) = 2 * sum(log(diag(L)))\n            log_det_V = 2 * np.sum(np.log(np.diag(L)))\n\n            # Solve linear systems to avoid explicit inversion of V\n            # Find w_x = V^{-1} * X\n            w_x = cho_solve((L, lower), X)\n            # Find w_y = V^{-1} * y\n            w_y = cho_solve((L, lower), y)\n\n            # Compute terms for the REML likelihood\n            s_xx = X.T @ w_x\n            # For p=1, s_xx is a 1x1 matrix.\n            s_xx_scalar = s_xx[0, 0]\n            log_det_S_xx = np.log(s_xx_scalar)\n\n            s_xy = X.T @ w_y\n            y_T_w_y = y.T @ w_y\n            \n            # Quadratic form y^T * P * y\n            y_P_y = y_T_w_y - (s_xy[0] ** 2) / s_xx_scalar\n            \n            # The negative REML log-likelihood (to be minimized)\n            # We ignore constant terms like log(2*pi)\n            neg_ll = 0.5 * (log_det_V + log_det_S_xx + y_P_y)\n            return neg_ll\n\n        # Initial guess for the optimizer\n        # A reasonable guess is that genetic and environmental variances\n        # are each half of the total phenotypic variance.\n        var_y = np.var(y, ddof=1)\n        initial_sigma2 = var_y / 2.0\n        # If variance is zero or negative (unlikely but possible with weird data)\n        if initial_sigma2 <= 0:\n            initial_sigma2 = 1.0\n            \n        x0 = np.array([np.log(initial_sigma2), np.log(initial_sigma2)])\n\n        # Perform the optimization using a robust method like Nelder-Mead\n        opt_result = minimize(\n            neg_reml_log_likelihood, \n            x0, \n            method='Nelder-Mead',\n            options={'xatol': 1e-8, 'fatol': 1e-8}\n        )\n        \n        # Extract estimated log variances\n        opt_log_vars = opt_result.x\n        \n        # Convert back to variance components\n        opt_sigma2_g = np.exp(opt_log_vars[0])\n        opt_sigma2_e = np.exp(opt_log_vars[1])\n\n        # Compute narrow-sense heritability\n        h2 = opt_sigma2_g / (opt_sigma2_g + opt_sigma2_e)\n        \n        # Handle potential boundary case where h2 is effectively zero\n        if h2 < 1e-8:\n             h2 = 0.0\n\n        results.append(round(h2, 4))\n\n    # Format the final output as a single string\n    formatted_results = f\"[{','.join(f'{r:.4f}' for r in results)}]\"\n    print(formatted_results)\n\nsolve()\n```"
        }
    ]
}