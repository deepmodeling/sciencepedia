{
    "hands_on_practices": [
        {
            "introduction": "Modern species delimitation is fundamentally a process of statistical model selection, where competing hypotheses about species boundaries are evaluated. The Bayes factor is a cornerstone of this process, providing a quantitative measure of evidence from the data in favor of one model (e.g., splitting two lineages) over another (e.g., lumping them). This exercise will give you direct practice in calculating Bayes factors from the log-marginal likelihoods commonly produced by phylogenetic software and in using standard scales to interpret the strength of evidence for a particular delimitation model. ",
            "id": "2752783",
            "problem": "In a multispecies coalescent framework for species delimitation, two competing models are considered for a focal clade: model $\\mathcal{M}_{A}$ (lumping two candidate lineages) and model $\\mathcal{M}_{B}$ (splitting them). Using thermodynamic integration via stepping-stone sampling, you obtain the following model evidences reported as natural logarithms of the marginal likelihoods: $\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{A}) = -17890.432$ and $\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{B}) = -17882.932$, where $\\mathcal{D}$ denotes the multilocus sequence dataset. Assume the same priors and data processing across models so that these values are directly comparable as model evidences.\n\nUsing the foundational definitions of marginal likelihood and Bayes factor, compute the Bayes factor in favor of $\\mathcal{M}_{B}$ relative to $\\mathcal{M}_{A}$. Then, interpret the strength of evidence for $\\mathcal{M}_{B}$ using the following pre-specified scale (Kass–Raftery style, adapted for direct Bayes factors $\\mathrm{BF}$):\n\n- $1 \\le \\mathrm{BF} < 3.2$: not worth more than a bare mention,\n- $3.2 \\le \\mathrm{BF} < 10$: substantial,\n- $10 \\le \\mathrm{BF} < 100$: strong,\n- $\\mathrm{BF} \\ge 100$: decisive.\n\nReport the Bayes factor as a pure number, rounded to four significant figures. Do not include any units. You may provide the interpretation in your reasoning, but the final reported answer must be only the Bayes factor value.",
            "solution": "The fundamental quantity for comparing two statistical models, $\\mathcal{M}_{A}$ and $\\mathcal{M}_{B}$, in a Bayesian framework is the Bayes factor. The Bayes factor, denoted $\\mathrm{BF}_{B,A}$, quantifies the evidence from the data $\\mathcal{D}$ in favor of model $\\mathcal{M}_{B}$ over model $\\mathcal{M}_{A}$. It is defined as the ratio of their marginal likelihoods:\n$$\n\\mathrm{BF}_{B,A} = \\frac{p(\\mathcal{D} \\mid \\mathcal{M}_{B})}{p(\\mathcal{D} \\mid \\mathcal{M}_{A})}\n$$\nThe marginal likelihood, $p(\\mathcal{D} \\mid \\mathcal{M})$, represents the probability of observing the data $\\mathcal{D}$ given the model $\\mathcal{M}$, integrated over the entire parameter space of the model. In practice, these values are often extremely small, so computations are performed using their natural logarithms, as provided in the problem statement.\n\nGiven are the log-marginal likelihoods:\n$$\n\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{A}) = -17890.432\n$$\n$$\n\\ln p(\\mathcal{D} \\mid \\mathcal{M}_{B}) = -17882.932\n$$\nTo compute the Bayes factor from these logarithmic values, we utilize the property of logarithms that the logarithm of a ratio is the difference of the logarithms:\n$$\n\\ln(\\mathrm{BF}_{B,A}) = \\ln\\left(\\frac{p(\\mathcal{D} \\mid \\mathcal{M}_{B})}{p(\\mathcal{D} \\mid \\mathcal{M}_{A})}\\right) = \\ln p(\\mathcal{D} \\mid \\mathcal{M}_{B}) - \\ln p(\\mathcal{D} \\mid \\mathcal{M}_{A})\n$$\nThis quantity, $\\ln(\\mathrm{BF}_{B,A})$, is often referred to as the log-Bayes factor. Substituting the given values:\n$$\n\\ln(\\mathrm{BF}_{B,A}) = -17882.932 - (-17890.432) = -17882.932 + 17890.432 = 7.500\n$$\nTo obtain the Bayes factor $\\mathrm{BF}_{B,A}$ itself, we must exponentiate the log-Bayes factor:\n$$\n\\mathrm{BF}_{B,A} = \\exp(\\ln(\\mathrm{BF}_{B,A})) = \\exp(7.500)\n$$\nCalculating this value:\n$$\n\\mathrm{BF}_{B,A} \\approx 1808.0424\n$$\nThe problem requires this value to be rounded to four significant figures. The first four significant digits are $1$, $8$, $0$, and $8$. The following digit is $0$, so no rounding up is performed. The value is $1808$.\n\nThe final step is to interpret this result using the provided scale. The computed Bayes factor is $\\mathrm{BF}_{B,A} = 1808$. Since $1808 \\ge 100$, the evidence in favor of model $\\mathcal{M}_{B}$ (the split model) over model $\\mathcal{M}_{A}$ (the lump model) is classified as \"decisive\". This indicates very strong support for the hypothesis that the two candidate lineages represent distinct species.",
            "answer": "$$\\boxed{1808}$$"
        },
        {
            "introduction": "A critical challenge in systematics is to differentiate between discrete species and continuous patterns of genetic variation arising from geographic structure. This theoretical exercise explores how isolation-by-distance (IBD), where genetic similarity declines with spatial separation, can mislead delimitation methods by creating apparent clusters. By deriving the expected genetic distance between two halves of a continuous population under a stepping-stone model, you will develop a first-principles appreciation for why accounting for spatial processes is essential for robust species delimitation. ",
            "id": "2752715",
            "problem": "Isolation-by-distance (IBD) is a population genetic consequence of limited dispersal whereby genetic similarity between individuals decreases as a function of increasing geographic distance. In a spatially structured population, this produces positive spatial autocorrelation in allele frequencies and a characteristic decay of genetic covariance with distance. Such clinal structure is known to confound clustering-based species delimitation methods that assume discrete panmictic units.\n\nConsider a one-dimensional stepping-stone population consisting of $K$ demes arranged linearly with equal spacing and equal effective sizes, at mutation–drift–migration equilibrium under neutral evolution. Assume $K$ is even. Let deme positions be indexed by $j \\in \\{0,1,\\dots,K-1\\}$ along a line segment of unit length, so the spatial step size is $h = 1/(K-1)$. For each unlinked, selectively neutral biallelic locus $\\ell \\in \\{1,\\dots,L\\}$, model the allele-frequency deviation from the global mean along demes as a zero-mean stationary Gaussian process (GP) with covariance function\n$$\nC(d) \\;=\\; \\sigma^{2} \\exp\\!\\left(-\\frac{d}{\\lambda}\\right),\n$$\nwhere $d$ is geographic distance, $\\sigma^{2}$ is the marginal variance, and $\\lambda$ is the spatial correlation length. Assume loci are independent and identically distributed with the same covariance parameters $\\sigma^{2}$ and $\\lambda$. Ignore within-deme sampling noise by taking the sample mean genotype in each deme to equal its expectation.\n\nYou artificially delimit two putative “species” by splitting demes into the left half and the right half: the left group consists of demes $j=0,1,\\dots,\\frac{K}{2}-1$ and the right group consists of demes $j=\\frac{K}{2},\\dots,K-1$. In the $L$-dimensional genotype space whose $\\ell$-th coordinate is twice the sample mean allele frequency at locus $\\ell$, define the two group centroids as the $L$-vectors of group-wise averages across their respective demes.\n\nStarting from the definition of IBD and the stepping-stone setting above, derive the expected squared Euclidean distance between these two centroids as a closed-form expression in terms of $K$, $L$, $\\sigma^{2}$, and $\\lambda$. You may denote $M \\equiv K/2$, and you may define $\\rho \\equiv \\exp\\!\\left(-\\frac{h}{\\lambda}\\right)$ to simplify your expression. Your final answer must be a single closed-form analytic expression. Do not approximate or round.",
            "solution": "Let $p_j^{(\\ell)}$ be the sample mean allele frequency for locus $\\ell$ in deme $j$. The problem states that the coordinate in the genotype space is $g_j^{(\\ell)} = 2p_j^{(\\ell)}$.\nLet $\\bar{p}^{(\\ell)}$ be the global mean allele frequency for locus $\\ell$. The allele frequency deviation is $y_j^{(\\ell)} = p_j^{(\\ell)} - \\bar{p}^{(\\ell)}$. By definition, $\\mathbb{E}[y_j^{(\\ell)}] = 0$.\nThe covariance is given by $\\mathbb{E}[y_i^{(\\ell)}y_j^{(\\ell')}] = \\delta_{\\ell\\ell'} C(d_{ij})$, where $\\delta_{\\ell\\ell'}$ is the Kronecker delta (reflecting independence of loci) and $d_{ij} = |i-j|h$ is the geographic distance between demes $i$ and $j$.\nUsing the provided notation, this covariance is $\\mathbb{E}[y_i^{(\\ell)}y_j^{(\\ell)}] = \\sigma^2 \\exp(-|i-j|h/\\lambda) = \\sigma^2 \\rho^{|i-j|}$ for a single locus $\\ell$.\n\nThe two groups are the left group $G_L = \\{0, \\dots, M-1\\}$ and the right group $G_R = \\{M, \\dots, 2M-1\\}$, where $M=K/2$.\nThe centroids $C_L$ and $C_R$ are $L$-dimensional vectors. Their respective $\\ell$-th components are:\n$$ (C_L)_{\\ell} = \\frac{1}{M} \\sum_{j=0}^{M-1} g_j^{(\\ell)} \\quad \\text{and} \\quad (C_R)_{\\ell} = \\frac{1}{M} \\sum_{j=M}^{2M-1} g_j^{(\\ell)} $$\nThe squared Euclidean distance between the centroids is $D^2 = \\|C_R - C_L\\|^2 = \\sum_{\\ell=1}^L ((C_R)_{\\ell} - (C_L)_{\\ell})^2$. We need to find its expectation, $\\mathbb{E}[D^2]$.\n\nFirst, consider the difference of the $\\ell$-th components of the centroids:\n$$ (C_R)_{\\ell} - (C_L)_{\\ell} = \\frac{1}{M} \\sum_{j=M}^{2M-1} 2p_j^{(\\ell)} - \\frac{1}{M} \\sum_{j=0}^{M-1} 2p_j^{(\\ell)} $$\nSubstituting $p_j^{(\\ell)} = y_j^{(\\ell)} + \\bar{p}^{(\\ell)}$:\n$$ (C_R)_{\\ell} - (C_L)_{\\ell} = \\frac{2}{M} \\left( \\sum_{j=M}^{2M-1} (y_j^{(\\ell)} + \\bar{p}^{(\\ell)}) - \\sum_{j=0}^{M-1} (y_j^{(\\ell)} + \\bar{p}^{(\\ell)}) \\right) $$\n$$ = \\frac{2}{M} \\left( \\sum_{j=M}^{2M-1} y_j^{(\\ell)} - \\sum_{j=0}^{M-1} y_j^{(\\ell)} + M\\bar{p}^{(\\ell)} - M\\bar{p}^{(\\ell)} \\right) = \\frac{2}{M} \\left( \\sum_{j \\in G_R} y_j^{(\\ell)} - \\sum_{j \\in G_L} y_j^{(\\ell)} \\right) $$\nLet $\\Delta_{\\ell} = (C_R)_{\\ell} - (C_L)_{\\ell}$. We have $\\mathbb{E}[\\Delta_{\\ell}] = 0$ since $\\mathbb{E}[y_j^{(\\ell)}]=0$.\nThe expected squared distance is:\n$$ \\mathbb{E}[D^2] = \\mathbb{E}\\left[\\sum_{\\ell=1}^L \\Delta_{\\ell}^2\\right] = \\sum_{\\ell=1}^L \\mathbb{E}[\\Delta_{\\ell}^2] $$\nSince all loci are i.i.d., $\\mathbb{E}[\\Delta_{\\ell}^2]$ is the same for all $\\ell$. Thus, $\\mathbb{E}[D^2] = L \\mathbb{E}[\\Delta_1^2]$.\nLet's compute $\\mathbb{E}[\\Delta_1^2]$ for a single locus (dropping the superscript $\\ell=1$ for clarity):\n$$ \\mathbb{E}[\\Delta^2] = \\mathbb{E}\\left[ \\left( \\frac{2}{M} \\left( \\sum_{i \\in G_R} y_i - \\sum_{j \\in G_L} y_j \\right) \\right)^2 \\right] = \\frac{4}{M^2} \\text{Var}\\left( \\sum_{i \\in G_R} y_i - \\sum_{j \\in G_L} y_j \\right) $$\nThe variance expands as:\n$$ \\text{Var}\\left( \\sum_{i \\in G_R} y_i - \\sum_{j \\in G_L} y_j \\right) = \\text{Var}\\left(\\sum_{i \\in G_R} y_i\\right) + \\text{Var}\\left(\\sum_{j \\in G_L} y_j\\right) - 2\\text{Cov}\\left(\\sum_{i \\in G_R} y_i, \\sum_{j \\in G_L} y_j\\right) $$\nLet's calculate each term. Let $A = \\sum_{j_1=0}^{M-1} \\sum_{j_2=0}^{M-1} \\rho^{|j_1-j_2|}$ and $B = \\sum_{i=M}^{2M-1} \\sum_{j=0}^{M-1} \\rho^{i-j}$. Then:\n$$ \\text{Var}\\left(\\sum_{j \\in G_L} y_j\\right) = \\sigma^2 A \\quad \\text{and by stationarity} \\quad \\text{Var}\\left(\\sum_{i \\in G_R} y_i\\right) = \\sigma^2 A $$\n$$ \\text{Cov}\\left(\\sum_{i \\in G_R} y_i, \\sum_{j \\in G_L} y_j\\right) = \\sigma^2 B $$\nPutting it together:\n$$ \\mathbb{E}[D^2] = L \\cdot \\frac{4}{M^2} \\left( \\sigma^2 A + \\sigma^2 A - 2\\sigma^2 B \\right) = \\frac{8L\\sigma^2}{M^2}(A-B) $$\n\nNow we must evaluate the sums $A$ and $B$. Standard formulas for such sums yield:\n$$ A = \\frac{M(1-\\rho^2) - 2\\rho(1-\\rho^M)}{(1-\\rho)^2} $$\n$$ B = \\frac{\\rho(1-\\rho^M)^2}{(1-\\rho)^2} $$\n\nNow, we compute the difference $A-B$:\n$$ A-B = \\frac{1}{(1-\\rho)^2} \\left[ M(1-\\rho^2) - 2\\rho(1-\\rho^M) - \\rho(1-\\rho^M)^2 \\right] $$\n$$ = \\frac{1}{(1-\\rho)^2} \\left[ M(1-\\rho^2) - \\rho(1-\\rho^M) (2 + (1-\\rho^M)) \\right] $$\n$$ = \\frac{1}{(1-\\rho)^2} \\left[ M(1-\\rho^2) - \\rho(1-\\rho^M)(3-\\rho^M) \\right] $$\nExpanding the numerator:\n$ N = M(1-\\rho^2) - \\rho(3 - 3\\rho^M - \\rho^M + \\rho^{2M}) = M(1-\\rho^2) - 3\\rho + 4\\rho^{M+1} - \\rho^{2M+1} $.\nSo,\n$$ A-B = \\frac{M(1-\\rho^2) - 3\\rho + 4\\rho^{M+1} - \\rho^{2M+1}}{(1-\\rho)^2} $$\nFinally, we substitute this into the expression for the expected squared distance:\n$$ \\mathbb{E}[D^2] = \\frac{8L\\sigma^2}{M^2}(A-B) = \\frac{8L\\sigma^2}{M^2(1-\\rho)^2} \\left( M(1-\\rho^2) - 3\\rho + 4\\rho^{M+1} - \\rho^{2M+1} \\right) $$\nThis is the final closed-form expression.",
            "answer": "$$ \\boxed{\\frac{8L\\sigma^2}{M^2(1-\\rho)^2} \\left( M(1-\\rho^2) - 3\\rho + 4\\rho^{M+1} - \\rho^{2M+1} \\right)} $$"
        },
        {
            "introduction": "The advent of phylogenomics has provided vast amounts of data for species delimitation, but also new statistical hurdles, such as non-independence among loci due to physical linkage. Many downstream analyses assume that loci are independent, and violating this assumption can lead to biased estimates and inflated statistical confidence. This practical coding exercise introduces a formal method for thinning a linked dataset to achieve a desired \"effective number of independent loci,\" equipping you with a robust tool to prepare genomic data for more reliable downstream analysis. ",
            "id": "2752737",
            "problem": "You are given Restriction site Associated DNA sequencing (RAD-seq) Single-Nucleotide Polymorphism (SNP) panels intended for species delimitation analysis, but the physical ordering of SNPs is unknown and thus the degree of Linkage Disequilibrium (LD) among adjacent SNPs is uncertain. To control for the inflation of variance in summary statistics that assume independent loci, you will implement a thinning strategy that retains every $t$-th SNP in index order (treating SNPs as an indexed sequence, not requiring physical position). Your design must control the effective number of independent loci and explicitly quantify the effect of thinning on variance estimates under a minimal, widely used correlation model.\n\nFundamental base and modeling assumptions:\n1. Each locus is modeled as a random variable with common variance $\\sigma^2$ and weak stationarity (constant mean and autocovariance that depends only on lag).\n2. The lag-$k$ correlation among adjacent loci in index order is modeled as $\\rho_k = r^k$, with $0 \\le r < 1$, i.e., a first-order autoregressive (AR(1)) correlation structure over the index. This serves as a conservative surrogate when physical distances are unknown, a common situation for RAD-seq contigs.\n3. For a stationary, weakly dependent sequence of length $n$ with lag correlations $\\rho_k$, the variance of the sample mean satisfies $\\mathrm{Var}(\\bar{X}) \\approx \\sigma^2 / n_{\\mathrm{eff}}$, where the effective sample size (effective number of independent loci) is\n$$\nn_{\\mathrm{eff}} \\approx \\frac{n}{1 + 2 \\sum_{k=1}^{\\infty} \\rho_k}.\n$$\nThis approximation is a standard result in statistics of correlated observations and is widely used for time series and genomic LD blocks.\n4. If you thin by stride $t \\in \\{1,2,\\dots,M\\}$, keeping the first SNP and then every $t$-th thereafter, the retained count is\n$$\nn' = \\left\\lceil \\frac{M}{t} \\right\\rceil,\n$$\nwhere $M$ is the total number of SNPs before thinning. Under the AR(1) index-correlation model, the correlation among retained loci has lag-$k$ correlation $\\rho_k' = (r^t)^k$, and thus\n$$\n\\sum_{k=1}^{\\infty} \\rho_k' = \\frac{r^t}{1 - r^t} \\quad \\text{for } 0 \\le r < 1.\n$$\nTherefore, the effective number of independent loci among the retained set is\n$$\nn_{\\mathrm{eff}}(t) \\approx n' \\cdot \\frac{1 - r^t}{1 + r^t}.\n$$\nThe variance inflation factor relative to independence for the retained set is\n$$\n\\mathrm{VIF}(t) = \\frac{n'}{n_{\\mathrm{eff}}(t)} = \\frac{1 + r^t}{1 - r^t}.\n$$\n\nYour programming task:\nGiven a test suite of tuples $(M, r, L_{\\mathrm{target}})$, where $M$ is the number of SNPs, $r$ is the adjacent-locus correlation parameter with $0 \\le r < 1$, and $L_{\\mathrm{target}}$ is the desired lower bound on the effective number of independent loci after thinning, design an algorithm to choose a thinning stride $t$ that maximizes thinning while meeting the effective-loci constraint:\n- If there exists at least one $t \\in \\{1,2,\\dots,M\\}$ such that $n_{\\mathrm{eff}}(t) \\ge L_{\\mathrm{target}}$, choose the largest such $t$ (i.e., the coarsest thinning that still meets the target).\n- If no such $t$ exists, report that the target is infeasible by setting the chosen stride to $-1$ and instead choose the $t$ that maximizes $n_{\\mathrm{eff}}(t)$ over $t \\in \\{1,2,\\dots,M\\}$; report this $t$ as the used stride together with its $n_{\\mathrm{eff}}$ and $\\mathrm{VIF}$.\n\nFor each test case, your program must output a list with four entries:\n- the chosen stride $t_{\\mathrm{choice}}$ (an integer, or $-1$ if the target is infeasible),\n- the used stride $t_{\\mathrm{used}}$ that determines the reported metrics (equals $t_{\\mathrm{choice}}$ if feasible, or the argmax of $n_{\\mathrm{eff}}(t)$ if infeasible),\n- the effective number of loci $n_{\\mathrm{eff}}(t_{\\mathrm{used}})$ rounded to three decimals,\n- the variance inflation factor $\\mathrm{VIF}(t_{\\mathrm{used}})$ rounded to three decimals.\n\nScientific realism and rationale:\n- Thinning cannot increase the total information beyond using all loci with proper modeling of LD, but it can reduce the variance inflation factor for analyses that assume independence, making variance estimates conservative and more robust in species delimitation pipelines that aggregate across loci.\n- For $r = 0$, you must treat $r^t = 0$, giving $n_{\\mathrm{eff}}(t) = n'$ and $\\mathrm{VIF}(t) = 1$.\n\nAngle units are not applicable. There are no physical units to report. All floats must be rounded to three decimals in the output.\n\nTest suite:\nUse exactly the following five cases as input, in this order:\n1. $(1000, 0.2, 300)$\n2. $(1000, 0.8, 100)$\n3. $(500, 0.0, 400)$\n4. $(75, 0.5, 20)$\n5. $(50, 0.95, 5)$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the case-specific list described above. For example: \n\"[[t_choice_case1,t_used_case1,neff_case1,VIF_case1],[t_choice_case2,t_used_case2,neff_case2,VIF_case2],...]\"\n\nNo user input is required; the program must hard-code the test suite above and print the results in the specified format.",
            "solution": "Fundamental base:\n1. Let $\\{X_i\\}_{i=1}^n$ be weakly stationary with $\\mathbb{E}[X_i] = \\mu$, $\\mathrm{Var}(X_i) = \\sigma^2$, and lag-$k$ correlation $\\rho_k = \\mathrm{Corr}(X_i, X_{i+k})$. For the sample mean $\\bar{X}_n = n^{-1} \\sum_{i=1}^n X_i$, the variance under weak dependence satisfies\n$$\n\\mathrm{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n} \\left(1 + 2 \\sum_{k=1}^{n-1} \\left(1 - \\frac{k}{n}\\right) \\rho_k \\right).\n$$\nFor large $n$ and summable correlations, this is well-approximated by\n$$\n\\mathrm{Var}(\\bar{X}_n) \\approx \\frac{\\sigma^2}{n_{\\mathrm{eff}}}, \\quad n_{\\mathrm{eff}} \\approx \\frac{n}{1 + 2 \\sum_{k=1}^{\\infty} \\rho_k}.\n$$\nThis effective sample size captures the information-equivalent count of independent observations and is a standard tool in time series and genomics where correlations are present.\n\n2. Under an autoregressive of order one (AR(1)) index-correlation model with parameter $r \\in [0,1)$, we have $\\rho_k = r^k$. Then\n$$\n\\sum_{k=1}^{\\infty} \\rho_k = \\sum_{k=1}^{\\infty} r^k = \\frac{r}{1 - r},\n$$\nand therefore\n$$\nn_{\\mathrm{eff}} \\approx \\frac{n}{1 + 2 \\cdot \\frac{r}{1 - r}} = n \\cdot \\frac{1 - r}{1 + r}.\n$$\n\nThinning and its effect:\nWe thin by stride $t \\in \\{1,2,\\dots,M\\}$, keeping the first locus and every $t$-th thereafter. This yields a retained count\n$$\nn' = \\left\\lceil \\frac{M}{t} \\right\\rceil,\n$$\nwhere $M$ is the total number of loci before thinning. Since the retained subsequence is sampled at lag $t$ in the index order, its lag-$k$ correlation becomes\n$$\n\\rho_k' = \\mathrm{Corr}(X_i, X_{i+kt}) = r^{kt} = (r^t)^k,\n$$\nwhich is again AR(1) with parameter $r^t$. Consequently,\n$$\n\\sum_{k=1}^{\\infty} \\rho_k' = \\frac{r^t}{1 - r^t}, \\quad n_{\\mathrm{eff}}(t) \\approx n' \\cdot \\frac{1 - r^t}{1 + r^t}.\n$$\nThe variance inflation factor (VIF) for the retained set relative to independence (i.e., relative to variance $\\sigma^2/n'$) is\n$$\n\\mathrm{VIF}(t) = \\frac{n'}{n_{\\mathrm{eff}}(t)} = \\frac{1 + r^t}{1 - r^t}.\n$$\nWhen $r = 0$, we have $r^t = 0$ for all $t$, yielding $n_{\\mathrm{eff}}(t) = n'$ and $\\mathrm{VIF}(t) = 1$, as expected for independent loci.\n\nOptimization criterion:\nWe are asked to control the effective number of loci via thinning. Given a target $L_{\\mathrm{target}}$, the constraint is\n$$\nn_{\\mathrm{eff}}(t) = \\left\\lceil \\frac{M}{t} \\right\\rceil \\cdot \\frac{1 - r^t}{1 + r^t} \\ge L_{\\mathrm{target}}.\n$$\nIf feasible, we are to maximize $t$ subject to the constraint, i.e.,\n$$\nt_{\\mathrm{choice}} = \\max \\left\\{ t \\in \\{1,2,\\dots,M\\} : \\left\\lceil \\frac{M}{t} \\right\\rceil \\cdot \\frac{1 - r^t}{1 + r^t} \\ge L_{\\mathrm{target}} \\right\\}.\n$$\nIf the feasible set is empty, we must declare infeasibility by outputting $t_{\\mathrm{choice}} = -1$ and instead select\n$$\nt_{\\mathrm{used}} \\in \\arg\\max_{t \\in \\{1,2,\\dots,M\\}} \\left( \\left\\lceil \\frac{M}{t} \\right\\rceil \\cdot \\frac{1 - r^t}{1 + r^t} \\right).\n$$\nThe reported effective count and VIF are then computed at $t_{\\mathrm{used}}$.\n\nAlgorithmic steps:\n1. For each test case $(M, r, L_{\\mathrm{target}})$, iterate $t$ from $1$ to $M$:\n   - Compute $n' = \\left\\lceil \\frac{M}{t} \\right\\rceil$ using integer arithmetic as $n' = \\left\\lfloor \\frac{M + t - 1}{t} \\right\\rfloor$.\n   - Compute $r^t$; if $r = 0$, treat $r^t = 0$ directly.\n   - Compute the factor $f(t) = \\frac{1 - r^t}{1 + r^t}$ and $n_{\\mathrm{eff}}(t) = n' \\cdot f(t)$.\n   - Track the set of feasible $t$ with $n_{\\mathrm{eff}}(t) \\ge L_{\\mathrm{target}}$; if non-empty, take $t_{\\mathrm{choice}}$ to be its maximum.\n   - Track the $t$ that maximizes $n_{\\mathrm{eff}}(t)$ to provide $t_{\\mathrm{used}}$ in the infeasible case.\n2. If the feasible set is non-empty, set $t_{\\mathrm{used}} = t_{\\mathrm{choice}}$ and compute $\\mathrm{VIF}(t_{\\mathrm{used}}) = \\frac{1 + r^{t_{\\mathrm{used}}}}{1 - r^{t_{\\mathrm{used}}}}$; else, set $t_{\\mathrm{choice}} = -1$ and use the argmax $t_{\\mathrm{used}}$ and the corresponding $n_{\\mathrm{eff}}$ and $\\mathrm{VIF}$.\n3. Round $n_{\\mathrm{eff}}(t_{\\mathrm{used}})$ and $\\mathrm{VIF}(t_{\\mathrm{used}})$ to three decimals for reporting.\n\nScientific justification:\n- The AR(1) model for index correlation provides a conservative decay of LD with index lag, which is reasonable for unknown ordering in RAD-seq; while it is an approximation, it captures that thinning by stride $t$ reduces short-range correlation to $r^t$, improving the independence approximation of retained loci.\n- The effective number of independent loci $n_{\\mathrm{eff}}(t)$ directly controls the variance of locus-averaged summary statistics commonly used in species delimitation methods (e.g., site frequency-based statistics, coalescent summary statistics), via $\\mathrm{Var}(\\bar{X}) \\approx \\sigma^2 / n_{\\mathrm{eff}}(t)$.\n- The variance inflation factor $\\mathrm{VIF}(t)$ quantifies the residual correlation among retained loci; thinning decreases $\\mathrm{VIF}(t)$ monotonically in $t$ because $r^t$ decreases with $t$, ensuring more reliable variance estimates under independence assumptions.\n\nApplying to the specified test suite:\n- Case $(1000, 0.2, 300)$: For $t = 3$, $n' = \\lceil 1000/3 \\rceil = 334$, $r^t = 0.2^3 = 0.008$, $f(3) = \\frac{1 - 0.008}{1 + 0.008} \\approx 0.984127$, giving $n_{\\mathrm{eff}}(3) \\approx 328.7 \\ge 300$, while $t = 4$ yields $n_{\\mathrm{eff}}(4) \\approx 242.1 < 300$, so $t_{\\mathrm{choice}} = 3$ and $\\mathrm{VIF}(3) \\approx 1.016$.\n- Case $(1000, 0.8, 100)$: The maximal $t$ with $n_{\\mathrm{eff}}(t) \\ge 100$ is $t = 5$, where $n' = 200$, $r^5 = 0.32768$, $f(5) \\approx 0.506521$, $n_{\\mathrm{eff}}(5) \\approx 101.3$, and $\\mathrm{VIF}(5) \\approx 1.974$.\n- Case $(500, 0.0, 400)$: Independence, so $n_{\\mathrm{eff}}(t) = n'$. The largest $t$ such that $n' \\ge 400$ is $t = 1$ (since $t = 2$ gives $n' = 250$), hence $t_{\\mathrm{choice}} = 1$, $n_{\\mathrm{eff}} = 500$, $\\mathrm{VIF} = 1$.\n- Case $(75, 0.5, 20)$: $t = 2$ gives $n' = 38$, $r^2 = 0.25$, $f(2) = 0.6$, so $n_{\\mathrm{eff}}(2) = 22.8 \\ge 20$; $t = 3$ gives $n_{\\mathrm{eff}}(3) \\approx 19.444 < 20$, hence $t_{\\mathrm{choice}} = 2$ and $\\mathrm{VIF}(2) = 1.667$.\n- Case $(50, 0.95, 5)$: The target is infeasible because $n_{\\mathrm{eff}}(t)$ never reaches $5$ for $t \\in \\{1,\\dots,50\\}$. We therefore set $t_{\\mathrm{choice}} = -1$ and choose $t_{\\mathrm{used}}$ to maximize $n_{\\mathrm{eff}}(t)$. A direct search shows the maximum is at $t = 24$ with $n' = \\lceil 50/24 \\rceil = 3$, $r^{24} \\approx 0.29199$, $f(24) \\approx 0.5480$, giving $n_{\\mathrm{eff}}(24) \\approx 1.644$ and $\\mathrm{VIF}(24) \\approx 1.825$.\n\nThe program will implement these steps exactly, compute the optimal strides, and print the case-wise results in the specified format with three-decimal rounding.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef effective_loci_and_vif(M: int, r: float, t: int):\n    \"\"\"\n    Compute n_eff(t) and VIF(t) for given M, r, and thinning stride t.\n    n' = ceil(M / t)\n    r_t = r**t\n    n_eff(t) = n' * (1 - r_t) / (1 + r_t)\n    VIF(t) = (1 + r_t) / (1 - r_t)\n    Special case: if r == 0.0, then r_t = 0.0, so n_eff = n' and VIF = 1.0\n    \"\"\"\n    n_retained = (M + t - 1) // t  # ceil division\n    if r <= 0.0:\n        r_t = 0.0\n    else:\n        r_t = r ** t\n    # Numerical safety: if r_t is extremely close to 1, cap within (0,1)\n    if r_t >= 1.0:\n        r_t = 1.0 - 1e-15\n    if r_t == 0.0:\n        n_eff = float(n_retained)\n        vif = 1.0\n    else:\n        factor = (1.0 - r_t) / (1.0 + r_t)\n        n_eff = n_retained * factor\n        vif = (1.0 + r_t) / (1.0 - r_t)\n    return n_eff, vif\n\ndef choose_thinning_stride(M: int, r: float, L_target: float):\n    \"\"\"\n    Choose the largest t in {1, ..., M} such that n_eff(t) >= L_target.\n    If no such t exists, return t_choice = -1 and t_used = argmax n_eff(t).\n    Returns (t_choice, t_used, n_eff_used, vif_used).\n    \"\"\"\n    feasible_t = None  # largest feasible t\n    best_t = 1\n    best_neff = -1.0\n\n    for t in range(1, M + 1):\n        n_eff, vif = effective_loci_and_vif(M, r, t)\n        # Track best n_eff across all t\n        if n_eff > best_neff + 1e-12:\n            best_neff = n_eff\n            best_t = t\n        # Check feasibility\n        if n_eff >= L_target - 1e-12:  # allow small tolerance\n            feasible_t = t  # since we iterate increasing t, this will end up as largest feasible\n\n    if feasible_t is not None:\n        # Use the largest feasible t\n        t_choice = feasible_t\n        t_used = feasible_t\n        n_eff_used, vif_used = effective_loci_and_vif(M, r, t_used)\n    else:\n        # Infeasible: use the t that maximizes n_eff\n        t_choice = -1\n        t_used = best_t\n        n_eff_used, vif_used = effective_loci_and_vif(M, r, t_used)\n\n    # Round as specified: three decimals\n    n_eff_used_rounded = round(n_eff_used + 1e-12, 3)\n    vif_used_rounded = round(vif_used + 1e-12, 3)\n    return [t_choice, t_used, n_eff_used_rounded, vif_used_rounded]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (M, r, L_target)\n    test_cases = [\n        (1000, 0.2, 300.0),\n        (1000, 0.8, 100.0),\n        (500, 0.0, 400.0),\n        (75, 0.5, 20.0),\n        (50, 0.95, 5.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        M, r, L_target = case\n        result = choose_thinning_stride(M, r, L_target)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Print as a single-line string representation of the list of lists.\n    # Ensure no extra spaces beyond commas to match the \"comma-separated list\" spirit.\n    def list_to_string(lst):\n        # Convert nested list of primitives to string without spaces after commas\n        if isinstance(lst, list):\n            return \"[\" + \",\".join(list_to_string(x) for x in lst) + \"]\"\n        else:\n            return str(lst)\n\n    print(list_to_string(results))\n\nsolve()\n```"
        }
    ]
}