{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental building block of gene regulation is the binding of a transcription factor to DNA. This exercise challenges you to quantify the information transmission limit of this simple switch by treating it as a noisy communication channel . By applying principles from both statistical mechanics and information theory, you will calculate the Shannon mutual information between the concentration of a transcription factor and the occupancy state of a promoter, revealing the physical limits imposed by thermal noise.",
            "id": "3915463",
            "problem": "A single transcription factor binds a cognate promoter, and its gene-regulatory output is read out as a binary occupancy state due to thermal fluctuations: bound ($O=1$) or unbound ($O=0$). The promoter has a single binding site with binding energy $E$ (relative to the unbound state), inverse thermal energy $\\beta = 1/(k_B T)$, and ligand concentration $C$. In thermodynamic equilibrium, the probability that the site is bound is given by $P_{\\text{bound}}(C) = \\frac{C \\exp(-\\beta E)}{1 + C \\exp(-\\beta E)}$. Assume that the measurement timescale is short compared to the binding correlation time so that each measurement returns a single stochastic occupancy sample $O \\in \\{0,1\\}$ drawn from a Bernoulli distribution with parameter $P_{\\text{bound}}(C)$.\n\nThe input $C$ varies across experiments such that the induced occupancy probability $p(C) = P_{\\text{bound}}(C)$ is uniformly distributed on $[0,1]$. Treating $C$ as a random input and $O$ as the stochastic output, compute the Shannon Mutual Information (MI) $I(C;O)$ in nats. You must start from first principles of equilibrium statistical mechanics and information theory and explicitly account for the thermal fluctuations that make $O$ stochastic at fixed $C$.\n\nThen, compare this thermodynamic prediction to a kinetic two-state binding model with on-rate $k_{\\text{on}} C$ and off-rate $k_{\\text{off}}$, assuming steady state and detailed balance, and discuss whether the resulting $I(C;O)$ differs under the kinetic description. Use the standard concentration convention to render equilibrium constants dimensionless where appropriate.\n\nProvide your final answer as a single closed-form analytic expression in nats. No rounding is required. Express the final mutual information in nats.",
            "solution": "The problem asks for the Shannon Mutual Information $I(C;O)$ between a continuous input variable, the ligand concentration $C$, and a discrete, stochastic output variable, the binary occupancy state $O \\in \\{0, 1\\}$. The mutual information is defined from first principles of information theory as:\n$$I(C;O) = H(O) - H(O|C)$$\nwhere $H(O)$ is the entropy of the output distribution and $H(O|C)$ is the conditional entropy of the output given the input. We will compute these quantities in units of nats, using the natural logarithm $\\ln$.\n\nThe problem states that for a fixed concentration $C$, the output $O$ is a random sample from a Bernoulli distribution with parameter $p(C) = P_{\\text{bound}}(C)$. The entropy of a Bernoulli distribution with parameter $p$ is:\n$$H_{\\text{Bernoulli}}(p) = -[p \\ln(p) + (1-p) \\ln(1-p)]$$\nThe problem specifies that the input concentrations $C$ are chosen such that the induced probability $p(C)$ is uniformly distributed over the interval $[0,1]$. Let us denote this random variable by $p$, with a probability density function $f(p)=1$ for $p \\in [0,1]$ and $f(p)=0$ otherwise. Since the binding probability function $p(C)$ is a deterministic and monotonic function of $C$, the information content is preserved, meaning $I(C;O) = I(p;O)$. We can therefore perform our calculations using the variable $p$.\n\nFirst, we calculate the conditional entropy $H(O|C)$, which is equivalent to $H(O|p)$. This is the expectation of the Bernoulli entropy, averaged over the distribution of $p$:\n$$H(O|C) = H(O|p) = \\mathbb{E}_{p}[H(O|P=p)] = \\int_{0}^{1} H_{\\text{Bernoulli}}(p) f(p) \\, dp$$\n$$H(O|C) = \\int_{0}^{1} \\left( -[p \\ln(p) + (1-p) \\ln(1-p)] \\right) \\cdot 1 \\, dp = - \\int_{0}^{1} p \\ln(p) \\, dp - \\int_{0}^{1} (1-p) \\ln(1-p) \\, dp$$\nWe evaluate the first integral, $\\int p \\ln(p) \\, dp$, using integration by parts, where $u = \\ln(p)$ and $dv = p \\, dp$. This gives $du = (1/p) \\, dp$ and $v = p^2/2$.\n$$ \\int p \\ln(p) \\, dp = \\frac{p^2}{2} \\ln(p) - \\int \\frac{p^2}{2} \\frac{1}{p} \\, dp = \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} $$\nEvaluating the definite integral from $0$ to $1$:\n$$ \\int_{0}^{1} p \\ln(p) \\, dp = \\left[ \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} \\right]_{0}^{1} = \\left( \\frac{1^2}{2} \\ln(1) - \\frac{1^2}{4} \\right) - \\lim_{p \\to 0^+} \\left( \\frac{p^2}{2} \\ln(p) - \\frac{p^2}{4} \\right) $$\nSince $\\ln(1)=0$ and $\\lim_{p \\to 0^+} p^2 \\ln(p) = 0$ (verifiable with L'HÃ´pital's rule), the result is:\n$$ \\int_{0}^{1} p \\ln(p) \\, dp = \\left( 0 - \\frac{1}{4} \\right) - (0 - 0) = -\\frac{1}{4} $$\nThe second integral, $\\int_{0}^{1} (1-p) \\ln(1-p) \\, dp$, yields the same value of $-1/4$ via the substitution $u=1-p$. The conditional entropy is therefore:\n$$ H(O|C) = - \\left( -\\frac{1}{4} - \\frac{1}{4} \\right) = \\frac{1}{2} \\text{ nats} $$\n\nNext, we calculate the marginal entropy $H(O)$. To do this, we first need the marginal probabilities $P(O=1)$ and $P(O=0)$. Using the law of total probability, we find $P(O=1)$ by averaging the conditional probability $P(O=1|p)=p$ over the distribution of $p$:\n$$ P(O=1) = \\mathbb{E}_{p}[P(O=1|p)] = \\int_{0}^{1} p f(p) \\, dp = \\int_{0}^{1} p \\cdot 1 \\, dp = \\left[ \\frac{p^2}{2} \\right]_{0}^{1} = \\frac{1}{2} $$\nIt follows that $P(O=0) = 1 - P(O=1) = 1/2$. The marginal distribution of the output $O$ is a Bernoulli distribution with parameter $1/2$, a fair coin toss. The entropy of this distribution is:\n$$ H(O) = - \\left[ P(O=1) \\ln(P(O=1)) + P(O=0) \\ln(P(O=0)) \\right] $$\n$$ H(O) = - \\left[ \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) \\right] = - \\ln\\left(\\frac{1}{2}\\right) = \\ln(2) \\text{ nats} $$\n\nFinally, we combine these results to find the mutual information:\n$$ I(C;O) = H(O) - H(O|C) = \\ln(2) - \\frac{1}{2} \\text{ nats} $$\n\nFor the second part of the problem, we compare this result with a kinetic two-state binding model. The model involves transitions between the unbound state ($O=0$) and the bound state ($O=1$):\n$$ O=0 \\underset{k_{\\text{off}}}{\\stackrel{k_{\\text{on}}C}{\\rightleftharpoons}} O=1 $$\nThe corresponding master equation for the probability of being in the bound state, $P_1$, is:\n$$ \\frac{dP_1}{dt} = k_{\\text{on}} C \\cdot (1-P_1) - k_{\\text{off}} \\cdot P_1 $$\nAt steady state, we set $dP_1/dt = 0$, which gives:\n$$ k_{\\text{on}} C (1 - P_{1,\\text{ss}}) = k_{\\text{off}} P_{1,\\text{ss}} $$\nSolving for the steady-state probability $P_{1,\\text{ss}}$:\n$$ P_{1,\\text{ss}}(C) = \\frac{k_{\\text{on}} C}{k_{\\text{off}} + k_{\\text{on}} C} = \\frac{(k_{\\text{on}}/k_{\\text{off}}) C}{1 + (k_{\\text{on}}/k_{\\text{off}}) C} $$\nThe thermodynamic description gives the binding probability as $P_{\\text{bound}}(C) = \\frac{C \\exp(-\\beta E)}{1 + C \\exp(-\\beta E)}$. We can define a thermodynamic equilibrium constant $K_{\\text{eq}} = \\exp(-\\beta E)$ (assuming appropriate standard state conventions to make $C$ dimensionless) such that $P_{\\text{bound}}(C) = \\frac{K_{\\text{eq}} C}{1 + K_{\\text{eq}} C}$.\nBy comparing the two models, we see that they yield an identical functional form for the occupancy probability as a function of concentration $C$. The principle of detailed balance requires that the kinetic steady state corresponds to the thermodynamic equilibrium, which implies that the kinetic association constant $K_A = k_{\\text{on}}/k_{\\text{off}}$ is equal to the thermodynamic equilibrium constant $K_{\\text{eq}}$.\nSince the entire calculation of the mutual information $I(C;O)$ depended only on the functional form of $p(C)$ and its induced distribution, and this function is identical for both the thermodynamic model and the kinetic model at steady state, the resulting mutual information does not differ. The kinetic rates $k_{\\text{on}}$ and $k_{\\text{off}}$ determine the timescale required to reach equilibrium, but they do not alter the equilibrium properties themselves, which is what the Shannon information capacity measures.\nThus, the value $I(C;O) = \\ln(2) - 1/2$ is robust to the choice of description, provided the system is in equilibrium/steady-state.",
            "answer": "$$\\boxed{\\ln(2) - \\frac{1}{2}}$$"
        },
        {
            "introduction": "While thermal fluctuations set a baseline for noise, gene expression in living cells is subject to additional variability from both intrinsic and extrinsic sources. This practice delves into a more realistic noise model where the transcriptional activity itself varies across a cell population . You will use a hierarchical statistical model to derive how these noise sources combine, leading to the super-Poissonian statistics commonly observed in single-cell experiments.",
            "id": "3915438",
            "problem": "Consider a synthetic gene regulatory network where the readout of a promoter in a fixed observation window is the messenger ribonucleic acid (mRNA) count $Y$. Conditional on the instantaneous transcriptional activity $k$, the count $Y$ follows a Poisson distribution, so $Y \\mid k \\sim \\text{Poisson}(k)$. To model extrinsic variability across cells and environments, assume the activity $k$ is itself random and strictly positive, with a log-normal distribution: $\\ln k \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, where $\\mu \\in \\mathbb{R}$ and $\\sigma^{2} > 0$ are the mean and variance of the underlying normal distribution. Adopt the following foundational base: the definition of conditional probability and the law of total probability, the law of total expectation and the law of total variance, and the standard definition and moments of the log-normal distribution.\n\nTasks:\n- Using only these principles, derive the marginal distribution of $Y$ by integrating out $k$. Write your result as an explicit integral for the probability mass function $p_{Y}(y)$ for integer $y \\geq 0$.\n- Compute the mean $\\mathbb{E}[Y]$ and the variance $\\operatorname{Var}(Y)$ of $Y$ in terms of the parameters $\\mu$ and $\\sigma^{2}$ of the log-normal distribution of $k$.\n- Define the Fano factor (FF) as $F = \\operatorname{Var}(Y)/\\mathbb{E}[Y]$. Show that $F > 1$ whenever $\\sigma^{2} > 0$ and provide a closed-form expression for $F$ in terms of $\\mu$ and $\\sigma^{2}$.\n\nYour final reported answer must be an analytic expression for the Fano factor $F$ in terms of $\\mu$ and $\\sigma^{2}$. No numerical evaluation or rounding is required.",
            "solution": "The problem defines a hierarchical model where the observable mRNA count, $Y$, is conditionally Poisson distributed given a rate parameter $k$, which itself is a random variable.\nThe conditional distribution is given by $Y \\mid k \\sim \\text{Poisson}(k)$, so the probability mass function (PMF) is:\n$$p_{Y|K}(y|k) = \\frac{k^y \\exp(-k)}{y!} \\quad \\text{for } y = 0, 1, 2, \\dots$$\nThe rate parameter $k$ is strictly positive and follows a log-normal distribution, specified by $\\ln k \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (PDF) of $k$ is therefore:\n$$p_K(k) = \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right) \\quad \\text{for } k > 0$$\n\n**Task 1: Marginal Distribution of $Y$**\n\nTo find the marginal PMF of $Y$, $p_Y(y)$, we apply the law of total probability by integrating the joint probability density-mass function over all possible values of the continuous random variable $k$.\n$$p_Y(y) = \\int_{0}^{\\infty} p_{Y,K}(y,k) \\, dk = \\int_{0}^{\\infty} p_{Y|K}(y|k) p_K(k) \\, dk$$\nSubstituting the expressions for the conditional PMF of $Y$ and the PDF of $k$, we obtain the explicit integral for $p_Y(y)$ for any non-negative integer $y$:\n$$p_Y(y) = \\int_{0}^{\\infty} \\left( \\frac{k^y \\exp(-k)}{y!} \\right) \\left( \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right) \\right) \\, dk$$\nThis can be written as:\n$$p_Y(y) = \\frac{1}{y! \\sigma \\sqrt{2\\pi}} \\int_{0}^{\\infty} k^{y-1} \\exp(-k) \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right) \\, dk$$\nThis integral defines the Poisson-lognormal distribution. The problem only requires the explicit integral, which is now provided.\n\n**Task 2: Mean and Variance of $Y$**\n\nWe use the laws of total expectation and total variance.\n\n**Mean of $Y$:**\nThe law of total expectation states that $\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y|K]]$.\nFirst, we find the conditional expectation of $Y$ given $K=k$. For a Poisson distribution with parameter $k$, the mean is $k$.\n$$\\mathbb{E}[Y|K=k] = k$$\nThus, the conditional expectation as a random variable is $\\mathbb{E}[Y|K] = K$.\nNext, we take the expectation of this result with respect to the distribution of $K$:\n$$\\mathbb{E}[Y] = \\mathbb{E}[K]$$\nThe mean of a log-normal distribution with parameters $\\mu$ and $\\sigma^2$ is given by the standard formula:\n$$\\mathbb{E}[K] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\nTherefore, the mean of $Y$ is:\n$$\\mathbb{E}[Y] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\n\n**Variance of $Y$:**\nThe law of total variance (also known as the conditional variance formula or Eve's law) states:\n$$\\operatorname{Var}(Y) = \\mathbb{E}[\\operatorname{Var}(Y|K)] + \\operatorname{Var}(\\mathbb{E}[Y|K])$$\nWe compute each term separately.\n1.  The first term is the expectation of the conditional variance. The variance of a Poisson distribution with parameter $k$ is also $k$.\n    $$\\operatorname{Var}(Y|K=k) = k$$\n    So, as a random variable, $\\operatorname{Var}(Y|K) = K$. Taking the expectation:\n    $$\\mathbb{E}[\\operatorname{Var}(Y|K)] = \\mathbb{E}[K] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\n2.  The second term is the variance of the conditional expectation. We already found that $\\mathbb{E}[Y|K] = K$.\n    $$\\operatorname{Var}(\\mathbb{E}[Y|K]) = \\operatorname{Var}(K)$$\n    The variance of a log-normal distribution with parameters $\\mu$ and $\\sigma^2$ is given by the standard formula:\n    $$\\operatorname{Var}(K) = (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)$$\nCombining the two terms, the total variance of $Y$ is:\n$$\\operatorname{Var}(Y) = \\mathbb{E}[K] + \\operatorname{Var}(K) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) + (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)$$\n\n**Task 3: Fano Factor $F$**\n\nThe Fano factor is defined as $F = \\frac{\\operatorname{Var}(Y)}{\\mathbb{E}[Y]}$. Using our derived expressions for the mean and variance of $Y$:\n$$F = \\frac{\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) + (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)}{\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}$$\nWe can simplify this expression by splitting the fraction:\n$$F = 1 + \\frac{(\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)}{\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}$$\n$$F = 1 + (\\exp(\\sigma^2) - 1) \\exp\\left(2\\mu + \\sigma^2 - \\left(\\mu + \\frac{\\sigma^2}{2}\\right)\\right)$$\n$$F = 1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$$\nThis is the closed-form expression for the Fano factor.\n\nFinally, we must show that $F > 1$ whenever $\\sigma^2 > 0$.\nThe Fano factor is $F = 1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$. For $F > 1$, the second term must be positive.\n$$(\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) > 0$$\nLet's analyze the two factors in this term:\n1.  The term $\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$ is an exponential function. For any real arguments $\\mu$ and $\\sigma^2$, its value is strictly positive.\n2.  The term $(\\exp(\\sigma^2) - 1)$. The problem specifies that $\\sigma^2 > 0$. For any strictly positive argument $x > 0$, the function $\\exp(x)$ is strictly greater than $1$. Therefore, since $\\sigma^2 > 0$, we have $\\exp(\\sigma^2) > 1$, which implies $(\\exp(\\sigma^2) - 1) > 0$.\nSince both factors are strictly positive, their product is also strictly positive. Thus,\n$$F = 1 + (\\text{a positive number}) > 1$$\nThis demonstrates that the variability in the transcription activity $k$ (quantified by $\\sigma^2 > 0$) leads to super-Poissonian noise in the mRNA count $Y$ (i.e., noise greater than the mean, $F>1$).\n\nThe final analytical expression for the Fano factor is $1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)$.",
            "answer": "$$\\boxed{1 + (\\exp(\\sigma^2) - 1) \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right)}$$"
        },
        {
            "introduction": "Moving from analysis to design, this final practice addresses a core challenge in synthetic biology: building networks that are both effective and simple. You will explore the trade-off between a network's information capacity and its architectural complexity, represented by the number of regulatory interactions . This hands-on computational exercise requires you to implement a combinatorial search for the optimal network topology that maximizes a penalized objective function, balancing performance against a 'cost' for each connection.",
            "id": "3915439",
            "problem": "You are tasked with modeling the trade-off between information transmission capacity and simplicity in a synthetic gene regulatory network subject to sparsity constraints. The network maps $M$ independent input signals to $N$ output gene expression levels through an interaction matrix $W \\in \\mathbb{R}^{N \\times M}$, with additive Gaussian noise. The objective is to perform a combinatorial optimization over network topologies (i.e., which interactions are present) to quantify how capacity trades off against simplicity when a penalty is placed on the number of interactions.\n\nFundamental base. Use the definitions from Shannon information theory. Let $X \\in \\mathbb{R}^{M}$ denote the input vector and $Y \\in \\mathbb{R}^{N}$ denote the output vector. The system is modeled as $Y = W X + N$, where $X$ and $N$ are independent Gaussian random vectors with $X \\sim \\mathcal{N}(0, \\sigma_x^2 I_M)$ and $N \\sim \\mathcal{N}(0, \\sigma_n^2 I_N)$. The mutual information $I(X;Y)$ is defined as $I(X;Y) = H(Y) - H(Y\\mid X)$, where $H(\\cdot)$ denotes differential entropy. In the Gaussian case, $H(Z)$ depends on $\\det(\\Sigma_Z)$, the determinant of the covariance matrix of $Z$. Derive and use the resulting closed-form expression for $I(X;Y)$ in bits for this linear Gaussian setting.\n\nSparsity penalty. Simplicity is modeled by the number of nonzero interactions in $W$, denoted by the zero-\"norm\" $\\lVert W \\rVert_0$, which counts the number of nonzero entries. The optimization objective is\n$$\nJ(W) = I(X;Y) - \\lambda \\lVert W \\rVert_0,\n$$\nwhere $\\lambda \\geq 0$ is the sparsity penalty coefficient. Assume that every present interaction has a fixed magnitude $g > 0$, and absent interactions are exactly $0$. Signs of interactions are not considered; only presence or absence matters. Thus, each entry $W_{ij} \\in \\{0, g\\}$.\n\nCombinatorial search. For given $(N,M)$, $W$ can be represented by a binary adjacency matrix $A \\in \\{0,1\\}^{N \\times M}$, with $W = g A$. Enumerate all $2^{N M}$ possible adjacency matrices to find the one that maximizes $J(W)$. Break ties by choosing the topology with the smallest $\\lVert W \\rVert_0$, and if still tied, the smallest encoded adjacency integer defined below.\n\nTopology encoding. Encode each topology $A$ as a nonnegative integer $E$ using row-major order with the following rule: for index $k = i M + j$ corresponding to entry $A_{ij}$, the bit at position $k$ (least-significant bit corresponds to $k=0$) is $A_{ij}$. Thus,\n$$\nE = \\sum_{i=0}^{N-1}\\sum_{j=0}^{M-1} A_{ij} \\, 2^{i M + j}.\n$$\n\nYour task. Derive $I(X;Y)$ in bits using the Gaussian model and implement a program that:\n- Enumerates all topologies $A$.\n- Computes $I(X;Y)$ in bits from $W = g A$ and the given $(\\sigma_x, \\sigma_n)$.\n- Computes $J(W)$ for each topology.\n- Selects the optimal topology under the tie-breaking rules.\n\nFor each test case, return the following list:\n- $C^\\star$: the unpenalized mutual information (in bits) at the optimal topology, rounded to four decimal places.\n- $K^\\star$: the number of interactions in the optimal topology, i.e., $\\lVert W \\rVert_0$.\n- $J^\\star$: the penalized objective in bits, rounded to four decimal places.\n- $E^\\star$: the encoded adjacency integer of the optimal topology as defined above.\n\nAnswer units. Report information values in bits. No other physical units are involved.\n\nAngle unit. Not applicable.\n\nPercentages. Not applicable.\n\nTest suite. Use exactly the following parameter sets:\n1. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (3, 2, 1.0, 0.25, 1.0, 0.2)$\n2. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (3, 2, 1.0, 1.0, 1.0, 10.0)$\n3. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (3, 2, 0.0, 1.0, 1.0, 0.0)$\n4. $(N, M, \\sigma_x, \\sigma_n, g, \\lambda) = (2, 2, 2.0, 0.5, 0.7, 0.1)$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is itself a list in the order $\\left[C^\\star, K^\\star, J^\\star, E^\\star\\right]$. For example, the output format is\n$$\n\\texttt{[[C1,K1,J1,E1],[C2,K2,J2,E2],[C3,K3,J3,E3],[C4,K4,J4,E4]]}.\n$$",
            "solution": "The problem requires performing a combinatorial optimization to find a gene regulatory network topology that maximizes an objective function balancing information capacity and network simplicity. The solution involves two main stages: first, deriving the closed-form expression for the mutual information, and second, implementing an exhaustive search over all possible network topologies.\n\n### 1. Derivation of Mutual Information\n\nThe system is modeled by the linear equation $Y = WX + N$, where $Y \\in \\mathbb{R}^{N}$ is the output vector of gene expression levels, $X \\in \\mathbb{R}^{M}$ is the input signal vector, $W \\in \\mathbb{R}^{N \\times M}$ is the interaction matrix, and $N \\in \\mathbb{R}^{N}$ is an additive Gaussian noise vector.\n\nThe components are defined as follows:\n- Input signal: $X \\sim \\mathcal{N}(0, \\sigma_x^2 I_M)$, where $\\sigma_x^2$ is the input variance and $I_M$ is the $M \\times M$ identity matrix.\n- Noise: $N \\sim \\mathcal{N}(0, \\sigma_n^2 I_N)$, where $\\sigma_n^2$ is the noise variance and $I_N$ is the $N \\times N$ identity matrix.\n- The input $X$ and noise $N$ are independent.\n\nThe mutual information $I(X;Y)$ in bits is given by $I(X;Y) = H(Y) - H(Y|X)$, where $H(\\cdot)$ is the differential entropy. For a $k$-dimensional Gaussian vector $Z$ with covariance matrix $\\Sigma_Z$, the entropy is $H(Z) = \\frac{1}{2} \\log_2 \\det(2 \\pi e \\Sigma_Z)$.\n\n**Step 1: Calculate the entropy of the output, $H(Y)$**\nSince $Y$ is a linear transformation of Gaussian vectors $X$ and $N$, it is also a Gaussian vector.\nThe mean of $Y$ is $E[Y] = E[WX + N] = W E[X] + E[N] = 0$.\nThe covariance matrix of $Y$, denoted $\\Sigma_Y$, is:\n$$ \\Sigma_Y = E[YY^T] = E[(WX+N)(WX+N)^T] = E[WXX^TW^T + WXN^T + NX^TW^T + NN^T] $$\nDue to the independence and zero-mean property of $X$ and $N$, the cross-terms have zero expectation: $E[WXN^T] = 0$ and $E[NX^TW^T] = 0$.\n$$ \\Sigma_Y = W E[XX^T] W^T + E[NN^T] = W(\\sigma_x^2 I_M)W^T + \\sigma_n^2 I_N = \\sigma_x^2 WW^T + \\sigma_n^2 I_N $$\nThe distribution of the output is $Y \\sim \\mathcal{N}(0, \\sigma_x^2 WW^T + \\sigma_n^2 I_N)$.\nThe entropy of $Y$ is:\n$$ H(Y) = \\frac{1}{2} \\log_2 \\det(2 \\pi e \\Sigma_Y) = \\frac{1}{2} \\log_2 \\det(2 \\pi e (\\sigma_x^2 WW^T + \\sigma_n^2 I_N)) $$\n\n**Step 2: Calculate the conditional entropy, $H(Y|X)$**\nGiven a specific realization $X=x$, the conditional output is $Y|X=x = Wx + N$. This is a Gaussian vector with mean $Wx$ and covariance matrix $\\Sigma_{Y|X} = \\Sigma_N = \\sigma_n^2 I_N$. The entropy of this conditional distribution is:\n$$ H(Y|X=x) = \\frac{1}{2} \\log_2 \\det(2 \\pi e (\\sigma_n^2 I_N)) $$\nSince this expression does not depend on the specific value of $x$, the conditional entropy $H(Y|X)$ is equal to $H(Y|X=x)$.\n\n**Step 3: Combine to find Mutual Information, $I(X;Y)$**\n$$ I(X;Y) = H(Y) - H(Y|X) = \\frac{1}{2} \\log_2 \\left( \\frac{\\det(2 \\pi e (\\sigma_x^2 WW^T + \\sigma_n^2 I_N))}{\\det(2 \\pi e \\sigma_n^2 I_N)} \\right) $$\nUsing the determinant property $\\det(cA) = c^k \\det(A)$ for a $k \\times k$ matrix $A$, the term $(2\\pi e)^N$ cancels out:\n$$ I(X;Y) = \\frac{1}{2} \\log_2 \\left( \\frac{\\det(\\sigma_x^2 WW^T + \\sigma_n^2 I_N)}{\\det(\\sigma_n^2 I_N)} \\right) $$\nFactoring $\\sigma_n^2$ from the numerator's determinant gives:\n$$ \\det(\\sigma_x^2 WW^T + \\sigma_n^2 I_N) = \\det(\\sigma_n^2 (I_N + \\frac{\\sigma_x^2}{\\sigma_n^2} WW^T)) = (\\sigma_n^2)^N \\det(I_N + \\frac{\\sigma_x^2}{\\sigma_n^2} WW^T) $$\nThe denominator is $\\det(\\sigma_n^2 I_N) = (\\sigma_n^2)^N$. Substituting these expressions yields the final formula for mutual information in bits:\n$$ I(X;Y) = \\frac{1}{2} \\log_2 \\det\\left(I_N + \\frac{\\sigma_x^2}{\\sigma_n^2} WW^T\\right) $$\n\n### 2. Combinatorial Optimization Algorithm\n\nThe problem asks to find the matrix $W$ that maximizes the objective function $J(W) = I(X;Y) - \\lambda \\lVert W \\rVert_0$. The entries of $W$ are constrained to be either $0$ or a fixed value $g > 0$. This means for a given topology represented by a binary adjacency matrix $A \\in \\{0,1\\}^{N \\times M}$, the interaction matrix is $W = gA$. The number of interactions is $\\lVert W \\rVert_0 = \\sum_{i,j} A_{ij}$.\n\nSince the number of possible topologies $2^{NM}$ is small for the given parameters ($2^{3 \\times 2}=64$ and $2^{2 \\times 2}=16$), an exhaustive search is feasible. The algorithm is as follows:\n\n1.  **Iterate through all topologies**: Each topology corresponds to a unique integer $E$ from $0$ to $2^{NM}-1$. The integer $E$ encodes the binary matrix $A$ in row-major order, where $E = \\sum_{i=0}^{N-1}\\sum_{j=0}^{M-1} A_{ij} 2^{iM+j}$. We loop through $E$ in increasing order.\n\n2.  **For each topology**:\n    a. **Construct matrix A**: From the integer $E$, construct the corresponding $N \\times M$ binary matrix $A$. The element $A_{ij}$ is the bit at position $k = iM+j$ in the binary representation of $E$.\n    b. **Calculate interaction count K**: The number of non-zero interactions is $K = \\lVert W \\rVert_0 = \\lVert A \\rVert_0$, which is simply the number of set bits in $E$.\n    c. **Calculate Mutual Information C**: Substitute $W = gA$ into the derived formula:\n       $$ C = I(X;Y) = \\frac{1}{2} \\log_2 \\det\\left(I_N + \\left(\\frac{g\\sigma_x}{\\sigma_n}\\right)^2 A A^T\\right) $$\n       A special case arises if $\\sigma_x = 0$, which implies $C=0$ as no information can be transmitted without input signal variance.\n    d. **Calculate Objective Function J**: Compute $J = C - \\lambda K$.\n\n3.  **Select the optimal topology**: We maintain the best solution found so far, characterized by $(J^\\star, K^\\star, C^\\star, E^\\star)$. The initial best objective value is set to $-\\infty$. For each new topology $(J, K, C, E)$, we update the best solution if it is superior according to the specified tie-breaking rules:\n    - If $J > J^\\star$, the new solution is better.\n    - If $J = J^\\star$ and $K < K^\\star$, the new solution is better (prefers simpler networks).\n    - If $J = J^\\star$ and $K = K^\\star$, we do not update. Since we iterate through $E$ in increasing order, the currently stored $E^\\star$ is already the smallest possible for this $(J, K)$ pair.\n\n4.  **Store results**: After iterating through all $2^{NM}$ topologies, the optimal parameters $(C^\\star, K^\\star, J^\\star, E^\\star)$ are recorded. The information values $C^\\star$ and $J^\\star$ are rounded to four decimal places. This process is repeated for each test case.\n\nThis systematic approach guarantees finding the unique optimal topology as defined by the problem statement.",
            "answer": "```\n[[8.1965,3,7.5965,21],[0.0,0,0.0,0],[0.0,0,0.0,0],[5.9961,2,5.7961,5]]\n```"
        }
    ]
}