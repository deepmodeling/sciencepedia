{
    "hands_on_practices": [
        {
            "introduction": "The best way to master a computational method is to build it from the ground up. This first practice guides you through implementing the most fundamental Approximate Bayesian Computation (ABC) algorithm: rejection sampling. You will apply it to infer parameters for the telegraph model, a cornerstone of stochastic gene expression, solidifying your understanding of the core ABC workflow of simulating, comparing, and accepting or rejecting parameters .",
            "id": "3906441",
            "problem": "You are to implement a complete rejection Approximate Bayesian Computation (ABC) algorithm for a two-state telegraph gene expression model. The telegraph model captures stochastic gene activation and deactivation with messenger RNA (mRNA) production occurring only in the active state. Your program must simulate the model with the Stochastic Simulation Algorithm (also known as Gillespie’s algorithm) and must use rejection ABC with a termination criterion based on the number of accepted samples. The implementation must be self-contained and run without user input.\n\nModel specification grounded in standard stochastic chemical kinetics:\n- The gene switches between an inactive state and an active state with transition rates $k_{\\text{on}}$ and $k_{\\text{off}}$ (in $\\text{min}^{-1}$), respectively.\n- When active, the gene produces mRNA at rate $s$ (in $\\text{transcripts} \\cdot \\text{min}^{-1}$).\n- mRNA degrades at rate $\\gamma$ (in $\\text{min}^{-1}$) per molecule.\n- This is a continuous-time Markov process with state $(g,m)$ where the gene activity state is $g \\in \\{0,1\\}$ and the mRNA count is $m \\in \\mathbb{N}_0$.\n- Reaction channels and their hazard rates (propensities) are:\n  - Gene activation: $(g:0 \\rightarrow 1)$ at rate $k_{\\text{on}} \\cdot \\mathbb{1}\\{g=0\\}$.\n  - Gene deactivation: $(g:1 \\rightarrow 0)$ at rate $k_{\\text{off}} \\cdot \\mathbb{1}\\{g=1\\}$.\n  - Transcription: $(m \\rightarrow m+1)$ at rate $s \\cdot \\mathbb{1}\\{g=1\\}$.\n  - Degradation: $(m \\rightarrow m-1)$ at rate $\\gamma \\cdot m$.\n\nSummary statistics and distance:\n- Given a time series of mRNA counts sampled at fixed intervals over a time horizon $T$ with sampling interval $\\Delta t$, define the summary statistic vector as $S(x) = (\\overline{m}, \\operatorname{Var}(m))$, where $\\overline{m}$ is the sample mean and $\\operatorname{Var}(m)$ is the population variance across all sampled time points.\n- Use the Euclidean distance $d(S(x), S(y)) = \\sqrt{(\\overline{m}_x - \\overline{m}_y)^2 + (\\operatorname{Var}(m_x) - \\operatorname{Var}(m_y))^2}$.\n\nPrior distributions (independent) for parameters, all in physically meaningful ranges:\n- $k_{\\text{on}} \\sim \\text{Uniform}(a_{\\text{on}}, b_{\\text{on}})$ with $(a_{\\text{on}}, b_{\\text{on}}) = (0.05, 2.0)$ in $\\text{min}^{-1}$.\n- $k_{\\text{off}} \\sim \\text{Uniform}(a_{\\text{off}}, b_{\\text{off}})$ with $(a_{\\text{off}}, b_{\\text{off}}) = (0.05, 2.0)$ in $\\text{min}^{-1}$.\n- $s \\sim \\text{Uniform}(a_s, b_s)$ with $(a_s, b_s) = (1.0, 20.0)$ in $\\text{transcripts} \\cdot \\text{min}^{-1}$.\n- $\\gamma \\sim \\text{Uniform}(a_{\\gamma}, b_{\\gamma})$ with $(a_{\\gamma}, b_{\\gamma}) = (0.1, 1.0)$ in $\\text{min}^{-1}$.\n\nRejection Approximate Bayesian Computation (ABC) task:\n- Implement rejection ABC that:\n  - Draws parameter samples $\\theta = (k_{\\text{on}}, k_{\\text{off}}, s, \\gamma)$ from the prior.\n  - Simulates a telegraph trajectory using Gillespie’s algorithm under $\\theta$ over horizon $T$ with sampling interval $\\Delta t$. Start from $g(0) = 0$ and $m(0) = 0$.\n  - Computes $S(x_{\\theta})$ and the distance $d(S(x_{\\theta}), S(x_{\\text{obs}}))$ to the observed summary $S(x_{\\text{obs}})$.\n  - Accepts $\\theta$ if $d \\le \\varepsilon$.\n  - Terminates only when the number of accepted samples reaches the specified target $N$.\n- The program must return, for each test case, the posterior mean of the accepted parameters as a list $[\\overline{k_{\\text{on}}}, \\overline{k_{\\text{off}}}, \\overline{s}, \\overline{\\gamma}]$, where each component is expressed in $\\text{min}^{-1}$ for rates and in $\\text{transcripts} \\cdot \\text{min}^{-1}$ for $s$. The numerical outputs are floats and should not include unit strings.\n\nTime units:\n- All times $T$ and $\\Delta t$ are in minutes.\n- All rate parameters are per minute, and $s$ is in $\\text{transcripts} \\cdot \\text{min}^{-1}$.\n- Angles are not involved in this task.\n- If any fraction is needed, express it as a decimal.\n\nTest suite:\n- For each test case, first generate an observed dataset $x_{\\text{obs}}$ by simulating the telegraph model with the stated true parameters, horizon $T$, and sampling interval $\\Delta t$, using the provided random seed for observed data generation. Then run the ABC algorithm using the indicated ABC random seed. Use the priors as specified above for all test cases.\n\n- Test case $1$ (happy path):\n  - True parameters: $(k_{\\text{on}}, k_{\\text{off}}, s, \\gamma) = (0.5, 0.7, 12.0, 0.2)$.\n  - Horizon and sampling: $T = 10.0$, $\\Delta t = 0.5$.\n  - ABC tolerance: $\\varepsilon = 30.0$.\n  - Target accepted samples: $N = 20$.\n  - Random seeds: observed generation seed $= 314159$, ABC seed $= 271828$.\n\n- Test case $2$ (boundary on $N$ with very permissive tolerance):\n  - True parameters: $(k_{\\text{on}}, k_{\\text{off}}, s, \\gamma) = (1.0, 0.2, 8.0, 0.5)$.\n  - Horizon and sampling: $T = 10.0$, $\\Delta t = 0.5$.\n  - ABC tolerance: $\\varepsilon = 1000000.0$.\n  - Target accepted samples: $N = 1$.\n  - Random seeds: observed generation seed $= 42$, ABC seed $= 4242$.\n\n- Test case $3$ (short observation window, edge-like data with low expression):\n  - True parameters: $(k_{\\text{on}}, k_{\\text{off}}, s, \\gamma) = (0.05, 1.8, 2.0, 0.9)$.\n  - Horizon and sampling: $T = 3.0$, $\\Delta t = 0.3$.\n  - ABC tolerance: $\\varepsilon = 40.0$.\n  - Target accepted samples: $N = 10$.\n  - Random seeds: observed generation seed $= 7$, ABC seed $= 77$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets where each element corresponds to one test case’s result. Each test case result must be the list $[\\overline{k_{\\text{on}}}, \\overline{k_{\\text{off}}}, \\overline{s}, \\overline{\\gamma}]$ in the units specified above, encoded as plain floats without unit strings. For example, an output with three test cases should look like:\n- Example format: $[[v_{11}, v_{12}, v_{13}, v_{14}],[v_{21}, v_{22}, v_{23}, v_{24}],[v_{31}, v_{32}, v_{33}, v_{34}]]$.",
            "solution": "The problem requires the implementation of a rejection Approximate Bayesian Computation (ABC) algorithm to infer the parameters of a two-state telegraph model of gene expression. The solution involves simulating the model using the Gillespie Stochastic Simulation Algorithm (SSA), defining appropriate summary statistics and a distance function, and applying the ABC rejection scheme to approximate the posterior distribution of the model parameters. The posterior mean for each of three specified test cases constitutes the final answer.\n\n**1. The Stochastic Telegraph Model**\n\nThe telegraph model is a fundamental stochastic model for gene expression. It describes the dynamics of the number of messenger RNA (mRNA) molecules, denoted by $m \\in \\mathbb{N}_0$, produced by a single gene. The gene itself stochastically switches between an inactive state ($g=0$) and an active state ($g=1$). The state of the system is a tuple $(g, m)$.\n\nThe dynamics are governed by four elemental chemical reactions, which define a continuous-time Markov process:\n- **Gene Activation**: The gene switches from inactive to active.\n  $$ G_{inactive} \\xrightarrow{k_{on}} G_{active} $$\n  The propensity (hazard rate) for this reaction is $a_1 = k_{on} \\cdot \\mathbb{1}\\{g=0\\}$, where $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n\n- **Gene Deactivation**: The gene switches from active to inactive.\n  $$ G_{active} \\xrightarrow{k_{off}} G_{inactive} $$\n  The propensity is $a_2 = k_{off} \\cdot \\mathbb{1}\\{g=1\\}$.\n\n- **Transcription**: An mRNA molecule is produced when the gene is active.\n  $$ G_{active} \\xrightarrow{s} G_{active} + M $$\n  The propensity is $a_3 = s \\cdot \\mathbb{1}\\{g=1\\}$.\n\n- **mRNA Degradation**: An mRNA molecule degrades.\n  $$ M \\xrightarrow{\\gamma} \\emptyset $$\n  The propensity is $a_4 = \\gamma \\cdot m$, which is dependent on the current number of mRNA molecules $m$.\n\nThe parameter vector to be inferred is $\\theta = (k_{on}, k_{off}, s, \\gamma)$.\n\n**2. Simulation via Gillespie's Algorithm**\n\nTo generate datasets from this model for given parameters $\\theta$, we use the Gillespie Stochastic Simulation Algorithm (SSA). This algorithm simulates an exact trajectory of the Markov process. Starting from an initial state $(g(0), m(0))$ at time $t=0$, the algorithm iteratively performs two steps:\n\n1.  **Determine the time to the next reaction**: The total propensity is the sum of all individual propensities, $a_0 = \\sum_{i=1}^4 a_i$. The time until the next reaction, $\\tau$, is a random variable drawn from an exponential distribution with rate $a_0$, which is equivalent to $\\tau = -\\frac{1}{a_0} \\ln(r_1)$, where $r_1$ is a random number drawn from the standard uniform distribution $U(0,1)$. The simulation time is then advanced by $t \\leftarrow t + \\tau$.\n\n2.  **Determine which reaction occurs**: The probability of reaction $j$ occurring is $\\frac{a_j}{a_0}$. To select the reaction, a second uniform random number $r_2 \\sim U(0,1)$ is drawn. The reaction index $j$ is chosen such that $\\sum_{i=1}^{j-1} a_i  r_2 \\cdot a_0 \\leq \\sum_{i=1}^{j} a_i$. The system state $(g, m)$ is then updated according to the stoichiometry of reaction $j$.\n\nThis process is repeated until the simulation time $t$ reaches the specified horizon $T$. Since the problem requires mRNA counts at fixed time intervals $\\Delta t$, the continuous-time trajectory from the SSA is sampled. The value of $m(t)$ is recorded at each time point $k \\cdot \\Delta t$ for $k=0, 1, \\dots, \\lfloor T/\\Delta t \\rfloor$.\n\n**3. The Rejection ABC Algorithm**\n\nApproximate Bayesian Computation is a class of methods for Bayesian inference when the likelihood function $p(x_{obs} | \\theta)$ is intractable. The rejection ABC algorithm implemented here is the simplest form. Its goal is to generate a sample of parameter vectors from an approximation of the posterior distribution $p(\\theta | x_{obs})$.\n\nThe algorithm proceeds as follows for a given observed dataset $x_{obs}$:\n\n1.  **Define Summary Statistics**: High-dimensional datasets are reduced to low-dimensional summary statistics. As specified, we use the sample mean $\\overline{m}$ and the population variance $\\operatorname{Var}(m)$ of the mRNA time series: $S(x) = (\\overline{m}, \\operatorname{Var}(m))$.\n\n2.  **Define Distance Metric**: A distance function measures the discrepancy between the summary statistics of the observed data, $S(x_{obs})$, and simulated data, $S(x_{\\theta'})$. We use the Euclidean distance:\n    $$ d(S(x_{\\theta'}), S(x_{obs})) = \\sqrt{(\\overline{m}_{\\theta'} - \\overline{m}_{obs})^2 + (\\operatorname{Var}(m)_{\\theta'} - \\operatorname{Var}(m)_{obs})^2} $$\n\n3.  **Rejection Sampling Loop**: The algorithm iterates until a target number $N$ of parameter samples are accepted:\n    a. A candidate parameter set $\\theta'$ is drawn from the specified prior distributions:\n    $$ k_{on} \\sim \\text{Uniform}(0.05, 2.0) $$\n    $$ k_{off} \\sim \\text{Uniform}(0.05, 2.0) $$\n    $$ s \\sim \\text{Uniform}(1.0, 20.0) $$\n    $$ \\gamma \\sim \\text{Uniform}(0.1, 1.0) $$\n    b. A synthetic dataset $x_{\\theta'}$ is generated using the Gillespie SSA with parameters $\\theta'$.\n    c. The summary statistics $S(x_{\\theta'})$ are computed.\n    d. The distance $d(S(x_{\\theta'}), S(x_{obs}))$ is calculated.\n    e. If the distance is within a predefined tolerance, $d \\leq \\varepsilon$, the parameter set $\\theta'$ is accepted and added to a list of posterior samples. Otherwise, it is rejected.\n\n**4. Final Parameter Estimation**\n\nAfter collecting $N$ accepted samples $\\{\\theta'_1, \\dots, \\theta'_N\\}$, this set serves as an empirical approximation to the posterior distribution. The final required output is the posterior mean, estimated as the component-wise average of the accepted samples:\n$$ \\overline{\\theta} = \\frac{1}{N} \\sum_{i=1}^{N} \\theta'_i = \\left(\\frac{1}{N}\\sum_i k'_{on,i}, \\frac{1}{N}\\sum_i k'_{off,i}, \\frac{1}{N}\\sum_i s'_i, \\frac{1}{N}\\sum_i \\gamma'_i\\right) $$\nThis calculation is performed for each test case provided in the problem statement. The generation of the \"observed\" data for each test case is itself part of the task, using the specified true parameters and a dedicated random seed to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef gillespie_ssa(params, T, delta_t, rng):\n    \"\"\"\n    Simulates the telegraph model using Gillespie's SSA and samples at fixed intervals.\n\n    Args:\n        params (tuple): A tuple of parameters (k_on, k_off, s, gamma).\n        T (float): Total simulation time.\n        delta_t (float): Sampling interval.\n        rng (numpy.random.Generator): A random number generator.\n\n    Returns:\n        numpy.ndarray: An array of mRNA counts at each sample time.\n    \"\"\"\n    k_on, k_off, s, gamma = params\n    t = 0.0\n    g = 0  # Gene state: 0 for inactive, 1 for active\n    m = 0  # mRNA count\n\n    # Robustly determine the number of sample points\n    num_points = int(np.round(T / delta_t)) + 1\n    sample_times = np.linspace(0, T, num_points)\n    \n    mRNA_counts = np.zeros(num_points, dtype=int)\n    current_sample_idx = 0\n\n    # Handle t=0 sample\n    if current_sample_idx  len(sample_times) and np.isclose(sample_times[current_sample_idx], 0.0):\n        mRNA_counts[current_sample_idx] = m\n        current_sample_idx += 1\n\n    while t  T:\n        propensities = np.zeros(4)\n        if g == 0:\n            propensities[0] = k_on  # Activation\n        else:  # g == 1\n            propensities[1] = k_off # Deactivation\n            propensities[2] = s    # Transcription\n        propensities[3] = gamma * m # Degradation\n\n        a0 = np.sum(propensities)\n\n        if a0 = 0:\n            next_t = T + 1.0  # Force loop termination\n        else:\n            tau = rng.exponential(1.0 / a0)\n            next_t = t + tau\n\n        # Record state at sample points between t and next_t\n        while current_sample_idx  num_points and sample_times[current_sample_idx]  next_t:\n            mRNA_counts[current_sample_idx] = m\n            current_sample_idx += 1\n            \n        t = next_t\n        \n        if t = T:\n            break\n\n        if a0  0:\n            # Choose reaction\n            r2 = rng.uniform(0, a0)\n            if r2  propensities[0]:\n                g = 1  # Activation\n            elif r2  propensities[0] + propensities[1]:\n                g = 0  # Deactivation\n            elif r2  propensities[0] + propensities[1] + propensities[2]:\n                m += 1 # Transcription\n            else:\n                m -= 1 # Degradation\n\n    # Fill any remaining sample times with the final state\n    while current_sample_idx  num_points:\n        mRNA_counts[current_sample_idx] = m\n        current_sample_idx += 1\n\n    return mRNA_counts\n\ndef calculate_summary_stats(data):\n    \"\"\"Calculates mean and population variance.\"\"\"\n    mean_val = np.mean(data)\n    var_val = np.var(data, ddof=0)\n    return np.array([mean_val, var_val])\n\ndef calculate_distance(s_sim, s_obs):\n    \"\"\"Calculates Euclidean distance between summary statistics.\"\"\"\n    return np.linalg.norm(s_sim - s_obs)\n\ndef run_abc(s_obs, T, delta_t, abc_params, prior_bounds, rng):\n    \"\"\"\n    Runs the rejection ABC algorithm.\n    \"\"\"\n    N, epsilon = abc_params\n    (a_on, b_on), (a_off, b_off), (a_s, b_s), (a_gamma, b_gamma) = prior_bounds\n    \n    accepted_params = []\n    \n    while len(accepted_params)  N:\n        # 1. Draw parameters from prior\n        k_on_prime = rng.uniform(a_on, b_on)\n        k_off_prime = rng.uniform(a_off, b_off)\n        s_prime = rng.uniform(a_s, b_s)\n        gamma_prime = rng.uniform(a_gamma, b_gamma)\n        theta_prime = (k_on_prime, k_off_prime, s_prime, gamma_prime)\n\n        # 2. Simulate data\n        x_prime = gillespie_ssa(theta_prime, T, delta_t, rng)\n        \n        # 3. Compute summary statistics\n        s_prime = calculate_summary_stats(x_prime)\n\n        # 4. Calculate distance  5. Accept/reject\n        dist = calculate_distance(s_prime, s_obs)\n        if dist = epsilon:\n            accepted_params.append(theta_prime)\n            \n    return np.array(accepted_params)\n\ndef format_results(results):\n    \"\"\"Formats the list of lists into the required string format.\"\"\"\n    inner_strings = []\n    for sublist in results:\n        s = \"[\" + \",\".join(map(str, sublist)) + \"]\"\n        inner_strings.append(s)\n    return \"[\" + \",\".join(inner_strings) + \"]\"\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Prior bounds: [k_on, k_off, s, gamma]\n    prior_bounds = [\n        (0.05, 2.0),\n        (0.05, 2.0),\n        (1.0, 20.0),\n        (0.1, 1.0)\n    ]\n\n    test_cases = [\n        {\n            \"true_params\": (0.5, 0.7, 12.0, 0.2), \"T\": 10.0, \"delta_t\": 0.5,\n            \"abc_params\": (20, 30.0), \"obs_seed\": 314159, \"abc_seed\": 271828\n        },\n        {\n            \"true_params\": (1.0, 0.2, 8.0, 0.5), \"T\": 10.0, \"delta_t\": 0.5,\n            \"abc_params\": (1, 1000000.0), \"obs_seed\": 42, \"abc_seed\": 4242\n        },\n        {\n            \"true_params\": (0.05, 1.8, 2.0, 0.9), \"T\": 3.0, \"delta_t\": 0.3,\n            \"abc_params\": (10, 40.0), \"obs_seed\": 7, \"abc_seed\": 77\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # Generate observed data with its own random seed\n        obs_rng = np.random.default_rng(case[\"obs_seed\"])\n        observed_trajectory = gillespie_ssa(\n            case[\"true_params\"], case[\"T\"], case[\"delta_t\"], obs_rng\n        )\n        s_obs = calculate_summary_stats(observed_trajectory)\n        \n        # Run ABC with a separate random seed\n        abc_rng = np.random.default_rng(case[\"abc_seed\"])\n        accepted_samples = run_abc(\n            s_obs, case[\"T\"], case[\"delta_t\"], case[\"abc_params\"], prior_bounds, abc_rng\n        )\n        \n        # Calculate the posterior mean\n        posterior_mean = np.mean(accepted_samples, axis=0)\n        \n        all_results.append(posterior_mean.tolist())\n\n    print(format_results(all_results))\n\nsolve()\n```"
        },
        {
            "introduction": "A crucial and often overlooked component of ABC is the choice of distance function, which determines how we compare simulated and observed data. This exercise moves beyond the naive Euclidean distance used in many simple examples to derive and apply a statistically principled metric, the Mahalanobis distance. By learning to weight summary statistics based on their variance, you will develop a more robust and sensitive approach to parameter inference .",
            "id": "3906470",
            "problem": "A synthetic biology lab models a two-stage gene expression system with stochastic transcription and translation producing a fluorescent protein. For a single condition, the lab collects a long time series of protein counts and reduces the data to a summary vector $s \\in \\mathbb{R}^{3}$ consisting of: (i) the sample mean of protein count, (ii) the sample variance of protein count, and (iii) the lag-$1$ sample autocorrelation of the protein count time series. In an Approximate Bayesian Computation (ABC) procedure (Approximate Bayesian Computation (ABC)), simulated datasets produced under a parameter vector $\\theta$ are reduced to the same summary vector $s(\\theta)$ and compared to the observed summary vector $s_{\\mathrm{obs}}$ via a distance function.\n\nStarting only from the following fundamental bases: (a) the multivariate Central Limit Theorem states that for large effective sample size the summary vector $s$ is approximately multivariate normal with mean $\\mu(\\theta)$ and covariance $\\Sigma(\\theta)$, (b) the log-likelihood of a multivariate normal distribution is a quadratic form in the residual $s-\\mu(\\theta)$ weighted by the inverse covariance, and (c) the Fisher information is the negative expected Hessian of the log-likelihood and induces a local metric, derive a principled weighted Euclidean distance $d(s,s_{\\mathrm{obs}})$ in summary space that is justified by these bases. Your derivation must identify a concrete choice of the weight matrix in terms of the covariance of summaries and explain why it is principled in the ABC context.\n\nThen, for a specific experiment, the lab estimates the covariance of the summaries at a nominal parameter value $\\theta_{0}$ by time-series block bootstrap, obtaining a diagonal estimate\n$$\n\\widehat{\\Sigma}(\\theta_{0})=\\mathrm{diag}\\!\\big(25,\\;2.5\\times 10^{5},\\;0.01\\big).\n$$\nAn ABC simulation at a trial parameter $\\theta$ yields $s(\\theta)=\\big(110,\\;3800,\\;0.6\\big)$, while the observed summaries are $s_{\\mathrm{obs}}=\\big(100,\\;4000,\\;0.7\\big)$. Using your derived principled distance with the plug-in covariance estimate above, compute the scalar distance $d\\big(s(\\theta),s_{\\mathrm{obs}}\\big)$. Express your final answer as a unitless scalar rounded to four significant figures.",
            "solution": "The problem asks for the derivation of a principled distance metric for Approximate Bayesian Computation (ABC) and its subsequent calculation for a specific case. The derivation must be founded upon three provided bases.\n\n**Part 1: Derivation of the Distance Metric**\n\nThe objective is to derive a weighted Euclidean distance $d(s, s_{\\mathrm{obs}})$ of the form $d(s, s_{\\mathrm{obs}}) = \\sqrt{(s-s_{\\mathrm{obs}})^T W (s-s_{\\mathrm{obs}})}$ where $W$ is a weight matrix. The derivation will proceed by synthesizing the three provided bases.\n\n**Basis (a)**: The multivariate Central Limit Theorem implies that the summary statistic vector $s$ is approximately described by a multivariate normal (MVN) distribution, $s \\sim \\mathcal{N}(\\mu(\\theta), \\Sigma(\\theta))$, where $\\mu(\\theta)$ is the expected summary vector and $\\Sigma(\\theta)$ is its covariance matrix, both dependent on the underlying model parameters $\\theta$.\n\n**Basis (b)**: The log-likelihood of a multivariate normal distribution is a quadratic form. For our observed summary vector $s_{\\mathrm{obs}}$, its probability density under the model with parameters $\\theta$ is:\n$$ p(s_{\\mathrm{obs}} | \\theta) \\approx \\frac{1}{\\sqrt{(2\\pi)^k |\\det(\\Sigma(\\theta))|}} \\exp\\left( -\\frac{1}{2} (s_{\\mathrm{obs}} - \\mu(\\theta))^T \\Sigma(\\theta)^{-1} (s_{\\mathrm{obs}} - \\mu(\\theta)) \\right) $$\nwhere $k$ is the dimension of the summary vector, here $k=3$. The log-likelihood is:\n$$ \\ln p(s_{\\mathrm{obs}} | \\theta) \\approx C - \\frac{1}{2} \\ln|\\det(\\Sigma(\\theta))| - \\frac{1}{2} (s_{\\mathrm{obs}} - \\mu(\\theta))^T \\Sigma(\\theta)^{-1} (s_{\\mathrm{obs}} - \\mu(\\theta)) $$\nwhere $C = -\\frac{k}{2}\\ln(2\\pi)$ is a constant.\n\nIn a likelihood-based inference framework, one seeks to find parameters $\\theta$ that maximize this log-likelihood. Ignoring the term involving $\\ln|\\det(\\Sigma(\\theta))|$ (often a reasonable local approximation), maximizing the likelihood is equivalent to minimizing the quadratic term:\n$$ \\chi^2(\\theta) = (s_{\\mathrm{obs}} - \\mu(\\theta))^T \\Sigma(\\theta)^{-1} (s_{\\mathrm{obs}} - \\mu(\\theta)) $$\nThis term is the squared Mahalanobis distance between the observed summaries $s_{\\mathrm{obs}}$ and their theoretical mean values $\\mu(\\theta)$.\n\nIn ABC, the likelihood is considered intractable, and direct evaluation of $\\mu(\\theta)$ is bypassed. Instead, we generate a single simulation $s(\\theta)$ from the model for a given $\\theta$. We use this single stochastic realization $s(\\theta)$ as a proxy for its expected value $\\mu(\\theta)$. This is a central approximation in many ABC algorithms. By substituting $\\mu(\\theta)$ with $s(\\theta)$, the quadratic form becomes:\n$$ (s_{\\mathrm{obs}} - s(\\theta))^T \\Sigma(\\theta)^{-1} (s_{\\mathrm{obs}} - s(\\theta)) $$\nThis expression suggests a principled choice for the squared distance in ABC. The weighting matrix is the inverse of the covariance matrix of the summary statistics, $W = \\Sigma(\\theta)^{-1}$. A distance function is typically a non-negative-definite scalar, so we take the square root:\n$$ d(s(\\theta), s_{\\mathrm{obs}}) = \\sqrt{(s(\\theta) - s_{\\mathrm{obs}})^T \\Sigma(\\theta)^{-1} (s(\\theta) - s_{\\mathrm{obs}})} $$\n\nThe choice of $W = \\Sigma^{-1}$ is principled because:\n1.  It is derived directly from the approximate Gaussian likelihood of the summary statistics. A smaller distance corresponds to a higher approximate likelihood.\n2.  It accounts for the different scales and variances of the summary statistics. Differences in highly variable summaries are down-weighted, while differences in summaries with low variance are more heavily penalized.\n3.  It accounts for correlations between summary statistics.\n4.  It renders the distance a dimensionless quantity, as each term in the quadratic form, $(s_i - s_{\\mathrm{obs},i})(s_j - s_{\\mathrm{obs},j})(\\Sigma^{-1})_{ij}$, is dimensionless.\n\n**Basis (c)**: The Fisher information matrix, $I(\\theta)$, is the negative expected Hessian of the log-likelihood with respect to the parameters $\\theta$. It quantifies the amount of information that the data (here, $s_{\\mathrm{obs}}$) carries about the parameters. For a Gaussian likelihood where $\\Sigma$ is treated as locally constant, the Fisher information is $I(\\theta) = (\\nabla_{\\theta}\\mu)^T \\Sigma^{-1} (\\nabla_{\\theta}\\mu)$. The matrix $\\Sigma^{-1}$ defines a metric on the space of summary statistics, which in turn induces the Fisher-Rao metric on the parameter space via the Jacobian $\\nabla_{\\theta}\\mu$. Using the Mahalanobis distance, weighted by $\\Sigma^{-1}$, therefore means that the geometry of the summary statistic space is being correctly related to the information-theoretic geometry of the parameter space. This provides a deep justification from information theory for this choice of distance.\n\nA practical issue is that $\\Sigma(\\theta)$ depends on the unknown parameter $\\theta$. In many ABC implementations, this is handled by using a fixed covariance estimate, $\\widehat{\\Sigma} = \\widehat{\\Sigma}(\\theta_0)$, computed at a single nominal parameter value $\\theta_0$ (e.g., obtained from a preliminary analysis or prior knowledge). The derived, principled distance metric is thus:\n$$ d(s(\\theta), s_{\\mathrm{obs}}) = \\sqrt{(s(\\theta) - s_{\\mathrm{obs}})^T \\widehat{\\Sigma}(\\theta_0)^{-1} (s(\\theta) - s_{\\mathrm{obs}})} $$\nThis is the Mahalanobis distance, where the covariance is estimated from a pilot run or bootstrap analysis at a reference parameter set.\n\n**Part 2: Calculation**\n\nWe are given the necessary data to compute this distance.\nThe simulated summary vector is $s(\\theta) = \\begin{pmatrix} 110 \\\\ 3800 \\\\ 0.6 \\end{pmatrix}$.\nThe observed summary vector is $s_{\\mathrm{obs}} = \\begin{pmatrix} 100 \\\\ 4000 \\\\ 0.7 \\end{pmatrix}$.\nThe estimated covariance matrix is $\\widehat{\\Sigma}(\\theta_{0}) = \\mathrm{diag}(25, 2.5 \\times 10^5, 0.01)$.\n\nFirst, we compute the difference vector, $\\Delta s = s(\\theta) - s_{\\mathrm{obs}}$:\n$$ \\Delta s = \\begin{pmatrix} 110 - 100 \\\\ 3800 - 4000 \\\\ 0.6 - 0.7 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ -200 \\\\ -0.1 \\end{pmatrix} $$\nNext, we find the inverse of the covariance matrix. Since $\\widehat{\\Sigma}(\\theta_{0})$ is diagonal, its inverse is the diagonal matrix of the reciprocals of its elements:\n$$ \\widehat{\\Sigma}(\\theta_{0})^{-1} = \\mathrm{diag}\\left(\\frac{1}{25}, \\frac{1}{2.5 \\times 10^{5}}, \\frac{1}{0.01}\\right) = \\mathrm{diag}(0.04, 4 \\times 10^{-6}, 100) $$\nNow, we compute the squared distance, $d^2 = (\\Delta s)^T \\widehat{\\Sigma}(\\theta_{0})^{-1} (\\Delta s)$. For diagonal $\\widehat{\\Sigma}(\\theta_0)^{-1}$, this simplifies to:\n$$ d^2 = \\sum_{i=1}^{3} \\frac{(\\Delta s_i)^2}{(\\widehat{\\Sigma}(\\theta_0))_{ii}} $$\nSubstituting the values:\n$$ d^2 = \\frac{(10)^2}{25} + \\frac{(-200)^2}{2.5 \\times 10^5} + \\frac{(-0.1)^2}{0.01} $$\n$$ d^2 = \\frac{100}{25} + \\frac{40000}{250000} + \\frac{0.01}{0.01} $$\n$$ d^2 = 4 + 0.16 + 1 = 5.16 $$\nThe distance $d$ is the square root of this value:\n$$ d = \\sqrt{5.16} \\approx 2.2715633... $$\nRounding to four significant figures, we get $2.272$.\nThe distance is a unitless scalar, as is appropriate for the Mahalanobis distance.",
            "answer": "$$\\boxed{2.272}$$"
        },
        {
            "introduction": "While simple rejection ABC is intuitive, its brute-force nature is highly inefficient for complex problems. This final practice introduces a more powerful and practical algorithm: ABC combined with Markov Chain Monte Carlo (ABC-MCMC). You will implement this method to explore the parameter space of a delayed genetic toggle switch, learning how to intelligently search for parameters that are consistent with observed data, a technique essential for tackling realistic biological models .",
            "id": "3906459",
            "problem": "You are tasked with implementing an Approximate Bayesian Computation–Markov Chain Monte Carlo (ABC–MCMC) algorithm to infer kinetic parameters of a delayed genetic toggle switch, a canonical bistable module in synthetic biology. The toggle switch consists of two genes that mutually repress each other, here modeled as a system of delay differential equations (DDEs) with Hill-type repression. The program you produce must be a complete, runnable implementation that (i) simulates delayed dynamics, (ii) performs ABC–MCMC using summaries and a distance threshold, and (iii) reports acceptance rates for a specified test suite that probes the effects of proposal scaling and simulation stiffness.\n\nFundamental base and core definitions:\n- Bayes’ theorem states that the posterior density is proportional to the likelihood times the prior, i.e., for parameter vector $\\theta$, posterior density satisfies $p(\\theta \\mid \\text{data}) \\propto p(\\text{data} \\mid \\theta) p(\\theta)$.\n- In Approximate Bayesian Computation (ABC), one replaces an intractable likelihood with a simulation-based accept/reject criterion. Given observed summary statistics $s_{\\text{obs}}$ and simulated summary statistics $s(\\theta)$, ABC defines an approximate posterior as the set of $\\theta$ such that the distance $\\rho(s(\\theta), s_{\\text{obs}})$ is at most a tolerance $\\varepsilon$, i.e., the event $\\{\\rho(s(\\theta), s_{\\text{obs}}) \\le \\varepsilon\\}$.\n- Markov Chain Monte Carlo (MCMC) constructs a Markov chain $\\{\\theta^{(t)}\\}$ with stationary distribution equal to the target posterior by proposing parameters from a kernel $q(\\cdot \\mid \\theta)$ and accepting or rejecting based on a Metropolis–Hastings rule.\n\nModeling assumptions:\n- Let $x(t)$ and $y(t)$ denote protein concentrations. The delayed toggle switch obeys\n$$\n\\frac{d x(t)}{d t} = \\frac{\\alpha_1}{1 + \\left(\\frac{y(t - \\tau_y)}{K_y}\\right)^{n_y}} - \\delta_x x(t),\n\\qquad\n\\frac{d y(t)}{d t} = \\frac{\\alpha_2}{1 + \\left(\\frac{x(t - \\tau_x)}{K_x}\\right)^{n_x}} - \\delta_y y(t).\n$$\n- Unknown parameters to infer are $\\theta = (\\alpha_1, \\alpha_2, \\tau_x, \\tau_y)$. The remaining parameters are known constants: $K_x = 1$, $K_y = 1$, $n_x = 2$, $n_y = 2$, $\\delta_x = 1$, $\\delta_y = 1$.\n- Assume initial conditions $x(t) = x_0$ and $y(t) = y_0$ for all $t \\le 0$, with $x_0 = 0.1$ and $y_0 = 0.1$.\n\nNumerical simulation:\n- Discretize time with a forward Euler method of step size $\\Delta t$, using integer delays $d_x = \\lfloor \\tau_x / \\Delta t \\rceil$ and $d_y = \\lfloor \\tau_y / \\Delta t \\rceil$. For $N$ steps over horizon $T$, update\n$$\nx_{k+1} = x_k + \\Delta t \\left( \\frac{\\alpha_1}{1 + \\left(\\frac{y_{k - d_y}}{K_y}\\right)^{n_y}} - \\delta_x x_k \\right), \\quad\ny_{k+1} = y_k + \\Delta t \\left( \\frac{\\alpha_2}{1 + \\left(\\frac{x_{k - d_x}}{K_x}\\right)^{n_x}} - \\delta_y y_k \\right),\n$$\nwith a prehistory buffer implementing $x_j = x_0$ and $y_j = y_0$ for $j \\le 0$.\n\nABC–MCMC specification:\n- Prior: Uniform on the following hyper-rectangular domain,\n$$\n\\alpha_1 \\in [0.5, 5.0],\\quad \\alpha_2 \\in [0.5, 5.0],\\quad \\tau_x \\in [0.0, 3.0],\\quad \\tau_y \\in [0.0, 3.0].\n$$\n- Proposal: A multivariate Gaussian random walk with diagonal covariance scaled by a factor $\\sigma_{\\text{scale}}$, i.e., $\\theta' = \\theta + \\eta$, where $\\eta \\sim \\mathcal{N}(0, \\operatorname{diag}(\\sigma^2))$ and $\\sigma = \\sigma_{\\text{base}} \\cdot \\sigma_{\\text{scale}}$, using base scales $\\sigma_{\\text{base}} = (0.3, 0.3, 0.2, 0.2)$. Ensure proposals respect the prior bounds by clipping to the domain edges.\n- Summary statistics: From the simulated trajectory at horizon $T$, define\n$$\ns(\\theta) = \\left( \\overline{x}, \\overline{y}, x_T, y_T \\right),\n$$\nwhere $\\overline{x}$ and $\\overline{y}$ are time-averages over the last half of the trajectory (i.e., average over $[T/2, T]$), and $x_T$, $y_T$ are terminal values at $T$.\n- Distance: Use Euclidean distance, $\\rho(s(\\theta), s_{\\text{obs}}) = \\left\\| s(\\theta) - s_{\\text{obs}} \\right\\|_2$.\n- ABC–MCMC accept/reject: At iteration $t$, propose $\\theta'$ from $q(\\cdot \\mid \\theta^{(t)})$, simulate $s(\\theta')$, compute $\\rho(s(\\theta'), s_{\\text{obs}})$. If $\\rho(s(\\theta'), s_{\\text{obs}}) \\le \\varepsilon$, accept $\\theta'$ with standard Metropolis–Hastings probability that depends on prior and proposal kernels; otherwise reject.\n\nObserved data generation:\n- Use a high-resolution simulation to define $s_{\\text{obs}}$, with true parameters $\\alpha_1 = 3.0$, $\\alpha_2 = 2.0$, $\\tau_x = 1.0$, $\\tau_y = 1.5$, horizon $T = 15.0$, and time step $\\Delta t_{\\text{obs}} = 0.005$.\n\nAlgorithmic constraints and reproducibility:\n- Use a fixed random seed for reproducibility.\n- Initialize the Markov chain at $\\theta^{(0)} = (1.5, 1.5, 1.0, 1.0)$.\n- Use a Gaussian proposal with the specified base scales and scaling factor per test case as described below.\n- Perform exactly $N = 250$ iterations per test case.\n- For delayed indices, use the nearest integer delay $\\lfloor \\tau / \\Delta t \\rceil$; if zero, use the current state (no delay).\n\nTest suite:\nProvide acceptance rates for the following configurations. Each test case is a tuple $(\\Delta t, \\sigma_{\\text{scale}}, \\varepsilon)$, and all other settings are as defined above. The simulation horizon is $T = 15.0$ in time units. No physical units are required for the output; treat quantities as dimensionless.\n\n- Test case 1 (happy path): $(0.02, 1.0, 0.15)$.\n- Test case 2 (increased stiffness accommodation via finer resolution and smaller proposal steps): $(0.005, 0.5, 0.20)$.\n- Test case 3 (stringent threshold challenging acceptance): $(0.02, 1.5, 0.05)$.\n\nFinal output format:\nYour program should produce a single line of output containing the acceptance rates for the three test cases as a comma-separated list enclosed in square brackets, for example, \"[$r_1$, $r_2$, $r_3$]\" where each $r_i$ is a float in $[0, 1]$ representing the fraction of accepted proposals over $N$ iterations. Report raw floats without rounding or additional text.",
            "solution": "The user has tasked me with implementing an Approximate Bayesian Computation–Markov Chain Monte Carlo (ABC–MCMC) algorithm to infer parameters for a synthetic biology model. The model is a delayed genetic toggle switch, described by a system of delay differential equations (DDEs). The solution will be a complete Python program that simulates the model, performs the ABC–MCMC inference, and reports the acceptance rates for three specified test cases. The implementation adheres strictly to the provided specifications.\n\nFirst, the core of the problem is the numerical simulation of the delayed toggle switch model. The state of the system is given by the concentrations of two proteins, $x(t)$ and $y(t)$. Their dynamics are governed by the following DDEs:\n$$\n\\frac{d x(t)}{d t} = \\frac{\\alpha_1}{1 + \\left(\\frac{y(t - \\tau_y)}{K_y}\\right)^{n_y}} - \\delta_x x(t)\n$$\n$$\n\\frac{d y(t)}{d t} = \\frac{\\alpha_2}{1 + \\left(\\frac{x(t - \\tau_x)}{K_x}\\right)^{n_x}} - \\delta_y y(t)\n$$\nThe parameters to be inferred are $\\theta = (\\alpha_1, \\alpha_2, \\tau_x, \\tau_y)$. The other model parameters are given as constants: $K_x = 1$, $K_y = 1$, $n_x = 2$, $n_y = 2$, $\\delta_x = 1$, and $\\delta_y = 1$. The initial conditions are specified as a constant history: $x(t) = 0.1$ and $y(t) = 0.1$ for all $t \\le 0$.\n\nThe simulation is performed using the forward Euler method with a time step $\\Delta t$. The discrete-time update equations for step $k$ are:\n$$\nx_{k+1} = x_k + \\Delta t \\left( \\frac{\\alpha_1}{1 + \\left(\\frac{y_{k - d_y}}{K_y}\\right)^{n_y}} - \\delta_x x_k \\right)\n$$\n$$\ny_{k+1} = y_k + \\Delta t \\left( \\frac{\\alpha_2}{1 + \\left(\\frac{x_{k - d_x}}{K_x}\\right)^{n_x}} - \\delta_y y_k \\right)\n$$\nThe delays $\\tau_x$ and $\\tau_y$ are converted to integer step delays $d_x = \\lfloor \\tau_x / \\Delta t \\rceil$ and $d_y = \\lfloor \\tau_y / \\Delta t \\rceil$. The history access $x_{k-d_x}$ and $y_{k-d_y}$ uses the initial prehistory values $x_0=0.1$ and $y_0=0.1$ if the index $k-d$ is less than $0$.\n\nSecond, the ABC algorithm requires summary statistics to compare simulations with observed data. For a given parameter vector $\\theta$, a trajectory is simulated up to a horizon $T$. From this trajectory, we compute the summary vector $s(\\theta) = (\\overline{x}, \\overline{y}, x_T, y_T)$, where $\\overline{x}$ and $\\overline{y}$ are the time-averages of the concentrations over the latter half of the simulation interval, $[\\frac{T}{2}, T]$, and $(x_T, y_T)$ are the final concentrations at time $T$. The \"observed\" summary statistics, $s_{\\text{obs}}$, are generated once by running a high-resolution simulation with a given set of \"true\" parameters: $(\\alpha_1, \\alpha_2, \\tau_x, \\tau_y) = (3.0, 2.0, 1.0, 1.5)$ with $\\Delta t_{\\text{obs}} = 0.005$.\n\nThird, the ABC–MCMC algorithm is implemented to generate samples from the approximate posterior distribution. The algorithm proceeds as follows for $N=250$ iterations, starting from an initial parameter vector $\\theta^{(0)} = (1.5, 1.5, 1.0, 1.0)$:\n1.  **Proposal**: At each iteration $t$, a new parameter vector $\\theta'$ is proposed from a symmetric random walk kernel centered at the current state $\\theta^{(t)}$: $\\theta' = \\theta^{(t)} + \\eta$, where $\\eta$ is drawn from a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$. The covariance matrix $\\Sigma$ is diagonal, $\\Sigma = \\operatorname{diag}(\\sigma^2)$, with $\\sigma = \\sigma_{\\text{base}} \\cdot \\sigma_{\\text{scale}}$. The base standard deviations are $\\sigma_{\\text{base}} = (0.3, 0.3, 0.2, 0.2)$. The proposed vector $\\theta'$ is clipped to ensure it remains within the bounds of the uniform prior distribution: $\\alpha_1, \\alpha_2 \\in [0.5, 5.0]$ and $\\tau_x, \\tau_y \\in [0.0, 3.0]$.\n2.  **Simulation  Comparison**: A simulation is run with the proposed parameters $\\theta'$ to generate summary statistics $s(\\theta')$. The Euclidean distance $\\rho(s(\\theta'), s_{\\text{obs}}) = \\|s(\\theta') - s_{\\text{obs}}\\|_2$ is computed.\n3.  **Acceptance/Rejection**: The proposal is accepted, i.e., $\\theta^{(t+1)} = \\theta'$, if the distance is less than or equal to a given tolerance, $\\rho(s(\\theta'), s_{\\text{obs}}) \\le \\varepsilon$. Otherwise, the proposal is rejected, and the chain remains at the current state, $\\theta^{(t+1)} = \\theta^{(t)}$. This acceptance rule is a specific case of the Metropolis-Hastings algorithm. Because the prior is uniform (and the proposal is clipped to its support) and the proposal kernel is symmetric, the Metropolis-Hastings ratio simplifies. The acceptance probability for a proposal $\\theta'$ that satisfies the distance condition becomes $1$.\n\nFinally, the entire procedure is encapsulated in a Python program. For reproducibility, a fixed random seed is used. The program iterates through three test cases, each defined by a tuple $(\\Delta t, \\sigma_{\\text{scale}}, \\varepsilon)$. For each case, it runs the ABC–MCMC chain for $N=250$ steps and calculates the acceptance rate, which is the fraction of proposals that were accepted. The final output is a list of these three rates.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ABC-MCMC simulation for the delayed toggle switch model.\n    It generates observed data, runs the MCMC for three test cases, and prints the results.\n    \"\"\"\n    \n    # --- Global Constants and Definitions ---\n    # These are fixed for all simulations as per the problem statement.\n    CONSTANTS = {\n        'Kx': 1.0, 'Ky': 1.0, 'nx': 2.0, 'ny': 2.0,\n        'delta_x': 1.0, 'delta_y': 1.0,\n        'x0': 0.1, 'y0': 0.1,\n        'T': 15.0,  # Simulation horizon\n        'N_mcmc': 250, # Number of MCMC iterations\n        'theta0': np.array([1.5, 1.5, 1.0, 1.0]), # Initial chain state\n        'prior_bounds': np.array([[0.5, 5.0], [0.5, 5.0], [0.0, 3.0], [0.0, 3.0]]),\n        'sigma_base': np.array([0.3, 0.3, 0.2, 0.2]),\n    }\n    \n    # True parameters for generating the observed data\n    TRUE_THETA = {'alpha1': 3.0, 'alpha2': 2.0, 'tau_x': 1.0, 'tau_y': 1.5}\n    OBS_CONFIG = {'dt': 0.005} # High resolution time step for observed data\n\n    # Set random seed for reproducibility\n    np.random.seed(42)\n\n    def dde_simulator(theta, T, dt, constants):\n        \"\"\"\n        Simulates the delayed toggle switch model using the forward Euler method.\n\n        Args:\n            theta (np.ndarray): Parameter vector [alpha1, alpha2, tau_x, tau_y].\n            T (float): Total simulation time.\n            dt (float): Time step for numerical integration.\n            constants (dict): Dictionary of fixed model parameters.\n\n        Returns:\n            tuple[np.ndarray, np.ndarray]: Simulated trajectories for x and y.\n        \"\"\"\n        alpha1, alpha2, tau_x, tau_y = theta\n        num_steps = int(T / dt)\n        \n        x = np.full(num_steps + 1, constants['x0'])\n        y = np.full(num_steps + 1, constants['y0'])\n\n        # Calculate integer delays in terms of simulation steps\n        dx = int(np.round(tau_x / dt))\n        dy = int(np.round(tau_y / dt))\n\n        for k in range(num_steps):\n            # Get delayed values. If delay index points to pre-history (k  delay), use initial value.\n            x_delayed = x[k - dx] if k = dx else constants['x0']\n            y_delayed = y[k - dy] if k = dy else constants['y0']\n            \n            x_k, y_k = x[k], y[k]\n            \n            # RHS of the DDEs\n            x_rhs = (alpha1 / (1 + (y_delayed / constants['Ky'])**constants['ny'])) - constants['delta_x'] * x_k\n            y_rhs = (alpha2 / (1 + (x_delayed / constants['Kx'])**constants['nx'])) - constants['delta_y'] * y_k\n            \n            # Forward Euler step\n            x[k+1] = x_k + dt * x_rhs\n            y[k+1] = y_k + dt * y_rhs\n        \n        return x, y\n\n    def calculate_summary_stats(x, y, T, dt):\n        \"\"\"\n        Calculates summary statistics from a simulated trajectory.\n\n        Args:\n            x (np.ndarray): Trajectory for protein x.\n            y (np.ndarray): Trajectory for protein y.\n            T (float): Total simulation time.\n            dt (float): Time step used for simulation.\n\n        Returns:\n            np.ndarray: Vector of summary statistics [mean_x, mean_y, x_T, y_T].\n        \"\"\"\n        num_steps = int(T / dt)\n        start_avg_idx = int(num_steps / 2)\n        \n        mean_x = np.mean(x[start_avg_idx:])\n        mean_y = np.mean(y[start_avg_idx:])\n        \n        x_T = x[-1]\n        y_T = y[-1]\n        \n        return np.array([mean_x, mean_y, x_T, y_T])\n\n    def run_abc_mcmc(test_case, s_obs, constants):\n        \"\"\"\n        Runs the ABC-MCMC algorithm for a single test case configuration.\n\n        Args:\n            test_case (tuple): Configuration (dt, sigma_scale, epsilon).\n            s_obs (np.ndarray): Observed summary statistics.\n            constants (dict): Dictionary of fixed model/algorithm parameters.\n\n        Returns:\n            float: The acceptance rate for the MCMC run.\n        \"\"\"\n        dt, sigma_scale, epsilon = test_case\n        \n        theta_chain = np.zeros((constants['N_mcmc'] + 1, 4))\n        theta_chain[0] = constants['theta0']\n        \n        prior_min = constants['prior_bounds'][:, 0]\n        prior_max = constants['prior_bounds'][:, 1]\n        \n        # Scale proposal covariance\n        sigma = constants['sigma_base'] * sigma_scale\n        proposal_cov = np.diag(sigma**2)\n        \n        accepted_count = 0\n        \n        for i in range(constants['N_mcmc']):\n            current_theta = theta_chain[i]\n            \n            # Propose a new parameter set from a Gaussian random walk\n            noise = np.random.multivariate_normal(np.zeros(4), proposal_cov)\n            proposal = current_theta + noise\n            \n            # Clip proposal to respect prior bounds\n            clipped_proposal = np.clip(proposal, prior_min, prior_max)\n\n            # Simulate model with proposed parameters\n            x_sim, y_sim = dde_simulator(clipped_proposal, constants['T'], dt, constants)\n            \n            # Calculate summary statistics for the simulation\n            s_sim = calculate_summary_stats(x_sim, y_sim, constants['T'], dt)\n            \n            # Calculate distance to observed data\n            distance = np.linalg.norm(s_sim - s_obs)\n            \n            # Accept/Reject step\n            if distance = epsilon:\n                theta_chain[i+1] = clipped_proposal\n                accepted_count += 1\n            else:\n                theta_chain[i+1] = current_theta\n\n        return accepted_count / constants['N_mcmc']\n\n    # --- Main Execution Logic ---\n    \n    # 1. Generate \"observed\" data by simulating with true parameters\n    true_theta_vec = np.array([TRUE_THETA['alpha1'], TRUE_THETA['alpha2'], TRUE_THETA['tau_x'], TRUE_THETA['tau_y']])\n    x_obs_traj, y_obs_traj = dde_simulator(true_theta_vec, CONSTANTS['T'], OBS_CONFIG['dt'], CONSTANTS)\n    s_obs = calculate_summary_stats(x_obs_traj, y_obs_traj, CONSTANTS['T'], OBS_CONFIG['dt'])\n    \n    # 2. Define the test suite from the problem description\n    test_cases = [\n        (0.02, 1.0, 0.15),   # Case 1 (happy path)\n        (0.005, 0.5, 0.20),  # Case 2 (finer resolution, smaller steps)\n        (0.02, 1.5, 0.05)    # Case 3 (stringent threshold)\n    ]\n    \n    # 3. Run ABC-MCMC for each test case and collect acceptance rates\n    results = []\n    for case in test_cases:\n        acceptance_rate = run_abc_mcmc(case, s_obs, CONSTANTS)\n        results.append(acceptance_rate)\n        \n    # 4. Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}