{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of automated model discovery is the ability to objectively compare and rank competing hypotheses. This practice delves into the core of Bayesian model selection, where this comparison is formalized through the model evidence. By deriving the Laplace approximation and calculating the resulting Occam factor, you will gain a quantitative understanding of how these systems penalize unnecessary complexity, providing a mathematical basis for the principle of parsimony.",
            "id": "3906825",
            "problem": "In an automated model discovery pipeline for synthetic biology, Bayesian model evidence is used to rank candidate dynamical models for biochemical reaction networks under mass-action kinetics. Consider a candidate model for a single gene product concentration $x(t)$ governed by the ordinary differential equation $dx/dt = k_1 - k_2 x$, where the parameter vector is $k = [k_1, k_2]^{\\top}$ with $k_1$ the production rate and $k_2$ the degradation rate. Let $y = \\{y_i\\}_{i=1}^{N}$ be noisy observations of $x(t)$ at times $\\{t_i\\}_{i=1}^{N}$, with independent Gaussian observation noise of known variance $\\sigma^2$. The automated model discovery task evaluates the marginal likelihood (model evidence) $p(y \\mid \\mathcal{M})$ under a Gaussian prior $p(k \\mid \\mathcal{M}) = \\mathcal{N}(\\mu_0, \\Sigma_0)$ for the parameters.\n\nUsing Bayesâ€™ theorem and a second-order Taylor expansion of the log-posterior density around the maximum a posteriori estimate $k^{\\ast}$, derive the Laplace approximation to the evidence $p(y \\mid \\mathcal{M})$ in terms of the log-likelihood, the log-prior, and the Hessian of the negative log-posterior at $k^{\\ast}$. From this derivation, isolate the Occam factor that penalizes model complexity as the contraction of parameter-space volume from the prior to the posterior. Express this Occam factor entirely in terms of the prior covariance $\\Sigma_0$ and the posterior covariance $\\Sigma_{\\text{post}}$ obtained from the local Gaussian approximation.\n\nThen, for a concrete instance of the pipeline using $d = 2$ parameters, suppose the prior covariance and the negative log-likelihood Hessian at $k^{\\ast}$ (computed from the design and noise model) are given by\n$$\n\\Sigma_0 = \\begin{pmatrix} 0.25  0 \\\\ 0  0.04 \\end{pmatrix}, \n\\qquad\nH_{\\text{LL}} = \\begin{pmatrix} 400  -120 \\\\ -120  50 \\end{pmatrix}.\n$$\nAssuming the posterior negative log-curvature satisfies $H_{\\text{post}} = H_{\\text{LL}} + \\Sigma_0^{-1}$ and the local Gaussian approximation has covariance $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$, compute the Occam factor under the definition you derived. Express your final answer as a single dimensionless real number and round your answer to four significant figures.",
            "solution": "The problem asks for two main tasks: first, to derive the Laplace approximation for the Bayesian model evidence and isolate the Occam factor representing the contraction of parameter-space volume; second, to compute the value of this Occam factor for a specific numerical instance.\n\n### Part 1: Derivation of the Laplace Approximation and Occam Factor\n\nThe Bayesian model evidence, or marginal likelihood, $p(y \\mid \\mathcal{M})$, is defined by marginalizing the joint probability of data $y$ and parameters $k$ over the entire parameter space:\n$$p(y \\mid \\mathcal{M}) = \\int p(y, k \\mid \\mathcal{M}) dk = \\int p(y \\mid k, \\mathcal{M}) p(k \\mid \\mathcal{M}) dk$$\nwhere $k \\in \\mathbb{R}^d$ is the parameter vector of dimension $d$.\n\nTo evaluate this integral, we employ the Laplace approximation. We first define an \"energy\" function $E(k)$ as the negative logarithm of the integrand:\n$$E(k) = -\\ln[p(y \\mid k, \\mathcal{M}) p(k \\mid \\mathcal{M})] = -\\ln p(y \\mid k, \\mathcal{M}) - \\ln p(k \\mid \\mathcal{M})$$\nNote that $E(k)$ is, up to an additive constant, the negative log-posterior probability. The evidence integral can be rewritten as:\n$$p(y \\mid \\mathcal{M}) = \\int \\exp(-E(k)) dk$$\nThe Laplace approximation is based on a second-order Taylor expansion of $E(k)$ around its minimum. The minimum of $E(k)$ corresponds to the maximum of the posterior probability, which is the maximum a posteriori (MAP) estimate, denoted $k^{\\ast}$. At this point, the gradient of $E(k)$ is zero: $\\nabla_k E(k) |_{k=k^{\\ast}} = 0$.\n\nThe Taylor expansion of $E(k)$ around $k^{\\ast}$ is:\n$$E(k) \\approx E(k^{\\ast}) + (k - k^{\\ast})^{\\top} (\\nabla_k E(k) |_{k=k^{\\ast}}) + \\frac{1}{2} (k - k^{\\ast})^{\\top} (H_{\\text{post}}) (k - k^{\\ast})$$\nwhere $H_{\\text{post}}$ is the Hessian matrix of $E(k)$ evaluated at $k^{\\ast}$:\n$$H_{\\text{post}} = \\nabla_k^2 E(k) |_{k=k^{\\ast}}$$\nThis Hessian is the curvature of the negative log-posterior at its maximum, as stated in the problem. Since the gradient at $k^{\\ast}$ is zero, the expansion simplifies to:\n$$E(k) \\approx E(k^{\\ast}) + \\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})$$\nSubstituting this approximation back into the evidence integral:\n$$p(y \\mid \\mathcal{M}) \\approx \\int \\exp\\left(-E(k^{\\ast}) - \\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})\\right) dk$$\n$$p(y \\mid \\mathcal{M}) \\approx \\exp(-E(k^{\\ast})) \\int \\exp\\left(-\\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})\\right) dk$$\nThe integral is a standard multi-dimensional Gaussian integral. The integrand is proportional to a Gaussian probability density function with mean $k^{\\ast}$ and covariance matrix $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$. The value of the integral is given by:\n$$\\int \\exp\\left(-\\frac{1}{2} (k - k^{\\ast})^{\\top} H_{\\text{post}} (k - k^{\\ast})\\right) dk = (2\\pi)^{d/2} |\\det(H_{\\text{post}})|^{-1/2}$$\nSubstituting this result, we obtain the Laplace approximation for the model evidence:\n$$p(y \\mid \\mathcal{M}) \\approx \\exp(-E(k^{\\ast})) (2\\pi)^{d/2} |\\det(H_{\\text{post}})|^{-1/2}$$\nNow, we expand the $\\exp(-E(k^{\\ast}))$ term:\n$$\\exp(-E(k^{\\ast})) = \\exp(-\\ln p(y \\mid k^{\\ast}, \\mathcal{M}) - \\ln p(k^{\\ast} \\mid \\mathcal{M})) = p(y \\mid k^{\\ast}, \\mathcal{M}) p(k^{\\ast} \\mid \\mathcal{M})$$\nSo, the evidence is:\n$$p(y \\mid \\mathcal{M}) \\approx p(y \\mid k^{\\ast}, \\mathcal{M}) p(k^{\\ast} \\mid \\mathcal{M}) (2\\pi)^{d/2} |\\det(H_{\\text{post}})|^{-1/2}$$\nThe term $p(y \\mid k^{\\ast}, \\mathcal{M})$ is the best-fit likelihood, which measures how well the model explains the data. The remaining terms constitute the Occam factor, which penalizes model complexity.\nThe problem asks to isolate the Occam factor that represents the \"contraction of parameter-space volume from the prior to the posterior\". This is interpreted as the ratio of the characteristic volumes of the posterior and prior probability distributions. For Gaussian distributions, this volume is proportional to the square root of the determinant of the covariance matrix.\n\nThe posterior distribution is locally approximated as a Gaussian $\\mathcal{N}(k^{\\ast}, \\Sigma_{\\text{post}})$ where $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$. Its characteristic volume is proportional to $|\\det(\\Sigma_{\\text{post}})|^{1/2}$. The prior distribution is given as a Gaussian $\\mathcal{N}(\\mu_0, \\Sigma_0)$, so its characteristic volume is proportional to $|\\det(\\Sigma_0)|^{1/2}$.\n\nThe Occam factor representing the volume contraction, let's call it $O_{\\text{vol}}$, is the ratio of these volumes:\n$$O_{\\text{vol}} = \\frac{\\text{Posterior Volume}}{\\text{Prior Volume}} \\propto \\frac{|\\det(\\Sigma_{\\text{post}})|^{1/2}}{|\\det(\\Sigma_0)|^{1/2}} = \\sqrt{\\frac{\\det(\\Sigma_{\\text{post}})}{\\det(\\Sigma_0)}}$$\nThis expression gives the Occam factor entirely in terms of the prior covariance $\\Sigma_0$ and the posterior covariance $\\Sigma_{\\text{post}}$, as requested.\n\n### Part 2: Numerical Computation\n\nWe are given the following values for a $d=2$ parameter problem:\n- Prior covariance: $\\Sigma_0 = \\begin{pmatrix} 0.25  0 \\\\ 0  0.04 \\end{pmatrix}$\n- Negative log-likelihood Hessian at $k^{\\ast}$: $H_{\\text{LL}} = \\begin{pmatrix} 400  -120 \\\\ -120  50 \\end{pmatrix}$\n- Posterior curvature: $H_{\\text{post}} = H_{\\text{LL}} + \\Sigma_0^{-1}$\n- Posterior covariance: $\\Sigma_{\\text{post}} = H_{\\text{post}}^{-1}$\n\nWe need to compute $O_{\\text{vol}} = \\sqrt{\\frac{\\det(\\Sigma_{\\text{post}})}{\\det(\\Sigma_0)}}$.\nUsing the property $\\det(A^{-1}) = (\\det(A))^{-1}$, we can write $\\det(\\Sigma_{\\text{post}}) = (\\det(H_{\\text{post}}))^{-1}$. The Occam factor becomes:\n$$O_{\\text{vol}} = \\sqrt{\\frac{1}{\\det(H_{\\text{post}}) \\det(\\Sigma_0)}} = \\frac{1}{\\sqrt{\\det(H_{\\text{post}}) \\det(\\Sigma_0)}}$$\nThis form is computationally more direct.\n\nFirst, we compute the determinant of the prior covariance matrix $\\Sigma_0$:\n$$\\det(\\Sigma_0) = (0.25) \\times (0.04) = 0.01$$\n\nNext, we find the inverse of the prior covariance matrix, $\\Sigma_0^{-1}$. Since $\\Sigma_0$ is diagonal, its inverse is the matrix of reciprocals of its diagonal elements:\n$$\\Sigma_0^{-1} = \\begin{pmatrix} 1/0.25  0 \\\\ 0  1/0.04 \\end{pmatrix} = \\begin{pmatrix} 4  0 \\\\ 0  25 \\end{pmatrix}$$\n\nNow, we compute the posterior curvature matrix $H_{\\text{post}}$:\n$$H_{\\text{post}} = H_{\\text{LL}} + \\Sigma_0^{-1} = \\begin{pmatrix} 400  -120 \\\\ -120  50 \\end{pmatrix} + \\begin{pmatrix} 4  0 \\\\ 0  25 \\end{pmatrix} = \\begin{pmatrix} 404  -120 \\\\ -120  75 \\end{pmatrix}$$\n\nWe then compute the determinant of $H_{\\text{post}}$:\n$$\\det(H_{\\text{post}}) = (404 \\times 75) - (-120 \\times -120) = 30300 - 14400 = 15900$$\n\nFinally, we substitute these determinants into the expression for the Occam factor:\n$$O_{\\text{vol}} = \\frac{1}{\\sqrt{\\det(H_{\\text{post}}) \\det(\\Sigma_0)}} = \\frac{1}{\\sqrt{15900 \\times 0.01}} = \\frac{1}{\\sqrt{159}}$$\nNow we compute the numerical value:\n$$O_{\\text{vol}} = \\frac{1}{\\sqrt{159}} \\approx \\frac{1}{12.60952021} \\approx 0.07930501...$$\nRounding the result to four significant figures gives 0.07931.",
            "answer": "$$\\boxed{0.07931}$$"
        },
        {
            "introduction": "Once a promising model structure is identified, we face a second critical challenge: can we uniquely determine its parameters from the available data? This exercise confronts the crucial issue of parameter identifiability, a frequent obstacle in the modeling of complex biological systems. You will implement the profile likelihood method, a powerful statistical tool, to diagnose identifiability issues for a Hill-type gene regulation model and explore how experimental design choices directly impact our ability to constrain parameter values .",
            "id": "3906800",
            "problem": "You are given a gene regulation scenario modeled by a Hill activation function grounded in thermodynamic binding equilibrium. Let $I$ denote the inducer concentration (in $\\mu$M), let $y$ denote the measured expression level (in unitless relative fluorescence units), and let $n$ denote the Hill coefficient. The canonical Hill activation model for expression is\n$$\ny(I;\\beta_0,\\beta,K,n) \\;=\\; \\beta_0 \\;+\\; \\beta \\,\\frac{I^n}{K^n + I^n},\n$$\nwhere $\\beta_0 \\ge 0$ is a basal expression level, $\\beta \\ge 0$ is the maximal inducible amplitude, and $K  0$ is the half-activation concentration. Assume additive Gaussian measurement noise consistent with Maximum Likelihood Estimation (MLE) under a Normal model, i.e., for each measurement $y_{\\text{obs}}$ at concentration $I$, \n$$\ny_{\\text{obs}} \\;=\\; y(I;\\beta_0,\\beta,K,n) \\;+\\; \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith known noise standard deviation $\\sigma$. The log-likelihood under independent Gaussian errors for a dataset $\\{(I_i,y_{\\text{obs},i})\\}_{i=1}^{N}$, ignoring the additive constant, is proportional to the negative residual sum of squares (RSS),\n$$\n\\ell(\\beta_0,\\beta,K,n) \\;=\\; -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}\\left[y_{\\text{obs},i} - y(I_i;\\beta_0,\\beta,K,n)\\right]^2.\n$$\nThe profile log-likelihood for the Hill coefficient $n$ is defined by\n$$\n\\ell_{\\text{prof}}(n) \\;=\\; \\max_{\\beta_0 \\ge 0,\\, \\beta \\ge 0,\\, K  0}\\; \\ell(\\beta_0,\\beta,K,n),\n$$\nwhich removes nuisance parameters by maximizing over them for each fixed $n$. In automated model discovery using Machine Learning (ML), such profiling guides the identifiability analysis and informs experimental design choices: a flat $\\ell_{\\text{prof}}(n)$ over a wide range of $n$ indicates non-identifiability of $n$ under the current data and design.\n\nYour task is to implement a complete program that, for each provided test case, computes the normalized profile log-likelihood $\\ell_{\\text{prof}}(n)$ over a grid of $n$ values and decides whether the profile exhibits a flat region indicative of non-identifiability. Use the following principle-based rules:\n\n1. For each fixed $n$ on a specified grid, perform Maximum Likelihood Estimation with respect to $(\\beta_0,\\beta,K)$ by minimizing the residual sum of squares, respecting the constraints $\\beta_0 \\ge 0$, $\\beta \\ge 0$, $K  0$. Interpret this minimization as ML under Gaussian noise with known $\\sigma$.\n\n2. Compute the profile $\\ell_{\\text{prof}}(n)$ and normalize it by subtracting its maximum value so that the peak value is $0$:\n$$\n\\tilde{\\ell}_{\\text{prof}}(n) \\;=\\; \\ell_{\\text{prof}}(n) - \\max_{n}\\ell_{\\text{prof}}(n).\n$$\n\n3. Quantify flatness by two complementary metrics:\n   - Quadratic-curvature width around the peak: fit a quadratic model to $\\tilde{\\ell}_{\\text{prof}}(n)$ in a small neighborhood around the maximizing $\\hat{n}$ to obtain curvature $a$ (the coefficient of $(n-\\hat{n})^2$). Using the single-parameter likelihood-ratio approximation, define the $95\\%$ drop level $d = 1.92$ and estimate the full width at the drop as\n   $$\n   w \\;=\\; 2\\sqrt{\\frac{d}{|a|}}\\quad\\text{if }a0,\\;\\text{and }w=+\\infty\\text{ if }a \\ge 0.\n   $$\n   - Global drop magnitude: compute $\\Delta \\;=\\; \\left|\\min_{n}\\tilde{\\ell}_{\\text{prof}}(n)\\right|$.\n\n4. Classify the Hill coefficient $n$ as non-identifiable if either $w  1.5$ or $\\Delta  0.8$. Otherwise, classify it as identifiable. Return a boolean per test case: $True$ indicates non-identifiable, $False$ indicates identifiable.\n\nThe program must implement the above using a grid of $n$ values from $0.5$ to $5.0$ in steps of $0.05$, respecting units for $I$ in $\\mu$M and treating $y$ as unitless. All optimization must be within the allowed runtime environment and adhere to constraints on libraries.\n\nTest Suite:\nUse the following four synthetic test cases (all generated internally by your program with a fixed random seed to ensure reproducibility):\n\n- Case $1$ (well-designed, low noise; expected identifiable): \n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=20$ values, logarithmically spaced from $0.1$ to $100.0$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n\n- Case $2$ (saturated-only design, low noise; expected non-identifiable):\n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=10$ values linearly spaced from $100.0$ to $200.0$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n\n- Case $3$ (subthreshold-only design, low noise; expected non-identifiable):\n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=10$ values linearly spaced from $0.01$ to $0.20$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n\n- Case $4$ (well-designed but high noise; expected non-identifiable):\n  - True parameters: $\\beta_0 = 0.1$, $\\beta = 1.0$, $K = 10.0$, $n_{\\text{true}} = 2.0$.\n  - Concentrations $I$ (in $\\mu$M): $N=8$ values logarithmically spaced from $0.1$ to $100.0$.\n  - Noise standard deviation: $\\sigma = 0.20$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is a boolean indicating whether the Hill coefficient $n$ is non-identifiable for the corresponding case under the specified classification rule. No additional text should be printed.",
            "solution": "The problem statement has been meticulously validated and is determined to be scientifically sound, well-posed, and objective. It presents a formal and verifiable task in the field of synthetic biology modeling and automated model discovery. We may therefore proceed with a complete solution.\n\nThe core of the problem is to assess the identifiability of the Hill coefficient, $n$, from experimental data. A parameter is considered identifiable if the available data are sufficient to constrain its value to a narrow range. The profile log-likelihood provides a rigorous statistical tool for this assessment. A sharply peaked profile indicates high identifiability, whereas a flat profile over a wide range of parameter values indicates non-identifiability. The task requires implementing an algorithm to compute and analyze this profile for different experimental scenarios.\n\nThe solution is structured into four principal stages:\n1.  Generation of synthetic experimental data for each test case.\n2.  Computation of the profile log-likelihood, $\\ell_{\\text{prof}}(n)$, over a grid of $n$ values.\n3.  Analysis of the normalized profile, $\\tilde{\\ell}_{\\text{prof}}(n)$, to quantify its flatness using two metrics.\n4.  Classification of $n$ as identifiable or non-identifiable based on quantitative thresholds.\n\n**1. Synthetic Data Generation**\n\nFor each test case, we first generate synthetic data that mimic experimental measurements. The true underlying process is described by the Hill activation function:\n$$\ny(I;\\beta_0,\\beta,K,n) \\;=\\; \\beta_0 \\;+\\; \\beta \\,\\frac{I^n}{K^n + I^n}\n$$\nwhere $I$ is the inducer concentration, $\\beta_0$ is the basal expression, $\\beta$ is the inducible amplitude, $K$ is the half-activation concentration, and $n$ is the Hill coefficient.\n\nFor a set of inducer concentrations $\\{I_i\\}_{i=1}^{N}$, the true expression levels $\\{y_{\\text{true},i}\\}_{i=1}^{N}$ are calculated using the true parameter values provided for each case. Experimental measurements, $y_{\\text{obs},i}$, are simulated by adding independent and identically distributed Gaussian noise to the true values:\n$$\ny_{\\text{obs},i} \\;=\\; y(I_i;\\beta_{0,\\text{true}},\\beta_{\\text{true}},K_{\\text{true}},n_{\\text{true}}) \\;+\\; \\varepsilon_i,\\quad \\text{where } \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n$$\nThe noise standard deviation, $\\sigma$, is known for each case. All data generation is performed using a fixed random seed to ensure reproducibility.\n\n**2. Profile Log-Likelihood Computation**\n\nThe profile log-likelihood for $n$, denoted $\\ell_{\\text{prof}}(n)$, is found by maximizing the log-likelihood function over the nuisance parameters $(\\beta_0, \\beta, K)$ for each fixed value of $n$. Assuming independent Gaussian noise, maximizing the log-likelihood is equivalent to minimizing the Residual Sum of Squares (RSS):\n$$\n\\text{RSS}(\\beta_0,\\beta,K) \\;=\\; \\sum_{i=1}^{N}\\left[y_{\\text{obs},i} - y(I_i;\\beta_0,\\beta,K,n)\\right]^2\n$$\nThus, for each $n$ on a predefined grid from $0.5$ to $5.0$, we solve the following constrained optimization problem:\n$$\n\\text{RSS}_{\\min}(n) \\;=\\; \\min_{\\beta_0 \\ge 0,\\, \\beta \\ge 0,\\, K  0} \\text{RSS}(\\beta_0,\\beta,K)\n$$\nThis is a non-linear least squares problem. We employ the `L-BFGS-B` algorithm, available through `scipy.optimize.minimize`, which is well-suited for this type of problem as it can handle the box constraints (i.e., lower bounds) required for the parameters: $\\beta_0 \\ge 0$, $\\beta \\ge 0$, and $K  0$. For numerical stability, the constraint on $K$ is implemented as $K \\ge \\epsilon$ for a small positive $\\epsilon$.\n\nOnce $\\text{RSS}_{\\min}(n)$ is found for each $n$ in the grid, the corresponding profile log-likelihood value is calculated as:\n$$\n\\ell_{\\text{prof}}(n) \\;=\\; -\\frac{\\text{RSS}_{\\min}(n)}{2\\sigma^2}\n$$\n\n**3. Profile Analysis and Identifiability Classification**\n\nTo facilitate comparison across different datasets and models, the profile is normalized by subtracting its maximum value:\n$$\n\\tilde{\\ell}_{\\text{prof}}(n) \\;=\\; \\ell_{\\text{prof}}(n) - \\max_{n'}\\ell_{\\text{prof}}(n')\n$$\nThe peak of this normalized profile is at $0$, corresponding to the maximum likelihood estimate of the Hill coefficient, $\\hat{n}$. The flatness of this profile is then quantified using two metrics:\n\n-   **Quadratic-Curvature Width ($w$)**: The profile's shape near its peak at $\\hat{n}$ is approximated by a quadratic function, $p(n) = a(n-\\hat{n})^2 + b(n-\\hat{n}) + c$. The curvature, $a$, is the coefficient of the squared term. This is obtained by fitting a second-degree polynomial to a small neighborhood of points around the peak of $\\tilde{\\ell}_{\\text{prof}}(n)$. Based on likelihood-ratio theory, a confidence interval can be estimated from this curvature. The full width of the profile at a drop of $d = 1.92$ (corresponding to a 95% confidence interval for a single parameter) is given by:\n    $$\n    w \\;=\\; \\begin{cases} 2\\sqrt{d/|a|}  \\text{if } a  0 \\\\ +\\infty  \\text{if } a \\ge 0 \\end{cases}\n    $$\n    An upward curvature ($a \\ge 0$) at the maximum implies an extremely flat or ill-behaved profile, signifying non-identifiability, which is captured by an infinite width.\n\n-   **Global Drop Magnitude ($\\Delta$)**: This metric captures the total drop in log-likelihood across the entire evaluated range of $n$. It is defined as the absolute value of the minimum of the normalized profile:\n    $$\n    \\Delta \\;=\\; \\left|\\min_{n}\\tilde{\\ell}_{\\text{prof}}(n)\\right|\n    $$\n    A small $\\Delta$ indicates that even far from the optimal $\\hat{n}$, other values of $n$ are nearly as plausible, which is a hallmark of non-identifiability.\n\nFinally, the Hill coefficient $n$ is classified as non-identifiable if the profile is either too wide or too shallow, according to the specified thresholds:\n$$\n\\text{Non-identifiable if } (w  1.5) \\text{ or } (\\Delta  0.8)\n$$\nA boolean value of `True` is returned for non-identifiable cases, and `False` otherwise.\n\n**4. Implementation**\n\nThe described algorithm is implemented in Python, utilizing `numpy` for efficient numerical computation and `scipy.optimize.minimize` for the constrained non-linear optimization at the core of the profile likelihood calculation. A main function iterates through the four specified test cases, each defined by a set of true parameters and an experimental design (concentration range and sampling). For each case, it generates data, computes the profile, analyzes it, and determines the identifiability, appending the final boolean result to a list. The program concludes by printing this list in the specified format. This systematic approach directly translates the principles of statistical identifiability analysis into a concrete computational workflow.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the identifiability problem for all test cases.\n    \"\"\"\n    # Set a single fixed random seed for reproducibility across all cases.\n    np.random.seed(42)\n\n    # Define the four test cases as specified in the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1: Well-designed, low noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0), # beta0, beta, K, n\n            \"I_config\": {\"type\": \"log\", \"N\": 20, \"start\": 0.1, \"end\": 100.0},\n            \"sigma\": 0.02\n        },\n        {\n            \"name\": \"Case 2: Saturated-only, low noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0),\n            \"I_config\": {\"type\": \"lin\", \"N\": 10, \"start\": 100.0, \"end\": 200.0},\n            \"sigma\": 0.02\n        },\n        {\n            \"name\": \"Case 3: Subthreshold-only, low noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0),\n            \"I_config\": {\"type\": \"lin\", \"N\": 10, \"start\": 0.01, \"end\": 0.20},\n            \"sigma\": 0.02\n        },\n        {\n            \"name\": \"Case 4: Well-designed, high noise\",\n            \"true_params\": (0.1, 1.0, 10.0, 2.0),\n            \"I_config\": {\"type\": \"log\", \"N\": 8, \"start\": 0.1, \"end\": 100.0},\n            \"sigma\": 0.20\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        is_non_identifiable = analyze_case(case)\n        results.append(is_non_identifiable)\n\n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef hill_function(params, I, n):\n    \"\"\"Calculates the Hill activation function response.\"\"\"\n    beta0, beta, K = params\n    # Use numerically stable form: 1 / ( (K/I)^n + 1 )\n    # Handle potential division by zero if I can be 0, though problem constraints avoid this.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        term = 1.0 / (1.0 + (K / I)**n)\n    # If I is very small, (K/I)^n can be inf, and term becomes 0, which is correct.\n    term[np.isinf(I)] = 1.0 # If I is infinite, term is 1.\n    term[I == 0] = 0.0 # If I is zero, term is 0.\n    return beta0 + beta * term\n\n\ndef rss_objective(params, I, y_obs, n):\n    \"\"\"Calculates the Residual Sum of Squares (RSS) for the Hill model.\"\"\"\n    y_model = hill_function(params, I, n)\n    return np.sum((y_obs - y_model)**2)\n\n\ndef analyze_case(case_spec):\n    \"\"\"\n    Performs the full identifiability analysis for a single test case.\n    \"\"\"\n    # 1. Generate synthetic data\n    beta0_true, beta_true, K_true, n_true = case_spec[\"true_params\"]\n    I_config = case_spec[\"I_config\"]\n    sigma = case_spec[\"sigma\"]\n\n    if I_config[\"type\"] == 'log':\n        I = np.logspace(np.log10(I_config[\"start\"]), np.log10(I_config[\"end\"]), I_config[\"N\"])\n    else: # 'lin'\n        I = np.linspace(I_config[\"start\"], I_config[\"end\"], I_config[\"N\"])\n\n    y_true = hill_function((beta0_true, beta_true, K_true), I, n_true)\n    noise = np.random.normal(0, sigma, size=I_config[\"N\"])\n    y_obs = y_true + noise\n\n    # 2. Compute profile log-likelihood\n    n_grid = np.arange(0.5, 5.0 + 1e-9, 0.05)\n    l_prof = np.zeros_like(n_grid)\n    \n    # Heuristic initial guess for optimizer\n    beta0_guess = np.min(y_obs) if np.min(y_obs) > 0 else 1e-3\n    beta_guess = np.max(y_obs) - beta0_guess if (np.max(y_obs) - beta0_guess) > 0 else 0.1\n    mid_y_range = beta0_guess + 0.5 * beta_guess\n    k_guess_idx = np.argmin(np.abs(y_obs - mid_y_range))\n    k_guess = I[k_guess_idx]\n    x0 = [beta0_guess, beta_guess, k_guess]\n\n    bounds = [(0, None), (0, None), (1e-9, None)]\n\n    for i, n_val in enumerate(n_grid):\n        opt_result = minimize(\n            rss_objective,\n            x0,\n            args=(I, y_obs, n_val),\n            bounds=bounds,\n            method='L-BFGS-B'\n        )\n        min_rss = opt_result.fun\n        l_prof[i] = -min_rss / (2 * sigma**2)\n\n    # 3. Analyze the profile\n    if np.all(np.isinf(l_prof)) or np.all(np.isnan(l_prof)):\n        return True # Profile computation failed, indicates extreme non-identifiability\n\n    l_prof_norm = l_prof - np.max(l_prof)\n\n    # Metric 1: Global drop magnitude\n    delta = np.abs(np.min(l_prof_norm))\n\n    # Metric 2: Quadratic-curvature width\n    idx_max = np.argmax(l_prof_norm)\n    \n    # Define neighborhood for quadratic fit, handling edges\n    width_pts = 3\n    start_idx = max(0, idx_max - width_pts)\n    end_idx = min(len(n_grid), idx_max + width_pts + 1)\n    \n    if (end_idx - start_idx)  3:\n        # Not enough points for a quadratic fit, implies a pathological profile (e.g., max at edge)\n        w = np.inf\n    else:\n        n_hood = n_grid[start_idx:end_idx]\n        l_prof_hood = l_prof_norm[start_idx:end_idx]\n        \n        # Fit a 2nd-degree polynomial: a*n^2 + b*n + c\n        coeffs = np.polyfit(n_hood, l_prof_hood, 2)\n        a = coeffs[0]\n\n        d = 1.92\n        if a >= -1e-9: # Curvature is non-negative (or very close to zero)\n            w = np.inf\n        else:\n            w = 2 * np.sqrt(d / np.abs(a))\n\n    # 4. Classify based on thresholds\n    is_non_identifiable = (w > 1.5) or (delta  0.8)\n    return is_non_identifiable\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Identifying a non-identifiable parameter is not an endpoint, but rather a signal that more informative experiments are needed. This hands-on practice closes the loop between modeling and experimentation by focusing on optimal experimental design (OED). You will use the Fisher Information Matrix, a fundamental concept in statistics, to computationally design an input stimulus that maximizes the information gained about a parameter of interest, demonstrating how to proactively resolve the ambiguities uncovered during identifiability analysis .",
            "id": "3906793",
            "problem": "Automated model discovery in synthetic biology often requires selecting informative input stimuli to estimate latent biophysical parameters from noisy time series data. Consider a minimal, mechanistically grounded transcription-translation module modeled as a first-order linear system driven by a scalar input stimulus. The dynamical state $x(t)$ (observable reporter level) obeys the ordinary differential equation $dx/dt = -k_2 x(t) + k_1 u(t)$ with zero initial condition, where $k_1$ and $k_2$ are unknown positive parameters and $u(t)$ is an externally controlled input stimulus that can be switched on and off. Measurements $y_i$ are recorded at discrete times $t_i = i \\Delta t$ for $i = 0,1,\\dots,N-1$, with independent additive Gaussian measurement noise of zero mean and variance $\\sigma^2$. Only the reporter $y_i = x(t_i)$ is observed. The ordered pair $(k_1,k_2)$ may be poorly identifiable under standard stimuli, but the ratio $\\phi = k_1/k_2$ is of primary interest.\n\nStarting from the linear system definition, the convolution representation for $x(t)$, and properties of independent Gaussian noise, derive a Fisher information based scalar objective for the ratio $\\phi$. Use the following foundational facts as the starting point: (i) superposition and convolution for linear time-invariant systems, (ii) the Fisher Information Matrix (FIM) for a model with additive independent Gaussian noise is $F = \\sigma^{-2} S^\\top S$ where $S$ is the sensitivity matrix whose entries are partial derivatives of the model output with respect to parameters, and (iii) the delta method implies that for a scalar transformation $\\phi = g(\\theta)$, where $\\theta = (k_1,k_2)$, the Fisher information for $\\phi$ is $I_\\phi = \\nabla_\\theta g(\\theta)^\\top F \\nabla_\\theta g(\\theta)$. You must not assume any preset stimuli; instead, you must computationally optimize the input pattern within a constrained family to maximize $I_\\phi$.\n\nAssume the following experimental design space: $u(t)$ is constrained to be a single contiguous rectangular pulse of amplitude $u_{\\max}$ and duration $L \\Delta t$, starting at some grid index $s \\in \\{0,1,\\dots,N-L\\}$, with $L$ chosen from a finite allowed set $\\mathcal{L} \\subset \\{0,1,\\dots,N\\}$. If $L=0$, the input is identically zero. For a given $(k_1,k_2,\\Delta t,N,\\sigma,u_{\\max},\\mathcal{L})$, your task is to search over all feasible pulse lengths $L \\in \\mathcal{L}$ and start indices $s$ to find the input that maximizes the Fisher information $I_\\phi$ for the ratio $\\phi = k_1/k_2$. You must compute $I_\\phi$ exactly as specified by the Fisher-based criteria described above using a Riemann sum approximation consistent with the discrete sampling grid.\n\nYour program must implement the following steps for each test case:\n- Construct the sample times $t_i = i \\Delta t$ for $i=0,\\dots,N-1$.\n- For a candidate pulse characterized by $(L,s)$ with amplitude $u_{\\max}$, construct the discrete input sequence $u_j$, $j=0,\\dots,N-1$, where $u_j = u_{\\max}$ for $j \\in \\{s,\\dots,s+L-1\\}$ and $u_j = 0$ otherwise (if $L=0$, then $u_j=0$ for all $j$).\n- Compute the sensitivities at each sample time using the convolutional representation and Riemann sums:\n  - $s_{i,1} = \\frac{\\partial x(t_i)}{\\partial k_1} \\approx \\sum_{j=0}^{i} u_j \\exp\\!\\left(-k_2 (t_i - t_j)\\right) \\Delta t$.\n  - $s_{i,2} = \\frac{\\partial x(t_i)}{\\partial k_2} \\approx -k_1 \\sum_{j=0}^{i} (t_i - t_j) u_j \\exp\\!\\left(-k_2 (t_i - t_j)\\right) \\Delta t$.\n- Form the sensitivity matrix $S \\in \\mathbb{R}^{N \\times 2}$ with columns $s_{\\cdot,1}$ and $s_{\\cdot,2}$, then compute the Fisher Information Matrix $F = \\sigma^{-2} S^\\top S$.\n- With $\\phi = k_1/k_2$, compute $\\nabla_\\theta g(\\theta) = \\left[\\frac{\\partial \\phi}{\\partial k_1}, \\frac{\\partial \\phi}{\\partial k_2}\\right]^\\top = \\left[\\frac{1}{k_2}, -\\frac{k_1}{k_2^2}\\right]^\\top$ and then $I_\\phi = \\nabla_\\theta g(\\theta)^\\top F \\nabla_\\theta g(\\theta)$.\n- Return the maximum $I_\\phi$ across all admissible pulse designs.\n\nTest Suite:\nFor each case, search the full admissible design family and report the single maximal value of $I_\\phi$ found. The required final output for the entire program is a single line containing a list of floating-point numbers, one per test case, rounded to six decimal places, enclosed in square brackets and separated by commas (for example, \"[1.234000,0.000000]\").\n\n- Case 1 (happy path):\n  - $k_1 = 2.0$, $k_2 = 0.8$, $\\Delta t = 0.5$, $N = 20$, $\\sigma = 0.1$, $u_{\\max} = 1.0$, $\\mathcal{L} = \\{2,4,6\\}$.\n- Case 2 (boundary, no input allowed):\n  - $k_1 = 1.2$, $k_2 = 0.6$, $\\Delta t = 0.5$, $N = 20$, $\\sigma = 0.1$, $u_{\\max} = 1.0$, $\\mathcal{L} = \\{0\\}$.\n- Case 3 (fast decay edge case):\n  - $k_1 = 1.5$, $k_2 = 3.0$, $\\Delta t = 0.2$, $N = 30$, $\\sigma = 0.2$, $u_{\\max} = 1.0$, $\\mathcal{L} = \\{3,6\\}$.\n- Case 4 (high noise, longer horizon):\n  - $k_1 = 1.0$, $k_2 = 1.0$, $\\Delta t = 0.5$, $N = 25$, $\\sigma = 1.0$, $u_{\\max} = 1.0$, $\\mathcal{L} = \\{5,10\\}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being the maximum $I_\\phi$ for the corresponding test case, rounded to six decimal places, in the order of the cases given above. No other output should be printed.",
            "solution": "The user-provided problem statement has been critically evaluated and is determined to be **valid**.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n\n- **Dynamical System**: First-order linear ordinary differential equation (ODE) $dx/dt = -k_2 x(t) + k_1 u(t)$, with initial condition $x(0) = 0$.\n- **Parameters**: Unknown positive parameters $k_1$ and $k_2$.\n- **Input Stimulus**: $u(t)$ is an externally controlled input.\n- **Measurement Model**: $y_i = x(t_i) + \\epsilon_i$, where measurements are taken at discrete times $t_i = i \\Delta t$ for $i = 0,1,\\dots,N-1$.\n- **Noise Model**: Measurement noise $\\epsilon_i$ is independent and identically distributed (i.i.d.) Gaussian with zero mean and variance $\\sigma^2$.\n- **Parameter of Interest**: The ratio $\\phi = k_1/k_2$.\n- **Experimental Design Space**:\n    - $u(t)$ is a single contiguous rectangular pulse.\n    - Amplitude: $u_{\\max}$.\n    - Duration: $L \\Delta t$, where the length parameter $L$ is chosen from a finite set $\\mathcal{L} \\subset \\{0,1,\\dots,N\\}$. If $L=0$, $u(t)=0$.\n    - Start index: $s \\in \\{0,1,\\dots,N-L\\}$.\n- **Mathematical Tools  Formulas**:\n    - **Fisher Information Matrix (FIM)**: $F = \\sigma^{-2} S^\\top S$, where $S$ is the sensitivity matrix.\n    - **Sensitivity Matrix entries ($S_{i,j}$)**: $S_{ik} = \\frac{\\partial x(t_i)}{\\partial \\theta_k}$ for $\\theta = (k_1, k_2)$.\n    - **Sensitivity Approximations (Riemann sums)**:\n        - $s_{i,1} = \\frac{\\partial x(t_i)}{\\partial k_1} \\approx \\sum_{j=0}^{i} u_j \\exp\\!\\left(-k_2 (t_i - t_j)\\right) \\Delta t$.\n        - $s_{i,2} = \\frac{\\partial x(t_i)}{\\partial k_2} \\approx -k_1 \\sum_{j=0}^{i} (t_i - t_j) u_j \\exp\\!\\left(-k_2 (t_i - t_j)\\right) \\Delta t$.\n        - where $u_j$ is the value of the input stimulus at time $t_j$.\n    - **Information for Transformed Parameter (Delta Method)**: $I_\\phi = \\nabla_\\theta g(\\theta)^\\top F \\nabla_\\theta g(\\theta)$ for $\\phi = g(\\theta)$.\n    - **Gradient of Transformation**: For $\\phi = k_1/k_2$, $\\nabla_\\theta g(\\theta) = \\left[\\frac{1}{k_2}, -\\frac{k_1}{k_2^2}\\right]^\\top$.\n- **Task**: For given parameter sets, find the input pulse design $(L,s)$ that maximizes $I_\\phi$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is **valid**.\n- **Scientifically Grounded**: The problem is built upon foundational principles of systems theory (linear ODEs, convolution), statistical inference (Fisher information), and numerical methods (Riemann sums). The model is a standard representation for simple biochemical processes like transcription and translation, a common subject in synthetic biology.\n- **Well-Posed**: The problem defines a clear, scalar objective function ($I_\\phi$) to be maximized over a finite, explicitly defined search space of experimental designs. This structure guarantees that a solution (a maximum value) exists.\n- **Objective**: The problem is stated using precise mathematical and scientific language, free of subjectivity or ambiguity.\n- **Self-Contained and Consistent**: All necessary parameters, equations, and constraints for each test case are provided. The mathematical formulas are internally consistent and derived from standard principles. The discrete approximations for the sensitivities are given explicitly, removing any ambiguity in their calculation.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation and Algorithmic Strategy\n\nThe objective is to find an optimal input stimulus $u(t)$ from a constrained family of functions that maximizes the Fisher information $I_\\phi$ for the parameter ratio $\\phi = k_1/k_2$. The input stimuli are rectangular pulses characterized by a length $L$ and a start index $s$.\n\nThe Fisher information for the transformed parameter $\\phi$ is given by the delta method:\n$$\nI_\\phi = \\nabla_\\theta g(\\theta)^\\top F \\nabla_\\theta g(\\theta)\n$$\nHere, $\\theta = (k_1, k_2)$ is the vector of model parameters, and $g(\\theta)=k_1/k_2$ is the scalar transformation. The components of this expression are:\n\n1.  **Gradient of the Transformation, $\\nabla_\\theta g(\\theta)$**:\n    The partial derivatives of $\\phi$ with respect to $k_1$ and $k_2$ are:\n    $$\n    \\frac{\\partial \\phi}{\\partial k_1} = \\frac{1}{k_2}\n    $$\n    $$\n    \\frac{\\partial \\phi}{\\partial k_2} = -\\frac{k_1}{k_2^2}\n    $$\n    Thus, the gradient vector is $\\nabla_\\theta g(\\theta) = \\begin{bmatrix} 1/k_2  -k_1/k_2^2 \\end{bmatrix}^\\top$. For any given test case, the values of $k_1$ and $k_2$ are known, so this vector is a constant.\n\n2.  **Fisher Information Matrix, $F$**:\n    For a model with additive i.i.d. Gaussian noise, the FIM for the parameters $\\theta$ is:\n    $$\n    F = \\frac{1}{\\sigma^2} S^\\top S\n    $$\n    The term $\\sigma^2$ is the noise variance, a known constant for each test case. The matrix $S$ is the sensitivity matrix, which captures how the model output changes with respect to the parameters. Its entries are $S_{ik} = \\frac{\\partial x(t_i)}{\\partial \\theta_k}$.\n\n3.  **Sensitivity Matrix, $S$**:\n    The sensitivity matrix $S$ is an $N \\times 2$ matrix where the first column contains the sensitivities with respect to $k_1$ and the second column contains sensitivities with respect to $k_2$, evaluated at each of the $N$ sample times.\n    $$\n    S = \\begin{bmatrix}\n    \\frac{\\partial x(t_0)}{\\partial k_1}  \\frac{\\partial x(t_0)}{\\partial k_2} \\\\\n    \\frac{\\partial x(t_1)}{\\partial k_1}  \\frac{\\partial x(t_1)}{\\partial k_2} \\\\\n    \\vdots  \\vdots \\\\\n    \\frac{\\partial x(t_{N-1})}{\\partial k_1}  \\frac{\\partial x(t_{N-1})}{\\partial k_2}\n    \\end{bmatrix}\n    $$\n    The crucial insight is that the sensitivity matrix $S$ is the only component in the expression for $I_\\phi$ that depends on the input stimulus $u(t)$. Therefore, our optimization problem reduces to finding the input $u(t)$ (i.e., the pulse $(L,s)$) that makes the quadratic form involving $S^\\top S$ as large as possible.\n\nThe problem provides the explicit Riemann sum approximations for the sensitivity values:\n-   $s_{i,1} = \\frac{\\partial x(t_i)}{\\partial k_1} \\approx \\Delta t \\sum_{j=0}^{i} u_j e^{-k_2 (t_i - t_j)}$\n-   $s_{i,2} = \\frac{\\partial x(t_i)}{\\partial k_2} \\approx -k_1 \\Delta t \\sum_{j=0}^{i} (t_i - t_j) u_j e^{-k_2 (t_i - t_j)}$\n\nThese summations must be computed for each time point $i \\in \\{0, \\dots, N-1\\}$.\n\n### Computational Search Procedure\n\nThe optimal design is found by systematically searching the entire discrete space of allowed input pulses. The algorithm for each test case is as follows:\n\n1.  Initialize a variable `max_I_phi` to $0.0$, as information is non-negative. The $L=0$ case (zero input) yields $I_\\phi = 0$.\n2.  Define the sampling times $t_i = i \\Delta t$ for $i=0, \\dots, N-1$.\n3.  Loop through each allowed pulse length $L \\in \\mathcal{L}$.\n4.  For each $L  0$, loop through all possible start indices $s \\in \\{0, 1, \\dots, N-L\\}$.\n5.  For each candidate design $(L,s)$:\n    a. Construct the discrete input vector $u \\in \\mathbb{R}^N$. Set $u_j = u_{\\max}$ for $j \\in \\{s, \\dots, s+L-1\\}$ and $u_j = 0$ otherwise.\n    b. Compute the $N \\times 2$ sensitivity matrix $S$. This requires two nested loops: an outer loop for the time index $i$ from $0$ to $N-1$, and an inner loop for the summation index $j$ from $0$ to $i$. Inside the loops, compute the terms for $s_{i,1}$ and $s_{i,2}$ according to their formulas.\n    c. Calculate the $2 \\times 2$ matrix $S^\\top S$.\n    d. Calculate the FIM $F = \\sigma^{-2} S^\\top S$.\n    e. Calculate the gradient vector $\\nabla_\\theta g(\\theta)$.\n    f. Compute the scalar Fisher information $I_\\phi = (\\nabla_\\theta g(\\theta))^\\top F (\\nabla_\\theta g(\\theta))$.\n    g. Update `max_I_phi` if the newly computed $I_\\phi$ is greater: `max_I_phi = max(max_I_phi, I_phi)`.\n6.  After all $(L,s)$ pairs have been evaluated, `max_I_phi` holds the maximum information achievable for the given experimental constraints. This value is stored as the result for the test case.\n\nThis procedure is implemented for each test case specified in the problem, and the final results are formatted as requested.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_max_fisher_information(k1, k2, dt, N, sigma, u_max, L_set):\n    \"\"\"\n    Computes the maximum Fisher information for the ratio phi = k1/k2 by searching\n    over a space of rectangular pulse experimental designs.\n\n    Args:\n        k1 (float): Parameter k1.\n        k2 (float): Parameter k2.\n        dt (float): Time step delta_t.\n        N (int): Number of time points.\n        sigma (float): Standard deviation of measurement noise.\n        u_max (float): Amplitude of the input pulse.\n        L_set (set): Set of allowed pulse lengths L (in grid units).\n\n    Returns:\n        float: The maximum Fisher information I_phi found.\n    \"\"\"\n    t = np.arange(N) * dt\n    max_I_phi = 0.0\n\n    # The gradient of phi = k1/k2 w.r.t. theta = (k1, k2) is constant for a given case.\n    # grad_g = [d(phi)/dk1, d(phi)/dk2] = [1/k2, -k1/k2^2]\n    grad_g = np.array([1 / k2, -k1 / (k2**2)])\n\n    # Handle the L=0 case separately. If L=0, u is always zero, so S=0, F=0, I_phi=0.\n    # Since max_I_phi is initialized to 0.0, this case is implicitly handled.\n    if 0 in L_set and len(L_set) == 1:\n        return 0.0\n        \n    for L in L_set:\n        if L == 0:\n            continue\n        \n        # Iterate over all possible start times for a pulse of length L\n        for s in range(N - L + 1):\n            # 1. Construct the input signal u for the current design (L, s)\n            u = np.zeros(N)\n            u[s : s + L] = u_max\n\n            # 2. Compute the sensitivity matrix S\n            S = np.zeros((N, 2))\n            for i in range(N):\n                sum1 = 0.0  # For sensitivity wrt k1\n                sum2 = 0.0  # For sensitivity wrt k2\n                \n                # The summation is from j=0 to i, as specified.\n                for j in range(i + 1):\n                    if u[j] > 0:\n                        time_diff = t[i] - t[j]\n                        exp_term = np.exp(-k2 * time_diff)\n                        \n                        # From problem: s_i1 = sum(u_j * exp(...) * dt)\n                        sum1 += u[j] * exp_term\n                        \n                        # From problem: s_i2 = -k1 * sum((t_i-t_j) * u_j * exp(...) * dt)\n                        sum2 += (time_diff * u[j] * exp_term)\n\n                S[i, 0] = sum1 * dt\n                S[i, 1] = -k1 * sum2 * dt\n\n            # 3. Compute the Fisher Information Matrix (FIM) for theta=(k1, k2)\n            # F = (1/sigma^2) * S^T * S\n            F = (1 / sigma**2) * (S.T @ S)\n\n            # 4. Compute Fisher information for phi\n            # I_phi = grad_g^T * F * grad_g\n            I_phi = grad_g.T @ F @ grad_g\n            \n            # 5. Update the maximum I_phi found\n            if I_phi > max_I_phi:\n                max_I_phi = I_phi\n                \n    return max_I_phi\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1: happy path\n        {'k1': 2.0, 'k2': 0.8, 'dt': 0.5, 'N': 20, 'sigma': 0.1, 'u_max': 1.0, 'L_set': {2, 4, 6}},\n        # Case 2: boundary, no input allowed\n        {'k1': 1.2, 'k2': 0.6, 'dt': 0.5, 'N': 20, 'sigma': 0.1, 'u_max': 1.0, 'L_set': {0}},\n        # Case 3: fast decay edge case\n        {'k1': 1.5, 'k2': 3.0, 'dt': 0.2, 'N': 30, 'sigma': 0.2, 'u_max': 1.0, 'L_set': {3, 6}},\n        # Case 4: high noise, longer horizon\n        {'k1': 1.0, 'k2': 1.0, 'dt': 0.5, 'N': 25, 'sigma': 1.0, 'u_max': 1.0, 'L_set': {5, 10}},\n    ]\n\n    results = []\n    for params in test_cases:\n        max_I_phi = calculate_max_fisher_information(**params)\n        results.append(max_I_phi)\n\n    # Format the final output string exactly as required.\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}