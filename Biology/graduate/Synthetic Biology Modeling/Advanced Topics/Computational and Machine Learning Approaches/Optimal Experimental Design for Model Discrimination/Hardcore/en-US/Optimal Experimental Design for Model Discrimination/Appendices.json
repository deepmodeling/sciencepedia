{
    "hands_on_practices": [
        {
            "introduction": "To discriminate between competing models, we first need a rigorous way to quantify how different their predictions are. The Kullback-Leibler (KL) divergence provides a powerful, information-theoretic measure of the dissimilarity between two probability distributions. This foundational exercise  will guide you through deriving the KL divergence for two Poisson models, a common scenario when modeling discrete events like mRNA transcript counts in single cells.",
            "id": "3924552",
            "problem": "A synthetic gene expression system is engineered with an optogenetic transcriptional activator that is controllably illuminated at intensity (design) $d>0$ during a fixed assay window. For a given $d$, a single-cell messenger ribonucleic acid (mRNA) count $Y_d$ is recorded. Two rival models, $\\mathcal{M}_0$ and $\\mathcal{M}_1$, posit that $Y_d$ follows a Poisson law with means $\\lambda_0(d)$ and $\\lambda_1(d)$, respectively, where $\\lambda_0(d)>0$ and $\\lambda_1(d)>0$ for all admissible $d$. Assume that the count under each model has probability mass function $p_j(y\\mid d)=\\exp(-\\lambda_j(d))\\,\\lambda_j(d)^y/y!$ for $y\\in\\{0,1,2,\\ldots\\}$ and $j\\in\\{0,1\\}$. Using only the foundational definition of Kullback–Leibler (KL) divergence and the Poisson probability mass function, derive a closed-form analytic expression for the Kullback–Leibler divergence $D_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right)$ from $\\mathcal{M}_0$ to $\\mathcal{M}_1$ as a function of $d$, expressed solely in terms of $\\lambda_0(d)$ and $\\lambda_1(d)$. Provide your final expression in closed form. No numerical evaluation is required, and no rounding is needed. The final answer must be a single analytic expression and must not include units.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique mathematical derivation. The scenario described is a standard application of information theory to model discrimination in quantitative biology.\n\nThe objective is to derive a closed-form expression for the Kullback–Leibler (KL) divergence from a model $\\mathcal{M}_0$ to a model $\\mathcal{M}_1$, denoted as $D_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right)$. The models describe the probability distribution of a discrete random variable $Y_d$, the mRNA count, which can take values in the set of non-negative integers $y \\in \\{0, 1, 2, \\ldots\\}$.\n\nThe foundational definition of the Kullback–Leibler divergence for two discrete probability mass functions (PMFs), $p_0(y)$ and $p_1(y)$, defined over the same sample space $\\mathcal{Y}$, is given by:\n$$\nD_{KL}(P_0 \\,\\|\\, P_1) = \\sum_{y \\in \\mathcal{Y}} p_0(y) \\ln\\left(\\frac{p_0(y)}{p_1(y)}\\right)\n$$\nIn this expression, $\\ln(\\cdot)$ denotes the natural logarithm. The summation is taken over all possible outcomes $y$.\n\nFor the given problem, the two distributions are $P_0(\\cdot\\mid d)$ and $P_1(\\cdot\\mid d)$, corresponding to models $\\mathcal{M}_0$ and $\\mathcal{M}_1$. Both are Poisson distributions with respective PMFs:\n$$\np_0(y \\mid d) = \\frac{\\exp(-\\lambda_0(d))\\,\\lambda_0(d)^y}{y!}\n$$\n$$\np_1(y \\mid d) = \\frac{\\exp(-\\lambda_1(d))\\,\\lambda_1(d)^y}{y!}\n$$\nThe sample space is $\\mathcal{Y}=\\{0, 1, 2, \\ldots\\}$. Applying the KL divergence definition, we have:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right)\n$$\nFirst, we analyze the term inside the logarithm, which is the ratio of the two PMFs:\n$$\n\\frac{p_0(y \\mid d)}{p_1(y \\mid d)} = \\frac{\\frac{\\exp(-\\lambda_0(d))\\,\\lambda_0(d)^y}{y!}}{\\frac{\\exp(-\\lambda_1(d))\\,\\lambda_1(d)^y}{y!}}\n$$\nThe factorial terms $y!$ cancel out. We can rearrange the remaining terms:\n$$\n\\frac{p_0(y \\mid d)}{p_1(y \\mid d)} = \\frac{\\exp(-\\lambda_0(d))}{\\exp(-\\lambda_1(d))} \\cdot \\frac{\\lambda_0(d)^y}{\\lambda_1(d)^y} = \\exp(\\lambda_1(d) - \\lambda_0(d)) \\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)^y\n$$\nNext, we take the natural logarithm of this ratio. Using the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(a^b) = b\\ln(a)$:\n$$\n\\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right) = \\ln\\left(\\exp(\\lambda_1(d) - \\lambda_0(d))\\right) + \\ln\\left(\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)^y\\right)\n$$\n$$\n\\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right) = (\\lambda_1(d) - \\lambda_0(d)) + y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nThe condition that $\\lambda_0(d) > 0$ and $\\lambda_1(d) > 0$ ensures that the argument of the logarithm is well-defined and positive.\n\nNow, we substitute this expression back into the summation for the KL divergence:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\left[ (\\lambda_1(d) - \\lambda_0(d)) + y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) \\right]\n$$\nWe can distribute the $p_0(y \\mid d)$ term and use the linearity of the summation to split the expression into two parts:\n$$\nD_{KL} = \\sum_{y=0}^{\\infty} p_0(y \\mid d) (\\lambda_1(d) - \\lambda_0(d)) + \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\cdot y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nLet's evaluate each summation separately. For the first term, the factor $(\\lambda_1(d) - \\lambda_0(d))$ is a constant with respect to the summation index $y$:\n$$\n\\sum_{y=0}^{\\infty} p_0(y \\mid d) (\\lambda_1(d) - \\lambda_0(d)) = (\\lambda_1(d) - \\lambda_0(d)) \\sum_{y=0}^{\\infty} p_0(y \\mid d)\n$$\nThe sum of a PMF over its entire sample space is, by definition, equal to $1$. Thus, $\\sum_{y=0}^{\\infty} p_0(y \\mid d) = 1$. The first term simplifies to:\n$$\n\\lambda_1(d) - \\lambda_0(d)\n$$\nFor the second term, the factor $\\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)$ is also a constant with respect to $y$:\n$$\n\\sum_{y=0}^{\\infty} p_0(y \\mid d) \\cdot y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) = \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) \\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d)\n$$\nThe summation $\\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d)$ is the definition of the expected value, or mean, of the random variable $Y_d$ under the distribution $P_0(\\cdot \\mid d)$. For a Poisson distribution with parameter $\\lambda_0(d)$, the mean is precisely $\\lambda_0(d)$. Therefore:\n$$\n\\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d) = \\mathbb{E}_{Y_d \\sim P_0}[Y_d] = \\lambda_0(d)\n$$\nThe second term simplifies to:\n$$\n\\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nCombining the two simplified parts gives the final expression for the KL divergence:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = (\\lambda_1(d) - \\lambda_0(d)) + \\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\nReordering the terms for clarity yields the final closed-form analytic expression:\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) - \\lambda_0(d) + \\lambda_1(d)\n$$\nThis expression is a function of the design parameter $d$ through the model-predicted means $\\lambda_0(d)$ and $\\lambda_1(d)$, as required.",
            "answer": "$$\n\\boxed{\\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) - \\lambda_0(d) + \\lambda_1(d)}\n$$"
        },
        {
            "introduction": "An effective experiment should be conducted under conditions where rival models make maximally different predictions. However, this separation must be weighed against the inherent uncertainty from both measurement noise and unknown model parameters. This practice problem  introduces a sensitivity-based discrimination score that formalizes this trade-off, allowing you to select an optimal inducer concentration that best separates two models in a realistic synthetic biology context.",
            "id": "3924566",
            "problem": "A synthetic gene circuit expresses a fluorescent reporter upon induction by a small molecule at concentration $u$ (in nanomolar). Two rival steady-state models, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$, describe the reporter intensity $y$ (in arbitrary units (a.u.)) at a fixed sampling time after induction. Each model $\\mathcal{M}_{i}$ has a single uncertain gain parameter $\\alpha_{i}$ and a known nominal mean prediction $\\mu_{i}(u)$ at $u \\in \\{20, 50, 100\\}$ nanomolar, together with the local sensitivity $s_{i}(u) = \\partial y_{i}/\\partial \\alpha_{i}$ at the same $u$. The parameter priors are Gaussian with variances $\\operatorname{Var}(\\alpha_{1}) = 2500$ a.u.$^{2}$ and $\\operatorname{Var}(\\alpha_{2}) = 3600$ a.u.$^{2}$. The measurement noise is zero-mean Gaussian with variance $\\sigma^{2} = 900$ a.u.$^{2}$. All sources of uncertainty are independent across models and measurement.\n\nThe nominal means and sensitivities at the candidate designs are:\n- At $u = 20$ nanomolar: $\\mu_{1}(20) = 300$ a.u., $\\mu_{2}(20) = 260$ a.u., $s_{1}(20) = 0.30$, $s_{2}(20) = 0.28$.\n- At $u = 50$ nanomolar: $\\mu_{1}(50) = 700$ a.u., $\\mu_{2}(50) = 550$ a.u., $s_{1}(50) = 0.70$, $s_{2}(50) = 0.61$.\n- At $u = 100$ nanomolar: $\\mu_{1}(100) = 900$ a.u., $\\mu_{2}(100) = 800$ a.u., $s_{1}(100) = 0.90$, $s_{2}(100) = 0.82$.\n\nUsing first-order (linear) uncertainty propagation for $y_{i}$ about the nominal $\\alpha_{i}$, model each $y_{i}(u)$ as Gaussian with mean $\\mu_{i}(u)$ and variance determined by the local sensitivity and parameter variance, augmented by the measurement variance. From these foundations and the properties of independent Gaussian variables, construct a scalar sensitivity-based discrimination score that standardizes the squared separation of the model means by the uncertainty of their difference, and compute this score at each candidate $u \\in \\{20, 50, 100\\}$ nanomolar. Select the inducer concentration $u$ (in nanomolar) that maximizes this score. Report the selected $u$ in nanomolar. No rounding is required beyond exact arithmetic with the provided values.",
            "solution": "The problem requires the selection of an optimal experimental design, specifically the inducer concentration $u$, that maximizes the ability to discriminate between two rival models, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$. The criterion for discrimination is a score that relates the difference in model predictions to the total uncertainty in those predictions. The analysis proceeds in three steps: first, quantifying the total uncertainty for each model's prediction; second, formulating the discrimination score; and third, calculating this score for each candidate design to identify the maximum.\n\nThe total uncertainty in the predicted reporter intensity, $y_{i}(u)$, for model $\\mathcal{M}_{i}$ at inducer concentration $u$ arises from two independent sources: the uncertainty in the model parameter $\\alpha_{i}$ and the measurement noise.\n\nFirst, we propagate the uncertainty from the parameter $\\alpha_{i}$ to the output $y_{i}$ using a first-order Taylor series approximation around the nominal parameter value. The model output is approximated as:\n$$y_{i} \\approx \\mu_{i}(u) + s_{i}(u) (\\alpha_{i} - \\bar{\\alpha}_{i})$$\nwhere $\\mu_{i}(u)$ is the nominal mean prediction, $s_{i}(u) = \\partial y_{i}/\\partial \\alpha_{i}$ is the local sensitivity, and $\\bar{\\alpha}_{i}$ is the nominal value of the parameter $\\alpha_{i}$. The variance of $y_{i}$ due to the parameter uncertainty, $\\operatorname{Var}(\\alpha_{i})$, is then given by:\n$$\\operatorname{Var}(y_{i})_{\\text{param}} = \\operatorname{Var}\\left(s_{i}(u) (\\alpha_{i} - \\bar{\\alpha}_{i})\\right) = s_{i}(u)^{2} \\operatorname{Var}(\\alpha_{i})$$\n\nSecond, the total variance of a measurement under model $\\mathcal{M}_{i}$, denoted $\\sigma_{i}^{2}(u)$, is the sum of the propagated parameter variance and the measurement noise variance, $\\sigma^{2}$, because these sources are stated to be independent.\n$$\\sigma_{i}^{2}(u) = \\operatorname{Var}(y_{i})_{\\text{param}} + \\sigma^{2} = s_{i}(u)^{2} \\operatorname{Var}(\\alpha_{i}) + \\sigma^{2}$$\nThus, the prediction of each model at a given $u$ is treated as a Gaussian random variable, $y_{i}(u) \\sim \\mathcal{N}(\\mu_{i}(u), \\sigma_{i}^{2}(u))$.\n\nThe problem specifies a discrimination score, let us call it $J(u)$, that \"standardizes the squared separation of the model means by the uncertainty of their difference.\" The squared separation of the means is $(\\mu_{1}(u) - \\mu_{2}(u))^{2}$. The difference in model predictions is the random variable $d(u) = y_{1}(u) - y_{2}(u)$. Since all sources of uncertainty are independent across models, the variables $y_{1}(u)$ and $y_{2}(u)$ are independent. Therefore, the variance of their difference is the sum of their individual variances:\n$$\\operatorname{Var}(d(u)) = \\operatorname{Var}(y_{1}(u) - y_{2}(u)) = \\operatorname{Var}(y_{1}(u)) + \\operatorname{Var}(y_{2}(u)) = \\sigma_{1}^{2}(u) + \\sigma_{2}^{2}(u)$$\nThe discrimination score is then:\n$$J(u) = \\frac{(\\mu_{1}(u) - \\mu_{2}(u))^{2}}{\\sigma_{1}^{2}(u) + \\sigma_{2}^{2}(u)} = \\frac{(\\mu_{1}(u) - \\mu_{2}(u))^{2}}{s_{1}(u)^{2} \\operatorname{Var}(\\alpha_{1}) + s_{2}(u)^{2} \\operatorname{Var}(\\alpha_{2}) + 2\\sigma^{2}}$$\nWe are given the following constants:\n$\\operatorname{Var}(\\alpha_{1}) = 2500$ a.u.$^{2}$\n$\\operatorname{Var}(\\alpha_{2}) = 3600$ a.u.$^{2}$\n$\\sigma^{2} = 900$ a.u.$^{2}$\n\nWe now compute $J(u)$ for each candidate concentration $u \\in \\{20, 50, 100\\}$.\n\nFor $u = 20$ nanomolar:\n$\\mu_{1}(20) = 300$, $\\mu_{2}(20) = 260$, $s_{1}(20) = 0.30$, $s_{2}(20) = 0.28$.\nThe numerator is $(\\mu_{1}(20) - \\mu_{2}(20))^{2} = (300 - 260)^{2} = 40^{2} = 1600$.\nThe denominator is $s_{1}(20)^{2} \\operatorname{Var}(\\alpha_{1}) + s_{2}(20)^{2} \\operatorname{Var}(\\alpha_{2}) + 2\\sigma^{2}$:\n$$(0.30)^{2}(2500) + (0.28)^{2}(3600) + 2(900)$$\n$$= (0.09)(2500) + (0.0784)(3600) + 1800$$\n$$= 225 + 282.24 + 1800 = 2307.24$$\nSo, $J(20) = \\frac{1600}{2307.24} \\approx 0.6935$.\n\nFor $u = 50$ nanomolar:\n$\\mu_{1}(50) = 700$, $\\mu_{2}(50) = 550$, $s_{1}(50) = 0.70$, $s_{2}(50) = 0.61$.\nThe numerator is $(\\mu_{1}(50) - \\mu_{2}(50))^{2} = (700 - 550)^{2} = 150^{2} = 22500$.\nThe denominator is $s_{1}(50)^{2} \\operatorname{Var}(\\alpha_{1}) + s_{2}(50)^{2} \\operatorname{Var}(\\alpha_{2}) + 2\\sigma^{2}$:\n$$(0.70)^{2}(2500) + (0.61)^{2}(3600) + 2(900)$$\n$$= (0.49)(2500) + (0.3721)(3600) + 1800$$\n$$= 1225 + 1339.56 + 1800 = 4364.56$$\nSo, $J(50) = \\frac{22500}{4364.56} \\approx 5.1552$.\n\nFor $u = 100$ nanomolar:\n$\\mu_{1}(100) = 900$, $\\mu_{2}(100) = 800$, $s_{1}(100) = 0.90$, $s_{2}(100) = 0.82$.\nThe numerator is $(\\mu_{1}(100) - \\mu_{2}(100))^{2} = (900 - 800)^{2} = 100^{2} = 10000$.\nThe denominator is $s_{1}(100)^{2} \\operatorname{Var}(\\alpha_{1}) + s_{2}(100)^{2} \\operatorname{Var}(\\alpha_{2}) + 2\\sigma^{2}$:\n$$(0.90)^{2}(2500) + (0.82)^{2}(3600) + 2(900)$$\n$$= (0.81)(2500) + (0.6724)(3600) + 1800$$\n$$= 2025 + 2420.64 + 1800 = 6245.64$$\nSo, $J(100) = \\frac{10000}{6245.64} \\approx 1.6011$.\n\nComparing the scores:\n$J(20) \\approx 0.6935$\n$J(50) \\approx 5.1552$\n$J(100) \\approx 1.6011$\nThe maximum score is obtained at $u = 50$ nanomolar. This concentration provides the greatest separation between the model predictions relative to their combined uncertainty, making it the optimal choice for an experiment aimed at discriminating between $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$.",
            "answer": "$$\\boxed{50}$$"
        },
        {
            "introduction": "The term 'optimal design' is meaningful only when the experimental objective is clearly defined. An experiment designed to precisely estimate a model's parameters (parameter estimation) may not be the best for distinguishing that model from an alternative (model discrimination). This insightful problem  presents a striking counterexample, demonstrating how a design that is D-optimal for parameter estimation can be the worst possible choice for model discrimination, underscoring the need to align design criteria with the scientific goal.",
            "id": "3924608",
            "problem": "Consider two candidate dynamical input-output models for a synthetic gene expression module measured at steady state under a single extracellular inducer concentration. The output signal is modeled with additive Gaussian noise. Let the experimental design consist of choosing a single inducer level $x \\in (0,\\infty)$, and acquiring one noisy measurement $y^{\\text{obs}}$ of the latent mean output $\\mu(x)$ with independent, identically distributed Gaussian noise of known variance $\\sigma^{2}$.\n\nThe two candidate models are Hill-type input-output maps with identical maximal activity $V$ and half-activation parameter $K$, but differing Hill coefficients. Model $\\mathcal{M}_{1}$ (Michaelis-Menten form, Hill coefficient $n=1$) is\n$$\n\\mu_{1}(x;V,K) \\;=\\; \\frac{V\\,x}{K + x},\n$$\nand model $\\mathcal{M}_{2}$ (cubic Hill form, Hill coefficient $n=3$) is\n$$\n\\mu_{2}(x;V,K) \\;=\\; \\frac{V\\,x^{3}}{K^{3} + x^{3}}.\n$$\n\nThe measurement model is $y^{\\text{obs}}(x) = \\mu_{j}(x;V,K) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ and $j \\in \\{1,2\\}$ denotes the model. Assume the nominal parameter values $V_{0} = 1$ and $K_{0} = 1$ are accurate, and the design is selected locally at these nominal values.\n\nDefinitions to use:\n- The Fisher Information Matrix (FIM) for a scalar parameter $\\theta$ under Gaussian noise is $I(\\theta;x) = \\frac{1}{\\sigma^{2}}\\left(\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}\\right)^{2}$. Determinant optimality (D-optimality) for a single parameter reduces to maximizing $I(\\theta;x)$ with respect to $x$.\n- The Kullback-Leibler (KL) divergence from a Gaussian model with mean $\\mu_{a}(x)$ to another with mean $\\mu_{b}(x)$ and identical variance $\\sigma^{2}$ is \n$$\nD_{KL}(a \\| b;x) \\;=\\; \\frac{1}{2\\,\\sigma^{2}}\\left(\\mu_{a}(x) - \\mu_{b}(x)\\right)^{2}.\n$$\n\nTask:\n1. Starting from these definitions, determine the locally determinant-optimal design $x^{\\text{D}}$ for estimating the scalar parameter $K$ in model $\\mathcal{M}_{1}$ at $(V_{0},K_{0})$ with a single observation.\n2. Using the same nominal values, evaluate the KL divergence $D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x)$ at $x^{\\text{D}}$, and argue whether this design maximizes or minimizes model distinguishability.\n3. Exhibit at least one explicit input $x \\in (0,\\infty)$ for which $D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x) > 0$ under $(V_{0},K_{0})$.\n4. Let $x^{\\text{KL}}$ denote any input that strictly increases the KL divergence relative to $x^{\\text{D}}$ under $(V_{0},K_{0})$. Compute the ratio\n$$\nR \\;=\\; \\frac{D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{D}})}{D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{KL}})}.\n$$\n\nProvide the final value of $R$ as your answer. No units are required. If you find a numerical value, do not round unless instructed; exact values are preferred.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The questions, while revealing a subtle point about experimental design, are formally specified and admit a unique, verifiable solution. We proceed with the solution by addressing each task in sequence.\n\nThe two candidate models are given by the mean output functions:\n$$\n\\mu_{1}(x;V,K) \\;=\\; \\frac{V\\,x}{K + x}\n$$\n$$\n\\mu_{2}(x;V,K) \\;=\\; \\frac{V\\,x^{3}}{K^{3} + x^{3}}\n$$\nThe nominal parameter values are $V_{0} = 1$ and $K_{0} = 1$. The measurement noise is Gaussian with mean $0$ and variance $\\sigma^2$.\n\nFirst, we address the determination of the locally determinant-optimal design $x^{\\text{D}}$ for estimating the parameter $K$ in model $\\mathcal{M}_{1}$. For a single parameter, D-optimality is equivalent to maximizing the Fisher Information. The Fisher Information for parameter $K$ is given by:\n$$\nI(K;x) = \\frac{1}{\\sigma^{2}}\\left(\\frac{\\partial \\mu_{1}(x;V,K)}{\\partial K}\\right)^{2}\n$$\nWe must first compute the partial derivative of $\\mu_{1}$ with respect to $K$:\n$$\n\\frac{\\partial \\mu_{1}}{\\partial K} = \\frac{\\partial}{\\partial K}\\left(\\frac{V\\,x}{K + x}\\right) = V\\,x \\cdot \\frac{\\partial}{\\partial K}\\left((K+x)^{-1}\\right) = V\\,x \\cdot (-1)(K+x)^{-2} = -\\frac{V\\,x}{(K+x)^{2}}\n$$\nSubstituting this into the expression for the Fisher Information gives:\n$$\nI(K;x) = \\frac{1}{\\sigma^{2}}\\left(-\\frac{V\\,x}{(K+x)^{2}}\\right)^{2} = \\frac{V^{2}x^{2}}{\\sigma^{2}(K+x)^{4}}\n$$\nTo find the locally optimal design, we evaluate this expression at the nominal parameter values $V_{0}=1$ and $K_{0}=1$:\n$$\nI(K;x)\\big|_{V=1,K=1} = \\frac{1^{2}x^{2}}{\\sigma^{2}(1+x)^{4}} = \\frac{1}{\\sigma^{2}}\\frac{x^{2}}{(1+x)^{4}}\n$$\nTo maximize $I(K;x)$ with respect to the input $x \\in (0, \\infty)$, we need to maximize the function $f(x) = \\frac{x^{2}}{(1+x)^{4}}$, since $\\frac{1}{\\sigma^2}$ is a positive constant. We find the critical points by setting the first derivative of $f(x)$ to zero:\n$$\nf'(x) = \\frac{d}{dx}\\left(\\frac{x^{2}}{(1+x)^{4}}\\right) = \\frac{2x(1+x)^{4} - x^{2} \\cdot 4(1+x)^{3}}{((1+x)^{4})^{2}} = \\frac{2x(1+x) - 4x^{2}}{(1+x)^{5}}\n$$\nSetting the numerator to zero for $x>0$:\n$$\n2x(1+x) - 4x^{2} = 0\n$$\n$$\n2x + 2x^{2} - 4x^{2} = 0\n$$\n$$\n2x - 2x^{2} = 0\n$$\n$$\n2x(1-x) = 0\n$$\nSince $x \\in (0,\\infty)$, the only critical point is $x=1$. To confirm this is a maximum, we can examine the sign of $f'(x)$. The denominator $(1+x)^{5}$ is positive for $x > 0$. The numerator $2x(1-x)$ is positive for $x \\in (0,1)$ and negative for $x \\in (1,\\infty)$. Thus, $f(x)$ increases for $x<1$ and decreases for $x>1$, confirming that $x=1$ is a local maximum. The locally D-optimal design is therefore $x^{\\text{D}} = 1$.\n\nSecond, we evaluate the Kullback-Leibler (KL) divergence $D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x)$ at this optimal design point $x^{\\text{D}}=1$. The KL divergence is defined as:\n$$\nD_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x) = \\frac{1}{2\\sigma^{2}}(\\mu_{1}(x) - \\mu_{2}(x))^{2}\n$$\nWe evaluate the model outputs at the nominal parameters $(V_{0}=1, K_{0}=1)$ and at the design point $x^{\\text{D}}=1$:\n$$\n\\mu_{1}(x=1; V=1, K=1) = \\frac{1 \\cdot 1}{1+1} = \\frac{1}{2}\n$$\n$$\n\\mu_{2}(x=1; V=1, K=1) = \\frac{1 \\cdot 1^{3}}{1^{3}+1^{3}} = \\frac{1}{1+1} = \\frac{1}{2}\n$$\nThe outputs of the two models are identical at $x=1$. Substituting these into the KL divergence formula:\n$$\nD_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2}; x^{\\text{D}}=1) = \\frac{1}{2\\sigma^{2}}\\left(\\frac{1}{2} - \\frac{1}{2}\\right)^{2} = \\frac{1}{2\\sigma^{2}}(0)^{2} = 0\n$$\nThe KL divergence is a measure of the information lost when one model is used to approximate another; it quantifies the distinguishability of two probabilistic models. A KL divergence of $0$ indicates that the models are indistinguishable at the given experimental condition. Therefore, the design $x^{\\text{D}}=1$, which is optimal for estimating parameter $K$ in model $\\mathcal{M}_1$, is simultaneously the worst possible design for discriminating between model $\\mathcal{M}_1$ and model $\\mathcal{M}_2$, as it completely minimizes their distinguishability.\n\nThird, we must exhibit an explicit input $x \\in (0,\\infty)$ for which $D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x) > 0$. The KL divergence is zero if and only if $\\mu_{1}(x) = \\mu_{2}(x)$. At the nominal parameters, this is:\n$$\n\\frac{x}{1+x} = \\frac{x^{3}}{1+x^{3}}\n$$\nFor $x>0$, we can multiply by $(1+x)(1+x^3)$ and divide by $x$:\n$$\n1+x^{3} = x^{2}(1+x) \\implies 1+x^{3} = x^{2}+x^{3} \\implies 1 = x^{2}\n$$\nSince $x \\in (0,\\infty)$, the only solution is $x=1$. Thus, for any $x \\in (0,\\infty)$ such that $x \\neq 1$, the model outputs will differ, and the KL divergence will be strictly positive. As an explicit example, we can choose $x=2$:\n$$\n\\mu_{1}(2) = \\frac{2}{1+2} = \\frac{2}{3}\n$$\n$$\n\\mu_{2}(2) = \\frac{2^{3}}{1+2^{3}} = \\frac{8}{9}\n$$\nSince $\\mu_{1}(2) \\neq \\mu_{2}(2)$, the KL divergence is positive: $D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};2) = \\frac{1}{2\\sigma^{2}}(\\frac{2}{3}-\\frac{8}{9})^2 > 0$.\n\nFourth, we compute the ratio $R = \\frac{D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{D}})}{D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{KL}})}$.\nWe have already found the numerator to be:\n$$\nD_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{D}}) = 0\n$$\nThe problem defines $x^{\\text{KL}}$ as any input that strictly increases the KL divergence relative to $x^{\\text{D}}$, which means:\n$$\nD_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{KL}}) > D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{D}})\n$$\nSubstituting the value of the numerator, this condition is:\n$$\nD_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{KL}}) > 0\n$$\nFrom the third part of our analysis, we know that such an $x^{\\text{KL}}$ exists; any $x \\in (0,\\infty)$ except $x=1$ satisfies this condition. For any such choice of $x^{\\text{KL}}$, the denominator of the ratio $R$ is a strictly positive number. The ratio is therefore:\n$$\nR = \\frac{0}{D_{KL}(\\mathcal{M}_{1} \\| \\mathcal{M}_{2};x^{\\text{KL}})} = 0\n$$\nThis result is independent of the specific choice of $x^{\\text{KL}}$, as long as it satisfies the given condition. The final value of $R$ is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}