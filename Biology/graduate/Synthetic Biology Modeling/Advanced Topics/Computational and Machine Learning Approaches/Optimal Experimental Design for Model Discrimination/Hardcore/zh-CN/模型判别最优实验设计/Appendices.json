{
    "hands_on_practices": [
        {
            "introduction": "理解模型之间差异的量化是实验设计的核心。本练习将通过一个在单细胞转录组学中常见的泊松计数模型，指导您从第一性原理出发，推导作为模型可辨识性基本度量的库尔贝克-莱布勒（KL）散度 ()。",
            "id": "3924552",
            "problem": "一个合成基因表达系统经过工程改造，配备了一个光遗传学转录激活因子。在固定的分析窗口期内，该激活因子以可控的强度（设计）$d>0$被照亮。对于给定的$d$，记录单细胞信使核糖核酸（mRNA）计数$Y_d$。两个相互竞争的模型$\\mathcal{M}_0$和$\\mathcal{M}_1$假设$Y_d$分别遵循均值为$\\lambda_0(d)$和$\\lambda_1(d)$的泊松分布，其中对于所有允许的$d$，都有$\\lambda_0(d)>0$和$\\lambda_1(d)>0$。假设在每个模型下，计数的概率质量函数为$p_j(y\\mid d)=\\exp(-\\lambda_j(d))\\,\\lambda_j(d)^y/y!$，其中$y\\in\\{0,1,2,\\ldots\\}$且$j\\in\\{0,1\\}$。仅使用Kullback–Leibler（KL）散度的基本定义和泊松概率质量函数，推导从$\\mathcal{M}_0$到$\\mathcal{M}_1$的Kullback–Leibler散度$D_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right)$的闭式解析表达式。该表达式应为$d$的函数，并仅用$\\lambda_0(d)$和$\\lambda_1(d)$表示。请以闭式形式提供您的最终表达式。无需进行数值计算，也无需四舍五入。最终答案必须是单一的解析表达式，且不得包含单位。",
            "solution": "该问题是有效的，因为它具有科学依据、提法明确、客观，并包含了进行唯一数学推导所需的所有必要信息。所描述的情景是信息论在定量生物学中用于模型判别的标准应用。\n\n目标是推导从模型$\\mathcal{M}_0$到模型$\\mathcal{M}_1$的Kullback–Leibler（KL）散度的闭式表达式，记为$D_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right)$。这些模型描述了离散随机变量$Y_d$（mRNA计数）的概率分布，该变量可以取非负整数集合$y \\in \\{0, 1, 2, \\ldots\\}$中的值。\n\n对于定义在相同样本空间$\\mathcal{Y}$上的两个离散概率质量函数（PMF）$p_0(y)$和$p_1(y)$，Kullback–Leibler散度的基本定义由下式给出：\n$$\nD_{KL}(P_0 \\,\\|\\, P_1) = \\sum_{y \\in \\mathcal{Y}} p_0(y) \\ln\\left(\\frac{p_0(y)}{p_1(y)}\\right)\n$$\n在此表达式中，$\\ln(\\cdot)$表示自然对数。求和是对所有可能的结果$y$进行的。\n\n对于给定的问题，两个分布是$P_0(\\cdot\\mid d)$和$P_1(\\cdot\\mid d)$，分别对应于模型$\\mathcal{M}_0$和$\\mathcal{M}_1$。两者都是泊松分布，其各自的PMF为：\n$$\np_0(y \\mid d) = \\frac{\\exp(-\\lambda_0(d))\\,\\lambda_0(d)^y}{y!}\n$$\n$$\np_1(y \\mid d) = \\frac{\\exp(-\\lambda_1(d))\\,\\lambda_1(d)^y}{y!}\n$$\n样本空间为$\\mathcal{Y}=\\{0, 1, 2, \\ldots\\}$。应用KL散度定义，我们得到：\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right)\n$$\n首先，我们分析对数内的项，即两个PMF的比率：\n$$\n\\frac{p_0(y \\mid d)}{p_1(y \\mid d)} = \\frac{\\frac{\\exp(-\\lambda_0(d))\\,\\lambda_0(d)^y}{y!}}{\\frac{\\exp(-\\lambda_1(d))\\,\\lambda_1(d)^y}{y!}}\n$$\n阶乘项$y!$被消掉。我们可以重新排列剩余的项：\n$$\n\\frac{p_0(y \\mid d)}{p_1(y \\mid d)} = \\frac{\\exp(-\\lambda_0(d))}{\\exp(-\\lambda_1(d))} \\cdot \\frac{\\lambda_0(d)^y}{\\lambda_1(d)^y} = \\exp(\\lambda_1(d) - \\lambda_0(d)) \\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)^y\n$$\n接下来，我们取该比率的自然对数。使用对数的性质$\\ln(ab) = \\ln(a) + \\ln(b)$和$\\ln(a^b) = b\\ln(a)$：\n$$\n\\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right) = \\ln\\left(\\exp(\\lambda_1(d) - \\lambda_0(d))\\right) + \\ln\\left(\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)^y\\right)\n$$\n$$\n\\ln\\left(\\frac{p_0(y \\mid d)}{p_1(y \\mid d)}\\right) = (\\lambda_1(d) - \\lambda_0(d)) + y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\n条件$\\lambda_0(d) > 0$和$\\lambda_1(d) > 0$确保了对数的参数是良定义且为正的。\n\n现在，我们将此表达式代回到KL散度的求和公式中：\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\left[ (\\lambda_1(d) - \\lambda_0(d)) + y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) \\right]\n$$\n我们可以分配$p_0(y \\mid d)$项，并利用求和的线性性质将表达式分成两部分：\n$$\nD_{KL} = \\sum_{y=0}^{\\infty} p_0(y \\mid d) (\\lambda_1(d) - \\lambda_0(d)) + \\sum_{y=0}^{\\infty} p_0(y \\mid d) \\cdot y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\n我们分别计算每个求和。对于第一项，因子$(\\lambda_1(d) - \\lambda_0(d))$相对于求和指数$y$是一个常数：\n$$\n\\sum_{y=0}^{\\infty} p_0(y \\mid d) (\\lambda_1(d) - \\lambda_0(d)) = (\\lambda_1(d) - \\lambda_0(d)) \\sum_{y=0}^{\\infty} p_0(y \\mid d)\n$$\n根据定义，一个PMF在其整个样本空间上的总和等于$1$。因此，$\\sum_{y=0}^{\\infty} p_0(y \\mid d) = 1$。第一项简化为：\n$$\n\\lambda_1(d) - \\lambda_0(d)\n$$\n对于第二项，因子$\\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)$相对于$y$也是一个常数：\n$$\n\\sum_{y=0}^{\\infty} p_0(y \\mid d) \\cdot y \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) = \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) \\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d)\n$$\n求和$\\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d)$是在分布$P_0(\\cdot \\mid d)$下随机变量$Y_d$的期望值（或均值）的定义。对于参数为$\\lambda_0(d)$的泊松分布，其均值恰好是$\\lambda_0(d)$。因此：\n$$\n\\sum_{y=0}^{\\infty} y \\cdot p_0(y \\mid d) = \\mathbb{E}_{Y_d \\sim P_0}[Y_d] = \\lambda_0(d)\n$$\n第二项简化为：\n$$\n\\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\n将两个简化部分合并，得到KL散度的最终表达式：\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = (\\lambda_1(d) - \\lambda_0(d)) + \\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right)\n$$\n为清晰起见，重新排列各项，得到最终的闭式解析表达式：\n$$\nD_{KL}\\!\\left(P_0(\\cdot\\mid d)\\,\\|\\,P_1(\\cdot\\mid d)\\right) = \\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) - \\lambda_0(d) + \\lambda_1(d)\n$$\n此表达式通过模型预测的均值$\\lambda_0(d)$和$\\lambda_1(d)$成为设计参数$d$的函数，符合要求。",
            "answer": "$$\n\\boxed{\\lambda_0(d) \\ln\\left(\\frac{\\lambda_0(d)}{\\lambda_1(d)}\\right) - \\lambda_0(d) + \\lambda_1(d)}\n$$"
        },
        {
            "introduction": "实验设计的目标并非一成不变，选择正确的目标至关重要。这个发人深省的练习将揭示一个关键概念：一个为精确参数估计而优化的实验（D-最优性），可能恰恰是用于区分两个竞争模型的最差设计 ()。",
            "id": "3924608",
            "problem": "考虑用于一个合成基因表达模块的两个候选动态输入输出模型，该模块在单一细胞外诱导物浓度下于稳态进行测量。输出信号采用加性高斯噪声建模。设实验设计包括选择一个诱导物水平 $x \\in (0,\\infty)$，并对潜在平均输出 $\\mu(x)$ 进行一次带噪声的测量，得到 $y^{\\mathrm{obs}}$。测量噪声为独立同分布的高斯噪声，其方差 $\\sigma^{2}$ 已知。\n\n这两个候选模型是 Hill 型输入输出映射，具有相同的最大活性 $V$ 和半活化参数 $K$，但 Hill 系数不同。模型 $M_{1}$（Michaelis-Menten 形式，Hill 系数 $n=1$）为\n$$\n\\mu_{1}(x;V,K) \\;=\\; \\frac{V\\,x}{K + x},\n$$\n以及模型 $M_{2}$（三次 Hill 形式，Hill 系数 $n=3$）为\n$$\n\\mu_{2}(x;V,K) \\;=\\; \\frac{V\\,x^{3}}{K^{3} + x^{3}}.\n$$\n\n测量模型为 $y^{\\mathrm{obs}}(x) = \\mu_{j}(x;V,K) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ 且 $j \\in \\{1,2\\}$ 表示所选模型。假设标称参数值 $V_{0} = 1$ 和 $K_{0} = 1$ 是准确的，并且实验设计是基于这些标称值进行局部选择的。\n\n使用的定义：\n- 在高斯噪声下，对于标量参数 $\\theta$ 的 Fisher 信息矩阵 (FIM) 为 $I(\\theta;x) = \\frac{1}{\\sigma^{2}}\\left(\\frac{\\partial \\mu(x;\\theta)}{\\partial \\theta}\\right)^{2}$。对于单个参数，行列式最优性（D-最优性）简化为关于 $x$ 最大化 $I(\\theta;x)$。\n- 从一个均值为 $\\mu_{a}(x)$ 的高斯模型到另一个均值为 $\\mu_{b}(x)$ 且方差同为 $\\sigma^{2}$ 的高斯模型的 Kullback-Leibler (KL) 散度为\n$$\nD_{\\mathrm{KL}}(a\\parallel b;x) \\;=\\; \\frac{1}{2\\,\\sigma^{2}}\\left(\\mu_{a}(x) - \\mu_{b}(x)\\right)^{2}.\n$$\n\n任务：\n1. 从这些定义出发，确定在参数值为 $(V_{0},K_{0})$ 时，通过单次观测来估计模型 $M_{1}$ 中标量参数 $K$ 的局部行列式最优设计 $x^{\\mathrm{D}}$。\n2. 使用相同的标称值，在 $x^{\\mathrm{D}}$ 处评估 KL 散度 $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x)$，并论证该设计是最大化还是最小化了模型的可区分性。\n3. 在 $(V_{0},K_{0})$ 条件下，给出一个明确的输入 $x \\in (0,\\infty)$，使得 $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x) > 0$。\n4. 在 $(V_{0},K_{0})$ 条件下，令 $x^{\\mathrm{KL}}$ 表示任何一个使得 KL 散度相对于在 $x^{\\mathrm{D}}$ 处的值严格增加的输入。计算比率\n$$\nR \\;=\\; \\frac{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}})}{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}})}.\n$$\n\n请提供 $R$ 的最终值作为答案。无需单位。如果得到一个数值，除非有指示，否则不要四舍五入；精确值是首选。",
            "solution": "该问题经验证是自洽的、有科学依据且定义明确的。这些问题虽然揭示了实验设计中的一个微妙之处，但其形式上是明确的，并允许一个唯一且可验证的解。我们按顺序解决每个任务来给出解答。\n\n这两个候选模型由以下平均输出函数给出：\n$$\n\\mu_{1}(x;V,K) \\;=\\; \\frac{V\\,x}{K + x}\n$$\n$$\n\\mu_{2}(x;V,K) \\;=\\; \\frac{V\\,x^{3}}{K^{3} + x^{3}}\n$$\n标称参数值为 $V_{0} = 1$ 和 $K_{0} = 1$。测量噪声是均值为 0、方差为 $\\sigma^2$ 的高斯噪声。\n\n首先，我们解决在模型 $M_{1}$ 中估计参数 $K$ 的局部行列式最优设计 $x^{\\mathrm{D}}$ 的确定问题。对于单个参数，D-最优性等价于最大化 Fisher 信息。参数 $K$ 的 Fisher 信息由下式给出：\n$$\nI(K;x) = \\frac{1}{\\sigma^{2}}\\left(\\frac{\\partial \\mu_{1}(x;V,K)}{\\partial K}\\right)^{2}\n$$\n我们必须首先计算 $\\mu_{1}$ 关于 $K$ 的偏导数：\n$$\n\\frac{\\partial \\mu_{1}}{\\partial K} = \\frac{\\partial}{\\partial K}\\left(\\frac{V\\,x}{K + x}\\right) = V\\,x \\cdot \\frac{\\partial}{\\partial K}\\left((K+x)^{-1}\\right) = V\\,x \\cdot (-1)(K+x)^{-2} = -\\frac{V\\,x}{(K+x)^{2}}\n$$\n将此代入 Fisher 信息的表达式中，得到：\n$$\nI(K;x) = \\frac{1}{\\sigma^{2}}\\left(-\\frac{V\\,x}{(K+x)^{2}}\\right)^{2} = \\frac{V^{2}x^{2}}{\\sigma^{2}(K+x)^{4}}\n$$\n为了找到局部最优设计，我们在标称参数值 $V_{0}=1$ 和 $K_{0}=1$ 处评估该表达式：\n$$\nI(K;x)\\big|_{V=1,K=1} = \\frac{1^{2}x^{2}}{\\sigma^{2}(1+x)^{4}} = \\frac{1}{\\sigma^{2}}\\frac{x^{2}}{(1+x)^{4}}\n$$\n为了关于输入 $x \\in (0, \\infty)$ 最大化 $I(K;x)$，我们需要最大化函数 $f(x) = \\frac{x^{2}}{(1+x)^{4}}$，因为 $\\frac{1}{\\sigma^2}$ 是一个正常数。我们通过将 $f(x)$ 的一阶导数设为零来找到临界点：\n$$\nf'(x) = \\frac{d}{dx}\\left(\\frac{x^{2}}{(1+x)^{4}}\\right) = \\frac{2x(1+x)^{4} - x^{2} \\cdot 4(1+x)^{3}}{((1+x)^{4})^{2}} = \\frac{2x(1+x) - 4x^{2}}{(1+x)^{5}}\n$$\n对于 $x>0$，将分子设为零：\n$$\n2x(1+x) - 4x^{2} = 0\n$$\n$$\n2x + 2x^{2} - 4x^{2} = 0\n$$\n$$\n2x - 2x^{2} = 0\n$$\n$$\n2x(1-x) = 0\n$$\n由于 $x \\in (0,\\infty)$，唯一的临界点是 $x=1$。为了确认这是一个最大值点，我们可以检验 $f'(x)$ 的符号。分母 $(1+x)^{5}$ 对于 $x>0$ 是正的。分子 $2x(1-x)$ 在 $x \\in (0,1)$ 时为正，在 $x \\in (1,\\infty)$ 时为负。因此，$f(x)$ 在 $x<1$ 时递增，在 $x>1$ 时递减，这证实了 $x=1$ 是一个局部最大值点。因此，局部 D-最优设计是 $x^{\\mathrm{D}} = 1$。\n\n其次，我们在这个最优设计点 $x^{\\mathrm{D}}=1$ 处评估 Kullback-Leibler (KL) 散度 $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x)$。KL 散度定义为：\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x) = \\frac{1}{2\\sigma^{2}}(\\mu_{1}(x) - \\mu_{2}(x))^{2}\n$$\n我们在标称参数 $(V_{0}=1, K_{0}=1)$ 和设计点 $x^{\\mathrm{D}}=1$ 处评估模型输出：\n$$\n\\mu_{1}(x=1; V=1, K=1) = \\frac{1 \\cdot 1}{1+1} = \\frac{1}{2}\n$$\n$$\n\\mu_{2}(x=1; V=1, K=1) = \\frac{1 \\cdot 1^{3}}{1^{3}+1^{3}} = \\frac{1}{1+1} = \\frac{1}{2}\n$$\n在 $x=1$ 处，两个模型的输出是相同的。将这些值代入 KL 散度公式：\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2}; x^{\\mathrm{D}}=1) = \\frac{1}{2\\sigma^{2}}\\left(\\frac{1}{2} - \\frac{1}{2}\\right)^{2} = \\frac{1}{2\\sigma^{2}}(0)^{2} = 0\n$$\nKL 散度是衡量用一个模型近似另一个模型时信息损失的度量；它量化了两个概率模型的可区分性。KL 散度为 0 表示在给定的实验条件下，这两个模型是不可区分的。因此，设计 $x^{\\mathrm{D}}=1$ 虽然对于估计模型 $M_1$ 中的参数 $K$ 是最优的，但同时它也是用于区分模型 $M_1$ 和模型 $M_2$ 的最差设计，因为它完全最小化了它们的可区分性。\n\n第三，我们必须给出一个明确的输入 $x \\in (0,\\infty)$，使得 $D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x) > 0$。KL 散度为零当且仅当 $\\mu_{1}(x) = \\mu_{2}(x)$。在标称参数下，即为：\n$$\n\\frac{x}{1+x} = \\frac{x^{3}}{1+x^{3}}\n$$\n对于 $x>0$，我们可以两边乘以 $(1+x)(1+x^3)$ 并除以 $x$：\n$$\n1+x^{3} = x^{2}(1+x) \\implies 1+x^{3} = x^{2}+x^{3} \\implies 1 = x^{2}\n$$\n由于 $x \\in (0,\\infty)$，唯一的解是 $x=1$。因此，对于任何 $x \\in (0,\\infty)$ 且 $x \\neq 1$，模型输出将不同，KL 散度将严格为正。作为一个明确的例子，我们可以选择 $x=2$：\n$$\n\\mu_{1}(2) = \\frac{2}{1+2} = \\frac{2}{3}\n$$\n$$\n\\mu_{2}(2) = \\frac{2^{3}}{1+2^{3}} = \\frac{8}{9}\n$$\n由于 $\\mu_{1}(2) \\neq \\mu_{2}(2)$，KL 散度为正：$D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};2) = \\frac{1}{2\\sigma^{2}}(\\frac{2}{3}-\\frac{8}{9})^2 > 0$。\n\n第四，我们计算比率 $R = \\frac{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}})}{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}})}$。\n我们已经发现分子为：\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}}) = 0\n$$\n问题将 $x^{\\mathrm{KL}}$ 定义为任何使得 KL 散度相对于在 $x^{\\mathrm{D}}$ 处的值严格增加的输入，这意味着：\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}}) > D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{D}})\n$$\n代入分子的值，该条件为：\n$$\nD_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}}) > 0\n$$\n从我们分析的第三部分可知，这样的 $x^{\\mathrm{KL}}$ 是存在的；任何 $x \\in (0,\\infty)$ 且 $x \\neq 1$ 都满足此条件。对于任何这样的 $x^{\\mathrm{KL}}$ 选择，比率 $R$ 的分母都是一个严格为正的数。因此，该比率为：\n$$\nR = \\frac{0}{D_{\\mathrm{KL}}(M_{1}\\parallel M_{2};x^{\\mathrm{KL}})} = 0\n$$\n这个结果与 $x^{\\mathrm{KL}}$ 的具体选择无关，只要它满足给定的条件。$R$ 的最终值为 $0$。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "本章的最终练习将理论与计算在一个真实的合成生物学场景中融会贯通。您将实现一个算法，使用基于KL散度的效用函数，找到最佳的CRISPRi靶向基因，从而最大程度地区分负反馈和前馈两种网络拓扑结构 ()。",
            "id": "3924592",
            "problem": "设计并实现一个程序，用于计算最优的单基因 Clustered Regularly Interspaced Short Palindromic Repeats interference (CRISPRi) 靶点，以在合成生物学中区分两种可选的基因调控网络模型：负反馈拓扑与前馈拓扑。决策论准则是最大化在数据生成过程中贝叶斯因子的期望对数。\n\n其基础由以下广泛接受的定义和模型构成：\n- 贝叶斯模型选择将两个模型之间的贝叶斯因子定义为似然比。对于一个结果向量 $\\mathbf{y}$，比较模型 $\\mathcal{M}_1$ 和模型 $\\mathcal{M}_2$ 的贝叶斯因子是 $B_{12}(\\mathbf{y}) = \\dfrac{p(\\mathbf{y}\\mid \\mathcal{M}_1)}{p(\\mathbf{y}\\mid \\mathcal{M}_2)}$，在指定的数据生成分布下，期望对数贝叶斯因子为 $\\mathbb{E}[\\log B_{12}(\\mathbf{y})]$。\n- 测量模型是加性独立高斯噪声：在测量时间 $t_k$，观测值为 $y_k = h^\\top \\mathbf{x}(t_k) + \\varepsilon_k$，其中 $h = [0, 0, 1]^\\top$，$\\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$ 在 $k$ 上独立分布，$\\mathbf{x}(t)$ 是基因表达偏离稳态的状态向量（无量纲）。\n- 每个模型的基因表达动力学由一个线性时不变常微分方程（ODE）近似，形式为 $\\dfrac{d\\mathbf{x}}{dt} = A\\,\\mathbf{x} + \\mathbf{d}$，其中 $A \\in \\mathbb{R}^{3\\times 3}$ 是一个稳定系统矩阵，$\\mathbf{d} \\in \\mathbb{R}^3$ 是一个常数输入，表示在 $t \\ge 0$ 时施加的 CRISPRi 基因敲降。初始条件是 $\\mathbf{x}(0) = \\mathbf{0}$（从稳态开始）。该常微分方程在常数输入下的精确解是经过充分检验的公式\n$$\n\\mathbf{x}(t) \\;=\\; \\int_0^t e^{A (t-\\tau)} \\,\\mathbf{d}\\, d\\tau \\;=\\; A^{-1}\\big(e^{A t} - I\\big)\\,\\mathbf{d},\n$$\n其中 $I$ 是单位矩阵，$e^{A t}$ 是矩阵指数。\n- 设计变量是 CRISPRi 靶点指数 $i \\in \\{0,1,2\\}$，对应于基因指数 $\\{A,B,C\\}$，其中常数输入为 $\\mathbf{d} = -\\alpha\\, \\mathbf{e}_i$，$\\alpha > 0$，$\\mathbf{e}_i$ 是标准基向量。\n\n需要区分的两个模型是：\n- $\\mathcal{M}_\\mathrm{FB}$（负反馈拓扑）：矩阵 $A_\\mathrm{FB}$ 编码了 $C \\to A$ 抑制、$A \\to B$ 激活和 $B \\to C$ 激活。\n- $\\mathcal{M}_\\mathrm{FF}$（前馈拓扑）：矩阵 $A_\\mathrm{FF}$ 编码了 $A \\to B$ 激活、$A \\to C$ 激活和 $B \\to C$ 激活，没有从 $C$ 到 $A$ 的反馈。\n\n对于给定的设计（靶点指数 $i$）、测量时间 $\\{t_k\\}_{k=1}^K$、噪声标准差 $\\sigma$、输入振幅 $\\alpha$ 以及一对模型矩阵 $(A_\\mathrm{FB}, A_\\mathrm{FF})$，程序必须：\n1. 计算每个模型下的平均预测输出 $\\boldsymbol{\\mu}_\\mathrm{FB} \\in \\mathbb{R}^K$ 和 $\\boldsymbol{\\mu}_\\mathrm{FF} \\in \\mathbb{R}^K$，其中第 $k$ 个分量是 $\\mu(t_k) = h^\\top \\mathbf{x}(t_k)$，使用相应的 $A$ 矩阵。\n2. 使用高斯观测模型，在对称设计效用下计算用于区分 $\\mathcal{M}_\\mathrm{FB}$ 和 $\\mathcal{M}_\\mathrm{FF}$ 的贝叶斯因子的期望对数。该效用函数是在两个模型作为数据生成过程且先验概率相等的情况下取平均。在推导或实现中，不要假设任何快捷公式；从上面给出的定义开始。\n3. 选择能够最大化此期望对数贝叶斯因子的靶点指数 $i^\\star \\in \\{0,1,2\\}$。如果在绝对容差 $\\epsilon = 10^{-9}$ 内有多个指数达到相同的最大值，则返回其中最小的指数。\n\n所有变量都是无量纲的，没有物理单位。不涉及角度。概率、似然和贝叶斯因子根据定义也是无量纲的。\n\n测试套件。您的程序必须为以下每个参数集计算最优靶点指数。对于每种情况，将结果列为 $\\{0,1,2\\}$ 中的一个整数，其中 $0$ 表示靶向 $A$，$1$ 表示靶向 $B$，$2$ 表示靶向 $C$。\n\n- 情况 1（基线，中等时间）：\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.2  & 0.0 & -0.5 \\\\ 0.7 & -1.1 & 0.0 \\\\ 0.0 & 0.6 & -1.0 \\end{bmatrix}$，\n    $A_\\mathrm{FF} = \\begin{bmatrix} -1.2 & 0.0 & 0.0 \\\\ 0.8 & -1.1 & 0.0 \\\\ 0.5 & 0.4 & -1.0 \\end{bmatrix}$。\n  - 时间 $\\{0.5, 1.0, 2.0, 3.0\\}$。\n  - 噪声标准差 $\\sigma = 0.1$。\n  - 输入振幅 $\\alpha = 0.5$。\n- 情况 2（边界情况，瞬时测量）：\n  - 与情况 1 相同的 $A_\\mathrm{FB}$ 和 $A_\\mathrm{FF}$。\n  - 时间 $\\{0.0\\}$。\n  - 噪声标准差 $\\sigma = 0.1$。\n  - 输入振幅 $\\alpha = 0.5$。\n- 情况 3（早期动力学，低噪声）：\n  - 与情况 1 相同的 $A_\\mathrm{FB}$ 和 $A_\\mathrm{FF}$。\n  - 时间 $\\{0.1, 0.2, 0.3, 0.4\\}$。\n  - 噪声标准差 $\\sigma = 0.05$。\n  - 输入振幅 $\\alpha = 0.5$。\n- 情况 4（替代拓扑强度）：\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.3 & 0.0 & -0.05 \\\\ 0.6 & -1.2 & 0.0 \\\\ 0.0 & 0.5 & -1.1 \\end{bmatrix}$，\n    $A_\\mathrm{FF} = \\begin{bmatrix} -1.3 & 0.0 & 0.0 \\\\ 0.6 & -1.2 & 0.0 \\\\ 1.0 & 0.5 & -1.1 \\end{bmatrix}$。\n  - 时间 $\\{1.0, 2.0, 3.0\\}$。\n  - 噪声标准差 $\\sigma = 0.2$。\n  - 输入振幅 $\\alpha = 0.5$。\n\n最终输出格式。您的程序应生成单行输出，其中包含四个情况的结果，格式为方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_j$ 是为情况 $j$ 选择的整数靶点指数。",
            "solution": "### 问题验证\n\n#### 步骤 1：提取已知信息\n\n问题提供了以下定义、模型和数据：\n- **模型**：两种可选的基因调控网络模型，$\\mathcal{M}_\\mathrm{FB}$（负反馈）和 $\\mathcal{M}_\\mathrm{FF}$（前馈）。\n- **动力学**：线性时不变常微分方程 $\\dfrac{d\\mathbf{x}}{dt} = A\\,\\mathbf{x} + \\mathbf{d}$，其中 $\\mathbf{x} \\in \\mathbb{R}^3$，$A \\in \\mathbb{R}^{3\\times 3}$ 是一个稳定矩阵，$\\mathbf{d} \\in \\mathbb{R}^3$ 是一个常数输入。\n- **初始条件**：$\\mathbf{x}(0) = \\mathbf{0}$。\n- **ODE 解**：$\\mathbf{x}(t) = A^{-1}\\big(e^{A t} - I\\big)\\,\\mathbf{d}$。\n- **测量模型**：$y_k = h^\\top \\mathbf{x}(t_k) + \\varepsilon_k$，观测向量为 $h = [0, 0, 1]^\\top$。噪声是独立同分布的，$\\varepsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$。\n- **设计变量**：CRISPRi 靶点指数 $i \\in \\{0,1,2\\}$，对应基因 $\\{A,B,C\\}$。\n- **输入向量**：基因敲降建模为 $\\mathbf{d} = -\\alpha\\, \\mathbf{e}_i$，其中 $\\alpha > 0$，$\\mathbf{e}_i$ 是标准基向量。\n- **贝叶斯模型选择**：贝叶斯因子为 $B_{12}(\\mathbf{y}) = \\dfrac{p(\\mathbf{y}\\mid \\mathcal{M}_1)}{p(\\mathbf{y}\\mid \\mathcal{M}_2)}$。\n- **目标函数**：最大化期望对数贝叶斯因子 $\\mathbb{E}[\\log B_{12}(\\mathbf{y})]$，在两个模型作为数据源的先验概率相等的对称效用下。\n- **平局打破规则**：如果多个指数在绝对容差 $\\epsilon = 10^{-9}$ 内达到最大效用，则返回其中最小的指数。\n- **情况 1 参数**：\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.2  & 0.0 & -0.5 \\\\ 0.7 & -1.1 & 0.0 \\\\ 0.0 & 0.6 & -1.0 \\end{bmatrix}$，$A_\\mathrm{FF} = \\begin{bmatrix} -1.2 & 0.0 & 0.0 \\\\ 0.8 & -1.1 & 0.0 \\\\ 0.5 & 0.4 & -1.0 \\end{bmatrix}$。\n  - 时间 $\\{0.5, 1.0, 2.0, 3.0\\}$。\n  - 噪声标准差 $\\sigma = 0.1$。\n  - 输入振幅 $\\alpha = 0.5$。\n- **情况 2 参数**：\n  - 与情况 1 相同的 $A_\\mathrm{FB}$、$A_\\mathrm{FF}$、$\\sigma$、$\\alpha$。\n  - 时间 $\\{0.0\\}$。\n- **情况 3 参数**：\n  - 与情况 1 相同的 $A_\\mathrm{FB}$、$A_\\mathrm{FF}$、$\\alpha$。\n  - 时间 $\\{0.1, 0.2, 0.3, 0.4\\}$。\n  - 噪声标准差 $\\sigma = 0.05$。\n- **情况 4 参数**：\n  - $A_\\mathrm{FB} = \\begin{bmatrix} -1.3 & 0.0 & -0.05 \\\\ 0.6 & -1.2 & 0.0 \\\\ 0.0 & 0.5 & -1.1 \\end{bmatrix}$，$A_\\mathrm{FF} = \\begin{bmatrix} -1.3 & 0.0 & 0.0 \\\\ 0.6 & -1.2 & 0.0 \\\\ 1.0 & 0.5 & -1.1 \\end{bmatrix}$。\n  - 时间 $\\{1.0, 2.0, 3.0\\}$。\n  - 噪声标准差 $\\sigma = 0.2$。\n  - 输入振幅 $\\alpha = 0.5$。\n\n#### 步骤 2：使用提取的已知信息进行验证\n\n- **科学依据**：该问题在系统生物学和合成生物学中有充分的依据。线性常微分方程是基因网络动力学的标准近似方法，CRISPRi 是一种标准的扰动技术，贝叶斯模型选择是假设检验的主要统计框架。所提供的常数输入下的 ODE 解在数学上是正确的。所有陈述的原则都是标准且合理的。矩阵经确认是稳定的，确保了动力学的良好行为和可逆性。\n- **适定性**：该问题要求在一个有限离散选择集 $\\{0,1,2\\}$ 上最大化一个明确定义的目标函数。这保证了最大值的存在。明确的平局打破规则确保了唯一解。\n- **客观性**：问题以精确、定量和无偏见的语言陈述。\n- **不完整或矛盾的设置**：问题是自洽的。为每种情况都提供了所有必需的矩阵、参数（$\\alpha, \\sigma$）、初始条件和测量时间。没有矛盾之处。\n- **不切实际或不可行**：该问题是一个理论建模练习。所选的值是无量纲的，并且在此类建模背景下代表了一个合理的情景。\n- **其他缺陷**：该问题并非微不足道，需要推导目标函数并进行涉及矩阵代数的数值计算。它不是隐喻性的、循环论证的或无法验证的。\n\n#### 步骤 3：结论与行动\n\n问题是有效的。将提供完整的解决方案。\n\n### 解法推导与算法\n\n目标是选择 CRISPRi 靶点指数 $i \\in \\{0, 1, 2\\}$，以最大化区分两个模型 $\\mathcal{M}_\\mathrm{FB}$ 和 $\\mathcal{M}_\\mathrm{FF}$ 的能力。指定的准则是贝叶斯因子的期望对数，该期望是在两个模型都作为潜在数据生成过程的情况下平均得到的。\n\n令 $\\mathcal{M}_1$ 代表 $\\mathcal{M}_\\mathrm{FB}$，$\\mathcal{M}_2$ 代表 $\\mathcal{M}_\\mathrm{FF}$。对于一个设计选择 $i$，目标函数是效用 $U(i)$：\n$$\nU(i) = \\mathbb{E}_{\\mathbf{y}}[\\log B_{12}(\\mathbf{y})]\n$$\n问题指定了一个对称设计效用，这意味着期望是针对数据分布 $p(\\mathbf{y})$ 计算的，该分布是每个模型下分布的混合，先验概率相等 $P(\\mathcal{M}_1) = P(\\mathcal{M}_2) = 1/2$：\n$$\np(\\mathbf{y}) = p(\\mathbf{y} | \\mathcal{M}_1)P(\\mathcal{M}_1) + p(\\mathbf{y} | \\mathcal{M}_2)P(\\mathcal{M}_2) = \\frac{1}{2} p(\\mathbf{y} | \\mathcal{M}_1) + \\frac{1}{2} p(\\mathbf{y} | \\mathcal{M}_2)\n$$\n将其代入期望公式，并使用贝叶斯因子的定义 $B_{12}(\\mathbf{y}) = p(\\mathbf{y}|\\mathcal{M}_1) / p(\\mathbf{y}|\\mathcal{M}_2)$，我们得到：\n$$\nU(i) = \\int \\left( \\frac{1}{2} p(\\mathbf{y}|\\mathcal{M}_1) + \\frac{1}{2} p(\\mathbf{y}|\\mathcal{M}_2) \\right) \\log \\frac{p(\\mathbf{y}|\\mathcal{M}_1)}{p(\\mathbf{y}|\\mathcal{M}_2)} d\\mathbf{y}\n$$\n$$\nU(i) = \\frac{1}{2} \\int p(\\mathbf{y}|\\mathcal{M}_1) \\log \\frac{p(\\mathbf{y}|\\mathcal{M}_1)}{p(\\mathbf{y}|\\mathcal{M}_2)} d\\mathbf{y} + \\frac{1}{2} \\int p(\\mathbf{y}|\\mathcal{M}_2) \\log \\frac{p(\\mathbf{y}|\\mathcal{M}_1)}{p(\\mathbf{y}|\\mathcal{M}_2)} d\\mathbf{y}\n$$\n第一个积分是 $\\mathcal{M}_2$ 到 $\\mathcal{M}_1$ 的 Kullback-Leibler (KL) 散度的定义，即 $D_{KL}(p(\\cdot|\\mathcal{M}_1) \\| p(\\cdot|\\mathcal{M}_2))$。第二个积分是 $-D_{KL}(p(\\cdot|\\mathcal{M}_2) \\| p(\\cdot|\\mathcal{M}_1))$。因此，效用是这两个散度之和的一半，称为 Jeffreys 散度：\n$$\nU(i) = \\frac{1}{2} \\left[ D_{KL}(p(\\cdot|\\mathcal{M}_1) \\| p(\\cdot|\\mathcal{M}_2)) + D_{KL}(p(\\cdot|\\mathcal{M}_2) \\| p(\\cdot|\\mathcal{M}_1)) \\right]\n$$\n测量模型指出，观测值 $\\mathbf{y} \\in \\mathbb{R}^K$（对于 $K$ 个时间点）受到独立同分布的高斯噪声 $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$ 的干扰。在模型 $\\mathcal{M}_j$ 下，观测向量 $\\mathbf{y}$ 服从多元正态分布：$p(\\mathbf{y}|\\mathcal{M}_j) = \\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\mu}_j, \\Sigma)$，其中 $\\boldsymbol{\\mu}_j$ 是平均预测向量，协方差为 $\\Sigma = \\sigma^2 I_K$。\n\n两个多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_a, \\Sigma_a)$ 和 $\\mathcal{N}(\\boldsymbol{\\mu}_b, \\Sigma_b)$ 之间的 KL 散度由下式给出：\n$$\nD_{KL}(\\mathcal{N}_a \\| \\mathcal{N}_b) = \\frac{1}{2} \\left[ \\log \\frac{|\\Sigma_b|}{|\\Sigma_a|} - K + \\mathrm{tr}(\\Sigma_b^{-1} \\Sigma_a) + (\\boldsymbol{\\mu}_b - \\boldsymbol{\\mu}_a)^\\top \\Sigma_b^{-1} (\\boldsymbol{\\mu}_b - \\boldsymbol{\\mu}_a) \\right]\n$$\n在我们的情况下，协方差矩阵是相同的，$\\Sigma_1 = \\Sigma_2 = \\Sigma = \\sigma^2 I_K$。公式大大简化：\n$$\nD_{KL}(p(\\cdot|\\mathcal{M}_1) \\| p(\\cdot|\\mathcal{M}_2)) = \\frac{1}{2} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^\\top \\Sigma^{-1} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1) = \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2^2\n$$\n由于这个表达式相对于 $\\boldsymbol{\\mu}_1$ 和 $\\boldsymbol{\\mu}_2$ 是对称的，我们有 $D_{KL}(p_1 \\| p_2) = D_{KL}(p_2 \\| p_1)$。效用函数变为：\n$$\nU(i) = \\frac{1}{2} \\left[ \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2^2 + \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2^2 \\right] = \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\mu}_1(i) - \\boldsymbol{\\mu}_2(i)\\|_2^2\n$$\n其中平均预测向量 $\\boldsymbol{\\mu}_j(i)$ 取决于所选的靶基因 $i$。最优设计 $i^\\star$ 是最大化此效用的那个：\n$$\ni^\\star = \\arg\\max_{i \\in \\{0,1,2\\}} U(i)\n$$\n由于对于任何给定的实验设置，$1/(2\\sigma^2)$ 是一个正常数，这等价于最大化两个模型预测的平均轨迹之间的平方欧几里得距离：\n$$\ni^\\star = \\arg\\max_{i \\in \\{0,1,2\\}} \\|\\boldsymbol{\\mu}_\\mathrm{FB}(i) - \\boldsymbol{\\mu}_\\mathrm{FF}(i)\\|_2^2 = \\arg\\max_{i \\in \\{0,1,2\\}} \\sum_{k=1}^K (\\mu_\\mathrm{FB}(t_k, i) - \\mu_\\mathrm{FF}(t_k, i))^2\n$$\n\n找到 $i^\\star$ 的算法如下：\n1. 对于每个测试用例，遍历每个可能的 CRISPRi 靶点指数 $i \\in \\{0, 1, 2\\}$。\n2. 对于每个指数 $i$，定义常数输入向量 $\\mathbf{d} = -\\alpha \\mathbf{e}_i$，其中 $\\mathbf{e}_i$ 是第 $i$ 个标准基向量（例如，对于 $i=0$，$\\mathbf{e}_0 = [1, 0, 0]^\\top$）。\n3. 对于每个模型，$\\mathcal{M}_\\mathrm{FB}$ 和 $\\mathcal{M}_\\mathrm{FF}$，其系统矩阵分别为 $A_\\mathrm{FB}$ 和 $A_\\mathrm{FF}$： a. 计算平均预测向量 $\\boldsymbol{\\mu} \\in \\mathbb{R}^K$。对于每个测量时间 $t_k$： i. 如果 $t_k=0$，则预测为 $\\mu(0) = 0$，因为 $\\mathbf{x}(0)=\\mathbf{0}$。 ii. 如果 $t_k > 0$，计算状态向量 $\\mathbf{x}(t_k) = A^{-1} (e^{A t_k} - I) \\mathbf{d}$。这需要计算矩阵的逆 $A^{-1}$ 和矩阵指数 $e^{A t_k}$。 iii. 平均预测是状态向量的第三个分量，$\\mu(t_k) = h^\\top \\mathbf{x}(t_k) = [\\mathbf{x}(t_k)]_2$（使用基于 0 的索引）。\n4. 计算预测向量 $\\boldsymbol{\\mu}_\\mathrm{FB}(i)$ 和 $\\boldsymbol{\\mu}_\\mathrm{FF}(i)$ 后，计算平方欧几里得距离 $S_i = \\|\\boldsymbol{\\mu}_\\mathrm{FB}(i) - \\boldsymbol{\\mu}_\\mathrm{FF}(i)\\|_2^2$。该值与设计效用成正比。\n5. 计算 $S_0$、$S_1$ 和 $S_2$ 后，找到最大值 $S_{\\max}$。\n6. 确定索引集合 $I_{opt} = \\{i \\mid S_{\\max} - S_i \\le \\epsilon \\}$，其中 $\\epsilon = 10^{-9}$ 是指定的容差。\n7. 最优靶点是该集合中的最小索引，$i^\\star = \\min(I_{opt})$。\n8. 对所有测试用例重复此过程。对于情况 2，唯一的测量时间是 $t=0$，两个模型的预测都是 $0$，因此所有靶点的距离都为 $0$。根据平局打破规则，答案必须是 $0$。\n\n该过程被实现以解决四个测试用例。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm, inv\n\ndef solve():\n    \"\"\"\n    Computes the optimal CRISPRi target for model discrimination for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A_FB\": np.array([[-1.2, 0.0, -0.5], [0.7, -1.1, 0.0], [0.0, 0.6, -1.0]]),\n            \"A_FF\": np.array([[-1.2, 0.0, 0.0], [0.8, -1.1, 0.0], [0.5, 0.4, -1.0]]),\n            \"times\": np.array([0.5, 1.0, 2.0, 3.0]),\n            \"sigma\": 0.1,\n            \"alpha\": 0.5,\n        },\n        {\n            \"A_FB\": np.array([[-1.2, 0.0, -0.5], [0.7, -1.1, 0.0], [0.0, 0.6, -1.0]]),\n            \"A_FF\": np.array([[-1.2, 0.0, 0.0], [0.8, -1.1, 0.0], [0.5, 0.4, -1.0]]),\n            \"times\": np.array([0.0]),\n            \"sigma\": 0.1,\n            \"alpha\": 0.5,\n        },\n        {\n            \"A_FB\": np.array([[-1.2, 0.0, -0.5], [0.7, -1.1, 0.0], [0.0, 0.6, -1.0]]),\n            \"A_FF\": np.array([[-1.2, 0.0, 0.0], [0.8, -1.1, 0.0], [0.5, 0.4, -1.0]]),\n            \"times\": np.array([0.1, 0.2, 0.3, 0.4]),\n            \"sigma\": 0.05,\n            \"alpha\": 0.5,\n        },\n        {\n            \"A_FB\": np.array([[-1.3, 0.0, -0.05], [0.6, -1.2, 0.0], [0.0, 0.5, -1.1]]),\n            \"A_FF\": np.array([[-1.3, 0.0, 0.0], [0.6, -1.2, 0.0], [1.0, 0.5, -1.1]]),\n            \"times\": np.array([1.0, 2.0, 3.0]),\n            \"sigma\": 0.2,\n            \"alpha\": 0.5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        optimal_index = compute_optimal_target(\n            case[\"A_FB\"], case[\"A_FF\"], case[\"times\"], case[\"alpha\"]\n        )\n        results.append(optimal_index)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_mean_predictions(A, d, times):\n    \"\"\"\n    Computes the mean predicted output trajectory for a given model and input.\n    \n    Args:\n        A (np.ndarray): The system matrix.\n        d (np.ndarray): The constant input vector.\n        times (np.ndarray): The vector of measurement time points.\n\n    Returns:\n        np.ndarray: The vector of mean predictions.\n    \"\"\"\n    predictions = []\n    # Pre-compute the inverse of A as it is constant for all time points\n    A_inv = inv(A)\n    I = np.identity(A.shape[0])\n    \n    for t in times:\n        if t == 0.0:\n            # At t=0, x(0)=0, so the prediction is 0.\n            predictions.append(0.0)\n        else:\n            # Calculate the state x(t) = A_inv * (expm(A*t) - I) * d\n            exp_At = expm(A * t)\n            x_t = A_inv @ (exp_At - I) @ d\n            # The observation is the 3rd component of the state vector (h^T * x)\n            # h = [0, 0, 1]^T\n            prediction = x_t[2]\n            predictions.append(prediction)\n            \n    return np.array(predictions)\n\ndef compute_optimal_target(A_FB, A_FF, times, alpha, num_genes=3, tolerance=1e-9):\n    \"\"\"\n    Finds the optimal gene target to maximize model discriminability.\n\n    Args:\n        A_FB (np.ndarray): System matrix for the feedback model.\n        A_FF (np.ndarray): System matrix for the feedforward model.\n        times (np.ndarray): Measurement time points.\n        alpha (float): CRISPRi knockdown amplitude.\n        num_genes (int): Number of genes in the network.\n        tolerance (float): Absolute tolerance for tie-breaking.\n\n    Returns:\n        int: The optimal target gene index (0, 1, or 2).\n    \"\"\"\n    \n    # As derived, we need to maximize the squared Euclidean distance between\n    # the mean prediction vectors of the two models. The 1/(2*sigma^2) term\n    # is a positive constant and does not affect the argmax.\n    \n    squared_distances = []\n\n    for i in range(num_genes):\n        # Define the input vector d for targeting gene i\n        d = np.zeros(num_genes)\n        d[i] = -alpha\n\n        # Compute mean prediction trajectories for both models\n        mu_FB = compute_mean_predictions(A_FB, d, times)\n        mu_FF = compute_mean_predictions(A_FF, d, times)\n        \n        # Compute the squared Euclidean distance\n        dist_sq = np.sum((mu_FB - mu_FF)**2)\n        squared_distances.append(dist_sq)\n\n    squared_distances = np.array(squared_distances)\n    \n    # Find the index/indices that maximize the distance\n    max_dist = np.max(squared_distances)\n    \n    # Find all indices that are within the tolerance of the maximum\n    candidate_indices = np.where(max_dist - squared_distances = tolerance)[0]\n    \n    # Return the smallest index among the candidates as per the tie-breaking rule\n    return np.min(candidate_indices)\n\nsolve()\n```"
        }
    ]
}