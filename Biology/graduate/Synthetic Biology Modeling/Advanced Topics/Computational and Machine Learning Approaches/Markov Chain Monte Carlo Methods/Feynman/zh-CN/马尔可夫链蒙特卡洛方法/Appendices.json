{
    "hands_on_practices": [
        {
            "introduction": "掌握马尔可夫链蒙特卡洛（MCMC）方法的第一步是理解其核心的决策机制。这个练习提供了一个具体的计算任务：在最基础的Metropolis算法中，为一个提议移动计算其接受概率 。通过这个实践，您将固化对接受率公式的理解，该公式精妙地平衡了向更高概率区域移动的倾向与探索整个参数空间的需求。",
            "id": "1371728",
            "problem": "一位数据科学家正在实现一个马尔可夫链蒙特卡洛（MCMC）模拟，用以从参数 $x$ 的后验概率分布中抽取样本。目标分布 $\\pi(x)$ 与参数负绝对值的指数成正比，即 $\\pi(x) \\propto \\exp(-|x|)$。\n\n该科学家使用 Metropolis 算法和一个对称提议分布 $q(x'|x)$，其中在给定当前状态 $x$ 的情况下提议新状态 $x'$ 的概率等于在给定 $x'$ 的情况下提议 $x$ 的概率（即 $q(x'|x) = q(x|x')$）。\n\n假设在模拟过程中的某一步，链的当前状态为 $x = 1.5$。然后，算法提议移动到一个新的候选状态 $x' = 2.0$。\n\n计算这次特定移动的接受概率。您的答案应该是一个无量纲实数。请将最终答案四舍五入到四位有效数字。",
            "solution": "对于对称提议 $q(x'|x)=q(x|x')$，从 $x$ 移动到 $x'$ 的 Metropolis 接受概率为\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\n给定目标分布 $\\pi(x)\\propto \\exp(-|x|)$，该比率可简化为\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\n当 $x=1.5$ 且 $x'=2.0$ 时，我们有 $|x|=1.5$ 和 $|x'|=2.0$，所以\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\n因此，\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\n数值上，$\\exp(-0.5)\\approx 0.6065$（四舍五入到四位有效数字）。",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "掌握了基础的Metropolis算法后，下一个关键是理解更通用的Metropolis-Hastings框架，特别是当提议分布不对称时。这个问题  引导您分析一个故意设计的、存在缺陷的MCMC采样器，该采样器在计算接受概率时忽略了关键的提议密度比率。通过亲自推导出这个错误算法所收敛到的平稳分布，您将深刻体会到细致平衡条件的重要性，并理解为何Hastings修正项是确保采样正确性的理论基石。",
            "id": "791632",
            "problem": "一个 MCMC 采样器被构建用于从定义在正实数轴上的目标概率分布 $\\pi(x) = \\lambda e^{-\\lambda x}$（$x > 0$）中抽取样本，其中 $\\lambda$ 是一个正的率参数。其提议机制如下：给定当前状态 $x$，通过从区间 $[1/M, M]$ 上的均匀分布中抽取一个随机乘数 $U$ 来提议一个新状态 $x'$，其中 $M > 1$ 是一个固定常数，并将提议设为 $x' = xU$。\n\n对于此提议，Metropolis-Hastings 接受概率应为 $A(x'|x) = \\min\\left(1, \\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)$，其中 $q(x'|x)$ 是提议概率密度。然而，由于一个编程错误，提议密度比被忽略了，算法以一个有缺陷的接受概率 $A_{\\text{flawed}}(x'|x) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right)$ 实现。\n\n这个有缺陷的算法的马尔可夫链仍然是遍历的，并将收敛到一个唯一的平稳分布 $\\tilde{\\pi}(x)$，该分布不同于预期的目标分布 $\\pi(x)$。请推导这个不正确的平稳分布 $\\tilde{\\pi}(x)$ 的归一化概率密度函数。",
            "solution": "马尔可夫链蒙特卡洛算法的平稳分布 $\\tilde{\\pi}(x)$ 必须满足细致平衡条件。对于所述的有缺陷的算法，此条件为：\n$$\n\\tilde{\\pi}(x) P(x \\to x') = \\tilde{\\pi}(x') P(x' \\to x)\n$$\n其中 $P(x \\to x')$ 是从状态 $x$ 到 $x'$ 的转移概率密度。对于 $x \\neq x'$，它由提议密度 $q(x'|x)$ 和接受概率 $A_{\\text{flawed}}(x'|x)$ 的乘积给出。\n$$\n\\tilde{\\pi}(x) q(x'|x) A_{\\text{flawed}}(x'|x) = \\tilde{\\pi}(x') q(x|x') A_{\\text{flawed}}(x|x')\n$$\n有缺陷的接受概率为 $A_{\\text{flawed}}(x'|x) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right)$。我们可以用它来重写细致平衡方程：\n$$\n\\tilde{\\pi}(x) q(x'|x) \\min\\Big(1, \\frac{\\pi(x')}{\\pi(x)}\\Big) = \\tilde{\\pi}(x') q(x|x') \\min\\Big(1, \\frac{\\pi(x)}{\\pi(x')}\\Big)\n$$\n使用恒等式 $\\min(1, a) = a \\min(1/a, 1)$，我们可以将 $\\min\\Big(1, \\frac{\\pi(x')}{\\pi(x)}\\Big) = \\frac{\\pi(x')}{\\pi(x)} \\min\\Big(\\frac{\\pi(x)}{\\pi(x')}, 1\\Big)$ 代入左侧：\n$$\n\\tilde{\\pi}(x) q(x'|x) \\frac{\\pi(x')}{\\pi(x)} \\min\\Big(1, \\frac{\\pi(x)}{\\pi(x')}\\Big) = \\tilde{\\pi}(x') q(x|x') \\min\\Big(1, \\frac{\\pi(x)}{\\pi(x')}\\Big)\n$$\n由于对于所有 $x>0$，$\\pi(x) > 0$，因此项 $\\min\\Big(1, \\frac{\\pi(x)}{\\pi(x')}\\Big)$ 是正的，可以从等式两边消去。这留下一个平稳分布 $\\tilde{\\pi}(x)$ 必须满足的简化函数方程：\n$$\n\\tilde{\\pi}(x) q(x'|x) \\frac{\\pi(x')}{\\pi(x)} = \\tilde{\\pi}(x') q(x|x')\n$$\n\n接下来，我们确定提议密度 $q(x'|x)$ 和 $q(x|x')$。提议是 $x' = xU$，其中 $U$ 从 $[1/M, M]$ 上的均匀分布中抽取。$U$ 的概率密度函数为 $f_U(u) = \\frac{1}{M-1/M}$，对于 $u \\in [1/M, M]$。\n为了求 $q(x'|x)$，我们进行从 $U$ 到 $x'$ 的变量替换。\n$$\nU = \\frac{x'}{x} \\implies \\left|\\frac{dU}{dx'}\\right| = \\frac{1}{x}\n$$\n于是提议密度为：\n$$\nq(x'|x) = f_U(x'/x) \\left|\\frac{dU}{dx'}\\right| = \\frac{1}{M-1/M} \\cdot \\frac{1}{x}\n$$\n这在 $x'/x \\in [1/M, M]$ 时成立，即 $x' \\in [x/M, xM]$。\n根据对称性，反向提议密度为：\n$$\nq(x|x') = \\frac{1}{M-1/M} \\cdot \\frac{1}{x'}\n$$\n这在 $x/x' \\in [1/M, M]$ 时成立，这等价于 $x \\in [x'/M, x'M]$，是相同的支撑域。\n\n现在我们将密度代入简化的细致平衡方程。目标分布为 $\\pi(x) = \\lambda e^{-\\lambda x}$。其比值为 $\\frac{\\pi(x')}{\\pi(x)} = \\frac{\\lambda e^{-\\lambda x'}}{\\lambda e^{-\\lambda x}} = e^{\\lambda(x-x')}$。\n$$\n\\tilde{\\pi}(x) \\left(\\frac{1}{x(M-1/M)}\\right) e^{\\lambda(x-x')} = \\tilde{\\pi}(x') \\left(\\frac{1}{x'(M-1/M)}\\right)\n$$\n常数项 $1/(M-1/M)$ 被消去：\n$$\n\\frac{\\tilde{\\pi}(x)}{x} e^{\\lambda(x-x')} = \\frac{\\tilde{\\pi}(x')}{x'}\n$$\n我们可以分离变量 $x$ 和 $x'$：\n$$\n\\frac{\\tilde{\\pi}(x)}{x} e^{\\lambda x} = \\frac{\\tilde{\\pi}(x')}{x'} e^{\\lambda x'}\n$$\n由于此方程必须对所有有效的对 $(x, x')$ 成立，该表达式必须等于一个常数，我们称之为 $K$。\n$$\n\\frac{\\tilde{\\pi}(y)}{y} e^{\\lambda y} = K\n$$\n解出 $\\tilde{\\pi}(y)$，得到未归一化的平稳分布：\n$$\n\\tilde{\\pi}(y) = K y e^{-\\lambda y}\n$$\n\n最后，我们通过对概率密度函数进行归一化来确定常数 $K$。\n$$\n\\int_0^\\infty \\tilde{\\pi}(x) dx = 1 \\implies \\int_0^\\infty K x e^{-\\lambda x} dx = 1\n$$\n该积分与伽马函数 $\\int_0^\\infty t^{z-1}e^{-t}dt = \\Gamma(z)$ 有关。令 $u = \\lambda x$，则 $du = \\lambda dx$。\n$$\n\\int_0^\\infty x e^{-\\lambda x} dx = \\int_0^\\infty \\left(\\frac{u}{\\lambda}\\right) e^{-u} \\left(\\frac{du}{\\lambda}\\right) = \\frac{1}{\\lambda^2} \\int_0^\\infty u e^{-u} du\n$$\n该积分为 $\\Gamma(2) = 1! = 1$。所以，\n$$\n\\int_0^\\infty x e^{-\\lambda x} dx = \\frac{1}{\\lambda^2}\n$$\n将此结果代回归一化条件：\n$$\nK \\left(\\frac{1}{\\lambda^2}\\right) = 1 \\implies K = \\lambda^2\n$$\n因此，归一化的不正确的平稳分布是：\n$$\n\\tilde{\\pi}(x) = \\lambda^2 x e^{-\\lambda x}\n$$\n这是形状参数为 2、率参数为 $\\lambda$ 的伽马分布的概率密度函数。",
            "answer": "$$ \\boxed{\\lambda^2 x e^{-\\lambda x}} $$"
        },
        {
            "introduction": "理论学习最终要服务于实际应用。在合成生物学中，我们经常需要基于一系列协变量来对二元结果（如基因表达开启/关闭）进行概率建模。这个综合性练习将指导您从头开始，为一个在生物医学研究中广泛应用的贝叶斯逻辑回归模型，构建一个完整的Metropolis-Hastings采样器 。它涵盖了从模型设定、先验选择、编写稳定数值的采样代码，到后验推断和预测的全过程，是连接MCMC理论与复杂数据分析实践的桥梁。",
            "id": "4809452",
            "problem": "考虑医学中病例-对照数据的独立二元结果，其中对于每个受试者 $i$，观测到一个响应 $y_i \\in \\{0,1\\}$，指示其为病例 ($y_i = 1$) 或对照 ($y_i = 0$)，以及协变量 $\\mathbf{x}_i \\in \\mathbb{R}^p$。假设一个使用标准 logit 链接函数的逻辑斯蒂回归模型，其中给定协变量时为病例的条件概率是 $p(y_i = 1 \\mid \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\sigma(\\eta_i)$，其中 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ 且 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。在独立观测下，似然函数是伯努利概率的乘积。假设一个贝叶斯模型，其中回归系数 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ 具有独立的、均值为 $\\boldsymbol{\\mu}$、对角方差为 $\\operatorname{diag}(\\boldsymbol{\\sigma}^2)$ 的正态先验。\n\n使用贝叶斯定理，后验密度与似然和先验的乘积成正比。在马尔可夫链蒙特卡洛 (MCMC) 框架内设计一个 Metropolis-Hastings (MH) 采样器，其中提议是从以当前状态为中心、具有指定对称协方差的多元正态随机游走中抽取的。在逻辑斯蒂似然评估中使用数值稳定的计算。对于下述每个测试用例，实现 MH 采样器，运行指定的迭代次数，丢弃老化期样本，并对链进行稀疏化。报告接受率（以小数形式）、每个系数的后验均值，以及在后验分布下针对指定新协变量向量的后验预测概率。\n\n使用的基本原理：\n- 伯努利似然：对于 $y_i \\in \\{0,1\\}$，$P(y_i \\mid \\eta_i) = \\sigma(\\eta_i)^{y_i} \\left(1 - \\sigma(\\eta_i)\\right)^{1 - y_i}$。\n- 逻辑斯蒂函数定义：$\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- 贝叶斯定理：$\\pi(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X}) \\propto L(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}) \\, \\pi(\\boldsymbol{\\beta})$。\n- 正态先验密度：对于独立分量，$\\pi(\\boldsymbol{\\beta}) = \\prod_{j=1}^p \\mathcal{N}(\\beta_j \\mid \\mu_j, \\sigma_j^2)$。\n\n您必须解决以下三个测试用例。在每个用例中，使用提供的随机种子和分布规范来确定性地构建病例-对照数据集。在所有情况下都包含截距项，因此 $\\mathbf{x}_i = (1, x_{i1}, x_{i2})^\\top$ 且 $p = 3$。对于病例-对照数据，病例设为 $y_i = 1$，对照设为 $y_i = 0$。\n\n测试用例 A（平衡，典型）：\n- 数据生成种子：$314159$。\n- 病例数：$50$；对照数：$50$。\n- 协变量 $x_{1}$（二元“暴露”）：对于病例，$x_{1} \\sim \\operatorname{Bernoulli}(0.6)$；对于对照，$x_{1} \\sim \\operatorname{Bernoulli}(0.3)$。\n- 协变量 $x_{2}$（连续“年龄 $z$-分数”）：对于病例，$x_{2} \\sim \\mathcal{N}(0.3, 1^2)$；对于对照，$x_{2} \\sim \\mathcal{N}(-0.3, 1^2)$。\n- 先验均值和方差：$\\boldsymbol{\\mu} = (0, 0, 0)$，$\\boldsymbol{\\sigma}^2 = (25, 9, 9)$。\n- 提议协方差 (随机游走 MH)：$\\operatorname{diag}(0.05^2, 0.05^2, 0.05^2)$。\n- MCMC 提议种子：$271828$。\n- 迭代次数：$12000$；老化期：$6000$；稀疏化：$6$。\n- 用于后验预测的新协变量：$\\mathbf{x}_{\\text{new}} = (1, 1, 0.5)$。\n\n测试用例 B（近分离，强信号）：\n- 数据生成种子：$161803$。\n- 病例数：$40$；对照数：$40$。\n- 协变量 $x_{1}$（连续“生物标志物”）：对于病例，$x_{1} \\sim \\mathcal{N}(2.5, 0.7^2)$；对于对照，$x_{1} \\sim \\mathcal{N}(-2.5, 0.7^2)$。\n- 协变量 $x_{2}$（二元“治疗”）：对于病例，$x_{2} \\sim \\operatorname{Bernoulli}(0.9)$；对于对照，$x_{2} \\sim \\operatorname{Bernoulli}(0.1)$。\n- 先验均值和方差：$\\boldsymbol{\\mu} = (0, 0, 0)$，$\\boldsymbol{\\sigma}^2 = (25, 4, 4)$。\n- 提议协方差 (随机游走 MH)：$\\operatorname{diag}(0.02^2, 0.02^2, 0.02^2)$。\n- MCMC 提议种子：$141421$。\n- 迭代次数：$16000$；老化期：$8000$；稀疏化：$8$。\n- 用于后验预测的新协变量：$\\mathbf{x}_{\\text{new}} = (1, 0, 1.0)$。\n\n测试用例 C（小样本边界）：\n- 数据生成种子：$123457$。\n- 病例数：$8$；对照数：$12$。\n- 协变量 $x_{1}$（二元“暴露”）：对于病例，$x_{1} \\sim \\operatorname{Bernoulli}(0.5)$；对于对照，$x_{1} \\sim \\operatorname{Bernoulli}(0.4)$。\n- 协变量 $x_{2}$（连续“标志物”）：对于病例，$x_{2} \\sim \\mathcal{N}(0.1, 1^2)$；对于对照，$x_{2} \\sim \\mathcal{N}(-0.1, 1^2)$。\n- 先验均值和方差：$\\boldsymbol{\\mu} = (0, 0, 0)$，$\\boldsymbol{\\sigma}^2 = (100, 16, 16)$。\n- 提议协方差 (随机游走 MH)：$\\operatorname{diag}(0.08^2, 0.08^2, 0.08^2)$。\n- MCMC 提议种子：$57721$。\n- 迭代次数：$10000$；老化期：$5000$；稀疏化：$5$。\n- 用于后验预测的新协变量：$\\mathbf{x}_{\\text{new}} = (1, 1, -0.2)$。\n\n算法要求：\n- 为对数似然使用数值稳定的表达式。具体来说，如果 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$，计算 $\\log L(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\operatorname{softplus}(\\eta_i) \\right)$，其中 $\\operatorname{softplus}(z) = \\log(1 + e^{z})$ 应以数值稳定的方式计算。\n- 为每个分量使用独立的正态先验：$\\log \\pi(\\boldsymbol{\\beta}) = \\sum_{j=1}^p \\left( -\\frac{1}{2} \\log(2\\pi \\sigma_j^2) - \\frac{(\\beta_j - \\mu_j)^2}{2\\sigma_j^2} \\right)$。\n- 采用对称的多元正态随机游走提议：$\\boldsymbol{\\beta}^{\\ast} = \\boldsymbol{\\beta}^{(t)} + \\mathbf{z}$，其中 $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q})$。由于提议是对称的，接受概率为 $\\alpha = \\min\\left\\{1, \\exp\\left[ \\log \\pi(\\boldsymbol{\\beta}^{\\ast} \\mid \\mathbf{y}, \\mathbf{X}) - \\log \\pi(\\boldsymbol{\\beta}^{(t)} \\mid \\mathbf{y}, \\mathbf{X}) \\right] \\right\\}$。\n- 采样后，计算接受率（总迭代中接受的提议所占比例）、已保存样本中每个系数的后验均值，以及通过样本均值估计的后验预测概率 $E\\left[ \\sigma(\\mathbf{x}_{\\text{new}}^\\top \\boldsymbol{\\beta}) \\mid \\mathbf{y}, \\mathbf{X} \\right]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表形式的结果。该列表必须依次包含测试用例 A、然后测试用例 B、然后测试用例 C 的结果：接受率、$\\beta_0$（截距）的后验均值、$\\beta_1$ 的后验均值、$\\beta_2$ 的后验均值，以及 $\\mathbf{x}_{\\text{new}}$ 的后验预测概率；然后对下一个测试用例重复这五个量，以此类推。例如，输出格式为 $[r_{A}, m_{A0}, m_{A1}, m_{A2}, p_{A}, r_{B}, m_{B0}, m_{B1}, m_{B2}, p_{B}, r_{C}, m_{C0}, m_{C1}, m_{C2}, p_{C}]$，其中每个符号代表一个实数。",
            "solution": "我们为病例-对照数据构建贝叶斯逻辑斯蒂回归，从基本原理出发指定似然和先验，并在马尔可夫链蒙特卡洛 (MCMC) 框架内推导 Metropolis-Hastings (MH) 采样方案。\n\n首先，定义逻辑斯蒂函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 和每个具有协变量 $\\mathbf{x}_i$ 和系数 $\\boldsymbol{\\beta}$ 的受试者 $i$ 的线性预测器 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$。单个观测的伯努利似然为 $P(y_i \\mid \\eta_i) = \\sigma(\\eta_i)^{y_i} \\left(1 - \\sigma(\\eta_i)\\right)^{1 - y_i}$。对于独立观测，联合似然是所有 $i$ 的乘积。在病例-对照设计中，$y_i$ 表示病例状态，但条件模型 $P(y_i \\mid \\mathbf{x}_i, \\boldsymbol{\\beta})$ 保持了伯努利-logit 形式；截距项吸收了任何抽样比例效应，协变量的回归系数仍然可以解释为对数优势比，这在医学证据综合中至关重要。\n\n为了数值稳定性，避免直接计算 $\\log \\left(1 - \\sigma(\\eta_i)\\right)$，而是使用以下恒等式\n$$\n\\log L(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log\\left(1 + e^{\\eta_i}\\right) \\right) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\operatorname{softplus}(\\eta_i) \\right),\n$$\n其中函数 $\\operatorname{softplus}(z) = \\log(1 + e^{z})$ 应使用数值稳定的分支进行计算：\n$$\n\\operatorname{softplus}(z) = \n\\begin{cases}\nz + \\log(1 + e^{-z}),  & z > 0, \\\\\n\\log(1 + e^{z}),  & z \\le 0.\n\\end{cases}\n$$\n这避免了当 $|z|$ 很大时 $e^{z}$ 的上溢或下溢。\n\n为每个系数指定独立的先验正态分布，\n$$\n\\pi(\\boldsymbol{\\beta}) = \\prod_{j=1}^p \\mathcal{N}(\\beta_j \\mid \\mu_j, \\sigma_j^2),\n$$\n其对数先验为\n$$\n\\log \\pi(\\boldsymbol{\\beta}) = \\sum_{j=1}^p \\left( -\\frac{1}{2} \\log(2\\pi \\sigma_j^2) - \\frac{(\\beta_j - \\mu_j)^2}{2\\sigma_j^2} \\right).\n$$\n根据贝叶斯定理，后验满足\n$$\n\\pi(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X}) \\propto L(\\mathbf{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}) \\, \\pi(\\boldsymbol{\\beta}),\n$$\n因此对数后验为\n$$\n\\log \\pi(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X}) = \\sum_{i=1}^n \\left( y_i \\mathbf{x}_i^\\top \\boldsymbol{\\beta} - \\operatorname{softplus}(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) \\right) + \\sum_{j=1}^p \\left( -\\frac{1}{2} \\log(2\\pi \\sigma_j^2) - \\frac{(\\beta_j - \\mu_j)^2}{2\\sigma_j^2} \\right) + C,\n$$\n其中 $C$ 是一个与比率无关的加性常数。\n\n为了从 $\\pi(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{X})$ 中采样，我们使用对称的多元正态随机游走提议来实现 Metropolis-Hastings 算法：\n- 在第 $t$ 次迭代，提议 $\\boldsymbol{\\beta}^{\\ast} = \\boldsymbol{\\beta}^{(t)} + \\mathbf{z}$，其中 $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q})$，$\\mathbf{Q}$ 是指定的提议协方差矩阵，为简化通常为对角矩阵。\n- 计算接受概率\n$$\n\\alpha = \\min\\left\\{1, \\exp\\left( \\log \\pi(\\boldsymbol{\\beta}^{\\ast} \\mid \\mathbf{y}, \\mathbf{X}) - \\log \\pi(\\boldsymbol{\\beta}^{(t)} \\mid \\mathbf{y}, \\mathbf{X}) \\right) \\right\\}.\n$$\n由于提议的对称性，Hastings 校正项相互抵消。以概率 $\\alpha$ 接受提议；否则，保留 $\\boldsymbol{\\beta}^{(t)}$。\n\n从 $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$ 开始初始化。运行指定的迭代次数。丢弃老化期的迭代以减轻初始状态的影响，然后通过保留每 $k$ 个样本来稀疏化链，以减少自相关；尽管理论上稀疏化并非绝对必要，但它有助于计算摘要并确保保存的样本数量是可管理的。计算接受率，即总迭代次数中被接受的提议所占的比例。\n\n对于后验摘要：\n- 每个系数的后验均值通过对保存的抽样取样本均值来估计，$\\hat{E}[\\beta_j \\mid \\mathbf{y}, \\mathbf{X}] = \\frac{1}{M} \\sum_{m=1}^M \\beta_{j}^{(m)}$。\n- 在新协变量向量 $\\mathbf{x}_{\\text{new}}$ 处的后验预测概率通过以下方式估计\n$$\n\\hat{E}\\left[ \\sigma(\\mathbf{x}_{\\text{new}}^\\top \\boldsymbol{\\beta}) \\mid \\mathbf{y}, \\mathbf{X} \\right] = \\frac{1}{M} \\sum_{m=1}^M \\sigma\\left(\\mathbf{x}_{\\text{new}}^\\top \\boldsymbol{\\beta}^{(m)}\\right),\n$$\n其中 $\\boldsymbol{\\beta}^{(m)}$ 是老化和稀疏化后保存的抽样。\n\n对于每个测试用例，使用提供的种子和分布确定性地构建数据。具体来说，给定 $n_{\\text{cases}}$ 和 $n_{\\text{controls}}$ 以及协变量分布，使用数据生成种子为病例和对照独立抽样协变量，为病例设置 $y_i=1$，为对照设置 $y_i=0$，并将所有受试者堆叠起来形成带有截距列的 $\\mathbf{X}$。使用指定的先验参数 $(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2)$、提议协方差 $\\mathbf{Q}$、MCMC 提议种子、总迭代次数、老化期和稀疏化。依次计算接受率、$\\beta_0$ 的后验均值、$\\beta_1$ 的后验均值、$\\beta_2$ 的后验均值，以及在指定 $\\mathbf{x}_{\\text{new}}$ 处的后验预测概率。\n\n最后，将测试用例 A、测试用例 B 和测试用例 C 的结果汇总成一个用方括号括起来的、以逗号分隔的列表，正如要求的那样。此设计测试了典型行为、通过先验进行正则化时对近分离情况的处理，以及小样本稳健性，反映了医学证据中的现实场景，在这些场景中，针对病例-对照数据的逻辑斯蒂回归很普遍，而贝叶斯正则化在具有挑战性的环境中能够稳定推断。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softplus(x):\n    # Numerically stable softplus\n    # softplus(x) = log(1 + exp(x))\n    return np.where(x > 0, x + np.log1p(np.exp(-x)), np.log1p(np.exp(x)))\n\ndef sigmoid(x):\n    # Numerically stable logistic function\n    # If x >= 0: 1/(1+exp(-x)) ; else: exp(x)/(1+exp(x))\n    return np.where(x >= 0, 1.0 / (1.0 + np.exp(-x)), np.exp(x) / (1.0 + np.exp(x)))\n\ndef log_posterior(beta, X, y, mu, sigma2):\n    # beta: (p,)\n    # X: (n, p), y: (n,)\n    eta = X @ beta\n    ll = np.sum(y * eta - softplus(eta))  # stable log-likelihood\n    # Independent normal priors\n    lp = -0.5 * np.sum(np.log(2.0 * np.pi * sigma2)) - 0.5 * np.sum(((beta - mu) ** 2) / sigma2)\n    return ll + lp\n\ndef metropolis_hastings(initial_beta, X, y, mu, sigma2, proposal_cov, n_iter, rng):\n    p = initial_beta.shape[0]\n    beta = initial_beta.copy()\n    samples = np.zeros((n_iter, p))\n    accept_count = 0\n    current_logpost = log_posterior(beta, X, y, mu, sigma2)\n    # Cholesky of proposal covariance for efficient sampling\n    chol = np.linalg.cholesky(proposal_cov)\n    for t in range(n_iter):\n        z = rng.normal(size=p)\n        proposal = beta + chol @ z\n        proposal_logpost = log_posterior(proposal, X, y, mu, sigma2)\n        log_alpha = proposal_logpost - current_logpost\n        if np.log(rng.uniform())  log_alpha:\n            beta = proposal\n            current_logpost = proposal_logpost\n            accept_count += 1\n        samples[t, :] = beta\n    return samples, accept_count\n\ndef generate_case_control(seed, n_cases, n_controls, dist_cases, dist_controls):\n    \"\"\"\n    dist_cases/controls: dict with keys 'x1', 'x2' specifying distributions:\n      For Bernoulli: ('bern', p)\n      For Normal: ('norm', mu, sigma)\n    Returns X (n,3) with intercept, y (n,)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Cases\n    if dist_cases['x1'][0] == 'bern':\n        x1_cases = rng.binomial(1, dist_cases['x1'][1], size=n_cases).astype(float)\n    else:\n        x1_cases = rng.normal(dist_cases['x1'][1], dist_cases['x1'][2], size=n_cases)\n    if dist_cases['x2'][0] == 'bern':\n        x2_cases = rng.binomial(1, dist_cases['x2'][1], size=n_cases).astype(float)\n    else:\n        x2_cases = rng.normal(dist_cases['x2'][1], dist_cases['x2'][2], size=n_cases)\n    y_cases = np.ones(n_cases, dtype=float)\n    # Controls\n    if dist_controls['x1'][0] == 'bern':\n        x1_controls = rng.binomial(1, dist_controls['x1'][1], size=n_controls).astype(float)\n    else:\n        x1_controls = rng.normal(dist_controls['x1'][1], dist_controls['x1'][2], size=n_controls)\n    if dist_controls['x2'][0] == 'bern':\n        x2_controls = rng.binomial(1, dist_controls['x2'][1], size=n_controls).astype(float)\n    else:\n        x2_controls = rng.normal(dist_controls['x2'][1], dist_controls['x2'][2], size=n_controls)\n    y_controls = np.zeros(n_controls, dtype=float)\n    # Stack\n    x1 = np.concatenate([x1_cases, x1_controls])\n    x2 = np.concatenate([x2_cases, x2_controls])\n    y = np.concatenate([y_cases, y_controls])\n    intercept = np.ones_like(y)\n    X = np.column_stack([intercept, x1, x2])\n    return X, y\n\ndef posterior_predictive_prob(samples, x_new):\n    # samples: (m, p)\n    # x_new: (p,)\n    eta = samples @ x_new\n    probs = sigmoid(eta)\n    return float(np.mean(probs))\n\ndef summarize_chain(samples, burn_in, thin):\n    # Return thinned samples after burn-in\n    post = samples[burn_in:]\n    if thin  1:\n        post = post[::thin]\n    return post\n\ndef run_test_case(X, y, mu, sigma2, prop_cov, n_iter, burn_in, thin, mcmc_seed, x_new):\n    rng = np.random.default_rng(mcmc_seed)\n    initial_beta = np.zeros(X.shape[1], dtype=float)\n    samples, accept_count = metropolis_hastings(initial_beta, X, y, mu, sigma2, prop_cov, n_iter, rng)\n    post = summarize_chain(samples, burn_in, thin)\n    accept_rate = accept_count / n_iter\n    beta_mean = np.mean(post, axis=0)\n    p_pred = posterior_predictive_prob(post, x_new)\n    return accept_rate, beta_mean, p_pred\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Test Case A\n    X_A, y_A = generate_case_control(\n        seed=314159,\n        n_cases=50, n_controls=50,\n        dist_cases={'x1': ('bern', 0.6), 'x2': ('norm', 0.3, 1.0)},\n        dist_controls={'x1': ('bern', 0.3), 'x2': ('norm', -0.3, 1.0)}\n    )\n    mu_A = np.array([0.0, 0.0, 0.0])\n    sigma2_A = np.array([25.0, 9.0, 9.0])\n    prop_cov_A = np.diag([0.05**2, 0.05**2, 0.05**2])\n    n_iter_A, burn_A, thin_A = 12000, 6000, 6\n    mcmc_seed_A = 271828\n    x_new_A = np.array([1.0, 1.0, 0.5])\n\n    # Test Case B\n    X_B, y_B = generate_case_control(\n        seed=161803,\n        n_cases=40, n_controls=40,\n        dist_cases={'x1': ('norm', 2.5, 0.7), 'x2': ('bern', 0.9)},\n        dist_controls={'x1': ('norm', -2.5, 0.7), 'x2': ('bern', 0.1)}\n    )\n    mu_B = np.array([0.0, 0.0, 0.0])\n    sigma2_B = np.array([25.0, 4.0, 4.0])\n    prop_cov_B = np.diag([0.02**2, 0.02**2, 0.02**2])\n    n_iter_B, burn_B, thin_B = 16000, 8000, 8\n    mcmc_seed_B = 141421\n    x_new_B = np.array([1.0, 0.0, 1.0])\n\n    # Test Case C\n    X_C, y_C = generate_case_control(\n        seed=123457,\n        n_cases=8, n_controls=12,\n        dist_cases={'x1': ('bern', 0.5), 'x2': ('norm', 0.1, 1.0)},\n        dist_controls={'x1': ('bern', 0.4), 'x2': ('norm', -0.1, 1.0)}\n    )\n    mu_C = np.array([0.0, 0.0, 0.0])\n    sigma2_C = np.array([100.0, 16.0, 16.0])\n    prop_cov_C = np.diag([0.08**2, 0.08**2, 0.08**2])\n    n_iter_C, burn_C, thin_C = 10000, 5000, 5\n    mcmc_seed_C = 57721\n    x_new_C = np.array([1.0, 1.0, -0.2])\n\n    results = []\n\n    # Run A\n    acc_A, beta_mean_A, p_pred_A = run_test_case(\n        X_A, y_A, mu_A, sigma2_A, prop_cov_A, n_iter_A, burn_A, thin_A, mcmc_seed_A, x_new_A\n    )\n    results.extend([acc_A, beta_mean_A[0], beta_mean_A[1], beta_mean_A[2], p_pred_A])\n\n    # Run B\n    acc_B, beta_mean_B, p_pred_B = run_test_case(\n        X_B, y_B, mu_B, sigma2_B, prop_cov_B, n_iter_B, burn_B, thin_B, mcmc_seed_B, x_new_B\n    )\n    results.extend([acc_B, beta_mean_B[0], beta_mean_B[1], beta_mean_B[2], p_pred_B])\n\n    # Run C\n    acc_C, beta_mean_C, p_pred_C = run_test_case(\n        X_C, y_C, mu_C, sigma2_C, prop_cov_C, n_iter_C, burn_C, thin_C, mcmc_seed_C, x_new_C\n    )\n    results.extend([acc_C, beta_mean_C[0], beta_mean_C[1], beta_mean_C[2], p_pred_C])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}