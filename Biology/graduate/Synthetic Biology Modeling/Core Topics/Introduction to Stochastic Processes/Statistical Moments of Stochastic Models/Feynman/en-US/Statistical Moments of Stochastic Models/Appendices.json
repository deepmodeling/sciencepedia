{
    "hands_on_practices": [
        {
            "introduction": "While deterministic ordinary differential equations ($ODEs$) provide a useful macroscopic view, they often fail to capture the behavior of stochastic systems with nonlinear dynamics. This exercise takes you to the heart of this discrepancy by deriving the exact equation for the mean copy number in a classic auto-activating gene circuit (). By starting from the Chemical Master Equation, you will discover firsthand why the evolution of the mean depends on higher-order moments, a fundamental issue known as the moment closure problem.",
            "id": "3931985",
            "problem": "Consider a single-species stochastic biochemical model for an auto-activating protein in a synthetic gene circuit. Let $X(t) \\in \\mathbb{N}_{0}$ denote the protein copy number at time $t$, governed by a continuous-time Markov chain with two reaction channels: a birth (synthesis) reaction that increases $X$ by $+1$ and a death (degradation) reaction that decreases $X$ by $-1$. The birth propensity is given by the Hill function $a(x)=\\alpha \\frac{x^{n}}{K^{n}+x^{n}}$ with parameters $\\alpha>0$, $K>0$, and Hill coefficient $n \\in \\mathbb{N}$, and the degradation propensity is $b(x)=\\mu x$ with $\\mu>0$. Assume that the dynamics are described by the Chemical Master Equation (CME), and that the process is non-explosive and has finite moments of all orders for all $t \\ge 0$.\n\nStarting from first principles—namely, the CME and the definition of the infinitesimal generator for a birth-death process—derive the Ordinary Differential Equation (ODE) that governs the time evolution of the mean copy number $m(t)=\\mathbb{E}[X(t)]$. In your derivation, make clear how the stoichiometric changes and propensities enter the generator, and use this to obtain the mean equation in exact form, without invoking any closure approximations.\n\nThen, provide a brief mechanistic explanation for why the resulting mean equation depends on statistical quantities beyond $m(t)$, and discuss the role of nonlinearity in the Hill propensity in creating that dependence. You may, if useful, refer to a smooth extension of the Hill function to the real line to justify moment expansions, but your final expression for the mean equation must be exact.\n\nYour final reported answer must be the fully simplified exact analytical expression for $\\frac{d}{dt}\\mathbb{E}[X(t)]$ in terms of the model parameters and expectations. Do not provide numerical values. Do not include any units in the final expression.",
            "solution": "The problem asks for the derivation of the exact Ordinary Differential Equation (ODE) governing the time evolution of the mean protein copy number, $m(t)=\\mathbb{E}[X(t)]$, for a stochastic auto-activating gene circuit. The derivation must start from the Chemical Master Equation (CME) or its operator equivalent, the infinitesimal generator, without making any closure approximations.\n\nThe system is described as a continuous-time Markov chain on the state space $\\mathbb{N}_{0}=\\{0, 1, 2, \\dots\\}$, where the state $x$ represents the number of protein molecules. The dynamics are governed by two reaction channels:\n1.  Birth (synthesis): $X \\to X+1$, with propensity function $a(x) = \\alpha \\frac{x^{n}}{K^{n}+x^{n}}$. The state change is $s_1=+1$.\n2.  Death (degradation): $X \\to X-1$, with propensity function $b(x) = \\mu x$. The state change is $s_2=-1$.\n\nThe time evolution of the probability distribution $P(x, t) = \\operatorname{Prob}(X(t)=x)$ is given by the CME. However, a more direct route to the evolution of moments is via the infinitesimal generator of the Markov process. The generator, denoted $\\mathcal{L}$, describes the expected rate of change of any function $f(x)$ of the state, starting from state $x$. For a birth-death process, its action on a function $f: \\mathbb{N}_{0} \\to \\mathbb{R}$ is defined as:\n$$ (\\mathcal{L}f)(x) = a(x)[f(x+1) - f(x)] + b(x)[f(x-1) - f(x)] $$\nThis expression sums the product of each reaction's propensity and the corresponding change in the function $f$.\n\nThe time evolution of the expectation of any observable $f(X(t))$ is given by Dynkin's formula, which for our purposes can be stated as:\n$$ \\frac{d}{dt}\\mathbb{E}[f(X(t))] = \\mathbb{E}[(\\mathcal{L}f)(X(t))] $$\nThis equation is a direct consequence of the CME and provides the fundamental relationship between the microscopic stochastic dynamics and the macroscopic evolution of expected values.\n\nTo derive the ODE for the mean copy number $m(t) = \\mathbb{E}[X(t)]$, we choose the function $f(x) = x$. We first compute the action of the generator $\\mathcal{L}$ on $f(x)=x$:\n$$ (\\mathcal{L}x)(x) = a(x)[(x+1) - x] + b(x)[(x-1) - x] $$\nThe terms in the brackets represent the stoichiometric changes of $+1$ and $-1$ for the birth and death reactions, respectively.\n$$ (\\mathcal{L}x)(x) = a(x)(1) + b(x)(-1) = a(x) - b(x) $$\n\nNow, we substitute this result into the evolution equation for the expectation:\n$$ \\frac{d}{dt}\\mathbb{E}[X(t)] = \\mathbb{E}[(\\mathcal{L}X)(X(t))] = \\mathbb{E}[a(X(t)) - b(X(t))] $$\nThe expectation operator $\\mathbb{E}[\\cdot]$ is linear, so we can separate the terms:\n$$ \\frac{d}{dt}\\mathbb{E}[X(t)] = \\mathbb{E}[a(X(t))] - \\mathbb{E}[b(X(t))] $$\n\nNext, we substitute the specific forms of the propensity functions $a(x)$ and $b(x)$ into this equation.\nFor the degradation term, the propensity $b(x) = \\mu x$ is a linear function of $x$. Therefore, its expectation is:\n$$ \\mathbb{E}[b(X(t))] = \\mathbb{E}[\\mu X(t)] = \\mu \\mathbb{E}[X(t)] $$\nFor the synthesis term, the propensity $a(x) = \\alpha \\frac{x^{n}}{K^{n}+x^{n}}$ is a nonlinear Hill function. Its expectation is:\n$$ \\mathbb{E}[a(X(t))] = \\mathbb{E}\\left[\\alpha \\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] = \\alpha \\mathbb{E}\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] $$\nCombining these results, we obtain the exact ODE for the mean protein copy number $m(t)=\\mathbb{E}[X(t)]$:\n$$ \\frac{d}{dt}\\mathbb{E}[X(t)] = \\alpha \\mathbb{E}\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] - \\mu \\mathbb{E}[X(t)] $$\n\nThis equation is exact and derived without any approximations. A crucial feature of this result is that it is not closed. The rate of change of the first moment, $\\frac{d}{dt}\\mathbb{E}[X(t)]$, depends not only on the first moment $\\mathbb{E}[X(t)]$ itself but also on the expectation of a nonlinear function of $X(t)$. This expectation, $\\mathbb{E}\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right]$, cannot be expressed as a simple function of $\\mathbb{E}[X(t)]$ alone. By definition, it is an average over the entire probability distribution at time $t$:\n$$ \\mathbb{E}\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] = \\sum_{x=0}^{\\infty} \\left(\\frac{x^{n}}{K^{n}+x^{n}}\\right) P(x,t) $$\nThis dependence on the full distribution means the equation for the first moment is coupled to higher-order statistical moments.\n\nThe mechanistic reason for this coupling lies in the nonlinearity of the birth propensity, $a(x)$. To illustrate this, consider a Taylor series expansion of the nonlinear function $h(x) = \\frac{x^{n}}{K^{n}+x^{n}}$ around the mean $m = \\mathbb{E}[X(t)]$:\n$$ h(X(t)) \\approx h(m) + h'(m)(X(t)-m) + \\frac{1}{2!}h''(m)(X(t)-m)^{2} + \\dots $$\nTaking the expectation of both sides yields:\n$$ \\mathbb{E}[h(X(t))] \\approx \\mathbb{E}[h(m)] + \\mathbb{E}[h'(m)(X(t)-m)] + \\mathbb{E}\\left[\\frac{1}{2}h''(m)(X(t)-m)^{2}\\right] + \\dots $$\nUsing the linearity of expectation and the definitions of the mean and variance ($\\mathbb{E}[X(t)-m] = 0$ and $\\operatorname{Var}(X(t)) = \\sigma^{2}(t) = \\mathbb{E}[(X(t)-m)^{2}]$), we find:\n$$ \\mathbb{E}[h(X(t))] \\approx h(m) + \\frac{1}{2}h''(m)\\sigma^{2}(t) + \\dots $$\nThe term $\\mathbb{E}[h(X(t))]$ depends on the mean $m(t)$, the variance $\\sigma^{2}(t)$, and all higher-order central moments (from subsequent terms in the expansion). Consequently, the equation for the mean is coupled to the equation for the variance, which in turn is coupled to the third moment, and so on, creating an infinite, unclosed hierarchy of moment equations. This is a general feature of stochastic systems with nonlinear reaction rates. The nonlinearity inherent in the cooperative auto-activation mechanism, modeled by the Hill function, is the direct cause of this moment-closure problem. If the birth propensity were linear, e.g., $a(x) = \\beta x$, then $\\mathbb{E}[a(X(t))] = \\mathbb{E}[\\beta X(t)] = \\beta \\mathbb{E}[X(t)] = a(\\mathbb{E}[X(t)])$, and the equation for the mean would be a simple, closed linear ODE: $\\frac{d}{dt}\\mathbb{E}[X(t)] = (\\beta - \\mu)\\mathbb{E}[X(t)]$.",
            "answer": "$$\\boxed{\\alpha \\mathbb{E}\\left[\\frac{X(t)^{n}}{K^{n}+X(t)^{n}}\\right] - \\mu \\mathbb{E}[X(t)]}$$"
        },
        {
            "introduction": "Having established that nonlinear stochastic systems lead to an unclosed hierarchy of moment equations, the practical challenge becomes finding a way to obtain approximate solutions. This hands-on coding practice guides you through implementing and comparing several powerful moment closure approximations for a gene circuit with negative feedback (). By applying mean-field, Gaussian, and log-normal closures, you will not only translate theory into a functional model but also critically evaluate how the assumptions of each method affect its predictive accuracy across different regulatory regimes.",
            "id": "3931963",
            "problem": "Consider a single-species stochastic gene circuit for a protein $X$ governed by a birth-death process. The birth propensity is a repressed Hill function $a(X) = \\dfrac{k}{1 + \\left(\\dfrac{X}{K}\\right)^{n}}$ representing negative feedback of $X$ on its own production, and the death propensity is linear $b(X) = \\gamma X$. Assume a well-mixed system and a Markovian Chemical Master Equation (CME).\n\nStarting from the CME and the definitions of statistical moments, derive the coupled steady-state equations for the first and second moments of $X$, namely the steady-state conditions for $\\mathbb{E}[X]$ and $\\mathbb{E}[X^{2}]$, without introducing ad hoc formulas. Show how the nonlinear propensity $a(X)$ couples the moments through $\\mathbb{E}[a(X)]$ and mixed terms such as $\\mathbb{E}[X\\,a(X)]$.\n\nImplement three closures to obtain a closed system and predict the steady-state mean $\\mathbb{E}[X]$ and variance $\\operatorname{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$:\n- Mean-field closure: approximate $\\mathbb{E}[a(X)]$ by $a(\\mathbb{E}[X])$ and $\\mathbb{E}[X\\,a(X)]$ by $\\mathbb{E}[X]\\,a(\\mathbb{E}[X])$.\n- Second-order normal (Gaussian) closure via the delta method: approximate $\\mathbb{E}[f(X)] \\approx f(\\mathbb{E}[X]) + \\dfrac{1}{2} f''(\\mathbb{E}[X])\\,\\operatorname{Var}(X)$ for smooth $f$, and apply it to $f(X) = a(X)$ and $f(X) = X\\,a(X)$. Compute the required derivatives analytically.\n- Log-normal closure: assume $X$ is approximately log-normally distributed with parameters chosen to match a given mean $m = \\mathbb{E}[X]$ and variance $v = \\operatorname{Var}(X)$. Use quadrature to evaluate $\\mathbb{E}[a(X)]$ and $\\mathbb{E}[X\\,a(X)]$ under this distribution, and enforce the steady-state conditions to solve for $m$ and $v$ self-consistently.\n\nAll computations must be expressed in dimensionless form; no physical units are required. Angles are not involved. Percentages are not involved.\n\nYour program should, for each parameter set in the test suite below, compute the steady-state predictions of $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$ for each closure. For closures that require solving nonlinear equations, use a robust numerical root-finding method. If an intermediate computation yields an invalid state (for example, a negative variance), regularize minimally to proceed while noting such behavior in your reasoning.\n\nProvide results for the following test suite of parameter sets $(k, \\gamma, K, n)$, which probe different regimes:\n- Case $1$ (general, moderate repression): $(k, \\gamma, K, n) = (50, 1, 100, 2)$.\n- Case $2$ (weak feedback limit): $(k, \\gamma, K, n) = (50, 1, 10000, 2)$.\n- Case $3$ (strong feedback, high cooperativity): $(k, \\gamma, K, n) = (50, 1, 10, 4)$.\n- Case $4$ (lower production, faster degradation): $(k, \\gamma, K, n) = (10, 2, 20, 3)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the six floats in the following order: $[\\mathbb{E}[X]_{\\text{MF}}, \\operatorname{Var}(X)_{\\text{MF}}, \\mathbb{E}[X]_{\\text{GA}}, \\operatorname{Var}(X)_{\\text{GA}}, \\mathbb{E}[X]_{\\text{LN}}, \\operatorname{Var}(X)_{\\text{LN}}]$, where the subscripts denote mean-field (MF), Gaussian (GA), and log-normal (LN) closures. Aggregate all cases in sequence into one flat list, for example $[r_{1,1}, r_{1,2}, \\dots, r_{4,6}]$ where $r_{i,j}$ is the $j$-th result for case $i$.",
            "solution": "The problem statement is scientifically grounded, self-contained, and well-posed. It describes a canonical model in stochastic systems biology—a birth-death process with nonlinear feedback—and asks for a standard analysis using moment closure approximations. All required parameters and definitions are provided, and the task is a valid application of mathematical modeling and numerical analysis. Therefore, a full solution is presented.\n\nThe time evolution of the probability distribution $P(x, t)$ for the number of molecules $X$ is governed by the Chemical Master Equation (CME). For a birth-death process, the CME is:\n$$ \\frac{dP(x, t)}{dt} = [a(x-1)P(x-1, t) - a(x)P(x, t)] + [b(x+1)P(x+1, t) - b(x)P(x, t)] $$\nwhere $a(x)$ is the birth propensity and $b(x)$ is the death propensity. The terms are defined to be zero for negative arguments (e.g., $P(-1, t) = 0$). In this problem, the propensities are given by the repressed Hill function for birth and linear degradation for death:\n$$ a(X) = \\frac{k}{1 + \\left(\\frac{X}{K}\\right)^{n}} \\quad \\text{and} \\quad b(X) = \\gamma X $$\n\nWe can derive the time evolution of the expectation of any function $f(X)$, denoted as $\\mathbb{E}[f(X)]$, directly from the CME:\n$$ \\frac{d\\mathbb{E}[f(X)]}{dt} = \\sum_{x=0}^{\\infty} f(x) \\frac{dP(x, t)}{dt} $$\nSubstituting the CME and re-indexing the sums yields the general relation for moment dynamics:\n$$ \\frac{d\\mathbb{E}[f(X)]}{dt} = \\mathbb{E}[a(X) [f(X+1) - f(X)]] + \\mathbb{E}[b(X) [f(X-1) - f(X)]] $$\nTo find the equations for the first two moments, we set $f(X)=X$ and $f(X)=X^2$.\n\nFor the first moment, $f(X) = X$:\n$f(X+1) - f(X) = (X+1) - X = 1$\n$f(X-1) - f(X) = (X-1) - X = -1$\nThe dynamics of the mean $\\mathbb{E}[X]$ are:\n$$ \\frac{d\\mathbb{E}[X]}{dt} = \\mathbb{E}[a(X) \\cdot 1] + \\mathbb{E}[\\gamma X \\cdot (-1)] = \\mathbb{E}[a(X)] - \\gamma \\mathbb{E}[X] $$\n\nFor the second moment, $f(X) = X^2$:\n$f(X+1) - f(X) = (X+1)^2 - X^2 = 2X+1$\n$f(X-1) - f(X) = (X-1)^2 - X^2 = -2X+1$\nThe dynamics of the second moment $\\mathbb{E}[X^2]$ are:\n$$ \\frac{d\\mathbb{E}[X^2]}{dt} = \\mathbb{E}[a(X)(2X+1)] + \\mathbb{E}[\\gamma X (-2X+1)] = 2\\mathbb{E}[Xa(X)] + \\mathbb{E}[a(X)] - 2\\gamma \\mathbb{E}[X^2] + \\gamma \\mathbb{E}[X] $$\n\nAt steady state, the time derivatives are zero. Let $m = \\mathbb{E}[X]$ and $m_2 = \\mathbb{E}[X^2]$. The steady-state conditions are a system of two equations:\n$$ (1) \\quad \\mathbb{E}[a(X)] - \\gamma m = 0 $$\n$$ (2) \\quad 2\\mathbb{E}[Xa(X)] + \\mathbb{E}[a(X)] - 2\\gamma m_2 + \\gamma m = 0 $$\nThe nonlinearity of the propensity function $a(X)$ means that $\\mathbb{E}[a(X)] \\neq a(\\mathbb{E}[X])$. The terms $\\mathbb{E}[a(X)]$ and $\\mathbb{E}[Xa(X)]$ depend on the entire probability distribution of $X$, coupling the equation for the first moment to the second, the second to the third, and so on, creating an infinite hierarchy. Moment closure approximations are used to truncate this hierarchy. We can simplify the second equation by substituting the first into it:\n$$ 2\\mathbb{E}[Xa(X)] + (\\gamma m) - 2\\gamma m_2 + \\gamma m = 0 \\implies \\mathbb{E}[Xa(X)] + \\gamma m - \\gamma m_2 = 0 $$\nThe system to be solved for $m$ and $v = \\operatorname{Var}(X) = m_2 - m^2$ is:\n$$ (A) \\quad \\mathbb{E}[a(X)] = \\gamma m $$\n$$ (B) \\quad \\mathbb{E}[Xa(X)] = \\gamma (m_2 - m) = \\gamma(v + m^2 - m) $$\n\nWe now apply three different closure schemes to approximate the expectation terms.\n\n**1. Mean-Field (MF) Closure**\nThis is the simplest closure, where we approximate the expectation of a nonlinear function by the function of the expectation: $\\mathbb{E}[a(X)] \\approx a(m)$ and $\\mathbb{E}[Xa(X)] \\approx m a(m)$.\nEquation (A) becomes: $a(m) - \\gamma m = 0$, which is a nonlinear equation for the mean $m$:\n$$ \\frac{k}{1 + (m/K)^n} = \\gamma m $$\nEquation (B) becomes: $m a(m) = \\gamma(v + m^2 - m)$. Substituting $a(m) = \\gamma m$ from the first equation gives:\n$$ m(\\gamma m) = \\gamma(v + m^2 - m) \\implies \\gamma m^2 = \\gamma v + \\gamma m^2 - \\gamma m \\implies v = m $$\nThe mean-field closure predicts a Fano factor $v/m = 1$, characteristic of a Poisson process. We first solve numerically for $m$ and then set $v=m$.\n\n**2. Second-Order Normal (Gaussian, GA) Closure**\nThis closure uses a second-order Taylor expansion of a function $f(X)$ around the mean $m=\\mathbb{E}[X]$ and then takes the expectation.\n$f(X) \\approx f(m) + f'(m)(X-m) + \\frac{1}{2}f''(m)(X-m)^2$\n$\\mathbb{E}[f(X)] \\approx f(m) + \\frac{1}{2}f''(m) \\mathbb{E}[(X-m)^2] = f(m) + \\frac{v}{2}f''(m)$, where $v = \\operatorname{Var}(X)$.\nWe apply this to $a(X)$ and $h(X) = Xa(X)$. We need the second derivatives $a''(X)$ and $h''(X)$.\nThe derivatives of $a(X)$ are:\n$a'(X) = -\\frac{nk}{K} \\frac{(X/K)^{n-1}}{(1+(X/K)^n)^2}$\n$a''(X) = \\frac{nk X^{n-2}}{K^n(1+(X/K)^n)^3} \\left[ (n+1)\\left(\\frac{X}{K}\\right)^n - (n-1) \\right]$\nThe second derivative of $h(X)$ is $h''(X) = 2a'(X) + Xa''(X)$.\nThe approximations for the expectations are:\n$\\mathbb{E}[a(X)] \\approx a(m) + \\frac{v}{2}a''(m)$\n$\\mathbb{E}[Xa(X)] \\approx h(m) + \\frac{v}{2}h''(m) = m a(m) + \\frac{v}{2}(2a'(m) + m a''(m)) = m a(m) + v a'(m) + \\frac{vm}{2}a''(m)$\nSubstituting these into the steady-state system (A) and (B) yields a coupled nonlinear system for $(m, v)$:\n$$ (A_{GA}) \\quad a(m) + \\frac{v}{2}a''(m) - \\gamma m = 0 $$\n$$ (B_{GA}) \\quad m a(m) + v a'(m) + \\frac{vm}{2}a''(m) - \\gamma(v + m^2 - m) = 0 $$\nThis system must be solved numerically for $m$ and $v$. A good initial guess can be the result from the mean-field closure. If the solver produces a negative variance $v$, it is regularized to a small positive value, as negative variance is unphysical.\n\n**3. Log-Normal (LN) Closure**\nThis method assumes that $X$ follows a log-normal distribution. A log-normal distribution is defined by two parameters, $\\mu$ and $\\sigma^2$, which are the mean and variance of the underlying normal variable $\\ln(X)$. These parameters can be related to the mean $m = \\mathbb{E}[X]$ and variance $v = \\operatorname{Var}(X)$ of $X$ itself:\n$m = e^{\\mu + \\sigma^2/2}$\n$v = (e^{\\sigma^2}-1)e^{2\\mu+\\sigma^2} = (e^{\\sigma^2}-1)m^2$\nSolving for $\\mu$ and $\\sigma^2$:\n$\\sigma^2 = \\ln(v/m^2 + 1)$\n$\\mu = \\ln(m) - \\sigma^2/2$\nThis requires $m > 0$ and $v \\ge 0$.\nThe expectations $\\mathbb{E}[a(X)]$ and $\\mathbb{E}[Xa(X)]$ are computed by integrating over the log-normal probability density function, which is performed numerically using Gauss-Hermite quadrature.\nThe expectation of a function $f(X)$ is given by:\n$$ \\mathbb{E}[f(X)] = \\int_0^{\\infty} f(x) p_{LN}(x; \\mu, \\sigma^2) dx = \\int_{-\\infty}^{\\infty} f(e^{\\mu + \\sigma z}) \\phi(z) dz $$\nwhere $\\phi(z)$ is the standard normal PDF. This integral is approximated by quadrature:\n$$ \\mathbb{E}[f(X)] \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N_{quad}} w_i f(e^{\\mu + \\sigma \\sqrt{2} u_i}) $$\nwhere $u_i$ and $w_i$ are the nodes and weights from Gauss-Hermite quadrature.\nThe closure involves finding $(m,v)$ that self-consistently solve the system (A) and (B) where the expectations are computed using this quadrature method. This again requires a numerical root-finding algorithm, with positivity constraints on $m$ and $v$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import root, root_scalar\nfrom scipy.special import hermite\nimport math\n\ndef solve():\n    \"\"\"\n    Computes steady-state mean and variance for a stochastic gene expression\n    model using three different moment closure approximations.\n    \"\"\"\n\n    test_cases = [\n        # (k, gamma, K, n)\n        (50, 1, 100, 2),\n        (50, 1, 10000, 2),\n        (50, 1, 10, 4),\n        (10, 2, 20, 3)\n    ]\n\n    all_results = []\n    \n    # Quadrature setup for log-normal closure\n    N_QUAD = 64\n    herm_nodes, herm_weights = np.polynomial.hermite.hermgauss(N_QUAD)\n\n    for params in test_cases:\n        k, gamma, K, n = params\n\n        # 1. Mean-Field (MF) Closure\n        m_mf, v_mf = solve_mf(k, gamma, K, n)\n        all_results.extend([m_mf, v_mf])\n\n        initial_guess = (m_mf, v_mf)\n\n        # 2. Gaussian (GA) Closure\n        m_ga, v_ga = solve_ga(k, gamma, K, n, initial_guess)\n        all_results.extend([m_ga, v_ga])\n\n        # 3. Log-Normal (LN) Closure\n        m_ln, v_ln = solve_ln(k, gamma, K, n, initial_guess, herm_nodes, herm_weights)\n        all_results.extend([m_ln, v_ln])\n\n    print(f\"[{','.join(f'{x:.6f}' for x in all_results)}]\")\n\ndef solve_mf(k, gamma, K, n):\n    \"\"\"\n    Solves for mean and variance using mean-field closure.\n    \"\"\"\n    # Equation to solve: a(m) - gamma*m = 0\n    def mf_residual(m):\n        if m < 0:\n            return k # Large penalty for negative mean\n        return k / (1.0 + (m / K)**n) - gamma * m\n\n    # The mean must be between 0 and k/gamma\n    sol = root_scalar(mf_residual, bracket=[0, k / gamma + 1], method='brentq')\n    \n    if sol.converged:\n        mean = sol.root\n        variance = mean  # For MF closure, v = m\n        return mean, variance\n    else:\n        return np.nan, np.nan\n\ndef solve_ga(k, gamma, K, n, initial_guess):\n    \"\"\"\n    Solves for mean and variance using second-order normal (Gaussian) closure.\n    \"\"\"\n    def a(x):\n        return k / (1.0 + (x / K)**n)\n\n    def a_prime(x):\n        if x < 0: x = 0\n        term_x_k = (x / K)\n        denom = (1.0 + term_x_k**n)**2\n        return -n * k / K * (term_x_k**(n - 1)) / denom if denom > 0 else 0\n\n    def a_double_prime(x):\n        if x <= 0: return 0\n        term_x_k_n = (x/K)**n\n        denom = (1.0 + term_x_k_n)**3\n        term1 = (n + 1) * term_x_k_n\n        term2 = (n - 1)\n        \n        factor = n * k * x**(n - 2) / (K**n)\n        \n        return factor * (term1 - term2) / denom if denom > 0 else 0\n        \n    def ga_residuals(vars):\n        m, v = vars\n        \n        # Regularization: variance must be non-negative\n        if m < 0 or v < 0:\n            return [1e6, 1e6]\n            \n        am = a(m)\n        a1m = a_prime(m)\n        a2m = a_double_prime(m)\n        \n        # This is E[a(X)] using second order approximation\n        E_aX = am + 0.5 * v * a2m\n        \n        # This is E[X*a(X)] using second order approximation\n        h_double_prime_m = 2*a1m + m*a2m\n        E_XaX = m * am + 0.5 * v * h_double_prime_m\n        \n        res1 = E_aX - gamma * m\n        res2 = E_XaX - gamma * (v + m**2 - m)\n        \n        return [res1, res2]\n\n    sol = root(ga_residuals, initial_guess, method='hybr')\n    \n    if sol.success:\n        mean, variance = sol.x\n        # Final regularization if variance is slightly negative due to numerical precision\n        variance = max(0.0, variance)\n        return mean, variance\n    else:\n        return np.nan, np.nan\n\ndef solve_ln(k, gamma, K, n, initial_guess, herm_nodes, herm_weights):\n    \"\"\"\n    Solves for mean and variance using log-normal closure with Gauss-Hermite quadrature.\n    \"\"\"\n    def ln_residuals(vars):\n        m, v = vars\n        \n        # Regularization: mean and variance must be positive.\n        if m <= 1e-9 or v <= 1e-9:\n            return [1e6, 1e6]\n\n        # Map (m, v) to log-normal parameters (mu, sigma)\n        try:\n            sigma2 = math.log(v / m**2 + 1.0)\n            mu = math.log(m) - 0.5 * sigma2\n            sigma = math.sqrt(sigma2)\n        except ValueError:\n            return [1e6, 1e6]  # Invalid parameters\n\n        # Quadrature points for standard normal distribution\n        z_nodes = np.sqrt(2.0) * herm_nodes\n        x_eval = np.exp(mu + sigma * z_nodes)\n\n        a_vals = k / (1.0 + (x_eval / K)**n)\n        xa_vals = x_eval * a_vals\n\n        # Compute expectations via quadrature\n        E_aX = np.sum(herm_weights * a_vals) / np.sqrt(np.pi)\n        E_XaX = np.sum(herm_weights * xa_vals) / np.sqrt(np.pi)\n\n        res1 = E_aX - gamma * m\n        res2 = E_XaX - gamma * (v + m**2 - m)\n\n        return [res1, res2]\n\n    sol = root(ln_residuals, initial_guess, method='hybr')\n    \n    if sol.success:\n        mean, variance = sol.x\n        # Final regularization\n        mean = max(1e-9, mean)\n        variance = max(0.0, variance)\n        return mean, variance\n    else:\n        # Fallback to MF if LN fails to converge\n        return initial_guess\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "After approximating the statistical moments of a system, a crucial question remains: what do these moments reveal about the underlying biological reality? This exercise serves as a critical capstone, demonstrating that a finite set of moments does not uniquely define a probability distribution (). You will construct two distinctly different distributions—one representing a single, homogeneous population and the other a bimodal, switching system—that are indistinguishable based on their first three moments, illustrating the fundamental challenge of statistical non-identifiability and the care required when interpreting experimental data.",
            "id": "3931938",
            "problem": "Consider a stochastic gene expression system modeled at steady state by the Chemical Master Equation (CME), where the observable is the protein copy number $X$. Suppose single-cell measurements provide only the first three moments and reveal a mean $\\mathbb{E}[X]=\\mu=10$, a variance $\\operatorname{Var}(X)=\\sigma^{2}=25$, and a third central moment $\\mathbb{E}[(X-\\mu)^{3}]=0$. In many synthetic biology inference pipelines, moment matching is used to fit models; yet limited moments can be insufficient to identify the underlying distribution.\n\nYou are tasked to demonstrate non-identifiability from moments up to order $k=3$ by constructing two distinct candidate steady-state distributions for $X$ that share the same first three raw moments:\n- Distribution A: a single Normal distribution with mean $\\mu$ and variance $\\sigma^{2}$.\n- Distribution B: a symmetric two-component Normal mixture with equal weights. The two components have means $\\mu+m$ and $\\mu-m$ and equal component variances $s^{2}$.\n\nAssume an independent dynamical measurement constrains the ratio of between-state separation to within-state variability such that $m^{2}/s^{2}=16/9$. Using only first principles (definitions of moments and conditioning), do the following:\n1. Derive the first three raw moments of Distribution A in terms of $\\mu$ and $\\sigma^{2}$.\n2. Derive the first three raw moments of Distribution B in terms of $\\mu$, $m$, and $s^{2}$.\n3. Impose the constraints $\\mathbb{E}[X]=10$, $\\operatorname{Var}(X)=25$, $\\mathbb{E}[(X-\\mu)^{3}]=0$, and $m^{2}/s^{2}=16/9$ to determine explicit values of $m$ and $s^{2}$ for which Distribution B matches Distribution A up to order $k=3$.\n\nExpress your final answer as the pair $(m,s^{2})$ in a single row matrix using standard mathematical notation. No rounding is required and no units are needed. Verify in your reasoning that the two distributions are distinct while sharing identical moment sets up to order $3$.",
            "solution": "The problem statement is evaluated for validity prior to attempting a solution.\n\n### Step 1: Extract Givens\n-   System: A stochastic gene expression system at steady state.\n-   Observable: Protein copy number, $X$.\n-   Provided single-cell measurement moments:\n    -   Mean: $\\mathbb{E}[X] = \\mu = 10$.\n    -   Variance: $\\operatorname{Var}(X) = \\sigma^{2} = 25$.\n    -   Third central moment: $\\mathbb{E}[(X-\\mu)^{3}] = 0$.\n-   Task: Demonstrate non-identifiability from moments up to order $k=3$.\n-   Candidate Distributions to be constructed:\n    -   Distribution A: A single Normal distribution, $X_A \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n    -   Distribution B: A symmetric two-component Normal mixture with equal weights ($1/2$). The components are $\\mathcal{N}(\\mu+m, s^2)$ and $\\mathcal{N}(\\mu-m, s^2)$, with equal variances $s^2$.\n-   Constraint: The ratio of between-state separation to within-state variability is fixed, $m^{2}/s^{2}=16/9$.\n-   Objectives:\n    1.  Derive the first three raw moments of Distribution A in terms of $\\mu$ and $\\sigma^{2}$.\n    2.  Derive the first three raw moments of Distribution B in terms of $\\mu$, $m$, and $s^{2}$.\n    3.  Determine the explicit values of $m$ and $s^{2}$ that make the first three moments of Distribution B match the given data.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It addresses a fundamental concept in statistical modeling—the non-identifiability of a distribution from a finite set of moments—within the valid scientific context of synthetic biology. The use of Normal and mixture-of-Normal distributions as models is a standard practice. The problem is well-posed, providing sufficient information and constraints to determine a unique solution for the parameters $m$ and $s^2$. The language is objective and precise. The data provided ($\\mu=10$, $\\sigma^2=25$, third central moment is $0$) are mutually consistent and plausible. The problem does not violate any fundamental principles of mathematics or science and is directly formalizable and solvable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to find the parameters $m$ and $s^{2}$ for Distribution B such that its first three moments match those of Distribution A, which are defined by the given experimental data.\n\n**1. Derivation of Moments for Distribution A**\nLet the random variable for Distribution A be $X_A \\sim \\mathcal{N}(\\mu, \\sigma^2)$. We need to find its first three raw moments, $\\mathbb{E}[X_A]$, $\\mathbb{E}[X_A^2]$, and $\\mathbb{E}[X_A^3]$.\n\n-   The first raw moment is the mean, which is given by the parameter $\\mu$:\n    $$\\mathbb{E}[X_A] = \\mu$$\n\n-   The second raw moment is related to the variance, $\\operatorname{Var}(X_A) = \\mathbb{E}[X_A^2] - (\\mathbb{E}[X_A])^2 = \\sigma^2$. Rearranging this gives:\n    $$\\mathbb{E}[X_A^2] = \\operatorname{Var}(X_A) + (\\mathbb{E}[X_A])^2 = \\sigma^2 + \\mu^2$$\n\n-   The third raw moment is related to the third central moment, $\\mathbb{E}[(X_A - \\mu)^3]$. For any symmetric distribution, including the Normal distribution, all odd central moments are zero. Thus, $\\mathbb{E}[(X_A - \\mu)^3] = 0$. We can expand this expression:\n    $$\\mathbb{E}[(X_A - \\mu)^3] = \\mathbb{E}[X_A^3 - 3\\mu X_A^2 + 3\\mu^2 X_A - \\mu^3] = 0$$\n    Using the linearity of expectation:\n    $$\\mathbb{E}[X_A^3] - 3\\mu \\mathbb{E}[X_A^2] + 3\\mu^2 \\mathbb{E}[X_A] - \\mu^3 = 0$$\n    Solving for $\\mathbb{E}[X_A^3]$ and substituting the expressions for the lower moments:\n    $$\\mathbb{E}[X_A^3] = 3\\mu \\mathbb{E}[X_A^2] - 3\\mu^2 \\mathbb{E}[X_A] + \\mu^3$$\n    $$\\mathbb{E}[X_A^3] = 3\\mu(\\sigma^2 + \\mu^2) - 3\\mu^2(\\mu) + \\mu^3$$\n    $$\\mathbb{E}[X_A^3] = 3\\mu\\sigma^2 + 3\\mu^3 - 3\\mu^3 + \\mu^3 = \\mu^3 + 3\\mu\\sigma^2$$\n\nSo, for Distribution A, the first three raw moments are: $\\mathbb{E}[X_A] = \\mu$, $\\mathbb{E}[X_A^2] = \\sigma^2 + \\mu^2$, and $\\mathbb{E}[X_A^3] = \\mu^3 + 3\\mu\\sigma^2$.\n\n**2. Derivation of Moments for Distribution B**\nLet the random variable for Distribution B be $X_B$. Its probability density function is a mixture of two Normal distributions:\n$f_{X_B}(x) = \\frac{1}{2}\\mathcal{N}(x; \\mu-m, s^2) + \\frac{1}{2}\\mathcal{N}(x; \\mu+m, s^2)$.\nLet the two component random variables be $Y_1 \\sim \\mathcal{N}(\\mu-m, s^2)$ and $Y_2 \\sim \\mathcal{N}(\\mu+m, s^2)$. The moments of $X_B$ can be calculated using the law of total expectation: $\\mathbb{E}[X_B^k] = \\frac{1}{2}\\mathbb{E}[Y_1^k] + \\frac{1}{2}\\mathbb{E}[Y_2^k]$.\n\n-   The first raw moment is:\n    $$\\mathbb{E}[X_B] = \\frac{1}{2}\\mathbb{E}[Y_1] + \\frac{1}{2}\\mathbb{E}[Y_2] = \\frac{1}{2}(\\mu-m) + \\frac{1}{2}(\\mu+m) = \\frac{\\mu}{2} - \\frac{m}{2} + \\frac{\\mu}{2} + \\frac{m}{2} = \\mu$$\n\n-   The second raw moment is:\n    Using the formula $\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) + (\\mathbb{E}[Y])^2$:\n    $\\mathbb{E}[Y_1^2] = s^2 + (\\mu-m)^2 = s^2 + \\mu^2 - 2\\mu m + m^2$.\n    $\\mathbb{E}[Y_2^2] = s^2 + (\\mu+m)^2 = s^2 + \\mu^2 + 2\\mu m + m^2$.\n    $$\\mathbb{E}[X_B^2] = \\frac{1}{2}(\\mathbb{E}[Y_1^2] + \\mathbb{E}[Y_2^2]) = \\frac{1}{2}((s^2 + \\mu^2 - 2\\mu m + m^2) + (s^2 + \\mu^2 + 2\\mu m + m^2))$$\n    $$\\mathbb{E}[X_B^2] = \\frac{1}{2}(2s^2 + 2\\mu^2 + 2m^2) = \\mu^2 + m^2 + s^2$$\n\n-   The third raw moment is:\n    Using the formula $\\mathbb{E}[Y^3] = \\mu_Y^3 + 3\\mu_Y\\sigma_Y^2$ for a Normal variable $Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)$:\n    $\\mathbb{E}[Y_1^3] = (\\mu-m)^3 + 3(\\mu-m)s^2 = (\\mu^3 - 3\\mu^2 m + 3\\mu m^2 - m^3) + (3\\mu s^2 - 3ms^2)$.\n    $\\mathbb{E}[Y_2^3] = (\\mu+m)^3 + 3(\\mu+m)s^2 = (\\mu^3 + 3\\mu^2 m + 3\\mu m^2 + m^3) + (3\\mu s^2 + 3ms^2)$.\n    $$\\mathbb{E}[X_B^3] = \\frac{1}{2}(\\mathbb{E}[Y_1^3] + \\mathbb{E}[Y_2^3])$$\n    Summing the terms for $\\mathbb{E}[Y_1^3]$ and $\\mathbb{E}[Y_2^3]$, the odd powers of $m$ cancel:\n    $$\\mathbb{E}[Y_1^3] + \\mathbb{E}[Y_2^3] = 2\\mu^3 + 6\\mu m^2 + 6\\mu s^2$$\n    $$\\mathbb{E}[X_B^3] = \\frac{1}{2}(2\\mu^3 + 6\\mu m^2 + 6\\mu s^2) = \\mu^3 + 3\\mu m^2 + 3\\mu s^2 = \\mu^3 + 3\\mu(m^2+s^2)$$\n\nSo, for Distribution B, the first three raw moments are: $\\mathbb{E}[X_B] = \\mu$, $\\mathbb{E}[X_B^2] = \\mu^2 + m^2 + s^2$, and $\\mathbb{E}[X_B^3] = \\mu^3 + 3\\mu(m^2+s^2)$.\n\n**3. Determination of $m$ and $s^2$**\nWe must now impose the given constraints to find $m$ and $s^{2}$. The constraints are that the moments of Distribution B must match the experimental data: $\\mathbb{E}[X] = \\mu = 10$, $\\operatorname{Var}(X) = \\sigma^2 = 25$, and $\\mathbb{E}[(X-\\mu)^3] = 0$.\n\n-   The mean of Distribution B is $\\mathbb{E}[X_B] = \\mu = 10$. This is satisfied by construction.\n\n-   The variance of Distribution B must be equal to $\\sigma^2=25$:\n    $$\\operatorname{Var}(X_B) = \\mathbb{E}[X_B^2] - (\\mathbb{E}[X_B])^2 = (\\mu^2 + m^2 + s^2) - \\mu^2 = m^2 + s^2$$\n    Therefore, we have our first equation:\n    $$m^2 + s^2 = 25$$\n\n-   The third central moment of Distribution B must be $0$. Let's verify this is true. Due to the symmetry of the mixture components around $\\mu$, the resulting distribution is symmetric around $\\mu$. All odd central moments of a symmetric distribution are zero. For completeness, we calculate it explicitly:\n    $$\\mathbb{E}[(X_B - \\mu)^3] = \\mathbb{E}[X_B^3] - 3\\mu \\mathbb{E}[X_B^2] + 3\\mu^2 \\mathbb{E}[X_B] - \\mu^3$$\n    $$= (\\mu^3 + 3\\mu(m^2+s^2)) - 3\\mu(\\mu^2 + m^2 + s^2) + 3\\mu^2(\\mu) - \\mu^3$$\n    $$= (\\mu^3 - 3\\mu^3 + 3\\mu^3 - \\mu^3) + (3\\mu(m^2+s^2) - 3\\mu(m^2+s^2)) = 0$$\n    This condition is automatically satisfied by the symmetric structure of Distribution B and provides no new constraints on $m$ or $s^2$.\n\nWe have one equation, $m^2 + s^2 = 25$, and two unknowns. The problem provides a second constraint derived from independent measurements:\n$$m^2/s^2 = 16/9$$\nThis gives us a system of two equations for $m^2$ and $s^2$. From the second equation, we express $m^2$ in terms of $s^2$:\n$$m^2 = \\frac{16}{9}s^2$$\nSubstituting this into the first equation:\n$$\\frac{16}{9}s^2 + s^2 = 25$$\n$$\\left(\\frac{16}{9} + 1\\right)s^2 = 25$$\n$$\\left(\\frac{16+9}{9}\\right)s^2 = 25$$\n$$\\frac{25}{9}s^2 = 25$$\nThis simplifies to:\n$$s^2 = 9$$\nNow we can find $m^2$ by substituting $s^2=9$ back into the expression for $m^2$:\n$$m^2 = \\frac{16}{9}(9) = 16$$\nSince $m$ represents half the separation between the distribution means, we take the positive root, $m=4$.\n\nThe two distributions are indeed distinct. Distribution A is a unimodal Normal distribution $\\mathcal{N}(10, 25)$. Distribution B is a bimodal distribution with density $\\frac{1}{2}\\mathcal{N}(x; 6, 9) + \\frac{1}{2}\\mathcal{N}(x; 14, 9)$, with peaks at $x=6$ and $x=14$. Despite their different shapes, they have identical first three moments. This demonstrates the non-identifiability of the underlying probability distribution from a finite set of moments.\n\nThe required pair is $(m, s^2)$. With $m=4$ and $s^2=9$, the pair is $(4, 9)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 4 & 9 \\end{pmatrix}}\n$$"
        }
    ]
}