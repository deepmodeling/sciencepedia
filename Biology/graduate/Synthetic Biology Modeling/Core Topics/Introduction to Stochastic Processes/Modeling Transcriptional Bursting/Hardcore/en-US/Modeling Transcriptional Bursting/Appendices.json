{
    "hands_on_practices": [
        {
            "introduction": "Understanding the dynamics of a system is as important as knowing its steady state. This exercise provides fundamental practice in deriving the time-evolution of system properties directly from the Chemical Master Equation (CME). By solving for the mean mRNA copy number, $\\langle m \\rangle(t)$, in the two-state model , you will see precisely how the promoter switching rates and mRNA turnover govern the system's relaxation to its equilibrium.",
            "id": "3921300",
            "problem": "Consider a minimal two-state promoter model for transcriptional bursting in a single gene. The promoter state $g(t) \\in \\{0,1\\}$ switches between an inactive state $g=0$ and an active state $g=1$ according to a continuous-time Markov process with transition rates $k_{on}$ (from $0$ to $1$) and $k_{off}$ (from $1$ to $0$). When the promoter is active ($g=1$), messenger ribonucleic acid (mRNA) molecules are synthesized as a Poisson process at rate $s$, and when the promoter is inactive ($g=0$), synthesis is absent. Each mRNA molecule degrades independently at rate $\\gamma_m$. Let $m(t)$ denote the mRNA copy number at time $t$, and let $\\langle m \\rangle(t)$ denote its expectation.\n\nStarting from the Chemical Master Equation (CME), derive the closed system of equations governing the time evolution of the mean mRNA copy number $\\langle m \\rangle(t)$ and the promoter active probability $P_{on}(t) = \\Pr[g(t)=1]$. Then, solve this system to obtain an explicit closed-form expression for $\\langle m \\rangle(t)$ for arbitrary initial conditions $m(0)=m_0$ and $P_{on}(0)=p_0$, assuming $\\gamma_m \\neq k_{on}+k_{off}$.\n\nYour final answer must be a single closed-form analytic expression for $\\langle m \\rangle(t)$ in terms of $t$, $m_0$, $p_0$, $k_{on}$, $k_{off}$, $s$, and $\\gamma_m$. Do not round; provide the exact expression. No units should be included in the final expression.",
            "solution": "The user has provided a valid problem statement.\n\nThe problem asks for the derivation of the time evolution equation for the mean mRNA copy number, $\\langle m \\rangle(t)$, and the promoter active probability, $P_{on}(t)$, and its solution for given initial conditions. The derivation must start from the Chemical Master Equation (CME).\n\nLet $P(m, g, t)$ be the probability of having $m$ mRNA molecules with the promoter in state $g \\in \\{0, 1\\}$ at time $t$. The set of reactions is:\n$1$. Promoter activation: $G_0 \\xrightarrow{k_{on}} G_1$\n$2$. Promoter deactivation: $G_1 \\xrightarrow{k_{off}} G_0$\n$3$. mRNA synthesis (transcription): $G_1 \\xrightarrow{s} G_1 + M$\n$4$. mRNA degradation: $M \\xrightarrow{\\gamma_m} \\emptyset$\n\nThe Chemical Master Equation describes the time evolution of the probability distribution $P(m, g, t)$. For the two promoter states, the CMEs are:\nFor the inactive state ($g=0$):\n$$ \\frac{dP(m, 0, t)}{dt} = \\gamma_m(m+1)P(m+1, 0, t) - \\gamma_m m P(m, 0, t) - k_{on} P(m, 0, t) + k_{off} P(m, 1, t) $$\nFor the active state ($g=1$):\n$$ \\frac{dP(m, 1, t)}{dt} = \\gamma_m(m+1)P(m+1, 1, t) - \\gamma_m m P(m, 1, t) + s P(m-1, 1, t) - s P(m, 1, t) - k_{off} P(m, 1, t) + k_{on} P(m, 0, t) $$\nwhere we assume $P(-1, 1, t) = 0$.\n\nFirst, we derive the equation for the promoter active probability, $P_{on}(t) = \\Pr[g(t)=1] = \\sum_{m=0}^{\\infty} P(m, 1, t)$. We sum the second CME over all possible mRNA copy numbers $m$:\n$$ \\frac{d}{dt} \\sum_{m=0}^{\\infty} P(m, 1, t) = \\sum_{m=0}^{\\infty} \\left[ \\gamma_m(m+1)P(m+1, 1, t) - \\gamma_m m P(m, 1, t) \\right] + \\sum_{m=0}^{\\infty} \\left[ s P(m-1, 1, t) - s P(m, 1, t) \\right] - k_{off} \\sum_{m=0}^{\\infty} P(m, 1, t) + k_{on} \\sum_{m=0}^{\\infty} P(m, 0, t) $$\nThe first two sums are telescoping series that evaluate to $0$. Let $P_{off}(t) = \\sum_{m=0}^{\\infty} P(m, 0, t)$. Since $P_{on}(t) + P_{off}(t) = 1$, we have $P_{off}(t) = 1 - P_{on}(t)$. The equation simplifies to:\n$$ \\frac{d P_{on}(t)}{dt} = -k_{off} P_{on}(t) + k_{on} P_{off}(t) = -k_{off} P_{on}(t) + k_{on} (1 - P_{on}(t)) $$\n$$ \\frac{d P_{on}(t)}{dt} = k_{on} - (k_{on} + k_{off}) P_{on}(t) $$\nThis is the first required ordinary differential equation (ODE).\n\nNext, we derive the equation for the mean mRNA copy number, $\\langle m \\rangle(t) = \\sum_{g \\in \\{0,1\\}} \\sum_{m=0}^{\\infty} m P(m, g, t)$. The time derivative is $\\frac{d\\langle m \\rangle}{dt} = \\sum_{g,m} m \\frac{dP(m, g, t)}{dt}$. We calculate this by multiplying each CME by $m$ and summing over $m$ and $g$.\n$$ \\frac{d\\langle m \\rangle}{dt} = \\sum_{m=0}^{\\infty} m \\frac{dP(m, 0, t)}{dt} + \\sum_{m=0}^{\\infty} m \\frac{dP(m, 1, t)}{dt} $$\nUsing the property $\\sum_{m} m f(m+1)P(m+1) = \\sum_k (k-1)f(k)P(k)$ and $\\sum_m m f(m-1)P(m-1) = \\sum_k (k+1)f(k)P(k)$, we find:\n- For degradation: $\\sum_{m=0}^{\\infty} m [\\gamma_m(m+1)P(m+1,g) - \\gamma_m m P(m,g)] = -\\gamma_m \\sum_{m=0}^{\\infty} m P(m,g) = -\\gamma_m \\langle m \\rangle_g(t)$, where $\\langle m \\rangle_g(t) = \\sum_m m P(m,g,t)$.\n- For synthesis: $\\sum_{m=0}^{\\infty} m [sP(m-1,1) - sP(m,1)] = s \\sum_m (m+1)P(m,1) - s\\sum_m m P(m,1) = s \\sum_m P(m,1) = sP_{on}(t)$.\n- For switching: The sum of $m(k_{off}P(m,1) - k_{on}P(m,0))$ and $m(k_{on}P(m,0) - k_{off}P(m,1))$ over $m$ cancels out when adding the two state contributions.\n\nCombining these results for both promoter states:\n$$ \\frac{d\\langle m \\rangle}{dt} = (-\\gamma_m\\langle m \\rangle_0 - k_{on}\\langle m \\rangle_0 + k_{off}\\langle m \\rangle_1) + (sP_{on}(t) - \\gamma_m\\langle m \\rangle_1 - k_{off}\\langle m \\rangle_1 + k_{on}\\langle m \\rangle_0) $$\n$$ \\frac{d\\langle m \\rangle(t)}{dt} = s P_{on}(t) - \\gamma_m (\\langle m \\rangle_0(t) + \\langle m \\rangle_1(t)) $$\n$$ \\frac{d \\langle m \\rangle(t)}{dt} = s P_{on}(t) - \\gamma_m \\langle m \\rangle(t) $$\nThis is the second required ODE.\n\nWe now solve the system of ODEs:\n$1$. $\\frac{d P_{on}(t)}{dt} = k_{on} - (k_{on} + k_{off}) P_{on}(t)$\n$2$. $\\frac{d \\langle m \\rangle(t)}{dt} = s P_{on}(t) - \\gamma_m \\langle m \\rangle(t)$\n\nFirst, solve for $P_{on}(t)$. This is a first-order linear ODE with constant coefficients. The solution with initial condition $P_{on}(0) = p_0$ is:\n$$ P_{on}(t) = P_{on,ss} + (p_0 - P_{on,ss}) \\exp(-(k_{on}+k_{off})t) $$\nwhere $P_{on,ss} = \\frac{k_{on}}{k_{on}+k_{off}}$ is the steady-state probability. So,\n$$ P_{on}(t) = \\frac{k_{on}}{k_{on}+k_{off}} + \\left(p_0 - \\frac{k_{on}}{k_{on}+k_{off}}\\right) \\exp(-(k_{on}+k_{off})t) $$\nNext, substitute this into the equation for $\\langle m \\rangle(t)$:\n$$ \\frac{d \\langle m \\rangle}{dt} + \\gamma_m \\langle m \\rangle = s \\left[ \\frac{k_{on}}{k_{on}+k_{off}} + \\left(p_0 - \\frac{k_{on}}{k_{on}+k_{off}}\\right) \\exp(-(k_{on}+k_{off})t) \\right] $$\nThis is a first-order linear ODE of the form $\\frac{dy}{dt} + ay = f(t)$. We solve it using an integrating factor $I(t) = \\exp(\\gamma_m t)$.\n$$ \\langle m \\rangle(t) = \\exp(-\\gamma_m t) \\left( \\int_0^t \\exp(\\gamma_m \\tau) s P_{on}(\\tau) d\\tau + \\langle m \\rangle(0) \\right) $$\nWith $\\langle m \\rangle(0) = m_0$:\n$$ \\langle m \\rangle(t) = m_0 \\exp(-\\gamma_m t) + s \\exp(-\\gamma_m t) \\int_0^t \\exp(\\gamma_m \\tau) \\left[ \\frac{k_{on}}{k_{on}+k_{off}} + \\left(p_0 - \\frac{k_{on}}{k_{on}+k_{off}}\\right) \\exp(-(k_{on}+k_{off})\\tau) \\right] d\\tau $$\nWe evaluate the integral:\n$$ \\int_0^t \\left[ \\frac{k_{on}}{k_{on}+k_{off}} \\exp(\\gamma_m \\tau) + \\left(p_0 - \\frac{k_{on}}{k_{on}+k_{off}}\\right) \\exp((\\gamma_m - k_{on} - k_{off})\\tau) \\right] d\\tau $$\n$$ = \\frac{k_{on}}{k_{on}+k_{off}} \\left[ \\frac{\\exp(\\gamma_m \\tau)}{\\gamma_m} \\right]_0^t + \\left(p_0 - \\frac{k_{on}}{k_{on}+k_{off}}\\right) \\left[ \\frac{\\exp((\\gamma_m - k_{on} - k_{off})\\tau)}{\\gamma_m - k_{on} - k_{off}} \\right]_0^t $$\nThe evaluation relies on the condition $\\gamma_m \\neq k_{on}+k_{off}$.\n$$ = \\frac{k_{on}}{\\gamma_m(k_{on}+k_{off})} (\\exp(\\gamma_m t) - 1) + \\frac{p_0 - \\frac{k_{on}}{k_{on}+k_{off}}}{\\gamma_m - k_{on} - k_{off}} (\\exp((\\gamma_m - k_{on} - k_{off})t) - 1) $$\nSubstituting this back into the expression for $\\langle m \\rangle(t)$:\n$$ \\langle m \\rangle(t) = m_0 \\exp(-\\gamma_m t) + s \\exp(-\\gamma_m t) \\left[ \\frac{k_{on}}{\\gamma_m(k_{on}+k_{off})} (\\exp(\\gamma_m t) - 1) + \\frac{p_0 - \\frac{k_{on}}{k_{on}+k_{off}}}{\\gamma_m - k_{on} - k_{off}} (\\exp((\\gamma_m - k_{on} - k_{off})t) - 1) \\right] $$\nDistributing the $s \\exp(-\\gamma_m t)$ term:\n$$ \\langle m \\rangle(t) = m_0 \\exp(-\\gamma_m t) + \\frac{s k_{on}}{\\gamma_m(k_{on}+k_{off})} (1 - \\exp(-\\gamma_m t)) + \\frac{s(p_0 - \\frac{k_{on}}{k_{on}+k_{off}})}{\\gamma_m - k_{on} - k_{off}} (\\exp(-(k_{on}+k_{off})t) - \\exp(-\\gamma_m t)) $$\nThis is the final closed-form expression for the mean mRNA copy number $\\langle m \\rangle(t)$.",
            "answer": "$$\\boxed{m_0 \\exp(-\\gamma_m t) + \\frac{s k_{on}}{\\gamma_m(k_{on}+k_{off})}(1-\\exp(-\\gamma_m t)) + \\frac{s \\left(p_0 - \\frac{k_{on}}{k_{on}+k_{off}}\\right)}{\\gamma_m - k_{on} - k_{off}} (\\exp(-(k_{on}+k_{off})t) - \\exp(-\\gamma_m t))}$$"
        },
        {
            "introduction": "Gene expression does not stop at transcription; messenger RNA is translated into protein, which is often the final functional product. This practice extends the telegraph model to a two-stage process, allowing you to investigate how transcriptional noise propagates to the protein level . Deriving and comparing the Fano factors for both mRNA and protein will reveal the crucial role that protein lifetime plays in filtering or transmitting upstream fluctuations.",
            "id": "3921303",
            "problem": "A two-state promoter model of transcriptional bursting is considered. The promoter state is represented by a binary variable $S \\in \\{0,1\\}$, where $S=1$ denotes the transcriptionally active state (ON) and $S=0$ denotes the inactive state (OFF). The promoter switches from OFF to ON with rate $k_{\\text{on}}$ and from ON to OFF with rate $k_{\\text{off}}$. When the promoter is ON, messenger ribonucleic acid (mRNA) molecules are transcribed with rate $s$. Each mRNA molecule degrades with rate $\\gamma_{m}$. Proteins are synthesized by translation from mRNA with rate $k_{\\text{tl}}$ per mRNA and degrade with rate $\\gamma_{p}$. Let $M$ denote the mRNA copy number and $P$ denote the protein copy number.\n\nStarting from the Chemical Master Equation (CME) and the birth-death process definitions, derive the stationary first and second moments required to compute the Fano factor of mRNA, defined as $F_{M} = \\operatorname{Var}(M)/\\mathbb{E}[M]$, and the Fano factor of protein, defined as $F_{P} = \\operatorname{Var}(P)/\\mathbb{E}[P]$. Use the fact that the CME implies linear evolution equations for the first and second moments of $S$, $M$, $P$, and their products under mass-action kinetics. Do not use any shortcut results; derive the necessary moment equations from first principles.\n\nImplement a program that:\n- Computes the stationary Fano factors $F_{M}$ and $F_{P}$ via the derived moment equations.\n- Outputs, for each test case, the ratio $R = F_{P}/F_{M}$ as a float, rounded to six decimal places.\n\nAll parameters are positive real numbers and represent rates per unit time; the outputs are dimensionless ratios and require no physical unit specification. Use the following test suite, each test case given as $(k_{\\text{on}}, k_{\\text{off}}, s, \\gamma_{m}, k_{\\text{tl}}, \\gamma_{p})$:\n1. $(1.0, 1.0, 10.0, 1.0, 5.0, 0.5)$, a general \"happy path\" case with moderate bursting and protein lifetime.\n2. $(100.0, 100.0, 10.0, 1.0, 5.0, 0.5)$, a fast-switching boundary case where promoter fluctuations average out rapidly.\n3. $(0.1, 0.1, 10.0, 1.0, 5.0, 0.05)$, a slow-switching case with long protein lifetime that filters mRNA bursts strongly.\n4. $(0.1, 0.1, 10.0, 1.0, 5.0, 5.0)$, a slow-switching case with short protein lifetime that tracks mRNA bursts closely.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and rounded to six decimal places, for example, $[r_{1},r_{2},r_{3},r_{4}]$ where each $r_{i}$ is the ratio $R$ for the $i$-th test case.",
            "solution": "The problem requires the derivation of the stationary Fano factors for messenger RNA (mRNA) and protein copy numbers, $F_{M}$ and $F_{P}$ respectively, from the Chemical Master Equation (CME) for a two-state model of gene expression. Subsequently, the ratio $R = F_{P}/F_{M}$ must be computed for several parameter sets.\n\nThe system is described by the state vector $(S, M, P)$, where $S \\in \\{0, 1\\}$ is the promoter state, $M$ is the number of mRNA molecules, and $P$ is the number of protein molecules. The reactions and their mass-action propensities are:\n1.  Promoter activation: OFF $\\xrightarrow{k_{\\text{on}}}$ ON. Propensity $a_1 = k_{\\text{on}}(1-S)$. Stoichiometric change: $\\Delta S = +1$.\n2.  Promoter inactivation: ON $\\xrightarrow{k_{\\text{off}}}$ OFF. Propensity $a_2 = k_{\\text{off}}S$. Stoichiometric change: $\\Delta S = -1$.\n3.  Transcription: ON $\\xrightarrow{s}$ ON + mRNA. Propensity $a_3 = sS$. Stoichiometric change: $\\Delta M = +1$.\n4.  mRNA degradation: mRNA $\\xrightarrow{\\gamma_{m}}$ $\\emptyset$. Propensity $a_4 = \\gamma_{m}M$. Stoichiometric change: $\\Delta M = -1$.\n5.  Translation: mRNA $\\xrightarrow{k_{\\text{tl}}}$ mRNA + Protein. Propensity $a_5 = k_{\\text{tl}}M$. Stoichiometric change: $\\Delta P = +1$.\n6.  Protein degradation: Protein $\\xrightarrow{\\gamma_{p}}$ $\\emptyset$. Propensity $a_6 = \\gamma_{p}P$. Stoichiometric change: $\\Delta P = -1$.\n\nThe time evolution of the expectation of any function $f(S, M, P)$ is given by the master equation formalism:\n$$\n\\frac{d\\mathbb{E}[f]}{dt} = \\sum_{j} \\mathbb{E}[ a_j \\cdot (f_{\\text{after}} - f_{\\text{before}}) ]\n$$\nwhere the sum is over all reactions. For linear propensities, this simplifies to linear ordinary differential equations for the moments. We seek the stationary solution where all time derivatives are zero.\n\n**1. First Moments at Steady State**\n\nFor any variable $X$, its first moment dynamics are $\\frac{d\\mathbb{E}[X]}{dt} = \\mathbb{E}[\\text{production rate}] - \\mathbb{E}[\\text{destruction rate}]$.\n- For $\\mathbb{E}[S]$: $\\frac{d\\mathbb{E}[S]}{dt} = \\mathbb{E}[k_{\\text{on}}(1-S) - k_{\\text{off}}S] = k_{\\text{on}}(1-\\mathbb{E}[S]) - k_{\\text{off}}\\mathbb{E}[S]$.\nSetting to zero: $k_{\\text{on}} = (k_{\\text{on}} + k_{\\text{off}})\\mathbb{E}[S]$. Let $k_s = k_{\\text{on}} + k_{\\text{off}}$.\n$$ \\mathbb{E}[S] = \\frac{k_{\\text{on}}}{k_{\\text{on}} + k_{\\text{off}}} = \\frac{k_{\\text{on}}}{k_s} $$\nThis is the stationary probability of the promoter being ON, denoted $p_{\\text{on}}$.\n\n- For $\\mathbb{E}[M]$: $\\frac{d\\mathbb{E}[M]}{dt} = \\mathbb{E}[sS - \\gamma_m M] = s\\mathbb{E}[S] - \\gamma_m\\mathbb{E}[M]$.\nSetting to zero:\n$$ \\mathbb{E}[M] = \\frac{s}{\\gamma_m}\\mathbb{E}[S] = \\frac{s}{\\gamma_m}\\frac{k_{\\text{on}}}{k_s} $$\n\n- For $\\mathbb{E}[P]$: $\\frac{d\\mathbb{E}[P]}{dt} = \\mathbb{E}[k_{\\text{tl}}M - \\gamma_p P] = k_{\\text{tl}}\\mathbb{E}[M] - \\gamma_p\\mathbb{E}[P]$.\nSetting to zero:\n$$ \\mathbb{E}[P] = \\frac{k_{\\text{tl}}}{\\gamma_p}\\mathbb{E}[M] = \\frac{k_{\\text{tl}}s}{\\gamma_p\\gamma_m}\\frac{k_{\\text{on}}}{k_s} $$\n\n**2. Covariance and Variance Dynamics**\nWe derive the dynamics for the second-order moments. For any two variables $X$ and $Y$, the covariance dynamics are given by:\n$$ \\frac{d\\operatorname{Cov}(X,Y)}{dt} = \\mathbb{E}[\\text{changes to } XY] - \\frac{d(\\mathbb{E}[X]\\mathbb{E}[Y])}{dt} $$\nThe fluctuation-dissipation theorem for this system gives a set of linear equations for the covariances at steady state.\n\n- For $\\operatorname{Cov}(S, M)$: The dynamics are $\\frac{d\\operatorname{Cov}(S,M)}{dt} = s\\operatorname{Var}(S) - (k_s + \\gamma_m)\\operatorname{Cov}(S,M)$.\nAt steady state, $(k_s + \\gamma_m)\\operatorname{Cov}(S,M) = s\\operatorname{Var}(S)$.\nSince $S$ is a binary variable, $S^2=S$, so $\\operatorname{Var}(S) = \\mathbb{E}[S^2]-\\mathbb{E}[S]^2 = \\mathbb{E}[S]-\\mathbb{E}[S]^2 = \\mathbb{E}[S](1-\\mathbb{E}[S]) = p_{\\text{on}}(1-p_{\\text{on}})$.\n$\\mathbb{E}[S](1-\\mathbb{E}[S]) = \\frac{k_{\\text{on}}}{k_s}(1 - \\frac{k_{\\text{on}}}{k_s}) = \\frac{k_{\\text{on}}k_{\\text{off}}}{k_s^2}$.\n$$ \\operatorname{Cov}(S,M) = \\frac{s}{k_s + \\gamma_m}\\operatorname{Var}(S) = \\frac{sk_{\\text{on}}k_{\\text{off}}}{k_s^2(k_s + \\gamma_m)} $$\n\n- For $\\operatorname{Var}(M)$: The dynamics are $\\frac{d\\operatorname{Var}(M)}{dt} = 2s\\operatorname{Cov}(S,M) - 2\\gamma_m \\operatorname{Var}(M) + s\\mathbb{E}[S] + \\gamma_m \\mathbb{E}[M]$.\nAt steady state, and using $s\\mathbb{E}[S]=\\gamma_m \\mathbb{E}[M]$, we get $2\\gamma_m \\operatorname{Var}(M) = 2s\\operatorname{Cov}(S,M) + 2\\gamma_m \\mathbb{E}[M]$.\n$$ \\operatorname{Var}(M) = \\frac{s}{\\gamma_m}\\operatorname{Cov}(S,M) + \\mathbb{E}[M] $$\n\n**3. mRNA Fano Factor ($F_M$)**\nThe Fano factor is $F_M = \\operatorname{Var}(M) / \\mathbb{E}[M]$.\n$$ F_M = \\frac{1}{\\mathbb{E}[M]} \\left(\\frac{s}{\\gamma_m}\\operatorname{Cov}(S,M) + \\mathbb{E}[M]\\right) = 1 + \\frac{s}{\\gamma_m \\mathbb{E}[M]}\\operatorname{Cov}(S,M) $$\nSubstituting the expressions for $\\mathbb{E}[M]$ and $\\operatorname{Cov}(S,M)$:\n$$ F_M = 1 + \\frac{s}{\\gamma_m \\frac{sk_{\\text{on}}}{\\gamma_m k_s}} \\frac{sk_{\\text{on}}k_{\\text{off}}}{k_s^2(k_s + \\gamma_m)} = 1 + \\frac{k_s}{k_{\\text{on}}} \\frac{sk_{\\text{on}}k_{\\text{off}}}{k_s^2(k_s + \\gamma_m)} $$\n$$ F_M = 1 + \\frac{sk_{\\text{off}}}{k_s(k_s + \\gamma_m)} = 1 + \\frac{sk_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})(k_{\\text{on}}+k_{\\text{off}}+\\gamma_m)} $$\n\n**4. Protein Variances and Covariances**\nWe follow the same procedure for protein-related moments.\n- For $\\operatorname{Cov}(S, P)$: The dynamics are $\\frac{d\\operatorname{Cov}(S,P)}{dt} = k_{\\text{tl}}\\operatorname{Cov}(S,M) - (k_s + \\gamma_p)\\operatorname{Cov}(S,P)$.\nAt steady state:\n$$ \\operatorname{Cov}(S,P) = \\frac{k_{\\text{tl}}}{k_s + \\gamma_p}\\operatorname{Cov}(S,M) $$\n- For $\\operatorname{Cov}(M, P)$: The dynamics are $\\frac{d\\operatorname{Cov}(M,P)}{dt} = s\\operatorname{Cov}(S,P) + k_{\\text{tl}}\\operatorname{Var}(M) - (\\gamma_m + \\gamma_p)\\operatorname{Cov}(M,P)$.\nAt steady state:\n$$ \\operatorname{Cov}(M,P) = \\frac{k_{\\text{tl}}\\operatorname{Var}(M) + s\\operatorname{Cov}(S,P)}{\\gamma_m + \\gamma_p} $$\n- For $\\operatorname{Var}(P)$: The dynamics are $\\frac{d\\operatorname{Var}(P)}{dt} = 2k_{\\text{tl}}\\operatorname{Cov}(M,P) - 2\\gamma_p \\operatorname{Var}(P) + k_{\\text{tl}}\\mathbb{E}[M] + \\gamma_p \\mathbb{E}[P]$.\nAt steady state, and using $k_{\\text{tl}}\\mathbb{E}[M]=\\gamma_p \\mathbb{E}[P]$, we get $2\\gamma_p\\operatorname{Var}(P) = 2k_{\\text{tl}}\\operatorname{Cov}(M,P) + 2\\gamma_p\\mathbb{E}[P]$.\n$$ \\operatorname{Var}(P) = \\frac{k_{\\text{tl}}}{\\gamma_p}\\operatorname{Cov}(M,P) + \\mathbb{E}[P] $$\n\n**5. Protein Fano Factor ($F_P$)**\nThe Fano factor is $F_P = \\operatorname{Var}(P) / \\mathbb{E}[P]$.\n$$ F_P = 1 + \\frac{k_{\\text{tl}}}{\\gamma_p \\mathbb{E}[P]}\\operatorname{Cov}(M,P) = 1 + \\frac{1}{\\mathbb{E}[M]}\\operatorname{Cov}(M,P) $$\nSubstitute the expression for $\\operatorname{Cov}(M,P)$:\n$$ F_P = 1 + \\frac{1}{\\mathbb{E}[M]} \\frac{k_{\\text{tl}}\\operatorname{Var}(M) + s\\operatorname{Cov}(S,P)}{\\gamma_m + \\gamma_p} = 1 + \\frac{k_{\\text{tl}}}{\\gamma_m+\\gamma_p}\\frac{\\operatorname{Var}(M)}{\\mathbb{E}[M]} + \\frac{s}{\\mathbb{E}[M](\\gamma_m+\\gamma_p)}\\operatorname{Cov}(S,P) $$\n$$ F_P = 1 + \\frac{k_{\\text{tl}}}{\\gamma_m + \\gamma_p}F_M + \\frac{s}{\\mathbb{E}[M](\\gamma_m + \\gamma_p)} \\frac{k_{\\text{tl}}}{k_s + \\gamma_p}\\operatorname{Cov}(S,M) $$\nWe know that $(F_M - 1) = \\frac{s}{\\gamma_m \\mathbb{E}[M]}\\operatorname{Cov}(S,M)$, which means $\\frac{s\\operatorname{Cov}(S,M)}{\\mathbb{E}[M]} = \\gamma_m(F_M - 1)$.\nSubstituting this into the expression for $F_P$:\n$$ F_P = 1 + \\frac{k_{\\text{tl}}}{\\gamma_m + \\gamma_p}F_M + \\frac{k_{\\text{tl}}}{(\\gamma_m + \\gamma_p)(k_s + \\gamma_p)} \\gamma_m (F_M-1) $$\nThis final expression relates $F_P$ to $F_M$ and the system parameters, providing a direct path for calculation.\n\nThe computational procedure is as follows:\n1. Given a set of parameters $(k_{\\text{on}}, k_{\\text{off}}, s, \\gamma_{m}, k_{\\text{tl}}, \\gamma_{p})$.\n2. Calculate $k_s = k_{\\text{on}} + k_{\\text{off}}$.\n3. Compute $F_M = 1 + \\frac{sk_{\\text{off}}}{k_s(k_s + \\gamma_m)}$.\n4. Compute $F_P = 1 + \\frac{k_{tl}}{\\gamma_m + \\gamma_p}F_M + \\frac{k_{tl} \\gamma_m}{(\\gamma_m + \\gamma_p)(k_s + \\gamma_p)}(F_M - 1)$.\n5. Compute the ratio $R = F_P / F_M$.\n6. Repeat for all test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of protein to mRNA Fano factors for transcriptional bursting.\n\n    The function implements analytical solutions for the stationary Fano factors\n    of mRNA (F_M) and protein (F_P) derived from the Chemical Master Equation\n    for a two-state model of gene expression.\n    \"\"\"\n\n    test_cases = [\n        # (k_on, k_off, s, gamma_m, k_tl, gamma_p)\n        (1.0, 1.0, 10.0, 1.0, 5.0, 0.5),\n        (100.0, 100.0, 10.0, 1.0, 5.0, 0.5),\n        (0.1, 0.1, 10.0, 1.0, 5.0, 0.05),\n        (0.1, 0.1, 10.0, 1.0, 5.0, 5.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        k_on, k_off, s, gamma_m, k_tl, gamma_p = case\n\n        # Total promoter switching rate\n        k_s = k_on + k_off\n\n        # mRNA Fano Factor (F_M)\n        # Formula: F_M = 1 + (s * k_off) / (k_s * (k_s + gamma_m))\n        f_m = 1.0 + (s * k_off) / (k_s * (k_s + gamma_m))\n\n        # Protein Fano Factor (F_P)\n        # Formula relates F_P to F_M\n        # F_P = 1 + (k_tl / (gamma_m + gamma_p)) * F_M \n        #         + (k_tl * gamma_m / ((gamma_m + gamma_p) * (k_s + gamma_p))) * (F_M - 1)\n        term1 = k_tl / (gamma_m + gamma_p)\n        term2 = (k_tl * gamma_m) / ((gamma_m + gamma_p) * (k_s + gamma_p))\n        \n        f_p = 1.0 + term1 * f_m + term2 * (f_m - 1.0)\n        \n        # Ratio R = F_P / F_M\n        ratio = f_p / f_m\n        \n        results.append(round(ratio, 6))\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Theoretical models are most powerful when they can be connected to experimental data. This exercise tackles the crucial task of parameter inference, where we estimate a model's parameters from single-cell measurements such as those from smFISH . You will implement a numerically robust method to calculate the likelihood of observed mRNA counts using a Beta-Poisson mixture model, a common and effective approximation for bursty gene expression data.",
            "id": "3921221",
            "problem": "Consider single-molecule Fluorescence In Situ Hybridization (smFISH) counts of messenger RNA (mRNA) per cell in a synthetic biology experiment where transcription exhibits bursting due to stochastic promoter activity. Assume that in a finite observation window the transcriptional activity can be approximated as follows: each cell has an effective promoter occupancy fraction $p \\in [0,1]$ describing the proportion of time the promoter is in the transcriptionally active state, and conditional on $p$ the observed mRNA count $n$ is generated by a Poisson process with rate $\\lambda(p) = \\lambda_{\\max} \\, p$, where $\\lambda_{\\max} > 0$ is the mean count expected for a promoter that is fully active over the entire window. To capture cell-to-cell heterogeneity in promoter occupancy, assume $p$ is distributed according to a Beta distribution with parameters $\\alpha > 0$ and $\\beta > 0$, that is $p \\sim \\mathrm{Beta}(\\alpha,\\beta)$.\n\nStarting from the definitions of the Poisson distribution and the Beta distribution, derive the marginal likelihood (probability mass function) for the observed smFISH count $n$ under this beta-Poisson mixture approximation:\n$$\n\\mathbb{P}(N=n \\mid \\alpha,\\beta,\\lambda_{\\max}) \n= \\int_{0}^{1} \\mathbb{P}(N=n \\mid \\lambda_{\\max} p) \\, f_{\\mathrm{Beta}}(p \\mid \\alpha,\\beta) \\, \\mathrm{d}p,\n$$\nwhere $\\mathbb{P}(N=n \\mid \\lambda)$ is the Poisson probability mass function and $f_{\\mathrm{Beta}}(p \\mid \\alpha,\\beta)$ is the Beta probability density function. From this, derive the log-likelihood for an independent dataset of counts $\\{n_i\\}_{i=1}^{M}$ as a function of $(\\alpha,\\beta,\\lambda_{\\max})$.\n\nBecause this integral generally lacks a closed form, design a numerically stable algorithm to evaluate the log-likelihood using Gaussian quadrature tailored to algebraic endpoint behavior. Your algorithm should:\n- Use Gauss-Jacobi quadrature with $K$ nodes to approximate the integral over $p \\in [0,1]$ with the Beta weight $p^{\\alpha-1}(1-p)^{\\beta-1}$.\n- Transform the standard Gauss-Jacobi nodes and weights on $[-1,1]$ to $[0,1]$ appropriately.\n- Evaluate the weighted integral using a stable log-sum-exp computation to avoid numerical underflow or overflow when $\\lambda_{\\max}$ is large or when $\\alpha$ or $\\beta$ are less than $1$.\n- Handle the case $n=0$ correctly.\n- Explicitly note potential approximation errors arising from finite quadrature order and model mismatch (for example, the beta-Poisson approximation versus a gamma-Poisson negative binomial alternative), and incorporate safeguards such as choosing a sufficiently large $K$.\n\nAll quantities in this problem are dimensionless counts and probabilities, so no physical units are required. Angles do not appear. The final implementation must produce values as specified for the test suite below.\n\nImplement a complete program that:\n1. Defines a function that returns $\\log \\mathbb{P}(N=n \\mid \\alpha,\\beta,\\lambda_{\\max})$ using Gauss-Jacobi quadrature with a user-specified number of nodes $K$.\n2. Defines a function that returns the total log-likelihood $\\sum_{i=1}^{M} \\log \\mathbb{P}(N=n_i \\mid \\alpha,\\beta,\\lambda_{\\max})$ for a dataset $\\{n_i\\}$.\n3. Evaluates the following test suite. For each case, compute the specified result:\n   - Case A (general, moderate parameters): $\\alpha = 2.2$, $\\beta = 5.5$, $\\lambda_{\\max} = 18.0$, dataset $\\{0,1,2,3,4,7,12\\}$, quadrature nodes $K=64$. Return the total log-likelihood as a float.\n   - Case B (boundary-dominated heterogeneity): $\\alpha = 0.6$, $\\beta = 0.7$, $\\lambda_{\\max} = 25.0$, dataset $\\{0,0,1,3,8\\}$, quadrature nodes $K=128$. Return the total log-likelihood as a float.\n   - Case C (large rate, heavier tail): $\\alpha = 3.5$, $\\beta = 1.8$, $\\lambda_{\\max} = 100.0$, dataset $\\{10,20,40,75\\}$, quadrature nodes $K=96$. Return the total log-likelihood as a float.\n   - Case D (approximation error indicator): Using the same parameters and dataset as Case A, compute the absolute difference between the total log-likelihood computed with $K=16$ and $K=64$. Return this difference as a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). The answers for all four cases must be floats.",
            "solution": "The problem statement is valid. It is scientifically grounded in the established theory of stochastic gene expression, mathematically well-posed, and presented objectively. All necessary information is provided to derive and implement a numerical solution.\n\nThe problem requires the derivation and numerical evaluation of the log-likelihood for a Beta-Poisson mixture model, which is frequently used to describe messenger RNA (mRNA) counts from single-molecule experiments in the presence of transcriptional bursting.\n\n### 1. Theoretical Formulation of the Marginal Likelihood\n\nWe are given that the mRNA count $n$ for a single cell, conditional on the promoter's active-state fraction $p$, follows a Poisson distribution with rate $\\lambda(p) = \\lambda_{\\max} p$. The probability mass function (PMF) is:\n$$ \\mathbb{P}(N=n \\mid p) = \\frac{(\\lambda_{\\max} p)^n e^{-\\lambda_{\\max} p}}{n!} $$\nThe promoter occupancy fraction $p$ is itself a random variable, drawn from a Beta distribution with parameters $\\alpha > 0$ and $\\beta > 0$. The probability density function (PDF) of $p$ is:\n$$ f_{\\mathrm{Beta}}(p \\mid \\alpha,\\beta) = \\frac{p^{\\alpha-1} (1-p)^{\\beta-1}}{B(\\alpha,\\beta)} $$\nwhere $B(\\alpha,\\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n\nThe marginal likelihood of observing a count $n$, $\\mathbb{P}(N=n)$, is obtained by integrating the conditional probability over all possible values of $p$, weighted by the probability of each $p$:\n$$ \\mathbbP(N=n \\mid \\alpha,\\beta,\\lambda_{\\max}) = \\int_{0}^{1} \\mathbb{P}(N=n \\mid p) \\, f_{\\mathrm{Beta}}(p \\mid \\alpha,\\beta) \\, \\mathrm{d}p $$\nSubstituting the PMF and PDF definitions, we get:\n$$ \\mathbb{P}(N=n) = \\int_{0}^{1} \\frac{(\\lambda_{\\max} p)^n e^{-\\lambda_{\\max} p}}{n!} \\frac{p^{\\alpha-1} (1-p)^{\\beta-1}}{B(\\alpha,\\beta)} \\, \\mathrm{d}p $$\nWe can rearrange the terms to separate the components that do not depend on $p$:\n$$ \\mathbb{P}(N=n) = \\frac{\\lambda_{\\max}^n}{n! \\, B(\\alpha,\\beta)} \\int_{0}^{1} p^{n+\\alpha-1} (1-p)^{\\beta-1} e^{-\\lambda_{\\max} p} \\, \\mathrm{d}p $$\nThis integral does not have a general closed-form solution in terms of elementary functions, which motivates the use of a numerical quadrature method as specified.\n\n### 2. Numerical Algorithm using Gauss-Jacobi Quadrature\n\nThe problem directs us to use Gauss-Jacobi quadrature, which is ideal for integrals with Beta-like weight functions. The method is designed to approximate integrals of the form:\n$$ \\int_{-1}^{1} (1-x)^a (1+x)^b f(x) \\, \\mathrm{d}x \\approx \\sum_{k=1}^{K} w_k f(x_k) $$\nwhere $\\{x_k\\}$ are the quadrature nodes and $\\{w_k\\}$ are the corresponding weights, determined by $K$, $a$, and $b$.\n\nTo apply this, we first rewrite our marginal likelihood integral to isolate the Beta distribution's kernel as a weight function, $w(p) = p^{\\alpha-1}(1-p)^{\\beta-1}$. The remaining part of the integrand is a function $h(p)$:\n$$ \\mathbb{P}(N=n) = \\frac{1}{B(\\alpha,\\beta)} \\int_{0}^{1} \\underbrace{\\left( \\frac{(\\lambda_{\\max} p)^n e^{-\\lambda_{\\max} p}}{n!} \\right)}_{h(p)} \\underbrace{p^{\\alpha-1} (1-p)^{\\beta-1}}_{w(p)} \\, \\mathrm{d}p $$\nNext, we transform the integration domain from $p \\in [0,1]$ to $x \\in [-1,1]$ using the substitution $p = (x+1)/2$, which implies $\\mathrm{d}p = \\mathrm{d}x/2$. The integral becomes:\n$$ \\int_{0}^{1} h(p) w(p) \\, \\mathrm{d}p = \\int_{-1}^{1} h\\left(\\frac{x+1}{2}\\right) \\left(\\frac{x+1}{2}\\right)^{\\alpha-1} \\left(\\frac{1-x}{2}\\right)^{\\beta-1} \\, \\frac{\\mathrm{d}x}{2} $$\n$$ = \\frac{1}{2^{\\alpha+\\beta-1}} \\int_{-1}^{1} h\\left(\\frac{x+1}{2}\\right) (1-x)^{\\beta-1} (1+x)^{\\alpha-1} \\, \\mathrm{d}x $$\nThis is now in the standard form for Gauss-Jacobi quadrature with parameters $a = \\beta-1$ and $b = \\alpha-1$. The function to be evaluated at the nodes is $h((x+1)/2)$. Let $\\{x_k\\}$ and $\\{w_k\\}$ be the $K$-point Gauss-Jacobi nodes and weights for these parameters. The integral is approximated as:\n$$ \\int_{0}^{1} h(p) w(p) \\, \\mathrm{d}p \\approx \\frac{1}{2^{\\alpha+\\beta-1}} \\sum_{k=1}^{K} w_k h\\left(\\frac{x_k+1}{2}\\right) $$\nLet's define the transformed nodes $p_k = (x_k+1)/2$. The approximation for the marginal likelihood is:\n$$ \\mathbb{P}(N=n) \\approx \\frac{1}{B(\\alpha,\\beta) \\, 2^{\\alpha+\\beta-1}} \\sum_{k=1}^{K} w_k h(p_k) = \\frac{1}{B(\\alpha,\\beta) \\, 2^{\\alpha+\\beta-1}} \\sum_{k=1}^{K} w_k \\frac{(\\lambda_{\\max} p_k)^n e^{-\\lambda_{\\max} p_k}}{n!} $$\n\n### 3. Numerically Stable Log-Likelihood Calculation\n\nDirectly computing the sum can lead to numerical underflow, especially for large $\\lambda_{\\max}$ or small probabilities. We must compute the log-likelihood by working with logarithms. The log-likelihood for a single observation $n$ is $\\log \\mathbb{P}(N=n)$.\n$$ \\log \\mathbb{P}(N=n) \\approx \\log\\left( \\sum_{k=1}^K A_k \\right) \\quad \\text{where} \\quad A_k = \\frac{w_k}{B(\\alpha,\\beta) \\, 2^{\\alpha+\\beta-1}} \\frac{(\\lambda_{\\max} p_k)^n e^{-\\lambda_{\\max} p_k}}{n!} $$\nWe can calculate the logarithm of each term, $z_k = \\log A_k$:\n$$ z_k = \\log(w_k) - \\log(B(\\alpha,\\beta)) - (\\alpha+\\beta-1)\\log(2) + n\\log(\\lambda_{\\max}) + n\\log(p_k) - \\lambda_{\\max}p_k - \\log(n!) $$\nThe logarithmic terms $\\log(B(\\alpha,\\beta))$ and $\\log(n!)$ are computed reliably using the log-gamma function, $\\mathrm{gammaln}$: $\\log(B(\\alpha,\\beta)) = \\mathrm{gammaln}(\\alpha) + \\mathrm{gammaln}(\\beta) - \\mathrm{gammaln}(\\alpha+\\beta)$ and $\\log(n!) = \\mathrm{gammaln}(n+1)$.\n\nTo compute the logarithm of the sum, we use the log-sum-exp trick. Let $M = \\max_{k} \\{z_k\\}$. Then,\n$$ \\log \\mathbb{P}(N=n) \\approx M + \\log\\left( \\sum_{k=1}^K e^{z_k - M} \\right) $$\nThis reformulation prevents the exponentiation of very small numbers and maintains numerical precision. This approach works correctly for all $n \\ge 0$.\n\n### 4. Total Log-Likelihood for a Dataset\n\nFor an independent and identically distributed dataset of $M$ counts, $\\{n_i\\}_{i=1}^{M}$, the total log-likelihood $\\mathcal{L}$ is the sum of the individual log-likelihoods:\n$$ \\mathcal{L}(\\alpha,\\beta,\\lambda_{\\max} \\mid \\{n_i\\}) = \\sum_{i=1}^{M} \\log \\mathbb{P}(N=n_i \\mid \\alpha,\\beta,\\lambda_{\\max}) $$\nFor computational efficiency, we can first find the unique counts in the dataset, calculate the log-likelihood for each unique count once, and then compute the total sum by multiplying by the frequency of each count.\n\n### 5. Potential Sources of Error\n\n- **Quadrature Approximation Error**: The Gauss-Jacobi quadrature provides an exact result only if the function $h(p)$ is a polynomial of a sufficiently low degree. Since $h(p)$ involves an exponential term, it is not a polynomial, and a finite $K$ introduces an approximation error. The accuracy of the approximation increases as the number of nodes $K$ increases. Case D in the problem is designed to quantify this error by comparing results for $K=16$ and $K=64$.\n- **Model Mismatch Error**: The Beta-Poisson model is a phenomenological abstraction of a complex biological process. It may not perfectly capture the true data-generating process. Alternative models, such as the Gamma-Poisson (which yields a Negative Binomial distribution), might provide a better or worse fit to a given dataset. This is a question of model selection, which is distinct from the numerical accuracy of evaluating the chosen model's likelihood.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_jacobi, gammaln\n\ndef log_prob_n(n, alpha, beta, lambda_max, K):\n    \"\"\"\n    Calculates the log marginal likelihood log P(N=n) for the Beta-Poisson model\n    using K-point Gauss-Jacobi quadrature.\n\n    Parameters:\n        n (int): The mRNA count.\n        alpha (float): Shape parameter > 0 of the Beta distribution.\n        beta (float): Shape parameter > 0 of the Beta distribution.\n        lambda_max (float): Max rate of the Poisson process.\n        K (int): Number of quadrature nodes.\n\n    Returns:\n        float: The log marginal likelihood log P(N=n).\n    \"\"\"\n    # Gauss-Jacobi quadrature is for integrals of the form:\n    # integral from -1 to 1 of (1-x)^a * (1+x)^b * f(x) dx\n    # Our weight function is p^(alpha-1) * (1-p)^(beta-1) on [0,1].\n    # After change of variables p = (x+1)/2, the weight becomes\n    # C * (1+x)^(alpha-1) * (1-x)^(beta-1).\n    # So, we set quadrature parameters a = beta - 1 and b = alpha - 1.\n    a_jacobi = beta - 1.0\n    b_jacobi = alpha - 1.0\n    \n    # Get standard Gauss-Jacobi nodes and weights for the interval [-1, 1]\n    x_k, w_k = roots_jacobi(K, a_jacobi, b_jacobi)\n\n    # Transform nodes from [-1, 1] to p_k in [0, 1]\n    p_k = (x_k + 1.0) / 2.0\n\n    # The full integrand term for the log-sum-exp trick is log(A_k), where\n    # P(N=n) = sum(A_k)\n    # A_k = (w_k / (B(a,b) * 2^(a+b-1))) * ( (lambda_max*p_k)^n * exp(-lambda_max*p_k) / n! )\n    \n    # log of B(alpha, beta)\n    log_beta_func = gammaln(alpha) + gammaln(beta) - gammaln(alpha + beta)\n    \n    # log of n!\n    log_n_factorial = gammaln(n + 1)\n    \n    # log of the Poisson probability component h(p_k)\n    # log(h(p_k)) = n*log(lambda_max) + n*log(p_k) - lambda_max*p_k - log(n!)\n    log_h_pk = n * np.log(lambda_max) + n * np.log(p_k) - lambda_max * p_k - log_n_factorial\n\n    # log of the full term A_k for each k\n    # log(A_k) = log(w_k) - log(B(a,b)) - (a+b-1)log(2) + log(h(p_k))\n    log_A_k = np.log(w_k) - log_beta_func - (alpha + beta - 1.0) * np.log(2.0) + log_h_pk\n    \n    # Stable computation of log(sum(exp(log_A_k)))\n    M = np.max(log_A_k)\n    log_probability = M + np.log(np.sum(np.exp(log_A_k - M)))\n\n    return log_probability\n\ndef total_log_likelihood(data, alpha, beta, lambda_max, K):\n    \"\"\"\n    Computes the total log-likelihood for a dataset under the Beta-Poisson model.\n\n    Parameters:\n        data (np.ndarray): Array of mRNA counts.\n        alpha (float): Shape parameter > 0 of the Beta distribution.\n        beta (float): Shape parameter > 0 of the Beta distribution.\n        lambda_max (float): Max rate of the Poisson process.\n        K (int): Number of quadrature nodes.\n\n    Returns:\n        float: The total log-likelihood.\n    \"\"\"\n    unique_ns, counts = np.unique(data, return_counts=True)\n    total_ll = 0.0\n    \n    for n, count in zip(unique_ns, counts):\n        total_ll += count * log_prob_n(int(n), alpha, beta, lambda_max, K)\n        \n    return total_ll\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem and evaluate test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'alpha': 2.2, 'beta': 5.5, 'lambda_max': 18.0, 'data': np.array([0, 1, 2, 3, 4, 7, 12]), 'K': 64, 'id': 'A'},\n        # Case B\n        {'alpha': 0.6, 'beta': 0.7, 'lambda_max': 25.0, 'data': np.array([0, 0, 1, 3, 8]), 'K': 128, 'id': 'B'},\n        # Case C\n        {'alpha': 3.5, 'beta': 1.8, 'lambda_max': 100.0, 'data': np.array([10, 20, 40, 75]), 'K': 96, 'id': 'C'},\n        # Case D\n        {'alpha': 2.2, 'beta': 5.5, 'lambda_max': 18.0, 'data': np.array([0, 1, 2, 3, 4, 7, 12]), 'K_low': 16, 'K_high': 64, 'id': 'D'}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        if case['id'] in ['A', 'B', 'C']:\n            result = total_log_likelihood(case['data'], case['alpha'], case['beta'], case['lambda_max'], case['K'])\n            results.append(result)\n        elif case['id'] == 'D':\n            ll_low = total_log_likelihood(case['data'], case['alpha'], case['beta'], case['lambda_max'], case['K_low'])\n            ll_high = total_log_likelihood(case['data'], case['alpha'], case['beta'], case['lambda_max'], case['K_high'])\n            result = np.abs(ll_low - ll_high)\n            results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}