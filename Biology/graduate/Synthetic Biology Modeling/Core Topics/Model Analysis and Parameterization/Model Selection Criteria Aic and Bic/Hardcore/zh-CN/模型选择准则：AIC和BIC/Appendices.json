{
    "hands_on_practices": [
        {
            "introduction": "在合成生物学中，我们经常需要将实验数据（如单分子荧光原位杂交 smFISH 的 mRNA 计数）与数学模型进行拟合。本练习将带您完成模型选择的基础流程：首先，为一个简单但基础的概率模型（泊松分布）计算其拟合优度，即最大化对数似然 $\\ell(\\hat{\\theta})$。然后，我们将这个拟合优度与模型的复杂度相结合，计算赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC)，这两个准则是评估模型性能的有力工具 。这项基础练习将为您提供从原始数据到模型选择决策所需的核心计算技能。",
            "id": "3919160",
            "problem": "在用于合成生物学建模的单分子荧光原位杂交 (smFISH) 实验中，我们观察了独立细胞中某个基因的单细胞信使 RNA (mRNA) 计数。假设一个生成模型，其中每个细胞的计数都是来自泊松分布的独立抽样，其未知速率参数为 $\\lambda$。这种设定通常用于评估均一条件下的转录活性。您观察到 $n = 12$ 个细胞的计数如下：\n$$\ny = \\{0, 1, 2, 4, 3, 5, 2, 0, 1, 3, 4, 2\\}.\n$$\n从泊松概率质量函数的定义和抽样独立性出发，推导出此数据集的最大似然估计 $\\hat{\\lambda}$ 和最大化对数似然 $\\ell(\\hat{\\lambda})$。然后，使用将赤池信息准则 (AIC) 与期望的 Kullback–Leibler 散度联系起来、以及将贝叶斯信息准则 (BIC) 与对数边际似然的大样本近似联系起来的第一性原理定义，确定对于一个具有 $k = 1$ 个自由参数和给定样本量 $n$ 的模型，$\\hat{\\lambda}$ 和 $\\ell(\\hat{\\lambda})$ 如何进入这些准则。计算所提供数据集的 $\\hat{\\lambda}$、$\\ell(\\hat{\\lambda})$、$\\mathrm{AIC}$ 和 $\\mathrm{BIC}$ 的数值。将所有数值结果四舍五入到四位有效数字。以 $\\mathrm{pmatrix}$ 格式的行向量 $(\\hat{\\lambda}, \\ell(\\hat{\\lambda}), \\mathrm{AIC}, \\mathrm{BIC})$ 表达您的最终答案。",
            "solution": "问题陈述经验证是完整的、一致的、有科学依据且适定的。该任务涉及将最大似然估计和模型选择理论标准地应用于定量生物学中的一个常见问题。\n\n求解过程如下：\n首先，我们推导泊松分布参数 $\\lambda$ 的最大似然估计 ($\\mathrm{MLE}$)。设观察到的 mRNA 计数为 $y = \\{y_1, y_2, \\ldots, y_n\\}$，其中 $n=12$。对于来自速率参数为 $\\lambda$ 的泊松分布的单个观测值 $y_i$，其概率质量函数 (PMF) 为：\n$$ P(Y=y_i | \\lambda) = \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} $$\n由于观测值是独立同分布的 (i.i.d.)，整个数据集的似然函数 $L(\\lambda | y)$ 是各个概率的乘积：\n$$ L(\\lambda | y) = \\prod_{i=1}^{n} P(Y=y_i | \\lambda) = \\prod_{i=1}^{n} \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} $$\n为了简化最大化过程，我们使用对数似然函数 $\\ell(\\lambda | y) = \\ln(L(\\lambda | y))$：\n$$ \\ell(\\lambda | y) = \\ln \\left( \\prod_{i=1}^{n} \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} \\right) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!} \\right) $$\n利用对数的性质，上式可简化为：\n$$ \\ell(\\lambda | y) = \\sum_{i=1}^{n} (y_i \\ln(\\lambda) - \\lambda - \\ln(y_i!)) = (\\ln \\lambda) \\sum_{i=1}^{n} y_i - n\\lambda - \\sum_{i=1}^{n} \\ln(y_i!) $$\n为了求得 MLE $\\hat{\\lambda}$，我们将 $\\ell(\\lambda | y)$ 对 $\\lambda$ 求导，并令其为零：\n$$ \\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda} \\left( (\\ln \\lambda) \\sum_{i=1}^{n} y_i - n\\lambda - \\sum_{i=1}^{n} \\ln(y_i!) \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^{n} y_i - n $$\n将导数设为零并求解 $\\lambda$，得到 MLE $\\hat{\\lambda}$：\n$$ \\frac{1}{\\hat{\\lambda}} \\sum_{i=1}^{n} y_i - n = 0 \\implies \\hat{\\lambda} = \\frac{\\sum_{i=1}^{n} y_i}{n} = \\bar{y} $$\n泊松速率参数的 MLE 是观测值的样本均值。对于给定的数据集 $y = \\{0, 1, 2, 4, 3, 5, 2, 0, 1, 3, 4, 2\\}$，样本量为 $n=12$，计数总和为：\n$$ \\sum_{i=1}^{12} y_i = 0 + 1 + 2 + 4 + 3 + 5 + 2 + 0 + 1 + 3 + 4 + 2 = 27 $$\n因此，MLE 的数值为：\n$$ \\hat{\\lambda} = \\frac{27}{12} = 2.25 $$\n接下来，我们通过将 $\\hat{\\lambda}$ 代回对数似然函数来计算最大化对数似然 $\\ell(\\hat{\\lambda})$：\n$$ \\ell(\\hat{\\lambda}) = (\\ln \\hat{\\lambda}) \\sum_{i=1}^{n} y_i - n\\hat{\\lambda} - \\sum_{i=1}^{n} \\ln(y_i!) $$\n代入数值 $\\sum y_i = 27$、$n=12$ 和 $\\hat{\\lambda}=2.25$：\n$$ \\ell(\\hat{\\lambda}) = 27 \\ln(2.25) - (12)(2.25) - \\sum_{i=1}^{12} \\ln(y_i!) = 27 \\ln(2.25) - 27 - \\sum_{i=1}^{12} \\ln(y_i!) $$\n项 $\\sum \\ln(y_i!)$ 根据数据计算得出：\n$$ \\sum_{i=1}^{12} \\ln(y_i!) = 2\\ln(0!) + 2\\ln(1!) + 3\\ln(2!) + 2\\ln(3!) + 2\\ln(4!) + 1\\ln(5!) $$\n因为 $0! = 1$ 且 $1! = 1$，所以 $\\ln(0!) = \\ln(1) = 0$。\n$$ \\sum_{i=1}^{12} \\ln(y_i!) = 3\\ln(2) + 2\\ln(6) + 2\\ln(24) + \\ln(120) $$\n$$ \\sum_{i=1}^{12} \\ln(y_i!) \\approx 3(0.693147) + 2(1.791759) + 2(3.178054) + 4.787492 \\approx 16.806581 $$\n现在，我们计算 $\\ell(\\hat{\\lambda})$：\n$$ \\ell(\\hat{\\lambda}) \\approx 27 \\ln(2.25) - 27 - 16.806581 \\approx 27(0.810930) - 27 - 16.806581 $$\n$$ \\ell(\\hat{\\lambda}) \\approx 21.895116 - 27 - 16.806581 = -21.911465 $$\n现在我们来看模型选择准则。赤池信息准则 ($\\mathrm{AIC}$) 从第一性原理定义，作为拟合模型与真实潜在生成过程之间期望 Kullback-Leibler 散度的估计量。对于一个有 $k$ 个自由参数的模型，其标准形式为：\n$$ \\mathrm{AIC} = -2\\ell(\\hat{\\theta}) + 2k $$\n其中 $\\hat{\\theta}$ 代表 MLE 参数的向量。在我们的例子中，模型只有一个自由参数 $\\lambda$，因此 $k=1$。项 $-2\\ell(\\hat{\\theta})$ 与拟合优度相关，而 $2k$ 是对模型复杂度的惩罚。\n$$ \\mathrm{AIC} = -2\\ell(\\hat{\\lambda}) + 2(1) \\approx -2(-21.911465) + 2 = 43.82293 + 2 = 45.82293 $$\n贝叶斯信息准则 ($\\mathrm{BIC}$) 是从边际似然（或模型证据）的对数的大样本近似推导出来的。其定义为：\n$$ \\mathrm{BIC} = -2\\ell(\\hat{\\theta}) + k \\ln(n) $$\n其中 $n$ 是样本量。对于 $n \\ge 8$，惩罚项 $k\\ln(n)$ 比 $\\mathrm{AIC}$ 的惩罚更严厉。对于我们的问题，$k=1$ 且 $n=12$。\n$$ \\mathrm{BIC} = -2\\ell(\\hat{\\lambda}) + (1)\\ln(12) \\approx 43.82293 + \\ln(12) $$\n$$ \\ln(12) \\approx 2.484907 $$\n$$ \\mathrm{BIC} \\approx 43.82293 + 2.484907 = 46.307837 $$\n最后，我们将计算出的值四舍五入到四位有效数字：\n- $\\hat{\\lambda} = 2.250$\n- $\\ell(\\hat{\\lambda}) = -21.91$\n- $\\mathrm{AIC} = 45.82$\n- $\\mathrm{BIC} = 46.31$",
            "answer": "$$ \\boxed{\\begin{pmatrix} 2.250  -21.91  45.82  46.31 \\end{pmatrix}} $$"
        },
        {
            "introduction": "当我们面对多个竞争性的生物学假设时，模型选择变得至关重要，例如，判断一个转录因子的抑制作用是协同的还是非协同的。本练习将引导您超越 AIC 和 BIC 的简单计算，深入探索贝叶斯信息准则 (BIC) 背后的理论基础 。您将了解到，BIC 不仅仅是一个惩罚复杂度的公式，它实际上是贝叶斯模型证据的一个近似，其差值 $\\Delta \\mathrm{BIC}$ 可以被解释为支持一个模型相对于另一个模型的证据强度。通过这个练习，您可以将 BIC 的计算与贝叶斯因子这一更深刻的概念联系起来，从而能够量化您对不同生物学假设的信心。",
            "id": "3919110",
            "problem": "考虑一个合成生物学建模研究，该研究涉及一个被阻遏的基因线路，其中信使核糖核酸 (mRNA) 的产生受到一个转录因子的抑制。将两个嵌套模型拟合到一个数据集，该数据集包含在重复实验中，稳态条件下 $n$ 次独立的 mRNA 浓度测量值。较简单的模型 $\\mathcal{M}_{0}$ 假设非协同阻遏，并有 $k_{0}$ 个自由参数（转录速率、降解速率和解离常数），而较复杂的模型 $\\mathcal{M}_{1}$ 通过一个希尔系数允许协同阻遏，并有 $k_{1}$ 个自由参数。这两个模型是嵌套的，因为当希尔系数等于 $1$ 时，$\\mathcal{M}_{1}$ 会简化为 $\\mathcal{M}_{0}$。最大似然估计分别得出 $\\mathcal{M}_{0}$ 和 $\\mathcal{M}_{1}$ 的最大化似然值 $\\hat{L}_{0}$ 和 $\\hat{L}_{1}$。\n\n从似然函数 $p(\\mathbf{y}\\mid \\boldsymbol{\\theta}, \\mathcal{M})$ 和边际似然 $p(\\mathbf{y}\\mid \\mathcal{M})=\\int p(\\mathbf{y}\\mid \\boldsymbol{\\theta}, \\mathcal{M})\\,p(\\boldsymbol{\\theta}\\mid \\mathcal{M})\\,\\mathrm{d}\\boldsymbol{\\theta}$ 的定义出发，并以大样本拉普拉斯近似为基础，推导用于拟合 $n$ 个观测值的具有 $k$ 个参数的模型的模型选择准则，即贝叶斯信息准则 (BIC)。然后，针对所述的两个嵌套模型，用 $n$、$k_{0}$、$k_{1}$ 以及最大化似然值 $\\hat{L}_{0}$ 和 $\\hat{L}_{1}$ 推导 BIC 差值 $\\Delta \\mathrm{BIC}=\\mathrm{BIC}_{1}-\\mathrm{BIC}_{0}$ 的表达式。使用此表达式，为以下经验获得的量计算 $\\Delta \\mathrm{BIC}$ 的数值：\n- $n=150$，\n- $k_{0}=3$，\n- $k_{1}=4$，\n- $\\hat{L}_{0}=1.2\\times 10^{-180}$，\n- $\\hat{L}_{1}=3.6\\times 10^{-175}$。\n\n最后，解释 $\\Delta \\mathrm{BIC}$ 如何近似于比较 $\\mathcal{M}_{1}$ 与 $\\mathcal{M}_{0}$ 的贝叶斯因子的 $2\\ln$ 值，并指出 $\\Delta \\mathrm{BIC}$ 的符号所暗示的证据方向。将您计算的 $\\Delta \\mathrm{BIC}$ 四舍五入到四位有效数字。在您的推导中必须明确区分赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC)；您的计算应专注于 BIC。",
            "solution": "该问题要求推导贝叶斯信息准则 (BIC)，将其应用于合成生物学中的模型选择问题，并解释结果。\n\n首先，我们根据问题陈述中阐述的原则来推导 BIC。贝叶斯模型选择的目标是根据给定数据 $\\mathbf{y}$ 的模型后验概率 $p(\\mathcal{M} \\mid \\mathbf{y})$ 来比较模型。使用贝叶斯定理，模型 $\\mathcal{M}$ 的后验概率为 $p(\\mathcal{M} \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid \\mathcal{M}) p(\\mathcal{M})}{p(\\mathbf{y})}$。当比较两个具有相等先验概率 $p(\\mathcal{M}_{0}) = p(\\mathcal{M}_{1})$ 的模型 $\\mathcal{M}_{0}$ 和 $\\mathcal{M}_{1}$ 时，比较可简化为评估它们的边际似然之比，也称为贝叶斯因子，$B_{10} = p(\\mathbf{y} \\mid \\mathcal{M}_{1}) / p(\\mathbf{y} \\mid \\mathcal{M}_{0})$。边际似然（或称模型证据）由似然函数在参数 $\\boldsymbol{\\theta}$ 的先验分布上的积分给出：\n$$p(\\mathbf{y} \\mid \\mathcal{M}) = \\int p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathcal{M}) \\, p(\\boldsymbol{\\theta} \\mid \\mathcal{M}) \\, \\mathrm{d}\\boldsymbol{\\theta}$$\n令 $L(\\boldsymbol{\\theta}) = p(\\mathbf{y} \\mid \\boldsymbol{\\theta}, \\mathcal{M})$ 为似然，$\\pi(\\boldsymbol{\\theta}) = p(\\boldsymbol{\\theta} \\mid \\mathcal{M})$ 为先验。该积分为 $\\int L(\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta}) \\, \\mathrm{d}\\boldsymbol{\\theta}$。处理被积函数的对数通常更为方便。我们定义 $S(\\boldsymbol{\\theta}) = \\ln(L(\\boldsymbol{\\theta})\\pi(\\boldsymbol{\\theta}))$。则边际似然为 $p(\\mathbf{y} \\mid \\mathcal{M}) = \\int \\exp(S(\\boldsymbol{\\theta})) \\, \\mathrm{d}\\boldsymbol{\\theta}$。\n\n我们使用拉普拉斯近似来评估此积分，当后验分布集中在其众数周围的一个小区域时，这种方法是有效的。我们在 $S(\\boldsymbol{\\theta})$ 的最大值点，即最大后验 (MAP) 估计 $\\tilde{\\boldsymbol{\\theta}}$ 附近，对其进行二阶泰勒级数展开：\n$$S(\\boldsymbol{\\theta}) \\approx S(\\tilde{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T \\nabla S(\\tilde{\\boldsymbol{\\theta}}) + \\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T H(\\tilde{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})$$\n其中 $H$ 是 $S$ 的 Hessian 矩阵。根据最大值的定义，梯度 $\\nabla S(\\tilde{\\boldsymbol{\\theta}})$ 为零。令 $A = -H(\\tilde{\\boldsymbol{\\theta}})$ 为众数处的负 Hessian 矩阵，它是一个正定矩阵。展开式简化为：\n$$S(\\boldsymbol{\\theta}) \\approx S(\\tilde{\\boldsymbol{\\theta}}) - \\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T A (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})$$\n将此代回边际似然的积分中：\n$$p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\int \\exp\\left(S(\\tilde{\\boldsymbol{\\theta}}) - \\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T A (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})\\right) \\, \\mathrm{d}\\boldsymbol{\\theta} = \\exp(S(\\tilde{\\boldsymbol{\\theta}})) \\int \\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})^T A (\\boldsymbol{\\theta} - \\tilde{\\boldsymbol{\\theta}})\\right) \\, \\mathrm{d}\\boldsymbol{\\theta}$$\n该积分是均值为 $\\tilde{\\boldsymbol{\\theta}}$、协方差矩阵为 $A^{-1}$ 的多元正态分布的核。其值为 $(2\\pi)^{k/2}(\\det A)^{-1/2}$，其中 $k$ 是 $\\boldsymbol{\\theta}$ 的维度（自由参数的数量）。因此，\n$$p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\exp(S(\\tilde{\\boldsymbol{\\theta}})) (2\\pi)^{k/2} (\\det A)^{-1/2} = L(\\tilde{\\boldsymbol{\\theta}})\\pi(\\tilde{\\boldsymbol{\\theta}}) (2\\pi)^{k/2} (\\det A)^{-1/2}$$\n对于大量的独立观测值 $n$，可以进行几个近似：\n1. MAP 估计 $\\tilde{\\boldsymbol{\\theta}}$ 收敛于最大似然估计 (MLE) $\\hat{\\boldsymbol{\\theta}}$。\n2. 与似然项相比，$\\pi(\\tilde{\\boldsymbol{\\theta}})$ 项变得可以忽略不计。\n3. 矩阵 $A = -H(\\tilde{\\boldsymbol{\\theta}})$ 主要由对数似然的 Hessian 矩阵决定，该矩阵约等于观测到的费雪信息矩阵 $I(\\hat{\\boldsymbol{\\theta}})$。对于大的 $n$，我们有 $I(\\hat{\\boldsymbol{\\theta}}) \\approx n \\cdot i(\\hat{\\boldsymbol{\\theta}})$，其中 $i(\\hat{\\boldsymbol{\\theta}})$ 是单个观测值的费雪信息。行列式的尺度关系为 $\\det(A) \\approx \\det(n \\cdot i(\\hat{\\boldsymbol{\\theta}})) = n^k \\det(i(\\hat{\\boldsymbol{\\theta}}))$。\n\n取边际似然的对数：\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) + \\ln\\pi(\\hat{\\boldsymbol{\\theta}}) + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det A)$$\n代入 $\\det A$ 的近似值：\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) + \\ln\\pi(\\hat{\\boldsymbol{\\theta}}) + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(n^k \\det(i(\\hat{\\boldsymbol{\\theta}})))$$\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) - \\frac{k}{2}\\ln n + \\left( \\ln\\pi(\\hat{\\boldsymbol{\\theta}}) + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(i(\\hat{\\boldsymbol{\\theta}}))) \\right)$$\n当 $n \\rightarrow \\infty$ 时，$\\ln L(\\hat{\\boldsymbol{\\theta}})$ 项的尺度为 $O(n)$，而 $\\frac{k}{2}\\ln n$ 的尺度为 $O(\\ln n)$。括号中的项为 $O(1)$ 阶。只保留随 $n$ 增长的项，我们得到大样本近似：\n$$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx \\ln L(\\hat{\\boldsymbol{\\theta}}) - \\frac{k}{2}\\ln n$$\n贝叶斯信息准则 (BIC) 通常通过将此式乘以 $-2$ 来定义，使其与赤池信息准则 (AIC) 等偏差统计量处于同一尺度。令 $\\hat{L} = L(\\hat{\\boldsymbol{\\theta}})$ 为最大化似然值。\n$$\\mathrm{BIC} = -2\\ln\\hat{L} + k\\ln n$$\n该准则对模型的复杂性（较大的 $k$）进行惩罚，并对拟合优度（较大的 $\\hat{L}$）进行奖励。应将其与 AIC 区分开，AIC 定义为 $\\mathrm{AIC} = -2\\ln\\hat{L} + 2k$。对于任何样本量 $n > \\exp(2) \\approx 7.4$（本例即是如此），BIC 的惩罚项 $k\\ln n$ 比 AIC 的惩罚项（$2k$）更严格。\n\n接下来，我们为两个嵌套模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{0}$ 推导 BIC 差值的表达式，$\\Delta\\mathrm{BIC} = \\mathrm{BIC}_{1} - \\mathrm{BIC}_{0}$。\n使用推导出的 BIC 公式：\n$$\\mathrm{BIC}_{0} = -2\\ln\\hat{L}_{0} + k_{0}\\ln n$$\n$$\\mathrm{BIC}_{1} = -2\\ln\\hat{L}_{1} + k_{1}\\ln n$$\n差值为：\n$$\\Delta\\mathrm{BIC} = \\mathrm{BIC}_{1} - \\mathrm{BIC}_{0} = (-2\\ln\\hat{L}_{1} + k_{1}\\ln n) - (-2\\ln\\hat{L}_{0} + k_{0}\\ln n)$$\n整理各项得：\n$$\\Delta\\mathrm{BIC} = -2(\\ln\\hat{L}_{1} - \\ln\\hat{L}_{0}) + (k_{1} - k_{0})\\ln n$$\n这可以写成似然比的形式：\n$$\\Delta\\mathrm{BIC} = -2\\ln\\left(\\frac{\\hat{L}_{1}}{\\hat{L}_{0}}\\right) + (k_{1} - k_{0})\\ln n$$\n\n现在，我们使用给定的量计算 $\\Delta\\mathrm{BIC}$ 的数值：\n$n=150$, $k_{0}=3$, $k_{1}=4$, $\\hat{L}_{0}=1.2\\times 10^{-180}$, $\\hat{L}_{1}=3.6\\times 10^{-175}$。\n首先，计算似然比 $\\frac{\\hat{L}_{1}}{\\hat{L}_{0}}$：\n$$\\frac{\\hat{L}_{1}}{\\hat{L}_{0}} = \\frac{3.6 \\times 10^{-175}}{1.2 \\times 10^{-180}} = \\frac{3.6}{1.2} \\times 10^{-175 - (-180)} = 3.0 \\times 10^{5}$$\n接下来，我们计算 $\\Delta\\mathrm{BIC}$ 表达式中的两项：\n对数似然比项是：\n$$-2\\ln\\left(\\frac{\\hat{L}_{1}}{\\hat{L}_{0}}\\right) = -2\\ln(3.0 \\times 10^{5}) = -2(\\ln(3.0) + 5\\ln(10)) \\approx -2(1.09861 + 5 \\times 2.30259) \\approx -2(12.61156) = -25.22312$$\n惩罚项是：\n$$(k_{1} - k_{0})\\ln n = (4 - 3)\\ln(150) = \\ln(150) \\approx 5.010635$$\n合并这两项：\n$$\\Delta\\mathrm{BIC} = -25.22312 + 5.010635 = -20.212485$$\n将结果四舍五入到四位有效数字，我们得到 $\\Delta\\mathrm{BIC} \\approx -20.21$。\n\n最后，我们解释 $\\Delta\\mathrm{BIC}$ 与贝叶斯因子之间的关系。支持模型 $\\mathcal{M}_{1}$ 而非 $\\mathcal{M}_{0}$ 的贝叶斯因子是 $B_{10} = \\frac{p(\\mathbf{y} \\mid \\mathcal{M}_{1})}{p(\\mathbf{y} \\mid \\mathcal{M}_{0})}$。根据我们的推导，$\\ln p(\\mathbf{y} \\mid \\mathcal{M}) \\approx -\\frac{1}{2}\\mathrm{BIC}$。因此，贝叶斯因子的对数的两倍是：\n$$2\\ln B_{10} = 2(\\ln p(\\mathbf{y} \\mid \\mathcal{M}_{1}) - \\ln p(\\mathbf{y} \\mid \\mathcal{M}_{0})) \\approx 2\\left(-\\frac{1}{2}\\mathrm{BIC}_{1} - \\left(-\\frac{1}{2}\\mathrm{BIC}_{0}\\right)\\right) = -(\\mathrm{BIC}_{1} - \\mathrm{BIC}_{0}) = -\\Delta\\mathrm{BIC}$$\n因此，$\\Delta\\mathrm{BIC}$ 近似于 $-2\\ln B_{10}$。问题要求将 $\\Delta\\mathrm{BIC}$ 与“贝叶斯因子的 $2\\ln$ 值”联系起来，最精确的表述是 $\\Delta\\mathrm{BIC} \\approx -2\\ln B_{10}$，或者等价地 $\\Delta\\mathrm{BIC} \\approx 2\\ln B_{01}$，其中 $B_{01} = 1/B_{10}$ 是支持 $\\mathcal{M}_{0}$ 而非 $\\mathcal{M}_{1}$ 的贝叶斯因子。\n\n$\\Delta\\mathrm{BIC}$ 的符号表明哪个模型更受偏好。使用 BIC 进行模型选择旨在找到具有最小 BIC 值的模型。\n- 如果 $\\Delta\\mathrm{BIC} = \\mathrm{BIC}_{1} - \\mathrm{BIC}_{0}  0$，那么 $\\mathrm{BIC}_{1}  \\mathrm{BIC}_{0}$，证据支持更复杂的模型 $\\mathcal{M}_{1}$。\n- 如果 $\\Delta\\mathrm{BIC} > 0$，那么 $\\mathrm{BIC}_{1} > \\mathrm{BIC}_{0}$，证据支持更简单的模型 $\\mathcal{M}_{0}$。\n在本例中，$\\Delta\\mathrm{BIC} \\approx -20.21$。这是一个很强的负值。这表明有非常强的证据支持更复杂的模型 $\\mathcal{M}_{1}$（协同阻遏）而非更简单的模型 $\\mathcal{M}_{0}$（非协同阻遏）。差值的幅度（$ 10$）通常被解释为支持偏好模型的“非常强”的证据。由额外参数（希尔系数）带来的拟合优度提升远超过了对模型复杂性增加的惩罚。",
            "answer": "$$\\boxed{-20.21}$$"
        },
        {
            "introduction": "在分析基因表达等随机过程时，一个关键挑战是辨别观测到的变异性的来源。这种变异性是源于随时间变化的系统参数（非平稳性），还是源于过程内在的随机爆发特性（过离散）？本练习将您置于一个高级的建模情境中，要求您使用信息准则来裁决两种结构上完全不同但都能解释数据变异性的模型 。通过比较一个时变泊松模型和一个稳态负二项模型，您将学会如何利用 AIC 和 BIC 来区分不同的潜在生物学机制。这项实践将您的模型选择技能从比较嵌套模型提升到辨别根本不同的机理假设，这是系统与合成生物学研究中的一项核心能力。",
            "id": "3919156",
            "problem": "您正在对合成基因表达中的时间分箱分子计数数据进行建模，其中转录活性可能随时间变化或产生突发性输出。对于跨越 $n$ 个分箱的独立整数计数观测向量 $(x_1, x_2, \\dots, x_n)$，考虑两个相互竞争的概率模型：\n\n- 模型 $\\mathcal{P}$ (泊松分布，时变率): 每个分箱 $i$ 都有其自身的率参数 $\\lambda_i$，$x_i$ 作为参数为 $\\lambda_i$ 的泊松随机变量分布，且在各分箱间独立。\n- 模型 $\\mathcal{N}$ (负二项分布，恒定率和离散度): 所有分箱共享单个均值参数 $\\mu$ 和单个离散度参数 $r$ (有时称为“大小”参数)，$x_i$ 是独立同分布的负二项随机变量。\n\n使用以下基本原理：\n\n- 对于模型 $\\mathcal{P}$，从整数计数 $k \\geq 0$ 且率参数 $\\lambda  0$ 的泊松概率质量函数的定义出发，并利用分箱的独立性来构建似然函数。使用最大似然估计来确定参数估计值。\n- 对于模型 $\\mathcal{N}$，从整数计数 $k \\geq 0$ 且均值 $\\mu  0$、离散度 $r  0$ 的负二项概率质量函数的定义出发，并利用分箱的独立同分布特性来构建似然函数。使用最大似然估计来确定参数估计值，注意均值参数 $\\mu$ 应与样本均值相关联，而 $r$ 需要进行优化。\n- 对于模型选择，使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC)：两者都是惩罚似然准则，分别源于对 Kullback–Leibler 散度的考量和对大样本贝叶斯证据的近似。从第一性原理推导计算它们所需的表达式，并将其应用于两个模型。不要引入简化公式；从似然和参数计数逻辑推导出每个量。\n\n科学真实性：假设 $x_i$ 是由合理的随机基因表达机制生成的。模型 $\\mathcal{P}$ 通过时变率 $\\lambda_i$ 来针对非平稳性，而模型 $\\mathcal{N}$ 通过在恒定均值 $\\mu$ 下由 $r$ 控制的过度离散来针对突发性。\n\n任务要求：\n\n- 实现一个完整的程序，对于每个测试用例，计算两个模型的最大似然估计，评估它们的对数似然，计算 AIC 和 BIC，并根据每个准则决定哪个模型更优（值越小表示越优）。\n- 参数计数必须反映模型：模型 $\\mathcal{P}$ 有 $n$ 个参数 $(\\lambda_1, \\dots, \\lambda_n)$；模型 $\\mathcal{N}$ 有 $2$ 个参数 $(\\mu, r)$。\n- 在贝叶斯信息准则的惩罚项中，使用分箱数 $n$ 作为样本大小。\n- 如果某个测试用例的两个准则完全相等，则优先选择参数较少的更简单模型，即模型 $\\mathcal{N}$。\n\n解释：在合成生物学方面，如果 AIC（或 BIC）偏好模型 $\\mathcal{P}$，则将数据解释为与随时间变化的非平稳转录活性更一致；如果它偏好模型 $\\mathcal{N}$，则将数据解释为与大致恒定的基础速率下的突发性（过度离散）更一致。\n\n测试套件：\n\n- 案例 1 (非平稳性，计数增加): $(0, 1, 2, 3, 5, 8, 13, 21)$。\n- 案例 2 (突发性，大量零值伴随零星大计数): $(0, 0, 5, 0, 0, 12, 0, 0, 7, 0, 0, 15)$。\n- 案例 3 (边界情况，全为零): $(0, 0, 0, 0, 0)$。\n- 案例 4 (恒定的中等计数): $(3, 3, 3, 3, 3, 3)$。\n- 案例 5 (突发性，高度过度离散): $(2, 10, 1, 8, 0, 9, 3, 11, 2, 7)$。\n- 案例 6 (非平稳性，带有波动的上升趋势): $(2, 2, 3, 5, 4, 6, 9, 7, 10, 12)$。\n\n最终输出规格：\n\n- 对于每个测试用例 $i$，计算 AIC 和 BIC 偏好的模型。将模型 $\\mathcal{P}$ 编码为 $0$，模型 $\\mathcal{N}$ 编码为 $1$。\n- 您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表形式的结果。按顺序连接每个案例的 2 个整数，产生一个扁平化列表，序列为 $[\\text{AIC 案例 }1, \\text{BIC 案例 }1, \\text{AIC 案例 }2, \\text{BIC 案例 }2, \\dots, \\text{AIC 案例 }6, \\text{BIC 案例 }6]$。",
            "solution": "该问题要求使用信息论模型选择准则，对计数数据的两种统计模型进行比较分析，这些数据可能源于合成生物学实验。这两种模型是时变泊松模型（表示为 $\\mathcal{P}$）和稳态负二项模型（表示为 $\\mathcal{N}$）。在实施计算解决方案之前，我们必须首先从第一性原理推导出必要的统计量。\n\n数据由一个包含 $n$ 个独立整数计数的向量 $X = (x_1, x_2, \\dots, x_n)$ 组成。\n\n**模型 $\\mathcal{P}$：时变泊松模型**\n\n该模型假设每个观测值 $x_i$ 都来自一个具有其自身率参数 $\\lambda_i$ 的泊松分布。这适应了底层过程的非平稳性。\n\n对于单个服从泊松分布且率参数为 $\\lambda_i  0$ 的随机变量 $X_i$，其概率质量函数 (PMF) 为：\n$$ P(X_i=k | \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^k}{k!} $$\n对于非负整数 $k$。\n\n鉴于观测值 $x_1, \\dots, x_n$ 是独立的，参数向量 $\\Lambda = (\\lambda_1, \\dots, \\lambda_n)$ 的联合似然函数 $L$ 是各个概率的乘积：\n$$ L(\\Lambda | X) = \\prod_{i=1}^n P(X_i=x_i | \\lambda_i) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{x_i}}{x_i!} $$\n对数似然函数 $\\ell(\\Lambda | X) = \\log L(\\Lambda | X)$ 更便于最大化：\n$$ \\ell(\\Lambda | X) = \\sum_{i=1}^n \\left( -\\lambda_i + x_i \\log \\lambda_i - \\log(x_i!) \\right) $$\n为了找到每个参数 $\\lambda_i$ 的最大似然估计 (MLE)，我们将 $\\ell$ 对 $\\lambda_i$ 求导并令其为零。由于参数是解耦的，我们可以独立地优化每一个。\n$$ \\frac{\\partial \\ell}{\\partial \\lambda_i} = -1 + \\frac{x_i}{\\lambda_i} = 0 \\implies \\hat{\\lambda}_i = x_i $$\n这个结果是直观的：在给定分箱中，泊松过程率的最佳估计是在该分箱中观察到的计数。二阶导数 $\\frac{\\partial^2 \\ell}{\\partial \\lambda_i^2} = -\\frac{x_i}{\\lambda_i^2}$ 是非正的，确认了这是一个最大值。在 $x_i=0$ 的情况下，MLE 是 $\\hat{\\lambda}_i=0$。在此上下文中，项 $x_i \\log \\lambda_i$ 在 $x_i=0$ 时被计算为 $0 \\log 0$，其值取为 0。\n\n将 MLEs $\\hat{\\lambda}_i = x_i$ 代入对数似然函数，得到最大化对数似然 $\\hat{\\ell}_{\\mathcal{P}}$：\n$$ \\hat{\\ell}_{\\mathcal{P}} = \\sum_{i=1}^n \\left( -x_i + x_i \\log x_i - \\log(x_i!) \\right) $$\n这里我们使用恒等式 $\\log(k!) = \\log\\Gamma(k+1)$，其中 $\\Gamma$ 是 Gamma 函数。该模型中的参数数量为 $k_{\\mathcal{P}} = n$。\n\n**模型 $\\mathcal{N}$：稳态负二项模型**\n\n该模型假设所有观测值 $x_i$ 都是来自单个负二项 (NB) 分布的独立同分布 (i.i.d.) 随机变量。这适用于建模相对于泊松分布“过度离散”的计数数据（即方差大于均值），这是突发性基因表达的一个共同特征。\n\nNB 分布可以通过其均值 $\\mu  0$ 和离散度参数 $r  0$ 进行参数化。对于一个观测值 $k$ 的 PMF 是：\n$$ P(X=k | \\mu, r) = \\frac{\\Gamma(k+r)}{\\Gamma(k+1)\\Gamma(r)} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^k $$\n均值为 $E[X] = \\mu$，方差为 $Var[X] = \\mu + \\frac{\\mu^2}{r}$。当 $r \\to \\infty$ 时，方差趋近于均值，负二项分布收敛于均值为 $\\mu$ 的泊松分布。\n\n对于 $n$ 个 i.i.d. 观测值，对数似然函数 $\\ell(\\mu, r | X)$ 为：\n$$ \\ell(\\mu, r | X) = \\sum_{i=1}^n \\log P(X_i=x_i | \\mu, r) $$\n$$ \\ell(\\mu, r | X) = \\sum_{i=1}^n \\left[ \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log\\Gamma(x_i+1) + r\\log\\left(\\frac{r}{r+\\mu}\\right) + x_i\\log\\left(\\frac{\\mu}{r+\\mu}\\right) \\right] $$\n为了找到 MLE，我们对 $\\mu$ 和 $r$ 求导。对于 $\\mu$：\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = \\sum_{i=1}^n \\left[ - \\frac{r}{r+\\mu} + \\frac{x_i}{\\mu} - \\frac{x_i}{r+\\mu} \\right] = \\frac{1}{\\mu} \\sum_{i=1}^n x_i - \\frac{1}{r+\\mu} \\sum_{i=1}^n(r+x_i) $$\n令其为零，并设样本均值为 $\\bar{x} = \\frac{1}{n}\\sum x_i$：\n$$ \\frac{n\\bar{x}}{\\mu} - \\frac{n(r+\\bar{x})}{r+\\mu} = 0 \\implies \\bar{x}(r+\\mu) = \\mu(r+\\bar{x}) \\implies \\bar{x}r + \\bar{x}\\mu = \\mu r + \\mu\\bar{x} \\implies \\hat{\\mu} = \\bar{x} $$\n均值的 MLE 是样本均值。\n\n将 $\\hat{\\mu} = \\bar{x}$ 代入 $\\ell$，得到 $r$ 的剖面对数似然：\n$$ \\ell(r) = \\sum_{i=1}^n \\left[ \\log\\Gamma(x_i+r) - \\log\\Gamma(r) - \\log(x_i+1)! + r\\log r + x_i\\log\\bar{x} - (r+x_i)\\log(r+\\bar{x}) \\right] $$\n对于最大化此函数的 MLE $\\hat{r}$，没有闭式解。必须使用数值优化来找到它。\n\n如果样本方差不大于样本均值，则会出现一个特殊情况。由于 NB 方差 $\\mu + \\mu^2/r$ 对于有限的 $r0$ 严格大于 $\\mu$，因此 NB 模型无法完美拟合此类数据。MLE 过程通过将 $r \\to \\infty$ 来处理这种情况，此时 NB 模型收敛到一个具有单一率参数 $\\mu$ 的简单泊松模型。在这种情况下，模型 $\\mathcal{N}$ 的最大化对数似然等同于一个对所有分箱都使用参数 $\\hat{\\mu} = \\bar{x}$ 的泊松模型的对数似然：\n$$ \\hat{\\ell}_{\\mathcal{N}} = \\sum_{i=1}^n \\left( -\\bar{x} + x_i \\log \\bar{x} - \\log(x_i!) \\right) \\quad (\\text{if } \\text{Var}(X) \\le \\bar{x}) $$\n另一个特殊情况是当所有 $x_i=0$ 时，$\\bar{x}=0$。此时，对于任何 $r0$，似然简化为 $P(X=0|\\mu \\to 0, r) = 1$，使得对数似然为 $0$。因此 $\\hat{\\ell}_{\\mathcal{N}}=0$。\n\n该模型的参数数量为 $k_{\\mathcal{N}}=2$，对应于 $\\mu$ 和 $r$。即使当 $\\hat{r} \\to \\infty$ 时，参数计数也反映了所拟合模型类的复杂性，而不是最终估计的具体值。\n\n**模型选择准则**\n\n我们使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 来比较模型。这两个准则都对拥有更多参数的模型进行惩罚，以平衡拟合优度（由 $\\hat{\\ell}$ 衡量）与模型复杂度。AIC 或 BIC 分数较低的模型更优。\n\n通用公式为：\n$$ AIC = 2k - 2\\hat{\\ell} $$\n$$ BIC = k \\log(m) - 2\\hat{\\ell} $$\n这里，$k$ 是模型参数的数量，$m$ 是样本大小，在此指定为分箱数 $n$。\n\n对于模型 $\\mathcal{P}$：\n$k_{\\mathcal{P}} = n$。\n$$ AIC_{\\mathcal{P}} = 2n - 2\\hat{\\ell}_{\\mathcal{P}} $$\n$$ BIC_{\\mathcal{P}} = n \\log(n) - 2\\hat{\\ell}_{\\mathcal{P}} $$\n\n对于模型 $\\mathcal{N}$：\n$k_{\\mathcal{N}} = 2$。\n$$ AIC_{\\mathcal{N}} = 2(2) - 2\\hat{\\ell}_{\\mathcal{N}} = 4 - 2\\hat{\\ell}_{\\mathcal{N}} $$\n$$ BIC_{\\mathcal{N}} = 2 \\log(n) - 2\\hat{\\ell}_{\\mathcal{N}} $$\n\n下面的实现为每个测试用例计算这些量，并根据每个准则确定偏好的模型，同时遵循指定的有利于更简单模型 $\\mathcal{N}$ 的平局打破规则。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize_scalar\n\ndef calculate_scores(x: np.ndarray):\n    \"\"\"\n    Calculates AIC and BIC for Model P (Poisson) and Model N (Negative Binomial).\n\n    Args:\n        x (np.ndarray): A 1D array of integer counts.\n\n    Returns:\n        tuple: A tuple containing (AIC_p, BIC_p, AIC_n, BIC_n).\n    \"\"\"\n    n = len(x)\n    x = np.array(x, dtype=np.float64)\n\n    # --- Model P (Time-Varying Poisson) ---\n    k_p = n\n    \n    # Handle x_i log(x_i) term where x_i=0\n    x_log_x = np.where(x  0, x * np.log(x), 0)\n    log_fact_x = gammaln(x + 1)\n    \n    # Maximized log-likelihood for Model P\n    # hat(lambda_i) = x_i\n    log_lik_p = np.sum(-x + x_log_x - log_fact_x)\n\n    aic_p = 2 * k_p - 2 * log_lik_p\n    bic_p = k_p * np.log(n) - 2 * log_lik_p\n\n    # --- Model N (Stationary Negative Binomial) ---\n    k_n = 2\n    \n    x_mean = np.mean(x)\n    \n    # Edge case: all counts are zero\n    if x_mean == 0:\n        log_lik_n = 0.0\n    else:\n        # Check for underdispersion (or exact Poisson dispersion)\n        # We use population variance (ddof=0)\n        x_var = np.var(x, ddof=0)\n        \n        if x_var = x_mean:\n            # Data is not overdispersed. MLE for r is infinity.\n            # Model N log-likelihood converges to that of a simple Poisson model.\n            # hat(mu) = x_mean\n            if x_mean  0:\n                log_x_mean_term = x * np.log(x_mean)\n                log_lik_n = np.sum(-x_mean + log_x_mean_term - log_fact_x)\n            else: # Should not happen due to outer if, but for robustness\n                log_lik_n = 0.0\n        else:\n            # Overdispersed data: numerically optimize for r.\n            sum_x = np.sum(x)\n            sum_log_fact_x = np.sum(log_fact_x)\n\n            def neg_log_lik_nb(r, x, n, x_mean, sum_x, sum_log_fact_x):\n                # The function to be minimized is the negative of the profile log-likelihood for r\n                if r = 0:\n                    return np.inf\n                \n                # gammaln(x + r) is an array operation\n                # gammaln(r) is a scalar\n                log_lik = np.sum(gammaln(x + r)) \\\n                          - n * gammaln(r) \\\n                          + n * r * np.log(r) \\\n                          + sum_x * np.log(x_mean) \\\n                          - (n * r + sum_x) * np.log(r + x_mean) \\\n                          - sum_log_fact_x\n                \n                return -log_lik\n\n            # Search for r on a log scale or with reasonable bounds\n            # Bounds need to be positive.\n            opt_result = minimize_scalar(\n                neg_log_lik_nb,\n                bounds=(1e-8, 1e8),  # r  0\n                method='bounded',\n                args=(x, n, x_mean, sum_x, sum_log_fact_x)\n            )\n            \n            log_lik_n = -opt_result.fun\n\n    aic_n = 2 * k_n - 2 * log_lik_n\n    bic_n = k_n * np.log(n) - 2 * log_lik_n\n    \n    return aic_p, bic_p, aic_n, bic_n\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Model P encoded as 0, Model N as 1.\n    test_cases = [\n        # Case 1 (nonstationarity, increasing counts)\n        np.array([0, 1, 2, 3, 5, 8, 13, 21]),\n        # Case 2 (bursty, many zeros with sporadic large counts)\n        np.array([0, 0, 5, 0, 0, 12, 0, 0, 7, 0, 0, 15]),\n        # Case 3 (boundary, all zeros)\n        np.array([0, 0, 0, 0, 0]),\n        # Case 4 (constant moderate counts)\n        np.array([3, 3, 3, 3, 3, 3]),\n        # Case 5 (bursty, high overdispersion)\n        np.array([2, 10, 1, 8, 0, 9, 3, 11, 2, 7]),\n        # Case 6 (nonstationarity, upward trend with fluctuations)\n        np.array([2, 2, 3, 5, 4, 6, 9, 7, 10, 12]),\n    ]\n\n    results = []\n    for counts in test_cases:\n        aic_p, bic_p, aic_n, bic_n = calculate_scores(counts)\n        \n        # Lower score is better. Tie-break favors simpler model (N, encoded as 1).\n        # aic_n = aic_p returns True (1) if N is preferred or scores are equal.\n        aic_pref = 1 if aic_n = aic_p else 0\n        bic_pref = 1 if bic_n = bic_p else 0\n        \n        results.extend([aic_pref, bic_pref])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}