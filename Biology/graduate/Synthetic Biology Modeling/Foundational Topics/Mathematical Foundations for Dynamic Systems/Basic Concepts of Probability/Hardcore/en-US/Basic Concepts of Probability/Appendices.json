{
    "hands_on_practices": [
        {
            "introduction": "The Gaussian distribution is fundamental to modeling, often representing measurement noise or the cumulative effect of many small, independent processes. Rather than calculating its moments like mean and variance through direct integration, which can be cumbersome, we can use the more powerful and elegant method of characteristic functions. This exercise guides you through the process of deriving the characteristic function for a Gaussian random variable from first principles and then using it to effortlessly extract the distribution's moments, a technique that is indispensable in advanced stochastic analysis .",
            "id": "3907107",
            "problem": "In a microfluidic chemostat experiment monitoring a fluorescent reporter in a synthetic gene circuit, the measured signal is corrupted by additive measurement noise due to camera readout and photon statistics at high count rates. A common modeling assumption is that this measurement noise can be represented by a Gaussian random variable $X$ with mean $\\mu$ (instrumental offset) and variance $\\sigma^{2}$ (aggregate fluctuation level), independent of the underlying biological process. Let the probability density function of $X$ be given by the Gaussian density $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\big(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\big)$, and let the characteristic function be defined by $\\varphi_{X}(t) = \\mathbb{E}[\\exp(i t X)] = \\int_{-\\infty}^{\\infty} \\exp(i t x) f_{X}(x)\\,dx$.\n\nUsing only the definition of the characteristic function and the Gaussian density, compute $\\varphi_{X}(t)$ in closed form by evaluating the defining integral from first principles. Then, using the relationship between derivatives of $\\varphi_{X}(t)$ at $t=0$ and raw moments $\\mathbb{E}[X^{n}]$, derive $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$ without appealing to any pre-memorized moment formulas for the Gaussian.\n\nReport the closed-form expression for $\\varphi_{X}(t)$ as your final answer. No numerical approximation or rounding is required; express your final result symbolically in terms of $\\mu$, $\\sigma$, and $t$.",
            "solution": "The problem statement is well-posed, self-contained, and scientifically grounded in standard probability theory. It presents a canonical derivation of the characteristic function for a Gaussian distribution and its use in calculating moments. We proceed with the solution from first principles as requested.\n\nThe problem defines a Gaussian random variable $X$ with mean $\\mu$ and variance $\\sigma^2$. Its probability density function (PDF) is given as:\n$$f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)$$\nThe characteristic function, $\\varphi_{X}(t)$, is defined as the expected value of $\\exp(itX)$:\n$$\\varphi_{X}(t) = \\mathbb{E}[\\exp(itX)] = \\int_{-\\infty}^{\\infty} \\exp(itx) f_{X}(x)\\,dx$$\n\nTo compute $\\varphi_{X}(t)$, we substitute the PDF into the integral definition:\n$$\\varphi_{X}(t) = \\int_{-\\infty}^{\\infty} \\exp(itx) \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx$$\nWe can combine the exponential terms and move the constant factor outside the integral:\n$$\\varphi_{X}(t) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left(itx - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx$$\nThe core of the derivation is to manipulate the argument of the exponential into a form that resembles a new Gaussian distribution, allowing the integral to be evaluated. We focus on the exponent, which we'll call $A(x)$:\n$$A(x) = itx - \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}$$\nExpanding the squared term:\n$$A(x) = itx - \\frac{x^2 - 2\\mu x + \\mu^2}{2\\sigma^2} = -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2\\mu x + \\mu^2 - 2i\\sigma^2tx \\right]$$\nWe group terms involving $x$ and complete the square. The terms with $x$ are $x^2 - 2\\mu x - 2i\\sigma^2tx = x^2 - 2x(\\mu + i\\sigma^2t)$.\nTo complete the square for an expression $x^2 - 2bx$, we write it as $(x-b)^2 - b^2$. Here, $b = \\mu + i\\sigma^2t$.\nSo, $x^2 - 2x(\\mu + i\\sigma^2t) = \\left(x - (\\mu + i\\sigma^2t)\\right)^2 - (\\mu + i\\sigma^2t)^2$.\nSubstituting this back into the expression for $A(x)$:\n$$A(x) = -\\frac{1}{2\\sigma^2} \\left[ \\left(x - (\\mu + i\\sigma^2t)\\right)^2 - (\\mu + i\\sigma^2t)^2 + \\mu^2 \\right]$$\n$$A(x) = -\\frac{\\left(x - (\\mu + i\\sigma^2t)\\right)^2}{2\\sigma^2} + \\frac{(\\mu + i\\sigma^2t)^2 - \\mu^2}{2\\sigma^2}$$\nNow we simplify the term not involving $x$:\n$$\\frac{(\\mu + i\\sigma^2t)^2 - \\mu^2}{2\\sigma^2} = \\frac{\\mu^2 + 2i\\mu\\sigma^2t + (i\\sigma^2t)^2 - \\mu^2}{2\\sigma^2} = \\frac{2i\\mu\\sigma^2t - \\sigma^4t^2}{2\\sigma^2} = i\\mu t - \\frac{\\sigma^2t^2}{2}$$\nSo, the exponent $A(x)$ becomes:\n$$A(x) = -\\frac{\\left(x - (\\mu + i\\sigma^2t)\\right)^2}{2\\sigma^2} + i\\mu t - \\frac{\\sigma^2t^2}{2}$$\nWe substitute this back into the integral for $\\varphi_X(t)$:\n$$\\varphi_{X}(t) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{\\left(x - (\\mu + i\\sigma^2t)\\right)^2}{2\\sigma^2} + i\\mu t - \\frac{\\sigma^2t^2}{2}\\right) dx$$\nThe term $\\exp\\left(i\\mu t - \\frac{\\sigma^2t^2}{2}\\right)$ is constant with respect to $x$ and can be factored out:\n$$\\varphi_{X}(t) = \\exp\\left(i\\mu t - \\frac{\\sigma^2t^2}{2}\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{\\left(x - (\\mu + i\\sigma^2t)\\right)^2}{2\\sigma^2}\\right) dx$$\nThe integral is the total area under a Gaussian PDF with mean $\\mu' = \\mu + i\\sigma^2t$ and standard deviation $\\sigma$. The integral of any PDF over its entire domain is $1$. Although the mean $\\mu'$ is complex, this result holds via contour integration in the complex plane, which confirms that the integral evaluates to $1$.\nThus, the characteristic function for the Gaussian distribution is:\n$$\\varphi_{X}(t) = \\exp\\left(i\\mu t - \\frac{\\sigma^2t^2}{2}\\right)$$\nThis completes the first part of the problem.\n\nFor the second part, we derive the moments from $\\varphi_{X}(t)$ using the property $\\mathbb{E}[X^n] = i^{-n} \\frac{d^n \\varphi_X(t)}{dt^n}\\bigg|_{t=0}$.\n\nTo find the mean $\\mathbb{E}[X]$ (the first raw moment, $n=1$):\n$$\\mathbb{E}[X] = i^{-1} \\frac{d\\varphi_X(t)}{dt}\\bigg|_{t=0}$$\nFirst, we compute the derivative of $\\varphi_X(t)$ with respect to $t$:\n$$\\frac{d\\varphi_X(t)}{dt} = \\frac{d}{dt} \\exp\\left(i\\mu t - \\frac{\\sigma^2t^2}{2}\\right) = \\exp\\left(i\\mu t - \\frac{\\sigma^2t^2}{2}\\right) \\cdot \\left(i\\mu - \\sigma^2t\\right)$$\nEvaluating at $t=0$:\n$$\\frac{d\\varphi_X(t)}{dt}\\bigg|_{t=0} = \\exp(0) \\cdot (i\\mu - 0) = i\\mu$$\nTherefore, the mean is:\n$$\\mathbb{E}[X] = \\frac{1}{i}(i\\mu) = \\mu$$\nThis confirms the mean of the distribution.\n\nTo find the variance $\\operatorname{Var}(X)$, we first need the second raw moment, $\\mathbb{E}[X^2]$ ($n=2$):\n$$\\mathbb{E}[X^2] = i^{-2} \\frac{d^2\\varphi_X(t)}{dt^2}\\bigg|_{t=0} = -\\frac{d^2\\varphi_X(t)}{dt^2}\\bigg|_{t=0}$$\nWe compute the second derivative by differentiating the first derivative using the product rule:\n$$\\frac{d^2\\varphi_X(t)}{dt^2} = \\frac{d}{dt} \\left[ \\exp\\left(i\\mu t - \\frac{\\sigma^2t^2}{2}\\right) (i\\mu - \\sigma^2t) \\right]$$\n$$= \\left[\\frac{d}{dt}\\exp(\\dots)\\right](i\\mu - \\sigma^2t) + \\exp(\\dots)\\left[\\frac{d}{dt}(i\\mu - \\sigma^2t)\\right]$$\n$$= \\left[\\exp(\\dots)(i\\mu - \\sigma^2t)\\right](i\\mu - \\sigma^2t) + \\exp(\\dots)(-\\sigma^2)$$\n$$= \\exp\\left(i\\mu t - \\frac{\\sigma^2t^2}{2}\\right) \\left[ (i\\mu - \\sigma^2t)^2 - \\sigma^2 \\right]$$\nEvaluating at $t=0$:\n$$\\frac{d^2\\varphi_X(t)}{dt^2}\\bigg|_{t=0} = \\exp(0) \\left[ (i\\mu - 0)^2 - \\sigma^2 \\right] = (i\\mu)^2 - \\sigma^2 = -\\mu^2 - \\sigma^2$$\nSo, the second moment is:\n$$\\mathbb{E}[X^2] = -(-\\mu^2 - \\sigma^2) = \\mu^2 + \\sigma^2$$\nFinally, the variance is calculated as $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$:\n$$\\operatorname{Var}(X) = (\\mu^2 + \\sigma^2) - (\\mu)^2 = \\sigma^2$$\nThis confirms the variance of the distribution. The derivations from the characteristic function are consistent with the given parameters. The final answer required is the closed-form expression for $\\varphi_{X}(t)$.",
            "answer": "$$\\boxed{\\exp\\left(i \\mu t - \\frac{\\sigma^{2}t^{2}}{2}\\right)}$$"
        },
        {
            "introduction": "Stochastic bursts of gene expression are often modeled using the Poisson process, where discrete events like transcription initiation occur randomly over time. However, biological systems are replete with non-linearities, such as the saturation of a promoter by transcription factors. This practice challenges you to compute the expected value of a saturating function applied to a Poisson random variable, providing a concrete example of how to analyze the population-average behavior of a non-linear biochemical system using fundamental definitions and series manipulation techniques .",
            "id": "3907161",
            "problem": "A single gene in a homogeneous environment is modeled such that initiation events of transcription by ribonucleic acid polymerase occur as a time-homogeneous Poisson process with constant hazard. Over a fixed observation window, let $X$ denote the total number of transcription initiation events. Under these assumptions, $X$ is modeled as a Poisson random variable with parameter $\\lambda0$, written $X \\sim \\mathrm{Poisson}(\\lambda)$, where $\\lambda$ is the expected number of initiations in the window.\n\nTo capture saturation of a limiting factor (for example, effective promoter occupancy or shared resource usage) by discrete initiation events, consider the saturating transform $g(x)=\\dfrac{x}{1+x}$ for $x \\in \\{0,1,2,\\ldots\\}$. Starting only from the definition of the expectation of a function of a discrete random variable and the probability mass function of the Poisson distribution, and using only well-tested series facts, derive a closed-form analytic expression in terms of $\\lambda$ for the population-average saturation $\\,\\mathbb{E}[g(X)]\\,$. Clearly justify any series manipulations you perform. Provide your final result as an exact expression in terms of $\\lambda$ (no numerical rounding).",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in standard stochastic modeling of gene expression, well-posed with a clear objective and sufficient information, and expressed in precise, objective language. The problem is a formal mathematical derivation that is verifiable and not trivial. We may, therefore, proceed with the solution.\n\nThe objective is to compute the expectation of the function $g(X)$, denoted as $\\mathbb{E}[g(X)]$, where $X$ is a Poisson random variable, $X \\sim \\mathrm{Poisson}(\\lambda)$, and the function $g$ is defined as $g(x) = \\dfrac{x}{1+x}$ for $x$ in the set of non-negative integers $\\{0, 1, 2, \\ldots\\}$. The parameter $\\lambda$ is a positive real number, $\\lambda  0$.\n\nBy the definition of the expectation of a function of a discrete random variable, we have:\n$$\n\\mathbb{E}[g(X)] = \\sum_{k=0}^{\\infty} g(k) P(X=k)\n$$\nThe probability mass function (PMF) for a Poisson random variable $X$ with parameter $\\lambda$ is given by:\n$$\nP(X=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\ldots\\}\n$$\nSubstituting the expressions for $g(k)$ and $P(X=k)$ into the definition of expectation, we obtain:\n$$\n\\mathbb{E}[g(X)] = \\sum_{k=0}^{\\infty} \\left(\\frac{k}{1+k}\\right) \\frac{\\lambda^k \\exp(-\\lambda)}{k!}\n$$\nWe can factor the constant term $\\exp(-\\lambda)$ out of the summation:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\sum_{k=0}^{\\infty} \\frac{k}{1+k} \\frac{\\lambda^k}{k!}\n$$\nThe first term of the series, for $k=0$, is $\\frac{0}{1+0} \\frac{\\lambda^0}{0!} = 0$. Thus, the summation can start from $k=1$:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\sum_{k=1}^{\\infty} \\frac{k}{1+k} \\frac{\\lambda^k}{k!}\n$$\nTo proceed, we perform an algebraic manipulation on the term $\\frac{k}{1+k}$:\n$$\n\\frac{k}{1+k} = \\frac{(k+1)-1}{1+k} = 1 - \\frac{1}{1+k}\n$$\nSubstituting this back into the summation gives:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\sum_{k=1}^{\\infty} \\left(1 - \\frac{1}{1+k}\\right) \\frac{\\lambda^k}{k!}\n$$\nWe can split this into two separate summations:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) \\left( \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{k!} - \\sum_{k=1}^{\\infty} \\frac{1}{1+k} \\frac{\\lambda^k}{k!} \\right)\n$$\nThis manipulation is permissible because both individual series converge absolutely for any finite $\\lambda  0$. The first series is a component of the Taylor series for $\\exp(\\lambda)$, and the absolute convergence of the second series can be confirmed by the ratio test.\n\nLet us evaluate the first summation, $S_1 = \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{k!}$. We recall the well-known Taylor series expansion for the exponential function around $0$:\n$$\n\\exp(\\lambda) = \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = \\frac{\\lambda^0}{0!} + \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{k!} = 1 + S_1\n$$\nTherefore, the first summation is:\n$$\nS_1 = \\exp(\\lambda) - 1\n$$\nNext, let us evaluate the second summation, $S_2 = \\sum_{k=1}^{\\infty} \\frac{1}{1+k} \\frac{\\lambda^k}{k!}$. We can simplify the term inside the summation:\n$$\n\\frac{1}{1+k} \\frac{\\lambda^k}{k!} = \\frac{\\lambda^k}{(k+1)k!} = \\frac{\\lambda^k}{(k+1)!}\n$$\nSo the second summation becomes:\n$$\nS_2 = \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k+1)!}\n$$\nTo relate this to the exponential series, we need the power of $\\lambda$ to match the argument of the factorial. We can multiply and divide by $\\lambda \\neq 0$:\n$$\nS_2 = \\frac{1}{\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k+1}}{(k+1)!}\n$$\nLet's introduce a new index $j = k+1$. As $k$ goes from $1$ to $\\infty$, $j$ goes from $2$ to $\\infty$:\n$$\nS_2 = \\frac{1}{\\lambda} \\sum_{j=2}^{\\infty} \\frac{\\lambda^j}{j!}\n$$\nWe can express this sum in terms of the full exponential series:\n$$\n\\sum_{j=2}^{\\infty} \\frac{\\lambda^j}{j!} = \\left(\\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!}\\right) - \\frac{\\lambda^0}{0!} - \\frac{\\lambda^1}{1!} = \\exp(\\lambda) - 1 - \\lambda\n$$\nSubstituting this result back into the expression for $S_2$:\n$$\nS_2 = \\frac{1}{\\lambda} (\\exp(\\lambda) - 1 - \\lambda)\n$$\nNow, we combine the results for $S_1$ and $S_2$ into the expression for $\\mathbb{E}[g(X)]$:\n$$\n\\mathbb{E}[g(X)] = \\exp(-\\lambda) (S_1 - S_2) = \\exp(-\\lambda) \\left[ (\\exp(\\lambda) - 1) - \\frac{1}{\\lambda}(\\exp(\\lambda) - 1 - \\lambda) \\right]\n$$\nDistributing the $\\exp(-\\lambda)$ term:\n$$\n\\mathbb{E}[g(X)] = (\\exp(-\\lambda)\\exp(\\lambda) - \\exp(-\\lambda)) - \\frac{\\exp(-\\lambda)}{\\lambda}(\\exp(\\lambda) - 1 - \\lambda)\n$$\n$$\n\\mathbb{E}[g(X)] = (1 - \\exp(-\\lambda)) - \\frac{1}{\\lambda}(1 - \\exp(-\\lambda) - \\lambda\\exp(-\\lambda))\n$$\nDistributing the $-\\frac{1}{\\lambda}$ term:\n$$\n\\mathbb{E}[g(X)] = 1 - \\exp(-\\lambda) - \\frac{1}{\\lambda} + \\frac{\\exp(-\\lambda)}{\\lambda} + \\frac{\\lambda\\exp(-\\lambda)}{\\lambda}\n$$\n$$\n\\mathbb{E}[g(X)] = 1 - \\exp(-\\lambda) - \\frac{1}{\\lambda} + \\frac{\\exp(-\\lambda)}{\\lambda} + \\exp(-\\lambda)\n$$\nThe terms $-\\exp(-\\lambda)$ and $+\\exp(-\\lambda)$ cancel out:\n$$\n\\mathbb{E}[g(X)] = 1 - \\frac{1}{\\lambda} + \\frac{\\exp(-\\lambda)}{\\lambda}\n$$\nCombining the terms with $\\lambda$ in the denominator, we arrive at the final closed-form expression:\n$$\n\\mathbb{E}[g(X)] = 1 - \\frac{1 - \\exp(-\\lambda)}{\\lambda}\n$$",
            "answer": "$$\n\\boxed{1 - \\frac{1 - \\exp(-\\lambda)}{\\lambda}}\n$$"
        },
        {
            "introduction": "Averages can be deceiving. In complex biological experiments, failing to account for underlying variables can lead to conclusions that are the exact opposite of the truth, a phenomenon known as Simpson's paradox. This exercise provides a realistic synthetic biology dataset where the effect of a treatment on gene activation appears to reverse when data from different cell-cycle stages are aggregated. By calculating the conditional probabilities, you will uncover the paradox and understand how a confounding variable—in this case, cell cycle stage—can distort the interpretation of experimental results, a crucial lesson for robust data analysis .",
            "id": "3907106",
            "problem": "A high-throughput single-cell assay in a synthetic biology experiment evaluates a transcriptional activation circuit that drives a Green Fluorescent Protein (GFP) reporter. Cells are split into two cohorts: a perturbation that recruits an epigenetic activator to the promoter (Treatment) and a promoter-only construct (Control). Because cell cycle stage modulates chromatin accessibility, the data are also stratified by S-phase ($S$) versus G1-phase ($\\mathrm{G1}$), determined by a DNA-content stain. A cell is counted as reporter-positive if its GFP exceeds a pre-specified threshold.\n\nThe following counts were obtained:\n- $S$:\n  - Treatment: $700$ cells total, $196$ reporter-positive.\n  - Control: $300$ cells total, $90$ reporter-positive.\n- $\\mathrm{G1}$:\n  - Treatment: $300$ cells total, $27$ reporter-positive.\n  - Control: $700$ cells total, $70$ reporter-positive.\n\nAssume each cell is an independent and identically distributed draw from its corresponding cohort-stage subpopulation. Based on the fundamental definition of conditional probability and the law of total probability applied to these data, determine which statements are true.\n\nA. Within both $S$ and $\\mathrm{G1}$, the probability of reporter activation given Treatment is lower than the probability given Control.\n\nB. Aggregated over $S$ and $\\mathrm{G1}$ (ignoring stage), the probability of reporter activation is higher under Treatment than under Control.\n\nC. The reversal between the within-stage comparisons and the aggregated comparison is an instance of Simpson’s paradox.\n\nD. If Treatment and Control had the same stage distribution (that is, the same proportions in $S$ and $\\mathrm{G1}$), then the aggregated comparison would align with the within-stage comparisons.\n\nE. The paradoxical reversal occurs because the law of total probability is violated in these data.",
            "solution": "We start from the definition of conditional probability. For any event $A$ and conditioning event $B$ with $P(B)  0$, the conditional probability is $P(A \\mid B) = \\dfrac{P(A \\cap B)}{P(B)}$. When we have a partition of the sample space into disjoint events $\\{B_i\\}$ with $\\sum_i P(B_i) = 1$, the law of total probability states that for any event $A$, $P(A) = \\sum_i P(A \\mid B_i) P(B_i)$.\n\nWe interpret the empirical frequencies as probability estimates under the independent and identically distributed assumption for cells within each cohort-stage stratum. Thus, stage-specific activation probabilities are given by reporter-positive counts divided by totals within each stratum.\n\nCompute the stage-specific activation probabilities:\n\n- In $S$:\n  - Treatment: $P(\\text{On} \\mid \\text{Trt}, S) \\approx \\dfrac{196}{700} = 0.28$.\n  - Control: $P(\\text{On} \\mid \\text{Ctl}, S) \\approx \\dfrac{90}{300} = 0.30$.\n  Therefore, in $S$, Treatment has a lower activation probability than Control.\n\n- In $\\mathrm{G1}$:\n  - Treatment: $P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) \\approx \\dfrac{27}{300} = 0.09$.\n  - Control: $P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1}) \\approx \\dfrac{70}{700} = 0.10$.\n  Therefore, in $\\mathrm{G1}$, Treatment also has a lower activation probability than Control.\n\nNow compute the aggregated activation probabilities by ignoring stage. This uses the law of total probability with weights equal to the stage proportions within each cohort.\n\nFor Treatment:\n- Total Treatment cells: $700 + 300 = 1000$.\n- Total Treatment positives: $196 + 27 = 223$.\n- Aggregated $P(\\text{On} \\mid \\text{Trt}) \\approx \\dfrac{223}{1000} = 0.223$.\n\nFor Control:\n- Total Control cells: $300 + 700 = 1000$.\n- Total Control positives: $90 + 70 = 160$.\n- Aggregated $P(\\text{On} \\mid \\text{Ctl}) \\approx \\dfrac{160}{1000} = 0.160$.\n\nThus, aggregated over stages, Treatment exhibits a higher activation probability than Control, despite being lower within each stage. This is the hallmark of Simpson’s paradox: a trend that appears in several groups reverses when the data are combined, driven by differing group proportions.\n\nWe also analyze the counterfactual in which Treatment and Control share the same stage distribution. Let $w_S$ denote the common fraction in $S$ and $w_{\\mathrm{G1}} = 1 - w_S$ the fraction in $\\mathrm{G1}$. Then the aggregated difference would be\n$$\n\\Delta = \\bigl[w_S \\, P(\\text{On} \\mid \\text{Trt}, S) + w_{\\mathrm{G1}} \\, P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1})\\bigr] - \\bigl[w_S \\, P(\\text{On} \\mid \\text{Ctl}, S) + w_{\\mathrm{G1}} \\, P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1})\\bigr].\n$$\nRearranging,\n$$\n\\Delta = w_S \\bigl[P(\\text{On} \\mid \\text{Trt}, S) - P(\\text{On} \\mid \\text{Ctl}, S)\\bigr] + w_{\\mathrm{G1}} \\bigl[P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) - P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1})\\bigr].\n$$\nHere, $P(\\text{On} \\mid \\text{Trt}, S) - P(\\text{On} \\mid \\text{Ctl}, S) = 0.28 - 0.30 = -0.02$ and $P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) - P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1}) = 0.09 - 0.10 = -0.01$, both negative. With $w_S \\in [0,1]$ and $w_{\\mathrm{G1}} \\in [0,1]$, $\\Delta$ is a convex combination of negative numbers and must be negative. Therefore, if stage distributions were equal between cohorts, the aggregated comparison would agree with the within-stage comparisons, showing Treatment lower than Control.\n\nFinally, the law of total probability is not violated. The aggregated probabilities above are computed precisely by summing stage-specific probabilities weighted by the cohort-specific stage proportions. The reversal arises because the cohorts have different stage distributions (Treatment is overrepresented in the higher-activation stage $S$), not because of any probabilistic law being broken.\n\nOption-by-option analysis:\n\n- Option A: Correct. We computed $P(\\text{On} \\mid \\text{Trt}, S) = 0.28  0.30 = P(\\text{On} \\mid \\text{Ctl}, S)$ and $P(\\text{On} \\mid \\text{Trt}, \\mathrm{G1}) = 0.09  0.10 = P(\\text{On} \\mid \\text{Ctl}, \\mathrm{G1})$.\n\n- Option B: Correct. Aggregated, $P(\\text{On} \\mid \\text{Trt}) = 0.223  0.160 = P(\\text{On} \\mid \\text{Ctl})$.\n\n- Option C: Correct. The within-stage disadvantage for Treatment reverses to an apparent advantage when data are aggregated, which is Simpson’s paradox.\n\n- Option D: Correct. With matched stage distributions, the aggregated difference is a weighted average of negative within-stage differences and remains negative.\n\n- Option E: Incorrect. The law of total probability holds; the paradox arises from different mixing proportions across stages, not from any violation of probabilistic laws.",
            "answer": "$$\\boxed{ABCD}$$"
        }
    ]
}