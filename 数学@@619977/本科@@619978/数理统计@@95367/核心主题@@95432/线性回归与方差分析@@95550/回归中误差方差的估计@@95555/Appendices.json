{"hands_on_practices": [{"introduction": "在初步学习统计学时，我们知道样本方差的分母是 $n-1$。然而，在简单线性回归中，误差方差的无偏估计量 $S^2$ 的分母却是 $n-2$，这常常会引起初学者的困惑。这个练习将通过计算一个“直观”但有偏的估计量的期望值，来揭示为什么需要这种调整，从而加深对自由度概念的理解。[@problem_id:1915695]", "problem": "一位初级数据科学家正在使用一个简单线性回归（SLR）模型来分析实验数据。该模型由 $Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ 给出，其中 $i=1, \\dots, n$，$x_i$ 是固定的非随机预测变量，不完全相等，随机误差 $\\varepsilon_i$ 是独立的，且 $E[\\varepsilon_i] = 0$ 和 $V(\\varepsilon_i) = \\sigma^2$。\n\n参数 $\\beta_0$ 和 $\\beta_1$ 使用普通最小二乘法进行估计，得到估计量 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$。拟合值为 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$，残差为 $e_i = y_i - \\hat{y}_i$。\n\n已知的误差方差 $\\sigma^2$ 的标准无偏估计量是 $S^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2$。但是，这位分析师回想起入门统计课程中的样本方差公式，考虑使用一个不同的分母，并提出了一个误差方差的替代估计量：\n$$ \\hat{\\sigma}^2_{alt} = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n\n为了评估这个替代估计量，你的任务是确定它的期望值。将 $E[\\hat{\\sigma}^2_{alt}]$ 表示为样本量 $n$ 和真实误差方差 $\\sigma^2$ 的表达式。\n\n对于你的推导，你可以不加证明地使用以下简单线性回归理论中的标准结果：\n1. 最小二乘估计量 $\\hat{\\beta}_1$ 是 $\\beta_1$ 的一个无偏估计量。\n2. 斜率估计量的方差是 $V(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{x})^2}$。\n3. 残差平方和（也称为误差平方和或 SSE）可以分解为 $\\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n(y_i - \\bar{y})^2 - \\hat{\\beta}_1^2 \\sum_{i=1}^n(x_i - \\bar{x})^2$。", "solution": "我们被要求计算替代估计量的期望\n$$\n\\hat{\\sigma}^{2}_{alt}=\\frac{1}{n-1}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}=\\frac{1}{n-1}\\,\\mathrm{SSE},\n$$\n所以\n$$\nE[\\hat{\\sigma}^{2}_{alt}]=\\frac{1}{n-1}\\,E[\\mathrm{SSE}].\n$$\n使用给定的分解，\n$$\n\\mathrm{SSE}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}-\\hat{\\beta}_{1}^{2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2},\n$$\n令 $S_{xx}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$。那么\n$$\nE[\\mathrm{SSE}]=E\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]-E[\\hat{\\beta}_{1}^{2}]\\,S_{xx}.\n$$\n\n计算 $E\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]$。在 $y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\varepsilon_{i}$，其中 $\\bar{y}=\\beta_{0}+\\beta_{1}\\bar{x}+\\bar{\\varepsilon}$ 且 $\\bar{\\varepsilon}=\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_{i}$ 的条件下，我们有\n$$\ny_{i}-\\bar{y}=\\beta_{1}(x_{i}-\\bar{x})+(\\varepsilon_{i}-\\bar{\\varepsilon}).\n$$\n因此，\n$$\n\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}=\\beta_{1}^{2}S_{xx}+2\\beta_{1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})(\\varepsilon_{i}-\\bar{\\varepsilon})+\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}.\n$$\n取期望，并利用 $E[\\varepsilon_{i}]=0$、误差独立性和 $x_{i}$ 是固定的，得到\n$$\nE\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]=\\beta_{1}^{2}S_{xx}+E\\!\\left[\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}\\right].\n$$\n现在\n$$\n\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}=\\sum_{i=1}^{n}\\varepsilon_{i}^{2}-n\\bar{\\varepsilon}^{2},\n$$\n所以\n$$\nE\\!\\left[\\sum_{i=1}^{n}(\\varepsilon_{i}-\\bar{\\varepsilon})^{2}\\right]=n\\sigma^{2}-n\\,\\mathrm{Var}(\\bar{\\varepsilon})=n\\sigma^{2}-n\\frac{\\sigma^{2}}{n}=(n-1)\\sigma^{2}.\n$$\n因此\n$$\nE\\!\\left[\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right]=\\beta_{1}^{2}S_{xx}+(n-1)\\sigma^{2}.\n$$\n\n接下来，根据给定的结果，\n$$\nE[\\hat{\\beta}_{1}]=\\beta_{1},\\qquad \\mathrm{Var}(\\hat{\\beta}_{1})=\\frac{\\sigma^{2}}{S_{xx}},\n$$\n所以\n$$\nE[\\hat{\\beta}_{1}^{2}]=\\mathrm{Var}(\\hat{\\beta}_{1})+(E[\\hat{\\beta}_{1}])^{2}=\\frac{\\sigma^{2}}{S_{xx}}+\\beta_{1}^{2},\n$$\n并因此\n$$\nE[\\hat{\\beta}_{1}^{2}]\\,S_{xx}=\\sigma^{2}+\\beta_{1}^{2}S_{xx}.\n$$\n\n将各部分组合起来：\n$$\nE[\\mathrm{SSE}]=\\bigl(\\beta_{1}^{2}S_{xx}+(n-1)\\sigma^{2}\\bigr)-\\bigl(\\sigma^{2}+\\beta_{1}^{2}S_{xx}\\bigr)=(n-2)\\sigma^{2}.\n$$\n因此，\n$$\nE[\\hat{\\sigma}^{2}_{alt}]=\\frac{1}{n-1}E[\\mathrm{SSE}]=\\frac{n-2}{n-1}\\,\\sigma^{2}.\n$$", "answer": "$$\\boxed{\\frac{n-2}{n-1}\\sigma^{2}}$$", "id": "1915695"}, {"introduction": "掌握了标准回归模型中自由度的概念后，我们来探讨一个特殊但重要的变体：强制回归线通过原点的模型。这个练习以物理学中的胡克定律为背景，要求你推导该模型下误差方差的无偏估计量。通过这个过程，你将认识到自由度的计算并非一成不变，而是取决于模型中所估计参数的数量，这有助于你建立一个更普适的理解。[@problem_id:1915696]", "problem": "一名物理系学生正在研究胡克定律 (Hooke's Law)，该定律指出，将弹簧从其平衡位置拉伸或压缩一定距离 $x$ 所需的力 $F$ 与该距离成正比。该关系的理论模型为 $F = \\beta_1 x$，其中 $\\beta_1$ 是弹簧常数。\n\n为了估计弹簧常数，该学生收集了 $n$ 组测量数据 $(x_i, y_i)$，其中 $x_i$ 是位移，$y_i$ 是测得的力。该学生提出了一个过原点回归模型来描述这些数据：\n$$Y_i = \\beta_1 x_i + \\epsilon_i \\quad \\text{for } i = 1, 2, \\dots, n$$\n在此模型中，解释变量 $x_i$ 被视为固定的非随机常数，并且 $x_i$ 不全为零。误差项 $\\epsilon_i$ 被假定为独立同分布的随机变量，其均值为 $E[\\epsilon_i] = 0$，方差为常数 $Var(\\epsilon_i) = \\sigma^2$。参数 $\\sigma^2$ 代表力的测量误差的方差。\n\n弹簧常数 $\\beta_1$ 的最小二乘估计量由下式给出：\n$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i Y_i}{\\sum_{i=1}^n x_i^2}$$\n\n为了估计误差方差 $\\sigma^2$，该学生考虑了以下形式的估计量：\n$$S^2 = k \\sum_{i=1}^n (Y_i - \\hat{\\beta}_1 x_i)^2$$\n其中 $k$ 是一个可能与样本大小 $n$ 有关的常数。\n\n求使 $S^2$ 成为 $\\sigma^2$ 的无偏估计量的 $k$ 值。请用 $n$ 的函数表示你的答案。", "solution": "我们将模型写成矩阵形式。令 $Y=(Y_{1},\\dots,Y_{n})'$，$X=(x_{1},\\dots,x_{n})'$ (一个 $n\\times 1$ 矩阵)，$\\beta=\\beta_{1}$，以及 $\\epsilon=(\\epsilon_{1},\\dots,\\epsilon_{n})'$。则\n$$\nY=X\\beta+\\epsilon,\\quad E[\\epsilon]=0,\\quad \\operatorname{Var}(\\epsilon)=\\sigma^{2}I_{n},\n$$\n其中 $X$ 非零，因此 $X'X>0$。最小二乘估计量为\n$$\n\\hat{\\beta}=(X'X)^{-1}X'Y,\n$$\n残差向量为\n$$\ne=Y-X\\hat{\\beta}=(I_{n}-H)Y,\\quad H=X(X'X)^{-1}X'.\n$$\n使用 $Y=X\\beta+\\epsilon$ 和 $(I_{n}-H)X=0$，我们得到\n$$\ne=(I_{n}-H)\\epsilon.\n$$\n残差平方和为\n$$\n\\operatorname{RSS}=e'e=\\epsilon'(I_{n}-H)\\epsilon.\n$$\n对于任意常数对称矩阵 $A$，以下恒等式成立\n$$\nE[\\epsilon'A\\epsilon]=\\operatorname{tr}\\!\\big(A\\,\\operatorname{Var}(\\epsilon)\\big)+E[\\epsilon]'A\\,E[\\epsilon]\n$$\n当 $A=I_{n}-H$，$E[\\epsilon]=0$ 且 $\\operatorname{Var}(\\epsilon)=\\sigma^{2}I_{n}$ 时，我们得到\n$$\nE[\\operatorname{RSS}]=\\sigma^{2}\\operatorname{tr}(I_{n}-H)=\\sigma^{2}\\big(n-\\operatorname{tr}(H)\\big).\n$$\n帽子矩阵 $H$ 是一个秩为 $p=1$ 的幂等投影，因为 $X$ 的秩为 $1$ (并非所有 $x_{i}$ 都为零)。因此 $\\operatorname{tr}(H)=1$，所以\n$$\nE[\\operatorname{RSS}]=(n-1)\\sigma^{2}.\n$$\n所提出的估计量为 $S^{2}=k\\,\\operatorname{RSS}$。其期望为\n$$\nE[S^{2}]=k\\,(n-1)\\sigma^{2}.\n$$\n无偏性，即 $E[S^{2}]=\\sigma^{2}$，因此要求\n$$\nk=\\frac{1}{n-1}.\n$$", "answer": "$$\\boxed{\\frac{1}{n-1}}$$", "id": "1915696"}, {"introduction": "为了真正内化自由度的概念，让我们考虑一个极限情况：仅用两个数据点拟合一条回归直线。这个练习是一个思想实验，它揭示了当数据量等于模型参数数量时会发生什么。通过分析这种情况下的残差平方和（$SSE$）与误差方差估计量（$s^2$），你将直观地理解为什么估计模型参数会“消耗”数据中的信息，以及为什么需要足够的“剩余”信息（即自由度）来可靠地估计误差。[@problem_id:1915683]", "problem": "一位分析师正在研究两个变量 $X$ 和 $Y$ 之间的关系。他们收集了一个仅包含两个数据点的小型数据集：$(x_1, y_1) = (10, 25)$ 和 $(x_2, y_2) = (30, 35)$。\n\n分析师对该数据集拟合了一个形式为 $Y = \\beta_0 + \\beta_1 X + \\epsilon$ 的简单线性回归模型，其中 $\\epsilon$ 代表随机误差项。该过程涉及为模型参数寻找估计值 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$。\n\n令 $SSE$ 表示拟合模型的误差平方和（也称为残差平方和），其计算公式为 $SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$，其中 $\\hat{y}_i$ 是来自回归线的预测值，而 $n$ 是数据点的数量。\n\n令 $s^2$ 为误差方差 $\\sigma^2$ 的标准无偏估计量。该估计量通常计算为 $s^2 = \\frac{SSE}{n-k}$，其中 $k$ 是回归模型中估计的参数数量。\n\n基于对给定的两个数据点拟合简单线性回归模型，确定得到的 $SSE$ 和 $s^2$ 的值。\n\nA. $SSE = 50$ 且 $s^2 = 50$\n\nB. $SSE > 0$ 且 $s^2 > 0$，但根据所给信息无法确定其确切值。\n\nC. $SSE = 0$ 且 $s^2 = 0$\n\nD. $SSE = 0$ 且 $s^2$ 未定义\n\nE. $SSE = 100$ 且 $s^2$ 未定义", "solution": "我们将简单线性回归模型 $Y=\\beta_{0}+\\beta_{1}X+\\epsilon$ 拟合到两个点 $(x_{1},y_{1})=(10,25)$和 $(x_{2},y_{2})=(30,35)$上。简单线性回归中的普通最小二乘估计值为\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}}, \\quad \\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1}\\bar{x},\n$$\n其中\n$$\n\\bar{x}=\\frac{x_{1}+x_{2}}{2}, \\quad \\bar{y}=\\frac{y_{1}+y_{2}}{2}, \\quad S_{xx}=\\sum_{i=1}^{2}(x_{i}-\\bar{x})^{2}, \\quad S_{xy}=\\sum_{i=1}^{2}(x_{i}-\\bar{x})(y_{i}-\\bar{y}).\n$$\n计算样本均值：\n$$\n\\bar{x}=\\frac{10+30}{2}=20, \\quad \\bar{y}=\\frac{25+35}{2}=30.\n$$\n计算 $S_{xx}$ 和 $S_{xy}$：\n$$\nS_{xx}=(10-20)^{2}+(30-20)^{2}=100+100=200,\n$$\n$$\nS_{xy}=(10-20)(25-30)+(30-20)(35-30)=(-10)(-5)+(10)(5)=50+50=100.\n$$\n因此，\n$$\n\\hat{\\beta}_{1}=\\frac{100}{200}=\\frac{1}{2}, \\quad \\hat{\\beta}_{0}=30-\\frac{1}{2}\\cdot 20=20.\n$$\n拟合的直线是 $\\hat{y}=20+\\frac{1}{2}x$。在观测到的 $x$ 处的预测值为\n$$\n\\hat{y}_{1}=20+\\tfrac{1}{2}\\cdot 10=25=y_{1}, \\quad \\hat{y}_{2}=20+\\tfrac{1}{2}\\cdot 30=35=y_{2}.\n$$\n因此，两个残差都为零，所以误差平方和为\n$$\nSSE=\\sum_{i=1}^{2}(y_{i}-\\hat{y}_{i})^{2}=0.\n$$\n无偏误差方差估计量定义为\n$$\ns^{2}=\\frac{SSE}{n-k},\n$$\n其中有 $n=2$ 个数据点和 $k=2$ 个估计参数（截距和斜率），得出 $n-k=0$。因此，\n$$\ns^{2}=\\frac{0}{0},\n$$\n由于自由度为零，该值是未定义的。因此，正确的选项是 $SSE=0$ 且 $s^{2}$ 未定义。", "answer": "$$\\boxed{D}$$", "id": "1915683"}]}