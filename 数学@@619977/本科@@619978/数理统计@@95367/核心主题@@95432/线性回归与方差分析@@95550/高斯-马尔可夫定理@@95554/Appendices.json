{"hands_on_practices": [{"introduction": "当我们对同一个物理量有多个精度不同的测量值时，如何才能将它们组合成一个最优的估计值？这个问题是实验科学中的一个核心问题。本练习将引导你从第一性原理出发，推导当测量误差不同（即存在异方差）时，均值 $\\mu$ 的最佳线性无偏估计量 (Best Linear Unbiased Estimator, BLUE)，从而揭示出优美而直观的反方差加权思想。[@problem_id:1919575]", "problem": "一个由 $n$ 个独立量子传感器组成的阵列用于测量一个基本物理常数，其真实值为 $\\mu$。每个传感器 $i$ 提供一次测量值 $y_i$。每个传感器的测量模型由 $y_i = \\mu + \\epsilon_i$ 给出，其中 $\\epsilon_i$ 是测量误差。已知误差的均值为零，即 $E[\\epsilon_i] = 0$，并且传感器之间不相关，即对于 $i \\neq j$，有 $Cov(\\epsilon_i, \\epsilon_j) = 0$。然而，由于轻微的制造差异，这些传感器的精度不同。传感器 $i$ 的误差方差是已知的，由 $Var(\\epsilon_i) = \\sigma_i^2$ 给出。\n\n为了估计 $\\mu$，我们决定使用一个线性估计量，其一般形式为 $\\hat{\\mu} = \\sum_{i=1}^n c_i y_i$，其中 $c_i$ 是待确定的常数系数。我们要求这个估计量是无偏的，即其期望值必须等于真实值，$E[\\hat{\\mu}] = \\mu$。在所有可能的线性无偏估计量中，我们寻求“最佳”的一个，定义为具有最小可能方差 $Var(\\hat{\\mu})$ 的估计量。\n\n找出这个关于 $\\mu$ 的最佳线性无偏估计量(BLUE)的解析表达式，该表达式应由测量值 $y_i$ 及其对应的误差方差 $\\sigma_i^2$ 表示。", "solution": "我们将每次测量建模为 $y_{i}=\\mu+\\epsilon_{i}$，其中 $E[\\epsilon_{i}]=0$，$Var(\\epsilon_{i})=\\sigma_{i}^{2}$，且对于 $i\\neq j$ 有 $Cov(\\epsilon_{i},\\epsilon_{j})=0$。考虑一个线性估计量 $\\hat{\\mu}=\\sum_{i=1}^{n}c_{i}y_{i}$。\n\n无偏性要求 $E[\\hat{\\mu}]=\\mu$。由于 $E[y_{i}]=\\mu$，我们有\n$$\nE[\\hat{\\mu}]=\\sum_{i=1}^{n}c_{i}E[y_{i}]=\\mu\\sum_{i=1}^{n}c_{i}.\n$$\n因此无偏性约束是\n$$\n\\sum_{i=1}^{n}c_{i}=1.\n$$\n\n利用误差不相关的特性，$\\hat{\\mu}$ 的方差为\n$$\nVar(\\hat{\\mu})=Var\\!\\left(\\sum_{i=1}^{n}c_{i}y_{i}\\right)=Var\\!\\left(\\sum_{i=1}^{n}c_{i}\\epsilon_{i}\\right)=\\sum_{i=1}^{n}c_{i}^{2}\\sigma_{i}^{2}.\n$$\n我们使用拉格朗日乘子 $\\lambda$，在约束条件 $\\sum_{i=1}^{n}c_{i}=1$ 下最小化 $\\sum_{i=1}^{n}c_{i}^{2}\\sigma_{i}^{2}$。定义\n$$\n\\mathcal{L}(c_{1},\\dots,c_{n},\\lambda)=\\sum_{i=1}^{n}c_{i}^{2}\\sigma_{i}^{2}-\\lambda\\left(\\sum_{i=1}^{n}c_{i}-1\\right).\n$$\n对每个 $i$ 将偏导数设为零，得到\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial c_{i}}=2c_{i}\\sigma_{i}^{2}-\\lambda=0\\quad\\Rightarrow\\quad c_{i}=\\frac{\\lambda}{2\\sigma_{i}^{2}}.\n$$\n代入约束条件，\n$$\n\\sum_{i=1}^{n}c_{i}=1\\quad\\Rightarrow\\quad \\frac{\\lambda}{2}\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}=1\\quad\\Rightarrow\\quad \\lambda=\\frac{2}{\\sum_{j=1}^{n}\\frac{1}{\\sigma_{j}^{2}}}.\n$$\n因此，\n$$\nc_{i}=\\frac{\\frac{1}{\\sigma_{i}^{2}}}{\\sum_{j=1}^{n}\\frac{1}{\\sigma_{j}^{2}}}.\n$$\n那么，最佳线性无偏估计量(BLUE)就是反方差加权平均值\n$$\n\\hat{\\mu}=\\sum_{i=1}^{n}c_{i}y_{i}=\\frac{\\sum_{i=1}^{n}\\frac{y_{i}}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}}.\n$$\n其最小方差为 $Var(\\hat{\\mu})=\\left(\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}\\right)^{-1}$，这证实了它在线性无偏估计量中的最优性。", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n}\\frac{y_{i}}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n}\\frac{1}{\\sigma_{i}^{2}}}}$$", "id": "1919575"}, {"introduction": "高斯-马尔可夫定理指出，在满足特定假设的条件下，普通最小二乘 (Ordinary Least Squares, OLS) 估计量是最佳线性无偏估计量。这个练习将理论付诸实践，让你在一个简单的线性回归模型中具体计算 OLS 斜率估计量 $\\hat{\\beta}_1$ 的方差。掌握这个计算是理解“最佳”一词含义的关键，也是体会 OLS 效率的重要一步。[@problem_id:1919549]", "problem": "在一次物理入门实验课中，一名学生研究了弹簧的伸长量与其上悬挂的质量之间的关系。该学生提出了一个简单的线性模型，用来描述因变量 $y_i$ 与自变量 $x_i$ 之间的关系。该模型为 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$，$i=1, 2, 3$。其中，$\\beta_0$ 和 $\\beta_1$ 分别是未知的截距和斜率参数。项 $\\epsilon_i$ 代表第 $i$ 次观测的随机测量误差。\n\n经典线性回归模型的标准假设成立：\n1. 误差项的期望值为零：$E[\\epsilon_i] = 0$。\n2. 误差具有恒定方差（同方差性）：$\\text{Var}(\\epsilon_i) = \\sigma^2$，其中 $\\sigma^2$ 是某个正常数。\n3. 各误差项之间不相关：对于所有 $i \\neq j$，有 $\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0$。\n\n学生进行了三次测量，选择将自变量 $x$ 的值分别设为 $x_1 = -1$，$x_2 = 0$ 和 $x_3 = 1$。参数 $\\beta_0$ 和 $\\beta_1$ 使用普通最小二乘法（OLS）进行估计。根据 Gauss-Markov 定理，斜率的OLS估计量（记为 $\\hat{\\beta_1}$）在所有线性无偏估计量中具有最小方差。\n\n确定 OLS 估计量 $\\hat{\\beta_1}$ 的方差。将答案表示为以误差方差 $\\sigma^2$ 表示的解析表达式。", "solution": "我们将数据建模为 $y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i}$，其中 $E[\\epsilon_{i}]=0$，$\\text{Var}(\\epsilon_{i})=\\sigma^{2}$，且对于 $i\\neq j$ 有 $\\text{Cov}(\\epsilon_{i},\\epsilon_{j})=0$。写成矩阵形式，设 $y=X\\beta+\\epsilon$，其中\n$$\nX=\\begin{pmatrix}\n1 & x_{1}\\\\\n1 & x_{2}\\\\\n1 & x_{3}\n\\end{pmatrix},\\quad\n\\beta=\\begin{pmatrix}\\beta_{0}\\\\ \\beta_{1}\\end{pmatrix},\\quad\n\\epsilon=\\begin{pmatrix}\\epsilon_{1}\\\\ \\epsilon_{2}\\\\ \\epsilon_{3}\\end{pmatrix}.\n$$\nOLS 估计量为 $\\hat{\\beta}=(X^{\\top}X)^{-1}X^{\\top}y$，在所述假设下，其协方差矩阵为\n$$\n\\text{Var}(\\hat{\\beta})=\\sigma^{2}(X^{\\top}X)^{-1}.\n$$\n当 $x_{1}=-1$，$x_{2}=0$，$x_{3}=1$ 时，我们计算\n$$\nX^{\\top}X=\\begin{pmatrix}\n\\sum_{i=1}^{3}1 & \\sum_{i=1}^{3}x_{i}\\\\\n\\sum_{i=1}^{3}x_{i} & \\sum_{i=1}^{3}x_{i}^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n3 & (-1)+0+1\\\\\n(-1)+0+1 & (-1)^{2}+0^{2}+1^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n3 & 0\\\\\n0 & 2\n\\end{pmatrix}.\n$$\n由于 $X^{\\top}X$ 是对角矩阵，其逆矩阵为\n$$\n(X^{\\top}X)^{-1}=\\begin{pmatrix}\n\\frac{1}{3} & 0\\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}.\n$$\n因此，\n$$\n\\text{Var}(\\hat{\\beta})=\\sigma^{2}\\begin{pmatrix}\n\\frac{1}{3} & 0\\\\\n0 & \\frac{1}{2}\n\\end{pmatrix},\n$$\n所以 OLS 斜率估计量的方差是第 $(2,2)$ 个元素，\n$$\n\\text{Var}(\\hat{\\beta}_{1})=\\sigma^{2}\\cdot\\frac{1}{2}=\\frac{\\sigma^{2}}{2}.\n$$", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{2}}$$", "id": "1919549"}, {"introduction": "深刻理解一个定理，意味着需要探索其成立的边界条件，以及当这些条件被违背时会发生什么。如果随机误差项 $\\epsilon_i$ 的均值不为零，OLS 估计量的优良性质会受到怎样的影响？本练习旨在探究当存在系统性偏差时，斜率估计量 $\\hat{\\beta}_1$ 是否仍然保持无偏性，从而帮助你辨析高斯-马尔可夫定理的各项假设分别对哪些结论至关重要。[@problem_id:1919602]", "problem": "考虑一个用于分析包含 $n$ 个观测值 $(x_i, y_i)$ 的数据集的简单线性回归模型：\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n其中 $i = 1, 2, \\dots, n$。预测变量 $x_i$ 被认为是固定的（非随机的）。截距和斜率的普通最小二乘 (OLS) 估计量分别用 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$ 表示。\n\n通常，高斯-马尔可夫定理的一个核心假设是误差项的期望值为零，即 $E[\\epsilon_i] = 0$。假设这个假设不成立，并且存在一个系统性测量误差，使得误差项具有一个恒定的非零均值：\n$$E[\\epsilon_i] = c$$\n其中 $c$ 是一个非零实常数。所有其他标准假设均成立，包括误差具有恒定方差（同方差性）且不相关。令 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$。\n\nOLS 斜率估计量 $E[\\hat{\\beta}_1]$ 的期望值是多少？\n\nA. $\\beta_1$\n\nB. $\\beta_1 + c$\n\nC. $\\beta_1 + \\frac{c}{\\bar{x}}$\n\nD. $\\beta_1 - c$\n\nE. $\\beta_1 + \\frac{c n}{\\sum_{i=1}^n (x_i - \\bar{x})}$", "solution": "给定具有固定回归量的简单线性回归模型：\n$$\ny_i = \\beta_{0} + \\beta_{1} x_i + \\epsilon_i, \\quad E[\\epsilon_i] = c, \\quad i = 1,\\dots,n,\n$$\n以及 OLS 斜率估计量\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}.\n$$\n使用恒等式 $\\sum_{i=1}^{n} (x_i - \\bar{x}) \\bar{y} = \\bar{y} \\sum_{i=1}^{n} (x_i - \\bar{x}) = 0$，我们可以等价地写出\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) y_i}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}.\n$$\n代入 $y_i = \\beta_{0} + \\beta_{1} x_i + \\epsilon_i$：\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) y_i\n= \\sum_{i=1}^{n} (x_i - \\bar{x})(\\beta_{0} + \\beta_{1} x_i + \\epsilon_i)\n= \\beta_{0} \\sum_{i=1}^{n} (x_i - \\bar{x}) + \\beta_{1} \\sum_{i=1}^{n} (x_i - \\bar{x}) x_i + \\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i.\n$$\n由于 $\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0$，第一项为零。对于第二项，将 $x_i$ 写成 $(x_i - \\bar{x}) + \\bar{x}$ 可得\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) x_i\n= \\sum_{i=1}^{n} (x_i - \\bar{x})\\big((x_i - \\bar{x}) + \\bar{x}\\big)\n= \\sum_{i=1}^{n} (x_i - \\bar{x})^{2} + \\bar{x} \\sum_{i=1}^{n} (x_i - \\bar{x})\n= \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}.\n$$\n因此，\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) y_i = \\beta_{1} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2} + \\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i,\n$$\n所以\n$$\n\\hat{\\beta}_{1} = \\beta_{1} + \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}.\n$$\n在固定 $x_i$ 的条件下取期望，并使用 $E[\\epsilon_i] = c$，\n$$\nE\\left[\\sum_{i=1}^{n} (x_i - \\bar{x}) \\epsilon_i\\right]\n= \\sum_{i=1}^{n} (x_i - \\bar{x}) E[\\epsilon_i]\n= c \\sum_{i=1}^{n} (x_i - \\bar{x})\n= 0.\n$$\n因此，\n$$\nE[\\hat{\\beta}_{1}] = \\beta_{1} + \\frac{0}{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}} = \\beta_{1}.\n$$\n因此，正确选项是 A。", "answer": "$$\\boxed{A}$$", "id": "1919602"}]}