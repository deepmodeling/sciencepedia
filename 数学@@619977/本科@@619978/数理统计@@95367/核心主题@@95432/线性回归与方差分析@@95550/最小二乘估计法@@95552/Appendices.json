{"hands_on_practices": [{"introduction": "最小二乘法的核心思想是找到一组参数，使得模型预测值与实际观测值之间的残差平方和最小。这个练习将带你从最基础的一步开始：为一个给定的数据集构建这个需要被最小化的目标函数[@problem_id:1935126]。通过亲手推导平方和函数 $S(\\beta_0, \\beta_1)$ 的具体形式，你将把抽象的优化目标转化为一个具体的数学表达式，从而深刻理解最小二乘法的本质。", "problem": "在统计建模中，通常使用简单线性回归模型来描述预测变量 $x$ 和响应变量 $y$ 之间的关系。该模型由方程 $y = \\beta_0 + \\beta_1 x$ 给出，其中 $\\beta_0$ 代表 y 轴截距，$\\beta_1$ 代表斜率。最小二乘法提供了一种估计参数 $\\beta_0$ 和 $\\beta_1$ 的方法，其通过找到一条直线，使观测响应值与线性模型预测的响应值之间的平方差之和最小化。\n\n假设一个实验得出以下三个数据点 $(x, y)$:\n$$ \\{(0, 1), (1, 3), (2, 4)\\} $$\n为了找到最小二乘回归线，必须最小化一个函数 $S(\\beta_0, \\beta_1)$，该函数代表此数据集的残差平方和。\n\n确定函数 $S(\\beta_0, \\beta_1)$ 关于参数 $\\beta_0$ 和 $\\beta_1$ 的显式形式。", "solution": "最小二乘准则旨在最小化残差平方和。对于线性模型 $y=\\beta_{0}+\\beta_{1}x$ 和数据点 $(x_{i},y_{i})$，第 $i$ 个点的残差是 $r_{i}=y_{i}-(\\beta_{0}+\\beta_{1}x_{i})$，残差平方和为\n$$\nS(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left[y_{i}-(\\beta_{0}+\\beta_{1}x_{i})\\right]^{2}.\n$$\n对于给定的三个点 $(0,1)$, $(1,3)$ 和 $(2,4)$，上式变为\n$$\nS(\\beta_{0},\\beta_{1})=\\left[1-(\\beta_{0}+\\beta_{1}\\cdot 0)\\right]^{2}+\\left[3-(\\beta_{0}+\\beta_{1}\\cdot 1)\\right]^{2}+\\left[4-(\\beta_{0}+\\beta_{1}\\cdot 2)\\right]^{2}.\n$$\n化简平方项中的表达式可得\n$$\nS(\\beta_{0},\\beta_{1})=(1-\\beta_{0})^{2}+(3-\\beta_{0}-\\beta_{1})^{2}+(4-\\beta_{0}-2\\beta_{1})^{2}.\n$$\n将每一项展开：\n$$\n(1-\\beta_{0})^{2}=\\beta_{0}^{2}-2\\beta_{0}+1,\n$$\n$$\n(3-\\beta_{0}-\\beta_{1})^{2}=\\beta_{0}^{2}+2\\beta_{0}\\beta_{1}+\\beta_{1}^{2}-6\\beta_{0}-6\\beta_{1}+9,\n$$\n$$\n(4-\\beta_{0}-2\\beta_{1})^{2}=\\beta_{0}^{2}+4\\beta_{0}\\beta_{1}+4\\beta_{1}^{2}-8\\beta_{0}-16\\beta_{1}+16.\n$$\n将各项相加并合并同类项，得到\n$$\nS(\\beta_{0},\\beta_{1})=3\\beta_{0}^{2}+6\\beta_{0}\\beta_{1}+5\\beta_{1}^{2}-16\\beta_{0}-22\\beta_{1}+26.\n$$", "answer": "$$\\boxed{3 \\beta_{0}^{2} + 6 \\beta_{0} \\beta_{1} + 5 \\beta_{1}^{2} - 16 \\beta_{0} - 22 \\beta_{1} + 26}$$", "id": "1935126"}, {"introduction": "理解了如何构建目标函数后，让我们通过一个理想化的思想实验来检验最小二乘法的行为。如果所有数据点都完美地落在一条直线上，那么“最佳拟合线”直觉上就应该是这条直线本身[@problem_id:1935161]。这个练习将验证这一直觉，展示在数据不存在任何随机误差的理想情况下，最小二乘估计量将精确地还原出真实的模型参数，并且残差平方和（SSE）为零。", "problem": "考虑一个简单线性回归模型 $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$，其中 $\\beta_0$ 和 $\\beta_1$ 是模型参数，$\\epsilon_i$ 是独立同分布的随机误差项。进行一项实验，收集了一个包含 $n$ 个数据点 $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$ 的数据集，其中 $n \\geq 2$ 且 $x_i$ 的值不完全相同。\n\n在分析数据时发现，每一个数据点都完美满足线性关系 $y_i = 2x_i + 1$。\n\n设 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$ 分别是 $\\beta_0$ 和 $\\beta_1$ 的标准最小二乘估计量。设 SSE 表示拟合回归线的残差平方和。\n\n计算表达式 $3\\hat{\\beta}_0 + 7\\hat{\\beta}_1 + \\text{SSE}$ 的数值。", "solution": "目标是求出最小二乘估计量 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$ 的值，以及残差平方和 (SSE)，已知数据点 $(x_i, y_i)$ 完美地落在直线 $y = 2x + 1$ 上。\n\n最小二乘法旨在找到 $\\beta_0$ 和 $\\beta_1$ 的值，以最小化观测值 $y_i$ 与模型预测值 $\\hat{y}_i = \\beta_0 + \\beta_1 x_i$ 之间差值的平方和。这个和就是目标函数 $S(\\beta_0, \\beta_1)$：\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2$$\n最小二乘估计量 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$ 是使该函数最小化的特定 $\\beta_0$ 和 $\\beta_1$ 的值。残差平方和 SSE 是该函数的最小值，即 $\\text{SSE} = S(\\hat{\\beta}_0, \\hat{\\beta}_1)$。\n\n题目给出对于每个数据点，都有 $y_i = 2x_i + 1$。我们将此信息代入目标函数 $S(\\beta_0, \\beta_1)$：\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} ((2x_i + 1) - (\\beta_0 + \\beta_1 x_i))^2$$\n这可以重写为：\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} ((1 - \\beta_0) + (2 - \\beta_1)x_i)^2$$\n函数 $S(\\beta_0, \\beta_1)$ 表示一个平方项的和。平方和总是非负的。$S(\\beta_0, \\beta_1)$ 的最小可能值为 0。当且仅当求和中的每一项都为零时，才能达到这个最小值。\n因此，我们需要找到是否存在 $\\beta_0$ 和 $\\beta_1$ 的值，使得对于所有的 $i=1, \\dots, n$：\n$$(1 - \\beta_0) + (2 - \\beta_1)x_i = 0$$\n让我们看看是否能找到这样的 $\\beta_0$ 和 $\\beta_1$。如果我们选择 $\\beta_0 = 1$ 和 $\\beta_1 = 2$，表达式变为：\n$$(1 - 1) + (2 - 2)x_i = 0 + 0 \\cdot x_i = 0$$\n这个等式对所有的 $x_i$ 值和所有的 $i=1, \\dots, n$ 都成立。\n\n因此，通过选择 $\\beta_0 = 1$ 和 $\\beta_1 = 2$，目标函数 $S(\\beta_0, \\beta_1)$ 达到其绝对最小值 0。\n$$S(1, 2) = \\sum_{i=1}^{n} (0)^2 = 0$$\n根据定义，最小二乘估计量是使 $S(\\beta_0, \\beta_1)$ 最小化的值。我们已经发现这些值是 $\\beta_0=1$ 和 $\\beta_1=2$。\n因此，最小二乘估计量为：\n$$\\hat{\\beta}_0 = 1$$\n$$\\hat{\\beta}_1 = 2$$\n残差平方和 (SSE) 是在这些估计量处计算的目标函数的值：\n$$\\text{SSE} = S(\\hat{\\beta}_0, \\hat{\\beta}_1) = S(1, 2) = 0$$\n这在直观上是合理的：如果所有数据点都完美地在一条直线上，“最佳拟合线”必然就是那条直线，并且误差（点到直线的距离）必然都为零。\n\n最后，我们需要计算表达式 $3\\hat{\\beta}_0 + 7\\hat{\\beta}_1 + \\text{SSE}$ 的值。代入我们求得的值：\n$$3\\hat{\\beta}_0 + 7\\hat{\\beta}_1 + \\text{SSE} = 3(1) + 7(2) + 0$$\n$$= 3 + 14 + 0$$\n$$= 17$$", "answer": "$$\\boxed{17}$$", "id": "1935161"}, {"introduction": "当我们通过最小二乘法找到最佳拟合线后，模型未能解释的部分就体现在残差（$e_i = y_i - \\hat{y}_i$）中。这些残差是完全随机的吗，还是它们具有某些系统性的性质？这个练习将引导你发现最小二乘法的一个核心性质：残差与原始的预测变量在代数上是正交的（不相关的）[@problem_id:1935149]。通过对残差本身进行回归分析，你将亲手验证这一深刻的几何特性，它不仅是理论上的一个精妙结果，更是后续进行回归诊断和假设检验的基石。", "problem": "在统计建模的背景下，考虑对一个包含 $n$ 对观测值 $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$ 的数据集进行简单线性回归，我们假设并非所有的 $x_i$ 值都相同。模型由 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ 给出。使用最小二乘法来求得截距 $\\hat{\\beta}_0$ 和斜率 $\\hat{\\beta}_1$ 的估计量。\n\n根据这个初始回归，拟合值计算为 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$。然后对每个观测值计算相应的残差，即 $e_i = y_i - \\hat{y}_i$。\n\n现在，假设你进行第二次新的简单线性回归。在这个新的回归中，你将计算出的残差 $e_i$ 作为新的响应变量，并将原始的 $x_i$ 值作为预测变量。这个新模型的形式为 $e_i = \\gamma_0 + \\gamma_1 x_i + \\delta_i$。\n\n确定对于这第二次回归，其截距 $\\hat{\\gamma}_0$ 和斜率 $\\hat{\\gamma}_1$ 的最小二乘估计量的值。将最终答案表示为一个包含两个元素的行矩阵，其中第一个元素是 $\\hat{\\gamma}_0$ 的值，第二个元素是 $\\hat{\\gamma}_1$ 的值。", "solution": "考虑第一个最小二乘问题，其模型为 $y_i=\\beta_{0}+\\beta_{1}x_i+\\epsilon_i$，残差为 $e_i=y_i-\\hat{y}_i=y_i-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_i$。最小二乘估计量 $(\\hat{\\beta}_{0},\\hat{\\beta}_{1})$ 最小化\n$$\nS(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)^{2}.\n$$\n通过将偏导数设为零，可以得到正规方程组：\n$$\n\\frac{\\partial S}{\\partial \\beta_{0}}=-2\\sum_{i=1}^{n}\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)=0,\\quad\n\\frac{\\partial S}{\\partial \\beta_{1}}=-2\\sum_{i=1}^{n}x_i\\left(y_i-\\beta_{0}-\\beta_{1}x_i\\right)=0.\n$$\n在 $(\\hat{\\beta}_{0},\\hat{\\beta}_{1})$ 处求值，可得\n$$\n\\sum_{i=1}^{n}e_i=0,\\qquad \\sum_{i=1}^{n}x_i e_i=0.\n$$\n因此，第一次回归的残差同时满足和为零以及与 $x$ 正交的性质。\n\n现在考虑以 $e_i$ 为响应变量、$x_i$ 为预测变量的第二次回归：\n$$\ne_i=\\gamma_{0}+\\gamma_{1}x_i+\\delta_i,\n$$\n其最小二乘估计量 $(\\hat{\\gamma}_{0},\\hat{\\gamma}_{1})$ 最小化\n$$\nT(\\gamma_{0},\\gamma_{1})=\\sum_{i=1}^{n}\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)^{2}.\n$$\n正规方程组为\n$$\n\\frac{\\partial T}{\\partial \\gamma_{0}}=-2\\sum_{i=1}^{n}\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)=0,\\qquad\n\\frac{\\partial T}{\\partial \\gamma_{1}}=-2\\sum_{i=1}^{n}x_i\\left(e_i-\\gamma_{0}-\\gamma_{1}x_i\\right)=0.\n$$\n令 $\\bar{e}=\\frac{1}{n}\\sum_{i=1}^{n}e_i$ 且 $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i$。以标准方式求解正规方程组得出\n$$\n\\hat{\\gamma}_{1}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(e_i-\\bar{e})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}},\\qquad\n\\hat{\\gamma}_{0}=\\bar{e}-\\hat{\\gamma}_{1}\\bar{x}.\n$$\n从第一次回归可知 $\\sum_{i=1}^{n}e_i=0$，故 $\\bar{e}=0$，且 $\\sum_{i=1}^{n}x_i e_i=0$。因此\n$$\n\\sum_{i=1}^{n}(x_i-\\bar{x})(e_i-\\bar{e})=\\sum_{i=1}^{n}x_i e_i - n\\bar{x}\\bar{e}=0- n\\bar{x}\\cdot 0=0.\n$$\n因为并非所有的 $x_i$ 都相等，所以分母 $\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}>0$。于是\n$$\n\\hat{\\gamma}_{1}=0,\\qquad \\hat{\\gamma}_{0}=\\bar{e}-\\hat{\\gamma}_{1}\\bar{x}=0-0\\cdot \\bar{x}=0.\n$$\n根据当 $\\sum_{i=1}^{n}(x_i-\\bar{x})^{2}>0$ 时最小二乘解的唯一性，这些就是第二次回归的最小二乘估计量。", "answer": "$$\\boxed{\\begin{pmatrix}0 & 0\\end{pmatrix}}$$", "id": "1935149"}]}