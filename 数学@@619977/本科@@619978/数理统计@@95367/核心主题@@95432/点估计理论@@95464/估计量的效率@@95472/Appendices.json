{"hands_on_practices": [{"introduction": "评估估计量的效率是数理统计的核心任务之一。克拉默-拉奥下界 (Cramér-Rao Lower Bound, CRLB) 为无偏估计量的方差设定了一个理论上的最小值，为我们提供了一个衡量效率的绝对基准。本练习将通过一个来自可靠性工程的经典场景，即指数分布，来引导你计算一个给定估计量的实际方差，并将其与CRLB进行比较，从而量化其效率。[@problem_id:1914827]", "problem": "某种电子元件的寿命服从指数分布，其概率密度函数 (PDF) 为 $f(x; \\lambda) = \\lambda \\exp(-\\lambda x)$，$x > 0$。其中，$\\lambda > 0$ 是恒定失效率。测试一个由 $n$ 个此类元件组成的随机样本 $X_1, X_2, \\dots, X_n$，并记录其寿命。\n\n一位工程师提出了以下用于估计失效率 $\\lambda$ 的估计量：\n$$\n\\hat{\\lambda} = \\frac{n-1}{\\sum_{i=1}^n X_i}\n$$\n已知对于样本量 $n > 1$，该估计量是 $\\lambda$ 的无偏估计量。\n\n假设样本量 $n > 2$，计算该估计量 $\\hat{\\lambda}$ 的效率。效率定义为 $\\lambda$ 的无偏估计量的克拉默-拉奥下界 (Cramér-Rao Lower Bound, CRLB) 与估计量 $\\hat{\\lambda}$ 的实际方差之比。将最终答案表示为 $n$ 的函数。", "solution": "设 $X_{1},\\dots,X_{n}$ 为独立同分布 (iid) 的随机变量，其密度函数为 $f(x;\\lambda)=\\lambda\\exp(-\\lambda x)$，$x>0$。它们的和 $S=\\sum_{i=1}^{n}X_{i}$ 服从形状参数为 $n$、率参数为 $\\lambda$ 的伽玛分布，即其密度函数为\n$$\ng(s;n,\\lambda)=\\frac{\\lambda^{n}}{\\Gamma(n)}s^{n-1}\\exp(-\\lambda s),\\quad s>0.\n$$\n所提出的估计量为 $\\hat{\\lambda}=(n-1)/S$。对于 $n>2$，我们通过计算 $S^{-1}$ 的矩来计算 $\\operatorname{Var}(\\hat{\\lambda})$。对于 $m<n$，\n$$\n\\mathbb{E}\\!\\left[S^{-m}\\right]=\\int_{0}^{\\infty}s^{-m}g(s;n,\\lambda)\\,ds=\\frac{\\lambda^{m}\\Gamma(n-m)}{\\Gamma(n)}.\n$$\n因此，\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{S}\\right]=\\frac{\\lambda\\,\\Gamma(n-1)}{\\Gamma(n)}=\\frac{\\lambda}{n-1},\\qquad\n\\mathbb{E}\\!\\left[\\frac{1}{S^{2}}\\right]=\\frac{\\lambda^{2}\\Gamma(n-2)}{\\Gamma(n)}=\\frac{\\lambda^{2}}{(n-1)(n-2)}.\n$$\n所以，\n$$\n\\operatorname{Var}\\!\\left(\\frac{1}{S}\\right)=\\mathbb{E}\\!\\left[\\frac{1}{S^{2}}\\right]-\\left(\\mathbb{E}\\!\\left[\\frac{1}{S}\\right]\\right)^{2}\n=\\frac{\\lambda^{2}}{(n-1)(n-2)}-\\frac{\\lambda^{2}}{(n-1)^{2}}\n=\\frac{\\lambda^{2}}{(n-1)^{2}(n-2)}.\n$$\n由此可得\n$$\n\\operatorname{Var}(\\hat{\\lambda})=\\operatorname{Var}\\!\\left(\\frac{n-1}{S}\\right)=(n-1)^{2}\\operatorname{Var}\\!\\left(\\frac{1}{S}\\right)=\\frac{\\lambda^{2}}{n-2}.\n$$\n\n接下来，计算 $\\lambda$ 的无偏估计量的克拉默-拉奥下界。样本的对数似然函数为\n$$\n\\ell(\\lambda)=\\sum_{i=1}^{n}\\ln f(X_{i};\\lambda)=n\\ln\\lambda-\\lambda\\sum_{i=1}^{n}X_{i}.\n$$\n则\n$$\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}=-\\frac{n}{\\lambda^{2}},\\quad\n\\mathcal{I}_{n}(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\\right]=\\frac{n}{\\lambda^{2}}.\n$$\n因此，$\\lambda$ 的任意无偏估计量的 CRLB 为\n$$\n\\operatorname{CRLB}=\\frac{1}{\\mathcal{I}_{n}(\\lambda)}=\\frac{\\lambda^{2}}{n}.\n$$\n\n因此，效率（定义为 $\\operatorname{CRLB}/\\operatorname{Var}(\\hat{\\lambda})$）为\n$$\n\\text{efficiency}=\\frac{\\lambda^{2}/n}{\\lambda^{2}/(n-2)}=\\frac{n-2}{n}.\n$$\n该结果仅取决于 $n$，且对 $n>2$ 成立。", "answer": "$$\\boxed{\\frac{n-2}{n}}$$", "id": "1914827"}, {"introduction": "在估计量的世界里，“无偏”并不总是等同于“更优”。均方误差 (Mean Squared Error, MSE) 是一个综合考量了估计量偏差和方差的更全面的性能指标。本练习通过比较均匀分布参数的两种估计量——有偏的最大似然估计量 (MLE) 与其对应的无偏版本——来揭示经典的偏差-方差权衡。通过这个练习，你将亲身体会到，有时牺牲一点无偏性可以换来整体性能上（即MSE）的显著提升。[@problem_id:1914869]", "problem": "设 $X_1, X_2, \\dots, X_n$ 是从区间 $[0, \\theta]$ 上的连续均匀分布中抽取的一个随机样本，其中 $\\theta > 0$ 是一个未知参数。\n$\\theta$ 的最大似然估计量 (MLE) 为 $\\hat{\\theta}_{MLE} = X_{(n)}$，其中 $X_{(n)}$ 是样本中的最大值，即 $X_{(n)} = \\max\\{X_1, X_2, \\dots, X_n\\}$。\n已知该估计量是有偏的。一个相应的 $\\theta$ 的无偏估计量为 $\\hat{\\theta}_{U} = \\frac{n+1}{n}X_{(n)}$。\n\n为比较这两个估计量的性能，我们可以使用均方误差 (MSE)，对于一个通用估计量 $\\hat{\\theta}$，其定义为 $\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$。\n\n计算有偏估计量的 MSE 与无偏估计量的 MSE 之比，即 $\\frac{\\text{MSE}(\\hat{\\theta}_{MLE})}{\\text{MSE}(\\hat{\\theta}_{U})}$。请将您的答案表示为样本量 $n$ 的函数。", "solution": "设 $X_{(n)}=\\max\\{X_{1},\\dots,X_{n}\\}$。对于从 $\\text{Uniform}(0,\\theta)$ 中抽取的独立同分布样本 $X_{i}$，$X_{(n)}$ 的密度函数是\n$$\nf_{X_{(n)}}(x)=\\frac{n}{\\theta^{n}}x^{n-1},\\quad 0<x<\\theta.\n$$\n等价地，可记 $X_{(n)}=\\theta Z$，其中 $Z\\sim \\text{Beta}(n,1)$。利用 Beta 分布的矩，\n$$\n\\mathbb{E}[Z]=\\frac{n}{n+1},\\qquad \\operatorname{Var}(Z)=\\frac{n}{(n+1)^{2}(n+2)}.\n$$\n通过尺度变换，可得\n$$\n\\mathbb{E}[X_{(n)}]=\\theta\\,\\frac{n}{n+1},\\qquad \\operatorname{Var}(X_{(n)})=\\theta^{2}\\,\\frac{n}{(n+1)^{2}(n+2)}.\n$$\n\n对于 MLE $\\hat{\\theta}_{\\text{MLE}}=X_{(n)}$，其偏差为\n$$\n\\operatorname{Bias}(\\hat{\\theta}_{\\text{MLE}})=\\mathbb{E}[X_{(n)}]-\\theta=\\theta\\left(\\frac{n}{n+1}-1\\right)=-\\frac{\\theta}{n+1},\n$$\n所以\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\text{MLE}})=\\operatorname{Var}(X_{(n)})+\\operatorname{Bias}(\\hat{\\theta}_{\\text{MLE}})^{2}\n=\\theta^{2}\\left(\\frac{n}{(n+1)^{2}(n+2)}+\\frac{1}{(n+1)^{2}}\\right).\n$$\n合并各项：\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\text{MLE}})=\\frac{\\theta^{2}}{(n+1)^{2}}\\left(\\frac{n}{n+2}+1\\right)\n=\\frac{\\theta^{2}}{(n+1)^{2}}\\cdot\\frac{2n+2}{n+2}\n=\\theta^{2}\\,\\frac{2}{(n+1)(n+2)}.\n$$\n\n对于无偏估计量 $\\hat{\\theta}_{U}=\\frac{n+1}{n}X_{(n)}$，注意它由构造是无偏的。因此其 MSE 等于其方差：\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{U})=\\operatorname{Var}\\!\\left(\\frac{n+1}{n}X_{(n)}\\right)\n=\\left(\\frac{n+1}{n}\\right)^{2}\\operatorname{Var}(X_{(n)})\n=\\left(\\frac{n+1}{n}\\right)^{2}\\theta^{2}\\,\\frac{n}{(n+1)^{2}(n+2)}\n=\\theta^{2}\\,\\frac{1}{n(n+2)}.\n$$\n\n因此，MSE 的比值为\n$$\n\\frac{\\operatorname{MSE}(\\hat{\\theta}_{\\text{MLE}})}{\\operatorname{MSE}(\\hat{\\theta}_{U})}\n=\\frac{\\theta^{2}\\,\\frac{2}{(n+1)(n+2)}}{\\theta^{2}\\,\\frac{1}{n(n+2)}}\n=\\frac{2n}{n+1}.\n$$", "answer": "$$\\boxed{\\frac{2n}{n+1}}$$", "id": "1914869"}, {"introduction": "在评估现有估计量之后，一个更高级的问题是：我们能否系统地构建出“最好”的无偏估计量？本练习将我们带入寻找一致最小方差无偏估计量 (Uniformly Minimum-Variance Unbiased Estimator, UMVUE) 的领域。你将应用强大的Lehmann–Scheffé定理，为一个在质量控制等领域至关重要的参数——伯努利试验的方差 $p(1-p)$ ——推导出其UMVUE。这个过程展示了如何从数据中提炼出关于参数的最优信息。[@problem_id:1914847]", "problem": "在一家半导体制造厂的质量控制过程中，每片硅片都会被检测是否存在一种特定类型的关键缺陷。每片硅片的检测结果可以建模为一次伯努利试验，即硅片要么是有缺陷的（用1表示），要么是无缺陷的（用0表示）。\n\n设 $X_1, X_2, \\ldots, X_n$ 是来自伯努利分布的一个样本容量为 $n$ 的随机样本，其缺陷概率 $p$ 未知。参数 $p$ 是 $X_i = 1$ 的概率。该过程的总体方差由 $\\theta = \\text{Var}(X_i) = p(1-p)$ 给出。\n\n你的任务是确定该方差的最佳估计量。具体来说，求参数 $\\theta = p(1-p)$ 的一致最小方差无偏估计量 (UMVUE)。\n\n将你的最终答案表示为一个关于样本容量 $n$ 和样本中观察到的缺陷硅片总数 $T = \\sum_{i=1}^n X_i$ 的单一闭式解析表达式。", "solution": "设 $X_{1},\\ldots,X_{n}$ 独立同分布于 $X_{i}\\sim\\text{Bernoulli}(p)$，并设 $T=\\sum_{i=1}^{n}X_{i}$。那么 $T\\sim\\text{Binomial}(n,p)$。根据因子分解定理，$T$ 是 $p$ 的一个充分统计量，因为联合概率质量函数仅通过 $T$ 来依赖于样本。此外，对于伯努利族，$T$ 是完备的：如果对于所有的 $p\\in(0,1)$ 都有 $E_{p}[h(T)]=0$，那么由二项概率构成的关于 $p$ 的多项式将迫使 $h(T)=0$ 几乎必然成立。因此，根据 Lehmann–Scheffé 定理，任何 $p$ 的函数的 UMVUE 是那个唯一的、作为 $T$ 的函数的无偏估计量。\n\n我们需要一个 $\\theta=p(1-p)$ 的无偏估计量。首先，我们使用 $T$ 来构造 $p$ 和 $p^2$ 的无偏估计量：\n$$\nE[T]=np\\quad\\Rightarrow\\quad E\\!\\left[\\frac{T}{n}\\right]=p,\n$$\n并且，利用阶乘矩，\n$$\nE\\!\\left[T(T-1)\\right]=n(n-1)p^{2}\\quad\\Rightarrow\\quad E\\!\\left[\\frac{T(T-1)}{n(n-1)}\\right]=p^{2}.\n$$\n因此\n$$\ng(T)\\equiv\\frac{T}{n}-\\frac{T(T-1)}{n(n-1)}\n$$\n满足 $E[g(T)]=p-p^{2}=p(1-p)=\\theta$，所以 $g(T)$ 是 $\\theta$ 的无偏估计量。因为 $g$ 是完备充分统计量 $T$ 的函数，所以 $g(T)$ 是 $\\theta$ 的 UMVUE。\n\n我们可以对 $g(T)$ 进行代数化简：\n$$\n\\frac{T}{n}-\\frac{T(T-1)}{n(n-1)}=\\frac{T(n-1)-T(T-1)}{n(n-1)}=\\frac{Tn-T^{2}}{n(n-1)}=\\frac{T\\,(n-T)}{n(n-1)}.\n$$\n因此，对于 $n\\geq 2$，$\\theta=p(1-p)$ 的 UMVUE 是\n$$\n\\widehat{\\theta}_{\\text{UMVUE}}=\\frac{T\\,(n-T)}{n(n-1)}.\n$$\n等价地，注意到对于伯努利变量有 $X_{i}^{2}=X_{i}$，无偏样本方差\n$$\nS^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}=\\frac{T-\\frac{T^{2}}{n}}{n-1}=\\frac{T(n-T)}{n(n-1)}\n$$\n与前面得到的 $T$ 的函数一致，这证实了它对于 $p(1-p)$ 是无偏的，并且根据完备性，它就是 UMVUE。", "answer": "$$\\boxed{\\frac{T\\,(n-T)}{n(n-1)}}$$", "id": "1914847"}]}