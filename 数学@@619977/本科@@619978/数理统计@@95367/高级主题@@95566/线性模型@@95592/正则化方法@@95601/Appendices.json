{"hands_on_practices": [{"introduction": "这个练习将引导你初步了解岭回归 (Ridge regression) 的计算机制。通过处理一个极简的假设数据集，你可以专注于最小化惩罚平方和的核心计算过程。这个实践将帮助你亲眼见证正则化项 $\\lambda \\beta^2$ 是如何调整系数估计，使其偏离标准最小二乘解的。", "problem": "一位数据科学家正在分析一个仅包含两个观测值 $(x_1, y_1) = (1, 2)$ 和 $(x_2, y_2) = (3, 4)$ 的非常小的数据集。他们选择使用一个通过原点的简单线性模型来对变量间的关系进行建模，该模型由方程 $y_i = \\beta x_i + \\epsilon_i$ 描述，其中 $\\beta$ 是待估计的唯一模型参数，而 $\\epsilon_i$ 代表第 $i$ 个观测值的误差项。\n\n为了防止过拟合，即使是对于这个极小的数据集，该科学家也决定使用岭回归。目标是找到能最小化带惩罚的残差平方和的 $\\beta$ 值，其表达式如下：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n给定正则化参数（或惩罚项） $\\lambda = 1$，计算岭回归估计值，记为 $\\hat{\\beta}_{\\text{Ridge}}$。请以精确分数的形式表示你的答案。", "solution": "我们最小化带惩罚的平方和\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导并令其为零，得到一阶条件\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}(y_{i}-\\beta x_{i})+2\\lambda \\beta=0.\n$$\n整理得，\n$$\n\\sum_{i=1}^{n}x_{i}y_{i}-\\beta \\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta=0\n\\quad\\Longrightarrow\\quad\n\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=\\sum_{i=1}^{n}x_{i}y_{i}.\n$$\n因此，岭估计量（无截距项）为\n$$\n\\hat{\\beta}_{\\text{Ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n对于给定的数据 $(x_{1},y_{1})=(1,2)$ 和 $(x_{2},y_{2})=(3,4)$ 以及 $\\lambda=1$，\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 2+3\\cdot 4=14,\\qquad \\sum_{i=1}^{2}x_{i}^{2}=1^{2}+3^{2}=10,\n$$\n所以\n$$\n\\hat{\\beta}_{\\text{Ridge}}=\\frac{14}{10+1}=\\frac{14}{11}.\n$$", "answer": "$$\\boxed{\\frac{14}{11}}$$", "id": "1950377"}, {"introduction": "现在我们将注意力转向 LASSO 方法，它与岭回归的目标有所不同。这个练习展示了 LASSO 最著名的特性：它能够将某些系数完全压缩至零，从而实现自动化的特征选择。你不仅需要计算一个 LASSO 估计值，还需要找出触发这种选择的关键惩罚值 $\\lambda_{crit}$，从而具体地理解稀疏性是如何在实践中运作的。", "problem": "一位数据科学家正在研究单个预测变量 $x$ 和响应变量 $y$ 之间的关系。提出的模型是简单线性回归：$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。为防止过拟合和进行特征选择，该数据科学家决定使用最小绝对收缩和选择算子 (LASSO) 方法进行参数估计。\n\n截距 $\\beta_0$ 和斜率 $\\beta_1$ 的估计值是通过最小化以下目标函数得到的：\n$$Q(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2 + \\lambda |\\beta_1|$$\n其中 $\\lambda \\geq 0$ 是正则化参数。注意，截距项 $\\beta_0$ 未被惩罚。\n\n收集到了一个包含 $n=4$ 个观测值的小数据集：\n预测变量 $x$：$(-2, -1, 1, 2)$\n响应变量 $y$：$(-1, 0, 2, 3)$\n\n你的任务是：\n1. 对于正则化参数值 $\\lambda = 12$，斜率系数的 LASSO 估计值 $\\hat{\\beta}_1$ 是多少？\n2. 能使斜率估计值 $\\hat{\\beta}_1$ 恰好为零的正则化参数的最小值（我们可记为 $\\lambda_{crit}$）是多少？\n\n请以有序数对 $(\\hat{\\beta}_1, \\lambda_{crit})$ 的形式给出你的精确数值答案。", "solution": "我们最小化 LASSO 目标函数\n$$Q(\\beta_{0},\\beta_{1})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)^{2}+\\lambda|\\beta_{1}|,$$\n其中 $\\beta_{0}$ 不受惩罚。对于任何固定的 $\\beta_{1}$，最小化截距是通过将平方误差部分对 $\\beta_{0}$ 的导数设为零来获得的，这得到\n$$\\hat{\\beta}_{0}(\\beta_{1})=\\bar{y}-\\beta_{1}\\bar{x}.$$\n代入后，得到以中心化变量 $x_{i}^{c}=x_{i}-\\bar{x}$ 和 $y_{i}^{c}=y_{i}-\\bar{y}$ 表示的剖面目标函数：\n$$Q(\\beta_{1})=\\sum_{i=1}^{n}\\left(y_{i}^{c}-\\beta_{1}x_{i}^{c}\\right)^{2}+\\lambda|\\beta_{1}|.$$\n定义\n$$S_{xx}=\\sum_{i=1}^{n}\\left(x_{i}^{c}\\right)^{2},\\qquad S_{xy}=\\sum_{i=1}^{n}x_{i}^{c}y_{i}^{c}.$$\n对于 $\\beta_{1}\\neq 0$，导数条件是\n$$\\frac{dQ}{d\\beta_{1}}=-2S_{xy}+2\\beta_{1}S_{xx}+\\lambda\\,\\mathrm{sign}(\\beta_{1})=0,$$\n这给出，在符号一致的情况下，\n$$\\beta_{1}=\\frac{S_{xy}-\\frac{\\lambda}{2}\\mathrm{sign}(\\beta_{1})}{S_{xx}}.$$\n在 $\\beta_{1}=0$ 处的次梯度条件是\n$$0\\in -2S_{xy}+[-\\lambda,\\lambda]\\quad\\Longleftrightarrow\\quad |2S_{xy}|\\leq \\lambda.$$\n等价地，解是软阈值形式\n$$\\hat{\\beta}_{1}=\\frac{\\mathrm{sign}(S_{xy})\\max\\left(|S_{xy}|-\\frac{\\lambda}{2},\\,0\\right)}{S_{xx}}.$$\n\n对于给定的数据，计算均值 $\\bar{x}$ 和 $\\bar{y}$：\n$$\\bar{x}=\\frac{-2-1+1+2}{4}=0,\\qquad \\bar{y}=\\frac{-1+0+2+3}{4}=1.$$\n因此，中心化变量为 $x^{c}=(-2,-1,1,2)$ 和 $y^{c}=(-2,-1,1,2)$。因此，\n$$S_{xx}=\\sum_{i=1}^{4}(x_{i}^{c})^{2}=4+1+1+4=10,\\qquad S_{xy}=\\sum_{i=1}^{4}x_{i}^{c}y_{i}^{c}=4+1+1+4=10.$$\n\n1) 对于 $\\lambda=12$，应用软阈值公式：\n$$\\hat{\\beta}_{1}=\\frac{\\max\\left(10-\\frac{12}{2},\\,0\\right)}{10}=\\frac{10-6}{10}=\\frac{2}{5}.$$\n\n2) 使 $\\hat{\\beta}_{1}=0$ 的最小 $\\lambda$ 是满足 $|2S_{xy}|\\leq \\lambda$ 的阈值，即，\n$$\\lambda_{\\text{crit}}=2|S_{xy}|=2\\cdot 10=20.$$\n\n因此有序数对是 $\\left(\\frac{2}{5},\\,20\\right)$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{5} & 20 \\end{pmatrix}}$$", "id": "1950382"}, {"introduction": "回归建模中的一个关键挑战是多重共线性 (multicollinearity)，即预测变量之间高度相关。这个实践将探究岭回归如何巧妙地处理这一问题。通过一个预测变量完全相关的假设情景，你将揭示岭回归分配系数权重的基本性质，从而理解为何在普通最小二乘法失效时，岭回归仍能提供稳定且唯一的解。", "problem": "一位数据科学家正在处理一个数据集以构建一个预测模型。在进行初步数据探索后，他们发现两个预测变量 $x_1$ 和 $x_2$ 之间存在完全线性关系。具体来说，对于样本量为 $n$ 的样本中的每个数据点 $i$，都有 $x_{i1} = x_{i2}$。此外，假设所有变量（预测变量和响应变量 $y$）都已中心化，即它们的均值为零。\n\n该数据科学家决定拟合一个不含截距的线性模型：\n$$y_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$$\n为了解决 $x_1$ 和 $x_2$ 之间的完全共线性问题，他们采用了岭回归。岭回归系数估计值 $(\\hat{\\beta}_1, \\hat{\\beta}_2)$ 是使以下目标函数最小化的 $(\\beta_1, \\beta_2)$ 的值：\n$$L(\\beta_1, \\beta_2) = \\sum_{i=1}^{n} (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^{2} + \\lambda (\\beta_1^{2} + \\beta_2^{2})$$\n其中，$\\lambda$ 是一个严格为正的正则化参数 ($\\lambda > 0$)。\n\n从数据中，计算出以下汇总统计量：\n- $\\sum_{i=1}^{n} y_i x_{i1} = A$\n- $\\sum_{i=1}^{n} x_{i1}^{2} = B$\n\n请用 $A$、$B$ 和 $\\lambda$ 表示估计系数 $\\hat{\\beta}_1$。", "solution": "我们最小化岭回归目标函数\n$$L(\\beta_{1},\\beta_{2})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{1}x_{i1}-\\beta_{2}x_{i2}\\right)^{2}+\\lambda\\left(\\beta_{1}^{2}+\\beta_{2}^{2}\\right),$$\n其中对于所有的 $i$ 都有 $x_{i1}=x_{i2}$，并且汇总统计量为 $\\sum_{i=1}^{n}y_{i}x_{i1}=A$ 和 $\\sum_{i=1}^{n}x_{i1}^{2}=B$。同时定义 $C=\\sum_{i=1}^{n}x_{i1}x_{i2}$。因为 $x_{i1}=x_{i2}$ 对每个点都成立，所以我们有 $C=B$，同理可得 $\\sum_{i=1}^{n}y_{i}x_{i2}=A$。\n\n将偏导数设为零。对于 $\\beta_{1}$：\n$$\\frac{\\partial L}{\\partial \\beta_{1}}=2\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{1}x_{i1}-\\beta_{2}x_{i2}\\right)(-x_{i1})+2\\lambda\\beta_{1}=0,$$\n展开后得到\n$$-2\\sum_{i=1}^{n}x_{i1}y_{i}+2\\beta_{1}\\sum_{i=1}^{n}x_{i1}^{2}+2\\beta_{2}\\sum_{i=1}^{n}x_{i1}x_{i2}+2\\lambda\\beta_{1}=0.$$\n两边除以 $2$ 并代入 $A$、$B$ 和 $C$ 可得\n$$(B+\\lambda)\\beta_{1}+C\\beta_{2}=A.$$\n根据对称性，对 $\\beta_{2}$ 求导可得\n$$C\\beta_{1}+(B+\\lambda)\\beta_{2}=A.$$\n\n使用 $C=B$，方程组变为\n$$(B+\\lambda)\\beta_{1}+B\\beta_{2}=A,\\qquad B\\beta_{1}+(B+\\lambda)\\beta_{2}=A.$$\n两式相减可得\n$$\\lambda(\\beta_{1}-\\beta_{2})=0.$$\n因为 $\\lambda>0$，所以必然有 $\\beta_{1}=\\beta_{2}$。将 $\\beta_{2}=\\beta_{1}$ 代入任意一个方程可得\n$$(2B+\\lambda)\\beta_{1}=A,$$\n所以\n$$\\hat{\\beta}_{1}=\\frac{A}{2B+\\lambda}.$$", "answer": "$$\\boxed{\\frac{A}{2B+\\lambda}}$$", "id": "1950412"}]}