{"hands_on_practices": [{"introduction": "要真正掌握岭回归，从其基本原理出发至关重要。本练习将带你从最简单的单变量模型入手，通过最小化带惩罚项的残差平方和（即岭回归的目标函数），亲手推导出系数的估计公式。这个过程不仅能加深你对正则化思想的理解，也为处理更复杂的多维问题奠定数学基础。[@problem_id:1951876]", "problem": "在机器学习背景下，我们的任务是使用一组包含 $n$ 个数据点 $(x_i, y_i)$ 的数据集来拟合一个不含截距项的简单线性模型 $y = \\beta x$。为了防止在小数据集上发生过拟合，我们采用岭回归。系数 $\\beta$ 的岭估计值是使惩罚平方和误差（也称为目标函数 $L(\\beta)$）最小化的值：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n其中 $\\lambda > 0$ 是控制收缩量的正则化参数。\n\n你的任务分为两部分。首先，通过最小化目标函数 $L(\\beta)$，推导出用数据点 $(x_i, y_i)$ 和参数 $\\lambda$ 表示的岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 的通用闭式解表达式。\n\n其次，将推导出的表达式应用于一个包含两个点 $(x_1, y_1) = (1, 3)$ 和 $(x_2, y_2) = (2, 5)$ 的特定数据集。使用正则化参数 $\\lambda = 1$ 计算岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 的数值。\n\n以精确分数的形式给出最终的数值。", "solution": "我们最小化不含截距项的线性模型 $y=\\beta x$ 的惩罚平方和误差，其目标函数为\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\n展开平方项，并合并关于 $\\beta$ 的同次幂项：\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导，并将导数设为零（一阶最优性条件）：\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\n解出 $\\beta$：\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n二阶导数为\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\n因此该解是唯一的最小值点。\n\n将此公式应用于 $(x_{1},y_{1})=(1,3)$，$(x_{2},y_{2})=(2,5)$ 且 $\\lambda=1$ 的情况：\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "在解决了单变量问题后，我们将转向更贴近实际应用的多变量场景。当模型包含多个预测变量时，使用矩阵代数是表达和求解岭回归问题最高效的方法。本练习要求你运用岭回归的矩阵公式，在给定关键矩阵 $X^T X$ 和 $X^T y$ 的情况下，计算模型的系数向量，这将帮助你熟练掌握岭回归的标准计算流程。[@problem_id:1951893]", "problem": "在机器学习领域，岭回归是用于正则化线性回归模型的一种常用技术。这对于防止过拟合和处理预测变量之间的多重共线性特别有用。岭回归的系数向量估计量 $\\hat{\\beta}_{\\lambda}$ 由以下公式给出：\n$$ \\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y $$\n其中，$X$ 是设计矩阵，$y$ 是观测结果向量，$I$ 是适当维度的单位矩阵，$\\lambda$ 是一个非负的正则化参数。\n\n假设对于一个具有两个预测变量的特定数据集，以下量已预先计算得出：\n$$ X^T X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} \\quad \\text{and} \\quad X^T y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\n使用正则化参数 $\\lambda = 5$，求岭回归的系数向量 $\\hat{\\beta}_5$。", "solution": "岭回归估计量由下式定义：\n$$\n\\hat{\\beta}_{\\lambda} = (X^{T}X + \\lambda I)^{-1} X^{T} y.\n$$\n根据给定的数据，\n$$\nX^{T}X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix}, \\quad X^{T} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\n计算正则化矩阵：\n$$\nX^{T}X + \\lambda I = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} + 5 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 15 & 5 \\\\ 5 & 15 \\end{pmatrix}.\n$$\n对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$，其逆矩阵由以下公式给出：\n$$\n\\left(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\n$$\n应用此公式，\n$$\n\\det(X^{T}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\n所以\n$$\n(X^{T}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix}.\n$$\n然后\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$", "id": "1951893"}, {"introduction": "除了从最小化目标函数的角度理解，岭回归还有一个非常巧妙的等价解释，它将新概念与我们熟悉的普通最小二乘法（OLS）联系起来。本练习旨在证明，岭回归的解等价于在一个经过特殊构造的“增广”数据集上进行OLS回归的结果。完成这个证明，不仅能让你欣赏不同统计方法间的优美联系，更能帮助你直观地理解正则化参数 $\\lambda$ 的作用机制。[@problem_id:1951855]", "problem": "在线性回归的背景下，普通最小二乘 (OLS) 法是估计线性模型系数的标准方法。对于一个由 $y = X\\beta + \\epsilon$ 描述的模型，其中 $y$ 是观测响应的 $n \\times 1$ 向量，$X$ 是预测变量的 $n \\times p$ 满秩矩阵（即设计矩阵），$\\beta$ 是未知系数的 $p \\times 1$ 向量，$\\epsilon$ 是误差向量，最小化残差平方和的 OLS 估计量由以下公式给出：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^T X)^{-1} X^T y\n$$\n\n现在，考虑一个我们构建“增广”数据集的场景。设 $\\lambda$ 为一个正常数标量。我们构造一个新的增广设计矩阵 $X_{\\text{aug}}$ 和一个新的响应向量 $y_{\\text{aug}}$ 如下：\n$$\nX_{\\text{aug}} = \\begin{pmatrix} X \\\\ \\sqrt{\\lambda}I \\end{pmatrix} \\quad \\text{和} \\quad y_{\\text{aug}} = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}\n$$\n此处，$I$ 是 $p \\times p$ 的单位矩阵，$0$ 是一个 $p \\times 1$ 的零列向量。矩阵 $X_{\\text{aug}}$ 的维度为 $(n+p) \\times p$，向量 $y_{\\text{aug}}$ 的维度为 $(n+p) \\times 1$。\n\n你的任务是将标准的 OLS 公式应用于这个增广系统，以求出相应的系数估计量，我们称之为 $\\hat{\\beta}_{\\text{aug}}$。请将 $\\hat{\\beta}_{\\text{aug}}$ 的最终答案表示为关于 $X$、$y$和 $\\lambda$ 的单一符号表达式。", "solution": "我们从应用于增广系统的标准 OLS 估计量开始。对于任意的设计矩阵 $Z$ 和响应向量 $w$，OLS 估计量由下式给出\n$$\n\\hat{\\beta}=(Z^{T}Z)^{-1}Z^{T}w\n$$\n只要 $Z^{T}Z$ 是可逆的。\n\n在这里，我们令 $Z=X_{\\text{aug}}$ 和 $w=y_{\\text{aug}}$，其中\n$$\nX_{\\text{aug}}=\\begin{pmatrix}X\\\\ \\sqrt{\\lambda}I\\end{pmatrix}, \\quad y_{\\text{aug}}=\\begin{pmatrix}y\\\\ 0\\end{pmatrix}.\n$$\n我们计算所需的两个组成部分：\n\n1) 增广设计的格拉姆矩阵：\n使用分块矩阵乘法恒等式 $\\begin{pmatrix}A\\\\ B\\end{pmatrix}^{T}\\begin{pmatrix}A\\\\ B\\end{pmatrix}=A^{T}A+B^{T}B$，我们得到\n$$\nX_{\\text{aug}}^{T}X_{\\text{aug}}=X^{T}X+(\\sqrt{\\lambda}I)^{T}(\\sqrt{\\lambda}I)=X^{T}X+\\lambda I,\n$$\n因为 $I^{T}=I$ 且标量与矩阵乘法是可交换的。\n\n2) 与增广响应的交叉项：\n使用 $\\begin{pmatrix}A\\\\ B\\end{pmatrix}^{T}\\begin{pmatrix}c\\\\ d\\end{pmatrix}=A^{T}c+B^{T}d$，我们有\n$$\nX_{\\text{aug}}^{T}y_{\\text{aug}}=X^{T}y+(\\sqrt{\\lambda}I)^{T}0=X^{T}y.\n$$\n\n因此，增广系统的 OLS 估计量是\n$$\n\\hat{\\beta}_{\\text{aug}}=\\left(X_{\\text{aug}}^{T}X_{\\text{aug}}\\right)^{-1}X_{\\text{aug}}^{T}y_{\\text{aug}}=\\left(X^{T}X+\\lambda I\\right)^{-1}X^{T}y.\n$$\n当 $\\lambda>0$ 时，该表达式是良定义的，因为在给定假设下 $X^{T}X+\\lambda I$ 是可逆的。", "answer": "$$\\boxed{(X^{T}X+\\lambda I)^{-1}X^{T}y}$$", "id": "1951855"}]}