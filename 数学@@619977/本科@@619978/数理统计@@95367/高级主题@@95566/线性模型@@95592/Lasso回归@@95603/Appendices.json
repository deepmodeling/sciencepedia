{"hands_on_practices": [{"introduction": "Lasso 回归的目标函数由残差平方和（RSS）与一个 $L_1$ 惩罚项组成。这个练习旨在通过考察惩罚参数 $\\lambda$ 为零的特殊情况，来探索 Lasso 与普通最小二乘法（OLS）之间的基本联系。理解这一联系是掌握 Lasso 如何推广传统回归方法的第一步，它有助于我们建立一个基准来理解正则化的作用 [@problem_id:1928607]。", "problem": "在统计建模中，最小绝对收缩和选择算子 (LASSO) 是一种回归分析方法，它同时进行变量选择和正则化，以提高其生成的统计模型的预测准确度和可解释性。对于一个包含 $p$ 个预测变量的一般线性模型，其系数 $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_p)$ 的 LASSO 估计值是使以下目标函数最小化的值：\n$$\n\\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| = \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j\\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n$$\n其中 $(x_{i1}, \\dots, x_{ip}, y_i)$ 是 $i=1, \\dots, n$ 的观测数据，$\\beta_0$ 是截距，而 $\\lambda \\ge 0$ 是一个调整参数。\n\n一位数据科学家正在分析一个数据集，以拟合一个形式为 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$ 的简单线性回归模型。他们决定使用 LASSO 框架来确定模型系数。然而，作为一项初始探索步骤，他们将调整参数 $\\lambda$ 设置为零。该数据集包含以下四个 $(x_i, y_i)$ 数据点：\n$$\n(1, 2), (2, 3), (3, 5), (4, 6)\n$$\n计算在此特定设置下获得的系数估计值 $\\hat{\\beta}_1$。将最终答案保留三位有效数字。", "solution": "将 $\\lambda=0$ 代入，LASSO 目标函数便简化为残差平方和，因此其最小化估计量即为普通最小二乘 (OLS) 解。对于带截距的简单线性回归，OLS 斜率为\n$$\n\\hat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}},\n$$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。对于数据 $(1,2),(2,3),(3,5),(4,6)$，其中 $n=4$，\n$$\n\\bar{x}=\\frac{1+2+3+4}{4}=\\frac{10}{4}=\\frac{5}{2},\\quad \\bar{y}=\\frac{2+3+5+6}{4}=\\frac{16}{4}=4.\n$$\n计算分子：\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})(y_{i}-\\bar{y})=(1-\\tfrac{5}{2})(2-4)+(2-\\tfrac{5}{2})(3-4)+(3-\\tfrac{5}{2})(5-4)+(4-\\tfrac{5}{2})(6-4).\n$$\n各项计算如下：\n$$\n(1-\\tfrac{5}{2})=-\\frac{3}{2},\\ (2-4)=-2\\ \\Rightarrow\\ \\left(-\\frac{3}{2}\\right)(-2)=3,\\\\\n(2-\\tfrac{5}{2})=-\\frac{1}{2},\\ (3-4)=-1\\ \\Rightarrow\\ \\left(-\\frac{1}{2}\\right)(-1)=\\frac{1}{2},\\\\\n(3-\\tfrac{5}{2})=\\frac{1}{2},\\ (5-4)=1\\ \\Rightarrow\\ \\left(\\frac{1}{2}\\right)(1)=\\frac{1}{2},\\\\\n(4-\\tfrac{5}{2})=\\frac{3}{2},\\ (6-4)=2\\ \\Rightarrow\\ \\left(\\frac{3}{2}\\right)(2)=3.\n$$\n求和得：\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})(y_{i}-\\bar{y})=3+\\frac{1}{2}+\\frac{1}{2}+3=7.\n$$\n计算分母：\n$$\n\\sum_{i=1}^{4}(x_{i}-\\bar{x})^{2}=\\left(-\\frac{3}{2}\\right)^{2}+\\left(-\\frac{1}{2}\\right)^{2}+\\left(\\frac{1}{2}\\right)^{2}+\\left(\\frac{3}{2}\\right)^{2}=\\frac{9}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{9}{4}=\\frac{20}{4}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{1}=\\frac{7}{5}=1.4.\n$$\n保留三位有效数字得到 $1.40$。", "answer": "$$\\boxed{1.40}$$", "id": "1928607"}, {"introduction": "在理解了没有惩罚的情况后，一个自然的问题是：在另一个极端，当惩罚变得非常大时会发生什么？本练习探讨当调节参数 $\\lambda$ 趋于无穷大时，Lasso 系数估计值的行为，从而最大限度地发挥惩罚项的影响。这个思想实验对于理解 Lasso 的“收缩”效应以及它如何强制实现模型稀疏性至关重要 [@problem_id:1928648]。", "problem": "在线性模型 $y = \\beta_0 + \\sum_{j=1}^{p} x_j \\beta_j + \\epsilon$ 的背景下，使用最小绝对收缩和选择算子 (LASSO) 方法来寻找系数 $(\\beta_0, \\beta_1, \\dots, \\beta_p)$ 的估计值。这些估计值是使以下目标函数最小化的值：\n$$ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n这里，对于 $i=1, \\dots, n$, $(y_i, x_{i1}, \\dots, x_{ip})$ 是观测到的数据点。项 $\\lambda \\ge 0$ 是一个控制惩罚强度的非负调整参数。请注意，截距项 $\\beta_0$ 不包括在惩罚项中。\n\n考虑当调整参数 $\\lambda$ 增大到一个非常大的值时（即，当 $\\lambda \\to \\infty$ 时），估计出的系数会发生什么变化。以下哪个陈述正确描述了 LASSO 系数估计值的极限行为？\n\nA. 所有系数估计值，包括截距 $\\hat{\\beta}_0$，都被强制变为零。\n\nB. 非截距项的系数估计值 ($\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) 被强制变为零，而截距项的估计值 $\\hat{\\beta}_0$ 收敛于响应变量的样本均值 $\\bar{y}$。\n\nC. 所有系数估计值 ($\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) 都收敛于它们的普通最小二乘 (OLS) 估计值。\n\nD. 至少有一个非截距项的系数估计值保持非零，而所有其他系数都被强制变为零。\n\nE. 非截距项的系数估计值 ($\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$) 的量级与 $\\lambda$ 成比例增长。", "solution": "我们考虑 LASSO 的目标函数\n$$\nQ_{\\lambda}(\\beta_{0},\\beta_{1},\\dots,\\beta_{p})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j}\\right)^{2}+\\lambda\\sum_{j=1}^{p}|\\beta_{j}|,\n$$\n其中 $\\lambda\\ge 0$，且截距 $\\beta_{0}$ 不受惩罚。\n\n第1步：当 $\\lambda\\to\\infty$ 时非截距项系数的行为。\n- 对于任何候选解 $(\\beta_{0},\\beta_{1},\\dots,\\beta_{p})$，如果至少有一个 $\\beta_{j}\\neq 0$ (对于某个 $j\\ge 1$)，则惩罚项满足 $\\sum_{j=1}^{p}|\\beta_{j}|>0$，因此当 $\\lambda\\to\\infty$ 时 $\\lambda\\sum_{j=1}^{p}|\\beta_{j}|\\to\\infty$。\n- 考虑另一种情况，将所有非截距项系数设为零：对于 $j\\ge 1$, $\\tilde{\\beta}_{j}=0$，并选择 $\\tilde{\\beta}_{0}$ 以最小化残差平方和。对于这种情况，惩罚项对所有 $\\lambda$ 都恒等于零，并且残差平方和是有限的。\n- 因此，对于足够大的 $\\lambda$，任何具有某个 $\\beta_{j}\\neq 0$ 的解的目标函数值都严格大于所有 $j\\ge 1$ 都满足 $\\beta_{j}=0$ 的解。因此，在极限 $\\lambda\\to\\infty$ 下，最小化者必须满足\n$$\n\\hat{\\beta}_{j}(\\lambda)\\to 0 \\quad \\text{for all } j=1,\\dots,p.\n$$\n\n第2步：当 $\\beta_{1}=\\dots=\\beta_{p}=0$ 时截距的极限值。\n- 当对于 $j\\ge 1$ 有 $\\beta_{j}=0$ 时，目标函数简化为在 $\\beta_{0}$ 上最小化残差平方和：\n$$\nR(\\beta_{0})=\\sum_{i=1}^{n}(y_{i}-\\beta_{0})^{2}.\n$$\n- 求导并令其为零，得到一阶条件：\n$$\n\\frac{dR}{d\\beta_{0}}=-2\\sum_{i=1}^{n}(y_{i}-\\beta_{0})=0 \\quad \\Longrightarrow \\quad n\\beta_{0}=\\sum_{i=1}^{n}y_{i}.\n$$\n因此，\n$$\n\\beta_{0}=\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}.\n$$\n- 二阶导数为 $\\frac{d^{2}R}{d\\beta_{0}^{2}}=2n>0$，这证实了它是一个最小值点。\n\n结论：当 $\\lambda\\to\\infty$ 时，非截距项的系数被驱动到零，而截距收敛于样本均值 $\\bar{y}$。这对应于选项 B。", "answer": "$$\\boxed{B}$$", "id": "1928648"}, {"introduction": "在 $\\lambda=0$ 和 $\\lambda \\to \\infty$ 这两个极端之间，蕴含着 Lasso 的真正威力：通过将某些系数精确地收缩到零来实现变量选择。本练习在一个简化的正交设定下，提供了一个动手机会，来推导出一个系数被剔除的确切临界点。这个过程揭示了作为 Lasso 特征选择能力核心的“软阈值”机制 [@problem_id:1928602]。", "problem": "考虑一个包含 $n$ 个观测值的线性回归模型。该模型有一个响应向量 $\\mathbf{y}$ 和两个预测向量 $\\mathbf{x}_1$ 和 $\\mathbf{x}_2$。假设所有变量都已中心化，均值为零，因此模型中不需要截距项。这两个预测向量经过处理，使得它们是正交的，并且它们的平方范数等于观测数 $n$。具体来说，它们满足以下条件：\n$$ \\sum_{i=1}^n x_{i1}^2 = n $$\n$$ \\sum_{i=1}^n x_{i2}^2 = n $$\n$$ \\sum_{i=1}^n x_{i1} x_{i2} = 0 $$\n回归模型由 $y_i = \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$ 给出。\n\n系数 $\\beta_1$ 和 $\\beta_2$ 使用最小绝对值收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO) 方法进行估计。LASSO 估计值，记为 $\\hat{\\beta}_1(\\lambda)$ 和 $\\hat{\\beta}_2(\\lambda)$，是在给定调谐参数 $\\lambda > 0$ 时最小化以下目标函数的值：\n$$ L(\\beta_1, \\beta_2) = \\frac{1}{2} \\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (|\\beta_1| + |\\beta_2|) $$\n当调谐参数 $\\lambda$ 从 0 开始增加时，估计系数的绝对值会连续地向零收缩。令 $S_1 = \\sum_{i=1}^n x_{i1} y_i$ 且 $S_2 = \\sum_{i=1}^n x_{i2} y_i$。假设我们处于 $|S_1| > |S_2| > 0$ 的情况，这意味着对于小的非零 $\\lambda$ 值，$\\hat{\\beta}_1(\\lambda)$ 和 $\\hat{\\beta}_2(\\lambda)$ 均为非零。\n\n确定调谐参数 $\\lambda$ 的精确值，在该值处系数估计 $\\hat{\\beta}_2(\\lambda)$ 首次变为零。用 $S_1$ 和/或 $S_2$ 表示你的答案。", "solution": "令 $X=[\\mathbf{x}_{1}\\ \\mathbf{x}_{2}]$，并注意到给定条件意味着 $X^{\\top}X=n I_{2}$ 且 $X^{\\top}\\mathbf{y}=(S_{1},S_{2})^{\\top}$。LASSO 目标函数可以写成\n$$\nL(\\beta_{1},\\beta_{2})=\\frac{1}{2}\\|\\mathbf{y}-X\\beta\\|^2+\\lambda\\left(|\\beta_{1}|+|\\beta_{2}|\\right),\n$$\n使用 $X^{\\top}X=n I_{2}$ 和 $X^{\\top}\\mathbf{y}=(S_{1},S_{2})^{\\top}$，上式可展开为\n$$\nL(\\beta_{1},\\beta_{2})=\\frac{1}{2}\\mathbf{y}^{\\top}\\mathbf{y}-S_{1}\\beta_{1}-S_{2}\\beta_{2}+\\frac{n}{2}\\left(\\beta_{1}^{2}+\\beta_{2}^{2}\\right)+\\lambda\\left(|\\beta_{1}|+|\\beta_{2}|\\right).\n$$\n这个表达式在 $\\beta_{1}$ 和 $\\beta_{2}$ 上是可分的。对于 $j\\in\\{1,2\\}$，定义\n$$\nf_{j}(\\beta_{j})=\\frac{n}{2}\\beta_{j}^{2}-S_{j}\\beta_{j}+\\lambda|\\beta_{j}|.\n$$\n我们最小化 $f_{2}(\\beta_{2})$。使用次梯度最优性条件：\n- 如果 $\\beta_{2}\\neq 0$，令 $s_{2}=\\operatorname{sign}(\\beta_{2})$，则一阶条件为\n$$\nn\\beta_{2}-S_{2}+\\lambda s_{2}=0\\quad\\Longrightarrow\\quad \\beta_{2}=\\frac{S_{2}-\\lambda s_{2}}{n}.\n$$\n一致性要求 $s_{2}=\\operatorname{sign}(\\beta_{2})=\\operatorname{sign}(S_{2}-\\lambda s_{2})$，这在且仅在 $|S_{2}|>\\lambda$ 时成立。在该情况下，\n$$\n\\hat{\\beta}_{2}(\\lambda)=\\frac{\\operatorname{sign}(S_{2})\\left(|S_{2}|-\\lambda\\right)}{n}.\n$$\n- 如果 $\\beta_{2}=0$，次梯度条件是存在某个 $u\\in[-1,1]$ 使得 $0\\in -S_{2}+\\lambda u$，这等价于 $|S_{2}|\\le \\lambda$。\n\n因此，当 $\\lambda$ 从 0 开始增加时，$\\hat{\\beta}_{2}(\\lambda)$ 会在 $\\lambda$ 达到 $|S_{2}|$ 时恰好变为零。在 $|S_1| > |S_2| > 0$ 的假设下，$\\hat{\\beta}_2$ 是第一个在 $\\lambda = |S_2|$ 时归零的系数。", "answer": "$$\\boxed{|S_{2}|}$$", "id": "1928602"}]}