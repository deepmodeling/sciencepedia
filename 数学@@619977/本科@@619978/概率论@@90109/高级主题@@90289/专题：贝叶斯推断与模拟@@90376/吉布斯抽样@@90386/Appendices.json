{"hands_on_practices": [{"introduction": "吉布斯采样将从复杂的多维分布中采样的难题，分解为一系列从更简单的单维条件分布中采样的过程。这个练习将带你亲手完成一次完整的吉布斯采样迭代，让你直观地理解其核心机制。通过这个基于指数分布和泊松分布的假设场景，你将掌握如何利用给定的条件分布和随机数，从一个状态点转移到下一个状态点 [@problem_id:1920320]。", "problem": "考虑一个二维随机向量 $(X, Y)$，其联合概率分布由以下全条件分布定义：\n- 给定 $Y=y$，$X$ 的条件分布是速率参数为 $y$ 的指数分布。其概率密度函数为 $p(x|y) = y \\exp(-yx)$，其中 $x > 0$。\n- 给定 $X=x$，$Y$ 的条件分布是均值参数为 $x$ 的泊松分布。其概率质量函数为 $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$，其中 $k \\in \\{0, 1, 2, \\dots\\}$。\n\n您的任务是执行一次吉布斯采样器的完整迭代。从初始状态 $(x^{(0)}, y^{(0)}) = (2, 3)$ 开始，您将生成一个新状态 $(x^{(1)}, y^{(1)})$。迭代过程如下：首先，从分布 $p(x|y^{(0)})$ 中抽取一个样本 $x^{(1)}$，然后，使用这个新值 $x^{(1)}$，从分布 $p(y|x^{(1)})$ 中抽取一个样本 $y^{(1)}$。\n\n为了生成所需的随机变量，您必须使用逆变换采样法。请使用以下从 Uniform(0,1) 分布中抽取的随机数：\n- 生成 $x^{(1)}$ 时，使用均匀随机数 $u_x = 0.600$。\n- 生成 $y^{(1)}$ 时，使用均匀随机数 $u_y = 0.750$。\n\n新状态 $(x^{(1)}, y^{(1)})$ 的数值是多少？$x^{(1)}$ 的值必须四舍五入到四位有效数字。", "solution": "我们使用逆变换采样法执行一次吉布斯更新。\n\n1) 从 $p(x \\mid y^{(0)}=3)$ 中采样 $x^{(1)}$。\n对于速率为 $y$ 的指数分布，其条件累积分布函数 (CDF) 为\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\n逆变换法使用 $u_{x}=F(x \\mid y)$，因此\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\n当 $y^{(0)}=3$ 且 $u_{x}=0.600$ 时，\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\n四舍五入到四位有效数字：$x^{(1)}=0.3054$。\n\n2) 使用 $u_{y}=0.750$ 从 $p(y \\mid x^{(1)})$ 中采样 $y^{(1)}$。\n对于均值为 $x$ 的泊松分布，其概率质量函数 (pmf) 为\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\n对于离散分布，逆变换法选择满足 $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$ 的最小整数 $k$。\n\n当 $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$ 时，计算\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\n由于 $p(0 \\mid x)=0.7368<0.750$，我们继续计算 $k=1$ 的情况：\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\n那么\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\n因此，满足 $F(k \\mid x)\\ge 0.750$ 的最小整数 $k$ 是 $1$。所以 $y^{(1)}=1$。\n\n因此，新状态为 $(x^{(1)},y^{(1)})=(0.3054,1)$，其中 $x^{(1)}$ 已四舍五入到四位有效数字。", "answer": "$$\\boxed{\\begin{pmatrix}0.3054 & 1\\end{pmatrix}}$$", "id": "1920320"}, {"introduction": "理论上，吉布斯采样器能够收敛到目标分布，但其效率在实践中至关重要。当变量高度相关时，采样器可能会在样本空间中移动得非常缓慢，这种现象被称为“慢混合”。本练习通过一个高精度制造过程的假设场景，展示了在变量间存在强相关性（$\\rho = 0.99$）时，采样器如何被困住，难以有效地探索整个分布 [@problem_id:1363745]。这个练习将帮助你认识到诊断采样效率的重要性。", "problem": "在一个高精度光学元件制造过程中，透镜的两个几何参数 $x_1$ 和 $x_2$ 对其性能至关重要。这些参数代表了与理想设计规格的归一化偏差。由于制造过程的物理原理，这些参数不是独立的。对生产数据的分析表明，它们的联合概率分布可以通过一个未归一化的密度函数 $f(x_1, x_2)$ 来建模，其形式如下：\n$$f(x_1, x_2) \\propto \\exp \\left( -\\frac{1}{2(1-\\rho^2)} (x_1^2 - 2\\rho x_1 x_2 + x_2^2) \\right)$$\n对于这个特定的过程，发现相关系数为 $\\rho = 0.99$。该分布的众数位于 $(0, 0)$，这对应于一个完美的元件。\n\n为了模拟过程变异，您需要使用一个 Gibbs 抽样器。您从一个初始状态 $(x_1^{(0)}, x_2^{(0)}) = (-4.0, -4.1)$ 开始，该状态代表一个处于可接受质量范围边缘的元件。\n\n您的任务是确定抽样器在两次完整迭代后的状态 $(x_1^{(2)}, x_2^{(2)})$。一次完整迭代包括先更新 $x_1$，然后更新 $x_2$。为了使计算具有确定性，您必须假设在每个抽样步骤中，为变量抽取的新值等于其条件分布的均值。\n\n计算状态 $(x_1^{(2)}, x_2^{(2)})$ 的坐标。在您的最终答案中报告这两个坐标，并四舍五入到四位有效数字。", "solution": "我们识别出给定的未归一化联合密度\n$$\nf(x_{1},x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(x_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}\\right)\\right)\n$$\n是一个二元正态分布的核，其均值向量为 $(0,0)$，方差为单位值，相关系数为 $\\rho$。为了推导全条件分布，对 $x_{1}$ 进行配方：\n$$\nx_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}=(x_{1}-\\rho x_{2})^{2}+(1-\\rho^{2})x_{2}^{2}.\n$$\n因此，以 $x_{2}$ 为条件，\n$$\nf(x_{1}\\mid x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}(x_{1}-\\rho x_{2})^{2}\\right),\n$$\n这是一个正态分布的核，其分布为\n$$\nx_{1}\\mid x_{2} \\sim \\mathcal{N}\\!\\left(\\rho x_{2},\\,1-\\rho^{2}\\right).\n$$\n根据对称性，我们也有\n$$\nx_{2}\\mid x_{1} \\sim \\mathcal{N}\\!\\left(\\rho x_{1},\\,1-\\rho^{2}\\right).\n$$\n\nGibbs 抽样器首先使用 $x_{2}$ 更新 $x_{1}$，然后使用新的 $x_{1}$ 更新 $x_{2}$。在每次抽样等于条件均值的确定性规则下，更新公式为\n$$\nx_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)},\\qquad x_{2}^{(t+1)}=\\rho\\,x_{1}^{(t+1)}.\n$$\n结合这些公式，\n$$\nx_{2}^{(t+1)}=\\rho^{2}\\,x_{2}^{(t)},\\qquad x_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)}.\n$$\n从 $(x_{1}^{(0)},x_{2}^{(0)})=(-4.0,-4.1)$ 开始，并使用 $\\rho=0.99$，第一次完整迭代得出\n$$\nx_{1}^{(1)}=\\rho\\,x_{2}^{(0)}=0.99\\times(-4.1)=-4.059,\\qquad\nx_{2}^{(1)}=\\rho\\,x_{1}^{(1)}=0.99\\times(-4.059)=-4.01841.\n$$\n随后的第二次完整迭代得出\n$$\nx_{1}^{(2)}=\\rho\\,x_{2}^{(1)}=0.99\\times(-4.01841)=-3.9782259,\\qquad\nx_{2}^{(2)}=\\rho\\,x_{1}^{(2)}=0.99\\times(-3.9782259)=-3.938443641.\n$$\n将每个坐标四舍五入到四位有效数字：\n$$\nx_{1}^{(2)}\\approx -3.978,\\qquad x_{2}^{(2)}\\approx -3.938.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-3.978 & -3.938\\end{pmatrix}}$$", "id": "1363745"}, {"introduction": "在掌握了吉布斯采样的基本原理和潜在挑战后，我们可以将其应用于解决真实的复杂问题。本练习将引导你进入计算经济学的世界，使用吉布斯采样来分析一个用于识别经济扩张与衰退周期的马尔可夫转换模型。你将构建一个完整的采样器，迭代地对模型的隐藏状态、各状态下的均值以及状态间的转移概率进行抽样，从而从宏观经济数据中揭示潜在的经济状态 [@problem_id:2398229]。这个实践是贝叶斯推断在现实世界中强大应用的一个缩影。", "problem": "给定一个用于描述季度实际国内生产总值（GDP）增长的双状态马尔可夫转换模型，该模型旨在捕捉扩张和衰退两种经济机制。时间 $t$ 的隐状态（记作 $s_t \\in \\{0,1\\}$）遵循一个时齐一阶马尔可夫链。在给定状态的条件下，观测到的增长率 $y_t$ 服从高斯分布，其均值依赖于当前机制，方差为共同的已知值。模型的完整设定如下：\n- 状态动态：$s_{t} \\mid s_{t-1} \\sim \\text{Categorical}\\left(P_{s_{t-1},\\cdot}\\right)$，其转移矩阵为 $P = \\begin{pmatrix} p_{00} & 1-p_{00} \\\\ 1-p_{11} & p_{11} \\end{pmatrix}$，其中 $p_{00} = \\mathbb{P}(s_t = 0 \\mid s_{t-1} = 0)$，$p_{11} = \\mathbb{P}(s_t = 1 \\mid s_{t-1} = 1)$。\n- 观测模型：$y_t \\mid s_t = k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$，对于 $k \\in \\{0,1\\}$ 和已知方差 $\\sigma^2$。\n- 先验分布：$\\mu_0 \\sim \\mathcal{N}(m_0, V_0)$，$\\mu_1 \\sim \\mathcal{N}(m_1, V_1)$，$p_{00} \\sim \\text{Beta}(a_{00}, b_{00})$，$p_{11} \\sim \\text{Beta}(a_{11}, b_{11})$。初始状态 $s_1$ 具有固定先验概率 $\\mathbb{P}(s_1=0) = \\mathbb{P}(s_1=1) = 0.5$。\n\n您的任务是实现一个 Gibbs 采样器，该采样器交替进行以下操作：使用前向滤波-后向采样（Forward-Filtering Backward-Sampling）算法对隐状态序列 $\\{s_t\\}_{t=1}^T$ 进行采样，使用其共轭高斯后验分布对机制均值 $\\mu_0$ 和 $\\mu_1$ 进行采样，以及使用其共轭 Beta 后验分布对转移概率 $p_{00}$ 和 $p_{11}$ 进行采样。请使用以下经过充分检验的事实和定义作为基本依据：\n- 用于条件概率的 Bayes 法则以及高斯分布和 Beta 分布的标准性质。\n- 一阶马尔可夫链的定义以及具有高斯发射的隐马尔可夫模型（HMM）结构。\n- 用于在 HMM 中采样隐状态的前向滤波-后向采样算法。\n\n对于下文中的每个测试用例，使用固定的随机种子、固定的迭代次数和指定的预烧期（burn-in）运行 Gibbs 采样器。在预烧期（用于确保收敛）之后，通过计算预烧期后所有抽样中出现衰退的蒙特卡洛频率，来估计每个时间点的边际后验衰退概率 $\\hat{\\pi}_t = \\mathbb{P}(s_t = 1 \\mid y_{1:T})$。如果 $\\hat{\\pi}_t \\geq 0.5$，则将时间点 $t$ 归类为衰退期。对于每个测试用例，输出被归类为衰退期的时间点总数（整数）。\n\n所有 GDP 增长值 $y_t$ 均以季度十进制小数形式给出（例如，0.008 表示 0.8% 的增长）。由于要求的输出是计数，最终答案中不包含任何物理单位。此问题不涉及角度。\n\n测试套件参数集：\n\n- 案例 A（机制分离清晰，中等持续性）：\n  - 观测值 $y_{1:20} = \\left(0.010, 0.008, 0.009, 0.007, 0.006, 0.007, -0.004, -0.006, -0.005, -0.007, -0.006, -0.004, 0.005, 0.006, 0.008, 0.009, 0.007, 0.006, 0.005, 0.007\\right)$。\n  - 已知方差 $\\sigma^2 = 0.000025$。\n  - 先验分布：$(m_0, V_0) = (0.007, 0.0001)$，$(m_1, V_1) = (-0.006, 0.0001)$，$(a_{00}, b_{00}) = (8, 2)$，$(a_{11}, b_{11}) = (8, 2)$。\n  - Gibbs 采样器设置：迭代次数 $N = 6000$，预烧期 $B = 3000$，种子 $= 12345$。\n\n- 案例 B（单一观测边界情况，对称先验）：\n  - 观测值 $y_{1:1} = \\left(0.000\\right)$。\n  - 已知方差 $\\sigma^2 = 0.000025$。\n  - 先验分布：$(m_0, V_0) = (0.004, 0.0001)$，$(m_1, V_1) = (-0.004, 0.0001)$，$(a_{00}, b_{00}) = (5, 5)$，$(a_{11}, b_{11}) = (5, 5)$。\n  - Gibbs 采样器设置：迭代次数 $N = 6000$，预烧期 $B = 3000$，种子 $= 12345$。\n\n- 案例 C（机制模糊，较低持续性的先验）：\n  - 观测值 $y_{1:12} = \\left(0.003, 0.004, 0.002, -0.001, 0.000, -0.002, -0.003, 0.001, 0.002, 0.003, -0.002, -0.001\\right)$。\n  - 已知方差 $\\sigma^2 = 0.000025$。\n  - 先验分布：$(m_0, V_0) = (0.002, 0.0002)$，$(m_1, V_1) = (-0.002, 0.0002)$，$(a_{00}, b_{00}) = (2, 2)$，$(a_{11}, b_{11}) = (2, 2)$。\n  - Gibbs 采样器设置：迭代次数 $N = 6000$，预烧期 $B = 3000$，种子 $= 12345$。\n\n实现您的程序以完成以下任务：\n- 对于每个案例，使用指定的参数运行上述 Gibbs 采样器。\n- 在预烧期结束后，将在所有样本中 $s_t=1$ 的蒙特卡洛频率计算为 $\\hat{\\pi}_t$。\n- 统计满足 $\\hat{\\pi}_t \\geq 0.5$ 的索引 $t$ 的数量。\n- 将三个案例对应的三个整数计数汇总到一个列表中。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，例如 $[x_A,x_B,x_C]$，其中 $x_A$、$x_B$ 和 $x_C$ 分别是案例 A、案例 B 和案例 C 的整数计数。", "solution": "该问题要求为实际 GDP 增长的双状态马尔可夫转换模型实现一个 Gibbs 采样器。模型参数，包括特定机制的均值和状态转移概率，将在贝叶斯框架内根据观测数据进行估计。最终目标是基于后验概率将每个时间点分类为“衰退”状态（$s_t = 1$），并为三个不同的测试用例计算此类时期的总数。\n\n该问题在科学上是适定的，提供了模型、先验、数据和所需算法的完整说明。它代表了马尔可夫链蒙特卡洛（MCMC）方法，特别是 Gibbs 采样，在隐马尔可夫模型（HMM）上的标准应用，这是计算计量经济学中的一项常见任务。所有参数都已指定，任务明确。因此，该问题被认为是有效的，并将构建一个解决方案。\n\n解决方案的核心在于从未知的变量的完全条件后验分布中迭代采样：包括隐状态序列 $\\{s_t\\}_{t=1}^T$、机制均值 $\\mu_0$ 和 $\\mu_1$ 以及转移概率 $p_{00}$ 和 $p_{11}$。这个过程构成了 Gibbs 采样器。\n\n设所有参数的集合为 $\\theta = \\{\\mu_0, \\mu_1, p_{00}, p_{11}\\}$，状态序列为 $S = \\{s_t\\}_{t=1}^T$。Gibbs 采样器通过初始化参数，然后迭代以下步骤来进行：\n1. 采样 $S^{(i+1)} \\sim p(S \\mid y_{1:T}, \\theta^{(i)})$。\n2. 采样 $\\mu_0^{(i+1)}, \\mu_1^{(i+1)} \\sim p(\\mu_0, \\mu_1 \\mid y_{1:T}, S^{(i+1)}, \\sigma^2)$。\n3. 采样 $p_{00}^{(i+1)}, p_{11}^{(i+1)} \\sim p(p_{00}, p_{11} \\mid S^{(i+1)})$。\n\n每个步骤详述如下。\n\n**1. 采样状态序列 $S = \\{s_t\\}_{t=1}^T$**\n\n状态序列使用前向滤波-后向采样（FFBS）算法从其条件后验分布 $p(S \\mid y_{1:T}, \\theta)$ 中采样。\n\n_前向滤波_：\n我们首先计算滤波概率 $\\alpha_t(k) = p(s_t = k, y_{1:t} \\mid \\theta)$，对于 $k \\in \\{0, 1\\}$ 和 $t=1, \\dots, T$。\n- **初始化 ($t=1$)**：初始状态先验给定为 $\\mathbb{P}(s_1=k) = 0.5$。滤波步骤开始于：\n  $$\n  \\alpha_1(k) = \\mathbb{P}(s_1=k) \\cdot p(y_1 \\mid s_1=k, \\theta) = 0.5 \\cdot \\mathcal{N}(y_1; \\mu_k, \\sigma^2)\n  $$\n  其中 $\\mathcal{N}(y; \\mu, \\sigma^2)$ 是正态分布的概率密度函数。\n- **递归 ($t=2, \\dots, T$)**：对于后续的时间步，使用马尔可夫性质更新滤波概率：\n  $$\n  \\alpha_t(k) = p(y_t \\mid s_t=k, \\theta) \\sum_{j=0}^{1} p(s_t=k \\mid s_{t-1}=j, \\theta) \\cdot \\alpha_{t-1}(j)\n  $$\n  $$\n  \\alpha_t(k) = \\mathcal{N}(y_t; \\mu_k, \\sigma^2) \\sum_{j=0}^{1} P_{jk} \\cdot \\alpha_{t-1}(j)\n  $$\n  其中 $P_{jk}$ 是从状态 $j$ 到状态 $k$ 的转移概率。为防止数值下溢，向量 $\\alpha_t = (\\alpha_t(0), \\alpha_t(1))$ 通常在每一步都被归一化。令 $\\hat{\\alpha}_t(k) = p(s_t=k \\mid y_{1:t}, \\theta) \\propto \\alpha_t(k)$。此归一化不影响后向采样步骤。\n\n_后向采样_：\n在计算完直到 $T$ 的滤波概率之后，我们以逆时间顺序对状态进行采样。\n- **初始化 ($t=T$)**：从最终的滤波分布中采样 $s_T$：\n  $$\n  p(s_T=k \\mid y_{1:T}, \\theta) \\propto \\alpha_T(k)\n  $$\n- **递归 ($t=T-1, \\dots, 1$)**：对于每个之前的时间步，在已采样的未来状态 $s_{t+1}$ 和滤波概率的条件下采样 $s_t$：\n  $$\n  p(s_t=j \\mid s_{t+1}=k, y_{1:T}, \\theta) \\propto p(s_{t+1}=k \\mid s_t=j) \\cdot p(s_t=j, y_{1:t}) \\propto P_{jk} \\cdot \\alpha_t(j)\n  $$\n  这给出了一个用于抽取 $s_t$ 的分类分布。\n\n**2. 采样机制均值 $\\mu_k$**\n\n均值 $\\mu_0$ 和 $\\mu_1$ 在状态序列 $S$ 的条件下独立采样。给定共轭先验设置（正态先验，正态似然），每个 $\\mu_k$ 的后验分布也是正态分布。\n设 $S$ 为采样的状态序列。令 $Y_k = \\{y_t \\mid s_t = k\\}$ 为在状态 $k$ 中出现的观测值子集，并令 $T_k = |Y_k|$ 为这类观测值的数量。$\\mu_k$ 的先验分布是 $\\mathcal{N}(m_k, V_k)$。$\\mu_k$ 的后验分布是 $p(\\mu_k \\mid S, y_{1:T}) \\sim \\mathcal{N}(\\mu_{k, post}, V_{k, post})$，其中后验方差 $V_{k, \\text{post}}$ 和后验均值 $\\mu_{k, \\text{post}}$ 由以下公式给出：\n$$\nV_{k, \\text{post}} = \\left( \\frac{1}{V_k} + \\frac{T_k}{\\sigma^2} \\right)^{-1}\n$$\n$$\n\\mu_{k, \\text{post}} = V_{k, \\text{post}} \\left( \\frac{m_k}{V_k} + \\frac{1}{\\sigma^2} \\sum_{y_t \\in Y_k} y_t \\right)\n$$\n如果在某次迭代中状态 $k$ 未被访问（即 $T_k = 0$），则 $\\mu_k$ 的后验分布等于其先验分布 $\\mathcal{N}(m_k, V_k)$。我们从此后验分布中为 $\\mu_k$ 抽取一个新样本。\n\n**3. 采样转移概率 $p_{kk}$**\n\n转移概率 $p_{00}$ 和 $p_{11}$ 在状态序列 $S$ 的条件下独立采样。先验是 Beta 分布，它是状态转移的二项（或伯努利）似然的共轭先验。\n令 $N_{jk} = \\sum_{t=2}^T \\mathbb{I}(s_{t-1}=j, s_t=k)$ 为在采样的序列 $S$ 中观测到的从状态 $j$ 到状态 $k$ 的转移次数。\n- $p_{00}$ 的先验分布是 $\\text{Beta}(a_{00}, b_{00})$。数据提供了 $N_{00}$ 次从状态 0 到 0 的转移和 $N_{01}$ 次从状态 0 到 1 的转移。$p_{00}$ 的后验分布是：\n  $$\n  p(p_{00} \\mid S) \\sim \\text{Beta}(a_{00} + N_{00}, b_{00} + N_{01})\n  $$\n- 类似地，$p_{11}$ 的后验分布是：\n  $$\n  p(p_{11} \\mid S) \\sim \\text{Beta}(a_{11} + N_{11}, b_{11} + N_{10})\n  $$\n我们从这些 Beta 后验分布中为 $p_{00}$ 和 $p_{11}$ 抽取新样本。如果 $T=1$，则没有转移发生，后验分布与先验分布相同。\n\n**4. 估计与分类**\n\n在运行 Gibbs 采样器 N 次迭代并丢弃前 B 次作为预烧期后，我们得到 $N-B$ 个来自联合后验分布的样本。在时间 t 处于衰退状态 ($s_t = 1$) 的边际后验概率是通过对预烧期后的状态序列样本 $\\{S^{(i)}\\}_{i=B+1}^N$ 进行蒙特卡洛平均来估计的：\n$$\n\\hat{\\pi}_t = \\mathbb{P}(s_t = 1 \\mid y_{1:T}) \\approx \\frac{1}{N-B} \\sum_{i=B+1}^{N} \\mathbb{I}(s_t^{(i)}=1)\n$$\n如果这个估计概率大于或等于 0.5，则时间点 t 被归类为衰退。每个测试用例的最终结果是被归类为衰退的时间索引的总数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# No other libraries outside the Python standard library are permitted.\n\ndef run_gibbs_sampler(y, sigma_sq, priors, settings):\n    \"\"\"\n    Runs a Gibbs sampler for the specified Markov-switching model.\n    \"\"\"\n    T = len(y)\n    m0, V0 = priors['mu0']\n    m1, V1 = priors['mu1']\n    a00, b00 = priors['p00']\n    a11, b11 = priors['p11']\n    \n    num_iter = settings['N']\n    burn_in = settings['B']\n    seed = settings['seed']\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Initialize parameters by drawing from priors\n    mu0 = rng.normal(m0, np.sqrt(V0))\n    mu1 = rng.normal(m1, np.sqrt(V1))\n    p00 = rng.beta(a00, b00)\n    p11 = rng.beta(a11, b11)\n    \n    # Storage for post-burn-in state samples\n    num_samples_to_store = num_iter - burn_in\n    if num_samples_to_store = 0:\n        raise ValueError(\"Number of iterations must be greater than burn-in.\")\n    state_samples = np.zeros((num_samples_to_store, T), dtype=np.int8)\n    \n    # 2. Gibbs sampling iterations\n    for i in range(num_iter):\n        mus = np.array([mu0, mu1])\n        P = np.array([[p00, 1.0 - p00], [1.0 - p11, p11]])\n\n        # a. Sample states S = {s_t} using Forward-Filtering Backward-Sampling (FFBS)\n        \n        # Forward filtering\n        alpha_hat = np.zeros((T, 2))\n        \n        # t=1\n        likelihood_1 = norm.pdf(y[0], loc=mus, scale=np.sqrt(sigma_sq))\n        # Initial state prob = 0.5 for both states\n        alpha_hat[0, :] = 0.5 * likelihood_1\n        sum_alpha = np.sum(alpha_hat[0, :])\n        if sum_alpha > 0:\n            alpha_hat[0, :] /= sum_alpha\n\n        # t > 1\n        for t in range(1, T):\n            likelihood_t = norm.pdf(y[t], loc=mus, scale=np.sqrt(sigma_sq))\n            alpha_hat[t, :] = likelihood_t * (alpha_hat[t-1, :] @ P)\n            sum_alpha = np.sum(alpha_hat[t, :])\n            if sum_alpha > 0:\n                alpha_hat[t, :] /= sum_alpha\n        \n        # Backward sampling\n        states = np.zeros(T, dtype=np.int8)\n        \n        # t=T\n        p_sT = alpha_hat[T-1, :]\n        states[T-1] = rng.choice([0, 1], p=p_sT)\n\n        # t  T\n        for t in range(T-2, -1, -1):\n            s_next = states[t+1]\n            p_st = alpha_hat[t, :] * P[:, s_next]\n            sum_p = np.sum(p_st)\n            if sum_p > 0:\n                 p_st /= sum_p\n            else: # Fallback if probabilities are zero\n                p_st = np.array([0.5, 0.5])\n            states[t] = rng.choice([0, 1], p=p_st)\n\n        # b. Sample means mu_k\n        y_s0 = y[states == 0]\n        T0 = len(y_s0)\n        if T0 > 0:\n            V0_inv = 1.0 / V0\n            sigma_sq_inv = 1.0 / sigma_sq\n            V0_post_inv = V0_inv + T0 * sigma_sq_inv\n            V0_post = 1.0 / V0_post_inv\n            mu0_post = V0_post * (V0_inv * m0 + sigma_sq_inv * np.sum(y_s0))\n            mu0 = rng.normal(mu0_post, np.sqrt(V0_post))\n        else: # Sample from prior if state is not visited\n            mu0 = rng.normal(m0, np.sqrt(V0))\n\n        y_s1 = y[states == 1]\n        T1 = len(y_s1)\n        if T1 > 0:\n            V1_inv = 1.0 / V1\n            sigma_sq_inv = 1.0 / sigma_sq\n            V1_post_inv = V1_inv + T1 * sigma_sq_inv\n            V1_post = 1.0 / V1_post_inv\n            mu1_post = V1_post * (V1_inv * m1 + sigma_sq_inv * np.sum(y_s1))\n            mu1 = rng.normal(mu1_post, np.sqrt(V1_post))\n        else:\n            mu1 = rng.normal(m1, np.sqrt(V1))\n\n        # c. Sample transition probabilities p_kk\n        if T > 1:\n            N00 = np.sum((states[:-1] == 0)  (states[1:] == 0))\n            N01 = np.sum((states[:-1] == 0)  (states[1:] == 1))\n            N11 = np.sum((states[:-1] == 1)  (states[1:] == 1))\n            N10 = np.sum((states[:-1] == 1)  (states[1:] == 0))\n            \n            p00 = rng.beta(a00 + N00, b00 + N01)\n            p11 = rng.beta(a11 + N11, b11 + N10)\n        else: # T=1, no transitions, sample from priors\n            p00 = rng.beta(a00, b00)\n            p11 = rng.beta(a11, b11)\n\n        # Store sample if past burn-in\n        if i >= burn_in:\n            state_samples[i - burn_in, :] = states\n\n    # 3. Post-processing\n    # Estimate marginal posterior probability of recession (state 1)\n    # This is the mean of the indicator variable (0 or 1) across samples\n    pi_hat = np.mean(state_samples, axis=0)\n    \n    # Classify as recession if prob >= 0.5 and count\n    recession_count = np.sum(pi_hat >= 0.5)\n    \n    return int(recession_count)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        { # Case A\n            'y': np.array([0.010, 0.008, 0.009, 0.007, 0.006, 0.007, -0.004, -0.006, -0.005, -0.007, -0.006, -0.004, 0.005, 0.006, 0.008, 0.009, 0.007, 0.006, 0.005, 0.007]),\n            'sigma_sq': 0.000025,\n            'priors': {'mu0': (0.007, 0.0001), 'mu1': (-0.006, 0.0001), 'p00': (8, 2), 'p11': (8, 2)},\n            'settings': {'N': 6000, 'B': 3000, 'seed': 12345}\n        },\n        { # Case B\n            'y': np.array([0.000]),\n            'sigma_sq': 0.000025,\n            'priors': {'mu0': (0.004, 0.0001), 'mu1': (-0.004, 0.0001), 'p00': (5, 5), 'p11': (5, 5)},\n            'settings': {'N': 6000, 'B': 3000, 'seed': 12345}\n        },\n        { # Case C\n            'y': np.array([0.003, 0.004, 0.002, -0.001, 0.000, -0.002, -0.003, 0.001, 0.002, 0.003, -0.002, -0.001]),\n            'sigma_sq': 0.000025,\n            'priors': {'mu0': (0.002, 0.0002), 'mu1': (-0.002, 0.0002), 'p00': (2, 2), 'p11': (2, 2)},\n            'settings': {'N': 6000, 'B': 3000, 'seed': 12345}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_gibbs_sampler(case['y'], case['sigma_sq'], case['priors'], case['settings'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2398229"}]}