{"hands_on_practices": [{"introduction": "本练习提供了一个将理论工具直接应用于实践的场景。通过分析一个理想化的测量过程，我们将看到，随着测量精度的不断提高，测量结果序列如何向真实值收敛。这个练习旨在帮助你掌握使用切比雪夫不等式证明依概率收敛的基本方法，这是估计理论和测量科学中的一个基石概念。[@problem_id:1910709]", "problem": "一位工程师正在开发一种新的传感器，用于测量一个特定的物理属性，其真值为一个未知常数 $c$。该工程师进行了一系列测量。令 $X_n$ 为表示第 $n$ 次测量结果的随机变量，其中 $n=1, 2, 3, \\ldots$。由于实验装置的改进，测量是无偏的，这意味着任何一次测量的期望值都是真值，即对于所有 $n \\geq 1$，$E[X_n] = c$。每次尝试的测量精度都会提高，第 $n$ 次测量的方差为 $Var(X_n) = \\frac{\\sigma^2}{n^2}$，其中 $\\sigma$ 是一个已知的正常数，代表基准测量不确定度。\n\n测量序列 $\\{X_n\\}$ 依概率收敛于一个特定值。确定这个值。", "solution": "我们要求 $X_{n}$ 当 $n \\to \\infty$ 时的概率极限。根据定义，$X_{n}$ 依概率收敛于 $c$，如果对于任意 $\\varepsilon > 0$，有\n$$\n\\lim_{n \\to \\infty} P\\left(|X_{n} - c| \\ge \\varepsilon\\right) = 0.\n$$\n由于测量是无偏的，所以对于所有 $n \\geq 1$，$E[X_{n}] = c$。根据切比雪夫不等式，对于任意 $\\varepsilon > 0$，\n$$\nP\\left(|X_{n} - E[X_{n}]| \\geq \\varepsilon\\right) \\leq \\frac{\\operatorname{Var}(X_{n})}{\\varepsilon^{2}}.\n$$\n使用 $\\operatorname{Var}(X_{n}) = \\frac{\\sigma^{2}}{n^{2}}$ 和 $E[X_{n}] = c$，我们得到\n$$\nP\\left(|X_{n} - c| \\geq \\varepsilon\\right) \\leq \\frac{\\sigma^{2}}{n^{2}\\varepsilon^{2}}.\n$$\n当 $n \\to \\infty$ 时，上式右侧趋向于 $0$。因此，根据依概率收敛的定义，有 $X_{n} \\xrightarrow{P} c$。等价地，可以观察到\n$$\nE\\left[(X_{n} - c)^{2}\\right] = \\operatorname{Var}(X_{n}) = \\frac{\\sigma^{2}}{n^{2}} \\to 0,\n$$\n所以 $X_{n}$ 均方收敛于 $c$，这蕴涵了 $X_{n}$ 依概率收敛于 $c$。", "answer": "$$\\boxed{c}$$", "id": "1910709"}, {"introduction": "这个思想实验挑战了我们对数据和估计的直观理解。它提出了一个看似合理但实际上无效的估计量，该估计量虽然是无偏的，但其表现并不会随着样本量的增加而改善。通过分析这个反例，你将深刻理解为什么估计量的构造对于其一致性至关重要，并学会审慎地评估统计方法的有效性。[@problem_id:1910737]", "problem": "设 $X_1, X_2, \\dots, X_n$ 是一个来自总体的独立同分布（i.i.d.）随机变量序列，该总体具有有限均值 $E[X_i] = \\mu$ 和有限的非零方差 $Var(X_i) = \\sigma^2$。\n\n一位研究者提出了一个用于估计总体均值 $\\mu$ 的估计量，定义为 $\\hat{\\mu}_n = X_1$。该估计量只使用第一个观测值，与总样本量 $n$ 无关。我们希望分析该估计量的一致性。\n\n当样本量 $n$ 趋近于无穷大时，以下哪个陈述正确地描述了估计量 $\\hat{\\mu}_n$ 的收敛性质？\n\nA. $\\hat{\\mu}_n$ 依概率收敛到 $\\mu$，因为它是 $\\mu$ 的一个无偏估计量。\n\nB. $\\hat{\\mu}_n$ 依概率收敛到 $\\mu$，因为大数定律保证了当样本量 $n$ 无限增大时，任何估计量都会收敛。\n\nC. $\\hat{\\mu}_n$ 不依概率收敛到 $\\mu$，因为随着 $n$ 的增加，该估计量与 $\\mu$ 的距离大于某个小值的概率保持为一个固定的正数。\n\nD. $\\hat{\\mu}_n$ 不依概率收敛到 $\\mu$，因为该估计量是有偏的，而有偏估计量永远不可能是一致的。\n\nE. 在不知道 $X_i$ 变量的基础分布是否为正态分布的情况下，无法确定 $\\hat{\\mu}_n$ 是否依概率收敛到 $\\mu$。", "solution": "设 $\\{X_{i}\\}_{i=1}^{n}$ 独立同分布，其 $E[X_{i}] = \\mu$ 且 $\\operatorname{Var}(X_{i}) = \\sigma^{2}$，其中 $\\sigma^{2} \\in (0,\\infty)$。所提出的估计量为 $\\hat{\\mu}_{n} = X_{1}$，对所有 $n$ 成立。\n\n根据定义，$\\hat{\\mu}_{n}$ 是 $\\mu$ 的一致估计量，当且仅当对于任意 $\\varepsilon > 0$，\n$$\n\\lim_{n \\to \\infty} P\\!\\left(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon\\right) = 0.\n$$\n由于对所有 $n$ 都有 $\\hat{\\mu}_{n} = X_{1}$，我们对任意 $\\varepsilon > 0$ 和任意 $n$ 都有\n$$\nP\\!\\left(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon\\right) = P\\!\\left(|X_{1} - \\mu| > \\varepsilon\\right) =: p(\\varepsilon),\n$$\n它不依赖于 $n$。\n\n我们现在证明，对于至少一个 $\\varepsilon > 0$，$p(\\varepsilon)$ 严格为正。用反证法，假设对所有 $\\varepsilon > 0$ 都有 $p(\\varepsilon) = 0$。那么对于每个满足 $\\varepsilon_{k} \\downarrow 0$ 的有理数序列 $\\{\\varepsilon_{k}\\}$，\n$$\nP\\!\\left(|X_{1} - \\mu| \\le \\varepsilon_{k}\\right) = 1 \\quad \\text{for all } k,\n$$\n因此\n$$\nP\\!\\left(\\bigcap_{k=1}^{\\infty} \\{|X_{1} - \\mu| \\le \\varepsilon_{k}\\}\\right) = 1,\n$$\n这意味着 $P(X_{1} = \\mu) = 1$，因此 $\\operatorname{Var}(X_{1}) = 0$，这与 $\\sigma^{2} > 0$ 相矛盾。因此存在 $\\varepsilon_{0} > 0$ 使得\n$$\np(\\varepsilon_{0}) = P\\!\\left(|X_{1} - \\mu| > \\varepsilon_{0}\\right) > 0.\n$$\n所以，\n$$\n\\lim_{n \\to \\infty} P\\!\\left(|\\hat{\\mu}_{n} - \\mu| > \\varepsilon_{0}\\right) = \\lim_{n \\to \\infty} p(\\varepsilon_{0}) = p(\\varepsilon_{0}) > 0,\n$$\n所以一致性条件不成立。因此 $\\hat{\\mu}_{n}$ 不依概率收敛到 $\\mu$。\n\n对各选项的分析：\n- A 是错误的：仅无偏性并不意味着一致性。\n- B 是错误的：大数定律适用于依赖于 $n$ 的样本均值，而不适用于忽略了 $n$ 的 $X_{1}$。\n- C 是正确的：对于某个小的 $\\varepsilon$，误差概率是一个固定的正常数，与 $n$ 无关。\n- D 是错误的：该估计量是无偏的，并且在任何情况下，有偏性通常并不排除一致性。\n- E 是错误的：正态性是不必要的；该论证仅依赖于 $\\sigma^{2} > 0$。\n\n因此，正确选项是 C。", "answer": "$$\\boxed{C}$$", "id": "1910737"}, {"introduction": "最后的这个练习将我们的视野拓展到一个更精妙的层面。在这个问题中，一个随机变量序列本身并不依概率收敛到一个常数，但其概率分布却趋于一个稳定且非退化的形式。这个例子清晰地揭示了随机变量本身的收敛（依概率收敛）与其分布的收敛（依分布收敛）之间的重要区别，这是高级概率论和统计推断中的核心议题。[@problem_id:1910714]", "problem": "一个大规模计算系统由 $n$ 个相同且独立的处理器构成。系统的可靠性是一个关键问题，并且一旦第一个处理器发生故障，整个系统就被认为失效。每个独立处理器的寿命（以千小时为单位）被建模为一个在区间 $(0, 1)$ 上均匀分布的随机变量。令 $T_n$ 为包含 $n$ 个处理器的整个系统的寿命。为了解处理器数量非常多时系统的故障特性，一位工程师研究了一个被称为缩放寿命的量，定义为 $S_n = n T_n$。\n\n当处理器数量 $n$ 趋于无穷大时，下列哪个陈述最准确地描述了缩放寿命 $S_n$ 的行为？\n\nA. 缩放寿命 $S_n$ 依概率收敛于常数 0。\n\nB. 缩放寿命 $S_n$ 依概率收敛于常数 1。\n\nC. 缩放寿命 $S_n$ 依概率收敛于一个服从速率参数为 1 的指数分布的随机变量。\n\nD. 缩放寿命 $S_n$ 不依概率收敛于任何常数，但其分布收敛于一个速率参数为 1 的指数分布。\n\nE. 缩放寿命 $S_n$ 不收敛于任何明确定义的极限分布。", "solution": "令 $X_{1},\\dots,X_{n}$ 为独立同分布的随机变量，且 $X_{i}\\sim \\text{Uniform}(0,1)$。系统在第一个处理器发生故障时失效，因此系统寿命是最小值 $T_{n}=\\min\\{X_{1},\\dots,X_{n}\\}$。对于 $t\\in(0,1)$，利用独立性计算 $T_{n}$ 的生存函数：\n$$\nP(T_{n}>t)=P(X_{1}>t,\\dots,X_{n}>t)=\\prod_{i=1}^{n}P(X_{i}>t)=(1-t)^{n},\n$$\n并且对于 $t\\leq 0$，$P(T_{n}>t)=1$；对于 $t\\geq 1$，$P(T_{n}>t)=0$。定义缩放寿命 $S_{n}=nT_{n}$。对于 $s\\geq 0$ 且 $s\\leq n$，$S_{n}$ 的生存函数为\n$$\nP(S_{n}>s)=P(T_{n}>s/n)=(1-s/n)^{n}.\n$$\n对于任意固定的 $s\\geq 0$，当 $n\\to\\infty$ 时取极限，并利用标准极限 $\\lim_{n\\to\\infty}(1-s/n)^{n}=\\exp(-s)$ 可得\n$$\n\\lim_{n\\to\\infty}P(S_{n}>s)=\\exp(-s).\n$$\n等价地，其分布函数对于所有 $s\\geq 0$ 满足 $\\lim_{n\\to\\infty}P(S_{n}\\leq s)=1-\\exp(-s)$，这是一个速率参数为 1 的指数分布的累积分布函数。因此，$S_{n}$ 依分布收敛于 $\\text{Exp}(1)$。由于极限分布是非退化的，$S_{n}$ 不依概率收敛于任何常数；如果它收敛于某个常数，那么它将依分布收敛于一个在该常数上的退化分布，这与已建立的指数极限相矛盾。因此，最准确的描述是 $S_{n}$ 不依概率收敛于任何常数，但其分布收敛于 $\\text{Exp}(1)$。", "answer": "$$\\boxed{D}$$", "id": "1910714"}]}