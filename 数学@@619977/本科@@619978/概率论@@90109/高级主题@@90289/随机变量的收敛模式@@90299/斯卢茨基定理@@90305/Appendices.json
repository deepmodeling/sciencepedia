{"hands_on_practices": [{"introduction": "本练习探讨了斯卢茨基定理 (Slutsky's theorem) 的一个基础应用。我们将分析两个随机序列之比的极限行为，这在信号处理等领域中很常见，例如当一个含噪信号需要通过一个估计参数进行校准时。这个实践对于理解该定理如何简化对此类组合统计量的分析至关重要。[@problem_id:1955680]", "problem": "在一个高级信号处理应用中，一位工程师分析了两个独立的测量序列。第一个序列由随机变量 $\\{A_n\\}_{n=1}^{\\infty}$ 表示，它捕获了一个已经中心化的含噪信号。理论模型预测，随着测量次数 $n$ 趋于无穷大，$A_n$ 的分布趋近于一个均值为 0、方差为 $\\sigma^2$ 的正态分布。这可以正式地表示为 $A_n \\xrightarrow{d} N(0, \\sigma^2)$，其中 $\\sigma > 0$。\n\n第二个序列 $\\{B_n\\}_{n=1}^{\\infty}$ 是一个系统参数的自适应估计。已知这个估计量是一致的，依概率收敛于一个非零正常数 $c$。这可以正式地表示为 $B_n \\xrightarrow{p} c$，其中 $c > 0$。\n\n工程师对校准后信号的统计特性感兴趣，该信号定义为比率 $Z_n = \\frac{A_n}{B_n}$。请确定当 $n \\to \\infty$ 时，序列 $\\{Z_n\\}$ 的极限分布。\n\n从以下选项中选择对极限分布的正确描述。\n\nA. 一个均值为 0、方差为 $\\sigma^2 c^2$ 的正态分布。\n\nB. 一个自由度为 $n-1$ 的学生t分布。\n\nC. 一个均值为 0、方差为 $\\frac{\\sigma^2}{c^2}$ 的正态分布。\n\nD. 一个卡方分布。\n\nE. 该分布退化为常数值 0。\n\nF. 一个均值为 0、方差为 $\\sigma^2$ 的正态分布。", "solution": "给定 $A_{n} \\xrightarrow{d} N(0,\\sigma^2)$（其中 $\\sigma>0$）和 $B_{n} \\xrightarrow{p} c$（其中 $c>0$）。定义 $Z_{n}=\\frac{A_{n}}{B_{n}}$。\n\n因为 $B_{n} \\xrightarrow{p} c$ 且 $c \\neq 0$，对于任何满足 $0<\\varepsilon<c$ 的 $\\varepsilon$，我们有\n$$\n\\Pr\\big(|B_{n}-c|<\\varepsilon\\big) \\to 1,\n$$\n这意味着\n$$\n\\Pr\\big(|B_{n}|>c-\\varepsilon\\big) \\to 1.\n$$\n特别地，选择 $\\varepsilon=\\frac{c}{2}$ 可得 $\\Pr(|B_{n}|>\\frac{c}{2}) \\to 1$，因此除以 $B_{n}$ 的操作是良定义的，且概率趋于 1。\n\n根据 Slutsky 定理（或将连续映射定理应用于在 $c \\neq 0$ 时于 $(x,c)$ 点连续的函数 $f(x,y)=x/y$），由于 $A_{n} \\xrightarrow{d} X$（其中 $X \\sim N(0,\\sigma^2)$）且 $B_{n} \\xrightarrow{p} c$，可以得出\n$$\n\\frac{A_{n}}{B_{n}} \\xrightarrow{d} \\frac{X}{c}.\n$$\n如果 $X \\sim N(0,\\sigma^2)$，那么通过一个常数进行缩放可得\n$$\n\\frac{X}{c} \\sim N\\!\\left(0,\\frac{\\sigma^2}{c^2}\\right),\n$$\n因为对于任意常数 $a$，有 $\\operatorname{Var}(aX)=a^2\\operatorname{Var}(X)$ 和 $\\operatorname{E}[aX]=a\\,\\operatorname{E}[X]$。\n\n因此，$Z_{n}$ 的极限分布是均值为 0、方差为 $\\frac{\\sigma^2}{c^2}$ 的正态分布，这对应于选项 C。", "answer": "$$\\boxed{C}$$", "id": "1955680"}, {"introduction": "在基础应用的铺垫之上，这个问题展示了斯卢茨基定理如何成为连接其他基本极限定理的有力桥梁。我们将研究一个由两个独立序列的乘积构成的统计量的极限定理，其中一个序列遵循中心极限定理 (Central Limit Theorem)，另一个则遵循大数定律 (Law of Large Numbers)。本练习突显了该定理在综合不同结果以分析更复杂估计量中的核心作用。[@problem_id:840102]", "problem": "设 $X_1, X_2, \\dots, X_n$ 是来自一个总体的独立同分布 (i.i.d.) 随机变量序列，该总体的均值为有限值 $\\mu$，方差为有限非零值 $\\sigma^2$。\n设 $Y_1, Y_2, \\dots, Y_n$ 是第二个独立同分布的随机变量序列，其与第一个序列独立，服从成功概率为 $p$ 的伯努利分布，其中 $p \\in (0, 1)$。\n\n定义第一个序列的样本均值为 $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$，第二个序列的样本成功比例为 $\\hat{p}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$。\n\n考虑由下式定义的随机变量序列 $W_n$：\n$$W_n = \\hat{p}_n \\left( \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma} \\right)$$\n使用适当的极限定理，推导当 $n \\to \\infty$ 时 $W_n$ 的极限分布的概率密度函数 (PDF)。请将答案表示为变量 $w$ 和参数 $p$ 的函数。", "solution": "1. 根据关于 $\\{X_i\\}$ 的中心极限定理，\n$$Z_n=\\frac{\\sqrt{n}(\\bar X_n-\\mu)}{\\sigma}\\;\\xrightarrow{d}\\;Z\\sim N(0,1).$$\n2. 根据关于 $\\{Y_i\\}$ 的大数定律，\n$$\\hat p_n=\\frac1n\\sum_{i=1}^nY_i\\;\\xrightarrow{p}\\;p.$$\n3. 由于 $\\{X_i\\}$ 和 $\\{Y_i\\}$ 相互独立，因此 $Z_n$ 和 $\\hat p_n$ 也相互独立。根据 Slutsky 定理，\n$$W_n=\\hat p_n\\,Z_n\\;\\xrightarrow{d}\\;p\\,Z.$$\n4. 因此，极限 $pZ$ 服从均值为 $0$、方差为 $p^2$ 的正态分布，所以其概率密度函数为\n$$f(w)=\\frac1{\\sqrt{2\\pi\\,p^2}}\\exp\\!\\biggl(-\\frac{w^2}{2p^2}\\biggr)\n=\\frac1{p\\sqrt{2\\pi}}\\exp\\!\\biggl(-\\frac{w^2}{2p^2}\\biggr)。$$", "answer": "$$\\boxed{\\frac{1}{p\\sqrt{2\\pi}}\\exp\\!\\bigl(-\\tfrac{w^2}{2p^2}\\bigr)}$$", "id": "840102"}, {"introduction": "最后的这个实践将我们带入应用统计分析的领域，并提出了一个关键问题：当一个统计检验的假设被违反时会发生什么？我们将使用斯卢茨基定理来剖析一个被广泛使用的双样本 $t$ 统计量，它被错误地应用于方差不相等的数据。这个进阶问题展现了该定理作为诊断工具的强大能力，使我们能够评估统计方法的渐近性质和稳健性。[@problem_id:840045]", "problem": "考虑两个独立样本：$X_1, X_2, \\dots, X_{n_1}$ 是来自均值为 $\\mu_1$、方差为 $\\sigma_1^2$ 的总体的独立同分布 (i.i.d.) 随机变量，而 $Y_1, Y_2, \\dots, Y_{n_2}$ 是来自均值为 $\\mu_2$、方差为 $\\sigma_2^2$ 的总体的独立同分布 (i.i.d.) 随机变量。假设总体方差不相等，即 $\\sigma_1^2 \\neq \\sigma_2^2$。\n\n一位研究人员错误地假设方差相等，使用合并方差 t-统计量来检验原假设 $H_0: \\mu_1 = \\mu_2$。该统计量定义为：\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\n其中 $\\bar{X}_{n_1}$ 和 $\\bar{Y}_{n_2}$ 是样本均值，而 $S_p^2$ 是合并样本方差估计量：\n$$\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}\n$$\n其中 $S_1^2$ 和 $S_2^2$ 是无偏样本方差。\n\n我们关心的是当两个样本容量都很大时，该统计量的渐近行为。假设 $n_1 \\to \\infty$ 且 $n_2 \\to \\infty$，且它们的方式使得其相对比例保持不变。具体来说，令 $N = n_1 + n_2$，并假设当 $N \\to \\infty$ 时，比率 $\\frac{n_1}{N}$ 收敛到一个常数 $\\lambda \\in (0, 1)$。\n\n在原假设 $H_0: \\mu_1 = \\mu_2$ 下，统计量 $T_{n_1, n_2}$ 依分布收敛到一个正态分布，即 $T_{n_1, n_2} \\xrightarrow{d} N(0, V)$。\n\n请用 $\\sigma_1^2$、$\\sigma_2^2$ 和 $\\lambda$ 来表示渐近方差 $V$ 的表达式。", "solution": "该问题要求解错误使用的合并方差 t-统计量的渐近方差。该统计量由下式给出：\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\n我们将使用 Slutsky 定理来求极限分布。该定理指出，如果 $A_n \\xrightarrow{d} A$ 且 $B_n \\xrightarrow{p} b$（一个常数），则 $A_n / B_n \\xrightarrow{d} A/b$。\n\n**第 1 步：分析分子**\n\n首先，我们来确定分子 $\\bar{X}_{n_1} - \\bar{Y}_{n_2}$ 在经过适当标准化后的极限分布。在原假设 $H_0: \\mu_1 = \\mu_2$ 下，$\\bar{X}_{n_1} - \\bar{Y}_{n_2}$ 的均值为 $E[\\bar{X}_{n_1} - \\bar{Y}_{n_2}] = \\mu_1 - \\mu_2 = 0$。其方差为 $\\text{Var}(\\bar{X}_{n_1} - \\bar{Y}_{n_2}) = \\text{Var}(\\bar{X}_{n_1}) + \\text{Var}(\\bar{Y}_{n_2}) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$。\n\n根据中心极限定理 (CLT)，标准化的均值差依分布收敛于标准正态分布：\n$$\nZ_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\xrightarrow{d} N(0, 1)\n$$\n\n**第 2 步：重写统计量并分析分母**\n\n我们可以用 $Z_{n_1, n_2}$ 来重写统计量 $T_{n_1, n_2}$：\n$$\nT_{n_1, n_2} = \\frac{\\bar{X}_{n_1} - \\bar{Y}_{n_2}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\cdot \\frac{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}{\\sqrt{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} = Z_{n_1, n_2} \\cdot \\sqrt{\\frac{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n$$\n让我们将平方根内的项定义为 $W_{n_1, n_2}^2$ 的倒数：\n$$\nW_{n_1, n_2}^2 = \\frac{S_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n根据 Slutsky 定理，如果 $W_{n_1, n_2}$ 依概率收敛于一个常数 $W$，那么 $T_{n_1, n_2} \\xrightarrow{d} N(0, 1) / W \\sim N(0, 1/W^2)$。因此，我们的目标是求出 $W_{n_1, n_2}^2$ 的概率极限。\n\n**第 3 步：求合并方差估计量 $S_p^2$ 的概率极限**\n\n合并方差估计量为：\n$$\nS_p^2 = \\frac{n_1-1}{n_1+n_2-2} S_1^2 + \\frac{n_2-1}{n_1+n_2-2} S_2^2\n$$\n根据大数定律 (LLN)，样本方差是总体方差的一致估计量：$S_1^2 \\xrightarrow{p} \\sigma_1^2$ 且 $S_2^2 \\xrightarrow{p} \\sigma_2^2$。\n\n现在，我们来分析当 $n_1, n_2 \\to \\infty$ 时的系数。令 $N = n_1 + n_2$。根据题设，$\\frac{n_1}{N} \\to \\lambda$，这意味着 $\\frac{n_2}{N} = \\frac{N-n_1}{N} = 1 - \\frac{n_1}{N} \\to 1-\\lambda$。\n$$\n\\lim_{N\\to\\infty} \\frac{n_1-1}{N-2} = \\lim_{N\\to\\infty} \\frac{n_1/N - 1/N}{1 - 2/N} = \\frac{\\lambda - 0}{1-0} = \\lambda\n$$\n$$\n\\lim_{N\\to\\infty} \\frac{n_2-1}{N-2} = \\lim_{N\\to\\infty} \\frac{n_2/N - 1/N}{1-2/N} = \\frac{1-\\lambda - 0}{1-0} = 1-\\lambda\n$$\n根据连续映射定理（Slutsky 定理的一个推论），$S_p^2$ 的概率极限是：\n$$\nS_p^2 \\xrightarrow{p} \\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2\n$$\n\n**第 4 步：求方差和之比的极限**\n\n让我们分析比率 $\\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}$。我们可以将分子和分母同乘以 $N = n_1+n_2$：\n$$\n\\frac{N(\\frac{1}{n_1} + \\frac{1}{n_2})}{N(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})} = \\frac{\\frac{N}{n_1} + \\frac{N}{n_2}}{\\frac{N}{n_1}\\sigma_1^2 + \\frac{N}{n_2}\\sigma_2^2}\n$$\n当 $N \\to \\infty$ 时，我们有 $\\frac{N}{n_1} \\to \\frac{1}{\\lambda}$ 和 $\\frac{N}{n_2} \\to \\frac{1}{1-\\lambda}$。因此，该比率的极限是：\n$$\n\\lim_{N \\to \\infty} \\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} = \\frac{\\frac{1}{\\lambda} + \\frac{1}{1-\\lambda}}{\\frac{\\sigma_1^2}{\\lambda} + \\frac{\\sigma_2^2}{1-\\lambda}} = \\frac{\\frac{1-\\lambda+\\lambda}{\\lambda(1-\\lambda)}}{\\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda(1-\\lambda)}} = \\frac{1}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n\n**第 5 步：综合结果以求 $W_{n_1, n_2}^2$ 的极限**\n\n现在我们可以求出 $W_{n_1, n_2}^2$ 的概率极限：\n$$\nW_{n_1, n_2}^2 = S_p^2 \\cdot \\frac{\\frac{1}{n_1} + \\frac{1}{n_2}}{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n$$\n使用我们在第3步和第4步中推导出的极限：\n$$\nW^2 = \\lim_{n_1,n_2 \\to \\infty} W_{n_1, n_2}^2 \\xrightarrow{p} (\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2) \\cdot \\frac{1}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n$$\nW^2 = \\frac{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}\n$$\n\n**第 6 步：Slutsky 定理的最终应用**\n\n我们有 $T_{n_1, n_2} = Z_{n_1, n_2} / W_{n_1, n_2}$。由于 $Z_{n_1, n_2} \\xrightarrow{d} N(0, 1)$ 且 $W_{n_1, n_2} \\xrightarrow{p} W$，根据 Slutsky 定理：\n$$\nT_{n_1, n_2} \\xrightarrow{d} \\frac{N(0, 1)}{W} \\sim N\\left(0, \\frac{1}{W^2}\\right)\n$$\n因此，渐近方差 $V$ 为 $1/W^2$。\n$$\nV = \\frac{1}{W^2} = \\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}\n$$\n该表达式给出了错误应用的 t-统计量的极限正态分布的方差。", "answer": "$$\n\\boxed{\\frac{(1-\\lambda)\\sigma_1^2 + \\lambda\\sigma_2^2}{\\lambda \\sigma_1^2 + (1-\\lambda) \\sigma_2^2}}\n$$", "id": "840045"}]}