{
    "hands_on_practices": [
        {
            "introduction": "In fusion plasma research, a key challenge is analyzing complex time-series data from simulations or experiments to distinguish different transport mechanisms. This exercise focuses on a fundamental task: separating large, intermittent \"avalanche\" events from the continuous background turbulence. You will implement a common algorithm based on thresholding and time-windowing to identify avalanches in a synthetic energy flux signal, which is a crucial first step toward quantifying the role of avalanche-like transport and testing the Self-Organized Criticality (SOC) hypothesis. ",
            "id": "4181717",
            "problem": "Consider a one-dimensional global transport model of fusion plasma turbulence in which the energy supply to the system is characterized by an externally applied input power time series, and the outward energy transport is measured by an event-resolved radial energy flux time series. Self-Organized Criticality (SOC) describes how intermittent avalanche-like events emerge from threshold dynamics and long-range correlations in driven dissipative systems. In this problem, you will distinguish between avalanche transport and continuous turbulence transport using a threshold-and-duration criterion on the measured flux, and then integrate those contributions over time to quantify how much of the input energy is transported by each mechanism.\n\nYou are given a deterministic recipe to construct synthetic but physically plausible time series. Let the discrete time grid be $t_n = n \\,\\Delta t$ for $n \\in \\{0,1,\\dots,N-1\\}$, with sampling interval $\\Delta t$ expressed in seconds. The input power $P_{\\text{in}}(t)$ is expressed in Watts, the radial energy flux $q(t)$ is expressed in Watts, and time $t$ is expressed in seconds. Energy, obtained by integrating power over time, is expressed in Joules. Avalanche intervals are defined by a detection threshold and post-processing rules consistent with event aggregation in SOC phenomenology:\n\n- Define a binary indicator $a_n$ as $a_n = 1$ if $q(t_n) \\ge T_{\\text{th}}$ and $a_n = 0$ otherwise.\n- Identify contiguous segments of indices where $a_n = 1$. These are preliminary avalanche segments.\n- Merge adjacent preliminary segments if the temporal gap between them is less than or equal to a refractory gap $\\tau_{\\text{gap}}$. After merging, discard any segment whose duration is less than a minimum avalanche duration $\\tau_{\\min}$.\n- The final avalanche set $\\mathcal{A}$ is the union of all retained segments. The complement set $\\mathcal{C}$ contains all times not in $\\mathcal{A}$.\n\nFrom first principles, the transported energies over a time window are the time integrals of power over that window. Compute the avalanche-interval transported energy\n$$\nQ_{\\text{av}} = \\int_{\\mathcal{A}} q(t)\\, dt\n$$\nand the continuous turbulence transported energy\n$$\nQ_{\\text{ct}} = \\int_{\\mathcal{C}} q(t)\\, dt,\n$$\nas well as the total input energy\n$$\nE_{\\text{in}} = \\int_{0}^{T_{\\text{end}}} P_{\\text{in}}(t)\\, dt,\n$$\nwhere $T_{\\text{end}} = (N-1)\\,\\Delta t$. Use the discrete-time approximation of the integrals, i.e., Riemann sums on the provided grid.\n\nReport the dimensionless fractions\n$$\nf_{\\text{av}} = \\frac{Q_{\\text{av}}}{E_{\\text{in}}}\n\\quad\\text{and}\\quad\nf_{\\text{ct}} = \\frac{Q_{\\text{ct}}}{E_{\\text{in}}}.\n$$\nThese are ratios of energies and thus unitless. The program must round each reported value to $6$ decimal places.\n\nSynthetic dataset construction rules are as follows. The background flux is a sum of a proportional part and a small sinusoidal modulation:\n$$\nq_{\\text{bg}}(t) = \\beta \\, P_{\\text{in}}(t) + \\gamma \\, \\sin(2\\pi f_{\\text{bg}} t) \\, P_0,\n$$\nwhere $P_0$ is a specified reference power. Avalanche pulses are deterministic triangular bursts added on top of the background:\n$$\nW(t; t_0, w) = \\max\\!\\left(0,\\,1 - \\frac{|t - t_0|}{w}\\right),\n\\quad\nq_{\\text{pulse}}(t) = \\sum_{i} A_i \\, W(t; t_{0,i}, w_i).\n$$\nThe total flux is\n$$\nq(t) = q_{\\text{bg}}(t) + q_{\\text{pulse}}(t).\n$$\n\nImplement the detection and integration procedure precisely as described, using the following test suite. In all cases, express time in seconds and power in Watts. The fractions $f_{\\text{av}}$ and $f_{\\text{ct}}$ are unitless decimals.\n\nTest Case $1$ (happy path moderate avalanches):\n- $\\Delta t = 10^{-3}$,\n- $N = 5000$,\n- $P_0 = 1.0 \\times 10^6$,\n- $P_{\\text{in}}(t) = P_0$,\n- $\\beta = 0.70$,\n- $\\gamma = 0.02$,\n- $f_{\\text{bg}} = 1.0$,\n- Avalanche centers $t_{0,i} \\in \\{0.5,\\,1.0,\\,2.5,\\,3.0\\}$,\n- Common width $w_i = 0.05$,\n- Common amplitude $A_i = 0.30\\,P_0$,\n- Threshold $T_{\\text{th}} = 0.90\\,P_0$,\n- Minimum duration $\\tau_{\\min} = 0.020$,\n- Refractory gap $\\tau_{\\text{gap}} = 0.010$.\n\nTest Case $2$ (no detected avalanches due to high threshold):\n- Same $t_n$, $P_{\\text{in}}(t)$, and $q(t)$ construction as in Test Case $1$,\n- Threshold $T_{\\text{th}} = 1.10\\,P_0$,\n- Minimum duration $\\tau_{\\min} = 0.020$,\n- Refractory gap $\\tau_{\\text{gap}} = 0.010$.\n\nTest Case $3$ (all times counted as avalanche due to low threshold):\n- Same $t_n$, $P_{\\text{in}}(t)$, and $q(t)$ construction as in Test Case $1$,\n- Threshold $T_{\\text{th}} = 0.65\\,P_0$,\n- Minimum duration $\\tau_{\\min} = 0.005$,\n- Refractory gap $\\tau_{\\text{gap}} = 0.005$.\n\nTest Case $4$ (variable input power and different avalanche parameters):\n- $\\Delta t = 10^{-3}$,\n- $N = 3000$,\n- $P_0 = 1.0 \\times 10^6$,\n- $P_{\\text{in}}(t) = P_0\\left(1 + 0.10 \\sin(2\\pi f_{\\text{in}} t)\\right)$ with $f_{\\text{in}} = 0.5$,\n- $\\beta = 0.60$,\n- $\\gamma = 0.02$,\n- $f_{\\text{bg}} = 3.0$,\n- Avalanche centers $t_{0,i} \\in \\{0.7,\\,1.5,\\,2.1\\}$,\n- Common width $w_i = 0.03$,\n- Common amplitude $A_i = 0.40\\,P_0$,\n- Threshold $T_{\\text{th}} = 0.85\\,P_0$,\n- Minimum duration $\\tau_{\\min} = 0.015$,\n- Refractory gap $\\tau_{\\text{gap}} = 0.008$.\n\nTest Case $5$ (pulses shorter than minimum duration, most transport counted as continuous):\n- $\\Delta t = 10^{-3}$,\n- $N = 2000$,\n- $P_0 = 1.0 \\times 10^6$,\n- $P_{\\text{in}}(t) = P_0$,\n- $\\beta = 0.65$,\n- $\\gamma = 0.01$,\n- $f_{\\text{bg}} = 2.0$,\n- Avalanche centers $t_{0,i} \\in \\{1.0,\\,1.5\\}$,\n- Common width $w_i = 0.005$,\n- Common amplitude $A_i = 0.35\\,P_0$,\n- Threshold $T_{\\text{th}} = 0.90\\,P_0$,\n- Minimum duration $\\tau_{\\min} = 0.020$,\n- Refractory gap $\\tau_{\\text{gap}} = 0.005$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, append two values in order: first $f_{\\text{av}}$, then $f_{\\text{ct}}$. Round each value to $6$ decimal places. For example, if there are $M$ test cases, the final output should contain $2M$ numbers and look like $[x_1,x_2,\\dots,x_{2M}]$.",
            "solution": "The problem presents a well-defined computational task grounded in the physics of transport in fusion plasmas. It requires the implementation of a specific algorithm to analyze a synthetic time series of energy flux, partitioning it into contributions from intermittent \"avalanche\" events and continuous background turbulence. This is a common paradigm in the study of Self-Organized Criticality (SOC), where systems spontaneously develop long-range correlations and release energy through scale-invariant events. The problem is valid as it is scientifically grounded, objective, and provides a complete, self-contained specification for both data generation and analysis.\n\nThe solution proceeds by first constructing the synthetic data according to the provided deterministic rules and then applying the specified event detection and integration algorithm. Each step is detailed below.\n\n### 1. Synthetic Data Generation\n\nThe foundation of the analysis is the time series for input power, $P_{\\text{in}}(t)$, and radial energy flux, $q(t)$. These are generated on a discrete time grid $t_n = n \\Delta t$ for $n \\in \\{0, 1, \\dots, N-1\\}$.\n\n- **Time Grid**: A one-dimensional array representing the time points $t_n$ is created.\n- **Input Power ($P_{\\text{in}}$)**: The input power is either a constant value, $P_0$, or a sinusoidal function of time, $P_{\\text{in}}(t) = P_0(1 + 0.10 \\sin(2\\pi f_{\\text{in}} t))$, as specified for each test case.\n- **Background Flux ($q_{\\text{bg}}$)**: The continuous component of the flux is modeled as $q_{\\text{bg}}(t) = \\beta P_{\\text{in}}(t) + \\gamma \\sin(2\\pi f_{\\text{bg}} t) P_0$. The first term, $\\beta P_{\\text{in}}(t)$, represents transport directly driven by the input power, while the second term introduces a small, persistent oscillation.\n- **Avalanche Pulse Flux ($q_{\\text{pulse}}$)**: Intermittent transport events are modeled as a series of triangular pulses. Each pulse is defined by the function $W(t; t_0, w) = \\max(0, 1 - |t - t_0|/w)$, which creates a triangular shape centered at $t_0$ with a half-width of $w$. The total pulse flux is the superposition of these individual events: $q_{\\text{pulse}}(t) = \\sum_{i} A_i W(t; t_{0,i}, w_i)$.\n- **Total Flux ($q$)**: The final radial flux time series is the sum of the background and pulse components: $q(t) = q_{\\text{bg}}(t) + q_{\\text{pulse}}(t)$.\n\n### 2. Avalanche Detection Algorithm\n\nThe core of the problem is to identify the time intervals corresponding to avalanches. This is achieved through a multi-step process designed to capture the essence of event detection in noisy, complex signals.\n\n- **Step 2a: Thresholding**: A binary indicator time series, $a_n$, is created. An avalanche is provisionally detected at time $t_n$ if the flux $q(t_n)$ exceeds a specified threshold $T_{\\text{th}}$.\n$$\na_n =\n\\begin{cases}\n1 & \\text{if } q(t_n) \\ge T_{\\text{th}} \\\\\n0 & \\text{if } q(t_n) < T_{\\text{th}}\n\\end{cases}\n$$\n- **Step 2b: Identifying Preliminary Segments**: Contiguous blocks of $a_n=1$ are identified. Each block represents a preliminary avalanche segment, defined by a start index and an end index on the time grid. This can be implemented efficiently by finding the indices where the difference of the padded binary array changes from $0$ to $1$ (starts) and $1$ to $0$ (ends).\n\n- **Step 2c: Merging Adjacent Segments**: In SOC systems, a large avalanche can appear as a rapid succession of smaller peaks. To account for this, we merge adjacent segments that are separated by a small time gap. If the temporal gap between the end of one segment and the start of the next is less than or equal to a user-defined refractory gap, $\\tau_{\\text{gap}}$, the two segments are combined into a single, longer one. This process is repeated until no more segments can be merged.\n\n- **Step 2d: Filtering by Duration**: True avalanche events are expected to have a certain persistence. Therefore, any merged segment whose total duration is less than a minimum avalanche duration, $\\tau_{\\min}$, is discarded. The duration of a segment spanning from index $i$ to $j$ is calculated as $(j - i) \\Delta t$.\n\nThe final set of time intervals, $\\mathcal{A}$, is the union of all segments that survive this filtering process. The complement set, $\\mathcal{C}$, comprises all other time points. A boolean mask over the time array is an effective way to represent these sets, where `True` indicates membership in $\\mathcal{A}$.\n\n### 3. Energy Integration and Fraction Calculation\n\nWith the time series partitioned into avalanche ($\\mathcal{A}$) and continuous turbulence ($\\mathcal{C}$) intervals, the next step is to quantify the energy transported by each mechanism. According to first principles, energy is the time integral of power. We approximate these integrals using Riemann sums over the discrete grid.\n\n- **Total Input Energy ($E_{\\text{in}}$)**: The total energy supplied to the system over the full duration is the integral of the input power.\n$$\nE_{\\text{in}} = \\int_{0}^{T_{\\text{end}}} P_{\\text{in}}(t)\\, dt \\approx \\sum_{n=0}^{N-1} P_{\\text{in}}(t_n) \\Delta t\n$$\n- **Avalanche Transported Energy ($Q_{\\text{av}}$)**: This is the integral of the flux $q(t)$ only over the avalanche intervals $\\mathcal{A}$.\n$$\nQ_{\\text{av}} = \\int_{\\mathcal{A}} q(t)\\, dt \\approx \\sum_{n \\in \\mathcal{A}} q(t_n) \\Delta t\n$$\n- **Continuous Turbulence Transported Energy ($Q_{\\text{ct}}$)**: This is the integral of the flux $q(t)$ over the remaining time intervals $\\mathcal{C}$.\n$$\nQ_{\\text{ct}} = \\int_{\\mathcal{C}} q(t)\\, dt \\approx \\sum_{n \\in \\mathcal{C}} q(t_n) \\Delta t\n$$\nNote that by construction, $Q_{\\text{av}} + Q_{\\text{ct}} = \\sum_{n=0}^{N-1} q(t_n) \\Delta t$, which is the total transported energy.\n\nFinally, the dimensionless fractions $f_{\\text{av}}$ and $f_{\\text{ct}}$ are computed by normalizing the transported energies by the total input energy. These fractions represent the portion of the energy budget accounted for by each transport channel.\n\n$$\nf_{\\text{av}} = \\frac{Q_{\\text{av}}}{E_{\\text{in}}} \\quad \\text{and} \\quad f_{\\text{ct}} = \\frac{Q_{\\text{ct}}}{E_{\\text{in}}}\n$$\n\nThe final values are rounded to $6$ decimal places as required. This entire procedure is encapsulated in a function and applied to each of the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case for avalanche analysis.\n    \n    Generates synthetic data, detects avalanches based on thresholding,\n    merging, and duration filtering, and computes energy transport fractions.\n    \"\"\"\n    # 1. Unpack parameters and generate time grid\n    dt = params['dt']\n    N = params['N']\n    P0 = params['P0']\n    \n    time = np.arange(N) * dt\n    \n    # 2. Generate synthetic time series data\n    # 2a. Input Power P_in(t)\n    if 'fin' in params:\n        P_in = P0 * (1.0 + 0.10 * np.sin(2 * np.pi * params['fin'] * time))\n    else:\n        P_in = np.full(N, P0)\n\n    # 2b. Background flux q_bg(t)\n    q_bg = params['beta'] * P_in + params['gamma'] * np.sin(2 * np.pi * params['fbg'] * time) * P0\n    \n    # 2c. Pulse flux q_pulse(t)\n    q_pulse = np.zeros(N)\n    for t0 in params['t0s']:\n        pulse = 1.0 - np.abs(time - t0) / params['w']\n        pulse = np.maximum(0, pulse)\n        q_pulse += params['A'] * pulse\n\n    # 2d. Total flux q(t)\n    q_total = q_bg + q_pulse\n    \n    # 3. Avalanche Detection Algorithm\n    T_th = params['T_th']\n    tau_gap = params['tau_gap']\n    tau_min = params['tau_min']\n\n    # 3a. Thresholding\n    a = q_total >= T_th\n    \n    # 3b. Identify preliminary segments\n    # Pad with False to correctly identify segments at the start/end\n    padded_a = np.concatenate(([False], a, [False]))\n    diffs = np.diff(padded_a.astype(int))\n    \n    starts = np.where(diffs == 1)[0]\n    ends = np.where(diffs == -1)[0] - 1\n    \n    if len(starts) == 0:\n        preliminary_segments = []\n    else:\n        preliminary_segments = [[start, end] for start, end in zip(starts, ends)]\n\n    # 3c. Merge adjacent segments\n    if not preliminary_segments:\n        merged_segments = []\n    else:\n        merged_segments = [preliminary_segments[0]]\n        for i in range(1, len(preliminary_segments)):\n            last_end = merged_segments[-1][1]\n            next_start = preliminary_segments[i][0]\n            \n            # Gap duration in seconds\n            gap_duration = (next_start - last_end - 1) * dt\n            \n            if gap_duration <= tau_gap:\n                # Merge by extending the end of the last segment\n                merged_segments[-1][1] = preliminary_segments[i][1]\n            else:\n                # Add as a new segment\n                merged_segments.append(preliminary_segments[i])\n\n    # 3d. Filter by minimum duration\n    final_segments = []\n    for start, end in merged_segments:\n        # Duration in seconds\n        duration = (end - start) * dt\n        if duration >= tau_min:\n            final_segments.append([start, end])\n            \n    # 4. Energy Integration\n    # Create a boolean mask representing the avalanche set A\n    is_avalanche = np.full(N, False, dtype=bool)\n    for start, end in final_segments:\n        is_avalanche[start:end+1] = True\n        \n    # Calculate energies using Riemann sums\n    E_in = np.sum(P_in) * dt\n    \n    # Check for E_in == 0 to prevent division by zero, though not expected here\n    if E_in == 0:\n        return 0.0, 0.0\n\n    Q_av = np.sum(q_total[is_avalanche]) * dt\n    Q_ct = np.sum(q_total[~is_avalanche]) * dt\n    \n    # 5. Calculate and round fractions\n    f_av = round(Q_av / E_in, 6)\n    f_ct = round(Q_ct / E_in, 6)\n    \n    return f_av, f_ct\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {\n            'dt': 1e-3, 'N': 5000, 'P0': 1.0e6,\n            'beta': 0.70, 'gamma': 0.02, 'fbg': 1.0,\n            't0s': [0.5, 1.0, 2.5, 3.0], 'w': 0.05, 'A': 0.30e6,\n            'T_th': 0.90e6, 'tau_min': 0.020, 'tau_gap': 0.010\n        },\n        # Test Case 2\n        {\n            'dt': 1e-3, 'N': 5000, 'P0': 1.0e6,\n            'beta': 0.70, 'gamma': 0.02, 'fbg': 1.0,\n            't0s': [0.5, 1.0, 2.5, 3.0], 'w': 0.05, 'A': 0.30e6,\n            'T_th': 1.10e6, 'tau_min': 0.020, 'tau_gap': 0.010\n        },\n        # Test Case 3\n        {\n            'dt': 1e-3, 'N': 5000, 'P0': 1.0e6,\n            'beta': 0.70, 'gamma': 0.02, 'fbg': 1.0,\n            't0s': [0.5, 1.0, 2.5, 3.0], 'w': 0.05, 'A': 0.30e6,\n            'T_th': 0.65e6, 'tau_min': 0.005, 'tau_gap': 0.005\n        },\n        # Test Case 4\n        {\n            'dt': 1e-3, 'N': 3000, 'P0': 1.0e6, 'fin': 0.5,\n            'beta': 0.60, 'gamma': 0.02, 'fbg': 3.0,\n            't0s': [0.7, 1.5, 2.1], 'w': 0.03, 'A': 0.40e6,\n            'T_th': 0.85e6, 'tau_min': 0.015, 'tau_gap': 0.008\n        },\n        # Test Case 5\n        {\n            'dt': 1e-3, 'N': 2000, 'P0': 1.0e6,\n            'beta': 0.65, 'gamma': 0.01, 'fbg': 2.0,\n            't0s': [1.0, 1.5], 'w': 0.005, 'A': 0.35e6,\n            'T_th': 0.90e6, 'tau_min': 0.020, 'tau_gap': 0.005\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        f_av, f_ct = process_case(case)\n        results.append(f_av)\n        results.append(f_ct)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond identifying avalanches in data, it is insightful to understand the physical mechanisms that drive their formation and propagation. This practice explores a simplified but powerful model of turbulence spreading using a reaction-diffusion equation. You will simulate how a localized turbulent perturbation in an unstable region can grow and \"invade\" a linearly stable region of the plasma, demonstrating the key phenomenon of non-local transport that is a hallmark of avalanche dynamics. ",
            "id": "4181750",
            "problem": "Consider a one-dimensional radial domain with coordinate $r \\in [0,L]$ representing a minor radius in a Magnetic Confinement Fusion (MCF) device. Let $I(r,t)$ denote a dimensionless measure of local turbulent intensity. Assume the evolution of $I(r,t)$ in normalized units is governed by a reaction-diffusion equation derived from a local energy balance, with a space-dependent linear growth (or damping) rate $\\gamma(r)$, a nonlinear saturation at $I_{\\mathrm{sat}}$, and an effective turbulent diffusion coefficient $D$:\n$$\n\\frac{\\partial I(r,t)}{\\partial t} = \\gamma(r)\\, I(r,t)\\left(1 - \\frac{I(r,t)}{I_{\\mathrm{sat}}}\\right) + D\\, \\frac{\\partial^2 I(r,t)}{\\partial r^2}.\n$$\nThis form models the competition between linear drive and nonlinear saturation in a fusion plasma, capturing the onset and propagation of avalanches coupled by spatial transport. The Self-Organized Criticality (SOC) perspective is represented by the threshold-like behavior: regions with negative $\\gamma(r)$ are subcritical and locally damp turbulence, but finite-amplitude perturbations from adjacent supercritical regions can still transiently invade those nominally stable zones via diffusive coupling.\n\nDefine the radially varying growth rate $\\gamma(r)$ to have a supercritical core and a subcritical edge with a smooth transition:\n$$\n\\gamma(r) = \\frac{\\gamma_0 + \\gamma_1}{2} + \\frac{\\gamma_0 - \\gamma_1}{2}\\, \\tanh\\!\\left(\\frac{r_c - r}{\\Delta}\\right),\n$$\nwhere $r_c$ is the center of the transition, $\\Delta$ controls its radial sharpness, $\\gamma_0 > 0$ (supercritical) and $\\gamma_1 < 0$ (subcritical). Use homogeneous Neumann boundary conditions at $r=0$ and $r=L$:\n$$\n\\left.\\frac{\\partial I}{\\partial r}\\right|_{r=0} = 0,\\quad \\left.\\frac{\\partial I}{\\partial r}\\right|_{r=L} = 0,\n$$\nand initialize with a localized finite-amplitude Gaussian perturbation centered just inside the supercritical side of the transition:\n$$\nI(r,0) = A \\exp\\!\\left(-\\frac{(r-r_0)^2}{2\\sigma^2}\\right),\\quad r_0 = r_c - \\delta.\n$$\n\nFor numerical simulation, discretize the domain uniformly with $N$ grid points and advance the system in time using an explicit scheme stable for the specified coefficients. The units are dimensionless, and the domain size is $L = 1$. The parameters to be used in all cases, unless varied in the test suite, are:\n- $L = 1$\n- $N = 256$\n- $\\gamma_0 = 1$\n- $\\gamma_1 = -0.6$\n- $r_c = 0.6$\n- $\\Delta = 0.02$\n- $I_{\\mathrm{sat}} = 1$\n- $\\sigma = 0.015$\n- $\\delta = 0.03$\n- Simulation horizon $T = 0.6$\n- Threshold for defining avalanche penetration $I_{\\mathrm{th}} = 0.05$\n\nDefine the avalanche penetration depth into the subcritical region as the maximum radial excursion beyond $r_c$ at which the intensity exceeds the threshold at any simulation time:\n$$\nd_{\\max} = \\max_{t \\in [0,T]} \\left(\\max\\{\\, r - r_c \\mid r \\ge r_c,\\ I(r,t) \\ge I_{\\mathrm{th}} \\,\\}\\right),\n$$\nand if no such $r$ exists set $d_{\\max} = 0$.\n\nYour task is to implement this simulation and compute $d_{\\max}$ for each of the following test cases, which vary the perturbation amplitude $A$ and the diffusion coefficient $D$:\n- Case $1$: $A = 0.02$, $D = 0.001$\n- Case $2$: $A = 0.15$, $D = 0.001$\n- Case $3$: $A = 0.15$, $D = 0.005$\n- Case $4$: $A = 0.40$, $D = 0.001$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above, for example $[d_1,d_2,d_3,d_4]$, where each $d_i$ is the computed $d_{\\max}$ for Case $i$. All quantities are dimensionless, so no physical units are required. Ensure scientific realism by using an explicit scheme with a time step chosen to satisfy stability for the diffusion term and reaction term, and enforce nonnegativity of $I(r,t)$ in the update.",
            "solution": "The problem requires the numerical solution of a one-dimensional, nonlinear partial differential equation (PDE) of the reaction-diffusion type. The approach will be based on the method of lines, where the spatial domain is discretized first, converting the PDE into a system of coupled ordinary differential equations (ODEs), which are then solved using a time-stepping scheme.\n\n**1. Spatial Discretization**\n\nThe radial domain $r \\in [0, L]$ is discretized into $N$ uniformly spaced grid points, $r_i = i \\cdot \\Delta r$ for $i=0, 1, \\dots, N-1$. The grid spacing is $\\Delta r = L / (N-1)$. The intensity $I(r_i, t)$ at each grid point is denoted as $I_i(t)$.\n\nThe spatial derivative term, the Laplacian $\\frac{\\partial^2 I}{\\partial r^2}$, is approximated using a second-order central difference formula for the interior points ($i=1, \\dots, N-2$):\n$$\n\\left.\\frac{\\partial^2 I}{\\partial r^2}\\right|_{r_i} \\approx \\frac{I_{i+1}(t) - 2I_i(t) + I_{i-1}(t)}{(\\Delta r)^2}\n$$\n\nThe homogeneous Neumann boundary conditions, $\\frac{\\partial I}{\\partial r} = 0$, are implemented by introducing \"ghost points\" $I_{-1}$ and $I_N$. The condition at $r=0$ implies $\\frac{I_1-I_{-1}}{2\\Delta r} = 0$, so $I_{-1} = I_1$. The condition at $r=L$ implies $\\frac{I_N-I_{N-2}}{2\\Delta r} = 0$, so $I_N = I_{N-2}$. Substituting these into the central difference formula for the boundary points ($i=0$ and $i=N-1$) yields:\n$$\n\\left.\\frac{\\partial^2 I}{\\partial r^2}\\right|_{r_0} \\approx \\frac{I_1 - 2I_0 + I_{-1}}{(\\Delta r)^2} = \\frac{2(I_1 - I_0)}{(\\Delta r)^2}\n$$\n$$\n\\left.\\frac{\\partial^2 I}{\\partial r^2}\\right|_{r_{N-1}} \\approx \\frac{I_N - 2I_{N-1} + I_{N-2}}{(\\Delta r)^2} = \\frac{2(I_{N-2} - I_{N-1})}{(\\Delta r)^2}\n$$\n\n**2. Time Integration**\n\nWith the spatial derivatives discretized, the PDE transforms into a system of $N$ coupled ODEs of the form $\\frac{dI_i}{dt} = F_i(I_0, I_1, \\dots, I_{N-1})$. A simple and explicit method, the Forward Euler scheme, is used for time integration. The intensity at the next time step $t^{n+1} = t^n + \\Delta t$ is given by:\n$$\nI_i^{n+1} = I_i^n + \\Delta t \\left[ \\gamma(r_i) I_i^n \\left(1 - \\frac{I_i^n}{I_{\\mathrm{sat}}}\\right) + D \\left(\\frac{\\partial^2 I}{\\partial r^2}\\right)_i^n \\right]\n$$\nwhere $(\\frac{\\partial^2 I}{\\partial r^2})_i^n$ is the finite-difference approximation of the Laplacian at grid point $i$ and time step $n$.\n\n**3. Stability Condition**\n\nExplicit schemes are only conditionally stable. The time step $\\Delta t$ must be small enough to prevent numerical instabilities. For the given reaction-diffusion equation, the stability is constrained by both the diffusion term and the reaction term. A conservative stability condition is:\n$$\n\\Delta t \\le \\frac{1}{\\frac{2D}{(\\Delta r)^2} + \\max|\\gamma(r)|}\n$$\nThe most restrictive component is the diffusion term. The a Courant-Friedrichs-Lewy (CFL) condition for the diffusion part is $\\Delta t \\le \\frac{(\\Delta r)^2}{2D}$. To ensure stability across all simulations and account for the nonlinear reaction term, we choose a time step $\\Delta t = C_s \\frac{(\\Delta r)^2}{D}$ for each case, with a safety factor $C_s < 0.5$. A value of $C_s=0.45$ is chosen.\n\n**4. Algorithmic Implementation**\n\nFor each test case (defined by a pair of parameters $A$ and $D$):\n1.  Set up the spatial grid $r$ and pre-calculate the growth rate profile $\\gamma(r)$.\n2.  Initialize the intensity profile $I(r,0)$ according to the specified Gaussian function.\n3.  Determine the stable time step $\\Delta t$ based on the given $D$.\n4.  Initialize the maximum penetration depth $d_{\\max}$ to $0$.\n5.  Iterate in time from $t=0$ to $T$. In each step:\n    a.  Compute the reaction term and the discretized diffusion term for all grid points.\n    b.  Update the intensity profile $I$ using the Forward Euler method.\n    c.  Enforce the physical constraint of non-negativity by setting any $I_i < 0$ to $0$.\n    d.  Identify the grid points in the subcritical region ($r_i \\ge r_c$) where the intensity exceeds the threshold ($I_i \\ge I_{\\mathrm{th}}$).\n    e.  If such points exist, find the maximum radial coordinate among them, $r_{\\max,t}$, and calculate the current penetration depth $r_{\\max,t} - r_c$.\n    f.  Update the overall maximum penetration: $d_{\\max} = \\max(d_{\\max}, r_{\\max,t} - r_c)$.\n6.  After the simulation time $T$ is reached, the final value of $d_{\\max}$ is recorded.\n\nThis procedure is repeated for all four test cases, and the results are collected and formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    \n    test_cases = [\n        # (A, D)\n        (0.02, 0.001),\n        (0.15, 0.001),\n        (0.15, 0.005),\n        (0.40, 0.001),\n    ]\n\n    results = []\n    for A, D in test_cases:\n        d_max = run_simulation(A, D)\n        results.append(f\"{d_max:.8f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_simulation(A, D):\n    \"\"\"\n    Executes the reaction-diffusion simulation for a single set of parameters.\n\n    Args:\n        A (float): Amplitude of the initial Gaussian perturbation.\n        D (float): Diffusion coefficient.\n\n    Returns:\n        float: The calculated maximum penetration depth (d_max).\n    \"\"\"\n    # --- 1. Define Fixed Parameters and Grid ---\n    L = 1.0           # Domain length\n    N = 256           # Number of grid points\n    gamma_0 = 1.0     # Supercritical growth rate\n    gamma_1 = -0.6    # Subcritical damping rate\n    r_c = 0.6         # Transition center\n    Delta = 0.02      # Transition sharpness\n    I_sat = 1.0       # Saturation intensity\n    sigma = 0.015     # Initial perturbation width\n    delta = 0.03      # Initial perturbation offset\n    T = 0.6           # Simulation time horizon\n    I_th = 0.05       # Penetration threshold\n\n    r = np.linspace(0, L, N)\n    dr = L / (N - 1)\n    \n    # --- 2. Initial and Boundary Conditions & Profiles ---\n    \n    # Growth rate profile\n    gamma = (gamma_0 + gamma_1) / 2.0 + (gamma_0 - gamma_1) / 2.0 * np.tanh((r_c - r) / Delta)\n\n    # Initial condition\n    r_0 = r_c - delta\n    I = A * np.exp(-(r - r_0)**2 / (2 * sigma**2))\n\n    # --- 3. Time-stepping and Simulation Loop ---\n\n    # Stable time step based on CFL for diffusion\n    # dt <= dr^2 / (2*D). We use a safety factor of 0.45.\n    dt = 0.45 * (dr**2) / D\n    num_steps = int(T / dt)\n\n    d_max = 0.0\n    \n    # Indices corresponding to the subcritical region (r >= r_c)\n    subcritical_indices = np.where(r >= r_c)[0]\n\n    for _ in range(num_steps):\n        # Calculate Laplacian using second-order central difference\n        d2I_dr2 = np.zeros(N)\n        \n        # Interior points\n        d2I_dr2[1:-1] = (I[2:] - 2 * I[1:-1] + I[:-2]) / dr**2\n        \n        # Neumann boundary conditions: dI/dr = 0\n        # at r=0 (i=0): I_-1 = I_1\n        d2I_dr2[0] = 2 * (I[1] - I[0]) / dr**2\n        # at r=L (i=N-1): I_N = I_N-2\n        d2I_dr2[-1] = 2 * (I[-2] - I[-1]) / dr**2\n        \n        # Calculate reaction term\n        reaction_term = gamma * I * (1 - I / I_sat)\n        \n        # Update I using Forward Euler\n        I_new = I + dt * (reaction_term + D * d2I_dr2)\n        \n        # Enforce non-negativity\n        I = np.maximum(0, I_new)\n\n        # --- 4. Calculate Penetration Depth ---\n        if subcritical_indices.size > 0:\n            I_subcritical = I[subcritical_indices]\n            r_subcritical = r[subcritical_indices]\n            \n            # Find points above the threshold in the subcritical region\n            penetrating_mask = I_subcritical >= I_th\n            \n            if np.any(penetrating_mask):\n                # Find the maximum radial position of penetration in this time step\n                max_penetrating_r = np.max(r_subcritical[penetrating_mask])\n                current_d = max_penetrating_r - r_c\n                # Update the overall maximum penetration depth\n                if current_d > d_max:\n                    d_max = current_d\n                    \n    return d_max\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A core prediction of Self-Organized Criticality is that avalanche sizes often follow a scale-free, power-law distribution. This practice moves beyond simple curve-fitting to a rigorous statistical test of this hypothesis using Bayesian model comparison. By computing the Bayes factor, you will quantitatively assess whether a power-law model is more plausible than competing models, such as an exponential or lognormal distribution, providing a robust framework for making evidence-based claims about the underlying physics. ",
            "id": "4181755",
            "problem": "You are tasked with evaluating evidence for Self-Organized Criticality (SOC) in fusion plasma turbulence by comparing probabilistic models for avalanche event sizes and inter-event waiting times. Self-Organized Criticality (SOC) refers to systems that evolve to a critical state where distributions of event sizes and waiting times often exhibit scale-free behavior. The goal is to compute Bayes factors between competing models for avalanche sizes and waiting times and to aggregate these results across multiple synthetic datasets. Your program must be a complete, runnable program that produces the specified output.\n\nBegin from first principles:\n\n- Let $D = \\{x_i\\}_{i=1}^n$ denote a dataset of independent observations, and let $M$ denote a probabilistic model with parameters $\\theta$. Let the likelihood be $p(D \\mid \\theta, M) = \\prod_{i=1}^n p(x_i \\mid \\theta, M)$.\n- Let the prior be $p(\\theta \\mid M)$ and the marginal likelihood (model evidence) be $$Z_M = p(D \\mid M) = \\int p(D \\mid \\theta, M)\\, p(\\theta \\mid M)\\, d\\theta.$$\n- The Bayes factor comparing models $M_1$ and $M_2$ is $$B_{12} = \\frac{Z_{M_1}}{Z_{M_2}}.$$\n- Use independent and identically distributed (i.i.d.) assumptions for observations and proper priors to ensure finite evidences.\n\nModels to be compared:\n\nAvalanche sizes $s_i \\ge s_{\\min}$:\n\n- Model $S_{\\text{PL}}$ (power-law, Pareto): for $s \\ge s_{\\min}$, use Probability Density Function (PDF) $$p(s \\mid \\alpha, S_{\\text{PL}}) = (\\alpha - 1)\\, s_{\\min}^{\\alpha - 1}\\, s^{-\\alpha},$$ with exponent $\\alpha > 1$ and known lower cutoff $s_{\\min}$.\n- Model $S_{\\text{EXP}}$ (shifted exponential): for $s \\ge s_{\\min}$, define $x = s - s_{\\min} \\ge 0$, with PDF $$p(s \\mid \\lambda, S_{\\text{EXP}}) = \\lambda\\, e^{-\\lambda (s - s_{\\min})},$$ where $\\lambda > 0$ is the rate.\n- Model $S_{\\text{LN}}$ (lognormal): for $s > 0$, let $y = \\ln s$ and use $$p(s \\mid \\mu, \\sigma^2, S_{\\text{LN}}) = \\frac{1}{s\\, \\sigma\\, \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(\\ln s - \\mu)^2}{2 \\sigma^2}\\right),$$ with mean $\\mu \\in \\mathbb{R}$ and variance $\\sigma^2 > 0$ of the log-transformed variable.\n\nWaiting times $t_i \\ge t_{\\min}$ in seconds:\n\n- Model $T_{\\text{PL}}$ (power-law, Pareto): for $t \\ge t_{\\min}$, $$p(t \\mid \\alpha_t, T_{\\text{PL}}) = (\\alpha_t - 1)\\, t_{\\min}^{\\alpha_t - 1}\\, t^{-\\alpha_t},$$ with $\\alpha_t > 1$ and known $t_{\\min}$.\n- Model $T_{\\text{EXP}}$ (shifted exponential): for $t \\ge t_{\\min}$, let $x = t - t_{\\min} \\ge 0$, with $$p(t \\mid \\lambda_t, T_{\\text{EXP}}) = \\lambda_t\\, e^{-\\lambda_t (t - t_{\\min})},$$ and $\\lambda_t > 0$.\n\nPrior choices:\n\n- For Pareto exponents, reparameterize $\\theta = \\alpha - 1$ and use a Gamma prior $$\\theta \\sim \\operatorname{Gamma}(a_0, b_0).$$\n- For exponential rates $\\lambda$ and $\\lambda_t$, use a Gamma prior $$\\lambda \\sim \\operatorname{Gamma}(a_0, b_0).$$\n- For lognormal parameters on $y = \\ln s$, use a Normal-Inverse-Gamma prior $$\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0), \\quad \\sigma^2 \\sim \\operatorname{Inv}\\text{-}\\operatorname{Gamma}(\\alpha_0, \\beta_0).$$\n\nTask:\n\n- For each dataset, compute the marginal likelihood for each model using the priors above and the i.i.d. assumption, and then compute Bayes factors:\n    - Avalanche sizes: $B_{S_{\\text{PL}}, S_{\\text{EXP}}}$ and $B_{S_{\\text{PL}}, S_{\\text{LN}}}$.\n    - Waiting times: $B_{T_{\\text{PL}}, T_{\\text{EXP}}}$.\n- The Bayes factors are unitless. The waiting times are to be treated in seconds; all sizes are unitless normalized avalanche sizes.\n\nTest suite:\n\n- Use the following datasets and parameters.\n\nCase $1$ (expected SOC-like heavy tails):\n- Avalanche sizes $s$ with $s_{\\min} = 1.0$: $[\\, $1.0$, $1.2$, $1.1$, $2.0$, $3.5$, $1.0$, $5.0$, $8.0$, $1.5$, $10.0$, $2.2$, $4.4$ \\,]$.\n- Waiting times $t$ with $t_{\\min} = 1.0 \\times 10^{-4}$ seconds: $[\\, $1.0 \\times 10^{-4}$, $2.0 \\times 10^{-4}$, $1.5 \\times 10^{-4}$, $5.0 \\times 10^{-4}$, $3.0 \\times 10^{-3}$, $1.2 \\times 10^{-4}$, $8.0 \\times 10^{-4}$, $1.0 \\times 10^{-2}$, $2.0 \\times 10^{-4}$, $4.0 \\times 10^{-3}$ \\,]$.\n\nCase $2$ (expected near-exponential behavior):\n- Avalanche sizes $s$ with $s_{\\min} = 1.0$: $[\\, $1.0$, $1.1$, $1.05$, $1.2$, $1.15$, $1.05$, $1.3$, $1.1$, $1.05$, $1.2$ \\,]$.\n- Waiting times $t$ with $t_{\\min} = 1.0 \\times 10^{-4}$ seconds: $[\\, $1.0 \\times 10^{-4}$, $1.2 \\times 10^{-4}$, $1.1 \\times 10^{-4}$, $1.3 \\times 10^{-4}$, $1.05 \\times 10^{-4}$, $1.4 \\times 10^{-4}$, $1.2 \\times 10^{-4}$, $1.3 \\times 10^{-4}$, $1.1 \\times 10^{-4}$, $1.25 \\times 10^{-4}$ \\,]$.\n\nCase $3$ (edge case: small sample size with an outlier):\n- Avalanche sizes $s$ with $s_{\\min} = 1.0$: $[\\, $1.0$, $1.0$, $1.0$, $4.0$ \\,]$.\n- Waiting times $t$ with $t_{\\min} = 1.0 \\times 10^{-4}$ seconds: $[\\, $1.0 \\times 10^{-4}$, $1.0 \\times 10^{-4}$, $5.0 \\times 10^{-4}$ \\,]$.\n\nHyperparameters (shared across all cases):\n\n- Pareto Gamma prior on $\\theta = \\alpha - 1$: $a_0 = 1.5$, $b_0 = 1.0$.\n- Exponential rate prior: $a_0 = 1.0$, $b_0 = 1.0$.\n- Lognormal Normal-Inverse-Gamma prior on $y = \\ln s$: $\\mu_0 = 0.0$, $\\kappa_0 = 1.0 \\times 10^{-3}$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0$.\n\nRequired output:\n\n- For each case, compute and return the list $[\\, B_{S_{\\text{PL}}, S_{\\text{EXP}}},\\, B_{S_{\\text{PL}}, S_{\\text{LN}}},\\, B_{T_{\\text{PL}}, T_{\\text{EXP}}} \\,]$ as floating-point numbers.\n- Aggregate the results over the $3$ cases into a single comma-separated list enclosed in square brackets, in the order of cases $1$, $2$, $3$, yielding $9$ numbers total: $$[\\, B_{S_{\\text{PL}}, S_{\\text{EXP}}}^{(1)},\\, B_{S_{\\text{PL}}, S_{\\text{LN}}}^{(1)},\\, B_{T_{\\text{PL}}, T_{\\text{EXP}}}^{(1)},\\, B_{S_{\\text{PL}}, S_{\\text{EXP}}}^{(2)},\\, B_{S_{\\text{PL}}, S_{\\text{LN}}}^{(2)},\\, B_{T_{\\text{PL}}, T_{\\text{EXP}}}^{(2)},\\, B_{S_{\\text{PL}}, S_{\\text{EXP}}}^{(3)},\\, B_{S_{\\text{PL}}, S_{\\text{LN}}}^{(3)},\\, B_{T_{\\text{PL}}, T_{\\text{EXP}}}^{(3)} \\,].$$\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,\\dots]$). The Bayes factors are unitless floats; no other text should be printed.\n\nNote: The evaluation is purely mathematical and logical; numerical values are unitless except for waiting times used in seconds. Angle units do not apply. Percentages do not apply. Ensure numerical stability in your computation of marginal likelihoods.",
            "solution": "The problem requires the computation of Bayes factors to compare several probabilistic models for avalanche sizes and inter-event waiting times, which are indicators of Self-Organized Criticality (SOC) in fusion plasma turbulence. The core of the task is to calculate the marginal likelihood, or model evidence $Z_M$, for each model $M$. Given a dataset $D=\\{x_i\\}_{i=1}^n$ of independent and identically distributed (i.i.d.) observations, the marginal likelihood is defined as the integral of the likelihood over the prior distribution of the model parameters $\\theta$:\n$$Z_M = p(D \\mid M) = \\int p(D \\mid \\theta, M) p(\\theta \\mid M) d\\theta$$\nwhere the likelihood is $p(D \\mid \\theta, M) = \\prod_{i=1}^n p(x_i \\mid \\theta, M)$. The problem specifies conjugate priors for all models, which allows for the analytical calculation of these integrals. For numerical stability, all calculations will be performed in log-space. The Bayes factor $B_{12}$ comparing model $M_1$ and $M_2$ is then computed as $B_{12} = \\frac{Z_{M_1}}{Z_{M_2}} = \\exp(\\ln Z_{M_1} - \\ln Z_{M_2})$.\n\nWe will now derive the analytical expressions for the log-marginal likelihood for each of the specified models.\n\n**1. Power-Law (Pareto) Model Evidence**\n\nThe probability density function (PDF) for the Pareto model is given for data $x \\ge x_{\\min}$ as $p(x \\mid \\alpha) = (\\alpha - 1) x_{\\min}^{\\alpha - 1} x^{-\\alpha}$, with $\\alpha>1$. The problem specifies a reparameterization $\\theta = \\alpha - 1 > 0$. The PDF becomes $p(x \\mid \\theta) = \\theta x_{\\min}^{\\theta} x^{-(\\theta+1)}$.\n\nThe likelihood for the dataset $D=\\{x_i\\}_{i=1}^n$ is:\n$$p(D \\mid \\theta) = \\prod_{i=1}^n \\theta x_{\\min}^{\\theta} x_i^{-(\\theta+1)} = \\theta^n x_{\\min}^{n\\theta} \\left(\\prod_{i=1}^n x_i\\right)^{-(\\theta+1)} = \\theta^n \\left(\\prod_{i=1}^n x_i^{-1}\\right) e^{-\\theta \\sum_{i=1}^n \\ln(x_i/x_{\\min})}$$\nA $\\operatorname{Gamma}(a_0, b_0)$ prior is placed on $\\theta$:\n$$p(\\theta) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\theta^{a_0-1} e^{-b_0 \\theta}$$\nThe marginal likelihood $Z_{\\text{PL}}$ is the integral of the product of the likelihood and the prior over $\\theta$:\n$$Z_{\\text{PL}} = \\int_0^\\infty \\left( \\theta^n \\left(\\prod_{i=1}^n x_i^{-1}\\right) e^{-\\theta S} \\right) \\left( \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\theta^{a_0-1} e^{-b_0 \\theta} \\right) d\\theta$$\nwhere $S = \\sum_{i=1}^n \\ln(x_i / x_{\\min})$. Grouping terms yields:\n$$Z_{\\text{PL}} = \\left(\\prod_{i=1}^n x_i^{-1}\\right) \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\int_0^\\infty \\theta^{n+a_0-1} e^{-(S+b_0)\\theta} d\\theta$$\nThe integral is the unnormalized form of a $\\operatorname{Gamma}(n+a_0, S+b_0)$ distribution, which evaluates to $\\frac{\\Gamma(n+a_0)}{(S+b_0)^{n+a_0}}$.\nThus, the marginal likelihood is:\n$$Z_{\\text{PL}} = \\left(\\prod_{i=1}^n x_i^{-1}\\right) \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\frac{\\Gamma(n+a_0)}{(S+b_0)^{n+a_0}}$$\nIn log-space, this becomes:\n$$\\ln Z_{\\text{PL}} = -\\sum_{i=1}^n \\ln x_i + a_0 \\ln b_0 + \\ln\\Gamma(n+a_0) - \\ln\\Gamma(a_0) - (n+a_0)\\ln(S+b_0)$$\n\n**2. Shifted Exponential Model Evidence**\n\nThe PDF for the shifted exponential model is $p(s \\mid \\lambda) = \\lambda e^{-\\lambda(s-s_{\\min})}$ for $s \\ge s_{\\min}$, with $\\lambda>0$. Let $y_i = s_i - s_{\\min}$. The likelihood for the dataset $D=\\{s_i\\}_{i=1}^n$ is:\n$$p(D \\mid \\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda y_i} = \\lambda^n e^{-\\lambda \\sum_{i=1}^n y_i}$$\nA $\\operatorname{Gamma}(a_0, b_0)$ prior is placed on the rate $\\lambda$. The structure of the calculation is identical to the Pareto case, but with different data statistics and no pre-factor from the likelihood.\n$$Z_{\\text{EXP}} = \\int_0^\\infty (\\lambda^n e^{-\\lambda \\sum y_i}) \\left( \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\lambda^{a_0-1} e^{-b_0 \\lambda} \\right) d\\lambda$$\n$$Z_{\\text{EXP}} = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\int_0^\\infty \\lambda^{n+a_0-1} e^{-(\\sum y_i + b_0)\\lambda} d\\lambda$$\nThis evaluates to:\n$$Z_{\\text{EXP}} = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\frac{\\Gamma(n+a_0)}{(\\sum y_i + b_0)^{n+a_0}}$$\nIn log-space:\n$$\\ln Z_{\\text{EXP}} = a_0 \\ln b_0 + \\ln\\Gamma(n+a_0) - \\ln\\Gamma(a_0) - (n+a_0) \\ln\\left(\\sum_{i=1}^n (s_i - s_{\\min}) + b_0\\right)$$\n\n**3. Lognormal Model Evidence**\n\nThe PDF for the lognormal model is given for $s > 0$ as $p(s \\mid \\mu, \\sigma^2) = \\frac{1}{s\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln s - \\mu)^2}{2\\sigma^2}\\right)$. Let $y = \\ln s$. The transformed variable $y$ follows a Normal distribution $y \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The likelihood for the dataset $D_s=\\{s_i\\}_{i=1}^n$ is:\n$$p(D_s \\mid \\mu, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{s_i\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln s_i - \\mu)^2}{2\\sigma^2}\\right) = \\left(\\prod_{i=1}^n s_i^{-1}\\right) p(D_y \\mid \\mu, \\sigma^2)$$\nwhere $D_y = \\{\\ln s_i\\}_{i=1}^n$.\nThe prior for $(\\mu, \\sigma^2)$ is a Normal-Inverse-Gamma distribution: $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0)$ and $\\sigma^2 \\sim \\operatorname{Inv-Gamma}(\\alpha_0, \\beta_0)$. This is a conjugate prior for a Normal likelihood. The marginal likelihood for the log-transformed data, $p(D_y)$, has a standard analytical form.\n\nThe posterior hyperparameters for the Normal-Inverse-Gamma distribution after observing $D_y$ are:\n$$ \\kappa_n = \\kappa_0 + n $$\n$$ \\mu_n = \\frac{\\kappa_0 \\mu_0 + n \\bar{y}}{\\kappa_n} \\quad \\text{where} \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i $$\n$$ \\alpha_n = \\alpha_0 + \\frac{n}{2} $$\n$$ \\beta_n = \\beta_0 + \\frac{1}{2}\\sum_{i=1}^n (y_i - \\bar{y})^2 + \\frac{n \\kappa_0}{2(n+\\kappa_0)}(\\bar{y} - \\mu_0)^2 $$\nThe marginal likelihood $p(D_y)$ is then given by:\n$$p(D_y) = \\frac{1}{(2\\pi)^{n/2}} \\frac{\\Gamma(\\alpha_n)}{\\Gamma(\\alpha_0)} \\frac{\\beta_0^{\\alpha_0}}{\\beta_n^{\\alpha_n}} \\sqrt{\\frac{\\kappa_0}{\\kappa_n}}$$\nThe marginal likelihood for the original data $D_s$ is therefore:\n$$Z_{\\text{LN}} = p(D_s) = \\left(\\prod_{i=1}^n s_i^{-1}\\right) p(D_y)$$\nIn log-space:\n$$\\ln Z_{\\text{LN}} = -\\sum_{i=1}^n \\ln s_i - \\frac{n}{2}\\ln(2\\pi) + \\ln\\Gamma(\\alpha_n) - \\ln\\Gamma(\\alpha_0) + \\alpha_0\\ln\\beta_0 - \\alpha_n\\ln\\beta_n + \\frac{1}{2}(\\ln\\kappa_0 - \\ln\\kappa_n)$$\nThese derived log-marginal likelihoods will be implemented to compute the required Bayes factors for the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef log_marginal_likelihood_pareto(x, x_min, a0, b0):\n    \"\"\"\n    Computes the log marginal likelihood for a Pareto distribution\n    with a Gamma prior on theta = alpha - 1.\n    \"\"\"\n    n = len(x)\n    \n    # Ensure all data points are above or at the minimum threshold\n    if np.any(x < x_min):\n        raise ValueError(\"All data points must be >= x_min.\")\n\n    log_x = np.log(x)\n    sum_log_x = np.sum(log_x)\n    \n    # S = sum(log(x_i / x_min))\n    S = sum_log_x - n * np.log(x_min)\n\n    # Posterior parameters\n    an = n + a0\n    bn = S + b0\n    \n    # Log marginal likelihood calculation\n    # ln Z_PL = -sum(ln(x_i)) + a0*ln(b0) + gammaln(n+a0) - gammaln(a0) - (n+a0)*ln(S+b0)\n    log_Z = (\n        -sum_log_x\n        + a0 * np.log(b0)\n        + gammaln(an)\n        - gammaln(a0)\n        - an * np.log(bn)\n    )\n    return log_Z\n\ndef log_marginal_likelihood_exponential(x, x_min, a0, b0):\n    \"\"\"\n    Computes the log marginal likelihood for a shifted exponential distribution\n    with a Gamma prior on the rate lambda.\n    \"\"\"\n    n = len(x)\n    \n    # Ensure all data points are above or at the minimum threshold\n    if np.any(x < x_min):\n        raise ValueError(\"All data points must be >= x_min.\")\n\n    y = x - x_min\n    sum_y = np.sum(y)\n    \n    # Posterior parameters\n    an = n + a0\n    bn = sum_y + b0\n\n    # Log marginal likelihood calculation\n    # ln Z_EXP = a0*ln(b0) + gammaln(n+a0) - gammaln(a0) - (n+a0)*ln(sum(y_i)+b0)\n    log_Z = (\n        a0 * np.log(b0)\n        + gammaln(an)\n        - gammaln(a0)\n        - an * np.log(bn)\n    )\n    return log_Z\n\ndef log_marginal_likelihood_lognormal(s, mu0, kappa0, alpha0, beta0):\n    \"\"\"\n    Computes the log marginal likelihood for a lognormal distribution\n    with a Normal-Inverse-Gamma prior on (mu, sigma^2).\n    \"\"\"\n    n = len(s)\n    \n    # Ensure all data points are positive\n    if np.any(s <= 0):\n        raise ValueError(\"All data points for lognormal must be > 0.\")\n\n    y = np.log(s)\n    sum_log_s = np.sum(y)\n    \n    # Precompute statistics of log-transformed data\n    y_bar = np.mean(y)\n    sum_sq_diff = np.sum((y - y_bar)**2)\n    \n    # Posterior hyperparameters\n    kappa_n = kappa0 + n\n    alpha_n = alpha0 + n / 2.0\n    beta_n = (\n        beta0 \n        + 0.5 * sum_sq_diff \n        + (n * kappa0 / (2.0 * (kappa0 + n))) * (y_bar - mu0)**2\n    )\n\n    # Log marginal likelihood for the log-transformed data y\n    log_p_D_y = (\n        -n / 2.0 * np.log(2 * np.pi)\n        + gammaln(alpha_n)\n        - gammaln(alpha0)\n        + alpha0 * np.log(beta0)\n        - alpha_n * np.log(beta_n)\n        + 0.5 * (np.log(kappa0) - np.log(kappa_n))\n    )\n    \n    # Total log marginal likelihood including the jacobian term\n    # ln Z_LN = -sum(ln(s_i)) + ln(p(D_y))\n    log_Z = -sum_log_s + log_p_D_y\n    return log_Z\n\ndef solve():\n    # Define the datasets and parameters from the problem statement.\n    test_cases = [\n        {\n            \"sizes\": {\"data\": np.array([1.0, 1.2, 1.1, 2.0, 3.5, 1.0, 5.0, 8.0, 1.5, 10.0, 2.2, 4.4]), \"min\": 1.0},\n            \"times\": {\"data\": np.array([1.0e-4, 2.0e-4, 1.5e-4, 5.0e-4, 3.0e-3, 1.2e-4, 8.0e-4, 1.0e-2, 2.0e-4, 4.0e-3]), \"min\": 1.0e-4},\n        },\n        {\n            \"sizes\": {\"data\": np.array([1.0, 1.1, 1.05, 1.2, 1.15, 1.05, 1.3, 1.1, 1.05, 1.2]), \"min\": 1.0},\n            \"times\": {\"data\": np.array([1.0e-4, 1.2e-4, 1.1e-4, 1.3e-4, 1.05e-4, 1.4e-4, 1.2e-4, 1.3e-4, 1.1e-4, 1.25e-4]), \"min\": 1.0e-4},\n        },\n        {\n            \"sizes\": {\"data\": np.array([1.0, 1.0, 1.0, 4.0]), \"min\": 1.0},\n            \"times\": {\"data\": np.array([1.0e-4, 1.0e-4, 5.0e-4]), \"min\": 1.0e-4},\n        }\n    ]\n\n    # Shared hyperparameters\n    hparams_pareto = {\"a0\": 1.5, \"b0\": 1.0}\n    hparams_exp = {\"a0\": 1.0, \"b0\": 1.0}\n    hparams_ln = {\"mu0\": 0.0, \"kappa0\": 1.0e-3, \"alpha0\": 1.0, \"beta0\": 1.0}\n\n    results = []\n    \n    for case in test_cases:\n        s_data, s_min = case[\"sizes\"][\"data\"], case[\"sizes\"][\"min\"]\n        t_data, t_min = case[\"times\"][\"data\"], case[\"times\"][\"min\"]\n        \n        # Avalanche Sizes\n        log_z_s_pl = log_marginal_likelihood_pareto(s_data, s_min, **hparams_pareto)\n        log_z_s_exp = log_marginal_likelihood_exponential(s_data, s_min, **hparams_exp)\n        log_z_s_ln = log_marginal_likelihood_lognormal(s_data, **hparams_ln)\n        \n        # Bayes Factors for Sizes\n        B_s_pl_exp = np.exp(log_z_s_pl - log_z_s_exp)\n        B_s_pl_ln = np.exp(log_z_s_pl - log_z_s_ln)\n        \n        # Waiting Times\n        log_z_t_pl = log_marginal_likelihood_pareto(t_data, t_min, **hparams_pareto)\n        log_z_t_exp = log_marginal_likelihood_exponential(t_data, t_min, **hparams_exp)\n        \n        # Bayes Factor for Times\n        B_t_pl_exp = np.exp(log_z_t_pl - log_z_t_exp)\n        \n        results.extend([B_s_pl_exp, B_s_pl_ln, B_t_pl_exp])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}