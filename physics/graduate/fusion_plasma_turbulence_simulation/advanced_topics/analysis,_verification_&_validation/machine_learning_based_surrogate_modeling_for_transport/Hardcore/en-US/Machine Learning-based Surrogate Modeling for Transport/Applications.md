## Applications and Interdisciplinary Connections

Having established the fundamental principles and mechanisms of machine learning-based [surrogate modeling](@entry_id:145866) for transport, this chapter explores the practical application and interdisciplinary reach of these powerful techniques. The focus now shifts from the theoretical underpinnings of surrogate construction to their integration into complex [scientific workflows](@entry_id:1131303), their role in data-driven discovery, and their extensibility to problems beyond [fusion plasma simulation](@entry_id:1125410). The objective is not to reiterate core concepts but to demonstrate their utility, showcasing how surrogates act as catalysts for accelerating research, enhancing physical fidelity, and enabling new modes of scientific inquiry. We will examine how these models are coupled with sophisticated numerical solvers, used in uncertainty quantification and inverse problems, and evaluated with scientific rigor.

### Integration into Transport Solvers

The primary motivation for developing transport surrogates is the potential for dramatic reductions in computational cost. However, realizing these gains requires careful integration into existing transport solvers and a holistic analysis of the entire simulation workflow.

A comprehensive assessment of performance gains extends beyond simply comparing the inference time of a surrogate to the evaluation time of a first-principles model. The total wall-clock runtime of a simulation workflow can be conceptualized through a generalized form of Amdahl's Law, comprising both sequential overheads ($T_{\text{seq}}$) and parallelizable computational work ($C$). For a system with $n$ parallel workers and a [parallel efficiency](@entry_id:637464) $e$, the total time is $T = T_{\text{seq}} + C/(ne)$. When a traditional quasilinear model with evaluation time $t_{\text{ql}}$ is replaced by a surrogate with a much smaller inference time $t_{\text{sg}}$, the parallelizable cost $C$ decreases significantly. However, this benefit must be weighed against new potential overheads. These can include a one-time, offline training cost ($T_{\text{train}}$), per-simulation overheads for model loading and warm-up ($t_{\text{wu}}$), and the computational cost associated with quality control mechanisms. For instance, a robust workflow may require a "fallback" to the more expensive quasilinear model for a fraction of evaluations where the surrogate's input features are out of its training distribution, or periodic "calibration" runs to maintain fidelity. A true end-to-end runtime analysis must account for all these factors to determine if the surrogate provides a net performance gain, especially in large-scale, multi-shot simulation campaigns. 

The practical utility of a surrogate hinges on its seamless integration with the numerical engine of a transport code. Many modern transport solvers rely on implicit time-stepping schemes to overcome the stiffness of the underlying partial differential equations (PDEs), which is often severe in [transport phenomena](@entry_id:147655). These schemes, such as the backward Euler method, require the solution of a large nonlinear system of equations at each time step, typically via a Newton-Raphson method or its variants. A critical component of Newton's method is the Jacobian matrix, which contains the [partial derivatives](@entry_id:146280) of the system's residual with respect to the [state variables](@entry_id:138790). When a transport coefficient like diffusivity $\chi$ is replaced by a neural network surrogate, its complex, nonlinear functional form must be differentiated to construct the Jacobian. Automatic Differentiation (AD) is the essential tool for this task. By applying the chain rule algorithmically through the [computational graph](@entry_id:166548) of the surrogate—including any input and output [normalization layers](@entry_id:636850)—AD can provide the exact analytical derivatives of the surrogate's output with respect to its inputs. These derivatives are then propagated, again via the chain rule, to the cell-centered state variables (e.g., temperature) to form the Jacobian entries needed by the implicit solver. This capability is paramount for ensuring the [numerical stability](@entry_id:146550) and rapid convergence of the coupled solver. 

For very large-scale problems, forming, storing, and inverting the full Jacobian matrix can become prohibitively expensive. In such cases, Jacobian-free Newton-Krylov (JFNK) methods offer a powerful alternative. These iterative methods only require the action of the Jacobian on a vector (a Jacobian-[vector product](@entry_id:156672), or JVP), which can be approximated by [finite differences](@entry_id:167874) or computed exactly using AD. Given a transport residual $\mathbf{R}(\mathbf{u})$ that depends on a surrogate flux model $\boldsymbol{\Gamma}(\mathbf{u}) = \boldsymbol{\Phi}(\mathbf{G}\mathbf{u})$, where $\mathbf{u}$ is the state vector, $\mathbf{G}$ is a linear operator computing features, and $\boldsymbol{\Phi}$ is the surrogate, the [chain rule](@entry_id:147422) allows for an analytical expression of the JVP. Forward-mode AD is particularly efficient for computing this JVP, providing an exact [directional derivative](@entry_id:143430) of the residual without ever forming the Jacobian matrix itself. This enables the coupling of complex surrogate models with highly scalable, matrix-free [implicit solvers](@entry_id:140315), unlocking performance gains that would be otherwise unattainable. 

### Enhancing Physical Fidelity and Predictive Accuracy

While computational speed is a primary driver, the ultimate goal of a surrogate is to be not just fast, but also accurate and physically realistic. This pursuit has led to the development of advanced training methodologies that go far beyond simple regression.

One powerful strategy is [residual learning](@entry_id:634200). Instead of training a surrogate to predict a transport flux from scratch, it can be trained to predict the *correction* or *residual* between a fast but approximate baseline physics model (like the quasilinear model TGLF) and a high-fidelity, nonlinear ground truth (from a [gyrokinetic simulation](@entry_id:181190)). This reframes the learning task from mastering the full complexity of transport physics to learning the comparatively simpler function of the baseline model's error. When designing such a framework, several best practices are critical. The loss function should account for heteroscedasticity in the training data—for instance, by weighting the squared error of each data point by the inverse of its variance, giving more importance to more certain ground-truth calculations. Furthermore, to learn state-dependent biases of the baseline model, its own predictions can be included as input features to the surrogate network. Finally, physical consistency can be encouraged by adding a regularization term to the loss function that penalizes unphysical behavior, such as a lack of [monotonicity](@entry_id:143760) between a driving gradient and the resulting total flux. 

A frontier in physics-informed machine learning is to directly incorporate the governing PDE into the surrogate optimization process. This can be framed as a [bilevel optimization](@entry_id:637138) problem. At the lower level, a standard [empirical risk](@entry_id:633993) (e.g., mean squared error on a dataset) is minimized to find the best possible surrogate parameters $\theta^{\star}$ and the corresponding minimum loss $L^{\star}$. At the upper level, the objective is to find a set of surrogate parameters $\theta$ that minimizes a physics-based error metric—for example, the squared difference between the resulting [steady-state temperature](@entry_id:136775) profile $T_{\theta}(r)$ and a trusted reference profile $T_{\text{ref}}(r)$. This upper-level optimization is constrained in two ways: first, by the PDE itself, which dictates the mapping from $\theta$ to $T_{\theta}(r)$, and second, by a data fidelity constraint, which requires that the [empirical risk](@entry_id:633993) of the chosen $\theta$ does not exceed the optimal risk $L^{\star}$ by more than a specified tolerance, i.e., $L(\theta) \le L^{\star} + \varepsilon$. This sophisticated framework seeks a surrogate that is not only accurate on a static dataset but also performs well when coupled with the transport solver, producing physically meaningful global solutions.  These advanced methods are all built upon the goal of accurately solving the underlying transport equations, which, even in their simplest steady-state, one-dimensional form, describe the fundamental balance between transport and sources that shapes plasma profiles. 

### Uncertainty Quantification and Data-Driven Discovery

Surrogate models are not only predictive tools but also powerful components in frameworks for uncertainty quantification (UQ) and automated scientific discovery. Their [computational efficiency](@entry_id:270255) makes it feasible to perform large ensembles of simulations that would be impossible with first-principles models.

A fundamental task in UQ is forward propagation: assessing how uncertainty in model inputs affects the outputs. If a surrogate's parameters $\boldsymbol{\theta}$ are uncertain, described by a probability distribution (e.g., a Gaussian with covariance matrix $\boldsymbol{\Sigma}$), this uncertainty can be propagated through the transport solver. Using first-order linearization, the variance of a predicted quantity, such as the temperature $T(r,t)$ at a future time, can be approximated. The variance of the output is given by $\boldsymbol{J} \boldsymbol{\Sigma} \boldsymbol{J}^{\top}$, where $\boldsymbol{J}$ is the Jacobian (sensitivity) of the output with respect to the surrogate parameters $\boldsymbol{\theta}$. This allows for a direct estimate of the confidence intervals on transport predictions due to surrogate [model uncertainty](@entry_id:265539). 

Surrogates are also indispensable for tackling inverse problems, where the goal is to infer model parameters from experimental data. Within a Bayesian framework, one seeks to find the [posterior probability](@entry_id:153467) distribution of model parameters given measurements. This involves combining a [likelihood function](@entry_id:141927), which quantifies the misfit between model predictions and experimental data (e.g., temperature measurements), with a prior distribution on the parameters. The computational cost of the forward model is often the bottleneck in such inference, as it must be evaluated many times by sampling algorithms like Markov Chain Monte Carlo. By replacing the expensive physics model with a fast surrogate, these inverse problems become tractable. This allows for the simultaneous inference of parameters for both the surrogate itself and other physics models (e.g., neoclassical transport), all constrained by experimental observations. Smoothness regularizers can be added to the objective function to ensure the inferred transport coefficients are physically plausible. 

The efficiency of surrogates also enables [active learning](@entry_id:157812), a strategy for optimizing the data generation process itself. Instead of generating a large, static training dataset, [active learning](@entry_id:157812) iteratively selects the most informative new high-fidelity simulations to run. In a goal-oriented framework, the "most informative" point is one that maximally reduces the uncertainty in a specific quantity of interest (QoI). This can be achieved by coupling the surrogate with [adjoint-based sensitivity analysis](@entry_id:746292). The adjoint method efficiently computes the sensitivity of the QoI to changes in the transport coefficients. A myopic [acquisition function](@entry_id:168889) can then be constructed to select the next simulation point that maximizes the expected reduction in the QoI's error, normalized by the simulation cost. This function typically prioritizes regions in the parameter space where both the surrogate's uncertainty (e.g., the predictive variance from a Gaussian Process surrogate) and the QoI's sensitivity are high. 

Finally, surrogate-driven transport models are key components in data assimilation frameworks for real-time forecasting and control. For instance, an Extended Kalman Filter (EKF) can be used to predict the evolution of a temperature profile. The surrogate model serves as the nonlinear forecast model, $x_{k+1} = f(x_k) + w_k$, where $f$ is the transport solver step and $w_k$ is [process noise](@entry_id:270644) representing the surrogate's uncertainty. Between experimental measurements, the EKF propagates the state estimate and its covariance forward in time. When a new measurement arrives, the Kalman update step corrects the forecast based on the discrepancy between the prediction and the data. This provides a continuously updated, high-fidelity estimate of the plasma state, which is essential for [feedback control systems](@entry_id:274717). 

### Interdisciplinary Connections and Broader Context

The principles of surrogate modeling for transport are not unique to [fusion plasma physics](@entry_id:749660). They are part of a broader toolkit in computational science and engineering, with direct applications in numerous other fields where systems are governed by complex transport phenomena described by PDEs.

A prominent example is the modeling of [lithium-ion batteries](@entry_id:150991). The performance of a battery is governed by the [coupled transport](@entry_id:144035) of ions and charge within porous electrodes. High-fidelity models, such as the Pseudo-Two-Dimensional (P2D) model, resolve these processes by solving a system of coupled PDEs for ion concentration and electrostatic potential in both the solid electrode material and the liquid electrolyte. The P2D model is computationally expensive, analogous to gyrokinetic models in fusion. Simplified versions, like the Single Particle Model with electrolyte (SPMe), reduce complexity by assuming uniform reaction currents, making them faster but less accurate, analogous to quasilinear models. This hierarchy of models provides a rich environment for surrogate development.  Just as in plasma physics, the goal is often to create a fast and accurate surrogate for tasks like drive-cycle simulation or [battery management system](@entry_id:1121417) design. The solution fields from battery simulations—containing concentrations and potentials across space and time—are high-dimensional and often exhibit sharp gradients and moving [reaction fronts](@entry_id:198197). This makes them ideal candidates for advanced [dimensionality reduction](@entry_id:142982) techniques. While linear methods like Principal Component Analysis (PCA) can provide a baseline, nonlinear methods like autoencoders are far more powerful. In particular, convolutional autoencoders are exceptionally well-suited to learn compact representations of data featuring translating structures, such as the moving lithiation fronts inside [battery electrodes](@entry_id:1121399), a task for which PCA is notoriously inefficient. 

Another critical aspect in the practical application of surrogates is [transfer learning](@entry_id:178540)—the ability to apply a model trained in one context to another. In fusion, this often means transferring a surrogate trained on simulations or data from one tokamak device (e.g., device $A$) to another (device $B$). This is a significant challenge due to "domain shift": the devices may operate in different physical regimes and, crucially, possess different diagnostic systems. These diagnostic differences can manifest as systematic calibration errors or different spatial resolutions. A robust transfer learning strategy must account for this. One approach involves an explicit [domain adaptation](@entry_id:637871) step, where a calibration operator is learned to map features from one device's diagnostic space to the other. This can be combined with [representation learning](@entry_id:634436) techniques, like minimizing the Maximum Mean Discrepancy (MMD) between the feature distributions of the two domains, to create a shared, device-invariant feature space. A failure to address [domain shift](@entry_id:637840) can lead to poor performance. For example, if device $B$ has lower-resolution diagnostics, it will measure smaller gradients. A surrogate trained on device $A$ data, when fed these artificially low gradients, may predict unphysically low transport. This failure mode might be missed by simple error metrics in feature space but is readily revealed by a physics-based validation metric, such as checking for the conservation of energy, which would show a large residual. 

### Scientific Practice: Benchmarking and Reproducibility

As surrogate models become increasingly integrated into scientific research, establishing rigorous standards for their validation and reporting is paramount. The credibility of scientific conclusions drawn from surrogate-accelerated simulations depends on the community's ability to trust the models and reproduce the results.

A scientifically sound benchmarking suite for a new transport surrogate must be both fair and comprehensive. Fairness is achieved by ensuring that all models—the surrogate and the baseline physics models (e.g., TGLF, QuaLiKiz)—are compared under identical conditions. This means using the same transport solver, numerical grid, boundary conditions, heating sources, and [neoclassical transport](@entry_id:188243) models. The only component that should differ is the turbulent transport closure being tested. Comprehensiveness requires evaluation from multiple perspectives: (1) local predictive accuracy on a held-out test dataset, measured by metrics like Normalized Root Mean Square Error; (2) global profile reproduction, assessed by integrating the model into the transport solver and comparing the resulting steady-state profiles; (3) physics fidelity, tested by specific challenges like gradient scans to probe transport stiffness; and (4) computational performance, measured by the speedup in wall-clock time within an identical hardware and software environment. 

Finally, the cornerstone of computational science is reproducibility. Given the stochastic nature of modern machine learning training pipelines and their sensitivity to the software environment, achieving reproducibility requires meticulous documentation and packaging. A reproducible study must provide more than just the final trained model. A minimal, sufficient package includes: the full dataset with complete provenance linking it back to the source simulations; the exact [data preprocessing](@entry_id:197920) and splitting procedures; the version-controlled source code for training and evaluation; a precise, machine-executable specification of the computational environment (e.g., a container file with pinned library versions); and the set of all random seeds used for initialization and data shuffling. Providing these components ensures that an independent research team can deterministically regenerate the trained model, the reported metrics, and all figures and tables, thereby verifying the study's findings to within numerical precision. This level of rigor is essential for building a cumulative and reliable body of knowledge based on [machine learning surrogates](@entry_id:1127558). 