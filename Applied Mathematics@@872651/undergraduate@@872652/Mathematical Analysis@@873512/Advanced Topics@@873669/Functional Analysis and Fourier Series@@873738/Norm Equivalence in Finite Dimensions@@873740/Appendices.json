{"hands_on_practices": [{"introduction": "Our exploration begins in the familiar setting of the two-dimensional plane, $\\mathbb{R}^2$. While the 'as-the-crow-flies' Euclidean distance is common, other measures of length, like the 'taxicab' or Manhattan distance, are equally valid. This first exercise provides a concrete calculation of the equivalence constants between these two fundamental norms, building essential skills for finding the tightest possible bounds in such comparisons. [@problem_id:1859194]", "problem": "In the vector space $\\mathbb{R}^2$, consider two different ways to measure the \"length\" of a vector $x = (x_1, x_2)$. The first is the taxicab norm, or $L_1$-norm, defined as $\\|x\\|_1 = |x_1| + |x_2|$. The second is the standard Euclidean norm, or $L_2$-norm, defined as $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$.\n\nA fundamental theorem in functional analysis states that on any finite-dimensional vector space, all norms are equivalent. For the norms $\\|x\\|_1$ and $\\|x\\|_2$ on $\\mathbb{R}^2$, this means there exist positive constants $C_1$ and $C_2$ such that the following double inequality holds for all vectors $x \\in \\mathbb{R}^2$:\n$$C_1 \\|x\\|_2 \\le \\|x\\|_1 \\le C_2 \\|x\\|_2$$\nYour task is to find the *optimal* pair of constants $(C_1, C_2)$. This means you must find the largest possible value for $C_1$ and the smallest possible value for $C_2$ for which the inequalities are true for every vector in $\\mathbb{R}^2$.\n\nWhich of the following pairs represents the optimal constants $(C_1, C_2)$?\n\nA. $(1, \\sqrt{2})$\n\nB. $(\\sqrt{2}, 2)$\n\nC. $(1/2, 1)$\n\nD. $(1, 2)$\n\nE. $(1/\\sqrt{2}, \\sqrt{2})$", "solution": "We seek the largest $C_{1}$ and smallest $C_{2}$ such that $C_{1}\\|x\\|_{2} \\le \\|x\\|_{1} \\le C_{2}\\|x\\|_{2}$ holds for all $x \\in \\mathbb{R}^{2}$. For any nonzero $x$, define the homogeneous ratio\n$$\nr(x) = \\frac{\\|x\\|_{1}}{\\|x\\|_{2}} = \\frac{|x_{1}| + |x_{2}|}{\\sqrt{x_{1}^{2} + x_{2}^{2}}}.\n$$\nSince $r(\\alpha x) = r(x)$ for all $\\alpha \\neq 0$, it suffices to analyze $r$ on the unit circle $S = \\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} = 1\\}$. On $S$, we have\n$$\nr(x) = |x_{1}| + |x_{2}|.\n$$\nUpper bound: By the Cauchyâ€“Schwarz inequality,\n$$\n|x_{1}| + |x_{2}| \\le \\sqrt{1^{2} + 1^{2}}\\,\\sqrt{x_{1}^{2} + x_{2}^{2}} = \\sqrt{2}.\n$$\nEquality holds when $|x_{1}| = |x_{2}|$ and $x$ is aligned with $(1,1)$ with nonnegative coordinates, specifically $x_{1} = x_{2} = \\frac{1}{\\sqrt{2}}$ on $S$. Hence\n$$\n\\sup_{x \\neq 0} \\frac{\\|x\\|_{1}}{\\|x\\|_{2}} = \\sqrt{2},\n$$\nso the optimal $C_{2} = \\sqrt{2}$.\n\nLower bound: For any $x$,\n$$\n(|x_{1}| + |x_{2}|)^{2} = x_{1}^{2} + x_{2}^{2} + 2|x_{1}x_{2}| \\ge x_{1}^{2} + x_{2}^{2}.\n$$\nOn $S$ this gives $|x_{1}| + |x_{2}| \\ge 1$. Equality holds when $|x_{1}x_{2}| = 0$, i.e., one coordinate is zero. On $S$ this is achieved at $(\\pm 1, 0)$ or $(0, \\pm 1)$. Therefore\n$$\n\\inf_{x \\neq 0} \\frac{\\|x\\|_{1}}{\\|x\\|_{2}} = 1,\n$$\nso the optimal $C_{1} = 1$.\n\nThus the optimal pair is $(C_{1}, C_{2}) = (1, \\sqrt{2})$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1859194"}, {"introduction": "Having grounded our understanding in $\\mathbb{R}^2$, we now generalize the concept to the vector space of $2 \\times 2$ matrices. This problem demonstrates that the principle of norm equivalence is not limited to column vectors, by comparing the Frobenius norm (analogous to the Euclidean norm) with the maximum entry norm. This practice reinforces the key idea that any finite-dimensional space, regardless of its representation, adheres to this powerful theorem. [@problem_id:2308369]", "problem": "Consider the vector space $V = M_2(\\mathbb{R})$ of all $2 \\times 2$ matrices with real entries. For any matrix $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ in $V$, we define two different ways to measure its size, known as norms:\n\n1.  The **maximum absolute entry norm**, denoted by $\\|A\\|_{\\max}$, is given by $\\|A\\|_{\\max} = \\max\\{|a|, |b|, |c|, |d|\\}$.\n2.  The **Frobenius norm**, denoted by $\\|A\\|_{F}$, is given by $\\|A\\|_{F} = \\sqrt{a^2 + b^2 + c^2 + d^2}$.\n\nIt is a known result that for this space, there exist positive constants $C_1$ and $C_2$ such that for every non-zero matrix $A \\in V$, the following inequality holds:\n$$\nC_1 \\|A\\|_{\\max} \\leq \\|A\\|_{F} \\leq C_2 \\|A\\|_{\\max}\n$$\nDetermine the values of the constants $C_1$ and $C_2$ that make this inequality as tight as possible. Specifically, find the largest possible value for $C_1$ and the smallest possible value for $C_2$.\n\nPresent your answer for the constants $C_1$ and $C_2$, in that order.", "solution": "Identify $V$ with $\\mathbb{R}^{4}$ via the entries of $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$. Let $m = \\|A\\|_{\\max} = \\max\\{|a|,|b|,|c|,|d|\\}$. Then, by definition of the Frobenius norm,\n$$\n\\|A\\|_{F} = \\sqrt{a^{2} + b^{2} + c^{2} + d^{2}}.\n$$\n\nFor the lower bound, note that at least one of $|a|,|b|,|c|,|d|$ equals $m$, hence $a^{2} + b^{2} + c^{2} + d^{2} \\geq m^{2}$. Therefore,\n$$\n\\|A\\|_{F} \\geq m = \\|A\\|_{\\max}.\n$$\nThis shows the inequality holds with $C_{1} = 1$. To see this is optimal, take $A$ with exactly one nonzero entry, say $A = \\begin{pmatrix} t  0 \\\\ 0  0 \\end{pmatrix}$ with $t \\neq 0$. Then $\\|A\\|_{F} = |t| = \\|A\\|_{\\max}$, so any $C_{1}  1$ would violate $C_{1}\\|A\\|_{\\max} \\leq \\|A\\|_{F}$.\n\nFor the upper bound, since each of $|a|,|b|,|c|,|d|$ is at most $m$, we have\n$$\na^{2} + b^{2} + c^{2} + d^{2} \\leq 4 m^{2},\n$$\nhence\n$$\n\\|A\\|_{F} \\leq 2 m = 2 \\|A\\|_{\\max}.\n$$\nThus the inequality holds with $C_{2} = 2$. To see minimality, take all entries equal to a common nonzero value $t$, i.e., $A = \\begin{pmatrix} t  t \\\\ t  t \\end{pmatrix}$ with $t \\neq 0$. Then $\\|A\\|_{\\max} = |t|$ and $\\|A\\|_{F} = \\sqrt{4 t^{2}} = 2|t|$, so any $C_{2}  2$ would fail.\n\nTherefore, the largest possible $C_{1}$ is $1$, and the smallest possible $C_{2}$ is $2$.", "answer": "$$\\boxed{\\begin{pmatrix} 1  2 \\end{pmatrix}}$$", "id": "2308369"}, {"introduction": "This final practice moves beyond standard norms to explore how new ones can be systematically constructed. We investigate a norm defined by a positive definite matrix $P$, which essentially creates a custom inner product and a new way to measure vector lengths. The exercise reveals a deep connection between the equivalence constants and the eigenvalues of $P$, linking the abstract concept of norm equivalence to the geometric idea of stretching and shrinking space. [@problem_id:2308400]", "problem": "In the vector space $\\mathbb{R}^2$, the familiar Euclidean norm of a vector $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ is given by $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$. However, other norms can be defined. Consider the matrix $P$ given by\n$$\nP = \\begin{pmatrix} 5  -2 \\\\ -2  2 \\end{pmatrix}.\n$$\nThis matrix can be used to define a new norm, the $P$-norm, for any vector $x \\in \\mathbb{R}^2$ as $\\|x\\|_P = \\sqrt{x^T P x}$, where $x^T$ is the transpose of $x$.\n\nIn a finite-dimensional vector space, any two norms are equivalent. Two norms, $\\|\\cdot\\|_a$ and $\\|\\cdot\\|_b$, are said to be equivalent if there exist positive real constants $c_1$ and $c_2$ such that the inequality $c_1 \\|x\\|_a \\le \\|x\\|_b \\le c_2 \\|x\\|_a$ holds for all vectors $x$ in the space.\n\nFind the best possible positive constants $c_1$ and $c_2$ that satisfy the equivalence relation $c_1 \\|x\\|_2 \\le \\|x\\|_P \\le c_2 \\|x\\|_2$ for all $x \\in \\mathbb{R}^2$. The \"best possible\" constants are those that make the bounds as tight as possible. Present your final answer as the pair of values $(c_1, c_2)$, with each value rounded to three significant figures.", "solution": "We seek the sharp constants $c_{1},c_{2}  0$ such that for all $x \\in \\mathbb{R}^{2}$,\n$$\nc_{1}\\|x\\|_{2} \\le \\|x\\|_{P} \\le c_{2}\\|x\\|_{2},\n$$\nwhere $\\|x\\|_{P} = \\sqrt{x^{T}Px}$ and $\\|x\\|_{2} = \\sqrt{x^{T}x}$. Squaring the inequality and using homogeneity, this is equivalent to finding the smallest and largest possible values of the Rayleigh quotient\n$$\nR(x) = \\frac{x^{T}Px}{x^{T}x}\n$$\nover all nonzero $x$. For a real symmetric matrix $P$, the Rayleigh quotient satisfies\n$$\n\\lambda_{\\min}(P) \\le R(x) \\le \\lambda_{\\max}(P)\n$$\nfor all nonzero $x$, with equality attained at eigenvectors corresponding to the extreme eigenvalues. Therefore, the best constants are\n$$\nc_{1} = \\sqrt{\\lambda_{\\min}(P)}, \\quad c_{2} = \\sqrt{\\lambda_{\\max}(P)}.\n$$\n\nWe compute the eigenvalues of $P = \\begin{pmatrix} 5  -2 \\\\ -2  2 \\end{pmatrix}$ from its characteristic polynomial:\n$$\n\\det(P - \\lambda I) = (5 - \\lambda)(2 - \\lambda) - (-2)(-2) = \\lambda^{2} - 7\\lambda + 6.\n$$\nSolving $\\lambda^{2} - 7\\lambda + 6 = 0$ yields\n$$\n\\lambda = \\frac{7 \\pm \\sqrt{49 - 24}}{2} = \\frac{7 \\pm 5}{2},\n$$\nso $\\lambda_{\\min}(P) = 1$ and $\\lambda_{\\max}(P) = 6$. Hence\n$$\nc_{1} = \\sqrt{1} = 1, \\quad c_{2} = \\sqrt{6}.\n$$\nRounding each to three significant figures gives $c_{1} = 1.00$ and $c_{2} = 2.45$.", "answer": "$$\\boxed{\\begin{pmatrix} 1.00  2.45 \\end{pmatrix}}$$", "id": "2308400"}]}