{"hands_on_practices": [{"introduction": "Projecting a vector onto a subspace is a fundamental operation in many fields, from computer graphics to signal processing. While any basis can define a subspace, calculations become significantly simpler with an orthonormal basis. This exercise [@problem_id:1381373] provides hands-on practice with the complete workflow: starting with a standard basis, applying the Gram-Schmidt process to build an orthogonal one, and then using it to find the closest vector in a subspace to a given point.", "problem": "In a simplified signal processing model, a two-dimensional 'signal' subspace $S$ within a three-dimensional ambient space $\\mathbb{R}^3$ is defined by the span of two generating vectors, $\\mathbf{g}_1 = (1, 1, 0)$ and $\\mathbf{g}_2 = (1, 0, 1)$. A raw measurement is received, represented by the vector $\\mathbf{m} = (1, 2, 3)$.\n\nThe signal component of $\\mathbf{m}$ is defined as the vector in the subspace $S$ that is closest to $\\mathbf{m}$. Let this signal component vector be denoted by $\\mathbf{s} = (s_1, s_2, s_3)$. Determine the value of the second component, $s_2$.\n\nExpress your answer as a number rounded to three significant figures.", "solution": "The problem asks for the second component of the vector $\\mathbf{s}$ in the subspace $S = \\text{span}\\{\\mathbf{g}_1, \\mathbf{g}_2\\}$ that is closest to the vector $\\mathbf{m}$. This closest vector is the orthogonal projection of $\\mathbf{m}$ onto the subspace $S$. The vectors defining the subspace are $\\mathbf{g}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\mathbf{g}_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. The measurement vector is $\\mathbf{m} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$.\n\nTo calculate the orthogonal projection, we first need an orthogonal basis for the subspace $S$. We can obtain one by applying the Gram-Schmidt process to the basis $\\{\\mathbf{g}_1, \\mathbf{g}_2\\}$.\n\nFirst, we check if the given basis is already orthogonal:\n$\\mathbf{g}_1 \\cdot \\mathbf{g}_2 = (1)(1) + (1)(0) + (0)(1) = 1$.\nSince the dot product is not zero, the vectors are not orthogonal, and we must proceed with the Gram-Schmidt process.\n\nLet the new orthogonal basis be $\\{\\mathbf{v}_1, \\mathbf{v}_2\\}$.\nStep 1: Set the first orthogonal vector $\\mathbf{v}_1$ to be $\\mathbf{g}_1$.\n$$\n\\mathbf{v}_1 = \\mathbf{g}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\nStep 2: Construct the second orthogonal vector $\\mathbf{v}_2$ by subtracting the projection of $\\mathbf{g}_2$ onto $\\mathbf{v}_1$ from $\\mathbf{g}_2$.\n$$\n\\mathbf{v}_2 = \\mathbf{g}_2 - \\text{proj}_{\\mathbf{v}_1}(\\mathbf{g}_2) = \\mathbf{g}_2 - \\frac{\\mathbf{g}_2 \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\mathbf{v}_1\n$$\nWe need to compute the dot products:\n$\\mathbf{g}_2 \\cdot \\mathbf{v}_1 = (1)(1) + (0)(1) + (1)(0) = 1$.\n$\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = (1)(1) + (1)(1) + (0)(0) = 2$.\nNow, substitute these values back into the expression for $\\mathbf{v}_2$:\n$$\n\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{2} \\\\ 0 - \\frac{1}{2} \\\\ 1 - 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 1 \\end{pmatrix}\n$$\nSo, an orthogonal basis for $S$ is $\\left\\{\\mathbf{v}_1, \\mathbf{v}_2\\right\\} = \\left\\{\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 1 \\end{pmatrix}\\right\\}$.\n\nNow we can calculate the orthogonal projection of $\\mathbf{m}$ onto $S$, which we denote by $\\mathbf{s}$. The formula for the projection onto a subspace spanned by an orthogonal basis is:\n$$\n\\mathbf{s} = \\text{proj}_S(\\mathbf{m}) = \\frac{\\mathbf{m} \\cdot \\mathbf{v}_1}{\\mathbf{v}_1 \\cdot \\mathbf{v}_1}\\mathbf{v}_1 + \\frac{\\mathbf{m} \\cdot \\mathbf{v}_2}{\\mathbf{v}_2 \\cdot \\mathbf{v}_2}\\mathbf{v}_2\n$$\nWe already have $\\mathbf{v}_1 \\cdot \\mathbf{v}_1 = 2$. Let's compute the other required dot products:\n$\\mathbf{m} \\cdot \\mathbf{v}_1 = (1)(1) + (2)(1) + (3)(0) = 1 + 2 = 3$.\n$\\mathbf{m} \\cdot \\mathbf{v}_2 = (1)(\\frac{1}{2}) + (2)(-\\frac{1}{2}) + (3)(1) = \\frac{1}{2} - 1 + 3 = \\frac{5}{2}$.\n$\\mathbf{v}_2 \\cdot \\mathbf{v}_2 = (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (1)^2 = \\frac{1}{4} + \\frac{1}{4} + 1 = \\frac{1}{2} + 1 = \\frac{3}{2}$.\n\nSubstitute these values into the projection formula:\n$$\n\\mathbf{s} = \\frac{3}{2}\\mathbf{v}_1 + \\frac{5/2}{3/2}\\mathbf{v}_2 = \\frac{3}{2}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\frac{5}{3}\\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 1 \\end{pmatrix}\n$$\nNow we compute the vector $\\mathbf{s} = (s_1, s_2, s_3)$:\n$$\n\\mathbf{s} = \\begin{pmatrix} \\frac{3}{2}(1) + \\frac{5}{3}(\\frac{1}{2}) \\\\ \\frac{3}{2}(1) + \\frac{5}{3}(-\\frac{1}{2}) \\\\ \\frac{3}{2}(0) + \\frac{5}{3}(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} + \\frac{5}{6} \\\\ \\frac{3}{2} - \\frac{5}{6} \\\\ 0 + \\frac{5}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{6} + \\frac{5}{6} \\\\ \\frac{9}{6} - \\frac{5}{6} \\\\ \\frac{5}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{14}{6} \\\\ \\frac{4}{6} \\\\ \\frac{5}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} \\\\ \\frac{2}{3} \\\\ \\frac{5}{3} \\end{pmatrix}\n$$\nThe problem asks for the second component, $s_2$.\n$$\ns_2 = \\frac{2}{3}\n$$\nFinally, we need to express this as a number rounded to three significant figures.\n$s_2 = \\frac{2}{3} \\approx 0.66666...$\nRounding to three significant figures gives $0.667$.", "answer": "$$\\boxed{0.667}$$", "id": "1381373"}, {"introduction": "A cornerstone of linear algebra is the spectral theorem, which tells us that real symmetric matrices have orthogonal eigenvectors. This powerful property means we can always find an orthonormal basis for the entire space composed of these eigenvectors. This practice problem [@problem_id:1381405] allows you to engage directly with this theorem by finding the eigenspace for a repeated eigenvalue of a symmetric matrix and then using the Gram-Schmidt process to construct a proper orthonormal basis for it.", "problem": "Consider the real symmetric matrix $A$ given by:\n$$\nA = \\begin{pmatrix} 3  -2  -2 \\\\ -2  3  -2 \\\\ -2  -2  3 \\end{pmatrix}\n$$\nIt is known that this matrix has an eigenvalue with an algebraic multiplicity of two. Your task is to find an orthonormal basis for the eigenspace corresponding to this repeated eigenvalue.\n\nLet the basis vectors be $\\vec{v}_1$ and $\\vec{v}_2$. To ensure a unique answer, you must construct your basis according to the following rules:\n1. The last component of the vector $\\vec{v}_1$ must be zero, and its first non-zero component must be positive.\n2. The vector $\\vec{v}_2$ must be orthogonal to $\\vec{v}_1$, and its first non-zero component must be positive.\n\nExpress your answer as a single row matrix containing the components of $\\vec{v}_1$ followed by the components of $\\vec{v}_2$. The components within each vector should be listed in order (i.e., first component, second component, third component).", "solution": "We first identify the repeated eigenvalue and its eigenspace. Observe that\n$$\nA = \\begin{pmatrix} 3  -2  -2 \\\\ -2  3  -2 \\\\ -2  -2  3 \\end{pmatrix} = 5I - 2J,\n$$\nwhere $I$ is the $3\\times 3$ identity matrix and $J$ is the $3\\times 3$ all-ones matrix. The matrix $J$ has eigenvalue $3$ with eigenvector $(1,1,1)^{\\top}$ and eigenvalue $0$ with multiplicity $2$ corresponding to the subspace $\\{ \\mathbf{x} \\in \\mathbb{R}^{3} : x_{1} + x_{2} + x_{3} = 0 \\}$. Therefore, for any vector $v$ with $v_{1} + v_{2} + v_{3} = 0$, we have $Jv = 0$ and hence\n$$\nAv = (5I - 2J)v = 5v,\n$$\nso $5$ is an eigenvalue of $A$ with algebraic and geometric multiplicity $2$. The remaining eigenvalue is obtained by applying $A$ to $(1,1,1)^{\\top}$:\n$$\nA(1,1,1)^{\\top} = (5I - 2J)(1,1,1)^{\\top} = (5 - 2\\cdot 3)(1,1,1)^{\\top} = -1\\cdot (1,1,1)^{\\top}.\n$$\nThus the repeated eigenvalue is $5$, and its eigenspace is\n$$\nE_{5} = \\{(x_{1},x_{2},x_{3})^{\\top} \\in \\mathbb{R}^{3} : x_{1} + x_{2} + x_{3} = 0\\}.\n$$\n\nWe construct an orthonormal basis for $E_{5}$ satisfying the given constraints.\n\n1) To satisfy that the last component of $\\vec{v}_{1}$ is zero and $x_{1}$ is the first non-zero positive component, impose $x_{3}=0$ and $x_{1} + x_{2} = 0$. A suitable choice is $\\vec{v}_{1}^{\\ast} = (1,-1,0)^{\\top}$. Normalize it:\n$$\n\\|\\vec{v}_{1}^{\\ast}\\| = \\sqrt{1^{2} + (-1)^{2} + 0^{2}} = \\sqrt{2}, \\quad \\vec{v}_{1} = \\frac{1}{\\sqrt{2}}(1,-1,0)^{\\top} = \\left(\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0\\right)^{\\top}.\n$$\nThis vector lies in $E_{5}$, has last component zero, and its first non-zero component is positive.\n\n2) For $\\vec{v}_{2}$, require $\\vec{v}_{2} \\in E_{5}$ and $\\vec{v}_{2} \\perp \\vec{v}_{1}$. Let $\\vec{v}_{2} = (a,b,c)^{\\top}$ with $a + b + c = 0$ and\n$$\n\\vec{v}_{1} \\cdot \\vec{v}_{2} = \\frac{1}{\\sqrt{2}}a + \\left(-\\frac{1}{\\sqrt{2}}\\right)b + 0 \\cdot c = \\frac{1}{\\sqrt{2}}(a - b) = 0 \\quad \\Rightarrow \\quad a = b.\n$$\nThen $c = -a - b = -2a$, so an unnormalized choice is $\\vec{v}_{2}^{\\ast} = (1,1,-2)^{\\top}$. Normalize it:\n$$\n\\|\\vec{v}_{2}^{\\ast}\\| = \\sqrt{1^{2} + 1^{2} + (-2)^{2}} = \\sqrt{6}, \\quad \\vec{v}_{2} = \\frac{1}{\\sqrt{6}}(1,1,-2)^{\\top} = \\left(\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}}, -\\frac{2}{\\sqrt{6}}\\right)^{\\top}.\n$$\nThis $\\vec{v}_{2}$ lies in $E_{5}$, is orthogonal to $\\vec{v}_{1}$, has unit norm, and its first non-zero component is positive.\n\nTherefore, an orthonormal basis for the eigenspace corresponding to the repeated eigenvalue, obeying the stated rules and ordering $\\vec{v}_{1}$ then $\\vec{v}_{2}$, is given by the row matrix containing their components in order.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{6}}  \\frac{1}{\\sqrt{6}}  -\\frac{2}{\\sqrt{6}}\\end{pmatrix}}$$", "id": "1381405"}, {"introduction": "The familiar concepts of length, distance, and angle are all derived from the standard dot product, but this is not the only way to define geometry in a vector space. In many physical and engineering applications, a weighted or \"non-standard\" inner product is required to accurately model the system. This advanced problem [@problem_id:1381374] challenges you to adapt your understanding of orthogonality by applying the Gram-Schmidt algorithm using a new set of rules defined by a matrix, demonstrating the true abstract power of the procedure.", "problem": "In certain physical systems, such as the analysis of vibrations in a discrete lattice, the standard Euclidean inner product is insufficient. Instead, a weighted inner product is used to account for the interactions between components. Consider the vector space $\\mathbb{R}^3$ equipped with a non-standard inner product defined by $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^T A \\mathbf{v}$, where $\\mathbf{u}$ and $\\mathbf{v}$ are column vectors in $\\mathbb{R}^3$ and $A$ is the positive definite matrix given by:\n$$A = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix}$$\nYou are given a basis $B = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$ for $\\mathbb{R}^3$, where\n$$ \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{v}_3 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\nApply the Gram-Schmidt orthogonalization process to the basis $B$ to produce an orthonormal basis $W = \\{\\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3\\}$ with respect to the given inner product.\n\nDetermine the third vector, $\\mathbf{w}_3$, of this new orthonormal basis.", "solution": "We use the inner product $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^{T}A\\mathbf{v}$ with \n$$\nA=\\begin{pmatrix}2-10\\\\-12-1\\\\0-12\\end{pmatrix},\n$$\nand apply Gramâ€“Schmidt in the form\n$$\n\\mathbf{u}_{1}=\\mathbf{v}_{1},\\quad \\mathbf{u}_{k}=\\mathbf{v}_{k}-\\sum_{j=1}^{k-1}\\frac{\\langle \\mathbf{v}_{k},\\mathbf{u}_{j}\\rangle}{\\langle \\mathbf{u}_{j},\\mathbf{u}_{j}\\rangle}\\mathbf{u}_{j},\\quad \\mathbf{w}_{k}=\\frac{\\mathbf{u}_{k}}{\\|\\mathbf{u}_{k}\\|},\n$$\nwhere $\\|\\mathbf{x}\\|=\\sqrt{\\langle \\mathbf{x},\\mathbf{x}\\rangle}$.\n\nFirst, compute for $\\mathbf{v}_{1}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$:\n$$\nA\\mathbf{v}_{1}=\\begin{pmatrix}2\\\\-1\\\\0\\end{pmatrix},\\quad \\langle \\mathbf{v}_{1},\\mathbf{v}_{1}\\rangle=\\mathbf{v}_{1}^{T}A\\mathbf{v}_{1}=2,\n$$\nso $\\mathbf{u}_{1}=\\mathbf{v}_{1}$ and $\\mathbf{w}_{1}=\\mathbf{v}_{1}/\\sqrt{2}$.\n\nNext, for $\\mathbf{v}_{2}=\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}$:\n$$\n\\langle \\mathbf{v}_{2},\\mathbf{u}_{1}\\rangle=\\mathbf{v}_{2}^{T}A\\mathbf{v}_{1}=\\begin{pmatrix}110\\end{pmatrix}\\begin{pmatrix}2\\\\-1\\\\0\\end{pmatrix}=1,\\quad \\langle \\mathbf{u}_{1},\\mathbf{u}_{1}\\rangle=2,\n$$\nhence\n$$\n\\mathbf{u}_{2}=\\mathbf{v}_{2}-\\frac{1}{2}\\mathbf{u}_{1}=\\begin{pmatrix}\\frac{1}{2}\\\\1\\\\0\\end{pmatrix}.\n$$\nCompute\n$$\nA\\mathbf{u}_{2}=\\begin{pmatrix}0\\\\\\frac{3}{2}\\\\-1\\end{pmatrix},\\quad \\langle \\mathbf{u}_{2},\\mathbf{u}_{2}\\rangle=\\mathbf{u}_{2}^{T}A\\mathbf{u}_{2}=\\frac{3}{2},\n$$\nso $\\|\\mathbf{u}_{2}\\|=\\sqrt{\\frac{3}{2}}$ and $\\mathbf{w}_{2}=\\mathbf{u}_{2}/\\sqrt{\\frac{3}{2}}$.\n\nFor $\\mathbf{v}_{3}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$, compute the needed inner products:\n$$\n\\langle \\mathbf{v}_{3},\\mathbf{u}_{1}\\rangle=\\mathbf{v}_{3}^{T}A\\mathbf{v}_{1}=\\begin{pmatrix}111\\end{pmatrix}\\begin{pmatrix}2\\\\-1\\\\0\\end{pmatrix}=1,\\quad \\langle \\mathbf{u}_{1},\\mathbf{u}_{1}\\rangle=2,\n$$\nso the first projection coefficient is $\\frac{1}{2}$. Also,\n$$\n\\langle \\mathbf{v}_{3},\\mathbf{u}_{2}\\rangle=\\mathbf{v}_{3}^{T}A\\mathbf{u}_{2}=\\begin{pmatrix}111\\end{pmatrix}\\begin{pmatrix}0\\\\\\frac{3}{2}\\\\-1\\end{pmatrix}=\\frac{1}{2},\\quad \\langle \\mathbf{u}_{2},\\mathbf{u}_{2}\\rangle=\\frac{3}{2},\n$$\nso the second projection coefficient is $\\frac{\\frac{1}{2}}{\\frac{3}{2}}=\\frac{1}{3}$. Therefore\n$$\n\\mathbf{u}_{3}=\\mathbf{v}_{3}-\\frac{1}{2}\\mathbf{u}_{1}-\\frac{1}{3}\\mathbf{u}_{2}\n=\\mathbf{v}_{3}-\\frac{1}{3}\\mathbf{v}_{2}-\\frac{1}{3}\\mathbf{v}_{1}\n=\\begin{pmatrix}\\frac{1}{3}\\\\\\frac{2}{3}\\\\1\\end{pmatrix}.\n$$\nCompute its norm:\n$$\nA\\mathbf{u}_{3}=\\begin{pmatrix}0\\\\0\\\\\\frac{4}{3}\\end{pmatrix},\\quad \\langle \\mathbf{u}_{3},\\mathbf{u}_{3}\\rangle=\\mathbf{u}_{3}^{T}A\\mathbf{u}_{3}=\\frac{4}{3},\n$$\nso $\\|\\mathbf{u}_{3}\\|=\\frac{2}{\\sqrt{3}}$. Hence the normalized third vector is\n$$\n\\mathbf{w}_{3}=\\frac{\\mathbf{u}_{3}}{\\|\\mathbf{u}_{3}\\|}=\\frac{\\sqrt{3}}{2}\\begin{pmatrix}\\frac{1}{3}\\\\\\frac{2}{3}\\\\1\\end{pmatrix}\n=\\begin{pmatrix}\\frac{\\sqrt{3}}{6}\\\\\\frac{\\sqrt{3}}{3}\\\\\\frac{\\sqrt{3}}{2}\\end{pmatrix}.\n$$\nThis $\\mathbf{w}_{3}$ is orthonormal (with respect to $\\langle \\cdot,\\cdot\\rangle$) to $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ and has unit norm.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\sqrt{3}}{6}\\\\\\frac{\\sqrt{3}}{3}\\\\\\frac{\\sqrt{3}}{2}\\end{pmatrix}}$$", "id": "1381374"}]}