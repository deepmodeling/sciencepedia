{"hands_on_practices": [{"introduction": "This first exercise grounds the abstract concept of linear dependence in a concrete, computational setting. While polynomials might seem different from the column vectors you're used to, they form a vector space and can be represented by their coefficients. This problem [@problem_id:1372990] demonstrates a powerful technique: using the determinant of a coefficient matrix to test for linear dependence, a method that is both efficient and widely applicable.", "problem": "In a simplified model of a physical system, the state of a particle is described by a function of time, $p(t)$, which is a polynomial of degree at most 2. These states are elements of the vector space $P_2$, which consists of all polynomials of the form $a_0 + a_1 t + a_2 t^2$, where $a_0, a_1, a_2$ are real numbers.\n\nAn engineer defines three fundamental state configurations for an experiment, represented by the following polynomials:\n$p_1(t) = 1 + 2t - 3t^2$\n$p_2(t) = 3 + 5t - 2t^2$\n$p_3(t) = -4 + ht + t^2$\n\nThe parameter $h$ represents a tunable setting in the experimental apparatus. For the three states to serve as a distinct set of references, they must be linearly independent. If the set of states is linearly dependent, it implies a redundancy in the setup, as one state can be expressed as a linear combination of the others.\n\nDetermine the specific value of the parameter $h$ for which the set of polynomials $\\{p_1(t), p_2(t), p_3(t)\\}$ is linearly dependent.", "solution": "We work in the basis $\\{1, t, t^{2}\\}$ of $P_{2}$, so each polynomial is represented by its coefficient vector:\n$$\np_{1}(t) \\leftrightarrow (1,\\,2,\\,-3),\\quad\np_{2}(t) \\leftrightarrow (3,\\,5,\\,-2),\\quad\np_{3}(t) \\leftrightarrow (-4,\\,h,\\,1).\n$$\nThe three polynomials are linearly dependent if and only if the determinant of the coefficient matrix is zero. Using the vectors as rows, define\n$$\nM=\\begin{pmatrix}\n1 & 2 & -3\\\\\n3 & 5 & -2\\\\\n-4 & h & 1\n\\end{pmatrix}.\n$$\nThen linear dependence is equivalent to $\\det(M)=0$. Compute $\\det(M)$ by expanding along the first row:\n$$\n\\det(M)=1\\cdot\\det\\begin{pmatrix}5 & -2\\\\ h & 1\\end{pmatrix}\n-2\\cdot\\det\\begin{pmatrix}3 & -2\\\\ -4 & 1\\end{pmatrix}\n+(-3)\\cdot\\det\\begin{pmatrix}3 & 5\\\\ -4 & h\\end{pmatrix}.\n$$\nEvaluate the $2\\times 2$ determinants:\n$$\n\\det\\begin{pmatrix}5 & -2\\\\ h & 1\\end{pmatrix}=5+2h,\\quad\n\\det\\begin{pmatrix}3 & -2\\\\ -4 & 1\\end{pmatrix}=3-8=-5,\\quad\n\\det\\begin{pmatrix}3 & 5\\\\ -4 & h\\end{pmatrix}=3h+20.\n$$\nSubstitute to obtain\n$$\n\\det(M)=(5+2h)-2(-5)+(-3)(3h+20)=(5+2h)+10-9h-60=-7h-45.\n$$\nSet this equal to zero for linear dependence:\n$$\n-7h-45=0\\quad\\Rightarrow\\quad h=-\\frac{45}{7}.\n$$\nTherefore, the set $\\{p_{1},p_{2},p_{3}\\}$ is linearly dependent precisely when $h=-\\frac{45}{7}$.", "answer": "$$\\boxed{-\\frac{45}{7}}$$", "id": "1372989"}, {"introduction": "Now we move from the finite-dimensional space of polynomials to the infinite-dimensional vector space of continuous functions. This problem [@problem_id:1372948] challenges you to apply the definition of linear independence in a new context, where visual intuition about functions like $x$ and $|x|$ can be tested with algebraic rigor. You'll discover that linear dependence in function spaces depends crucially on the domain over which the functions are considered.", "problem": "In linear algebra, a set of vectors $\\{v_1, v_2, \\dots, v_n\\}$ in a vector space $V$ is defined as **linearly independent** if the only solution to the equation $c_1 v_1 + c_2 v_2 + \\dots + c_n v_n = \\mathbf{0}$ (where $\\mathbf{0}$ is the zero vector in $V$) is the trivial solution $c_1=c_2=\\dots=c_n=0$. If a non-trivial solution exists, the set is **linearly dependent**.\n\nConsider the vector space $V = C(I)$, which is the space of all continuous real-valued functions defined on an interval $I \\subseteq \\mathbb{R}$. In this space, the \"vectors\" are the functions, and the zero vector is the function $f(x)=0$ for all $x \\in I$.\n\nAnalyze the linear independence of the following sets of functions within different vector spaces.\n\n*   **Set 1:** $\\{f_1(x) = \\exp(x), f_2(x) = \\exp(2x)\\}$\n*   **Set 2:** $\\{g_1(x) = 1, g_2(x) = \\cos(2x), g_3(x) = \\sin^2(x)\\}$\n*   **Set 3:** $\\{h_1(x) = x, h_2(x) = |x|\\}$\n*   **Set 4:** $\\{k_1(x) = x^3, k_2(x) = |x^3|\\}$\n\nWhich of the following statements is true?\n\nA. Set 1 is linearly dependent in the space $C(\\mathbb{R})$.\n\nB. Set 2 is linearly independent in the space $C(\\mathbb{R})$.\n\nC. Set 3 is linearly dependent in the space $C([-2, 2])$, but linearly independent in the space $C([0, 2])$.\n\nD. Set 4 is linearly independent in the space $C([-2, 2])$, but linearly dependent in the space $C([-2, 0])$.", "solution": "We use the definition of linear independence in the function space context: a set $\\{f_{1},\\dots,f_{n}\\} \\subset C(I)$ is linearly independent if the only solution to $c_{1}f_{1}(x)+\\cdots+c_{n}f_{n}(x)=0$ for all $x \\in I$ is $c_{1}=\\cdots=c_{n}=0$.\n\nSet 1: $\\{f_{1}(x)=\\exp(x), f_{2}(x)=\\exp(2x)\\}$ in $C(\\mathbb{R})$. Suppose $a\\exp(x)+b\\exp(2x)=0$ for all $x \\in \\mathbb{R}$. Factor $\\exp(x)$ to obtain\n$$\n\\exp(x)\\big(a+b\\exp(x)\\big)=0 \\quad \\text{for all } x.\n$$\nSince $\\exp(x)\\neq 0$ for all $x$, this implies $a+b\\exp(x)=0$ for all $x$. The function $b\\exp(x)$ is nonconstant unless $b=0$. Hence $b=0$, which then forces $a=0$. Therefore $f_{1},f_{2}$ are linearly independent, so the statement “Set 1 is linearly dependent in $C(\\mathbb{R})$” is false.\n\nSet 2: $\\{g_{1}(x)=1,g_{2}(x)=\\cos(2x),g_{3}(x)=\\sin^{2}(x)\\}$ in $C(\\mathbb{R})$. Use the identity\n$$\n\\sin^{2}(x)=\\frac{1-\\cos(2x)}{2}.\n$$\nRewriting gives\n$$\n2\\sin^{2}(x)-1+\\cos(2x)=0 \\quad \\text{for all } x.\n$$\nThus the nontrivial linear combination $(-1)\\cdot g_{1}(x)+(1)\\cdot g_{2}(x)+2\\cdot g_{3}(x)=0$ holds identically, so the set is linearly dependent. Therefore the statement “Set 2 is linearly independent in $C(\\mathbb{R})$” is false.\n\nSet 3: $\\{h_{1}(x)=x,h_{2}(x)=|x|\\}$. First consider $C([-2,2])$. Suppose $a x+b|x|=0$ for all $x\\in[-2,2]$. For $x\\in[0,2]$ we have $|x|=x$, hence $(a+b)x=0$ for all $x\\in[0,2]$, which implies $a+b=0$. For $x\\in[-2,0]$ we have $|x|=-x$, hence $(a-b)x=0$ for all $x\\in[-2,0]$, which implies $a-b=0$. Solving $a+b=0$ and $a-b=0$ gives $a=0$ and $b=0$, so the set is linearly independent on $[-2,2]$. Now consider $C([0,2])$. On $[0,2]$, $|x|=x$, so $h_{2}=h_{1}$, and thus there exists the nontrivial relation $h_{2}-h_{1}=0$. Hence the set is linearly dependent on $[0,2]$. Therefore the statement “Set 3 is linearly dependent in $C([-2,2])$, but linearly independent in $C([0,2])$” is false.\n\nSet 4: $\\{k_{1}(x)=x^{3},k_{2}(x)=|x^{3}|\\}$. Note that $|x^{3}|=x^{3}$ for $x\\geq 0$ and $|x^{3}|=-x^{3}$ for $x\\leq 0$. In $C([-2,2])$, suppose $a x^{3}+b|x^{3}|=0$ for all $x\\in[-2,2]$. For $x\\in[0,2]$, $(a+b)x^{3}=0$ for all $x$ implies $a+b=0$. For $x\\in[-2,0]$, $(a-b)x^{3}=0$ for all $x$ implies $a-b=0$. Thus $a=b=0$, and the set is linearly independent on $[-2,2]$. In $C([-2,0])$, for all $x\\leq 0$ we have $|x^{3}|=-x^{3}$, so $k_{2}=-k_{1}$ on this interval. Therefore $k_{1}+k_{2}=0$ is a nontrivial linear relation, and the set is linearly dependent on $[-2,0]$. Hence the statement “Set 4 is linearly independent in $C([-2,2])$, but linearly dependent in $C([-2,0])$” is true.\n\nOnly statement D is true.", "answer": "$$\\boxed{D}$$", "id": "1372948"}, {"introduction": "Our final practice problem takes a more abstract and theoretical turn, stripping the concept of linear dependence down to its core definition. Instead of specific vectors or functions, you'll work with abstract vectors and explore how their linear combinations behave. This exercise [@problem_id:1372976] reveals a fascinating and subtle point: the very nature of linear independence can depend on the underlying number system, or field, in which the vector space is defined.", "problem": "In a theoretical model of a multi-particle system, its possible configurations are represented by vectors in a vector space $V$ over a field $F$. A set of three fundamental states, represented by the vectors $\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3 \\}$, has been identified and is known to form a linearly independent set.\n\nTo analyze the system's dynamics, an analyst constructs a new set of three 'composite' states, $S'$, which are formed by pairwise combinations of the fundamental states:\n- $\\mathbf{w}_1 = \\mathbf{v}_1 + \\mathbf{v}_2$\n- $\\mathbf{w}_2 = \\mathbf{v}_2 + \\mathbf{v}_3$\n- $\\mathbf{w}_3 = \\mathbf{v}_3 + \\mathbf{v}_1$\n\nThe set of these composite states is thus $S' = \\{ \\mathbf{w}_1, \\mathbf{w}_2, \\mathbf{w}_3 \\}$. Which of the following statements provides a complete and accurate description of the linear independence of the set $S'$?\n\nA. The set $S'$ is always linearly independent, regardless of the field $F$.\n\nB. The set $S'$ is always linearly dependent, regardless of the field $F$.\n\nC. The linear independence of $S'$ depends on the specific choice of the vectors $\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3$.\n\nD. The linear independence of $S'$ depends on the characteristic of the field $F$.\n\nE. The linear independence of $S'$ depends on the dimension of the vector space $V$.", "solution": "Let $\\alpha,\\beta,\\gamma \\in F$ satisfy the linear relation\n$$\n\\alpha \\mathbf{w}_{1}+\\beta \\mathbf{w}_{2}+\\gamma \\mathbf{w}_{3}=\\mathbf{0}.\n$$\nUsing the definitions $\\mathbf{w}_{1}=\\mathbf{v}_{1}+\\mathbf{v}_{2}$, $\\mathbf{w}_{2}=\\mathbf{v}_{2}+\\mathbf{v}_{3}$, and $\\mathbf{w}_{3}=\\mathbf{v}_{3}+\\mathbf{v}_{1}$, expand and collect terms:\n$$\n\\alpha(\\mathbf{v}_{1}+\\mathbf{v}_{2})+\\beta(\\mathbf{v}_{2}+\\mathbf{v}_{3})+\\gamma(\\mathbf{v}_{3}+\\mathbf{v}_{1})\n=(\\alpha+\\gamma)\\mathbf{v}_{1}+(\\alpha+\\beta)\\mathbf{v}_{2}+(\\beta+\\gamma)\\mathbf{v}_{3}.\n$$\nSince $\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\mathbf{v}_{3}\\}$ is linearly independent, each coefficient must vanish:\n$$\n\\alpha+\\gamma=0,\\quad \\alpha+\\beta=0,\\quad \\beta+\\gamma=0.\n$$\nFrom $\\alpha+\\beta=0$ and $\\alpha+\\gamma=0$ we obtain $\\beta=-\\alpha$ and $\\gamma=-\\alpha$. Substituting into $\\beta+\\gamma=0$ gives\n$$\n(-\\alpha)+(-\\alpha)= -2\\alpha=0 \\quad \\Longleftrightarrow \\quad 2\\alpha=0.\n$$\nTherefore:\n- If $\\operatorname{char}(F)\\neq 2$, then $2\\neq 0$ in $F$, so $\\alpha=0$, hence $\\beta=0$ and $\\gamma=0$. Thus $\\{\\mathbf{w}_{1},\\mathbf{w}_{2},\\mathbf{w}_{3}\\}$ is linearly independent.\n- If $\\operatorname{char}(F)=2$, then $2=0$ in $F$, so any $\\alpha\\in F$ with $\\beta=\\gamma=-\\alpha=\\alpha$ yields a nontrivial relation. Indeed,\n$$\n\\mathbf{w}_{1}+\\mathbf{w}_{2}+\\mathbf{w}_{3}\n=2(\\mathbf{v}_{1}+\\mathbf{v}_{2}+\\mathbf{v}_{3})=\\mathbf{0}\n$$\nin characteristic $2$, so $\\{\\mathbf{w}_{1},\\mathbf{w}_{2},\\mathbf{w}_{3}\\}$ is linearly dependent.\n\nHence the linear independence of $S'$ depends precisely on the characteristic of the field $F$.", "answer": "$$\\boxed{D}$$", "id": "1372976"}]}