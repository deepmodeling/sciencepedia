{"hands_on_practices": [{"introduction": "The Conjugate Gradient method can seem abstract at first, but the best way to understand its mechanics is to perform a calculation by hand. This first exercise [@problem_id:1393666] guides you through a single, complete iteration of the algorithm, from calculating the initial residual to finding the first updated solution vector $x_1$. Mastering this foundational step will demystify the process and build your confidence in applying the method.", "problem": "Consider the linear system of equations $Ax=b$, where the matrix $A$ and the vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\nThe matrix $A$ is symmetric and positive-definite.\n\nStarting with an initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one iteration of the Conjugate Gradient method to find the first updated solution, $x_1$.\n\nExpress your answer as a row matrix containing the components of $x_1$ as exact fractions.", "solution": "We apply the Conjugate Gradient method for a symmetric positive-definite matrix $A$ starting from $x_{0}$. The standard first-iteration formulas are:\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\nGiven $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$, compute\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\nNext, compute $A p_{0}$:\n$$\nA p_{0} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\nCompute the scalar products:\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\nThus,\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\nUpdate the solution:\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\nExpressed as a row matrix, the components of $x_{1}$ are $\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}}$$", "id": "1393666"}, {"introduction": "A core property of the Conjugate Gradient method is that it is an optimization algorithm that monotonically decreases the error in the \"energy\" or $A$-norm. However, it is a common misconception that the size of the residual vector, measured by the standard Euclidean norm $\\|r_k\\|_2$, must also decrease at every step. This exercise [@problem_id:1393663] provides a concrete example where the residual norm actually *increases* after the first iteration, highlighting a subtle but crucial aspect of the algorithm's convergence behavior.", "problem": "The Conjugate Gradient (CG) method is an iterative algorithm for solving systems of linear equations $Ax=b$, where the matrix $A$ is symmetric and positive-definite. The algorithm generates a sequence of solutions $x_k$ that converge to the true solution.\n\nThe standard algorithm is defined as follows, starting with an initial guess $x_0$:\n1.  Initialize the residual $r_0 = b - Ax_0$ and the search direction $p_0 = r_0$.\n2.  For $k = 0, 1, 2, \\dots$ until convergence:\n    a. Compute the step size: $\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}$\n    b. Update the solution: $x_{k+1} = x_k + \\alpha_k p_k$\n    c. Update the residual: $r_{k+1} = r_k - \\alpha_k A p_k$\n    d. Compute the improvement factor: $\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$\n    e. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$\n\nConsider the linear system $Ax=b$ where the matrix $A$ and vector $b$ are given by:\n$$ A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 10 & 0 \\\\ 0 & 0 & 100 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix} $$\nUsing an initial guess of $x_0 = (0, 0, 0)^T$, perform one full iteration of the Conjugate Gradient algorithm (i.e., the calculations corresponding to $k=0$ in the loop) to determine the first updated residual vector, $r_1$.\n\nCalculate the Euclidean norm (also known as the $L^2$-norm) of this residual vector, $\\|r_1\\|_2$. Report your answer as a numerical value rounded to four significant figures.", "solution": "We apply the Conjugate Gradient (CG) algorithm for one iteration starting from $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nCompute the initial residual and search direction:\n$$\nr_{0} = b - A x_{0} = b = \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix}, \\quad p_{0} = r_{0}.\n$$\nThe step size is\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{1^{2} + 0.1^{2} + 0.01^{2}}{1 \\cdot 1^{2} + 10 \\cdot (0.1)^{2} + 100 \\cdot (0.01)^{2}} = \\frac{1.0101}{1.11} = 0.91.\n$$\nCompute $A p_{0}$:\n$$\nA p_{0} = A r_{0} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 10 & 0 \\\\ 0 & 0 & 100 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nUpdate the residual:\n$$\nr_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 0.1 \\\\ 0.01 \\end{pmatrix} - 0.91 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.09 \\\\ -0.81 \\\\ -0.9 \\end{pmatrix}.\n$$\nCompute its Euclidean norm:\n$$\n\\|r_{1}\\|_{2} = \\sqrt{(0.09)^{2} + (-0.81)^{2} + (-0.9)^{2}} = \\sqrt{0.0081 + 0.6561 + 0.81} = \\sqrt{1.4742} \\approx 1.214.\n$$\nRounded to four significant figures, the norm is $1.214$.", "answer": "$$\\boxed{1.214}$$", "id": "1393663"}, {"introduction": "The theoretical elegance and guaranteed convergence of the Conjugate Gradient method hinge on a critical assumption: the system's matrix $A$ must be symmetric and positive-definite. But what happens if we apply the algorithm when this condition is violated? This computational practice [@problem_id:2382427] challenges you to explore precisely that scenario, demonstrating the potential for erratic behavior or failure and underscoring why specialized methods are needed for non-symmetric systems.", "problem": "You are given square matrices, right-hand side vectors, an initial vector, a tolerance, and a maximum number of allowed iterations. For each case, generate the sequence produced by the standard unpreconditioned conjugate gradient iteration for solving the linear system $A x = b$ starting from $x_0$, and assess its behavior by three quantitative criteria. Use the Euclidean vector norm throughout.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$, and $x_0 \\in \\mathbb{R}^{n}$. Define the initial residual $r_0 = b - A x_0$ and the initial search direction $p_0 = r_0$. For iteration index $k = 0, 1, 2, \\dots$, define\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\nIf $p_k^T A p_k = 0$ at any iteration, declare a breakdown and stop. At each step, track the relative residual norm\n$$\n\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}.\n$$\nStop the iteration when either $\\rho_k \\le \\tau$ or the iteration count reaches $k_{\\max}$ or a breakdown is detected. Define that the sequence $\\{\\rho_k\\}$ exhibits monotone nonincreasing behavior if $\\rho_{k+1} \\le \\rho_k + \\varepsilon$ for all consecutive pairs, where $\\varepsilon = 10^{-12}$.\n\nFor each test case below, produce a result list containing four entries in the following order:\n1. A boolean indicating whether the stopping condition $\\rho_k \\le \\tau$ was met before reaching $k_{\\max}$ iterations and without breakdown.\n2. An integer equal to the number of iterations actually performed (the count of updates from $x_k$ to $x_{k+1}$).\n3. A float equal to the final relative residual norm $\\rho_{\\text{final}}$ rounded to six decimal places.\n4. A boolean indicating whether the sequence $\\{\\rho_k\\}$ was monotone nonincreasing according to the definition above.\n\nTest suite (use exactly these data, in the order listed):\n\n- Case $1$ (symmetric positive definite reference):\n  - Dimension $n = 10$.\n  - Matrix $A_1$ with entries $(A_1)_{ii} = 2$ for $i = 1, \\dots, n$, $(A_1)_{i,i+1} = (A_1)_{i+1,i} = -1$ for $i = 1, \\dots, n-1$, and zero elsewhere.\n  - Right-hand side $b_1$ with components $(b_1)_i = 1$ for $i = 1, \\dots, n$.\n  - Initial vector $x_{0,1}$ with $(x_{0,1})_i = 0$ for $i = 1, \\dots, n$.\n  - Tolerance $\\tau_1 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,1} = 10$.\n\n- Case $2$ (mildly non-symmetric):\n  - Dimension $n = 10$.\n  - Let $S$ be the strictly skew-symmetric matrix with $S_{i,i+1} = 1$ and $S_{i+1,i} = -1$ for $i = 1, \\dots, n-1$, and zero elsewhere.\n  - Matrix $A_2 = A_1 + \\gamma S$ with $\\gamma = 0.1$.\n  - Right-hand side $b_2 = b_1$.\n  - Initial vector $x_{0,2} = x_{0,1}$.\n  - Tolerance $\\tau_2 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,2} = 10$.\n\n- Case $3$ (strongly non-symmetric, upper triangular non-normal):\n  - Dimension $n = 10$.\n  - Matrix $A_3$ with $(A_3)_{ii} = 2$ for $i = 1, \\dots, n$, $(A_3)_{ij} = 1$ for $i < j$, and $(A_3)_{ij} = 0$ for $i > j$.\n  - Right-hand side $b_3 = b_1$.\n  - Initial vector $x_{0,3} = x_{0,1}$.\n  - Tolerance $\\tau_3 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,3} = 10$.\n\n- Case $4$ (boundary, minimal dimension):\n  - Dimension $n = 1$.\n  - Matrix $A_4 = [2]$.\n  - Right-hand side $b_4 = [1]$.\n  - Initial vector $x_{0,4} = [0]$.\n  - Tolerance $\\tau_4 = 10^{-14}$.\n  - Maximum iterations $k_{\\max,4} = 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sublist in the same order as above. The format must be:\n\"[[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool]]\"\nFor the float, round to six decimal places. No physical units are involved, angles are not used, and no percentages appear in the output.", "solution": "The problem as stated is a valid and well-posed exercise in the field of computational physics and numerical linear algebra. It is scientifically grounded, free of contradictions, and provides all necessary information to proceed with a solution. The task is to implement the standard Conjugate Gradient (CG) algorithm and evaluate its performance on a series of well-defined test cases.\n\nThe Conjugate Gradient method is an iterative algorithm designed for solving large, sparse linear systems of equations of the form $A x = b$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive-definite (SPD). The method's effectiveness is based on the iterative construction of a set of $A$-orthogonal (or conjugate) search directions $\\{p_k\\}_{k=0}^{n-1}$, which satisfy $p_i^T A p_j = 0$ for $i \\neq j$. This $A$-orthogonality guarantees that the error is minimized in the $A$-norm at each step over the expanding Krylov subspace, $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$. A key consequence is that the sequence of residuals $\\{r_k\\}$ are mutually orthogonal, i.e., $r_i^T r_j = 0$ for $i \\neq j$. In exact arithmetic, this process guarantees convergence to the exact solution in at most $n$ iterations.\n\nWhen the matrix $A$ is not symmetric, as in Cases $2$ and $3$ of the problem, the theoretical foundations of the CG method are no longer valid. The properties of $A$-orthogonality of search directions and orthogonality of residuals are lost. As a result, convergence is not guaranteed. The residual norm, $\\lVert r_k \\rVert_2$, may exhibit erratic, non-monotonic behavior, and the algorithm may fail to converge or even diverge. The provided test cases are structured to demonstrate this principle:\n- Case $1$: The matrix $A_1$ is a symmetric positive-definite matrix (the discrete one-dimensional Laplacian), representing the ideal scenario for the CG method.\n- Case $2$: The matrix $A_2$ is a mildly non-symmetric perturbation of $A_1$. The CG method may still converge, but its ideal performance characteristics, such as monotonic residual reduction, may be lost.\n- Case $3$: The matrix $A_3$ is a strongly non-symmetric, non-normal, upper triangular matrix. Applying the formal CG algorithm here is expected to yield poor results, demonstrating the method's limitations.\n- Case $4$: A trivial $n=1$ case which must converge in a single step to the exact solution.\n\nThe implementation will strictly follow the algorithm provided in the problem statement. Starting from an initial guess $x_0$, we compute the initial residual $r_0 = b - A x_0$ and the initial search direction $p_0 = r_0$. The iterative process for $k = 0, 1, 2, \\dots$ involves the following calculations:\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\nThe iteration terminates based on three conditions:\n1.  **Convergence**: The relative residual norm $\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}$ drops below a specified tolerance $\\tau$.\n2.  **Maximum Iterations**: The number of iterations reaches the maximum allowed, $k_{\\max}$.\n3.  **Breakdown**: The denominator in the expression for $\\alpha_k$, $p_k^T A p_k$, becomes zero. For an SPD matrix this can only occur if $p_k=0$, which implies the solution has been found. For a general matrix, this can happen for $p_k \\neq 0$, constituting a fatal breakdown of the algorithm.\n\nFrom the execution trace of the algorithm on each test case, the four specified quantitative metrics will be determined:\n1.  A boolean indicating if convergence (via the $\\tau$ condition) was achieved without breakdown and within the $k_{\\max}$ limit.\n2.  The total number of iterations performed.\n3.  The final relative residual norm $\\rho_{\\text{final}}$, rounded to six decimal places.\n4.  A boolean indicating whether the sequence of relative residual norms $\\{\\rho_k\\}$ was monotonically non-increasing, defined as $\\rho_{k+1} \\le \\rho_k + \\varepsilon$ for all $k$, where the tolerance $\\varepsilon = 10^{-12}$ accounts for minor floating-point fluctuations.", "answer": "[[True, 10, 0.000000, True], [False, 10, 0.280140, False], [False, 10, 6.786523, False], [True, 1, 0.000000, True]]", "id": "2382427"}]}