{"hands_on_practices": [{"introduction": "To truly understand invariant subspaces, the first step is to apply the definition directly. This exercise moves beyond the familiar space of column vectors into the space of $2 \\times 2$ matrices, challenging you to test several common types of subspaces for invariance under a specific linear operator. By working through each case, you will build a solid, practical understanding of what it means for a subspace to be \"closed\" under a transformation. [@problem_id:1368900]", "problem": "Let $V = M_2(\\mathbb{R})$ be the vector space of all $2 \\times 2$ matrices with real entries. Consider the linear operator $T: V \\to V$ defined by the action $T(A) = BA$, where $B$ is the fixed matrix\n$$\nB = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nDetermine which of the following subspaces of $V$ are $T$-invariant. Select all correct options.\n\nA. The subspace of all symmetric matrices (where $A^T = A$).\n\nB. The subspace of all skew-symmetric matrices (where $A^T = -A$).\n\nC. The subspace of all matrices with a trace of zero.\n\nD. The subspace of all matrices whose second row is zero.\n\nE. The subspace of all matrices whose second column is zero.", "solution": "We are given the linear operator $T: V \\to V$ defined by $T(A) = BA$, where\n$$\nB = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nA subspace $S \\subseteq V$ is $T$-invariant if and only if $A \\in S$ implies $BA \\in S$. Let a general matrix be\n$$\nA = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}.\n$$\nThen\n$$\nBA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} a + c & b + d \\\\ c & d \\end{pmatrix}.\n$$\n\nAnalyze each subspace:\n\nA. Symmetric matrices: $A^{T} = A$ means $A = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}$. Then\n$$\nBA = \\begin{pmatrix} a + b & b + d \\\\ b & d \\end{pmatrix}.\n$$\nFor $BA$ to be symmetric, we need $b + d = b$, i.e., $d = 0$, which does not hold for all symmetric $A$. A concrete counterexample is $A = I$, for which $BA = B$ is not symmetric. Hence A is not invariant.\n\nB. Skew-symmetric matrices: $A^{T} = -A$ implies $A = \\begin{pmatrix} 0 & s \\\\ -s & 0 \\end{pmatrix}$. Then\n$$\nBA = \\begin{pmatrix} -s & s \\\\ -s & 0 \\end{pmatrix},\n$$\nwhose first diagonal entry is $-s$, which is zero only if $s = 0$. Thus $BA$ is not skew-symmetric in general. Hence B is not invariant.\n\nC. Trace-zero matrices: $\\operatorname{tr}(A) = a + d = 0$. Compute\n$$\n\\operatorname{tr}(BA) = (a + c) + d = a + d + c = \\operatorname{tr}(A) + c = c.\n$$\nThis equals zero for all $A$ with trace zero only if $c = 0$, which is not required in the subspace. For example, with $A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$ we have $\\operatorname{tr}(A) = 0$ but $\\operatorname{tr}(BA) = 1$. Hence C is not invariant.\n\nD. Matrices with second row zero: $A = \\begin{pmatrix} x & y \\\\ 0 & 0 \\end{pmatrix}$. Then\n$$\nBA = \\begin{pmatrix} x & y \\\\ 0 & 0 \\end{pmatrix} = A,\n$$\nso the second row remains zero. Alternatively, row-wise, left multiplication by $B$ maps the second row to $0 \\cdot \\text{row}_{1}(A) + 1 \\cdot \\text{row}_{2}(A) = \\text{row}_{2}(A)$, which remains zero. Hence D is invariant.\n\nE. Matrices with second column zero: $A = \\begin{pmatrix} x & 0 \\\\ y & 0 \\end{pmatrix}$. Then\n$$\nBA = \\begin{pmatrix} x + y & 0 \\\\ y & 0 \\end{pmatrix},\n$$\nwhose second column is still zero. Equivalently, column-wise, $BA = [B \\cdot \\text{col}_{1}(A), B \\cdot \\text{col}_{2}(A)]$ and if $\\text{col}_{2}(A) = 0$ then the second column remains zero. Hence E is invariant.\n\nTherefore, the $T$-invariant subspaces among the given options are D and E.", "answer": "$$\\boxed{DE}$$", "id": "1368900"}, {"introduction": "While we can check if a given subspace is invariant, how do we actively find them? One-dimensional invariant subspaces, which are lines through the origin, are particularly important and provide deep geometric insight. This practice asks you to find the invariant line for a shear transformation in $\\mathbb{R}^2$, revealing the fundamental and powerful connection between one-dimensional invariant subspaces and eigenvectors. [@problem_id:1368921]", "problem": "A linear transformation $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$, which represents a horizontal shear, is defined by the matrix-vector product $T(\\mathbf{v}) = A\\mathbf{v}$, where $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ and the matrix $A$ is given by:\n$$ A = \\begin{pmatrix} 1 & 3 \\\\ 0 & 1 \\end{pmatrix} $$\nA one-dimensional subspace $W$ of $\\mathbb{R}^2$ is a line passing through the origin. Such a subspace is said to be *invariant* under the transformation $T$ if for every vector $\\mathbf{w}$ in $W$, the transformed vector $T(\\mathbf{w})$ is also in $W$.\n\nFind the basis vector for the one-dimensional invariant subspace of this transformation. Express your answer as a column vector $\\begin{pmatrix} a \\\\ b \\end{pmatrix}$ such that the first non-zero component is 1.", "solution": "A one-dimensional subspace $W$ is invariant under $T$ if and only if it is spanned by an eigenvector of $A$. Thus we find the eigenvectors of $A$.\n\nCompute the characteristic polynomial:\n$$\np(\\lambda)=\\det(A-\\lambda I)=\\det\\begin{pmatrix}1-\\lambda & 3 \\\\ 0 & 1-\\lambda\\end{pmatrix}=(1-\\lambda)^{2}.\n$$\nHence the only eigenvalue is $\\lambda=1$.\n\nSolve for the eigenvectors by $(A-I)\\mathbf{v}=\\mathbf{0}$:\n$$\nA-I=\\begin{pmatrix}0 & 3 \\\\ 0 & 0\\end{pmatrix},\\quad \\begin{pmatrix}0 & 3 \\\\ 0 & 0\\end{pmatrix}\\begin{pmatrix}v_{1} \\\\ v_{2}\\end{pmatrix}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}\\implies 3v_{2}=0\\implies v_{2}=0,\n$$\nwith $v_{1}$ free. Therefore the eigenvectors are all nonzero multiples of $\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$. The basis vector with first nonzero component equal to $1$ is\n$$\n\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}.\n$$\nThis corresponds to the $x$-axis, the unique one-dimensional invariant subspace for this shear.", "answer": "$$\\boxed{\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}}$$", "id": "1368921"}, {"introduction": "Abstract theorems in linear algebra are not just for theoretical interest; they are powerful problem-solving tools. This exercise demonstrates a key result: if two operators $S$ and $T$ commute ($ST=TS$), then the kernel of $S$ is invariant under $T$. By first verifying this property, you can find a clever and efficient shortcut to solve what might otherwise seem like a more complex problem. [@problem_id:1368923]", "problem": "Let $V$ be the real vector space $\\mathbb{R}^3$. Consider two linear operators, $S: V \\to V$ and $T: V \\to V$, which are represented by the following matrices with respect to the standard basis:\n$$\nS = \\begin{pmatrix} 1 & -1 & 0 \\\\ 1 & 0 & -1 \\\\ 0 & 1 & -1 \\end{pmatrix}, \\quad T = \\begin{pmatrix} 3 & -1 & 0 \\\\ 1 & 2 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nLet $W$ be the kernel (or null space) of the operator $S$, defined as $W = \\ker(S) = \\{v \\in V \\mid S(v) = 0\\}$. Furthermore, let $U$ be the subspace that results from applying the operator $T$ to every vector in $W$. That is, $U = \\{T(w) \\mid w \\in W\\}$.\n\nWhich one of the following vectors, represented as a column matrix, is NOT an element of the subspace $U$?\n\nA. $\\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$\n\nB. $\\begin{pmatrix} -5 \\\\ -5 \\\\ -5 \\end{pmatrix}$\n\nC. $\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$\n\nD. $\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$\n\nE. $\\begin{pmatrix} \\pi \\\\ \\pi \\\\ \\pi \\end{pmatrix}$", "solution": "The problem asks to identify which of the given vectors is not in the subspace $U = \\{T(w) \\mid w \\in W\\}$, where $W = \\ker(S)$.\n\nA key theorem in linear algebra states that if two linear operators $S$ and $T$ on a vector space $V$ commute, i.e., $ST = TS$, then the kernel of $S$, $\\ker(S)$, is an invariant subspace under $T$. This means that for any vector $w \\in \\ker(S)$, its image under $T$, which is $T(w)$, must also be in $\\ker(S)$.\n\nLet's first verify if the given operators $S$ and $T$ commute by checking if their matrix representations commute.\nWe compute the product $ST$:\n$$\nST = \\begin{pmatrix} 1 & -1 & 0 \\\\ 1 & 0 & -1 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 & -1 & 0 \\\\ 1 & 2 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(3)+(-1)(1)+(0)(0) & (1)(-1)+(-1)(2)+(0)(1) & (1)(0)+(-1)(-1)+(0)(1) \\\\ (1)(3)+(0)(1)+(-1)(0) & (1)(-1)+(0)(2)+(-1)(1) & (1)(0)+(0)(-1)+(-1)(1) \\\\ (0)(3)+(1)(1)+(-1)(0) & (0)(-1)+(1)(2)+(-1)(1) & (0)(0)+(1)(-1)+(-1)(1) \\end{pmatrix}\n$$\n$$\nST = \\begin{pmatrix} 2 & -3 & 1 \\\\ 3 & -2 & -1 \\\\ 1 & 1 & -2 \\end{pmatrix}\n$$\nNext, we compute the product $TS$:\n$$\nTS = \\begin{pmatrix} 3 & -1 & 0 \\\\ 1 & 2 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 0 \\\\ 1 & 0 & -1 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} (3)(1)+(-1)(1)+(0)(0) & (3)(-1)+(-1)(0)+(0)(1) & (3)(0)+(-1)(-1)+(0)(-1) \\\\ (1)(1)+(2)(1)+(-1)(0) & (1)(-1)+(2)(0)+(-1)(1) & (1)(0)+(2)(-1)+(-1)(-1) \\\\ (0)(1)+(1)(1)+(1)(0) & (0)(-1)+(1)(0)+(1)(1) & (0)(0)+(1)(-1)+(1)(-1) \\end{pmatrix}\n$$\n$$\nTS = \\begin{pmatrix} 2 & -3 & 1 \\\\ 3 & -2 & -1 \\\\ 1 & 1 & -2 \\end{pmatrix}\n$$\nSince $ST = TS$, the operators $S$ and $T$ commute.\n\nAccording to the theorem on invariant subspaces, since $S$ and $T$ commute, $W = \\ker(S)$ is a $T$-invariant subspace. This means that for any vector $w \\in W$, the vector $T(w)$ is also in $W$. The set of all such vectors $T(w)$ is the subspace $U$. Therefore, the subspace $U$ must be a subset of the subspace $W$, i.e., $U \\subseteq W$.\n\nThis implies that any vector that is an element of $U$ must also be an element of $W$. Consequently, if we find a vector that is NOT in $W$, it cannot possibly be in $U$. The problem then simplifies to finding the vector among the options that does not belong to $W = \\ker(S)$.\n\nTo find $W = \\ker(S)$, we must find all vectors $v = (x, y, z)^T$ such that $S v = 0$.\n$$\n\\begin{pmatrix} 1 & -1 & 0 \\\\ 1 & 0 & -1 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix equation corresponds to the following system of linear equations:\n1. $x - y = 0 \\implies x = y$\n2. $x - z = 0 \\implies x = z$\n3. $y - z = 0 \\implies y = z$\n\nFrom these equations, we see that $x=y=z$. This means any vector in the kernel of $S$ must be of the form $(c, c, c)^T$ for some scalar $c \\in \\mathbb{R}$. The subspace $W = \\ker(S)$ is the set of all scalar multiples of the vector $(1, 1, 1)^T$.\n$$\nW = \\text{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right\\}\n$$\nNow we examine each option to see if it belongs to $W$. A vector belongs to $W$ if and only if all its components are equal.\n\nA. $\\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. This vector is in $W$.\n\nB. $\\begin{pmatrix} -5 \\\\ -5 \\\\ -5 \\end{pmatrix} = -5 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. This vector is in $W$.\n\nC. $\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. This vector is in $W$.\n\nD. $\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$. The components are not equal ($1 \\neq 2$). This vector is NOT in $W$.\n\nE. $\\begin{pmatrix} \\pi \\\\ \\pi \\\\ \\pi \\end{pmatrix} = \\pi \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. This vector is in $W$.\n\nSince the vector in option D is not in $W$, and we have established that $U \\subseteq W$, it follows that this vector cannot be in $U$. The other options are all in $W$, and could potentially be in $U$. For instance, for option A, $T \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3-1 \\\\ 1+2-1 \\\\ 1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$, so option A is in $U$. Regardless, we are looking for the vector that is definitively NOT in $U$, and D is the only one.", "answer": "$$\\boxed{D}$$", "id": "1368923"}]}