## Introduction
In many scientific and engineering problems, from analyzing the stability of a physical system to finding the minimum of a complex function, the concept of [matrix definiteness](@entry_id:156061) is paramount. It describes whether a [quadratic form](@entry_id:153497) associated with a matrix consistently produces positive, negative, or mixed values. While the definition of a [positive definite matrix](@entry_id:150869)—one that yields a positive value for any non-[zero vector](@entry_id:156189)—is straightforward, verifying this property across an infinite number of vectors is impossible. This presents a significant practical challenge: how can we reliably and efficiently determine if a matrix is definite?

This article introduces Sylvester's criterion, a powerful and elegant algebraic tool that provides a definitive answer. In the following chapters, we will explore this topic in depth.
*   **Principles and Mechanisms** will lay the groundwork by exploring the link between quadratic forms and [symmetric matrices](@entry_id:156259), and then formally state the criterion for positive, negative, and indefinite matrices.
*   **Applications and Interdisciplinary Connections** will demonstrate the criterion's vast utility, showing how it is used to assess stability in physics, optimize functions in engineering, and define geometric structures.
*   Finally, the **Hands-On Practices** chapter offers a series of guided problems to solidify your understanding and build practical skills in applying the criterion to real-world scenarios.

## Principles and Mechanisms

Having established the importance of [matrix definiteness](@entry_id:156061) in the introductory chapter, we now delve into the core principles and mechanisms for its analysis. The central tool we will develop is Sylvester's criterion, a powerful and computationally efficient method for determining the definiteness of a [symmetric matrix](@entry_id:143130). To fully appreciate this criterion, we must first clarify the fundamental connection between quadratic forms and symmetric matrices, as this relationship forms the bedrock upon which all subsequent analysis is built.

### Quadratic Forms and the Associated Symmetric Matrix

A **quadratic form** is a [homogeneous polynomial](@entry_id:178156) of degree two. In $n$ variables $x_1, \dots, x_n$, any such form $q(\mathbf{x})$ can be written as a matrix product $\mathbf{x}^T A \mathbf{x}$, where $\mathbf{x}$ is the column vector of variables and $A$ is an $n \times n$ matrix. For example, the form $q(x_1, x_2) = x_1^2 + 2x_1x_2 + 3x_2^2$ can be generated by the matrix $A = \begin{pmatrix} 1  2 \\ 0  3 \end{pmatrix}$, since:

$$
\mathbf{x}^T A \mathbf{x} = \begin{pmatrix} x_1  x_2 \end{pmatrix} \begin{pmatrix} 1  2 \\ 0  3 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} x_1  2x_1 + 3x_2 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = x_1^2 + (2x_1 + 3x_2)x_2 = x_1^2 + 2x_1x_2 + 3x_2^2
$$

However, this matrix $A$ is not symmetric. A crucial insight is that the properties of a [quadratic form](@entry_id:153497), such as its sign for non-zero vectors $\mathbf{x}$, depend only on the symmetric part of the matrix $A$. For any square matrix $A$, we can decompose it into a symmetric part $S$ and a skew-symmetric part $K$:

$$
A = S + K \quad \text{where} \quad S = \frac{A + A^T}{2} \quad \text{and} \quad K = \frac{A - A^T}{2}
$$

The quadratic form generated by the skew-symmetric part is always zero: $\mathbf{x}^T K \mathbf{x} = 0$. This is because $\mathbf{x}^T K \mathbf{x}$ is a scalar, so it equals its own transpose. Taking the transpose gives $(\mathbf{x}^T K \mathbf{x})^T = \mathbf{x}^T K^T \mathbf{x} = \mathbf{x}^T (-K) \mathbf{x} = -\mathbf{x}^T K \mathbf{x}$. The only scalar equal to its own negative is zero.

Consequently, $\mathbf{x}^T A \mathbf{x} = \mathbf{x}^T (S + K) \mathbf{x} = \mathbf{x}^T S \mathbf{x} + \mathbf{x}^T K \mathbf{x} = \mathbf{x}^T S \mathbf{x}$. This means that any [quadratic form](@entry_id:153497) has a **unique symmetric [matrix representation](@entry_id:143451)**. To analyze the definiteness of a [quadratic form](@entry_id:153497), we must always work with this unique symmetric matrix. For the non-[symmetric matrix](@entry_id:143130) $A = \begin{pmatrix} 1  2 \\ 0  3 \end{pmatrix}$ from our example, the corresponding [symmetric matrix](@entry_id:143130) $S$ is [@problem_id:1391412]:

$$
S = \frac{1}{2} \left( \begin{pmatrix} 1  2 \\ 0  3 \end{pmatrix} + \begin{pmatrix} 1  0 \\ 2  3 \end{pmatrix} \right) = \frac{1}{2} \begin{pmatrix} 2  2 \\ 2  6 \end{pmatrix} = \begin{pmatrix} 1  1 \\ 1  3 \end{pmatrix}
$$

Indeed, $\mathbf{x}^T S \mathbf{x} = x_1^2 + 2x_1x_2 + 3x_2^2$, the same quadratic form. This preliminary step is non-negotiable; criteria for definiteness, including Sylvester's criterion, are formulated exclusively for symmetric (or Hermitian) matrices.

With this established, we formally define definiteness for a symmetric matrix $S$:
-   **Positive definite**: $\mathbf{x}^T S \mathbf{x} > 0$ for all non-zero vectors $\mathbf{x}$.
-   **Positive semidefinite**: $\mathbf{x}^T S \mathbf{x} \ge 0$ for all vectors $\mathbf{x}$.
-   **Negative definite**: $\mathbf{x}^T S \mathbf{x}  0$ for all non-zero vectors $\mathbf{x}$.
-   **Negative semidefinite**: $\mathbf{x}^T S \mathbf{x} \le 0$ for all vectors $\mathbf{x}$.
-   **Indefinite**: $\mathbf{x}^T S \mathbf{x}$ takes both positive and negative values.

### Sylvester's Criterion for Positive Definiteness

While the definition of positive definiteness is conceptually clear, checking it for all infinitely many non-zero vectors $\mathbf{x}$ is impractical. Sylvester's criterion provides a finite and decisive test. The criterion relies on the **[leading principal minors](@entry_id:154227)** of the matrix. For an $n \times n$ matrix $A$, the $k$-th leading principal minor, denoted $D_k$, is the determinant of the top-left $k \times k$ submatrix of $A$.

**Sylvester's Criterion:** A [symmetric matrix](@entry_id:143130) $A$ is [positive definite](@entry_id:149459) if and only if all its [leading principal minors](@entry_id:154227) are strictly positive.
$$
D_1 > 0, \quad D_2 > 0, \quad \dots, \quad D_n > 0
$$

Consider a physical system, such as a crystal lattice, where the potential energy $V$ near an equilibrium point is approximated by a [quadratic form](@entry_id:153497). For the equilibrium to be stable, the potential energy must have a strict [local minimum](@entry_id:143537), which requires the quadratic form to be positive definite. Suppose the potential energy is given by $V(x, y, z) = 2x^2 + 2y^2 + 3z^2 + 2xy - 2xz - 2yz$ [@problem_id:1391422]. The associated symmetric matrix $A$ is:

$$
A = \begin{pmatrix} 2  1  -1 \\ 1  2  -1 \\ -1  -1  3 \end{pmatrix}
$$

To test for positive definiteness, we compute the [leading principal minors](@entry_id:154227):
-   $D_1 = \det(2) = 2$
-   $D_2 = \det \begin{pmatrix} 2  1 \\ 1  2 \end{pmatrix} = (2)(2) - (1)(1) = 3$
-   $D_3 = \det(A) = 2(6-1) - 1(3-1) - 1(-1+2) = 10 - 2 - 1 = 7$

Since $D_1 = 2 > 0$, $D_2 = 3 > 0$, and $D_3 = 7 > 0$, Sylvester's criterion confirms that the matrix $A$ is [positive definite](@entry_id:149459), and thus the equilibrium is stable. This method is also indispensable when dealing with matrices containing parameters. For instance, in analyzing a mechanical system, we might encounter a matrix whose entries depend on a coupling stiffness $k$. By setting the [leading principal minors](@entry_id:154227) to be greater than zero, we can derive the precise range of $k$ that ensures stability [@problem_id:1391408].

### Classifying Negative Definite and Indefinite Matrices

Sylvester's criterion can be adapted to test for negative definiteness. A matrix $A$ is [negative definite](@entry_id:154306) if and only if $-A$ is [positive definite](@entry_id:149459). Applying the criterion to $-A$ yields a condition on the minors of $A$ itself. Since the $k$-th leading principal minor of $-A$ is $(-1)^k D_k$, the condition for positive definiteness of $-A$ (i.e., $(-1)^k D_k > 0$ for all $k$) translates to a condition for negative definiteness of $A$.

**Criterion for Negative Definiteness:** A [symmetric matrix](@entry_id:143130) $A$ is [negative definite](@entry_id:154306) if and only if its [leading principal minors](@entry_id:154227) alternate in sign, starting with a negative value.
$$
D_1  0, \quad D_2 > 0, \quad D_3  0, \quad \dots, \quad (-1)^k D_k > 0
$$

For example, consider the matrix $A = \begin{pmatrix} -2  1  1 \\ 1  -1  1 \\ 1  1  -6 \end{pmatrix}$ [@problem_id:1391450]. Its [leading principal minors](@entry_id:154227) are:
-   $D_1 = -2$
-   $D_2 = \det \begin{pmatrix} -2  1 \\ 1  -1 \end{pmatrix} = 2 - 1 = 1$
-   $D_3 = \det(A) = -1$

Let's check the condition $(-1)^k D_k > 0$:
-   For $k=1$: $(-1)^1 D_1 = (-1)(-2) = 2 > 0$.
-   For $k=2$: $(-1)^2 D_2 = (1)(1) = 1 > 0$.
-   For $k=3$: $(-1)^3 D_3 = (-1)(-1) = 1 > 0$.
All conditions are met, so the matrix is [negative definite](@entry_id:154306).

What if the sequence of minors does not follow the pattern for positive or negative definiteness? This is a strong indicator of indefiniteness. A common misconception is that a positive determinant and positive diagonal entries are sufficient for [positive definiteness](@entry_id:178536). This is false, and it highlights why every leading principal minor must be checked.

Let's construct a counterexample [@problem_id:1391411]. Consider a [symmetric matrix](@entry_id:143130) with $a_{11}=1$ and $a_{22}=1$. The first minor is $D_1=1>0$. To violate the criterion, we can force the second minor to be negative. Let's choose $a_{12}=2$.
$$
D_2 = \det \begin{pmatrix} 1  2 \\ 2  1 \end{pmatrix} = 1 - 4 = -3  0
$$
Because $D_2  0$, the matrix cannot be [positive definite](@entry_id:149459). Since $D_1 > 0$, it also cannot be [negative definite](@entry_id:154306). Thus, regardless of the remaining entries, the matrix is indefinite or negative semidefinite. We can choose the remaining entries to make the full determinant positive, thereby falsifying the conjecture. For instance, the matrix $A = \begin{pmatrix} 1  2  4 \\ 2  1  2 \\ 4  2  1 \end{pmatrix}$ has $D_1=1 > 0$, $D_2=-3  0$, and $D_3 = \det(A)=9 > 0$. It has positive diagonal entries and a positive determinant, yet it is not positive definite. For the vector $\mathbf{x} = (1, -1, 0)^T$, we find $\mathbf{x}^T A \mathbf{x} = -2$, confirming the matrix is indefinite.

### The Boundary Case: Semidefinite Matrices

Sylvester's criterion in its standard form provides a strict test for definiteness. The boundary between definiteness and indefiniteness is occupied by semidefinite matrices, where the quadratic form can be zero for some non-zero vectors. This occurs when the matrix is singular, i.e., $\det(A) = D_n = 0$.

A more general criterion exists for semidefiniteness, but it is more demanding. A symmetric matrix is **positive semidefinite** if and only if *all* of its **principal minors** are non-negative. A principal minor is the determinant of a submatrix formed by selecting the same set of row and column indices (not necessarily the first $k$).

Consider a matrix $A(\alpha)$ depending on a parameter $\alpha$, and we wish to find the value of $\alpha$ for which the matrix is positive semidefinite but not [positive definite](@entry_id:149459) [@problem_id:1386468]. This requires that the matrix be singular, so $\det(A(\alpha)) = 0$, and that all other principal minors be non-negative. For the matrix $A(\alpha) = \begin{pmatrix} 2  1  0 \\ 1  \alpha  -1 \\ 0  -1  3 \end{pmatrix}$, being positive semidefinite requires all principal minors $\ge 0$. The $1 \times 1$ minors are $2, \alpha, 3$, so $\alpha \ge 0$. The $2 \times 2$ minors are $2\alpha-1, 6, 3\alpha-1$, so $\alpha \ge 1/2$. The $3 \times 3$ minor is $\det(A) = 6\alpha - 5$, so $\alpha \ge 5/6$. For the matrix to be positive definite, we need all these to be strictly positive, which requires $\alpha > 5/6$. The boundary case, where it is positive semidefinite but not [positive definite](@entry_id:149459), occurs exactly when the determinant is zero: $6\alpha - 5 = 0$, or $\alpha = 5/6$.

A simple case of semidefiniteness arises if a diagonal entry is zero. For a symmetric matrix $A$, if $a_{ii}=0$, then for the standard basis vector $\mathbf{e}_i$, we have $\mathbf{e}_i^T A \mathbf{e}_i = a_{ii} = 0$. Since we have found a non-[zero vector](@entry_id:156189) for which the quadratic form is zero, the matrix cannot be positive definite or [negative definite](@entry_id:154306). It can, at best, be semidefinite or indefinite [@problem_id:1391439].

### Advanced Topics: The Role of Symmetry and Gram Matrices

The entire framework we have built rests on a single, indispensable property: the matrix must be symmetric. For [non-symmetric matrices](@entry_id:153254), the connection between principal minors and eigenvalue signs completely breaks down. Consider the matrix $A = \begin{pmatrix} 0  1  0  0\\ 0  0  1  0\\ 0  0  0  1\\ 1  0  0  0 \end{pmatrix}$ [@problem_id:2735072]. One can verify that all of its principal minors are non-negative (they are all either 0 or 1). However, its [characteristic polynomial](@entry_id:150909) is $\lambda^4 - 1 = 0$, giving eigenvalues $\{1, -1, i, -i\}$. The presence of the negative eigenvalue $\lambda = -1$ despite non-negative principal minors is a stark demonstration that the theory of definiteness does not apply to [non-symmetric matrices](@entry_id:153254) in the same way. The quadratic form $\mathbf{x}^T S \mathbf{x}$ associated with $A$ is still well-behaved, but the eigenvalues of $A$ and the minors of $A$ are disconnected from it.

Finally, we explore a profound connection between Sylvester's criterion and geometry. In many applications, particularly statistics and machine learning, one encounters matrices of the form $A = X^T X$, known as **Gram matrices**. Here, $X$ is an $m \times n$ matrix, often with $m \ge n$. If the columns of $X$ are linearly independent, the Gram matrix $A$ is guaranteed to be positive definite.

Sylvester's criterion provides a beautiful explanation for this [@problem_id:1391425]. Let $X_k$ be the $m \times k$ matrix consisting of the first $k$ columns of $X$. The $k$-th leading [principal submatrix](@entry_id:201119) of $A=X^T X$ is precisely $X_k^T X_k$. Its determinant, $D_k = \det(X_k^T X_k)$, is the Gram determinant of the first $k$ columns of $X$. Geometrically, this determinant represents the squared $k$-dimensional volume of the parallelepiped spanned by these column vectors. Since the columns of $X$ are linearly independent, any subset of them is also [linearly independent](@entry_id:148207). Therefore, for each $k=1, \dots, n$, the volume is strictly positive, and its square, $D_k$, is also strictly positive. Since all [leading principal minors](@entry_id:154227) are positive, $A=X^T X$ must be [positive definite](@entry_id:149459).

This principle is not just a theoretical curiosity; it is the reason why the normal equations in [linear regression](@entry_id:142318) have a unique solution. The same principle extends to more abstract settings, such as [inner product spaces](@entry_id:271570) of functions, where matrices with entries of the form $M_{ij} = \langle f_i, f_j \rangle$ are positive definite if the functions $\{f_i\}$ are [linearly independent](@entry_id:148207) [@problem_id:1391431]. This connection reveals Sylvester's criterion not just as a computational trick, but as a reflection of a deep geometric property of linear independence.