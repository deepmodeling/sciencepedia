{"hands_on_practices": [{"introduction": "Before we can use the pseudoinverse to solve problems, we must first master the mechanics of calculating it. This first exercise [@problem_id:1400691] focuses on computing the Moore-Penrose pseudoinverse $A^+$ for the common case of a matrix $A$ with full column rank, using the formula $A^+ = (A^T A)^{-1} A^T$. Mastering this calculation provides the foundational tool for finding least-squares solutions to the overdetermined systems that frequently arise in data analysis and engineering.", "problem": "An experimental physicist is studying a newly discovered electronic component. They hypothesize that the relationship between the applied voltage $V$ and the resulting current $I$ follows a model of the form $V = c_1 + c_2 I^2$, where $c_1$ and $c_2$ are unknown device-specific parameters. To determine these parameters, the physicist takes three measurements of voltage for three different currents. The measured currents are $I_1=1$, $I_2=2$, and $I_3=3$ (in some consistent units).\n\nThis set of measurements leads to an overdetermined linear system of equations of the form $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$ contains the unknown parameters and $A$ is the design matrix derived from the current values. The least-squares solution for the parameters can be found using the Moore-Penrose pseudoinverse $A^+$ of the matrix $A$.\n\nGiven the model and the current values used, determine the Moore-Penrose pseudoinverse $A^+$ of the design matrix $A$.", "solution": "The problem asks for the Moore-Penrose pseudoinverse of the design matrix $A$ associated with the model $V = c_1 + c_2 I^2$ and the measurements taken at currents $I=1, 2, 3$.\n\nFirst, we construct the system of linear equations. Each measurement $(I_i, V_i)$ gives one equation:\nFor $I_1=1$: $c_1 \\cdot 1 + c_2 \\cdot 1^2 = V_1 \\implies c_1 + c_2 = V_1$\nFor $I_2=2$: $c_1 \\cdot 1 + c_2 \\cdot 2^2 = V_2 \\implies c_1 + 4c_2 = V_2$\nFor $I_3=3$: $c_1 \\cdot 1 + c_2 \\cdot 3^2 = V_3 \\implies c_1 + 9c_2 = V_3$\n\nThis system can be written in matrix form $A\\mathbf{x} = \\mathbf{b}$ as:\n$$\n\\begin{pmatrix} 1 & 1 \\\\ 1 & 4 \\\\ 1 & 9 \\end{pmatrix}\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\n=\n\\begin{pmatrix} V_1 \\\\ V_2 \\\\ V_3 \\end{pmatrix}\n$$\nThe design matrix $A$ is therefore:\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 4 \\\\ 1 & 9 \\end{pmatrix}\n$$\nThe matrix $A$ is a $3 \\times 2$ matrix. Its columns are linearly independent, so it has full column rank. For a matrix $A$ with full column rank, its Moore-Penrose pseudoinverse $A^+$ is given by the formula:\n$$\nA^+ = (A^T A)^{-1} A^T\n$$\nWe will compute this in steps.\n\nStep 1: Compute the transpose of $A$, which is $A^T$.\n$$\nA^T = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 4 & 9 \\end{pmatrix}\n$$\n\nStep 2: Compute the product $A^T A$.\n$$\nA^T A = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 4 & 9 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 4 \\\\ 1 & 9 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(1)(1)+(1)(1) & (1)(1)+(1)(4)+(1)(9) \\\\ (1)(1)+(4)(1)+(9)(1) & (1)(1)+(4)(4)+(9)(9) \\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix} 1+1+1 & 1+4+9 \\\\ 1+4+9 & 1+16+81 \\end{pmatrix} = \\begin{pmatrix} 3 & 14 \\\\ 14 & 98 \\end{pmatrix}\n$$\n\nStep 3: Compute the inverse of $A^T A$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $A^T A$ is:\n$$\n\\det(A^T A) = (3)(98) - (14)(14) = 294 - 196 = 98\n$$\nThe inverse is:\n$$\n(A^T A)^{-1} = \\frac{1}{98} \\begin{pmatrix} 98 & -14 \\\\ -14 & 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{98}{98} & -\\frac{14}{98} \\\\ -\\frac{14}{98} & \\frac{3}{98} \\end{pmatrix} = \\begin{pmatrix} 1 & -\\frac{1}{7} \\\\ -\\frac{1}{7} & \\frac{3}{98} \\end{pmatrix}\n$$\n\nStep 4: Compute the final product $A^+ = (A^T A)^{-1} A^T$.\n$$\nA^+ = \\begin{pmatrix} 1 & -\\frac{1}{7} \\\\ -\\frac{1}{7} & \\frac{3}{98} \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 4 & 9 \\end{pmatrix}\n$$\nWe perform the matrix multiplication entry by entry:\nEntry (1,1): $(1)(1) + (-\\frac{1}{7})(1) = 1 - \\frac{1}{7} = \\frac{6}{7}$\nEntry (1,2): $(1)(1) + (-\\frac{1}{7})(4) = 1 - \\frac{4}{7} = \\frac{3}{7}$\nEntry (1,3): $(1)(1) + (-\\frac{1}{7})(9) = 1 - \\frac{9}{7} = -\\frac{2}{7}$\n\nEntry (2,1): $(-\\frac{1}{7})(1) + (\\frac{3}{98})(1) = -\\frac{14}{98} + \\frac{3}{98} = -\\frac{11}{98}$\nEntry (2,2): $(-\\frac{1}{7})(1) + (\\frac{3}{98})(4) = -\\frac{14}{98} + \\frac{12}{98} = -\\frac{2}{98} = -\\frac{1}{49}$\nEntry (2,3): $(-\\frac{1}{7})(1) + (\\frac{3}{98})(9) = -\\frac{14}{98} + \\frac{27}{98} = \\frac{13}{98}$\n\nAssembling these entries into the matrix $A^+$ gives the final result.\n$$\nA^+ = \\begin{pmatrix} \\frac{6}{7} & \\frac{3}{7} & -\\frac{2}{7} \\\\ -\\frac{11}{98} & -\\frac{1}{49} & \\frac{13}{98} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{6}{7} & \\frac{3}{7} & -\\frac{2}{7} \\\\ -\\frac{11}{98} & -\\frac{1}{49} & \\frac{13}{98} \\end{pmatrix}\n}\n$$", "id": "1400691"}, {"introduction": "With the ability to compute the pseudoinverse, we can now apply it to its most common task: finding the best possible solution to an inconsistent system of equations. This practice [@problem_id:1400693] simulates a real-world scenario where measurement errors make a perfect fit impossible. By setting up and solving the system, you will see how the least-squares solution $\\mathbf{x}^+ = A^+\\mathbf{b}$ yields the optimal parameters for a model by minimizing the discrepancy between its predictions and the observed data.", "problem": "An engineer is calibrating a novel sensor whose output, $S$, is modeled as a linear function of two adjustable input parameters, $p_1$ and $p_2$. The theoretical model is given by the equation $S = x_1 p_1 + x_2 p_2$, where $x_1$ and $x_2$ are the unknown calibration constants that need to be determined. To find these constants, a series of three experiments are conducted, yielding the following measurements:\n\n1.  With $(p_1, p_2) = (1, 0)$, the sensor output was $S=1$.\n2.  With $(p_1, p_2) = (0, 1)$, the sensor output was $S=2$.\n3.  With $(p_1, p_2) = (1, 1)$, the sensor output was $S=2$.\n\nDue to measurement noise, this set of observations forms an overdetermined and inconsistent system of linear equations for the constants $x_1$ and $x_2$. Determine the values of $x_1$ and $x_2$ that constitute the least-squares solution to this system.\n\nExpress your answer as a column vector $\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$. The entries in your vector must be given as exact fractions.", "solution": "The model is $S = x_{1} p_{1} + x_{2} p_{2}$. Collecting the three experiments into matrix form yields\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix},\\quad\n\\boldsymbol{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix},\\quad\n\\boldsymbol{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix},\n$$\nso the system is $A \\boldsymbol{x} = \\boldsymbol{b}$, which is overdetermined. The least-squares solution minimizes $\\|A \\boldsymbol{x} - \\boldsymbol{b}\\|^{2}$ and satisfies the normal equations\n$$\nA^{\\top} A \\boldsymbol{x} = A^{\\top} \\boldsymbol{b}.\n$$\nCompute\n$$\nA^{\\top} A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix},\n\\quad\nA^{\\top} \\boldsymbol{b} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}.\n$$\nThus, the normal equations are\n$$\n\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix},\n$$\ni.e.,\n$$\n\\begin{cases}\n2 x_{1} + x_{2} = 3, \\\\\nx_{1} + 2 x_{2} = 4.\n\\end{cases}\n$$\nFrom the first equation, $x_{2} = 3 - 2 x_{1}$. Substitute into the second:\n$$\nx_{1} + 2(3 - 2 x_{1}) = 4 \\;\\Rightarrow\\; x_{1} + 6 - 4 x_{1} = 4 \\;\\Rightarrow\\; -3 x_{1} = -2 \\;\\Rightarrow\\; x_{1} = \\frac{2}{3}.\n$$\nThen\n$$\nx_{2} = 3 - 2 \\cdot \\frac{2}{3} = 3 - \\frac{4}{3} = \\frac{5}{3}.\n$$\nTherefore, the least-squares solution is\n$$\n\\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{5}{3} \\end{pmatrix}}$$", "id": "1400693"}, {"introduction": "The least-squares method always gives us an answer, but how do we know if this answer is a perfect fit or just the best approximation? This final practice [@problem_id:1400688] delves into the theory behind the pseudoinverse, connecting it to the geometric concept of projection. You will identify the precise condition involving the projector $XX^+$ that determines if the observation vector $\\mathbf{y}$ lies entirely within the column space of the data matrix $X$, which is the necessary and sufficient condition for the least-squares solution to also be an exact one.", "problem": "A data scientist is working with a linear regression model to predict an outcome variable based on a set of features. The model is represented by the matrix equation $X\\beta = y$, where $X$ is an $m \\times n$ matrix of feature data for $m$ observations and $n$ features, $\\beta$ is an $n \\times 1$ vector of unknown model parameters, and $y$ is an $m \\times 1$ vector of the observed outcomes.\n\nTypically, the number of observations $m$ is greater than the number of features $n$, making the system of equations overdetermined. In such cases, an exact solution for $\\beta$ that perfectly satisfies the equation for all observations does not usually exist. The standard practice is to find the least-squares solution, denoted $\\beta_{ls}$, which minimizes the sum of squared errors, $\\|X\\beta - y\\|^2$. This unique solution is given by $\\beta_{ls} = X^{+}y$, where $X^{+}$ is the Moore-Penrose pseudoinverse of $X$.\n\nThe data scientist wants to establish a definitive test to check if, for a particular set of observations $y$, the model provides a perfect fit. A perfect fit means the least-squares solution is also an exact solution, satisfying $X\\beta_{ls} = y$ with zero error.\n\nWhich one of the following mathematical conditions, involving the matrices $X$, $X^{+}$, and the vector $y$, is both a necessary and sufficient condition for the least-squares solution to be an exact solution?\n\nA. $X^{+}X y = y$\n\nB. $X X^{+} y = y$\n\nC. $X^{T} y = \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector.\n\nD. $X^{+} y = \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector.\n\nE. The determinant of $X^T X$ is non-zero.", "solution": "We are given a linear system $X\\beta=y$ with $X\\in\\mathbb{R}^{m\\times n}$, $\\beta\\in\\mathbb{R}^{n}$, $y\\in\\mathbb{R}^{m}$. The least-squares solution is $\\beta_{ls}=X^{+}y$, where $X^{+}$ is the Moore-Penrose pseudoinverse of $X$. The corresponding fitted vector is\n$$\nX\\beta_{ls}=X X^{+} y.\n$$\nBy the defining properties of the Moore-Penrose pseudoinverse, the matrix $P:=X X^{+}$ is the orthogonal projector onto the column space $\\mathcal{R}(X)$, satisfying\n$$\nP^{2}=P,\\quad P^{T}=P,\\quad \\mathcal{R}(P)=\\mathcal{R}(X).\n$$\nThe least-squares residual is\n$$\nr:=y - X\\beta_{ls}=y - X X^{+} y=(I - X X^{+})y.\n$$\nA perfect fit means zero residual, i.e.,\n$$\nr=\\mathbf{0}\\quad\\Longleftrightarrow\\quad (I - X X^{+})y=\\mathbf{0}\\quad\\Longleftrightarrow\\quad X X^{+} y = y.\n$$\nTherefore, $X\\beta_{ls}=y$ if and only if $X X^{+} y = y$. Equivalently, $y\\in\\mathcal{R}(X)$, since $X X^{+}$ is the orthogonal projector onto $\\mathcal{R}(X)$.\n\nWe now assess the options:\n- Option B: $X X^{+} y = y$ is exactly the necessary and sufficient condition derived above.\n- Option A: $X^{+}X y = y$ is generally not even dimensionally valid, since $X^{+}X$ is $n\\times n$ while $y\\in\\mathbb{R}^{m}$; thus it is not a valid general condition.\n- Option C: $X^{T}y=\\mathbf{0}$ means $y\\in\\mathcal{R}(X)^{\\perp}$. A perfect fit would then require $y\\in\\mathcal{R}(X)\\cap\\mathcal{R}(X)^{\\perp}$, which implies $y=\\mathbf{0}$; thus this is neither necessary nor sufficient in general.\n- Option D: $X^{+}y=\\mathbf{0}$ implies $\\beta_{ls}=\\mathbf{0}$. For a perfect fit this would force $y=X\\mathbf{0}=\\mathbf{0}$; again neither necessary nor sufficient in general.\n- Option E: $\\det(X^{T}X)\\neq 0$ asserts full column rank, which ensures uniqueness of the least-squares solution but does not guarantee $y\\in\\mathcal{R}(X)$; hence it is neither necessary nor sufficient for a perfect fit.\n\nThus the necessary and sufficient condition is $X X^{+} y = y$.", "answer": "$$\\boxed{B}$$", "id": "1400688"}]}