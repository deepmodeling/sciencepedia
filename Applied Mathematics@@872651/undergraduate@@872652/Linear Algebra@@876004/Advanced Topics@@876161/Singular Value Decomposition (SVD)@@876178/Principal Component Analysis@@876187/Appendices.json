{"hands_on_practices": [{"introduction": "To truly master an algorithm, it is essential to first understand its fundamental mechanics. This exercise demystifies Principal Component Analysis by guiding you through the complete calculation by hand. By working with a small, manageable dataset, you will perform the core steps of centering the data, computing the sample covariance matrix, and determining the first principal component loading vector through eigendecomposition [@problem_id:2416060]. This foundational practice will build a concrete intuition for how PCA identifies the primary axis of variation within a dataset.", "problem": "A gene expression experiment measured log-transformed expression values for $G_1$, $G_2$, and $G_3$ across $S_1$, $S_2$, $S_3$, and $S_4$. The data matrix $X \\in \\mathbb{R}^{4 \\times 3}$ has samples as rows and genes as columns:\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\nTreat samples as independent observations and genes as variables. Using principal component analysis (PCA), compute the first principal component loading vector in gene space by:\n- column-centering $X$,\n- forming the sample covariance matrix across genes with denominator $n-1$ for $n=4$ samples, and\n- taking the unit-norm eigenvector of this covariance matrix corresponding to the largest eigenvalue.\n\nReport the loading vector as a $1 \\times 3$ row matrix ordered as $(G_1, G_2, G_3)$, with the sign chosen so that its first nonzero entry is positive. No rounding is required.", "solution": "We are asked to compute the first principal component loading vector in gene space using the eigen-decomposition of the sample covariance matrix across genes. Let $n=4$ be the number of samples and $p=3$ be the number of genes. The data matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows as samples and columns as genes.\n\nStep $1$: Column-centering. Compute the column means of $X$:\n$$\n\\bar{x}_{\\cdot 1} = \\frac{1+2+3+4}{4} = 2.5,\\quad\n\\bar{x}_{\\cdot 2} = \\frac{2+3+4+5}{4} = 3.5,\\quad\n\\bar{x}_{\\cdot 3} = \\frac{3+4+5+6}{4} = 4.5.\n$$\nSubtract these means from each column to obtain the centered matrix $Z$:\n$$\nZ = X - \\mathbf{1}\\bar{x}^{\\top}\n=\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\nStep $2$: Sample covariance matrix across genes. Using the denominator $n-1=3$, the sample covariance of genes is\n$$\nS = \\frac{1}{n-1}\\, Z^{\\top} Z = \\frac{1}{3}\\, Z^{\\top} Z.\n$$\nObserve that each row of $Z$ is a scalar multiple of $(1,\\,1,\\,1)$, so all three centered gene columns are identical. Compute $Z^{\\top}Z$ by noting that for any two columns $j$ and $k$, the $(j,k)$ entry equals $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$. Since all three columns are identical, every entry of $Z^{\\top}Z$ equals the sum of squares of a single centered column:\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} = (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} = 2.25 + 0.25 + 0.25 + 2.25 = 5.\n$$\nTherefore,\n$$\nZ^{\\top} Z = 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS = \\frac{1}{3} Z^{\\top}Z = \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\nStep $3$: Eigen-decomposition. Let $J \\in \\mathbb{R}^{3 \\times 3}$ denote the all-ones matrix, i.e., $J_{jk}=1$ for all $j,k$. It is known from first principles that $J$ has rank $1$ with eigenvalues $3$ and $0$ (with multiplicity $2$). A corresponding unit-norm eigenvector for the eigenvalue $3$ is proportional to $(1,\\,1,\\,1)^{\\top}$, specifically:\n$$\nv = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nSince $S = \\frac{5}{3} J$, the eigenvalues of $S$ are $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ and $\\lambda_{2} = 0$, $\\lambda_{3} = 0$, with the same eigenvectors as $J$. Thus, the first principal component loading vector in gene space is the unit-norm eigenvector associated with $\\lambda_{1}=5$, namely $v$ as above. Choosing the sign so that the first nonzero entry is positive is already satisfied by $v$.\n\nTherefore, ordered as $(G_1, G_2, G_3)$ and written as a $1 \\times 3$ row matrix, the first principal component loading vector is:\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2416060"}, {"introduction": "With the mechanics established, we can now explore the theoretical underpinnings that make PCA so powerful. This practice uses a idealized two-variable system to reveal the direct relationship between the correlation of variables and the structure of the principal components. By deriving the proportion of variance explained by the first principal component as a function of the correlation coefficient $\\rho$, you will gain a deeper conceptual understanding of PCA's core objective: to efficiently capture the shared variance among variables [@problem_id:1946278].", "problem": "An autonomous environmental monitoring drone uses a pair of identical sensors to measure atmospheric pressure. Let the readings of the two sensors, after being centered by subtracting their long-term average, be represented by the random variables $X_1$ and $X_2$.\n\nThe joint behavior of these readings is described by a bivariate random vector $(X_1, X_2)$ with a covariance matrix $\\Sigma$. Because the sensors are of the same type and subject to similar environmental fluctuations, they have the same variance, $\\text{Var}(X_1) = \\text{Var}(X_2) = \\sigma^2$, for some constant $\\sigma > 0$. Their readings are also correlated, with a correlation coefficient $\\rho$ such that $0 < \\rho < 1$. The covariance matrix is therefore given by:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma^2 & \\rho\\sigma^2 \\\\ \\rho\\sigma^2 & \\sigma^2 \\end{pmatrix}\n$$\nTo reduce data redundancy and identify the primary axis of variation, the engineering team applies Principal Component Analysis (PCA). PCA transforms the original correlated variables $(X_1, X_2)$ into a new set of uncorrelated variables, known as principal components. The first principal component is defined as the linear combination of $X_1$ and $X_2$ that captures the maximum possible variance.\n\nDetermine the proportion of the total variance in the data that is explained by the first principal component. Express your answer as a symbolic expression in terms of $\\rho$.", "solution": "We are given a centered bivariate random vector with covariance matrix:\n$$\n\\Sigma = \\begin{pmatrix}\\sigma^{2} & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}\\end{pmatrix}\n$$\nwhere $0<\\rho<1$ and $\\sigma>0$. In PCA, the variances of the principal components are the eigenvalues of the covariance matrix. The proportion of total variance explained by the first principal component equals its eigenvalue divided by the total variance, which is the trace of $\\Sigma$.\n\nFirst, we compute the eigenvalues of $\\Sigma$ by solving the characteristic equation $\\det(\\Sigma-\\lambda I)=0$:\n$$\n\\det\\begin{pmatrix}\\sigma^{2}-\\lambda & \\rho\\sigma^{2} \\\\ \\rho\\sigma^{2} & \\sigma^{2}-\\lambda\\end{pmatrix} = (\\sigma^{2}-\\lambda)^{2}-(\\rho\\sigma^{2})^{2}=0\n$$\nThis gives us:\n$$\n(\\sigma^{2}-\\lambda)^{2} = \\rho^{2}\\sigma^{4}\n$$\nTaking the square root of both sides:\n$$\n\\sigma^{2}-\\lambda = \\pm\\rho\\sigma^{2}\n$$\nSolving for $\\lambda$:\n$$\n\\lambda = \\sigma^{2} \\mp \\rho\\sigma^{2} = \\sigma^{2}(1 \\mp \\rho)\n$$\nThe two eigenvalues are $\\lambda = \\sigma^{2}(1+\\rho)$ and $\\lambda = \\sigma^{2}(1-\\rho)$. Since $0<\\rho<1$, the largest eigenvalue is:\n$$\n\\lambda_{1}=\\sigma^{2}(1+\\rho)\n$$\nThe total variance equals the trace of $\\Sigma$:\n$$\n\\text{Tr}(\\Sigma)=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2}\n$$\nThis is also equal to the sum of the eigenvalues: $\\sigma^{2}(1+\\rho)+\\sigma^{2}(1-\\rho)=2\\sigma^{2}$.\n\nTherefore, the proportion of total variance explained by the first principal component is:\n$$\n\\frac{\\lambda_{1}}{\\text{Tr}(\\Sigma)}=\\frac{\\sigma^{2}(1+\\rho)}{2\\sigma^{2}}=\\frac{1+\\rho}{2}\n$$", "answer": "$$\\boxed{\\frac{1+\\rho}{2}}$$", "id": "1946278"}, {"introduction": "Bridging theory and practice is crucial for effective data analysis, and PCA is no exception. This computational exercise confronts a critical pitfall encountered in real-world applications: the sensitivity of PCA to the scale of the variables. By generating synthetic data and comparing the results of PCA on raw versus standardized data, you will witness firsthand how variables with large, arbitrary units can dominate and distort the analysis [@problem_id:2421735]. This powerful demonstration highlights why standardization is a vital preprocessing step for obtaining meaningful results from PCA.", "problem": "You are asked to demonstrate, using first principles of Principal Component Analysis (PCA), how failing to standardize variables measured in different units can distort the estimated principal directions and the explained variance. Work in a purely mathematical framework with a synthetic data-generating process that models typical financial variables such as prices and volumes. You will implement the full pipeline and report quantitative diagnostics that compare PCA on raw data versus PCA on standardized data.\n\nFundamental base:\n- PCA seeks orthonormal directions that maximize sample variance. Given a centered data matrix $X \\in \\mathbb{R}^{T \\times n}$, the sample covariance matrix is $\\Sigma = \\frac{1}{T-1} X^\\top X$. The principal components are the eigenvectors of $\\Sigma$ corresponding to its eigenvalues, ordered from largest to smallest.\n- Standardization transforms each variable $x_j$ to $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$, where $\\bar{x}_j$ is the sample mean and $\\hat{\\sigma}_j$ is the sample standard deviation, so that each standardized variable has unit sample variance. PCA on standardized data is PCA on the sample correlation matrix.\n- Diagonal rescaling $D = \\operatorname{diag}(s_1,\\dots,s_n)$ applied to variables, $X \\mapsto X D$, multiplies the covariance entries by $s_i s_j$, thereby altering eigenvectors unless all $s_j$ are equal.\n\nData-generating process:\n- For each test case $k$, fix $T_k \\in \\mathbb{N}$, number of variables $n_k \\in \\mathbb{N}$, factor loadings $b^{(k)} \\in \\mathbb{R}^{n_k}$, idiosyncratic standard deviations $u^{(k)} \\in \\mathbb{R}^{n_k}$, and unit scales $s^{(k)} \\in \\mathbb{R}^{n_k}$.\n- Generate a single common factor $f_t \\sim \\mathcal{N}(0,1)$ for $t = 1,\\dots,T_k$, and idiosyncratic noises $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$, all mutually independent across $t$ and $j$.\n- Construct raw observations $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$ for $t=1,\\dots,T_k$ and $j=1,\\dots,n_k$.\n- Center each column of $X$ by subtracting its sample mean before computing any covariance.\n\nComputation tasks per test case:\n- Compute the sample covariance matrix $\\Sigma_{\\text{raw}}$ from the centered raw data $X$ and obtain the first principal component eigenvector $v_{\\text{raw}}$ (unit norm) and its eigenvalue $\\lambda_{\\text{raw}}$.\n- Standardize each column of $X$ to unit sample variance to obtain $Z$, compute $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$ (the sample correlation matrix), and obtain the first principal component eigenvector $v_{\\text{std}}$ (unit norm) and its eigenvalue $\\lambda_{\\text{std}}$.\n- Compute the angle $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$; report $\\theta$ in radians.\n- Compute the difference in explained variance shares as $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\text{Tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\text{Tr}(\\Sigma_{\\text{std}})} \\right|$, which must be reported as a decimal fraction (not a percentage).\n\nRandomness and reproducibility:\n- Use a fixed pseudorandom number generator seed equal to $314159$ for the entire experiment to ensure reproducible results.\n\nTest suite:\n- There are $3$ test cases. For each test case $k$, use the following parameters $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$:\n  - Case $1$ (similar units, two variables):\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$.\n  - Case $2$ (mismatched units, two variables: one dominates by scale):\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$.\n  - Case $3$ (mismatched units, three variables: one huge-scale, one moderate, one tiny-scale):\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$.\n\nRequired outputs per test case:\n- A list of two floats $[\\theta, \\Delta]$ where $\\theta$ is the angle in radians and $\\Delta$ is the absolute difference in explained variance shares. Both values must be rounded to exactly $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the per-case lists, enclosed in square brackets, for example, $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$, with each float rounded to exactly $6$ decimal places and angles in radians.", "solution": "The problem presented is a valid and well-posed exercise in computational statistics, specifically demonstrating the sensitivity of Principal Component Analysis (PCA) to the scaling of variables. It is scientifically sound, resting on foundational principles of linear algebra and statistics, and all parameters and procedures are specified with sufficient clarity to permit a unique, verifiable solution. We will proceed with the analysis.\n\nThe central thesis is that PCA, as a variance-maximization technique, is not scale-invariant. When variables are measured in disparate units (e.g., a stock price in dollars versus its trading volume in millions of shares), the variable with the largest variance will mechanically dominate the first principal component. This is often an artifact of the units chosen rather than an indicator of true underlying importance. Standardization is the standard remedy, transforming all variables to a common scale (unit variance) so that the analysis focuses on the correlation structure of the data, not the arbitrary measurement scales.\n\nWe begin by formalizing the data generation and analysis pipeline.\n\n**1. Data-Generating Process**\n\nFor each test case $k$, we are given a sample size $T_k$, number of variables $n_k$, factor loadings $b^{(k)} \\in \\mathbb{R}^{n_k}$, idiosyncratic standard deviations $u^{(k)} \\in \\mathbb{R}^{n_k}$, and unit scales $s^{(k)} \\in \\mathbb{R}^{n_k}$.\n\nThe data are generated from a single-factor model. A common latent factor $f_t$ is drawn from a standard normal distribution, $f_t \\sim \\mathcal{N}(0, 1)$, for each time point $t=1, \\dots, T_k$. For each variable $j=1, \\dots, n_k$, an idiosyncratic noise term $e_{t,j}$ is drawn from $\\mathcal{N}(0, (u^{(k)}_j)^2)$. All $f_t$ and $e_{t,j}$ are mutually independent.\n\nThe observed value for variable $j$ at time $t$ is constructed as:\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\nThis forms a data matrix $X \\in \\mathbb{R}^{T_k \\times n_k}$ whose columns represent the different variables. The scale factor $s^{(k)}_j$ represents the arbitrary unit of measurement for variable $j$.\n\n**2. PCA on Raw Data (Covariance-Based PCA)**\n\nThe first step in PCA is to center the data by subtracting the column-wise sample mean. Let $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ be the sample mean of the $j$-th variable. The centered data matrix, denoted $X_c$, has entries $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$.\n\nThe sample covariance matrix $\\Sigma_{\\text{raw}}$ is then computed:\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\nThe principal components are the eigenvectors of $\\Sigma_{\\text{raw}}$. We perform an eigendecomposition of this matrix:\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\nwhere $V$ is the matrix of orthonormal eigenvectors and $\\Lambda$ is the diagonal matrix of corresponding eigenvalues. The eigenvalues are sorted in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$. The first principal component is the eigenvector $v_1$ associated with the largest eigenvalue $\\lambda_1$. For this problem, we denote this eigenvector as $v_{\\text{raw}}$ and the eigenvalue as $\\lambda_{\\text{raw}}$.\n\n**3. PCA on Standardized Data (Correlation-Based PCA)**\n\nTo remove the effect of arbitrary scaling, we standardize the data. For each column $j$ of the original data matrix $X$, we compute its sample standard deviation, $\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$.\n\nThe standardized data matrix $Z$ is constructed with entries:\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\nBy construction, each column of $Z$ has a sample mean of $0$ and a sample variance of $1$.\n\nPCA is then performed on this standardized data $Z$. The relevant matrix is the sample covariance matrix of $Z$, which we denote $\\Sigma_{\\text{std}}$:\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\nSince each column of $Z$ has unit variance, the diagonal elements of $\\Sigma_{\\text{std}}$ are all $1$, and the off-diagonal elements $(i, j)$ are the sample correlation coefficients between the original variables $x_i$ and $x_j$. Thus, $\\Sigma_{\\text{std}}$ is the sample correlation matrix of $X$.\n\nWe perform an eigendecomposition of $\\Sigma_{\\text{std}}$ to find its largest eigenvalue, $\\lambda_{\\text{std}}$, and the corresponding eigenvector, $v_{\\text{std}}$.\n\n**4. Diagnostic Metrics**\n\nTo quantify the distortion caused by failing to standardize, we compute two metrics:\n\n- **Angle between Principal Components**: The principal component directions $v_{\\text{raw}}$ and $v_{\\text{std}}$ are unit vectors in $\\mathbb{R}^{n_k}$. The angle between them measures how much the direction of maximum variance shifts. Since eigenvectors are defined only up to a sign (i.e., if $v$ is an eigenvector, so is $-v$), we compute the acute angle between the lines they span:\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  A value of $\\theta=0$ indicates perfect alignment, while a large angle (approaching $\\pi/2$) indicates severe misalignment.\n\n- **Difference in Explained Variance Share**: The fraction of total variance explained by the first principal component is given by its eigenvalue divided by the sum of all eigenvalues. The sum of eigenvalues is equal to the trace of the matrix, $\\text{Tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$, which represents the total variance in the data. We compute the absolute difference in the explained variance share:\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\text{Tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\text{Tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  Note that for standardized data, $\\text{Tr}(\\Sigma_{\\text{std}}) = n_k$, the number of variables. A large $\\Delta$ indicates that the two methods give a very different assessment of the importance of the first component.\n\nThe procedure will be executed for each test case using the specified parameters and a fixed random seed for reproducibility. The results are expected to show minimal distortion for Case $1$ (similar scales) and significant distortion for Cases $2$ and $3$ (disparate scales), validating the necessity of standardization in practice.", "answer": "$$\\boxed{\\texttt{[[0.038487,0.000330],[0.816641,0.499097],[1.242784,0.665979]]}}$$", "id": "2421735"}]}