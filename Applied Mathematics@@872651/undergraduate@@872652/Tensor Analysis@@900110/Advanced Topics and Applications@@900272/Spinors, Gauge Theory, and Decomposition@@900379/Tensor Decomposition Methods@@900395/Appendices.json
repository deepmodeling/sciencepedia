{"hands_on_practices": [{"introduction": "The CANDECOMP/PARAFAC (CP) decomposition models a tensor as a sum of rank-one components, which are themselves constructed from a set of factor matrices. The column-wise Khatri-Rao product is the essential operation used to combine these factor matrices and form the CP model. This exercise provides fundamental practice in computing this product, a skill that is crucial for understanding how CP decompositions are represented and implemented [@problem_id:1542398].", "problem": "In tensor analysis, the column-wise Khatri-Rao product is a fundamental operation used in various tensor decomposition methods, such as CANDECOMP/PARAFAC (CP) decomposition.\n\nConsider two matrices, $B$ and $C$, with real-valued entries, given by:\n$$\nB = \\begin{pmatrix} 1 & 5 \\\\ 2 & 0 \\\\ 3 & 1 \\end{pmatrix}\n$$\nand\n$$\nC = \\begin{pmatrix} 4 & 2 \\\\ 6 & 3 \\\\ 7 & 8 \\end{pmatrix}\n$$\n\nCompute their column-wise Khatri-Rao product, denoted as $A = C \\odot B$.", "solution": "The column-wise Khatri-Rao product of two matrices with the same number of columns is defined as follows: if $B \\in \\mathbb{R}^{I \\times K}$ and $C \\in \\mathbb{R}^{J \\times K}$, then $C \\odot B \\in \\mathbb{R}^{IJ \\times K}$ has columns given by $(C \\odot B)_{:, k} = c_{k} \\otimes b_{k}$ for $k = 1, \\ldots, K$, where $\\otimes$ denotes the Kronecker product.\n\nGiven\n$$\nB = \\begin{pmatrix} 1 & 5 \\\\ 2 & 0 \\\\ 3 & 1 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 4 & 2 \\\\ 6 & 3 \\\\ 7 & 8 \\end{pmatrix},\n$$\nboth have $K = 2$ columns, so $A = C \\odot B \\in \\mathbb{R}^{9 \\times 2}$.\n\nCompute the first column using $c_{1} = \\begin{pmatrix} 4 \\\\ 6 \\\\ 7 \\end{pmatrix}$ and $b_{1} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$:\n$$\nc_{1} \\otimes b_{1} = \\begin{pmatrix} 4 b_{1} \\\\ 6 b_{1} \\\\ 7 b_{1} \\end{pmatrix}\n= \\begin{pmatrix} 4 \\\\ 8 \\\\ 12 \\\\ 6 \\\\ 12 \\\\ 18 \\\\ 7 \\\\ 14 \\\\ 21 \\end{pmatrix}.\n$$\n\nCompute the second column using $c_{2} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 8 \\end{pmatrix}$ and $b_{2} = \\begin{pmatrix} 5 \\\\ 0 \\\\ 1 \\end{pmatrix}$:\n$$\nc_{2} \\otimes b_{2} = \\begin{pmatrix} 2 b_{2} \\\\ 3 b_{2} \\\\ 8 b_{2} \\end{pmatrix}\n= \\begin{pmatrix} 10 \\\\ 0 \\\\ 2 \\\\ 15 \\\\ 0 \\\\ 3 \\\\ 40 \\\\ 0 \\\\ 8 \\end{pmatrix}.\n$$\n\nStacking these as columns gives\n$$\nA = C \\odot B = \\begin{pmatrix}\n4 & 10 \\\\\n8 & 0 \\\\\n12 & 2 \\\\\n6 & 15 \\\\\n12 & 0 \\\\\n18 & 3 \\\\\n7 & 40 \\\\\n14 & 0 \\\\\n21 & 8\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\n4 & 10 \\\\\n8 & 0 \\\\\n12 & 2 \\\\\n6 & 15 \\\\\n12 & 0 \\\\\n18 & 3 \\\\\n7 & 40 \\\\\n14 & 0 \\\\\n21 & 8\n\\end{pmatrix}}$$", "id": "1542398"}, {"introduction": "The rank of a tensor is a fundamental property, but unlike matrix rank, it can be very difficult to compute. For certain cases, however, we can use simple tests to establish a lower bound on the rank. This problem demonstrates a straightforward criterion for a symmetric $2 \\times 2 \\times 2$ tensor, providing a tangible way to prove that a tensor's rank must be greater than one and illustrating that not all tensors can be simplified to a single component [@problem_id:1542423].", "problem": "In the field of data analytics, the CANDECOMP/PARAFAC (CP) decomposition is a fundamental tool for analyzing multi-dimensional datasets, which are mathematically represented as tensors. A third-order tensor $\\mathcal{T}$ is an array of numbers with three indices, $T_{ijk}$. A tensor is symmetric if its components are invariant under any permutation of their indices (e.g., $T_{ijk} = T_{jik} = T_{kji}$).\n\nThe CP rank of a tensor is the minimum number of rank-1 tensors required to represent it as a sum. For a symmetric $2 \\times 2 \\times 2$ tensor $\\mathcal{T}$ (where indices $i, j, k$ can be 1 or 2), a CP rank of 1 means it can be expressed as the outer product of a single vector $\\mathbf{v} = (v_1, v_2)$ with itself three times, i.e., $T_{ijk} = v_i v_j v_k$.\n\nA key property for any rank-1 symmetric $2 \\times 2 \\times 2$ tensor is that its unique components must satisfy the relation $T_{111} T_{222} = T_{112} T_{122}$. Therefore, a non-zero value for the quantity $D = T_{111} T_{222} - T_{112} T_{122}$ serves as a direct indicator that the tensor's rank is greater than 1.\n\nConsider a symmetric tensor $\\mathcal{S}$ that models a three-way interaction in a physical system. Its unique non-zero components are given by $S_{111} = 2$, $S_{112} = 1$, $S_{122} = 1$, and $S_{222} = 8$.\n\nCalculate the value of the quantity $D = S_{111} S_{222} - S_{112} S_{122}$ for this tensor $\\mathcal{S}$.", "solution": "We are given a symmetric $2 \\times 2 \\times 2$ tensor $\\mathcal{S}$ with unique non-zero components $S_{111} = 2$, $S_{112} = 1$, $S_{122} = 1$, and $S_{222} = 8$. The quantity to compute is defined by\n$$\nD = S_{111} S_{222} - S_{112} S_{122}.\n$$\nSubstituting the given values,\n$$\nD = (2)(8) - (1)(1).\n$$\nEvaluate each product:\n$$\n(2)(8) = 16, \\quad (1)(1) = 1.\n$$\nSubtract to obtain\n$$\nD = 16 - 1 = 15.\n$$\nTherefore, the value of $D$ for the tensor $\\mathcal{S}$ is $15$.", "answer": "$$\\boxed{15}$$", "id": "1542423"}, {"introduction": "Tensor decompositions are not just theoretical constructs; they are found using iterative algorithms that progressively refine an approximation. This exercise bridges theory and computation by guiding you to formulate the core update rule for the tensor power method. This powerful algorithm, a higher-order extension of the well-known matrix power method, is used to find the dominant rank-1 component of a symmetric tensor, offering a glimpse into the computational engines that drive modern tensor analysis [@problem_id:1542377].", "problem": "In higher-order data analysis, a 3rd-order tensor $T \\in \\mathbb{R}^{d \\times d \\times d}$ can model three-way relationships within a dataset. We consider a tensor $T$ that is fully symmetric, meaning its components are invariant under any permutation of their indices, i.e., $T_{ijk} = T_{ikj} = T_{jik}$, etc.\n\nA fundamental task is to find the dominant rank-1 component of $T$. This involves finding the best approximation of $T$ by a rank-1 tensor of the form $\\lambda(u \\otimes u \\otimes u)$, where $u \\in \\mathbb{R}^d$ is a unit vector ($||u||_2=1$), $\\lambda$ is a scalar, and $\\otimes$ denotes the outer product. The components of this rank-1 tensor are given by $(\\lambda(u \\otimes u \\otimes u))_{ijk} = \\lambda u_i u_j u_k$.\n\nThe vector $u$ that provides the best approximation is one that maximizes the objective function $f(u) = \\sum_{i,j,k=1}^{d} T_{ijk} u_i u_j u_k$, subject to the constraint $||u||_2 = 1$. The stationary points of this constrained optimization problem satisfy the nonlinear eigenvalue equation $T(u,u) = \\lambda u$, where $T(u,u)$ is a vector whose $i$-th component is defined by the double contraction $(T(u,u))_i = \\sum_{j,k=1}^{d} T_{ijk} u_j u_k$.\n\nThis equation inspires a simple iterative algorithm, analogous to the matrix power method, to find the \"dominant\" eigenvector $u$. Starting with a random initial unit vector $u_0$, the algorithm generates a sequence of vectors via the two-step process:\n1.  Compute an unnormalized vector for the next step: $v_{k+1} = \\text{operation}(T, u_k)$\n2.  Normalize the vector: $u_{k+1} = \\frac{v_{k+1}}{||v_{k+1}||_2}$\n\nBased on the structure of the nonlinear eigenvalue equation $T(u,u) = \\lambda u$, determine the expression for the unnormalized vector $v_{k+1}$ in step 1. Express the $i$-th component of this vector, $(v_{k+1})_i$, in terms of the components of the tensor $T$ (e.g., $T_{ijl}$) and the components of the vector from the previous step, $u_k$ (e.g., $(u_k)_j$).", "solution": "We seek the dominant rank-1 component of a fully symmetric tensor $T \\in \\mathbb{R}^{d \\times d \\times d}$ by maximizing $f(u) = \\sum_{i,j,k=1}^{d} T_{ijk} u_{i} u_{j} u_{k}$ subject to the constraint $\\|u\\|_{2} = 1$. The stationary points of this constrained optimization satisfy the nonlinear eigenvalue equation $T(u,u) = \\lambda u$, where $T(u,u)$ denotes the double contraction of $T$ with $u$ in two modes and has components\n$$\n\\big(T(u,u)\\big)_{i} = \\sum_{j=1}^{d} \\sum_{k=1}^{d} T_{ijk} u_{j} u_{k}.\n$$\nThis follows from the Lagrangian $L(u,\\mu) = f(u) - \\mu(u^{\\top}u - 1)$, whose gradient condition gives $\\nabla f(u) = 2 \\mu u$. Using symmetry of $T$, the gradient of $f$ is\n$$\n\\frac{\\partial f}{\\partial u_{i}} = 3 \\sum_{j=1}^{d} \\sum_{k=1}^{d} T_{ijk} u_{j} u_{k} = 3 \\big(T(u,u)\\big)_{i},\n$$\nleading to $T(u,u) = \\lambda u$ with $\\lambda = \\frac{2}{3}\\mu$. \n\nA power-method-like iteration mirrors this eigen-equation by evaluating its left-hand side at the current iterate $u_{k}$ and then normalizing. Therefore, the unnormalized update is the double contraction of $T$ with $u_{k}$:\n$$\nv_{k+1} = T(u_{k}, u_{k}),\n$$\nwhose $i$-th component is obtained by contracting over the second and third indices:\n$$\n(v_{k+1})_{i} = \\sum_{j=1}^{d} \\sum_{l=1}^{d} T_{ijl} (u_{k})_{j} (u_{k})_{l}.\n$$\nBecause $T$ is fully symmetric, the choice of which two indices to contract is immaterial, and the expression above is the standard form for the unnormalized update.", "answer": "$$\\boxed{\\sum_{j=1}^{d}\\sum_{l=1}^{d} T_{ijl}\\,(u_{k})_{j}\\,(u_{k})_{l}}$$", "id": "1542377"}]}