{"hands_on_practices": [{"introduction": "The concept of a martingale transform, or a discrete stochastic integral, is foundational to quantitative finance, where it models the cumulative gains from a dynamic trading strategy. This first exercise invites you to build this concept from the ground up in the context of a simple symmetric random walk. By computing the expectation of a stopped transform using only first principles, you will uncover the core reason why these transforms are martingales—a property that elegantly captures the notion of a 'fair game' [@problem_id:3065398].", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{n})_{n\\geq 0},\\mathbb{P})$ be a filtered probability space carrying a simple symmetric random walk $(S_{n})_{n\\geq 0}$ defined by $S_{0}=0$ and $S_{n}=\\sum_{k=1}^{n}X_{k}$, where $(X_{k})_{k\\geq 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_{k}=1)=\\mathbb{P}(X_{k}=-1)=\\tfrac{1}{2}$ and $X_{k}$ is $\\mathcal{F}_{k}$-measurable for each $k$. Let $(H_{n})_{n\\geq 1}$ be a bounded predictable process with respect to $(\\mathcal{F}_{n})_{n\\geq 0}$, meaning $H_{n}$ is $\\mathcal{F}_{n-1}$-measurable for each $n$ and there exists a finite constant $C0$ such that $|H_{n}|\\leq C$ almost surely for all $n$. Define the discrete-time stochastic integral (martingale transform) by\n$$(H\\cdot S)_{n}\\;=\\;\\sum_{k=1}^{n}H_{k}\\,(S_{k}-S_{k-1})\\;=\\;\\sum_{k=1}^{n}H_{k}\\,X_{k}.$$\nLet $\\tau$ be a bounded stopping time with respect to $(\\mathcal{F}_{n})_{n\\geq 0}$, that is, there exists a finite deterministic $T\\in\\mathbb{N}$ such that $\\tau\\leq T$ almost surely. For a fixed $n\\in\\mathbb{N}$, compute the quantity $\\mathbb{E}\\big[(H\\cdot S)_{\\tau\\wedge n}\\big]$ using only fundamental properties of conditional expectation and the definitions above, and explain how your computation is consistent with the optional stopping theorem (OST) for martingales under bounded stopping times. Your final answer must be a single closed-form expression.", "solution": "The problem asks for the computation of the quantity $\\mathbb{E}\\big[(H\\cdot S)_{\\tau\\wedge n}\\big]$ for a fixed $n\\in\\mathbb{N}$. We are to use fundamental properties of conditional expectation and relate the result to the optional stopping theorem (OST).\n\nLet us denote the discrete-time stochastic integral, or martingale transform, by $M_n = (H\\cdot S)_n$. The problem defines this as\n$$M_n = \\sum_{k=1}^{n}H_{k}\\,(S_{k}-S_{k-1}) = \\sum_{k=1}^{n}H_{k}\\,X_{k}.$$\nBy definition, the sum is empty for $n=0$, so $M_0 = 0$. We wish to compute $\\mathbb{E}[M_{\\tau \\wedge n}]$.\n\nFirst, we perform the computation directly using fundamental properties. The term $M_{\\tau \\wedge n}$ is the value of the process $M$ at the stopped time $\\tau \\wedge n$. The sum can be written as:\n$$M_{\\tau \\wedge n} = \\sum_{k=1}^{\\tau \\wedge n} H_k X_k.$$\nTo handle the random upper limit of the summation, we can rewrite the sum using indicator functions up to the deterministic time $n$. An equivalent expression for the stopped sum is:\n$$M_{\\tau \\wedge n} = \\sum_{k=1}^{n} H_k X_k \\mathbb{I}_{\\{k \\le \\tau\\}}.$$\nTo verify this identity, consider any sample path $\\omega \\in \\Omega$. Let $t = \\tau(\\omega)$.\nCase 1: $t \\ge n$. Then $\\tau(\\omega) \\wedge n = n$. The left side is $\\sum_{k=1}^{n} H_k(\\omega) X_k(\\omega)$. On the right side, for $k \\in \\{1, \\dots, n\\}$, we have $k \\le n \\le t = \\tau(\\omega)$, so $\\mathbb{I}_{\\{k \\le \\tau(\\omega)\\}} = 1$. The right side becomes $\\sum_{k=1}^{n} H_k(\\omega) X_k(\\omega)$, matching the left side.\nCase 2: $t  n$. Then $\\tau(\\omega) \\wedge n = t$. The left side is $\\sum_{k=1}^{t} H_k(\\omega) X_k(\\omega)$. On the right side, for $k \\le t$, $\\mathbb{I}_{\\{k \\le \\tau(\\omega)\\}} = 1$. For $k  t$, $\\mathbb{I}_{\\{k \\le \\tau(\\omega)\\}} = 0$. So the sum on the right side effectively stops at $t$, becoming $\\sum_{k=1}^{t} H_k(\\omega) X_k(\\omega)$, which again matches the left side.\nThe identity is thus established.\n\nNow we can compute the expectation:\n$$\\mathbb{E}[M_{\\tau \\wedge n}] = \\mathbb{E}\\left[\\sum_{k=1}^{n} H_k X_k \\mathbb{I}_{\\{k \\le \\tau\\}}\\right].$$\nBy linearity of expectation, which applies since the sum is finite, we can write:\n$$\\mathbb{E}[M_{\\tau \\wedge n}] = \\sum_{k=1}^{n} \\mathbb{E}\\left[H_k X_k \\mathbb{I}_{\\{k \\le \\tau\\}}\\right].$$\nWe analyze each term in the summation using the tower property of conditional expectation:\n$$\\mathbb{E}\\left[H_k X_k \\mathbb{I}_{\\{k \\le \\tau\\}}\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[H_k X_k \\mathbb{I}_{\\{k \\le \\tau\\}} \\mid \\mathcal{F}_{k-1}\\right]\\right].$$\nThe process $(H_n)_{n \\ge 1}$ is predictable, which means by definition that $H_k$ is $\\mathcal{F}_{k-1}$-measurable for each $k \\ge 1$.\nThe indicator function $\\mathbb{I}_{\\{k \\le \\tau\\}}$ is also $\\mathcal{F}_{k-1}$-measurable. This is because the event $\\{k \\le \\tau\\}$ is the complement of the event $\\{\\tau  k\\}$. The event $\\{\\tau  k\\}$ is equivalent to $\\bigcup_{j=0}^{k-1} \\{\\tau=j\\}$. Since $\\tau$ is a stopping time, the event $\\{\\tau=j\\}$ is in $\\mathcal{F}_j$ for each $j$. As $\\mathcal{F}_j \\subseteq \\mathcal{F}_{k-1}$ for $j  k$, the union $\\bigcup_{j=0}^{k-1} \\{\\tau=j\\}$ is an element of $\\mathcal{F}_{k-1}$. Therefore, $\\{\\tau  k\\} \\in \\mathcal{F}_{k-1}$, and so is its complement $\\{k \\le \\tau\\}$.\n\nSince both $H_k$ and $\\mathbb{I}_{\\{k \\le \\tau\\}}$ are $\\mathcal{F}_{k-1}$-measurable, we can pull them out of the inner conditional expectation:\n$$\\mathbb{E}\\left[\\mathbb{E}\\left[H_k X_k \\mathbb{I}_{\\{k \\le \\tau\\}} \\mid \\mathcal{F}_{k-1}\\right]\\right] = \\mathbb{E}\\left[H_k \\mathbb{I}_{\\{k \\le \\tau\\}} \\mathbb{E}\\left[X_k \\mid \\mathcal{F}_{k-1}\\right]\\right].$$\nThe random variables $(X_k)_{k \\ge 1}$ are independent. Thus, $X_k$ is independent of the filtration $\\mathcal{F}_{k-1}$, which is generated by $X_1, \\ldots, X_{k-1}$. This implies that the conditional expectation of $X_k$ given $\\mathcal{F}_{k-1}$ is simply its unconditional expectation:\n$$\\mathbb{E}[X_k \\mid \\mathcal{F}_{k-1}] = \\mathbb{E}[X_k].$$\nThe distribution of $X_k$ is given as $\\mathbb{P}(X_k=1) = \\mathbb{P}(X_k=-1) = \\frac{1}{2}$. Thus, its expectation is:\n$$\\mathbb{E}[X_k] = 1 \\cdot \\mathbb{P}(X_k=1) + (-1) \\cdot \\mathbb{P}(X_k=-1) = 1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{2} = 0.$$\nSubstituting this back, we get for each term in the sum:\n$$\\mathbb{E}\\left[H_k X_k \\mathbb{I}_{\\{k \\le \\tau\\}}\\right] = \\mathbb{E}\\left[H_k \\mathbb{I}_{\\{k \\le \\tau\\}} \\cdot 0\\right] = \\mathbb{E}[0] = 0.$$\nSince every term in the summation is zero, the total sum is zero:\n$$\\mathbb{E}[M_{\\tau \\wedge n}] = \\sum_{k=1}^{n} 0 = 0.$$\n\nThis result is consistent with the optional stopping theorem. To see this, we first establish that the process $(M_n)_{n \\geq 0}$ is a martingale with respect to the filtration $(\\mathcal{F}_n)_{n \\geq 0}$.\n1.  **Adaptedness**: For each $n$, $M_n = \\sum_{k=1}^{n} H_k X_k$. $H_k$ is $\\mathcal{F}_{k-1}$-measurable and $X_k$ is $\\mathcal{F}_k$-measurable. Thus, their product $H_k X_k$ is $\\mathcal{F}_k$-measurable. Since $\\mathcal{F}_k \\subseteq \\mathcal{F}_n$ for $k \\le n$, each term in the sum is $\\mathcal{F}_n$-measurable, and so is $M_n$.\n2.  **Integrability**: Since $|H_k| \\le C$ and $|X_k|=1$ almost surely, we have $|M_n| = |\\sum_{k=1}^n H_k X_k| \\le \\sum_{k=1}^n |H_k||X_k| \\le \\sum_{k=1}^n C = nC$. Thus, $\\mathbb{E}[|M_n|] \\le nC  \\infty$.\n3.  **Martingale property**: We must show $\\mathbb{E}[M_n \\mid \\mathcal{F}_{n-1}] = M_{n-1}$ for $n \\ge 1$.\n    $$\\mathbb{E}[M_n \\mid \\mathcal{F}_{n-1}] = \\mathbb{E}[M_{n-1} + H_n X_n \\mid \\mathcal{F}_{n-1}].$$\n    By linearity and since $M_{n-1}$ is $\\mathcal{F}_{n-1}$-measurable:\n    $$\\mathbb{E}[M_n \\mid \\mathcal{F}_{n-1}] = M_{n-1} + \\mathbb{E}[H_n X_n \\mid \\mathcal{F}_{n-1}].$$\n    As $H_n$ is $\\mathcal{F}_{n-1}$-measurable and $X_n$ is independent of $\\mathcal{F}_{n-1}$ with $\\mathbb{E}[X_n]=0$:\n    $$\\mathbb{E}[H_n X_n \\mid \\mathcal{F}_{n-1}] = H_n \\mathbb{E}[X_n \\mid \\mathcal{F}_{n-1}] = H_n \\mathbb{E}[X_n] = H_n \\cdot 0 = 0.$$\n    Therefore, $\\mathbb{E}[M_n \\mid \\mathcal{F}_{n-1}] = M_{n-1}$, confirming that $(M_n)_{n \\ge 0}$ is a martingale.\n\nThe optional stopping theorem for bounded stopping times states that if $(M_n)_{n \\ge 0}$ is a martingale and $\\sigma$ is a bounded stopping time, then $\\mathbb{E}[M_\\sigma] = \\mathbb{E}[M_0]$.\nIn our problem, the stopping time is $\\sigma = \\tau \\wedge n$. Since $\\tau$ is a stopping time and $n$ is a constant, $\\sigma$ is also a stopping time. We are given that $\\tau \\le T$ for a deterministic constant $T$, so $\\sigma = \\tau \\wedge n \\le n$. This means $\\sigma$ is a bounded stopping time.\nApplying the OST, we get:\n$$\\mathbb{E}[M_{\\tau \\wedge n}] = \\mathbb{E}[M_0].$$\nThe process starts at $M_0 = (H \\cdot S)_0 = \\sum_{k=1}^0 H_k X_k = 0$.\nSo, the OST gives:\n$$\\mathbb{E}[M_{\\tau \\wedge n}] = 0.$$\nThis confirms that our direct calculation, which also yielded $0$, is consistent with the optional stopping theorem.", "answer": "$$\\boxed{0}$$", "id": "3065398"}, {"introduction": "Having established that martingale transforms represent fair games, a natural impulse is to apply the powerful Optional Stopping Theorem (OST), which relates the value of a martingale at a later time to its initial value. This practice serves as a critical stress test for that impulse, exploring scenarios where the theorem applies perfectly and where it fails spectacularly [@problem_id:3065401]. By contrasting a stopping time with a finite expectation against one that is almost surely finite but has an infinite expectation, you will develop a sharp intuition for the subtle but essential conditions that govern the use of this cornerstone theorem.", "problem": "Consider a discrete-time Symmetric Simple Random Walk (SSRW) defined by $S_0=0$ and $S_n=\\sum_{k=1}^n X_k$, where $(X_k)_{k\\ge 1}$ are independent and identically distributed random variables with $\\mathbb{P}(X_k=1)=\\mathbb{P}(X_k=-1)=\\tfrac{1}{2}$. Let $(\\mathcal{F}_n)_{n\\ge 0}$ be the natural filtration generated by $(X_k)_{1\\le k\\le n}$. Define the stopping times\n- $T_a=\\inf\\{n\\ge 0:\\lvert S_n\\rvert=a\\}$ for a fixed integer $a\\ge 1$, and\n- $T_1=\\inf\\{n\\ge 1:S_n=1\\}$.\n\nLet the predictable process $(H_k)_{k\\ge 1}$ be given by $H_k=2S_{k-1}$, and consider the martingale transform\n$$M_n=\\sum_{k=1}^n H_k X_k=2\\sum_{k=1}^n S_{k-1}X_k=S_n^2-n.$$\n\nUse the fundamental definitions of martingale and predictable process, and a valid form of the Optional Stopping Theorem (OST) for bounded stopping times, to reason about the expectations $\\mathbb{E}[S_{T_a}]$, $\\mathbb{E}[T_a]$, and what happens when one replaces $T_a$ by the recurrence-based $T_1$. Select all statements that are true.\n\nA. $(S_n)_{n\\ge 0}$ is a martingale, and applying the Optional Stopping Theorem (OST) to $T_a$ yields $\\mathbb{E}[S_{T_a}]=0$.\n\nB. $(M_n)_{n\\ge 0}$ is a martingale transform of $(S_n)_{n\\ge 0}$, and applying OST to $T_a$ gives $\\mathbb{E}[T_a]=a^2$.\n\nC. $T_1$ is almost surely finite; therefore, applying OST to $(S_n)_{n\\ge 0}$ at $T_1$ gives $\\mathbb{E}[S_{T_1}]=0$.\n\nD. $T_1$ is almost surely finite but satisfies $\\mathbb{E}[T_1]=\\infty$, so OST may fail; in fact, $\\mathbb{E}[S_{T_1}]=1\\ne 0$.\n\nE. Applying OST to $(M_n)_{n\\ge 0}$ at $T_1$ yields $\\mathbb{E}[T_1]=1$, since $S_{T_1}^2=1$.", "solution": "The problem statement is analyzed for validity as a mandatory first step.\n\n**Step 1: Extract Givens**\n- **Process**: A discrete-time Symmetric Simple Random Walk (SSRW) is defined by $S_0=0$ and $S_n=\\sum_{k=1}^n X_k$ for $n \\ge 1$.\n- **Increments**: $(X_k)_{k\\ge 1}$ are independent and identically distributed (i.i.d.) random variables with $\\mathbb{P}(X_k=1)=\\mathbb{P}(X_k=-1)=\\tfrac{1}{2}$.\n- **Filtration**: $(\\mathcal{F}_n)_{n\\ge 0}$ is the natural filtration generated by $(X_k)_{1\\le k\\le n}$.\n- **Stopping Times**:\n    - $T_a=\\inf\\{n\\ge 0:\\lvert S_n\\rvert=a\\}$ for a fixed integer $a\\ge 1$.\n    - $T_1=\\inf\\{n\\ge 1:S_n=1\\}$.\n- **Predictable Process**: $(H_k)_{k\\ge 1}$ is given by $H_k=2S_{k-1}$.\n- **Martingale Transform**: $M_n=\\sum_{k=1}^n H_k X_k=2\\sum_{k=1}^n S_{k-1}X_k$.\n- **Identity**: $M_n=S_n^2-n$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined within the standard mathematical framework of stochastic processes. All terms are standard.\n- The processes $(S_n)_{n\\ge0}$ and $(M_n)_{n\\ge0}$ are well-known martingales. For $(S_n)$, we have $\\mathbb{E}[X_k]=0$, so $\\mathbb{E}[S_{n+1}|\\mathcal{F}_n] = S_n + \\mathbb{E}[X_{n+1}|\\mathcal{F}_n] = S_n$. For $(M_n=S_n^2-n)$, $\\mathbb{E}[M_{n+1}|\\mathcal{F}_n] = \\mathbb{E}[S_{n+1}^2-(n+1)|\\mathcal{F}_n] = \\mathbb{E}[(S_n+X_{n+1})^2-n-1|\\mathcal{F}_n] = S_n^2 + 2S_n\\mathbb{E}[X_{n+1}] + \\mathbb{E}[X_{n+1}^2]-n-1 = S_n^2+0+1-n-1=S_n^2-n=M_n$.\n- The stopping times $T_a$ and $T_1$ are standard first passage times.\n- The identity $M_n=S_n^2-n$ is correct, as shown by the martingale calculation above or by direct summation: $S_k^2-S_{k-1}^2 = (S_{k-1}+X_k)^2-S_{k-1}^2=2S_{k-1}X_k+X_k^2=2S_{k-1}X_k+1$. Summing from $k=1$ to $n$ gives $S_n^2-S_0^2 = \\sum_{k=1}^n(2S_{k-1}X_k+1)$, which simplifies to $S_n^2 = 2\\sum_{k=1}^n S_{k-1}X_k + n$, confirming the identity.\n- The problem is scientifically grounded, well-posed, and objective.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution proceeds by analyzing each option.\n\n**Analysis of Option A**\nStatement: $(S_n)_{n\\ge 0}$ is a martingale, and applying the Optional Stopping Theorem (OST) to $T_a$ yields $\\mathbb{E}[S_{T_a}]=0$.\n- The process $(S_n)_{n\\ge 0}$ is a martingale with respect to its natural filtration $(\\mathcal{F}_n)_{n\\ge 0}$ because the increments have zero mean: $\\mathbb{E}[S_{n+1}|\\mathcal{F}_n] = \\mathbb{E}[S_n+X_{n+1}|\\mathcal{F}_n] = S_n + \\mathbb{E}[X_{n+1}] = S_n+0 = S_n$.\n- To apply the Optional Stopping Theorem (OST), we must verify its conditions for the stopping time $T_a$. While $T_a$ is not bounded, it is a well-known result for a $1$-dimensional SSRW that the first passage time to any level has a finite expectation. So, $\\mathbb{E}[T_a]\\infty$.\n- One form of OST states that if $(Y_n)$ is a martingale, $T$ is a stopping time with $\\mathbb{E}[T]\\infty$, and the increments of the martingale are uniformly bounded (i.e., $|Y_n-Y_{n-1}| \\le C$ for some constant C), then $\\mathbb{E}[Y_T]=\\mathbb{E}[Y_0]$.\n- For our martingale $(S_n)$, the increments are $|S_n-S_{n-1}|=|X_n|=1$, which are uniformly bounded by $C=1$. Since $\\mathbb{E}[T_a]\\infty$, the conditions of this version of OST are met.\n- Therefore, we can conclude that $\\mathbb{E}[S_{T_a}] = \\mathbb{E}[S_0]$. Since $S_0=0$, we get $\\mathbb{E}[S_{T_a}]=0$.\n- By definition of $T_a$, $S_{T_a}$ can only take values $a$ or $-a$. The result $\\mathbb{E}[S_{T_a}]=0$ implies that $\\mathbb{P}(S_{T_a}=a)=\\mathbb{P}(S_{T_a}=-a)=\\tfrac{1}{2}$, which is expected from the symmetry of the walk.\n- The statement is therefore correct.\n\nVerdict for A: **Correct**.\n\n**Analysis of Option B**\nStatement: $(M_n)_{n\\ge 0}$ is a martingale transform of $(S_n)_{n\\ge 0}$, and applying OST to $T_a$ gives $\\mathbb{E}[T_a]=a^2$.\n- The process $(M_n=S_n^2-n)$ is a martingale, as verified during validation. It is also a martingale transform as stated.\n- To apply OST to $(M_n)$ and $T_a$, we check the conditions. The increments $|M_n-M_{n-1}| = |2S_{n-1}X_n| = 2|S_{n-1}|$ are not uniformly bounded, so the previous version of OST does not apply.\n- A more general approach is required, as hinted by the problem's reference to bounded stopping times. We use OST for the bounded stopping time $T_a \\wedge n = \\min(T_a, n)$ for any $n \\ge 1$. For any bounded stopping time $\\tau$, $\\mathbb{E}[M_{\\tau}]=\\mathbb{E}[M_0]=0$.\n- Applying this, $\\mathbb{E}[M_{T_a \\wedge n}] = 0$. Substituting the definition of $M_n$, we have $\\mathbb{E}[S_{T_a \\wedge n}^2 - (T_a \\wedge n)] = 0$, which implies $\\mathbb{E}[S_{T_a \\wedge n}^2] = \\mathbb{E}[T_a \\wedge n]$.\n- We can now take the limit as $n \\to \\infty$.\n  - For the right-hand side, since $T_a \\wedge n$ is a non-decreasing sequence of non-negative random variables converging to $T_a$, the Monotone Convergence Theorem gives $\\lim_{n\\to\\infty} \\mathbb{E}[T_a \\wedge n] = \\mathbb{E}[\\lim_{n\\to\\infty} (T_a \\wedge n)] = \\mathbb{E}[T_a]$.\n  - For the left-hand side, for any $k \\le T_a$, we have $|S_k|a$, and at $k=T_a$, $|S_{T_a}|=a$. Thus, for all $n$, $|S_{T_a \\wedge n}| \\le a$. This means that the sequence of random variables $(S_{T_a \\wedge n}^2)_{n\\ge 1}$ is uniformly bounded by $a^2$. Since $T_a$ is almost surely finite for a $1$D SSRW, $S_{T_a \\wedge n} \\to S_{T_a}$ almost surely. By the Bounded Convergence Theorem, we can interchange limit and expectation: $\\lim_{n\\to\\infty} \\mathbb{E}[S_{T_a \\wedge n}^2] = \\mathbb{E}[\\lim_{n\\to\\infty} S_{T_a \\wedge n}^2] = \\mathbb{E}[S_{T_a}^2]$.\n- By definition of $T_a$, $|S_{T_a}|=a$, so $S_{T_a}^2 = a^2$. Thus, $\\mathbb{E}[S_{T_a}^2] = \\mathbb{E}[a^2] = a^2$.\n- Equating the limits of both sides gives $\\mathbb{E}[T_a] = a^2$. This result is known as Wald's second identity. The reasoning is sound.\n- The statement is therefore correct.\n\nVerdict for B: **Correct**.\n\n**Analysis of Option C**\nStatement: $T_1$ is almost surely finite; therefore, applying OST to $(S_n)_{n\\ge 0}$ at $T_1$ gives $\\mathbb{E}[S_{T_1}]=0$.\n- For a $1$D SSRW, it is a classical result that the walk is recurrent, meaning it visits every integer with probability $1$. Thus, $T_1 = \\inf\\{n\\ge 1:S_n=1\\}$ is almost surely finite, i.e., $\\mathbb{P}(T_1\\infty)=1$. The premise is correct.\n- However, the logical connective \"therefore\" implies that $\\mathbb{P}(T_1\\infty)=1$ is a sufficient condition to apply OST. This is false.\n- As discussed for option A, sufficient conditions for $\\mathbb{E}[S_T]=\\mathbb{E}[S_0]$ include $T$ being bounded, or $\\mathbb{E}[T]\\infty$ (given bounded increments), or uniform integrability of the stopped process.\n- For $T_1$, none of these conditions are met. In particular, it is another classical result that for a $1$D SSRW, the expected time to reach any state $k\\neq 0$ is infinite. Thus, $\\mathbb{E}[T_1]=\\infty$.\n- Since the conditions for OST are not met, its conclusion is not guaranteed. In fact, by the very definition of $T_1$, we have $S_{T_1}=1$. Therefore, $\\mathbb{E}[S_{T_1}] = \\mathbb{E}[1]=1$. This contradicts the statement's conclusion that $\\mathbb{E}[S_{T_1}]=0$.\n- The statement contains a fallacious implication and a false conclusion.\n\nVerdict for C: **Incorrect**.\n\n**Analysis of Option D**\nStatement: $T_1$ is almost surely finite but satisfies $\\mathbb{E}[T_1]=\\infty$, so OST may fail; in fact, $\\mathbb{E}[S_{T_1}]=1\\ne 0$.\n- This statement accurately summarizes the situation with the martingale $(S_n)$ and the stopping time $T_1$.\n- As established in the analysis of C, $T_1$ is almost surely finite.\n- As established in the analysis of C, $\\mathbb{E}[T_1]=\\infty$ for a $1$D SSRW.\n- The fact that $\\mathbb{E}[T_1]=\\infty$ violates a key condition for the simplest non-trivial version of OST, so the conclusion of OST is not guaranteed to hold (\"OST may fail\").\n- As a matter of fact, $\\mathbb{E}[S_{T_1}]=1$, since $S_{T_1}=1$ with probability $1$.\n- Since $\\mathbb{E}[S_0]=0$, we have $\\mathbb{E}[S_{T_1}]=1 \\ne 0 = \\mathbb{E}[S_0]$, which explicitly demonstrates that OST does fail in this case.\n- Every part of this statement is correct.\n\nVerdict for D: **Correct**.\n\n**Analysis of Option E**\nStatement: Applying OST to $(M_n)_{n\\ge 0}$ at $T_1$ yields $\\mathbb{E}[T_1]=1$, since $S_{T_1}^2=1$.\n- This statement describes the result of a formal, but invalid, application of OST to the martingale $M_n = S_n^2-n$ with stopping time $T_1$.\n- If OST were applicable, we would have $\\mathbb{E}[M_{T_1}] = \\mathbb{E}[M_0] = 0$.\n- This would mean $\\mathbb{E}[S_{T_1}^2 - T_1] = 0$, or $\\mathbb{E}[S_{T_1}^2] - \\mathbb{E}[T_1] = 0$.\n- Since $S_{T_1}=1$, we have $S_{T_1}^2=1$, and $\\mathbb{E}[S_{T_1}^2]=1$.\n- The equation would become $1 - \\mathbb{E}[T_1] = 0$, yielding the result $\\mathbb{E}[T_1]=1$.\n- However, we know from fundamental theory (and as stated in option D) that $\\mathbb{E}[T_1]=\\infty$.\n- The conclusion $\\mathbb{E}[T_1]=1$ is false. This contradiction proves that the initial assumption—that OST is applicable to $(M_n)$ and $T_1$—must be false.\n- A theorem cannot be \"applied\" if its hypotheses are not satisfied. Stating that an invalid application \"yields\" a certain result is a mathematically incorrect statement.\n\nVerdict for E: **Incorrect**.\n\n**Summary of Conclusions**\n- A: Correct.\n- B: Correct.\n- C: Incorrect.\n- D: Correct.\n- E: Incorrect.\n\nThe correct options are A, B, and D.", "answer": "$$\\boxed{ABD}$$", "id": "3065401"}, {"introduction": "We now transition from the discrete steps of a random walk to the continuous paths of its limit, the Brownian motion. This final exercise applies the concepts of martingales and optional stopping to a classic problem: calculating the expected time for a process to hit a specific target level $a$ [@problem_id:3065407]. This multi-part problem guides you through proving that the hitting time is a valid stopping time, applying a localized version of OST, and ultimately discovering through rigorous analysis that the expected time is infinite, beautifully illustrating why the direct application of OST is not permissible.", "problem": "Let $\\{B_{t}\\}_{t \\ge 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\ge 0},\\mathbb{P})$ with the usual conditions. For a fixed $a0$, define the first hitting time of level $a$ by\n$$\n\\tau_{a} \\;=\\; \\inf\\{\\, t \\ge 0 : B_{t} = a \\,\\}.\n$$\nAnswer the following.\n\n(a) Prove from first principles that $\\tau_{a}$ is an $\\{\\mathcal{F}_{t}\\}$-stopping time. Your argument should rely only on the definition of a stopping time, the continuity of Brownian motion sample paths, and the measurability of suprema indexed by rational times.\n\n(b) Using Itô’s formula to identify a basic martingale associated with $\\{B_{t}\\}$, justify that $M_{t} := B_{t}^{2} - t$ is a martingale with respect to $\\{\\mathcal{F}_{t}\\}$. Then, for each fixed $n \\in \\mathbb{N}$, apply the Optional Stopping Theorem (OST) for bounded stopping times to the stopped time $\\sigma_{n} := \\tau_{a} \\wedge n$ and obtain an identity that relates $\\mathbb{E}[\\tau_{a} \\wedge n]$ to the distribution of $B_{\\sigma_{n}}$.\n\n(c) Combine part (b) with a rigorous asymptotic analysis of $\\mathbb{P}(\\tau_{a}  t)$ as $t \\to \\infty$ (for instance, via the reflection principle for Brownian motion and a small-argument expansion of the standard normal distribution function) and an appropriate tail integral representation of expectation to determine the value of $\\mathbb{E}[\\tau_{a}]$. Be explicit about each logical step you use, including any dominated or monotone convergence arguments.\n\nYour final answer should be the value of $\\mathbb{E}[\\tau_{a}]$ as a single closed-form analytic expression. No numerical rounding is required or permitted.", "solution": "This problem is divided into three parts, which we will address sequentially. The overarching goal is to determine the expectation of the first hitting time $\\tau_a$ for a standard Brownian motion.\n\n(a) Proof that $\\tau_{a}$ is a stopping time.\n\nA random variable $T: \\Omega \\to [0, \\infty]$ is an $\\{\\mathcal{F}_{t}\\}_{t \\ge 0}$-stopping time if the event $\\{T \\le t\\}$ belongs to the sigma-algebra $\\mathcal{F}_{t}$ for all $t \\ge 0$. We need to show that for the first hitting time $\\tau_a = \\inf\\{\\, s \\ge 0 : B_{s} = a \\,\\}$, the event $\\{\\tau_a \\le t\\}$ is in $\\mathcal{F}_t$ for every $t \\ge 0$.\n\nThe event $\\{\\tau_a \\le t\\}$ means that the Brownian motion path has reached the level $a$ at some time $s$ in the interval $[0, t]$. This is equivalent to the statement that the supremum of the process over the interval $[0, t]$ is at least $a$. Formally, for a given sample path $\\omega \\in \\Omega$,\n$$\n\\tau_a(\\omega) \\le t \\iff \\exists s \\in [0, t] \\text{ such that } B_s(\\omega) = a.\n$$\nSince standard Brownian motion has continuous sample paths almost surely, and starts at $B_0 = 0  a$, the intermediate value theorem implies that if $B_s = a$ for some $s \\in [0, t]$, then the supremum of the path on $[0, t]$ must be at least $a$. Conversely, if the supremum is at least $a$, by continuity the path must attain the value $a$ at some point in $[0, t]$ (since it starts at $0$). Thus, we can write the event as:\n$$\n\\{\\tau_a \\le t\\} = \\{\\omega \\in \\Omega : \\sup_{s \\in [0, t]} B_s(\\omega) \\ge a\\}.\n$$\nThe problem requires a proof relying on the measurability of suprema over rational times. Due to the continuity of the sample paths $s \\mapsto B_s(\\omega)$, the supremum over the continuous interval $[0, t]$ is equal to the supremum over the set of rational numbers in that interval, $\\mathbb{Q} \\cap [0, t]$.\n$$\n\\sup_{s \\in [0, t]} B_s(\\omega) = \\sup_{q \\in \\mathbb{Q} \\cap [0, t]} B_q(\\omega).\n$$\nThis allows us to write the event as:\n$$\n\\{\\tau_a \\le t\\} = \\left\\{ \\sup_{q \\in \\mathbb{Q} \\cap [0, t]} B_q \\ge a \\right\\}.\n$$\nThe set of rational numbers $\\mathbb{Q} \\cap [0, t]$ is a countable set. For any $q \\in \\mathbb{Q} \\cap [0, t]$, we have $q \\le t$. By definition of the filtration $\\{\\mathcal{F}_t\\}_{t \\ge 0}$, the random variable $B_q$ is $\\mathcal{F}_q$-measurable. Since the filtration is non-decreasing (i.e., $\\mathcal{F}_q \\subseteq \\mathcal{F}_t$ for $q \\le t$), $B_q$ is also $\\mathcal{F}_t$-measurable for all $q \\in \\mathbb{Q} \\cap [0, t]$.\n\nThe supremum of a countable collection of $\\mathcal{F}_t$-measurable random variables is an $\\mathcal{F}_t$-measurable random variable. Let $Y_t = \\sup_{q \\in \\mathbb{Q} \\cap [0, t]} B_q$. Then $Y_t$ is $\\mathcal{F}_t$-measurable. Consequently, the set $\\{\\omega \\in \\Omega : Y_t(\\omega) \\ge a\\}$ is an event in $\\mathcal{F}_t$.\nTherefore, $\\{\\tau_a \\le t\\} \\in \\mathcal{F}_t$ for all $t \\ge 0$, which proves that $\\tau_a$ is an $\\{\\mathcal{F}_{t}\\}$-stopping time. The assumption that the filtration satisfies the \"usual conditions\" further solidifies this result, but this argument from first principles using rationals is self-contained as requested.\n\n(b) Martingale identification and application of the Optional Stopping Theorem.\n\nFirst, we show that $M_t := B_t^2 - t$ is a martingale. We apply Itô's formula to the function $f(x) = x^2$ applied to the process $B_t$. We have $f'(x) = 2x$ and $f''(x) = 2$. Itô's formula states:\n$$\ndf(B_t) = f'(B_t) dB_t + \\frac{1}{2} f''(B_t) d\\langle B, B \\rangle_t.\n$$\nFor a standard Brownian motion, the quadratic variation is $d\\langle B, B \\rangle_t = dt$. Substituting the derivatives and the quadratic variation, we get:\n$$\nd(B_t^2) = 2B_t dB_t + \\frac{1}{2}(2) dt = 2B_t dB_t + dt.\n$$\nThis is the differential form of the equation. We are interested in the process $M_t = B_t^2 - t$. Its differential is:\n$$\ndM_t = d(B_t^2 - t) = d(B_t^2) - dt = (2B_t dB_t + dt) - dt = 2B_t dB_t.\n$$\nIntegrating from $0$ to $t$ gives:\n$$\nM_t - M_0 = \\int_0^t 2B_s dB_s.\n$$\nSince $B_0 = 0$, we have $M_0 = B_0^2 - 0 = 0$. Thus, $M_t = \\int_0^t 2B_s dB_s$. An Itô integral of the form $\\int_0^t H_s dB_s$ is a martingale provided the integrand $H_s$ is adapted and satisfies $\\mathbb{E}[\\int_0^t H_s^2 ds]  \\infty$. Here $H_s = 2B_s$, which is adapted. Also, $\\mathbb{E}[\\int_0^t (2B_s)^2 ds] = 4\\int_0^t \\mathbb{E}[B_s^2] ds = 4\\int_0^t s ds = 2t^2  \\infty$. Thus, $M_t = B_t^2 - t$ is a martingale.\n\nNext, we apply the Optional Stopping Theorem (OST) for bounded stopping times. Let $\\sigma_n := \\tau_a \\wedge n = \\min(\\tau_a, n)$ for a fixed integer $n \\in \\mathbb{N}$. Since $\\tau_a$ is a stopping time (from part (a)) and the constant time $n$ is trivially a stopping time, their minimum $\\sigma_n$ is also a stopping time. Crucially, $\\sigma_n$ is a bounded stopping time because $\\sigma_n(\\omega) \\le n$ for all $\\omega \\in \\Omega$.\n\nThe OST for bounded stopping times states that for a martingale $M_t$ and a bounded stopping time $T$, we have $\\mathbb{E}[M_T] = \\mathbb{E}[M_0]$. Applying this to our martingale $M_t$ and stopping time $\\sigma_n$:\n$$\n\\mathbb{E}[M_{\\sigma_n}] = \\mathbb{E}[M_0].\n$$\nWe calculated $M_0 = 0$. So, $\\mathbb{E}[M_{\\sigma_n}] = 0$.\nSubstituting the definition of $M_t$:\n$$\n\\mathbb{E}[B_{\\sigma_n}^2 - \\sigma_n] = 0.\n$$\nBy the linearity of expectation, this becomes:\n$$\n\\mathbb{E}[B_{\\sigma_n}^2] - \\mathbb{E}[\\sigma_n] = 0.\n$$\nRearranging gives the desired identity relating $\\mathbb{E}[\\tau_a \\wedge n]$ to the distribution of $B_{\\sigma_n}$:\n$$\n\\mathbb{E}[\\tau_a \\wedge n] = \\mathbb{E}[B_{\\tau_a \\wedge n}^2].\n$$\n\n(c) Determination of $\\mathbb{E}[\\tau_a]$.\n\nWe wish to find $\\mathbb{E}[\\tau_a]$. Since $\\tau_a$ is a non-negative random variable, a standard approach is to compute $\\lim_{n \\to \\infty} \\mathbb{E}[\\tau_a \\wedge n]$. The sequence of random variables $X_n = \\tau_a \\wedge n$ is non-negative and non-decreasing ($X_n \\le X_{n+1}$). The pointwise limit is $\\lim_{n \\to \\infty} (\\tau_a \\wedge n) = \\tau_a$. By the Monotone Convergence Theorem (MCT), we can interchange the limit and expectation:\n$$\n\\mathbb{E}[\\tau_a] = \\mathbb{E}[\\lim_{n \\to \\infty} (\\tau_a \\wedge n)] = \\lim_{n \\to \\infty} \\mathbb{E}[\\tau_a \\wedge n].\n$$\nAn alternative representation for the expectation of a non-negative random variable $X$ is the tail integral formula: $\\mathbb{E}[X] = \\int_0^\\infty \\mathbb{P}(X  t) dt$. Applying this to $\\tau_a$:\n$$\n\\mathbb{E}[\\tau_a] = \\int_0^\\infty \\mathbb{P}(\\tau_a  t) dt.\n$$\nWe need to find the tail probability $\\mathbb{P}(\\tau_a  t)$. The event $\\{\\tau_a  t\\}$ is the same as the event that the maximum value of the Brownian motion on $[0, t]$ is less than $a$, i.e., $\\{\\sup_{0 \\le s \\le t} B_s  a\\}$. The probability of the complement, $\\mathbb{P}(\\tau_a \\le t) = \\mathbb{P}(\\sup_{0 \\le s \\le t} B_s \\ge a)$, can be computed using the reflection principle for Brownian motion. For $a  0$, the principle states:\n$$\n\\mathbb{P}(\\sup_{0 \\le s \\le t} B_s \\ge a) = 2 \\mathbb{P}(B_t \\ge a).\n$$\nSince $B_t$ is normally distributed with mean $0$ and variance $t$, i.e., $B_t \\sim N(0, t)$, we can write $B_t = \\sqrt{t} Z$ where $Z \\sim N(0, 1)$.\nLet $\\Phi$ be the cumulative distribution function (CDF) of the standard normal distribution.\n$$\n\\mathbb{P}(B_t \\ge a) = \\mathbb{P}(\\sqrt{t} Z \\ge a) = \\mathbb{P}(Z \\ge a/\\sqrt{t}) = 1 - \\Phi(a/\\sqrt{t}).\n$$\nThus, $\\mathbb{P}(\\tau_a \\le t) = 2(1 - \\Phi(a/\\sqrt{t}))$.\nThe required tail probability is then:\n$$\n\\mathbb{P}(\\tau_a  t) = 1 - \\mathbb{P}(\\tau_a \\le t) = 1 - 2(1 - \\Phi(a/\\sqrt{t})) = 2\\Phi(a/\\sqrt{t}) - 1.\n$$\nThis expression can also be written using the error function or by relating it to the probability $\\mathbb{P}(-a/\\sqrt{t}  Z  a/\\sqrt{t})$.\n\nNow we can evaluate the expectation:\n$$\n\\mathbb{E}[\\tau_a] = \\int_0^\\infty (2\\Phi(a/\\sqrt{t}) - 1) dt.\n$$\nTo determine if this integral converges, we perform an asymptotic analysis for large $t$. As $t \\to \\infty$, the argument $x = a/\\sqrt{t}$ approaches $0$. The Taylor expansion of the standard normal CDF $\\Phi(x)$ around $x=0$ is $\\Phi(x) = \\Phi(0) + \\Phi'(0)x + O(x^3)$. With $\\Phi(0) = 1/2$ and $\\Phi'(x) = \\phi(x) = (2\\pi)^{-1/2}\\exp(-x^2/2)$, we have $\\Phi'(0) = (2\\pi)^{-1/2}$.\nSo, for small $x$, $\\Phi(x) \\approx 1/2 + x/\\sqrt{2\\pi}$.\nSubstituting $x=a/\\sqrt{t}$:\n$$\n2\\Phi(a/\\sqrt{t}) - 1 \\approx 2\\left(\\frac{1}{2} + \\frac{a}{\\sqrt{t}\\sqrt{2\\pi}}\\right) - 1 = \\sqrt{\\frac{2}{\\pi}}\\frac{a}{\\sqrt{t}}.\n$$\nFor large $t$, the integrand $\\mathbb{P}(\\tau_a  t)$ behaves like $C t^{-1/2}$ for some constant $C = a\\sqrt{2/\\pi}$. The integral of $t^{-1/2}$ from any positive number to infinity diverges:\n$$\n\\int_1^\\infty t^{-1/2} dt = [2t^{1/2}]_1^\\infty = \\lim_{T \\to \\infty} (2\\sqrt{T} - 2) = \\infty.\n$$\nSince the integrand is non-negative and its tail behaves like $t^{-1/2}$, the integral $\\int_0^\\infty \\mathbb{P}(\\tau_a  t) dt$ diverges to infinity.\n\nTherefore, the expected value of the first hitting time is infinite.\n$$\n\\mathbb{E}[\\tau_a] = \\infty.\n$$\nThis result is consistent with the identity from part (b). If we take the limit $n \\to \\infty$ in $\\mathbb{E}[\\tau_a \\wedge n] = \\mathbb{E}[B_{\\tau_a \\wedge n}^2]$, the left-hand side tends to $\\mathbb{E}[\\tau_a]$. The right-hand side is $\\mathbb{E}[a^2 \\cdot \\mathbf{1}_{\\{\\tau_a \\le n\\}} + B_n^2 \\cdot \\mathbf{1}_{\\{\\tau_a  n\\}}]$. It can be shown that the second term, $\\mathbb{E}[B_n^2 \\cdot \\mathbf{1}_{\\{\\tau_a  n\\}}]$, grows asymptotically like $\\sqrt{n}$, causing the right-hand side to diverge. This confirms that a naive application of OST to the unbounded stopping time $\\tau_a$ (which would incorrectly yield $\\mathbb{E}[\\tau_a]=a^2$) is invalid because its conditions are not met. The rigorous path through bounded stopping times and convergence theorems reveals the true infinite expectation.", "answer": "$$\\boxed{\\infty}$$", "id": "3065407"}]}