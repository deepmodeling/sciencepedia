{"hands_on_practices": [{"introduction": "A core concept in stochastic calculus is the Itô integral, which forms the building block of solutions to many stochastic differential equations. This first practice invites you to derive the distribution of a specific Itô integral from first principles [@problem_id:3068161]. By constructing the integral as a limit of Riemann sums, you will see precisely how the properties of Brownian motion lead to a Gaussian distribution, a result that underpins the ubiquity of Gaussian vectors in SDEs.", "problem": "Let $T0$ and $\\alpha0$ be fixed constants. Let $\\{W_{t}\\}_{t\\geq 0}$ be a standard Brownian motion (also called a Wiener process), characterized by $W_{0}=0$, independent increments, and $W_{t}-W_{s}\\sim \\mathcal{N}(0,t-s)$ for $0\\leq st$, with continuous sample paths. Consider the stochastic Itô integral\n$$\nX \\;=\\; \\int_{0}^{T} \\exp(-\\alpha t)\\, \\mathrm{d}W_{t}.\n$$\nStarting from the construction of the Itô integral as an $L^{2}$-limit of stochastic Riemann sums for deterministic square-integrable integrands, and using only the defining properties of Brownian motion and the characterization of Gaussian random vectors by linear combinations of independent Gaussian increments, determine the probability distribution of the random variable $X$. Your answer must identify the mean and variance of $X$ and give its law in closed form. Express the final distribution concisely as a single analytic expression. No numerical rounding is required.", "solution": "The problem asks for the probability distribution of the random variable $X$ defined by the Itô integral $X = \\int_{0}^{T} \\exp(-\\alpha t)\\, \\mathrm{d}W_{t}$, where $\\{W_{t}\\}_{t\\geq 0}$ is a standard Brownian motion and $T0$, $\\alpha0$ are constants. We are instructed to derive this starting from the definition of the Itô integral as a limit of stochastic Riemann sums.\n\nThe integrand is the deterministic function $f(t) = \\exp(-\\alpha t)$. This function is continuous on the interval $[0, T]$ and therefore square-integrable, i.e., $\\int_{0}^{T} |f(t)|^{2} \\, \\mathrm{d}t  \\infty$. For such an integrand, the Itô integral is well-defined as the limit in mean square ($L^{2}$) of a sequence of approximating sums.\n\nLet us construct such a sequence. Consider a sequence of partitions of the interval $[0, T]$, denoted by $\\Pi_{n} = \\{0 = t_{0}^{(n)}  t_{1}^{(n)}  \\dots  t_{m_{n}}^{(n)} = T\\}$, such that the mesh of the partition, $\\|\\Pi_{n}\\| = \\max_{k} (t_{k+1}^{(n)} - t_{k}^{(n)})$, converges to $0$ as $n \\to \\infty$.\n\nThe Itô integral $X$ is defined as the $L^{2}$-limit of the sequence of random variables $\\{X_{n}\\}_{n \\in \\mathbb{N}}$, where each $X_{n}$ is a stochastic Riemann sum corresponding to the partition $\\Pi_n$:\n$$\nX_{n} = \\sum_{k=0}^{m_{n}-1} f(t_{k}^{(n)}) (W_{t_{k+1}^{(n)}} - W_{t_{k}^{(n)}})\n$$\nSubstituting the specific integrand $f(t) = \\exp(-\\alpha t)$, we have:\n$$\nX_{n} = \\sum_{k=0}^{m_{n}-1} \\exp(-\\alpha t_{k}^{(n)}) (W_{t_{k+1}^{(n)}} - W_{t_{k}^{(n)}})\n$$\nBy definition, $X = \\lim_{n\\to\\infty} X_{n}$ in the $L^{2}$ sense.\n\nWe now determine the probability distribution of each $X_{n}$. Each $X_{n}$ is a finite linear combination of the increments of the Brownian motion, $\\Delta W_{k}^{(n)} = W_{t_{k+1}^{(n)}} - W_{t_{k}^{(n)}}$. According to the properties of Brownian motion, these increments are independent random variables. Furthermore, for any $s  t$, the increment $W_{t} - W_{s}$ is a Gaussian (normal) random variable with mean $0$ and variance $t-s$, which we denote as $W_{t} - W_{s} \\sim \\mathcal{N}(0, t-s)$.\nTherefore, each increment $\\Delta W_{k}^{(n)}$ in the sum follows the distribution $\\mathcal{N}(0, t_{k+1}^{(n)} - t_{k}^{(n)})$.\n\nThe term $\\exp(-\\alpha t_{k}^{(n)})$ is a deterministic constant for each $k$. A constant multiple of a Gaussian random variable is also a Gaussian random variable. Let $Y_{k}^{(n)} = \\exp(-\\alpha t_{k}^{(n)}) \\Delta W_{k}^{(n)}$. The mean of $Y_{k}^{(n)}$ is:\n$$\nE[Y_{k}^{(n)}] = E[\\exp(-\\alpha t_{k}^{(n)}) \\Delta W_{k}^{(n)}] = \\exp(-\\alpha t_{k}^{(n)}) E[\\Delta W_{k}^{(n)}] = \\exp(-\\alpha t_{k}^{(n)}) \\cdot 0 = 0\n$$\nThe variance of $Y_{k}^{(n)}$ is:\n$$\n\\text{Var}(Y_{k}^{(n)}) = \\text{Var}(\\exp(-\\alpha t_{k}^{(n)}) \\Delta W_{k}^{(n)}) = (\\exp(-\\alpha t_{k}^{(n)}))^{2} \\text{Var}(\\Delta W_{k}^{(n)}) = \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)})\n$$\nSo, $Y_{k}^{(n)} \\sim \\mathcal{N}(0, \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)}))$.\n\nThe random variable $X_{n}$ is the sum of these independent Gaussian random variables $Y_{k}^{(n)}$: $X_{n} = \\sum_{k=0}^{m_{n}-1} Y_{k}^{(n)}$. A fundamental property of Gaussian distributions is that a sum of independent Gaussian random variables is also Gaussian. Thus, $X_{n}$ is a Gaussian random variable.\n\nThe mean of $X_{n}$ is the sum of the means of $Y_{k}^{(n)}$:\n$$\nE[X_{n}] = E\\left[\\sum_{k=0}^{m_{n}-1} Y_{k}^{(n)}\\right] = \\sum_{k=0}^{m_{n}-1} E[Y_{k}^{(n)}] = \\sum_{k=0}^{m_{n}-1} 0 = 0\n$$\nThe variance of $X_{n}$ is the sum of the variances of the independent $Y_{k}^{(n)}$:\n$$\n\\text{Var}(X_{n}) = \\text{Var}\\left(\\sum_{k=0}^{m_{n}-1} Y_{k}^{(n)}\\right) = \\sum_{k=0}^{m_{n}-1} \\text{Var}(Y_{k}^{(n)}) = \\sum_{k=0}^{m_{n}-1} \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)})\n$$\nSo, for each $n$, the distribution of $X_{n}$ is $\\mathcal{N}(0, \\sigma_{n}^{2})$ where $\\sigma_{n}^{2} = \\text{Var}(X_{n})$.\n\nNow we take the limit as $n \\to \\infty$. The sequence of random variables $X_{n}$ converges in $L^{2}$ to $X$. This implies that $X_{n}$ also converges in distribution to $X$. A standard result in probability theory states that if a sequence of Gaussian random variables converges in distribution, the limiting distribution is also Gaussian. The parameters of the limiting Gaussian distribution are the limits of the parameters of the sequence.\n\nThe mean of $X$ is the limit of the means of $X_{n}$:\n$$\nE[X] = \\lim_{n\\to\\infty} E[X_{n}] = \\lim_{n\\to\\infty} 0 = 0\n$$\nThe variance of $X$ is the limit of the variances of $X_{n}$:\n$$\n\\text{Var}(X) = \\lim_{n\\to\\infty} \\text{Var}(X_{n}) = \\lim_{n\\to\\infty} \\sum_{k=0}^{m_{n}-1} \\exp(-2\\alpha t_{k}^{(n)}) (t_{k+1}^{(n)} - t_{k}^{(n)})\n$$\nThis sum is a Riemann sum for the function $g(t) = \\exp(-2\\alpha t)$ over the interval $[0, T]$. As the mesh of the partition $\\|\\Pi_{n}\\|$ goes to $0$, this sum converges to the definite integral of $g(t)$:\n$$\n\\text{Var}(X) = \\int_{0}^{T} \\exp(-2\\alpha t) \\, \\mathrm{d}t\n$$\nThis integral is straightforward to evaluate. Since $\\alpha  0$:\n$$\n\\int_{0}^{T} \\exp(-2\\alpha t) \\, \\mathrm{d}t = \\left[ \\frac{\\exp(-2\\alpha t)}{-2\\alpha} \\right]_{0}^{T} = \\frac{\\exp(-2\\alpha T)}{-2\\alpha} - \\frac{\\exp(-2\\alpha \\cdot 0)}{-2\\alpha} = -\\frac{\\exp(-2\\alpha T)}{2\\alpha} + \\frac{1}{2\\alpha} = \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\n$$\nThis result is a specific case of the Itô isometry for deterministic integrands, which states that $E\\left[\\left(\\int_{0}^{T} f(t) \\mathrm{d}W_{t}\\right)^{2}\\right] = \\int_{0}^{T} |f(t)|^{2} \\mathrm{d}t$. Since the mean is $0$, the second moment is equal to the variance.\n\nIn summary, the random variable $X = \\int_{0}^{T} \\exp(-\\alpha t) \\, \\mathrm{d}W_{t}$ is the $L^{2}$-limit of a sequence of Gaussian random variables $\\{X_{n}\\}$, each with mean $0$. Therefore, $X$ itself must be a Gaussian random variable with a mean equal to the limit of the means ($0$) and a variance equal to the limit of the variances.\n\nThe mean of $X$ is $E[X]=0$.\nThe variance of $X$ is $\\text{Var}(X) = \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}$.\n\nThus, the probability distribution of $X$ is a normal (Gaussian) distribution with mean $0$ and variance $\\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}$. This can be concisely written as:\n$$\nX \\sim \\mathcal{N}\\left(0, \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\\right)\n$$\nThis is the closed-form law for the random variable $X$.", "answer": "$$\n\\boxed{\\mathcal{N}\\left(0, \\frac{1 - \\exp(-2\\alpha T)}{2\\alpha}\\right)}\n$$", "id": "3068161"}, {"introduction": "One of the most powerful properties of the multivariate normal distribution is that uncorrelated components are also independent. This is a special feature not shared by other distributions, and this exercise is designed to make that distinction crystal clear [@problem_id:3068184]. You will first analyze a non-Gaussian pair of variables that are uncorrelated but functionally dependent, and then contrast this with a jointly Gaussian pair to prove why zero covariance guarantees independence in the Gaussian world.", "problem": "Let $B_{t}$ be a one-dimensional standard Brownian motion. Consider the Ornstein–Uhlenbeck process $U(t)$ defined as the solution to the stochastic differential equation (SDE) $dU(t) = -\\lambda U(t)\\,dt + \\sigma\\,dB_{t}$ with constants $\\lambda  0$ and $\\sigma  0$, and assume $U(0)$ is independent of $B_{t}$ and chosen so that $\\{U(t)\\}_{t \\geq 0}$ is strictly stationary. Fix any time $t  0$ and define the random variables $X := U(t)$ and $Y := U(t)^{2} - \\mathbb{E}[U(t)^{2}]$. Using only foundational properties of solutions to linear SDEs driven by Gaussian inputs and the definition of a Gaussian random vector, derive from first principles the exact value of $\\operatorname{Cov}(X,Y)$ and explain briefly why $X$ and $Y$ are not independent.\n\nFor contrast, construct a jointly Gaussian pair from the same Brownian motion by defining $X_{G} := \\int_{0}^{1} \\phi(s)\\,dB_{s}$ and $Y_{G} := \\int_{0}^{1} \\psi(s)\\,dB_{s}$ for deterministic functions $\\phi,\\psi \\in L^{2}([0,1])$ that satisfy $\\int_{0}^{1} \\phi(s)\\psi(s)\\,ds = 0$. Let $v_{\\phi} := \\int_{0}^{1} \\phi(s)^{2}\\,ds$ and $v_{\\psi} := \\int_{0}^{1} \\psi(s)^{2}\\,ds$. Starting from the definition of a Gaussian random vector and the Itô isometry, derive the joint probability density function $f_{X_{G},Y_{G}}(x,y)$ and show that it factorizes into the product of the marginal densities, thereby establishing independence in the Gaussian case when the covariance is zero.\n\nReport your final answer as the pair consisting of the value of $\\operatorname{Cov}(X,Y)$ in the non-Gaussian construction and the explicit analytic expression for $f_{X_{G},Y_{G}}(x,y)$ in the Gaussian construction. No rounding is required.", "solution": "The problem presents two distinct scenarios to illustrate the relationship between covariance and independence. The first involves a non-Gaussian vector constructed from an Ornstein-Uhlenbeck process, while the second involves a jointly Gaussian vector constructed from Itô integrals. We will validate and solve each part sequentially.\n\nThe problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard, non-trivial problem in stochastic calculus that correctly frames the distinction between uncorrelated and independent random variables. The problem is therefore deemed valid.\n\n**Part 1: The Ornstein-Uhlenbeck Process Case**\n\nFirst, we analyze the random variables $X = U(t)$ and $Y = U(t)^{2} - \\mathbb{E}[U(t)^{2}]$, where $U(t)$ is a stationary Ornstein-Uhlenbeck (OU) process. The SDE for $U(t)$ is given by $dU(t) = -\\lambda U(t)\\,dt + \\sigma\\,dB_{t}$, with $\\lambda  0$ and $\\sigma  0$.\n\nThe solution to this linear SDE can be found using an integrating factor $e^{\\lambda t}$:\n$$ d(e^{\\lambda t} U(t)) = \\lambda e^{\\lambda t} U(t)\\,dt + e^{\\lambda t} dU(t) = e^{\\lambda t}(-\\lambda U(t)\\,dt + \\sigma\\,dB_{t}) + \\lambda e^{\\lambda t} U(t)\\,dt = \\sigma e^{\\lambda t}\\,dB_{t} $$\nIntegrating from $0$ to $t$ gives:\n$$ e^{\\lambda t} U(t) - U(0) = \\int_{0}^{t} \\sigma e^{\\lambda s}\\,dB_{s} $$\n$$ U(t) = e^{-\\lambda t} U(0) + \\sigma \\int_{0}^{t} e^{-\\lambda(t-s)}\\,dB_{s} $$\nSince the integrand $e^{-\\lambda(t-s)}$ is deterministic, the Itô integral $\\int_{0}^{t} e^{-\\lambda(t-s)}\\,dB_{s}$ is a Gaussian random variable. $U(0)$ is assumed to be independent of the Brownian motion $\\{B_s\\}_{s \\geq 0}$. For $U(t)$ to be Gaussian, $U(0)$ must also be Gaussian. Let us assume $U(0) \\sim N(\\mu_0, \\sigma_0^2)$.\n\nThe mean of $U(t)$ is:\n$$ \\mathbb{E}[U(t)] = e^{-\\lambda t} \\mathbb{E}[U(0)] + \\mathbb{E}\\left[\\sigma \\int_{0}^{t} e^{-\\lambda(t-s)}\\,dB_{s}\\right] = e^{-\\lambda t}\\mu_0 $$\nFor the process to be stationary, its distribution cannot depend on time $t$. Thus, we must have $\\mathbb{E}[U(t)] = \\mathbb{E}[U(0)] = \\mu_0$. This requires $\\mu_0 = e^{-\\lambda t}\\mu_0$ for all $t  0$, which implies $\\mu_0 = 0$.\n\nThe variance of $U(t)$ is calculated using the independence of $U(0)$ and the Itô integral, along with the Itô isometry:\n$$ \\operatorname{Var}(U(t)) = \\operatorname{Var}(e^{-\\lambda t} U(0)) + \\operatorname{Var}\\left(\\sigma \\int_{0}^{t} e^{-\\lambda(t-s)}\\,dB_{s}\\right) $$\n$$ \\operatorname{Var}(U(t)) = e^{-2\\lambda t} \\operatorname{Var}(U(0)) + \\sigma^2 \\int_{0}^{t} (e^{-\\lambda(t-s)})^2\\,ds $$\n$$ \\int_{0}^{t} e^{-2\\lambda(t-s)}\\,ds = e^{-2\\lambda t} \\int_{0}^{t} e^{2\\lambda s}\\,ds = e^{-2\\lambda t} \\left[\\frac{e^{2\\lambda s}}{2\\lambda}\\right]_0^t = \\frac{e^{-2\\lambda t}}{2\\lambda} (e^{2\\lambda t}-1) = \\frac{1-e^{-2\\lambda t}}{2\\lambda} $$\nLetting $\\sigma_U^2 = \\operatorname{Var}(U(t))$, we have:\n$$ \\sigma_U^2 = e^{-2\\lambda t} \\sigma_0^2 + \\frac{\\sigma^2}{2\\lambda}(1-e^{-2\\lambda t}) $$\nFor stationarity, we require $\\operatorname{Var}(U(t)) = \\operatorname{Var}(U(0))$, so $\\sigma_U^2 = \\sigma_0^2$.\n$$ \\sigma_0^2 = e^{-2\\lambda t} \\sigma_0^2 + \\frac{\\sigma^2}{2\\lambda}(1-e^{-2\\lambda t}) \\implies \\sigma_0^2(1-e^{-2\\lambda t}) = \\frac{\\sigma^2}{2\\lambda}(1-e^{-2\\lambda t}) $$\nThis implies that the stationary variance is $\\sigma_U^2 = \\frac{\\sigma^2}{2\\lambda}$.\nThus, for any $t \\geq 0$, the stationary OU process $U(t)$ is distributed as a Gaussian random variable with mean $0$ and variance $\\frac{\\sigma^2}{2\\lambda}$: $U(t) \\sim N(0, \\frac{\\sigma^2}{2\\lambda})$.\n\nNow we compute $\\operatorname{Cov}(X,Y)$ where $X = U(t)$ and $Y = U(t)^2 - \\mathbb{E}[U(t)^2]$.\nBy definition, $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$.\nWe have $\\mathbb{E}[X] = \\mathbb{E}[U(t)] = 0$.\nAlso, $\\mathbb{E}[Y] = \\mathbb{E}[U(t)^2 - \\mathbb{E}[U(t)^2]] = \\mathbb{E}[U(t)^2] - \\mathbb{E}[U(t)^2] = 0$.\nSo, $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] = \\mathbb{E}[U(t)(U(t)^2 - \\mathbb{E}[U(t)^2])]$.\n$$ \\operatorname{Cov}(X,Y) = \\mathbb{E}[U(t)^3] - \\mathbb{E}[U(t)]\\mathbb{E}[U(t)^2] $$\nSince $\\mathbb{E}[U(t)] = 0$, this simplifies to $\\operatorname{Cov}(X,Y) = \\mathbb{E}[U(t)^3]$.\n$U(t)$ is a centered Gaussian random variable (mean is $0$). All odd moments of a centered Gaussian distribution are zero. Therefore, $\\mathbb{E}[U(t)^3] = 0$.\nSo, $\\operatorname{Cov}(X,Y) = 0$.\n\nDespite having zero covariance, $X$ and $Y$ are not independent. Independence of two random variables $X$ and $Y$ would imply that for any (measurable) function $g$, $\\mathbb{E}[g(X)|Y] = \\mathbb{E}[g(X)]$. Here, $Y$ is a deterministic function of $X$: $Y = X^2 - \\mathbb{E}[X^2]$. This functional dependence precludes independence, unless one of the variables is almost surely constant. Since $U(t)$ follows a non-degenerate Gaussian distribution, neither $X=U(t)$ nor $Y=U(t)^2 - \\mathbb{E}[U(t)^2]$ is constant. Knowing the value of $X$ completely determines the value of $Y$. For example, if $X=x$, then $Y$ must be $x^2 - \\mathbb{E}[X^2]$. The conditional distribution of $Y$ given $X=x$ is a point mass, which is drastically different from its marginal distribution (a scaled and shifted chi-squared distribution). Therefore, $X$ and $Y$ are dependent. This case highlights that zero covariance does not imply independence for random variables that are not jointly Gaussian. The vector $(X, Y) = (U(t), U(t)^2 - c)$ is not jointly Gaussian because the component $Y$ is not a Gaussian random variable.\n\n**Part 2: The Jointly Gaussian Case**\n\nNext, we analyze the random variables $X_G := \\int_{0}^{1} \\phi(s)\\,dB_{s}$ and $Y_G := \\int_{0}^{1} \\psi(s)\\,dB_{s}$, where $\\phi, \\psi \\in L^{2}([0,1])$ satisfy $\\int_{0}^{1} \\phi(s)\\psi(s)\\,ds = 0$.\n\nA vector $(X_G, Y_G)$ is a Gaussian random vector if every linear combination $aX_G + bY_G$ is a Gaussian random variable for all $a, b \\in \\mathbb{R}$.\n$$ aX_G + bY_G = a\\int_{0}^{1} \\phi(s)\\,dB_{s} + b\\int_{0}^{1} \\psi(s)\\,dB_{s} = \\int_{0}^{1} (a\\phi(s) + b\\psi(s))\\,dB_{s} $$\nSince the integrand $a\\phi(s) + b\\psi(s)$ is a deterministic function in $L^2([0,1])$, the resulting Itô integral is a Gaussian random variable with mean $0$ and variance given by the Itô isometry:\n$$ \\operatorname{Var}(aX_G + bY_G) = \\int_{0}^{1} (a\\phi(s) + b\\psi(s))^2\\,ds = a^2\\int_{0}^{1} \\phi(s)^2\\,ds + 2ab\\int_{0}^{1} \\phi(s)\\psi(s)\\,ds + b^2\\int_{0}^{1} \\psi(s)^2\\,ds $$\nUsing the given definitions $v_{\\phi} = \\int_{0}^{1} \\phi(s)^{2}\\,ds$, $v_{\\psi} = \\int_{0}^{1} \\psi(s)^{2}\\,ds$, and the orthogonality condition $\\int_{0}^{1} \\phi(s)\\psi(s)\\,ds = 0$, the variance is $a^2v_{\\phi} + b^2v_{\\psi}$.\nSince any linear combination is Gaussian, $(X_G, Y_G)$ is a bivariate Gaussian random vector.\n\nThe distribution of a Gaussian vector is determined by its mean vector and covariance matrix.\nThe mean vector $\\boldsymbol{\\mu}$ has components:\n$$ \\mathbb{E}[X_G] = 0, \\quad \\mathbb{E}[Y_G] = 0 $$\nThe covariance matrix $\\boldsymbol{\\Sigma}$ has components:\n$$ \\operatorname{Var}(X_G) = \\mathbb{E}[X_G^2] = \\int_{0}^{1} \\phi(s)^2\\,ds = v_{\\phi} $$\n$$ \\operatorname{Var}(Y_G) = \\mathbb{E}[Y_G^2] = \\int_{0}^{1} \\psi(s)^2\\,ds = v_{\\psi} $$\n$$ \\operatorname{Cov}(X_G, Y_G) = \\mathbb{E}[X_G Y_G] = \\mathbb{E}\\left[\\left(\\int_{0}^{1} \\phi(s)\\,dB_{s}\\right)\\left(\\int_{0}^{1} \\psi(s)\\,dB_{s}\\right)\\right] = \\int_{0}^{1} \\phi(s)\\psi(s)\\,ds = 0 $$\nSo, the mean vector is $\\boldsymbol{\\mu} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the covariance matrix is $\\boldsymbol{\\Sigma} = \\begin{pmatrix} v_{\\phi}  0 \\\\ 0  v_{\\psi} \\end{pmatrix}$.\n\nThe joint probability density function (PDF) for a bivariate normal distribution with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ is:\n$$ f_{X_{G},Y_{G}}(x,y) = \\frac{1}{2\\pi\\sqrt{\\det(\\boldsymbol{\\Sigma})}} \\exp\\left(-\\frac{1}{2}( \\begin{pmatrix} x \\\\ y \\end{pmatrix} - \\boldsymbol{\\mu} )^T \\boldsymbol{\\Sigma}^{-1} ( \\begin{pmatrix} x \\\\ y \\end{pmatrix} - \\boldsymbol{\\mu} ) \\right) $$\nHere, $\\det(\\boldsymbol{\\Sigma}) = v_{\\phi}v_{\\psi}$ and $\\boldsymbol{\\Sigma}^{-1} = \\begin{pmatrix} 1/v_{\\phi}  0 \\\\ 0  1/v_{\\psi} \\end{pmatrix}$. The quadratic form in the exponent is:\n$$ \\begin{pmatrix} x  y \\end{pmatrix} \\begin{pmatrix} 1/v_{\\phi}  0 \\\\ 0  1/v_{\\psi} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\frac{x^2}{v_{\\phi}} + \\frac{y^2}{v_{\\psi}} $$\nSubstituting these into the PDF formula yields:\n$$ f_{X_{G},Y_{G}}(x,y) = \\frac{1}{2\\pi\\sqrt{v_{\\phi}v_{\\psi}}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x^2}{v_{\\phi}} + \\frac{y^2}{v_{\\psi}}\\right)\\right) $$\nThis expression can be factorized:\n$$ f_{X_{G},Y_{G}}(x,y) = \\left(\\frac{1}{\\sqrt{2\\pi v_{\\phi}}} \\exp\\left(-\\frac{x^2}{2v_{\\phi}}\\right)\\right) \\cdot \\left(\\frac{1}{\\sqrt{2\\pi v_{\\psi}}} \\exp\\left(-\\frac{y^2}{2v_{\\psi}}\\right)\\right) $$\nThe two terms are precisely the marginal probability density functions $f_{X_G}(x)$ of a $N(0, v_{\\phi})$ random variable and $f_{Y_G}(y)$ of a $N(0, v_{\\psi})$ random variable. Since $f_{X_{G},Y_{G}}(x,y) = f_{X_G}(x)f_{Y_G}(y)$, the random variables $X_G$ and $Y_G$ are independent. This confirms the well-known property that for jointly Gaussian random variables, zero covariance is a sufficient condition for independence.\n\nThe final answer consists of the value of $\\operatorname{Cov}(X,Y)$ and the expression for $f_{X_{G},Y_{G}}(x,y)$.\n$\\operatorname{Cov}(X,Y) = 0$.\n$f_{X_{G},Y_{G}}(x,y) = \\frac{1}{2\\pi \\sqrt{v_{\\phi}v_{\\psi}}} \\exp\\left(-\\frac{x^2}{2v_{\\phi}} - \\frac{y^2}{2v_{\\psi}}\\right)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  \\frac{1}{2\\pi \\sqrt{v_{\\phi} v_{\\psi}}} \\exp\\left(-\\frac{x^2}{2v_{\\phi}} - \\frac{y^2}{2v_{\\psi}}\\right)\n\\end{pmatrix}\n}\n$$", "id": "3068184"}, {"introduction": "The multivariate normal distribution is not just an abstract formula; it has a rich geometric structure where surfaces of constant probability density are ellipsoids. This exercise explores this geometry by asking you to calculate the probability of a Gaussian vector falling within such an ellipsoid [@problem_id:3068167]. By transforming the problem, you will discover a beautiful connection between the multivariate Gaussian distribution and the univariate chi-squared distribution, providing a powerful tool for statistical analysis.", "problem": "Consider a $d$-dimensional Gaussian random vector $X$ that arises as the stationary state of a stable linear stochastic differential equation (SDE), so that $X \\sim \\mathcal{N}_{d}(\\mu,\\Sigma)$ with mean vector $\\mu \\in \\mathbb{R}^{d}$ and a symmetric positive definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. For a fixed threshold $c0$, define the ellipsoid \n$$\\mathcal{E}_{c}=\\{x \\in \\mathbb{R}^{d}:(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)\\le c\\}.$$\nStarting only from core definitions of the multivariate normal distribution and basic properties of linear transformations, derive from first principles a closed-form analytic expression, without unevaluated integrals, for the probability content \n$$\\mathbb{P}\\big(X \\in \\mathcal{E}_{c}\\big)=\\mathbb{P}\\big((X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)\\le c\\big),$$\nexpressed purely in terms of $d$ and $c$ using the Gamma function. Your final answer must be a single closed-form expression. Do not perform numerical approximations.", "solution": "The user wants to find the probability that a multivariate normal random variable $X$ falls within a specific ellipsoid $\\mathcal{E}_c$.\n\n### Step 1: Extract Givens\n-   A $d$-dimensional random vector $X \\sim \\mathcal{N}_{d}(\\mu,\\Sigma)$\n-   Mean vector $\\mu \\in \\mathbb{R}^{d}$\n-   Covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, symmetric and positive definite\n-   Threshold $c  0$\n-   Ellipsoid $\\mathcal{E}_{c}=\\{x \\in \\mathbb{R}^{d}:(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)\\le c\\}$\n-   The objective is to find a closed-form expression for the probability $\\mathbb{P}(X \\in \\mathcal{E}_{c}) = \\mathbb{P}\\big((X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)\\le c\\big)$ in terms of $d$ and $c$, using the Gamma function.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically and scientifically sound. It is a standard problem in multivariate statistics concerning the properties of the Mahalanobis distance for a Gaussian random vector. The premises are well-defined: $X$ follows a specified multivariate normal distribution, and the covariance matrix $\\Sigma$ is symmetric and positive definite, which guarantees that its inverse $\\Sigma^{-1}$ and its square root $\\Sigma^{1/2}$ exist and are well-behaved. The problem is self-contained, objective, and well-posed, admitting a unique, derivable solution. There are no contradictions, ambiguities, or violations of fundamental principles. The request to derive the result from first principles makes it a substantive exercise rather than a trivial query.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete derivation will be provided.\n\n### Derivation\nThe problem is to compute the probability $\\mathbb{P}\\big((X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)\\le c\\big)$.\n\nFirst, we perform a standardization of the random vector $X$. Let $Z$ be a random vector defined as $Z = X - \\mu$. Since $X$ is a Gaussian random vector, any affine transformation of $X$ is also Gaussian. The mean of $Z$ is\n$$ \\mathbb{E}[Z] = \\mathbb{E}[X - \\mu] = \\mathbb{E}[X] - \\mu = \\mu - \\mu = 0. $$\nThe covariance matrix of $Z$ is\n$$ \\text{Cov}(Z) = \\text{Cov}(X - \\mu) = \\text{Cov}(X) = \\Sigma. $$\nTherefore, $Z$ follows a centered multivariate normal distribution, $Z \\sim \\mathcal{N}_{d}(0, \\Sigma)$. The probability we wish to compute can be expressed in terms of $Z$ as\n$$ \\mathbb{P}(Z^{\\top}\\Sigma^{-1}Z \\le c). $$\nThe quadratic form $Z^{\\top}\\Sigma^{-1}Z$ involves the inverse of the covariance matrix, which suggests a change of variables to simplify the expression. Since $\\Sigma$ is a symmetric and positive definite matrix, its inverse $\\Sigma^{-1}$ is also symmetric and positive definite. Consequently, there exists a unique symmetric positive definite matrix, denoted $\\Sigma^{-1/2}$, such that $(\\Sigma^{-1/2})^2 = \\Sigma^{-1}$.\nWe define a new random vector $U$ by the linear transformation\n$$ U = \\Sigma^{-1/2} Z. $$\nSince $Z$ is a Gaussian vector, $U$ is also a Gaussian vector. Its mean is\n$$ \\mathbb{E}[U] = \\mathbb{E}[\\Sigma^{-1/2} Z] = \\Sigma^{-1/2} \\mathbb{E}[Z] = \\Sigma^{-1/2} \\cdot 0 = 0. $$\nIts covariance matrix is\n$$ \\text{Cov}(U) = \\text{Cov}(\\Sigma^{-1/2} Z) = \\Sigma^{-1/2} \\text{Cov}(Z) (\\Sigma^{-1/2})^{\\top}. $$\nSince $\\Sigma^{-1/2}$ is symmetric, $(\\Sigma^{-1/2})^{\\top} = \\Sigma^{-1/2}$. Thus,\n$$ \\text{Cov}(U) = \\Sigma^{-1/2} \\Sigma \\Sigma^{-1/2}. $$\nRecognizing that $\\Sigma = (\\Sigma^{1/2})^{2}$ and $\\Sigma^{-1/2}\\Sigma^{1/2} = I_d$ (where $I_d$ is the $d \\times d$ identity matrix), we have\n$$ \\text{Cov}(U) = \\Sigma^{-1/2} (\\Sigma^{1/2}\\Sigma^{1/2}) \\Sigma^{-1/2} = (\\Sigma^{-1/2}\\Sigma^{1/2})(\\Sigma^{1/2}\\Sigma^{-1/2}) = I_d I_d = I_d. $$\nSo, the random vector $U$ follows a standard multivariate normal distribution, $U \\sim \\mathcal{N}_{d}(0, I_d)$. This implies that the components of $U$, denoted $U_1, U_2, \\dots, U_d$, are independent and identically distributed standard normal random variables, $U_i \\sim \\mathcal{N}(0, 1)$ for $i=1, \\dots, d$.\n\nNow, let's express the quadratic form $Z^{\\top}\\Sigma^{-1}Z$ in terms of $U$. From $U = \\Sigma^{-1/2} Z$, we have $Z = \\Sigma^{1/2} U$. Substituting this into the quadratic form gives\n$$ Z^{\\top}\\Sigma^{-1}Z = (\\Sigma^{1/2} U)^{\\top} \\Sigma^{-1} (\\Sigma^{1/2} U) = U^{\\top}(\\Sigma^{1/2})^{\\top} \\Sigma^{-1} \\Sigma^{1/2} U. $$\nSince $\\Sigma^{1/2}$ is symmetric, $(\\Sigma^{1/2})^{\\top} = \\Sigma^{1/2}$. Therefore,\n$$ Z^{\\top}\\Sigma^{-1}Z = U^{\\top}\\Sigma^{1/2} \\Sigma^{-1} \\Sigma^{1/2} U = U^{\\top} (\\Sigma^{1/2} (\\Sigma^{-1/2}\\Sigma^{-1/2}) \\Sigma^{1/2}) U = U^{\\top} I_d U = U^{\\top}U. $$\nThe term $U^{\\top}U$ is the sum of the squares of the components of $U$:\n$$ U^{\\top}U = \\sum_{i=1}^{d} U_i^2. $$\nLet the random variable $Y = (X-\\mu)^{\\top}\\Sigma^{-1}(X-\\mu)$. We have shown that $Y = \\sum_{i=1}^{d} U_i^2$, where $U_i$ are independent $\\mathcal{N}(0,1)$ random variables. By definition, a random variable that is the sum of the squares of $d$ independent standard normal variables follows a chi-squared distribution with $d$ degrees of freedom, denoted $Y \\sim \\chi^2_d$.\n\nThe problem is now reduced to finding $\\mathbb{P}(Y \\le c)$ where $Y \\sim \\chi^2_d$. This requires the probability density function (PDF) of the $\\chi^2_d$ distribution. The $\\chi^2_d$ distribution is a special case of the Gamma distribution, $\\Gamma(k, \\theta)$, with shape parameter $k = d/2$ and scale parameter $\\theta = 2$. The PDF of a $\\Gamma(k, \\theta)$ distributed random variable $y$ is given by\n$$ f(y; k, \\theta) = \\frac{y^{k-1} e^{-y/\\theta}}{\\theta^k \\Gamma(k)}, \\quad \\text{for } y  0, $$\nwhere $\\Gamma(k) = \\int_0^\\infty t^{k-1} e^{-t} dt$ is the Gamma function.\nSubstituting $k = d/2$ and $\\theta = 2$, the PDF of $Y \\sim \\chi^2_d$ is\n$$ f_Y(y) = \\frac{y^{(d/2) - 1} e^{-y/2}}{2^{d/2} \\Gamma(d/2)}, \\quad \\text{for } y  0. $$\nThe probability $\\mathbb{P}(Y \\le c)$ is the cumulative distribution function (CDF) of $Y$ evaluated at $c$. This is found by integrating the PDF from $0$ to $c$:\n$$ \\mathbb{P}(Y \\le c) = \\int_0^c f_Y(y) dy = \\int_0^c \\frac{y^{(d/2) - 1} e^{-y/2}}{2^{d/2} \\Gamma(d/2)} dy. $$\nTo evaluate this integral, we perform a change of variables. Let $t = y/2$. Then $y = 2t$ and $dy = 2dt$. The limits of integration change from $y=0$ to $t=0$ and from $y=c$ to $t=c/2$.\n$$ \\mathbb{P}(Y \\le c) = \\frac{1}{2^{d/2} \\Gamma(d/2)} \\int_0^{c/2} (2t)^{(d/2) - 1} e^{-t} (2dt) $$\n$$ = \\frac{1}{2^{d/2} \\Gamma(d/2)} \\int_0^{c/2} 2^{(d/2) - 1} t^{(d/2) - 1} e^{-t} \\cdot 2 dt $$\n$$ = \\frac{2^{d/2}}{2^{d/2} \\Gamma(d/2)} \\int_0^{c/2} t^{(d/2) - 1} e^{-t} dt $$\n$$ = \\frac{1}{\\Gamma(d/2)} \\int_0^{c/2} t^{d/2 - 1} e^{-t} dt. $$\nThe integral in this final expression is the definition of the lower incomplete gamma function, $\\gamma(s, x)$, defined as\n$$ \\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt. $$\nIn our case, the shape parameter is $s = d/2$ and the upper limit of integration is $x = c/2$. Therefore, the probability is\n$$ \\mathbb{P}(Y \\le c) = \\frac{\\gamma(d/2, c/2)}{\\Gamma(d/2)}. $$\nThis expression is known as the regularized lower incomplete gamma function, often denoted $P(s, x)$. It provides the required closed-form analytic expression for the probability content in terms of $d$ and $c$, using both the complete and incomplete Gamma functions, and contains no unevaluated integrals in the sense of the prompt.", "answer": "$$\\boxed{\\frac{\\gamma\\left(\\frac{d}{2}, \\frac{c}{2}\\right)}{\\Gamma\\left(\\frac{d}{2}\\right)}}$$", "id": "3068167"}]}