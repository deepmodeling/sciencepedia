{"hands_on_practices": [{"introduction": "The evolution of a Markov process is governed by the Chapman-Kolmogorov equation, which describes how a system transitions from one state to another over two consecutive time intervals. For Brownian motion, this principle manifests as a convolution of its transition densities. This practice [@problem_id:3049567] offers a concrete calculation to verify this property, demonstrating how the Gaussian distribution's \"semigroup\" nature under convolution is the analytical counterpart to adding independent random increments.", "problem": "Let $\\{X_{t}\\}_{t \\geq 0}$ be a one-dimensional standard Brownian motion, which is the canonical solution to the stochastic differential equation (SDE) $dX_{t} = dW_{t}$ with $X_{0} = x \\in \\mathbb{R}$, where $\\{W_{t}\\}_{t \\geq 0}$ denotes a standard Wiener process. Brownian motion has stationary and independent increments: for any $t,s  0$, the increment $X_{t+s} - X_{t}$ is independent of $\\{X_{u}\\}_{0 \\leq u \\leq t}$ and is normally distributed with mean $0$ and variance $s$. The transition density of a Gaussian with mean $0$ and variance $t$ evaluated at $u \\in \\mathbb{R}$ is given by\n$$\n\\phi_{t}(u) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\!\\left(-\\frac{u^{2}}{2t}\\right), \\quad t0.\n$$\nConsider the integral representation of the two-step transition via an intermediate spatial point $y \\in \\mathbb{R}$,\n$$\nI(t,s,x,z) = \\int_{-\\infty}^{\\infty} \\phi_{t}(y-x)\\,\\phi_{s}(z-y)\\,dy,\n$$\nfor fixed $t0$, $s0$, and $x,z \\in \\mathbb{R}$. Using only the fundamental properties of Gaussian densities and independence of increments, evaluate $I(t,s,x,z)$ explicitly as a simplified closed-form analytic expression in terms of $t$, $s$, $x$, and $z$. Your final answer must be a single analytic expression. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard calculation in the theory of stochastic processes involving the transition probability densities of Brownian motion. We are asked to evaluate the integral\n$$\nI(t,s,x,z) = \\int_{-\\infty}^{\\infty} \\phi_{t}(y-x)\\,\\phi_{s}(z-y)\\,dy,\n$$\nwhere $\\phi_{t}(u)$ is the probability density function of a Gaussian random variable with mean $0$ and variance $t  0$. The parameters $t, s, x, z$ are real numbers with $t0$ and $s0$.\n\nThis integral represents the Chapman-Kolmogorov equation for the transition density of a one-dimensional standard Brownian motion. Let $p(t, x, y)$ be the probability density of the process being at position $y$ at time $t$, given it started at position $x$ at time $0$. For a standard Brownian motion starting at $X_0=x$, the position $X_t$ is a random variable with a normal distribution of mean $x$ and variance $t$. Its density is given by $p(t,x,y) = \\frac{1}{\\sqrt{2\\pi t}} \\exp(-\\frac{(y-x)^2}{2t}) = \\phi_t(y-x)$. The integral $I(t,s,x,z)$ is thus a convolution of two such transition densities:\n$$\nI(t,s,x,z) = \\int_{-\\infty}^{\\infty} p(t,x,y) p(s,y,z) dy.\n$$\nThis calculates the probability density of transitioning from $x$ to $z$ in a total time of $t+s$ by passing through any intermediate point $y$ at time $t$. Due to the stationary and independent increments of Brownian motion, the total displacement $X_{t+s} - X_0$ is the sum of two independent increments, $(X_t - X_0)$ and $(X_{t+s} - X_t)$, which are normally distributed with variances $t$ and $s$, respectively. Their sum is therefore also a normal random variable with variance $t+s$. Thus, we expect the result to be the transition density for a time interval of $t+s$, which is $\\phi_{t+s}(z-x)$. We shall now verify this through direct computation of the integral.\n\nWe begin by substituting the explicit form of the Gaussian densities into the integral:\n$$\nI(t,s,x,z) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{(y-x)^{2}}{2t}\\right) \\frac{1}{\\sqrt{2\\pi s}} \\exp\\left(-\\frac{(z-y)^{2}}{2s}\\right) dy.\n$$\nWe can combine the constant pre-factors and the arguments of the exponential functions:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(y-x)^{2}}{2t} - \\frac{(z-y)^{2}}{2s} \\right) dy.\n$$\nLet us analyze the exponent, which we denote by $E(y)$:\n$$\nE(y) = -\\left( \\frac{(y-x)^{2}}{2t} + \\frac{(z-y)^{2}}{2s} \\right) = -\\frac{s(y-x)^{2} + t(z-y)^{2}}{2ts}.\n$$\nExpanding the squared terms in the numerator:\n$$\ns(y^{2} - 2xy + x^{2}) + t(z^{2} - 2zy + y^{2}) = sy^{2} - 2sxy + sx^{2} + tz^{2} - 2tzy + ty^{2}.\n$$\nWe group the terms based on powers of the integration variable $y$:\n$$\n(s+t)y^{2} - 2(sx+tz)y + (sx^{2}+tz^{2}).\n$$\nOur goal is to complete the square for the terms involving $y$. The general form of a quadratic $Ay^{2} - 2By + C$ can be written as $A(y-B/A)^2 + C - B^2/A$. Here, $A = s+t$ and $B = sx+tz$.\nSo, the quadratic in $y$ can be expressed as:\n$$\n(s+t)\\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{(sx+tz)^{2}}{s+t} + (sx^{2}+tz^{2}).\n$$\nLet's simplify the terms not involving $y$:\n\\begin{align*}\n(sx^{2}+tz^{2}) - \\frac{(sx+tz)^{2}}{s+t} = \\frac{(s+t)(sx^{2}+tz^{2}) - (s^{2}x^{2}+2stxz+t^{2}z^{2})}{s+t} \\\\\n= \\frac{(s^{2}x^{2}+stz^{2}+stx^{2}+t^{2}z^{2}) - (s^{2}x^{2}+2stxz+t^{2}z^{2})}{s+t} \\\\\n= \\frac{stx^{2} - 2stxz + stz^{2}}{s+t} \\\\\n= \\frac{st(x^{2}-2xz+z^{2})}{s+t} = \\frac{st(z-x)^{2}}{s+t}.\n\\end{align*}\nSubstituting this back into the expression for the exponent $E(y)$:\n$$\nE(y) = -\\frac{1}{2ts} \\left[ (s+t)\\left(y - \\frac{sx+tz}{s+t}\\right)^{2} + \\frac{st(z-x)^{2}}{s+t} \\right].\n$$\n$$\nE(y) = -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{st(z-x)^{2}}{2ts(s+t)} = -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{(z-x)^{2}}{2(s+t)}.\n$$\nNow, we substitute this form of the exponent back into the integral for $I(t,s,x,z)$:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} - \\frac{(z-x)^{2}}{2(s+t)} \\right) dy.\n$$\nThe term involving $(z-x)^{2}$ is constant with respect to $y$ and can be factored out of the integral:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{s+t}{2ts} \\left(y - \\frac{sx+tz}{s+t}\\right)^{2} \\right) dy.\n$$\nThe remaining integral is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-a(y-\\mu)^{2})dy = \\sqrt{\\frac{\\pi}{a}}$.\nIn our case, $a = \\frac{s+t}{2ts}$ and $\\mu = \\frac{sx+tz}{s+t}$. The value of the integral is:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{s+t}{2ts} \\left(y - \\mu\\right)^{2} \\right) dy = \\sqrt{\\frac{\\pi}{(s+t)/(2ts)}} = \\sqrt{\\frac{2\\pi ts}{s+t}}.\n$$\nSubstituting this result back into the expression for $I(t,s,x,z)$:\n$$\nI(t,s,x,z) = \\frac{1}{2\\pi\\sqrt{ts}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right) \\sqrt{\\frac{2\\pi ts}{s+t}}.\n$$\nSimplifying the expression by canceling terms:\n$$\nI(t,s,x,z) = \\frac{\\sqrt{2\\pi ts}}{2\\pi\\sqrt{ts}\\sqrt{s+t}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right) = \\frac{1}{\\sqrt{2\\pi(s+t)}} \\exp\\left(-\\frac{(z-x)^{2}}{2(s+t)}\\right).\n$$\nThis is the probability density function for a normal distribution with mean $0$ and variance $t+s$, evaluated at $z-x$. According to the problem's notation, this is $\\phi_{t+s}(z-x)$. The result matches our initial expectation based on the properties of Brownian motion, thus confirming the calculation.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi(t+s)}} \\exp\\left(-\\frac{(z-x)^{2}}{2(t+s)}\\right)}$$", "id": "3049567"}, {"introduction": "Often, we are interested not just in the position of a process at a single moment, but in a statistic that combines its values at multiple times. This exercise [@problem_id:3049614] provides a foundational technique for finding the distribution of such a statistic—in this case, a linear combination of a Brownian path's values. By working from first principles, you will use the defining covariance structure of Brownian motion to compute the exact Gaussian density of this new random variable.", "problem": "Let $\\{W_t\\}_{t \\ge 0}$ be a standard Brownian motion, meaning $W_0 = 0$, $\\mathbb{E}[W_t] = 0$ for all $t \\ge 0$, and $\\operatorname{Cov}(W_s, W_t) = \\min\\{s,t\\}$ for all $s,t \\ge 0$. Fix integers $n \\ge 1$, real times $0  t_1  t_2  \\dots  t_n$, and real coefficients $c_1, \\dots, c_n$ that are not all zero. Consider the linear functional\n$$\nL(W) \\;=\\; \\sum_{i=1}^{n} c_i\\, W_{t_i}.\n$$\nUsing only the defining properties of Brownian motion stated above, the linearity of expectation and covariance, and the well-tested fact that finite collections of Brownian motion values are jointly Gaussian so that linear functionals of them are Gaussian, compute the mean and variance of $L(W)$ from first principles via the covariance structure. Then, by the change-of-variables formula for probability densities applied to a Gaussian transformation, derive the exact analytic expression for the probability density function $f_L(x)$ of $L(W)$ as a function of $x \\in \\mathbb{R}$, in closed form in terms of $c_i$ and $t_i$.\n\nYour final answer must be a single closed-form expression for $f_L(x)$, written in terms of $x$, $\\{c_i\\}_{i=1}^{n}$, and $\\{t_i\\}_{i=1}^{n}$. Do not include any integrals in the final expression. No rounding is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It presents a standard exercise in the theory of stochastic processes concerning the properties of Brownian motion. All provided definitions and statements are correct and standard in the field. The problem is self-contained and free of contradictions or ambiguities.\n\nThe problem asks for the probability density function (PDF) of the random variable $L(W) = \\sum_{i=1}^{n} c_i W_{t_i}$. It is given that $L(W)$ is a a Gaussian (normal) random variable because it is a linear combination of jointly Gaussian random variables $(W_{t_1}, \\dots, W_{t_n})$. A Gaussian distribution is completely characterized by its mean and variance. Therefore, our first step is to compute these two parameters for $L(W)$.\n\nFirst, we compute the mean of $L(W)$, denoted by $\\mu_L$. Using the linearity of the expectation operator, we have:\n$$\n\\mu_L = \\mathbb{E}[L(W)] = \\mathbb{E}\\left[\\sum_{i=1}^{n} c_i W_{t_i}\\right] = \\sum_{i=1}^{n} c_i \\mathbb{E}[W_{t_i}]\n$$\nAccording to the problem definition, a standard Brownian motion has zero mean, i.e., $\\mathbb{E}[W_t] = 0$ for all $t \\ge 0$. Substituting this into our expression gives:\n$$\n\\mu_L = \\sum_{i=1}^{n} c_i (0) = 0\n$$\nThus, the mean of the random variable $L(W)$ is $0$.\n\nNext, we compute the variance of $L(W)$, denoted by $\\sigma_L^2$. The variance is defined as $\\operatorname{Var}(L(W)) = \\mathbb{E}[(L(W) - \\mu_L)^2]$. Since we found that $\\mu_L = 0$, the variance simplifies to the second moment:\n$$\n\\sigma_L^2 = \\mathbb{E}[L(W)^2]\n$$\nWe substitute the definition of $L(W)$:\n$$\n\\sigma_L^2 = \\mathbb{E}\\left[ \\left(\\sum_{i=1}^{n} c_i W_{t_i}\\right)^2 \\right] = \\mathbb{E}\\left[ \\left(\\sum_{i=1}^{n} c_i W_{t_i}\\right) \\left(\\sum_{j=1}^{n} c_j W_{t_j}\\right) \\right] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j W_{t_i} W_{t_j} \\right]\n$$\nBy the linearity of expectation, we can move the expectation operator inside the summations:\n$$\n\\sigma_L^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\mathbb{E}[W_{t_i} W_{t_j}]\n$$\nThe term $\\mathbb{E}[W_{t_i} W_{t_j}]$ is related to the covariance. The covariance between two random variables $X$ and $Y$ is $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$. For Brownian motion, $\\mathbb{E}[W_{t_i}]=0$ and $\\mathbb{E}[W_{t_j}]=0$, so $\\mathbb{E}[W_{t_i} W_{t_j}] = \\operatorname{Cov}(W_{t_i}, W_{t_j})$. The problem states that $\\operatorname{Cov}(W_s, W_t) = \\min\\{s,t\\}$. Therefore, $\\mathbb{E}[W_{t_i} W_{t_j}] = \\min\\{t_i, t_j\\}$.\nSubstituting this result into the expression for the variance yields:\n$$\n\\sigma_L^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}\n$$\nThis expression is a quadratic form $\\mathbf{c}^T \\Sigma \\mathbf{c}$, where $\\mathbf{c} = (c_1, \\dots, c_n)^T$ and $\\Sigma$ is the covariance matrix with entries $\\Sigma_{ij} = \\min\\{t_i, t_j\\}$. This matrix is known to be positive definite. Since the coefficients $c_i$ are not all zero, the vector $\\mathbf{c}$ is non-zero, which guarantees that $\\sigma_L^2 = \\mathbf{c}^T \\Sigma \\mathbf{c}  0$.\n\nNow that we have the mean $\\mu_L = 0$ and the variance $\\sigma_L^2$, we can determine the PDF of $L(W)$. We are given that $L(W)$ is a Gaussian random variable. Any Gaussian variable can be expressed as an affine transformation of a standard normal variable $Z \\sim N(0,1)$. The PDF of a standard normal variable $Z$ is $f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\nLet $x$ be a realization of the random variable $L(W)$. We can write $L(W) = \\mu_L + \\sigma_L Z$. Since $\\mu_L = 0$, we have $L(W) = \\sigma_L Z$. This is the \"Gaussian transformation\" mentioned in the problem, mapping the standard normal variable $Z$ to our variable $L(W)$.\nTo find the PDF $f_L(x)$ of $L(W)$, we use the change of variables formula for probability densities. The transformation is $x(z) = \\sigma_L z$. The inverse transformation is $z(x) = x/\\sigma_L$. The formula for the PDF is:\n$$\nf_L(x) = f_Z(z(x)) \\left| \\frac{dz}{dx} \\right|\n$$\nThe derivative is $\\frac{dz}{dx} = \\frac{1}{\\sigma_L}$. Since $\\sigma_L^2  0$, $\\sigma_L$ is a positive real number, so the absolute value is $|\\frac{1}{\\sigma_L}| = \\frac{1}{\\sigma_L}$.\nSubstituting the expressions for $z(x)$ and the Jacobian into the formula:\n$$\nf_L(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{x}{\\sigma_L}\\right)^2\\right) \\cdot \\frac{1}{\\sigma_L}\n$$\nSimplifying this gives the general form of a centered Gaussian PDF:\n$$\nf_L(x) = \\frac{1}{\\sigma_L \\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2\\sigma_L^2}\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma_L^2}} \\exp\\left(-\\frac{x^2}{2\\sigma_L^2}\\right)\n$$\nFinally, we substitute the full expression for the variance $\\sigma_L^2$ to obtain the final analytic expression for the PDF in terms of the given parameters:\n$$\nf_L(x) = \\frac{1}{\\sqrt{2\\pi \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}\\right)}} \\exp\\left( -\\frac{x^2}{2 \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}\\right)} \\right)\n$$\nThis is the closed-form expression for the probability density function of $L(W)$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}}} \\exp\\left( -\\frac{x^2}{2 \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_i c_j \\min\\{t_i, t_j\\}} \\right)}$$", "id": "3049614"}, {"introduction": "A central question in the application of SDEs is how to relate different models to one another, a concept formalized by the change of probability measure. This practice [@problem_id:3049572] provides a hands-on introduction to this powerful idea by deriving the likelihood ratio, or Radon-Nikodym derivative, between two diffusion models that differ only in their drift. This calculation is a cornerstone of statistical inference and parameter estimation for stochastic processes, showing how we can quantify the evidence for one model over another based on observed data.", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ denote a standard Brownian motion (also called a Wiener process). Consider two constant-coefficient diffusion models for a scalar process $\\{X_{t}\\}_{t \\geq 0}$:\n$$\ndX_{t}^{(i)} \\;=\\; \\mu_{i}\\,dt \\;+\\; \\sigma\\,dW_{t}, \\quad i \\in \\{1,2\\},\n$$\nwhere $\\mu_{1},\\mu_{2} \\in \\mathbb{R}$ are constant drifts, and $\\sigma0$ is a constant diffusion coefficient. You observe the process discretely over a fixed time increment $\\Delta0$ and record a single increment\n$$\nY \\;=\\; X_{t+\\Delta} - X_{t}.\n$$\nAssume the observation model is consistent with the properties of Brownian motion increments, so that under either drift, the increment distribution is Gaussian. Using the foundational properties of Brownian motion and diffusion processes, and the concept of change of measure formalized by Girsanov’s theorem (without invoking any shortcut formulas), derive the likelihood ratio (the Radon–Nikodym derivative) of the probability law induced by $dX_{t}=\\mu_{1}\\,dt+\\sigma\\,dW_{t}$ with respect to the law induced by $dX_{t}=\\mu_{2}\\,dt+\\sigma\\,dW_{t}$, based solely on the single discrete observation $Y$. Express your final answer as a single closed-form analytic expression in terms of $Y$, $\\mu_{1}$, $\\mu_{2}$, $\\sigma$, and $\\Delta$. No rounding is required, and no units are associated with the final expression.", "solution": "The problem asks for the likelihood ratio, which is the Radon-Nikodym derivative of the probability measure induced by the first diffusion model with respect to the second. Let $P_1$ be the probability measure under which the process $\\{X_t\\}$ follows $dX_{t} = \\mu_{1}\\,dt + \\sigma\\,dW_{t}^{(1)}$, and let $P_2$ be the measure under which it follows $dX_{t} = \\mu_{2}\\,dt + \\sigma\\,dW_{t}^{(2)}$. Here, $\\{W_{t}^{(1)}\\}_{t \\ge 0}$ is a standard Brownian motion under $P_1$, and $\\{W_{t}^{(2)}\\}_{t \\ge 0}$ is a standard Brownian motion under $P_2$. We seek to find the Radon-Nikodym derivative $\\frac{dP_1}{dP_2}$ evaluated for the single observation $Y = X_{t+\\Delta} - X_{t}$.\n\nWe will use Girsanov's theorem for the change of measure. Let's designate $P_2$ as the reference measure. Girsanov's theorem states that if $\\{W_s^{(2)}\\}_{s \\ge 0}$ is a $P_2$-Brownian motion and $\\{\\theta_s\\}_{s \\ge 0}$ is a suitably regular process (satisfying Novikov's condition), then there exists a measure $P_1$ equivalent to $P_2$ such that the process $\\{W_s^{(1)}\\}_{s \\ge 0}$ defined by\n$$\ndW_s^{(1)} = dW_s^{(2)} - \\theta_s ds\n$$\nis a $P_1$-Brownian motion. The relationship between the measures over a time horizon $[0, T]$ is given by the Radon-Nikodym derivative:\n$$\n\\frac{dP_1}{dP_2}\\bigg|_{\\mathcal{F}_T} = Z_T = \\exp\\left( \\int_0^T \\theta_s dW_s^{(2)} - \\frac{1}{2} \\int_0^T \\theta_s^2 ds \\right)\n$$\nwhere $\\mathcal{F}_T$ is the filtration generated by the process up to time $T$.\n\nOur goal is to find a process $\\theta_s$ that transforms the dynamics under $P_2$ into the dynamics under $P_1$.\nUnder the measure $P_1$, the process evolves as:\n$$\ndX_s = \\mu_1 ds + \\sigma dW_s^{(1)}\n$$\nWe substitute the relationship from Girsanov's theorem, $dW_s^{(1)} = dW_s^{(2)} - \\theta_s ds$:\n$$\ndX_s = \\mu_1 ds + \\sigma (dW_s^{(2)} - \\theta_s ds) = (\\mu_1 - \\sigma \\theta_s) ds + \\sigma dW_s^{(2)}\n$$\nWe require this process under measure $P_1$ to be identical to the process given in the problem statement. We equate this with the dynamics specified under the reference measure $P_2$:\n$$\ndX_s = \\mu_2 ds + \\sigma dW_s^{(2)}\n$$\nBy comparing the drift coefficients of the two expressions for $dX_s$ (which is a process on a single underlying probability space, whose law is interpreted differently under $P_1$ and $P_2$), we must have:\n$$\n\\mu_2 = \\mu_1 - \\sigma \\theta_s\n$$\nSolving for $\\theta_s$, we find:\n$$\n\\theta_s = \\frac{\\mu_1 - \\mu_2}{\\sigma}\n$$\nSince $\\mu_1$, $\\mu_2$, and $\\sigma$ are constants, $\\theta_s$ is a constant, which we denote by $\\theta$. This constant process trivially satisfies Novikov's condition, $E_{P_2}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T \\theta^2 ds\\right)\\right] = \\exp\\left(\\frac{1}{2}\\theta^2 T\\right)  \\infty$ for any finite $T$.\n\nThe observation $Y = X_{t+\\Delta} - X_t$ depends on the process over the time interval $[t, t+\\Delta]$. By the time-homogeneity of the process, we can analyze the interval $[0, \\Delta]$ without loss of generality. The Radon-Nikodym derivative for the laws of the process paths over this interval is given by $Z_\\Delta$:\n$$\nZ_\\Delta = \\exp\\left( \\int_0^\\Delta \\theta dW_s^{(2)} - \\frac{1}{2} \\int_0^\\Delta \\theta^2 ds \\right)\n$$\nWe need to express the two integrals in terms of the observation $Y = X_\\Delta - X_0$.\n\nFirst, the deterministic integral is straightforward:\n$$\n\\int_0^\\Delta \\theta^2 ds = \\theta^2 \\Delta = \\left(\\frac{\\mu_1 - \\mu_2}{\\sigma}\\right)^2 \\Delta\n$$\nNext, we evaluate the stochastic integral. Under the reference measure $P_2$, the SDE is $dX_s = \\mu_2 ds + \\sigma dW_s^{(2)}$. Integrating this from $s=0$ to $s=\\Delta$:\n$$\n\\int_0^\\Delta dX_s = \\int_0^\\Delta \\mu_2 ds + \\int_0^\\Delta \\sigma dW_s^{(2)}\n$$\n$$\nX_\\Delta - X_0 = \\mu_2 \\Delta + \\sigma (W_\\Delta^{(2)} - W_0^{(2)})\n$$\nAssuming the Brownian motion starts at $W_0^{(2)}=0$, and using the definition $Y=X_\\Delta - X_0$:\n$$\nY = \\mu_2 \\Delta + \\sigma W_\\Delta^{(2)}\n$$\nThe stochastic integral $\\int_0^\\Delta dW_s^{(2)}$ is simply $W_\\Delta^{(2)} - W_0^{(2)} = W_\\Delta^{(2)}$. We can solve for $W_\\Delta^{(2)}$ from the expression for $Y$:\n$$\nW_\\Delta^{(2)} = \\frac{Y - \\mu_2 \\Delta}{\\sigma}\n$$\nNow we can express the stochastic integral term in $Z_\\Delta$ in terms of $Y$:\n$$\n\\int_0^\\Delta \\theta dW_s^{(2)} = \\theta \\int_0^\\Delta dW_s^{(2)} = \\theta W_\\Delta^{(2)} = \\left(\\frac{\\mu_1 - \\mu_2}{\\sigma}\\right) \\left(\\frac{Y - \\mu_2 \\Delta}{\\sigma}\\right) = \\frac{(\\mu_1 - \\mu_2)(Y - \\mu_2 \\Delta)}{\\sigma^2}\n$$\nSubstituting the expressions for both integrals back into the formula for $Z_\\Delta$:\n$$\nZ_\\Delta = \\exp\\left( \\frac{(\\mu_1 - \\mu_2)(Y - \\mu_2 \\Delta)}{\\sigma^2} - \\frac{1}{2}\\left(\\frac{\\mu_1 - \\mu_2}{\\sigma}\\right)^2 \\Delta \\right)\n$$\nThis expression $Z_\\Delta$ is a random variable that depends on $Y$. It represents the Radon-Nikodym derivative of the measures induced on the space of possible outcomes for $Y$. Let $\\nu_1$ and $\\nu_2$ be the probability laws of the random variable $Y$ under $P_1$ and $P_2$, respectively. The relationship is $d\\nu_1(y) = Z_\\Delta(y) d\\nu_2(y)$. Thus, $Z_\\Delta$ is the likelihood ratio we seek. We simplify the exponent for the final expression:\n\\begin{align*}\n\\text{Exponent} = \\frac{(\\mu_1 - \\mu_2)Y - (\\mu_1 - \\mu_2)\\mu_2 \\Delta}{\\sigma^2} - \\frac{(\\mu_1 - \\mu_2)^2 \\Delta}{2\\sigma^2} \\\\\n= \\frac{(\\mu_1 - \\mu_2)Y}{\\sigma^2} - \\frac{\\Delta}{\\sigma^2} \\left[ (\\mu_1 - \\mu_2)\\mu_2 + \\frac{1}{2}(\\mu_1 - \\mu_2)^2 \\right] \\\\\n= \\frac{(\\mu_1 - \\mu_2)Y}{\\sigma^2} - \\frac{\\Delta}{\\sigma^2} \\left[ \\mu_1\\mu_2 - \\mu_2^2 + \\frac{1}{2}(\\mu_1^2 - 2\\mu_1\\mu_2 + \\mu_2^2) \\right] \\\\\n= \\frac{(\\mu_1 - \\mu_2)Y}{\\sigma^2} - \\frac{\\Delta}{\\sigma^2} \\left[ \\frac{1}{2}\\mu_1^2 - \\frac{1}{2}\\mu_2^2 \\right] \\\\\n= \\frac{(\\mu_1 - \\mu_2)Y}{\\sigma^2} - \\frac{(\\mu_1^2 - \\mu_2^2)\\Delta}{2\\sigma^2}\n\\end{align*}\nTherefore, the likelihood ratio is:\n$$\n\\frac{d P_1}{d P_2}(Y) = \\exp\\left( \\frac{(\\mu_1 - \\mu_2)Y}{\\sigma^2} - \\frac{(\\mu_1^2 - \\mu_2^2)\\Delta}{2\\sigma^2} \\right)\n$$\nThis derivation is based on first principles using Girsanov's theorem as required.", "answer": "$$\\boxed{\\exp\\left(\\frac{(\\mu_{1} - \\mu_{2})Y}{\\sigma^{2}} - \\frac{(\\mu_{1}^{2} - \\mu_{2}^{2})\\Delta}{2\\sigma^{2}}\\right)}$$", "id": "3049572"}]}