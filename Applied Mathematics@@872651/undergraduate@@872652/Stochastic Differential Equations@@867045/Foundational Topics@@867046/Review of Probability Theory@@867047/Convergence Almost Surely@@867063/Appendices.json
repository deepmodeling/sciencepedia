{"hands_on_practices": [{"introduction": "The concept of almost sure convergence often addresses whether events in an infinite sequence occur a finite or infinite number of times. The first Borel-Cantelli lemma provides a powerful criterion for this, stating that if the probabilities of events diminish quickly enough, we can be almost certain they will eventually stop happening. This exercise presents an intuitive carnival game scenario to demonstrate how this powerful theorem works in practice, showing that you will almost surely stop winning after a finite number of attempts [@problem_id:1352851].", "problem": "A new carnival game, known as the \"Diminishing Returns Challenge,\" involves a sequence of independent trials, indexed by the positive integers $n = 1, 2, 3, \\ldots$. The probability of winning the $n$-th trial is given by $p_n = \\frac{1}{n^2}$. A player attempts every trial in the sequence.\n\nAssuming the player continues to play indefinitely, what is the probability that the total number of wins they accumulate is finite?\n\nThe final answer should be a single real number.", "solution": "Let $A_{n}$ be the event that the player wins on trial $n$. By the problem, $\\Pr(A_{n})=p_{n}=\\frac{1}{n^{2}}$, and the $A_{n}$ are independent. Consider the series\n$$\n\\sum_{n=1}^{\\infty}\\Pr(A_{n})=\\sum_{n=1}^{\\infty}\\frac{1}{n^{2}}=\\frac{\\pi^{2}}{6}\\infty.\n$$\nBy the first Borel-Cantelli lemma, if $\\sum_{n=1}^{\\infty}\\Pr(A_{n})\\infty$, then\n$$\n\\Pr(A_{n}\\ \\text{i.o.})=0,\n$$\nwhere $A_{n}$ i.o. denotes that infinitely many of the events $A_{n}$ occur. Therefore, with probability $1$, only finitely many $A_{n}$ occur, which is exactly the event that the total number of wins is finite. Hence\n$$\n\\Pr(\\text{total number of wins is finite})=1-\\Pr(A_{n}\\ \\text{i.o.})=1-0=1.\n$$", "answer": "$$\\boxed{1}$$", "id": "1352851"}, {"introduction": "In contrast to events that are destined to cease, some are guaranteed to recur infinitely often. This is the domain of the second Borel-Cantelli lemma, which applies when event probabilities do not decrease rapidly enough. This practice explores the running maximum of a sequence of random variables, a perfect setting to understand why some processes are almost surely guaranteed to break their previous records and grow unbounded over time [@problem_id:1352847].", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ be a sequence of independent and identically distributed (i.i.d.) random variables. Each $X_n$ follows a standard Normal distribution, denoted by $N(0, 1)$, which has a mean of 0 and a variance of 1.\n\nDefine a new sequence of random variables $\\{M_n\\}_{n=1}^{\\infty}$ where $M_n$ is the maximum of the first $n$ variables in the sequence:\n$$M_n = \\max(X_1, X_2, \\dots, X_n)$$\nWe are interested in the long-term behavior of this maximum value as $n$ approaches infinity. Which of the following statements correctly describes the almost sure convergence of the sequence $M_n$?\n\nA. $M_n$ converges almost surely to 0.\n\nB. $M_n$ converges almost surely to a finite, non-zero constant.\n\nC. $M_n$ converges almost surely to a non-degenerate random variable (i.e., a random variable that is not a constant).\n\nD. $M_n$ does not converge almost surely to any finite limit; instead, it almost surely diverges to infinity.\n\nE. The sequence $M_n$ fails to converge, with its limit superior being a finite constant and its limit inferior being $-\\infty$ almost surely.", "solution": "The problem asks for the almost sure limiting behavior of the sequence $M_n = \\max(X_1, X_2, \\dots, X_n)$, where $X_i$ are i.i.d. standard normal random variables.\n\nWe want to determine if $M_n$ converges to a finite limit almost surely. A sequence of random variables $Y_n$ converges almost surely to a limit $Y$ if $P(\\lim_{n \\to \\infty} Y_n = Y) = 1$. If $M_n$ were to converge to a finite limit $L$ almost surely, it would imply that for almost every outcome $\\omega$ in the sample space, the sequence of real numbers $M_n(\\omega)$ converges to $L$. A necessary condition for a real sequence to converge is that it must be bounded. We will show that the sequence $M_n$ is almost surely unbounded, which will rule out convergence to any finite limit.\n\nTo do this, we can use the second Borel-Cantelli Lemma. This lemma states that for a sequence of independent events $\\{A_n\\}_{n=1}^{\\infty}$, if the sum of their probabilities diverges, i.e., $\\sum_{n=1}^{\\infty} P(A_n) = \\infty$, then the probability that infinitely many of these events occur is 1.\n\nLet's fix an arbitrary, large positive number $K$. Define the event $A_n$ as $A_n = \\{X_n  K\\}$. This event corresponds to the $n$-th observation exceeding the threshold $K$.\n\nWe check the conditions of the second Borel-Cantelli Lemma for the sequence of events $\\{A_n\\}$:\n1.  **Independence**: The random variables $X_n$ are independent by definition. Therefore, the events $A_n = \\{X_n  K\\}$, which depend only on $X_n$, are also independent.\n2.  **Sum of Probabilities**: The random variables are identically distributed, so $P(A_n) = P(X_n  K)$ is a constant for all $n$. Let this probability be $p = P(X_1  K)$. The standard normal distribution $N(0,1)$ has a probability density function $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$, which is positive for all real $x$. This means that for any finite value $K$, the probability of observing a value greater than $K$ is strictly positive. So, $p = P(X_1  K) = \\int_K^\\infty f(x) dx  0$.\n\nNow we consider the sum of the probabilities:\n$$ \\sum_{n=1}^{\\infty} P(A_n) = \\sum_{n=1}^{\\infty} p $$\nSince $p$ is a positive constant, this sum is a sum of infinitely many identical positive numbers, which diverges to infinity: $\\sum_{n=1}^{\\infty} p = \\infty$.\n\nSince the events $\\{A_n\\}$ are independent and the sum of their probabilities diverges, the second Borel-Cantelli Lemma applies. It tells us that the probability of $A_n$ occurring infinitely often is 1.\n$$ P(A_n \\text{ i.o.}) = 1 $$\nThis means that, with probability 1, the event $\\{X_n  K\\}$ happens for infinitely many values of $n$.\n\nNow we relate this back to the maximum, $M_n$. If, for a given outcome, $X_n  K$ for infinitely many $n$, then the sequence of maximums $M_n = \\max(X_1, \\dots, X_n)$ must eventually exceed $K$. More strongly, the sequence $M_n$ cannot be bounded by $K$. Every time an $X_m  K$ occurs, $M_m$ will be at least $K$. As this happens infinitely often, the sequence $M_n$ cannot converge to any value less than or equal to $K$.\n\nThis argument holds for any choice of a finite number $K$, no matter how large. We can choose $K=100$, $K=1000$, or any other large number. For each such $K$, it is almost certain that $M_n$ will eventually exceed it. This implies that the sequence $M_n$ is almost surely unbounded.\n\nA sequence that is unbounded cannot converge to a finite limit. Therefore, options A and B are incorrect.\nThe sequence $M_n$ is a non-decreasing sequence, because $M_{n+1} = \\max(M_n, X_{n+1}) \\ge M_n$. A non-decreasing sequence of real numbers always has a limit, which is either a finite number or $+\\infty$. Since we have shown that $M_n$ is almost surely unbounded, it cannot converge to a finite limit. Therefore, it must diverge to $+\\infty$ almost surely.\nSo, $\\lim_{n \\to \\infty} M_n = \\infty$ almost surely.\n\nThis means that $M_n$ does not converge to a non-degenerate random variable (Option C) or have a finite limit superior (Option E). The correct statement is that $M_n$ almost surely diverges to infinity.\n\nThus, option D is the correct description.", "answer": "$$\\boxed{D}$$", "id": "1352847"}, {"introduction": "A crucial distinction in probability theory lies between the convergence of random variables themselves and the convergence of their expectations. A sequence can vanish for almost every outcome, yet its average value can remain stubbornly constant. This classic exercise guides you through constructing such a sequence, often visualized as a \"traveling bump,\" to solidify your understanding of what \"almost surely\" truly means and how it differs from other modes of convergence like convergence in expectation [@problem_id:1352897].", "problem": "Consider a sequence of non-negative random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on the probability space $(\\Omega, \\mathcal{F}, P)$, where the sample space is the unit interval $\\Omega = [0, 1]$, $\\mathcal{F}$ is the Borel sigma-algebra on $[0, 1]$, and $P$ is the Lebesgue measure, which corresponds to a uniform probability distribution on the interval.\n\nWhich of the following constructions defines a sequence that satisfies BOTH of the following properties?\n1. The expected value of each random variable is one, i.e., $E[X_n] = 1$ for all $n \\ge 1$.\n2. The sequence of random variables converges almost surely to zero, i.e., $P(\\{\\omega \\in \\Omega : \\lim_{n \\to \\infty} X_n(\\omega) = 0\\}) = 1$.\n\nLet $\\mathbf{1}_A(\\omega)$ denote the indicator function for a set $A \\subseteq [0,1]$, which is 1 if $\\omega \\in A$ and 0 if $\\omega \\notin A$.\n\nA. $X_n(\\omega) = n \\cdot \\mathbf{1}_{[0, 1/n]}(\\omega)$\n\nB. $X_n(\\omega) = n^{2} \\cdot \\mathbf{1}_{[0, 1/n^2]}(\\omega)$\n\nC. The sequence of intervals $\\{A_n\\}_{n=1}^\\infty$ is constructed by first taking $A_1 = [0,1]$, then $A_2=[0,1/2], A_3=[1/2,1]$, then $A_4=[0,1/3], A_5=[1/3,2/3], A_6=[2/3,1]$, and so on, by partitioning $[0,1]$ into $k$ disjoint intervals of length $1/k$ for $k=1, 2, 3, \\ldots$ and listing them sequentially. Let $L_n$ be the length of the interval $A_n$. The random variable is defined as $X_n(\\omega) = \\frac{1}{L_n} \\mathbf{1}_{A_n}(\\omega)$.\n\nD. $X_n(\\omega) = \\mathbf{1}_{[0, 1]}(\\omega)$ for all $n$.", "solution": "We check each construction against the two properties.\n\nGeneral facts used:\n- For $X_{n}(\\omega)=c_{n}\\mathbf{1}_{A_{n}}(\\omega)$ with $A_{n}\\subseteq[0,1]$ Borel and $c_{n}\\ge 0$, the expectation is\n$$\nE[X_{n}]=\\int_{0}^{1}X_{n}(\\omega)\\,dP(\\omega)=c_{n}P(A_{n}).\n$$\n- Almost sure convergence to zero requires $P\\big(\\{\\omega:\\lim_{n\\to\\infty}X_{n}(\\omega)=0\\}\\big)=1$. Sets of Lebesgue measure zero (e.g., singletons) do not affect almost sure statements.\n\nOption A: $X_{n}(\\omega)=n\\,\\mathbf{1}_{[0,1/n]}(\\omega)$.\n- Expectation:\n$$\nE[X_{n}]=n\\cdot P([0,1/n])=n\\cdot \\frac{1}{n}=1.\n$$\n- Almost sure convergence: For any fixed $\\omega0$, there are only finitely many $n$ such that $\\omega\\in[0,1/n]$ (equivalently $1/n\\ge\\omega$, i.e., $n\\le 1/\\omega$), hence $X_{n}(\\omega)=0$ for all sufficiently large $n$, so $\\lim_{n\\to\\infty}X_{n}(\\omega)=0$. For $\\omega=0$, $X_{n}(0)=n\\to\\infty$, but $P(\\{0\\})=0$. Therefore $X_{n}\\to 0$ almost surely. Thus A satisfies both properties.\n\nOption B: $X_{n}(\\omega)=n^{2}\\,\\mathbf{1}_{[0,1/n^{2}]}(\\omega)$.\n- Expectation:\n$$\nE[X_{n}]=n^{2}\\cdot P([0,1/n^{2}])=n^{2}\\cdot \\frac{1}{n^{2}}=1.\n$$\n- Almost sure convergence: For any fixed $\\omega0$, $\\omega\\in[0,1/n^{2}]$ holds only for finitely many $n$ (since $1/n^{2}\\ge\\omega$ implies $n\\le \\omega^{-1/2}$), so $X_{n}(\\omega)=0$ eventually and $\\lim_{n\\to\\infty}X_{n}(\\omega)=0$. For $\\omega=0$, $X_{n}(0)=n^{2}\\to\\infty$, but $P(\\{0\\})=0$. Hence $X_{n}\\to 0$ almost surely. Thus B satisfies both properties.\n\nOption C: $A_{n}$ enumerates all intervals of the equipartitions of $[0,1]$ into $k$ intervals of length $1/k$, for $k=1,2,\\ldots$, and $X_{n}(\\omega)=\\frac{1}{L_{n}}\\mathbf{1}_{A_{n}}(\\omega)$ with $L_{n}=\\text{length}(A_{n})$.\n- Expectation:\n$$\nE[X_{n}]=\\frac{1}{L_{n}}P(A_{n})=\\frac{1}{L_{n}}\\cdot L_{n}=1.\n$$\n- Almost sure convergence: Fix $\\omega\\in[0,1]$. For each $k$, exactly one interval in the partition of size $k$ contains $\\omega$, say $A_{n_{k}}$, with $L_{n_{k}}=1/k$. Then\n$$\nX_{n_{k}}(\\omega)=\\frac{1}{L_{n_{k}}}=k\\to\\infty.\n$$\nFor other $n$, $X_{n}(\\omega)=0$ when $\\omega\\notin A_{n}$. Hence $X_{n}(\\omega)$ does not converge to $0$ (indeed it does not converge at all). Therefore property 2 fails. C is invalid.\n\nOption D: $X_{n}(\\omega)=\\mathbf{1}_{[0,1]}(\\omega)$ for all $n$.\n- Expectation:\n$$\nE[X_{n}]=P([0,1])=1.\n$$\n- Almost sure convergence: $X_{n}(\\omega)=1$ for all $n$ and all $\\omega$, so $\\lim_{n\\to\\infty}X_{n}(\\omega)=1\\neq 0$. Therefore property 2 fails. D is invalid.\n\nThus the constructions that satisfy both properties are A and B.", "answer": "$$\\boxed{\\text{A and B}}$$", "id": "1352897"}]}