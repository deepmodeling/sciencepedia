{"hands_on_practices": [{"introduction": "The concept of quadratic variation is a cornerstone of stochastic calculus, fundamentally distinguishing it from ordinary calculus where continuous paths have zero variation. This practice solidifies your understanding in two ways: first, by proving the uniqueness of the quadratic variation process $[M]_t$ as the component that makes $M_t^2 - [M]_t$ a martingale, a result known as the Doob-Meyer decomposition. Second, it gives you direct experience in applying the theory to compute the quadratic variation for a process defined by a common type of stochastic differential equation. [@problem_id:3070787]", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\geq 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions, and let $(W_t)_{t \\geq 0}$ be a standard Brownian motion. A continuous local martingale $(M_t)_{t \\geq 0}$ with $M_0=0$ admits a quadratic variation process $[M]_t$, defined almost surely as the limit along partitions of $[0,t]$ of mesh size tending to $0$ of the sum of squared increments of $M$. A continuous semimartingale $(X_t)_{t \\geq 0}$ is a process that can be written as the sum of a continuous local martingale and a continuous finite variation process. \n\nTask (i): Starting from the core definitions of martingale and quadratic variation and the consequences of Itô calculus, show that for any continuous local martingale $(M_t)_{t \\geq 0}$, the quadratic variation $[M]_t$ is, almost surely, the unique continuous increasing adapted process with $[M]_0=0$ such that the process $M_t^2 - [M]_t$ is a martingale. In particular, argue carefully why no other continuous increasing adapted process can replace $[M]_t$ in this property.\n\nTask (ii): Consider the Stochastic Differential Equation (SDE) \n$$\ndX_t = \\sin(t)\\,dt + \\exp(-t)\\,dW_t, \\quad X_0 = x_0,\n$$\nwhere $x_0 \\in \\mathbb{R}$ is deterministic. Using only foundational facts about quadratic variation of continuous finite variation processes and stochastic integrals, compute the quadratic variation $[X]_t$ for $t \\geq 0$ and give it as a single closed-form analytic expression in $t$. Provide your final expression for $[X]_t$ only; do not include an equation sign in your final answer.", "solution": "The problem is divided into two parts. Part (i) is a theoretical proof regarding the characterization of quadratic variation, and part (ii) is a computation of the quadratic variation for a specific stochastic process.\n\nPart (i): Show that for any continuous local martingale $(M_t)_{t \\geq 0}$, the quadratic variation $[M]_t$ is, almost surely, the unique continuous increasing adapted process with $[M]_0=0$ such that the process $M_t^2 - [M]_t$ is a martingale.\n\nLet $(M_t)_{t \\geq 0}$ be a continuous local martingale with $M_0=0$ on the filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\geq 0},\\mathbb{P})$. We first establish the existence of such a process, namely $[M]_t$, and then prove its uniqueness.\n\nExistence:\nWe apply Itô's formula to the process $M_t$ and the function $f(x) = x^2$. The function $f$ is twice continuously differentiable, with $f'(x) = 2x$ and $f''(x) = 2$. Itô's formula for a continuous semimartingale $X_t$ applied to $f(x)$ states:\n$$f(X_t) = f(X_0) + \\int_0^t f'(X_s) \\, dX_s + \\frac{1}{2} \\int_0^t f''(X_s) \\, d[X]_s$$\nSince $M_t$ is a continuous local martingale, it is also a continuous semimartingale. Applying the formula to $M_t$:\n$$f(M_t) = f(M_0) + \\int_0^t f'(M_s) \\, dM_s + \\frac{1}{2} \\int_0^t f''(M_s) \\, d[M]_s$$\nSubstituting $f(x)=x^2$ and its derivatives, and using the initial condition $M_0=0$ so $f(M_0)=0^2=0$:\n$$M_t^2 = \\int_0^t 2M_s \\, dM_s + \\frac{1}{2} \\int_0^t 2 \\, d[M]_s$$\nThe second integral simplifies to $\\int_0^t d[M]_s = [M]_t - [M]_0$. By definition, the quadratic variation $[M]_t$ starts at $[M]_0=0$. Thus, the equation becomes:\n$$M_t^2 = 2 \\int_0^t M_s \\, dM_s + [M]_t$$\nRearranging the terms, we get:\n$$M_t^2 - [M]_t = 2 \\int_0^t M_s \\, dM_s$$\nThe term on the right-hand side, $N_t = \\int_0^t M_s \\, dM_s$, is a stochastic integral of an adapted continuous process $M_s$ (which is therefore predictable) with respect to the continuous local martingale $M_t$. A fundamental property of stochastic integrals is that such an integral is itself a continuous local martingale, with $N_0=0$. Consequently, $M_t^2 - [M]_t$ is a continuous local martingale.\nThe problem statement uses the term \"martingale\". For a general continuous local martingale $M_t$, the process $M_t^2 - [M]_t$ is a continuous local martingale. It is a true martingale under stronger conditions, for example, if $M_t$ is a martingale in $L^2$ (i.e., $\\sup_{t \\geq 0} \\mathbb{E}[M_t^2] < \\infty$). The uniqueness property, however, holds with the weaker \"local martingale\" condition.\n\nUniqueness:\nLet $(A_t)_{t \\geq 0}$ be any other process with the same properties as $[M]_t$. That is, $A_t$ is a continuous, increasing, and adapted process with $A_0=0$, and it is such that the process $Y_t = M_t^2 - A_t$ is a continuous local martingale.\n\nWe have established that $Z_t = M_t^2 - [M]_t$ is also a continuous local martingale.\nConsider the difference process $D_t = Z_t - Y_t$:\n$$D_t = (M_t^2 - [M]_t) - (M_t^2 - A_t) = A_t - [M]_t$$\nSince $Z_t$ and $Y_t$ are both continuous local martingales, their difference $D_t$ must also be a continuous local martingale.\nFurthermore, the process $D_t$ is a process of finite variation. This is because both $[M]_t$ (by definition) and $A_t$ (by assumption) are continuous increasing processes. The difference of two increasing processes is a process of finite variation.\nSo, $D_t = A_t - [M]_t$ is a continuous local martingale that is also of finite variation. It also starts at $D_0 = A_0 - [M]_0 = 0 - 0 = 0$.\n\nA cornerstone result in stochastic calculus states that any continuous local martingale of finite variation starting at zero must be identically zero almost surely. To see this, let $D_t$ be such a process. Since $D_t$ is of finite variation, its quadratic variation $[D]_t$ is zero for all $t \\geq 0$. For a continuous local martingale, the expectation of its quadratic variation is related to its second moment. Specifically, for any stopping time $\\tau$, $\\mathbb{E}[(D_{t \\land \\tau})^2] = \\mathbb{E}[[D]_{t \\land \\tau}]$. In our case, $[D]_t = 0$, so $\\mathbb{E}[D_{t \\land \\tau}^2] = \\mathbb{E}[0] = 0$. By choosing a localizing sequence of stopping times, we can conclude that $D_t = 0$ almost surely for all $t \\geq 0$.\n\nSince $D_t = A_t - [M]_t = 0$ almost surely for all $t \\geq 0$, it follows that $A_t = [M]_t$ almost surely for all $t \\geq 0$. This proves that the quadratic variation process $[M]_t$ is the unique process satisfying the given properties. This characterization is known as the Doob-Meyer decomposition of the submartingale $M_t^2$.\n\nPart (ii): Compute the quadratic variation $[X]_t$ for the process given by the SDE $dX_t = \\sin(t)\\,dt + \\exp(-t)\\,dW_t$.\n\nThe process $X_t$ is given in differential form. Its integral form is:\n$$X_t = X_0 + \\int_0^t \\sin(s)\\,ds + \\int_0^t \\exp(-s)\\,dW_s$$\nwhere the initial condition $X_0 = x_0$ is a deterministic constant.\nThe process $X_t$ is a continuous semimartingale, which can be decomposed as $X_t = B_t + M_t$, where $B_t$ is a continuous finite variation process and $M_t$ is a continuous local martingale.\nIn this case, we can identify:\n- The finite variation part: $B_t = x_0 + \\int_0^t \\sin(s)\\,ds$. This process is continuous and adapted. Its variation on $[0,t]$ is given by $\\int_0^t |\\sin(s)|\\,ds$, which is finite for any finite $t$.\n- The local martingale part: $M_t = \\int_0^t \\exp(-s)\\,dW_s$. This is an Itô integral with respect to Brownian motion, and is therefore a continuous local martingale.\n\nThe quadratic variation of a semimartingale $X_t = B_t + M_t$ is defined as:\n$$[X]_t = [B+M]_t = [B]_t + 2[B,M]_t + [M]_t$$\nWe use two fundamental properties of quadratic variation:\n1. The quadratic variation of a continuous process of finite variation is zero. Therefore, $[B]_t = 0$.\n2. The cross-variation between a continuous local martingale and any continuous process of finite variation is zero. Therefore, $[B,M]_t = 0$.\n\nSubstituting these into the expression for $[X]_t$, we get:\n$$[X]_t = 0 + 2(0) + [M]_t = [M]_t$$\nThus, the quadratic variation of the semimartingale $X_t$ is equal to the quadratic variation of its local martingale part $M_t$.\nThe local martingale part is $M_t = \\int_0^t \\exp(-s)\\,dW_s$. For an Itô integral of the form $\\int_0^t \\sigma_s \\,dW_s$, the quadratic variation is given by the formula:\n$$[M]_t = \\int_0^t \\sigma_s^2 \\,ds$$\nIn our problem, the volatility process is $\\sigma_s = \\exp(-s)$.\nTherefore, the quadratic variation of $X_t$ is:\n$$[X]_t = \\int_0^t (\\exp(-s))^2 \\,ds = \\int_0^t \\exp(-2s) \\,ds$$\nWe now evaluate this definite integral:\n$$\\int_0^t \\exp(-2s) \\,ds = \\left[ -\\frac{1}{2} \\exp(-2s) \\right]_0^t$$\n$$= \\left( -\\frac{1}{2} \\exp(-2t) \\right) - \\left( -\\frac{1}{2} \\exp(-2 \\cdot 0) \\right)$$\n$$= -\\frac{1}{2} \\exp(-2t) - \\left( -\\frac{1}{2} \\exp(0) \\right)$$\n$$= -\\frac{1}{2} \\exp(-2t) + \\frac{1}{2}$$\n$$= \\frac{1}{2} (1 - \\exp(-2t))$$\nThis is the closed-form analytic expression for the quadratic variation $[X]_t$.", "answer": "$$\\boxed{\\frac{1}{2}(1 - \\exp(-2t))}$$", "id": "3070787"}, {"introduction": "Stochastic differential equations are powerful tools for modeling dynamics constrained to a surface or manifold, not just free movement in Euclidean space. This exercise challenges you to apply Itô's lemma to investigate which SDEs correctly describe a diffusion on the unit circle, a foundational example of a geometric stochastic process. Successfully solving it requires a firm grasp of the Itô correction term and highlights the crucial difference between the Itô and Stratonovich formulations of a stochastic integral. [@problem_id:3070785]", "problem": "Let $T>0$. Consider the metric space $C([0,T];\\mathbb{R}^2)$ equipped with the uniform metric $d(f,g)=\\sup_{t\\in[0,T]}\\|f(t)-g(t)\\|$. The support of a probability measure $\\mu$ on a metric space $(E,d)$ is the set of points $x\\in E$ such that every open ball $B(x,\\varepsilon)$ with radius $\\varepsilon>0$ has strictly positive $\\mu$-measure. Let $x_0\\in\\mathbb{R}^2$ satisfy $\\|x_0\\|=1$. Let $W$ denote a $1$-dimensional standard Brownian motion and let $J=\\begin{pmatrix}0 & -1\\\\ 1 & 0\\end{pmatrix}$ be the rotation by $90^\\circ$. A diffusion is called degenerate if its diffusion matrix $a(x)=\\sigma(x)\\sigma(x)^\\top$ is singular for all relevant $x$.\n\nWhich of the following stochastic differential equations (SDEs) on $\\mathbb{R}^2$ define a degenerate diffusion starting at $X_0=x_0$ whose induced law on $C([0,T];\\mathbb{R}^2)$ has support contained in the set of continuous paths that remain on the unit circle $S^1=\\{x\\in\\mathbb{R}^2:\\|x\\|=1\\}$ for all $t\\in[0,T]$?\n\nA. Itô SDE: $dX_t=JX_t\\,dW_t$.\n\nB. Stratonovich SDE: $dX_t=JX_t\\circ dW_t$.\n\nC. Itô SDE: $dX_t=-\\frac{1}{2}X_t\\,dt+JX_t\\,dW_t$.\n\nD. Reflecting Brownian motion in the closed unit disk: $dX_t=dW^{(2)}_t+n(X_t)\\,dL_t$, where $W^{(2)}$ is a $2$-dimensional standard Brownian motion, $n(x)$ is the inward unit normal at $x$ on the boundary $\\{x:\\|x\\|=1\\}$, and $L_t$ is the boundary local time increasing only when $\\|X_t\\|=1$.", "solution": "The problem asks to identify which of the given stochastic differential equations (SDEs) define a process $X_t$ starting from $X_0=x_0$ on the unit circle $S^1$ that satisfies two conditions:\n1. The diffusion is degenerate.\n2. The support of the induced probability measure on the space of continuous paths $C([0,T];\\mathbb{R}^2)$ is contained in the set of paths that remain on the unit circle, $S^1 = \\{x \\in \\mathbb{R}^2 : \\|x\\|=1\\}$.\n\nLet's validate the problem statement first.\nThe problem is well-posed and self-contained. It provides all necessary definitions (metric space, support of a measure, degenerate diffusion) and parameters ($x_0, J$). The concepts are standard in the theory of stochastic differential equations. There are no scientific or logical inconsistencies in the problem statement. The problem is valid.\n\nWe will analyze each option against the two conditions.\n\n**Condition 1: Degenerate Diffusion**\nA diffusion is defined as degenerate if its diffusion matrix $a(x) = \\sigma(x)\\sigma(x)^\\top$ is singular. The process $X_t$ takes values in $\\mathbb{R}^2$.\n\nFor options A, B, and C, the SDE is driven by a $1$-dimensional standard Brownian motion $W_t$. The general form is $dX_t = b(X_t) dt + \\sigma(X_t) dW_t$, where $\\sigma(x)$ is a $2 \\times 1$ matrix (a column vector). The diffusion matrix is $a(x) = \\sigma(x)\\sigma(x)^\\top$, which is a $2 \\times 2$ matrix. The rank of $a(x)$ is at most the rank of $\\sigma(x)$, which is at most $1$. Any $2 \\times 2$ matrix with rank less than $2$ is singular. Thus, the diffusions defined by SDEs A, B, and C are all degenerate.\n\nFor option D, the SDE is $dX_t=dW^{(2)}_t+n(X_t)\\,dL_t$. The diffusion part is driven by a $2$-dimensional standard Brownian motion $W^{(2)}_t$. This corresponds to a diffusion coefficient matrix $\\sigma(x) = I$, where $I$ is the $2 \\times 2$ identity matrix. The diffusion matrix is $a(x) = \\sigma(x)\\sigma(x)^\\top = I I^\\top = I$. The determinant of the identity matrix is $1$, which is non-zero. Therefore, the diffusion is non-degenerate.\n\nBased on the first condition, option D is incorrect. We proceed to check options A, B, and C against the second condition.\n\n**Condition 2: Support of the Law on the Unit Circle**\nThe set of continuous paths that remain on the unit circle, let's call it $C_{S^1} = \\{\\gamma \\in C([0,T];\\mathbb{R}^2) : \\|\\gamma(t)\\|=1 \\text{ for all } t \\in [0,T]\\}$, is a closed set under the uniform metric. If we can show that for a process $X_t$, the probability of its paths remaining on the circle is $1$, i.e., $\\mathbb{P}(X_\\cdot \\in C_{S^1}) = 1$, then the support of its law must be contained in $C_{S^1}$.\nThis is equivalent to showing that if $\\|X_0\\|=1$, then $\\|X_t\\|=1$ for all $t > 0$ almost surely. Let $f(x) = \\|x\\|^2 = x_1^2+x_2^2$. We need to check if $d(f(X_t)) = d(\\|X_t\\|^2) = 0$.\n\nLet's analyze the remaining options. For all these options, we have the matrix $J=\\begin{pmatrix}0 & -1\\\\ 1 & 0\\end{pmatrix}$. A key property is that for any vector $x \\in \\mathbb{R}^2$, the vectors $x$ and $Jx$ are orthogonal, meaning $x^\\top Jx=0$. Also, $\\|Jx\\|^2 = \\|x\\|^2$ since $J$ is an orthogonal matrix.\n\n**Option A: Itô SDE: $dX_t=JX_t\\,dW_t$.**\nThis is an Itô SDE with drift $b(x)=0$ and diffusion coefficient $\\sigma(x)=Jx$. We use Itô's formula for $f(x)=\\|x\\|^2$. The gradient is $\\nabla f(x) = 2x$ and the Hessian is $H_f(x) = 2I$.\n$$d\\|X_t\\|^2 = \\nabla f(X_t)^\\top dX_t + \\frac{1}{2}\\text{Tr}\\left(\\sigma(X_t)\\sigma(X_t)^\\top H_f(X_t)\\right)\\,dt$$\n$$d\\|X_t\\|^2 = (2X_t^\\top)(JX_t\\,dW_t) + \\frac{1}{2}\\text{Tr}\\left((JX_t)(JX_t)^\\top(2I)\\right)\\,dt$$\nThe first term is $2(X_t^\\top JX_t)dW_t = 0 \\cdot dW_t = 0$.\nThe second term is $\\text{Tr}((JX_t)(JX_t)^\\top)\\,dt = \\|JX_t\\|^2\\,dt = \\|X_t\\|^2\\,dt$.\nSo, we have $d\\|X_t\\|^2 = \\|X_t\\|^2\\,dt$.\nGiven the initial condition $\\|X_0\\|^2=1$, this ordinary differential equation for $\\|X_t\\|^2$ has the solution $\\|X_t\\|^2=e^t$. The norm of the process grows exponentially, so the paths do not stay on the unit circle.\n**Verdict for A: Incorrect.**\n\n**Option B: Stratonovich SDE: $dX_t=JX_t\\circ dW_t$.**\nFor a Stratonovich SDE, we can use the Stratonovich chain rule, which follows the rules of ordinary calculus. For $f(x)=\\|x\\|^2$:\n$$d\\|X_t\\|^2 = \\nabla f(X_t)^\\top \\circ dX_t$$\n$$d\\|X_t\\|^2 = 2X_t^\\top \\circ (JX_t \\circ dW_t) = (2X_t^\\top JX_t) \\circ dW_t$$\nAs shown before, $X_t^\\top JX_t=0$ for any $X_t \\in \\mathbb{R}^2$. Thus, the process $2X_t^\\top JX_t$ is identically zero.\n$$d\\|X_t\\|^2 = 0 \\circ dW_t = 0$$\nThis means $\\|X_t\\|^2$ is a conserved quantity. Since $\\|X_0\\|^2=1$, it follows that $\\|X_t\\|^2=1$ for all $t$. The process remains on the unit circle. Since it also defines a degenerate diffusion, option B satisfies both conditions.\n**Verdict for B: Correct.**\n\n**Option C: Itô SDE: $dX_t=-\\frac{1}{2}X_t\\,dt+JX_t\\,dW_t$.**\nThis is an Itô SDE with drift $b(x)=-\\frac{1}{2}x$ and diffusion $\\sigma(x)=Jx$. We apply Itô's formula for $f(x)=\\|x\\|^2$:\n$$d\\|X_t\\|^2 = \\nabla f(X_t)^\\top dX_t + \\frac{1}{2}\\text{Tr}\\left(\\sigma(X_t)\\sigma(X_t)^\\top H_f(X_t)\\right)\\,dt$$\n$$d\\|X_t\\|^2 = (2X_t^\\top)\\left(-\\frac{1}{2}X_t\\,dt+JX_t\\,dW_t\\right) + \\frac{1}{2}\\text{Tr}\\left((JX_t)(JX_t)^\\top(2I)\\right)\\,dt$$\n$$d\\|X_t\\|^2 = -X_t^\\top X_t\\,dt + 2X_t^\\top JX_t\\,dW_t + \\|JX_t\\|^2\\,dt$$\nUsing $X_t^\\top X_t = \\|X_t\\|^2$, $X_t^\\top JX_t = 0$, and $\\|JX_t\\|^2 = \\|X_t\\|^2$, this simplifies to:\n$$d\\|X_t\\|^2 = -\\|X_t\\|^2\\,dt + 0\\cdot dW_t + \\|X_t\\|^2\\,dt = 0$$\nThus, $\\|X_t\\|^2$ is conserved. With $\\|X_0\\|^2=1$, all paths remain on the unit circle. The diffusion is degenerate. Option C satisfies both conditions. (Note: The SDE in C is the Itô representation of the Stratonovich SDE in B, so they describe the same process.)\n**Verdict for C: Correct.**\n\n**Option D: Reflecting Brownian motion in the closed unit disk.**\nAs determined earlier, this SDE defines a non-degenerate diffusion, so it fails the first condition. Furthermore, the process explores the interior of the unit disk, not just its boundary. A path of a reflecting Brownian motion in a disk is not confined to the boundary circle for all $t \\in [0,T]$. So it fails the second condition as well.\n**Verdict for D: Incorrect.**\n\nIn summary, options B and C both describe a degenerate diffusion whose paths, starting on the unit circle, remain on the unit circle for all time.", "answer": "$$\\boxed{BC}$$", "id": "3070785"}, {"introduction": "When we alter a parameter in a stochastic model, how \"different\" is the resulting probability law of the solution? This question is vital in fields ranging from financial modeling to statistical inference. This exercise provides hands-on practice in quantifying this difference by calculating two fundamental metrics: the total variation distance and the Kullback-Leibler divergence. [@problem_id:3070782] By working through the example of two shifted Gaussian measures, you will gain intuition for how these measures of distance behave and relate to each other.", "problem": "Consider the probability measures $P_{0}$ and $P_{\\theta}$ on $\\mathbb{R}$, where $P_{0}$ is the standard normal distribution $N(0,1)$ and $P_{\\theta}$ is the normal distribution $N(\\theta,1)$ with a location shift $\\theta \\in \\mathbb{R}$. In the context of comparing laws of stochastic processes (such as solutions to stochastic differential equations) under parameter perturbations, two foundational notions are the total variation distance and the Kullback–Leibler divergence. Using only the core definitions of probability measures, their densities, and these divergences, compute the exact total variation distance $\\mathrm{TV}(P_{0},P_{\\theta})$ and the exact Kullback–Leibler divergence $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ (from $P_{0}$ to $P_{\\theta}$). Then, derive the leading-order small-$\\theta$ asymptotic term for each of these quantities as $\\theta \\to 0$. Your derivation must start from the definitions\n$$\\mathrm{TV}(P_{0},P_{\\theta})=\\sup_{A \\in \\mathcal{B}(\\mathbb{R})}\\big|P_{0}(A)-P_{\\theta}(A)\\big|$$\nand\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta})=\\int_{\\mathbb{R}} p_{0}(x)\\,\\ln\\!\\left(\\frac{p_{0}(x)}{p_{\\theta}(x)}\\right)\\,\\mathrm{d}x,$$\nwhere $p_{0}$ and $p_{\\theta}$ are the Lebesgue densities of $P_{0}$ and $P_{\\theta}$, respectively, and $\\mathcal{B}(\\mathbb{R})$ denotes the Borel $\\sigma$-algebra on $\\mathbb{R}$. Express your final answer as a single row matrix containing, in order: the exact $\\mathrm{TV}(P_{0},P_{\\theta})$, the exact $\\mathrm{KL}(P_{0}\\|P_{\\theta})$, the leading-order term of $\\mathrm{TV}(P_{0},P_{\\theta})$ as $\\theta \\to 0$, and the leading-order term of $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ as $\\theta \\to 0$. No rounding is required, and no physical units are involved. Define any special functions you use in your solution, and do not invoke any prepackaged formulas beyond the stated definitions.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard, verifiable calculation in probability theory and information theory. All necessary definitions and conditions are provided, and there are no internal contradictions or ambiguities. The problem is therefore deemed valid and a solution will be provided.\n\nThe problem asks for the computation of the total variation distance $\\mathrm{TV}(P_{0},P_{\\theta})$ and the Kullback–Leibler divergence $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ between two normal distributions, $P_{0} \\sim N(0,1)$ and $P_{\\theta} \\sim N(\\theta,1)$, along with their leading-order asymptotics as $\\theta \\to 0$.\n\nThe Lebesgue densities for $P_0$ and $P_\\theta$ are, respectively:\n$$p_{0}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\n$$p_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)$$\n\n**1. Total Variation Distance**\n\nThe total variation distance is defined as:\n$$\\mathrm{TV}(P_{0},P_{\\theta})=\\sup_{A \\in \\mathcal{B}(\\mathbb{R})}\\big|P_{0}(A)-P_{\\theta}(A)\\big|$$\nwhere $P(A) = \\int_A p(x)\\,\\mathrm{d}x$. This can be written as:\n$$\\mathrm{TV}(P_{0},P_{\\theta})=\\sup_{A \\in \\mathcal{B}(\\mathbb{R})}\\left|\\int_{A} \\left(p_{0}(x) - p_{\\theta}(x)\\right)\\,\\mathrm{d}x\\right|$$\nThe supremum is attained on the set $A^* = \\{x \\in \\mathbb{R} : p_{0}(x) \\ge p_{\\theta}(x)\\}$. On this set, the absolute value is unnecessary. Thus,\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\int_{A^*} \\left(p_{0}(x) - p_{\\theta}(x)\\right)\\,\\mathrm{d}x$$\nThis is also equal to the well-known formula $\\frac{1}{2}\\int_{\\mathbb{R}} |p_0(x) - p_\\theta(x)| \\, \\mathrm{d}x$. Let's proceed from the maximizing set $A^*$.\n\nWe determine the set $A^*$ by solving the inequality $p_{0}(x) \\ge p_{\\theta}(x)$:\n$$\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) \\ge \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)$$\n$$-\\frac{x^2}{2} \\ge -\\frac{(x-\\theta)^2}{2}$$\n$$x^2 \\le (x-\\theta)^2 = x^2 - 2x\\theta + \\theta^2$$\n$$0 \\le -2x\\theta + \\theta^2 \\implies 2x\\theta \\le \\theta^2$$\nIf $\\theta > 0$, this simplifies to $x \\le \\frac{\\theta}{2}$. So, $A^* = (-\\infty, \\theta/2]$.\nIf $\\theta < 0$, this simplifies to $x \\ge \\frac{\\theta}{2}$. So, $A^* = [\\theta/2, \\infty)$.\nIf $\\theta = 0$, the inequality is $0 \\le 0$, which is true for all $x$, so $p_0(x) = p_\\theta(x)$ and $\\mathrm{TV}=0$.\n\nCase 1: $\\theta > 0$.\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\int_{-\\infty}^{\\theta/2} p_{0}(x)\\,\\mathrm{d}x - \\int_{-\\infty}^{\\theta/2} p_{\\theta}(x)\\,\\mathrm{d}x$$\nLet $\\Phi(z) = \\int_{-\\infty}^{z} p_{0}(t)\\,\\mathrm{d}t$ be the cumulative distribution function (CDF) of the standard normal distribution. The first term is simply $\\Phi(\\theta/2)$. For the second term, we change variables with $u = x-\\theta$, $\\mathrm{d}u=\\mathrm{d}x$:\n$$\\int_{-\\infty}^{\\theta/2} p_{\\theta}(x)\\,\\mathrm{d}x = \\int_{-\\infty}^{\\theta/2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)\\,\\mathrm{d}x = \\int_{-\\infty}^{-\\theta/2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right)\\,\\mathrm{d}u = \\Phi(-\\theta/2)$$\nSo, for $\\theta > 0$, $\\mathrm{TV}(P_{0},P_{\\theta}) = \\Phi(\\theta/2) - \\Phi(-\\theta/2)$.\nUsing the symmetry property $\\Phi(-z) = 1 - \\Phi(z)$, this becomes:\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\Phi(\\theta/2) - (1 - \\Phi(\\theta/2)) = 2\\Phi(\\theta/2) - 1$$\n\nCase 2: $\\theta < 0$. $A^* = [\\theta/2, \\infty)$.\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = \\int_{\\theta/2}^{\\infty} p_{0}(x)\\,\\mathrm{d}x - \\int_{\\theta/2}^{\\infty} p_{\\theta}(x)\\,\\mathrm{d}x$$\nThis equals $(1 - \\Phi(\\theta/2)) - (1 - \\Phi(\\theta/2 - \\theta)) = \\Phi(-\\theta/2) - \\Phi(\\theta/2)$.\nSince $\\theta < 0$, $|\\theta| = -\\theta$. The expression becomes $\\Phi(|\\theta|/2) - \\Phi(-|\\theta|/2) = 2\\Phi(|\\theta|/2) - 1$.\n\nThus, for any $\\theta \\in \\mathbb{R}$, the exact total variation distance is:\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = 2\\Phi(|\\theta|/2) - 1$$\n\nTo find the asymptotic behavior as $\\theta \\to 0$, we perform a Taylor expansion of $\\Phi(z)$ around $z=0$:\n$$\\Phi(z) = \\Phi(0) + \\Phi'(0)z + O(z^3)$$\nWe have $\\Phi(0) = 1/2$. The derivative is $\\Phi'(z) = p_{0}(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$.\nSo, $\\Phi'(0) = p_{0}(0) = \\frac{1}{\\sqrt{2\\pi}}$. The second derivative $\\Phi''(z) = p_0'(z) = -z p_0(z)$ is zero at $z=0$, so the next term is of order $z^3$.\n$$\\Phi(z) = \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}z + O(z^3)$$\nSubstituting $z=|\\theta|/2$:\n$$\\mathrm{TV}(P_{0},P_{\\theta}) = 2\\left(\\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}\\frac{|\\theta|}{2} + O(|\\theta|^3)\\right) - 1 = 1 + \\frac{|\\theta|}{\\sqrt{2\\pi}} + O(|\\theta|^3) - 1 = \\frac{|\\theta|}{\\sqrt{2\\pi}} + O(|\\theta|^3)$$\nThe leading-order term for $\\mathrm{TV}(P_{0},P_{\\theta})$ as $\\theta \\to 0$ is $\\frac{|\\theta|}{\\sqrt{2\\pi}}$.\n\n**2. Kullback-Leibler Divergence**\n\nThe Kullback-Leibler divergence is defined as:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta})=\\int_{\\mathbb{R}} p_{0}(x)\\,\\ln\\!\\left(\\frac{p_{0}(x)}{p_{\\theta}(x)}\\right)\\,\\mathrm{d}x$$\nFirst, we compute the logarithm of the ratio of the densities:\n$$\\frac{p_{0}(x)}{p_{\\theta}(x)} = \\frac{\\exp(-x^2/2)}{\\exp(-(x-\\theta)^2/2)} = \\exp\\left(-\\frac{x^2}{2} + \\frac{(x-\\theta)^2}{2}\\right) = \\exp\\left(-\\frac{x^2}{2} + \\frac{x^2 - 2x\\theta + \\theta^2}{2}\\right)$$\n$$\\frac{p_{0}(x)}{p_{\\theta}(x)} = \\exp\\left(\\frac{-2x\\theta + \\theta^2}{2}\\right) = \\exp\\left(-x\\theta + \\frac{\\theta^2}{2}\\right)$$\nTaking the natural logarithm:\n$$\\ln\\left(\\frac{p_{0}(x)}{p_{\\theta}(x)}\\right) = -x\\theta + \\frac{\\theta^2}{2}$$\nNow, we substitute this into the integral definition of KL divergence:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta}) = \\int_{\\mathbb{R}} p_{0}(x) \\left(-x\\theta + \\frac{\\theta^2}{2}\\right)\\,\\mathrm{d}x$$\nWe can split the integral due to linearity:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta}) = -\\theta \\int_{\\mathbb{R}} x p_{0}(x)\\,\\mathrm{d}x + \\frac{\\theta^2}{2} \\int_{\\mathbb{R}} p_{0}(x)\\,\\mathrm{d}x$$\nThe first integral, $\\int_{\\mathbb{R}} x p_{0}(x)\\,\\mathrm{d}x$, is the expected value of a random variable following the standard normal distribution $N(0,1)$, which is $0$.\nThe second integral, $\\int_{\\mathbb{R}} p_{0}(x)\\,\\mathrm{d}x$, is the total probability, which is $1$.\nSubstituting these values:\n$$\\mathrm{KL}(P_{0}\\|P_{\\theta}) = -\\theta \\cdot 0 + \\frac{\\theta^2}{2} \\cdot 1 = \\frac{\\theta^2}{2}$$\nThis is the exact expression for the KL divergence.\n\nFor the asymptotic behavior as $\\theta \\to 0$, the exact expression is a polynomial in $\\theta$. The leading-order term is the lowest power term, which is the expression itself.\nThe leading-order term for $\\mathrm{KL}(P_{0}\\|P_{\\theta})$ as $\\theta \\to 0$ is $\\frac{\\theta^2}{2}$.\n\n**Summary of Results**\n- Exact total variation distance: $2\\Phi(|\\theta|/2) - 1$, where $\\Phi$ is the CDF of the standard normal distribution.\n- Exact Kullback-Leibler divergence: $\\frac{\\theta^2}{2}$.\n- Leading-order term of TV distance as $\\theta \\to 0$: $\\frac{|\\theta|}{\\sqrt{2\\pi}}$.\n- Leading-order term of KL divergence as $\\theta \\to 0$: $\\frac{\\theta^2}{2}$.\nThese four results will be presented in the final answer as a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2\\Phi(|\\theta|/2) - 1 & \\frac{\\theta^2}{2} & \\frac{|\\theta|}{\\sqrt{2\\pi}} & \\frac{\\theta^2}{2}\n\\end{pmatrix}\n}\n$$", "id": "3070782"}]}