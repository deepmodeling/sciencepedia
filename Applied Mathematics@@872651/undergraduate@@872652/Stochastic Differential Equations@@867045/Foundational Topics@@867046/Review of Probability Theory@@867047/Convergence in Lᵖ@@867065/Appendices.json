{"hands_on_practices": [{"introduction": "Understanding the hierarchy of convergence modes is foundational in probability theory. This exercise explores the subtle relationship between convergence in probability and convergence in $L^p$. By constructing a specific sequence of random variables [@problem_id:3046407], you will demonstrate that convergence in probability, even when paired with uniformly bounded $L^p$ moments, is not sufficient to guarantee $L^p$ convergence. This classic counterexample is instrumental in revealing the crucial role of uniform integrability, a condition that prevents the \"mass\" of the probability distribution from escaping to infinity.", "problem": "Let $p \\geq 1$ be fixed. In the study of stochastic differential equations (SDE), one often considers convergence of terminal random variables in various senses. Recall the following foundational definitions:\n- Convergence in probability: a sequence of random variables $X_{n}$ converges to $X$ in probability if for every $\\varepsilon  0$, $\\mathbb{P}\\left(|X_{n} - X|  \\varepsilon\\right) \\to 0$ as $n \\to \\infty$.\n- Convergence in $L^{p}$: a sequence $X_{n}$ converges to $X$ in $L^{p}$ if $\\mathbb{E}\\left(|X_{n} - X|^{p}\\right) \\to 0$ as $n \\to \\infty$.\n- Uniform integrability: a family $\\{Y_{n}\\}$ of integrable random variables is uniformly integrable if $\\lim_{M \\to \\infty} \\sup_{n} \\mathbb{E}\\left(|Y_{n}| \\,\\mathbf{1}_{\\{|Y_{n}|  M\\}}\\right) = 0$, where $\\mathbf{1}_{A}$ denotes the indicator of the event $A$.\n\nConstruct, on a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ with $\\Omega = [0,1]$ and $\\mathbb{P}$ the Lebesgue measure, an explicit sequence $\\{X_{n}\\}_{n \\geq 1}$ and a limit $X$ such that $X_{n} \\to X$ in probability and $\\sup_{n} \\mathbb{E}\\left(|X_{n}|^{p}\\right)  \\infty$, but $X_{n} \\not\\to X$ in $L^{p}$. Explain, in terms of the definition of uniform integrability, why the failure of $\\{|X_{n}|^{p}\\}$ to be uniformly integrable obstructs $L^{p}$ convergence despite bounded $p$-th moments.\n\nFinally, compute the value of the limit superior\n$$\n\\limsup_{n \\to \\infty} \\mathbb{E}\\left(|X_{n} - X|^{p}\\right),\n$$\nand present your answer as a single real number. No rounding is required.", "solution": "The problem requires the construction of a sequence of random variables $\\{X_n\\}_{n \\ge 1}$ on the probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ where $\\Omega = [0,1]$, $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $[0,1]$, and $\\mathbb{P}$ is the Lebesgue measure. This sequence must satisfy several specified properties concerning convergence. We will construct such a sequence, verify that it meets all the stated criteria, provide an explanation for its convergence behavior based on the concept of uniform integrability, and finally compute the requested limit superior for our constructed sequence.\n\nFirst, we establish the framework for our construction. Let the limit random variable be the zero function, $X(\\omega) = 0$ for all $\\omega \\in [0,1]$. We define the sequence of random variables $\\{X_n\\}_{n \\ge 1}$ on this space as a \"traveling spike\" that becomes progressively taller and narrower. For any fixed $p \\ge 1$, we define $X_n$ for $n \\ge 1$ as:\n$$\nX_n(\\omega) = n^{1/p} \\mathbf{1}_{[0, 1/n]}(\\omega)\n$$\nThis means $X_n(\\omega) = n^{1/p}$ if $\\omega \\in [0, 1/n]$ and $X_n(\\omega) = 0$ if $\\omega \\in (1/n, 1]$.\n\nWe now verify that this sequence and its limit $X=0$ satisfy all the required properties.\n\n1.  **Convergence in probability**: We must show that $X_n \\to 0$ in probability, which means $\\lim_{n \\to \\infty} \\mathbb{P}(|X_n - 0|  \\varepsilon) = 0$ for every $\\varepsilon  0$.\n    For any given $\\varepsilon  0$, consider the inequality $|X_n(\\omega)|  \\varepsilon$, which is $n^{1/p}  \\varepsilon$. Since $p \\ge 1$ is fixed, the term $n^{1/p}$ grows to infinity as $n \\to \\infty$. Therefore, we can find an integer $N$ such that for all $n  N$, we have $n^{1/p}  \\varepsilon$. For any such $n$, the set of $\\omega$ where $|X_n(\\omega)|  \\varepsilon$ is precisely the set where $X_n(\\omega) = n^{1/p}$, which is the interval $[0, 1/n]$.\n    The probability of this event is its Lebesgue measure:\n    $$\n    \\mathbb{P}(|X_n|  \\varepsilon) = \\mathbb{P}([0, 1/n]) = \\frac{1}{n}\n    $$\n    As $n \\to \\infty$, we have $\\mathbb{P}(|X_n|  \\varepsilon) = \\frac{1}{n} \\to 0$. Thus, $X_n \\to 0$ in probability.\n\n2.  **Boundedness of $p$-th moments**: We need to show that $\\sup_n \\mathbb{E}[|X_n|^p]  \\infty$.\n    We compute the expectation of $|X_n|^p$ by integrating over $\\Omega = [0,1]$:\n    $$\n    \\mathbb{E}[|X_n|^p] = \\int_0^1 |X_n(\\omega)|^p \\,d\\omega = \\int_0^{1/n} (n^{1/p})^p \\,d\\omega + \\int_{1/n}^1 0^p \\,d\\omega = \\int_0^{1/n} n \\,d\\omega = n \\cdot \\left(\\frac{1}{n} - 0\\right) = 1\n    $$\n    Since $\\mathbb{E}[|X_n|^p] = 1$ for all integers $n \\ge 1$, the supremum of these values is $\\sup_n \\mathbb{E}[|X_n|^p] = 1$, which is finite.\n\n3.  **Failure of convergence in $L^p$**: We must demonstrate that $X_n$ does not converge to $X=0$ in $L^p$. This requires showing that $\\lim_{n \\to \\infty} \\mathbb{E}[|X_n - 0|^p] \\neq 0$.\n    From the previous calculation, we have that $\\mathbb{E}[|X_n - 0|^p] = \\mathbb{E}[|X_n|^p] = 1$ for all $n \\ge 1$.\n    Therefore, the limit is:\n    $$\n    \\lim_{n \\to \\infty} \\mathbb{E}[|X_n - 0|^p] = \\lim_{n \\to \\infty} 1 = 1\n    $$\n    Since this limit is $1$ and not $0$, the sequence $\\{X_n\\}$ does not converge to $0$ in $L^p$.\n\nNext, we explain why the failure of uniform integrability obstructs $L^p$ convergence. The Vitali Convergence Theorem states that for a sequence of random variables $\\{Z_n\\}$ in $L^1$, convergence $Z_n \\to Z$ in $L^1$ holds if and only if $Z_n \\to Z$ in probability and the family $\\{Z_n\\}$ is uniformly integrable. Let $Y_n = |X_n - X|^p = |X_n|^p$. Then $L^p$ convergence of $X_n$ to $X$ is equivalent to $L^1$ convergence of $Y_n$ to $Y=0$. We have established that $Y_n \\to 0$ in probability. According to the theorem, the failure of $L^1$ convergence (and thus $L^p$ convergence) must be due to the fact that the family $\\{Y_n\\} = \\{|X_n|^p\\}$ is not uniformly integrable.\n\nLet's show this explicitly. By definition, $\\{Y_n\\}$ is uniformly integrable if $\\lim_{M \\to \\infty} \\sup_n \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n|  M\\}}] = 0$. In our construction, $Y_n(\\omega) = n \\cdot \\mathbf{1}_{[0, 1/n]}(\\omega)$. The random variable $Y_n$ only takes values $n$ or $0$.\nThe condition $|Y_n|  M$ is met only if $n  M$.\n- If $n \\le M$, the set $\\{\\omega: |Y_n(\\omega)|  M\\}$ is empty, so $\\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n|  M\\}}] = 0$.\n- If $n  M$, the set $\\{\\omega: |Y_n(\\omega)|  M\\}$ is the interval $[0, 1/n]$. For such $n$:\n  $$\n  \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n|  M\\}}] = \\mathbb{E}[n \\cdot \\mathbf{1}_{[0, 1/n]}] = n \\cdot \\mathbb{P}([0, 1/n]) = n \\cdot \\frac{1}{n} = 1\n  $$\nFor any $M  0$, we can always find an integer $n  M$. Thus, the supremum over all $n$ will always include values of $1$:\n$$\n\\sup_n \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n|  M\\}}] = \\sup(\\{0 \\text{ for } n \\le M\\} \\cup \\{1 \\text{ for } n  M\\}) = 1\n$$\nTaking the limit as $M \\to \\infty$:\n$$\n\\lim_{M \\to \\infty} \\sup_n \\mathbb{E}[|Y_n| \\mathbf{1}_{\\{|Y_n|  M\\}}] = \\lim_{M \\to \\infty} 1 = 1\n$$\nSince this limit is not $0$, the family $\\{|X_n|^p\\}$ is not uniformly integrable. This failure reflects that a persistent amount of the integral's mass ($1$ in this case) comes from arbitrarily large values of $|X_n|^p$, even as the probability of observing these values shrinks. This \"escaping mass\" prevents the total integral from converging to $0$.\n\nFinally, we compute the value of the requested limit superior for our constructed sequence. We need to find $\\limsup_{n \\to \\infty} \\mathbb{E}[|X_n - X|^p]$.\nAs we have shown, for our choice of $X_n$ and $X=0$, the expectation is constant for all $n \\ge 1$:\n$$\n\\mathbb{E}[|X_n - X|^p] = 1\n$$\nThe limit superior of a constant sequence $\\{1, 1, 1, \\dots\\}$ is simply the value of the constant.\n$$\n\\limsup_{n \\to \\infty} \\mathbb{E}[|X_n - X|^p] = \\limsup_{n \\to \\infty} 1 = 1\n$$", "answer": "$$\\boxed{1}$$", "id": "3046407"}, {"introduction": "We now apply these foundational concepts to the realm of stochastic calculus. This practice investigates the convergence of stochastic convolutions, a common structure in the solution of stochastic differential equations. You will analyze a sequence of Itô integrals where the deterministic kernels converge pointwise to zero, which might naively suggest that the integrals themselves should also converge to zero. By working through this problem [@problem_id:3046413], you will discover that pointwise convergence is insufficient and see how the $L^2$ norm of the kernel governs the behavior of the resulting stochastic integral.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,1]},\\mathbb{P})$ be a filtered probability space supporting a standard Wiener process (also called standard Brownian motion) $W=(W_{t})_{t\\in[0,1]}$. For each integer $n\\geq 1$ and each $t\\in(0,1]$, define the deterministic kernel\n$$\nK_{n}(t,s) \\coloneqq \\sqrt{n}\\,\\mathbf{1}_{(t-\\frac{1}{n},\\,t]}(s),\\qquad s\\in[0,t],\n$$\nand the associated stochastic convolution\n$$\nX_{n}(t) \\coloneqq \\int_{0}^{t} K_{n}(t,s)\\,\\mathrm{d}W_{s}.\n$$\nIt is known that the Itô stochastic integral is well defined for deterministic integrands that are square-integrable on $[0,t]$, and that the distribution of $W_{t}-W_{s}$ for $0\\leq st\\leq 1$ is Gaussian with mean $0$ and variance $t-s$.\n\nWork at the fixed terminal time $t=1$. The family $\\{K_{n}(1,\\cdot)\\}_{n\\geq 1}$ converges pointwise to $0$ on $[0,1)$ but fails to be uniformly integrable as a family in $L^{1}([0,1])$ when squared. This illustrates that pointwise convergence of kernels is insufficient to guarantee $L^{p}$ convergence of the stochastic convolutions $X_{n}(1)$ to $0$ without an appropriate uniform integrability or domination condition.\n\nYour tasks are:\n- Verify the pointwise convergence $K_{n}(1,s)\\to 0$ for each fixed $s\\in[0,1)$.\n- Explain why the family $\\{K_{n}(1,\\cdot)^{2}\\}_{n\\geq 1}$ is not uniformly integrable on $[0,1]$ with respect to Lebesgue measure.\n- Using only fundamental properties of the Wiener process and the definition of the Itô integral, determine the exact value of\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\big[|X_{n}(1)|^{p}\\big]\n$$\nfor a fixed but arbitrary exponent $p0$, as a closed-form analytic expression in $p$.\n\nThe final answer must be a single closed-form analytic expression in $p$. No rounding is required.", "solution": "We are tasked with determining the limit of the $p$-th absolute moment of a sequence of stochastic convolutions, $\\lim_{n\\to\\infty}\\mathbb{E}\\big[|X_{n}(1)|^{p}\\big]$, and addressing two preliminary points concerning the properties of the convolution kernel $K_n(t,s)$. We will address each part in sequence.\n\nFirst, we analyze the properties of the deterministic kernel $K_{n}(t,s)$ at the terminal time $t=1$. For $n\\geq 1$ and $s\\in[0,1]$, the kernel is given by\n$$\nK_{n}(1,s) = \\sqrt{n}\\,\\mathbf{1}_{(1-\\frac{1}{n},\\,1]}(s).\n$$\nThe problem statement asks to verify two claims about this family of functions.\n\n1.  **Pointwise Convergence of $K_{n}(1,s)$**: We verify that for any fixed $s \\in [0,1)$, the sequence $K_{n}(1,s)$ converges to $0$ as $n \\to \\infty$.\n    Let $s$ be a fixed number in the interval $[0,1)$. Because $s  1$, the difference $1-s$ is a positive constant. We can choose an integer $N$ large enough such that $\\frac{1}{N}  1-s$, which implies $s  1 - \\frac{1}{N}$. For any integer $n \\geq N$, it follows that $\\frac{1}{n} \\le \\frac{1}{N}  1-s$, and thus $s  1 - \\frac{1}{n}$. This means that for all $n \\geq N$, the point $s$ is not in the interval $(1-\\frac{1}{n}, 1]$. Consequently, the indicator function $\\mathbf{1}_{(1-\\frac{1}{n},\\,1]}(s)$ equals $0$ for all $n \\geq N$. Therefore, $K_{n}(1,s) = \\sqrt{n} \\cdot 0 = 0$ for all $n \\geq N$. This shows that $\\lim_{n\\to\\infty} K_{n}(1,s) = 0$ for each fixed $s \\in [0,1)$.\n\n2.  **Lack of Uniform Integrability of $\\{K_{n}(1,\\cdot)^{2}\\}_{n\\geq 1}$**: We are asked to explain why the family of functions $\\{K_{n}(1,\\cdot)^{2}\\}_{n\\geq 1}$, given by $f_{n}(s) \\coloneqq K_{n}(1,s)^{2} = n\\,\\mathbf{1}_{(1-\\frac{1}{n},\\,1]}(s)$, is not uniformly integrable on $[0,1]$ with respect to the Lebesgue measure $\\lambda$.\n    A family of integrable functions $\\{f_n\\}$ is uniformly integrable if for every $\\epsilon  0$, there exists a $\\delta  0$ such that for any measurable set $A \\subseteq [0,1]$ with $\\lambda(A)  \\delta$, we have $\\sup_{n} \\int_{A} |f_n(s)| \\,\\mathrm{d}s  \\epsilon$.\n    Let us test this condition. Consider the sequence of sets $A_m = (1-\\frac{1}{m}, 1]$ for $m \\geq 1$. The Lebesgue measure of these sets is $\\lambda(A_m) = \\frac{1}{m}$, which tends to $0$ as $m\\to\\infty$.\n    Now, let's evaluate the integral of $f_n$ over such a set, specifically for $n=m$.\n    For any $n \\geq 1$, consider the set $A_{n} = (1-\\frac{1}{n}, 1]$. We have $\\lambda(A_n) = \\frac{1}{n}$. The integral of the function $f_n$ over this set is\n    $$\n    \\int_{A_n} f_n(s)\\,\\mathrm{d}s = \\int_{(1-\\frac{1}{n},\\,1]} n\\,\\mathbf{1}_{(1-\\frac{1}{n},\\,1]}(s)\\,\\mathrm{d}s = \\int_{1-\\frac{1}{n}}^{1} n\\,\\mathrm{d}s = n \\cdot \\left(1 - \\left(1-\\frac{1}{n}\\right)\\right) = n \\cdot \\frac{1}{n} = 1.\n    $$\n    If the family $\\{f_n\\}$ were uniformly integrable, then for any $\\epsilon  0$ (e.g., $\\epsilon = \\frac{1}{2}$), there would have to be a $\\delta  0$ such that $\\int_A f_n(s)\\,\\mathrm{d}s  \\epsilon$ for all $n$ whenever $\\lambda(A)  \\delta$. However, we can always choose an integer $n_0$ large enough so that $\\frac{1}{n_0}  \\delta$. Then for the set $A = A_{n_0} = (1-\\frac{1}{n_0}, 1]$, we have $\\lambda(A)  \\delta$, but the integral for the specific index $n=n_0$ is $\\int_{A_{n_0}} f_{n_0}(s)\\,\\mathrm{d}s = 1$, which is not less than $\\epsilon = \\frac{1}{2}$. This contradicts the definition of uniform integrability. Therefore, the family $\\{K_{n}(1,\\cdot)^{2}\\}_{n\\geq 1}$ is not uniformly integrable.\n\nNow we proceed to the main task: determining the value of $\\lim_{n\\to\\infty}\\mathbb{E}\\big[|X_{n}(1)|^{p}\\big]$.\nThe stochastic process $X_n(t)$ is defined by the Itô integral\n$$\nX_{n}(t) = \\int_{0}^{t} K_{n}(t,s)\\,\\mathrm{d}W_{s}.\n$$\nAt the terminal time $t=1$, we have\n$$\nX_{n}(1) = \\int_{0}^{1} K_{n}(1,s)\\,\\mathrm{d}W_{s} = \\int_{0}^{1} \\sqrt{n}\\,\\mathbf{1}_{(1-\\frac{1}{n},\\,1]}(s)\\,\\mathrm{d}W_{s}.\n$$\nThe integrand $f_n(s) \\coloneqq K_{n}(1,s)$ is a deterministic function for each $n$. A fundamental property of the Itô integral is that for any deterministic function $f \\in L^2([0,T])$, the stochastic integral $I = \\int_0^T f(s)\\,\\mathrm{d}W_s$ is a Gaussian random variable with mean $0$ and variance given by the Itô isometry:\n$$\n\\mathbb{E}[I] = 0, \\qquad \\text{Var}(I) = \\mathbb{E}[I^2] = \\int_0^T f(s)^2\\,\\mathrm{d}s.\n$$\nWe must first verify that our integrand $f_n(s) = K_n(1,s)$ is in $L^2([0,1])$. The squared norm is\n$$\n\\int_{0}^{1} K_{n}(1,s)^2\\,\\mathrm{d}s = \\int_{0}^{1} \\left(\\sqrt{n}\\,\\mathbf{1}_{(1-\\frac{1}{n},\\,1]}(s)\\right)^2\\,\\mathrm{d}s = \\int_{0}^{1} n\\,\\mathbf{1}_{(1-\\frac{1}{n},\\,1]}(s)\\,\\mathrm{d}s = n \\int_{1-\\frac{1}{n}}^{1} 1\\,\\mathrm{d}s = n\\left(\\frac{1}{n}\\right)=1.\n$$\nSince the integral is finite (and equal to $1$), $K_n(1,\\cdot) \\in L^2([0,1])$ for all $n \\geq 1$. Thus, for each $n$, $X_n(1)$ is a normally distributed random variable with mean $\\mathbb{E}[X_n(1)]=0$ and variance $\\text{Var}(X_n(1)) = 1$.\nIn other words, for every $n \\geq 1$, the random variable $X_n(1)$ follows a standard normal distribution, $X_n(1) \\sim \\mathcal{N}(0,1)$.\n\nThe sequence of random variables $\\{X_n(1)\\}_{n\\geq 1}$ is a sequence of identically distributed random variables. Consequently, the expectation $\\mathbb{E}\\big[|X_{n}(1)|^{p}\\big]$ does not depend on $n$. Let $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0,1)$. Then for any $n \\geq 1$,\n$$\n\\mathbb{E}\\big[|X_{n}(1)|^{p}\\big] = \\mathbb{E}\\big[|Z|^{p}\\big].\n$$\nThe limit is therefore equal to this constant value:\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\big[|X_{n}(1)|^{p}\\big] = \\mathbb{E}\\big[|Z|^{p}\\big].\n$$\nWe now compute this value. The probability density function of $Z$ is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$. The $p$-th absolute moment is given by the integral:\n$$\n\\mathbb{E}\\big[|Z|^{p}\\big] = \\int_{-\\infty}^{\\infty} |z|^{p} \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)\\,\\mathrm{d}z.\n$$\nSince the integrand is an even function, we can write this as\n$$\n\\mathbb{E}\\big[|Z|^{p}\\big] = 2 \\int_{0}^{\\infty} z^{p} \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)\\,\\mathrm{d}z = \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} z^{p} \\exp\\left(-\\frac{z^2}{2}\\right)\\,\\mathrm{d}z.\n$$\nWe perform a change of variables. Let $u = \\frac{z^2}{2}$. Then $z^2 = 2u$, so $z = \\sqrt{2u}$ for $z0$. The differential is $\\mathrm{d}z = \\frac{1}{2\\sqrt{2u}} (2\\mathrm{d}u) = \\frac{1}{\\sqrt{2u}}\\mathrm{d}u$. Substituting these into the integral:\n\\begin{align*}\n\\mathbb{E}\\big[|Z|^{p}\\big] = \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} (\\sqrt{2u})^{p} \\exp(-u) \\frac{1}{\\sqrt{2u}}\\,\\mathrm{d}u \\\\\n= \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} (2u)^{p/2} (2u)^{-1/2} \\exp(-u) \\,\\mathrm{d}u \\\\\n= \\frac{2}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} 2^{(p-1)/2} u^{(p-1)/2} \\exp(-u) \\,\\mathrm{d}u \\\\\n= \\frac{2 \\cdot 2^{(p-1)/2}}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} u^{\\frac{p+1}{2}-1} \\exp(-u) \\,\\mathrm{d}u.\n\\end{align*}\nThe integral is the definition of the Gamma function, $\\Gamma(k) = \\int_0^\\infty x^{k-1} e^{-x} \\mathrm{d}x$, with $k = \\frac{p+1}{2}$. The constant factor simplifies as $\\frac{2^{(p+1)/2}}{\\sqrt{2\\pi}} = \\frac{2^{(p+1)/2}}{2^{1/2}\\pi^{1/2}} = \\frac{2^{p/2}}{\\sqrt{\\pi}}$.\nThus, the expectation is\n$$\n\\mathbb{E}\\big[|Z|^{p}\\big] = \\frac{2^{p/2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{p+1}{2}\\right).\n$$\nThis is the value of the limit.", "answer": "$$\n\\boxed{\\frac{2^{p/2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{p+1}{2}\\right)}\n$$", "id": "3046413"}, {"introduction": "Building on the previous examples, this advanced practice delves into a more subtle scenario involving stochastic, rather than deterministic, integrands. We will examine whether the convergence of integrands to zero in the $L^2(\\Omega\\times[0,T])$ norm is strong enough to ensure the convergence of the resulting process in the supremum $L^p$ norm, a key question when analyzing the stability of SDE solutions. This problem [@problem_id:3046412] constructs a clever counterexample for $p \\gt 2$, illustrating how rare but large events, permissible under $L^2$ convergence, can prevent the overall maximum of the process from vanishing, a phenomenon deeply connected to the Burkholder-Davis-Gundy inequality.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\in[0,T]},\\mathbb{P}\\right)$ be a filtered probability space that supports a standard Brownian motion $\\{W_t\\}_{t\\in[0,T]}$, with the usual augmentation, and let $p2$ be fixed. Consider the Itô integral of a predictable, square-integrable process $H$ against $\\{W_t\\}$, defined for $t\\in[0,T]$ by $M_t=\\int_{0}^{t} H_s\\,\\mathrm{d}W_s$. Convergence in $L^2$ for integrands means $\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right]\\to 0$ as $n\\to\\infty$. Convergence in $L^p$ for the supremum of stochastic integrals means $\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|\\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s\\right|^p\\right]\\to 0$ as $n\\to\\infty$.\n\nConstruct an explicit sequence of predictable processes $\\{H^n\\}_{n\\in\\mathbb{N}}$ such that $H^n\\to 0$ in $L^2(\\Omega\\times[0,T])$ but the sequence of stochastic integrals $M_t^n=\\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s$ fails to converge to $0$ in $L^p$ in the sense of the supremum over $[0,T]$. Your construction must use only simple predictable processes and standard properties of Brownian motion.\n\nConcretely, define events $A_n\\in\\mathcal{F}_0$ with $\\mathbb{P}(A_n)=n^{-p}$, independent of $\\{W_t\\}_{t\\in[0,T]}$, and set\n$$\nH^n_t \\;=\\; n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t),\\qquad t\\in[0,T].\n$$\nVerify that $H^n\\to 0$ in $L^2(\\Omega\\times[0,T])$. Then, for $M_t^n=\\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s$, compute the exact value of the limit\n$$\n\\lim_{n\\to\\infty}\\,\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right].\n$$\nYour final answer must be a single closed-form analytic expression. Do not provide inequalities. If you express your answer in terms of an expectation involving Brownian motion, standardize it on the unit interval $[0,1]$ using Brownian scaling.", "solution": "We begin by verifying the first required property: the convergence of $\\{H^n_t\\}$ to $0$ in $L^2(\\Omega\\times[0,T])$. The squared norm in this space is given by $\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right]$. We compute this quantity for the given process $H^n_t$.\nThe process is defined as $H^n_t = n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t)$. Its squared magnitude is $|H^n_t|^2 = \\left(n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t)\\right)^2 = n^2\\,\\mathbf{1}_{A_n}^2\\,\\mathbf{1}_{[0,T]}(t)^2 = n^2\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(t)$, since the indicator function $\\mathbf{1}_E$ satisfies $\\mathbf{1}_E^2 = \\mathbf{1}_E$.\n\nWe first integrate with respect to time over the interval $[0,T]$. The random variable $\\mathbf{1}_{A_n}$ is constant with respect to the time variable $s$.\n$$\n\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s = \\int_{0}^{T} n^2\\,\\mathbf{1}_{A_n}\\,\\mathrm{d}s = n^2\\,\\mathbf{1}_{A_n} \\int_{0}^{T} \\mathrm{d}s = n^2\\,T\\,\\mathbf{1}_{A_n}.\n$$\nNext, we take the expectation with respect to the probability measure $\\mathbb{P}$.\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right] = \\mathbb{E}\\!\\left[n^2\\,T\\,\\mathbf{1}_{A_n}\\right] = n^2\\,T\\,\\mathbb{E}\\!\\left[\\mathbf{1}_{A_n}\\right].\n$$\nThe expectation of an indicator function of an event is the probability of that event, $\\mathbb{E}[\\mathbf{1}_{A_n}] = \\mathbb{P}(A_n)$. We are given that $\\mathbb{P}(A_n) = n^{-p}$.\n$$\n\\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right] = n^2\\,T\\,n^{-p} = T\\,n^{2-p}.\n$$\nWe are given that $p2$, which implies $2-p  0$. Therefore, as $n \\to \\infty$, the term $n^{2-p}$ approaches $0$.\n$$\n\\lim_{n\\to\\infty} \\mathbb{E}\\!\\left[\\int_{0}^{T} |H^n_s|^2\\,\\mathrm{d}s\\right] = \\lim_{n\\to\\infty} T\\,n^{2-p} = 0.\n$$\nThis verifies that $H^n \\to 0$ in $L^2(\\Omega\\times[0,T])$.\n\nNow, we proceed to compute the limit of the expectation of the $p$-th power of the supremum of the stochastic integral $M_t^n = \\int_{0}^{t} H^n_s\\,\\mathrm{d}W_s$.\nFirst, we express $M_t^n$ more explicitly. Since $A_n \\in \\mathcal{F}_0$, the process $\\mathbf{1}_{A_n}$ is $\\mathcal{F}_0$-measurable, and therefore predictable. We can pull the random variable $\\mathbf{1}_{A_n}$ and the constant $n$ out of the stochastic integral.\n$$\nM_t^n = \\int_{0}^{t} n\\,\\mathbf{1}_{A_n}\\,\\mathbf{1}_{[0,T]}(s)\\,\\mathrm{d}W_s = n\\,\\mathbf{1}_{A_n} \\int_{0}^{t} \\mathrm{d}W_s = n\\,\\mathbf{1}_{A_n}\\,W_t, \\quad \\text{for } t \\in [0,T].\n$$\nNext, we find the supremum of the absolute value of this process over the interval $[0,T]$.\n$$\n\\sup_{0\\le t\\le T} \\left|M_t^n\\right| = \\sup_{0\\le t\\le T} \\left|n\\,\\mathbf{1}_{A_n}\\,W_t\\right| = n\\,|\\mathbf{1}_{A_n}| \\sup_{0\\le t\\le T} |W_t| = n\\,\\mathbf{1}_{A_n} \\sup_{0\\le t\\le T} |W_t|.\n$$\nWe are interested in the expectation of the $p$-th power of this expression.\n$$\n\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = \\mathbb{E}\\!\\left[\\left(n\\,\\mathbf{1}_{A_n} \\sup_{0\\le t\\le T} |W_t|\\right)^p\\right] = \\mathbb{E}\\!\\left[n^p\\,\\mathbf{1}_{A_n}^p \\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nUsing $\\mathbf{1}_{A_n}^p = \\mathbf{1}_{A_n}$, this simplifies to:\n$$\n\\mathbb{E}\\!\\left[n^p\\,\\mathbf{1}_{A_n} \\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nThe problem states that the event $A_n$ is independent of the Brownian motion $\\{W_t\\}_{t\\in[0,T]}$. This means the random variable $\\mathbf{1}_{A_n}$ is independent of the random variable $\\sup_{0\\le t\\le T} |W_t|$. For independent random variables, the expectation of their product is the product of their expectations.\n$$\n\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = n^p\\,\\mathbb{E}\\!\\left[\\mathbf{1}_{A_n}\\right]\\,\\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nSubstituting $\\mathbb{E}[\\mathbf{1}_{A_n}]=\\mathbb{P}(A_n)=n^{-p}$:\n$$\n\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = n^p\\,n^{-p}\\,\\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right] = \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nThis expression is a constant with respect to $n$. Therefore, its limit as $n\\to\\infty$ is the expression itself.\n$$\n\\lim_{n\\to\\infty}\\,\\mathbb{E}\\!\\left[\\sup_{0\\le t\\le T} \\left|M_t^n\\right|^{p}\\right] = \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right].\n$$\nSince $p0$, this expectation is strictly positive, proving that the sequence of stochastic integrals does not converge to $0$ in the specified $L^p$ sense.\n\nFinally, we must express this result in the required form, standardized on the unit interval $[0,1]$ using Brownian scaling. The scaling property of Brownian motion states that for any constant $c0$, the process $\\{W_{ct}\\}_{t\\ge 0}$ has the same distribution as the process $\\{\\sqrt{c}\\,W_t\\}_{t\\ge 0}$.\nLet $t=Tu$, where $u \\in [0,1]$. Then $\\{W_{Tu}\\}_{u\\in[0,1]}$ has the same distribution as $\\{\\sqrt{T}\\,W_u\\}_{u\\in[0,1]}$.\nTherefore, the random variable $\\sup_{0\\le t\\le T}|W_t| = \\sup_{0\\le u\\le 1}|W_{Tu}|$ has the same distribution as $\\sup_{0\\le u\\le 1}|\\sqrt{T}\\,W_u| = \\sqrt{T}\\sup_{0\\le u\\le 1}|W_u|$.\nThis implies that their moments are equal.\n$$\n\\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le T} |W_t|\\right)^p\\right] = \\mathbb{E}\\!\\left[\\left(\\sqrt{T}\\sup_{0\\le u\\le 1} |W_u|\\right)^p\\right] = \\mathbb{E}\\!\\left[T^{\\frac{p}{2}}\\left(\\sup_{0\\le u\\le 1} |W_u|\\right)^p\\right].\n$$\nPulling the constant $T^{\\frac{p}{2}}$ out of the expectation gives the final expression:\n$$\nT^{\\frac{p}{2}} \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le u\\le 1} |W_u|\\right)^p\\right].\n$$\nAs per the problem's instructions, this is a closed-form expression in terms of $T$, $p$, and a universal constant defined by an expectation of a functional of standard Brownian motion on the unit interval.", "answer": "$$\\boxed{T^{\\frac{p}{2}} \\mathbb{E}\\!\\left[\\left(\\sup_{0\\le t\\le 1} |W_t|\\right)^{p}\\right]}$$", "id": "3046412"}]}