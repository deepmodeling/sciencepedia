{"hands_on_practices": [{"introduction": "A cornerstone of probability theory is understanding how properties of random variables are affected by transformations. This exercise tackles a fundamental principle: the preservation of independence under measurable functions. You will first establish this crucial theorem from first principles and then apply it to a concrete example involving the increments of a Brownian motion, demonstrating how abstract theory directly translates into practical calculations. [@problem_id:3059599]", "problem": "Let $\\{B_{t}\\}_{t \\geq 0}$ be a standard Brownian motion (BM). Recall that for $0 \\leq s  t$, the increment $B_{t} - B_{s}$ is Gaussian with mean $0$ and variance $t - s$, and increments over disjoint time intervals are independent. Define the sign function $\\operatorname{sgn}:\\mathbb{R}\\to\\{-1,0,1\\}$ by $\\operatorname{sgn}(x)=1$ if $x0$, $\\operatorname{sgn}(x)=-1$ if $x0$, and $\\operatorname{sgn}(0)=0$.\n\nStarting only from the definition of independence of random variables and the basic properties of $\\sigma$-algebras and measurable functions, establish that if $X$ and $Y$ are independent real-valued random variables and $f$ and $g$ are monotone Borel measurable functions, then $f(X)$ and $g(Y)$ are independent. Then apply this to Brownian increments as follows.\n\nFix times $0 = u_{0}  u_{1}  u_{2}  u_{3}$. Let $X := B_{u_{1}} - B_{u_{0}}$ and $Y := B_{u_{3}} - B_{u_{2}}$. Define $S_{1} := \\operatorname{sgn}(X)$ and $S_{2} := \\operatorname{sgn}(Y)$. Compute the joint probability $\\mathbb{P}(S_{1} = 1, S_{2} = 1)$. Express your final answer as a fraction.", "solution": "The problem consists of two parts. First, to establish a general theorem about the independence of functions of independent random variables. Second, to apply this theorem to a specific problem involving Brownian motion increments.\n\nPart 1: Proof of the General Theorem\n\nThe problem asks to prove that if $X$ and $Y$ are independent real-valued random variables and $f$ and $g$ are monotone Borel measurable functions, then the random variables $f(X)$ and $g(Y)$ are independent. We will prove a slightly more general result: the conclusion holds for any Borel measurable functions $f$ and $g$, with monotonicity being a sufficient but not necessary condition.\n\nLet $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be the underlying probability space. A real-valued random variable is a measurable function from $(\\Omega, \\mathcal{F})$ to $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$, where $\\mathcal{B}(\\mathbb{R})$ is the Borel $\\sigma$-algebra on $\\mathbb{R}$.\n\nThe independence of two random variables $X$ and $Y$ is defined by the independence of the $\\sigma$-algebras they generate, denoted $\\sigma(X)$ and $\\sigma(Y)$. That is, for any set $A' \\in \\sigma(X)$ and any set $B' \\in \\sigma(Y)$, we have $\\mathbb{P}(A' \\cap B') = \\mathbb{P}(A') \\mathbb{P}(B')$.\n\nThe $\\sigma$-algebra generated by a random variable $X$, denoted $\\sigma(X)$, is the smallest $\\sigma$-algebra on $\\Omega$ with respect to which $X$ is measurable. It is given by $\\sigma(X) = \\{X^{-1}(B) : B \\in \\mathcal{B}(\\mathbb{R})\\}$, where $X^{-1}(B) = \\{\\omega \\in \\Omega : X(\\omega) \\in B\\}$.\n\nNow, consider the random variable $U = f(X)$. The $\\sigma$-algebra generated by $U$ is $\\sigma(U) = \\sigma(f(X))$. By definition, this is the collection of sets of the form $(f(X))^{-1}(A)$ for all $A \\in \\mathcal{B}(\\mathbb{R})$.\nThe preimage $(f(X))^{-1}(A)$ can be written as:\n$$ (f(X))^{-1}(A) = \\{\\omega \\in \\Omega : f(X(\\omega)) \\in A\\} = \\{\\omega \\in \\Omega : X(\\omega) \\in f^{-1}(A)\\} = X^{-1}(f^{-1}(A)) $$\nSince $f$ is a Borel measurable function, for any Borel set $A \\in \\mathcal{B}(\\mathbb{R})$, its preimage $f^{-1}(A)$ is also a Borel set, i.e., $f^{-1}(A) \\in \\mathcal{B}(\\mathbb{R})$.\nLet $C = f^{-1}(A)$. Then $C \\in \\mathcal{B}(\\mathbb{R})$. Thus, any set in $\\sigma(f(X))$ is of the form $X^{-1}(C)$ for some $C \\in \\mathcal{B}(\\mathbb{R})$.\nBy the definition of $\\sigma(X)$, any such set $X^{-1}(C)$ is an element of $\\sigma(X)$.\nTherefore, we have shown that every set in $\\sigma(f(X))$ is also in $\\sigma(X)$, which means $\\sigma(f(X)) \\subseteq \\sigma(X)$.\n\nSimilarly, for the random variable $V = g(Y)$ and the Borel measurable function $g$, we can show that $\\sigma(g(Y)) \\subseteq \\sigma(Y)$.\n\nWe are given that $X$ and $Y$ are independent, which means the $\\sigma$-algebras $\\sigma(X)$ and $\\sigma(Y)$ are independent. To show that $f(X)$ and $g(Y)$ are independent, we need to show that their generated $\\sigma$-algebras, $\\sigma(f(X))$ and $\\sigma(g(Y))$, are independent.\n\nLet $A''$ be an arbitrary set in $\\sigma(f(X))$ and $B''$ be an arbitrary set in $\\sigma(g(Y))$.\nFrom our reasoning above, since $\\sigma(f(X)) \\subseteq \\sigma(X)$, we have $A'' \\in \\sigma(X)$.\nSimilarly, since $\\sigma(g(Y)) \\subseteq \\sigma(Y)$, we have $B'' \\in \\sigma(Y)$.\nBecause $\\sigma(X)$ and $\\sigma(Y)$ are independent, it follows that:\n$$ \\mathbb{P}(A'' \\cap B'') = \\mathbb{P}(A'') \\mathbb{P}(B'') $$\nThis holds for any $A'' \\in \\sigma(f(X))$ and $B'' \\in \\sigma(g(Y))$. This is the definition of independence for the $\\sigma$-algebras $\\sigma(f(X))$ and $\\sigma(g(Y))$. Consequently, the random variables $f(X)$ and $g(Y)$ are independent. This completes the proof.\n\nPart 2: Application to Brownian Motion\n\nWe are asked to compute $\\mathbb{P}(S_{1} = 1, S_{2} = 1)$.\nThe random variables are defined as $X := B_{u_{1}} - B_{u_{0}}$ and $Y := B_{u_{3}} - B_{u_{2}}$, where $0 = u_{0}  u_{1}  u_{2}  u_{3}$.\nThe time intervals $[u_{0}, u_{1}]$ and $[u_{2}, u_{3}]$ are disjoint. A fundamental property of Brownian motion is that increments over disjoint time intervals are independent. Therefore, the random variables $X$ and $Y$ are independent.\n\nThe variables $S_{1}$ and $S_{2}$ are defined as $S_{1} := \\operatorname{sgn}(X)$ and $S_{2} := \\operatorname{sgn}(Y)$. This corresponds to applying the function $h(x) = \\operatorname{sgn}(x)$ to $X$ and $Y$, so we have $S_1 = h(X)$ and $S_2 = h(Y)$.\n\nTo apply the theorem from Part 1, we must verify that $h(x) = \\operatorname{sgn}(x)$ is a Borel measurable function (the problem specifies a monotone one, which is a stronger condition but also satisfied).\nThe function is $\\operatorname{sgn}(x) = 1$ for $x > 0$, $\\operatorname{sgn}(x) = -1$ for $x  0$, and $\\operatorname{sgn}(0) = 0$.\nThe function is non-decreasing: if $x_{1} \\leq x_{2}$, then $\\operatorname{sgn}(x_{1}) \\leq \\operatorname{sgn}(x_{2})$. Thus, it is monotone.\nTo check for Borel measurability, we can examine the preimages of subsets of the codomain $\\{-1, 0, 1\\}$. The preimages of the singleton sets are:\n$\\operatorname{sgn}^{-1}(\\{1\\}) = (0, \\infty)$\n$\\operatorname{sgn}^{-1}(\\{0\\}) = \\{0\\}$\n$\\operatorname{sgn}^{-1}(\\{-1\\}) = (-\\infty, 0)$\nEach of these preimages is a Borel set in $\\mathbb{R}$. Any set in the codomain's $\\sigma$-algebra is a union of these singletons, and its preimage will be a union of the corresponding Borel sets, which is also a Borel set. Thus, $\\operatorname{sgn}(x)$ is a Borel measurable function.\n\nSince $X$ and $Y$ are independent and $\\operatorname{sgn}$ is a Borel measurable function, we can apply the theorem from Part 1 to conclude that $S_{1} = \\operatorname{sgn}(X)$ and $S_{2} = \\operatorname{sgn}(Y)$ are independent random variables.\n\nBecause $S_1$ and $S_2$ are independent, the joint probability is the product of the marginal probabilities:\n$$ \\mathbb{P}(S_{1} = 1, S_{2} = 1) = \\mathbb{P}(S_{1} = 1) \\mathbb{P}(S_{2} = 1) $$\nNow we compute the marginal probabilities.\nThe event $\\{S_{1} = 1\\}$ is equivalent to the event $\\{\\operatorname{sgn}(X) = 1\\}$, which means $\\{X > 0\\}$.\nThe random variable $X = B_{u_{1}} - B_{u_{0}}$ is a Brownian motion increment. It is normally distributed with mean $0$ and variance $u_{1} - u_{0}$. So, $X \\sim \\mathcal{N}(0, u_{1} - u_{0})$. Since $u_{1} > u_{0} = 0$, the variance $\\sigma_X^2 = u_1 - u_0$ is positive.\nA normal distribution with mean $0$ is symmetric about $0$. For any continuous random variable $X$ with a probability density function symmetric about $0$, $\\mathbb{P}(X > 0) = \\mathbb{P}(X  0)$. Since $\\mathbb{P}(X=0)=0$ for a continuous distribution, we have $\\mathbb{P}(X > 0) + \\mathbb{P}(X  0) = 1$, which implies $\\mathbb{P}(X > 0) = \\frac{1}{2}$.\nTherefore, $\\mathbb{P}(S_{1} = 1) = \\mathbb{P}(X > 0) = \\frac{1}{2}$.\n\nSimilarly, the event $\\{S_{2} = 1\\}$ is equivalent to $\\{Y > 0\\}$.\nThe random variable $Y = B_{u_{3}} - B_{u_{2}}$ is normally distributed with mean $0$ and variance $u_{3} - u_{2}$. So, $Y \\sim \\mathcal{N}(0, u_{3} - u_{2})$. Since $u_{3} > u_{2}$, the variance $\\sigma_Y^2 = u_3 - u_2$ is positive.\nThe distribution of $Y$ is also continuous and symmetric about $0$.\nThus, $\\mathbb{P}(S_{2} = 1) = \\mathbb{P}(Y > 0) = \\frac{1}{2}$.\n\nFinally, we can compute the joint probability:\n$$ \\mathbb{P}(S_{1} = 1, S_{2} = 1) = \\mathbb{P}(S_{1} = 1) \\mathbb{P}(S_{2} = 1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} $$\nThe result is independent of the specific times $u_1, u_2, u_3$ as long as their ordering is preserved.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "3059599"}, {"introduction": "While knowing when independence is preserved is vital, it is equally important to recognize when it is broken. This practice explores common scenarios where independence is lost, particularly when variables are combined through joint functions. By analyzing the relationship between independent variables $X$ and $Y$ and their sum $Z = X+Y$, you will confront and resolve frequent misconceptions about dependence and correlation. [@problem_id:3059605]", "problem": "Consider independent random variables $X$ and $Y$ on a common probability space, neither almost surely constant. Define $Z = X + Y$. In addition, let $(B_t)_{t \\ge 0}$ and $(W_t)_{t \\ge 0}$ be independent standard Brownian motions (also called Wiener processes), and for fixed $t \\ge 0$ define $S_t = B_t + W_t$. Using only the foundational definitions of independence and conditional independence of random variables and sigma-algebras, the linearity of expectation, the definition of covariance, and the basic properties of Brownian motion (in particular, independent increments and joint Gaussianity of finite-dimensional distributions), determine which of the following statements are true. Select all that apply.\n\nA. If $X$ and $Y$ are independent, then $X$ and $Z$ are independent.\n\nB. If $X$ and $Y$ are independent with finite variance and $\\operatorname{Var}(X) > 0$, then $X$ and $Z$ are not independent.\n\nC. If $X$ and $Y$ are independent, then $X$ and $Z$ are conditionally independent given $Y$.\n\nD. For any measurable functions $g$ and $h$, the random variables $g(X)$ and $h(Y)$ are independent, but $X$ and $f(X,Y)$ for a measurable function $f$ of both arguments need not be independent.\n\nE. For any fixed $t  0$, $B_t$ and $S_t$ are independent if and only if $t = 0$.", "solution": "The problem statement will first be validated for scientific soundness, clarity, and completeness.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\n1.  $X$ and $Y$ are independent random variables on a common probability space.\n2.  Neither $X$ nor $Y$ is almost surely constant.\n3.  $Z = X + Y$.\n4.  $(B_t)_{t \\ge 0}$ and $(W_t)_{t \\ge 0}$ are independent standard Brownian motions.\n5.  For a fixed $t \\ge 0$, $S_t = B_t + W_t$.\n6.  The allowed tools are: foundational definitions of independence and conditional independence, linearity of expectation, definition of covariance, and basic properties of Brownian motion (independent increments and joint Gaussianity of finite-dimensional distributions).\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is set in the standard mathematical framework of probability theory and stochastic processes. All concepts used—independence, conditional independence, covariance, Brownian motion—are cornerstone definitions in these fields. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** The problem provides a clear setup and asks for the evaluation of five distinct mathematical statements. Each statement is a proposition that can be rigorously proven true or false based on the provided givens and the allowed foundational principles. A unique solution (the set of true statements) exists.\n3.  **Objective:** The problem is stated using precise, unambiguous mathematical language. There are no subjective or opinion-based elements.\n4.  **Completeness and Consistency:** The givens are sufficient to evaluate each option. The conditions, such as $X$ and $Y$ not being almost surely constant, are crucial for avoiding trivial cases and are consistent with the questions asked. The two parts of the problem (the general case with $X, Y, Z$ and the specific case with Brownian motions) are distinct but both fall under the umbrella of probability theory. There are no contradictions.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and internally consistent. The solution process may proceed.\n\n**Solution Derivation**\n\nThe problem asks to evaluate five statements concerning the independence of random variables. We will analyze each statement individually based on the provided framework.\n\n**Analysis of Option A**\n\nThe statement is: If $X$ and $Y$ are independent, then $X$ and $Z$ are independent. Here, $Z = X+Y$.\n\nIndependence requires that for any two bounded, measurable functions $f$ and $g$, $E[f(X)g(Z)] = E[f(X)]E[g(Z)]$. A necessary condition for independence, provided the variables have finite second moments, is that their covariance is zero. Let us compute the covariance of $X$ and $Z$, assuming they have finite variances.\n$$\n\\operatorname{Cov}(X, Z) = \\operatorname{Cov}(X, X + Y)\n$$\nBy the bilinearity of the covariance operator:\n$$\n\\operatorname{Cov}(X, Z) = \\operatorname{Cov}(X, X) + \\operatorname{Cov}(X, Y)\n$$\nWe know that $\\operatorname{Cov}(X, X) = \\operatorname{Var}(X)$. Since $X$ and $Y$ are independent, their covariance is zero: $\\operatorname{Cov}(X, Y) = 0$. Therefore,\n$$\n\\operatorname{Cov}(X, Z) = \\operatorname{Var}(X)\n$$\nThe problem states that $X$ is not an almost surely constant random variable. This implies that if its variance is finite, it must be strictly positive, i.e., $\\operatorname{Var}(X)  0$.\nSince $\\operatorname{Cov}(X, Z) = \\operatorname{Var}(X)  0$, $X$ and $Z$ are correlated. Correlation is a form of dependence, so $X$ and $Z$ are not independent.\n\nThis conclusion holds even without the assumption of finite variance. Intuitively, the value of $X$ directly influences the value of $Z$. More formally, the conditional distribution of $Z$ given $X=x$ is the distribution of $x+Y$. This distribution depends on the value of $x$. Since the conditional distribution of $Z$ given $X$ is not the same as the unconditional distribution of $Z$ (unless $X$ is almost surely constant, which is disallowed), $X$ and $Z$ are not independent.\n\nVerdict for A: **Incorrect**.\n\n**Analysis of Option B**\n\nThe statement is: If $X$ and $Y$ are independent with finite variance and $\\operatorname{Var}(X) > 0$, then $X$ and $Z$ are not independent.\n\nThis statement formalizes the argument used to disprove option A. The premises are that $X$ and $Y$ are independent, they have finite variance, and $\\operatorname{Var}(X) > 0$. As derived above:\n$$\n\\operatorname{Cov}(X, Z) = \\operatorname{Cov}(X, X+Y) = \\operatorname{Var}(X)\n$$\nGiven the premise $\\operatorname{Var}(X) > 0$, it follows that $\\operatorname{Cov}(X, Z) > 0$. Random variables with non-zero covariance are not independent. Therefore, the statement is true.\n\nVerdict for B: **Correct**.\n\n**Analysis of Option C**\n\nThe statement is: If $X$ and $Y$ are independent, then $X$ and $Z$ are conditionally independent given $Y$.\n\nThe condition for $X$ and $Z$ to be conditionally independent given $Y$ is that for all suitable sets $A$ and $B$,\n$$\nP(X \\in A, Z \\in B \\mid Y=y) = P(X \\in A \\mid Y=y) P(Z \\in B \\mid Y=y)\n$$ for almost every $y$.\nLet's analyze each term:\n-   Since $X$ and $Y$ are independent, $P(X \\in A \\mid Y=y) = P(X \\in A)$.\n-   Given $Y=y$, $Z = X+Y$ becomes $Z=X+y$. Thus, $P(Z \\in B \\mid Y=y) = P(X+y \\in B \\mid Y=y) = P(X \\in B-y)$, where $B-y = \\{b-y \\mid b \\in B\\}$.\n-   Similarly, the left-hand side is $P(X \\in A, X+y \\in B \\mid Y=y) = P(X \\in A \\text{ and } X \\in B-y) = P(X \\in A \\cap (B-y))$.\n\nSo, the condition for conditional independence becomes:\n$$\nP(X \\in A \\cap (B-y)) = P(X \\in A) P(X \\in B-y)\n$$\nThis is the definition of independence for the events $\\{X \\in A\\}$ and $\\{X \\in B-y\\}$. This does not hold in general. For example, let $y=0$ and choose $B=A$. The condition becomes $P(X \\in A) = P(X \\in A)^2$. This would imply $P(X \\in A)$ is either $0$ or $1$ for any choice of set $A$, which means $X$ must be an almost surely constant random variable. This is explicitly ruled out by the problem statement.\nTherefore, $X$ and $Z$ are not conditionally independent given $Y$.\n\nVerdict for C: **Incorrect**.\n\n**Analysis of Option D**\n\nThe statement is: For any measurable functions $g$ and $h$, the random variables $g(X)$ and $h(Y)$ are independent, but $X$ and $f(X,Y)$ for a measurable function $f$ of both arguments need not be independent.\n\nThis statement consists of two parts.\n\nPart 1: If $X$ and $Y$ are independent, then for any measurable functions $g$ and $h$, the random variables $g(X)$ and $h(Y)$ are independent.\nThis is a fundamental theorem of probability. To prove it, let $U = g(X)$ and $V = h(Y)$. For any measurable sets $A$ and $B$, we have:\n$$\nP(U \\in A, V \\in B) = P(g(X) \\in A, h(Y) \\in B) = P(X \\in g^{-1}(A), Y \\in h^{-1}(B))\n$$\nSince $X$ and $Y$ are independent,\n$$\nP(X \\in g^{-1}(A), Y \\in h^{-1}(B)) = P(X \\in g^{-1}(A)) P(Y \\in h^{-1}(B))\n$$\nBy definition, $P(X \\in g^{-1}(A)) = P(g(X) \\in A) = P(U \\in A)$ and $P(Y \\in h^{-1}(B)) = P(h(Y) \\in B) = P(V \\in B)$.\nThus, $P(U \\in A, V \\in B) = P(U \\in A)P(V \\in B)$. This proves independence. Part 1 is true.\n\nPart 2: $X$ and $f(X,Y)$ for a measurable function $f$ of both arguments need not be independent.\nThis is also true. A counterexample is sufficient. Let $f(X,Y) = X+Y = Z$. As demonstrated in the analysis of option A, $X$ and $Z$ are not independent. Part 2 is true.\n\nSince both parts of the statement are true, the entire statement is true.\n\nVerdict for D: **Correct**.\n\n**Analysis of Option E**\n\nThe statement is: For any fixed $t  0$, $B_t$ and $S_t$ are independent if and only if $t = 0$.\n\nLet's analyze the relationship between $B_t$ and $S_t = B_t + W_t$.\nGiven that $(B_t)_{t \\ge 0}$ and $(W_t)_{t \\ge 0}$ are independent standard Brownian motions, for any fixed $t \\ge 0$, $B_t \\sim N(0, t)$ and $W_t \\sim N(0, t)$. The random variables $B_t$ and $W_t$ are independent.\nThe vector $(B_t, W_t)$ is a bivariate Gaussian random vector. The vector $(B_t, S_t) = (B_t, B_t + W_t)$ is obtained by a linear transformation of $(B_t, W_t)$:\n$$\n\\begin{pmatrix} B_t \\\\ S_t \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} B_t \\\\ W_t \\end{pmatrix}\n$$\nSince linear transformations of Gaussian vectors result in Gaussian vectors, $(B_t, S_t)$ is also a bivariate Gaussian random vector. For jointly Gaussian random variables, independence is equivalent to having zero covariance. Let's compute the covariance:\n$$\n\\operatorname{Cov}(B_t, S_t) = \\operatorname{Cov}(B_t, B_t + W_t) = \\operatorname{Cov}(B_t, B_t) + \\operatorname{Cov}(B_t, W_t)\n$$\nWe have $\\operatorname{Cov}(B_t, B_t) = \\operatorname{Var}(B_t) = t$. Since $B_t$ and $W_t$ are independent, $\\operatorname{Cov}(B_t, W_t) = 0$.\nTherefore,\n$$\n\\operatorname{Cov}(B_t, S_t) = t\n$$\nFor $B_t$ and $S_t$ to be independent, their covariance must be zero. This occurs if and only if $t=0$.\n\nNow we must parse the logical structure of the statement: \"For any fixed $t  0$, ($B_t$ and $S_t$ are independent) if and only if ($t = 0$)\".\nLet us pick an arbitrary but fixed value $t_0  0$. The statement asserts that the following biconditional is true:\n$$\n(B_{t_0} \\text{ and } S_{t_0} \\text{ are independent}) \\iff (t_0 = 0)\n$$\nLet $P$ be the proposition \"$B_{t_0}$ and $S_{t_0}$ are independent\", and $Q$ be the proposition \"$t_0=0$\". We are evaluating $P \\iff Q$.\n-   Since we fixed $t_0  0$, the proposition $Q$ (\"$t_0=0$\") is false.\n-   We found that $B_{t_0}$ and $S_{t_0}$ are independent if and only if their covariance, $t_0$, is $0$. Since $t_0  0$, their covariance is non-zero, and thus they are not independent. So the proposition $P$ is false.\n\nThe biconditional statement is therefore `False` $\\iff$ `False`. This is a logically true statement.\nSince our choice of $t_0  0$ was arbitrary, this holds for any fixed $t  0$.\n\nVerdict for E: **Correct**.", "answer": "$$\\boxed{BDE}$$", "id": "3059605"}, {"introduction": "We now move from theoretical foundations to a critical real-world application: the numerical simulation of stochastic processes. This practice reveals the profound impact of the independence assumption by analyzing an Euler-Maruyama scheme with correlated noise increments. By calculating the resulting weak error, you will quantify how even a small deviation from the ideal of independent increments can lead to a systematic failure of the numerical method to converge to the correct solution. [@problem_id:3059597]", "problem": "Consider the stochastic differential equation (SDE) $dX_{t}=\\sigma\\,dW_{t}$ with deterministic initial condition $X_{0}=x_{0}$, where $W_{t}$ is a standard Brownian motion. The Euler–Maruyama time discretization with uniform step size $\\Delta t=T/N$ over $[0,T]$ constructs the approximate path via\n$$\nX_{n+1}^{\\Delta t}=X_{n}^{\\Delta t}+\\sigma\\,\\Delta W_{n},\\quad n=0,1,\\dots,N-1,\\quad X_{0}^{\\Delta t}=x_{0},\n$$\nwhere $\\Delta W_{n}=W_{(n+1)\\Delta t}-W_{n\\Delta t}$. In the ideal setting, the increments $(\\Delta W_{n})$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\Delta t$. Suppose, however, that the pseudo-random number generator used in the simulation produces a sequence of Gaussian increments with correct marginal distribution $N(0,\\Delta t)$ but with nearest-neighbor correlation: for all $i,j\\in\\{0,1,\\dots,N-1\\}$,\n$$\n\\mathbb{E}[\\Delta W_{i}]=0,\\quad \\mathbb{E}[(\\Delta W_{i})^{2}]=\\Delta t,\\quad \\mathbb{E}[\\Delta W_{i}\\Delta W_{j}]=\\begin{cases}\n\\rho\\,\\Delta t,|i-j|=1,\\\\\n0,|i-j|\\geq 2,\\\\\n\\Delta t,i=j,\n\\end{cases}\n$$\nwhere $\\rho\\in[-\\tfrac{1}{2},\\tfrac{1}{2}]$ is a fixed constant ensuring that the covariance matrix is positive semidefinite.\n\nStarting from the definitions of Brownian motion, independence, and the Euler–Maruyama method, derive the weak error for the test function $\\varphi(x)=x^{2}$ at the terminal time $T$, defined as\n$$\n\\mathbb{E}\\big[\\varphi(X_{T}^{\\Delta t})\\big]-\\mathbb{E}\\big[\\varphi(X_{T})\\big]=\\mathbb{E}\\big[(X_{T}^{\\Delta t})^{2}\\big]-\\mathbb{E}\\big[X_{T}^{2}\\big],\n$$\nand show how the dependence in $(\\Delta W_{n})$ alters the asymptotic behavior as $\\Delta t\\to 0$. Express the final answer as the closed-form expression for the limiting weak error $\\displaystyle\\lim_{\\Delta t\\to 0}\\big(\\mathbb{E}[(X_{T}^{\\Delta t})^{2}]-\\mathbb{E}[X_{T}^{2}]\\big)$ in terms of $\\sigma$, $\\rho$, and $T$. Your final answer must be a single analytic expression. No rounding is required, and no units are involved.", "solution": "The problem asks for the limiting weak error of the Euler–Maruyama scheme for a specific stochastic differential equation (SDE) when the increments of the driving noise are not independent. The weak error is defined with respect to the test function $\\varphi(x) = x^2$ at a terminal time $T$.\n\nFirst, we validate the problem statement.\n### Step 1: Extract Givens\n-   SDE: $dX_{t}=\\sigma\\,dW_{t}$\n-   Initial condition: $X_{0}=x_{0}$ (deterministic)\n-   $W_{t}$ is a standard Brownian motion.\n-   Euler–Maruyama discretization: $X_{n+1}^{\\Delta t}=X_{n}^{\\Delta t}+\\sigma\\,\\Delta W_{n}$, for $n=0,1,\\dots,N-1$.\n-   Initial condition for discretization: $X_{0}^{\\Delta t}=x_{0}$.\n-   Time step: $\\Delta t=T/N$.\n-   Increments: $\\Delta W_{n}=W_{(n+1)\\Delta t}-W_{n\\Delta t}$.\n-   Statistical properties of the generated increments $\\Delta W_{n}$:\n    -   Mean: $\\mathbb{E}[\\Delta W_{i}]=0$ for all $i$.\n    -   Variance: $\\mathbb{E}[(\\Delta W_{i})^{2}]=\\Delta t$ for all $i$.\n    -   Covariance: $\\mathbb{E}[\\Delta W_{i}\\Delta W_{j}]=\\rho\\,\\Delta t$ for $|i-j|=1$, and $0$ for $|i-j|\\geq 2$. Here $\\rho\\in[-\\tfrac{1}{2},\\tfrac{1}{2}]$.\n-   Test function: $\\varphi(x)=x^{2}$.\n-   Objective: Calculate the limiting weak error: $\\displaystyle\\lim_{\\Delta t\\to 0}\\left(\\mathbb{E}\\big[(X_{T}^{\\Delta t})^{2}\\big]-\\mathbb{E}\\big[X_{T}^{2}\\big]\\right)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the fields of stochastic calculus and numerical analysis of SDEs. The SDE is a basic model for a scaled Brownian motion. The Euler–Maruyama method is a standard numerical scheme. The core of the problem lies in analyzing the effect of a non-standard noise structure, specifically correlated increments, which is a common topic in advanced numerical SDE studies. The condition on $\\rho$ ensures the covariance matrix of the noise vector is positive semidefinite, which is a mathematically consistent and necessary constraint. The problem is well-posed, objective, and self-contained, providing all necessary information to derive a unique solution. There are no scientific flaws, contradictions, or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe solution requires computing two quantities: $\\mathbb{E}[X_T^2]$ for the exact solution and $\\mathbb{E}[(X_T^{\\Delta t})^2]$ for the numerical approximation.\n\n**1. Analysis of the Exact Solution**\n\nThe SDE is $dX_t = \\sigma dW_t$ with the initial condition $X_0 = x_0$. We can integrate this SDE from $t=0$ to $t=T$:\n$$\n\\int_{0}^{T} dX_t = \\int_{0}^{T} \\sigma dW_t\n$$\n$$\nX_T - X_0 = \\sigma (W_T - W_0)\n$$\nSince $W_t$ is a standard Brownian motion, $W_0 = 0$. Using the initial condition $X_0=x_0$, we get the expression for the exact solution at time $T$:\n$$\nX_T = x_0 + \\sigma W_T\n$$\nNow, we compute the expectation of its square, $\\mathbb{E}[X_T^2]$:\n$$\n\\mathbb{E}[X_T^2] = \\mathbb{E}[(x_0 + \\sigma W_T)^2] = \\mathbb{E}[x_0^2 + 2x_0\\sigma W_T + \\sigma^2 W_T^2]\n$$\nUsing the linearity of the expectation operator:\n$$\n\\mathbb{E}[X_T^2] = x_0^2 + 2x_0\\sigma \\mathbb{E}[W_T] + \\sigma^2 \\mathbb{E}[W_T^2]\n$$\nFor a standard Brownian motion, we know that $\\mathbb{E}[W_T] = 0$ and the variance is $\\text{Var}(W_T) = \\mathbb{E}[W_T^2] - (\\mathbb{E}[W_T])^2 = T$. This implies $\\mathbb{E}[W_T^2] = T$. Substituting these values:\n$$\n\\mathbb{E}[X_T^2] = x_0^2 + 2x_0\\sigma(0) + \\sigma^2 T = x_0^2 + \\sigma^2 T\n$$\n\n**2. Analysis of the Numerical Solution**\n\nThe Euler–Maruyama scheme is given by the recursive formula $X_{n+1}^{\\Delta t} = X_n^{\\Delta t} + \\sigma \\Delta W_n$ with $X_0^{\\Delta t} = x_0$. We can unroll this recursion to find an explicit formula for $X_N^{\\Delta t}$, which corresponds to the numerical solution at the terminal time $T=N\\Delta t$, denoted by $X_T^{\\Delta t}$:\n$$\nX_N^{\\Delta t} = X_0^{\\Delta t} + \\sum_{n=0}^{N-1} (X_{n+1}^{\\Delta t} - X_n^{\\Delta t}) = x_0 + \\sum_{n=0}^{N-1} \\sigma \\Delta W_n = x_0 + \\sigma \\sum_{n=0}^{N-1} \\Delta W_n\n$$\nNext, we compute the expectation of its square, $\\mathbb{E}[(X_T^{\\Delta t})^2] = \\mathbb{E}[(X_N^{\\Delta t})^2]$:\n$$\n\\mathbb{E}[(X_N^{\\Delta t})^2] = \\mathbb{E}\\left[\\left(x_0 + \\sigma \\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right] = \\mathbb{E}\\left[x_0^2 + 2x_0\\sigma \\sum_{n=0}^{N-1} \\Delta W_n + \\sigma^2 \\left(\\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[(X_N^{\\Delta t})^2] = x_0^2 + 2x_0\\sigma \\mathbb{E}\\left[\\sum_{n=0}^{N-1} \\Delta W_n\\right] + \\sigma^2 \\mathbb{E}\\left[\\left(\\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right]\n$$\nThe first expectation term is $\\mathbb{E}[\\sum_{n=0}^{N-1} \\Delta W_n] = \\sum_{n=0}^{N-1} \\mathbb{E}[\\Delta W_n] = \\sum_{n=0}^{N-1} 0 = 0$.\nThe expression simplifies to:\n$$\n\\mathbb{E}[(X_N^{\\Delta t})^2] = x_0^2 + \\sigma^2 \\mathbb{E}\\left[\\left(\\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right]\n$$\nThe core of the calculation is the expectation of the squared sum. We expand it:\n$$\n\\mathbb{E}\\left[\\left(\\sum_{i=0}^{N-1} \\Delta W_i\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{i=0}^{N-1} \\Delta W_i\\right)\\left(\\sum_{j=0}^{N-1} \\Delta W_j\\right)\\right] = \\mathbb{E}\\left[\\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\Delta W_i \\Delta W_j\\right] = \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\mathbb{E}[\\Delta W_i \\Delta W_j]\n$$\nWe use the given covariance structure to evaluate this double summation. We can split the sum into three cases for the indices $(i, j)$: $i=j$, $|i-j|=1$, and $|i-j|\\geq 2$.\n-   Case 1: $i=j$. These are the diagonal terms.\n$$\n\\sum_{i=0}^{N-1} \\mathbb{E}[(\\Delta W_i)^2] = \\sum_{i=0}^{N-1} \\Delta t = N \\Delta t = T\n$$\n-   Case 2: $|i-j|=1$. These are the nearest-neighbor off-diagonal terms. This occurs when $j=i+1$ or $j=i-1$.\nThe sum over these pairs is $\\sum_{i=0}^{N-2} \\mathbb{E}[\\Delta W_i \\Delta W_{i+1}] + \\sum_{i=1}^{N-1} \\mathbb{E}[\\Delta W_i \\Delta W_{i-1}]$.\nThe first part involves $N-1$ pairs, from $(0,1)$ to $(N-2, N-1)$. The second part also involves $N-1$ pairs. By symmetry $\\mathbb{E}[\\Delta W_i \\Delta W_{i-1}] = \\mathbb{E}[\\Delta W_{i-1} \\Delta W_i]$. The total number of unique nearest-neighbor pairs $(i,j)$ with $ij$ is $N-1$. The total contribution to the sum is $2 \\sum_{i=0}^{N-2} \\mathbb{E}[\\Delta W_i \\Delta W_{i+1}]$.\nSo, the sum over all pairs with $|i-j|=1$ is:\n$$\n\\sum_{|i-j|=1} \\mathbb{E}[\\Delta W_i \\Delta W_j] = 2(N-1) \\times (\\rho \\Delta t)\n$$\n-   Case 3: $|i-j|\\geq 2$. For these terms, $\\mathbb{E}[\\Delta W_i \\Delta W_j] = 0$.\n\nCombining all contributions:\n$$\n\\mathbb{E}\\left[\\left(\\sum_{n=0}^{N-1} \\Delta W_n\\right)^2\\right] = T + 2(N-1)\\rho \\Delta t\n$$\nSubstituting this back into the expression for $\\mathbb{E}[(X_N^{\\Delta t})^2]$:\n$$\n\\mathbb{E}[(X_N^{\\Delta t})^2] = x_0^2 + \\sigma^2 \\big(T + 2(N-1)\\rho \\Delta t\\big)\n$$\n\n**3. Calculation of the Weak Error**\n\nThe weak error for the test function $\\varphi(x)=x^2$ is the difference between the two computed expectations:\n$$\n\\text{Weak Error} = \\mathbb{E}[(X_T^{\\Delta t})^2] - \\mathbb{E}[X_T^2] = \\left[x_0^2 + \\sigma^2 \\big(T + 2(N-1)\\rho \\Delta t\\big)\\right] - \\left[x_0^2 + \\sigma^2 T\\right]\n$$\n$$\n\\text{Weak Error} = \\sigma^2 T + 2\\sigma^2(N-1)\\rho \\Delta t - \\sigma^2 T = 2\\sigma^2(N-1)\\rho \\Delta t\n$$\n\n**4. Asymptotic Behavior and Limiting Error**\n\nThe problem asks for the limit of this error as $\\Delta t \\to 0$. We must express the error in terms of $T$ and $\\Delta t$. Using the relation $N=T/\\Delta t$:\n$$\n\\text{Weak Error} = 2\\sigma^2\\left(\\frac{T}{\\Delta t}-1\\right)\\rho \\Delta t = 2\\sigma^2\\rho\\left(\\frac{T}{\\Delta t}\\Delta t - \\Delta t\\right) = 2\\sigma^2\\rho(T - \\Delta t)\n$$\nNow, we take the limit as $\\Delta t \\to 0$:\n$$\n\\lim_{\\Delta t \\to 0} \\left(\\mathbb{E}[(X_T^{\\Delta t})^2] - \\mathbb{E}[X_T^2]\\right) = \\lim_{\\Delta t \\to 0} 2\\sigma^2\\rho(T - \\Delta t) = 2\\sigma^2\\rho T\n$$\nThis result shows how the dependence in the increments alters the behavior. In the standard case where increments are independent ($\\rho=0$), the weak error is exactly zero for this SDE and test function, for any $\\Delta t$. The numerical scheme is exact in this weak sense. However, when $\\rho \\neq 0$, the correlation introduces a systematic bias. The weak error does not vanish as $\\Delta t \\to 0$; instead, it converges to a constant value $2\\sigma^2\\rho T$. This indicates that the numerical method does not converge weakly to the true solution. It converges to a different stochastic process whose variance is incorrect by a factor of $(1+2\\rho)$.", "answer": "$$\n\\boxed{2 \\rho \\sigma^{2} T}\n$$", "id": "3059597"}]}