{"hands_on_practices": [{"introduction": "Understanding how the moments of a stochastic process evolve is fundamental to characterizing its behavior. We begin with arithmetic Brownian motion, one of the simplest yet most important SDEs. This exercise guides you through a first-principles calculation of its variance, leveraging the explicit solution of the SDE and the basic properties of Brownian motion to build a solid intuition for how randomness propagates over time.", "problem": "Consider the one-dimensional stochastic differential equation (SDE) driven by standard Brownian motion on a filtered probability space satisfying the usual conditions: \n$$dX_{t}=\\lambda\\,dt+\\sigma\\,dW_{t},\\quad t\\ge 0,$$\nwith deterministic initial condition $X_{0}=x_{0}$, where $\\lambda\\in\\mathbb{R}$ and $\\sigma0$ are constants, and $\\{W_{t}\\}_{t\\ge 0}$ is standard Brownian motion. Using only fundamental properties of Brownian motion and the Itô integral with constant integrand, derive from first principles a closed-form expression for the variance of $X_{t}$ as a function of $t$, $\\lambda$, $\\sigma$, and $x_{0}$. Express your final answer as a single simplified analytic expression. No numerical rounding is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-contained. It describes a standard arithmetic Brownian motion, which is a fundamental model in the theory of stochastic processes. All necessary parameters ($\\lambda$, $\\sigma$), the initial condition ($x_{0}$), and the nature of the driving process ($\\{W_{t}\\}_{t\\ge 0}$ as standard Brownian motion) are clearly and consistently defined. The task is to derive a standard result from first principles, which is a legitimate and well-defined mathematical exercise. There are no contradictions, ambiguities, or factual unsoundness present.\n\nThe variance of a random variable $Y$ is defined by the formula $\\text{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$. To find the variance of the process $X_{t}$, we must first compute its expectation, $\\mathbb{E}[X_{t}]$, and its second moment, $\\mathbb{E}[X_{t}^2]$.\n\nFirst, we find the explicit solution to the given stochastic differential equation (SDE). The SDE is:\n$$dX_{s} = \\lambda\\,ds + \\sigma\\,dW_{s}$$\nWe integrate this equation from time $s=0$ to $s=t$:\n$$\\int_{0}^{t} dX_s = \\int_{0}^{t} \\lambda\\,ds + \\int_{0}^{t} \\sigma\\,dW_s$$\nThe integral on the left side is $X_t - X_0$. The first integral on the right side is a standard Riemann integral of a constant, which evaluates to $\\lambda t$. The second integral is an Itô integral with a constant integrand, $\\sigma$. This is defined as $\\sigma$ multiplied by the Itô integral of the function $f(s)=1$, which is $\\sigma(W_t - W_0)$.\n$$X_t - X_0 = \\lambda t + \\sigma(W_t - W_0)$$\nGiven the deterministic initial condition $X_{0}=x_{0}$ and the property of standard Brownian motion that $W_{0}=0$ almost surely, the explicit solution for $X_{t}$ is:\n$$X_t = x_{0} + \\lambda t + \\sigma W_t$$\nWith this closed-form solution, we can compute the required moments.\n\nWe begin with the expectation of $X_t$. Using the linearity of the expectation operator:\n$$\\mathbb{E}[X_t] = \\mathbb{E}[x_{0} + \\lambda t + \\sigma W_t] = \\mathbb{E}[x_{0}] + \\mathbb{E}[\\lambda t] + \\mathbb{E}[\\sigma W_t]$$\nSince $x_{0}$, $\\lambda$, and $t$ are deterministic quantities, their expectations are themselves, so $\\mathbb{E}[x_{0}] = x_{0}$ and $\\mathbb{E}[\\lambda t] = \\lambda t$. The expectation of the stochastic component is $\\mathbb{E}[\\sigma W_t] = \\sigma \\mathbb{E}[W_t]$. A fundamental property of standard Brownian motion is that its mean is zero for all $t \\ge 0$, i.e., $\\mathbb{E}[W_t] = 0$.\nTherefore, the expectation of $X_t$ is:\n$$\\mathbb{E}[X_t] = x_{0} + \\lambda t + \\sigma(0) = x_{0} + \\lambda t$$\n\nNext, we compute the second moment of $X_t$, which is $\\mathbb{E}[X_t^2]$.\n$$\\mathbb{E}[X_t^2] = \\mathbb{E}[(x_{0} + \\lambda t + \\sigma W_t)^2]$$\nFor clarity, let the deterministic mean be denoted by $\\mu_t = \\mathbb{E}[X_t] = x_{0} + \\lambda t$. Then $X_t = \\mu_t + \\sigma W_t$.\n$$\\mathbb{E}[X_t^2] = \\mathbb{E}[(\\mu_t + \\sigma W_t)^2] = \\mathbb{E}[\\mu_t^2 + 2\\mu_t \\sigma W_t + \\sigma^2 W_t^2]$$\nAgain, applying the linearity of expectation:\n$$\\mathbb{E}[X_t^2] = \\mathbb{E}[\\mu_t^2] + \\mathbb{E}[2\\mu_t \\sigma W_t] + \\mathbb{E}[\\sigma^2 W_t^2]$$\nAs $\\mu_t$ and $\\sigma$ are deterministic:\n$$\\mathbb{E}[X_t^2] = \\mu_t^2 + 2\\mu_t \\sigma \\mathbb{E}[W_t] + \\sigma^2 \\mathbb{E}[W_t^2]$$\nWe have already used $\\mathbb{E}[W_t] = 0$. For the term $\\mathbb{E}[W_t^2]$, we use another fundamental property of standard Brownian motion: its variance is equal to time, $\\text{Var}(W_t) = t$. From the definition of variance, $\\text{Var}(W_t) = \\mathbb{E}[W_t^2] - (\\mathbb{E}[W_t])^2$.\nSubstituting the known values gives $t = \\mathbb{E}[W_t^2] - (0)^2$, which implies that the second moment of a standard Brownian motion is $\\mathbb{E}[W_t^2] = t$.\nSubstituting these results back into the expression for $\\mathbb{E}[X_t^2]$ yields:\n$$\\mathbb{E}[X_t^2] = \\mu_t^2 + 2\\mu_t \\sigma (0) + \\sigma^2 t = \\mu_t^2 + \\sigma^2 t$$\nSubstituting back the expression for $\\mu_t = x_{0} + \\lambda t$:\n$$\\mathbb{E}[X_t^2] = (x_{0} + \\lambda t)^2 + \\sigma^2 t$$\n\nFinally, we compute the variance of $X_t$ using its definition:\n$$\\text{Var}(X_t) = \\mathbb{E}[X_t^2] - (\\mathbb{E}[X_t])^2$$\nSubstituting the expressions we have derived for the first and second moments:\n$$\\text{Var}(X_t) = \\left((x_{0} + \\lambda t)^2 + \\sigma^2 t\\right) - (x_{0} + \\lambda t)^2$$\nThe terms involving the mean, $(x_{0} + \\lambda t)^2$, cancel out:\n$$\\text{Var}(X_t) = \\sigma^2 t$$\nThis expression provides the variance of $X_t$ as a function of the model parameters. Notably, the variance increases linearly with time $t$ and is proportional to the square of the volatility parameter $\\sigma$. It is independent of the initial condition $x_{0}$ and the drift coefficient $\\lambda$, as these parameters only shift the mean of the process, not its dispersion.", "answer": "$$\\boxed{\\sigma^{2}t}$$", "id": "3052666"}, {"introduction": "In practice, most stochastic differential equations do not have explicit solutions, making direct moment calculations impossible. This practice introduces a more powerful and general technique using Itô's formula to derive a system of ordinary differential equations (ODEs) that govern the moments themselves [@problem_id:3052625]. Mastering this method is crucial as it allows us to analyze the moment dynamics for a vast class of SDEs and introduces the critical concept of moment closure.", "problem": "Consider the one-dimensional Itô stochastic differential equation (SDE)\n$$\ndX_t = \\mu(X_t,t)\\,dt + \\sigma(X_t,t)\\,dW_t,\\qquad X_0 = x_0,\n$$\nwhere $W_t$ is a standard Brownian motion, and $\\mu$ and $\\sigma$ are sufficiently smooth functions in their arguments. For an integer $k\\ge 1$, define the $k$-th moment $m_k(t) := \\mathbb{E}[X_t^k]$. Assume that all regularity and integrability conditions needed to apply Itô’s formula to $f(x)=x^k$ and to justify differentiation under the expectation are satisfied.\n\nUsing only fundamental principles, derive an expression for the time derivative $\\frac{d}{dt}m_k(t)$ in terms of expectations involving $X_t$, $\\mu(X_t,t)$, and $\\sigma(X_t,t)$. Your derivation should start from Itô’s formula and standard properties of stochastic integrals (e.g., the expectation of an Itô integral is zero under appropriate integrability). Do not use any pre-stated moment recursion formulas.\n\nThen, within your written solution only, discuss when the resulting hierarchy of ordinary differential equations for $\\{m_k(t)\\}_{k\\ge 0}$ is closed (that is, when $\\frac{d}{dt}m_k(t)$ depends only on a finite set $\\{m_j(t)\\}_{j\\le k}$) by identifying structural conditions on $\\mu$ and $\\sigma$ that ensure closure, and give one example that yields closure and one that does not. No computation beyond the general derivation is required for this discussion.\n\nAnswer specification:\n- Your final answer must be a single closed-form analytic expression for the right-hand side of $\\frac{d}{dt}m_k(t)$ as a function of $k$, $\\mu$, $\\sigma$, and expectations of powers of $X_t$. Do not present it as an equation; provide only the expression for $\\frac{d}{dt}m_k(t)$.\n- No rounding is required.\n- No physical units are involved.", "solution": "The problem statement has been validated as a well-posed and scientifically grounded question within the domain of stochastic calculus. The requested derivation is a fundamental procedure in the analysis of stochastic differential equations, and all necessary assumptions for its validity have been provided.\n\nLet the stochastic process $X_t$ be the solution to the one-dimensional Itô stochastic differential equation (SDE):\n$$\ndX_t = \\mu(X_t, t) \\,dt + \\sigma(X_t, t) \\,dW_t, \\quad X_0 = x_0\n$$\nwhere $W_t$ is a standard Brownian motion. The $k$-th moment of $X_t$ is defined as $m_k(t) := \\mathbb{E}[X_t^k]$ for an integer $k \\ge 1$. We are tasked with deriving an expression for the time derivative $\\frac{d}{dt} m_k(t)$.\n\nThe derivation starts with the application of Itô's formula. Let $f(x) = x^k$. Since $f$ does not have explicit time dependence, Itô's formula for $f(X_t)$ is:\n$$\nd(f(X_t)) = f'(X_t) \\,dX_t + \\frac{1}{2} f''(X_t) \\,(dX_t)^2\n$$\nThe required derivatives of $f(x)$ with respect to $x$ are:\n$$\nf'(x) = \\frac{d}{dx}(x^k) = kx^{k-1}\n$$\n$$\nf''(x) = \\frac{d^2}{dx^2}(x^k) = k(k-1)x^{k-2}\n$$\nThe quadratic variation term, $(dX_t)^2$, is computed using the Itô multiplication rules: $(dt)^2 = 0$, $dt\\,dW_t = 0$, and $(dW_t)^2 = dt$.\n$$\n(dX_t)^2 = (\\mu(X_t, t) \\,dt + \\sigma(X_t, t) \\,dW_t)^2 = (\\mu(X_t, t))^2 \\,(dt)^2 + 2\\mu(X_t, t)\\sigma(X_t, t) \\,dt\\,dW_t + (\\sigma(X_t, t))^2 \\,(dW_t)^2 = \\sigma^2(X_t, t) \\,dt\n$$\nSubstituting these components back into the Itô formula for $f(X_t) = X_t^k$:\n$$\nd(X_t^k) = kX_t^{k-1} \\,dX_t + \\frac{1}{2} k(k-1)X_t^{k-2} \\,(dX_t)^2\n$$\nNow, substituting the expressions for $dX_t$ and $(dX_t)^2$:\n$$\nd(X_t^k) = kX_t^{k-1} \\left( \\mu(X_t, t) \\,dt + \\sigma(X_t, t) \\,dW_t \\right) + \\frac{1}{2} k(k-1)X_t^{k-2} \\left( \\sigma^2(X_t, t) \\,dt \\right)\n$$\nWe group the $dt$ and $dW_t$ terms to obtain the stochastic differential for $X_t^k$:\n$$\nd(X_t^k) = \\left( kX_t^{k-1}\\mu(X_t, t) + \\frac{1}{2}k(k-1)X_t^{k-2}\\sigma^2(X_t, t) \\right) \\,dt + kX_t^{k-1}\\sigma(X_t, t) \\,dW_t\n$$\nTo find the evolution of the expectation $\\mathbb{E}[X_t^k]$, we express this differential relation in its integral form from an initial time $t=0$ to a general time $t$:\n$$\nX_t^k - X_0^k = \\int_0^t \\left( kX_s^{k-1}\\mu(X_s, s) + \\frac{1}{2}k(k-1)X_s^{k-2}\\sigma^2(X_s, s) \\right) \\,ds + \\int_0^t kX_s^{k-1}\\sigma(X_s, s) \\,dW_s\n$$\nNext, we take the expectation of the entire equation:\n$$\n\\mathbb{E}[X_t^k - X_0^k] = \\mathbb{E}\\left[ \\int_0^t \\left( kX_s^{k-1}\\mu(X_s, s) + \\frac{1}{2}k(k-1)X_s^{k-2}\\sigma^2(X_s, s) \\right) \\,ds \\right] + \\mathbb{E}\\left[ \\int_0^t kX_s^{k-1}\\sigma(X_s, s) \\,dW_s \\right]\n$$\nBy the linearity of expectation, $\\mathbb{E}[X_t^k - X_0^k] = \\mathbb{E}[X_t^k] - \\mathbb{E}[X_0^k]$. Since the initial condition $X_0 = x_0$ is deterministic, $\\mathbb{E}[X_0^k] = x_0^k$, which we denote as $m_k(0)$. The problem states that all required regularity and integrability conditions are satisfied, which allows us to use two fundamental properties:\n1. The expectation of an Itô integral with a suitable integrand is zero: $\\mathbb{E}\\left[ \\int_0^t \\phi_s \\,dW_s \\right] = 0$.\n2. The order of expectation and Lebesgue integration can be interchanged (Fubini-Tonelli theorem): $\\mathbb{E}\\left[ \\int_0^t \\psi_s \\,ds \\right] = \\int_0^t \\mathbb{E}[\\psi_s] \\,ds$.\n\nApplying these properties simplifies the equation for the moments:\n$$\nm_k(t) - m_k(0) = \\int_0^t \\mathbb{E}\\left[ kX_s^{k-1}\\mu(X_s, s) + \\frac{1}{2}k(k-1)X_s^{k-2}\\sigma^2(X_s, s) \\right] \\,ds\n$$\nTo find the time derivative $\\frac{d}{dt}m_k(t)$, we differentiate both sides of this integral equation with respect to $t$. By the Fundamental Theorem of Calculus, the derivative of the integral with respect to its upper limit $t$ is the integrand evaluated at $t$:\n$$\n\\frac{d}{dt}m_k(t) = \\mathbb{E}\\left[ kX_t^{k-1}\\mu(X_t, t) + \\frac{1}{2}k(k-1)X_t^{k-2}\\sigma^2(X_t, t) \\right]\n$$\nFinally, by the linearity of the expectation operator, we can separate the terms:\n$$\n\\frac{d}{dt}m_k(t) = k\\,\\mathbb{E}[X_t^{k-1}\\mu(X_t, t)] + \\frac{k(k-1)}{2}\\mathbb{E}[X_t^{k-2}\\sigma^2(X_t, t)]\n$$\nThis is the general expression for the time derivative of the $k$-th moment.\n\nAs requested, we now discuss the conditions for the closure of the moment hierarchy. The derived system of ordinary differential equations (ODEs) for the moments $\\{m_k(t)\\}_{k \\ge 0}$ is said to be closed if, for any $k$, the expression for $\\frac{d}{dt}m_k(t)$ can be expressed solely in terms of moments $m_j(t)$ with order $j \\le k$.\nThis closure property depends on the functional form of the drift $\\mu(x, t)$ and the squared diffusion $\\sigma^2(x, t)$ with respect to the state variable $x$. The system closes if and only if $\\mathbb{E}[X_t^{k-1}\\mu(X_t, t)]$ and $\\mathbb{E}[X_t^{k-2}\\sigma^2(X_t, t)]$ can be written as linear combinations of moments $m_j(t)$ for $j \\le k$. This occurs if $\\mu(x,t)$ and $\\sigma^2(x,t)$ are polynomials in $x$.\nLet $\\mu(x,t) = \\sum_{i=0}^{N_\\mu} a_i(t)x^i$ and $\\sigma^2(x,t) = \\sum_{j=0}^{N_\\sigma} b_j(t)x^j$. The expectation terms become:\n$$ \\mathbb{E}[X_t^{k-1}\\mu(X_t, t)] = \\sum_{i=0}^{N_\\mu} a_i(t)\\mathbb{E}[X_t^{k-1+i}] = \\sum_{i=0}^{N_\\mu} a_i(t)m_{k-1+i}(t) $$\n$$ \\mathbb{E}[X_t^{k-2}\\sigma^2(X_t, t)] = \\sum_{j=0}^{N_\\sigma} b_j(t)\\mathbb{E}[X_t^{k-2+j}] = \\sum_{j=0}^{N_\\sigma} b_j(t)m_{k-2+j}(t) $$\nFor the derivative $\\frac{d}{dt}m_k(t)$ to depend only on moments of order up to $k$, the highest moment order on the right-hand side must be no greater than $k$. This must hold for all $k \\ge 1$. This implies the constraints:\n$k - 1 + N_\\mu \\le k \\implies N_\\mu \\le 1$\n$k - 2 + N_\\sigma \\le k \\implies N_\\sigma \\le 2$\nTherefore, the moment hierarchy is closed if and only if the drift coefficient $\\mu(x,t)$ is a polynomial in $x$ of degree at most $1$, and the squared diffusion coefficient $\\sigma^2(x,t)$ is a polynomial in $x$ of degree at most $2$.\n\nAn example that yields closure is the Ornstein-Uhlenbeck process, $dX_t = -\\theta X_t \\,dt + \\nu \\,dW_t$, for constants $\\theta$ and $\\nu$. Here, $\\mu(x, t) = -\\theta x$ (degree $1$) and $\\sigma^2(x, t) = \\nu^2$ (degree $0$), satisfying the conditions. The resulting ODE, $\\frac{d}{dt}m_k(t) = -k\\theta m_k(t) + \\frac{k(k-1)\\nu^2}{2} m_{k-2}(t)$, is closed.\n\nAn example that does not yield closure is the SDE $dX_t = X_t^3 \\,dt + dW_t$. Here, $\\mu(x,t) = x^3$ has degree $3$, violating the closure condition $N_\\mu \\le 1$. The moment equation is $\\frac{d}{dt}m_k(t) = k m_{k+2}(t) + \\frac{k(k-1)}{2} m_{k-2}(t)$. The dependency of $\\frac{d}{dt}m_k(t)$ on $m_{k+2}(t)$ creates an open, infinite hierarchy of coupled moment equations.", "answer": "$$\\boxed{k\\,\\mathbb{E}[X_t^{k-1} \\mu(X_t, t)] + \\frac{k(k-1)}{2}\\mathbb{E}[X_t^{k-2} \\sigma^2(X_t, t)]}$$", "id": "3052625"}, {"introduction": "The structure of an SDE's coefficients dictates the stability of its solution and moments. This exercise presents a compelling, albeit simplified, case where the drift term grows \"too quickly\" (superlinearly), leading to a finite-time \"explosion\" where the solution and its moments diverge to infinity [@problem_id:3052681]. By analyzing this phenomenon in a deterministic setting, you will gain a clear and concrete understanding of the conditions that can lead to such instabilities, a key consideration in model building.", "problem": "Consider the degenerate stochastic differential equation (SDE with zero diffusion)\n$$\n\\mathrm{d}X_t = X_t^{1+\\alpha} \\mathrm{d}t,\\qquad X_0 = x_0 > 0,\n$$\nwhere $\\alpha > 0$ is a given constant. This SDE has superlinear drift and no confining term. Let $p > 0$ be fixed. Using only foundational tools such as separation of variables for ordinary differential equations, the definition of explosion time, and the definition of the $p$-th moment $M_p(t) = \\mathbb{E}[X_t^p]$, do the following:\n\n- Solve for $X_t$ up to the maximal time of existence and determine the explosion time in terms of $\\alpha$ and $x_0$.\n- Derive the explicit expression for the $p$-th moment $M_p(t)$ on the interval where it is finite, and explain why this furnishes a counterexample showing that superlinear drift without confining terms can lead to moment explosion in finite time.\n\nGive your final answer as the single analytic expression for $M_p(t)$ on its interval of finiteness. No rounding is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard mathematical problem concerning the solution of an ordinary differential equation (ODE) and the behavior of its moments, framed within the context of stochastic differential equations (SDEs). All parameters and conditions are clearly defined.\n\nThe given stochastic differential equation is:\n$$ \\mathrm{d}X_t = X_t^{1+\\alpha} \\mathrm{d}t, \\quad X_0 = x_0 > 0 $$\nwhere $\\alpha > 0$ is a constant. This is a degenerate SDE because the diffusion coefficient is zero. Consequently, the process $X_t$ is deterministic, and its evolution is described by the ordinary differential equation:\n$$ \\frac{\\mathrm{d}X_t}{\\mathrm{d}t} = X_t^{1+\\alpha} $$\nwith the initial condition $X(0) = x_0$. The problem asks to solve this equation using separation of variables.\n\nFirst, we solve for $X_t$. Since $x_0 > 0$ and the derivative is positive, $X_t$ will remain positive for all $t$ where it is defined. We can separate the variables:\n$$ \\frac{\\mathrm{d}X_t}{X_t^{1+\\alpha}} = \\mathrm{d}t $$\nIntegrating both sides from the initial time $t=0$ to a later time $t$, and from the initial state $X_0=x_0$ to the state $X_t$, we get:\n$$ \\int_{x_0}^{X_t} x^{-(1+\\alpha)} \\mathrm{d}x = \\int_0^t \\mathrm{d}s $$\nThe integral of $x^{-(1+\\alpha)}$ with respect to $x$ is $\\frac{x^{-(1+\\alpha)+1}}{-(1+\\alpha)+1} = \\frac{x^{-\\alpha}}{-\\alpha}$. Evaluating the definite integral on the left-hand side:\n$$ \\left[ -\\frac{1}{\\alpha} x^{-\\alpha} \\right]_{x_0}^{X_t} = t $$\n$$ -\\frac{1}{\\alpha} \\left( X_t^{-\\alpha} - x_0^{-\\alpha} \\right) = t $$\nNow, we solve for $X_t^{-\\alpha}$:\n$$ X_t^{-\\alpha} - x_0^{-\\alpha} = -\\alpha t $$\n$$ X_t^{-\\alpha} = x_0^{-\\alpha} - \\alpha t $$\nFinally, raising both sides to the power of $-1/\\alpha$:\n$$ X_t = \\left( x_0^{-\\alpha} - \\alpha t \\right)^{-\\frac{1}{\\alpha}} $$\nThis is the solution for $X_t$.\n\nNext, we determine the maximal time of existence, also known as the explosion time, $T_{\\text{expl}}$. The solution $X_t$ is real and positive as long as the term inside the parentheses is positive.\n$$ x_0^{-\\alpha} - \\alpha t > 0 $$\n$$ x_0^{-\\alpha} > \\alpha t $$\n$$ t  \\frac{x_0^{-\\alpha}}{\\alpha} $$\nAs $t$ approaches this critical value from below, the term $(x_0^{-\\alpha} - \\alpha t)$ approaches $0$ from the positive side. Since the exponent $-1/\\alpha$ is negative (as $\\alpha > 0$), $X_t$ will tend to $+\\infty$. This phenomenon is known as finite-time blow-up or explosion. The explosion time is therefore:\n$$ T_{\\text{expl}} = \\frac{x_0^{-\\alpha}}{\\alpha} = \\frac{1}{\\alpha x_0^{\\alpha}} $$\nThe solution $X_t$ exists and is finite for $t \\in [0, T_{\\text{expl}})$.\n\nNow, we derive the explicit expression for the $p$-th moment, $M_p(t) = \\mathbb{E}[X_t^p]$, for a fixed constant $p > 0$. The expectation $\\mathbb{E}[\\cdot]$ is taken with respect to the underlying probability measure. However, since the SDE has zero diffusion and a deterministic initial condition $X_0 = x_0$, the solution path $X_t$ is a deterministic function of time. There is no randomness in the process. Therefore, the expectation of any function of $X_t$ is simply the function evaluated at the deterministic value of $X_t$.\n$$ M_p(t) = \\mathbb{E}[X_t^p] = (X_t)^p $$\nSubstituting the expression we found for $X_t$:\n$$ M_p(t) = \\left[ \\left( x_0^{-\\alpha} - \\alpha t \\right)^{-\\frac{1}{\\alpha}} \\right]^p = \\left( x_0^{-\\alpha} - \\alpha t \\right)^{-\\frac{p}{\\alpha}} $$\nThis expression for the $p$-th moment $M_p(t)$ is finite precisely when $X_t$ is finite, which is for $t \\in [0, T_{\\text{expl}})$.\n\nFinally, we explain why this serves as a counterexample. The drift term in the SDE is $b(x) = x^{1+\\alpha}$. Since $\\alpha > 0$, the exponent $1+\\alpha$ is greater than $1$, which means the drift is superlinear. Furthermore, for $X_t > 0$, the drift is always positive, pushing the process towards larger values. There is no \"confining\" part of the drift that would pull the process back towards the origin or some other finite value. Our calculation shows that for any $p > 0$, the $p$-th moment $M_p(t)$ becomes infinite as $t \\to T_{\\text{expl}}^-$, where $T_{\\text{expl}}$ is a finite time. This explicitly demonstrates that a process with superlinear drift and no confining term can experience a moment explosion in finite time. This simple, deterministic example isolates the effect of the drift's growth rate on the moments of the solution.", "answer": "$$\\boxed{\\left(x_0^{-\\alpha} - \\alpha t\\right)^{-\\frac{p}{\\alpha}}}$$", "id": "3052681"}]}