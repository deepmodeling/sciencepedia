{"hands_on_practices": [{"introduction": "The first step in mastering moment generating functions (MGFs) and characteristic functions (CFs) is to compute them directly from their integral definitions. This foundational exercise applies this skill to the Gamma distribution, a versatile model for positive random variables like waiting times or server loads. This practice will not only build your computational fluency but also reinforce the crucial concept of the MGF's region of convergence, which determines where the function is well-defined [@problem_id:3066890].", "problem": "Let $X$ be a positive random variable with the $\\mathrm{Gamma}(k,\\theta)$ distribution, with shape parameter $k>0$ and scale parameter $\\theta>0$. Its probability density function is\n$$\nf_X(x)=\\frac{1}{\\Gamma(k)\\,\\theta^{k}}\\,x^{k-1}\\,\\exp\\!\\left(-\\frac{x}{\\theta}\\right),\\quad x>0,\n$$\nwhere $\\Gamma(k)$ denotes the gamma function. Using only the definitions of the moment generating function $M_X(t)=\\mathbb{E}[\\exp(tX)]$ and the characteristic function $\\phi_X(u)=\\mathbb{E}[\\exp(iuX)]$, derive closed-form expressions for $M_X(t)$ and $\\phi_X(u)$ in terms of $k$ and $\\theta$. Then, determine the abscissa of convergence $t_{\\star}=\\sup\\{t\\in\\mathbb{R}:M_X(t)<\\infty\\}$ of the moment generating function. Express your final results in closed form; no numerical approximation is required. The final answer must be a single composite expression containing all three items.", "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed, objective, and self-contained. All definitions and parameters are standard and correctly specified.\n\nWe are tasked with deriving the moment generating function (MGF), $M_X(t)$, the characteristic function (CF), $\\phi_X(u)$, and the abscissa of convergence, $t_{\\star}$, for a random variable $X$ following a Gamma distribution, $X \\sim \\mathrm{Gamma}(k, \\theta)$, with shape parameter $k>0$ and scale parameter $\\theta>0$.\n\nThe probability density function (PDF) is given by:\n$$f_X(x) = \\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x>0$$\n\nFirst, we derive the moment generating function, $M_X(t) = \\mathbb{E}[\\exp(tX)]$. By definition, for a continuous random variable, this is:\n$$M_X(t) = \\int_{-\\infty}^{\\infty} \\exp(tx) f_X(x) \\, dx$$\nSince $X$ is a positive random variable, the lower limit of integration is $0$. Substituting the PDF, we have:\n$$M_X(t) = \\int_{0}^{\\infty} \\exp(tx) \\left(\\frac{1}{\\Gamma(k)\\theta^k} x^{k-1} \\exp\\left(-\\frac{x}{\\theta}\\right)\\right) \\, dx$$\nWe can factor out the constant term and combine the exponentials:\n$$M_X(t) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(tx - \\frac{x}{\\theta}\\right) \\, dx$$\n$$M_X(t) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(-x\\left(\\frac{1}{\\theta} - t\\right)\\right) \\, dx$$\nFor this integral to converge, the term in the exponent must be negative for all $x>0$. This requires that the coefficient of $-x$ be positive:\n$$\\frac{1}{\\theta} - t > 0 \\implies t < \\frac{1}{\\theta}$$\nAssuming this condition holds, we can evaluate the integral. The integral is in the form of the kernel of a Gamma distribution. We can make a substitution. Let $y = x\\left(\\frac{1}{\\theta} - t\\right) = x\\frac{1-t\\theta}{\\theta}$. Then $x = y \\frac{\\theta}{1-t\\theta}$, and $dx = \\frac{\\theta}{1-t\\theta} dy$. The limits of integration remain $0$ and $\\infty$.\nAlternatively, we directly recognize the form of the Gamma function integral: $\\int_0^\\infty z^{a-1} \\exp(-bz) \\, dz = \\frac{\\Gamma(a)}{b^a}$. In our case, $a=k$ and $b = \\frac{1}{\\theta} - t$.\nUsing this identity, the integral becomes:\n$$\\int_{0}^{\\infty} x^{k-1} \\exp\\left(-x\\left(\\frac{1}{\\theta} - t\\right)\\right) \\, dx = \\frac{\\Gamma(k)}{\\left(\\frac{1}{\\theta} - t\\right)^k}$$\nNow, we substitute this back into the expression for $M_X(t)$:\n$$M_X(t) = \\frac{1}{\\Gamma(k)\\theta^k} \\cdot \\frac{\\Gamma(k)}{\\left(\\frac{1}{\\theta} - t\\right)^k} = \\frac{1}{\\theta^k \\left(\\frac{1-t\\theta}{\\theta}\\right)^k}$$\n$$M_X(t) = \\frac{1}{\\theta^k \\frac{(1-t\\theta)^k}{\\theta^k}} = \\frac{1}{(1-t\\theta)^k}$$\nThis expression for the MGF is valid for $t < 1/\\theta$. The MGF is $M_X(t) = (1 - t\\theta)^{-k}$.\n\nNext, we determine the abscissa of convergence, $t_{\\star}$, which is defined as $t_{\\star} = \\sup\\{t \\in \\mathbb{R} : M_X(t) < \\infty\\}$. The derived MGF is finite if and only if $1-t\\theta > 0$, which is the condition $t < 1/\\theta$. The set of values for which the MGF is finite is the interval $(-\\infty, 1/\\theta)$. The supremum of this set is $1/\\theta$.\n$$t_{\\star} = \\sup\\left\\{ t \\in \\mathbb{R} : t < \\frac{1}{\\theta} \\right\\} = \\frac{1}{\\theta}$$\n\nFinally, we derive the characteristic function, $\\phi_X(u) = \\mathbb{E}[\\exp(iuX)]$, where $i$ is the imaginary unit. Using its definition:\n$$\\phi_X(u) = \\int_{0}^{\\infty} \\exp(iux) f_X(x) \\, dx$$\nThe calculation is analogous to the MGF derivation. We substitute the PDF:\n$$\\phi_X(u) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(iux - \\frac{x}{\\theta}\\right) \\, dx$$\n$$\\phi_X(u) = \\frac{1}{\\Gamma(k)\\theta^k} \\int_{0}^{\\infty} x^{k-1} \\exp\\left(-x\\left(\\frac{1}{\\theta} - iu\\right)\\right) \\, dx$$\nThis integral is of the form $\\int_0^\\infty z^{a-1} \\exp(-bz) \\, dz$, with $a=k$ and $b = \\frac{1}{\\theta} - iu$. The integral converges if the real part of the coefficient $b$ is positive.\n$$\\mathrm{Re}(b) = \\mathrm{Re}\\left(\\frac{1}{\\theta} - iu\\right) = \\frac{1}{\\theta}$$\nSince $\\theta > 0$ is given, $\\mathrm{Re}(b) > 0$, so the integral converges for all real values of $u$. The value of the integral is $\\frac{\\Gamma(k)}{b^k}$.\nSubstituting this result back into the expression for $\\phi_X(u)$:\n$$\\phi_X(u) = \\frac{1}{\\Gamma(k)\\theta^k} \\cdot \\frac{\\Gamma(k)}{\\left(\\frac{1}{\\theta} - iu\\right)^k} = \\frac{1}{\\theta^k \\left(\\frac{1 - i\\theta u}{\\theta}\\right)^k}$$\n$$\\phi_X(u) = \\frac{1}{(1 - i\\theta u)^k}$$\nThis is the characteristic function, $\\phi_X(u) = (1 - i\\theta u)^{-k}$, valid for all $u \\in \\mathbb{R}$. This result can also be obtained by substituting $t = iu$ into the expression for the MGF, as $\\phi_X(u) = M_X(iu)$.\n\nIn summary, the three quantities are:\n1. Moment Generating Function: $M_X(t) = (1 - t\\theta)^{-k}$\n2. Characteristic Function: $\\phi_X(u) = (1 - i\\theta u)^{-k}$\n3. Abscissa of Convergence: $t_{\\star} = \\frac{1}{\\theta}$", "answer": "$$\\boxed{\\begin{pmatrix} (1 - t\\theta)^{-k} & (1 - i\\theta u)^{-k} & \\frac{1}{\\theta} \\end{pmatrix}}$$", "id": "3066890"}, {"introduction": "Moment and characteristic functions are not just abstract probabilistic tools; they often emerge as natural solutions in other scientific domains. This exercise reveals a profound connection between the physical law of heat diffusion and the random motion of a particle described by Brownian motion. By solving the one-dimensional heat equation using the powerful method of Fourier transforms, we will derive the probability density of a particle's position and, in the process, naturally identify its characteristic function [@problem_id:3066892].", "problem": "Let $u(t,x)$ be the fundamental solution to the one-dimensional heat equation associated with a standard Brownian motion $B_{t}$ satisfying $B_{0}=0$. Specifically, for $t>0$ and $x \\in \\mathbb{R}$, $u$ satisfies the partial differential equation $\\partial_{t}u(t,x)=\\frac{1}{2}\\partial_{xx}u(t,x)$ with initial condition $u(0,x)=\\delta_{0}(x)$, where $\\delta_{0}$ is the Dirac delta at $0$. Use the spatial Fourier transform to solve for $u(t,x)$ by the following program: take the Fourier transform in $x$ of the heat equation and its initial condition, solve the resulting ordinary differential equation in $t$ for the transformed solution, and then apply Fourier inversion to recover $u(t,x)$. Adopt the convention that for a suitable function $f$, its Fourier transform is $\\widehat{f}(\\xi)=\\int_{-\\infty}^{\\infty}\\exp(i\\xi x)f(x)\\,dx$ and inversion is $f(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}\\exp(-i\\xi x)\\widehat{f}(\\xi)\\,d\\xi$. Interpret the resulting $u(t,x)$ as the transition density of $B_{t}$ at time $t$, and then compute the characteristic function $\\varphi_{B_{t}}(\\xi)=\\mathbb{E}[\\exp(i\\xi B_{t})]$ and the moment generating function $M_{B_{t}}(s)=\\mathbb{E}[\\exp(s B_{t})]$, for real $\\xi$ and real $s$ where these expectations are finite. Provide your final answer as a single row matrix containing, in order, the explicit expressions for $u(t,x)$, $\\varphi_{B_{t}}(\\xi)$, and $M_{B_{t}}(s)$. No numerical approximation is required, and no units are involved in the final expressions.", "solution": "We are tasked with solving the heat equation $\\partial_{t}u = \\frac{1}{2}\\partial_{xx}u$ with the initial condition $u(0,x) = \\delta_0(x)$. Let $\\widehat{u}(t,\\xi)$ denote the spatial Fourier transform of $u(t,x)$ with respect to $x$.\n\n**Part 1: Solving the PDE for $u(t,x)$**\n\nFirst, we take the Fourier transform of the entire PDE. The transform of the time derivative is:\n$$\n\\mathcal{F}[\\partial_t u(t,x)] = \\int_{-\\infty}^{\\infty} \\exp(i\\xi x) \\partial_t u(t,x) \\,dx = \\partial_t \\int_{-\\infty}^{\\infty} \\exp(i\\xi x) u(t,x) \\,dx = \\partial_t \\widehat{u}(t,\\xi)\n$$\nUsing the specified Fourier transform convention, $\\mathcal{F}[\\partial_{x}f(x)] = -i\\xi \\widehat{f}(\\xi)$. Applying this twice gives:\n$$\n\\mathcal{F}[\\partial_{xx}u(t,x)] = (-i\\xi)^2 \\widehat{u}(t,\\xi) = -\\xi^2 \\widehat{u}(t,\\xi)\n$$\nSubstituting these results into the heat equation gives an ordinary differential equation (ODE) for $\\widehat{u}(t,\\xi)$ in the variable $t$, with $\\xi$ as a parameter:\n$$\n\\partial_t \\widehat{u}(t,\\xi) = \\frac{1}{2}(-\\xi^2 \\widehat{u}(t,\\xi)) = -\\frac{\\xi^2}{2} \\widehat{u}(t,\\xi)\n$$\nThis is a first-order linear ODE with the solution:\n$$\n\\widehat{u}(t,\\xi) = C(\\xi) \\exp\\left(-\\frac{\\xi^2 t}{2}\\right)\n$$\nTo determine the function $C(\\xi)$, we use the initial condition $u(0,x) = \\delta_0(x)$. We transform this condition:\n$$\n\\widehat{u}(0,\\xi) = \\int_{-\\infty}^{\\infty} \\exp(i\\xi x) \\delta_0(x) \\,dx = \\exp(i\\xi \\cdot 0) = 1\n$$\nAt $t=0$, our ODE solution gives $\\widehat{u}(0,\\xi) = C(\\xi)\\exp(0) = C(\\xi)$. Therefore, $C(\\xi)=1$, and the solution in the Fourier domain is:\n$$\n\\widehat{u}(t,\\xi) = \\exp\\left(-\\frac{\\xi^2 t}{2}\\right)\n$$\nTo find $u(t,x)$, we apply the specified inverse Fourier transform:\n$$\nu(t,x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-i\\xi x) \\widehat{u}(t,\\xi) \\,d\\xi = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-i\\xi x) \\exp\\left(-\\frac{\\xi^2 t}{2}\\right) \\,d\\xi\n$$\nWe combine the terms in the exponent and complete the square with respect to $\\xi$:\n$$\n-\\frac{t}{2}\\xi^2 - i x \\xi = -\\frac{t}{2}\\left(\\xi^2 + \\frac{2ix}{t}\\xi\\right) = -\\frac{t}{2}\\left[\\left(\\xi + \\frac{ix}{t}\\right)^2 - \\left(\\frac{ix}{t}\\right)^2\\right] = -\\frac{t}{2}\\left(\\xi + \\frac{ix}{t}\\right)^2 - \\frac{x^2}{2t}\n$$\nSubstituting this back into the integral:\n$$\nu(t,x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{t}{2}\\left(\\xi + \\frac{ix}{t}\\right)^2 - \\frac{x^2}{2t}\\right) \\,d\\xi = \\frac{1}{2\\pi} \\exp\\left(-\\frac{x^2}{2t}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{t}{2}\\left(\\xi + \\frac{ix}{t}\\right)^2\\right) \\,d\\xi\n$$\nThe integral is a standard Gaussian integral whose value is $\\sqrt{2\\pi/t}$. Substituting this result back, we obtain the expression for $u(t,x)$:\n$$\nu(t,x) = \\frac{1}{2\\pi} \\exp\\left(-\\frac{x^2}{2t}\\right) \\sqrt{\\frac{2\\pi}{t}} = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^2}{2t}\\right)\n$$\nThis is the probability density function (PDF) of a normal distribution with mean $0$ and variance $t$, denoted $\\mathcal{N}(0,t)$, which is the transition density of a standard Brownian motion $B_t$.\n\n**Part 2: Computing the Characteristic Function**\n\nThe characteristic function of a random variable $X$ with PDF $p(x)$ is given by $\\varphi_X(\\xi) = \\mathbb{E}[\\exp(i\\xi X)] = \\int_{-\\infty}^{\\infty} \\exp(i\\xi x) p(x) \\,dx$. This is precisely the definition of the Fourier transform given in the problem statement.\nTherefore, the characteristic function of $B_t$, whose PDF is $u(t,x)$, is simply its Fourier transform $\\widehat{u}(t,\\xi)$:\n$$\n\\varphi_{B_{t}}(\\xi) = \\widehat{u}(t,\\xi) = \\exp\\left(-\\frac{\\xi^2 t}{2}\\right) = \\exp\\left(-\\frac{t\\xi^2}{2}\\right)\n$$\n\n**Part 3: Computing the Moment Generating Function**\n\nThe moment generating function (MGF) is defined as $M_{B_{t}}(s) = \\mathbb{E}[\\exp(s B_t)]$ for real $s$. We can compute this directly using the PDF $u(t,x)$:\n$$\nM_{B_{t}}(s) = \\int_{-\\infty}^{\\infty} \\exp(sx) u(t,x) \\,dx = \\int_{-\\infty}^{\\infty} \\exp(sx) \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^2}{2t}\\right) \\,dx\n$$\n$$\nM_{B_{t}}(s) = \\frac{1}{\\sqrt{2\\pi t}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^2}{2t} + sx\\right) \\,dx\n$$\nWe complete the square for the exponent:\n$$\n-\\frac{x^2}{2t} + sx = -\\frac{1}{2t}(x^2 - 2stx) = -\\frac{1}{2t}\\left[(x-st)^2 - (st)^2\\right] = -\\frac{(x-st)^2}{2t} + \\frac{s^2t}{2}\n$$\nSubstituting this into the integral:\n$$\nM_{B_{t}}(s) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(\\frac{s^2t}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(x-st)^2}{2t}\\right) \\,dx\n$$\nThe integral $\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(x-st)^2}{2t}\\right) \\,dx$ is the integral of a Gaussian PDF (up to a normalization constant) and equals $\\sqrt{2\\pi t}$.\nThus, the MGF is:\n$$\nM_{B_{t}}(s) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(\\frac{s^2t}{2}\\right) \\sqrt{2\\pi t} = \\exp\\left(\\frac{s^2t}{2}\\right) = \\exp\\left(\\frac{ts^2}{2}\\right)\n$$\nThis result could also be obtained from the characteristic function by the formal substitution $\\xi = -is$: $M_{B_t}(s) = \\varphi_{B_t}(-is) = \\exp\\left(-\\frac{t(-is)^2}{2}\\right) = \\exp\\left(\\frac{ts^2}{2}\\right)$.\n\n**Final Assembly**\nThe three requested expressions are:\n1. $u(t,x) = \\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^2}{2t}\\right)$\n2. $\\varphi_{B_{t}}(\\xi) = \\exp\\left(-\\frac{t\\xi^2}{2}\\right)$\n3. $M_{B_{t}}(s) = \\exp\\left(\\frac{ts^2}{2}\\right)$\nThese are presented as a single row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2\\pi t}} \\exp\\left(-\\frac{x^{2}}{2t}\\right) & \\exp\\left(-\\frac{t\\xi^{2}}{2}\\right) & \\exp\\left(\\frac{ts^{2}}{2}\\right)\n\\end{pmatrix}\n}\n$$", "id": "3066892"}, {"introduction": "While analytical derivations are elegant, many practical applications in finance and engineering involve processes too complex for closed-form solutions. This exercise bridges the gap between pure theory and computational practice, guiding you through the estimation of MGFs and CGFs for an Ornstein-Uhlenbeck process using Monte Carlo simulation. This is a vital skill for modern quantitative analysis, forcing us to confront and analyze the two primary sources of error: the numerical discretization of the process and the statistical variability from finite sampling [@problem_id:3066856].", "problem": "Consider the Ornstein–Uhlenbeck stochastic differential equation (SDE): $$dX_t=-\\theta X_t\\,dt+\\sigma\\,dW_t,$$ with initial condition $$X_0=x_0,$$ where $$\\theta&gt;0$$ and $$\\sigma&gt;0$$ are constants and $$\\{W_t\\}_{t\\ge 0}$$ is a standard Wiener process. Let $$T&gt;0$$ be a fixed time horizon. You will approximate the law of $$X_T$$ via the Euler–Maruyama time discretization with time step $$\\Delta t&gt;0$$ and compute moment generating function (MGF) and cumulant generating function (CGF) estimators by Monte Carlo (MC). Your goals are: (i) implement a Monte Carlo estimator of the MGF and CGF from simulated SDE paths, (ii) derive and compute the estimator’s bias and variance contributions, and (iii) compare the discretization bias with the sampling variability.\n\nFundamental definitions and facts to use as starting points:\n- The Euler–Maruyama scheme for the SDE over $$n$$ steps with $$\\Delta t=T/n$$ is $$X_{k+1}=X_k+(-\\theta X_k)\\Delta t+\\sigma\\sqrt{\\Delta t}\\,\\xi_k,$$ for $$k=0,1,\\dots,n-1,$$ with $$X_0=x_0$$ and independent standard normal random variables $$\\xi_k\\sim\\mathcal{N}(0,1)$$.\n- The moment generating function (MGF) of a real-valued random variable $$X$$ is $$M_X(t)=\\mathbb{E}[e^{tX}]$$ for real $$t$$ in a neighborhood of $$0$$.\n- The cumulant generating function (CGF) is $$K_X(t)=\\log M_X(t)$$ where the logarithm is the natural logarithm.\n- For a Monte Carlo sample $$X^{(1)},\\dots,X^{(N)}$$ from a distribution, the MGF estimator at $$t$$ is $$\\widehat{M}(t)=\\frac{1}{N}\\sum_{i=1}^N e^{t X^{(i)}}.$$\n- For large $$N,$$ a second-order Taylor expansion implies an approximate bias for the CGF estimator $$\\widehat{K}(t)=\\log \\widehat{M}(t)$$ of order $$\\mathcal{O}(1/N),$$ specifically $$\\mathbb{E}[\\widehat{K}(t)]-K_X(t)\\approx -\\frac{\\mathrm{Var}(e^{tX})}{2N\\,M_X(t)^2}.$$\n\nTasks to perform in your program:\n1. Simulate $$N$$ independent paths of the Euler–Maruyama scheme with step size $$\\Delta t$$ up to time $$T$$ and collect the terminal values $$X_T^{(1)},\\dots,X_T^{(N)}.$$\n2. Compute the Monte Carlo MGF estimator $$\\widehat{M}(t)=\\frac{1}{N}\\sum_{i=1}^N e^{t X_T^{(i)}}.$$\n3. Compute the Monte Carlo CGF estimator $$\\widehat{K}(t)=\\log \\widehat{M}(t).$$\n4. For the Euler–Maruyama terminal distribution, use the fact that $$X_T$$ is Gaussian (by linear-Gaussian recursion) with mean $$\\mu_{\\mathrm{E}}=x_0 a^n$$ and variance $$v_{\\mathrm{E}}=\\sigma^2 \\Delta t\\sum_{j=0}^{n-1} a^{2j},$$ where $$a=1-\\theta\\Delta t$$ and $$n=T/\\Delta t$$ is an integer. Using these, compute the “Euler law” MGF $$M_{\\mathrm{E}}(t)$$ and the theoretical variance of the MGF estimator $$\\mathrm{Var}(\\widehat{M}(t))=\\frac{1}{N}\\left(\\mathbb{E}[e^{2tX_T}]-M_{\\mathrm{E}}(t)^2\\right),$$ both under the Euler–Maruyama distribution.\n5. Compute the “true OU law” mean and variance at time $$T,$$ namely $$\\mu_{\\mathrm{OU}}=x_0 e^{-\\theta T}$$ and $$v_{\\mathrm{OU}}=\\frac{\\sigma^2}{2\\theta}\\left(1-e^{-2\\theta T}\\right),$$ and from them compute the “true OU” MGF $$M_{\\mathrm{OU}}(t).$$\n6. Report for each test case:\n   - the Monte Carlo MGF estimate $$\\widehat{M}(t),$$\n   - the true OU MGF $$M_{\\mathrm{OU}}(t),$$\n   - the Euler–Maruyama MGF $$M_{\\mathrm{E}}(t),$$\n   - the Monte Carlo bias relative to the true OU MGF, $$\\widehat{M}(t)-M_{\\mathrm{OU}}(t),$$\n   - the discretization bias in MGF, $$M_{\\mathrm{E}}(t)-M_{\\mathrm{OU}}(t),$$\n   - the theoretical variance of $$\\widehat{M}(t)$$ under the Euler–Maruyama law, $$\\mathrm{Var}(\\widehat{M}(t)),$$\n   - the Monte Carlo CGF estimate $$\\widehat{K}(t),$$\n   - the second-order delta-method approximation to the CGF estimator bias under the Euler–Maruyama law, $$-\\frac{\\mathrm{Var}(\\widehat{M}(t))}{2 M_{\\mathrm{E}}(t)^2}.$$\n\nImplementation requirements:\n- Use a fixed pseudorandom seed to ensure deterministic output. Take the seed to be $$s=123456.$$\n- If $$T/\\Delta t$$ is not an integer, replace $$\\Delta t$$ by $$T/n$$ with $$n=\\mathrm{round}(T/\\Delta t)$$ so that $$n$$ is an integer and $$n\\Delta t=T.$$\n- For each test case, output the eight quantities listed in item $$6$$ above, in that order.\n\nTest suite:\n- Case $$1$$ (happy path): $$\\theta=1.0,$$ $$\\sigma=0.8,$$ $$x_0=0.5,$$ $$T=1.0,$$ $$\\Delta t=0.01,$$ $$N=30000,$$ $$t=0.5.$$\n- Case $$2$$ (boundary at $$t=0$$): $$\\theta=1.0,$$ $$\\sigma=0.8,$$ $$x_0=0.5,$$ $$T=1.0,$$ $$\\Delta t=0.01,$$ $$N=30000,$$ $$t=0.0.$$\n- Case $$3$$ (higher drift and different sign initial condition): $$\\theta=1.5,$$ $$\\sigma=0.7,$$ $$x_0=-0.4,$$ $$T=1.2,$$ $$\\Delta t=0.01,$$ $$N=40000,$$ $$t=1.0.$$\n\nFinal output format:\nYour program should produce a single line containing a comma-separated list of all results concatenated in the order of the test cases, with each test case contributing its eight numbers in the order specified in item $$6$$. The line must be enclosed in square brackets, with no spaces. For example, the format is\n$$[\\widehat{M}_1,M_{\\mathrm{OU},1},M_{\\mathrm{E},1},\\widehat{M}_1-M_{\\mathrm{OU},1},M_{\\mathrm{E},1}-M_{\\mathrm{OU},1},\\mathrm{Var}(\\widehat{M}_1),\\widehat{K}_1,\\mathrm{Bias}_{\\widehat{K},1},\\widehat{M}_2,\\dots].$$\nAll outputs must be real numbers (floating-point). No physical units are involved. Angles are not used. Express any ratios as decimal numbers.", "solution": "The problem requires a numerical and theoretical analysis of the moment generating function (MGF) and cumulant generating function (CGF) for the terminal value of an Ornstein-Uhlenbeck (OU) process. The analysis is based on the Euler-Maruyama discretization scheme and Monte Carlo (MC) simulation. We will compute estimators for the MGF and CGF and analyze their errors, separating the discretization bias from the statistical sampling variability.\n\nThe Ornstein-Uhlenbeck stochastic differential equation (SDE) is given by\n$$dX_t = -\\theta X_t dt + \\sigma dW_t$$\nwith a deterministic initial condition $X_0 = x_0$, and positive constants $\\theta > 0$ and $\\sigma > 0$.\n\nFirst, we establish the theoretical ground truths for both the continuous-time process and its discrete-time approximation.\n\n**1. True Ornstein-Uhlenbeck Law**\nThe exact solution to the OU SDE is a Gaussian process. At any fixed time $T > 0$, the random variable $X_T$ follows a Normal distribution, $X_T \\sim \\mathcal{N}(\\mu_{\\mathrm{OU}}, v_{\\mathrm{OU}})$. The mean and variance are given by:\n$$ \\mu_{\\mathrm{OU}} = x_0 e^{-\\theta T} $$\n$$ v_{\\mathrm{OU}} = \\frac{\\sigma^2}{2\\theta}(1 - e^{-2\\theta T}) $$\nThe MGF of a general Normal random variable $Z \\sim \\mathcal{N}(\\mu, v)$ is $M_Z(t) = \\mathbb{E}[e^{tZ}] = \\exp(\\mu t + \\frac{1}{2}v t^2)$. Applying this formula, the true MGF of $X_T$, denoted $M_{\\mathrm{OU}}(t)$, is:\n$$ M_{\\mathrm{OU}}(t) = \\exp\\left(\\mu_{\\mathrm{OU}} t + \\frac{1}{2} v_{\\mathrm{OU}} t^2\\right) $$\nThis value serves as the benchmark against which our numerical results will be compared.\n\n**2. Euler-Maruyama Discretization Law**\nThe Euler-Maruyama scheme approximates the SDE over a time grid $0, \\Delta t, 2\\Delta t, \\dots, n\\Delta t=T$. The recursive formula for the approximate process value, denoted $X_k$, is:\n$$ X_{k+1} = X_k + (-\\theta X_k)\\Delta t + \\sigma\\sqrt{\\Delta t}\\,\\xi_k = (1-\\theta\\Delta t)X_k + \\sigma\\sqrt{\\Delta t}\\,\\xi_k $$\nwhere $\\xi_k \\sim \\mathcal{N}(0, 1)$ are independent standard normal random variables. Let $a = 1 - \\theta\\Delta t$. The solution to this linear recurrence is:\n$$ X_n = a^n X_0 + \\sigma\\sqrt{\\Delta t} \\sum_{j=0}^{n-1} a^{n-1-j} \\xi_j $$\nSince $X_n$ is a sum of Gaussian random variables (plus a constant), it is also normally distributed, $X_n \\sim \\mathcal{N}(\\mu_{\\mathrm{E}}, v_{\\mathrm{E}})$. The mean is $\\mathbb{E}[X_n] = a^n x_0$, so:\n$$ \\mu_{\\mathrm{E}} = x_0 (1-\\theta\\Delta t)^n $$\nThe variance is the variance of the stochastic part:\n$$ v_{\\mathrm{E}} = \\mathrm{Var}\\left(\\sigma\\sqrt{\\Delta t} \\sum_{j=0}^{n-1} a^{n-1-j} \\xi_j\\right) = \\sigma^2 \\Delta t \\sum_{j=0}^{n-1} (a^{n-1-j})^2 = \\sigma^2 \\Delta t \\sum_{k=0}^{n-1} (a^2)^k $$\nThe sum is a geometric series. If $a^2 = 1$, the sum is $n$. Otherwise, it is $\\frac{(a^2)^n - 1}{a^2 - 1}$. The \"Euler law\" MGF, $M_{\\mathrm{E}}(t)$, for the terminal value of the discretized process is:\n$$ M_{\\mathrm{E}}(t) = \\exp\\left(\\mu_{\\mathrm{E}} t + \\frac{1}{2} v_{\\mathrm{E}} t^2\\right) $$\nThe difference $M_{\\mathrm{E}}(t) - M_{\\mathrm{OU}}(t)$ is the discretization bias, an error introduced by approximating the continuous SDE with a discrete-time scheme.\n\n**3. Monte Carlo Estimation**\nWe simulate $N$ independent paths of the Euler-Maruyama scheme to obtain $N$ samples of the terminal value, $\\{X_T^{(i)}\\}_{i=1}^N$, which are i.i.d. draws from $\\mathcal{N}(\\mu_{\\mathrm{E}}, v_{\\mathrm{E}})$.\nThe MGF estimator, $\\widehat{M}(t)$, is the sample mean of $e^{tX_T^{(i)}}$:\n$$ \\widehat{M}(t) = \\frac{1}{N} \\sum_{i=1}^N e^{tX_T^{(i)}} $$\nThe CGF estimator, $\\widehat{K}(t)$, is the natural logarithm of the MGF estimator:\n$$ \\widehat{K}(t) = \\log\\left(\\widehat{M}(t)\\right) $$\n\n**4. Error Analysis**\nThe total error of the MC estimator relative to the true MGF is $\\widehat{M}(t) - M_{\\mathrm{OU}}(t)$. This error can be additively decomposed into a statistical error and a discretization error:\n$$ \\widehat{M}(t) - M_{\\mathrm{OU}}(t) = \\underbrace{\\left(\\widehat{M}(t) - M_{\\mathrm{E}}(t)\\right)}_{\\text{Statistical Error}} + \\underbrace{\\left(M_{\\mathrm{E}}(t) - M_{\\mathrm{OU}}(t)\\right)}_{\\text{Discretization Error}} $$\nThe problem requires reporting the total error and the discretization error component.\n\nThe variability of the statistical error is quantified by the variance of the estimator $\\widehat{M}(t)$. Treating the Euler-Maruyama law as the ground truth for the simulation, the variance of $\\widehat{M}(t)$ is:\n$$ \\mathrm{Var}(\\widehat{M}(t)) = \\mathrm{Var}\\left(\\frac{1}{N} \\sum_{i=1}^N e^{tX_T^{(i)}}\\right) = \\frac{1}{N} \\mathrm{Var}(e^{tX_T^{(i)}}) $$\nUsing $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$, we get:\n$$ \\mathrm{Var}(e^{tX_T^{(i)}}) = \\mathbb{E}[e^{2tX_T^{(i)}}] - (\\mathbb{E}[e^{tX_T^{(i)}}])^2 = M_{\\mathrm{E}}(2t) - (M_{\\mathrm{E}}(t))^2 $$\nTherefore, the theoretical variance of the MGF estimator is:\n$$ \\mathrm{Var}(\\widehat{M}(t)) = \\frac{1}{N} \\left(M_{\\mathrm{E}}(2t) - (M_{\\mathrm{E}}(t))^2\\right) $$\n\nFinally, the CGF estimator $\\widehat{K}(t) = \\log\\widehat{M}(t)$ is biased with respect to $K_{\\mathrm{E}}(t) = \\log(M_{\\mathrm{E}}(t))$ due to the nonlinear logarithm function. A second-order Taylor expansion (delta method) provides an approximation for this bias:\n$$ \\mathbb{E}[\\widehat{K}(t)] - K_{\\mathrm{E}}(t) \\approx -\\frac{\\mathrm{Var}(\\widehat{M}(t))}{2 (M_{\\mathrm{E}}(t))^2} $$\nThis term is of order $\\mathcal{O}(1/N)$ and quantifies the systematic error introduced by applying the logarithm to the MGF sample mean.\n\nThe implementation will first compute the theoretical quantities $M_{\\mathrm{OU}}(t)$ and $M_{\\mathrm{E}}(t)$. Then, it will perform the Monte Carlo simulation to find $\\widehat{M}(t)$ and $\\widehat{K}(t)$. Finally, it will use these intermediate results to calculate the required eight output values for each test case, employing a fixed random seed for reproducibility. The number of steps $n$ is adjusted via $n=\\mathrm{round}(T/\\Delta t)$ to ensure $n\\Delta t=T$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating an Ornstein-Uhlenbeck process,\n    computing MGF and CGF estimators, and analyzing their biases and variances.\n    \"\"\"\n    # Use a fixed pseudorandom seed for deterministic output.\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (theta, sigma, x0, T, dt_in, N, t_mgf)\n        (1.0, 0.8, 0.5, 1.0, 0.01, 30000, 0.5),\n        (1.0, 0.8, 0.5, 1.0, 0.01, 30000, 0.0),\n        (1.5, 0.7, -0.4, 1.2, 0.01, 40000, 1.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        theta, sigma, x0, T, dt_in, N, t_mgf = case\n\n        # 1. Adjust n and dt as per implementation requiremements.\n        if T == 0:\n            n = 0\n            dt = 0.0\n        else:\n            n = int(round(T / dt_in))\n            dt = T / n\n\n        # 2. Simulate N independent paths of the Euler-Maruyama scheme.\n        X_T_samples = np.full(N, x0, dtype=np.float64)\n        if n > 0:\n            a_sim = 1.0 - theta * dt\n            for _ in range(n):\n                xi = rng.standard_normal(N)\n                X_T_samples = a_sim * X_T_samples + sigma * np.sqrt(dt) * xi\n\n        # 3. Compute Monte Carlo MGF and CGF estimators.\n        exp_tX = np.exp(t_mgf * X_T_samples)\n        M_hat = np.mean(exp_tX)\n        K_hat = np.log(M_hat)\n\n        # 4. Compute \"Euler law\" quantities.\n        # This is for the terminal distribution of the Euler-Maruyama scheme.\n        a = 1.0 - theta * dt\n        if n == 0:\n            mu_E = x0\n            v_E = 0.0\n        else:\n            mu_E = x0 * (a**n)\n            a_sq = a**2\n            # Use geometric series sum formula for variance.\n            # Handle the case a^2=1 separately for numerical stability.\n            if np.isclose(a_sq, 1.0):\n                sum_a_2j = n\n            else:\n                sum_a_2j = (a_sq**n - 1.0) / (a_sq - 1.0)\n            v_E = sigma**2 * dt * sum_a_2j\n        \n        M_E = np.exp(mu_E * t_mgf + 0.5 * v_E * t_mgf**2)\n\n        # 5. Compute the theoretical variance of the MGF estimator.\n        # This is under the Euler-Maruyama law.\n        M_E_2t = np.exp(mu_E * (2 * t_mgf) + 0.5 * v_E * (2 * t_mgf)**2)\n        Var_M_hat = (M_E_2t - M_E**2) / N\n\n        # 6. Compute \"true OU law\" quantities.\n        mu_OU = x0 * np.exp(-theta * T)\n        if T == 0:\n            v_OU = 0.0\n        else:\n            if theta == 0: # Problem states theta > 0, but for robustness\n                v_OU = sigma**2 * T\n            else:\n                v_OU = (sigma**2 / (2 * theta)) * (1.0 - np.exp(-2 * theta * T))\n        \n        M_OU = np.exp(mu_OU * t_mgf + 0.5 * v_OU * t_mgf**2)\n\n        # 7. Report the eight required quantities.\n        # MC bias relative to true OU MGF\n        mc_bias_vs_true = M_hat - M_OU\n        # Discretization bias in MGF\n        discretization_bias = M_E - M_OU\n        # Second-order delta-method approximation to the CGF estimator bias\n        # Check for M_E being zero is not necessary as it's an exponential.\n        cgf_bias_approx = -Var_M_hat / (2.0 * M_E**2) if M_E > 0 else 0.0\n\n        results_case = [\n            M_hat,                 # 1. Monte Carlo MGF estimate\n            M_OU,                  # 2. True OU MGF\n            M_E,                   # 3. Euler-Maruyama MGF\n            mc_bias_vs_true,       # 4. MC bias relative to true OU MGF\n            discretization_bias,   # 5. Discretization bias in MGF\n            Var_M_hat,             # 6. Theoretical variance of M_hat\n            K_hat,                 # 7. Monte Carlo CGF estimate\n            cgf_bias_approx        # 8. CGF estimator bias approximation\n        ]\n        all_results.extend(results_case)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3066856"}]}