{"hands_on_practices": [{"introduction": "To effectively model real-world phenomena, we often begin with a basic, well-understood random variable and transform it to suit our specific needs. This practice guides you through the foundational technique of deriving the probability distribution of a linearly transformed random variable. By starting from the first principles of cumulative distribution functions, you will build a universal formula and solidify your understanding of how a variable's location and scale are altered, a skill that is indispensable in constructing and interpreting stochastic models [@problem_id:3055425].", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let $X$ be a real-valued random variable with cumulative distribution function $F_{X}(x) = \\mathbb{P}(X \\leq x)$, and suppose $X$ admits a probability density function (pdf) $f_{X}(x)$ with respect to Lebesgue measure. For fixed constants $a \\neq 0$ and $b \\in \\mathbb{R}$, define the transformed random variable $Y = a X + b$. Using only the foundational definitions of a distribution function and the change-of-variables principle for monotone transformations, derive the cumulative distribution function $F_{Y}(y)$ in terms of $F_{X}$ and, under the assumption that $f_{X}$ exists, derive the pdf $f_{Y}(y)$ in terms of $f_{X}$. Then illustrate your result by taking $X \\sim \\mathcal{N}(0,1)$, the standard normal distribution, and explicitly determining $f_{Y}(y)$. Provide your reasoning starting from first principles, and present your final reported answer as a single closed-form analytic expression for $f_{Y}(y)$ in terms of $f_{X}(x)$, $a$, and $b$.", "solution": "The problem requires the derivation of the cumulative distribution function (CDF) and the probability density function (PDF) for a linear transformation $Y = aX+b$ of a random variable $X$. The derivation must proceed from first principles, specifically the definition of the CDF.\n\nLet $X$ be a real-valued random variable with CDF $F_{X}(x) = \\mathbb{P}(X \\leq x)$ and PDF $f_{X}(x) = \\frac{d}{dx}F_{X}(x)$. Let $Y$ be the transformed random variable defined as $Y = aX+b$, where $a \\in \\mathbb{R}$, $a \\neq 0$, and $b \\in \\mathbb{R}$. We seek to find the CDF $F_{Y}(y)$ and PDF $f_{Y}(y)$ of $Y$.\n\nThe derivation begins with the definition of the CDF of $Y$:\n$$F_{Y}(y) = \\mathbb{P}(Y \\leq y)$$\nSubstituting the expression for $Y$, we get:\n$$F_{Y}(y) = \\mathbb{P}(aX + b \\leq y)$$\nTo express this probability in terms of the CDF of $X$, we must isolate $X$.\n$$F_{Y}(y) = \\mathbb{P}(aX \\leq y-b)$$\nThe next step depends on the sign of the constant $a$. We must consider two separate cases.\n\nCase 1: $a > 0$.\nIf $a$ is positive, we can divide the inequality by $a$ without changing its direction:\n$$\\mathbb{P}\\left(X \\leq \\frac{y-b}{a}\\right)$$\nBy the definition of the CDF of $X$, this is:\n$$F_{Y}(y) = F_{X}\\left(\\frac{y-b}{a}\\right)$$\n\nCase 2: $a  0$.\nIf $a$ is negative, dividing the inequality by $a$ reverses the direction of the inequality:\n$$\\mathbb{P}\\left(X \\geq \\frac{y-b}{a}\\right)$$\nThe probability of $X$ being greater than or equal to a value can be expressed in terms of its CDF.\n$$\\mathbb{P}\\left(X \\geq \\frac{y-b}{a}\\right) = 1 - \\mathbb{P}\\left(X  \\frac{y-b}{a}\\right)$$\nSince we have assumed that $X$ admits a PDF $f_X(x)$, the random variable $X$ is continuous. For any continuous random variable, the probability of it taking on any single specific value is zero. That is, for any $z \\in \\mathbb{R}$, $\\mathbb{P}(X=z) = 0$. Therefore, $\\mathbb{P}(X  z) = \\mathbb{P}(X \\leq z) = F_X(z)$. Applying this, we get:\n$$F_{Y}(y) = 1 - F_{X}\\left(\\frac{y-b}{a}\\right)$$\n\nHaving derived the CDF $F_Y(y)$ for both cases, we now derive the PDF $f_Y(y)$ by differentiating $F_Y(y)$ with respect to $y$, i.e., $f_{Y}(y) = \\frac{d}{dy}F_{Y}(y)$. We again proceed by cases.\n\nCase 1: $a > 0$.\nWe have $F_{Y}(y) = F_{X}\\left(\\frac{y-b}{a}\\right)$. Applying the chain rule for differentiation:\n$$f_{Y}(y) = \\frac{d}{dy} F_{X}\\left(\\frac{y-b}{a}\\right) = F_{X}'\\left(\\frac{y-b}{a}\\right) \\cdot \\frac{d}{dy}\\left(\\frac{y-b}{a}\\right)$$\nSince $F_{X}'(x) = f_{X}(x)$ and $\\frac{d}{dy}\\left(\\frac{y-b}{a}\\right) = \\frac{1}{a}$, this becomes:\n$$f_{Y}(y) = f_{X}\\left(\\frac{y-b}{a}\\right) \\cdot \\frac{1}{a}$$\n\nCase 2: $a  0$.\nWe have $F_{Y}(y) = 1 - F_{X}\\left(\\frac{y-b}{a}\\right)$. Differentiating with respect to $y$:\n$$f_{Y}(y) = \\frac{d}{dy} \\left[1 - F_{X}\\left(\\frac{y-b}{a}\\right)\\right] = 0 - F_{X}'\\left(\\frac{y-b}{a}\\right) \\cdot \\frac{d}{dy}\\left(\\frac{y-b}{a}\\right)$$\n$$f_{Y}(y) = -f_{X}\\left(\\frac{y-b}{a}\\right) \\cdot \\frac{1}{a}$$\n\nWe can unify these two results into a single expression. Note that if $a  0$, then $|a| = a$, so $\\frac{1}{a} = \\frac{1}{|a|}$. If $a  0$, then $|a| = -a$, so $-\\frac{1}{a} = \\frac{1}{-a} = \\frac{1}{|a|}$. In both cases, the multiplicative factor is $\\frac{1}{|a|}$.\nTherefore, the general expression for the PDF of $Y$ is:\n$$f_{Y}(y) = \\frac{1}{|a|} f_{X}\\left(\\frac{y-b}{a}\\right)$$\nThis is a standard result known as the change-of-variables formula for probability densities, here derived from first principles.\n\nTo illustrate this result, let us consider the case where $X$ follows a standard normal distribution, $X \\sim \\mathcal{N}(0,1)$. The PDF of $X$ is given by:\n$$f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\nUsing the derived formula, the PDF of $Y = aX+b$ is:\n$$f_{Y}(y) = \\frac{1}{|a|} f_{X}\\left(\\frac{y-b}{a}\\right) = \\frac{1}{|a|} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{y-b}{a}\\right)^2\\right)$$\n$$f_{Y}(y) = \\frac{1}{\\sqrt{2\\pi}|a|} \\exp\\left(-\\frac{(y-b)^2}{2a^2}\\right)$$\nWe can write $|a| = \\sqrt{a^2}$. So, the expression becomes:\n$$f_{Y}(y) = \\frac{1}{\\sqrt{2\\pi a^2}} \\exp\\left(-\\frac{(y-b)^2}{2a^2}\\right)$$\nThis is the PDF of a normal distribution with mean $\\mu = b$ and variance $\\sigma^2 = a^2$. Thus, we have shown that if $X \\sim \\mathcal{N}(0,1)$, then $Y = aX+b \\sim \\mathcal{N}(b, a^2)$, which is a well-established property of the normal distribution and confirms our general derivation.\n\nThe final answer requested is the single closed-form analytic expression for $f_Y(y)$ in terms of the function $f_X$ and the constants $a$ and $b$. Based on our derivation, this expression is the unified formula that holds for any $a \\neq 0$.", "answer": "$$\n\\boxed{\\frac{1}{|a|} f_{X}\\left(\\frac{y-b}{a}\\right)}\n$$", "id": "3055425"}, {"introduction": "The structure of a stochastic process is defined by the relationships between its constituent random variables, where the concept of independence is paramount. However, independence can be a slippery notion. This exercise explores the crucial and often misunderstood distinction between pairwise independence and the much stronger condition of joint independence [@problem_id:3055430]. By constructing a classic counterexample, you will see firsthand why assuming that independence between all pairs implies collective independence is a critical error in probabilistic modeling.", "problem": "In the study of stochastic processes that underlie stochastic differential equations, a key structural property is independence. A family of random variables is called pairwise independent if every two distinct variables are independent, whereas joint (mutual) independence requires that all finite subfamilies satisfy the product rule for probabilities of events determined by those variables. Using only these definitions, determine which of the following constructions yield three random variables that are pairwise independent but not jointly independent. Select all that apply.\n\nA. Let $U$ and $V$ be independent Bernoulli random variables with parameter $1/2$ on the probability space $(\\{0,1\\}^{2},\\mathcal{P}(\\{0,1\\}^{2}),\\mathbb{P})$, where each outcome has probability $1/4$. Define $X=U$, $Y=V$, and $Z=(U+V)\\bmod 2$ (exclusive-or).\n\nB. Let $U$ and $V$ be independent Bernoulli random variables with parameter $1/2$ on the probability space $(\\{0,1\\}^{2},\\mathcal{P}(\\{0,1\\}^{2}),\\mathbb{P})$, where each outcome has probability $1/4$. Define $X=U$, $Y=V$, and $Z=U$.\n\nC. Let the sample space be $\\Omega=\\{(0,0,0),(0,1,1),(1,0,1),(1,1,0)\\}$ with the $\\sigma$-algebra $\\mathcal{F}=\\mathcal{P}(\\Omega)$ and the uniform probability measure $\\mathbb{P}$ assigning probability $1/4$ to each outcome. Define $X(\\omega)$, $Y(\\omega)$, and $Z(\\omega)$ as the first, second, and third coordinates of $\\omega$, respectively.\n\nD. Let $X$, $Y$, and $Z$ be independent Bernoulli random variables with parameter $1/2$ constructed on a common probability space so that $(X,Y,Z)$ is a triple of independent coin flips.\n\nE. Let $X$ and $Y$ be independent Rademacher random variables, that is, $\\mathbb{P}(X=1)=\\mathbb{P}(X=-1)=1/2$ and $\\mathbb{P}(Y=1)=\\mathbb{P}(Y=-1)=1/2$, with $(X,Y)$ independent. Define $Z=XY$.\n\nYour task: Using only the definitions of pairwise independence and joint independence, determine which options construct three random variables that are pairwise independent but not jointly independent. Provide a justification based on explicit probability calculations for your selection(s).", "solution": "### Analysis of a General Case\nTo be pairwise independent, we must have:\n1. $\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\mathbb{P}(Y=y)$ for all $x, y$.\n2. $\\mathbb{P}(X=x, Z=z) = \\mathbb{P}(X=x)\\mathbb{P}(Z=z)$ for all $x, z$.\n3. $\\mathbb{P}(Y=y, Z=z) = \\mathbb{P}(Y=y)\\mathbb{P}(Z=z)$ for all $y, z$.\n\nTo not be jointly independent, there must exist at least one triplet $(x, y, z)$ such that:\n$$ \\mathbb{P}(X=x, Y=y, Z=z) \\neq \\mathbb{P}(X=x)\\mathbb{P}(Y=y)\\mathbb{P}(Z=z) $$\n\nWe now analyze each option based on these criteria.\n\n### Option A\nLet $U$ and $V$ be independent Bernoulli random variables with parameter $1/2$. The outcomes for $(U,V)$ are $(0,0), (0,1), (1,0), (1,1)$, each with probability $1/4$.\nWe define $X=U$, $Y=V$, and $Z=(U+V)\\bmod 2$.\n\n**Marginal Distributions:**\n- $X=U \\sim \\text{Bernoulli}(1/2)$, so $\\mathbb{P}(X=0) = \\mathbb{P}(X=1) = 1/2$.\n- $Y=V \\sim \\text{Bernoulli}(1/2)$, so $\\mathbb{P}(Y=0) = \\mathbb{P}(Y=1) = 1/2$.\n- For $Z$:\n  - $\\mathbb{P}(Z=0) = \\mathbb{P}((U,V) \\in \\{(0,0), (1,1)\\}) = \\mathbb{P}(U=0,V=0) + \\mathbb{P}(U=1,V=1) = 1/4 + 1/4 = 1/2$.\n  - $\\mathbb{P}(Z=1) = \\mathbb{P}((U,V) \\in \\{(0,1), (1,0)\\}) = \\mathbb{P}(U=0,V=1) + \\mathbb{P}(U=1,V=0) = 1/4 + 1/4 = 1/2$.\n- So, $Z$ is also a Bernoulli random variable with parameter $1/2$.\n\n**Pairwise Independence:**\n- **(X, Y):** $X=U$ and $Y=V$ are given as independent.\n- **(X, Z):** We must check if $\\mathbb{P}(X=x, Z=z) = \\mathbb{P}(X=x)\\mathbb{P}(Z=z)$ for all $x,z \\in \\{0,1\\}$. The product $\\mathbb{P}(X=x)\\mathbb{P}(Z=z)$ is always $(1/2)(1/2) = 1/4$.\n  - $\\mathbb{P}(X=0, Z=0) = \\mathbb{P}(U=0, (U+V)\\bmod 2 = 0) = \\mathbb{P}(U=0, V=0) = 1/4$. This holds.\n  - $\\mathbb{P}(X=0, Z=1) = \\mathbb{P}(U=0, (U+V)\\bmod 2 = 1) = \\mathbb{P}(U=0, V=1) = 1/4$. This holds.\n  - $\\mathbb{P}(X=1, Z=0) = \\mathbb{P}(U=1, (U+V)\\bmod 2 = 0) = \\mathbb{P}(U=1, V=1) = 1/4$. This holds.\n  - $\\mathbb{P}(X=1, Z=1) = \\mathbb{P}(U=1, (U+V)\\bmod 2 = 1) = \\mathbb{P}(U=1, V=0) = 1/4$. This holds.\n- Thus, $X$ and $Z$ are independent.\n- **(Y, Z):** By symmetry, the argument for $(Y,Z)$ is identical to that for $(X,Z)$. $Y$ and $Z$ are independent.\n- Conclusion: $X, Y, Z$ are pairwise independent.\n\n**Joint Independence:**\n- We check if $\\mathbb{P}(X=x, Y=y, Z=z) = \\mathbb{P}(X=x)\\mathbb{P}(Y=y)\\mathbb{P}(Z=z)$. The product of marginals is $(1/2)(1/2)(1/2) = 1/8$ for any choice of $x,y,z$.\n- Consider the event $(X=0, Y=0, Z=0)$.\n  - $\\mathbb{P}(X=0, Y=0, Z=0) = \\mathbb{P}(U=0, V=0, (U+V)\\bmod 2 = 0)$.\n  - If $U=0$ and $V=0$, then $(U+V)\\bmod 2=0$ is automatically satisfied.\n  - So, $\\mathbb{P}(X=0, Y=0, Z=0) = \\mathbb{P}(U=0, V=0) = 1/4$.\n- We have $\\mathbb{P}(X=0, Y=0, Z=0) = 1/4$, but $\\mathbb{P}(X=0)\\mathbb{P}(Y=0)\\mathbb{P}(Z=0) = 1/8$.\n- Since $1/4 \\neq 1/8$, the random variables are not jointly independent.\n\n**Verdict for A:** The construction yields variables that are pairwise independent but not jointly independent. **Correct**.\n\n### Option B\nLet $U$ and $V$ be independent Bernoulli variables with parameter $1/2$. We define $X=U$, $Y=V$, and $Z=U$.\n- To check for pairwise independence, we examine the pair $(X,Z)$.\n- $X=U$ and $Z=U$. The variables are identical.\n- Let's check if $X$ and $Z$ are independent.\n  - $\\mathbb{P}(X=1, Z=0) = \\mathbb{P}(U=1, U=0) = 0$.\n  - $\\mathbb{P}(X=1) = 1/2$ and $\\mathbb{P}(Z=0) = 1/2$.\n  - Thus, $\\mathbb{P}(X=1)\\mathbb{P}(Z=0) = (1/2)(1/2) = 1/4$.\n- Since $0 \\neq 1/4$, $X$ and $Z$ are not independent. The variables are not pairwise independent.\n\n**Verdict for B:** The construction does not yield pairwise independent variables. **Incorrect**.\n\n### Option C\nThe sample space is $\\Omega=\\{(0,0,0),(0,1,1),(1,0,1),(1,1,0)\\}$ with uniform probability $1/4$ on each outcome. $X, Y, Z$ are the coordinate functions.\n\n**Marginal Distributions:**\n- $\\mathbb{P}(X=0) = \\mathbb{P}(\\{(0,0,0)\\}) + \\mathbb{P}(\\{(0,1,1)\\}) = 1/4 + 1/4 = 1/2$. So $\\mathbb{P}(X=1) = 1/2$.\n- $\\mathbb{P}(Y=0) = \\mathbb{P}(\\{(0,0,0)\\}) + \\mathbb{P}(\\{(1,0,1)\\}) = 1/4 + 1/4 = 1/2$. So $\\mathbb{P}(Y=1) = 1/2$.\n- $\\mathbb{P}(Z=0) = \\mathbb{P}(\\{(0,0,0)\\}) + \\mathbb{P}(\\{(1,1,0)\\}) = 1/4 + 1/4 = 1/2$. So $\\mathbb{P}(Z=1) = 1/2$.\n- All three are Bernoulli variables with parameter $1/2$.\n\n**Pairwise Independence:**\n- The required product of marginal probabilities is $\\mathbb{P}(V_1=v_1)\\mathbb{P}(V_2=v_2) = (1/2)(1/2) = 1/4$ for any pair of variables $V_1, V_2 \\in \\{X,Y,Z\\}$ and values $v_1, v_2 \\in \\{0,1\\}$.\n- **(X, Y):** $\\mathbb{P}(X=0, Y=0) = \\mathbb{P}(\\{(0,0,0)\\}) = 1/4$. This holds. By checking all four pairs of values, we find they are independent.\n- **(X, Z):** $\\mathbb{P}(X=0, Z=0) = \\mathbb{P}(\\{(0,0,0)\\}) = 1/4$. This holds. Checking all four pairs confirms independence.\n- **(Y, Z):** $\\mathbb{P}(Y=0, Z=0) = \\mathbb{P}(\\{(0,0,0)\\}) = 1/4$. This holds. Checking all four pairs confirms independence.\n- Conclusion: $X, Y, Z$ are pairwise independent.\n\n**Joint Independence:**\n- The product of marginal probabilities is $\\mathbb{P}(X=x)\\mathbb{P}(Y=y)\\mathbb{P}(Z=z) = (1/2)(1/2)(1/2) = 1/8$.\n- Consider the event $(X=0, Y=0, Z=0)$.\n  - $\\mathbb{P}(X=0, Y=0, Z=0) = \\mathbb{P}(\\{(0,0,0)\\}) = 1/4$.\n- Since $1/4 \\neq 1/8$, the random variables are not jointly independent. In fact, for any point not in $\\Omega$, like $(0,0,1)$, its probability is $0$, which also differs from $1/8$.\n- This construction is an explicit representation of the joint distribution from Option A, where $Z = (X+Y) \\bmod 2$.\n\n**Verdict for C:** The construction yields variables that are pairwise independent but not jointly independent. **Correct**.\n\n### Option D\n$X, Y, Z$ are defined as independent Bernoulli random variables with parameter $1/2$. The term \"independent\" without qualification implies mutual (joint) independence.\n- **Pairwise Independence:** A set of jointly independent random variables is necessarily pairwise independent.\n- **Joint Independence:** Given by definition.\nThe condition \"not jointly independent\" is not met.\n\n**Verdict for D:** The variables are jointly independent. **Incorrect**.\n\n### Option E\n$X$ and $Y$ are independent Rademacher random variables, i.e., taking values $\\{-1, 1\\}$ with probability $1/2$ each. $Z$ is defined as $Z=XY$.\n\n**Marginal Distributions:**\n- $X$ and $Y$ are given as Rademacher.\n- For $Z=XY$:\n  - $\\mathbb{P}(Z=1) = \\mathbb{P}(\\{X=1,Y=1\\} \\cup \\{X=-1,Y=-1\\}) = \\mathbb{P}(X=1)\\mathbb{P}(Y=1) + \\mathbb{P}(X=-1)\\mathbb{P}(Y=-1) = (1/2)(1/2) + (1/2)(1/2) = 1/4 + 1/4 = 1/2$.\n  - $\\mathbb{P}(Z=-1) = 1 - \\mathbb{P}(Z=1) = 1/2$.\n- $Z$ is also a Rademacher random variable.\n\n**Pairwise Independence:**\n- **(X, Y):** Given as independent.\n- **(X, Z):** We check if $\\mathbb{P}(X=x, Z=z) = \\mathbb{P}(X=x)\\mathbb{P}(Z=z)$. The product is always $(1/2)(1/2) = 1/4$.\n  - $\\mathbb{P}(X=1, Z=1) = \\mathbb{P}(X=1, XY=1) = \\mathbb{P}(X=1, Y=1) = 1/4$. This holds.\n  - $\\mathbb{P}(X=1, Z=-1) = \\mathbb{P}(X=1, XY=-1) = \\mathbb{P}(X=1, Y=-1) = 1/4$. This holds.\n  - $\\mathbb{P}(X=-1, Z=1) = \\mathbb{P}(X=-1, XY=1) = \\mathbb{P}(X=-1, Y=-1) = 1/4$. This holds.\n  - $\\mathbb{P}(X=-1, Z=-1) = \\mathbb{P}(X=-1, XY=-1) = \\mathbb{P}(X=-1, Y=1) = 1/4$. This holds.\n- Thus, $X$ and $Z$ are independent.\n- **(Y, Z):** By symmetry, $Y$ and $Z$ are also independent.\n- Conclusion: $X, Y, Z$ are pairwise independent.\n\n**Joint Independence:**\n- The product of the marginals is $\\mathbb{P(X=x)P(Y=y)P(Z=z)} = (1/2)(1/2)(1/2) = 1/8$.\n- Consider the event $(X=1, Y=1, Z=1)$.\n  - $\\mathbb{P}(X=1, Y=1, Z=1) = \\mathbb{P}(X=1, Y=1, XY=1)$.\n  - If $X=1$ and $Y=1$, then $Z=XY=1$ is determined.\n  - Thus, $\\mathbb{P}(X=1, Y=1, Z=1) = \\mathbb{P}(X=1, Y=1) = (1/2)(1/2) = 1/4$.\n- Since $1/4 \\neq 1/8$, the random variables are not jointly independent.\n\n**Verdict for E:** The construction yields variables that are pairwise independent but not jointly independent. **Correct**.\n\n### Summary\nThe constructions in options A, C, and E all produce random variables that are pairwise independent but not jointly independent. Options B and D fail to meet the required criteria.", "answer": "$$\\boxed{ACE}$$", "id": "3055430"}, {"introduction": "At the heart of stochastic differential equations lies the Itô integral, a powerful tool for integrating with respect to the erratic path of a Brownian motion. This practice demystifies this abstract concept by guiding you through the calculation of a simple, yet fundamental, stochastic integral. Using the Itô isometry—a property directly linked to the construction of the integral—you will discover how the properties of Brownian motion's increments determine the distribution of the resulting integral, providing a concrete first step into the world of stochastic calculus [@problem_id:3055415].", "problem": "Let $W=\\{W_{s}:s\\geq 0\\}$ be a standard Brownian motion (also called a Wiener process) on a filtered probability space satisfying the usual conditions. For a fixed $t0$, consider the Itô stochastic integral\n$$\nI(t)=\\int_{0}^{t}s\\,dW_{s},\n$$\ndefined via the $L^{2}$ limit of adapted step processes. Starting from the foundational definitions of standard Brownian motion, its independent and stationary increments, and the construction of the Itô integral from simple processes, justify why $I(t)$ is a Gaussian random variable and determine its mean. Then, using the Itô isometry, compute the variance of $I(t)$ as a function of $t$. Provide your final answer as a single closed-form analytic expression in $t$ for the variance of $I(t)$. Do not quote any pre-derived shortcut formulas without justification from the above foundations, and ensure that each step in your reasoning is made explicit. No rounding is required.", "solution": "The stochastic integral $I(t) = \\int_{0}^{t}s\\,dW_{s}$ is defined for the deterministic integrand $f(s) = s$ with respect to a standard 1-dimensional Brownian motion $\\{W_s\\}_{s \\ge 0}$ on a filtered probability space $(\\Omega, \\mathcal{F}, \\{\\mathcal{F}_s\\}_{s \\geq 0}, P)$ satisfying the usual conditions. We are tasked to demonstrate that $I(t)$ is a Gaussian random variable, and to compute its mean and variance based on foundational principles.\n\nFirst, we justify the Gaussian nature of $I(t)$. The Itô integral is constructed as a limit in $L^2(\\Omega, \\mathcal{F}, P)$ of integrals of simple processes. A simple process $\\phi(s)$ on $[0, t]$ is a process of the form $\\phi(s) = \\sum_{i=0}^{n-1} \\xi_i \\mathbb{I}_{(t_i, t_{i+1}]}(s)$, where $0 = t_0  t_1  \\dots  t_n = t$ is a partition of $[0, t]$ and each $\\xi_i$ is an $\\mathcal{F}_{t_i}$-measurable random variable. The integral of such a simple process is defined as $\\int_0^t \\phi(s) dW_s = \\sum_{i=0}^{n-1} \\xi_i (W_{t_{i+1}} - W_{t_i})$.\n\nThe integrand in our problem is the deterministic function $f(s) = s$, which is continuous and therefore belongs to the space $L^2([0, t])$ since $\\int_0^t s^2 ds = t^3/3  \\infty$. We can approximate $f(s) = s$ by a sequence of simple processes. Let's consider a sequence of partitions of $[0, t]$ with mesh size tending to zero. For a given $n \\in \\mathbb{N}$, let the partition points be $t_i = \\frac{i t}{n}$ for $i=0, 1, \\dots, n$. We define a sequence of simple processes $\\phi_n(s)$ by taking the value of the function at the left endpoint of each subinterval:\n$$\n\\phi_n(s) = \\sum_{i=0}^{n-1} f(t_i) \\mathbb{I}_{(t_i, t_{i+1}]}(s) = \\sum_{i=0}^{n-1} t_i \\mathbb{I}_{(t_i, t_{i+1}]}(s).\n$$\nSince $f(s)=s$ is deterministic, the coefficients $\\xi_i$ are constants, $\\xi_i = f(t_i) = t_i$. The Itô integral for this simple process $\\phi_n(s)$ is the random variable $I_n(t)$:\n$$\nI_n(t) = \\int_0^t \\phi_n(s) dW_s = \\sum_{i=0}^{n-1} t_i (W_{t_{i+1}} - W_{t_i}).\n$$\nBy definition, a standard Brownian motion has independent increments. The random variables $\\Delta W_i = W_{t_{i+1}} - W_{t_i}$ are independent for different indices $i$. Furthermore, each increment is a Gaussian random variable with distribution $\\Delta W_i \\sim N(0, t_{i+1} - t_i)$.\nThe random variable $I_n(t)$ is a linear combination of independent Gaussian random variables. A fundamental property of the Gaussian distribution is that any linear combination of independent Gaussian random variables is itself a Gaussian random variable. Thus, for each $n$, $I_n(t)$ is a Gaussian random variable.\n\nThe Itô integral $I(t)$ is defined as the $L^2$-limit of the sequence of random variables $\\{I_n(t)\\}_{n \\in \\mathbb{N}}$ as $n \\to \\infty$. A key theorem in probability theory states that if a sequence of Gaussian random variables converges in $L^2$ to a random variable $X$, then $X$ must also be a Gaussian random variable. This is because convergence in $L^2$ implies convergence in distribution. The characteristic function of $I_n(t)$ is of Gaussian form, and the limit of these characteristic functions must be the characteristic function of the limit random variable $I(t)$, which must therefore also be Gaussian. Hence, we conclude that $I(t) = \\int_{0}^{t}s\\,dW_{s}$ is a Gaussian random variable.\n\nNext, we determine the mean of $I(t)$. Since convergence in $L^2$ implies convergence in $L^1$, we can interchange the expectation and the limit:\n$$\n\\mathbb{E}[I(t)] = \\mathbb{E}[\\lim_{n \\to \\infty} I_n(t)] = \\lim_{n \\to \\infty} \\mathbb{E}[I_n(t)].\n$$\nWe compute the expectation of $I_n(t)$ using its definition and the linearity of expectation:\n$$\n\\mathbb{E}[I_n(t)] = \\mathbb{E}\\left[\\sum_{i=0}^{n-1} t_i (W_{t_{i+1}} - W_{t_i})\\right] = \\sum_{i=0}^{n-1} \\mathbb{E}[t_i (W_{t_{i+1}} - W_{t_i})].\n$$\nSince the $t_i$ values are deterministic constants, we have:\n$$\n\\mathbb{E}[I_n(t)] = \\sum_{i=0}^{n-1} t_i \\mathbb{E}[W_{t_{i+1}} - W_{t_i}].\n$$\nFor a standard Brownian motion, the increments have mean zero: $\\mathbb{E}[W_{t_{i+1}} - W_{t_i}] = 0$. Therefore, for every $n$:\n$$\n\\mathbb{E}[I_n(t)] = \\sum_{i=0}^{n-1} t_i \\cdot 0 = 0.\n$$\nConsequently, the mean of $I(t)$ is:\n$$\n\\mathbb{E}[I(t)] = \\lim_{n \\to \\infty} 0 = 0.\n$$\n\nFinally, we compute the variance of $I(t)$. The variance is given by $\\text{Var}(I(t)) = \\mathbb{E}[I(t)^2] - (\\mathbb{E}[I(t)])^2$. Since we have shown that $\\mathbb{E}[I(t)]=0$, the variance simplifies to the second moment:\n$$\n\\text{Var}(I(t)) = \\mathbb{E}[I(t)^2].\n$$\nTo compute this, we use the Itô isometry property. The Itô isometry states that for any adapted process $\\phi_s$ belonging to the space of processes with finite expected integrated square, i.e., $\\mathbb{E}[\\int_0^t \\phi_s^2 ds]  \\infty$, the following equality holds:\n$$\n\\mathbb{E}\\left[\\left(\\int_0^t \\phi_s dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_0^t \\phi_s^2 ds\\right].\n$$\nThis property is foundational to the construction of the Itô integral. It is first established for simple processes. For a simple process $\\phi_s = \\sum_{i=0}^{n-1} \\xi_i \\mathbb{I}_{(t_i, t_{i+1}]}(s)$, its integral is $I_\\phi(t) = \\sum_{i=0}^{n-1} \\xi_i \\Delta W_i$. The second moment is $\\mathbb{E}[I_\\phi(t)^2] = \\mathbb{E}[(\\sum_i \\xi_i \\Delta W_i)(\\sum_j \\xi_j \\Delta W_j)] = \\sum_{i,j} \\mathbb{E}[\\xi_i \\xi_j \\Delta W_i \\Delta W_j]$. Due to the independent increments property of Brownian motion, the expectation of cross-terms, e.g., for $i  j$, is $\\mathbb{E}[\\xi_i \\xi_j \\Delta W_i \\Delta W_j] = \\mathbb{E}[\\mathbb{E}[\\xi_i \\xi_j \\Delta W_i \\Delta W_j | \\mathcal{F}_{t_j}]] = \\mathbb{E}[\\xi_i \\xi_j \\Delta W_i \\mathbb{E}[\\Delta W_j | \\mathcal{F}_{t_j}]] = 0$. So all cross-terms ($i \\neq j$) vanish. The sum reduces to the diagonal terms: $\\sum_i \\mathbb{E}[\\xi_i^2 (\\Delta W_i)^2]$. Using the tower property and independence of $\\Delta W_i$ from $\\mathcal{F}_{t_i}$, this becomes $\\mathbb{E}[\\xi_i^2 (\\Delta W_i)^2] = \\mathbb{E}[\\xi_i^2 \\mathbb{E}[(\\Delta W_i)^2|\\mathcal{F}_{t_i}]] = \\mathbb{E}[\\xi_i^2]\\text{Var}(\\Delta W_i) = \\mathbb{E}[\\xi_i^2](t_{i+1}-t_i)$. Thus, $\\mathbb{E}[I_\\phi(t)^2] = \\sum_i \\mathbb{E}[\\xi_i^2](t_{i+1}-t_i) = \\mathbb{E}[\\int_0^t \\phi_s^2 ds]$. The property is extended to general integrands via the $L^2$ limiting procedure that defines the Itô integral.\n\nIn our problem, the integrand is the deterministic function $\\phi_s = s$. A deterministic function is trivially adapted. Applying the Itô isometry:\n$$\n\\text{Var}(I(t)) = \\mathbb{E}\\left[\\left(\\int_0^t s dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_0^t s^2 ds\\right].\n$$\nSince the integrand $s^2$ in the right-hand side integral is deterministic, the expectation operator has no effect. The expression simplifies to a standard definite integral:\n$$\n\\text{Var}(I(t)) = \\int_0^t s^2 ds.\n$$\nWe evaluate this integral:\n$$\n\\int_0^t s^2 ds = \\left[ \\frac{s^3}{3} \\right]_0^t = \\frac{t^3}{3} - \\frac{0^3}{3} = \\frac{t^3}{3}.\n$$\nTherefore, the variance of the stochastic integral $I(t) = \\int_0^t s dW_s$ is $\\frac{t^3}{3}$. The distribution of $I(t)$ is fully characterized as $I(t) \\sim N(0, \\frac{t^3}{3})$. The problem asks for the variance of $I(t)$ as the final answer.", "answer": "$$\\boxed{\\frac{t^3}{3}}$$", "id": "3055415"}]}