{"hands_on_practices": [{"introduction": "In stochastic calculus, not all approximation schemes are created equal. For a stepwise approximation to be mathematically sound, its coefficients at each step must be \"predictable\"—they can only depend on information available *before* the next random fluctuation occurs. This hands-on problem challenges you to construct a more advanced scheme with an adaptive step size and rigorously prove its predictability, reinforcing the foundational concepts of stopping times and filtrations. [@problem_id:3074508]", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ be a filtered probability space supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\ge 0}$ with its usual augmented filtration. Consider the linear stochastic differential equation $dX_t=\\alpha X_t\\,dt+\\beta X_t\\,dW_t$ with initial condition $X_0=x_0$, where $\\alpha,\\beta,x_0\\in\\mathbb{R}$ are constants.\n\nFix a base step size $h0$ and a threshold $R0$. Define a random, adaptively refined Euler–Maruyama time grid and approximation as follows. Set $t_0=0$, $X^{E}_0=x_0$, $\\Delta_0=h$. For each $k\\in\\mathbb{N}\\cup\\{0\\}$, define recursively\n- $t_{k+1}=t_k+\\Delta_k$,\n- $X^{E}_{k+1}=X^{E}_k+\\alpha X^{E}_k\\,\\Delta_k+\\beta X^{E}_k\\,(W_{t_{k+1}}-W_{t_k})$,\n- and the adaptive step update\n$$\n\\Delta_{k+1}=\\begin{cases}\n\\Delta_k/2,\\text{if }|X^{E}_k|R,\\\\\n\\Delta_k,\\text{if }|X^{E}_k|\\le R.\n\\end{cases}\n$$\nDefine the stepwise coefficient processes\n$$\nH(t):=\\sum_{k=0}^{\\infty}\\alpha X^{E}_k\\,\\mathbf{1}_{(t_k,t_{k+1}]}(t),\\qquad\nG(t):=\\sum_{k=0}^{\\infty}\\beta X^{E}_k\\,\\mathbf{1}_{(t_k,t_{k+1}]}(t).\n$$\n\nStarting from the basic properties of Brownian motion (independent, stationary Gaussian increments with mean $0$ and variance equal to the time increment) and the definitions above, do the following:\n\n1) Construct the scheme rigorously by arguing that for each $k$, the random variables $\\Delta_k$, $t_k$, and $X^{E}_k$ are $\\mathcal{F}_{t_k}$-measurable, and conclude that $H$ and $G$ are simple predictable processes with respect to $(\\mathcal{F}_t)_{t\\ge 0}$.\n\n2) Using only these measurability properties and the increment properties of $W$, derive a closed-form expression for the conditional expectation $\\mathbb{E}[X^{E}_{k+1}\\mid\\mathcal{F}_{t_k}]$ as a function of $X^{E}_k$, $\\alpha$, $\\beta$, and $\\Delta_k$.\n\nProvide your final answer as the single analytic expression for $\\mathbb{E}[X^{E}_{k+1}\\mid\\mathcal{F}_{t_k}]$. No numerical approximation is required or permitted.", "solution": "The problem is well-posed and scientifically grounded in the theory of stochastic differential equations and their numerical approximation. We proceed with the solution.\n\nThe problem asks for two main derivations. First, we must rigorously establish the measurability properties of the adaptive Euler-Maruyama scheme. Second, we must use these properties to compute a specific conditional expectation.\n\n1) Rigorous construction and predictability of the scheme.\nWe need to show that for each $k \\in \\mathbb{N} \\cup \\{0\\}$, the random variables $t_k$, $\\Delta_k$, and $X^{E}_k$ are $\\mathcal{F}_{t_k}$-measurable. A crucial part of this is to first show that each $t_k$ is a stopping time with respect to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$. We proceed by induction.\n\nLet the inductive hypothesis, $P(k)$, be the statement: \"$t_k$ is a stopping time, and the random variables $X^{E}_k$ and $\\Delta_k$ are $\\mathcal{F}_{t_k}$-measurable.\"\n\nBase case ($k=0$):\n- $t_0 = 0$. The constant time $0$ is a trivial stopping time. For any $t \\ge 0$, the set $\\{t_0 \\le t\\}$ is either $\\Omega$ (if $t \\ge 0$) or $\\emptyset$ (if $t  0$), both of which are in $\\mathcal{F}_t$.\n- $X^{E}_0 = x_0$. Since $x_0$ is a constant, $X^{E}_0$ is a constant random variable. A constant random variable is measurable with respect to any $\\sigma$-algebra, including $\\mathcal{F}_{t_0} = \\mathcal{F}_0$.\n- $\\Delta_0 = h$. Similarly, since $h$ is a constant, $\\Delta_0$ is $\\mathcal{F}_0$-measurable.\nThus, $P(0)$ holds.\n\nInductive step:\nAssume that for some integer $k \\ge 0$, the hypothesis $P(k)$ holds. That is, $t_k$ is a stopping time, and $X^{E}_k$ and $\\Delta_k$ are $\\mathcal{F}_{t_k}$-measurable. We need to show that $P(k+1)$ holds.\n\nFirst, let's establish that $t_{k+1}$ is a stopping time. By definition, $t_{k+1} = t_k + \\Delta_k$. By the inductive hypothesis, $t_k$ is a stopping time and $\\Delta_k$ is a non-negative, $\\mathcal{F}_{t_k}$-measurable random variable (since $h0$ and $\\Delta_k$ is always a positive fraction of $h$). A standard result in stochastic process theory states that the sum of a stopping time and a non-negative, measurable function of the history up to that time is also a stopping time. Therefore, $t_{k+1}$ is a stopping time.\n\nNext, we show that $X^{E}_{k+1}$ is $\\mathcal{F}_{t_{k+1}}$-measurable. The definition is\n$$X^{E}_{k+1} = X^{E}_k + \\alpha X^{E}_k \\Delta_k + \\beta X^{E}_k (W_{t_{k+1}} - W_{t_k}).$$\nBy the inductive hypothesis, $X^{E}_k$ and $\\Delta_k$ are $\\mathcal{F}_{t_k}$-measurable. Since $t_k \\le t_{k+1}$ (because $\\Delta_k  0$), we have $\\mathcal{F}_{t_k} \\subseteq \\mathcal{F}_{t_{k+1}}$. Thus, $X^{E}_k$ and $\\Delta_k$ are also $\\mathcal{F}_{t_{k+1}}$-measurable. The constants $\\alpha$ and $\\beta$ are also $\\mathcal{F}_{t_{k+1}}$-measurable.\nBy the definition of the filtration $(\\mathcal{F}_t)$, for any stopping time $T$, the random variable $W_T$ is $\\mathcal{F}_T$-measurable. Hence, $W_{t_{k+1}}$ is $\\mathcal{F}_{t_{k+1}}$-measurable and $W_{t_k}$ is $\\mathcal{F}_{t_k}}$-measurable (and thus also $\\mathcal{F}_{t_{k+1}}$-measurable).\nTherefore, every term on the right-hand side of the equation for $X^{E}_{k+1}$ is $\\mathcal{F}_{t_{k+1}}$-measurable. Since the set of $\\mathcal{F}_{t_{k+1}}$-measurable random variables is closed under addition and multiplication, $X^{E}_{k+1}$ is $\\mathcal{F}_{t_{k+1}}$-measurable.\n\nFinally, we show that $\\Delta_{k+1}$ is $\\mathcal{F}_{t_{k+1}}$-measurable. The update rule is\n$$\\Delta_{k+1} = \\frac{\\Delta_k}{2} \\mathbf{1}_{\\{|X^{E}_k|  R\\}} + \\Delta_k \\mathbf{1}_{\\{|X^{E}_k| \\le R\\}}.$$\nBy the inductive hypothesis, $X^{E}_k$ and $\\Delta_k$ are $\\mathcal{F}_{t_k}$-measurable. Since $|X^{E}_k|$ is a measurable function of $X^{E}_k$, it is also $\\mathcal{F}_{t_k}$-measurable. The set $\\{|X^{E}_k|  R\\}$ is therefore in $\\mathcal{F}_{t_k}$. This implies its indicator function $\\mathbf{1}_{\\{|X^{E}_k|  R\\}}$ is an $\\mathcal{F}_{t_k}$-measurable random variable. The same logic applies to $\\mathbf{1}_{\\{|X^{E}_k| \\le R\\}}$.\nSince $\\Delta_k$ is also $\\mathcal{F}_{t_k}$-measurable, and products and sums of $\\mathcal{F}_{t_k}$-measurable variables are $\\mathcal{F}_{t_k}$-measurable, it follows that $\\Delta_{k+1}$ is $\\mathcal{F}_{t_k}$-measurable.\nSince $\\mathcal{F}_{t_k} \\subseteq \\mathcal{F}_{t_{k+1}}$, $\\Delta_{k+1}$ is also $\\mathcal{F}_{t_{k+1}}$-measurable.\n\nThis completes the inductive step. We have shown that $P(k)$ implies $P(k+1)$. By the principle of induction, $P(k)$ is true for all $k \\in \\mathbb{N} \\cup \\{0\\}$.\n\nNow we conclude that the processes $H(t)$ and $G(t)$ are simple predictable processes. A process $Y(t)$ is a simple predictable process if it can be written in the form $Y(t) = \\sum_{j=0}^{\\infty} \\xi_j \\mathbf{1}_{(T_j, T_{j+1}]}(t)$, where $(T_j)_{j \\ge 0}$ is an increasing sequence of stopping times and each $\\xi_j$ is an $\\mathcal{F}_{T_j}$-measurable random variable.\nThe processes $H(t)$ and $G(t)$ are given by\n$$H(t) = \\sum_{k=0}^{\\infty} (\\alpha X^{E}_k) \\mathbf{1}_{(t_k, t_{k+1}]}(t), \\qquad G(t) = \\sum_{k=0}^{\\infty} (\\beta X^{E}_k) \\mathbf{1}_{(t_k, t_{k+1}]}(t).$$\nWe have just established that $(t_k)_{k \\ge 0}$ is a sequence of stopping times. We also showed that for each $k$, $X^{E}_k$ is $\\mathcal{F}_{t_k}$-measurable. Since $\\alpha$ and $\\beta$ are constants, the coefficients $\\alpha X^{E}_k$ and $\\beta X^{E}_k$ are $\\mathcal{F}_{t_k}$-measurable. Therefore, $H(t)$ and $G(t)$ perfectly match the definition of simple predictable processes.\n\n2) Derivation of the conditional expectation $\\mathbb{E}[X^{E}_{k+1} \\mid \\mathcal{F}_{t_k}]$.\nWe start with the recursive formula for $X^{E}_{k+1}$:\n$$X^{E}_{k+1} = X^{E}_k + \\alpha X^{E}_k \\Delta_k + \\beta X^{E}_k (W_{t_{k+1}} - W_{t_k}).$$\nWe take the conditional expectation with respect to $\\mathcal{F}_{t_k}$ on both sides. By the linearity of conditional expectation, we get:\n$$\\mathbb{E}[X^{E}_{k+1} \\mid \\mathcal{F}_{t_k}] = \\mathbb{E}[X^{E}_k \\mid \\mathcal{F}_{t_k}] + \\mathbb{E}[\\alpha X^{E}_k \\Delta_k \\mid \\mathcal{F}_{t_k}] + \\mathbb{E}[\\beta X^{E}_k (W_{t_{k+1}} - W_{t_k}) \\mid \\mathcal{F}_{t_k}].$$\nWe evaluate each term on the right-hand side:\n- For the first term, $\\mathbb{E}[X^{E}_k \\mid \\mathcal{F}_{t_k}]$: From part 1), we know $X^{E}_k$ is an $\\mathcal{F}_{t_k}$-measurable random variable. A fundamental property of conditional expectation is that if a random variable $Z$ is $\\mathcal{G}$-measurable, then $\\mathbb{E}[Z \\mid \\mathcal{G}] = Z$. Thus, $\\mathbb{E}[X^{E}_k \\mid \\mathcal{F}_{t_k}] = X^{E}_k$.\n- For the second term, $\\mathbb{E}[\\alpha X^{E}_k \\Delta_k \\mid \\mathcal{F}_{t_k}]$: From part 1), both $X^{E}_k$ and $\\Delta_k$ are $\\mathcal{F}_{t_k}$-measurable. Since $\\alpha$ is a constant, their product $\\alpha X^{E}_k \\Delta_k$ is also $\\mathcal{F}_{t_k}$-measurable. Therefore, $\\mathbb{E}[\\alpha X^{E}_k \\Delta_k \\mid \\mathcal{F}_{t_k}] = \\alpha X^{E}_k \\Delta_k$.\n- For the third term, $\\mathbb{E}[\\beta X^{E}_k (W_{t_{k+1}} - W_{t_k}) \\mid \\mathcal{F}_{t_k}]$: We use the property of \"taking out what is known\". Since $\\beta X^{E}_k$ is $\\mathcal{F}_{t_k}$-measurable, we can factor it out of the conditional expectation:\n$$\\mathbb{E}[\\beta X^{E}_k (W_{t_{k+1}} - W_{t_k}) \\mid \\mathcal{F}_{t_k}] = \\beta X^{E}_k \\mathbb{E}[W_{t_{k+1}} - W_{t_k} \\mid \\mathcal{F}_{t_k}].$$\nNow we must evaluate the remaining expectation, $\\mathbb{E}[W_{t_{k+1}} - W_{t_k} \\mid \\mathcal{F}_{t_k}]$. Here, $t_k$ is a stopping time. The strong Markov property of Brownian motion states that the process $B_s = W_{t_k+s} - W_{t_k}$ for $s \\ge 0$ is a standard Brownian motion that is independent of the pre-$t_k$ sigma-algebra $\\mathcal{F}_{t_k}$. We are interested in the increment $W_{t_{k+1}} - W_{t_k} = W_{t_k+\\Delta_k} - W_{t_k} = B_{\\Delta_k}$. Since $\\Delta_k$ is $\\mathcal{F}_{t_k}$-measurable, to compute $\\mathbb{E}[B_{\\Delta_k} \\mid \\mathcal{F}_{t_k}]$, we can reason as follows: for any given value $d  0$ that $\\Delta_k$ may take, the conditional expectation of $B_d$ given $\\mathcal{F}_{t_k}$ is simply $\\mathbb{E}[B_d]$ due to the independence of the process $B$ from $\\mathcal{F}_{t_k}$. The expectation of a Brownian motion at time $d$ is $\\mathbb{E}[B_d] = 0$. Since this holds for any possible value of $\\Delta_k$, the conditional expectation is $0$. More formally, this is a consequence of Wald's identity for martingales. Hence,\n$$\\mathbb{E}[W_{t_{k+1}} - W_{t_k} \\mid \\mathcal{F}_{t_k}] = 0.$$\nSubstituting this back into the expression for the third term gives:\n$$\\beta X^{E}_k \\mathbb{E}[W_{t_{k+1}} - W_{t_k} \\mid \\mathcal{F}_{t_k}] = \\beta X^{E}_k \\cdot 0 = 0.$$\n\nCombining all the simplified terms, we obtain the final expression:\n$$\\mathbb{E}[X^{E}_{k+1} \\mid \\mathcal{F}_{t_k}] = X^{E}_k + \\alpha X^{E}_k \\Delta_k + 0 = (1 + \\alpha \\Delta_k) X^{E}_k.$$\nThis is the closed-form expression for the conditional expectation as a function of $X^{E}_k$, $\\alpha$, and $\\Delta_k$.", "answer": "$$\\boxed{(1 + \\alpha \\Delta_k) X^{E}_k}$$", "id": "3074508"}, {"introduction": "Once we have a well-defined predictable approximation, such as the Euler-Maruyama scheme, we can analyze its local behavior. This exercise applies the scheme to the famous Geometric Brownian Motion model, which is central to financial mathematics. By calculating the conditional mean and variance of a single numerical step, you will see precisely how the approximation captures the underlying \"drift\" and \"diffusion\" dynamics dictated by the SDE. [@problem_id:3074485]", "problem": "Consider the geometric Brownian motion (GBM) defined by the stochastic differential equation (SDE) $dX_t=\\mu X_t\\,dt+\\sigma X_t\\,dW_t$, where $W_t$ is a standard Brownian motion and $\\mu,\\sigma$ are real constants. Let $\\{t_k\\}_{k\\ge 0}$ be a uniform time grid with step size $\\Delta t0$, so that $t_{k+1}-t_k=\\Delta t$, and define the Brownian increment $\\Delta W_k:=W_{t_{k+1}}-W_{t_k}$. Use the Euler–Maruyama time-stepping scheme to write a single-step approximation for $X_{k+1}$ in terms of $X_k$, $\\Delta t$, and $\\Delta W_k$, and then compute the conditional expectation $\\mathbb{E}[X_{k+1}\\mid X_k]$ and conditional variance $\\operatorname{Var}(X_{k+1}\\mid X_k)$ explicitly. You may use the facts that Brownian motion has independent increments and that $\\Delta W_k\\sim \\mathcal{N}(0,\\Delta t)$ (a mean-zero normal random variable with variance $\\Delta t$), independent of $\\mathcal{F}_{t_k}$, the filtration up to time $t_k$. The final answer should be given as two closed-form analytic expressions. No rounding is required.", "solution": "The user has provided a problem that requires analysis of the Euler-Maruyama discretization of a geometric Brownian motion process.\n\nThe problem is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- The stochastic differential equation (SDE) for geometric Brownian motion (GBM) is $dX_t=\\mu X_t\\,dt+\\sigma X_t\\,dW_t$.\n- $W_t$ is a standard Brownian motion.\n- $\\mu$ and $\\sigma$ are real constants.\n- $\\{t_k\\}_{k\\ge 0}$ is a uniform time grid with step size $\\Delta t  0$, where $t_{k+1}-t_k=\\Delta t$.\n- The Brownian increment is defined as $\\Delta W_k:=W_{t_{k+1}}-W_{t_k}$.\n- The numerical method to be used is the Euler–Maruyama time-stepping scheme.\n- The objective is to compute the conditional expectation $\\mathbb{E}[X_{k+1}\\mid X_k]$ and conditional variance $\\operatorname{Var}(X_{k+1}\\mid X_k)$.\n- It is given that Brownian motion has independent increments.\n- It is given that $\\Delta W_k$ is distributed as a normal random variable with mean $0$ and variance $\\Delta t$, denoted $\\Delta W_k\\sim \\mathcal{N}(0,\\Delta t)$.\n- It is given that $\\Delta W_k$ is independent of the filtration $\\mathcal{F}_{t_k}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the standard model of geometric Brownian motion and the Euler-Maruyama method, both of which are fundamental and well-established concepts in the field of stochastic differential equations and quantitative finance. The properties of Brownian motion cited are correct. The problem is scientifically sound.\n- **Well-Posed**: The problem statement is clear, and all necessary information is provided. The task is to derive two specific, well-defined quantities based on the given information. A unique analytical solution can be determined.\n- **Objective**: The problem is stated in precise, formal mathematical language, devoid of any subjective or ambiguous terms.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard exercise in the study of numerical solutions to stochastic differential equations. The solution process may proceed.\n\n### Solution Derivation\n\nThe SDE for geometric Brownian motion is given by:\n$$dX_t = \\mu X_t dt + \\sigma X_t dW_t$$\nThe general form of the Euler-Maruyama approximation for an SDE $dY_t = a(t, Y_t)dt + b(t, Y_t)dW_t$ over a time step $\\Delta t = t_{k+1} - t_k$ is:\n$$Y_{k+1} = Y_k + a(t_k, Y_k)\\Delta t + b(t_k, Y_k) \\Delta W_k$$\nwhere $Y_k \\approx Y_{t_k}$ and $\\Delta W_k = W_{t_{k+1}} - W_{t_k}$.\n\nFor the given GBM, the drift coefficient is $a(t, X_t) = \\mu X_t$ and the diffusion coefficient is $b(t, X_t) = \\sigma X_t$. Applying the Euler-Maruyama scheme, we substitute these into the general formula to obtain the single-step approximation for $X_{k+1}$:\n$$X_{k+1} = X_k + (\\mu X_k)\\Delta t + (\\sigma X_k)\\Delta W_k$$\nFactoring out $X_k$ from the right-hand side gives the required expression for $X_{k+1}$ in terms of $X_k$, $\\Delta t$, and $\\Delta W_k$:\n$$X_{k+1} = X_k(1 + \\mu \\Delta t + \\sigma \\Delta W_k)$$\n\nNext, we compute the conditional expectation $\\mathbb{E}[X_{k+1} \\mid X_k]$. The notation $\\mathbb{E}[\\cdot \\mid X_k]$ implies conditioning on the value of the process at time $t_k$. Since $X_k$ is known at time $t_k$ (i.e., it is $\\mathcal{F}_{t_k}$-measurable), it can be treated as a constant within the conditional expectation.\n$$\\mathbb{E}[X_{k+1} \\mid X_k] = \\mathbb{E}[X_k(1 + \\mu \\Delta t + \\sigma \\Delta W_k) \\mid X_k]$$\nUsing the linearity of expectation and the property that $\\mathbb{E}[A Y \\mid A] = A \\mathbb{E}[Y \\mid A]$ for a known value $A$:\n$$\\mathbb{E}[X_{k+1} \\mid X_k] = X_k \\mathbb{E}[1 + \\mu \\Delta t + \\sigma \\Delta W_k \\mid X_k]$$\n$$\\mathbb{E}[X_{k+1} \\mid X_k] = X_k ( \\mathbb{E}[1 \\mid X_k] + \\mathbb{E}[\\mu \\Delta t \\mid X_k] + \\mathbb{E}[\\sigma \\Delta W_k \\mid X_k] )$$\nThe terms $1$, $\\mu$, and $\\Delta t$ are deterministic constants. The term $\\sigma$ is also a constant.\n$$\\mathbb{E}[X_{k+1} \\mid X_k] = X_k (1 + \\mu \\Delta t + \\sigma \\mathbb{E}[\\Delta W_k \\mid X_k])$$\nWe are given that the increment $\\Delta W_k$ is independent of the filtration $\\mathcal{F}_{t_k}$, and since $X_k$ is $\\mathcal{F}_{t_k}$-measurable, $\\Delta W_k$ is independent of $X_k$. Therefore, the conditional expectation of $\\Delta W_k$ is equal to its unconditional expectation:\n$$\\mathbb{E}[\\Delta W_k \\mid X_k] = \\mathbb{E}[\\Delta W_k]$$\nWe are given that $\\Delta W_k \\sim \\mathcal{N}(0, \\Delta t)$, which implies its mean is $0$.\n$$\\mathbb{E}[\\Delta W_k] = 0$$\nSubstituting this result back into the expression for the conditional expectation of $X_{k+1}$:\n$$\\mathbb{E}[X_{k+1} \\mid X_k] = X_k(1 + \\mu \\Delta t + \\sigma \\cdot 0) = X_k(1 + \\mu \\Delta t)$$\n\nFinally, we compute the conditional variance $\\operatorname{Var}(X_{k+1} \\mid X_k)$. The definition of conditional variance is:\n$$\\operatorname{Var}(Y \\mid Z) = \\mathbb{E}[(Y - \\mathbb{E}[Y \\mid Z])^2 \\mid Z]$$\nIn our case, $Y = X_{k+1}$ and $Z = X_k$.\n$$\\operatorname{Var}(X_{k+1} \\mid X_k) = \\mathbb{E}[(X_{k+1} - \\mathbb{E}[X_{k+1} \\mid X_k])^2 \\mid X_k]$$\nLet's first find the term inside the square:\n$$X_{k+1} - \\mathbb{E}[X_{k+1} \\mid X_k] = X_k(1 + \\mu \\Delta t + \\sigma \\Delta W_k) - X_k(1 + \\mu \\Delta t)$$\n$$X_{k+1} - \\mathbb{E}[X_{k+1} \\mid X_k] = X_k + X_k \\mu \\Delta t + X_k \\sigma \\Delta W_k - X_k - X_k \\mu \\Delta t$$\n$$X_{k+1} - \\mathbb{E}[X_{k+1} \\mid X_k] = X_k \\sigma \\Delta W_k$$\nNow, substitute this back into the variance formula:\n$$\\operatorname{Var}(X_{k+1} \\mid X_k) = \\mathbb{E}[(X_k \\sigma \\Delta W_k)^2 \\mid X_k] = \\mathbb{E}[X_k^2 \\sigma^2 (\\Delta W_k)^2 \\mid X_k]$$\nSince $X_k$ and $\\sigma$ are known given the conditioning, we can factor them out of the expectation:\n$$\\operatorname{Var}(X_{k+1} \\mid X_k) = X_k^2 \\sigma^2 \\mathbb{E}[(\\Delta W_k)^2 \\mid X_k]$$\nDue to the independence of $\\Delta W_k$ and $X_k$:\n$$\\mathbb{E}[(\\Delta W_k)^2 \\mid X_k] = \\mathbb{E}[(\\Delta W_k)^2]$$\nThe variance of any random variable $Z$ is given by $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$. For $Z = \\Delta W_k$, we know $\\operatorname{Var}(\\Delta W_k) = \\Delta t$ and $\\mathbb{E}[\\Delta W_k] = 0$.\n$$\\Delta t = \\mathbb{E}[(\\Delta W_k)^2] - (0)^2 \\implies \\mathbb{E}[(\\Delta W_k)^2] = \\Delta t$$\nSubstituting this result into the expression for conditional variance:\n$$\\operatorname{Var}(X_{k+1} \\mid X_k) = X_k^2 \\sigma^2 \\Delta t$$\nRewriting for clarity using standard conventions, the variance is $\\sigma^2 X_k^2 \\Delta t$.\n\nThe two requested expressions are the conditional expectation, $\\mathbb{E}[X_{k+1}\\mid X_k] = X_k(1 + \\mu \\Delta t)$, and the conditional variance, $\\operatorname{Var}(X_{k+1}\\mid X_k) = \\sigma^2 X_k^2 \\Delta t$.", "answer": "$$\n\\boxed{\\begin{pmatrix} X_k(1 + \\mu \\Delta t)  \\sigma^{2} X_k^{2} \\Delta t \\end{pmatrix}}\n$$", "id": "3074485"}, {"introduction": "What happens if we break the rule of predictability? This provocative thought experiment explores the consequences by constructing a simple process that is \"anticipative,\" meaning its coefficients are allowed to peek into the future. By performing a direct calculation, you will discover that the Itô Isometry—a cornerstone of stochastic integration—fails to hold, providing a concrete demonstration of why predictability is a non-negotiable requirement in the theory. [@problem_id:3074477]", "problem": "Let $\\{W_{t}\\}_{t \\in [0,1]}$ be a standard Brownian motion with $W_{0}=0$. Consider the partition $0=t_{0}t_{1}t_{2}=1$ with $t_{1}=\\frac{1}{2}$. Define a simple process $H_{t}$ on $(0,1]$ by choosing, for $k \\in \\{0,1\\}$, the coefficients $\\xi_{k}$ to depend on the right-endpoint values\n$$\n\\xi_{0} := W_{t_{1}}, \\qquad \\xi_{1} := W_{t_{2}},\n$$\nand setting\n$$\nH_{t} := \\xi_{0}\\,\\mathbf{1}_{(t_{0},t_{1}]}(t) + \\xi_{1}\\,\\mathbf{1}_{(t_{1},t_{2}]}(t).\n$$\nDefine the stepwise integral and its quadratic form by\n$$\nI := \\sum_{k=0}^{1} \\xi_{k}\\,\\big(W_{t_{k+1}}-W_{t_{k}}\\big), \\qquad Q := \\sum_{k=0}^{1} \\xi_{k}^{2}\\,(t_{k+1}-t_{k}).\n$$\nUsing only the independence and Gaussianity of Brownian increments, compute explicitly $\\mathbb{E}[I^{2}]$ and $\\mathbb{E}[Q]$ and then determine the exact value of\n$$\n\\mathbb{E}[I^{2}] - \\mathbb{E}[Q].\n$$\nGive your final answer as a single exact number. No rounding is required.", "solution": "The problem statement is entirely self-contained, mathematically well-posed, and free of scientific or factual inconsistencies. Therefore, we may proceed with the solution.\n\nThe problem specifies a partition of the interval $[0,1]$ as $t_{0}=0$, $t_{1}=\\frac{1}{2}$, and $t_{2}=1$. The coefficients of the simple process $H_t$ are given by $\\xi_{0} := W_{t_{1}} = W_{1/2}$ and $\\xi_{1} := W_{t_{2}} = W_{1}$.\n\nThe stepwise integral $I$ is defined as\n$$\nI := \\sum_{k=0}^{1} \\xi_{k}\\,\\big(W_{t_{k+1}}-W_{t_{k}}\\big) = \\xi_{0}(W_{t_{1}}-W_{t_{0}}) + \\xi_{1}(W_{t_{2}}-W_{t_{1}}).\n$$\nSubstituting the expressions for $\\xi_k$ and the given values of $t_k$, and using the fact that $W_{t_0}=W_0=0$, we have\n$$\nI = W_{1/2}(W_{1/2}-W_{0}) + W_{1}(W_{1}-W_{1/2}) = W_{1/2}^{2} + W_{1}(W_{1}-W_{1/2}).\n$$\n\nThe quadratic form $Q$ is defined as\n$$\nQ := \\sum_{k=0}^{1} \\xi_{k}^{2}\\,(t_{k+1}-t_{k}) = \\xi_{0}^{2}(t_{1}-t_{0}) + \\xi_{1}^{2}(t_{2}-t_{1}).\n$$\nSubstituting the expressions for $\\xi_k$ and the values of $t_k$:\n$$\nQ = W_{1/2}^{2}\\left(\\frac{1}{2}-0\\right) + W_{1}^{2}\\left(1-\\frac{1}{2}\\right) = \\frac{1}{2}W_{1/2}^{2} + \\frac{1}{2}W_{1}^{2}.\n$$\n\nThe objective is to compute $\\mathbb{E}[I^{2}] - \\mathbb{E}[Q]$. We will compute $\\mathbb{E}[Q]$ and $\\mathbb{E}[I^{2}]$ separately.\n\nFirst, let us compute $\\mathbb{E}[Q]$. By the linearity of expectation,\n$$\n\\mathbb{E}[Q] = \\mathbb{E}\\left[\\frac{1}{2}W_{1/2}^{2} + \\frac{1}{2}W_{1}^{2}\\right] = \\frac{1}{2}\\mathbb{E}[W_{1/2}^{2}] + \\frac{1}{2}\\mathbb{E}[W_{1}^{2}].\n$$\nFor a standard Brownian motion $\\{W_t\\}_{t \\ge 0}$, the random variable $W_t$ follows a normal distribution $N(0, t)$. The second moment is $\\mathbb{E}[W_t^2] = \\text{Var}(W_t) + (\\mathbb{E}[W_t])^2 = t + 0^2 = t$.\nUsing this property, we find $\\mathbb{E}[W_{1/2}^{2}] = \\frac{1}{2}$ and $\\mathbb{E}[W_{1}^{2}] = 1$.\nSubstituting these into the expression for $\\mathbb{E}[Q]$:\n$$\n\\mathbb{E}[Q] = \\frac{1}{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}(1) = \\frac{1}{4} + \\frac{1}{2} = \\frac{3}{4}.\n$$\n\nNext, we compute $\\mathbb{E}[I^{2}]$. To facilitate this calculation, we express the process in terms of independent increments. Let us define the increments:\n$$\n\\Delta_{1} := W_{t_{1}} - W_{t_{0}} = W_{1/2} - 0 = W_{1/2}\n$$\n$$\n\\Delta_{2} := W_{t_{2}} - W_{t_{1}} = W_{1} - W_{1/2}\n$$\nFrom the properties of standard Brownian motion, $\\Delta_{1}$ and $\\Delta_{2}$ are independent random variables. Their distributions are $\\Delta_{1} \\sim N(0, t_{1}-t_{0}) = N(0, 1/2)$ and $\\Delta_{2} \\sim N(0, t_{2}-t_{1}) = N(0, 1/2)$.\n\nWe can write $W_{1/2}$ and $W_{1}$ in terms of these increments:\n$$\nW_{1/2} = \\Delta_{1}\n$$\n$$\nW_{1} = W_{1/2} + (W_{1}-W_{1/2}) = \\Delta_{1} + \\Delta_{2}\n$$\nNow, substitute these into the expression for $I$:\n$$\nI = W_{1/2}^{2} + W_{1}(W_{1}-W_{1/2}) = \\Delta_{1}^{2} + (\\Delta_{1}+\\Delta_{2})\\Delta_{2} = \\Delta_{1}^{2} + \\Delta_{1}\\Delta_{2} + \\Delta_{2}^{2}.\n$$\nWe need to compute the expectation of the square of this expression:\n$$\n\\mathbb{E}[I^2] = \\mathbb{E}\\left[(\\Delta_{1}^{2} + \\Delta_{1}\\Delta_{2} + \\Delta_{2}^{2})^2\\right].\n$$\nExpanding the square:\n$$\nI^2 = (\\Delta_{1}^{2})^2 + (\\Delta_{1}\\Delta_{2})^2 + (\\Delta_{2}^{2})^2 + 2(\\Delta_{1}^{2})(\\Delta_{1}\\Delta_{2}) + 2(\\Delta_{1}^{2})(\\Delta_{2}^{2}) + 2(\\Delta_{1}\\Delta_{2})(\\Delta_{2}^{2})\n$$\n$$\nI^2 = \\Delta_{1}^{4} + \\Delta_{1}^{2}\\Delta_{2}^{2} + \\Delta_{2}^{4} + 2\\Delta_{1}^{3}\\Delta_{2} + 2\\Delta_{1}^{2}\\Delta_{2}^{2} + 2\\Delta_{1}\\Delta_{2}^{3}.\n$$\nCombining terms, we get:\n$$\nI^2 = \\Delta_{1}^{4} + \\Delta_{2}^{4} + 3\\Delta_{1}^{2}\\Delta_{2}^{2} + 2\\Delta_{1}^{3}\\Delta_{2} + 2\\Delta_{1}\\Delta_{2}^{3}.\n$$\nTaking the expectation and using the independence of $\\Delta_1$ and $\\Delta_2$:\n$$\n\\mathbb{E}[I^2] = \\mathbb{E}[\\Delta_{1}^{4}] + \\mathbb{E}[\\Delta_{2}^{4}] + 3\\mathbb{E}[\\Delta_{1}^{2}]\\mathbb{E}[\\Delta_{2}^{2}] + 2\\mathbb{E}[\\Delta_{1}^{3}]\\mathbb{E}[\\Delta_{2}] + 2\\mathbb{E}[\\Delta_{1}]\\mathbb{E}[\\Delta_{2}^{3}].\n$$\nFor a Gaussian random variable $X \\sim N(0, \\sigma^2)$, the moments are $\\mathbb{E}[X]=0$, $\\mathbb{E}[X^2]=\\sigma^2$, $\\mathbb{E}[X^3]=0$, and $\\mathbb{E}[X^4]=3(\\sigma^2)^2$.\nFor both $\\Delta_{1}$ and $\\Delta_{2}$, the variance is $\\sigma^2 = 1/2$. Thus:\n- $\\mathbb{E}[\\Delta_{1}] = \\mathbb{E}[\\Delta_{2}] = 0$\n- $\\mathbb{E}[\\Delta_{1}^{2}] = \\mathbb{E}[\\Delta_{2}^{2}] = 1/2$\n- $\\mathbb{E}[\\Delta_{1}^{3}] = \\mathbb{E}[\\Delta_{2}^{3}] = 0$\n- $\\mathbb{E}[\\Delta_{1}^{4}] = \\mathbb{E}[\\Delta_{2}^{4}] = 3(1/2)^2 = 3/4$\n\nSubstituting these values into the expression for $\\mathbb{E}[I^2]$:\n$$\n\\mathbb{E}[I^2] = \\frac{3}{4} + \\frac{3}{4} + 3\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + 2(0)(0) + 2(0)(0) = \\frac{6}{4} + \\frac{3}{4} = \\frac{9}{4}.\n$$\nFinally, we compute the required difference:\n$$\n\\mathbb{E}[I^{2}] - \\mathbb{E}[Q] = \\frac{9}{4} - \\frac{3}{4} = \\frac{6}{4} = \\frac{3}{2}.\n$$\nThe result highlights that for an anticipative simple process (where coefficients depend on future values of the Brownian motion), the Itô isometry, which would state $\\mathbb{E}[I^2] = \\mathbb{E}[Q]$, does not hold. The discrepancy $\\mathbb{E}[I^2] - \\mathbb{E}[Q]$ is a non-zero value which we have computed to be $\\frac{3}{2}$.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "3074477"}]}