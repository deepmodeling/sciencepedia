{"hands_on_practices": [{"introduction": "A foundational application of the Feynman-Kac formula is to answer questions about how long a stochastic process is expected to remain within a certain domain. This practice exercise guides you through a classic problem: calculating the expected exit time of a Brownian motion from an interval [@problem_id:3080617]. By translating a probabilistic question about a random duration into a deterministic boundary value problem, you will see firsthand how the generator of a process provides a direct bridge to a solvable differential equation.", "problem": "Consider a one-dimensional standard Brownian motion $X_t$ started at $X_0 = x \\in (0,L)$, where $L > 0$, and let the state space be the open interval $D = (0,L)$ with absorbing boundaries at $0$ and $L$. Define the first exit time $\\tau = \\inf\\{t \\geq 0 : X_t \\notin D\\}$. Let the running accumulation rate be the constant function $f \\equiv c$ with $c > 0$. Define the functional\n$$\nu(x) = \\mathbb{E}_x\\left[\\int_{0}^{\\tau} c \\, dt\\right],\n$$\nwhich represents the expected accumulated amount until exit.\n\nUsing only first principles from stochastic calculus (in particular, Itô’s formula for time-homogeneous functions of Brownian motion and the characterization of the infinitesimal generator of Brownian motion), derive the boundary value problem that $u$ must satisfy and solve it explicitly to obtain $u(x)$ as a closed-form analytic expression in terms of $c$, $L$, and $x$.\n\nYour final answer must be a single closed-form expression. No rounding is required and no units are involved.", "solution": "The problem asks for the explicit closed-form expression for the functional $u(x) = \\mathbb{E}_x\\left[\\int_{0}^{\\tau} c \\, dt\\right]$, where $X_t$ is a one-dimensional standard Brownian motion starting at $X_0=x \\in (0,L)$, $\\tau$ is the first exit time from the open interval $D=(0,L)$, and $c$ is a positive constant. The derivation must use first principles from stochastic calculus.\n\nFirst, we can simplify the expression for $u(x)$. Since $c$ is a constant, we can take it out of the integral and the expectation:\n$$\nu(x) = \\mathbb{E}_x\\left[c \\int_{0}^{\\tau} 1 \\, dt\\right] = c \\, \\mathbb{E}_x\\left[\\tau\\right]\n$$\nThis shows that $u(x)$ is proportional to the expected exit time from the interval $(0,L)$. We aim to find a differential equation that $u(x)$ must satisfy.\n\nThe core principle we will use is the construction of a martingale. Let $u(x)$ be a function in $C^2((0,L))$, the space of twice continuously differentiable functions on $(0,L)$. The process $X_t$ is a standard Brownian motion, so its stochastic differential equation is $dX_t = dB_t$, where $B_t$ is a standard Wiener process. The quadratic variation of $X_t$ is $d[X]_t = dt$.\n\nLet us define a new process $M_t = u(X_t) + \\int_0^t c \\, ds = u(X_t) + ct$. We will find the function $u(x)$ such that the stopped process $M_{t \\wedge \\tau}$ is a martingale. A process is a martingale if its drift term is zero. To find the drift, we apply Itô's formula to $u(X_t)$. For a time-homogeneous function $u(x)$, Itô's formula states:\n$$\ndu(X_t) = u'(X_t) dX_t + \\frac{1}{2} u''(X_t) d[X]_t\n$$\nSubstituting $dX_t = dB_t$ and $d[X]_t = dt$, we get:\n$$\ndu(X_t) = u'(X_t) dB_t + \\frac{1}{2} u''(X_t) dt\n$$\nNow, let's find the differential of the process $M_t$:\n$$\ndM_t = du(X_t) + d(ct) = \\left(u'(X_t) dB_t + \\frac{1}{2} u''(X_t) dt\\right) + c \\, dt\n$$\n$$\ndM_t = u'(X_t) dB_t + \\left(\\frac{1}{2} u''(X_t) + c\\right) dt\n$$\nFor $M_t$ to be a local martingale, its drift term (the coefficient of $dt$) must be zero for all $x \\in (0,L)$. The term $\\frac{1}{2}u''(x)$ is the action of the infinitesimal generator of the standard Brownian motion, $\\mathcal{A} = \\frac{1}{2}\\frac{d^2}{dx^2}$, on the function $u(x)$. The condition that the drift is zero, $\\mathcal{A}u(x) + c = 0$, gives us the following second-order ordinary differential equation (ODE):\n$$\n\\frac{1}{2} u''(x) + c = 0\n$$\nwhich simplifies to:\n$$\nu''(x) = -2c\n$$\nThis ODE must hold for all $x \\in D = (0,L)$.\n\nNext, we must determine the boundary conditions for this ODE. The functional $u(x)$ is defined in terms of the first exit time $\\tau = \\inf\\{t \\geq 0 : X_t \\notin (0,L)\\}$. If the process starts at one of the boundaries, i.e., $x=0$ or $x=L$, it is already not in the open interval $D=(0,L)$. Therefore, the first exit time is $\\tau=0$.\nFor $x=0$:\n$$\nu(0) = \\mathbb{E}_0\\left[\\int_{0}^{0} c \\, dt\\right] = \\mathbb{E}_0[0] = 0\n$$\nFor $x=L$:\n$$\nu(L) = \\mathbb{E}_L\\left[\\int_{0}^{0} c \\, dt\\right] = \\mathbb{E}_L[0] = 0\n$$\nSo, we have a boundary value problem (BVP) consisting of the ODE and two boundary conditions:\n$$\n\\begin{cases}\nu''(x) = -2c, & x \\in (0,L) \\\\\nu(0) = 0 \\\\\nu(L) = 0\n\\end{cases}\n$$\nWe solve this BVP by integrating the ODE twice with respect to $x$.\nFirst integration yields the derivative $u'(x)$:\n$$\nu'(x) = \\int -2c \\, dx = -2cx + A\n$$\nwhere $A$ is a constant of integration.\nSecond integration yields $u(x)$:\n$$\nu(x) = \\int (-2cx + A) \\, dx = -c x^2 + Ax + B\n$$\nwhere $B$ is a second constant of integration.\n\nNow, we apply the boundary conditions to determine the constants $A$ and $B$.\nUsing $u(0) = 0$:\n$$\n-c(0)^2 + A(0) + B = 0 \\implies B=0\n$$\nThe solution simplifies to $u(x) = -cx^2 + Ax$.\nUsing $u(L) = 0$:\n$$\n-cL^2 + A L = 0\n$$\nSince $L > 0$ as given in the problem, we can divide by $L$:\n$$\n-cL + A = 0 \\implies A = cL\n$$\nSubstituting the determined values of $A=cL$ and $B=0$ back into the general solution for $u(x)$, we obtain the explicit closed-form expression:\n$$\nu(x) = -cx^2 + (cL)x\n$$\nWe can factor this expression to write it more compactly:\n$$\nu(x) = cx(L-x)\n$$\nThis function represents the expected accumulated amount $c$ until the Brownian motion exits the interval $(0,L)$, starting from position $x$.", "answer": "$$\\boxed{cx(L-x)}$$", "id": "3080617"}, {"introduction": "Building on the concept of accumulated values, this exercise introduces two critical features common in financial and economic applications: a continuous discount rate and a state-dependent payoff. You will calculate the present value of an infinite stream of payments generated by an asset whose price follows a Brownian motion [@problem_id:1337995]. This problem illustrates how the Feynman-Kac framework elegantly handles valuation problems by incorporating the time value of money and variable cash flows into its governing partial differential equation.", "problem": "An asset's price, denoted by $X_s$ at time $s$, is modeled as a standard one-dimensional Brownian motion. Its evolution is described by the stochastic differential equation $dX_s = dW_s$, where $W_s$ is a standard Wiener process. The asset starts at an initial price $X_0 = x$.\n\nThis asset generates a continuous stream of payments. At any time $s \\ge 0$, the instantaneous rate of payment is given by the square of the asset's price, i.e., the payment rate is $f(X_s) = X_s^2$.\n\nAn investor wishes to calculate the total present value of all future payments from this asset over an infinite time horizon. The future payments are discounted continuously at a constant risk-free interest rate $r > 0$.\n\nDetermine the expected total discounted value of this infinite stream of payments as a function of the initial price $x$ and the discount rate $r$.", "solution": "Let $X_{s}$ be a standard Brownian motion starting from $X_{0}=x$, so $dX_{s}=dW_{s}$ and $X_{s}=x+W_{s}$. The total discounted value of the running payoff $f(X_{s})=X_{s}^{2}$ is\n$$\nV(x)=\\mathbb{E}_{x}\\left[\\int_{0}^{\\infty} e^{-r s}\\,X_{s}^{2}\\,ds\\right], \\quad r>0.\n$$\nSince $X_{s}^{2}\\ge 0$, Tonelli’s theorem justifies interchanging expectation and integration:\n$$\nV(x)=\\int_{0}^{\\infty} e^{-r s}\\,\\mathbb{E}_{x}[X_{s}^{2}]\\,ds.\n$$\nBecause $X_{s}=x+W_{s}$ with $\\mathbb{E}[W_{s}]=0$ and $\\mathbb{V}\\mathrm{ar}(W_{s})=s$, we have\n$$\n\\mathbb{E}_{x}[X_{s}^{2}]=\\mathbb{V}\\mathrm{ar}(X_{s})+\\left(\\mathbb{E}_{x}[X_{s}]\\right)^{2}=s+x^{2}.\n$$\nHence\n$$\nV(x)=\\int_{0}^{\\infty} e^{-r s}\\,(x^{2}+s)\\,ds\n= x^{2}\\int_{0}^{\\infty} e^{-r s}\\,ds + \\int_{0}^{\\infty} s\\,e^{-r s}\\,ds.\n$$\nFor $r>0$,\n$$\n\\int_{0}^{\\infty} e^{-r s}\\,ds=\\frac{1}{r}.\n$$\nFor the second integral, integrate by parts with $u=s$ and $dv=e^{-r s}\\,ds$, so $du=ds$ and $v=-(1/r)e^{-r s}$:\n$$\n\\int_{0}^{\\infty} s\\,e^{-r s}\\,ds=\\left.-\\frac{s}{r}e^{-r s}\\right|_{0}^{\\infty}+\\frac{1}{r}\\int_{0}^{\\infty}e^{-r s}\\,ds\n=0+\\frac{1}{r}\\cdot\\frac{1}{r}=\\frac{1}{r^{2}}.\n$$\nTherefore,\n$$\nV(x)=\\frac{x^{2}}{r}+\\frac{1}{r^{2}}.\n$$\nAs a consistency check via the Feynman–Kac formula, $v(x)=V(x)$ solves the ODE $(r-\\mathcal{L})v=f$ where $f(x)=x^2$ and the generator is $\\mathcal{L}=\\frac{1}{2}\\frac{d^{2}}{dx^{2}}$. This gives the equation $r v(x)-\\frac{1}{2}v''(x)=x^{2}$. We test the proposed solution $v(x)=\\frac{x^{2}}{r}+\\frac{1}{r^{2}}$. Its second derivative is $v''(x) = \\frac{2}{r}$. Substituting into the ODE: $r\\left(\\frac{x^{2}}{r}+\\frac{1}{r^{2}}\\right) - \\frac{1}{2}\\left(\\frac{2}{r}\\right) = \\left(x^2 + \\frac{1}{r}\\right) - \\frac{1}{r} = x^2$. The equation holds. The homogeneous solutions $\\exp(\\pm\\sqrt{2r}\\,x)$ are excluded by the requirement of at most polynomial growth, confirming the result.", "answer": "$$\\boxed{\\frac{x^{2}}{r}+\\frac{1}{r^{2}}}$$", "id": "1337995"}, {"introduction": "To demonstrate the versatility of the Feynman-Kac formula, we now move from standard Brownian motion to the mean-reverting Ornstein-Uhlenbeck process, a cornerstone for modeling phenomena in both finance and the physical sciences. This advanced practice challenges you to find the exact evolution of an initial probability distribution over time by solving the associated forward Kolmogorov equation [@problem_id:3080589]. This exercise highlights the power of the probabilistic approach in solving complex partial differential equations and understanding how expectations evolve.", "problem": "Consider the one-dimensional Itô stochastic differential equation (SDE) for the Ornstein–Uhlenbeck process given by\n$$\ndX_{t}=-\\theta\\,X_{t}\\,dt+\\sigma\\,dW_{t},\\qquad X_{0}=x,\n$$\nwhere $\\theta>0$, $\\sigma>0$, $x\\in\\mathbb{R}$, and $(W_{t})_{t\\geq 0}$ is a standard Brownian motion. Let $\\mathcal{L}$ denote the infinitesimal generator associated with this SDE. Define $u:[0,\\infty)\\times\\mathbb{R}\\to\\mathbb{R}$ as the classical solution to the linear parabolic partial differential equation\n$$\n\\partial_{t}u(t,x)=\\mathcal{L}u(t,x),\\qquad t>0,\\ x\\in\\mathbb{R},\n$$\nwith the Gaussian initial condition\n$$\nu(0,x)=g(x):=\\exp\\!\\big(-a\\,x^{2}+b\\,x+c\\big),\n$$\nwhere $a>0$, $b\\in\\mathbb{R}$, and $c\\in\\mathbb{R}$ are fixed constants. Starting from the basic definitions of the generator of an Itô diffusion and the continuous-time Markov semigroup, and using only the foundational relationship between diffusion generators and linear parabolic equations provided by the Feynman–Kac (F-K) formula, derive a representation of $u(t,x)$ as an expectation with respect to the law of $X_{t}$ and compute $u(t,x)$ explicitly. Express your final answer as a single closed-form analytic expression in terms of $a$, $b$, $c$, $\\theta$, $\\sigma$, $t$, and $x$. No rounding is required, and no units are involved.", "solution": "The problem asks for the explicit solution $u(t,x)$ to a linear parabolic partial differential equation (PDE) whose generator is associated with an Ornstein-Uhlenbeck process, with a Gaussian initial condition. The derivation must use the Feynman–Kac formula.\n\n**Step 1: Infinitesimal Generator**\nFor a general one-dimensional Itô diffusion $dY_t = \\mu(Y_t) dt + \\eta(Y_t) dW_t$, the infinitesimal generator $\\mathcal{L}$ acting on a sufficiently smooth function $f(y)$ is given by\n$$\n\\mathcal{L}f(y) = \\mu(y) \\frac{df}{dy}(y) + \\frac{1}{2} \\eta(y)^2 \\frac{d^2f}{dy^2}(y).\n$$\nFor the given Ornstein–Uhlenbeck process, the drift coefficient is $\\mu(x) = -\\theta x$ and the diffusion coefficient is $\\eta(x) = \\sigma$ (a constant). Therefore, the generator is\n$$\n\\mathcal{L} = -\\theta x \\frac{\\partial}{\\partial x} + \\frac{\\sigma^2}{2} \\frac{\\partial^2}{\\partial x^2}.\n$$\n\n**Step 2: Feynman–Kac Representation**\nThe given PDE is $\\partial_{t}u(t,x)=\\mathcal{L}u(t,x)$ with initial condition $u(0,x)=g(x)$. This is the forward Kolmogorov equation associated with the process $X_t$. The solution is given by the action of the Markov semigroup $(T_t)_{t \\ge 0}$ on the initial function $g(x)$. The Feynman–Kac formula (in its simplest form, corresponding to a zero potential) provides the solution as an expectation with respect to the law of the stochastic process $X_t$ starting at $x$.\nLet $X_t^x$ denote the solution to the SDE with initial condition $X_0=x$. The solution to the PDE is\n$$\nu(t,x) = T_t g(x) = \\mathbb{E}[g(X_t^x)].\n$$\nSubstituting the given form of $g(x)$, we have\n$$\nu(t,x) = \\mathbb{E}\\left[\\exp\\left(-a(X_t^x)^{2}+bX_t^x+c\\right)\\right].\n$$\n\n**Step 3: The Ornstein–Uhlenbeck Process**\nThe SDE $dX_{t}=-\\theta\\,X_{t}\\,dt+\\sigma\\,dW_{t}$ is a linear SDE. We can solve it explicitly using an integrating factor $e^{\\theta t}$. Let $Y_t = e^{\\theta t} X_t$. By Itô's product rule:\n$$\ndY_t = d(e^{\\theta t} X_t) = (\\theta e^{\\theta t} X_t) dt + e^{\\theta t} dX_t = \\theta e^{\\theta t} X_t dt + e^{\\theta t}(-\\theta X_t dt + \\sigma dW_t) = \\sigma e^{\\theta t} dW_t.\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\nY_t - Y_0 = \\int_0^t \\sigma e^{\\theta s} dW_s.\n$$\nSubstituting back $Y_t = e^{\\theta t} X_t$ and $Y_0=X_0=x$:\n$$\ne^{\\theta t} X_t - x = \\sigma \\int_0^t e^{\\theta s} dW_s.\n$$\nSolving for $X_t$ (which we denote as $X_t^x$ to emphasize the starting point):\n$$\nX_t^x = x e^{-\\theta t} + \\sigma \\int_0^t e^{-\\theta(t-s)} dW_s.\n$$\nSince $X_t^x$ is defined by a constant plus an Itô integral of a deterministic function, it is a Gaussian random variable. We compute its mean and variance.\nThe mean is:\n$$\n\\mu_t(x) = \\mathbb{E}[X_t^x] = \\mathbb{E}[x e^{-\\theta t}] + \\sigma \\mathbb{E}\\left[\\int_0^t e^{-\\theta(t-s)} dW_s\\right] = x e^{-\\theta t},\n$$\nsince the expectation of an Itô integral is zero.\nThe variance is, by Itô isometry:\n$$\n\\Sigma_t^2 = \\text{Var}(X_t^x) = \\mathbb{E}[(X_t^x - \\mu_t(x))^2] = \\mathbb{E}\\left[\\left(\\sigma \\int_0^t e^{-\\theta(t-s)} dW_s\\right)^2\\right] = \\sigma^2 \\int_0^t \\left(e^{-\\theta(t-s)}\\right)^2 ds.\n$$\n$$\n\\Sigma_t^2 = \\sigma^2 \\int_0^t e^{-2\\theta(t-s)} ds = \\sigma^2 e^{-2\\theta t} \\int_0^t e^{2\\theta s} ds = \\sigma^2 e^{-2\\theta t} \\left[\\frac{e^{2\\theta s}}{2\\theta}\\right]_0^t = \\sigma^2 e^{-2\\theta t} \\frac{e^{2\\theta t}-1}{2\\theta} = \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t}).\n$$\nThus, $X_t^x$ is normally distributed, $X_t^x \\sim \\mathcal{N}\\left(\\mu_t(x), \\Sigma_t^2\\right)$, where the mean depends on $x$ but the variance does not.\n\n**Step 4: Computation of the Expectation**\nWe need to compute $u(t,x) = \\mathbb{E}[\\exp(-a(X_t^x)^2 + bX_t^x + c)]$. Let $X_t^x$ be represented as $X_t^x = \\mu_t(x) + \\Sigma_t Z$, where $Z \\sim \\mathcal{N}(0,1)$.\n$$\nu(t,x) = e^c \\, \\mathbb{E}\\left[\\exp\\left(-a(\\mu_t(x)+\\Sigma_t Z)^2 + b(\\mu_t(x)+\\Sigma_t Z)\\right)\\right]\n$$\n$$\nu(t,x) = e^c \\exp(-a\\mu_t(x)^2 + b\\mu_t(x)) \\, \\mathbb{E}\\left[\\exp\\left(-a\\Sigma_t^2 Z^2 + (b\\Sigma_t - 2a\\mu_t(x)\\Sigma_t)Z\\right)\\right].\n$$\nWe use the general formula for a Gaussian expectation of this form. For $Z \\sim \\mathcal{N}(0,1)$ and constants $A> -1/2$, $B$:\n$$\n\\mathbb{E}[\\exp(-AZ^2+BZ)] = \\int_{-\\infty}^\\infty \\exp(-Az^2+Bz) \\frac{e^{-z^2/2}}{\\sqrt{2\\pi}} dz = \\frac{1}{\\sqrt{1+2A}}\\exp\\left(\\frac{B^2}{2(1+2A)}\\right).\n$$\nIn our case, $A = a\\Sigma_t^2$ and $B = (b - 2a\\mu_t(x))\\Sigma_t$. Since $a>0$ and $\\Sigma_t^2 \\geq 0$, we have $A \\ge 0$, so the condition $A > -1/2$ is satisfied.\nThe expectation term evaluates to:\n$$\n\\frac{1}{\\sqrt{1+2a\\Sigma_t^2}} \\exp\\left(\\frac{((b-2a\\mu_t(x))\\Sigma_t)^2}{2(1+2a\\Sigma_t^2)}\\right) = \\frac{1}{\\sqrt{1+2a\\Sigma_t^2}} \\exp\\left(\\frac{\\Sigma_t^2(b-2a\\mu_t(x))^2}{2(1+2a\\Sigma_t^2)}\\right).\n$$\nSubstituting this back into the expression for $u(t,x)$:\n$$\nu(t,x) = \\frac{e^c}{\\sqrt{1+2a\\Sigma_t^2}} \\exp\\left(-a\\mu_t(x)^2 + b\\mu_t(x) + \\frac{\\Sigma_t^2(b-2a\\mu_t(x))^2}{2(1+2a\\Sigma_t^2)}\\right).\n$$\nLet's simplify the argument of the exponential. Let $D(t) = 1+2a\\Sigma_t^2$. The argument of the exponential (excluding the leading $c$) becomes:\n$$\n\\frac{(-a\\mu_t(x)^2+b\\mu_t(x))(1+2a\\Sigma_t^2) + \\frac{1}{2}\\Sigma_t^2(b-2a\\mu_t(x))^2}{1+2a\\Sigma_t^2}\n$$\nFocusing on the fraction's numerator:\n$$\n(-a\\mu_t^2+b\\mu_t)(1+2a\\Sigma_t^2) + \\frac{1}{2}\\Sigma_t^2(b^2-4ab\\mu_t+4a^2\\mu_t^2)\n= -a\\mu_t^2 + b\\mu_t -2a^2\\mu_t^2\\Sigma_t^2+2ab\\mu_t\\Sigma_t^2 + \\frac{1}{2}b^2\\Sigma_t^2-2ab\\mu_t\\Sigma_t^2+2a^2\\mu_t^2\\Sigma_t^2\n= -a\\mu_t^2 + b\\mu_t + \\frac{1}{2}b^2\\Sigma_t^2.\n$$\nSo, the full expression for $u(t,x)$ becomes:\n$$\nu(t,x) = \\frac{1}{\\sqrt{1+2a\\Sigma_t^2}} \\exp\\left(c + \\frac{-a\\mu_t(x)^2 + b\\mu_t(x) + \\frac{1}{2}b^2\\Sigma_t^2}{1+2a\\Sigma_t^2}\\right).\n$$\nFinally, we substitute the expressions for $\\mu_t(x) = xe^{-\\theta t}$ and $\\Sigma_t^2 = \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$:\nThe denominator term $1+2a\\Sigma_t^2$ is $1+2a\\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t}) = 1+\\frac{a\\sigma^2}{\\theta}(1-e^{-2\\theta t})$.\nThe numerator of the fraction inside the exponential is $-a(xe^{-\\theta t})^2 + b(xe^{-\\theta t}) + \\frac{1}{2}b^2\\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t}) = -ax^2e^{-2\\theta t} + bxe^{-\\theta t} + \\frac{b^2\\sigma^2}{4\\theta}(1-e^{-2\\theta t})$.\nCombining these, we obtain the final closed-form solution:\n$$\nu(t,x) = \\frac{1}{\\sqrt{1+\\frac{a\\sigma^2}{\\theta}(1-e^{-2\\theta t})}} \\exp\\left(c + \\frac{-a x^{2} e^{-2\\theta t} + b x e^{-\\theta t} + \\frac{b^2\\sigma^2}{4\\theta}(1-e^{-2\\theta t})}{1+\\frac{a\\sigma^2}{\\theta}(1-e^{-2\\theta t})}\\right).\n$$", "answer": "$$\\boxed{\\frac{1}{\\sqrt{1+\\frac{a\\sigma^{2}}{\\theta}(1-\\exp(-2\\theta t))}} \\exp\\left(c + \\frac{-a x^{2} \\exp(-2\\theta t) + b x \\exp(-\\theta t) + \\frac{b^2\\sigma^2}{4\\theta}(1-\\exp(-2\\theta t))}{1+\\frac{a\\sigma^2}{\\theta}(1-\\exp(-2\\theta t))}\\right)}$$", "id": "3080589"}]}