{"hands_on_practices": [{"introduction": "A fundamental question for any numerical method is: how accurate is it? This practice tackles that question by measuring the *strong convergence* of the Euler-Maruyama method, which quantifies how well the numerical solution approximates the exact path of the SDE. We will apply the method to the Geometric Brownian Motion (GBM) SDE and compare the numerical results against the known exact solution, empirically verifying the method's convergence rate. To provide context, we will also implement the higher-order Milstein method to see how its accuracy improves [@problem_id:3226794].", "problem": "You are asked to compare the strong convergence behavior of two numerical schemes for stochastic differential equations (SDEs): the Euler-Maruyama method and the Milstein method. Work entirely in a scalar setting for a Geometric Brownian Motion, whose dynamics are given by the Itô SDE\n$$\ndX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t,\n$$\nwith initial condition $X_0 = x_0$, where $\\mu \\in \\mathbb{R}$ is the drift, $\\sigma \\in \\mathbb{R}$ is the diffusion parameter, and $W_t$ is a standard Wiener process. The exact solution is the well-known closed form\n$$\nX_T = X_0 \\exp\\big((\\mu - \\tfrac{1}{2}\\sigma^2) T + \\sigma W_T\\big).\n$$\nYour task is to implement a Monte Carlo study to estimate the strong convergence order at terminal time $T$ of:\n- the Euler-Maruyama method (Euler-Maruyama is the simplest Itô-integral-based discretization that replaces the Itô integral by the increment $\\,\\Delta W_n\\,$), and\n- the Milstein method (the first-order Itô–Taylor method for scalar SDEs that augments the Euler-Maruyama increment by the leading stochastic Taylor correction involving the diffusion derivative $\\,b'(x)\\,$ for $\\,b(x)=\\sigma x\\,$).\n\nBegin from the core definitions of the Itô integral, Itô’s formula, and the Itô–Taylor expansion, and use these to design the two time-stepping updates. Use the exact solution above to compute the terminal strong error. For each method and each time step, estimate the root mean square (RMS) strong error at time $T$:\n$$\n\\varepsilon_{\\mathrm{RMS}}(h) = \\Big(\\mathbb{E}\\big[|X_T^{(h)} - X_T|^2\\big]\\Big)^{1/2},\n$$\nwhere $X_T^{(h)}$ denotes the numerical approximation using uniform step size $h$.\n\nTo estimate the convergence order, consider a set of uniform step sizes $h_k = T/N_k$ and fit the model\n$$\n\\log \\varepsilon_{\\mathrm{RMS}}(h_k) \\approx p \\log h_k + C\n$$\nby least squares in $\\log$–$\\log$ scale to obtain the estimated strong order $p$.\n\nMonte Carlo requirements:\n- Use $M = 10000$ independent sample paths.\n- For variance reduction and fair comparison across step sizes, generate the Brownian increments on the finest grid and obtain coarser-grid increments by summing consecutive fine-grid increments (i.e., use a consistent refinement so that $N_{\\max}/N_k \\in \\mathbb{N}$ and the same underlying noise drives all resolutions).\n- For reproducibility, use the specified pseudorandom seeds per test case.\n\nUse the following test suite. For each test, simulate with the common set of time partitions $N_k \\in \\{16, 32, 64, 128\\}$ (so $h_k = T/N_k$), and the given number of paths and seed:\n- Test A (happy path, moderate noise): $x_0 = 1.0$, $\\mu = 1.0$, $\\sigma = 0.5$, $T = 1.0$, seed $= 314159$.\n- Test B (edge case with small noise): $x_0 = 2.0$, $\\mu = 0.3$, $\\sigma = 0.05$, $T = 2.0$, seed $= 271828$.\n- Test C (negative drift, stronger noise): $x_0 = 1.0$, $\\mu = -1.0$, $\\sigma = 1.0$, $T = 1.0$, seed $= 161803$.\n\nYour program must:\n- Implement both the Euler-Maruyama method and the Milstein method for the given SDE using their respective principled constructions from Itô calculus.\n- For each test case and each $N_k \\in \\{16, 32, 64, 128\\}$, compute the RMS strong error using $M = 10000$ simulated paths with coupled Brownian increments as described, and then fit the slope $p$ in the $\\log$–$\\log$ model separately for Euler-Maruyama and for Milstein.\n- Return, for each test case, the pair of estimated strong orders $[p_{\\mathrm{EM}}, p_{\\mathrm{Mil}}]$, each rounded to two decimal places.\n\nFinal output format:\n- Your program should print a single line containing a list of three entries, one per test case, in the order A, B, C. Each entry is the two-element list $[p_{\\mathrm{EM}}, p_{\\mathrm{Mil}}]$ for that test.\n- Concretely, your program should produce exactly one line like\n$$\n[[p_{\\mathrm{EM,A}}, p_{\\mathrm{Mil,A}}],[p_{\\mathrm{EM,B}}, p_{\\mathrm{Mil,B}}],[p_{\\mathrm{EM,C}}, p_{\\mathrm{Mil,C}}]]\n$$\nwith all six values as decimal floats rounded to two decimal places, and with no extra characters or whitespace beyond commas, brackets, and digits.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the theory of stochastic calculus and numerical methods for SDEs, well-posed with all necessary parameters defined, and objective in its formulation and requirements.\n\nThis problem requires a comparative study of the strong convergence orders for the Euler-Maruyama and Milstein numerical methods applied to the Geometric Brownian Motion (GBM) stochastic differential equation (SDE). The SDE is given by:\n$$\ndX_t = \\mu X_t \\, dt + \\sigma X_t \\, dW_t, \\quad X(0) = x_0\n$$\nwhere $\\mu \\in \\mathbb{R}$ is the drift parameter, $\\sigma \\in \\mathbb{R}$ is the diffusion parameter, and $W_t$ is a standard Wiener process. This is an Itô process with drift coefficient $a(x) = \\mu x$ and diffusion coefficient $b(x) = \\sigma x$. The problem provides the exact solution at time $T$:\n$$\nX_T = X_0 \\exp\\left((\\mu - \\tfrac{1}{2}\\sigma^2) T + \\sigma W_T\\right)\n$$\nwhere $W_T = \\int_0^T dW_s$ is the total increment of the Wiener process over $[0, T]$.\n\nThe solution involves deriving the numerical schemes, implementing them in a Monte Carlo simulation, computing the strong error, and finally estimating the convergence order via regression.\n\n**1. Derivation of Numerical Schemes**\n\nThe numerical schemes are derived by discretizing the integral form of the SDE over a time interval $[t_n, t_{n+1}]$ of length $h = t_{n+1} - t_n$. The integral form is:\n$$\nX_{t_{n+1}} = X_{t_n} + \\int_{t_n}^{t_{n+1}} a(X_s) \\, ds + \\int_{t_n}^{t_{n+1}} b(X_s) \\, dW_s\n$$\n\n**Euler-Maruyama (EM) Method:**\nThe EM method is the simplest discretization, obtained by approximating the integrands $a(X_s)$ and $b(X_s)$ as constants over the interval, equal to their values at the start of the interval, $X_n \\equiv X_{t_n}$.\n$$\nX_{n+1} \\approx X_n + a(X_n) \\int_{t_n}^{t_{n+1}} \\, ds + b(X_n) \\int_{t_n}^{t_{n+1}} \\, dW_s\n$$\nThe integrals evaluate to $\\int_{t_n}^{t_{n+1}} ds = h$ and $\\int_{t_n}^{t_{n+1}} dW_s = \\Delta W_n = W_{t_{n+1}} - W_{t_n}$. The term $\\Delta W_n$ represents a random draw from a normal distribution with mean $0$ and variance $h$, i.e., $\\Delta W_n \\sim \\mathcal{N}(0, h)$. The EM update rule is:\n$$\nX_{n+1} = X_n + a(X_n) h + b(X_n) \\Delta W_n\n$$\nFor the given GBM SDE with $a(x) = \\mu x$ and $b(x) = \\sigma x$, the EM scheme is:\n$$\nX_{n+1} = X_n + \\mu X_n h + \\sigma X_n \\Delta W_n = X_n (1 + \\mu h + \\sigma \\Delta W_n)\n$$\nThe EM method has a strong order of convergence of $0.5$ for general SDEs.\n\n**Milstein Method:**\nThe Milstein method improves upon the EM method by including an additional term from the Itô-Taylor expansion. For a scalar SDE, the expansion is:\n$$\nX_{t_{n+1}} = X_{t_n} + a(X_n) h + b(X_n) \\Delta W_n + \\frac{1}{2} b(X_n) b'(X_n) \\left((\\Delta W_n)^2 - h\\right) + \\mathcal{O}(h^{3/2})\n$$\nwhere $b'(x)$ is the derivative of the diffusion coefficient with respect to $x$. This scheme is obtained by including the next-order term in the stochastic integral approximation.\nThe update rule for the Milstein method is:\n$$\nX_{n+1} = X_n + a(X_n) h + b(X_n) \\Delta W_n + \\frac{1}{2} b(X_n) b'(X_n) \\left((\\Delta W_n)^2 - h\\right)\n$$\nFor the GBM SDE, we have $b(x) = \\sigma x$ and its derivative $b'(x) = \\sigma$. Substituting these into the Milstein scheme gives:\n$$\nX_{n+1} = X_n + \\mu X_n h + \\sigma X_n \\Delta W_n + \\frac{1}{2} (\\sigma X_n) (\\sigma) \\left((\\Delta W_n)^2 - h\\right)\n$$\n$$\nX_{n+1} = X_n \\left(1 + \\mu h + \\sigma \\Delta W_n + \\frac{1}{2} \\sigma^2 \\left((\\Delta W_n)^2 - h\\right)\\right)\n$$\nThe Milstein method typically has a strong order of convergence of $1.0$.\n\n**2. Monte Carlo Simulation Framework**\n\nThe goal is to estimate the strong convergence order $p$ from the relationship $\\varepsilon_{\\mathrm{RMS}}(h) \\propto h^p$. The root mean square (RMS) strong error is defined as:\n$$\n\\varepsilon_{\\mathrm{RMS}}(h) = \\left( \\mathbb{E}\\left[|X_T^{(h)} - X_T|^2\\right] \\right)^{1/2}\n$$\nwhere $X_T^{(h)}$ is the numerical solution at time $T$ with step size $h$, and $X_T$ is the exact solution. The expectation $\\mathbb{E}[\\cdot]$ is approximated by a sample mean over $M = 10000$ independent simulated paths:\n$$\n\\varepsilon_{\\mathrm{RMS}}(h) \\approx \\sqrt{ \\frac{1}{M} \\sum_{i=1}^{M} \\left|X_{T,i}^{(h)} - X_{T,i}\\right|^2 }\n$$\nTo ensure a fair comparison and reduce variance, the simulations for different step sizes $h_k = T/N_k$ for $N_k \\in \\{16, 32, 64, 128\\}$ are driven by the same underlying noise. This is achieved by first generating Brownian increments for the finest grid, $N_{\\max} = 128$, with step size $h_{\\text{fine}} = T/N_{\\max}$. For a coarser grid with $N_k  N_{\\max}$, the a coarse increment $\\Delta W_n^{(k)}$ is obtained by summing $R_k = N_{\\max}/N_k$ consecutive fine increments. This ensures that the total Wiener path $W_T$ is identical for all simulations.\n\n**3. Estimation of Convergence Order**\n\nThe convergence order $p$ is estimated by fitting a linear model to the log-transformed errors and step sizes:\n$$\n\\log \\varepsilon_{\\mathrm{RMS}}(h_k) = p \\log h_k + C\n$$\nThis is a simple linear regression problem for the data points $(\\log h_k, \\log \\varepsilon_{\\mathrm{RMS}}(h_k))$. The slope of the best-fit line, $p$, is the estimated order of convergence. This is computed separately for the Euler-Maruyama and Milstein methods for each test case.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Initialize parameters ($\\mu, \\sigma, x_0, T$) and simulation settings ($M, N_k, \\text{seed}$).\n2.  Generate $M$ paths of fine-grid Brownian increments $\\{\\Delta W_n^{\\text{fine}}\\}$ for $N_{\\max}=128$ steps.\n3.  Calculate the exact solution $X_{T,i}$ for each path $i$ using the total Wiener increment $W_{T,i} = \\sum_n \\Delta W_{n,i}^{\\text{fine}}$.\n4.  For each step count $N_k \\in \\{16, 32, 64, 128\\}$:\n    a. Determine the step size $h_k = T/N_k$.\n    b. Construct coarse-grid Brownian increments by summing fine-grid increments.\n    c. Simulate $M$ paths from $t=0$ to $t=T$ using both the EM and Milstein schemes.\n    d. Compute the RMS strong error $\\varepsilon_{\\mathrm{RMS}}(h_k)$ for both methods.\n5.  Using the set of calculated errors, perform a linear regression on $(\\log h_k, \\log \\varepsilon_k)$ to find the slope $p_{\\mathrm{EM}}$ for the Euler-Maruyama method and $p_{\\mathrm{Mil}}$ for the Milstein method.\n6.  Round the estimated orders to two decimal places and store the resulting pair.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo study to estimate the strong convergence orders of the\n    Euler-Maruyama and Milstein methods for the Geometric Brownian Motion SDE.\n    \"\"\"\n    test_cases = [\n        # (x0, mu, sigma, T, seed)\n        (1.0, 1.0, 0.5, 1.0, 314159),  # Test A\n        (2.0, 0.3, 0.05, 2.0, 271828), # Test B\n        (1.0, -1.0, 1.0, 1.0, 161803), # Test C\n    ]\n\n    all_results = []\n\n    M = 10000  # Number of Monte Carlo paths\n    N_steps_list = [16, 32, 64, 128]\n    N_max = max(N_steps_list)\n\n    for x0, mu, sigma, T, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # Step 1: Generate fine-grained Brownian increments for all paths\n        h_fine = T / N_max\n        # (M, N_max) matrix of increments\n        fine_increments = rng.normal(0, np.sqrt(h_fine), (M, N_max))\n        \n        # Step 2: Calculate the exact solution at time T\n        # Total Wiener increment for each path\n        W_T_paths = np.sum(fine_increments, axis=1)\n        X_exact_T = x0 * np.exp((mu - 0.5 * sigma**2) * T + sigma * W_T_paths)\n        \n        log_h_values = []\n        log_em_errors = []\n        log_mil_errors = []\n\n        # Step 3: Loop over different step sizes\n        for N_k in N_steps_list:\n            h_k = T / N_k\n            log_h_values.append(np.log(h_k))\n            \n            # Step 3a: Construct coarse Brownian increments from fine ones\n            refinement_ratio = N_max // N_k\n            coarse_increments = fine_increments.reshape(M, N_k, refinement_ratio).sum(axis=2)\n            \n            # Initialize numerical solutions at t=0\n            X_em = np.full(M, x0)\n            X_mil = np.full(M, x0)\n            \n            # Step 3b: Simulate paths for both schemes\n            for n in range(N_k):\n                dW = coarse_increments[:, n]\n                \n                # Euler-Maruyama update\n                X_em = X_em * (1 + mu * h_k + sigma * dW)\n                \n                # Milstein update\n                mil_correction = 0.5 * sigma**2 * (dW**2 - h_k)\n                X_mil = X_mil * (1 + mu * h_k + sigma * dW + mil_correction)\n            \n            # Step 3c: Calculate RMS strong error for this step size\n            em_error_sq = np.mean((X_em - X_exact_T)**2)\n            mil_error_sq = np.mean((X_mil - X_exact_T)**2)\n            \n            log_em_errors.append(0.5 * np.log(em_error_sq))\n            log_mil_errors.append(0.5 * np.log(mil_error_sq))\n\n        # Step 4: Perform log-log regression to find the convergence order\n        # np.polyfit returns [slope, intercept]\n        p_em = np.polyfit(log_h_values, log_em_errors, 1)[0]\n        p_mil = np.polyfit(log_h_values, log_mil_errors, 1)[0]\n        \n        # Round results to two decimal places\n        p_em_rounded = round(p_em, 2)\n        p_mil_rounded = round(p_mil, 2)\n        \n        all_results.append([p_em_rounded, p_mil_rounded])\n\n    # Step 5: Format the final output string\n    inner_strings = [f\"[{p_em},{p_mil}]\" for p_em, p_mil in all_results]\n    print(f\"[{','.join(inner_strings)}]\")\n\nsolve()\n```", "id": "3226794"}, {"introduction": "Beyond quantitative error, a good numerical method should ideally preserve the essential qualitative features of the system it models. The Geometric Brownian Motion SDE, often used to model phenomena like stock prices, has a strictly positive solution. In this practice, we investigate whether the Euler-Maruyama approximation respects this positivity constraint. You will derive an analytical expression for the probability that a single EM step yields a non-positive value, revealing a crucial limitation of the method [@problem_id:3080335].", "problem": "Consider the geometric Brownian motion (GBM) stochastic differential equation (SDE)\n$$\ndX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t},\\quad X_{0}=x_{0}0,\n$$\nwhere $W_{t}$ is a standard Wiener process and $\\mu,\\sigma$ are real constants with $\\sigma0$. Let a uniform time grid $t_{n}=n h$ with step size $h0$ be given.\n\nUsing only the integral form of the SDE over $[t_{n},t_{n+1}]$ and the definition of the Euler-Maruyama (EM) method as a left-point Riemann approximation to the Itô integrals, derive the EM update for $X_{n+1}$ in terms of $X_{n}$. Then, conditioning on $X_{n}0$, use the basic properties of Gaussian increments of the Wiener process to obtain a closed-form analytic expression for the probability that the EM one-step approximation $X_{n+1}$ is nonpositive.\n\nFinally, explain qualitatively how this probability depends on $h$, $\\mu$, and $\\sigma$, and under what conditions the EM approximation is likely to violate positivity, even though the exact GBM solution is strictly positive for all $t0$ given $x_{0}0$.\n\nProvide your final answer as the closed-form analytic expression for the one-step probability $\\mathbb{P}(X_{n+1}\\le 0\\,|\\,X_{n}0)$ as a function of $\\mu$, $\\sigma$, and $h$. No numerical approximation or rounding is required, and no units are to be reported.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary information is provided, and the tasks are clearly defined within the standard framework of stochastic differential equations and numerical methods. The problem is valid.\n\nThe stochastic differential equation (SDE) for geometric Brownian motion (GBM) is given by\n$$\ndX_{t}=\\mu X_{t}\\,dt+\\sigma X_{t}\\,dW_{t},\n$$\nwith initial condition $X_{0}=x_{0}0$. Here, $W_{t}$ is a standard Wiener process, and $\\mu, \\sigma$ are real constants with $\\sigma0$.\n\nThe first step is to derive the Euler–Maruyama (EM) numerical scheme for this SDE. The integral form of the SDE over a time interval $[t_{n}, t_{n+1}]$ is\n$$\nX_{t_{n+1}} - X_{t_{n}} = \\int_{t_{n}}^{t_{n+1}} \\mu X_{s}\\,ds + \\int_{t_{n}}^{t_{n+1}} \\sigma X_{s}\\,dW_{s}.\n$$\nLet $h = t_{n+1} - t_{n}$ be the uniform time step. Let $X_{n}$ denote the numerical approximation to $X_{t_{n}}$. The Euler-Maruyama method is derived by approximating the integrands in the above equation by their values at the left endpoint of the interval, $t_{n}$. This is a left-point Riemann approximation.\n\nFor the drift term (a standard Riemann integral):\n$$\n\\int_{t_{n}}^{t_{n+1}} \\mu X_{s}\\,ds \\approx \\mu X_{t_{n}} \\int_{t_{n}}^{t_{n+1}} ds = \\mu X_{n} (t_{n+1} - t_{n}) = \\mu X_{n} h.\n$$\nFor the diffusion term (an Itô integral):\n$$\n\\int_{t_{n}}^{t_{n+1}} \\sigma X_{s}\\,dW_{s} \\approx \\sigma X_{t_{n}} \\int_{t_{n}}^{t_{n+1}} dW_{s} = \\sigma X_{n} (W_{t_{n+1}} - W_{t_{n}}).\n$$\nSubstituting these approximations into the integral equation yields the EM update rule:\n$$\nX_{n+1} = X_{n} + \\mu X_{n} h + \\sigma X_{n} (W_{t_{n+1}} - W_{t_{n}}).\n$$\nThe increment of the Wiener process, $\\Delta W_{n} = W_{t_{n+1}} - W_{t_{n}}$, is a normally distributed random variable with mean $0$ and variance $t_{n+1} - t_{n} = h$. We can write $\\Delta W_{n} = \\sqrt{h} Z_{n}$, where $Z_{n}$ is a standard normal random variable, $Z_{n} \\sim \\mathcal{N}(0, 1)$.\n\nThe EM update rule can therefore be written as:\n$$\nX_{n+1} = X_{n} (1 + \\mu h + \\sigma \\sqrt{h} Z_{n}).\n$$\nThis is the derived EM update for $X_{n+1}$ in terms of $X_{n}$.\n\nNext, we calculate the probability that the one-step approximation $X_{n+1}$ is nonpositive, conditioned on $X_{n}0$. We are interested in $\\mathbb{P}(X_{n+1} \\le 0 \\,|\\, X_{n}  0)$.\nUsing the EM update formula:\n$$\n\\mathbb{P}(X_{n} (1 + \\mu h + \\sigma \\sqrt{h} Z_{n}) \\le 0 \\,|\\, X_{n}  0).\n$$\nSince the conditioning is on $X_{n}  0$, we can divide the inequality by $X_{n}$ without changing its direction:\n$$\n\\mathbb{P}(1 + \\mu h + \\sigma \\sqrt{h} Z_{n} \\le 0).\n$$\nWe now isolate the standard normal random variable $Z_{n}$:\n$$\n\\sigma \\sqrt{h} Z_{n} \\le -(1 + \\mu h).\n$$\nSince $\\sigma0$ and $h0$, we have $\\sigma\\sqrt{h}0$. We can divide by this term:\n$$\nZ_{n} \\le -\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}.\n$$\nThe probability of this event is given by the cumulative distribution function (CDF) of the standard normal distribution, denoted by $\\Phi(z) = \\mathbb{P}(Z \\le z)$ for $Z \\sim \\mathcal{N}(0,1)$.\nTherefore, the probability is:\n$$\n\\mathbb{P}(X_{n+1} \\le 0 \\,|\\, X_{n}  0) = \\Phi\\left(-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}\\right).\n$$\nThis is the closed-form analytic expression for the probability.\n\nFinally, we provide a qualitative explanation of the result. The probability of violating positivity, $P = \\Phi\\left(-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}\\right)$, depends on the argument of the CDF. Since $\\Phi$ is a monotonically increasing function, the probability $P$ increases as its argument, $z = -\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}$, increases (becomes less negative).\n\n- **Dependence on step size $h$**: As the step size $h$ increases from $0$, the term $\\sqrt{h}$ in the denominator increases, and the term $(1+\\mu h)$ in the numerator also increases. The dominant term for small $h$ is $-1/(\\sigma\\sqrt{h})$. As $h \\to 0^{+}$, this term goes to $-\\infty$, and thus the probability goes to $0$. As $h$ increases, the argument $z$ generally increases, leading to a higher probability of negativity. A larger time step allows for a larger random jump variance $(\\sigma X_n \\sqrt{h})^2$, making it more likely that a single step can overcome the current positive value.\n\n- **Dependence on volatility $\\sigma$**: As the volatility $\\sigma$ increases, the magnitude of the denominator $\\sigma\\sqrt{h}$ increases. This makes the argument $z$, which is negative (assuming $1+\\mu h  0$), less negative (i.e., it increases). A larger $z$ results in a larger value of $\\Phi(z)$. Therefore, a higher volatility $\\sigma$ increases the probability of the EM approximation becoming nonpositive. This is because higher volatility implies larger potential random fluctuations.\n\n- **Dependence on drift $\\mu$**: As the drift $\\mu$ increases, the numerator $1+\\mu h$ increases. This makes the argument $z$ more negative (i.e., it decreases). A smaller $z$ results in a smaller value of $\\Phi(z)$. Thus, a higher positive drift $\\mu$ decreases the probability of non-positivity, as it systematically pushes the process to higher values, providing a larger buffer against negative random shocks.\n\nThe exact solution of the GBM SDE is $X_t = X_0 \\exp\\left((\\mu - \\frac{1}{2}\\sigma^2)t + \\sigma W_t\\right)$, which is strictly positive for $X_0  0$. The EM method, being a discrete approximation, can fail to preserve this positivity. The violation of positivity becomes likely when the argument $-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}$ is not a large negative number. This occurs when the \"safety margin\" represented by the number of standard deviations to reach zero, $\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}$, is small. This condition is promoted by a large step size $h$, a large volatility $\\sigma$, or a small (or negative) drift $\\mu$. In essence, the EM approximation can become nonpositive when the random component of a single step, $\\sigma X_{n}\\sqrt{h}Z_{n}$, is sufficiently large and negative to overwhelm the deterministic part, $X_{n}(1+\\mu h)$.", "answer": "$$\n\\boxed{\\Phi\\left(-\\frac{1 + \\mu h}{\\sigma \\sqrt{h}}\\right)}\n$$", "id": "3080335"}, {"introduction": "Many systems in science and finance involve processes that operate on vastly different time scales, creating what is known as a \"stiff\" system. This final practice explores the performance of the Euler-Maruyama method on such a system. We will discover that the presence of a \"fast\" component imposes a severe restriction on the time step size $h$ required for the simulation to remain stable, a critical concept known as mean-square stability [@problem_id:3226679].", "problem": "Consider a two-dimensional linear stochastic differential equation (SDE) that exhibits two distinct time scales, defined by independent Ornstein–Uhlenbeck components. Let the state be $(X_t, Y_t)$ with initial condition $(X_0, Y_0)$ and dynamics\n$$\ndX_t = -\\alpha X_t\\,dt + \\sigma_1\\,dW_t^{(1)}, \\quad dY_t = -\\beta Y_t\\,dt + \\sigma_2\\,dW_t^{(2)},\n$$\nwhere $W_t^{(1)}$ and $W_t^{(2)}$ are independent one-dimensional standard Wiener processes, $\\alpha  0$ is the slow decay rate, and $\\beta \\gg \\alpha$ is the fast decay rate. This system is stiff because the fast component imposes a severe stability restriction on explicit time-stepping methods.\n\nStarting from the core definition of the Euler-Maruyama method for SDEs, derive the discrete-time dynamics for each component on a uniform time grid with step size $h$ and $N = \\lfloor T / h \\rfloor$ steps up to time $t_N = Nh$. Using fundamental properties of linear SDEs and Itō calculus, derive closed-form expressions for the exact mean and variance of $X_{t_N}$ and $Y_{t_N}$, and for the mean and variance produced by the explicit Euler-Maruyama discretization at step $N$. Do not assume or use any pre-derived stability or error formulas; instead, obtain them from first principles (linearity, Itō isometry, and the definition of the Euler-Maruyama method).\n\nDefine the relative error for any positive quantity $q$ and its approximation $\\hat{q}$ as\n$$\n\\mathrm{RelErr}(q,\\hat{q}) = \\frac{|q - \\hat{q}|}{\\max(|q|, \\varepsilon)},\n$$\nwhere $\\varepsilon = 10^{-12}$ is a fixed small positive number to avoid division by zero. For each parameter set, compute four relative errors: mean and variance of $X_{t_N}$ and mean and variance of $Y_{t_N}$. Report a single scalar for the parameter set equal to the maximum of these four relative errors.\n\nYour program must implement the above using the Euler-Maruyama method and closed-form exact statistics, and produce the maximum relative error for each of the following test cases. There are no physical units involved.\n\nTest Suite:\n- Case A (happy path, small step size): $\\alpha = 1$, $\\beta = 100$, $\\sigma_1 = 0.5$, $\\sigma_2 = 0.5$, $X_0 = 1$, $Y_0 = 1$, $T = 1$, $h = 10^{-3}$.\n- Case B (boundary condition, near instability threshold for fast scale): same parameters as Case A, but $h = 0.02$.\n- Case C (edge case, beyond explicit Euler stability threshold for fast scale): same parameters as Case A, but $h = 0.03$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$r_1$,$r_2$,$r_3$]\"), where $r_i$ is the maximum relative error for Case $i$. Each $r_i$ must be a floating-point number.", "solution": "The user-provided problem statement has been validated and is determined to be sound. It is scientifically grounded, well-posed, and objective. It presents a formal problem in the numerical analysis of stochastic differential equations without any apparent flaws. We may therefore proceed with a full solution.\n\nThe problem asks for an analysis of the Euler-Maruyama method applied to a two-dimensional linear stochastic differential equation (SDE) with stiff dynamics. The state vector is $(X_t, Y_t)$, and the components evolve independently according to Ornstein-Uhlenbeck processes:\n$$\ndX_t = -\\alpha X_t\\,dt + \\sigma_1\\,dW_t^{(1)}, \\quad X(0) = X_0\n$$\n$$\ndY_t = -\\beta Y_t\\,dt + \\sigma_2\\,dW_t^{(2)}, \\quad Y(0) = Y_0\n$$\nSince the dynamics of $X_t$ and $Y_t$ are of the same form and are uncoupled, we can perform a general derivation for a single Ornstein-Uhlenbeck process and then apply the results to each component. Let us consider the generic SDE:\n$$\ndZ_t = -k Z_t\\,dt + \\sigma dW_t, \\quad Z(0) = Z_0\n$$\nwhere $k  0$ and $\\sigma$ are constants, and $W_t$ is a standard one-dimensional Wiener process. The initial condition $Z_0$ is treated as a deterministic constant.\n\n### Derivation of Exact Mean and Variance\n\nWe solve this linear SDE using an integrating factor, $f(t) = e^{kt}$. By Itō's product rule, the differential of $e^{kt}Z_t$ is:\n$$\nd(e^{kt}Z_t) = (de^{kt})Z_t + e^{kt}(dZ_t) + d(e^{kt})d(Z_t)\n$$\nThe differentials are $de^{kt} = k e^{kt} dt$ and $dZ_t = -kZ_t dt + \\sigma dW_t$. The quadratic covariation term $d(e^{kt})d(Z_t)$ is of order $dt \\cdot dW_t \\sim dt^{3/2}$ and $dt \\cdot dt \\sim dt^2$, which are both zero in the calculus.\nSubstituting the differentials:\n$$\nd(e^{kt}Z_t) = (k e^{kt} dt)Z_t + e^{kt}(-kZ_t dt + \\sigma dW_t)\n$$\n$$\nd(e^{kt}Z_t) = k e^{kt} Z_t dt - k e^{kt} Z_t dt + \\sigma e^{kt} dW_t = \\sigma e^{kt} dW_t\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\n\\int_0^t d(e^{ks}Z_s) = \\int_0^t \\sigma e^{ks} dW_s\n$$\n$$\ne^{kt}Z_t - e^{k \\cdot 0}Z_0 = \\sigma \\int_0^t e^{ks} dW_s\n$$\n$$\nZ_t = Z_0 e^{-kt} + \\sigma e^{-kt} \\int_0^t e^{ks} dW_s = Z_0 e^{-kt} + \\sigma \\int_0^t e^{-k(t-s)} dW_s\n$$\nThis is the exact solution for $Z_t$.\n\nThe **exact mean** $\\mathbb{E}[Z_t]$ is found by taking the expectation of the solution. Using the linearity of expectation and the fact that the expectation of an Itō integral is zero:\n$$\n\\mathbb{E}[Z_t] = \\mathbb{E}[Z_0 e^{-kt}] + \\mathbb{E}\\left[\\sigma \\int_0^t e^{-k(t-s)} dW_s\\right] = Z_0 e^{-kt} + 0\n$$\n$$\n\\mathbb{E}[Z_t] = Z_0 e^{-kt}\n$$\nThe **exact variance** $\\mathrm{Var}(Z_t)$ is the expectation of the squared deviation from the mean:\n$$\n\\mathrm{Var}(Z_t) = \\mathbb{E}[(Z_t - \\mathbb{E}[Z_t])^2] = \\mathbb{E}\\left[\\left(\\sigma \\int_0^t e^{-k(t-s)} dW_s\\right)^2\\right]\n$$\nUsing the Itō isometry property, which states $\\mathbb{E}[(\\int_0^t f(s)dW_s)^2] = \\int_0^t \\mathbb{E}[f(s)^2]ds$:\n$$\n\\mathrm{Var}(Z_t) = \\sigma^2 \\int_0^t (e^{-k(t-s)})^2 ds = \\sigma^2 \\int_0^t e^{-2k(t-s)} ds\n$$\nThe integral evaluates to:\n$$\n\\int_0^t e^{-2k(t-s)} ds = \\left[ \\frac{e^{-2k(t-s)}}{2k} \\right]_0^t = \\frac{e^0 - e^{-2kt}}{2k} = \\frac{1 - e^{-2kt}}{2k}\n$$\nTherefore, the exact variance is:\n$$\n\\mathrm{Var}(Z_t) = \\frac{\\sigma^2}{2k}(1 - e^{-2kt})\n$$\n\n### Derivation of Euler-Maruyama Mean and Variance\n\nThe Euler-Maruyama method discretizes the SDE on a uniform time grid $t_n = nh$ for $n=0, 1, \\dots, N$. The update rule is:\n$$\n\\hat{Z}_{n+1} = \\hat{Z}_n - k \\hat{Z}_n h + \\sigma (W_{t_{n+1}} - W_{t_n})\n$$\nwhere $\\hat{Z}_n$ is the numerical approximation of $Z_{t_n}$. The Wiener increment $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is a normally distributed random variable with mean $0$ and variance $h$. We can write $\\Delta W_n = \\sqrt{h} \\xi_n$, where $\\xi_n \\sim \\mathcal{N}(0,1)$ are independent and identically distributed standard normal random variables. The recurrence relation for the approximation is:\n$$\n\\hat{Z}_{n+1} = (1 - kh) \\hat{Z}_n + \\sigma \\sqrt{h} \\xi_n\n$$\nStarting from $\\hat{Z}_0 = Z_0$, we can unroll the recurrence to find an explicit expression for $\\hat{Z}_N$:\n$$\n\\hat{Z}_N = (1-kh)^N Z_0 + \\sigma \\sqrt{h} \\sum_{n=0}^{N-1} (1-kh)^{N-1-n} \\xi_n\n$$\n\nThe **mean of the numerical approximation** $\\mathbb{E}[\\hat{Z}_N]$ is found by taking the expectation. Since $\\mathbb{E}[\\xi_n] = 0$:\n$$\n\\mathbb{E}[\\hat{Z}_N] = \\mathbb{E}[(1-kh)^N Z_0] + \\sigma \\sqrt{h} \\sum_{n=0}^{N-1} (1-kh)^{N-1-n} \\mathbb{E}[\\xi_n] = (1-kh)^N Z_0\n$$\n$$\n\\mathbb{E}[\\hat{Z}_N] = Z_0 (1-kh)^N\n$$\nThe **variance of the numerical approximation** $\\mathrm{Var}(\\hat{Z}_N)$ is:\n$$\n\\mathrm{Var}(\\hat{Z}_N) = \\mathbb{E}[(\\hat{Z}_N - \\mathbb{E}[\\hat{Z}_N])^2] = \\mathbb{E}\\left[\\left(\\sigma \\sqrt{h} \\sum_{n=0}^{N-1} (1-kh)^{N-1-n} \\xi_n\\right)^2\\right]\n$$\n$$\n\\mathrm{Var}(\\hat{Z}_N) = \\sigma^2 h \\mathbb{E}\\left[\\left(\\sum_{n=0}^{N-1} (1-kh)^{N-1-n} \\xi_n\\right)^2\\right]\n$$\nSince the $\\xi_n$ are independent with unit variance, the expectation of the squared sum simplifies to the sum of squares of the coefficients:\n$$\n\\mathrm{Var}(\\hat{Z}_N) = \\sigma^2 h \\sum_{n=0}^{N-1} \\left((1-kh)^{N-1-n}\\right)^2 = \\sigma^2 h \\sum_{j=0}^{N-1} ((1-kh)^2)^j\n$$\nThis is a geometric series with ratio $r = (1-kh)^2$.\nIf $r \\neq 1$, the sum is $\\frac{1-r^N}{1-r}$. The variance is:\n$$\n\\mathrm{Var}(\\hat{Z}_N) = \\sigma^2 h \\frac{1 - ((1-kh)^2)^N}{1 - (1-kh)^2} = \\sigma^2 h \\frac{1 - (1-kh)^{2N}}{1 - (1 - 2kh + k^2h^2)} = \\sigma^2 h \\frac{1 - (1-kh)^{2N}}{2kh - k^2h^2}\n$$\n$$\n\\mathrm{Var}(\\hat{Z}_N) = \\frac{\\sigma^2}{k(2-kh)} (1 - (1-kh)^{2N})\n$$\nThe mean-square stability of the method requires $|1-kh|1$, which gives $0  kh  2$. The fast component $Y_t$ has $k=\\beta$, so its stability requires $h  2/\\beta$.\nA special case occurs on the boundary of stability, when $kh=2$. Here, the ratio $r=(1-2)^2=1$. The geometric series sum becomes $\\sum_{j=0}^{N-1} 1^j = N$. The variance is:\n$$\n\\mathrm{Var}(\\hat{Z}_N) = \\sigma^2 h N = \\sigma^2 t_N \\quad (\\text{for } kh=2)\n$$\n\n### Computational Procedure\n\nFor each test case, we compute the number of steps $N = \\lfloor T / h \\rfloor$ and the final time $t_N = Nh$. We then compute four sets of statistics: exact and numerical for both the $X$ and $Y$ components.\n\n- **For component X**: Use $k = \\alpha$, $\\sigma = \\sigma_1$, $Z_0 = X_0$.\n- **For component Y**: Use $k = \\beta$, $\\sigma = \\sigma_2$, $Z_0 = Y_0$.\n\nThe following quantities are calculated for a generic component $Z$ at time $t_N$:\n- Exact Mean: $\\mathbb{E}[Z_{t_N}] = Z_0 e^{-kt_N}$\n- Exact Variance: $\\mathrm{Var}(Z_{t_N}) = \\frac{\\sigma^2}{2k}(1 - e^{-2kt_N})$\n- Numerical Mean: $\\mathbb{E}[\\hat{Z}_N] = Z_0 (1-kh)^N$\n- Numerical Variance: $\\mathrm{Var}(\\hat{Z}_N)$, using the appropriate formula depending on whether $kh=2$.\n\nThe relative error between an exact quantity $q$ and its approximation $\\hat{q}$ is calculated as:\n$$\n\\mathrm{RelErr}(q,\\hat{q}) = \\frac{|q - \\hat{q}|}{\\max(|q|, \\varepsilon)}, \\quad \\varepsilon = 10^{-12}\n$$\nFor each test case, we compute the four relative errors for the means and variances of the $X$ and $Y$ components. The final result for the case is the maximum of these four errors.\n- $\\mathrm{RelErr}(\\mathbb{E}[X_{t_N}], \\mathbb{E}[\\hat{X}_N])$\n- $\\mathrm{RelErr}(\\mathrm{Var}(X_{t_N}), \\mathrm{Var}(\\hat{X}_N))$\n- $\\mathrm{RelErr}(\\mathbb{E}[Y_{t_N}], \\mathbb{E}[\\hat{Y}_N])$\n- $\\mathrm{RelErr}(\\mathrm{Var}(Y_{t_N}), \\mathrm{Var}(\\hat{Y}_N))$\n\nThe test cases explore different regimes of stability for the fast component $Y_t$, for which the stability threshold is $h  2/\\beta = 2/100 = 0.02$.\n- Case A ($h=0.001$): Stable regime.\n- Case B ($h=0.02$): Boundary of stability.\n- Case C ($h=0.03$): Unstable regime.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the maximum relative error between exact and Euler-Maruyama statistics\n    for a stiff two-component Ornstein-Uhlenbeck process.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path, small step size)\n        {\"alpha\": 1.0, \"beta\": 100.0, \"sigma1\": 0.5, \"sigma2\": 0.5, \"X0\": 1.0, \"Y0\": 1.0, \"T\": 1.0, \"h\": 1e-3},\n        # Case B (boundary condition, near instability threshold for fast scale)\n        {\"alpha\": 1.0, \"beta\": 100.0, \"sigma1\": 0.5, \"sigma2\": 0.5, \"X0\": 1.0, \"Y0\": 1.0, \"T\": 1.0, \"h\": 0.02},\n        # Case C (edge case, beyond explicit Euler stability threshold for fast scale)\n        {\"alpha\": 1.0, \"beta\": 100.0, \"sigma1\": 0.5, \"sigma2\": 0.5, \"X0\": 1.0, \"Y0\": 1.0, \"T\": 1.0, \"h\": 0.03},\n    ]\n\n    results = []\n    \n    # Epsilon for relative error calculation to avoid division by zero\n    epsilon = 1e-12\n\n    def calculate_statistics(k, sigma, z0, h, N, t_N):\n        \"\"\"\n        Calculates exact and numerical mean and variance for a generic\n        Ornstein-Uhlenbeck process dZ = -k*Z*dt + sigma*dW.\n        \"\"\"\n        # Exact statistics\n        exact_mean = z0 * np.exp(-k * t_N)\n        exact_var = (sigma**2 / (2.0 * k)) * (1.0 - np.exp(-2.0 * k * t_N))\n\n        # Euler-Maruyama statistics\n        numerical_mean = z0 * np.power(1.0 - k * h, N)\n\n        kh = k * h\n        # Check for the stability boundary condition kh = 2\n        if np.isclose(kh, 2.0):\n            numerical_var = sigma**2 * t_N\n        else:\n            term = 1.0 - np.power(1.0 - kh, 2 * N)\n            denominator = k * (2.0 - kh)\n            numerical_var = (sigma**2 / denominator) * term\n        \n        return exact_mean, exact_var, numerical_mean, numerical_var\n\n    def relative_error(q, q_hat):\n        \"\"\"Calculates the relative error.\"\"\"\n        return np.abs(q - q_hat) / np.maximum(np.abs(q), epsilon)\n\n    for case in test_cases:\n        alpha, beta, sigma1, sigma2, X0, Y0, T, h = (\n            case[\"alpha\"], case[\"beta\"], case[\"sigma1\"], case[\"sigma2\"],\n            case[\"X0\"], case[\"Y0\"], case[\"T\"], case[\"h\"]\n        )\n\n        N = int(np.floor(T / h))\n        t_N = N * h\n        \n        # Calculate statistics for the X component (slow)\n        ex_mean_X, ex_var_X, num_mean_X, num_var_X = calculate_statistics(\n            alpha, sigma1, X0, h, N, t_N\n        )\n\n        # Calculate statistics for the Y component (fast)\n        ex_mean_Y, ex_var_Y, num_mean_Y, num_var_Y = calculate_statistics(\n            beta, sigma2, Y0, h, N, t_N\n        )\n        \n        # Compute the four relative errors\n        err_mean_X = relative_error(ex_mean_X, num_mean_X)\n        err_var_X = relative_error(ex_var_X, num_var_X)\n        err_mean_Y = relative_error(ex_mean_Y, num_mean_Y)\n        err_var_Y = relative_error(ex_var_Y, num_var_Y)\n        \n        # Find the maximum of the four errors\n        max_rel_error = max(err_mean_X, err_var_X, err_mean_Y, err_var_Y)\n        results.append(max_rel_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3226679"}]}