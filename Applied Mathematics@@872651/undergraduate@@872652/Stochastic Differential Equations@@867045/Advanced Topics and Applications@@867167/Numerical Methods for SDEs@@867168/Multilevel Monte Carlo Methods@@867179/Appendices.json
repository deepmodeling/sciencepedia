{"hands_on_practices": [{"introduction": "The efficiency of the Multilevel Monte Carlo (MLMC) method hinges on a single, powerful idea: variance reduction through coupling. Rather than simulating each resolution level independently, we intelligently link them to ensure that the correction terms between levels have a much smaller variance. This practice provides a crystal-clear illustration of this principle by asking you to compare the variance of a level-difference estimator for a simple SDE under two scenarios: one with proper coupling and one without [@problem_id:3068034]. By working through the calculation, you will see exactly where the \"magic\" of MLMC comes from and why it dramatically outperforms standard Monte Carlo methods.", "problem": "Consider the scalar stochastic differential equation (SDE) $dX_t = \\sigma\\, dW_t$ with $X_0 = 0$ on the time interval $[0,T]$, where $W_t$ is a standard Wiener process and $\\sigma > 0$ is a constant. Let $P_\\ell$ denote the Euler–Maruyama terminal approximation of $X_T$ computed on level $\\ell$ with $N_\\ell$ steps of size $h_\\ell = T/N_\\ell$, and let $P_{\\ell-1}$ denote the corresponding approximation computed on level $\\ell-1$ with $N_{\\ell-1} = N_\\ell/2$ steps of size $h_{\\ell-1} = 2 h_\\ell$. In both cases, the Euler–Maruyama scheme is defined by $X_{n+1} = X_n + \\sigma\\, \\Delta W_n$, where each Brownian increment $\\Delta W_n$ is distributed as $\\mathcal{N}(0,h)$ with $h$ equal to the relevant step size. Define $P_\\ell = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n$ and $P_{\\ell-1} = \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m$. In the Multilevel Monte Carlo (MLMC) method, coupling across levels is used to reduce the variance of the level differences.\n\nTwo coupling strategies are considered:\n- Correct coupling: for each $m \\in \\{1,\\dots,N_{\\ell-1}\\}$, set $\\Delta W^{(\\ell-1)}_m = \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m}$, where the fine-level increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$ are independent and identically distributed with law $\\mathcal{N}(0,h_\\ell)$.\n- Incorrect coupling: generate $\\{\\Delta W^{(\\ell-1)}_m\\}_{m=1}^{N_{\\ell-1}}$ as independent and identically distributed $\\mathcal{N}(0,h_{\\ell-1})$ increments that are independent of the fine-level increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$.\n\nStarting only from the properties of independent Gaussian increments of Brownian motion and the variance additivity for independent random variables, derive $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ under each coupling strategy. Then, express the incorrect-coupling variance $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ as a simplified closed-form analytic expression in terms of $\\sigma$ and $T$. Your final answer must be this expression. No rounding is required.", "solution": "The problem requires the derivation of the variance of the difference between a fine and a coarse Euler-Maruyama approximation, $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$, under two different coupling strategies for the stochastic differential equation (SDE) $dX_t = \\sigma\\, dW_t$ with $X_0 = 0$. The final answer is the specific expression for this variance under the incorrect coupling strategy.\n\nLet $P_\\ell$ and $P_{\\ell-1}$ be the random variables representing the numerical approximations at the terminal time $T$ on level $\\ell$ (fine) and level $\\ell-1$ (coarse), respectively. The general formula for the variance of the difference of two random variables is:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(P_\\ell) + \\mathrm{Var}(P_{\\ell-1}) - 2 \\mathrm{Cov}(P_\\ell, P_{\\ell-1})\n$$\nWe analyze each coupling strategy by first calculating the individual variances, $\\mathrm{Var}(P_\\ell)$ and $\\mathrm{Var}(P_{\\ell-1})$, and then evaluating the covariance term.\n\nThe fine-level approximation is given by $P_\\ell = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n$, where $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$ are independent and identically distributed (i.i.d.) random variables with distribution $\\mathcal{N}(0,h_\\ell)$. The variance of $P_\\ell$ is calculated using the property of variance additivity for independent random variables and the scaling property $\\mathrm{Var}(aZ) = a^2 \\mathrm{Var}(Z)$:\n$$\n\\mathrm{Var}(P_\\ell) = \\mathrm{Var}\\left(\\sigma \\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n\\right) = \\sigma^2 \\mathrm{Var}\\left(\\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n\\right)\n$$\nSince the increments $\\Delta W^{(\\ell)}_n$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(P_\\ell) = \\sigma^2 \\sum_{n=1}^{N_\\ell} \\mathrm{Var}(\\Delta W^{(\\ell)}_n) = \\sigma^2 \\sum_{n=1}^{N_\\ell} h_\\ell = \\sigma^2 N_\\ell h_\\ell\n$$\nGiven that the step size is $h_\\ell = T/N_\\ell$, we have $N_\\ell h_\\ell = T$. Therefore, the variance of the fine-level approximation is:\n$$\n\\mathrm{Var}(P_\\ell) = \\sigma^2 T\n$$\nSimilarly, the coarse-level approximation is given by $P_{\\ell-1} = \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m$, where the increments $\\Delta W^{(\\ell-1)}_m$ are i.i.d. $\\mathcal{N}(0,h_{\\ell-1})$. Following the same logic:\n$$\n\\mathrm{Var}(P_{\\ell-1}) = \\sigma^2 \\sum_{m=1}^{N_{\\ell-1}} \\mathrm{Var}(\\Delta W^{(\\ell-1)}_m) = \\sigma^2 \\sum_{m=1}^{N_{\\ell-1}} h_{\\ell-1} = \\sigma^2 N_{\\ell-1} h_{\\ell-1}\n$$\nGiven $h_{\\ell-1} = T/N_{\\ell-1}$, we have $N_{\\ell-1} h_{\\ell-1} = T$. Thus, the variance of the coarse-level approximation is:\n$$\n\\mathrm{Var}(P_{\\ell-1}) = \\sigma^2 T\n$$\nWe now proceed to evaluate $\\mathrm{Var}(P_\\ell - P_{\\ell-1})$ for each coupling strategy.\n\n**1. Incorrect Coupling Strategy**\n\nUnder this strategy, the set of coarse increments $\\{\\Delta W^{(\\ell-1)}_m\\}_{m=1}^{N_{\\ell-1}}$ is generated independently from the set of fine increments $\\{\\Delta W^{(\\ell)}_n\\}_{n=1}^{N_\\ell}$. Since $P_\\ell$ is a function only of the fine increments and $P_{\\ell-1}$ is a function only of the coarse increments, the random variables $P_\\ell$ and $P_{\\ell-1}$ are independent. For independent random variables, the covariance is zero:\n$$\n\\mathrm{Cov}(P_\\ell, P_{\\ell-1}) = 0\n$$\nSubstituting this into the general variance formula gives:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(P_\\ell) + \\mathrm{Var}(P_{\\ell-1})\n$$\nUsing the previously derived variances for $P_\\ell$ and $P_{\\ell-1}$:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\sigma^2 T + \\sigma^2 T = 2 \\sigma^2 T\n$$\nThis is the variance under the incorrect coupling.\n\n**2. Correct Coupling Strategy**\n\nUnder this strategy, the coarse increments are constructed from the fine increments: $\\Delta W^{(\\ell-1)}_m = \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m}$. We compute the difference $P_\\ell - P_{\\ell-1}$ directly:\n$$\nP_\\ell - P_{\\ell-1} = \\sum_{n=1}^{N_\\ell} \\sigma\\, \\Delta W^{(\\ell)}_n - \\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m\n$$\nSubstituting the coupling definition into the second term:\n$$\n\\sum_{m=1}^{N_{\\ell-1}} \\sigma\\, \\Delta W^{(\\ell-1)}_m = \\sigma \\sum_{m=1}^{N_{\\ell-1}} \\left( \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m} \\right)\n$$\nThis sum covers all the fine increments from $n=1$ to $n=2N_{\\ell-1}$. Since $N_\\ell = 2N_{\\ell-1}$, this sum is precisely the sum of all fine increments:\n$$\n\\sigma \\sum_{m=1}^{N_{\\ell-1}} \\left( \\Delta W^{(\\ell)}_{2m-1} + \\Delta W^{(\\ell)}_{2m} \\right) = \\sigma \\sum_{n=1}^{N_\\ell} \\Delta W^{(\\ell)}_n = P_\\ell\n$$\nTherefore, for this specific SDE, the coarse approximation $P_{\\ell-1}$ is identical to the fine approximation $P_\\ell$ when correct coupling is used. The difference is:\n$$\nP_\\ell - P_{\\ell-1} = P_\\ell - P_\\ell = 0\n$$\nThe variance of a constant is zero:\n$$\n\\mathrm{Var}(P_\\ell - P_{\\ell-1}) = \\mathrm{Var}(0) = 0\n$$\nThis result highlights the a priori variance reduction achieved by correct coupling, which is the foundational principle of the Multilevel Monte Carlo method. For this simple SDE, the reduction is maximal.\n\nThe problem asks for the simplified closed-form analytic expression for the incorrect-coupling variance in terms of $\\sigma$ and $T$. As derived above, this expression is $2 \\sigma^2 T$.", "answer": "$$\n\\boxed{2 \\sigma^{2} T}\n$$", "id": "3068034"}, {"introduction": "Now that you've seen how coupling reduces variance, the next critical question is how to allocate computational resources efficiently across the different levels. Since finer levels are more accurate but also more expensive, we must strike an optimal balance. This exercise guides you through the theoretical heart of MLMC resource allocation by using the method of Lagrange multipliers to derive the optimal number of samples, $N_{\\ell}$, for each level $\\ell$ [@problem_id:3067970]. Solving this foundational optimization problem will reveal the elegant logic that dictates why we perform many cheap simulations on coarse levels and progressively fewer on expensive, fine levels.", "problem": "Consider a scalar stochastic differential equation (SDE) of the form $dX_{t}=a(X_{t})\\,dt+b(X_{t})\\,dW_{t}$, where $a$ and $b$ are sufficiently regular for strong existence and uniqueness, and $W_{t}$ is a standard Brownian motion. The goal is to approximate $\\mathbb{E}[\\varphi(X_{T})]$ for a Lipschitz payoff $\\varphi$ using Multilevel Monte Carlo (MLMC). Let levels be indexed by $\\ell\\in\\{0,1,\\dots,L\\}$ with time steps $h_{\\ell}=h_{0}\\,2^{-\\ell}$, where $h_{0}>0$ and $L\\in\\mathbb{N}$ is chosen to control the bias. Assume Euler–Maruyama coupling of adjacent levels and that the variance of the level-difference estimator and the expected cost per coupled sample scale as\n$$V_{\\ell}=k_{v}\\,h_{\\ell}\\quad\\text{and}\\quad C_{\\ell}=k_{c}\\,h_{\\ell}^{-1},$$\nfor all $\\ell\\in\\{0,1,\\dots,L\\}$, with $k_{v}>0$ and $k_{c}>0$ independent of $\\ell$. Suppose the mean-squared error (MSE) tolerance is $\\varepsilon>0$, and the level $L$ has already been chosen so that the squared bias is at most $\\varepsilon^{2}/2$. You are to choose the number of samples per level $N_{\\ell}\\in\\mathbb{R}_{>0}$ to minimize the total expected cost subject to a variance budget equal to the remaining half of the MSE tolerance:\n$$\\text{minimize }\\sum_{\\ell=0}^{L} C_{\\ell}\\,N_{\\ell}\\quad\\text{subject to}\\quad \\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}}=\\frac{\\varepsilon^{2}}{2}.$$\nStarting only from these assumptions and optimization primitives, derive the exact closed-form expressions for:\n- the optimal $N_{\\ell}$ as a function of $\\varepsilon$, $L$, $h_{\\ell}$, $k_{v}$, and $k_{c}$,\n- the resulting contribution to the expected cost on level $\\ell$, namely $C_{\\ell}\\,N_{\\ell}$,\n- the resulting contribution to the estimator variance from level $\\ell$, namely $V_{\\ell}/N_{\\ell}$.\n\nExpress your final result as three closed-form expressions depending on $\\ell$ in terms of $\\varepsilon$, $L$, $h_{\\ell}$, $k_{v}$, and $k_{c}$. Your final answer must be a single row with three entries, each entry being one of these expressions. Do not introduce any undefined symbols. Do not eliminate $L$ or $h_{\\ell}$ from your final expressions. No numerical rounding is required.", "solution": "The user wants me to solve a constrained optimization problem derived from the Multilevel Monte Carlo (MLMC) method for stochastic differential equations (SDEs).\n\n### Step 1: Extract Givens\n-   SDE: $dX_{t}=a(X_{t})\\,dt+b(X_{t})\\,dW_{t}$\n-   Goal: Approximate $\\mathbb{E}[\\varphi(X_{T})]$\n-   Levels: $\\ell \\in \\{0, 1, \\dots, L\\}$\n-   Time steps: $h_{\\ell} = h_{0}\\,2^{-\\ell}$ for $h_{0} > 0$\n-   Variance scaling: $V_{\\ell} = k_{v}\\,h_{\\ell}$ for $k_{v} > 0$\n-   Cost scaling: $C_{\\ell} = k_{c}\\,h_{\\ell}^{-1}$ for $k_{c} > 0$\n-   Mean-squared error (MSE) tolerance: $\\varepsilon > 0$\n-   Bias constraint: Squared bias is at most $\\varepsilon^{2}/2$, achieved by the choice of $L$.\n-   Optimization problem:\n    -   Minimize: $\\sum_{\\ell=0}^{L} C_{\\ell}\\,N_{\\ell}$\n    -   Subject to: $\\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{\\varepsilon^{2}}{2}$\n    -   Variables: $N_{\\ell} \\in \\mathbb{R}_{>0}$ for $\\ell = 0, \\dots, L$.\n-   Required outputs: Closed-form expressions for $N_{\\ell}$, $C_{\\ell}N_{\\ell}$, and $V_{\\ell}/N_{\\ell}$ as functions of $\\varepsilon$, $L$, $h_{\\ell}$, $k_{v}$, and $k_{c}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly rooted in the theory of numerical methods for SDEs, specifically the MLMC method. The assumed scaling relations for variance ($V_{\\ell} \\propto h_{\\ell}$) and computational cost ($C_{\\ell} \\propto h_{\\ell}^{-1}$) are standard for the Euler-Maruyama discretization scheme applied to SDEs with globally Lipschitz coefficients. The problem is scientifically and mathematically sound.\n2.  **Well-Posed:** The problem is a classic constrained optimization task: minimizing a linear objective function subject to a convex constraint. The domain for the variables $N_{\\ell}$ is the set of positive real numbers, which allows for a straightforward solution using calculus (e.g., Lagrange multipliers). A unique, stable, and meaningful solution exists.\n3.  **Objective:** The problem is stated in precise mathematical language, free of any subjectivity or ambiguity.\n4.  **Complete and Consistent:** The problem provides all necessary information to solve the optimization problem: the objective function, the constraint, the variables, and the definitions of all terms involved. There are no contradictions.\n5.  **Not Unrealistic:** The setup is a standard theoretical model for analyzing the complexity of MLMC methods. It is a valid and widely used simplification.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed to derive the solution.\n\n### Derivation\nThe problem is to minimize the total cost $J = \\sum_{\\ell=0}^{L} C_{\\ell} N_{\\ell}$ subject to the total variance constraint $\\sum_{\\ell=0}^{L} V_{\\ell}/N_{\\ell} = \\varepsilon^{2}/2$. We can solve this using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(N_0, \\dots, N_L, \\lambda) = \\sum_{\\ell=0}^{L} C_{\\ell} N_{\\ell} + \\lambda \\left( \\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}} - \\frac{\\varepsilon^{2}}{2} \\right) $$\nwhere $\\lambda$ is the Lagrange multiplier. To find the optimal values of $N_{\\ell}$, we take the partial derivative of $\\mathcal{L}$ with respect to each $N_{k}$ for $k \\in \\{0, \\dots, L\\}$ and set it to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial N_{k}} = C_{k} - \\lambda \\frac{V_{k}}{N_{k}^{2}} = 0 $$\nSolving for $N_{k}$, we get:\n$$ C_{k} N_{k}^{2} = \\lambda V_{k} \\implies N_{k} = \\sqrt{\\lambda} \\sqrt{\\frac{V_{k}}{C_{k}}} $$\nNote that we take the positive root since $N_{k}$ must be positive.\n\nNow, we substitute this expression for $N_{\\ell}$ back into the variance constraint to determine the value of $\\lambda$.\nFirst, let's find the expression for the term $V_{\\ell}/N_{\\ell}$:\n$$ \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{V_{\\ell}}{\\sqrt{\\lambda} \\sqrt{V_{\\ell}/C_{\\ell}}} = \\frac{1}{\\sqrt{\\lambda}} \\sqrt{V_{\\ell} C_{\\ell}} $$\nNext, we substitute the given scaling relations $V_{\\ell} = k_{v} h_{\\ell}$ and $C_{\\ell} = k_{c} h_{\\ell}^{-1}$:\n$$ \\sqrt{V_{\\ell} C_{\\ell}} = \\sqrt{(k_{v} h_{\\ell}) (k_{c} h_{\\ell}^{-1})} = \\sqrt{k_{v} k_{c}} $$\nThis shows that the product $\\sqrt{V_{\\ell} C_{\\ell}}$ is a constant, independent of the level $\\ell$.\nThe variance constraint becomes:\n$$ \\sum_{\\ell=0}^{L} \\frac{\\sqrt{k_{v} k_{c}}}{\\sqrt{\\lambda}} = \\frac{\\varepsilon^{2}}{2} $$\nSince the term in the sum is constant for all $\\ell$, the sum over $L+1$ levels is:\n$$ (L+1) \\frac{\\sqrt{k_{v} k_{c}}}{\\sqrt{\\lambda}} = \\frac{\\varepsilon^{2}}{2} $$\nSolving for $\\sqrt{\\lambda}$:\n$$ \\sqrt{\\lambda} = \\frac{2 (L+1) \\sqrt{k_{v} k_{c}}}{\\varepsilon^{2}} $$\nNow we have determined the Lagrange multiplier and can find the explicit expressions for the required quantities.\n\n**1. Optimal number of samples $N_{\\ell}$**\nSubstitute the expression for $\\sqrt{\\lambda}$ into the equation for $N_{\\ell}$:\n$$ N_{\\ell} = \\sqrt{\\lambda} \\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}} = \\left( \\frac{2 (L+1) \\sqrt{k_{v} k_{c}}}{\\varepsilon^{2}} \\right) \\sqrt{\\frac{k_{v} h_{\\ell}}{k_{c} h_{\\ell}^{-1}}} $$\n$$ N_{\\ell} = \\left( \\frac{2 (L+1) \\sqrt{k_{v} k_{c}}}{\\varepsilon^{2}} \\right) \\left( \\sqrt{\\frac{k_{v}}{k_{c}}} h_{\\ell} \\right) $$\n$$ N_{\\ell} = \\frac{2 (L+1)}{\\varepsilon^{2}} \\left( \\sqrt{k_{v}k_{c}} \\sqrt{\\frac{k_{v}}{k_{c}}} \\right) h_{\\ell} = \\frac{2 (L+1)}{\\varepsilon^{2}} \\sqrt{k_{v}^{2}} h_{\\ell} $$\nSince $k_{v} > 0$, we have $\\sqrt{k_{v}^2} = k_v$. Thus, the optimal number of samples on level $\\ell$ is:\n$$ N_{\\ell} = \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell} $$\n\n**2. Cost contribution on level $\\ell$, $C_{\\ell} N_{\\ell}$**\nUsing the expression for $N_{\\ell}$ and the given $C_{\\ell} = k_{c} h_{\\ell}^{-1}$:\n$$ C_{\\ell} N_{\\ell} = (k_{c} h_{\\ell}^{-1}) \\left( \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell} \\right) $$\nThe terms $h_{\\ell}^{-1}$ and $h_{\\ell}$ cancel out, yielding a constant cost per level:\n$$ C_{\\ell} N_{\\ell} = \\frac{2 (L+1) k_{c} k_{v}}{\\varepsilon^{2}} $$\n\n**3. Variance contribution from level $\\ell$, $V_{\\ell}/N_{\\ell}$**\nUsing the expression for $N_{\\ell}$ and the given $V_{\\ell} = k_{v} h_{\\ell}$:\n$$ \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{k_{v} h_{\\ell}}{\\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell}} $$\nThe terms $k_{v}$ and $h_{\\ell}$ cancel out, yielding a constant variance contribution per level:\n$$ \\frac{V_{\\ell}}{N_{\\ell}} = \\frac{1}{\\frac{2(L+1)}{\\varepsilon^{2}}} = \\frac{\\varepsilon^{2}}{2 (L+1)} $$\nAs a consistency check, the total variance is $\\sum_{\\ell=0}^{L} V_{\\ell}/N_{\\ell} = \\sum_{\\ell=0}^{L} \\frac{\\varepsilon^{2}}{2(L+1)} = (L+1) \\frac{\\varepsilon^{2}}{2(L+1)} = \\frac{\\varepsilon^{2}}{2}$, which satisfies the constraint.\n\nThe three required expressions are:\n1.  $N_{\\ell} = \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell}$\n2.  $C_{\\ell} N_{\\ell} = \\frac{2 (L+1) k_{c} k_{v}}{\\varepsilon^{2}}$\n3.  $\\frac{V_{\\ell}}{N_{\\ell}} = \\frac{\\varepsilon^{2}}{2(L+1)}$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{2 (L+1) k_{v}}{\\varepsilon^{2}} h_{\\ell} & \\frac{2 (L+1) k_{c} k_{v}}{\\varepsilon^{2}} & \\frac{\\varepsilon^{2}}{2(L+1)} \\end{pmatrix} } $$", "id": "3067970"}, {"introduction": "Theory becomes most powerful when it is put into practice. This final exercise bridges the gap between abstract formulas and a concrete numerical task by simulating a common scenario in computational finance [@problem_id:3067966]. You will use hypothetical but realistic estimates for level-wise variance and cost, applying the optimal allocation formula you explored previously to determine the specific number of samples required at each level. Calculating the resulting total work will give you a tangible feel for the computational savings achieved by the MLMC approach and solidify your understanding of the entire workflow, from pilot estimates to final cost calculation.", "problem": "Consider the geometric Brownian motion stochastic differential equation (SDE) $\\mathrm{d}X_t=\\mu X_t\\,\\mathrm{d}t+\\sigma X_t\\,\\mathrm{d}W_t$ on $t\\in[0,1]$ with initial condition $X_0=1$ and parameters $\\mu,\\sigma\\in\\mathbb{R}$. Let the goal be to approximate $\\mathbb{E}[P]$ for a Lipschitz payoff $P=g(X_1)$ using Multilevel Monte Carlo (MLMC). A hierarchy of Euler–Maruyama time steps $h_\\ell=2^{-\\ell}$ for $\\ell=0,1,2,3,4$ has been fixed by an independent bias analysis ensuring that the squared bias contribution is at most $\\epsilon^2/2$ with target accuracy $\\epsilon=10^{-2}$. From pilot simulations, the level-difference variances and per-sample costs are estimated as $V_\\ell=\\operatorname{Var}(Y_\\ell)=2^{-\\ell}$ and $C_\\ell=2^{\\ell}$ for $\\ell=0,1,2,3,4$, where $Y_0=P_0$ and $Y_\\ell=P_\\ell-P_{\\ell-1}$ for $\\ell\\ge 1$.\n\nDetermine integer sample counts $N_\\ell\\in\\mathbb{N}$, for $\\ell=0,1,2,3,4$, that minimize the total computational work subject to the variance constraint $\\sum_{\\ell=0}^{4}\\frac{V_\\ell}{N_\\ell}\\le \\epsilon^2/2$. Use only first principles of constrained optimization to derive the allocation. Compute the resulting total work $W=\\sum_{\\ell=0}^{4}N_\\ell\\,C_\\ell$ using your integer $N_\\ell$. Report only the total work as your final answer. No rounding instruction applies to the final number; provide the exact integer.", "solution": "The user has provided a scientifically grounded and well-posed problem statement. The task is to determine the integer sample counts $N_\\ell$ for a Multilevel Monte Carlo (MLMC) estimator that minimize the total computational work, subject to a constraint on the estimator's variance. The problem is a classic constrained optimization exercise.\n\nFirst, we formalize the optimization problem.\nLet $L=4$ be the maximum level.\nThe total computational work, which we aim to minimize, is the objective function:\n$$W = \\sum_{\\ell=0}^{L} N_\\ell C_\\ell$$\nHere, $N_\\ell$ are the number of samples on level $\\ell$ and $C_\\ell$ is the computational cost per sample on level $\\ell$.\n\nThe variance of the MLMC estimator is given by $\\sum_{\\ell=0}^{L} \\frac{V_\\ell}{N_\\ell}$, where $V_\\ell$ is the variance of the level difference estimator $Y_\\ell$. The problem specifies a constraint on this variance:\n$$\\sum_{\\ell=0}^{L} \\frac{V_\\ell}{N_\\ell} \\le \\frac{\\epsilon^2}{2}$$\nwhere $\\epsilon$ is the target accuracy.\n\nWe are given the following problem-specific data for levels $\\ell = 0, 1, 2, 3, 4$:\n- Target accuracy: $\\epsilon = 10^{-2}$, so the variance constraint is $\\frac{\\epsilon^2}{2} = \\frac{(10^{-2})^2}{2} = \\frac{10^{-4}}{2} = 5 \\times 10^{-5}$.\n- Level-difference variances: $V_\\ell = 2^{-\\ell}$.\n- Per-sample costs: $C_\\ell = 2^{\\ell}$.\n- The number of levels is $L+1 = 5$.\n\nTo find the minimum of $W$ subject to the constraint, we use the method of Lagrange multipliers. We first treat the $N_\\ell$ as continuous real variables. The minimum work will occur when the variance constraint is active, i.e., at equality:\n$$\\sum_{\\ell=0}^{L} \\frac{V_\\ell}{N_\\ell} = \\frac{\\epsilon^2}{2}$$\n\nThe Lagrangian function $\\mathcal{L}$ is:\n$$\\mathcal{L}(N_0, \\dots, N_L, \\lambda) = \\sum_{\\ell=0}^{L} N_\\ell C_\\ell + \\lambda \\left( \\sum_{\\ell=0}^{L} \\frac{V_\\ell}{N_\\ell} - \\frac{\\epsilon^2}{2} \\right)$$\nwhere $\\lambda$ is the Lagrange multiplier.\n\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each $N_k$ to zero:\n$$\\frac{\\partial \\mathcal{L}}{\\partial N_k} = C_k - \\lambda \\frac{V_k}{N_k^2} = 0$$\nSolving for $N_k$ gives:\n$$C_k N_k^2 = \\lambda V_k \\implies N_k^2 = \\lambda \\frac{V_k}{C_k} \\implies N_k = \\sqrt{\\lambda} \\sqrt{\\frac{V_k}{C_k}}$$\nThis shows that the optimal number of samples $N_k$ is proportional to $\\sqrt{V_k/C_k}$.\n\nNext, we determine the value of $\\sqrt{\\lambda}$ by substituting this expression for $N_k$ back into the active variance constraint:\n$$\\sum_{\\ell=0}^{L} \\frac{V_\\ell}{\\sqrt{\\lambda} \\sqrt{V_\\ell/C_\\ell}} = \\frac{\\epsilon^2}{2}$$\n$$\\frac{1}{\\sqrt{\\lambda}} \\sum_{\\ell=0}^{L} \\frac{V_\\ell \\sqrt{C_\\ell}}{\\sqrt{V_\\ell}} = \\frac{\\epsilon^2}{2}$$\n$$\\frac{1}{\\sqrt{\\lambda}} \\sum_{\\ell=0}^{L} \\sqrt{V_\\ell C_\\ell} = \\frac{\\epsilon^2}{2}$$\nSolving for $\\sqrt{\\lambda}$:\n$$\\sqrt{\\lambda} = \\frac{2}{\\epsilon^2} \\sum_{\\ell=0}^{L} \\sqrt{V_\\ell C_\\ell}$$\n\nNow we substitute this expression for $\\sqrt{\\lambda}$ back into the equation for $N_k$:\n$$N_k = \\left( \\frac{2}{\\epsilon^2} \\sum_{\\ell=0}^{L} \\sqrt{V_\\ell C_\\ell} \\right) \\sqrt{\\frac{V_k}{C_k}}$$\n\nLet's compute the terms for this specific problem.\nThe product $\\sqrt{V_\\ell C_\\ell}$ is:\n$$\\sqrt{V_\\ell C_\\ell} = \\sqrt{2^{-\\ell} \\cdot 2^{\\ell}} = \\sqrt{1} = 1$$\nThe sum over all levels $L=4$ is:\n$$\\sum_{\\ell=0}^{4} \\sqrt{V_\\ell C_\\ell} = \\sum_{\\ell=0}^{4} 1 = 5$$\nThe term $\\sqrt{V_k/C_k}$ is:\n$$\\sqrt{\\frac{V_k}{C_k}} = \\sqrt{\\frac{2^{-k}}{2^k}} = \\sqrt{2^{-2k}} = 2^{-k}$$\n\nNow we can compute the optimal (real-valued) $N_k$:\n$$N_k = \\left( \\frac{2}{(10^{-2})^2} \\cdot 5 \\right) \\cdot 2^{-k} = \\left( \\frac{2}{10^{-4}} \\cdot 5 \\right) \\cdot 2^{-k} = (2 \\cdot 10^4 \\cdot 5) \\cdot 2^{-k} = 10^5 \\cdot 2^{-k}$$\n\nThe problem requires integer sample counts $N_k \\in \\mathbb{N}$. Let us compute the values:\n- For $\\ell=0$: $N_0 = 10^5 \\cdot 2^{-0} = 100000$\n- For $\\ell=1$: $N_1 = 10^5 \\cdot 2^{-1} = 50000$\n- For $\\ell=2$: $N_2 = 10^5 \\cdot 2^{-2} = 25000$\n- For $\\ell=3$: $N_3 = 10^5 \\cdot 2^{-3} = 12500$\n- For $\\ell=4$: $N_4 = 10^5 \\cdot 2^{-4} = 6250$\n\nSince all calculated optimal $N_\\ell$ values are integers, we do not need to perform any rounding. These are the integer sample counts that minimize the cost.\n\nFinally, we compute the total work $W$ using these optimal sample counts:\n$$W = \\sum_{\\ell=0}^{4} N_\\ell C_\\ell = \\sum_{\\ell=0}^{4} (10^5 \\cdot 2^{-\\ell}) \\cdot (2^{\\ell})$$\n$$W = \\sum_{\\ell=0}^{4} 10^5 = 10^5 \\cdot (4 - 0 + 1) = 5 \\cdot 10^5 = 500000$$\n\nAlternatively, one can derive the minimal work directly:\n$$W = \\sum_{\\ell=0}^{L} N_\\ell C_\\ell = \\sum_{\\ell=0}^{L} \\left( \\sqrt{\\lambda} \\sqrt{\\frac{V_\\ell}{C_\\ell}} \\right) C_\\ell = \\sqrt{\\lambda} \\sum_{\\ell=0}^{L} \\sqrt{V_\\ell C_\\ell}$$\nSubstituting the expression for $\\sqrt{\\lambda}$:\n$$W = \\left( \\frac{2}{\\epsilon^2} \\sum_{j=0}^{L} \\sqrt{V_j C_j} \\right) \\left( \\sum_{\\ell=0}^{L} \\sqrt{V_\\ell C_\\ell} \\right) = \\frac{2}{\\epsilon^2} \\left( \\sum_{\\ell=0}^{L} \\sqrt{V_\\ell C_\\ell} \\right)^2$$\nPlugging in the values for this problem:\n$$W = \\frac{2}{(10^{-2})^2} (5)^2 = \\frac{2}{10^{-4}} \\cdot 25 = 2 \\cdot 10^4 \\cdot 25 = 50 \\cdot 10^4 = 500000$$\nBoth methods yield the same result. The minimal total computational work is $500000$.", "answer": "$$\\boxed{500000}$$", "id": "3067966"}]}