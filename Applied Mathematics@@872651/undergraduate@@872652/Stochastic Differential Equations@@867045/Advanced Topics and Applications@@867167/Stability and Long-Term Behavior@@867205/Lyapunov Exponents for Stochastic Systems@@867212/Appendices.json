{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will calculate the top Lyapunov exponent for the most fundamental linear stochastic differential equation: the Geometric Brownian Motion. This exercise is crucial as it demonstrates the core analytical technique, involving Itô's lemma and the properties of Brownian motion, to determine the long-term average exponential growth rate of a system driven by multiplicative noise. Mastering this calculation provides the foundation for analyzing stability in a wide range of stochastic models, from finance to population dynamics [@problem_id:3064458].", "problem": "Consider the scalar stochastic differential equation (SDE) $dX_t = a X_t \\, dt + \\sigma X_t \\, dW_t$ on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})$, where $W_t$ is a standard Brownian motion (also called a Wiener process), $a \\in \\mathbb{R}$ and $\\sigma \\in \\mathbb{R}$ are constants, and $X_0 \\in \\mathbb{R} \\setminus \\{0\\}$ is deterministic. Using only foundational tools such as Itô’s lemma and basic martingale properties, compute the $\\mathbb{P}$-almost sure (a.s.) limit $\\lim_{t \\to \\infty} \\frac{1}{t} \\ln |X_t|$. Your final answer must be a single closed-form analytic expression in terms of $a$ and $\\sigma$.", "solution": "The problem asks for the computation of the almost sure limit $\\lim_{t \\to \\infty} \\frac{1}{t} \\ln |X_t|$, where $X_t$ is the solution to the scalar stochastic differential equation (SDE):\n$$dX_t = a X_t \\, dt + \\sigma X_t \\, dW_t$$\nThe initial condition is $X_0 \\in \\mathbb{R} \\setminus \\{0\\}$, a deterministic constant. The parameters $a$ and $\\sigma$ are real constants, and $W_t$ is a standard one-dimensional Brownian motion. This SDE is a classical example of Geometric Brownian Motion.\n\nTo find an expression for $\\ln|X_t|$, we apply Itô's lemma to the function $f(x) = \\ln|x|$. For $x \\neq 0$, the first and second derivatives of $f(x)$ are $f'(x) = \\frac{1}{x}$ and $f''(x) = -\\frac{1}{x^2}$. The solution to the given SDE starting from a non-zero value $X_0$ will almost surely never reach $0$ for $t  0$, so $f(X_t)$ is well-defined.\n\nLet $Y_t = f(X_t) = \\ln|X_t|$. According to Itô's lemma, the differential $dY_t$ is given by:\n$$dY_t = f'(X_t) \\, dX_t + \\frac{1}{2} f''(X_t) \\, (dX_t)^2$$\nWe substitute the expressions for $dX_t$, $f'(X_t)$, and $f''(X_t)$:\n$$dY_t = \\frac{1}{X_t} (a X_t \\, dt + \\sigma X_t \\, dW_t) + \\frac{1}{2} \\left(-\\frac{1}{X_t^2}\\right) (dX_t)^2$$\nThe quadratic variation term $(dX_t)^2$ is computed using the rules of Itô calculus: $(dt)^2 = 0$, $dt \\, dW_t = 0$, and $(dW_t)^2 = dt$.\n$$(dX_t)^2 = (a X_t \\, dt + \\sigma X_t \\, dW_t)^2 = (\\sigma X_t)^2 (dW_t)^2 = \\sigma^2 X_t^2 \\, dt$$\nWe substitute this back into the expression for $dY_t$:\n$$dY_t = \\frac{1}{X_t} (a X_t \\, dt + \\sigma X_t \\, dW_t) - \\frac{1}{2} \\frac{1}{X_t^2} (\\sigma^2 X_t^2 \\, dt)$$\nSimplifying the terms, we get:\n$$dY_t = (a \\, dt + \\sigma \\, dW_t) - \\frac{1}{2} \\sigma^2 \\, dt$$\n$$dY_t = \\left(a - \\frac{1}{2} \\sigma^2\\right) dt + \\sigma \\, dW_t$$\nThis is an SDE for $Y_t = \\ln|X_t|$. We can solve for $Y_t$ by integrating from $s=0$ to $s=t$:\n$$\\int_0^t dY_s = \\int_0^t \\left(a - \\frac{1}{2} \\sigma^2\\right) ds + \\int_0^t \\sigma \\, dW_s$$\nThis yields:\n$$Y_t - Y_0 = \\left(a - \\frac{1}{2} \\sigma^2\\right) t + \\sigma (W_t - W_0)$$\nGiven that $Y_t = \\ln|X_t|$ and by convention $W_0=0$, we have:\n$$\\ln|X_t| - \\ln|X_0| = \\left(a - \\frac{1}{2} \\sigma^2\\right) t + \\sigma W_t$$\nRearranging the terms gives the explicit solution for $\\ln|X_t|$:\n$$\\ln|X_t| = \\ln|X_0| + \\left(a - \\frac{1}{2} \\sigma^2\\right) t + \\sigma W_t$$\nNow we can compute the desired limit. We divide the entire expression by $t$:\n$$\\frac{1}{t} \\ln|X_t| = \\frac{\\ln|X_0|}{t} + a - \\frac{1}{2} \\sigma^2 + \\sigma \\frac{W_t}{t}$$\nWe take the limit as $t \\to \\infty$:\n$$\\lim_{t \\to \\infty} \\frac{1}{t} \\ln|X_t| = \\lim_{t \\to \\infty} \\left( \\frac{\\ln|X_0|}{t} + a - \\frac{1}{2} \\sigma^2 + \\sigma \\frac{W_t}{t} \\right)$$\nWe evaluate the limit of each term separately. The limit operation is performed on a $\\mathbb{P}$-almost sure basis.\n$1$. The term $\\frac{\\ln|X_0|}{t}$: Since $X_0$ is a non-zero deterministic constant, $\\ln|X_0|$ is a finite constant. Thus, $\\lim_{t \\to \\infty} \\frac{\\ln|X_0|}{t} = 0$.\n$2$. The term $a - \\frac{1}{2} \\sigma^2$: This is a constant, so its limit is itself.\n$3$. The term $\\sigma \\frac{W_t}{t}$: The key component is the asymptotic behavior of the standard Brownian motion $W_t$. The Strong Law of Large Numbers for Brownian Motion states that $\\lim_{t \\to \\infty} \\frac{W_t}{t} = 0$ almost surely. Since $\\sigma$ is a constant, we have $\\lim_{t \\to \\infty} \\sigma \\frac{W_t}{t} = \\sigma \\cdot 0 = 0$ almost surely.\n\nCombining these results, we find the almost sure limit:\n$$\\lim_{t \\to \\infty} \\frac{1}{t} \\ln|X_t| = 0 + \\left(a - \\frac{1}{2} \\sigma^2\\right) + 0 = a - \\frac{1}{2} \\sigma^2$$\nThis limit is known as the top Lyapunov exponent of the stochastic dynamical system defined by the SDE. It characterizes the long-term average exponential growth or decay rate of the solutions.", "answer": "$$\\boxed{a - \\frac{1}{2}\\sigma^2}$$", "id": "3064458"}, {"introduction": "A fascinating and often counter-intuitive aspect of stochastic systems is that the stability of a typical trajectory can differ from the stability of statistical moments like the mean or variance. This practice delves into this concept by distinguishing between the almost sure Lyapunov exponent, which describes sample-path behavior, and the mean-square Lyapunov exponent, which describes the growth of the second moment. By calculating both for the same system, you will gain a deeper understanding of the subtleties of stochastic stability and why noise can simultaneously stabilize individual paths while destabilizing the system on average [@problem_id:3064421].", "problem": "Consider the scalar linear stochastic differential equation (SDE), where SDE stands for stochastic differential equation,\n$$\n\\mathrm{d}Y_t = a\\,Y_t\\,\\mathrm{d}t + b\\,Y_t\\,\\mathrm{d}W_t,\\quad Y_00,\n$$\nwith constant coefficients $a,b\\in\\mathbb{R}$ and $W_t$ a standard Brownian motion. The almost sure Lyapunov exponent is defined by\n$$\n\\lambda_1 = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln Y_t \\quad \\text{almost surely},\n$$\nand the mean-square Lyapunov exponent is defined by\n$$\n\\lambda_2 = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln \\mathbb{E}\\!\\left[Y_t^2\\right].\n$$\nStarting from the given SDE and using only fundamental definitions and well-tested facts of Itô calculus and Brownian motion, determine the correct expressions for $\\lambda_1$ and $\\lambda_2$ and the correct comparison between them.\n\nWhich option is correct?\n\nA. $\\lambda_1 = a$ and $\\lambda_2 = 2a$.\n\nB. $\\lambda_1 = a - \\frac{b^2}{2}$ and $\\lambda_2 = 2a + b^2$, and $\\lambda_2  2\\lambda_1$ whenever $b\\neq 0$.\n\nC. $\\lambda_1 = a + \\frac{b^2}{2}$ and $\\lambda_2 = 2a + 2b^2$, and $\\lambda_2 = 2\\lambda_1$.\n\nD. $\\lambda_1 = a - \\frac{b^2}{2}$ and $\\lambda_2 = 2a - b^2$, and $\\lambda_2 = 2\\lambda_1$.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Stochastic Differential Equation (SDE):** $\\mathrm{d}Y_t = a\\,Y_t\\,\\mathrm{d}t + b\\,Y_t\\,\\mathrm{d}W_t$\n- **Initial Condition:** $Y_00$\n- **Coefficients:** $a,b\\in\\mathbb{R}$ are constants.\n- **Stochastic Process:** $W_t$ is a standard Brownian motion.\n- **Almost Sure Lyapunov Exponent Definition:** $\\lambda_1 = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln Y_t$ almost surely.\n- **Mean-Square Lyapunov Exponent Definition:** $\\lambda_2 = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln \\mathbb{E}\\!\\left[Y_t^2\\right]$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem describes a geometric Brownian motion, a cornerstone model in stochastic calculus with wide applications (e.g., in finance). The definitions of the Lyapunov exponents are standard in the theory of dynamical systems and stochastic analysis. The methods required for the solution rely on Itô calculus, a well-established and rigorous mathematical framework. The problem is scientifically sound.\n2.  **Well-Posed:** The problem is well-posed. The given SDE has a unique, explicit strong solution. The limits defining $\\lambda_1$ and $\\lambda_2$ exist for this SDE. The question asks for the derivation of these exponents and a comparison, which is a clearly defined mathematical task.\n3.  **Objective:** The problem is stated in precise, formal mathematical language, free of any subjectivity or ambiguity.\n4.  **Flaw Checklist:**\n    - No scientific or factual unsoundness.\n    - The problem is formalizable and directly relevant to the topic of Lyapunov exponents for SDEs.\n    - The setup is complete and not contradictory. The condition $Y_0  0$ ensures that $Y_t$ remains positive, so $\\ln Y_t$ is well-defined.\n    - The problem is a standard theoretical exercise and does not involve any unrealistic or infeasible physical conditions.\n    - The problem is well-structured and admits a unique solution.\n    - The problem is a non-trivial application of Itô's lemma and properties of Brownian motion, not a tautology.\n    - The results are mathematically verifiable through rigorous proof.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of the Almost Sure Lyapunov Exponent ($\\lambda_1$)\n\nTo find $\\lambda_1$, we need an expression for $\\ln Y_t$. We apply Itô's lemma to the function $f(Y_t) = \\ln Y_t$. The derivatives of $f(y) = \\ln y$ are $f'(y) = \\frac{1}{y}$ and $f''(y) = -\\frac{1}{y^2}$.\n\nItô's lemma states that $\\mathrm{d}f(Y_t) = f'(Y_t)\\,\\mathrm{d}Y_t + \\frac{1}{2}f''(Y_t)(\\mathrm{d}Y_t)^2$.\n\nFirst, we calculate the quadratic variation term $(\\mathrm{d}Y_t)^2$:\n$$\n(\\mathrm{d}Y_t)^2 = (a\\,Y_t\\,\\mathrm{d}t + b\\,Y_t\\,\\mathrm{d}W_t)^2\n$$\nUsing the Itô multiplication rules $(\\mathrm{d}t)^2 = 0$, $\\mathrm{d}t\\,\\mathrm{d}W_t = 0$, and $(\\mathrm{d}W_t)^2 = \\mathrm{d}t$, we get:\n$$\n(\\mathrm{d}Y_t)^2 = (b\\,Y_t)^2(\\mathrm{d}W_t)^2 = b^2Y_t^2\\,\\mathrm{d}t\n$$\nNow, substitute the terms into Itô's lemma for $\\mathrm{d}(\\ln Y_t)$:\n$$\n\\mathrm{d}(\\ln Y_t) = \\left(\\frac{1}{Y_t}\\right)(a\\,Y_t\\,\\mathrm{d}t + b\\,Y_t\\,\\mathrm{d}W_t) + \\frac{1}{2}\\left(-\\frac{1}{Y_t^2}\\right)(b^2Y_t^2\\,\\mathrm{d}t)\n$$\n$$\n\\mathrm{d}(\\ln Y_t) = (a\\,\\mathrm{d}t + b\\,\\mathrm{d}W_t) - \\frac{b^2}{2}\\,\\mathrm{d}t\n$$\n$$\n\\mathrm{d}(\\ln Y_t) = \\left(a - \\frac{b^2}{2}\\right)\\mathrm{d}t + b\\,\\mathrm{d}W_t\n$$\nIntegrating this SDE from $0$ to $t$:\n$$\n\\ln Y_t - \\ln Y_0 = \\int_0^t \\left(a - \\frac{b^2}{2}\\right)\\mathrm{d}s + \\int_0^t b\\,\\mathrm{d}W_s\n$$\n$$\n\\ln Y_t = \\ln Y_0 + \\left(a - \\frac{b^2}{2}\\right)t + b\\,W_t\n$$\nNow we can compute $\\lambda_1$:\n$$\n\\lambda_1 = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln Y_t = \\lim_{t\\to\\infty}\\frac{1}{t}\\left[\\ln Y_0 + \\left(a - \\frac{b^2}{2}\\right)t + b\\,W_t\\right]\n$$\n$$\n\\lambda_1 = \\lim_{t\\to\\infty}\\left(\\frac{\\ln Y_0}{t}\\right) + \\left(a - \\frac{b^2}{2}\\right) + \\lim_{t\\to\\infty}\\left(\\frac{b\\,W_t}{t}\\right)\n$$\nBy the strong law of large numbers for Brownian motion, $\\lim_{t\\to\\infty} \\frac{W_t}{t} = 0$ almost surely. Also, $\\lim_{t\\to\\infty}\\frac{\\ln Y_0}{t} = 0$. Therefore,\n$$\n\\lambda_1 = 0 + \\left(a - \\frac{b^2}{2}\\right) + 0 = a - \\frac{b^2}{2}\n$$\n\n### Derivation of the Mean-Square Lyapunov Exponent ($\\lambda_2$)\n\nTo find $\\lambda_2$, we need an expression for $\\mathbb{E}[Y_t^2]$. Let's find an SDE for $Y_t^2$ by applying Itô's lemma to the function $g(y) = y^2$. The derivatives are $g'(y) = 2y$ and $g''(y) = 2$.\n$$\n\\mathrm{d}(Y_t^2) = g'(Y_t)\\,\\mathrm{d}Y_t + \\frac{1}{2}g''(Y_t)(\\mathrm{d}Y_t)^2\n$$\n$$\n\\mathrm{d}(Y_t^2) = (2Y_t)(a\\,Y_t\\,\\mathrm{d}t + b\\,Y_t\\,\\mathrm{d}W_t) + \\frac{1}{2}(2)(b^2Y_t^2\\,\\mathrm{d}t)\n$$\n$$\n\\mathrm{d}(Y_t^2) = 2aY_t^2\\,\\mathrm{d}t + 2bY_t^2\\,\\mathrm{d}W_t + b^2Y_t^2\\,\\mathrm{d}t\n$$\n$$\n\\mathrm{d}(Y_t^2) = (2a + b^2)Y_t^2\\,\\mathrm{d}t + 2bY_t^2\\,\\mathrm{d}W_t\n$$\nNow, let $m_2(t) = \\mathbb{E}[Y_t^2]$. Taking the expectation of the integral form of the above SDE:\n$$\n\\mathbb{E}[Y_t^2] - \\mathbb{E}[Y_0^2] = \\mathbb{E}\\left[\\int_0^t (2a + b^2)Y_s^2\\,\\mathrm{d}s\\right] + \\mathbb{E}\\left[\\int_0^t 2bY_s^2\\,\\mathrm{d}W_s\\right]\n$$\nBy Fubini's theorem (for the $\\mathrm{d}s$ integral) and the property that the expectation of an Itô integral is zero, we get:\n$$\nm_2(t) - m_2(0) = \\int_0^t (2a + b^2)\\mathbb{E}[Y_s^2]\\,\\mathrm{d}s + 0\n$$\nDifferentiating with respect to $t$, we obtain an ordinary differential equation (ODE) for $m_2(t)$:\n$$\n\\frac{\\mathrm{d}m_2(t)}{\\mathrm{d}t} = (2a + b^2)m_2(t)\n$$\nThe solution to this ODE, with initial condition $m_2(0) = \\mathbb{E}[Y_0^2] = Y_0^2$, is:\n$$\nm_2(t) = Y_0^2 e^{(2a + b^2)t}\n$$\nNow we can compute $\\lambda_2$:\n$$\n\\lambda_2 = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln \\mathbb{E}[Y_t^2] = \\lim_{t\\to\\infty}\\frac{1}{t}\\ln\\left(Y_0^2 e^{(2a + b^2)t}\\right)\n$$\n$$\n\\lambda_2 = \\lim_{t\\to\\infty}\\frac{1}{t}\\left[\\ln(Y_0^2) + (2a + b^2)t\\right]\n$$\n$$\n\\lambda_2 = \\lim_{t\\to\\infty}\\left(\\frac{\\ln(Y_0^2)}{t}\\right) + (2a + b^2) = 0 + 2a + b^2\n$$\n$$\n\\lambda_2 = 2a + b^2\n$$\n\n### Comparison\n\nWe have found the expressions:\n- $\\lambda_1 = a - \\frac{b^2}{2}$\n- $\\lambda_2 = 2a + b^2$\n\nLet's compare $\\lambda_2$ with $2\\lambda_1$:\n$$\n2\\lambda_1 = 2\\left(a - \\frac{b^2}{2}\\right) = 2a - b^2\n$$\nThe difference is:\n$$\n\\lambda_2 - 2\\lambda_1 = (2a + b^2) - (2a - b^2) = 2b^2\n$$\nSince $b \\in \\mathbb{R}$, $b^2 \\ge 0$, which implies $2b^2 \\ge 0$.\nThus, $\\lambda_2 \\ge 2\\lambda_1$.\nThe equality $\\lambda_2 = 2\\lambda_1$ holds if and only if $2b^2 = 0$, which means $b = 0$.\nIf $b \\neq 0$, then $b^2  0$, and $\\lambda_2  2\\lambda_1$.\n\n### Evaluation of Options\n\n**A. $\\lambda_1 = a$ and $\\lambda_2 = 2a$.**\nThis is incorrect. Our derived formulas are $\\lambda_1 = a - b^2/2$ and $\\lambda_2 = 2a + b^2$. Option A is only valid in the deterministic case where $b=0$.\n\n**B. $\\lambda_1 = a - \\frac{b^2}{2}$ and $\\lambda_2 = 2a + b^2$, and $\\lambda_2  2\\lambda_1$ whenever $b\\neq 0$.**\nThe expression for $\\lambda_1$ matches our derivation. The expression for $\\lambda_2$ matches our derivation. The comparison shows that $\\lambda_2 - 2\\lambda_1 = 2b^2$, so if $b \\neq 0$, then $2b^2  0$ and $\\lambda_2  2\\lambda_1$. This option is fully consistent with our results.\n**Correct**.\n\n**C. $\\lambda_1 = a + \\frac{b^2}{2}$ and $\\lambda_2 = 2a + 2b^2$, and $\\lambda_2 = 2\\lambda_1$.**\nThis is incorrect. The expression for $\\lambda_1$ has the wrong sign for the $b^2$ term (this would be correct for the Stratonovich interpretation, but the problem specifies standard Brownian motion and Itô calculus). The expression for $\\lambda_2$ is also incorrect. The comparison $\\lambda_2 = 2\\lambda_1$ gives $2a + 2b^2 = 2(a + b^2/2) = 2a + b^2$, which implies $2b^2=b^2$, so $b=0$. The statement is only true for $b=0$, not generally.\n\n**D. $\\lambda_1 = a - \\frac{b^2}{2}$ and $\\lambda_2 = 2a - b^2$, and $\\lambda_2 = 2\\lambda_1$.**\nThis is incorrect. The expression for $\\lambda_1$ is correct, but the expression for $\\lambda_2$ is not. Our derivation yielded $\\lambda_2 = 2a + b^2$, not $2a-b^2$. Their expression for $\\lambda_2$ is actually equal to $2\\lambda_1$, making their comparison internally consistent but based on a false premise for $\\lambda_2$.", "answer": "$$\\boxed{B}$$", "id": "3064421"}, {"introduction": "While analytical solutions are invaluable for building intuition, most real-world systems are too complex to be solved with pen and paper, especially in higher dimensions. This final practice bridges the gap between theory and computation by guiding you to implement a robust numerical algorithm for estimating the top Lyapunov exponent of a general multi-dimensional linear SDE. You will use the respected QR decomposition method to track the growth of tangent vectors, a technique widely used in scientific computing to analyze the stability of complex dynamical systems [@problem_id:3064480].", "problem": "Consider the linear stochastic differential equation with multiplicative noise in $\\mathbb{R}^d$,\n$$\ndY_t = A\\,Y_t\\,dt + \\sum_{i=1}^{m} B_i\\,Y_t\\,dW_t^{(i)},\n$$\nwhere $A \\in \\mathbb{R}^{d\\times d}$ is a constant drift matrix, each $B_i \\in \\mathbb{R}^{d\\times d}$ is a constant diffusion (noise) matrix, and $W_t^{(i)}$ are independent standard Brownian motions (Wiener processes). The top Lyapunov exponent $\\lambda_1$ of this random linear flow is defined by the limit\n$$\n\\lambda_1 = \\lim_{t\\to\\infty} \\frac{1}{t}\\,\\log \\,\\frac{\\|Y_t\\|}{\\|Y_0\\|},\n$$\nfor typical initial conditions $Y_0 \\neq 0$, where $\\|\\cdot\\|$ denotes the Euclidean norm on $\\mathbb{R}^d$.\n\nWrite a complete, runnable program that estimates $\\lambda_1$ using a numerical algorithm that evolves multiple tangent vectors and performs periodic reorthonormalization via the orthogonal-triangular (QR) decomposition (QR). Your program must:\n- Discretize time with step $\\Delta t$ and use the Euler–Maruyama update for the linear system. For each time step, approximate Brownian increments by independent Gaussian random variables $\\Delta W^{(i)} \\sim \\mathcal{N}(0,\\Delta t)$ and update the flow with the matrix\n$$\nM_k = I_d + A\\,\\Delta t + \\sum_{i=1}^{m} B_i\\,\\Delta W_k^{(i)},\n$$\nwhere $I_d$ is the $d\\times d$ identity matrix and $k$ is the time-step index.\n- Evolve $d$ tangent vectors simultaneously, arranged as columns of a matrix $Q_k \\in \\mathbb{R}^{d\\times d}$, starting from an orthonormal basis (for instance, $Q_0 = I_d$). Over a block of $p$ time steps, accumulate the product $V = M_{k+p-1}\\cdots M_{k} Q_k$.\n- Every $p$ time steps, perform QR decomposition $V = Q\\,R$ with $Q \\in \\mathbb{R}^{d\\times d}$ orthogonal and $R \\in \\mathbb{R}^{d\\times d}$ upper-triangular. Record the logarithm of the absolute value of the first diagonal entry $\\log|R_{11}|$. Replace $Q_k$ by $Q$ and continue. If the simulation ends between reorthonormalization points, perform one final QR step to record the remaining growth.\n- Estimate the top Lyapunov exponent by dividing the accumulated sum of $\\log|R_{11}|$ over all QR steps by the total simulated time $T$.\n\nYour program must implement this algorithm and produce results for the following test suite. Use the given random seed for reproducibility in cases with noise. In all cases, set the initial orthonormal matrix to $Q_0 = I_d$.\n\nTest suite:\n1. Analytic baseline in one dimension:\n   - Dimension: $d=1$.\n   - Number of noise components: $m=1$.\n   - Drift: $A = [0.3]$.\n   - Diffusion matrices: $B_1 = [0.8]$.\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 10$.\n   - Random seed: $42$.\n\n2. Deterministic baseline in two dimensions:\n   - Dimension: $d=2$.\n   - Number of noise components: $m=0$.\n   - Drift: $A = \\mathrm{diag}(0.1,\\,-0.5)$.\n   - Diffusion matrices: none.\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 10$.\n   - Random seed: $123$ (unused but include for a consistent interface).\n\n3. Stable drift with isotropic multiplicative noise in two dimensions:\n   - Dimension: $d=2$.\n   - Number of noise components: $m=1$.\n   - Drift: $A = \\mathrm{diag}(-0.2,\\,-0.3)$.\n   - Diffusion matrices: $B_1 = \\mathrm{diag}(0.7,\\,0.7)$.\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 10$.\n   - Random seed: $2023$.\n\n4. Non-commuting drift and noise in three dimensions:\n   - Dimension: $d=3$.\n   - Number of noise components: $m=2$.\n   - Drift:\n     $$\n     A = \\begin{bmatrix}\n     0.0  1.0  0.0 \\\\\n     -2.0  -0.1  0.0 \\\\\n     0.0  0.0  -0.2\n     \\end{bmatrix}.\n     $$\n   - Diffusion matrices:\n     $$\n     B_1 = \\begin{bmatrix}\n     0.3  0.0  0.0 \\\\\n     0.0  0.3  0.0 \\\\\n     0.0  0.0  0.0\n     \\end{bmatrix}, \\quad\n     B_2 = \\begin{bmatrix}\n     0.0  0.5  0.0 \\\\\n     -0.5  0.0  0.0 \\\\\n     0.0  0.0  0.0\n     \\end{bmatrix}.\n     $$\n   - Total time: $T = 50.0$.\n   - Time step: $\\Delta t = 10^{-3}$.\n   - Reorthonormalization period (in steps): $p = 5$.\n   - Random seed: $7$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_j$ is the estimated top Lyapunov exponent for test case $j$. No other text should be printed. Since no physical units are involved, report each $r_j$ as a floating-point number in plain numerical form.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically sound, well-posed, and contains all necessary information to implement the specified numerical algorithm for estimating the top Lyapunov exponent of a linear stochastic differential equation (SDE).\n\n### Principle of the Method\n\nThe problem asks for the numerical estimation of the top Lyapunov exponent, $\\lambda_1$, for a linear stochastic system. The Lyapunov exponents of a dynamical system quantify the average exponential rates of divergence or convergence of nearby trajectories in the state space. For a linear system $dY_t = F(Y_t) dt + G(Y_t) dW_t$, the evolution of an infinitesimal perturbation vector $\\delta Y_t$ is governed by the linearized equation, which in this case is the original SDE itself because the system is already linear:\n$$\nd(\\delta Y_t) = A\\,(\\delta Y_t)\\,dt + \\sum_{i=1}^{m} B_i\\,(\\delta Y_t)\\,dW_t^{(i)}\n$$\nThe top Lyapunov exponent, $\\lambda_1$, corresponds to the maximum average exponential growth rate of the norm of a solution vector, $\\|\\delta Y_t\\|$.\n\nA direct numerical simulation of this equation for a single vector would cause its direction to rapidly align with the direction of maximal growth, making it impossible to resolve other, smaller exponents. Furthermore, its magnitude would either explode or vanish, leading to numerical overflow or underflow.\n\nThe standard algorithm to overcome this, and the one specified in the problem, is based on tracking the evolution of a full orthonormal basis of $d$ tangent vectors. This basis is periodically reorthonormalized to preserve directional information and numerical stability.\n\n### Algorithmic Steps\n\n1.  **Discretization:** The continuous-time SDE is approximated by a discrete-time map using the Euler-Maruyama scheme. Over a small time interval $\\Delta t$, the change in the state $Y_t$ is approximated as:\n    $$\n    \\Delta Y_t \\approx A\\,Y_t\\,\\Delta t + \\sum_{i=1}^{m} B_i\\,Y_t\\,\\Delta W_t^{(i)}\n    $$\n    where the continuous Brownian motion increment $dW_t^{(i)}$ is replaced by a discrete random variable $\\Delta W_k^{(i)} \\sim \\mathcal{N}(0, \\Delta t)$. This gives the update rule $Y_{k+1} = Y_k + \\Delta Y_k$, which can be written as $Y_{k+1} = M_k Y_k$ where $M_k$ is the random update matrix:\n    $$\n    M_k = I_d + A\\,\\Delta t + \\sum_{i=1}^{m} B_i\\,\\Delta W_k^{(i)}\n    $$\n    Here, $I_d$ is the $d \\times d$ identity matrix.\n\n2.  **Tangent Vector Evolution and Reorthonormalization:** Instead of evolving a single vector, we evolve a set of $d$ orthonormal vectors, which form the columns of an orthogonal matrix $Q_k \\in \\mathbb{R}^{d\\times d}$. We initialize with an orthonormal basis, typically $Q_0 = I_d$.\n    The algorithm proceeds in blocks of $p$ steps.\n    - At the beginning of a block, we have an orthonormal matrix $Q_{\\text{start}}$.\n    - We compute the product of the next $p$ random update matrices with $Q_{\\text{start}}$:\n      $$\n      V = (M_{k+p-1} \\cdots M_{k+1} M_k) \\, Q_{\\text{start}}\n      $$\n    - The columns of $V$ represent the evolved (and now generally non-orthonormal) basis vectors. To extract their growth and new orientations, we perform a QR decomposition: $V = QR$.\n    - The matrix $Q \\in \\mathbb{R}^{d\\times d}$ is orthogonal and its columns form the new orthonormal basis for the next block.\n    - The matrix $R \\in \\mathbb{R}^{d\\times d}$ is upper-triangular. Its diagonal elements $R_{ii}$ represent the expansion or contraction factors of the corresponding vectors in the frame. Specifically, $R_{11}$ measures the growth of the first vector, which aligns with the direction of maximal growth over the block.\n\n3.  **Lyapunov Exponent Estimation:** The top Lyapunov exponent is the average rate of logarithmic growth of the most rapidly growing direction. Over one block of $p$ steps (time duration $p\\,\\Delta t$), the logarithmic growth of the first vector is $\\log|R_{11}|$. To find the average rate over the total time $T$, we sum these logarithmic growths over all reorthonormalization steps and divide by $T$.\n    $$\n    \\lambda_1 \\approx \\frac{1}{T} \\sum_{j=1}^{N_{\\text{blocks}}} \\log|R_{11}^{(j)}|\n    $$\n    where $R^{(j)}$ is the R-matrix from the $j$-th QR decomposition. This procedure is robust and converges to the true exponent as $T \\to \\infty$. The implementation will handle a final, partial block by performing a final QR decomposition on the product accumulated over the remaining steps.\n\n### Implementation Details\n\nA single function will implement the described algorithm for a given set of parameters ($d, m, A, \\{B_i\\}, T, \\Delta t, p$, and a random seed).\n- A modern random number generator (`numpy.random.default_rng`) is used for reproducibility.\n- The total number of steps is calculated as $N = \\text{round}(T/\\Delta t)$.\n- A `while` loop iterates through the time steps, processing them in blocks of size at most $p$. This structure naturally handles any final partial block.\n- Within each block, the product $V = M_{k+\\text{steps\\_in\\_block}-1}\\cdots M_k Q_{start}$ is computed.\n- The QR decomposition is performed using `numpy.linalg.qr`.\n- The sum of $\\log|R_{11}|$ is accumulated.\n- The final estimate for $\\lambda_1$ is computed by dividing this sum by the total time $T$.\n- This process is repeated for each test case provided in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_top_lyapunov_exponent(d, m, A, B_list, T, dt, p, seed):\n    \"\"\"\n    Estimates the top Lyapunov exponent for a linear SDE using QR reorthonormalization.\n\n    Args:\n        d (int): Dimension of the system.\n        m (int): Number of noise components.\n        A (np.ndarray): Drift matrix (d x d).\n        B_list (list of np.ndarray): List of diffusion matrices (each d x d).\n        T (float): Total simulation time.\n        dt (float): Time step for discretization.\n        p (int): Number of steps between reorthonormalizations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The estimated top Lyapunov exponent.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    n_steps = int(round(T / dt))\n    \n    # Initialize the orthonormal basis Q and the sum of log-growths.\n    Q = np.identity(d)\n    log_growth_sum = 0.0\n\n    step_idx = 0\n    while step_idx  n_steps:\n        # Determine the number of steps for the current block.\n        # This handles the final partial block automatically.\n        steps_in_block = min(p, n_steps - step_idx)\n        \n        # Start the accumulation with the current orthonormal basis.\n        V = np.copy(Q)\n        \n        # Evolve over the steps in the block.\n        for _ in range(steps_in_block):\n            # Generate Brownian increments for this time step.\n            # dW ~ N(0, dt), so standard normal scaled by sqrt(dt).\n            dW = rng.normal(0.0, np.sqrt(dt), size=m)\n\n            # Construct the Euler-Maruyama update matrix M_k.\n            M = np.identity(d) + A * dt\n            if m  0:\n                noise_term = sum(B * dW_i for B, dW_i in zip(B_list, dW))\n                M += noise_term\n            \n            # Apply the update matrix.\n            V = M @ V\n        \n        # Perform QR decomposition on the evolved matrix V.\n        # V = Q_new * R\n        Q, R = np.linalg.qr(V)\n        \n        # Accumulate the log of the growth factor for the first vector.\n        # The problem asks for log|R_11|.\n        log_growth_sum += np.log(np.abs(R[0, 0]))\n\n        # Increment step_idx by the number of steps processed.\n        step_idx += steps_in_block\n\n    # The Lyapunov exponent is the time-average of the log-growth.\n    lambda_1_estimate = log_growth_sum / T\n    \n    return lambda_1_estimate\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the results in the required format.\n    \"\"\"\n    test_cases = [\n        # 1. Analytic baseline in one dimension\n        {\n            \"d\": 1, \"m\": 1,\n            \"A\": np.array([[0.3]]),\n            \"B_list\": [np.array([[0.8]])],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 10, \"seed\": 42\n        },\n        # 2. Deterministic baseline in two dimensions\n        {\n            \"d\": 2, \"m\": 0,\n            \"A\": np.diag([0.1, -0.5]),\n            \"B_list\": [],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 10, \"seed\": 123\n        },\n        # 3. Stable drift with isotropic multiplicative noise in two dimensions\n        {\n            \"d\": 2, \"m\": 1,\n            \"A\": np.diag([-0.2, -0.3]),\n            \"B_list\": [np.diag([0.7, 0.7])],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 10, \"seed\": 2023\n        },\n        # 4. Non-commuting drift and noise in three dimensions\n        {\n            \"d\": 3, \"m\": 2,\n            \"A\": np.array([\n                [0.0, 1.0, 0.0],\n                [-2.0, -0.1, 0.0],\n                [0.0, 0.0, -0.2]\n            ]),\n            \"B_list\": [\n                np.array([\n                    [0.3, 0.0, 0.0],\n                    [0.0, 0.3, 0.0],\n                    [0.0, 0.0, 0.0]\n                ]),\n                np.array([\n                    [0.0, 0.5, 0.0],\n                    [-0.5, 0.0, 0.0],\n                    [0.0, 0.0, 0.0]\n                ])\n            ],\n            \"T\": 50.0, \"dt\": 1e-3, \"p\": 5, \"seed\": 7\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = calculate_top_lyapunov_exponent(**case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3064480"}]}