{"hands_on_practices": [{"introduction": "This practice builds your understanding of the Kalman-Bucy filter from the ground up by connecting the estimation error directly to the Riccati equation. By deriving and solving the error's stochastic differential equation under a constant gain, you will see how the filter's long-term stability and performance are determined. This foundational exercise [@problem_id:3080857] reinforces essential skills in Itô calculus and illuminates the mathematical engine behind the filter's steady-state solution.", "problem": "Consider the scalar linear Gaussian continuous-time system described by the stochastic differential equations (SDEs)\n$$\n\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\qquad \\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\gamma\\,\\mathrm{d}V_t,\n$$\nwhere $a\\in\\mathbb{R}$, $c\\in\\mathbb{R}\\setminus\\{0\\}$, $\\sigma>0$, $\\gamma>0$, and $W_t$, $V_t$ are independent standard Brownian motions. Let $\\hat{x}_t$ denote the estimator produced by the continuous-time Kalman-Bucy filter (KBF) written in innovations form:\n$$\n\\mathrm{d}\\hat{x}_t = a\\,\\hat{x}_t\\,\\mathrm{d}t + K\\big(\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t\\big),\n$$\nwhere the Kalman gain $K$ is a constant. Define the estimation error $\\tilde{x}_t := x_t - \\hat{x}_t$ with initial condition $\\tilde{x}_0$ independent of $(W_t,V_t)$ and with variance $\\mathbb{E}[\\tilde{x}_0^2] = P_0 \\ge 0$.\n\nUsing only first principles of Itô calculus, the independence of the driving noises, and the definition of the innovations process $\\mathrm{d}I_t := \\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t$, carry out the following:\n\n1. Derive and solve explicitly the SDE satisfied by the estimation error $\\tilde{x}_t$ when $K$ is constant.\n2. Compute the variance $P(t) := \\mathbb{E}[\\tilde{x}_t^2]$ in closed form in terms of $a$, $c$, $\\sigma$, $\\gamma$, $K$, and $P_0$, and determine its long-time limit $P_{\\infty}$ under the stability condition $a - Kc < 0$.\n3. In the innovations approach to filtering, the steady-state Kalman gain and the steady-state error variance satisfy the well-tested relationship $K = \\frac{c}{\\gamma^{2}}\\,P_{\\infty}$. Use this relationship to eliminate $K$ from your expression for $P_{\\infty}$ and verify that the resulting $P_{\\infty}$ satisfies the algebraic Riccati steady-state equation for this scalar system. Provide your final answer as a single closed-form expression for $P_{\\infty}$ in terms of $a$, $c$, $\\sigma$, and $\\gamma$.\n\nYour final answer must be a single analytical expression. No rounding is required and no units are involved.", "solution": "The problem statement is parsed and validated.\n\n### Step 1: Extract Givens\n- State process SDE: $\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t$\n- Observation process SDE: $\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\gamma\\,\\mathrm{d}V_t$\n- Parameters: $a\\in\\mathbb{R}$, $c\\in\\mathbb{R}\\setminus\\{0\\}$, $\\sigma>0$, $\\gamma>0$\n- Noise processes: $W_t$ and $V_t$ are independent standard Brownian motions.\n- Estimator (Kalman-Bucy filter) SDE: $\\mathrm{d}\\hat{x}_t = a\\,\\hat{x}_t\\,\\mathrm{d}t + K\\big(\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t\\big)$\n- Kalman gain $K$ is a constant.\n- Estimation error: $\\tilde{x}_t := x_t - \\hat{x}_t$\n- Initial error variance: $\\mathbb{E}[\\tilde{x}_0^2] = P_0 \\ge 0$, with $\\tilde{x}_0$ independent of $W_t$ and $V_t$.\n- Innovations process: $\\mathrm{d}I_t := \\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t$\n- Stability condition: $a - Kc < 0$ for the long-time limit.\n- Steady-state relationship: $K = \\frac{c}{\\gamma^{2}}\\,P_{\\infty}$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in stochastic filtering theory, specifically concerning the continuous-time Kalman-Bucy filter for a scalar linear Gaussian system.\n- **Scientifically Grounded**: The problem is based on the well-established mathematical framework of stochastic differential equations and filtering theory. The models are standard.\n- **Well-Posed**: The problem is clearly defined, with all necessary parameters, equations, and conditions provided. The stability condition ensures the existence of a unique steady-state solution.\n- **Objective**: The problem is stated in precise, mathematical language, free of any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the invalidity flaws. It is self-contained, consistent, and scientifically sound.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution is developed in three parts as requested by the problem statement.\n\n**Part 1: Derivation and solution of the SDE for the estimation error $\\tilde{x}_t$**\n\nThe estimation error is defined as $\\tilde{x}_t = x_t - \\hat{x}_t$. Using Itô's lemma, its differential is $\\mathrm{d}\\tilde{x}_t = \\mathrm{d}x_t - \\mathrm{d}\\hat{x}_t$. We substitute the given SDEs for the state $x_t$ and the estimator $\\hat{x}_t$:\n$$\n\\mathrm{d}\\tilde{x}_t = (a\\,x_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t) - \\left(a\\,\\hat{x}_t\\,\\mathrm{d}t + K(\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t)\\right)\n$$\nNext, we substitute the SDE for the observation process $\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\gamma\\,\\mathrm{d}V_t$:\n$$\n\\mathrm{d}\\tilde{x}_t = a\\,x_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t - a\\,\\hat{x}_t\\,\\mathrm{d}t - K\\left((c\\,x_t\\,\\mathrm{d}t + \\gamma\\,\\mathrm{d}V_t) - c\\,\\hat{x}_t\\,\\mathrm{d}t\\right)\n$$\nGroup the terms involving $\\mathrm{d}t$, $\\mathrm{d}W_t$, and $\\mathrm{d}V_t$:\n$$\n\\mathrm{d}\\tilde{x}_t = (a\\,x_t - a\\,\\hat{x}_t - K(c\\,x_t - c\\,\\hat{x}_t))\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t - K\\gamma\\,\\mathrm{d}V_t\n$$\nUsing the definition $\\tilde{x}_t = x_t - \\hat{x}_t$, we obtain the SDE for the estimation error:\n$$\n\\mathrm{d}\\tilde{x}_t = (a\\tilde{x}_t - Kc\\tilde{x}_t)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t - K\\gamma\\,\\mathrm{d}V_t\n$$\n$$\n\\mathrm{d}\\tilde{x}_t = (a-Kc)\\tilde{x}_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t - K\\gamma\\,\\mathrm{d}V_t\n$$\nThis is a linear SDE of the Ornstein-Uhlenbeck type. To solve it explicitly, we use an integrating factor, $Z_t = \\exp(-(a-Kc)t)\\tilde{x}_t$. Applying Itô's product rule:\n$$\n\\mathrm{d}Z_t = -(a-Kc)\\exp(-(a-Kc)t)\\tilde{x}_t\\,\\mathrm{d}t + \\exp(-(a-Kc)t)\\mathrm{d}\\tilde{x}_t\n$$\nSubstituting the derived SDE for $\\mathrm{d}\\tilde{x}_t$:\n$$\n\\mathrm{d}Z_t = -(a-Kc)\\exp(-(a-Kc)t)\\tilde{x}_t\\,\\mathrm{d}t + \\exp(-(a-Kc)t)\\left((a-Kc)\\tilde{x}_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t - K\\gamma\\,\\mathrm{d}V_t\\right)\n$$\nThe drift terms cancel, leaving:\n$$\n\\mathrm{d}Z_t = \\exp(-(a-Kc)t)(\\sigma\\,\\mathrm{d}W_t - K\\gamma\\,\\mathrm{d}V_t)\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\nZ_t - Z_0 = \\int_0^t \\exp(-(a-Kc)s)\\sigma\\,\\mathrm{d}W_s - \\int_0^t \\exp(-(a-Kc)s)K\\gamma\\,\\mathrm{d}V_s\n$$\nSubstituting back $Z_t = \\exp(-(a-Kc)t)\\tilde{x}_t$ and $Z_0 = \\tilde{x}_0$, we get the explicit solution for $\\tilde{x}_t$:\n$$\n\\tilde{x}_t = \\exp((a-Kc)t)\\tilde{x}_0 + \\int_0^t \\exp((a-Kc)(t-s))\\sigma\\,\\mathrm{d}W_s - \\int_0^t \\exp((a-Kc)(t-s))K\\gamma\\,\\mathrm{d}V_s\n$$\n\n**Part 2: Computation of the error variance $P(t)$ and its long-time limit $P_{\\infty}$**\n\nThe error variance is $P(t) := \\mathbb{E}[\\tilde{x}_t^2]$. We can derive an ordinary differential equation (ODE) for $P(t)$. Applying Itô's lemma to $f(\\tilde{x}_t) = \\tilde{x}_t^2$:\n$$\n\\mathrm{d}(\\tilde{x}_t^2) = 2\\tilde{x}_t\\,\\mathrm{d}\\tilde{x}_t + \\frac{1}{2}(2)(\\mathrm{d}\\tilde{x}_t)^2 = 2\\tilde{x}_t\\,\\mathrm{d}\\tilde{x}_t + (\\mathrm{d}\\tilde{x}_t)^2\n$$\nThe quadratic variation term $(\\mathrm{d}\\tilde{x}_t)^2$ is:\n$$\n(\\mathrm{d}\\tilde{x}_t)^2 = (\\sigma\\,\\mathrm{d}W_t - K\\gamma\\,\\mathrm{d}V_t)^2 = \\sigma^2(\\mathrm{d}W_t)^2 - 2\\sigma K\\gamma\\,\\mathrm{d}W_t\\mathrm{d}V_t + (K\\gamma)^2(\\mathrm{d}V_t)^2\n$$\nUsing the Itô rules $(\\mathrm{d}W_t)^2=\\mathrm{d}t$, $(\\mathrm{d}V_t)^2=\\mathrm{d}t$, and $\\mathrm{d}W_t\\mathrm{d}V_t = 0$ (due to independence of $W_t$ and $V_t$), we have:\n$$\n(\\mathrm{d}\\tilde{x}_t)^2 = (\\sigma^2 + K^2\\gamma^2)\\,\\mathrm{d}t\n$$\nSubstituting $\\mathrm{d}\\tilde{x}_t$ and $(\\mathrm{d}\\tilde{x}_t)^2$ into the expression for $\\mathrm{d}(\\tilde{x}_t^2)$:\n$$\n\\mathrm{d}(\\tilde{x}_t^2) = 2\\tilde{x}_t((a-Kc)\\tilde{x}_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t - K\\gamma\\,\\mathrm{d}V_t) + (\\sigma^2 + K^2\\gamma^2)\\,\\mathrm{d}t\n$$\n$$\n\\mathrm{d}(\\tilde{x}_t^2) = (2(a-Kc)\\tilde{x}_t^2 + \\sigma^2 + K^2\\gamma^2)\\,\\mathrm{d}t + 2\\sigma\\tilde{x}_t\\,\\mathrm{d}W_t - 2K\\gamma\\tilde{x}_t\\,\\mathrm{d}V_t\n$$\nTaking the expectation of both sides, we note that the expectations of the Itô integral terms are zero. This gives an ODE for $P(t) = \\mathbb{E}[\\tilde{x}_t^2]$:\n$$\n\\frac{\\mathrm{d}P(t)}{\\mathrm{d}t} = 2(a-Kc)P(t) + \\sigma^2 + K^2\\gamma^2\n$$\nThis is a linear first-order ODE with the initial condition $P(0)=P_0$. Its solution is:\n$$\nP(t) = P_0\\exp(2(a-Kc)t) + \\frac{\\sigma^2 + K^2\\gamma^2}{2(a-Kc)}(\\exp(2(a-Kc)t)-1)\n$$\nTo find the long-time limit $P_{\\infty} = \\lim_{t\\to\\infty} P(t)$, we use the stability condition $a-Kc < 0$. Under this condition, $\\exp(2(a-Kc)t) \\to 0$ as $t \\to \\infty$. Therefore:\n$$\nP_{\\infty} = 0 + \\frac{\\sigma^2 + K^2\\gamma^2}{2(a-Kc)}(0-1) = - \\frac{\\sigma^2 + K^2\\gamma^2}{2(a-Kc)}\n$$\n$$\nP_{\\infty} = \\frac{\\sigma^2 + K^2\\gamma^2}{2(Kc-a)}\n$$\n\n**Part 3: Derivation of the final expression for $P_{\\infty}$**\n\nWe are given the steady-state relationship $K = \\frac{c}{\\gamma^2}P_{\\infty}$. We substitute this into our expression for $P_{\\infty}$:\n$$\nP_{\\infty} = \\frac{\\sigma^2 + \\left(\\frac{c}{\\gamma^2}P_{\\infty}\\right)^2\\gamma^2}{2\\left(\\left(\\frac{c}{\\gamma^2}P_{\\infty}\\right)c-a\\right)} = \\frac{\\sigma^2 + \\frac{c^2}{\\gamma^2}P_{\\infty}^2}{2\\frac{c^2}{\\gamma^2}P_{\\infty}-2a}\n$$\nMultiplying both sides by the denominator gives:\n$$\nP_{\\infty}\\left(2\\frac{c^2}{\\gamma^2}P_{\\infty}-2a\\right) = \\sigma^2 + \\frac{c^2}{\\gamma^2}P_{\\infty}^2\n$$\n$$\n2\\frac{c^2}{\\gamma^2}P_{\\infty}^2 - 2aP_{\\infty} = \\sigma^2 + \\frac{c^2}{\\gamma^2}P_{\\infty}^2\n$$\nRearranging the terms, we obtain a quadratic equation for $P_{\\infty}$:\n$$\n\\frac{c^2}{\\gamma^2}P_{\\infty}^2 - 2aP_{\\infty} - \\sigma^2 = 0\n$$\nThis equation is the algebraic Riccati equation (ARE) for this scalar system. The general continuous-time ARE is $AP + PA^T - PC^T(DD^T)^{-1}CP + BB^T = 0$. For our scalar case, $A=a$, $B= \\sigma$, $C=c$, $D=\\gamma$, and $P=P_\\infty$. The ARE becomes $aP_\\infty + P_\\infty a - P_\\infty c(\\gamma^2)^{-1}c P_\\infty + \\sigma^2 = 0$, which simplifies to $2aP_\\infty - \\frac{c^2}{\\gamma^2}P_\\infty^2 + \\sigma^2 = 0$, equivalent to the equation we derived.\n\nWe solve this quadratic equation for $P_{\\infty}$ using the quadratic formula $X = \\frac{-B \\pm \\sqrt{B^2-4AC}}{2A}$, with $X=P_{\\infty}$, $A = \\frac{c^2}{\\gamma^2}$, $B = -2a$, and $C = -\\sigma^2$:\n$$\nP_{\\infty} = \\frac{-(-2a) \\pm \\sqrt{(-2a)^2 - 4\\left(\\frac{c^2}{\\gamma^2}\\right)(-\\sigma^2)}}{2\\left(\\frac{c^2}{\\gamma^2}\\right)} = \\frac{2a \\pm \\sqrt{4a^2 + 4\\frac{c^2\\sigma^2}{\\gamma^2}}}{\\frac{2c^2}{\\gamma^2}}\n$$\n$$\nP_{\\infty} = \\frac{2a \\pm 2\\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}}}{\\frac{2c^2}{\\gamma^2}} = \\frac{\\gamma^2}{c^2}\\left(a \\pm \\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}}\\right)\n$$\nSince $P_{\\infty}$ represents variance, it must be non-negative. We analyze the two solutions. The term under the square root is strictly positive, so $\\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}} > \\sqrt{a^2} = |a|$.\nThe solution with the minus sign, $a - \\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}}$, is always negative.\nThe solution with the plus sign, $a + \\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}}$, is always positive because $\\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}} > |a| \\ge -a$.\nTherefore, we must choose the positive root:\n$$\nP_{\\infty} = \\frac{\\gamma^2}{c^2}\\left(a + \\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}}\\right)\n$$\nThis is the final closed-form expression for the steady-state error variance.", "answer": "$$\n\\boxed{\\frac{\\gamma^2}{c^2}\\left(a + \\sqrt{a^2 + \\frac{c^2\\sigma^2}{\\gamma^2}}\\right)}\n$$", "id": "3080857"}, {"introduction": "A key skill for any scientist or engineer is developing intuition for a model's behavior under different conditions. This exercise [@problem_id:3080842] explores the filter's performance in a \"poor signal-to-noise\" scenario, where observation noise is high. By analyzing how the steady-state filter gain and innovations variance scale with increasing noise, you will gain a practical understanding of how the filter intelligently weighs new, unreliable information.", "problem": "Consider the scalar linear stochastic differential equation (SDE) state-observation model\n$$\n\\mathrm{d}X_t \\;=\\; g\\,\\mathrm{d}W_t,\\qquad \\mathrm{d}Y_t \\;=\\; h\\,X_t\\,\\mathrm{d}t \\;+\\; \\sqrt{R}\\,\\mathrm{d}V_t,\n$$\nwhere $X_t\\in\\mathbb{R}$ is the state, $Y_t\\in\\mathbb{R}$ is the observation, $g>0$ and $h\\neq 0$ are constants, $R>0$ is the measurement noise intensity, and $W_t$ and $V_t$ are independent standard Wiener processes. Assume the optimal continuous-time linear minimum-variance filter (Kalman–Bucy filter) is used, and suppose a steady-state error variance exists. Define the innovations process by\n$$\n\\mathrm{d}\\nu_t \\;=\\; \\mathrm{d}Y_t \\;-\\; h\\,\\widehat{X}_t\\,\\mathrm{d}t,\n$$\nwhere $\\widehat{X}_t$ is the state estimate produced by the filter. In the regime of poor signal-to-noise ratio (SNR) in the observation, interpreted as $R$ large relative to the process noise intensity $g^2$ (equivalently $R\\gg g^2$), which of the following statements best characterizes how the steady-state Kalman gain magnitude and the instantaneous variance of the innovations respond as $R$ increases?\n\nA. As $R$ increases, the steady-state Kalman gain magnitude decreases proportionally to $R^{-1/2}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=R\\,\\mathrm{d}t$, so it increases linearly with $R$.\n\nB. As $R$ increases, the steady-state Kalman gain magnitude increases proportionally to $R^{1/2}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=R\\,\\mathrm{d}t$, so it increases linearly with $R$.\n\nC. As $R$ increases, the steady-state Kalman gain magnitude decreases proportionally to $R^{-1}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=\\mathrm{d}t$, independent of $R$ due to whitening.\n\nD. As $R$ increases, the steady-state Kalman gain magnitude decreases proportionally to $R^{-1/2}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=(h^2 P + R)\\,\\mathrm{d}t$, where $P$ is the steady-state error variance, so it increases faster than linearly because $P$ also increases with $R$.\n\nSelect the single best answer. State your reasoning based on first principles of the innovations approach and continuous-time filtering, not on memorized conclusions.", "solution": "The user has provided a problem in the domain of stochastic differential equations and optimal filtering. I will first validate the problem statement and then proceed to a full solution.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- State equation: $\\mathrm{d}X_t = g\\,\\mathrm{d}W_t$\n- Observation equation: $\\mathrm{d}Y_t = h\\,X_t\\,\\mathrm{d}t + \\sqrt{R}\\,\\mathrm{d}V_t$\n- $X_t \\in \\mathbb{R}$ is the state, $Y_t \\in \\mathbb{R}$ is the observation.\n- $g > 0$, $h \\neq 0$, and $R>0$ are constants.\n- $W_t$ and $V_t$ are independent standard Wiener processes.\n- An optimal continuous-time linear minimum-variance filter (Kalman–Bucy filter) is used.\n- A steady-state error variance is assumed to exist.\n- The innovations process is defined as $\\mathrm{d}\\nu_t = \\mathrm{d}Y_t - h\\,\\widehat{X}_t\\,\\mathrm{d}t$, where $\\widehat{X}_t$ is the state estimate.\n- The regime of interest is the poor signal-to-noise ratio (SNR) case, where $R$ is large relative to $g^2$, which can be written as $R \\gg g^2$.\n- The question asks to characterize the behavior of the steady-state Kalman gain magnitude and the instantaneous variance of the innovations as $R$ increases.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard application of the Kalman-Bucy filter to a simple linear time-invariant system. This is a canonical problem in stochastic control and estimation theory, firmly based on established mathematical principles. The model represents a random walk process being observed with noise.\n2.  **Well-Posed**: The system is fully specified by linear stochastic differential equations. The parameters are defined, and the existence of a steady state is posited, which is a standard assumption that can be verified for this system. The question asks for a characterization of specific, well-defined quantities in the limit of a parameter, which is a meaningful mathematical question.\n3.  **Objective**: The problem is stated in precise, mathematical language. The term \"poor SNR\" is objectively defined as $R \\gg g^2$. There are no subjective or ambiguous statements.\n\nThe problem statement is scientifically sound, well-posed, and objective. It contains no inconsistencies, missing information, or other flaws listed in the validation criteria.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed to derive the solution.\n\n### Derivation\nThe general continuous-time linear system is given by:\n$$\n\\mathrm{d}X_t = A X_t\\,\\mathrm{d}t + G\\,\\mathrm{d}W'_t \\\\\n\\mathrm{d}Y_t = H X_t\\,\\mathrm{d}t + \\sqrt{R_{obs}}\\,\\mathrm{d}V_t\n$$\nwhere the process noise has covariance $Q = GG^T$. The Kalman-Bucy filter for this system involves the error variance $P_t = \\mathbb{E}[(X_t - \\widehat{X}_t)(X_t - \\widehat{X}_t)^T]$, which evolves according to the continuous-time differential Riccati equation:\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = A P_t + P_t A^T + Q - P_t H^T R_{obs}^{-1} H P_t\n$$\nThe Kalman gain is given by $K_t = P_t H^T R_{obs}^{-1}$.\n\nFor the specific problem given:\n- The state and observation are scalars.\n- The state dynamics matrix is $A=0$.\n- The process noise input matrix is $g$, so the process noise covariance is $Q = g g^T = g^2$.\n- The observation matrix is $H=h$.\n- The observation noise intensity is $R_{obs}=R$.\n\nSubstituting these into the Riccati equation gives the equation for the error variance $P_t$:\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = (0)P_t + P_t(0)^T + g^2 - P_t (h)^T (R)^{-1} (h) P_t \\\\\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = g^2 - \\frac{h^2}{R} P_t^2\n$$\nThe problem assumes a steady-state error variance exists. Let this be $P$. In steady-state, $\\frac{\\mathrm{d}P}{\\mathrm{d}t} = 0$. This yields the algebraic Riccati equation:\n$$\n0 = g^2 - \\frac{h^2}{R} P^2\n$$\nSolving for $P$:\n$$\n\\frac{h^2}{R} P^2 = g^2 \\implies P^2 = \\frac{g^2 R}{h^2}\n$$\nSince variance must be non-negative ($P \\ge 0$), and we are given $g>0$:\n$$\nP = \\sqrt{\\frac{g^2 R}{h^2}} = \\frac{g \\sqrt{R}}{|h|}\n$$\nNow, we find the steady-state Kalman gain, $K$.\n$$\nK = P H^T R_{obs}^{-1} = P h R^{-1}\n$$\nSubstituting the expression for the steady-state variance $P$:\n$$\nK = \\left( \\frac{g \\sqrt{R}}{|h|} \\right) \\frac{h}{R} = \\frac{gh}{|h|} \\frac{\\sqrt{R}}{R} = \\frac{gh}{|h|} \\frac{1}{\\sqrt{R}}\n$$\nThe magnitude of the steady-state Kalman gain is:\n$$\n|K| = \\left| \\frac{gh}{|h|} \\frac{1}{\\sqrt{R}} \\right| = \\frac{g|h|}{|h|} \\frac{1}{\\sqrt{R}} = \\frac{g}{\\sqrt{R}}\n$$\nSince $g$ is a positive constant, the gain magnitude $|K|$ is proportional to $R^{-1/2}$. As the measurement noise intensity $R$ increases, the gain decreases.\n\nNext, we analyze the innovations process, $\\mathrm{d}\\nu_t$.\n$$\n\\mathrm{d}\\nu_t = \\mathrm{d}Y_t - h\\,\\widehat{X}_t\\,\\mathrm{d}t\n$$\nSubstitute the expression for $\\mathrm{d}Y_t$:\n$$\n\\mathrm{d}\\nu_t = (h\\,X_t\\,\\mathrm{d}t + \\sqrt{R}\\,\\mathrm{d}V_t) - h\\,\\widehat{X}_t\\,\\mathrm{d}t\n$$\nRearranging terms, we get:\n$$\n\\mathrm{d}\\nu_t = h(X_t - \\widehat{X}_t)\\,\\mathrm{d}t + \\sqrt{R}\\,\\mathrm{d}V_t\n$$\nThe term $X_t - \\widehat{X}_t$ is the estimation error, $\\tilde{X}_t$. The equation is an Ito process of the form $\\mathrm{d}Z_t = \\mu_t \\mathrm{d}t + \\sigma_t \\mathrm{d}B_t$. The instantaneous variance (or quadratic variation) is given by $(\\mathrm{d}Z_t)^2 = \\sigma_t^2 (\\mathrm{d}B_t)^2 = \\sigma_t^2 \\mathrm{d}t$. The drift term $\\mu_t \\mathrm{d}t$ does not contribute to the quadratic variation in Ito calculus.\nFor the innovations process $\\mathrm{d}\\nu_t$, the drift term is $\\mu_t = h(X_t - \\widehat{X}_t)$ and the diffusion term is $\\sigma_t \\mathrm{d}B_t = \\sqrt{R}\\,\\mathrm{d}V_t$.\nTherefore, the quadratic variation is:\n$$\n(\\mathrm{d}\\nu_t)^2 = (\\sqrt{R}\\,\\mathrm{d}V_t)^2 = R\\,(\\mathrm{d}V_t)^2 = R\\,\\mathrm{d}t\n$$\nThe quantity $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]$ represents this very incremental variance of the process. So, $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right] = R\\,\\mathrm{d}t$.\nThis implies the innovations variance rate is $R$, which increases linearly with $R$. This is a fundamental result from filtering theory: the innovations process is a Wiener process with the same covariance as the observation noise.\n\n### Option-by-Option Analysis\n\n**A. As $R$ increases, the steady-state Kalman gain magnitude decreases proportionally to $R^{-1/2}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=R\\,\\mathrm{d}t$, so it increases linearly with $R$.**\n- **Kalman Gain:** Our derivation shows $|K| = g/\\sqrt{R}$, which is proportional to $R^{-1/2}$. This statement is correct.\n- **Innovations Variance:** Our derivation shows $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right] = R\\,\\mathrm{d}t$. The instantaneous variance is $R$, which increases linearly with $R$. This statement is also correct.\n- **Verdict:** Correct.\n\n**B. As $R$ increases, the steady-state Kalman gain magnitude increases proportionally to $R^{1/2}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=R\\,\\mathrm{d}t$, so it increases linearly with $R$.**\n- **Kalman Gain:** This claims the gain magnitude increases with $R$. Our derivation shows it decreases. This statement is incorrect.\n- **Innovations Variance:** The statement about the innovations variance is correct.\n- **Verdict:** Incorrect.\n\n**C. As $R$ increases, the steady-state Kalman gain magnitude decreases proportionally to $R^{-1}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=\\mathrm{d}t$, independent of $R$ due to whitening.**\n- **Kalman Gain:** This claims the gain magnitude is proportional to $R^{-1}$. Our derivation shows it is proportional to $R^{-1/2}$. This statement is incorrect.\n- **Innovations Variance:** This claims the innovations variance rate is $1$, independent of $R$. Our derivation shows the rate is $R$. This would only be true for the *normalized* innovations process $\\mathrm{d}\\tilde{\\nu}_t = (1/\\sqrt{R})\\mathrm{d}\\nu_t$. The statement is incorrect for the innovations process $\\mathrm{d}\\nu_t$ itself.\n- **Verdict:** Incorrect.\n\n**D. As $R$ increases, the steady-state Kalman gain magnitude decreases proportionally to $R^{-1/2}$, and the innovations variance satisfies $\\mathbb{E}\\!\\left[(\\mathrm{d}\\nu_t)^2\\mid\\mathcal{F}_t\\right]=(h^2 P + R)\\,\\mathrm{d}t$, where $P$ is the steady-state error variance, so it increases faster than linearly because $P$ also increases with $R$.**\n- **Kalman Gain:** The statement about the gain decreasing proportionally to $R^{-1/2}$ is correct.\n- **Innovations Variance:** This proposes the formula $(h^2 P + R)$ for the instantaneous variance. This formula is dimensionally inconsistent. The units of $R$ are $[Y]^2/[T]$, while the units of $h^2 P$ are $([Y]/([X][T]))^2 [X]^2 = [Y]^2/[T]^2$. These cannot be added. This formula is a misapplication of the discrete-time innovations covariance formula to the continuous-time case. Furthermore, even if the formula were correct, the sum $h^2 P + R = |h|g\\sqrt{R} + R$ is asymptotically linear in $R$ for large $R$, so the claim that it \"increases faster than linearly\" is misleading. The expression for the variance is incorrect.\n- **Verdict:** Incorrect.\n\nBased on the analysis, only option A is fully consistent with the principles of continuous-time stochastic filtering.", "answer": "$$\\boxed{A}$$", "id": "3080842"}, {"introduction": "The innovations process is not just a theoretical concept; it is a powerful diagnostic tool for model validation. The theory states that if the filter model is correct, the resulting innovations sequence will be white noise. This coding practice [@problem_id:3080859] challenges you to verify this principle by implementing a filter, generating innovations from simulated data, and performing a statistical whiteness test to detect potential model misspecification.", "problem": "Consider a continuous-time linear Gaussian system modeled by the stochastic differential equations (SDEs) for the state and observation processes, respectively, given by $$d x_t = a x_t \\, dt + \\sigma \\, d W_t$$ and $$d y_t = c \\, x_t \\, dt + \\sqrt{r} \\, d V_t,$$ where $x_t$ is the unobserved state, $y_t$ is the observed process, $a \\in \\mathbb{R}$ is the drift coefficient, $\\sigma > 0$ is the diffusion coefficient, $c \\in \\mathbb{R}$ is the observation gain, $r > 0$ is the observation noise variance, and $W_t$ and $V_t$ are independent standard Wiener processes. In the innovations approach to filtering, the innovation process $d \\nu_t$ is defined as the difference between the observation increment and its filter-based predicted drift. Under correct model specification and the Kalman–Bucy filter, $d \\nu_t$ is a transformed Wiener increment, which is white in the sense of having zero autocorrelation at nonzero lags.\n\nYour task is to empirically test the whiteness of innovations by discretizing time and computing sample autocorrelations of a discretized innovation sequence. Use a uniform grid $t_k = k \\Delta t$ for $k = 0,1,\\dots,N$ with $\\Delta t > 0$ and $N = \\lfloor T / \\Delta t \\rfloor$. Let $\\hat{x}_t$ denote the filter estimate of $x_t$ constructed from the observations $y_t$ and the model parameters used by the filter, which may be misspecified. Define the discretized innovations sequence $\\{\\nu_k\\}_{k=1}^N$ by $$\\nu_k = \\big(y_{t_k} - y_{t_{k-1}}\\big) - c_f \\, \\hat{x}_{t_{k-1}} \\, \\Delta t,$$ where $c_f$ is the observation gain used by the filter. Implement the standard continuous-time linear Gaussian filter (Kalman–Bucy filter) using Euler discretization to produce $\\hat{x}_{t_k}$ and its associated covariance. Then conduct a whiteness test by computing the sample autocorrelations of $\\nu_k$ at lags $\\ell = 1,2,\\dots,L$ using $$\\hat{\\rho}(\\ell) = \\frac{\\sum_{k=1}^{N-\\ell} \\big(\\nu_k - \\bar{\\nu}\\big)\\big(\\nu_{k+\\ell} - \\bar{\\nu}\\big)}{\\sum_{k=1}^{N} \\big(\\nu_k - \\bar{\\nu}\\big)^2},$$ where $\\bar{\\nu} = \\frac{1}{N} \\sum_{k=1}^{N} \\nu_k$. Under the null hypothesis of whiteness and for large $N$, the sample autocorrelations $\\hat{\\rho}(\\ell)$ are approximately independent and normally distributed with mean $0$ and variance $1/N$. Use a two-sided significance level $\\alpha \\in (0,1)$ and accept whiteness if $$\\max_{1 \\le \\ell \\le L} \\big|\\hat{\\rho}(\\ell)\\big| \\le \\frac{z_{1-\\alpha/2}}{\\sqrt{N}},$$ where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution.\n\nYou must write a complete, runnable program that:\n- Simulates the state $x_t$ and observation $y_t$ using Euler–Maruyama discretization for the SDEs.\n- Implements the Kalman–Bucy filter in one dimension via Euler discretization to obtain $\\hat{x}_{t_k}$ and covariance $P_{t_k}$.\n- Computes the discretized innovation sequence $\\{\\nu_k\\}_{k=1}^N$.\n- Computes the sample autocorrelations $\\hat{\\rho}(\\ell)$ for $\\ell = 1, \\dots, L$.\n- Performs the whiteness test using the criterion above and returns a boolean for each test case indicating whether whiteness is accepted (True) or rejected (False) at the specified $\\alpha$.\n\nThere are no physical units involved in this problem. Use radians for any angles if applicable, although no angles are expected here. Express any proportions as decimals.\n\nImplement your program to run the following test suite. Each test case provides the true system parameters $(a, \\sigma, c, r)$, the filter parameters $(a_f, c_f)$, the discretization parameters $(\\Delta t, T)$, the initial conditions $(x_0, P_0)$, the whiteness test parameters $(L, \\alpha)$, and a random seed for reproducibility. For each case, output a boolean indicating whether whiteness is accepted.\n\nTest Suite:\n1. Happy path (correct model, fine discretization): $a = -0.6$, $\\sigma = 0.7$, $c = 1.0$, $r = 0.4$; $a_f = -0.6$, $c_f = 1.0$; $\\Delta t = 0.001$, $T = 3.0$; $x_0 = 0.0$, $P_0 = 1.0$; $L = 10$, $\\alpha = 0.05$; seed $= 42$.\n2. Misspecified observation gain (should violate whiteness): $a = -0.6$, $\\sigma = 0.7$, $c = 1.0$, $r = 0.1$; $a_f = -0.6$, $c_f = 0.0$; $\\Delta t = 0.01$, $T = 10.0$; $x_0 = 0.0$, $P_0 = 1.0$; $L = 10$, $\\alpha = 0.05$; seed $= 123$.\n3. Edge case with coarse discretization (discretization error may violate whiteness even under correct model): $a = -0.6$, $\\sigma = 0.7$, $c = 1.0$, $r = 0.4$; $a_f = -0.6$, $c_f = 1.0$; $\\Delta t = 0.05$, $T = 5.0$; $x_0 = 0.0$, $P_0 = 1.0$; $L = 10$, $\\alpha = 0.05$; seed $= 7$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the boolean results for the three test cases as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\"). The program must be self-contained, require no user input, and strictly adhere to the specified runtime environment and libraries.", "solution": "The user-provided problem requires an empirical validation of the whiteness property of the innovation process in a continuous-time linear Gaussian filtering problem. This is to be achieved by simulating the system, applying a discretized Kalman-Bucy filter, computing the resulting discretized innovations, and performing a statistical test based on their sample autocorrelations.\n\nThe solution is developed in four stages:\n1.  Discretized simulation of the true state and observation processes.\n2.  Implementation of the discretized continuous-time Kalman-Bucy filter.\n3.  Computation of the discretized innovation sequence.\n4.  Statistical whiteness test based on sample autocorrelations.\n\nThe system is described by the following stochastic differential equations (SDEs):\n- State process: $d x_t = a x_t \\, dt + \\sigma \\, d W_t$\n- Observation process: $d y_t = c \\, x_t \\, dt + \\sqrt{r} \\, d V_t$\n\nHere, $x_t$ is the latent state, $y_t$ is the observation, $W_t$ and $V_t$ are independent standard Wiener processes, and $(a, \\sigma, c, r)$ are the true system parameters.\n\nThe simulation and filtering are performed over a discrete time grid $t_k = k \\Delta t$ for $k = 0, 1, \\dots, N$, where $N = \\lfloor T / \\Delta t \\rfloor$.\n\n**1. System Simulation via Euler-Maruyama Discretization**\n\nThe continuous-time SDEs are simulated using the Euler-Maruyama method. For each time step $\\Delta t$, the state $x_{t_k}$ and the observation increment $\\Delta y_k = y_{t_k} - y_{t_{k-1}}$ are approximated as:\n$$x_{t_k} = x_{t_{k-1}} + a x_{t_{k-1}} \\Delta t + \\sigma \\sqrt{\\Delta t} Z_k$$\n$$\\Delta y_k = c x_{t_{k-1}} \\Delta t + \\sqrt{r \\Delta t} Q_k$$\nwhere $Z_k$ and $Q_k$ are independent random variables drawn from a standard normal distribution $\\mathcal{N}(0, 1)$. The simulation starts from the given initial state $x_{t_0} = x_0$.\n\n**2. Discretized Kalman-Bucy Filter**\n\nThe Kalman-Bucy filter provides the optimal estimate $\\hat{x}_t$ of the state $x_t$ given the history of observations. The filter's dynamics are governed by a set of parameters $(a_f, c_f, \\sigma, r)$, where $a_f$ and $c_f$ may be misspecified (i.e., $a_f \\neq a$ or $c_f \\neq c$). The filter equations for the state estimate $\\hat{x}_t$ and its error covariance $P_t = \\mathbb{E}[(x_t - \\hat{x}_t)^2]$ are:\n$$d\\hat{x}_t = a_f \\hat{x}_t dt + K_t (dy_t - c_f \\hat{x}_t dt)$$\n$$\\frac{dP_t}{dt} = 2 a_f P_t + \\sigma^2 - \\frac{P_t^2 c_f^2}{r}$$\nThe term $K_t = P_t c_f / r$ is the Kalman gain. The second equation is the continuous-time Riccati differential equation.\n\nThese continuous-time equations are discretized using the forward Euler method. Given the estimate $\\hat{x}_{t_{k-1}}$ and covariance $P_{t_{k-1}}$ at time $t_{k-1}$, the values at time $t_k$ are computed as follows:\n\nFirst, the Kalman gain at $t_{k-1}$ is computed:\n$$K_{t_{k-1}} = \\frac{P_{t_{k-1}} c_f}{r}$$\n\nThe discretized innovation $\\nu_k$ for the interval $(t_{k-1}, t_k]$ is defined as the difference between the observed increment $\\Delta y_k$ and its predicted value based on the filter's state at $t_{k-1}$:\n$$\\nu_k = \\Delta y_k - c_f \\hat{x}_{t_{k-1}} \\Delta t$$\n\nThe state estimate is then updated using this innovation:\n$$\\hat{x}_{t_k} = \\hat{x}_{t_{k-1}} + a_f \\hat{x}_{t_{k-1}} \\Delta t + K_{t_{k-1}} \\nu_k$$\n\nFinally, the covariance is updated by discretizing the Riccati equation:\n$$P_{t_k} = P_{t_{k-1}} + \\left( 2 a_f P_{t_{k-1}} + \\sigma^2 - \\frac{P_{t_{k-1}}^2 c_f^2}{r} \\right) \\Delta t$$\nThe filter is initialized with $\\hat{x}_{t_0} = x_0$ and $P_{t_0} = P_0$.\n\n**3. Innovation Whiteness and Statistical Testing**\n\nA key property of the Kalman filter is that if the model parameters are correctly specified ($a_f=a$, $c_f=c$), the innovation process is \"white,\" meaning it is a sequence of uncorrelated random variables. Model misspecification typically introduces correlations into the innovations.\n\nTo test this property, we compute the sample autocorrelations of the generated innovation sequence $\\{\\nu_k\\}_{k=1}^N$. The sample autocorrelation at lag $\\ell$ is given by:\n$$\\hat{\\rho}(\\ell) = \\frac{\\sum_{k=1}^{N-\\ell} \\big(\\nu_k - \\bar{\\nu}\\big)\\big(\\nu_{k+\\ell} - \\bar{\\nu}\\big)}{\\sum_{k=1}^{N} \\big(\\nu_k - \\bar{\\nu}\\big)^2}$$\n\nUnder the null hypothesis of whiteness and for a large number of samples $N$, each $\\hat{\\rho}(\\ell)$ is approximately distributed as $\\mathcal{N}(0, 1/N)$. The specified statistical test rejects the whiteness hypothesis if the maximum absolute autocorrelation over lags $\\ell=1, \\dots, L$ exceeds a critical threshold:\n$$\\max_{1 \\le \\ell \\le L} \\big|\\hat{\\rho}(\\ell)\\big| > \\frac{z_{1-\\alpha/2}}{\\sqrt{N}}$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution and $\\alpha$ is the significance level. If the maximum absolute autocorrelation is within this bound, whiteness is accepted.\n\nThe overall algorithm proceeds by initializing the system and filter, looping through time steps to simulate the path and apply the filter, storing the innovations, and finally computing the autocorrelations to perform the whiteness test. This is repeated for each test case specified in the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_whiteness_test(true_params, filter_params, sim_params, init_conds, test_params, seed):\n    \"\"\"\n    Simulates a linear Gaussian system, applies a Kalman-Bucy filter,\n    and performs a whiteness test on the innovations.\n    \"\"\"\n    # Unpack parameters\n    a, sigma, c, r = true_params\n    a_f, c_f = filter_params\n    dt, T = sim_params\n    x0, P0 = init_conds\n    L, alpha = test_params\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Simulation setup\n    N = int(T / dt)\n    \n    # State and filter variable arrays\n    x = np.zeros(N + 1)\n    x_hat = np.zeros(N + 1)\n    P = np.zeros(N + 1)\n    innovations = np.zeros(N)\n\n    # Initial conditions\n    x[0] = x0\n    x_hat[0] = x0\n    P[0] = P0\n\n    # Pre-generate random shocks for efficiency\n    Z = np.random.normal(0, 1, N)\n    Q = np.random.normal(0, 1, N)\n\n    # Main simulation and filtering loop\n    for k in range(1, N + 1):\n        # Time indices: k-1 is previous, k is current\n        \n        # 1. Simulate the true system state and observation increment\n        # Euler-Maruyama for the state SDE\n        x[k] = x[k-1] + a * x[k-1] * dt + sigma * np.sqrt(dt) * Z[k-1]\n        \n        # Observation increment\n        delta_y_k = c * x[k-1] * dt + np.sqrt(r * dt) * Q[k-1]\n        \n        # 2. Apply the discretized Kalman-Bucy filter\n        # Compute the discretized innovation (based on state at t_{k-1})\n        nu_k = delta_y_k - c_f * x_hat[k-1] * dt\n        innovations[k-1] = nu_k  # Store in 0-indexed array\n\n        # Compute Kalman gain (based on covariance at t_{k-1})\n        # Avoid division by zero if r is zero, though problem states r > 0.\n        K_km1 = P[k-1] * c_f / r if r != 0 else 0.0\n\n        # Update state estimate\n        x_hat[k] = x_hat[k-1] + a_f * x_hat[k-1] * dt + K_km1 * nu_k\n\n        # Update covariance via discretized Riccati equation\n        P_dot_km1 = 2 * a_f * P[k-1] + sigma**2 - (P[k-1]**2 * c_f**2) / r if r != 0 else 2 * a_f * P[k-1] + sigma**2\n        P[k] = P[k-1] + P_dot_km1 * dt\n        \n        # Ensure covariance remains non-negative\n        if P[k] < 0:\n            P[k] = 0.0\n\n    # 3. Perform the whiteness test on the innovations sequence\n    nu = innovations\n    nu_bar = np.mean(nu)\n\n    # Denominator for autocorrelation calculation\n    denom = np.sum((nu - nu_bar)**2)\n    if denom == 0:\n        # If innovations have zero variance, they are perfectly predictable.\n        # Autocorrelations are undefined. We can consider this as non-white\n        # unless L=0. Since L>=1, we can return False.\n        # Or, if they are all zero, there is no signal, so correlation is vacuously zero.\n        # This case is unlikely given the problem setup but is handled for robustness.\n        # Whiteness is arguably accepted in this case.\n        return True\n\n    max_abs_rho = 0.0\n    for l in range(1, L + 1):\n        nu_centered = nu - nu_bar\n        # Sum of (nu_k - nu_bar) * (nu_{k+l} - nu_bar) for k=0...N-l-1\n        num = np.sum(nu_centered[:N-l] * nu_centered[l:])\n        rho_l = num / denom\n        if abs(rho_l) > max_abs_rho:\n            max_abs_rho = abs(rho_l)\n            \n    # Calculate the statistical threshold\n    z_quantile = norm.ppf(1 - alpha / 2)\n    threshold = z_quantile / np.sqrt(N)\n    \n    # Return True if whiteness is accepted, False otherwise\n    return max_abs_rho <= threshold\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Happy path (correct model)\n        {\n            \"true_params\": (-0.6, 0.7, 1.0, 0.4),\n            \"filter_params\": (-0.6, 1.0),\n            \"sim_params\": (0.001, 3.0),\n            \"init_conds\": (0.0, 1.0),\n            \"test_params\": (10, 0.05),\n            \"seed\": 42\n        },\n        # 2. Misspecified observation gain\n        {\n            \"true_params\": (-0.6, 0.7, 1.0, 0.1),\n            \"filter_params\": (-0.6, 0.0),\n            \"sim_params\": (0.01, 10.0),\n            \"init_conds\": (0.0, 1.0),\n            \"test_params\": (10, 0.05),\n            \"seed\": 123\n        },\n        # 3. Coarse discretization\n        {\n            \"true_params\": (-0.6, 0.7, 1.0, 0.4),\n            \"filter_params\": (-0.6, 1.0),\n            \"sim_params\": (0.05, 5.0),\n            \"init_conds\": (0.0, 1.0),\n            \"test_params\": (10, 0.05),\n            \"seed\": 7\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_whiteness_test(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3080859"}]}