{"hands_on_practices": [{"introduction": "The Kalman-Bucy filter's performance is fundamentally determined by the evolution of the estimation error covariance, $P_t$. This evolution is described by the famous Riccati differential equation. To truly grasp how the filter optimizes its estimates, it is essential to understand this equation's origins and behavior. This practice guides you through a first-principles derivation and analytical solution for a scalar system, building a solid mathematical foundation for the filter's mechanics [@problem_id:3080944].", "problem": "Consider the scalar, linear, time-invariant continuous-time model driven by independent standard Brownian motions,\n$$\n\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t,\\qquad\n\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t,\n$$\nwhere $a\\in\\mathbb{R}$, $c\\in\\mathbb{R}$, $q\\ge 0$, $r>0$ are given constants, and $W_t$ and $V_t$ are independent standard Wiener processes. Let $\\hat{x}_t$ denote the estimate produced by the Kalman-Bucy filter (KBF), and define the estimation error $e_t = x_t - \\hat{x}_t$ with error variance $P_t = \\mathbb{E}[e_t^2]$. Assume an initial variance $P_0 \\ge 0$.\n\nStarting from the stochastic differential equation (SDE) model and the definition of $e_t$, derive from first principles (without quoting pre-packaged filter formulas) the ordinary differential equation (ODE) governing $P_t$ for the Kalman-Bucy filter. Then, solve this scalar ODE explicitly for $P_t$ with initial condition $P_0$. Next, determine the precise conditions on $a$, $c$, $q$, $r$, and $P_0$ under which $P_t$ converges to a finite steady-state value $P_{\\infty} = \\lim_{t\\to\\infty} P_t$, and give a closed-form expression for $P_{\\infty}$.\n\nFinally, for the numerical values $a=0.4$, $c=1.5$, $q=0.2$, $r=0.5$, and $P_0=1.0$, evaluate the steady-state value $P_{\\infty}$ numerically. Round your answer to four significant figures. No units are required in the final answer.", "solution": "The problem asks for the derivation of the variance dynamics for a scalar Kalman-Bucy filter, the solution to the resulting differential equation, the conditions for steady-state convergence, and a numerical evaluation of the steady-state variance.\n\nThe process begins with the given linear time-invariant system:\nState equation:\n$$\n\\mathrm{d}x_t = a\\,x_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t\n$$\nMeasurement equation:\n$$\n\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t\n$$\nwhere $a, c, q, r$ are constants with $q \\ge 0$ and $r>0$, and $W_t, V_t$ are independent standard Wiener processes.\n\nThe Kalman-Bucy filter provides the optimal estimate $\\hat{x}_t$ of the state $x_t$. The filter for this system has the form:\n$$\n\\mathrm{d}\\hat{x}_t = a\\,\\hat{x}_t\\,\\mathrm{d}t + K_t(\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t)\n$$\nwhere $K_t$ is the Kalman gain, which we will determine. The term $\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t$ is the innovations process.\n\n**1. Derivation of the ODE for the Error Variance $P_t$**\n\nThe estimation error is defined as $e_t = x_t - \\hat{x}_t$. We derive its SDE by taking the differential: $\\mathrm{d}e_t = \\mathrm{d}x_t - \\mathrm{d}\\hat{x}_t$.\nSubstituting the expressions for $\\mathrm{d}x_t$ and $\\mathrm{d}\\hat{x}_t$:\n$$\n\\mathrm{d}e_t = (a\\,x_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t) - \\left( a\\,\\hat{x}_t\\,\\mathrm{d}t + K_t(\\mathrm{d}y_t - c\\,\\hat{x}_t\\,\\mathrm{d}t) \\right)\n$$\nNow, substitute $\\mathrm{d}y_t = c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t$ into the equation:\n$$\n\\mathrm{d}e_t = a\\,(x_t - \\hat{x}_t)\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t(c\\,x_t\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t - c\\,\\hat{x}_t\\,\\mathrm{d}t)\n$$\nGrouping terms by $e_t = x_t - \\hat{x}_t$:\n$$\n\\mathrm{d}e_t = a\\,e_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t(c\\,(x_t - \\hat{x}_t)\\,\\mathrm{d}t + \\sqrt{r}\\,\\mathrm{d}V_t)\n$$\n$$\n\\mathrm{d}e_t = (a - K_t c)\\,e_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t\\sqrt{r}\\,\\mathrm{d}V_t\n$$\nThe error variance is $P_t = \\mathbb{E}[e_t^2]$. To find its dynamic equation, we apply Itô's lemma to the function $f(e_t) = e_t^2$.\n$$\n\\mathrm{d}(e_t^2) = 2\\,e_t\\,\\mathrm{d}e_t + (\\mathrm{d}e_t)^2\n$$\nThe quadratic variation term $(\\mathrm{d}e_t)^2$ is calculated from the stochastic part of $\\mathrm{d}e_t$:\n$$\n(\\mathrm{d}e_t)^2 = (\\sqrt{q}\\,\\mathrm{d}W_t - K_t\\sqrt{r}\\,\\mathrm{d}V_t)^2 = q(\\mathrm{d}W_t)^2 + (K_t\\sqrt{r})^2(\\mathrm{d}V_t)^2 - 2\\sqrt{q}K_t\\sqrt{r}\\,\\mathrm{d}W_t\\mathrm{d}V_t\n$$\nSince $W_t$ and $V_t$ are independent, we have $(\\mathrm{d}W_t)^2 = \\mathrm{d}t$, $(\\mathrm{d}V_t)^2 = \\mathrm{d}t$, and $\\mathrm{d}W_t\\mathrm{d}V_t = 0$.\n$$\n(\\mathrm{d}e_t)^2 = (q + K_t^2 r)\\,\\mathrm{d}t\n$$\nSubstituting $\\mathrm{d}e_t$ and $(\\mathrm{d}e_t)^2$ into the expression for $\\mathrm{d}(e_t^2)$:\n$$\n\\mathrm{d}(e_t^2) = 2\\,e_t \\left( (a - K_t c)\\,e_t\\,\\mathrm{d}t + \\sqrt{q}\\,\\mathrm{d}W_t - K_t\\sqrt{r}\\,\\mathrm{d}V_t \\right) + (q + K_t^2 r)\\,\\mathrm{d}t\n$$\n$$\n\\mathrm{d}(e_t^2) = 2(a - K_t c)\\,e_t^2\\,\\mathrm{d}t + (q + K_t^2 r)\\,\\mathrm{d}t + 2\\sqrt{q}\\,e_t\\,\\mathrm{d}W_t - 2K_t\\sqrt{r}\\,e_t\\,\\mathrm{d}V_t\n$$\nTo get the ODE for $P_t = \\mathbb{E}[e_t^2]$, we take the expectation of both sides. The expectations of the stochastic integral terms (martingales) are zero.\n$$\n\\mathrm{d}P_t = \\mathrm{d}\\mathbb{E}[e_t^2] = \\mathbb{E}[\\mathrm{d}(e_t^2)] = \\mathbb{E}[2(a - K_t c)\\,e_t^2\\,\\mathrm{d}t + (q + K_t^2 r)\\,\\mathrm{d}t]\n$$\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2(a - K_t c)\\,\\mathbb{E}[e_t^2] + q + K_t^2 r = 2(a - K_t c)\\,P_t + q + K_t^2 r\n$$\nThe Kalman-Bucy filter gain $K_t$ is chosen to minimize the error variance $P_t$. This is achieved by minimizing the rate of change $\\frac{\\mathrm{d}P_t}{\\mathrm{d}t}$ at each instant. We treat the right-hand side as a function of $K_t$ and find the minimum by setting its derivative to zero:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}K_t} \\left( 2(a - K_t c)\\,P_t + q + K_t^2 r \\right) = -2c\\,P_t + 2r\\,K_t = 0\n$$\nThis gives the optimal Kalman gain:\n$$\nK_t = \\frac{c\\,P_t}{r}\n$$\nSubstituting this optimal gain back into the ODE for $P_t$:\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2\\left(a - \\frac{c\\,P_t}{r} c\\right)P_t + q + \\left(\\frac{c\\,P_t}{r}\\right)^2 r\n$$\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2a\\,P_t - \\frac{2c^2}{r}P_t^2 + q + \\frac{c^2 P_t^2}{r}\n$$\nThis simplifies to the continuous-time scalar Riccati differential equation:\n$$\n\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = -\\frac{c^2}{r}P_t^2 + 2a\\,P_t + q\n$$\n\n**2. Solution of the Riccati ODE**\n\nThis is a first-order nonlinear ODE. For $c \\ne 0$, we can solve it by finding its steady-state solutions and using separation of variables. Let $P_{\\infty}$ be a steady-state solution, where $\\frac{\\mathrm{d}P}{\\mathrm{d}t}=0$:\n$$\n-\\frac{c^2}{r}P_{\\infty}^2 + 2a\\,P_{\\infty} + q = 0 \\implies \\frac{c^2}{r}P_{\\infty}^2 - 2a\\,P_{\\infty} - q = 0\n$$\nThe roots of this quadratic equation are:\n$$\nP_{\\infty} = \\frac{2a \\pm \\sqrt{4a^2 - 4(\\frac{c^2}{r})(-q)}}{2(\\frac{c^2}{r})} = \\frac{r}{c^2}\\left(a \\pm \\sqrt{a^2 + \\frac{c^2 q}{r}}\\right)\n$$\nLet $\\gamma = \\sqrt{a^2 + \\frac{c^2 q}{r}}$. The two roots are $\\lambda_1 = \\frac{r}{c^2}(a+\\gamma)$ and $\\lambda_2 = \\frac{r}{c^2}(a-\\gamma)$. Since $\\gamma \\ge |a|$, $\\lambda_1 \\ge 0$ and $\\lambda_2 \\le 0$. The variance $P_t$ must be non-negative. $\\lambda_1$ is the stable equilibrium for $P_t \\ge 0$.\nThe ODE can be written as $\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = -\\frac{c^2}{r}(P_t - \\lambda_1)(P_t - \\lambda_2)$. Using separation of variables:\n$$\n\\int_{P_0}^{P_t} \\frac{\\mathrm{d}P}{(P - \\lambda_1)(P - \\lambda_2)} = -\\frac{c^2}{r} \\int_0^t \\mathrm{d}s = -\\frac{c^2}{r}t\n$$\nUsing partial fractions, $\\frac{1}{(P - \\lambda_1)(P - \\lambda_2)} = \\frac{1}{\\lambda_1 - \\lambda_2}\\left(\\frac{1}{P - \\lambda_1} - \\frac{1}{P - \\lambda_2}\\right)$. The integral becomes:\n$$\n\\frac{1}{\\lambda_1 - \\lambda_2} \\left[ \\ln\\left|\\frac{P-\\lambda_1}{P-\\lambda_2}\\right| \\right]_{P_0}^{P_t} = -\\frac{c^2}{r} t\n$$\nSince $P_t$ converges to $\\lambda_1$ from $P_0$ without crossing $\\lambda_2$ (as $\\lambda_2 \\le 0$ and $P_t \\ge 0$), the signs of $(P_t-\\lambda_1)$ and $(P_0-\\lambda_1)$ are the same, and $(P_t-\\lambda_2)>0, (P_0-\\lambda_2)>0$. We can drop the absolute value.\n$$\n\\ln\\left(\\frac{P_t-\\lambda_1}{P_t-\\lambda_2}\\right) - \\ln\\left(\\frac{P_0-\\lambda_1}{P_0-\\lambda_2}\\right) = -(\\lambda_1 - \\lambda_2)\\frac{c^2}{r} t\n$$\nNoting that $\\lambda_1 - \\lambda_2 = \\frac{2r\\gamma}{c^2}$, the exponent becomes $-(\\frac{2r\\gamma}{c^2})\\frac{c^2}{r} t = -2\\gamma t$.\n$$\n\\frac{P_t-\\lambda_1}{P_t-\\lambda_2} = \\frac{P_0-\\lambda_1}{P_0-\\lambda_2} \\exp(-2\\gamma t)\n$$\nSolving for $P_t$:\n$$\nP_t = \\frac{\\lambda_1(P_0-\\lambda_2) - \\lambda_2(P_0-\\lambda_1)\\exp(-2\\gamma t)}{(P_0-\\lambda_2) - (P_0-\\lambda_1)\\exp(-2\\gamma t)}\n$$\nwhere $\\lambda_1 = \\frac{r}{c^2}(a+\\gamma)$, $\\lambda_2 = \\frac{r}{c^2}(a-\\gamma)$, and $\\gamma = \\sqrt{a^2 + \\frac{c^2 q}{r}}$.\n\n**3. Convergence Conditions and Steady-State Value $P_{\\infty}$**\n\nFor $P_t$ to converge to a finite steady-state value $P_{\\infty} = \\lim_{t\\to\\infty} P_t$, the solution must approach a finite limit. For this scalar system, this occurs if the system is \"detectable,\" which means that any unstable or neutrally stable mode ($a \\ge 0$) must be observable ($c \\ne 0$). This is equivalent to the condition:\n$$\na < 0 \\quad \\text{or} \\quad c \\ne 0\n$$\nIf this condition holds, we can find $P_{\\infty}$ by taking the limit of $P_t$ as $t \\to \\infty$. Assuming $\\gamma > 0$ (which is true if $a \\neq 0$ or $c^2q \\neq 0$), the term $\\exp(-2\\gamma t)$ goes to $0$.\n$$\nP_{\\infty} = \\lim_{t\\to\\infty} P_t = \\frac{\\lambda_1(P_0-\\lambda_2) - 0}{(P_0-\\lambda_2) - 0} = \\lambda_1\n$$\nThis corresponds to the stable, non-negative equilibrium point of the Riccati equation. The closed-form expression for the steady-state variance is:\n$$\nP_{\\infty} = \\frac{r}{c^2}\\left(a + \\sqrt{a^2 + \\frac{c^2 q}{r}}\\right)\n$$\nThis expression is valid for $c \\ne 0$. If $c=0$, the convergence condition requires $a<0$. In this case, the Riccati ODE becomes linear: $\\frac{\\mathrm{d}P_t}{\\mathrm{d}t} = 2aP_t + q$, which has the steady-state solution $P_{\\infty} = -q/(2a)$. The formula for $c \\ne 0$ correctly tends to this limit as $c \\to 0$ for $a < 0$.\n\n**4. Numerical Evaluation**\n\nGiven the numerical values: $a=0.4$, $c=1.5$, $q=0.2$, $r=0.5$, and $P_0=1.0$.\nFirst, we check the convergence condition. We have $a=0.4 > 0$, but $c=1.5 \\ne 0$. The condition ($a < 0$ or $c \\ne 0$) is satisfied, so a finite steady-state variance exists. We use the formula for $P_{\\infty}$:\n$$\nP_{\\infty} = \\frac{r}{c^2}\\left(a + \\sqrt{a^2 + \\frac{c^2 q}{r}}\\right)\n$$\nSubstitute the given values:\n$$\nP_{\\infty} = \\frac{0.5}{(1.5)^2}\\left(0.4 + \\sqrt{(0.4)^2 + \\frac{(1.5)^2 (0.2)}{0.5}}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{0.16 + \\frac{2.25 \\times 0.2}{0.5}}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{0.16 + \\frac{0.45}{0.5}}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{0.16 + 0.9}\\right)\n$$\n$$\nP_{\\infty} = \\frac{0.5}{2.25}\\left(0.4 + \\sqrt{1.06}\\right)\n$$\nCalculating the numerical value:\n$$\nP_{\\infty} \\approx \\frac{0.5}{2.25}\\left(0.4 + 1.029563\\right) \\approx 0.222222 \\times (1.429563) \\approx 0.3176806\n$$\nRounding to four significant figures, we get:\n$$\nP_{\\infty} \\approx 0.3177\n$$", "answer": "$$\n\\boxed{0.3177}\n$$", "id": "3080944"}, {"introduction": "A filter is only useful if its estimation error remains bounded. The stability of the Kalman-Bucy filter is not guaranteed; it depends on a crucial property of the system known as \"detectability,\" which ensures that any unstable behavior in the system can be seen by the measurements. This hands-on coding exercise provides a powerful demonstration of this principle, allowing you to simulate systems where detectability fails and observe the resulting divergence of the estimation error covariance firsthand [@problem_id:3080976].", "problem": "Consider a continuous-time linear stochastic system and measurement model with Gaussian white noise,\n$$\n\\mathrm{d}x_t = A x_t \\,\\mathrm{d}t + G \\,\\mathrm{d}W_t,\\quad \\mathrm{d}y_t = C x_t \\,\\mathrm{d}t + D \\,\\mathrm{d}V_t,\n$$\nwhere $x_t \\in \\mathbb{R}^n$, $y_t \\in \\mathbb{R}^m$, $A \\in \\mathbb{R}^{n \\times n}$, $C \\in \\mathbb{R}^{m \\times n}$, $G \\in \\mathbb{R}^{n \\times r}$, $D \\in \\mathbb{R}^{m \\times s}$, and $W_t, V_t$ are standard Wiener processes. Let the process noise covariance be $Q = G G^\\top \\in \\mathbb{R}^{n \\times n}$ and the measurement noise covariance be $R = D D^\\top \\in \\mathbb{R}^{m \\times m}$, with $R$ positive definite. The Kalman-Bucy filter (KBF) estimates $x_t$ with an estimator $\\hat{x}_t$ and a symmetric positive semidefinite estimation error covariance $P_t = \\mathbb{E}[(x_t - \\hat{x}_t)(x_t - \\hat{x}_t)^\\top]$ governed by the continuous-time Riccati differential equation\n$$\n\\dot{P}_t = A P_t + P_t A^\\top + Q - P_t C^\\top R^{-1} C P_t,\\quad P(0) = P_0.\n$$\nThe pair $(A,C)$ is called detectable if every eigenvalue of $A$ with nonnegative real part corresponds to a state that is observable under $C$; equivalently, all unobservable modes are exponentially stable. In this problem you will construct an example where $(A,C)$ is not detectable and show numerically that the error covariance $P_t$ diverges and that a sampled innovation variance grows without bound. To quantify the latter in a way that is meaningful from first principles, define the sampled innovation residual over a fixed sampling interval $\\Delta t$ by\n$$\nr_k = y_{t_k+\\Delta t} - y_{t_k} - C \\hat{x}_{t_k}\\,\\Delta t,\n$$\nwhose variance under the linear-Gaussian model and small $\\Delta t$ is well-approximated by\n$$\n\\mathrm{Var}(r_k) \\approx C P_{t_k} C^\\top\\,\\Delta t^2 + R\\,\\Delta t.\n$$\nYour program must integrate the Riccati differential equation for each test case, compute $P_T$ at a final time $T$, and report:\n- the largest eigenvalue $\\lambda_{\\max}(P_T)$, and\n- the sampled innovation variance $S_T = C P_T C^\\top\\,\\Delta t^2 + R\\,\\Delta t$\nat the final time.\n\nUse the following fixed numerical settings for all test cases:\n- dimension $n=2$, measurement dimension $m=1$,\n- initial covariance $P_0 = I_2$ (the $2 \\times 2$ identity),\n- process noise matrix $G = I_2$, so $Q = I_2$,\n- measurement noise matrix $D = 1$ (scalar), so $R = [1]$,\n- time step $\\Delta t = 0.01$ and final time $T = 20$,\n- integrate $\\dot{P}_t$ using a stable explicit Runge–Kutta method of order $4$, ensuring $P_t$ remains symmetric at each step.\n\nTest Suite:\n- Case $1$ (not detectable; unstable unobservable mode): $A = \\mathrm{diag}(0.5,-1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$.\n- Case $2$ (detectable; unstable mode observed): $A = \\mathrm{diag}(0.5,-1.0)$, $C = \\begin{bmatrix}1 & 0\\end{bmatrix}$.\n- Case $3$ (detectable but not observable; all unobservable modes stable): $A = \\mathrm{diag}(-0.2,-0.3)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$.\n- Case $4$ (not detectable boundary; marginally stable unobservable mode): $A = \\mathrm{diag}(0.0,-1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$.\n\nFor each case, numerically integrate the Riccati equation from $t=0$ to $t=T$. Then compute $\\lambda_{\\max}(P_T)$ and $S_T$ as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the list has four items (one per test case) and each item is itself a two-element list $[\\lambda_{\\max}(P_T), S_T]$. For example, the output format must be\n$$\n[[\\lambda_{\\max}^{(1)},S^{(1)}],[\\lambda_{\\max}^{(2)},S^{(2)}],[\\lambda_{\\max}^{(3)},S^{(3)}],[\\lambda_{\\max}^{(4)},S^{(4)}]],\n$$\nwith each quantity represented as a floating-point number, and no additional text printed.", "solution": "The user's problem requires the numerical integration of the continuous-time matrix Riccati differential equation (RDE) associated with a Kalman-Bucy filter. The analysis will focus on the concept of detectability and its impact on the stability of the estimation error covariance.\n\nFirst, we formalize the problem. The system is governed by the state and measurement equations:\n$$\n\\mathrm{d}x_t = A x_t \\,\\mathrm{d}t + G \\,\\mathrm{d}W_t\n$$\n$$\n\\mathrm{d}y_t = C x_t \\,\\mathrm{d}t + D \\,\\mathrm{d}V_t\n$$\nThe estimation error covariance, $P_t = \\mathbb{E}[(x_t - \\hat{x}_t)(x_t - \\hat{x}_t)^\\top]$, evolves according to the RDE:\n$$\n\\dot{P}_t = F(P_t) = A P_t + P_t A^\\top + Q - P_t C^\\top R^{-1} C P_t\n$$\nwhere $Q = G G^\\top$ and $R = D D^\\top$. The problem provides fixed parameters: state dimension $n=2$, measurement dimension $m=1$, initial covariance $P_0 = I_2$ (the $2 \\times 2$ identity matrix), process noise matrix $G=I_2$ (so $Q=I_2$), and measurement noise matrix $D=1$ (a scalar, so $R=[1]$ and $R^{-1}=[1]$). The integration is to be performed from $t=0$ to a final time $T=20$ using a time step $\\Delta t = 0.01$.\n\nThe RDE is a matrix-valued ordinary differential equation, which we solve using the fourth-order Runge-Kutta (RK4) method. For an ODE $\\dot{P}_t = F(P_t)$, the update from time $t_k$ to $t_{k+1}=t_k+\\Delta t$ is given by:\n$$\nK_1 = F(P_{t_k})\n$$\n$$\nK_2 = F(P_{t_k} + \\frac{\\Delta t}{2} K_1)\n$$\n$$\nK_3 = F(P_{t_k} + \\frac{\\Delta t}{2} K_2)\n$$\n$$\nK_4 = F(P_{t_k} + \\Delta t K_3)\n$$\n$$\nP_{t_{k+1}} = P_{t_k} + \\frac{\\Delta t}{6} (K_1 + 2K_2 + 2K_3 + K_4)\n$$\nThe covariance matrix $P_t$ must be symmetric. The RDE function $F(P_t)$ preserves symmetry: if $P_t$ is symmetric, then $F(P_t)$ is also symmetric. Consequently, the RK4 algorithm, which is a linear combination of evaluations of $F$, also preserves symmetry. To counteract any potential numerical drift from floating-point inaccuracies, symmetry is explicitly enforced at each integration step by setting $P_{t_{k+1}} \\leftarrow \\frac{1}{2}(P_{t_{k+1}} + P_{t_{k+1}}^\\top)$.\n\nA core concept in this problem is the detectability of the pair $(A,C)$. A pair $(A,C)$ is detectable if every eigenvalue of $A$ with a non-negative real part is observable through the measurement matrix $C$. If $(A,C)$ is detectable and the pair $(A,G)$ is stabilizable (satisfied here since $Q=I_2>0$), the solution $P_t$ of the RDE converges to a unique, finite, positive semi-definite steady-state matrix $P_{ss}$ as $t \\to \\infty$. If $(A,C)$ is not detectable, the error covariance $P_t$ will diverge. We analyze each test case:\n- **Case 1**: $A = \\mathrm{diag}(0.5, -1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$. The eigenvalue $\\lambda=0.5$ is unstable (non-negative real part). Its corresponding eigenvector is $e_1 = [1, 0]^\\top$. The observability of this mode is checked via the product $C e_1 = \\begin{bmatrix}0 & 1\\end{bmatrix} [1, 0]^\\top = 0$. The unstable mode is unobservable. Therefore, the pair $(A,C)$ is not detectable, and we expect the error covariance $P_t$ to diverge.\n- **Case 2**: $A = \\mathrm{diag}(0.5, -1.0)$, $C = \\begin{bmatrix}1 & 0\\end{bmatrix}$. The unstable mode with $\\lambda=0.5$ is now tested with the new $C$: $C e_1 = \\begin{bmatrix}1 & 0\\end{bmatrix} [1, 0]^\\top = 1 \\neq 0$. The unstable mode is observable. The pair $(A,C)$ is detectable, and we expect $P_t$ to converge to a finite steady-state solution.\n- **Case 3**: $A = \\mathrm{diag}(-0.2, -0.3)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$. Both eigenvalues, $-0.2$ and $-0.3$, have negative real parts. All modes are stable. The condition for detectability (concerning unstable or marginally stable modes) is therefore vacuously satisfied. The pair $(A,C)$ is detectable, and we expect $P_t$ to converge.\n- **Case 4**: $A = \\mathrm{diag}(0.0, -1.0)$, $C = \\begin{bmatrix}0 & 1\\end{bmatrix}$. The eigenvalue $\\lambda=0.0$ is marginally stable (non-negative real part). Its eigenvector is $e_1 = [1, 0]^\\top$. The observability test gives $C e_1 = \\begin{bmatrix}0 & 1\\end{bmatrix} [1, 0]^\\top = 0$. The marginally stable mode is unobservable. The pair $(A,C)$ is not detectable, and we expect $P_t$ to diverge.\n\nAfter integrating the RDE to obtain $P_T$, we compute two quantities for each case:\n1. The largest eigenvalue of the final covariance matrix, $\\lambda_{\\max}(P_T)$. This value quantifies the magnitude of the largest component of estimation uncertainty.\n2. The sampled innovation variance at time $T$, given by $S_T = C P_T C^\\top\\,\\Delta t^2 + R\\,\\Delta t$. This metric reflects how predictable the next measurement is, based on the current state estimate. A diverging $P_T$ in a direction \"seen\" by $C$ would lead to a large $S_T$.\n\nThe following program implements this entire procedure. It iterates through the four cases, performs the numerical integration, computes the required metrics, and formats the output as specified.", "answer": "```python\nimport numpy as np\nfrom scipy import __version__ as scipy_version  # For environment spec adherence only\n\ndef solve():\n    \"\"\"\n    Solves the continuous-time Riccati differential equation for four test cases\n    and reports the largest eigenvalue and sampled innovation variance at the final time.\n    \"\"\"\n    # Define fixed numerical settings from the problem statement.\n    n = 2\n    P0 = np.identity(n)\n    G = np.identity(n)\n    Q = G @ G.T  # Results in the 2x2 identity matrix\n    D = np.array([[1.0]])\n    R = D @ D.T  # Results in the 1x1 matrix [1]\n    try:\n        R_inv = np.linalg.inv(R)\n    except np.linalg.LinAlgError:\n        # Fallback for scalar-like R\n        R_inv = np.array([[1.0 / R[0, 0]]])\n\n    DT = 0.01\n    T_FINAL = 20.0\n\n    # Define the test cases.\n    test_cases = [\n        # Case 1: Not detectable (unstable unobservable mode)\n        {\"A\": np.diag([0.5, -1.0]), \"C\": np.array([[0.0, 1.0]])},\n        # Case 2: Detectable (unstable mode observed)\n        {\"A\": np.diag([0.5, -1.0]), \"C\": np.array([[1.0, 0.0]])},\n        # Case 3: Detectable (all modes stable)\n        {\"A\": np.diag([-0.2, -0.3]), \"C\": np.array([[0.0, 1.0]])},\n        # Case 4: Not detectable (marginally stable unobservable mode)\n        {\"A\": np.diag([0.0, -1.0]), \"C\": np.array([[0.0, 1.0]])},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A = case[\"A\"]\n        C = case[\"C\"]\n        \n        # Initialize the covariance matrix for the current case.\n        P = P0.copy()\n        \n        # Define the right-hand side of the Riccati differential equation.\n        def riccati_rhs(p_matrix):\n            return A @ p_matrix + p_matrix @ A.T + Q - p_matrix @ C.T @ R_inv @ C @ p_matrix\n\n        num_steps = int(T_FINAL / DT)\n        for _ in range(num_steps):\n            # Fourth-order Runge-Kutta (RK4) integration step.\n            k1 = riccati_rhs(P)\n            k2 = riccati_rhs(P + 0.5 * DT * k1)\n            k3 = riccati_rhs(P + 0.5 * DT * k2)\n            k4 = riccati_rhs(P + DT * k3)\n            \n            P_next = P + (DT / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n            \n            # Enforce symmetry to prevent numerical drift.\n            P = 0.5 * (P_next + P_next.T)\n\n        P_T = P\n\n        # 1. Compute the largest eigenvalue of P_T.\n        # np.linalg.eigvalsh is used as it is efficient for symmetric matrices.\n        lambda_max_PT = np.max(np.linalg.eigvalsh(P_T))\n\n        # 2. Compute the sampled innovation variance S_T.\n        # C @ P_T @ C.T results in a (1, 1) matrix for m=1.\n        innovation_var_term = C @ P_T @ C.T\n        S_T_matrix = innovation_var_term * (DT**2) + R * DT\n        S_T = S_T_matrix[0, 0]  # Extract the scalar value.\n\n        results.append([lambda_max_PT, S_T])\n\n    # Format the final output string precisely as required: [[v1,v2],[v3,v4],...].\n    # This manual formatting avoids spaces that str(list) would insert.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "3080976"}, {"introduction": "In real-world applications, our models of reality are never perfect. A critical question for any engineer is: \"How do I know if my filter is working correctly?\" The theory of optimal filtering provides a powerful diagnostic tool: the innovation sequence—the stream of errors between new measurements and their predictions—must be \"white\" noise. This practice simulates this vital validation process, guiding you to test the whiteness of innovations and see how a mismatched filter corrupts this property [@problem_id:3080965].", "problem": "Consider a linear stochastic system observed continuously and filtered by the continuous-time linear Kalman-Bucy filter. The state process is given by the stochastic differential equation $dx_t = A x_t \\, dt + G \\, dW_t$ where $A \\in \\mathbb{R}^{n \\times n}$ is a stable drift matrix, $G \\in \\mathbb{R}^{n \\times r}$ is the diffusion input matrix, and $W_t$ is a standard $r$-dimensional Wiener process. The observation process is given by $dy_t = H x_t \\, dt + d\\eta_t$, where $H \\in \\mathbb{R}^{m \\times n}$ is the observation matrix and $\\eta_t$ is an $m$-dimensional Wiener process with instantaneous covariance intensity $R_{\\text{eff}} = H R H^{\\top}$ for some positive definite matrix $R$. The innovation process of the Kalman-Bucy filter is the discrepancy between the observed increment and its model-predicted increment.\n\nStarting from the definitions of linear stochastic differential equations and the innovation process in the Kalman-Bucy filter, design a program that performs the following steps from first principles:\n\n- Discretize the above continuous-time model over a uniform sampling grid with step size $dt$ (in seconds), using a scientifically sound method that is consistent for Itô processes. Generate sample paths for $x_t$ and $y_t$ adhering to the definitions above, ensuring that the covariance of the observation noise increments is $R_{\\text{eff}} \\, dt$.\n- Implement the Kalman-Bucy filtering equations in a manner that is consistent with basic definitions, without using any pre-derived shortcut formulas beyond the continuous-time linear Gaussian setting. Use an explicit Euler method to approximate the continuous-time filter update.\n- Compute the innovation sequence increments $\\Delta \\nu_k$ at each time step based on the model-predicted observation increment and the realized observation increment.\n- Estimate the sample autocovariance matrices of the innovation sequence at lags $0$, $1$, and $2$. Use unbiased or consistent finite-sample estimators derived from the definition of covariance between random vectors.\n- Compare the estimated lag-0 innovation covariance to its theoretical value, $R_{\\text{eff}} \\, dt$. For an optimal filter, the autocovariance at non-zero lags should be the zero matrix.\n\nYour program must simulate and test the whiteness of innovations under the following test suite of parameter sets. In each, $dt$ is specified in seconds, and the number of steps $N$ is a positive integer. All matrices must be scientifically plausible, with stable $A$ and positive definite noise covariances:\n\n- Test Case $1$ (baseline two-dimensional system, well-tuned filter):\n  - $n = 2$, $m = 2$, $r = 2$.\n  - $A = \\begin{bmatrix} -0.5 & 0.0 \\\\ 0.0 & -1.0 \\end{bmatrix}$, $G = I_2$, $H = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n  - $Q = I_2$ (process noise covariance intensity), $R = \\operatorname{diag}(0.4, 0.6)$ (to construct $R_{\\text{eff}} = H R H^{\\top}$).\n  - $dt = 0.01$ seconds, $N = 5000$, initial mean $x_0 = 0$, initial filter covariance $P_0 = I_2$.\n  - The filter must use the true $R_{\\text{eff}}$.\n\n- Test Case $2$ (boundary one-dimensional system, small step):\n  - $n = 1$, $m = 1$, $r = 1$.\n  - $A = [-2.0]$, $G = [1.0]$, $H = [1.0]$.\n  - $Q = [1.0]$, $R = [0.2]$, hence $R_{\\text{eff}} = H R H^{\\top} = [0.2]$.\n  - $dt = 0.002$ seconds, $N = 15000$, initial mean $x_0 = 0$, initial filter covariance $P_0 = [1.0]$.\n  - The filter must use the true $R_{\\text{eff}}$.\n\n- Test Case $3$ (mismatched measurement noise in the filter to induce non-whiteness):\n  - Same as Test Case $1$, except the filter must use a mismatched measurement noise covariance intensity $\\widehat{R}_{\\text{eff}} = 0.25 \\times R_{\\text{eff}}$ while the simulated observation noise still uses the true $R_{\\text{eff}}$.\n\nFor each test case, compute two scalar metrics:\n- The normalized lag-$0$ mismatch $\\rho_0 = \\dfrac{\\| \\widehat{\\Gamma}_0 - R_{\\text{eff}} \\, dt \\|_F}{\\| R_{\\text{eff}} \\, dt \\|_F}$, where $\\widehat{\\Gamma}_\\ell$ denotes the sample autocovariance at lag $\\ell$ and $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n- The normalized lag-$1$ magnitude $\\rho_1 = \\dfrac{\\| \\widehat{\\Gamma}_1 \\|_F}{\\| \\widehat{\\Gamma}_0 \\|_F}$.\n\nDeclare a boolean pass/fail outcome for each test case defined as true if both $\\rho_0 \\le 0.3$ and $\\rho_1 \\le 0.25$, and false otherwise. These thresholds are dimensionless and must be applied consistently for all test cases.\n\nYour program should produce a single line of output containing the pass/fail outcomes for the three test cases as a comma-separated list enclosed in square brackets, for example $[ \\text{true}, \\text{false}, \\text{true} ]$. The output values are booleans and dimensionless. No other output must be printed.", "solution": "The objective of this problem is to numerically validate a fundamental property of the continuous-time linear Kalman-Bucy filter: the whiteness of its innovation process. The task involves simulating a linear stochastic system, applying a discretized version of the Kalman-Bucy filter, and analyzing the statistical properties of the resulting innovation sequence. The analysis will be performed for three distinct test cases, including a well-tuned filter and a filter with mismatched parameters, to observe the theoretical properties and their breakdown.\n\nThe system is described by a pair of Itô stochastic differential equations (SDEs). The state process $x_t \\in \\mathbb{R}^n$ evolves according to:\n$$dx_t = A x_t \\, dt + G \\, dW_t$$\nand the observation process $y_t \\in \\mathbb{R}^m$ is given by:\n$$dy_t = H x_t \\, dt + d\\eta_t$$\nHere, $A$ is the state dynamics matrix, $G$ is the process noise input matrix, and $H$ is the observation matrix. $W_t$ and $\\eta_t$ are Wiener processes. The process noise term $G \\, dW_t$ has a covariance intensity matrix $Q_c = G Q G^T$, where the covariance of the underlying Wiener process $W_t$ increments is $Q \\, dt$. The observation noise $d\\eta_t$ has a specified covariance intensity matrix $R_{\\text{eff}} = H R H^T$, where $R$ is a given positive definite matrix.\n\n**1. System and Filter Discretization**\n\nTo simulate the system and filter on a digital computer, we must discretize the continuous-time equations over a uniform time grid with step size $dt$.\n\n**System Discretization**: We use the Euler-Maruyama method, a standard first-order numerical scheme for SDEs. For a small time step $dt$, the state at time $t_{k+1} = t_k + dt$ is approximated as:\n$$x_{k+1} \\approx x_k + A x_k \\, dt + \\Delta\\xi_k$$\nwhere $\\Delta\\xi_k$ is a Gaussian random vector with zero mean and covariance matrix $E[\\Delta\\xi_k \\Delta\\xi_k^T] = Q_c \\, dt = (G Q G^T) \\, dt$.\n\nThe observation equation is discretized by integrating over the interval $[t_k, t_{k+1}]$, yielding the observation increment $\\Delta y_k = y_{k+1} - y_k$:\n$$\\Delta y_k \\approx H x_k \\, dt + \\Delta\\eta_k$$\nwhere $\\Delta\\eta_k$ is a Gaussian random vector with zero mean and covariance matrix $E[\\Delta\\eta_k \\Delta\\eta_k^T] = R_{\\text{eff}} \\, dt$.\n\n**Filter Discretization**: The Kalman-Bucy filter provides an optimal estimate $\\hat{x}_t$ of the state $x_t$. The estimate and its error covariance matrix $P_t = E[(x_t - \\hat{x}_t)(x_t - \\hat{x}_t)^T]$ evolve according to continuous-time differential equations. We approximate their solution using the explicit Euler method.\n\nThe state estimate update is given by the SDE:\n$$d\\hat{x}_t = A \\hat{x}_t \\, dt + K_t (dy_t - H \\hat{x}_t \\, dt)$$\nIts Euler discretization is:\n$$\\hat{x}_{k+1} \\approx \\hat{x}_k + (A \\hat{x}_k) \\, dt + K_k (\\Delta y_k - H \\hat{x}_k \\, dt)$$\nThe error covariance $P_t$ evolves according to the continuous-time algebraic Riccati equation:\n$$\\frac{dP_t}{dt} = A P_t + P_t A^T - P_t H^T \\widehat{R}_{\\text{eff}}^{-1} H P_t + Q_c$$\nwhere $\\widehat{R}_{\\text{eff}}$ is the measurement noise covariance intensity assumed by the filter, which may be mismatched from the true $R_{\\text{eff}}$. The Kalman gain $K_t$ is given by $K_t = P_t H^T \\widehat{R}_{\\text{eff}}^{-1}$. Substituting the gain expression, the Riccati equation becomes $\\frac{dP_t}{dt} = A P_t + P_t A^T - K_t H P_t + Q_c$. The explicit Euler update for the covariance is:\n$$P_{k+1} \\approx P_k + (A P_k + P_k A^T - K_k H P_k + Q_c) \\, dt$$\nIn each step, the gain $K_k = P_k H^T \\widehat{R}_{\\text{eff}}^{-1}$ is computed using the current covariance $P_k$.\n\n**2. Innovation Sequence and Autocovariance Analysis**\n\nA key component of the Kalman filter is the innovation process, which represents the new information provided by the latest measurement. The continuous-time innovation is $d\\nu_t = dy_t - H \\hat{x}_t \\, dt$. For an optimal filter, the innovation process is a Wiener process with the same statistics as the measurement noise, i.e., $E[d\\nu_t d\\nu_s^T] = R_{\\text{eff}} \\delta(t-s) \\, dt ds$.\n\nThe discretized innovation increment at step $k$ is:\n$$\\Delta \\nu_k = \\Delta y_k - H \\hat{x}_k \\, dt$$\nThe theoretical properties imply that for an optimal filter, the sequence $\\{\\Delta\\nu_k\\}$ should be a zero-mean, uncorrelated (white) vector sequence with a lag-zero covariance matrix $\\Gamma_0 = E[\\Delta\\nu_k \\Delta\\nu_k^T] = R_{\\text{eff}} \\, dt$, and zero covariance for all non-zero lags, $\\Gamma_\\ell = E[\\Delta\\nu_{k+\\ell} \\Delta\\nu_k^T] = 0$ for $\\ell \\ge 1$.\n\nTo verify this property, we estimate the sample autocovariance matrices from the simulated innovation sequence $\\{\\Delta\\nu_0, \\Delta\\nu_1, \\dots, \\Delta\\nu_{N-1}\\}$:\n$$\\widehat{\\Gamma}_\\ell = \\frac{1}{N-\\ell} \\sum_{k=0}^{N-1-\\ell} \\Delta\\nu_{k+\\ell} \\Delta\\nu_k^T$$\nWe compute this for lags $\\ell=0$ and $\\ell=1$.\n\n**3. Performance Metrics and Evaluation**\n\nThe problem specifies two metrics to quantify the whiteness of the estimated innovation sequence:\n1.  **Normalized Lag-0 Mismatch ($\\rho_0$)**: This measures the deviation of the estimated lag-$0$ covariance from its theoretical value.\n    $$\\rho_0 = \\frac{\\| \\widehat{\\Gamma}_0 - R_{\\text{eff}} \\, dt \\|_F}{\\| R_{\\text{eff}} \\, dt \\|_F}$$\n    where $\\|\\cdot\\|_F$ is the Frobenius norm. A small $\\rho_0$ indicates the filter correctly captures the innovation's variance.\n\n2.  **Normalized Lag-1 Magnitude ($\\rho_1$)**: This measures the degree of correlation at lag $1$, normalized by the lag-$0$ power.\n    $$\\rho_1 = \\frac{\\| \\widehat{\\Gamma}_1 \\|_F}{\\| \\widehat{\\Gamma}_0 \\|_F}$$\n    A small $\\rho_1$ indicates that the innovations are close to being uncorrelated (white), as theoretically expected.\n\nA test case is considered to pass if it satisfies both $\\rho_0 \\le 0.3$ and $\\rho_1 \\le 0.25$. This criterion checks if the filter performance is close to optimal. Test Case 3, which uses a mismatched noise covariance $\\widehat{R}_{\\text{eff}}$, is expected to yield a suboptimal filter, resulting in colored innovations and likely failing this test.\n\nThe program implements this entire procedure: it simulates the system and filter for each parameter set, computes the innovation sequence, estimates its autocovariance, calculates the metrics $\\rho_0$ and $\\rho_1$, and determines the pass/fail outcome.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(params):\n    \"\"\"\n    Simulates a linear stochastic system, applies a Kalman-Bucy filter,\n    and analyzes the whiteness of the innovation sequence.\n    \"\"\"\n    n, m, A, G, H, Q, R, dt, N, P0, R_eff_mismatch_factor = params\n    \n    # --- 1. Initialization ---\n    \n    # Ensure all matrices are 2D arrays for consistent matrix multiplication\n    A, G, H, Q, R, P0 = map(np.atleast_2d, (A, G, H, Q, R, P0))\n\n    # True continuous-time noise covariance matrices\n    Qc = G @ Q @ G.T\n    R_eff = H @ R @ H.T\n    \n    # Filter's assumed continuous-time measurement noise covariance\n    R_eff_hat = R_eff * R_eff_mismatch_factor\n    R_eff_hat_inv = np.linalg.inv(R_eff_hat)\n\n    # Initial states as column vectors\n    x = np.zeros((n, 1))\n    x_hat = np.zeros((n, 1))\n    P = np.copy(P0)\n\n    # Storage for innovation increments\n    innovations = np.zeros((N, m))\n    \n    # Discrete-time noise covariance matrices for simulation\n    Q_proc_dt = Qc * dt\n    R_obs_dt = R_eff * dt\n\n    mean_n = np.zeros(n)\n    mean_m = np.zeros(m)\n    \n    # --- 2. Simulation Loop ---\n    for k in range(N):\n        # Generate noise increments as column vectors\n        proc_noise = np.random.multivariate_normal(mean_n, Q_proc_dt).reshape(n, 1)\n        obs_noise = np.random.multivariate_normal(mean_m, R_obs_dt).reshape(m, 1)\n        \n        # S-1: Simulate true system observation increment at step k\n        delta_y = (H @ x) * dt + obs_noise\n        \n        # S-2: Simulate true system state update for step k+1\n        x_next = x + (A @ x) * dt + proc_noise\n        \n        # F-1: Calculate innovation increment at step k\n        delta_nu = delta_y - (H @ x_hat) * dt\n        innovations[k, :] = delta_nu.flatten()\n\n        # F-2: Calculate Kalman gain at step k\n        K = P @ H.T @ R_eff_hat_inv\n        \n        # F-3: Update filter state estimate for step k+1\n        x_hat_next = x_hat + (A @ x_hat) * dt + K @ delta_nu\n        \n        # F-4: Update filter covariance for step k+1 via Euler on Riccati eq.\n        dP_dt = A @ P + P @ A.T - K @ H @ P + Qc\n        P_next = P + dP_dt * dt\n        \n        # Advance time step\n        x, x_hat, P = x_next, x_hat_next, P_next\n        \n    # --- 3. Post-Processing ---\n    \n    # Estimate autocovariances using vectorized operations\n    Gamma_hat_0 = (innovations.T @ innovations) / N\n    Gamma_hat_1 = (innovations[1:].T @ innovations[:-1]) / (N - 1)\n    \n    # Theoretical lag-0 covariance\n    Gamma_0_th = R_eff * dt\n    \n    # Calculate Frobenius norms for metrics\n    norm_gamma_th_0 = np.linalg.norm(Gamma_0_th, 'fro')\n    if norm_gamma_th_0 < 1e-15: norm_gamma_th_0 = 1.0\n\n    norm_gamma_hat_0 = np.linalg.norm(Gamma_hat_0, 'fro')\n    if norm_gamma_hat_0 < 1e-15: norm_gamma_hat_0 = 1.0\n\n    # Calculate metrics\n    rho_0 = np.linalg.norm(Gamma_hat_0 - Gamma_0_th, 'fro') / norm_gamma_th_0\n    rho_1 = np.linalg.norm(Gamma_hat_1, 'fro') / norm_gamma_hat_0\n\n    # Apply pass/fail criterion\n    is_pass = rho_0 <= 0.3 and rho_1 <= 0.25\n    return is_pass\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the simulations.\n    \"\"\"\n    # Test Case 1: Baseline 2D system, well-tuned filter\n    A1 = np.array([[-0.5, 0.0], [0.0, -1.0]])\n    G1 = np.eye(2)\n    H1 = np.array([[1.0, 0.5], [0.0, 1.0]])\n    Q1 = np.eye(2)\n    R1 = np.diag([0.4, 0.6])\n    P0_1 = np.eye(2)\n    case1 = (2, 2, A1, G1, H1, Q1, R1, 0.01, 5000, P0_1, 1.0)\n\n    # Test Case 2: Boundary 1D system, well-tuned filter\n    A2 = np.array([[-2.0]])\n    G2 = np.array([[1.0]])\n    H2 = np.array([[1.0]])\n    Q2 = np.array([[1.0]])\n    R2 = np.array([[0.2]])\n    P0_2 = np.array([[1.0]])\n    case2 = (1, 1, A2, G2, H2, Q2, R2, 0.002, 15000, P0_2, 1.0)\n\n    # Test Case 3: Mismatched filter (underestimates measurement noise)\n    case3 = (2, 2, A1, G1, H1, Q1, R1, 0.01, 5000, P0_1, 0.25)\n    \n    test_cases = [case1, case2, case3]\n    \n    # Using a fixed seed for reproducibility of the stochastic simulation results.\n    np.random.seed(42)\n    \n    results = []\n    for case in test_cases:\n        pass_fail = run_simulation(case)\n        results.append(str(pass_fail).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3080965"}]}