{"hands_on_practices": [{"introduction": "This first exercise is a foundational workout, designed to build your intuition for Backward Stochastic Differential Equations (BSDEs). By solving a simple, linear BSDE directly from its definition, you will engage with the core mechanics of these equations. This practice reinforces the fundamental roles of conditional expectation and the martingale property of Itô integrals, providing a solid base for tackling more complex problems. [@problem_id:3040130]", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions, and let $(W_t)_{t\\in[0,T]}$ be a standard Brownian motion with $W_0=0$ adapted to $(\\mathcal{F}_t)_{t\\in[0,T]}$. Consider the Backward Stochastic Differential Equation (BSDE)\n$$\nY_t \\;=\\; \\xi \\;+\\; \\int_t^T \\alpha\\,ds \\;-\\; \\int_t^T Z_s\\,dW_s,\\qquad t\\in[0,T],\n$$\nwhere $\\alpha\\in\\mathbb{R}$ is a constant and the terminal condition is $\\xi=W_T$. Assume $Y$ and $Z$ are square-integrable adapted processes. Using only the fundamental properties of conditional expectation, the martingale property of the Itô integral, and the defining properties of Brownian motion (in particular, that Brownian motion has independent increments and $\\mathbb{E}[W_T\\mid\\mathcal{F}_t]=W_t$), derive closed-form expressions for $Y_t$ and $Z_t$ for $t\\in[0,T]$. Express your final answer as analytic formulas for $Y_t$ and $Z_t$ in terms of $t$, $T$, $\\alpha$, and $W_t$.", "solution": "The problem presents a Backward Stochastic Differential Equation (BSDE) and asks for the closed-form expressions for the solution processes $(Y_t, Z_t)$. The BSDE is given by\n$$\nY_t = \\xi + \\int_t^T \\alpha \\,ds - \\int_t^T Z_s \\,dW_s, \\qquad t \\in [0,T]\n$$\nwhere $\\alpha \\in \\mathbb{R}$ is a constant, $(W_t)_{t\\in[0,T]}$ is a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$, and the terminal condition is $\\xi = W_T$. The processes $Y$ and $Z$ are assumed to be square-integrable and adapted to the filtration $(\\mathcal{F}_t)_{t\\in[0,T]}$.\n\nThe solution can be found by following two main steps: first finding $Y_t$, and then using that result to find $Z_t$. The derivation will adhere to the specified constraint of using only fundamental properties of conditional expectation, Itô integrals, and Brownian motion.\n\nFirst, we derive the expression for $Y_t$. A key insight is to relate the value of the process $Y_t$ to a conditional expectation. Let's rewrite the BSDE in its equivalent forward differential form. A process pair $(Y_t, Z_t)$ is a solution to the BSDE if and only if it satisfies the forward stochastic differential equation (SDE)\n$$\ndY_t = -\\alpha \\,dt + Z_t \\,dW_t\n$$\nfor $t \\in [0,T]$, with the terminal condition $Y_T = \\xi$.\n\nNow, let us define an auxiliary process $M_t$ as\n$$\nM_t = Y_t + \\int_0^t \\alpha \\,du = Y_t + \\alpha t.\n$$\nUsing Itô's rule for differentials (or by direct substitution), the differential of $M_t$ is\n$$\ndM_t = dY_t + d(\\alpha t) = (-\\alpha \\,dt + Z_t \\,dW_t) + \\alpha \\,dt = Z_t \\,dW_t.\n$$\nSince the dynamics of $M_t$ contain no drift term ($dt$ term), the process $M_t$ is a local martingale. Given the square-integrability assumption on $Z$, $M_t = M_0 + \\int_0^t Z_s \\,dW_s$ is a true martingale.\n\nA fundamental property of any martingale $M$ is that for any two times $s  t$, $\\mathbb{E}[M_t \\mid \\mathcal{F}_s] = M_s$. Applying this property over the interval $[t, T]$, we have\n$$\nM_t = \\mathbb{E}[M_T \\mid \\mathcal{F}_t].\n$$\nWe can express $M_T$ using its definition and the problem's terminal condition for $Y_T$:\n$$\nM_T = Y_T + \\alpha T = \\xi + \\alpha T.\n$$\nSubstituting the given terminal condition $\\xi = W_T$:\n$$\nM_T = W_T + \\alpha T.\n$$\nNow we can compute the conditional expectation to find $M_t$:\n$$\nM_t = \\mathbb{E}[W_T + \\alpha T \\mid \\mathcal{F}_t].\n$$\nBy linearity of conditional expectation:\n$$\nM_t = \\mathbb{E}[W_T \\mid \\mathcal{F}_t] + \\mathbb{E}[\\alpha T \\mid \\mathcal{F}_t].\n$$\nThe problem explicitly states to use the property $\\mathbb{E}[W_T \\mid \\mathcal{F}_t] = W_t$, which is a consequence of the fact that standard Brownian motion is a martingale. The term $\\alpha T$ is a deterministic constant, so its conditional expectation is itself. Therefore,\n$$\nM_t = W_t + \\alpha T.\n$$\nWe now have two expressions for the same martingale $M_t$: its definition, $M_t = Y_t + \\alpha t$, and the expression we just derived, $M_t = W_t + \\alpha T$. Equating these two expressions allows us to solve for $Y_t$:\n$$\nY_t + \\alpha t = W_t + \\alpha T.\n$$\n$$\nY_t = W_t + \\alpha T - \\alpha t = W_t + \\alpha(T-t).\n$$\nThis provides the closed-form expression for the process $Y_t$.\n\nSecond, we derive the expression for $Z_t$. We do this by substituting the now-known expression for $Y_t$ back into the original BSDE definition:\n$$\nY_t = \\xi + \\int_t^T \\alpha \\,ds - \\int_t^T Z_s \\,dW_s.\n$$\nSubstituting $Y_t = W_t + \\alpha(T-t)$ and $\\xi = W_T$:\n$$\nW_t + \\alpha(T-t) = W_T + \\int_t^T \\alpha \\,ds - \\int_t^T Z_s \\,dW_s.\n$$\nThe integral $\\int_t^T \\alpha \\,ds$ is equal to $\\alpha(T-t)$. So, the equation becomes:\n$$\nW_t + \\alpha(T-t) = W_T + \\alpha(T-t) - \\int_t^T Z_s \\,dW_s.\n$$\nThe term $\\alpha(T-t)$ appears on both sides and can be cancelled:\n$$\nW_t = W_T - \\int_t^T Z_s \\,dW_s.\n$$\nRearranging this equation to isolate the stochastic integral involving $Z_s$:\n$$\n\\int_t^T Z_s \\,dW_s = W_T - W_t.\n$$\nThe right-hand side, $W_T - W_t$, is the increment of the Brownian motion. By the definition of the Itô integral, this increment can be represented as a stochastic integral of the constant function $1$:\n$$\nW_T - W_t = \\int_t^T 1 \\,dW_s.\n$$\nComparing the two expressions, we obtain the identity:\n$$\n\\int_t^T Z_s \\,dW_s = \\int_t^T 1 \\,dW_s.\n$$\nThis implies that for all $t \\in [0, T]$,\n$$\n\\int_t^T (Z_s - 1) \\,dW_s = 0.\n$$\nA fundamental property of the Itô stochastic integral is its uniqueness: if the integral of an adapted process is zero over all such intervals, the process itself must be zero (in the sense of being zero almost everywhere with respect to the Lebesgue measure on time, for almost every path). This follows from the Itô isometry, which states that $\\mathbb{E}[(\\int_t^T f(s) dW_s)^2] = \\mathbb{E}[\\int_t^T f(s)^2 ds]$. Applying this here:\n$$\n\\mathbb{E}\\left[\\left(\\int_t^T (Z_s - 1) \\,dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_t^T (Z_s - 1)^2 \\,ds\\right] = 0.\n$$\nSince the integrand $(Z_s - 1)^2$ is non-negative, its integral over $[t, T]$ is also non-negative. For the expectation of this non-negative random variable to be zero, the random variable itself must be zero almost surely. That is, $\\int_t^T (Z_s - 1)^2 \\,ds=0$ a.s. This in turn implies the integrand must be zero for almost every $s \\in [t, T]$. As this holds for any $tT$, we conclude that\n$$\nZ_s = 1 \\quad \\text{for almost every } s \\in [0, T].\n$$\nThus, the process $Z_t$ is the constant process $Z_t=1$.\n\nThe solution to the BSDE is the pair $(Y_t, Z_t)$:\n$$\nY_t = W_t + \\alpha(T-t)\n$$\n$$\nZ_t = 1\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nW_t + \\alpha(T-t)  1\n\\end{pmatrix}\n}\n$$", "id": "3040130"}, {"introduction": "Having solved a BSDE from first principles, we now explore one of its most powerful applications: the deep connection to partial differential equations (PDEs). This practice guides you through a concrete example of the Feynman-Kac formula, revealing how the solution pair $(Y_t, Z_t)$ of a BSDE corresponds to a classical PDE solution and its gradient. You will use Itô's formula to make this connection explicit, highlighting the role of BSDEs as a potent probabilistic tool for solving analytical problems. [@problem_id:3040157]", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P}\\right)$ be a filtered probability space supporting a one-dimensional standard Brownian motion $W$. Fix constants $\\sigma0$, $\\beta\\in\\mathbb{R}$, $T0$, and $x\\in\\mathbb{R}$. Consider the forward stochastic differential equation $dX_{t}=\\sigma\\,dW_{t}$ with $X_{0}=x$, and the backward stochastic differential equation (BSDE) with zero driver and terminal condition $Y_{T}=g(X_{T})$ where $g(x)=\\exp(\\beta x)$.\n\nDefine the Markovian candidate $u(t,x):=\\mathbb{E}\\!\\left[g(X_{T})\\mid X_{t}=x\\right]$ and the process $Y_{t}:=u(t,X_{t})$. Using only Itô’s formula, properties of conditional expectations, and the stochastic differential equations definitions above, carry out the following steps:\n\n- Compute the explicit closed-form expression for $u(t,x)$ and verify that $u$ is differentiable in $t$ and twice differentiable in $x$.\n- Apply Itô’s formula to $u(t,X_{t})$ to identify the $(\\mathcal{F}_{t})$-adapted process $Z_{t}$ appearing in the martingale representation $dY_{t}=Z_{t}\\,dW_{t}$ for the zero-driver BSDE with $Y_{T}=g(X_{T})$.\n- Provide the resulting closed-form analytic expression for $Z_{t}$ in terms of $t$, $X_{t}$, and the parameters.\n\nYour final answer must be a single closed-form analytic expression for $Z_{t}$. Do not approximate or round.", "solution": "### Solution Derivation\n\nThe solution follows the three steps outlined in the problem statement.\n\n**Part 1: Compute the explicit expression for $u(t,x)$**\n\nThe forward process is given by the SDE $dX_{s}=\\sigma\\,dW_{s}$. To find the distribution of $X_{T}$ conditional on $X_{t}=x$, we solve this SDE over the interval $[t, T]$ with the starting condition $X_{t}=x$.\nIntegrating the SDE from $t$ to $T$ yields:\n$$ X_{T} = X_{t} + \\int_{t}^{T} \\sigma \\,dW_{s} $$\nGiven $X_{t}=x$, we have:\n$$ X_{T} = x + \\sigma (W_{T}-W_{t}) $$\nThe increment of a standard Brownian motion, $W_{T}-W_{t}$, is a normally distributed random variable with mean $0$ and variance $T-t$. That is, $W_{T}-W_{t} \\sim \\mathcal{N}(0, T-t)$.\nConsequently, conditional on $X_{t}=x$, the random variable $X_{T}$ is also normally distributed:\n$$ X_{T} \\mid (X_{t}=x) \\sim \\mathcal{N}(x, \\sigma^2(T-t)) $$\nThe function $u(t,x)$ is defined as the conditional expectation of $g(X_T)$:\n$$ u(t,x) = \\mathbb{E}\\!\\left[g(X_{T})\\mid X_{t}=x\\right] = \\mathbb{E}\\!\\left[\\exp(\\beta X_{T})\\mid X_{t}=x\\right] $$\nThis is the moment-generating function of the normal distribution $\\mathcal{N}(x, \\sigma^2(T-t))$ evaluated at $\\beta$. For a random variable $V \\sim \\mathcal{N}(\\mu, \\Sigma^2)$, its moment-generating function is $M_V(k) = \\mathbb{E}[\\exp(kV)] = \\exp(k\\mu + \\frac{1}{2}k^2\\Sigma^2)$.\nIn our case, $\\mu=x$, $\\Sigma^2 = \\sigma^2(T-t)$, and $k=\\beta$. Therefore, the explicit closed-form expression for $u(t,x)$ is:\n$$ u(t,x) = \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) $$\nTo verify differentiability, we compute the partial derivatives:\n$$ \\frac{\\partial u}{\\partial t}(t,x) = \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) \\cdot \\left(-\\frac{1}{2}\\beta^2\\sigma^2\\right) = -\\frac{1}{2}\\beta^2\\sigma^2 u(t,x) $$\n$$ \\frac{\\partial u}{\\partial x}(t,x) = \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) \\cdot \\beta = \\beta u(t,x) $$\n$$ \\frac{\\partial^2 u}{\\partial x^2}(t,x) = \\frac{\\partial}{\\partial x}(\\beta u(t,x)) = \\beta^2 u(t,x) = \\beta^2 \\exp\\left(\\beta x + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) $$\nSince the exponential function is infinitely differentiable and all parameters are constants, these derivatives exist and are continuous for $(t,x) \\in [0,T) \\times \\mathbb{R}$. Thus, $u$ is at least $C^{1,2}$, satisfying the differentiability requirement.\n\n**Part 2: Apply Itô’s formula to $Y_t = u(t,X_t)$**\n\nWe apply Itô’s formula for a function of time and a stochastic process. For $Y_t = u(t, X_t)$, the formula is:\n$$ dY_{t} = \\frac{\\partial u}{\\partial t}(t,X_{t})\\,dt + \\frac{\\partial u}{\\partial x}(t,X_{t})\\,dX_{t} + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(t,X_{t})\\,d\\langle X \\rangle_{t} $$\nWe have the dynamics of $X_t$, which is $dX_t = \\sigma\\,dW_t$. The quadratic variation of $X_t$ is $d\\langle X \\rangle_{t} = (\\sigma)^2\\,dt = \\sigma^2\\,dt$.\nNow, we substitute the expressions for the partial derivatives (evaluated at $(t, X_t)$) and the dynamics of $X_t$ into Itô's formula:\n$$ dY_{t} = \\left(-\\frac{1}{2}\\beta^2\\sigma^2 u(t,X_{t})\\right)dt + \\left(\\beta u(t,X_{t})\\right)(\\sigma\\,dW_{t}) + \\frac{1}{2}\\left(\\beta^2 u(t,X_{t})\\right)(\\sigma^2\\,dt) $$\nWe collect the $dt$ and $dW_t$ terms:\n$$ dY_{t} = \\left(-\\frac{1}{2}\\beta^2\\sigma^2 u(t,X_{t}) + \\frac{1}{2}\\beta^2\\sigma^2 u(t,X_{t})\\right)dt + \\left(\\beta \\sigma u(t,X_{t})\\right)dW_{t} $$\nThe terms in the coefficient of $dt$ cancel out:\n$$ dY_{t} = (0)\\,dt + \\left(\\beta \\sigma u(t,X_{t})\\right)dW_{t} $$\n$$ dY_{t} = \\beta \\sigma u(t,X_{t})\\,dW_{t} $$\nThe problem states that the pair $(Y_t, Z_t)$ solves the BSDE, which in the general case is written as $dY_t = f(t, X_t, Y_t, Z_t)dt + Z_t dW_t$. For a zero-driver BSDE, $f=0$, so the representation is $dY_t = Z_t dW_t$. By comparing this form with our result from Itô's formula, we can identify the process $Z_t$.\n\n**Part 3: Provide the closed-form analytic expression for $Z_t$**\n\nFrom the previous step, we have $dY_{t} = \\left(\\beta \\sigma u(t,X_{t})\\right)dW_{t}$. Comparing this with the martingale representation $dY_t = Z_t dW_t$, we identify $Z_t$ as:\n$$ Z_t = \\beta \\sigma u(t,X_{t}) $$\nSubstituting the explicit expression for $u(t,X_{t})$:\n$$ Z_t = \\beta \\sigma \\exp\\left(\\beta X_t + \\frac{1}{2}\\beta^2\\sigma^2(T-t)\\right) $$\nThis is the required closed-form analytic expression for the process $Z_t$ in terms of $t$, $X_t$, and the given parameters.", "answer": "$$\\boxed{\\beta\\sigma\\exp\\left(\\beta X_{t} + \\frac{1}{2}\\beta^{2}\\sigma^{2}(T-t)\\right)}$$", "id": "3040157"}, {"introduction": "While analytical solutions offer deep insight, most real-world applications of BSDEs rely on numerical methods. This final hands-on practice bridges the gap between abstract theory and practical computation by guiding you to implement a numerical solver for a BSDE. By coding a regression-based algorithm, you will gain experience with a powerful technique used extensively in quantitative finance and engineering to find solutions where no closed-form expression exists. [@problem_id:3040102]", "problem": "Consider a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$ on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$. Let the forward process $X=(X_t)_{t\\in[0,T]}$ satisfy the stochastic differential equation $dX_t=\\sigma\\,dW_t$ with $X_0=x_0$ and constant volatility $\\sigma0$. Consider the Backward Stochastic Differential Equation (BSDE) defined by $Y_t=g(X_T)+\\int_t^T f(s,Y_s,Z_s)\\,ds-\\int_t^T Z_s\\,dW_s$, where $g:\\mathbb{R}\\to\\mathbb{R}$ is a terminal payoff and $f:[0,T]\\times\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ is a driver function. In this problem, take the terminal function $g(x)=x^2$ and the driver $f(t,y,z)=\\lambda\\,y+\\mu\\,z$ for constants $\\lambda\\in\\mathbb{R}$ and $\\mu\\in\\mathbb{R}$.\n\nStarting from the fundamental definition of a Backward Stochastic Differential Equation (BSDE) and the standard Euler discretization for stochastic differential equations, construct an Euler time discretization for the BSDE on a uniform grid $0=t_0t_1\\cdotst_M=T$ with time step $\\Delta t=T/M$. Use the backward recursion $Y_{t_k}=\\mathbb{E}\\!\\left[Y_{t_{k+1}}+f(t_k,Y_{t_{k+1}},Z_{t_k})\\,\\Delta t\\mid\\mathcal{F}_{t_k}\\right]$ where $Z_{t_k}$ is approximated via regression on a fixed polynomial basis of the forward state $X_{t_k}$. Specifically, estimate $Z_{t_k}$ using Least Squares (LS) regression of the target $U_k=\\frac{Y_{t_{k+1}}\\,\\Delta W_k}{\\Delta t}$ onto the basis $\\phi(X_{t_k})=(1,X_{t_k},X_{t_k}^2)$, where $\\Delta W_k=W_{t_{k+1}}-W_{t_k}$. Then estimate $Y_{t_k}$ by LS regression of the target $V_k=Y_{t_{k+1}}+f(t_k,Y_{t_{k+1}},\\widehat{Z}_{t_k})\\,\\Delta t$ onto the same basis, with $\\widehat{Z}_{t_k}$ the regression estimate.\n\nYour program must:\n- Simulate $N$ independent paths of the forward process $X$ using the Euler scheme with the given parameters.\n- At $t_M=T$, set $Y_{t_M}=g(X_{t_M})$ pathwise.\n- For $k=M-1,M-2,\\ldots,0$, perform the two-stage LS regression described above to obtain pathwise estimates of $\\widehat{Z}_{t_k}$ and $\\widehat{Y}_{t_k}$. Use the polynomial basis $\\phi(x)=(1,x,x^2)$ at each time level.\n- Return the final estimate of $Y_{t_0}$ as a single float for each test case.\n\nAll random number generation must be reproducible by fixing the seed specified in each test case. All quantities in this problem are dimensionless, so no unit specification is required.\n\nImplement the algorithm for the following test suite of parameter values, using the notation $(T,M,N,x_0,\\sigma,\\lambda,\\mu,\\text{seed})$:\n- Test case $1$: $(T=\\;1,\\;M=\\;40,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0,\\;\\mu=\\;0,\\;\\text{seed}=\\;12345)$, a baseline case where the true value satisfies $Y_{0}=\\mathbb{E}[W_T^2]=T$.\n- Test case $2$: $(T=\\;0.5,\\;M=\\;30,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0.3,\\;\\mu=\\;0,\\;\\text{seed}=\\;23456)$, a case with a nonzero linear dependence on $Y$ in the driver.\n- Test case $3$: $(T=\\;1,\\;M=\\;40,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0,\\;\\mu=\\;0.2,\\;\\text{seed}=\\;34567)$, a case with a nonzero linear dependence on $Z$ in the driver.\n- Test case $4$: $(T=\\;0.1,\\;M=\\;10,\\;N=\\;5000,\\;x_0=\\;0,\\;\\sigma=\\;1,\\;\\lambda=\\;0.5,\\;\\mu=\\;-0.1,\\;\\text{seed}=\\;45678)$, a small-horizon edge case with mixed signs in the driver coefficients.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the estimated value of $Y_{0}$ for the corresponding test case.", "solution": "### Solution Derivation\n\nThe problem requires the implementation of a regression-based numerical method to solve a specific BSDE. The method proceeds backward in time, from $T$ to $0$.\n\n1.  **Forward Path Simulation**: First, we simulate $N$ paths of the forward process $X_t$. The SDE is $dX_t = \\sigma dW_t$ with $X_0 = x_0$. Using an Euler-Maruyama scheme on the time grid $t_k = k\\Delta t$ for $k=0, \\dots, M$, we have:\n    $$\n    X_{t_{k+1}} = X_{t_k} + \\sigma \\Delta W_k\n    $$\n    where $\\Delta W_k = W_{t_{k+1}} - W_{t_k}$ are independent random variables drawn from a normal distribution $\\mathcal{N}(0, \\Delta t)$. We generate and store the paths $\\{X_{t_k}^{(i)}\\}_{k=0,\\dots,M}$ for each simulation $i=1,\\dots,N$.\n\n2.  **Terminal Condition**: The backward recursion starts at the terminal time $t_M = T$. For each path $i$, the value of $Y$ is given by the terminal function $g$:\n    $$\n    \\widehat{Y}_{t_M}^{(i)} = g(X_{t_M}^{(i)}) = (X_{t_M}^{(i)})^2\n    $$\n    This gives us a vector of $N$ values for $Y$ at time $T$.\n\n3.  **Backward Recursion Loop**: We iterate backward from $k = M-1$ down to $0$. At each step $k$, we have the estimates $\\widehat{Y}_{t_{k+1}}^{(i)}$ from the previous step (time $t_{k+1}$) and aim to compute $\\widehat{Y}_{t_k}^{(i)}$. This is done in two stages.\n\n    **Stage A: Estimating $Z_{t_k}$**\n    The process $Z_t$ is related to $Y_t$ and $W_t$ through the martingale representation part of the BSDE. A common approximation stems from the discretized SDE for $Y_t$: $Y_{t_{k+1}} - Y_{t_k} \\approx -f(\\dots)\\Delta t + Z_{t_k}\\Delta W_k$. Multiplying by $\\Delta W_k$ and taking conditional expectation $\\mathbb{E}[\\cdot \\mid \\mathcal{F}_{t_k}]$, we use $\\mathbb{E}[\\Delta W_k \\mid \\mathcal{F}_{t_k}]=0$ and $\\mathbb{E}[(\\Delta W_k)^2 \\mid \\mathcal{F}_{t_k}]=\\Delta t$ to show that $Z_{t_k} \\approx \\frac{1}{\\Delta t} \\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}]$.\n    The problem specifies approximating this conditional expectation by assuming $Z_{t_k}$ is a function of $X_{t_k}$ within the space spanned by $\\phi(x) = (1, x, x^2)$. We perform a least-squares regression of the sample values of the target $U_k^{(i)} = \\frac{\\widehat{Y}_{t_{k+1}}^{(i)}\\Delta W_k^{(i)}}{\\Delta t}$ on the basis functions evaluated at the states $X_{t_k}^{(i)}$. Let $\\mathbf{A}_k$ be the $N \\times 3$ matrix whose rows are $\\phi(X_{t_k}^{(i)})$. We solve for the coefficient vector $\\mathbf{c}_{Z,k}$:\n    $$\n    \\mathbf{c}_{Z,k} = \\arg\\min_{\\mathbf{c} \\in \\mathbb{R}^3} \\sum_{i=1}^N \\left( \\left( \\mathbf{A}_k \\mathbf{c} \\right)_i - U_k^{(i)} \\right)^2\n    $$\n    The pathwise estimates for $Z_{t_k}$ are then given by $\\widehat{\\mathbf{Z}}_{t_k} = \\mathbf{A}_k \\mathbf{c}_{Z,k}$.\n\n    **Stage B: Estimating $Y_{t_k}$**\n    The recursion for $Y_{t_k}$ is given by $Y_{t_k} = \\mathbb{E}[V_k \\mid \\mathcal{F}_{t_k}]$, where $V_k = Y_{t_{k+1}} + f(t_k, Y_{t_{k+1}}, \\widehat{Z}_{t_k}) \\Delta t$.\n    Again, we approximate the conditional expectation via regression on the same basis $\\phi(X_{t_k})$. We compute the sample values of the target for each path:\n    $$\n    V_k^{(i)} = \\widehat{Y}_{t_{k+1}}^{(i)} + \\left( \\lambda \\widehat{Y}_{t_{k+1}}^{(i)} + \\mu \\widehat{Z}_{t_k}^{(i)} \\right) \\Delta t\n    $$\n    We then solve the least-squares problem for the coefficient vector $\\mathbf{c}_{Y,k}$:\n    $$\n    \\mathbf{c}_{Y,k} = \\arg\\min_{\\mathbf{c} \\in \\mathbb{R}^3} \\sum_{i=1}^N \\left( \\left( \\mathbf{A}_k \\mathbf{c} \\right)_i - V_k^{(i)} \\right)^2\n    $$\n    The new estimates for $Y$ at time $t_k$ are $\\widehat{\\mathbf{Y}}_{t_k} = \\mathbf{A}_k \\mathbf{c}_{Y,k}$. This vector of estimates is then used as $\\widehat{\\mathbf{Y}}_{t_{k+1}}$ in the next iteration of the loop (for time $t_{k-1}$).\n\n4.  **Final Result**: The loop continues until $k = 0$. At this final step, we compute the coefficients $\\mathbf{c}_{Y,0}$. The estimated value of $Y_0$ is the value of the resulting regression function evaluated at the initial state $X_0 = x_0$. The estimated function is $\\widehat{y}_0(x) = (\\mathbf{c}_{Y,0})_1 \\cdot 1 + (\\mathbf{c}_{Y,0})_2 \\cdot x + (\\mathbf{c}_{Y,0})_3 \\cdot x^2$. Since all test cases have $x_0=0$, the regression matrix $\\mathbf{A}_0$ consists of $N$ identical rows $(1, 0, 0)$. In this specific case, evaluating $\\widehat{y}_0(0)$ simply yields the first coefficient, $(\\mathbf{c}_{Y,0})_1$. More generally, the least-squares solution for the evaluated function $\\widehat{y}_0(x_0)$ is precisely the sample mean of the regression targets $V_0^{(i)}$. The implemented algorithm will compute `poly_basis @ coeffs_Y`, which correctly evaluates to this mean value for all paths, as $\\mathbf{A}_0 \\mathbf{c}_{Y,0}$ simplifies to this result. Therefore, the estimate for $Y_0$ is the first (and only unique) element of the final vector $\\widehat{\\mathbf{Y}}_{t_0}$.\n\nThis procedure is implemented for each set of parameters provided in the test suite.", "answer": "```python\nimport numpy as np\n\ndef solve_bsde_regression(T, M, N, x0, sigma, lam, mu, seed):\n    \"\"\"\n    Solves a Backward Stochastic Differential Equation (BSDE) using a\n    regression-based Euler scheme (Longstaff-Schwartz type method).\n\n    Parameters:\n    T (float): Time horizon.\n    M (int): Number of time steps.\n    N (int): Number of simulated paths.\n    x0 (float): Initial value of the forward process X.\n    sigma (float): Volatility of the forward process X.\n    lam (float): Coefficient for the Y term in the BSDE driver.\n    mu (float): Coefficient for the Z term in the BSDE driver.\n    seed (int): Seed for the random number generator.\n\n    Returns:\n    float: The estimated value of Y_0.\n    \"\"\"\n    # Time step size\n    dt = T / M\n    \n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Forward Path Simulation\n    # Generate N paths of Brownian increments\n    dW = rng.normal(0, np.sqrt(dt), size=(N, M))\n    \n    # Simulate N paths of the forward process X_t\n    X = np.zeros((N, M + 1))\n    X[:, 0] = x0\n    # Use cumsum for efficient path generation\n    X[:, 1:] = x0 + sigma * np.cumsum(dW, axis=1)\n\n    # 2. Terminal Condition for Y\n    # At time T, Y_T = g(X_T) = X_T^2\n    Y = X[:, -1]**2\n\n    # 3. Backward Recursion Loop\n    for k in range(M - 1, -1, -1):\n        # Get states and increments for the current time step t_k\n        X_k = X[:, k]\n        dW_k = dW[:, k]\n        \n        # Construct the regression basis matrix from state X_k\n        # Basis is phi(x) = (1, x, x^2)\n        poly_basis = np.stack([np.ones(N), X_k, X_k**2], axis=1)\n\n        # --------- Stage A: Estimate Z_k ---------\n        # Define the regression target for Z_k\n        target_Z = Y * dW_k / dt\n        \n        # Perform least-squares regression to find coefficients for Z_k\n        coeffs_Z = np.linalg.lstsq(poly_basis, target_Z, rcond=None)[0]\n        \n        # Compute the pathwise estimate of Z_k using the regression model\n        Z_k = poly_basis @ coeffs_Z\n\n        # --------- Stage B: Estimate Y_k ---------\n        # Define the regression target for Y_k\n        # V_k = Y_{k+1} + f(t_k, Y_{k+1}, Z_k_hat) * dt\n        # f(t,y,z) = lam*y + mu*z\n        target_Y = Y + (lam * Y + mu * Z_k) * dt\n        \n        # Perform least-squares regression to find coefficients for Y_k\n        coeffs_Y = np.linalg.lstsq(poly_basis, target_Y, rcond=None)[0]\n        \n        # Compute the pathwise estimate for Y_k. This becomes Y_{k+1} for the next loop.\n        Y = poly_basis @ coeffs_Y\n\n    # 4. Final Result\n    # At k=0, X_k is a constant x0 vector. The regression result Y will be a\n    # constant vector where each element is the mean of target_Y.\n    # We return this single value.\n    return Y[0]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the BSDE solver on the suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (T, M, N, x0, sigma, lambda, mu, seed)\n        (1.0, 40, 5000, 0.0, 1.0, 0.0, 0.0, 12345),\n        (0.5, 30, 5000, 0.0, 1.0, 0.3, 0.0, 23456),\n        (1.0, 40, 5000, 0.0, 1.0, 0.0, 0.2, 34567),\n        (0.1, 10, 5000, 0.0, 1.0, 0.5, -0.1, 45678),\n    ]\n\n    results = []\n    for params in test_cases:\n        T, M, N, x0, sigma, lam, mu, seed = params\n        y0_estimate = solve_bsde_regression(T, M, N, x0, sigma, lam, mu, seed)\n        results.append(y0_estimate)\n\n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3040102"}]}