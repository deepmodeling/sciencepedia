{"hands_on_practices": [{"introduction": "Before tackling complex, coupled systems, it is essential to master the solution techniques for simpler cases. This practice focuses on a decoupled linear Forward-Backward Stochastic Differential Equation (FBSDE), a class of problems that can be solved analytically. By deriving a closed-form solution [@problem_id:3054739], you will gain hands-on experience with fundamental tools like the change of measure technique and the solution formula for linear Backward Stochastic Differential Equations (BSDEs), providing a solid benchmark for more advanced applications.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$, with a fixed horizon $T>0$. The forward component of a decoupled Forward-Backward Stochastic Differential Equation (FBSDE) is given by the stochastic differential equation\n$$\ndX_t=\\mu\\,dt+\\sigma\\,dW_t,\\qquad X_0=x_0,\n$$\nwhere $\\mu\\in\\mathbb{R}$ and $\\sigma>0$ are constants, and $x_0\\in\\mathbb{R}$ is deterministic. The backward component $(Y,Z)$ satisfies\n$$\ndY_t=-f(t,X_t,Y_t,Z_t)\\,dt+Z_t\\,dW_t,\\qquad Y_T=g(X_T),\n$$\nwith the driver and terminal function specified by\n$$\nf(t,x,y,z)=\\alpha\\,y+\\beta\\,z+\\gamma,\\qquad g(x)=\\lambda\\,x+\\delta,\n$$\nwhere $\\alpha\\in\\mathbb{R}\\setminus\\{0\\}$, $\\beta,\\gamma,\\lambda,\\delta\\in\\mathbb{R}$ are constants. Assume all usual conditions (including integrability and the Novikov condition for the constant $\\beta$) ensuring existence and uniqueness of a square-integrable solution $(Y,Z)$.\n\nStarting from the fundamental definitions of Itô processes and the backward stochastic differential equation, derive an explicit closed-form expression for $Y_0$ in terms of $x_0$, $T$, and the constants $\\mu$, $\\sigma$, $\\alpha$, $\\beta$, $\\gamma$, $\\lambda$, and $\\delta$. Your final answer must be a single closed-form analytic expression. No rounding is required and no physical units are involved.", "solution": "The problem provides a decoupled forward-backward stochastic differential equation (FBSDE) system and asks for the value of the backward process at time $t=0$, denoted as $Y_0$. The system is defined on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ with a standard one-dimensional Brownian motion $W_t$.\n\nThe forward process $X_t$ is an arithmetic Brownian motion given by the stochastic differential equation (SDE):\n$$dX_t = \\mu\\,dt + \\sigma\\,dW_t, \\quad X_0 = x_0$$\nwhere $\\mu \\in \\mathbb{R}$, $\\sigma > 0$, and $x_0 \\in \\mathbb{R}$ are constants.\n\nThe backward process $(Y_t, Z_t)$ is defined by the backward SDE (BSDE):\n$$dY_t = -f(t,X_t,Y_t,Z_t)\\,dt + Z_t\\,dW_t, \\quad Y_T = g(X_T)$$\nThe driver function $f$ and terminal function $g$ are linear:\n$$f(t,x,y,z) = \\alpha y + \\beta z + \\gamma$$\n$$g(x) = \\lambda x + \\delta$$\nwhere $\\alpha \\in \\mathbb{R}\\setminus\\{0\\}$, and $\\beta, \\gamma, \\lambda, \\delta \\in \\mathbb{R}$ are constants. The problem assumes conditions for the existence of a unique square-integrable solution pair $(Y_t, Z_t)$.\n\nSubstituting the specific form of the driver $f$ into the BSDE yields:\n$$dY_t = -(\\alpha Y_t + \\beta Z_t + \\gamma)\\,dt + Z_t\\,dW_t$$\nThis is a linear BSDE. A standard method to solve this is to use a change of measure to eliminate the term involving $Z_t$ in the drift. We define a new probability measure $\\mathbb{Q}$ equivalent to $\\mathbb{P}$. Let $M_t = \\int_0^t \\beta \\, dW_s = \\beta W_t$. The Doléans-Dade exponential of $M_t$ is $\\mathcal{E}(M)_t = \\exp(\\beta W_t - \\frac{1}{2}\\beta^2 t)$. We define the Radon-Nikodym derivative as $\\frac{d\\mathbb{Q}}{d\\mathbb{P}}|_{\\mathcal{F}_T} = \\mathcal{E}(M)_T$. The assumption that the Novikov condition is satisfied ensures that $\\mathcal{E}(M)_t$ is a martingale and $\\mathbb{E}[\\mathcal{E}(M)_T] = 1$.\n\nBy Girsanov's theorem, the process $\\widetilde{W}_t$ defined by\n$$\\widetilde{W}_t = W_t - \\int_0^t \\beta \\, ds = W_t - \\beta t$$\nis a standard Brownian motion under the measure $\\mathbb{Q}$. We can express $dW_t$ as $dW_t = d\\widetilde{W}_t + \\beta \\, dt$.\n\nNow, we rewrite both SDEs under the measure $\\mathbb{Q}$.\nThe forward SDE for $X_t$ becomes:\n$$dX_t = \\mu\\,dt + \\sigma(d\\widetilde{W}_t + \\beta\\,dt) = (\\mu + \\sigma\\beta)\\,dt + \\sigma\\,d\\widetilde{W}_t$$\nThe backward SDE for $Y_t$ becomes:\n$$dY_t = -(\\alpha Y_t + \\beta Z_t + \\gamma)\\,dt + Z_t(d\\widetilde{W}_t + \\beta\\,dt)$$\n$$dY_t = -(\\alpha Y_t + \\gamma)\\,dt - \\beta Z_t\\,dt + Z_t\\,\\beta\\,dt + Z_t\\,d\\widetilde{W}_t$$\n$$dY_t = -(\\alpha Y_t + \\gamma)\\,dt + Z_t\\,d\\widetilde{W}_t$$\nThis BSDE under $\\mathbb{Q}$ is simpler as its drift is independent of $Z_t$. To solve it, we use an integrating factor. Let's consider the process $e^{\\alpha t}Y_t$. Applying Itô's product rule:\n$$d(e^{\\alpha t}Y_t) = (\\alpha e^{\\alpha t}Y_t)\\,dt + e^{\\alpha t}\\,dY_t$$\nSubstituting the expression for $dY_t$:\n$$d(e^{\\alpha t}Y_t) = \\alpha e^{\\alpha t}Y_t\\,dt + e^{\\alpha t}(-(\\alpha Y_t + \\gamma)\\,dt + Z_t\\,d\\widetilde{W}_t)$$\n$$d(e^{\\alpha t}Y_t) = \\alpha e^{\\alpha t}Y_t\\,dt - \\alpha e^{\\alpha t}Y_t\\,dt - \\gamma e^{\\alpha t}\\,dt + e^{\\alpha t}Z_t\\,d\\widetilde{W}_t$$\n$$d(e^{\\alpha t}Y_t) = -\\gamma e^{\\alpha t}\\,dt + e^{\\alpha t}Z_t\\,d\\widetilde{W}_t$$\nIntegrating this equation from an arbitrary time $t \\in [0,T]$ to $T$:\n$$e^{\\alpha T}Y_T - e^{\\alpha t}Y_t = \\int_t^T -\\gamma e^{\\alpha s}\\,ds + \\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s$$\nWe can solve for $e^{\\alpha t}Y_t$:\n$$e^{\\alpha t}Y_t = e^{\\alpha T}Y_T + \\gamma \\int_t^T e^{\\alpha s}\\,ds - \\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s$$\nTaking the conditional expectation under $\\mathbb{Q}$ with respect to the filtration $\\mathcal{F}_t$:\n$$\\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha t}Y_t | \\mathcal{F}_t] = \\mathbb{E}_{\\mathbb{Q}}\\left[e^{\\alpha T}Y_T + \\gamma \\int_t^T e^{\\alpha s}\\,ds - \\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s \\bigg| \\mathcal{F}_t\\right]$$\nSince $Y_t$ is $\\mathcal{F}_t$-measurable, $\\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha t}Y_t | \\mathcal{F}_t] = e^{\\alpha t}Y_t$. The stochastic integral $\\int_t^T e^{\\alpha s}Z_s\\,d\\widetilde{W}_s$ is a $\\mathbb{Q}$-martingale starting from $0$ at time $t$, so its conditional expectation is zero. The term involving the integral of $\\gamma e^{\\alpha s}$ is deterministic.\n$$e^{\\alpha t}Y_t = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T | \\mathcal{F}_t] + \\gamma \\int_t^T e^{\\alpha s}\\,ds$$\nLet's evaluate the deterministic integral:\n$$\\int_t^T e^{\\alpha s}\\,ds = \\left[\\frac{e^{\\alpha s}}{\\alpha}\\right]_t^T = \\frac{1}{\\alpha}(e^{\\alpha T} - e^{\\alpha t})$$\nSubstituting this back and solving for $Y_t$:\n$$Y_t = e^{-\\alpha t} \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T | \\mathcal{F}_t] + \\frac{\\gamma}{\\alpha}e^{-\\alpha t}(e^{\\alpha T} - e^{\\alpha t})$$\n$$Y_t = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha(T-t)}Y_T | \\mathcal{F}_t] + \\frac{\\gamma}{\\alpha}(e^{\\alpha(T-t)} - 1)$$\nWe are asked to find $Y_0$. Setting $t=0$:\n$$Y_0 = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T | \\mathcal{F}_0] + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nSince $X_0=x_0$ is deterministic, the filtration $\\mathcal{F}_0$ is trivial. Thus, the conditional expectation is just the unconditional expectation under $\\mathbb{Q}$:\n$$Y_0 = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}Y_T] + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nNow substitute the terminal condition $Y_T = g(X_T) = \\lambda X_T + \\delta$:\n$$Y_0 = \\mathbb{E}_{\\mathbb{Q}}[e^{\\alpha T}(\\lambda X_T + \\delta)] + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\n$$Y_0 = \\lambda e^{\\alpha T}\\mathbb{E}_{\\mathbb{Q}}[X_T] + \\delta e^{\\alpha T} + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nThe final step is to compute $\\mathbb{E}_{\\mathbb{Q}}[X_T]$. The dynamics of $X_t$ under $\\mathbb{Q}$ are $dX_t = (\\mu + \\sigma\\beta)\\,dt + \\sigma\\,d\\widetilde{W}_t$. Integrating from $0$ to $T$:\n$$X_T = X_0 + \\int_0^T (\\mu + \\sigma\\beta)\\,ds + \\int_0^T \\sigma\\,d\\widetilde{W}_s = x_0 + (\\mu + \\sigma\\beta)T + \\sigma\\widetilde{W}_T$$\nTaking the expectation under $\\mathbb{Q}$ and using $\\mathbb{E}_{\\mathbb{Q}}[\\widetilde{W}_T]=0$:\n$$\\mathbb{E}_{\\mathbb{Q}}[X_T] = x_0 + (\\mu + \\sigma\\beta)T$$\nFinally, we substitute this into the expression for $Y_0$:\n$$Y_0 = \\lambda e^{\\alpha T}(x_0 + (\\mu + \\sigma\\beta)T) + \\delta e^{\\alpha T} + \\frac{\\gamma}{\\alpha}(e^{\\alpha T} - 1)$$\nThis can be expanded and rearranged to provide the final closed-form expression:\n$$Y_0 = \\lambda x_0 e^{\\alpha T} + \\lambda(\\mu + \\sigma\\beta)T e^{\\alpha T} + \\delta e^{\\alpha T} + \\frac{\\gamma}{\\alpha}e^{\\alpha T} - \\frac{\\gamma}{\\alpha}$$\n$$Y_0 = e^{\\alpha T}\\left(\\lambda x_0 + \\lambda(\\mu + \\sigma\\beta)T + \\delta + \\frac{\\gamma}{\\alpha}\\right) - \\frac{\\gamma}{\\alpha}$$\n$$Y_0 = \\exp(\\alpha T)\\left(\\lambda x_0 + \\lambda(\\mu + \\sigma\\beta)T + \\delta\\right) + \\frac{\\gamma}{\\alpha}(\\exp(\\alpha T) - 1)$$", "answer": "$$\\boxed{\\exp(\\alpha T)\\left( \\lambda x_0 + \\lambda(\\mu + \\sigma\\beta)T + \\delta \\right) + \\frac{\\gamma}{\\alpha}(\\exp(\\alpha T) - 1)}$$", "id": "3054739"}, {"introduction": "While analytical solutions are invaluable, most FBSDEs encountered in practice lack closed-form expressions and must be solved numerically. This exercise shifts our focus to computation by asking you to identify a consistent time-discretization scheme for a general FBSDE. Understanding how to correctly combine a forward-stepping method like Euler-Maruyama for the $X$ process with a backward-stepping recursion for the $(Y, Z)$ pair is the first step toward implementing your own FBSDE solver [@problem_id:3054605].", "problem": "Consider a one-dimensional forward-backward stochastic differential equation on the time interval $[0,T]$ driven by a standard Brownian motion, with the forward component given by\n$$\n\\mathrm{d}X_t = b(t,X_t)\\,\\mathrm{d}t + \\sigma(t,X_t)\\,\\mathrm{d}W_t, \\quad X_0 = x_0,\n$$\nand the backward component given in integral form by\n$$\nY_t = g(X_T) + \\int_t^T f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_t^T Z_s\\,\\mathrm{d}W_s,\n$$\nwhere $b$, $\\sigma$, $f$, and $g$ are suitably regular functions and $(W_t)_{t\\in[0,T]}$ is a standard Brownian motion. Let a uniform time grid be defined by $0=t_0<t_1<\\cdots<t_N=T$ with step size $h=T/N$, and denote Brownian increments by $\\Delta W_i = W_{t_{i+1}}-W_{t_i}$ and the $\\sigma$-algebra at time $t_i$ by $\\mathcal{F}_{t_i}$. You are to identify a scientifically consistent time discretization scheme in which the forward process $X$ is approximated by the Euler–Maruyama method, while the backward pair $(Y,Z)$ is approximated by a backward recursion using conditional expectations. Select the option that correctly specifies such a scheme.\n\nA. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\frac{1}{h}\\,\\mathbb{E}\\!\\left[\\,Y_{i+1}^h\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h + f\\!\\left(t_i,X_i^h,Y_{i+1}^h,Z_i^h\\right)\\,h \\,\\big|\\, \\mathcal{F}_{t_i} \\right],\n$$\nfor $i=N-1,\\dots,0$.\n\nB. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\frac{1}{\\sqrt{h}}\\,\\mathbb{E}\\!\\left[\\,Y_{i+1}^h\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = Y_{i+1}^h + f\\!\\left(t_i,X_i^h,Y_{i+1}^h,Z_i^h\\right)\\,h,\n$$\nfor $i=N-1,\\dots,0$.\n\nC. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\mathbb{E}\\!\\left[\\,Z_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right] + f\\!\\left(t_i,X_i^h,Y_i^h,Z_i^h\\right)\\,h,\n$$\nfor $i=N-1,\\dots,0$.\n\nD. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward recursion:\n$$\nY_N^h = g(X_N^h), \\quad Z_i^h = \\frac{1}{h}\\,\\mathbb{E}\\!\\left[\\,\\big(Y_{i+1}^h + f\\!\\left(t_i,X_i^h,Y_{i+1}^h,Z_{i+1}^h\\right)\\,h\\big)\\,\\Delta W_i \\,\\big|\\, \\mathcal{F}_{t_i} \\right], \\quad \nY_i^h = \\mathbb{E}\\!\\left[\\, Y_{i+1}^h \\,\\big|\\, \\mathcal{F}_{t_i} \\right],\n$$\nfor $i=N-1,\\dots,0$.\n\nE. Forward Euler–Maruyama for $X$: \n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i.\n$$\nBackward via a partial differential equation ansatz:\n$$\nY_i^h = u(t_i,X_i^h), \\quad Z_i^h = \\partial_x u(t_i,X_i^h)\\,\\sigma(t_i,X_i^h),\n$$\nwhere $u$ solves a semilinear parabolic partial differential equation with terminal condition $u(T,x)=g(x)$.", "solution": "The problem requires a numerical scheme for a coupled FBSDE system. The scheme must use the Euler-Maruyama method for the forward SDE and a backward recursion based on conditional expectations for the BSDE.\n\nThe Euler-Maruyama method for the forward SDE is correctly stated in all options:\n$$\nX_{i+1}^h = X_i^h + b(t_i,X_i^h)\\,h + \\sigma(t_i,X_i^h)\\,\\Delta W_i, \\quad X_0^h = x_0\n$$\n\nFor the backward process, we start from the BSDE's integral form between times $t_i$ and $t_{i+1}$:\n$$\nY_{t_i} = Y_{t_{i+1}} + \\int_{t_i}^{t_{i+1}} f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s\n$$\nTaking the conditional expectation with respect to $\\mathcal{F}_{t_i}$ and noting that $\\mathbb{E}[\\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s | \\mathcal{F}_{t_i}]=0$:\n$$\nY_{t_i} = \\mathbb{E}[Y_{t_{i+1}} | \\mathcal{F}_{t_i}] + \\mathbb{E}\\left[\\int_{t_i}^{t_{i+1}} f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s \\bigg| \\mathcal{F}_{t_i}\\right]\n$$\nUsing a first-order approximation for the integral, $\\int_{t_i}^{t_{i+1}} f(\\dots)\\,\\mathrm{d}s \\approx f(t_i,X_{t_i},Y_{t_{i+1}},Z_{t_i})h$, gives the update rule for the numerical approximation $Y_i^h$:\n$$\nY_i^h = \\mathbb{E}\\left[Y_{i+1}^h + f(t_i,X_i^h,Y_{i+1}^h,Z_i^h)h \\mid \\mathcal{F}_{t_i}\\right]\n$$\nTo find an expression for $Z_i^h$, we rearrange the BSDE equation, multiply by $\\Delta W_i$, and take conditional expectation.\n$$\n\\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s = Y_{t_{i+1}} - Y_{t_i} + \\int_{t_i}^{t_{i+1}} f(\\dots)\\,\\mathrm{d}s\n$$\nMultiplying by $\\Delta W_i = \\int_{t_i}^{t_{i+1}} \\mathrm{d}W_s$ and taking $\\mathbb{E}[\\cdot|\\mathcal{F}_{t_i}]$:\n$$\n\\mathbb{E}\\left[\\left(\\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}W_s\\right)\\Delta W_i \\mid \\mathcal{F}_{t_i}\\right] = \\mathbb{E}[(Y_{t_{i+1}} - Y_{t_i})\\Delta W_i \\mid \\mathcal{F}_{t_i}] + \\mathbb{E}\\left[\\left(\\int_{t_i}^{t_{i+1}} f(\\dots)\\,\\mathrm{d}s\\right)\\Delta W_i \\mid \\mathcal{F}_{t_i}\\right]\n$$\nThe left side, by Itô isometry, becomes $\\mathbb{E}[\\int_{t_i}^{t_{i+1}} Z_s\\,\\mathrm{d}s \\mid \\mathcal{F}_{t_i}] \\approx Z_{t_i}h$. On the right side, $\\mathbb{E}[(Y_{t_{i+1}} - Y_{t_i})\\Delta W_i \\mid \\mathcal{F}_{t_i}] = \\mathbb{E}[Y_{t_{i+1}}\\Delta W_i \\mid \\mathcal{F}_{t_i}]$. The term involving $f$ is of a higher order and is dropped in a first-order scheme. This yields the approximation:\n$$\nZ_{t_i}h \\approx \\mathbb{E}[Y_{t_{i+1}}\\Delta W_i | \\mathcal{F}_{t_i}]\n$$\nThus, the update rule for the numerical approximation $Z_i^h$ is:\n$$\nZ_i^h = \\frac{1}{h}\\mathbb{E}[Y_{i+1}^h \\Delta W_i | \\mathcal{F}_{t_i}]\n$$\nThe complete scheme is initialized at $Y_N^h = g(X_N^h)$ and iterates backward from $i=N-1$ to $0$.\n\nComparing this derived scheme with the given options:\n-   **A:** The formulas for $Y_N^h$, $Z_i^h$, and $Y_i^h$ all match our derivation. This is a standard explicit backward scheme for BSDEs.\n-   **B:** The scaling for $Z_i^h$ is wrong ($1/\\sqrt{h}$ instead of $1/h$), and the update for $Y_i^h$ is missing the conditional expectation.\n-   **C:** The update for $Z_i^h$ is incorrect; $Z$ is not a martingale. The update for $Y_i^h$ is an implicit one, which is different from the standard explicit scheme asked for.\n-   **D:** The update for $Y_i^h$ is missing the drift term from the driver $f$.\n-   **E:** This describes a different class of methods (four-step schemes) based on the PDE representation (Feynman-Kac formula), not a backward recursion using conditional expectations as requested.\n\nTherefore, option A is the only one that correctly describes the specified type of numerical scheme.", "answer": "$$\\boxed{A}$$", "id": "3054605"}, {"introduction": "In the backward numerical scheme from the previous practice, estimating the $Z$ process is a critical and non-obvious step. This practice delves into the theoretical underpinnings of one of the most common estimation techniques used in Monte Carlo methods. You will explore how the Itô isometry provides a rigorous justification for estimating $Z_{t_k}$ by regressing the product of the future value $Y_{t_{k+1}}$ and the Brownian increment $\\Delta W_k$ onto functions of the current state $X_{t_k}$ [@problem_id:3054590], connecting abstract stochastic calculus to a concrete computational procedure.", "problem": "Consider a Markovian forward-backward stochastic differential equation (FBSDE) on a filtered probability space with a standard Brownian motion $W$, where the forward component $X$ solves the stochastic differential equation\n$$\n\\mathrm{d}X_t = b(t,X_t)\\,\\mathrm{d}t + \\sigma(t,X_t)\\,\\mathrm{d}W_t,\\quad X_0 = x,\n$$\nand the backward component $(Y,Z)$ solves\n$$\nY_t = g(X_T) + \\int_t^T f(s,X_s,Y_s,Z_s)\\,\\mathrm{d}s - \\int_t^T Z_s\\,\\mathrm{d}W_s,\\quad t\\in[0,T].\n$$\nFix a uniform grid $0=t_0<t_1<\\cdots<t_N=T$ with time step $\\Delta t = t_{k+1}-t_k$ and Brownian increment $\\Delta W_k := W_{t_{k+1}}-W_{t_k}$. In Monte Carlo least-squares algorithms for backward stochastic differential equations (BSDEs), one often estimates $Z_{t_k}$ by regressing the random variable $Y_{t_{k+1}}\\Delta W_k$ on a finite set of basis functions of $X_{t_k}$.\n\nWhich of the following statements correctly explains and justifies, using the Itô isometry and basic conditional expectation properties, why regressing $Y_{t_{k+1}}\\Delta W_k$ on functions of $X_{t_k}$ produces an $L^2$-best approximation to $Z_{t_k}\\Delta t$ (and hence to $Z_{t_k}$ after division by $\\Delta t$), under standard Markovian and square-integrability assumptions?\n\nA. Because $\\Delta W_k$ is independent of $\\mathcal{F}_{t_k}$ with $\\mathbb{E}[\\Delta W_k]=0$ and $\\mathbb{E}[(\\Delta W_k)^2]=\\Delta t$, and because $Y_{t_{k+1}}-Y_{t_k} = -\\int_{t_k}^{t_{k+1}} f\\,\\mathrm{d}s + \\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s$, taking $\\mathbb{E}[\\cdot\\mid\\mathcal{F}_{t_k}]$ of the product with $\\Delta W_k$ and using the Itô isometry yields $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k\\mid\\mathcal{F}_{t_k}] \\approx Z_{t_k}\\Delta t$. In the Markovian case $Z_{t_k}=z(t_k,X_{t_k})$, so least-squares regression of $Y_{t_{k+1}}\\Delta W_k$ onto functions of $X_{t_k}$ computes the $L^2$-projection of $Z_{t_k}\\Delta t$ onto the chosen function space, justifying the estimator.\n\nB. By the Itô isometry, $\\mathbb{E}[Y_{t_{k+1}}^2]=\\mathbb{E}\\!\\left[\\int_0^{t_{k+1}} Z_s^2\\,\\mathrm{d}s\\right]$, so regressing $Y_{t_{k+1}}$ on functions of $X_{t_k}$ directly recovers $Z_{t_k}$ without using $\\Delta W_k$.\n\nC. Since $Y_{t_{k+1}}$ is independent of $\\Delta W_k$, $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k\\mid\\mathcal{F}_{t_k}]=0$, and therefore regression correctly yields $Z_{t_k}=0$.\n\nD. Because $\\mathbb{E}[\\Delta W_k\\mid\\mathcal{F}_{t_k}]\\neq 0$ when the drift $b$ is nonzero, a bias appears. Regressing on functions of $X_{t_k}$ removes this bias and reconstructs $Z_{t_k}$.\n\nE. The validity of the regression relies on identifying $Z_{t_k}$ with the Malliavin derivative of $Y_{t_{k+1}}$; Itô isometry is not relevant for justifying the use of $Y_{t_{k+1}}\\Delta W_k$ in regression.", "solution": "The core of the problem is to justify the approximation $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}] \\approx Z_{t_k}\\Delta t$ and explain its connection to least-squares regression.\n\n1.  **Decomposition and Conditional Expectation**:\n    We start with the BSDE dynamics over the interval $[t_k, t_{k+1}]$:\n    $$Y_{t_{k+1}} = Y_{t_k} - \\int_{t_k}^{t_{k+1}} f_s\\,\\mathrm{d}s + \\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s$$\n    We want to compute $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}]$. We multiply the right-hand side by $\\Delta W_k$ and analyze each term's conditional expectation:\n    -   $\\mathbb{E}[Y_{t_k}\\Delta W_k \\mid \\mathcal{F}_{t_k}] = Y_{t_k}\\mathbb{E}[\\Delta W_k \\mid \\mathcal{F}_{t_k}] = Y_{t_k} \\cdot 0 = 0$, because $Y_{t_k}$ is $\\mathcal{F}_{t_k}$-measurable and $\\Delta W_k$ is independent of $\\mathcal{F}_{t_k}$ with mean zero.\n    -   $-\\mathbb{E}[(\\int_{t_k}^{t_{k+1}} f_s\\,\\mathrm{d}s) \\Delta W_k \\mid \\mathcal{F}_{t_k}]$ is a higher-order term, approximately of size $O((\\Delta t)^{3/2})$, which is negligible in a first-order scheme.\n    -   $\\mathbb{E}[(\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s) \\Delta W_k \\mid \\mathcal{F}_{t_k}]$. This is the key term.\n\n2.  **Application of Itô Isometry**:\n    We use the property that for two Itô integrals over the same interval, $\\mathbb{E}[(\\int H_s dW_s)(\\int G_s dW_s) | \\mathcal{F}_{t_k}] = \\mathbb{E}[\\int H_s G_s ds | \\mathcal{F}_{t_k}]$.\n    Here, $H_s = Z_s$ and $G_s = 1$ (since $\\Delta W_k = \\int_{t_k}^{t_{k+1}} 1 \\cdot dW_s$). So,\n    $$\\mathbb{E}\\left[\\left(\\int_{t_k}^{t_{k+1}} Z_s\\,\\mathrm{d}W_s\\right) \\Delta W_k \\mid \\mathcal{F}_{t_k}\\right] = \\mathbb{E}\\left[\\int_{t_k}^{t_{k+1}} Z_s \\cdot 1\\,\\mathrm{d}s \\mid \\mathcal{F}_{t_k}\\right]$$\n    Approximating the integral for small $\\Delta t$ gives $\\mathbb{E}[Z_{t_k}\\Delta t \\mid \\mathcal{F}_{t_k}]$. Since $Z_{t_k}$ is $\\mathcal{F}_{t_k}$-measurable, this simplifies to $Z_{t_k}\\Delta t$.\n\n3.  **Putting it Together and Connecting to Regression**:\n    Combining the terms, we get the central result:\n    $$\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid \\mathcal{F}_{t_k}] \\approx Z_{t_k}\\Delta t$$\n    In a Markovian setting, any $\\mathcal{F}_{t_k}$-measurable variable is a function of the state $X_{t_k}$. Thus, $Z_{t_k} = z(t_k, X_{t_k})$ and conditioning on $\\mathcal{F}_{t_k}$ is equivalent to conditioning on $X_{t_k}$. The equation becomes $\\mathbb{E}[Y_{t_{k+1}}\\Delta W_k \\mid X_{t_k}] \\approx z(t_k, X_{t_k})\\Delta t$.\n    Least-squares regression of a variable $A$ on functions of $B$ is a numerical method to compute an approximation of the conditional expectation $\\mathbb{E}[A \\mid B]$. Therefore, regressing samples of $Y_{t_{k+1}}\\Delta W_k$ onto basis functions of $X_{t_k}$ is a valid way to estimate $Z_{t_k}\\Delta t$.\n\n-   **Option A** perfectly captures this entire line of reasoning. It correctly states the premises, invokes the Itô isometry, arrives at the correct approximation, and correctly connects it to least-squares regression in the Markovian context.\n-   **Option B** fundamentally misuses the Itô isometry and targets the wrong quantity ($\\mathbb{E}[Y_{t_{k+1}}|\\dots]$ instead of something related to $Z$).\n-   **Option C** makes a false assumption of independence between $Y_{t_{k+1}}$ and $\\Delta W_k$.\n-   **Option D** makes a false claim about the conditional expectation of a Brownian increment.\n-   **Option E** is misleading. While Malliavin calculus provides a deeper connection, it's incorrect to state that Itô isometry is irrelevant for this common numerical justification. The question specifically asks for a justification using Itô isometry.\n\nThus, option A provides the correct and complete explanation.", "answer": "$$\\boxed{A}$$", "id": "3054590"}]}