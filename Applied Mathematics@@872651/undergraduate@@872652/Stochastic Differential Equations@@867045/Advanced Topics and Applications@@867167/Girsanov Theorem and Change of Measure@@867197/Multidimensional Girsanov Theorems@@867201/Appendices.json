{"hands_on_practices": [{"introduction": "The Girsanov theorem provides a powerful way to change the probability measure governing a stochastic process, but this transformation is not always valid. A crucial part of applying the theorem is verifying that certain conditions are met. This first practice problem [@problem_id:3067604] offers a clear and concrete exercise in checking the well-known Novikov's condition for a simple, deterministic process, helping to build a solid foundation for understanding when a change of measure is permissible.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ satisfying the usual conditions and supporting a three-dimensional standard Brownian motion (BM) $(W_{t})_{t\\geq 0}$ adapted to $(\\mathcal{F}_{t})_{t\\geq 0}$. Let $T=2$. Construct a bounded predictable process $\\theta=(\\theta_{t})_{t\\in[0,T]}$ with values in $\\mathbb{R}^{3}$ by setting\n$$\n\\theta_{t}=\\begin{cases}\n(1,-2,3)  \\text{for } t\\in[0,1],\\\\\n(0,1,-1)  \\text{for } t\\in(1,2].\n\\end{cases}\n$$\nUsing the definition of predictability and boundedness for stochastic processes, verify that $\\theta$ is predictable and bounded. Then, starting only from the definitions of expectation, Euclidean norm, and Lebesgue integration on $[0,T]$, compute the quantity\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{T}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right],\n$$\nand conclude whether Novikov's condition for the multidimensional Girsanov theorem holds on $[0,T]$ for this choice of $\\theta$. Express your final answer as an exact analytic expression; no rounding is required.", "solution": "The problem asks for three things: first, to verify that the given process $\\theta = (\\theta_t)_{t \\in [0,T]}$ is predictable and bounded; second, to compute the value of a specific expectation involving $\\theta$; and third, to use this result to determine if Novikov's condition is met. We will address each part in sequence. The time horizon is $T=2$.\n\nFirst, we verify the properties of the process $\\theta_t$. The process is defined as:\n$$\n\\theta_{t}=\\begin{cases}\n(1,-2,3)  \\text{for } t\\in[0,1],\\\\\n(0,1,-1)  \\text{for } t\\in(1,2].\n\\end{cases}\n$$\nThis is a process with values in $\\mathbb{R}^3$.\n\n**Verification of Predictability and Boundedness**\n\nA process is predictable if it is measurable with respect to the predictable $\\sigma$-algebra on $[0,T] \\times \\Omega$. A sufficient condition for a process to be predictable is that it is adapted to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$ and its sample paths are left-continuous.\n\n1.  **Adaptedness**: The process $\\theta_t$ is a deterministic function of time $t$. For any given $t \\in [0,T]$, $\\theta_t$ is a constant vector. A constant random variable is measurable with respect to any $\\sigma$-algebra. Therefore, for each $t \\ge 0$, $\\theta_t$ is $\\mathcal{F}_t$-measurable. Thus, the process $(\\theta_t)_{t \\in [0,T]}$ is adapted to the filtration $(\\mathcal{F}_t)_{t \\ge 0}$.\n\n2.  **Left-Continuity**: We examine the sample paths $t \\mapsto \\theta_t$. Since the process is deterministic, we only need to check the function $t \\mapsto \\theta_t$ for left-continuity on the interval $(0,T]$. The function is constant on the intervals $[0,1]$ and $(1,2]$, so it is continuous at all points $t \\in (0,2]$ except possibly at $t=1$. We check the left limit at $t=1$:\n    $$ \\lim_{s \\to 1^{-}} \\theta_s = \\lim_{s \\to 1^{-}} (1, -2, 3) = (1, -2, 3) $$\n    The value of the process at $t=1$ is $\\theta_1 = (1, -2, 3)$. Since $\\lim_{s \\to 1^{-}} \\theta_s = \\theta_1$, the process is left-continuous at $t=1$. It is continuous (and thus left-continuous) at all other points in $(0,2]$. Therefore, the process $\\theta_t$ is left-continuous.\n\nSince $\\theta_t$ is both adapted and left-continuous, it is a predictable process.\n\n3.  **Boundedness**: A process $(\\theta_t)_{t \\in [0,T]}$ is bounded if there exists a constant $M  0$ such that $\\|\\theta_t\\| \\le M$ for all $t \\in [0,T]$. We compute the Euclidean norm $\\|\\theta_t\\|$ for $t \\in [0,2]$.\n    For $t \\in [0,1]$, the vector is $\\theta_t = (1, -2, 3)$. Its squared norm is $\\|\\theta_t\\|^2 = 1^2 + (-2)^2 + 3^2 = 1 + 4 + 9 = 14$. Thus, $\\|\\theta_t\\| = \\sqrt{14}$.\n    For $t \\in (1,2]$, the vector is $\\theta_t = (0, 1, -1)$. Its squared norm is $\\|\\theta_t\\|^2 = 0^2 + 1^2 + (-1)^2 = 0 + 1 + 1 = 2$. Thus, $\\|\\theta_t\\| = \\sqrt{2}$.\n\n    The set of values for $\\|\\theta_t\\|$ over the interval $[0,2]$ is $\\{\\sqrt{2}, \\sqrt{14}\\}$. The maximum value is $\\sqrt{14}$. We can choose $M = \\sqrt{14}$. For all $t \\in [0,2]$, we have $\\|\\theta_t\\| \\le \\sqrt{14}$. Hence, the process $\\theta_t$ is bounded.\n\n**Computation of the Expectation**\n\nWe are asked to compute $E^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{T}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right]$, with $T=2$. Let's first evaluate the integral inside the exponential.\nThe integrand is $\\|\\theta_s\\|^2$. Since $\\theta_s$ is a deterministic process, so is $\\|\\theta_s\\|^2$. The integral is a deterministic quantity, not a random variable.\nUsing the definition of the Lebesgue integral, we can split the domain of integration $[0,2]$ into $[0,1]$ and $(1,2]$:\n$$\n\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds = \\int_{0}^{1}\\|\\theta_{s}\\|^{2}\\,ds + \\int_{1}^{2}\\|\\theta_{s}\\|^{2}\\,ds\n$$\nOn the interval $s \\in [0,1]$, we have $\\|\\theta_s\\|^2 = 14$.\nOn the interval $s \\in (1,2]$, we have $\\|\\theta_s\\|^2 = 2$.\nThe integral is therefore:\n$$\n\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds = \\int_{0}^{1} 14 \\,ds + \\int_{1}^{2} 2 \\,ds = 14 \\cdot (1-0) + 2 \\cdot (2-1) = 14 + 2 = 16\n$$\nNow, we substitute this value back into the expression for the expectation:\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right] = E^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2} \\cdot 16\\right)\\right] = E^{\\mathbb{P}}[\\exp(8)]\n$$\nThe expression inside the expectation, $\\exp(8)$, is a constant. The expectation of a constant random variable is the constant itself. Formally, if $C$ is a constant, then by definition of expectation:\n$$\nE^{\\mathbb{P}}[C] = \\int_{\\Omega} C \\,d\\mathbb{P}(\\omega) = C \\int_{\\Omega} d\\mathbb{P}(\\omega) = C \\cdot \\mathbb{P}(\\Omega) = C \\cdot 1 = C\n$$\nIn our case, the constant is $\\exp(8)$. Therefore,\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right] = \\exp(8)\n$$\n\n**Conclusion on Novikov's Condition**\n\nNovikov's condition for the predictable process $\\theta$ on the time interval $[0,T]$ is the requirement that the following expectation is finite:\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{T}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right]  \\infty\n$$\nThis condition is a sufficient condition to ensure that the Doléans-Dade exponential $Z_t = \\exp\\left(\\int_0^t \\theta_s \\cdot dW_s - \\frac{1}{2}\\int_0^t \\|\\theta_s\\|^2 ds\\right)$ is a martingale on $[0,T]$.\n\nFrom our calculation above, with $T=2$, we found:\n$$\nE^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_{0}^{2}\\|\\theta_{s}\\|^{2}\\,ds\\right)\\right] = \\exp(8)\n$$\nSince $\\exp(8)$ is a positive, finite real number, we have $\\exp(8)  \\infty$.\nTherefore, Novikov's condition holds for the given process $\\theta$ on the interval $[0,2]$.", "answer": "$$\\boxed{\\exp(8)}$$", "id": "3067604"}, {"introduction": "With an understanding of the conditions for a valid measure change, we can now turn to the primary application of the Girsanov theorem: altering the dynamics of a stochastic process. This exercise [@problem_id:3067598] demonstrates how to transform a stochastic differential equation (SDE) from one measure to another. By working through a specific two-dimensional example, you will see precisely how the change of measure modifies the drift term of the SDE, a fundamental technique used extensively in financial mathematics and engineering.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\in [0,T]},\\mathbb{P})$ supporting a two-dimensional standard Brownian motion $(W_t)_{t \\in [0,T]}$ with $W_t=\\big(W_t^{(1)},W_t^{(2)}\\big)$. Let $X_t \\in \\mathbb{R}^2$ solve the linear stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t \\;=\\; A\\,X_t\\,\\mathrm{d}t \\;+\\; \\Sigma\\,\\mathrm{d}W_t,\\qquad X_0=x\\in\\mathbb{R}^2,\n$$\nwhere\n$$\nA \\;=\\; \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}, \\qquad \\Sigma \\;=\\; \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix},\n$$\nwith $a\\in\\mathbb{R}$ and $\\sigma0$. Let $\\theta=(\\theta_t)_{t\\in[0,T]}$ be an $\\mathbb{R}^2$-valued progressively measurable process with components $\\theta_t=\\big(\\theta_t^{(1)},\\theta_t^{(2)}\\big)$ satisfying the Novikov condition\n$$\n\\mathbb{E}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_0^T \\|\\theta_s\\|^2\\,\\mathrm{d}s\\right)\\right] \\;\\;\\infty.\n$$\nDefine a new probability measure $\\mathbb{Q}$ on $(\\Omega,\\mathcal{F}_T)$ by the Radon–Nikodym derivative\n$$\n\\frac{\\mathrm{d}\\mathbb{Q}}{\\mathrm{d}\\mathbb{P}}\\Big|_{\\mathcal{F}_T}\n\\;=\\;\n\\exp\\!\\left(\\int_0^T \\theta_s^\\top\\,\\mathrm{d}W_s \\;-\\; \\frac{1}{2}\\int_0^T \\|\\theta_s\\|^2\\,\\mathrm{d}s\\right).\n$$\nUsing only the foundational definition of the change of measure via the stochastic exponential and the multidimensional version of the Girsanov theorem (which asserts that $W_t^{\\mathbb{Q}}:=W_t-\\int_0^t \\theta_s\\,\\mathrm{d}s$ is a two-dimensional Brownian motion under $\\mathbb{Q}$), write the SDE for $X_t$ under $\\mathbb{Q}$ in terms of the $\\mathbb{Q}$-Brownian motion $W^{\\mathbb{Q}}$, and determine the drift vector under $\\mathbb{Q}$ as an explicit function of $a$, $\\sigma$, $X_t$, and $\\theta_t$. Your final answer should be a single closed-form analytic expression for the drift vector, written as a $1\\times 2$ row using the $\\mathrm{pmatrix}$ environment. No numerical rounding is required.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard application of the multidimensional Girsanov theorem for changing the drift of a linear stochastic differential equation (SDE). All necessary components are provided and are mathematically consistent.\n\nThe process $X_t \\in \\mathbb{R}^2$ is governed by the following SDE under the probability measure $\\mathbb{P}$:\n$$\n\\mathrm{d}X_t = A\\,X_t\\,\\mathrm{d}t + \\Sigma\\,\\mathrm{d}W_t\n$$\nwhere $W_t = \\big(W_t^{(1)}, W_t^{(2)}\\big)^\\top$ is a two-dimensional standard Brownian motion under $\\mathbb{P}$. The matrices $A$ and $\\Sigma$ are given by:\n$$\nA = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}, \\qquad \\Sigma = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThe problem defines a new probability measure $\\mathbb{Q}$ and states that, by Girsanov's theorem, the process $W_t^{\\mathbb{Q}}$ defined as\n$$\nW_t^{\\mathbb{Q}} = W_t - \\int_0^t \\theta_s\\,\\mathrm{d}s\n$$\nis a standard two-dimensional Brownian motion under $\\mathbb{Q}$. Here, $\\theta_t = \\big(\\theta_t^{(1)}, \\theta_t^{(2)}\\big)^\\top$ is a progressively measurable process satisfying the Novikov condition.\n\nOur objective is to find the SDE for $X_t$ under the new measure $\\mathbb{Q}$, which means expressing it in terms of the $\\mathbb{Q}$-Brownian motion $W_t^{\\mathbb{Q}}$. To do this, we first express the $\\mathbb{P}$-Brownian motion $W_t$ in terms of $W_t^{\\mathbb{Q}}$. From the definition of $W_t^{\\mathbb{Q}}$, we can write its differential form:\n$$\n\\mathrm{d}W_t^{\\mathbb{Q}} = \\mathrm{d}W_t - \\theta_t\\,\\mathrm{d}t\n$$\nRearranging this equation to solve for $\\mathrm{d}W_t$, we get:\n$$\n\\mathrm{d}W_t = \\mathrm{d}W_t^{\\mathbb{Q}} + \\theta_t\\,\\mathrm{d}t\n$$\nNow, we substitute this expression for $\\mathrm{d}W_t$ into the original SDE for $X_t$:\n$$\n\\mathrm{d}X_t = A\\,X_t\\,\\mathrm{d}t + \\Sigma\\,(\\mathrm{d}W_t^{\\mathbb{Q}} + \\theta_t\\,\\mathrm{d}t)\n$$\nTo identify the drift and diffusion terms under $\\mathbb{Q}$, we group the terms containing $\\mathrm{d}t$ and $\\mathrm{d}W_t^{\\mathbb{Q}}$:\n$$\n\\mathrm{d}X_t = A\\,X_t\\,\\mathrm{d}t + \\Sigma\\,\\theta_t\\,\\mathrm{d}t + \\Sigma\\,\\mathrm{d}W_t^{\\mathbb{Q}}\n$$\n$$\n\\mathrm{d}X_t = (A\\,X_t + \\Sigma\\,\\theta_t)\\,\\mathrm{d}t + \\Sigma\\,\\mathrm{d}W_t^{\\mathbb{Q}}\n$$\nThis is the SDE for $X_t$ under the measure $\\mathbb{Q}$. The drift vector under $\\mathbb{Q}$ is the coefficient of the $\\mathrm{d}t$ term. Let us denote this drift vector by $\\mu_t^{\\mathbb{Q}}$.\n$$\n\\mu_t^{\\mathbb{Q}} = A\\,X_t + \\Sigma\\,\\theta_t\n$$\nThe problem requires this drift vector to be expressed as an explicit function of $a$, $\\sigma$, $X_t$, and $\\theta_t$. Let $X_t = \\begin{pmatrix} X_t^{(1)} \\\\ X_t^{(2)} \\end{pmatrix}$ and $\\theta_t = \\begin{pmatrix} \\theta_t^{(1)} \\\\ \\theta_t^{(2)} \\end{pmatrix}$. We can now compute the vector $\\mu_t^{\\mathbb{Q}}$ by performing the matrix-vector multiplications:\n$$\nA\\,X_t = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} X_t^{(1)} \\\\ X_t^{(2)} \\end{pmatrix} = \\begin{pmatrix} a\\,X_t^{(1)} \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\Sigma\\,\\theta_t = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} \\theta_t^{(1)} \\\\ \\theta_t^{(2)} \\end{pmatrix} = \\begin{pmatrix} \\sigma\\,\\theta_t^{(1)} \\\\ 0 \\end{pmatrix}\n$$\nSubstituting these results back into the expression for $\\mu_t^{\\mathbb{Q}}$:\n$$\n\\mu_t^{\\mathbb{Q}} = \\begin{pmatrix} a\\,X_t^{(1)} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\sigma\\,\\theta_t^{(1)} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} a\\,X_t^{(1)} + \\sigma\\,\\theta_t^{(1)} \\\\ 0 \\end{pmatrix}\n$$\nThis is the drift vector, which is a $2 \\times 1$ column vector. The problem asks for the final answer to be written as a $1 \\times 2$ row vector. Therefore, we take the transpose of $\\mu_t^{\\mathbb{Q}}$:\n$$\n(\\mu_t^{\\mathbb{Q}})^\\top = \\begin{pmatrix} a\\,X_t^{(1)} + \\sigma\\,\\theta_t^{(1)}  0 \\end{pmatrix}\n$$\nThe first component of the drift is $a\\,X_t^{(1)} + \\sigma\\,\\theta_t^{(1)}$, and the second component is $0$. This reflects the fact that the dynamics of the second component of $X_t$, i.e., $X_t^{(2)}$, are trivial ($\\mathrm{d}X_t^{(2)}=0$) and unaffected by the change of measure.", "answer": "$$\n\\boxed{\\begin{pmatrix} a X_t^{(1)} + \\sigma \\theta_t^{(1)}  0 \\end{pmatrix}}\n$$", "id": "3067598"}, {"introduction": "Mathematical theorems often come with conditions that are sufficient but not always necessary. This final problem [@problem_id:3067610] delves into this important subtlety by presenting a scenario where Novikov's condition fails, yet the change of measure remains valid. By constructing a process with carefully chosen random growth, this exercise challenges you to look beyond the standard criteria and use more fundamental principles to justify the Girsanov transformation, deepening your understanding of the theorem's true scope and power.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ be a filtered probability space supporting an $m$-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$, where $m\\in\\mathbb{N}$ and $T0$. Enlarge the filtration to include an independent nonnegative random variable $Y$ with Pareto density $f_Y(y)=\\alpha y^{-(\\alpha+1)}$ for $y\\geq 1$ and fixed $\\alpha0$. Let $v\\in\\mathbb{R}^m$ be a deterministic vector with $\\|v\\|=1$ and define the progressively measurable process $\\theta=(\\theta_t)_{t\\in[0,T]}$ by $\\theta_t=Y\\,v$ for all $t\\in[0,T]$.\n\nStarting from the definitions of stochastic integrals and the stochastic exponential, and using only properties of Gaussian random variables and conditional expectation, proceed as follows:\n\n- Show that the integral $\\int_0^T\\|\\theta_s\\|^2\\,ds$ is finite almost surely and compute $\\mathbb{E}^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_0^T\\|\\theta_s\\|^2\\,ds\\right)\\right]$ in terms of $Y$, $T$, and $\\alpha$. Determine whether this expectation is finite or infinite.\n\n- Define the candidate Radon–Nikodym density process $Z=(Z_t)_{t\\in[0,T]}$ by\n$$\nZ_t=\\exp\\!\\left(\\int_0^t \\theta_s\\cdot dW_s-\\frac{1}{2}\\int_0^t \\|\\theta_s\\|^2\\,ds\\right),\n$$\nand, by conditioning on $Y$, justify whether $(Z_t)_{t\\in[0,T]}$ is a true martingale with $\\mathbb{E}^{\\mathbb{P}}[Z_T]=1$. Conclude whether the measure $\\mathbb{Q}$ defined by $d\\mathbb{Q}/d\\mathbb{P}\\big|_{\\mathcal{F}_T}=Z_T$ is equivalent to $\\mathbb{P}$, and explain the role of the growth of $\\theta$ in relation to the sufficiency (but not necessity) of Novikov’s condition.\n\nFinally, compute the explicit closed-form expression for the Radon–Nikodym derivative $d\\mathbb{Q}/d\\mathbb{P}\\big|_{\\mathcal{F}_T}$ in terms of $Y$, $v$, $W_T$, and $T$. Your final answer must be a single analytic expression. No numerical approximation is required.", "solution": "The problem statement is a well-posed exercise in stochastic calculus concerning the conditions for the Girsanov change of measure. It is scientifically grounded, self-contained, and objective. We shall proceed with the solution.\n\nLet the given filtered probability space be $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$, supporting an $m$-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$. The filtration is enlarged to make a random variable $Y$ measurable for all $t \\ge 0$. The random variable $Y$ is independent of $W$ and follows a Pareto distribution with density $f_Y(y) = \\alpha y^{-(\\alpha+1)}$ for $y \\ge 1$ and parameter $\\alpha0$. The process $\\theta_t$ is defined as $\\theta_t = Yv$ for a deterministic vector $v \\in \\mathbb{R}^m$ with $\\|v\\|=1$.\n\nFirst, we analyze the integral $\\int_0^T \\|\\theta_s\\|^2\\,ds$. Substituting the definition of $\\theta_s$, we have:\n$$\n\\|\\theta_s\\|^2 = \\|Yv\\|^2 = |Y|^2 \\|v\\|^2\n$$\nSince $Y$ is a non-negative random variable, $|Y|=Y$. Given that $\\|v\\|=1$, this simplifies to $\\|\\theta_s\\|^2 = Y^2$. The process $s \\mapsto \\|\\theta_s\\|^2$ is constant with respect to the time variable $s$. The integral is therefore:\n$$\n\\int_0^T \\|\\theta_s\\|^2\\,ds = \\int_0^T Y^2\\,ds = Y^2 \\int_0^T ds = Y^2 T\n$$\nSince $Y$ takes values in $[1, \\infty)$, it is finite almost surely. Consequently, the product $Y^2 T$ is also finite almost surely.\n\nNext, we compute the expectation $\\mathbb{E}^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_0^T\\|\\theta_s\\|^2\\,ds\\right)\\right]$. Using the previously derived expression for the integral, we need to compute:\n$$\n\\mathbb{E}^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2} Y^2 T\\right)\\right]\n$$\nBy the law of the unconscious statistician, this expectation is given by the integral of the function $\\exp(\\frac{1}{2}Ty^2)$ against the probability density function of $Y$:\n$$\n\\mathbb{E}^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2} Y^2 T\\right)\\right] = \\int_1^\\infty \\exp\\left(\\frac{1}{2} T y^2\\right) f_Y(y)\\,dy = \\int_1^\\infty \\exp\\left(\\frac{1}{2} T y^2\\right) \\alpha y^{-(\\alpha+1)}\\,dy\n$$\nWe analyze the convergence of this integral. The integrand is a product of an exponential function $\\exp(\\frac{1}{2} T y^2)$, which grows very rapidly as $y \\to \\infty$, and a power-law function $y^{-(\\alpha+1)}$, which decays. For any positive constants $C_1$ and $C_2$, and any real number $p$, the exponential term dominates the power-law term, i.e., $\\lim_{y\\to\\infty} y^p \\exp(-C_1 y^2) = 0$. Equivalently, $\\lim_{y\\to\\infty} \\frac{\\exp(C_1 y^2)}{y^{-p}} = \\infty$.\nIn our case, the integrand $\\alpha \\exp(\\frac{1}{2} T y^2) y^{-(\\alpha+1)}$ is positive for all $y \\ge 1$. As $y \\to \\infty$, the term $\\exp(\\frac{1}{2} T y^2)$ grows unboundedly, much faster than $y^{-(\\alpha+1)}$ decays. Therefore, the integral diverges to infinity.\n$$\n\\mathbb{E}^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\int_0^T\\|\\theta_s\\|^2\\,ds\\right)\\right] = \\infty\n$$\n\nNow we consider the candidate Radon-Nikodym density process $Z_t$, which is the stochastic exponential (or Doléans-Dade exponential) of the process $\\int \\theta \\cdot dW$:\n$$\nZ_t=\\exp\\!\\left(\\int_0^t \\theta_s\\cdot dW_s-\\frac{1}{2}\\int_0^t \\|\\theta_s\\|^2\\,ds\\right)\n$$\nBy definition, $(Z_t)_{t\\in[0,T]}$ is a positive local martingale with $Z_0=1$. A key question in the theory of Girsanov transformations is whether $(Z_t)$ is a true martingale, which is equivalent to checking if $\\mathbb{E}^{\\mathbb{P}}[Z_T]=1$. A sufficient condition for this is Novikov's condition, which states that if $\\mathbb{E}^{\\mathbb{P}}\\!\\left[\\exp\\!\\left(\\frac{1}{2}\\langle M \\rangle_T\\right)\\right]  \\infty$ where $M_t = \\int_0^t \\theta_s \\cdot dW_s$, then $(Z_t)$ is a martingale. The quadratic variation of $M_t$ is $\\langle M \\rangle_t = \\int_0^t \\|\\theta_s\\|^2\\,ds$. Novikov's condition is therefore precisely the expectation we just computed. Since we found this expectation to be infinite, Novikov's condition is not satisfied.\n\nHowever, Novikov's condition is sufficient but not necessary. We must justify the martingale property by other means, as suggested by the problem, by conditioning on the random variable $Y$. Let us compute the conditional expectation of $Z_T$ given $Y$.\n$$\n\\mathbb{E}^{\\mathbb{P}}[Z_T | Y] = \\mathbb{E}^{\\mathbb{P}}\\!\\left[\\left. \\exp\\!\\left(\\int_0^T Yv\\cdot dW_s - \\frac{1}{2}\\int_0^T Y^2\\|v\\|^2\\,ds\\right) \\right| Y \\right]\n$$\nLet's fix a value $y \\ge 1$ for $Y$. The expression becomes:\n$$\n\\mathbb{E}^{\\mathbb{P}}[Z_T | Y=y] = \\mathbb{E}^{\\mathbb{P}}\\!\\left[\\left. \\exp\\!\\left(y\\int_0^T v\\cdot dW_s - \\frac{1}{2} y^2 T\\right) \\right| Y=y \\right]\n$$\nSince $W$ is independent of $Y$, conditioning on $Y=y$ does not affect the distribution of $W$. So we can remove the condition from the expectation:\n$$\n\\mathbb{E}^{\\mathbb{P}}[Z_T | Y=y] = \\mathbb{E}^{\\mathbb{P}}\\!\\left[ \\exp\\!\\left(y\\int_0^T v\\cdot dW_s - \\frac{1}{2} y^2 T\\right) \\right]\n$$\nThe stochastic integral $\\int_0^T v\\cdot dW_s$ is a Gaussian random variable. Its mean is $\\mathbb{E}[\\int_0^T v\\cdot dW_s] = 0$. Its variance is $\\mathbb{E}[(\\int_0^T v\\cdot dW_s)^2] = \\int_0^T \\|v\\|^2\\,ds = \\int_0^T 1\\,ds = T$. So, let $X = \\int_0^T v\\cdot dW_s$; then $X \\sim \\mathcal{N}(0, T)$.\nWe need to compute $\\mathbb{E}^{\\mathbb{P}}[\\exp(yX - \\frac{1}{2}y^2T)] = \\exp(-\\frac{1}{2}y^2T) \\mathbb{E}^{\\mathbb{P}}[\\exp(yX)]$.\nThe moment-generating function of a Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(kX)] = \\exp(k\\mu + \\frac{1}{2}k^2\\sigma^2)$. Here, $\\mu=0$, $\\sigma^2=T$, and $k=y$.\n$$\n\\mathbb{E}^{\\mathbb{P}}[\\exp(yX)] = \\exp\\!\\left(y \\cdot 0 + \\frac{1}{2}y^2 T\\right) = \\exp\\!\\left(\\frac{1}{2}y^2 T\\right)\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}^{\\mathbb{P}}[Z_T | Y=y] = \\exp\\!\\left(\\frac{1}{2}y^2 T\\right) \\exp\\!\\left(-\\frac{1}{2}y^2 T\\right) = 1\n$$\nThis result holds for any value $y$ that $Y$ can take. Thus, the conditional expectation as a random variable is $\\mathbb{E}^{\\mathbb{P}}[Z_T | Y] = 1$.\nBy the law of total expectation (tower property), the unconditional expectation is:\n$$\n\\mathbb{E}^{\\mathbb{P}}[Z_T] = \\mathbb{E}^{\\mathbb{P}}\\!\\left[\\mathbb{E}^{\\mathbb{P}}[Z_T | Y]\\right] = \\mathbb{E}^{\\mathbb{P}}[1] = 1\n$$\nSince $(Z_t)$ is a positive local martingale and $\\mathbb{E}[Z_T]=1$, it is a true martingale.\n\nThe measure $\\mathbb{Q}$ defined by the Radon-Nikodym derivative $d\\mathbb{Q}/d\\mathbb{P}\\big|_{\\mathcal{F}_T}=Z_T$ is a probability measure because $\\mathbb{E}^{\\mathbb{P}}[Z_T]=1$. The measures $\\mathbb{Q}$ and $\\mathbb{P}$ are equivalent if and only if $Z_T0$ almost surely. Since $Z_T$ is the exponential of a real-valued random variable, it is always strictly positive. Therefore, $\\mathbb{Q}$ is equivalent to $\\mathbb{P}$.\n\nThis problem serves as a canonical example where Novikov's condition fails, yet the change of measure is valid. The growth of $\\|\\theta_t\\|^2=Y^2$ is controlled by the heavy-tailed Pareto random variable $Y$. The exponential of this term has an infinite expectation, causing Novikov's condition to fail. However, the martingale property is preserved because for any large but fixed value of $Y$, the Gaussian nature of the stochastic integral term provides a perfect cancellation in expectation.\n\nFinally, we find the explicit closed-form expression for the Radon-Nikodym derivative $Z_T = d\\mathbb{Q}/d\\mathbb{P}\\big|_{\\mathcal{F}_T}$. We have already calculated the components of the exponent:\nThe stochastic integral term is:\n$$\n\\int_0^T \\theta_s\\cdot dW_s = \\int_0^T Yv\\cdot dW_s = Y \\int_0^T v\\cdot dW_s = Y(v \\cdot W_T)\n$$\nwhere we used the property $\\int_0^T v\\cdot dW_s = v \\cdot (W_T - W_0) = v \\cdot W_T$ for a deterministic vector $v$.\nThe quadratic variation term is:\n$$\n\\int_0^T \\|\\theta_s\\|^2\\,ds = Y^2 T\n$$\nSubstituting these into the definition of $Z_T$:\n$$\nZ_T = \\exp\\!\\left(Y(v \\cdot W_T) - \\frac{1}{2}Y^2 T\\right)\n$$\nThis is the final expression for the Radon-Nikodym derivative in terms of the specified variables.", "answer": "$$\n\\boxed{\\exp\\left(Y(v \\cdot W_T) - \\frac{1}{2}Y^2 T\\right)}\n$$", "id": "3067610"}]}