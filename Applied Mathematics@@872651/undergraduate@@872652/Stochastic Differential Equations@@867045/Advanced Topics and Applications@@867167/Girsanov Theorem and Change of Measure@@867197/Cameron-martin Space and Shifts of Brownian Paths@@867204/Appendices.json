{"hands_on_practices": [{"introduction": "Before we can analyze the effect of shifts, we must first understand the geometric structure of the space in which these shifts occur. This practice guides you through the construction of an orthonormal basis for the Cameron-Martin space $\\mathcal{H}$, which is analogous to defining a coordinate system [@problem_id:3043091]. By leveraging the isometric relationship between $\\mathcal{H}$ and the space of square-integrable functions $L^2([0,1])$, you will build a concrete basis, transforming an abstract concept into a tangible set of functions.", "problem": "Let $H$ denote the Cameron–Martin space associated with standard Brownian motion on the interval $[0,1]$, defined as the Hilbert space of all absolutely continuous functions $h:[0,1]\\to\\mathbb{R}$ with $h(0)=0$ and weak derivative $h'\\in L^{2}(0,1)$, equipped with the inner product\n$$\n\\langle h,g\\rangle_{H} \\;=\\; \\int_{0}^{1} h'(t)\\,g'(t)\\,dt.\n$$\nUsing only these definitions and basic properties of Fourier series on $[0,1]$, construct an explicit orthonormal basis $\\{e_{n}\\}_{n\\ge 1}$ for $H$ whose elements start at zero and arise from primitives of functions that are either piecewise linear on subintervals or generated from a sine series having zero at $t=0$. Your construction must:\n- Provide a closed-form expression for the general element $e_{n}(t)$.\n- Rigorously verify that $\\langle e_{n},e_{m}\\rangle_{H}$ equals the Kronecker delta for all $m,n\\in\\mathbb{N}$.\n- Justify that the family $\\{e_{n}\\}_{n\\ge 1}$ is complete in $H$.\n\nState as your final answer the explicit formula for $e_{n}(t)$ and the explicit value of $\\langle e_{n},e_{m}\\rangle_{H}$ in terms of $m$ and $n$. Your final answer must be a single analytical expression. No numerical rounding is required.", "solution": "The core strategy for constructing an orthonormal basis for the Cameron-Martin space $H$ is to leverage its isometric relationship with the space of square-integrable functions $L^2(0,1)$.\n\nLet us define a derivative map $D: H \\to L^2(0,1)$ such that $D(h) = h'$, where $h'$ is the weak derivative of $h$. For any function $h \\in H$, the fundamental theorem of calculus for absolutely continuous functions, combined with the condition $h(0)=0$, implies that $h(t) = \\int_0^t h'(s)ds$. This means that the map $D$ has a well-defined inverse, $I: L^2(0,1) \\to H$, given by $I(f)(t) = \\int_0^t f(s)ds$.\n\nThe map $D$ is an isometry, which can be seen by comparing the inner products of the two spaces. For any $h, g \\in H$:\n$$\n\\langle h, g \\rangle_H = \\int_0^1 h'(t)g'(t)dt = \\int_0^1 (D(h))(t) (D(g))(t) dt = \\langle D(h), D(g) \\rangle_{L^2(0,1)}\n$$\nThis isometric isomorphism guarantees that if we take a complete orthonormal basis (ONB) of $L^2(0,1)$, denoted by $\\{\\phi_n\\}_{n\\ge 1}$, and apply the inverse map $I$ to each element, the resulting set $\\{e_n = I(\\phi_n)\\}_{n\\ge 1}$ will form a complete orthonormal basis for $H$.\n\nA standard ONB for $L^2(0,1)$ is the Fourier sine basis, given by the functions $\\phi_n(t) = \\sqrt{2}\\sin(n\\pi t)$ for $n \\in \\mathbb{N}, n \\ge 1$.\n\nWe now construct the basis elements $e_n(t)$ for $H$ by applying the integration operator $I$ to these basis functions $\\phi_n(t)$:\n$$\ne_n(t) = \\int_0^t \\phi_n(s)ds = \\int_0^t \\sqrt{2}\\sin(n\\pi s)ds\n$$\nEvaluating the integral yields the explicit closed-form expression for $e_n(t)$:\n$$\ne_n(t) = \\sqrt{2} \\left[ -\\frac{\\cos(n\\pi s)}{n\\pi} \\right]_0^t = \\sqrt{2} \\left( -\\frac{\\cos(n\\pi t)}{n\\pi} - \\left(-\\frac{\\cos(0)}{n\\pi}\\right) \\right) = \\frac{\\sqrt{2}}{n\\pi} (1 - \\cos(n\\pi t))\n$$\nThis provides the required expression for the general element of the basis. By construction, $e_n(0) = \\frac{\\sqrt{2}}{n\\pi} (1 - \\cos(0)) = 0$, and its derivative $e_n'(t) = \\phi_n(t) = \\sqrt{2}\\sin(n\\pi t)$ is in $L^2(0,1)$, so $e_n \\in H$.\n\nNext, we must rigorously verify the orthonormality of the constructed basis $\\{e_n\\}_{n\\ge 1}$ in $H$. We compute the inner product $\\langle e_n, e_m \\rangle_H$:\n$$\n\\langle e_n, e_m \\rangle_H = \\int_0^1 e_n'(t)e_m'(t)dt = \\int_0^1 \\left(\\sqrt{2}\\sin(n\\pi t)\\right)\\left(\\sqrt{2}\\sin(m\\pi t)\\right)dt = 2 \\int_0^1 \\sin(n\\pi t)\\sin(m\\pi t)dt\n$$\nFor $n=m$, the integral becomes:\n$$\n\\langle e_n, e_n \\rangle_H = 2 \\int_0^1 \\sin^2(n\\pi t)dt = 2 \\int_0^1 \\frac{1-\\cos(2n\\pi t)}{2}dt = \\int_0^1 (1-\\cos(2n\\pi t))dt = \\left[t - \\frac{\\sin(2n\\pi t)}{2n\\pi}\\right]_0^1 = 1\n$$\nFor $n \\neq m$, using the product-to-sum identity $\\sin(A)\\sin(B) = \\frac{1}{2}(\\cos(A-B) - \\cos(A+B))$, the integral is:\n$$\n\\langle e_n, e_m \\rangle_H = \\int_0^1 (\\cos((n-m)\\pi t) - \\cos((n+m)\\pi t))dt = \\left[\\frac{\\sin((n-m)\\pi t)}{(n-m)\\pi} - \\frac{\\sin((n+m)\\pi t)}{(n+m)\\pi}\\right]_0^1 = 0\n$$\nThus, we have shown that $\\langle e_n, e_m \\rangle_H = \\delta_{nm}$, where $\\delta_{nm}$ is the Kronecker delta. The set $\\{e_n\\}_{n\\ge 1}$ is orthonormal.\n\nFinally, we must justify the completeness of the basis. The completeness of $\\{e_n\\}_{n\\ge 1}$ in $H$ is a direct consequence of the completeness of $\\{\\phi_n\\}_{n\\ge 1}$ in $L^2(0,1)$ and the isometric isomorphism between the spaces. For any $h \\in H$, its derivative $h' \\in L^2(0,1)$ can be expanded in the ONB $\\{\\phi_n\\}$:\n$$\nh' = \\sum_{n=1}^\\infty \\langle h', \\phi_n \\rangle_{L^2(0,1)} \\phi_n\n$$\nThe convergence is in the $L^2$-norm. The coefficients are $\\langle h', \\phi_n \\rangle_{L^2(0,1)} = \\int_0^1 h'(t)e_n'(t)dt = \\langle h, e_n \\rangle_H$. Let $S_N(t) = \\sum_{n=1}^N \\langle h, e_n \\rangle_H e_n(t)$. The squared norm of the difference $h - S_N$ in $H$ is:\n$$\n\\|h - S_N\\|_H^2 = \\|h' - S_N'\\|_{L^2(0,1)}^2 = \\left\\|h' - \\sum_{n=1}^N \\langle h', \\phi_n \\rangle_{L^2(0,1)} \\phi_n\\right\\|_{L^2(0,1)}^2\n$$\nBy Parseval's identity for the complete basis $\\{\\phi_n\\}$ in $L^2(0,1)$, this expression tends to zero as $N \\to \\infty$. This proves that the basis $\\{e_n\\}_{n\\ge 1}$ is complete in $H$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{2}}{n \\pi} (1 - \\cos(n \\pi t))  \\delta_{nm}\n\\end{pmatrix}\n}\n$$", "id": "3043091"}, {"introduction": "The cornerstone of this topic is the Cameron-Martin theorem, which describes how a deterministic shift of a Brownian path changes the underlying probability measure. This exercise provides a powerful verification of the theorem by asking you to compute the expectation of a functional on a shifted path using two distinct methods [@problem_id:3043120]. By confirming that the abstract change-of-measure formula and a direct calculation based on Gaussian properties yield the same result, you will build deep intuition for this fundamental principle.", "problem": "Consider the classical Wiener space $C([0,T],\\mathbb{R})$ equipped with the Wiener measure, and let $W=(W_s)_{0\\leq s\\leq T}$ be a standard Brownian motion with $W_0=0$. Fix $T0$ and $t\\in(0,T]$. Let $h\\in\\mathcal{H}$ belong to the Cameron–Martin space, meaning that $h$ is absolutely continuous on $[0,T]$, satisfies $h(0)=0$, and has derivative $\\dot{h}\\in L^2([0,T])$. Define the functional $F:C([0,T],\\mathbb{R})\\to\\mathbb{R}$ by $F(x)=\\exp(\\lambda x(t))$ for a fixed $\\lambda\\in\\mathbb{R}$.\n\nUsing the Cameron–Martin theorem, express $\\mathbb{E}[F(W+h)]$ as an expectation under the Wiener measure weighted by the appropriate density involving $\\dot{h}$, and evaluate it explicitly. Independently, compute $\\mathbb{E}[F(W+h)]$ directly by recognizing the finite-dimensional marginal of $W$ under a deterministic shift $h$ and evaluating the resulting Gaussian integral. Confirm that both methods produce the same value and provide the final closed-form expression for $\\mathbb{E}[F(W+h)]$ in terms of $\\lambda$, $t$, and $h(t)$.\n\nYour final answer must be a single closed-form analytic expression.", "solution": "The problem requires the computation of $\\mathbb{E}[\\exp(\\lambda (W_t+h(t)))]$ using two distinct methods.\n\n**Method 1: Using the Cameron-Martin Theorem**\n\nThe Cameron-Martin theorem, as a special case of Girsanov's theorem, provides a change-of-measure formula. The expectation of a functional of the shifted path, $F(W+h)$, can be written as an expectation with respect to the original Wiener measure $\\mathbb{P}$ by introducing the Radon-Nikodym derivative density:\n$$\n\\mathbb{E}[F(W+h)] = \\mathbb{E}_{\\mathbb{P}}\\left[ F(W) \\frac{d\\mathbb{P}_{W+h}}{d\\mathbb{P}}(W) \\right]\n$$\nHere, $F(W) = \\exp(\\lambda W_t)$ and the density is:\n$$\n\\frac{d\\mathbb{P}_{W+h}}{d\\mathbb{P}}(W) = \\exp\\left( \\int_0^T \\dot{h}(s) dW_s - \\frac{1}{2} \\int_0^T \\dot{h}(s)^2 ds \\right)\n$$\nSubstituting these into the expectation gives:\n$$\n\\mathbb{E}[F(W+h)] = \\mathbb{E}\\left[ \\exp(\\lambda W_t) \\exp\\left( \\int_0^T \\dot{h}(s) dW_s - \\frac{1}{2} \\int_0^T \\dot{h}(s)^2 ds \\right) \\right]\n$$\n$$\n= \\exp\\left( - \\frac{1}{2} \\int_0^T \\dot{h}(s)^2 ds \\right) \\mathbb{E}\\left[ \\exp\\left( \\lambda W_t + \\int_0^T \\dot{h}(s) dW_s \\right) \\right]\n$$\nThe exponent in the expectation, $X = \\lambda W_t + \\int_0^T \\dot{h}(s) dW_s$, is a mean-zero Gaussian random variable. Its variance, by the Itô isometry, is:\n$$\n\\mathrm{Var}(X) = \\int_0^T (\\lambda 1_{[0,t]}(s) + \\dot{h}(s))^2 ds = \\lambda^2 \\int_0^t ds + 2\\lambda \\int_0^t \\dot{h}(s) ds + \\int_0^T \\dot{h}(s)^2 ds\n$$\n$$\n= \\lambda^2 t + 2\\lambda h(t) + \\int_0^T \\dot{h}(s)^2 ds\n$$\nUsing the moment-generating function for a Gaussian, $\\mathbb{E}[\\exp(X)] = \\exp(\\frac{1}{2}\\mathrm{Var}(X))$:\n$$\n\\mathbb{E}\\left[ \\exp(X) \\right] = \\exp\\left( \\frac{1}{2}(\\lambda^2 t + 2\\lambda h(t) + \\int_0^T \\dot{h}(s)^2 ds) \\right) = \\exp\\left( \\frac{1}{2}\\lambda^2 t + \\lambda h(t) + \\frac{1}{2}\\int_0^T \\dot{h}(s)^2 ds \\right)\n$$\nSubstituting this back, the integral terms cancel, yielding:\n$$\n\\mathbb{E}[F(W+h)] = \\exp\\left( \\lambda h(t) + \\frac{1}{2}\\lambda^2 t \\right)\n$$\n\n**Method 2: Direct Computation**\n\nWe compute the expectation $\\mathbb{E}[\\exp(\\lambda (W_t+h(t)))]$ directly. Since $h(t)$ is deterministic, we can factor it out:\n$$\n\\mathbb{E}[\\exp(\\lambda W_t + \\lambda h(t))] = \\exp(\\lambda h(t)) \\mathbb{E}[\\exp(\\lambda W_t)]\n$$\nThe random variable $W_t$ is normally distributed as $\\mathcal{N}(0,t)$. Its moment-generating function is $\\mathbb{E}[\\exp(\\lambda W_t)] = \\exp(\\frac{1}{2}\\lambda^2 t)$. Substituting this gives:\n$$\n\\mathbb{E}[F(W+h)] = \\exp(\\lambda h(t)) \\exp\\left(\\frac{1}{2}\\lambda^2 t\\right) = \\exp\\left( \\lambda h(t) + \\frac{1}{2}\\lambda^2 t \\right)\n$$\nBoth methods yield the same correct result.", "answer": "$$\\boxed{\\exp\\left(\\lambda h(t) + \\frac{1}{2}t\\lambda^{2}\\right)}$$", "id": "3043120"}, {"introduction": "The Cameron-Martin framework finds powerful applications in the study of stochastic differential equations (SDEs), where it provides a foundation for stochastic control theory. This practice explores how a shift in the driving Brownian motion affects the solution of an Ornstein-Uhlenbeck SDE, a ubiquitous model for mean-reverting processes [@problem_id:3043093]. By calculating the new mean and variance, you will see how an abstract shift in the driver translates into a concrete, deterministic forcing term, demonstrating how the theory can be used to analyze and steer stochastic systems.", "problem": "Let $\\{W_t\\}_{t \\geq 0}$ be a standard Brownian motion and consider the Ornstein–Uhlenbeck stochastic differential equation (SDE), defined as\n$$\ndX_t=-\\theta X_t\\,dt+dW_t,\\qquad X_0=x_0,\n$$\nwhere $\\theta0$ and $x_0 \\in \\mathbb{R}$ are deterministic constants. On the time interval $[0,t]$, the Cameron–Martin space (CM) consists of all absolutely continuous functions $h:[0,t]\\to\\mathbb{R}$ with $h(0)=0$ and derivative $\\dot{h}\\in L^2([0,t])$. Consider the Cameron–Martin shift $h(s)=a s$ with $a\\in\\mathbb{R}$, which belongs to this space. Define the shifted driver $W_s^{(h)}=W_s+h(s)$, and let $X_s^{(h)}$ be the solution to the SDE driven by $W^{(h)}$:\n$$\ndX_s^{(h)}=-\\theta X_s^{(h)}\\,ds+dW_s^{(h)},\\qquad X_0^{(h)}=x_0.\n$$\nStarting from fundamental properties of Itô integrals (zero mean and Itô isometry) and the integrating factor method for linear SDEs, derive closed-form expressions for the expectation $\\mathbb{E}[X_t^{(h)}]$ and the variance $\\mathrm{Var}(X_t^{(h)})$ in terms of $\\theta$, $a$, $t$, and $x_0$. Express your final answer as a row matrix with the first entry equal to $\\mathbb{E}[X_t^{(h)}]$ and the second entry equal to $\\mathrm{Var}(X_t^{(h)})$.", "solution": "The problem asks for the expectation $\\mathbb{E}[X_t^{(h)}]$ and variance $\\mathrm{Var}(X_t^{(h)})$ of a process $X_t^{(h)}$ governed by a stochastic differential equation (SDE) with a shifted driver. Let $\\{W_t\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion.\n\nThe SDE for the shifted process $X_s^{(h)}$ is given by:\n$$\ndX_s^{(h)} = -\\theta X_s^{(h)}\\,ds + dW_s^{(h)}, \\quad X_0^{(h)} = x_0\n$$\nwhere $\\theta  0$, $x_0 \\in \\mathbb{R}$, and the shifted driver is $W_s^{(h)} = W_s + h(s)$. The Cameron-Martin shift is specified as $h(s) = a s$ for some constant $a \\in \\mathbb{R}$.\n\nFirst, we express the differential $dW_s^{(h)}$ in terms of the original Brownian motion $W_s$. Since $h(s)$ is a differentiable function of time, its differential is $dh(s) = \\dot{h}(s)ds$. For $h(s) = as$, the derivative is $\\dot{h}(s) = a$.\nThus, we have:\n$$\ndW_s^{(h)} = d(W_s + as) = dW_s + a\\,ds\n$$\nSubstituting this into the SDE for $X_s^{(h)}$ yields:\n$$\ndX_s^{(h)} = -\\theta X_s^{(h)}\\,ds + (dW_s + a\\,ds)\n$$\nRearranging the terms, we obtain a linear SDE in standard form:\n$$\ndX_s^{(h)} + \\theta X_s^{(h)}\\,ds = a\\,ds + dW_s\n$$\nThis is a linear first-order differential equation, which can be solved using an integrating factor. Let the integrating factor be $I_s = \\exp(\\int_0^s \\theta \\,du) = \\exp(\\theta s)$. Multiplying the SDE by $I_s = \\exp(\\theta s)$:\n$$\n\\exp(\\theta s)dX_s^{(h)} + \\theta \\exp(\\theta s) X_s^{(h)}\\,ds = a \\exp(\\theta s)\\,ds + \\exp(\\theta s)dW_s\n$$\nThe left-hand side is, by the product rule for Itô processes (applied to the deterministic function $\\exp(\\theta s)$ and the process $X_s^{(h)}$), the differential of the product $\\exp(\\theta s)X_s^{(h)}$:\n$$\nd(\\exp(\\theta s)X_s^{(h)}) = \\exp(\\theta s)dX_s^{(h)} + \\theta \\exp(\\theta s) X_s^{(h)}\\,ds\n$$\nTherefore, our equation simplifies to:\n$$\nd(\\exp(\\theta s)X_s^{(h)}) = a \\exp(\\theta s)\\,ds + \\exp(\\theta s)dW_s\n$$\nWe integrate both sides from $s=0$ to $s=t$:\n$$\n\\int_0^t d(\\exp(\\theta s)X_s^{(h)}) = \\int_0^t a \\exp(\\theta s)\\,ds + \\int_0^t \\exp(\\theta s)dW_s\n$$\nEvaluating the integrals gives:\n$$\n\\exp(\\theta t)X_t^{(h)} - \\exp(0)X_0^{(h)} = a \\left[\\frac{\\exp(\\theta s)}{\\theta}\\right]_0^t + \\int_0^t \\exp(\\theta s)dW_s\n$$\nUsing the initial condition $X_0^{(h)} = x_0$ and evaluating the definite integral:\n$$\n\\exp(\\theta t)X_t^{(h)} - x_0 = \\frac{a}{\\theta}(\\exp(\\theta t) - 1) + \\int_0^t \\exp(\\theta s)dW_s\n$$\nTo obtain the explicit solution for $X_t^{(h)}$, we multiply by $\\exp(-\\theta t)$:\n$$\nX_t^{(h)} = x_0 \\exp(-\\theta t) + \\frac{a}{\\theta}(1 - \\exp(-\\theta t)) + \\int_0^t \\exp(-\\theta(t-s))dW_s\n$$\nThis expression provides the closed-form solution for the process $X_t^{(h)}$. We now proceed to compute its expectation and variance.\n\n**Expectation $\\mathbb{E}[X_t^{(h)}]$**\nWe take the expectation of the solution for $X_t^{(h)}$. By linearity of expectation:\n$$\n\\mathbb{E}[X_t^{(h)}] = \\mathbb{E}\\left[x_0 \\exp(-\\theta t) + \\frac{a}{\\theta}(1 - \\exp(-\\theta t)) + \\int_0^t \\exp(-\\theta(t-s))dW_s\\right]\n$$\nThe terms $x_0 \\exp(-\\theta t)$ and $\\frac{a}{\\theta}(1 - \\exp(-\\theta t))$ are deterministic. The expectation of a constant is the constant itself. The third term is an Itô integral with a deterministic integrand, $f(s,t) = \\exp(-\\theta(t-s))$. A fundamental property of Itô integrals is that they have zero mean:\n$$\n\\mathbb{E}\\left[\\int_0^t f(s)dW_s\\right] = 0\n$$\nApplying these facts, we get:\n$$\n\\mathbb{E}[X_t^{(h)}] = x_0 \\exp(-\\theta t) + \\frac{a}{\\theta}(1 - \\exp(-\\theta t)) + 0\n$$\nThus, the expectation is:\n$$\n\\mathbb{E}[X_t^{(h)}] = x_0 \\exp(-\\theta t) + \\frac{a}{\\theta}(1 - \\exp(-\\theta t))\n$$\n\n**Variance $\\mathrm{Var}(X_t^{(h)}]$**\nThe variance of a random variable $Y$ is defined as $\\mathrm{Var}(Y) = \\mathbb{E}[(Y - \\mathbb{E}[Y])^2]$.\nFrom our previous results, we can write:\n$$\nX_t^{(h)} - \\mathbb{E}[X_t^{(h)}] = \\int_0^t \\exp(-\\theta(t-s))dW_s\n$$\nTherefore, the variance is:\n$$\n\\mathrm{Var}(X_t^{(h)}) = \\mathrm{Var}\\left(x_0 \\exp(-\\theta t) + \\frac{a}{\\theta}(1 - \\exp(-\\theta t)) + \\int_0^t \\exp(-\\theta(t-s))dW_s\\right)\n$$\nSince the first two terms are deterministic, they do not contribute to the variance. Hence:\n$$\n\\mathrm{Var}(X_t^{(h)}) = \\mathrm{Var}\\left(\\int_0^t \\exp(-\\theta(t-s))dW_s\\right)\n$$\nThe variance of a zero-mean random variable is the expectation of its square. So we need to compute:\n$$\n\\mathbb{E}\\left[\\left(\\int_0^t \\exp(-\\theta(t-s))dW_s\\right)^2\\right]\n$$\nWe now apply the Itô isometry property, which states that for a deterministic square-integrable function $f(s)$:\n$$\n\\mathbb{E}\\left[\\left(\\int_0^t f(s)dW_s\\right)^2\\right] = \\int_0^t f(s)^2 ds\n$$\nIn our case, the integrand is $f(s) = \\exp(-\\theta(t-s))$. Squaring it gives:\n$$\nf(s)^2 = (\\exp(-\\theta(t-s)))^2 = \\exp(-2\\theta(t-s))\n$$\nNow we integrate this squared function from $0$ to $t$:\n$$\n\\mathrm{Var}(X_t^{(h)}) = \\int_0^t \\exp(-2\\theta(t-s))\\,ds = \\int_0^t \\exp(-2\\theta t)\\exp(2\\theta s)\\,ds\n$$\nThe term $\\exp(-2\\theta t)$ is constant with respect to the integration variable $s$:\n$$\n\\mathrm{Var}(X_t^{(h)}) = \\exp(-2\\theta t) \\int_0^t \\exp(2\\theta s)\\,ds\n$$\nEvaluating the integral:\n$$\n\\int_0^t \\exp(2\\theta s)\\,ds = \\left[\\frac{\\exp(2\\theta s)}{2\\theta}\\right]_0^t = \\frac{\\exp(2\\theta t) - \\exp(0)}{2\\theta} = \\frac{\\exp(2\\theta t) - 1}{2\\theta}\n$$\nSubstituting this back into the expression for the variance:\n$$\n\\mathrm{Var}(X_t^{(h)}) = \\exp(-2\\theta t) \\left( \\frac{\\exp(2\\theta t) - 1}{2\\theta} \\right) = \\frac{\\exp(-2\\theta t)\\exp(2\\theta t) - \\exp(-2\\theta t)}{2\\theta} = \\frac{1 - \\exp(-2\\theta t)}{2\\theta}\n$$\nThe variance is independent of the shift parameter $a$ and the initial condition $x_0$, which is expected as these deterministic components only affect the mean of the process.\n\nThe final expressions are:\nExpectation: $\\mathbb{E}[X_t^{(h)}] = x_0 \\exp(-\\theta t) + \\frac{a}{\\theta}(1 - \\exp(-\\theta t))$\nVariance: $\\mathrm{Var}(X_t^{(h)}) = \\frac{1 - \\exp(-2\\theta t)}{2\\theta}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_0 \\exp(-\\theta t) + \\frac{a}{\\theta}(1 - \\exp(-\\theta t))  \\frac{1 - \\exp(-2\\theta t)}{2\\theta}\n\\end{pmatrix}\n}\n$$", "id": "3043093"}]}