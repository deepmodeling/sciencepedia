{"hands_on_practices": [{"introduction": "The Martingale Representation Property is built upon the foundation of the Itô stochastic integral. This first exercise takes us back to basics, asking us to compute the integral of the simplest possible non-zero integrand—a constant—directly from its definition. By doing so, we will not only derive a fundamental result but also directly verify the Itô isometry, which links the second moment of the stochastic integral to the integral of the squared integrand, a crucial property in the $L^2$ theory of martingales. [@problem_id:3065258]", "problem": "Let $\\{W_{t}\\}_{t \\ge 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\ge 0},\\mathbb{P})$ satisfying the usual conditions, where $\\{\\mathcal{F}_{t}\\}_{t \\ge 0}$ is the completed natural filtration of $W$. Fix $T0$ and a constant $a \\in \\mathbb{R}$. Consider the predictable process $H_{t} \\equiv a$ for all $t \\in [0,T]$ and the Itô integral $\\int_{0}^{T} H_{s}\\,dW_{s}$. Starting from the definition of the Itô integral for simple predictable processes and the basic properties of Brownian motion (in particular, independent Gaussian increments with variance equal to the time increment), do the following:\n\n- Compute $\\int_{0}^{T} H_{s}\\,dW_{s}$ explicitly in terms of $a$ and $W$.\n- Compute its variance under $\\mathbb{P}$.\n- Verify directly, without assuming it a priori, that the Itô isometry holds in this case by showing that $\\mathbb{E}\\!\\left[\\left(\\int_{0}^{T} H_{s}\\,dW_{s}\\right)^{2}\\right]$ equals $\\int_{0}^{T}\\mathbb{E}[H_{s}^{2}]\\,ds$.\n\nYour final answer must be given as a single row matrix containing, in this order, the expression for the Itô integral and its variance, both in closed form as functions of $a$ and $T$. No rounding is required.", "solution": "The problem requires us to compute the Itô integral of a constant predictable process, find its variance, and verify the Itô isometry for this specific case. Let $\\{W_{t}\\}_{t \\ge 0}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\ge 0},\\mathbb{P})$, with $\\{\\mathcal{F}_{t}\\}_{t \\ge 0}$ being the completed natural filtration of $W$. By definition, a standard Brownian motion starts at zero, so $W_0=0$ almost surely. Its increments $W_t - W_s$ over non-overlapping time intervals are independent, and for $s  t$, the increment $W_t - W_s$ follows a normal distribution with mean $0$ and variance $t-s$, denoted $N(0, t-s)$. The process $H_t = a$ for $a \\in \\mathbb{R}$ is a deterministic, hence predictable, process.\n\nThe problem asks to proceed from the definition of the Itô integral for simple predictable processes. A general predictable process $H_{s}$ is approximated by a sequence of simple predictable processes. For a simple process of the form $K_t = \\sum_{i=0}^{n-1} Y_i \\mathbf{1}_{(t_i, t_{i+1}]}(t)$, where $0 = t_0  t_1  \\dots  t_n = T$ and each $Y_i$ is an $\\mathcal{F}_{t_i}$-measurable random variable, the Itô integral is defined as $\\int_{0}^{T} K_s \\,dW_s = \\sum_{i=0}^{n-1} Y_i (W_{t_{i+1}} - W_{t_i})$. For a general process $H$, we find a sequence of simple processes $H^{(k)}$ such that $\\mathbb{E}[\\int_{0}^{T} (H_t - H_t^{(k)})^2 dt] \\to 0$ as $k \\to \\infty$. The Itô integral $\\int_{0}^{T} H_s \\,dW_s$ is then defined as the limit in $L^2(\\Omega)$ of the integrals $\\int_{0}^{T} H_s^{(k)} \\,dW_s$.\n\nLet's construct an approximating sequence for $H_t = a$. For any partition $\\pi_n = \\{0 = t_0  t_1  \\dots  t_n = T\\}$ of the interval $[0, T]$, we can define a simple process\n$$ H_t^{(n)} = \\sum_{i=0}^{n-1} H_{t_i} \\mathbf{1}_{(t_i, t_{i+1}]}(t) $$\nSince $H_t = a$ for all $t$, we have $H_{t_i} = a$ for all $i$. Thus,\n$$ H_t^{(n)} = \\sum_{i=0}^{n-1} a \\cdot \\mathbf{1}_{(t_i, t_{i+1}]}(t) = a \\cdot \\mathbf{1}_{(0, T]}(t) $$\nThis approximating process is actually independent of the specific partition $\\pi_n$, as long as it spans $[0,T]$.\n\n**Part 1: Computation of the Itô Integral**\n\nThe Itô integral for the simple process $H_t^{(n)}$ is given by the definition:\n$$ \\int_{0}^{T} H_s^{(n)} \\,dW_s = \\sum_{i=0}^{n-1} a (W_{t_{i+1}} - W_{t_i}) $$\nWe can factor out the constant $a$:\n$$ \\int_{0}^{T} H_s^{(n)} \\,dW_s = a \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i}) $$\nThe sum is a telescoping series:\n$$ \\sum_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i}) = (W_{t_1} - W_{t_0}) + (W_{t_2} - W_{t_1}) + \\dots + (W_{t_n} - W_{t_{n-1}}) = W_{t_n} - W_{t_0} $$\nGiven that $t_n = T$ and $t_0 = 0$, and standard Brownian motion starts at $0$ (i.e., $W_0 = 0$ a.s.), the sum simplifies to $W_T - W_0 = W_T$.\nTherefore, for any partition $\\pi_n$, the integral of the approximating simple process is:\n$$ \\int_{0}^{T} H_s^{(n)} \\,dW_s = a W_T $$\nThe Itô integral of $H_s = a$ is the $L^2$ limit of these integrals. Since the integral for every approximating simple process is the same constant random variable $a W_T$, the limit is trivially $a W_T$.\n$$ \\int_{0}^{T} H_s \\,dW_s = \\int_{0}^{T} a \\,dW_s = a W_T $$\n\n**Part 2: Computation of the Variance**\n\nThe random variable representing the integral is $I_T = a W_T$. We need to compute its variance, $\\text{Var}(I_T)$.\nUsing the properties of variance, for a constant $c$ and a random variable $X$, $\\text{Var}(cX) = c^2 \\text{Var}(X)$.\n$$ \\text{Var}(I_T) = \\text{Var}(a W_T) = a^2 \\text{Var}(W_T) $$\nFor a standard Brownian motion, the random variable $W_T$ is normally distributed with mean $\\mathbb{E}[W_T] = 0$ and variance $\\text{Var}(W_T) = T$.\nSubstituting the variance of $W_T$:\n$$ \\text{Var}(I_T) = a^2 T $$\n\n**Part 3: Verification of the Itô Isometry**\n\nThe Itô isometry states that for a predictable process $K_s$, $\\mathbb{E}\\left[\\left(\\int_{0}^{T} K_s \\,dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\int_{0}^{T} K_s^2 \\,ds\\right]$. The problem asks to verify this for $H_s = a$ by showing that $\\mathbb{E}\\left[\\left(\\int_{0}^{T} H_s \\,dW_s\\right)^2\\right] = \\int_{0}^{T} \\mathbb{E}[H_s^2] \\,ds$.\n\nLet's compute the left-hand side (LHS) and the right-hand side (RHS) separately.\n\n**Left-Hand Side (LHS):**\nThe LHS is $\\mathbb{E}\\left[\\left(\\int_{0}^{T} H_s \\,dW_s\\right)^2\\right]$. From Part 1, we have $\\int_{0}^{T} H_s \\,dW_s = a W_T$.\nSo,\n$$ \\text{LHS} = \\mathbb{E}[(a W_T)^2] = \\mathbb{E}[a^2 W_T^2] = a^2 \\mathbb{E}[W_T^2] $$\nThe variance of $W_T$ is defined as $\\text{Var}(W_T) = \\mathbb{E}[W_T^2] - (\\mathbb{E}[W_T])^2$.\nWe know $\\text{Var}(W_T) = T$ and $\\mathbb{E}[W_T] = 0$.\nTherefore, $T = \\mathbb{E}[W_T^2] - 0^2$, which implies $\\mathbb{E}[W_T^2] = T$.\nSubstituting this into the expression for the LHS:\n$$ \\text{LHS} = a^2 T $$\nTo be more rigorous and follow the instruction to verify this \"directly\", we can compute the second moment of the integral of the simple approximating process $H_t^{(n)}$ over a partition $\\pi_n = \\{0=t_0, \\dots, t_n=T\\}$. Let $\\Delta W_i = W_{t_{i+1}} - W_{t_i}$.\n$$ \\mathbb{E}\\left[\\left(\\int_0^T H_s^{(n)} \\,dW_s\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{i=0}^{n-1} a \\Delta W_i\\right)^2\\right] = a^2 \\mathbb{E}\\left[\\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1} \\Delta W_i \\Delta W_j \\right] $$\nBy linearity of expectation:\n$$ a^2 \\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1} \\mathbb{E}[\\Delta W_i \\Delta W_j] $$\nThe increments of Brownian motion are independent over non-overlapping intervals. Thus, for $i \\ne j$, $\\Delta W_i$ and $\\Delta W_j$ are independent.\nSince $\\mathbb{E}[\\Delta W_i] = \\mathbb{E}[W_{t_{i+1}} - W_{t_i}] = 0$ for all $i$, we have for $i \\ne j$:\n$$ \\mathbb{E}[\\Delta W_i \\Delta W_j] = \\mathbb{E}[\\Delta W_i] \\mathbb{E}[\\Delta W_j] = 0 \\cdot 0 = 0 $$\nThe double summation reduces to the terms where $i=j$:\n$$ a^2 \\sum_{i=0}^{n-1} \\mathbb{E}[(\\Delta W_i)^2] $$\nSince $\\mathbb{E}[\\Delta W_i] = 0$, we have $\\mathbb{E}[(\\Delta W_i)^2] = \\text{Var}(\\Delta W_i) = t_{i+1} - t_i$.\nThe expression becomes:\n$$ a^2 \\sum_{i=0}^{n-1} (t_{i+1} - t_i) = a^2 (t_n - t_0) = a^2 (T - 0) = a^2 T $$\nSince this result holds for any partition, the second moment of the limit is also $a^2 T$. This rigorously confirms our LHS calculation.\n\n**Right-Hand Side (RHS):**\nThe RHS is $\\int_{0}^{T}\\mathbb{E}[H_{s}^{2}]\\,ds$.\nThe process is $H_s = a$, which is a constant.\nSo, $H_s^2 = a^2$.\nSince $a^2$ is a deterministic constant, its expectation is itself: $\\mathbb{E}[H_s^2] = \\mathbb{E}[a^2] = a^2$.\nNow we compute the integral:\n$$ \\text{RHS} = \\int_{0}^{T} a^2 \\,ds $$\nSince $a^2$ is a constant with respect to the integration variable $s$, we have:\n$$ \\text{RHS} = a^2 \\int_{0}^{T} 1 \\,ds = a^2 [s]_{0}^{T} = a^2(T-0) = a^2 T $$\n\n**Conclusion of Verification:**\nWe have found that LHS $= a^2 T$ and RHS $= a^2 T$. Thus, LHS $=$ RHS, which verifies that the Itô isometry holds for the constant process $H_t = a$.\n\nThe final answer requires the expression for the Itô integral and its variance.\nExpression for the Itô integral: $aW_T$.\nVariance of the Itô integral: $a^2 T$.", "answer": "$$ \\boxed{ \\begin{pmatrix} aW_T  a^2 T \\end{pmatrix} } $$", "id": "3065258"}, {"introduction": "With the Itô integral established, we can now tackle the core task of finding a predictable representation for a given martingale. This practice explores martingales defined as the conditional expectation of a future value of Brownian motion raised to a power, $M_t = \\mathbb{E}[W_T^n | \\mathcal{F}_t]$. You will use the connection between this expectation and the heat equation to find the predictable integrand $H_t$, revealing a fascinating link to the family of Hermite polynomials. [@problem_id:3065239]", "problem": "Let $(W_t)_{t\\ge 0}$ be a one-dimensional standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ with the natural filtration. Fix $T0$ and an integer $n\\ge 1$. Consider the random variable $\\xi = W_T^n$ and the martingale $M_t = \\mathbb{E}\\!\\left[\\xi \\mid \\mathcal{F}_t\\right]$. The Brownian filtration has the Predictable Representation Property (PRP), meaning any square-integrable $(\\mathcal{F}_t)$-martingale can be represented as a stochastic integral with respect to $W$. \n\nStarting from the Markov property of Brownian motion, the independence and Gaussianity of increments, the generator characterization of the heat semigroup, and Itô’s formula, carry out the following:\n\n- Show that there exists a function $u:[0,T]\\times \\mathbb{R}\\to \\mathbb{R}$ with $M_t = u(t,W_t)$, where $u$ solves the backward heat equation $u_t + \\tfrac{1}{2}u_{xx} = 0$ with terminal condition $u(T,x) = x^n$.\n\n- Derive an explicit closed-form expression for $u(t,x)$ by writing $u(t,x) = \\mathbb{E}\\!\\left[(x+Z)^n\\right]$ with $Z \\sim \\mathcal{N}(0,T-t)$ independent of $W_t$, and evaluating the Gaussian moments. Identify the resulting polynomial family in $x$ as scaled probabilists’ Hermite polynomials.\n\n- Using the backward heat equation and Itô’s formula, deduce the martingale representation $M_t = M_0 + \\int_0^t H_s\\,\\mathrm{d}W_s$ and determine the integrand $H_t$ as $H_t = u_x(t,W_t)$.\n\nCompute $H_t$ explicitly for the cases $n=1$, $n=2$, $n=3$, and $n=4$. Provide your final answer as a single row vector containing the four expressions for $H_t$ in the order $n=1,2,3,4$. No approximation or rounding is required.", "solution": "Let $(W_t)_{t \\ge 0}$ be a standard one-dimensional Brownian motion on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\ge 0}, \\mathbb{P})$, with $(\\mathcal{F}_t)_{t \\ge 0}$ being the natural filtration generated by $W$. We are given a terminal time $T0$, an integer $n \\ge 1$, and a random variable $\\xi = W_T^n$. We consider the martingale $M_t = \\mathbb{E}[\\xi \\mid \\mathcal{F}_t]$ for $t \\in [0,T]$.\n\n### Part 1: PDE Formulation for the Martingale\n\nThe martingale $M_t$ is the conditional expectation of $\\xi = W_T^n$ given the information available at time $t$, which is represented by the sigma-algebra $\\mathcal{F}_t$. Due to the Markov property of Brownian motion, this conditional expectation depends only on the current value of the process, $W_t$.\n$$\nM_t = \\mathbb{E}[W_T^n \\mid \\mathcal{F}_t] = \\mathbb{E}[W_T^n \\mid W_t]\n$$\nThis implies that there exists a deterministic function $u(t,x)$ such that $M_t = u(t, W_t)$. To find this function, we express the conditional expectation explicitly.\n$$\nu(t,x) = \\mathbb{E}[W_T^n \\mid W_t = x]\n$$\nWe can write $W_T = W_t + (W_T - W_t)$. The increment $W_T - W_t$ is independent of $\\mathcal{F}_t$ and thus independent of $W_t$. The distribution of this increment is Gaussian with mean $0$ and variance $T-t$. So, conditioning on $W_t = x$, the random variable $W_T$ has the same distribution as $x + Z$, where $Z \\sim \\mathcal{N}(0, T-t)$.\n$$\nu(t,x) = \\mathbb{E}[(x+Z)^n] \\quad \\text{where } Z \\sim \\mathcal{N}(0, T-t)\n$$\nThis is an application of the Feynman-Kac theorem. The generator of a standard one-dimensional Brownian motion is the operator $\\mathcal{A} = \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2}$. The function $u(t,x)$ defined as $\\mathbb{E}[f(X_T) \\mid X_t=x]$ for a diffusion $X$ with generator $\\mathcal{A}$ and terminal value $f(x)$ must satisfy the backward Kolmogorov equation $(\\partial_t + \\mathcal{A})u = 0$.\nIn our case, with $X_t = W_t$ and $f(x) = x^n$, the function $u(t,x)$ must satisfy:\n$$\n\\frac{\\partial u}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 u}{\\partial x^2} = 0, \\quad \\text{for } (t,x) \\in [0,T) \\times \\mathbb{R}\n$$\nThis is the backward heat equation. The terminal condition at $t=T$ is given by:\n$$\nu(T,x) = \\mathbb{E}[W_T^n \\mid W_T=x] = x^n\n$$\nThus, we have shown that $M_t = u(t,W_t)$, where $u(t,x)$ is the solution to the backward heat equation $u_t + \\frac{1}{2} u_{xx} = 0$ with the terminal condition $u(T,x) = x^n$.\n\n### Part 2: Explicit Expression for $u(t,x)$\n\nWe derive the explicit form of $u(t,x)$ by evaluating the expectation $u(t,x) = \\mathbb{E}[(x+Z)^n]$, where $Z \\sim \\mathcal{N}(0, T-t)$. Let $\\sigma^2 = T-t$.\nUsing the binomial theorem, we have:\n$$\nu(t,x) = \\mathbb{E}\\left[\\sum_{k=0}^{n} \\binom{n}{k} x^{n-k} Z^k\\right] = \\sum_{k=0}^{n} \\binom{n}{k} x^{n-k} \\mathbb{E}[Z^k]\n$$\nThe moments of a centered Gaussian random variable $Z \\sim \\mathcal{N}(0,\\sigma^2)$ are given by:\n$$\n\\mathbb{E}[Z^k] =\n\\begin{cases}\n0  \\text{if } k \\text{ is odd} \\\\\n\\sigma^k (k-1)!! = \\sigma^k \\frac{k!}{(k/2)! 2^{k/2}}  \\text{if } k \\text{ is even}\n\\end{cases}\n$$\nSubstituting $k=2j$ for the even-indexed terms, the sum becomes:\n$$\nu(t,x) = \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\binom{n}{2j} x^{n-2j} \\mathbb{E}[Z^{2j}] = \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\frac{n!}{(2j)!(n-2j)!} x^{n-2j} (T-t)^j \\frac{(2j)!}{j! 2^j}\n$$\nSimplifying the expression, we obtain the closed-form for $u(t,x)$:\n$$\nu(t,x) = \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\frac{n!}{(n-2j)! j!} \\left(\\frac{T-t}{2}\\right)^j x^{n-2j}\n$$\nThis family of polynomials in $x$ with parameter $\\tau = T-t$ are known as the heat polynomials, which we denote by $p_n(x, \\tau)$. So, $u(t,x) = p_n(x, T-t)$. These polynomials are a generalization of the probabilists' Hermite polynomials $He_n(x)$, which can be recovered by setting the time parameter to $-1$, i.e., $He_n(x) = p_n(x,-1)$.\n\n### Part 3: Martingale Representation and the Integrand $H_t$\n\nThe Predictable Representation Property (PRP) states that any $(\\mathcal{F}_t)$-martingale (in this case, since $\\xi=W_T^n \\in L^2$, $M_t$ is a square-integrable martingale) can be written as a stochastic integral with respect to the Brownian motion $W$. We seek the predictable process $H_s$ such that $M_t = M_0 + \\int_0^t H_s \\, \\mathrm{d}W_s$.\n\nWe apply Itô's formula to the process $M_t = u(t,W_t)$:\n$$\n\\mathrm{d}M_t = \\frac{\\partial u}{\\partial t}(t,W_t)\\,\\mathrm{d}t + \\frac{\\partial u}{\\partial x}(t,W_t)\\,\\mathrm{d}W_t + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(t,W_t)\\,\\mathrm{d}\\langle W,W \\rangle_t\n$$\nSince $\\mathrm{d}\\langle W,W \\rangle_t = \\mathrm{d}t$, we can group the terms:\n$$\n\\mathrm{d}M_t = \\left(\\frac{\\partial u}{\\partial t}(t,W_t) + \\frac{1}{2}\\frac{\\partial^2 u}{\\partial x^2}(t,W_t)\\right)\\,\\mathrm{d}t + \\frac{\\partial u}{\\partial x}(t,W_t)\\,\\mathrm{d}W_t\n$$\nFrom Part 1, we know that $u(t,x)$ solves the backward heat equation $u_t + \\frac{1}{2}u_{xx} = 0$. Therefore, the drift term (the term multiplying $\\mathrm{d}t$) is zero.\n$$\n\\mathrm{d}M_t = \\frac{\\partial u}{\\partial x}(t,W_t)\\,\\mathrm{d}W_t\n$$\nIntegrating from $s=0$ to $s=t$ yields the integral representation:\n$$\nM_t - M_0 = \\int_0^t \\frac{\\partial u}{\\partial x}(s,W_s)\\,\\mathrm{d}W_s\n$$\nBy comparing this with the general form $M_t = M_0 + \\int_0^t H_s\\,\\mathrm{d}W_s$, we identify the integrand as $H_s = \\frac{\\partial u}{\\partial x}(s,W_s)$. At time $t$, the integrand is $H_t = \\frac{\\partial u}{\\partial x}(t,W_t)$.\n\nTo find $H_t$ explicitly, we compute the derivative of $u(t,x) = p_n(x, T-t)$ with respect to $x$. Let $\\tau=T-t$.\n$$\n\\frac{\\partial u}{\\partial x}(t,x) = \\frac{\\partial}{\\partial x} p_n(x, \\tau) = \\frac{\\partial}{\\partial x} \\sum_{j=0}^{\\lfloor n/2 \\rfloor} \\frac{n!}{(n-2j)! j!} \\left(\\frac{\\tau}{2}\\right)^j x^{n-2j}\n$$\n$$\n= \\sum_{j=0}^{\\lfloor (n-1)/2 \\rfloor} \\frac{n! (n-2j)}{(n-2j)! j!} \\left(\\frac{\\tau}{2}\\right)^j x^{n-2j-1} = n \\sum_{j=0}^{\\lfloor (n-1)/2 \\rfloor} \\frac{(n-1)!}{(n-1-2j)! j!} \\left(\\frac{\\tau}{2}\\right)^j x^{n-1-2j}\n$$\nThe resulting sum is precisely $n \\cdot p_{n-1}(x,\\tau)$. Thus, $\\frac{\\partial u}{\\partial x}(t,x) = n \\cdot p_{n-1}(x, T-t)$.\nThe integrand process is therefore $H_t = n \\cdot p_{n-1}(W_t, T-t)$.\n\n### Part 4: Explicit Computation of $H_t$ for $n=1,2,3,4$\n\nWe use the formula $H_t = n \\cdot p_{n-1}(W_t, T-t)$ and the explicit forms of the heat polynomials $p_k(x,\\tau) = \\sum_{j=0}^{\\lfloor k/2 \\rfloor} \\frac{k!}{(k-2j)! j!} (\\frac{\\tau}{2})^j x^{k-2j}$.\nLet $x=W_t$ and $\\tau=T-t$.\n\n- For $n=1$:\n$H_t = 1 \\cdot p_0(x, \\tau)$.\n$p_0(x,\\tau) = \\frac{0!}{0!0!}(\\frac{\\tau}{2})^0 x^0 = 1$.\nSo, $H_t = 1$.\n\n- For $n=2$:\n$H_t = 2 \\cdot p_1(x, \\tau)$.\n$p_1(x,\\tau) = \\frac{1!}{1!0!}(\\frac{\\tau}{2})^0 x^1 = x$.\nSo, $H_t = 2x = 2W_t$.\n\n- For $n=3$:\n$H_t = 3 \\cdot p_2(x, \\tau)$.\n$p_2(x,\\tau) = \\frac{2!}{2!0!}(\\frac{\\tau}{2})^0 x^2 + \\frac{2!}{0!1!}(\\frac{\\tau}{2})^1 x^0 = x^2 + \\tau$.\nSo, $H_t = 3(x^2 + \\tau) = 3(W_t^2 + T-t)$.\n\n- For $n=4$:\n$H_t = 4 \\cdot p_3(x, \\tau)$.\n$p_3(x,\\tau) = \\frac{3!}{3!0!}(\\frac{\\tau}{2})^0 x^3 + \\frac{3!}{1!1!}(\\frac{\\tau}{2})^1 x^1 = x^3 + 3\\tau x$.\nSo, $H_t = 4(x^3 + 3\\tau x) = 4(W_t^3 + 3(T-t)W_t)$.\n\nThese are the explicit expressions for the integrand $H_t$ for $n=1,2,3,4$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2W_t  3(W_t^2 + T-t)  4(W_t^3 + 3(T-t)W_t) \\end{pmatrix}}\n$$", "id": "3065239"}, {"introduction": "This final exercise generalizes the method from the previous problem to a non-polynomial function, demonstrating the full power of the \"PDE approach\" to the Martingale Representation Property. By representing the conditional expectation as a solution to the heat equation via the Brownian semigroup, we can find the integrand for a much broader class of martingales. This problem solidifies the deep relationship between stochastic analysis and partial differential equations. [@problem_id:3065248]", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t \\geq 0},\\mathbb{P})$ supporting a one-dimensional standard Brownian motion $W=(W_t)_{t \\geq 0}$, with the usual augmentation of the filtration. Let $T0$ be fixed, and let $f:\\mathbb{R}\\to\\mathbb{R}$ be given by $f(x)=\\exp(-\\gamma x^{2})$ for a constant $\\gamma0$. Define the process $M=(M_t)_{0\\leq t\\leq T}$ by $M_t=\\mathbb{E}[f(W_T)\\mid \\mathcal{F}_t]$. Starting from the Markov property of $W$ and the definition of the Brownian motion semigroup $P_t f(x)=\\mathbb{E}[f(x+W_t)]$, and using the generator of $W$ together with Itō’s formula, derive the stochastic integral representation of $M$ and identify the unique predictable integrand $H=(H_t)_{0\\leq t\\leq T}$ given by the Predictable Representation Property (PRP). Then, compute $H_t$ explicitly in closed form for the given $f$. Your final answer must be a single analytical expression for $H_t$ in terms of $W_t$, $t$, $T$, and $\\gamma$.", "solution": "We begin with the fundamental properties of Brownian motion. The process $W=(W_t)_{t \\geq 0}$ has independent increments and, for $st$, the increment $W_t-W_s$ is independent of $\\mathcal{F}_s$ and is normally distributed with mean $0$ and variance $t-s$. The Markov property implies that, for any bounded Borel function $f$, one has\n$$\n\\mathbb{E}\\!\\left[f(W_T)\\mid \\mathcal{F}_t\\right]=\\mathbb{E}\\!\\left[f(W_t+(W_T-W_t))\\mid \\mathcal{F}_t\\right]=\\mathbb{E}\\!\\left[f(W_t+Z_{T-t})\\right],\n$$\nwhere $Z_{T-t}\\sim \\mathcal{N}(0,T-t)$ is independent of $\\mathcal{F}_t$. This motivates the Brownian motion semigroup defined by\n$$\nP_h f(x)=\\mathbb{E}\\!\\left[f(x+W_h)\\right], \\quad h\\geq 0, \\ x\\in\\mathbb{R},\n$$\nso that $M_t=P_{T-t}f(W_t)$.\n\nThe generator $\\mathcal{L}$ of $W$ acts on twice continuously differentiable functions $g$ via $\\mathcal{L}g(x)=\\frac{1}{2}g''(x)$. The function $u(t,x)=P_{T-t}f(x)$ is known to solve the backward heat Partial Differential Equation (PDE)\n$$\n\\partial_t u(t,x)+\\tfrac{1}{2}\\partial_{xx}u(t,x)=0, \\quad (t,x)\\in[0,T)\\times\\mathbb{R}, \\quad u(T,x)=f(x).\n$$\nApplying Itō’s formula to the semimartingale $u(t,W_t)$ yields\n$$\n\\mathrm{d}u(t,W_t)=\\left(\\partial_t u(t,W_t)+\\tfrac{1}{2}\\partial_{xx}u(t,W_t)\\right)\\mathrm{d}t+\\partial_x u(t,W_t)\\,\\mathrm{d}W_t.\n$$\nSince $u$ solves the backward heat PDE, the drift term vanishes, and we obtain\n$$\nu(t,W_t)=u(0,W_0)+\\int_0^t \\partial_x u(s,W_s)\\,\\mathrm{d}W_s,\n$$\nthat is,\n$$\nM_t=P_{T-t}f(W_t)=P_{T}f(W_0)+\\int_0^t \\partial_x P_{T-s}f(W_s)\\,\\mathrm{d}W_s.\n$$\nBy the Predictable Representation Property (PRP) for the Brownian filtration, the integrand is unique (up to indistinguishability), and we identify\n$$\nH_s=\\partial_x P_{T-s}f(W_s), \\quad 0\\leq s\\leq T.\n$$\nIt remains to compute $H_t$ explicitly for $f(x)=\\exp(-\\gamma x^{2})$.\n\nWe compute $P_h f(x)$ by Gaussian convolution. Since $W_h\\sim \\mathcal{N}(0,h)$ has density $\\varphi_h(y)=\\frac{1}{\\sqrt{2\\pi h}}\\exp\\!\\left(-\\frac{y^{2}}{2h}\\right)$, we have\n$$\nP_h f(x)=\\int_{\\mathbb{R}} \\exp\\!\\left(-\\gamma (x+y)^{2}\\right)\\,\\frac{1}{\\sqrt{2\\pi h}}\\,\\exp\\!\\left(-\\frac{y^{2}}{2h}\\right)\\,\\mathrm{d}y.\n$$\nCombine exponents:\n$$\n-\\gamma (x+y)^{2}-\\frac{y^{2}}{2h}=-\\gamma x^{2}-2\\gamma x y-\\left(\\gamma+\\frac{1}{2h}\\right)y^{2}.\n$$\nLet $A=\\gamma+\\frac{1}{2h}$ and $B=2\\gamma x$. Complete the square in $y$:\n$$\n-A y^{2}-B y=-A\\left(y+\\frac{B}{2A}\\right)^{2}+\\frac{B^{2}}{4A}.\n$$\nThus,\n$$\nP_h f(x)=\\frac{1}{\\sqrt{2\\pi h}}\\exp\\!\\left(-\\gamma x^{2}+\\frac{B^{2}}{4A}\\right)\\int_{\\mathbb{R}}\\exp\\!\\left(-A\\left(y+\\frac{B}{2A}\\right)^{2}\\right)\\mathrm{d}y.\n$$\nThe Gaussian integral evaluates to\n$$\n\\int_{\\mathbb{R}}\\exp\\!\\left(-A\\left(y+\\frac{B}{2A}\\right)^{2}\\right)\\mathrm{d}y=\\sqrt{\\frac{\\pi}{A}},\n$$\ngiving\n$$\nP_h f(x)=\\frac{1}{\\sqrt{2\\pi h}}\\sqrt{\\frac{\\pi}{A}}\\exp\\!\\left(-\\gamma x^{2}+\\frac{B^{2}}{4A}\\right)=\\frac{1}{\\sqrt{2hA}}\\exp\\!\\left(-\\gamma x^{2}+\\frac{\\gamma^{2}x^{2}}{A}\\right).\n$$\nNote that $2hA=2h\\gamma+1$, and\n$$\n-\\gamma x^{2}+\\frac{\\gamma^{2}x^{2}}{A}=\\gamma x^{2}\\left(\\frac{\\gamma}{A}-1\\right)=\\gamma x^{2}\\left(\\frac{\\gamma-(\\gamma+\\frac{1}{2h})}{A}\\right)=-\\frac{\\gamma x^{2}}{2hA}.\n$$\nHence,\n$$\nP_h f(x)=\\frac{1}{\\sqrt{1+2\\gamma h}}\\exp\\!\\left(-\\frac{\\gamma x^{2}}{1+2\\gamma h}\\right).\n$$\nDifferentiate with respect to $x$ to obtain\n$$\n\\partial_x P_h f(x)=\\frac{1}{\\sqrt{1+2\\gamma h}}\\left(-\\frac{2\\gamma x}{1+2\\gamma h}\\right)\\exp\\!\\left(-\\frac{\\gamma x^{2}}{1+2\\gamma h}\\right)=-\\frac{2\\gamma x}{(1+2\\gamma h)^{3/2}}\\exp\\!\\left(-\\frac{\\gamma x^{2}}{1+2\\gamma h}\\right).\n$$\nSubstitute $h=T-t$ and $x=W_t$ to obtain\n$$\nH_t=\\partial_x P_{T-t} f(W_t)=-\\frac{2\\gamma W_t}{\\left(1+2\\gamma (T-t)\\right)^{3/2}}\\exp\\!\\left(-\\frac{\\gamma W_t^{2}}{1+2\\gamma (T-t)}\\right).\n$$\nThis $H_t$ is predictable and square-integrable on $[0,T]$ for $\\gamma0$, and it provides the unique PRP integrand in the martingale representation of $M$.", "answer": "$$\\boxed{-\\dfrac{2\\gamma W_t}{\\left(1+2\\gamma (T-t)\\right)^{3/2}}\\exp\\!\\left(-\\dfrac{\\gamma W_t^{2}}{1+2\\gamma (T-t)}\\right)}$$", "id": "3065248"}]}