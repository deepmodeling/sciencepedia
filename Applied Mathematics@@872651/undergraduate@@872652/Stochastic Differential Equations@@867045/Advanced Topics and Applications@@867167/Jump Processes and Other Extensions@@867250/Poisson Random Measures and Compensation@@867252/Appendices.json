{"hands_on_practices": [{"introduction": "Before building complex models, it's essential to understand the fundamental properties of our tools. This first exercise focuses on the compensated Poisson random measure itself, asking you to verify its most basic statistical properties from first principles. By calculating its mean and second moment, you will confirm why \"compensation\" is a fitting name and discover the direct link between the process's variance and its underlying intensity, a cornerstone of the theory.", "problem": "Consider a probability space supporting a Poisson random measure $N$ on $\\mathbb{R}_{+} \\times E$, where $\\mathbb{R}_{+} = (0,\\infty)$ and $(E,\\mathcal{E})$ is a measurable space. Assume $N$ has intensity measure $\\nu$ given by the product of Lebesgue measure on time and a $\\sigma$-finite measure $\\lambda$ on $E$, that is, $\\nu(\\mathrm{d}t,\\mathrm{d}x) = \\mathrm{d}t \\,\\lambda(\\mathrm{d}x)$. The compensated Poisson random measure $\\tilde{N}$ is defined by\n$$\n\\tilde{N}(B) := N(B) - \\nu(B)\n$$\nfor any measurable set $B \\subset \\mathbb{R}_{+} \\times E$ with $\\nu(B)  \\infty$. Let $t > 0$ be fixed and $A \\in \\mathcal{E}$ be a measurable set with $\\lambda(A)  \\infty$. Focus on the random variable $\\tilde{N}\\big((0,t] \\times A\\big)$.\n\nUsing the core definition that for a Poisson random measure with intensity $\\nu$, the count $N(B)$ in a measurable set $B$ with finite intensity $\\nu(B)$ is a Poisson random variable with parameter $\\nu(B)$, and the definition of compensation, derive from first principles:\n- why $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)\\right] = 0$, and\n- an explicit expression for $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)^{2}\\right]$ in terms of $t$ and $\\lambda(A)$.\n\nYour final answer must be a two-entry row matrix, where the first entry is $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)\\right]$ and the second entry is $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)^{2}\\right]$. No numerical rounding is required.", "solution": "Let the set of interest be $B = (0,t] \\times A$. The intensity measure of this set is given by\n$$\n\\mu := \\nu(B) = \\nu\\big((0,t] \\times A\\big) = \\int_{0}^{t} \\mathrm{d}s \\int_{A} \\lambda(\\mathrm{d}x) = t \\lambda(A)\n$$\nSince $t > 0$ and $0 \\leq \\lambda(A)  \\infty$, the intensity $\\mu$ is a finite non-negative real number. According to the problem statement, the random variable $N(B)$, which represents the number of points of the Poisson measure in the set $B$, follows a Poisson distribution with parameter $\\mu$, which we denote as $N(B) \\sim \\text{Poisson}(\\mu)$.\n\nThe random variable of interest is the compensated measure evaluated on this set:\n$$\n\\tilde{N}(B) = \\tilde{N}\\big((0,t] \\times A\\big) = N(B) - \\nu(B) = N(B) - \\mu\n$$\n\n**1. Calculation of the Expectation $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)\\right]$**\n\nWe use the linearity of the expectation operator. The expectation of a Poisson random variable with parameter $\\mu$ is $\\mu$.\n$$\n\\mathbb{E}[\\tilde{N}(B)] = \\mathbb{E}[N(B) - \\mu] = \\mathbb{E}[N(B)] - \\mathbb{E}[\\mu]\n$$\nSince $\\mu$ is a deterministic constant, $\\mathbb{E}[\\mu] = \\mu$. We have $\\mathbb{E}[N(B)] = \\mu$. Therefore,\n$$\n\\mathbb{E}[\\tilde{N}(B)] = \\mu - \\mu = 0\n$$\nThus, the expected value of the compensated Poisson random measure on the set $(0,t] \\times A$ is $0$.\n\n**2. Calculation of the Second Moment $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)^{2}\\right]$**\n\nThe second moment of $\\tilde{N}(B)$ is its variance, since its mean is zero:\n$$\n\\mathbb{E}[\\tilde{N}(B)^2] = \\text{Var}(\\tilde{N}(B))\n$$\nSince $\\mu$ is a deterministic constant, subtracting it does not change the variance:\n$$\n\\text{Var}(\\tilde{N}(B)) = \\text{Var}(N(B) - \\mu) = \\text{Var}(N(B))\n$$\nThe variance of a Poisson-distributed random variable $X \\sim \\text{Poisson}(\\mu)$ is equal to its parameter, $\\text{Var}(X) = \\mu$. Applying this to $N(B) \\sim \\text{Poisson}(\\mu)$, we have:\n$$\n\\text{Var}(N(B)) = \\mu\n$$\nCombining these steps and recalling that $\\mu = t\\lambda(A)$, we get:\n$$\n\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)^{2}\\right] = \\mu = t\\lambda(A)\n$$\nThis fundamental result shows that the variance of the compensated count equals the intensity of the set.\n\nThe two required quantities are:\n- $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)\\right] = 0$\n- $\\mathbb{E}\\!\\left[\\tilde{N}\\big((0,t] \\times A\\big)^{2}\\right] = t\\lambda(A)$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  t\\lambda(A) \\end{pmatrix}}\n$$", "id": "3070078"}, {"introduction": "Stochastic integrals with respect to Poisson measures are the building blocks for modeling jump processes, but their calculus can be tricky. This exercise [@problem_id:3070042] illuminates the crucial role of compensation in creating well-behaved martingales suitable for Itô's calculus. You will explore the relationship between the optional quadratic variation, which tracks the actual squared jumps, and the predictable quadratic variation, which represents its \"expected\" path and is fundamental for martingale theory.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ be a filtered probability space satisfying the usual hypotheses. Let $E$ be a measurable space with sigma-algebra $\\mathcal{E}$. Consider a Poisson random measure (PRM) $N(\\mathrm{d}t,\\mathrm{d}x)$ on $[0,\\infty)\\times E$ with intensity measure $\\nu(\\mathrm{d}x)\\mathrm{d}t$, where $\\nu$ is a sigma-finite measure on $(E,\\mathcal{E})$. Define the compensated Poisson random measure $\\tilde{N}(\\mathrm{d}t,\\mathrm{d}x) = N(\\mathrm{d}t,\\mathrm{d}x) - \\nu(\\mathrm{d}x)\\mathrm{d}t$. Let $H:[0,\\infty)\\times \\Omega \\times E \\to \\mathbb{R}$ be a bounded predictable function such that for each $T>0$ the integral $\\int_0^T \\int_E H(t,\\omega,x)^2 \\,\\nu(\\mathrm{d}x)\\mathrm{d}t$ is finite almost surely. Define the processes\n$$\nX_t := \\int_0^t \\int_E H(s,\\omega,x)\\, N(\\mathrm{d}s,\\mathrm{d}x), \\qquad M_t := \\int_0^t \\int_E H(s,\\omega,x)\\, \\tilde{N}(\\mathrm{d}s,\\mathrm{d}x),\n$$\nfor $t\\ge 0$. Here, the stochastic integrals are well-defined in the usual sense for random measures under the stated boundedness and predictability conditions.\n\nRecall that for a semimartingale $Y$, the optional quadratic variation $[Y]_t$ is defined as the unique increasing càdlàg adapted process such that $Y_t^2 - [Y]_t$ is a local martingale, and for a local martingale $M$, the predictable quadratic variation (also called the predictable bracket) $\\langle M \\rangle_t$ is the unique increasing predictable process such that $M_t^2 - \\langle M \\rangle_t$ is a local martingale.\n\nWhich of the following statements are correct under the setting above?\n\nA. The optional quadratic variation processes satisfy $[X]_t = [M]_t = \\int_0^t \\int_E H(s,x)^2 \\, N(\\mathrm{d}s,\\mathrm{d}x)$ for all $t \\ge 0$, and the predictable quadratic variation of $M$ is $\\langle M \\rangle_t = \\int_0^t \\int_E H(s,x)^2 \\, \\nu(\\mathrm{d}x)\\mathrm{d}s$.\n\nB. The process $X$ is a martingale, and its predictable quadratic variation equals $\\int_0^t \\int_E H(s,x)^2 \\, \\nu(\\mathrm{d}x)\\mathrm{d}s$.\n\nC. The compensator $\\nu(\\mathrm{d}x)\\mathrm{d}t$ subtracts the jumps of $N$, so $M$ has no jumps and therefore $[M]_t \\equiv 0$.\n\nD. In predictable bracket computations, the compensator replaces the random measure $N(\\mathrm{d}t,\\mathrm{d}x)$ by its deterministic intensity $\\nu(\\mathrm{d}x)\\mathrm{d}t$, so for square-integrable predictable $H$ one has $\\langle M \\rangle_t = \\int_0^t \\int_E H(s,x)^2 \\, \\nu(\\mathrm{d}x)\\mathrm{d}s$.\n\nSelect all correct options.", "solution": "### Analysis of the Processes\n\nWe are given two processes:\n1.  $X_t = \\int_0^t \\int_E H(s,x)\\, N(\\mathrm{d}s,\\mathrm{d}x)$, an integral with respect to the Poisson random measure.\n2.  $M_t = \\int_0^t \\int_E H(s,x)\\, \\tilde{N}(\\mathrm{d}s,\\mathrm{d}x)$, an integral with respect to the compensated Poisson random measure.\n\nBy definition, $\\tilde{N} = N - \\nu \\mathrm{d}t$. We can relate $X_t$ and $M_t$:\n$$\nM_t = \\int_0^t \\int_E H(s,x)\\, (N(\\mathrm{d}s,\\mathrm{d}x) - \\nu(\\mathrm{d}x)\\mathrm{d}s) = X_t - \\int_0^t \\int_E H(s,x)\\,\\nu(\\mathrm{d}x)\\mathrm{d}s\n$$\nLet $A_t = \\int_0^t \\int_E H(s,x)\\,\\nu(\\mathrm{d}x)\\mathrm{d}s$. Since $H$ is predictable and $\\nu$ is deterministic, $A_t$ is a predictable process. It is also continuous and has finite variation. Thus, $X_t = M_t + A_t$.\n\nUnder the given conditions, the stochastic integral $M_t$ with respect to the compensated measure is a square-integrable martingale. The process $X_t$ is the sum of a martingale ($M_t$) and a finite-variation process ($A_t$), making $X_t$ a semimartingale. Generally, $A_t$ is non-zero, so $X_t$ is not a martingale.\n\n### Evaluation of Options\n\n**A. The optional quadratic variation processes satisfy $[X]_t = [M]_t = \\int_0^t \\int_E H(s,x)^2 \\, N(\\mathrm{d}s,\\mathrm{d}x)$ for all $t \\ge 0$, and the predictable quadratic variation of $M$ is $\\langle M \\rangle_t = \\int_0^t \\int_E H(s,x)^2 \\, \\nu(\\mathrm{d}x)\\mathrm{d}s$.**\n*   **Optional Quadratic Variation:** The optional quadratic variation $[Y]_t$ is the sum of the squared jumps of the process $Y$ up to time $t$. Since $X_t$ and $M_t$ differ by a continuous process $A_t$, they have the same jumps: $\\Delta X_s = \\Delta M_s$. Therefore, their optional quadratic variations are identical: $[X]_t = [M]_t$. The jumps of $X_t$ are given by the values of $H(s,x)$ at the random points of $N$, so the sum of their squares is correctly expressed as an integral with respect to $N$: $\\int_0^t \\int_E H(s,x)^2 N(\\mathrm{d}s,\\mathrm{d}x)$. This part is correct.\n*   **Predictable Quadratic Variation:** For a martingale defined by an integral with respect to a compensated PRM, its predictable quadratic variation (or bracket) is a fundamental result of stochastic calculus. The formula $\\langle M \\rangle_t = \\int_0^t \\int_E H(s,x)^2 \\nu(\\mathrm{d}x)\\mathrm{d}s$ is correct.\n*   **Verdict:** Statement A is **correct**.\n\n**B. The process $X$ is a martingale, and its predictable quadratic variation equals $\\int_0^t \\int_E H(s,x)^2 \\, \\nu(\\mathrm{d}x)\\mathrm{d}s$.**\n*   As established, $X_t$ is a semimartingale but not a martingale in general due to the drift-like term $A_t$. The concept of a predictable quadratic variation is typically defined for local martingales. Therefore, the statement is incorrect.\n*   **Verdict:** Statement B is **incorrect**.\n\n**C. The compensator $\\nu(\\mathrm{d}x)\\mathrm{d}t$ subtracts the jumps of $N$, so $M$ has no jumps and therefore $[M]_t \\equiv 0$.**\n*   This reflects a misunderstanding of compensation. The compensator is a deterministic (or predictable) continuous measure. Subtracting it does not remove the discrete, random jumps of $N$. The process $M_t$ has exactly the same jumps as $X_t$. Since $X_t$ is a pure-jump process, its jumps are generally non-zero, and thus $[M]_t$ is not identically zero.\n*   **Verdict:** Statement C is **incorrect**.\n\n**D. In predictable bracket computations, the compensator replaces the random measure $N(\\mathrm{d}t,\\mathrm{d}x)$ by its deterministic intensity $\\nu(\\mathrm{d}x)\\mathrm{d}t$, so for square-integrable predictable $H$ one has $\\langle M \\rangle_t = \\int_0^t \\int_E H(s,x)^2 \\, \\nu(\\mathrm{d}x)\\mathrm{d}s$.**\n*   This statement provides both a heuristic and a formula. The formula is correct, as stated in the analysis of option A. The heuristic describes how the predictable bracket $\\langle M \\rangle_t$ relates to the optional variation $[M]_t$. Specifically, $\\langle M \\rangle_t$ is the compensator of $[M]_t$. Since $[M]_t = \\int_0^t \\int_E H(s,x)^2 N(\\mathrm{d}s,\\mathrm{d}x)$, its compensator is found by replacing the random measure $N$ with its intensity measure $\\nu(\\mathrm{d}x)\\mathrm{d}t$. The reasoning is sound and represents a key principle in the calculus of jump processes.\n*   **Verdict:** Statement D is **correct**.\n\nThe correct options are A and D.", "answer": "$$\\boxed{AD}$$", "id": "3070042"}, {"introduction": "The infinitesimal generator acts as a bridge between a stochastic process and deterministic differential equations, providing a complete description of its local behavior. This capstone exercise [@problem_id:3070057] challenges you to apply your knowledge of Itô's formula for jump-diffusions to derive the generator for a general process driven by both Brownian motion and compensated Poisson jumps. This practice demonstrates the power of the compensation framework in neatly decomposing the dynamics into drift, diffusion, and jump components.", "problem": "Consider a filtered probability space supporting an $r$-dimensional standard Brownian motion $W$ and a Poisson random measure $N$ on $(0,\\infty)\\times E$ with compensator $dt\\,\\lambda(dz)$, where $(E,\\mathcal{E})$ is a measurable space and $\\lambda$ is a $\\sigma$-finite measure. Let $\\tilde{N}(dt,dz) \\equiv N(dt,dz) - dt\\,\\lambda(dz)$ denote the compensated Poisson random measure. For dimension $d \\in \\mathbb{N}$, consider the $\\mathbb{R}^{d}$-valued Markov process $X$ solving the stochastic differential equation\n$$\ndX_{t} \\;=\\; b(X_{t-})\\,dt \\;+\\; \\sigma(X_{t-})\\,dW_{t} \\;+\\; \\int_{E} \\gamma(X_{t-},z)\\mathbf{1}_{\\{|\\gamma(X_{t-},z)|\\le 1\\}}\\,\\tilde{N}(dt,dz) \\;+\\; \\int_{E} \\gamma(X_{t-},z)\\mathbf{1}_{\\{|\\gamma(X_{t-},z)| 1\\}}\\,N(dt,dz),\n$$\nwith deterministic initial condition $X_{0}=x\\in\\mathbb{R}^{d}$, where $b:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ and $\\sigma:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times r}$ are measurable functions, and $\\gamma:\\mathbb{R}^{d}\\times E\\to\\mathbb{R}^{d}$ is measurable. Assume the usual conditions that ensure existence of a unique strong solution and that justify Itô’s formula, including the standard Lévy-type integrability condition\n$$\n\\int_{E}\\left(|\\gamma(x,z)|^{2}\\wedge 1 \\right)\\,\\lambda(dz)\\;\\;\\infty\n\\quad\\text{for each }x\\in\\mathbb{R}^{d}.\n$$\nLet $a(x)\\equiv \\sigma(x)\\sigma(x)^{\\top}$. For a test function $\\varphi\\in C_{b}^{2}(\\mathbb{R}^{d})$, define the infinitesimal generator $\\mathcal{L}$ of $X$ via Dynkin’s formula, so that for sufficiently small $t0$,\n$$\n\\mathbb{E}^{x}[\\varphi(X_{t})] - \\varphi(x) \\;=\\; \\int_{0}^{t}\\mathbb{E}^{x}\\big[(\\mathcal{L}\\varphi)(X_{s})\\big]\\,ds.\n$$\nStarting from Itô’s formula for jump-diffusions, derive the analytic expression for $(\\mathcal{L}\\varphi)(x)$ acting on $\\varphi\\in C_{b}^{2}(\\mathbb{R}^{d})$. The final expression should decompose the jump contribution into terms corresponding to small jumps ($|\\gamma(x,z)|\\le 1$) and large jumps ($|\\gamma(x,z)| 1$). Provide your final answer as a single closed-form analytic expression for $(\\mathcal{L}\\varphi)(x)$ in terms of $b$, $\\sigma$, $\\gamma$, and $\\lambda$.", "solution": "The infinitesimal generator $\\mathcal{L}$ of the Markov process $X$ is defined such that the process $\\varphi(X_t) - \\varphi(X_0) - \\int_0^t (\\mathcal{L}\\varphi)(X_s) ds$ is a local martingale. We can derive the form of $(\\mathcal{L}\\varphi)(x)$ by applying Itô's formula to $\\varphi(X_t)$ and collecting all the predictable finite-variation terms (the \"drift\" terms).\n\nThe SDE for $X_t$ is given as the sum of a drift, a continuous diffusion, a compensated small-jump process, and an uncompensated large-jump process.\nThe general Itô formula for a jump-diffusion process $\\varphi(X_t)$ is:\n$$\n\\varphi(X_t) - \\varphi(X_0) = \\int_0^t \\nabla \\varphi(X_{s-})^{\\top} dX_s + \\frac{1}{2} \\int_0^t \\mathrm{Tr}\\left( H_{\\varphi}(X_{s-}) d[X^c, X^c]_s \\right) + \\sum_{0s\\le t} \\left( \\varphi(X_s) - \\varphi(X_{s-}) - \\nabla\\varphi(X_{s-})^{\\top} \\Delta X_s \\right)\n$$\nwhere $X^c$ is the continuous martingale part of $X$.\n\nLet's identify the predictable finite-variation parts that contribute to $(\\mathcal{L}\\varphi)(x)$:\n\n1.  **Drift and Diffusion Terms:** The explicit drift $b(X_{t-})dt$ and the continuous quadratic variation term contribute to the generator. The continuous martingale part is $X^c_t = \\int_0^t \\sigma(X_{s-})dW_s$, so $d[X^c, X^c]_s = \\sigma(X_{s-})\\sigma(X_{s-})^\\top ds = a(X_{s-})ds$. These contribute:\n    $$ \\nabla\\varphi(x)^{\\top}b(x) + \\frac{1}{2} \\mathrm{Tr}(H_\\varphi(x)a(x)) = \\sum_{i=1}^d b_i(x) \\frac{\\partial\\varphi}{\\partial x_i} + \\frac{1}{2}\\sum_{i,j=1}^d a_{ij}(x) \\frac{\\partial^2\\varphi}{\\partial x_i \\partial x_j} $$\n\n2.  **Jump Term:** The jump part of the Itô formula is $\\sum_{0s\\le t} (\\dots) = \\int_0^t \\int_E [\\varphi(X_{s-}+\\gamma) - \\varphi(X_{s-}) - \\nabla\\varphi(X_{s-})^\\top\\gamma] N(ds,dz)$. We use the compensator identity $N(ds,dz) = \\tilde{N}(ds,dz) + \\lambda(dz)ds$ to find the drift contribution from the jumps. This drift is:\n    $$ \\int_E [\\varphi(x+\\gamma(x,z)) - \\varphi(x) - \\nabla\\varphi(x)^\\top\\gamma(x,z)] \\lambda(dz) $$\n    The SDE is defined with a split between small and large jumps. We apply this split to the generator.\n    *   **Small Jumps ($|\\gamma| \\le 1$):** The SDE includes the term $\\int_E \\gamma \\mathbf{1}_{|\\gamma|\\le 1} \\tilde{N}(dt,dz)$. Following the logic above, its contribution to the generator is the compensation term:\n        $$ \\int_E \\left[ \\varphi(x+\\gamma(x,z)) - \\varphi(x) - \\nabla\\varphi(x)^\\top\\gamma(x,z) \\right] \\mathbf{1}_{\\{|\\gamma(x,z)|\\le 1\\}} \\lambda(dz) $$\n    *   **Large Jumps ($|\\gamma| > 1$):** The SDE includes the term $\\int_E \\gamma \\mathbf{1}_{|\\gamma|> 1} N(dt,dz)$. This is a compound Poisson process. It is not compensated in the SDE, so its contribution to the generator is its full compensator:\n        $$ \\int_E \\left[ \\varphi(x+\\gamma(x,z)) - \\varphi(x) \\right] \\mathbf{1}_{\\{|\\gamma(x,z)| > 1\\}} \\lambda(dz) $$\n\nCombining all these parts gives the full expression for the infinitesimal generator $(\\mathcal{L}\\varphi)(x)$.", "answer": "$$\n\\boxed{\n\\begin{aligned}\n(\\mathcal{L}\\varphi)(x) = \\sum_{i=1}^{d} b_i(x) \\frac{\\partial \\varphi(x)}{\\partial x_i} + \\frac{1}{2}\\sum_{i,j=1}^{d} (\\sigma(x)\\sigma(x)^{\\top})_{ij} \\frac{\\partial^2 \\varphi(x)}{\\partial x_i \\partial x_j} \\\\\n+ \\int_{E} \\left[ \\varphi(x+\\gamma(x,z)) - \\varphi(x) - \\sum_{i=1}^{d} \\gamma_i(x,z)\\frac{\\partial\\varphi(x)}{\\partial x_i} \\right] \\mathbf{1}_{\\{|\\gamma(x,z)|\\le 1\\}} \\lambda(dz) \\\\\n+ \\int_{E} \\left[ \\varphi(x+\\gamma(x,z)) - \\varphi(x) \\right] \\mathbf{1}_{\\{|\\gamma(x,z)| 1\\}} \\lambda(dz)\n\\end{aligned}\n}\n$$", "id": "3070057"}]}