{"hands_on_practices": [{"introduction": "A defining feature of a Lévy process $\\{X_t\\}$ is that its distribution at any time $t > 0$ must be infinitely divisible. This exercise [@problem_id:3063708] provides a hands-on method to test this property using characteristic functions, a crucial tool in probability theory. By analyzing the simple and familiar uniform distribution, you will prove it cannot be the law of a Lévy process, thereby gaining a deeper appreciation for the structural constraints imposed by stationary, independent increments.", "problem": "Let $\\left(X_{t}\\right)_{t \\geq 0}$ be a Lévy process, that is, a stochastic process with stationary and independent increments and $X_{0}=0$ almost surely. It is a well-tested fact that for each fixed $t>0$, the law of $X_{t}$ is infinitely divisible and its characteristic function has the Lévy–Khintchine representation $\\varphi_{X_{t}}(\\theta)=\\exp\\!\\left(t\\,\\Psi(\\theta)\\right)$ for a suitable characteristic exponent $\\Psi$. Consider the uniform distribution on $\\left[0,1\\right]$ as a candidate marginal law for $X_{t}$.\n\nStarting from the definition of a characteristic function $\\varphi_{Y}(\\theta)=\\mathbb{E}\\left[\\exp\\!\\left(i\\,\\theta\\,Y\\right)\\right]$, do the following:\n- derive the characteristic function of a random variable $U$ uniformly distributed on $\\left[0,1\\right]$;\n- compute the smallest positive real number $\\theta^{\\ast}>0$ such that $\\varphi_{U}\\!\\left(\\theta^{\\ast}\\right)=0$;\n- use this to justify why the uniform distribution on $\\left[0,1\\right]$ cannot be the law of $X_{t}$ for any Lévy process at any $t>0$.\n\nProvide as your final answer the value of $\\theta^{\\ast}$. No rounding is required, and no units are involved.", "solution": "We begin from the definition of a characteristic function. If $U \\sim \\mathrm{Unif}\\!\\left(0,1\\right)$, then for any real $\\theta$,\n\n$$\n\\varphi_{U}(\\theta)=\\mathbb{E}\\!\\left[\\exp\\!\\left(i\\,\\theta\\,U\\right)\\right]=\\int_{0}^{1}\\exp\\!\\left(i\\,\\theta\\,x\\right)\\,\\mathrm{d}x.\n$$\n\nThis integral can be computed explicitly:\n\n$$\n\\varphi_{U}(\\theta)=\\left.\\frac{\\exp\\!\\left(i\\,\\theta\\,x\\right)}{i\\,\\theta}\\right|_{x=0}^{x=1}=\\frac{\\exp\\!\\left(i\\,\\theta\\right)-1}{i\\,\\theta}.\n$$\n\nAn equivalent factorized form that makes zeros apparent is obtained by multiplying numerator and denominator by $\\exp\\!\\left(-i\\,\\theta/2\\right)$:\n\n$$\n\\varphi_{U}(\\theta)=\\exp\\!\\left(i\\,\\frac{\\theta}{2}\\right)\\,\\frac{\\exp\\!\\left(i\\,\\frac{\\theta}{2}\\right)-\\exp\\!\\left(-i\\,\\frac{\\theta}{2}\\right)}{i\\,\\theta}\n=\\exp\\!\\left(i\\,\\frac{\\theta}{2}\\right)\\,\\frac{2\\,\\sin\\!\\left(\\frac{\\theta}{2}\\right)}{\\theta}.\n$$\n\nThus, the zeros of $\\varphi_{U}$ occur exactly when $\\sin\\!\\left(\\frac{\\theta}{2}\\right)=0$ with $\\theta \\neq 0$ (to avoid the removable singularity at $\\theta=0$). Hence,\n\n$$\n\\frac{\\theta}{2}=\\pi\\,k \\quad \\text{for} \\quad k \\in \\mathbb{Z}\\setminus\\{0\\} \\;\\;\\Longleftrightarrow\\;\\; \\theta=2\\pi\\,k,\\; k \\in \\mathbb{Z}\\setminus\\{0\\}.\n$$\n\nThe smallest positive zero is therefore\n\n$$\n\\theta^{\\ast}=2\\pi.\n$$\n\n\nWe now justify why the uniform distribution on $\\left[0,1\\right]$ cannot be the law of $X_{t}$ for any Lévy process at any $t>0$. A Lévy process has stationary and independent increments and $X_{0}=0$ almost surely. A well-tested and fundamental characterization states that for each fixed $t>0$, the characteristic function of $X_{t}$ admits the Lévy–Khintchine representation\n\n$$\n\\varphi_{X_{t}}(\\theta)=\\exp\\!\\left(t\\,\\Psi(\\theta)\\right),\n$$\n\nwhere $\\Psi$ is the characteristic (Lévy–Khintchine) exponent. Since the exponential function never vanishes, $\\varphi_{X_{t}}(\\theta)\\neq 0$ for all real $\\theta$. In particular, the characteristic function of any $X_{t}$ has no real zeros.\n\nBy contrast, we have shown that $\\varphi_{U}(\\theta)$ has real zeros, with the smallest positive one at $\\theta^{\\ast}=2\\pi$. Therefore, the uniform distribution on $\\left[0,1\\right]$ is not infinitely divisible and cannot coincide with the law of $X_{t}$ for any Lévy process at any $t>0$.\n\nConsequently, the requested smallest positive zero is $\\theta^{\\ast}=2\\pi$.", "answer": "$$\\boxed{2\\pi}$$", "id": "3063708"}, {"introduction": "Having established a key distributional property, we now turn to the structure of the sample paths themselves for the compound Poisson process, a canonical pure-jump Lévy process. This practice [@problem_id:3063766] challenges you to connect the macroscopic behavior of the path to its microscopic jump components. You will derive the total variation and quadratic variation, two fundamental measures of a path's roughness, directly from the underlying jumps.", "problem": "Consider a compound Poisson process $\\{X_t\\}_{t \\ge 0}$ defined by $X_t = \\sum_{k=1}^{N_t} J_k$, where $\\{N_t\\}_{t \\ge 0}$ is a Poisson process with rate $\\lambda > 0$, and $\\{J_k\\}_{k \\ge 1}$ are independent and identically distributed jump sizes with $\\mathbb{E}[|J_1|]  \\infty$, independent of $\\{N_t\\}_{t \\ge 0}$. The sample paths of $X$ are right-continuous with left limits and have only jump discontinuities. For a fixed time horizon $t > 0$, recall the pathwise total variation on $[0,t]$,\n$$\nV_t(X) \\equiv \\sup_{\\pi} \\sum_{i=1}^{n} \\left| X_{t_i} - X_{t_{i-1}} \\right|, \\quad \\pi = \\{0 = t_0  t_1  \\cdots  t_n = t\\},\n$$\nand the quadratic variation on $[0,t]$,\n$$\n[X]_t \\equiv \\lim_{\\|\\pi\\| \\to 0} \\sum_{i=1}^{n} \\left( X_{t_i} - X_{t_{i-1}} \\right)^2, \\quad \\|\\pi\\| \\equiv \\max_{1 \\le i \\le n} (t_i - t_{i-1}).\n$$\nStarting from these definitions and the structural properties of compound Poisson processes (in particular, that there are almost surely only finitely many jumps on any finite interval and the process is piecewise constant between jumps), derive explicit expressions for $V_t(X)$ and $[X]_t$ on $[0,t]$ in terms of $N_t$ and the jump sizes $\\{J_k\\}_{k=1}^{N_t}$. Express your final answer as a single row vector using the $\\mathrm{pmatrix}$ environment, with the first entry equal to $V_t(X)$ and the second entry equal to $[X]_t$.", "solution": "Let $\\{X_t\\}_{t \\ge 0}$ be a compound Poisson process as defined. A key feature of such a process is that its sample paths are step functions. For any finite time interval $[0, t]$, the Poisson process $\\{N_s\\}_{s \\in [0,t]}$ will almost surely have a finite number of jumps, say $N_t$. Let these jump times be $0  \\tau_1  \\tau_2  \\cdots  \\tau_{N_t} \\le t$. The process $X_t$ is constant between these jump times. The value of the process is given by\n$$\nX_s = \\sum_{k=1}^{N_s} J_k.\n$$\nAt each jump time $\\tau_k$, the process has a discontinuity. The jump size at $\\tau_k$ is given by the random variable $J_k$. Specifically, the change in the process value at $\\tau_k$ is\n$$\n\\Delta X_{\\tau_k} \\equiv X_{\\tau_k} - X_{\\tau_k^-} = \\left(\\sum_{i=1}^k J_i\\right) - \\left(\\sum_{i=1}^{k-1} J_i\\right) = J_k,\n$$\nwhere $X_{\\tau_k^-} = \\lim_{s \\uparrow \\tau_k} X_s$. For any time $s \\in [0,t]$ that is not a jump time, $\\Delta X_s = 0$.\n\n**1. Calculation of Total Variation, $V_t(X)$**\n\nThe total variation of $X$ on $[0,t]$ is given by $V_t(X) = \\sup_{\\pi} \\sum_{i=1}^{n} | X_{t_i} - X_{t_{i-1}} |$. Let $\\pi = \\{0 = t_0  t_1  \\cdots  t_n = t\\}$ be an arbitrary partition of $[0,t]$. The term $|X_{t_i} - X_{t_{i-1}}|$ is non-zero only if the interval $(t_{i-1}, t_i]$ contains at least one jump time $\\tau_k$. The change $X_{t_i} - X_{t_{i-1}}$ is the sum of all jumps occurring in $(t_{i-1}, t_i]$.\nBy the triangle inequality, for any interval $(t_{i-1}, t_i]$, we have\n$$\n|X_{t_i} - X_{t_{i-1}}| = \\left| \\sum_{k: \\tau_k \\in (t_{i-1}, t_i]} J_k \\right| \\le \\sum_{k: \\tau_k \\in (t_{i-1}, t_i]} |J_k|.\n$$\nSumming over all intervals of the partition, we get\n$$\n\\sum_{i=1}^{n} | X_{t_i} - X_{t_{i-1}} | \\le \\sum_{i=1}^{n} \\sum_{k: \\tau_k \\in (t_{i-1}, t_i]} |J_k| = \\sum_{k=1}^{N_t} |J_k|.\n$$\nThis inequality holds for any partition $\\pi$. The expression on the right-hand side, $\\sum_{k=1}^{N_t} |J_k|$, is an upper bound for the sums over all partitions. To show that this is the least upper bound (supremum), we need to construct a sequence of partitions for which the sum approaches this value.\n\nConsider a specific partition $\\pi^*$ that includes all jump times $\\tau_k$ as partition points. For such a partition, we can isolate each jump. The sum of absolute differences is precisely $\\sum_{k=1}^{N_t} |J_k|$. Because this value can be achieved, it must be the supremum.\n\nThus, the total variation is the sum of the absolute magnitudes of all jumps occurring up to time $t$.\n$$\nV_t(X) = \\sum_{k=1}^{N_t} |J_k|.\n$$\n\n**2. Calculation of Quadratic Variation, $[X]_t$**\n\nThe quadratic variation is defined as the limit of the sum of squared increments as the mesh of the partition goes to zero: $[X]_t = \\lim_{\\|\\pi\\| \\to 0} \\sum_{i=1}^{n} ( X_{t_i} - X_{t_{i-1}} )^2$.\nLet $\\pi = \\{0=t_0, t_1, \\ldots, t_n=t\\}$ be a partition of $[0,t]$. Almost surely, there is a finite number $N_t$ of jumps in $[0,t]$. As the mesh $\\|\\pi\\| \\to 0$, we can ensure that any partition interval $(t_{i-1}, t_i]$ contains at most one jump time $\\tau_k$. This is because the minimum distance between distinct jump times is almost surely positive.\n\nLet's analyze the sum $\\sum_{i=1}^{n} ( X_{t_i} - X_{t_{i-1}} )^2$ for a fine partition.\n- If an interval $(t_{i-1}, t_i]$ contains no jump times, then $X_t$ is constant on this interval. Thus, $X_{t_i} - X_{t_{i-1}} = 0$, and its contribution to the sum is $0^2 = 0$.\n- If an interval $(t_{i-1}, t_i]$ contains exactly one jump time $\\tau_k$, then $X_{t_i} - X_{t_{i-1}} = X_{\\tau_k} - X_{\\tau_k^-} = J_k$. Its contribution to the sum is $J_k^2$.\n\nAs $\\|\\pi\\| \\to 0$, the partition becomes fine enough to isolate every one of the $N_t$ jumps into its own distinct interval. The sum of squared increments will therefore converge to the sum of the squares of the individual jumps.\n$$\n\\lim_{\\|\\pi\\| \\to 0} \\sum_{i=1}^{n} ( X_{t_i} - X_{t_{i-1}} )^2 = \\sum_{k=1}^{N_t} J_k^2.\n$$\nThis is a standard result for pure-jump processes. The quadratic variation of a càdlàg process is the sum of the squares of its jumps plus the quadratic variation of its continuous part. For a compound Poisson process, the continuous part is zero, so its quadratic variation is zero. The jumps are $\\Delta X_s$ which are non-zero only for $s \\in \\{\\tau_1, \\dots, \\tau_{N_t}\\}$.\nHence, the quadratic variation is the sum of the squares of all jumps up to time $t$:\n$$\n[X]_t = \\sum_{s \\le t} (\\Delta X_s)^2 = \\sum_{k=1}^{N_t} J_k^2.\n$$\n\nThe final expressions for the total variation and quadratic variation are $V_t(X) = \\sum_{k=1}^{N_t} |J_k|$ and $[X]_t = \\sum_{k=1}^{N_t} J_k^2$, respectively.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{k=1}^{N_t} |J_k|  \\sum_{k=1}^{N_t} J_k^2 \\end{pmatrix}}\n$$", "id": "3063766"}, {"introduction": "This final practice bridges the gap between theoretical understanding and computational implementation, a vital skill for any modern practitioner. Building on the structure of the compound Poisson process, you will first derive its key statistical moments and characteristic function using foundational probabilistic laws [@problem_id:3063719]. You will then translate this knowledge into a simulation algorithm, bringing the abstract process to life and verifying theory through computation.", "problem": "A compound Poisson process is a canonical example of a pure-jump Lévy process with stationary independent increments. Let $t \\geq 0$ and fix a rate parameter $\\lambda  0$. Consider a Poisson process $\\{N_s : s \\geq 0\\}$ with rate $\\lambda$ and an independent sequence of identically distributed jump sizes $\\{Y_i\\}_{i \\geq 1}$ with common distribution $F$ and finite second moment. Define the compound Poisson process $\\{X_s : s \\geq 0\\}$ by\n$$\nX_s = \\sum_{i=1}^{N_s} Y_i, \\quad s \\geq 0,\n$$\nwith the convention that $X_s = 0$ if $N_s = 0$. The process $\\{X_s\\}$ has stationary independent increments and is a Lévy process. The Characteristic Function (CF) of a random variable $Z$ is defined by $\\varphi_Z(u) = \\mathbb{E}[e^{i u Z}]$, where $i = \\sqrt{-1}$.\n\nStarting only from the definitions of the Poisson process and independence, derive how to simulate the process $X_s$ on the interval $[0,t]$ by first sampling the number of jumps and then sampling the jump sizes. Derive the distributional properties needed to justify the simulation and compute the expectation $\\mathbb{E}[X_t]$, the variance $\\mathrm{Var}(X_t)$, and the CF $\\varphi_{X_t}(u)$ in terms of $\\lambda$, $t$, and the distribution of $Y_1$.\n\nThen, implement a program that:\n- For each specified test case, simulates a single realization of $X_t$ by:\n  1. Sampling $N_t \\sim \\mathrm{Poisson}(\\lambda t)$.\n  2. Sampling $N_t$ independent and identically distributed jump sizes from the specified distribution $F$.\n  3. Computing $X_t = \\sum_{i=1}^{N_t} Y_i$.\n- Computes, for each test case, the theoretically derived quantities $\\mathbb{E}[X_t]$, $\\mathrm{Var}(X_t)$, and the CF $\\varphi_{X_t}(u)$ evaluated at the given real argument $u$, using only your derivations.\n- Uses a fixed random seed per test case to ensure deterministic output.\n\nYour program must use the following test suite of parameter values, which collectively test typical and boundary scenarios:\n1. $t = 5.0$, $\\lambda = 1.2$, $F$ is Normal with mean $\\mu = 0.5$ and standard deviation $\\sigma = 0.3$, and CF argument $u = 1.0$. Seed $= 42$.\n2. $t = 5.0$, $\\lambda = 0.0$, $F$ is Normal with mean $\\mu = 0.5$ and standard deviation $\\sigma = 0.3$, and CF argument $u = 1.0$. Seed $= 123$.\n3. $t = 0.0$, $\\lambda = 3.0$, $F$ is Exponential with rate $\\beta = 2.0$ (that is, mean $1/\\beta$), and CF argument $u = 0.5$. Seed $= 7$.\n4. $t = 1.0$, $\\lambda = 5.0$, $F$ is symmetric Bernoulli with values $\\pm 1$ each with probability $1/2$, and CF argument $u = 2.0$. Seed $= 99$.\n\nFor each test case, your program must output a list in the order:\n$$\n\\left[ N_t, \\; X_t, \\; \\mathbb{E}[X_t], \\; \\mathrm{Var}(X_t), \\; \\operatorname{Re}\\left(\\varphi_{X_t}(u)\\right), \\; \\operatorname{Im}\\left(\\varphi_{X_t}(u)\\right) \\right],\n$$\nwhere $N_t$ is an integer and the other quantities are real numbers. Your program should produce a single line of output containing all test case results as a comma-separated list of these lists, enclosed in square brackets, for example:\n$$\n\\big[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\big].\n$$\nNo physical units are involved; angles for characteristic functions are in radians.", "solution": "### 1. Justification of the Simulation Method\nThe compound Poisson process $\\{X_s : s \\geq 0\\}$ is defined by the random sum:\n$$\nX_s = \\sum_{i=1}^{N_s} Y_i\n$$\nwhere $\\{N_s : s \\geq 0\\}$ is a Poisson process with rate $\\lambda$, and $\\{Y_i\\}_{i \\geq 1}$ is a sequence of independent and identically distributed (i.i.d.) random variables, also independent of the process $\\{N_s\\}$. By convention, if the number of jumps $N_s$ is zero, the sum is empty and $X_s = 0$.\n\nTo simulate a single realization of the process at a fixed time $t > 0$, we are asked to first sample the number of jumps and then the jump sizes. This procedure is a direct implementation of the definition of the process and relies on the properties of conditional probability. The joint distribution of $(N_t, X_t)$ can be factored as $P(N_t=n, X_t \\leq x) = P(X_t \\leq x | N_t=n) P(N_t=n)$. The simulation algorithm follows this factorization:\n\n1.  **Sample the number of jumps:** The random variable $N_t$ counts the number of events of a Poisson process in the interval $[0, t]$. Its distribution is known to be Poisson with parameter $\\lambda t$. We draw a single integer sample, let's call it $n$, from this distribution: $n \\sim \\mathrm{Poisson}(\\lambda t)$.\n\n2.  **Sample the jump sizes:** Conditional on $N_t = n$, the process value is $X_t = \\sum_{i=1}^{n} Y_i$. Since the jump sizes $\\{Y_i\\}$ are i.i.d. and independent of $N_t$, this conditional sum is simply a sum of $n$ i.i.d. random variables drawn from their common distribution $F$. We therefore generate $n$ independent samples, $y_1, y_2, \\dots, y_n$, from the distribution $F$.\n\n3.  **Compute the sum:** The realization of $X_t$ is the sum of these jump sizes, $x_t = \\sum_{i=1}^{n} y_i$. If $n=0$, this sum is empty and $x_t=0$ as per the convention.\n\nThis simulation procedure correctly generates a sample from the distribution of $X_t$.\n\n### 2. Derivation of Distributional Properties\nLet $\\mu_Y = \\mathbb{E}[Y_1]$ and $\\sigma_Y^2 = \\mathrm{Var}(Y_1)$ be the mean and variance of the jump size distribution, respectively. We are given that the second moment, $\\mathbb{E}[Y_1^2] = \\sigma_Y^2 + \\mu_Y^2$, is finite. For the Poisson process counter $N_t$, we know that $\\mathbb{E}[N_t] = \\mathrm{Var}(N_t) = \\lambda t$.\n\n#### Expectation $\\mathbb{E}[X_t]$\nWe use the Law of Total Expectation (also known as the tower property): $\\mathbb{E}[Z] = \\mathbb{E}[\\mathbb{E}[Z|W]]$. Let $Z=X_t$ and $W=N_t$.\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}\\left[\\mathbb{E}\\left[X_t | N_t\\right]\\right]\n$$\nFirst, we compute the inner expectation, conditional on $N_t = n$:\n$$\n\\mathbb{E}[X_t | N_t = n] = \\mathbb{E}\\left[ \\sum_{i=1}^{n} Y_i \\right] = \\sum_{i=1}^{n} \\mathbb{E}[Y_i] = n \\mathbb{E}[Y_1] = n \\mu_Y\n$$\nThis holds for any specific value $n$. Thus, as a random variable, the conditional expectation is $\\mathbb{E}[X_t | N_t] = N_t \\mu_Y$.\nNow, we take the outer expectation:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[N_t \\mu_Y] = \\mu_Y \\mathbb{E}[N_t] = \\mu_Y (\\lambda t)\n$$\nSo, the expectation is $\\mathbb{E}[X_t] = \\lambda t \\mu_Y$.\n\n#### Variance $\\mathrm{Var}(X_t)$\nWe use the Law of Total Variance: $\\mathrm{Var}(Z) = \\mathbb{E}[\\mathrm{Var}(Z|W)] + \\mathrm{Var}(\\mathbb{E}[Z|W])$.\n$$\n\\mathrm{Var}(X_t) = \\mathbb{E}[\\mathrm{Var}(X_t | N_t)] + \\mathrm{Var}(\\mathbb{E}[X_t | N_t])\n$$\nWe compute each term separately.\n1.  **First term: $\\mathbb{E}[\\mathrm{Var}(X_t | N_t)]$**\n    The conditional variance, given $N_t=n$, is:\n    $$\n    \\mathrm{Var}(X_t | N_t = n) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} Y_i\\right) = \\sum_{i=1}^{n} \\mathrm{Var}(Y_i) = n \\mathrm{Var}(Y_1) = n \\sigma_Y^2\n    $$\n    This uses the independence of the $Y_i$. Thus, $\\mathrm{Var}(X_t | N_t) = N_t \\sigma_Y^2$. Taking the expectation:\n    $$\n    \\mathbb{E}[\\mathrm{Var}(X_t | N_t)] = \\mathbb{E}[N_t \\sigma_Y^2] = \\sigma_Y^2 \\mathbb{E}[N_t] = \\sigma_Y^2 (\\lambda t)\n    $$\n2.  **Second term: $\\mathrm{Var}(\\mathbb{E}[X_t | N_t])$**\n    From the expectation calculation, we know $\\mathbb{E}[X_t | N_t] = N_t \\mu_Y$. We compute its variance:\n    $$\n    \\mathrm{Var}(\\mathbb{E}[X_t | N_t]) = \\mathrm{Var}(N_t \\mu_Y) = \\mu_Y^2 \\mathrm{Var}(N_t) = \\mu_Y^2 (\\lambda t)\n    $$\nCombining the two terms:\n$$\n\\mathrm{Var}(X_t) = \\lambda t \\sigma_Y^2 + \\lambda t \\mu_Y^2 = \\lambda t (\\sigma_Y^2 + \\mu_Y^2)\n$$\nSince $\\mathbb{E}[Y_1^2] = \\mathrm{Var}(Y_1) + (\\mathbb{E}[Y_1])^2 = \\sigma_Y^2 + \\mu_Y^2$, the variance can be expressed as:\n$$\n\\mathrm{Var}(X_t) = \\lambda t \\mathbb{E}[Y_1^2]\n$$\n\n#### Characteristic Function $\\varphi_{X_t}(u)$\nLet $\\varphi_Y(u) = \\mathbb{E}[e^{iuY_1}]$ be the characteristic function (CF) of the jump size distribution. We again use the Law of Total Expectation.\n$$\n\\varphi_{X_t}(u) = \\mathbb{E}[e^{iuX_t}] = \\mathbb{E}\\left[\\mathbb{E}[e^{iuX_t} | N_t]\\right]\n$$\nThe inner conditional expectation, given $N_t=n$:\n$$\n\\mathbb{E}[e^{iuX_t} | N_t=n] = \\mathbb{E}\\left[e^{iu\\sum_{j=1}^{n} Y_j}\\right] = \\mathbb{E}\\left[\\prod_{j=1}^{n} e^{iuY_j}\\right]\n$$\nDue to the independence of the $Y_j$, this becomes:\n$$\n\\prod_{j=1}^{n} \\mathbb{E}[e^{iuY_j}] = \\prod_{j=1}^{n} \\varphi_Y(u) = (\\varphi_Y(u))^n\n$$\nThus, as a random variable, $\\mathbb{E}[e^{iuX_t} | N_t] = (\\varphi_Y(u))^{N_t}$. Now we take the outer expectation:\n$$\n\\varphi_{X_t}(u) = \\mathbb{E}\\left[(\\varphi_Y(u))^{N_t}\\right]\n$$\nThis expression is the probability-generating function (PGF) of $N_t$, $G_{N_t}(z) = \\mathbb{E}[z^{N_t}]$, evaluated at $z = \\varphi_Y(u)$. The PGF for a Poisson random variable $K \\sim \\mathrm{Poisson}(\\Lambda)$ is $G_K(z) = e^{\\Lambda(z-1)}$. For $N_t \\sim \\mathrm{Poisson}(\\lambda t)$, we have $\\Lambda = \\lambda t$. Substituting $z = \\varphi_Y(u)$, we obtain the characteristic function of the compound Poisson process:\n$$\n\\varphi_{X_t}(u) = \\exp\\left(\\lambda t \\left(\\varphi_Y(u) - 1\\right)\\right)\n$$\nThis is the Lévy–Khintchine formula for a compound Poisson process.\n\n### 3. Properties for Specific Test Cases\nWe now state the required properties for the jump distributions specified in the problem.\n\n1.  **Normal Distribution:** $Y \\sim N(\\mu, \\sigma^2)$\n    -   Mean: $\\mu_Y = \\mu$\n    -   Second Moment: $\\mathbb{E}[Y^2] = \\sigma^2 + \\mu^2$\n    -   Characteristic Function: $\\varphi_Y(u) = \\exp(i u \\mu - \\frac{1}{2}\\sigma^2 u^2)$\n\n2.  **Exponential Distribution:** $Y \\sim \\mathrm{Exponential}(\\beta)$ (with PDF $f(y) = \\beta e^{-\\beta y}$ for $y \\ge 0$)\n    -   Mean: $\\mu_Y = 1/\\beta$\n    -   Variance: $\\sigma_Y^2 = 1/\\beta^2$\n    -   Second Moment: $\\mathbb{E}[Y^2] = \\sigma_Y^2 + \\mu_Y^2 = 2/\\beta^2$\n    -   Characteristic Function: $\\varphi_Y(u) = \\mathbb{E}[e^{iuY}] = \\int_0^\\infty e^{iuy}\\beta e^{-\\beta y} dy = \\beta/(\\beta - iu) = 1/(1 - iu/\\beta)$\n\n3.  **Symmetric Bernoulli Distribution:** $P(Y=1) = P(Y=-1) = 1/2$\n    -   Mean: $\\mu_Y = (1)(1/2) + (-1)(1/2) = 0$\n    -   Second Moment: $\\mathbb{E}[Y^2] = (1)^2(1/2) + (-1)^2(1/2) = 1$\n    -   Characteristic Function: $\\varphi_Y(u) = e^{iu(1)}(1/2) + e^{iu(-1)}(1/2) = \\frac{e^{iu}+e^{-iu}}{2} = \\cos(u)$\n\nFor cases where $t=0$ or $\\lambda=0$, the product $\\lambda t = 0$. In these scenarios, $N_t$ is deterministically $0$, which implies $X_t=0$. The theoretical formulas correctly degenerate:\n- $\\mathbb{E}[X_t] = 0 \\cdot \\mu_Y = 0$\n- $\\mathrm{Var}(X_t) = 0 \\cdot \\mathbb{E}[Y_1^2] = 0$\n- $\\varphi_{X_t}(u) = \\exp(0 \\cdot (\\varphi_Y(u)-1)) = e^0 = 1$. The CF of a constant $0$ is indeed $\\mathbb{E}[e^{iu \\cdot 0}] = 1$.\n\nThe implementation will now follow from these derived formulas.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used but is an allowed library.\n\ndef solve():\n    \"\"\"\n    Simulates a compound Poisson process and calculates its theoretical properties\n    for a given set of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"t\": 5.0, \"lam\": 1.2, \"u\": 1.0, \"seed\": 42,\n            \"dist\": \"normal\", \"params\": {\"mu\": 0.5, \"sigma\": 0.3}\n        },\n        {\n            \"t\": 5.0, \"lam\": 0.0, \"u\": 1.0, \"seed\": 123,\n            \"dist\": \"normal\", \"params\": {\"mu\": 0.5, \"sigma\": 0.3}\n        },\n        {\n            \"t\": 0.0, \"lam\": 3.0, \"u\": 0.5, \"seed\": 7,\n            \"dist\": \"exponential\", \"params\": {\"beta\": 2.0}\n        },\n        {\n            \"t\": 1.0, \"lam\": 5.0, \"u\": 2.0, \"seed\": 99,\n            \"dist\": \"bernoulli\", \"params\": {}\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        t, lam, u, seed = case[\"t\"], case[\"lam\"], case[\"u\"], case[\"seed\"]\n        dist, params = case[\"dist\"], case[\"params\"]\n\n        rng = np.random.default_rng(seed)\n\n        # Theoretical properties of the jump distribution Y\n        mu_y, e_y2, phi_y = 0.0, 0.0, 0.0 + 0.0j\n\n        if dist == \"normal\":\n            mu, sigma = params[\"mu\"], params[\"sigma\"]\n            mu_y = mu\n            e_y2 = sigma**2 + mu**2\n            phi_y = np.exp(1j * u * mu - 0.5 * sigma**2 * u**2)\n        elif dist == \"exponential\":\n            beta = params[\"beta\"]\n            mu_y = 1.0 / beta\n            e_y2 = 2.0 / beta**2\n            phi_y = 1.0 / (1.0 - 1j * u / beta)\n        elif dist == \"bernoulli\":\n            mu_y = 0.0\n            e_y2 = 1.0\n            phi_y = np.cos(u)\n\n        # 1. Simulation\n        # Sample N_t from Poisson(lambda*t)\n        lambda_t = lam * t\n        n_t = rng.poisson(lambda_t)\n\n        # Sample N_t jumps and compute X_t\n        x_t = 0.0\n        if n_t > 0:\n            if dist == \"normal\":\n                jumps = rng.normal(params[\"mu\"], params[\"sigma\"], size=n_t)\n            elif dist == \"exponential\":\n                jumps = rng.exponential(scale=1.0/params[\"beta\"], size=n_t)\n            elif dist == \"bernoulli\":\n                jumps = rng.choice([-1, 1], size=n_t)\n            x_t = np.sum(jumps)\n        # If n_t is 0, x_t remains 0.0, which is correct.\n\n        # 2. Theoretical Calculations\n        # Expectation E[X_t] = lambda*t*E[Y]\n        e_xt = lambda_t * mu_y\n\n        # Variance Var(X_t) = lambda*t*E[Y^2]\n        var_xt = lambda_t * e_y2\n\n        # Characteristic function phi_Xt(u) = exp(lambda*t*(phi_Y(u) - 1))\n        phi_xt = np.exp(lambda_t * (phi_y - 1))\n\n        # Store results for this case\n        all_results.append([\n            int(n_t),\n            x_t,\n            e_xt,\n            var_xt,\n            phi_xt.real,\n            phi_xt.imag\n        ])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the required format.\n    print(all_results)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3063719"}]}