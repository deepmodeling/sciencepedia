{"hands_on_practices": [{"introduction": "The principle of linearity of expectation is a remarkably powerful tool that allows us to compute the expected value of a sum of random variables by simply summing their individual expected values, even if they are dependent. This first exercise provides a classic and elegant application of this principle. By defining simple indicator random variables for each edge in a graph, we can tackle what seems like a complex global counting problem with straightforward local calculations [@problem_id:1369264].", "problem": "Consider a model for a simple, fully connected computer network consisting of $n$ nodes, where $n \\geq 2$. A complete graph $K_n$ is used to represent this network, where the vertices are the nodes and the edges represent direct communication links between every pair of nodes. Each node in the network is independently assigned a state, either \"active\" or \"idle\", with an equal probability of $\\frac{1}{2}$ for each state. An edge in this network is defined as \"monochromatic\" if the two nodes it connects are in the same state (i.e., both are active or both are idle).\n\nDetermine a closed-form expression in terms of $n$ for the expected number of monochromatic edges in the network.", "solution": "Let $K_{n}$ be the complete graph on $n$ vertices. The total number of edges is $\\binom{n}{2}$. Assign to each vertex independently one of two states, \"active\" or \"idle\", each with probability $\\frac{1}{2}$.\n\nDefine for each edge $e$ the indicator random variable $I_{e}$ where $I_{e}=1$ if $e$ is monochromatic (its endpoints share the same state) and $I_{e}=0$ otherwise. The total number of monochromatic edges is\n$$\nX=\\sum_{e} I_{e},\n$$\nwhere the sum is over all $\\binom{n}{2}$ edges.\n\nFor any fixed edge $e=\\{u,v\\}$, the probability that it is monochromatic is\n$$\n\\mathbb{P}(I_{e}=1)=\\mathbb{P}(\\text{$u$ and $v$ both active})+\\mathbb{P}(\\text{$u$ and $v$ both idle})=\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)=\\frac{1}{2}.\n$$\n\nUsing linearity of expectation,\n$$\n\\mathbb{E}[X]=\\sum_{e} \\mathbb{E}[I_{e}]=\\sum_{e} \\mathbb{P}(I_{e}=1)=\\binom{n}{2}\\cdot \\frac{1}{2}.\n$$\nSince $\\binom{n}{2}=\\frac{n(n-1)}{2}$, we obtain\n$$\n\\mathbb{E}[X]=\\frac{n(n-1)}{2}\\cdot \\frac{1}{2}=\\frac{n(n-1)}{4}.\n$$", "answer": "$$\\boxed{\\frac{n(n-1)}{4}}$$", "id": "1369264"}, {"introduction": "Building on the indicator variable method, this practice explores a common scenario in testing and data analysis: determining the number of unique outcomes in a series of random trials. The key to solving this problem lies in shifting our perspective from calculating the probability that an event occurs to calculating the probability that it *never* occurs over multiple independent trials. This exercise [@problem_id:1369273] sharpens your skills in applying linearity of expectation to problems involving probabilistic complements.", "problem": "A software engineer is developing a new Pseudo-Random Number Generator (PRNG). The PRNG is designed to output an integer chosen uniformly at random from the set $\\{1, 2, \\dots, k\\}$, where $k$ is a positive integer. To perform a basic validation test, the engineer calls the generator $n$ times and records the sequence of outputs.\n\nAssuming that each call to the generator is an independent event, determine the expected number of distinct integers that will be observed in this sequence of $n$ outputs. Express your answer as a function of $n$ and $k$.", "solution": "Let $X$ denote the number of distinct integers observed in $n$ independent calls to the PRNG that outputs each of the $k$ values in $\\{1,2,\\dots,k\\}$ uniformly at random.\n\nFor each $j \\in \\{1,2,\\dots,k\\}$, define the indicator variable $I_{j}$ by\n$$\nI_{j} =\n\\begin{cases}\n1, & \\text{if the value } j \\text{ appears at least once in the } n \\text{ outputs},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThen the total number of distinct values observed is\n$$\nX = \\sum_{j=1}^{k} I_{j}.\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[X] = \\sum_{j=1}^{k} \\mathbb{E}[I_{j}] = \\sum_{j=1}^{k} \\mathbb{P}(I_{j}=1).\n$$\nFor a fixed $j$, the event $I_{j}=1$ means that $j$ appears at least once. The probability that a single call does not produce $j$ is $1-\\frac{1}{k}$. Since calls are independent, the probability that $j$ does not appear in any of the $n$ calls is $\\left(1-\\frac{1}{k}\\right)^{n}$. Therefore,\n$$\n\\mathbb{P}(I_{j}=1) = 1 - \\left(1-\\frac{1}{k}\\right)^{n}.\n$$\nThis value is the same for each $j$, so\n$$\n\\mathbb{E}[X] = k \\left(1 - \\left(1-\\frac{1}{k}\\right)^{n}\\right).\n$$", "answer": "$$\\boxed{k\\left(1-\\left(1-\\frac{1}{k}\\right)^{n}\\right)}$$", "id": "1369273"}, {"introduction": "We now transition from expected value to variance, a measure of how spread out a random variable's values are from its average. This problem tackles the variance of the famous \"coupon collector's problem,\" a scenario with wide applications from software testing to biology. The solution requires a more advanced modeling technique: decomposing the total waiting time into a sum of independent geometric random variables, each representing the time to acquire the *next* new item. This practice [@problem_id:1369276] is a fantastic introduction to analyzing the variability and predictability of a random process.", "problem": "A software company is deploying a new machine learning algorithm. The quality assurance team has identified $n$ critical and distinct edge cases that must be validated by an automated testing system before deployment. In each test run, the system simulates a scenario that triggers exactly one of these $n$ edge cases. The selection of the edge case for any given run is independent and uniformly random from the set of all $n$ possibilities.\n\nLet $T$ be the random variable representing the total number of test runs required until each of the $n$ distinct edge cases has been triggered at least once.\n\nFind a closed-form analytic expression for the variance of $T$, denoted as $\\text{Var}(T)$, in terms of $n$. Your expression may be written using summation notation.", "solution": "Let $T$ be the total number of test runs required to observe all $n$ distinct edge cases. We can decompose this total time into a sum of random variables representing the waiting times for each new edge case.\n\nLet $T_i$ be the number of additional test runs required to find the $i$-th new edge case, given that $i-1$ distinct edge cases have already been found. The total number of runs is then the sum of these waiting times:\n$$T = T_1 + T_2 + \\dots + T_n$$\nThe first run always yields a new edge case, so $T_1 = 1$, which is a deterministic constant.\n\nNow, let's determine the distribution of $T_i$ for $i > 1$. Suppose we have already observed $i-1$ distinct edge cases. There are $n - (i-1) = n-i+1$ new, unobserved edge cases. Since each of the $n$ edge cases is equally likely to be triggered in any given run, the probability of observing a new edge case (a \"success\") in the next run is:\n$$p_i = \\frac{n - (i-1)}{n} = \\frac{n-i+1}{n}$$\nThe random variable $T_i$ represents the number of trials needed to get the first success in a sequence of independent Bernoulli trials with success probability $p_i$. This means $T_i$ follows a geometric distribution, $T_i \\sim \\text{Geometric}(p_i)$.\n\nThe process of finding the next new edge case is independent of the history of finding the previous ones. Therefore, the random variables $T_1, T_2, \\dots, T_n$ are mutually independent. The variance of a sum of independent random variables is the sum of their variances:\n$$\\text{Var}(T) = \\text{Var}\\left(\\sum_{i=1}^{n} T_i\\right) = \\sum_{i=1}^{n} \\text{Var}(T_i)$$\nThe variance of a random variable $X \\sim \\text{Geometric}(p)$ is given by $\\text{Var}(X) = \\frac{1-p}{p^2}$. We apply this formula to each $T_i$ with its corresponding success probability $p_i$:\n$$\\text{Var}(T_i) = \\frac{1-p_i}{p_i^2} = \\frac{1 - \\frac{n-i+1}{n}}{\\left(\\frac{n-i+1}{n}\\right)^2}$$\nSimplifying the numerator:\n$$1 - \\frac{n-i+1}{n} = \\frac{n - (n-i+1)}{n} = \\frac{i-1}{n}$$\nSubstituting this back into the variance expression:\n$$\\text{Var}(T_i) = \\frac{\\frac{i-1}{n}}{\\frac{(n-i+1)^2}{n^2}} = \\frac{i-1}{n} \\cdot \\frac{n^2}{(n-i+1)^2} = \\frac{n(i-1)}{(n-i+1)^2}$$\nNote that for $i=1$, $\\text{Var}(T_1) = \\frac{n(1-1)}{(n-1+1)^2} = 0$, which is correct since $T_1$ is a constant.\n\nNow we can find the total variance by summing $\\text{Var}(T_i)$ from $i=1$ to $n$:\n$$\\text{Var}(T) = \\sum_{i=1}^{n} \\text{Var}(T_i) = \\sum_{i=1}^{n} \\frac{n(i-1)}{(n-i+1)^2}$$\nTo simplify this summation, we perform a change of index. Let $k = n-i+1$.\n- When $i=1$, $k = n-1+1=n$.\n- When $i=n$, $k = n-n+1=1$.\n- The term $i-1$ can be expressed in terms of $k$: $i = n-k+1 \\Rightarrow i-1 = n-k$.\n\nSubstituting these into the summation, and noting that summing from $k=n$ down to $1$ is the same as summing from $k=1$ up to $n$:\n$$\\text{Var}(T) = \\sum_{k=1}^{n} \\frac{n(n-k)}{k^2}$$\nWe can split this summation into two parts:\n$$\\text{Var}(T) = \\sum_{k=1}^{n} \\left(\\frac{n^2}{k^2} - \\frac{nk}{k^2}\\right) = \\sum_{k=1}^{n} \\frac{n^2}{k^2} - \\sum_{k=1}^{n} \\frac{n}{k}$$\nFactoring out the terms that do not depend on the summation index $k$:\n$$\\text{Var}(T) = n^2 \\sum_{k=1}^{n} \\frac{1}{k^2} - n \\sum_{k=1}^{n} \\frac{1}{k}$$\nThis is the final closed-form expression for the variance of $T$.", "answer": "$$\\boxed{n^{2} \\sum_{k=1}^{n} \\frac{1}{k^{2}} - n \\sum_{k=1}^{n} \\frac{1}{k}}$$", "id": "1369276"}]}