{"hands_on_practices": [{"introduction": "Understanding a probability distribution involves not only calculating its properties from a given parameter but also working backward from observed data. This first practice challenges you to deduce the fundamental probability of success, $p$, given only the variance of a Bernoulli trial [@problem_id:1392758]. This exercise reinforces the crucial formula for variance, $\\text{Var}(X) = p(1-p)$, and reveals an interesting symmetry inherent in the process.", "problem": "A data analytics firm is studying consumer behavior for a new online subscription service. The action of a single, randomly selected consumer either purchasing the subscription or not is modeled as a discrete random event. A random variable $X$ is defined to represent this outcome: $X=1$ if the consumer makes a purchase, and $X=0$ if they do not. After analyzing a large sample of consumers, the firm determines that the variance of this random variable, $\\text{Var}(X)$, is $0.21$.\n\nLet $p$ represent the probability that a consumer makes a purchase. Based on the given variance, determine the two possible values for $p$. Present your two answers as decimal numbers in ascending order.", "solution": "Let $X$ be a Bernoulli random variable with success probability $p$. The variance of a Bernoulli random variable is given by\n$$\n\\mathrm{Var}(X)=p(1-p).\n$$\nWe are given $\\mathrm{Var}(X)=0.21$, so\n$$\np(1-p)=0.21.\n$$\nRewriting,\n$$\np-p^{2}=0.21 \\;\\;\\Longrightarrow\\;\\; p^{2}-p+0.21=0.\n$$\nSolving the quadratic equation using the quadratic formula,\n$$\np=\\frac{1\\pm \\sqrt{1-4\\cdot 0.21}}{2}=\\frac{1\\pm \\sqrt{0.16}}{2}=\\frac{1\\pm 0.4}{2}.\n$$\nThus the two solutions are\n$$\np=\\frac{1-0.4}{2}=0.3 \\quad \\text{and} \\quad p=\\frac{1+0.4}{2}=0.7,\n$$\nboth of which lie in the interval $[0,1]$. In ascending order, these are $0.3$ and $0.7$.", "answer": "$$\\boxed{\\begin{pmatrix}0.3  0.7\\end{pmatrix}}$$", "id": "1392758"}, {"introduction": "Real-world phenomena rarely involve just a single event; more often, we are interested in the collective outcome of many. Building on the properties of a single trial, we now explore what happens when we combine two independent Bernoulli events [@problem_id:1392801]. This practice serves as a vital stepping stone to understanding sums of random variables and lays the conceptual groundwork for the Binomial distribution, which models the number of successes in a fixed number of trials.", "problem": "A factory manufactures a specific type of microchip. Due to minor variations in the fabrication process, each microchip has a probability $p$ of being defective, independently of all other microchips. A quality control procedure involves randomly selecting two microchips from a large batch for testing.\n\nLet $X_1$ and $X_2$ be two independent and identically distributed (i.i.d.) Bernoulli random variables that model the state of the two selected microchips. For each microchip $i$ (where $i=1, 2$), let $X_i = 1$ if the microchip is defective and $X_i = 0$ if it is not.\n\nDetermine the probability that exactly one of the two selected microchips is defective. Express your answer as a function of $p$.", "solution": "Let $X_{1}$ and $X_{2}$ be i.i.d. Bernoulli random variables with parameter $p$, so that for each $i \\in \\{1,2\\}$, $\\Pr(X_{i}=1)=p$ and $\\Pr(X_{i}=0)=1-p$. The event that exactly one microchip is defective is the event $\\{X_{1}+X_{2}=1\\}$, which is the disjoint union of the two cases:\n$$\n\\{X_{1}=1, X_{2}=0\\} \\quad \\text{and} \\quad \\{X_{1}=0, X_{2}=1\\}.\n$$\nBy independence, $\\Pr(X_{1}=1, X_{2}=0)=\\Pr(X_{1}=1)\\Pr(X_{2}=0)=p(1-p)$ and $\\Pr(X_{1}=0, X_{2}=1)=\\Pr(X_{1}=0)\\Pr(X_{2}=1)=(1-p)p$. Summing these disjoint probabilities gives\n$$\n\\Pr(X_{1}+X_{2}=1)=p(1-p)+(1-p)p=2p(1-p).\n$$\nEquivalently, using the binomial distribution for $X_{1}+X_{2} \\sim \\text{Binomial}(2,p)$, the probability of exactly one success is\n$$\n\\binom{2}{1}p^{1}(1-p)^{1}=2p(1-p).\n$$", "answer": "$$\\boxed{2p(1-p)}$$", "id": "1392801"}, {"introduction": "The principles of probability can lead to surprisingly elegant solutions for practical engineering challenges. This final practice introduces the classic von Neumann randomness extractor, a clever algorithm for producing a perfectly fair outcome from a potentially biased source [@problem_id:1392786]. By working through this problem, you will see how to creatively sequence Bernoulli trials to achieve a desired result and analyze the efficiency of such a process using concepts like expected value.", "problem": "An engineer is designing a subsystem for a deep-space probe that relies on a fundamentally noisy physical process to generate random bits. The raw output is a sequence of bits, $X_1, X_2, X_3, \\dots$, which can be modeled as a sequence of independent and identically distributed Bernoulli random variables. For each bit $X_i$, the probability of it being a '1' is an unknown but constant value $p$, and the probability of it being a '0' is $1-p$. The system operates under the constraint that $p$ is strictly between 0 and 1, i.e., $p \\in (0,1)$.\n\nTo produce a \"fair\" bit (i.e., a bit with a probability of 0.5 of being '1'), the engineer implements the following algorithm, often attributed to John von Neumann:\n1.  The raw bits are consumed in non-overlapping consecutive pairs: $(X_1, X_2), (X_3, X_4), (X_5, X_6), \\dots$.\n2.  Each pair is processed according to the following rules:\n    *   If the pair is $(0,1)$, the algorithm outputs a single bit '0' and halts.\n    *   If the pair is $(1,0)$, the algorithm outputs a single bit '1' and halts.\n    *   If the pair is $(0,0)$ or $(1,1)$, the pair is discarded, and the algorithm proceeds to the next pair to repeat the process.\n\nThis procedure is guaranteed to eventually produce one output bit. Let's call this first generated output bit $Y$.\n\n(a) Calculate the probability that the output bit is a '1', i.e., find $P(Y=1)$.\n(b) Calculate the expected number of raw bits from the source sequence ($X_i$) that must be consumed to produce the single output bit $Y$.\n\nProvide your answer as a row matrix containing the symbolic expressions for Part (a) and Part (b), in that order. Express your answer for Part (b) in terms of $p$.", "solution": "Let the raw bits be independent and identically distributed with $P(X_{i}=1)=p$ and $P(X_{i}=0)=1-p$, with $p\\in(0,1)$. Consider disjoint consecutive pairs $(X_{1},X_{2}), (X_{3},X_{4}), \\dots$. By independence and identical distribution of the $X_{i}$, these pairs are independent and identically distributed.\n\nFor a single pair, the probabilities of pair outcomes are:\n$$\nP((1,0))=p(1-p),\\quad P((0,1))=(1-p)p,\\quad P(\\text{equal})=P((0,0))+P((1,1))=p^{2}+(1-p)^{2}.\n$$\nDefine $q=P(\\text{unequal})=P((1,0))+P((0,1))=2p(1-p)$.\n\nThe algorithm halts at the first pair that is unequal. Let $K$ be the index of this first unequal pair. Then $K$ is a geometric random variable on $\\{1,2,\\dots\\}$ with success probability $q=2p(1-p)$:\n$$\nP(K=k)=\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot 2p(1-p).\n$$\n\n(a) The algorithm outputs $Y=1$ exactly when the first unequal pair is $(1,0)$. Thus\n$$\nP(Y=1)=\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}\\cdot p(1-p)\n= p(1-p)\\sum_{k=1}^{\\infty}\\left(p^{2}+(1-p)^{2}\\right)^{k-1}.\n$$\nThis is a geometric series with ratio $r=p^{2}+(1-p)^{2}$, so\n$$\n\\sum_{k=1}^{\\infty}r^{\\,k-1}=\\frac{1}{1-r}=\\frac{1}{1-\\left(p^{2}+(1-p)^{2}\\right)}=\\frac{1}{2p(1-p)}.\n$$\nTherefore,\n$$\nP(Y=1)=p(1-p)\\cdot \\frac{1}{2p(1-p)}=\\frac{1}{2}.\n$$\n\n(b) Each processed pair consumes exactly $2$ raw bits. Since $K$ is geometric with success probability $q=2p(1-p)$, its expectation is $E[K]=\\frac{1}{q}=\\frac{1}{2p(1-p)}$. Hence the expected number of raw bits consumed is\n$$\nE[\\text{bits}]=2E[K]=\\frac{2}{2p(1-p)}=\\frac{1}{p(1-p)}.\n$$\nThus the required expressions are $P(Y=1)=\\frac{1}{2}$ and $E[\\text{bits}]=\\frac{1}{p(1-p)}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  \\frac{1}{p(1-p)}\\end{pmatrix}}$$", "id": "1392786"}]}