{"hands_on_practices": [{"introduction": "The first step in mastering any new tool is to apply it in a straightforward scenario. This exercise provides a classic application of Markov's inequality, allowing you to calculate an upper probability bound using only the average value of a non-negative random variable. This fundamental practice [@problem_id:1316852] builds the core skill of translating a real-world problem into the framework of the inequality.", "problem": "A company's internal server monitoring system tracks the number of critical errors logged per hour. Over a long-term observation period, it has been determined that the system logs an average of 4.8 critical errors per hour. Assume that the number of errors is a non-negative random variable. Without making any other assumptions about the probability distribution of the errors, determine the best possible upper bound for the probability that the server logs 20 or more critical errors in any given hour.\n\nExpress your answer as a decimal.", "solution": "Let $X$ denote the number of critical errors logged in an hour. We are given that $X \\ge 0$ and $\\mathbb{E}[X] = 4.8$.\n\nBy Markov's inequality, for any $a  0$ and any non-negative random variable $X$,\n$$\n\\Pr(X \\ge a) \\le \\frac{\\mathbb{E}[X]}{a}.\n$$\nSetting $a = 20$ yields\n$$\n\\Pr(X \\ge 20) \\le \\frac{\\mathbb{E}[X]}{20} = \\frac{4.8}{20} = 0.24.\n$$\nThis bound is best possible under the given information. To see tightness, consider a distribution supported on $\\{0,20\\}$ with\n$$\n\\Pr(X=20) = \\frac{4.8}{20}, \\quad \\Pr(X=0) = 1 - \\frac{4.8}{20}.\n$$\nThen $\\mathbb{E}[X] = 20 \\cdot \\frac{4.8}{20} = 4.8$ and $\\Pr(X \\ge 20) = \\frac{4.8}{20} = 0.24$, which attains the bound.", "answer": "$$\\boxed{0.24}$$", "id": "1316852"}, {"introduction": "While Markov's inequality provides a universal bound, it's often not a tight one. This thought experiment [@problem_id:1316841] challenges you to explore the specific, and rather strict, conditions under which the inequality becomes an exact equality. By analyzing a random variable that meets this condition, you will gain a deeper intuition for why the bound holds and when it is sharpest.", "problem": "A specialized electronic component's power consumption, denoted by the non-negative random variable $X$, can take one of three discrete values. It can be in an idle state with $0$ Watts, a nominal state with $A$ Watts, or a high-performance state with $B$ Watts. The parameters satisfy $0  \\mu  A  B$, where $\\mu$ is the long-term average power consumption, i.e., $E[X] = \\mu$. A performance analysis of the component reveals that the probability of it consuming power at or above the nominal level, $P(X \\ge A)$, is exactly equal to the theoretical upper bound given by Markov's inequality. Based on this information, determine the probability that the component is in its high-performance state, $P(X = B)$.", "solution": "Let $p = P(X = B)$, $q = P(X = A)$, and $r = P(X = 0) = 1 - p - q$. Since $X$ is non-negative and takes values in $\\{0, A, B\\}$ with $0  \\mu  A  B$, we have:\n$$\nE[X] = \\mu = A q + B p.\n$$\nMarkov's inequality for non-negative $X$ states that for any $t  0$,\n$$\nP(X \\ge t) \\le \\frac{E[X]}{t}.\n$$\nTaking $t = A$ and using the given equality case,\n$$\nP(X \\ge A) = \\frac{\\mu}{A}.\n$$\nBut $P(X \\ge A) = P(X = A) + P(X = B) = q + p$, hence\n$$\nq + p = \\frac{\\mu}{A}.\n$$\nWe now solve the system\n$$\n\\begin{cases}\nq + p = \\frac{\\mu}{A}, \\\\\nA q + B p = \\mu.\n\\end{cases}\n$$\nFrom the first equation, $q = \\frac{\\mu}{A} - p$. Substitute into the second:\n$$\nA\\left(\\frac{\\mu}{A} - p\\right) + B p = \\mu \\;\\;\\Rightarrow\\;\\; \\mu - A p + B p = \\mu \\;\\;\\Rightarrow\\;\\; (B - A) p = 0.\n$$\nSince $B  A$, it follows that $p = 0$. Therefore,\n$$\nP(X = B) = 0.\n$$\nThis is also consistent with the equality condition in Markov's inequality: equality requires $X = A \\mathbf{1}_{\\{X \\ge A\\}}$ almost surely, which precludes any positive probability at values strictly greater than $A$.", "answer": "$$\\boxed{0}$$", "id": "1316841"}, {"introduction": "The power of the principle behind Markov's inequality extends far beyond its basic form. This problem [@problem_id:1933078] guides you through a more advanced application, where we apply the inequality not to the variable $X$ itself, but to a cleverly chosen function of $X$. By optimizing this function, you will derive a much tighter probability bound, known as Cantelli's inequality, demonstrating how incorporating more information like variance leads to stronger results.", "problem": "Let $X$ be a random variable with a well-defined mean $E[X] = \\mu$ and a finite, non-zero variance $\\text{Var}(X) = \\sigma^2$. We want to find a tight upper bound for the tail probability $P(X \\ge a)$, where $a$ is a constant strictly greater than the mean, i.e., $a  \\mu$.\n\nA general method for deriving such bounds involves transforming the random variable. Consider the function $B(b) = \\frac{E[(X-b)^2]}{(a-b)^2}$, where $b$ is a real-valued parameter. This function provides an upper bound for $P(X \\ge a)$ for any choice of $b  a$.\n\nYour task is to find the tightest possible bound that can be obtained from this family of functions. To do this, you must find the minimum value of $B(b)$ by optimizing the choice of the parameter $b$ over its valid domain ($ba$).\n\nExpress this optimal (minimum) value of $B(b)$ as a closed-form analytic expression in terms of $\\mu$, $\\sigma$, and $a$.", "solution": "We are given that for any $ba$,\n$$\nP(X\\ge a)\\le B(b):=\\frac{E[(X-b)^{2}]}{(a-b)^{2}}.\n$$\nUsing $E[(X-b)^{2}]=\\operatorname{Var}(X)+\\left(E[X]-b\\right)^{2}$, we have\n$$\nE[(X-b)^{2}]=\\sigma^{2}+(\\mu-b)^{2},\n$$\nso the bound is\n$$\nB(b)=\\frac{\\sigma^{2}+(\\mu-b)^{2}}{(a-b)^{2}},\\quad ba.\n$$\n\nTo minimize $B(b)$ over $ba$, define\n$$\nf(b)=\\frac{\\sigma^{2}+(b-\\mu)^{2}}{(a-b)^{2}},\n$$\nwith numerator $N(b)=\\sigma^{2}+(b-\\mu)^{2}$ and denominator $D(b)=(a-b)^{2}$. Then\n$$\nf'(b)=\\frac{N'(b)D(b)-N(b)D'(b)}{D(b)^{2}},\n$$\nwith $N'(b)=2(b-\\mu)$ and $D'(b)=-2(a-b)$. Hence\n$$\nf'(b)=\\frac{2(b-\\mu)(a-b)^{2}+2\\left[\\sigma^{2}+(b-\\mu)^{2}\\right](a-b)}{(a-b)^{4}}\n=\\frac{2(a-b)\\left((b-\\mu)(a-b)+\\sigma^{2}+(b-\\mu)^{2}\\right)}{(a-b)^{4}}.\n$$\nFor $ba$, we have $a-b0$, so critical points satisfy\n$$\n(b-\\mu)(a-b)+\\sigma^{2}+(b-\\mu)^{2}=0.\n$$\nLet $t=b-\\mu$ and write $a-b=(a-\\mu)-t$. Then the equation becomes\n$$\nt\\big((a-\\mu)-t\\big)+\\sigma^{2}+t^{2}=t(a-\\mu)-t^{2}+\\sigma^{2}+t^{2}=t(a-\\mu)+\\sigma^{2}=0,\n$$\nso\n$$\nt^{*}=-\\frac{\\sigma^{2}}{a-\\mu},\\qquad b^{*}=\\mu+t^{*}=\\mu-\\frac{\\sigma^{2}}{a-\\mu}.\n$$\nSince $a\\mu$, we have $a-\\mu0$ and thus\n$$\nb^{*}a \\;\\;\\Longleftrightarrow\\;\\; \\mu-\\frac{\\sigma^{2}}{a-\\mu}a \\;\\;\\Longleftrightarrow\\;\\; -\\sigma^{2}(a-\\mu)^{2},\n$$\nwhich is always true. Therefore $b^{*}$ is feasible. Moreover, $f(b)\\to+\\infty$ as $b\\to a^{-}$ and $f(b)\\to 1$ as $b\\to -\\infty$, and there is a unique critical point, so $b^{*}$ yields the global minimum.\n\nEvaluate $B(b)$ at $b=b^{*}$. Set $d=a-\\mu0$. Then\n$$\n\\mu-b^{*}=\\frac{\\sigma^{2}}{d},\\qquad a-b^{*}=d+\\frac{\\sigma^{2}}{d}=\\frac{d^{2}+\\sigma^{2}}{d}.\n$$\nThus\n$$\nB(b^{*})=\\frac{\\sigma^{2}+(\\mu-b^{*})^{2}}{(a-b^{*})^{2}}\n=\\frac{\\sigma^{2}+\\left(\\frac{\\sigma^{2}}{d}\\right)^{2}}{\\left(\\frac{d^{2}+\\sigma^{2}}{d}\\right)^{2}}\n=\\frac{\\sigma^{2}\\left(1+\\frac{\\sigma^{2}}{d^{2}}\\right)}{\\frac{(d^{2}+\\sigma^{2})^{2}}{d^{2}}}\n=\\frac{\\sigma^{2}\\left(\\frac{d^{2}+\\sigma^{2}}{d^{2}}\\right)}{\\frac{(d^{2}+\\sigma^{2})^{2}}{d^{2}}}\n=\\frac{\\sigma^{2}}{d^{2}+\\sigma^{2}}\n=\\frac{\\sigma^{2}}{(a-\\mu)^{2}+\\sigma^{2}}.\n$$\n\nTherefore, the optimal (minimum) value of $B(b)$ over $ba$ is $\\sigma^{2}/\\big((a-\\mu)^{2}+\\sigma^{2}\\big)$.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{(a-\\mu)^{2}+\\sigma^{2}}}$$", "id": "1933078"}]}