{"hands_on_practices": [{"introduction": "The Weak Law of Large Numbers tells us that a sample mean will converge to the true mean, but it doesn't specify how many samples are needed for a given level of precision. This exercise makes the concept tangible by applying it to a system reliability problem [@problem_id:1345681]. Using Chebyshev's inequality, a key tool in proving the WLLN, you will calculate the minimum sample size required to ensure an estimator is within a specified tolerance, connecting abstract theory to practical engineering design.", "problem": "A cloud computing platform uses a load balancer to manage incoming tasks. Each task is assigned an integer priority level, chosen randomly and uniformly from the set $\\{1, 2, \\dots, M\\}$. The priority levels of successive tasks are independent of each other.\n\nTo make resource allocation decisions, the load balancer computes the running average of the priority levels of the last $n$ tasks. The system is designed with the requirement that this running average must be a reliable estimator of the true mean priority level.\n\nThe maximum priority level is $M = 50$. The system's reliability specification demands that the probability of the running average deviating from the true mean priority level by $0.5$ or more must be no greater than $0.05$.\n\nWhat is the minimum number of tasks, $n$, that the load balancer must average to satisfy this stringent requirement? The final answer must be an integer.", "solution": "Let $X_{1}, X_{2}, \\dots, X_{n}$ be independent and identically distributed with the discrete uniform distribution on $\\{1,2,\\dots,M\\}$, where $M=50$. Then the true mean is $\\mu = \\frac{M+1}{2}$ and the variance is $\\operatorname{Var}(X_{1}) = \\frac{M^{2}-1}{12}$.\n\nThe running average is the sample mean $\\overline{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$. Using independence,\n$$\n\\operatorname{Var}(\\overline{X}_{n}) = \\frac{\\operatorname{Var}(X_{1})}{n}.\n$$\nBy Chebyshev's inequality, for any $\\epsilon0$,\n$$\n\\mathbb{P}\\big(|\\overline{X}_{n}-\\mu|\\geq \\epsilon\\big) \\leq \\frac{\\operatorname{Var}(\\overline{X}_{n})}{\\epsilon^{2}} = \\frac{\\operatorname{Var}(X_{1})}{n\\,\\epsilon^{2}}.\n$$\nSet $\\epsilon = 0.5$ and $M=50$. Then\n$$\n\\operatorname{Var}(X_{1}) = \\frac{50^{2}-1}{12} = \\frac{2499}{12},\n$$\nso\n$$\n\\mathbb{P}\\big(|\\overline{X}_{n}-\\mu|\\geq 0.5\\big) \\leq \\frac{\\frac{2499}{12}}{n \\cdot \\left(\\frac{1}{4}\\right)} = \\frac{2499}{12}\\cdot \\frac{4}{n} = \\frac{833}{n}.\n$$\nTo meet the requirement that this probability be no greater than $0.05$, we need\n$$\n\\frac{833}{n} \\leq 0.05 \\quad \\Longleftrightarrow \\quad n \\geq \\frac{833}{0.05} = 833 \\cdot 20 = 16660.\n$$\nTherefore, the minimum integer $n$ that guarantees the specification is $16660$.", "answer": "$$\\boxed{16660}$$", "id": "1345681"}, {"introduction": "A deep understanding of any scientific law requires knowing not only where it applies but also where it breaks down. The WLLN is powerful, but its conclusions rest on critical assumptions. This practice explores a classic scenario where the law fails—the Cauchy distribution—forcing us to confront the importance of the underlying conditions [@problem_id:1345655]. By identifying the specific property that prevents convergence, you will gain a more robust and nuanced understanding of why the WLLN holds true in other cases.", "problem": "Consider a sequence of random variables $X_1, X_2, \\dots, X_n$ that are independent and identically distributed (i.i.d.), drawn from a standard Cauchy distribution. The Probability Density Function (PDF) for a standard Cauchy random variable $X$ is given by\n$$\nf(x) = \\frac{1}{\\pi(1+x^2)} \\quad \\text{for} \\quad x \\in (-\\infty, \\infty)\n$$\nLet $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ be the sample mean of the first $n$ variables. A well-known result in probability, the Weak Law of Large Numbers (WLLN), describes the convergence of the sample mean to a constant for many distributions. However, for a sequence of i.i.d. Cauchy random variables, the sample mean $\\bar{X}_n$ does not converge in probability to a constant value as $n \\to \\infty$.\n\nWhat is the fundamental mathematical property of the Cauchy distribution that causes this failure of the Weak Law of Large Numbers?\n\nA. The random variables $X_i$ are not truly independent.\n\nB. The variance of the Cauchy distribution is infinite.\n\nC. The standard Cauchy distribution is symmetric about zero.\n\nD. The expected value $E[X_i]$ of the Cauchy distribution is undefined.\n\nE. The PDF of the Cauchy distribution is non-zero over an infinite domain.", "solution": "We recall a standard form of the Weak Law of Large Numbers: if $\\{X_{i}\\}_{i=1}^{\\infty}$ are i.i.d. with finite expectation $E[X_{1}]=\\mu$ (equivalently $E[|X_{1}|]\\infty$), then the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ converges in probability to $\\mu$.\n\nTo identify why the WLLN fails for i.i.d. standard Cauchy variables, we examine whether the expectation exists. For a standard Cauchy random variable $X$ with density $f(x)=\\frac{1}{\\pi(1+x^{2})}$, the expectation, if it existed, would be\n$$\nE[X]=\\int_{-\\infty}^{\\infty} x\\,f(x)\\,dx=\\int_{-\\infty}^{\\infty} \\frac{x}{\\pi(1+x^{2})}\\,dx.\n$$\nThis improper integral does not converge in the Lebesgue sense because the integral of the absolute value diverges:\n$$\nE[|X|]=\\int_{-\\infty}^{\\infty} |x|\\,f(x)\\,dx=\\frac{2}{\\pi}\\int_{0}^{\\infty} \\frac{x}{1+x^{2}}\\,dx.\n$$\nEvaluating the antiderivative,\n$$\n\\int \\frac{x}{1+x^{2}}\\,dx=\\frac{1}{2}\\ln\\!\\bigl(1+x^{2}\\bigr),\n$$\nhence\n$$\n\\int_{0}^{R} \\frac{x}{1+x^{2}}\\,dx=\\frac{1}{2}\\ln\\!\\bigl(1+R^{2}\\bigr)\\xrightarrow[R\\to\\infty]{}\\infty,\n$$\nso $E[|X|]=\\infty$. Consequently, $E[X]$ is undefined (the positive and negative parts both diverge), and the condition required by the WLLN fails.\n\nTherefore, the fundamental property causing the failure of the Weak Law of Large Numbers for the sample mean of i.i.d. Cauchy random variables is that the expected value does not exist. While it is also true that the variance is infinite, a finite variance is not necessary for the WLLN in general; what is essential is the existence of a finite mean. Hence the correct choice is that $E[X_{i}]$ is undefined.\n\nFor completeness, this is consistent with the stability property of the Cauchy distribution: if $X_{1},\\dots,X_{n}$ are i.i.d. standard Cauchy, then $S_{n}=\\sum_{i=1}^{n}X_{i}$ is also Cauchy with the same scale up to a linear factor, and $\\bar{X}_{n}=S_{n}/n$ is again standard Cauchy, so it does not converge in probability to any constant.", "answer": "$$\\boxed{D}$$", "id": "1345655"}, {"introduction": "While many introductory examples of the WLLN focus on independent and identically distributed (i.i.d.) random variables, the law's principles can be extended to more complex situations. This problem challenges you to move beyond the i.i.d. assumption and consider a sequence of independent variables whose variances change over time [@problem_id:1407182]. Determining the conditions under which their sample mean still converges to the true mean will deepen your understanding of the core mechanism behind the law of large numbers.", "problem": "In a data acquisition system, a sequence of measurements, represented by independent random variables $X_1, X_2, \\dots, X_k, \\dots$, is recorded. Ideally, each measurement should have a mean of zero, so we have $E[X_k] = 0$ for all $k \\ge 1$. However, the measurement apparatus degrades over time, causing the uncertainty of the measurements to increase. This degradation is modeled by the variance of each measurement, which follows a power law: $Var(X_k) = c k^{\\alpha}$, where $c$ is a known positive constant and $\\alpha$ is a real-valued parameter describing the rate of degradation.\n\nThe system is considered reliable if the sample mean of the measurements, defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{k=1}^{n} X_k$, converges in probability to the true mean of zero. Convergence in probability to zero means that for any arbitrarily small positive tolerance $\\epsilon$, the probability $P(|\\bar{X}_n| \\ge \\epsilon)$ tends to zero as the number of measurements $n$ approaches infinity.\n\nDetermine the supremum of the set of all possible values of the parameter $\\alpha$ for which the system remains reliable.", "solution": "Let $S_{n}=\\sum_{k=1}^{n}X_{k}$ and $\\bar{X}_{n}=S_{n}/n$. Since the $X_{k}$ are independent with $E[X_{k}]=0$ and $\\operatorname{Var}(X_{k})=c k^{\\alpha}$, we have\n$$\nE[\\bar{X}_{n}]=\\frac{1}{n}\\sum_{k=1}^{n}E[X_{k}]=0,\n$$\nand by independence,\n$$\n\\operatorname{Var}(\\bar{X}_{n})=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\right)=\\frac{1}{n^{2}}\\sum_{k=1}^{n}\\operatorname{Var}(X_{k})=\\frac{c}{n^{2}}\\sum_{k=1}^{n}k^{\\alpha}.\n$$\nBy Chebyshev’s inequality, for any $\\epsilon0$,\n$$\n\\mathbb{P}\\!\\left(|\\bar{X}_{n}|\\ge \\epsilon\\right)\\le \\frac{\\operatorname{Var}(\\bar{X}_{n})}{\\epsilon^{2}}=\\frac{c}{\\epsilon^{2}n^{2}}\\sum_{k=1}^{n}k^{\\alpha}.\n$$\nTherefore $\\bar{X}_{n}\\to 0$ in probability whenever\n$$\n\\frac{1}{n^{2}}\\sum_{k=1}^{n}k^{\\alpha}\\to 0.\n$$\nWe now evaluate the asymptotics of the partial sums. Using the integral comparison test:\n- If $\\alpha-1$, then\n$$\n\\sum_{k=1}^{n}k^{\\alpha}\\sim \\frac{n^{\\alpha+1}}{\\alpha+1},\n$$\nso\n$$\n\\operatorname{Var}(\\bar{X}_{n})\\sim \\frac{c}{\\alpha+1}\\,n^{\\alpha-1}\\to 0 \\quad \\text{iff} \\quad \\alpha1.\n$$\n- If $\\alpha=-1$, then $\\sum_{k=1}^{n}k^{-1}\\sim \\ln n$, so\n$$\n\\operatorname{Var}(\\bar{X}_{n})\\sim \\frac{c\\ln n}{n^{2}}\\to 0.\n$$\n- If $\\alpha-1$, then $\\sum_{k=1}^{n}k^{\\alpha}$ converges to a finite limit as $n\\to\\infty$, hence\n$$\n\\operatorname{Var}(\\bar{X}_{n})=\\mathcal{O}\\!\\left(\\frac{1}{n^{2}}\\right)\\to 0.\n$$\nThus for all $\\alpha1$, $\\operatorname{Var}(\\bar{X}_{n})\\to 0$, and Chebyshev’s inequality gives $\\bar{X}_{n}\\to 0$ in probability; the system is reliable for all $\\alpha1$.\n\nTo show sharpness at and beyond $\\alpha\\ge 1$, consider the valid choice $X_{k}\\sim \\mathcal{N}(0, c k^{\\alpha})$ independent. Then $\\bar{X}_{n}$ is Gaussian with mean $0$ and variance\n$$\nv_{n}=\\operatorname{Var}(\\bar{X}_{n})=\\frac{c}{n^{2}}\\sum_{k=1}^{n}k^{\\alpha}.\n$$\nFor $\\alpha=1$, $\\sum_{k=1}^{n}k\\sim \\frac{n^{2}}{2}$, so $v_{n}\\to \\frac{c}{2}$ and hence $\\bar{X}_{n}$ converges in distribution to $\\mathcal{N}\\!\\left(0,\\frac{c}{2}\\right)$. Consequently, for any fixed $\\epsilon0$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}\\!\\left(|\\bar{X}_{n}|\\ge \\epsilon\\right)=2\\int_{\\epsilon}^{\\infty}\\frac{1}{\\sqrt{\\pi c}}\\exp\\!\\left(-\\frac{x^{2}}{c}\\right)\\,dx0,\n$$\nso $\\bar{X}_{n}$ does not converge to $0$ in probability.\n\nFor $\\alpha1$, we have $v_{n}\\sim \\frac{c}{\\alpha+1}n^{\\alpha-1}\\to \\infty$. Then for any fixed $\\epsilon0$,\n$$\n\\mathbb{P}\\!\\left(|\\bar{X}_{n}|\\epsilon\\right)=\\int_{-\\epsilon}^{\\epsilon}\\frac{1}{\\sqrt{2\\pi v_{n}}}\\exp\\!\\left(-\\frac{x^{2}}{2v_{n}}\\right)\\,dx\\le \\frac{2\\epsilon}{\\sqrt{2\\pi v_{n}}}\\to 0,\n$$\nso $\\mathbb{P}\\!\\left(|\\bar{X}_{n}|\\ge \\epsilon\\right)\\to 1$, and $\\bar{X}_{n}$ certainly does not converge to $0$ in probability.\n\nTherefore, the set of $\\alpha$ for which the system is reliable is $(-\\infty,1)$, and its supremum is $1$.", "answer": "$$\\boxed{1}$$", "id": "1407182"}]}