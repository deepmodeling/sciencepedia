{"hands_on_practices": [{"introduction": "The Central Limit Theorem is one of the most remarkable results in probability, stating that the sum of many independent and identically distributed random variables tends toward a normal distribution. This practice challenges you to apply this powerful theorem in a less obvious context [@problem_id:1353072]. By examining a standardized chi-squared variable, you will see firsthand how normality can emerge from the sum of non-normal distributions.", "problem": "Let $Z_1, Z_2, Z_3, \\dots$ be a sequence of independent and identically distributed (i.i.d.) random variables, each following a standard normal distribution, $\\mathcal{N}(0, 1)$. For any positive integer $n$, define a new random variable $X_n$ as the sum of the squares of the first $n$ variables in the sequence:\n$$X_n = \\sum_{i=1}^{n} Z_i^2$$\nNow, consider the standardized random variable $Y_n$ defined as:\n$$Y_n = \\frac{X_n - n}{\\sqrt{2n}}$$\nAs $n$ approaches infinity, the distribution of $Y_n$ converges to a specific well-known probability distribution. This phenomenon is known as convergence in distribution.\n\nWhich of the following distributions is the limiting distribution of $Y_n$ as $n \\to \\infty$?\n\nA. A Chi-squared distribution with 1 degree of freedom, $\\chi_1^2$.\n\nB. A standard Normal distribution, $\\mathcal{N}(0, 1)$.\n\nC. A Normal distribution with mean 0 and variance 2, $\\mathcal{N}(0, 2)$.\n\nD. A Uniform distribution on the interval $[0, 1]$, $U(0, 1)$.\n\nE. The distribution of a constant random variable that is always equal to 0.", "solution": "Let $Z_{1},Z_{2},\\dots$ be i.i.d. with $Z_{i}\\sim \\mathcal{N}(0,1)$. Define $X_{n}=\\sum_{i=1}^{n}Z_{i}^{2}$ and $Y_{n}=\\frac{X_{n}-n}{\\sqrt{2n}}$.\n\nStep 1: Identify the distribution and moments of the summands. For each $i$, $X_{i}^{(1)}:=Z_{i}^{2}\\sim \\chi^{2}_{1}$ because the square of a standard normal is chi-squared with one degree of freedom. For a chi-squared random variable with $k$ degrees of freedom, the mean and variance are\n$$\n\\mathbb{E}[\\chi^{2}_{k}]=k,\\qquad \\operatorname{Var}(\\chi^{2}_{k})=2k.\n$$\nHence, for $X_{i}^{(1)}\\sim \\chi^{2}_{1}$,\n$$\n\\mu:=\\mathbb{E}[X_{i}^{(1)}]=1,\\qquad \\sigma^{2}:=\\operatorname{Var}(X_{i}^{(1)})=2.\n$$\n\nStep 2: Express $Y_{n}$ as a standardized sum. Since $X_{n}=\\sum_{i=1}^{n}X_{i}^{(1)}$, we have\n$$\n\\mathbb{E}[X_{n}]=n\\mu=n,\\qquad \\operatorname{Var}(X_{n})=n\\sigma^{2}=2n.\n$$\nTherefore,\n$$\nY_{n}=\\frac{X_{n}-\\mathbb{E}[X_{n}]}{\\sqrt{\\operatorname{Var}(X_{n})}}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}^{(1)}-\\mu\\right)}{\\sigma\\sqrt{n}}.\n$$\n\nStep 3: Apply the Central Limit Theorem (CLT). The sequence $\\{X_{i}^{(1)}\\}$ is i.i.d. with finite mean $\\mu$ and finite variance $\\sigma^{2}$. By the classical Central Limit Theorem,\n$$\n\\frac{\\sum_{i=1}^{n}\\left(X_{i}^{(1)}-\\mu\\right)}{\\sigma\\sqrt{n}}\\xrightarrow{d}\\mathcal{N}(0,1)\\quad \\text{as }n\\to\\infty.\n$$\nHence,\n$$\nY_{n}\\xrightarrow{d}\\mathcal{N}(0,1)\\quad \\text{as }n\\to\\infty.\n$$\n\nTherefore, the limiting distribution is a standard Normal distribution, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1353072"}, {"introduction": "Once we understand the limiting behavior of a statistic like the sample mean, we often want to know the distribution of a function of that statistic. The Delta method is a fundamental tool for this purpose, using a linear approximation to translate convergence results. This exercise asks you to apply the Delta method to find the asymptotic distribution of the sample odds ratio, a common measure in many scientific fields [@problem_id:1910243].", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a Bernoulli distribution with a parameter $p$ representing the probability of success, where $0  p  1$. The sample proportion of successes is given by $\\hat{p}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n\nIn many fields, such as epidemiology and social sciences, it is common to analyze the \"odds\" of an event occurring. The sample odds ratio is defined as the ratio of the sample proportion of successes to the sample proportion of failures, given by the statistic $O_n = \\frac{\\hat{p}_n}{1-\\hat{p}_n}$.\n\nFor a large sample size $n$, the sampling distribution of $O_n$ can be approximated by a normal distribution. Determine the variance of this asymptotic normal distribution for $O_n$. Express your answer as a formula in terms of $n$ and $p$.", "solution": "The problem asks for the variance of the asymptotic distribution of the sample odds ratio, $O_n = \\frac{\\hat{p}_n}{1-\\hat{p}_n}$. We can find this using the Delta method.\n\nFirst, we establish the limiting distribution of the sample proportion, $\\hat{p}_n$. The random variables $X_i$ are i.i.d. Bernoulli($p$). The mean and variance of each $X_i$ are:\n$E[X_i] = \\mu = p$\n$Var(X_i) = \\sigma^2 = p(1-p)$\n\nThe sample proportion $\\hat{p}_n$ is the sample mean of these variables. According to the Central Limit Theorem (CLT), for a large sample size $n$, the distribution of the sample mean is approximately normal. More formally, the CLT states:\n$$\n\\sqrt{n}(\\hat{p}_n - p) \\xrightarrow{d} N(0, p(1-p))\n$$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution. This tells us that the random variable $\\sqrt{n}(\\hat{p}_n - p)$ converges to a normal distribution with a mean of 0 and a variance of $p(1-p)$.\n\nNext, we apply the Delta method. The Delta method provides a way to find the limiting distribution of a function of a random variable whose limiting distribution is known. Our statistic of interest, $O_n$, is a function of $\\hat{p}_n$. Let's define this function as $g(x) = \\frac{x}{1-x}$. Then, $O_n = g(\\hat{p}_n)$.\n\nThe Delta method states that if $\\sqrt{n}(Y_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)$, then for a differentiable function $g$ such that $g'(\\mu) \\neq 0$:\n$$\n\\sqrt{n}(g(Y_n) - g(\\mu)) \\xrightarrow{d} N(0, [g'(\\mu)]^2 \\sigma^2)\n$$\n\nIn our case, $Y_n = \\hat{p}_n$, $\\mu = p$, and $\\sigma^2 = p(1-p)$. We need to find the derivative of our function $g(x)$. Using the quotient rule:\n$$\ng'(x) = \\frac{d}{dx} \\left( \\frac{x}{1-x} \\right) = \\frac{(1)(1-x) - (x)(-1)}{(1-x)^2} = \\frac{1-x+x}{(1-x)^2} = \\frac{1}{(1-x)^2}\n$$\n\nNow, we evaluate this derivative at the mean $\\mu=p$:\n$$\ng'(p) = \\frac{1}{(1-p)^2}\n$$\n\nWe can now find the variance of the limiting distribution of $\\sqrt{n}(O_n - g(p))$. Let's call this asymptotic variance $\\sigma_{asymp}^2$:\n$$\n\\sigma_{asymp}^2 = [g'(p)]^2 \\times (\\text{asymptotic variance of } \\sqrt{n}(\\hat{p}_n - p))\n$$\n$$\n\\sigma_{asymp}^2 = \\left( \\frac{1}{(1-p)^2} \\right)^2 \\times p(1-p) = \\frac{1}{(1-p)^4} \\times p(1-p) = \\frac{p}{(1-p)^3}\n$$\n\nSo, we have the limiting distribution:\n$$\n\\sqrt{n}(O_n - \\frac{p}{1-p}) \\xrightarrow{d} N\\left(0, \\frac{p}{(1-p)^3}\\right)\n$$\nThis result implies that for large $n$, the random variable $O_n$ itself is approximately normally distributed. We can see this by dividing the term inside the parenthesis by $\\sqrt{n}$:\n$$\nO_n - \\frac{p}{1-p} \\approx N\\left(0, \\frac{1}{n} \\frac{p}{(1-p)^3}\\right)\n$$\nOr, equivalently:\n$$\nO_n \\approx N\\left(\\frac{p}{1-p}, \\frac{p}{n(1-p)^3}\\right)\n$$\nThe question asks for the variance of this asymptotic normal distribution for $O_n$. From the expression above, this variance is $\\frac{p}{n(1-p)^3}$.", "answer": "$$\\boxed{\\frac{p}{n(1-p)^{3}}}$$", "id": "1910243"}, {"introduction": "While the Central Limit Theorem describes the behavior of sums, other stochastic phenomena follow different limiting laws. This practice shifts our focus from the 'center' of a distribution to its 'edge' by investigating the behavior of an extreme valueâ€”the minimum of a sample [@problem_id:1910191]. You will discover that a different, non-normal distribution emerges, highlighting the rich variety of limiting behaviors in probability theory.", "problem": "Let $U_1, U_2, \\ldots, U_n$ be a random sample of size $n$ from the standard Uniform distribution on the interval $(0, 1)$. This means the random variables are independent and identically distributed (i.i.d.). Let $U_{(1)} = \\min(U_1, U_2, \\ldots, U_n)$ be the smallest value in the sample. Consider the new random variable defined by $Y_n = nU_{(1)}$. As the sample size $n$ approaches infinity, the distribution of $Y_n$ converges to a well-known limiting distribution.\n\nWhich of the following correctly identifies this limiting distribution?\n\nA. The standard Uniform distribution on the interval $(0, 1)$.\n\nB. The standard Normal distribution with mean 0 and variance 1.\n\nC. The Exponential distribution with mean 1.\n\nD. The Gamma distribution with shape parameter 2 and rate parameter 1.\n\nE. A degenerate distribution where all probability mass is at 0.", "solution": "Let $U_{1},\\ldots,U_{n}$ be i.i.d. Uniform$(0,1)$ and $U_{(1)}=\\min(U_{1},\\ldots,U_{n})$. Define $Y_{n}=nU_{(1)}$. We derive the distribution of $Y_{n}$.\n\nFor $x\\in(0,1)$, using independence,\n$$\n\\mathbb{P}\\!\\left(U_{(1)}x\\right)=\\mathbb{P}\\!\\left(U_{1}x,\\ldots,U_{n}x\\right)=\\prod_{i=1}^{n}\\mathbb{P}(U_{i}x)=(1-x)^{n}.\n$$\nThus, for $y\\in\\mathbb{R}$,\n$$\n\\mathbb{P}(Y_{n}y)=\\mathbb{P}\\!\\left(nU_{(1)}y\\right)=\\mathbb{P}\\!\\left(U_{(1)}\\frac{y}{n}\\right)=\n\\begin{cases}\n1,  y0,\\\\\n\\left(1-\\frac{y}{n}\\right)^{n},  0\\le y\\le n,\\\\\n0,  yn.\n\\end{cases}\n$$\nTherefore, for fixed $y\\ge 0$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(Y_{n}y)=\\lim_{n\\to\\infty}\\left(1-\\frac{y}{n}\\right)^{n}=\\exp(-y),\n$$\nusing the standard limit $\\lim_{n\\to\\infty}\\left(1-\\frac{a}{n}\\right)^{n}=\\exp(-a)$ for fixed $a$. Consequently, for all $y\\in\\mathbb{R}$,\n$$\n\\lim_{n\\to\\infty}\\mathbb{P}(Y_{n}\\le y)=\n\\begin{cases}\n0,  y0,\\\\\n1-\\exp(-y),  y\\ge 0,\n\\end{cases}\n$$\nwhich is the cumulative distribution function of an Exponential distribution with rate $1$ (mean $1$). Hence $Y_{n}\\xrightarrow{d}\\text{Exp}(1)$, corresponding to option C.", "answer": "$$\\boxed{C}$$", "id": "1910191"}]}