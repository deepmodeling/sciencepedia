{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) is a cornerstone of probability theory, providing a vital link between theoretical expectation and empirical averages from repeated trials. This first exercise grounds the SLLN in a tangible calculation, demonstrating how the long-term average of a sequence of independent and identically distributed random variables converges to the distribution's expected value. By working through this problem [@problem_id:1460774], you will solidify your understanding of this fundamental convergence.", "problem": "A research team is studying the output of a novel signal processor. Each time the processor runs, it generates a normalized value, which can be modeled as a random variable. A sequence of these values, $X_1, X_2, X_3, \\dots$, is recorded. These values are considered to be independent and identically distributed (i.i.d.) random variables. The theoretical model for the probability distribution of any single value $X_i$ is described by the probability density function (PDF):\n$$\nf(x) = \\begin{cases} 3x^2  \\text{for } 0 \\le x \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe long-term average of these measurements after $n$ trials is given by the sample mean $S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. As the number of trials $n$ grows infinitely large, the value of $S_n$ will converge almost surely to a specific constant.\n\nDetermine the value of this constant. Express your answer as a fraction in simplest form.", "solution": "We are given independent and identically distributed random variables $X_{1},X_{2},\\dots$ with common density\n$$\nf(x)=\\begin{cases}\n3x^{2},  0\\le x\\le 1,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nFirst, verify that $f$ is a valid probability density by checking normalization:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1}3x^{2}\\,dx=3\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1.\n$$\nBy the Strong Law of Large Numbers, since the $X_{i}$ are i.i.d. with finite mean $E[|X_{1}|]\\infty$ (which holds because $0\\le X_{1}\\le 1$ almost surely), the sample mean\n$$\nS_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\n$$\nconverges almost surely to $E[X_{1}]$ as $n\\to\\infty$.\n\nCompute the expectation:\n$$\nE[X_{1}]=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=3\\int_{0}^{1} x^{3}\\,dx=3\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty} S_{n}=\\frac{3}{4}\\quad \\text{almost surely}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1460774"}, {"introduction": "While powerful, the Strong Law of Large Numbers is not a universal guarantee; its application hinges on a critical precondition regarding the random variables' distribution. This practice explores this essential requirement by examining a \"heavy-tailed\" Pareto distribution, which is often used to model extreme events in finance and insurance. This problem [@problem_id:1406772] challenges you to determine when the sample mean is guaranteed to converge, highlighting the crucial role of a finite expected value ($E[|X|] \\lt \\infty$) for the law to hold.", "problem": "An insurance company analyzes risk associated with catastrophic events. The financial size of a claim, denoted by a random variable $X$, is modeled using a Pareto distribution, which is suitable for phenomena with heavy tails. A sequence of claims $X_1, X_2, \\dots, X_n$ are assumed to be independent and identically distributed (i.i.d.).\n\nThe probability density function (PDF) for a single claim size $X$ is given by:\n$$f(x) = \\begin{cases} \\frac{\\alpha x_m^{\\alpha}}{x^{\\alpha+1}}  \\text{for } x \\ge x_m \\\\ 0  \\text{for } x  x_m \\end{cases}$$\nHere, $\\alpha  0$ is the shape parameter that determines the heaviness of the tail, and $x_m  0$ is the minimum possible claim size (the scale parameter).\n\nFor the company's long-term financial models to be stable, it is essential that the sample mean of the claim sizes, $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, converges almost surely to a finite, non-zero constant as the number of claims $n$ approaches infinity.\n\nWhich of the following conditions on the shape parameter $\\alpha$ ensures this convergence?\n\nA. $\\alpha > 0$\n\nB. $\\alpha > 1$\n\nC. $\\alpha > 2$\n\nD. $0  \\alpha \\le 1$\n\nE. $1  \\alpha \\le 2$", "solution": "We are given i.i.d. claim sizes $X_{1},X_{2},\\dots,X_{n}$ with common Pareto distribution having density\n$$\nf(x)=\\begin{cases}\n\\dfrac{\\alpha x_{m}^{\\alpha}}{x^{\\alpha+1}}  \\text{for } x\\geq x_{m},\\\\\n0  \\text{for } xx_{m},\n\\end{cases}\n$$\nwhere $\\alpha0$ and $x_{m}0$. The sample mean is $\\bar{X}_{n}=\\dfrac{1}{n}\\sum_{i=1}^{n}X_{i}$. By the strong law of large numbers (SLLN), if the $X_{i}$ are i.i.d. and $\\mathbb{E}[|X|]\\infty$, then\n$$\n\\bar{X}_{n}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X].\n$$\nTherefore, for $\\bar{X}_{n}$ to converge almost surely to a finite, non-zero constant, it is necessary and sufficient that $\\mathbb{E}[X]$ exist, be finite, and be positive.\n\nWe compute $\\mathbb{E}[X]$ for the Pareto distribution:\n$$\n\\mathbb{E}[X]=\\int_{x_{m}}^{\\infty}x\\,f(x)\\,dx=\\int_{x_{m}}^{\\infty}x\\cdot\\frac{\\alpha x_{m}^{\\alpha}}{x^{\\alpha+1}}\\,dx=\\alpha x_{m}^{\\alpha}\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx.\n$$\nFor $\\alpha\\neq 1$, we evaluate the integral\n$$\n\\int x^{-\\alpha}\\,dx=\\frac{x^{1-\\alpha}}{1-\\alpha},\n$$\nso\n$$\n\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx=\\lim_{b\\to\\infty}\\frac{b^{1-\\alpha}-x_{m}^{1-\\alpha}}{1-\\alpha}.\n$$\nThis converges if and only if $\\alpha1$. When $\\alpha1$, $1-\\alpha0$ and $b^{1-\\alpha}\\to 0$, giving\n$$\n\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx=\\frac{0-x_{m}^{1-\\alpha}}{1-\\alpha}=\\frac{x_{m}^{1-\\alpha}}{\\alpha-1}.\n$$\nHence, for $\\alpha1$,\n$$\n\\mathbb{E}[X]=\\alpha x_{m}^{\\alpha}\\cdot\\frac{x_{m}^{1-\\alpha}}{\\alpha-1}=\\frac{\\alpha x_{m}}{\\alpha-1},\n$$\nwhich is finite and strictly positive since $x_{m}0$.\n\nFor $\\alpha=1$,\n$$\n\\mathbb{E}[X]=\\alpha x_{m}^{\\alpha}\\int_{x_{m}}^{\\infty}x^{-1}\\,dx=x_{m}\\int_{x_{m}}^{\\infty}\\frac{dx}{x},\n$$\nwhich diverges to $+\\infty$. For $0\\alpha1$, the integral $\\int_{x_{m}}^{\\infty}x^{-\\alpha}\\,dx$ diverges because $1-\\alpha0$ implies $b^{1-\\alpha}\\to\\infty$. Thus $\\mathbb{E}[X]=\\infty$ for $\\alpha\\leq 1$.\n\nTherefore, the SLLN yields almost sure convergence of $\\bar{X}_{n}$ to the finite, non-zero constant $\\mathbb{E}[X]=\\dfrac{\\alpha x_{m}}{\\alpha-1}$ if and only if $\\alpha1$. Among the options, this corresponds to $\\alpha1$.\n\nOption C, $\\alpha2$, is sufficient but not necessary; the minimal and correct condition is $\\alpha1$, which is option B.", "answer": "$$\\boxed{B}$$", "id": "1406772"}, {"introduction": "The principles of the SLLN extend beyond single processes, enabling us to analyze the long-term behavior of systems with multiple independent components. This exercise [@problem_id:1957051] presents a scenario comparing the lifetimes of two different types of components, each following its own exponential distribution. You will apply the SLLN to each sequence of measurements separately and then combine the results to determine the long-term limit of their ratio, a common task in comparative reliability and performance analysis.", "problem": "A technology company is testing the long-term reliability of two different types of memory chips, Type A and Type B, for use in their new line of mobile devices. The time-to-failure for each chip is modeled as a random variable.\n\nThe lifetime of a Type A chip, measured in thousands of hours, is a random variable that follows an exponential distribution with a rate parameter $\\lambda_1  0$. The lifetime of a Type B chip, also in thousands of hours, follows an exponential distribution with a rate parameter $\\lambda_2  0$. The probability density function for an exponential distribution with rate parameter $\\lambda$ is given by $f(t) = \\lambda \\exp(-\\lambda t)$ for $t \\geq 0$.\n\nLet $X_1, X_2, \\ldots, X_n$ be the observed lifetimes from a sequence of $n$ Type A chips, and let $Y_1, Y_2, \\ldots, Y_n$ be the observed lifetimes from an independent sequence of $n$ Type B chips. Within each type, the lifetimes are independent and identically distributed (i.i.d.).\n\nLet $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ and $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ be the sample mean lifetimes for Type A and Type B chips, respectively. As the number of tested chips $n$ becomes very large, the ratio of the sample mean lifetimes, $\\frac{\\bar{X}_n}{\\bar{Y}_n}$, will stabilize.\n\nDetermine the value to which this ratio converges almost surely as $n \\to \\infty$. Your answer should be an expression in terms of $\\lambda_1$ and $\\lambda_2$.", "solution": "We are given two independent sequences of lifetimes. For Type A, $X_{1}, X_{2}, \\ldots, X_{n}$ are i.i.d. with density $f_{X}(t)=\\lambda_{1}\\exp(-\\lambda_{1} t)$ for $t\\geq 0$. For Type B, $Y_{1}, Y_{2}, \\ldots, Y_{n}$ are i.i.d. with density $f_{Y}(t)=\\lambda_{2}\\exp(-\\lambda_{2} t)$ for $t\\geq 0$. We seek the almost sure limit of the ratio of sample means $\\bar{X}_{n}/\\bar{Y}_{n}$ as $n\\to\\infty$.\n\nFirst, compute the expectations. For a generic exponential random variable $T$ with rate $\\lambda0$,\n$$\n\\mathbb{E}[T]=\\int_{0}^{\\infty} t\\,\\lambda \\exp(-\\lambda t)\\,dt.\n$$\nUsing integration by parts with $u=t$, $dv=\\lambda \\exp(-\\lambda t)\\,dt$, hence $du=dt$ and $v=-\\exp(-\\lambda t)$, we get\n$$\n\\mathbb{E}[T]=\\left[-t\\exp(-\\lambda t)\\right]_{0}^{\\infty}+\\int_{0}^{\\infty}\\exp(-\\lambda t)\\,dt.\n$$\nThe boundary term is zero because $\\lim_{t\\to\\infty}t\\exp(-\\lambda t)=0$ and the value at $t=0$ is zero. The remaining integral evaluates to\n$$\n\\int_{0}^{\\infty}\\exp(-\\lambda t)\\,dt=\\left[-\\frac{1}{\\lambda}\\exp(-\\lambda t)\\right]_{0}^{\\infty}=\\frac{1}{\\lambda}.\n$$\nTherefore, for our variables,\n$$\n\\mathbb{E}[X_{1}]=\\frac{1}{\\lambda_{1}},\\qquad \\mathbb{E}[Y_{1}]=\\frac{1}{\\lambda_{2}}.\n$$\n\nBy the Strong Law of Large Numbers (SLLN), since $X_{i}$ are i.i.d. with finite mean, we have\n$$\n\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[X_{1}]=\\frac{1}{\\lambda_{1}}.\n$$\nSimilarly, for the $Y_{i}$,\n$$\n\\bar{Y}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}\\xrightarrow{\\text{a.s.}}\\mathbb{E}[Y_{1}]=\\frac{1}{\\lambda_{2}}.\n$$\n\nSince $\\frac{1}{\\lambda_{2}}0$, on the event of probability one where both convergences hold, the limit of $\\bar{Y}_{n}$ is strictly positive. Hence $1/\\bar{Y}_{n}\\xrightarrow{\\text{a.s.}}\\lambda_{2}$ by continuity of the reciprocal function at positive arguments. Consequently,\n$$\n\\frac{\\bar{X}_{n}}{\\bar{Y}_{n}}=\\bar{X}_{n}\\cdot\\frac{1}{\\bar{Y}_{n}}\\xrightarrow{\\text{a.s.}}\\frac{1}{\\lambda_{1}}\\cdot \\lambda_{2}=\\frac{\\lambda_{2}}{\\lambda_{1}}.\n$$\n\nTherefore, the ratio of sample means converges almost surely to $\\lambda_{2}/\\lambda_{1}$.", "answer": "$$\\boxed{\\frac{\\lambda_{2}}{\\lambda_{1}}}$$", "id": "1957051"}]}