{"hands_on_practices": [{"introduction": "Mastering mean square convergence begins with a direct application of its definition. This first practice presents a simple model of signal degradation [@problem_id:1318343], where you will calculate the mean squared error and determine if the signal's amplitude converges to zero. This exercise provides a foundational check of your understanding of the core concept.", "problem": "A research team is studying a simple model for signal degradation over a noisy channel. A sequence of random variables, $\\{X_n\\}_{n=1}^{\\infty}$, represents the signal's amplitude at discrete time steps $n=1, 2, 3, \\ldots$. The model proposes that the signal at step $n$ is related to an initial random disturbance $Y$ by the formula $X_n = \\frac{Y}{n}$. The only information known about the initial disturbance $Y$ is that it has a well-defined and finite second moment, that is, $E[Y^2]  \\infty$. The team needs to determine if this signal model predicts that the signal will eventually fade to zero. Specifically, does the sequence $\\{X_n\\}$ converge in mean square to the zero random variable, $X=0$?\n\nSelect the correct statement from the options below.\n\nA. Yes, the sequence converges in mean square to 0.\n\nB. No, the sequence does not converge in mean square to 0 because the limit of the mean square error is a non-zero finite value.\n\nC. No, the sequence does not converge in mean square to 0 because the limit of the mean square error diverges.\n\nD. It is impossible to determine convergence in mean square without more information about the probability distribution of $Y$.", "solution": "Convergence in mean square to a random variable $X$ means that $E\\!\\left[(X_{n}-X)^{2}\\right] \\to 0$ as $n \\to \\infty$. Here $X_{n}=\\frac{Y}{n}$ and $X=0$, so we need to evaluate\n$$\nE\\!\\left[\\left(X_{n}-0\\right)^{2}\\right]=E\\!\\left[\\left(\\frac{Y}{n}\\right)^{2}\\right].\n$$\nUsing the scaling property of expectation for deterministic constants, $E\\!\\left[c^{2}Z^{2}\\right]=c^{2}E\\!\\left[Z^{2}\\right]$, we obtain\n$$\nE\\!\\left[\\left(\\frac{Y}{n}\\right)^{2}\\right]=\\frac{1}{n^{2}}E\\!\\left[Y^{2}\\right].\n$$\nBy assumption, $E\\!\\left[Y^{2}\\right]\\infty$, so it is a finite constant independent of $n$. Therefore,\n$$\n\\lim_{n\\to\\infty}E\\!\\left[\\left(\\frac{Y}{n}\\right)^{2}\\right]=\\lim_{n\\to\\infty}\\frac{E\\!\\left[Y^{2}\\right]}{n^{2}}=0,\n$$\nsince $\\frac{1}{n^{2}}\\to 0$ as $n\\to\\infty$. Hence $X_{n}\\to 0$ in mean square. No additional distributional information about $Y$ is required beyond the finiteness of $E[Y^{2}]$.\n\nTherefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1318343"}, {"introduction": "Mean square convergence is a stricter condition than convergence in probability, a distinction that is crucial in stochastic analysis. This exercise [@problem_id:1318373] explores a classic sequence of random variables that converges in probability but fails to converge in mean square. By analyzing this case, you will gain a deeper appreciation for how the magnitude of rare events can impact convergence.", "problem": "Consider a sequence of discrete random variables $\\{X_n\\}_{n=1}^{\\infty}$. For each integer $n \\geq 1$, the random variable $X_n$ can take one of two values: it is equal to $n$ with a probability of $p_n = \\frac{1}{n^2}$, and it is equal to $0$ with a probability of $1 - p_n$.\n\nWhich of the following statements accurately describes the convergence properties of the sequence $\\{X_n\\}$ as $n \\to \\infty$?\n\nA. The sequence $\\{X_n\\}$ converges to 0 in mean square.\n\nB. The sequence $\\{X_n\\}$ converges to 1 in mean square.\n\nC. The sequence $\\{X_n\\}$ converges to 0 in probability, but does not converge to 0 in mean square.\n\nD. The sequence $\\{X_n\\}$ does not converge to 0 in probability.", "solution": "We analyze convergence in mean square and in probability for the sequence defined by $P(X_{n}=n)=p_{n}=\\frac{1}{n^{2}}$ and $P(X_{n}=0)=1-p_{n}$.\n\nFirst, check mean square convergence to $0$. By definition, convergence in mean square to $0$ requires $\\lim_{n\\to\\infty}\\mathbb{E}[X_{n}^{2}]=0$. Compute\n$$\n\\mathbb{E}[X_{n}^{2}]=n^{2}\\cdot \\frac{1}{n^{2}}+0^{2}\\cdot\\left(1-\\frac{1}{n^{2}}\\right)=1.\n$$\nThus $\\mathbb{E}[X_{n}^{2}]=1$ for all $n$, so it does not tend to $0$. Therefore, $\\{X_{n}\\}$ does not converge to $0$ in mean square, and option A is false.\n\nNext, check mean square convergence to $1$. Convergence in mean square to $1$ requires $\\lim_{n\\to\\infty}\\mathbb{E}[(X_{n}-1)^{2}]=0$. Compute\n$$\n\\mathbb{E}[(X_{n}-1)^{2}]=(n-1)^{2}\\cdot \\frac{1}{n^{2}}+(0-1)^{2}\\cdot\\left(1-\\frac{1}{n^{2}}\\right)\n=\\frac{(n-1)^{2}}{n^{2}}+1-\\frac{1}{n^{2}}.\n$$\nSimplify:\n$$\n\\frac{(n-1)^{2}}{n^{2}}+1-\\frac{1}{n^{2}}=\\frac{n^{2}-2n+1}{n^{2}}+\\frac{n^{2}-1}{n^{2}}=\\frac{2n^{2}-2n}{n^{2}}=2-\\frac{2}{n}\\to 2.\n$$\nSince the limit is $2\\neq 0$, $\\{X_{n}\\}$ does not converge to $1$ in mean square, so option B is false.\n\nNow check convergence in probability to $0$. For any fixed $\\epsilon0$,\n$$\nP(|X_{n}-0|\\epsilon)=P(|X_{n}|\\epsilon)=\n\\begin{cases}\n\\frac{1}{n^{2}},  n\\epsilon,\\\\\n0,  n\\leq \\epsilon.\n\\end{cases}\n$$\nIn either case, $\\lim_{n\\to\\infty}P(|X_{n}|\\epsilon)=0$, because for all sufficiently large $n$ we have $n\\epsilon$ and then $P(|X_{n}|\\epsilon)=\\frac{1}{n^{2}}\\to 0$. Therefore, $X_{n}\\to 0$ in probability. Hence option D is false, and the precise statement is that $X_{n}$ converges to $0$ in probability but not in mean square.\n\nConsequently, the correct choice is that the sequence converges to $0$ in probability, but does not converge to $0$ in mean square.", "answer": "$$\\boxed{C}$$", "id": "1318373"}, {"introduction": "The principles of mean square convergence have direct applications in statistics, particularly in evaluating the quality of estimators. This final practice [@problem_id:1318365] challenges you to assess an unconventional estimator for a population mean. You will use the mean square convergence criterion to determine if this estimator is consistent, thereby connecting theoretical concepts to practical statistical evaluation.", "problem": "Let $X_1, X_2, \\ldots, X_n$ be a sequence of independent and identically distributed (i.i.d.) random variables. Each random variable $X_i$ has a common finite mean $E[X_i] = \\theta$ and a common finite variance $\\text{Var}(X_i) = \\sigma^2$.\n\nConsider an unconventional estimator for the mean $\\theta$, denoted by $\\hat{\\theta}_n$, which is defined for any sample size $n \\geq 1$ as:\n$$\n\\hat{\\theta}_n = X_1\n$$\nThis estimator uses only the first observation from the sequence, regardless of how many subsequent observations are available.\n\nUnder which of the following conditions does this estimator $\\hat{\\theta}_n$ converge in mean square to the true mean $\\theta$?\n\nA. It always converges in mean square for any finite $\\sigma^2$.\n\nB. It converges in mean square if and only if $\\sigma^2 = 0$.\n\nC. It converges in mean square if and only if $\\theta = 0$.\n\nD. It never converges in mean square.\n\nE. It converges in mean square if and only if $\\sigma^2 > 0$.", "solution": "We require convergence in mean square (also called convergence in $L^{2}$) of $\\hat{\\theta}_{n}$ to $\\theta$. By definition, $\\hat{\\theta}_{n} \\to \\theta$ in mean square if and only if\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}\\big[(\\hat{\\theta}_{n}-\\theta)^{2}\\big]=0.\n$$\nHere $\\hat{\\theta}_{n}=X_{1}$ for all $n \\geq 1$, so\n$$\n\\mathbb{E}\\big[(\\hat{\\theta}_{n}-\\theta)^{2}\\big]=\\mathbb{E}\\big[(X_{1}-\\theta)^{2}\\big].\n$$\nUsing the identity $\\mathbb{E}\\big[(Y-a)^{2}\\big]=\\text{Var}(Y)+\\big(\\mathbb{E}[Y]-a\\big)^{2}$ with $Y=X_{1}$ and $a=\\theta$, and the given $\\mathbb{E}[X_{1}]=\\theta$, we obtain\n$$\n\\mathbb{E}\\big[(X_{1}-\\theta)^{2}\\big]=\\text{Var}(X_{1})=\\sigma^{2}.\n$$\nThis expression does not depend on $n$, hence\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}\\big[(\\hat{\\theta}_{n}-\\theta)^{2}\\big]=\\sigma^{2}.\n$$\nTherefore, $\\hat{\\theta}_{n}$ converges in mean square to $\\theta$ if and only if this limit equals $0$, which holds if and only if $\\sigma^{2}=0$. Among the given options, this corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1318365"}]}