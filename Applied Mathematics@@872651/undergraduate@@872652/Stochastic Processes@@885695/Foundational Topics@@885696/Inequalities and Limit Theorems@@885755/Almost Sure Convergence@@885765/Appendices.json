{"hands_on_practices": [{"introduction": "The Strong Law of Large Numbers (SLLN) is a cornerstone of probability theory, formalizing the intuition that the average of a long sequence of random trials should converge to its expected value. This first exercise provides a direct and intuitive application of the SLLN. By analyzing the proportion of random variables falling below the median, you will see how this fundamental law confirms that empirical frequencies converge to theoretical probabilities almost surely. [@problem_id:1281050]", "problem": "Let $X_1, X_2, \\dots$ be a sequence of independent and identically distributed (i.i.d.) random variables drawn from a continuous probability distribution. The distribution is characterized by a Cumulative Distribution Function (CDF), denoted as $F(x)$, which is strictly increasing over its support. Let $m$ be the unique median of this distribution, defined by the property $F(m) = 0.5$.\n\nConsider the sample proportion, denoted by $P_n$, which is the fraction of the first $n$ random variables in the sequence that are strictly smaller than the median $m$. That is,\n$$P_n = \\frac{\\text{Number of } X_k < m \\text{ for } k=1, \\dots, n}{n}$$\n\nDetermine the value to which the sequence of random variables $P_n$ converges almost surely as $n$ approaches infinity. Your answer should be a single real number.", "solution": "Define the indicator variables $I_k = \\boldsymbol{1}_{\\{X_k  m\\}}$ for $k=1, \\dots, n$. Since $X_1, X_2, \\dots$ are i.i.d. with continuous, strictly increasing CDF $F$, we have $\\mathbb{P}(X_1  m) = F(m) = \\frac{1}{2}$ and $\\mathbb{P}(X_1 = m) = 0$. Therefore, the $I_k$ are i.i.d. Bernoulli random variables with parameter $p = \\mathbb{P}(X_1  m) = \\frac{1}{2}$, and their common expectation is $\\mathbb{E}[I_1] = \\frac{1}{2}$.\n\nBy definition,\n$$\nP_n = \\frac{1}{n}\\sum_{k=1}^{n}I_k.\n$$\nBy the Strong Law of Large Numbers, if $\\{Y_k\\}$ are i.i.d. with $\\mathbb{E}[|Y_1|]  \\infty$, then $\\frac{1}{n}\\sum_{k=1}^{n}Y_k \\to \\mathbb{E}[Y_1]$ almost surely as $n \\to \\infty$. Applying this to $Y_k=I_k$ yields\n$$\nP_n=\\frac{1}{n}\\sum_{k=1}^{n}I_k\\xrightarrow[\\text{a.s.}]{n\\to\\infty}\\mathbb{E}[I_1]=\\frac{1}{2}.\n$$\nHence $P_n$ converges almost surely to $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1281050"}, {"introduction": "Building on the basic application of the Strong Law, this practice explores how almost sure convergence interacts with functions of random sequences. Real-world analysis often involves comparing the long-term performance of different processes, such as the two trading algorithms in this hypothetical scenario. This problem asks you to determine the limit of a ratio of cumulative sums, demonstrating how the SLLN, combined with properties of limits, allows us to analyze more complex composite quantities. [@problem_id:1281015]", "problem": "A financial technology company launches two new automated trading algorithms, named Alpha and Beta. Let the random variable $X_k$ represent the net profit generated by algorithm Alpha on day $k$, and $Y_k$ be the net profit from algorithm Beta on the same day. The daily profit pairs $(X_k, Y_k)$ for $k=1, 2, 3, \\ldots$ are modeled as a sequence of independent and identically distributed (i.i.d.) random vectors.\n\nThe company's analysts have established the long-term expected daily profits for each algorithm. The expected profit for Alpha is $E[X_k] = \\mu_X$, and for Beta is $E[Y_k] = \\mu_Y$. Both $\\mu_X$ and $\\mu_Y$ are finite constants, and it is known that the Beta algorithm is, on average, profitable, so $\\mu_Y \\neq 0$.\n\nTo assess the long-term relative performance, the company wants to study the ratio of the total cumulative profit from Alpha to the total cumulative profit from Beta. Determine the value to which this ratio converges almost surely as the number of observation days, $n$, tends to infinity. Your final answer should be an expression in terms of $\\mu_X$ and $\\mu_Y$.", "solution": "Let $S_{n}^{X}=\\sum_{k=1}^{n}X_{k}$ and $S_{n}^{Y}=\\sum_{k=1}^{n}Y_{k}$. The ratio of cumulative profits is\n$$\nR_{n}=\\frac{S_{n}^{X}}{S_{n}^{Y}}=\\frac{\\frac{1}{n}S_{n}^{X}}{\\frac{1}{n}S_{n}^{Y}}.\n$$\nBy the strong law of large numbers applied to each coordinate of the i.i.d. sequence $(X_k, Y_k)$, and using that $E[|X_1|]  \\infty$ and $E[|Y_1|]  \\infty$, we have almost surely\n$$\n\\frac{1}{n}\\sum_{k=1}^{n}X_{k}\\to \\mu_{X},\\qquad \\frac{1}{n}\\sum_{k=1}^{n}Y_{k}\\to \\mu_{Y}.\n$$\nSince $\\mu_{Y}\\neq 0$, the denominator limit is nonzero, hence by the continuous mapping theorem (continuity of $(a,b)\\mapsto a/b$ at $(\\mu_X, \\mu_Y)$), it follows almost surely that\n$$\nR_{n}=\\frac{\\frac{1}{n}S_{n}^{X}}{\\frac{1}{n}S_{n}^{Y}}\\to \\frac{\\mu_{X}}{\\mu_{Y}}.\n$$\nMoreover, because $\\frac{1}{n}S_n^Y \\to \\mu_Y \\neq 0$ almost surely, there exists a (random) $N$ such that for all $n \\ge N$, $\\frac{1}{n}S_n^Y \\neq 0$, ensuring the ratio is eventually well-defined almost surely. Therefore, the ratio of cumulative profits converges almost surely to $\\frac{\\mu_X}{\\mu_Y}$.", "answer": "$$\\boxed{\\frac{\\mu_{X}}{\\mu_{Y}}}$$", "id": "1281015"}, {"introduction": "Convergence theorems are powerful, but their conclusions depend critically on their underlying assumptions. This exercise challenges you to investigate a famous case where the Strong Law of Large Numbers does not apply: the Cauchy distribution, which lacks a finite mean. By exploring whether a subsequence can converge to zero, you will delve into the more subtle logic of the Borel-Cantelli lemmas, gaining a deeper appreciation for the conditions that guarantee almost sure convergence. [@problem_id:1280996]", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, X_3, \\dots$. Each random variable $X_n$ follows a standard Cauchy distribution, which is characterized by the Probability Density Function (PDF) given by $f(x) = \\frac{1}{\\pi(1+x^2)}$ for $-\\infty  x  \\infty$.\n\nA subsequence of this sequence is denoted by $\\{X_{n_k}\\}$, which is formed by selecting terms from the original sequence such that $1 \\le n_1  n_2  n_3  \\dots$ is a strictly increasing sequence of integers.\n\nWe are interested in whether it is possible to find a subsequence $\\{X_{n_k}\\}$ that converges almost surely to 0. Recall that a sequence of random variables $Y_k$ converges almost surely to a constant $c$ if the probability of the set of outcomes where the limit of the sequence equals $c$ is 1, i.e., $P(\\lim_{k \\to \\infty} Y_k = c) = 1$.\n\nWhich of the following statements is true regarding this scenario?\n\nA. Yes, for any sequence of i.i.d. standard Cauchy random variables, a subsequence converging almost surely to 0 can always be constructed.\n\nB. No, for any sequence of i.i.d. standard Cauchy random variables, no subsequence converges almost surely to 0.\n\nC. The existence of such a subsequence is an event with a probability strictly between 0 and 1.\n\nD. The question is ill-posed as almost sure convergence is not a meaningful concept for Cauchy random variables because they lack a finite mean.\n\nE. Yes, such a subsequence exists, but only a specific one, for example, by selecting the indices as perfect squares, i.e., $X_{k^2}$.", "solution": "Let $\\{X_n\\}_{n\\geq 1}$ be i.i.d. standard Cauchy with density $f(x) = \\frac{1}{\\pi(1+x^2)}$. Consider any deterministic subsequence of indices $\\{n_k\\}_{k\\geq 1}$ with $1 \\leq n_1  n_2  \\cdots$, and the corresponding subsequence $\\{X_{n_k}\\}$.\n\nFix $\\epsilon0$ and define the events\n$$\nA_k^{(\\epsilon)} = \\{|X_{n_k}| \\ge \\epsilon\\}, \\quad k \\ge 1.\n$$\nBecause the $X_n$ are i.i.d., the events $A_k^{(\\epsilon)}$ are independent and identically distributed, with\n$$\n\\mathbb{P}\\big(A_k^{(\\epsilon)}\\big) = \\mathbb{P}(|X_1| \\ge \\epsilon) = 1 - \\mathbb{P}(|X_1|  \\epsilon).\n$$\nFor a standard Cauchy, the cumulative distribution function is $F(x) = \\frac{1}{\\pi}\\arctan(x) + \\frac{1}{2}$, hence\n$$\n\\mathbb{P}(|X_1|  \\epsilon) = F(\\epsilon) - F(-\\epsilon) = \\frac{2}{\\pi}\\arctan(\\epsilon),\n$$\nso\n$$\n\\mathbb{P}\\big(A_k^{(\\epsilon)}\\big) = 1 - \\frac{2}{\\pi}\\arctan(\\epsilon) =: p_\\epsilon,\n$$\nwhere $p_\\epsilon \\in (0,1)$ for every $\\epsilon0$.\n\nTherefore\n$$\n\\sum_{k=1}^{\\infty}\\mathbb{P}\\big(A_k^{(\\epsilon)}\\big) = \\sum_{k=1}^{\\infty}p_\\epsilon = +\\infty.\n$$\nBy the second Borelâ€“Cantelli lemma (which applies because $\\{A_k^{(\\epsilon)}\\}$ are independent),\n$$\n\\mathbb{P}\\big(A_k^{(\\epsilon)}\\ \\text{i.o.}\\big)=1,\n$$\ni.e., with probability one, $|X_{n_k}| \\ge \\epsilon$ occurs infinitely often. Since this holds for every fixed $\\epsilon0$, it follows that\n$$\n\\mathbb{P}\\Big(\\lim_{k\\to\\infty}X_{n_k}=0\\Big)=0.\n$$\nIndeed, convergence to $0$ would require that for each $m \\in \\mathbb{N}$, only finitely many $k$ satisfy $|X_{n_k}| \\ge \\frac{1}{m}$, but for each $m$, the event $\\{|X_{n_k}| \\ge \\frac{1}{m}\\ \\text{i.o.}\\}$ has probability $1$.\n\nHence no deterministic subsequence of an i.i.d. Cauchy sequence can converge almost surely to $0$. Consequently:\n- A is false (such a subsequence cannot be constructed deterministically).\n- B is true.\n- C is false (the event is tail-measurable and, by the above, has probability $0$, not strictly between $0$ and $1$).\n- D is false (almost sure convergence is meaningful regardless of the existence of a mean).\n- E is false (choosing $n_k=k^2$ is still deterministic and fails by the same argument).", "answer": "$$\\boxed{B}$$", "id": "1280996"}]}