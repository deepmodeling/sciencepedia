{"hands_on_practices": [{"introduction": "To truly master the concept of convergence in probability, it is as crucial to understand what it isn't as what it is. This first practice explores a classic counterexample: a simple, deterministic sequence that oscillates between two values. By rigorously applying the formal definition of convergence in probability, you will see why this bounded sequence fails to converge, reinforcing the idea that the probability of deviating from the limit must vanish. [@problem_id:1910711]", "problem": "Consider a sequence of random variables $\\{X_n\\}_{n=1}^{\\infty}$ that models the state of a simple digital switch at discrete time steps $n=1, 2, 3, \\ldots$. The switch is designed to be in one of two states, represented by the values $-1$ and $1$. The state of the switch at time $n$ is deterministic and follows the rule $X_n = (-1)^n$. This means that for each $n$, the random variable $X_n$ takes the value $(-1)^n$ with a probability of 1.\n\nThe sequence of states is therefore $X_1 = -1$, $X_2 = 1$, $X_3 = -1$, $X_4 = 1$, and so on.\n\nWhich of the following statements correctly describes the convergence behavior of this sequence?\n\nA. The sequence $\\{X_n\\}$ converges in probability to 0.\n\nB. The sequence $\\{X_n\\}$ converges in probability to 1.\n\nC. The sequence $\\{X_n\\}$ converges in probability to -1.\n\nD. The sequence $\\{X_n\\}$ does not converge in probability.\n\nE. The sequence $\\{X_n\\}$ converges in probability, but the limit is not a constant.", "solution": "We recall the definition: a sequence of random variables $\\{X_{n}\\}$ converges in probability to a random variable $X$ if for every $\\varepsilon0$,\n$$\n\\lim_{n\\to\\infty} P(|X_{n}-X|\\varepsilon)=0.\n$$\nHere each $X_{n}$ is deterministic with $X_{n}=(-1)^{n}$ almost surely.\n\nFirst, check convergence in probability to a constant $c\\in\\mathbb{R}$. If $c\\notin\\{-1,1\\}$, set $\\delta=\\min\\{|c-1|,|c+1|\\}0$ and choose $\\varepsilon=\\delta/2$. Then for every $n$,\n$$\n|X_{n}-c|=\\left|(-1)^{n}-c\\right|\\geq \\delta\\varepsilon,\n$$\nso $P(|X_{n}-c|\\varepsilon)=1$ for all $n$, which cannot tend to $0$. Hence no limit $c\\notin\\{-1,1\\}$ is possible.\n\nIf $c=1$, choose $\\varepsilon=\\frac{1}{2}$. Then for odd $n$,\n$$\n|X_{n}-1|=|(-1)-1|=2\\frac{1}{2},\n$$\nso $P(|X_{n}-1|\\frac{1}{2})=1$ for all odd $n$, which does not tend to $0$. Similarly, if $c=-1$, for even $n$,\n$$\n|X_{n}-(-1)|=|1-(-1)|=2\\frac{1}{2},\n$$\nso $P(|X_{n}+1|\\frac{1}{2})=1$ for all even $n$, which also does not tend to $0$. Therefore $\\{X_{n}\\}$ does not converge in probability to any constant, ruling out A, B, and C.\n\nNext, consider whether $\\{X_{n}\\}$ could converge in probability to a non-constant random variable $X$. Take $\\varepsilon=\\frac{1}{2}$. Then\n$$\nP(|X_{n}-X|\\frac{1}{2})=P(|(-1)^{n}-X|\\frac{1}{2}).\n$$\nDecompose $X$ by its mass on $\\{-1,1\\}$: let $p=P(X=1)$, $q=P(X=-1)$, and $r=1-p-q=P(X\\notin\\{-1,1\\})$. For even $n$ (so $X_{n}=1$),\n$$\nP(|X_{n}-X|\\frac{1}{2})=P(|1-X|\\frac{1}{2})=q+r.\n$$\nFor odd $n$ (so $X_{n}=-1$),\n$$\nP(|X_{n}-X|\\frac{1}{2})=P(|-1-X|\\frac{1}{2})=p+r.\n$$\nThus the sequence of probabilities alternates between the constants $q+r$ and $p+r$. For convergence in probability, we would need both $q+r\\to 0$ and $p+r\\to 0$, which forces $p=q=r=0$, contradicting $p+q+r=1$. Hence no random limit (constant or not) exists. Therefore the sequence does not converge in probability, and option D is correct.", "answer": "$$\\boxed{D}$$", "id": "1910711"}, {"introduction": "Building upon the foundational definition, we now explore how convergent sequences interact. This exercise demonstrates a key property: the preservation of convergence under addition, often referred to as a part of Slutsky's Theorem. You will combine a sequence that converges to a constant with another sequence whose variance diminishes to zero, using Chebyshev's inequality to prove the latter converges to zero and then showing their sum converges to the expected constant. [@problem_id:1910723]", "problem": "Let $\\{X_n\\}_{n=1}^{\\infty}$ and $\\{Y_n\\}_{n=1}^{\\infty}$ be two sequences of random variables. The sequence $\\{X_n\\}$ is known to converge in probability to the constant 5. The sequence $\\{Y_n\\}$ is characterized by having a mean of $E[Y_n] = 0$ and a variance of $Var(Y_n) = \\frac{1}{\\sqrt{n}}$ for all positive integers $n$.\n\nA new sequence of random variables, $\\{Z_n\\}_{n=1}^{\\infty}$, is defined by the sum $Z_n = X_n + Y_n$.\n\nDetermine the numerical value to which the sequence $\\{Z_n\\}$ converges in probability.", "solution": "We are given that $X_n \\xrightarrow{p} 5$, that is, for every $\\varepsilon0$,\n$$\n\\lim_{n\\to\\infty} P(|X_n-5|\\varepsilon)=0.\n$$\nFor $Y_n$, we have $E[Y_n]=0$ and $Var(Y_n)=n^{-1/2}$ for all $n$. By Chebyshev’s inequality, for any $\\varepsilon0$,\n$$\nP(|Y_n|\\varepsilon)=P(|Y_n-E[Y_n]|\\varepsilon)\\leq \\frac{Var(Y_n)}{\\varepsilon^{2}}=\\frac{n^{-1/2}}{\\varepsilon^{2}}\\xrightarrow[n\\to\\infty]{}0.\n$$\nHence $Y_n \\xrightarrow{p} 0$.\n\nDefine $Z_n=X_n+Y_n$. For any $\\varepsilon0$, by the triangle inequality,\n$$\n|Z_n-5|=|(X_n-5)+Y_n|\\leq |X_n-5|+|Y_n|.\n$$\nTherefore, using the union bound,\n$$\nP(|Z_n-5|\\varepsilon)\\leq P(|X_n-5|\\varepsilon/2)+P(|Y_n|\\varepsilon/2)\\xrightarrow[n\\to\\infty]{}0,\n$$\nbecause the first term tends to zero by $X_n \\xrightarrow{p} 5$ and the second term tends to zero by Chebyshev’s inequality shown above. Thus $Z_n \\xrightarrow{p} 5$.\n\nTherefore, the numerical value to which $Z_n$ converges in probability is $5$.", "answer": "$$\\boxed{5}$$", "id": "1910723"}, {"introduction": "This final practice bridges theory and application, showcasing why convergence in probability is a cornerstone of statistical inference. You will analyze the consistency of an estimator for a thermoelectric generator's efficiency, which is a ratio of two sample means. This problem ties together the Weak Law of Large Numbers and the Continuous Mapping Theorem, illustrating how these powerful results guarantee that our statistical estimates approach the true parameter values as we collect more data. [@problem_id:1910693]", "problem": "An engineer is characterizing a new type of thermoelectric generator. In a series of $n$ independent trials, two physical quantities are measured for each trial $i$: the heat flow across the generator, $X_i$, and the resulting electrical power output, $Y_i$.\n\nThe sequences of measurements, $\\{X_1, X_2, \\dots, X_n\\}$ and $\\{Y_1, Y_2, \\dots, Y_n\\}$, can be modeled as follows:\n- The heat flow measurements $\\{X_i\\}$ are independent and identically distributed (i.i.d.) random variables with a true mean heat flow $E[X_i] = \\mu_{Q}$ and a finite variance.\n- The power output measurements $\\{Y_i\\}$ are i.i.d. random variables with a true mean power output $E[Y_i] = \\mu_{P}$ and a finite variance.\n- It is known that the true mean heat flow is non-zero, i.e., $\\mu_{Q} \\neq 0$.\n\nTo estimate the generator's conversion efficiency, the engineer computes the sample means of the two sets of measurements after $n$ trials:\n$$\n\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\quad \\text{and} \\quad \\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\nThe engineer then defines an estimator for the efficiency, $\\eta_n$, as the ratio of the sample mean power output to the sample mean heat flow:\n$$\n\\eta_n = \\frac{\\bar{Y}_n}{\\bar{X}_n}\n$$\nDetermine the value to which the sequence of random variables $\\eta_n$ converges in probability as the number of trials $n$ approaches infinity. Express your answer as an analytic expression in terms of $\\mu_{Q}$ and $\\mu_{P}$.", "solution": "By assumption, the heat flow measurements $\\{X_i\\}$ are i.i.d. with finite variance and mean $E[X_i]=\\mu_{Q}$, and the power measurements $\\{Y_i\\}$ are i.i.d. with finite variance and mean $E[Y_i]=\\mu_{P}$. Define the sample means\n$$\n\\bar{X}_n=\\frac{1}{n}\\sum_{i=1}^{n}X_i,\\qquad \\bar{Y}_n=\\frac{1}{n}\\sum_{i=1}^{n}Y_i.\n$$\nBy the Weak Law of Large Numbers (WLLN), finite variance implies\n$$\n\\bar{X}_n \\xrightarrow{p} \\mu_{Q}\\quad\\text{and}\\quad \\bar{Y}_n \\xrightarrow{p} \\mu_{P}\\quad\\text{as }n\\to\\infty.\n$$\nFrom these, joint convergence in probability of the pair follows: for any $\\varepsilon0$,\n$$\nP(\\|(\\bar{X}_n,\\bar{Y}_n)-(\\mu_{Q},\\mu_{P})\\|\\varepsilon)\n\\leq P(|\\bar{X}_n-\\mu_{Q}|\\frac{\\varepsilon}{2})+P(|\\bar{Y}_n-\\mu_{P}|\\frac{\\varepsilon}{2})\\to 0,\n$$\nso $(\\bar{X}_n,\\bar{Y}_n)\\xrightarrow{p}(\\mu_{Q},\\mu_{P})$. Because $\\mu_{Q}\\neq 0$, the mapping $g(x,y)=y/x$ is continuous at $(\\mu_{Q},\\mu_{P})$. By the Continuous Mapping Theorem,\n$$\n\\eta_n=\\frac{\\bar{Y}_n}{\\bar{X}_n}=g(\\bar{X}_n,\\bar{Y}_n)\\xrightarrow{p} g(\\mu_{Q},\\mu_{P})=\\frac{\\mu_{P}}{\\mu_{Q}}.\n$$\nFinally, the fact that $\\mu_{Q}\\neq 0$ also ensures that $P(|\\bar{X}_n|\\frac{|\\mu_{Q}|}{2})\\to 1$, so the ratio is well-defined with probability tending to one.\nTherefore, $\\eta_n$ converges in probability to $\\mu_{P}/\\mu_{Q}$.", "answer": "$$\\boxed{\\frac{\\mu_{P}}{\\mu_{Q}}}$$", "id": "1910693"}]}