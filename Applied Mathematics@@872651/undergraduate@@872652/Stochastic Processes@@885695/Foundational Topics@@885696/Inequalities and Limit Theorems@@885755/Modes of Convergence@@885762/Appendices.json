{"hands_on_practices": [{"introduction": "Understanding how a sequence of random variables behaves in the long run is a central theme in probability theory. The most fundamental type of convergence is convergence in distribution, which tells us whether the overall probabilistic behavior of the sequence settles into a stable form. This exercise [@problem_id:1936874] challenges our intuition by examining a deterministic sequence, forcing us to rigorously apply the definition of convergence in distribution using Cumulative Distribution Functions (CDFs) and see that the convergence of random variables is a more subtle concept than the convergence of a simple sequence of numbers.", "problem": "Consider a sequence of deterministic real numbers $\\{c_n\\}_{n=1}^{\\infty}$ defined by $c_n = \\sin(\\frac{\\pi n}{2})$. Let $\\{X_n\\}_{n=1}^{\\infty}$ be a sequence of discrete random variables such that for each $n$, the random variable $X_n$ takes the value $c_n$ with probability 1. In other words, $P(X_n = c_n) = 1$.\n\nDoes the sequence of random variables $\\{X_n\\}$ converge in distribution? If so, to what limiting distribution? If not, why not? Select the single most accurate statement from the options below.\n\nA. Yes, it converges in distribution to a random variable $X$ that takes the value 0 with probability 1.\n\nB. Yes, it converges in distribution to a discrete random variable $X$ with probability mass function $P(X=-1) = \\frac{1}{4}$, $P(X=0) = \\frac{1}{2}$, and $P(X=1) = \\frac{1}{4}$.\n\nC. No, because the sequence of real numbers $c_n = \\sin(\\frac{\\pi n}{2})$ does not converge as $n \\to \\infty$.\n\nD. No, because there exist values of $x$ for which the sequence of cumulative distribution functions $\\{F_{X_n}(x)\\}$ does not converge.", "solution": "We analyze the deterministic sequence given by $c_{n}=\\sin\\left(\\frac{\\pi n}{2}\\right)$. Using the periodicity of the sine function and evaluating at integer multiples, we have the exact pattern\n$$\nc_{n}=\\begin{cases}\n1, n\\equiv 1 \\pmod{4},\\\\\n0, n\\equiv 0 \\text{ or } 2 \\pmod{4},\\\\\n-1, n\\equiv 3 \\pmod{4}.\n\\end{cases}\n$$\nThus the sequence $\\{c_{n}\\}$ cycles through $1,0,-1,0,\\dots$ and does not converge as $n\\to\\infty$.\n\nFor each $n$, $X_{n}$ is a degenerate random variable at $c_{n}$, so its cumulative distribution function is\n$$\nF_{X_{n}}(x)=\\mathbb{P}(X_{n}\\le x)=\\begin{cases}\n0, xc_{n},\\\\\n1, x\\ge c_{n}.\n\\end{cases}\n$$\nTo check convergence in distribution, by definition $\\{X_{n}\\}$ converges in distribution to some random variable $X$ with CDF $F$ if and only if\n$$\n\\lim_{n\\to\\infty}F_{X_{n}}(x)=F(x)\\quad\\text{for all continuity points }x\\text{ of }F.\n$$\nWe test the pointwise behavior of $F_{X_{n}}(x)$ at fixed $x$:\n\n1. If $x-1$, then $xc_{n}$ for all $n$, hence $F_{X_{n}}(x)=0$ for all $n$, so the limit is $0$.\n\n2. If $x1$, then $x\\ge c_{n}$ for all $n$, hence $F_{X_{n}}(x)=1$ for all $n$, so the limit is $1$.\n\n3. If $-1x0$, then for $n\\equiv 3 \\pmod{4}$ we have $c_{n}=-1$ and thus $F_{X_{n}}(x)=1$, while for $n\\equiv 1 \\pmod{4}$ we have $c_{n}=1$ and $F_{X_{n}}(x)=0$, and for even $n$ we have $c_{n}=0$ and $F_{X_{n}}(x)=0$. Hence $F_{X_{n}}(x)$ oscillates between $1$ and $0$ and has no limit.\n\n4. If $0x1$, then for $n\\equiv 1 \\pmod{4}$ we have $c_{n}=1$ and $F_{X_{n}}(x)=0$, while for $n$ even, $c_{n}=0$ gives $F_{X_{n}}(x)=1$, and for $n\\equiv 3 \\pmod{4}$, $c_{n}=-1$ gives $F_{X_{n}}(x)=1$. Again the sequence $F_{X_{n}}(x)$ oscillates and has no limit.\n\nSince there exist values of $x$ (specifically any $x$ in $(-1,0)$ or in $(0,1)$) for which the sequence $\\{F_{X_{n}}(x)\\}$ does not converge, there is no distribution function $F$ such that $F_{X_{n}}(x)\\to F(x)$ at all continuity points of $F$. Therefore $\\{X_{n}\\}$ does not converge in distribution.\n\nThis directly supports statement D. Statement C cites non-convergence of $\\{c_{n}\\}$ alone, which is insufficient in general to preclude convergence in distribution; the definitive reason is the non-convergence of the CDFs at many $x$. Statements A and B are false because the pointwise CDF limits required for those candidate limits fail at points in $(-1,1)$.", "answer": "$$\\boxed{D}$$", "id": "1936874"}, {"introduction": "Beyond the shape of the distribution, we often care about how close the actual values of random variables get to a certain number. This leads to stronger modes of convergence, such as convergence in probability and convergence in mean square. This practice problem [@problem_id:1319233] presents a hypothetical scenario where rare but increasingly large \"surge\" events occur. It provides a perfect setting to explore the relationship between these two modes of convergence and answer a critical question: if the likelihood of an error vanishes, does its average energetic impact also vanish?", "problem": "An experimental data transmission protocol is being tested. On the $n$-th trial (for $n=1, 2, 3, \\dots$), a packet of data is transmitted. Due to noise in the channel, a surge of energy can corrupt the packet. The magnitude of this energy surge, if it occurs, is precisely $E_n = n^2$ joules. The probability of such a surge occurring on the $n$-th trial is $p_n = 1/n^3$. If no surge occurs, the energy level is 0 joules. Let the random variable $X_n$ represent the energy of the surge on the $n$-th trial.\n\nWe are interested in whether the sequence of energy surges $\\{X_n\\}$ eventually settles down to zero. Analyze the convergence of the sequence of random variables $\\{X_n\\}$ to the constant random variable $X=0$ in two different ways: convergence in probability and convergence in mean square.\n\nWhich of the following statements correctly describes the convergence properties of the sequence $\\{X_n\\}$?\n\nA. The sequence $\\{X_n\\}$ converges to 0 in probability and in mean square.\n\nB. The sequence $\\{X_n\\}$ converges to 0 in probability, but does not converge to 0 in mean square.\n\nC. The sequence $\\{X_n\\}$ does not converge to 0 in probability, but it converges to 0 in mean square.\n\nD. The sequence $\\{X_n\\}$ does not converge to 0 in probability and does not converge to 0 in mean square.", "solution": "Define the random variables by $P(X_{n}=n^{2})=p_{n}=\\frac{1}{n^{3}}$ and $P(X_{n}=0)=1-\\frac{1}{n^{3}}$. We analyze convergence to the constant random variable $X=0$.\n\nConvergence in probability: By definition, $X_{n}\\to 0$ in probability if for every $\\varepsilon0$,\n$$\n\\lim_{n\\to\\infty}P\\big(|X_{n}-0|\\varepsilon\\big)=0.\n$$\nSince $X_{n}\\in\\{0,n^{2}\\}$, for any fixed $\\varepsilon0$ and all $n$ such that $n^{2}\\varepsilon$, we have\n$$\n\\{|X_{n}|\\varepsilon\\}=\\{X_{n}=n^{2}\\},\n$$\nhence\n$$\nP(|X_{n}|\\varepsilon)=P(X_{n}=n^{2})=\\frac{1}{n^{3}}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty}P(|X_{n}|\\varepsilon)=\\lim_{n\\to\\infty}\\frac{1}{n^{3}}=0,\n$$\nwhich proves $X_{n}\\to 0$ in probability.\n\nConvergence in mean square: By definition, $X_{n}\\to 0$ in mean square if\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}\\big[(X_{n}-0)^{2}\\big]=0.\n$$\nCompute\n$$\n\\mathbb{E}[X_{n}^{2}]=n^{4}\\cdot\\frac{1}{n^{3}}+0\\cdot\\Big(1-\\frac{1}{n^{3}}\\Big)=n.\n$$\nThus\n$$\n\\lim_{n\\to\\infty}\\mathbb{E}[X_{n}^{2}]=\\lim_{n\\to\\infty}n=+\\infty\\neq 0,\n$$\nso $X_{n}$ does not converge to $0$ in mean square.\n\nTherefore, the sequence converges to $0$ in probability but not in mean square, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1319233"}, {"introduction": "The Law of Large Numbers (LLN) is a cornerstone of statistics, formalizing the idea that the sample average of a large number of trials should be close to the expected value. This law is a powerful statement about convergence in probability. However, the LLN relies on certain assumptions, and this hands-on practice [@problem_id:1319185] explores what happens when a key assumption—the existence of a finite mean—is violated. By examining the sample mean of the famously volatile Cauchy distribution, we can directly observe a failure of the Law of Large Numbers and gain a deeper appreciation for the conditions that make statistical estimation reliable.", "problem": "Let $X_1, X_2, \\dots, X_n$ be a sequence of $n$ independent and identically distributed (i.i.d.) random variables, where $n$ is a positive integer. Each random variable $X_i$ follows a standard Cauchy distribution, which is defined by the probability density function (PDF):\n$$f(x) = \\frac{1}{\\pi(1 + x^2)}, \\quad \\text{for } -\\infty  x  \\infty$$\nThe sample mean of these random variables is defined as $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$.\n\nIdentify the distribution of the sample mean $\\bar{X}_n$.\n\nA. A Normal distribution with mean 0 and variance $\\frac{1}{n}$.\n\nB. A Normal distribution with mean 0 and variance 1.\n\nC. A Cauchy distribution with location parameter 0 and scale parameter $n$.\n\nD. A standard Cauchy distribution (location parameter 0, scale parameter 1).\n\nE. A Student's t-distribution with $n-1$ degrees of freedom.\n\nF. The distribution is undefined because the mean of the Cauchy distribution does not exist.", "solution": "Let $X_{1},\\dots,X_{n}$ be i.i.d. standard Cauchy with density $f(x)=\\frac{1}{\\pi(1+x^{2})}$. The characteristic function of a standard Cauchy random variable is\n$$\n\\phi_{X}(t)=\\int_{-\\infty}^{\\infty}\\exp(itx)\\frac{1}{\\pi(1+x^{2})}\\,dx=\\exp(-|t|).\n$$\nBy independence, the sum $S_{n}=\\sum_{i=1}^{n}X_{i}$ has characteristic function\n$$\n\\phi_{S_{n}}(t)=\\prod_{i=1}^{n}\\phi_{X}(t)=[\\exp(-|t|)]^{n}=\\exp(-n|t|).\n$$\nThis is the characteristic function of a Cauchy distribution with location $0$ and scale parameter $n$. For the sample mean $\\bar{X}_{n}=\\frac{S_{n}}{n}$, use the scaling property of characteristic functions: if $Y=aZ$, then $\\phi_{Y}(t)=\\phi_{Z}(at)$. Hence\n$$\n\\phi_{\\bar{X}_{n}}(t)=\\phi_{S_{n}}\\!\\left(\\frac{t}{n}\\right)=\\exp\\!\\left(-n\\left|\\frac{t}{n}\\right|\\right)=\\exp(-|t|),\n$$\nwhich is exactly the characteristic function of the standard Cauchy distribution (location $0$, scale $1$). Therefore, $\\bar{X}_{n}$ is standard Cauchy for every $n$. Although the mean of a Cauchy distribution does not exist, the distribution of the sample mean is well defined and equals the original standard Cauchy distribution, so option F is incorrect.\n\nThe correct choice is the standard Cauchy distribution, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1319185"}]}