{"hands_on_practices": [{"introduction": "This first exercise provides a classic entry point into Bayesian thinking. By working through a hypothetical scenario involving fair and two-headed coins, you will practice the fundamental mechanics of Bayes' theorem [@problem_id:376]. The goal is to see how a single piece of evidence—observing \"heads\"—allows us to update our initial belief about which type of coin we are holding.", "problem": "An opaque bag contains a large number of coins. The coins are of two types: fair and two-headed. A fair coin has one side \"heads\" and one side \"tails\", with an equal probability of landing on either side when flipped. A two-headed coin has \"heads\" on both sides.\n\nThe fraction of fair coins in the bag is given by the parameter $p$, where $0  p  1$. Consequently, the fraction of two-headed coins is $1-p$.\n\nAn individual randomly selects one coin from the bag and flips it once. The outcome of the flip is observed to be \"heads\".\n\nDerive a symbolic expression for the probability that the selected coin was a two-headed coin, given this observation.", "solution": "Let $TH$ be the event that the selected coin is two-headed, and let $F$ be the event that the selected coin is fair. Let $H$ be the event that the outcome of the coin flip is heads. We are asked to find the conditional probability $P(TH | H)$.\n\nFrom the problem statement, we are given the prior probabilities of selecting each type of coin:\n$$\nP(F) = p\n$$\n$$\nP(TH) = 1 - p\n$$\n\nNext, we identify the conditional probabilities of observing heads, given the type of coin selected.\nIf the coin is fair, the probability of flipping heads is:\n$$\nP(H | F) = \\frac{1}{2}\n$$\nIf the coin is two-headed, the probability of flipping heads is certain:\n$$\nP(H | TH) = 1\n$$\n\nWe will use Bayes' theorem to find $P(TH | H)$:\n$$\nP(TH | H) = \\frac{P(H | TH) P(TH)}{P(H)}\n$$\n\nTo use this formula, we first need to calculate the total probability of observing heads, $P(H)$. We can find this using the law of total probability, summing over all possible types of coins:\n$$\nP(H) = P(H | F)P(F) + P(H | TH)P(TH)\n$$\n\nSubstituting the known values into this expression:\n$$\nP(H) = \\left(\\frac{1}{2}\\right) (p) + (1) (1-p)\n$$\n$$\nP(H) = \\frac{p}{2} + 1 - p\n$$\n$$\nP(H) = 1 - \\frac{p}{2}\n$$\n$$\nP(H) = \\frac{2-p}{2}\n$$\n\nNow we have all the components for Bayes' theorem. We substitute the expressions for $P(H | TH)$, $P(TH)$, and $P(H)$:\n$$\nP(TH | H) = \\frac{(1)(1 - p)}{\\frac{2-p}{2}}\n$$\n\nSimplifying the expression gives the final result:\n$$\nP(TH | H) = \\frac{2(1 - p)}{2 - p}\n$$", "answer": "$$\n\\boxed{\\frac{2(1 - p)}{2 - p}}\n$$", "id": "376"}, {"introduction": "Building on the basics, this problem explores a more complex and realistic scenario involving witness testimony. Here, we must weigh the reliability of the evidence against the underlying base rates of the different taxi companies [@problem_id:691263]. This exercise is crucial for understanding how to handle multiple, independent pieces of evidence and for appreciating why base rates are a critical component of Bayesian reasoning.", "problem": "In a city, three taxi companies operate: Azure Autos, Crimson Cabs, and Garnet Group. A witness to a hit-and-run accident at night reported the cab involved. Due to the challenging lighting conditions, witness testimonies are not perfectly reliable. The city's taxi fleet is composed of a fraction $p_A$ from Azure Autos, $p_C$ from Crimson Cabs, and $p_G$ from Garnet Group, where $p_A + p_C + p_G = 1$.\n\nThe color of Azure Autos' cabs is a distinct blue, while the cabs from Crimson Cabs and Garnet Group are two different but easily confusable shades of red. An investigation into witness reliability under similar conditions has established the following conditional probabilities for a single witness's report. Let $E$ be the event that a witness reports seeing an Azure Autos cab.\n\n- The probability that a witness correctly reports an Azure Autos cab when it was indeed from Azure Autos is $P(E | \\text{Azure}) = \\alpha$.\n- The probability that a witness mistakenly reports an Azure Autos cab when it was actually from Crimson Cabs is $P(E | \\text{Crimson}) = \\delta$.\n- The probability that a witness mistakenly reports an Azure Autos cab when it was actually from Garnet Group is $P(E | \\text{Garnet}) = \\delta$.\n\nIn the specific incident in question, two independent witnesses were present, and both of them reported that the cab was from Azure Autos.\n\nDerive a symbolic expression for the posterior probability that the cab involved in the accident was from Azure Autos, given the identical testimony of the two independent witnesses. Your final expression should be in terms of the parameters $p_A$, $\\alpha$, and $\\delta$.", "solution": "1. By Bayes’ theorem for two independent witnesses reporting Azure Autos ($E_1,E_2$):\n$$\nP(\\text{Azure}\\mid E_1,E_2)\n=\\frac{P(E_1,E_2\\mid \\text{Azure})\\,p_A}\n{P(E_1,E_2\\mid \\text{Azure})\\,p_A\n+P(E_1,E_2\\mid \\text{Crimson})\\,p_C\n+P(E_1,E_2\\mid \\text{Garnet})\\,p_G}\\,.\n$$\n2. Independence gives\n$$P(E_1,E_2\\mid \\text{Azure})=\\alpha^2,\\quad\nP(E_1,E_2\\mid \\text{Crimson})=\\delta^2,\\quad\nP(E_1,E_2\\mid \\text{Garnet})=\\delta^2.\n$$\n3. Substitute and use $p_C+p_G=1-p_A$:\n$$\nP(\\text{Azure}\\mid E_1,E_2)\n=\\frac{\\alpha^2\\,p_A}\n{\\alpha^2\\,p_A+\\delta^2\\,p_C+\\delta^2\\,p_G}\n=\\frac{\\alpha^2\\,p_A}{\\alpha^2\\,p_A+\\delta^2(1-p_A)}.\n$$", "answer": "$$\\boxed{\\frac{p_A \\alpha^2}{p_A \\alpha^2 + (1-p_A)\\delta^2}}$$", "id": "691263"}, {"introduction": "This final practice moves beyond simply calculating posterior probabilities to the powerful application of Bayesian estimation. We will use the posterior distribution, which represents our updated belief about a parameter, to find the best possible estimate for a related quantity under a specified loss function [@problem_id:691444]. This problem bridges the gap between Bayesian inference and decision-making, demonstrating how to derive a Bayes estimator for a parameter of interest.", "problem": "Consider a sequence of $n$ independent Bernoulli trials, where the probability of success in each trial is $p$. The number of successes observed is $k$. We are interested in estimating the quantity $\\theta = p^2$.\n\nWithin the framework of Bayesian inference, we adopt a squared-error loss function, $L(\\theta, \\hat{\\theta}) = (\\theta - \\hat{\\theta})^2$, for our estimation problem. The prior belief about the success probability $p$ is modeled by a Beta distribution with hyperparameters $\\alpha  0$ and $\\beta  0$, denoted as $p \\sim \\text{Beta}(\\alpha, \\beta)$. The probability density function of this prior is given by:\n$$ \\pi(p) = \\frac{p^{\\alpha-1} (1-p)^{\\beta-1}}{B(\\alpha, \\beta)} \\quad \\text{for } 0  p  1 $$\nwhere $B(\\alpha, \\beta)$ is the Beta function.\n\nThe Bayes estimator for a quantity is defined as the value that minimizes the posterior expected loss. For a squared-error loss function, this estimator is the mean of the posterior distribution of the quantity being estimated.\n\nDerive the Bayes estimator for $\\theta = p^2$, denoted as $\\hat{\\theta}_{B}$, as a function of the observed number of trials $n$, the number of successes $k$, and the prior hyperparameters $\\alpha$ and $\\beta$.", "solution": "1. After observing $k$ successes in $n$ Bernoulli trials, the posterior distribution of $p$ is \n$$p\\mid \\text{data}\\sim \\mathrm{Beta}(\\alpha+k,\\;\\beta+n-k).$$\n2. The Bayes estimator under squared‐error loss for $\\theta=p^2$ is \n$$\\hat\\theta_B=E[p^2\\mid \\text{data}].$$\n3. For $X\\sim\\mathrm{Beta}(a,b)$, one has \n$$E[X^2]=\\frac{a(a+1)}{(a+b)(a+b+1)}.$$\n4. Here $a=\\alpha+k$ and $b=\\beta+n-k$, giving\n$$\\hat\\theta_B\n=\\frac{(\\alpha+k)(\\alpha+k+1)}{\\bigl((\\alpha+k)+(\\beta+n-k)\\bigr)\\bigl((\\alpha+k)+(\\beta+n-k)+1\\bigr)}\n=\\frac{(\\alpha+k)(\\alpha+k+1)}{(\\alpha+\\beta+n)(\\alpha+\\beta+n+1)}.$$", "answer": "$$\\boxed{\\frac{(\\alpha + k)(\\alpha + k + 1)}{(\\alpha + \\beta + n)(\\alpha + \\beta + n + 1)}}$$", "id": "691444"}]}