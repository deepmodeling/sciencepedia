{"hands_on_practices": [{"introduction": "Let's begin with a classic scenario involving dice rolls to build our core understanding of conditional distributions. This exercise demonstrates the fundamental principle of conditioning: how observing one event (the sum of the dice) changes the probabilities of another related event (their product). By systematically enumerating the possibilities, you will practice constructing a conditional probability mass function from the ground up, a crucial first step in mastering conditional probability. [@problem_id:1291288]", "problem": "Two standard, fair, six-sided dice are rolled independently. Let the random variable $X$ be the outcome of the first die and $Y$ be the outcome of the second die. The outcomes $X$ and $Y$ can each take any integer value from 1 to 6 with equal probability.\n\nTwo new random variables are defined based on these outcomes: the sum $S = X+Y$ and the product $P = XY$.\n\nSuppose it is observed that the sum of the outcomes is exactly 7. Given this information, determine the conditional probability mass function (PMF) of the product $P$. This PMF is denoted by $p(k) = \\mathbb{P}(P=k | S=7)$. Which of the following options correctly describes this conditional PMF for all non-zero probabilities?\n\nA. $p(6) = \\frac{1}{3}$, $p(10) = \\frac{1}{3}$, $p(12) = \\frac{1}{3}$\n\nB. $p(5) = \\frac{2}{5}$, $p(8) = \\frac{2}{5}$, $p(9) = \\frac{1}{5}$\n\nC. $p(6) = \\frac{2}{5}$, $p(10) = \\frac{1}{5}$, $p(12) = \\frac{2}{5}$\n\nD. $p(12) = \\frac{2}{5}$, $p(15) = \\frac{2}{5}$, $p(16) = \\frac{1}{5}$", "solution": "We use independence and uniformity of the two fair dice: each ordered pair $(x,y)$ with $x,y \\in \\{1,2,3,4,5,6\\}$ is equally likely with probability $\\frac{1}{36}$. Conditioning on the event $S=7$ where $S=X+Y$, we first enumerate all ordered pairs satisfying $x+y=7$.\n\nSolve $y=7-x$ with $x \\in \\{1,2,3,4,5,6\\}$. This yields the six ordered pairs:\n$$(1,6),(2,5),(3,4),(4,3),(5,2),(6,1).$$\nUnder the conditioning $S=7$, these six outcomes are equally likely, each with conditional probability $\\frac{1}{6}$.\n\nDefine $P=XY$ and compute the products for these six outcomes:\n$$(1,6) \\mapsto 6,\\quad (2,5) \\mapsto 10,\\quad (3,4) \\mapsto 12,\\quad (4,3) \\mapsto 12,\\quad (5,2) \\mapsto 10,\\quad (6,1) \\mapsto 6.$$\nThus the distinct values of $P$ are $6,10,12$, each occurring exactly twice among the six equally likely outcomes. Therefore, for $k \\in \\{6,10,12\\}$,\n$$p(k)=\\mathbb{P}(P=k \\mid S=7)=\\frac{2}{6}=\\frac{1}{3},$$\nand $p(k)=0$ for all other $k$.\n\nThis matches option A.", "answer": "$$\\boxed{A}$$", "id": "1291288"}, {"introduction": "Having explored discrete variables, we now extend the concept of conditioning to continuous random variables. This practice involves a point chosen from a geometric region, a common setup in spatial modeling. You will learn how to derive a conditional probability density function and use it to compute a conditional expectation, which reveals the average value of one variable when the value of another is known. [@problem_id:1291258]", "problem": "Let $(X, Y)$ be the coordinates of a point chosen uniformly at random from the triangular region in the Cartesian plane with vertices at $(0,0)$, $(1,0)$, and $(1,1)$. The random variables $X$ and $Y$ represent the x-coordinate and y-coordinate of the point, respectively. Determine the conditional expectation of $Y$ given that $X=x$, denoted as $E[Y|X=x]$, for any valid $x$ in the support of the random variable $X$. Express your answer as a function of $x$.", "solution": "The triangular region with vertices at $(0,0)$, $(1,0)$, and $(1,1)$ is the set $S=\\{(x,y): 0 \\leq y \\leq x \\leq 1\\}$. Since the point is chosen uniformly over $S$, the joint density is constant on $S$ and equals the reciprocal of the area of $S$. The area of $S$ is $\\frac{1}{2}$, so the joint density is\n$$\nf_{X,Y}(x,y)=2 \\quad \\text{for } (x,y)\\in S,\\ \\text{and } 0 \\text{ otherwise.}\n$$\nThe marginal density of $X$ is obtained by integrating over the conditional range of $y$ for a fixed $x\\in[0,1]$:\n$$\nf_{X}(x)=\\int_{0}^{x} 2 \\, dy = 2x \\quad \\text{for } 0 \\leq x \\leq 1,\\ \\text{and } 0 \\text{ otherwise.}\n$$\nThus, for $x\\in[0,1]$, the conditional density of $Y$ given $X=x$ is\n$$\nf_{Y\\mid X}(y\\mid x)=\\frac{f_{X,Y}(x,y)}{f_{X}(x)}=\\frac{2}{2x}=\\frac{1}{x} \\quad \\text{for } 0 \\leq y \\leq x,\\ \\text{and } 0 \\text{ otherwise.}\n$$\nTherefore $Y\\mid X=x$ is uniform on $[0,x]$, and the conditional expectation is\n$$\n\\operatorname{E}[Y\\mid X=x]=\\int_{0}^{x} y \\, f_{Y\\mid X}(y\\mid x)\\, dy=\\int_{0}^{x} y \\cdot \\frac{1}{x}\\, dy=\\frac{1}{x}\\cdot \\frac{x^{2}}{2}=\\frac{x}{2},\n$$\nvalid for $x\\in[0,1]$.", "answer": "$$\\boxed{\\frac{x}{2}}$$", "id": "1291258"}, {"introduction": "Conditional distributions are the engine that drives many stochastic processes. In this exercise, we examine the Ehrenfest urn model, a cornerstone for understanding diffusion and equilibrium. You will apply your knowledge of conditional probability to \"look back in time\" within a Markov chain, discovering a profound property known as time-reversibility and seeing how conditioning governs the system's dynamics from one step to the next. [@problem_id:1291292]", "problem": "Consider a simplified model for particle diffusion known as the Ehrenfest urn model. The system consists of two chambers, Chamber A and Chamber B, containing a total of $N$ distinguishable particles. At each discrete time step, one particle is chosen uniformly at random from the total population of $N$ particles and is moved to the opposite chamber.\n\nLet $X_t$ denote the number of particles in Chamber A at time step $t$. The process can be modeled as a discrete-time Markov chain with state space $\\{0, 1, \\dots, N\\}$. Assume the system has been operating for a sufficiently long time to have reached its stationary distribution.\n\nGiven that at a particular time step $n$, Chamber A is observed to contain $k$ particles (where $0 \\le k \\le N$), determine the conditional probability that at the previous time step, $n-1$, Chamber A contained $j$ particles. Your answer should be a single closed-form analytic expression in terms of $N$, $j$, and $k$. You may use the Kronecker delta, $\\delta_{a,b}$, which is 1 if $a=b$ and 0 otherwise.", "solution": "Let $X_{t}$ be the number of particles in Chamber A at time $t$. In the Ehrenfest urn model, given $X_{t}=i$, one particle is chosen uniformly from the $N$ distinguishable particles and moved to the opposite chamber. Therefore the one-step transition probabilities of the Markov chain are\n$$\nP_{i,i+1}=\\frac{N-i}{N},\\qquad P_{i,i-1}=\\frac{i}{N},\\qquad P_{i,j}=0\\ \\text{otherwise}.\n$$\nThe stationary distribution is the binomial law with parameter $\\frac{1}{2}$:\n$$\n\\pi_{i}=\\mathbb{P}(X=i)=2^{-N}\\binom{N}{i},\\qquad i\\in\\{0,1,\\dots,N\\}.\n$$\nWe want the conditional probability at stationarity\n$$\n\\mathbb{P}(X_{n-1}=j\\mid X_{n}=k)=\\frac{\\mathbb{P}(X_{n}=k\\mid X_{n-1}=j)\\,\\mathbb{P}(X_{n-1}=j)}{\\mathbb{P}(X_{n}=k)}=\\frac{P_{j,k}\\,\\pi_{j}}{\\pi_{k}}.\n$$\nUsing detailed balance for this reversible chain, we have\n$$\n\\pi_{j}P_{j,k}=\\pi_{k}P_{k,j},\n$$\nwhich can be verified explicitly, for example for adjacent states:\n$$\n\\pi_{i}P_{i,i+1}=2^{-N}\\binom{N}{i}\\frac{N-i}{N}=2^{-N}\\binom{N}{i+1}\\frac{i+1}{N}=\\pi_{i+1}P_{i+1,i},\n$$\nand similarly for $i$ and $i-1$. Hence\n$$\n\\mathbb{P}(X_{n-1}=j\\mid X_{n}=k)=P_{k,j}.\n$$\nSubstituting the explicit transition probabilities from state $k$ gives\n$$\n\\mathbb{P}(X_{n-1}=j\\mid X_{n}=k)=\\frac{k}{N}\\,\\delta_{j,k-1}+\\frac{N-k}{N}\\,\\delta_{j,k+1}.\n$$\nThis also correctly handles the boundary cases $k=0$ and $k=N$.", "answer": "$$\\boxed{\\frac{k}{N}\\,\\delta_{j,k-1}+\\frac{N-k}{N}\\,\\delta_{j,k+1}}$$", "id": "1291292"}]}