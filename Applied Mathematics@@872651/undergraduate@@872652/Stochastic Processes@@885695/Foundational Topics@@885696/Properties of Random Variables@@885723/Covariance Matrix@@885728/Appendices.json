{"hands_on_practices": [{"introduction": "The first step to mastering the covariance matrix is learning to build one from the ground up. This exercise guides you through the fundamental process using a simple pair of discrete random variables. By calculating the expected values, variances, and covariance from a given joint probability distribution, you will solidify your understanding of the definitions that underpin this crucial statistical tool [@problem_id:1294505].", "problem": "Consider a simple game involving two discrete random variables, $X$ and $Y$. The variable $X$ can take values from the set $\\{0, 1\\}$, and the variable $Y$ can take values from the set $\\{0, 1, 2\\}$. The joint probability mass function, $p(x, y) = P(X=x, Y=y)$, for these two variables is defined as follows:\n\n$p(0, 0) = \\frac{1}{10}$\n$p(0, 1) = \\frac{2}{10}$\n$p(0, 2) = 0$\n$p(1, 0) = \\frac{3}{10}$\n$p(1, 1) = \\frac{1}{10}$\n$p(1, 2) = \\frac{3}{10}$\n\nYour task is to compute the covariance matrix for the random vector $(X, Y)$. The covariance matrix, $K$, is a $2 \\times 2$ matrix where the element $K_{ij}$ is the covariance of the $i$-th and $j$-th random variables. For the vector $(X, Y)$, this is given by:\n$$\nK = \\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X,Y) \\\\ \\text{Cov}(Y,X) & \\text{Var}(Y) \\end{pmatrix}\n$$\nProvide the resulting $2 \\times 2$ matrix.", "solution": "To find the covariance matrix, we need to calculate three quantities: the variance of $X$, $\\text{Var}(X)$; the variance of $Y$, $\\text{Var}(Y)$; and the covariance between $X$ and $Y$, $\\text{Cov}(X,Y)$. Recall that $\\text{Cov}(Y,X) = \\text{Cov}(X,Y)$.\n\nFirst, let's find the marginal probability mass functions for $X$ and $Y$.\nThe marginal probability for $X$ is $P(X=x) = \\sum_{y} p(x,y)$.\n$P(X=0) = p(0,0) + p(0,1) + p(0,2) = \\frac{1}{10} + \\frac{2}{10} + 0 = \\frac{3}{10}$.\n$P(X=1) = p(1,0) + p(1,1) + p(1,2) = \\frac{3}{10} + \\frac{1}{10} + \\frac{3}{10} = \\frac{7}{10}$.\n\nThe marginal probability for $Y$ is $P(Y=y) = \\sum_{x} p(x,y)$.\n$P(Y=0) = p(0,0) + p(1,0) = \\frac{1}{10} + \\frac{3}{10} = \\frac{4}{10} = \\frac{2}{5}$.\n$P(Y=1) = p(0,1) + p(1,1) = \\frac{2}{10} + \\frac{1}{10} = \\frac{3}{10}$.\n$P(Y=2) = p(0,2) + p(1,2) = 0 + \\frac{3}{10} = \\frac{3}{10}$.\n\nNext, we compute the expected values (means) of $X$ and $Y$.\n$E[X] = \\sum_{x} x P(X=x) = (0)P(X=0) + (1)P(X=1) = 0 \\cdot \\frac{3}{10} + 1 \\cdot \\frac{7}{10} = \\frac{7}{10}$.\n$E[Y] = \\sum_{y} y P(Y=y) = (0)P(Y=0) + (1)P(Y=1) + (2)P(Y=2) = 0 \\cdot \\frac{4}{10} + 1 \\cdot \\frac{3}{10} + 2 \\cdot \\frac{3}{10} = \\frac{3}{10} + \\frac{6}{10} = \\frac{9}{10}$.\n\nNow we calculate the variances. The variance of a random variable $Z$ is given by $\\text{Var}(Z) = E[Z^2] - (E[Z])^2$. We need to compute $E[X^2]$ and $E[Y^2]$.\n$E[X^2] = \\sum_{x} x^2 P(X=x) = (0^2)P(X=0) + (1^2)P(X=1) = 0 \\cdot \\frac{3}{10} + 1 \\cdot \\frac{7}{10} = \\frac{7}{10}$.\n$\\text{Var}(X) = E[X^2] - (E[X])^2 = \\frac{7}{10} - \\left(\\frac{7}{10}\\right)^2 = \\frac{7}{10} - \\frac{49}{100} = \\frac{70}{100} - \\frac{49}{100} = \\frac{21}{100}$.\n\n$E[Y^2] = \\sum_{y} y^2 P(Y=y) = (0^2)P(Y=0) + (1^2)P(Y=1) + (2^2)P(Y=2) = 0 \\cdot \\frac{4}{10} + 1 \\cdot \\frac{3}{10} + 4 \\cdot \\frac{3}{10} = \\frac{3}{10} + \\frac{12}{10} = \\frac{15}{10} = \\frac{3}{2}$.\n$\\text{Var}(Y) = E[Y^2] - (E[Y])^2 = \\frac{15}{10} - \\left(\\frac{9}{10}\\right)^2 = \\frac{15}{10} - \\frac{81}{100} = \\frac{150}{100} - \\frac{81}{100} = \\frac{69}{100}$.\n\nFinally, we compute the covariance, $\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$. We first need to find $E[XY]$.\n$E[XY] = \\sum_{x,y} xy \\cdot p(x,y)$. The non-zero terms are:\n$(1)(1)p(1,1) = 1 \\cdot \\frac{1}{10} = \\frac{1}{10}$.\n$(1)(2)p(1,2) = 2 \\cdot \\frac{3}{10} = \\frac{6}{10}$.\nSo, $E[XY] = \\frac{1}{10} + \\frac{6}{10} = \\frac{7}{10}$.\n\nNow we can calculate the covariance:\n$\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = \\frac{7}{10} - \\left(\\frac{7}{10}\\right)\\left(\\frac{9}{10}\\right) = \\frac{7}{10} - \\frac{63}{100} = \\frac{70}{100} - \\frac{63}{100} = \\frac{7}{100}$.\n\nWe now assemble the covariance matrix:\n$$\nK = \\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X,Y) \\\\ \\text{Cov}(Y,X) & \\text{Var}(Y) \\end{pmatrix} = \\begin{pmatrix} \\frac{21}{100} & \\frac{7}{100} \\\\ \\frac{7}{100} & \\frac{69}{100} \\end{pmatrix}\n$$", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{21}{100} & \\frac{7}{100} \\\\ \\frac{7}{100} & \\frac{69}{100} \\end{pmatrix}}$$", "id": "1294505"}, {"introduction": "Moving from discrete to continuous variables, this practice explores a special but vital scenario: the relationship between statistically independent variables. You will calculate the covariance matrix for two independent variables that are uniformly distributed, a common assumption in modeling and simulation. This exercise demonstrates the key principle that independence implies zero covariance, leading to a diagonal covariance matrix [@problem_id:1294509].", "problem": "A software engineer is testing a new pseudo-random number generator. The generator is designed to produce pairs of floating-point numbers, $(X, Y)$, which are intended to be statistically independent. As a modeling assumption, both $X$ and $Y$ are treated as continuous random variables that are uniformly distributed over the interval $[0, 1]$.\n\nLet the random vector representing the output be $\\mathbf{V} = \\begin{pmatrix} X \\\\ Y \\end{pmatrix}$. Assuming the design specifications (independence and uniform distribution on $[0,1]$ for both variables) are perfectly met, calculate the theoretical covariance matrix of $\\mathbf{V}$.", "solution": "We denote the covariance matrix of the random vector $\\mathbf{V} = \\begin{pmatrix} X \\\\ Y \\end{pmatrix}$ by $\\text{Cov}(\\mathbf{V})$. By definition,\n$$\n\\text{Cov}(\\mathbf{V})=\\begin{pmatrix}\n\\text{Var}(X) & \\text{Cov}(X,Y) \\\\\n\\text{Cov}(Y,X) & \\text{Var}(Y)\n\\end{pmatrix}.\n$$\nFor independent $X$ and $Y$, the covariance is\n$$\n\\text{Cov}(X,Y)=E[XY]-E[X]E[Y],\n$$\nand independence implies $E[XY]=E[X]E[Y]$, hence $\\text{Cov}(X,Y)=0$. Symmetry gives $\\text{Cov}(Y,X)=0$.\n\nTo compute the variances, use $\\text{Var}(X)=E[X^{2}]-(E[X])^{2}$ for $X \\sim \\text{Uniform}[0,1]$. First,\n$$\nE[X]=\\int_{0}^{1} x \\, dx=\\left[\\frac{x^{2}}{2}\\right]_{0}^{1}=\\frac{1}{2},\n$$\nand\n$$\nE[X^{2}]=\\int_{0}^{1} x^{2} \\, dx=\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=\\frac{1}{3}.\n$$\nTherefore,\n$$\n\\text{Var}(X)=E[X^{2}]-(E[X])^{2}=\\frac{1}{3}-\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{3}-\\frac{1}{4}=\\frac{1}{12}.\n$$\nBy identical reasoning, $\\text{Var}(Y)=\\frac{1}{12}$.\n\nPutting these results together,\n$$\n\\text{Cov}(\\mathbf{V})=\\begin{pmatrix}\n\\frac{1}{12} & 0 \\\\\n0 & \\frac{1}{12}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{12} & 0 \\\\ 0 & \\frac{1}{12}\\end{pmatrix}}$$", "id": "1294509"}, {"introduction": "This practice elevates our understanding from describing relationships to actively using them for optimization, a common task in signal processing and finance. Here, you will use the components of a given covariance matrix to find a coefficient $\\alpha$ that minimizes the variance of a new signal, $Z = Y - \\alpha X$. This problem illustrates the powerful concept of creating an uncorrelated signal, which forms the basis for techniques like linear regression and noise cancellation [@problem_id:1294483].", "problem": "In a signal processing application, we are working with a pair of correlated random signals represented by a two-dimensional random vector $W = (X, Y)^T$. Both signals $X$ and $Y$ have been centered, meaning their expected values are zero, i.e., $E[X] = E[Y] = 0$. The statistical relationship between these signals is captured by their covariance matrix, $K_W$, which is given by:\n$$\nK_W = \\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X, Y) \\\\ \\text{Cov}(Y, X) & \\text{Var}(Y) \\end{pmatrix} = \\begin{pmatrix} 9 & 3 \\\\ 3 & 5 \\end{pmatrix}\n$$\nTo filter the signal $Y$, we construct a new signal $Z = Y - \\alpha X$, where $\\alpha$ is a real-valued scalar coefficient. The goal is to choose $\\alpha$ such that the power of the filtered signal $Z$, which is equivalent to its variance, is minimized.\n\nLet $\\alpha_0$ be the specific value of $\\alpha$ that minimizes $\\text{Var}(Y - \\alpha X)$. Now, consider a new random vector $V = (X, Y - \\alpha_0 X)^T$. What is the determinant of the covariance matrix of this new vector $V$? Your final answer should be a single real number.", "solution": "We are given a centered random vector $W=(X,Y)^{T}$ with covariance matrix\n$$\nK_{W}=\\begin{pmatrix} \\text{Var}(X) & \\text{Cov}(X,Y) \\\\ \\text{Cov}(Y,X) & \\text{Var}(Y) \\end{pmatrix}=\\begin{pmatrix} 9 & 3 \\\\ 3 & 5 \\end{pmatrix}.\n$$\nDefine $Z=Y-\\alpha X$. Using the variance formula for linear combinations, namely $\\text{Var}(aU+bV)=a^{2}\\text{Var}(U)+b^{2}\\text{Var}(V)+2ab\\,\\text{Cov}(U,V)$, we obtain\n$$\n\\text{Var}(Y-\\alpha X)=\\text{Var}(Y)-2\\alpha\\,\\text{Cov}(X,Y)+\\alpha^{2}\\text{Var}(X).\n$$\nTo minimize this quadratic in $\\alpha$, differentiate with respect to $\\alpha$ and set the derivative to zero:\n$$\n\\frac{d}{d\\alpha}\\text{Var}(Y-\\alpha X)=-2\\,\\text{Cov}(X,Y)+2\\alpha\\,\\text{Var}(X)=0,\n$$\nwhich yields\n$$\n\\alpha_{0}=\\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}=\\frac{3}{9}=\\frac{1}{3}.\n$$\nThe second derivative equals $2\\,\\text{Var}(X)>0$, so this is indeed the minimizer. Substituting $\\alpha_{0}$ back into the variance gives the well-known residual variance formula\n$$\n\\text{Var}(Y-\\alpha_{0}X)=\\text{Var}(Y)-\\frac{\\text{Cov}(X,Y)^{2}}{\\text{Var}(X)}=5-\\frac{3^{2}}{9}=5-1=4.\n$$\nMoreover, the minimizing choice makes the residual uncorrelated with $X$:\n$$\n\\text{Cov}\\bigl(X,\\,Y-\\alpha_{0}X\\bigr)=\\text{Cov}(X,Y)-\\alpha_{0}\\text{Var}(X)=3-\\frac{1}{3}\\cdot 9=0.\n$$\nTherefore, for $V=(X,\\,Y-\\alpha_{0}X)^{T}$, the covariance matrix is diagonal,\n$$\nK_{V}=\\begin{pmatrix} \\text{Var}(X) & 0 \\\\ 0 & \\text{Var}(Y-\\alpha_{0}X) \\end{pmatrix}=\\begin{pmatrix} 9 & 0 \\\\ 0 & 4 \\end{pmatrix},\n$$\nand its determinant is the product of the diagonal entries:\n$$\n\\det(K_{V})=9\\cdot 4=36.\n$$\nAs a consistency check, note that $V=M W$ with $M=\\begin{pmatrix}1&0\\\\ -\\alpha_{0}&1\\end{pmatrix}$, so $K_{V}=M K_{W} M^{T}$ and thus $\\det(K_{V})=\\det(M)^{2}\\det(K_{W})=\\det(K_{W})=9\\cdot 5-3^{2}=36$, in agreement with the direct computation.", "answer": "$$\\boxed{36}$$", "id": "1294483"}]}