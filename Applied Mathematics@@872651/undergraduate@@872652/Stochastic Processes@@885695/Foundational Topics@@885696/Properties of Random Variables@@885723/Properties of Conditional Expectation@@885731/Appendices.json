{"hands_on_practices": [{"introduction": "We begin our practice with a classic scenario involving discrete random variables: the roll of two dice. This exercise [@problem_id:1381964] provides a concrete foundation for understanding conditional expectation by requiring us to manually restrict the sample space based on new information. By calculating the expected sum given the maximum value, you'll practice applying the fundamental definition of conditional expectation in a clear and intuitive setting.", "problem": "Let $X_1$ and $X_2$ be the outcomes of two independent rolls of a fair six-sided die. The set of possible outcomes for each die is $\\{1, 2, 3, 4, 5, 6\\}$, with each outcome having a probability of $1/6$.\n\nDefine two new random variables: the sum $S = X_1 + X_2$ and the maximum value $M = \\max(X_1, X_2)$.\n\nDetermine the conditional expectation of the sum $S$ given that the maximum value observed is $k$, where $k$ is an integer such that $1 \\le k \\le 6$. Express your answer as a function of $k$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent and uniformly distributed on $\\{1,2,3,4,5,6\\}$. Define $S = X_{1} + X_{2}$ and $M = \\max(X_{1}, X_{2})$. We seek $\\mathbb{E}[S \\mid M = k]$ for $1 \\leq k \\leq 6$.\n\nFirst, characterize the event $\\{M = k\\}$. It consists of all ordered pairs $(i,j)$ with $1 \\leq i,j \\leq k$ such that at least one coordinate equals $k$. The total number of such pairs is\n$$\n|\\{(i,j) : \\max(i,j) = k\\}| = k^{2} - (k-1)^{2} = 2k - 1.\n$$\nSince each ordered pair has probability $\\frac{1}{36}$ and conditioning on $\\{M=k\\}$ restricts to these $2k-1$ equally likely pairs, the conditional distribution on this set is uniform.\n\nTherefore,\n$$\n\\mathbb{E}[S \\mid M = k] = \\frac{1}{2k - 1} \\sum_{\\max(i,j)=k} (i + j).\n$$\nPartition the set of pairs $(i,j)$ where $\\max(i,j)=k$ into the disjoint union of $B = \\{(k,j) : 1 \\leq j \\leq k\\}$ and $C = \\{(i,k) : 1 \\leq i \\leq k-1\\}$. Then\n$$\n\\sum_{(i,j)\\in B} (i + j) = \\sum_{j=1}^{k} (k + j) = k^{2} + \\frac{k(k+1)}{2},\n$$\nand\n$$\n\\sum_{(i,j)\\in C} (i + j) = \\sum_{i=1}^{k-1} (i + k) = \\frac{(k-1)k}{2} + k(k-1) = \\frac{3k(k-1)}{2}.\n$$\nHence the total sum over $\\{(i,j): \\max(i,j)=k\\}$ is\n$$\n\\left(k^{2} + \\frac{k(k+1)}{2}\\right) + \\left(\\frac{3k(k-1)}{2}\\right) = k^{2} + \\frac{k(k+1) + 3k(k-1)}{2} = k^{2} + \\frac{k^2+k+3k^2-3k}{2} = k^{2} + \\frac{4k^2-2k}{2} = k^2 + 2k^2 - k = 3k^{2} - k.\n$$\nDividing by the number of equally likely pairs $2k - 1$ gives\n$$\n\\mathbb{E}[S \\mid M = k] = \\frac{3k^{2} - k}{2k - 1}.\n$$\nThis holds for every integer $k$ with $1 \\leq k \\leq 6$.", "answer": "$$\\boxed{\\frac{3k^{2} - k}{2k - 1}}$$", "id": "1381964"}, {"introduction": "Having mastered the discrete case, we now transition to the continuous domain with the 'broken stick' problem [@problem_id:1381953]. Here, we explore the lengths of segments created by random breaks, a scenario that requires moving from sums to integrals. This practice is essential for learning how to work with conditional probability density functions and provides an elegant, intuitive result about expected lengths.", "problem": "A straight, rigid rod of length $L$ is placed along the positive x-axis, with one end at the origin ($x=0$) and the other at $x=L$. Two points along the rod are selected for breaking it. The locations of these two breaks are determined by two random variables, $X_1$ and $X_2$, which are drawn independently from a uniform distribution on the interval $[0, L]$.\n\nThese two breaks divide the rod into three smaller segments. Let $U$ be the random variable representing the location of the break point closer to the origin, and let $V$ be the random variable for the location of the break point farther from the origin. Consequently, we have $U = \\min(X_1, X_2)$ and $V = \\max(X_1, X_2)$. The length of the middle segment is thus given by the random variable $M = V - U$.\n\nYour task is to determine the conditional expectation of the length of the middle segment, $M$, given that the first break (the one closer to the origin) occurs at a specific position $u$, where $0  u  L$.\n\nExpress your answer as a symbolic expression in terms of $L$ and $u$.", "solution": "Let $X_{1}$ and $X_{2}$ be independent and identically distributed with uniform density on $[0,L]$, so $f_{X_{1},X_{2}}(x_{1},x_{2}) = L^{-2}$ on $[0,L]^{2}$. Define the order statistics $U=\\min(X_{1},X_{2})$ and $V=\\max(X_{1},X_{2})$. The joint density of $(U,V)$ for $0uvL$ is the standard order-statistics result\n$$\nf_{U,V}(u,v) = \\frac{2}{L^{2}}, \\quad 0uvL.\n$$\nThe marginal density of $U$ is obtained by integrating over $v$:\n$$\nf_{U}(u) = \\int_{v=u}^{L} \\frac{2}{L^{2}} \\, dv = \\frac{2(L-u)}{L^{2}}, \\quad 0uL.\n$$\nHence, the conditional density of $V$ given $U=u$ is\n$$\nf_{V\\mid U}(v\\mid u) = \\frac{f_{U,V}(u,v)}{f_{U}(u)} = \\frac{1}{L-u}, \\quad uvL,\n$$\nso $V\\mid U=u$ is uniform on $(u,L)$. The middle segment length is $M=V-U$, so conditioned on $U=u$ we have $M=V-u$. Therefore,\n$$\n\\mathbb{E}[M\\mid U=u] = \\int_{u}^{L} (v-u)\\,\\frac{1}{L-u}\\,dv\n= \\frac{1}{L-u}\\left[\\frac{(v-u)^{2}}{2}\\right]_{v=u}^{v=L}\n= \\frac{1}{L-u}\\cdot \\frac{(L-u)^{2}}{2}\n= \\frac{L-u}{2}.\n$$\nThis holds for $0uL$.", "answer": "$$\\boxed{\\frac{L-u}{2}}$$", "id": "1381953"}, {"introduction": "This final practice problem [@problem_id:1327108] showcases the power of conditional expectation in a real-world application from signal processing. You are tasked with finding the best possible estimate of a signal's amplitude given a measurement of its power, a common challenge in receiver design. This exercise demonstrates how conditional expectation serves as an optimal estimator and reveals a fascinating connection between Gaussian variables and the hyperbolic tangent function.", "problem": "In a digital communication system, the amplitude of a received signal pulse, denoted by the random variable $V$, is modeled by a Gaussian distribution. This signal has a non-zero Direct Current (DC) component $\\mu$ and is corrupted by additive Gaussian noise with zero mean, resulting in a total signal standard deviation of $\\sigma$. Thus, the probability density function of $V$ is given by $f_V(v) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(v-\\mu)^2}{2\\sigma^2}\\right)$, where $\\mu$ and $\\sigma$ are positive real constants.\n\nA non-linear detector in the receiver circuit measures a quantity $P$ which is equal to the square of the signal amplitude, i.e., $P = V^2$. For optimal signal reconstruction, it is necessary to compute the best estimate of the original amplitude $V$ given this measurement $P$. In the sense of minimizing the mean squared error, this best estimate is the conditional expectation of $V$ given $P$.\n\nDetermine the conditional expectation $E[V | P]$. Your answer should be a function of the measured quantity $P$, the DC component $\\mu$, and the standard deviation $\\sigma$.", "solution": "Let $V \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ with density $f_{V}(v)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(v-\\mu)^{2}}{2\\sigma^{2}}\\right)$, and let $P=V^{2}$. We seek $E[V \\mid P]$, which must be a measurable function of $P$. For $p0$, the mapping $h(v)=v^{2}$ has two preimages $v_{1}=\\sqrt{p}$ and $v_{2}=-\\sqrt{p}$ with derivative $h'(v)=2v$, so $|h'(v_{1})|=|h'(v_{2})|=2\\sqrt{p}$. The conditional distribution of $V$ given $P=p$ is discrete on $\\{\\sqrt{p},-\\sqrt{p}\\}$ with weights proportional to $f_{V}(v_{i})/|h'(v_{i})|$. Therefore,\n$$\nE[V \\mid P=p]\n=\\frac{\\sqrt{p}\\,\\frac{f_{V}(\\sqrt{p})}{2\\sqrt{p}}+(-\\sqrt{p})\\,\\frac{f_{V}(-\\sqrt{p})}{2\\sqrt{p}}}{\\frac{f_{V}(\\sqrt{p})}{2\\sqrt{p}}+\\frac{f_{V}(-\\sqrt{p})}{2\\sqrt{p}}}\n=\\sqrt{p}\\,\\frac{f_{V}(\\sqrt{p})-f_{V}(-\\sqrt{p})}{f_{V}(\\sqrt{p})+f_{V}(-\\sqrt{p})}.\n$$\nUsing the Gaussian density,\n$$\nf_{V}(\\sqrt{p})=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(\\sqrt{p}-\\mu)^{2}}{2\\sigma^{2}}\\right),\\quad\nf_{V}(-\\sqrt{p})=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{(\\sqrt{p}+\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nLet\n$$\nA=\\exp\\!\\left(-\\frac{(\\sqrt{p}-\\mu)^{2}}{2\\sigma^{2}}\\right),\\quad\nB=\\exp\\!\\left(-\\frac{(\\sqrt{p}+\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nThen\n$$\nA=\\exp\\!\\left(-\\frac{p+\\mu^{2}}{2\\sigma^{2}}\\right)\\exp\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right),\\quad\nB=\\exp\\!\\left(-\\frac{p+\\mu^{2}}{2\\sigma^{2}}\\right)\\exp\\!\\left(-\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right),\n$$\nso\n$$\n\\frac{A-B}{A+B}\n=\\frac{\\exp\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)-\\exp\\!\\left(-\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)}{\\exp\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)+\\exp\\!\\left(-\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right)}\n=\\tanh\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right).\n$$\nTherefore, for $p0$,\n$$\nE[V \\mid P=p]=\\sqrt{p}\\,\\tanh\\!\\left(\\frac{\\mu\\sqrt{p}}{\\sigma^{2}}\\right).\n$$\nAt $p=0$, the only preimage is $v=0$, hence $E[V \\mid P=0]=0$, which agrees with the limit of the above expression as $p \\to 0^{+}$. Thus, as a function of the measured $P$,\n$$\nE[V \\mid P]=\\sqrt{P}\\,\\tanh\\!\\left(\\frac{\\mu\\sqrt{P}}{\\sigma^{2}}\\right).\n$$", "answer": "$$\\boxed{\\sqrt{P}\\,\\tanh\\!\\left(\\frac{\\mu\\sqrt{P}}{\\sigma^{2}}\\right)}$$", "id": "1327108"}]}