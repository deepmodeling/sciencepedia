{"hands_on_practices": [{"introduction": "The Moment Generating Function (MGF) is not just a theoretical curiosity; it's a powerful computational tool. One of its most fundamental applications is generating the moments of a random variable. This first exercise provides hands-on practice in this core skill, showing how to derive the mean and variance directly from the MGF's derivatives. Furthermore, it illustrates how these results can be used to analyze linear transformations of random variables, a common scenario in many statistical applications [@problem_id:1937144].", "problem": "Let $Y$ be a continuous random variable. The statistical properties of $Y$ are fully described by its Moment Generating Function (MGF), which is given by the expression:\n$$M_Y(t) = \\exp(3t + 8t^2)$$\nwhere $t$ is a real variable for which the function is defined.\n\nConsider a new random variable $X$ that is defined as a linear transformation of $Y$:\n$$X = 5 - 2Y$$\nCalculate the variance of the random variable $X$.", "solution": "We are given the moment generating function of $Y$ as $M_{Y}(t)=\\exp(3t+8t^{2})$. By definition of the MGF, the first and second moments of $Y$ are obtained from derivatives at $t=0$:\n- $\\mathbb{E}[Y]=M_{Y}'(0)$,\n- $\\mathbb{E}[Y^{2}]=M_{Y}''(0)$,\nand the variance is $\\operatorname{Var}(Y)=\\mathbb{E}[Y^{2}]-\\left(\\mathbb{E}[Y]\\right)^{2}$.\n\nDifferentiate $M_{Y}(t)$ using the chain rule. Let $g(t)=\\exp(3t+8t^{2})$. Then\n$$\nM_{Y}'(t)=\\frac{d}{dt}\\exp(3t+8t^{2})=(3+16t)\\exp(3t+8t^{2})=(3+16t)g(t).\n$$\nEvaluating at $t=0$ gives\n$$\nM_{Y}'(0)=(3+0)\\exp(0)=3,\n$$\nso $\\mathbb{E}[Y]=3$.\n\nFor the second derivative, use the product rule with $f(t)=3+16t$ and $g(t)=\\exp(3t+8t^{2})$:\n$$\nM_{Y}''(t)=f'(t)g(t)+f(t)g'(t).\n$$\nWe have $f'(t)=16$ and $g'(t)=(3+16t)g(t)$ by the chain rule, hence\n$$\nM_{Y}''(t)=16\\,g(t)+(3+16t)(3+16t)\\,g(t)=\\left(16+(3+16t)^{2}\\right)\\exp(3t+8t^{2}).\n$$\nEvaluating at $t=0$ yields\n$$\nM_{Y}''(0)=\\left(16+3^{2}\\right)\\exp(0)=25,\n$$\nso $\\mathbb{E}[Y^{2}]=25$. Therefore,\n$$\n\\operatorname{Var}(Y)=\\mathbb{E}[Y^{2}]-\\left(\\mathbb{E}[Y]\\right)^{2}=25-3^{2}=16.\n$$\n\nNow consider $X=5-2Y$. For a linear transformation $X=a+bY$, the variance satisfies $\\operatorname{Var}(X)=b^{2}\\operatorname{Var}(Y)$. Here $b=-2$, so\n$$\n\\operatorname{Var}(X)=(-2)^{2}\\operatorname{Var}(Y)=4\\cdot 16=64.\n$$", "answer": "$$\\boxed{64}$$", "id": "1937144"}, {"introduction": "Building upon the skill of calculating variance, we now explore another key feature of MGFs: their elegant handling of sums of independent random variables. Instead of wrestling with complex convolutions to find the distribution of a sum, we can simply multiply the individual MGFs. This practice problem demonstrates how to find the variance of a sum of two independent variables by working with their MGFs, a technique frequently used in areas like signal processing and financial modeling [@problem_id:1376258]. Note that the problem presents a hypothetical scenario involving bit errors to illustrate the underlying mathematical principle.", "problem": "In a simplified model of a digital communication channel, data is transmitted in packets of two types, Type A and Type B. Let the random variable $X$ represent the number of bit errors found in a randomly selected packet of Type A, and let $Y$ be the number of bit errors in a randomly selected packet of Type B. The statistical properties of these errors are captured by their Moment Generating Functions (MGFs), which are given as follows:\n\nThe MGF of $X$ is $M_X(t) = 0.1 + 0.2e^t + 0.4e^{2t} + 0.3e^{3t}$.\nThe MGF of $Y$ is $M_Y(t) = 0.6e^{2t} + 0.4e^{4t}$.\n\nAssume that the number of errors in a Type A packet is statistically independent of the number of errors in a Type B packet. A single transmission session consists of sending exactly one packet of Type A and one packet of Type B.\n\nCalculate the variance of the total number of bit errors in a single transmission session.", "solution": "Let $S=X+Y$ be the total number of bit errors in one session. By independence, $\\operatorname{Var}(S)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)$. From MGFs, for any random variable $Z$ with MGF $M_{Z}(t)$, we use $M_{Z}'(0)=\\mathbb{E}[Z]$ and $M_{Z}''(0)=\\mathbb{E}[Z^{2}]$, hence $\\operatorname{Var}(Z)=M_{Z}''(0)-\\left(M_{Z}'(0)\\right)^{2}$ since $M_{Z}(0)=1$.\n\nFor $X$ with $M_{X}(t)=0.1+0.2e^{t}+0.4e^{2t}+0.3e^{3t}$,\n$$\nM_{X}'(t)=0.2e^{t}+0.8e^{2t}+0.9e^{3t},\\quad M_{X}'(0)=0.2+0.8+0.9=1.9,\n$$\n$$\nM_{X}''(t)=0.2e^{t}+1.6e^{2t}+2.7e^{3t},\\quad M_{X}''(0)=0.2+1.6+2.7=4.5.\n$$\nTherefore,\n$$\n\\operatorname{Var}(X)=4.5-(1.9)^{2}=4.5-3.61=0.89.\n$$\n\nFor $Y$ with $M_{Y}(t)=0.6e^{2t}+0.4e^{4t}$,\n$$\nM_{Y}'(t)=1.2e^{2t}+1.6e^{4t},\\quad M_{Y}'(0)=1.2+1.6=2.8,\n$$\n$$\nM_{Y}''(t)=2.4e^{2t}+6.4e^{4t},\\quad M_{Y}''(0)=2.4+6.4=8.8.\n$$\nTherefore,\n$$\n\\operatorname{Var}(Y)=8.8-(2.8)^{2}=8.8-7.84=0.96.\n$$\n\nBy independence,\n$$\n\\operatorname{Var}(S)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)=0.89+0.96=1.85.\n$$", "answer": "$$\\boxed{1.85}$$", "id": "1376258"}, {"introduction": "Perhaps the most powerful property of the MGF is its uniqueness: a specific MGF corresponds to exactly one probability distribution. This turns the MGF into a unique \"fingerprint\" for a random variable. This final practice exercise challenges you to use this property by matching a given MGF to its corresponding distribution family. Mastering this skill is crucial for identifying the nature of random variables that emerge from complex theoretical derivations [@problem_id:1937151].", "problem": "A continuous random variable $X$ has a moment generating function (MGF), denoted by $M_X(t)$, which is defined for all $t  1/2$. The specific form of the MGF for $X$ is given by:\n$$M_X(t) = (1 - 2t)^{-4}$$\nWhich of the following options correctly identifies the probability distribution of $X$ and its associated parameters?\n\nA. Exponential distribution with rate parameter $\\lambda = 1/2$.\n\nB. Normal distribution with mean $\\mu = 8$ and variance $\\sigma^2 = 16$.\n\nC. Gamma distribution with shape parameter $\\alpha = 4$ and scale parameter $\\beta = 1/2$.\n\nD. Chi-squared distribution with 4 degrees of freedom.\n\nE. Chi-squared distribution with 8 degrees of freedom.", "solution": "We compare the given moment generating function to known MGF forms.\n\n1) For a Gamma distribution with shape parameter $\\alpha$ and scale parameter $\\theta$, the MGF is\n$$\nM(t) = (1 - \\theta t)^{-\\alpha}, \\quad \\text{for } t  \\frac{1}{\\theta}.\n$$\nThe given $M_{X}(t) = (1 - 2t)^{-4}$ matches this form with $\\alpha = 4$ and $\\theta = 2$, since\n$$\n(1 - \\theta t)^{-\\alpha} = (1 - 2t)^{-4} \\implies \\theta = 2, \\ \\alpha = 4,\n$$\nand the stated domain $t  \\frac{1}{2}$ agrees with $t  \\frac{1}{\\theta}$.\n\n2) The chi-squared distribution with $\\nu$ degrees of freedom is a special case of the Gamma distribution with shape $\\alpha = \\nu/2$ and scale $\\theta = 2$. Its MGF is\n$$\nM(t) = (1 - 2t)^{-\\nu/2}, \\quad \\text{for } t  \\frac{1}{2}.\n$$\nComparing with $(1 - 2t)^{-4}$ gives\n$$\n-\\frac{\\nu}{2} = -4 \\implies \\nu = 8.\n$$\n\n3) We now evaluate the options:\n- A is incorrect because an exponential distribution has MGF $(1 - \\theta t)^{-1}$ (or $(1 - t/\\lambda)^{-1}$ under rate parameterization) with exponent $-1$, not $-4$.\n- B is incorrect because a normal distribution has MGF $\\exp(\\mu t + \\frac{1}{2}\\sigma^{2} t^{2})$, not a rational power form.\n- C is incorrect because it states scale $\\beta = \\frac{1}{2}$ (i.e., $\\theta = \\frac{1}{2}$), which would give $M(t) = (1 - \\frac{1}{2} t)^{-4}$, not $(1 - 2t)^{-4}$.\n- D is incorrect because $\\nu = 4$ would yield $(1 - 2t)^{-2}$.\n- E is correct because $\\nu = 8$ yields $(1 - 2t)^{-4}$.\n\nTherefore, the correct identification is the chi-squared distribution with $8$ degrees of freedom.", "answer": "$$\\boxed{E}$$", "id": "1937151"}]}