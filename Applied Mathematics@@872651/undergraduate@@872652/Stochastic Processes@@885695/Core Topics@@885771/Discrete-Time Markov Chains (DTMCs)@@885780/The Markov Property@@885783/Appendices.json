{"hands_on_practices": [{"introduction": "The essence of the Markov property is its 'memorylessness'â€”the future depends only on the present, not the past. This first exercise challenges you to apply this core definition to two fundamental processes: a running sum and a running maximum. By determining whether these processes are Markovian, you will build a solid intuition for what constitutes a valid 'state' and how it encapsulates all necessary information to predict the future [@problem_id:1342487].", "problem": "Let $X_1, X_2, X_3, \\dots$ be a sequence of independent and identically distributed (i.i.d.) continuous random variables. Based on this sequence, we can define several discrete-time stochastic processes. Consider the following two processes for $n \\ge 1$:\n\n1.  The Running Sum Process, $\\{S_n\\}_{n \\ge 1}$, where $S_n = \\sum_{i=1}^{n} X_i$.\n2.  The Running Maximum Process, $\\{M_n\\}_{n \\ge 1}$, where $M_n = \\max\\{X_1, X_2, \\dots, X_n\\}$.\n\nA stochastic process $\\{Z_n\\}$ is said to have the Markov property (or is a Markov process) if the conditional probability distribution of future states of the process, given the present state, depends only upon the current state and not on the sequence of events that preceded it. Formally, for any time step $n$ and any set of values $z_1, \\dots, z_{n+1}$, the following must hold:\n$P(Z_{n+1} \\le z_{n+1} \\mid Z_n = z_n, Z_{n-1}=z_{n-1}, \\dots, Z_1=z_1) = P(Z_{n+1} \\le z_{n+1} \\mid Z_n = z_n)$.\n\nWhich of the following statements about these two processes is correct?\n\nA. Only the process $\\{S_n\\}_{n \\ge 1}$ is a Markov process.\n\nB. Only the process $\\{M_n\\}_{n \\ge 1}$ is a Markov process.\n\nC. Both $\\{S_n\\}_{n \\ge 1}$ and $\\{M_n\\}_{n \\ge 1}$ are Markov processes.\n\nD. Neither $\\{S_n\\}_{n \\ge 1}$ nor $\\{M_n\\}_{n \\ge 1}$ is a Markov process.", "solution": "Let $\\{X_{n}\\}_{n \\ge 1}$ be i.i.d. continuous with cumulative distribution function $F$. Define $S_{n}=\\sum_{i=1}^{n}X_{i}$ and $M_{n}=\\max\\{X_{1},\\dots,X_{n}\\}$. Let $\\mathcal{F}_{n}=\\sigma(X_{1},\\dots,X_{n})$.\n\nFor the running sum process $\\{S_{n}\\}$, we have for any real $y$,\n$$\n\\begin{aligned}\nP(S_{n+1}\\le y\\mid \\mathcal{F}_{n})\n=P(S_{n}+X_{n+1}\\le y\\mid \\mathcal{F}_{n})\\\\\n=P(X_{n+1}\\le y-S_{n}\\mid \\mathcal{F}_{n})\\\\\n=P(X_{n+1}\\le y-S_{n}) \\quad \\text{(independence of $X_{n+1}$ and $\\mathcal{F}_{n}$)}\\\\\n=F(y-S_{n}).\n\\end{aligned}\n$$\nThe right-hand side is a measurable function of $S_{n}$ only, so\n$$\nP(S_{n+1}\\le y\\mid S_{n})=F(y-S_{n}).\n$$\nThus, the conditional distribution of $S_{n+1}$ given the full past depends only on $S_{n}$, and $\\{S_{n}\\}$ is a Markov process.\n\nFor the running maximum process $\\{M_{n}\\}$, observe that $M_{n+1}=\\max(M_{n},X_{n+1})$. For any real $y$,\n$$\n\\begin{aligned}\nP(M_{n+1}\\le y\\mid \\mathcal{F}_{n})\n=P(\\max(M_{n},X_{n+1})\\le y\\mid \\mathcal{F}_{n})\\\\\n=\\mathbf{1}_{\\{M_{n}\\le y\\}}\\,P(X_{n+1}\\le y\\mid \\mathcal{F}_{n})\\\\\n=\\mathbf{1}_{\\{M_{n}\\le y\\}}\\,F(y) \\quad \\text{(independence of $X_{n+1}$ and $\\mathcal{F}_{n}$)}.\n\\end{aligned}\n$$\nEquivalently, for $m\\in \\mathbb{R}$,\n$$\nP(M_{n+1}\\le y\\mid M_{n}=m)=\n\\begin{cases}\n0,  ym,\\\\\nF(y),  y\\ge m,\n\\end{cases}\n$$\nwhich depends only on $m=M_{n}$. Hence $\\{M_{n}\\}$ is also a Markov process.\n\nTherefore, both $\\{S_{n}\\}$ and $\\{M_{n}\\}$ possess the Markov property.", "answer": "$$\\boxed{C}$$", "id": "1342487"}, {"introduction": "Once we understand the definition of a Markov process, the next step is to see how it works in practice by calculating probabilities. This problem presents a hypothetical self-correcting system modeled as a random walk, where the transition probabilities depend on the current position. Your task is to compute the probability of the system being at a specific location after a few steps, which requires you to meticulously trace possible paths and apply the Markov property at each step [@problem_id:1342491].", "problem": "Consider a simplified one-dimensional model for a self-correcting alignment system. A particle moves along the integer line $\\mathbb{Z}$. Let $X_n$ denote the position of the particle at a discrete time step $n$. The system's objective is to keep the particle as close to the origin (position 0) as possible.\n\nAt each time step, the particle's position changes by exactly one unit, i.e., $X_{n+1} = X_n \\pm 1$. The direction of movement is determined by a probabilistic rule designed to favor returning to the origin. The probability of moving to the right ($X_{n+1} = X_n + 1$) depends on the current position $X_n = x$ and is given by the function $p(x)$. The probability of moving to the left ($X_{n+1} = X_n - 1$) is therefore $1-p(x)$.\n\nThe function $p(x)$ is defined as follows:\n$$\np(x) = \\begin{cases}\nq  \\text{if } x  0 \\\\\n1/2  \\text{if } x = 0 \\\\\n1-q  \\text{if } x  0\n\\end{cases}\n$$\nwhere $q$ is a given constant satisfying $1/2  q  1$.\n\nSuppose the particle starts at the position $X_0 = 1$. Calculate the probability that the particle is at the origin at time step $n=3$, i.e., find $P(X_3 = 0)$. Express your answer as a function of $q$.", "solution": "We model the motion as a Markov chain on $\\mathbb{Z}$ with nearest-neighbor steps. From a current position $x$, the probability of moving to the right is $p(x)$ and to the left is $1-p(x)$, where $p(x)=1-q$ for $x0$, $p(0)=\\frac{1}{2}$, and $p(x)=q$ for $x0$, with $X_{0}=1$ and $\\frac{1}{2}q1$. The probability of any particular length-$3$ path is the product of its stepwise transition probabilities, conditioned on the current state at each step.\n\nTo have $X_{3}=0$ starting from $X_{0}=1$, the net displacement after $3$ steps must be $-1$. With steps of $\\pm 1$, this requires exactly two left moves and one right move. The possible sequences are RLL, LRL, and LLR, where R denotes a right move and L denotes a left move. We compute each path probability by following the states and using the appropriate $p(x)$ at each state:\n\n1) RLL: $1 \\to 2 \\to 1 \\to 0$.\n- From $10$, $\\Pr(\\text{R})=1-q$.\n- From $20$, $\\Pr(\\text{L})=q$.\n- From $10$, $\\Pr(\\text{L})=q$.\nThus the path probability is $(1-q)\\,q\\,q=(1-q)q^{2}$.\n\n2) LRL: $1 \\to 0 \\to 1 \\to 0$.\n- From $10$, $\\Pr(\\text{L})=q$.\n- From $0$, $\\Pr(\\text{R})=\\frac{1}{2}$.\n- From $10$, $\\Pr(\\text{L})=q$.\nThus the path probability is $q\\cdot \\frac{1}{2}\\cdot q=\\frac{q^{2}}{2}$.\n\n3) LLR: $1 \\to 0 \\to -1 \\to 0$.\n- From $10$, $\\Pr(\\text{L})=q$.\n- From $0$, $\\Pr(\\text{L})=\\frac{1}{2}$.\n- From $-10$, $\\Pr(\\text{R})=q$.\nThus the path probability is $q\\cdot \\frac{1}{2}\\cdot q=\\frac{q^{2}}{2}$.\n\nSumming over the disjoint paths yields\n$$\nP(X_{3}=0)=(1-q)q^{2}+\\frac{q^{2}}{2}+\\frac{q^{2}}{2}=(1-q)q^{2}+q^{2}=q^{2}(2-q).\n$$", "answer": "$$\\boxed{q^{2}(2-q)}$$", "id": "1342491"}, {"introduction": "In many applications, we encounter stochastic processes that are derived from simpler Markov chains, or processes that don't appear to be Markov at first glance. This exercise explores the conditions under which transformations of a Markov chain remain Markovian. It introduces critical concepts like state-space augmentation and lumpability, providing you with a toolkit for both identifying and constructing Markov processes in more complex scenarios [@problem_id:1342456].", "problem": "Let $\\{X_n\\}_{n \\ge 0}$ be a time-homogeneous Markov chain with a discrete state space $S_X$ and transition probabilities $P_{ij} = P(X_{n+1}=j | X_n=i)$. Based on this chain, four new stochastic processes are constructed. Your task is to determine which of these new processes are also necessarily Markov processes under the conditions given for each case.\n\nI. A process $\\{Y_n\\}$ is defined as $Y_n = X_n^2$. For this specific case, the base process $\\{X_n\\}$ is defined on the state space $S_X = \\{-1, 1, 2\\}$. Its transition probabilities are such that moving from state 1 to state 2 is possible ($P_{1,2}  0$), but moving from state -1 to state 2 is impossible ($P_{-1,2} = 0$).\n\nII. A process $\\{Z_n\\}$ is defined for $n \\ge 1$ by taking pairs of consecutive states from the original chain, $Z_n = (X_n, X_{n-1})$. The state space for $\\{Z_n\\}$ is $S_X \\times S_X$.\n\nIII. A process $\\{U_n\\}$ is defined via a bijective (one-to-one and onto) function $f: S_X \\to S_U$, where $U_n = f(X_n)$.\n\nIV. A process $\\{V_n\\}$ is defined by reversing time. It is assumed that the original chain $\\{X_n\\}$ is stationary, possessing a stationary distribution $\\pi = (\\pi_i)_{i \\in S_X}$ with all $\\pi_i  0$. The time-reversed process is defined as $V_n = X_{-n}$ for $n \\in \\mathbb{Z}$.\n\nWhich of the processes described above are guaranteed to be Markov processes under the specified conditions?\n\nA. I and II only\n\nB. II and III only\n\nC. I, II, and III only\n\nD. II, III, and IV only\n\nE. I, III, and IV only", "solution": "We analyze each construction and test the Markov property, using explicit conditional probabilities and the lumpability criterion when appropriate.\n\nI. Define $Y_{n}=X_{n}^{2}$ with $S_{X}=\\{-1,1,2\\}$ and transitions satisfying $P_{1,2}0$ and $P_{-1,2}=0$. The image state space is $S_{Y}=\\{1,4\\}$ with the preimage classes $f^{-1}(1)=\\{-1,1\\}$ and $f^{-1}(4)=\\{2\\}$. A function of a Markov chain is itself Markov if and only if the partition induced by the function is strongly lumpable, i.e., for any $y\\in S_{Y}$, any $x,x'\\in f^{-1}(y)$, and any $y'\\in S_{Y}$,\n$$\n\\sum_{j\\in f^{-1}(y')}P_{xj}=\\sum_{j\\in f^{-1}(y')}P_{x'j}.\n$$\nHere, take $y=1$, so $x=1$, $x'=-1$, and take $y'=4$, so $f^{-1}(4)=\\{2\\}$. Then\n$$\n\\sum_{j\\in f^{-1}(4)}P_{1j}=P_{1,2}0,\\qquad \\sum_{j\\in f^{-1}(4)}P_{-1,j}=P_{-1,2}=0,\n$$\nwhich are not equal. Therefore the lumpability condition fails, and $\\{Y_{n}\\}$ is not necessarily Markov under the given assumptions.\n\nII. Define $Z_{n}=(X_{n},X_{n-1})$ on $S_{X}\\times S_{X}$. For any $(i,j)\\in S_{X}\\times S_{X}$,\n$$\n\\mathbb{P}\\big(Z_{n+1}=(k,i)\\mid Z_{n}=(i,j), Z_{n-1}, Z_{n-2},\\dots\\big)=\\mathbb{P}(X_{n+1}=k\\mid X_{n}=i)=P_{ik},\n$$\nwhich depends only on the current state $(i,j)$ through $i$, and not on earlier history. Hence $\\{Z_{n}\\}$ is a first-order Markov chain on $S_{X}\\times S_{X}$.\n\nIII. Let $f:S_{X}\\to S_{U}$ be bijective and define $U_{n}=f(X_{n})$. Since $f$ is invertible, for $u,u'\\in S_{U}$,\n$$\n\\mathbb{P}(U_{n+1}=u'\\mid U_{n}=u)=\\mathbb{P}\\big(X_{n+1}=f^{-1}(u')\\mid X_{n}=f^{-1}(u)\\big)=P_{f^{-1}(u),\\,f^{-1}(u')},\n$$\nwhich depends only on the current state $u$. Therefore $\\{U_{n}\\}$ is Markov.\n\nIV. Assume $\\{X_{n}\\}_{n\\in\\mathbb{Z}}$ is stationary with stationary distribution $\\pi=(\\pi_{i})_{i\\in S_{X}}$ satisfying $\\pi_{i}0$ for all $i$, and define $V_{n}=X_{-n}$. For $i,j\\in S_{X}$, the transition probabilities of the time-reversed chain are\n$$\nP^{*}_{ij}=\\mathbb{P}(V_{n+1}=j\\mid V_{n}=i)=\\frac{\\pi_{j}P_{ji}}{\\pi_{i}},\n$$\nwhich are well-defined because $\\pi_{i}0$. This shows that $\\{V_{n}\\}$ is Markov (the standard time-reversal of a stationary Markov chain).\n\nCombining the conclusions: I is not necessarily Markov, while II, III, and IV are Markov under the stated conditions. The correct choice is D.", "answer": "$$\\boxed{D}$$", "id": "1342456"}]}