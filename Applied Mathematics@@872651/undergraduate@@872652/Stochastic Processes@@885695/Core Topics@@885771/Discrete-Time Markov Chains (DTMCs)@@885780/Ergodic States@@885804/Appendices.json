{"hands_on_practices": [{"introduction": "To understand the long-term behavior of a system, we first need to classify its states. An ergodic state is one that is both positive recurrent and aperiodic, a combination of properties that guarantees the system will eventually settle into a predictable steady state. This first practice problem [@problem_id:1299388] challenges you to examine a simple random walk on a line and determine if a state is ergodic by carefully checking for periodicity in its movements.", "problem": "A particle undergoes a random walk on a set of four discrete positions, which we can label as states {1, 2, 3, 4}. This process is modeled as a discrete-time Markov chain. The positions are arranged in a line, and the particle's movement is governed by the following transition rules:\n- When at an interior position $i \\in \\{2, 3\\}$, the particle moves to position $i-1$ with probability $0.5$ or to position $i+1$ with probability $0.5$.\n- Positions 1 and 4 act as reflecting boundaries. From position 1, the particle always moves to position 2 in the next step (i.e., with probability 1). From position 4, the particle always moves to position 3 in the next step (with probability 1).\n\nBased on this model, which of the following statements provides the correct classification for state 2?\n\nA. State 2 is ergodic.\n\nB. State 2 is not ergodic because it is a periodic state.\n\nC. State 2 is not ergodic because it is a transient state.\n\nD. State 2 is not ergodic because it is a null recurrent state.", "solution": "We model the chain on states $\\{1,2,3,4\\}$ with transition matrix $P$ (rows and columns ordered as $1,2,3,4$):\n$$\nP=\\begin{pmatrix}\n0 & 1 & 0 & 0\\\\\n\\frac{1}{2} & 0 & \\frac{1}{2} & 0\\\\\n0 & \\frac{1}{2} & 0 & \\frac{1}{2}\\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}.\n$$\nFirst, we determine irreducibility and recurrence. The underlying graph is connected via nearest-neighbor moves with reflecting boundaries: $1 \\leftrightarrow 2 \\leftrightarrow 3 \\leftrightarrow 4$, where each direction has strictly positive probability. Hence every state communicates with every other; the chain is irreducible. Since the state space is finite and the chain is irreducible, every state is positive recurrent. Therefore state $2$ is not transient and not null recurrent.\n\nNext, we determine the period of state $2$. The period $d(2)$ is defined by\n$$\nd(2)=\\gcd\\{n\\ge 1 : (P^{n})_{2,2}>0\\}.\n$$\nEvery step changes parity of the position (moves between odd and even states), so starting from the even state $2$, after $n$ steps the chain is at an even state if and only if $n$ is even. Thus $(P^{n})_{2,2}=0$ for all odd $n$. For even $n=2m$, there exists a path returning to $2$, for example\n$$\n2 \\to 1 \\to 2 \\to 1 \\to \\cdots \\to 2 \\quad \\text{(length }2m\\text{),}\n$$\nwhose probability equals $(\\frac{1}{2})^{m}>0$ (each $2\\to 1$ has probability $\\frac{1}{2}$ and each $1\\to 2$ has probability $1$). Hence $(P^{2m})_{2,2}>0$ for all $m\\ge 1$. Therefore\n$$\n\\{n\\ge 1 : (P^{n})_{2,2}>0\\}=\\{2,4,6,\\dots\\}\n$$\nand\n$$\nd(2)=\\gcd\\{2,4,6,\\dots\\}=2.\n$$\nSo state $2$ is periodic with period $2$, and while it is positive recurrent (finite irreducible chain), it is not aperiodic. In the standard classification where an ergodic state is positive recurrent and aperiodic within a closed irreducible class, state $2$ is not ergodic because it is periodic.\n\nTherefore the correct option is B.", "answer": "$$\\boxed{B}$$", "id": "1299388"}, {"introduction": "Having explored what can prevent a state from being ergodic, we now turn to a scenario where ergodicity naturally arises. In many real-world processes, there's a chance of the state remaining unchanged in a given step, which ensures aperiodicity in an irreducible chain. This practice problem [@problem_id:1299404] models a delivery truck with a chance of breakdown, demonstrating how this simple feature makes the system ergodic and allows us to calculate its predictable long-term behavior.", "problem": "A logistics company operates a single autonomous delivery truck that serves three cities: Metropolis (A), Gotham (B), and Zenith (C). The truck follows a fixed, one-way daily route: from Metropolis it travels to Gotham, from Gotham to Zenith, and from Zenith back to Metropolis. This travel between cities takes one full day. However, the truck's aging engine has a reliability issue. On any given day, regardless of its location, there is a constant probability $p$ that the truck will experience a mechanical breakdown. If a breakdown occurs, the truck remains in its current city for that day to undergo repairs, and its scheduled travel is postponed to the next day. The probability of a breakdown, $p$, is a fixed value satisfying $0 < p < 1$.\n\nAssuming the truck has been operating for a very long time, what is the long-run proportion of days that the truck is located in Metropolis? Provide your answer as an exact fraction.", "solution": "Model the system as a discrete-time Markov chain observed at the start of each day, with state space $\\{A,B,C\\}$ representing the city where the truck is located at that time. Each day:\n- With probability $p$, a breakdown occurs and the truck remains in its current city, so the next day's state is the same as today's.\n- With probability $1-p$, no breakdown occurs; the truck completes the scheduled leg and the next day's state is the next city along the cycle $A \\to B \\to C \\to A$.\n\nThus, the one-step transition probabilities are:\n$$\nP(A \\to A)=p,\\quad P(A \\to B)=1-p,\\quad P(A \\to C)=0,\n$$\n$$\nP(B \\to B)=p,\\quad P(B \\to C)=1-p,\\quad P(B \\to A)=0,\n$$\n$$\nP(C \\to C)=p,\\quad P(C \\to A)=1-p,\\quad P(C \\to B)=0.\n$$\nFor $0<p<1$, the chain is irreducible (each state can reach the others through successive moves) and aperiodic (each state has a self-loop with positive probability $p$), so it has a unique stationary distribution, and the long-run proportion of days in each city equals that stationary distribution.\n\nLet $(\\pi_{A},\\pi_{B},\\pi_{C})$ denote the stationary probabilities. Stationarity $\\pi=\\pi P$ gives the balance equations:\n$$\n\\pi_{A}=\\pi_{A}p+\\pi_{C}(1-p),\\qquad\n\\pi_{B}=\\pi_{B}p+\\pi_{A}(1-p),\\qquad\n\\pi_{C}=\\pi_{C}p+\\pi_{B}(1-p),\n$$\ntogether with the normalization $\\pi_{A}+\\pi_{B}+\\pi_{C}=1$.\n\nRewriting,\n$$\n(1-p)(\\pi_{A}-\\pi_{C})=0,\\quad (1-p)(\\pi_{B}-\\pi_{A})=0,\\quad (1-p)(\\pi_{C}-\\pi_{B})=0.\n$$\nSince $1-p>0$, these imply\n$$\n\\pi_{A}=\\pi_{B}=\\pi_{C}.\n$$\nUsing $\\pi_{A}+\\pi_{B}+\\pi_{C}=1$ yields\n$$\n\\pi_{A}=\\pi_{B}=\\pi_{C}=\\frac{1}{3}.\n$$\nTherefore, the long-run proportion of days that the truck is located in Metropolis is $\\frac{1}{3}$.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1299404"}, {"introduction": "The power of ergodic theory extends far beyond simple random walks, allowing us to analyze complex adaptive systems. In this final exercise, we will model a game of Rock-Paper-Scissors where one player uses a \"win-stay, lose-or-draw-shift\" strategy that is subject to random noise. By setting up the transition matrix for this strategic behavior, you will see how the presence of noise guarantees ergodicity and enables the calculation of the player's long-term action distribution [@problem_id:1299380].", "problem": "Consider a repeated game of Rock-Paper-Scissors between two players. Player 1 is predictable and always chooses 'Rock'. Player 2 employs a more complex, adaptive strategy influenced by random noise. Let Player 2's action on any turn be one of {Rock, Paper, Scissors}.\n\nPlayer 2's strategy is defined as follows:\nOn each turn, Player 2's action is determined by a probabilistic rule. With probability $1-\\epsilon$, Player 2 follows a \"base strategy\". With probability $\\epsilon$, Player 2 disregards the base strategy and instead chooses one of the three actions {Rock, Paper, Scissors} uniformly at random (i.e., with probability $1/3$ for each).\n\nThe base strategy is a \"win-stay, lose-or-draw-shift\" rule based on the outcome of the previous turn against Player 1's 'Rock':\n- **Win-stay**: If Player 2 won the previous turn, their base action is to play the same action again.\n- **Lose-or-draw-shift**: If Player 2 lost or drew on the previous turn, their base action is to randomly choose one of the other two actions, each with a probability of $1/2$.\n\nAssume the noise parameter $\\epsilon$ is a constant satisfying $0 < \\epsilon < 3/2$.\n\nDetermine the long-run proportion of time that Player 2 chooses the 'Paper' action. Express your answer as a symbolic expression in terms of $\\epsilon$.", "solution": "Label Player 2’s current action by the Markov state set $\\{R,P,S\\}$. Against Player 1’s fixed $R$, the base rule is:\n- From $P$ (win): stay at $P$ with probability $1$.\n- From $R$ (draw): switch uniformly to one of the other two actions, i.e., to $P$ or $S$ with probability $\\frac{1}{2}$ each.\n- From $S$ (loss): switch uniformly to one of the other two actions, i.e., to $R$ or $P$ with probability $\\frac{1}{2}$ each.\n\nThus the base transition matrix $T_{0}$ (rows are current state, columns are next state) is\n$$\nT_{0}=\\begin{pmatrix}\n0 & \\frac{1}{2} & \\frac{1}{2}\\\\\n0 & 1 & 0\\\\\n\\frac{1}{2} & \\frac{1}{2} & 0\n\\end{pmatrix}.\n$$\nWith probability $\\epsilon$, Player 2 ignores the base rule and plays uniformly at random; let $J$ be the $3\\times 3$ matrix with every row equal to $\\left(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right)$. The overall transition matrix is\n$$\nT=(1-\\epsilon)T_{0}+\\epsilon J.\n$$\nLet the stationary distribution be $\\pi=(r,p,s)$ for $(R,P,S)$. Stationarity gives\n$$\n\\pi=\\pi T=(1-\\epsilon)\\,\\pi T_{0}+\\epsilon\\left(\\tfrac{1}{3},\\tfrac{1}{3},\\tfrac{1}{3}\\right),\n$$\nbecause $\\pi J=\\left(\\tfrac{1}{3},\\tfrac{1}{3},\\tfrac{1}{3}\\right)$. Compute $\\pi T_{0}$:\n$$\n\\pi T_{0}\n= r\\,(0,\\tfrac{1}{2},\\tfrac{1}{2})+p\\,(0,1,0)+s\\,(\\tfrac{1}{2},\\tfrac{1}{2},0)\n=\\left(\\tfrac{1}{2}s,\\ \\tfrac{1}{2}r+p+\\tfrac{1}{2}s,\\ \\tfrac{1}{2}r\\right).\n$$\nHence the component equations are\n$$\nr=(1-\\epsilon)\\tfrac{1}{2}s+\\tfrac{\\epsilon}{3},\\quad\np=(1-\\epsilon)\\left(\\tfrac{1}{2}r+p+\\tfrac{1}{2}s\\right)+\\tfrac{\\epsilon}{3},\\quad\ns=(1-\\epsilon)\\tfrac{1}{2}r+\\tfrac{\\epsilon}{3},\n$$\ntogether with $r+p+s=1$. Let $a=\\tfrac{1-\\epsilon}{2}$. Then\n$$\nr=a\\,s+\\tfrac{\\epsilon}{3},\\qquad s=a\\,r+\\tfrac{\\epsilon}{3}.\n$$\nSubstitute $r$ from the first into the second:\n$$\ns=a\\left(a\\,s+\\tfrac{\\epsilon}{3}\\right)+\\tfrac{\\epsilon}{3}\n=a^{2}s+\\tfrac{a\\epsilon}{3}+\\tfrac{\\epsilon}{3}.\n$$\nTherefore\n$$\ns-a^{2}s=\\tfrac{(a+1)\\epsilon}{3} \\implies s(1-a^{2})=\\tfrac{(a+1)\\epsilon}{3}.\n$$\nSince $1-a^{2}=(1-a)(1+a)$ and $1+a=\\tfrac{3-\\epsilon}{2}\\neq 0$ for $0<\\epsilon<3/2$, we can cancel $(1+a)$ to get\n$$\ns(1-a)=\\tfrac{\\epsilon}{3}.\n$$\nNow $1-a=1-\\tfrac{1-\\epsilon}{2}=\\tfrac{1+\\epsilon}{2}$, so\n$$\ns=\\frac{\\epsilon/3}{(1+\\epsilon)/2}=\\frac{2\\epsilon}{3(1+\\epsilon)}.\n$$\nBy symmetry the same algebra yields\n$$\nr=\\frac{2\\epsilon}{3(1+\\epsilon)}.\n$$\nFinally, use $r+p+s=1$ to obtain\n$$\np=1-r-s=1-\\frac{4\\epsilon}{3(1+\\epsilon)}=\\frac{3(1+\\epsilon)-4\\epsilon}{3(1+\\epsilon)}=\\frac{3-\\epsilon}{3(1+\\epsilon)}.\n$$\nThis $p$ also satisfies the middle stationarity equation, confirming consistency. Therefore, the long-run proportion of time that Player 2 chooses Paper is $\\frac{3-\\epsilon}{3(1+\\epsilon)}$.", "answer": "$$\\boxed{\\frac{3-\\epsilon}{3\\left(1+\\epsilon\\right)}}$$", "id": "1299380"}]}