{"hands_on_practices": [{"introduction": "This first exercise is a foundational practice in defining a stochastic process from the ground up. By calculating the joint probability mass function for a simple normalized random walk at its first two time steps, you will construct a finite-dimensional distribution directly from first principles. This practice makes the abstract concept concrete and helps visualize how the process's state at one time influences its potential states at a later time [@problem_id:1302889].", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $\\{Z_i\\}_{i \\ge 1}$, which represent the steps of a random walk. Each step is either forward or backward with equal probability, such that the probability mass function for each $Z_i$ is given by $P(Z_i = 1) = 1/2$ and $P(Z_i = -1) = 1/2$.\n\nThe position of the random walk at time $n$ is given by the sum of the steps, $S_n = \\sum_{i=1}^n Z_i$, with the starting position being $S_0=0$. A normalized stochastic process $\\{X_n\\}_{n \\ge 1}$ is constructed from this random walk, where $X_n = S_n / n$.\n\nDetermine the two-dimensional finite-dimensional distribution for this process at times $n=1$ and $n=2$. That is, find the joint probability mass function $P(X_1=x_1, X_2=x_2)$. Which of the following tables correctly represents this joint distribution?\n\nA.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2)  x_2 = -1  x_2 = 0  x_2 = 1 \\\\\n\\hline\nx_1 = -1  1/4  1/4  0 \\\\\nx_1 = 1  0  1/4  1/4 \\\\\n\\end{array}\n$$\n\nB.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2)  x_2 = -1  x_2 = 0  x_2 = 1 \\\\\n\\hline\nx_1 = -1  1/8  1/4  1/8 \\\\\nx_1 = 1  1/8  1/4  1/8 \\\\\n\\end{array}\n$$\n\nC.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2)  x_2 = -1  x_2 = 0  x_2 = 1 \\\\\n\\hline\nx_1 = -1  1/4  0  1/4 \\\\\nx_1 = 1  1/4  1/4  0 \\\\\n\\end{array}\n$$\n\nD.\n$$\n\\begin{array}{c|ccc}\n P(X_1, X_2)  x_2 = -1  x_2 = 0  x_2 = 1 \\\\\n\\hline\nx_1 = -1  1/2  0  0 \\\\\nx_1 = 1  0  0  1/2 \\\\\n\\end{array}\n$$", "solution": "We are given i.i.d. steps with $P(Z_{i}=1)=\\frac{1}{2}$ and $P(Z_{i}=-1)=\\frac{1}{2}$. The random walk positions are $S_{n}=\\sum_{i=1}^{n}Z_{i}$ with $S_{0}=0$, and the normalized process is $X_{n}=S_{n}/n$.\n\nAt the specified times,\n$$\nX_{1}=\\frac{S_{1}}{1}=Z_{1}, \\qquad X_{2}=\\frac{S_{2}}{2}=\\frac{Z_{1}+Z_{2}}{2}.\n$$\nBy independence and identical distribution, for each pair $(z_{1},z_{2})\\in\\{-1,1\\}^{2}$,\n$$\nP(Z_{1}=z_{1},Z_{2}=z_{2})=P(Z_{1}=z_{1})P(Z_{2}=z_{2})=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$\n\nEnumerate the four possible outcomes and map them to $(X_{1},X_{2})$:\n- If $(Z_{1},Z_{2})=(1,1)$, then $X_{1}=1$ and $X_{2}=\\frac{1+1}{2}=1$, giving $(X_{1},X_{2})=(1,1)$ with probability $\\frac{1}{4}$.\n- If $(Z_{1},Z_{2})=(1,-1)$, then $X_{1}=1$ and $X_{2}=\\frac{1+(-1)}{2}=0$, giving $(X_{1},X_{2})=(1,0)$ with probability $\\frac{1}{4}$.\n- If $(Z_{1},Z_{2})=(-1,1)$, then $X_{1}=-1$ and $X_{2}=\\frac{-1+1}{2}=0$, giving $(X_{1},X_{2})=(-1,0)$ with probability $\\frac{1}{4}$.\n- If $(Z_{1},Z_{2})=(-1,-1)$, then $X_{1}=-1$ and $X_{2}=\\frac{-1+(-1)}{2}=-1$, giving $(X_{1},X_{2})=(-1,-1)$ with probability $\\frac{1}{4}$.\n\nTherefore, the joint pmf has nonzero entries only at $(-1,-1)$, $(-1,0)$, $(1,0)$, and $(1,1)$ with probabilities $\\frac{1}{4}$ each, and zeros elsewhere. Comparing with the provided tables, this matches option A.", "answer": "$$\\boxed{A}$$", "id": "1302889"}, {"introduction": "Often, calculating the complete joint distribution is intractable; instead, we characterize it using key statistical moments. This problem has you compute the covariance matrix for a \"running average\" process, a common tool in signal processing and time-series analysis. Mastering this calculation provides a practical method for quantifying the variability of a process and the linear relationship between its values at different points in time [@problem_id:1302890].", "problem": "Consider a sequence of Independent and Identically Distributed (IID) random variables $\\{Z_k\\}_{k=1, 2, \\dots}$. These random variables have a mean of $E[Z_k] = 0$ and a finite, non-zero variance of $\\text{Var}(Z_k) = \\sigma^2$ for all $k$.\n\nA new stochastic process, called the running average process, is defined as $\\{X_n\\}_{n=1, 2, \\dots}$ where\n$$ X_n = \\frac{1}{n} \\sum_{k=1}^{n} Z_k $$\n\nDetermine the $2 \\times 2$ covariance matrix of the random vector $(X_n, X_{n+1})$. Express your answer as a matrix whose entries are in terms of $n$ and $\\sigma$.", "solution": "The covariance matrix for the random vector $(X_n, X_{n+1})$ is a $2 \\times 2$ matrix given by:\n$$ \\mathbf{C} = \\begin{pmatrix} \\text{Var}(X_n)  \\text{Cov}(X_n, X_{n+1}) \\\\ \\text{Cov}(X_{n+1}, X_n)  \\text{Var}(X_{n+1}) \\end{pmatrix} $$\nWe need to compute its three unique components: $\\text{Var}(X_n)$, $\\text{Var}(X_{n+1})$, and $\\text{Cov}(X_n, X_{n+1})$.\n\nFirst, let's find the expected value of the process $X_n$. Using the linearity of expectation:\n$$ E[X_n] = E\\left[\\frac{1}{n} \\sum_{k=1}^{n} Z_k\\right] = \\frac{1}{n} \\sum_{k=1}^{n} E[Z_k] $$\nSince we are given that $E[Z_k] = 0$ for all $k$, we have:\n$$ E[X_n] = \\frac{1}{n} \\sum_{k=1}^{n} 0 = 0 $$\nSimilarly, $E[X_{n+1}] = 0$.\n\nNext, we compute the variance of $X_n$, which is a diagonal element of the covariance matrix. The variance of a random variable $Y$ is given by $\\text{Var}(Y) = E[Y^2] - (E[Y])^2$. Since $E[X_n]=0$, this simplifies to $\\text{Var}(X_n) = E[X_n^2]$.\n$$ \\text{Var}(X_n) = \\text{Var}\\left(\\frac{1}{n} \\sum_{k=1}^{n} Z_k\\right) $$\nUsing the property that $\\text{Var}(aY) = a^2 \\text{Var}(Y)$, we get:\n$$ \\text{Var}(X_n) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{k=1}^{n} Z_k\\right) $$\nSince the random variables $\\{Z_k\\}$ are independent, the variance of their sum is the sum of their variances:\n$$ \\text{Var}(X_n) = \\frac{1}{n^2} \\sum_{k=1}^{n} \\text{Var}(Z_k) $$\nGiven that $\\text{Var}(Z_k) = \\sigma^2$ for all $k$:\n$$ \\text{Var}(X_n) = \\frac{1}{n^2} \\sum_{k=1}^{n} \\sigma^2 = \\frac{1}{n^2} (n \\sigma^2) = \\frac{\\sigma^2}{n} $$\nBy the same logic, the other diagonal element is:\n$$ \\text{Var}(X_{n+1}) = \\frac{\\sigma^2}{n+1} $$\n\nNow, we compute the off-diagonal term, the covariance $\\text{Cov}(X_n, X_{n+1})$. The covariance is defined as $\\text{Cov}(Y, W) = E[YW] - E[Y]E[W]$. Since $E[X_n] = 0$ and $E[X_{n+1}]=0$, this simplifies to $\\text{Cov}(X_n, X_{n+1}) = E[X_n X_{n+1}]$.\nLet's express $X_{n+1}$ in terms of $X_n$:\n$$ X_{n+1} = \\frac{1}{n+1} \\sum_{k=1}^{n+1} Z_k = \\frac{1}{n+1} \\left( \\left(\\sum_{k=1}^{n} Z_k\\right) + Z_{n+1} \\right) $$\nRecognizing that $\\sum_{k=1}^{n} Z_k = n X_n$, we can write:\n$$ X_{n+1} = \\frac{1}{n+1} (n X_n + Z_{n+1}) $$\nNow we can compute the covariance:\n$$ \\text{Cov}(X_n, X_{n+1}) = \\text{Cov}\\left(X_n, \\frac{n X_n + Z_{n+1}}{n+1}\\right) $$\nUsing the bilinearity property of covariance, $\\text{Cov}(A, bB+cC) = b \\text{Cov}(A,B) + c \\text{Cov}(A,C)$:\n$$ \\text{Cov}(X_n, X_{n+1}) = \\frac{n}{n+1} \\text{Cov}(X_n, X_n) + \\frac{1}{n+1} \\text{Cov}(X_n, Z_{n+1}) $$\nWe know that $\\text{Cov}(X_n, X_n) = \\text{Var}(X_n) = \\frac{\\sigma^2}{n}$.\nThe second term is $\\text{Cov}(X_n, Z_{n+1}) = \\text{Cov}\\left(\\frac{1}{n} \\sum_{k=1}^{n} Z_k, Z_{n+1}\\right)$. Since $Z_{n+1}$ is independent of all $Z_k$ for $k=1, \\dots, n$, it is also independent of any function of them, including their average $X_n$. The covariance of independent random variables is zero. Thus, $\\text{Cov}(X_n, Z_{n+1}) = 0$.\nSubstituting these results back:\n$$ \\text{Cov}(X_n, X_{n+1}) = \\frac{n}{n+1} \\left(\\frac{\\sigma^2}{n}\\right) + \\frac{1}{n+1} (0) = \\frac{\\sigma^2}{n+1} $$\nSince the covariance matrix is symmetric, $\\text{Cov}(X_{n+1}, X_n) = \\text{Cov}(X_n, X_{n+1})$.\n\nFinally, we can assemble the covariance matrix:\n$$ \\mathbf{C} = \\begin{pmatrix} \\text{Var}(X_n)  \\text{Cov}(X_n, X_{n+1}) \\\\ \\text{Cov}(X_n, X_{n+1})  \\text{Var}(X_{n+1}) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sigma^2}{n}  \\frac{\\sigma^2}{n+1} \\\\ \\frac{\\sigma^2}{n+1}  \\frac{\\sigma^2}{n+1} \\end{pmatrix} $$\nThis can also be written by factoring out $\\sigma^2$:\n$$ \\mathbf{C} = \\sigma^2 \\begin{pmatrix} \\frac{1}{n}  \\frac{1}{n+1} \\\\ \\frac{1}{n+1}  \\frac{1}{n+1} \\end{pmatrix} $$", "answer": "$$\\boxed{\\sigma^{2} \\begin{pmatrix} \\frac{1}{n}  \\frac{1}{n+1} \\\\ \\frac{1}{n+1}  \\frac{1}{n+1} \\end{pmatrix}}$$", "id": "1302890"}, {"introduction": "Our final practice explores a process where the dependencies between time points are more subtle. By analyzing the \"record indicator\" process, you will use combinatorial reasoning about the ordering of random variables to derive the joint distribution. This exercise sharpens your problem-solving skills and demonstrates how the entire history of a process can influence its future behavior in non-obvious ways [@problem_id:1302866].", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $Z_1, Z_2, Z_3, \\dots$, where each $Z_n$ follows a Uniform distribution on the interval $[0, 1]$. We can define a stochastic process known as the record indicator process based on this sequence. A \"record\" is said to occur at time $n$ if the value $Z_n$ is greater than all values that came before it.\n\nFor $n \\ge 2$, the record indicator random variable $I_n$ is defined as:\n$$\nI_n = \\begin{cases} \n1  \\text{if } Z_n  \\max\\{Z_1, Z_2, \\dots, Z_{n-1}\\} \\\\\n0  \\text{otherwise} \n\\end{cases}\n$$\nYour task is to determine the joint probability mass function (PMF) of the random vector $(I_2, I_3)$. This PMF is specified by the four probabilities $p_{ij} = P(I_2=i, I_3=j)$ for $i,j \\in \\{0, 1\\}$.\n\nWhich of the following options correctly gives the tuple of probabilities $(p_{00}, p_{01}, p_{10}, p_{11})$?\n\nA. $(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$\n\nB. $(\\frac{1}{3}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{3})$\n\nC. $(\\frac{4}{9}, \\frac{2}{9}, \\frac{2}{9}, \\frac{1}{9})$\n\nD. $(\\frac{1}{6}, \\frac{1}{3}, \\frac{1}{6}, \\frac{1}{3})$\n\nE. $(\\frac{1}{3}, \\frac{1}{6}, \\frac{1}{3}, \\frac{1}{6})$", "solution": "We use the fact that for i.i.d. continuous random variables (such as Uniform on $[0,1]$), all strict orderings of $(Z_{1},Z_{2},Z_{3})$ are equally likely, each with probability $1/3!=1/6$, and ties have probability $0$. The indicators are determined by ranks only: $I_{2}=1$ iff $Z_{2}Z_{1}$, and $I_{3}=1$ iff $Z_{3}\\max\\{Z_{1},Z_{2}\\}$, i.e., $Z_{3}$ is the largest among the three.\n\nList the six strict orderings and the corresponding $(I_{2},I_{3})$:\n1) $Z_{1}Z_{2}Z_{3}$ gives $(I_{2},I_{3})=(0,0)$.\n2) $Z_{1}Z_{3}Z_{2}$ gives $(I_{2},I_{3})=(0,0)$.\n3) $Z_{2}Z_{1}Z_{3}$ gives $(I_{2},I_{3})=(1,0)$.\n4) $Z_{2}Z_{3}Z_{1}$ gives $(I_{2},I_{3})=(1,0)$.\n5) $Z_{3}Z_{1}Z_{2}$ gives $(I_{2},I_{3})=(0,1)$.\n6) $Z_{3}Z_{2}Z_{1}$ gives $(I_{2},I_{3})=(1,1)$.\n\nSumming the probabilities $1/6$ over the cases for each pair, we obtain\n$$\np_{00}=P(I_{2}=0,I_{3}=0)=\\frac{2}{6}=\\frac{1}{3},\\quad\np_{01}=P(I_{2}=0,I_{3}=1)=\\frac{1}{6},\\quad\np_{10}=P(I_{2}=1,I_{3}=0)=\\frac{2}{6}=\\frac{1}{3},\\quad\np_{11}=P(I_{2}=1,I_{3}=1)=\\frac{1}{6}.\n$$\nThus $(p_{00},p_{01},p_{10},p_{11})=\\left(\\frac{1}{3},\\frac{1}{6},\\frac{1}{3},\\frac{1}{6}\\right)$, which corresponds to option E.", "answer": "$$\\boxed{E}$$", "id": "1302866"}]}