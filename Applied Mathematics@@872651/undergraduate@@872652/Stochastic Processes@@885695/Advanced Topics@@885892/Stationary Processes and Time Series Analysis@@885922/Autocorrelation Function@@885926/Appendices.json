{"hands_on_practices": [{"introduction": "The journey into understanding time-dependent data begins with the core concepts of variance and covariance. The autocorrelation function (ACF) provides a standardized, scale-free measure of how a time series is related to its past values. This first exercise is a direct application of the fundamental definition of the ACF, helping you solidify the relationship between the autocovariance at a given lag, the process's variance, and the resulting correlation value.", "problem": "An economist is studying a country's quarterly Gross Domestic Product (GDP) growth rate. The time series of these growth rates, after appropriate transformations, is modeled as a stationary stochastic process, $\\{X_t\\}$. The autocovariance function of this process, which measures the covariance between observations separated by a lag of $h$ quarters, is denoted by $\\gamma(h)$. Specifically, $\\gamma(h) = \\text{Cov}(X_t, X_{t+h})$.\n\nFrom the historical data, the variance of the process is calculated to be $\\gamma(0) = 0.0036$. The autocovariance between the growth rates of two consecutive quarters (a lag of $h=1$) is found to be $\\gamma(1) = -0.0012$. The Autocorrelation Function (ACF), denoted by $\\rho(h)$, provides a scale-free measure of the correlation between observations as a function of the time lag.\n\nCalculate the value of the ACF at lag 1, $\\rho(1)$, for this time series.", "solution": "For a weakly stationary process $\\{X_t\\}$, the autocorrelation function at lag $h$ is defined by\n$$\n\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)}.\n$$\nGiven $\\gamma(0) = 0.0036$ and $\\gamma(1) = -0.0012$, compute\n$$\n\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\frac{-0.0012}{0.0036}.\n$$\nMultiply numerator and denominator by $10^4$ to clear decimals:\n$$\n\\rho(1) = \\frac{-12}{36} = -\\frac{1}{3}.\n$$\nThus, the ACF at lag $1$ is $-\\frac{1}{3}$.", "answer": "$$\\boxed{-\\frac{1}{3}}$$", "id": "1897241"}, {"introduction": "While theoretical definitions are crucial, in any practical scenario we work with a finite set of observed data. This requires us to move from the true, unobservable ACF to its estimate, the sample ACF. This practice problem bridges that gap, guiding you through the step-by-step computation of the sample ACF from a small dataset, a fundamental skill for any time series analyst.", "problem": "Consider a discrete time series representing four sequential observations, given by the sequence $\\{X_t\\}_{t=1}^4 = \\{2, 8, 4, 10\\}$. For the purposes of this calculation, you are to assume that the true mean of the process generating this data is zero, and therefore, the sample mean $\\bar{X}$ should be taken as 0.\n\nThe sample Autocorrelation Function (ACF) at a given lag $h$ for a time series $\\{X_t\\}_{t=1}^n$ with a sample mean $\\bar{X}$ is defined as:\n$$r_h = \\frac{\\sum_{t=1}^{n-h} (X_t - \\bar{X})(X_{t+h} - \\bar{X})}{\\sum_{t=1}^{n} (X_t - \\bar{X})^2}$$\nYour task is to compute the value of the sample ACF at lag $h=1$ for the provided time series.\n\nExpress your final answer as a fraction in its simplest form.", "solution": "We are given the time series $\\{X_t\\}_{t=1}^4=\\{2,8,4,10\\}$ and instructed to take $\\bar{X}=0$. The sample ACF at lag $h=1$ is defined by\n$$\nr_1 = \\frac{\\sum_{t=1}^{n-1}(X_t - \\bar{X})(X_{t+1} - \\bar{X})}{\\sum_{t=1}^{n}(X_t - \\bar{X})^2}.\n$$\nWith $\\bar{X}=0$ and $n=4$, the numerator is\n$$\n\\sum_{t=1}^{3} X_t X_{t+1} = X_1 X_2 + X_2 X_3 + X_3 X_4 = 2 \\cdot 8 + 8 \\cdot 4 + 4 \\cdot 10 = 16+32+40=88.\n$$\nThe denominator is\n$$\n\\sum_{t=1}^{4} X_t^2 = 2^2 + 8^2 + 4^2 + 10^2 = 4+64+16+100=184.\n$$\nTherefore,\n$$\nr_1 = \\frac{88}{184}.\n$$\nSimplifying the fraction by dividing numerator and denominator by $8$ gives\n$$\nr_1 = \\frac{11}{23}.\n$$", "answer": "$$\\boxed{\\frac{11}{23}}$$", "id": "1897249"}, {"introduction": "Different stochastic processes have unique temporal structures, and the ACF serves as a distinctive \"signature\" that helps us identify the underlying model. This exercise challenges you to derive the theoretical ACF for a Moving Average (MA) process, one of the foundational models in time series analysis. By working through this problem, you will see how a model's specific construction—in this case, as a weighted sum of past shocks—directly determines its correlation pattern.", "problem": "A financial analyst is modeling the daily change in a particular commodity's price using a simple time series model. The model for the price change on day $t$, denoted by $X_t$, is given by a moving average process of order 1, MA(1):\n$$X_t = \\epsilon_t + 0.8 \\epsilon_{t-1}$$\nHere, $\\epsilon_t$ represents a \"shock\" or new information arriving on day $t$. The sequence of shocks $\\{\\epsilon_t\\}$ is assumed to be a white noise process with the following properties:\n1. The expected value of any shock is zero, i.e., $\\mathbb{E}[\\epsilon_t] = 0$ for all $t$.\n2. The variance of any shock is a constant, finite value $\\sigma^2$, i.e., $\\text{Var}(\\epsilon_t) = \\sigma^2$ for all $t$.\n3. Shocks at different times are uncorrelated, i.e., $\\text{Cov}(\\epsilon_t, \\epsilon_s) = 0$ for all $t \\neq s$.\n\nThe theoretical Autocorrelation Function (ACF) at lag $k$, denoted $\\rho(k)$, measures the correlation between the time series and its value $k$ periods ago. It is defined as:\n$$\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}$$\nwhere $\\gamma(k) = \\text{Cov}(X_t, X_{t-k})$ is the autocovariance function at lag $k$, and $\\gamma(0) = \\text{Var}(X_t)$ is the variance of the process.\n\nCalculate the theoretical autocorrelation at lag 1, $\\rho(1)$, for this model. Express your answer as an exact fraction in its simplest form.", "solution": "We are given the MA(1) process $X_t = \\epsilon_t + \\theta \\epsilon_{t-1}$ with $\\theta = \\frac{4}{5}$. The process $\\{\\epsilon_t\\}$ is white noise, so $\\mathbb{E}[\\epsilon_t]=0$, $\\text{Var}(\\epsilon_t)=\\sigma^2$, and $\\text{Cov}(\\epsilon_t, \\epsilon_s)=0$ for $t \\neq s$. The autocorrelation at lag $k$ is $\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}$, where $\\gamma(k) = \\text{Cov}(X_t, X_{t-k})$.\n\nFirst, compute the variance $\\gamma(0) = \\text{Var}(X_t)$. Using the property $\\text{Var}(aY+bZ) = a^2\\text{Var}(Y) + b^2\\text{Var}(Z) + 2ab\\text{Cov}(Y,Z)$ and the white noise properties,\n$$\n\\gamma(0) = \\text{Var}(\\epsilon_t + \\theta \\epsilon_{t-1}) = \\text{Var}(\\epsilon_t) + \\theta^2\\text{Var}(\\epsilon_{t-1}) + 2\\theta\\text{Cov}(\\epsilon_t, \\epsilon_{t-1}).\n$$\nSince $\\text{Cov}(\\epsilon_t, \\epsilon_{t-1})=0$, this simplifies to\n$$\n\\gamma(0)=(1+\\theta^2)\\sigma^2 = \\left(1+\\left(\\frac{4}{5}\\right)^2\\right)\\sigma^2 = \\left(1+\\frac{16}{25}\\right)\\sigma^2 = \\frac{41}{25}\\sigma^2.\n$$\n\nNext, compute the lag-1 autocovariance $\\gamma(1)=\\text{Cov}(X_t, X_{t-1})$. Using $X_{t-1}=\\epsilon_{t-1}+\\theta \\epsilon_{t-2}$,\n$$\n\\gamma(1) = \\text{Cov}(\\epsilon_t + \\theta \\epsilon_{t-1}, \\epsilon_{t-1} + \\theta \\epsilon_{t-2}).\n$$\nExpanding covariance bilinearly,\n$$\n\\gamma(1) = \\text{Cov}(\\epsilon_t, \\epsilon_{t-1}) + \\theta \\text{Cov}(\\epsilon_t, \\epsilon_{t-2}) + \\theta \\text{Cov}(\\epsilon_{t-1}, \\epsilon_{t-1}) + \\theta^2 \\text{Cov}(\\epsilon_{t-1}, \\epsilon_{t-2}).\n$$\nBy the white noise properties, $\\text{Cov}(\\epsilon_t, \\epsilon_{t-1})=0$, $\\text{Cov}(\\epsilon_t, \\epsilon_{t-2})=0$, and $\\text{Cov}(\\epsilon_{t-1}, \\epsilon_{t-2})=0$, while $\\text{Cov}(\\epsilon_{t-1}, \\epsilon_{t-1}) = \\text{Var}(\\epsilon_{t-1})=\\sigma^2$. Hence\n$$\n\\gamma(1) = \\theta \\sigma^2 = \\frac{4}{5}\\sigma^2.\n$$\n\nTherefore, the lag-1 autocorrelation is\n$$\n\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\frac{\\frac{4}{5}\\sigma^2}{\\frac{41}{25}\\sigma^2} = \\frac{4}{5} \\cdot \\frac{25}{41} = \\frac{20}{41}.\n$$\nThis fraction is already in simplest form.", "answer": "$$\\boxed{\\frac{20}{41}}$$", "id": "1897209"}]}