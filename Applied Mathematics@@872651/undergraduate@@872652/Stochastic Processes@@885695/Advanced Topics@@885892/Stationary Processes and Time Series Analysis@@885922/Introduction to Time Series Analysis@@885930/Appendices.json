{"hands_on_practices": [{"introduction": "Before we can use a time series model for forecasting or analysis, we must ensure it is fundamentally stable. In time series analysis, this stability is captured by the concept of stationarity, which implies that the statistical properties of the process do not change over time. This exercise provides hands-on practice in testing an autoregressive (AR) model for stationarity by examining the roots of its characteristic polynomial, a critical first step in any valid modeling workflow. [@problem_id:1312127]", "problem": "An analyst is modeling a financial time series, denoted by a process $\\{X_t\\}$, using an Autoregressive (AR) model of order 2. An AR(2) process is generally defined by the equation $X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + Z_t$, where $Z_t$ is a white noise process with zero mean. For this particular model, the constant term $c$ is found to be zero, and the model is given by:\n\n$$X_t = X_{t-1} - 0.25 X_{t-2} + Z_t$$\n\nThe stability and properties of this model are determined by its characteristic polynomial, $P(z)$, which is derived from the autoregressive coefficients. A key property is stationarity, which requires all roots of the characteristic equation $P(z)=0$ to lie outside the unit circle in the complex plane (i.e., the magnitude of every root must be greater than 1).\n\nGiven the model, determine its characteristic polynomial and whether the process is stationary. Which of the following statements is correct?\n\nA. The characteristic polynomial is $P(z) = 1 - z + 0.25z^2$, and the process is stationary.\nB. The characteristic polynomial is $P(z) = 1 - z + 0.25z^2$, and the process is not stationary.\nC. The characteristic polynomial is $P(z) = 1 + z - 0.25z^2$, and the process is not stationary.\nD. The characteristic polynomial is $P(z) = z^2 - z + 0.25$, and the process is not stationary.", "solution": "For an AR(2) process $X_{t}=\\phi_{1}X_{t-1}+\\phi_{2}X_{t-2}+Z_{t}$, the autoregressive polynomial in the lag operator $B$ is $\\phi(B)=1-\\phi_{1}B-\\phi_{2}B^{2}$, obtained by rearranging to $\\phi(B)X_{t}=Z_{t}$. The characteristic polynomial in the complex variable $z$ is then $P(z)=1-\\phi_{1}z-\\phi_{2}z^{2}$.\n\nIn the given model $X_{t}=X_{t-1}-0.25\\,X_{t-2}+Z_{t}$, we identify $\\phi_{1}=1$ and $\\phi_{2}=-0.25$. Therefore,\n$$\nP(z)=1-\\phi_{1}z-\\phi_{2}z^{2}=1-z+0.25\\,z^{2}.\n$$\n\nTo assess stationarity, we solve $P(z)=0$ and require all roots to satisfy $|z|1$. Solve\n$$\n0.25\\,z^{2}-z+1=0.\n$$\nWith $a=0.25$, $b=-1$, $c=1$, the discriminant is\n$$\n\\Delta=b^{2}-4ac=1-4\\cdot 0.25\\cdot 1=0,\n$$\nso there is a repeated root\n$$\nz=\\frac{-b}{2a}=\\frac{1}{0.5}=2.\n$$\nSince $|2|1$, all roots lie outside the unit circle, and the process is stationary.\n\nThus the correct statement is that $P(z)=1-z+0.25z^{2}$ and the process is stationary.", "answer": "$$\\boxed{A}$$", "id": "1312127"}, {"introduction": "A key task in time series analysis is model identificationâ€”deducing the structure of an unknown process from observed data. The autocorrelation function (ACF), which describes how a series is correlated with its past values, provides a characteristic signature for different models. This problem challenges you to work like a real analyst: by using a known correlation value from the data to infer the underlying parameter of an AR(1) model, you will practice the essential skill of connecting empirical observations to theoretical model structure. [@problem_id:1925246]", "problem": "A researcher models the daily deviation of a certain atmospheric measurement, $X_t$, from its long-term average. The process is assumed to be stationary and can be described by a first-order Autoregressive (AR(1)) model:\n$$X_t = \\phi X_{t-1} + W_t$$\nHere, $\\phi$ is a constant parameter, and $W_t$ is a white noise process, meaning the $W_t$ are independent and identically distributed random variables with a mean of zero and a constant variance $\\sigma^2_W$. The noise $W_t$ is also uncorrelated with all past values of the process, i.e., $E[X_{t-k} W_t] = 0$ for all $k  0$.\n\nEmpirical analysis of the data reveals that the correlation between the measurement on a day $t$ and the measurement on day $t-2$ is exactly $1/4$.\n\nDetermine all possible real values of the parameter $\\phi$ that are consistent with this observation.", "solution": "We consider the stationary AR(1) process defined by $X_{t}=\\phi X_{t-1}+W_{t}$ with $\\{W_{t}\\}$ white noise, mean zero, variance $\\sigma_{W}^{2}$, and uncorrelated with the past $\\{X_{t-k}:k0\\}$. Let $\\gamma(k)=\\operatorname{Cov}(X_{t},X_{t-k})$ denote the autocovariance function and $\\rho(k)=\\gamma(k)/\\gamma(0)$ the autocorrelation function.\n\nTo derive the Yule-Walker relation, multiply both sides of the AR(1) equation by $X_{t-k}$ and take expectations for $k\\geq 1$:\n$$\n\\operatorname{E}[X_{t}X_{t-k}]=\\phi\\,\\operatorname{E}[X_{t-1}X_{t-k}]+\\operatorname{E}[W_{t}X_{t-k}].\n$$\nBy the assumed uncorrelatedness, $\\operatorname{E}[W_{t}X_{t-k}]=0$ for $k0$. Using stationarity, $\\operatorname{E}[X_{t}X_{t-k}]=\\gamma(k)$ and $\\operatorname{E}[X_{t-1}X_{t-k}]=\\gamma(k-1)$, which gives the recursion\n$$\n\\gamma(k)=\\phi\\,\\gamma(k-1), \\quad k\\geq 1.\n$$\nApplying this twice,\n$$\n\\gamma(2)=\\phi\\,\\gamma(1)=\\phi^{2}\\gamma(0),\n$$\nso the lag-2 autocorrelation is\n$$\n\\rho(2)=\\frac{\\gamma(2)}{\\gamma(0)}=\\phi^{2}.\n$$\nThe empirical observation is $\\rho(2)=\\frac{1}{4}$, hence\n$$\n\\phi^{2}=\\frac{1}{4} \\quad \\Longrightarrow \\quad \\phi=\\pm \\frac{1}{2}.\n$$\nThe stationarity condition for an AR(1) process is $|\\phi|1$, and both solutions satisfy this condition. Therefore, both values are admissible.", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{1}{2}  \\frac{1}{2}\\end{pmatrix}}$$", "id": "1925246"}, {"introduction": "Analysts often smooth time series data to reduce volatility and reveal underlying trends. However, this common preprocessing step is not statistically neutral and can introduce patterns that were not present in the original data. This practice demonstrates the Slutsky-Yule effect, a foundational concept showing that even a simple moving average can create artificial autocorrelation in a completely random series. Completing this exercise serves as a crucial cautionary tale about how data manipulation can inadvertently create the very signals we are seeking. [@problem_id:1925233]", "problem": "An analyst is studying a time series of daily asset returns, $\\{X_t\\}$, which they model as a white noise process. This means that for any integer time index $t$, the expected value of the return is $E[X_t] = 0$, the variance is $Var(X_t) = \\sigma^2$ for some constant $\\sigma^2  0$, and the returns on different days are uncorrelated, i.e., $Cov(X_t, X_s) = 0$ for $t \\neq s$.\n\nTo reduce the apparent day-to-day volatility, the analyst creates a new, smoothed time series $\\{S_t\\}$ by applying a simple two-period moving average, defined as:\n$$S_t = \\frac{1}{2}(X_t + X_{t-1})$$\nWhile this smoothing process is intended to make underlying patterns more visible, it can also introduce statistical dependencies into the new series that were not present in the original data. Your task is to quantify this effect.\n\nCalculate the theoretical autocorrelation of the smoothed series $\\{S_t\\}$ at lag 1.", "solution": "The goal is to calculate the autocorrelation of the time series $\\{S_t\\}$ at lag 1, denoted as $\\rho_S(1)$. The autocorrelation function at lag $k$ for a stationary process is defined as the ratio of its autocovariance at lag $k$ to its variance.\n$$ \\rho_S(k) = \\frac{\\gamma_S(k)}{\\gamma_S(0)} $$\nwhere $\\gamma_S(k) = Cov(S_t, S_{t-k})$ is the autocovariance function of $\\{S_t\\}$ and $\\gamma_S(0) = Var(S_t)$ is the variance of $\\{S_t\\}$. We need to compute $\\gamma_S(0)$ and $\\gamma_S(1)$.\n\nFirst, let's establish the properties of the original white noise process $\\{X_t\\}$. We are given:\n1.  $E[X_t] = 0$ for all $t$.\n2.  $Var(X_t) = E[X_t^2] - (E[X_t])^2 = E[X_t^2] = \\sigma^2$ for all $t$.\n3.  $Cov(X_t, X_s) = E[X_t X_s] - E[X_t]E[X_s] = E[X_t X_s] = 0$ for $t \\neq s$.\n\nThe smoothed series is defined as $S_t = \\frac{1}{2}(X_t + X_{t-1})$. Let's first check if $\\{S_t\\}$ is a zero-mean process.\n$$ E[S_t] = E\\left[\\frac{1}{2}(X_t + X_{t-1})\\right] = \\frac{1}{2}(E[X_t] + E[X_{t-1}]) = \\frac{1}{2}(0 + 0) = 0 $$\nSince the mean is zero and time-independent, and the structure of $S_t$ is time-invariant, we can proceed to calculate its variance and autocovariance.\n\nNext, we calculate the variance of $S_t$, which is the autocovariance at lag 0, $\\gamma_S(0)$.\n$$ \\gamma_S(0) = Var(S_t) = Var\\left(\\frac{1}{2}(X_t + X_{t-1})\\right) $$\nUsing the property $Var(aY) = a^2 Var(Y)$, we get:\n$$ \\gamma_S(0) = \\left(\\frac{1}{2}\\right)^2 Var(X_t + X_{t-1}) = \\frac{1}{4} Var(X_t + X_{t-1}) $$\nSince $X_t$ and $X_{t-1}$ are uncorrelated (as they are from a white noise process at different time steps), the variance of their sum is the sum of their variances:\n$$ Var(X_t + X_{t-1}) = Var(X_t) + Var(X_{t-1}) = \\sigma^2 + \\sigma^2 = 2\\sigma^2 $$\nSubstituting this back, we find the variance of $S_t$:\n$$ \\gamma_S(0) = \\frac{1}{4}(2\\sigma^2) = \\frac{\\sigma^2}{2} $$\n\nNow, we calculate the autocovariance of $S_t$ at lag 1, $\\gamma_S(1)$.\n$$ \\gamma_S(1) = Cov(S_t, S_{t-1}) $$\nSince $E[S_t]=0$, the covariance is simply the expected value of the product:\n$$ \\gamma_S(1) = E[S_t S_{t-1}] = E\\left[ \\left(\\frac{1}{2}(X_t + X_{t-1})\\right) \\left(\\frac{1}{2}(X_{t-1} + X_{t-2})\\right) \\right] $$\n$$ \\gamma_S(1) = \\frac{1}{4} E[(X_t + X_{t-1})(X_{t-1} + X_{t-2})] $$\nWe expand the product inside the expectation:\n$$ \\gamma_S(1) = \\frac{1}{4} E[X_t X_{t-1} + X_t X_{t-2} + X_{t-1}^2 + X_{t-1}X_{t-2}] $$\nUsing the linearity of the expectation operator:\n$$ \\gamma_S(1) = \\frac{1}{4} (E[X_t X_{t-1}] + E[X_t X_{t-2}] + E[X_{t-1}^2] + E[X_{t-1}X_{t-2}]) $$\nNow we use the properties of the white noise process $\\{X_t\\}$:\n-   $E[X_t X_{t-1}] = Cov(X_t, X_{t-1}) = 0$ (since $t \\neq t-1$)\n-   $E[X_t X_{t-2}] = Cov(X_t, X_{t-2}) = 0$ (since $t \\neq t-2$)\n-   $E[X_{t-1}^2] = Var(X_{t-1}) = \\sigma^2$\n-   $E[X_{t-1}X_{t-2}] = Cov(X_{t-1}, X_{t-2}) = 0$ (since $t-1 \\neq t-2$)\n\nSubstituting these values into the expression for $\\gamma_S(1)$:\n$$ \\gamma_S(1) = \\frac{1}{4} (0 + 0 + \\sigma^2 + 0) = \\frac{\\sigma^2}{4} $$\n\nFinally, we can calculate the autocorrelation at lag 1, $\\rho_S(1)$, by taking the ratio of the autocovariance at lag 1 to the variance.\n$$ \\rho_S(1) = \\frac{\\gamma_S(1)}{\\gamma_S(0)} = \\frac{\\frac{\\sigma^2}{4}}{\\frac{\\sigma^2}{2}} $$\n$$ \\rho_S(1) = \\frac{\\sigma^2}{4} \\cdot \\frac{2}{\\sigma^2} = \\frac{2}{4} = \\frac{1}{2} $$\nThe parameter $\\sigma^2$ cancels out, yielding a constant value for the autocorrelation at lag 1. This shows that the smoothing operation has indeed introduced a positive correlation between consecutive terms in the new series $\\{S_t\\}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1925233"}]}