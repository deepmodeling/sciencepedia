{"hands_on_practices": [{"introduction": "The foundation of any Monte Carlo method for Markov chains is the ability to generate a sample path. This exercise provides a direct, hands-on experience in simulating the step-by-step evolution of a simple Markov chain. By using a given set of random numbers, you will translate a transition probability matrix into a concrete sequence of states, which is the fundamental process underlying these simulation techniques [@problem_id:1319941].", "problem": "A simple generative model for creating sequences of characters uses a discrete-time Markov chain. Consider a system with three possible states: {A, B, C}. The behavior of the chain is governed by the following one-step transition probability matrix $P$, where the element $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$. The states are ordered alphabetically (A, B, C).\n\n$$\nP = \\begin{pmatrix} 0.1  0.6  0.3 \\\\ 0.5  0.0  0.5 \\\\ 0.2  0.2  0.6 \\end{pmatrix}\n$$\n\nYou are tasked with performing a Monte Carlo simulation to generate a 5-character sequence. The simulation starts in state A. For each subsequent step, a random number uniformly distributed in $[0, 1)$ determines the next state. To perform the simulation, use the following ordered sequence of four random numbers: $0.73, 0.41, 0.89, 0.15$.\n\nThe convention for determining the next state is as follows: If the current state is $i$, and the transition probabilities to states A, B, and C are $p_{iA}$, $p_{iB}$, and $p_{iC}$, then the next state is determined by the random number $u$ according to the rule:\n- State A if $0 \\le u  p_{iA}$\n- State B if $p_{iA} \\le u  p_{iA} + p_{iB}$\n- State C if $p_{iA} + p_{iB} \\le u  1$\n\nWhich of the following sequences is generated by this simulation?\n\nA. ABACA\n\nB. ACCCA\n\nC. ACBAC\n\nD. AAAAA\n\nE. ABCCB", "solution": "We start in state A and apply the given rule using cumulative transition intervals for each current state. For a row with probabilities $(p_{iA},p_{iB},p_{iC})$, the intervals for $u$ are $[0,p_{iA})$ for A, $[p_{iA},p_{iA}+p_{iB})$ for B, and $[p_{iA}+p_{iB},1)$ for C.\n\nFrom A, $(p_{AA},p_{AB},p_{AC})=(0.1,0.6,0.3)$, so the intervals are $[0,0.1)$ for A, $[0.1,0.7)$ for B, $[0.7,1)$ for C.\nFrom B, $(p_{BA},p_{BB},p_{BC})=(0.5,0.0,0.5)$, so the intervals are $[0,0.5)$ for A, $[0.5,0.5)$ for B, $[0.5,1)$ for C.\nFrom C, $(p_{CA},p_{CB},p_{CC})=(0.2,0.2,0.6)$, so the intervals are $[0,0.2)$ for A, $[0.2,0.4)$ for B, $[0.4,1)$ for C.\n\nStep 1: Start at A with $u_{1}=0.73$. Since $0.73\\in[0.7,1)$ for A’s row, the next state is C.\nStep 2: Current C with $u_{2}=0.41$. Since $0.41\\in[0.4,1)$ for C’s row, the next state is C.\nStep 3: Current C with $u_{3}=0.89$. Since $0.89\\in[0.4,1)$ for C’s row, the next state is C.\nStep 4: Current C with $u_{4}=0.15$. Since $0.15\\in[0,0.2)$ for C’s row, the next state is A.\n\nThus the 5-character sequence is A, C, C, C, A, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1319941"}, {"introduction": "In many real-world applications, we want to sample from a complex target distribution $\\pi$ that we can't sample from directly. The Metropolis-Hastings algorithm is a powerful tool for this, using a simpler 'proposal' distribution and a clever 'accept/reject' rule. This practice focuses on the heart of that rule: calculating the acceptance probability, a critical step that guarantees the resulting chain of samples correctly reflects the desired target distribution [@problem_id:1319963].", "problem": "An engineer is using a computational model to study a system that can exist in one of three discrete states: $S_1$, $S_2$, and $S_3$. The goal is to simulate the system's behavior according to a target stationary probability distribution $\\pi$. Due to the complexity of the system, direct sampling from $\\pi$ is intractable. Instead, the engineer employs the Metropolis-Hastings algorithm, a Markov Chain Monte Carlo method, to generate a sequence of states that will eventually be distributed according to $\\pi$.\n\nThe target distribution $\\pi$ is known up to a normalization constant, with the unnormalized weights for the states given as:\n$\\tilde{\\pi}(S_1) = 5$, $\\tilde{\\pi}(S_2) = 2$, and $\\tilde{\\pi}(S_3) = 8$.\n\nThe Metropolis-Hastings algorithm uses a proposal distribution, defined by a transition matrix $Q$, to suggest moves between states. For this system, the proposal matrix is:\n$$\nQ = \\begin{pmatrix}\n0.1  0.5  0.4 \\\\\n0.6  0.2  0.2 \\\\\n0.3  0.3  0.4\n\\end{pmatrix}\n$$\nwhere the entry $Q_{ij}$ represents the probability of proposing a move to state $S_j$ given the current state is $S_i$.\n\nSuppose the simulation is currently in state $S_3$, and the algorithm proposes a move to state $S_1$. Calculate the acceptance probability for this proposed move. Give your answer as a decimal rounded to three significant figures.", "solution": "In the Metropolis-Hastings algorithm with proposal kernel $Q$, the acceptance probability for a proposed move from state $i$ to state $j$ is\n$$\n\\alpha(i \\to j) = \\min\\!\\left(1,\\ \\frac{\\pi(j)\\,Q_{j,i}}{\\pi(i)\\,Q_{i,j}}\\right).\n$$\nWhen $\\pi$ is known up to a normalization constant, the normalized probabilities cancel, so we may use the unnormalized weights $\\tilde{\\pi}$:\n$$\n\\alpha(i \\to j) = \\min\\!\\left(1,\\ \\frac{\\tilde{\\pi}(j)\\,Q_{j,i}}{\\tilde{\\pi}(i)\\,Q_{i,j}}\\right).\n$$\nHere $i=3$ and $j=1$. Using $\\tilde{\\pi}(S_{1})=5$, $\\tilde{\\pi}(S_{3})=8$, $Q_{1,3}=0.4$, and $Q_{3,1}=0.3$, we compute\n$$\n\\frac{\\tilde{\\pi}(S_{1})\\,Q_{1,3}}{\\tilde{\\pi}(S_{3})\\,Q_{3,1}}=\\frac{5 \\cdot 0.4}{8 \\cdot 0.3}=\\frac{2}{2.4}=\\frac{5}{6}.\n$$\nThus,\n$$\n\\alpha(3 \\to 1)=\\min\\!\\left(1,\\frac{5}{6}\\right)=\\frac{5}{6}\\approx 0.833333\\ldots\n$$\nRounded to three significant figures, the acceptance probability is $0.833$.", "answer": "$$\\boxed{0.833}$$", "id": "1319963"}, {"introduction": "Running an MCMC sampler is not a 'fire-and-forget' process; it requires careful monitoring and a deep understanding of its potential pitfalls. This practice explores a classic failure mode where an MCMC sampler, despite appearing to run well with a high acceptance rate, fails to explore the full probability landscape. By analyzing this scenario, you will learn to critically assess sampler performance and diagnose problems like getting trapped in a local probability mode, a crucial skill for any practitioner [@problem_id:1932795].", "problem": "A data scientist is employing a Markov chain Monte Carlo (MCMC) method to draw samples from a complex, one-dimensional target probability density function, $\\pi(x)$. The target distribution is known to be a symmetric bimodal distribution, specifically an equal-weight mixture of two Gaussian distributions. The density is proportional to the sum of two Gaussian probability density functions:\n$$ \\pi(x) \\propto \\exp\\left(-\\frac{(x - \\mu_A)^2}{2\\sigma_{mode}^2}\\right) + \\exp\\left(-\\frac{(x - \\mu_B)^2}{2\\sigma_{mode}^2}\\right) $$\nThe parameters are given as $\\mu_A = -10$, $\\mu_B = 10$, and $\\sigma_{mode} = 1$. This structure results in two narrow, well-separated probability modes centered at $x=-10$ and $x=10$, with a region of extremely low probability density between them.\n\nThe scientist uses a random-walk Metropolis algorithm. At each step, a new state $x'$ is proposed from a Gaussian distribution centered at the current state $x_t$, i.e., $x' \\sim \\mathcal{N}(x_t, \\sigma_{step}^2)$. Seeking to achieve a high acceptance rate, the scientist chooses a very small step size variance, setting $\\sigma_{step} = 0.1$. The MCMC chain is initialized at the peak of one of the modes, $x_0 = -10$, and is run for $N=10^6$ iterations.\n\nWhich of the following statements most accurately describes the behavior of the MCMC sampler and the statistical properties of the resulting sample set $\\{x_1, x_2, \\dots, x_N\\}$?\n\nA. The samples will be distributed around the true mean of the target distribution, which is $x=0$. The sample mean will be close to 0, but the sample variance will be large (greater than 100), accurately reflecting the significant separation between the two modes.\n\nB. The acceptance rate of proposed moves will be very low (close to 0) because the step size is not well-tuned to the overall scale of the target distribution. The chain will remain at or very near its initial position, $x_0 = -10$.\n\nC. The acceptance rate of proposed moves will be very high (close to 1). The generated samples will thoroughly explore the region corresponding to the mode at $x=-10$, but the chain will fail to transition to the other mode at $x=10$. The sample mean will be approximately $-10$.\n\nD. The sampler will efficiently explore the entire state space. The chain will frequently jump back and forth between the two modes, and the histogram of the samples will correctly form two distinct peaks centered at $x=-10$ and $x=10$.\n\nE. The sampler will behave like a simple random walk, causing the samples to diffuse away from the starting point. The final collection of samples will be approximately uniformly distributed over a wide interval centered at $x=-10$.", "solution": "We model the target as an equal-weight mixture of two Gaussian densities with common standard deviation $\\sigma_{mode}$ and means $\\mu_{A}$ and $\\mu_{B}$. Up to proportionality,\n$$\n\\pi(x) \\propto \\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right) + \\exp\\!\\left(-\\frac{(x-\\mu_{B})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nWith a random-walk Metropolis sampler using a symmetric Gaussian proposal $q(x' \\mid x)=\\mathcal{N}(x,\\sigma_{step}^{2})$, the Metropolis–Hastings acceptance probability at state $x_{t}$ for a proposal $x'$ is\n$$\n\\alpha(x_{t},x')=\\min\\!\\left(1,\\frac{\\pi(x')}{\\pi(x_{t})}\\right).\n$$\n\nInitialize at $x_{0}=\\mu_{A}$. Because the modes are well separated, when $x$ is near $\\mu_{A}$ the contribution from the $\\mu_{B}$-component in $\\pi(x)$ is negligible relative to that from the $\\mu_{A}$-component. For a small proposal increment $\\epsilon:=x'-x$ with $|\\epsilon| \\ll \\sigma_{mode}$, the dominant-ratio approximation gives\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\frac{\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}{\\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}=\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}-(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nAt $x\\approx\\mu_{A}$, this simplifies for small $\\epsilon$ to\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\exp\\!\\left(-\\frac{\\epsilon^{2}}{2\\sigma_{mode}^{2}}\\right),\n$$\nso the acceptance probability is close to $1$ when $\\sigma_{step} \\ll \\sigma_{mode}$ because typical $|\\epsilon|$ is on the order of $\\sigma_{step}$. Hence the acceptance rate is very high while the chain explores the vicinity of the starting mode.\n\nA direct jump from the neighborhood of $\\mu_{A}$ to the neighborhood of $\\mu_{B}$ in one proposal requires a displacement of order $|\\mu_{B}-\\mu_{A}|$. Under a Gaussian proposal with variance $\\sigma_{step}^{2}$, the probability of such a jump is of order\n$$\n\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{2\\sigma_{step}^{2}}\\right),\n$$\nwhich is negligible when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{step}$.\n\nCrossing the low-density region via many small accepted steps is also overwhelmingly unlikely within a finite run because the stationary density in the valley is exponentially smaller than at the peak. At the midpoint $x^{\\star}=(\\mu_{A}+\\mu_{B})/2$, the target density is\n$$\n\\pi(x^{\\star}) \\propto 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right),\n$$\nwhile near $x=\\mu_{A}$ it is $\\pi(\\mu_{A}) \\propto 1$ (the other component there is negligible). Thus the ratio\n$$\n\\frac{\\pi(x^{\\star})}{\\pi(\\mu_{A})} \\approx 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right)\n$$\nis exponentially small when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{mode}$. This implies an exponentially large expected time to reach the valley or the other mode, on the order of the inverse of this ratio, which far exceeds the given $N$ when the separation is large and the proposal is very local.\n\nTherefore, with $\\sigma_{step}$ chosen very small relative to $\\sigma_{mode}$ and with well-separated modes, the chain has a very high acceptance rate, thoroughly explores the local basin around the starting mode at $x=\\mu_{A}$, essentially never transitions to the other mode within the run, and yields a sample mean approximately equal to $\\mu_{A}$. Among the options, this corresponds to statement C.", "answer": "$$\\boxed{C}$$", "id": "1932795"}]}