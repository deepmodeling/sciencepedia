{"hands_on_practices": [{"introduction": "One of the fundamental questions we can ask about a Hidden Markov Model is: given the model's parameters, what is the probability of observing a particular sequence of events? This is known as the \"Evaluation Problem.\" This exercise [@problem_id:1306030] will guide you through calculating this probability using the Forward Algorithm, a cornerstone of HMMs that systematically accounts for all possible hidden state paths that could have generated the observations.", "problem": "An industrial sensor is used to monitor the quality of components on an assembly line. The sensor's status, which is not directly observable, can be modeled as a system with two hidden states: 'Operational' and 'Malfunctioning'. At each time step, which corresponds to the inspection of one component, the sensor either remains in its current state or transitions to the other. The sensor outputs one of two signals for each component it inspects: 'Pass' or 'Defect'.\n\nThe behavior of this system can be described by a Hidden Markov Model (HMM) defined by the following parameters:\n\nLet the set of hidden states be $S = \\{S_{\\text{Op}}, S_{\\text{Mal}}\\}$, where $S_{\\text{Op}}$ represents the 'Operational' state and $S_{\\text{Mal}}$ represents the 'Malfunctioning' state.\n\n1.  **Initial State Probabilities ($\\pi$):** At the beginning of the observation period (time $t=1$), the probabilities of the sensor being in each state are:\n    *   $P(S_{\\text{Op}}) = 0.95$\n    *   $P(S_{\\text{Mal}}) = 0.05$\n\n2.  **State Transition Probability Matrix ($A$):** This matrix gives the probability of transitioning from one state to another between consecutive time steps.\n    $$\n    A = \n    \\begin{pmatrix}\n    P(S_{t+1}=S_{\\text{Op}} | S_t=S_{\\text{Op}}) & P(S_{t+1}=S_{\\text{Mal}} | S_t=S_{\\text{Op}}) \\\\\n    P(S_{t+1}=S_{\\text{Op}} | S_t=S_{\\text{Mal}}) & P(S_{t+1}=S_{\\text{Mal}} | S_t=S_{\\text{Mal}})\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    0.98 & 0.02 \\\\\n    0.15 & 0.85\n    \\end{pmatrix}\n    $$\n\n3.  **Observation Emission Probability Matrix ($B$):** This matrix gives the probability of observing a particular signal given the sensor's current hidden state.\n    $$\n    B = \n    \\begin{pmatrix}\n    P(\\text{'Pass'} | S_{\\text{Op}}) & P(\\text{'Defect'} | S_{\\text{Op}}) \\\\\n    P(\\text{'Pass'} | S_{\\text{Mal}}) & P(\\text{'Defect'} | S_{\\text{Mal}})\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    0.99 & 0.01 \\\\\n    0.20 & 0.80\n    \\end{pmatrix}\n    $$\n\nAn engineer observes the sequence of signals from the sensor for three consecutive components: ('Pass', 'Pass', 'Defect').\n\nCalculate the total probability of observing this specific sequence. Round your final answer to four significant figures.", "solution": "We model the observation sequence using the forward algorithm. Let $\\alpha_{t}(s) = P(o_{1:t}, S_{t}=s)$, where $o_{1:t}$ denotes the observations from time $1$ to $t$, and $S_{t}$ is the hidden state at time $t$. The recursions are:\n$$\n\\alpha_{1}(s) = \\pi(s)\\, b_{s}(o_{1}), \\quad\n\\alpha_{t+1}(s') = \\left[\\sum_{s \\in S} \\alpha_{t}(s)\\, a_{s \\to s'}\\right] b_{s'}(o_{t+1}),\n$$\nand the total probability of the observation sequence $o_{1:3} = (\\text{Pass}, \\text{Pass}, \\text{Defect})$ is\n$$\nP(o_{1:3}) = \\sum_{s \\in S} \\alpha_{3}(s).\n$$\n\nInitial step for $t=1$ with $o_{1}=\\text{Pass}$:\n$$\n\\alpha_{1}(S_{\\text{Op}}) = \\pi(S_{\\text{Op}})\\, b_{S_{\\text{Op}}}(\\text{Pass}) = 0.95 \\cdot 0.99 = 0.9405,\n$$\n$$\n\\alpha_{1}(S_{\\text{Mal}}) = \\pi(S_{\\text{Mal}})\\, b_{S_{\\text{Mal}}}(\\text{Pass}) = 0.05 \\cdot 0.20 = 0.01.\n$$\n\nInduction for $t=2$ with $o_{2}=\\text{Pass}$:\n$$\n\\alpha_{2}(S_{\\text{Op}}) = \\left[\\alpha_{1}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Op}} + \\alpha_{1}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Op}}\\right] b_{S_{\\text{Op}}}(\\text{Pass})\n= \\left[0.9405 \\cdot 0.98 + 0.01 \\cdot 0.15\\right] \\cdot 0.99\n= 0.92319 \\cdot 0.99 = 0.9139581,\n$$\n$$\n\\alpha_{2}(S_{\\text{Mal}}) = \\left[\\alpha_{1}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Mal}} + \\alpha_{1}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Mal}}\\right] b_{S_{\\text{Mal}}}(\\text{Pass})\n= \\left[0.9405 \\cdot 0.02 + 0.01 \\cdot 0.85\\right] \\cdot 0.20\n= 0.02731 \\cdot 0.20 = 0.005462.\n$$\n\nInduction for $t=3$ with $o_{3}=\\text{Defect}$:\n$$\n\\alpha_{3}(S_{\\text{Op}}) = \\left[\\alpha_{2}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Op}} + \\alpha_{2}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Op}}\\right] b_{S_{\\text{Op}}}(\\text{Defect})\n= \\left[0.9139581 \\cdot 0.98 + 0.005462 \\cdot 0.15\\right] \\cdot 0.01\n= 0.896498238 \\cdot 0.01 = 0.00896498238,\n$$\n$$\n\\alpha_{3}(S_{\\text{Mal}}) = \\left[\\alpha_{2}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Mal}} + \\alpha_{2}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Mal}}\\right] b_{S_{\\text{Mal}}}(\\text{Defect})\n= \\left[0.9139581 \\cdot 0.02 + 0.005462 \\cdot 0.85\\right] \\cdot 0.80\n= 0.022921862 \\cdot 0.80 = 0.0183374896.\n$$\n\nTotal probability of the sequence is\n$$\nP(\\text{Pass}, \\text{Pass}, \\text{Defect}) = \\alpha_{3}(S_{\\text{Op}}) + \\alpha_{3}(S_{\\text{Mal}}) = 0.00896498238 + 0.0183374896 = 0.02730247198.\n$$\n\nRounding to four significant figures gives $0.02730$.", "answer": "$$\\boxed{0.02730}$$", "id": "1306030"}, {"introduction": "While the Forward Algorithm calculates the total probability of an observation sequence, we often want to infer the specific sequence of hidden states that was most likely to have produced our observations. This is the \"Decoding Problem.\" This practice [@problem_id:1306022] introduces the Viterbi algorithm, a powerful dynamic programming approach for uncovering the single most probable hidden path, which is invaluable in applications from wildlife tracking to speech recognition.", "problem": "A wildlife biologist is studying the behavior of a solitary predator using a GPS collar. The biologist models the animal's behavior as a simple stochastic process with two unobservable internal states: `Hunting` and `Resting`. At the beginning of each hour, the animal is in exactly one of these two states. The collar, however, can only provide an observable signal indicating whether the animal is `Moving` or `Stationary`.\n\nThe parameters of the biologist's model are as follows:\n\n**1. Initial State Probabilities:**\nAt the very beginning of the observation period (at hour 1), the probabilities of the animal's initial state are:\n- Probability of being in the `Hunting` state: $0.2$\n- Probability of being in the `Resting` state: $0.8$\n\n**2. State Transition Probabilities (per hour):**\nThese probabilities describe how the animal's internal state changes from one hour to the next.\n- If the animal is `Hunting`, the probability it will be `Hunting` in the next hour is $0.7$.\n- If the animal is `Hunting`, the probability it will switch to `Resting` in the next hour is $0.3$.\n- If the animal is `Resting`, the probability it will remain `Resting` in the next hour is $0.9$.\n- If the animal is `Resting`, the probability it will switch to `Hunting` in the next hour is $0.1$.\n\n**3. Emission Probabilities:**\nThese probabilities link the unobservable internal state to the observable GPS signal.\n- If the animal is `Hunting`, the probability the collar reports `Moving` is $0.8$, and `Stationary` is $0.2$.\n- If the animal is `Resting`, the probability the collar reports `Moving` is $0.1$, and `Stationary` is $0.9$.\n\nOver a period of three hours, the biologist observes the following sequence of signals from the collar: `(Moving, Stationary, Moving)`.\n\nGiven this sequence of observations, determine the single most probable sequence of the animal's internal states for these three hours. Choose from the options below.\n\nA. (`Hunting`, `Hunting`, `Hunting`)\n\nB. (`Hunting`, `Resting`, `Hunting`)\n\nC. (`Resting`, `Resting`, `Hunting`)\n\nD. (`Resting`, `Resting`, `Resting`)\n\nE. (`Hunting`, `Resting`, `Resting`)", "solution": "We have a hidden Markov model with hidden states Hunting (H) and Resting (R), observations Moving (M) and Stationary (St), initial probabilities $\\pi_{H}=0.2$, $\\pi_{R}=0.8$, transition probabilities $a_{HH}=0.7$, $a_{HR}=0.3$, $a_{RH}=0.1$, $a_{RR}=0.9$, and emission probabilities $b_{H}(M)=0.8$, $b_{H}(\\text{St})=0.2$, $b_{R}(M)=0.1$, $b_{R}(\\text{St})=0.9$. The observed sequence is $(M,\\text{St},M)$. We apply the Viterbi algorithm, which uses the recursion\n$$\n\\delta_{t}(i)=b_{i}(o_{t})\\max_{j}\\left[\\delta_{t-1}(j)a_{ji}\\right],\\quad\n\\psi_{t}(i)=\\arg\\max_{j}\\left[\\delta_{t-1}(j)a_{ji}\\right],\n$$\nwith initialization $\\delta_{1}(i)=\\pi_{i}b_{i}(o_{1})$.\n\nInitialization at $t=1$ for $o_{1}=M$:\n$$\n\\delta_{1}(H)=\\pi_{H}b_{H}(M)=0.2\\cdot 0.8=0.16,\\quad\n\\delta_{1}(R)=\\pi_{R}b_{R}(M)=0.8\\cdot 0.1=0.08.\n$$\n\nRecursion at $t=2$ for $o_{2}=\\text{St}$:\n$$\n\\delta_{2}(H)=b_{H}(\\text{St})\\max\\{\\delta_{1}(H)a_{HH},\\ \\delta_{1}(R)a_{RH}\\}\n=0.2\\max\\{0.16\\cdot 0.7,\\ 0.08\\cdot 0.1\\}\n=0.2\\cdot 0.112=0.0224,\n$$\nwith backpointer $\\psi_{2}(H)=H$ since $0.112>0.008$.\n$$\n\\delta_{2}(R)=b_{R}(\\text{St})\\max\\{\\delta_{1}(H)a_{HR},\\ \\delta_{1}(R)a_{RR}\\}\n=0.9\\max\\{0.16\\cdot 0.3,\\ 0.08\\cdot 0.9\\}\n=0.9\\cdot 0.072=0.0648,\n$$\nwith backpointer $\\psi_{2}(R)=R$ since $0.072>0.048$.\n\nRecursion at $t=3$ for $o_{3}=M$:\n$$\n\\delta_{3}(H)=b_{H}(M)\\max\\{\\delta_{2}(H)a_{HH},\\ \\delta_{2}(R)a_{RH}\\}\n=0.8\\max\\{0.0224\\cdot 0.7,\\ 0.0648\\cdot 0.1\\}\n=0.8\\cdot 0.01568=0.012544,\n$$\nwith backpointer $\\psi_{3}(H)=H$ since $0.01568>0.00648$.\n$$\n\\delta_{3}(R)=b_{R}(M)\\max\\{\\delta_{2}(H)a_{HR},\\ \\delta_{2}(R)a_{RR}\\}\n=0.1\\max\\{0.0224\\cdot 0.3,\\ 0.0648\\cdot 0.9\\}\n=0.1\\cdot 0.05832=0.005832,\n$$\nwith backpointer $\\psi_{3}(R)=R$ since $0.05832>0.00672$.\n\nTermination and backtracking: since $\\delta_{3}(H)=0.012544>\\delta_{3}(R)=0.005832$, the most probable final state is $H$. Tracing back through the pointers gives $\\psi_{3}(H)=H$ and $\\psi_{2}(H)=H$, so the most probable state sequence is $(H,H,H)$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1306022"}, {"introduction": "A true understanding of a model comes not just from applying its algorithms, but from exploring its theoretical boundaries and special cases. This thought experiment [@problem_id:1305982] challenges you to analyze an HMM with a highly specific transition matrix. By determining how this constraint simplifies the model's output, you will gain deeper insight into the crucial role the transition probabilities play in creating dependencies between states and, consequently, between observations.", "problem": "Consider a Hidden Markov Model (HMM), a statistical model used in applications like speech recognition and bioinformatics. An HMM is specified by a tuple $(\\mathcal{S}, \\mathcal{V}, A, B, \\pi)$, where:\n- $\\mathcal{S} = \\{s_1, s_2, \\dots, s_N\\}$ is the set of $N$ hidden states.\n- $\\mathcal{V} = \\{v_1, v_2, \\dots, v_M\\}$ is the set of $M$ possible observations.\n- $A$ is the $N \\times N$ state transition probability matrix, where $A_{ij} = P(q_t = s_j | q_{t-1} = s_i)$ is the probability of transitioning from state $s_i$ to state $s_j$.\n- $B$ is the $N \\times M$ emission probability matrix, where $B_{jk} = P(o_t = v_k | q_t = s_j)$ is the probability of observing $v_k$ when the system is in state $s_j$.\n- $\\pi$ is the initial state probability distribution, where $\\pi_i = P(q_1 = s_i)$.\n\nA data scientist is analyzing a particular HMM and finds that its transition matrix $A$ has a peculiar property: all of its rows are identical. Let this common row be denoted by the probability vector $\\mathbf{p} = (p_1, p_2, \\dots, p_N)$, where $p_j > 0$ for all $j$ and $\\sum_{j=1}^{N} p_j = 1$. Thus, for any pair of states $(s_i, s_j)$, the transition probability is $A_{ij} = p_j$. Furthermore, the initial state distribution $\\pi$ is also found to be equal to this vector, i.e., $\\pi_j = p_j$ for all $j=1, \\dots, N$.\n\nUnder these specific conditions, the stochastic process that generates the sequence of observations $(o_1, o_2, o_3, \\dots, o_T)$ degenerates into a much simpler, well-known type of process. Which of the following options best describes the resulting process for the sequence of observations?\n\nA. A simple Markov chain where each observation depends only on the previous observation.\n\nB. A Martingale process.\n\nC. A Poisson process.\n\nD. A sequence of independent and identically distributed (i.i.d.) random variables.\n\nE. A Bernoulli process.", "solution": "Let $q_{t} \\in \\mathcal{S}$ denote the hidden state at time $t$ and $o_{t} \\in \\mathcal{V}$ the observation at time $t$. The HMM specifies\n$$\nP(q_{1} = s_{j}) = \\pi_{j} = p_{j}, \\quad P(q_{t} = s_{j} \\mid q_{t-1} = s_{i}) = A_{ij} = p_{j},\n$$\nand\n$$\nP(o_{t} = v_{k} \\mid q_{t} = s_{j}) = B_{jk}.\n$$\n\nFirst, we derive the distribution and independence structure of the hidden states. For any sequence $(i_{1}, i_{2}, \\dots, i_{T})$ with $i_{t} \\in \\{1, \\dots, N\\}$,\n$$\nP(q_{1} = s_{i_{1}}, \\dots, q_{T} = s_{i_{T}}) = P(q_{1} = s_{i_{1}}) \\prod_{t=2}^{T} P(q_{t} = s_{i_{t}} \\mid q_{t-1} = s_{i_{t-1}}).\n$$\nSubstituting the given $\\pi$ and $A$,\n$$\nP(q_{1} = s_{i_{1}}, \\dots, q_{T} = s_{i_{T}}) = p_{i_{1}} \\prod_{t=2}^{T} p_{i_{t}} = \\prod_{t=1}^{T} p_{i_{t}}.\n$$\nThus, $(q_{t})_{t=1}^{T}$ are independent and identically distributed (i.i.d.) with\n$$\nP(q_{t} = s_{j}) = p_{j} \\quad \\text{for all } t.\n$$\n\nNext, we examine the observations. Conditional on the hidden states, the observations are independent with\n$$\nP(o_{t} = v_{k} \\mid q_{t} = s_{j}) = B_{jk}.\n$$\nThe marginal distribution of each $o_{t}$ is obtained by summing over the hidden state:\n$$\nP(o_{t} = v_{k}) = \\sum_{j=1}^{N} P(o_{t} = v_{k} \\mid q_{t} = s_{j}) P(q_{t} = s_{j}) = \\sum_{j=1}^{N} B_{jk} p_{j}.\n$$\nThis expression does not depend on $t$, so the $o_{t}$ are identically distributed.\n\nTo establish independence of the observations, compute the joint distribution for any fixed $(k_{1}, \\dots, k_{T})$:\n$$\nP(o_{1} = v_{k_{1}}, \\dots, o_{T} = v_{k_{T}}) = \\sum_{i_{1}=1}^{N} \\cdots \\sum_{i_{T}=1}^{N} P(q_{1} = s_{i_{1}}, \\dots, q_{T} = s_{i_{T}}) \\prod_{t=1}^{T} P(o_{t} = v_{k_{t}} \\mid q_{t} = s_{i_{t}}).\n$$\nUsing the factorization derived for the states,\n$$\nP(o_{1} = v_{k_{1}}, \\dots, o_{T} = v_{k_{T}}) = \\sum_{i_{1}=1}^{N} \\cdots \\sum_{i_{T}=1}^{N} \\prod_{t=1}^{T} \\left( p_{i_{t}} B_{i_{t} k_{t}} \\right) = \\prod_{t=1}^{T} \\left( \\sum_{j=1}^{N} p_{j} B_{j k_{t}} \\right).\n$$\nHence,\n$$\nP(o_{1} = v_{k_{1}}, \\dots, o_{T} = v_{k_{T}}) = \\prod_{t=1}^{T} P(o_{t} = v_{k_{t}}),\n$$\nwhich shows that $(o_{t})_{t=1}^{T}$ are independent. Combined with identical marginals, the observation sequence is i.i.d., with observation distribution given by the mixture\n$$\nP(o_{t} = v_{k}) = \\sum_{j=1}^{N} p_{j} B_{jk}.\n$$\n\nAmong the options, this corresponds to a sequence of independent and identically distributed random variables. Option E (a Bernoulli process) is a special case only when $M=2$, whereas the conclusion holds for general $M$, so the best description is Option D.", "answer": "$$\\boxed{D}$$", "id": "1305982"}]}