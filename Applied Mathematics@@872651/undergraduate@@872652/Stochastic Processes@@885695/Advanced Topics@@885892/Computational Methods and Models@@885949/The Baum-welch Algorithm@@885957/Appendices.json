{"hands_on_practices": [{"introduction": "The Baum-Welch algorithm hinges on first understanding the probability of observing a data sequence given the model. The forward algorithm provides an efficient way to do this as part of the Expectation-step. This first practice focuses on the fundamental recursive calculation of the forward variables, allowing you to compute the joint probability of an observation sequence and the system's hidden state at a given time. [@problem_id:1336466]", "problem": "A simplified financial market model is described using a Hidden Markov Model (HMM). The market can be in one of two hidden states: $S = \\{\\text{Bull}, \\text{Bear}\\}$. At the end of each day, an observable market movement is recorded, which can be either 'Up' or 'Down'.\n\nThe components of the HMM are defined as follows:\n\n1.  **Initial State Probability Distribution ($\\pi$)**: The probabilities of the market starting in each state.\n    *   $P(\\text{state at day 1 is Bull}) = \\pi_{\\text{Bull}} = 0.6$\n    *   $P(\\text{state at day 1 is Bear}) = \\pi_{\\text{Bear}} = 0.4$\n\n2.  **State Transition Probability Matrix ($A$)**: The probability of moving from one state to another on consecutive days.\n    *   $P(\\text{state}_{t+1}=\\text{Bull} | \\text{state}_t=\\text{Bull}) = a_{\\text{Bull},\\text{Bull}} = 0.7$\n    *   $P(\\text{state}_{t+1}=\\text{Bear} | \\text{state}_t=\\text{Bull}) = a_{\\text{Bull},\\text{Bear}} = 0.3$\n    *   $P(\\text{state}_{t+1}=\\text{Bull} | \\text{state}_t=\\text{Bear}) = a_{\\text{Bear},\\text{Bull}} = 0.4$\n    *   $P(\\text{state}_{t+1}=\\text{Bear} | \\text{state}_t=\\text{Bear}) = a_{\\text{Bear},\\text{Bear}} = 0.6$\n\n3.  **Observation Emission Probability Matrix ($B$)**: The probability of observing a certain market movement given the current hidden state.\n    *   $P(\\text{observation}=\\text{Up} | \\text{state}=\\text{Bull}) = b_{\\text{Bull}}(\\text{Up}) = 0.8$\n    *   $P(\\text{observation}=\\text{Down} | \\text{state}=\\text{Bull}) = b_{\\text{Bull}}(\\text{Down}) = 0.2$\n    *   $P(\\text{observation}=\\text{Up} | \\text{state}=\\text{Bear}) = b_{\\text{Bear}}(\\text{Up}) = 0.3$\n    *   $P(\\text{observation}=\\text{Down} | \\text{state}=\\text{Bear}) = b_{\\text{Bear}}(\\text{Down}) = 0.7$\n\nAn analyst observes the following sequence of market movements over two days: $Y = (\\text{Up}, \\text{Down})$.\n\nThe forward variable, $\\alpha_t(i)$, is defined as the joint probability of observing the partial sequence of observations up to time $t$ and being in state $i$ at time $t$. For the first day, given the observation 'Up', the forward variables have been calculated as:\n*   $\\alpha_1(\\text{Bull}) = 0.480$\n*   $\\alpha_1(\\text{Bear}) = 0.120$\n\nUsing the provided information, calculate the value of the forward variable $\\alpha_2(\\text{Bull})$. Round your final answer to four significant figures.", "solution": "The forward algorithm uses the recursion\n$$\n\\alpha_{t}(i)=\\left[\\sum_{j\\in S}\\alpha_{t-1}(j)\\,a_{j,i}\\right]\\,b_{i}(o_{t}),\n$$\nwhere $o_{t}$ is the observation at time $t$. For $t=2$, the observation is Down and we seek $\\alpha_{2}(\\text{Bull})$. Substituting the given values,\n$$\n\\alpha_{2}(\\text{Bull})=\\left[\\alpha_{1}(\\text{Bull})\\,a_{\\text{Bull},\\text{Bull}}+\\alpha_{1}(\\text{Bear})\\,a_{\\text{Bear},\\text{Bull}}\\right]\\,b_{\\text{Bull}}(\\text{Down}).\n$$\nCompute the sum inside the brackets:\n$$\n\\alpha_{1}(\\text{Bull})\\,a_{\\text{Bull},\\text{Bull}}+\\alpha_{1}(\\text{Bear})\\,a_{\\text{Bear},\\text{Bull}}=0.480\\cdot 0.7+0.120\\cdot 0.4=0.336+0.048=0.384.\n$$\nNow multiply by the emission probability for Down in the Bull state:\n$$\n\\alpha_{2}(\\text{Bull})=0.384\\cdot 0.2=0.0768.\n$$\nRounding to four significant figures gives $0.07680$.", "answer": "$$\\boxed{0.07680}$$", "id": "1336466"}, {"introduction": "Once the E-step provides the expected state visitations, the Maximization-step updates the model parameters to better fit the data. This exercise isolates the M-step, providing you with the necessary outputs from the E-step (the $\\gamma$ probabilities). Your task is to apply the re-estimation formula to update an emission probability, offering a clear look at how the HMM \"learns\" from observations. [@problem_id:1336492]", "problem": "A computational linguist is using a Hidden Markov Model (HMM) to analyze the structure of a simplified language. The model is designed with two hidden states: $S_V$ (a state that tends to generate vowels) and $S_C$ (a state that tends to generate consonants). The linguist uses an observed sequence of letters, $O = o_1, o_2, \\ldots, o_T$, to train the HMM's parameters via the Baum-Welch algorithm.\n\nThe Baum-Welch algorithm is an iterative procedure. After completing the Expectation-step (E-step) of the first iteration, the linguist calculates the quantity $\\gamma_t(i)$, which represents the probability of being in a specific state $i$ at time $t$, given the entire observed sequence and the initial model parameters.\n\nFrom these calculations for the vowel state $S_V$, two aggregate values are computed:\n1.  The total expected number of times the model is in state $S_V$, which is the sum of $\\gamma_t(S_V)$ over all time steps in the sequence: $\\sum_{t=1}^{T} \\gamma_t(S_V) = 20.0$.\n2.  The total expected number of times the model is in state $S_V$ and the observed letter is 'e', which is the sum of $\\gamma_t(S_V)$ over all time steps $t$ where the observation $o_t$ was the letter 'e': $\\sum_{t | o_t=\\text{'e'}} \\gamma_t(S_V) = 12.5$.\n\nBased on these values, perform the Maximization-step (M-step) of the Baum-Welch algorithm to find the updated emission probability for observing the letter 'e' from the vowel state $S_V$. Let this updated probability be denoted by $b'_{S_V}(\\text{'e'})$.\n\nCalculate the exact decimal value of $b'_{S_V}(\\text{'e'})$.", "solution": "In the Baum-Welch algorithm, the M-step updates the emission probability from state $i$ of observing symbol $v$ by the ratio of the expected number of times state $i$ emits $v$ to the expected total number of times state $i$ is visited. The update formula is\n$$\nb'_{i}(v)=\\frac{\\sum_{t: o_{t}=v}\\gamma_{t}(i)}{\\sum_{t=1}^{T}\\gamma_{t}(i)}.\n$$\nApplying this to the vowel state $S_{V}$ and the symbol $\\text{'e'}$, with the provided E-step aggregates,\n$$\nb'_{S_{V}}(\\text{'e'})=\\frac{\\sum_{t: o_{t}=\\text{'e'}}\\gamma_{t}(S_{V})}{\\sum_{t=1}^{T}\\gamma_{t}(S_{V})}=\\frac{12.5}{20.0}.\n$$\nSimplifying,\n$$\n\\frac{12.5}{20.0}=\\frac{125}{200}=\\frac{5}{8}=0.625.\n$$\nTherefore, the updated emission probability for observing $\\text{'e'}$ from $S_{V}$ is $0.625$.", "answer": "$$\\boxed{0.625}$$", "id": "1336492"}, {"introduction": "Theoretical formulas can encounter issues with real-world data, which is often sparse. If a specific transition or emission never occurs in your training set, the standard algorithm might assign it a zero probability, which is often undesirable. This final practice introduces Laplace smoothing to address this issue and walks you through a complete, more robust iteration of the Baum-Welch algorithm. [@problem_id:1336464]", "problem": "A simplified Hidden Markov Model (HMM) is used to model the behavior of an automated \"Moody Weather Bot\" that posts daily weather summaries. The bot has two hidden internal states (its \"mood\") and can post one of three types of summaries.\n\nThe parameters of the HMM are defined as follows:\n-   **Hidden States ($S$)**: A set of $N=2$ states, $S = \\{S_1, S_2\\}$, where $S_1$ is 'Happy' and $S_2$ is 'Sad'.\n-   **Observation Symbols ($V$)**: A set of $M=3$ observation symbols, $V = \\{V_1, V_2, V_3\\}$, where $V_1$ is 'Sunny', $V_2$ is 'Rainy', and $V_3$ is 'Cloudy'.\n-   **Initial State Probabilities ($\\pi$)**: The probability distribution of the initial state.\n    $$ \\pi = [\\pi_i] = \\begin{pmatrix} P(q_1=S_1) \\\\ P(q_1=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} $$\n-   **Transition Probability Matrix ($A$)**: The probability of transitioning from state $S_i$ to state $S_j$.\n    $$ A = [a_{ij}] = \\begin{pmatrix} P(q_t=S_1|q_{t-1}=S_1) & P(q_t=S_2|q_{t-1}=S_1) \\\\ P(q_t=S_1|q_{t-1}=S_2) & P(q_t=S_2|q_{t-1}=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.7 & 0.3 \\\\ 0.4 & 0.6 \\end{pmatrix} $$\n-   **Emission Probability Matrix ($B$)**: The probability of observing symbol $V_k$ given the system is in state $S_j$.\n    $$ B = [b_j(k)] = \\begin{pmatrix} P(O_t=V_1|q_t=S_1) & P(O_t=V_2|q_t=S_1) & P(O_t=V_3|q_t=S_1) \\\\ P(O_t=V_1|q_t=S_2) & P(O_t=V_2|q_t=S_2) & P(O_t=V_3|q_t=S_2) \\end{pmatrix} = \\begin{pmatrix} 0.8 & 0.1 & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\end{pmatrix} $$\n\nYou are given a short observation sequence of length $T=3$:\n$$ O = (O_1, O_2, O_3) = (\\text{'Sunny'}, \\text{'Rainy'}, \\text{'Sunny'}) $$\nwhich corresponds to the symbol sequence $(V_1, V_2, V_1)$.\n\nYour task is to perform a single iteration of the Baum-Welch algorithm to find the re-estimated model parameters, $\\bar{A}$ and $\\bar{B}$. For this task, you must incorporate Laplace smoothing (also known as add-one smoothing) into the re-estimation formulas of the M-step to account for sparse data. The smoothed re-estimation formulas are:\n$$ \\bar{a}_{ij} = \\frac{(\\text{expected number of transitions from } S_i \\text{ to } S_j) + 1}{(\\text{expected number of transitions from } S_i) + N} $$\n$$ \\bar{b}_{j}(k) = \\frac{(\\text{expected number of times observing } V_k \\text{ in state } S_j) + 1}{(\\text{expected number of times in state } S_j) + M} $$\n\nCalculate the updated values for the transition probability $\\bar{a}_{12}$ (the probability of transitioning from 'Happy' to 'Sad') and the emission probability $\\bar{b}_{2}(3)$ (the probability of observing 'Cloudy' while in the 'Sad' state).\n\nProvide your final answers for $\\bar{a}_{12}$ and $\\bar{b}_{2}(3)$ respectively, rounded to three significant figures.", "solution": "We perform one Baum-Welch iteration with Laplace smoothing on the given HMM for the observation sequence $O=(V_{1},V_{2},V_{1})$.\n\nForward variables $\\alpha_{t}(i)$ are defined by $\\alpha_{1}(i)=\\pi_{i} b_{i}(O_{1})$ and, for $t\\geq 2$, $\\alpha_{t}(j)=\\left(\\sum_{i=1}^{N}\\alpha_{t-1}(i)a_{ij}\\right)b_{j}(O_{t})$. Using $\\pi=(0.5,0.5)$, $A=\\begin{pmatrix}0.7&0.3\\\\0.4&0.6\\end{pmatrix}$, $B=\\begin{pmatrix}0.8&0.1&0.1\\\\0.2&0.6&0.2\\end{pmatrix}$, and $O=(V_{1},V_{2},V_{1})$, we compute:\n$$\n\\alpha_{1}(1)=0.5\\cdot 0.8=0.4,\\quad \\alpha_{1}(2)=0.5\\cdot 0.2=0.1,\n$$\n$$\n\\alpha_{2}(1)=(0.4\\cdot 0.7+0.1\\cdot 0.4)\\cdot 0.1=0.032,\\quad \\alpha_{2}(2)=(0.4\\cdot 0.3+0.1\\cdot 0.6)\\cdot 0.6=0.108,\n$$\n$$\n\\alpha_{3}(1)=(0.032\\cdot 0.7+0.108\\cdot 0.4)\\cdot 0.8=0.05248,\\quad \\alpha_{3}(2)=(0.032\\cdot 0.3+0.108\\cdot 0.6)\\cdot 0.2=0.01488.\n$$\nThus $P(O)=\\sum_{j}\\alpha_{3}(j)=0.06736$.\n\nBackward variables $\\beta_{t}(i)$ satisfy $\\beta_{3}(i)=1$ and, for $t\\leq T-1$, $\\beta_{t}(i)=\\sum_{j=1}^{N}a_{ij} b_{j}(O_{t+1})\\beta_{t+1}(j)$. Hence:\n$$\n\\beta_{2}(1)=0.7\\cdot 0.8+0.3\\cdot 0.2=0.62,\\quad \\beta_{2}(2)=0.4\\cdot 0.8+0.6\\cdot 0.2=0.44,\n$$\n$$\n\\beta_{1}(1)=0.7\\cdot 0.1\\cdot 0.62+0.3\\cdot 0.6\\cdot 0.44=0.1226,\\quad \\beta_{1}(2)=0.4\\cdot 0.1\\cdot 0.62+0.6\\cdot 0.6\\cdot 0.44=0.1832.\n$$\n\nThe state posterior $\\gamma_{t}(i)=\\frac{\\alpha_{t}(i)\\beta_{t}(i)}{P(O)}$ gives:\n$$\n\\gamma_{1}(1)=\\frac{0.04904}{0.06736}=\\frac{613}{842},\\quad \\gamma_{1}(2)=\\frac{0.01832}{0.06736}=\\frac{229}{842},\n$$\n$$\n\\gamma_{2}(1)=\\frac{0.01984}{0.06736}=\\frac{124}{421},\\quad \\gamma_{2}(2)=\\frac{0.04752}{0.06736}=\\frac{297}{421},\n$$\n$$\n\\gamma_{3}(1)=\\frac{0.05248}{0.06736}=\\frac{328}{421},\\quad \\gamma_{3}(2)=\\frac{0.01488}{0.06736}=\\frac{93}{421}.\n$$\n\nThe pairwise posterior $\\xi_{t}(i,j)=\\frac{\\alpha_{t}(i)a_{ij}b_{j}(O_{t+1})\\beta_{t+1}(j)}{P(O)}$ yields the expected transitions from $S_{1}$ to $S_{2}$:\n$$\n\\xi_{1}(1,2)=\\frac{0.4\\cdot 0.3\\cdot 0.6\\cdot 0.44}{0.06736}=\\frac{198}{421},\\quad\n\\xi_{2}(1,2)=\\frac{0.032\\cdot 0.3\\cdot 0.2\\cdot 1}{0.06736}=\\frac{12}{421},\n$$\nso the expected number of $S_{1}\\to S_{2}$ transitions is\n$$\n\\sum_{t=1}^{2}\\xi_{t}(1,2)=\\frac{198}{421}+\\frac{12}{421}=\\frac{210}{421}.\n$$\nThe expected number of transitions from $S_{1}$ (i.e., occupancy of $S_{1}$ at $t=1,2$) is\n$$\n\\sum_{t=1}^{2}\\gamma_{t}(1)=\\frac{613}{842}+\\frac{124}{421}=\\frac{861}{842}.\n$$\n\nWith Laplace smoothing, for $N=2$, the re-estimated transition probability is\n$$\n\\bar{a}_{12}=\\frac{\\left(\\sum_{t=1}^{2}\\xi_{t}(1,2)\\right)+1}{\\left(\\sum_{t=1}^{2}\\gamma_{t}(1)\\right)+N}\n=\\frac{\\frac{210}{421}+1}{\\frac{861}{842}+2}\n=\\frac{\\frac{631}{421}}{\\frac{2545}{842}}\n=\\frac{1262}{2545}\\approx 0.49587\\;\\Rightarrow\\;0.496\\text{ (three significant figures)}.\n$$\n\nFor emissions, the expected number of times observing $V_{3}$ in state $S_{2}$ is\n$$\n\\sum_{t=1}^{3}\\mathbb{I}\\{O_{t}=V_{3}\\}\\gamma_{t}(2)=0,\n$$\nwhile the expected total time in state $S_{2}$ is\n$$\n\\sum_{t=1}^{3}\\gamma_{t}(2)=\\frac{229}{842}+\\frac{297}{421}+\\frac{93}{421}=\\frac{1009}{842}.\n$$\nWith Laplace smoothing and $M=3$,\n$$\n\\bar{b}_{2}(3)=\\frac{0+1}{\\left(\\sum_{t=1}^{3}\\gamma_{t}(2)\\right)+M}\n=\\frac{1}{\\frac{1009}{842}+3}\n=\\frac{842}{3535}\\approx 0.238190\\;\\Rightarrow\\;0.238\\text{ (three significant figures)}.\n$$\nTherefore, the updated values are $\\bar{a}_{12}\\approx 0.496$ and $\\bar{b}_{2}(3)\\approx 0.238$.", "answer": "$$\\boxed{\\begin{pmatrix} 0.496 & 0.238 \\end{pmatrix}}$$", "id": "1336464"}]}