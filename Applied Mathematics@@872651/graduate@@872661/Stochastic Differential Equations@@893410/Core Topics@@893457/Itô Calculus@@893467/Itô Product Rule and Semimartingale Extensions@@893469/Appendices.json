{"hands_on_practices": [{"introduction": "Mastering the generalized Itô formula requires a solid grasp of the continuous case. This foundational exercise guides you to derive the predictable quadratic variation of a Brownian integral, $\\langle M \\rangle_t$, using only the Itô product rule and the uniqueness of the Doob-Meyer decomposition. This practice solidifies the core mechanics of quadratic variation before we introduce the complexities of jumps. [@problem_id:2982685]", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t \\geq 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions and carrying a standard one-dimensional Brownian motion $B=(B_{t})_{t \\geq 0}$. Let $H=(H_{t})_{t \\geq 0}$ be a real-valued predictable process such that for every $T0$ one has $\\mathbb{E}\\!\\left[\\int_{0}^{T} H_{s}^{2}\\,ds\\right]\\infty$. Define the stochastic integral\n$$\nM_{t} := \\int_{0}^{t} H_{s}\\,dB_{s}, \\quad t \\geq 0,\n$$\nwhich is a square-integrable continuous martingale.\n\nUsing only foundational tools for continuous semimartingales, namely Itô's product rule, properties of Brownian motion increments, and the Doob–Meyer decomposition (DMD) for submartingales, determine the predictable quadratic variation process $\\langle M\\rangle = (\\langle M\\rangle_{t})_{t \\geq 0}$ in closed form as a function of $t$. Your final answer must be a single analytical expression depending on $t$ and $H$. Do not assume any specialized formulas for $\\langle M\\rangle$ beyond what can be derived from these principles.", "solution": "The problem asks for the predictable quadratic variation process, denoted $\\langle M \\rangle = (\\langle M\\rangle_{t})_{t \\geq 0}$, of the continuous martingale $M_{t} := \\int_{0}^{t} H_{s}\\,dB_{s}$. The process $\\langle M \\rangle$ is defined by the Doob–Meyer decomposition (DMD) of the submartingale $M_{t}^{2}$. Specifically, $M_{t}^{2}$ can be uniquely decomposed as the sum of a continuous local martingale and a continuous, non-decreasing, predictable process of finite variation starting at zero. This latter process is $\\langle M \\rangle_{t}$. Our strategy will be to find this decomposition explicitly.\n\nFirst, we apply Itô's product rule for continuous semimartingales, which states that for two such processes $X$ and $Y$, the differential of their product is $d(X_t Y_t) = X_t dY_t + Y_t dX_t + d\\langle X, Y \\rangle_t$. Let's apply this with $X_{t} = Y_{t} = M_{t}$.\nThe process $M_t$ is a continuous semimartingale (in fact, a continuous local martingale). Applying the rule gives:\n$$\nd(M_{t}^{2}) = M_{t} dM_{t} + M_{t} dM_{t} + d\\langle M, M \\rangle_{t} = 2M_{t} dM_{t} + d\\langle M \\rangle_{t}\n$$\nIntegrating this from $t=0$ to a generic time $t \\geq 0$, we get:\n$$\nM_{t}^{2} - M_{0}^{2} = \\int_{0}^{t} 2M_{s}\\,dM_{s} + \\int_{0}^{t} d\\langle M \\rangle_{s}\n$$\nFrom the definition of $M_t$, we have $M_{0} = \\int_{0}^{0} H_s\\,dB_s = 0$. By definition, the quadratic variation process starts at zero, so $\\langle M \\rangle_{0} = 0$. The equation simplifies to:\n$$\nM_{t}^{2} = 2\\int_{0}^{t} M_{s}\\,dM_{s} + \\langle M \\rangle_{t}\n$$\nThe integral $\\int_{0}^{t} M_{s}\\,dM_{s}$ is a stochastic integral with respect to the continuous local martingale $M_s$. The integrand $M_s$ is an adapted process. Therefore, the process $N_{t}^{(1)} := 2\\int_{0}^{t} M_{s}\\,dM_{s}$ is a continuous local martingale. The process $A_{t}^{(1)} := \\langle M \\rangle_{t}$ is, by definition, a continuous, non-decreasing, and predictable process of finite variation starting at $0$. Thus, the equation $M_{t}^{2} = N_{t}^{(1)} + A_{t}^{(1)}$ provides one such decomposition for the submartingale $M_{t}^{2}$.\n\nNext, we establish a second decomposition for $M_{t}^{2}$ using the properties of Brownian motion. Let us define a process $A_{t}^{(2)} := \\int_{0}^{t} H_{s}^{2}\\,ds$.\nSince $H_s$ is a predictable process, $H_s^2$ is also a predictable process. The integral of a non-negative predictable process with respect to Lebesgue measure yields a continuous, non-decreasing, and predictable process. Since $H_s^2 \\ge 0$, $A_{t}^{(2)}$ is non-decreasing. The condition $\\mathbb{E}[\\int_{0}^{T} H_s^2 ds]  \\infty$ implies that $\\int_{0}^{T} H_s^2 ds  \\infty$ almost surely, ensuring $A_t^{(2)}$ is continuous for $t \\in [0, T]$. Finally, as an integral of a predictable process, $A_t^{(2)}$ is itself predictable. Thus, $A_{t}^{(2)}$ is a valid candidate for the predictable part in a Doob-Meyer decomposition.\n\nNow, consider the process $N_{t}^{(2)} := M_{t}^{2} - A_{t}^{(2)} = M_{t}^{2} - \\int_{0}^{t} H_{s}^{2}\\,ds$. We will show that $N_{t}^{(2)}$ is a continuous martingale. It is continuous because both $M_t^2$ and $A_t^{(2)}$ are continuous. To show it is a martingale, we need to show that for any $0 \\leq s  t$, $\\mathbb{E}[N_{t}^{(2)} - N_{s}^{(2)} | \\mathcal{F}_{s}] = 0$.\nThis is equivalent to showing $\\mathbb{E}[M_{t}^{2} - M_{s}^{2} | \\mathcal{F}_{s}] = \\mathbb{E}[\\int_{s}^{t} H_{u}^{2}\\,du | \\mathcal{F}_{s}]$.\nWe have $M_{t} - M_{s} = \\int_{s}^{t} H_{u}\\,dB_{u}$. Since $M_s$ is $\\mathcal{F}_s$-measurable and $\\mathbb{E}[M_t - M_s | \\mathcal{F}_s] = 0$ (martingale property of $M_t$), we calculate:\n$$\n\\mathbb{E}[M_{t}^{2} - M_{s}^{2} | \\mathcal{F}_{s}] = \\mathbb{E}[(M_{t} - M_{s} + M_{s})^{2} - M_{s}^{2} | \\mathcal{F}_{s}] = \\mathbb{E}[(M_{t} - M_{s})^{2} + 2M_s(M_t - M_s) + M_s^2 - M_s^2 | \\mathcal{F}_{s}]\n$$\n$$\n= \\mathbb{E}[(M_{t} - M_{s})^{2} | \\mathcal{F}_{s}] + 2M_{s}\\mathbb{E}[M_{t} - M_{s} | \\mathcal{F}_{s}] = \\mathbb{E}\\left[\\left(\\int_{s}^{t} H_{u}\\,dB_{u}\\right)^{2} \\Big| \\mathcal{F}_{s}\\right]\n$$\nThis is the conditional Itô isometry. By a standard approximation argument (starting with simple predictable processes), this can be shown to be equal to $\\mathbb{E}[\\int_s^t H_u^2 \\,du | \\mathcal{F}_s]$. Since $H_u$ is predictable for $u > s$, it is \"known\" given $\\mathcal{F}_s$ in a limiting sense, which simplifies the expression. The Itô isometry states that $\\mathbb{E}[(\\int_s^t H_u dB_u)^2] = \\mathbb{E}[\\int_s^t H_u^2 du]$. A similar conditional argument holds, confirming that $\\mathbb{E}[M_{t}^{2} - M_{s}^{2} | \\mathcal{F}_{s}] = \\mathbb{E}[\\int_{s}^{t} H_{u}^{2}\\,du | \\mathcal{F}_{s}]$. For a predictable integrand $H$, $\\mathbb{E}[\\int_{s}^{t} H_{u}^{2}\\,du | \\mathcal{F}_{s}] = \\int_{s}^{t} \\mathbb{E}[H_{u}^{2} | \\mathcal{F}_{s}] \\,du$. However, a more direct proof shows $\\mathbb{E}[(\\int_s^t H_u dB_u)^2 | \\mathcal{F}_s] = \\mathbb{E}[\\int_s^t H_u^2 du | \\mathcal{F}_s]$. Thus, we have confirmed that $N_t^{(2)} = M_t^2 - \\int_0^t H_s^2 ds$ is a continuous martingale.\n\nWe now have two decompositions for the continuous submartingale $M_{t}^{2}$:\n1. $M_{t}^{2} = N_{t}^{(1)} + A_{t}^{(1)} = \\left(2\\int_{0}^{t} M_{s}\\,dM_{s}\\right) + \\langle M \\rangle_{t}$\n2. $M_{t}^{2} = N_{t}^{(2)} + A_{t}^{(2)} = \\left(M_{t}^{2} - \\int_{0}^{t} H_{s}^{2}\\,ds\\right) + \\left(\\int_{0}^{t} H_{s}^{2}\\,ds\\right)$\n\nIn both cases, we have decomposed $M_t^2$ into a continuous local martingale ($N^{(1)}$, $N^{(2)}$) and a continuous, non-decreasing, predictable process ($A^{(1)}$, $A^{(2)}$). The Doob–Meyer decomposition for a continuous submartingale is unique. Therefore, the corresponding components of these two decompositions must be equal. Comparing the predictable, non-decreasing parts, we must have $A_{t}^{(1)} = A_{t}^{(2)}$.\n$$\n\\langle M \\rangle_{t} = \\int_{0}^{t} H_{s}^{2}\\,ds\n$$\nThis result provides the closed-form expression for the predictable quadratic variation process of the Itô integral $M_t$.", "answer": "$$\\boxed{\\int_{0}^{t} H_{s}^{2}\\,ds}$$", "id": "2982685"}, {"introduction": "Having established the framework for continuous processes, we now turn to the world of jumps. The quadratic covariation $[X, Y]_t$ captures the coupled variation of two processes, but what happens when they are driven by independent sources of randomness? This problem provides a clear and insightful answer by examining two martingales driven by independent Poisson random measures, demonstrating a key principle of orthogonality in stochastic calculus. [@problem_id:2982635]", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P}\\right)$ be a filtered probability space carrying two independent Poisson random measures $N^{(1)}$ on $\\mathbb{R}_{+}\\times E_{1}$ and $N^{(2)}$ on $\\mathbb{R}_{+}\\times E_{2}$ with compensators $\\mathrm{d}t\\,\\nu_{1}(\\mathrm{d}z)$ and $\\mathrm{d}t\\,\\nu_{2}(\\mathrm{d}w)$, respectively, where $E_{1},E_{2}\\subset \\mathbb{R}\\setminus\\{0\\}$ are Borel sets and $\\nu_{1},\\nu_{2}$ are $\\sigma$-finite measures satisfying $\\int_{E_{1}} z^{2}\\,\\nu_{1}(\\mathrm{d}z)\\infty$ and $\\int_{E_{2}} w^{2}\\,\\nu_{2}(\\mathrm{d}w)\\infty$. Let $\\tilde{N}^{(1)}:=N^{(1)}-\\mathrm{d}t\\,\\nu_{1}$ and $\\tilde{N}^{(2)}:=N^{(2)}-\\mathrm{d}t\\,\\nu_{2}$ denote the compensated Poisson random measures. Define the $\\mathbb{R}$-valued processes\n$$\nX_{t}:=\\int_{0}^{t}\\int_{E_{1}} z\\,\\tilde{N}^{(1)}(\\mathrm{d}s,\\mathrm{d}z),\\qquad\nY_{t}:=\\int_{0}^{t}\\int_{E_{2}} w\\,\\tilde{N}^{(2)}(\\mathrm{d}s,\\mathrm{d}w),\\qquad t\\geq 0.\n$$\nUsing only fundamental definitions for semimartingales with jumps, compute the quadratic covariation process $[X,Y]_{t}$ for $t\\geq 0$ and briefly justify how independence is reflected via the cross-bracket. Your final answer must be given as a single simplified analytic expression in $t$ (if any dependence on $t$ remains). Do not present an equation; report only the expression.", "solution": "The problem asks for the computation of the quadratic covariation process $[X,Y]_{t}$ for the processes $X_t$ and $Y_t$ defined as stochastic integrals with respect to compensated Poisson random measures.\n\nLet the given processes be\n$$\nX_{t}:=\\int_{0}^{t}\\int_{E_{1}} z\\,\\tilde{N}^{(1)}(\\mathrm{d}s,\\mathrm{d}z)\n$$\nand\n$$\nY_{t}:=\\int_{0}^{t}\\int_{E_{2}} w\\,\\tilde{N}^{(2)}(\\mathrm{d}s,\\mathrm{d}w)\n$$\nfor $t\\geq 0$. The conditions $\\int_{E_{1}} z^{2}\\,\\nu_{1}(\\mathrm{d}z)\\infty$ and $\\int_{E_{2}} w^{2}\\,\\nu_{2}(\\mathrm{d}w)\\infty$ ensure that $X_t$ and $Y_t$ are square-integrable martingales. As martingales are a specific class of semimartingales (with the finite variation part being zero), $X_t$ and $Y_t$ are semimartingales.\n\nFurthermore, because they are constructed from Poisson random measures, $X_t$ and $Y_t$ are pure-jump processes. Their evolution is entirely determined by the accumulation of jumps. For any semimartingale $Z_t$, its jump at time $s$ is denoted by $\\Delta Z_s := Z_s - Z_{s-}$.\n\nA fundamental definition of the quadratic covariation $[X,Y]_t$ for two pure-jump semimartingales $X_t$ and $Y_t$ is given by the sum of the products of their simultaneous jumps up to time $t$:\n$$\n[X,Y]_{t} = \\sum_{0  s \\le t} \\Delta X_s \\Delta Y_s\n$$\nWe must therefore identify the jumps of $X_t$ and $Y_t$.\n\nThe process $X_t$ is an integral with respect to the compensated Poisson random measure $\\tilde{N}^{(1)} = N^{(1)} - \\mathrm{d}t\\,\\nu_1$. The compensator $\\mathrm{d}t\\,\\nu_1$ is a deterministic and continuous process in $t$, so it does not contribute to the jumps of $X_t$. The jumps of $X_t$ are sourced entirely from the Poisson random measure $N^{(1)}$. A jump of $X_t$ occurs at time $s$ if and only if the measure $N^{(1)}$ has an atom at time $s$, i.e., $N^{(1)}(\\{s\\}\\times E_1) > 0$. If $N^{(1)}$ has an atom at $(s,z)$, the jump in $X_t$ at time $s$ is $\\Delta X_s = z$.\n\nSimilarly, the jumps of $Y_t$ are sourced from the Poisson random measure $N^{(2)}$. A jump of $Y_t$ occurs at time $s$ if and only if $N^{(2)}$ has an atom at time $s$, i.e., $N^{(2)}(\\{s\\}\\times E_2) > 0$. If $N^{(2)}$ has an atom at $(s,w)$, the jump in $Y_t$ at time $s$ is $\\Delta Y_s = w$.\n\nFor a term $\\Delta X_s \\Delta Y_s$ in the sum defining $[X,Y]_t$ to be non-zero, it is necessary that both $\\Delta X_s \\neq 0$ and $\\Delta Y_s \\neq 0$. This would require both $N^{(1)}$ and $N^{(2)}$ to have an atom at the exact same time $s$.\n\nThe problem states that the Poisson random measures $N^{(1)}$ and $N^{(2)}$ are independent. The set of jump times for $X_t$, let's call it $J_X = \\{ s>0 : \\Delta X_s \\neq 0 \\}$, is the set of times $s$ where $N^{(1)}(\\{s\\} \\times E_1) > 0$. This set forms a Poisson point process on $\\mathbb{R}_+$. Likewise, the set of jump times for $Y_t$, $J_Y = \\{ s>0 : \\Delta Y_s \\neq 0 \\}$, forms another Poisson point process. Due to the independence of $N^{(1)}$ and $N^{(2)}$, these two point processes, $J_X$ and $J_Y$, are independent.\n\nThe compensators of $N^{(1)}$ and $N^{(2)}$ are given as $\\mathrm{d}t\\,\\nu_1(\\mathrm{d}z)$ and $\\mathrm{d}t\\,\\nu_2(\\mathrm{d}w)$, respectively. The temporal component of these compensators is the Lebesgue measure $\\mathrm{d}t$. This implies that the intensity measures for the jump-time processes $J_X$ and $J_Y$ are absolutely continuous with respect to the Lebesgue measure on $\\mathbb{R}_+$. Such processes do not have fixed (i.e., deterministic) jump times. A fundamental property of two independent point processes with diffuse (continuous) intensity measures is that the probability of them having a common point is zero. In other words, the probability of $X_t$ and $Y_t$ having a simultaneous jump is zero:\n$$\n\\mathbb{P}(J_X \\cap J_Y \\neq \\emptyset) = 0\n$$\nThis means that, almost surely, for any time $s > 0$, at least one of $\\Delta X_s$ or $\\Delta Y_s$ must be zero. Consequently, their product $\\Delta X_s \\Delta Y_s$ is almost surely zero for all $s > 0$.\n\nTherefore, the sum over all jump times must also be zero:\n$$\n[X,Y]_t = \\sum_{0  s \\le t} \\Delta X_s \\Delta Y_s = \\sum_{0  s \\le t} 0 = 0\n$$\nThis holds for all $t \\geq 0$, almost surely.\n\nThe independence of the driving noise sources, $N^{(1)}$ and $N^{(2)}$, is directly reflected in the quadratic covariation (or \"cross-bracket\"). The independence implies that the jump events for $X_t$ and $Y_t$ occur at disjoint sets of times. Since the quadratic covariation for pure-jump processes is defined as the sum of products of simultaneous jump sizes, the absence of simultaneous jumps forces the quadratic covariation to be identically zero. In the language of martingale theory, this means that $X_t$ and $Y_t$ are orthogonal martingales.", "answer": "$$\n\\boxed{0}\n$$", "id": "2982635"}, {"introduction": "The previous exercise demonstrated that independent jump processes are orthogonal. We now explore the opposite and more common scenario in financial modeling: processes subject to simultaneous shocks. This practice bridges theory and application by quantifying the systematic error, or bias, that arises when a numerical scheme for a product of two jump-diffusion processes ignores their joint jump component. By calculating this bias, you will gain a tangible understanding of the importance of the term $\\sum \\Delta X_s \\Delta Y_s$ in the Itô product rule. [@problem_id:2982652]", "problem": "Consider a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})$ supporting the following processes:\n- Two standard Brownian motions $W$ and $B$ with instantaneous correlation $\\rho \\in (-1,1)$, that is $d\\langle W, B \\rangle_t = \\rho \\, dt$.\n- A Poisson process $N$ with constant intensity $\\lambda  0$.\n- An independent and identically distributed sequence of $\\mathbb{R}^2$-valued jump sizes $(U_i, V_i)_{i \\in \\mathbb{N}}$ with $\\mathbb{E}[|U|]  \\infty$, $\\mathbb{E}[|V|]  \\infty$, and $\\mathbb{E}[|U V|]  \\infty$, independent of $(W,B)$ and $N$.\n\nDefine two jump-diffusion semimartingales $(X_t)_{t \\geq 0}$ and $(Y_t)_{t \\geq 0}$ by\n$$\nX_t = X_0 + \\int_0^t \\mu_X \\, ds + \\int_0^t \\sigma_X \\, dW_s + \\sum_{i=1}^{N_t} U_i,\n\\qquad\nY_t = Y_0 + \\int_0^t \\mu_Y \\, ds + \\int_0^t \\sigma_Y \\, dB_s + \\sum_{i=1}^{N_t} V_i,\n$$\nwhere $\\mu_X, \\mu_Y \\in \\mathbb{R}$ and $\\sigma_X, \\sigma_Y  0$ are constants. The jump component $\\sum_{i=1}^{N_t} (U_i, V_i)$ represents simultaneous jumps (common shocks) in both $X$ and $Y$.\n\nLet $T  0$ be a fixed finite time horizon and let $\\pi = \\{0 = t_0  t_1  \\dots  t_n = T\\}$ be any partition of $[0,T]$ with mesh $|\\pi| = \\max_k (t_{k+1} - t_k)$. Consider the discretization of the product $X_T Y_T$ that omits the jump cross-term, defined by\n$$\n\\widehat{(XY)}_T := X_0 Y_0 + \\sum_{k=0}^{n-1} X_{t_k} \\big( Y_{t_{k+1}} - Y_{t_k} \\big) + \\sum_{k=0}^{n-1} Y_{t_k} \\big( X_{t_{k+1}} - X_{t_k} \\big) + [X^c, Y^c]_T,\n$$\nwhere $[X^c, Y^c]_T$ denotes the quadratic covariation of the continuous local martingale parts of $X$ and $Y$ over $[0,T]$.\n\nDefine the bias over $[0,T]$ by\n$$\nB_T := \\mathbb{E}\\big[ X_T Y_T - \\widehat{(XY)}_T \\big].\n$$\n\nStarting only from the semimartingale decomposition and the definition of quadratic covariation, prove that ignoring the jump cross-term $\\Delta X \\, \\Delta Y$ yields a bias that is first-order in the horizon $T$. Then, compute $B_T$ in closed form in terms of $\\lambda$ and the jump-size moments. Your final answer must be a single closed-form analytic expression for $B_T$.", "solution": "The goal is to compute the bias $B_T = \\mathbb{E}\\big[ X_T Y_T - \\widehat{(XY)}_T \\big]$. We start with the exact expression for the product $X_T Y_T$ given by the Itô product rule for semimartingales.\n\n1.  **Exact Product using Itô's Rule:**\n    For two semimartingales $X$ and $Y$, the product rule in integral form is:\n    $$\n    X_T Y_T = X_0 Y_0 + \\int_0^T X_{s-} \\,dY_s + \\int_0^T Y_{s-} \\,dX_s + [X, Y]_T\n    $$\n    The quadratic covariation $[X, Y]_T$ can be decomposed into its continuous and jump parts:\n    $$\n    [X, Y]_T = [X^c, Y^c]_T + [X^d, Y^d]_T = [X^c, Y^c]_T + \\sum_{0  s \\le T} \\Delta X_s \\Delta Y_s\n    $$\n    where $X^c, Y^c$ are the continuous local martingale parts and $X^d, Y^d$ are the pure jump parts.\n    Substituting this into the product formula gives:\n    $$\n    X_T Y_T = X_0 Y_0 + \\int_0^T X_{s-} \\,dY_s + \\int_0^T Y_{s-} \\,dX_s + [X^c, Y^c]_T + \\sum_{0  s \\le T} \\Delta X_s \\Delta Y_s\n    $$\n\n2.  **Analyzing the Approximation:**\n    The problem defines the approximation $\\widehat{(XY)}_T$ as:\n    $$\n    \\widehat{(XY)}_T := X_0 Y_0 + \\sum_{k=0}^{n-1} X_{t_k} \\big( Y_{t_{k+1}} - Y_{t_k} \\big) + \\sum_{k=0}^{n-1} Y_{t_k} \\big( X_{t_{k+1}} - X_{t_k} \\big) + [X^c, Y^c]_T\n    $$\n    The two summation terms are Riemann-Stieltjes sums. As the mesh of the partition $|\\pi| \\to 0$, these sums converge in probability to the corresponding Itô integrals where the integrands are the predictable left-limit processes:\n    $$\n    \\lim_{|\\pi| \\to 0} \\sum_{k=0}^{n-1} X_{t_k} \\big( Y_{t_{k+1}} - Y_{t_k} \\big) = \\int_0^T X_{s-} \\,dY_s\n    $$\n    $$\n    \\lim_{|\\pi| \\to 0} \\sum_{k=0}^{n-1} Y_{t_k} \\big( X_{t_{k+1}} - X_{t_k} \\big) = \\int_0^T Y_{s-} \\,dX_s\n    $$\n    Thus, in the limit, the approximation represents the Itô product rule but without the jump component of the quadratic covariation.\n\n3.  **Calculating the Error Term:**\n    The error (or difference) between the true product and the limit of the approximation is:\n    $$\n    X_T Y_T - \\lim_{|\\pi| \\to 0} \\widehat{(XY)}_T = \\sum_{0  s \\le T} \\Delta X_s \\Delta Y_s\n    $$\n    This term is precisely the contribution to the quadratic covariation from the simultaneous jumps of $X$ and $Y$. The bias $B_T$ is the expectation of this difference. Assuming sufficient integrability to pass the limit through the expectation, we have:\n    $$\n    B_T = \\mathbb{E}\\left[ \\sum_{0  s \\le T} \\Delta X_s \\Delta Y_s \\right]\n    $$\n\n4.  **Computing the Expectation:**\n    The jumps of $X_t$ and $Y_t$ are driven by the compound Poisson process. Jumps only occur at the event times $T_i$ of the Poisson process $N_t$. The jump sizes at time $T_i$ are given by:\n    $$\n    \\Delta X_{T_i} = U_i \\quad \\text{and} \\quad \\Delta Y_{T_i} = V_i\n    $$\n    Therefore, the sum of the products of simultaneous jumps is:\n    $$\n    \\sum_{0  s \\le T} \\Delta X_s \\Delta Y_s = \\sum_{i=1}^{N_T} U_i V_i\n    $$\n    The bias is the expectation of this random sum:\n    $$\n    B_T = \\mathbb{E}\\left[ \\sum_{i=1}^{N_T} U_i V_i \\right]\n    $$\n    We can compute this using the law of total expectation (or Wald's Identity), since the number of jumps $N_T$ is independent of the i.i.d. jump sizes $(U_i, V_i)$:\n    $$\n    B_T = \\mathbb{E}[N_T] \\cdot \\mathbb{E}[U_i V_i]\n    $$\n    Let $\\mathbb{E}[UV]$ denote the common expectation $\\mathbb{E}[U_i V_i]$ for any $i$. The process $N_t$ is a Poisson process with constant intensity $\\lambda$, so the expected number of jumps by time $T$ is:\n    $$\n    \\mathbb{E}[N_T] = \\lambda T\n    $$\n    Substituting this into our expression for the bias, we get the final closed-form result:\n    $$\n    B_T = \\lambda T \\, \\mathbb{E}[UV]\n    $$\n    This proves that the bias is first-order in the horizon $T$, as it is directly proportional to $T$. The constant of proportionality is the product of the jump frequency ($\\lambda$) and the expected product of the jump sizes ($\\mathbb{E}[UV]$).", "answer": "$$ \\boxed{\\lambda T \\, \\mathbb{E}[UV]} $$", "id": "2982652"}]}