{"hands_on_practices": [{"introduction": "The Contraction Principle is a cornerstone of Large Deviation Theory, allowing us to derive the rate function for a transformed random variable if the original's is known. This practice provides a fundamental application of this principle, starting from the well-known Schilder's theorem for Brownian motion [@problem_id:2968426]. You will calculate the rate function for the time-averaged velocity of a path, which solidifies the connection between the abstract principle and concrete variational problems.", "problem": "Let $T0$ be fixed and let $\\{W(t)\\}_{t \\in [0,T]}$ be a standard Wiener process. Consider the small-noise family $\\{X^{\\varepsilon}(t)\\}_{t \\in [0,T]}$ defined by $X^{\\varepsilon}(t) = \\varepsilon W(t)$ for $\\varepsilon  0$, viewed as random variables in the path space $C([0,T];\\mathbb{R})$ equipped with the uniform topology. By Schilder's theorem, the family $\\{X^{\\varepsilon}\\}_{\\varepsilon0}$ satisfies a Large Deviation Principle (LDP) with speed $\\varepsilon^{-2}$ and good rate function given by the Schilder action $S_{T}(\\varphi)$, namely\n$$\nS_{T}(\\varphi) \\;=\\; \\begin{cases}\n\\frac{1}{2}\\displaystyle\\int_{0}^{T}|\\dot{\\varphi}(t)|^{2}\\,dt,  \\text{if $\\varphi$ is absolutely continuous, $\\dot{\\varphi}\\in L^{2}([0,T])$, and $\\varphi(0)=0$,} \\\\\n+\\infty,  \\text{otherwise,}\n\\end{cases}\n$$\nwhere the derivative $\\dot{\\varphi}$ is understood in the sense of the Cameron–Martin space.\n\nDefine the real-valued observable $A_{T}$ on paths by\n$$\nA_{T}(\\varphi)\\;=\\;\\frac{1}{T}\\int_{0}^{T}\\dot{\\varphi}(t)\\,dt.\n$$\nUsing the Contraction Principle (CP) for large deviations, derive the LDP for the family $\\{A_{T}(X^{\\varepsilon})\\}_{\\varepsilon0}$ on $\\mathbb{R}$, and compute the corresponding good rate function $J_{T}(v)$ for $v\\in\\mathbb{R}$ explicitly as a closed-form analytic expression.\n\nYour final answer must be a single analytic expression for $J_{T}(v)$ as a function of $v$ and $T$.", "solution": "The problem is valid. It is a well-posed question within the mathematical framework of Large Deviation Principle (LDP) theory, using standard definitions and theorems such as Schilder's theorem and the Contraction Principle. All necessary information is provided, and the problem is free of scientific flaws or ambiguity.\n\nThe objective is to derive the good rate function for the family of real-valued random variables $\\{A_{T}(X^{\\varepsilon})\\}_{\\varepsilon0}$ using the Contraction Principle. The family of path-valued random variables $\\{X^{\\varepsilon}\\}_{\\varepsilon0}$ in $C([0,T];\\mathbb{R})$ is given to satisfy an LDP with speed $\\varepsilon^{-2}$ and good rate function $S_T(\\varphi)$. The observable $A_T$ is a mapping from the path space $C([0,T];\\mathbb{R})$ to the real line $\\mathbb{R}$.\n\nThe Contraction Principle states that if a family of random variables $\\{X^{\\varepsilon}\\}$ on a topological space $\\mathcal{X}$ satisfies an LDP with rate function $I(x)$, and $F: \\mathcal{X} \\to \\mathcal{Y}$ is a continuous map to another topological space $\\mathcal{Y}$, then the family of random variables $\\{F(X^{\\varepsilon})\\}$ satisfies an LDP on $\\mathcal{Y}$ with the same speed, and its good rate function $J(y)$ is given by:\n$$\nJ(y) = \\inf_{x \\in \\mathcal{X} : F(x)=y} I(x)\n$$\nIn our case, the space $\\mathcal{X}$ is $C([0,T];\\mathbb{R})$, the rate function is $I(\\varphi) = S_{T}(\\varphi)$, the map is $F = A_{T}$, the space $\\mathcal{Y}$ is $\\mathbb{R}$, and the variable in $\\mathcal{Y}$ is $v$. The map $A_T$ needs to be continuous for the principle to apply. The domain of paths $\\varphi$ for which $S_T(\\varphi)  +\\infty$ are absolutely continuous functions starting at zero, i.e., $\\varphi(0)=0$. For such paths, the integral in the definition of $A_T$ can be evaluated using the fundamental theorem of calculus:\n$$\nA_{T}(\\varphi) = \\frac{1}{T}\\int_{0}^{T}\\dot{\\varphi}(t)\\,dt = \\frac{\\varphi(T)-\\varphi(0)}{T} = \\frac{\\varphi(T)}{T}\n$$\nThe mapping $\\varphi \\mapsto \\varphi(T)$ is the evaluation functional at time $T$, which is continuous on $C([0,T];\\mathbb{R})$ equipped with the uniform (sup-norm) topology. Therefore, $A_T$ is a continuous map, and the Contraction Principle can be applied.\n\nThe rate function $J_{T}(v)$ for the family $\\{A_{T}(X^{\\varepsilon})\\}$ is thus given by the formula:\n$$\nJ_{T}(v) = \\inf_{\\varphi \\in C([0,T];\\mathbb{R}) : A_{T}(\\varphi)=v} S_{T}(\\varphi)\n$$\nWe need to find this infimum. The infimum is taken over all paths $\\varphi$ for which $A_T(\\varphi)=v$. If this set of paths contains no path for which $S_T(\\varphi)$ is finite, the infimum is $+\\infty$. Otherwise, we are looking for:\n$$\nJ_{T}(v) = \\inf \\left\\{ \\frac{1}{2}\\int_{0}^{T}|\\dot{\\varphi}(t)|^{2}\\,dt \\right\\}\n$$\nsubject to the constraints:\n1. $\\varphi$ is absolutely continuous and $\\varphi(0)=0$.\n2. $\\dot{\\varphi} \\in L^{2}([0,T])$.\n3. $A_{T}(\\varphi) = \\frac{1}{T}\\int_{0}^{T}\\dot{\\varphi}(t)\\,dt = v$.\n\nLet $f(t) = \\dot{\\varphi}(t)$. The problem is transformed into finding the infimum of $\\frac{1}{2}\\int_{0}^{T}|f(t)|^{2}\\,dt$ for functions $f \\in L^{2}([0,T])$ that are derivatives of some path $\\varphi$ starting at $0$, and that satisfy the constraint $\\frac{1}{T}\\int_{0}^{T}f(t)\\,dt = v$. Every $f \\in L^2([0,T])$ is the derivative of an absolutely continuous function $\\varphi(t) = \\int_0^t f(s)ds$, which automatically satisfies $\\varphi(0)=0$. Thus, the problem reduces to a constrained minimization problem in $L^2([0,T])$:\nMinimize $\\frac{1}{2}\\|f\\|_{L^2}^2$ subject to the constraint $\\langle f, \\mathbf{1} \\rangle = vT$, where $\\mathbf{1}$ is the function identically equal to $1$ on $[0,T]$ and $\\langle \\cdot, \\cdot \\rangle$ is the standard $L^2$ inner product.\n\nWe apply the Cauchy-Schwarz inequality to the functions $f(t)$ and $g(t) = 1$:\n$$\n\\left| \\int_{0}^{T} f(t) \\cdot 1 \\, dt \\right|^2 \\leq \\left( \\int_{0}^{T} |f(t)|^2 \\, dt \\right) \\left( \\int_{0}^{T} |1|^2 \\, dt \\right)\n$$\nThe terms in the inequality are:\n- $\\int_{0}^{T} f(t) \\, dt = vT$ from the constraint.\n- $\\int_{0}^{T} |f(t)|^2 \\, dt = \\|f\\|_{L^2}^2$.\n- $\\int_{0}^{T} |1|^2 \\, dt = T$.\n\nSubstituting these into the Cauchy-Schwarz inequality:\n$$\n(vT)^2 \\leq \\|f\\|_{L^2}^2 \\cdot T\n$$\n$$\nv^2 T^2 \\leq T \\int_{0}^{T} |f(t)|^2 \\, dt\n$$\nAssuming $T0$, we can divide by $T$:\n$$\nv^2 T \\leq \\int_{0}^{T} |f(t)|^2 \\, dt\n$$\nMultiplying by $\\frac{1}{2}$, we get a lower bound for the quantity we want to minimize:\n$$\n\\frac{1}{2}v^2 T \\leq \\frac{1}{2}\\int_{0}^{T} |f(t)|^2 \\, dt\n$$\nThe minimum value of the integral is thus at least $\\frac{1}{2}v^2 T$. The equality in the Cauchy-Schwarz inequality holds if and only if one function is a scalar multiple of the other, i.e., $f(t) = c \\cdot 1 = c$ for some constant $c \\in \\mathbb{R}$.\n\nWe can determine the constant $c$ by imposing the constraint:\n$$\n\\frac{1}{T}\\int_{0}^{T} f(t) \\, dt = \\frac{1}{T}\\int_{0}^{T} c \\, dt = \\frac{1}{T}(cT) = c\n$$\nSince this must be equal to $v$, we have $c=v$.\nSo, the infimum is achieved for the function $f(t) = v$. This corresponds to the path derivative $\\dot{\\varphi}(t) = v$. The optimal path $\\varphi^*(t)$ is found by integrating the derivative:\n$$\n\\varphi^*(t) = \\int_{0}^{t} \\dot{\\varphi}^*(s) \\, ds + \\varphi^*(0) = \\int_{0}^{t} v \\, ds + 0 = vt\n$$\nThis path $\\varphi^*(t) = vt$ satisfies all the conditions: it is absolutely continuous, its derivative $\\dot{\\varphi}^*(t)=v$ is in $L^2([0,T])$, and $\\varphi^*(0)=0$.\n\nThe value of the rate function $J_T(v)$ is the infimum, which is the value of $S_T(\\varphi)$ evaluated at this optimal path $\\varphi^*(t) = vt$:\n$$\nJ_T(v) = S_T(\\varphi^*) = \\frac{1}{2}\\int_{0}^{T} |\\dot{\\varphi}^*(t)|^2 \\, dt = \\frac{1}{2}\\int_{0}^{T} |v|^2 \\, dt = \\frac{1}{2}v^2 \\int_{0}^{T} dt = \\frac{1}{2}v^2 T\n$$\nThus, the good rate function for the family $\\{A_T(X^\\varepsilon)\\}$ is $J_T(v) = \\frac{T v^2}{2}$.", "answer": "$$\n\\boxed{\\frac{T v^2}{2}}\n$$", "id": "2968426"}, {"introduction": "While the Freidlin–Wentzell action functional provides a theoretical basis for understanding rare transitions, its direct minimization can be challenging. For linear systems, however, the associated Hamilton-Jacobi-Bellman equation for the quasipotential simplifies dramatically [@problem_id:2968464]. This exercise will guide you through solving for the quasipotential using a quadratic ansatz, revealing a deep connection between large deviations and the algebraic Riccati equations of optimal control theory.", "problem": "Consider the small-noise limit of the linear Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X_{t}^{\\varepsilon} \\;=\\; A\\,X_{t}^{\\varepsilon}\\,\\mathrm{d}t \\;+\\; \\sqrt{\\varepsilon}\\,\\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $X_{t}^{\\varepsilon}\\in\\mathbb{R}^{2}$, $A\\in\\mathbb{R}^{2\\times 2}$ is constant, $\\sigma\\in\\mathbb{R}^{2\\times 2}$ is constant, and $a := \\sigma\\sigma^{\\top}$ is positive definite and constant. Let the drift matrix and diffusion matrix be\n$$\nA \\;=\\; \\begin{pmatrix} -3  1 \\\\ 1  -2 \\end{pmatrix}, \n\\qquad\na \\;=\\; 5I \\;+\\; A \\;=\\; \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}.\n$$\nThe equilibrium $x=0$ is asymptotically stable for the deterministic system $\\dot{x}=A x$. The Freidlin–Wentzell action functional for an absolutely continuous path $\\varphi:[0,T]\\to\\mathbb{R}^{2}$ is\n$$\nI_{0T}(\\varphi) \\;=\\; \\frac{1}{2}\\int_{0}^{T} \\big(\\dot{\\varphi}(t) - A\\,\\varphi(t)\\big)^{\\top} a^{-1}\\,\\big(\\dot{\\varphi}(t) - A\\,\\varphi(t)\\big)\\,\\mathrm{d}t.\n$$\nDefine the quasipotential with respect to the attractor at the origin by\n$$\nV(x) \\;=\\; \\inf_{T0}\\;\\inf\\Big\\{ I_{0T}(\\varphi)\\,:\\,\\varphi(0)=0,\\;\\varphi(T)=x,\\;\\varphi\\in AC([0,T];\\mathbb{R}^{2}) \\Big\\}.\n$$\nStarting from the variational principle above and the corresponding stationary Hamilton–Jacobi equation for $V$, derive the algebraic Riccati equation for a quadratic ansatz $V(x)=\\frac{1}{2}x^{\\top}Sx$ with a symmetric matrix $S$, and solve it using the spectral structure of the pair $(A,a)$. Then evaluate the minimal action $V(x^{\\star})$ at the target point\n$$\nx^{\\star} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nProvide your final answer as a single real number. No rounding is required.", "solution": "The problem asks for the value of the quasipotential $V(x^{\\star})$ for a linear stochastic differential equation. The quasipotential $V(x)$ is defined via an action functional $I_{0T}(\\varphi)$ and represents the minimum \"cost\" to reach a state $x$ from the stable equilibrium at the origin.\n\nThe first step is to establish the Hamilton-Jacobi-Bellman (HJB) equation satisfied by the quasipotential $V(x)$. The action functional is given by\n$$\nI_{0T}(\\varphi) = \\frac{1}{2}\\int_{0}^{T} \\big(\\dot{\\varphi}(t) - A\\,\\varphi(t)\\big)^{\\top} a^{-1}\\,\\big(\\dot{\\varphi}(t) - A\\,\\varphi(t)\\big)\\,\\mathrm{d}t.\n$$\nThis corresponds to an optimal control problem where the dynamics are $\\dot{x}(t) = Ax(t) + u(t)$ and the cost is $\\int_0^T L(x,u) \\mathrm{d}t$ with Lagrangian $L(x, u) = \\frac{1}{2} u(t)^\\top a^{-1} u(t)$. The Hamiltonian $H(x,p)$ is the Legendre transform of the Lagrangian with respect to the velocity $\\dot{x}$:\n$$\nH(x, p) = \\sup_{\\dot{x} \\in \\mathbb{R}^2} \\left\\{ p^\\top \\dot{x} - L(x, \\dot{x}) \\right\\}.\n$$\nLetting $u = \\dot{x} - Ax$, we have $\\dot{x} = Ax + u$. The Hamiltonian becomes:\n$$\nH(x, p) = \\sup_{u \\in \\mathbb{R}^2} \\left\\{ p^\\top (Ax + u) - \\frac{1}{2} u^\\top a^{-1} u \\right\\} = p^\\top Ax + \\sup_{u \\in \\mathbb{R}^2} \\left\\{ p^\\top u - \\frac{1}{2} u^\\top a^{-1} u \\right\\}.\n$$\nThe supremum is found by setting the gradient with respect to $u$ to zero: $p - a^{-1}u = 0$, which gives the optimal control $u_{opt} = a p$. Substituting this back into the expression for $H(x,p)$:\n$$\nH(x, p) = p^\\top Ax + p^\\top(ap) - \\frac{1}{2}(ap)^\\top a^{-1} (ap) = p^\\top Ax + p^\\top a p - \\frac{1}{2} p^\\top a^\\top a^{-1} a p.\n$$\nSince $a = \\sigma\\sigma^\\top$, it is symmetric ($a=a^\\top$), so this simplifies to:\n$$\nH(x, p) = p^\\top Ax + \\frac{1}{2} p^\\top a p.\n$$\nThe quasipotential $V(x)$ for an asymptotically stable equilibrium satisfies the stationary HJB equation $H(x, \\nabla V(x)) = 0$.\n$$\n(\\nabla V(x))^\\top A x + \\frac{1}{2} (\\nabla V(x))^\\top a (\\nabla V(x)) = 0.\n$$\nWe are given a quadratic ansatz for the quasipotential, $V(x) = \\frac{1}{2}x^{\\top}Sx$, where $S$ is a symmetric positive definite matrix. The gradient is $\\nabla V(x) = Sx$. Substituting this into the HJB equation:\n$$\n(Sx)^\\top A x + \\frac{1}{2} (Sx)^\\top a (Sx) = 0.\n$$\n$$\nx^\\top S^\\top A x + \\frac{1}{2} x^\\top S^\\top a S x = 0.\n$$\nSince $S$ is symmetric ($S^\\top=S$), this is $x^\\top S A x + \\frac{1}{2} x^\\top S a S x = 0$. The first term $x^\\top S A x$ is a scalar and thus equal to its transpose $x^\\top A^\\top S x$. We can write it in symmetric form as $\\frac{1}{2}(x^\\top SAx + x^\\top A^\\top S x) = \\frac{1}{2}x^\\top(SA+A^\\top S)x$. The HJB equation becomes:\n$$\n\\frac{1}{2} x^\\top (A^\\top S + SA) x + \\frac{1}{2} x^\\top S a S x = 0.\n$$\nThis equation must hold for all $x \\in \\mathbb{R}^2$. This implies that the symmetric matrix within the quadratic form must be the zero matrix:\n$$\nA^\\top S + SA + S a S = 0.\n$$\nThis is the algebraic Riccati equation (ARE) for the matrix $S$.\n\nNow we must solve this equation for $S$. We are given the matrices\n$$\nA = \\begin{pmatrix} -3  1 \\\\ 1  -2 \\end{pmatrix}, \\quad a = 5I + A = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}.\n$$\nWe note that the matrix $A$ is symmetric, so $A^\\top = A$. The ARE simplifies to:\n$$\nAS + SA + S a S = 0.\n$$\nThe problem specifies that $a = 5I+A$. Since $A$ commutes with the identity matrix $I$ and with itself, $A$ commutes with $a$: $Aa = A(5I+A) = 5A+A^2 = (5I+A)A = aA$. This is a crucial property.\n\nLet's test the hypothesis that the solution $S$ also commutes with $A$, i.e., $AS = SA$. If this is true, the ARE becomes:\n$$\n2AS + S a S = 0.\n$$\nSince S is positive definite, it is invertible. Multiplying by $S^{-1}$ on the right, we get $2A + Sa = 0$, which gives $Sa = -2A$. Therefore, $S = -2A a^{-1}$. Since $A$ and $a$ commute, $A$ and $a^{-1}$ also commute, and thus this candidate $S$ commutes with $A$, confirming our assumption is consistent. The unique positive definite solution to the ARE is what we seek.\n\nLet's compute $S$. We are given $a = A+5I$.\n$$\nS = -2A(a)^{-1}.\n$$\nThe inverse of $a$ is:\n$$\n\\det(a) = (2)(3) - (1)(1) = 5.\n$$\n$$\na^{-1} = \\frac{1}{5} \\begin{pmatrix} 3  -1 \\\\ -1  2 \\end{pmatrix}.\n$$\nNow we compute $S$:\n$$\nS = -2 \\begin{pmatrix} -3  1 \\\\ 1  -2 \\end{pmatrix} \\frac{1}{5} \\begin{pmatrix} 3  -1 \\\\ -1  2 \\end{pmatrix} = -\\frac{2}{5} \\begin{pmatrix} (-3)(3)+(1)(-1)  (-3)(-1)+(1)(2) \\\\ (1)(3)+(-2)(-1)  (1)(-1)+(-2)(2) \\end{pmatrix}.\n$$\n$$\nS = -\\frac{2}{5} \\begin{pmatrix} -9-1  3+2 \\\\ 3+2  -1-4 \\end{pmatrix} = -\\frac{2}{5} \\begin{pmatrix} -10  5 \\\\ 5  -5 \\end{pmatrix} = \\begin{pmatrix} 4  -2 \\\\ -2  2 \\end{pmatrix}.\n$$\nThis matrix $S$ is symmetric. Its principal minors are $4  0$ and $\\det(S) = (4)(2) - (-2)^2 = 8 - 4 = 4  0$, so $S$ is positive definite. This confirms it is the correct solution.\n\nFinally, we evaluate the quasipotential $V(x^{\\star})$ at the target point $x^{\\star} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n$$\nV(x^{\\star}) = \\frac{1}{2} (x^{\\star})^\\top S x^{\\star} = \\frac{1}{2} \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 4  -2 \\\\ -2  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nFirst, we compute the product $S x^{\\star}$:\n$$\nS x^{\\star} = \\begin{pmatrix} 4  -2 \\\\ -2  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (4)(1) + (-2)(-1) \\\\ (-2)(1) + (2)(-1) \\end{pmatrix} = \\begin{pmatrix} 4+2 \\\\ -2-2 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ -4 \\end{pmatrix}.\n$$\nThen, we compute the full quadratic form:\n$$\nV(x^{\\star}) = \\frac{1}{2} \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ -4 \\end{pmatrix} = \\frac{1}{2} ((1)(6) + (-1)(-4)) = \\frac{1}{2} (6+4) = \\frac{1}{2}(10) = 5.\n$$\nThe minimal action to reach $x^{\\star}$ is $5$.", "answer": "$$\n\\boxed{5}\n$$", "id": "2968464"}, {"introduction": "Large deviation theory extends beyond single paths to describe the long-term statistical behavior of a process through its empirical measure. The Donsker-Varadhan rate function governs these deviations, but its abstract variational form can be hard to work with [@problem_id:2968435]. In this advanced practice, you will first derive a more explicit formula for this rate function for reversible diffusions, linking it directly to the generator's Dirichlet form, and then apply this powerful result to a concrete example.", "problem": "Consider the overdamped Langevin diffusion on $\\mathbb{R}^{d}$ given by the stochastic differential equation $dX_{t}=-\\nabla U(X_{t})\\,dt+\\sqrt{2}\\,dW_{t}$, where $U:\\mathbb{R}^{d}\\to\\mathbb{R}$ is a smooth confining potential such that the probability measure $\\pi(dx)=Z^{-1}\\exp(-U(x))\\,dx$ is invariant and the associated generator $L=\\Delta-\\nabla U\\cdot\\nabla$ is symmetric in $L^{2}(\\pi)$. Let $\\{L_{T}\\}_{T0}$ be the empirical measures $L_{T}=\\frac{1}{T}\\int_{0}^{T}\\delta_{X_{t}}\\,dt$. It is known that $\\{L_{T}\\}$ satisfies a Large Deviation Principle (LDP) in the topology of weak convergence with a good rate function $I$ given by the Donsker-Varadhan (DV) variational representation\n$$\nI(\\mu)=\\sup_{g\\in\\mathcal{D}(L),\\,g0}\\left\\{-\\int \\frac{L g}{g}\\,d\\mu\\right\\},\n$$\nfor probability measures $\\mu$ on $\\mathbb{R}^{d}$.\n\n(a) Starting from the above DV variational representation, and using only properties that follow from the symmetry of $L$ in $L^{2}(\\pi)$ and the carré du champ calculus for diffusion generators, derive an explicit formula for $I(\\mu)$ that depends on the density $f=\\frac{d\\mu}{d\\pi}$ (when $\\mu\\ll\\pi$) and the carré du champ of the generator. Your derivation must not assume any a priori closed form for $I(\\mu)$; instead, obtain it by a first-principles optimization over positive test functions and by using integration by parts in $L^{2}(\\pi)$.\n\n(b) Specialize to $d=1$ with $U(x)=\\frac{x^{2}}{2}$, so that $\\pi$ is the standard normal law with density $(2\\pi)^{-\\frac{1}{2}}\\exp(-\\frac{x^{2}}{2})$. Let $\\beta\\in\\mathbb{R}$ and consider the probability measure $\\mu$ absolutely continuous with respect to $\\pi$ with density\n$$\nf(x)=\\exp\\!\\big(2\\beta x-2\\beta^{2}\\big)\\quad\\text{with respect to }\\pi.\n$$\nUsing your result from part (a), compute the value of the rate function $I(\\mu)$ in closed form as a function of $\\beta$. Provide your final answer as a single analytic expression with no units. Do not approximate or round.", "solution": "The problem is well-posed and scientifically grounded within the mathematical theory of stochastic processes and large deviations. We can proceed with a full solution.\n\nPart (a): Derivation of the rate function formula.\n\nWe are given the Donsker-Varadhan (DV) variational representation for the rate function $I(\\mu)$ governing the large deviations of the empirical measure $L_T$:\n$$\nI(\\mu) = \\sup_{g \\in \\mathcal{D}(L), g0} \\left\\{ -\\int \\frac{Lg}{g} d\\mu \\right\\}\n$$\nwhere $L = \\Delta - \\nabla U \\cdot \\nabla$ is the generator of the Langevin diffusion, and $\\mu$ is a probability measure on $\\mathbb{R}^{d}$. We are asked to derive a formula for $I(\\mu)$ in the case where $\\mu$ is absolutely continuous with respect to the invariant measure $\\pi$. Let the density be $f = \\frac{d\\mu}{d\\pi}$, so that $d\\mu = f d\\pi$. We assume that $f$ is sufficiently regular for the following manipulations to be valid.\n\nSubstituting $d\\mu = f d\\pi$ into the DV formula, we obtain:\n$$\nI(\\mu) = \\sup_{g0} \\left\\{ -\\int \\left(\\frac{Lg}{g}\\right) f d\\pi \\right\\}\n$$\nThe generator $L$ is given to be symmetric in the space $L^2(\\pi)$. The associated Dirichlet form $\\mathcal{E}(\\phi, \\psi)$ for functions $\\phi, \\psi$ in the domain of $L$ is defined via the carré du champ operator $\\Gamma(\\phi, \\psi)$. Specifically, for a symmetric Markov generator, the identity $-\\int (L\\phi) \\psi \\, d\\pi = \\int \\Gamma(\\phi, \\psi) \\, d\\pi$ holds. For our diffusion generator $L = \\Delta - \\nabla U \\cdot \\nabla$, the carré du champ operator is given by $\\Gamma(\\phi, \\psi) = \\nabla \\phi \\cdot \\nabla \\psi$. Thus, the integration by parts formula in $L^2(\\pi)$ is:\n$$\n-\\int (L\\phi) \\psi \\, d\\pi = \\int (\\nabla \\phi \\cdot \\nabla \\psi) \\, d\\pi\n$$\nApplying this formula to our expression for $I(\\mu)$, we let $\\phi=g$ and $\\psi = f/g$. This yields:\n$$\n-\\int (Lg) \\frac{f}{g} d\\pi = \\int \\nabla g \\cdot \\nabla\\left(\\frac{f}{g}\\right) d\\pi\n$$\nWe compute the gradient of the quotient $\\nabla\\left(\\frac{f}{g}\\right)$:\n$$\n\\nabla\\left(\\frac{f}{g}\\right) = \\frac{g \\nabla f - f \\nabla g}{g^2} = \\frac{\\nabla f}{g} - \\frac{f \\nabla g}{g^2}\n$$\nSubstituting this back into the integral, we get:\n$$\n\\int \\nabla g \\cdot \\left(\\frac{\\nabla f}{g} - \\frac{f \\nabla g}{g^2}\\right) d\\pi = \\int \\left( \\frac{\\nabla g \\cdot \\nabla f}{g} - f \\frac{|\\nabla g|^2}{g^2} \\right) d\\pi\n$$\nWe can express the terms using the logarithmic gradient of $g$, let $H = \\nabla(\\ln g) = \\frac{\\nabla g}{g}$. The expression for $I(\\mu)$ becomes:\n$$\nI(\\mu) = \\sup_{g0} \\int \\left( H \\cdot \\nabla f - f |H|^2 \\right) d\\pi\n$$\nThe supremum is over all sufficiently regular positive functions $g$. This implies the supremum is over all vector fields $H$ that are gradients of some scalar function $\\ln g$. We relax this constraint and perform the optimization over all vector fields $H(x)$, and then verify if the optimal field is a gradient. The optimization can be performed pointwise for the integrand at each $x \\in \\mathbb{R}^d$:\n$$\n\\sup_{v \\in \\mathbb{R}^d} \\{ v \\cdot \\nabla f(x) - f(x)|v|^2 \\}\n$$\nThis is a simple quadratic maximization problem in the vector $v = H(x)$. For a fixed $x$, the expression is maximized when $v$ is parallel to $\\nabla f(x)$. Let $v = \\alpha \\nabla f(x)$. The expression becomes $\\alpha |\\nabla f(x)|^2 - f(x) \\alpha^2 |\\nabla f(x)|^2$. The maximum of $ax - bx^2$ is at $x=a/(2b)$. Here, the optimal $\\alpha$ is $\\frac{|\\nabla f(x)|^2}{2 f(x) |\\nabla f(x)|^2} = \\frac{1}{2f(x)}$.\nThus, the optimal vector field is $H_{opt}(x) = v_{opt} = \\frac{\\nabla f(x)}{2f(x)}$.\nThis optimal field is indeed a gradient, since $H_{opt} = \\nabla\\left(\\frac{1}{2} \\ln f\\right) = \\nabla(\\ln \\sqrt{f})$. This corresponds to choosing $g = \\sqrt{f}$ (up to a multiplicative constant), which is a valid choice as long as $f$ is positive and regular enough.\n\nSubstituting the optimal vector field back into the integrand, the maximum value of the integrand at each point $x$ is:\n$$\n\\frac{\\nabla f}{2f} \\cdot \\nabla f - f \\left|\\frac{\\nabla f}{2f}\\right|^2 = \\frac{|\\nabla f|^2}{2f} - f \\frac{|\\nabla f|^2}{4f^2} = \\frac{|\\nabla f|^2}{2f} - \\frac{|\\nabla f|^2}{4f} = \\frac{|\\nabla f|^2}{4f}\n$$\nIntegrating this pointwise maximum gives the value of the supremum:\n$$\nI(\\mu) = \\int \\frac{|\\nabla f(x)|^2}{4f(x)} d\\pi(x)\n$$\nUsing the notation of the carré du champ, $\\Gamma(f,f) = |\\nabla f|^2$, we arrive at the final formula for part (a):\n$$\nI(\\mu) = \\frac{1}{4} \\int \\frac{\\Gamma(f,f)}{f} d\\pi\n$$\n\nPart (b): Calculation for a specific case.\n\nWe are given the specialization to $d=1$ with potential $U(x) = \\frac{x^2}{2}$. The invariant measure $\\pi$ is the standard normal distribution, with probability density function $\\phi(x) = (2\\pi)^{-1/2}\\exp(-x^2/2)$. The measure of interest, $\\mu$, is defined by its density $f$ with respect to $\\pi$:\n$$\nf(x) = \\exp(2\\beta x - 2\\beta^2)\n$$\nWe need to compute $I(\\mu)$ using the formula derived in part (a). For $d=1$, the gradient is the derivative $\\frac{d}{dx}$, and the carré du champ is $\\Gamma(f,f) = (f'(x))^2$. The formula becomes:\n$$\nI(\\mu) = \\frac{1}{4} \\int_{-\\infty}^{\\infty} \\frac{(f'(x))^2}{f(x)} d\\pi(x)\n$$\nFirst, we calculate the derivative of $f(x)$:\n$$\nf'(x) = \\frac{d}{dx} \\exp(2\\beta x - 2\\beta^2) = 2\\beta \\exp(2\\beta x - 2\\beta^2) = 2\\beta f(x)\n$$\nNext, we form the term in the integrand:\n$$\n\\frac{(f'(x))^2}{f(x)} = \\frac{(2\\beta f(x))^2}{f(x)} = \\frac{4\\beta^2 f(x)^2}{f(x)} = 4\\beta^2 f(x)\n$$\nNow we substitute this into the integral for $I(\\mu)$:\n$$\nI(\\mu) = \\frac{1}{4} \\int_{-\\infty}^{\\infty} (4\\beta^2 f(x)) d\\pi(x) = \\beta^2 \\int_{-\\infty}^{\\infty} f(x) d\\pi(x)\n$$\nThe integral $\\int_{-\\infty}^{\\infty} f(x) d\\pi(x)$ is, by definition, the total mass of the measure $\\mu$, since $d\\mu = f d\\pi$. The problem states that $\\mu$ is a probability measure, which implies its total mass is $1$.\n$$\n\\int_{-\\infty}^{\\infty} d\\mu(x) = \\int_{-\\infty}^{\\infty} f(x) d\\pi(x) = 1\n$$\nWe can verify this explicitly:\n$$\n\\int_{-\\infty}^{\\infty} f(x) \\phi(x) dx = \\int_{-\\infty}^{\\infty} \\exp(2\\beta x - 2\\beta^2) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\n$$\n= \\frac{\\exp(-2\\beta^2)}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^2}{2} + 2\\beta x\\right) dx\n$$\nThe exponent can be completed to a square: $-\\frac{x^2}{2} + 2\\beta x = -\\frac{1}{2}(x^2 - 4\\beta x) = -\\frac{1}{2}((x-2\\beta)^2 - 4\\beta^2) = -\\frac{1}{2}(x-2\\beta)^2 + 2\\beta^2$.\nSo the integral becomes:\n$$\n\\frac{\\exp(-2\\beta^2)}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{1}{2}(x-2\\beta)^2 + 2\\beta^2\\right) dx = \\frac{\\exp(-2\\beta^2)\\exp(2\\beta^2)}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{1}{2}(x-2\\beta)^2\\right) dx\n$$\nThe remaining integral is the integral of the probability density function of a normal distribution with mean $2\\beta$ and variance $1$, which equals $\\sqrt{2\\pi}$. So, the total integral is $\\frac{1}{\\sqrt{2\\pi}} \\cdot \\sqrt{2\\pi} = 1$.\nTherefore, the integral $\\int f(x) d\\pi(x)$ is indeed $1$.\nSubstituting this result back into our expression for $I(\\mu)$:\n$$\nI(\\mu) = \\beta^2 \\cdot 1 = \\beta^2\n$$", "answer": "$$\\boxed{\\beta^{2}}$$", "id": "2968435"}]}