{"hands_on_practices": [{"introduction": "While the Itô integral is restricted to adapted processes, Malliavin calculus provides a powerful extension known as the Skorohod integral, which can handle non-adapted or \"anticipating\" integrands. This exercise invites you to compute the Skorohod integral for one of the simplest yet most illustrative anticipating processes, $u_t = B_T$. Through this computation, you will gain a concrete grasp of the duality relationship and integration-by-parts formulas that define and characterize this integral [@problem_id:2986329].", "problem": "Fix a time horizon $T0$ and consider the classical Wiener space $(\\Omega,\\mathcal{F},\\mathbb{P})$ endowed with the canonical filtration generated by a standard Brownian motion $(B_t)_{t\\in[0,T]}$ with $B_0=0$. Let $H:=L^2([0,T])$ be the Cameron–Martin space and write $W(h):=\\int_0^T h(t)\\,dB_t$ for the Wiener integral of a deterministic $h\\in H$. For a smooth cylindrical functional $F=f\\big(W(h_1),\\dots,W(h_n)\\big)$ with $f\\in C_b^\\infty(\\mathbb{R}^n)$ and $h_i\\in H$, the Malliavin derivative $D F$ is the $H$-valued random variable defined by $D_t F=\\sum_{i=1}^n \\partial_i f\\big(W(h_1),\\dots,W(h_n)\\big)\\,h_i(t)$. The Skorohod integral $\\delta(u)$ of a process $u\\in L^2(\\Omega;H)$ is defined (when it exists) by the duality relation $\\mathbb{E}[F\\,\\delta(u)]=\\mathbb{E}[\\langle D F,u\\rangle_H]$ for all such $F$, where $\\langle \\cdot,\\cdot\\rangle_H$ denotes the inner product in $H$. It is a standard fact that for deterministic $h\\in H$, one has $\\delta(h)=W(h)$.\n  \nDefine the anticipating integrand $u=(u_t)_{t\\in[0,T]}$ by $u_t:=B_T$ for all $t\\in[0,T]$. Starting only from the definitions above and basic Gaussian moment identities for $B_T$, perform the following:\n  \n1. Prove that $u\\in\\mathrm{Dom}(\\delta)$, the domain of the Skorohod integral.\n2. Compute $\\delta(u)$ explicitly as an element of $L^2(\\Omega)$.\n3. Compute $\\mathbb{E}[\\delta(u)]$ and $\\mathrm{Var}(\\delta(u))$.\n4. Using the definition of the Russo–Vallois forward integral $\\int_0^T u_t\\,d^{-}B_t$ as the limit in probability of forward Riemann sums $\\sum_{k} u_{t_k}\\big(B_{t_{k+1}}-B_{t_k}\\big)$ over partitions of $[0,T]$ with mesh going to $0$, compute $\\int_0^T u_t\\,d^{-}B_t$ for this $u$ and relate it to $\\delta(u)$ through the Malliavin derivative $D_t u_t$.\n  \nProvide your final answer by giving the closed-form analytic expression for $\\delta(u)$ in terms of $B_T$ and $T$. No rounding is required and no units are involved.", "solution": "The problem is well-posed and scientifically grounded within the framework of Malliavin calculus. We proceed with the solution.\n\nThe integrand process is defined as $u_t := B_T$ for all $t \\in [0,T]$. This can be written as $u = B_T \\cdot \\mathbf{1}$, where $\\mathbf{1}$ is the function identically equal to $1$ on $[0,T]$. Since $\\mathbf{1} \\in L^2([0,T]) = H$, $u$ is an $H$-valued random variable.\n\n1. Prove that $u \\in \\mathrm{Dom}(\\delta)$.\nTo show that $u$ is in the domain of the Skorohod integral, $\\mathrm{Dom}(\\delta)$, we must verify two conditions:\n(a) $u \\in L^2(\\Omega; H)$.\n(b) The mapping $F \\mapsto \\mathbb{E}[\\langle DF, u \\rangle_H]$ is continuous on the space of smooth cylindrical functionals with respect to the $L^2(\\Omega)$ norm.\n\nFor condition (a), we compute the squared norm of $u$ in $L^2(\\Omega; H)$:\n$$ \\mathbb{E}\\left[ \\|u\\|_H^2 \\right] = \\mathbb{E}\\left[ \\int_0^T u_t^2 \\, dt \\right] = \\mathbb{E}\\left[ \\int_0^T B_T^2 \\, dt \\right] $$\nBy Fubini's theorem, we can switch the expectation and the integral:\n$$ \\int_0^T \\mathbb{E}\\left[ B_T^2 \\right] \\, dt = \\int_0^T T \\, dt = T^2 $$\nSince $T^2  \\infty$, we have shown that $u \\in L^2(\\Omega; H)$.\n\nFor condition (b), we analyze the expression $\\mathbb{E}[\\langle DF, u \\rangle_H]$ for a smooth cylindrical functional $F$.\n$$ \\mathbb{E}[\\langle DF, u \\rangle_H] = \\mathbb{E}\\left[ \\int_0^T (D_t F) u_t \\, dt \\right] = \\mathbb{E}\\left[ B_T \\int_0^T D_t F \\, dt \\right] $$\nWe use an integration by parts argument. Consider the product $B_T F$. Its Malliavin derivative is given by the product rule:\n$$ D_t(B_T F) = F (D_t B_T) + B_T (D_t F) $$\nThe Brownian motion at time $T$ can be written as the Wiener integral $B_T = \\int_0^T 1 \\, dB_s$. The Malliavin derivative of such an integral is $D_t B_T = D_t \\left(\\int_0^T \\mathbf{1}(s) \\, dB_s\\right) = \\mathbf{1}(t) = 1$ for $t \\in [0,T]$. So,\n$$ D_t(B_T F) = F + B_T D_t F $$\nNow, consider the duality relation between the derivative $D$ and the Wiener integral $W(h) = \\int_0^T h(s) \\, dB_s$. For a general smooth functional $G$ and $h \\in H$, we have $\\mathbb{E}[\\langle DG, h \\rangle_H] = \\mathbb{E}[G W(h)]$. Let $G = B_T F$ and $h = \\mathbf{1}$. Then $W(\\mathbf{1}) = B_T$.\n$$ \\mathbb{E}[\\langle D(B_T F), \\mathbf{1} \\rangle_H] = \\mathbb{E}[(B_T F) W(\\mathbf{1})] = \\mathbb{E}[B_T^2 F] $$\nOn the other hand, we can compute the inner product directly:\n$$ \\mathbb{E}[\\langle D(B_T F), \\mathbf{1} \\rangle_H] = \\mathbb{E}\\left[ \\int_0^T D_t(B_T F) \\cdot 1 \\, dt \\right] = \\mathbb{E}\\left[ \\int_0^T (F + B_T D_t F) \\, dt \\right] $$\n$$ = \\mathbb{E}\\left[ F \\int_0^T 1 \\, dt + B_T \\int_0^T D_t F \\, dt \\right] = \\mathbb{E}[T F] + \\mathbb{E}\\left[ B_T \\int_0^T D_t F \\, dt \\right] $$\nEquating the two expressions for $\\mathbb{E}[\\langle D(B_T F), \\mathbf{1} \\rangle_H]$, we get:\n$$ \\mathbb{E}[B_T^2 F] = T \\mathbb{E}[F] + \\mathbb{E}\\left[ B_T \\int_0^T D_t F \\, dt \\right] $$\nRearranging gives the expression we seek:\n$$ \\mathbb{E}[\\langle DF, u \\rangle_H] = \\mathbb{E}\\left[ B_T \\int_0^T D_t F \\, dt \\right] = \\mathbb{E}[B_T^2 F] - T \\mathbb{E}[F] = \\mathbb{E}[(B_T^2 - T)F] $$\nThe continuity condition is now evident. By the Cauchy-Schwarz inequality:\n$$ |\\mathbb{E}[(B_T^2 - T)F]| \\le \\| B_T^2 - T \\|_{L^2(\\Omega)} \\| F \\|_{L^2(\\Omega)} $$\nSince $B_T \\sim N(0,T)$, all its moments are finite, which implies $\\| B_T^2 - T \\|_{L^2(\\Omega)}$ is a finite constant. Thus, the mapping $F \\mapsto \\mathbb{E}[\\langle DF, u \\rangle_H]$ is continuous, completing the proof that $u \\in \\mathrm{Dom}(\\delta)$.\n\n2. Compute $\\delta(u)$ explicitly.\nThe Skorohod integral $\\delta(u)$ is defined as the unique element in $L^2(\\Omega)$ satisfying the duality relation:\n$$ \\mathbb{E}[F \\delta(u)] = \\mathbb{E}[\\langle DF, u \\rangle_H] $$\nfor all smooth cylindrical functionals $F$. From our work in part 1, we found:\n$$ \\mathbb{E}[\\langle DF, u \\rangle_H] = \\mathbb{E}[(B_T^2 - T)F] $$\nTherefore, we must have:\n$$ \\mathbb{E}[F \\delta(u)] = \\mathbb{E}[(B_T^2 - T)F] $$\nSince the space of smooth cylindrical functionals is dense in $L^2(\\Omega)$, this equality implies that the random variables themselves must be equal almost surely.\n$$ \\delta(u) = B_T^2 - T $$\n\n3. Compute $\\mathbb{E}[\\delta(u)]$ and $\\mathrm{Var}(\\delta(u))$.\nUsing the expression for $\\delta(u)$, we compute its expectation:\n$$ \\mathbb{E}[\\delta(u)] = \\mathbb{E}[B_T^2 - T] = \\mathbb{E}[B_T^2] - T $$\nSince $B_T$ is a centered normal random variable with variance $T$ (i.e., $B_T \\sim N(0,T)$), its second moment is $\\mathbb{E}[B_T^2] = T$.\n$$ \\mathbb{E}[\\delta(u)] = T - T = 0 $$\nNext, we compute the variance:\n$$ \\mathrm{Var}(\\delta(u)) = \\mathrm{Var}(B_T^2 - T) = \\mathrm{Var}(B_T^2) $$\nThe variance is given by $\\mathrm{Var}(B_T^2) = \\mathbb{E}[(B_T^2)^2] - (\\mathbb{E}[B_T^2])^2 = \\mathbb{E}[B_T^4] - T^2$.\nFor a centered normal random variable $X \\sim N(0, \\sigma^2)$, the fourth moment is $\\mathbb{E}[X^4] = 3\\sigma^4$. For $B_T$, we have $\\sigma^2 = T$, so $\\mathbb{E}[B_T^4] = 3T^2$.\n$$ \\mathrm{Var}(\\delta(u)) = 3T^2 - T^2 = 2T^2 $$\n\n4. Compute the Russo-Vallois forward integral and relate it to $\\delta(u)$.\nThe Russo-Vallois forward integral is defined by the limit in probability of the forward Riemann sums:\n$$ \\int_0^T u_t \\, d^-B_t = \\lim_{\\|\\pi_n\\| \\to 0} \\sum_{k=0}^{n-1} u_{t_k} (B_{t_{k+1}} - B_{t_k}) $$\nwhere $\\pi_n = \\{0=t_0  t_1  \\dots  t_n = T\\}$ is a partition of $[0,T]$. For our integrand $u_t = B_T$, the sum for any partition $\\pi_n$ is:\n$$ \\sum_{k=0}^{n-1} B_T (B_{t_{k+1}} - B_{t_k}) = B_T \\sum_{k=0}^{n-1} (B_{t_{k+1}} - B_{t_k}) $$\nThis is a telescoping sum:\n$$ B_T ( (B_{t_1} - B_{t_0}) + (B_{t_2} - B_{t_1}) + \\dots + (B_{t_n} - B_{t_{n-1}}) ) = B_T (B_{t_n} - B_{t_0}) = B_T (B_T - B_0) $$\nSince $B_0=0$, the sum is exactly $B_T^2$, regardless of the partition. Therefore, the limit is trivial:\n$$ \\int_0^T u_t \\, d^-B_t = B_T^2 $$\nTo relate this to $\\delta(u)$, we use the general identity (the Hitsuda-Skorohod identity) which connects the Skorohod integral to this forward integral:\n$$ \\int_0^T u_t \\, d^-B_t = \\delta(u) + \\int_0^T D_t u_t \\, dt $$\nThe term $\\int_0^T D_t u_t \\, dt$ is the trace of the Malliavin derivative of the process $u$. Let's compute it. The process is $u_t = B_T$. Its Malliavin derivative at time $s$ is $D_s u_t = D_s B_T = 1$ for all $s,t \\in [0,T]$.\nThe trace requires evaluating the derivative at the same time index as the process, i.e., $s=t$. So, $D_t u_t = 1$.\nThe integral of the trace is then:\n$$ \\int_0^T D_t u_t \\, dt = \\int_0^T 1 \\, dt = T $$\nSubstituting our results back into the identity, we verify its correctness:\n$$ \\int_0^T u_t \\, d^-B_t = B_T^2 $$\n$$ \\delta(u) + \\int_0^T D_t u_t \\, dt = (B_T^2 - T) + T = B_T^2 $$\nThe identity holds, establishing the relationship between the Skorohod integral of $u$ and its Russo-Vallois forward integral via the trace of its Malliavin derivative. The final answer requested is the expression for $\\delta(u)$.", "answer": "$$\n\\boxed{B_T^2 - T}\n$$", "id": "2986329"}, {"introduction": "The Malliavin derivative is not just a computational tool; it provides deep insights into the analytical properties of the laws of random variables. This practice demonstrates how the Malliavin covariance matrix acts as a powerful criterion for determining if a random vector possesses a probability density function. By analyzing a degenerate Gaussian vector, you will see firsthand how a singular Malliavin covariance matrix reveals that the vector's distribution is concentrated on a lower-dimensional space [@problem_id:2986307].", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be the classical Wiener space carrying a standard Brownian motion $(B_t)_{t\\in[0,1]}$. Consider the deterministic functions $h:[0,1]\\to\\mathbb{R}$ and $k:[0,1]\\to\\mathbb{R}$ defined by $h(t)=\\mathbf{1}_{[0,1/2]}(t)$ and $k(t)=2\\,\\mathbf{1}_{[0,1/2]}(t)$. Define the $\\mathbb{R}^2$-valued random variable $F=(F_1,F_2)$ by\n$$\nF_1=\\int_0^1 h(t)\\,\\mathrm{d}B_t,\\qquad F_2=\\int_0^1 k(t)\\,\\mathrm{d}B_t.\n$$\nYou may use the fundamental facts that: (i) for a deterministic $h\\in L^2([0,1])$, the Wiener integral $\\int_0^1 h(t)\\,\\mathrm{d}B_t$ is a centered Gaussian random variable of variance $\\int_0^1 h(t)^2\\,\\mathrm{d}t$, (ii) the Malliavin derivative $D_t\\!\\left(\\int_0^1 h(s)\\,\\mathrm{d}B_s\\right)=h(t)$ for almost every $t\\in[0,1]$, and (iii) the Malliavin covariance matrix of a finite-dimensional random vector $F=(F_1,\\dots,F_n)$ with components in the domain of the Malliavin derivative is the $n\\times n$ matrix $\\Gamma_F=\\left(\\int_0^1 D_t F_i\\,D_t F_j\\,\\mathrm{d}t\\right)_{1\\le i,j\\le n}$.\n\nCarry out the following steps from first principles:\n- Compute the Malliavin derivative $D_t F$ and express the Malliavin covariance matrix $\\Gamma_F$ in terms of $h$ and $k$.\n- Compute the determinant of $\\Gamma_F$ explicitly.\n- Using only basic properties of Gaussian measures and the definition of absolute continuity with respect to Lebesgue measure on $\\mathbb{R}^2$, explain whether the law of $F$ is absolutely continuous or singular.\n\nProvide as your final answer only the value of $\\det(\\Gamma_F)$, in exact form without approximation. No units are involved.", "solution": "The problem statement has been validated and is deemed valid. It is self-contained, scientifically grounded in the theory of stochastic analysis, and well-posed. We may proceed with the solution.\n\nThe problem asks us to analyze the random vector $F=(F_1,F_2)$ defined by Wiener integrals with respect to a standard one-dimensional Brownian motion $(B_t)_{t\\in[0,1]}$. The components of $F$ are given by:\n$$\nF_1=\\int_0^1 h(t)\\,\\mathrm{d}B_t,\\qquad F_2=\\int_0^1 k(t)\\,\\mathrm{d}B_t\n$$\nwhere the deterministic integrands are $h(t)=\\mathbf{1}_{[0,1/2]}(t)$ and $k(t)=2\\,\\mathbf{1}_{[0,1/2]}(t)$. Here, $\\mathbf{1}_A(t)$ is the indicator function of the set $A$.\n\nThe first step is to compute the Malliavin derivative of the vector $F$. The Malliavin derivative $D_t F$ is a vector of processes, $(D_t F_1, D_t F_2)$. Using the given property (ii), for a deterministic integrand $g \\in L^2([0,1])$, the Malliavin derivative of the Wiener integral $\\int_0^1 g(s)\\,\\mathrm{d}B_s$ is given by $D_t\\left(\\int_0^1 g(s)\\,\\mathrm{d}B_s\\right) = g(t)$ for almost every $t \\in [0,1]$.\n\nApplying this to $F_1$ and $F_2$, we find:\n$$\nD_t F_1 = h(t) = \\mathbf{1}_{[0,1/2]}(t)\n$$\n$$\nD_t F_2 = k(t) = 2\\,\\mathbf{1}_{[0,1/2]}(t)\n$$\nfor almost every $t \\in [0,1]$.\n\nThe second step is to compute the Malliavin covariance matrix $\\Gamma_F$. According to property (iii), this is the $2 \\times 2$ matrix whose entries are the inner products of the Malliavin derivatives in the Hilbert space $L^2([0,1])$. The entries $(\\Gamma_F)_{ij}$ are given by $\\int_0^1 D_t F_i\\,D_t F_j\\,\\mathrm{d}t$.\n\nLet's compute the four entries of $\\Gamma_F$:\nThe $(1,1)$ entry is:\n$$\n(\\Gamma_F)_{11} = \\int_0^1 (D_t F_1)^2\\,\\mathrm{d}t = \\int_0^1 h(t)^2\\,\\mathrm{d}t = \\int_0^1 \\left(\\mathbf{1}_{[0,1/2]}(t)\\right)^2\\,\\mathrm{d}t = \\int_0^{1/2} 1^2\\,\\mathrm{d}t = \\frac{1}{2}\n$$\nThe $(1,2)$ entry is:\n$$\n(\\Gamma_F)_{12} = \\int_0^1 (D_t F_1)(D_t F_2)\\,\\mathrm{d}t = \\int_0^1 h(t)k(t)\\,\\mathrm{d}t = \\int_0^1 \\left(\\mathbf{1}_{[0,1/2]}(t)\\right)\\left(2\\,\\mathbf{1}_{[0,1/2]}(t)\\right)\\,\\mathrm{d}t = \\int_0^{1/2} 2\\,\\mathrm{d}t = 2 \\times \\frac{1}{2} = 1\n$$\nThe $(2,1)$ entry is equal to the $(1,2)$ entry because the matrix is symmetric:\n$$\n(\\Gamma_F)_{21} = \\int_0^1 (D_t F_2)(D_t F_1)\\,\\mathrm{d}t = (\\Gamma_F)_{12} = 1\n$$\nThe $(2,2)$ entry is:\n$$\n(\\Gamma_F)_{22} = \\int_0^1 (D_t F_2)^2\\,\\mathrm{d}t = \\int_0^1 k(t)^2\\,\\mathrm{d}t = \\int_0^1 \\left(2\\,\\mathbf{1}_{[0,1/2]}(t)\\right)^2\\,\\mathrm{d}t = \\int_0^{1/2} 4\\,\\mathrm{d}t = 4 \\times \\frac{1}{2} = 2\n$$\nSo, the Malliavin covariance matrix is:\n$$\n\\Gamma_F = \\begin{pmatrix} 1/2  1 \\\\ 1  2 \\end{pmatrix}\n$$\n\nThe third step is to compute the determinant of $\\Gamma_F$.\n$$\n\\det(\\Gamma_F) = \\left(\\frac{1}{2}\\right) \\times 2 - 1 \\times 1 = 1 - 1 = 0\n$$\n\nThe final part of the problem asks us to determine if the law of $F$ is absolutely continuous or singular with respect to the Lebesgue measure on $\\mathbb{R}^2$.\n\nA fundamental result in stochastic analysis states that for a random vector $F$ whose components are in the domain of the Malliavin derivative, the law of $F$ is absolutely continuous with respect to the Lebesgue measure if its Malliavin covariance matrix $\\Gamma_F$ is invertible, i.e., $\\det(\\Gamma_F) \\neq 0$.\n\nIn our case, we have found that $\\det(\\Gamma_F)=0$. This indicates that the law of $F$ is not absolutely continuous; it is singular. Let's explain this from first principles as requested.\n\nThe vector $F = (F_1, F_2)$ is a centered Gaussian vector, as its components are Wiener integrals of deterministic functions. For a Gaussian vector, the Malliavin covariance matrix $\\Gamma_F$ coincides with the usual covariance matrix $C_F = (\\mathbb{E}[F_i F_j])_{i,j}$. The fact that $\\det(C_F) = 0$ means the Gaussian distribution is degenerate. A degenerate Gaussian distribution in $\\mathbb{R}^2$ does not have a probability density function with respect to the two-dimensional Lebesgue measure, $\\lambda_2$. Its probability mass is concentrated on a lower-dimensional affine subspace, which in this case is a line through the origin.\n\nLet's make this more explicit. The definition of absolute continuity of a probability measure $\\mu_F$ (the law of $F$) with respect to the Lebesgue measure $\\lambda_2$ is that for any Borel set $A \\subset \\mathbb{R}^2$, if $\\lambda_2(A)=0$, then $\\mu_F(A) = \\mathbb{P}(F \\in A) = 0$.\nTo show the law is singular, we must find a set $A_0$ such that $\\lambda_2(A_0)=0$ but $\\mathbb{P}(F \\in A_0)  0$.\n\nConsider the relationship between the integrands $h(t)$ and $k(t)$:\n$$\nk(t) = 2\\,\\mathbf{1}_{[0,1/2]}(t) = 2\\,h(t)\n$$\nUsing the linearity of the Wiener integral, we can relate $F_2$ to $F_1$:\n$$\nF_2 = \\int_0^1 k(t)\\,\\mathrm{d}B_t = \\int_0^1 2\\,h(t)\\,\\mathrm{d}B_t = 2 \\int_0^1 h(t)\\,\\mathrm{d}B_t = 2F_1\n$$\nThis relationship $F_2 = 2F_1$ holds almost surely. This means that the random vector $F=(F_1, F_2)$ is constrained to lie on the line $L = \\{(x,y) \\in \\mathbb{R}^2 \\mid y=2x\\}$. The entire probability mass of the distribution of $F$ is supported on this line.\n\nThe two-dimensional Lebesgue measure of the line $L$ is $\\lambda_2(L)=0$. However, the probability that $F$ lies on this line is:\n$$\n\\mathbb{P}(F \\in L) = \\mathbb{P}(F_2 = 2F_1) = 1\n$$\nSince we have found a set $L$ for which $\\lambda_2(L)=0$ and $\\mathbb{P}(F \\in L) = 1  0$, the condition for absolute continuity is violated. Therefore, the law of $F$ is singular with respect to the Lebesgue measure on $\\mathbb{R}^2$. This confirms the implication of $\\det(\\Gamma_F)=0$.", "answer": "$$\\boxed{0}$$", "id": "2986307"}, {"introduction": "One of the most celebrated results linking the Malliavin derivative to classical stochastic calculus is the Clark-Ocone formula, which gives an explicit martingale representation for functionals of Brownian motion. This exercise guides you through applying this formula to the payoff of a European call option, $F = \\max(W_T, 0)$, a foundational problem in mathematical finance. By employing a crucial smooth approximation technique, you will derive the explicit predictable integrand, which corresponds to the dynamic hedging strategy for this derivative [@problem_id:3000571].", "problem": "Let $\\{W_{t}\\}_{t \\in [0,T]}$ be a standard Brownian motion on a filtered probability space $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_{t}\\}_{t \\in [0,T]},\\mathbb{P})$ satisfying the usual conditions. Consider the terminal functional $F = \\max(W_{T},0)$.\n\n1. Construct a family $\\{f_{\\epsilon}\\}_{\\epsilon0}$ of smooth functions $f_{\\epsilon}:\\mathbb{R}\\to\\mathbb{R}$ such that $f_{\\epsilon}(x) \\to x^{+}$ pointwise as $\\epsilon \\to 0$, and $f_{\\epsilon}$ has a bounded and continuous first derivative for each fixed $\\epsilon0$. Justify that $f_{\\epsilon}(W_{T}) \\to F$ in $L^{2}(\\Omega)$ as $\\epsilon \\to 0$.\n\n2. Using foundational definitions from the Malliavin calculus on the Wiener space (specifically, the Malliavin derivative of functionals of $W_{T}$ and the chain rule), compute the Malliavin derivative $D_{t}\\big(f_{\\epsilon}(W_{T})\\big)$ for $t \\in [0,T]$.\n\n3. Use independence of Brownian increments and standard properties of conditional expectation to express $\\mathbb{E}\\big[D_{t}\\big(f_{\\epsilon}(W_{T})\\big)\\,|\\,\\mathcal{F}_{t}\\big]$ in terms of $W_{t}$ and the law of $W_{T}-W_{t}$. Then analyze the limit as $\\epsilon \\to 0$ to obtain the predictable integrand $g_{t}$ that appears in the martingale representation of $F$ with respect to $\\{W_{t}\\}$.\n\nYour final task is to report the limiting analytic expression for $g_{t}$ obtained in step 3 (valid for $t \\in [0,T)$). The final answer must be a single closed-form expression.", "solution": "The problem requires finding the predictable integrand $g_{t}$ in the Clark-Ocone martingale representation of the functional $F = \\max(W_{T},0) = W_{T}^{+}$. The Clark-Ocone formula states that for a suitable functional $F$, its Itô representation is given by $F = \\mathbb{E}[F] + \\int_{0}^{T} g_{t} dW_{t}$, where the integrand is $g_{t} = \\mathbb{E}[D_{t}F | \\mathcal{F}_{t}]$. The problem guides us through a three-step procedure involving a smooth approximation of $F$.\n\nThe function $x \\mapsto \\max(x,0)$ is not differentiable, so we cannot directly apply the chain rule of Malliavin calculus. We proceed as directed.\n\n1.  Construction of a smooth approximation and $L^{2}$ convergence.\nThe function to be approximated is $x^{+} = \\max(x,0)$. We can express this as $x^{+} = \\frac{1}{2}(x + |x|)$. The non-smoothness comes from the absolute value function $|x| = \\sqrt{x^{2}}$. A standard way to smoothen this is to introduce a small positive parameter $\\epsilon$.\nWe define the family of functions $\\{f_{\\epsilon}\\}_{\\epsilon0}$ by\n$$f_{\\epsilon}(x) = \\frac{1}{2}(x + \\sqrt{x^{2} + \\epsilon^{2}})$$\nFor any fixed $\\epsilon > 0$, the term under the square root is strictly positive, so $f_{\\epsilon}$ is infinitely differentiable ($\\mathcal{C}^\\infty$) on $\\mathbb{R}$. As $\\epsilon \\to 0$, we have $\\sqrt{x^{2} + \\epsilon^{2}} \\to \\sqrt{x^2} = |x|$, so $f_{\\epsilon}(x) \\to \\frac{1}{2}(x + |x|) = x^+$ for every $x \\in \\mathbb{R}$. This shows pointwise convergence.\nThe first derivative of $f_{\\epsilon}$ is:\n$$f_{\\epsilon}'(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(x + \\sqrt{x^{2} + \\epsilon^{2}}) \\right] = \\frac{1}{2}\\left(1 + \\frac{2x}{2\\sqrt{x^{2} + \\epsilon^{2}}}\\right) = \\frac{1}{2}\\left(1 + \\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}\\right)$$\nFor $\\epsilon > 0$, $f_{\\epsilon}'(x)$ is a continuous function of $x$. To check if it is bounded, we analyze the term $\\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}$. Its absolute value is $\\left|\\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}\\right| = \\frac{|x|}{\\sqrt{x^{2} + \\epsilon^{2}}} \\le \\frac{|x|}{\\sqrt{x^{2}}} = 1$ for $x \\ne 0$. The inequality is strict for all $x$. Thus, $-1  \\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}  1$, which implies $0  1 + \\frac{x}{\\sqrt{x^{2} + \\epsilon^{2}}}  2$. Therefore, $0  f_{\\epsilon}'(x)  1$. The derivative is continuous and bounded for each $\\epsilon > 0$.\n\nTo show that $f_{\\epsilon}(W_{T}) \\to F$ in $L^{2}(\\Omega)$, we must show that $\\mathbb{E}\\left[|f_{\\epsilon}(W_{T}) - W_{T}^{+}|^{2}\\right] \\to 0$ as $\\epsilon \\to 0$. Let's examine the difference $|f_{\\epsilon}(x) - x^{+}|$:\n$$|f_{\\epsilon}(x) - x^{+}| = \\left|\\frac{1}{2}(x + \\sqrt{x^{2} + \\epsilon^{2}}) - \\frac{1}{2}(x + |x|)\\right| = \\frac{1}{2}|\\sqrt{x^{2} + \\epsilon^{2}} - |x||$$\nMultiplying by the conjugate, we get:\n$$\\frac{1}{2} \\left| \\frac{(\\sqrt{x^{2} + \\epsilon^{2}} - |x|)(\\sqrt{x^{2} + \\epsilon^{2}} + |x|)}{\\sqrt{x^{2} + \\epsilon^{2}} + |x|} \\right| = \\frac{1}{2} \\frac{|x^{2} + \\epsilon^{2} - x^{2}|}{\\sqrt{x^{2} + \\epsilon^{2}} + |x|} = \\frac{\\epsilon^{2}}{2(\\sqrt{x^{2} + \\epsilon^{2}} + |x|)}$$\nSince $\\sqrt{x^{2} + \\epsilon^{2}} \\ge \\sqrt{\\epsilon^{2}} = \\epsilon$ and $|x| \\ge 0$, the denominator is at least $\\epsilon$.\n$$|f_{\\epsilon}(x) - x^{+}| \\le \\frac{\\epsilon^{2}}{2\\epsilon} = \\frac{\\epsilon}{2}$$\nThis bound is uniform in $x$. Thus, applied to $W_{T}$, we have $|f_{\\epsilon}(W_{T}) - W_{T}^{+}| \\le \\frac{\\epsilon}{2}$ for all $\\omega \\in \\Omega$.\nThe $L^{2}$ norm of the difference is then bounded by:\n$$\\mathbb{E}\\left[|f_{\\epsilon}(W_{T}) - W_{T}^{+}|^{2}\\right] \\le \\mathbb{E}\\left[\\left(\\frac{\\epsilon}{2}\\right)^{2}\\right] = \\frac{\\epsilon^{2}}{4}$$\nAs $\\epsilon \\to 0$, $\\frac{\\epsilon^{2}}{4} \\to 0$, which proves that $f_{\\epsilon}(W_{T}) \\to W_{T}^{+}$ in $L^{2}(\\Omega)$.\n\n2.  Computation of the Malliavin derivative.\nThe Malliavin derivative of a functional of the form $f(W_{T})$, where $f$ is continuously differentiable with a bounded derivative, is given by the chain rule: $D_{t}(f(W_{T})) = f'(W_{T}) D_{t}W_{T}$. The Malliavin derivative of $W_{T} = \\int_{0}^{T} 1 \\, dW_{s}$ is $D_{t}W_{T} = 1$ for $t \\in [0,T]$.\nApplying this to our approximation $F_{\\epsilon} = f_{\\epsilon}(W_{T})$, we get:\n$$D_{t}(f_{\\epsilon}(W_{T})) = f_{\\epsilon}'(W_{T})$$\nfor $t \\in [0,T]$. Substituting the expression for $f_{\\epsilon}'(x)$ from Step 1:\n$$D_{t}(f_{\\epsilon}(W_{T})) = \\frac{1}{2}\\left(1 + \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}}\\right)$$\n\n3.  Conditional expectation and limiting expression.\nThe integrand in the Clark-Ocone representation for $F$ is $g_{t} = \\lim_{\\epsilon \\to 0} \\mathbb{E}[D_{t}(f_{\\epsilon}(W_{T})) | \\mathcal{F}_{t}]$. Let's first compute the conditional expectation for fixed $\\epsilon$:\n$$g_{t}^{\\epsilon} = \\mathbb{E}[D_{t}(f_{\\epsilon}(W_{T})) | \\mathcal{F}_{t}] = \\mathbb{E}\\left[\\frac{1}{2}\\left(1 + \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}}\\right) \\bigg| \\mathcal{F}_{t}\\right]$$\nBy linearity of conditional expectation:\n$$g_{t}^{\\epsilon} = \\frac{1}{2} + \\frac{1}{2}\\mathbb{E}\\left[\\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}} \\bigg| \\mathcal{F}_{t}\\right]$$\nThe random variable inside the expectation, $h_{\\epsilon}(W_{T}) = \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}}$, is bounded in absolute value by $1$. As $\\epsilon \\to 0$, $h_{\\epsilon}(x) \\to \\text{sgn}(x)$ for $x \\ne 0$. Since $\\mathbb{P}(W_{T}=0)=0$, this convergence happens almost surely. By the bounded convergence theorem for conditional expectations, we can interchange the limit and the expectation:\n$$g_{t} = \\lim_{\\epsilon \\to 0} g_{t}^{\\epsilon} = \\frac{1}{2} + \\frac{1}{2}\\mathbb{E}\\left[\\lim_{\\epsilon \\to 0} \\frac{W_{T}}{\\sqrt{W_{T}^{2} + \\epsilon^{2}}} \\bigg| \\mathcal{F}_{t}\\right] = \\frac{1}{2}\\left(1 + \\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}]\\right)$$\nNow we compute $\\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}]$. Note that $\\text{sgn}(x) = \\mathbf{1}_{x>0} - \\mathbf{1}_{x0}$.\n$$\\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}] = \\mathbb{E}[\\mathbf{1}_{W_{T}>0} | \\mathcal{F}_{t}] - \\mathbb{E}[\\mathbf{1}_{W_{T}0} | \\mathcal{F}_{t}] = \\mathbb{P}(W_{T}>0 | \\mathcal{F}_{t}) - \\mathbb{P}(W_{T}0 | \\mathcal{F}_{t})$$\nWe decompose $W_{T}$ as $W_{T} = W_{t} + (W_{T} - W_{t})$. Given $\\mathcal{F}_{t}$, $W_{t}$ is a known value, and the increment $W_{T} - W_{t}$ is independent of $\\mathcal{F}_{t}$ and follows a normal distribution with mean $0$ and variance $T-t$. So, $W_{T} - W_{t} \\sim N(0, T-t)$.\nWe have, for $t  T$:\n$$\\mathbb{P}(W_{T} > 0 | \\mathcal{F}_{t}) = \\mathbb{P}(W_{t} + (W_{T} - W_{t}) > 0 | \\mathcal{F}_{t}) = \\mathbb{P}(W_{T} - W_{t} > -W_{t})$$\nLet $Z \\sim N(0,1)$ be a standard normal variable. Then $W_{T} - W_{t}$ has the same distribution as $\\sqrt{T-t} Z$.\n$$\\mathbb{P}(\\sqrt{T-t}Z > -W_{t}) = \\mathbb{P}\\left(Z > -\\frac{W_{t}}{\\sqrt{T-t}}\\right)$$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} \\exp(-\\frac{y^{2}}{2}) dy$.\nThe probability is $1 - \\Phi\\left(-\\frac{W_{t}}{\\sqrt{T-t}}\\right)$. Using the symmetry property $\\Phi(-z) = 1 - \\Phi(z)$, we get:\n$$\\mathbb{P}(W_{T} > 0 | \\mathcal{F}_{t}) = \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)$$\nSimilarly, $\\mathbb{P}(W_{T}  0 | \\mathcal{F}_{t}) = \\mathbb{P}(Z  -\\frac{W_{t}}{\\sqrt{T-t}}) = \\Phi(-\\frac{W_{t}}{\\sqrt{T-t}}) = 1 - \\Phi(\\frac{W_{t}}{\\sqrt{T-t}})$.\nTherefore,\n$$\\mathbb{E}[\\text{sgn}(W_{T}) | \\mathcal{F}_{t}] = \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right) - \\left(1 - \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)\\right) = 2\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right) - 1$$\nSubstituting this back into the expression for $g_{t}$:\n$$g_{t} = \\frac{1}{2}\\left(1 + \\left(2\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right) - 1\\right)\\right) = \\frac{1}{2}\\left(2\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)\\right) = \\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)$$\nThe expression is valid for $t \\in [0,T)$.\n\nThe final result for the predictable integrand is the CDF of the standard normal distribution evaluated at $\\frac{W_{t}}{\\sqrt{T-t}}$.", "answer": "$$\\boxed{\\Phi\\left(\\frac{W_{t}}{\\sqrt{T-t}}\\right)}$$", "id": "3000571"}]}