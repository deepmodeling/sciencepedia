{"hands_on_practices": [{"introduction": "A powerful way to understand a new, general theory is to see how it recovers familiar, specific cases. This first practice problem guides you through the process of connecting the Stochastic Maximum Principle (SMP) back to the classical Pontryagin's Maximum Principle (PMP) [@problem_id:3003302]. By analyzing a linear-quadratic system where the control-dependent noise is removed, you will discover the precise role of the adjoint martingale process $q_t$ and why it vanishes in the deterministic limit, solidifying your intuition for the structure of the SMP.", "problem": "Consider the scalar controlled system on the finite horizon $[0,T]$ given by the stochastic differential equation\n$$\ndX_t \\;=\\; \\big(a\\,X_t + b\\,u_t\\big)\\,dt \\;+\\; D\\,u_t\\,dW_t,\\qquad X_0 = x_0,\n$$\nwhere $a$, $b$, $D$, and $x_0$ are real constants, $u_t$ is an admissible control process taking values in $\\mathbb{R}$, and $W_t$ is a standard Brownian motion. The performance index is\n$$\nJ(u) \\;=\\; \\mathbb{E}\\!\\left[\\,\\frac{1}{2}\\,M\\,X_T^2 \\;+\\; \\int_0^T \\frac{1}{2}\\,\\big(Q\\,X_t^2 + R\\,u_t^2\\big)\\,dt \\right],\n$$\nwith $M0$, $Q\\ge 0$, and $R0$ constants. Define the Hamiltonian $H$ associated to the Stochastic Maximum Principle (SMP) as\n$$\nH(t,x,u,p,q) \\;=\\; p\\,\\big(a\\,x + b\\,u\\big) \\;+\\; q\\,\\big(D\\,u\\big) \\;+\\; \\frac{1}{2}\\,\\big(Q\\,x^2 + R\\,u^2\\big),\n$$\nwhere $p_t$ and $q_t$ are the adjoint (costate and martingale) processes entering the backward stochastic differential equation (BSDE) for the SMP. \n\nAnalyze the special case $D=0$ and rigorously recover, starting from the variational structure of the SMP and fundamental properties of Itô processes and quadratic variation, the classical Pontryagin Maximum Principle (PMP) structure: a deterministic adjoint ordinary differential equation (ODE) and a pointwise-in-time stationarity (minimum) condition for the Hamiltonian. In particular, identify the adjoint martingale term $q_t$ and explain its role, then determine its explicit expression when $D=0$.\n\nYour final answer must be the explicit analytic expression for the adjoint martingale term $q_t$ in the case $D=0$. No rounding is necessary. Express your final answer as a single closed-form analytic expression.", "solution": "The analysis begins by recalling the general formulation of the Stochastic Maximum Principle (SMP) for the given problem. The system is described by the state equation\n$$dX_t = b(t, X_t, u_t) dt + \\sigma(t, X_t, u_t) dW_t$$\nand the cost functional to be minimized is\n$$J(u) = \\mathbb{E}\\left[g(X_T) + \\int_0^T f(t, X_t, u_t) dt\\right]$$\nFor the specific problem statement, we identify the components:\n- Drift: $b(t,x,u) = a\\,x + b\\,u$\n- Diffusion: $\\sigma(t,x,u) = D\\,u$\n- Terminal cost: $g(x) = \\frac{1}{2}\\,M\\,x^2$\n- Running cost: $f(t,x,u) = \\frac{1}{2}\\,\\big(Q\\,x^2 + R\\,u^2\\big)$\n\nThe Hamiltonian given is $H(t,x,u,p,q) = p\\,(a\\,x + b\\,u) + q\\,(D\\,u) + \\frac{1}{2}\\,\\big(Q\\,x^2 + R\\,u^2\\big)$, which corresponds to the standard minimization definition $H = p b + q \\sigma + f$.\n\nThe necessary conditions for an optimal control $\\hat{u}_t$ and corresponding state $\\hat{X}_t$ are given by the SMP. These conditions involve a pair of adjoint processes, the costate $p_t$ and the martingale term $q_t$, which satisfy the following backward stochastic differential equation (BSDE):\n$$-dp_t = \\nabla_x H(t, \\hat{X}_t, \\hat{u}_t, p_t, q_t) dt - q_t dW_t, \\qquad p_T = \\nabla_x g(\\hat{X}_T)$$\nwhere $\\nabla_x H$ denotes the partial derivative of the Hamiltonian with respect to the state variable $x$. Moreover, the optimal control $\\hat{u}_t$ must satisfy the minimum condition:\n$$H(t, \\hat{X}_t, \\hat{u}_t, p_t, q_t) = \\inf_{v \\in \\mathbb{R}} H(t, \\hat{X}_t, v, p_t, q_t)$$\nFor our specific problem, the partial derivative of the Hamiltonian with respect to $x$ is:\n$$\\nabla_x H = \\frac{\\partial}{\\partial x} \\left[ p(ax+bu) + q(Du) + \\frac{1}{2}(Qx^2+Ru^2) \\right] = a\\,p_t + Q\\,\\hat{X}_t$$\nThe terminal condition for $p_t$ is:\n$$p_T = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2} M x^2 \\right) \\bigg|_{x=\\hat{X}_T} = M\\,\\hat{X}_T$$\nThus, the adjoint BSDE is:\n$$-dp_t = (a\\,p_t + Q\\,\\hat{X}_t) dt - q_t dW_t$$\nor equivalently, in forward form:\n$$dp_t = -(a\\,p_t + Q\\,\\hat{X}_t) dt + q_t dW_t$$\nSince the control set is $\\mathbb{R}$ and the Hamiltonian is strictly convex in $u$ (because $R0$), the minimum condition is equivalent to the first-order stationarity condition $\\nabla_u H = 0$:\n$$\\nabla_u H = \\frac{\\partial}{\\partial u} \\left[ p(ax+bu) + q(Du) + \\frac{1}{2}(Qx^2+Ru^2) \\right] = b\\,p_t + D\\,q_t + R\\,\\hat{u}_t = 0$$\nThis gives the optimal control in terms of the adjoint processes:\n$$\\hat{u}_t = -\\frac{1}{R}(b\\,p_t + D\\,q_t)$$\n\nNow, we analyze the special case where $D=0$.\nIn this case, the system dynamics become:\n$$dX_t = (a\\,X_t + b\\,u_t)\\,dt, \\qquad X_0 = x_0$$\nThis is a deterministic ordinary differential equation (ODE) for any given control function $u(t)$. The Brownian motion $W_t$ no longer drives the state dynamics. The cost functional remains:\n$$J(u) = \\mathbb{E}\\!\\left[\\,\\frac{1}{2}\\,M\\,X_T^2 \\;+\\; \\int_0^T \\frac{1}{2}\\,\\big(Q\\,X_t^2 + R\\,u_t^2\\big)\\,dt \\right]$$\nThe problem is to find the optimal control $u_t$ within the class of admissible controls, which are processes adapted to the filtration $\\mathcal{F}_t$ generated by $W_t$.\n\nThe crucial step is to demonstrate that the optimal control $\\hat{u}_t$ must be deterministic. Let $u_t$ be any admissible adapted control. We can decompose it into its deterministic mean and a stochastic part with zero mean:\n$$u_t = \\bar{u}_t + \\tilde{u}_t, \\quad \\text{where} \\quad \\bar{u}_t = \\mathbb{E}[u_t] \\quad \\text{and} \\quad \\mathbb{E}[\\tilde{u}_t] = 0$$\nDue to the linearity of the state equation, the state $X_t$ can be similarly decomposed as $X_t = \\bar{X}_t + \\tilde{X}_t$, where:\n$$\\frac{d\\bar{X}_t}{dt} = a\\,\\bar{X}_t + b\\,\\bar{u}_t, \\quad \\bar{X}_0 = x_0$$\n$$\\frac{d\\tilde{X}_t}{dt} = a\\,\\tilde{X}_t + b\\,\\tilde{u}_t, \\quad \\tilde{X}_0 = 0$$\nSince $\\bar{X}_t$ and $\\bar{u}_t$ are deterministic, and $\\mathbb{E}[\\tilde{X}_t] = \\mathbb{E}[\\int_0^t e^{a(t-s)}b\\,\\tilde{u}_s ds] = \\int_0^t e^{a(t-s)}b\\,\\mathbb{E}[\\tilde{u}_s] ds = 0$.\n\nLet's expand the cost functional:\n$$J(u) = \\frac{1}{2} \\mathbb{E} \\left[ M(\\bar{X}_T + \\tilde{X}_T)^2 + \\int_0^T \\left( Q(\\bar{X}_t + \\tilde{X}_t)^2 + R(\\bar{u}_t + \\tilde{u}_t)^2 \\right) dt \\right]$$\nExpanding the quadratic terms and using the linearity of expectation:\n$$J(u) = \\frac{1}{2} \\left( M\\bar{X}_T^2 + \\int_0^T (Q\\bar{X}_t^2 + R\\bar{u}_t^2) dt \\right) + \\frac{1}{2} \\mathbb{E}\\left[ M\\tilde{X}_T^2 + \\int_0^T (Q\\tilde{X}_t^2 + R\\tilde{u}_t^2) dt \\right]$$\nAll cross-terms vanish because, for example, $\\mathbb{E}[\\bar{X}_t \\tilde{X}_t] = \\bar{X}_t \\mathbb{E}[\\tilde{X}_t] = 0$. The first term is precisely the cost functional for the deterministic control $\\bar{u}_t$, i.e., $J(\\bar{u})$. The second term is a non-negative quantity, as $M0$, $Q\\ge 0$, and $R0$.\n$$J(u) = J(\\bar{u}) + \\frac{1}{2} \\mathbb{E}\\left[ M\\tilde{X}_T^2 + \\int_0^T (Q\\tilde{X}_t^2 + R\\tilde{u}_t^2) dt \\right]$$\nThis inequality shows that $J(u) \\ge J(\\bar{u})$. The minimum is achieved if and only if the second term is zero. Since $R0$, this requires $\\mathbb{E}[\\int_0^T R\\tilde{u}_t^2 dt] = 0$, which implies $\\tilde{u}_t = 0$ for almost every $t \\in [0,T]$, almost surely. Therefore, any optimal control $\\hat{u}_t$ must be deterministic.\n\nNow we return to the SMP results for the case $D=0$. The optimal control is given by:\n$$\\hat{u}_t = -\\frac{1}{R}(b\\,p_t + (0)\\,q_t) = -\\frac{b}{R}p_t$$\nSince we have rigorously established that $\\hat{u}_t$ must be a deterministic function of time, and $b$ and $R$ are constants, the adjoint process $p_t$ must also be a deterministic function of time.\n\nThe process $p_t$ is the solution to the BSDE:\n$$dp_t = -(a\\,p_t + Q\\,\\hat{X}_t) dt + q_t dW_t, \\qquad p_T = M\\,\\hat{X}_T$$\nAn Itô process is deterministic if and only if its stochastic integral component is identically zero. The stochastic differential of a deterministic, differentiable function $p(t)$ is simply $dp(t) = \\frac{dp}{dt} dt$. For the Itô process $p_t$ to be deterministic, its diffusion coefficient must be zero. The diffusion part of $dp_t$ is $q_t dW_t$. For this term to be zero, the coefficient process $q_t$ must be zero for almost every $t \\in [0,T]$, almost surely.\n\nThus, we must have $q_t = 0$.\n\nThe role of the adjoint martingale term $q_t$ in the general SMP is to model the effect of uncertainty propagation in the adjoint system. It captures how the \"shadow price\" $p_t$ must adjust in response to the same random shocks that affect the state system. When the state system's evolution becomes independent of the random shocks (i.e., $D=0$), there is no longer any stochastic information for the adjoint process to react to, and its martingale component $q_t$ must vanish.\n\nWith $q_t=0$, the SMP system reduces to:\n1. State ODE: $\\frac{d\\hat{X}_t}{dt} = a\\,\\hat{X}_t + b\\,\\hat{u}_t$\n2. Adjoint ODE: $\\frac{dp_t}{dt} = -(a\\,p_t + Q\\,\\hat{X}_t)$\n3. Transversality condition: $p_T = M\\,\\hat{X}_T$\n4. Optimality condition: $\\hat{u}_t = -\\frac{b}{R} p_t$\n\nThis is precisely the set of necessary conditions that constitutes the Pontryagin Maximum Principle (PMP) for the corresponding deterministic linear-quadratic optimal control problem. The analysis shows how the more general SMP gracefully degrades to the classical PMP when the stochastic elements are removed from the system dynamics. The explicit expression for the adjoint martingale term $q_t$ in this case is zero.", "answer": "$$\\boxed{0}$$", "id": "3003302"}, {"introduction": "The Stochastic Maximum Principle furnishes a set of necessary conditions, identifying candidate optimal controls by finding stationary points of the Hamiltonian. This practice problem demonstrates a crucial subtlety: without appropriate convexity assumptions, these stationary points are not guaranteed to be globally optimal [@problem_id:3003286]. By working through a counterexample with a non-convex cost function, you will see how multiple controls can satisfy the first-order conditions while yielding vastly different outcomes, emphasizing the need for a deeper analysis.", "problem": "Consider the following stochastic control problem over a fixed time horizon $T0$. Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ carry a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$. The scalar state $X=(X_t)_{t\\in[0,T]}$ evolves according to the stochastic differential equation\n$$\ndX_t \\;=\\; u_t\\,dt \\;+\\; \\sigma\\,dW_t,\\qquad X_0=x_0,\n$$\nwhere $\\sigma\\in\\mathbb{R}$ is a constant and the control $u=(u_t)_{t\\in[0,T]}$ is progressively measurable with respect to $(\\mathcal{F}_t)_{t\\in[0,T]}$ and satisfies $\\mathbb{E}\\!\\left[\\int_0^T |u_t|^2\\,dt\\right]\\infty$. The objective is to minimize the cost functional\n$\nJ(u)\\;=\\;\\mathbb{E}\\!\\left[\\int_0^T \\ell(u_t)\\,dt\\right],\n$\nwith running cost\n$\n\\ell(u)\\;=\\;(u^2-1)^2,\n$\nand no terminal cost.\n\nUsing only the foundational definitions of stochastic differential equations and the first-order necessary conditions given by the stochastic maximum principle (SMP), perform the following:\n\n1) Derive the Hamiltonian, the adjoint backward stochastic differential equation, and the first-order stationarity condition characterizing necessary candidates for optimal controls in this problem. Do not assume any convexity.\n\n2) Identify a time-constant, admissible control $u^{\\mathrm{stat}}$ that satisfies the stationarity condition but is not a global minimizer of $J(u)$.\n\n3) Exhibit a time-constant, admissible control $u^{\\mathrm{opt}}$ that strictly improves the cost over $u^{\\mathrm{stat}}$.\n\n4) Compute the exact analytic value of the cost gap $J(u^{\\mathrm{stat}})-J(u^{\\mathrm{opt}})$ as a function of $T$.\n\nExpress your final answer as an exact analytic expression in terms of $T$ only. No rounding is required, and no physical units are involved.", "solution": "The solution is developed in four parts as requested by the problem statement.\n\n### 1) Hamiltonian, Adjoint Equation, and Stationarity Condition\n\nThe stochastic control problem is defined by the state dynamics $dX_t = b(X_t, u_t)\\,dt + \\sigma(X_t, u_t)\\,dW_t$ and the cost $J(u) = \\mathbb{E}[\\int_0^T f(X_t, u_t)\\,dt + g(X_T)]$. In this specific problem, we have:\n- Drift: $b(x,u) = u$.\n- Diffusion: $\\sigma(x,u) = \\sigma$.\n- Running cost: $f(x,u) = (u^2-1)^2$.\n- Terminal cost: $g(x) = 0$.\nNotice that the functions $b$, $\\sigma$, and $f$ do not depend on the state $x$.\n\n**Hamiltonian:**\nThe Hamiltonian $H$ for a minimization problem is defined as $H(x, u, p, q) = p \\cdot b(x,u) + q \\cdot \\sigma(x,u) + f(x,u)$. Substituting the given functions, we get:\n$$\nH(x, u, p, q) = p u + q \\sigma + (u^2-1)^2\n$$\nSince the functions do not depend on $x$, the Hamiltonian is also independent of $x$.\n\n**Adjoint Backward Stochastic Differential Equation (BSDE):**\nThe adjoint processes, the scalar process $p=(p_t)_{t\\in[0,T]}$ and the scalar process $q=(q_t)_{t\\in[0,T]}$, are given by the solution to the BSDE:\n$$\ndp_t = -\\nabla_x H(X_t, u_t, p_t, q_t)\\,dt + q_t\\,dW_t\n$$\nwith the terminal condition:\n$$\np_T = \\nabla_x g(X_T)\n$$\nWe compute the partial derivatives:\n$$\n\\nabla_x H = \\frac{\\partial}{\\partial x} \\left[ p u + q \\sigma + (u^2-1)^2 \\right] = 0\n$$\n$$\n\\nabla_x g = \\frac{\\partial}{\\partial x} [0] = 0\n$$\nThe adjoint BSDE simplifies to:\n$$\ndp_t = 0 \\cdot dt + q_t\\,dW_t = q_t\\,dW_t, \\quad p_T = 0\n$$\nThe unique solution to this BSDE is $(p_t, q_t) = (0, 0)$ for all $t \\in [0,T]$. This can be seen by integrating from $t$ to $T$: $p_T - p_t = \\int_t^T q_s dW_s$. Since $p_T=0$, we have $p_t = -\\int_t^T q_s dW_s$. The process $p_t$ is a martingale with $p_T=0$, which implies $p_t = \\mathbb{E}[p_T | \\mathcal{F}_t] = 0$. From $dp_t=0$, we have $q_t dW_t=0$, which implies $q_t=0$ for almost every $t$.\n\n**First-Order Stationarity Condition:**\nThe stochastic maximum principle provides a set of necessary conditions for optimality. The first-order stationarity condition states that for an optimal control $\\hat{u}_t$, the derivative of the Hamiltonian with respect to the control variable must be zero, provided the control set is open.\n$$\n\\nabla_u H(X_t, \\hat{u}_t, p_t, q_t) = 0\n$$\nWe compute the partial derivative of our Hamiltonian:\n$$\n\\nabla_u H = \\frac{\\partial}{\\partial u} \\left[ p_t u_t + q_t \\sigma + (u_t^2-1)^2 \\right] = p_t + 2(u_t^2-1)(2u_t) = p_t + 4u_t(u_t^2-1)\n$$\nSince we found that $p_t=0$ for all $t$, the stationarity condition becomes:\n$$\n4u_t(u_t^2-1) = 0\n$$\nThis equation yields the possible values for a stationary control at any time $t$:\n$$\nu_t(u_t^2-1) = 0 \\implies u_t \\in \\{0, 1, -1\\}\n$$\nSo, any optimal control must take values in the set $\\{0, 1, -1\\}$ for almost every $t \\in [0,T]$.\n\n### 2) A Non-Optimal Stationary Control $u^{\\mathrm{stat}}$\n\nThe problem asks for a time-constant control $u^{\\mathrm{stat}}$ that satisfies the stationarity condition but is not a global minimizer. The time-constant candidates from the stationarity condition are $u(t) \\equiv 0$, $u(t) \\equiv 1$, and $u(t) \\equiv -1$. All three are admissible since for any constant $c$, $\\mathbb{E}[\\int_0^T |c|^2 dt] = c^2T  \\infty$.\n\nLet's evaluate the cost $J(u)$ for each of these constant controls. For a constant control $u_t=c$, the cost is:\n$$\nJ(c) = \\mathbb{E}\\left[\\int_0^T (c^2-1)^2 dt\\right] = (c^2-1)^2 \\int_0^T dt = T(c^2-1)^2\n$$\n- For $c=0$: $J(0) = T(0^2-1)^2 = T$.\n- For $c=1$: $J(1) = T(1^2-1)^2 = 0$.\n- For $c=-1$: $J(-1) = T((-1)^2-1)^2 = 0$.\n\nThe cost functional is always non-negative, i.e., $J(u) = \\mathbb{E}[\\int_0^T (u_t^2-1)^2 dt] \\ge 0$. The minimum possible cost is $0$.\nThe controls $u_t=1$ and $u_t=-1$ achieve this minimum cost, so they are global minimizers.\nThe control $u_t=0$ yields a cost of $J(0)=T  0$. Therefore, it satisfies the stationarity condition but is not a global minimizer.\nWe identify $u^{\\mathrm{stat}}$ as:\n$$\nu^{\\mathrm{stat}}(t) = 0 \\quad \\text{for all } t \\in [0,T]\n$$\n\n### 3) An Optimal Control $u^{\\mathrm{opt}}$ that Improves Over $u^{\\mathrm{stat}}$\n\nWe need to exhibit a time-constant control $u^{\\mathrm{opt}}$ that strictly improves the cost over $u^{\\mathrm{stat}}$. From the calculations above, both $u_t=1$ and $u_t=-1$ result in a cost of $0$, which is strictly less than the cost $T$ of $u^{\\mathrm{stat}}$. We can choose either one. Let's select:\n$$\nu^{\\mathrm{opt}}(t) = 1 \\quad \\text{for all } t \\in [0,T]\n$$\nThe cost for this control is $J(u^{\\mathrm{opt}}) = J(1) = 0$. The cost for $u^{\\mathrm{stat}}$ is $J(u^{\\mathrm{stat}}) = J(0) = T$. Since $T0$, we have $J(u^{\\mathrm{opt}})  J(u^{\\mathrm{stat}})$, so the improvement is strict.\n\n### 4) The Cost Gap $J(u^{\\mathrm{stat}})-J(u^{\\mathrm{opt}})$\n\nWe compute the exact value of the cost gap as a function of $T$.\n$$\nJ(u^{\\mathrm{stat}}) = J(0) = T(0^2-1)^2 = T\n$$\n$$\nJ(u^{\\mathrm{opt}}) = J(1) = T(1^2-1)^2 = 0\n$$\nThe cost gap is therefore:\n$$\nJ(u^{\\mathrm{stat}}) - J(u^{\\mathrm{opt}}) = T - 0 = T\n$$\nThe final result is solely dependent on the time horizon $T$.", "answer": "$$ \\boxed{T} $$", "id": "3003286"}, {"introduction": "This advanced exercise tackles a scenario where the first-order conditions of the SMP are inconclusive, a situation known as a singular arc. It introduces the fundamental technique of using second-order variational analysis to determine the optimality of a singular control candidate [@problem_id:3003273]. Mastering this approach is essential for handling a broad class of problems where the Hamiltonian is linear in the control, providing a more powerful tool for your analytical toolkit.", "problem": "Consider a one-dimensional controlled stochastic differential equation driven by a standard Brownian motion $W$ on the time interval $[0,1]$,\n$$\ndx_t \\;=\\; u_t\\,dt \\;+\\; \\nu\\,u_t\\,dW_t,\\qquad x_0 \\;=\\; 0,\n$$\nwhere $\\nu\\in\\mathbb{R}$ is a given constant and the control $u$ is progressively measurable and takes values in a fixed compact interval $U=[-\\bar{u},\\bar{u}]$ with $\\bar{u}0$. The performance functional to be maximized is\n$$\nJ(u) \\;=\\; \\mathbb{E}\\!\\left[\\tfrac{1}{2}\\,x_1^2\\right].\n$$\nWork within the framework of the stochastic maximum principle (SMP) and its second-order extension. Begin from the core definitions: the Hamiltonian $H(x,u,p,q)$ for a control-affine system with running cost equal to zero, and the first-order adjoint backward stochastic differential equation with terminal condition given by the gradient of the terminal cost. Show that the first-order SMP is inconclusive at the candidate control $u_t\\equiv 0$, in the sense that the stationarity condition for the Hamiltonian with respect to $u$ and the Legendre condition provide no information to distinguish optimality.\n\nThen analyze a small control perturbation $u^{\\varepsilon}_t=\\varepsilon\\,v_t$ around the singular candidate $u_t\\equiv 0$, where $v_t$ is a deterministic direction and $\\varepsilon$ is a small scalar satisfying $0|\\varepsilon|\\leq \\varepsilon_0$ for some $\\varepsilon_00$ small enough to keep $u^{\\varepsilon}$ in $U$. For the specific choice $v_t\\equiv 1$ on $[0,1]$, derive the second-order variation coefficient $Q(\\nu)$ defined by the relation\n$$\nJ(u^{\\varepsilon}) - J(0) \\;=\\; \\varepsilon^2\\,Q(\\nu) \\;+\\; o(\\varepsilon^2)\\quad\\text{as }\\varepsilon\\to 0,\n$$\nusing only well-tested facts such as Itô’s isometry and basic properties of stochastic integrals. Your derivation must start from the controlled dynamics, construct $x_1$ under $u^{\\varepsilon}$, and compute the expectation in $J(u^{\\varepsilon})$ exactly. Conclude, in words, how the sign of $Q(\\nu)$ eliminates the non-optimal singular control $u\\equiv 0$ via second-order conditions for a maximization problem. Express your final answer for $Q(\\nu)$ as a closed-form analytic expression in terms of $\\nu$. No rounding is required and no units are involved.", "solution": "The solution proceeds in three parts as requested by the problem statement.\n\nFirst, we analyze the candidate control $u_t \\equiv 0$ using the first-order stochastic maximum principle (SMP). The system is control-affine with dynamics $dx_t = b(u_t) dt + \\sigma(u_t) dW_t$, where $b(u) = u$ and $\\sigma(u) = \\nu u$. The running cost is $f(x,u) = 0$ and the terminal cost is $g(x) = \\frac{1}{2}x^2$. The objective is maximization.\n\nThe Hamiltonian for this system is defined as:\n$$\nH(x, u, p, q) = p\\,b(u) + q\\,\\sigma(u) - f(x,u) = p u + q (\\nu u) = (p + \\nu q) u\n$$\nThe adjoint processes $(p_t, q_t)$ satisfy the backward stochastic differential equation (BSDE):\n$$\ndp_t = -\\frac{\\partial H}{\\partial x}(x_t, u_t, p_t, q_t) dt + q_t dW_t, \\quad p_1 = -\\frac{dg}{dx}(x_1)\n$$\nNote the minus sign on the terminal cost gradient for maximization. Since the Hamiltonian $H$ does not depend on the state $x$, we have $\\frac{\\partial H}{\\partial x} = 0$. The terminal condition is $p_1 = -\\frac{d}{dx}(\\frac{1}{2}x^2)\\big|_{x=x_1} = -x_1$.\nThus, the adjoint BSDE is $dp_t = q_t dW_t$ with $p_1 = -x_1$.\n\nLet's evaluate this for the singular candidate control $u_t \\equiv 0$. For $u_t = 0$, the state dynamics become $dx_t = 0$ with $x_0 = 0$. This yields the deterministic state trajectory $x_t \\equiv 0$ for all $t \\in [0, 1]$. Consequently, the terminal state is $x_1 = 0$.\nThe terminal condition for the adjoint process $p_t$ becomes $p_1 = -x_1 = 0$. The unique solution to the BSDE $dp_t = q_t dW_t$ with $p_1 = 0$ is $p_t \\equiv 0$ and $q_t \\equiv 0$ for all $t \\in [0, 1]$.\n\nThe first-order necessary condition of the SMP (Pontryagin's principle) states that an optimal control $\\hat{u}_t$ must satisfy for a.e. $t \\in [0, 1]$:\n$$\nH( \\hat{x}_t, \\hat{u}_t, p_t, q_t) = \\sup_{v \\in U} H( \\hat{x}_t, v, p_t, q_t)\n$$\nFor our candidate $u_t \\equiv 0$, we have $p_t = 0$ and $q_t = 0$. The Hamiltonian becomes $H(x_t, u, p_t, q_t) = (0 + \\nu \\cdot 0) u = 0$ for any value of $u \\in U$. The optimality condition reduces to $0 = \\sup_{v \\in [-\\bar{u}, \\bar{u}]} 0$, which is a tautology. It provides no information to determine the optimal control, as any $v \\in U$ is a maximizer. The stationarity condition $\\frac{\\partial H}{\\partial u} = p_t + \\nu q_t = 0$ is satisfied, and the second-derivative (Legendre-Clebsch) condition $\\frac{\\partial^2 H}{\\partial u^2} = 0$ is also uninformative. Thus, the first-order SMP is inconclusive.\n\nNext, we perform a second-order analysis. We consider the specified control perturbation around the singular control $u_t \\equiv 0$. The perturbed control is $u_t^\\varepsilon = \\varepsilon v_t$ where $v_t \\equiv 1$, so $u_t^\\varepsilon = \\varepsilon$ for $t \\in [0, 1]$. We assume $\\varepsilon$ is small enough such that $\\varepsilon \\in [-\\bar{u}, \\bar{u}]$.\n\nThe state SDE under $u^\\varepsilon$ is:\n$$\ndx_t^\\varepsilon = \\varepsilon dt + \\nu \\varepsilon dW_t, \\quad x_0^\\varepsilon = 0\n$$\nThis is a linear SDE which can be solved by direct integration:\n$$\nx_t^\\varepsilon = \\int_0^t \\varepsilon ds + \\int_0^t \\nu \\varepsilon dW_s = \\varepsilon t + \\nu \\varepsilon W_t\n$$\nAt the terminal time $t=1$, the state is:\n$$\nx_1^\\varepsilon = \\varepsilon (1) + \\nu \\varepsilon W_1 = \\varepsilon (1 + \\nu W_1)\n$$\nThe performance functional for this perturbed control is:\n$$\nJ(u^\\varepsilon) = \\mathbb{E}\\left[\\frac{1}{2} (x_1^\\varepsilon)^2\\right] = \\mathbb{E}\\left[\\frac{1}{2} \\left(\\varepsilon (1 + \\nu W_1)\\right)^2\\right] = \\frac{1}{2}\\varepsilon^2 \\mathbb{E}\\left[(1 + \\nu W_1)^2\\right]\n$$\nWe expand the term inside the expectation:\n$$\n\\mathbb{E}\\left[(1 + \\nu W_1)^2\\right] = \\mathbb{E}\\left[1 + 2\\nu W_1 + \\nu^2 W_1^2\\right]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\mathbb{E}\\left[1\\right] + 2\\nu \\mathbb{E}\\left[W_1\\right] + \\nu^2 \\mathbb{E}\\left[W_1^2\\right]\n$$\nFor a standard Brownian motion $W_t$, we have the fundamental properties $\\mathbb{E}[W_t] = 0$ and $\\text{Var}(W_t) = \\mathbb{E}[W_t^2] - (\\mathbb{E}[W_t])^2 = t$. For $t=1$, this gives $\\mathbb{E}[W_1] = 0$ and $\\mathbb{E}[W_1^2] = 1$. Substituting these values:\n$$\n\\mathbb{E}\\left[(1 + \\nu W_1)^2\\right] = 1 + 2\\nu(0) + \\nu^2(1) = 1 + \\nu^2\n$$\nReturning to the expression for $J(u^\\varepsilon)$, we find:\n$$\nJ(u^\\varepsilon) = \\frac{1}{2}\\varepsilon^2 (1 + \\nu^2)\n$$\nThe cost corresponding to the singular control $u_t \\equiv 0$ is $J(0) = \\mathbb{E}[\\frac{1}{2} (0)^2] = 0$.\nThe variation in the cost functional is therefore:\n$$\nJ(u^\\varepsilon) - J(0) = \\frac{1}{2}\\varepsilon^2 (1 + \\nu^2) - 0 = \\frac{1}{2}\\varepsilon^2 (1 + \\nu^2)\n$$\nThe problem defines the second-order variation coefficient $Q(\\nu)$ through the expansion $J(u^\\varepsilon) - J(0) = \\varepsilon^2 Q(\\nu) + o(\\varepsilon^2)$. Comparing our exact expression with this definition, we identify:\n$$\nQ(\\nu) = \\frac{1}{2}(1 + \\nu^2)\n$$\nThere are no higher-order terms in $\\varepsilon$, so the $o(\\varepsilon^2)$ term is zero.\n\nFinally, we use the sign of $Q(\\nu)$ to draw a conclusion about the optimality of $u_t \\equiv 0$. The problem is a maximization problem. A necessary condition for $u_t \\equiv 0$ to be a local maximum is that for any admissible perturbation, the corresponding variation of the cost must be non-positive, i.e., $J(u^\\varepsilon) - J(0) \\le 0$ for all sufficiently small $\\varepsilon$. This translates to the second-order necessary condition $Q(\\nu) \\le 0$ for the given type of perturbation.\nHowever, we found $Q(\\nu) = \\frac{1}{2}(1 + \\nu^2)$. Since $\\nu \\in \\mathbb{R}$, $\\nu^2 \\ge 0$, which implies $1 + \\nu^2 \\ge 1$. Consequently, for any value of the parameter $\\nu$:\n$$\nQ(\\nu) = \\frac{1}{2}(1 + \\nu^2) \\ge \\frac{1}{2}  0\n$$\nSince $Q(\\nu)$ is strictly positive, the variation $J(u^\\varepsilon) - J(0) = \\varepsilon^2 Q(\\nu)$ is strictly positive for any $\\varepsilon \\neq 0$. This means that any small constant perturbation away from zero strictly increases the value of the cost functional. Therefore, the singular control $u_t \\equiv 0$ is not a maximizer. The sign of $Q(\\nu)$ being positive definitively eliminates $u_t \\equiv 0$ as a candidate for the optimal control; in fact, it reveals it to be a strict local minimum with respect to such perturbations.", "answer": "$$\\boxed{\\frac{1}{2}(1+\\nu^2)}$$", "id": "3003273"}]}