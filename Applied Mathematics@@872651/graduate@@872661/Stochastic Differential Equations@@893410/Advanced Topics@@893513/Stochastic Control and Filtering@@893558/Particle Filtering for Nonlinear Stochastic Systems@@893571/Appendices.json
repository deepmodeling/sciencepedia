{"hands_on_practices": [{"introduction": "Moving from the mathematical theory of particle filters to a working implementation reveals crucial practical challenges, chief among them being numerical stability. The weight update step involves multiplying likelihoods, which can be exceedingly small numbers, leading to catastrophic floating-point underflow where all particle weights become zero. This exercise [@problem_id:2990126] guides you through the implementation of a robust solution using the \"log-sum-exp\" trick, a standard and essential technique for performing weight normalization in the logarithmic domain.", "problem": "Consider a nonlinear hidden Markov model in discrete time obtained by Euler–Maruyama discretization of a stochastic differential equation. Let the hidden state be $x_k \\in \\mathbb{R}^n$ evolving according to\n$$\nx_k = x_{k-1} + f(x_{k-1})\\,\\Delta t + G(x_{k-1})\\,\\sqrt{\\Delta t}\\,\\eta_k,\n$$\nwhere $f(\\cdot)$ is a drift term, $G(\\cdot)$ is a diffusion term, $\\Delta t$ is the time step, and $\\eta_k$ is a standard Gaussian random vector. The observation is given by\n$$\ny_k = h(x_k) + v_k,\n$$\nwith $h(\\cdot)$ a nonlinear sensor model and $v_k$ a Gaussian noise vector.\n\nIn a particle filter for this system, suppose we have $N$ particles $\\{x_k^{(i)}\\}_{i=1}^N$ and their previous normalized weights $\\{w_{k-1}^{(i)}\\}_{i=1}^N$, which satisfy $\\sum_{i=1}^N w_{k-1}^{(i)} = 1$. The measurement update at time $k$ is based on Bayes’ rule and the principle of importance sampling: the unnormalized new weights are proportional to the product of the previous weights and the likelihood,\n$$\n\\tilde{w}_k^{(i)} \\propto w_{k-1}^{(i)}\\,p(y_k \\mid x_k^{(i)}),\n$$\nand the normalized weights are then\n$$\nw_k^{(i)} = \\frac{\\tilde{w}_k^{(i)}}{\\sum_{j=1}^N \\tilde{w}_k^{(j)}}.\n$$\n\nWhen the likelihood $p(y_k \\mid x_k^{(i)})$ is very small (for instance due to strong mismatch or small measurement noise covariance), direct computation of $\\tilde{w}_k^{(i)}$ in floating point can underflow to zero. A robust computational strategy is to operate in the logarithmic domain. Define the log-weights\n$$\n\\ell_{k-1}^{(i)} = \\log w_{k-1}^{(i)}, \\quad \\lambda_k^{(i)} = \\log p(y_k \\mid x_k^{(i)}),\n$$\nand the updated log-weights\n$$\n\\ell_k^{(i)} = \\ell_{k-1}^{(i)} + \\lambda_k^{(i)}.\n$$\nThe normalized $w_k^{(i)}$ can then be recovered stably using a numerically safe approach that avoids catastrophic underflow even when the $\\lambda_k^{(i)}$ are extremely negative.\n\nYour task is to derive, from the above foundations and without assuming any shortcut formulas, a numerically stable normalization procedure for $\\{w_k^{(i)}\\}_{i=1}^N$ based on log-weights that remains robust when some $\\lambda_k^{(i)}$ are very small or $-\\infty$ (representing zero likelihood). Implement this procedure in a complete program that, for each test case provided below, takes arrays of previous weights and log-likelihood increments, computes updated log-weights, and normalizes them in a numerically stable manner. If all updated log-weights are $-\\infty$ (i.e., all particles have zero likelihood under the current observation), you must return a fallback of uniform weights $w_k^{(i)} = 1/N$ to avoid degeneracy.\n\nDesign your program to process the following test suite. For each case, the input consists of the previous normalized weights $\\{w_{k-1}^{(i)}\\}$ and the log-likelihood increments $\\{\\lambda_k^{(i)}\\}$, and the output is the list of normalized weights $\\{w_k^{(i)}\\}$ rounded to twelve decimal places:\n\n- Test Case 1 (happy path with common extreme scaling): $N=5$. Previous weights $[0.2,\\,0.3,\\,0.1,\\,0.25,\\,0.15]$. Log-likelihood increments $[-1000.0,\\,-1000.0,\\,-1000.0,\\,-1000.0,\\,-1000.0]$.\n- Test Case 2 (mixed extremes and impossible particle): $N=5$. Previous weights $[0.2,\\,0.2,\\,0.2,\\,0.2,\\,0.2]$. Log-likelihood increments $[-1000.0,\\,-2000.0,\\,-\\infty,\\,-1200.0,\\,-1100.0]$.\n- Test Case 3 (single dominant support amid severe mismatch): $N=5$. Previous weights $[0.05,\\,0.05,\\,0.8,\\,0.05,\\,0.05]$. Log-likelihood increments $[-1000.0,\\,-1000.0,\\,0.0,\\,-1000.0,\\,-1000.0]$.\n- Test Case 4 (all impossible, fallback to uniform): $N=4$. Previous weights $[0.1,\\,0.2,\\,0.3,\\,0.4]$. Log-likelihood increments $[-\\infty,\\,-\\infty,\\,-\\infty,\\,-\\infty]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list must be the list of normalized weights for the corresponding test case, rounded to twelve decimal places. For example, your output line must look like\n$[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] ]$\nwith no additional text. Angles and physical units do not apply to this problem, and the only outputs are lists of floats rounded to twelve decimal places. Ensure that your implementation is numerically stable and robust under the provided edge cases.", "solution": "The problem requires the derivation and implementation of a numerically stable procedure to normalize particle weights in a particle filter, specifically when operating in the logarithmic domain to mitigate floating-point underflow.\n\nThe starting point is the standard importance sampling update rule for particle weights. Given a set of $N$ particles with previous normalized weights $\\{w_{k-1}^{(i)}\\}_{i=1}^N$ such that $\\sum_{i=1}^N w_{k-1}^{(i)} = 1$, the unnormalized updated weights $\\tilde{w}_k^{(i)}$ at time step $k$ are given by the product of the prior weight and the likelihood of the new observation $y_k$ given the particle's state $x_k^{(i)}$:\n$$\n\\tilde{w}_k^{(i)} = w_{k-1}^{(i)} \\, p(y_k \\mid x_k^{(i)})\n$$\nNormalization then yields the new weights $w_k^{(i)}$:\n$$\nw_k^{(i)} = \\frac{\\tilde{w}_k^{(i)}}{\\sum_{j=1}^N \\tilde{w}_k^{(j)}}\n$$\nDirect computation of this expression is prone to numerical underflow. If the likelihood values $p(y_k \\mid x_k^{(i)})$ are extremely small, the products $\\tilde{w}_k^{(i)}$ can evaluate to zero in finite-precision arithmetic for all $i$, leading to a division by zero.\n\nTo circumvent this, we operate in the logarithmic domain. Let $\\ell_{k-1}^{(i)} = \\log w_{k-1}^{(i)}$ be the previous log-weights and $\\lambda_k^{(i)} = \\log p(y_k \\mid x_k^{(i)})$ be the log-likelihoods. The unnormalized updated log-weight, $\\ell_k^{(i)}$, is:\n$$\n\\ell_k^{(i)} = \\log(\\tilde{w}_k^{(i)}) = \\log(w_{k-1}^{(i)}) + \\log(p(y_k \\mid x_k^{(i)})) = \\ell_{k-1}^{(i)} + \\lambda_k^{(i)}\n$$\nFrom this, the normalized weights $w_k^{(i)}$ can be expressed as:\n$$\nw_k^{(i)} = \\frac{\\exp(\\ell_k^{(i)})}{\\sum_{j=1}^N \\exp(\\ell_k^{(j)})}\n$$\nThis form still suffers from the same underflow issue, as $\\ell_k^{(i)}$ can be large negative numbers (e.g., $-1000$), causing $\\exp(\\ell_k^{(i)})$ to evaluate to zero.\n\nThe key to a stable computation is the \"log-sum-exp\" trick. Let $L_{max}$ be the maximum value among all the unnormalized updated log-weights:\n$$\nL_{max} = \\max_{j \\in \\{1, \\dots, N\\}} \\{\\ell_k^{(j)}\\}\n$$\nWe can multiply the numerator and the denominator of the expression for $w_k^{(i)}$ by the same non-zero factor, $\\exp(-L_{max})$, without changing its value:\n$$\nw_k^{(i)} = \\frac{\\exp(\\ell_k^{(i)}) \\cdot \\exp(-L_{max})}{\\left(\\sum_{j=1}^N \\exp(\\ell_k^{(j)})\\right) \\cdot \\exp(-L_{max})}\n$$\nUsing the property $\\exp(a)\\exp(b) = \\exp(a+b)$ and distributing the factor into the sum, we obtain:\n$$\nw_k^{(i)} = \\frac{\\exp(\\ell_k^{(i)} - L_{max})}{\\sum_{j=1}^N \\exp(\\ell_k^{(j)} - L_{max})}\n$$\nThis expression is numerically stable. Let's analyze its components:\n1.  The exponent $(\\ell_k^{(j)} - L_{max})$ for any term in the sum is always less than or equal to $0$, as $L_{max}$ is the maximum value in the set $\\{\\ell_k^{(j)}\\}$. This prevents the argument of $\\exp(\\cdot)$ from being a large positive number, thus avoiding overflow.\n2.  There exists at least one particle, say with index $m$, for which $\\ell_k^{(m)} = L_{max}$. For this particle, the term in the sum is $\\exp(\\ell_k^{(m)} - L_{max}) = \\exp(0) = 1$.\n3.  Therefore, the denominator, $S = \\sum_{j=1}^N \\exp(\\ell_k^{(j)} - L_{max})$, is guaranteed to be greater than or equal to $1$. This prevents the sum from underflowing to zero, which was the primary numerical instability.\n4.  The terms $\\exp(\\ell_k^{(i)} - L_{max})$ may still underflow to zero if $\\ell_k^{(i)}$ is significantly smaller than $L_{max}$. This, however, is a correct and desirable outcome, as it reflects that particle $i$ has a negligible weight compared to the most likely particles.\n\nA special case arises when all particles have zero likelihood, which corresponds to all log-likelihoods $\\lambda_k^{(i)}$ being $-\\infty$ or a combination leading to all $\\ell_k^{(i)}$ being $-\\infty$. In this scenario, $L_{max} = -\\infty$. A direct application of the formula is undefined. The problem specifies a robust fallback mechanism: if all updated log-weights are $-\\infty$, the filter is reset by assigning uniform weights, $w_k^{(i)} = 1/N$ for all $i$. This prevents filter degeneracy.\n\nThe complete, numerically stable algorithm is as follows:\n1.  For each particle $i \\in \\{1, \\dots, N\\}$, calculate the unnormalized log-weight: $\\ell_k^{(i)} = \\log(w_{k-1}^{(i)}) + \\lambda_k^{(i)}$. Note that if $w_{k-1}^{(i)}=0$, then $\\ell_{k-1}^{(i)}=-\\infty$, and consequently $\\ell_k^{(i)}=-\\infty$.\n2.  Find the maximum log-weight: $L_{max} = \\max_{i} \\{\\ell_k^{(i)}\\}$.\n3.  Check for the degenerate case: If $L_{max} = -\\infty$, set $w_k^{(i)} = 1/N$ for all $i=1, \\dots, N$ and terminate.\n4.  If $L_{max}$ is finite, compute the shifted log-weights: $\\ell'^{(i)}_k = \\ell_k^{(i)} - L_{max}$.\n5.  Compute the stable unnormalized weights: $w'^{(i)}_k = \\exp(\\ell'^{(i)}_k)$.\n6.  Calculate their sum: $S = \\sum_{j=1}^N w'^{(j)}_k$.\n7.  Compute the final normalized weights: $w_k^{(i)} = \\frac{w'^{(i)}_k}{S}$.\nThis procedure ensures robust and accurate weight normalization even under extreme values of log-likelihoods.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the particle filter weight normalization problem for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: happy path with common extreme scaling\n        (np.array([0.2, 0.3, 0.1, 0.25, 0.15]), np.array([-1000.0, -1000.0, -1000.0, -1000.0, -1000.0])),\n        # Test Case 2: mixed extremes and impossible particle\n        (np.array([0.2, 0.2, 0.2, 0.2, 0.2]), np.array([-1000.0, -2000.0, -np.inf, -1200.0, -1100.0])),\n        # Test Case 3: single dominant support amid severe mismatch\n        (np.array([0.05, 0.05, 0.8, 0.05, 0.05]), np.array([-1000.0, -1000.0, 0.0, -1000.0, -1000.0])),\n        # Test Case 4: all impossible, fallback to uniform\n        (np.array([0.1, 0.2, 0.3, 0.4]), np.array([-np.inf, -np.inf, -np.inf, -np.inf]))\n    ]\n    \n    results = []\n\n    for prev_weights, log_likelihoods in test_cases:\n        # Step 1: Compute previous log-weights and updated unnormalized log-weights.\n        # np.log handles arrays and np.log(0) correctly returns -inf.\n        # Since all given prev_weights are  0, we don't encounter np.log(0).\n        log_prev_weights = np.log(prev_weights)\n        \n        # This is l_k^{(i)} in the problem description.\n        updated_log_weights = log_prev_weights + log_likelihoods\n        \n        # Step 2: Find the maximum log-weight.\n        max_log_weight = np.max(updated_log_weights)\n        \n        # Step 3: Handle the degenerate case where all particles have zero likelihood.\n        if max_log_weight == -np.inf:\n            num_particles = len(prev_weights)\n            normalized_weights = np.full(num_particles, 1.0 / num_particles)\n        else:\n            # Step 4: Compute shifted log-weights.\n            # This is l_k^{(i)} - L_max.\n            shifted_log_weights = updated_log_weights - max_log_weight\n            \n            # Step 5  6: Exponentiate and compute the sum.\n            # This is exp(l_k^{(i)} - L_max).\n            exp_weights = np.exp(shifted_log_weights)\n            sum_exp_weights = np.sum(exp_weights)\n            \n            # Step 7: Compute the final normalized weights.\n            normalized_weights = exp_weights / sum_exp_weights\n            \n        # Round the final weights to 12 decimal places as required.\n        rounded_weights = np.round(normalized_weights, 12).tolist()\n        results.append(rounded_weights)\n\n    # Format the final output string as specified.\n    # The problem specifies that the output for each test case is a list.\n    # The overall output is a list of these lists.\n    # A simple map to str over `results` would create strings of lists.\n    # We must format it carefully.\n    result_strings = [str(r) for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2990126"}, {"introduction": "With a numerically stable filter in hand, a crucial next step is to understand its performance characteristics. As the number of particles $N$ or the state dimension $d$ grows, how does the computational cost scale? This practice [@problem_id:2990065] walks you through a first-principles analysis of the computational complexity of the three main phases of a particle filter—propagation, weighting, and resampling—allowing you to identify the primary computational bottlenecks that drive the overall runtime.", "problem": "Consider a nonlinear continuous-time stochastic dynamical system in state dimension $d$ governed by a Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; f(X_{t})\\,\\mathrm{d}t \\;+\\; G(X_{t})\\,\\mathrm{d}W_{t},\n$$\nwhere $X_{t}\\in\\mathbb{R}^{d}$ is the state, $f:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is a drift field, $G:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times d}$ is a diffusion field that is dense for typical states, and $W_{t}$ is a standard $d$-dimensional Wiener process. Discrete-time observations at times $\\{t_{k}\\}$ are given by\n$$\nY_{k} \\;=\\; h(X_{t_{k}}) \\;+\\; V_{k},\n$$\nwhere $h:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is a nonlinear measurement function and $V_{k}\\sim \\mathcal{N}(0,S)$ is Gaussian measurement noise with a time-invariant, dense covariance $S\\in\\mathbb{R}^{d\\times d}$. Assume $S$ is nonsingular and its inverse or a Cholesky factorization is precomputed once and reused.\n\nA Sequential Monte Carlo (SMC) particle filter with $N$ particles is used to approximate the filtering distribution. At each time step $t_{k}\\to t_{k+1}$, the filter executes three phases:\n\n(1) Propagation: Each particle $X^{(i)}_{t_{k}}$ is advanced to $X^{(i)}_{t_{k+1}}$ using the Euler–Maruyama method with step size $\\Delta t$, namely\n$$\nX^{(i)}_{t_{k+1}} \\;=\\; X^{(i)}_{t_{k}} \\;+\\; f\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta t \\;+\\; G\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta W^{(i)}_{k},\n$$\nwhere $\\Delta W^{(i)}_{k}\\sim \\mathcal{N}(0,\\Delta t\\,I_{d})$ are independent for each particle. Evaluating $f$ is assumed to require $\\Theta(d)$ arithmetic operations per particle, $G(X)$ is dense so forming $G(X)\\in\\mathbb{R}^{d\\times d}$ and multiplying it by $\\Delta W\\in\\mathbb{R}^{d}$ requires $\\Theta(d^{2})$ arithmetic operations per particle.\n\n(2) Weight computation: For each particle state $x^{(i)}$, compute the residual $r^{(i)}=y_{k+1}-h(x^{(i)})$, assumed to require $\\Theta(d)$ operations for $h$, and evaluate the Gaussian likelihood (up to normalization) via the quadratic form $Q^{(i)}=(r^{(i)})^{\\top}S^{-1}r^{(i)}$ or, equivalently, using a precomputed Cholesky factorization $S=LL^{\\top}$ by solving $Lz=r^{(i)}$ followed by $Q^{(i)}=\\|z\\|^{2}$. For dense $L\\in\\mathbb{R}^{d\\times d}$, the triangular solve costs $\\Theta(d^{2})$ operations per particle, and computing $\\|z\\|^{2}$ costs $\\Theta(d)$.\n\n(3) Resampling: Perform systematic resampling to draw $N$ new particles from the weighted set. Computing normalized weights, their cumulative sum, and generating the resampled indices is assumed to cost $\\Theta(N)$ per time step.\n\nStarting from the above first-principles descriptions of the SDE discretization, Gaussian likelihood evaluation, and systematic resampling, derive the leading-order asymptotic computational complexity per time step of the entire particle filter as a single expression in $N$ and $d$, ignoring all constant factors and lower-order terms. Your derivation must make clear which phase(s) constitute the computational bottleneck(s) under the stated assumptions. Provide your final answer as one closed-form asymptotic expression in big-$O$ notation. No numerical approximation or rounding is required.", "solution": "The problem requires the derivation of the leading-order asymptotic computational complexity per time step for a Sequential Monte Carlo (SMC) particle filter with $N$ particles applied to a system of state dimension $d$. The total complexity is the sum of the complexities of its three constituent phases: Propagation, Weight Computation, and Resampling. We will analyze each phase in turn.\n\nLet $N$ be the number of particles and $d$ be the dimension of the state space $\\mathbb{R}^{d}$.\n\n1.  **Propagation Phase:**\n    This phase advances each of the $N$ particles from time $t_{k}$ to $t_{k+1}$. The cost of this phase is the cost per particle multiplied by the number of particles, $N$. For a single particle $X^{(i)}_{t_{k}}$, the update rule is:\n    $$\n    X^{(i)}_{t_{k+1}} \\;=\\; X^{(i)}_{t_{k}} \\;+\\; f\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta t \\;+\\; G\\!\\big(X^{(i)}_{t_{k}}\\big)\\,\\Delta W^{(i)}_{k}\n    $$\n    The computational cost for one particle is the sum of the costs of evaluating the terms on the right-hand side and performing the vector additions.\n    -   Evaluation of the drift term $f(X^{(i)}_{t_{k}})$ is stated to require $\\Theta(d)$ operations. The subsequent scalar-vector multiplication by $\\Delta t$ also takes $\\Theta(d)$ operations.\n    -   Evaluation of the diffusion term involves forming the dense $d \\times d$ matrix $G(X^{(i)}_{t_{k}})$ and multiplying it by the $d \\times 1$ vector $\\Delta W^{(i)}_{k}$. A dense matrix-vector product requires $\\Theta(d^{2})$ arithmetic operations. The generation of the random vector $\\Delta W^{(i)}_{k}$ from $\\mathcal{N}(0, \\Delta t\\,I_{d})$ involves drawing $d$ independent Gaussian samples, which costs $\\Theta(d)$.\n    -   The two vector additions cost $\\Theta(d)$ each.\n    The total cost per particle for the propagation step is the sum of these individual costs: $\\Theta(d) + \\Theta(d) + (\\Theta(d^{2}) + \\Theta(d)) + \\Theta(d) + \\Theta(d)$. The dominant term in this sum is $\\Theta(d^{2})$, arising from the dense matrix-vector multiplication $G \\Delta W$.\n    Therefore, the complexity for propagating one particle is $\\Theta(d^{2})$.\n    For all $N$ particles, the total complexity of the propagation phase is:\n    $$\n    C_{\\text{prop}} = N \\times \\Theta(d^{2}) = \\Theta(Nd^{2})\n    $$\n\n2.  **Weight Computation Phase:**\n    This phase computes an importance weight for each of the $N$ propagated particles. The cost is again the cost per particle multiplied by $N$. For a single particle $x^{(i)} = X^{(i)}_{t_{k+1}}$ and the observation $y_{k+1}$, the unnormalized weight is proportional to the likelihood of the observation given the particle's state. This involves computing the quadratic form $Q^{(i)}=(r^{(i)})^{\\top}S^{-1}r^{(i)}$, where $r^{(i)}=y_{k+1}-h(x^{(i)})$.\n    -   The evaluation of the measurement function $h(x^{(i)})$ is stated to cost $\\Theta(d)$.\n    -   The computation of the residual vector $r^{(i)}$ via vector subtraction costs an additional $\\Theta(d)$.\n    -   The calculation of the quadratic form $Q^{(i)}$ is performed using a precomputed Cholesky factorization $S = LL^{\\top}$. This involves two steps: first, solving the lower triangular system $Lz = r^{(i)}$ for $z$ using forward substitution, and second, computing the squared norm $Q^{(i)} = z^{\\top}z = \\|z\\|^{2}$. For a dense $d \\times d$ matrix $L$, forward substitution costs $\\Theta(d^{2})$ operations. The subsequent computation of the squared norm of the $d$-dimensional vector $z$ costs $\\Theta(d)$. The evaluation of the likelihood, e.g., $\\exp(-Q^{(i)}/2)$, is a scalar operation of cost $\\Theta(1)$.\n    The total cost per particle for weight computation is the sum of these costs: $(\\Theta(d) + \\Theta(d)) + (\\Theta(d^{2}) + \\Theta(d)) + \\Theta(1)$. The dominant term here is $\\Theta(d^{2})$, which comes from solving the triangular linear system.\n    Therefore, the complexity for weighting one particle is $\\Theta(d^{2})$.\n    For all $N$ particles, the total complexity of the weight computation phase is:\n    $$\n    C_{\\text{weight}} = N \\times \\Theta(d^{2}) = \\Theta(Nd^{2})\n    $$\n\n3.  **Resampling Phase:**\n    This phase draws a new set of $N$ particles from the current weighted set to combat degeneracy. The problem states that systematic resampling, which involves computing normalized weights, their cumulative sum, and drawing the new indices, has a total computational cost of $\\Theta(N)$. This cost is notably independent of the state dimension $d$.\n    $$\n    C_{\\text{resample}} = \\Theta(N)\n    $$\n\n**Total Complexity and Bottlenecks**\nThe total computational complexity per time step, $C_{\\text{total}}$, is the sum of the complexities of the three phases:\n$$\nC_{\\text{total}} = C_{\\text{prop}} + C_{\\text{weight}} + C_{\\text{resample}} = \\Theta(Nd^{2}) + \\Theta(Nd^{2}) + \\Theta(N)\n$$\nTo find the leading-order asymptotic complexity, we identify the term that grows fastest as $N$ and $d$ become large. Since $d \\ge 1$, the term $Nd^{2}$ dominates $N$.\n$$\nC_{\\text{total}} = \\Theta(Nd^{2})\n$$\nThe computational bottlenecks are the phases whose complexities match this leading-order term. In this analysis, both the **Propagation** phase and the **Weight Computation** phase have a complexity of $\\Theta(Nd^{2})$. Their cost scales quadratically with the state dimension $d$, stemming from linear algebra operations involving the dense matrices $G$ and $S$ (or its Cholesky factor $L$). The Resampling phase, with complexity $\\Theta(N)$, is not a bottleneck for $d>1$. Therefore, both propagation and weighting are the dominant computational burdens for this particle filter configuration.\n\nThe final answer, expressed in big-O notation as requested, is the leading-order term derived from this analysis.", "answer": "$$\n\\boxed{O(Nd^{2})}\n$$", "id": "2990065"}, {"introduction": "The complexity analysis often reveals that particle filters can be computationally intensive, limiting their real-time applicability. This advanced practice [@problem_id:2990093] addresses this challenge by exploring parallelization, a key technique for high-performance filtering. You will learn to design data-parallel propagation and weighting steps suitable for modern hardware like GPUs and, critically, how to correctly implement and validate the synchronized resampling step, ensuring correctness in a parallel execution environment.", "problem": "You are tasked with designing and validating a parallelizable particle filter for a nonlinear stochastic system, using a principled derivation from the Bayesian filtering recursion and the discretization of a stochastic differential equation. Your program must implement a particle filter whose particle propagation and weight computations are data-parallel (amenable to Single Instruction Multiple Data execution as on a Graphics Processing Unit (GPU) or vectorized code on a Central Processing Unit (CPU)), while the resampling step uses an algorithm that is correct under global synchronization constraints. Your implementation must produce deterministic equivalence between a vectorized (parallel-friendly) resampler and a sequential resampler when provided with identical randomness, thereby demonstrating correct synchronization.\n\nFundamental basis to be used for derivation:\n- The Bayesian filtering recursion for a hidden Markov model with state $x_k$ and observation $y_k$:\n$$p(x_k \\mid y_{1:k}) \\propto p(y_k \\mid x_k) \\int p(x_k \\mid x_{k-1}) p(x_{k-1} \\mid y_{1:k-1}) \\, dx_{k-1}.$$\n- The stochastic differential equation in Itô form:\n$$dX_t = f(X_t) \\, dt + \\sigma \\, dW_t,$$\nand the Euler–Maruyama discretization for a time step $\\Delta t$:\n$$X_{k+1} \\approx X_k + f(X_k) \\, \\Delta t + \\sigma \\sqrt{\\Delta t} \\, \\xi_k,$$\nwith $\\xi_k \\sim \\mathcal{N}(0,1)$ independent across $k$ and particles.\n- The Sequential Monte Carlo (SMC) approximation with $N$ particles $\\{x_k^{(i)}, w_k^{(i)}\\}_{i=1}^N$, where, after propagation, the importance weights are proportional to the likelihood:\n$$w_k^{(i)} \\propto p(y_k \\mid x_k^{(i)}), \\quad \\sum_{i=1}^N w_k^{(i)} = 1.$$\n\nYour state-space model is defined as follows.\n- Scalar state dynamics with drift $f(x) = \\alpha x - \\beta x^3$ and diffusion parameter $\\sigma$:\n$$X_{k+1} = X_k + (\\alpha X_k - \\beta X_k^3) \\, \\Delta t + \\sigma \\sqrt{\\Delta t} \\, \\xi_k.$$\n- Scalar observation model with observation function $h(x) = x^2 / c$ and Gaussian noise variance $R$:\n$$Y_k = h(X_k) + \\varepsilon_k, \\quad \\varepsilon_k \\sim \\mathcal{N}(0, R).$$\n\nRequirements for algorithm design:\n- Particle propagation and weight computation must be fully vectorized over particles, consistent with data-parallel execution. The likelihood $p(y_k \\mid x_k^{(i)})$ must be evaluated using the Gaussian density with variance $R$. For numerical stability, logarithms may be used for the likelihood and weights, but the final normalized weights must satisfy $\\sum_i w_k^{(i)} = 1$.\n- Resampling must be implemented in two ways:\n  - A vectorized, parallel-friendly method based on a single inclusive prefix sum (cumulative sum) of weights and a batched lower-bound search (e.g., systematic or stratified resampling implemented with a cumulative distribution function lookup).\n  - A sequential baseline that uses a simple loop to walk over the cumulative weights to assign ancestors.\n- Synchronization constraint: Given a set of normalized weights $\\{w^{(i)}\\}_{i=1}^N$ and a fixed set of uniforms used by the resampling scheme (for systematic resampling, a single $u_0 \\in [0, 1/N)$; for stratified resampling, uniforms $\\{u_i\\}_{i=0}^{N-1}$ with $u_i \\in [0,1)$), both the vectorized and sequential resamplers must produce identical ancestor index arrays. This requires treating the prefix sum as a global barrier and using the same uniforms across both implementations. You must ensure that any randomness used for particle propagation is pre-generated per time step and per child index so that it is independent of the ancestor mapping and thus consistent across different resamplers.\n\nObservation generation for validation:\n- For tests involving filtering, you must generate a single ground-truth trajectory $\\{X_k^\\star\\}_{k=0}^{T}$ and observations $\\{Y_k\\}_{k=1}^{T}$ using the same state and observation models, with fixed seeds specified below. The initial state must be $X_0^\\star = x_0$.\n\nFiltering protocol:\n- Use a bootstrap particle filter with $N$ particles initialized as $X_0^{(i)} \\sim \\mathcal{N}(0,1)$ i.i.d. The per-time-step procedure is:\n  - Propagate all particles in parallel with the Euler–Maruyama step using pre-generated $\\{\\xi_k^{(j)}\\}_{j=1}^N$ that depend only on the child index $j$ and time $k$, not on the ancestor.\n  - Compute weights $w_k^{(i)} \\propto p(Y_k \\mid X_k^{(i)})$, normalize to $\\sum_i w_k^{(i)} = 1$, and compute the effective sample size $N_{\\mathrm{eff}} = 1 / \\sum_i (w_k^{(i)})^2$.\n  - Compute the posterior mean estimate $\\hat{x}_k = \\sum_i w_k^{(i)} X_k^{(i)}$.\n  - If $N_{\\mathrm{eff}}  \\tau N$, perform resampling using both the vectorized and the sequential resampler with the same uniforms to obtain ancestor indices, set $X_k^{(i)} \\leftarrow X_k^{(a_i)}$ and reset weights to $1/N$ for the next time step.\n- Run two filters in lockstep (one with the vectorized resampler and one with the sequential resampler), sharing initial particles, propagation noises, and resampling uniforms across time steps, and report the maximum absolute difference of their posterior mean time series, $\\max_k |\\hat{x}_k^{\\mathrm{vec}} - \\hat{x}_k^{\\mathrm{seq}}|$.\n\nTest suite:\nImplement the following five test cases, each producing a single numeric result. Use the exact numerical parameter values and seeds listed.\n\n- Test case $1$ (systematic resampling index equivalence, odd $N$):\n  - $N = 1001$.\n  - Weights: draw i.i.d. positive values from a Gamma distribution with shape $k = 2$ and scale $\\theta = 1$ with seed $s_w = 42$, then normalize to sum to $1$.\n  - Systematic resampling: use offset $u_0 = \\frac{0.3141592653589793}{N}$.\n  - Output the integer count of mismatched indices between vectorized and sequential resamplers.\n\n- Test case $2$ (stratified resampling index equivalence, power-of-two $N$):\n  - $N = 2048$.\n  - Weights: draw i.i.d. positive values from a Gamma distribution with shape $k = 3$ and scale $\\theta = 1$ with seed $s_w = 123$, then normalize to sum to $1$.\n  - Stratified resampling: draw $\\{u_i\\}_{i=0}^{N-1}$ i.i.d. from $\\mathcal{U}[0,1)$ with seed $s_u = 2024$, and set $u_i' = \\frac{i + u_i}{N}$.\n  - Output the integer count of mismatched indices between vectorized and sequential resamplers.\n\n- Test case $3$ (full particle filter equivalence with resampling):\n  - Dynamics: $\\alpha = 0.8$, $\\beta = 0.2$, $\\sigma = 0.6$, $\\Delta t = 0.05$, $c = 5$, $R = 0.25$, $T = 15$, $x_0 = 0.1$.\n  - Data generation seed: $s_{\\mathrm{data}} = 777$.\n  - Filter initial particles seed: $s_0 = 2021$.\n  - Propagation noise matrix $\\Xi \\in \\mathbb{R}^{T \\times N}$ with entries $\\xi_k^{(j)} \\sim \\mathcal{N}(0,1)$ generated with seed $s_\\Xi = 2022$.\n  - Systematic resampling offsets $\\{u_{0,k}\\}_{k=1}^T$ with $u_{0,k} \\in [0, 1/N)$ from seed $s_{u0} = 2023$ using $u_{0,k} = \\frac{r_k}{N}$ with $r_k \\sim \\mathcal{U}[0,1)$.\n  - $N = 8192$, resampling threshold fraction $\\tau = 0.5$.\n  - Output the floating-point maximum absolute difference $\\max_k |\\hat{x}_k^{\\mathrm{vec}} - \\hat{x}_k^{\\mathrm{seq}}|$.\n\n- Test case $4$ (degenerate weight vector):\n  - $N = 512$, weights are zero except a single position $j_\\star = 133$ has weight $1$.\n  - Systematic resampling offset $u_0 = \\frac{0.2718281828459045}{N}$.\n  - Output the integer count of mismatched indices between vectorized and sequential resamplers.\n\n- Test case $5$ (full particle filter equivalence without resampling):\n  - Same dynamics and observation parameters as in Test case $3$.\n  - Data generation seed: $s_{\\mathrm{data}} = 31415$.\n  - Filter initial particles seed: $s_0 = 27182$.\n  - Propagation noise matrix seed: $s_\\Xi = 14142$.\n  - Systematic resampling offsets seed: $s_{u0} = 17320$ (will not be used because resampling is disabled).\n  - $N = 4096$, resampling threshold fraction $\\tau = 0$ (no resampling at any time).\n  - Output the floating-point maximum absolute difference $\\max_k |\\hat{x}_k^{\\mathrm{vec}} - \\hat{x}_k^{\\mathrm{seq}}|$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Test cases $1$ through $5$, for example, \"[$r_1,r_2,r_3,r_4,r_5$]\". Each $r_i$ must be either an integer or a floating-point number as specified above. No other text should be printed.", "solution": "The problem statement is a well-posed and scientifically grounded exercise in computational statistics and parallel programming, specifically addressing the design and verification of a parallelizable particle filter for a nonlinear stochastic system. It is self-contained, providing all necessary mathematical models, parameters, and sources of randomness to ensure a deterministic and reproducible outcome. The core of the problem lies in demonstrating the equivalence of a sequential resampling algorithm with its vectorized, data-parallel counterpart, which is a critical aspect of implementing particle filters on modern hardware like GPUs. The problem is valid.\n\nThe solution is developed by first deriving the components of the particle filter from the provided theoretical basis and then assembling them into a complete algorithm that meets the specified design requirements.\n\n**1. Theoretical Foundation**\n\nThe particle filter is a numerical method for approximating the Bayesian filtering recursion:\n$$p(x_k \\mid y_{1:k}) \\propto p(y_k \\mid x_k) \\int p(x_k \\mid x_{k-1}) p(x_{k-1} \\mid y_{1:k-1}) \\, dx_{k-1}$$\nHere, the posterior distribution at time $k-1$, $p(x_{k-1} \\mid y_{1:k-1})$, is represented by a set of $N$ weighted particles $\\{x_{k-1}^{(i)}, w_{k-1}^{(i)}\\}_{i=1}^N$. The integral represents the prediction step, and multiplication by the likelihood $p(y_k \\mid x_k)$ represents the update step.\n\n**2. Algorithmic Design**\n\nThe particle filter algorithm proceeds in a sequence of steps for each time index $k = 1, 2, \\dots, T$. A bootstrap filter is used, which simplifies the process by using the dynamics model $p(x_k \\mid x_{k-1})$ as the proposal distribution.\n\n**Step 2.1: Particle Propagation (Prediction)**\n\nThe state dynamics are given by the stochastic differential equation (SDE):\n$$dX_t = f(X_t) \\, dt + \\sigma \\, dW_t$$\nwhere the drift is $f(x) = \\alpha x - \\beta x^3$. The Euler-Maruyama discretization with time step $\\Delta t$ is:\n$$X_{k} = X_{k-1} + (\\alpha X_{k-1} - \\beta X_{k-1}^3) \\, \\Delta t + \\sigma \\sqrt{\\Delta t} \\, \\xi_{k-1}$$\nwhere $\\xi_{k-1} \\sim \\mathcal{N}(0,1)$.\n\nThis propagation step is applied to each particle $\\{x_{k-1}^{(i)}\\}_{i=1}^N$ to obtain the predicted particle set $\\{x_k^{(i)}\\}_{i=1}^N$. This operation is data-parallel and can be fully vectorized. For each particle $i$ at time step $k$:\n$$x_k^{(i)} = x_{k-1}^{(i)} + (\\alpha x_{k-1}^{(i)} - \\beta (x_{k-1}^{(i)})^3) \\, \\Delta t + \\sigma \\sqrt{\\Delta t} \\, \\xi_{k-1}^{(i)}$$\nThe problem specifies that the noise terms $\\{\\xi_{k-1}^{(i)}\\}_{i=1}^N$ are pre-generated and depend only on the time index $k-1$ and the particle index $i$, ensuring consistency across different algorithm implementations.\n\n**Step 2.2: Weight Computation (Update)**\n\nThe observation model is $Y_k = h(X_k) + \\varepsilon_k$, with $h(x) = x^2/c$ and $\\varepsilon_k \\sim \\mathcal{N}(0, R)$. The likelihood of observing $Y_k$ given a particle's state $x_k^{(i)}$ is given by the Gaussian probability density function (PDF):\n$$p(Y_k \\mid x_k^{(i)}) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{(Y_k - h(x_k^{(i)}))^2}{2R}\\right) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{(Y_k - (x_k^{(i)})^2/c)^2}{2R}\\right)$$\nThe importance weights are proportional to these likelihoods: $w_k^{(i)} \\propto p(Y_k \\mid x_k^{(i)})$. To avoid numerical underflow, computations are performed in the log domain. The unnormalized log-weight for particle $i$ is:\n$$\\log \\tilde{w}_k^{(i)} = -\\frac{1}{2}\\log(2\\pi R) - \\frac{(Y_k - (x_k^{(i)})^2/c)^2}{2R}$$\nThese log-weights are then normalized using the log-sum-exp trick to ensure $\\sum_i w_k^{(i)} = 1$:\n$$\\log w_k^{(i)} = \\log \\tilde{w}_k^{(i)} - \\mathrm{logsumexp}(\\{\\log \\tilde{w}_k^{(j)}\\}_{j=1}^N)$$\n$$w_k^{(i)} = \\exp(\\log w_k^{(i)})$$\nThis entire computation is vectorized, applying the same calculation to all particles simultaneously. After weighting, the posterior mean is estimated as $\\hat{x}_k = \\sum_{i=1}^N w_k^{(i)} x_k^{(i)}$.\n\n**Step 2.3: Resampling**\n\nTo combat weight degeneracy, a resampling step is performed when the effective sample size, $N_{\\mathrm{eff}} = 1 / \\sum_{i=1}^N (w_k^{(i)})^2$, drops below a threshold $\\tau N$. Resampling replaces the current particle set with a new set by drawing $N$ times with replacement from the current set, where the probability of drawing particle $i$ is its weight $w_k^{(i)}$. The weights of the new particles are reset to $1/N$.\n\nThe core of this problem is to implement and validate two versions of the resampling index generation. Both rely on the cumulative distribution function (CDF) of the weights, $C_j = \\sum_{i=1}^j w_k^{(i)}$. Given a set of $N$ uniform random samples $\\{u_j\\}_{j=1}^N$ drawn according to a specific scheme (systematic or stratified), the goal is to find for each $u_j$ the ancestor index $a_j$ such that $C_{a_j-1}  u_j \\le C_{a_j}$ (with $C_0=0$).\n\n- **Sequential Resampler**: This is a direct implementation of the inverse CDF method. For each of the $N$ required children, we loop through the CDF array to find the first index $i$ that satisfies $u_j \\le C_i$. This involves nested loops and is not parallel-friendly.\n    ```\n    For j from 1 to N:\n      i = 1\n      While u_j  C_i:\n        i = i + 1\n      ancestor_indices[j] = i\n    ```\n\n- **Vectorized Resampler**: This implementation leverages hardware parallelism by replacing the inner loop with a vectorized search. The `numpy.searchsorted` function is ideal for this. It performs a batched binary search to find the insertion points for all values in $\\{u_j\\}$ into the sorted array $C$. With `side='left'`, it finds the first index $i$ such that $C_i \\ge u_j$, which is equivalent to the sequential loop's logic.\n    ```\n    ancestor_indices = numpy.searchsorted(C, {u_j}, side='left')\n    ```\n    This single line of code replaces the entire sequential loop structure, allowing for efficient parallel execution.\n\nThe problem's synchronization constraint requires that for a given set of weights $\\{w_k^{(i)}\\}$ and uniforms $\\{u_j\\}$, both resamplers must produce the exact same array of ancestor indices. This verifies the correctness of the vectorized implementation.\n\n**Step 2.4: Test Suite Execution**\n\nThe test cases are designed to validate the equivalence of the resamplers and the full filter.\n- **Tests 1, 2, 4**: These directly compare the output of `resample_sequential` and `resample_vectorized` for different weight distributions (Gamma-distributed, degenerate) and resampling schemes (systematic, stratified). The expected outcome is zero mismatches, confirming the logical equivalence of the two implementations.\n- **Tests 3, 5**: These embed the resamplers into the full particle filter loop. Two filters are run in lockstep, sharing all sources of randomness (initial states, propagation noise, resampling uniforms). One filter uses the sequential resampler, the other uses the vectorized one. Since the resamplers are proven equivalent, the particle sets in both filters should remain identical at all times. Therefore, the posterior mean estimates $\\hat{x}_k^{\\mathrm{seq}}$ and $\\hat{x}_k^{\\mathrm{vec}}$ must also be identical, up to floating-point precision. The tests measure the maximum absolute difference between these time series, which is expected to be near zero. Test $5$ acts as a control by disabling resampling, in which case the two filters are trivially identical. Test $3$ confirms this identity holds even when the resampling step is active.\n\nBy following this principled design, we construct a program that not only implements a particle filter but rigorously validates its parallelizable components against a sequential baseline, ensuring correctness.", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef resample_sequential(cdf, uniforms):\n    \"\"\"\n    Sequential resampling based on a loop over a cumulative distribution function.\n    \"\"\"\n    N = len(cdf)\n    indices = np.zeros(N, dtype=int)\n    for j in range(N):\n        u = uniforms[j]\n        i = 0\n        while cdf[i]  u:\n            i += 1\n        indices[j] = i\n    return indices\n\ndef resample_vectorized(cdf, uniforms):\n    \"\"\"\n    Vectorized resampling using numpy.searchsorted.\n    \"\"\"\n    # 'left' side finds the first index i such that cdf[i] = u,\n    # which is equivalent to the logic of the sequential loop.\n    return np.searchsorted(cdf, uniforms, side='left')\n\ndef run_particle_filter(\n    N, T, x0, alpha, beta, sigma, c, R, delta_t,\n    s_data, s_0, s_Xi, s_u0, tau, resampler_type):\n    \"\"\"\n    Runs a particle filter for the given system and parameters.\n    \"\"\"\n    # 1. Generate ground truth and observations\n    rng_data = np.random.default_rng(s_data)\n    X_true = np.zeros(T + 1)\n    Y_obs = np.zeros(T + 1)\n    X_true[0] = x0\n    for k in range(T):\n        noise_state = rng_data.normal(0, 1)\n        X_true[k+1] = X_true[k] + (alpha * X_true[k] - beta * X_true[k]**3) * delta_t \\\n                      + sigma * np.sqrt(delta_t) * noise_state\n    \n    noise_obs = rng_data.normal(0, np.sqrt(R), T + 1)\n    Y_obs = (X_true**2) / c + noise_obs\n\n    # 2. Initialize filter\n    rng_init = np.random.default_rng(s_0)\n    particles = rng_init.normal(0, 1, N)\n    \n    # Pre-generate all noise for reproducibility\n    rng_prop = np.random.default_rng(s_Xi)\n    propagation_noise = rng_prop.normal(0, 1, size=(T, N))\n    \n    rng_resample_uniforms = np.random.default_rng(s_u0)\n    # For systematic resampling, draw one U(0,1) per time step\n    base_resample_uniforms = rng_resample_uniforms.uniform(0, 1, T)\n\n    posterior_means = np.zeros(T)\n    \n    #\n    # 3. Main filter loop\n    #\n    weights = np.ones(N) / N  # Start with uniform weights at k=0\n\n    for k in range(T):\n        # I. Propagate\n        # Resampling happened at the end of k-1, particles are unweighted before propagation for k.\n        noise_k = propagation_noise[k, :]\n        particles = particles + (alpha * particles - beta * particles**3) * delta_t \\\n                    + sigma * np.sqrt(delta_t) * noise_k\n\n        # II. Weight\n        # Log-likelihood for numerical stability\n        h_x = particles**2 / c\n        log_likelihood = -0.5 * np.log(2 * np.pi * R) - 0.5 * ((Y_obs[k+1] - h_x)**2) / R\n        \n        # Normalize weights\n        log_weights = log_likelihood - logsumexp(log_likelihood)\n        weights = np.exp(log_weights)\n        \n        # Ensure weights sum to 1.0, crucial for CDF\n        weights /= np.sum(weights)\n\n        # III. Compute posterior mean\n        posterior_means[k] = np.sum(weights * particles)\n        \n        # IV. Resample if needed\n        N_eff = 1.0 / np.sum(weights**2)\n        if N_eff  tau * N:\n            cdf = np.cumsum(weights)\n            cdf[-1] = 1.0 # Enforce sum to 1 to avoid float errors with searchsorted\n\n            # Generate uniforms for this step (systematic resampling)\n            u0 = base_resample_uniforms[k] / N\n            uniforms = u0 + np.arange(N) / N\n\n            # Select resampler based on type\n            if resampler_type == 'sequential':\n                ancestor_indices = resample_sequential(cdf, uniforms)\n            elif resampler_type == 'vectorized':\n                ancestor_indices = resample_vectorized(cdf, uniforms)\n            else:\n                raise ValueError(\"Invalid resampler type\")\n\n            particles = particles[ancestor_indices]\n            # Weights are reset to 1/N implicitly for the next step's prior\n            # Not explicitly needed as we don't carry them over in bootstrap filter\n            \n    return posterior_means\n\n\ndef solve():\n    results = []\n\n    # Test case 1: Systematic resampling index equivalence, odd N\n    N1 = 1001\n    rng_w1 = np.random.default_rng(42)\n    weights1 = rng_w1.gamma(2, 1, N1)\n    weights1 /= np.sum(weights1)\n    cdf1 = np.cumsum(weights1)\n    cdf1[-1] = 1.0\n    u0_1 = 0.3141592653589793 / N1\n    uniforms1 = u0_1 + np.arange(N1) / N1\n    indices_seq1 = resample_sequential(cdf1, uniforms1)\n    indices_vec1 = resample_vectorized(cdf1, uniforms1)\n    mismatches1 = np.sum(indices_seq1 != indices_vec1)\n    results.append(mismatches1)\n\n    # Test case 2: Stratified resampling index equivalence, power-of-two N\n    N2 = 2048\n    rng_w2 = np.random.default_rng(123)\n    weights2 = rng_w2.gamma(3, 1, N2)\n    weights2 /= np.sum(weights2)\n    cdf2 = np.cumsum(weights2)\n    cdf2[-1] = 1.0\n    rng_u2 = np.random.default_rng(2024)\n    base_uniforms2 = rng_u2.uniform(0, 1, N2)\n    uniforms2 = (np.arange(N2) + base_uniforms2) / N2\n    indices_seq2 = resample_sequential(cdf2, uniforms2)\n    indices_vec2 = resample_vectorized(cdf2, uniforms2)\n    mismatches2 = np.sum(indices_seq2 != indices_vec2)\n    results.append(mismatches2)\n\n    # Test case 3: Full particle filter equivalence with resampling\n    params_3 = {\n        'N': 8192, 'T': 15, 'x0': 0.1,\n        'alpha': 0.8, 'beta': 0.2, 'sigma': 0.6,\n        'c': 5.0, 'R': 0.25, 'delta_t': 0.05,\n        's_data': 777, 's_0': 2021, 's_Xi': 2022, 's_u0': 2023,\n        'tau': 0.5\n    }\n    means_seq3 = run_particle_filter(**params_3, resampler_type='sequential')\n    means_vec3 = run_particle_filter(**params_3, resampler_type='vectorized')\n    max_diff3 = np.max(np.abs(means_seq3 - means_vec3))\n    results.append(max_diff3)\n\n    # Test case 4: Degenerate weight vector\n    N4 = 512\n    weights4 = np.zeros(N4)\n    weights4[133] = 1.0\n    cdf4 = np.cumsum(weights4)\n    u0_4 = 0.2718281828459045 / N4\n    uniforms4 = u0_4 + np.arange(N4) / N4\n    indices_seq4 = resample_sequential(cdf4, uniforms4)\n    indices_vec4 = resample_vectorized(cdf4, uniforms4)\n    mismatches4 = np.sum(indices_seq4 != indices_vec4)\n    results.append(mismatches4)\n    \n    # Test case 5: Full particle filter equivalence without resampling\n    params_5 = {\n        'N': 4096, 'T': 15, 'x0': 0.1,\n        'alpha': 0.8, 'beta': 0.2, 'sigma': 0.6,\n        'c': 5.0, 'R': 0.25, 'delta_t': 0.05,\n        's_data': 31415, 's_0': 27182, 's_Xi': 14142, 's_u0': 17320,\n        'tau': 0.0  # Disables resampling\n    }\n    means_seq5 = run_particle_filter(**params_5, resampler_type='sequential')\n    means_vec5 = run_particle_filter(**params_5, resampler_type='vectorized')\n    max_diff5 = np.max(np.abs(means_seq5 - means_vec5))\n    results.append(max_diff5)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2990093"}]}