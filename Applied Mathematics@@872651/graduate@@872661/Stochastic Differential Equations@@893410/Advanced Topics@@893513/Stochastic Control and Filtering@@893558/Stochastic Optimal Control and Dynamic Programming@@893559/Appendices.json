{"hands_on_practices": [{"introduction": "We begin with a fundamental exercise that bridges the discrete and continuous worlds of dynamic programming. You will start from the Dynamic Programming Principle (DPP) for a system over a small time interval, and by passing to the limit, formally derive the continuous-time Hamilton-Jacobi-Bellman (HJB) equation. This practice is foundational for understanding where the HJB equation comes from and solidifies the connection between the value function and its governing partial differential equation. [@problem_id:2998133]", "problem": "Consider a finite time horizon $T>0$ and fix any $t \\in [0,T]$ and $x \\in \\mathbb{R}$. Let $\\{W_s\\}_{s \\ge 0}$ be a standard Brownian motion and consider the controlled stochastic differential equation (SDE) $dX_s = a_s\\,ds + \\sigma\\,dW_s$ for $s \\in [t,T]$ with $X_t = x$, where $\\sigma>0$ is constant and the control process $\\{a_s\\}_{s \\in [t,T]}$ is progressively measurable with respect to the filtration generated by $\\{W_s\\}_{s \\ge 0}$ and satisfies $\\mathbb{E}\\big[\\int_t^T a_s^2\\,ds\\big] < \\infty$. The performance criterion is the running cost $f(x,a) = \\frac{1}{2}a^2$ and the terminal cost $g \\equiv 0$, so the value function is\n$$\nV(t,x) \\coloneqq \\inf_{\\{a_s\\}} \\,\\mathbb{E}\\!\\left[\\int_t^T \\frac{1}{2}a_s^2\\,ds\\right].\n$$\nUsing the Dynamic Programming Principle (DPP), approximate the problem by restricting to controls that are piecewise constant on a uniform partition of $[t,T]$ of mesh size $\\Delta$, derive the discrete-time dynamic programming recursion, and pass to the limit $\\Delta \\to 0$ to obtain the associated continuous-time Hamilton–Jacobi–Bellman equation. Then solve the resulting boundary value problem to compute the exact analytic expression for $V(t,x)$ as a function of $t$, $x$, $\\sigma$, and $T$. You may assume standard regularity conditions ensuring existence and uniqueness of solutions to the SDE and justifying Taylor expansions and interchanges of limits with expectations. Express your final answer as a single closed-form expression. No rounding is required, and no physical units need be reported.", "solution": "The problem presented is a standard finite-horizon stochastic optimal control problem. The objective is to determine the value function $V(t,x)$ by first deriving the associated Hamilton-Jacobi-Bellman (HJB) equation using the Dynamic Programming Principle (DPP), and subsequently solving the resulting partial differential equation (PDE) with its terminal condition.\n\n**Step 1: Derivation of the Hamilton-Jacobi-Bellman Equation**\n\nThe value function is defined as:\n$$ V(t,x) = \\inf_{\\{a_s\\}} \\mathbb{E}\\left[\\int_t^T \\frac{1}{2}a_s^2\\,ds\\right] $$\nsubject to the state dynamics $dX_s = a_s\\,ds + \\sigma\\,dW_s$ with $X_t=x$.\n\nThe Dynamic Programming Principle states that for any time $t \\in [0, T)$ and a small time increment $\\Delta t > 0$ such that $t+\\Delta t \\le T$, the value function must satisfy:\n$$ V(t,x) = \\inf_{\\{a_s\\}_{s \\in [t, t+\\Delta t]}} \\mathbb{E}\\left[ \\int_t^{t+\\Delta t} \\frac{1}{2}a_s^2\\,ds + V(t+\\Delta t, X_{t+\\Delta t}) \\right] $$\nWe consider a piecewise constant control approximation, where $a_s = a_t$ for $s \\in [t, t+\\Delta t]$. The integral becomes $\\frac{1}{2}a_t^2 \\Delta t$. The state at time $t+\\Delta t$ is given by the discretized SDE:\n$$ X_{t+\\Delta t} = x + a_t \\Delta t + \\sigma (W_{t+\\Delta t} - W_t) = x + a_t \\Delta t + \\sigma \\Delta W_t $$\nwhere $\\Delta W_t \\sim N(0, \\Delta t)$.\n\nThe DPP equation can be written as:\n$$ V(t,x) \\approx \\inf_{a_t} \\left\\{ \\frac{1}{2} a_t^2 \\Delta t + \\mathbb{E}\\left[ V(t+\\Delta t, x + a_t \\Delta t + \\sigma \\Delta W_t) \\right] \\right\\} $$\nWe assume the value function $V(t,x)$ is of class $C^{1,2}([0,T] \\times \\mathbb{R})$. Applying Itô's lemma to $V(s, X_s)$ over the interval $[t, t+\\Delta t]$ gives:\n$$ dV(s,X_s) = \\left( \\frac{\\partial V}{\\partial s} + a_s \\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right) ds + \\sigma \\frac{\\partial V}{\\partial x} dW_s $$\nIntegrating from $t$ to $t+\\Delta t$ and taking the expectation yields:\n$$ \\mathbb{E}[V(t+\\Delta t, X_{t+\\Delta t})] - V(t,x) = \\mathbb{E}\\left[ \\int_t^{t+\\Delta t} \\left( \\frac{\\partial V}{\\partial s} + a_s \\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right) ds \\right] $$\nFor a small $\\Delta t$, and approximating the integrand as constant at time $t$ with control $a_t$:\n$$ \\mathbb{E}[V(t+\\Delta t, X_{t+\\Delta t})] - V(t,x) \\approx \\left( \\frac{\\partial V}{\\partial t} + a_t \\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right) \\Delta t $$\nSubstituting this into the DPP equation:\n$$ V(t,x) \\approx \\inf_{a_t} \\left\\{ \\frac{1}{2}a_t^2 \\Delta t + V(t,x) + \\left( \\frac{\\partial V}{\\partial t} + a_t \\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right) \\Delta t \\right\\} $$\nSubtracting $V(t,x)$, dividing by $\\Delta t > 0$, and taking the limit as $\\Delta t \\to 0$ gives:\n$$ 0 = \\lim_{\\Delta t \\to 0} \\inf_{a_t} \\left\\{ \\frac{1}{2}a_t^2 + \\frac{\\partial V}{\\partial t} + a_t \\frac{\\partial V}{\\partial x} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} \\right\\} $$\nThe terms not depending on $a_t$ can be moved outside the infimum:\n$$ 0 = \\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} + \\inf_{a \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}a^2 + a \\frac{\\partial V}{\\partial x} \\right\\} $$\nTo find the infimum, we minimize the quadratic function $f(a) = \\frac{1}{2}a^2 + a \\frac{\\partial V}{\\partial x}$ with respect to $a$. The first-order condition $\\frac{df}{da} = 0$ gives:\n$$ a + \\frac{\\partial V}{\\partial x} = 0 \\implies a^* = -\\frac{\\partial V}{\\partial x} $$\nThis is the optimal control in feedback form. Substituting $a^*$ back into the expression:\n$$ \\inf_{a \\in \\mathbb{R}} \\{ f(a) \\} = \\frac{1}{2}\\left(-\\frac{\\partial V}{\\partial x}\\right)^2 + \\left(-\\frac{\\partial V}{\\partial x}\\right)\\frac{\\partial V}{\\partial x} = \\frac{1}{2}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 - \\left(\\frac{\\partial V}{\\partial x}\\right)^2 = -\\frac{1}{2}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 $$\nSubstituting this result into the equation gives the Hamilton-Jacobi-Bellman equation:\n$$ \\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} - \\frac{1}{2}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0 $$\nThis PDE must be solved subject to the terminal condition $V(T,x) = g(x)$, which is given as $g(x) = 0$. Thus, we have the terminal condition:\n$$ V(T,x) = 0 \\quad \\text{for all } x \\in \\mathbb{R} $$\n\n**Step 2: Solving the Hamilton-Jacobi-Bellman Equation**\n\nWe have the following final-value problem:\n$$ \\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2} - \\frac{1}{2}\\left(\\frac{\\partial V}{\\partial x}\\right)^2 = 0, \\quad V(T,x) = 0 $$\nWe observe that the running cost $f(x,a) = \\frac{1}{2}a^2$ and the terminal cost $g(x)=0$ are independent of the state variable $x$. This strongly suggests that the value function $V(t,x)$ will also be independent of $x$. We make the ansatz that $V(t,x)$ is only a function of time, $V(t,x) = \\phi(t)$.\n\nWith this ansatz, the partial derivatives with respect to $x$ are zero:\n$$ \\frac{\\partial V}{\\partial x} = 0 \\quad \\text{and} \\quad \\frac{\\partial^2 V}{\\partial x^2} = 0 $$\nSubstituting these into the HJB equation simplifies it to an ordinary differential equation for $\\phi(t)$:\n$$ \\frac{d\\phi}{dt} + \\frac{1}{2}\\sigma^2(0) - \\frac{1}{2}(0)^2 = 0 \\implies \\frac{d\\phi}{dt} = 0 $$\nThe general solution is $\\phi(t) = C$, where $C$ is a constant.\nWe use the terminal condition $V(T,x) = 0$ to find $C$. This condition translates to $\\phi(T) = 0$.\nEvaluating our solution at $t=T$, we get $\\phi(T) = C$. Therefore, we must have $C=0$.\n\nThe unique classical solution to the HJB equation satisfying the terminal condition is $V(t,x) = 0$.\n\nThis result can be confirmed by direct inspection of the cost functional. The cost is $J(\\{a_s\\}) = \\mathbb{E}[\\int_t^T \\frac{1}{2}a_s^2 ds]$. Since $a_s^2 \\ge 0$, the cost is always non-negative, so $V(t,x) \\ge 0$. The control $a_s = 0$ for all $s \\in [t,T]$ is admissible, as $\\mathbb{E}[\\int_t^T 0^2 ds] = 0 < \\infty$. The cost for this control is $J(\\{0\\}) = 0$. Since we have found an admissible control that achieves a cost of $0$, and the cost cannot be lower than $0$, the infimum must be $0$.\n\nHence, the value function is $V(t,x) = 0$ for all $(t,x) \\in [0,T] \\times \\mathbb{R}$.", "answer": "$$\\boxed{0}$$", "id": "2998133"}, {"introduction": "With the HJB equation now derived from first principles, this next exercise provides practice in solving it for a classic exit-time problem. You will work with a stationary (time-independent) HJB equation for a controlled diffusion on a bounded interval, with the goal of maximizing the expected exit location. This problem is essential for mastering the techniques of solving the HJB as a boundary value problem to find an optimal strategy and its associated value. [@problem_id:2998163]", "problem": "Consider the controlled one-dimensional stochastic differential equation on the open interval $(0,1)$,\n$$\ndX_{t} \\;=\\; a_{t}\\,dt \\;+\\; \\sigma\\,dW_{t}, \\qquad X_{0}=x \\in (0,1),\n$$\nwhere $\\sigma>0$ is a given constant, $W_{t}$ is a standard Brownian motion, and the control process $a_{t}$ is progressively measurable with values in the compact set $A=[-1,1]$. Let the system be absorbed at the boundary: define the exit time\n$$\n\\tau \\;=\\; \\inf\\{t\\ge 0 \\,:\\, X_{t} \\notin (0,1)\\},\n$$\nand the absorbed process $X_{t\\wedge\\tau}$. The performance criterion is an exit-time objective with zero running cost and terminal reward equal to location, namely $f \\equiv 0$ and $h(x)=x$, so that for an admissible control $a=(a_{t})_{t\\ge 0}$,\n$$\nJ^{a}(x) \\;=\\; \\mathbb{E}_{x}\\!\\left[\\,h\\!\\left(X_{\\tau}\\right)\\right] \\;=\\; \\mathbb{E}_{x}\\!\\left[X_{\\tau}\\right],\n$$\nand the value function is\n$$\nV(x) \\;=\\; \\sup_{a}\\, J^{a}(x), \\qquad x\\in(0,1),\n$$\nwith absorbing boundary conditions $V(0)=h(0)=0$ and $V(1)=h(1)=1$.\n\nStarting from the dynamic programming principle for exit-time problems and the infinitesimal generator of the controlled diffusion, derive the associated stationary Hamilton–Jacobi–Bellman (HJB) equation on $(0,1)$ and solve it explicitly under the above boundary conditions. Your final answer must be a single closed-form analytic expression for the value function $V(x)$ in terms of $x$ and $\\sigma$.", "solution": "We begin from the dynamic programming principle for exit-time problems in stochastic control. For a sufficiently smooth candidate value function $V$, the principle implies that $V$ should satisfy, in the viscosity sense and in the classical sense if $V\\in C^{2}((0,1))$, the stationary Hamilton–Jacobi–Bellman (HJB) equation\n$$\n0 \\;=\\; \\sup_{a\\in A} \\left\\{ \\mathcal{L}^{a} V(x) \\right\\}, \\qquad x\\in(0,1),\n$$\nsubject to the absorbing boundary conditions $V(0)=0$ and $V(1)=1$. Here $\\mathcal{L}^{a}$ is the infinitesimal generator of the controlled diffusion,\n$$\n\\mathcal{L}^{a} \\phi(x) \\;=\\; a\\,\\phi'(x) \\;+\\; \\frac{\\sigma^{2}}{2}\\,\\phi''(x).\n$$\nPlugging this into the HJB yields the second-order ordinary differential equation (elliptic HJB in one dimension),\n$$\n0 \\;=\\; \\sup_{a\\in[-1,1]} \\left\\{ a\\,V'(x) \\;+\\; \\frac{\\sigma^{2}}{2}\\,V''(x) \\right\\}, \\qquad x\\in(0,1),\n$$\nwith $V(0)=0$ and $V(1)=1$.\n\nThe supremum over $a\\in[-1,1]$ of the linear expression $a\\,V'(x)$ is given by\n$$\n\\sup_{a\\in[-1,1]} a\\,V'(x) \\;=\\; |V'(x)|.\n$$\nThus the HJB can be written as\n$$\n\\frac{\\sigma^{2}}{2}\\,V''(x) \\;+\\; |V'(x)| \\;=\\; 0, \\qquad x\\in(0,1),\n$$\nwith $V(0)=0$ and $V(1)=1$.\n\nWe will solve this explicitly. Anticipating that the optimal strategy will push the state upward (toward $x=1$) due to the monotone terminal reward $h(x)=x$, it is natural to seek a solution $V$ that is nondecreasing. Suppose $V'(x)\\ge 0$ on $(0,1)$. Then $|V'(x)|=V'(x)$ and the HJB reduces to the linear ordinary differential equation\n$$\n\\frac{\\sigma^{2}}{2}\\,V''(x) \\;+\\; V'(x) \\;=\\; 0.\n$$\nLet $Y(x)=V'(x)$. Then\n$$\n\\frac{\\sigma^{2}}{2}\\,Y'(x) \\;+\\; Y(x) \\;=\\; 0,\n$$\nwhich is a first-order linear equation. Solving this gives\n$$\nY'(x) \\;=\\; -\\frac{2}{\\sigma^{2}}\\,Y(x) \\quad \\Longrightarrow \\quad Y(x) \\;=\\; C\\,\\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right),\n$$\nfor some constant $C$. Integrating $Y$ yields\n$$\nV(x) \\;=\\; \\int_{0}^{x} Y(s)\\,ds \\;+\\; V(0) \\;=\\; \\int_{0}^{x} C\\,\\exp\\!\\left(-\\frac{2s}{\\sigma^{2}}\\right)\\,ds \\;+\\; 0 \\;=\\; \\frac{C\\,\\sigma^{2}}{2}\\left(1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)\\right).\n$$\nImposing the boundary condition at $x=1$, we obtain\n$$\nV(1) \\;=\\; \\frac{C\\,\\sigma^{2}}{2}\\left(1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)\\right) \\;=\\; 1,\n$$\nso\n$$\nC \\;=\\; \\frac{2}{\\sigma^{2}\\left(1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)\\right)}.\n$$\nSubstituting $C$ back into $V(x)$ gives the explicit expression\n$$\nV(x) \\;=\\; \\frac{1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)}, \\qquad x\\in[0,1].\n$$\nWe verify that $V(0)=0$ and $V(1)=1$, and that $V'(x)=\\dfrac{\\frac{2}{\\sigma^{2}}\\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)} \\ge 0$ for $x\\in(0,1)$, so our initial monotonicity assumption is satisfied. Consequently, the Hamiltonian maximizer is $a^{*}(x)=1$ for all $x\\in(0,1)$.\n\nTo verify optimality, apply Itô's formula to $V(X_{t\\wedge\\tau})$ under any admissible control $a_{t}$:\n$$\nV(X_{t\\wedge\\tau}) \\;=\\; V(x) \\;+\\; \\int_{0}^{t\\wedge\\tau} \\left(a_{s}\\,V'(X_{s}) + \\frac{\\sigma^{2}}{2}\\,V''(X_{s})\\right)ds \\;+\\; \\int_{0}^{t\\wedge\\tau} \\sigma\\,V'(X_{s})\\,dW_{s}.\n$$\nBy the HJB, $\\sup_{a} \\{ a\\,V'(x) + \\frac{\\sigma^{2}}{2}V''(x) \\}=0$, so for any $a_{t}$ we have $a_{t}\\,V'(X_{t}) + \\frac{\\sigma^{2}}{2}V''(X_{t}) \\le 0$, making $V(X_{t\\wedge\\tau})$ a supermartingale. Taking expectations and letting $t\\to\\infty$ yields\n$$\nV(x) \\;\\ge\\; \\mathbb{E}_{x}\\!\\left[V\\!\\left(X_{\\tau}\\right)\\right] \\;=\\; \\mathbb{E}_{x}\\!\\left[h\\!\\left(X_{\\tau}\\right)\\right] \\;=\\; J^{a}(x),\n$$\nfor every admissible $a$. For the feedback control $a^{*}(x)=1$, the inequality becomes equality because $a^{*}\\,V'(x) + \\frac{\\sigma^{2}}{2}V''(x)=0$ pointwise, and $V(X_{t\\wedge\\tau})$ is a martingale. Hence $V$ is indeed the value function and $a^{*}(x)=1$ is optimal.\n\nTherefore, the associated elliptic HJB is solved explicitly by\n$$\nV(x) \\;=\\; \\frac{1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)}.\n$$", "answer": "$$\\boxed{\\frac{1 - \\exp\\!\\left(-\\frac{2x}{\\sigma^{2}}\\right)}{1 - \\exp\\!\\left(-\\frac{2}{\\sigma^{2}}\\right)}}$$", "id": "2998163"}, {"introduction": "This final practice problem delves into the theoretical foundations of optimal control by contrasting dynamic programming with the Pontryagin Maximum Principle (SMP). By analyzing a deterministic problem with a non-convex cost, you will see firsthand why the necessary conditions provided by the SMP may not be sufficient for global optimality. This exercise is crucial for developing a deeper appreciation for the global nature of the Hamilton-Jacobi-Bellman approach and understanding the potential pitfalls of alternative methods. [@problem_id:2998136]", "problem": "Consider the controlled stochastic differential equation (specialized here to a degenerate diffusion with zero diffusion coefficient) on the finite horizon $[0,1]$,\n$$\n\\mathrm{d}x_t \\;=\\; u_t\\,\\mathrm{d}t,\\qquad x_0 \\;=\\; 0,\n$$\nwith admissible controls satisfying $u_t \\in [-1,1]$ and the cost functional\n$$\nJ(u) \\;=\\; \\phi\\!\\big(x_1\\big),\\qquad \\phi(y) \\;=\\; \\big(y^2 - 1\\big)^2.\n$$\nYou will use both the stochastic maximum principle (SMP), also known as the Pontryagin Maximum Principle, and dynamic programming to analyze this problem. The terminal cost $\\phi$ is nonconvex, and there is no running cost.\n\nTasks:\n1. Starting from the fundamental definitions of optimal control and the Pontryagin Maximum Principle (SMP), derive the necessary conditions for optimality for this problem, explicitly identifying the Hamiltonian and the adjoint (co-state) dynamics.\n2. Show that the control $u_t \\equiv 0$ is a candidate extremal according to the SMP necessary conditions, and compute its cost $J(u)$.\n3. Using the dynamic programming principle and reachable-set reasoning from first principles, determine the exact optimal value $J^{\\star} := \\inf_{u} J(u)$ and identify a control that attains it.\n\nFinally, provide the exact numerical value of the suboptimality gap $J(u^{\\mathrm{MP}}) - J^{\\star}$, where $u^{\\mathrm{MP}}$ denotes the SMP candidate control found in Task 2. Express the final suboptimality gap as an exact real number. No rounding is required.", "solution": "The problem is to find the optimal control for the system governed by the deterministic ordinary differential equation $\\mathrm{d}x_t = u_t\\,\\mathrm{d}t$ with initial condition $x_0 = 0$ on the time horizon $t \\in [0, 1]$. The control $u_t$ is constrained to the set $[-1, 1]$. The objective is to minimize the cost functional $J(u) = \\phi(x_1)$, where the terminal cost function is given by $\\phi(y) = (y^2 - 1)^2$.\n\nThis is a problem in deterministic optimal control, which can be viewed as a special case of stochastic optimal control where the diffusion coefficient is zero. We will analyze this problem using both the Pontryagin Maximum Principle (SMP) and dynamic programming concepts.\n\n**Task 1: Pontryagin Maximum Principle (SMP) Necessary Conditions**\n\nThe Pontryagin Maximum Principle provides necessary conditions for a control $u_t^*$ and its corresponding state trajectory $x_t^*$ to be optimal. We introduce an adjoint (or co-state) variable $p_t$.\n\nThe Hamiltonian for this system is defined as:\n$$\nH(x, u, p) = L(x, u) + p \\cdot f(x, u)\n$$\nwhere $L$ is the running cost and $f$ is the dynamics function. In this problem, the running cost is zero, $L=0$, and the dynamics are $f(x, u) = u$. Thus, the Hamiltonian is:\n$$\nH(x_t, u_t, p_t) = p_t u_t\n$$\nThe state and adjoint equations form a Hamiltonian system. The state equation is given:\n$$\n\\frac{\\mathrm{d}x_t}{\\mathrm{d}t} = \\frac{\\partial H}{\\partial p} = u_t, \\quad x_0 = 0\n$$\nThe adjoint equation for $p_t$ is given by:\n$$\n\\frac{\\mathrm{d}p_t}{\\mathrm{d}t} = -\\frac{\\partial H}{\\partial x} = -\\frac{\\partial}{\\partial x}(p_t u_t) = 0\n$$\nThis implies that the adjoint variable $p_t$ is constant over the time interval $[0, 1]$. Let us denote this constant value by $p$.\n$$\np_t = p \\quad \\forall t \\in [0, 1]\n$$\nThe transversality condition at the terminal time $t=1$ connects the adjoint variable to the terminal cost function $\\phi$:\n$$\np_1 = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}(x_1)\n$$\nGiven $\\phi(y) = (y^2 - 1)^2$, its derivative is $\\frac{\\mathrm{d}\\phi}{\\mathrm{d}y} = 2(y^2-1)(2y) = 4y(y^2-1)$.\nTherefore, the transversality condition is:\n$$\np_1 = 4x_1(x_1^2 - 1)\n$$\nSince $p_t$ is constant, we have $p = p_1 = 4x_1(x_1^2 - 1)$.\n\nThe final necessary condition is Pontryagin's maximum principle itself: for an optimal control $u_t^*$, it must maximize the Hamiltonian for almost every $t \\in [0, 1]$:\n$$\nu_t^* \\in \\arg\\max_{v \\in [-1, 1]} H(x_t^*, v, p_t) = \\arg\\max_{v \\in [-1, 1]} (p_t v)\n$$\nThe form of the optimal control therefore depends on the sign of the constant adjoint state $p$:\n- If $p > 0$, then $u_t^* = 1$.\n- If $p < 0$, then $u_t^* = -1$.\n- If $p = 0$, then any control $u_t^* \\in [-1, 1]$ maximizes the Hamiltonian, which is identically zero. This is the singular case.\n\n**Task 2: Analysis of the Candidate Control $u_t \\equiv 0$**\n\nWe examine whether the control $u_t \\equiv 0$ for all $t \\in [0, 1]$ satisfies the necessary conditions derived from the SMP.\n\nFirst, we determine the state trajectory corresponding to this control. Integrating the state equation with $x_0=0$:\n$$\nx_t = x_0 + \\int_0^t u_s\\,\\mathrm{d}s = 0 + \\int_0^t 0\\,\\mathrm{d}s = 0\n$$\nSo, $x_t \\equiv 0$ for all $t \\in [0, 1]$. The terminal state is $x_1 = 0$.\n\nNext, we determine the corresponding adjoint state $p$ using the transversality condition evaluated at $x_1=0$:\n$$\np = p_1 = 4x_1(x_1^2 - 1) = 4(0)(0^2 - 1) = 0\n$$\nSo, the adjoint state corresponding to this trajectory is $p_t \\equiv 0$.\n\nFinally, we check if the control $u_t \\equiv 0$ satisfies the maximum principle condition with $p_t \\equiv 0$. The condition requires that for almost every $t$,\n$$\nu_t \\in \\arg\\max_{v \\in [-1, 1]} (p_t v)\n$$\nSubstituting $p_t = 0$ and $u_t = 0$:\n$$\n0 \\in \\arg\\max_{v \\in [-1, 1]} (0 \\cdot v)\n$$\nThe expression to be maximized is $0 \\cdot v = 0$, which is constant for all $v \\in [-1, 1]$. Thus, any control $v \\in [-1, 1]$ is a maximizer. Since $0 \\in [-1, 1]$, the choice $u_t \\equiv 0$ is valid.\nTherefore, the control $u_t \\equiv 0$ is a candidate extremal trajectory satisfying the necessary conditions of the Pontryagin Maximum Principle.\n\nThe cost associated with this control, denoted as $u^{\\mathrm{MP}}$ in the problem statement, is:\n$$\nJ(u^{\\mathrm{MP}}) = J(0) = \\phi(x_1) = \\phi(0) = (0^2 - 1)^2 = (-1)^2 = 1\n$$\n\n**Task 3: Optimal Value via Dynamic Programming and Reachable Set Analysis**\n\nThe principle of dynamic programming implies that the optimal cost, $J^\\star$, is the minimum possible value of the cost functional.\n$$\nJ^\\star = \\inf_{u(\\cdot)} J(u) = \\inf_{u(\\cdot)} \\phi(x_1)\n$$\nTo find this infimum, we first determine the set of all possible values for the terminal state $x_1$. This is known as the reachable set at time $t=1$.\nThe state at time $t=1$ is given by the integral of the control:\n$$\nx_1 = \\int_0^1 u_t\\,\\mathrm{d}t\n$$\nGiven the constraint $u_t \\in [-1, 1]$, we can bound the integral:\n$$\n\\int_0^1 (-1)\\,\\mathrm{d}t \\le \\int_0^1 u_t\\,\\mathrm{d}t \\le \\int_0^1 (1)\\,\\mathrm{d}t\n$$\nThis gives:\n$$\n-1 \\le x_1 \\le 1\n$$\nThe reachable set for $x_1$ is the closed interval $[-1, 1]$. Any value $y \\in [-1, 1]$ is reachable, for example by using the constant control $u_t = y$, which is admissible since $y \\in [-1, 1]$.\n\nThe optimization problem is now reduced to a simple minimization of a function of one variable:\n$$\nJ^\\star = \\min_{y \\in [-1, 1]} \\phi(y) = \\min_{y \\in [-1, 1]} (y^2 - 1)^2\n$$\nThe function $\\phi(y) = (y^2 - 1)^2$ is non-negative for all real $y$. Its value is zero if and only if $y^2 - 1 = 0$, which occurs at $y = 1$ and $y = -1$.\nBoth of these values, $1$ and $-1$, lie within the reachable set $[-1, 1]$.\nTherefore, the minimum value of $\\phi(y)$ on this interval is $0$.\n$$\nJ^\\star = 0\n$$\nThis minimum value is attained. An optimal control must produce a terminal state of $x_1=1$ or $x_1=-1$.\n- To achieve $x_1=1$, we can use the constant control $u_t \\equiv 1$ for $t \\in [0, 1]$. This is an admissible control, and $x_1 = \\int_0^1 1\\,\\mathrm{d}t = 1$. The cost is $J(1) = (1^2 - 1)^2 = 0$.\n- To achieve $x_1=-1$, we can use the constant control $u_t \\equiv -1$ for $t \\in [0, 1]$. This gives $x_1 = \\int_0^1 (-1)\\,\\mathrm{d}t = -1$. The cost is $J(-1) = ((-1)^2 - 1)^2 = 0$.\n\nThus, the exact optimal value is $J^\\star = 0$. The non-convexity of the cost function $\\phi(y)$ is key: it has a local maximum at $y=0$ which corresponds to the suboptimal extremal found by the SMP, and global minima at $y=\\pm 1$ which correspond to the true optimal solutions.\n\n**Suboptimality Gap Calculation**\n\nThe problem asks for the suboptimality gap $J(u^{\\mathrm{MP}}) - J^\\star$, where $u^{\\mathrm{MP}}$ is the SMP candidate control from Task 2.\nFrom Task 2, the candidate control is $u^{\\mathrm{MP}}_t \\equiv 0$, and its corresponding cost is:\n$$\nJ(u^{\\mathrm{MP}}) = 1\n$$\nFrom Task 3, the true optimal value is:\n$$\nJ^\\star = 0\n$$\nThe suboptimality gap is the difference between these two values:\n$$\nJ(u^{\\mathrm{MP}}) - J^\\star = 1 - 0 = 1\n$$\nThis gap highlights that the necessary conditions from the Pontryagin Maximum Principle are not always sufficient for global optimality, especially in problems with non-convex cost functions. The SMP identified a trajectory that is locally stationary but globally suboptimal.", "answer": "$$\\boxed{1}$$", "id": "2998136"}]}