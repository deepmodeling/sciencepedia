{"hands_on_practices": [{"introduction": "A primary motivation for viscosity solutions is that value functions in optimal control problems are often not differentiable everywhere. This exercise brings this issue to the forefront by examining the canonical non-smooth function, $u(x)=|x|$. By working directly from the foundational definitions of subjets and supersolutions, you will see precisely how the viscosity framework gracefully handles points of non-differentiability where classical analysis fails [@problem_id:3005560]. This practice solidifies the core concepts that underpin the entire theory.", "problem": "Let $u:\\mathbb{R}\\to\\mathbb{R}$ be given by $u(x)=|x|$. Consider the Hamilton–Jacobi–Bellman (HJB) operator $F:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ defined for $p\\in\\mathbb{R}$ and $X\\in\\mathbb{R}$ by\n$$\nF(p,X) \\;=\\; \\max\\{ -X,\\; 1 - |p| \\}.\n$$\nThis operator is degenerate elliptic in the sense of viscosity theory, and it depends only on the first- and second-order jets. Work at the point $x=0$.\n\nUsing only the foundational definitions of second-order semijets and viscosity supersolutions, proceed as follows:\n\n1) Starting from the definition of derivative, explain why $D^2 u(0)$ does not exist.\n\n2) Starting from the definition of the second-order subjet $J^{2,-}u(0)$ (defined by $u(x)\\ge u(0)+p(x-0)+\\tfrac{1}{2}X(x-0)^2 + o(|x|^2)$ as $x\\to 0$), characterize the full set $J^{2,-}u(0)$.\n\n3) Argue why the second-order superjet $J^{2,+}u(0)$ is empty.\n\n4) In the viscosity-supersolution sense for the equation $F(Du,D^2u)=0$, one must check $F(p,X)\\ge 0$ for all $(p,X)\\in J^{2,-}u(0)$. To quantify the sharpness of this inequality at $x=0$, compute the exact value of\n$$\n\\inf_{(p,X)\\in J^{2,-}u(0)} F(p,X).\n$$\n\nYour final answer must be a single real number. No rounding is required.", "solution": "The problem asks for an analysis of the function $u(x)=|x|$ at the point $x=0$ in the context of the viscosity solution framework for the Hamilton-Jacobi-Bellman (HJB) equation governed by the operator $F(p,X) = \\max\\{ -X,\\; 1 - |p| \\}$. We will address each of the four parts of the problem in sequence.\n\nFirst, we address the classical differentiability of $u(x)$ at $x=0$.\nThe first derivative of a function $u$ at a point $x_0$ is defined as $u'(x_0) = \\lim_{h\\to 0} \\frac{u(x_0+h)-u(x_0)}{h}$, provided this limit exists. For $u(x)=|x|$ at $x_0=0$, we examine the one-sided limits:\nThe right-hand limit is:\n$$ \\lim_{h\\to 0^+} \\frac{u(0+h)-u(0)}{h} = \\lim_{h\\to 0^+} \\frac{|h|-|0|}{h} = \\lim_{h\\to 0^+} \\frac{h}{h} = 1 $$\nThe left-hand limit is:\n$$ \\lim_{h\\to 0^-} \\frac{u(0+h)-u(0)}{h} = \\lim_{h\\to 0^-} \\frac{|h|-|0|}{h} = \\lim_{h\\to 0^-} \\frac{-h}{h} = -1 $$\nSince the left-hand and right-hand limits are not equal, the limit does not exist, and thus the first derivative $u'(0)$ is not defined. The second derivative, $D^2u(0)$ or $u''(0)$, is defined as the derivative of the first derivative $u'(x)$ at $x=0$. As $u'(0)$ does not exist, the classical second derivative $D^2u(0)$ cannot exist either.\n\nSecond, we characterize the second-order subjet $J^{2,-}u(0)$. By definition, $(p,X) \\in \\mathbb{R}\\times\\mathbb{R}$ is an element of $J^{2,-}u(0)$ if and only if\n$$ u(y) \\ge u(0) + p(y-0) + \\frac{1}{2}X(y-0)^2 + o(|y|^2) \\quad \\text{as } y \\to 0. $$\nSubstituting $u(y)=|y|$ and $u(0)=0$, this inequality becomes:\n$$ |y| \\ge py + \\frac{1}{2}Xy^2 + o(y^2). $$\nThis is equivalent to the condition that the test function $\\phi(y) = py + \\frac{1}{2}Xy^2$ lies below $u(y)$ near $y=0$ in the specified sense. More formally, this can be stated as:\n$$ \\liminf_{y \\to 0} \\frac{|y| - py - \\frac{1}{2}Xy^2}{y^2} \\ge 0. $$\nTo analyze this, we must check the limits as $y \\to 0^+$ and $y \\to 0^-$.\nFor $y \\to 0^+$:\n$$ \\liminf_{y \\to 0^+} \\frac{y - py - \\frac{1}{2}Xy^2}{y^2} = \\liminf_{y \\to 0^+} \\left( \\frac{1-p}{y} - \\frac{X}{2} \\right) \\ge 0. $$\nIf $p  1$, then $1-p > 0$, and $\\frac{1-p}{y} \\to +\\infty$ as $y \\to 0^+$. The inequality holds for any $X \\in \\mathbb{R}$.\nIf $p > 1$, then $1-p  0$, and $\\frac{1-p}{y} \\to -\\infty$. The inequality fails.\nIf $p = 1$, the expression becomes $\\liminf_{y \\to 0^+} (-\\frac{X}{2}) = -\\frac{X}{2}$. The condition is $-\\frac{X}{2} \\ge 0$, which implies $X \\le 0$.\nFor $y \\to 0^-$:\n$$ \\liminf_{y \\to 0^-} \\frac{-y - py - \\frac{1}{2}Xy^2}{y^2} = \\liminf_{y \\to 0^-} \\left( \\frac{-(1+p)}{y} - \\frac{X}{2} \\right) \\ge 0. $$\nLet $y = -h$ where $h \\to 0^+$. The expression is $\\liminf_{h \\to 0^+} (\\frac{1+p}{h} - \\frac{X}{2}) \\ge 0$.\nIf $p > -1$, then $1+p > 0$, and $\\frac{1+p}{h} \\to +\\infty$. The inequality holds for any $X \\in \\mathbb{R}$.\nIf $p  -1$, then $1+p  0$, and $\\frac{1+p}{h} \\to -\\infty$. The inequality fails.\nIf $p = -1$, the expression is $\\liminf_{h \\to 0^+} (-\\frac{X}{2}) = -\\frac{X}{2}$. The condition is $-\\frac{X}{2} \\ge 0$, which implies $X \\le 0$.\nCombining these conditions: $p$ must be in the interval $[-1, 1]$.\n- If $-1  p  1$, both one-sided conditions are met for any $X \\in \\mathbb{R}$.\n- If $p=1$, we must have $X \\le 0$.\n- If $p=-1$, we must have $X \\le 0$.\nSo, the second-order subjet is the set:\n$$ J^{2,-}u(0) = \\{ (p, X) \\in \\mathbb{R} \\times \\mathbb{R} \\mid p \\in (-1, 1), X \\in \\mathbb{R} \\} \\cup \\{ (\\pm 1, X) \\mid X \\le 0 \\}. $$\n\nThird, we argue that the second-order superjet $J^{2,+}u(0)$ is empty. By definition, $(p,X) \\in J^{2,+}u(0)$ if\n$$ u(y) \\le u(0) + py + \\frac{1}{2}Xy^2 + o(|y|^2) \\quad \\text{as } y \\to 0. $$\nFor $u(x)=|x|$ at $x=0$, this is $|y| \\le py + \\frac{1}{2}Xy^2 + o(|y|^2)$. This implies that $|y| - py \\le O(y^2)$.\nDividing by $|y|$ for $y \\ne 0$, we get $1 - p \\frac{y}{|y|} \\le O(|y|)$.\nTaking the limit as $y \\to 0^+$, we have $\\frac{y}{|y|}=1$, so $1 - p \\le 0$, which implies $p \\ge 1$.\nTaking the limit as $y \\to 0^-$, we have $\\frac{y}{|y|}=-1$, so $1 - p(-1) \\le 0$, which is $1+p \\le 0$, implying $p \\le -1$.\nA single real number $p$ cannot satisfy both $p \\ge 1$ and $p \\le -1$. This contradiction shows that no such pair $(p,X)$ exists. Hence, the set $J^{2,+}u(0)$ is empty.\n\nFourth, we compute the infimum of $F(p,X)$ over the set $J^{2,-}u(0)$.\nThe function is $F(p,X) = \\max\\{ -X,\\; 1 - |p| \\}$. The domain is the set $J^{2,-}u(0)$ determined above. We can compute the infimum by considering the two disjoint parts of the domain.\n\nCase 1: $(p,X)$ such that $-1  p  1$ and $X \\in \\mathbb{R}$.\nFor a fixed $p \\in (-1,1)$, the quantity $1-|p|$ is a positive constant. Let $C = 1-|p| > 0$. We want to find $\\inf_{X \\in \\mathbb{R}} \\max\\{ -X, C \\}$.\nThe value of $\\max\\{-X, C\\}$ is always greater than or equal to $C$. The infimum is achieved as $-X$ approaches $C$ from below. For any $X$ such that $-X \\le C$ (i.e., $X \\ge -C$), the maximum is $C$. Thus, $\\inf_{X \\in \\mathbb{R}} \\max\\{ -X, C \\} = C = 1-|p|$.\nNow, we take the infimum over $p \\in (-1,1)$:\n$$ \\inf_{p \\in (-1,1)} (1-|p|). $$\nThe function $1-|p|$ approaches its minimum value as $|p|$ approaches $1$. The infimum is $0$, although it is not attained in this part of the domain.\n\nCase 2: $p=\\pm 1$ and $X \\le 0$.\nFor these values of $p$, we have $|p|=1$. The function becomes:\n$$ F(p,X) = \\max\\{ -X, 1 - 1 \\} = \\max\\{ -X, 0 \\}. $$\nWe are given that $X \\le 0$, which implies $-X \\ge 0$.\nTherefore, for this part of the domain, $\\max\\{ -X, 0 \\} = -X$.\nWe want to compute $\\inf_{X \\le 0} (-X)$.\nAs $X$ varies over the interval $(-\\infty, 0]$, the expression $-X$ varies over $[0, \\infty)$.\nThe infimum of the set $[0, \\infty)$ is $0$. This value is attained when $X=0$. The points $(1,0)$ and $(-1,0)$ are in this part of the domain.\n\nCombining the results from both cases, the infimum of $F(p,X)$ over the entire set $J^{2,-}u(0)$ is the minimum of the infima found for each part:\n$$ \\inf_{(p,X)\\in J^{2,-}u(0)} F(p,X) = \\min\\{ 0, 0 \\} = 0. $$\nThis value confirms that $u(x)=|x|$ is a viscosity supersolution of $F(Du,D^2u)=0$, and the inequality $F(p,X) \\ge 0$ is sharp, as its infimum is exactly $0$.", "answer": "$$\\boxed{0}$$", "id": "3005560"}, {"introduction": "The form of a Hamilton-Jacobi-Bellman (HJB) equation is not arbitrary; it is a direct reflection of the underlying stochastic dynamics it describes. This exercise explores the profound connection between the diffusion term in a controlled SDE and the second-order part of the HJB operator [@problem_id:3005553]. By analyzing cases where the diffusion is degenerate—meaning noise is absent in certain directions—you will gain a concrete understanding of why the HJB equation may be second-order in some variables but only first-order in others.", "problem": "Consider the controlled Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X_{t} = b(X_{t},a_{t})\\,\\mathrm{d}t + \\sigma(X_{t})\\,\\mathrm{d}W_{t},\n$$\nwhere $X_{t} \\in \\mathbb{R}^{n}$ is the state, $a_{t} \\in A$ is the control process taking values in a compact metric set $A$, $b:\\mathbb{R}^{n}\\times A \\to \\mathbb{R}^{n}$ is a drift, and $\\sigma:\\mathbb{R}^{n} \\to \\mathbb{R}^{n \\times m}$ is a diffusion matrix, with $W_{t}$ an $m$-dimensional Brownian Motion (BM). Assume $b$ and $\\sigma$ are bounded and uniformly continuous, and the standard growth and Lipschitz conditions that guarantee existence and uniqueness of strong solutions. Let $u:\\mathbb{R}^{n} \\to \\mathbb{R}$ be the value function for a stationary infinite-horizon discounted control problem with discount $\\lambda0$ and running cost $L:\\mathbb{R}^{n}\\times A \\to \\mathbb{R}$, and suppose $u$ is a viscosity solution to the stationary Hamilton–Jacobi–Bellman (HJB) partial differential equation (PDE) associated with the above SDE and control.\n\nStarting from the Dynamic Programming Principle (DPP) and Itô’s formula, derive the second-order term appearing in the HJB operator and use this derivation to study the structural consequences when the diffusion covariance matrix $\\sigma(x)\\sigma(x)^{\\top}$ is rank-deficient. Focus on the second-order term at a fixed point $x \\in \\mathbb{R}^{n}$ for a smooth test function $\\varphi$ touching $u$ at $x$, with Hessian $D^{2}\\varphi(x)=M \\in \\mathbb{S}^{n}$ (the space of symmetric $n\\times n$ matrices).\n\nYou are given two concrete, constant diffusion matrices that are rank-deficient:\n\n- Example 1 (three-dimensional state, two-dimensional noise): \n$$\n\\sigma_{(1)} \\in \\mathbb{R}^{3\\times 2}, \\quad \\sigma_{(1)} = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  1\n\\end{pmatrix}.\n$$\n\n- Example 2 (two-dimensional state, one-dimensional noise): \n$$\n\\sigma_{(2)} \\in \\mathbb{R}^{2\\times 1}, \\quad \\sigma_{(2)} = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\n\nFor each example, determine the explicit analytic form of the diffusion contribution to the HJB operator at $x$ acting on the Hessian $M=D^{2}\\varphi(x)$, namely the quantity\n$$\n\\frac{1}{2}\\,\\mathrm{Tr}\\!\\left(\\sigma_{(i)}\\sigma_{(i)}^{\\top} \\, M\\right),\n$$\nexpressed directly in terms of the entries of $M$, where $i\\in\\{1,2\\}$ and $M \\in \\mathbb{S}^{n}$ has entries $M_{jk}$ in the corresponding state dimension.\n\nThen, using only fundamental definitions and well-tested facts (Itô’s formula, the Dynamic Programming Principle, and the definition of viscosity solutions), deduce the structural property of the HJB operator in the directions belonging to the kernel of $\\sigma_{(i)}(x)^{\\top}$ for each example; namely, explain what happens to the second-order dependence of the operator along those directions and how this reflects degenerate ellipticity.\n\nYour final answer must be the pair of expressions for the diffusion contribution, one for each example, arranged as a single row matrix using the LaTeX pmatrix environment. No rounding is required. No units are required.", "solution": "We begin by providing a heuristic derivation of the stationary HJB equation from the Dynamic Programming Principle (DPP) to identify the second-order term. Let $u:\\mathbb{R}^{n} \\to \\mathbb{R}$ be the value function for the infinite-horizon discounted cost problem. The DPP states that for any time $t > 0$ and any admissible control process $\\{a_s\\}_{s \\ge 0}$,\n$$\nu(x) = \\inf_{a_{\\cdot}} \\mathbb{E}\\left[ \\int_{0}^{t} e^{-\\lambda s} L(X_s, a_s) \\mathrm{d}s + e^{-\\lambda t} u(X_t) \\bigg| X_0=x \\right].\n$$\nThis implies that for a small time interval $h > 0$ and a constant control $a \\in A$,\n$$\nu(x) \\le \\mathbb{E}\\left[ \\int_{0}^{h} e^{-\\lambda s} L(X_s, a) \\mathrm{d}s + e^{-\\lambda h} u(X_h) \\bigg| X_0=x \\right].\n$$\nAssuming sufficient smoothness of the functions involved, we approximate the integral as $\\int_{0}^{h} e^{-\\lambda s} L(X_s, a) \\mathrm{d}s \\approx L(x,a)h$. To analyze the term $e^{-\\lambda h} u(X_h)$, we apply Itô's formula to the process $f(t,X_t) = e^{-\\lambda t} u(X_t)$. The stochastic differential is:\n$$\n\\mathrm{d}(e^{-\\lambda t} u(X_t)) = \\frac{\\partial}{\\partial t}(e^{-\\lambda t} u(X_t)) \\mathrm{d}t + e^{-\\lambda t} Du(X_t)^{\\top} \\mathrm{d}X_t + \\frac{1}{2} e^{-\\lambda t} \\mathrm{Tr}\\left(D^2u(X_t) (\\mathrm{d}X_t)(\\mathrm{d}X_t)^{\\top}\\right).\n$$\nThe partial derivative with respect to time is $-\\lambda e^{-\\lambda t} u(X_t)$. The quadratic covariation term is $(\\mathrm{d}X_t)(\\mathrm{d}X_t)^{\\top} = \\sigma(X_t)\\sigma(X_t)^{\\top}\\mathrm{d}t$. Substituting the SDE for $\\mathrm{d}X_t$ yields:\n$$\n\\mathrm{d}(e^{-\\lambda t} u(X_t)) = e^{-\\lambda t} \\left[ -\\lambda u(X_t) + Du(X_t)^{\\top}b(X_t,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(X_t)\\sigma(X_t)^{\\top}D^2u(X_t)\\right) \\right]\\mathrm{d}t + e^{-\\lambda t}Du(X_t)^{\\top}\\sigma(X_t)\\mathrm{d}W_t.\n$$\nIntegrating from $0$ to $h$ and taking the expectation conditional on $X_0=x$ eliminates the martingale term:\n$$\n\\mathbb{E}[e^{-\\lambda h} u(X_h)] - u(x) \\approx \\left[ -\\lambda u(x) + Du(x)^{\\top}b(x,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right) \\right]h.\n$$\nSubstituting this and the integral approximation back into the DPP inequality gives:\n$$\nu(x) \\le L(x,a)h + u(x) + \\left[ -\\lambda u(x) + Du(x)^{\\top}b(x,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right) \\right]h + o(h).\n$$\nRearranging, dividing by $h > 0$, and taking the limit as $h \\to 0$ gives:\n$$\n0 \\le L(x,a) - \\lambda u(x) + Du(x)^{\\top}b(x,a) + \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right).\n$$\nSince this holds for any control $a \\in A$, and with equality for the optimal control, we take the infimum over $a$, leading to the stationary HJB equation:\n$$\n\\lambda u(x) - \\sup_{a \\in A}\\left\\{ -L(x,a) - b(x,a)^{\\top}Du(x) \\right\\} - \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2u(x)\\right) = 0.\n$$\nThe value function $u$ is generally not twice differentiable. The robust formulation is via viscosity solutions, where $u$ is compared to smooth test functions $\\varphi$. If $u-\\varphi$ has a local maximum at $x$ ($u$ is touched from above by $\\varphi$), we have $Du(x)=D\\varphi(x)$ and $D^2u(x) \\le D^2\\varphi(x)$ in the matrix sense. The viscosity subsolution definition requires:\n$$\n\\lambda\\varphi(x) - \\sup_{a \\in A}\\left\\{ -L(x,a) - b(x,a)^{\\top}D\\varphi(x) \\right\\} - \\frac{1}{2}\\mathrm{Tr}\\left(\\sigma(x)\\sigma(x)^{\\top}D^2\\varphi(x)\\right) \\le 0.\n$$\nAn analogous inequality holds for supersolutions. The term of interest is the diffusion contribution to the HJB operator, which acts on the Hessian $M=D^2\\varphi(x)$ of the test function. This term is $\\frac{1}{2}\\mathrm{Tr}(\\sigma(x)\\sigma(x)^{\\top}M)$.\n\nWe now analyze this term for the two given examples.\n\nExample 1: The state dimension is $n=3$ and the noise dimension is $m=2$. The diffusion matrix is\n$$\n\\sigma_{(1)} = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  1\n\\end{pmatrix}.\n$$\nThe covariance matrix is $S_{(1)} = \\sigma_{(1)}\\sigma_{(1)}^{\\top}$:\n$$\nS_{(1)} = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0  0 \\\\\n0  1  1\n\\end{pmatrix} = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  1 \\\\\n0  1  1\n\\end{pmatrix}.\n$$\nLet $M \\in \\mathbb{S}^3$ be a symmetric $3 \\times 3$ matrix with entries $M_{jk}$. The diffusion contribution is calculated using the property $\\mathrm{Tr}(AB) = \\sum_{j,k} A_{jk} B_{kj}$. Since $M$ is symmetric ($M_{kj}=M_{jk}$), this becomes $\\sum_{j,k} (S_{(1)})_{jk} M_{jk}$.\n$$\n\\mathrm{Tr}(S_{(1)}M) = (S_{(1)})_{11}M_{11} + (S_{(1)})_{22}M_{22} + (S_{(1)})_{33}M_{33} + (S_{(1)})_{23}M_{23} + (S_{(1)})_{32}M_{32}.\n$$\nSubstituting the values from $S_{(1)}$ and using $M_{23}=M_{32}$:\n$$\n\\mathrm{Tr}(S_{(1)}M) = 1 \\cdot M_{11} + 1 \\cdot M_{22} + 1 \\cdot M_{33} + 1 \\cdot M_{23} + 1 \\cdot M_{32} = M_{11} + M_{22} + M_{33} + 2M_{23}.\n$$\nThe diffusion contribution is therefore:\n$$\n\\frac{1}{2}\\mathrm{Tr}\\left(\\sigma_{(1)}\\sigma_{(1)}^{\\top} M\\right) = \\frac{1}{2}(M_{11} + M_{22} + 2M_{23} + M_{33}).\n$$\n\nExample 2: The state dimension is $n=2$ and the noise dimension is $m=1$. The diffusion matrix is\n$$\n\\sigma_{(2)} = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nThe covariance matrix is $S_{(2)} = \\sigma_{(2)}\\sigma_{(2)}^{\\top}$:\n$$\nS_{(2)} = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0\n\\end{pmatrix} = \\begin{pmatrix}\n1  0 \\\\\n0  0\n\\end{pmatrix}.\n$$\nLet $M \\in \\mathbb{S}^2$ be a symmetric $2 \\times 2$ matrix with entries $M_{jk}$.\n$$\n\\mathrm{Tr}(S_{(2)}M) = (S_{(2)})_{11}M_{11} + (S_{(2)})_{22}M_{22} + (S_{(2)})_{12}M_{12} + (S_{(2)})_{21}M_{21} = 1 \\cdot M_{11} + 0 \\cdot M_{22} + 0 + 0 = M_{11}.\n$$\nThe diffusion contribution is:\n$$\n\\frac{1}{2}\\mathrm{Tr}\\left(\\sigma_{(2)}\\sigma_{(2)}^{\\top} M\\right) = \\frac{1}{2} M_{11}.\n$$\n\nNow we study the structural consequences. The ellipticity of the HJB equation is determined by the matrix $\\sigma\\sigma^{\\top}$. If this matrix is positive definite, the equation is uniformly elliptic. If it is singular (rank-deficient), the equation is termed degenerate elliptic. This singularity implies that the diffusion acts only in a subspace of the state space. The kernel of $\\sigma^{\\top}$ comprises the directions in which there is no diffusion.\n\nFor Example 1, $\\mathrm{rank}(S_{(1)}) = 2  3$, so it is singular. We find the kernel of $\\sigma_{(1)}^{\\top}$:\n$$\n\\sigma_{(1)}^{\\top}v = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  1\n\\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies\n\\begin{cases}\nv_1 = 0 \\\\\nv_2 + v_3 = 0\n\\end{cases}.\n$$\nThe kernel is spanned by the vector $v_{\\ker} = (0, 1, -1)^{\\top}$. The second-order term of the HJB operator is insensitive to the curvature of the test function $\\varphi$ in this direction. To see this, consider a Hessian representing curvature only in this direction, $M = v_{\\ker}v_{\\ker}^{\\top}$:\n$$\nM = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 0  1  -1 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 0  1  -1 \\\\ 0  -1  1 \\end{pmatrix}.\n$$\nHere, $M_{11}=0, M_{22}=1, M_{33}=1, M_{23}=-1$. Plugging this into our derived form for the diffusion contribution:\n$$\n\\frac{1}{2}(M_{11} + M_{22} + 2M_{23} + M_{33}) = \\frac{1}{2}(0 + 1 + 2(-1) + 1) = 0.\n$$\nThis explicitly shows that the HJB equation has no second derivative terms associated with the direction $(0, 1, -1)^{\\top}$. The PDE is degenerate; it behaves like a first-order transport equation in this direction, while being second-order in the orthogonal subspace spanned by the columns of $\\sigma_{(1)}$, i.e., $(1,0,0)^{\\top}$ and $(0,1,1)^{\\top}$.\n\nFor Example 2, $S_{(2)}$ is also singular, with $\\mathrm{rank}(S_{(2)})=1  2$. The kernel of $\\sigma_{(2)}^{\\top}$ is found by:\n$$\n\\sigma_{(2)}^{\\top}v = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = 0 \\implies v_1 = 0.\n$$\nThe kernel is the $x_2$-axis, spanned by $v_{\\ker} = (0, 1)^{\\top}$. Our derived diffusion contribution, $\\frac{1}{2}M_{11}$, which is $\\frac{1}{2}\\frac{\\partial^2\\varphi}{\\partial x_1^2}$, explicitly demonstrates the structural consequence. The term is independent of $M_{22} = \\frac{\\partial^2\\varphi}{\\partial x_2^2}$ and $M_{12} = \\frac{\\partial^2\\varphi}{\\partial x_1 \\partial x_2}$. The HJB equation is second-order in the $x_1$ variable but only first-order in the $x_2$ variable. This reflects that the underlying SDE has noise only in the first component, leaving the second component's evolution to be driven solely by the drift, which is a first-order effect. This is a classic case of degenerate ellipticity.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}(M_{11} + M_{22} + 2M_{23} + M_{33})  \\frac{1}{2}M_{11}\n\\end{pmatrix}\n}\n$$", "id": "3005553"}, {"introduction": "The comparison principle is the cornerstone of viscosity solution theory, guaranteeing the uniqueness of solutions and their continuous dependence on problem data. Its proof is a masterclass in analysis, famously employing a \"doubling of variables\" argument powered by the Crandall-Ishii lemma. This exercise takes you into the engine room of that proof, focusing on the key algebraic step involving the penalization function [@problem_id:3005566]. Mastering this calculation is essential for a deep understanding of why comparison holds and for tackling advanced problems in the field.", "problem": "Let $u \\colon \\mathbb{R}^{n} \\to \\mathbb{R}$ be a bounded upper semicontinuous viscosity subsolution and $v \\colon \\mathbb{R}^{n} \\to \\mathbb{R}$ be a bounded lower semicontinuous viscosity supersolution of a fully nonlinear Hamilton-Jacobi-Bellman (HJB) equation arising from a controlled Itô diffusion, written abstractly as\n$$F\\big(x, u(x), Du(x), D^{2}u(x)\\big) = 0 \\quad \\text{in } \\mathbb{R}^{n},$$\nwhere $F$ is degenerate elliptic and continuous in all its arguments. In the standard doubling-of-variables comparison proof, consider the penalization\n$$\\psi_{\\varepsilon}(x,y) = \\frac{1}{2\\varepsilon}\\,|x-y|^{2}, \\qquad \\varepsilon  0,$$\nand define\n$$\\Phi_{\\varepsilon}(x,y) = u(x) - v(y) - \\psi_{\\varepsilon}(x,y).$$\nAssume that $\\Phi_{\\varepsilon}$ attains its global maximum at $(x_{\\varepsilon}, y_{\\varepsilon}) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{n}$. By applying Ishii’s lemma (also known as the Crandall–Ishii lemma), one obtains symmetric matrices $X, Y \\in \\mathbb{S}^{n}$ such that $(D_{x}\\psi_{\\varepsilon}(x_{\\varepsilon},y_{\\varepsilon}), X)$ lies in the second-order superjet of $u$ at $x_{\\varepsilon}$ and $(D_{y}\\psi_{\\varepsilon}(x_{\\varepsilon},y_{\\varepsilon}), Y)$ lies in the second-order subjet of $v$ at $y_{\\varepsilon}$, and a block-matrix inequality bounding\n$$\\begin{pmatrix} X  0 \\\\[4pt] 0  -Y \\end{pmatrix}$$\nin terms of the Hessian $D^{2}\\psi_{\\varepsilon}(x_{\\varepsilon},y_{\\varepsilon})$ and its square, with an arbitrary parameter $\\alpha  0$ arising from the lemma.\n\nStarting from the fundamental definitions of viscosity sub/supersolutions and the well-tested Crandall–Ishii lemma, derive the explicit form of $D^{2}\\psi_{\\varepsilon}(x_{\\varepsilon},y_{\\varepsilon})$ and its square, and compute the smallest scalar coefficient $C(\\varepsilon,\\alpha)$ such that the upper bound from Ishii’s lemma can be written in the form\n$$\\begin{pmatrix} X  0 \\\\[4pt] 0  -Y \\end{pmatrix} \\;\\le\\; C(\\varepsilon,\\alpha)\\, \\begin{pmatrix} I  -I \\\\[4pt] -I  I \\end{pmatrix},$$\nwhere $I$ denotes the $n \\times n$ identity matrix and the inequality is in the sense of symmetric matrices. Provide your final expression for $C(\\varepsilon,\\alpha)$ in closed form. No rounding is required.", "solution": "The problem requires us to determine a specific coefficient, denoted as $C(\\varepsilon, \\alpha)$, that appears in a matrix inequality derived from the Crandall-Ishii lemma in the context of viscosity solutions. The core of the task is an algebraic manipulation of the Hessian matrix of the penalization function $\\psi_{\\varepsilon}(x,y)$.\n\nFirst, we must compute the Hessian matrix of the penalization function $\\psi_{\\varepsilon}(x,y)$. The function is defined as:\n$$ \\psi_{\\varepsilon}(x,y) = \\frac{1}{2\\varepsilon} |x-y|^{2} $$\nwhere $x, y \\in \\mathbb{R}^{n}$ and $\\varepsilon > 0$. We can write this in terms of the components of $x = (x_1, \\dots, x_n)$ and $y = (y_1, \\dots, y_n)$:\n$$ \\psi_{\\varepsilon}(x,y) = \\frac{1}{2\\varepsilon} \\sum_{i=1}^{n} (x_i - y_i)^2 $$\nTo find the Hessian, we need to compute all second-order partial derivatives with respect to the components of $x$ and $y$. The Hessian, denoted by $D^2 \\psi_{\\varepsilon}$, is a $2n \\times 2n$ block matrix:\n$$ D^2 \\psi_{\\varepsilon}(x,y) = \\begin{pmatrix} D_{xx}^{2}\\psi_{\\varepsilon}(x,y)  D_{xy}^{2}\\psi_{\\varepsilon}(x,y) \\\\ D_{yx}^{2}\\psi_{\\varepsilon}(x,y)  D_{yy}^{2}\\psi_{\\varepsilon}(x,y) \\end{pmatrix} $$\nLet's compute the entries of the blocks. For any $j, k \\in \\{1, \\dots, n\\}$:\nThe first partial derivatives with respect to $x_k$ and $y_k$ are:\n$$ \\frac{\\partial \\psi_{\\varepsilon}}{\\partial x_k} = \\frac{1}{\\varepsilon}(x_k - y_k) $$\n$$ \\frac{\\partial \\psi_{\\varepsilon}}{\\partial y_k} = -\\frac{1}{\\varepsilon}(x_k - y_k) $$\nNow, we compute the second-order partial derivatives:\n$$ \\frac{\\partial^2 \\psi_{\\varepsilon}}{\\partial x_j \\partial x_k} = \\frac{1}{\\varepsilon}\\delta_{jk} $$\n$$ \\frac{\\partial^2 \\psi_{\\varepsilon}}{\\partial y_j \\partial y_k} = \\frac{1}{\\varepsilon}\\delta_{jk} $$\n$$ \\frac{\\partial^2 \\psi_{\\varepsilon}}{\\partial y_j \\partial x_k} = -\\frac{1}{\\varepsilon}\\delta_{jk} $$\nHere, $\\delta_{jk}$ is the Kronecker delta. These components form the blocks of the Hessian. The block $D_{xx}^{2}\\psi_{\\varepsilon}$ has entries $(\\frac{1}{\\varepsilon}\\delta_{jk})$, which corresponds to the matrix $\\frac{1}{\\varepsilon}I$, where $I$ is the $n \\times n$ identity matrix. Similarly, $D_{yy}^{2}\\psi_{\\varepsilon} = \\frac{1}{\\varepsilon}I$, and the mixed-partial block is $D_{xy}^{2}\\psi_{\\varepsilon} = D_{yx}^{2}\\psi_{\\varepsilon} = -\\frac{1}{\\varepsilon}I$.\nThus, the explicit form of the Hessian matrix $D^{2}\\psi_{\\varepsilon}(x_{\\varepsilon},y_{\\varepsilon})$ is independent of the point $(x_{\\varepsilon}, y_{\\varepsilon})$ and is given by:\n$$ D^{2}\\psi_{\\varepsilon} = \\frac{1}{\\varepsilon} \\begin{pmatrix} I  -I \\\\ -I  I \\end{pmatrix} $$\nFor notational convenience, let us define the matrix $J = \\begin{pmatrix} I  -I \\\\ -I  I \\end{pmatrix}$. Then, $D^{2}\\psi_{\\varepsilon} = \\frac{1}{\\varepsilon}J$.\n\nNext, as required by the problem, we compute the square of the Hessian matrix:\n$$ (D^{2}\\psi_{\\varepsilon})^2 = \\left(\\frac{1}{\\varepsilon}J\\right)^2 = \\frac{1}{\\varepsilon^2}J^2 $$\nWe calculate $J^2$:\n$$ J^2 = \\begin{pmatrix} I  -I \\\\ -I  I \\end{pmatrix} \\begin{pmatrix} I  -I \\\\ -I  I \\end{pmatrix} = \\begin{pmatrix} I \\cdot I + (-I) \\cdot (-I)  I \\cdot (-I) + (-I) \\cdot I \\\\ (-I) \\cdot I + I \\cdot (-I)  (-I) \\cdot (-I) + I \\cdot I \\end{pmatrix} $$\n$$ J^2 = \\begin{pmatrix} I + I  -I - I \\\\ -I - I  I + I \\end{pmatrix} = \\begin{pmatrix} 2I  -2I \\\\ -2I  2I \\end{pmatrix} = 2 \\begin{pmatrix} I  -I \\\\ -I  I \\end{pmatrix} = 2J $$\nTherefore, the square of the Hessian is:\n$$ (D^{2}\\psi_{\\varepsilon})^2 = \\frac{2}{\\varepsilon^2}J = \\frac{2}{\\varepsilon^2} \\begin{pmatrix} I  -I \\\\ -I  I \\end{pmatrix} $$\nThe problem states that the upper bound from Ishii’s lemma is formulated in terms of the Hessian and its square, with a parameter $\\alpha > 0$. A standard version of the lemma (Theorem on sums) provides the following matrix as the upper bound for the block matrix $\\begin{pmatrix} X  0 \\\\ 0  -Y \\end{pmatrix}$:\n$$ D^{2}\\psi_{\\varepsilon} + \\alpha (D^{2}\\psi_{\\varepsilon})^2 $$\nSubstituting the expressions we derived:\n$$ D^{2}\\psi_{\\varepsilon} + \\alpha (D^{2}\\psi_{\\varepsilon})^2 = \\frac{1}{\\varepsilon}J + \\alpha \\left(\\frac{2}{\\varepsilon^2}J\\right) $$\nWe can factor out the matrix $J$:\n$$ D^{2}\\psi_{\\varepsilon} + \\alpha (D^{2}\\psi_{\\varepsilon})^2 = \\left(\\frac{1}{\\varepsilon} + \\frac{2\\alpha}{\\varepsilon^2}\\right) J $$\nThe problem asks for the smallest scalar coefficient $C(\\varepsilon,\\alpha)$ such that the upper bound can be written in the form $C(\\varepsilon,\\alpha) \\begin{pmatrix} I  -I \\\\ -I  I \\end{pmatrix}$, which is $C(\\varepsilon,\\alpha)J$.\nBy comparing the derived upper bound with the target form, we must have:\n$$ \\left(\\frac{1}{\\varepsilon} + \\frac{2\\alpha}{\\varepsilon^2}\\right) J = C(\\varepsilon,\\alpha) J $$\nThis equality directly yields the expression for $C(\\varepsilon,\\alpha)$:\n$$ C(\\varepsilon,\\alpha) = \\frac{1}{\\varepsilon} + \\frac{2\\alpha}{\\varepsilon^2} $$\nThis expression can be combined into a single fraction:\n$$ C(\\varepsilon,\\alpha) = \\frac{\\varepsilon + 2\\alpha}{\\varepsilon^2} $$\nThis is the smallest such coefficient because it provides equality. Any smaller value would violate the inequality in the sense of symmetric matrices, as the matrix $J$ is positive semidefinite.", "answer": "$$\\boxed{\\frac{\\varepsilon + 2\\alpha}{\\varepsilon^2}}$$", "id": "3005566"}]}