{"hands_on_practices": [{"introduction": "Understanding the fundamental statistical properties of the solution to the stochastic heat equation is a crucial first step in its analysis. This practice focuses on computing the second moment, or variance, of the solution at a specific point in space and time. By applying the Itô isometry for stochastic integrals with respect to space-time white noise, you will derive how the solution's variance grows over time, providing direct insight into the diffusive and stochastic nature of the process [@problem_id:3003014].", "problem": "Let $G(t,x)$ denote the one-dimensional heat kernel on $\\mathbb{R}$, defined for $t>0$ by $G(t,x)=(4\\pi t)^{-1/2}\\exp\\!\\big(-x^{2}/(4t)\\big)$. Let $W$ be a space-time white noise on $\\mathbb{R}_{+}\\times\\mathbb{R}$, modeled as an isonormal Gaussian family over $L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$ so that, for deterministic $\\varphi\\in L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$, the Walsh integral $\\int_{0}^{\\infty}\\int_{\\mathbb{R}}\\varphi(s,y)\\,W(\\mathrm{d}s,\\mathrm{d}y)$ is a centered Gaussian random variable with variance determined by the $L^{2}$ norm of $\\varphi$. Consider the additive-noise stochastic heat equation on $\\mathbb{R}$ with zero initial condition,\n$$\n\\partial_{t}u(t,x)=\\partial_{xx}u(t,x)+\\sigma\\,\\dot{W}(t,x),\\qquad u(0,x)=0,\n$$\nwhere $\\sigma\\in\\mathbb{R}$ is a constant and $\\dot{W}$ is the formal derivative of $W$. The mild solution is given by the stochastic convolution\n$$\nu(t,x)=\\sigma\\int_{0}^{t}\\int_{\\mathbb{R}}G(t-s,x-y)\\,W(\\mathrm{d}s,\\mathrm{d}y).\n$$\nStarting from the definition of the Walsh integral as an isometry on $L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$ and the semigroup property of the heat kernel, derive the Itô isometry for the stochastic convolution with the heat kernel in this setting, and use it to compute the closed-form expression of $\\mathbb{E}\\big[|u(t,x)|^{2}\\big]$ as a function of $t$ and $\\sigma$. Provide the exact expression; no rounding is required. Express your final answer as a single analytic expression.", "solution": "The problem asks for the derivation of the second moment of the mild solution to the one-dimensional stochastic heat equation with additive space-time white noise and zero initial conditions. The mild solution $u(t,x)$ for $t>0$ and $x\\in\\mathbb{R}$ is given by the stochastic convolution:\n$$\nu(t,x) = \\sigma \\int_{0}^{t}\\int_{\\mathbb{R}} G(t-s, x-y) \\,W(\\mathrm{d}s,\\mathrm{d}y)\n$$\nHere, $G(t,x)=(4\\pi t)^{-1/2}\\exp(-x^{2}/(4t))$ is the one-dimensional heat kernel, $\\sigma \\in \\mathbb{R}$ is a constant, and $W$ is a space-time white noise on $\\mathbb{R}_{+} \\times \\mathbb{R}$. The integral is a Walsh-Itô stochastic integral.\n\nThe Walsh integral $\\int_{0}^{\\infty}\\int_{\\mathbb{R}}\\varphi(s,y)\\,W(\\mathrm{d}s,\\mathrm{d}y)$ for a deterministic function $\\varphi \\in L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})$ is a centered Gaussian random variable. Its variance is given by the Itô isometry:\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{\\infty}\\int_{\\mathbb{R}}\\varphi(s,y)\\,W(\\mathrm{d}s,\\mathrm{d}y)\\right)^2\\right] = \\lVert\\varphi\\rVert_{L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})}^{2} = \\int_{0}^{\\infty}\\int_{\\mathbb{R}} |\\varphi(s,y)|^2 \\,\\mathrm{d}y\\,\\mathrm{d}s\n$$\nThe solution $u(t,x)$ is a specific instance of such an integral. We can identify the integrand as a function $\\varphi_{t,x}(s,y)$ defined by:\n$$\n\\varphi_{t,x}(s,y) = \\sigma \\, G(t-s, x-y) \\, \\mathbf{1}_{[0,t)}(s)\n$$\nwhere $\\mathbf{1}_{[0,t)}(s)$ is the indicator function for the time interval $[0,t)$. Since the integral defining $u(t,x)$ is a centered Gaussian random variable, its second moment $\\mathbb{E}[|u(t,x)|^{2}]$ is equal to its variance. We can compute this variance by applying the Itô isometry. This constitutes the application of the Itô isometry for the stochastic convolution with the heat kernel.\n\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\mathbb{E}\\big[u(t,x)^2\\big] = \\lVert\\varphi_{t,x}\\rVert_{L^{2}(\\mathbb{R}_{+}\\times\\mathbb{R})}^{2}\n$$\nSubstituting the expression for $\\varphi_{t,x}(s,y)$:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\int_{0}^{\\infty}\\int_{\\mathbb{R}} \\left| \\sigma \\, G(t-s, x-y) \\, \\mathbf{1}_{[0,t)}(s) \\right|^2 \\,\\mathrm{d}y\\,\\mathrm{d}s\n$$\nWe can simplify this expression. The constant $\\sigma$ can be factored out, and the indicator function restricts the time integral to the interval $[0,t)$.\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\sigma^2 \\int_{0}^{t}\\int_{\\mathbb{R}} G(t-s, x-y)^2 \\,\\mathrm{d}y\\,\\mathrm{d}s\n$$\nTo evaluate this double integral, we first focus on the inner integral with respect to $y$:\n$$\nI_{inner}(s) = \\int_{\\mathbb{R}} G(t-s, x-y)^2 \\,\\mathrm{d}y\n$$\nWe perform a change of variables $z = x-y$, so that $\\mathrm{d}z = -\\mathrm{d}y$. The limits of integration for $z$ are from $+\\infty$ to $-\\infty$, and the negative sign from the differential cancels the reversal of limits.\n$$\nI_{inner}(s) = \\int_{\\mathbb{R}} G(t-s, z)^2 \\,\\mathrm{d}z\n$$\nThe heat kernel is an even function of its spatial argument, i.e., $G(\\tau, z) = G(\\tau, -z)$. Therefore, we can write $G(t-s, z)^2 = G(t-s, 0-z)G(t-s, z-0)$. The integral becomes:\n$$\nI_{inner}(s) = \\int_{\\mathbb{R}} G(t-s, 0-z)G(t-s, z-0) \\,\\mathrm{d}z\n$$\nThis expression has the form of a convolution. We use the semigroup property of the heat kernel, which states that for $t_1, t_2 > 0$:\n$$\n\\int_{\\mathbb{R}} G(t_1, x-z) G(t_2, z-y) \\,\\mathrm{d}z = G(t_1+t_2, x-y)\n$$\nApplying this property with $t_1 = t_2 = t-s$ and $x=y=0$, we find:\n$$\nI_{inner}(s) = G((t-s)+(t-s), 0-0) = G(2(t-s), 0)\n$$\nNow we use the definition of the heat kernel $G(\\tau, \\xi) = (4\\pi \\tau)^{-1/2}\\exp(-\\xi^2/(4\\tau))$ with $\\tau = 2(t-s)$ and $\\xi=0$:\n$$\nI_{inner}(s) = G(2(t-s), 0) = \\left(4\\pi \\cdot 2(t-s)\\right)^{-1/2} \\exp(0) = (8\\pi(t-s))^{-1/2}\n$$\nThis result for the inner integral is independent of $x$, as expected from the spatial translation invariance of the setup. Now, we substitute this back into the expression for the variance:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\sigma^2 \\int_{0}^{t} (8\\pi(t-s))^{-1/2} \\,\\mathrm{d}s\n$$\nThis is a standard integral. We can pull the constant factors out:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\frac{\\sigma^2}{\\sqrt{8\\pi}} \\int_{0}^{t} (t-s)^{-1/2} \\,\\mathrm{d}s\n$$\nWe evaluate the integral using the substitution $v = t-s$, which gives $\\mathrm{d}v = -\\mathrm{d}s$. The limits of integration change from $s=0$ to $v=t$ and from $s=t$ to $v=0$.\n$$\n\\int_{0}^{t} (t-s)^{-1/2} \\,\\mathrm{d}s = \\int_{t}^{0} v^{-1/2} (-\\mathrm{d}v) = \\int_{0}^{t} v^{-1/2} \\,\\mathrm{d}v\n$$\nThis is an improper integral, but it converges.\n$$\n\\int_{0}^{t} v^{-1/2} \\,\\mathrm{d}v = \\left[ 2v^{1/2} \\right]_{0}^{t} = 2\\sqrt{t} - 0 = 2\\sqrt{t}\n$$\nFinally, we assemble all the parts:\n$$\n\\mathbb{E}\\big[|u(t,x)|^{2}\\big] = \\frac{\\sigma^2}{\\sqrt{8\\pi}} \\cdot 2\\sqrt{t} = \\frac{2\\sigma^2\\sqrt{t}}{2\\sqrt{2\\pi}} = \\sigma^2 \\sqrt{\\frac{t}{2\\pi}}\n$$\nThis is the closed-form expression for the second moment of the solution at a fixed point $(t,x)$.", "answer": "$$\\boxed{\\sigma^{2}\\sqrt{\\frac{t}{2\\pi}}}$$", "id": "3003014"}, {"introduction": "When the stochastic heat equation is considered on a bounded domain, the boundary conditions play a pivotal role in shaping the solution's behavior. This exercise explores the case of Neumann boundary conditions, which physically correspond to an insulated boundary with no heat flux. By decomposing the SPDE into its spectral modes, you will discover that the zero mode, corresponding to the spatial average of the solution, exhibits unique dynamics that differ fundamentally from the higher-frequency modes [@problem_id:3003016]. This analysis reveals how conserved quantities in deterministic systems transform into martingales in their stochastic counterparts.", "problem": "Consider the stochastic partial differential equation (SPDE) for the heat equation with homogeneous Neumann boundary conditions on a bounded, connected, smooth domain $\\Omega \\subset \\mathbb{R}^d$:\n$$\n\\partial_t u(t,x) \\;=\\; \\Delta u(t,x) \\;+\\; \\xi(t,x), \\qquad x \\in \\Omega, \\; t \\ge 0,\n$$\nwith the Neumann boundary condition\n$$\n\\partial_{\\mathbf{n}} u(t,x) \\;=\\; 0, \\qquad x \\in \\partial \\Omega, \\; t \\ge 0,\n$$\nand the initial condition $u(0,x) = u_0(x) \\in L^2(\\Omega)$. Here $\\partial_{\\mathbf{n}}$ denotes the outward normal derivative, and $\\xi$ is an additive, centered Gaussian noise that is white in time with spatial covariance operator $Q$ acting on $L^2(\\Omega)$. Assume $Q$ is self-adjoint, nonnegative, and trace class, and that $\\xi$ can be realized as a $Q$-Wiener process in $L^2(\\Omega)$ (that is, a Gaussian process with covariance $Q$ and independent increments in time).\n\nLet $\\{\\phi_k\\}_{k \\ge 0}$ be an orthonormal basis of $L^2(\\Omega)$ consisting of eigenfunctions of the Neumann Laplacian, with corresponding eigenvalues $\\{\\lambda_k\\}_{k \\ge 0}$, so that\n$$\n-\\Delta \\phi_k \\;=\\; \\lambda_k \\, \\phi_k, \\qquad \\partial_{\\mathbf{n}} \\phi_k \\;=\\; 0 \\text{ on } \\partial \\Omega,\n$$\nwhere $\\lambda_0 = 0$ and $\\phi_0$ is the constant eigenfunction normalized by $\\|\\phi_0\\|_{L^2(\\Omega)} = 1$, and $\\lambda_k > 0$ for all $k \\ge 1$. Denote the spatial average of $u$ by\n$$\n\\bar{u}(t) \\;=\\; \\frac{1}{|\\Omega|} \\int_{\\Omega} u(t,x) \\, dx.\n$$\nThe zero mode coefficient is defined as $a_0(t) = \\langle u(t,\\cdot), \\phi_0 \\rangle_{L^2(\\Omega)}$.\n\nUsing only the eigenfunction expansion theory for the Neumann Laplacian, the definition of the $Q$-Wiener process, and Itô’s calculus for stochastic integrals in Hilbert spaces, analyze the dynamics of the zero mode coefficient $a_0(t)$ and the behavior of the spatial average $\\bar{u}(t)$, under additive noise. In particular, address whether the average can be non-decaying.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. The zero mode coefficient $a_0(t)$ satisfies a stochastic differential equation of the form $d a_0(t) = \\sigma_0 \\, d \\beta(t)$, where $\\beta$ is a scalar Brownian motion arising from projecting the noise onto $\\phi_0$ and $\\sigma_0 \\ge 0$ depends on $Q$. Consequently, $\\bar{u}(t)$ is a martingale with variance growing linearly in $t$, and it does not decay.\n\nB. The Neumann boundary condition enforces exponential decay of the zero mode, so $a_0(t)$ decays to $0$ in expectation even in the presence of additive noise.\n\nC. If the covariance operator annihilates constants, i.e., $Q \\phi_0 = 0$, then the zero mode is unaffected by the noise and $\\bar{u}(t)$ remains equal to its initial value for all $t \\ge 0$.\n\nD. For any additive noise white in time, the spatial average $\\bar{u}(t)$ obeys a deterministic ordinary differential equation with exponential decay governed by the first positive eigenvalue of the Neumann Laplacian, hence $\\bar{u}(t)$ decays to $0$.\n\nE. The zero mode under additive noise behaves as an Ornstein–Uhlenbeck process with linear drift $- \\lambda_0 a_0(t)$ and diffusion coefficient determined by $Q$, so it has a stationary Gaussian distribution as $t \\to \\infty$.", "solution": "We analyze the SPDE using the spectral method. We expand the solution $u(t,x)$ and the $Q$-Wiener process $W(t,x)$ (whose formal time derivative is $\\xi$) in the orthonormal basis of eigenfunctions $\\{\\phi_k\\}_{k \\ge 0}$:\n$$\nu(t,x) = \\sum_{k=0}^{\\infty} a_k(t) \\phi_k(x), \\quad \\text{where } a_k(t) = \\langle u(t,\\cdot), \\phi_k \\rangle_{L^2(\\Omega)}\n$$\n$$\nW(t,x) = \\sum_{k=0}^{\\infty} \\beta_k^Q(t) \\phi_k(x), \\quad \\text{where } d\\beta_k^Q(t) = \\langle dW(t,\\cdot), \\phi_k \\rangle_{L^2(\\Omega)}\n$$\nThe processes $\\beta_k^Q(t)$ are real-valued Brownian motions with covariance $\\mathbb{E}[\\beta_j^Q(t) \\beta_k^Q(s)] = \\min(t,s) \\langle Q\\phi_j, \\phi_k \\rangle_{L^2(\\Omega)}$.\n\nSubstituting the expansion of $u$ into the SPDE, $\\partial_t u = \\Delta u + \\xi$, and taking the inner product with an eigenfunction $\\phi_k$, we obtain a system of SDEs for the coefficients $a_k(t)$.\n$$\nd a_k(t) = \\langle \\Delta u, \\phi_k \\rangle_{L^2} dt + d\\beta_k^Q(t)\n$$\nUsing the self-adjointness of the Laplacian $\\Delta$ and the eigenfunction property $-\\Delta\\phi_k = \\lambda_k \\phi_k$:\n$$\n\\langle \\Delta u, \\phi_k \\rangle = \\langle u, \\Delta \\phi_k \\rangle = \\langle u, -\\lambda_k \\phi_k \\rangle = -\\lambda_k \\langle u, \\phi_k \\rangle = -\\lambda_k a_k(t)\n$$\nThus, the SDE for the $k$-th mode coefficient is:\n$$\nd a_k(t) = -\\lambda_k a_k(t) dt + d\\beta_k^Q(t)\n$$\nFor the zero mode, we have $k=0$ and the eigenvalue is $\\lambda_0 = 0$. The SDE becomes:\n$$\nd a_0(t) = -(0) a_0(t) dt + d\\beta_0^Q(t) \\implies d a_0(t) = d\\beta_0^Q(t)\n$$\nHere, $\\beta_0^Q(t)$ is a real-valued Brownian motion with variance parameter $\\sigma_0^2 = \\langle Q\\phi_0, \\phi_0 \\rangle \\ge 0$. So we can write $d a_0(t) = \\sigma_0 d\\beta(t)$, where $\\beta(t)$ is a standard Brownian motion. The solution is $a_0(t) = a_0(0) + \\sigma_0 \\beta(t)$. This is a martingale.\n\nThe spatial average $\\bar{u}(t)$ is related to $a_0(t)$ via the constant eigenfunction $\\phi_0(x) = |\\Omega|^{-1/2}$:\n$$\na_0(t) = \\int_{\\Omega} u(t,x) \\phi_0(x) \\, dx = \\int_{\\Omega} u(t,x) \\frac{1}{\\sqrt{|\\Omega|}} \\, dx = \\frac{|\\Omega|}{\\sqrt{|\\Omega|}} \\bar{u}(t) = \\sqrt{|\\Omega|} \\, \\bar{u}(t)\n$$\nSince $\\bar{u}(t)$ is proportional to the martingale $a_0(t)$, it is also a martingale. Its variance, $\\text{Var}(\\bar{u}(t)) = \\frac{\\sigma_0^2}{|\\Omega|} t$, grows linearly in time (unless $\\sigma_0=0$) and it does not decay. This confirms statement A.\n\nStatement B is incorrect. The Neumann condition leads to $\\lambda_0=0$, which removes the drift and prevents decay.\n\nFor statement C, if $Q\\phi_0 = 0$, then the variance parameter $\\sigma_0^2 = \\langle Q\\phi_0, \\phi_0 \\rangle = \\langle 0, \\phi_0 \\rangle = 0$. The SDE for the zero mode becomes $d a_0(t) = 0$, so $a_0(t)$ is constant. Consequently, $\\bar{u}(t)$ is also constant. Statement C is correct.\n\nStatement D is incorrect. The dynamics are stochastic, not deterministic, and there is no decay.\n\nStatement E is incorrect. An Ornstein-Uhlenbeck process requires a non-zero mean-reverting drift term ($\\lambda_k > 0$). For the zero mode, the drift is zero, so it is a degenerate OU process, i.e., a martingale, which does not have a stationary distribution.\n\nTherefore, the correct statements are A and C.", "answer": "$$\\boxed{AC}$$", "id": "3003016"}, {"introduction": "Bridging the gap between theoretical analysis and practical application is a key skill in computational science. This comprehensive practice guides you through the process of implementing the spectral Galerkin method, a standard numerical technique for solving the stochastic heat equation. You will first derive the theoretical convergence rate of this numerical approximation and then verify your analytical predictions through a concrete implementation [@problem_id:3003074]. This exercise highlights the synergy between rigorous mathematical analysis and numerical simulation in studying SPDEs.", "problem": "Consider the stochastic heat equation on a bounded domain given by the interval $D = (0,1)$ with homogeneous Dirichlet boundary conditions. Let $H = L^2(D)$, and let $A$ denote the Dirichlet Laplacian on $H$ defined by $A = \\frac{d^2}{dx^2}$ with domain $D(A) = H^2(D) \\cap H_0^1(D)$. The operator $A$ is self-adjoint, negative definite, and has a complete orthonormal basis of eigenfunctions $\\{e_k\\}_{k \\ge 1}$ with corresponding eigenvalues $\\{\\lambda_k\\}_{k \\ge 1}$ satisfying $A e_k = -\\lambda_k e_k$, where $\\lambda_k = \\pi^2 k^2$ and $e_k(x) = \\sqrt{2} \\sin(\\pi k x)$ for $x \\in D$.\n\nConsider the $Q$-Wiener process $W^Q(t)$ on $H$, where $Q : H \\to H$ is a symmetric, nonnegative, trace-class covariance operator, diagonal in the same basis $\\{e_k\\}$: $Q e_k = q_k e_k$. Assume $q_k = k^{-2\\beta}$ for a fixed parameter $\\beta > \\tfrac{1}{2}$, so that $\\mathrm{Tr}(Q) = \\sum_{k=1}^{\\infty} q_k  \\infty$.\n\nWe study the linear stochastic partial differential equation (SPDE)\n$$\ndX(t) = A X(t) \\, dt + dW^Q(t), \\quad X(0) = 0,\n$$\nunderstood in the mild sense on $H$. The spectral Galerkin approximation of order $N$ is the projection onto the first $N$ eigenfunctions,\n$$\nX^{(N)}(t) = \\sum_{k=1}^{N} X_k(t) \\, e_k,\n$$\nwhere $X_k(t) = \\langle X(t), e_k \\rangle_H$ are the spectral coefficients.\n\nYour tasks are:\n\n1) Starting from the definitions of the analytic semigroup generated by $A$ and the mild solution of the linear SPDE, derive the scalar stochastic differential equation solved by each spectral coefficient $X_k(t)$ and prove that each $X_k(t)$ is an Ornstein–Uhlenbeck (OU) process. Then derive the exact variance $\\mathbb{V}\\mathrm{ar}[X_k(t)]$ in terms of $q_k$, $\\lambda_k$, and $t$. Use this to express the mean-square error in the $H$-norm between the full solution and the spectral Galerkin approximation as\n$$\n\\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] = \\sum_{k=N+1}^{\\infty} \\mathbb{V}\\mathrm{ar}[X_k(t)].\n$$\n\n2) Using the explicit eigenvalues $\\lambda_k = \\pi^2 k^2$ and the assumption $q_k = k^{-2\\beta}$ with $\\beta > \\tfrac{1}{2}$, derive the asymptotic convergence rate in mean-square of the spectral Galerkin method as $N \\to \\infty$. Your derivation must show an explicit exponent $\\gamma(\\beta)$ such that\n$$\n\\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] \\sim C(t,\\beta) \\, N^{-\\gamma(\\beta)} \\quad \\text{as } N \\to \\infty,\n$$\nfor a finite constant $C(t,\\beta)$ depending on $t$ and $\\beta$, and you must determine $\\gamma(\\beta)$ from first principles. You must not assume or quote the convergence rate; you must derive it explicitly from the variance formula and the eigenvalue asymptotics.\n\n3) Implement a spectral Galerkin discretization routine that computes the mean-square error $\\mathbb{E}[\\|X(t) - X^{(N)}(t)\\|_{H}^2]$ exactly as an infinite series, accelerated by an analytically justified tail approximation. Specifically, compute\n$$\n\\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] = \\sum_{k=N+1}^{K} \\frac{q_k}{2 \\lambda_k} \\big(1 - e^{-2 \\lambda_k t}\\big) + R_{K}(N,\\beta,t),\n$$\nwhere $K$ is chosen adaptively large so that $e^{-2 \\lambda_k t}$ is negligible for $k  K$, and $R_{K}(N,\\beta,t)$ is a rigorously derived tail remainder expressed in terms of the Hurwitz zeta function $\\zeta(s,a)$ with exponent $s  1$.\n\n4) Using your implementation, estimate the empirical convergence rate by performing a linear regression of $\\log\\big(\\mathbb{E}[\\|X(t) - X^{(N)}(t)\\|_{H}^2]\\big)$ against $\\log(N)$ for several values of $N$, and compare the estimated slope to the theoretically derived exponent $\\gamma(\\beta)$. Report the absolute difference between the empirical slope magnitude and $\\gamma(\\beta)$.\n\n5) For a single test instance, perform a Monte Carlo validation of the mean-square error by simulating the tail coefficients $\\{X_k(t)\\}_{kN}$ as independent Gaussian random variables with the exact variances for a finite set of modes and samples, and then add the analytic tail remainder to approximate the infinite sum. Report the absolute difference between the Monte Carlo estimate and your exact series-based value.\n\nImplementation details:\n\n- Domain: $D = (0,1)$; boundary conditions: homogeneous Dirichlet.\n- Eigenpairs: $\\lambda_k = \\pi^2 k^2$, $e_k(x) = \\sqrt{2} \\sin(\\pi k x)$.\n- Noise spectrum: $q_k = k^{-2\\beta}$ with $\\beta > \\tfrac{1}{2}$.\n- Time: fixed $t > 0$.\n- Tail remainder: use the Hurwitz zeta function $\\zeta(s,a)$ for $s = 2\\beta + 2$ to evaluate $\\sum_{k=K+1}^{\\infty} k^{-s}$, multiplied by the appropriate constant deduced in your derivation.\n- Monte Carlo: use a fixed pseudorandom seed for reproducibility.\n\nTest suite:\n\n- Case A (boundary case): Compute $\\mathbb{E}[\\|X(t) - X^{(N)}(t)\\|_{H}^2]$ for $N = 0$, $\\beta = 1.0$, $t = 0.5$. Return a single float.\n- Case B (rate check, moderate smoothness): For $\\beta = 0.6$, $t = 0.5$, $N \\in \\{4, 8, 16, 32\\}$, estimate the empirical slope and return the absolute difference to the theoretical exponent $\\gamma(\\beta)$ as a single float.\n- Case C (rate check, smoother noise): For $\\beta = 1.0$, $t = 0.5$, $N \\in \\{8, 16, 32, 64\\}$, estimate the empirical slope and return the absolute difference to $\\gamma(\\beta)$ as a single float.\n- Case D (rate check, very smooth noise): For $\\beta = 2.0$, $t = 0.5$, $N \\in \\{8, 16, 24, 32\\}$, estimate the empirical slope and return the absolute difference to $\\gamma(\\beta)$ as a single float.\n- Case E (Monte Carlo validation): For $\\beta = 0.8$, $t = 0.5$, $N = 16$, simulate $M = 5000$ samples of the tail coefficients for $K_{\\mathrm{sim}} = 800$ modes using a fixed seed, add the analytic remainder, and return the absolute difference between the Monte Carlo estimate and your exact series-based value as a single float.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly five floats corresponding to Cases A through E, in order, for example, \"[result_A,result_B,result_C,result_D,result_E]\". No physical units are involved in this problem; all outputs are dimensionless real numbers.", "solution": "This solution provides the theoretical derivations for tasks 1 and 2, followed by a Python implementation for tasks 3, 4, and 5.\n\n**Task 1: SDE for Spectral Coefficients and Variance**\n\nProjecting the SPDE $dX(t) = AX(t)dt + dW^Q(t)$ onto the eigenfunction $e_k$ gives an SDE for the spectral coefficient $X_k(t) = \\langle X(t), e_k \\rangle_H$:\n$$ dX_k(t) = \\langle AX(t), e_k \\rangle_H dt + \\langle dW^Q(t), e_k \\rangle_H $$\nUsing the self-adjointness of $A$ and the property $Ae_k = -\\lambda_k e_k$, the drift term becomes $\\langle X(t), Ae_k \\rangle_H = \\langle X(t), -\\lambda_k e_k \\rangle_H = -\\lambda_k X_k(t)$. The noise term is the increment of the $k$-th component of the $Q$-Wiener process, which is a scaled Wiener process $\\sqrt{q_k}d\\beta_k(t)$, where $\\{\\beta_k\\}$ are independent standard Wiener processes. Thus, each coefficient solves the scalar SDE:\n$$ dX_k(t) = -\\lambda_k X_k(t)dt + \\sqrt{q_k}d\\beta_k(t), \\quad X_k(0) = 0 $$\nThis is the SDE for an Ornstein-Uhlenbeck process. Since $X_k(0)=0$ and the noise has zero mean, $\\mathbb{E}[X_k(t)]=0$. The variance is found using Itô's isometry on the integral form of the solution:\n$$ \\mathbb{V}\\mathrm{ar}[X_k(t)] = \\mathbb{E}[X_k(t)^2] = \\mathbb{E}\\left[\\left( \\int_0^t e^{-\\lambda_k(t-s)}\\sqrt{q_k}d\\beta_k(s) \\right)^2\\right] = q_k \\int_0^t e^{-2\\lambda_k(t-s)}ds = \\frac{q_k}{2\\lambda_k}(1 - e^{-2\\lambda_k t}). $$\nBy Parseval's identity, the mean-square error of the Galerkin approximation is the sum of the variances of the truncated modes:\n$$ \\mathbb{E}\\big[\\|X(t) - X^{(N)}(t)\\|_{H}^2\\big] = \\mathbb{E}\\left[\\left\\|\\sum_{k=N+1}^\\infty X_k(t) e_k\\right\\|_H^2\\right] = \\sum_{k=N+1}^\\infty \\mathbb{E}[X_k(t)^2] = \\sum_{k=N+1}^\\infty \\mathbb{V}\\mathrm{ar}[X_k(t)]. $$\n\n**Task 2: Asymptotic Convergence Rate**\n\nThe error is given by the series:\n$$ \\mathcal{E}(N,t,\\beta) = \\sum_{k=N+1}^\\infty \\frac{q_k}{2\\lambda_k}(1 - e^{-2\\lambda_k t}) = \\sum_{k=N+1}^\\infty \\frac{k^{-2\\beta}}{2\\pi^2 k^2}(1 - e^{-2\\pi^2 k^2 t}) = \\frac{1}{2\\pi^2} \\sum_{k=N+1}^\\infty k^{-(2\\beta+2)}(1 - e^{-2\\pi^2 k^2 t}). $$\nFor large $N$, the terms in the tail of the sum are dominated by the power-law decay, since $e^{-2\\pi^2k^2t} \\to 0$ rapidly. The summand is asymptotic to $\\frac{1}{2\\pi^2} k^{-(2\\beta+2)}$. We can approximate the sum by an integral:\n$$ \\mathcal{E}(N,t,\\beta) \\sim \\frac{1}{2\\pi^2} \\int_N^\\infty x^{-(2\\beta+2)} dx = \\frac{1}{2\\pi^2} \\left[ \\frac{x^{-(2\\beta+1)}}{-(2\\beta+1)} \\right]_N^\\infty = \\frac{1}{2\\pi^2(2\\beta+1)} N^{-(2\\beta+1)}. $$\nThe error converges as $N^{-\\gamma(\\beta)}$, where the theoretical convergence rate is $\\gamma(\\beta) = 2\\beta + 1$.\n\n**Tasks 3-5: Numerical Implementation**\n\n```python\nimport numpy as np\nfrom scipy.special import zeta\nfrom scipy.stats import linregress\n\ndef compute_total_error(N, beta, t, K_cutoff=20000):\n    \"\"\"\n    Computes the mean-square error E[||X(t) - X^N(t)||^2] using a \n    finite sum and a Hurwitz zeta function tail approximation.\n\n    The error is sum_{k=N+1 to inf} Var(X_k(t)).\n    Var(X_k(t)) = (q_k / (2*lambda_k)) * (1 - exp(-2*lambda_k*t))\n    q_k = k**(-2*beta)\n    lambda_k = pi**2 * k**2\n    \"\"\"\n    s = 2 * beta + 2\n    const = 1.0 / (2 * np.pi**2)\n    \n    # Finite sum part up to K_cutoff\n    k_vals = np.arange(N + 1, K_cutoff + 1)\n    if k_vals.size == 0:\n        finite_sum = 0.0\n    else:\n        lambda_k = np.pi**2 * k_vals**2\n        variances = (const * np.power(k_vals, -s) * \n                     (1.0 - np.exp(-2 * lambda_k * t)))\n        finite_sum = np.sum(variances)\n    \n    # Tail approximation part from K_cutoff+1 to infinity\n    # We approximate (1 - exp(-2*lambda_k*t)) as 1 for k > K_cutoff\n    tail_remainder = const * zeta(s, K_cutoff + 1)\n    \n    return finite_sum + tail_remainder\n\ndef estimate_rate_and_compare(Ns, beta, t):\n    \"\"\"\n    Estimates the convergence rate gamma and compares it to the theoretical value.\n    \"\"\"\n    log_Ns = np.log(Ns)\n    errors = [compute_total_error(n, beta, t) for n in Ns]\n    log_errors = np.log(errors)\n    \n    # Perform linear regression to find the slope\n    slope, _, _, _, _ = linregress(log_Ns, log_errors)\n    empirical_gamma = -slope\n    \n    theoretical_gamma = 2 * beta + 1\n    \n    return abs(empirical_gamma - theoretical_gamma)\n\ndef mc_validation(N, beta, t, M, K_sim, seed=42):\n    \"\"\"\n    Performs Monte Carlo validation of the error calculation.\n    \"\"\"\n    # Use a large K_cutoff for a more \"exact\" reference value\n    exact_value = compute_total_error(N, beta, t, K_cutoff=40000)\n\n    # The MC estimate is a hybrid: MC for [N+1, K_sim], analytic for [K_sim+1, inf]\n    \n    # 1. Analytic tail for the MC part (sum from K_sim + 1 to infinity)\n    analytic_tail_mc = compute_total_error(K_sim, beta, t, K_cutoff=40000)\n\n    # 2. MC simulation for modes [N+1, K_sim]\n    k_vals_mc = np.arange(N + 1, K_sim + 1)\n    s = 2 * beta + 2\n    const = 1.0 / (2 * np.pi**2)\n    \n    lambda_k_mc = np.pi**2 * k_vals_mc**2\n    variances_mc = (const * np.power(k_vals_mc, -s) *\n                    (1.0 - np.exp(-2 * lambda_k_mc * t)))\n    sigmas_mc = np.sqrt(variances_mc)\n    \n    rng = np.random.default_rng(seed)\n    # Generate M x num_modes standard normal samples\n    normal_samples = rng.normal(loc=0.0, scale=1.0, size=(M, len(k_vals_mc))) \n    \n    # Scale samples by standard deviations to get samples of X_k\n    # Broadcasting sigmas_mc (1, num_modes) over normal_samples (M, num_modes)\n    x_k_samples = normal_samples * sigmas_mc\n\n    # For each of the M paths, compute sum of squares over modes\n    sum_sq_per_path = np.sum(np.power(x_k_samples, 2), axis=1)\n\n    # Average the sum of squares over all M paths\n    mc_mean_sum_sq = np.mean(sum_sq_per_path)\n\n    # Total MC estimate is the mean of the simulated part plus the analytic tail\n    mc_estimate = mc_mean_sum_sq + analytic_tail_mc\n    \n    return abs(mc_estimate - exact_value)\n\ndef solve():\n    \"\"\"\n    Main solver function to execute all test cases.\n    \"\"\"\n    results = []\n\n    # Case A\n    # N=0 means we compute the total variance of the solution X(t)\n    result_A = compute_total_error(N=0, beta=1.0, t=0.5)\n    results.append(result_A)\n\n    # Case B\n    result_B = estimate_rate_and_compare(Ns=[4, 8, 16, 32], beta=0.6, t=0.5)\n    results.append(result_B)\n\n    # Case C\n    result_C = estimate_rate_and_compare(Ns=[8, 16, 32, 64], beta=1.0, t=0.5)\n    results.append(result_C)\n    \n    # Case D\n    result_D = estimate_rate_and_compare(Ns=[8, 16, 24, 32], beta=2.0, t=0.5)\n    results.append(result_D)\n\n    # Case E\n    # Note: Using a fixed seed for reproducibility.\n    result_E = mc_validation(N=16, beta=0.8, t=0.5, M=5000, K_sim=800, seed=0)\n    results.append(result_E)\n\n    # Format the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "answer": "```python\nimport numpy as np\nfrom scipy.special import zeta\nfrom scipy.stats import linregress\n\ndef compute_total_error(N, beta, t, K_cutoff=20000):\n    \"\"\"\n    Computes the mean-square error E[||X(t) - X^N(t)||^2] using a \n    finite sum and a Hurwitz zeta function tail approximation.\n\n    The error is sum_{k=N+1 to inf} Var(X_k(t)).\n    Var(X_k(t)) = (q_k / (2*lambda_k)) * (1 - exp(-2*lambda_k*t))\n    q_k = k**(-2*beta)\n    lambda_k = pi**2 * k**2\n    \"\"\"\n    s = 2 * beta + 2\n    const = 1.0 / (2 * np.pi**2)\n    \n    # Finite sum part up to K_cutoff\n    k_vals = np.arange(N + 1, K_cutoff + 1)\n    if k_vals.size == 0:\n        finite_sum = 0.0\n    else:\n        lambda_k = np.pi**2 * k_vals**2\n        variances = (const * np.power(k_vals, -s) * \n                     (1.0 - np.exp(-2 * lambda_k * t)))\n        finite_sum = np.sum(variances)\n    \n    # Tail approximation part from K_cutoff+1 to infinity\n    # We approximate (1 - exp(-2*lambda_k*t)) as 1 for k  K_cutoff\n    tail_remainder = const * zeta(s, K_cutoff + 1)\n    \n    return finite_sum + tail_remainder\n\ndef estimate_rate_and_compare(Ns, beta, t):\n    \"\"\"\n    Estimates the convergence rate gamma and compares it to the theoretical value.\n    \"\"\"\n    log_Ns = np.log(Ns)\n    errors = [compute_total_error(n, beta, t) for n in Ns]\n    log_errors = np.log(errors)\n    \n    # Perform linear regression to find the slope\n    slope, _, _, _, _ = linregress(log_Ns, log_errors)\n    empirical_gamma = -slope\n    \n    theoretical_gamma = 2 * beta + 1\n    \n    return abs(empirical_gamma - theoretical_gamma)\n\ndef mc_validation(N, beta, t, M, K_sim, seed=42):\n    \"\"\"\n    Performs Monte Carlo validation of the error calculation.\n    \"\"\"\n    # Use a large K_cutoff for a more \"exact\" reference value\n    exact_value = compute_total_error(N, beta, t, K_cutoff=40000)\n\n    # The MC estimate is a hybrid: MC for [N+1, K_sim], analytic for [K_sim+1, inf]\n    \n    # 1. Analytic tail for the MC part (sum from K_sim + 1 to infinity)\n    analytic_tail_mc = compute_total_error(K_sim, beta, t, K_cutoff=40000)\n\n    # 2. MC simulation for modes [N+1, K_sim]\n    k_vals_mc = np.arange(N + 1, K_sim + 1)\n    s = 2 * beta + 2\n    const = 1.0 / (2 * np.pi**2)\n    \n    lambda_k_mc = np.pi**2 * k_vals_mc**2\n    variances_mc = (const * np.power(k_vals_mc, -s) *\n                    (1.0 - np.exp(-2 * lambda_k_mc * t)))\n    sigmas_mc = np.sqrt(variances_mc)\n    \n    rng = np.random.default_rng(seed)\n    # Generate M x num_modes standard normal samples\n    normal_samples = rng.normal(loc=0.0, scale=1.0, size=(M, len(k_vals_mc))) \n    \n    # Scale samples by standard deviations to get samples of X_k\n    # Broadcasting sigmas_mc (1, num_modes) over normal_samples (M, num_modes)\n    x_k_samples = normal_samples * sigmas_mc\n\n    # For each of the M paths, compute sum of squares over modes\n    sum_sq_per_path = np.sum(np.power(x_k_samples, 2), axis=1)\n\n    # Average the sum of squares over all M paths\n    mc_mean_sum_sq = np.mean(sum_sq_per_path)\n\n    # Total MC estimate is the mean of the simulated part plus the analytic tail\n    mc_estimate = mc_mean_sum_sq + analytic_tail_mc\n    \n    return abs(mc_estimate - exact_value)\n\ndef solve():\n    \"\"\"\n    Main solver function to execute all test cases.\n    \"\"\"\n    results = []\n\n    # Case A\n    # N=0 means we compute the total variance of the solution X(t)\n    result_A = compute_total_error(N=0, beta=1.0, t=0.5)\n    results.append(result_A)\n\n    # Case B\n    result_B = estimate_rate_and_compare(Ns=[4, 8, 16, 32], beta=0.6, t=0.5)\n    results.append(result_B)\n\n    # Case C\n    result_C = estimate_rate_and_compare(Ns=[8, 16, 32, 64], beta=1.0, t=0.5)\n    results.append(result_C)\n    \n    # Case D\n    result_D = estimate_rate_and_compare(Ns=[8, 16, 24, 32], beta=2.0, t=0.5)\n    results.append(result_D)\n\n    # Case E\n    # Note: Using a fixed seed for reproducibility.\n    result_E = mc_validation(N=16, beta=0.8, t=0.5, M=5000, K_sim=800, seed=0)\n    results.append(result_E)\n\n    # Format the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3003074"}]}