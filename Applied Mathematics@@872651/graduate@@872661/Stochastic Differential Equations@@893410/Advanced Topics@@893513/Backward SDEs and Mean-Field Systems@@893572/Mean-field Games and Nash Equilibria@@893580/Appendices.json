{"hands_on_practices": [{"introduction": "The journey into solving a mean-field game begins with understanding the perspective of a single, representative agent. This foundational exercise hones your ability to apply the Hamilton-Jacobi-Bellman (HJB) principle to find an agent's optimal strategy when the behavior of the crowd is taken as a given. By working through the ubiquitous linear-quadratic (LQ) framework, you will master the essential technique of deriving an optimal feedback control, a critical first step in analyzing more complex game structures. [@problem_id:2987064]", "problem": "Consider a linear-quadratic mean-field game with a continuum of identical agents. For a representative agent, the controlled state process $(X_t)_{t \\in [0,T]}$ evolves according to the stochastic differential equation\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t,\n$$\nwhere $a$, $b$, $c$, and $\\sigma$ are real constants, $(W_t)_{t \\in [0,T]}$ is a standard Brownian motion, $\\alpha_t$ is the agent’s control, and $m_t$ denotes the population mean $m_t = \\mathbb{E}[X_t]$. The representative agent seeks to minimize the expected quadratic cost\n$$\nJ(\\alpha) = \\mathbb{E}\\Bigg[\\int_{0}^{T} \\left(q X_t^{2} + \\bar{q}\\left(X_t - m_t\\right)^{2} + r \\alpha_t^{2}\\right)\\mathrm{d}t + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2}\\Bigg],\n$$\nwith given constants $q>0$, $\\bar{q}\\geq 0$, $r>0$, $g\\geq 0$, and $\\bar{g}\\geq 0$. Assume that the mean field $(m_t)_{t\\in[0,T]}$ is a given deterministic function of time when solving the representative agent’s problem.\n\nStarting from the dynamic programming principle and the Hamilton–Jacobi–Bellman (HJB) equation, use an ansatz for the value function of the form\n$$\nV(t,x) = \\tfrac{1}{2}P_t x^{2} + S_t m_t x + \\tfrac{1}{2}U_t m_t^{2} + \\phi_t,\n$$\nwhere $(P_t,S_t,U_t,\\phi_t)$ are deterministic coefficient functions of time chosen so that $V$ solves the HJB equation with terminal condition $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$. By minimizing the Hamiltonian in the HJB equation with respect to the control, derive the feedback control $\\alpha_t$ in terms of the functions $P_t$ and $S_t$. You may use that the coefficient matching in the HJB equation yields a coupled Riccati system for $(P_t,S_t)$, but you should not solve that system explicitly. Express your final answer as a single closed-form analytic expression for $\\alpha_t$ in terms of $b$, $r$, $P_t$, $S_t$, $X_t$, and $m_t$. No numerical evaluation is required.", "solution": "The problem is valid as it presents a standard, well-posed problem in the field of mean-field game theory, specifically a linear-quadratic (LQ) model. All parameters and objectives are clearly defined, and the premises are scientifically sound within the context of stochastic optimal control.\n\nThe objective is to find the optimal feedback control $\\alpha_t$ for a representative agent. The agent's state $X_t$ follows the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_t = \\left(a X_t + b \\alpha_t + c m_t\\right)\\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\nThe agent seeks to minimize a quadratic cost functional $J(\\alpha)$. In the mean-field game framework, the first step is to solve the representative agent's optimization problem, assuming the mean-field term $m_t = \\mathbb{E}[X_t]$ is a known deterministic function of time.\n\nThe value function for the agent's problem is defined as:\n$$\nV(t,x) = \\min_{\\alpha} \\mathbb{E}\\Bigg[\\int_{t}^{T} \\left(q X_s^{2} + \\bar{q}\\left(X_s - m_s\\right)^{2} + r \\alpha_s^{2}\\right)\\mathrm{d}s + g X_T^{2} + \\bar{g}\\left(X_T - m_T\\right)^{2} \\Bigg| X_t=x \\Bigg]\n$$\nThis value function $V(t,x)$ must satisfy the Hamilton-Jacobi-Bellman (HJB) partial differential equation. The HJB equation is given by:\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\left\\{ L(t,x,\\alpha,m_t) + \\mathcal{L}^\\alpha V(t,x) \\right\\}\n$$\nwhere $L(t,x,\\alpha,m_t)$ is the running cost and $\\mathcal{L}^\\alpha$ is the infinitesimal generator of the state process $X_t$ under control $\\alpha$.\n\nThe running cost is:\n$$\nL(t,x,\\alpha,m_t) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2}\n$$\nThe infinitesimal generator $\\mathcal{L}^\\alpha$ applied to the value function $V(t,x)$ is:\n$$\n\\mathcal{L}^\\alpha V(t,x) = \\left(a x + b \\alpha + c m_t\\right) \\frac{\\partial V}{\\partial x}(t,x) + \\frac{1}{2}\\sigma^2 \\frac{\\partial^2 V}{\\partial x^2}(t,x)\n$$\nThe term inside the minimization operator in the HJB equation is the Hamiltonian, $\\mathcal{H}$, which depends on the state $x$, time $t$, control $\\alpha$, and the partial derivatives of the value function, $V_x = \\frac{\\partial V}{\\partial x}$ and $V_{xx} = \\frac{\\partial^2 V}{\\partial x^2}$.\n$$\n\\mathcal{H}(t, x, \\alpha, V_x, V_{xx}) = q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + r \\alpha^{2} + \\left(a x + b \\alpha + c m_t\\right) V_x + \\frac{1}{2}\\sigma^2 V_{xx}\n$$\nThe HJB equation can thus be written as:\n$$\n-\\frac{\\partial V}{\\partial t}(t,x) = \\min_{\\alpha \\in \\mathbb{R}} \\mathcal{H}(t, x, \\alpha, V_x, V_{xx})\n$$\nTo find the optimal control $\\alpha_t^*$, we must minimize the Hamiltonian with respect to $\\alpha$ for a fixed state $(t,x)$. We can rewrite the Hamiltonian to isolate the terms involving $\\alpha$:\n$$\n\\mathcal{H} = r \\alpha^{2} + b \\alpha V_x + \\left( q x^{2} + \\bar{q}\\left(x - m_t\\right)^{2} + (a x + c m_t) V_x + \\frac{1}{2}\\sigma^2 V_{xx} \\right)\n$$\nThis is a quadratic function of $\\alpha$. Since the parameter $r > 0$, the parabola opens upwards, and its minimum can be found by setting its derivative with respect to $\\alpha$ to zero.\n$$\n\\frac{\\partial \\mathcal{H}}{\\partial \\alpha} = 2r \\alpha + b V_x = 0\n$$\nSolving for $\\alpha$ gives the optimal control, denoted $\\alpha^*$, in terms of the value function's spatial derivative:\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\frac{\\partial V}{\\partial x}(t,x)\n$$\nThe problem provides an ansatz for the value function:\n$$\nV(t,x) = \\frac{1}{2}P_t x^{2} + S_t m_t x + \\frac{1}{2}U_t m_t^{2} + \\phi_t\n$$\nwhere $P_t$, $S_t$, $U_t$, and $\\phi_t$ are deterministic functions of time. We compute the partial derivative of this ansatz with respect to $x$:\n$$\n\\frac{\\partial V}{\\partial x}(t,x) = P_t x + S_t m_t\n$$\nSubstituting this expression into the formula for the optimal control $\\alpha^*(t,x)$ yields:\n$$\n\\alpha^*(t,x) = -\\frac{b}{2r} \\left(P_t x + S_t m_t\\right)\n$$\nThis is the optimal control in feedback form for a given state $x$ at time $t$. For the representative agent whose state at time $t$ is $X_t$, the optimal control $\\alpha_t$ is obtained by replacing the generic state variable $x$ with the random variable $X_t$:\n$$\n\\alpha_t = -\\frac{b}{2r} \\left(P_t X_t + S_t m_t\\right)\n$$\nThis expression provides the optimal control $\\alpha_t$ in terms of the state $X_t$, the mean field $m_t$, the problem parameters $b$ and $r$, and the coefficient functions $P_t$ and $S_t$ from the value function ansatz, as requested. The terminal condition $V(T,x) = g x^{2} + \\bar{g}\\left(x - m_T\\right)^{2}$ provides the terminal values for the coefficients, e.g., $P_T = 2(g+\\bar{g})$ and $S_T = -2\\bar{g}$, which would be used to solve the Riccati system for $P_t$ and $S_t$. However, solving this system is not required by the problem statement.", "answer": "$$\n\\boxed{-\\frac{b}{2r}(P_t X_t + S_t m_t)}\n$$", "id": "2987064"}, {"introduction": "A mean-field game is solved only when the individual's optimal strategy and the collective behavior of the population are in harmony—a Nash Equilibrium. This practice moves beyond the single-agent problem to finding this self-consistent solution in a finite-state, ergodic setting, a common paradigm in economic and social modeling. You will learn to couple the agent's optimality conditions with the population's stationarity equation to explicitly compute the equilibrium, providing a complete picture of the game's outcome. [@problem_id:2987138]", "problem": "Consider a Mean Field Game (MFG) in continuous time with a continuum of identical agents whose individual state is a node in the two-point set $\\{0,1\\}$. An agent at node $i \\in \\{0,1\\}$ chooses a transition rate $u_i \\in [0,\\bar{u}_i]$ to jump to node $1-i$. The population evolves according to a controlled Continuous-Time Markov Chain (CTMC) with generator $\\mathcal{L}^{u}$ given by $\\mathcal{L}^{u} f(i) = u_i \\big(f(1-i) - f(i)\\big)$ for any bounded function $f$, under a stationary Markov control $u = (u_0,u_1)$.\n\nAgents seek to minimize the ergodic (long-run average) cost. The instantaneous running cost at node $i$ is the linear congestion cost $\\gamma_i m_i$ plus the linear control cost $\\alpha_i u_i$, where $m_i$ is the stationary fraction of the population at node $i$. The parameters are\n$$\n\\gamma_0 = 0.6,\\quad \\gamma_1 = 1.4,\\quad \\alpha_0 = -1,\\quad \\alpha_1 = 0.5,\\quad \\bar{u}_0 = 3,\\quad \\bar{u}_1 = 2.\n$$\nA mean-field Nash equilibrium is a pair $(u^{\\ast},m^{\\ast})$ such that:\n- For the stationary distribution $m^{\\ast} = (m_0^{\\ast},m_1^{\\ast})$, the control $u^{\\ast}$ minimizes the agent’s ergodic Hamilton–Jacobi–Bellman (HJB) equations with value function $V = (V_0,V_1)$ and ergodic constant $\\lambda$.\n- The stationary distribution $m^{\\ast}$ is consistent with the CTMC driven by $u^{\\ast}$, i.e., it satisfies the stationarity (global balance) conditions.\n\nStarting from these definitions and principles, derive the ergodic HJB optimality conditions for this two-node CTMC with linear costs and bounded transition rates. Then, solve explicitly for the equilibrium transition rates $u_0^{\\ast}$ and $u_1^{\\ast}$ and the stationary distribution $m_0^{\\ast}$ and $m_1^{\\ast}$.\n\nExpress your final answer as a single row matrix containing $(u_0^{\\ast},\\,u_1^{\\ast},\\,m_0^{\\ast},\\,m_1^{\\ast})$. No rounding is required.", "solution": "The problem asks for the mean-field Nash equilibrium of a two-state continuous-time Markov chain (CTMC) model. An equilibrium is a pair $(u^{\\ast}, m^{\\ast})$ consisting of an optimal control policy $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$ and a consistent stationary population distribution $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ that satisfy two main conditions: agent optimality and population consistency.\n\n**1. Agent Optimality: The Ergodic Hamilton-Jacobi-Bellman (HJB) Equations**\n\nFor an individual agent, the population distribution $m = (m_0, m_1)$ is considered fixed. The agent's goal is to choose a control policy $u = (u_0, u_1)$ to minimize their long-run average (ergodic) cost. Let $V = (V_0, V_1)$ be the value function representing the total expected future cost at each state, and let $\\lambda$ be the constant ergodic cost. The optimality conditions are given by the ergodic Hamilton-Jacobi-Bellman (HJB) equations. For each state $i \\in \\{0, 1\\}$, the HJB equation is:\n$$ \\lambda = \\min_{u_i \\in [0, \\bar{u}_i]} \\left\\{ \\text{instantaneous cost at } i + \\mathcal{L}^{u} V(i) \\right\\} $$\nThe instantaneous cost at state $i$ is $\\gamma_i m_i + \\alpha_i u_i$. The generator of the CTMC is given as $\\mathcal{L}^{u} V(i) = u_i (V_{1-i} - V_i)$. Substituting these into the HJB system, we get:\n$$ \\lambda = \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ \\gamma_0 m_0 + \\alpha_0 u_0 + u_0(V_1 - V_0) \\right\\} $$\n$$ \\lambda = \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ \\gamma_1 m_1 + \\alpha_1 u_1 + u_1(V_0 - V_1) \\right\\} $$\nTo simplify, we define the relative value, or potential, $P = V_1 - V_0$. The HJB equations can then be rewritten as:\n$$ \\lambda = \\gamma_0 m_0 + \\min_{u_0 \\in [0, \\bar{u}_0]} \\left\\{ u_0 (\\alpha_0 + P) \\right\\} $$\n$$ \\lambda = \\gamma_1 m_1 + \\min_{u_1 \\in [0, \\bar{u}_1]} \\left\\{ u_1 (\\alpha_1 - P) \\right\\} $$\nSince the control $u_i$ appears linearly in the objective, the optimal control $u_i^{\\ast}$ is determined by the sign of its coefficient. This results in a \"bang-bang\" or singular control policy.\nThe optimal control $u_0^{\\ast}$ is:\n$$ u_0^{\\ast}(P) = \\begin{cases} \\bar{u}_0 & \\text{if } \\alpha_0 + P < 0 \\\\ \\text{any value in } [0, \\bar{u}_0] & \\text{if } \\alpha_0 + P = 0 \\\\ 0 & \\text{if } \\alpha_0 + P > 0 \\end{cases} $$\nThe optimal control $u_1^{\\ast}$ is:\n$$ u_1^{\\ast}(P) = \\begin{cases} \\bar{u}_1 & \\text{if } \\alpha_1 - P < 0 \\\\ \\text{any value in } [0, \\bar{u}_1] & \\text{if } \\alpha_1 - P = 0 \\\\ 0 & \\text{if } \\alpha_1 - P > 0 \\end{cases} $$\n\n**2. Population Consistency: The Stationary Distribution**\n\nThe stationary distribution $m^{\\ast} = (m_0^{\\ast}, m_1^{\\ast})$ must be consistent with the dynamics induced by the optimal control $u^{\\ast} = (u_0^{\\ast}, u_1^{\\ast})$. This is captured by the stationarity (or global balance) equation for the CTMC, which states that the flow of agents from state $0$ to $1$ must equal the flow from state $1$ to $0$:\n$$ m_0^{\\ast} u_0^{\\ast} = m_1^{\\ast} u_1^{\\ast} $$\nTogether with the fact that the population fractions must sum to one, $m_0^{\\ast} + m_1^{\\ast} = 1$, we can solve for the stationary distribution. Assuming $u_0^{\\ast} + u_1^{\\ast} > 0$, we have:\n$$ m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} \\quad \\text{and} \\quad m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} $$\n\n**3. Solving for the Mean-Field Equilibrium**\n\nAn equilibrium is a state $(u^{\\ast}, m^{\\ast}, P)$ that solves the HJB equations and the consistency condition simultaneously. We can find the equilibrium by equating the two HJB equations to eliminate $\\lambda$:\n$$ \\gamma_0 m_0^{\\ast} + u_0^{\\ast}(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + u_1^{\\ast}(\\alpha_1 - P) $$\nWe are given the parameters: $\\gamma_0 = 0.6$, $\\gamma_1 = 1.4$, $\\alpha_0 = -1$, $\\alpha_1 = 0.5$, $\\bar{u}_0 = 3$, $\\bar{u}_1 = 2$.\nThe critical values for the potential $P$ that determine the control policy are $P = -\\alpha_0 = 1$ and $P = \\alpha_1 = 0.5$. This partitions the search for $P$ into several cases. Let's analyze the case where the equilibrium controls are at their maximum values, which occurs when $\\alpha_0 + P < 0$ and $\\alpha_1 - P < 0$. This corresponds to the interval $P \\in (0.5, 1)$.\n\nIn this candidate regime, the optimal controls are:\n- $u_0^{\\ast} = \\bar{u}_0 = 3$ (since $P < 1 \\implies P - 1 < 0 \\implies \\alpha_0 + P < 0$)\n- $u_1^{\\ast} = \\bar{u}_1 = 2$ (since $P > 0.5 \\implies 0.5 - P < 0 \\implies \\alpha_1 - P < 0$)\n\nWith these controls, the consistent stationary distribution is:\n- $m_0^{\\ast} = \\frac{u_1^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{2}{3 + 2} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{u_0^{\\ast}}{u_0^{\\ast} + u_1^{\\ast}} = \\frac{3}{3 + 2} = \\frac{3}{5}$\n\nNow, we must verify that these values are consistent with the HJB equality for $P$. Substituting $u_0^{\\ast}, u_1^{\\ast}, m_0^{\\ast}, m_1^{\\ast}$ into the equation:\n$$ \\gamma_0 m_0^{\\ast} + \\bar{u}_0(\\alpha_0 + P) = \\gamma_1 m_1^{\\ast} + \\bar{u}_1(\\alpha_1 - P) $$\nPlugging in the numerical values:\n$$ (0.6)\\left(\\frac{2}{5}\\right) + 3(-1 + P) = (1.4)\\left(\\frac{3}{5}\\right) + 2(0.5 - P) $$\n$$ 0.24 - 3 + 3P = 0.84 + 1 - 2P $$\n$$ 3P - 2.76 = 1.84 - 2P $$\n$$ 5P = 1.84 + 2.76 $$\n$$ 5P = 4.6 $$\n$$ P = \\frac{4.6}{5} = 0.92 $$\nThe value $P = 0.92$ lies within the assumed interval $(0.5, 1)$, which confirms that our choice of control policies is correct and the solution is self-consistent. Exhaustive analysis of the other cases (e.g., $P < 0.5$ or $P > 1$) leads to contradictions, confirming this equilibrium is unique.\n\nThus, the equilibrium transition rates and stationary distribution are:\n- $u_0^{\\ast} = 3$\n- $u_1^{\\ast} = 2$\n- $m_0^{\\ast} = \\frac{2}{5}$\n- $m_1^{\\ast} = \\frac{3}{5}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 3 & 2 & \\frac{2}{5} & \\frac{3}{5} \\end{pmatrix}\n}\n$$", "id": "2987138"}, {"introduction": "What truly separates game theory from straightforward optimization is the potential for multiple, self-sustaining outcomes. This advanced problem delves into this subtlety by contrasting a decentralized mean-field game with its centralized \"social planner\" counterpart, showing how the former can admit a multitude of equilibria while the latter has a unique solution. By identifying the precise conditions under which uniqueness is lost, you will gain a profound understanding of why the structure of interactions is critical and appreciate the significance of concepts like Lasry-Lions monotonicity. [@problem_id:2987096]", "problem": "Consider a continuum of identical agents with states governed by the stochastic differential equation $dX_t = u_t\\,dt + \\sigma\\,dW_t$ on the time interval $[0,T]$, where $W_t$ is a standard Brownian motion, $\\sigma>0$ is the diffusion coefficient, and $u_t$ is a progressively measurable control with finite quadratic energy. The initial state $X_0$ is integrable with mean $m_0 := \\mathbb{E}[X_0]$. Let $\\bar{m}_t := \\mathbb{E}[X_t]$ denote the population mean at time $t$.\n\nEach agent evaluates a linear-quadratic cost functional\n$$\nJ(u;\\bar{m}_T) \\;=\\; \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt \\;+\\; \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right],\n$$\nwith parameters $r>0$, $\\delta>0$, and coupling strength $\\alpha\\in\\mathbb{R}$. The coupling term depends on the terminal population mean $\\bar{m}_T$. Consider two formulations:\n\n1. Mean-Field Game (MFG): A Nash equilibrium is a control that minimizes $J(u;\\bar{m}_T)$ for a given $\\bar{m}_T$, and whose induced terminal mean $\\bar{m}_T$ is consistent with the population distribution generated by the optimal control.\n\n2. Mean-Field Control (social planner problem): A common control for the population is chosen to minimize the average cost, equivalently minimizing $J(u;\\bar{m}_T)$ with the understanding that $\\bar{m}_T$ is determined by the chosen control.\n\nStarting from first principles (the definition of Nash equilibrium in Mean-Field Games, the structure of linear-quadratic stochastic control, and elementary inequalities such as the Cauchy–Schwarz inequality), perform the following:\n\n(a) Derive the scalar fixed-point equation satisfied by the terminal mean $\\bar{m}_T$ in the Mean-Field Game, expressed in terms of $\\alpha$, $r$, $\\delta$, $T$, and $m_0$.\n\n(b) Show that, even when the Lasry–Lions monotonicity condition fails for the coupling $x\\mapsto \\frac{\\delta}{2}\\left(x - \\alpha\\,\\bar{m}_T\\right)^2$, the Mean-Field Control problem admits a unique optimal terminal mean $\\bar{m}_T$ due to strict convexity. Provide a brief argument establishing the failure of the Lasry–Lions monotonicity condition for $\\alpha>0$.\n\n(c) Specialize to the case $m_0 = 0$. Determine the exact coupling strength $\\alpha^{\\star}$ at which the Mean-Field Game admits infinitely many Nash equilibria (i.e., the fixed-point equation in (a) has infinitely many solutions for $\\bar{m}_T$), while the Mean-Field Control optimizer remains unique under the same data. Express your final answer as a closed-form analytic expression for $\\alpha^{\\star}$.", "solution": "This is a linear-quadratic (LQ) stochastic game, which allows for explicit solutions. We will address each part in sequence.\n\n**(a) Derivation of the Mean-Field Game Fixed-Point Equation**\n\nIn the MFG setting, each agent solves an optimal control problem for a *given*, fixed terminal population mean, $\\bar{m}_T$. The agent seeks to minimize:\n$$J(u) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$$\nsubject to the dynamics $dX_t = u_t\\,dt + \\sigma\\,dW_t$. This is a standard LQ stochastic control problem. We use Pontryagin's maximum principle for SDEs. The Hamiltonian is:\n$$H(t, x, y, u) = \\frac{r}{2} u^2 + y u$$\nThe first-order optimality condition is $\\frac{\\partial H}{\\partial u} = ru + y = 0$, which gives a candidate for the optimal control: $u_t^* = -\\frac{1}{r} Y_t$.\nHere, $(Y_t, Z_t)$ is the adjoint process governed by the backward SDE (BSDE):\n$$dY_t = -\\frac{\\partial H}{\\partial x}(t, X_t^*, Y_t, u_t^*)\\,dt + Z_t\\,dW_t = 0 \\cdot dt + Z_t dW_t = Z_t dW_t$$\nwith the terminal condition given by the derivative of the terminal cost:\n$$Y_T = \\frac{\\partial}{\\partial x} \\left[\\frac{\\delta}{2}(x - \\alpha\\bar{m}_T)^2\\right]\\bigg|_{x=X_T^*} = \\delta(X_T^* - \\alpha\\bar{m}_T)$$\nSince $dY_t = Z_t dW_t$, $Y_t$ is a martingale, and its expected value $\\bar{y}_t := \\mathbb{E}[Y_t]$ must be constant over time. Let $\\bar{y} := \\mathbb{E}[Y_t]$ for all $t \\in [0,T]$.\nTaking the expectation of the terminal condition for $Y_T$:\n$$\\bar{y} = \\mathbb{E}[Y_T] = \\mathbb{E}[\\delta(X_T^* - \\alpha\\bar{m}_T)] = \\delta(\\mathbb{E}[X_T^*] - \\alpha\\bar{m}_T) = \\delta(\\bar{m}_T^* - \\alpha\\bar{m}_T)$$\nwhere $\\bar{m}_T^*$ is the terminal mean induced by the agent's optimal control.\n\nNext, we find the dynamics of the mean state $\\bar{m}_t^* = \\mathbb{E}[X_t^*]$. Taking the expectation of the state SDE:\n$$d\\bar{m}_t^* = \\mathbb{E}[dX_t^*] = \\mathbb{E}[u_t^*]\\,dt + \\mathbb{E}[\\sigma\\,dW_t] = \\mathbb{E}[-\\frac{1}{r}Y_t]\\,dt = -\\frac{1}{r}\\bar{y}\\,dt$$\nIntegrating from $t=0$ to $t=T$:\n$$\\bar{m}_T^* - \\bar{m}_0 = \\int_0^T \\left(-\\frac{1}{r}\\bar{y}\\right) dt = -\\frac{T}{r}\\bar{y}$$\nGiven that $\\bar{m}_0=m_0$, we have $\\bar{m}_T^* = m_0 - \\frac{T}{r}\\bar{y}$.\n\nAt this point, we have two equations relating the resulting mean $\\bar{m}_T^*$ and the constant expected adjoint value $\\bar{y}$, for a given anticipated mean $\\bar{m}_T$:\n1. $\\bar{y} = \\delta(\\bar{m}_T^* - \\alpha\\bar{m}_T)$\n2. $\\bar{m}_T^* = m_0 - \\frac{T}{r}\\bar{y}$\n\nThe MFG consistency condition requires that the anticipated mean equals the resulting mean: $\\bar{m}_T^* = \\bar{m}_T$. Substituting this into the equations:\n1. $\\bar{y} = \\delta(\\bar{m}_T - \\alpha\\bar{m}_T) = \\delta(1-\\alpha)\\bar{m}_T$\n2. $\\bar{m}_T = m_0 - \\frac{T}{r}\\bar{y}$\n\nWe can now find the fixed-point equation for $\\bar{m}_T$. Substitute $\\bar{y}$ from the first equation into the second:\n$$\\bar{m}_T = m_0 - \\frac{T}{r} \\left( \\delta(1-\\alpha)\\bar{m}_T \\right)$$\nRearranging the terms to solve for $\\bar{m}_T$:\n$$\\bar{m}_T + \\frac{\\delta T(1-\\alpha)}{r}\\bar{m}_T = m_0$$\n$$\\bar{m}_T \\left( 1 + \\frac{\\delta T(1-\\alpha)}{r} \\right) = m_0$$\nMultiplying by $r$:\n$$\\bar{m}_T \\left( r + \\delta T(1-\\alpha) \\right) = m_0 r$$\nThis is the scalar fixed-point equation satisfied by the terminal mean $\\bar{m}_T$ in the Mean-Field Game.\n\n**(b) Lasry–Lions Monotonicity and Mean-Field Control Uniqueness**\n\nThe Lasry-Lions (LL) monotonicity condition is a sufficient condition for the uniqueness of a MFG equilibrium. For this problem, the coupling is through the term $\\frac{\\delta}{2}(X_T - \\alpha\\bar{m}_T)^2$. The 'force' exerted on an agent by the mean field can be thought of as related to the derivative of the cost with respect to the mean, which is $-\\delta\\alpha(X_T - \\alpha\\bar{m}_T)$. For $\\alpha>0$, agents are penalized for being close to $\\alpha\\bar{m}_T$, which can be interpreted as a repulsive interaction. Such repulsive interactions generally violate the LL monotonicity condition. For instance, if $\\alpha > 1$, the feedback loop from the assumed mean to the resulting mean can have a gain greater than $1$, leading to multiple equilibria. More formally, the mapping from the assumed mean $\\mu$ to the resulting mean is $\\Phi(\\mu) = m_0 - \\frac{T\\delta(m_0-\\alpha\\mu)}{r+\\delta T}$. Its derivative, $\\Phi'(\\mu) = \\frac{\\alpha T \\delta}{r+\\delta T}$, is positive for $\\alpha>0$, indicating a positive feedback loop that typifies non-monotone games. Thus, for $\\alpha>0$, the system has a non-monotone structure, and the LL condition can fail, opening the possibility of non-unique equilibria.\n\nNow, consider the Mean-Field Control (social planner) problem. The planner chooses a common control $u_t$ for all agents to minimize the average cost. Since the control is common, it must be deterministic. The state of a representative agent is $X_t = X_0 + \\int_0^t u_s\\,ds + \\sigma W_t$. The mean state is $\\bar{m}_t = m_0 + \\int_0^t u_s\\,ds$. The planner's cost function is:\n$$J_{MFC}(u) = \\mathbb{E}\\left[ \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta}{2}\\left(X_T - \\alpha\\,\\bar{m}_T\\right)^2 \\right]$$\nLet's analyze the terminal cost term by expanding it around the mean:\n$$\\mathbb{E}[(X_T - \\alpha\\bar{m}_T)^2] = \\mathbb{E}[ ( (X_T - \\bar{m}_T) + (1-\\alpha)\\bar{m}_T )^2 ]$$\n$$= \\mathbb{E}[(X_T-\\bar{m}_T)^2] + 2(1-\\alpha)\\bar{m}_T\\mathbb{E}[X_T-\\bar{m}_T] + (1-\\alpha)^2\\bar{m}_T^2$$\nSince $\\mathbb{E}[X_T-\\bar{m}_T]=0$, the cross-term vanishes. The first term is the variance of $X_T$:\n$$\\text{Var}(X_T) = \\text{Var}(X_0 + \\text{const} + \\sigma W_T) = \\text{Var}(X_0) + \\sigma^2 T$$\nThis variance is independent of the control $u_t$. Therefore, minimizing $J_{MFC}$ is equivalent to minimizing:\n$$\\hat{J}_{MFC}(u) = \\frac{r}{2} \\int_0^T u_t^2\\,dt + \\frac{\\delta(1-\\alpha)^2}{2} \\bar{m}_T^2$$\nwhere $\\bar{m}_T = m_0 + \\int_0^T u_t\\,dt$. Let $U = \\int_0^T u_t\\,dt$. By the Cauchy-Schwarz inequality, for a fixed value of $U$, the integral $\\int_0^T u_t^2\\,dt$ is minimized when $u_t$ is constant, i.e., $u_t = U/T$. In this case, $\\int_0^T u_t^2\\,dt = (U/T)^2 T = U^2/T$. The problem reduces to minimizing a function of a single scalar variable $U$:\n$$F(U) = \\frac{r}{2T}U^2 + \\frac{\\delta(1-\\alpha)^2}{2}(m_0+U)^2$$\nThis is a quadratic function of $U$. To check for uniqueness of the minimum, we examine its second derivative:\n$$F''(U) = \\frac{d^2 F}{d U^2} = \\frac{r}{T} + \\delta(1-\\alpha)^2$$\nSince $r>0$, $T>0$, $\\delta>0$, and $(1-\\alpha)^2 \\ge 0$, the second derivative $F''(U)$ is strictly positive for all $\\alpha \\in \\mathbb{R}$. This proves that $F(U)$ is a strictly convex function. A strictly convex function has at most one minimizer. Since $F(U)$ is a quadratic that opens upwards, it has a unique global minimum. Therefore, the Mean-Field Control problem admits a unique optimal control and a unique optimal terminal mean $\\bar{m}_T$, irrespective of the value of $\\alpha$.\n\n**(c) Condition for Infinite Equilibria in MFG**\n\nWe specialize to the case $m_0=0$. The fixed-point equation from part (a) becomes:\n$$\\bar{m}_T \\left( r + \\delta T(1-\\alpha) \\right) = 0$$\nThis equation governs the possible equilibrium values of $\\bar{m}_T$.\n- If the coefficient $r + \\delta T(1-\\alpha) \\neq 0$, the only solution is $\\bar{m}_T=0$. In this case, the MFG has a unique Nash equilibrium.\n- If the coefficient $r + \\delta T(1-\\alpha) = 0$, the equation becomes $0 \\cdot \\bar{m}_T = 0$, which is satisfied for any $\\bar{m}_T \\in \\mathbb{R}$. In this scenario, any real number is a valid equilibrium terminal mean, implying the existence of infinitely many Nash equilibria.\n\nWe are looking for the coupling strength $\\alpha^{\\star}$ that leads to this second case. We set the coefficient to zero:\n$$r + \\delta T(1-\\alpha^{\\star}) = 0$$\n$$r + \\delta T - \\delta T \\alpha^{\\star} = 0$$\n$$\\delta T \\alpha^{\\star} = r + \\delta T$$\n$$\\alpha^{\\star} = \\frac{r+\\delta T}{\\delta T} = 1 + \\frac{r}{\\delta T}$$\nAt this critical value $\\alpha^{\\star}$, the Mean-Field Game admits infinitely many equilibria. As established in part (b), the Mean-Field Control problem's objective function remains strictly convex since $F''(U) = \\frac{r}{T} + \\delta(1-\\alpha^{\\star})^2 = \\frac{r}{T} + \\delta(-\\frac{r}{\\delta T})^2 = \\frac{r}{T} + \\frac{r^2}{\\delta T^2} > 0$. Therefore, even at $\\alpha = \\alpha^{\\star}$, the social planner's problem still has a unique solution (which is $U=0$, leading to $\\bar{m}_T = 0$), contrasting sharply with the multiplicity of equilibria in the game setting.", "answer": "$$\\boxed{1 + \\frac{r}{\\delta T}}$$", "id": "2987096"}]}