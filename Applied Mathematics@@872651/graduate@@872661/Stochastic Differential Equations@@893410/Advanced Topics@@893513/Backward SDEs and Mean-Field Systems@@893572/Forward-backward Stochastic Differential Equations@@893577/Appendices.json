{"hands_on_practices": [{"introduction": "Before tackling fully coupled systems, it is essential to understand the core relationship between the forward and backward components. This first exercise [@problem_id:2977079] explores this in the simplest possible setting: a decoupled system with a zero driver. By solving this system, you will uncover the fundamental link between the terminal condition and the $Y$ process, and see how the volatility of the forward process directly determines the control process $Z$.", "problem": "Consider a complete filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$. Let $X=(X_t)_{t\\in[0,T]}$ be the unique strong solution to the forward stochastic differential equation $dX_t=\\mu\\,dt+\\sigma\\,dW_t$ with deterministic initial condition $X_0=x_0\\in\\mathbb{R}$, where $\\mu\\in\\mathbb{R}$ and $\\sigma\\in(0,\\infty)$ are constants. Consider the decoupled forward-backward stochastic differential equation with zero driver $f\\equiv 0$ and terminal condition $Y_T=g(X_T)$, where $g(x)=x$. The backward component $(Y,Z)$ is adapted to $(\\mathcal{F}_t)_{t\\in[0,T]}$ and satisfies the backward stochastic differential equation\n$$\nY_t=g(X_T)-\\int_t^T Z_s\\,dW_s,\\qquad t\\in[0,T].\n$$\nStarting from first principles of Itô calculus, the martingale property of stochastic integrals, and the definition of conditional expectation in the natural filtration of $W$, derive an explicit expression for the process $Z=(Z_t)_{t\\in[0,T]}$ in closed form. Your final answer must be a single closed-form analytic expression. No rounding is required and no units are involved.", "solution": "The backward stochastic differential equation (BSDE) is given for $t\\in[0,T]$ as:\n$$\nY_t = Y_T + \\int_t^T f(s,X_s,Y_s,Z_s)\\,ds - \\int_t^T Z_s\\,dW_s\n$$\nIn this problem, the driver is $f \\equiv 0$ and the terminal condition is $Y_T = g(X_T) = X_T$. The BSDE simplifies to:\n$$\nY_t = X_T - \\int_t^T Z_s\\,dW_s\n$$\nA fundamental result in the theory of BSDEs states that the solution component $Y_t$ is given by the conditional expectation of the terminal value plus the integral of the future driver. With $f \\equiv 0$, this relation is:\n$$\nY_t = \\mathbb{E}[Y_T | \\mathcal{F}_t]\n$$\nSubstituting the terminal condition $Y_T = X_T$, we have:\n$$\nY_t = \\mathbb{E}[X_T | \\mathcal{F}_t]\n$$\nTo evaluate this conditional expectation, we first need to find an explicit expression for the forward process $X_T$. The forward SDE is:\n$$\ndX_t = \\mu\\,dt + \\sigma\\,dW_t\n$$\nwith the deterministic initial condition $X_0 = x_0$. This is a linear SDE with constant coefficients. We can solve it by direct integration from time $s=0$ to $s=t$:\n$$\n\\int_0^t dX_s = \\int_0^t \\mu\\,ds + \\int_0^t \\sigma\\,dW_s\n$$\n$$\nX_t - X_0 = \\mu t + \\sigma (W_t - W_0)\n$$\nSince $W_0=0$ by definition of a standard Brownian motion, and given $X_0=x_0$, the solution is:\n$$\nX_t = x_0 + \\mu t + \\sigma W_t\n$$\nAt the terminal time $t=T$, the value of the process is:\n$$\nX_T = x_0 + \\mu T + \\sigma W_T\n$$\nNow, we can substitute this expression into the formula for $Y_t$:\n$$\nY_t = \\mathbb{E}[x_0 + \\mu T + \\sigma W_T | \\mathcal{F}_t]\n$$\nUsing the linearity of conditional expectation and the fact that $x_0$, $\\mu$, $T$, and $\\sigma$ are constants:\n$$\nY_t = x_0 + \\mu T + \\sigma \\mathbb{E}[W_T | \\mathcal{F}_t]\n$$\nTo evaluate $\\mathbb{E}[W_T | \\mathcal{F}_t]$, we use the tower property of Brownian motion. For $t \\le T$, we can write $W_T = W_t + (W_T - W_t)$. Thus,\n$$\n\\mathbb{E}[W_T | \\mathcal{F}_t] = \\mathbb{E}[W_t + (W_T - W_t) | \\mathcal{F}_t] = \\mathbb{E}[W_t | \\mathcal{F}_t] + \\mathbb{E}[W_T - W_t | \\mathcal{F}_t]\n$$\nSince $W_t$ is $\\mathcal{F}_t$-measurable, $\\mathbb{E}[W_t | \\mathcal{F}_t] = W_t$. The increment $W_T - W_t$ is independent of the filtration $\\mathcal{F}_t$, so its conditional expectation is equal to its unconditional expectation, which is $\\mathbb{E}[W_T - W_t] = 0$. Therefore,\n$$\n\\mathbb{E}[W_T | \\mathcal{F}_t] = W_t + 0 = W_t\n$$\nSubstituting this result back into our expression for $Y_t$, we obtain an explicit formula for the process $Y$:\n$$\nY_t = x_0 + \\mu T + \\sigma W_t\n$$\nWe now have the process $Y_t$. To find the process $Z_t$, we write the BSDE in differential form:\n$$\ndY_t = f(t,X_t,Y_t,Z_t)\\,dt + Z_t\\,dW_t\n$$\nSince $f \\equiv 0$, this becomes:\n$$\ndY_t = Z_t\\,dW_t\n$$\nWe can also find the differential of our explicit solution for $Y_t$ using Itô calculus. For $Y_t = x_0 + \\mu T + \\sigma W_t$, the terms $x_0$ and $\\mu T$ are constants, so their differentials are zero. The differential of $Y_t$ is:\n$$\ndY_t = d(x_0 + \\mu T + \\sigma W_t) = \\sigma\\,dW_t\n$$\nBy comparing the two expressions for the stochastic differential $dY_t$, we have:\n$$\nZ_t\\,dW_t = \\sigma\\,dW_t\n$$\nThe uniqueness of the Itô process decomposition implies that the integrands with respect to $dW_t$ must be equal almost surely for each $t$. Therefore, we conclude that:\n$$\nZ_t = \\sigma\n$$\nThis holds for all $t \\in [0,T]$. The process $Z = (Z_t)_{t\\in[0,T]}$ is thus a constant process, equal to the volatility parameter $\\sigma$ of the forward process.", "answer": "$$\n\\boxed{\\sigma}\n$$", "id": "2977079"}, {"introduction": "Most interesting FBSDEs are coupled and cannot be solved in a single step, requiring iterative methods. The Picard iteration is a natural and powerful approach that constructs a solution by successively refining an initial guess. This hands-on exercise [@problem_id:2977112] makes this abstract algorithm concrete by asking you to explicitly compute the first two iterations for a linear FBSDE, providing a tangible feel for the mechanics of the solution process.", "problem": "Consider a complete filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\in[0,T]},\\mathbb{P})$ satisfying the usual conditions and supporting a one-dimensional standard Brownian motion $W=(W_t)_{t\\in[0,T]}$. Fix deterministic constants $x_0\\in\\mathbb{R}$, $\\sigma\\in\\mathbb{R}$, and $\\alpha\\in\\mathbb{R}\\setminus\\{0\\}$, and a horizon $T0$ that is sufficiently small to ensure contraction of the Picard mapping for forward-backward stochastic differential equations (FBSDE). Consider the forward-backward system\n$$\n\\mathrm{d}X_t \\;=\\; Y_t\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_t,\\qquad X_0 \\;=\\; x_0,\n$$\nand\n$$\nY_t \\;=\\; X_T \\;+\\; \\int_t^T \\alpha\\,Y_s\\,\\mathrm{d}s \\;-\\; \\int_t^T Z_s\\,\\mathrm{d}W_s,\\qquad t\\in[0,T].\n$$\nStarting from the foundational definitions of Itô integrals and martingales, define the Picard iteration on the pair $(Y,Z)$ by the initialization $Y^{(0)}_t\\equiv 0$ and $Z^{(0)}_t\\equiv 0$ for $t\\in[0,T]$, and then, for each $n\\geq 0$, define $X^{(n+1)}$ by\n$$\nX^{(n+1)}_t \\;=\\; x_0 \\;+\\; \\int_0^t Y^{(n)}_s\\,\\mathrm{d}s \\;+\\; \\sigma\\,W_t,\\qquad t\\in[0,T],\n$$\nand $(Y^{(n+1)},Z^{(n+1)})$ as the unique adapted solution to the linear backward stochastic differential equation\n$$\nY^{(n+1)}_t \\;=\\; X^{(n+1)}_T \\;+\\; \\int_t^T \\alpha\\,Y^{(n+1)}_s\\,\\mathrm{d}s \\;-\\; \\int_t^T Z^{(n+1)}_s\\,\\mathrm{d}W_s,\\qquad t\\in[0,T].\n$$\nCompute explicitly the first two Picard iterates $(X^{(1)},Y^{(1)},Z^{(1)})$ and $(X^{(2)},Y^{(2)},Z^{(2)})$ as closed-form adapted processes. Your final answer must be the single row matrix containing the six expressions $(X^{(1)}_t,\\,Y^{(1)}_t,\\,Z^{(1)}_t,\\,X^{(2)}_t,\\,Y^{(2)}_t,\\,Z^{(2)}_t)$ for a generic $t\\in[0,T]$. No rounding is required.", "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem within the field of stochastic differential equations. It provides all necessary information to compute the requested quantities.\n\nThe task is to compute the first two Picard iterates, denoted $(X^{(1)}, Y^{(1)}, Z^{(1)})$ and $(X^{(2)}, Y^{(2)}, Z^{(2)})$, for the given forward-backward stochastic differential equation (FBSDE) system. The iteration starts with the initialization $Y^{(0)}_t \\equiv 0$ and $Z^{(0)}_t \\equiv 0$.\n\nFor each iteration step $n \\ge 0$, the forward process $X^{(n+1)}$ is defined by\n$$\nX^{(n+1)}_t \\;=\\; x_0 \\;+\\; \\int_0^t Y^{(n)}_s\\,\\mathrm{d}s \\;+\\; \\sigma\\,W_t,\n$$\nand the backward pair $(Y^{(n+1)}, Z^{(n+1)})$ is the solution to the linear backward stochastic differential equation (BSDE)\n$$\nY^{(n+1)}_t \\;=\\; X^{(n+1)}_T \\;+\\; \\int_t^T \\alpha\\,Y^{(n+1)}_s\\,\\mathrm{d}s \\;-\\; \\int_t^T Z^{(n+1)}_s\\,\\mathrm{d}W_s.\n$$\nThis BSDE can be written in differential form as $-\\mathrm{d}Y^{(n+1)}_t = \\alpha Y^{(n+1)}_t \\mathrm{d}t - Z^{(n+1)}_t \\mathrm{d}W_t$, with terminal condition $Y^{(n+1)}_T = X^{(n+1)}_T$. The solution for $Y^{(n+1)}_t$ is given by the conditional expectation formula:\n$$\nY^{(n+1)}_t \\;=\\; \\mathbb{E}\\left[ \\exp\\left(\\int_t^T \\alpha\\,\\mathrm{d}s\\right) X^{(n+1)}_T \\,\\middle|\\, \\mathcal{F}_t \\right] \\;=\\; \\mathbb{E}\\left[ \\exp(\\alpha(T-t)) X^{(n+1)}_T \\,\\middle|\\, \\mathcal{F}_t \\right].\n$$\nThe process $Z^{(n+1)}_t$ is then identified through the martingale representation theorem applied to the martingale $M_t = \\mathbb{E}[\\exp(\\alpha T) X^{(n+1)}_T | \\mathcal{F}_t]$. Specifically, we have $Y^{(n+1)}_t = \\exp(-\\alpha t) M_t$, and applying Itô's product rule gives $\\mathrm{d}Y^{(n+1)}_t = -\\alpha Y^{(n+1)}_t \\mathrm{d}t + \\exp(-\\alpha t)\\mathrm{d}M_t$. By comparing this with the differential form of the BSDE, we identify $Z^{(n+1)}_t \\mathrm{d}W_t = \\exp(-\\alpha t)\\mathrm{d}M_t$.\n\n**First Iterate: $(X^{(1)}, Y^{(1)}, Z^{(1)})$**\n\nWe begin with $n=0$, using the initialization $Y^{(0)}_s = 0$.\n\n1.  **Compute $X^{(1)}_t$**:\n    $$\n    X^{(1)}_t \\;=\\; x_0 \\;+\\; \\int_0^t Y^{(0)}_s\\,\\mathrm{d}s \\;+\\; \\sigma\\,W_t \\;=\\; x_0 \\;+\\; \\int_0^t 0\\,\\mathrm{d}s \\;+\\; \\sigma\\,W_t \\;=\\; x_0 \\;+\\; \\sigma\\,W_t.\n    $$\n\n2.  **Compute $Y^{(1)}_t$**:\n    The terminal condition is $X^{(1)}_T = x_0 + \\sigma W_T$.\n    $$\n    Y^{(1)}_t \\;=\\; \\mathbb{E}\\left[ \\exp(\\alpha(T-t)) (x_0 + \\sigma W_T) \\,\\middle|\\, \\mathcal{F}_t \\right].\n    $$\n    Using the linearity of conditional expectation and the martingale property $\\mathbb{E}[W_T|\\mathcal{F}_t] = W_t$:\n    $$\n    Y^{(1)}_t \\;=\\; \\exp(\\alpha(T-t)) (x_0 + \\sigma \\mathbb{E}[W_T|\\mathcal{F}_t]) \\;=\\; (x_0 + \\sigma W_t) \\exp(\\alpha(T-t)).\n    $$\n\n3.  **Compute $Z^{(1)}_t$**:\n    We consider the martingale $M_t = \\mathbb{E}[\\exp(\\alpha T) X^{(1)}_T | \\mathcal{F}_t] = \\exp(\\alpha T)(x_0 + \\sigma W_t)$.\n    Its differential is $\\mathrm{d}M_t = \\sigma \\exp(\\alpha T) \\mathrm{d}W_t$.\n    From the relationship $Z^{(1)}_t \\mathrm{d}W_t = \\exp(-\\alpha t)\\mathrm{d}M_t$, we get:\n    $$\n    Z^{(1)}_t \\;=\\; \\exp(-\\alpha t) (\\sigma \\exp(\\alpha T)) \\;=\\; \\sigma \\exp(\\alpha(T-t)).\n    $$\n\n**Second Iterate: $(X^{(2)}, Y^{(2)}, Z^{(2)})$**\n\nNow we proceed with $n=1$, using the derived expression for $Y^{(1)}_t$.\n\n1.  **Compute $X^{(2)}_t$**:\n    $$\n    X^{(2)}_t \\;=\\; x_0 \\;+\\; \\int_0^t Y^{(1)}_s\\,\\mathrm{d}s \\;+\\; \\sigma\\,W_t \\;=\\; x_0 \\;+\\; \\sigma W_t \\;+\\; \\int_0^t (x_0 + \\sigma W_s) \\exp(\\alpha(T-s))\\,\\mathrm{d}s.\n    $$\n    The integral is split into a deterministic and a stochastic part:\n    $\\int_0^t x_0 \\exp(\\alpha(T-s))\\,\\mathrm{d}s = x_0 \\exp(\\alpha T) [-\\frac{1}{\\alpha}\\exp(-\\alpha s)]_0^t = \\frac{x_0}{\\alpha}(\\exp(\\alpha T) - \\exp(\\alpha(T-t)))$.\n    For the term involving $W_s$, we use integration by parts: $\\int u \\mathrm{d}v = uv - \\int v \\mathrm{d}u$.\n    $\\int_0^t \\sigma W_s \\exp(\\alpha(T-s))\\,\\mathrm{d}s = \\sigma \\exp(\\alpha T) \\int_0^t W_s \\exp(-\\alpha s) \\mathrm{d}s$.\n    $\\int_0^t W_s \\exp(-\\alpha s) \\mathrm{d}s = [W_s(-\\frac{1}{\\alpha}\\exp(-\\alpha s))]_0^t - \\int_0^t (-\\frac{1}{\\alpha}\\exp(-\\alpha s)) \\mathrm{d}W_s = -\\frac{1}{\\alpha}W_t \\exp(-\\alpha t) + \\frac{1}{\\alpha}\\int_0^t \\exp(-\\alpha s)\\mathrm{d}W_s$.\n    Combining these results:\n    $$\n    X^{(2)}_t \\;=\\; x_0 + \\sigma W_t + \\frac{x_0}{\\alpha}(\\exp(\\alpha T) - \\exp(\\alpha(T-t))) - \\frac{\\sigma}{\\alpha}W_t \\exp(\\alpha(T-t)) + \\frac{\\sigma \\exp(\\alpha T)}{\\alpha} \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s.\n    $$\n    Grouping terms, we have:\n    $$\n    X^{(2)}_t \\;=\\; x_0\\left(1 + \\frac{\\exp(\\alpha T) - \\exp(\\alpha(T-t))}{\\alpha}\\right) + \\sigma W_t\\left(1 - \\frac{\\exp(\\alpha(T-t))}{\\alpha}\\right) + \\frac{\\sigma \\exp(\\alpha T)}{\\alpha} \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s.\n    $$\n\n2.  **Compute $Y^{(2)}_t$**:\n    First, we determine the terminal value $X^{(2)}_T$ by setting $t=T$ in the expression for $X^{(2)}_t$:\n    $$\n    X^{(2)}_T \\;=\\; x_0\\left(1 + \\frac{\\exp(\\alpha T) - 1}{\\alpha}\\right) + \\sigma W_T\\left(1 - \\frac{1}{\\alpha}\\right) + \\frac{\\sigma \\exp(\\alpha T)}{\\alpha} \\int_0^T \\exp(-\\alpha s)\\,\\mathrm{d}W_s.\n    $$\n    Now, we compute $Y^{(2)}_t = \\mathbb{E}[\\exp(\\alpha(T-t)) X^{(2)}_T | \\mathcal{F}_t]$. We take the conditional expectation of each term in $X^{(2)}_T$.\n    $\\mathbb{E}[W_T|\\mathcal{F}_t] = W_t$ and $\\mathbb{E}[\\int_0^T \\exp(-\\alpha s)\\,\\mathrm{d}W_s | \\mathcal{F}_t] = \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s$.\n    $$\n    \\mathbb{E}[X^{(2)}_T | \\mathcal{F}_t] \\;=\\; x_0\\left(\\frac{\\alpha + \\exp(\\alpha T) - 1}{\\alpha}\\right) + \\sigma W_t\\left(\\frac{\\alpha - 1}{\\alpha}\\right) + \\frac{\\sigma \\exp(\\alpha T)}{\\alpha} \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s.\n    $$\n    Multiplying by $\\exp(\\alpha(T-t))$ gives $Y^{(2)}_t$:\n    $$\n    Y^{(2)}_t \\;=\\; \\frac{\\exp(\\alpha(T-t))}{\\alpha} \\left( x_0(\\exp(\\alpha T) + \\alpha - 1) + \\sigma W_t(\\alpha-1) + \\sigma \\exp(\\alpha T) \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s \\right).\n    $$\n\n3.  **Compute $Z^{(2)}_t$**:\n    We consider the martingale $M_t = \\mathbb{E}[\\exp(\\alpha T) X^{(2)}_T | \\mathcal{F}_t] = \\exp(\\alpha T) \\mathbb{E}[X^{(2)}_T | \\mathcal{F}_t]$.\n    Its differential is found by taking the differential of the stochastic terms in $M_t$:\n    $$\n    \\mathrm{d}M_t \\;=\\; \\exp(\\alpha T) \\left( \\sigma\\left(\\frac{\\alpha-1}{\\alpha}\\right)\\mathrm{d}W_t + \\frac{\\sigma \\exp(\\alpha T)}{\\alpha} \\exp(-\\alpha t)\\mathrm{d}W_t \\right) \\;=\\; \\left( \\frac{\\sigma(\\alpha-1)\\exp(\\alpha T)}{\\alpha} + \\frac{\\sigma\\exp(2\\alpha T - \\alpha t)}{\\alpha} \\right)\\mathrm{d}W_t.\n    $$\n    Using $Z^{(2)}_t = \\exp(-\\alpha t) \\frac{\\mathrm{d}M_t}{\\mathrm{d}W_t}$:\n    $$\n    Z^{(2)}_t \\;=\\; \\exp(-\\alpha t) \\left( \\frac{\\sigma(\\alpha-1)\\exp(\\alpha T)}{\\alpha} + \\frac{\\sigma\\exp(2\\alpha T - \\alpha t)}{\\alpha} \\right) \\;=\\; \\frac{\\sigma(\\alpha-1)}{\\alpha}\\exp(\\alpha(T-t)) + \\frac{\\sigma}{\\alpha}\\exp(2\\alpha(T-t)).\n    $$\n    This simplifies to:\n    $$\n    Z^{(2)}_t \\;=\\; \\frac{\\sigma}{\\alpha} \\left( (\\alpha-1)\\exp(\\alpha(T-t)) + \\exp(2\\alpha(T-t)) \\right).\n    $$\n\nSummary of the expressions for the final answer:\n\\begin{itemize}\n    \\item $X^{(1)}_t = x_0 + \\sigma W_t$\n    \\item $Y^{(1)}_t = (x_0 + \\sigma W_t) \\exp(\\alpha(T-t))$\n    \\item $Z^{(1)}_t = \\sigma \\exp(\\alpha(T-t))$\n    \\item $X^{(2)}_t = x_0\\left(1 + \\frac{\\exp(\\alpha T) - \\exp(\\alpha(T-t))}{\\alpha}\\right) + \\sigma W_t\\left(1 - \\frac{\\exp(\\alpha(T-t))}{\\alpha}\\right) + \\frac{\\sigma \\exp(\\alpha T)}{\\alpha} \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s$\n    \\item $Y^{(2)}_t = \\frac{\\exp(\\alpha(T-t))}{\\alpha} \\left( x_0(\\exp(\\alpha T) + \\alpha - 1) + \\sigma W_t(\\alpha-1) + \\sigma \\exp(\\alpha T) \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s \\right)$\n    \\item $Z^{(2)}_t = \\frac{\\sigma}{\\alpha} ( (\\alpha-1)\\exp(\\alpha(T-t)) + \\exp(2\\alpha(T-t)) )$\n\\end{itemize}", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_0 + \\sigma W_t  (x_0 + \\sigma W_t) \\exp(\\alpha(T-t))  \\sigma \\exp(\\alpha(T-t))  x_0\\left(1 + \\frac{\\exp(\\alpha T) - \\exp(\\alpha(T-t))}{\\alpha}\\right) + \\sigma W_t\\left(1 - \\frac{\\exp(\\alpha(T-t))}{\\alpha}\\right) + \\frac{\\sigma \\exp(\\alpha T)}{\\alpha} \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s  \\frac{\\exp(\\alpha(T-t))}{\\alpha} \\left( x_0(\\exp(\\alpha T) + \\alpha - 1) + \\sigma W_t(\\alpha-1) + \\sigma \\exp(\\alpha T) \\int_0^t \\exp(-\\alpha s)\\,\\mathrm{d}W_s \\right)  \\frac{\\sigma}{\\alpha} \\left( (\\alpha-1)\\exp(\\alpha(T-t)) + \\exp(2\\alpha(T-t)) \\right)\n\\end{pmatrix}\n}\n$$", "id": "2977112"}, {"introduction": "While the Picard iteration is a powerful tool, its convergence is not always guaranteed. This final practice [@problem_id:2977074] investigates the crucial question of when and why this iterative method succeeds or fails. By analyzing the properties of the iteration map for a linear FBSDE, you will discover a critical dependence on the time horizon $T$, a non-obvious result with significant implications for both theory and numerical applications.", "problem": "Consider the scalar linear Forward-Backward Stochastic Differential Equation (FBSDE) on the time interval $\\left[0,T\\right]$ with $T0$:\n- Forward equation: $\\,dX_t = \\alpha\\,Y_t\\,dt + \\sigma\\,dW_t\\,,\\quad X_0=x_0\\,$,\n- Backward equation: $\\,dY_t = -\\,q\\,X_t\\,dt + Z_t\\,dW_t\\,,\\quad Y_T = 0\\,$,\nwhere $\\alpha0$, $q0$, $\\sigma\\ge 0$ are constants and $\\left(W_t\\right)_{t\\in\\left[0,T\\right]}$ is a standard Brownian motion on a filtered probability space satisfying the usual conditions. Consider the canonical Picard fixed-point iteration that alternates between:\n- Given an adapted process $Y^{(k)}$, define $X^{(k+1)}$ by solving the forward equation with $Y^{(k)}$ in place of $Y$.\n- Then define $\\left(Y^{(k+1)},Z^{(k+1)}\\right)$ by solving the backward equation with $X^{(k+1)}$ in place of $X$.\nAssume the iteration is posed on the space $L^2\\left(\\Omega\\times\\left[0,T\\right]\\right)$ with the usual norm, and analyze the linearization of the Picard map around the origin to determine whether it is a contraction. Starting only from the definitions above and standard properties of linear BSDEs with drivers independent of $\\left(Y,Z\\right)$, derive the spectral radius of the linearized map in terms of the parameters and determine a parameter regime in which the iteration fails to converge. Select the correct statement below.\n\nA. With $Y_T=0$, the linearized Picard map has spectral radius $\\rho = \\dfrac{4}{\\pi^2}\\,q\\,\\alpha\\,T^2$, hence the map is not a contraction and the iteration fails to converge whenever $q\\,\\alpha\\,T^2 \\ge \\dfrac{\\pi^2}{4}$.\n\nB. With $Y_T=0$, the linearized Picard map has spectral radius $\\rho = \\sqrt{q\\,\\alpha}\\,T$, hence the iteration fails to converge whenever $\\sqrt{q\\,\\alpha}\\,T \\ge 1$.\n\nC. With $Y_T=0$, the spectral radius of the linearized Picard map is independent of $T$, so convergence or divergence depends only on $\\alpha$ and $q$.\n\nD. For sufficiently large $\\sigma$, the spectral radius of the linearized Picard map strictly decreases to below $1$ for any fixed $\\alpha$ and $q$, thereby restoring convergence regardless of $T$.", "solution": "The Picard iteration defines a map $\\Phi$ on the space of adapted processes in $L^2\\left(\\Omega\\times\\left[0,T\\right]\\right)$. Let $Y \\in L^2\\left(\\Omega\\times\\left[0,T\\right]\\right)$. The map is given by $Y' = \\Phi(Y)$, where $Y'$ is obtained through the two-step process:\n\n1.  Solve for $X$ given $Y$:\n    $$X_t = x_0 + \\int_0^t \\alpha\\,Y_s\\,ds + \\int_0^t \\sigma\\,dW_s = x_0 + \\alpha \\int_0^t Y_s\\,ds + \\sigma\\,W_t$$\n2.  Solve for $Y'$ given $X$: The solution to the BSDE $dY'_t = -q\\,X_t\\,dt + Z'_t\\,dW_t$ with $Y'_T=0$ is given by the conditional expectation:\n    $$Y'_t = E\\left[\\int_t^T q\\,X_s\\,ds \\Bigg| \\mathcal{F}_t\\right]$$\n\nSubstituting the expression for $X_s$ into the formula for $Y'_t$:\n$$Y'_t = E\\left[\\int_t^T q\\,\\left( x_0 + \\alpha\\int_0^s Y_u\\,du + \\sigma\\,W_s \\right) ds \\Bigg| \\mathcal{F}_t\\right]$$\n$$Y'_t = E\\left[q\\,\\alpha \\int_t^T \\left(\\int_0^s Y_u\\,du\\right)ds \\Bigg| \\mathcal{F}_t \\right] + E\\left[q \\int_t^T \\left(x_0 + \\sigma\\,W_s\\right)ds \\Bigg| \\mathcal{F}_t \\right]$$\n\nThis defines an affine map $Y' = \\Phi(Y) = K(Y) + C$, where $K$ is the linear operator and $C$ is the constant (affine) part. The convergence of the iteration $Y^{(k+1)} = \\Phi(Y^{(k)})$ is governed by the spectral radius of the linear operator $K$.\n\nThe Fréchet derivative of $\\Phi$ at any point $Y$ is the linear operator $K$:\n$$(KY)_t = E\\left[q\\,\\alpha \\int_t^T \\left(\\int_0^s Y_u\\,du\\right)ds \\Bigg| \\mathcal{F}_t \\right]$$\nWe observe that this linear operator $K$ is independent of the point of linearization. Furthermore, it does not depend on the parameters $x_0$ or $\\sigma$. The parameter $\\sigma$ only affects the affine part $C_t = E\\left[q \\int_t^T (x_0 + \\sigma\\,W_s)ds | \\mathcal{F}_t \\right]$. Therefore, the spectral radius of the linearized map will be independent of $\\sigma$.\n\nTo find the spectral radius of $K$, we look for its eigenvalues $\\lambda$ by solving the equation $KY = \\lambda Y$ for non-trivial eigenfunctions $Y \\in L^2\\left(\\Omega\\times\\left[0,T\\right]\\right)$.\n\nLet us first consider the subspace of deterministic functions, i.e., $Y_t = y(t)$ for some $y \\in L^2\\left(\\left[0,T\\right]\\right)$. For such functions, the conditional expectation is redundant:\n$$(Ky)(t) = q\\,\\alpha \\int_t^T \\left(\\int_0^s y(u)\\,du\\right)ds$$\nThis shows that the operator $K$ maps deterministic functions to deterministic functions. The restriction of $K$ to this subspace is a linear operator on $L^2\\left(\\left[0,T\\right]\\right)$, which we denote by $K_{det}$.\nThe eigenvalue equation for $K_{det}$ is:\n$$\\lambda\\,y(t) = q\\,\\alpha \\int_t^T \\left(\\int_0^s y(u)\\,du\\right)ds$$\nThis is an integral equation. We can convert it to a differential equation by differentiating with respect to $t$.\nDifferentiating once:\n$$\\lambda\\,y'(t) = -q\\,\\alpha \\int_0^t y(u)\\,du$$\nDifferentiating a second time:\n$$\\lambda\\,y''(t) = -q\\,\\alpha\\,y(t) \\implies y''(t) + \\frac{q\\,\\alpha}{\\lambda} y(t) = 0$$\nLet $\\omega^2 = \\frac{q\\,\\alpha}{\\lambda}$. The equation is $y''(t) + \\omega^2 y(t) = 0$.\n\nWe need to find the boundary conditions associated with the integral equation.\nFrom $\\lambda\\,y(t) = q\\,\\alpha \\int_t^T (\\dots)\\,ds$, setting $t=T$ gives $\\lambda\\,y(T) = 0$. Since we are looking for non-zero eigenvalues $\\lambda$, this implies $y(T)=0$.\nFrom $\\lambda\\,y'(t) = -q\\,\\alpha \\int_0^t y(u)\\,du$, setting $t=0$ gives $\\lambda\\,y'(0)=0$, which implies $y'(0)=0$.\n\nSo, we must solve the Sturm-Liouville boundary value problem:\n$$y''(t) + \\omega^2 y(t) = 0, \\quad y'(0)=0, \\quad y(T)=0$$\nThe general solution to the ODE is $y(t) = A\\,\\cos(\\omega\\,t) + B\\,\\sin(\\omega\\,t)$.\nThe first boundary condition $y'(0)=0$ requires:\n$y'(t) = -A\\,\\omega\\,\\sin(\\omega\\,t) + B\\,\\omega\\,\\cos(\\omega\\,t)$.\n$y'(0) = B\\,\\omega = 0$. For a non-trivial solution, we need $\\omega \\ne 0$, so we must have $B=0$.\nThe solution is of the form $y(t) = A\\,\\cos(\\omega\\,t)$.\nThe second boundary condition $y(T)=0$ requires:\n$A\\,\\cos(\\omega\\,T) = 0$.\nFor a non-trivial solution ($A \\ne 0$), we must have $\\cos(\\omega\\,T) = 0$.\nThis condition is met when $\\omega\\,T$ is an odd multiple of $\\pi/2$:\n$$\\omega_n T = \\left(n + \\frac{1}{2}\\right)\\pi, \\quad \\text{for } n = 0, 1, 2, \\dots$$\nSo, the allowed values for $\\omega$ are $\\omega_n = \\frac{(n + 1/2)\\pi}{T}$.\n\nThe eigenvalues $\\lambda_n$ are related to $\\omega_n$ by $\\lambda_n = \\frac{q\\,\\alpha}{\\omega_n^2}$:\n$$\\lambda_n = \\frac{q\\,\\alpha}{\\left(\\frac{(n + 1/2)\\pi}{T}\\right)^2} = \\frac{q\\,\\alpha\\,T^2}{(n + 1/2)^2\\pi^2}$$\nThe set of eigenvalues of the operator $K_{det}$ is the spectrum, as it is a compact, self-adjoint operator. The spectral radius $\\rho(K_{det})$ is the supremum of the absolute values of the eigenvalues. Since $q0$ and $\\alpha0$, all $\\lambda_n$ are positive. The largest eigenvalue corresponds to the smallest value of $n$, which is $n=0$:\n$$\\rho(K_{det}) = \\lambda_0 = \\frac{q\\,\\alpha\\,T^2}{(0 + 1/2)^2\\pi^2} = \\frac{q\\,\\alpha\\,T^2}{\\pi^2/4} = \\frac{4\\,q\\,\\alpha\\,T^2}{\\pi^2}$$\nIt can be shown that the spectrum of the full stochastic operator $K$ on $L^2(\\Omega \\times [0,T])$ is the same as the spectrum of its deterministic restriction $K_{det}$ on $L^2([0,T])$. Thus, the spectral radius of the linearized Picard map is $\\rho(K) = \\rho(K_{det})$.\n\nThe Picard iteration is a contraction if the norm of the linear operator is less than $1$. A sufficient condition for convergence is that the spectral radius $\\rho(K)  1$. The iteration may fail to converge if the map is not a contraction, which occurs if $\\rho(K) \\ge 1$.\nThe condition for non-convergence is therefore:\n$$\\rho = \\frac{4\\,q\\,\\alpha\\,T^2}{\\pi^2} \\ge 1$$\n$$\\implies q\\,\\alpha\\,T^2 \\ge \\frac{\\pi^2}{4}$$\nThis analysis matches option A.", "answer": "$$\\boxed{A}$$", "id": "2977074"}]}