{"hands_on_practices": [{"introduction": "The minorization condition is a cornerstone of Harris-type theorems, formalizing the idea that from any point in a \"small\" set, the process has a uniform chance of transitioning into any part of a given measure's support. For processes driven by SDEs, this property is in-timately linked to how randomness propagates through the system's dynamics. This exercise [@problem_id:2978599] demonstrates a critical case where a degenerate diffusion coefficient prevents minorization, and shows how coupling between state components can restore it by satisfying Hörmander's celebrated \"Lie bracket condition.\"", "problem": "Consider the time-homogeneous Markov process in $\\mathbb{R}^{2}$ generated by the following Stochastic Differential Equation (SDE) with degenerate diffusion coefficient:\n- For a fixed $t0$, let $\\{W_{s}\\}_{s\\ge 0}$ be a standard one-dimensional Brownian motion.\n- For an initial condition $x=(x_{1},x_{2})\\in\\mathbb{R}^{2}$, define $X_{s}^{x}=(X_{s,1}^{x},X_{s,2}^{x})$ by\n$$\n\\mathrm{d}X_{s,1}^{x} = \\mathrm{d}W_{s},\\qquad \\mathrm{d}X_{s,2}^{x} = 0,\\qquad X_{0}^{x}=x.\n$$\nLet $P_{t}(x,\\cdot)$ denote the transition probability kernel of $X_{t}^{x}$. Fix $t=1$ and the compact set\n$$\nC \\equiv \\{x\\in\\mathbb{R}^{2}:\\ |x_{1}|\\le 1,\\ |x_{2}|\\le 1\\}.\n$$\nDefine the minorization constant $\\varepsilon^{\\star}$ on $C$ at time $t=1$ by\n$$\n\\varepsilon^{\\star} \\equiv \\sup\\Big\\{\\varepsilon\\ge 0:\\ \\exists\\ \\text{a probability measure }\\nu\\ \\text{on }\\mathbb{R}^{2}\\ \\text{such that}\\ P_{1}(x,\\cdot)\\ge \\varepsilon\\,\\nu(\\cdot)\\ \\text{for all }x\\in C\\Big\\}.\n$$\n- Compute the value of $\\varepsilon^{\\star}$.\n\nIn your reasoning, start from first principles: the explicit form of the solution to the SDE, the definition of the transition kernel and its support, and the definition of the minorization condition. Then briefly indicate, without computing any new numerical constant, how one can restore a positive minorization constant on compact sets by modifying the second coordinate to\n$$\n\\mathrm{d}X_{s,2}^{x} = X_{s,1}^{x}\\,\\mathrm{d}s,\n$$\nand invoking structural conditions from Malliavin calculus or the Lie bracket condition of Hörmander. Your final answer must be the exact value of $\\varepsilon^{\\star}$ as a single real number with no units. No rounding is required.", "solution": "We begin by solving the given Stochastic Differential Equation (SDE) explicitly. The system is\n$$\n\\mathrm{d}X_{s,1}^{x}=\\mathrm{d}W_{s},\\qquad \\mathrm{d}X_{s,2}^{x}=0,\\qquad X_{0,1}^{x}=x_{1},\\ X_{0,2}^{x}=x_{2}.\n$$\nBy direct integration,\n$$\nX_{s,1}^{x}=x_{1}+W_{s},\\qquad X_{s,2}^{x}=x_{2},\\qquad s\\ge 0.\n$$\nTherefore, at time $t=1$, the law of $X_{1}^{x}$ is the product of a one-dimensional Gaussian in the first coordinate and a point mass in the second coordinate:\n$$\nP_{1}(x,\\cdot)=\\mathcal{N}(x_{1},1)\\otimes \\delta_{x_{2}}.\n$$\nIn particular, the support of $P_{1}(x,\\cdot)$ is the affine line\n$$\n\\mathrm{supp}\\,P_{1}(x,\\cdot)=\\{(y_{1},y_{2})\\in\\mathbb{R}^{2}:\\ y_{2}=x_{2}\\}.\n$$\nWe now apply the definition of the minorization condition. For a fixed set $C=\\{x:\\ |x_{1}|\\le 1,\\ |x_{2}|\\le 1\\}$, we ask for the largest $\\varepsilon^{\\star}\\ge 0$ such that there exists a probability measure $\\nu$ on $\\mathbb{R}^{2}$ with\n$$\nP_{1}(x,\\cdot)\\ge \\varepsilon\\,\\nu(\\cdot)\\quad\\text{for all }x\\in C.\n$$\nWe claim that $\\varepsilon^{\\star}=0$. To see this, we use the support structure of $P_{1}(x,\\cdot)$. Choose two points $x,y\\in C$ with $x_{2}\\neq y_{2}$; for instance, take $x=(0,0)$ and $y=(0,1)$. Then\n$$\n\\mathrm{supp}\\,P_{1}(x,\\cdot)=\\mathbb{R}\\times\\{x_{2}\\},\\qquad \\mathrm{supp}\\,P_{1}(y,\\cdot)=\\mathbb{R}\\times\\{y_{2}\\},\n$$\nand these two supports are disjoint because $x_{2}\\neq y_{2}$.\n\nAssume, for contradiction, that there exists some $\\varepsilon0$ and a probability measure $\\nu$ on $\\mathbb{R}^{2}$ such that\n$$\nP_{1}(x,\\cdot)\\ge \\varepsilon\\,\\nu(\\cdot)\\quad\\text{and}\\quad P_{1}(y,\\cdot)\\ge \\varepsilon\\,\\nu(\\cdot).\n$$\nLet $E\\equiv \\mathbb{R}\\times\\{x_{2}\\}$. Then $P_{1}(x,E)=1$ and $P_{1}(y,E)=0$ because $E$ is disjoint from $\\mathrm{supp}\\,P_{1}(y,\\cdot)$. The inequalities imply\n$$\n\\nu(E)\\le \\frac{1}{\\varepsilon}P_{1}(y,E)=0\\quad\\Rightarrow\\quad \\nu(E)=0.\n$$\nOn the other hand, for the complement $E^{c}$, we have $P_{1}(x,E^{c})=0$, hence\n$$\n\\nu(E^{c})\\le \\frac{1}{\\varepsilon}P_{1}(x,E^{c})=0\\quad\\Rightarrow\\quad \\nu(E^{c})=0.\n$$\nTherefore $\\nu(E)+\\nu(E^{c})=0$, which contradicts that $\\nu$ is a probability measure. Consequently, no $\\varepsilon0$ can satisfy the minorization for all $x\\in C$, and thus\n$$\n\\varepsilon^{\\star}=0.\n$$\n\nWe now indicate how structural conditions from Malliavin calculus or the Lie bracket condition of Hörmander can restore a positive minorization on compact sets by modifying the second coordinate to\n$$\n\\mathrm{d}X_{s,1}^{x}=\\mathrm{d}W_{s},\\qquad \\mathrm{d}X_{s,2}^{x}=X_{s,1}^{x}\\,\\mathrm{d}s.\n$$\nThis system is still driven by a one-dimensional Brownian motion, but the drift now couples the second coordinate to the first. The drift vector field is $V_{0}(x)=(0,x_{1})$ and the diffusion vector field is $V_{1}(x)=(1,0)$. The Lie bracket is\n$$\n[V_{0},V_{1}](x)=\\mathrm{D}V_{0}(x)\\,V_{1}(x)-\\mathrm{D}V_{1}(x)\\,V_{0}(x)=\\begin{pmatrix}00\\\\ 10\\end{pmatrix}\\begin{pmatrix}1\\\\ 0\\end{pmatrix}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}.\n$$\nHence $\\{V_{1},[V_{0},V_{1}]\\}$ spans $\\mathbb{R}^{2}$ at every $x$, so the (uniform) Lie bracket condition of Hörmander holds. From Malliavin calculus, this implies that for each $t0$, the Malliavin covariance matrix is invertible and the transition law admits a smooth density that is strictly positive on open sets.\n\nWe can see this directly by solving the linear system explicitly:\n$$\nX_{t,1}^{x}=x_{1}+W_{t},\\qquad X_{t,2}^{x}=x_{2}+\\int_{0}^{t}X_{s,1}^{x}\\,\\mathrm{d}s=x_{2}+x_{1}t+\\int_{0}^{t}W_{s}\\,\\mathrm{d}s.\n$$\nThus, $(X_{t,1}^{x},X_{t,2}^{x})$ is a nondegenerate Gaussian vector with mean $(x_{1},\\,x_{2}+x_{1}t)$ and covariance matrix\n$$\n\\Sigma_{t}=\\begin{pmatrix}\\mathrm{Var}(W_{t})  \\mathrm{Cov}(W_{t},\\int_{0}^{t}W_{s}\\,\\mathrm{d}s)\\\\\n\\mathrm{Cov}(W_{t},\\int_{0}^{t}W_{s}\\,\\mathrm{d}s)  \\mathrm{Var}(\\int_{0}^{t}W_{s}\\,\\mathrm{d}s)\\end{pmatrix}\n=\\begin{pmatrix} t  \\frac{t^{2}}{2}\\\\[4pt] \\frac{t^{2}}{2}  \\frac{t^{3}}{3}\\end{pmatrix}.\n$$\nIts determinant equals\n$$\n\\det(\\Sigma_{t})=t\\cdot \\frac{t^{3}}{3}-\\left(\\frac{t^{2}}{2}\\right)^{2}=\\frac{t^{4}}{12}0\\quad\\text{for all }t0,\n$$\nconfirming nondegeneracy. Because the covariance is fixed and positive definite for fixed $t$, and the means vary over a compact set when $x\\in C$, one can use continuity and Gaussian tail bounds to find a common Borel set $A\\subset \\mathbb{R}^{2}$ and a constant $\\varepsilon0$ such that\n$$\nP_{t}(x,A)\\ge \\varepsilon\\quad \\text{for all }x\\in C,\n$$\nwhich yields a minorization $P_{t}(x,\\cdot)\\ge \\varepsilon\\,\\nu(\\cdot)$ with $\\nu$ chosen as the restriction of Lebesgue measure to $A$ normalized to a probability measure. This illustrates how the degeneracy that destroyed minorization in the first system is repaired by the Hörmander bracket condition, equivalently by nondegeneracy of the Malliavin covariance in the modified system. However, for the original degenerate system, the computed optimal minorization constant on $C$ at time $t=1$ is exactly zero.", "answer": "$$\\boxed{0}$$", "id": "2978599"}, {"introduction": "Harris-type theorems guarantee convergence to a unique stationary distribution, but they do not always tell us what that distribution is. For many physical systems, we can postulate a candidate for the invariant measure based on statistical mechanics principles. This practice [@problem_id:2978611] provides a fundamental calculation, showing how to verify that the Gibbs-Boltzmann measure $\\pi(dx) \\propto \\exp(-U(x))dx$ is indeed the invariant measure for the overdamped Langevin SDE by demonstrating that its density lies in the null space of the Fokker-Planck operator $\\mathcal{L}^*$.", "problem": "Consider the overdamped Langevin Stochastic Differential Equation (SDE) on $\\mathbb{R}^{d}$ given by\n$$\ndX_{t}=-\\nabla U(X_{t})\\,dt+\\sqrt{2}\\,dW_{t},\n$$\nwhere $W_{t}$ is a standard $d$-dimensional Brownian motion and $U:\\mathbb{R}^{d}\\to\\mathbb{R}$ is a twice continuously differentiable function. Assume that $U$ is confining in the sense that there exists $p1$ and $c0$ such that $U(x)\\geq c|x|^{p}$ for all $|x|$ sufficiently large, and that all derivatives of $U$ up to order $2$ have at most polynomial growth. These assumptions ensure that $\\int_{\\mathbb{R}^{d}}\\exp(-U(x))\\,dx\\infty$ and justify integration by parts without boundary terms on $\\mathbb{R}^{d}$. Define the infinitesimal generator $\\mathcal{L}$ of the SDE on $C_{c}^{\\infty}(\\mathbb{R}^{d})$ by\n$$\n\\mathcal{L}f(x)=-\\nabla U(x)\\cdot\\nabla f(x)+\\Delta f(x),\n$$\nand let $\\mathcal{L}^{\\ast}$ denote the adjoint of $\\mathcal{L}$ with respect to Lebesgue measure, so that for smooth compactly supported $f$ and smooth integrable $\\rho$,\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx=\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx.\n$$\nThe Kolmogorov forward equation (KFE, also called the Fokker–Planck equation) for the law of $X_{t}$ is $\\partial_{t}\\rho_{t}=\\mathcal{L}^{\\ast}\\rho_{t}$. Consider the probability measure\n$$\n\\pi(dx)=Z^{-1}\\exp(-U(x))\\,dx,\\qquad Z=\\int_{\\mathbb{R}^{d}}\\exp(-U(x))\\,dx.\n$$\nUsing only the definitions above and first principles (integration by parts and differentiation rules), compute the explicit simplified expression of $(\\mathcal{L}^{\\ast}\\rho)(x)$ when $\\rho(x)=Z^{-1}\\exp(-U(x))$, and report the final simplified value. Your final answer must be a single real number or a single closed-form analytic expression. No rounding is required.", "solution": "The objective is to compute the explicit simplified expression for $(\\mathcal{L}^{\\ast}\\rho)(x)$ where $\\rho(x) = Z^{-1}\\exp(-U(x))$ and $\\mathcal{L}^{\\ast}$ is the formal adjoint of the infinitesimal generator $\\mathcal{L}$ with respect to the Lebesgue measure on $\\mathbb{R}^d$.\n\nThe operator $\\mathcal{L}$ is defined for a function $f \\in C_{c}^{\\infty}(\\mathbb{R}^{d})$ as:\n$$\n\\mathcal{L}f(x) = -\\nabla U(x)\\cdot\\nabla f(x)+\\Delta f(x)\n$$\nThe adjoint operator $\\mathcal{L}^{\\ast}$ is defined by the relation:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx=\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx\n$$\nfor any smooth compactly supported function $f$ and a suitable function $\\rho$. The problem asks for the evaluation of $(\\mathcal{L}^{\\ast}\\rho)(x)$ for the specific probability density $\\rho(x) = Z^{-1}\\exp(-U(x))$, where $Z = \\int_{\\mathbb{R}^{d}}\\exp(-U(x))\\,dx$.\n\nTo find $(\\mathcal{L}^{\\ast}\\rho)(x)$, we will evaluate the left-hand side of the adjoint definition, $\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx$, and manipulate the expression using integration by parts to move all derivatives from $f$ to the other terms. This will reveal the form of $(\\mathcal{L}^{\\ast}\\rho)(x)$.\n\nLet's substitute the expression for $\\mathcal{L}f(x)$ into the integral:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = \\int_{\\mathbb{R}^{d}} \\left[-\\nabla U(x)\\cdot\\nabla f(x)+\\Delta f(x)\\right] \\rho(x)\\,dx\n$$\nWe can split this integral into two parts:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = \\underbrace{\\int_{\\mathbb{R}^{d}} \\Delta f(x) \\rho(x)\\,dx}_{I_1} - \\underbrace{\\int_{\\mathbb{R}^{d}} (\\nabla U(x)\\cdot\\nabla f(x)) \\rho(x)\\,dx}_{I_2}\n$$\nLet's analyze the first integral, $I_1$. The term $\\Delta f(x)$ is the divergence of the gradient of $f(x)$, i.e., $\\Delta f(x) = \\nabla \\cdot (\\nabla f(x))$. We use integration by parts, which in this multidimensional setting is a consequence of the divergence theorem. For a vector field $\\mathbf{F}$ and a scalar function $\\phi$, we have $\\int_{V} \\phi (\\nabla \\cdot \\mathbf{F}) dV = -\\int_{V} \\mathbf{F} \\cdot (\\nabla \\phi) dV$ plus boundary terms. Since $f \\in C_{c}^{\\infty}(\\mathbb{R}^{d})$, $f$ and its derivatives are zero outside a compact set, which means all boundary terms at infinity vanish.\nApplying this with $\\phi = \\rho(x)$ and $\\mathbf{F} = \\nabla f(x)$:\n$$\nI_1 = \\int_{\\mathbb{R}^{d}} \\rho(x) (\\nabla \\cdot (\\nabla f(x)))\\,dx = - \\int_{\\mathbb{R}^{d}} \\nabla f(x) \\cdot \\nabla \\rho(x)\\,dx\n$$\nNow, we must compute the gradient of our specific $\\rho(x) = Z^{-1}\\exp(-U(x))$. Using the chain rule:\n$$\n\\nabla \\rho(x) = \\nabla \\left(Z^{-1}\\exp(-U(x))\\right) = Z^{-1}\\exp(-U(x)) \\cdot (-\\nabla U(x)) = -\\rho(x) \\nabla U(x)\n$$\nSubstituting this result back into the expression for $I_1$:\n$$\nI_1 = - \\int_{\\mathbb{R}^{d}} \\nabla f(x) \\cdot (-\\rho(x) \\nabla U(x))\\,dx = \\int_{\\mathbb{R}^{d}} \\rho(x) (\\nabla U(x) \\cdot \\nabla f(x))\\,dx\n$$\nNow let's look at the second integral, $I_2$:\n$$\nI_2 = \\int_{\\mathbb{R}^{d}} (\\nabla U(x)\\cdot\\nabla f(x)) \\rho(x)\\,dx\n$$\nComparing our results for $I_1$ and $I_2$, we see that they are identical:\n$$\nI_1 = I_2\n$$\nSubstituting this back into the expression for the full integral:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = I_1 - I_2 = I_1 - I_1 = 0\n$$\nWe have thus shown that for our specific choice of $\\rho(x) = Z^{-1}\\exp(-U(x))$, the following holds for any test function $f \\in C_{c}^{\\infty}(\\mathbb{R}^{d})$:\n$$\n\\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx = 0\n$$\nFrom the definition of the adjoint operator, we also have:\n$$\n\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx = \\int_{\\mathbb{R}^{d}}(\\mathcal{L}f)(x)\\,\\rho(x)\\,dx\n$$\nTherefore, we must have:\n$$\n\\int_{\\mathbb{R}^{d}}f(x)\\,(\\mathcal{L}^{\\ast}\\rho)(x)\\,dx = 0\n$$\nSince this equality holds for all functions $f$ in the dense set $C_{c}^{\\infty}(\\mathbb{R}^{d})$, it implies that the function $(\\mathcal{L}^{\\ast}\\rho)(x)$ must be zero for all $x \\in \\mathbb{R}^{d}$ (by the fundamental lemma of the calculus of variations).\n\nThe explicit simplified expression of $(\\mathcal{L}^{\\ast}\\rho)(x)$ is therefore $0$. This result is fundamental in the theory of stochastic processes, as it demonstrates that the Gibbs measure $\\pi(dx) = \\rho(x)dx$ is the stationary (or invariant) distribution for the Langevin dynamics, because its density $\\rho(x)$ is in the null space of the forward operator $\\mathcal{L}^{\\ast}$.\nThe final simplified value is the number $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "2978611"}, {"introduction": "Perhaps the most powerful feature of the modern Harris-theoretic framework is its ability to provide explicit, non-asymptotic convergence rates. The theory provides a recipe for translating qualitative assumptions—a drift condition ensuring stability and a minorization condition ensuring local mixing—into a quantitative bound on the convergence to equilibrium. In this exercise [@problem_id:2978638], you will apply this recipe to derive the constants $M$ and $\\beta$ for $V$-geometric ergodicity directly from the parameters of the drift and minorization conditions, making the abstract power of the theorem concrete.", "problem": "Consider the time-$1$ skeleton $\\{X_{n}\\}_{n \\geq 0}$ of a non-explosive, strong Markov solution to a stochastic differential equation on $\\mathbb{R}^{d}$ with a Feller transition kernel $P$. Let $V:\\mathbb{R}^{d} \\to [1,\\infty)$ be a measurable Lyapunov function and let $C \\subset \\mathbb{R}^{d}$ be a measurable set. Assume the following two properties hold:\n\n1. (Drift/Lyapunov condition) There exist constants $\\lambda \\in (0,1)$ and $b \\in (0,\\infty)$ such that\n$$\nP V(x) \\leq \\lambda V(x) + b \\,\\mathbf{1}_{C}(x)\n\\quad \\text{for all } x \\in \\mathbb{R}^{d}.\n$$\n\n2. (Minorization/small set condition) There exist a constant $\\epsilon \\in (0,1]$ and a probability measure $\\nu$ on $\\mathbb{R}^{d}$ such that\n$$\nP(x,\\cdot) \\geq \\epsilon \\,\\nu(\\cdot)\n\\quad \\text{for all } x \\in C,\n$$\nand the $\\nu$-moment $v_{\\nu} := \\int_{\\mathbb{R}^{d}} V(y) \\,\\nu(dy)$ is finite.\n\nDefine the weighted total variation norm of a signed measure $\\mu$ by\n$$\n\\|\\mu\\|_{V} := \\sup\\left\\{ \\left| \\int_{\\mathbb{R}^{d}} f(x) \\,\\mu(dx) \\right| : f:\\mathbb{R}^{d} \\to \\mathbb{R}, \\ |f(x)| \\leq V(x) \\ \\forall x \\right\\}.\n$$\n\nBy Harris-type theory and the standard Nummelin splitting construction, under the above hypotheses the Markov chain admits a unique invariant probability measure $\\pi$ and is $V$-geometrically ergodic: there exist finite constants $M0$ and $\\beta \\in (0,1)$ such that\n$$\n\\|P^{n}(x,\\cdot) - \\pi\\|_{V} \\leq M \\,V(x)\\, \\beta^{n}\n\\quad \\text{for all } x \\in \\mathbb{R}^{d}, \\ n \\in \\mathbb{N}.\n$$\n\nUsing only the drift and minorization information given above, and following the standard Meyn–Tweedie recipe based on Nummelin splitting and regeneration, derive explicit expressions for $M$ and $\\beta$ in terms of the constants $(\\epsilon,\\nu)$ and $(\\lambda,b)$ and $v_{\\nu}$. Your final answer must be a single analytical expression containing your choices of $M$ and $\\beta$ arranged as a row matrix. No numerical approximation is required.", "solution": "The user wants to find explicit expressions for the constants $M$ and $\\beta$ in the $V$-geometric ergodicity inequality for a Markov chain, based on its drift and minorization properties.\n\nThe problem asks for a derivation following the standard Meyn-Tweedie recipe, which is based on the Nummelin splitting construction and regeneration times. The core of this method is to establish a geometric moment bound for the first regeneration time $\\tau$. Specifically, we need to show that for some $\\rho  1$ and some constant $C$, the inequality $E_x[\\rho^\\tau] \\le C V(x)$ holds for all initial states $x \\in \\mathbb{R}^d$. Once this is established, standard results from renewal theory for Markov chains yield the desired $V$-geometric convergence rate.\n\nLet $\\tau$ be the first regeneration time. The construction of $\\tau$ involves two stages:\n1.  The time $\\tau_C = \\inf\\{n \\ge 0 : X_n \\in C\\}$ for the chain to first enter the small set $C$.\n2.  Once in $C$, the time until regeneration occurs. At each step $k$ where $X_{k-1} \\in C$, regeneration occurs with probability $\\epsilon$, independently of the past.\n\nThe relationship between the moment generating functions (MGFs) of these times is central. Let $\\tau'$ be the regeneration time for a chain starting in a state $z \\in C$. Its MGF is:\n$$\nE_z[\\rho^{\\tau'}] = \\sum_{k=1}^\\infty \\rho^k P_z(\\tau' = k) = \\sum_{k=1}^\\infty \\rho^k \\epsilon(1-\\epsilon)^{k-1} = \\epsilon\\rho \\sum_{j=0}^\\infty (\\rho(1-\\epsilon))^j\n$$\nThis geometric series converges if $|\\rho(1-\\epsilon)|  1$. Since $\\rho  1$ and $\\epsilon \\in (0,1]$, we require $\\rho  \\frac{1}{1-\\epsilon}$. If this holds, the MGF is constant for all $z \\in C$:\n$$\nE_z[\\rho^{\\tau'}] = \\frac{\\epsilon\\rho}{1-\\rho(1-\\epsilon)}\n$$\nThe MGF of the full regeneration time $\\tau$ starting from $x$ can be found by conditioning on the first hitting time $\\tau_C$ and the state $X_{\\tau_C}$:\n$$\nE_x[\\rho^\\tau] = E_x[E[\\rho^\\tau | \\mathcal{F}_{\\tau_C}]] = E_x[\\rho^{\\tau_C} E_{X_{\\tau_C}}[\\rho^{\\tau'}]]\n$$\nSince $E_{z}[\\rho^{\\tau'}]$ is constant for all $z=X_{\\tau_C} \\in C$, we have:\n$$\nE_x[\\rho^\\tau] = \\left(\\frac{\\epsilon\\rho}{1-\\rho(1-\\epsilon)}\\right) E_x[\\rho^{\\tau_C}]\n$$\nNext, we must bound the MGF of $\\tau_C$. This bound is derived from the drift condition. For any $k  \\tau_C$, the state $X_{k-1}$ is in the complement of $C$, denoted $C^c$. On this set, the drift condition is $PV(x) \\le \\lambda V(x)$. Since $V \\ge 1$ is given, we can bound the probability of not hitting $C$:\n$$\nP_x(\\tau_C  n) = P_x(X_0 \\notin C, \\dots, X_{n-1} \\notin C) = \\int_{C^c} \\dots \\int_{C^c} P(x,dx_1)\\dots P(x_{n-1},dx_n)\n$$\nLet $P_{C^c}$ be the kernel of the process killed upon entering $C$. $P_x(\\tau_C  n) = P_{C^c}^n(x, \\mathbb{R}^d)$.\nSince $V(y) \\ge 1$ for all $y$, we have $P_{C^c}^n(x, \\mathbb{R}^d) = \\int P_{C^c}^n(x,dy) \\cdot 1 \\le \\int P_{C^c}^n(x,dy) V(y) = P_{C^c}^n V(x)$.\nFor any $x$, $P_{C^c}V(x) = \\int_{C^c} V(y) P(x,dy) \\le \\int V(y) P(x,dy) = PV(x)$.\nIf $x \\in C^c$, $PV(x) \\le \\lambda V(x)$. Thus, $P_{C^c}V(x) \\le \\lambda V(x)$ for $x \\in C^c$.\nBy induction, $P_{C^c}^n V(x) \\le \\lambda^n V(x)$ for $x \\in C^c$. If $x \\in C$, $\\tau_C=0$ and the bound is trivial. For $x \\notin C$, we have:\n$$\nP_x(\\tau_C  n) \\le \\lambda^n V(x)\n$$\nThe MGF of $\\tau_C$ can be expressed using the tail probabilities: $E[\\rho^{\\tau_C}] = \\sum_{n=0}^\\infty P(\\rho^{\\tau_C} \\rho^n) \\approx \\sum_{n=0}^\\infty \\rho^n P(\\tau_C  n)$. More precisely:\n$$\nE_x[\\rho^{\\tau_C}] = \\sum_{n=1}^\\infty \\rho^n P_x(\\tau_C = n) = \\sum_{n=0}^\\infty (\\rho^{n+1}-\\rho^n) P_x(\\tau_C  n) = (\\rho-1) \\sum_{n=0}^\\infty \\rho^n P_x(\\tau_C  n)\n$$\nUsing our bound, for $x \\notin C$:\n$$\nE_x[\\rho^{\\tau_C}] \\le (\\rho-1) \\sum_{n=0}^\\infty \\rho^n \\lambda^n V(x) = (\\rho-1)V(x) \\sum_{n=0}^\\infty (\\rho\\lambda)^n\n$$\nThis series converges if $|\\rho\\lambda|1$, i.e., $\\rho  1/\\lambda$. Under this condition:\n$$\nE_x[\\rho^{\\tau_C}] \\le \\frac{\\rho-1}{1-\\rho\\lambda} V(x)\n$$\nThis bound also holds trivially for $x \\in C$, where $\\tau_C=0$ and $E_x[\\rho^{\\tau_C}]=1$, provided the constant factor is greater than $1/V(x)$, which is true for large $V(x)$. Note that since $V(x) \\ge 1$, we have $E_x[\\rho^{\\tau_C}]=1 \\le \\frac{\\rho-1}{1-\\rho\\lambda} V(x)$ if $\\frac{\\rho-1}{1-\\rho\\lambda} \\ge 1$, i.e., $\\rho-1 \\ge 1-\\rho\\lambda$, which means $\\rho(1+\\lambda) \\ge 2$. This may not hold, but the eventual bound for geometric ergodicity will absorb such details.\n\nCombining the bounds, we get for $\\rho \\in (1, \\min(1/\\lambda, 1/(1-\\epsilon)))$:\n$$\nE_x[\\rho^\\tau] \\le \\left(\\frac{\\epsilon\\rho}{1-\\rho(1-\\epsilon)}\\right) \\left(\\frac{\\rho-1}{1-\\rho\\lambda}\\right) V(x)\n$$\nThe existence of such a $\\rho1$ is guaranteed since $\\lambda \\in (0,1)$ and $\\epsilon \\in (0,1]$, so $\\min(1/\\lambda, 1/(1-\\epsilon))  1$.\n\nA standard result (e.g., from coupling proofs based on regeneration, or from renewal theory) states that if $E_x[\\rho^\\tau] \\le C V(x)$, then the chain is $V$-geometrically ergodic with a rate $\\beta = 1/\\rho$. The constant $M$ is of the form $M = C' (1 + \\pi(V))$, where $\\pi(V) = \\int V d\\pi$. The drift condition implies $E_x[V(X_n)] \\le \\lambda^n V(x) + \\frac{b}{1-\\lambda}$, and taking the limit $n \\to \\infty$ gives $\\pi(V) \\le \\frac{b}{1-\\lambda}$. A common form for $M$ derived via coupling is of the order $2 C (1+\\pi(V))$. Using $V(x) \\ge 1$, we can contain all additive constants within a factor multiplying $V(x)$. The relation $\\|P^n(x,\\cdot) - \\pi\\|_V \\le 2(E_x[\\rho^\\tau] + E_\\pi[\\rho^\\tau])\\rho^{-n}$ leads to:\n$$\n\\|P^n(x,\\cdot) - \\pi\\|_V \\le 2 \\rho^{-n} \\left( C V(x) + C \\pi(V)\\right) \\le 2C\\left(1+\\frac{b}{1-\\lambda}\\right) \\rho^{-n} V(x)\n$$\nLet us choose a rate of convergence $\\beta \\in (\\max(\\lambda, 1-\\epsilon), 1)$. Then we can set $\\rho = 1/\\beta$. This choice of $\\beta$ ensures that $\\rho  1/\\lambda$ and $\\rho  1/(1-\\epsilon)$, validating our calculations.\nWith $\\rho=1/\\beta$, the constant becomes:\n$$\nC = \\frac{\\epsilon \\beta^{-1}}{1-\\beta^{-1}(1-\\epsilon)} \\frac{\\beta^{-1}-1}{1-\\beta^{-1}\\lambda} = \\frac{\\epsilon}{\\beta-(1-\\epsilon)} \\frac{1-\\beta}{\\beta-\\lambda}\n$$\nAnd the pre-factor $M$ is:\n$$\nM = 2\\left(1+\\frac{b}{1-\\lambda}\\right)C = 2\\left(\\frac{1-\\lambda+b}{1-\\lambda}\\right) \\frac{\\epsilon(1-\\beta)}{(\\beta-(1-\\epsilon))(\\beta-\\lambda)}\n$$\nThe problem asks for explicit expressions for $M$ and $\\beta$. We can make a specific choice for $\\beta$ to satisfy the condition $\\beta \\in (\\max(\\lambda, 1-\\epsilon), 1)$. A simple choice is the midpoint of the interval:\n$$\n\\beta = \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2}\n$$\nWith this choice, we have explicit expressions for both $\\beta$ and $M$.\n\nLet's present the solution with $\\beta$ as a parameter in its valid range, as it is more general. Alternatively, we can substitute the chosen $\\beta$. The former is more robust.\nWe'll choose $\\beta$ as an arbitrary value in the valid range, for instance $\\beta = \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2}$. This gives a concrete answer as requested.\n\nLet $\\gamma = \\max(\\lambda, 1-\\epsilon)$. We choose $\\beta = \\frac{1+\\gamma}{2}$.\nThen $\\beta - \\gamma = \\frac{1+\\gamma}{2} - \\gamma = \\frac{1-\\gamma}{2}$.\nThe denominator in $M$ contains $(\\beta-(1-\\epsilon))(\\beta-\\lambda)$. Since $\\beta  \\lambda$ and $\\beta  1-\\epsilon$, both terms are positive.\n\n$M = 2 \\left( \\frac{1-\\lambda+b}{1-\\lambda} \\right) \\frac{\\epsilon \\left(1 - \\frac{1+\\gamma}{2}\\right)}{\\left(\\frac{1+\\gamma}{2} - (1-\\epsilon)\\right) \\left(\\frac{1+\\gamma}{2} - \\lambda\\right)} = 2 \\left( \\frac{1-\\lambda+b}{1-\\lambda} \\right) \\frac{\\epsilon \\frac{1-\\gamma}{2}}{\\left(\\frac{\\gamma-1+2\\epsilon}{2}\\right) \\left(\\frac{\\gamma-2\\lambda+1}{2}\\right)}$\n$M = \\frac{4\\epsilon(1-\\lambda+b)(1-\\gamma)}{(1-\\lambda)(\\gamma-1+2\\epsilon)(\\gamma-2\\lambda+1)}$\nThis expression simplifies depending on whether $\\gamma = \\lambda$ or $\\gamma = 1-\\epsilon$.\n\nCase 1: $\\lambda \\ge 1-\\epsilon$. Then $\\gamma=\\lambda$.\n$\\beta = \\frac{1+\\lambda}{2}$.\n$M = \\frac{4\\epsilon(1-\\lambda+b)(1-\\lambda)}{(1-\\lambda)(\\lambda-1+2\\epsilon)(\\lambda-2\\lambda+1)} = \\frac{4\\epsilon(1-\\lambda+b)}{(\\lambda-1+2\\epsilon)(1-\\lambda)}$.\n\nCase 2: $\\lambda  1-\\epsilon$. Then $\\gamma=1-\\epsilon$.\n$\\beta = \\frac{2-\\epsilon}{2}$.\n$M = \\frac{4\\epsilon(1-\\lambda+b)(1-(1-\\epsilon))}{(1-\\lambda)(1-\\epsilon-1+2\\epsilon)(1-\\epsilon-2\\lambda+1)} = \\frac{4\\epsilon^2(1-\\lambda+b)}{(1-\\lambda)\\epsilon(2-2\\lambda-\\epsilon)} = \\frac{4\\epsilon(1-\\lambda+b)}{(1-\\lambda)(2-2\\lambda-\\epsilon)}$.\n\nThis detailed case analysis provides an explicit, albeit complex, answer. For the final answer, a single unified expression using $\\max$ is preferable.\n\nLet $\\beta = \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2}$.\nThen $1-\\beta = \\frac{1-\\max(\\lambda, 1-\\epsilon)}{2}$.\n$\\beta-\\lambda = \\frac{1+\\max(\\lambda, 1-\\epsilon)-2\\lambda}{2}$.\n$\\beta-(1-\\epsilon) = \\frac{1+\\max(\\lambda, 1-\\epsilon)-2(1-\\epsilon)}{2} = \\frac{-1+\\max(\\lambda, 1-\\epsilon)+2\\epsilon}{2}$.\nSubstituting these into the expression for $M$:\n$M = 2\\left(\\frac{1-\\lambda+b}{1-\\lambda}\\right) \\frac{\\epsilon \\frac{1-\\max(\\lambda,1-\\epsilon)}{2}}{\\frac{-1+\\max(\\lambda, 1-\\epsilon)+2\\epsilon}{2} \\frac{1+\\max(\\lambda, 1-\\epsilon)-2\\lambda}{2}} = \\frac{4\\epsilon(1-\\lambda+b)(1-\\max(\\lambda, 1-\\epsilon))}{(1-\\lambda)(-1+2\\epsilon+\\max(\\lambda, 1-\\epsilon))(1-2\\lambda+\\max(\\lambda, 1-\\epsilon))}$.\nThis is a single analytical expression.", "answer": "$$ \\boxed{\\pmatrix{ \\frac{4\\epsilon(1-\\lambda+b)(1-\\max(\\lambda, 1-\\epsilon))}{(1-\\lambda)(-1+2\\epsilon+\\max(\\lambda, 1-\\epsilon))(1-2\\lambda+\\max(\\lambda, 1-\\epsilon))}  \\frac{1+\\max(\\lambda, 1-\\epsilon)}{2} }} $$", "id": "2978638"}]}