{"hands_on_practices": [{"introduction": "A crucial first step in analyzing any stochastic process is to understand its long-term behavior. This practice focuses on deriving the invariant probability measure for the Wright-Fisher diffusion, which describes the equilibrium distribution of allele frequencies in a population. By solving the stationary Fokker-Planck equation with appropriate boundary conditions [@problem_id:2981156], you will uncover the celebrated Beta distribution, a cornerstone result in population genetics.", "problem": "Consider a neutral Fleming–Viot limit of an interacting particle system on a one-dimensional type space, where the empirical type frequency converges to a one-dimensional diffusion of allele frequency. Let $X_{t} \\in [0,1]$ denote the frequency of a focal type at time $t$. Assume neutral resampling (no selection) and parent-independent linear mutation toward a baseline mean type level $\\mu \\in (0,1)$ with overall mutation intensity $\\theta0$. The macroscopic limit is a one-dimensional diffusion with generator\n$$\n\\mathcal{L} f(x) \\;=\\; \\frac{1}{2}\\,x(1-x)\\,f''(x) \\;+\\; \\frac{\\theta}{2}\\,(\\mu - x)\\,f'(x),\n$$\nacting on twice continuously differentiable test functions $f$ on $[0,1]$. The process is constrained by reflecting boundaries at $x=0$ and $x=1$ in the sense that the stationary probability current vanishes at the endpoints.\n\nUsing only the fundamental definitions of the Kolmogorov forward equation (also called the Fokker–Planck equation) for one-dimensional diffusions and the reflecting boundary condition interpreted as zero probability flux at the boundaries, compute the invariant probability density $\\pi(x)$ on $[0,1]$ of $X_{t}$. Express your final answer as a single closed-form analytic expression in terms of $x$, $\\theta$, and $\\mu$. No numerical approximation is required.", "solution": "The problem will first be validated for scientific soundness, well-posedness, and objectivity.\n\n### Step 1: Extract Givens\n\n-   **Process**: A neutral Fleming-Viot limit of an interacting particle system.\n-   **Type Space**: A one-dimensional space.\n-   **State Variable**: $X_{t} \\in [0,1]$ is the frequency of a focal type at time $t$.\n-   **Dynamics Assumptions**: Neutral resampling (no selection) and parent-independent linear mutation.\n-   **Mutation Parameters**: The baseline mean type level is $\\mu \\in (0,1)$, and the overall mutation intensity is $\\theta  0$.\n-   **Diffusion Generator**: The macroscopic limit is a one-dimensional diffusion with the generator $\\mathcal{L}$ acting on twice continuously differentiable test functions $f$ on $[0,1]$:\n    $$\n    \\mathcal{L} f(x) \\;=\\; \\frac{1}{2}\\,x(1-x)\\,f''(x) \\;+\\; \\frac{\\theta}{2}\\,(\\mu - x)\\,f'(x)\n    $$\n-   **Boundary Conditions**: The process has reflecting boundaries at $x=0$ and $x=1$, which is interpreted as the stationary probability current vanishing at the endpoints.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem describes the Wright-Fisher diffusion model with mutation, a canonical and extensively studied process in population genetics and probability theory. The generator $\\mathcal{L}$ is the standard generator for this diffusion. The concepts of an invariant measure, the Fokker-Planck equation, and reflecting boundaries are fundamental and rigorously defined within the theory of stochastic processes. The problem is firmly rooted in established mathematical and scientific principles.\n-   **Well-Posed**: The task is to find the unique stationary probability density for a given diffusion on a compact interval with specified boundary conditions. The generator is well-defined, and the parameters $\\theta  0$ and $\\mu \\in (0,1)$ are in their standard, valid ranges, ensuring a non-degenerate solution. The problem is structured to admit a unique, stable, and meaningful solution.\n-   **Objective**: The problem is stated using precise, formal mathematical language. It contains no subjective claims, ambiguities, or non-technical terms.\n\n### Step 3: Verdict and Action\n\nThe problem is scientifically sound, well-posed, objective, and self-contained. It is therefore deemed **valid**. A solution will be derived from first principles as requested.\n\nThe invariant (or stationary) probability density, denoted $\\pi(x)$, is the time-independent solution to the Kolmogorov forward equation, also known as the Fokker-Planck equation. For a one-dimensional diffusion process $X_t$ with drift coefficient $a(x)$ and diffusion coefficient $b(x)$, the Fokker-Planck equation for the probability density $p(x,t)$ is\n$$\n\\frac{\\partial p(x,t)}{\\partial t} = -\\frac{\\partial}{\\partial x} \\left[ a(x) p(x,t) \\right] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2} \\left[ b(x) p(x,t) \\right].\n$$\nThe generator of a diffusion is related to these coefficients by $\\mathcal{L}f(x) = a(x)f'(x) + \\frac{1}{2}b(x)f''(x)$. By comparing this general form with the given generator,\n$$\n\\mathcal{L} f(x) = \\frac{\\theta}{2}(\\mu - x) f'(x) + \\frac{1}{2}x(1-x) f''(x),\n$$\nwe identify the drift and diffusion coefficients for the process $X_t$:\n-   Drift coefficient: $a(x) = \\frac{\\theta}{2}(\\mu-x)$\n-   Diffusion coefficient: $b(x) = x(1-x)$\n\nThe stationary density $\\pi(x)$ is defined by the condition $\\frac{\\partial \\pi(x)}{\\partial t} = 0$. The stationary Fokker-Planck equation is therefore\n$$\n0 = -\\frac{d}{dx} \\left[ a(x) \\pi(x) \\right] + \\frac{1}{2}\\frac{d^2}{dx^2} \\left[ b(x) \\pi(x) \\right].\n$$\nIntegrating this equation with respect to $x$ yields\n$$\n\\text{constant} = -a(x) \\pi(x) + \\frac{1}{2}\\frac{d}{dx} \\left[ b(x) \\pi(x) \\right].\n$$\nThe right-hand side is the negative of the probability current, or flux, $J(x) = a(x)\\pi(x) - \\frac{1}{2}\\frac{d}{dx}[b(x)\\pi(x)]$. The stationary equation thus implies that the probability flux $J(x)$ must be constant for all $x \\in [0,1]$.\n\nThe problem specifies reflecting boundaries, which means that there is zero net flow of probability mass across the boundaries. This is enforced by setting the probability flux to zero at the endpoints: $J(0) = 0$ and $J(1)=0$. Since $J(x)$ must be constant, it must be that $J(x)=0$ for all $x \\in [0,1]$.\n\nWe must therefore solve the first-order ordinary differential equation:\n$$\na(x)\\pi(x) - \\frac{1}{2}\\frac{d}{dx}[b(x)\\pi(x)] = 0.\n$$\nSubstituting the expressions for $a(x)$ and $b(x)$:\n$$\n\\frac{\\theta}{2}(\\mu-x)\\pi(x) - \\frac{1}{2}\\frac{d}{dx}[x(1-x)\\pi(x)] = 0.\n$$\nMultiplying by $2$ and rearranging gives\n$$\n\\frac{d}{dx}[x(1-x)\\pi(x)] = \\theta(\\mu-x)\\pi(x).\n$$\nApplying the product rule to the left side:\n$$\nx(1-x)\\pi'(x) + (1-2x)\\pi(x) = \\theta(\\mu-x)\\pi(x).\n$$\nThis is a separable differential equation. We collect terms involving $\\pi(x)$:\n$$\nx(1-x)\\pi'(x) = [\\theta(\\mu-x) - (1-2x)]\\pi(x)\n$$\n$$\nx(1-x)\\pi'(x) = (\\theta\\mu - 1 + (2-\\theta)x)\\pi(x).\n$$\nSeparating variables $\\pi$ and $x$:\n$$\n\\frac{\\pi'(x)}{\\pi(x)} = \\frac{\\theta\\mu - 1 + (2-\\theta)x}{x(1-x)}.\n$$\nTo integrate the right-hand side, we use partial fraction decomposition:\n$$\n\\frac{\\theta\\mu - 1 + (2-\\theta)x}{x(1-x)} = \\frac{A}{x} + \\frac{B}{1-x}.\n$$\nSolving for $A$ and $B$:\n-   $A = \\left.\\frac{\\theta\\mu - 1 + (2-\\theta)x}{1-x}\\right|_{x=0} = \\theta\\mu - 1$.\n-   $B = \\left.\\frac{\\theta\\mu - 1 + (2-\\theta)x}{x}\\right|_{x=1} = \\theta\\mu - 1 + 2 - \\theta = \\theta(1-\\mu) - 1$.\n\nSubstituting back and integrating both sides:\n$$\n\\int \\frac{d\\pi}{\\pi} = \\int \\left( \\frac{\\theta\\mu - 1}{x} + \\frac{\\theta(1-\\mu) - 1}{1-x} \\right) dx.\n$$\n$$\n\\ln(\\pi(x)) = (\\theta\\mu - 1)\\ln(x) - (\\theta(1-\\mu) - 1)\\ln(1-x) + C_1,\n$$\nwhere $C_1$ is the constant of integration. For $x \\in (0,1)$, the arguments of the logarithms are positive.\nExponentiating both sides:\n$$\n\\pi(x) = \\exp(C_1) \\cdot \\exp((\\theta\\mu - 1)\\ln(x)) \\cdot \\exp(-(\\theta(1-\\mu) - 1)\\ln(1-x))\n$$\n$$\n\\pi(x) = C \\cdot x^{\\theta\\mu-1} (1-x)^{\\theta(1-\\mu)-1},\n$$\nwhere $C = \\exp(C_1)$ is a normalization constant.\n\nThe constant $C$ is determined by the condition that $\\pi(x)$ must be a probability density, so its integral over the state space $[0,1]$ must equal $1$:\n$$\n\\int_0^1 \\pi(x) dx = \\int_0^1 C \\cdot x^{\\theta\\mu-1} (1-x)^{\\theta(1-\\mu)-1} dx = 1.\n$$\nThe integral is the definition of the Beta function, $B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1} dt$, with parameters $\\alpha = \\theta\\mu$ and $\\beta = \\theta(1-\\mu)$.\nTherefore,\n$$\nC \\cdot B(\\theta\\mu, \\theta(1-\\mu)) = 1.\n$$\nThe Beta function is related to the Gamma function $\\Gamma(z)$ by $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$. In our case, $\\alpha+\\beta = \\theta\\mu + \\theta(1-\\mu) = \\theta$.\nSo,\n$$\nC \\cdot \\frac{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))}{\\Gamma(\\theta)} = 1.\n$$\nSolving for $C$:\n$$\nC = \\frac{\\Gamma(\\theta)}{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))}.\n$$\nSubstituting this normalization constant back into the expression for $\\pi(x)$, we obtain the final form of the invariant probability density:\n$$\n\\pi(x) = \\frac{\\Gamma(\\theta)}{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))} x^{\\theta\\mu - 1} (1-x)^{\\theta(1-\\mu) - 1}.\n$$\nThis is the probability density function for a Beta distribution with shape parameters $\\alpha = \\theta\\mu$ and $\\beta = \\theta(1-\\mu)$.", "answer": "$$\n\\boxed{\\frac{\\Gamma(\\theta)}{\\Gamma(\\theta\\mu)\\Gamma(\\theta(1-\\mu))} x^{\\theta\\mu - 1} (1-x)^{\\theta(1-\\mu) - 1}}\n$$", "id": "2981156"}, {"introduction": "To understand the dynamics of a process, we must characterize its infinitesimal evolution, which is encapsulated by its generator. This exercise guides you through the application of Itô's formula to functionals of the Fleming-Viot measure-valued process, allowing you to derive the generator directly from its martingale problem formulation [@problem_id:2981136]. Mastering this technique is essential for working with the dynamics of measure-valued diffusions and understanding the interplay between mutation and genetic drift.", "problem": "Consider a neutral Fleming–Viot measure-valued diffusion $\\{\\mu_t:t\\ge 0\\}$ on a compact Polish type space $E$ with mutation described by the generator $A$ of a conservative Feller semigroup on $C_b(E)$ and resampling rate $\\kappa0$. Assume that for every $\\phi\\in\\mathcal{D}(A)$, the process\n$$\nM_t(\\phi) \\equiv \\langle \\mu_t,\\phi\\rangle - \\langle \\mu_0,\\phi\\rangle - \\int_0^t \\langle \\mu_s, A\\phi\\rangle\\,ds\n$$\nis a square-integrable martingale with quadratic covariation specified by\n$$\nd\\langle M(\\phi),M(\\psi)\\rangle_t \\;=\\; \\kappa\\left(\\langle \\mu_t,\\phi\\psi\\rangle - \\langle \\mu_t,\\phi\\rangle\\,\\langle \\mu_t,\\psi\\rangle\\right)\\,dt,\n\\qquad \\phi,\\psi\\in\\mathcal{D}(A).\n$$\nFix $m\\in\\mathbb{N}$ and functions $\\phi_1,\\ldots,\\phi_m\\in\\mathcal{D}(A)\\cap C_b(E)$. Consider the polynomial functional on probability measures\n$$\nF(\\mu)\\;=\\;\\prod_{i=1}^m \\langle \\mu,\\phi_i\\rangle.\n$$\nUsing only the martingale characterization above, the multivariate Itô formula for semimartingales, and the product rule for derivatives, compute the drift coefficient in the semimartingale decomposition of $F(\\mu_t)$ and thereby identify the action of the generator $G$ of the Fleming–Viot process on $F$. Your final answer must be a single closed-form analytic expression for $G F(\\mu)$ in terms of $\\mu$, $\\kappa$, $A$, and the family $\\{\\phi_i\\}_{i=1}^m$.", "solution": "The problem is valid. It presents a standard, well-posed question in the theory of measure-valued stochastic processes, specifically the characterization of the generator of a Fleming–Viot process. All terms are mathematically precise, and the premises are consistent with established theory.\n\nThe objective is to find the action of the generator $G$ of the Fleming–Viot process $\\{\\mu_t\\}_{t \\ge 0}$ on the polynomial functional $F(\\mu) = \\prod_{i=1}^m \\langle \\mu, \\phi_i \\rangle$. The generator $G$ is identified from the drift term in the semimartingale decomposition of the process $Y_t = F(\\mu_t)$. The decomposition of $Y_t$ is of the form $dY_t = G F(\\mu_t) dt + d(\\text{martingale})_t$. We will compute this decomposition using the multivariate Itô formula.\n\nLet us define the vector of real-valued processes $X_t = (X_t^{(1)}, \\ldots, X_t^{(m)})$, where each component is given by $X_t^{(i)} = \\langle \\mu_t, \\phi_i \\rangle$ for $i \\in \\{1, \\ldots, m\\}$. The functional $F(\\mu_t)$ can then be written as a function of $X_t$:\n$$\nF(\\mu_t) = f(X_t^{(1)}, \\ldots, X_t^{(m)}) = \\prod_{i=1}^m X_t^{(i)}.\n$$\nThe problem provides the martingale characterization of the Fleming–Viot process. For any $\\phi \\in \\mathcal{D}(A)$, the process $M_t(\\phi) = \\langle \\mu_t, \\phi \\rangle - \\langle \\mu_0, \\phi \\rangle - \\int_0^t \\langle \\mu_s, A\\phi \\rangle \\,ds$ is a martingale. In differential form, the semimartingale decomposition for each $X_t^{(i)}$ is:\n$$\ndX_t^{(i)} = \\langle \\mu_t, A\\phi_i \\rangle \\,dt + dM_t(\\phi_i).\n$$\nThis decomposition separates the process into a finite-variation drift part and a martingale part. The quadratic covariation between the martingale parts $M_t(\\phi_i)$ and $M_t(\\phi_j)$ is given, which is identical to the quadratic covariation of the processes $X_t^{(i)}$ and $X_t^{(j)}$ themselves. For any $i,j \\in \\{1,\\ldots,m\\}$:\n$$\nd\\langle X^{(i)}, X^{(j)} \\rangle_t = d\\langle M(\\phi_i), M(\\phi_j) \\rangle_t = \\kappa \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - \\langle \\mu_t, \\phi_i \\rangle \\langle \\mu_t, \\phi_j \\rangle \\right) dt.\n$$\nWe apply the multivariate Itô formula to $Y_t = f(X_t)$:\n$$\ndY_t = \\sum_{i=1}^m \\frac{\\partial f}{\\partial x_i}(X_t) \\,dX_t^{(i)} + \\frac{1}{2} \\sum_{i,j=1}^m \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(X_t) \\,d\\langle X^{(i)}, X^{(j)} \\rangle_t.\n$$\nThe function is $f(x_1, \\ldots, x_m) = \\prod_{k=1}^m x_k$. We compute its partial derivatives using the product rule:\nThe first-order partial derivative with respect to $x_i$ is:\n$$\n\\frac{\\partial f}{\\partial x_i} = \\prod_{k \\ne i} x_k = \\frac{f(x_1, \\ldots, x_m)}{x_i}.\n$$\nThe second-order partial derivatives are:\nFor $i \\ne j$:\n$$\n\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\prod_{k \\ne i} x_k \\right) = \\prod_{k \\ne i, j} x_k = \\frac{f(x_1, \\ldots, x_m)}{x_i x_j}.\n$$\nFor $i=j$:\n$$\n\\frac{\\partial^2 f}{\\partial x_i^2} = \\frac{\\partial}{\\partial x_i} \\left( \\prod_{k \\ne i} x_k \\right) = 0.\n$$\nNow, we substitute these derivatives evaluated at $X_t$, along with the expressions for $dX_t^{(i)}$ and $d\\langle X^{(i)}, X^{(j)} \\rangle_t$, into the Itô formula.\n\nThe first term of Itô's formula becomes:\n$$\n\\sum_{i=1}^m \\frac{\\partial f}{\\partial x_i}(X_t) \\,dX_t^{(i)} = \\sum_{i=1}^m \\left( \\prod_{k \\ne i} X_t^{(k)} \\right) dX_t^{(i)} = \\sum_{i=1}^m \\left( \\prod_{k \\ne i} \\langle \\mu_t, \\phi_k \\rangle \\right) \\left( \\langle \\mu_t, A\\phi_i \\rangle \\,dt + dM_t(\\phi_i) \\right).\n$$\nThe second term of Itô's formula, noting that the second partials are non-zero only for $i \\ne j$, becomes:\n$$\n\\frac{1}{2} \\sum_{i \\ne j} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(X_t) \\,d\\langle X^{(i)}, X^{(j)} \\rangle_t = \\frac{1}{2} \\sum_{i \\ne j} \\left( \\prod_{k \\ne i,j} X_t^{(k)} \\right) \\kappa \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - X_t^{(i)} X_t^{(j)} \\right) dt.\n$$\nSubstituting $X_t^{(i)} = \\langle \\mu_t, \\phi_i \\rangle$:\n$$\n\\frac{\\kappa}{2} \\sum_{i \\ne j} \\left( \\prod_{k \\ne i,j} \\langle \\mu_t, \\phi_k \\rangle \\right) \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - \\langle \\mu_t, \\phi_i \\rangle \\langle \\mu_t, \\phi_j \\rangle \\right) dt.\n$$\nCombining these results gives the full stochastic differential for $Y_t = F(\\mu_t)$:\n$$\ndY_t = \\sum_{i=1}^m \\left(\\prod_{k \\ne i} \\langle \\mu_t, \\phi_k \\rangle\\right) \\langle \\mu_t, A\\phi_i \\rangle \\,dt + \\frac{\\kappa}{2} \\sum_{i \\ne j} \\left(\\prod_{k \\ne i,j} \\langle \\mu_t, \\phi_k \\rangle\\right) \\left( \\langle \\mu_t, \\phi_i \\phi_j \\rangle - \\langle \\mu_t, \\phi_i \\rangle \\langle \\mu_t, \\phi_j \\rangle \\right) dt + d(\\text{martingale})_t.\n$$\nThe drift coefficient of this process is, by definition, the action of the generator $G$ on the functional $F$. We can write the expression for $GF(\\mu)$ by taking the coefficient of $dt$ and replacing $\\mu_t$ with a generic measure $\\mu$:\n$$\nGF(\\mu) = \\sum_{i=1}^m \\left(\\prod_{k \\ne i} \\langle \\mu, \\phi_k \\rangle\\right) \\langle \\mu, A\\phi_i \\rangle + \\frac{\\kappa}{2} \\sum_{i \\ne j} \\left(\\prod_{k \\ne i,j} \\langle \\mu, \\phi_k \\rangle\\right) \\left( \\langle \\mu, \\phi_i \\phi_j \\rangle - \\langle \\mu, \\phi_i \\rangle \\langle \\mu, \\phi_j \\rangle \\right).\n$$\nThe sum $\\sum_{i \\ne j}$ runs over all $m(m-1)$ ordered pairs $(i,j)$ where $i \\ne j$. The summand is symmetric with respect to swapping $i$ and $j$. Therefore, we can rewrite the sum over the $\\binom{m}{2}$ unordered pairs $\\{i,j\\}$ with $ij$, doubling the result:\n$$\n\\sum_{i \\ne j} (\\cdot)_{ij} = \\sum_{1 \\le i  j \\le m} \\left( (\\cdot)_{ij} + (\\cdot)_{ji} \\right) = 2 \\sum_{1 \\le i  j \\le m} (\\cdot)_{ij}.\n$$\nApplying this to the second term yields:\n$$\nGF(\\mu) = \\sum_{i=1}^m \\left(\\prod_{k \\ne i} \\langle \\mu, \\phi_k \\rangle\\right) \\langle \\mu, A\\phi_i \\rangle + \\kappa \\sum_{1 \\le i  j \\le m} \\left(\\prod_{k \\ne i,j} \\langle \\mu, \\phi_k \\rangle\\right) \\left( \\langle \\mu, \\phi_i \\phi_j \\rangle - \\langle \\mu, \\phi_i \\rangle \\langle \\mu, \\phi_j \\rangle \\right).\n$$\nThis is the final expression for the action of the generator $G$ on the functional $F$. The first part corresponds to the generator for mutation, and the second part corresponds to the generator for resampling (genetic drift).", "answer": "$$\n\\boxed{\\sum_{i=1}^m \\langle \\mu, A\\phi_i \\rangle \\left(\\prod_{k \\ne i} \\langle \\mu, \\phi_k \\rangle\\right) + \\kappa \\sum_{1 \\le i  j \\le m} \\left( \\langle \\mu, \\phi_i \\phi_j \\rangle - \\langle \\mu, \\phi_i \\rangle \\langle \\mu, \\phi_j \\rangle \\right) \\left(\\prod_{k \\ne i,j} \\langle \\mu, \\phi_k \\rangle\\right)}\n$$", "id": "2981136"}, {"introduction": "Beyond the stationary state, a complete description of a diffusion process is given by its transition density. This advanced practice delves into the spectral theory of the Wright-Fisher generator to construct its transition density explicitly [@problem_id:2981119]. You will see how the problem can be mapped to the classical Jacobi differential equation, revealing a deep connection between population genetics models and the theory of orthogonal polynomials.", "problem": "Consider the two-type Wright–Fisher diffusion, which is the two-dimensional marginal of the Fleming–Viot process on a finite type space. Let $X_{t} \\in [0,1]$ denote the frequency of type $1$ in a population with parent-independent mutation, with positive mutation parameters $\\alpha0$ and $\\beta0$. The infinitesimal generator $\\mathcal{L}$ of $X_{t}$ acts on twice continuously differentiable functions $f$ as\n$$\n\\mathcal{L} f(x) \\;=\\; \\frac{1}{2}\\,x(1-x)\\,f''(x) \\;+\\; \\frac{1}{2}\\,\\big(\\beta(1-x) - \\alpha x\\big)\\,f'(x), \\quad x \\in (0,1).\n$$\nIt is known that the invariant probability density $\\pi$ of this diffusion is the Beta distribution with parameters $(\\beta,\\alpha)$, given by\n$$\n\\pi(x) \\;=\\; \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\\,x^{\\beta-1}\\,(1-x)^{\\alpha-1}, \\quad x \\in (0,1),\n$$\nwhere $\\Gamma$ denotes the Gamma function. The generator $\\mathcal{L}$ is self-adjoint on $L^{2}(\\pi)$ and has a complete orthogonal basis of polynomial eigenfunctions.\n\nStarting from fundamental definitions of self-adjoint generators in divergence form and the Sturm–Liouville theory for orthogonal polynomials, derive the explicit spectral representation of the transition density $p_{t}(x,y)$ of the Wright–Fisher diffusion with respect to the Lebesgue measure, expressed in terms of Jacobi polynomials $P_{n}^{(\\alpha-1,\\beta-1)}$. Your derivation must:\n\n- Establish the divergence-form (self-adjoint) representation of $\\mathcal{L}$ with respect to $\\pi$ and reduce the eigenvalue problem to the Jacobi differential equation on $[-1,1]$ by the change of variables $z=2x-1$.\n- Identify all eigenvalues $\\lambda_{n}$ and corresponding eigenfunctions in terms of Jacobi polynomials, and determine the precise orthogonality relations in $L^{2}(\\pi)$.\n- Compute the normalization constants of the eigenfunctions explicitly by evaluating the Jacobi norms and mapping them to the $x$-variable under $\\pi$.\n- Assemble the spectral expansion to obtain the explicit series expression for the transition density $p_{t}(x,y)$ with respect to the Lebesgue measure on $[0,1]$.\n\nVerify the orthogonality relations of the Jacobi polynomials under the weight $\\pi(x)$ and ensure that the resulting transition density integrates to $1$ in $y$, for each fixed $x$ and $t0$.\n\nProvide your final answer as a single closed-form analytic expression for $p_{t}(x,y)$ in terms of $\\alpha$, $\\beta$, $t$, $x$, $y$, and Jacobi polynomials. No numerical approximation is required.", "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded, well-posed, objective, and internally consistent, providing all necessary information for a rigorous derivation. We may therefore proceed with the solution.\n\nThe objective is to derive the spectral representation of the transition density $p_t(x,y)$ for the two-type Wright–Fisher diffusion on the interval $[0,1]$. The process is described by the infinitesimal generator $\\mathcal{L}$ acting on a suitable function $f(x)$ as:\n$$\n\\mathcal{L} f(x) = \\frac{1}{2}x(1-x) f''(x) + \\frac{1}{2}(\\beta(1-x) - \\alpha x) f'(x)\n$$\nfor $x \\in (0,1)$, with mutation parameters $\\alpha  0$ and $\\beta  0$. The invariant probability density is the Beta distribution $\\pi(x)$ with parameters $(\\beta, \\alpha)$:\n$$\n\\pi(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\beta-1}(1-x)^{\\alpha-1}\n$$\n\n**1. Self-Adjoint Form and Reduction to the Jacobi Equation**\n\nThe generator $\\mathcal{L}$ is self-adjoint in the Hilbert space $L^2(\\pi)$ of functions square-integrable with respect to the measure $\\pi(x)dx$. A differential operator of the form $\\mathcal{L}f(x) = a(x)f''(x) + b(x)f'(x)$ is self-adjoint with respect to a weight function $\\pi(x)$ if it can be written in the divergence or Sturm-Liouville form $\\mathcal{L}f(x) = \\frac{1}{\\pi(x)} \\frac{d}{dx} \\left( a(x)\\pi(x) \\frac{df}{dx} \\right)$. This requires that the coefficient of $f'(x)$ be $b(x) = \\frac{d}{dx}a(x) + a(x)\\frac{\\pi'(x)}{\\pi(x)}$.\n\nIn our case, $a(x) = \\frac{1}{2}x(1-x)$ and $b(x) = \\frac{1}{2}(\\beta(1-x) - \\alpha x)$. The logarithmic derivative of $\\pi(x)$ is:\n$$\n\\frac{\\pi'(x)}{\\pi(x)} = \\frac{d}{dx} \\big( (\\beta-1)\\ln(x) + (\\alpha-1)\\ln(1-x) + \\text{const.} \\big) = \\frac{\\beta-1}{x} - \\frac{\\alpha-1}{1-x}\n$$\nThus, $a(x)\\pi'(x)/\\pi(x)$ is:\n$$\n\\frac{1}{2}x(1-x) \\left( \\frac{\\beta-1}{x} - \\frac{\\alpha-1}{1-x} \\right) = \\frac{1}{2}\\big( (\\beta-1)(1-x) - (\\alpha-1)x \\big) = \\frac{1}{2}\\big( \\beta-1 - (\\alpha+\\beta-2)x \\big)\n$$\nAlso, $\\frac{d}{dx}a(x) = \\frac{d}{dx} \\left( \\frac{1}{2}(x-x^2) \\right) = \\frac{1}{2}(1-2x)$.\nSumming these two terms, we get:\n$$\n\\frac{d}{dx}a(x) + a(x)\\frac{\\pi'(x)}{\\pi(x)} = \\frac{1}{2}(1-2x) + \\frac{1}{2}\\big( \\beta-1 - (\\alpha+\\beta-2)x \\big) = \\frac{1}{2}(\\beta - (\\alpha+\\beta)x) = b(x)\n$$\nThis confirms the condition for self-adjointness, and the generator can be written as:\n$$\n\\mathcal{L}f(x) = \\frac{1}{\\pi(x)} \\frac{d}{dx} \\left(\\frac{1}{2}x(1-x)\\pi(x) f'(x)\\right)\n$$\nThe eigenvalue problem is $\\mathcal{L}f_n(x) = \\lambda_n f_n(x)$. We seek polynomial solutions.\nTo solve this Sturm-Liouville problem, we perform the change of variables $z = 2x-1$, which maps $x \\in (0,1)$ to $z \\in (-1,1)$. Let $f(x) = g(z)$. The derivatives transform as:\n$$\n\\frac{df}{dx} = \\frac{dg}{dz}\\frac{dz}{dx} = 2g'(z) \\quad \\text{and} \\quad \\frac{d^2f}{dx^2} = 4g''(z)\n$$\nThe coefficient terms transform as:\n$$\nx(1-x) = \\frac{1+z}{2} \\left(1-\\frac{1+z}{2}\\right) = \\frac{1-z^2}{4}\n$$\n$$\n\\beta(1-x) - \\alpha x = \\beta\\left(\\frac{1-z}{2}\\right) - \\alpha\\left(\\frac{1+z}{2}\\right) = \\frac{(\\beta-\\alpha) - (\\alpha+\\beta)z}{2}\n$$\nSubstituting these into the eigenvalue equation $\\mathcal{L}f = \\lambda f$:\n$$\n\\frac{1}{2}\\left(\\frac{1-z^2}{4}\\right)(4g''(z)) + \\frac{1}{2}\\left(\\frac{(\\beta-\\alpha) - (\\alpha+\\beta)z}{2}\\right)(2g'(z)) = \\lambda g(z)\n$$\nMultiplying by $2$ simplifies the equation to:\n$$\n(1-z^2)g''(z) + \\big((\\beta-\\alpha) - (\\alpha+\\beta)z\\big)g'(z) - 2\\lambda g(z) = 0\n$$\nThis is the Jacobi differential equation:\n$$\n(1-z^2)y'' + \\big(B-A - (A+B+2)z\\big)y' + n(n+A+B+1)y = 0\n$$\nwhose solutions are the Jacobi polynomials $P_n^{(A,B)}(z)$. By comparing coefficients, we identify the parameters:\n$B-A = \\beta-\\alpha$\n$A+B+2 = \\alpha+\\beta \\implies A+B = \\alpha+\\beta-2$\nSolving this system gives $A = \\alpha-1$ and $B = \\beta-1$.\n\nThe eigenvalue $\\lambda_n$ is obtained by comparing the coefficients of the last term for $n \\in \\{0, 1, 2, ...\\}$:\n$$\n-2\\lambda_n = n(n+A+B+1) = n(n+(\\alpha-1)+(\\beta-1)+1) = n(n+\\alpha+\\beta-1)\n$$\n$$\n\\lambda_n = -\\frac{1}{2} n(n+\\alpha+\\beta-1)\n$$\n\n**2. Eigenfunctions and Orthogonality**\n\nThe eigenfunctions of $\\mathcal{L}$ are thus given by the Jacobi polynomials with the identified parameters, transformed back to the $x$ variable:\n$$\n\\phi_n(x) = P_n^{(\\alpha-1, \\beta-1)}(2x-1)\n$$\nThe Sturm-Liouville theory guarantees that these eigenfunctions form a complete orthogonal basis for the space $L^2(\\pi)$. The orthogonality relation is:\n$$\n\\langle \\phi_n, \\phi_m \\rangle_{\\pi} = \\int_0^1 \\phi_n(x)\\phi_m(x)\\pi(x)dx = 0 \\quad \\text{for } n \\neq m.\n$$\nThis can be explicitly verified by transforming the integral to the $z$ variable:\n$dx = dz/2$ and $\\pi(x) \\propto x^{\\beta-1}(1-x)^{\\alpha-1} \\propto (1+z)^{\\beta-1}(1-z)^{\\alpha-1}$. The integral becomes proportional to the standard orthogonality integral for Jacobi polynomials $P_n^{(\\alpha-1, \\beta-1)}(z)$ with weight function $(1-z)^{\\alpha-1}(1+z)^{\\beta-1}$.\n\n**3. Normalization of Eigenfunctions**\n\nTo construct the spectral expansion, we must compute the squared norm $||\\phi_n||^2_\\pi = \\langle \\phi_n, \\phi_n \\rangle_{\\pi}$.\n$$\n||\\phi_n||^2_\\pi = \\int_0^1 \\left(P_n^{(\\alpha-1, \\beta-1)}(2x-1)\\right)^2 \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\beta-1}(1-x)^{\\alpha-1} dx\n$$\nChanging variables to $z=2x-1$:\n$$\n||\\phi_n||^2_\\pi = \\int_{-1}^1 \\left(P_n^{(\\alpha-1, \\beta-1)}(z)\\right)^2 \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\left(\\frac{1+z}{2}\\right)^{\\beta-1} \\left(\\frac{1-z}{2}\\right)^{\\alpha-1} \\frac{dz}{2}\n$$\n$$\n||\\phi_n||^2_\\pi = \\frac{\\Gamma(\\alpha+\\beta)}{2^{\\alpha+\\beta-1}\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_{-1}^1 \\left(P_n^{(\\alpha-1, \\beta-1)}(z)\\right)^2 (1-z)^{\\alpha-1}(1+z)^{\\beta-1} dz\n$$\nThe standard formula for the norm of Jacobi polynomials $P_n^{(A,B)}(z)$ is:\n$$\n\\int_{-1}^1 (P_n^{(A,B)}(z))^2 (1-z)^A (1+z)^B dz = \\frac{2^{A+B+1}}{2n+A+B+1} \\frac{\\Gamma(n+A+1)\\Gamma(n+B+1)}{n! \\Gamma(n+A+B+1)}\n$$\nSubstituting $A=\\alpha-1$ and $B=\\beta-1$:\n$$\n\\int_{-1}^1 \\dots dz = \\frac{2^{\\alpha+\\beta-1}}{2n+\\alpha+\\beta-1} \\frac{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}{n! \\Gamma(n+\\alpha+\\beta-1)}\n$$\nCombining this with the pre-factor gives the norm in $L^2(\\pi)$:\n$$\n||\\phi_n||^2_\\pi = \\frac{\\Gamma(\\alpha+\\beta)}{2^{\\alpha+\\beta-1}\\Gamma(\\alpha)\\Gamma(\\beta)} \\cdot \\frac{2^{\\alpha+\\beta-1}}{2n+\\alpha+\\beta-1} \\frac{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}{n! \\Gamma(n+\\alpha+\\beta-1)}\n$$\n$$\n||\\phi_n||^2_\\pi = \\frac{\\Gamma(\\alpha+\\beta)\\Gamma(n+\\alpha)\\Gamma(n+\\beta)}{n!\\Gamma(\\alpha)\\Gamma(\\beta)\\Gamma(n+\\alpha+\\beta-1)(2n+\\alpha+\\beta-1)}\n$$\nFor $n=0$, $\\phi_0(x)=1$, $\\lambda_0=0$, and $||\\phi_0||^2_\\pi = \\int_0^1 \\pi(x)dx=1$.\n\n**4. Spectral Expansion of the Transition Density**\n\nThe transition density $p_t(x,y)$ with respect to the Lebesgue measure is related to the transition kernel in $L^2(\\pi)$ by $p_t(x,y) = k_t(x,y)\\pi(y)$. The spectral expansion of the kernel is given in terms of the orthonormal eigenfunctions $\\psi_n(x) = \\phi_n(x)/||\\phi_n||_{\\pi}$:\n$$\nk_t(x,y) = \\sum_{n=0}^{\\infty} e^{\\lambda_n t} \\psi_n(x)\\psi_n(y) = \\sum_{n=0}^{\\infty} e^{\\lambda_n t} \\frac{\\phi_n(x)\\phi_n(y)}{||\\phi_n||^2_\\pi}\n$$\nTherefore, the transition density is:\n$$\np_t(x,y) = \\pi(y) \\sum_{n=0}^{\\infty} e^{\\lambda_n t} \\frac{\\phi_n(x)\\phi_n(y)}{||\\phi_n||^2_\\pi}\n$$\nSubstituting the expressions for $\\pi(y)$, $\\lambda_n$, $\\phi_n$, and $||\\phi_n||^2_\\pi$:\n$$\np_t(x,y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} y^{\\beta-1}(1-y)^{\\alpha-1} \\sum_{n=0}^{\\infty} \\exp\\left(-\\frac{n(n+\\alpha+\\beta-1)}{2}t\\right) \\times\n$$\n$$\n\\frac{n!\\Gamma(\\alpha)\\Gamma(\\beta)\\Gamma(n+\\alpha+\\beta-1)(2n+\\alpha+\\beta-1)}{\\Gamma(\\alpha+\\beta)\\Gamma(n+\\alpha)\\Gamma(n+\\beta)} P_n^{(\\alpha-1, \\beta-1)}(2x-1)P_n^{(\\alpha-1, \\beta-1)}(2y-1)\n$$\nThe normalization constant of the Beta distribution, $\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}$, cancels out, yielding the final expression:\n$$\np_t(x,y) = y^{\\beta-1}(1-y)^{\\alpha-1} \\sum_{n=0}^{\\infty} \\frac{n!(2n+\\alpha+\\beta-1)\\Gamma(n+\\alpha+\\beta-1)}{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)} \\times\n$$\n$$\n\\exp\\left(-\\frac{n(n+\\alpha+\\beta-1)}{2}t\\right) P_n^{(\\alpha-1, \\beta-1)}(2x-1)P_n^{(\\alpha-1, \\beta-1)}(2y-1)\n$$\n\n**Verification of Total Probability**\nAs required, we verify that $\\int_0^1 p_t(x,y)dy = 1$ for all $x \\in [0,1]$ and $t0$.\n$$\n\\int_0^1 p_t(x,y) dy = \\int_0^1 \\sum_{n=0}^{\\infty} e^{\\lambda_n t} \\frac{\\phi_n(x)\\phi_n(y)}{||\\phi_n||_\\pi^2} \\pi(y) dy\n$$\nAssuming uniform convergence allows swapping the integral and sum:\n$$\n\\int_0^1 p_t(x,y) dy = \\sum_{n=0}^{\\infty} e^{\\lambda_n t} \\frac{\\phi_n(x)}{||\\phi_n||_\\pi^2} \\int_0^1 \\phi_n(y)\\pi(y) dy\n$$\nSince $\\phi_0(y) = P_0^{(\\alpha-1,\\beta-1)}(2y-1) = 1$, the integral is $\\int_0^1 \\phi_n(y)\\phi_0(y)\\pi(y)dy = \\langle \\phi_n, \\phi_0 \\rangle_\\pi$. Due to orthogonality, this inner product is non-zero only for $n=0$, where it equals $||\\phi_0||_\\pi^2=1$.\nThe sum collapses to the $n=0$ term:\n$$\n\\int_0^1 p_t(x,y) dy = e^{\\lambda_0 t} \\frac{\\phi_0(x)}{||\\phi_0||_\\pi^2} ||\\phi_0||_\\pi^2 = e^{0 \\cdot t} \\cdot \\phi_0(x) = 1\n$$\nThe derivation is consistent and complete.", "answer": "$$\n\\boxed{y^{\\beta-1}(1-y)^{\\alpha-1} \\sum_{n=0}^{\\infty} \\frac{n!(2n+\\alpha+\\beta-1)\\Gamma(n+\\alpha+\\beta-1)}{\\Gamma(n+\\alpha)\\Gamma(n+\\beta)} \\exp\\left(-\\frac{n(n+\\alpha+\\beta-1)}{2}t\\right) P_{n}^{(\\alpha-1,\\beta-1)}(2x-1)P_{n}^{(\\alpha-1,\\beta-1)}(2y-1)}\n$$", "id": "2981119"}]}