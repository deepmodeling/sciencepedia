{"hands_on_practices": [{"introduction": "Antithetic variates are often introduced as a highly effective technique for functionals that are monotone with respect to the underlying random drivers. This exercise [@problem_id:3005274] pushes beyond that simple case, forcing a confrontation with the fundamental condition for variance reduction: a negative covariance between the functional evaluated on a path and its antithetic counterpart. By analyzing a non-monotone functional, the realized quadratic variation, you will discover why the method can fail and develop a robust diagnostic based on covariance signs, building a more critical and nuanced understanding of the technique.", "problem": "Consider a scalar Itô stochastic differential equation (SDE) with constant coefficients, $dX_t = \\mu \\, dt + \\sigma \\, dW_t$, where $W_t$ is a standard Brownian motion and $\\mu, \\sigma \\in \\mathbb{R}$ are constants with $\\sigma \\neq 0$. Fix a time horizon $T > 0$, an integer $m \\ge 1$, and a uniform grid $t_k = k \\Delta t$ with $\\Delta t = T/m$. The Euler–Maruyama scheme with standardized independent Gaussian increments $Z_k \\sim \\mathcal{N}(0,1)$, $k = 1, \\dots, m$, yields the discrete dynamics $X_{t_k} = X_{t_{k-1}} + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} \\, Z_k$, with $X_{t_0} = X_0 \\in \\mathbb{R}$. Let $Z = (Z_1, \\dots, Z_m)$ denote the vector of standardized increments and let $\\widetilde{Z} = -Z$ denote the antithetic vector. The antithetic path $(\\widetilde{X}_{t_k})$ is generated by applying the same scheme to the antithetic increments $\\widetilde{Z}$, starting from $\\widetilde{X}_{t_0} = X_0$.\n\nLet $F$ be a square-integrable path functional of the entire path $(X_{t_k})_{k=0}^m$, and define $Y = F(X_{t_0}, \\dots, X_{t_m})$ and $Y' = F(\\widetilde{X}_{t_0}, \\dots, \\widetilde{X}_{t_m})$. A practitioner considers variance reduction via antithetic variates across time steps by using the per-pair average $A = \\frac{1}{2}(Y + Y')$ in place of two independent realizations. The practitioner is interested in understanding when antithetic pairing across time steps fails to reduce variance for non-monotone path functionals and in formulating a practical, computable diagnostic based on covariance signs.\n\nTo anchor ideas, consider the realized quadratic variation functional $F_{\\mathrm{RV}}$ defined by\n$$\nF_{\\mathrm{RV}}(X_{t_0}, \\dots, X_{t_m}) = \\sum_{k=1}^m \\left(X_{t_k} - X_{t_{k-1}}\\right)^2,\n$$\nand note that when $\\mu = 0$, one has $X_{t_k} - X_{t_{k-1}} = \\sigma \\sqrt{\\Delta t} \\, Z_k$.\n\nSelect all statements that are correct:\n\nA. When $\\mu = 0$, antithetic pairing across time steps for estimating $\\mathbb{E}[F_{\\mathrm{RV}}]$ fails to reduce variance relative to using two independent samples; indeed $Y' = Y$ almost surely, so $\\operatorname{Cov}(Y,Y') = \\operatorname{Var}(Y) \\ge 0$ and the per-pair antithetic average $A$ has the same variance as a single sample.\n\nB. For any square-integrable path functional $F$ of the Euler–Maruyama path driven by $Z$, the antithetic estimator $A = \\frac{1}{2}(F(Z) + F(-Z))$ has lower variance than the average of two independent samples if and only if $\\operatorname{Cov}(F(Z), F(-Z)) < 0$. Equivalently, antithetic pairing fails (relative to two independent samples) whenever $\\operatorname{Cov}(F(Z), F(-Z)) \\ge 0$.\n\nC. If $F$ is coordinate-wise monotone increasing in each $Z_k$ for fixed past, then $\\operatorname{Cov}(F(Z), F(-Z))$ is strictly positive, so antithetic pairing necessarily fails.\n\nD. In the additive case $F(Z) = \\sum_{k=1}^m g_k(Z_k)$ with $Z_k$ independent and each $g_k$ square-integrable, antithetic pairing across time steps reduces variance compared to two independent samples if and only if $\\sum_{k=1}^m \\operatorname{Cov}\\!\\big(g_k(Z_k), g_k(-Z_k)\\big) < 0$; otherwise it fails.\n\nE. A practical stepwise diagnostic for non-monotone functionals is to compute empirical signs of $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$ across $k = 1, \\dots, m$, where $\\Delta_k$ denotes the contribution of the $k$-th time step to $F$ (in an additive or incremental decomposition) and $\\Delta_k^{(A)}$ denotes the corresponding contribution under antithetic pairing. A predominance of positive empirical covariance signs indicates likely failure of variance reduction, whereas a predominance of negative signs indicates likely success.\n\nYour task is to determine which statements are correct, providing justification from first principles based on definitions of variance, covariance, and the structure of the Euler–Maruyama scheme and antithetic variates across time steps. In particular, analyze when antithetic pairing fails for non-monotone path functionals and articulate the covariance-sign diagnostic rigorously (and, when applicable, its stepwise specialization for additive functionals). Avoid assuming any shortcut formulas beyond the foundational definitions and the Euler–Maruyama discretization given above. Provide clear reasoning that is scientifically realistic and self-consistent.", "solution": "The problem statement will first be validated for scientific soundness, consistency, and clarity.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **SDE:** $dX_t = \\mu \\, dt + \\sigma \\, dW_t$, where $W_t$ is a standard Brownian motion, $\\mu, \\sigma \\in \\mathbb{R}$, and $\\sigma \\neq 0$.\n*   **Time Discretization:** Time horizon $T > 0$, integer $m \\ge 1$, uniform grid $t_k = k \\Delta t$ with $\\Delta t = T/m$.\n*   **Euler-Maruyama (EM) Scheme:** $X_{t_k} = X_{t_{k-1}} + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} \\, Z_k$, starting from $X_{t_0} = X_0 \\in \\mathbb{R}$. The $Z_k \\sim \\mathcal{N}(0,1)$ are independent and identically distributed.\n*   **Random Inputs:** $Z = (Z_1, \\dots, Z_m)$.\n*   **Antithetic Inputs:** The problem defines the antithetic path $(\\widetilde{X}_{t_k})$ as the one generated by applying the same EM scheme to the antithetic increments $\\widetilde{Z} = -Z$.\n*   **Path Functional:** $Y = F(X_{t_0}, \\dots, X_{t_m})$ and $Y' = F(\\widetilde{X}_{t_0}, \\dots, \\widetilde{X}_{t_m})$, where $F$ is square-integrable.\n*   **Antithetic Estimator:** $A = \\frac{1}{2}(Y + Y')$.\n*   **Specific Functional:** Realized quadratic variation, $F_{\\mathrm{RV}}(X_{t_0}, \\dots, X_{t_m}) = \\sum_{k=1}^m \\left(X_{t_k} - X_{t_{k-1}}\\right)^2$.\n*   **Special Case:** When $\\mu = 0$, $X_{t_k} - X_{t_{k-1}} = \\sigma \\sqrt{\\Delta t} \\, Z_k$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded in the established theory of numerical methods for SDEs and Monte Carlo variance reduction. All concepts are standard and mathematically rigorous. The definitions, including the recursive formula for the antithetic path, are standard and well-posed. The problem is free from contradictions and can be solved as stated.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. We can proceed with the derivation.\n\n### Solution Derivation\n\nThe goal of antithetic variates is to reduce the variance of a Monte Carlo estimator. The standard estimator for $\\mathbb{E}[Y]$ using the average of two independent samples, $Y_1$ and $Y_2$, is $M = \\frac{1}{2}(Y_1 + Y_2)$. Its variance is:\n$$\n\\operatorname{Var}(M) = \\operatorname{Var}\\left(\\frac{1}{2}(Y_1 + Y_2)\\right) = \\frac{1}{4} \\left(\\operatorname{Var}(Y_1) + \\operatorname{Var}(Y_2)\\right) = \\frac{1}{4} (2 \\operatorname{Var}(Y)) = \\frac{1}{2}\\operatorname{Var}(Y)\n$$\nsince $Y_1, Y_2$ are IID copies of $Y$.\n\nThe antithetic estimator is $A = \\frac{1}{2}(Y + Y')$, where $Y = F(\\text{Path}(Z))$ and $Y' = F(\\text{Path}(-Z))$. Its variance is:\n$$\n\\operatorname{Var}(A) = \\operatorname{Var}\\left(\\frac{1}{2}(Y + Y')\\right) = \\frac{1}{4} \\operatorname{Var}(Y + Y') = \\frac{1}{4} (\\operatorname{Var}(Y) + \\operatorname{Var}(Y') + 2 \\operatorname{Cov}(Y, Y'))\n$$\nSince $Z_k \\sim \\mathcal{N}(0,1)$, its distribution is symmetric, so $Z_k$ and $-Z_k$ are identically distributed. Consequently, the random vectors $Z = (Z_1, \\dots, Z_m)$ and $-Z = (-Z_1, \\dots, -Z_m)$ are identically distributed. This implies that the paths they generate have the same distribution, and thus $Y$ and $Y'$ are identically distributed. Therefore, $\\operatorname{Var}(Y) = \\operatorname{Var}(Y')$. The variance of the antithetic estimator becomes:\n$$\n\\operatorname{Var}(A) = \\frac{1}{4} (2 \\operatorname{Var}(Y) + 2 \\operatorname{Cov}(Y, Y')) = \\frac{1}{2} (\\operatorname{Var}(Y) + \\operatorname{Cov}(Y, Y'))\n$$\nVariance reduction is achieved if $\\operatorname{Var}(A) < \\operatorname{Var}(M)$. This gives:\n$$\n\\frac{1}{2} (\\operatorname{Var}(Y) + \\operatorname{Cov}(Y, Y')) < \\frac{1}{2} \\operatorname{Var}(Y) \\iff \\operatorname{Cov}(Y, Y') < 0\n$$\nAntithetic pairing fails to reduce variance (i.e., is either ineffective or detrimental) if $\\operatorname{Var}(A) \\ge \\operatorname{Var}(M)$, which is equivalent to $\\operatorname{Cov}(Y, Y') \\ge 0$.\n\n### Option-by-Option Analysis\n\n**A. When $\\mu = 0$, antithetic pairing across time steps for estimating $\\mathbb{E}[F_{\\mathrm{RV}}]$ fails to reduce variance relative to using two independent samples; indeed $Y' = Y$ almost surely, so $\\operatorname{Cov}(Y,Y') = \\operatorname{Var}(Y) \\ge 0$ and the per-pair antithetic average $A$ has the same variance as a single sample.**\n\nLet $Y = F_{\\mathrm{RV}}(X) = \\sum_{k=1}^m (X_{t_k} - X_{t_{k-1}})^2$.\nWhen $\\mu = 0$, the EM increment is $X_{t_k} - X_{t_{k-1}} = \\sigma \\sqrt{\\Delta t} Z_k$.\nSo, $Y = \\sum_{k=1}^m (\\sigma \\sqrt{\\Delta t} Z_k)^2 = \\sigma^2 \\Delta t \\sum_{k=1}^m Z_k^2$.\nFor the antithetic path, the increment is $\\widetilde{X}_{t_k} - \\widetilde{X}_{t_{k-1}} = \\sigma \\sqrt{\\Delta t} (-Z_k)$.\nThe antithetic functional value is $Y' = F_{\\mathrm{RV}}(\\widetilde{X}) = \\sum_{k=1}^m (\\widetilde{X}_{t_k} - \\widetilde{X}_{t_{k-1}})^2$.\n$Y' = \\sum_{k=1}^m (\\sigma \\sqrt{\\Delta t} (-Z_k))^2 = \\sum_{k=1}^m \\sigma^2 \\Delta t (-Z_k)^2 = \\sigma^2 \\Delta t \\sum_{k=1}^m Z_k^2 = Y$.\nThus, $Y' = Y$ almost surely.\nThe covariance is $\\operatorname{Cov}(Y, Y') = \\operatorname{Cov}(Y, Y) = \\operatorname{Var}(Y)$. Since $\\sigma \\neq 0$ and $m \\ge 1$, $Y$ is a non-constant random variable, so $\\operatorname{Var}(Y) > 0$. The condition for failure, $\\operatorname{Cov}(Y, Y') \\ge 0$, is met.\nThe antithetic estimator is $A = \\frac{1}{2}(Y + Y') = \\frac{1}{2}(Y + Y) = Y$.\nThe variance of this estimator is $\\operatorname{Var}(A) = \\operatorname{Var}(Y)$.\nThe variance of a single sample is indeed $\\operatorname{Var}(Y)$. So the statement that $A$ has the same variance as a single sample is correct.\nThe variance of the average of two independent samples is $\\frac{1}{2} \\operatorname{Var}(Y)$.\nSince $\\operatorname{Var}(A) = \\operatorname{Var}(Y) > \\frac{1}{2}\\operatorname{Var}(Y)$, the technique not only fails to reduce variance but actually increases it compared to using two independent samples. The statement is entirely correct.\n\nVerdict: **Correct**.\n\n**B. For any square-integrable path functional $F$ of the Euler–Maruyama path driven by $Z$, the antithetic estimator $A = \\frac{1}{2}(F(Z) + F(-Z))$ has lower variance than the average of two independent samples if and only if $\\operatorname{Cov}(F(Z), F(-Z)) < 0$. Equivalently, antithetic pairing fails (relative to two independent samples) whenever $\\operatorname{Cov}(F(Z), F(-Z)) \\ge 0$.**\n\nThis statement summarizes the fundamental condition for the effectiveness of antithetic variates. As derived in the preliminary analysis:\n$\\operatorname{Var}(A) < \\operatorname{Var}(\\text{average of two indep. samples})$\n$\\iff \\frac{1}{2} (\\operatorname{Var}(Y) + \\operatorname{Cov}(Y, Y')) < \\frac{1}{2} \\operatorname{Var}(Y)$\n$\\iff \\operatorname{Cov}(Y, Y') < 0$.\nThe failure condition is the logical negation, $\\operatorname{Cov}(Y, Y') \\ge 0$. The statement is a precise and correct formulation of this principle.\n\nVerdict: **Correct**.\n\n**C. If $F$ is coordinate-wise monotone increasing in each $Z_k$ for fixed past, then $\\operatorname{Cov}(F(Z), F(-Z))$ is strictly positive, so antithetic pairing necessarily fails.**\n\nLet $Y = g(Z_1, \\dots, Z_m)$ where the function $g$ represents the evaluation of the functional $F$ on the path generated by $Z$. If $F$ is monotone increasing in each $Z_k$, then $g$ is a non-decreasing function of its arguments. Let's analyze the covariance term, $\\operatorname{Cov}(g(Z), g(-Z))$.\nLet the function $h$ be defined as $h(Z) = g(-Z)$. Since $g$ is non-decreasing in each component, $h$ must be non-increasing in each component.\nA fundamental result in probability theory, a consequence of Chebyshev's covariance inequality, states that for a random vector $Z$ and two functions $f_1, f_2$, if $f_1$ is non-decreasing and $f_2$ is non-increasing, then $\\operatorname{Cov}(f_1(Z), f_2(Z)) \\le 0$.\nApplying this result with $f_1 = g$ and $f_2 = h$, we get:\n$$\n\\operatorname{Cov}(g(Z), h(Z)) = \\operatorname{Cov}(g(Z), g(-Z)) \\le 0.\n$$\nThe statement claims the covariance is strictly positive. This is the opposite of the correct result. A non-positive covariance indicates that antithetic pairing will be successful (or at worst, neutral). For most non-trivial monotone functions, the covariance will be strictly negative, guaranteeing variance reduction.\n\nVerdict: **Incorrect**.\n\n**D. In the additive case $F(Z) = \\sum_{k=1}^m g_k(Z_k)$ with $Z_k$ independent and each $g_k$ square-integrable, antithetic pairing across time steps reduces variance compared to two independent samples if and only if $\\sum_{k=1}^m \\operatorname{Cov}\\!\\big(g_k(Z_k), g_k(-Z_k)\\big) < 0$; otherwise it fails.**\n\nLet $Y = F(Z) = \\sum_{k=1}^m g_k(Z_k)$ and $Y' = F(-Z) = \\sum_{k=1}^m g_k(-Z_k)$.\nWe need to find the condition for $\\operatorname{Cov}(Y, Y') < 0$. Using the bilinearity of covariance:\n$$\n\\operatorname{Cov}(Y, Y') = \\operatorname{Cov}\\left(\\sum_{i=1}^m g_i(Z_i), \\sum_{j=1}^m g_j(-Z_j)\\right) = \\sum_{i=1}^m \\sum_{j=1}^m \\operatorname{Cov}(g_i(Z_i), g_j(-Z_j)).\n$$\nThe increments $Z_k$ are independent by definition. For any $i \\neq j$, $Z_i$ and $Z_j$ are independent. Therefore, any functions of them, $g_i(Z_i)$ and $g_j(-Z_j)$, are also independent. The covariance of independent random variables is zero.\n$$\n\\operatorname{Cov}(g_i(Z_i), g_j(-Z_j)) = 0 \\quad \\text{for } i \\neq j.\n$$\nThe double summation thus collapses to the terms where $i=j$:\n$$\n\\operatorname{Cov}(Y, Y') = \\sum_{k=1}^m \\operatorname{Cov}(g_k(Z_k), g_k(-Z_k)).\n$$\nFrom the general principle (Option B), variance reduction occurs if and only if $\\operatorname{Cov}(Y, Y') < 0$. Therefore, for this additive functional, the condition is precisely $\\sum_{k=1}^m \\operatorname{Cov}(g_k(Z_k), g_k(-Z_k)) < 0$. Otherwise, it fails. The statement is correct.\n\nVerdict: **Correct**.\n\n**E. A practical stepwise diagnostic for non-monotone functionals is to compute empirical signs of $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$ across $k = 1, \\dots, m$, where $\\Delta_k$ denotes the contribution of the $k$-th time step to $F$ (in an additive or incremental decomposition) and $\\Delta_k^{(A)}$ denotes the corresponding contribution under antithetic pairing. A predominance of positive empirical covariance signs indicates likely failure of variance reduction, whereas a predominance of negative signs indicates likely success.**\n\nThis statement proposes a heuristic. For a general path functional $F$, one might be able to express it, at least approximately, as a sum of contributions from each time step: $F \\approx \\sum_{k=1}^m \\Delta_k$. Here $\\Delta_k$ is the contribution from step $k$, and $\\Delta_k^{(A)}$ is its antithetic counterpart. From the analysis in Option D, if $F$ were exactly this sum and the $\\Delta_k$ terms depended only on independent increments, the total covariance would be the sum of $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$.\nFor a general path-dependent functional, $\\Delta_k$ would depend on $(Z_1, \\dots, Z_k)$, and the cross-covariance terms $\\operatorname{Cov}(\\Delta_i, \\Delta_j^{(A)})$ for $i \\neq j$ are not necessarily zero. The total covariance is $\\operatorname{Cov}(Y, Y') = \\sum_{i,j} \\operatorname{Cov}(\\Delta_i, \\Delta_j^{(A)})$.\nHowever, the diagonal terms, $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$, are often dominant. The influence of an early shock $Z_i$ on a late contribution $\\Delta_j$ (with $j \\gg i$) might be weak in many SDE applications. Thus, examining the signs of the diagonal terms is a highly reasonable \"practical diagnostic\". If most $\\operatorname{Cov}(\\Delta_k, \\Delta_k^{(A)})$ are positive, it is plausible (\"likely\") that their sum (and the full sum including off-diagonal terms) will be positive, leading to failure. Conversely, a predominance of negative signs suggests likely success. The phrasing \"practical stepwise diagnostic\", \"likely failure/success\", and \"predominance\" correctly captures the heuristic nature of this approach for general functionals. This is a sound and standard method of analysis in computational practice.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ABDE}$$", "id": "3005274"}, {"introduction": "The control variate method is a cornerstone of variance reduction, leveraging known expectations of correlated variables to improve estimator precision. This practice [@problem_id:3005280] offers a comprehensive conceptual check on the application of control variates to a Geometric Brownian Motion process, a ubiquitous model in finance. You will explore the critical aspects of the method, from deriving the exact expectation of the control and understanding sources of bias, to finding the optimal control coefficient and achieving perfect variance reduction in idealized cases.", "problem": "Consider a one-dimensional geometric Brownian motion (GBM) $X_t$ solving the stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = \\mu X_t\\,\\mathrm{d}t + \\sigma X_t\\,\\mathrm{d}W_t,\\quad X_0>0,\n$$\nwhere $\\mu\\in\\mathbb{R}$ and $\\sigma>0$ are constants and $W_t$ is a standard Brownian motion. You want to estimate the expectation of a payoff $Y:=g(X_T)$ at a fixed time $T>0$ by Monte Carlo (MC), where $g:\\mathbb{R}_+\\to\\mathbb{R}$ is measurable and integrable under the GBM law. You consider the control variate $C:=X_T$, and you plan to use the control variate estimator with $n\\in\\mathbb{N}$ independent and identically distributed (i.i.d.) samples:\n$$\n\\widehat{\\theta}_{n,\\beta,m} \\;:=\\; \\frac{1}{n}\\sum_{i=1}^n \\Big( g\\!\\big(X_T^{(i)}\\big) - \\beta\\big(X_T^{(i)} - m\\big)\\Big),\n$$\nwhere $m$ is a number you plug in as the mean of $X_T$, and $\\beta\\in\\mathbb{R}$ is a coefficient you choose.\n\nSelect all statements that are correct.\n\nA. From the SDE structure, one can derive that $\\mathbb{E}[X_T] = X_0\\,\\mathrm{e}^{\\mu T}$ for any $\\sigma>0$.\n\nB. For any fixed deterministic $\\beta\\in\\mathbb{R}$, if $m=\\mathbb{E}[X_T]$ is correct, then $\\widehat{\\theta}_{n,\\beta,m}$ is an unbiased estimator of $\\mathbb{E}[Y]$. If instead you use $m+\\delta$ for some $\\delta\\in\\mathbb{R}$ in place of the true mean $m$, then the estimator’s bias equals $\\beta\\,\\delta$.\n\nC. Among all fixed deterministic choices of $\\beta$, the variance of $\\widehat{\\theta}_{n,\\beta,m}$ is minimized at\n$$\n\\beta^\\star \\;=\\; \\frac{\\operatorname{Cov}\\!\\big(g(X_T),X_T\\big)}{\\operatorname{Var}(X_T)}.\n$$\n\nD. If $m$ is mis-specified, estimating $\\beta$ from the same $n$ samples by least squares regression of $g(X_T)$ on $X_T$ nevertheless guarantees unbiasedness of $\\widehat{\\theta}_{n,\\widehat{\\beta},m}$ because of orthogonality conditions.\n\nE. If $g$ is affine, $g(x)=a x + b$ with $a\\neq 0$, and $m=\\mathbb{E}[X_T]$ is used, then choosing $\\beta=a$ yields a zero-variance estimator of $\\mathbb{E}[Y]$ for any $n\\ge 1$.", "solution": "The user wants a critical analysis of a problem concerning a control variate technique for Monte Carlo estimation related to a geometric Brownian motion.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n-   The stochastic process is a one-dimensional geometric Brownian motion (GBM) $X_t$ satisfying the stochastic differential equation (SDE):\n    $$\n    \\mathrm{d}X_t = \\mu X_t\\,\\mathrm{d}t + \\sigma X_t\\,\\mathrm{d}W_t\n    $$\n-   The initial condition is $X_0 > 0$.\n-   The parameters are constants: $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$.\n-   $W_t$ is a standard Brownian motion.\n-   The quantity to be estimated is the expectation of a payoff $Y := g(X_T)$ at a fixed time $T > 0$.\n-   The function $g: \\mathbb{R}_+ \\to \\mathbb{R}$ is measurable and integrable under the law of the GBM.\n-   A control variate technique is used, with the control variate being $C := X_T$.\n-   The control variate estimator based on $n \\in \\mathbb{N}$ i.i.d. samples is:\n    $$\n    \\widehat{\\theta}_{n,\\beta,m} \\;:=\\; \\frac{1}{n}\\sum_{i=1}^n \\Big( g\\!\\big(X_T^{(i)}\\big) - \\beta\\big(X_T^{(i)} - m\\big)\\Big)\n    $$\n-   The parameters of the estimator are a chosen coefficient $\\beta \\in \\mathbb{R}$ and a number $m$ used as a proxy for the mean of $X_T$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly rooted in the established theories of stochastic calculus (specifically, geometric Brownian motion) and numerical statistics (Monte Carlo methods and variance reduction techniques). All concepts are standard and mathematically rigorous.\n-   **Well-Posed:** The SDE for GBM is well-posed and has a unique strong solution. The problem of analyzing the statistical properties (bias, variance) of the given control variate estimator is a well-defined mathematical task.\n-   **Objective:** The problem is stated using precise, objective mathematical language, free from ambiguity or subjectivity.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. The model and methods are standard.\n2.  **Non-Formalizable or Irrelevant:** None. The problem is formal and directly relevant to the topic.\n3.  **Incomplete or Contradictory Setup:** None. All necessary definitions are provided.\n4.  **Unrealistic or Infeasible:** None. GBM is a widely used model.\n5.  **Ill-Posed or Poorly Structured:** None. The questions about the estimator's properties admit unique answers.\n6.  **Pseudo-Profound, Trivial, or Tautological:** None. The questions test fundamental and non-trivial concepts.\n7.  **Outside Scientific Verifiability:** None. All statements can be mathematically proven or disproven.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the analysis of each option.\n\n### Solution Derivation\n\nLet $Y = g(X_T)$ and $C=X_T$. The quantity of interest is $\\theta = \\mathbb{E}[Y]$. The control variate estimator for a single sample is $Y'_i = Y_i - \\beta(C_i - m)$, where $Y_i = g(X_T^{(i)})$ and $C_i = X_T^{(i)}$. The full estimator is the sample mean of these adjusted values, $\\widehat{\\theta}_{n,\\beta,m} = \\frac{1}{n}\\sum_{i=1}^n Y'_i$.\n\nThe explicit solution to the GBM SDE is $X_t = X_0 \\exp\\left((\\mu - \\frac{1}{2}\\sigma^2)t + \\sigma W_t\\right)$. Since $W_T \\sim \\mathcal{N}(0, T)$, the random variable $\\ln(X_T)$ is normally distributed: $\\ln(X_T) \\sim \\mathcal{N}\\left(\\ln(X_0) + (\\mu - \\frac{1}{2}\\sigma^2)T, \\sigma^2 T\\right)$. This means $X_T$ follows a log-normal distribution.\n\n**Analysis of Option A:**\nThis statement concerns the expectation of $X_T$. We can compute this directly from the SDE. Let $m(t)=\\mathbb{E}[X_t]$. Integrating the SDE from $0$ to $T$ and taking expectations yields:\n$$\n\\mathbb{E}[X_T] = \\mathbb{E}[X_0] + \\mathbb{E}\\left[\\int_0^T \\mu X_s \\mathrm{d}s\\right] + \\mathbb{E}\\left[\\int_0^T \\sigma X_s \\mathrm{d}W_s\\right]\n$$\nGiven $X_0$ is a positive constant, $\\mathbb{E}[X_0]=X_0$. Using Fubini's theorem for the Lebesgue-Stieltjes integral, we can swap expectation and the time integral. The expectation of the Itô integral is zero, provided the integrand satisfies certain square-integrability conditions, which $X_t$ does.\n$$\n\\mathbb{E}[X_T] = X_0 + \\mu \\int_0^T \\mathbb{E}[X_s] \\mathrm{d}s\n$$\nThis gives the ordinary differential equation for the mean $m(t) = \\mathbb{E}[X_t]$:\n$m'(t) = \\mu m(t)$, with the initial condition $m(0) = X_0$. The unique solution is $m(t) = X_0 e^{\\mu t}$.\nThus, $\\mathbb{E}[X_T] = X_0 e^{\\mu T}$. This result holds for any $\\sigma > 0$. The derivation is sound.\nAlternatively, using the explicit solution:\n$$\n\\mathbb{E}[X_T] = X_0 \\exp\\left( (\\mu - \\frac{1}{2}\\sigma^2)T \\right) \\mathbb{E}\\left[\\exp(\\sigma W_T)\\right]\n$$\nThe term $\\sigma W_T$ is a normal random variable with mean $0$ and variance $\\sigma^2 T$. The moment-generating function for a normal variable $Z \\sim \\mathcal{N}(m_z, s_z^2)$ is $\\mathbb{E}[e^Z] = e^{m_z + s_z^2/2}$. For $\\sigma W_T$, this is $e^{0 + (\\sigma^2 T)/2} = e^{\\frac{1}{2}\\sigma^2 T}$.\n$$\n\\mathbb{E}[X_T] = X_0 \\exp\\left( (\\mu - \\frac{1}{2}\\sigma^2)T \\right) e^{\\frac{1}{2}\\sigma^2 T} = X_0 e^{\\mu T}\n$$\nThe statement is **Correct**.\n\n**Analysis of Option B:**\nThis statement concerns the bias of the estimator. The estimator is $\\widehat{\\theta}_{n,\\beta,m_{used}} = \\frac{1}{n}\\sum_i (g(X_T^{(i)}) - \\beta(X_T^{(i)} - m_{used}))$.\nLet the true mean be $m_{true} = \\mathbb{E}[X_T]$. The statement considers using $m_{used} = m_{true} + \\delta$.\nThe expectation of the estimator is:\n$$\n\\mathbb{E}[\\widehat{\\theta}_{n,\\beta,m_{true}+\\delta}] = \\mathbb{E}\\left[ \\frac{1}{n}\\sum_{i=1}^n \\left( g(X_T^{(i)}) - \\beta(X_T^{(i)} - (m_{true}+\\delta))\\right) \\right]\n$$\nSince the samples are i.i.d., this simplifies to the expectation of a single adjusted sample:\n$$\n\\mathbb{E}[\\widehat{\\theta}_{n,\\beta,m_{true}+\\delta}] = \\mathbb{E}\\left[ g(X_T) - \\beta(X_T - m_{true} - \\delta) \\right]\n$$\nUsing the linearity of expectation:\n$$\n\\mathbb{E}[\\widehat{\\theta}_{n,\\beta,m_{true}+\\delta}] = \\mathbb{E}[g(X_T)] - \\beta\\mathbb{E}[X_T - m_{true} - \\delta]\n$$\n$$\n= \\mathbb{E}[g(X_T)] - \\beta(\\mathbb{E}[X_T] - m_{true} - \\delta)\n$$\nSince $\\mathbb{E}[X_T] = m_{true}$, this becomes:\n$$\n= \\mathbb{E}[g(X_T)] - \\beta(m_{true} - m_{true} - \\delta) = \\mathbb{E}[g(X_T)] + \\beta\\delta\n$$\nThe true quantity to estimate is $\\theta = \\mathbb{E}[g(X_T)]$. The bias is defined as $\\text{Bias} = \\mathbb{E}[\\widehat{\\theta}] - \\theta$.\nIn this case, $\\text{Bias} = (\\mathbb{E}[g(X_T)] + \\beta\\delta) - \\mathbb{E}[g(X_T)] = \\beta\\delta$.\nIf $m = \\mathbb{E}[X_T]$ is used correctly, then $\\delta=0$ and the bias is $0$. The estimator is unbiased.\nIf $m+\\delta$ is used, the bias is $\\beta\\delta$.\nThe statement makes both these claims.\nThe statement is **Correct**.\n\n**Analysis of Option C:**\nThis statement is about minimizing the variance. The variance of the estimator is:\n$$\n\\operatorname{Var}(\\widehat{\\theta}_{n,\\beta,m}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n \\left( g(X_T^{(i)}) - \\beta(X_T^{(i)} - m)\\right) \\right)\n$$\nSince the samples are i.i.d., this is $\\frac{1}{n}$ times the variance of a single adjusted sample:\n$$\n\\operatorname{Var}(\\widehat{\\theta}_{n,\\beta,m}) = \\frac{1}{n} \\operatorname{Var}\\left( g(X_T) - \\beta(X_T - m) \\right)\n$$\nThe constant term $m$ does not affect variance, so we minimize $\\operatorname{Var}(g(X_T) - \\beta X_T)$. Let $Y=g(X_T)$ and $C=X_T$.\n$$\nf(\\beta) := \\operatorname{Var}(Y - \\beta C) = \\operatorname{Var}(Y) + \\operatorname{Var}(-\\beta C) + 2\\operatorname{Cov}(Y, -\\beta C)\n$$\n$$\nf(\\beta) = \\operatorname{Var}(Y) + \\beta^2\\operatorname{Var}(C) - 2\\beta\\operatorname{Cov}(Y,C)\n$$\nThis is a quadratic in $\\beta$. To find the minimum, we set its derivative with respect to $\\beta$ to zero:\n$$\n\\frac{\\mathrm{d}f}{\\mathrm{d}\\beta} = 2\\beta\\operatorname{Var}(C) - 2\\operatorname{Cov}(Y,C) = 0\n$$\n$$\n\\implies \\beta = \\frac{\\operatorname{Cov}(Y,C)}{\\operatorname{Var}(C)}\n$$\nThe second derivative is $2\\operatorname{Var}(C) = 2\\operatorname{Var}(X_T)$. Since $\\sigma > 0$, $X_T$ is not a constant, so its variance is positive. This confirms we have found a minimum. Substituting back the original variables:\n$$\n\\beta^\\star = \\frac{\\operatorname{Cov}(g(X_T),X_T)}{\\operatorname{Var}(X_T)}\n$$\nThis matches the formula in the statement.\nThe statement is **Correct**.\n\n**Analysis of Option D:**\nThis statement claims that if $\\beta$ is estimated from the same samples, the estimator remains unbiased even if $m$ is mis-specified. Let $\\widehat{\\beta}$ be the ordinary least squares (OLS) estimator of the slope in a regression of $g(X_T^{(i)})$ on $X_T^{(i)}$.\nThe estimator becomes $\\widehat{\\theta}_{n,\\widehat{\\beta},m} = \\frac{1}{n}\\sum_i (g(X_T^{(i)}) - \\widehat{\\beta}(X_T^{(i)} - m))$.\nLet $\\bar{g} = \\frac{1}{n}\\sum g(X_T^{(i)})$ and $\\bar{X} = \\frac{1}{n}\\sum X_T^{(i)}$. The estimator can be written as $\\widehat{\\theta}_{n,\\widehat{\\beta},m} = \\bar{g} - \\widehat{\\beta}(\\bar{X} - m)$.\nThe OLS estimator $\\widehat{\\beta}$ is $\\widehat{\\beta} = \\frac{\\sum_i (X_T^{(i)}-\\bar{X})(g(X_T^{(i)})-\\bar{g})}{\\sum_i (X_T^{(i)}-\\bar{X})^2}$.\n$\\widehat{\\beta}$ is a random variable that depends on all samples $\\{X_T^{(i)}\\}_{i=1}^n$. The term $\\bar{X}$ also depends on all samples. Therefore, $\\widehat{\\beta}$ and $\\bar{X}$ are not independent.\nLet's analyze the expectation:\n$$\n\\mathbb{E}[\\widehat{\\theta}_{n,\\widehat{\\beta},m}] = \\mathbb{E}[\\bar{g} - \\widehat{\\beta}(\\bar{X} - m)] = \\mathbb{E}[\\bar{g}] - \\mathbb{E}[\\widehat{\\beta}(\\bar{X} - m)]\n$$\n$\\mathbb{E}[\\bar{g}] = \\mathbb{E}[g(X_T)]$. For unbiasedness, we would need $\\mathbb{E}[\\widehat{\\beta}(\\bar{X} - m)] = 0$.\nIn general, $\\mathbb{E}[\\widehat{\\beta}(\\bar{X} - m)] = \\mathbb{E}[\\widehat{\\beta}](\\mathbb{E}[\\bar{X}]-m) + \\operatorname{Cov}(\\widehat{\\beta}, \\bar{X})$.\nEven if $m = \\mathbb{E}[X_T]$, the term $\\operatorname{Cov}(\\widehat{\\beta}, \\bar{X})$ is not generally zero. $\\widehat{\\beta}$ and $\\bar{X}$ are correlated functions of the same underlying sample. This introduces a bias, which is a well-known issue in control variate methods when the optimal coefficient is estimated from the same sample. This bias is typically of order $1/n$ and vanishes asymptotically, but it exists for any finite $n$. The claim of \"guaranteed unbiasedness\" is therefore false. The \"orthogonality conditions\" of OLS refer to properties within the sample (e.g., the sum of residuals is zero, the sample covariance of regressors and residuals is zero), not to expectations over repeated experiments.\nThe statement is **Incorrect**.\n\n**Analysis of Option E:**\nHere, $g(x) = ax+b$ for constants $a\\neq 0, b$. We use $m=\\mathbb{E}[X_T]$ and choose $\\beta=a$.\nLet's examine the adjusted value for a single sample $i$:\n$$\nY'_i = g(X_T^{(i)}) - \\beta(X_T^{(i)} - m) = (a X_T^{(i)} + b) - a(X_T^{(i)} - \\mathbb{E}[X_T])\n$$\n$$\n= a X_T^{(i)} + b - a X_T^{(i)} + a \\mathbb{E}[X_T] = b + a \\mathbb{E}[X_T]\n$$\nThis value $Y'_i$ is a constant, as it does not depend on the specific sample outcome $X_T^{(i)}$.\nThe estimator is the average of these identical constant values:\n$$\n\\widehat{\\theta}_{n,a,m} = \\frac{1}{n} \\sum_{i=1}^n (b + a \\mathbb{E}[X_T]) = b + a \\mathbb{E}[X_T]\n$$\nSince the estimator itself is a constant for any sample size $n \\ge 1$, its variance is zero: $\\operatorname{Var}(\\widehat{\\theta}_{n,a,m}) = 0$.\nFurthermore, the value of the estimator is $b + a \\mathbb{E}[X_T]$. The true quantity we wish to estimate is $\\mathbb{E}[Y] = \\mathbb{E}[g(X_T)] = \\mathbb{E}[aX_T + b] = a\\mathbb{E}[X_T] + b$.\nThe estimator gives the exact true value with zero variance. This is the principle of a perfect control variate. The conditions are met.\nThe statement is **Correct**.\n\n### Summary of Correct Statements\n- A: Correct.\n- B: Correct.\n- C: Correct.\n- D: Incorrect.\n- E: Correct.\n\nThe correct options are A, B, C, and E.", "answer": "$$\n\\boxed{ABCE}\n$$", "id": "3005280"}, {"introduction": "Stratified sampling reduces variance by enforcing a more representative sampling across the domain of the underlying random variables, eliminating the clustering effects that can occur in simple Monte Carlo. This exercise [@problem_id:3005299] provides a focused, hands-on opportunity to apply this principle in a clean, tractable setting: a single step of an Euler–Maruyama discretization. Your task is to construct a simple two-stratum scheme and perform a first-principles derivation of the exact variance reduction factor, providing a clear, quantitative link between the theory of stratification and its numerical benefit.", "problem": "Consider a scalar Itô Stochastic Differential Equation (SDE), $dX_{s}=\\mu(X_{s},s)\\,ds+\\sigma(X_{s},s)\\,dW_{s}$, with fixed time $t$, state $X_{t}=x$, and a small step $h>0$. The one-step Euler–Maruyama discretization produces\n$$\nX_{t+h}^{\\mathrm{E}} \\;=\\; x+\\mu(x,t)\\,h+\\sigma(x,t)\\,\\sqrt{h}\\,Z,\n$$\nwhere $Z\\sim \\mathcal{N}(0,1)$ is a standard normal random variable. You want to estimate the expectation $\\mathbb{E}[X_{t+h}^{\\mathrm{E}}]$ by Monte Carlo (MC) simulation using $n$ samples.\n\nConstruct a stratified sampling scheme that uses two equal-probability strata for $Z$, namely $\\{Z\\le 0\\}$ and $\\{Z>0\\}$, and proportional allocation (i.e., $n/2$ samples in each stratum). Specify precisely how to sample conditionally within each stratum. Using only fundamental probability facts and definitions, derive the exact variance reduction factor $R$ defined as\n$$\nR \\;=\\; \\frac{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}})}{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}})},\n$$\nthe ratio of the variance of the crude MC estimator to that of the stratified estimator, for this particular task of estimating $\\mathbb{E}[X_{t+h}^{\\mathrm{E}}]$.\n\nYour final answer must be a single closed-form analytic expression for $R$, independent of $x$, $\\mu$, $\\sigma$, and $h$. Do not approximate or round your final expression.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded, and formally stated problem in the field of numerical stochastic analysis. It is free of contradictions, ambiguities, or unsound premises. We may therefore proceed with a formal derivation.\n\nLet the random variable to be estimated be denoted by $Y$. Based on the one-step Euler-Maruyama discretization, we have:\n$$\nY \\equiv X_{t+h}^{\\mathrm{E}} = x + \\mu(x,t)h + \\sigma(x,t)\\sqrt{h}Z\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$. For notational simplicity, we let $\\mu \\equiv \\mu(x,t)$ and $\\sigma \\equiv \\sigma(x,t)$, which are constants with respect to the random variable $Z$. The quantity we wish to estimate is the expectation $\\mathbb{E}[Y]$.\n\nFirst, we analyze the crude Monte Carlo (MC) estimator. The estimator, $\\widehat{\\mathbb{E}}_{\\mathrm{crude}}$, is the sample mean of $n$ independent and identically distributed samples $Y_1, \\dots, Y_n$:\n$$\n\\widehat{\\mathbb{E}}_{\\mathrm{crude}} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\nThe variance of this estimator is given by:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}}) = \\frac{1}{n} \\operatorname{Var}(Y)\n$$\nWe compute the variance of $Y$:\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(x + \\mu h + \\sigma \\sqrt{h}Z)\n$$\nSince $x$, $\\mu$, and $h$ are constants, they do not contribute to the variance.\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\sigma \\sqrt{h}Z) = (\\sigma \\sqrt{h})^2 \\operatorname{Var}(Z)\n$$\nGiven that $Z$ is a standard normal random variable, its variance is $\\operatorname{Var}(Z) = 1$. Therefore:\n$$\n\\operatorname{Var}(Y) = \\sigma^2 h\n$$\nThe variance of the crude MC estimator is thus:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}}) = \\frac{\\sigma^2 h}{n}\n$$\n\nNext, we analyze the stratified sampling estimator. The sample space of $Z$ is partitioned into two strata: $S_1 = (-\\infty, 0]$ and $S_2 = (0, \\infty)$. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution. The probability weights of these strata are:\n$$\np_1 = P(Z \\in S_1) = P(Z \\le 0) = \\Phi(0) = \\frac{1}{2}\n$$\n$$\np_2 = P(Z \\in S_2) = P(Z > 0) = 1 - \\Phi(0) = \\frac{1}{2}\n$$\nThe problem specifies proportional allocation, so the number of samples drawn from each stratum is $n_1 = n p_1 = n/2$ and $n_2 = n p_2 = n/2$.\n\nThe stratified estimator for $\\mathbb{E}[Y]$ is given by:\n$$\n\\widehat{\\mathbb{E}}_{\\mathrm{strat}} = p_1 \\bar{Y}_1 + p_2 \\bar{Y}_2 = \\frac{1}{2} (\\bar{Y}_1 + \\bar{Y}_2)\n$$\nwhere $\\bar{Y}_j$ is the sample mean of $n_j$ samples drawn from stratum $j$. The samples drawn from different strata are independent. The variance of the stratified estimator is:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}}) = p_1^2 \\operatorname{Var}(\\bar{Y}_1) + p_2^2 \\operatorname{Var}(\\bar{Y}_2) = p_1^2 \\frac{\\sigma_1^2}{n_1} + p_2^2 \\frac{\\sigma_2^2}{n_2}\n$$\nwhere $\\sigma_j^2 = \\operatorname{Var}(Y | Z \\in S_j)$. Substituting $p_j = 1/2$ and $n_j = n/2$:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}}) = \\frac{(1/2)^2}{n/2} \\sigma_1^2 + \\frac{(1/2)^2}{n/2} \\sigma_2^2 = \\frac{1/4}{n/2} (\\sigma_1^2 + \\sigma_2^2) = \\frac{1}{2n}(\\sigma_1^2 + \\sigma_2^2)\n$$\nTo proceed, we must compute the conditional variances $\\sigma_1^2$ and $\\sigma_2^2$.\n$$\n\\sigma_j^2 = \\operatorname{Var}(Y | Z \\in S_j) = \\operatorname{Var}(x + \\mu h + \\sigma \\sqrt{h}Z | Z \\in S_j) = \\sigma^2 h \\operatorname{Var}(Z | Z \\in S_j)\n$$\nThe conditional variance is found using the formula $\\operatorname{Var}(Z | Z \\in S_j) = \\mathbb{E}[Z^2 | Z \\in S_j] - (\\mathbb{E}[Z | Z \\in S_j])^2$.\n\nLet's compute the conditional moments for stratum $S_1 = (-\\infty, 0]$. Let $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$ be the probability density function (PDF) of $Z$. The conditional PDF of $Z$ given $Z \\le 0$ is $f_1(z) = \\frac{\\phi(z)}{p_1} = 2\\phi(z)$ for $z \\le 0$.\n\nThe conditional expectation is:\n$$\n\\mathbb{E}[Z | Z \\le 0] = \\int_{-\\infty}^{0} z (2\\phi(z)) dz = \\frac{2}{\\sqrt{2\\pi}} \\int_{-\\infty}^{0} z \\exp(-z^2/2) dz\n$$\nBy substitution $u = -z^2/2$, $du = -z dz$, the integral becomes:\n$$\n\\mathbb{E}[Z | Z \\le 0] = \\frac{2}{\\sqrt{2\\pi}} \\left[ -\\exp(-z^2/2) \\right]_{-\\infty}^{0} = \\frac{-2}{\\sqrt{2\\pi}} (\\exp(0) - \\lim_{z \\to -\\infty} \\exp(-z^2/2)) = \\frac{-2}{\\sqrt{2\\pi}}(1-0) = -\\sqrt{\\frac{2}{\\pi}}\n$$\nThe conditional second moment is:\n$$\n\\mathbb{E}[Z^2 | Z \\le 0] = \\int_{-\\infty}^{0} z^2 (2\\phi(z)) dz = 2 \\int_{-\\infty}^{0} z^2 \\phi(z) dz\n$$\nSince $\\mathbb{E}[Z^2]=\\operatorname{Var}(Z)=1$, and the a integrand $z^2\\phi(z)$ is an even function, we have $\\int_{-\\infty}^0 z^2 \\phi(z) dz = \\frac{1}{2} \\int_{-\\infty}^{\\infty} z^2 \\phi(z) dz = \\frac{1}{2} \\mathbb{E}[Z^2] = \\frac{1}{2}$.\n$$\n\\mathbb{E}[Z^2 | Z \\le 0] = 2 \\times \\frac{1}{2} = 1\n$$\nThe conditional variance for stratum $S_1$ is:\n$$\n\\operatorname{Var}(Z | Z \\le 0) = \\mathbb{E}[Z^2 | Z \\le 0] - (\\mathbb{E}[Z | Z \\le 0])^2 = 1 - \\left(-\\sqrt{\\frac{2}{\\pi}}\\right)^2 = 1 - \\frac{2}{\\pi}\n$$\nBy symmetry of the normal distribution, $\\mathbb{E}[Z | Z > 0] = \\sqrt{2/\\pi}$ and $\\mathbb{E}[Z^2 | Z > 0] = 1$, which gives the same conditional variance for stratum $S_2$:\n$$\n\\operatorname{Var}(Z | Z > 0) = 1 - \\left(\\sqrt{\\frac{2}{\\pi}}\\right)^2 = 1 - \\frac{2}{\\pi}\n$$\nThus, the conditional variances of $Y$ are equal:\n$$\n\\sigma_1^2 = \\sigma_2^2 = \\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right)\n$$\nSubstituting this into the expression for the variance of the stratified estimator:\n$$\n\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}}) = \\frac{1}{2n} \\left[ \\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right) + \\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right) \\right] = \\frac{1}{2n} \\left[ 2\\sigma^2 h \\left(1 - \\frac{2}{\\pi}\\right) \\right] = \\frac{\\sigma^2 h}{n} \\left(1 - \\frac{2}{\\pi}\\right)\n$$\nAs part of the task, we specify the conditional sampling procedure. Using the inverse transform method, a random variate $Z_1$ from stratum $S_1$ is generated by drawing $U_1 \\sim \\mathcal{U}(0,1)$ and setting $Z_1 = \\Phi^{-1}(U_1/2)$. Similarly, a variate $Z_2$ from $S_2$ is generated from $U_2 \\sim \\mathcal{U}(0,1)$ by setting $Z_2 = \\Phi^{-1}((U_2+1)/2)$. A total of $n/2$ samples are drawn from each stratum independently.\n\nFinally, we compute the variance reduction factor $R$:\n$$\nR = \\frac{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{crude}})}{\\operatorname{Var}(\\widehat{\\mathbb{E}}_{\\mathrm{strat}})} = \\frac{\\frac{\\sigma^2 h}{n}}{\\frac{\\sigma^2 h}{n} \\left(1 - \\frac{2}{\\pi}\\right)} = \\frac{1}{1 - \\frac{2}{\\pi}}\n$$\nSimplifying the expression:\n$$\nR = \\frac{1}{\\frac{\\pi - 2}{\\pi}} = \\frac{\\pi}{\\pi - 2}\n$$\nThis result is a constant, independent of $x$, $\\mu$, $\\sigma$, and $h$, as required.", "answer": "$$\\boxed{\\frac{\\pi}{\\pi - 2}}$$", "id": "3005299"}]}