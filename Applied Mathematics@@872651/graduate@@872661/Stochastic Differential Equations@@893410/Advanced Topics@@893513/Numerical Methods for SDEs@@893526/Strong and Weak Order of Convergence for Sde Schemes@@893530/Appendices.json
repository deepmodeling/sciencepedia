{"hands_on_practices": [{"introduction": "To build a solid foundation, we first dissect the performance of the Euler-Maruyama scheme, the most fundamental numerical method for SDEs. This exercise requires you to compute the leading error constants for both strong and weak convergence from first principles [@problem_id:2998588]. By working through a solvable multidimensional linear SDE, you will gain a concrete understanding of how these distinct error types arise and how they depend on the underlying system's drift and diffusion parameters.", "problem": "Consider the $d$-dimensional Itô stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t = M\\,X_t\\,\\mathrm{d}t + D\\,X_t\\,\\mathrm{d}W_t,\\qquad X_0 = x_0\\in\\mathbb{R}^d,\n$$\nwhere $W_t$ is a $d$-dimensional standard Brownian motion with independent components, $M\\in\\mathbb{R}^{d\\times d}$ and $D\\in\\mathbb{R}^{d\\times d}$ are deterministic matrices that commute and are simultaneously diagonalizable. Work in the common eigenbasis so that\n$$\nM = \\mathrm{diag}(\\mu_1,\\dots,\\mu_d),\\qquad D = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_d),\n$$\nwith $\\mu_i\\in\\mathbb{R}$ and $\\sigma_i\\in\\mathbb{R}$ for $i=1,\\dots,d$. Let $T0$ be fixed and let $\\{t_n\\}_{n=0}^N$ be a uniform partition of $[0,T]$ with step size $h = T/N$ and $N\\in\\mathbb{N}$.\n\nDefine the Euler–Maruyama (EM) scheme by\n$$\nX_{n+1}^h = X_n^h + M\\,X_n^h\\,h + D\\,X_n^h\\,\\Delta W_n,\\qquad \\Delta W_n := W_{t_{n+1}}-W_{t_n},\n$$\nand denote by $X_T^h := X_N^h$ the terminal EM approximation. For the strong error, define the leading strong mean-square error constant $K_{\\mathrm{s}}$ by the asymptotic expansion\n$$\n\\mathbb{E}\\big[\\,\\|X_T - X_T^h\\|^2\\,\\big] = K_{\\mathrm{s}}\\,h + o(h)\\quad\\text{as}\\quad h\\to 0,\n$$\nwhere $\\|\\cdot\\|$ is the Euclidean norm on $\\mathbb{R}^d$. For the weak error, fix a deterministic vector $u\\in\\mathbb{R}^d$ and the linear functional $\\varphi(x) = u^{\\top}x$, and define the leading weak error constant $K_{\\mathrm{w}}$ by\n$$\n\\mathbb{E}\\big[\\,\\varphi(X_T^h)\\,\\big] - \\mathbb{E}\\big[\\,\\varphi(X_T)\\,\\big] = K_{\\mathrm{w}}\\,h + o(h)\\quad\\text{as}\\quad h\\to 0.\n$$\n\nStarting only from the core definitions of the Itô SDE, the Euler–Maruyama scheme, and basic properties of independent Gaussian increments (including their moment generating function), derive closed-form analytic expressions for $K_{\\mathrm{s}}$ and $K_{\\mathrm{w}}$ in terms of $T$, the diagonal entries $\\mu_i$, $\\sigma_i$, the initial coordinates $x_{0,i}$ of $x_0$, and the components $u_i$ of $u$. Explicitly verify how the dimension $d$ enters these constants. Your final answer must be the pair $(K_{\\mathrm{s}},K_{\\mathrm{w}})$ given as a single closed-form analytic expression. No rounding is required.", "solution": "The assumption that the matrices $M$ and $D$ are simultaneously diagonalizable allows us to work in a basis where both are diagonal. This decouples the $d$-dimensional stochastic differential equation (SDE) into $d$ independent scalar SDEs. Let $X_t = (X_t^{(1)}, \\dots, X_t^{(d)})^{\\top}$ and $x_0 = (x_{0,1}, \\dots, x_{0,d})^{\\top}$ be the vector representations in this common eigenbasis. For each component $i \\in \\{1, \\dots, d\\}$, the SDE is a geometric Brownian motion:\n$$\n\\mathrm{d}X_t^{(i)} = \\mu_i X_t^{(i)} \\mathrm{d}t + \\sigma_i X_t^{(i)} \\mathrm{d}W_t^{(i)}, \\qquad X_0^{(i)} = x_{0,i}\n$$\nwhere $W_t^{(i)}$ are independent, one-dimensional standard Brownian motions. The exact solution is:\n$$\nX_t^{(i)} = x_{0,i} \\exp\\left( (\\mu_i - \\frac{1}{2}\\sigma_i^2)t + \\sigma_i W_t^{(i)} \\right)\n$$\nThe Euler-Maruyama (EM) scheme also decouples component-wise:\n$$\nX_{n+1}^{h,(i)} = X_n^{h,(i)} (1 + \\mu_i h + \\sigma_i \\Delta W_n^{(i)})\n$$\n\n### Derivation of the Strong Error Constant $K_{\\mathrm{s}}$\n\nThe mean-square strong error is $\\mathbb{E}\\big[\\,\\|X_T - X_T^h\\|^2\\,\\big] = \\sum_{i=1}^d \\mathbb{E}\\big[\\,(X_T^{(i)} - X_T^{h,(i)})^2\\,\\big]$ due to the decoupling and orthonormality of the eigenbasis. A standard result of SDE numerics states that the global mean-square error of the EM scheme is of order $h$, and for an interval $[0,T]$ it can be approximated by integrating the contribution from the local errors. The dominant term in the local strong error for the EM scheme comes from the absence of the Milstein term in the Itô-Taylor expansion. The local one-step error is $\\rho_t^{(i)} = X_{t+h}^{(i)} - \\tilde{X}_{t+h}^{(i)}$, where $\\tilde{X}_{t+h}^{(i)}$ is the one-step EM approximation from the exact solution $X_t^{(i)}$. The leading term of this local error is:\n$$\n\\rho_t^{(i)} \\approx \\frac{1}{2}\\sigma_i^2 X_t^{(i)} \\left( (\\Delta W_t^{(i)})^2 - h \\right)\n$$\nThe conditional expectation of its square is $\\mathbb{E}\\big[\\,(\\rho_t^{(i)})^2 \\,|\\, \\mathcal{F}_t\\,\\big] \\approx \\frac{1}{4}\\sigma_i^4 (X_t^{(i)})^2 \\mathbb{E}\\big[\\,((\\Delta W_t^{(i)})^2-h)^2\\,\\big]$. Using $\\mathbb{E}[(\\Delta W)^4]=3h^2$, we find $\\mathbb{E}\\big[\\,((\\Delta W)^2-h)^2\\,\\big] = 2h^2$. Thus, the conditional expectation of the squared local error is approximately $\\frac{1}{2}\\sigma_i^4 (X_t^{(i)})^2 h^2$. The global error constant $K_s$ is found by integrating the leading local error coefficient over $[0,T]$:\n$$\nK_{\\mathrm{s}} = \\sum_{i=1}^d \\int_0^T \\frac{1}{2}\\sigma_i^4 \\mathbb{E}\\big[\\,(X_t^{(i)})^2\\,\\big] \\mathrm{d}t\n$$\nWe compute $\\mathbb{E}\\big[\\,(X_t^{(i)})^2\\,\\big] = x_{0,i}^2 \\mathbb{E}\\big[\\,\\exp(2(\\mu_i - \\frac{1}{2}\\sigma_i^2)t + 2\\sigma_i W_t^{(i)})\\,\\big]$. Using the moment-generating function for a normal variable, this evaluates to $x_{0,i}^2 \\exp((2\\mu_i+\\sigma_i^2)t)$. Substituting this into the integral for $K_{\\mathrm{s}}$:\n$$\nK_{\\mathrm{s}} = \\sum_{i=1}^d \\frac{1}{2}\\sigma_i^4 x_{0,i}^2 \\int_0^T \\exp((2\\mu_i+\\sigma_i^2)t) \\mathrm{d}t\n$$\nThe integral gives $\\frac{\\exp((2\\mu_i+\\sigma_i^2)T)-1}{2\\mu_i+\\sigma_i^2}$ (which tends to $T$ if the denominator is zero). Thus,\n$$\nK_{\\mathrm{s}} = \\sum_{i=1}^d \\frac{1}{2}\\sigma_i^4 x_{0,i}^2 \\frac{\\exp((2\\mu_i+\\sigma_i^2)T) - 1}{2\\mu_i+\\sigma_i^2}\n$$\n\n### Derivation of the Weak Error Constant $K_{\\mathrm{w}}$\n\nThe weak error is $\\mathbb{E}[\\varphi(X_T^h)] - \\mathbb{E}[\\varphi(X_T)] = \\sum_{i=1}^d u_i \\left( \\mathbb{E}[X_T^{h,(i)}] - \\mathbb{E}[X_T^{(i)}] \\right)$. First, the exact mean is $\\mathbb{E}[X_T^{(i)}] = x_{0,i} \\exp(\\mu_i T)$. Next, we find the mean of the EM approximation. The recurrence $\\mathbb{E}\\big[\\,X_{n+1}^{h,(i)}\\,\\big|\\,\\mathcal{F}_{t_n}\\big] = X_n^{h,(i)} (1+\\mu_i h)$ leads to $\\mathbb{E}[X_N^{h,(i)}] = (1+\\mu_i h)^N x_{0,i}$. With $N = T/h$, we expand this term for small $h$:\n$$\n\\mathbb{E}[X_T^{h,(i)}] = x_{0,i} (1+\\mu_i h)^{T/h} = x_{0,i} \\exp\\left(\\frac{T}{h}\\ln(1+\\mu_i h)\\right)\n$$\nUsing the Taylor series $\\ln(1+x) = x - \\frac{x^2}{2} + O(x^3)$:\n$$\n\\frac{T}{h} \\ln(1+\\mu_i h) = \\frac{T}{h} \\left(\\mu_i h - \\frac{1}{2}\\mu_i^2 h^2 + O(h^3)\\right) = \\mu_i T - \\frac{1}{2}\\mu_i^2 T h + O(h^2)\n$$\nSubstituting this back into the exponential and using $\\exp(a+b) \\approx \\exp(a)(1+b)$:\n$$\n\\exp\\left(\\mu_i T - \\frac{1}{2}\\mu_i^2 T h + O(h^2)\\right) = \\exp(\\mu_i T)\\left(1 - \\frac{1}{2}\\mu_i^2 T h + O(h^2)\\right)\n$$\nThe difference in expectations for the $i$-th component is:\n$$\n\\mathbb{E}[X_T^{h,(i)}] - \\mathbb{E}[X_T^{(i)}] = x_{0,i} \\left(\\exp(\\mu_i T)\\left(1 - \\frac{1}{2}\\mu_i^2 T h\\right) - \\exp(\\mu_i T) + O(h^2)\\right) = -\\frac{1}{2} x_{0,i} \\mu_i^2 T \\exp(\\mu_i T) h + O(h^2)\n$$\nSumming over all components gives the total weak error, from which we identify the constant $K_{\\mathrm{w}}$:\n$$\nK_{\\mathrm{w}} = -\\frac{T}{2} \\sum_{i=1}^d u_i x_{0,i} \\mu_i^2 \\exp(\\mu_i T)\n$$\nThe dimension $d$ enters both constants simply as the number of terms in the summation. This is a direct consequence of the decoupling of the SDE system.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\displaystyle\\sum_{i=1}^d \\frac{1}{2}\\sigma_i^4 x_{0,i}^2 \\frac{\\exp((2\\mu_i+\\sigma_i^2)T) - 1}{2\\mu_i+\\sigma_i^2}  -\\frac{T}{2} \\displaystyle\\sum_{i=1}^d u_i x_{0,i} \\mu_i^2 \\exp(\\mu_i T) \\end{pmatrix}}\n$$", "id": "2998588"}, {"introduction": "After establishing the baseline error for the Euler-Maruyama scheme, we investigate how to achieve better accuracy for computed expectations. This practice [@problem_id:2998612] challenges you to compare a first-order weak scheme with a second-order weak scheme. By explicitly deriving the weak error coefficients for both methods, you will see precisely how the higher-order scheme is constructed to eliminate the dominant error term, offering a practical glimpse into the design of more efficient numerical methods.", "problem": "Consider the scalar Itô stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; \\lambda\\, X_{t}\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t}, \\qquad X_{0}=x_{0},\n$$\nwith constants $\\lambda\\in\\mathbb{R}$, $\\sigma0$, and time horizon $T0$. Let the observable be the smooth test function $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ defined by $\\varphi(x)=x$. Denote by $X_{T}$ the exact solution at time $T$, and by $X_{T}^{h,\\mathrm{EM}}$ and $X_{T}^{h,\\mathrm{Pl}}$ the one-step Euler–Maruyama and Platen weak second-order approximations, respectively, computed with a uniform step size $h=T/N$ and $N\\in\\mathbb{N}$ steps.\n\nThe Euler–Maruyama step is\n$$\nX_{n+1} \\;=\\; X_{n} \\;+\\; \\lambda\\,X_{n}\\,h \\;+\\; \\sigma\\,\\Delta W_{n},\n$$\nwhere $\\Delta W_{n}=W_{t_{n+1}}-W_{t_{n}}$ are independent, identically distributed normal random variables with mean $0$ and variance $h$, and $t_{n}=nh$. The Platen weak second-order step for this additive-noise SDE is\n$$\nX_{n+1} \\;=\\; X_{n} \\;+\\; \\lambda\\,X_{n}\\,h \\;+\\; \\sigma\\,\\Delta W_{n} \\;+\\; \\tfrac{1}{2}\\lambda^{2}\\,X_{n}\\,h^{2}.\n$$\n\nDefine the Talay–Tubaro weak error expansion coefficients $C_{1}^{\\mathrm{EM}}$ and $C_{2}^{\\mathrm{Pl}}$ by\n$$\n\\mathbb{E}\\big[\\varphi(X_{T}^{h,\\mathrm{EM}})\\big] \\;-\\; \\mathbb{E}\\big[\\varphi(X_{T})\\big] \\;=\\; C_{1}^{\\mathrm{EM}}\\,h \\;+\\; o(h),\n$$\nand\n$$\n\\mathbb{E}\\big[\\varphi(X_{T}^{h,\\mathrm{Pl}})\\big] \\;-\\; \\mathbb{E}\\big[\\varphi(X_{T})\\big] \\;=\\; C_{2}^{\\mathrm{Pl}}\\,h^{2} \\;+\\; o(h^{2}).\n$$\n\nStarting only from the SDE definition, the properties of the Wiener process, and the two numerical step formulas above, derive the explicit closed-form expressions of $C_{1}^{\\mathrm{EM}}$ and $C_{2}^{\\mathrm{Pl}}$ as functions of $\\lambda$, $\\sigma$, $T$, and $x_{0}$. Your final answer must be a single analytic expression containing both coefficients arranged as a row matrix in the order $\\big(C_{1}^{\\mathrm{EM}},\\,C_{2}^{\\mathrm{Pl}}\\big)$. No rounding is required and no physical units are to be included.", "solution": "The objective is to compute the weak error coefficients $C_{1}^{\\mathrm{EM}}$ and $C_{2}^{\\mathrm{Pl}}$. Since the observable is $\\varphi(x)=x$, we must compute the leading-order terms in the expansion of the error in the mean: $\\mathbb{E}[X_{T}^{h}] - \\mathbb{E}[X_{T}]$.\n\n**1. Expectation of the Exact Solution**\n\nThe given SDE is a linear Ornstein-Uhlenbeck process. The exact solution at time $T$ starting from $X_0=x_0$ is given by:\n$$\nX_{T} = x_{0}\\exp(\\lambda T) + \\sigma \\int_{0}^{T} \\exp(\\lambda(T-s))\\,\\mathrm{d}W_{s}\n$$\nWe take the expectation to find $\\mathbb{E}[\\varphi(X_T)] = \\mathbb{E}[X_T]$. The expectation of the Itô integral is zero. Therefore, the expectation of the exact solution is:\n$$\n\\mathbb{E}[X_{T}] = x_{0}\\exp(\\lambda T)\n$$\n\n**2. Analysis of the Euler–Maruyama Scheme**\n\nThe Euler–Maruyama scheme is given by the recurrence relation:\n$$\nX_{n+1} = (1+\\lambda h)X_{n} + \\sigma \\Delta W_{n}\n$$\nLet $m_{n}^{\\mathrm{EM}} = \\mathbb{E}[X_{n}]$. Taking the expectation of the recurrence, and using $\\mathbb{E}[\\Delta W_{n}]=0$, we obtain a recurrence for the mean:\n$$\nm_{n+1}^{\\mathrm{EM}} = (1+\\lambda h) m_{n}^{\\mathrm{EM}}\n$$\nWith the initial condition $m_{0}^{\\mathrm{EM}} = x_{0}$, this is a geometric progression. After $N$ steps, at time $T=Nh$:\n$$\nm_{N}^{\\mathrm{EM}} = x_{0}(1+\\lambda h)^{N} = x_{0}\\left(1 + \\frac{\\lambda T}{N}\\right)^{N}\n$$\nTo find the weak error, we expand this expression for large $N$. Using the expansions $\\ln(1+u) = u - u^{2}/2 + O(u^{3})$ and $\\exp(v) = 1+v+O(v^{2})$:\n$$\n\\left(1 + \\frac{\\lambda T}{N}\\right)^{N} = \\exp\\left(N \\ln\\left(1 + \\frac{\\lambda T}{N}\\right)\\right) = \\exp\\left(N\\left(\\frac{\\lambda T}{N} - \\frac{1}{2}\\left(\\frac{\\lambda T}{N}\\right)^{2} + O(N^{-3})\\right)\\right)\n$$\n$$\n= \\exp\\left(\\lambda T - \\frac{\\lambda^{2}T^{2}}{2N}\\right) + O(N^{-2}) = \\exp(\\lambda T)\\left(1 - \\frac{\\lambda^{2}T^{2}}{2N} + O(N^{-2})\\right)\n$$\nSubstituting $1/N = h/T$, we get:\n$$\n\\mathbb{E}[X_{T}^{h,\\mathrm{EM}}] = x_{0}\\exp(\\lambda T)\\left(1 - \\frac{1}{2}\\lambda^{2}T h + O(h^{2})\\right) = x_{0}\\exp(\\lambda T) - \\frac{1}{2}x_{0}\\lambda^{2}T\\exp(\\lambda T)h + O(h^{2})\n$$\nThe weak error is therefore:\n$$\n\\mathbb{E}[X_{T}^{h,\\mathrm{EM}}] - \\mathbb{E}[X_{T}] = -\\frac{1}{2}x_{0}\\lambda^{2}T\\exp(\\lambda T)h + O(h^{2})\n$$\nBy definition, we identify the coefficient:\n$$\nC_{1}^{\\mathrm{EM}} = -\\frac{1}{2}x_{0}\\lambda^{2}T\\exp(\\lambda T)\n$$\n\n**3. Analysis of the Platen Scheme**\n\nThe Platen weak second-order scheme for this SDE is given by:\n$$\nX_{n+1} = \\left(1 + \\lambda h + \\frac{1}{2}\\lambda^{2}h^{2}\\right)X_{n} + \\sigma\\Delta W_{n}\n$$\nLet $m_{n}^{\\mathrm{Pl}} = \\mathbb{E}[X_{n}]$. Taking the expectation gives the recurrence:\n$$\nm_{n+1}^{\\mathrm{Pl}} = \\left(1 + \\lambda h + \\frac{1}{2}\\lambda^{2}h^{2}\\right) m_{n}^{\\mathrm{Pl}}\n$$\nAfter $N=T/h$ steps, the solution is:\n$$\nm_{N}^{\\mathrm{Pl}} = x_{0}\\left(1 + \\lambda h + \\frac{1}{2}\\lambda^{2}h^{2}\\right)^{N} = x_{0}\\exp\\left(N\\ln\\left(1 + \\lambda h + \\frac{1}{2}\\lambda^{2}h^{2}\\right)\\right)\n$$\nWe expand the logarithm for small $h$, keeping terms up to $O(h^3)$:\n$$\n\\ln\\left(1 + \\lambda h + \\frac{\\lambda^{2}h^{2}}{2}\\right) = \\left(\\lambda h + \\frac{\\lambda^{2}h^{2}}{2}\\right) - \\frac{1}{2}(\\lambda h)^2 + \\frac{1}{3}(\\lambda h)^3 + \\dots = \\lambda h - \\frac{\\lambda^{3}h^{3}}{6} + O(h^4)\n$$\nNow, we exponentiate $N$ times this expression:\n$$\nm_{N}^{\\mathrm{Pl}} = x_{0}\\exp\\left(N \\left(\\lambda h - \\frac{\\lambda^{3}h^{3}}{6} + O(h^4)\\right)\\right) = x_{0}\\exp\\left(\\lambda T - \\frac{\\lambda^{3}T h^2}{6} + O(h^{3})\\right)\n$$\n$$\n= x_{0}\\exp(\\lambda T)\\left(1 - \\frac{1}{6}\\lambda^{3}T h^{2} + O(h^{3})\\right)\n$$\nThe mean of the Platen scheme approximation is:\n$$\n\\mathbb{E}[X_{T}^{h,\\mathrm{Pl}}] = x_{0}\\exp(\\lambda T) - \\frac{1}{6}x_{0}\\lambda^{3}T\\exp(\\lambda T)h^{2} + O(h^{3})\n$$\nThe weak error is therefore:\n$$\n\\mathbb{E}[X_{T}^{h,\\mathrm{Pl}}] - \\mathbb{E}[X_{T}] = -\\frac{1}{6}x_{0}\\lambda^{3}T\\exp(\\lambda T)h^{2} + O(h^{3})\n$$\nBy definition, we identify the coefficient:\n$$\nC_{2}^{\\mathrm{Pl}} = -\\frac{1}{6}x_{0}\\lambda^{3}T\\exp(\\lambda T)\n$$\nThe derived expressions show that the Platen scheme successfully eliminates the $O(h)$ error term present in the Euler-Maruyama scheme, achieving a higher order of weak convergence.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2} x_{0} \\lambda^{2} T \\exp(\\lambda T)  -\\frac{1}{6} x_{0} \\lambda^{3} T \\exp(\\lambda T) \\end{pmatrix}}\n$$", "id": "2998612"}, {"introduction": "We now transition from analyzing schemes to understanding their design principles. This advanced exercise [@problem_id:2998607] guides you through the construction of a weak second-order stochastic Runge-Kutta method for a general class of multidimensional SDEs. You will discover why achieving higher weak order necessitates a more complex structure, including the use of multiple independent random numbers per step, and determine the minimum number of samples required by this design.", "problem": "Consider the Itô stochastic differential equation in $\\mathbb{R}^{n}$ driven by $d$-dimensional Brownian motion,\n$$\n\\mathrm{d}X_{t} \\;=\\; a(X_{t})\\,\\mathrm{d}t \\;+\\; \\sum_{k=1}^{d} b_{k}(X_{t})\\,\\mathrm{d}W_{t}^{k},\n$$\nwhere $a:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ and $b_{k}:\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ are sufficiently smooth with bounded derivatives up to order $4$, and the diffusion vector fields are commutative in the sense that the Lie brackets satisfy $[b_{i},b_{j}](x) \\equiv \\nabla b_{j}(x)\\,b_{i}(x) - \\nabla b_{i}(x)\\,b_{j}(x) = 0$ for all $x\\in\\mathbb{R}^{n}$ and all $i,j\\in\\{1,\\dots,d\\}$. Let $h0$ denote a fixed time step. The weak order of a one-step method $\\Psi_{h}$ is defined by the requirement that, for every test function $\\varphi\\in C_{b}^{4}(\\mathbb{R}^{n},\\mathbb{R})$, there exists a constant $C(\\varphi)$ independent of $h$ such that\n$$\n\\left| \\mathbb{E}\\big[\\varphi\\big(\\Psi_{h}(x,\\omega)\\big)\\big] \\;-\\; \\mathbb{E}\\big[\\varphi\\big(X_{t+h}\\big)\\,\\big|\\,X_{t}=x\\big] \\right| \\;\\le\\; C(\\varphi)\\, h^{p+1} \\quad \\text{for some } p\\ge 1.\n$$\nYou are asked to design a two-stage stochastic Runge–Kutta method that achieves weak order $2$ under the commutative noise assumption, starting from the Itô–Taylor expansion of the weak flow and the Kolmogorov backward operator\n$$\nL^{0}\\varphi(x) \\;=\\; a(x)\\cdot\\nabla\\varphi(x) \\;+\\; \\tfrac{1}{2}\\sum_{k=1}^{d}\\big(b_{k}(x)b_{k}(x)^{\\top}\\big):\\nabla^{2}\\varphi(x), \n\\qquad\nL^{k}\\varphi(x) \\;=\\; b_{k}(x)\\cdot\\nabla\\varphi(x),\n$$\nand by matching the moments required up to order $4$ in $h$ for the commutative case. In particular, you must explain why the use of multiple independent Gaussian samples per step is necessary to fulfill the weak order two conditions, and you must construct a concrete scheme whose stage values use independent Gaussian perturbations of the same variance as the Brownian increments while remaining independent of them.\n\nConclude by determining, as a function of the Brownian dimension $d$, the minimal number $m(d)$ of independent standard normal scalar samples that must be drawn per time step by your design in order to satisfy the weak order two moment conditions under the commutative noise assumption. Your final answer must be the closed-form analytic expression for $m(d)$. Do not provide an inequality or an equation. No rounding is required.", "solution": "To design a two-stage stochastic Runge-Kutta (SRK) method of weak order $2$ for the given SDE under the commutative noise assumption, we must match the moments of the numerical approximation with those of the true solution up to a sufficiently high order.\n\nFirst, let us establish the target. For a sufficiently smooth test function $\\varphi$, the expectation of $\\varphi(X_{t+h})$ conditional on $X_t=x$ has the following expansion in powers of the time step $h$:\n$$\n\\mathbb{E}[\\varphi(X_{t+h})|X_{t}=x] = \\varphi(x) + h L^0\\varphi(x) + \\frac{h^2}{2}\\left( (L^0)^2 + \\sum_{k=1}^d (L^k)^2 \\right)\\varphi(x) + O(h^3).\n$$\nThis expansion makes use of the commutativity assumption $[b_i, b_j]=0$, which implies that the operators $[L^i, L^j]=0$. The goal is to construct a numerical approximation $Y_1$ starting from $x$ such that $\\mathbb{E}[\\varphi(Y_1)]$ matches this expansion up to terms of order $h^2$.\n\nLet us propose the following two-stage SRK method, a design often attributed to Platen and Rößler:\n$$\n\\begin{align*}\nK_1 = x \\\\\nK_2 = x + h\\,a(K_1) + \\sum_{k=1}^d b_k(K_1) \\tilde{I}_{(k)} \\\\\nY_1 = x + \\frac{h}{2}\\left(a(K_1) + a(K_2)\\right) + \\sum_{k=1}^d b_k(K_1) I_{(k)}\n\\end{align*}\n$$\nHere, $I_{(k)}$ and $\\tilde{I}_{(k)}$ are random variables representing the stochastic increments. For the method to achieve weak order $2$, their joint moments must match those of two independent $d$-dimensional Wiener process increments. Let $\\Delta W_h$ and $\\Delta \\tilde{W}_h$ be two independent Wiener process increments over $[t, t+h]$. The scheme achieves weak order $2$ if we set $I_{(k)} = \\Delta W_h^k$ and $\\tilde{I}_{(k)} = \\Delta \\tilde{W}_h^k$.\n\nThe necessity of two independent sets of random variables becomes clear when examining the structure of the weak Itô-Taylor expansion. In the expansion of $\\mathbb{E}[\\varphi(Y_1)]$, cross-terms involving products of the perturbations in the stage values and the final increments appear. For example, a term arises from the expectation of the product of the drift update and the stochastic term. The leading part of the drift update involving randomness is $\\frac{h}{2}(a(K_2) - a(x)) \\approx \\frac{h}{2} \\sum_j (L^j a)(x) \\tilde{I}_{(j)}$. When multiplied by the stochastic term $\\sum_k (L^k\\varphi)(x) I_{(k)}$, the expectation involves $\\mathbb{E}[\\tilde{I}_{(j)} I_{(k)}]$. For the scheme's expansion to match the true expansion, these cross-terms must vanish. This is guaranteed if we choose the sets $\\{I_{(k)}\\}$ and $\\{\\tilde{I}_{(k)}\\}$ to be independent, which implies $\\mathbb{E}[\\tilde{I}_{(j)} I_{(k)}] = 0$.\n\nFurthermore, the perturbation of the argument of the drift function $a$ in the second stage $K_2$ is essential for capturing the $(L^k)^2\\varphi(x)$ terms. The use of random variables $\\tilde{I}_{(k)}$ with Gaussian moments ensures that the corrective terms generated by the stage value $K_2$ are of the correct magnitude to match the true solution's expansion.\n\nNow, we determine the minimal number of independent standard normal scalar samples, $m(d)$, required per time step for this design.\nOur scheme requires two sets of random vectors:\n1. $I = (I_{(1)}, \\dots, I_{(d)})^\\top$. We set $I_{(k)} = \\sqrt{h}\\,\\zeta_k$, where $\\zeta_k \\sim N(0,1)$ are independent for $k=1, \\dots, d$. This requires $d$ independent standard normal samples.\n2. $\\tilde{I} = (\\tilde{I}_{(1)}, \\dots, \\tilde{I}_{(d)})^\\top$. We set $\\tilde{I}_{(k)} = \\sqrt{h}\\,\\eta_k$, where $\\eta_k \\sim N(0,1)$ are independent for $k=1, \\dots, d$. Crucially, the set $\\{\\eta_k\\}$ must be independent of the set $\\{\\zeta_k\\}$.\n\nLet the total required set of $2d$ standard normal variables be $Z = (\\zeta_1, \\dots, \\zeta_d, \\eta_1, \\dots, \\eta_d)^\\top$. We need to generate $Z$ such that its covariance matrix is the identity matrix, $\\mathrm{Cov}(Z) = I_{2d}$.\nSuppose we generate $Z$ from $m$ underlying independent standard normal scalar samples $\\xi = (\\xi_1, \\dots, \\xi_m)^\\top$ via a linear transformation $Z = M \\xi$, where $M$ is a $(2d) \\times m$ matrix.\nThe covariance of $Z$ is\n$$\n\\mathrm{Cov}(Z) = \\mathbb{E}[Z Z^\\top] = \\mathbb{E}[(M\\xi)(M\\xi)^\\top] = M \\mathbb{E}[\\xi\\xi^\\top] M^\\top = M I_m M^\\top = M M^\\top.\n$$\nFor our scheme to work, we need $\\mathrm{Cov}(Z) = I_{2d}$. This gives the condition $M M^\\top = I_{2d}$.\nThe rank of a matrix product is at most the minimum of the ranks of the factors. The rank of the $(2d) \\times m$ matrix $M$ is at most $m$. Thus, the rank of $M M^\\top$ is at most $m$. The rank of the identity matrix $I_{2d}$ is $2d$.\nThis leads to the inequality:\n$$\n\\mathrm{rank}(I_{2d}) \\le \\mathrm{rank}(M M^\\top) \\le m \\implies 2d \\le m.\n$$\nThe minimal number of independent standard normal samples required is therefore $m(d) = 2d$. This minimum is achieved by simply drawing $2d$ independent standard normal samples and partitioning them into the two required sets $\\{\\zeta_k\\}_{k=1}^d$ and $\\{\\eta_k\\}_{k=1}^d$. While other advanced schemes might reduce this number, for the specified two-stage SRK design which relies on two independent Gaussian-like increments, this is the minimal number.", "answer": "$$\n\\boxed{2d}\n$$", "id": "2998607"}]}