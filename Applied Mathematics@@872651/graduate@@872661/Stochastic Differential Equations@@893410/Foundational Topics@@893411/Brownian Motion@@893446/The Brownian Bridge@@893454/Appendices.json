{"hands_on_practices": [{"introduction": "Our first practice is a foundational exercise in characterizing the Brownian bridge. By treating the bridge as a conditioned Wiener process, we can use the properties of multivariate Gaussian distributions to derive its marginal law at any given time. This practice [@problem_id:3000082] is crucial for understanding how the uncertainty of the process evolves, peaking at the midpoint of the interval before collapsing back to the known endpoint.", "problem": "Let $(W_{s})_{0 \\leq s \\leq T}$ be a standard Brownian motion, defined by $W_{0} = 0$, stationary independent increments, and Gaussian increments with $\\mathbb{E}[W_{s}] = 0$ and $\\operatorname{Cov}(W_{s}, W_{u}) = \\min\\{s, u\\}$ for all $s, u \\in [0, T]$. Define the Brownian bridge $(X_{s})_{0 \\leq s \\leq T}$ from $0$ to $0$ over the interval $[0, T]$ as the process obtained by conditioning the Brownian motion on the terminal value $W_{T} = 0$, that is, $X_{s}$ has the same law as $W_{s}$ given $W_{T} = 0$. Using only properties of jointly Gaussian random variables and the covariance structure of Brownian motion, derive the marginal law of $X_{t}$ for a fixed $t \\in (0, T)$. Express your final answer as a single closed-form distribution expression. No rounding is required and no units apply.", "solution": "The problem requires the derivation of the marginal law of a Brownian bridge process $X_t$ at a fixed time $t \\in (0, T)$. By definition, the law of $X_t$ is the law of a standard Brownian motion $W_t$ conditioned on the event $W_T = 0$. The derivation must be based on the properties of jointly Gaussian random variables.\n\nA standard Brownian motion $(W_s)_{s \\geq 0}$ is a Gaussian process, which implies that for any finite set of time points, the corresponding random variables follow a multivariate normal distribution. To determine the law of $W_t$ given $W_T=0$, we consider the random vector $(W_t, W_T)^T$ for a fixed time $t$ such that $0  t  T$. This vector follows a bivariate normal distribution.\n\nFirst, we determine the parameters of this bivariate normal distribution, namely the mean vector $\\boldsymbol{\\mu}$ and the covariance matrix $\\boldsymbol{\\Sigma}$.\n\nThe mean of a standard Brownian motion at any time $s$ is zero, i.e., $\\mathbb{E}[W_s] = 0$. Therefore, the mean vector of $(W_t, W_T)^T$ is the zero vector:\n$$\n\\boldsymbol{\\mu} = \\begin{pmatrix} \\mathbb{E}[W_t] \\\\ \\mathbb{E}[W_T] \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ has entries given by the covariance function of the Brownian motion, $\\operatorname{Cov}(W_s, W_u) = \\min\\{s, u\\}$. The components of $\\boldsymbol{\\Sigma}$ are:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\operatorname{Cov}(W_t, W_t)  \\operatorname{Cov}(W_t, W_T) \\\\ \\operatorname{Cov}(W_T, W_t)  \\operatorname{Cov}(W_T, W_T) \\end{pmatrix}.\n$$\nUsing the provided covariance function, we calculate each term:\n- The variance of $W_t$ is $\\operatorname{Var}(W_t) = \\operatorname{Cov}(W_t, W_t) = \\min\\{t, t\\} = t$.\n- The variance of $W_T$ is $\\operatorname{Var}(W_T) = \\operatorname{Cov}(W_T, W_T) = \\min\\{T, T\\} = T$.\n- The covariance between $W_t$ and $W_T$, given that $tT$, is $\\operatorname{Cov}(W_t, W_T) = \\min\\{t, T\\} = t$.\n- The covariance matrix is symmetric, so $\\operatorname{Cov}(W_T, W_t) = t$.\n\nThus, the covariance matrix is:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} t  t \\\\ t  T \\end{pmatrix}.\n$$\nThe random vector $(W_t, W_T)^T$ therefore follows the distribution:\n$$\n\\begin{pmatrix} W_t \\\\ W_T \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} t  t \\\\ t  T \\end{pmatrix} \\right).\n$$\n\nWe now utilize the standard result for the conditional distribution of a multivariate Gaussian random variable. For a general bivariate normal vector $(Y_1, Y_2)^T$ with mean $(\\mu_1, \\mu_2)^T$ and covariance matrix with components $\\sigma_{ij}$, the conditional distribution of $Y_1$ given $Y_2 = y_2$ is a normal distribution with mean $\\mu_{1|2}$ and variance $\\sigma_{1|2}^2$ given by:\n$$\n\\mu_{1|2} = \\mu_1 + \\sigma_{12} \\sigma_{22}^{-1} (y_2 - \\mu_2)\n$$\n$$\n\\sigma_{1|2}^2 = \\sigma_{11} - \\sigma_{12} \\sigma_{22}^{-1} \\sigma_{21}\n$$\nIn our specific case, we have $Y_1 = W_t$, $Y_2 = W_T$, and we condition on the event $y_2 = W_T = 0$. The parameters from our bivariate distribution are: $\\mu_1 = 0$, $\\mu_2 = 0$, $\\sigma_{11} = t$, $\\sigma_{22} = T$, and $\\sigma_{12} = \\sigma_{21} = t$.\n\nSubstituting these values into the formulas, we find the mean and variance of $X_t$, which are the conditional mean and variance of $W_t$ given $W_T = 0$.\n\nThe conditional mean is:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[W_t | W_T = 0] = 0 + t \\cdot T^{-1} \\cdot (0 - 0) = 0.\n$$\nThe conditional variance is:\n$$\n\\operatorname{Var}(X_t) = \\operatorname{Var}(W_t | W_T = 0) = t - t \\cdot T^{-1} \\cdot t = t - \\frac{t^2}{T}.\n$$\nThis expression for the variance can be written in a more compact form:\n$$\n\\operatorname{Var}(X_t) = \\frac{tT - t^2}{T} = \\frac{t(T-t)}{T}.\n$$\nSince the conditional distribution of a Gaussian is always Gaussian, the marginal law of the Brownian bridge $X_t$ at time $t$ is a normal distribution. We have found its mean to be $0$ and its variance to be $\\frac{t(T-t)}{T}$.\n\nTherefore, the marginal law of $X_t$ for a fixed $t \\in (0, T)$ is given by the normal distribution $\\mathcal{N}\\left(0, \\frac{t(T-t)}{T}\\right)$.", "answer": "$$\\boxed{\\mathcal{N}\\left(0, \\frac{t(T-t)}{T}\\right)}$$", "id": "3000082"}, {"introduction": "Having established the distribution at a single point in time, we now turn to the relationship between the bridge's values at two different times. This exercise [@problem_id:1286123] involves calculating the correlation coefficient between two points on the bridge, providing direct, hands-on experience with its unique covariance structure. Mastering this calculation is key to understanding the process's memory and path-dependent behavior.", "problem": "A standard Wiener process, denoted by $W(t)$ for $t \\geq 0$, is a stochastic process characterized by the following properties:\n1. $W(0) = 0$.\n2. The process has independent increments.\n3. For any $s  t$, the increment $W(t) - W(s)$ is normally distributed with mean 0 and variance $t-s$.\nFrom these properties, it follows that the mean of the process is $\\mathbb{E}[W(t)] = 0$ and the covariance is $\\text{Cov}(W(s), W(t)) = \\min(s, t)$.\n\nA standard Brownian bridge on the time interval $[0, 1]$, denoted by $B(t)$, is constructed from a standard Wiener process $W(t)$ as follows:\n$$B(t) = W(t) - t W(1), \\quad \\text{for } t \\in [0, 1].$$\nThis process is \"pinned\" at both ends, meaning $B(0) = 0$ and $B(1) = 0$.\n\nConsider the values of the standard Brownian bridge at two specific times, $t_1 = 1/3$ and $t_2 = 2/3$. Calculate the correlation coefficient, $\\rho(B(1/3), B(2/3))$, between these two random variables. Express your answer as a single fraction.", "solution": "We start from the definition of the Brownian bridge $B(t)=W(t)-tW(1)$ for $t\\in[0,1]$, where $W$ is a standard Wiener process with $\\mathbb{E}[W(t)]=0$ and $\\text{Cov}(W(s),W(t))=\\min(s,t)$.\n\nFor $0\\leq s\\leq t\\leq 1$, compute the covariance:\n$$\n\\text{Cov}(B(s),B(t))=\\text{Cov}(W(s)-sW(1),\\,W(t)-tW(1)).\n$$\nUsing bilinearity of covariance and the Wiener process covariance,\n$$\n\\text{Cov}(B(s),B(t))\n=\\text{Cov}(W(s),W(t)) - t\\,\\text{Cov}(W(s),W(1)) - s\\,\\text{Cov}(W(1),W(t)) + st\\,\\text{Var}(W(1)).\n$$\nSubstitute $\\text{Cov}(W(s),W(t))=s$, $\\text{Cov}(W(s),W(1))=s$, $\\text{Cov}(W(1),W(t))=t$, and $\\text{Var}(W(1))=1$:\n$$\n\\text{Cov}(B(s),B(t))=s - ts - st + st = s(1-t).\n$$\nIn particular, the variance is\n$$\n\\text{Var}(B(u))=\\text{Cov}(B(u),B(u))=u(1-u), \\quad u\\in[0,1].\n$$\n\nSet $s=\\frac{1}{3}$ and $t=\\frac{2}{3}$. Then\n$$\n\\text{Cov}\\big(B(\\tfrac{1}{3}),B(\\tfrac{2}{3})\\big)=\\tfrac{1}{3}\\big(1-\\tfrac{2}{3}\\big)=\\tfrac{1}{9},\n$$\nand\n$$\n\\text{Var}\\big(B(\\tfrac{1}{3})\\big)=\\tfrac{1}{3}\\big(1-\\tfrac{1}{3}\\big)=\\tfrac{2}{9}, \\qquad \\text{Var}\\big(B(\\tfrac{2}{3})\\big)=\\tfrac{2}{3}\\big(1-\\tfrac{2}{3}\\big)=\\tfrac{2}{9}.\n$$\nTherefore, the correlation coefficient is\n$$\n\\rho\\big(B(\\tfrac{1}{3}),B(\\tfrac{2}{3})\\big)\n=\\frac{\\text{Cov}\\big(B(\\tfrac{1}{3}),B(\\tfrac{2}{3})\\big)}{\\sqrt{\\text{Var}\\big(B(\\tfrac{1}{3})\\big)\\,\\text{Var}\\big(B(\\tfrac{2}{3})\\big)}}\n=\\frac{\\tfrac{1}{9}}{\\sqrt{\\tfrac{2}{9}\\cdot\\tfrac{2}{9}}}\n=\\frac{\\tfrac{1}{9}}{\\tfrac{2}{9}}\n=\\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1286123"}, {"introduction": "This final practice synthesizes our theoretical understanding into a powerful, practical tool: an exact simulation algorithm. Here, we will first derive the one-step transition kernel of the bridge, which describes how the process moves from one point to the next, given the fixed endpoint. This derivation is then applied in a computational exercise [@problem_id:3000113] to generate and statistically validate sample paths, bringing the abstract concept of a Brownian bridge to life.", "problem": "Consider a standard Brownian motion $B_t$ with $B_0=0$ and a fixed time horizon $T>0$. Define the Brownian bridge $X_t$ on the interval $[0,T]$ as the Gaussian process obtained by conditioning $B_t$ to satisfy $X_0=a$ and $X_T=b$ for given real numbers $a$ and $b$. Starting only from the fundamental properties of Brownian motion (independent increments, Gaussianity, stationarity of increments, and $\\mathrm{Var}(B_t)=t$) and the definition of conditioning for multivariate Gaussian distributions, derive the conditional Gaussian structure of the Brownian bridge. Use this structure to obtain the exact one-step transition kernel of $X_t$ from an arbitrary time $s$ to a later time $t$ with $0\\le s  t  T$, given the state $X_s$ and the endpoint $X_T=b$. From this, design an exact simulation algorithm for $X_t$ on an arbitrary strictly increasing time grid $0=t_0t_1\\cdotst_n=T$ by sequentially sampling $X_{t_{k+1}}$ given $X_{t_k}$ and $X_T=b$.\n\nYou must implement the algorithm in a program that, for each specified test case, generates $N$ independent sample paths of the Brownian bridge on the provided grid using the derived exact transition. To verify consistency with the transition densities, use the probability integral transform: for each simulated pair $(X_s,X_t)$ at a chosen transition $(s\\to t)$, compute the cumulative distribution function value $U=\\Phi\\big((X_t-m)/\\sqrt{v}\\big)$ where $\\Phi$ is the standard normal cumulative distribution function, and $m$ and $v$ are the derived conditional mean and variance of $X_t$ given $X_s$ and $X_T=b$. Under correct conditional Gaussian simulation, these $U$ values are independent and identically distributed Uniform on $[0,1]$. Quantify the uniformity by the Kolmogorov-Smirnov (KS) statistic, defined as the supremum of the absolute difference between the empirical distribution function and the uniform distribution function on $[0,1]$. Additionally, verify the marginal distribution of $X_{t^\\ast}$ at a chosen interior time $t^\\astT$ using the same probability integral transform with the derived marginal mean and variance of the Brownian bridge; the transformed values should also be Uniform on $[0,1]$, and you should compute the corresponding KS statistic.\n\nYour program must:\n- Use a fixed Monte Carlo sample size $N=20000$ and a fixed pseudorandom number generator seed $123456$ for reproducibility.\n- For each test case, compute two KS statistics: one for the transition probability integral transform at the specified $(s\\to t)$ and one for the marginal probability integral transform at the specified $t^\\ast$.\n- Declare the test case as passing if and only if both KS statistics are less than the threshold $\\delta=0.03$.\n- Produce a single line of output containing the pass/fail results for all test cases as a comma-separated list of booleans enclosed in square brackets, for example, $[{\\tt True},{\\tt False},\\dots]$.\n\nThe test suite consists of four cases that together cover a general case, near-terminal behavior, irregular grids and non-unit horizons, and extremely small time steps. In each case, all times and parameters are real numbers expressed in natural units of time and state without physical units:\n\n- Case $1$ (general case):\n  - Parameters: $T=1.0$, $a=0.0$, $b=0.8$.\n  - Grid: $[0.0,0.2,0.5,0.9,1.0]$.\n  - Transition test: $s=0.5$, $t=0.9$.\n  - Marginal test: $t^\\ast=0.5$.\n\n- Case $2$ (near-terminal behavior):\n  - Parameters: $T=1.0$, $a=0.5$, $b=-0.5$.\n  - Grid: $[0.0,0.95,0.99,1.0]$.\n  - Transition test: $s=0.95$, $t=0.99$.\n  - Marginal test: $t^\\ast=0.99$.\n\n- Case $3$ (irregular grid, non-unit horizon):\n  - Parameters: $T=2.5$, $a=-1.0$, $b=0.5$.\n  - Grid: $[0.0,0.1,0.4,1.3,2.0,2.5]$.\n  - Transition test: $s=1.3$, $t=2.0$.\n  - Marginal test: $t^\\ast=2.0$.\n\n- Case $4$ (extremely small time step):\n  - Parameters: $T=1.0$, $a=0.2$, $b=0.0$.\n  - Grid: $[0.0,0.4,0.4005,1.0]$.\n  - Transition test: $s=0.4$, $t=0.4005$.\n  - Marginal test: $t^\\ast=0.4005$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[{\\tt True},{\\tt True},{\\tt True},{\\tt True}]$.\n\nAll mathematical symbols, variables, functions, operators, and numbers must be written in LaTeX. The Kolmogorov-Smirnov (KS) acronym must be expanded on first use as Kolmogorov-Smirnov (KS).", "solution": "The problem requires the derivation and implementation of an exact simulation algorithm for a Brownian bridge, verified through statistical tests. The derivation will be based on the fundamental properties of Brownian motion and the rules of conditioning for multivariate Gaussian distributions.\n\nA standard one-dimensional Brownian motion $\\{B_t\\}_{t \\ge 0}$ starting at $B_0=0$ is a Gaussian process characterized by:\n1.  $B_0 = 0$.\n2.  $\\mathbb{E}[B_t] = 0$ for all $t \\ge 0$.\n3.  $\\mathrm{Cov}(B_s, B_t) = \\min(s, t)$ for all $s, t \\ge 0$.\n4.  It has independent increments: for $0 \\le t_1  t_2  \\dots  t_n$, the random variables $(B_{t_2}-B_{t_1}), (B_{t_3}-B_{t_2}), \\dots, (B_{t_n}-B_{t_{n-1}})$ are independent.\n5.  It has stationary Gaussian increments: $B_t - B_s \\sim \\mathcal{N}(0, t-s)$ for $s  t$.\n\nA Brownian motion starting at $a \\in \\mathbb{R}$ can be defined as $W_t = a + B_t$. It is a Gaussian process with mean function $\\mathbb{E}[W_t] = a$ and covariance $\\mathrm{Cov}(W_s, W_t) = \\mathrm{Cov}(B_s, B_t) = \\min(s, t)$.\n\nA Brownian bridge $\\{X_t\\}_{0 \\le t \\le T}$ is defined as the process $W_t$ conditioned on the event that its value at time $T$ is $b$. That is, $X_t = (W_t | W_T = b)$, with the boundary conditions $X_0=a$ and $X_T=b$. Since $W_t$ is a Gaussian process, the conditioned process $X_t$ is also a Gaussian process. Our first task is to determine its conditional Gaussian structure, specifically its mean and variance.\n\n**1. Marginal Distribution of the Brownian Bridge**\n\nTo perform the marginal verification test, we need the distribution of $X_t$ for an arbitrary time $t \\in (0, T)$. This corresponds to the distribution of $W_t$ conditional on $W_T = b$. We consider the bivariate random vector $(W_t, W_T)$. This vector is jointly Gaussian.\n\nIts mean vector is $\\mathbb{E}[(W_t, W_T)] = (a, a)$.\nIts covariance matrix is, for $0  t  T$:\n$$\n\\Sigma = \\begin{pmatrix} \\mathrm{Var}(W_t)  \\mathrm{Cov}(W_t, W_T) \\\\ \\mathrm{Cov}(W_T, W_t)  \\mathrm{Var}(W_T) \\end{pmatrix} = \\begin{pmatrix} t  \\min(t,T) \\\\ \\min(t,T)  T \\end{pmatrix} = \\begin{pmatrix} t  t \\\\ t  T \\end{pmatrix}\n$$\nFor a general partitioned Gaussian vector $(Y_1, Y_2)$ with mean $(\\mu_1, \\mu_2)$ and block covariance matrix $\\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$, the conditional distribution of $Y_1$ given $Y_2=y_2$ is Gaussian with:\nMean: $\\mu_{1|2} = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (y_2 - \\mu_2)$\nVariance: $\\Sigma_{11|2} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}$\n\nIn our case, $Y_1 = W_t$, $Y_2 = W_T$, and $y_2 = b$. We have $\\mu_1 = a$, $\\mu_2 = a$, $\\Sigma_{11} = t$, $\\Sigma_{12} = t$, $\\Sigma_{21} = t$, and $\\Sigma_{22} = T$.\n\nThe marginal mean of the Brownian bridge $X_t$ is:\n$$\n\\mathbb{E}[X_t] = \\mathbb{E}[W_t | W_T=b] = a + t \\cdot T^{-1} \\cdot (b - a) = a \\left(1 - \\frac{t}{T}\\right) + b \\frac{t}{T} = a\\frac{T-t}{T} + b\\frac{t}{T}\n$$\nThe marginal variance of the Brownian bridge $X_t$ is:\n$$\n\\mathrm{Var}(X_t) = \\mathrm{Var}(W_t | W_T=b) = t - t \\cdot T^{-1} \\cdot t = t - \\frac{t^2}{T} = \\frac{t(T-t)}{T}\n$$\nThus, the marginal distribution of the Brownian bridge at time $t \\in (0, T)$ is:\n$$\nX_t \\sim \\mathcal{N}\\left(a\\frac{T-t}{T} + b\\frac{t}{T}, \\frac{t(T-t)}{T}\\right)\n$$\n\n**2. One-Step Transition Kernel**\n\nFor the simulation algorithm, we need the distribution of $X_t$ at time $t$ given its value $x_s$ at a previous time $s$, where $0 \\le s  t  T$. This corresponds to the distribution of $W_t$ conditional on both $W_s = x_s$ and $W_T = b$. We analyze the trivariate Gaussian vector $(W_s, W_t, W_T)$. To apply the conditioning formula, we partition it as $(Y_1, Y_2)$ where $Y_1 = W_t$ and $Y_2 = (W_s, W_T)$.\n\nThe joint mean vector is $(a, a, a)$. The covariance matrix for times $0 \\le s  t  T$ is:\n$$\n\\mathrm{Cov}((W_s, W_t, W_T)) = \\begin{pmatrix} s  s  s \\\\ s  t  t \\\\ s  t  T \\end{pmatrix}\n$$\nWe partition this matrix according to $Y_1$ and $Y_2$:\n$\\Sigma_{11} = t$\n$\\Sigma_{12} = \\begin{pmatrix} s  t \\end{pmatrix}$\n$\\Sigma_{21} = \\begin{pmatrix} s \\\\ t \\end{pmatrix}$\n$\\Sigma_{22} = \\begin{pmatrix} s  s \\\\ s  T \\end{pmatrix}$\n\nWe need the inverse of $\\Sigma_{22}$ (for $s>0$):\n$$\n\\Sigma_{22}^{-1} = \\frac{1}{\\det(\\Sigma_{22})} \\begin{pmatrix} T  -s \\\\ -s  s \\end{pmatrix} = \\frac{1}{sT - s^2} \\begin{pmatrix} T  -s \\\\ -s  s \\end{pmatrix} = \\frac{1}{s(T-s)} \\begin{pmatrix} T  -s \\\\ -s  s \\end{pmatrix}\n$$\nThe conditional mean of $X_t$ given $X_s=x_s$ (and $X_T=b$) is:\n$$\n\\mathbb{E}[X_t | X_s=x_s, X_T=b] = a + \\Sigma_{12} \\Sigma_{22}^{-1} \\begin{pmatrix} x_s - a \\\\ b - a \\end{pmatrix}\n$$\nThe matrix product is $\\Sigma_{12} \\Sigma_{22}^{-1} = \\frac{1}{T-s} \\begin{pmatrix} T-t  t-s \\end{pmatrix}$. Substituting this gives:\n$$\n= a + \\frac{(T-t)(x_s - a) + (t-s)(b - a)}{T-s}\n$$\n$$\n= \\frac{a(T-s) + (T-t)x_s - a(T-t) + (t-s)b - a(t-s)}{T-s}\n$$\n$$\n= \\frac{a(T-s - (T-t) - (t-s)) + x_s(T-t) + b(t-s)}{T-s} = x_s\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}\n$$\nThe conditional variance is:\n$$\n\\mathrm{Var}(X_t | X_s=x_s, X_T=b) = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n$$\n$$\n= t - \\left( \\frac{1}{T-s} \\begin{pmatrix} T-t  t-s \\end{pmatrix} \\right) \\begin{pmatrix} s \\\\ t \\end{pmatrix} = t - \\frac{s(T-t) + t(t-s)}{T-s}\n$$\n$$\n= \\frac{t(T-s) - (sT - st + t^2 - st)}{T-s} = \\frac{tT - st - sT + 2st - t^2}{T-s}\n$$\n$$\n= \\frac{tT - t^2 - sT + st}{T-s} = \\frac{t(T-t) - s(T-t)}{T-s} = \\frac{(t-s)(T-t)}{T-s}\n$$\nThese formulas also hold for $s=0$ by taking the limit or by re-deriving with $W_0=a$ as a deterministic value. The conditional distribution, or transition kernel, is thus:\n$$\n(X_t | X_s=x_s) \\sim \\mathcal{N}\\left(x_s\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}, \\frac{(t-s)(T-t)}{T-s}\\right)\n$$\n\n**3. Exact Simulation Algorithm**\n\nGiven a strictly increasing time grid $0=t_0  t_1  \\dots  t_n = T$, a sample path $(X_{t_0}, \\dots, X_{t_n})$ can be generated sequentially:\n1.  Initialize: Set $X_{t_0} = a$.\n2.  Iterate for $k = 0, 1, \\dots, n-1$:\n    - The state $X_{t_k}$ is known.\n    - Sample $X_{t_{k+1}}$ from the conditional distribution derived above, with $s=t_k$, $t=t_{k+1}$, and $x_s=X_{t_k}$.\n    - Let $s=t_k$ and $t=t_{k+1}$. The conditional mean is $m_k = X_{t_k}\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}$. The conditional variance is $v_k = \\frac{(t-s)(T-t)}{T-s}$.\n    - Sample a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$.\n    - Set $X_{t_{k+1}} = m_k + \\sqrt{v_k} Z$.\n3.  The final value $X_{t_n}$ will be exactly $b$, as for the last step ($k=n-1$), $s=t_{n-1}$ and $t=t_n=T$, yielding $m_{n-1}=b$ and $v_{n-1}=0$.\n\n**4. Verification Methodology**\n\nThe correctness of the simulation is assessed using the probability integral transform (PIT). If a continuous random variable $Y$ has cumulative distribution function (CDF) $F_Y$, then the random variable $U = F_Y(Y)$ is uniformly distributed on $[0,1]$. We use this property to test both the transition and marginal distributions.\n\n-   **Transition Test**: For a specified transition from $s$ to $t$, we generate $N=20000$ sample paths. For each path $i$, we obtain a pair $(X_s^{(i)}, X_t^{(i)})$. The conditional CDF of $X_t^{(i)}$ given $X_s^{(i)}$ is that of a normal distribution with mean $m^{(i)} = X_s^{(i)}\\frac{T-t}{T-s} + b\\frac{t-s}{T-s}$ and variance $v = \\frac{(t-s)(T-t)}{T-s}$. We compute the PIT values $U_{trans}^{(i)} = \\Phi\\left( (X_t^{(i)} - m^{(i)})/\\sqrt{v} \\right)$, where $\\Phi$ is the standard normal CDF.\n\n-   **Marginal Test**: For a specified time $t^\\ast \\in (0, T)$, we extract the simulated values $X_{t^\\ast}^{(i)}$ for $i=1, \\dots, N$. The theoretical marginal distribution is $\\mathcal{N}(m_{marg}, v_{marg})$, where $m_{marg} = a\\frac{T-t^\\ast}{T} + b\\frac{t^\\ast}{T}$ and $v_{marg} = \\frac{t^\\ast(T-t^\\ast)}{T}$. We compute the PIT values $U_{marg}^{(i)} = \\Phi\\left( (X_{t^\\ast}^{(i)} - m_{marg})/\\sqrt{v_{marg}} \\right)$.\n\nFor both sets of transformed values, $\\{U_{trans}^{(i)}\\}$ and $\\{U_{marg}^{(i)}\\}$, we test the hypothesis that they are drawn from a uniform distribution on $[0,1]$. This is done using the Kolmogorov-Smirnov (KS) test, which computes the statistic $D_N = \\sup_x |F_N(x) - F(x)|$, where $F_N$ is the empirical distribution function of the sample and $F$ is the CDF of the uniform distribution, $F(x)=x$. A test case passes if both the transition KS statistic and the marginal KS statistic are below the given threshold $\\delta = 0.03$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, kstest\n\ndef solve():\n    \"\"\"\n    Implements the Brownian bridge simulation and verification as per the problem statement.\n    \"\"\"\n    # Define constants from the problem statement.\n    N = 20000\n    SEED = 123456\n    DELTA = 0.03\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, a, b, grid, s_test, t_test, t_star_test)\n        (1.0, 0.0, 0.8, [0.0, 0.2, 0.5, 0.9, 1.0], 0.5, 0.9, 0.5),\n        (1.0, 0.5, -0.5, [0.0, 0.95, 0.99, 1.0], 0.95, 0.99, 0.99),\n        (2.5, -1.0, 0.5, [0.0, 0.1, 0.4, 1.3, 2.0, 2.5], 1.3, 2.0, 2.0),\n        (1.0, 0.2, 0.0, [0.0, 0.4, 0.4005, 1.0], 0.4, 0.4005, 0.4005),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        T, a, b, grid, s_test, t_test, t_star_test = case\n        \n        time_grid = np.array(grid, dtype=float)\n        n_points = len(time_grid)\n        \n        # Find indices for verification points\n        s_idx = np.where(np.isclose(time_grid, s_test))[0][0]\n        t_idx = np.where(np.isclose(time_grid, t_test))[0][0]\n        t_star_idx = np.where(np.isclose(time_grid, t_star_test))[0][0]\n\n        # Initialize pseudorandom number generator for reproducibility.\n        rng = np.random.default_rng(SEED)\n\n        # Array to store all N paths.\n        x_paths = np.zeros((N, n_points))\n        x_paths[:, 0] = a\n\n        # Sequentially generate points for each path.\n        for k in range(n_points - 1):\n            s_k = time_grid[k]\n            t_k_plus_1 = time_grid[k+1]\n            \n            # This check handles the final step where t_k_plus_1 = T, which has 0 variance.\n            # In this case, mean is b and variance is 0.\n            if np.isclose(T - s_k, 0):\n                x_paths[:, k+1] = b\n                continue\n\n            prev_x = x_paths[:, k]\n            \n            # Derived one-step transition kernel parameters\n            # Mean: x_s * (T-t)/(T-s) + b * (t-s)/(T-s)\n            # Var: (t-s)*(T-t)/(T-s)\n            mean_trans = prev_x * (T - t_k_plus_1) / (T - s_k) + b * (t_k_plus_1 - s_k) / (T - s_k)\n            var_trans = (t_k_plus_1 - s_k) * (T - t_k_plus_1) / (T - s_k)\n            \n            # Ensure variance is non-negative due to potential float inaccuracies\n            if var_trans  0:\n                var_trans = 0.0\n\n            std_trans = np.sqrt(var_trans)\n            \n            # Generate standard normal samples\n            z = rng.standard_normal(N)\n\n            # Sample the next point in the paths\n            x_paths[:, k+1] = mean_trans + std_trans * z\n            \n        # --- Verification ---\n\n        # 1. Transition probability integral transform test\n        x_s_samples = x_paths[:, s_idx]\n        x_t_samples = x_paths[:, t_idx]\n        \n        # Theoretical conditional mean and variance for the test transition (s_test - t_test)\n        m_cond = x_s_samples * (T - t_test) / (T - s_test) + b * (t_test - s_test) / (T - s_test)\n        v_cond = (t_test - s_test) * (T - t_test) / (T - s_test)\n        s_cond = np.sqrt(v_cond)\n\n        u_trans = norm.cdf((x_t_samples - m_cond) / s_cond)\n        ks_stat_trans, _ = kstest(u_trans, 'uniform')\n\n        # 2. Marginal probability integral transform test\n        x_t_star_samples = x_paths[:, t_star_idx]\n\n        # Theoretical marginal mean and variance at t_star_test\n        m_marg = a * (T - t_star_test) / T + b * t_star_test / T\n        v_marg = t_star_test * (T - t_star_test) / T\n        \n        # Failsafe for t_star=0 or t_star=T, though problem states t_star is interior.\n        if v_marg  0:\n            s_marg = np.sqrt(v_marg)\n            u_marg = norm.cdf((x_t_star_samples - m_marg) / s_marg)\n            ks_stat_marg, _ = kstest(u_marg, 'uniform')\n        else: # If variance is 0, the distribution is a Dirac delta. KS test is not applicable.\n              # Assign a failing stat. This case should not be reached with valid inputs.\n            ks_stat_marg = 1.0\n\n        # Check if both KS statistics are below the threshold.\n        passed = (ks_stat_trans  DELTA) and (ks_stat_marg  DELTA)\n        results.append(str(passed))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3000113"}]}