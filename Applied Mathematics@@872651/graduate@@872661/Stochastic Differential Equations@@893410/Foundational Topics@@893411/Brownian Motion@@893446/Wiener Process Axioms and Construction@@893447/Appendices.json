{"hands_on_practices": [{"introduction": "The Wiener process is the canonical example of a Markov process, where the future evolution depends only on the present state, not the entire past history. This exercise provides a rigorous, hands-on derivation of this property by computing the conditional distribution of the process at a future time $t$ given its value at a present time $s$. Mastering this calculation [@problem_id:3006300] is essential for understanding how information propagates through the process and forms the basis for prediction and filtering in stochastic systems.", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\ge 0},\\mathbb{P}\\right)$ be a filtered probability space carrying a standard Wiener process (also called a standard Brownian motion) $W=\\{W_{t}:t\\ge 0\\}$, defined by the following axioms: (i) $W_{0}=0$ almost surely; (ii) sample paths $t\\mapsto W_{t}$ are almost surely continuous; (iii) for any $0\\le rst$, the increment $W_{t}-W_{s}$ is independent of $\\mathcal{F}_{r}$, where $\\mathcal{F}_{r}=\\sigma(W_{u}:0\\le u\\le r)$; and (iv) for any $0\\le st$, the increment $W_{t}-W_{s}$ is Gaussian with mean $0$ and variance $t-s$. Starting solely from these axioms and standard properties of Gaussian random variables and characteristic functions, derive the conditional distribution of $W_{t}$ given $W_{s}$ for $0\\le st$. Your final answer must be a single closed-form analytic expression for the conditional law of $W_{t}$ given $W_{s}$.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and based on established principles of stochastic processes. We may proceed with the derivation.\n\nOur objective is to determine the conditional distribution of the random variable $W_{t}$ given the value of the random variable $W_{s}$, for a standard Wiener process $W$ and times $0 \\le s  t$. This is denoted as the law of $W_{t} | W_{s}$.\n\nWe begin by expressing the random variable $W_{t}$ in terms of $W_{s}$ and an increment. For any $s  t$, we can write the identity:\n$$\nW_{t} = W_{s} + (W_{t} - W_{s})\n$$\nWhen we condition on $W_{s}$, the value of $W_{s}$ is considered known. Therefore, the conditional distribution of $W_{t}$ is determined by the distribution of the increment $W_{t} - W_{s}$ and its relationship with $W_{s}$.\n\nThe cornerstone of this derivation lies in establishing the independence of the increment $W_{t} - W_{s}$ from the sigma-algebra $\\mathcal{F}_{s} = \\sigma(W_{u} : 0 \\le u \\le s)$. The random variable $W_{s}$ is, by definition, measurable with respect to $\\mathcal{F}_{s}$. Thus, showing that $W_{t} - W_{s}$ is independent of $\\mathcal{F}_{s}$ will imply that it is independent of $W_{s}$.\n\nAxiom (iii) states that for any $0 \\le r  s  t$, the increment $W_{t} - W_{s}$ is independent of the sigma-algebra $\\mathcal{F}_{r}$. We must extend this to show independence from $\\mathcal{F}_{s}$. Let us define the collection of sets $\\mathcal{A} = \\bigcup_{rs} \\mathcal{F}_{r}$.\n1.  $\\mathcal{A}$ is a $\\pi$-system: If $A_{1} \\in \\mathcal{F}_{r_{1}}$ and $A_{2} \\in \\mathcal{F}_{r_{2}}$ with $r_{1}  s$ and $r_{2}  s$, then $A_{1} \\cap A_{2} \\in \\mathcal{F}_{\\max(r_{1}, r_{2})}$. Since $\\max(r_{1}, r_{2})  s$, it follows that $A_{1} \\cap A_{2} \\in \\mathcal{A}$.\n2.  The sigma-algebra generated by $\\mathcal{A}$ is $\\mathcal{F}_{s}$. This is because $\\sigma(\\mathcal{A}) = \\sigma(\\bigcup_{rs} \\mathcal{F}_{r}) = \\mathcal{F}_{s}$. The latter equality holds for filtrations generated by right-continuous processes like the Wiener process (whose paths are continuous almost surely by Axiom (ii)).\n3.  By Axiom (iii), for any set $A \\in \\mathcal{A}$, there exists an $r  s$ such that $A \\in \\mathcal{F}_{r}$. For this $r$, the increment $W_{t}-W_{s}$ is independent of $A$.\n4.  The collection of all events $B \\in \\mathcal{F}$ for which $W_{t} - W_{s}$ is independent of $B$ forms a $\\lambda$-system. Since this $\\lambda$-system contains the $\\pi$-system $\\mathcal{A}$, an application of Dynkin's $\\pi$-$\\lambda$ theorem allows us to conclude that it also contains $\\sigma(\\mathcal{A}) = \\mathcal{F}_{s}$.\n\nTherefore, the increment $W_{t} - W_{s}$ is independent of the sigma-algebra $\\mathcal{F}_{s}$, and consequently, it is independent of the random variable $W_{s}$.\n\nWith independence established, we can formally derive the conditional law using characteristic functions. Let $\\phi_{X}(k) = \\mathbb{E}[\\exp(ikX)]$ denote the characteristic function of a random variable $X$. We seek the conditional characteristic function of $W_{t}$ given $W_{s}=w_{s}$, denoted $\\phi_{W_{t}|W_{s}}(k|w_{s})$:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\mathbb{E}[\\exp(ikW_{t}) | W_{s}=w_{s}]\n$$\nSubstitute the decomposition $W_{t} = W_{s} + (W_{t}-W_{s})$:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\mathbb{E}[\\exp(ik(W_{s} + (W_{t}-W_{s}))) | W_{s}=w_{s}] = \\mathbb{E}[\\exp(ikW_{s})\\exp(ik(W_{t}-W_{s})) | W_{s}=w_{s}]\n$$\nBy the properties of conditional expectation, since we are conditioning on $W_{s}=w_{s}$, the term $\\exp(ikW_{s})$ becomes the constant $\\exp(ikw_{s})$ and can be factored out:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\exp(ikw_{s}) \\mathbb{E}[\\exp(ik(W_{t}-W_{s})) | W_{s}=w_{s}]\n$$\nBecause the increment $W_{t}-W_{s}$ is independent of $W_{s}$, the conditional expectation is equal to the unconditional expectation:\n$$\n\\mathbb{E}[\\exp(ik(W_{t}-W_{s})) | W_{s}=w_{s}] = \\mathbb{E}[\\exp(ik(W_{t}-W_{s}))]\n$$\nThis term is simply the characteristic function of the random variable $W_{t}-W_{s}$. According to Axiom (iv), for $s  t$, the increment $W_{t}-W_{s}$ follows a Gaussian (Normal) distribution with mean $0$ and variance $t-s$, i.e., $W_{t}-W_{s} \\sim \\mathcal{N}(0, t-s)$.\n\nThe characteristic function of a general Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ is given by $\\phi_{X}(k) = \\exp(ik\\mu - \\frac{1}{2}k^{2}\\sigma^{2})$. For the increment $W_{t}-W_{s}$, we have $\\mu=0$ and $\\sigma^{2}=t-s$. Thus, its characteristic function is:\n$$\n\\mathbb{E}[\\exp(ik(W_{t}-W_{s}))] = \\exp\\left(ik(0) - \\frac{1}{2}k^{2}(t-s)\\right) = \\exp\\left(-\\frac{1}{2}k^{2}(t-s)\\right)\n$$\nSubstituting this result back into our expression for the conditional characteristic function of $W_{t}$:\n$$\n\\phi_{W_{t}|W_{s}}(k|w_{s}) = \\exp(ikw_{s}) \\exp\\left(-\\frac{1}{2}k^{2}(t-s)\\right) = \\exp\\left(ikw_{s} - \\frac{1}{2}k^{2}(t-s)\\right)\n$$\nWe now identify this resulting function. It has the form $\\exp(ik\\mu_{cond} - \\frac{1}{2}k^{2}\\sigma_{cond}^{2})$, which is the characteristic function of a Gaussian distribution. By comparing terms, we find the parameters of this conditional distribution:\n-   The conditional mean is $\\mu_{cond} = w_{s}$.\n-   The conditional variance is $\\sigma_{cond}^{2} = t-s$.\n\nThis shows that the distribution of $W_{t}$ conditional on $W_{s}=w_{s}$ is a normal distribution with mean $w_{s}$ and variance $t-s$. Expressed in terms of the random variable $W_{s}$, the conditional law of $W_{t}$ given $W_{s}$ is a normal distribution with mean $W_{s}$ and variance $t-s$. This is a complete specification of the conditional law.", "answer": "$$\n\\boxed{\\mathcal{N}(W_s, t-s)}\n$$", "id": "3006300"}, {"introduction": "The paths of a Wiener process, while continuous, are nowhere differentiable and exhibit a unique form of \"roughness\". This practice explores this property by calculating the process's $p$-variation, which measures the cumulative size of its fluctuations. You will discover the remarkable result that for $p=2$, the variation converges to a deterministic value, a property known as finite quadratic variation [@problem_id:3006312] that distinguishes the Wiener process from smooth functions and necessitates the development of It√¥ calculus.", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard Wiener process (also called standard Brownian motion) with respect to a filtration $\\{\\mathcal{F}_{t}\\}_{t \\geq 0}$ on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, defined by the following axioms: $W_{0}=0$ almost surely, $W$ has independent increments, $W_{t}-W_{s} \\sim \\mathcal{N}(0,t-s)$ for $0 \\leq s  t$ (mean zero, variance $t-s$), and $W$ has almost surely continuous sample paths. Fix a time horizon $t0$ and, for each integer $n \\geq 1$, consider the uniform dyadic partition $\\mathcal{P}_{n}$ of $[0,t]$ given by points $t_{k}^{(n)} = k t / 2^{n}$ for $k=0,1,\\dots,2^{n}$.\n\nFor a real number $p \\geq 2$, define the $p$-variation along $\\mathcal{P}_{n}$ by\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) \\;=\\; \\sum_{k=1}^{2^{n}} \\big|W_{t_{k}^{(n)}} - W_{t_{k-1}^{(n)}}\\big|^{p}.\n$$\nAssume the limit $V^{(p)}_{t}(W) = \\lim_{n \\to \\infty} V^{(p)}_{t}(W;\\mathcal{P}_{n})$ exists almost surely. Using only the Wiener process axioms, basic properties of the normal distribution, and standard probabilistic limit tools, determine $V^{(p)}_{t}(W)$ for $p2$ and for $p=2$. Your derivation must start from the foundational definitions above and should not invoke any prepackaged results beyond those definitions and the elementary moments of the normal distribution. Express your final answer as a single closed-form piecewise analytic expression in terms of $p$ and $t$. No numerical approximation is required.", "solution": "The problem asks for the almost sure limit, $V^{(p)}_{t}(W)$, of the $p$-variation $V^{(p)}_{t}(W;\\mathcal{P}_{n})$ of a standard Wiener process $\\{W_t\\}_{t \\geq 0}$ along a sequence of dyadic partitions $\\mathcal{P}_n$ of the interval $[0,t]$. The problem is to be solved for $p2$ and for $p=2$.\n\nLet the uniform dyadic partition $\\mathcal{P}_{n}$ of $[0,t]$ be defined by the points $t_{k}^{(n)} = \\frac{kt}{2^n}$ for $k=0, 1, \\dots, 2^n$. The length of each subinterval is constant:\n$$\n\\Delta t^{(n)} = t_{k}^{(n)} - t_{k-1}^{(n)} = \\frac{t}{2^n}.\n$$\nLet $\\Delta W_{k}^{(n)} = W_{t_{k}^{(n)}} - W_{t_{k-1}^{(n)}}$ be the increment of the Wiener process over the $k$-th subinterval. According to the axioms of the Wiener process, these increments are independent, and each is normally distributed with mean $0$ and variance equal to the length of the time interval. That is, for each $k \\in \\{1, \\dots, 2^n\\}$,\n$$\n\\Delta W_{k}^{(n)} \\sim \\mathcal{N}\\left(0, \\Delta t^{(n)}\\right).\n$$\nThis distributional property allows us to represent each increment as $\\Delta W_{k}^{(n)} = \\sqrt{\\Delta t^{(n)}} Z_k$, where $\\{Z_k\\}_{k=1}^{2^n}$ is a sequence of independent and identically distributed (i.i.d.) standard normal random variables, $Z_k \\sim \\mathcal{N}(0,1)$.\n\nThe $p$-variation along the partition $\\mathcal{P}_{n}$ is given by\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) = \\sum_{k=1}^{2^{n}} \\left|\\Delta W_{k}^{(n)}\\right|^{p}.\n$$\nSubstituting the representation of the increments in terms of $Z_k$, we obtain\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) = \\sum_{k=1}^{2^{n}} \\left|\\sqrt{\\Delta t^{(n)}} Z_k\\right|^{p} = \\sum_{k=1}^{2^{n}} \\left(\\Delta t^{(n)}\\right)^{p/2} |Z_k|^p = \\left(\\Delta t^{(n)}\\right)^{p/2} \\sum_{k=1}^{2^{n}} |Z_k|^p.\n$$\nReplacing $\\Delta t^{(n)}$ with $\\frac{t}{2^n}$, the expression becomes\n$$\nV^{(p)}_{t}(W;\\mathcal{P}_{n}) = \\left(\\frac{t}{2^n}\\right)^{p/2} \\sum_{k=1}^{2^{n}} |Z_k|^p.\n$$\nTo determine the limit of this expression as $n \\to \\infty$, we will analyze its behavior by examining its moments. Let $m_r = \\mathbb{E}[|Z|^r]$ denote the $r$-th absolute moment of a standard normal random variable. Since all moments of a normal distribution are finite, $m_r$ is a finite constant for any $r \\geq 0$.\n\nLet us compute the expectation of $V^{(p)}_{t}(W;\\mathcal{P}_{n})$. By the linearity of expectation and the fact that the $Z_k$ are i.i.d.,\n$$\n\\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right] = \\mathbb{E}\\left[\\left(\\frac{t}{2^n}\\right)^{p/2} \\sum_{k=1}^{2^{n}} |Z_k|^p\\right] = \\left(\\frac{t}{2^n}\\right)^{p/2} \\sum_{k=1}^{2^{n}} \\mathbb{E}[|Z_k|^p] = \\left(\\frac{t}{2^n}\\right)^{p/2} (2^n) m_p.\n$$\nSimplifying the expression for the expectation yields\n$$\n\\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right] = t^{p/2} \\frac{1}{(2^n)^{p/2-1}} m_p = t^{p/2} (2^n)^{1-p/2} m_p.\n$$\nWe now analyze the limit of this expectation for the two cases specified in the problem.\n\nCase 1: $p  2$\nFor $p  2$, the exponent $1 - \\frac{p}{2}$ is strictly negative. Consequently, as $n \\to \\infty$, the term $(2^n)^{1-p/2}$ converges to $0$.\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right] = \\lim_{n \\to \\infty} t^{p/2} (2^n)^{1-p/2} m_p = 0.\n$$\nThe random variable $V^{(p)}_{t}(W;\\mathcal{P}_{n})$ is a sum of non-negative terms, so it is itself non-negative. For any sequence of non-negative random variables $X_n$, if $\\mathbb{E}[X_n] \\to 0$, then $X_n \\to 0$ in probability. This follows from Markov's inequality, which states that for any $\\epsilon  0$,\n$$\n\\mathbb{P}\\left(V^{(p)}_{t}(W;\\mathcal{P}_{n}) \\geq \\epsilon\\right) \\leq \\frac{\\mathbb{E}\\left[V^{(p)}_{t}(W;\\mathcal{P}_{n})\\right]}{\\epsilon}.\n$$\nAs the right-hand side tends to $0$ as $n \\to \\infty$, we conclude that $V^{(p)}_{t}(W;\\mathcal{P}_{n})$ converges to $0$ in probability. The problem states that the limit $V^{(p)}_{t}(W) = \\lim_{n \\to \\infty} V^{(p)}_{t}(W;\\mathcal{P}_{n})$ exists almost surely. Almost sure convergence implies convergence in probability to the same limit. Since the limit in probability is unique, the almost sure limit must be $0$. Therefore,\n$$\nV^{(p)}_{t}(W) = 0 \\quad \\text{for } p  2.\n$$\n\nCase 2: $p=2$\nFor $p=2$, the exponent $1 - \\frac{p}{2}$ is $1 - \\frac{2}{2} = 0$. The expectation becomes\n$$\n\\mathbb{E}\\left[V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right] = t^{2/2} (2^n)^{1-1} m_2 = t \\cdot m_2.\n$$\nThe moment $m_2 = \\mathbb{E}[|Z|^2] = \\mathbb{E}[Z^2]$ for $Z \\sim \\mathcal{N}(0,1)$. The variance of a standard normal random variable is $\\mathrm{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2 = 1$. Since $\\mathbb{E}[Z]=0$, we have $\\mathbb{E}[Z^2] = 1$. Thus, $m_2=1$.\nThe expectation of the quadratic variation is constant for all $n$:\n$$\n\\mathbb{E}\\left[V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right] = t.\n$$\nTo show that $V^{(2)}_{t}(W;\\mathcal{P}_{n})$ converges to the constant $t$, we analyze its variance. Convergence in mean square to a constant implies convergence in probability to that constant.\n$$\n\\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = \\mathrm{Var}\\left(\\sum_{k=1}^{2^{n}} \\left(\\Delta W_{k}^{(n)}\\right)^2\\right).\n$$\nSince the increments $\\Delta W_{k}^{(n)}$ are independent, the squared increments $\\left(\\Delta W_{k}^{(n)}\\right)^2$ are also independent. Thus, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = \\sum_{k=1}^{2^{n}} \\mathrm{Var}\\left(\\left(\\Delta W_{k}^{(n)}\\right)^2\\right).\n$$\nFor each term, $\\mathrm{Var}\\left(\\left(\\Delta W_{k}^{(n)}\\right)^2\\right) = \\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^4\\right] - \\left(\\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^2\\right]\\right)^2$.\nWe know $\\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^2\\right] = \\mathrm{Var}(\\Delta W_{k}^{(n)}) = \\Delta t^{(n)}$.\nThe fourth moment of a normal distribution $\\mathcal{N}(0,\\sigma^2)$ is $3\\sigma^4$. Here, $\\sigma^2 = \\Delta t^{(n)}$, so $\\mathbb{E}\\left[\\left(\\Delta W_{k}^{(n)}\\right)^4\\right] = 3\\left(\\Delta t^{(n)}\\right)^2$.\nSubstituting these moments into the variance expression for a single term:\n$$\n\\mathrm{Var}\\left(\\left(\\Delta W_{k}^{(n)}\\right)^2\\right) = 3\\left(\\Delta t^{(n)}\\right)^2 - \\left(\\Delta t^{(n)}\\right)^2 = 2\\left(\\Delta t^{(n)}\\right)^2.\n$$\nNow, summing over all $k=1, \\dots, 2^n$:\n$$\n\\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = \\sum_{k=1}^{2^{n}} 2\\left(\\Delta t^{(n)}\\right)^2 = 2^n \\cdot 2\\left(\\frac{t}{2^n}\\right)^2 = 2^{n+1} \\frac{t^2}{(2^n)^2} = \\frac{2t^2}{2^n}.\n$$\nAs $n \\to \\infty$, the variance converges to zero: $\\lim_{n \\to \\infty} \\mathrm{Var}\\left(V^{(2)}_{t}(W;\\mathcal{P}_{n})\\right) = 0$.\nThe sequence of random variables $V^{(2)}_{t}(W;\\mathcal{P}_{n})$ has a constant mean $t$ and a variance that tends to $0$. This implies convergence in $L^2$ and therefore in probability to the mean $t$. By the same logic as in case 1, the assumed almost sure limit must be equal to this limit. Therefore,\n$$\nV^{(2)}_{t}(W) = t.\n$$\nCombining both cases, we can express the p-variation $V^{(p)}_{t}(W)$ as a piecewise function of $p$.", "answer": "$$\n\\boxed{\n\\begin{cases}\nt  \\text{if } p=2 \\\\\n0  \\text{if } p2\n\\end{cases}\n}\n$$", "id": "3006312"}, {"introduction": "The Wiener process serves as a fundamental building block for constructing other important stochastic models. This exercise introduces the Brownian bridge, a process constrained to begin and end at specific points, by applying a simple transformation to a standard Wiener process. By deriving its mean and covariance functions [@problem_id:3006273], you will gain practical experience in manipulating Gaussian processes and see how conditioning can fundamentally alter a process's statistical structure.", "problem": "Fix a deterministic horizon $T0$. Let $\\{W_t\\}_{t\\in[0,T]}$ be a Wiener process (also called standard Brownian motion) started at $W_0=0$, characterized by the axioms: (i) $W_0=0$ almost surely, (ii) for $0\\leq st\\leq T$, the increment $W_t-W_s$ is Gaussian with mean $0$ and variance $t-s$, (iii) disjoint increments are independent, and (iv) sample paths are almost surely continuous. Define the process $\\{B_t\\}_{t\\in[0,T]}$ by\n$$\nB_t \\coloneqq W_t - \\frac{t}{T}\\,W_T.\n$$\nStarting only from the above axioms and the elementary properties of expectation and covariance of random variables, derive the mean function $m(t)=\\mathbb{E}[B_t]$ and the covariance function $K(s,t)=\\mathrm{Cov}(B_s,B_t)$ for $s,t\\in[0,T]$ in closed form. Provide your final answer as the pair $m(t),K(s,t)$ expressed analytically in terms of $s$, $t$, and $T$.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in the theory of stochastic processes. All provided axioms and definitions are standard and self-consistent. We proceed to the solution.\n\nThe problem asks for the mean function $m(t)=\\mathbb{E}[B_t]$ and the covariance function $K(s,t)=\\mathrm{Cov}(B_s,B_t)$ of the process $\\{B_t\\}_{t\\in[0,T]}$ defined by $B_t \\coloneqq W_t - \\frac{t}{T}W_T$, where $\\{W_t\\}_{t\\in[0,T]}$ is a standard Wiener process.\n\nFirst, we derive the mean function $m(t)$. By the linearity of the expectation operator, we have:\n$$\nm(t) = \\mathbb{E}[B_t] = \\mathbb{E}\\left[W_t - \\frac{t}{T}W_T\\right] = \\mathbb{E}[W_t] - \\frac{t}{T}\\mathbb{E}[W_T].\n$$\nAccording to the axioms of the Wiener process, for any $u \\in [0,T]$, the increment $W_u - W_0$ is a Gaussian random variable with mean $0$. Since axiom (i) states that $W_0=0$ almost surely, it follows that $W_u$ itself has a mean of $0$.\n$$\n\\mathbb{E}[W_u] = \\mathbb{E}[W_u - W_0] = 0 \\quad \\text{for all } u \\in [0,T].\n$$\nApplying this result to our expression for $m(t)$, we find:\n$$\nm(t) = 0 - \\frac{t}{T} \\cdot 0 = 0.\n$$\nThus, the mean function is $m(t) = 0$ for all $t \\in [0,T]$.\n\nNext, we derive the covariance function $K(s,t)$ for $s,t \\in [0,T]$. The covariance is defined as:\n$$\nK(s,t) = \\mathrm{Cov}(B_s, B_t) = \\mathbb{E}[(B_s - \\mathbb{E}[B_s])(B_t - \\mathbb{E}[B_t])].\n$$\nSince we have just shown that $\\mathbb{E}[B_s]=0$ and $\\mathbb{E}[B_t]=0$, the expression simplifies to:\n$$\nK(s,t) = \\mathbb{E}[B_s B_t].\n$$\nWe substitute the definition of $B_s$ and $B_t$:\n$$\nK(s,t) = \\mathbb{E}\\left[\\left(W_s - \\frac{s}{T}W_T\\right)\\left(W_t - \\frac{t}{T}W_T\\right)\\right].\n$$\nExpanding the product inside the expectation:\n$$\nK(s,t) = \\mathbb{E}\\left[W_s W_t - \\frac{t}{T}W_s W_T - \\frac{s}{T}W_t W_T + \\frac{st}{T^2}W_T^2\\right].\n$$\nUsing the linearity of expectation, we get:\n$$\nK(s,t) = \\mathbb{E}[W_s W_t] - \\frac{t}{T}\\mathbb{E}[W_s W_T] - \\frac{s}{T}\\mathbb{E}[W_t W_T] + \\frac{st}{T^2}\\mathbb{E}[W_T^2].\n$$\nTo evaluate this, we must first find a general expression for $\\mathbb{E}[W_u W_v]$ for $u, v \\in [0,T]$. This term is the covariance $\\mathrm{Cov}(W_u, W_v)$, since the mean of the Wiener process is $0$. Let us assume, without loss of generality, that $u \\leq v$. We can write $W_v = W_u + (W_v - W_u)$. Then,\n$$\n\\mathbb{E}[W_u W_v] = \\mathbb{E}[W_u (W_u + W_v - W_u)] = \\mathbb{E}[W_u^2 + W_u(W_v - W_u)].\n$$\nBy linearity of expectation, this becomes $\\mathbb{E}[W_u^2] + \\mathbb{E}[W_u(W_v - W_u)]$. The increment $W_u = W_u - W_0$ is over the interval $[0,u]$, and the increment $W_v - W_u$ is over the interval $[u,v]$. Since $u \\leq v$, these intervals are disjoint. By axiom (iii), the random variables $W_u$ and $W_v - W_u$ are independent. Therefore, the expectation of their product is the product of their expectations:\n$$\n\\mathbb{E}[W_u(W_v - W_u)] = \\mathbb{E}[W_u]\\mathbb{E}[W_v - W_u] = 0 \\cdot 0 = 0.\n$$\nThis leaves us with $\\mathbb{E}[W_u W_v] = \\mathbb{E}[W_u^2]$. The term $\\mathbb{E}[W_u^2]$ is the variance of $W_u$, which by axiom (ii) is equal to $u-0=u$. So, for $u \\leq v$, $\\mathbb{E}[W_u W_v] = u$.\nBy symmetry, if $v \\leq u$, $\\mathbb{E}[W_u W_v] = v$. We can combine these two cases into a single expression:\n$$\n\\mathrm{Cov}(W_u,W_v) = \\mathbb{E}[W_u W_v] = \\min(u,v).\n$$\nNow we can use this result to evaluate the terms in our expression for $K(s,t)$:\n1. $\\mathbb{E}[W_s W_t] = \\min(s,t)$.\n2. $\\mathbb{E}[W_s W_T] = \\min(s,T) = s$, since $s \\in [0,T]$.\n3. $\\mathbb{E}[W_t W_T] = \\min(t,T) = t$, since $t \\in [0,T]$.\n4. $\\mathbb{E}[W_T^2] = \\mathbb{E}[W_T W_T] = \\min(T,T) = T$.\n\nSubstituting these into the formula for $K(s,t)$:\n$$\nK(s,t) = \\min(s,t) - \\frac{t}{T}(s) - \\frac{s}{T}(t) + \\frac{st}{T^2}(T).\n$$\nSimplifying the terms:\n$$\nK(s,t) = \\min(s,t) - \\frac{st}{T} - \\frac{st}{T} + \\frac{st}{T}.\n$$\nThis gives the final closed-form expression for the covariance function:\n$$\nK(s,t) = \\min(s,t) - \\frac{st}{T}.\n$$\nIn summary, the mean function is $m(t)=0$ and the covariance function is $K(s,t) = \\min(s,t) - \\frac{st}{T}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\min(s,t) - \\frac{st}{T} \\end{pmatrix}}\n$$", "id": "3006273"}]}