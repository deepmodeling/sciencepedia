{"hands_on_practices": [{"introduction": "Before we can properly study martingales, we must understand the structure of the information flow, or filtration, and its consequences. This practice explores the fundamental Debut Theorem, which establishes a deep connection between a filtration's properties and the crucial concept of a stopping time. You will verify why the assumption of right-continuity is essential for ensuring that the first time a process enters a set is a well-behaved stopping time, a cornerstone of the general theory of processes [@problem_id:2972107].", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ where the filtration $(\\mathcal{F}_t)_{t\\ge 0}$ is right-continuous, that is, for every $t\\ge 0$, $\\mathcal{F}_t=\\bigcap_{st}\\mathcal{F}_s$. The optional $\\sigma$-field $\\mathcal{O}$ on $\\Omega\\times[0,\\infty)$ is defined as the smallest $\\sigma$-field that makes every adapted right-continuous process $(Y_t)_{t\\ge 0}$ measurable as a function $(\\omega,t)\\mapsto Y_t(\\omega)$. A set $A\\subset \\Omega\\times[0,\\infty)$ is called optional if $A\\in\\mathcal{O}$. For any set $A\\subset \\Omega\\times[0,\\infty)$, define its debut $D_A:\\Omega\\to[0,\\infty]$ by\n$$\nD_A(\\omega):=\\inf\\{t\\ge 0:(\\omega,t)\\in A\\},\n$$\nwith the convention $\\inf\\varnothing=+\\infty$.\n\nUsing only the above core definitions and the right-continuity of the filtration (together with standard, widely accepted facts about measurability of optional sets), verify that for any optional set $A$, the debut $D_A$ is a $(\\mathcal{F}_t)_{t\\ge 0}$-stopping time; namely, for each $t\\ge 0$, the event $\\{D_A\\le t\\}$ belongs to $\\mathcal{F}_t$. Then, let $(X_t)_{t\\ge 0}$ be an adapted càdlàg (right-continuous with left limits) process. For a fixed $a\\in\\mathbb{R}$, consider the optional set\n$$\nA=\\{(\\omega,t)\\in\\Omega\\times[0,\\infty):X_t(\\omega)\\ge a\\}.\n$$\nCompute the debut $D_A$ explicitly in terms of $(X_t)_{t\\ge 0}$ and $a$. Your final answer must be a single symbolic expression. No numerical rounding is required.", "solution": "The problem statement is a valid mathematical exercise in the theory of stochastic processes. It is self-contained, scientifically grounded in the standard framework of general process theory, and well-posed. I will proceed with a full solution.\n\nThe problem consists of two parts. First, to verify that the debut of any optional set is a stopping time, given a right-continuous filtration. Second, to compute the debut for a specific optional set defined by a càdlàg process.\n\n**Part 1: Verification that the debut of an optional set is a stopping time**\n\nLet $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t\\ge 0}, \\mathbb{P})$ be a filtered probability space with a right-continuous filtration, i.e., $\\mathcal{F}_t = \\bigcap_{st} \\mathcal{F}_s$ for all $t \\ge 0$. Let $A \\subset \\Omega \\times [0, \\infty)$ be an optional set, meaning $A$ belongs to the optional $\\sigma$-field $\\mathcal{O}$. The debut of $A$ is the random variable $D_A: \\Omega \\to [0, \\infty]$ defined by $D_A(\\omega) = \\inf\\{t \\ge 0 : (\\omega, t) \\in A\\}$. We must verify that $D_A$ is an $(\\mathcal{F}_t)_{t\\ge 0}$-stopping time, which means showing that the event $\\{D_A \\le t\\}$ is in $\\mathcal{F}_t$ for every $t \\ge 0$.\n\nThe statement that the debut of any optional set is a stopping time under the assumption of a right-continuous filtration is a fundamental result in the general theory of processes, known as the **Debut Theorem**. A full proof for an arbitrary optional set $A$ is technical and typically involves a monotone class argument. The strategy is to first prove the result for a generating class of the optional $\\sigma$-field $\\mathcal{O}$ and then extend it to all sets in $\\mathcal{O}$.\n\nThe problem defines $\\mathcal{O}$ as the $\\sigma$-field generated by all adapted, right-continuous processes. A key generating class for $\\mathcal{O}$ consists of elementary optional sets of the form $A_{Y,B} = \\{(\\omega, t) : Y_t(\\omega) \\in B\\}$, where $(Y_t)_{t \\ge 0}$ is an adapted, right-continuous process and $B \\subset \\mathbb{R}$ is a Borel set. We will verify the theorem for this class of sets. The property for general Borel sets $B$ can be established by first considering open and closed sets.\n\nCase 1: $B$ is an open set.\nLet $A = \\{(\\omega, t) : Y_t(\\omega) \\in B\\}$ where $B$ is open. The debut is $D_A = \\inf\\{t \\ge 0 : Y_t \\in B\\}$. We want to show $\\{D_A \\le t\\} \\in \\mathcal{F}_t$.\nThe event $\\{D_A \\le t\\}$ occurs if and only if there exists some time $s \\in [0, t]$ such that $Y_s(\\omega) \\in B$. Because the process $(Y_t)$ is right-continuous and $B$ is an open set, if a path enters $B$ at time $s$, it must be in $B$ for some interval $[s, s+\\delta)$, and therefore at some rational time. Consequently, we can write the event using only rational times:\n$$\n\\{D_A \\le t\\} = \\{\\omega : \\exists s \\in [0, t] \\text{ such that } Y_s(\\omega) \\in B\\} = \\bigcup_{q \\in \\mathbb{Q} \\cap [0, t]} \\{\\omega : Y_q(\\omega) \\in B\\}\n$$\nFor each rational $q \\in [0, t]$, the process $(Y_t)$ is adapted, so $Y_q$ is $\\mathcal{F}_q$-measurable. Since $B$ is a Borel set, the event $\\{Y_q \\in B\\}$ is in $\\mathcal{F}_q$. As $q \\le t$, we have $\\mathcal{F}_q \\subset \\mathcal{F}_t$, so $\\{Y_q \\in B\\} \\in \\mathcal{F}_t$. The set $\\mathbb{Q} \\cap [0, t]$ is countable. A countable union of sets in the $\\sigma$-field $\\mathcal{F}_t$ is also in $\\mathcal{F}_t$. Therefore, $\\{D_A \\le t\\} \\in \\mathcal{F}_t$, and $D_A$ is a stopping time.\n\nCase 2: $B$ is a closed set.\nLet $A = \\{(\\omega, t) : Y_t(\\omega) \\in B\\}$ where $B$ is a closed set $F$. For this case, we consider a process $(Y_t)$ which is not only right-continuous but also has left limits (càdlàg), as specified in the second part of the problem. Let $D_F = \\inf\\{t \\ge 0 : Y_t \\in F\\}$.\nFor each integer $n \\ge 1$, define the open set $F_n = \\{x \\in \\mathbb{R} : d(x, F)  1/n\\}$, where $d(x,F)$ is the distance from a point $x$ to the set $F$.\nLet $T_n = \\inf\\{t \\ge 0 : Y_t \\in F_n\\}$. From Case 1, each $T_n$ is a stopping time.\nSince $F_{n+1} \\subset F_n$, we have $T_n \\le T_{n+1}$. Thus, the sequence of stopping times $(T_n)_{n\\ge 1}$ is non-decreasing. The limit $T = \\lim_{n \\to \\infty} T_n = \\sup_n T_n$ is also a stopping time, because for any $t \\ge 0$, $\\{T \\le t\\} = \\bigcap_n \\{T_n \\le t\\}$, and since each set $\\{T_n \\le t\\}$ is in $\\mathcal{F}_t$, their intersection is also in $\\mathcal{F}_t$.\nWe now show that $T = D_F$.\nSince $F \\subset F_n$ for all $n$, we have $T_n \\le D_F$ for all $n$, which implies $T \\le D_F$.\nFor the other inequality, fix an $\\omega \\in \\Omega$. Let $t_n = T_n(\\omega)$. For each $n$, by the right-continuity of the path $Y_.(\\omega)$ at time $t_n$, we have $d(Y_{t_n}(\\omega), F) \\le 1/n$. Let $t^* = T(\\omega) = \\sup_n t_n$. Since the process $(Y_t)$ is càdlàg, its paths have left limits. As $t_n \\uparrow t^*$, the limit $Y_{t^*-}(\\omega) = \\lim_{n \\to \\infty} Y_{t_n}(\\omega)$ exists. Since the distance function $d$ is continuous, we have:\n$$\nd(Y_{t^*-}(\\omega), F) = d\\left(\\lim_{n \\to \\infty} Y_{t_n}(\\omega), F\\right) = \\lim_{n \\to \\infty} d(Y_{t_n}(\\omega), F) \\le \\lim_{n \\to \\infty} \\frac{1}{n} = 0\n$$\nAs $F$ is a closed set, $d(x, F)=0$ implies $x \\in F$. So, $Y_{t^*-}(\\omega) \\in F$. This means that the process enters the set $F$ at or before time $t^*$. By definition of the debut, $D_F(\\omega) \\le t^* = T(\\omega)$.\nCombining the two inequalities, we have $D_F = T$. Since $T$ is a stopping time, $D_F$ is a stopping time.\n\nBy demonstrating that the debut is a stopping time for elementary optional sets generated by hitting open and closed sets, and noting that these generate the full class of such sets for any Borel set $B$, we have verified the theorem for this generating class. The full Debut Theorem asserts this property holds for *all* sets in the optional $\\sigma$-field $\\mathcal{O}$, a result which follows from a monotone class argument that we will not reproduce here. The right-continuity of the filtration is a crucial hypothesis for this general theorem to hold.\n\n**Part 2: Computation of the debut for a specific set**\n\nWe are given an adapted càdlàg process $(X_t)_{t\\ge 0}$ and a real number $a \\in \\mathbb{R}$. The optional set is defined as:\n$$\nA = \\{(\\omega,t) \\in \\Omega \\times [0,\\infty) : X_t(\\omega) \\ge a\\}\n$$\nWe are asked to compute its debut, $D_A$.\n\nBy the definition of the debut of a set provided in the problem statement:\n$$\nD_A(\\omega) = \\inf\\{t \\ge 0 : (\\omega,t) \\in A\\}\n$$\nSubstituting the specific definition of the set $A$ into this formula, we obtain:\n$$\nD_A(\\omega) = \\inf\\{t \\ge 0 : X_t(\\omega) \\ge a\\}\n$$\nThis expression defines the debut $D_A$ explicitly in terms of the process $(X_t)_{t\\ge 0}$ and the constant $a$. This is the standard definition of the first hitting time of the set $[a, \\infty)$ by the process $(X_t)$. There is no further simplification for a general càdlàg process.\n\nThe set $B = [a, \\infty)$ is a closed subset of $\\mathbb{R}$. As $(X_t)_{t \\ge 0}$ is an adapted càdlàg process, the argument from Part 1, Case 2, confirms that this debut $D_A$ is indeed a stopping time with respect to the (right-continuous) filtration $(\\mathcal{F}_t)_{t\\ge 0}$, ensuring the internal consistency of the problem's components.\nThe final computed expression for the debut $D_A$, omitting the dependence on $\\omega$ for notational simplicity, is the definition of this first hitting time.", "answer": "$$\n\\boxed{\\inf\\{t \\ge 0 : X_t \\ge a\\}}\n$$", "id": "2972107"}, {"introduction": "Stochastic integrals with respect to compensated random measures are canonical examples of martingales. This exercise provides a concrete application of the Itô isometry for jump processes, a tool for calculating the first and second moments of such integrals [@problem_id:2972117]. Mastering this calculation is fundamental for understanding the variance and behavior of martingales constructed from random measures, paralleling the well-known results for Brownian motion.", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\geq 0},\\mathbb{P})$ be a filtered probability space satisfying the usual conditions (in particular, the filtration is right-continuous and $\\mathcal{F}_{0}$ contains all $\\mathbb{P}$-null sets). Let $\\mu$ be a Poisson random measure (PRM) on $(0,\\infty)\\times\\mathbb{R}$ with $\\sigma$-finite intensity measure $\\Pi(\\mathrm{d}x)\\,\\mathrm{d}s$, adapted to $(\\mathcal{F}_{t})_{t\\geq 0}$. Let the compensated Poisson random measure be $\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)=\\mu(\\mathrm{d}s,\\mathrm{d}x)-\\Pi(\\mathrm{d}x)\\,\\mathrm{d}s$. Fix $t0$ and a deterministic measurable function $f:\\mathbb{R}\\to\\mathbb{R}$ such that $\\int_{\\mathbb{R}}f(x)^{2}\\,\\Pi(\\mathrm{d}x)\\infty$, and define the stochastic integral\n$$\nM_{t}:=\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x).\n$$\nUsing only foundational properties of Poisson random measures, compensated Poisson random measures, and continuous-time martingales under right-continuous filtrations, compute the expectation $\\mathbb{E}[M_{t}]$ and the variance $\\mathrm{Var}(M_{t})$ in closed form as expressions involving $f$ and $\\Pi$. Your final answer must be a single analytic expression or a single row matrix containing both quantities.", "solution": "The problem is valid. It is a well-posed question within the established mathematical framework of stochastic calculus, specifically concerning integration with respect to Poisson random measures. All terms are formally defined, and the given conditions are sufficient and consistent for deriving a unique solution.\n\nWe are asked to compute the expectation $\\mathbb{E}[M_{t}]$ and the variance $\\mathrm{Var}(M_{t})$ of the stochastic integral\n$$\nM_{t}:=\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)\n$$\nwhere $\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)=\\mu(\\mathrm{d}s,\\mathrm{d}x)-\\Pi(\\mathrm{d}x)\\,\\mathrm{d}s$ is the compensated Poisson random measure. The function $f:\\mathbb{R}\\to\\mathbb{R}$ is deterministic and measurable, satisfying $\\int_{\\mathbb{R}}f(x)^{2}\\,\\Pi(\\mathrm{d}x)\\infty$.\n\n**1. Calculation of the Expectation $\\mathbb{E}[M_{t}]$**\n\nA fundamental property of stochastic integrals with respect to a compensated random measure is that they are local martingales. Given the condition $\\int_{\\mathbb{R}}f(x)^{2}\\,\\Pi(\\mathrm{d}x)\\infty$, the process $(M_s)_{s\\ge 0}$ defined by $M_{s}:=\\int_{0}^{s}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}u,\\mathrm{d}x)$ is a square-integrable martingale.\n\nAs $(M_s)_{s \\geq 0}$ is a martingale with respect to the filtration $(\\mathcal{F}_s)_{s \\geq 0}$, its expectation is constant over time. We have:\n$$\n\\mathbb{E}[M_{t}] = \\mathbb{E}[M_{0}]\n$$\nThe value of the process at time $s=0$ is given by the integral over the interval $[0,0]$:\n$$\nM_{0} = \\int_{0}^{0}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x) = 0\n$$\nTherefore, the expectation is:\n$$\n\\mathbb{E}[M_{t}] = 0\n$$\n\nAlternatively, we can use the definition of the compensated measure and linearity of expectation. Assuming we can apply a stochastic Fubini theorem (which holds since $f$ is deterministic and meets the integrability condition):\n$$\n\\mathbb{E}[M_{t}] = \\mathbb{E}\\left[\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)\\right] = \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\mathbb{E}[\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)]\n$$\nBy definition of the compensated measure $\\tilde{\\mu}$, its expectation is zero. To see this, recall that for any measurable set $A \\subset (0,\\infty)\\times\\mathbb{R}$, the expectation of the Poisson random measure $\\mu(A)$ is its intensity $\\nu(A) = \\int_A \\Pi(dx)ds$. So, for an infinitesimal region, $\\mathbb{E}[\\mu(\\mathrm{d}s,\\mathrm{d}x)] = \\Pi(\\mathrm{d}x)\\mathrm{d}s$. Thus:\n$$\n\\mathbb{E}[\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x)] = \\mathbb{E}[\\mu(\\mathrm{d}s,\\mathrm{d}x) - \\Pi(\\mathrm{d}x)\\mathrm{d}s] = \\mathbb{E}[\\mu(\\mathrm{d}s,\\mathrm{d}x)] - \\Pi(\\mathrm{d}x)\\mathrm{d}s = \\Pi(\\mathrm{d}x)\\mathrm{d}s - \\Pi(\\mathrm{d}x)\\mathrm{d}s = 0\n$$\nSubstituting this back into the integral for $\\mathbb{E}[M_t]$ yields:\n$$\n\\mathbb{E}[M_{t}] = \\int_{0}^{t}\\int_{\\mathbb{R}} f(x) \\cdot 0 = 0\n$$\n\n**2. Calculation of the Variance $\\mathrm{Var}(M_{t})$**\n\nThe variance of a random variable $X$ is defined as $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Since we have established that $\\mathbb{E}[M_t]=0$, the variance is simply the second moment:\n$$\n\\mathrm{Var}(M_{t}) = \\mathbb{E}[M_{t}^{2}]\n$$\nTo compute $\\mathbb{E}[M_{t}^{2}]$, we employ the Itô isometry for stochastic integrals with respect to random measures. This is a foundational property for square-integrable martingales. For a predictable process $H(s,x)$, the isometry states:\n$$\n\\mathbb{E}\\left[ \\left( \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x) \\right)^2 \\right] = \\mathbb{E}\\left[ \\int_{0}^{t}\\int_{\\mathbb{R}} H(s,x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s \\right]\n$$\nThis property originates from the fact that the compensated Poisson process has orthogonal increments, meaning $\\mathbb{E}[\\tilde{\\mu}(A)\\tilde{\\mu}(B)]=0$ for disjoint measurable sets $A, B$. The variance of the integral is thus the sum (or integral) of the variances of its infinitesimal parts.\n\nIn our problem, the integrand is $H(s,x) = f(x)$. Since $f$ is a deterministic function, it is a predictable process. We can therefore apply the isometry:\n$$\n\\mathrm{Var}(M_{t}) = \\mathbb{E}[M_{t}^{2}] = \\mathbb{E}\\left[ \\left( \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)\\,\\tilde{\\mu}(\\mathrm{d}s,\\mathrm{d}x) \\right)^2 \\right] = \\mathbb{E}\\left[ \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s \\right]\n$$\nThe expression inside the expectation on the right-hand side is\n$$\n\\int_{0}^{t}\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s\n$$\nSince $f(x)$, $\\Pi(\\mathrm{d}x)$, and the integration limits $0$ and $t$ are all deterministic, the entire expression is a deterministic quantity, not a random variable. The expectation of a constant is the constant itself. Therefore, we can remove the expectation operator:\n$$\n\\mathrm{Var}(M_{t}) = \\int_{0}^{t}\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\\mathrm{d}s\n$$\nThe inner integral, $\\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)$, is a constant with respect to the integration variable $s$. Let us denote this constant by $C$:\n$$\nC = \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\n$$\nThe problem statement guarantees that $C  \\infty$. We can now evaluate the outer integral:\n$$\n\\mathrm{Var}(M_{t}) = \\int_{0}^{t} C\\,\\mathrm{d}s = C \\int_{0}^{t} \\mathrm{d}s = C [s]_{0}^{t} = C(t-0) = Ct\n$$\nSubstituting the expression for $C$ back, we obtain the final result for the variance:\n$$\n\\mathrm{Var}(M_{t}) = t \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\n$$\n\nIn summary, the expectation and variance are:\n$$\n\\mathbb{E}[M_{t}] = 0\n$$\n$$\n\\mathrm{Var}(M_{t}) = t \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x)\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  t \\int_{\\mathbb{R}} f(x)^2\\,\\Pi(\\mathrm{d}x) \\end{pmatrix}}\n$$", "id": "2972117"}, {"introduction": "The Doléans–Dade exponential, $\\mathcal{E}(M)$, is a special process that is central to the theory of measure changes, but it is guaranteed only to be a local martingale. This advanced practice investigates the conditions under which $\\mathcal{E}(M)$ is a true martingale, a property critical for applications like the Girsanov theorem [@problem_id:2972098]. You will apply Novikov’s condition, a powerful sufficient criterion, and probe its limits by constructing a case where the condition fails, yet the process remains a true martingale, deepening your understanding of this important tool.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_t)_{t\\ge 0},\\mathbb{P})$ satisfying the usual conditions with a right-continuous filtration, and let $(W_t)_{t\\ge 0}$ be a standard Brownian motion (BM) adapted to $(\\mathcal{F}_t)_{t\\ge 0}$. Let $M$ be a continuous local martingale with $M_0=0$ and quadratic variation process $\\langle M\\rangle$. The Doléans–Dade stochastic exponential of $M$ is the process\n$$\n\\mathcal{E}(M)_t := \\exp\\!\\Big(M_t - \\tfrac{1}{2}\\langle M\\rangle_t\\Big),\\quad t\\ge 0.\n$$\nNovikov’s condition states that if\n$$\n\\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right]  \\infty\n$$\nfor some fixed $T0$, then $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is a true martingale with respect to the given right-continuous filtration.\n\nYou will verify Novikov’s condition and its implication in a concrete setting, and then construct a counterexample in which Novikov’s condition fails while the stochastic exponential remains a true martingale.\n\nFix $T0$ and proceed in two parts:\n\nPart A (verification under Novikov). Define the predictable integrand $\\theta_t := c\\,\\exp(-\\lambda t)$ for $t\\in[0,T]$, where $c0$ and $\\lambda0$ are constants, and set\n$$\nM_t := \\int_0^t \\theta_s\\,\\mathrm{d}W_s,\\quad 0\\le t\\le T.\n$$\nStarting from the definitions of quadratic variation and the stochastic exponential for continuous local martingales driven by Brownian motion, derive $\\langle M\\rangle_T$ and compute the quantity\n$$\nI := \\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right].\n$$\nUse this computation to verify Novikov’s condition and explain why right-continuity of the filtration guarantees that the martingale property is assessed on $[0,T]$. Conclude, via first principles (Gaussian integrals conditional on the deterministic integrand), that $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is a true martingale and determine $\\mathbb{E}[\\mathcal{E}(M)_T]$.\n\nPart B (counterexample to the necessity of Novikov). Let $\\xi$ be an $\\mathcal{F}_0$-measurable random variable independent of $(W_t)_{t\\ge 0}$ with a Pareto distribution of shape $\\alpha2$ and scale $x_m0$, i.e.,\n$$\n\\mathbb{P}(\\xix)=\\left(\\frac{x_m}{x}\\right)^{\\alpha}\\quad\\text{for }x\\ge x_m,\n$$\nand set $\\theta_t := \\xi$ for $t\\in[0,T]$, $M_t := \\int_0^t \\theta_s\\,\\mathrm{d}W_s = \\xi W_t$. Using only foundational facts about Gaussian integrals and heavy-tail distributions, show that Novikov’s condition fails by establishing\n$$\n\\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] \\,=\\, \\mathbb{E}\\!\\left[\\exp\\!\\left(\\tfrac{1}{2}\\,\\xi^2 T\\right)\\right] \\,=\\, \\infty.\n$$\nDespite this failure, justify carefully—by conditioning on $\\xi$ and using properties of the right-continuous filtration enlarged at time $0$ by $\\sigma(\\xi)$—that $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is still a true martingale and compute\n$$\nJ := \\mathbb{E}[\\mathcal{E}(M)_T].\n$$\n\nAnswer specification: Return the ordered pair $(I,J)$ as a single analytic expression. No rounding is required. No physical units are involved. The angle unit is not applicable.", "solution": "The problem is validated as sound, well-posed, and grounded in the established mathematical theory of stochastic calculus. All definitions and conditions are standard, and the tasks are logically structured to test core concepts related to martingales and stochastic exponentials.\n\n### Part A: Verification under Novikov's Condition\n\nWe are given the process $M_t := \\int_0^t \\theta_s\\,\\mathrm{d}W_s$ for $t \\in [0,T]$, where the integrand $\\theta_s := c\\,\\exp(-\\lambda s)$ is a deterministic function with constants $c0$ and $\\lambda0$.\n\nFirst, we derive the quadratic variation process $\\langle M \\rangle_t$. For a continuous local martingale of the form $M_t = \\int_0^t \\theta_s \\, \\mathrm{d}W_s$, where $\\theta$ is a predictable process, the quadratic variation is given by $\\langle M \\rangle_t = \\int_0^t \\theta_s^2 \\, \\mathrm{d}s$.\nIn our case, $\\theta_s$ is deterministic, so we have:\n$$\n\\langle M \\rangle_t = \\int_0^t (c\\,\\exp(-\\lambda s))^2\\,\\mathrm{d}s = c^2 \\int_0^t \\exp(-2\\lambda s)\\,\\mathrm{d}s.\n$$\nEvaluating this integral at the fixed time $T0$:\n$$\n\\langle M \\rangle_T = c^2 \\left[ -\\frac{1}{2\\lambda} \\exp(-2\\lambda s) \\right]_0^T = c^2 \\left( -\\frac{1}{2\\lambda} \\exp(-2\\lambda T) - \\left(-\\frac{1}{2\\lambda} \\exp(0)\\right) \\right) = \\frac{c^2}{2\\lambda} \\left(1 - \\exp(-2\\lambda T)\\right).\n$$\nNext, we compute the quantity $I := \\mathbb{E}[\\exp(\\frac{1}{2}\\langle M\\rangle_T)]$. Since the integrand $\\theta_s$ is deterministic, the quadratic variation $\\langle M \\rangle_T$ is a deterministic constant, not a random variable. The expectation of a constant is the constant itself.\n$$\nI = \\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] = \\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right) = \\exp\\left(\\frac{1}{2} \\cdot \\frac{c^2}{2\\lambda} \\left(1 - \\exp(-2\\lambda T)\\right)\\right).\n$$\n$$\nI = \\exp\\left(\\frac{c^2}{4\\lambda} \\left(1 - \\exp(-2\\lambda T)\\right)\\right).\n$$\nSince $c$, $\\lambda$, and $T$ are finite positive constants, the value of $\\langle M \\rangle_T$ is a finite positive number. Consequently, $I$ is also a finite positive number. Thus, Novikov's condition, $\\mathbb{E}[\\exp(\\frac{1}{2}\\langle M\\rangle_T)]  \\infty$, is satisfied.\n\nThe problem states that the underlying filtered probability space satisfies the \"usual conditions,\" which implies that the filtration $(\\mathcal{F}_t)_{t\\ge 0}$ is right-continuous and complete. The right-continuity of the filtration is a crucial technical condition in the general theory of stochastic processes. It ensures that the class of martingales is stable and that important theorems, such as the optional stopping theorem, hold for all bounded stopping times. For a process on a finite interval $[0,T]$, being a martingale under a right-continuous filtration implies uniform integrability, which is a key property. These conditions establish a robust framework, precluding certain mathematical pathologies and ensuring that continuous local martingales that are martingales for fixed times remain so in the broader sense required by the theory.\n\nBy Novikov's theorem, since the condition is met, the process $(\\mathcal{E}(M)_t)_{0\\le t\\le T}$ is a true martingale. A fundamental property of a martingale $X_t$ is that $\\mathbb{E}[X_t] = \\mathbb{E}[X_0]$ for all $t$. Here, $\\mathcal{E}(M)_0 = \\exp(M_0 - \\frac{1}{2}\\langle M\\rangle_0) = \\exp(0-0) = 1$. Therefore, we must have $\\mathbb{E}[\\mathcal{E}(M)_T] = 1$.\n\nWe can also verify this from first principles. We need to compute $\\mathbb{E}[\\mathcal{E}(M)_T]$.\n$$\n\\mathbb{E}[\\mathcal{E}(M)_T] = \\mathbb{E}\\left[\\exp\\left(M_T - \\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right].\n$$\nSince $\\langle M \\rangle_T$ is deterministic, we can factor it out of the expectation:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_T] = \\exp\\left(-\\tfrac{1}{2}\\langle M\\rangle_T\\right) \\mathbb{E}\\left[\\exp(M_T)\\right].\n$$\nThe process $M_T = \\int_0^T \\theta_s \\, \\mathrm{d}W_s$ is a Wiener integral with a deterministic integrand. As such, $M_T$ is a Gaussian random variable with mean $\\mathbb{E}[M_T] = \\mathbb{E}[\\int_0^T \\theta_s \\, \\mathrm{d}W_s] = \\int_0^T \\theta_s \\, \\mathbb{E}[\\mathrm{d}W_s] = 0$ and variance $\\mathrm{Var}(M_T) = \\mathbb{E}[M_T^2] = \\mathbb{E}[(\\int_0^T \\theta_s \\, \\mathrm{d}W_s)^2] = \\int_0^T \\theta_s^2 \\, \\mathrm{d}s = \\langle M \\rangle_T$.\nSo, $M_T \\sim \\mathcal{N}(0, \\langle M \\rangle_T)$. The moment-generating function of a normal random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $\\mathbb{E}[\\exp(uX)] = \\exp(u\\mu + \\frac{1}{2}u^2\\sigma^2)$. For $M_T$ with $u=1$, $\\mu=0$ and $\\sigma^2=\\langle M \\rangle_T$, we have:\n$$\n\\mathbb{E}[\\exp(M_T)] = \\exp\\left(1 \\cdot 0 + \\tfrac{1}{2} \\cdot 1^2 \\cdot \\langle M \\rangle_T\\right) = \\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right).\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_T] = \\exp\\left(-\\tfrac{1}{2}\\langle M\\rangle_T\\right) \\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right) = 1.\n$$\nSince $(\\mathcal{E}(M)_t)_{t\\ge 0}$ is a positive continuous local martingale and $\\mathbb{E}[\\mathcal{E}(M)_T]=\\mathcal{E}(M)_0=1$, it is a true martingale on $[0,T]$.\n\n### Part B: Counterexample to the Necessity of Novikov's Condition\n\nHere, the integrand is $\\theta_t = \\xi$ for $t \\in [0,T]$, where $\\xi$ is an $\\mathcal{F}_0$-measurable random variable, independent of $(W_t)_{t \\ge 0}$, with a Pareto distribution defined by $\\mathbb{P}(\\xi  x) = (x_m/x)^\\alpha$ for $x \\ge x_m$, with shape $\\alpha2$ and scale $x_m0$. Thus, $M_t = \\int_0^t \\xi \\, \\mathrm{d}W_s = \\xi W_t$.\n\nThe quadratic variation is $\\langle M \\rangle_t = \\int_0^t \\theta_s^2 \\, \\mathrm{d}s = \\int_0^t \\xi^2 \\, \\mathrm{d}s = \\xi^2 t$. At time $T$, we have $\\langle M \\rangle_T = \\xi^2 T$.\n\nWe first show that Novikov's condition fails. We must evaluate $\\mathbb{E}[\\exp(\\frac{1}{2}\\langle M\\rangle_T)]$.\n$$\n\\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] = \\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\xi^2 T\\right)\\right].\n$$\nTo compute this expectation, we need the probability density function (PDF) of $\\xi$. The cumulative distribution function (CDF) for $x \\ge x_m$ is $F_\\xi(x) = 1 - \\mathbb{P}(\\xi  x) = 1 - (x_m/x)^\\alpha$. Differentiating with respect to $x$ gives the PDF:\n$$\nf_\\xi(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(1 - x_m^\\alpha x^{-\\alpha}\\right) = \\alpha x_m^\\alpha x^{-\\alpha-1}, \\quad \\text{for } x \\ge x_m.\n$$\nThe expectation is then given by the integral:\n$$\n\\mathbb{E}\\left[\\exp\\left(\\tfrac{T}{2}\\xi^2\\right)\\right] = \\int_{x_m}^{\\infty} \\exp\\left(\\tfrac{T}{2}x^2\\right) f_\\xi(x)\\,\\mathrm{d}x = \\int_{x_m}^{\\infty} \\exp\\left(\\tfrac{T}{2}x^2\\right) \\alpha x_m^\\alpha x^{-\\alpha-1}\\,\\mathrm{d}x.\n$$\nThe integrand involves the product of an exponential function $\\exp(\\frac{T}{2}x^2)$, which grows faster than any polynomial in $x$, and a term $x^{-\\alpha-1}$, which decays polynomially. For any constants $K0$ and $p0$, the function $\\exp(Kx^2)$ grows faster than $x^p$ as $x \\to \\infty$. Specifically, for any $p$, there exists an $x_0$ such that for all $x  x_0$, $\\exp(\\frac{T}{2}x^2)  x^{\\alpha+1}$. For such $x$, the integrand is bounded below:\n$$\n\\exp\\left(\\tfrac{T}{2}x^2\\right) \\alpha x_m^\\alpha x^{-\\alpha-1}  x^{\\alpha+1} \\alpha x_m^\\alpha x^{-\\alpha-1} = \\alpha x_m^\\alpha  0.\n$$\nSince the integrand is asymptotically bounded below by a positive constant, its integral over an infinite interval $[x_m, \\infty)$ diverges. Therefore,\n$$\n\\mathbb{E}\\left[\\exp\\left(\\tfrac{1}{2}\\langle M\\rangle_T\\right)\\right] = \\infty.\n$$\nNovikov's condition fails.\n\nDespite this, we can show that $(\\mathcal{E}(M)_t)_{0 \\le t \\le T}$ is a true martingale. The process is $\\mathcal{E}(M)_t = \\exp(M_t - \\frac{1}{2}\\langle M\\rangle_t) = \\exp(\\xi W_t - \\frac{1}{2}\\xi^2 t)$. We verify the martingale property by showing $\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathcal{E}(M)_s$ for $0 \\le s  t \\le T$.\nWe can write $\\mathcal{E}(M)_t$ as:\n$$\n\\mathcal{E}(M)_t = \\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\exp\\left(\\xi W_s - \\tfrac{1}{2}\\xi^2 s\\right) = \\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\mathcal{E}(M)_s.\n$$\nTaking the conditional expectation with respect to $\\mathcal{F}_s$:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathbb{E}\\left[\\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\mathcal{E}(M)_s \\Big| \\mathcal{F}_s\\right].\n$$\nSince $\\mathcal{E}(M)_s$ is $\\mathcal{F}_s$-measurable, we can factor it out:\n$$\n\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathcal{E}(M)_s \\, \\mathbb{E}\\left[\\exp\\left(\\xi(W_t - W_s) - \\tfrac{1}{2}\\xi^2(t-s)\\right) \\Big| \\mathcal{F}_s\\right].\n$$\nThe random variable $\\xi$ is $\\mathcal{F}_0$-measurable, hence it is $\\mathcal{F}_s$-measurable for any $s \\ge 0$. The increment $W_t - W_s$ is independent of the filtration $\\mathcal{F}_s$. We use the property that if $X$ is $\\mathcal{G}$-measurable and $Y$ is independent of $\\mathcal{G}$, then $\\mathbb{E}[g(X,Y)|\\mathcal{G}] = h(X)$, where $h(x) = \\mathbb{E}[g(x,Y)]$.\nHere, $X = \\xi$, $Y=W_t - W_s$, $\\mathcal{G}=\\mathcal{F}_s$, and $g(X,Y) = \\exp(X Y - \\frac{1}{2}X^2(t-s))$.\nWe compute $h(x) = \\mathbb{E}[g(x, W_t - W_s)]$. The increment $W_t-W_s \\sim \\mathcal{N}(0, t-s)$.\n$$\nh(x) = \\mathbb{E}\\left[\\exp\\left(x(W_t - W_s) - \\tfrac{1}{2}x^2(t-s)\\right)\\right] = \\exp\\left(-\\tfrac{1}{2}x^2(t-s)\\right) \\mathbb{E}\\left[\\exp\\left(x(W_t - W_s)\\right)\\right].\n$$\nThe MGF of $W_t-W_s$ evaluated at $x$ is $\\exp(\\frac{1}{2}x^2(t-s))$. So,\n$$\nh(x) = \\exp\\left(-\\tfrac{1}{2}x^2(t-s)\\right) \\exp\\left(\\tfrac{1}{2}x^2(t-s)\\right) = 1.\n$$\nSince this holds for any value $x$ that $\\xi$ may take, we have $\\mathbb{E}\\left[\\exp\\left(\\xi(W_t - W_s) - \\frac{1}{2}\\xi^2(t-s)\\right) \\big| \\mathcal{F}_s\\right] = 1$.\nTherefore, $\\mathbb{E}[\\mathcal{E}(M)_t|\\mathcal{F}_s] = \\mathcal{E}(M)_s \\cdot 1 = \\mathcal{E}(M)_s$. This proves that $(\\mathcal{E}(M)_t)_{0 \\le t \\le T}$ is a true martingale.\n\nFinally, we compute $J := \\mathbb{E}[\\mathcal{E}(M)_T]$. Since it is a true martingale and $\\mathcal{E}(M)_0 = 1$, we must have $J=1$. We verify this directly using the law of total expectation (conditioning on $\\xi$):\n$$\nJ = \\mathbb{E}[\\mathcal{E}(M)_T] = \\mathbb{E}\\left[\\mathbb{E}[\\mathcal{E}(M)_T | \\xi]\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\exp\\left(\\xi W_T - \\tfrac{1}{2}\\xi^2 T\\right) \\Big| \\xi\\right]\\right].\n$$\nThe inner expectation is computed by treating $\\xi$ as a constant, say $x$. Since $W_T$ is independent of $\\xi$ and $W_T \\sim \\mathcal{N}(0,T)$:\n$$\n\\mathbb{E}\\left[\\exp\\left(x W_T - \\tfrac{1}{2}x^2 T\\right)\\right] = \\exp\\left(-\\tfrac{1}{2}x^2 T\\right) \\mathbb{E}\\left[\\exp\\left(x W_T\\right)\\right] = \\exp\\left(-\\tfrac{1}{2}x^2 T\\right)\\exp\\left(\\tfrac{1}{2}x^2 T\\right) = 1.\n$$\nThis result holds for any value $x$ of $\\xi$. Thus, the conditional expectation $\\mathbb{E}[\\mathcal{E}(M)_T | \\xi]$ is equal to $1$ almost surely.\nThe outer expectation is then trivial:\n$$\nJ = \\mathbb{E}[1] = 1.\n$$\nThe condition $\\alpha2$ ensures $\\mathbb{E}[\\xi^2]  \\infty$, which implies that the Itô integral $M_t = \\xi W_t$ is an $L^2$-martingale, but this fact is not essential for the argument regarding $\\mathcal{E}(M)_t$, which relies on direct conditioning.\n\nThe ordered pair to be returned is $(I,J)$.\n$I = \\exp\\left(\\frac{c^2}{4\\lambda}(1 - \\exp(-2\\lambda T))\\right)$.\n$J=1$.", "answer": "$$\n\\boxed{\\pmatrix{\\exp\\left(\\frac{c^2}{4\\lambda}\\left(1 - \\exp(-2\\lambda T)\\right)\\right)  1}}\n$$", "id": "2972098"}]}