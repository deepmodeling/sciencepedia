{"hands_on_practices": [{"introduction": "A cornerstone in the study of martingales and stochastic integration is the concept of uniform integrability (UI). It provides the necessary and sufficient condition for the convergence of conditional expectations and is crucial for theorems like optional stopping. While it's a standard result that a family of random variables bounded in $L^p$ for some $p>1$ is uniformly integrable, the same is not true for $L^1$-boundedness. This first exercise challenges you to build, from first principles, the canonical counterexample that illustrates this fundamental gap, reinforcing the precise definition of uniform integrability. [@problem_id:2973879]", "problem": "Consider the probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ where $\\Omega=[0,1]$, $\\mathcal{F}$ is the Borel $\\sigma$-algebra on $[0,1]$, and $\\mathbb{P}$ is Lebesgue measure. For each $n\\in\\mathbb{N}$, let $A_{n}=[0,1/n)$ and define the sequence of random variables $\\{X_{n}\\}_{n\\in\\mathbb{N}}$ by $X_{n}(\\omega)=n\\,\\mathbf{1}_{A_{n}}(\\omega)$. This construction yields a sequence with $\\mathbb{E}[|X_{n}|]=1$ for all $n\\in\\mathbb{N}$. \n\nUsing first-principles definitions from measure-theoretic probability, compute, for an arbitrary $K0$, the quantity \n$$\nS(K)=\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|K\\}}\\,\\big],\n$$\nand then determine the limit \n$$\n\\lim_{K\\to\\infty}S(K).\n$$\nReport the value of $\\lim_{K\\to\\infty}S(K)$ as your final answer. No rounding is required, and no units are involved. Your reasoning should start from the core definitions of uniform integrability and the integrability of random variables, and should remain scientifically and mathematically consistent with standard facts used in advanced treatments of stochastic differential equations, including the relationship between $L^{p}$-boundedness (for $p1$) and uniform integrability.", "solution": "We begin by recalling the core definitions relevant to the task. A family of integrable random variables $\\{Y_{i}\\}_{i\\in I}$ is said to be uniformly integrable if \n$$\n\\lim_{K\\to\\infty}\\sup_{i\\in I}\\mathbb{E}\\big[\\,|Y_{i}|\\,\\mathbf{1}_{\\{|Y_{i}|K\\}}\\,\\big]=0.\n$$\nThis is the standard characterization of uniform integrability in measure-theoretic probability and is foundational in the study of stochastic processes, including martingales encountered in stochastic differential equations. It is also a well-known fact that $L^{p}$-boundedness for $p>1$ implies uniform integrability, whereas mere $L^{1}$-boundedness does not guarantee uniform integrability.\n\nWe analyze the given sequence $\\{X_{n}\\}_{n\\in\\mathbb{N}}$ defined by $X_{n}(\\omega)=n\\,\\mathbf{1}_{A_{n}}(\\omega)$, where $A_{n}=[0,1/n)$ and $\\mathbb{P}(A_{n})=1/n$. First, we verify integrability and compute $\\mathbb{E}[|X_{n}|]$:\n$$\n\\mathbb{E}[|X_{n}|]=\\int_{\\Omega}|X_{n}(\\omega)|\\,\\mathrm{d}\\mathbb{P}(\\omega)=\\int_{\\Omega}n\\,\\mathbf{1}_{A_{n}}(\\omega)\\,\\mathrm{d}\\mathbb{P}(\\omega)=n\\,\\mathbb{P}(A_{n})=n\\cdot\\frac{1}{n}=1.\n$$\nThus each $X_{n}$ is integrable with $\\mathbb{E}[|X_{n}|]=1$, so the family is $L^{1}$-bounded.\n\nNext, we compute the tail functional $S(K)=\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|K\\}}\\,\\big]$. Note that $|X_{n}|$ takes only the values $0$ and $n$. Therefore, the indicator can be simplified as \n$$\n\\mathbf{1}_{\\{|X_{n}|K\\}}=\\mathbf{1}_{\\{nK\\}}\\,\\mathbf{1}_{A_{n}},\n$$\nbecause $|X_{n}|K$ occurs if and only if the value $n$ is realized and $nK$. Hence,\n$$\n\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|K\\}}\\,\\big]\n=\\mathbb{E}\\big[\\,n\\,\\mathbf{1}_{A_{n}}\\,\\mathbf{1}_{\\{nK\\}}\\,\\big]\n=n\\,\\mathbf{1}_{\\{nK\\}}\\mathbb{P}(A_{n})\n=n\\,\\mathbf{1}_{\\{nK\\}}\\cdot\\frac{1}{n}\n=\\mathbf{1}_{\\{nK\\}}.\n$$\nThus, for any fixed $K0$,\n$$\n\\sup_{n\\in\\mathbb{N}}\\mathbb{E}\\big[\\,|X_{n}|\\,\\mathbf{1}_{\\{|X_{n}|K\\}}\\,\\big]\n=\\sup_{n\\in\\mathbb{N}}\\mathbf{1}_{\\{nK\\}}=1,\n$$\nbecause for every finite $K0$ there exist integers $nK$, making the indicator equal to $1$ for those $n$.\n\nConsequently,\n$$\nS(K)=1\\quad\\text{for all }K0,\n$$\nand the limit is\n$$\n\\lim_{K\\to\\infty}S(K)=\\lim_{K\\to\\infty}1=1.\n$$\n\nThis shows the family $\\{X_{n}\\}$ is not uniformly integrable, since the defining criterion for uniform integrability requires the limit to be $0$. For additional context connecting to $L^{p}$ theories and Doob-type controls, we compute higher moments: for any $p>1$,\n$$\n\\mathbb{E}\\big[|X_{n}|^{p}\\big]=\\mathbb{E}\\big[n^{p}\\,\\mathbf{1}_{A_{n}}\\big]=n^{p}\\cdot\\frac{1}{n}=n^{p-1},\n$$\nwhich is unbounded in $n$, implying the sequence is not $L^{p}$-bounded for any $p>1$. Since $L^{p}$-boundedness for $p>1$ implies uniform integrability, the failure of such boundedness aligns with the explicit failure of uniform integrability we computed via the tail functional.\n\nTherefore, the requested limit equals $1$.", "answer": "$$\\boxed{1}$$", "id": "2973857"}, {"introduction": "Having established that $L^1$-boundedness alone does not guarantee uniform integrability, we turn to more powerful characterizations like the de la Vallée-Poussin criterion. This theorem provides a test for UI using a convex function $\\Phi$ that grows super-linearly. This practice problem invites you to probe the theoretical necessity of the convexity condition. By constructing a counterexample with a concave function, you will demonstrate why convexity is not just a technical convenience but an essential ingredient of the criterion. [@problem_id:2973849]", "problem": "Let $\\{X_{n}\\}_{n \\geq 1}$ be a sequence of integrable random variables on a fixed probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$. Recall that the family $\\{X_{n}\\}_{n \\geq 1}$ is called uniformly integrable (UI) if\n$$\n\\lim_{K \\to \\infty} \\sup_{n \\geq 1} \\mathbb{E}\\big[|X_{n}| \\mathbf{1}_{\\{|X_{n}|  K\\}}\\big] \\;=\\; 0.\n$$\nIn the theory of submartingales and stochastic differential equations, Doob’s $L^{p}$ inequalities (for $p1$) imply that an $L^{p}$-bounded submartingale is uniformly integrable. A classical characterization of UI for a family $\\{X_{n}\\}$ uses the de la Vallée-Poussin criterion with a convex Young function $\\Phi$ such that $\\Phi(x)/x \\to \\infty$ as $x \\to \\infty$. In this problem, you will demonstrate that replacing convexity by concavity completely breaks this characterization.\n\nConstruct, with full justification from first principles, a counterexample exhibiting a nondecreasing concave function $\\Phi:[0,\\infty)\\to[0,\\infty)$ with $\\Phi(0)=0$ and a sequence $\\{X_{n}\\}_{n\\ge 1}$ of integrable random variables such that:\n- $\\sup_{n\\ge 1}\\mathbb{E}[\\Phi(|X_{n}|)]\\infty$,\n- but $\\{X_{n}\\}_{n\\ge 1}$ fails to be uniformly integrable.\n\nYour construction must be explicit on a standard atomless probability space, for example by using an independent sequence of random variables uniformly distributed on $[0,1]$, and must verify both properties via direct calculation using the definition of uniform integrability.\n\nReport, as your final answer, only the explicit analytical expressions for your chosen $\\Phi$ and $X_{n}$, formatted as a single row matrix $\\big(\\Phi(x),\\,X_{n}\\big)$, with $X_{n}$ written as a measurable function of your auxiliary uniform random variables. No rounding is required, and no physical units are involved.", "solution": "The problem statement is a valid exercise in measure-theoretic probability. It asks for the construction of a counterexample to demonstrate that the convexity condition in the de la Vallée-Poussin criterion for uniform integrability is necessary and cannot be replaced by concavity. The problem is well-posed, scientifically grounded, and contains no ambiguities or contradictions. We shall proceed to construct the required counterexample.\n\nOur objective is to find a nondecreasing concave function $\\Phi:[0,\\infty)\\to[0,\\infty)$ with $\\Phi(0)=0$ and a sequence of integrable random variables $\\{X_{n}\\}_{n\\ge 1}$ on a suitable probability space such that $\\sup_{n\\ge 1}\\mathbb{E}[\\Phi(|X_{n}|)]\\infty$, but the sequence $\\{X_{n}\\}_{n\\ge 1}$ is not uniformly integrable.\n\nLet our probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ be the interval $[0,1]$ equipped with the Borel $\\sigma$-algebra and the Lebesgue measure. Let $\\{\\xi_{n}\\}_{n\\ge 1}$ be a sequence of independent random variables, each uniformly distributed on $[0,1]$.\n\n**Step 1: Construction of the function $\\Phi$**\n\nWe need to choose a function $\\Phi$ that is nondecreasing, concave, and satisfies $\\Phi(0)=0$. A canonical choice for such a function is the square root function. Let us define\n$$\n\\Phi(x) = \\sqrt{x}, \\quad \\text{for } x \\in [0,\\infty).\n$$\nWe verify its properties:\n- $\\Phi(0) = \\sqrt{0} = 0$.\n- For $x  0$, the first derivative is $\\Phi'(x) = \\frac{1}{2\\sqrt{x}}  0$, so $\\Phi$ is strictly increasing, and thus nondecreasing, on $[0,\\infty)$.\n- For $x  0$, the second derivative is $\\Phi''(x) = -\\frac{1}{4x^{3/2}}  0$, which implies that $\\Phi$ is strictly concave on $(0,\\infty)$. By continuity, it is concave on $[0,\\infty)$.\nThus, $\\Phi(x) = \\sqrt{x}$ satisfies all the required conditions.\n\n**Step 2: Construction of the sequence of random variables $\\{X_{n}\\}_{n \\ge 1}$**\n\nWe need to construct a sequence $\\{X_n\\}$ which is not uniformly integrable. A typical way to construct such a sequence is to define random variables that take very large values with small probabilities, such that \"mass escapes to infinity\". Let us define the sequence $\\{X_{n}\\}_{n \\ge 1}$ using our auxiliary sequence $\\{\\xi_n\\}$ as follows:\n$$\nX_{n} = n^{2} \\mathbf{1}_{\\{\\xi_{n} \\le 1/n^{2}\\}}.\n$$\nHere, $\\mathbf{1}_{A}$ is the indicator function of the event $A$. For each $n \\ge 1$, $X_n$ is a simple random variable that takes the value $n^2$ with probability $\\mathbb{P}(\\xi_n \\le 1/n^2) = 1/n^2$, and the value $0$ with probability $\\mathbb{P}(\\xi_n  1/n^2) = 1 - 1/n^2$.\n\nThe random variables are nonnegative, so $|X_n| = X_n$. We verify they are integrable:\n$$\n\\mathbb{E}[|X_{n}|] = \\mathbb{E}[X_n] = n^{2} \\cdot \\mathbb{P}(\\xi_n \\le 1/n^2) + 0 \\cdot \\mathbb{P}(\\xi_n  1/n^2) = n^{2} \\cdot \\frac{1}{n^{2}} = 1.\n$$\nSince $\\mathbb{E}[|X_n|] = 1$ for all $n \\ge 1$, the sequence is $L^1$-bounded.\n\n**Step 3: Verification of the condition on $\\mathbb{E}[\\Phi(|X_n|)]$**\n\nWe now check if $\\sup_{n\\ge 1}\\mathbb{E}[\\Phi(|X_{n}|)]$ is finite. Using our choices for $\\Phi$ and $X_n$:\n$$\n\\Phi(|X_{n}|) = \\Phi(X_n) = \\sqrt{X_n} = \\sqrt{n^{2} \\mathbf{1}_{\\{\\xi_n \\le 1/n^2\\}}} = n \\mathbf{1}_{\\{\\xi_n \\le 1/n^2\\}}.\n$$\nThe random variable $\\Phi(|X_n|)$ takes the value $n$ with probability $1/n^2$ and the value $0$ otherwise.\nThe expectation is therefore:\n$$\n\\mathbb{E}[\\Phi(|X_{n}|)] = n \\cdot \\mathbb{P}(\\xi_n \\le 1/n^2) + 0 \\cdot \\mathbb{P}(\\xi_n  1/n^2) = n \\cdot \\frac{1}{n^{2}} = \\frac{1}{n}.\n$$\nNow we evaluate the supremum over all $n \\ge 1$:\n$$\n\\sup_{n\\ge 1}\\mathbb{E}[\\Phi(|X_{n}|)] = \\sup_{n\\ge 1} \\frac{1}{n} = 1.\n$$\nSince $1  \\infty$, the condition $\\sup_{n\\ge 1}\\mathbb{E}[\\Phi(|X_{n}|)]  \\infty$ is satisfied.\n\n**Step 4: Verification that $\\{X_{n}\\}_{n \\ge 1}$ is not uniformly integrable**\n\nA family $\\{Y_i\\}$ is uniformly integrable if $\\lim_{K \\to \\infty} \\sup_{i} \\mathbb{E}[|Y_i| \\mathbf{1}_{\\{|Y_i|  K\\}}] = 0$. We must show that this condition fails for our sequence $\\{X_n\\}$.\n\nLet us compute the quantity $\\mathbb{E}[|X_n| \\mathbf{1}_{\\{|X_n|  K\\}}]$ for an arbitrary $K0$. Since $X_n \\ge 0$, this is $\\mathbb{E}[X_n \\mathbf{1}_{\\{X_n  K\\}}]$.\nThe random variable $X_n$ takes only two values: $0$ and $n^2$.\n- If $K \\ge n^2$, the event $\\{X_n  K\\}$ is impossible (has probability $0$), so $\\mathbb{E}[X_n \\mathbf{1}_{\\{X_n  K\\}}] = 0$.\n- If $0 \\le K  n^2$, the event $\\{X_n  K\\}$ is equivalent to the event $\\{X_n = n^2\\}$, which has probability $1/n^2$. In this case,\n$$\n\\mathbb{E}[X_n \\mathbf{1}_{\\{X_n  K\\}}] = \\mathbb{E}[X_n \\mathbf{1}_{\\{X_n = n^2\\}}] = n^2 \\cdot \\mathbb{P}(X_n = n^2) = n^2 \\cdot \\frac{1}{n^2} = 1.\n$$\n\nNow, let's analyze the expression $\\sup_{n \\ge 1} \\mathbb{E}[|X_n| \\mathbf{1}_{\\{|X_n|  K\\}}]$. For any given $K0$, we can always choose an integer $n_0$ large enough such that $n_0^2  K$ (for instance, any $n_0  \\sqrt{K}$). For this specific choice $n=n_0$, we have $K  n_0^2$, and therefore:\n$$\n\\mathbb{E}[|X_{n_0}| \\mathbf{1}_{\\{|X_{n_0}|  K\\}}] = 1.\n$$\nThis implies that for any $K0$, the supremum is at least $1$:\n$$\n\\sup_{n \\ge 1} \\mathbb{E}[|X_{n}| \\mathbf{1}_{\\{|X_{n}|  K\\}}] \\ge \\mathbb{E}[|X_{n_0}| \\mathbf{1}_{\\{|X_{n_0}|  K\\}}] = 1.\n$$\nTaking the limit as $K \\to \\infty$, we find:\n$$\n\\lim_{K \\to \\infty} \\sup_{n \\ge 1} \\mathbb{E}[|X_{n}| \\mathbf{1}_{\\{|X_{n}|  K\\}}] \\ge 1.\n$$\nSince the limit is not $0$, the sequence $\\{X_n\\}_{n\\ge 1}$ is not uniformly integrable.\n\n**Conclusion**\nWe have successfully constructed a nondecreasing concave function $\\Phi(x) = \\sqrt{x}$ and a sequence of integrable random variables $X_n = n^{2} \\mathbf{1}_{\\{\\xi_n \\le 1/n^2\\}}$ such that $\\sup_{n\\ge 1}\\mathbb{E}[\\Phi(|X_{n}|)] = 1  \\infty$, but the family $\\{X_n\\}_{n \\ge 1}$ is not uniformly integrable. This demonstrates that the convexity of $\\Phi$ is an essential hypothesis in the de la Vallée-Poussin criterion for uniform integrability.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{x}  n^{2} \\mathbf{1}_{\\{\\xi_n \\le 1/n^2\\}}\n\\end{pmatrix}\n}\n$$", "id": "2973849"}, {"introduction": "We now apply our understanding of uniform integrability and related inequalities to a quintessential object in financial mathematics and stochastic calculus: the Doléans-Dade exponential martingale. This exercise provides a comprehensive workout, guiding you from the construction of a martingale via conditional expectation to proving its uniform integrability by establishing $L^p$-boundedness for $p1$. The final step involves applying Doob's celebrated $L^p$ maximal inequality to derive an explicit bound on the supremum of the process, showcasing how these theoretical tools provide powerful control over the paths of stochastic processes. [@problem_id:2973872]", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$ supporting a standard Brownian motion $(W_{t})_{t\\in[0,T]}$ with its usual augmented filtration $(\\mathcal{F}_{t})_{t\\in[0,T]}$. Fix parameters $T0$ and $\\alpha\\in\\mathbb{R}$. Define the terminal random variable\n$$\n\\xi \\coloneqq \\exp\\!\\big(\\alpha W_{T} - \\tfrac{1}{2}\\alpha^{2}T\\big),\n$$\nand the continuous-time process\n$$\nX_{t} \\coloneqq \\mathbb{E}\\!\\left[\\xi \\,\\middle|\\, \\mathcal{F}_{t}\\right], \\quad t\\in[0,T].\n$$\nYou will work from first principles: the definitions of conditional expectation, the independent increment and Gaussian distribution properties of Brownian motion, the definition of submartingale, and the statement of Doob’s $L^{p}$ inequalities for $p1$, without assuming any specialized shortcut formulas beyond these foundational facts.\n\nTasks:\n1. Show that $(X_{t})_{t\\in[0,T]}$ is a nonnegative submartingale and determine a closed-form expression for $X_{t}$.\n2. For a fixed $p1$, verify integrability of $X_{t}$ at all finite times by computing $\\sup_{0\\le t\\le T}\\mathbb{E}\\!\\left[X_{t}^{p}\\right]$ in closed form. Explain why this implies uniform integrability on $[0,T]$ via the de la Vallée-Poussin criterion.\n3. Using Doob’s $L^{p}$ inequality for nonnegative submartingales, provide the simplest closed-form upper bound for $\\mathbb{E}\\!\\left[\\sup_{0\\le s\\le T} X_{s}^{p}\\right]$ in terms of $p$, $\\alpha$, and $T$.\n\nExpress your final answer as a single closed-form analytic expression with no units.", "solution": "The problem statement is validated as mathematically sound and well-posed. We proceed with the solution.\n\nThe problem asks for a three-part analysis of the process $X_{t} \\coloneqq \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{t}]$ for $t\\in[0,T]$, where $\\xi \\coloneqq \\exp(\\alpha W_{T} - \\tfrac{1}{2}\\alpha^{2}T)$.\n\nFirst, we address Task 1: to show that $(X_{t})_{t\\in[0,T]}$ is a nonnegative submartingale and to determine its closed-form expression.\nThe terminal variable $\\xi$ is defined as $\\xi = \\exp(\\alpha W_{T} - \\frac{1}{2}\\alpha^{2}T)$. Since the exponential function has a strictly positive range, we have $\\xi  0$ almost surely. The process $X_{t}$ is the conditional expectation of this strictly positive random variable, $X_{t} = \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{t}]$. The conditional expectation of a non-negative (or strictly positive) random variable is itself non-negative. Therefore, $X_{t} \\ge 0$ for all $t\\in[0,T]$, establishing non-negativity.\n\nTo show that $(X_{t})_{t\\in[0,T]}$ is a submartingale, we must verify two conditions:\n1. $X_{t}$ is $\\mathcal{F}_{t}$-adapted and $\\mathbb{E}[|X_{t}|]  \\infty$ for all $t \\in [0,T]$.\n2. For any $s, t$ with $0 \\le s  t \\le T$, we have $\\mathbb{E}[X_{t} \\,|\\, \\mathcal{F}_{s}] \\ge X_{s}$.\n\nThe first condition holds: $X_{t}$ is $\\mathcal{F}_{t}$-adapted by its definition as a conditional expectation with respect to $\\mathcal{F}_{t}$. Its integrability will be demonstrated explicitly in Task 2 for any power $p \\ge 1$, which includes the case $p=1$. For the second condition, we use the tower property of conditional expectation. For $st$,\n$$\n\\mathbb{E}[X_{t} \\,|\\, \\mathcal{F}_{s}] = \\mathbb{E}\\big[\\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{t}] \\,\\big|\\, \\mathcal{F}_{s}\\big] = \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{s}].\n$$\nBy the definition of $X_{s}$, we have $X_{s} = \\mathbb{E}[\\xi \\,|\\, \\mathcal{F}_{s}]$. Thus, we find that $\\mathbb{E}[X_{t} \\,|\\, \\mathcal{F}_{s}] = X_{s}$. This indicates that $(X_{t})_{t\\in[0,T]}$ is a martingale. Since equality implies the 'greater than or equal to' condition, every martingale is also a submartingale. So, $(X_{t})_{t\\in[0,T]}$ is a nonnegative submartingale.\n\nNext, we find the closed-form expression for $X_{t}$.\n$$\nX_{t} = \\mathbb{E}\\left[\\exp\\left(\\alpha W_{T} - \\frac{1}{2}\\alpha^{2}T\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right].\n$$\nWe decompose the Brownian motion path $W_T$ as $W_{T} = W_{t} + (W_{T}-W_{t})$ and rewrite the exponent:\n$$\nX_{t} = \\mathbb{E}\\left[\\exp\\left(\\alpha W_{t} + \\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}t - \\frac{1}{2}\\alpha^{2}(T-t)\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right]\n$$\n$$\nX_{t} = \\mathbb{E}\\left[\\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right) \\exp\\left(\\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}(T-t)\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right].\n$$\nThe term $\\exp(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t)$ is $\\mathcal{F}_{t}$-measurable, so it can be factored out of the conditional expectation:\n$$\nX_{t} = \\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right) \\mathbb{E}\\left[\\exp\\left(\\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}(T-t)\\right) \\,\\middle|\\, \\mathcal{F}_{t}\\right].\n$$\nThe increment $W_{T}-W_{t}$ is independent of the filtration $\\mathcal{F}_{t}$. Consequently, any function of this increment is also independent of $\\mathcal{F}_{t}$. Therefore, the conditional expectation becomes an unconditional expectation:\n$$\nX_{t} = \\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right) \\mathbb{E}\\left[\\exp\\left(\\alpha(W_{T}-W_{t}) - \\frac{1}{2}\\alpha^{2}(T-t)\\right)\\right].\n$$\nLet the random variable $Z = W_{T}-W_{t}$. From the properties of standard Brownian motion, $Z$ follows a Gaussian distribution with mean $0$ and variance $T-t$, i.e., $Z \\sim \\mathcal{N}(0, T-t)$. The expectation term is:\n$$\n\\mathbb{E}\\left[\\exp(\\alpha Z)\\right] \\exp\\left(-\\frac{1}{2}\\alpha^{2}(T-t)\\right).\n$$\nThe moment-generating function of a Gaussian random variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ is $\\mathbb{E}[\\exp(sY)] = \\exp(s\\mu + \\frac{1}{2}s^{2}\\sigma^{2})$. For $Z$, we have $s=\\alpha$, $\\mu=0$, and $\\sigma^{2}=T-t$. Thus,\n$$\n\\mathbb{E}[\\exp(\\alpha Z)] = \\exp\\left(\\alpha \\cdot 0 + \\frac{1}{2}\\alpha^{2}(T-t)\\right) = \\exp\\left(\\frac{1}{2}\\alpha^{2}(T-t)\\right).\n$$\nSubstituting this back, the expectation term evaluates to:\n$$\n\\exp\\left(\\frac{1}{2}\\alpha^{2}(T-t)\\right) \\exp\\left(-\\frac{1}{2}\\alpha^{2}(T-t)\\right) = \\exp(0) = 1.\n$$\nTherefore, the closed-form expression for $X_{t}$ is\n$$\nX_{t} = \\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right).\n$$\nThis is the well-known Doléans-Dade exponential martingale.\n\nFor Task 2, we fix $p1$ and compute $\\sup_{0\\le t\\le T}\\mathbb{E}[X_{t}^{p}]$.\nFirst, we compute $\\mathbb{E}[X_{t}^{p}]$ for any $t \\in [0,T]$.\n$$\nX_{t}^{p} = \\left(\\exp\\left(\\alpha W_{t} - \\frac{1}{2}\\alpha^{2}t\\right)\\right)^{p} = \\exp\\left(p\\alpha W_{t} - \\frac{p}{2}\\alpha^{2}t\\right).\n$$\nTaking the expectation:\n$$\n\\mathbb{E}[X_{t}^{p}] = \\mathbb{E}\\left[\\exp\\left(p\\alpha W_{t} - \\frac{p}{2}\\alpha^{2}t\\right)\\right] = \\exp\\left(-\\frac{p}{2}\\alpha^{2}t\\right) \\mathbb{E}\\left[\\exp(p\\alpha W_{t})\\right].\n$$\nWe use the moment-generating function for $W_{t} \\sim \\mathcal{N}(0,t)$ with parameter $s=p\\alpha$:\n$$\n\\mathbb{E}\\left[\\exp(p\\alpha W_{t})\\right] = \\exp\\left(\\frac{1}{2}(p\\alpha)^{2}t\\right) = \\exp\\left(\\frac{p^{2}\\alpha^{2}t}{2}\\right).\n$$\nSubstituting this into the expression for $\\mathbb{E}[X_{t}^{p}]$:\n$$\n\\mathbb{E}[X_{t}^{p}] = \\exp\\left(-\\frac{p\\alpha^{2}t}{2}\\right) \\exp\\left(\\frac{p^{2}\\alpha^{2}t}{2}\\right) = \\exp\\left(\\frac{p^{2}\\alpha^{2}t - p\\alpha^{2}t}{2}\\right) = \\exp\\left(\\frac{p(p-1)\\alpha^{2}t}{2}\\right).\n$$\nNow, we find the supremum over $t \\in [0,T]$. Let $C = \\frac{1}{2}p(p-1)\\alpha^{2}$. Since $p>1$, we have $p-1>0$, and thus $C \\ge 0$. The function $f(t) = \\exp(Ct)$ is non-decreasing for $C \\ge 0$. Therefore, its supremum on the interval $[0,T]$ is attained at $t=T$.\n$$\n\\sup_{0\\le t\\le T}\\mathbb{E}[X_{t}^{p}] = \\mathbb{E}[X_{T}^{p}] = \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right).\n$$\nThis value is finite for any given $p>1$, $\\alpha \\in \\mathbb{R}$ and $T>0$. For the second part of Task 2, we explain why this implies uniform integrability. The de la Vallée-Poussin criterion states that a family of random variables $\\{Y_i\\}_{i \\in I}$ is uniformly integrable if there exists a non-negative, increasing, and convex function $\\Phi: [0, \\infty) \\to [0, \\infty)$ such that $\\lim_{x\\to\\infty} \\frac{\\Phi(x)}{x} = \\infty$ and $\\sup_{i \\in I} \\mathbb{E}[\\Phi(|Y_i|)]  \\infty$.\nWe consider the family of random variables $\\{X_t\\}_{t \\in [0,T]}$. Since we are given $p>1$, we can choose the function $\\Phi(x) = x^{p}$. This function is non-negative, increasing for $x \\ge 0$, and convex. It also satisfies $\\lim_{x\\to\\infty} \\frac{x^p}{x} = \\lim_{x\\to\\infty} x^{p-1} = \\infty$ since $p-1 > 0$. We have shown that\n$$\n\\sup_{t \\in [0,T]} \\mathbb{E}[\\Phi(|X_t|)] = \\sup_{t \\in [0,T]} \\mathbb{E}[X_t^p] = \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right)  \\infty.\n$$\nTherefore, by the de la Vallée-Poussin criterion, the family $\\{X_t\\}_{t \\in [0,T]}$ is uniformly integrable.\n\nFinally, for Task 3, we use Doob's $L^{p}$ inequality to find an upper bound for $\\mathbb{E}[\\sup_{0\\le s\\le T} X_{s}^{p}]$.\nDoob's $L^{p}$ inequality for a non-negative submartingale $(Y_{t})_{t\\in[0,T]}$ and a constant $p>1$ states:\n$$\n\\mathbb{E}\\left[\\left(\\sup_{0\\le t\\le T} Y_{t}\\right)^{p}\\right] \\le \\left(\\frac{p}{p-1}\\right)^{p} \\mathbb{E}[Y_{T}^{p}].\n$$\nWe have already established that $(X_{t})_{t\\in[0,T]}$ is a non-negative submartingale. Since the function $x \\mapsto x^p$ is increasing for non-negative $x$, we have $(\\sup_{0\\le s\\le T} X_s)^p = \\sup_{0\\le s\\le T} X_s^p$. Applying the inequality to $X_t$:\n$$\n\\mathbb{E}\\left[\\sup_{0\\le s\\le T} X_{s}^{p}\\right] \\le \\left(\\frac{p}{p-1}\\right)^{p} \\mathbb{E}[X_{T}^{p}].\n$$\nFrom Task 2, we have the closed-form expression for $\\mathbb{E}[X_{T}^{p}]$:\n$$\n\\mathbb{E}[X_{T}^{p}] = \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right).\n$$\nSubstituting this into the inequality, we obtain the simplest closed-form upper bound:\n$$\n\\mathbb{E}\\left[\\sup_{0\\le s\\le T} X_{s}^{p}\\right] \\le \\left(\\frac{p}{p-1}\\right)^{p} \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right).\n$$\nThis expression provides the required upper bound in terms of $p$, $\\alpha$, and $T$.", "answer": "$$\n\\boxed{\\left(\\frac{p}{p-1}\\right)^{p} \\exp\\left(\\frac{1}{2}p(p-1)\\alpha^{2}T\\right)}\n$$", "id": "2973872"}]}