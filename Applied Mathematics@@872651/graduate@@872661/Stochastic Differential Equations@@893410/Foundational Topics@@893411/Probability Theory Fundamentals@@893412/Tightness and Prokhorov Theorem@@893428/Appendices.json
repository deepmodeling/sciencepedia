{"hands_on_practices": [{"introduction": "To effectively apply Prokhorov's theorem, we must first have a firm grasp of what weak convergence entails and, just as importantly, what it does not. This exercise [@problem_id:3005015] provides a crucial hands-on example distinguishing weak convergence from the stronger notion of convergence in total variation. By analyzing a sequence of Gaussian laws converging to a Dirac measure, you will see firsthand how the choice of test functions—continuous versus general measurable—defines the nature of the convergence and can lead to starkly different results.", "problem": "Let $\\left(\\Omega,\\mathcal{F},\\mathbb{P}\\right)$ be a probability space supporting a one-dimensional standard Brownian motion $W=\\left(W_t\\right)_{t\\ge 0}$. Consider, for each integer $n\\ge 1$, the linear stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_t^{(n)} \\;=\\; -\\,X_t^{(n)}\\,\\mathrm{d}t \\;+\\; \\sigma_n\\,\\mathrm{d}W_t,\\qquad X_0^{(n)}=0,\n$$\nwhere $\\sigma_n0$ and $\\sigma_n\\downarrow 0$ as $n\\to\\infty$. Let $\\mu_n$ denote the law of $X_1^{(n)}$ on $\\mathbb{R}$ with its Borel $\\sigma$-algebra, and let $\\mu$ denote the Dirac measure at $0$, written $\\mu=\\delta_0$.\n\nUse the following foundational definitions and facts as your starting point:\n\n- Weak convergence (convergence in distribution): For probability measures $\\mu_n,\\mu$ on a Polish space (complete separable metric space), $\\mu_n\\Rightarrow\\mu$ if and only if $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded continuous function $f$.\n- Total variation distance: For probability measures $\\mu$ and $\\nu$ on a measurable space, define $\\|\\mu-\\nu\\|_{\\mathrm{TV}}:=\\sup_{A\\in\\mathcal{B}}|\\mu(A)-\\nu(A)|$, where $\\mathcal{B}$ is the $\\sigma$-algebra of measurable sets.\n- Tightness: A family $\\{\\mu_n:n\\ge 1\\}$ of probability measures on a Polish space is tight if for every $\\varepsilon0$ there exists a compact set $K$ such that $\\inf_{n\\ge 1}\\mu_n(K)\\ge 1-\\varepsilon$.\n- Prokhorov's theorem: On a Polish space, tightness of a family of probability measures is equivalent to relative compactness with respect to weak convergence; that is, every sequence has a weakly convergent subsequence.\n\nSelect all statements that are correct.\n\nA. For $\\sigma_n=1/\\sqrt{n}$, one has $\\mu_n\\Rightarrow\\mu$, $\\{\\mu_n\\}_{n\\ge 1}$ is tight, and moreover $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}=1$ for all $n$, so $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.\n\nB. By Prokhorov's theorem, tightness of $\\{\\mu_n\\}_{n\\ge 1}$ together with uniqueness of the weak limit implies $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\to 0$.\n\nC. By the portmanteau theorem, $\\mu_n\\Rightarrow\\mu$ implies $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded measurable function $f$, hence total variation convergence follows.\n\nD. The failure of total variation convergence in this example is explained by the fact that total variation distance tests all bounded measurable functions (equivalently, all measurable sets), including indicators of sets such as $\\{0\\}$ whose indicator function is discontinuous, whereas weak convergence only tests bounded continuous functions.\n\nE. The sequence $\\{\\mu_n\\}_{n\\ge 1}$ cannot be tight because $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.", "solution": "The problem presents a sequence of stochastic processes $X_t^{(n)}$ governed by linear stochastic differential equations (SDEs) and asks to evaluate statements concerning the convergence of the laws of these processes at time $t=1$.\n\nFirst, we solve the SDE for $X_t^{(n)}$. The SDE is a standard Ornstein-Uhlenbeck process:\n$$\n\\mathrm{d}X_t^{(n)} = -X_t^{(n)}\\mathrm{d}t + \\sigma_n\\mathrm{d}W_t, \\quad X_0^{(n)} = 0\n$$\nThe explicit solution to this SDE is given by applying Itô's formula to $e^t X_t^{(n)}$ or by using an integrating factor $e^t$. The solution is:\n$$\nX_t^{(n)} = e^{-t}X_0^{(n)} + \\int_0^t e^{-(t-s)} \\sigma_n \\mathrm{d}W_s\n$$\nWith the initial condition $X_0^{(n)}=0$, this simplifies to:\n$$\nX_t^{(n)} = \\sigma_n \\int_0^t e^{-(t-s)} \\mathrm{d}W_s\n$$\nThe random variable $X_t^{(n)}$ is an Itô integral with a deterministic integrand, and as such, it is a Gaussian random variable. Its mean is:\n$$\n\\mathbb{E}[X_t^{(n)}] = \\mathbb{E}\\left[\\sigma_n \\int_0^t e^{-(t-s)} \\mathrm{d}W_s\\right] = 0\n$$\nIts variance is calculated using the Itô isometry:\n$$\n\\mathrm{Var}(X_t^{(n)}) = \\mathbb{E}[(X_t^{(n)})^2] = \\sigma_n^2 \\mathbb{E}\\left[\\left(\\int_0^t e^{-(t-s)} \\mathrm{d}W_s\\right)^2\\right] = \\sigma_n^2 \\int_0^t (e^{-(t-s)})^2 \\mathrm{d}s\n$$\n$$\n\\mathrm{Var}(X_t^{(n)}) = \\sigma_n^2 \\int_0^t e^{-2(t-s)} \\mathrm{d}s = \\sigma_n^2 \\left[\\frac{e^{-2(t-s)}}{2}\\right]_{s=0}^{s=t} = \\frac{\\sigma_n^2}{2}(1 - e^{-2t})\n$$\nWe are interested in the law $\\mu_n$ of $X_1^{(n)}$, so we set $t=1$. The random variable $X_1^{(n)}$ is normally distributed with mean $0$ and variance $v_n^2 := \\frac{\\sigma_n^2}{2}(1-e^{-2})$. So, $\\mu_n = \\mathcal{N}\\left(0, v_n^2\\right)$.\n\nThe problem states that $\\sigma_n > 0$ and $\\sigma_n \\downarrow 0$ as $n\\to\\infty$. This implies that the variance $v_n^2 \\to 0$ as $n\\to\\infty$. The limit measure is $\\mu = \\delta_0$, which is the law of a random variable that is identically $0$. This can be viewed as a degenerate normal distribution $\\mathcal{N}(0,0)$.\n\nNow we analyze the convergence properties.\n**Weak Convergence ($\\mu_n \\Rightarrow \\mu$):**\nA sequence of Gaussian distributions $\\mathcal{N}(m_n, s_n^2)$ converges weakly to $\\mathcal{N}(m, s^2)$ if and only if $m_n \\to m$ and $s_n^2 \\to s^2$. Here, $m_n=0$ for all $n$, and $v_n^2 \\to 0$. The limit measure $\\delta_0$ corresponds to $m=0$ and $s^2=0$. Thus, $\\mu_n = \\mathcal{N}(0, v_n^2)$ converges weakly to $\\delta_0 = \\mu$. This can also be seen by noting that $X_1^{(n)} \\to 0$ in $L^2(\\Omega)$, since $\\mathbb{E}[(X_1^{(n)})^2] = v_n^2 \\to 0$, which implies convergence in probability, which in turn implies convergence in distribution (weak convergence).\n\n**Tightness:**\nA sequence of probability measures on a Polish space that converges weakly is necessarily tight. This is a direct consequence of Prokhorov's theorem. Therefore, the family $\\{\\mu_n\\}_{n\\ge 1}$ is tight. We can also show this directly. For any $\\varepsilon > 0$, we need to find a compact set $K_M = [-M, M]$ such that $\\mu_n(K_M) \\ge 1-\\varepsilon$ for all $n$. By Chebyshev's inequality:\n$$\n\\mu_n(\\mathbb{R} \\setminus K_M) = \\mathbb{P}(|X_1^{(n)}| > M) \\le \\frac{\\mathrm{Var}(X_1^{(n)})}{M^2} = \\frac{v_n^2}{M^2}\n$$\nSince $v_n^2 \\to 0$, the sequence $\\{v_n^2\\}_{n\\ge 1}$ is bounded, say by $V_{\\max} = v_1^2 = \\frac{\\sigma_1^2}{2}(1-e^{-2})$. Thus, $\\mathbb{P}(|X_1^{(n)}| > M) \\le \\frac{V_{\\max}}{M^2}$. Choosing $M > \\sqrt{V_{\\max}/\\varepsilon}$ ensures that $\\mathbb{P}(|X_1^{(n)}| > M)  \\varepsilon$ for all $n \\ge 1$. Hence, $\\{\\mu_n\\}_{n\\ge 1}$ is tight.\n\n**Total Variation Convergence ($\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} \\to 0$):**\nThe measure $\\mu_n = \\mathcal{N}(0, v_n^2)$ is absolutely continuous with respect to the Lebesgue measure on $\\mathbb{R}$ for any $n \\ge 1$ (since $\\sigma_n0 \\implies v_n^20$). The limit measure $\\mu = \\delta_0$ is a point mass, which is singular with respect to the Lebesgue measure.\nFor any two probability measures $\\nu_1$ and $\\nu_2$ that are mutually singular (i.e., there exists a set $A$ such that $\\nu_1(A)=1$ and $\\nu_2(A^c)=1$), their total variation distance is $\\|\\nu_1-\\nu_2\\|_{\\mathrm{TV}}=1$. In our case, let $A=\\{0\\}$. Then $\\mu_n(A^c) = \\mu_n(\\mathbb{R}\\setminus\\{0\\}) = 1$ since $\\mu_n$ has a density, and $\\mu(A) = \\delta_0(\\{0\\}) = 1$. Thus, $\\mu_n$ and $\\mu$ are mutually singular for every $n \\ge 1$.\nAlternatively, we compute the distance directly from the definition:\n$$\n\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} = \\sup_{A \\in \\mathcal{B}(\\mathbb{R})} |\\mu_n(A) - \\mu(A)|\n$$\nLet's choose the set $A = \\{0\\}$. We have $\\mu_n(\\{0\\}) = 0$ (as $\\mu_n$ is a continuous distribution) and $\\mu(\\{0\\}) = \\delta_0(\\{0\\}) = 1$.\nTherefore, $|\\mu_n(\\{0\\}) - \\mu(\\{0\\})| = |0-1| = 1$.\nSince the total variation distance is bounded by $1$, the supremum must be exactly $1$.\nSo, $\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} = 1$ for all $n \\ge 1$. This implies that the sequence does not converge in total variation, i.e., $\\|\\mu_n - \\mu\\|_{\\mathrm{TV}} \\not\\to 0$.\n\nNow we evaluate each option:\n\n**A. For $\\sigma_n=1/\\sqrt{n}$, one has $\\mu_n\\Rightarrow\\mu$, $\\{\\mu_n\\}_{n\\ge 1}$ is tight, and moreover $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}=1$ for all $n$, so $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.**\nThe choice $\\sigma_n = 1/\\sqrt{n}$ satisfies the condition $\\sigma_n > 0$ and $\\sigma_n \\downarrow 0$. Our general analysis above applies.\n- $\\mu_n\\Rightarrow\\mu$: This is correct.\n- $\\{\\mu_n\\}_{n\\ge 1}$ is tight: This is correct.\n- $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}=1$ for all $n$: This is correct.\n- so $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$: This is a correct conclusion.\nEvery part of this statement is correct.\n**Verdict: Correct.**\n\n**B. By Prokhorov's theorem, tightness of $\\{\\mu_n\\}_{n\\ge 1}$ together with uniqueness of the weak limit implies $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\to 0$.**\nProkhorov's theorem states that on a Polish space, tightness is equivalent to relative compactness in the topology of weak convergence. Tightness and uniqueness of the weak limit point together imply that the sequence converges weakly ($\\mu_n \\Rightarrow \\mu$). The statement incorrectly claims that this implies convergence in total variation. As demonstrated by this very problem, we have tightness and weak convergence, but not total variation convergence. Thus, the implication is false.\n**Verdict: Incorrect.**\n\n**C. By the portmanteau theorem, $\\mu_n\\Rightarrow\\mu$ implies $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded measurable function $f$, hence total variation convergence follows.**\nThis statement misrepresents the portmanteau theorem. Weak convergence, $\\mu_n\\Rightarrow\\mu$, is equivalent to $\\int f\\,\\mathrm{d}\\mu_n\\to\\int f\\,\\mathrm{d}\\mu$ for every bounded and *continuous* function $f$ (or bounded and uniformly continuous), not for every bounded *measurable* function $f$. Convergence for all bounded measurable functions is a characterization of convergence in total variation. The premise is false. A counterexample is the function $f = \\mathbf{1}_{\\{0\\}}$, which is bounded and measurable but not continuous. For this $f$, $\\int f\\,\\mathrm{d}\\mu_n = \\mu_n(\\{0\\}) = 0$ for all $n$, while $\\int f\\,\\mathrm{d}\\mu = \\mu(\\{0\\}) = 1$. The limit $0$ is not equal to $1$.\n**Verdict: Incorrect.**\n\n**D. The failure of total variation convergence in this example is explained by the fact that total variation distance tests all bounded measurable functions (equivalently, all measurable sets), including indicators of sets such as $\\{0\\}$ whose indicator function is discontinuous, whereas weak convergence only tests bounded continuous functions.**\nThis statement accurately diagnoses why weak convergence does not imply total variation convergence in this context. Total variation convergence, defined as $\\sup_A |\\mu_n(A) - \\mu(A)| \\to 0$, is a stronger mode of convergence than weak convergence. The class of test functions for weak convergence (bounded continuous functions) is smaller and more restrictive than the class for total variation convergence (bounded measurable functions, which includes discontinuous indicators of sets like $\\mathbf{1}_{\\{0\\}}$). The discontinuity of test functions like $\\mathbf{1}_{\\{0\\}}$ is exactly what allows total variation distance to \"detect\" the difference between the continuous measure $\\mu_n$ and the discrete measure $\\mu$, a difference that weak convergence \"ignores\". The explanation is sound.\n**Verdict: Correct.**\n\n**E. The sequence $\\{\\mu_n\\}_{n\\ge 1}$ cannot be tight because $\\|\\mu_n-\\mu\\|_{\\mathrm{TV}}\\not\\to 0$.**\nThis statement proposes a false implication. It claims that the lack of convergence in total variation implies a lack of tightness. Our analysis of the problem shows that the sequence $\\{\\mu_n\\}_{n\\ge 1}$ is indeed tight, while at the same time it does not converge in total variation. Therefore, this statement is a direct contradiction of what we have demonstrated. Tightness is related to the non-escape of mass to infinity and is a prerequisite for weak convergence, not for total variation convergence.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AD}$$", "id": "3005015"}, {"introduction": "Proving tightness for a sequence of laws on a path space like $D([0,T];\\mathbb{R}^d)$ can seem daunting, but powerful criteria exist to make the task manageable. This exercise [@problem_id:3005014] introduces Aldous’s criterion, a fundamental tool that translates the abstract notion of pre-compactness into two concrete, verifiable conditions. By engaging with its formulation, you will develop an intuition for how to control path behavior, particularly by using stopping times to prevent oscillations from concentrating at unpredictable random times.", "problem": "Consider a sequence $\\{X^n\\}_{n \\in \\mathbb{N}}$ of $\\mathbb{R}^d$-valued càdlàg (right-continuous with left limits) stochastic processes on $[0,T]$, each adapted to a filtration and regarded as random elements of the Skorokhod space $D([0,T];\\mathbb{R}^d)$ equipped with the Skorokhod $J_1$ topology. Recall the definition of tightness for probability measures on a metrizable space: a family $\\{\\mu_n\\}$ on a metric space $(S,d)$ is tight if for every $\\varepsilon0$ there exists a compact $K \\subset S$ such that $\\sup_{n} \\mu_n(K^c)  \\varepsilon$. By Prokhorov’s theorem, tightness implies relative compactness of the family of laws on $(S,d)$. A stopping time $\\tau$ with respect to a filtration $(\\mathcal{F}_t)_{t \\in [0,T]}$ is a random time with the property that $\\{\\tau \\le t\\} \\in \\mathcal{F}_t$ for all $t \\in [0,T]$. Let $\\mathcal{T}_T$ denote the set of all stopping times bounded by $T$.\n\nIn the context of establishing tightness of the laws of $\\{X^n\\}$ in $D([0,T];\\mathbb{R}^d)$, it is often necessary to control both the distributions of $X^n(t)$ at fixed times and the small-time oscillations of the paths in probability. Select all options that correctly state Aldous’s tightness criterion for processes in $D([0,T];\\mathbb{R}^d)$ using bounded stopping times and small increments, and select the option that best captures its intuition. There may be more than one correct option.\n\nA. For every $t \\in [0,T]$, the family $\\{X^n(t): n \\in \\mathbb{N}\\}$ is tight in $\\mathbb{R}^d$, and for every $\\eta0$,\n$$\n\\lim_{\\delta \\downarrow 0}\\ \\sup_{n \\in \\mathbb{N}}\\ \\sup_{\\tau \\in \\mathcal{T}_T}\\ \\sup_{0 \\le \\theta \\le \\delta}\\ \\mathbb{P}\\!\\left(\\left|X^n(\\tau+\\theta) - X^n(\\tau)\\right| \\ge \\eta\\right) = 0.\n$$\nUnder these conditions, the laws of $\\{X^n\\}$ are tight in $D([0,T];\\mathbb{R}^d)$.\n\nB. For every $\\eta0$,\n$$\n\\lim_{\\delta \\downarrow 0}\\ \\sup_{n \\in \\mathbb{N}}\\ \\sup_{s \\in [0,T-\\delta]}\\ \\sup_{0 \\le \\theta \\le \\delta}\\ \\mathbb{P}\\!\\left(\\left|X^n(s+\\theta) - X^n(s)\\right| \\ge \\eta\\right) = 0,\n$$\nwhich alone ensures tightness in $D([0,T];\\mathbb{R}^d)$; the use of stopping times is unnecessary.\n\nC. There exists a deterministic modulus $\\omega: [0,\\infty) \\to [0,\\infty)$ with $\\lim_{\\delta \\downarrow 0} \\omega(\\delta) = 0$ such that, almost surely and uniformly in $n \\in \\mathbb{N}$,\n$$\n\\sup_{|t-s| \\le \\delta} \\left|X^n(t) - X^n(s)\\right| \\le \\omega(\\delta)\\quad \\text{for all } \\delta0,\n$$\nand this is both necessary and sufficient for tightness in $D([0,T];\\mathbb{R}^d)$.\n\nD. Tightness of $\\{X^n\\}$ in $D([0,T];\\mathbb{R}^d)$ is equivalent to the existence of a constant $C0$ such that\n$$\n\\sup_{n \\in \\mathbb{N}} \\mathbb{E}\\!\\left[\\sup_{t \\in [0,T]} \\left|X^n(t)\\right|^2\\right] \\le C\n\\quad \\text{and} \\quad\n\\sup_{n \\in \\mathbb{N}} \\mathbb{E}\\!\\left([X^n]_T\\right) \\le C,\n$$\nwhere $[X^n]_T$ denotes the quadratic variation of $X^n$ up to time $T$.\n\nE. Intuition: The criterion prevents jump processes by enforcing continuity of sample paths, so tightness holds only for continuous processes.\n\nF. Intuition: The criterion demands that, for arbitrarily short time windows, the processes do not make large moves with non-negligible probability when sampled at arbitrary bounded stopping times, thereby ruling out concentration of large oscillations at random times. Together with tightness of the marginals at fixed times, this prevents escape of mass and yields tightness in $D([0,T];\\mathbb{R}^d)$ via Prokhorov’s theorem.\n\nSelect all correct options.", "solution": "The problem statement will first be validated for scientific soundness, self-consistency, and clarity.\n\n### Step 1: Extract Givens\n- A sequence of stochastic processes $\\{X^n\\}_{n \\in \\mathbb{N}}$ with values in $\\mathbb{R}^d$.\n- The processes are càdlàg (right-continuous with left limits) on the time interval $[0,T]$.\n- Each process $X^n$ is adapted to a filtration.\n- The processes are considered as random elements of the Skorokhod space $D([0,T];\\mathbb{R}^d)$ equipped with the Skorokhod $J_1$ topology.\n- Definition of tightness for a family of probability measures $\\{\\mu_n\\}$ on a metric space $(S,d)$: For every $\\varepsilon0$, there exists a compact set $K \\subset S$ such that $\\sup_{n} \\mu_n(K^c)  \\varepsilon$.\n- Statement of Prokhorov’s theorem: Tightness implies relative compactness of the family of laws on $(S,d)$.\n- Definition of a stopping time $\\tau$ with respect to a filtration $(\\mathcal{F}_t)_{t \\in [0,T]}$: A random time such that for all $t \\in [0,T]$, the event $\\{\\tau \\le t\\}$ is in the sigma-algebra $\\mathcal{F}_t$.\n- Notation $\\mathcal{T}_T$: The set of all stopping times bounded by $T$.\n- The central question is to identify the correct mathematical statement of Aldous's tightness criterion for processes in $D([0,T];\\mathbb{R}^d)$ and the best description of its intuition.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a formal question within the mathematical field of stochastic processes, specifically the theory of weak convergence of probability measures on function spaces.\n\n- **Scientifically Grounded:** All definitions and concepts presented (càdlàg processes, Skorokhod space, $J_1$ topology, tightness, Prokhorov's theorem, stopping times) are standard, rigorously defined, and central to modern probability theory. The problem is firmly based on established mathematical principles.\n- **Well-Posed:** The question asks for the identification of a specific, named theorem (Aldous's criterion) and its corresponding intuition. The theorem has a well-established formulation, making the question well-posed with a definite answer.\n- **Objective:** The problem is stated in precise, objective mathematical language, free of ambiguity, subjectivity, or opinion.\n- **Completeness and Consistency:** The problem is self-contained. It provides all necessary definitions and context for a specialist to understand and answer the question. There are no internal contradictions.\n- **Other criteria:** The problem is not trivial, not metaphorical, and is verifiable by consulting standard literature on the subject (e.g., Billingsley's \"Convergence of Probability Measures\" or Ethier and Kurtz's \"Markov Processes: Characterization and Convergence\").\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a clear and correct question about a fundamental theorem in the theory of stochastic processes. I will now proceed to analyze each option.\n\n### Solution Derivation and Option Analysis\n\nThe goal is to determine which of the options correctly state Aldous's criterion for tightness of the laws of the processes $\\{X^n\\}$ in the Skorokhod space $D([0,T];\\mathbb{R}^d)$ and which option best describes its intuition. A family of probability laws of processes $\\{X^n\\}$ on $D([0,T];\\mathbb{R}^d)$ is tight if and only if two conditions are met:\n1.  For any dense set of times in $[0,T]$ (and in particular for all $t \\in [0,T]$), the family of marginal distributions of $\\{X^n(t)\\}$ is tight in $\\mathbb{R}^d$.\n2.  A condition controlling the modulus of continuity of the paths in probability. Aldous's criterion provides such a condition using stopping times.\n\nLet us evaluate each option based on this understanding.\n\n**A. For every $t \\in [0,T]$, the family $\\{X^n(t): n \\in \\mathbb{N}\\}$ is tight in $\\mathbb{R}^d$, and for every $\\eta0$,**\n$$\n\\lim_{\\delta \\downarrow 0}\\ \\sup_{n \\in \\mathbb{N}}\\ \\sup_{\\tau \\in \\mathcal{T}_T}\\ \\sup_{0 \\le \\theta \\le \\delta}\\ \\mathbb{P}\\!\\left(\\left|X^n(\\tau+\\theta) - X^n(\\tau)\\right| \\ge \\eta\\right) = 0.\n$$\n**Under these conditions, the laws of $\\{X^n\\}$ are tight in $D([0,T];\\mathbb{R}^d)$.**\n\nThis option presents two conditions.\n1.  **Tightness of marginals:** \"For every $t \\in [0,T]$, the family $\\{X^n(t): n \\in \\mathbb{N}\\}$ is tight in $\\mathbb{R}^d$.\" This is the first standard condition for tightness on $D([0,T];\\mathbb{R}^d)$. It ensures that the process values do not \"escape to infinity\" at any fixed time.\n2.  **Oscillation control:** The limit condition involving stopping times is a standard and correct formulation of the second part of Aldous's criterion. It requires that, uniformly across the sequence of processes and for any bounded stopping time $\\tau$, the probability of a significant change in the process's value over a small subsequent time interval $[ \\tau, \\tau+\\theta ]$ vanishes as the interval length $\\delta$ shrinks to zero. This use of stopping times is crucial to prevent pathological oscillations that could concentrate at unpredictable times.\n\nThe conjunction of these two conditions is precisely the statement of Aldous's criterion for tightness in $D([0,T];\\mathbb{R}^d)$.\n**Verdict: Correct.**\n\n**B. For every $\\eta0$,**\n$$\n\\lim_{\\delta \\downarrow 0}\\ \\sup_{n \\in \\mathbb{N}}\\ \\sup_{s \\in [0,T-\\delta]}\\ \\sup_{0 \\le \\theta \\le \\delta}\\ \\mathbb{P}\\!\\left(\\left|X^n(s+\\theta) - X^n(s)\\right| \\ge \\eta\\right) = 0,\n$$\n**which alone ensures tightness in $D([0,T];\\mathbb{R}^d)$; the use of stopping times is unnecessary.**\n\nThis option makes two incorrect claims. First, it discards the condition on the tightness of marginals, which is essential. Without it, a sequence of processes like $X^n(t) = n$ would satisfy this oscillation condition (since $X^n(s+\\theta) - X^n(s) = 0$), but the laws of $\\{X^n\\}$ are clearly not tight. Second, it replaces the supremum over stopping times $\\sup_{\\tau \\in \\mathcal{T}_T}$ with a supremum over deterministic times $\\sup_{s \\in [0,T-\\delta]}$. This is a substantially weaker condition. It fails to control for oscillations or jumps that may concentrate at a sequence of \"moving\" random times, which is the exact problem Aldous's criterion is designed to solve. The claim that stopping times are unnecessary is a fundamental misunderstanding of the difficulties of proving tightness in $D([0,T];\\mathbb{R}^d)$ versus $C([0,T];\\mathbb{R}^d)$.\n**Verdict: Incorrect.**\n\n**C. There exists a deterministic modulus $\\omega: [0,\\infty) \\to [0,\\infty)$ with $\\lim_{\\delta \\downarrow 0} \\omega(\\delta) = 0$ such that, almost surely and uniformly in $n \\in \\mathbb{N}$,**\n$$\n\\sup_{|t-s| \\le \\delta} \\left|X^n(t) - X^n(s)\\right| \\le \\omega(\\delta)\\quad \\text{for all } \\delta0,\n$$\n**and this is both necessary and sufficient for tightness in $D([0,T];\\mathbb{R}^d)$.**\n\nThis condition is a statement about the sample paths themselves, holding almost surely and uniformly across the entire sequence $\\{X^n\\}$. This implies that for a fixed sample point $\\omega$ (outside a null set), the family of paths $\\{X^n(\\omega, \\cdot)\\}_{n \\in \\mathbb{N}}$ is equicontinuous. This forces all paths to be continuous. This is a sufficient condition for the relative compactness of the paths in the space of continuous functions $C([0,T];\\mathbb{R}^d)$, not the space of càdlàg functions $D([0,T];\\mathbb{R}^d)$. The space $D$ explicitly includes functions with jumps (discontinuities). Therefore, this condition is far too restrictive; it is not necessary for tightness in $D$. For example, the law of a single Poisson process is tight, but its sample paths have jumps and do not satisfy this condition. Furthermore, tightness is a condition on the laws of the processes (a condition \"in probability\"), not an almost sure pathwise property.\n**Verdict: Incorrect.**\n\n**D. Tightness of $\\{X^n\\}$ in $D([0,T];\\mathbb{R}^d)$ is equivalent to the existence of a constant $C0$ such that**\n$$\n\\sup_{n \\in \\mathbb{N}} \\mathbb{E}\\!\\left[\\sup_{t \\in [0,T]} \\left|X^n(t)\\right|^2\\right] \\le C\n\\quad \\text{and} \\quad\n\\sup_{n \\in \\mathbb{N}} \\mathbb{E}\\!\\left([X^n]_T\\right] \\le C,\n$$\n**where $[X^n]_T$ denotes the quadratic variation of $X^n$ up to time $T$.**\n\nThis statement is incorrect for several reasons. First, it claims equivalence (\"is equivalent to\"), which is a very strong claim. Second, the quadratic variation $[X^n]_T$ is typically defined for semimartingales. The problem states that $\\{X^n\\}$ are general càdlàg adapted processes, which are not necessarily semimartingales, so $[X^n]_T$ may not even be well-defined. Third, even if we restrict the context to semimartingales, these moment conditions represent a *sufficient* criterion for tightness, not a general necessary and sufficient one. They are related to criteria like Rebolledo's theorem for martingales, but they are not Aldous's criterion and are not applicable to all processes in $D([0,T];\\mathbb{R}^d)$. Aldous's criterion is more general.\n**Verdict: Incorrect.**\n\n**E. Intuition: The criterion prevents jump processes by enforcing continuity of sample paths, so tightness holds only for continuous processes.**\n\nThis statement fundamentally misinterprets the purpose of Aldous's criterion and the space $D([0,T];\\mathbb{R}^d)$. As explained for option C, the entire framework is designed to handle processes with jumps (discontinuities). The criterion does not enforce pathwise continuity; rather, it controls the \"bad behavior\" of jumps, such as their frequency and magnitude, in a way that allows the set of laws to be relatively compact. If all processes were continuous, one would use the simpler Arzelà-Ascoli theorem in the space $C([0,T];\\mathbb{R}^d)$.\n**Verdict: Incorrect.**\n\n**F. Intuition: The criterion demands that, for arbitrarily short time windows, the processes do not make large moves with non-negligible probability when sampled at arbitrary bounded stopping times, thereby ruling out concentration of large oscillations at random times. Together with tightness of the marginals at fixed times, this prevents escape of mass and yields tightness in $D([0,T];\\mathbb{R}^d)$ via Prokhorov’s theorem.**\n\nThis option provides an excellent and precise summary of the intuition behind Aldous's criterion.\n- \"...demands that...processes do not make large moves...when sampled at arbitrary bounded stopping times...\": This perfectly captures the meaning of the limit condition in option A.\n- \"...ruling out concentration of large oscillations at random times.\": This correctly identifies the key innovation and purpose of using stopping times instead of fixed times.\n- \"Together with tightness of the marginals at fixed times...\": This acknowledges the necessity of both parts of the criterion.\n- \"...prevents escape of mass and yields tightness...via Prokhorov’s theorem.\": This correctly situates the criterion within the broader theory of weak convergence. The \"escape of mass\" concept correctly alludes to both the process values growing too large (handled by marginal tightness) and the paths becoming too oscillatory to lie in a compact set (handled by the oscillation condition).\n**Verdict: Correct.**\n\n### Conclusion\nOption A provides a correct mathematical statement of Aldous's criterion. Option F provides an accurate and insightful explanation of the intuition behind it. The problem asks to select all correct options.", "answer": "$$\\boxed{AF}$$", "id": "3005014"}, {"introduction": "The first condition of Aldous’s criterion requires establishing the tightness of the processes’ marginal distributions, which means we must control the probability that paths wander too far from the origin. This practice [@problem_id:3005017] provides a hands-on demonstration of a core technique: using a uniform exponential moment bound to derive a uniform tail probability estimate. Mastering this application of Markov's inequality is a key step in applying tightness criteria to sequences of processes arising from stochastic differential equations.", "problem": "Consider a filtered probability space $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$ supporting a $d$-dimensional Brownian motion $\\{W_{t}\\}_{t\\in[0,T]}$. For each integer $n\\geq 1$, let $\\{X^{(n)}_{t}\\}_{t\\in[0,T]}$ be the unique strong solution to the Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}X^{(n)}_{t}=b_{n}(X^{(n)}_{t})\\,\\mathrm{d}t+\\sigma_{n}(X^{(n)}_{t})\\,\\mathrm{d}W_{t},\\quad X^{(n)}_{0}\\in\\mathbb{R}^{d},\n$$\nwhere the coefficients $b_{n}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ and $\\sigma_{n}:\\mathbb{R}^{d}\\to\\mathbb{R}^{d\\times d}$ are measurable and satisfy conditions ensuring existence and uniqueness of a strong solution. Denote by $\\mu_{n}$ the law of $X^{(n)}$ as a random element of the space $C([0,T];\\mathbb{R}^{d})$ equipped with the supremum norm $\\|x\\|_{\\infty}=\\sup_{t\\in[0,T]}\\|x(t)\\|$, where $\\|\\cdot\\|$ is the Euclidean norm on $\\mathbb{R}^{d}$. Assume there exist constants $\\alpha0$ and $K\\infty$ such that the family has uniformly bounded exponential moments of the supremum:\n$$\n\\sup_{n\\geq 1}\\,\\mathbb{E}\\Big[\\exp\\Big(\\alpha\\,\\sup_{t\\in[0,T]}\\|X^{(n)}_{t}\\|\\Big)\\Big]\\leq K.\n$$\nUsing only foundational definitions from probability theory and the topology of $C([0,T];\\mathbb{R}^{d})$, derive an explicit closed-form expression for a function $B(R)$, depending only on $R0$, $\\alpha$, and $K$, such that the following uniform tail control holds for all $R0$:\n$$\n\\sup_{n\\geq 1}\\,\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big)\\leq B(R).\n$$\nExpress your final answer as a single analytic expression $B(R)$ in terms of $R$, $\\alpha$, and $K$. Do not provide an inequality or any explanatory text in the final answer.", "solution": "The problem requires us to find a function $B(R)$ that provides a uniform upper bound for the tail probability of the supremum norm of a sequence of stochastic processes. The provided information is a uniform bound on the exponential moments of these supremum norms.\n\nLet the space of continuous functions from $[0,T]$ to $\\mathbb{R}^{d}$ be denoted by $C([0,T];\\mathbb{R}^{d})$. For each integer $n \\geq 1$, the law of the process $X^{(n)}$ is a probability measure $\\mu_{n}$ on this space. The quantity we wish to bound is\n$$\n\\sup_{n\\geq 1}\\,\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big)\n$$\nwhere $\\|x\\|_{\\infty} = \\sup_{t\\in[0,T]}\\|x(t)\\|$ and $R  0$ is a given positive real number.\n\nBy the definition of the law $\\mu_{n}$ of the random element $X^{(n)}$, the probability of the set $\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}$ under $\\mu_{n}$ is equal to the probability of the event that the sample path of $X^{(n)}$ has a supremum norm greater than or equal to $R$. That is,\n$$\n\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big) = \\mathbb{P}\\Big(\\sup_{t\\in[0,T]}\\|X^{(n)}_{t}\\| \\geq R\\Big).\n$$\nFor notational simplicity, let $Z_{n} = \\sup_{t\\in[0,T]}\\|X^{(n)}_{t}\\|$. $Z_n$ is a non-negative random variable for each $n$. The problem is to find an upper bound for $\\sup_{n \\geq 1} \\mathbb{P}(Z_{n} \\geq R)$.\n\nThe given condition is that there exist constants $\\alpha0$ and $K\\infty$ such that\n$$\n\\sup_{n\\geq 1}\\,\\mathbb{E}\\Big[\\exp\\big(\\alpha Z_{n}\\big)\\Big]\\leq K.\n$$\nThis implies that for any individual $n \\geq 1$, we have $\\mathbb{E}\\big[\\exp(\\alpha Z_{n})\\big]\\leq K$.\n\nWe can use Markov's inequality, a foundational result in probability theory. For a non-negative random variable $Y$ and any constant $a0$, Markov's inequality states that\n$$\n\\mathbb{P}(Y \\geq a) \\leq \\frac{\\mathbb{E}[Y]}{a}.\n$$\nWe apply this inequality not to $Z_n$ directly, but to the transformed random variable $Y_n = \\exp(\\alpha Z_n)$. Since $\\alpha  0$ and $Z_n \\geq 0$, $Y_n$ is a non-negative random variable.\n\nThe event $\\{Z_n \\geq R\\}$ can be related to an event involving $Y_n$. Since $\\alpha  0$, the inequality $Z_n \\geq R$ is equivalent to $\\alpha Z_n \\geq \\alpha R$. Furthermore, because the exponential function $u \\mapsto \\exp(u)$ is strictly increasing on $\\mathbb{R}$, this is equivalent to $\\exp(\\alpha Z_n) \\geq \\exp(\\alpha R)$.\nThus, we have the equality of events:\n$$\n\\{Z_n \\geq R\\} = \\{\\exp(\\alpha Z_n) \\geq \\exp(\\alpha R)\\} = \\{Y_n \\geq \\exp(\\alpha R)\\}.\n$$\nThis implies that their probabilities are equal:\n$$\n\\mathbb{P}(Z_n \\geq R) = \\mathbb{P}(Y_n \\geq \\exp(\\alpha R)).\n$$\nNow, we apply Markov's inequality to the random variable $Y_n$ with the constant $a = \\exp(\\alpha R)$. Since $R0$ and $\\alpha0$, we have $a  0$, so the application is valid.\n$$\n\\mathbb{P}(Y_n \\geq \\exp(\\alpha R)) \\leq \\frac{\\mathbb{E}[Y_n]}{\\exp(\\alpha R)}.\n$$\nSubstituting $Y_n = \\exp(\\alpha Z_n)$, we get:\n$$\n\\mathbb{P}(Z_n \\geq R) \\leq \\frac{\\mathbb{E}[\\exp(\\alpha Z_n)]}{\\exp(\\alpha R)}.\n$$\nWe are given that $\\mathbb{E}[\\exp(\\alpha Z_n)] \\leq K$ for all $n \\geq 1$. Therefore, for any $n \\geq 1$,\n$$\n\\mathbb{P}(Z_n \\geq R) \\leq \\frac{K}{\\exp(\\alpha R)}.\n$$\nThis upper bound, $\\frac{K}{\\exp(\\alpha R)}$, is independent of $n$. Since the inequality holds for every $n \\geq 1$, it must also hold for the supremum over all $n \\geq 1$.\n$$\n\\sup_{n \\geq 1} \\mathbb{P}(Z_n \\geq R) \\leq \\frac{K}{\\exp(\\alpha R)}.\n$$\nSubstituting back the definition of $Z_n$ and the probabilistic interpretation of $\\mu_n$, we obtain the desired uniform tail control:\n$$\n\\sup_{n\\geq 1}\\,\\mu_{n}\\big(\\{x\\in C([0,T];\\mathbb{R}^{d}):\\|x\\|_{\\infty}\\geq R\\}\\big) \\leq K\\exp(-\\alpha R).\n$$\nThe problem asks for an explicit closed-form expression for a function $B(R)$ that serves as this upper bound. Based on our derivation, we can define $B(R)$ as:\n$$\nB(R) = K\\exp(-\\alpha R).\n$$\nThis function depends only on $R$, $\\alpha$, and $K$, as required.", "answer": "$$\\boxed{K\\exp(-\\alpha R)}$$", "id": "3005017"}]}