{"hands_on_practices": [{"introduction": "To begin, we explore conditional expectation in a simple yet illustrative geometric setting. By considering a point chosen uniformly from a unit disk, this exercise demonstrates how conditioning on a piece of information—in this case, the radial distance $R$ from the center—simplifies the problem. This practice helps build a foundational intuition for conditional expectation as an averaging process over the remaining sources of randomness. [@problem_id:467048]", "problem": "Let $(X,Y)$ be a random vector with a uniform probability distribution over the unit disk $D = \\{(x,y) \\in \\mathbb{R}^2 : x^2 + y^2 \\le 1\\}$. The probability measure $P$ on the Borel subsets of $D$ is given by $P(A) = \\frac{\\text{Area}(A)}{\\pi}$. Define a new random variable $R$ as the radial coordinate, $R = \\sqrt{X^2+Y^2}$.\n\nDetermine the conditional expectation of the random variable $X^2$ given the random variable $R$, denoted as $E[X^2 | R]$. The final result should be an expression in terms of $R$.", "solution": "We use polar coordinates: $X = R\\cos\\theta$, $Y = R\\sin\\theta$, and conditional on $R=r$, the angle $\\theta$ is uniform on $[0,2\\pi]$.\n\nHence\n$$\nE[X^2 \\mid R=r]\n= E[r^2\\cos^2\\theta]\n= r^2\\,E[\\cos^2\\theta].\n$$\n\nWe compute\n$$\nE[\\cos^2\\theta]\n=\\frac{1}{2\\pi}\\int_0^{2\\pi}\\cos^2\\theta\\,d\\theta\n=\\frac{1}{2\\pi}\\cdot\\pi\n=\\tfrac12.\n$$\n\nTherefore\n$$\nE[X^2 \\mid R=r]\n=\\frac{r^2}{2}.\n$$", "answer": "$$\\boxed{\\frac{R^2}{2}}$$", "id": "467048"}, {"introduction": "We now turn to the cornerstone case of jointly Gaussian random variables, which is fundamental in stochastic processes and filtering theory. This problem guides you to derive the celebrated result that the conditional expectation $\\mathbb{E}[X|\\sigma(Y)]$ is a linear function of $Y$. By approaching this from the perspective of an $L^2$-orthogonal projection, you will reinforce your understanding of the deep geometric structure underlying conditional expectation and its connection to linear estimation. [@problem_id:2971565]", "problem": "Let $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$ be a filtered probability space supporting a standard one-dimensional Brownian motion $W=(W_{t})_{t\\in[0,T]}$. Fix $m,n\\in\\mathbb{N}$, and let $a:[0,T]\\to\\mathbb{R}^{m}$ and $B:[0,T]\\to\\mathbb{R}^{n}$ be deterministic functions with $a\\in L^{2}([0,T];\\mathbb{R}^{m})$ and $B\\in L^{2}([0,T];\\mathbb{R}^{n})$. Define the random vectors\n$$\nX \\coloneqq \\int_{0}^{T} a(t)\\,\\mathrm{d}W_{t}\\in\\mathbb{R}^{m},\\qquad\nY \\coloneqq \\int_{0}^{T} B(t)\\,\\mathrm{d}W_{t}\\in\\mathbb{R}^{n}.\n$$\nThese are well-defined Itô integrals, and hence $(X,Y)$ is jointly Gaussian with means zero. Let the block covariance matrix $\\Sigma$ of $(X,Y)$ be\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n\\Sigma_{XX}  \\Sigma_{XY} \\\\\n\\Sigma_{YX}  \\Sigma_{YY}\n\\end{pmatrix},\n$$\nwhere $\\Sigma_{XX}\\in\\mathbb{R}^{m\\times m}$, $\\Sigma_{XY}\\in\\mathbb{R}^{m\\times n}$, $\\Sigma_{YX}=\\Sigma_{XY}^{\\top}$, and $\\Sigma_{YY}\\in\\mathbb{R}^{n\\times n}$, with entries given by\n$$\n\\Sigma_{XX}=\\int_{0}^{T} a(t)a(t)^{\\top}\\,\\mathrm{d}t,\\qquad\n\\Sigma_{XY}=\\int_{0}^{T} a(t)B(t)^{\\top}\\,\\mathrm{d}t,\\qquad\n\\Sigma_{YY}=\\int_{0}^{T} B(t)B(t)^{\\top}\\,\\mathrm{d}t.\n$$\nAssume $\\Sigma_{YY}$ is invertible.\n\nUsing only foundational principles appropriate for stochastic differential equations (SDE), namely: the definition of conditional expectation as the $L^{2}$-orthogonal projection onto $\\sigma(Y)$, the fact that linear transformations of Gaussian vectors are Gaussian, and the fact that for jointly Gaussian vectors uncorrelatedness implies independence, derive from first principles that $\\mathbb{E}[X\\mid\\sigma(Y)]$ is a linear function of $Y$ and compute the coefficient matrix explicitly in terms of $\\Sigma$. Your final answer must be a single closed-form analytic expression for $\\mathbb{E}[X\\mid\\sigma(Y)]$ written in terms of the blocks of $\\Sigma$ and $Y$.", "solution": "The problem requires the derivation of the conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ for two jointly Gaussian random vectors $X$ and $Y$ derived from Itô integrals. The derivation must be based on first principles as specified: the interpretation of conditional expectation as an $L^{2}$-orthogonal projection, and properties of Gaussian vectors.\n\nLet the random vector $X$ be an element of the Hilbert space $L^{2}(\\Omega, \\mathcal{F}, \\mathbb{P}; \\mathbb{R}^{m})$ of square-integrable $\\mathbb{R}^{m}$-valued random variables. The conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ is the orthogonal projection of $X$ onto the closed subspace $L^{2}(\\Omega, \\sigma(Y), \\mathbb{P}; \\mathbb{R}^{m})$ of $\\sigma(Y)$-measurable random vectors.\n\nWe hypothesize that this projection is a linear function of $Y$, i.e., there exists a deterministic matrix $C \\in \\mathbb{R}^{m \\times n}$ such that $\\mathbb{E}[X\\mid\\sigma(Y)] = CY$. Let us denote $\\hat{X} \\coloneqq CY$. According to the projection theorem, if $\\hat{X}$ is indeed the orthogonal projection, then the error vector $X - \\hat{X}$ must be orthogonal to the subspace $L^{2}(\\Omega, \\sigma(Y), \\mathbb{P}; \\mathbb{R}^{m})$.\n\nFor zero-mean random vectors, orthogonality is equivalent to uncorrelatedness. Thus, the condition is that $X - CY$ must be uncorrelated with every $\\sigma(Y)$-measurable random vector. Since the linear combinations of the components of $Y$ are dense in the Gaussian space generated by $Y$, it is sufficient to require that $X - CY$ is uncorrelated with $Y$ itself. This is expressed through the covariance matrix:\n$$\n\\mathbb{E}\\left[ (X - CY) Y^{\\top} \\right] = 0_{m \\times n}\n$$\nwhere $0_{m \\times n}$ is the $m \\times n$ zero matrix.\n\nUsing the linearity of expectation, we can expand this expression:\n$$\n\\mathbb{E}\\left[ XY^{\\top} \\right] - \\mathbb{E}\\left[ (CY) Y^{\\top} \\right] = 0_{m \\times n}\n$$\nSince $C$ is a deterministic matrix, it can be factored out of the expectation:\n$$\n\\mathbb{E}\\left[ XY^{\\top} \\right] - C \\mathbb{E}\\left[ YY^{\\top} \\right] = 0_{m \\times n}\n$$\nBy definition, the covariance matrices are $\\Sigma_{XY} = \\mathbb{E}[XY^{\\top}]$ and $\\Sigma_{YY} = \\mathbb{E}[YY^{\\top}]$. The equation becomes:\n$$\n\\Sigma_{XY} - C \\Sigma_{YY} = 0_{m \\times n}\n$$\nThe problem states that the matrix $\\Sigma_{YY}$ is invertible, which allows us to solve for the unique matrix $C$:\n$$\nC = \\Sigma_{XY} \\Sigma_{YY}^{-1}\n$$\nThe dimensions are consistent: $\\Sigma_{XY}$ is $m \\times n$ and $\\Sigma_{YY}^{-1}$ is $n \\times n$, so $C$ is an $m \\times n$ matrix.\n\nSo far, we have shown that if the conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ is a linear function of $Y$, it must be $\\Sigma_{XY} \\Sigma_{YY}^{-1} Y$. We must now rigorously prove that this is indeed the correct expression, using the specified principles.\n\nLet us define a new random vector $U \\in \\mathbb{R}^{m}$:\n$$\nU \\coloneqq X - \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\n$$\nThe random vectors $X$ and $Y$ are jointly Gaussian. The vector $U$ is a linear transformation of the vector $(X,Y)$, so the augmented vector $(U,Y)$ is also jointly Gaussian.\n\nLet us compute the covariance of $U$ and $Y$:\n$$\n\\mathbb{E}[UY^{\\top}] = \\mathbb{E}\\left[ \\left(X - \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\\right) Y^{\\top} \\right]\n$$\n$$\n\\mathbb{E}[UY^{\\top}] = \\mathbb{E}[XY^{\\top}] - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\mathbb{E}[YY^{\\top}]\n$$\n$$\n\\mathbb{E}[UY^{\\top}] = \\Sigma_{XY} - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YY} = \\Sigma_{XY} - \\Sigma_{XY} = 0_{m \\times n}\n$$\nThis shows that $U$ and $Y$ are uncorrelated. A fundamental property of jointly Gaussian random vectors is that uncorrelatedness implies independence. Therefore, $U$ and $Y$ are independent random vectors. Independence of $U$ from $Y$ implies independence of $U$ from the sigma-algebra generated by $Y$, $\\sigma(Y)$.\n\nNow we can compute the conditional expectation of $X$ given $\\sigma(Y)$:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = \\mathbb{E}[U + \\Sigma_{XY} \\Sigma_{YY}^{-1} Y \\mid \\sigma(Y)]\n$$\nBy the linearity of conditional expectation:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = \\mathbb{E}[U\\mid\\sigma(Y)] + \\mathbb{E}[\\Sigma_{XY} \\Sigma_{YY}^{-1} Y \\mid \\sigma(Y)]\n$$\nLet's analyze each term:\n1.  For the first term, since $U$ is independent of $\\sigma(Y)$, its conditional expectation given $\\sigma(Y)$ is simply its unconditional expectation: $\\mathbb{E}[U\\mid\\sigma(Y)] = \\mathbb{E}[U]$.\n2.  For the second term, the vector $\\Sigma_{XY} \\Sigma_{YY}^{-1} Y$ is a function of $Y$ and is therefore $\\sigma(Y)$-measurable. For any $\\sigma(Y)$-measurable random vector $Z$, it holds that $\\mathbb{E}[Z\\mid\\sigma(Y)] = Z$. Thus, $\\mathbb{E}[\\Sigma_{XY} \\Sigma_{YY}^{-1} Y \\mid \\sigma(Y)] = \\Sigma_{XY} \\Sigma_{YY}^{-1} Y$.\n\nCombining these results, we get:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = \\mathbb{E}[U] + \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\n$$\nWe now compute the unconditional expectation of $U$:\n$$\n\\mathbb{E}[U] = \\mathbb{E}[X - \\Sigma_{XY} \\Sigma_{YY}^{-1} Y] = \\mathbb{E}[X] - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\mathbb{E}[Y]\n$$\nThe problem states that $X$ and $Y$ are zero-mean Itô integrals, so $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Y] = 0$. Therefore, $\\mathbb{E}[U] = 0$.\n\nSubstituting this back, we obtain the final expression for the conditional expectation:\n$$\n\\mathbb{E}[X\\mid\\sigma(Y)] = 0 + \\Sigma_{XY} \\Sigma_{YY}^{-1} Y = \\Sigma_{XY} \\Sigma_{YY}^{-1} Y\n$$\nThis derivation confirms that, for jointly Gaussian vectors, the conditional expectation $\\mathbb{E}[X\\mid\\sigma(Y)]$ is a linear function of $Y$, and it provides the explicit form of the coefficient matrix in terms of the block covariance matrices. The derivation relied only on the specified first principles.", "answer": "$$\n\\boxed{\\Sigma_{XY} \\Sigma_{YY}^{-1} Y}\n$$", "id": "2971565"}, {"introduction": "Beyond the Gaussian world, many real-world systems exhibit complex, non-linear dependencies. This advanced practice introduces copula theory as a powerful tool for modeling such relationships and computing conditional expectations. By deriving a general formula and then applying it to the Farlie–Gumbel–Morgenstern copula, you will learn how to separate marginal behavior from the dependence structure, a technique essential for sophisticated modeling in fields like quantitative finance and risk management. [@problem_id:2971556]", "problem": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and let $(X,Y)$ be a pair of real-valued random variables with continuous marginal distribution functions $F_{X}$ and $F_{Y}$. Assume that $(X,Y)$ admits a copula $C$ in the sense of Sklar’s theorem and that $C$ is twice continuously differentiable on $(0,1)^{2}$. Denote by $c(u,v)$ the copula density, i.e., $c(u,v)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)$. Let $\\sigma(Y)$ be the $\\sigma$-algebra generated by $Y$.\n\n1) Starting from the definition of conditional expectation with respect to $\\sigma(Y)$, and using only the fundamental facts that (i) Sklar’s theorem represents the joint distribution via the copula $C$ and the marginals $F_{X}$ and $F_{Y}$, and (ii) the probability integral transform maps $X$ to $U=F_{X}(X)$ and $Y$ to $V=F_{Y}(Y)$ which are uniform on $(0,1)$, derive an explicit integral representation for $E[X\\mid\\sigma(Y)]$ in terms of $C$, $F_{X}$, $F_{Y}$, and $c$. Your representation must be given as a function of $Y$, without assuming any specific form of $F_{X}$, $F_{Y}$, or $C$ beyond the differentiability stated above.\n\n2) Specialize to the case where $X$ and $Y$ have uniform marginals on $[0,1]$ and the copula is the Farlie–Gumbel–Morgenstern copula with parameter $\\theta\\in[-1,1]$, given by $C(u,v)=uv\\bigl(1+\\theta(1-u)(1-v)\\bigr)$. Compute a closed-form expression for $E[X\\mid\\sigma(Y)]$ as a function of $Y$ and $\\theta$, and determine whether it is nondecreasing in $Y$ and for which values of $\\theta$ it is strictly increasing, strictly decreasing, or constant. Provide your final answer as a single simplified analytic expression. No rounding is required and no units are involved.", "solution": "We begin from core definitions and well-tested facts. By the definition of conditional expectation with respect to the sub-$\\sigma$-algebra $\\sigma(Y)$, there exists a measurable function $g$ such that $E[X\\mid\\sigma(Y)]=g(Y)$ almost surely, and $g(y)=E[X\\mid Y=y]$ wherever the regular conditional distribution exists. Since $F_{X}$ and $F_{Y}$ are continuous and the copula $C$ is twice differentiable, a regular conditional distribution exists and can be expressed via $C$.\n\nBy Sklar’s theorem, the joint cumulative distribution function satisfies\n$$\nF_{X,Y}(x,y)=C\\bigl(F_{X}(x),F_{Y}(y)\\bigr).\n$$\nDefine the probability integral transforms $U=F_{X}(X)$ and $V=F_{Y}(Y)$. Then $U$ and $V$ are each uniform on $(0,1)$ and have joint distribution with cumulative distribution function $C(u,v)$ and density $c(u,v)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)$. The marginal density of $V$ is\n$$\nf_{V}(v)=\\int_{0}^{1}c(u,v)\\,du.\n$$\nFor a copula, $f_{V}(v)=1$ for all $v\\in(0,1)$ because $V$ is uniform. Consequently, differentiating $C(u,v)$ with respect to $v$ yields the conditional distribution function of $U$ given $V=v$:\n$$\nF_{U\\mid V=v}(u)=\\frac{\\partial}{\\partial v}C(u,v).\n$$\nDifferentiating $F_{U\\mid V=v}(u)$ with respect to $u$ gives the conditional density of $U$ given $V=v$:\n$$\nf_{U\\mid V=v}(u)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)=c(u,v),\\quad u\\in(0,1).\n$$\n\nWe now relate $E[X\\mid Y=y]$ to the conditional distribution of $U$ given $V=v$ by the law of the unconscious statistician. Since $X=F_{X}^{-1}(U)$ almost surely, we have\n$$\nE[X\\mid Y=y]=E\\bigl[F_{X}^{-1}(U)\\mid V=F_{Y}(y)\\bigr]\n=\\int_{0}^{1}F_{X}^{-1}(u)\\,f_{U\\mid V=F_{Y}(y)}(u)\\,du\n=\\int_{0}^{1}F_{X}^{-1}(u)\\,c\\bigl(u,F_{Y}(y)\\bigr)\\,du.\n$$\nTherefore,\n$$\nE[X\\mid\\sigma(Y)]=g(Y),\\quad\\text{where}\\quad g(y)=\\int_{0}^{1}F_{X}^{-1}(u)\\,c\\bigl(u,F_{Y}(y)\\bigr)\\,du.\n$$\nThis completes the general expression.\n\nWe now specialize to the Farlie–Gumbel–Morgenstern copula with parameter $\\theta\\in[-1,1]$ and uniform marginals on $[0,1]$. In this case, $F_{X}(x)=x$ and $F_{Y}(y)=y$ for $x,y\\in[0,1]$, and thus $F_{X}^{-1}(u)=u$ and $V=Y$. The copula is\n$$\nC(u,v)=uv\\bigl(1+\\theta(1-u)(1-v)\\bigr),\\quad (u,v)\\in[0,1]^{2}.\n$$\nThe copula density is\n$$\nc(u,v)=\\frac{\\partial^{2}}{\\partial u\\,\\partial v}C(u,v)=1+\\theta(1-2u)(1-2v).\n$$\nConsequently,\n$$\nE[X\\mid Y=y]=\\int_{0}^{1}u\\left[1+\\theta(1-2u)(1-2y)\\right]\\,du\n=\\int_{0}^{1}u\\,du+\\theta(1-2y)\\int_{0}^{1}u(1-2u)\\,du.\n$$\nWe compute the elementary integrals:\n$$\n\\int_{0}^{1}u\\,du=\\frac{1}{2},\\qquad\n\\int_{0}^{1}u(1-2u)\\,du=\\int_{0}^{1}(u-2u^{2})\\,du=\\left[\\frac{1}{2}-\\frac{2}{3}\\right]=-\\frac{1}{6}.\n$$\nTherefore,\n$$\nE[X\\mid Y=y]=\\frac{1}{2}+\\theta(1-2y)\\left(-\\frac{1}{6}\\right)\n=\\frac{1}{2}-\\frac{\\theta}{6}+\\frac{\\theta}{3}\\,y.\n$$\nSince $E[X\\mid\\sigma(Y)]=E[X\\mid Y]$ almost surely when $Y$ is the conditioning variable, we obtain\n$$\nE[X\\mid\\sigma(Y)]=\\frac{1}{2}-\\frac{\\theta}{6}+\\frac{\\theta}{3}\\,Y.\n$$\nThis function is affine in $Y$ with slope $\\frac{\\theta}{3}$. Hence it is nondecreasing in $Y$ if and only if $\\theta\\ge 0$, strictly increasing if $\\theta0$, strictly decreasing if $\\theta0$, and constant if $\\theta=0$ (the independence case, yielding $E[X\\mid\\sigma(Y)]=\\frac{1}{2}$).\n\nThus the required closed-form expression is the affine function of $Y$ and $\\theta$ given above.", "answer": "$$\\boxed{\\frac{\\theta}{3}\\,Y+\\frac{1}{2}-\\frac{\\theta}{6}}$$", "id": "2971556"}]}