{"hands_on_practices": [{"introduction": "A beautiful illustration of the power of the Borel-Cantelli lemmas lies in characterizing the limiting behavior of Brownian motion. This practice invites you to derive a critical threshold related to the celebrated Law of the Iterated Logarithm. By analyzing a sequence of independent events, you will determine the precise condition that separates whether path maxima exceed a growing boundary infinitely often or only finitely often, providing a hands-on feel for the sharp dichotomy offered by the first and second Borel-Cantelli lemmas [@problem_id:2991415].", "problem": "Let $\\{W_{t}\\}_{t \\geq 0}$ be a standard one-dimensional Brownian motion with $W_{0}=0$. For each integer $n \\geq 3$ and constant $c0$, define the event\n$$\nA_{n}=\\left\\{\\sup_{t \\in [n,n+1]} \\left|W_{t}-W_{n}\\right|\\sqrt{2c\\,\\ln n}\\right\\}.\n$$\nUsing only foundational properties of Brownian motion (independent and stationary increments, the reflection principle for one-sided maxima over a unit interval, and standard asymptotics of the standard normal tail), determine the exact value of the critical constant $c^{\\ast}$ such that the following dichotomy holds:\n- If $cc^{\\ast}$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_{n})=\\infty$, and by the second Borel–Cantelli lemma the events $A_{n}$ occur infinitely often almost surely (a.s.).\n- If $cc^{\\ast}$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_{n})\\infty$, and hence $A_{n}$ occurs only finitely often a.s.\n\nYour final answer must be the value of $c^{\\ast}$ as a single real number. No rounding is required.", "solution": "The problem requires finding a critical constant $c^{\\ast}$ that determines the convergence or divergence of the series $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_n)$, where $A_{n}=\\left\\{\\sup_{t \\in [n,n+1]} \\left|W_{t}-W_{n}\\right|\\sqrt{2c\\,\\ln n}\\right\\}$ and $\\{W_t\\}_{t \\ge 0}$ is a standard one-dimensional Brownian motion.\n\nFirst, we analyze the probability $\\mathbb{P}(A_n)$. Let $a_n = \\sqrt{2c \\ln n}$. The event $A_n$ concerns the behavior of the process $W_t - W_n$ over the time interval $t \\in [n, n+1]$. We define a new stochastic process $\\tilde{W}_s = W_{n+s} - W_n$ for $s \\in [0, 1]$. Due to the stationary and independent increments property of Brownian motion, $\\{\\tilde{W}_s\\}_{s \\in [0,1]}$ is itself a standard Brownian motion starting at $\\tilde{W}_0 = 0$. The distribution of this process is identical to that of $\\{W_s\\}_{s \\in [0,1]}$.\n\nThe probability $\\mathbb{P}(A_n)$ can thus be rewritten as:\n$$\n\\mathbb{P}(A_n) = \\mathbb{P}\\left(\\sup_{s \\in [0,1]} |\\tilde{W}_s| > a_n\\right) = \\mathbb{P}\\left(\\sup_{s \\in [0,1]} |W_s| > a_n\\right)\n$$\nThis shows that $\\mathbb{P}(A_n)$ depends on $n$ only through the threshold $a_n$. Furthermore, the events $\\{A_n\\}_{n \\ge 3}$ are independent because they are defined on disjoint time intervals $[n, n+1]$, and Brownian motion has independent increments.\n\nLet $M_1 = \\sup_{s \\in [0,1]} W_s$ and $m_1 = \\inf_{s \\in [0,1]} W_s$. The event of interest is $\\left\\{\\sup_{s \\in [0,1]} |W_s| > a_n\\right\\}$, which is equivalent to the event $\\{M_1 > a_n \\text{ or } m_1  -a_n\\}$. We can bound its probability using the union of events property:\n$$\n\\mathbb{P}(M_1 > a_n) \\le \\mathbb{P}(A_n) \\le \\mathbb{P}(M_1 > a_n) + \\mathbb{P}(m_1  -a_n)\n$$\nThe problem states we must use the reflection principle for one-sided maxima. For any $a > 0$, the reflection principle gives the probability of the maximum of a standard Brownian motion over $[0,1]$:\n$$\n\\mathbb{P}(M_1 > a) = 2 \\mathbb{P}(W_1 > a)\n$$\nBy the symmetry of Brownian motion, the process $\\{-W_s\\}_{s \\ge 0}$ is also a standard Brownian motion. Therefore, the distribution of the infimum is related to the distribution of the supremum:\n$$\n\\mathbb{P}(m_1  -a) = \\mathbb{P}\\left(\\inf_{s \\in [0,1]} W_s  -a\\right) = \\mathbb{P}\\left(\\sup_{s \\in [0,1]} (-W_s) > a\\right) = \\mathbb{P}(M_1 > a)\n$$\nSubstituting this into our expression for $\\mathbb{P}(m_1  -a_n)$ yields:\n$$\n\\mathbb{P}(m_1  -a_n) = \\mathbb{P}(M_1 > a_n) = 2 \\mathbb{P}(W_1 > a_n)\n$$\nUsing these results, we can establish bounds for $\\mathbb{P}(A_n)$:\n$$\n2 \\mathbb{P}(W_1 > a_n) \\le \\mathbb{P}(A_n) \\le 4 \\mathbb{P}(W_1 > a_n)\n$$\nThe lower bound comes from $\\mathbb{P}(A_n) \\ge \\mathbb{P}(M_1 > a_n) = 2 \\mathbb{P}(W_1 > a_n)$ (and also $\\mathbb{P}(A_n) \\ge \\mathbb{P}(m_1  -a_n)$). The upper bound is the union bound.\n\nThe convergence of the series $\\sum_{n=3}^\\infty \\mathbb{P}(A_n)$ is therefore determined by the convergence of $\\sum_{n=3}^\\infty \\mathbb{P}(W_1 > a_n)$, according to the limit comparison test for series with positive terms.\n\nThe random variable $W_1$ is distributed as a standard normal variable, $W_1 \\sim \\mathcal{N}(0,1)$. The problem directs us to use the standard asymptotic formula for the tail of a standard normal distribution. For a random variable $Z \\sim \\mathcal{N}(0,1)$, as $x \\to \\infty$:\n$$\n\\mathbb{P}(Z > x) = \\int_x^\\infty \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz \\sim \\frac{1}{\\sqrt{2\\pi}x} \\exp\\left(-\\frac{x^2}{2}\\right)\n$$\nWe apply this formula with $x = a_n = \\sqrt{2c \\ln n}$. Since $c > 0$, $a_n \\to \\infty$ as $n \\to \\infty$.\n$$\n\\mathbb{P}(W_1 > a_n) \\sim \\frac{1}{\\sqrt{2\\pi} \\sqrt{2c \\ln n}} \\exp\\left(-\\frac{(\\sqrt{2c \\ln n})^2}{2}\\right)\n$$\n$$\n\\mathbb{P(W_1 > a_n)} \\sim \\frac{1}{\\sqrt{4\\pi c \\ln n}} \\exp(-c \\ln n) = \\frac{1}{\\sqrt{4\\pi c}} \\frac{1}{n^c (\\ln n)^{1/2}}\n$$\nLet $p_n = \\mathbb{P}(W_1 > a_n)$. The series $\\sum p_n$ has the same convergence behavior as the series $\\sum b_n$ where $b_n = \\frac{1}{n^c (\\ln n)^{1/2}}$. We now analyze the convergence of $\\sum b_n$ using the integral test.\n\nCase 1: $c > 1$.\nThe series $\\sum_{n=3}^\\infty \\frac{1}{n^c (\\ln n)^{1/2}}$ can be compared with the p-series $\\sum_{n=3}^\\infty \\frac{1}{n^p}$ for any $p$ such that $1  p  c$. Since $\\lim_{n\\to\\infty} \\frac{n^{-c}(\\ln n)^{-1/2}}{n^{-p}} = \\lim_{n\\to\\infty} n^{p-c}(\\ln n)^{-1/2} = 0$ and $\\sum n^{-p}$ converges, the series $\\sum b_n$ converges. Consequently, $\\sum p_n$ converges, and by our earlier bounds, $\\sum \\mathbb{P}(A_n)$ converges.\n\nCase 2: $c  1$.\nThe series $\\sum_{n=3}^\\infty \\frac{1}{n^c (\\ln n)^{1/2}}$ can be compared with the divergent p-series $\\sum_{n=3}^\\infty \\frac{1}{n^c}$. For $n \\ge 3$, $(\\ln n)^{1/2} > 1$, so $\\frac{1}{n^c (\\ln n)^{1/2}}  \\frac{1}{n^c}$; this comparison is not useful for divergence. Instead, we compare to $\\sum n^{-p}$ where $c  p  1$. $\\lim_{n \\to\\infty} \\frac{n^{-c}(\\ln n)^{-1/2}}{n^{-p}} = \\lim_{n \\to\\infty} n^{p-c}(\\ln n)^{-1/2} = \\infty$. Since $\\sum n^{-p}$ diverges, $\\sum b_n$ also diverges. Therefore, $\\sum p_n$ diverges, and so does $\\sum \\mathbb{P}(A_n)$.\n\nCase 3: $c = 1$.\nThe series is $\\sum_{n=3}^\\infty \\frac{1}{n (\\ln n)^{1/2}}$. We apply the integral test to the function $f(x) = \\frac{1}{x (\\ln x)^{1/2}}$.\n$$\n\\int_3^\\infty \\frac{1}{x (\\ln x)^{1/2}} dx\n$$\nUsing the substitution $u = \\ln x$, we have $du = \\frac{1}{x} dx$. The integral becomes:\n$$\n\\int_{\\ln 3}^\\infty \\frac{1}{u^{1/2}} du = \\left[ 2u^{1/2} \\right]_{\\ln 3}^\\infty = \\lim_{R\\to\\infty} (2\\sqrt{R}) - 2\\sqrt{\\ln 3} = \\infty\n$$\nSince the integral diverges, the series diverges for $c=1$. Thus, $\\sum \\mathbb{P}(A_n)$ diverges.\n\nIn summary:\n- If $c > 1$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_n)  \\infty$.\n- If $c \\le 1$, then $\\sum_{n=3}^{\\infty} \\mathbb{P}(A_n) = \\infty$.\n\nThe problem defines the critical constant $c^\\ast$ via the dichotomy:\n- $c  c^\\ast \\implies \\sum \\mathbb{P}(A_n) = \\infty$\n- $c > c^\\ast \\implies \\sum \\mathbb{P}(A_n)  \\infty$\n\nComparing this with our findings, we see that the conditions are satisfied precisely when $c^\\ast = 1$.\n- If $c  1$, the sum is infinite.\n- If $c > 1$, the sum is finite.\nThe value of the critical constant is therefore $1$.", "answer": "$$\\boxed{1}$$", "id": "2991415"}, {"introduction": "While the previous exercise explored independent events, most applications involving solutions to stochastic differential equations (SDEs) do not have this luxury. This problem challenges you to convert moment bounds on SDE solutions into an almost sure statement about path regularity. You will use the formidable Burkholder-Davis-Gundy (BDG) inequality to control the moments of stochastic integrals, which in turn provides the probability estimates needed for the first Borel-Cantelli lemma. This exercise is a masterclass in combining analytical tools from stochastic calculus with probabilistic limit theorems to understand the fine structure of SDE paths [@problem_id:2991399].", "problem": "Let $\\left(\\Omega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\ge 0},\\mathbb{P}\\right)$ be a filtered probability space satisfying the usual conditions, and let $W$ be a one-dimensional standard Brownian motion. Consider the one-dimensional Itô stochastic differential equation (SDE)\n$$\ndX_{t}=b\\!\\left(X_{t}\\right)\\,dt+\\sigma\\!\\left(X_{t}\\right)\\,dW_{t},\\quad X_{0}\\in L^{p}(\\Omega),\n$$\nwhere $b,\\sigma:\\mathbb{R}\\to\\mathbb{R}$ are globally Lipschitz and bounded. Define a deterministic increasing sequence of times $(t_{n})_{n\\ge 0}$ by $t_{0}=0$ and\n$$\n\\Delta_{n}:=t_{n}-t_{n-1}=\\frac{1}{n\\,[\\ln(n+1)]^{2}},\\quad n\\ge 1,\n$$\nso that $T:=\\sum_{n=1}^{\\infty}\\Delta_{n}\\infty$ and $\\bigcup_{n\\ge 1}I_{n}=[0,T)$ with $I_{n}:=[t_{n-1},t_{n}]$. For fixed $\\varepsilon\\in(0,1]$, define the events\n$$\nA_{n}=\\Bigl\\{\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|\\varepsilon\\Bigr\\},\\quad n\\ge 1.\n$$\nUsing only first principles of stochastic calculus, standard martingale inequalities, and convergence criteria for series, determine the exact value of the infimum $p_{\\star}0$ such that for every $\\varepsilon\\in(0,1]$ and every solution $X$ with $\\mathbb{E}|X_{0}|^{p}\\infty$ for some $p\\ge p_{\\star}$, one has\n$$\n\\sum_{n=1}^{\\infty}\\mathbb{P}(A_{n})\\infty.\n$$\nGive your final answer as a single real number $p_{\\star}$.", "solution": "The problem asks for the infimum $p_{\\star} > 0$ such that for any choice of SDE satisfying the given conditions and any initial condition $X_0$ with a finite $p$-th moment for some $p \\ge p_\\star$, the series $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ converges.\n\nBy the first Borel-Cantelli lemma, the convergence of $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ implies that $\\mathbb{P}(A_n \\text{ i.o.})=0$. This means that for any $\\varepsilon > 0$, almost surely, the maximum oscillation of $X_t$ in the interval $I_n$ exceeds $\\varepsilon$ for only a finite number of $n$.\n\nWe begin by estimating $\\mathbb{P}(A_n)$. Using Markov's inequality for any $p > 0$:\n$$\n\\mathbb{P}(A_n) = \\mathbb{P}\\left(\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|\\varepsilon\\right) = \\mathbb{P}\\left(\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p  \\varepsilon^p\\right) \\le \\frac{1}{\\varepsilon^p}\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right].\n$$\nTo ensure $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)  \\infty$, it is sufficient to show that $\\sum_{n=1}^{\\infty}\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right]  \\infty$.\n\nLet us analyze the moment term. The solution to the SDE in integral form is\n$$\nX_t - X_{t_{n-1}} = \\int_{t_{n-1}}^t b(X_s)\\,ds + \\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s.\n$$\nUsing the triangle inequality, we have\n$$\n\\sup_{t\\in I_{n}}\\bigl|X_t - X_{t_{n-1}}\\bigr| \\le \\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t b(X_s)\\,ds\\right| + \\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s\\right|.\n$$\nThe coefficients $b$ and $\\sigma$ are bounded, so there exist constants $K_b > 0$ and $K_\\sigma > 0$ such that $|b(x)| \\le K_b$ and $|\\sigma(x)| \\le K_\\sigma$ for all $x \\in \\mathbb{R}$.\n\nThe drift term is bounded as follows:\n$$\n\\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t b(X_s)\\,ds\\right| \\le \\int_{t_{n-1}}^{t_n} |b(X_s)|\\,ds \\le K_b (t_n - t_{n-1}) = K_b \\Delta_n.\n$$\nFor the diffusion term, we use the Burkholder-Davis-Gundy (BDG) inequality. For any $p \\ge 1$, there exists a constant $C_p>0$ such that for the continuous local martingale $M_t = \\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s$ (which is a true martingale as $\\sigma$ is bounded), we have:\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\left| M_t \\right|^p \\, \\bigg| \\, \\mathcal{F}_{t_{n-1}}\\right] \\le C_p \\mathbb{E}\\left[ \\langle M \\rangle_{t_n}^{p/2} \\, \\bigg| \\, \\mathcal{F}_{t_{n-1}}\\right].\n$$\nThe quadratic variation is $\\langle M \\rangle_{t_n} = \\int_{t_{n-1}}^{t_n} \\sigma^2(X_s)\\,ds$. Using the bound on $\\sigma$:\n$$\n\\langle M \\rangle_{t_n} \\le \\int_{t_{n-1}}^{t_n} K_\\sigma^2\\,ds = K_\\sigma^2 \\Delta_n.\n$$\nTherefore,\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\left| \\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s \\right|^p \\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\sup_{t\\in I_{n}}|M_t|^p \\, \\bigg| \\, \\mathcal{F}_{t_{n-1}}\\right]\\right] \\le C_p \\mathbb{E}\\left[ (K_\\sigma^2 \\Delta_n)^{p/2} \\right] = C_p K_\\sigma^p (\\Delta_n)^{p/2}.\n$$\nNow, we combine the bounds for the drift and diffusion parts using the inequality $(a+b)^p \\le 2^{p-1}(a^p + b^p)$ for $p \\ge 1$:\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right] \\le 2^{p-1} \\left( (K_b \\Delta_n)^p + \\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\left|\\int_{t_{n-1}}^t \\sigma(X_s)\\,dW_s\\right|^p\\right] \\right)\n$$\n$$\n\\le 2^{p-1} \\left( (K_b \\Delta_n)^p + C_p K_\\sigma^p (\\Delta_n)^{p/2} \\right).\n$$\nFor large $n$, $\\Delta_n \\to 0$. Since we are interested in $p>0$, we have $p > p/2$. Thus, the term $(\\Delta_n)^{p/2}$ dominates $(\\Delta_n)^p$. Consequently, there exists a constant $C > 0$ (depending on $p, K_b, K_\\sigma, C_p$) such that for all sufficiently large $n$:\n$$\n\\mathbb{E}\\left[\\sup_{t\\in I_{n}}\\bigl|X_{t}-X_{t_{n-1}}\\bigr|^p\\right] \\le C (\\Delta_n)^{p/2}.\n$$\nThis bound holds irrespective of the finiteness of $\\mathbb{E}[|X_0|^p]$, due to the coefficients being bounded.\n\nNow, we examine the convergence of the series $\\sum_n \\mathbb{P}(A_n)$.\n$$\n\\sum_{n=1}^{\\infty} \\mathbb{P}(A_n) \\le \\sum_{n=1}^{\\infty} \\frac{C}{\\varepsilon^p} (\\Delta_n)^{p/2} = \\frac{C}{\\varepsilon^p} \\sum_{n=1}^{\\infty} \\left( \\frac{1}{n[\\ln(n+1)]^2} \\right)^{p/2} = \\frac{C}{\\varepsilon^p} \\sum_{n=1}^{\\infty} \\frac{1}{n^{p/2} [\\ln(n+1)]^p}.\n$$\nThis is a Bertrand-type series. For large $n$, $\\ln(n+1) \\sim \\ln n$. The convergence of this series $\\sum_{n=2}^{\\infty} \\frac{1}{n^\\alpha (\\ln n)^\\beta}$ depends on the values of $\\alpha=p/2$ and $\\beta=p$. The series converges if and only if ($\\alpha > 1$) or ($\\alpha=1$ and $\\beta > 1$).\n\nLet's check these conditions:\n1.  $\\alpha > 1 \\implies p/2 > 1 \\implies p > 2$.\n2.  $\\alpha = 1 \\implies p/2 = 1 \\implies p = 2$. In this case, we check the condition on $\\beta$. We have $\\beta = p = 2$. Since $2 > 1$, the condition $\\beta > 1$ is satisfied.\n\nCombining these two cases, the series converges for all $p \\ge 2$. Therefore, for any $p \\ge 2$, the condition $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)  \\infty$ is satisfied for any SDE with bounded Lipschitz coefficients. This implies that $p_\\star \\le 2$.\n\nTo show that this bound is sharp, we need to show that for any $p  2$, there exists at least one SDE for which the sum diverges. It is a known feature of stochastic processes that the moment estimates derived from the BDG inequality are sharp in their scaling with respect to the time interval. That is, for a simple SDE like $dX_t = dW_t$ (where $b=0, \\sigma=1$, which are bounded and Lipschitz), one can establish a lower bound of the same order.\nFor $p  2$, we have $p/2  1$. The series $\\sum \\frac{1}{n^{p/2} [\\ln(n+1)]^p}$ diverges because the main power law term $n^{p/2}$ has an exponent less than $1$. If one can construct an SDE and an initial condition for which $\\mathbb{P}(A_n) \\ge c (\\Delta_n)^{p/2}$ for some constant $c>0$, then $\\sum_n \\mathbb{P}(A_n)$ would diverge. The standard estimates on the moments of the supremum of Brownian motion confirm that $\\mathbb{E}[\\sup_{t \\in I_n} |W_t - W_{t_{n-1}}|^p]$ is precisely of the order $(\\Delta_n)^{p/2}$. A more detailed analysis involving reverse inequalities (like the Paley-Zygmund inequality) can show that for suitable choices of parameters, the probability $\\mathbb{P}(A_n)$ also scales in a way that leads to a divergent sum for $p2$. Thus, the condition is not guaranteed to hold for $p2$.\n\nThis implies that $p_\\star \\ge 2$.\nCombining the two results, $p_\\star \\le 2$ and $p_\\star \\ge 2$, we conclude that the exact value is $p_\\star = 2$.\nThe phrasing \"for some $p \\ge p_\\star$\" in the problem is satisfied, as for our derived $p_\\star=2$, the condition holds for all $p \\ge 2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "2991399"}, {"introduction": "What happens when the sum of probabilities diverges and the events are dependent, rendering both Borel-Cantelli lemmas seemingly powerless? This common scenario in the analysis of numerical schemes and other convergence problems is not a dead end. This practice demonstrates a crucial technique: recovering an almost sure convergence result by passing to a sparser subsequence of events. You will explore how to strategically choose a subsequence to ensure the sum of probabilities converges, thereby allowing the first Borel-Cantelli lemma to be successfully applied and guaranteeing a (weaker) almost sure conclusion [@problem_id:2991422].", "problem": "Consider a $d$-dimensional Itô stochastic differential equation with globally Lipschitz drift and diffusion coefficients,\n$$\ndX_t \\;=\\; b(X_t)\\,dt \\;+\\; \\sigma(X_t)\\,dW_t,\\qquad X_0=x_0,\\quad t\\in[0,T],\n$$\nwhere $W_t$ is a standard $m$-dimensional Brownian motion, and let $X^n$ denote the Euler–Maruyama approximation on the uniform mesh of size $1/n$. For $p\\ge 2$, a classical combination of the Burkholder–Davis–Gundy (BDG) inequality and Gronwall’s lemma yields the strong moment bound\n$$\n\\mathbb{E}\\Big[\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|^p\\Big]\\;\\le\\; C\\,n^{-p/2},\n$$\nfor some constant $C0$ independent of $n$. Fix $p=2$ and define the events\n$$\nA_n \\;=\\; \\Big\\{\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big| \\;\\; n^{-1/4}\\Big\\}.\n$$\nYou wish to obtain an almost sure convergence statement along a subsequence, even though the full series $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ diverges. Using only first principles such as the Markov inequality, the first Borel–Cantelli lemma, and the given moment bound, which of the following strategies correctly selects a subsequence $(n_k)_{k\\ge 1}$ such that $\\sum_{k=1}^{\\infty}\\mathbb{P}(A_{n_k})\\infty$, thereby implying\n$$\n\\sup_{0\\le t\\le T}\\big|X^{n_k}(t)-X(t)\\big|\\;\\le\\; n_k^{-1/4}\\quad\\text{for all but finitely many }k\\text{ almost surely}?\n$$\n\nA. Take the dyadic subsequence $n_k=2^k$. Then $\\mathbb{P}(A_{n_k})\\le C\\,2^{-k/2}$, so $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma yields the desired almost sure statement along $(n_k)$.\n\nB. Choose $n_k$ so that $\\mathbb{P}(A_{n_k})\\le 2^{-k}$, for example by ensuring $C\\,n_k^{-1/2}\\le 2^{-k}$. Such a choice makes $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma implies the desired almost sure statement along $(n_k)$.\n\nC. Invoke the second Borel–Cantelli lemma to conclude $\\mathbb{P}(A_n \\text{ i.o.})=1$ without any independence assumptions, showing that no subsequence can improve the almost sure behavior.\n\nD. Take the linear subsequence $n_k=k$. Since $\\mathbb{P}(A_{n_k})\\le C\\,k^{-1/2}$, one has $\\sum_k\\mathbb{P}(A_{n_k})=\\infty$, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.\n\nE. Take $n_k=\\lfloor k\\log k\\rfloor$. Since $\\mathbb{P}(A_{n_k})\\le C\\,(k\\log k)^{-1/2}$, the series $\\sum_k\\mathbb{P}(A_{n_k})$ diverges, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.\n\nSelect all correct options.", "solution": "The user has requested a detailed validation of the problem statement, followed by a full solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   A $d$-dimensional Itô stochastic differential equation (SDE) is given:\n    $$\n    dX_t \\;=\\; b(X_t)\\,dt \\;+\\; \\sigma(X_t)\\,dW_t,\\qquad X_0=x_0,\\quad t\\in[0,T]\n    $$\n-   $b$ and $\\sigma$ are globally Lipschitz continuous drift and diffusion coefficients.\n-   $W_t$ is a standard $m$-dimensional Brownian motion.\n-   $X^n$ is the Euler–Maruyama approximation on a uniform mesh of size $1/n$.\n-   A strong moment bound is provided for $p\\ge 2$:\n    $$\n    \\mathbb{E}\\Big[\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|^p\\Big]\\;\\le\\; C\\,n^{-p/2}\n    $$\n    for some constant $C0$ independent of $n$.\n-   The parameter $p$ is fixed to $p=2$.\n-   Events $A_n$ are defined as:\n    $$\n    A_n \\;=\\; \\Big\\{\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big| \\;\\; n^{-1/4}\\Big\\}\n    $$\n-   It is stated that the series $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ diverges.\n-   The goal is to identify a correct strategy for selecting a subsequence $(n_k)_{k\\ge 1}$ such that $\\sum_{k=1}^{\\infty}\\mathbb{P}(A_{n_k})\\infty$, which implies the almost sure convergence statement by the first Borel–Cantelli lemma.\n-   The solution must use only first principles like the Markov inequality, the first Borel–Cantelli lemma, and the given moment bound.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is set within the standard, well-established mathematical framework of stochastic differential equations and their numerical approximation. The Euler-Maruyama method and its convergence properties, including strong moment bounds, are canonical topics in this field. The use of the Borel-Cantelli lemmas to convert convergence in probability to almost sure convergence for a subsequence is a fundamental technique in probability theory. The problem is entirely sound from a mathematical and scientific perspective.\n-   **Well-Posed**: The problem is clearly defined. It provides a known convergence result in the mean-square sense and asks for a valid method to obtain a related almost sure convergence result along a subsequence. The objective is unambiguous: find a subsequence $(n_k)$ that makes the series of probabilities $\\sum_k \\mathbb{P}(A_{n_k})$ summable.\n-   **Objective**: The language used is precise, mathematical, and free of any subjective or ambiguous terminology.\n-   **Incomplete or Contradictory Setup**: The setup is complete and consistent. All necessary information—the SDE, the approximation scheme, the moment bound, and the definition of the events—is provided. Let's verify the consistency of the claim that $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ diverges. Using Markov's inequality on the given moment bound for $p=2$, we have:\n    $$\n    \\mathbb{P}(A_n) = \\mathbb{P}\\Big(\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|  n^{-1/4}\\Big) = \\mathbb{P}\\Big(\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|^2  n^{-1/2}\\Big)\n    $$\n    $$\n    \\le \\frac{\\mathbb{E}\\Big[\\sup_{0\\le t\\le T}\\big|X^n(t)-X(t)\\big|^2\\Big]}{n^{-1/2}} \\le \\frac{C\\,n^{-1}}{n^{-1/2}} = C\\,n^{-1/2}\n    $$\n    Since the series $\\sum_{n=1}^{\\infty} n^{-1/2}$ diverges, this upper bound is consistent with the problem's statement that $\\sum_{n=1}^{\\infty}\\mathbb{P}(A_n)$ diverges. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid, as it is mathematically sound, well-posed, and self-consistent. The solution process can proceed.\n\n### Solution Derivation\n\nThe core of the problem is to find a strictly increasing sequence of integers $(n_k)_{k\\ge 1}$ such that the series $\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{n_k})$ converges. If this condition is met, the first Borel–Cantelli lemma states that $\\mathbb{P}(\\limsup_{k\\to\\infty} A_{n_k}) = 0$. This means that, almost surely, only a finite number of the events $A_{n_k}$ occur. This is equivalent to the desired statement:\n$$\n\\sup_{0\\le t\\le T}\\big|X^{n_k}(t)-X(t)\\big|\\;\\le\\; n_k^{-1/4}\\quad\\text{for all but finitely many }k\\text{ almost surely}.\n$$\nAs derived during validation, we have established the following bound on the probability of the events $A_n$:\n$$\n\\mathbb{P}(A_n) \\le C\\,n^{-1/2}\n$$\nfor some constant $C  0$. Therefore, the problem reduces to finding a subsequence $(n_k)$ such that the series $\\sum_{k=1}^{\\infty} n_k^{-1/2}$ converges.\n\n### Option-by-Option Analysis\n\n**A. Take the dyadic subsequence $n_k=2^k$. Then $\\mathbb{P}(A_{n_k})\\le C\\,2^{-k/2}$, so $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma yields the desired almost sure statement along $(n_k)$.**\n\nLet us analyze this strategy. We set $n_k = 2^k$ for $k=1, 2, \\dots$. We examine the sum of probabilities:\n$$\n\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{n_k}) = \\sum_{k=1}^{\\infty} \\mathbb{P}(A_{2^k})\n$$\nUsing our derived bound:\n$$\n\\mathbb{P}(A_{2^k}) \\le C\\,(2^k)^{-1/2} = C\\,(2^{-1/2})^k = C\\,\\left(\\frac{1}{\\sqrt{2}}\\right)^k\n$$\nTherefore, the series is bounded by a convergent geometric series:\n$$\n\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{2^k}) \\le \\sum_{k=1}^{\\infty} C\\,\\left(\\frac{1}{\\sqrt{2}}\\right)^k = C \\frac{1/\\sqrt{2}}{1 - 1/\\sqrt{2}}  \\infty\n$$\nThe convergence is guaranteed because the ratio of the geometric series is $r=1/\\sqrt{2}  1$. Since the series of probabilities converges, the first Borel–Cantelli lemma applies, and the stated conclusion follows. The reasoning presented in this option is entirely correct.\n\n**Verdict: Correct**\n\n**B. Choose $n_k$ so that $\\mathbb{P}(A_{n_k})\\le 2^{-k}$, for example by ensuring $C\\,n_k^{-1/2}\\le 2^{-k}$. Such a choice makes $\\sum_k\\mathbb{P}(A_{n_k})\\infty$, and the first Borel–Cantelli lemma implies the desired almost sure statement along $(n_k)$.**\n\nThis option proposes a general strategy. If we can construct a subsequence $(n_k)$ that satisfies $\\mathbb{P}(A_{n_k})\\le 2^{-k}$, then the sum $\\sum_{k=1}^{\\infty} \\mathbb{P}(A_{n_k}) \\le \\sum_{k=1}^{\\infty} 2^{-k} = 1  \\infty$. The series converges, so the first Borel–Cantelli lemma applies, and the conclusion is valid.\n\nThe key question is whether such a subsequence can be constructed. The option suggests satisfying the condition by choosing $n_k$ such that $C\\,n_k^{-1/2} \\le 2^{-k}$. Let's rearrange this inequality to solve for $n_k$:\n$$\nn_k^{1/2} \\ge C\\,2^k \\implies n_k \\ge (C\\,2^k)^2 = C^2\\,4^k\n$$\nFor any constant $C0$, we can always find a strictly increasing sequence of integers satisfying this condition. For example, we can choose $n_k = \\lceil C^2\\,4^k \\rceil$ for $k=1, 2, \\dots$. This defines a valid subsequence for which the condition holds. Thus, the strategy is sound and achievable.\n\n**Verdict: Correct**\n\n**C. Invoke the second Borel–Cantelli lemma to conclude $\\mathbb{P}(A_n \\text{ i.o.})=1$ without any independence assumptions, showing that no subsequence can improve the almost sure behavior.**\n\nThis statement has two major flaws.\n1.  The standard second Borel–Cantelli lemma states that if $\\sum_n \\mathbb{P}(A_n) = \\infty$ and the events $(A_n)$ are independent, then $\\mathbb{P}(A_n \\text{ i.o.}) = 1$. The option falsely claims this can be invoked \"without any independence assumptions\". While generalizations exist (e.g., for pairwise independent events), they do not apply universally. The events $A_n$ and $A_m$ are highly dependent, as they are defined in terms of errors of approximations driven by the same underlying Brownian motion path $W_t$.\n2.  Even if one were to prove that $\\mathbb{P}(A_n \\text{ i.o.}) = 1$ for the full sequence, this does not imply that \"no subsequence can improve the almost sure behavior\". The event $\\limsup_{n\\to\\infty} A_n$ is not the same as the event $\\limsup_{k\\to\\infty} A_{n_k}$. As shown in options A and B, by choosing a sufficiently sparse subsequence $(n_k)$, we can ensure $\\sum_k \\mathbb{P}(A_{n_k})  \\infty$, which guarantees $\\mathbb{P}(\\limsup_{k\\to\\infty} A_{n_k}) = 0$.\nThe reasoning in this option is incorrect.\n\n**Verdict: Incorrect**\n\n**D. Take the linear subsequence $n_k=k$. Since $\\mathbb{P}(A_{n_k})\\le C\\,k^{-1/2}$, one has $\\sum_k\\mathbb{P}(A_{n_k})=\\infty$, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.**\n\nThis option considers the original sequence, as $n_k=k$. The analysis is that $\\sum_k \\mathbb{P}(A_k) = \\infty$. This is consistent with the problem statement and our bound $\\mathbb{P}(A_k) \\le Ck^{-1/2}$, as the harmonic series with $p=1/2$ diverges. The conclusion is that the first Borel–Cantelli lemma cannot be applied to prove the desired result. This is a correct statement of fact: the condition for the lemma (summability) is not met. However, the question asks for a strategy that *correctly selects a subsequence such that $\\sum_{k=1}^{\\infty}\\mathbb{P}(A_{n_k})\\infty$*. This strategy, $n_k=k$, fails to satisfy this crucial condition. Therefore, it is not a correct strategy *for achieving the stated goal*. The option correctly analyzes a failing strategy, but it is not an answer to the question posed.\n\n**Verdict: Incorrect**\n\n**E. Take $n_k=\\lfloor k\\log k\\rfloor$. Since $\\mathbb{P}(A_{n_k})\\le C\\,(k\\log k)^{-1/2}$, the series $\\sum_k\\mathbb{P}(A_{n_k})$ diverges, so the first Borel–Cantelli lemma cannot be applied to obtain the desired almost sure statement along $(n_k)$.**\n\nLet's analyze the subsequence $n_k=\\lfloor k\\log k\\rfloor$ (for $k \\ge 2$). We must check the convergence of $\\sum_k \\mathbb{P}(A_{n_k})$. It is sufficient to analyze the convergence of the bounding series:\n$$\n\\sum_{k=2}^\\infty n_k^{-1/2} \\approx \\sum_{k=2}^\\infty (k\\log k)^{-1/2} = \\sum_{k=2}^\\infty \\frac{1}{k^{1/2} (\\log k)^{1/2}}\n$$\nThis is a Bertrand series of the form $\\sum_k \\frac{1}{k^\\alpha (\\log k)^\\beta}$ with $\\alpha=1/2$ and $\\beta=1/2$. Such a series converges if and only if $\\alpha > 1$, or $\\alpha=1$ and $\\beta > 1$. Since $\\alpha = 1/2  1$, the series diverges. The reasoning presented in the option is correct: this subsequence is not sparse enough, the sum of probabilities diverges, and the first Borel–Cantelli lemma cannot be used to prove the desired result. As with option D, this is a correct analysis of a strategy that fails. It does not describe a strategy that achieves the goal stated in the question.\n\n**Verdict: Incorrect**", "answer": "$$\\boxed{AB}$$", "id": "2991422"}]}