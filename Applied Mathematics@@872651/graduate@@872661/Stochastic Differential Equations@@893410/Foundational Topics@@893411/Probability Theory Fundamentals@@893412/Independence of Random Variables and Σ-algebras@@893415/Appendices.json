{"hands_on_practices": [{"introduction": "We begin by testing our understanding of independence at its most fundamental level. This exercise [@problem_id:2980204] challenges us to determine when a random variable's sign, $\\operatorname{sign}(X)$, is independent of its magnitude, $|X|$, based solely on the symmetry of its distribution. The solution is surprisingly sharp and reveals how independence can hinge on the probability mass at a single point, forcing a rigorous application of the definition through $\\sigma$-algebras.", "problem": "Let $X$ be a real-valued random variable on a probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ whose distribution is symmetric about $0$ in the sense that $X \\stackrel{d}{=} -X$. Define $S=\\operatorname{sign}(X)$ with the convention $\\operatorname{sign}(0)=0$, and $Y=|X|$. Recall the definitions: two random variables $U$ and $V$ are independent if $\\sigma(U)$ and $\\sigma(V)$ are independent $\\sigma$-algebras, equivalently if $\\mathbb{P}(U\\in A,V\\in B)=\\mathbb{P}(U\\in A)\\mathbb{P}(V\\in B)$ for all Borel sets $A,B\\subset\\mathbb{R}$. Using only these definitions and the symmetry property $X \\stackrel{d}{=} -X$, determine necessary and sufficient conditions (in terms of the mass at $0$) under which $S$ is independent of $Y$ (equivalently, $\\sigma(S)$ is independent of $\\sigma(Y)$). Then apply your criterion to the Gaussian case and to non-Gaussian symmetric laws.\n\nWhich of the following statements are correct?\n\nA. $S$ and $Y$ are independent if and only if either $\\mathbb{P}(X=0)=0$ or $\\mathbb{P}(X=0)=1$. In particular, for any symmetric law with no atom at $0$ (discrete or continuous), $S$ and $Y$ are independent; if $0\\mathbb{P}(X=0)1$, they are not independent.\n\nB. Independence of $S$ and $Y$ characterizes the Gaussian among symmetric laws: if $X$ is symmetric and $S$ and $Y$ are independent, then $X$ must be Gaussian.\n\nC. If $X$ is symmetric and absolutely continuous with an even density $f$ and $\\mathbb{P}(X=0)=0$, then $S$ and $Y$ are independent, but if $X$ is symmetric and supported on finitely many nonzero points, then $S$ and $Y$ are never independent.\n\nD. If $X\\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma0$, then $S$ and $Y$ are independent and $\\sigma(S)$ is independent of $\\sigma(Y)$. If $X$ is symmetric Bernoulli, taking values $\\{-1,+1\\}$ with probability $1/2$ each, then $S$ and $Y$ are independent.\n\nE. If $X$ has a symmetric distribution with an atom at $0$ and positive mass elsewhere, then redefining $\\operatorname{sign}(0)=+1$ makes $S$ and $Y$ independent.", "solution": "We begin from the core definitions. Independence of random variables $U$ and $V$ means $\\mathbb{P}(U\\in A, V\\in B)=\\mathbb{P}(U\\in A)\\mathbb{P}(V\\in B)$ for all Borel sets $A,B\\subset\\mathbb{R}$, which is equivalent to independence of the generated $\\sigma$-algebras $\\sigma(U)$ and $\\sigma(V)$. Symmetry $X\\stackrel{d}{=}-X$ means $\\mathbb{P}(X\\in B)=\\mathbb{P}(X\\in -B)$ for all Borel $B$, where $-B=\\{-x:x\\in B\\}$.\n\nSet $S=\\operatorname{sign}(X)$ with $\\operatorname{sign}(0)=0$, and $Y=|X|$. Let $p_{0}=\\mathbb{P}(X=0)=\\mathbb{P}(Y=0)=\\mathbb{P}(S=0)$.\n\nStep 1: A necessary condition via events away from $0$. Fix any Borel set $B\\subset(0,\\infty)$ (so $0\\notin B$). Then the events $\\{X\\in B\\}$ and $\\{X\\in -B\\}$ are disjoint, and by symmetry $\\mathbb{P}(X\\in B)=\\mathbb{P}(X\\in -B)$. Hence\n$$\n\\mathbb{P}(Y\\in B)=\\mathbb{P}(X\\in B\\cup -B)\n=\\mathbb{P}(X\\in B)+\\mathbb{P}(X\\in -B)=2\\,\\mathbb{P}(X\\in B).\n$$\nMoreover,\n$$\n\\mathbb{P}(Y\\in B, S=+1)=\\mathbb{P}(|X|\\in B, X0)=\\mathbb{P}(X\\in B),\n$$\nsince for $B\\subset(0,\\infty)$ the intersection $\\{|X|\\in B, X0\\}$ is exactly $\\{X\\in B\\}$. If $S$ and $Y$ are independent, then for every such $B$,\n$$\n\\mathbb{P}(X\\in B)\n=\\mathbb{P}(Y\\in B, S=+1)\n=\\mathbb{P}(Y\\in B)\\,\\mathbb{P}(S=+1).\n$$\nUsing $\\mathbb{P}(Y\\in B)=2\\,\\mathbb{P}(X\\in B)$, we get\n$$\n\\mathbb{P}(X\\in B)=2\\,\\mathbb{P}(X\\in B)\\,\\mathbb{P}(S=+1).\n$$\nIf there exists $B\\subset(0,\\infty)$ with $\\mathbb{P}(X\\in B)0$, then we can cancel to obtain\n$$\n\\mathbb{P}(S=+1)=\\tfrac{1}{2}.\n$$\nFor a symmetric $X$, we have $\\mathbb{P}(X0)=\\mathbb{P}(X0)=(1-p_{0})/2$ and $\\mathbb{P}(S=+1)=\\mathbb{P}(X0)=(1-p_{0})/2$. Thus the condition $\\mathbb{P}(S=+1)=\\tfrac{1}{2}$ is equivalent to $p_{0}=0$.\n\nTherefore, if $S$ and $Y$ are independent and there exists any positive mass away from $0$ (i.e., $\\mathbb{P}(|X|0)0$, equivalently $p_{0}1$), then necessarily $p_{0}=0$.\n\nStep 2: Edge case when all mass is at $0$. If $p_{0}=1$, then $X=0$ almost surely. Consequently $S=0$ almost surely and $Y=0$ almost surely. In this degenerate case both $S$ and $Y$ are constants, and constant random variables are independent of any other random variable. Hence independence holds when $p_{0}=1$.\n\nCombining with Step 1, we have shown necessity: if $S$ and $Y$ are independent, then either $p_{0}=1$ or $p_{0}=0$.\n\nStep 3: Sufficiency when $p_{0}=0$. Suppose $p_{0}=0$. For any $B\\subset(0,\\infty)$ as above,\n$$\n\\mathbb{P}(Y\\in B, S=+1)=\\mathbb{P}(X\\in B)=\\tfrac{1}{2}\\,\\mathbb{P}(X\\in B\\cup -B)=\\tfrac{1}{2}\\,\\mathbb{P}(Y\\in B),\n$$\nwhere we used symmetry and disjointness of $B$ and $-B$. Since $\\mathbb{P}(S=+1)=\\tfrac{1}{2}$ when $p_{0}=0$, we have shown $\\mathbb{P}(Y\\in B, S=+1)=\\mathbb{P}(Y\\in B)\\mathbb{P}(S=+1)$ for all $B\\subset(0,\\infty)$. A similar computation gives $\\mathbb{P}(Y\\in B, S=-1)=\\mathbb{P}(Y\\in B)\\mathbb{P}(S=-1)$ for all $B\\subset(0,\\infty)$. For sets $B$ containing $0$ we use that $\\mathbb{P}(Y=0)=0$ in the case $p_{0}=0$, so adding or removing $\\{0\\}$ does not change probabilities. By a monotone class argument (or a standard $\\pi$-$\\lambda$ argument) these equalities extend to all Borel $B\\subset[0,\\infty)$, proving independence of $S$ and $Y$.\n\nStep 4: Sufficiency when $p_{0}=1$. As noted, if $X=0$ almost surely, then $S=0$ and $Y=0$ almost surely, so independence of $S$ and $Y$ is immediate.\n\nConclusion: For symmetric $X$, $S$ and $Y$ are independent if and only if either $p_{0}=0$ or $p_{0}=1$. Equivalently, independence fails exactly when $0p_{0}1$.\n\nSince independence of random variables is equivalent to independence of their generated $\\sigma$-algebras, the same criterion describes when $\\sigma(S)$ is independent of $\\sigma(Y)$.\n\nApplications:\n- Gaussian case: If $X\\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma0$, then $p_{0}=0$ and $X$ is symmetric, hence $S$ and $Y$ are independent and $\\sigma(S)\\perp\\sigma(Y)$. This is a familiar special case used in the analysis of Brownian motion at fixed times $t0$ where $B_{t}\\sim \\mathcal{N}(0,t)$.\n- Non-Gaussian examples where independence holds: symmetric Bernoulli $X\\in\\{-1,+1\\}$ with probability $1/2$ each has $p_{0}=0$. Then $Y\\equiv 1$ is constant and $S=X$, so $S$ and $Y$ are independent. Likewise, symmetric Laplace or Cauchy laws have $p_{0}=0$ and yield independence.\n- Symmetric laws with an atom at $0$ and additional mass elsewhere: if $0p_{0}1$ (for example, $\\mathbb{P}(X=0)=p_{0}$ and with the remaining mass split symmetrically at $\\pm 1$), then independence fails because conditioning on $Y=0$ determines $S=0$ with probability $1$, whereas $\\mathbb{P}(S=0)=p_{0}\\in(0,1)$.\n\nOption-by-option analysis:\n- Option A: This matches the derived necessary and sufficient condition. It correctly includes both the non-atomic case at $0$ (i.e., $p_{0}=0$) and the degenerate case $p_{0}=1$, and correctly states failure when $0p_{0}1$. Verdict: Correct.\n- Option B: False. Independence of $S$ and $Y$ does not characterize the Gaussian; many non-Gaussian symmetric laws with $\\mathbb{P}(X=0)=0$ (e.g., symmetric Bernoulli, Laplace, Cauchy) also have $S$ and $Y$ independent.\n- Option C: The first clause is true (it is a special case of the general criterion), but the second clause is false: for symmetric discrete laws supported on finitely many nonzero points (e.g., symmetric Bernoulli $\\{-1,+1\\}$), $S$ and $Y$ can be independent. Hence as a whole the statement is false. Verdict: Incorrect.\n- Option D: For $X\\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma0$, we have $p_{0}=0$ and independence holds; independence of random variables implies independence of their generated $\\sigma$-algebras. For symmetric Bernoulli $\\{-1,+1\\}$, $Y\\equiv 1$ is constant and therefore independent of $S=X$. Verdict: Correct.\n- Option E: Redefining $\\operatorname{sign}(0)$ to a deterministic value (e.g., $+1$) does not restore independence when $0p_{0}1$, because $\\mathbb{P}(S=+1\\mid Y=0)=1$ while $\\mathbb{P}(S=+1)1$ for a non-degenerate symmetric law. Verdict: Incorrect.", "answer": "$$\\boxed{AD}$$", "id": "2980204"}, {"introduction": "Moving from a single variable to combinations of variables, this problem [@problem_id:2980206] explores how independence behaves under linear transformations, a common operation in multivariate analysis. By examining the sum $S = X + Y$ and difference $D = X - Y$ of two independent variables, we uncover the conditions required to preserve independence. This practice is crucial as it demonstrates that such preservation is not guaranteed and highlights the special properties of Gaussian distributions, a cornerstone of stochastic differential equation theory.", "problem": "Consider a filtered probability space supporting two independent standard Brownian motions $\\{W^{(1)}_t\\}_{t \\ge 0}$ and $\\{W^{(2)}_t\\}_{t \\ge 0}$, and fix a deterministic time $T  0$. Define two scalar stochastic differential equations (SDEs) with purely diffusive dynamics by\n$$\ndX_t = \\sigma_1\\, dW^{(1)}_t,\\quad X_0 = 0,\\qquad dY_t = \\sigma_2\\, dW^{(2)}_t,\\quad Y_0 = 0,\n$$\nwhere $\\sigma_1, \\sigma_2  0$ are fixed constants. Set $X := X_T$ and $Y := Y_T$, so that $X$ and $Y$ are centered random variables adapted to disjoint driving noises. Consider the linear combinations $S := X + Y$ and $D := X - Y$ and the $\\sigma$-algebras $\\sigma(X)$, $\\sigma(Y)$, $\\sigma(S)$, and $\\sigma(D)$ they generate. Which of the following statements are correct?\n\nA. The pair $(X,Y)$ consists of independent centered Gaussian random variables, but $S$ and $D$ are dependent whenever $\\sigma_1 \\neq \\sigma_2$. This dependence arises because $(S,D)$ is obtained by an orthogonal mixing of $(X,Y)$, and orthogonal transforms preserve independence only when the joint law of $(X,Y)$ is spherical.\n\nB. For any independent mean-zero random variables $X$ and $Y$ with finite variances, the orthogonal mixing into $S = X + Y$ and $D = X - Y$ preserves independence, so $S$ and $D$ are always independent.\n\nC. If $X$ and $Y$ are independent and identically distributed centered Laplace random variables with common scale parameter $b  0$, then $S$ and $D$ are independent due to the symmetry of the Laplace law.\n\nD. Independence of the $\\sigma$-algebras $\\sigma(X)$ and $\\sigma(Y)$ implies independence of $\\sigma(S)$ and $\\sigma(D)$ under any invertible linear transformation from $(X,Y)$ to $(S,D)$.\n\nE. If $(X,Y)$ are independent centered Gaussian random variables with $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$, then $S$ and $D$ are independent because the joint law of $(X,Y)$ is spherical and orthogonal transforms preserve that structure.", "solution": "The problem statement must first be validated for scientific soundness, self-consistency, and well-posedness.\n\n### Step 1: Extract Givens\n- A filtered probability space $(\\Omega, \\mathcal{F}, \\{\\mathcal{F}_t\\}_{t \\ge 0}, \\mathbb{P})$.\n- Two independent standard Brownian motions, $\\{W^{(1)}_t\\}_{t \\ge 0}$ and $\\{W^{(2)}_t\\}_{t \\ge 0}$.\n- A fixed deterministic time $T  0$.\n- Two stochastic processes, $X_t$ and $Y_t$, defined by the stochastic differential equations (SDEs):\n  $$dX_t = \\sigma_1\\, dW^{(1)}_t, \\quad X_0 = 0$$\n  $$dY_t = \\sigma_2\\, dW^{(2)}_t, \\quad Y_0 = 0$$\n  where $\\sigma_1  0$ and $\\sigma_2  0$ are fixed constants.\n- Two random variables, $X$ and $Y$, are defined as the values of the processes at time $T$: $X := X_T$ and $Y := Y_T$.\n- Two linear combinations of $X$ and $Y$ are defined: $S := X + Y$ and $D := X - Y$.\n- The question concerns the relationship between the $\\sigma$-algebras generated by these variables: $\\sigma(X)$, $\\sigma(Y)$, $\\sigma(S)$, and $\\sigma(D)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the standard mathematical framework of stochastic calculus and probability theory.\n1.  **Scientific Groundedness**: The concepts of Brownian motion, SDEs, random variables, and $\\sigma$-algebras are fundamental to modern probability theory. The problem investigates the effect of linear transformations on the independence of random variables, a core and well-established topic. The problem is scientifically sound.\n2.  **Well-Posedness**: The SDEs for $X_t$ and $Y_t$ have unique strong solutions given by stochastic integrals:\n    $$X_t = \\int_0^t \\sigma_1 \\, dW^{(1)}_s = \\sigma_1 W^{(1)}_t$$\n    $$Y_t = \\int_0^t \\sigma_2 \\, dW^{(2)}_s = \\sigma_2 W^{(2)}_t$$\n    At time $T$, the random variables are $X = \\sigma_1 W^{(1)}_T$ and $Y = \\sigma_2 W^{(2)}_T$. Since $W^{(1)}_T$ and $W^{(2)}_T$ are independent draws from a normal distribution with mean $0$ and variance $T$, it follows that $X \\sim \\mathcal{N}(0, \\sigma_1^2 T)$ and $Y \\sim \\mathcal{N}(0, \\sigma_2^2 T)$. Due to the independence of the driving Brownian motions, $X$ and $Y$ are independent random variables. The linear combinations $S$ and $D$ are well-defined. The question of their independence is a standard and solvable problem in probability.\n3.  **Objectivity**: The problem statement is composed of objective mathematical definitions and queries. It is free of ambiguity and subjective language.\n4.  **Completeness and Consistency**: All necessary parameters ($\\sigma_1, \\sigma_2, T$) are defined, and their properties (positive constants) are specified. There are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A rigorous solution can be derived.\n\n### Derivation of the Condition for Independence\n\nThe random variables $X$ and $Y$ are independent and follow centered Normal distributions: $X \\sim \\mathcal{N}(0, \\sigma_1^2 T)$ and $Y \\sim \\mathcal{N}(0, \\sigma_2^2 T)$.\nThe vector $(X, Y)$ is therefore a centered bivariate Gaussian random vector.\n\nThe variables $S$ and $D$ are defined by a linear transformation of $(X, Y)$:\n$$\n\\begin{pmatrix} S \\\\ D \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} X \\\\ Y \\end{pmatrix}\n$$\nA linear transformation of a Gaussian random vector results in another Gaussian random vector. Thus, $(S, D)$ is also a centered bivariate Gaussian random vector.\n\nFor a jointly Gaussian random vector, the components are independent if and only if their covariance is zero. We compute the covariance of $S$ and $D$:\n$$\n\\operatorname{Cov}(S, D) = E[S \\cdot D] - E[S]E[D]\n$$\nSince $X$ and $Y$ are centered, $E[X] = 0$ and $E[Y] = 0$, it follows that $E[S] = E[X+Y] = E[X] + E[Y] = 0$ and $E[D] = E[X-Y] = E[X] - E[Y] = 0$.\nTherefore, the covariance simplifies to:\n$$\n\\operatorname{Cov}(S, D) = E[S \\cdot D] = E[(X+Y)(X-Y)] = E[X^2 - Y^2]\n$$\nBy the linearity of expectation, this becomes:\n$$\n\\operatorname{Cov}(S, D) = E[X^2] - E[Y^2]\n$$\nFor a centered random variable, its second moment is equal to its variance: $E[X^2] = \\operatorname{Var}(X)$ and $E[Y^2] = \\operatorname{Var}(Y)$. We know that $\\operatorname{Var}(X) = \\sigma_1^2 T$ and $\\operatorname{Var}(Y) = \\sigma_2^2 T$.\nSubstituting these into the covariance expression:\n$$\n\\operatorname{Cov}(S, D) = \\sigma_1^2 T - \\sigma_2^2 T = T(\\sigma_1^2 - \\sigma_2^2)\n$$\nSince $T  0$, the covariance $\\operatorname{Cov}(S, D)$ is zero if and only if $\\sigma_1^2 = \\sigma_2^2$. Given that $\\sigma_1  0$ and $\\sigma_2  0$, this is equivalent to $\\sigma_1 = \\sigma_2$.\n\nIn summary, for the specific Gaussian variables $X$ and $Y$ defined in the problem, their linear combinations $S = X+Y$ and $D = X-Y$ are independent if and only if $\\sigma_1 = \\sigma_2$, which is equivalent to $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$.\n\n### Option-by-Option Analysis\n\n**A. The pair $(X,Y)$ consists of independent centered Gaussian random variables, but $S$ and $D$ are dependent whenever $\\sigma_1 \\neq \\sigma_2$. This dependence arises because $(S,D)$ is obtained by an orthogonal mixing of $(X,Y)$, and orthogonal transforms preserve independence only when the joint law of $(X,Y)$ is spherical.**\n\n-   The initial claims that $(X,Y)$ are independent centered Gaussians and that $S$ and $D$ are dependent whenever $\\sigma_1 \\neq \\sigma_2$ are correct, as established in our derivation above.\n-   The reasoning provided is also sound. The transformation matrix $M = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$ is orthogonal up to a scaling factor, as $M^T M = 2I$.\n-   The joint probability density of $(X,Y)$ is spherical (radially symmetric) if and only if its level sets are circles. The density is proportional to $\\exp\\left(-\\frac{1}{2T} \\left(\\frac{x^2}{\\sigma_1^2} + \\frac{y^2}{\\sigma_2^2}\\right)\\right)$. The level sets are ellipses unless $\\sigma_1 = \\sigma_2$, in which case they are circles and the distribution is spherical.\n-   When $\\sigma_1 \\neq \\sigma_2$, the distribution is not spherical. The statement correctly identifies that for this orthogonal transformation of independent Gaussian variables, the lack of sphericity in the joint distribution leads to a loss of independence in the transformed variables. The reasoning is a correct characterization for the Gaussian case.\n-   Verdict: **Correct**.\n\n**B. For any independent mean-zero random variables $X$ and $Y$ with finite variances, the orthogonal mixing into $S = X + Y$ and $D = X - Y$ preserves independence, so $S$ and $D$ are always independent.**\n\n-   This statement is a vast overgeneralization. Independence of $S$ and $D$ requires, at a minimum, that they are uncorrelated. As shown previously, $\\operatorname{Cov}(S,D) = \\operatorname{Var}(X) - \\operatorname{Var}(Y)$.\n-   If we take any two independent, mean-zero random variables $X$ and $Y$ such that $\\operatorname{Var}(X) \\neq \\operatorname{Var}(Y)$, then $\\operatorname{Cov}(S,D) \\neq 0$, which immediately implies that $S$ and $D$ are not independent.\n-   For instance, let $X \\sim \\mathcal{N}(0,1)$ and $Y \\sim \\mathcal{N}(0,4)$, which are independent. Then $\\operatorname{Cov}(S,D) = 1 - 4 = -3 \\neq 0$. $S$ and $D$ are dependent.\n-   The statement is thus false. In fact, a celebrated result in probability theory (the Bernstein-Skitovich-Darmois theorem) states that if $X$ and $Y$ are independent and their linear combinations $aX+bY$ and $cX+dY$ (with $abcd \\neq 0$) are also independent, then $X$ and $Y$ must be Gaussian. This option claims independence holds for *any* distribution, which is incorrect.\n-   Verdict: **Incorrect**.\n\n**C. If $X$ and $Y$ are independent and identically distributed centered Laplace random variables with common scale parameter $b  0$, then $S$ and $D$ are independent due to the symmetry of the Laplace law.**\n\n-   Let $X, Y \\sim \\text{Laplace}(0,b)$, i.i.d. Since they are i.i.d., $\\operatorname{Var}(X) = \\operatorname{Var}(Y) = 2b^2$, which implies $\\operatorname{Cov}(S,D) = \\operatorname{Var}(X) - \\operatorname{Var}(Y) = 0$. So $S$ and $D$ are uncorrelated.\n-   However, for non-Gaussian variables, uncorrelation does not imply independence. We must check the joint characteristic function $\\phi_{S,D}(t_1, t_2)$. The characteristic function for a $\\text{Laplace}(0,b)$ variable is $\\phi(t) = (1+b^2t^2)^{-1}$.\n-   $\\phi_{S,D}(t_1, t_2) = E[e^{i(t_1 S + t_2 D)}] = E[e^{i((t_1+t_2)X + (t_1-t_2)Y)}]$.\n-   Due to independence and identical distribution of $X$ and $Y$:\n    $\\phi_{S,D}(t_1, t_2) = \\phi(t_1+t_2) \\phi(t_1-t_2) = \\frac{1}{1+b^2(t_1+t_2)^2} \\frac{1}{1+b^2(t_1-t_2)^2}$.\n-   For independence of $S$ and $D$, we would need $\\phi_{S,D}(t_1, t_2)$ to factorize as $\\phi_S(t_1)\\phi_D(t_2)$.\n-   $\\phi_S(t_1) = \\phi_{S,D}(t_1, 0) = \\phi(t_1)\\phi(t_1) = (1+b^2t_1^2)^{-2}$.\n-   $\\phi_D(t_2) = \\phi_{S,D}(0, t_2) = \\phi(t_2)\\phi(-t_2) = (1+b^2t_2^2)^{-2}$.\n-   The product is $\\phi_S(t_1)\\phi_D(t_2) = (1+b^2t_1^2)^{-2}(1+b^2t_2^2)^{-2}$.\n-   This is clearly not equal to $\\frac{1}{(1+b^2(t_1+t_2)^2)(1+b^2(t_1-t_2)^2)}$ for all $t_1, t_2$. For instance, taking $b=1, t_1=1, t_2=1$ gives $\\frac{1}{(1+4)(1+0)} = \\frac{1}{5}$ for the joint characteristic function, but $\\frac{1}{(1+1)^2(1+1)^2} = \\frac{1}{16}$ for the product of marginals.\n-   Therefore, $S$ and $D$ are not independent. This is a classic counterexample showing that the property of preserving independence under this rotation is characteristic of the Gaussian distribution.\n-   Verdict: **Incorrect**.\n\n**D. Independence of the $\\sigma$-algebras $\\sigma(X)$ and $\\sigma(Y)$ implies independence of $\\sigma(S)$ and $\\sigma(D)$ under any invertible linear transformation from $(X,Y)$ to $(S,D)$.**\n\n-   This is a very strong and false claim. Independence of $\\sigma(X)$ and $\\sigma(Y)$ is equivalent to the independence of the random variables $X$ and $Y$.\n-   The problem setup itself provides a counterexample. If $\\sigma_1 \\neq \\sigma_2$, then $X$ and $Y$ are independent, but $S=X+Y$ and $D=X-Y$ are not. The transformation matrix $M = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$ is invertible (its determinant is $-2$). This directly refutes the statement.\n-   The counterexample from option C (i.i.d. Laplace variables) also disproves this statement.\n-   Verdict: **Incorrect**.\n\n**E. If $(X,Y)$ are independent centered Gaussian random variables with $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$, then $S$ and $D$ are independent because the joint law of $(X,Y)$ is spherical and orthogonal transforms preserve that structure.**\n\n-   The premise is that $X$ and $Y$ are independent, centered Gaussians, and $\\operatorname{Var}(X) = \\operatorname{Var}(Y)$. In the context of our problem, this corresponds to the case $\\sigma_1 = \\sigma_2$.\n-   As shown in the initial derivation, if $\\sigma_1 = \\sigma_2$, then $\\operatorname{Cov}(S,D) = 0$. Since $(S,D)$ is a jointly Gaussian vector, zero covariance implies independence. The conclusion that $S$ and $D$ are independent is correct.\n-   The reasoning provided is also correct. If $X, Y$ are i.i.d. centered Gaussians, their joint density is proportional to $\\exp(-(x^2+y^2)/(2\\sigma^2))$, which is a function of the squared Euclidean norm $x^2+y^2$. This defines a spherical distribution. The transformation from $(X,Y)$ to $(S,D)$ is an orthogonal transformation (up to scale). Applying such a transformation to a spherically symmetric distribution results in a new distribution that is also spherically symmetric. For Gaussian variables, a spherical joint distribution implies that the components are independent.\n-   Verdict: **Correct**.", "answer": "$$\\boxed{AE}$$", "id": "2980206"}, {"introduction": "Our final practice applies these concepts to a core model in stochastic dynamics: the Ornstein-Uhlenbeck process. This exercise [@problem_id:2980191] grounds the abstract theory of Gaussian vectors and conditional independence in a concrete, time-dependent setting. By analyzing the process at discrete time points, you will prove independence between a future state's innovation and the present state, a result that is fundamental to filtering and prediction theory.", "problem": "Consider the Ornstein–Uhlenbeck process defined by the linear stochastic differential equation $dX_t=-a X_t \\, dt + \\sigma \\, dW_t$ with parameters $a0$ and $\\sigma0$, where $W_t$ is a standard Brownian motion and $X_t$ is assumed to be strictly stationary with mean $0$ and finite second moments. Fix a time step $\\Delta0$ and consider the jointly Gaussian random vector $Z=(X_0, X_{\\Delta}, X_{2\\Delta})^{\\top}$. Let $\\mathcal{G}=\\sigma(X_{\\Delta})$ be the sigma-algebra generated by $X_{\\Delta}$.\n\nTasks:\n- Using only the linearity of the stochastic differential equation, properties of stochastic integrals with respect to Brownian motion, and the definition of independence of sigma-algebras, establish that $Z$ is jointly Gaussian and compute its covariance structure in terms of $a$, $\\sigma$, and $\\Delta$.\n- Define the residual $R=X_{2\\Delta}-\\mathbb{E}[X_{2\\Delta}\\mid \\mathcal{G}]$. Prove that $R$ is independent of $\\mathcal{G}$ by explicitly identifying $R$ as a measurable function of Brownian increments independent of $\\mathcal{G}$, and also by showing that for the jointly Gaussian vector $Z$, the residual is uncorrelated with $X_{\\Delta}$ and therefore independent.\n- Form the covariance matrix of $Z$ and compute the Schur complement corresponding to conditioning on the middle component $X_{\\Delta}$. Identify the $(2,2)$-entry of the conditional covariance of $(X_0, X_{2\\Delta})^{\\top}$ given $X_{\\Delta}$, and show that it equals $\\operatorname{Var}(R)$.\n\nProvide, as your final answer, the closed-form analytic expression of $\\operatorname{Var}(R)$ in terms of $a$, $\\sigma$, and $\\Delta$. Do not round your answer. The final answer must be a single simplified expression with no units.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   The stochastic differential equation (SDE) is $dX_t = -a X_t \\, dt + \\sigma \\, dW_t$.\n-   Parameters are $a  0$ and $\\sigma  0$.\n-   $W_t$ is a standard Brownian motion.\n-   $X_t$ is a strictly stationary process with mean $\\mathbb{E}[X_t] = 0$ and finite second moments.\n-   A fixed time step $\\Delta  0$ is given.\n-   A random vector $Z = (X_0, X_{\\Delta}, X_{2\\Delta})^{\\top}$ is defined.\n-   A sigma-algebra $\\mathcal{G} = \\sigma(X_{\\Delta})$ is generated by $X_{\\Delta}$.\n-   The tasks are:\n    1.  Establish that $Z$ is jointly Gaussian and compute its covariance structure.\n    2.  Define $R = X_{2\\Delta} - \\mathbb{E}[X_{2\\Delta} \\mid \\mathcal{G}]$ and prove its independence from $\\mathcal{G}$ via two specified methods.\n    3.  Compute the Schur complement of the covariance matrix of $Z$ to find the conditional covariance of $(X_0, X_{2\\Delta})^{\\top}$ given $X_{\\Delta}$, and show that its $(2,2)$-entry equals $\\operatorname{Var}(R)$.\n-   The final output must be the closed-form expression for $\\operatorname{Var}(R)$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on the Ornstein–Uhlenbeck process, a cornerstone of stochastic calculus and statistical physics. All concepts—stationarity, Gaussian processes, conditional expectation, covariance, and Schur complements—are standard and well-defined within mathematics.\n-   **Well-Posed**: The problem is clearly formulated. The assumption of stationarity for the OU process with $a0$ is standard and leads to a unique stationary distribution, ensuring a well-posed setup. The tasks are specific and lead to a single, computable quantity.\n-   **Objective**: The problem is stated in precise, objective mathematical language.\n-   **Completeness and Consistency**: The problem provides all necessary information. The SDE, the properties of the process ($X_t$), and the definitions of the quantities of interest ($Z$, $\\mathcal{G}$, $R$) are fully specified and consistent with each other.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, and internally consistent. A full solution will be provided.\n\n### Solution\nThe solution to the Ornstein-Uhlenbeck SDE $dX_t = -a X_t \\, dt + \\sigma \\, dW_t$ for $t \\ge s$ with initial condition $X_s$ is given by:\n$$X_t = e^{-a(t-s)} X_s + \\sigma \\int_{s}^{t} e^{-a(t-u)} \\, dW_u$$\nThis expression shows that $X_t$ is a linear functional of the initial state $X_s$ and the Wiener process increments.\n\n**Part 1: Joint Gaussianity and Covariance Structure**\nThe random vector is $Z = (X_0, X_{\\Delta}, X_{2\\Delta})^{\\top}$.\nThe process $X_t$ is stationary and Gaussian. Any finite collection of samples from a Gaussian process is jointly Gaussian. Therefore, $Z$ is a jointly Gaussian random vector. Its distribution is fully characterized by its mean and covariance matrix.\nThe problem states $\\mathbb{E}[X_t] = 0$, so the mean vector of $Z$ is $(0, 0, 0)^{\\top}$.\n\nTo find the covariance matrix, we first determine the stationary variance $\\operatorname{Var}(X_t)$. Let $V = \\operatorname{Var}(X_t)$. From the SDE solution starting at $t=0$:\n$\\operatorname{Var}(X_t) = \\operatorname{Var}\\left(e^{-at} X_0 + \\sigma \\int_0^t e^{-a(t-u)} \\, dW_u\\right)$.\nSince $X_0$ is part of the stationary process, it is determined by the history of the Wiener process up to time $0$ and is independent of the increments $dW_u$ for $u \\ge 0$.\n$\\operatorname{Var}(X_t) = \\operatorname{Var}(e^{-at} X_0) + \\operatorname{Var}\\left(\\sigma \\int_0^t e^{-a(t-u)} \\, dW_u\\right)$\n$V = e^{-2at} \\operatorname{Var}(X_0) + \\sigma^2 \\int_0^t e^{-2a(t-u)} du$\nBy Ito isometry, the variance of the stochastic integral is computed. For stationarity, $\\operatorname{Var}(X_0) = V$.\n$V = e^{-2at} V + \\sigma^2 \\left[ \\frac{e^{-2a(t-u)}}{2a} \\right]_0^t = e^{-2at} V + \\frac{\\sigma^2}{2a}(1 - e^{-2at})$.\n$V(1 - e^{-2at}) = \\frac{\\sigma^2}{2a}(1 - e^{-2at})$. For this to hold for all $t  0$, we must have:\n$$V = \\operatorname{Var}(X_t) = \\frac{\\sigma^2}{2a}$$\nThe autocovariance function for a lag $\\tau = t-s \\ge 0$ is:\n$\\operatorname{Cov}(X_s, X_t) = \\mathbb{E}[X_s X_t] = \\mathbb{E}\\left[X_s \\left(e^{-a(t-s)} X_s + \\sigma \\int_s^t e^{-a(t-u)} \\, dW_u\\right)\\right]$\n$= e^{-a(t-s)} \\mathbb{E}[X_s^2] + \\sigma \\mathbb{E}\\left[X_s \\int_s^t e^{-a(t-u)} \\, dW_u\\right]$\nThe second term is zero because $X_s$ is $\\mathcal{F}_s$-measurable.\n$\\operatorname{Cov}(X_s, X_t) = e^{-a(t-s)} \\operatorname{Var}(X_s) = \\frac{\\sigma^2}{2a} e^{-a(t-s)}$. For any $\\tau$, $\\operatorname{Cov}(X_t, X_{t+\\tau}) = \\frac{\\sigma^2}{2a} e^{-a|\\tau|}$.\n\nThe covariance matrix $\\Sigma$ of $Z = (X_0, X_{\\Delta}, X_{2\\Delta})^{\\top}$ has entries $\\Sigma_{ij} = \\operatorname{Cov}(X_{(i-1)\\Delta}, X_{(j-1)\\Delta})$.\n$\\Sigma_{11} = \\Sigma_{22} = \\Sigma_{33} = \\operatorname{Var}(X_0) = \\frac{\\sigma^2}{2a}$.\n$\\Sigma_{12} = \\Sigma_{21} = \\operatorname{Cov}(X_0, X_{\\Delta}) = \\frac{\\sigma^2}{2a} e^{-a\\Delta}$.\n$\\Sigma_{23} = \\Sigma_{32} = \\operatorname{Cov}(X_{\\Delta}, X_{2\\Delta}) = \\frac{\\sigma^2}{2a} e^{-a\\Delta}$.\n$\\Sigma_{13} = \\Sigma_{31} = \\operatorname{Cov}(X_0, X_{2\\Delta}) = \\frac{\\sigma^2}{2a} e^{-2a\\Delta}$.\nSo, the covariance matrix is:\n$$\\Sigma = \\frac{\\sigma^2}{2a} \\begin{pmatrix} 1  e^{-a\\Delta}  e^{-2a\\Delta} \\\\ e^{-a\\Delta}  1  e^{-a\\Delta} \\\\ e^{-2a\\Delta}  e^{-a\\Delta}  1 \\end{pmatrix}$$\n\n**Part 2: Independence of the Residual $R$**\nThe residual is $R = X_{2\\Delta} - \\mathbb{E}[X_{2\\Delta} \\mid \\mathcal{G}]$, where $\\mathcal{G} = \\sigma(X_{\\Delta})$. Since $(X_{\\Delta}, X_{2\\Delta})$ are jointly Gaussian and zero-mean, the conditional expectation is the linear projection:\n$\\mathbb{E}[X_{2\\Delta} \\mid X_{\\Delta}] = \\frac{\\operatorname{Cov}(X_{2\\Delta}, X_{\\Delta})}{\\operatorname{Var}(X_{\\Delta})} X_{\\Delta} = \\frac{\\frac{\\sigma^2}{2a}e^{-a\\Delta}}{\\frac{\\sigma^2}{2a}} X_{\\Delta} = e^{-a\\Delta} X_{\\Delta}$.\nThus, $R = X_{2\\Delta} - e^{-a\\Delta} X_{\\Delta}$.\n\nMethod (a): We express $R$ using the SDE solution:\n$X_{2\\Delta} = e^{-a\\Delta} X_{\\Delta} + \\sigma \\int_{\\Delta}^{2\\Delta} e^{-a(2\\Delta-u)} \\, dW_u$.\nSubstituting this into the expression for $R$:\n$R = \\left(e^{-a\\Delta} X_{\\Delta} + \\sigma \\int_{\\Delta}^{2\\Delta} e^{-a(2\\Delta-u)} \\, dW_u\\right) - e^{-a\\Delta} X_{\\Delta} = \\sigma \\int_{\\Delta}^{2\\Delta} e^{-a(2\\Delta-u)} \\, dW_u$.\nThis shows $R$ is a function of the Brownian increments over the interval $[\\Delta, 2\\Delta]$. The sigma-algebra $\\mathcal{G} = \\sigma(X_{\\Delta})$ is contained in $\\mathcal{F}_{\\Delta} = \\sigma(W_s : s \\le \\Delta)$, the sigma-algebra of the Brownian path up to time $\\Delta$. Since Brownian increments over disjoint intervals are independent, $R$ is independent of $\\mathcal{F}_{\\Delta}$ and therefore independent of $\\mathcal{G}$.\n\nMethod (b): For jointly Gaussian random variables, uncorrelation implies independence. $R$ and $X_{\\Delta}$ are jointly Gaussian as they are linear combinations of the components of $Z$. We check their covariance:\n$\\mathbb{E}[R] = \\mathbb{E}[X_{2\\Delta}] - e^{-a\\Delta} \\mathbb{E}[X_{\\Delta}] = 0 - 0 = 0$.\n$\\operatorname{Cov}(R, X_{\\Delta}) = \\mathbb{E}[R X_{\\Delta}] = \\mathbb{E}[(X_{2\\Delta} - e^{-a\\Delta} X_{\\Delta}) X_{\\Delta}]$\n$= \\mathbb{E}[X_{2\\Delta} X_{\\Delta}] - e^{-a\\Delta} \\mathbb{E}[X_{\\Delta}^2] = \\operatorname{Cov}(X_{2\\Delta}, X_{\\Delta}) - e^{-a\\Delta} \\operatorname{Var}(X_{\\Delta})$\n$= \\frac{\\sigma^2}{2a}e^{-a\\Delta} - e^{-a\\Delta} \\left(\\frac{\\sigma^2}{2a}\\right) = 0$.\nSince $\\operatorname{Cov}(R, X_{\\Delta}) = 0$, $R$ is independent of $X_{\\Delta}$ and hence of $\\mathcal{G} = \\sigma(X_{\\Delta})$.\n\n**Part 3: Schur Complement and $\\operatorname{Var}(R)$**\nWe partition the vector $Z$ to condition on $X_{\\Delta}$. Let the vector be $(X_{\\Delta}, (X_0, X_{2\\Delta})^\\top)^\\top$. The corresponding block covariance matrix is:\n$\\Sigma' = \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$, where\n$\\Sigma_{11} = \\operatorname{Var}(X_{\\Delta}) = \\frac{\\sigma^2}{2a}$.\n$\\Sigma_{21} = \\Sigma_{12}^\\top = \\begin{pmatrix} \\operatorname{Cov}(X_0, X_{\\Delta}) \\\\ \\operatorname{Cov}(X_{2\\Delta}, X_{\\Delta}) \\end{pmatrix} = \\frac{\\sigma^2}{2a} \\begin{pmatrix} e^{-a\\Delta} \\\\ e^{-a\\Delta} \\end{pmatrix}$.\n$\\Sigma_{22} = \\operatorname{Cov}((X_0, X_{2\\Delta})^\\top) = \\frac{\\sigma^2}{2a} \\begin{pmatrix} 1  e^{-2a\\Delta} \\\\ e^{-2a\\Delta}  1 \\end{pmatrix}$.\n\nThe conditional covariance matrix of $(X_0, X_{2\\Delta})^{\\top}$ given $X_{\\Delta}$ is the Schur complement of $\\Sigma_{11}$ in $\\Sigma'$:\n$\\operatorname{Cov}((X_0, X_{2\\Delta})^\\top | X_{\\Delta}) = \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}$.\n$\\Sigma_{11}^{-1} = \\left(\\frac{\\sigma^2}{2a}\\right)^{-1} = \\frac{2a}{\\sigma^2}$.\n$\\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} = \\frac{\\sigma^2}{2a} \\begin{pmatrix} e^{-a\\Delta} \\\\ e^{-a\\Delta} \\end{pmatrix} \\frac{2a}{\\sigma^2} \\frac{\\sigma^2}{2a} \\begin{pmatrix} e^{-a\\Delta}  e^{-a\\Delta} \\end{pmatrix}^\\top = \\frac{\\sigma^2}{2a} \\begin{pmatrix} e^{-a\\Delta} \\\\ e^{-a\\Delta} \\end{pmatrix} \\begin{pmatrix} e^{-a\\Delta}  e^{-a\\Delta} \\end{pmatrix}$\n$= \\frac{\\sigma^2}{2a} \\begin{pmatrix} e^{-2a\\Delta}  e^{-2a\\Delta} \\\\ e^{-2a\\Delta}  e^{-2a\\Delta} \\end{pmatrix}$.\n\n$\\operatorname{Cov}((X_0, X_{2\\Delta})^\\top | X_{\\Delta}) = \\frac{\\sigma^2}{2a} \\begin{pmatrix} 1  e^{-2a\\Delta} \\\\ e^{-2a\\Delta}  1 \\end{pmatrix} - \\frac{\\sigma^2}{2a} \\begin{pmatrix} e^{-2a\\Delta}  e^{-2a\\Delta} \\\\ e^{-2a\\Delta}  e^{-2a\\Delta} \\end{pmatrix} = \\frac{\\sigma^2}{2a} \\begin{pmatrix} 1 - e^{-2a\\Delta}  0 \\\\ 0  1 - e^{-2a\\Delta} \\end{pmatrix}$.\n\nThe $(2,2)$-entry of this matrix is $\\operatorname{Var}(X_{2\\Delta} | X_{\\Delta}) = \\frac{\\sigma^2}{2a}(1 - e^{-2a\\Delta})$.\nBy definition, for Gaussian variables, the conditional variance is the variance of the residual:\n$\\operatorname{Var}(X_{2\\Delta} | X_{\\Delta}) = \\mathbb{E}[(X_{2\\Delta} - \\mathbb{E}[X_{2\\Delta}|X_{\\Delta}])^2] = \\mathbb{E}[R^2] = \\operatorname{Var}(R)$.\nWe can verify this by directly computing $\\operatorname{Var}(R)$ from its integral representation:\n$\\operatorname{Var}(R) = \\operatorname{Var}\\left(\\sigma \\int_{\\Delta}^{2\\Delta} e^{-a(2\\Delta-u)} \\, dW_u\\right) = \\sigma^2 \\int_{\\Delta}^{2\\Delta} e^{-2a(2\\Delta-u)} \\, du$.\nLet $v = 2\\Delta - u$, $dv = -du$. The limits change from $u \\in [\\Delta, 2\\Delta]$ to $v \\in [\\Delta, 0]$.\n$\\operatorname{Var}(R) = \\sigma^2 \\int_{\\Delta}^{0} e^{-2av} (-dv) = \\sigma^2 \\int_{0}^{\\Delta} e^{-2av} \\, dv = \\sigma^2 \\left[\\frac{e^{-2av}}{-2a}\\right]_0^{\\Delta}$\n$= \\frac{\\sigma^2}{-2a}(e^{-2a\\Delta} - 1) = \\frac{\\sigma^2}{2a}(1 - e^{-2a\\Delta})$.\nThis confirms that the $(2,2)$-entry of the conditional covariance matrix is indeed equal to $\\operatorname{Var}(R)$.\n\nThe final expression for $\\operatorname{Var}(R)$ is $\\frac{\\sigma^2}{2a}(1 - e^{-2a\\Delta})$.", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{2a}(1 - \\exp(-2a\\Delta))}$$", "id": "2980191"}]}