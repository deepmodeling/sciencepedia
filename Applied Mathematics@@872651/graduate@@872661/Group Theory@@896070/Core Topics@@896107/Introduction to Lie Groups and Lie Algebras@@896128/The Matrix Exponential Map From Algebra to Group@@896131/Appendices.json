{"hands_on_practices": [{"introduction": "The infinite series defining the matrix exponential, $\\exp(X) = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!}$, can seem daunting to compute directly. This practice problem demonstrates a crucial simplification that occurs for a special class of matrices: nilpotent matrices. By understanding that for a nilpotent matrix $X$, the sequence of its powers terminates at zero, you will see how the infinite series elegantly truncates to a finite polynomial, making the calculation of the group element $\\exp(X)$ straightforward [@problem_id:818269].", "problem": "Consider a matrix $X$ that is an element of the Lie algebra $\\mathfrak{sl}(3, \\mathbb{R})$, which is the space of $3 \\times 3$ real matrices with a trace of zero. The matrix $X$ is parameterized by a real number $\\alpha$ and is given by:\n$$\nX(\\alpha) = \\begin{pmatrix}\n\\frac{\\alpha-2}{2}  \\frac{\\alpha+2}{2}  \\frac{2-\\alpha}{2} \\\\\n-1  1  1 \\\\\n\\frac{\\alpha}{2}  \\frac{\\alpha}{2}  -\\frac{\\alpha}{2}\n\\end{pmatrix}\n$$\nIt is a known property of $X(\\alpha)$ that it is a nilpotent matrix for any real value of $\\alpha$.\n\nThe matrix $X(\\alpha)$ is mapped to an element $G(\\alpha)$ of the corresponding Lie group $SL(3, \\mathbb{R})$ through the matrix exponential map, $G(\\alpha) = \\exp(X(\\alpha))$. The exponential of a matrix is defined by its Taylor series expansion:\n$$\n\\exp(X) = I + X + \\frac{1}{2!}X^2 + \\frac{1}{3!}X^3 + \\dots = \\sum_{k=0}^{\\infty} \\frac{1}{k!}X^k\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix.\n\nGiven that the element in the first row and second column of the resulting matrix $G(\\alpha)$ is $G_{12} = 7$, derive the value of the parameter $\\alpha$.", "solution": "The problem asks for the value of the parameter $\\alpha$ given a specific entry of the matrix $G(\\alpha) = \\exp(X(\\alpha))$.\n\nFirst, let's verify that $X(\\alpha) \\in \\mathfrak{sl}(3, \\mathbb{R})$ by checking if its trace is zero.\n$$\n\\text{Tr}(X) = X_{11} + X_{22} + X_{33} = \\frac{\\alpha-2}{2} + 1 + \\left(-\\frac{\\alpha}{2}\\right) = \\frac{\\alpha}{2} - 1 + 1 - \\frac{\\alpha}{2} = 0\n$$\nThe trace is indeed zero for any value of $\\alpha$.\n\nThe problem states that $X(\\alpha)$ is a nilpotent matrix. For a $3 \\times 3$ matrix, nilpotency implies that $X^k = 0$ for some integer $k \\le 3$. By the Cayley-Hamilton theorem, a nilpotent matrix satisfies its characteristic equation $p(\\lambda) = -\\lambda^3=0$. This implies that $p(X)=-X^3=0$, so we must have $X^3 = 0$.\n\nThe nilpotency of $X$ simplifies the infinite series for the matrix exponential. Since $X^3 = 0$, all higher powers $X^k$ for $k \\ge 3$ are also zero. The series truncates:\n$$\nG(\\alpha) = \\exp(X(\\alpha)) = I + X(\\alpha) + \\frac{1}{2}X(\\alpha)^2\n$$\n\nWe are given $G_{12} = 7$. From the expression for $G(\\alpha)$, we can write the component $G_{12}$ as:\n$$\nG_{12} = I_{12} + X_{12} + \\frac{1}{2}(X^2)_{12}\n$$\nThe components of the identity matrix are $I_{ij} = \\delta_{ij}$, so $I_{12} = 0$.\nFrom the definition of $X(\\alpha)$, we have $X_{12} = \\frac{\\alpha+2}{2}$.\n\nThe main task is to compute the $(1,2)$-entry of $X^2$. According to the rule of matrix multiplication, $(X^2)_{ij} = \\sum_{k=1}^3 X_{ik} X_{kj}$. For the $(1,2)$-entry:\n$$\n(X^2)_{12} = X_{11}X_{12} + X_{12}X_{22} + X_{13}X_{32}\n$$\nSubstituting the entries of $X(\\alpha)$:\n$$\nX_{11} = \\frac{\\alpha-2}{2}, \\quad X_{12} = \\frac{\\alpha+2}{2}, \\quad X_{13} = \\frac{2-\\alpha}{2}\n$$\n$$\nX_{22} = 1, \\quad X_{32} = \\frac{\\alpha}{2}\n$$\nNow, we compute the product:\n$$\n(X^2)_{12} = \\left(\\frac{\\alpha-2}{2}\\right)\\left(\\frac{\\alpha+2}{2}\\right) + \\left(\\frac{\\alpha+2}{2}\\right)(1) + \\left(\\frac{2-\\alpha}{2}\\right)\\left(\\frac{\\alpha}{2}\\right)\n$$\nLet's simplify this expression term by term:\n$$\n(X^2)_{12} = \\frac{(\\alpha-2)(\\alpha+2)}{4} + \\frac{\\alpha+2}{2} + \\frac{(2-\\alpha)\\alpha}{4}\n$$\n$$\n(X^2)_{12} = \\frac{\\alpha^2-4}{4} + \\frac{2(\\alpha+2)}{4} + \\frac{2\\alpha-\\alpha^2}{4}\n$$\nCombine the numerators:\n$$\n(X^2)_{12} = \\frac{(\\alpha^2-4) + (2\\alpha+4) + (2\\alpha-\\alpha^2)}{4}\n$$\n$$\n(X^2)_{12} = \\frac{\\alpha^2 - 4 + 2\\alpha + 4 + 2\\alpha - \\alpha^2}{4} = \\frac{4\\alpha}{4} = \\alpha\n$$\nSo, the $(1,2)$-entry of $X^2$ is simply $\\alpha$.\n\nNow we can assemble the expression for $G_{12}$:\n$$\nG_{12} = I_{12} + X_{12} + \\frac{1}{2}(X^2)_{12} = 0 + \\frac{\\alpha+2}{2} + \\frac{1}{2}(\\alpha)\n$$\n$$\nG_{12} = \\frac{\\alpha+2}{2} + \\frac{\\alpha}{2} = \\frac{\\alpha+2+\\alpha}{2} = \\frac{2\\alpha+2}{2} = \\alpha+1\n$$\nWe are given that $G_{12} = 7$. Therefore, we can set up the final equation for $\\alpha$:\n$$\n\\alpha+1 = 7\n$$\nSolving for $\\alpha$:\n$$\n\\alpha = 7 - 1 = 6\n$$\nThe value of the parameter $\\alpha$ is 6.", "answer": "$$\n\\boxed{6}\n$$", "id": "818269"}, {"introduction": "A core principle of Lie theory states that if two Lie algebra elements $X$ and $Y$ commute, their corresponding group elements $\\exp(X)$ and $\\exp(Y)$ also commute. This exercise invites you to explore the more subtle converse: what happens if the group elements commute, but the algebra elements do not? This problem [@problem_id:818152] provides a concrete scenario within $\\mathfrak{sl}(2, \\mathbb{R})$ to uncover the specific conditions that allow for this non-trivial commutation, deepening your understanding of the exponential map's properties.", "problem": "In the study of Lie theory, the matrix exponential map provides a bridge from a Lie algebra $\\mathfrak{g}$ to its corresponding Lie group $G$. For the special linear Lie algebra $\\mathfrak{sl}(2, \\mathbb{R})$, consisting of $2 \\times 2$ real matrices with zero trace, this map is given by $X \\mapsto \\exp(X)$, where $X \\in \\mathfrak{sl}(2, \\mathbb{R})$ and $\\exp(X) \\in SL(2, \\mathbb{R})$. A fundamental property is that if two generators $X, Y \\in \\mathfrak{g}$ commute (i.e., their Lie bracket $[X, Y] = XY - YX = 0$), then their corresponding group elements $\\exp(X)$ and $\\exp(Y)$ also commute. However, the converse is not generally true: it is possible for $\\exp(X)$ and $\\exp(Y)$ to commute even when $[X, Y] \\neq 0$.\n\nConsider two generators from $\\mathfrak{sl}(2, \\mathbb{R})$ defined as:\n$$\nX = \\begin{pmatrix} \\lambda  0 \\\\ 0  -\\lambda \\end{pmatrix}\n$$\n$$\nY(\\alpha) = \\begin{pmatrix} 0  \\alpha \\\\ \\beta  0 \\end{pmatrix}\n$$\nHere, $\\lambda$ is a non-zero real constant, $\\beta$ is a fixed negative real constant, and $\\alpha$ is a positive real parameter.\n\nDetermine the smallest positive value of the parameter $\\alpha$ for which the group elements $G_X = \\exp(X)$ and $G_Y = \\exp(Y(\\alpha))$ commute, under the condition that the generators $X$ and $Y(\\alpha)$ themselves do not commute. Express your answer as a symbolic expression in terms of $\\beta$.", "solution": "The problem requires finding the smallest positive value of $\\alpha$ such that $\\exp(X)$ and $\\exp(Y(\\alpha))$ commute, given that $[X, Y(\\alpha)] \\neq 0$.\n\n**Step 1: Compute the matrix exponentials**\n\nThe generator $X$ is a diagonal matrix. Its exponential is found by exponentiating its diagonal entries:\n$$\nG_X = \\exp(X) = \\exp\\begin{pmatrix} \\lambda  0 \\\\ 0  -\\lambda \\end{pmatrix} = \\begin{pmatrix} e^\\lambda  0 \\\\ 0  e^{-\\lambda} \\end{pmatrix}\n$$\n\nFor the generator $Y(\\alpha)$, we first compute its square:\n$$\nY^2 = \\begin{pmatrix} 0  \\alpha \\\\ \\beta  0 \\end{pmatrix}\\begin{pmatrix} 0  \\alpha \\\\ \\beta  0 \\end{pmatrix} = \\begin{pmatrix} \\alpha\\beta  0 \\\\ 0  \\alpha\\beta \\end{pmatrix} = \\alpha\\beta I\n$$\nGiven that $\\alpha > 0$ and $\\beta  0$, the product $\\alpha\\beta$ is negative. Let's define a positive real number $\\mu$ such that $\\mu^2 = -\\alpha\\beta$. This gives $\\mu = \\sqrt{-\\alpha\\beta}$. With this, we have $Y^2 = -\\mu^2 I$.\n\nThe exponential of a matrix $Y$ satisfying $Y^2 = -\\mu^2 I$ can be found using the power series expansion, which simplifies to:\n$$\n\\exp(Y) = (\\cos\\mu)I + \\frac{\\sin\\mu}{\\mu}Y\n$$\nSubstituting the matrix for $Y$, we get:\n$$\nG_Y = \\exp(Y(\\alpha)) = \\begin{pmatrix} \\cos\\mu  0 \\\\ 0  \\cos\\mu \\end{pmatrix} + \\frac{\\sin\\mu}{\\mu}\\begin{pmatrix} 0  \\alpha \\\\ \\beta  0 \\end{pmatrix} = \\begin{pmatrix} \\cos\\mu  \\frac{\\alpha\\sin\\mu}{\\mu} \\\\ \\frac{\\beta\\sin\\mu}{\\mu}  \\cos\\mu \\end{pmatrix}\n$$\n\n**Step 2: Apply the commutation condition**\n\nWe need to find when $G_X G_Y = G_Y G_X$. Let's compute both products:\n$$\nG_X G_Y = \\begin{pmatrix} e^\\lambda  0 \\\\ 0  e^{-\\lambda} \\end{pmatrix} \\begin{pmatrix} \\cos\\mu  \\frac{\\alpha\\sin\\mu}{\\mu} \\\\ \\frac{\\beta\\sin\\mu}{\\mu}  \\cos\\mu \\end{pmatrix} = \\begin{pmatrix} e^\\lambda \\cos\\mu  \\frac{e^\\lambda \\alpha\\sin\\mu}{\\mu} \\\\ \\frac{e^{-\\lambda} \\beta\\sin\\mu}{\\mu}  e^{-\\lambda} \\cos\\mu \\end{pmatrix}\n$$\n$$\nG_Y G_X = \\begin{pmatrix} \\cos\\mu  \\frac{\\alpha\\sin\\mu}{\\mu} \\\\ \\frac{\\beta\\sin\\mu}{\\mu}  \\cos\\mu \\end{pmatrix} \\begin{pmatrix} e^\\lambda  0 \\\\ 0  e^{-\\lambda} \\end{pmatrix} = \\begin{pmatrix} e^\\lambda \\cos\\mu  \\frac{e^{-\\lambda} \\alpha\\sin\\mu}{\\mu} \\\\ \\frac{e^\\lambda \\beta\\sin\\mu}{\\mu}  e^{-\\lambda} \\cos\\mu \\end{pmatrix}\n$$\nFor these two matrices to be equal, their off-diagonal elements must match:\n$$\n\\frac{e^\\lambda \\alpha\\sin\\mu}{\\mu} = \\frac{e^{-\\lambda} \\alpha\\sin\\mu}{\\mu} \\implies (e^\\lambda - e^{-\\lambda})\\frac{\\alpha\\sin\\mu}{\\mu} = 0\n$$\nThe problem states that $\\lambda$ is non-zero, so $e^\\lambda - e^{-\\lambda} \\neq 0$. Also, $\\alpha > 0$. Therefore, for this equation to hold, we must have $\\sin\\mu = 0$.\n\n**Step 3: Solve for $\\alpha$**\n\nThe condition $\\sin\\mu = 0$ implies that $\\mu = n\\pi$ for some integer $n$. Since $\\mu = \\sqrt{-\\alpha\\beta}$ must be positive (as it's a square root of a positive number), we must have $n \\in \\{1, 2, 3, \\dots\\}$.\n\nWe can now solve for $\\alpha$:\n$$\n\\mu^2 = n^2\\pi^2\n$$\n$$\n-\\alpha\\beta = n^2\\pi^2\n$$\n$$\n\\alpha = -\\frac{n^2\\pi^2}{\\beta}\n$$\nWe are looking for the smallest positive value of $\\alpha$. Since $\\beta  0$, the expression is positive for all $n \\ge 1$. The smallest value occurs for the smallest possible value of $n$, which is $n=1$.\n\nThus, the smallest positive value of $\\alpha$ is:\n$$\n\\alpha = -\\frac{\\pi^2}{\\beta}\n$$\nThis fulfills the condition, and we can check that for this value, the original generators $X$ and $Y$ do not commute, as required.", "answer": "$$\\boxed{-\\frac{\\pi^2}{\\beta}}$$", "id": "818152"}, {"introduction": "Calculating the exponential of a large, dense matrix can be computationally prohibitive. A powerful strategy in Lie theory is to find a change of basis that simplifies the Lie algebra element itself. This advanced problem [@problem_id:818294] challenges you to apply this idea to an element of $\\mathfrak{so}(4)$, demonstrating how a similarity transformation, using the property $\\exp(PXP^{-1}) = P\\exp(X)P^{-1}$, can decompose a complex generator into a sum of simpler, commuting generators whose exponentials are far easier to compute.", "problem": "In the study of Lie groups and their algebras, the exponential map provides a bridge from the algebra to the group. For matrix Lie groups, the Lie algebra $\\mathfrak{g}$ is a vector space of matrices, and the Lie group $G$ consists of invertible matrices. The map is the standard matrix exponential, $R = \\exp(X)$ for $X \\in \\mathfrak{g}$ and $R \\in G$.\n\nConsider the Lie group $SO(4)$, the group of $4 \\times 4$ real orthogonal matrices with determinant 1. Its Lie algebra, denoted $\\mathfrak{so}(4)$, consists of all $4 \\times 4$ real skew-symmetric matrices.\n\nLet a specific element $X$ of the Lie algebra $\\mathfrak{so}(4)$ be given by the matrix:\n$$\nX = \\begin{pmatrix}\n0  -(\\cos\\phi)\\alpha  -(\\sin\\phi)\\beta  0 \\\\\n(\\cos\\phi)\\alpha  0  0  (\\sin\\phi)\\alpha \\\\\n(\\sin\\phi)\\beta  0  0  -(\\cos\\phi)\\beta \\\\\n0  -(\\sin\\phi)\\alpha  (\\cos\\phi)\\beta  0\n\\end{pmatrix}\n$$\nwhere $\\alpha$, $\\beta$, and $\\phi$ are real parameters.\n\nDerive the matrix element $R_{11}$ of the resulting group element $R = \\exp(X)$.", "solution": "The problem asks for the $(1,1)$ entry of the matrix $R = \\exp(X)$. A direct computation of the matrix exponential is complex. A more effective strategy is to simplify $X$ via a similarity transformation. The eigenvalues of a skew-symmetric matrix are imaginary, and for $X \\in \\mathfrak{so}(4)$, they come in pairs $\\pm i\\theta_1, \\pm i\\theta_2$. It can be shown that the eigenvalues of the given matrix $X$ are $\\pm i\\alpha$ and $\\pm i\\beta$. This suggests that $X$ is related to a simpler, block-diagonal matrix representing two independent rotations.\n\nLet's define a block-diagonal generator $X_0$ in $\\mathfrak{so}(4)$ corresponding to rotations with rates $\\alpha$ and $\\beta$ in the $x_1-x_2$ and $x_3-x_4$ planes, respectively:\n$$\nX_0 = \\begin{pmatrix}\n0  -\\alpha  0  0 \\\\\n\\alpha  0  0  0 \\\\\n0  0  0  -\\beta \\\\\n0  0  \\beta  0\n\\end{pmatrix} = \\alpha L_{12} - \\beta L_{34}\n$$\nThe given matrix $X$ can be interpreted as a version of $X_0$ viewed in a rotated basis. We can find an orthogonal matrix $P$ such that $X = P X_0 P^T$. The key property of the matrix exponential is $\\exp(PMP^{-1}) = P\\exp(M)P^{-1}$. This allows us to write:\n$$\nR = \\exp(X) = \\exp(P X_0 P^T) = P \\exp(X_0) P^T\n$$\nSince the generators $L_{12}$ and $L_{34}$ commute, the exponential of their sum is the product of their exponentials:\n$$\n\\exp(X_0) = \\exp(\\alpha L_{12} - \\beta L_{34}) = \\exp(\\alpha L_{12}) \\exp(-\\beta L_{34})\n$$\nThe exponential of $\\alpha L_{12}$ is a rotation by $\\alpha$ in the $x_1-x_2$ plane, and the exponential of $-\\beta L_{34}$ is a rotation by $-\\beta$ in the $x_3-x_4$ plane. Their product gives the block-diagonal rotation matrix:\n$$\nR_0 = \\exp(X_0) = \\begin{pmatrix}\n\\cos\\alpha  -\\sin\\alpha  0  0 \\\\\n\\sin\\alpha  \\cos\\alpha  0  0 \\\\\n0  0  \\cos(-\\beta)  -\\sin(-\\beta) \\\\\n0  0  \\sin(-\\beta)  \\cos(-\\beta)\n\\end{pmatrix} = \\begin{pmatrix}\n\\cos\\alpha  -\\sin\\alpha  0  0 \\\\\n\\sin\\alpha  \\cos\\alpha  0  0 \\\\\n0  0  \\cos\\beta  \\sin\\beta \\\\\n0  0  -\\sin\\beta  \\cos\\beta\n\\end{pmatrix}\n$$\nThe matrix $X$ in the problem can be constructed using a rotation matrix $P$ that rotates by $\\phi$ in the 1-4 plane:\n$$\nP = \\begin{pmatrix}\n\\cos\\phi  0  0  \\sin\\phi \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n-\\sin\\phi  0  0  \\cos\\phi\n\\end{pmatrix}\n$$\nNow we compute $R = P R_0 P^T$. We only need the element $R_{11}$:\n$$\nR_{11} = (P R_0 P^T)_{11} = \\sum_{k,l=1}^4 P_{1k} (R_0)_{kl} (P^T)_{l1}\n$$\nSince $P$ is orthogonal, $P^T_{l1} = P_{1l}$. The expression becomes:\n$$\nR_{11} = \\sum_{k,l=1}^4 P_{1k} P_{1l} (R_0)_{kl}\n$$\nThe first row of $P$ is $(\\cos\\phi, 0, 0, \\sin\\phi)$. This means only terms where $k, l \\in \\{1, 4\\}$ will be non-zero.\n\\begin{align*}\nR_{11} = P_{11}P_{11}(R_0)_{11} + P_{11}P_{14}(R_0)_{14} + P_{14}P_{11}(R_0)_{41} + P_{14}P_{14}(R_0)_{44} \\\\\n= (\\cos^2\\phi)(R_0)_{11} + (\\cos\\phi\\sin\\phi)(R_0)_{14} + (\\sin\\phi\\cos\\phi)(R_0)_{41} + (\\sin^2\\phi)(R_0)_{44}\n\\end{align*}\nFrom the matrix $R_0$, we have $(R_0)_{11} = \\cos\\alpha$, $(R_0)_{44} = \\cos\\beta$, and the off-diagonal block elements $(R_0)_{14} = (R_0)_{41} = 0$.\nSubstituting these values:\n$$\nR_{11} = (\\cos^2\\phi) (\\cos\\alpha) + (\\sin^2\\phi) (\\cos\\beta)\n$$\nThis gives the final expression for the matrix element $R_{11}$.", "answer": "$$\n\\boxed{\\cos^2\\phi \\cos\\alpha + \\sin^2\\phi \\cos\\beta}\n$$", "id": "818294"}]}