{"hands_on_practices": [{"introduction": "This first practice explores the critical boundaries of the Lyapunov indirect method. By analyzing a simple scalar system where the linearization at the equilibrium has a zero eigenvalue, you will see firsthand why the method can be inconclusive [@problem_id:2721924]. This exercise highlights the necessity of the direct Lyapunov method to resolve stability in such critical cases and demonstrates that the behavior of nonlinear terms can dominate when the linear approximation provides no information.", "problem": "Consider the scalar nonlinear system defined by the differential equation $\\dot{x} = -x^{3}$ with state $x \\in \\mathbb{R}$. The origin $x=0$ is an equilibrium. Work from the fundamental definitions of linearization, Lyapunov stability, and the Lyapunov direct method (also called the second method) to analyze stability as follows:\n\n- First, compute the Jacobian of the vector field at the origin and identify the linearized system. Use the definition of Lyapunov’s indirect method to explain why the linearization is inconclusive about the stability of the origin.\n\n- Next, using a smooth, radially unbounded, positive definite function built from first principles (that is, constructed from the state and its energy-like properties), carry out a direct Lyapunov analysis to establish global asymptotic stability of the origin. Explicitly compute the derivative of your Lyapunov function along trajectories, and reduce it to a closed differential equation in the Lyapunov function alone.\n\n- Finally, quantify the convergence by solving the resulting scalar differential equation to obtain the exact hitting time $T$ needed for a trajectory initialized at $x(0) = x_{0}$ with $|x_{0}| > r > 0$ to enter the closed ball $\\{x \\in \\mathbb{R}: |x| \\le r\\}$. Provide the exact analytic expression of $T$ as a function of $r$ and $x_{0}$.\n\nYour final answer must be a single closed-form expression for $T$. No numerical rounding is required and no units are needed.", "solution": "The problem is submitted for analysis. It is first necessary to validate the problem statement. The givens are a scalar nonlinear system described by the ordinary differential equation $\\dot{x} = -x^{3}$ where the state is $x \\in \\mathbb{R}$. The equilibrium point under investigation is the origin, $x=0$. The task is to analyze stability using Lyapunov's indirect and direct methods, and then to compute a specific hitting time $T$ for a trajectory initialized at $x(0) = x_{0}$ to reach the closed ball $\\{x \\in \\mathbb{R}: |x| \\le r\\}$, given $|x_{0}| > r > 0$. The problem is scientifically grounded in control theory, is mathematically well-posed, internally consistent, and contains sufficient information for a unique solution. The problem is therefore deemed valid and a solution may proceed.\n\nFirst, we analyze the stability using Lyapunov's indirect method, also known as stability by linearization. The nonlinear system is given by $\\dot{x} = f(x)$, where the vector field is $f(x) = -x^{3}$. To linearize the system around the equilibrium point $x_e = 0$, we compute the Jacobian matrix of $f(x)$ evaluated at this point. In this scalar case, the Jacobian is the first derivative:\n$$\nJ(x) = \\frac{df}{dx} = \\frac{d}{dx}(-x^{3}) = -3x^{2}\n$$\nThe matrix $A$ of the linearized system $\\dot{\\delta x} = A \\delta x$ is obtained by evaluating the Jacobian at the equilibrium point $x_e=0$:\n$$\nA = J(0) = -3(0)^{2} = 0\n$$\nThe linearized system is therefore $\\dot{\\delta x} = 0$. The eigenvalue of the matrix $A$ is $\\lambda = 0$. Lyapunov's indirect method states that if the real part of any eigenvalue of the linearization matrix $A$ is zero, then the method is inconclusive. One cannot determine the stability properties (stability, asymptotic stability, or instability) of the nonlinear system's equilibrium point from its linearization. Thus, this first analysis fails to provide a conclusion.\n\nNext, we proceed with Lyapunov's direct method. We must construct a suitable Lyapunov candidate function $V(x)$. A standard choice, motivated by the concept of energy, is a quadratic function of the state. Let us propose the function:\n$$\nV(x) = \\frac{1}{2}x^{2}\n$$\nThis function must be checked for the required properties. It is continuously differentiable. It is positive definite, as $V(0) = 0$ and $V(x) > 0$ for all $x \\neq 0$. It is also radially unbounded, since $V(x) \\to \\infty$ as $|x| \\to \\infty$. Now, we compute the time derivative of $V(x)$ along the trajectories of the system $\\dot{x} = -x^{3}$:\n$$\n\\dot{V}(x) = \\frac{dV}{dt} = \\frac{\\partial V}{\\partial x} \\frac{dx}{dt} = \\frac{\\partial V}{\\partial x} \\dot{x}\n$$\nThe partial derivative is $\\frac{\\partial V}{\\partial x} = x$. Substituting this and the system dynamics, we find:\n$$\n\\dot{V}(x) = (x)(-x^{3}) = -x^{4}\n$$\nThe time derivative $\\dot{V}(x)$ is negative definite, because $\\dot{V}(0) = 0$ and $\\dot{V}(x) < 0$ for all $x \\neq 0$. Since we have found a positive definite, radially unbounded Lyapunov function $V(x)$ for which its time derivative $\\dot{V}(x)$ is negative definite, we can conclude that the equilibrium point $x=0$ is globally asymptotically stable.\n\nThe problem requires a differential equation in terms of the Lyapunov function $V$ alone. We have the relations $V = \\frac{1}{2}x^{2}$ and $\\dot{V} = -x^{4}$. From the first relation, we can write $x^{2} = 2V$. Substituting this into the expression for $\\dot{V}$, we obtain:\n$$\n\\dot{V} = -(x^{2})^{2} = -(2V)^{2} = -4V^{2}\n$$\nThis is the required scalar differential equation for $V(t)$.\n\nFinally, we must quantify the convergence time $T$. We are asked to find the time it takes for a trajectory starting at $x(0) = x_{0}$ to enter the set $|x| \\le r$, where it is given that $|x_{0}| > r > 0$. The initial condition for the Lyapunov function is $V(0) = V_{0} = \\frac{1}{2}x_{0}^{2}$. The trajectory enters the specified region at time $T$, at which point $|x(T)| = r$. The value of the Lyapunov function at this time is $V(T) = V_{r} = \\frac{1}{2}r^{2}$. We solve the differential equation $\\frac{dV}{dt} = -4V^{2}$ by separation of variables:\n$$\n\\frac{dV}{V^{2}} = -4 dt\n$$\nWe integrate this equation from the initial time $t=0$ to the final time $t=T$:\n$$\n\\int_{V_{0}}^{V_{r}} V^{-2} dV = \\int_{0}^{T} -4 dt\n$$\nPerforming the integration yields:\n$$\n\\left[ -V^{-1} \\right]_{V_{0}}^{V_{r}} = \\left[ -4t \\right]_{0}^{T}\n$$\n$$\n-\\frac{1}{V_{r}} - \\left(-\\frac{1}{V_{0}}\\right) = -4T - 0\n$$\n$$\n\\frac{1}{V_{0}} - \\frac{1}{V_{r}} = -4T\n$$\nSolving for the hitting time $T$:\n$$\nT = \\frac{1}{4} \\left( \\frac{1}{V_{r}} - \\frac{1}{V_{0}} \\right)\n$$\nNow, substitute the expressions for $V_{r}$ and $V_{0}$ in terms of $r$ and $x_{0}$:\n$$\nT = \\frac{1}{4} \\left( \\frac{1}{\\frac{1}{2}r^{2}} - \\frac{1}{\\frac{1}{2}x_{0}^{2}} \\right)\n$$\n$$\nT = \\frac{1}{4} \\left( \\frac{2}{r^{2}} - \\frac{2}{x_{0}^{2}} \\right)\n$$\nSimplifying this expression gives the final analytical result for the hitting time:\n$$\nT = \\frac{1}{2} \\left( \\frac{1}{r^{2}} - \\frac{1}{x_{0}^{2}} \\right)\n$$\nThis is the exact time required for the state to travel from an initial value $x_{0}$ to the boundary of the region defined by $|x| \\le r$. Since $|x_0| > r > 0$, it follows that $x_0^2 > r^2$, and thus $\\frac{1}{r^2} > \\frac{1}{x_0^2}$, ensuring that $T>0$, as required for a physical time interval.", "answer": "$$\n\\boxed{\\frac{1}{2}\\left(\\frac{1}{r^2} - \\frac{1}{x_0^2}\\right)}\n$$", "id": "2721924"}, {"introduction": "Building on the principles of linearization, this exercise shows how to move beyond a simple declaration of local stability to a quantitative result. You will construct a quadratic Lyapunov function for a system with a stable linearization and use it to compute an explicit inner estimate of the domain of attraction [@problem_id:2721953]. This practice is fundamental for understanding how to determine a safe operating region around a stable equilibrium.", "problem": "Consider the nonlinear autonomous system in two dimensions\n$$\n\\dot{x} = f(x) := A x + \\begin{bmatrix} x_{1}^{2} \\\\ 0 \\end{bmatrix}, \\quad A = \\begin{bmatrix} -2 & 0 \\\\ 0 & -1 \\end{bmatrix},\n$$\nwith equilibrium at the origin. Using Lyapunov's indirect method (stability via linearization) as the foundational justification for local asymptotic stability, construct a quadratic Lyapunov function of the form\n$$\nV(x) = x^{\\top} P x, \\quad P = P^{\\top} \\succ 0,\n$$\nchosen so that it certifies the stability of the linearized dynamics and can be used to quantify an inner estimate of the domain of attraction for the full nonlinear system. Define the quadratic sublevel sets\n$$\n\\Omega_{\\rho} := \\{ x \\in \\mathbb{R}^{2} : V(x) \\le \\rho \\}.\n$$\nDetermine the largest scalar value $\\rho^{\\star} > 0$ such that $\\dot{V}(x) \\le 0$ for all $x \\in \\Omega_{\\rho}$ whenever $0 < \\rho \\le \\rho^{\\star}$. Report the exact value of $\\rho^{\\star}$ as your final answer. You do not need to report the set $\\Omega_{\\rho^{\\star}}$ itself. The final answer must be a single real number.", "solution": "The provided problem is a standard exercise in the Lyapunov stability analysis of nonlinear systems. It is scientifically sound, self-contained, mathematically well-posed, and free of any ambiguities or contradictions. All provided information is sufficient to determine a unique, meaningful solution. Therefore, the problem is deemed valid and we shall proceed with its formal resolution.\n\nThe system dynamics are given by\n$$\n\\dot{x} = f(x) = A x + g(x)\n$$\nwhere $x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix}$, $A = \\begin{bmatrix} -2 & 0 \\\\ 0 & -1 \\end{bmatrix}$, and $g(x) = \\begin{bmatrix} x_{1}^{2} \\\\ 0 \\end{bmatrix}$ is the nonlinear part. The equilibrium is at the origin, $x_e = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\nFirst, we verify the stability of the linearized system $\\dot{z} = A z$. The matrix $A$ is diagonal, so its eigenvalues are its diagonal entries, $\\lambda_1 = -2$ and $\\lambda_2 = -1$. Since both eigenvalues have strictly negative real parts, the linearized system is asymptotically stable. By Lyapunov's indirect method, this implies that the origin is a locally asymptotically stable equilibrium for the original nonlinear system.\n\nTo construct a quadratic Lyapunov function $V(x) = x^{\\top} P x$ that certifies the stability of the linear system, we must solve the continuous-time Lyapunov equation:\n$$\nA^{\\top} P + P A = -Q\n$$\nwhere $P$ is a symmetric positive definite matrix ($P = P^{\\top} \\succ 0$) and $Q$ is an arbitrary symmetric positive definite matrix. A standard and convenient choice for $Q$ is the identity matrix, $Q=I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$.\nLet $P = \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix}$. Since $A$ is diagonal, we can anticipate that $P$ will also be diagonal.\nThe equation becomes:\n$$\n\\begin{bmatrix} -2 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} + \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} \\begin{bmatrix} -2 & 0 \\\\ 0 & -1 \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}\n$$\nPerforming the matrix multiplication yields:\n$$\n\\begin{bmatrix} -4p_{11} & -3p_{12} \\\\ -3p_{12} & -2p_{22} \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}\n$$\nEquating the matrix elements gives the following system of linear equations:\n\\begin{itemize}\n    \\item $-4p_{11} = -1 \\implies p_{11} = \\frac{1}{4}$\n    \\item $-3p_{12} = 0 \\implies p_{12} = 0$\n    \\item $-2p_{22} = -1 \\implies p_{22} = \\frac{1}{2}$\n\\end{itemize}\nThus, the matrix $P$ is found to be $P = \\begin{bmatrix} \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix}$. This matrix is diagonal with positive entries, so it is indeed symmetric and positive definite. The corresponding Lyapunov function is:\n$$\nV(x) = x^{\\top} P x = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\frac{1}{4}x_{1}^{2} + \\frac{1}{2}x_{2}^{2}\n$$\nNext, we compute the time derivative of $V(x)$ along the trajectories of the full nonlinear system:\n$$\n\\dot{V}(x) = (\\nabla V)^{\\top} \\dot{x} = (\\nabla V)^{\\top} (A x + g(x))\n$$\nWe have $\\nabla V(x) = 2 P x = 2 \\begin{bmatrix} \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2}x_1 \\\\ x_2 \\end{bmatrix}$.\nSubstituting this and the expression for $\\dot{x}$ gives:\n$$\n\\dot{V}(x) = \\begin{bmatrix} \\frac{1}{2}x_1 & x_2 \\end{bmatrix} \\left( \\begin{bmatrix} -2 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\begin{bmatrix} x_1^2 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} \\frac{1}{2}x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} -2x_1 + x_1^2 \\\\ -x_2 \\end{bmatrix}\n$$\n$$\n\\dot{V}(x) = \\frac{1}{2}x_1(-2x_1 + x_1^2) + x_2(-x_2) = -x_1^2 + \\frac{1}{2}x_1^3 - x_2^2\n$$\nAlternatively, using the Lyapunov equation:\n$\\dot{V}(x) = x^{\\top}(A^{\\top}P+PA)x + 2g(x)^{\\top}Px = -x^{\\top}Qx + 2g(x)^{\\top}Px$.\nWith $Q=I$, this is $-x_1^2 - x_2^2 + 2\\begin{bmatrix} x_1^2 & 0 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{4} & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = -x_1^2 - x_2^2 + 2( \\frac{1}{4}x_1^3 ) = -x_1^2 - x_2^2 + \\frac{1}{2}x_1^3$. The result is consistent.\n\nWe need to find the largest $\\rho^{\\star} > 0$ such that for any $\\rho \\in (0, \\rho^{\\star}]$, the condition $V(x) \\le \\rho$ implies $\\dot{V}(x) \\le 0$. This is equivalent to finding the largest $\\rho^{\\star}$ such that the sublevel set $\\Omega_{\\rho^{\\star}} = \\{x \\in \\mathbb{R}^2 : V(x) \\le \\rho^{\\star}\\}$ is contained entirely within the region where $\\dot{V}(x) \\le 0$.\n\nLet us analyze the condition $\\dot{V}(x) \\le 0$:\n$$\n-x_1^2 - x_2^2 + \\frac{1}{2}x_1^3 \\le 0\n$$\n$$\n\\frac{1}{2}x_1^3 \\le x_1^2 + x_2^2\n$$\nThis inequality can be rewritten as:\n$$\n-x_1^2(1 - \\frac{1}{2}x_1) - x_2^2 \\le 0\n$$\nWe examine the term $(1 - \\frac{1}{2}x_1)$.\nIf $x_1 \\le 2$, then $1 - \\frac{1}{2}x_1 \\ge 0$. In this case, $-x_1^2(1 - \\frac{1}{2}x_1) \\le 0$. Since $-x_2^2 \\le 0$, their sum is always less than or equal to zero.\nTherefore, the condition $\\dot{V}(x) \\le 0$ is satisfied for all $x$ such that $x_1 \\le 2$.\n\nThe problem now reduces to finding the largest $\\rho^{\\star} > 0$ for which the sublevel set (an ellipse) $\\Omega_{\\rho^{\\star}} = \\{x : \\frac{1}{4}x_1^2 + \\frac{1}{2}x_2^2 \\le \\rho^{\\star}\\}$ is contained in the half-plane $\\{x : x_1 \\le 2\\}$. For this inclusion to hold, the maximum value of $x_1$ within the set $\\Omega_{\\rho^{\\star}}$ must be less than or equal to $2$.\n\nThe boundary of the sublevel set is the ellipse $\\frac{1}{4}x_1^2 + \\frac{1}{2}x_2^2 = \\rho^{\\star}$. The maximum value of $x_1$ on this ellipse occurs when $x_2=0$. Substituting $x_2=0$ into the equation of the ellipse, we find:\n$$\n\\frac{1}{4}x_1^2 = \\rho^{\\star} \\implies x_1^2 = 4\\rho^{\\star} \\implies x_1 = \\pm 2\\sqrt{\\rho^{\\star}}\n$$\nThe maximum value of $x_1$ is $x_{1,\\text{max}} = 2\\sqrt{\\rho^{\\star}}$.\nWe enforce the constraint $x_{1,\\text{max}} \\le 2$:\n$$\n2\\sqrt{\\rho^{\\star}} \\le 2 \\implies \\sqrt{\\rho^{\\star}} \\le 1\n$$\nSince $\\rho^{\\star} > 0$, this implies $\\rho^{\\star} \\le 1$.\nThe largest value of $\\rho^{\\star}$ satisfying this condition is $\\rho^{\\star} = 1$.\n\nFor $\\rho^{\\star}=1$, every point $x \\in \\Omega_1$ satisfies $V(x) \\le 1$, which implies $\\frac{1}{4}x_1^2 \\le \\frac{1}{4}x_1^2 + \\frac{1}{2}x_2^2 \\le 1$, so $x_1^2 \\le 4$, or $-2 \\le x_1 \\le 2$. As established, for any $x_1$ in this range, $\\dot{V}(x) \\le 0$. Thus, $\\Omega_1$ is an invariant set and an estimate of the domain of attraction. The largest such scalar is $\\rho^{\\star} = 1$.", "answer": "$$\\boxed{1}$$", "id": "2721953"}, {"introduction": "This final practice bridges the gap between analytical theory and computational validation, a crucial skill in modern control engineering. You will first derive a theoretical bound for the domain of attraction using the Lyapunov equation, and then design a numerical experiment to test the conservativeness of this analytical result [@problem_id:2721989]. This hands-on simulation task provides valuable intuition about the practical size of the stability region versus the mathematically provable estimate.", "problem": "Consider the autonomous nonlinear ordinary differential equation (ODE) in continuous time given by $\\dot{x} = f(x)$ for the state $x \\in \\mathbb{R}^2$, where\n$$\nf(x) = A x + g(x), \\quad A = \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix},\n$$\nand\n$$\ng(x) = \\begin{bmatrix}\nc_1 x_1^3 + c_2 x_1 x_2^2 \\\\\nc_3 x_1^2 x_2 + c_4 x_2^3\n\\end{bmatrix}.\n$$\nWork from the following foundational base in control theory:\n- The Jacobian linearization of $\\dot{x} = f(x)$ at the equilibrium $x = 0$ is given by $A = \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=0}$, and Lyapunov's indirect method states: if $A$ is Hurwitz (all eigenvalues have strictly negative real parts), then the equilibrium is locally asymptotically stable.\n- For any symmetric positive definite matrix $Q \\in \\mathbb{S}_{++}^2$, the continuous-time Lyapunov equation $A^\\top P + P A = -Q$ has a unique solution $P \\in \\mathbb{S}_{++}^2$ when $A$ is Hurwitz.\n- A quadratic Lyapunov function candidate can be taken as $V(x) = x^\\top P x$, and along trajectories $\\dot{V}(x) = \\nabla V(x)^\\top f(x)$.\n- Use Euclidean norms and the Cauchy–Schwarz inequality, together with elementary bounds such as $\\lvert x_i \\rvert \\le \\lVert x \\rVert_2$ for $i \\in \\{1,2\\}$.\n\nYour task is to formalize and implement a numerical experiment that estimates a local domain of attraction around the origin for the above system by simulating trajectories that start on the boundary of quadratic level sets. You must relate the numerical outcomes to a theoretically derived sufficient bound that guarantees negativity of $\\dot{V}$ within a quadratic level set.\n\nCarry out the following, without using any shortcut formulas given in advance.\n\n- Show that $A$ is Hurwitz and choose $Q = I_2$, then compute $P \\succ 0$ from $A^\\top P + P A = -Q$.\n- Define $V(x) = x^\\top P x$. For the nonlinear term $g(x)$, derive a norm bound of the form $\\lVert g(x) \\rVert_2 \\le c \\lVert x \\rVert_2^3$ with a computable constant $c$ expressed only in terms of the coefficients $c_1, c_2, c_3, c_4$.\n- Using only the above foundational tools, derive a sufficient condition that guarantees $\\dot{V}(x) &lt; 0$ for all $x$ in the quadratic sublevel set $\\{x \\in \\mathbb{R}^2 \\mid V(x) \\le \\rho\\}$. Express your condition as an explicit upper bound $\\rho_\\star$ such that if $\\rho \\in (0,\\rho_\\star)$ is chosen, then $\\dot{V}(x) &lt; 0$ holds for all $x$ in that sublevel set.\n- Design a numerical experiment that, for a given $\\rho$, samples initial conditions on the ellipse $\\{x \\in \\mathbb{R}^2 \\mid V(x) = \\rho\\}$, simulates the trajectories under $\\dot{x} = f(x)$ for a fixed time horizon, and labels a trial as successful if the state approaches the origin numerically. Use the following specifications:\n  - Use $Q = I_2$.\n  - Use a fixed time horizon $T = 20$.\n  - Use $N = 24$ initial conditions, taken uniformly in angle on the ellipse $V(x) = \\rho$ via an exact transformation from the Euclidean circle to that ellipse.\n  - Use absolute tolerance $\\mathrm{atol} = 10^{-12}$ and relative tolerance $\\mathrm{rtol} = 10^{-9}$ in the numerical integrator.\n  - Declare numerical convergence if the Euclidean norm satisfies $\\lVert x(T) \\rVert_2 \\le 10^{-3}$ or if an event detects crossing into the ball of radius $10^{-3}$ around the origin at any earlier time.\n  - Declare numerical divergence if an event detects that the Euclidean norm exceeds $10$ at any time.\n- For each test case below, first compute $\\rho_\\star$ from your derived sufficient condition. Then set $\\rho = s \\rho_\\star$ using the scale factor $s$ from the test case. Run the simulation experiment described above using this $\\rho$. Return a boolean for each test case equal to $\\mathrm{True}$ if and only if all simulated trajectories from the $N$ initial conditions converge numerically as defined above; otherwise return $\\mathrm{False}$.\n\nUse the following test suite of parameter values:\n- Test case $1$: $(c_1,c_2,c_3,c_4) = (0.5,0.5,0.5,0.5)$ and $s = 0.5$.\n- Test case $2$: $(c_1,c_2,c_3,c_4) = (1,1,1,1)$ and $s = 0.99$.\n- Test case $3$: $(c_1,c_2,c_3,c_4) = (2,2,2,2)$ and $s = 20$.\n\nAll quantities are nondimensional and unit-free.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[r_1,r_2,r_3]$ where each $r_i$ is a boolean. No additional text must be printed.", "solution": "The problem statement is scrutinized and found to be valid. It is scientifically grounded in the principles of Lyapunov stability theory, well-posed with all necessary information provided, and objective in its formulation. No contradictions, ambiguities, or factual unsoundness are present. We shall proceed with the derivation and solution.\n\nThe problem demands a two-part analysis: first, a theoretical estimation of a region of attraction for a given nonlinear system, and second, a numerical experiment to test the boundaries of this theoretical estimate.\n\nThe system dynamics are given by $\\dot{x} = f(x)$, where $x \\in \\mathbb{R}^2$, with $f(x) = A x + g(x)$,\n$$\nA = \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix}, \\quad g(x) = \\begin{bmatrix}\nc_1 x_1^3 + c_2 x_1 x_2^2 \\\\\nc_3 x_1^2 x_2 + c_4 x_2^3\n\\end{bmatrix}.\n$$\nThe origin $x=0$ is an equilibrium point, as $f(0) = 0$. The Jacobian of $f(x)$ at the origin is the matrix $A$.\n\nFirst, we establish the local asymptotic stability of the equilibrium point for the linearized system $\\dot{x} = Ax$. This is determined by the eigenvalues of $A$. The characteristic equation is $\\det(A - \\lambda I) = 0$, which is\n$$\n\\det \\begin{bmatrix} -\\lambda & 1 \\\\ -2 & -3-\\lambda \\end{bmatrix} = (-\\lambda)(-3-\\lambda) - (1)(-2) = \\lambda^2 + 3\\lambda + 2 = 0.\n$$\nThis factors as $(\\lambda+1)(\\lambda+2) = 0$, yielding eigenvalues $\\lambda_1 = -1$ and $\\lambda_2 = -2$. Since both eigenvalues have strictly negative real parts, the matrix $A$ is Hurwitz. By Lyapunov's indirect method, the equilibrium $x=0$ of the nonlinear system is locally asymptotically stable.\n\nNext, we construct a quadratic Lyapunov function $V(x) = x^\\top P x$ to analyze the domain of attraction. We must find the symmetric positive definite matrix $P$ by solving the continuous-time Lyapunov equation $A^\\top P + P A = -Q$. The problem specifies using $Q=I_2$, the $2 \\times 2$ identity matrix. Let $P = \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix}$. The equation becomes:\n$$\n\\begin{bmatrix} 0 & -2 \\\\ 1 & -3 \\end{bmatrix} \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} + \\begin{bmatrix} p_{11} & p_{12} \\\\ p_{12} & p_{22} \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ -2 & -3 \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n$$\nPerforming the matrix multiplication and summation yields:\n$$\n\\begin{bmatrix} -4p_{12} & p_{11}-3p_{12}-2p_{22} \\\\ p_{11}-3p_{12}-2p_{22} & 2p_{12}-6p_{22} \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n$$\nEquating the elements gives a system of linear equations for the entries of $P$:\n$1. \\quad -4p_{12} = -1 \\implies p_{12} = \\frac{1}{4}$.\n$2. \\quad 2p_{12}-6p_{22} = -1 \\implies 2(\\frac{1}{4}) - 6p_{22} = -1 \\implies \\frac{1}{2} - 6p_{22} = -1 \\implies p_{22} = \\frac{1}{4}$.\n$3. \\quad p_{11}-3p_{12}-2p_{22} = 0 \\implies p_{11} = 3(\\frac{1}{4}) + 2(\\frac{1}{4}) = \\frac{5}{4}$.\nThus, the solution is $P = \\frac{1}{4} \\begin{bmatrix} 5 & 1 \\\\ 1 & 1 \\end{bmatrix}$. To confirm $P$ is positive definite, we check its principal minors: $p_{11} = \\frac{5}{4} > 0$ and $\\det(P) = (\\frac{5}{4})(\\frac{1}{4}) - (\\frac{1}{4})^2 = \\frac{4}{16} = \\frac{1}{4} > 0$. Both are positive, so $P \\succ 0$.\n\nNow, we derive a sufficient condition for the time derivative of the Lyapunov function, $\\dot{V}(x)$, to be negative. The derivative along the trajectories of the nonlinear system is:\n$$\n\\dot{V}(x) = \\nabla V(x)^\\top f(x) = (2Px)^\\top (Ax + g(x)) = x^\\top(A^\\top P + PA)x + 2x^\\top P g(x).\n$$\nUsing $A^\\top P + PA = -I_2$, this simplifies to:\n$$\n\\dot{V}(x) = -x^\\top I_2 x + 2x^\\top P g(x) = -\\lVert x \\rVert_2^2 + 2x^\\top P g(x).\n$$\nFor $\\dot{V}(x)$ to be negative, we require $2x^\\top P g(x) < \\lVert x \\rVert_2^2$. We bound the term involving $g(x)$. First, we find a bound on the norm of $g(x)$. Using the inequality $|x_i| \\le \\lVert x \\rVert_2$:\n$$\n\\lvert g_1(x) \\rvert = \\lvert c_1 x_1^3 + c_2 x_1 x_2^2 \\rvert \\le \\lvert c_1 \\rvert \\lvert x_1 \\rvert^3 + \\lvert c_2 \\rvert \\lvert x_1 \\rvert \\lvert x_2 \\rvert^2 \\le (\\lvert c_1 \\rvert + \\lvert c_2 \\rvert) \\lVert x \\rVert_2^3.\n$$\n$$\n\\lvert g_2(x) \\rvert = \\lvert c_3 x_1^2 x_2 + c_4 x_2^3 \\rvert \\le \\lvert c_3 \\rvert \\lvert x_1 \\rvert^2 \\lvert x_2 \\rvert + \\lvert c_4 \\rvert \\lvert x_2 \\rvert^3 \\le (\\lvert c_3 \\rvert + \\lvert c_4 \\rvert) \\lVert x \\rVert_2^3.\n$$\nThe Euclidean norm of $g(x)$ is therefore bounded by:\n$$\n\\lVert g(x) \\rVert_2 = \\sqrt{\\lvert g_1(x) \\rvert^2 + \\lvert g_2(x) \\rvert^2} \\le \\sqrt{(\\lvert c_1 \\rvert + \\lvert c_2 \\rvert)^2 \\lVert x \\rVert_2^6 + (\\lvert c_3 \\rvert + \\lvert c_4 \\rvert)^2 \\lVert x \\rVert_2^6} = c \\lVert x \\rVert_2^3,\n$$\nwhere $c = \\sqrt{(\\lvert c_1 \\rvert + \\lvert c_2 \\rvert)^2 + (\\lvert c_3 \\rvert + \\lvert c_4 \\rvert)^2}$.\n\nUsing this bound and the Cauchy-Schwarz inequality, we bound $2x^\\top P g(x)$:\n$$\n2x^\\top P g(x) \\le 2 \\lvert x^\\top P g(x) \\rvert \\le 2 \\lVert Px \\rVert_2 \\lVert g(x) \\rVert_2.\n$$\nSince $P$ is symmetric positive definite, its induced $2$-norm is its largest eigenvalue, $\\lVert P \\rVert_2 = \\lambda_{\\max}(P)$. Thus, $\\lVert Px \\rVert_2 \\le \\lVert P \\rVert_2 \\lVert x \\rVert_2 = \\lambda_{\\max}(P) \\lVert x \\rVert_2$. Substituting the bounds:\n$$\n2x^\\top P g(x) \\le 2 (\\lambda_{\\max}(P) \\lVert x \\rVert_2) (c \\lVert x \\rVert_2^3) = 2c\\lambda_{\\max}(P) \\lVert x \\rVert_2^4.\n$$\nThe condition $\\dot{V}(x) < 0$ is therefore guaranteed if $-\\lVert x \\rVert_2^2 + 2c\\lambda_{\\max}(P) \\lVert x \\rVert_2^4 < 0$. For $x \\ne 0$, this simplifies to $2c\\lambda_{\\max}(P) \\lVert x \\rVert_2^2 < 1$, or $\\lVert x \\rVert_2^2 < \\frac{1}{2c\\lambda_{\\max}(P)}$.\n\nWe need to find an upper bound $\\rho_\\star$ on $\\rho$ such that for any $x$ in the sublevel set $\\{x \\mid V(x) \\le \\rho\\}$, the condition $\\dot{V}(x) < 0$ holds. For any $x$ in this set, we have the inequality $V(x) \\ge \\lambda_{\\min}(P) \\lVert x \\rVert_2^2$, which implies $\\lVert x \\rVert_2^2 \\le \\frac{V(x)}{\\lambda_{\\min}(P)} \\le \\frac{\\rho}{\\lambda_{\\min}(P)}$. To satisfy the negativity condition for all $x$ in the sublevel set, we require its boundary to be within the region of guaranteed negativity. We impose the strict inequality:\n$$\n\\frac{\\rho}{\\lambda_{\\min}(P)} < \\frac{1}{2c\\lambda_{\\max}(P)} \\implies \\rho < \\frac{\\lambda_{\\min}(P)}{2c\\lambda_{\\max}(P)}.\n$$\nThis gives the desired explicit upper bound $\\rho_\\star = \\frac{\\lambda_{\\min}(P)}{2c\\lambda_{\\max}(P)}$. The eigenvalues of $P$ are found from the characteristic equation $4\\lambda^2 - 6\\lambda + 1 = 0$, yielding $\\lambda = \\frac{3 \\pm \\sqrt{5}}{4}$. Thus, $\\lambda_{\\min}(P) = \\frac{3-\\sqrt{5}}{4}$ and $\\lambda_{\\max}(P) = \\frac{3+\\sqrt{5}}{4}$.\nThe bound is $\\rho_\\star = \\frac{(3-\\sqrt{5})/4}{2c(3+\\sqrt{5})/4} = \\frac{3-\\sqrt{5}}{2c(3+\\sqrt{5})} = \\frac{(3-\\sqrt{5})^2}{2c(9-5)} = \\frac{14 - 6\\sqrt{5}}{8c} = \\frac{7 - 3\\sqrt{5}}{4c}$.\n\nFor the numerical experiment, we generate initial conditions on the ellipse $x^\\top P x = \\rho$. A point $x$ on this ellipse can be generated from a point $y$ on a circle of radius $\\sqrt{\\rho}$ through a linear transformation. Let $P=LL^\\top$ be the Cholesky decomposition of $P$. Then $x^\\top P x = x^\\top L L^\\top x = (L^\\top x)^\\top (L^\\top x) = \\lVert L^\\top x \\rVert_2^2 = \\rho$. We define $y = L^\\top x$, so that $\\lVert y \\rVert_2 = \\sqrt{\\rho}$. The transformation is $x = (L^\\top)^{-1}y$. We will sample $N=24$ points $y_k = \\sqrt{\\rho} [\\cos \\theta_k, \\sin \\theta_k]^\\top$ with $\\theta_k = 2\\pi k/N$ for $k=0, \\dots, N-1$, and transform them to initial conditions $x_k$ for numerical integration.\n\nThe simulation of $\\dot{x}=f(x)$ for each $x_k$ will be performed over a time horizon $T=20$ using a numerical integrator with specified tolerances. Convergence is declared if the trajectory enters a small ball of radius $10^{-3}$ around the origin. Divergence is declared if the trajectory norm exceeds $10$. If all $N=24$ trajectories from the initial conditions on the ellipse $V(x) = \\rho = s \\rho_\\star$ converge, the test case is successful.\n\nThis completes the formal analytical derivation and the design of the numerical experiment. The implementation will proceed based on these results.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run the specified numerical experiment and print results.\n    \"\"\"\n    # Define test cases: ((c1, c2, c3, c4), s)\n    test_cases = [\n        ((0.5, 0.5, 0.5, 0.5), 0.5),\n        ((1.0, 1.0, 1.0, 1.0), 0.99),\n        ((2.0, 2.0, 2.0, 2.0), 20.0),\n    ]\n\n    results = []\n    for c_coeffs, s in test_cases:\n        results.append(run_experiment(c_coeffs, s))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_experiment(c_coeffs, s_factor):\n    \"\"\"\n    Computes the theoretical bound rho_star, sets up the simulation radius rho,\n    and runs the simulation for N initial conditions.\n\n    Returns:\n        bool: True if all trajectories converge, False otherwise.\n    \"\"\"\n    # --- Theoretical Analysis ---\n    # Matrix A\n    A = np.array([[0, 1], [-2, -3]])\n    \n    # Solution P to A^T P + P A = -I\n    P = np.array([[5/4, 1/4], [1/4, 1/4]])\n    \n    # Eigenvalues of P\n    lambda_min_P = (3 - np.sqrt(5)) / 4\n    lambda_max_P = (3 + np.sqrt(5)) / 4\n    \n    # Constant c for the bound on g(x)\n    c1, c2, c3, c4 = c_coeffs\n    c = np.sqrt((abs(c1) + abs(c2))**2 + (abs(c3) + abs(c4))**2)\n    \n    # Theoretical bound rho_star\n    rho_star = (7 - 3 * np.sqrt(5)) / (4 * c)\n    \n    # Simulation level set parameter rho\n    rho = s_factor * rho_star\n\n    # --- Numerical Experiment ---\n    N_samples = 24\n    time_horizon = 20.0\n    \n    # Transformation matrix to map circle to ellipse V(x)=rho\n    # P = L L^T, x = (L^T)^-1 y\n    # L_inv_T = inv(L.T) where L is the Cholesky factor of P\n    L_inv_T = np.array([[2/np.sqrt(5), -1/np.sqrt(5)], [0, np.sqrt(5)]])\n    \n    # Generate initial conditions on the ellipse x^T P x = rho\n    thetas = np.linspace(0, 2 * np.pi, N_samples, endpoint=False)\n    radius = np.sqrt(rho)\n    y_coords = radius * np.vstack([np.cos(thetas), np.sin(thetas)])\n    initial_conditions = L_inv_T @ y_coords\n    \n    # --- Simulation Loop ---\n    all_converged = True\n    for i in range(N_samples):\n        x0 = initial_conditions[:, i]\n        converged = simulate_trajectory(A, c_coeffs, x0, time_horizon)\n        if not converged:\n            all_converged = False\n            break\n            \n    return all_converged\n\ndef simulate_trajectory(A, c_coeffs, x0, T):\n    \"\"\"\n    Integrates the ODE for one initial condition and checks for convergence.\n    \"\"\"\n    # Define the nonlinear system dynamics: dx/dt = f(x)\n    def ode_func(t, x, A_mat, coeffs):\n        c1, c2, c3, c4 = coeffs\n        gx = np.array([\n            c1 * x[0]**3 + c2 * x[0] * x[1]**2,\n            c3 * x[0]**2 * x[1] + c4 * x[1]**3\n        ])\n        return A_mat @ x + gx\n\n    # Event for numerical convergence\n    def convergence_event(t, x, A_mat, coeffs):\n        return np.linalg.norm(x) - 1e-3\n    convergence_event.terminal = True\n    convergence_event.direction = -1\n\n    # Event for numerical divergence\n    def divergence_event(t, x, A_mat, coeffs):\n        return np.linalg.norm(x) - 10.0\n    divergence_event.terminal = True\n    divergence_event.direction = 1\n\n    sol = solve_ivp(\n        fun=ode_func,\n        t_span=(0, T),\n        y0=x0,\n        method='RK45',\n        args=(A, c_coeffs),\n        rtol=1e-9,\n        atol=1e-12,\n        events=[convergence_event, divergence_event]\n    )\n    \n    # Check for convergence\n    # 1. Did the convergence event trigger?\n    if sol.t_events[0].size > 0:\n        return True\n    # 2. Did the simulation finish at T? Check final state norm.\n    if sol.status == 0 and np.linalg.norm(sol.y[:, -1]) = 1e-3:\n        return True\n    \n    # If neither of the above, it's considered not to have converged.\n    # This includes divergence event or finishing at T with norm > 1e-3.\n    return False\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "2721989"}]}