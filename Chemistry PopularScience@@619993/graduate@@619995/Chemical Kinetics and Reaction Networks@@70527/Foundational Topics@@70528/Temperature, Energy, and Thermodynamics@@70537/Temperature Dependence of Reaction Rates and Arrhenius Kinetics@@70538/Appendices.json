{"hands_on_practices": [{"introduction": "The Arrhenius equation provides a powerful linear model for temperature-dependent kinetics, but real experimental data is often noisy and may exhibit subtle curvature. A common temptation is to fit high-order polynomials to capture every wiggle, but this often leads to overfitting—creating a model that describes noise rather than the underlying physics. This practice will guide you through using cross-validation, a fundamental technique in modern data science, to select a model of appropriate complexity that balances goodness-of-fit with predictive power. [@problem_id:2682859]", "problem": "A unimolecular isomerization is studied over a moderate temperature window where transition state theory is applicable. Let the temperature be denoted by $T$ and the rate constant by $k(T)$. Under transition state theory (Eyring form), $k(T)$ can be expressed in terms of activation free energy $\\Delta G^{\\ddagger}(T)$, and, when activation enthalpy $\\Delta H^{\\ddagger}$ and activation entropy $\\Delta S^{\\ddagger}$ vary weakly with $T$ over the window of interest, the empirical Arrhenius description is widely used: plotting $y = \\ln k$ against $x = 1/T$ yields an approximately linear relationship over modest ranges of $T$. Deviations from strict linearity can arise from the $T$ dependence of $\\Delta H^{\\ddagger}$, $\\Delta S^{\\ddagger}$, or from the $T$ factor in the Eyring prefactor, and from experimental noise. A common, but potentially hazardous, practice is to fit high-order polynomials in $x = 1/T$ to $y = \\ln k$ to capture such deviations.\n\nYou are given $N = 12$ measurements $\\{(T_i, k_i)\\}_{i=1}^{12}$ with $T_i$ in the range $T \\in [290\\,\\mathrm{K}, 400\\,\\mathrm{K}]$ sampled approximately uniformly, and with approximately homoscedastic Gaussian errors in $y_i = \\ln k_i$ of standard deviation $\\sigma \\approx 0.03$. Consider polynomial models of degree $d$ for $y$ as a function of $x = 1/T$:\n$$\ny(x) = \\sum_{j=0}^{d} a_j x^j,\\quad d \\in \\{1,2,3,4,5\\}.\n$$\nThe degree $d=1$ fit (simple linear regression of $y$ on $x$) yields an estimated slope $m = -6025\\,\\mathrm{K}$ and intercept $b$ (intercept not used below). For the $N=12$ training points, the residual sum of squares (RSS) of the degree-$d$ fits are empirically found to be:\n- $d=1$: $\\mathrm{RSS} = 0.015$\n- $d=2$: $\\mathrm{RSS} = 0.009$\n- $d=3$: $\\mathrm{RSS} = 0.006$\n- $d=4$: $\\mathrm{RSS} = 0.004$\n- $d=5$: $\\mathrm{RSS} = 0.003$\n\nTo assess generalization and guard against overfitting, you perform $K$-fold cross-validation (CV), with $K=6$ folds of size $2$, stratified across the temperature range so that each fold spans the domain. For each degree $d$, you compute the average validation mean squared error (MSE) over the $6$ held-out folds:\n- $d=1$: $\\mathrm{CV\\text{-}MSE} = 0.0016$\n- $d=2$: $\\mathrm{CV\\text{-}MSE} = 0.0012$\n- $d=3$: $\\mathrm{CV\\text{-}MSE} = 0.00125$\n- $d=4$: $\\mathrm{CV\\text{-}MSE} = 0.0016$\n- $d=5$: $\\mathrm{CV\\text{-}MSE} = 0.0021$\n\nUse the gas constant $R = 8.314\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$. Based on first principles of temperature-dependent kinetics and statistical model selection, answer the following. Select all options that are correct.\n\nA. A model chosen by $K$-fold cross-validation should minimize the average validation error, not the training RSS. For the given $\\mathrm{CV\\text{-}MSE}$ values, the degree $d=2$ model is preferred over $d=1$ and $d \\ge 3$.\n\nB. The degree $d=1$ slope $m=-6025\\,\\mathrm{K}$ implies an activation energy estimate $E_a \\approx 50.1\\,\\mathrm{kJ\\,mol^{-1}}$, which is physically plausible for a unimolecular isomerization over the studied range.\n\nC. Because the degree $d=5$ polynomial nearly interpolates the $N=12$ data with the smallest training RSS, it will yield the most reliable extrapolations of $\\ln k$ beyond the measured temperature range.\n\nD. Leave-one-out cross-validation (LOOCV, $K=N$) has lower variance than $K=6$ cross-validation because each training split uses more data, so it will always select the same optimal degree as $K=6$ CV on any such dataset.\n\nE. If the underlying kinetics obey the Eyring form $k(T) = (k_{\\mathrm{B}} T/h)\\,\\exp(\\Delta S^{\\ddagger}/R)\\,\\exp(-\\Delta H^{\\ddagger}/(R T))$ with weak temperature dependence in $\\Delta H^{\\ddagger}$ and $\\Delta S^{\\ddagger}$, mild curvature in $y=\\ln k$ versus $x=1/T$ is expected; a quadratic model in $x$ can better approximate this behavior over a finite window, and the corresponding effective activation energy becomes temperature-dependent via the local slope $E_a^{\\mathrm{eff}}(T) = -R\\,\\partial (\\ln k)/\\partial (1/T)$.", "solution": "The problem statement has been validated and found to be scientifically sound, well-posed, and internally consistent. It provides a realistic scenario for applying principles of chemical kinetics and statistical model selection. We may proceed with a detailed analysis of each option.\n\nThe core of the problem involves relating a physical model, the temperature dependence of a reaction rate constant, to a statistical procedure for fitting empirical data. The Arrhenius equation, $k(T) = A \\exp(-E_a/(RT))$, where $A$ is the pre-exponential factor and $E_a$ is the activation energy, suggests a linear relationship between $y = \\ln k$ and $x = 1/T$:\n$$y = \\ln A - \\frac{E_a}{R} x$$\nThis corresponds to a polynomial model of degree $d=1$. The Eyring equation from transition state theory provides a more physically detailed model:\n$$k(T) = \\frac{k_{\\mathrm{B}} T}{h} \\exp\\left(\\frac{\\Delta S^{\\ddagger}}{R}\\right) \\exp\\left(-\\frac{\\Delta H^{\\ddagger}}{RT}\\right)$$\nHere, $k_{\\mathrm{B}}$ is the Boltzmann constant, $h$ is the Planck constant, $\\Delta H^{\\ddagger}$ is the activation enthalpy, and $\\Delta S^{\\ddagger}$ is the activation entropy. Taking the logarithm gives:\n$$\\ln k(T) = \\ln\\left(\\frac{k_{\\mathrm{B}}}{h}\\right) + \\ln T + \\frac{\\Delta S^{\\ddagger}}{R} - \\frac{\\Delta H^{\\ddagger}}{RT}$$\nIn terms of $y = \\ln k$ and $x = 1/T$, this is:\n$$y(x) = \\left(\\ln\\left(\\frac{k_{\\mathrm{B}}}{h}\\right) + \\frac{\\Delta S^{\\ddagger}}{R}\\right) - \\ln x - \\left(\\frac{\\Delta H^{\\ddagger}}{R}\\right) x$$\nEven if $\\Delta H^{\\ddagger}$ and $\\Delta S^{\\ddagger}$ are constant, the $-\\ln x$ term introduces a non-linearity (curvature) in the plot of $y$ versus $x$. If $\\Delta H^{\\ddagger}$ and $\\Delta S^{\\ddagger}$ also depend on temperature, the curvature is modified. The problem explores fitting this potentially curved relationship with polynomials of increasing degree $d$ and uses cross-validation to select the optimal degree.\n\n**Analysis of Option A:**\nThis statement comprises two parts. First, \"A model chosen by $K$-fold cross-validation should minimize the average validation error, not the training RSS.\" This is a foundational principle of statistical learning. The training error, measured here by the Residual Sum of Squares (RSS), naturally decreases as model complexity (polynomial degree $d$) increases. A model with sufficiently high complexity will eventually fit the noise in the training data, a phenomenon known as overfitting. This leads to poor predictive performance on new, unseen data (poor generalization). Cross-validation provides an estimate of this generalization error. The goal is to select a model that optimally balances the trade-off between bias (underfitting) and variance (overfitting). The model that minimizes the cross-validated error is typically chosen as the best compromise.\n\nThe second part is: \"For the given $\\mathrm{CV\\text{-}MSE}$ values, the degree $d=2$ model is preferred over $d=1$ and $d \\ge 3$.\" We are given the following average validation mean squared errors:\n- $d=1$: $\\mathrm{CV\\text{-}MSE} = 0.0016$\n- $d=2$: $\\mathrm{CV\\text{-}MSE} = 0.0012$\n- $d=3$: $\\mathrm{CV\\text{-}MSE} = 0.00125$\n- $d=4$: $\\mathrm{CV\\text{-}MSE} = 0.0016$\n- $d=5$: $\\mathrm{CV\\text{-}MSE} = 0.0021$\nThe minimum value of $\\mathrm{CV\\text{-}MSE}$ is $0.0012$, which occurs for the degree $d=2$ polynomial. All other models, including $d=1$ and those with $d \\ge 3$, have a higher estimated generalization error. Therefore, based on the principle of minimizing cross-validation error, the $d=2$ model is indeed the preferred choice. The statement is a correct application of the principle to the provided data.\nVerdict: **Correct**.\n\n**Analysis of Option B:**\nThis statement claims that the slope from the linear fit ($d=1$), $m = -6025\\,\\mathrm{K}$, corresponds to an activation energy $E_a \\approx 50.1\\,\\mathrm{kJ\\,mol^{-1}}$, and that this value is physically plausible.\nFrom the simple Arrhenius model, the slope $m$ of the $\\ln k$ versus $1/T$ plot is related to the activation energy $E_a$ by $m = -E_a/R$.\nWe can calculate the implied $E_a$:\n$$E_a = -m \\times R$$\nUsing the given values $m = -6025\\,\\mathrm{K}$ and $R = 8.314\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$:\n$$E_a = -(-6025\\,\\mathrm{K}) \\times (8.314\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}) = 50085.35\\,\\mathrm{J\\,mol^{-1}}$$\nTo convert this to $\\mathrm{kJ\\,mol^{-1}}$, we divide by $1000$:\n$$E_a = 50.08535\\,\\mathrm{kJ\\,mol^{-1}} \\approx 50.1\\,\\mathrm{kJ\\,mol^{-1}}$$\nThe calculation is correct.\nThe physical plausibility of this value must be assessed. Activation energies for unimolecular isomerizations can span a wide range, but a value of approximately $50\\,\\mathrm{kJ\\,mol^{-1}}$ (which is about $12\\,\\mathrm{kcal\\,mol^{-1}}$) is entirely reasonable. Such a barrier is low enough for the reaction to proceed at a measurable rate in the temperature range of $290\\,\\mathrm{K}$ to $400\\,\\mathrm{K}$, but high enough not to be instantaneous. Thus, the value is physically plausible.\nVerdict: **Correct**.\n\n**Analysis of Option C:**\nThis statement argues that the $d=5$ model is best for extrapolation because it has the smallest training RSS. This is a severe and common error in reasoning. The training RSS is given as:\n- $d=1: 0.015$; $d=2: 0.009$; $d=3: 0.006$; $d=4: 0.004$; $d=5: 0.003$.\nThe RSS for $d=5$ is indeed the smallest. However, this only indicates that the $d=5$ model fits the training data points most closely. It does not imply good predictive power. The cross-validation results show the opposite: the $d=5$ model has the highest $\\mathrm{CV\\text{-}MSE}$ ($0.0021$), indicating it generalizes poorly even to points within the same temperature domain. High-degree polynomials are notoriously prone to overfitting and exhibit wild oscillations, especially when used for extrapolation (predicting outside the range of the training data). The model that \"nearly interpolates\" the noisy data has learned the noise, not the underlying physical law, making it the least reliable for any form of prediction, and catastrophically unreliable for extrapolation.\nVerdict: **Incorrect**.\n\n**Analysis of Option D:**\nThis statement makes two strong claims about Leave-one-out cross-validation (LOOCV), which is $K$-fold CV with $K=N$.\nFirst, it claims LOOCV \"has lower variance than $K=6$ cross-validation because each training split uses more data\". While it is true that each training fold in LOOCV is larger (size $N-1=11$) than in $K=6$ CV (size $N(K-1)/K = 10$), the conclusion about variance is incorrect. The variance in question is that of the CV error estimate itself. Because the $N$ training sets in LOOCV are almost identical (each missing just one different point), the model fits are highly correlated. This high correlation among the fold-specific error estimates tends to increase, not decrease, the variance of their average. Standard statistical literature establishes that $K$-fold CV with moderate $K$ (e.g., $5$ or $10$) often provides a more stable (lower variance) estimate of generalization error than LOOCV, at the cost of a slightly higher bias. The premise is false.\nSecond, it claims LOOCV \"will always select the same optimal degree as $K=6$ CV on any such dataset.\" This is an absolute statement (\"always\") that is easily falsified. LOOCV and $K=6$ CV are different estimators for generalization error. As they are stochastic procedures whose outcomes depend on the specific data sample, there is no theoretical basis to expect them to yield the same result. Different choices of $K$, and even different random partitions for the same $K$, can lead to the selection of different models. The claim is patently false.\nVerdict: **Incorrect**.\n\n**Analysis of Option E:**\nThis statement connects the empirical observation of curvature to the underlying physical theory.\nFirst, it posits that the Eyring equation predicts mild curvature in the $y = \\ln k$ versus $x=1/T$ plot. As derived previously, $\\ln k(T)$ contains a $\\ln T$ term, which becomes a $-\\ln x$ term in the plot of $y$ versus $x$. This term makes the relationship non-linear, introducing curvature. This is a known feature distinguishing the Eyring model from the simpler Arrhenius model. This part is correct.\nSecond, it suggests a quadratic model ($d=2$) is a good approximation for this behavior. Expanding the function $y(x) = C - \\ln x - C'x$ as a Taylor series in $x$ about a point $x_0$ yields a polynomial. A quadratic truncation is $y(x) \\approx a_0 + a_1 x + a_2 x^2$, which is often an excellent local approximation for a smoothly curving function. The CV results, which favor the $d=2$ model, provide direct empirical support for this claim. This part is also correct.\nThird, it defines the effective activation energy as $E_a^{\\mathrm{eff}}(T) = -R\\,\\partial (\\ln k)/\\partial (1/T)$. This is the standard definition of a temperature-dependent activation energy derived from a non-linear Arrhenius-type plot. The slope of the tangent to the curve at a given $T$ (or $x=1/T$) defines the local apparent activation energy. If the plot is curved, the slope varies with $T$, and thus $E_a^{\\mathrm{eff}}$ is temperature-dependent. For the quadratic fit $y(x) = a_0 + a_1 x + a_2 x^2$, the derivative is $\\partial y/\\partial x = a_1 + 2a_2 x$, so $E_a^{\\mathrm{eff}}(T) = -R(a_1 + 2a_2/T)$, which explicitly shows the temperature dependence. The definition and its consequence are correct.\nThis option provides a coherent and physically correct explanation for why a non-linear fit might be superior to a linear one.\nVerdict: **Correct**.", "answer": "$$\\boxed{ABE}$$", "id": "2682859"}, {"introduction": "Once we establish that a non-linear model may better represent our kinetic data, the next step is to seek a physical interpretation for the model's parameters. Measurable curvature in an Arrhenius plot is not merely a mathematical nuisance; it is a signature of the temperature dependence of the activation enthalpy and entropy, as described by Transition State Theory. This exercise will challenge you to derive the connection between the empirical curvature of a plot of $\\ln k$ versus $1/T$ and the activation heat capacity, $\\Delta C_{p}^{\\ddagger}$, allowing you to extract thermodynamic information directly from kinetic measurements. [@problem_id:2682876]", "problem": "A unimolecular isomerization is studied over a narrow temperature window centered at a reference temperature $T_{0} = 300\\,\\mathrm{K}$. The Arrhenius plot of $\\ln k$ versus $1/T$ shows measurable curvature over this window. To summarize the temperature dependence locally, the following quadratic model is fit by least squares:\n$$\n\\ln k(T) = a + b\\left(\\frac{1}{T}\\right) + c\\left(\\frac{1}{T}\\right)^{2}\n$$,\nwith the resulting coefficient $c = 9.0000 \\times 10^{4}\\,\\mathrm{K}^{2}$.\n\nAssume Transition State Theory (TST) holds and that the activation heat capacity $\\Delta C_{p}^{\\ddagger}$ is temperature independent over the window. Use as the fundamental base:\n- The TST rate expression $k(T) = \\frac{k_{B} T}{h} \\exp\\!\\left(\\frac{\\Delta S^{\\ddagger}(T)}{R}\\right)\\exp\\!\\left(-\\frac{\\Delta H^{\\ddagger}(T)}{R T}\\right)$.\n- The standard thermodynamic temperature dependencies for constant activation heat capacity,\n$$\n\\Delta H^{\\ddagger}(T) = \\Delta H^{\\ddagger}(T_{0}) + \\Delta C_{p}^{\\ddagger}\\,(T - T_{0}), \\quad\n\\Delta S^{\\ddagger}(T) = \\Delta S^{\\ddagger}(T_{0}) + \\Delta C_{p}^{\\ddagger}\\,\\ln\\!\\left(\\frac{T}{T_{0}}\\right).\n$$\n\nStarting from these bases and without invoking any additional shortcut formulas, derive the local relation between the curvature of $\\ln k$ with respect to $1/T$ and $\\Delta C_{p}^{\\ddagger}$ evaluated at $T_{0}$. Then, interpret the fitted quadratic coefficient $c$ in terms of $\\Delta C_{p}^{\\ddagger}$ and compute the numerical value of $\\Delta C_{p}^{\\ddagger}$ at $T_{0} = 300\\,\\mathrm{K}$. Use the universal gas constant $R = 8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$. Round your final numerical answer to four significant figures. Express the final value in $\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Empirical model for the rate constant $k(T)$: $\\ln k(T) = a + b\\left(\\frac{1}{T}\\right) + c\\left(\\frac{1}{T}\\right)^{2}$.\n- Reference temperature: $T_{0} = 300\\,\\mathrm{K}$.\n- Fitted quadratic coefficient: $c = 9.0000 \\times 10^{4}\\,\\mathrm{K}^{2}$.\n- Theoretical model: Transition State Theory (TST).\n- TST rate expression: $k(T) = \\frac{k_{B} T}{h} \\exp\\left(\\frac{\\Delta S^{\\ddagger}(T)}{R}\\right)\\exp\\left(-\\frac{\\Delta H^{\\ddagger}(T)}{R T}\\right)$.\n- Assumption: The activation heat capacity $\\Delta C_{p}^{\\ddagger}$ is constant over the temperature window.\n- Thermodynamic relationships:\n  $\\Delta H^{\\ddagger}(T) = \\Delta H^{\\ddagger}(T_{0}) + \\Delta C_{p}^{\\ddagger}\\,(T - T_{0})$\n  $\\Delta S^{\\ddagger}(T) = \\Delta S^{\\ddagger}(T_{0}) + \\Delta C_{p}^{\\ddagger}\\,\\ln\\left(\\frac{T}{T_{0}}\\right)$\n- Universal gas constant: $R = 8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, relying on established principles of chemical kinetics (Transition State Theory, Arrhenius equation) and thermodynamics (Kirchhoff's laws). It is well-posed, providing a self-contained set of equations and data necessary to derive a unique solution for $\\Delta C_{p}^{\\ddagger}$. The language is objective and precise. The assumptions, such as a constant $\\Delta C_{p}^{\\ddagger}$ over a narrow temperature range, are standard and physically reasonable approximations. The problem does not violate any scientific principles, is not incomplete, and presents a non-trivial but solvable challenge in physical chemistry.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be derived.\n\nThe derivation commences from the provided Transition State Theory expression for the rate constant, $k(T)$. Taking the natural logarithm of the expression gives:\n$$\n\\ln k(T) = \\ln\\left(\\frac{k_{B}T}{h}\\right) + \\frac{\\Delta S^{\\ddagger}(T)}{R} - \\frac{\\Delta H^{\\ddagger}(T)}{RT}\n$$\nThe problem specifies a plot of $\\ln k$ versus $1/T$. The curvature of this plot is given by the second derivative, $\\frac{d^2(\\ln k)}{d(1/T)^2}$. To compute this, we first establish the relationship between derivatives with respect to $1/T$ and $T$. Let $x = 1/T$, which implies $T = 1/x$ and $dT/dx = -1/x^2 = -T^2$. Using the chain rule:\n$$\n\\frac{d}{dx} = \\frac{dT}{dx}\\frac{d}{dT} = -T^2 \\frac{d}{dT}\n$$\nThe first derivative of $\\ln k$ with respect to $x=1/T$ is:\n$$\n\\frac{d(\\ln k)}{d(1/T)} = -T^2 \\frac{d(\\ln k)}{dT}\n$$\nThe Arrhenius activation energy, $E_a$, is defined as $E_a = RT^2 \\frac{d(\\ln k)}{dT}$. From Transition State Theory, it is also known that $E_a = \\Delta H^{\\ddagger}(T) + RT$. Therefore, we can express the derivative with respect to $T$ as:\n$$\n\\frac{d(\\ln k)}{dT} = \\frac{E_a}{RT^2} = \\frac{\\Delta H^{\\ddagger}(T) + RT}{RT^2} = \\frac{\\Delta H^{\\ddagger}(T)}{RT^2} + \\frac{1}{T}\n$$\nThe second derivative with respect to $x=1/T$ is found by differentiating again:\n$$\n\\frac{d^2(\\ln k)}{d(1/T)^2} = \\frac{d}{d(1/T)}\\left(-T^2 \\frac{d(\\ln k)}{dT}\\right) = -T^2 \\frac{d}{dT}\\left(-T^2 \\frac{d(\\ln k)}{dT}\\right)\n$$\nSubstituting the expression for the first derivative:\n$$\n\\frac{d^2(\\ln k)}{d(1/T)^2} = T^2 \\frac{d}{dT}\\left(T^2 \\left(\\frac{\\Delta H^{\\ddagger}(T)}{RT^2} + \\frac{1}{T}\\right)\\right) = T^2 \\frac{d}{dT}\\left(\\frac{\\Delta H^{\\ddagger}(T)}{R} + T\\right)\n$$\nThe derivative of $\\Delta H^{\\ddagger}(T)$ with respect to $T$ is the activation heat capacity, $\\Delta C_{p}^{\\ddagger}$. Since $\\Delta C_{p}^{\\ddagger}$ is assumed constant:\n$$\n\\frac{d(\\Delta H^{\\ddagger}(T))}{dT} = \\frac{d}{dT}\\left(\\Delta H^{\\ddagger}(T_0) + \\Delta C_p^{\\ddagger}(T - T_0)\\right) = \\Delta C_p^{\\ddagger}\n$$\nApplying this to the expression for curvature:\n$$\n\\frac{d^2(\\ln k)}{d(1/T)^2} = T^2 \\left(\\frac{1}{R}\\frac{d(\\Delta H^{\\ddagger}(T))}{dT} + 1\\right) = T^2 \\left(\\frac{\\Delta C_{p}^{\\ddagger}}{R} + 1\\right)\n$$\nThis fundamental expression relates the curvature of the Arrhenius plot to the activation heat capacity at a given temperature $T$.\n\nNow, we interpret the empirical quadratic model, $\\ln k(T) = a + b(1/T) + c(1/T)^2$. Differentiating twice with respect to $1/T$:\n$$\n\\frac{d(\\ln k)}{d(1/T)} = b + 2c\\left(\\frac{1}{T}\\right)\n$$\n$$\n\\frac{d^2(\\ln k)}{d(1/T)^2} = 2c\n$$\nThe coefficient $c$ is therefore directly proportional to the constant curvature of the quadratic model. For a model fit over a narrow temperature range centered at $T_0$, this empirical curvature $2c$ is the best-fit approximation to the theoretical curvature evaluated at $T_0$. Thus, we equate the two expressions for curvature at $T = T_0$:\n$$\n2c = T_0^2 \\left(1 + \\frac{\\Delta C_{p}^{\\ddagger}}{R}\\right)\n$$\nThis is the derived local relation. The interpretation of the fitted coefficient $c$ is that it is half the theoretical curvature of the Arrhenius plot at the reference temperature $T_0$.\n\nTo find the value of $\\Delta C_{p}^{\\ddagger}$, we rearrange the equation:\n$$\n\\frac{2c}{T_0^2} - 1 = \\frac{\\Delta C_{p}^{\\ddagger}}{R}\n$$\n$$\n\\Delta C_{p}^{\\ddagger} = R \\left(\\frac{2c}{T_0^2} - 1\\right)\n$$\nWe are given the values $c = 9.0000 \\times 10^{4}\\,\\mathrm{K}^{2}$, $T_0 = 300\\,\\mathrm{K}$, and $R = 8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$. The reference temperature $T_0$ is treated as an exact value for the calculation, a standard convention for reference state definitions.\nFirst, we compute the term in the parenthesis:\n$$\n\\frac{2c}{T_0^2} = \\frac{2 \\times (9.0000 \\times 10^{4}\\,\\mathrm{K}^{2})}{(300\\,\\mathrm{K})^2} = \\frac{1.8000 \\times 10^{5}\\,\\mathrm{K}^{2}}{90000\\,\\mathrm{K}^{2}} = 2.0000\n$$\nThis result is a dimensionless quantity with five significant figures, limited by the precision of $c$.\nNow, substitute this back into the expression for $\\Delta C_{p}^{\\ddagger}$:\n$$\n\\Delta C_{p}^{\\ddagger} = (8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}) \\times (2.0000 - 1) = (8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}) \\times 1.0000\n$$\nThe result of the multiplication is $8.314462618\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$. The number of significant figures is determined by $1.0000$, which has five. Thus, the value is $8.3145\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$. Rounding to the requested four significant figures gives $8.314\\,\\mathrm{J\\,mol^{-1}\\,K^{-1}}$.", "answer": "$$\n\\boxed{8.314}\n$$", "id": "2682876"}, {"introduction": "Effective research requires not only analyzing data but also designing experiments that are maximally informative. Instead of choosing measurement temperatures arbitrarily, optimal experimental design (OED) provides a rigorous framework for planning experiments to achieve the most precise parameter estimates with the least effort. In this practice, you will use the Fisher Information Matrix—a cornerstone of statistical inference—to quantify the information content of different temperature schedules and select the optimal design based on established criteria like D- and A-optimality. [@problem_id:2682865]", "problem": "You are given a single-step reaction whose temperature dependence obeys the Arrhenius law. For absolute temperature $T$ in $\\mathrm{K}$, the rate constant is $k(T) = A \\exp\\!\\left(-E_a/(R T)\\right)$, where $E_a$ is the activation energy in $\\mathrm{J/mol}$, $A$ is the pre-exponential factor, and $R$ is the universal gas constant in $\\mathrm{J/(mol\\cdot K)}$. You perform $n$ measurements of the rate constant at temperatures $T_1,\\dots,T_n$, and consider the logarithm of the measured rate constants. Assume an additive, independent, identically distributed Gaussian noise model on the logarithm:\n- For each $i \\in \\{1,\\dots,n\\}$, the measurement is $y_i = \\ln k(T_i) + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_y^2)$, mutually independent.\n- The parameter vector is $\\theta = (E_a, \\ln A)$.\n- The universal gas constant is $R = 8.314462618\\,\\mathrm{J/(mol\\cdot K)}$.\n\nYour task is to, for each candidate temperature schedule (a finite multiset of temperatures $T_i$), compute the Fisher Information Matrix (FIM) for the parameters $E_a$ and $\\ln A$ using only the foundational definition of the Fisher information for independent Gaussian noise, and then evaluate two design criteria across the candidate schedules:\n- Determinant-optimality (D-optimality): maximize $\\det(\\mathcal{I}(\\theta))$.\n- A-optimality: minimize $\\operatorname{tr}(\\mathcal{I}(\\theta)^{-1})$.\n\nFundamental base you may use:\n- Arrhenius law $k(T) = A \\exp\\!\\left(-E_a/(R T)\\right)$ and the definition $y_i = \\ln k(T_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_y^2)$.\n- The definition of the Fisher Information Matrix (FIM) for independent observations with Gaussian noise of known variance $\\sigma_y^2$:\n  $$\\mathcal{I}(\\theta) = \\mathbb{E}\\!\\left[-\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^\\top} \\log L(\\theta; y_1,\\dots,y_n)\\right] = \\sum_{i=1}^n \\frac{1}{\\sigma_y^2}\\, \\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^\\top,$$\n  where $\\mu_i(\\theta) = \\mathbb{E}[y_i \\mid \\theta]$.\n\nImportant implementation details:\n- Physical units: Use $T$ in $\\mathrm{K}$, $R$ in $\\mathrm{J/(mol\\cdot K)}$, $E_a$ in $\\mathrm{J/mol}$, and $\\sigma_y$ as a dimensionless standard deviation of $y_i = \\ln k_i$. The program’s outputs are dimensionless indices and real-valued criteria; no physical units need to be attached to the outputs.\n- Numerical stability: To determine singularity and compute the determinant and the trace of the inverse robustly, use the eigenvalues $\\lambda_1,\\lambda_2$ of the symmetric $2\\times 2$ Fisher Information Matrix. Let $\\lambda_{\\max} = \\max(\\lambda_1,\\lambda_2)$. Define a tolerance $\\tau = 10^{-12}$. If $\\lambda_{\\max} = 0$ or any $\\lambda_j \\le \\tau \\lambda_{\\max}$, treat the matrix as singular for the purpose of inversion; in that case, set $\\det(\\mathcal{I}) = 0$ and $\\operatorname{tr}(\\mathcal{I}^{-1}) = +\\infty$. Otherwise, compute $\\det(\\mathcal{I}) = \\lambda_1 \\lambda_2$ and $\\operatorname{tr}(\\mathcal{I}^{-1}) = \\lambda_1^{-1} + \\lambda_2^{-1}$.\n- Tie-breaking: When comparing determinants for D-optimality, if two determinants differ by at most a relative tolerance of $10^{-9}$, select the schedule with the smaller index. When comparing traces of the inverse for A-optimality, if two traces differ by at most a relative tolerance of $10^{-9}$, select the schedule with the smaller index.\n- Indexing: Candidate schedule indices are zero-based integers $0,1,2,\\dots$.\n\nTest suite to implement and solve:\n- Use $R = 8.314462618\\,\\mathrm{J/(mol\\cdot K)}$ and the following cases. For each case $c$, you are given $\\sigma_y$ (dimensionless) and a list of candidate schedules $\\mathcal{S}_c = \\{S_{c,0}, S_{c,1}, \\dots \\}$, where each $S_{c,j}$ is a list of temperatures in $\\mathrm{K}$.\n- Case $1$: $\\sigma_y = 0.05$. Candidate schedules:\n  - $S_{1,0} = [290, 310, 330, 350]\\,\\mathrm{K}$.\n  - $S_{1,1} = [300, 400, 500, 600]\\,\\mathrm{K}$.\n  - $S_{1,2} = [320, 325, 330, 335]\\,\\mathrm{K}$.\n  - $S_{1,3} = [280, 280, 600, 600]\\,\\mathrm{K}$.\n- Case $2$: $\\sigma_y = 0.05$. Candidate schedules:\n  - $S_{2,0} = [500, 505, 510, 515]\\,\\mathrm{K}$.\n  - $S_{2,1} = [290, 450, 700, 900]\\,\\mathrm{K}$.\n  - $S_{2,2} = [300, 300, 300, 300]\\,\\mathrm{K}$.\n- Case $3$: $\\sigma_y = 0.2$. Candidate schedules:\n  - $S_{3,0} = [300, 350, 400, 450, 500]\\,\\mathrm{K}$.\n  - $S_{3,1} = [280, 280, 600, 600, 600]\\,\\mathrm{K}$.\n  - $S_{3,2} = [320, 420, 520, 620, 720]\\,\\mathrm{K}$.\n- Case $4$: $\\sigma_y = 0.05$. Candidate schedules:\n  - $S_{4,0} = [250, 1000]\\,\\mathrm{K}$.\n  - $S_{4,1} = [250, 250, 1000]\\,\\mathrm{K}$.\n  - $S_{4,2} = [500, 500]\\,\\mathrm{K}$.\n\nProgram requirements:\n- For each case $c$, and for each candidate schedule $S_{c,j}$, compute the $2\\times 2$ Fisher Information Matrix $\\mathcal{I}_{c,j}$ for $\\theta = (E_a, \\ln A)$ using the foundational definition above. Then compute $\\det(\\mathcal{I}_{c,j})$ and $\\operatorname{tr}(\\mathcal{I}_{c,j}^{-1})$ using the eigenvalue-based rules and tolerance described.\n- For each case $c$, identify the D-optimal index $j_D$ that maximizes $\\det(\\mathcal{I}_{c,j})$ (with tie-breaking) and the A-optimal index $j_A$ that minimizes $\\operatorname{tr}(\\mathcal{I}_{c,j}^{-1})$ (with tie-breaking). Also report the best D-optimal determinant value $\\det(\\mathcal{I}_{c,j_D})$ and the best A-optimal trace-of-inverse value $\\operatorname{tr}(\\mathcal{I}_{c,j_A}^{-1})$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of per-case summaries, where each case summary is a list of the form $[j_D, j_A, \\det(\\mathcal{I}_{c,j_D}), \\operatorname{tr}(\\mathcal{I}_{c,j_A}^{-1})]$. The overall output must be a single list of lists, for example:\n  $$\\texttt{[[jD\\_1,jA\\_1,det\\_1,traceinv\\_1],[jD\\_2,jA\\_2,det\\_2,traceinv\\_2],\\dots]}.$$", "solution": "The user-provided problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and complete. All provided information is consistent, and the task is a standard application of optimal experimental design theory to a fundamental model in chemical kinetics. The problem is therefore valid. We now proceed with the solution.\n\nThe core task is to identify optimal experimental designs for estimating the parameters of the Arrhenius equation. The design, in this context, is the choice of temperatures $T_i$ at which to measure the reaction rate constant. The optimality is evaluated using criteria derived from the Fisher Information Matrix (FIM).\n\nThe model for the logarithm of the rate constant $k(T)$ is given by\n$$y_i = \\ln k(T_i) + \\varepsilon_i$$\nwhere $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_y^2)$ are independent and identically distributed Gaussian noise terms. The Arrhenius law states $k(T) = A \\exp(-E_a/(RT))$, so the mean of our measurement $y_i$ is\n$$\\mu_i(\\theta) = \\mathbb{E}[y_i \\mid \\theta] = \\ln(A \\exp(-E_a/(RT_i))) = \\ln A - \\frac{E_a}{RT_i}$$\nThe vector of unknown parameters is $\\theta = (E_a, \\ln A)^\\top$. The model for the mean response $\\mu_i(\\theta)$ can be written as a linear function of the parameters:\n$$\\mu_i(\\theta) = \\begin{pmatrix} -1/(RT_i) & 1 \\end{pmatrix} \\begin{pmatrix} E_a \\\\ \\ln A \\end{pmatrix}$$\nThis is a linear regression model. For such models, the Fisher Information Matrix does not depend on the true values of the parameters $\\theta$, which is a crucial property that allows for experimental design before the experiment is even run.\n\nThe Fisher Information Matrix, $\\mathcal{I}(\\theta)$, for $n$ independent measurements with known Gaussian noise variance $\\sigma_y^2$ is given by the foundational formula:\n$$\\mathcal{I}(\\theta) = \\sum_{i=1}^n \\frac{1}{\\sigma_y^2} \\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right) \\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^\\top$$\nFirst, we compute the gradient of the mean response $\\mu_i$ with respect to the parameter vector $\\theta = (E_a, \\ln A)^\\top$:\n$$\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta} = \\begin{pmatrix} \\partial \\mu_i / \\partial E_a \\\\ \\partial \\mu_i / \\partial \\ln A \\end{pmatrix} = \\begin{pmatrix} -1/(RT_i) \\\\ 1 \\end{pmatrix}$$\nLet us denote this gradient vector as $g(T_i)$. The contribution of a single measurement at temperature $T_i$ to the FIM is the outer product of this gradient vector with itself, scaled by $1/\\sigma_y^2$:\n$$\\mathcal{I}_i = \\frac{1}{\\sigma_y^2} g(T_i) g(T_i)^\\top = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} -1/(RT_i) \\\\ 1 \\end{pmatrix} \\begin{pmatrix} -1/(RT_i) & 1 \\end{pmatrix} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} 1/(RT_i)^2 & -1/(RT_i) \\\\ -1/(RT_i) & 1 \\end{pmatrix}$$\nThe total FIM for a schedule of $n$ measurements at temperatures $T_1, \\dots, T_n$ is the sum of the individual contributions:\n$$\\mathcal{I} = \\sum_{i=1}^n \\mathcal{I}_i = \\frac{1}{\\sigma_y^2} \\sum_{i=1}^n \\begin{pmatrix} 1/(RT_i)^2 & -1/(RT_i) \\\\ -1/(RT_i) & 1 \\end{pmatrix}$$\n$$ \\mathcal{I} = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\frac{1}{R^2} \\sum_{i=1}^n \\frac{1}{T_i^2} & -\\frac{1}{R} \\sum_{i=1}^n \\frac{1}{T_i} \\\\ -\\frac{1}{R} \\sum_{i=1}^n \\frac{1}{T_i} & n \\end{pmatrix} $$\nThe FIM is singular, meaning its determinant is zero, if and only if all gradient vectors $g(T_i)$ are collinear. For our specific gradient vector, this occurs if and only if all temperatures $T_i$ are identical. Physically, this means that if measurements are taken at only one temperature, it is impossible to uniquely determine both the activation energy $E_a$ (related to the slope of the $\\ln k$ vs $1/T$ plot) and the pre-exponential factor $\\ln A$ (the intercept). This leads to non-identifiability of the parameters.\n\nWe evaluate two standard optimality criteria:\n1.  **D-optimality**: Maximize $\\det(\\mathcal{I})$. The inverse of the FIM, $\\mathcal{I}^{-1}$, is proportional to the covariance matrix of the parameter estimates. The volume of the confidence ellipsoid for the parameters is proportional to $1/\\sqrt{\\det(\\mathcal{I})}$. Therefore, maximizing $\\det(\\mathcal{I})$ is equivalent to minimizing the volume of the confidence region, leading to the most precise parameter estimates jointly.\n2.  **A-optimality**: Minimize $\\operatorname{tr}(\\mathcal{I}^{-1})$. Since $\\operatorname{tr}(\\mathcal{I}^{-1})$ is proportional to the sum of the variances of the parameter estimates, A-optimality aims to minimize the average variance of the estimates.\n\nThe computational procedure for each candidate temperature schedule is as follows:\n1.  Given a schedule of temperatures $\\{T_1, \\dots, T_n\\}$, compute the necessary sums: $S_1 = \\sum_{i=1}^n 1/T_i$ and $S_2 = \\sum_{i=1}^n 1/T_i^2$.\n2.  Construct the $2 \\times 2$ FIM $\\mathcal{I}$ using the derived formula, the given noise standard deviation $\\sigma_y$, and the gas constant $R = 8.314462618\\,\\mathrm{J/(mol\\cdot K)}$.\n3.  Compute the two eigenvalues, $\\lambda_1$ and $\\lambda_2$, of the symmetric matrix $\\mathcal{I}$.\n4.  Apply the specified numerical stability rule: Let $\\lambda_{\\max} = \\max(\\lambda_1, \\lambda_2)$ and $\\lambda_{\\min} = \\min(\\lambda_1, \\lambda_2)$. If $\\lambda_{\\max} = 0$ or $\\lambda_{\\min} \\le \\tau \\lambda_{\\max}$ with $\\tau = 10^{-12}$, the matrix is treated as singular.\n5.  If singular, the D-optimality criterion value is $\\det(\\mathcal{I}) = 0$ and the A-optimality criterion value is $\\operatorname{tr}(\\mathcal{I}^{-1}) = \\infty$.\n6.  If non-singular, the criteria are computed as $\\det(\\mathcal{I}) = \\lambda_1 \\lambda_2$ and $\\operatorname{tr}(\\mathcal{I}^{-1}) = 1/\\lambda_1 + 1/\\lambda_2$.\n\nFor each test case, we compute these two criteria for all candidate schedules. We then identify the optimal schedule for each criterion. The selection process incorporates a tie-breaking rule: if two schedules yield criterion values that differ by a relative tolerance of at most $10^{-9}$, the schedule with the smaller index is chosen. This is implemented by first finding the globally optimal value (maximum determinant or minimum trace of inverse) across all schedules in a case, then identifying all schedules whose criterion values are within the tolerance band of this optimum, and finally selecting the one with the minimum index from this subset. This procedure is systematically applied to all test cases to produce the final result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Fisher Information Matrices for different temperature schedules\n    and determines optimal schedules based on D- and A-optimality criteria.\n    \"\"\"\n\n    # Global constants defined in the problem\n    R = 8.314462618  # J/(mol*K)\n    SINGULARITY_TOL = 1e-12\n    TIE_BREAK_TOL = 1e-9\n\n    # Test suite from the problem statement\n    test_cases = [\n        {\n            \"sigma_y\": 0.05,\n            \"schedules\": [\n                [290, 310, 330, 350],\n                [300, 400, 500, 600],\n                [320, 325, 330, 335],\n                [280, 280, 600, 600],\n            ],\n        },\n        {\n            \"sigma_y\": 0.05,\n            \"schedules\": [\n                [500, 505, 510, 515],\n                [290, 450, 700, 900],\n                [300, 300, 300, 300],\n            ],\n        },\n        {\n            \"sigma_y\": 0.2,\n            \"schedules\": [\n                [300, 350, 400, 450, 500],\n                [280, 280, 600, 600, 600],\n                [320, 420, 520, 620, 720],\n            ],\n        },\n        {\n            \"sigma_y\": 0.05,\n            \"schedules\": [\n                [250, 1000], \n                [250, 250, 1000],\n                [500, 500]\n            ],\n        },\n    ]\n\n    def calculate_criteria(T_schedule, sigma_y):\n        \"\"\"\n        Calculates D- and A-optimality criteria for a given temperature schedule.\n        \"\"\"\n        T = np.array(T_schedule, dtype=np.float64)\n        n = len(T)\n\n        if n < 2:  # Not enough points to estimate two parameters\n            return 0.0, np.inf\n\n        # FIM elements calculation\n        sum_inv_T = np.sum(1.0 / T)\n        sum_inv_T2 = np.sum(1.0 / (T**2))\n\n        # The core matrix M such that FIM = (1/sigma_y^2) * M\n        m11 = sum_inv_T2 / (R**2)\n        m12 = -sum_inv_T / R\n        m22 = float(n)\n        \n        fim = (1.0 / sigma_y**2) * np.array([[m11, m12], [m12, m22]])\n\n        # Calculate eigenvalues for robust computation\n        try:\n            eigvals = np.linalg.eigvalsh(fim)\n        except np.linalg.LinAlgError:\n            return 0.0, np.inf\n        \n        # Ensure eigenvalues are non-negative\n        lambda1, lambda2 = max(0, eigvals[0]), max(0, eigvals[1])\n        \n        lambda_max = max(lambda1, lambda2)\n        lambda_min = min(lambda1, lambda2)\n\n        # Evaluate singularity based on problem-defined rule\n        if lambda_max == 0 or lambda_min <= SINGULARITY_TOL * lambda_max:\n            det_val = 0.0\n            trace_inv_val = np.inf\n        else:\n            det_val = lambda1 * lambda2\n            trace_inv_val = (1.0 / lambda1) + (1.0 / lambda2)\n\n        return det_val, trace_inv_val\n\n    all_results = []\n    for case in test_cases:\n        sigma_y = case[\"sigma_y\"]\n        schedules = case[\"schedules\"]\n\n        results_per_schedule = []\n        for i, schedule in enumerate(schedules):\n            d_val, a_val = calculate_criteria(schedule, sigma_y)\n            results_per_schedule.append({\"idx\": i, \"d\": d_val, \"a\": a_val})\n\n        # --- D-optimality (maximize determinant) ---\n        max_d_val = -1.0\n        for res in results_per_schedule:\n            if res[\"d\"] > max_d_val:\n                max_d_val = res[\"d\"]\n        \n        d_candidates = []\n        if max_d_val >= 0:\n            # Handle division-by-zero for relative tolerance\n            norm = max_d_val if max_d_val > 0 else 1.0\n            for res in results_per_schedule:\n                if abs(res[\"d\"] - max_d_val) <= TIE_BREAK_TOL * norm:\n                    d_candidates.append(res[\"idx\"])\n        \n        best_d_idx = min(d_candidates)\n        final_d_val = results_per_schedule[best_d_idx][\"d\"]\n\n        # --- A-optimality (minimize trace of inverse) ---\n        min_a_val = np.inf\n        for res in results_per_schedule:\n            if res[\"a\"] < min_a_val:\n                min_a_val = res[\"a\"]\n                \n        a_candidates = []\n        if np.isfinite(min_a_val):\n             # Handle division-by-zero for relative tolerance\n            norm = min_a_val if min_a_val > 0 else 1.0\n            for res in results_per_schedule:\n                if abs(res[\"a\"] - min_a_val) <= TIE_BREAK_TOL * norm:\n                    a_candidates.append(res[\"idx\"])\n        else: # min_a_val is inf\n            for res in results_per_schedule:\n                if not np.isfinite(res[\"a\"]):\n                    a_candidates.append(res[\"idx\"])\n\n        best_a_idx = min(a_candidates)\n        final_a_val = results_per_schedule[best_a_idx][\"a\"]\n\n        all_results.append([best_d_idx, best_a_idx, final_d_val, final_a_val])\n\n    # Construct the final output string exactly as specified\n    case_strings = []\n    for res in all_results:\n        # Use a format that automatically handles scientific notation if needed\n        s = f\"[{res[0]},{res[1]},{res[2]:.15g},{res[3]:.15g}]\"\n        case_strings.append(s)\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2682865"}]}