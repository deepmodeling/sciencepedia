{"hands_on_practices": [{"introduction": "The method of initial rates is a cornerstone of experimental kinetics, designed to elucidate the functional form of a rate law by observing how the reaction rate changes with initial reactant concentrations. This practice provides hands-on experience in transforming the non-linear power-law model, $r_0 = k_{\\text{eff}} [\\mathrm{A}]_0^{\\alpha} [\\mathrm{B}]_0^{\\beta}$, into a linear regression problem via logarithmic scaling. By fitting experimental data within this framework, you will not only estimate the partial reaction orders but also quantify the statistical confidence in your estimates, a critical skill for interpreting kinetic data [@problem_id:2665183].", "problem": "You are given initial-rate measurements for a bimolecular reaction between species A and B at a fixed temperature, with the system assumed to follow an empirical power-law rate law at sufficiently low conversion. The objective is to estimate the partial reaction orders with respect to A and B and to quantify their statistical uncertainties from first principles. Use only fundamental definitions and well-tested rules as a base, including the empirical power-law form of the rate law under constant temperature and the classical linear regression framework.\n\nYour program must:\n- Start from the foundational assumption that, at fixed temperature, the initial rate $r_0$ is proportional to a constant times $[\\mathrm{A}]_0^{\\alpha}[\\mathrm{B}]_0^{\\beta}$, where $\\alpha$ and $\\beta$ are unknown, dimensionless reaction orders to be inferred from data, and the constant absorbs the rate coefficient and any unit dependence.\n- Transform the data appropriately to cast the inference of $\\alpha$ and $\\beta$ as an instance of a linear regression problem justified by the transformation’s algebraic properties.\n- Use ordinary least squares (OLS) for parameter estimation, and compute the standard errors of $\\alpha$ and $\\beta$ based on the covariance of the OLS estimator under the classical assumptions.\n- Use the natural logarithm for all logarithmic transforms.\n- Report, for each dataset, a list of four values $[\\hat{\\alpha}, \\hat{\\beta}, \\sigma_{\\hat{\\alpha}}, \\sigma_{\\hat{\\beta}}]$, where $\\hat{\\alpha}$ and $\\hat{\\beta}$ are the OLS estimates of the reaction orders and $\\sigma_{\\hat{\\alpha}}$, $\\sigma_{\\hat{\\beta}}$ are their one-standard-deviation uncertainties (standard errors). All reported values are dimensionless. Round each reported value to four decimal places.\n\nScientific realism notes:\n- The constant temperature implies a fixed rate coefficient for a given dataset; only concentration effects are analyzed here.\n- Taking logarithms rescales the model in a way that isolates exponents as linear coefficients, while the intercept absorbs the constant and unit dependence. Estimates of $\\alpha$ and $\\beta$ are invariant to consistent choices of units for $r_0$, $[\\mathrm{A}]_0$, and $[\\mathrm{B}]_0$; the intercept is not.\n- Uncertainties must arise from the residual variance estimated from the data; a perfectly consistent dataset may yield effectively zero residual variance and hence zero standard errors in finite precision arithmetic.\n\nPhysical units:\n- Initial rates $r_0$ are provided in $\\mathrm{mol}\\,\\mathrm{L}^{-1}\\,\\mathrm{s}^{-1}$.\n- Concentrations are provided in $\\mathrm{mol}\\,\\mathrm{L}^{-1}$.\n- Use the numerical values of the provided quantities in their stated units when taking logarithms.\n- The required outputs are dimensionless and therefore require no unit conversion.\n\nTest suite:\nFor each dataset below, the experiments are listed as ordered triples $\\left([\\mathrm{A}]_0, [\\mathrm{B}]_0, r_0\\right)$ with $[\\mathrm{A}]_0$ in $\\mathrm{mol}\\,\\mathrm{L}^{-1}$, $[\\mathrm{B}]_0$ in $\\mathrm{mol}\\,\\mathrm{L}^{-1}$, and $r_0$ in $\\mathrm{mol}\\,\\mathrm{L}^{-1}\\,\\mathrm{s}^{-1}$. Your program must process all experiments in a dataset jointly to produce a single result list $[\\hat{\\alpha}, \\hat{\\beta}, \\sigma_{\\hat{\\alpha}}, \\sigma_{\\hat{\\beta}}]$.\n\n- Dataset D1 (happy path; internally consistent power law with broad dynamic range):\n  1. $\\left(0.10,\\;0.05,\\;1.767766953\\times 10^{-5}\\right)$\n  2. $\\left(0.10,\\;0.10,\\;2.500000000\\times 10^{-5}\\right)$\n  3. $\\left(0.10,\\;0.20,\\;3.535533906\\times 10^{-5}\\right)$\n  4. $\\left(0.20,\\;0.05,\\;5.000000000\\times 10^{-5}\\right)$\n  5. $\\left(0.20,\\;0.10,\\;7.071067810\\times 10^{-5}\\right)$\n  6. $\\left(0.20,\\;0.20,\\;1.000000000\\times 10^{-4}\\right)$\n  7. $\\left(0.50,\\;0.05,\\;1.976423538\\times 10^{-4}\\right)$\n  8. $\\left(0.50,\\;0.10,\\;2.795084972\\times 10^{-4}\\right)$\n  9. $\\left(0.50,\\;0.20,\\;3.952847075\\times 10^{-4}\\right)$\n\n- Dataset D2 (moderate multiplicative noise; checks robustness and uncertainty quantification):\n  1. $\\left(0.02,\\;0.02,\\;6.72\\times 10^{-7}\\right)$\n  2. $\\left(0.02,\\;0.10,\\;3.136\\times 10^{-6}\\right)$\n  3. $\\left(0.02,\\;0.30,\\;9.696\\times 10^{-6}\\right)$\n  4. $\\left(0.05,\\;0.02,\\;3.88\\times 10^{-6}\\right)$\n  5. $\\left(0.05,\\;0.10,\\;2.04\\times 10^{-5}\\right)$\n  6. $\\left(0.05,\\;0.30,\\;5.76\\times 10^{-5}\\right)$\n  7. $\\left(0.10,\\;0.02,\\;1.648\\times 10^{-5}\\right)$\n  8. $\\left(0.10,\\;0.10,\\;8.32\\times 10^{-5}\\right)$\n  9. $\\left(0.10,\\;0.30,\\;2.28\\times 10^{-4}\\right)$\n\n- Dataset D3 (near-collinearity in $[\\mathrm{B}]_0$ variation; assesses variance inflation in the estimate of $\\beta$):\n  1. $\\left(0.050,\\;0.095,\\;7.125\\times 10^{-5}\\right)$\n  2. $\\left(0.050,\\;0.100,\\;7.575\\times 10^{-5}\\right)$\n  3. $\\left(0.050,\\;0.105,\\;7.79625\\times 10^{-5}\\right)$\n  4. $\\left(0.100,\\;0.095,\\;1.4535\\times 10^{-4}\\right)$\n  5. $\\left(0.100,\\;0.100,\\;1.5000\\times 10^{-4}\\right)$\n  6. $\\left(0.100,\\;0.105,\\;1.5435\\times 10^{-4}\\right)$\n  7. $\\left(0.200,\\;0.095,\\;2.8785\\times 10^{-4}\\right)$\n  8. $\\left(0.200,\\;0.100,\\;2.9700\\times 10^{-4}\\right)$\n  9. $\\left(0.200,\\;0.105,\\;3.2445\\times 10^{-4}\\right)$\n\nFinal output format:\n- Your program should produce a single line of output containing a top-level list with three inner lists, one per dataset in the order $\\mathrm{D1}, \\mathrm{D2}, \\mathrm{D3}$. Each inner list must be $[\\hat{\\alpha}, \\hat{\\beta}, \\sigma_{\\hat{\\alpha}}, \\sigma_{\\hat{\\beta}}]$, with each value rounded to four decimal places. For example, the output must look like:\n- $[[a_1,b_1,s_{a1},s_{b1}],[a_2,b_2,s_{a2},s_{b2}],[a_3,b_3,s_{a3},s_{b3}]]$\n- Output only this single line with no additional text.", "solution": "The problem statement has been rigorously evaluated and is determined to be valid. It constitutes a well-posed problem in chemical kinetics that is scientifically grounded, objective, and internally consistent. It requires the application of fundamental principles of reaction rate laws and statistical parameter estimation. We will proceed with a solution founded on first principles.\n\nThe starting point is the empirical power-law rate expression for a bimolecular reaction at constant temperature $T$:\n$$r_0 = k_{\\text{eff}} [\\mathrm{A}]_0^{\\alpha} [\\mathrm{B}]_0^{\\beta}$$\nHere, $r_0$ is the initial reaction rate, $[\\mathrm{A}]_0$ and $[\\mathrm{B}]_0$ are the initial concentrations of reactants $\\mathrm{A}$ and $\\mathrm{B}$, respectively. The parameters $\\alpha$ and $\\beta$ are the partial orders of reaction with respect to $\\mathrm{A}$ and $\\mathrm{B}$. The term $k_{\\text{eff}}$ is an effective rate constant which absorbs the true rate coefficient and any dependencies related to the choice of units. The objective is to determine the values of $\\alpha$ and $\\beta$ and their statistical uncertainties from experimental data.\n\nThis model is nonlinear in its parameters $\\alpha$ and $\\beta$. To apply the powerful and simple framework of linear regression, we must first linearize the equation. This is achieved by taking the natural logarithm of both sides:\n$$\\ln(r_0) = \\ln(k_{\\text{eff}} [\\mathrm{A}]_0^{\\alpha} [\\mathrm{B}]_0^{\\beta})$$\nUsing the properties of logarithms, this expression expands to a linear form:\n$$\\ln(r_0) = \\ln(k_{\\text{eff}}) + \\alpha \\ln([\\mathrm{A}]_0) + \\beta \\ln([\\mathrm{B}]_0)$$\nThis equation has the structure of a multiple linear regression model. For a set of $n$ experimental measurements, we can write the $i$-th measurement ($i=1, \\dots, n$) as:\n$$y_i = \\theta_0 + \\theta_1 x_{i,1} + \\theta_2 x_{i,2} + \\epsilon_i$$\nwhere we have made the following identifications:\n- The dependent variable is $y_i = \\ln(r_{0,i})$.\n- The independent variables (predictors) are $x_{i,1} = \\ln([\\mathrm{A}]_{0,i})$ and $x_{i,2} = \\ln([\\mathrm{B}]_{0,i})$.\n- The parameters to be estimated are the intercept $\\theta_0 = \\ln(k_{\\text{eff}})$, and the slopes $\\theta_1 = \\alpha$ and $\\theta_2 = \\beta$.\n- The term $\\epsilon_i$ represents the random experimental error, assumed to be additive in the logarithmic scale.\n\nThis system of $n$ linear equations can be expressed concisely in matrix notation as $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}$, where:\n- $\\mathbf{Y}$ is the $n \\times 1$ column vector of observed responses: $\\mathbf{Y} = [\\ln(r_{0,1}), \\dots, \\ln(r_{0,n})]^T$.\n- $\\mathbf{X}$ is the $n \\times p$ design matrix, with $p=3$ parameters. Each row corresponds to an experiment, and each column to a predictor variable, including a constant for the intercept:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & \\ln([\\mathrm{A}]_{0,1}) & \\ln([\\mathrm{B}]_{0,1}) \\\\\n1 & \\ln([\\mathrm{A}]_{0,2}) & \\ln([\\mathrm{B}]_{0,2}) \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & \\ln([\\mathrm{A}]_{0,n}) & \\ln([\\mathrm{B}]_{0,n})\n\\end{pmatrix}\n$$\n- $\\boldsymbol{\\theta}$ is the $p \\times 1$ vector of unknown parameters: $\\boldsymbol{\\theta} = [\\theta_0, \\theta_1, \\theta_2]^T = [\\ln(k_{\\text{eff}}), \\alpha, \\beta]^T$.\n- $\\boldsymbol{\\epsilon}$ is the $n \\times 1$ vector of errors.\n\nThe parameters are estimated using the method of Ordinary Least Squares (OLS), which finds the vector $\\hat{\\boldsymbol{\\theta}}$ that minimizes the sum of squared residuals, $\\mathrm{RSS} = \\sum_{i=1}^n \\epsilon_i^2 = \\boldsymbol{\\epsilon}^T\\boldsymbol{\\epsilon}$. The analytical solution to this minimization problem is given by the normal equations:\n$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}$$\nThe estimates for the reaction orders are the second and third elements of this vector: $\\hat{\\alpha} = \\hat{\\theta}_1$ and $\\hat{\\beta} = \\hat{\\theta}_2$.\n\nThe uncertainty of these estimates is quantified by their standard errors. Under the standard assumptions of OLS (including homoscedasticity of errors), the covariance matrix of the estimator $\\hat{\\boldsymbol{\\theta}}$ is:\n$$\\mathrm{Cov}(\\hat{\\boldsymbol{\\theta}}) = \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}$$\nThe variance of the errors, $\\sigma^2$, is typically unknown and must be estimated from the data. An unbiased estimator for $\\sigma^2$ is the mean squared error:\n$$\\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n-p}$$\nwhere $\\mathrm{RSS} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\theta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\theta}})$ is the residual sum of squares calculated using the estimated parameters $\\hat{\\boldsymbol{\\theta}}$. The denominator $n-p$ represents the degrees of freedom for the error term. For this problem, each dataset has $n=9$ experiments and we are estimating $p=3$ parameters, so the degrees of freedom are $9-3=6$.\n\nThe estimated covariance matrix is then $\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol{\\theta}}) = \\hat{\\sigma}^2 (\\mathbf{X}^T \\mathbf{X})^{-1}$. The standard errors of the individual parameter estimates are the square roots of the diagonal elements of this matrix. Specifically, for $\\hat{\\alpha}$ and $\\hat{\\beta}$:\n$$\\sigma_{\\hat{\\alpha}} = \\sqrt{\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol{\\theta}})_{2,2}}$$\n$$\\sigma_{\\hat{\\beta}} = \\sqrt{\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol{\\theta}})_{3,3}}$$\nThese standard errors quantify the precision of the parameter estimates. For Dataset D1, which is constructed to be perfectly consistent with the power-law model, the residuals and thus $\\hat{\\sigma}^2$ will be effectively zero, leading to zero uncertainty. For Dataset D2, which contains noise, $\\hat{\\sigma}^2$ will be non-zero, yielding non-zero uncertainties. For Dataset D3, the near-collinearity between the intercept column and the $\\ln([\\mathrm{B}]_0)$ column in $\\mathbf{X}$ is expected to make the matrix $(\\mathbf{X}^T \\mathbf{X})$ ill-conditioned, which will manifest as a large variance and thus a large standard error $\\sigma_{\\hat{\\beta}}$, correctly reflecting the difficulty in precisely estimating $\\beta$ from the given data.\n\nThe computational procedure for each dataset is to:\n1. Construct the vector $\\mathbf{Y}$ and matrix $\\mathbf{X}$ from the natural logarithms of the provided data.\n2. Compute the parameter estimates $\\hat{\\boldsymbol{\\theta}}$ using the OLS formula.\n3. Calculate the residuals and the RSS, then estimate the error variance $\\hat{\\sigma}^2$.\n4. Compute the covariance matrix $\\widehat{\\mathrm{Cov}}(\\hat{\\boldsymbol{\\theta}})$.\n5. Extract the square roots of the second and third diagonal elements to find $\\sigma_{\\hat{\\alpha}}$ and $\\sigma_{\\hat{\\beta}}$.\n6. Report the required values $[\\hat{\\alpha}, \\hat{\\beta}, \\sigma_{\\hat{\\alpha}}, \\sigma_{\\hat{\\beta}}]$ rounded to the specified precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all datasets and print the final result.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    d1 = [\n        (0.10, 0.05, 1.767766953e-5),\n        (0.10, 0.10, 2.500000000e-5),\n        (0.10, 0.20, 3.535533906e-5),\n        (0.20, 0.05, 5.000000000e-5),\n        (0.20, 0.10, 7.071067810e-5),\n        (0.20, 0.20, 1.000000000e-4),\n        (0.50, 0.05, 1.976423538e-4),\n        (0.50, 0.10, 2.795084972e-4),\n        (0.50, 0.20, 3.952847075e-4),\n    ]\n\n    d2 = [\n        (0.02, 0.02, 6.72e-7),\n        (0.02, 0.10, 3.136e-6),\n        (0.02, 0.30, 9.696e-6),\n        (0.05, 0.02, 3.88e-6),\n        (0.05, 0.10, 2.04e-5),\n        (0.05, 0.30, 5.76e-5),\n        (0.10, 0.02, 1.648e-5),\n        (0.10, 0.10, 8.32e-5),\n        (0.10, 0.30, 2.28e-4),\n    ]\n\n    d3 = [\n        (0.050, 0.095, 7.125e-5),\n        (0.050, 0.100, 7.575e-5),\n        (0.050, 0.105, 7.79625e-5),\n        (0.100, 0.095, 1.4535e-4),\n        (0.100, 0.100, 1.5000e-4),\n        (0.100, 0.105, 1.5435e-4),\n        (0.200, 0.095, 2.8785e-4),\n        (0.200, 0.100, 2.9700e-4),\n        (0.200, 0.105, 3.2445e-4),\n    ]\n\n    test_cases = [d1, d2, d3]\n\n    results = []\n    for case in test_cases:\n        result = analyze_kinetics_data(case)\n        results.append(result)\n\n    # Format the final output string to match [[...],[...],[...]]\n    # using list comprehensions and string formatting\n    formatted_results = [\n        \"[\" + \",\".join([f\"{val:.4f}\" for val in res]) + \"]\"\n        for res in results\n    ]\n    final_output_string = \"[\" + \",\".join(formatted_results) + \"]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\ndef analyze_kinetics_data(data):\n    \"\"\"\n    Performs Ordinary Least Squares regression to find reaction orders and their standard errors.\n    \n    Args:\n        data (list): A list of tuples, where each tuple is ([A]0, [B]0, r0).\n        \n    Returns:\n        list: A list containing [alpha_hat, beta_hat, sigma_alpha, sigma_beta].\n    \"\"\"\n    # 1. Extract data and perform natural log transformation.\n    # The data is structured as (concentration_A, concentration_B, rate).\n    conc_a = np.array([d[0] for d in data])\n    conc_b = np.array([d[1] for d in data])\n    rates = np.array([d[2] for d in data])\n\n    log_a = np.log(conc_a)\n    log_b = np.log(conc_b)\n    log_rates = np.log(rates)\n\n    # 2. Construct the design matrix X and the response vector Y.\n    # The linear model is ln(r) = ln(k_eff) + alpha*ln([A]) + beta*ln([B]).\n    # The parameters are [ln(k_eff), alpha, beta].\n    n = len(data)\n    X = np.column_stack([np.ones(n), log_a, log_b])\n    Y = log_rates\n    \n    # The number of parameters (p) is the number of columns in X.\n    p = X.shape[1]\n\n    # 3. Estimate parameters using the OLS normal equation: beta = (X^T * X)^-1 * X^T * Y.\n    # Using np.linalg.solve is more numerically stable than direct inversion.\n    XTX = X.T @ X\n    XTY = X.T @ Y\n    # beta_hat will be a vector [intercept, alpha_hat, beta_hat]\n    beta_hat = np.linalg.solve(XTX, XTY)\n    \n    alpha_hat = beta_hat[1]\n    beta_hat_val = beta_hat[2]\n\n    # 4. Compute the standard errors of the parameter estimates.\n    # Calculate residuals.\n    Y_hat = X @ beta_hat\n    residuals = Y - Y_hat\n    \n    # Calculate Residual Sum of Squares (RSS).\n    RSS = np.sum(residuals**2)\n    \n    # Degrees of freedom for error term.\n    dof = n - p\n    \n    # Estimate the error variance (sigma^2).\n    # If dof <= 0, we cannot estimate variance. Not an issue for this problem's data.\n    if dof > 0:\n        sigma_sq_hat = RSS / dof\n    else:\n        # This case is not expected to be reached.\n        sigma_sq_hat = 0\n        \n    # Handle the perfect fit case (RSS is zero or very close to it).\n    if sigma_sq_hat < 1e-20: # A small tolerance for floating point inaccuracies.\n        sigma_alpha = 0.0\n        sigma_beta = 0.0\n    else:\n        # Compute the covariance matrix of parameters: sigma^2 * (X^T * X)^-1.\n        cov_beta = sigma_sq_hat * np.linalg.inv(XTX)\n        \n        # Standard errors are the square root of the diagonal elements of the covariance matrix.\n        std_errors = np.sqrt(np.diag(cov_beta))\n        \n        sigma_alpha = std_errors[1]\n        sigma_beta = std_errors[2]\n\n    # 5. Return the results. The problem asks for values, not rounded strings yet.\n    # Rounding will be handled in the final print statement.\n    return [alpha_hat, beta_hat_val, sigma_alpha, sigma_beta]\n\nsolve()\n```", "id": "2665183"}, {"introduction": "While initial rates provide a snapshot of a reaction, analyzing the full concentration profile over time offers a more complete picture. This exercise transitions from the differential form of the rate law to its integrated form, which describes the evolution of reactant concentration throughout the reaction. You will learn to linearize these integrated rate laws for first- and second-order reactions, fit them to time-course data, and formally discriminate between these competing hypotheses using statistical tools like the Akaike Information Criterion (AIC) [@problem_id:2665160].", "problem": "Consider an irreversible decay of species A in a closed, well-stirred, isothermal system with no volume change. The fundamental base for modeling the time evolution of the concentration $[\\mathrm{A}]$ is the deterministic rate law definition that the rate $-\\dfrac{d[\\mathrm{A}]}{dt}$ equals a product of a rate constant and a concentration dependence whose power (the reaction order) must be inferred from data. Assume either a first-order law where $-\\dfrac{d[\\mathrm{A}]}{dt}$ is proportional to $[\\mathrm{A}]$, or a second-order law where $-\\dfrac{d[\\mathrm{A}]}{dt}$ is proportional to $[\\mathrm{A}]^2$. Starting strictly from this base, derive the appropriate integrated relations that allow linear regression using transformations of $[\\mathrm{A}]$ and $t$ and justify the use of ordinary least squares on those linearized variables.\n\nYour task is to write a program that, for each provided dataset, performs the following steps in a model-comparison framework using only the linearized integrated forms you derive:\n\n- Construct two linear regressions: one appropriate to the first-order hypothesis and another appropriate to the second-order hypothesis.\n- For each linear fit, compute the residual vector, the residual sum of squares $RSS$, and the coefficient of determination (the coefficient of determination is denoted $R^2$).\n- Compute the Akaike Information Criterion (AIC) under a Gaussian, independent, identically distributed error model on the linearized variable with $p$ equal to $2$ parameters (slope and intercept): $$AIC = n \\ln(RSS/n) + 2p$$ where $n$ is the number of data points and $RSS$ is the residual sum of squares on the linearized variable.\n- Select the order by the lowest $AIC$. If the absolute difference in $AIC$ between the two models is less than $10^{-8}$, break the tie by selecting the model with the higher $R^2$. If still tied, select first-order.\n- From the selected model, extract the rate constant $k$ from the regression slope and compute the half-life using the appropriate formula implied by your derivation. Use the initial concentration $[\\mathrm{A}]_0$ taken as the first concentration measurement at $t=0$ where needed. All time inputs are in seconds, all concentration inputs are in moles per liter, and you must report half-life in seconds. The units of $k$ must be consistent with the chosen order: $\\mathrm{s}^{-1}$ for first-order and $\\mathrm{L} \\cdot \\mathrm{mol}^{-1} \\cdot \\mathrm{s}^{-1}$ for second-order.\n- For the dataset that contains two experiments at different temperatures $T$, first perform the above model selection and parameter estimation at each temperature separately. Then, using the two estimated rate constants $k(T_1)$ and $k(T_2)$ and temperatures $T_1$ and $T_2$ (in kelvin), compute the activation energy $E_\\mathrm{a}$ implied by the Arrhenius relation you derive from first principles. Report $E_\\mathrm{a}$ in $\\mathrm{J} \\cdot \\mathrm{mol}^{-1}$.\n\nImportant mathematical and implementation requirements:\n\n- All regression, residual, and information-criterion calculations must be carried out in the linearized variable space that follows from your integrated forms, not by non-linear regression in the original variables.\n- The program must round all reported floating-point outputs to six significant figures.\n- The gas constant to be used in any temperature dependence calculation is $R = 8.314462618 \\ \\mathrm{J} \\cdot \\mathrm{mol}^{-1} \\cdot \\mathrm{K}^{-1}$.\n\nTest suite (each dataset consists of time $t$ in seconds and concentration $[\\mathrm{A}]$ in moles per liter; all numbers below are exact and must be used as given):\n\n- Dataset $1$ (single-temperature experiment): \n  - $t = [0, 5, 10, 15, 20, 25, 30]$\n  - $[\\mathrm{A}] = [0.820, 0.401, 0.189, 0.095, 0.045, 0.021, 0.011]$\n- Dataset $2$ (single-temperature experiment):\n  - $t = [0, 20, 40, 60, 80, 100, 120]$\n  - $[\\mathrm{A}] = [0.500, 0.386, 0.310, 0.266, 0.226, 0.200, 0.179]$\n- Dataset $3$ (single-temperature experiment; short-time, low-curvature edge case):\n  - $t = [0, 30, 60, 90, 120]$\n  - $[\\mathrm{A}] = [1.200, 0.889, 0.661, 0.491, 0.362]$\n- Dataset $4$ (paired-temperature experiments for Arrhenius analysis; two separate experiments):\n  - Experiment $4\\mathrm{a}$ at $T_1 = 295 \\ \\mathrm{K}$:\n    - $t = [0, 300, 600, 900, 1200]$\n    - $[\\mathrm{A}] = [1.000, 0.497, 0.240, 0.121, 0.058]$\n  - Experiment $4\\mathrm{b}$ at $T_2 = 325 \\ \\mathrm{K}$:\n    - $t = [0, 60, 120, 180, 240]$\n    - $[\\mathrm{A}] = [1.000, 0.262, 0.069, 0.017, 0.005]$\n\nFinal output format:\n\n- For Datasets $1$–$3$, output a list with the following seven entries in this exact order:\n  - Selected order as an integer ($1$ or $2$),\n  - Estimated $k$ (float, rounded to six significant figures),\n  - Estimated half-life in seconds (float, rounded to six significant figures),\n  - $R^2$ for the first-order linearized fit (float, rounded to six significant figures),\n  - $R^2$ for the second-order linearized fit (float, rounded to six significant figures),\n  - $AIC$ for the first-order linearized fit (float, rounded to six significant figures),\n  - $AIC$ for the second-order linearized fit (float, rounded to six significant figures).\n- For Dataset $4$, output a list with the following fifteen entries in this exact order:\n  - Selected order at $T_1$ as an integer,\n  - Estimated $k(T_1)$ (float),\n  - Estimated half-life at $T_1$ in seconds (float),\n  - $R^2$ for the first-order linearized fit at $T_1$ (float),\n  - $R^2$ for the second-order linearized fit at $T_1$ (float),\n  - $AIC$ for the first-order linearized fit at $T_1$ (float),\n  - $AIC$ for the second-order linearized fit at $T_1$ (float),\n  - Selected order at $T_2$ as an integer,\n  - Estimated $k(T_2)$ (float),\n  - Estimated half-life at $T_2$ in seconds (float),\n  - $R^2$ for the first-order linearized fit at $T_2$ (float),\n  - $R^2$ for the second-order linearized fit at $T_2$ (float),\n  - $AIC$ for the first-order linearized fit at $T_2$ (float),\n  - $AIC$ for the second-order linearized fit at $T_2$ (float),\n  - Estimated activation energy $E_\\mathrm{a}$ in $\\mathrm{J} \\cdot \\mathrm{mol}^{-1}$ (float).\n- Your program should produce a single line of output containing the results for all four datasets as a comma-separated list of the per-dataset lists, enclosed in a single pair of square brackets, with no extra spaces or text, for example: $[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] ]$, but without spaces.\n\nAll numeric outputs must be rounded to six significant figures. All times are in seconds, all concentrations are in $\\mathrm{mol} \\cdot \\mathrm{L}^{-1}$, $k$ must be in $\\mathrm{s}^{-1}$ for first-order or $\\mathrm{L} \\cdot \\mathrm{mol}^{-1} \\cdot \\mathrm{s}^{-1}$ for second-order as appropriate, and $E_\\mathrm{a}$ must be in $\\mathrm{J} \\cdot \\mathrm{mol}^{-1}$.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in chemical kinetics, requiring the determination of reaction order and associated parameters from experimental data. It is internally consistent, provides all necessary information, and adheres to fundamental principles of physical chemistry. The problem is therefore deemed valid and a full solution shall be provided.\n\n### Derivation of Integrated Rate Laws and Linear Forms\n\nWe begin from the fundamental deterministic rate law for the irreversible decay of a species $\\mathrm{A}$:\n$$-\\frac{d[\\mathrm{A}]}{dt} = k [\\mathrm{A}]^n$$\nwhere $[\\mathrm{A}]$ is the concentration of species $\\mathrm{A}$, $t$ is time, $k$ is the rate constant, and $n$ is the reaction order. We will consider the cases where $n=1$ and $n=2$.\n\n#### First-Order Kinetics ($n=1$)\nThe differential rate law for a first-order reaction is:\n$$-\\frac{d[\\mathrm{A}]}{dt} = k_1 [\\mathrm{A}]$$\nTo find the integrated rate law, we separate the variables and integrate from the initial condition $(t=0, [\\mathrm{A}]=[\\mathrm{A}]_0)$ to a later state $(t, [\\mathrm{A}])$:\n$$\\int_{[\\mathrm{A}]_0}^{[\\mathrm{A}]} \\frac{d[\\mathrm{A}]'}{[\\mathrm{A}]'} = -\\int_0^t k_1 dt'$$\nSolving the integrals yields:\n$$\\ln[\\mathrm{A}] - \\ln[\\mathrm{A}]_0 = -k_1 t$$\nThis equation can be rearranged into a linear form, $y = mx + c$:\n$$\\ln[\\mathrm{A}] = -k_1 t + \\ln[\\mathrm{A}]_0$$\nHere, the dependent variable is $y = \\ln[\\mathrm{A}]$, the independent variable is $x = t$, the slope is $m = -k_1$, and the intercept is $c = \\ln[\\mathrm{A}]_0$. A plot of $\\ln[\\mathrm{A}]$ versus $t$ will be a straight line for a first-order reaction, and the rate constant $k_1$ can be determined from the negative of the slope.\n\nThe half-life, $t_{1/2}$, is the time at which $[\\mathrm{A}] = \\frac{1}{2}[\\mathrm{A}]_0$. Substituting this into the integrated rate law:\n$$\\ln\\left(\\frac{1}{2}[\\mathrm{A}]_0\\right) - \\ln[\\mathrm{A}]_0 = -k_1 t_{1/2}$$\n$$\\ln\\left(\\frac{1}{2}\\right) = -k_1 t_{1/2}$$\n$$-\\ln(2) = -k_1 t_{1/2}$$\n$$t_{1/2} = \\frac{\\ln(2)}{k_1}$$\nThe half-life for a first-order reaction is independent of the initial concentration.\n\n#### Second-Order Kinetics ($n=2$)\nThe differential rate law for a second-order reaction is:\n$$-\\frac{d[\\mathrm{A}]}{dt} = k_2 [\\mathrm{A}]^2$$\nSeparating variables and integrating gives:\n$$\\int_{[\\mathrm{A}]_0}^{[\\mathrm{A}]} \\frac{d[\\mathrm{A}]'}{[\\mathrm{A}]'^2} = -\\int_0^t k_2 dt'$$\n$$\\left[ -\\frac{1}{[\\mathrm{A}]'} \\right]_{[\\mathrm{A}]_0}^{[\\mathrm{A}]} = -k_2 [t']_0^t$$\n$$-\\frac{1}{[\\mathrm{A}]} - \\left(-\\frac{1}{[\\mathrm{A}]_0}\\right) = -k_2 t$$\n$$\\frac{1}{[\\mathrm{A}]} - \\frac{1}{[\\mathrm{A}]_0} = k_2 t$$\nRearranging into the linear form $y = mx + c$:\n$$\\frac{1}{[\\mathrm{A}]} = k_2 t + \\frac{1}{[\\mathrm{A}]_0}$$\nHere, the dependent variable is $y = 1/[\\mathrm{A}]$, the independent variable is $x = t$, the slope is $m = k_2$, and the intercept is $c = 1/[\\mathrm{A}]_0$. A plot of $1/[\\mathrm{A}]$ versus $t$ will be linear for a second-order reaction, and the rate constant $k_2$ is equal to the slope.\n\nThe half-life, $t_{1/2}$, is found by setting $[\\mathrm{A}] = \\frac{1}{2}[\\mathrm{A}]_0$:\n$$\\frac{1}{\\frac{1}{2}[\\mathrm{A}]_0} - \\frac{1}{[\\mathrm{A}]_0} = k_2 t_{1/2}$$\n$$\\frac{2}{[\\mathrm{A}]_0} - \\frac{1}{[\\mathrm{A}]_0} = k_2 t_{1/2}$$\n$$t_{1/2} = \\frac{1}{k_2 [\\mathrm{A}]_0}$$\nThe half-life for a second-order reaction depends on the initial concentration.\n\n### Justification for Ordinary Least Squares (OLS)\nOrdinary least squares regression provides the best linear unbiased estimates for the model parameters (slope and intercept) under the Gauss-Markov assumptions. These assumptions require that the errors (residuals) in the dependent variable are uncorrelated, have zero mean, and have constant variance (homoscedasticity). The problem statement explicitly mandates the assumption of a \"Gaussian, independent, identically distributed error model on the linearized variable.\" This premise directly satisfies the conditions for the validity of OLS applied to the transformed variables ($y = \\ln[\\mathrm{A}]$ or $y = 1/[\\mathrm{A}]$). Therefore, the use of OLS on these linearized forms is justified by the constraints of the problem itself.\n\n### Model Selection and Arrhenius Relation\n\n#### Model Selection Framework\nFor each dataset, we will perform two linear regressions: one for the first-order hypothesis ($\\ln[\\mathrm{A}]$ vs. $t$) and one for the second-order hypothesis ($1/[\\mathrm{A}]$ vs. $t$). For each fit, we calculate the coefficient of determination, $R^2$, and the residual sum of squares, $RSS = \\sum(y_i - \\hat{y}_i)^2$, where $y_i$ are the observed transformed concentrations and $\\hat{y}_i$ are the values predicted by the linear fit.\n\nModel selection is based on the Akaike Information Criterion (AIC), which balances model fit ($RSS$) with model complexity. For a model with $p$ parameters fit to $n$ data points, the AIC is given by:\n$$AIC = n \\ln\\left(\\frac{RSS}{n}\\right) + 2p$$\nIn our case, both linear models have $p=2$ parameters (slope and intercept). The model with the lower $AIC$ value is preferred. A tie-breaking rule based on $R^2$ is specified for cases where the absolute difference in $AIC$ is negligible ($< 10^{-8}$).\n\n#### Arrhenius Equation for Activation Energy\nThe temperature dependence of the rate constant $k$ is described by the Arrhenius equation:\n$$k(T) = A e^{-E_a / (RT)}$$\nwhere $E_a$ is the activation energy, $R$ is the universal gas constant, $T$ is the absolute temperature, and $A$ is the pre-exponential factor.\nTo determine $E_a$ from rate constants $k_1$ and $k_2$ measured at two different temperatures $T_1$ and $T_2$, we take the natural logarithm of the equation for each temperature:\n$$\\ln(k_1) = \\ln(A) - \\frac{E_a}{RT_1}$$\n$$\\ln(k_2) = \\ln(A) - \\frac{E_a}{RT_2}$$\nSubtracting the first equation from the second eliminates $\\ln(A)$:\n$$\\ln(k_2) - \\ln(k_1) = \\left( -\\frac{E_a}{RT_2} \\right) - \\left( -\\frac{E_a}{RT_1} \\right)$$\n$$\\ln\\left(\\frac{k_2}{k_1}\\right) = \\frac{E_a}{R} \\left( \\frac{1}{T_1} - \\frac{1}{T_2} \\right)$$\nSolving for the activation energy $E_a$ yields the two-point form:\n$$E_a = R \\frac{\\ln(k_2/k_1)}{\\frac{1}{T_1} - \\frac{1}{T_2}}$$\nThis formula will be used with the rate constants estimated for the experiments in Dataset $4$. The validity of this calculation rests on the assumption that the reaction order, and thus the units of $k$, are the same at both temperatures, which is a reasonable expectation for elementary reactions over a modest temperature range.\n\nThe program below implements this complete methodology.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import linregress\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the chemical kinetics problem by performing model selection and parameter estimation.\n    \"\"\"\n\n    # Define a helper function for rounding to six significant figures.\n    sf_round = lambda num: float(f\"{num:.5e}\") if num != 0 else 0.0\n\n    def analyze_dataset(t_data, conc_data):\n        \"\"\"\n        Analyzes a single concentration vs. time dataset to determine reaction order and parameters.\n        \"\"\"\n        t = np.array(t_data)\n        conc_A = np.array(conc_data)\n        n = len(t)\n        p = 2  # Number of parameters (slope, intercept)\n        conc_A0 = conc_A[0]\n\n        # --- First-Order Analysis ---\n        y1 = np.log(conc_A)\n        slope1, intercept1, r_val1, _, _ = linregress(t, y1)\n        \n        k1 = -slope1\n        r2_1 = r_val1**2\n        \n        y1_pred = slope1 * t + intercept1\n        rss1 = np.sum((y1 - y1_pred)**2)\n        # Avoid math.log(0) if the fit is perfect (rss1=0)\n        aic1 = n * math.log(rss1 / n) + 2 * p if rss1 > 0 else -np.inf\n        \n        t_half_1 = math.log(2) / k1 if k1 > 0 else float('inf')\n\n        # --- Second-Order Analysis ---\n        y2 = 1.0 / conc_A\n        slope2, intercept2, r_val2, _, _ = linregress(t, y2)\n        \n        k2 = slope2\n        r2_2 = r_val2**2\n        \n        y2_pred = slope2 * t + intercept2\n        rss2 = np.sum((y2 - y2_pred)**2)\n        aic2 = n * math.log(rss2 / n) + 2 * p if rss2 > 0 else -np.inf\n\n        t_half_2 = 1.0 / (k2 * conc_A0) if k2 > 0 else float('inf')\n\n        # --- Model Selection ---\n        selected_order = 0\n        k = 0.0\n        t_half = 0.0\n\n        if abs(aic1 - aic2) < 1e-8:\n            if r2_1 >= r2_2:  # Tie-break with R^2, and if still tied, pick order 1\n                selected_order = 1\n                k = k1\n                t_half = t_half_1\n            else:\n                selected_order = 2\n                k = k2\n                t_half = t_half_2\n        elif aic1 < aic2:\n            selected_order = 1\n            k = k1\n            t_half = t_half_1\n        else:\n            selected_order = 2\n            k = k2\n            t_half = t_half_2\n        \n        # Format results to six significant figures\n        k_out = sf_round(k)\n        t_half_out = sf_round(t_half)\n        r2_1_out = sf_round(r2_1)\n        r2_2_out = sf_round(r2_2)\n        aic1_out = sf_round(aic1)\n        aic2_out = sf_round(aic2)\n\n        return [selected_order, k_out, t_half_out, r2_1_out, r2_2_out, aic1_out, aic2_out], (k, selected_order)\n\n    # Test suite datasets\n    test_cases = [\n        # Dataset 1\n        {'t': [0, 5, 10, 15, 20, 25, 30], 'A': [0.820, 0.401, 0.189, 0.095, 0.045, 0.021, 0.011]},\n        # Dataset 2\n        {'t': [0, 20, 40, 60, 80, 100, 120], 'A': [0.500, 0.386, 0.310, 0.266, 0.226, 0.200, 0.179]},\n        # Dataset 3\n        {'t': [0, 30, 60, 90, 120], 'A': [1.200, 0.889, 0.661, 0.491, 0.362]},\n        # Dataset 4\n        {\n            'exp_a': {'T': 295, 't': [0, 300, 600, 900, 1200], 'A': [1.000, 0.497, 0.240, 0.121, 0.058]},\n            'exp_b': {'T': 325, 't': [0, 60, 120, 180, 240], 'A': [1.000, 0.262, 0.069, 0.017, 0.005]}\n        }\n    ]\n\n    gas_constant_R = 8.314462618\n    all_results = []\n\n    # Process datasets 1-3\n    for i in range(3):\n        case = test_cases[i]\n        results, _ = analyze_dataset(case['t'], case['A'])\n        all_results.append(results)\n\n    # Process dataset 4\n    case4 = test_cases[3]\n    T1 = case4['exp_a']['T']\n    t1_data = case4['exp_a']['t']\n    A1_data = case4['exp_a']['A']\n    \n    T2 = case4['exp_b']['T']\n    t2_data = case4['exp_b']['t']\n    A2_data = case4['exp_b']['A']\n\n    results_a, (k1_raw, order1) = analyze_dataset(t1_data, A1_data)\n    results_b, (k2_raw, order2) = analyze_dataset(t2_data, A2_data)\n\n    # Calculate activation energy Ea, assuming orders are consistent\n    Ea = 0.0\n    if order1 == order2 and k1_raw > 0 and k2_raw > 0:\n        Ea = gas_constant_R * math.log(k2_raw / k1_raw) / (1.0/T1 - 1.0/T2)\n    \n    results_4 = results_a + results_b + [sf_round(Ea)]\n    all_results.append(results_4)\n\n    # Format final output string\n    str_results = []\n    for res_list in all_results:\n        str_results.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    final_output = f\"[{','.join(str_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2665160"}, {"introduction": "A complete kinetic model must also account for the profound influence of temperature, which is captured by the Arrhenius equation. However, experimental data sometimes reveals a slight curvature in an Arrhenius plot, suggesting that the classical model may be insufficient. This advanced practice challenges you to compare the standard Arrhenius equation, $k(T) = A \\exp(-E_a/RT)$, with a modified form that includes a temperature-dependent pre-exponential factor, embodying the crucial principle of parsimony in scientific modeling by using the corrected Akaike Information Criterion (AICc) to justify the added complexity [@problem_id:2665169].", "problem": "You are given the task of building a program that, for several synthetic datasets of temperature-dependent rate constants that exhibit mild curvature on a semilogarithmic temperature plot, will: derive and fit two alternative kinetic models from first principles, evaluate their relative support using an information criterion grounded in likelihood theory, and select the more parsimonious representation when the evidence is indecisive.\n\nStarting point and modeling assumptions to be used and justified in your solution:\n- Begin from the physical picture of elementary reaction rates controlled by an activation barrier and the distribution of molecular energies. Use this to derive the classical temperature dependence of the rate constant and a principled extension in which the prefactor contains a weak temperature dependence capturing state density or collisional frequency effects.\n- Assume independent and identically distributed Gaussian measurement errors in the natural logarithm of the rate constant, i.e., if the true rate constant is denoted by $k(T)$ then the observations satisfy $\\ln k_{\\text{obs}}(T) = \\ln k(T) + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent across temperatures.\n- Under these assumptions, derive the linear-in-parameters regression representations appropriate to each model, and derive the finite-sample corrected Akaike Information Criterion (AICc) from the Gaussian log-likelihood to compare models with different numbers of parameters.\n- Explicitly state and use a tie-breaking rule: if the difference in AICc between the two models is smaller in magnitude than a specified threshold, choose the simpler model.\n\nScientific and numerical specifications:\n- Use absolute temperature in Kelvin, denoted $T$ in units of $\\mathrm{K}$.\n- Use the molar gas constant $R = 8.314462618\\,\\mathrm{J}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$.\n- Use activation energies in $\\mathrm{J}\\,\\mathrm{mol}^{-1}$ and rate constants in $\\mathrm{s}^{-1}$.\n- The classical form is the one that follows directly from an activation barrier with a temperature-independent prefactor obtained from collision theory or transition state theory under the assumption of weak temperature dependence. The extended form introduces a weak temperature exponent multiplying the prefactor.\n- Because the errors are assumed Gaussian in $\\ln k$, fit in logarithmic space (i.e., regress $\\ln k$ on the appropriate predictors).\n- For model selection, use the finite-sample corrected Akaike Information Criterion (AICc) appropriate to Gaussian residuals, derived from first principles. Denote the number of observations by $n$ and the number of free parameters by $p$ for each model.\n- Use the following tie-breaking rule: if the AICc difference between the models is smaller than $2$ in magnitude, choose the simpler model (the one with fewer parameters).\n\nSynthetic data generation for the test suite:\n- For each dataset, generate a temperature grid using an arithmetic progression. Then generate $\\ln k$ values from the chosen true model with added independent Gaussian noise of specified standard deviation applied in $\\ln k$ (equivalently, multiplicative lognormal noise in $k$). Finally, exponentiate to obtain $k$ in $\\mathrm{s}^{-1}$.\n- Use the fixed pseudorandom number generator seed $20240601$ to ensure determinism.\n- For clarity, you may assume the following parameterization for the synthetic truth when generating data: a prefactor parameter $A$ in $\\mathrm{s}^{-1}$, an activation energy $E_{\\mathrm{a}}$ in $\\mathrm{J}\\,\\mathrm{mol}^{-1}$, and a temperature exponent $n$ (dimensionless). The classical case corresponds to the special case $n=0$ of the extended model. Do not use this knowledge in selection; it is only for data generation.\n\nTest suite (five datasets), each specified by the $7$-tuple $\\left(A, E_{\\mathrm{a}}, n, T_{\\min}, T_{\\max}, N, \\sigma_{\\ln}\\right)$:\n- Dataset $1$: $\\left(1.0\\times 10^{12}, 75{,}000.0, 0.0, 550.0, 900.0, 10, 0.05\\right)$\n- Dataset $2$: $\\left(2.5\\times 10^{11}, 80{,}000.0, 0.6, 500.0, 1000.0, 12, 0.03\\right)$\n- Dataset $3$: $\\left(1.0\\times 10^{12}, 70{,}000.0, 0.05, 500.0, 900.0, 15, 0.02\\right)$\n- Dataset $4$: $\\left(5.0\\times 10^{10}, 65{,}000.0, 0.9, 700.0, 900.0, 6, 0.04\\right)$\n- Dataset $5$: $\\left(3.0\\times 10^{12}, 90{,}000.0, 0.0, 500.0, 1000.0, 25, 0.12\\right)$\n\nFor each dataset:\n- Generate $N$ temperatures uniformly spaced between $T_{\\min}$ and $T_{\\max}$ inclusive, in units of $\\mathrm{K}$.\n- Compute the true $\\ln k(T)$ from the chosen generative parameters and add independent Gaussian noise with standard deviation $\\sigma_{\\ln}$, then return $k(T)$ by exponentiation to enforce positivity.\n- Fit both the classical and the extended model to the noisy $(T, k(T))$ data in $\\ln$-space by linear least squares in the appropriate predictors, compute the residual sum of squares, and evaluate the AICc for each.\n- Implement the tie-breaking rule with threshold $2$ as stated above.\n\nOutput specification:\n- Use model code $0$ for the classical model (the simpler model) and model code $1$ for the extended model (the one with the additional temperature exponent).\n- Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the order of datasets $1$ through $5$ (e.g., $\\left[\\dots\\right]$).\n- No physical units are to appear in the output; the results are pure integers.\n\nYour program must be fully self-contained: hard-code the test suite above, fix the pseudorandom number generator seed to $20240601$, and print exactly one line in the format described. The algorithm must be derived from the assumptions and laws stated here and must not rely on any external files or user input.", "solution": "The problem requires a rigorous comparison of two kinetic models describing the temperature dependence of a reaction rate constant, $k(T)$. We must first derive these models from fundamental principles, then establish a statistical framework for fitting them to data and for model selection, and finally apply this framework to a series of synthetic datasets.\n\n**Derivation of Kinetic Models**\n\nThe temperature dependence of an elementary reaction rate is fundamentally governed by the energy distribution of molecules. For a reaction to occur, colliding molecules must possess sufficient energy to overcome an activation energy barrier, denoted $E_{\\mathrm{a}}$. According to the Boltzmann distribution, the fraction of molecules possessing at least this energy is proportional to the Boltzmann factor, $\\exp\\left(-\\frac{E_{\\mathrm{a}}}{RT}\\right)$, where $R$ is the molar gas constant and $T$ is the absolute temperature. The rate constant is therefore proportional to this term.\n\n**Model 0: The Classical Arrhenius Equation**\n\nThe simplest model, proposed by Arrhenius, assumes that all temperature dependence is contained within the exponential term. The proportionality constant, known as the pre-exponential factor $A$, is taken to be independent of temperature. This factor encapsulates terms related to collision frequency and steric factors. The resulting equation is:\n$$k(T) = A \\exp\\left(-\\frac{E_{\\mathrm{a}}}{RT}\\right)$$\nThis model has two parameters: $A$ and $E_{\\mathrm{a}}$. To facilitate linear regression, as required by the problem's error model specification, we take the natural logarithm:\n$$\\ln k(T) = \\ln A - \\frac{E_{\\mathrm{a}}}{R} \\left(\\frac{1}{T}\\right)$$\nThis equation is of the form $y = \\beta_0 + \\beta_1 x$, where the response variable is $y = \\ln k$, the predictor variable is $x = 1/T$, the intercept is $\\beta_0 = \\ln A$, and the slope is $\\beta_1 = -E_{\\mathrm{a}}/R$. This is a simple linear regression model with two coefficients.\n\n**Model 1: The Extended (Modified Arrhenius) Equation**\n\nMore sophisticated theories, such as collision theory or transition state theory, predict that the pre-exponential factor $A$ itself has a weak dependence on temperature. This dependence is typically a power-law relationship, $T^n$. Incorporating this leads to the extended or modified Arrhenius equation:\n$$k(T) = A T^n \\exp\\left(-\\frac{E_{\\mathrm{a}}}{RT}\\right)$$\nHere, $A$ is a temperature-independent constant, and $n$ is a small-valued exponent. This model has three parameters: $A$, $n$, and $E_{\\mathrm{a}}$. Again, we linearize the model by taking the natural logarithm:\n$$\\ln k(T) = \\ln A + n \\ln T - \\frac{E_{\\mathrm{a}}}{R} \\left(\\frac{1}{T}\\right)$$\nThis equation is of the form $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$, where the response is $y = \\ln k$, and the predictors are $x_1 = \\ln T$ and $x_2 = 1/T$. The regression coefficients are $\\beta_0 = \\ln A$, $\\beta_1 = n$, and $\\beta_2 = -E_{\\mathrm{a}}/R$. This is a multiple linear regression model with three coefficients.\n\n**Statistical Framework and Model Selection**\n\nThe problem specifies that the observational errors are independent and identically distributed Gaussian in the logarithm of the rate constant: $\\ln k_{\\text{obs}} = \\ln k_{\\text{true}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. Under this assumption, fitting the linearized equations using the method of least squares is equivalent to Maximum Likelihood Estimation.\n\nTo compare the two models, one of which (Model 1) is a more complex extension of the other (Model 0), we must use a criterion that balances goodness-of-fit against model complexity. The Akaike Information Criterion (AIC) is designed for this purpose. For small sample sizes, the finite-sample correction (AICc) is necessary and is given by:\n$$\\text{AICc} = N \\ln\\left(\\frac{\\text{RSS}}{N}\\right) + 2p + \\frac{2p(p+1)}{N-p-1}$$\nHere, $N$ is the number of data points, $\\text{RSS}$ is the residual sum of squares from the least-squares fit, and $p$ is the total number of estimated parameters in the model. The number of parameters $p$ must include the variance of the residuals, $\\sigma^2$, in addition to the regression coefficients.\n\nFor Model 0 (classical), there are two coefficients ($\\beta_0, \\beta_1$), so the total number of parameters is $p_0 = 2 + 1 = 3$.\nFor Model 1 (extended), there are three coefficients ($\\beta_0, \\beta_1, \\beta_2$), so the total number of parameters is $p_1 = 3 + 1 = 4$.\n\nThe model with the lower AICc value is considered to provide a better representation of the data. However, the principle of parsimony dictates that a more complex model should be chosen only if it offers a substantially better fit. The problem specifies a quantitative threshold for this decision.\n\n**Decision Rule**\n\nLet $\\text{AICc}_0$ and $\\text{AICc}_1$ be the AICc values for the classical and extended models, respectively.\n1. If $|\\text{AICc}_1 - \\text{AICc}_0| < 2$, the evidence in favor of the more complex model is insufficient. By the principle of parsimony, we select the simpler model (Model 0).\n2. If $|\\text{AICc}_1 - \\text{AICc}_0| \\ge 2$, we select the model with the lower AICc value.\n\nThis logic can be combined into a single condition: we select the more complex Model 1 if and only if it is substantially better, meaning its AICc is lower by at least $2$. Mathematically:\n- If $\\text{AICc}_0 - \\text{AICc}_1 \\ge 2$, select Model 1 (code 1).\n- Otherwise, select Model 0 (code 0).\n\n**Computational Algorithm**\n\nFor each supplied dataset, we execute the following procedure:\n1.  Set the pseudorandom number generator seed to `20240601` for reproducibility.\n2.  Generate $N$ temperatures $T_i$ as an arithmetic progression from $T_{\\min}$ to $T_{\\max}$.\n3.  For these temperatures, calculate the \"true\" values of $\\ln k_{\\text{true}, i}$ using the generative parameters $(A, E_{\\mathrm{a}}, n)$ provided for the test case.\n4.  Add Gaussian noise with standard deviation $\\sigma_{\\ln}$ to obtain the synthetic observations $\\ln k_{\\text{obs}, i}$.\n5.  **Fit Model 0:**\n    a. Construct the $N \\times 2$ design matrix $\\mathbf{X}_0$, with the first column being all ones (for the intercept) and the second being $1/T_i$.\n    b. Perform a linear least-squares regression of $\\ln k_{\\text{obs}, i}$ on $\\mathbf{X}_0$ to obtain the coefficients and the residual sum of squares, $\\text{RSS}_0$.\n    c. Calculate $\\text{AICc}_0$ using $N$, $\\text{RSS}_0$, and $p_0=3$.\n6.  **Fit Model 1:**\n    a. Construct the $N \\times 3$ design matrix $\\mathbf{X}_1$, with columns of ones, $\\ln T_i$, and $1/T_i$.\n    b. Perform a multiple linear least-squares regression of $\\ln k_{\\text{obs}, i}$ on $\\mathbf{X}_1$ to obtain $\\text{RSS}_1$.\n    c. Calculate $\\text{AICc}_1$ using $N$, $\\text{RSS}_1$, and $p_1=4$.\n7.  Apply the decision rule: if $\\text{AICc}_0 - \\text{AICc}_1 \\ge 2$, the chosen model is $1$. Otherwise, it is $0$.\n8.  Store the integer result and repeat for all datasets. The final output is the list of these integers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and fits two kinetic models to synthetic data, and selects the\n    best model using the Akaike Information Criterion (AICc).\n    \"\"\"\n    # Fixed pseudorandom number generator seed for determinism.\n    RNG_SEED = 20240601\n    np.random.seed(RNG_SEED)\n\n    # Molar gas constant in J mol^-1 K^-1.\n    R_CONST = 8.314462618\n\n    # Test suite: (A, E_a, n, T_min, T_max, N, sigma_ln)\n    test_cases = [\n        # Dataset 1: True model is classical (n=0)\n        (1.0e12, 75000.0, 0.0, 550.0, 900.0, 10, 0.05),\n        # Dataset 2: True model is extended (n=0.6)\n        (2.5e11, 80000.0, 0.6, 500.0, 1000.0, 12, 0.03),\n        # Dataset 3: True model is extended but with very small n\n        (1.0e12, 70000.0, 0.05, 500.0, 900.0, 15, 0.02),\n        # Dataset 4: True model is extended, very small sample size N\n        (5.0e10, 65000.0, 0.9, 700.0, 900.0, 6, 0.04),\n        # Dataset 5: True model is classical, high noise and large N\n        (3.0e12, 90000.0, 0.0, 500.0, 1000.0, 25, 0.12),\n    ]\n\n    results = []\n    for case in test_cases:\n        A, E_a, n_gen, T_min, T_max, N, sigma_ln = case\n\n        # 1. Generate synthetic data\n        T = np.linspace(T_min, T_max, N)\n        ln_k_true = np.log(A) + n_gen * np.log(T) - E_a / (R_CONST * T)\n        noise = np.random.normal(0, sigma_ln, N)\n        ln_k_obs = ln_k_true + noise\n        \n        y = ln_k_obs\n\n        # 2. Fit Classical Model (Model 0)\n        # Model: ln(k) = b0 + b1 * (1/T)\n        # p0 = 2 coefficients + 1 variance = 3 parameters\n        p0 = 3\n        X0 = np.vstack([np.ones(N), 1.0 / T]).T\n        _, rss0_array, _, _ = np.linalg.lstsq(X0, y, rcond=None)\n        \n        # Check if regression was underdetermined which shouldn't happen here\n        if rss0_array.size == 0 or N - p0 - 1 <= 0:\n            # Assign a very high AICc if model is invalid (e.g. N <= p)\n            aicc0 = np.inf\n        else:\n            rss0 = rss0_array[0]\n            # AICc formula for Gaussian residuals\n            aicc0 = N * np.log(rss0 / N) + 2 * p0 + (2 * p0 * (p0 + 1)) / (N - p0 - 1)\n\n        # 3. Fit Extended Model (Model 1)\n        # Model: ln(k) = b0 + b1*ln(T) + b2*(1/T)\n        # p1 = 3 coefficients + 1 variance = 4 parameters\n        p1 = 4\n        X1 = np.vstack([np.ones(N), np.log(T), 1.0 / T]).T\n        _, rss1_array, _, _ = np.linalg.lstsq(X1, y, rcond=None)\n        \n        if rss1_array.size == 0 or N - p1 - 1 <= 0:\n            # Assign a very high AICc if model is invalid\n            aicc1 = np.inf\n        else:\n            rss1 = rss1_array[0]\n            # AICc formula for Gaussian residuals\n            aicc1 = N * np.log(rss1 / N) + 2 * p1 + (2 * p1 * (p1 + 1)) / (N - p1 - 1)\n\n        # 4. Model Selection\n        # Select model 1 only if it is substantially better (AICc drops by >= 2).\n        # Otherwise, select the simpler model 0 due to parsimony.\n        delta_aicc_threshold = 2.0\n        if aicc0 - aicc1 >= delta_aicc_threshold:\n            selected_model = 1\n        else:\n            selected_model = 0\n            \n        results.append(selected_model)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2665169"}]}