{
    "hands_on_practices": [
        {
            "introduction": "To effectively design and debug deep learning models, one must understand how data is transformed at each layer. This first practice provides a hands-on calculation of feature map dimensions as they pass through a U-Net, a cornerstone architecture for medical segmentation. By tracing the data flow, you will gain a concrete understanding of how design choices like kernel size and padding impact the network's structure and the critical skip connections. ",
            "id": "4535986",
            "problem": "A radiomics pipeline uses a U-shaped convolutional neural network (U-Net) for automated lesion segmentation on two-dimensional medical images. Consider a network configured as follows on an input of spatial size $256 \\times 256$ (height and width):\n\n- Contracting path (encoder): At each of two levels, apply two two-dimensional convolutions with kernel size $3 \\times 3$, stride $1$, and zero padding $0$ (valid convolution), followed by max-pooling with kernel size $2 \\times 2$ and stride $2$.\n- Bottleneck: Apply two two-dimensional convolutions with kernel size $3 \\times 3$, stride $1$, and zero padding $0$.\n- Expanding path (decoder): At each of two levels, apply a transposed convolution with kernel size $2 \\times 2$, stride $2$, zero padding $0$, and output padding $0$, then concatenate (along the channel dimension) with the encoder’s corresponding pre-pooling feature map after center-cropping that encoder feature map as needed to match spatial dimensions. After each concatenation, apply two two-dimensional convolutions with kernel size $3 \\times 3$, stride $1$, and zero padding $0$.\n- Output head: Apply a single two-dimensional convolution with kernel size $1 \\times 1$ and stride $1$ to produce the segmentation logits.\n\nStarting from the standard definitions for the spatial size of the output of a discrete convolution, discrete max-pooling, and discrete transposed convolution, determine the spatial sizes of the feature maps after every operation along the encoder, bottleneck, and decoder, and verify the dimensional consistency at both skip concatenations by computing the necessary center-crop sizes per spatial dimension. Finally, compute the total number of pixels that are cropped across both skip connections (i.e., the sum of all removed pixels from both encoder feature maps due to center-cropping), and report that total as a single integer with no units.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded question about the architecture of a U-Net, which is a standard model in deep learning for image segmentation. All parameters are clearly defined, and the task is to perform a series of calculations based on established formulas for convolutional neural network operations.\n\nThe solution requires tracking the spatial dimensions of the feature maps through the network. We begin by defining the formulas for the output size of each type of layer used. Let the input feature map have a spatial size of $H_{\\text{in}} \\times W_{\\text{in}}$, and the output be $H_{\\text{out}} \\times W_{\\text{out}}$. For simplicity, as the operations are symmetric, we only show the calculation for one dimension, $H$.\n\n1.  **Two-Dimensional Convolution**: For a kernel of size $K \\times K$, stride $S$, and padding $P$, the output dimension is $H_{\\text{out}} = \\lfloor \\frac{H_{\\text{in}} + 2P - K}{S} \\rfloor + 1$. In our case, the convolutions are specified as \"valid,\" meaning zero padding ($P=0$), with a kernel size of $K=3$ and stride $S=1$. The formula simplifies to $H_{\\text{out}} = H_{\\text{in}} - 3 + 1 = H_{\\text{in}} - 2$.\n\n2.  **Max-Pooling**: For a kernel of size $K \\times K$ and stride $S$, the output dimension is $H_{\\text{out}} = \\lfloor \\frac{H_{\\text{in}} - K}{S} \\rfloor + 1$. In our case, the kernel size is $K=2$ and stride $S=2$. Assuming the input dimension is even, this simplifies to $H_{\\text{out}} = \\frac{H_{\\text{in}}}{2}$.\n\n3.  **Transposed Convolution**: For a kernel of size $K \\times K$, stride $S$, padding $P$, and output padding $O_p$, the output dimension is $H_{\\text{out}} = (H_{\\text{in}} - 1)S - 2P + K + O_p$. In our case, $K=2$, $S=2$, $P=0$, and $O_p=0$. The formula simplifies to $H_{\\text{out}} = (H_{\\text{in}} - 1) \\times 2 + 2 = 2H_{\\text{in}}$.\n\nWe now trace the spatial dimensions through the network, starting with the $256 \\times 256$ input.\n\n**Contracting Path (Encoder)**\n\n*   **Encoder Level 1:**\n    *   Input: $256 \\times 256$\n    *   First $3 \\times 3$ convolution: $256 - 2 = 254$. Size is $254 \\times 254$.\n    *   Second $3 \\times 3$ convolution: $254 - 2 = 252$. Size is $252 \\times 252$. This is the feature map for the first skip connection, let's call its size $S_1 = 252 \\times 252$.\n    *   $2 \\times 2$ max-pooling: $252 / 2 = 126$. Size is $126 \\times 126$.\n\n*   **Encoder Level 2:**\n    *   Input: $126 \\times 126$\n    *   First $3 \\times 3$ convolution: $126 - 2 = 124$. Size is $124 \\times 124$.\n    *   Second $3 \\times 3$ convolution: $124 - 2 = 122$. Size is $122 \\times 122$. This is the feature map for the second skip connection, let's call its size $S_2 = 122 \\times 122$.\n    *   $2 \\times 2$ max-pooling: $122 / 2 = 61$. Size is $61 \\times 61$.\n\n**Bottleneck**\n\n*   Input: $61 \\times 61$\n*   First $3 \\times 3$ convolution: $61 - 2 = 59$. Size is $59 \\times 59$.\n*   Second $3 \\times 3$ convolution: $59 - 2 = 57$. Size is $57 \\times 57$.\n\n**Expanding Path (Decoder)**\n\n*   **Decoder Level 1:**\n    *   Input from bottleneck: $57 \\times 57$\n    *   $2 \\times 2$ transposed convolution: $57 \\times 2 = 114$. Size is $114 \\times 114$. This is the upsampled feature map.\n    *   **Skip Connection 1 (from Encoder Level 2):** To concatenate, the feature map from the encoder ($S_2 = 122 \\times 122$) must be cropped to match the decoder's upsampled map ($114 \\times 114$).\n        *   Crop size per dimension: $122 - 114 = 8$ pixels.\n        *   Center-cropping removes $8/2 = 4$ pixels from each of the four sides.\n        *   Number of pixels removed from this feature map: $(122 \\times 122) - (114 \\times 114) = 14884 - 12996 = 1888$.\n    *   After concatenation, the size is $114 \\times 114$.\n    *   First $3 \\times 3$ convolution: $114 - 2 = 112$. Size is $112 \\times 112$.\n    *   Second $3 \\times 3$ convolution: $112 - 2 = 110$. Size is $110 \\times 110$.\n\n*   **Decoder Level 2:**\n    *   Input from Decoder Level 1: $110 \\times 110$\n    *   $2 \\times 2$ transposed convolution: $110 \\times 2 = 220$. Size is $220 \\times 220$. This is the upsampled feature map.\n    *   **Skip Connection 2 (from Encoder Level 1):** To concatenate, the feature map from the encoder ($S_1 = 252 \\times 252$) must be cropped to match the decoder's upsampled map ($220 \\times 220$).\n        *   Crop size per dimension: $252 - 220 = 32$ pixels.\n        *   Center-cropping removes $32/2 = 16$ pixels from each of the four sides.\n        *   Number of pixels removed from this feature map: $(252 \\times 252) - (220 \\times 220) = 63504 - 48400 = 15104$.\n    *   After concatenation, the size is $220 \\times 220$.\n    *   First $3 \\times 3$ convolution: $220 - 2 = 218$. Size is $218 \\times 218$.\n    *   Second $3 \\times 3$ convolution: $218 - 2 = 216$. Size is $216 \\times 216$.\n\n**Output Head**\n\n*   Input: $216 \\times 216$\n*   $1 \\times 1$ convolution ($K=1, S=1, P=0$): $H_{\\text{out}} = (216 + 2 \\times 0 - 1) / 1 + 1 = 216$. The size remains $216 \\times 216$.\n\n**Final Calculation**\n\nThe problem asks for the total number of pixels cropped across both skip connections. We sum the pixels removed at each concatenation step.\n\n*   Pixels cropped at the first skip connection (Decoder Level 1): $1888$.\n*   Pixels cropped at the second skip connection (Decoder Level 2): $15104$.\n\nTotal cropped pixels = $1888 + 15104 = 16992$.",
            "answer": "$$\n\\boxed{16992}\n$$"
        },
        {
            "introduction": "The learning process in a neural network is driven by minimizing a loss function through gradient descent. This exercise moves from network architecture to the mechanics of training by asking you to derive the gradient for the soft Dice loss, a crucial tool for segmentation tasks. Understanding this derivative reveals how the model's predictions are corrected at each step and highlights practical challenges like numerical stability. ",
            "id": "4535984",
            "problem": "In automated lesion segmentation for radiomics from Computed Tomography (CT) images using a Convolutional Neural Network (CNN) U-Net architecture, a common overlap-based loss is the soft Dice loss. Let there be $n$ voxels indexed by $i \\in \\{1,\\dots,n\\}$. For each voxel, denote the model’s predicted foreground probability by $p_i \\in [0,1]$ and the ground-truth label by $g_i \\in \\{0,1\\}$. The soft Dice coefficient $D(\\mathbf{p},\\mathbf{g})$ is defined by\n$$\nD(\\mathbf{p},\\mathbf{g}) \\equiv \\frac{2 \\sum_{i=1}^{n} p_i g_i}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i},\n$$\nand the soft Dice loss is $L(\\mathbf{p},\\mathbf{g}) \\equiv 1 - D(\\mathbf{p},\\mathbf{g})$.\n\nStarting only from these definitions and standard rules of differential calculus, perform the following:\n\n- Derive the analytic expression for the partial derivative $\\frac{\\partial L}{\\partial p_k}$ for an arbitrary fixed index $k \\in \\{1,\\dots,n\\}$.\n- Using your derived expression, reason about numerical stability when the denominator $\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i$ is small, including the edge case where $\\sum_{i=1}^{n} g_i = 0$. Based on this analysis, propose an $\\epsilon$-regularized soft Dice loss\n$$\nL_{\\epsilon}(\\mathbf{p},\\mathbf{g}) \\equiv 1 - \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon},\n$$\nwith $\\epsilon > 0$, and compute the closed-form expression for $\\frac{\\partial L_{\\epsilon}}{\\partial p_k}$.\n\nYour final answer must be a single closed-form analytic expression for $\\frac{\\partial L_{\\epsilon}}{\\partial p_k}$ using only $n$, $\\{p_i\\}_{i=1}^{n}$, $\\{g_i\\}_{i=1}^{n}$, and $\\epsilon$. Do not include units. Do not provide any numerical approximation.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the field of deep learning for medical image analysis, mathematically well-posed, and an objective request for a formal derivation. All components—the soft Dice loss, its $\\epsilon$-regularized variant, and the request for partial derivatives—are standard and well-defined concepts. The problem is self-contained and free of contradictions or ambiguities.\n\nWe will proceed with the derivation in three parts as requested by the problem statement.\n\nFirst, we derive the analytic expression for the partial derivative $\\frac{\\partial L}{\\partial p_k}$ of the soft Dice loss $L(\\mathbf{p},\\mathbf{g})$. The loss is defined as $L(\\mathbf{p},\\mathbf{g}) = 1 - D(\\mathbf{p},\\mathbf{g})$, where the soft Dice coefficient $D(\\mathbf{p},\\mathbf{g})$ is:\n$$\nD(\\mathbf{p},\\mathbf{g}) = \\frac{2 \\sum_{i=1}^{n} p_i g_i}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i}\n$$\nThe partial derivative of the loss $L$ with respect to a specific prediction $p_k$ is:\n$$\n\\frac{\\partial L}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 1 - D(\\mathbf{p},\\mathbf{g}) \\right) = - \\frac{\\partial D}{\\partial p_k}\n$$\nTo compute $\\frac{\\partial D}{\\partial p_k}$, we apply the quotient rule for derivatives. Let us define the numerator as $U = 2 \\sum_{i=1}^{n} p_i g_i$ and the denominator as $V = \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i$. The quotient rule states that $\\frac{\\partial}{\\partial p_k} \\left(\\frac{U}{V}\\right) = \\frac{\\frac{\\partial U}{\\partial p_k} V - U \\frac{\\partial V}{\\partial p_k}}{V^2}$.\n\nWe first find the partial derivatives of $U$ and $V$ with respect to $p_k$:\n$$\n\\frac{\\partial U}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 2 \\sum_{i=1}^{n} p_i g_i \\right) = 2 \\sum_{i=1}^{n} \\frac{\\partial p_i}{\\partial p_k} g_i = 2 g_k\n$$\nThis is because $\\frac{\\partial p_i}{\\partial p_k} = \\delta_{ik}$, where $\\delta_{ik}$ is the Kronecker delta, which is $1$ if $i=k$ and $0$ otherwise.\n$$\n\\frac{\\partial V}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right) = \\sum_{i=1}^{n} \\frac{\\partial p_i}{\\partial p_k} + 0 = 1\n$$\nThe term $\\sum_{i=1}^{n} g_i$ is constant with respect to any $p_k$.\n\nSubstituting these into the quotient rule:\n$$\n\\frac{\\partial D}{\\partial p_k} = \\frac{(2 g_k) \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right) - \\left( 2 \\sum_{i=1}^{n} p_i g_i \\right) (1)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2}\n$$\nTherefore, the partial derivative of the loss function $L$ is:\n$$\n\\frac{\\partial L}{\\partial p_k} = - \\frac{2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right) - 2 \\sum_{i=1}^{n} p_i g_i}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2} = \\frac{2 \\sum_{i=1}^{n} p_i g_i - 2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2}\n$$\n\nSecond, we analyze the numerical stability of this expression. The denominator of the derivative is $\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i \\right)^2$. Numerical instability, specifically division by zero or a very small number leading to exploding gradients, occurs when this denominator approaches zero. This happens if both $\\sum_{i=1}^{n} p_i \\to 0$ and $\\sum_{i=1}^{n} g_i \\to 0$. The case $\\sum_{i=1}^{n} g_i = 0$ is particularly important; it corresponds to an image patch containing no foreground pixels (e.g., healthy tissue only). In this case, all $g_i=0$, which implies $g_k=0$ for any $k$.\nThe derivative numerator becomes $2 \\sum_{i=1}^{n} p_i (0) - 2 (0) \\left( \\sum_{i=1}^{n} p_i + 0 \\right) = 0$. The denominator becomes $\\left(\\sum_{i=1}^{n} p_i\\right)^2$. If the model correctly predicts no foreground, then $\\sum_{i=1}^{n} p_i \\to 0$, and the gradient becomes the indeterminate form $\\frac{0}{0}$. This is numerically unstable and can disrupt the training of a neural network.\n\nThe proposed solution, adding a small positive constant $\\epsilon$, is a standard technique known as smoothing. The $\\epsilon$-regularized soft Dice loss is given as:\n$$\nL_{\\epsilon}(\\mathbf{p},\\mathbf{g}) = 1 - \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon}\n$$\nThis modification ensures the denominator can never be zero (or smaller than $\\epsilon$), thus preventing division by zero and stabilizing the gradient calculation.\n\nThird, we compute the closed-form expression for $\\frac{\\partial L_{\\epsilon}}{\\partial p_k}$. The structure of the calculation is identical to the first part, but with the modified numerator and denominator.\n$$\n\\frac{\\partial L_{\\epsilon}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 1 - \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon} \\right) = - \\frac{\\partial}{\\partial p_k} \\left( \\frac{2 \\sum_{i=1}^{n} p_i g_i + \\epsilon}{\\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon} \\right)\n$$\nLet the regularized numerator be $U_{\\epsilon} = 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon$ and the regularized denominator be $V_{\\epsilon} = \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon$.\n\nWe compute their partial derivatives with respect to $p_k$:\n$$\n\\frac{\\partial U_{\\epsilon}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) = 2 g_k\n$$\n$$\n\\frac{\\partial V_{\\epsilon}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right) = 1\n$$\nApplying the quotient rule to $\\frac{U_{\\epsilon}}{V_{\\epsilon}}$:\n$$\n\\frac{\\partial}{\\partial p_k} \\left( \\frac{U_{\\epsilon}}{V_{\\epsilon}} \\right) = \\frac{\\frac{\\partial U_{\\epsilon}}{\\partial p_k} V_{\\epsilon} - U_{\\epsilon} \\frac{\\partial V_{\\epsilon}}{\\partial p_k}}{V_{\\epsilon}^2} = \\frac{(2 g_k) \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right) - \\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) (1)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^2}\n$$\nFinally, we obtain the partial derivative of the regularized loss function $L_{\\epsilon}$ by negating this expression:\n$$\n\\frac{\\partial L_{\\epsilon}}{\\partial p_k} = - \\frac{2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right) - \\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^2}\n$$\nThis expression can be rewritten by absorbing the negative sign into the numerator:\n$$\n\\frac{\\partial L_{\\epsilon}}{\\partial p_k} = \\frac{\\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) - 2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^2}\n$$\nThis is the required closed-form analytic expression for the gradient of the $\\epsilon$-regularized soft Dice loss.",
            "answer": "$$ \\boxed{ \\frac{\\left( 2 \\sum_{i=1}^{n} p_i g_i + \\epsilon \\right) - 2 g_k \\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)}{\\left( \\sum_{i=1}^{n} p_i + \\sum_{i=1}^{n} g_i + \\epsilon \\right)^{2}} } $$"
        },
        {
            "introduction": "A trained model is only as good as our ability to measure its performance. This final practice focuses on evaluation, guiding you through the process of computing key metrics—sensitivity, specificity, and precision—from a confusion matrix. You will not only calculate these values but also analyze how they are influenced by factors like decision thresholds and data imbalance, a critical skill for interpreting and reporting model results responsibly. ",
            "id": "4535965",
            "problem": "A biomedical image segmentation pipeline for Radiomics uses a U-shaped convolutional neural network (U-Net), a type of Convolutional Neural Network (CNN), to produce a per-voxel probability map for a binary task: lesion versus background. After applying a fixed probability threshold $t$ to the network output, a predicted binary mask is obtained and compared to a manual ground-truth mask, yielding a confusion matrix with counts of True Positive ($TP$), False Positive ($FP$), True Negative ($TN$), and False Negative ($FN$) voxels. Assume an axial slice contains $N = 100{,}000$ voxels, with ground-truth lesion prevalence $p = 0.02$, so there are $pN = 2{,}000$ lesion voxels and $(1 - p)N = 98{,}000$ background voxels. For threshold $t = 0.6$, suppose the resulting confusion matrix is $TP = 1{,}620$, $FP = 1{,}800$, $TN = 96{,}200$, and $FN = 380$.\n\nUsing the standard, first-principles definitions of binary classification rates for segmentation derived from the confusion matrix, compute the sensitivity, specificity, and precision for the threshold $t = 0.6$. Then, starting from those foundational definitions and without invoking any shortcut formulas provided to you, explain how changing the threshold $t$ and changing the prevalence $p$ affects these three metrics, emphasizing the role of the denominators in their definitions and conditioning on the true class where appropriate. Finally, report the harmonic mean of sensitivity and precision as your single numeric result, rounded to $4$ significant figures. Express the result as a dimensionless decimal number.",
            "solution": "The U-shaped convolutional neural network (U-Net) produces a probability for each voxel of belonging to the lesion class. Thresholding at $t = 0.6$ yields a predicted binary mask. The confusion matrix counts $TP$, $FP$, $TN$, and $FN$ summarize the joint outcomes between predictions and ground truth.\n\nFrom first principles of binary classification in segmentation, sensitivity (also called the true positive rate or recall) measures the conditional probability of predicting lesion given that the voxel is truly lesion. Specificity (true negative rate) measures the conditional probability of predicting background given that the voxel is truly background. Precision (positive predictive value) measures the conditional probability that a voxel is truly lesion given that it is predicted as lesion. These are defined using the confusion matrix as follows:\n- Sensitivity is the conditional probability $\\Pr(\\hat{Y} = 1 \\mid Y = 1)$, which is computed empirically as the fraction of true lesion voxels that are correctly predicted: \n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN}.\n$$\n- Specificity is the conditional probability $\\Pr(\\hat{Y} = 0 \\mid Y = 0)$, computed as the fraction of true background voxels that are correctly predicted:\n$$\n\\text{specificity} = \\frac{TN}{TN + FP}.\n$$\n- Precision is the conditional probability $\\Pr(Y = 1 \\mid \\hat{Y} = 1)$, computed as the fraction of predicted lesion voxels that are truly lesion:\n$$\n\\text{precision} = \\frac{TP}{TP + FP}.\n$$\n\nUsing the provided counts $TP = 1{,}620$, $FP = 1{,}800$, $TN = 96{,}200$, and $FN = 380$:\n1. Compute sensitivity:\n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN} = \\frac{1{,}620}{1{,}620 + 380} = \\frac{1{,}620}{2{,}000} = \\frac{81}{100} = 0.81.\n$$\n2. Compute specificity:\n$$\n\\text{specificity} = \\frac{TN}{TN + FP} = \\frac{96{,}200}{96{,}200 + 1{,}800} = \\frac{96{,}200}{98{,}000} = \\frac{481}{490} \\approx 0.981632653\\ldots\n$$\n3. Compute precision:\n$$\n\\text{precision} = \\frac{TP}{TP + FP} = \\frac{1{,}620}{1{,}620 + 1{,}800} = \\frac{1{,}620}{3{,}420} = \\frac{9}{19} \\approx 0.473684210\\ldots\n$$\n\nDependence on threshold $t$:\n- Thresholding maps probabilities to binary predictions by declaring a voxel lesion if its predicted probability exceeds $t$. As $t$ decreases, more voxels are labeled as lesion. From the definitions above:\n  - $TP$ generally increases (more true lesion voxels exceed the lower threshold), and $FN$ generally decreases, so the denominator $TP + FN$ is fixed by the ground truth; thus the sensitivity $\\frac{TP}{TP + FN}$ tends to increase as $t$ decreases.\n  - $FP$ generally increases (more background voxels cross the lower threshold spuriously), and $TN$ generally decreases, so specificity $\\frac{TN}{TN + FP}$ tends to decrease as $t$ decreases.\n  - Precision $\\frac{TP}{TP + FP}$ can either increase or decrease as $t$ changes, depending on the balance between increases in $TP$ versus increases in $FP$. If $FP$ grows faster than $TP$, precision decreases; if $TP$ grows faster than $FP$, precision increases.\nThese monotonicities arise because the denominators $TP + FN$ and $TN + FP$ are fixed by the ground-truth class sizes, while the numerators shift with threshold-induced decision changes.\n\nDependence on prevalence $p$:\n- Let $p = \\Pr(Y = 1)$ denote lesion prevalence and $1 - p = \\Pr(Y = 0)$ denote background prevalence. For a given classifier operating at a fixed threshold $t$, define the true positive rate $\\text{TPR} = \\Pr(\\hat{Y} = 1 \\mid Y = 1)$ and the false positive rate $\\text{FPR} = \\Pr(\\hat{Y} = 1 \\mid Y = 0)$. Then, in expectation over the population,\n$$\nTP \\approx pN \\cdot \\text{TPR}, \\quad FN \\approx pN \\cdot (1 - \\text{TPR}), \\quad FP \\approx (1 - p)N \\cdot \\text{FPR}, \\quad TN \\approx (1 - p)N \\cdot (1 - \\text{FPR}).\n$$\nThus,\n$$\n\\text{sensitivity} = \\frac{TP}{TP + FN} = \\frac{pN \\cdot \\text{TPR}}{pN \\cdot \\text{TPR} + pN \\cdot (1 - \\text{TPR})} = \\text{TPR},\n$$\nwhich is independent of $p$ because the denominator $TP + FN$ conditions on $Y = 1$ (the true lesion class). Similarly,\n$$\n\\text{specificity} = \\frac{TN}{TN + FP} = \\frac{(1 - p)N \\cdot (1 - \\text{FPR})}{(1 - p)N \\cdot (1 - \\text{FPR}) + (1 - p)N \\cdot \\text{FPR}} = 1 - \\text{FPR},\n$$\nwhich is also independent of $p$ because the denominator $TN + FP$ conditions on $Y = 0$ (the true background class). In contrast, precision depends on prevalence:\n$$\n\\text{precision} = \\frac{TP}{TP + FP} = \\frac{pN \\cdot \\text{TPR}}{pN \\cdot \\text{TPR} + (1 - p)N \\cdot \\text{FPR}} = \\frac{p \\cdot \\text{TPR}}{p \\cdot \\text{TPR} + (1 - p) \\cdot \\text{FPR}},\n$$\nwhich increases with $p$ (holding $\\text{TPR}$ and $\\text{FPR}$ fixed). Therefore, precision is sensitive to class prevalence, while sensitivity and specificity are not, due to their denominators conditioning on the true class.\n\nFinally, the requested single numeric result is the harmonic mean of sensitivity and precision. The harmonic mean $H$ of two positive numbers $a$ and $b$ is defined by\n$$\nH = \\frac{2}{\\frac{1}{a} + \\frac{1}{b}} = \\frac{2ab}{a + b}.\n$$\nSetting $a$ to the sensitivity and $b$ to the precision:\n$$\na = \\frac{81}{100}, \\quad b = \\frac{9}{19}.\n$$\nCompute\n$$\nH = \\frac{2ab}{a + b} = \\frac{2 \\cdot \\frac{81}{100} \\cdot \\frac{9}{19}}{\\frac{81}{100} + \\frac{9}{19}} = \\frac{\\frac{1458}{1900}}{\\frac{2439}{1900}} = \\frac{1458}{2439} = \\frac{162}{271} \\approx 0.597786\\ldots\n$$\nRounded to $4$ significant figures, the harmonic mean is $0.5978$.",
            "answer": "$$\\boxed{0.5978}$$"
        }
    ]
}