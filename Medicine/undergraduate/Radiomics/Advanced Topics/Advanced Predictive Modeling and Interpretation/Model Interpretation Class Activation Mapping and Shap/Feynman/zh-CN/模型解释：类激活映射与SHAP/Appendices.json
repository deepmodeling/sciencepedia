{
    "hands_on_practices": [
        {
            "introduction": "类激活图（Class Activation Mapping, CAM）是一种强大的可视化技术，它能揭示卷积神经网络在做出决策时关注图像的哪些区域。本练习将带你动手实践CAM的核心计算过程，从最后的卷积层特征图和分类层权重出发，一步步生成热力图。通过这个过程，你将直观地理解模型是如何将高级特征“定位”回输入空间的。",
            "id": "4551422",
            "problem": "在一个简化的放射组学场景中，考虑一个用于对医学图像进行分类的卷积神经网络，其最后一个卷积层产生两个大小为 $2\\times 2$ 的空间特征图 $f_1$ 和 $f_2$。类别 $c$ 的分类器在全局平均特征上是线性的，学习到的权重为 $w_1^c$ 和 $w_2^c$。假设，与类激活映射（Class Activation Mapping, CAM）中的标准用法一致，某个空间位置上的正类别证据与该位置上特征图的修正线性组合成正比，使用相同的类别权重，并且通过修正来抑制负证据，以强调对类别 $c$ 有正向贡献的区域。\n\n给定\n$f_1=\\begin{bmatrix}1 & 2 \\\\ 0 & 1\\end{bmatrix}$，$f_2=\\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}$，$w_1^c=2$ 和 $w_2^c=-1$。\n请按以下步骤进行：\n\n1. 根据卷积特征的线性和空间等变性的第一性原理，如上所述构建类别 $c$ 的 $2\\times 2$ 类激活图 $M_c$，包括对负值进行修正。\n\n2. 使用角点对齐的双线性插值法将 $M_c$ 上采样到一个 $4\\times 4$ 的网格（即 $2\\times 2$ 网格的角点映射到 $4\\times 4$ 网格的角点）。假设间距均匀，通过在一个轴上应用一维线性插值，然后在另一个轴上应用，来定义整数网格坐标 $(i,j)$（其中 $i,j\\in\\{0,1,2,3\\}$）处的插值。\n\n3. 在 $4\\times 4$ 的上采样图中，找出具有最大插值的三个网格坐标。如果在临界点出现平局，则按 $(i,j)$ 的字典序打破平局，优先选择较小的 $i$，然后是较小的 $j$。\n\n4. 将输入分辨率下 SHapley Additive exPlanations (SHAP) 的逐像素归因解释为与上采样的 CAM 成正比，并在整个 $4\\times 4$ 网格上归一化，使其总和为 $1$。也就是说，如果上采样的值为 $\\{v_{ij}\\}$，则定义 $s_{ij}=v_{ij}/\\sum_{p,q} v_{pq}$。\n\n计算在步骤 $3$ 中确定的三个坐标上的类 SHAP 归因的总和的精确值。将最终答案表示为最简分数。不要包含任何单位。",
            "solution": "用户希望我解决一个在简化的放射组学背景下，与类激活映射（CAM）和 SHAP 相关的多步骤问题。\n\n这是我的计划：\n1.  **验证问题陈述：** 我将仔细地执行验证步骤。\n    *   **步骤1：提取给定条件：** 我将列出所有提供的矩阵、权重和程序描述。\n    *   **步骤2：验证：** 我将检查其科学合理性、适定性、客观性等。该问题是一个基于 CAM 和 SHAP 原理的、定义明确的数学练习。它不违反任何科学定律，是自洽的，并使用精确的语言。步骤清晰明了。它是有效的。\n    *   **步骤3：结论：** 结论将是“有效”。\n\n2.  **解决问题（如果有效）：** 我将遵循问题陈述中概述的四个步骤。\n\n    *   **第1部分：构建类激活图（CAM），$M_c$。**\n        *   问题陈述 CAM 是“该位置上特征图的修正线性组合，使用相同的类别权重”。\n        *   每个空间位置 $(u, v)$ 上的线性组合由 $w_1^c f_1(u,v) + w_2^c f_2(u,v)$ 给出。\n        *   修正是指应用 ReLU 函数：$\\text{ReLU}(x) = \\max(0, x)$。\n        *   因此，$M_c(u,v) = \\text{ReLU}(w_1^c f_1(u,v) + w_2^c f_2(u,v))$。\n        *   给定值：\n            *   $f_1 = \\begin{bmatrix} 1  2 \\\\ 0  1 \\end{bmatrix}$\n            *   $f_2 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}$\n            *   $w_1^c = 2$\n            *   $w_2^c = -1$\n        *   我们先计算线性组合：$L = w_1^c f_1 + w_2^c f_2$。\n            *   $L = 2 \\begin{bmatrix} 1  2 \\\\ 0  1 \\end{bmatrix} + (-1) \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}$\n            *   $L = \\begin{bmatrix} 2  4 \\\\ 0  2 \\end{bmatrix} + \\begin{bmatrix} 0  -1 \\\\ -1  0 \\end{bmatrix}$\n            *   $L = \\begin{bmatrix} 2+0  4-1 \\\\ 0-1  2+0 \\end{bmatrix} = \\begin{bmatrix} 2  3 \\\\ -1  2 \\end{bmatrix}$\n        *   现在，应用修正得到 $M_c$：\n            *   $M_c = \\text{ReLU}(L) = \\begin{bmatrix} \\max(0, 2)  \\max(0, 3) \\\\ \\max(0, -1)  \\max(0, 2) \\end{bmatrix}$\n            *   $M_c = \\begin{bmatrix} 2  3 \\\\ 0  2 \\end{bmatrix}$\n\n    *   **第2部分：使用双线性插值将 $M_c$ 上采样到 $4\\times 4$ 网格。**\n        *   源网格是 $2 \\times 2$，我们称之为 $S$。其值为 $S_{00}=2, S_{10}=0, S_{01}=3, S_{11}=2$。\n        *   目标网格是 $4 \\times 4$，我们称之为 $D$。索引为 $(i,j)$，其中 $i, j \\in \\{0, 1, 2, 3\\}$。\n        *   问题指定了“角点对齐”。这意味着 $2 \\times 2$ 网格的角点直接映射到 $4 \\times 4$ 网格的角点。\n        *   源坐标 $(u, v)$ 可以是 $[0, 1] \\times [0, 1]$ 区间内的实数。\n        *   $S$ 的值位于整数坐标 $(0,0), (0,1), (1,0), (1,1)$。\n        *   目标坐标 $(i, j)$ 需要映射到源坐标 $(u, v)$。\n        *   对于一个 $W \\times H$ 的源和一个 $W' \\times H'$ 的目标，使用角点对齐：\n            *   $u = i \\times \\frac{W-1}{W'-1}$\n            *   $v = j \\times \\frac{H-1}{H'-1}$\n        *   这里，$W=2, H=2$ 且 $W'=4, H'=4$。\n            *   $u = i \\times \\frac{2-1}{4-1} = i/3$\n            *   $v = j \\times \\frac{2-1}{4-1} = j/3$\n        *   四个已知点是 $M_c(0,0)=2$, $M_c(1,0)=0$, $M_c(0,1)=3$, $M_c(1,1)=2$。\n        *   单位正方形内点 $(u,v)$ 的双线性插值通用公式为：\n            *   $V(u,v) = M_c(0,0)(1-u)(1-v) + M_c(1,0)u(1-v) + M_c(0,1)(1-u)v + M_c(1,1)uv$\n        *   我们代入我们的值：\n            *   $V(u,v) = 2(1-u)(1-v) + 0 \\cdot u(1-v) + 3(1-u)v + 2uv$\n            *   $V(u,v) = 2(1-u-v+uv) + 3v - 3uv + 2uv$\n            *   $V(u,v) = 2 - 2u - 2v + 2uv + 3v - 3uv + 2uv$\n            *   $V(u,v) = 2 - 2u + v + uv$\n        *   现在我需要为 $i,j \\in \\{0,1,2,3\\}$ 计算值 $v_{ij} = V(i/3, j/3)$。\n        *   $v_{ij} = 2 - 2(i/3) + (j/3) + (i/3)(j/3) = 2 - \\frac{2i}{3} + \\frac{j}{3} + \\frac{ij}{9}$。\n        *   我们来构建这个 $4 \\times 4$ 矩阵。这些值将是分母为9的分数。\n            *   $v_{ij} = \\frac{18 - 6i + 3j + ij}{9}$\n        *   $i=0$: $v_{0j} = \\frac{18 + 3j}{9} = 2 + \\frac{j}{3}$。\n            *   $j=0: v_{00} = 2 + 0 = 2 = 18/9$。\n            *   $j=1: v_{01} = 2 + 1/3 = 7/3 = 21/9$。\n            *   $j=2: v_{02} = 2 + 2/3 = 8/3 = 24/9$。\n            *   $j=3: v_{03} = 2 + 3/3 = 3 = 27/9$。\n        *   $i=1$: $v_{1j} = \\frac{18 - 6 + 3j + j}{9} = \\frac{12 + 4j}{9}$。\n            *   $j=0: v_{10} = 12/9 = 4/3$。\n            *   $j=1: v_{11} = 16/9$。\n            *   $j=2: v_{12} = 20/9$。\n            *   $j=3: v_{13} = 24/9 = 8/3$。\n        *   $i=2$: $v_{2j} = \\frac{18 - 12 + 3j + 2j}{9} = \\frac{6 + 5j}{9}$。\n            *   $j=0: v_{20} = 6/9 = 2/3$。\n            *   $j=1: v_{21} = 11/9$。\n            *   $j=2: v_{22} = 16/9$。\n            *   $j=3: v_{23} = 21/9 = 7/3$。\n        *   $i=3$: $v_{3j} = \\frac{18 - 18 + 3j + 3j}{9} = \\frac{6j}{9} = \\frac{2j}{3}$。\n            *   $j=0: v_{30} = 0$。\n            *   $j=1: v_{31} = 2/3 = 6/9$。\n            *   $j=2: v_{32} = 4/3 = 12/9$。\n            *   $j=3: v_{33} = 2 = 18/9$。\n        *   所以上采样的值图 $\\{v_{ij}\\}$ 是：\n            $$ V = \\frac{1}{9} \\begin{pmatrix}\n            18  21  24  27 \\\\\n            12  16  20  24 \\\\\n            6  11  16  21 \\\\\n            0  6  12  18\n            \\end{pmatrix} $$\n\n    *   **第3部分：找出最大的三个值及其坐标。**\n        *   我们列出（值，坐标 $(i, j)$）对，并首先按值降序排序，然后对平局按坐标的字典序排序。\n        *   分子的矩阵是 $\\begin{pmatrix} 18  21  24  27 \\\\ 12  16  20  24 \\\\ 6  11  16  21 \\\\ 0  6  12  18 \\end{pmatrix}$。\n        *   最大的分子是 $27$，在 $(0,3)$ 位置。值为 $v_{03} = 27/9 = 3$。\n        *   次大的分子是 $24$，出现在 $(0,2)$ 和 $(1,3)$。值为 $v_{02} = v_{13} = 24/9 = 8/3$。\n        *   再下一个大的分子是 $21$，出现在 $(0,1)$ 和 $(2,3)$。值为 $v_{01} = v_{23} = 21/9 = 7/3$。\n        *   根据排序规则：\n            1.  值 $27/9$，在 $(0,3)$。\n            2.  值 $24/9$。坐标为 $(0,2)$ 和 $(1,3)$。按字典序，$(0,2)  (1,3)$。所以，第二个坐标是 $(0,2)$。\n            3.  第三个坐标必须是 $(1,3)$，其值也为 $24/9$。\n        *   因此，具有最大插值的三个坐标是 $(0,3)$、$(0,2)$ 和 $(1,3)$。对应的值是 $v_{03}=3$、$v_{02}=8/3$ 和 $v_{13}=8/3$。\n\n    *   **第4部分：计算类 SHAP 归因的总和。**\n        *   一个像素 $(i,j)$ 的类 SHAP 归因是 $s_{ij} = v_{ij} / \\sum_{p,q} v_{pq}$。\n        *   首先，我们计算所有上采样值的总和 $\\sum_{p,q} v_{pq}$。这是矩阵 $V$ 中所有元素的总和。\n        *   分子总和：\n            第0行和：$18+21+24+27 = 90$\n            第1行和：$12+16+20+24 = 72$\n            第2行和：$6+11+16+21 = 54$\n            第3行和：$0+6+12+18 = 36$\n        *   分子总和 = $90 + 72 + 54 + 36 = 252$。\n        *   值的总和是 $\\sum_{p,q} v_{pq} = \\frac{252}{9} = 28$。\n        *   我们需要计算前三个坐标的归因总和：$S = s_{03} + s_{02} + s_{13}$。\n        *   $s_{03} = \\frac{v_{03}}{\\sum v} = \\frac{3}{28}$。\n        *   $s_{02} = \\frac{v_{02}}{\\sum v} = \\frac{8/3}{28} = \\frac{8}{3 \\times 28} = \\frac{8}{84} = \\frac{2}{21}$。\n        *   $s_{13} = \\frac{v_{13}}{\\sum v} = \\frac{8/3}{28} = \\frac{8}{3 \\times 28} = \\frac{2}{21}$。\n        *   总和是 $S = \\frac{3}{28} + \\frac{2}{21} + \\frac{2}{21} = \\frac{3}{28} + \\frac{4}{21}$。\n        *   $28 = 2^2 \\times 7$ 和 $21 = 3 \\times 7$ 的最小公倍数是 $2^2 \\times 3 \\times 7 = 84$。\n        *   $S = \\frac{3 \\times 3}{28 \\times 3} + \\frac{4 \\times 4}{21 \\times 4} = \\frac{9}{84} + \\frac{16}{84} = \\frac{9+16}{84} = \\frac{25}{84}$。\n        *   分子 $25 = 5^2$ 和分母 $84 = 2^2 \\times 3 \\times 7$ 没有公因数，所以分数已是最简分数。",
            "answer": "$$\n\\boxed{\\frac{25}{84}}\n$$"
        },
        {
            "introduction": "从图像层面的归因转向特征层面的归因，我们来探讨SHAP（SHapley Additive exPlanations）值。这个练习通过一个简单的线性模型揭示了一个深刻的见解：模型中两个具有相同权重的特征，其SHAP归因值可能完全不同。通过计算，你将理解SHAP值不仅取决于模型本身，还取决于特定样本的特征值与其背景分布的偏离程度。",
            "id": "4551432",
            "problem": "在一项放射组学研究中，根据从 CT 扫描中提取的两个定量特征构建了一个线性风险评分。将这些特征表示为 $X_1$ 和 $X_2$。训练队列具有以下属性：\n- 特征是统计独立的。\n- 均值为 $\\mu_1 = 0$ 和 $\\mu_2 = 0$。\n- 标准差为 $\\sigma_1 = 3$ 和 $\\sigma_2 = 1$。\n\n模型是线性函数 $f(\\mathbf{x}) = \\beta \\left(x_1 + x_2\\right)$，其中 $\\beta = 2$ 且截距为零。为了解释模型，我们考虑 Shapley 加性解释 (SHAP)，它通过合作博弈论中的 Shapley 值定义，其背景选择由上述经验训练分布给出。也就是说，对于特征的任何子集 $S$，联盟值为 $v(S) = \\mathbb{E}\\!\\left[f(X_1, X_2)\\,\\middle|\\, X_S = x_S\\right]$，其中期望是根据训练分布计算的。\n\n考虑一个特定患者，其每个测量特征值均比队列均值高一个标准差，即 $x_1 = \\mu_1 + \\sigma_1$ 和 $x_2 = \\mu_2 + \\sigma_2$。\n\n仅使用 Shapley 值的定义（即所有联盟边际贡献的平均值）和所述背景的统计特性，推导该患者的 SHAP 归因值 $\\phi_1$ 和 $\\phi_2$。然后计算差值 $\\Delta = \\phi_1 - \\phi_2$。将最终答案表示为精确数字（不要四舍五入）。答案没有单位。",
            "solution": "用户希望我解决以下问题：\n在一项放射组学研究中，根据从 CT 扫描中提取的两个定量特征构建了一个线性风险评分。将这些特征表示为 $X_1$ 和 $X_2$。训练队列具有以下属性：\n- 特征是统计独立的。\n- 均值为 $\\mu_1 = 0$ 和 $\\mu_2 = 0$。\n- 标准差为 $\\sigma_1 = 3$ 和 $\\sigma_2 = 1$。\n\n模型是线性函数 $f(\\mathbf{x}) = \\beta \\left(x_1 + x_2\\right)$，其中 $\\beta = 2$ 且截距为零。为了解释模型，我们考虑 Shapley 加性解释 (SHAP)，它通过合作博弈论中的 Shapley 值定义，其背景选择由上述经验训练分布给出。也就是说，对于特征的任何子集 $S$，联盟值为 $v(S) = \\mathbb{E}\\!\\left[f(X_1, X_2)\\,\\middle|\\, X_S = x_S\\right]$，其中期望是根据训练分布计算的。\n\n考虑一个特定患者，其每个测量特征值均比队列均值高一个标准差，即 $x_1 = \\mu_1 + \\sigma_1$ 和 $x_2 = \\mu_2 + \\sigma_2$。\n\n仅使用 Shapley 值的定义（即所有联盟边际贡献的平均值）和所述背景的统计特性，推导该患者的 SHAP 归因值 $\\phi_1$ 和 $\\phi_2$。然后计算差值 $\\Delta = \\phi_1 - \\phi_2$。将最终答案表示为精确数字（不要四舍五入）。答案没有单位。\n\n### 步骤 1：提取已知条件\n- 特征：$X_1$, $X_2$。\n- 训练分布的统计特性：\n  - $X_1$ 和 $X_2$ 是统计独立的。\n  - $X_1$ 的均值：$\\mathbb{E}[X_1] = \\mu_1 = 0$。\n  - $X_2$ 的均值：$\\mathbb{E}[X_2] = \\mu_2 = 0$。\n  - $X_1$ 的标准差：$\\sigma_1 = 3$。\n  - $X_2$ 的标准差：$\\sigma_2 = 1$。\n- 模型函数：$f(x_1, x_2) = \\beta(x_1 + x_2)$，其中 $\\beta = 2$，因此 $f(x_1, x_2) = 2(x_1 + x_2)$。\n- 联盟值函数的定义：$v(S) = \\mathbb{E}[f(X_1, X_2) | X_S = x_S]$。\n- 需要解释的特定实例（患者）：\n  - $x_1 = \\mu_1 + \\sigma_1 = 0 + 3 = 3$。\n  - $x_2 = \\mu_2 + \\sigma_2 = 0 + 1 = 1$。\n- 目标：计算 $\\phi_1$、$\\phi_2$ 和 $\\Delta = \\phi_1 - \\phi_2$。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据：** 该问题在可解释性人工智能（XAI）领域有充分的依据，特别侧重于 SHAP（Shapley 加性解释），这是一种广泛使用且理论上完善的方法。背景是放射组学，这是一个有效的医学图像分析领域。所有概念（统计独立性、均值、标准差、线性模型、条件期望）都是数学和统计学中的标准概念。\n- **适定性：** 该问题提供了计算唯一解所需的所有必要定义、常数和数据。模型、背景分布的统计特性以及需要解释的实例都得到了明确的定义。\n- **客观性：** 该问题以精确、客观、数学化的语言陈述，没有歧义或主观论断。\n\n该问题没有问题验证清单中列出的任何缺陷。这是应用数学和机器学习领域一个标准的、可解的问题。\n\n该问题有效。我现在开始解答。\n\n###\n对于一组包含 $N$ 个特征的集合，特征 $i$ 的 SHAP 值 $\\phi_i$ 是其对所有可能的特征联盟 $S$ 的边际贡献的加权平均值。公式如下：\n$$ \\phi_i(f, \\mathbf{x}) = \\sum_{S \\subseteq \\{X_1, \\dots, X_N\\} \\setminus \\{X_i\\}} \\frac{|S|!(N - |S| - 1)!}{N!} \\left[ v(S \\cup \\{X_i\\}) - v(S) \\right] $$\n在本问题中，我们有 $N=2$ 个特征，$X_1$ 和 $X_2$。特征集为 $F = \\{X_1, X_2\\}$。\n\n首先，我们必须计算所有可能的联盟 $S \\subseteq F$ 的值函数 $v(S)$。这些联盟是 $\\emptyset$、$\\{X_1\\}$、$\\{X_2\\}$ 和 $\\{X_1, X_2\\}$。值函数定义为 $v(S) = \\mathbb{E}[f(X_1, X_2) | X_S = x_S]$，其中期望是针对不在 $S$ 中的特征的背景分布计算的。需要解释的实例的特征值为 $x_1 = 3$ 和 $x_2 = 1$。\n\n1.  联盟 $S = \\emptyset$：\n    值函数是模型输出的无条件期望，即基准值 $\\phi_0$。\n    $$ v(\\emptyset) = \\mathbb{E}[f(X_1, X_2)] = \\mathbb{E}[2(X_1 + X_2)] $$\n    根据期望的线性性质：\n    $$ v(\\emptyset) = 2(\\mathbb{E}[X_1] + \\mathbb{E}[X_2]) = 2(0 + 0) = 0 $$\n\n2.  联盟 $S = \\{X_1\\}$：\n    值函数是在 $X_1$ 固定为其值 $x_1=3$ 的条件下的期望模型输出。\n    $$ v(\\{X_1\\}) = \\mathbb{E}[f(X_1, X_2) | X_1 = x_1] = \\mathbb{E}[2(X_1 + X_2) | X_1 = 3] $$\n    $$ v(\\{X_1\\}) = \\mathbb{E}[2(3 + X_2) | X_1 = 3] $$\n    由于 $X_1$ 和 $X_2$ 是统计独立的，因此 $X_2$ 的分布不受 $X_1$ 值的影响。\n    $$ v(\\{X_1\\}) = \\mathbb{E}[6 + 2X_2] = 6 + 2\\mathbb{E}[X_2] = 6 + 2(0) = 6 $$\n\n3.  联盟 $S = \\{X_2\\}$：\n    类似地，我们将 $X_2$ 固定为其值 $x_2=1$。\n    $$ v(\\{X_2\\}) = \\mathbb{E}[f(X_1, X_2) | X_2 = x_2] = \\mathbb{E}[2(X_1 + X_2) | X_2 = 1] $$\n    $$ v(\\{X_2\\}) = \\mathbb{E}[2(X_1 + 1) | X_2 = 1] $$\n    同样，由于独立性：\n    $$ v(\\{X_2\\}) = \\mathbb{E}[2X_1 + 2] = 2\\mathbb{E}[X_1] + 2 = 2(0) + 2 = 2 $$\n\n4.  联盟 $S = \\{X_1, X_2\\}$：\n    在这里，两个特征都固定为其值。常数的期望就是常数本身。这就是特定实例的模型输出。\n    $$ v(\\{X_1, X_2\\}) = \\mathbb{E}[f(X_1, X_2) | X_1 = 3, X_2 = 1] = f(3, 1) = 2(3 + 1) = 8 $$\n\n现在我们可以计算 SHAP 值 $\\phi_1$ 和 $\\phi_2$。\n\n对于特征 $X_1$，它可以加入的可能联盟是 $S=\\emptyset$ 和 $S=\\{X_2\\}$。当 $N=2$ 时，$\\phi_1$ 的公式是：\n$$ \\phi_1 = \\frac{0!(2-0-1)!}{2!} [v(\\{X_1\\}) - v(\\emptyset)] + \\frac{1!(2-1-1)!}{2!} [v(\\{X_1, X_2\\}) - v(\\{X_2\\})] $$\n$$ \\phi_1 = \\frac{1 \\cdot 1}{2} [v(\\{X_1\\}) - v(\\emptyset)] + \\frac{1 \\cdot 1}{2} [v(\\{X_1, X_2\\}) - v(\\{X_2\\})] $$\n代入计算出的值：\n$$ \\phi_1 = \\frac{1}{2} [6 - 0] + \\frac{1}{2} [8 - 2] = \\frac{6}{2} + \\frac{6}{2} = 3 + 3 = 6 $$\n\n对于特征 $X_2$，它可以加入的可能联盟是 $S=\\emptyset$ 和 $S=\\{X_1\\}$。当 $N=2$ 时，$\\phi_2$ 的公式是：\n$$ \\phi_2 = \\frac{0!(2-0-1)!}{2!} [v(\\{X_2\\}) - v(\\emptyset)] + \\frac{1!(2-1-1)!}{2!} [v(\\{X_1, X_2\\}) - v(\\{X_1\\})] $$\n$$ \\phi_2 = \\frac{1}{2} [v(\\{X_2\\}) - v(\\emptyset)] + \\frac{1}{2} [v(\\{X_1, X_2\\}) - v(\\{X_1\\})] $$\n代入计算出的值：\n$$ \\phi_2 = \\frac{1}{2} [2 - 0] + \\frac{1}{2} [8 - 6] = \\frac{2}{2} + \\frac{2}{2} = 1 + 1 = 2 $$\n\nSHAP 值为 $\\phi_1 = 6$ 和 $\\phi_2 = 2$。\n作为验证，SHAP 值的有效性属性指出，归因值的总和必须等于实例的预测值与基准值之间的差：$f(\\mathbf{x}) - \\mathbb{E}[f(X)] = \\phi_1 + \\phi_2$。\n这里，$f(3, 1) = 8$ 且 $\\mathbb{E}[f(X)] = v(\\emptyset) = 0$。因此，$8-0=8$。\nSHAP 值的总和是 $\\phi_1 + \\phi_2 = 6 + 2 = 8$。该属性成立。\n\n对于具有独立特征的线性模型，存在一个显著的简化方法。对于模型 $f(\\mathbf{x}) = \\sum_j w_j x_j + b$，特征 $i$ 的 SHAP 值为 $\\phi_i = w_i(x_i - \\mathbb{E}[X_i])$。在我们的例子中，$f(x_1, x_2) = 2x_1 + 2x_2$，所以 $w_1=2$，$w_2=2$，$b=0$。特征是独立的，且 $\\mathbb{E}[X_1]=0$ 和 $\\mathbb{E}[X_2]=0$。实例为 $x_1=3$，$x_2=1$。\n应用这个简化公式：\n$\\phi_1 = w_1(x_1 - \\mathbb{E}[X_1]) = 2(3 - 0) = 6$。\n$\\phi_2 = w_2(x_2 - \\mathbb{E}[X_2]) = 2(1 - 0) = 2$。\n这证实了从完整 Shapley 值定义中获得的结果。\n\n最后一步是计算差值 $\\Delta = \\phi_1 - \\phi_2$。\n$$ \\Delta = 6 - 2 = 4 $$\n差值为 $4$。标准差 $\\sigma_1=3$ 和 $\\sigma_2=1$ 用于确定特定患者的特征值，但除了设定 $x_1=3$ 和 $x_2=1$ 之外，它们不直接出现在最终的 SHAP 计算本身中。",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "在现实世界的放射组学数据中，特征之间往往存在相关性，这给模型解释带来了挑战。本练习在前一个练习的基础上更进一步，探讨了当特征不再独立时SHAP值的计算方式。你将通过计算区分“干预式”和“观察式”两种SHAP语义，并量化特征相关性对归因结果的具体影响，这是准确解释模型行为的关键一步。",
            "id": "4551492",
            "problem": "一个影像组学流程从肺结节的计算机断层扫描（CT）中提取两个标准化的定量特征：纹理熵 $X_{1}$ 和形状紧凑度 $X_{2}$。一个用于预测恶性评分的线性预测器被训练为 $f(x_{1},x_{2}) = w_{1} x_{1} + w_{2} x_{2} + b$。在临床模型解释中，类别激活图（CAM）为卷积神经网络突出显示空间区域，而 SHapley 加性解释（SHAP）则将特征的加性贡献分配给模型输出。在表格特征影像组学中，区分干预语义（缺失特征从其边际分布中独立采样）和观测语义（在给定观测特征的条件下，缺失特征根据其条件分布被积分排除）至关重要。\n\n考虑一个观测特征为 $x_{1} = 0.65$ 和 $x_{2} = 0.75$ 的单一患者。训练好的线性模型参数为 $w_{1} = 2.0$，$w_{2} = 1.5$ 和 $b = -0.2$。训练总体中的特征向量 $(X_{1},X_{2})$ 可以用一个二元高斯分布很好地建模，其均值为 $\\mu_{1} = 0.50$，$\\mu_{2} = 0.80$，标准差为 $\\sigma_{1} = 0.10$，$\\sigma_{2} = 0.15$，相关系数为 $\\rho = 0.60 \\neq 0$。假设多元高斯分布的标准性质和 SHAP 值的合作博弈论定义，并将 $f$ 视为支付函数。\n\n从第一性原理出发，计算该患者在干预和观测两种语义下特征 $X_{1}$ 和 $X_{2}$ 的 SHAP 值。然后，将 $X_{1}$ 的依赖引起的归因偏移量化为观测 SHAP 值和干预 SHAP 值之差：\n$$\\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}}.$$\n最终答案只提供 $\\Delta_{1}$。将最终数值答案四舍五入到四位有效数字。答案以纯数字形式表示，不带单位。",
            "solution": "### 步骤 1：提取已知条件\n-   **模型：** 恶性评分的线性预测器，$f(x_{1},x_{2}) = w_{1} x_{1} + w_{2} x_{2} + b$。\n-   **特征：** 纹理熵 $X_{1}$ 和形状紧凑度 $X_{2}$。\n-   **指定实例：** $x_1 = 0.65$，$x_2 = 0.75$。\n-   **模型参数：** $w_1 = 2.0$，$w_2 = 1.5$，$b = -0.2$。\n-   **数据分布：** $(X_{1},X_{2})$ 是一个二元高斯分布。\n    -   均值：$\\mu_1 = 0.50$，$\\mu_2 = 0.80$。\n    -   标准差：$\\sigma_1 = 0.10$，$\\sigma_2 = 0.15$。\n    -   相关系数：$\\rho = 0.60$。\n-   **任务：** 计算 $X_{1}$ 的依赖引起的归因偏移，定义为 $\\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}}$，其中 $\\phi_{1}^{\\mathrm{obs}}$ 和 $\\phi_{1}^{\\mathrm{int}}$ 分别是在观测和干预语义下 $X_{1}$ 的 SHAP 值。最终答案必须四舍五入到四位有效数字。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据：** 该问题牢固地建立在机器学习可解释性（SHAP 值）和统计学（多元高斯分布）的既定概念之上。干预性和观测性条件期望之间的区别是因果关系和模型解释文献中的一个关键课题。解释影像组学特征上的线性模型的场景在科学上是现实的。\n2.  **适定性：** 这是一个适定问题。它提供了所有必要的参数和一个清晰、明确的目标。其底层数学框架（SHAP 值的定义，高斯分布的性质）保证了唯一、稳定且有意义的解的存在。\n3.  **客观性：** 该问题使用精确、客观的语言和定量数据进行陈述。没有主观或基于观点的论断。\n\n### 步骤 3：结论与行动\n该问题是有效的。它在科学上是合理的、适定的和客观的，具有完整且一致的设置。我将继续进行完整解答。\n\n### 解题推导\n一个特征的 SHAP (SHapley Additive exPlanations) 值基于合作博弈论的原理，量化了该特征对特定实例模型预测的贡献。对于一个具有两个特征 $X_1$ 和 $X_2$ 的模型，在特定实例 $x = (x_1, x_2)$ 处特征 $X_1$ 的 SHAP 值由其在所有可能的特征子集（联盟）上的边际贡献的加权平均值给出：\n$$ \\phi_{1} = \\frac{1}{2} \\left[ (E[f(X) | X_1=x_1] - E[f(X)]) + (f(x_1, x_2) - E[f(X) | X_2=x_2]) \\right] $$\n其中 $f(x_1, x_2)$ 是完整实例的模型预测值，$E[f(X)]$ 是在数据分布上的基线期望预测值。干预语义和观测语义之间的区别在于条件期望 $E[f(X) | X_S = x_S]$ 的计算方式。\n\n模型是线性的：$f(X_1, X_2) = w_1 X_1 + w_2 X_2 + b$。\n两种语义的基线期望是相同的：\n$$ E[f(X)] = E[w_1 X_1 + w_2 X_2 + b] = w_1 E[X_1] + w_2 E[X_2] + b = w_1 \\mu_1 + w_2 \\mu_2 + b $$\n\n**1. 干预语义 ($\\phi_{1}^{\\mathrm{int}}$)**\n干预语义假设特征独立。当以特征子集 S 为条件时，剩余的特征会在其边际分布上被积分排除，这与 S 中的值无关。\n\n-   $E^{\\mathrm{int}}[f(X) | X_1=x_1] = E[w_1 x_1 + w_2 X_2 + b] = w_1 x_1 + w_2 E[X_2] + b = w_1 x_1 + w_2 \\mu_2 + b$。\n-   $E^{\\mathrm{int}}[f(X) | X_2=x_2] = E[w_1 X_1 + w_2 x_2 + b] = w_1 E[X_1] + w_2 x_2 + b = w_1 \\mu_1 + w_2 x_2 + b$。\n\n将这些代入 $\\phi_1$ 的 SHAP 公式中：\n$$ \\phi_{1}^{\\mathrm{int}} = \\frac{1}{2} \\left[ (w_1 x_1 + w_2 \\mu_2 + b) - (w_1 \\mu_1 + w_2 \\mu_2 + b) \\right] + \\frac{1}{2} \\left[ (w_1 x_1 + w_2 x_2 + b) - (w_1 \\mu_1 + w_2 x_2 + b) \\right] $$\n$$ \\phi_{1}^{\\mathrm{int}} = \\frac{1}{2} [w_1 (x_1 - \\mu_1)] + \\frac{1}{2} [w_1 (x_1 - \\mu_1)] $$\n$$ \\phi_{1}^{\\mathrm{int}} = w_1 (x_1 - \\mu_1) $$\n\n**2. 观测语义 ($\\phi_{1}^{\\mathrm{obs}}$)**\n观测语义使用特征的真实条件概率分布。对于二元高斯分布，$X_i$ 在给定 $X_j=x_j$ 条件下的条件期望为：\n$$ E[X_i | X_j=x_j] = \\mu_i + \\rho \\frac{\\sigma_i}{\\sigma_j} (x_j - \\mu_j) $$\n\n模型输出所需的条件期望为：\n-   $E^{\\mathrm{obs}}[f(X) | X_1=x_1] = E[w_1 x_1 + w_2 X_2 + b | X_1=x_1] = w_1 x_1 + w_2 E[X_2 | X_1=x_1] + b$\n    $$ E^{\\mathrm{obs}}[f(X) | X_1=x_1] = w_1 x_1 + w_2 \\left(\\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1)\\right) + b $$\n-   $E^{\\mathrm{obs}}[f(X) | X_2=x_2] = E[w_1 X_1 + w_2 x_2 + b | X_2=x_2] = w_1 E[X_1 | X_2=x_2] + w_2 x_2 + b$\n    $$ E^{\\mathrm{obs}}[f(X) | X_2=x_2] = w_1 \\left(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2)\\right) + w_2 x_2 + b $$\n\n现在，我们计算 $\\phi_{1}^{\\mathrm{obs}}$ 的两个边际贡献项：\n-   项 1：$E^{\\mathrm{obs}}[f(X) | X_1=x_1] - E[f(X)]$\n    $$ = \\left(w_1 x_1 + w_2 \\left(\\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1)\\right) + b\\right) - (w_1 \\mu_1 + w_2 \\mu_2 + b) $$\n    $$ = w_1 (x_1 - \\mu_1) + w_2 \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) $$\n-   项 2：$f(x_1, x_2) - E^{\\mathrm{obs}}[f(X) | X_2=x_2]$\n    $$ = (w_1 x_1 + w_2 x_2 + b) - \\left(w_1 \\left(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2)\\right) + w_2 x_2 + b\\right) $$\n    $$ = w_1 x_1 - w_1 \\left(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2)\\right) = w_1 (x_1 - \\mu_1) - w_1 \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) $$\n\n组合这些项来求得 $\\phi_{1}^{\\mathrm{obs}}$：\n$$ \\phi_{1}^{\\mathrm{obs}} = \\frac{1}{2} \\left[ w_1 (x_1 - \\mu_1) + w_2 \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) \\right] + \\frac{1}{2} \\left[ w_1 (x_1 - \\mu_1) - w_1 \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right] $$\n$$ \\phi_{1}^{\\mathrm{obs}} = w_1 (x_1 - \\mu_1) + \\frac{1}{2} \\rho \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) $$\n\n**3. 依赖引起的归因偏移 ($\\Delta_{1}$)**\n该偏移是观测 SHAP 值和干预 SHAP 值之间的差值。\n$$ \\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}} $$\n$$ \\Delta_{1} = \\left[ w_1 (x_1 - \\mu_1) + \\frac{1}{2} \\rho \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) \\right] - \\left[ w_1 (x_1 - \\mu_1) \\right] $$\n$$ \\Delta_{1} = \\frac{\\rho}{2} \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) $$\n\n**4. 数值计算**\n我们将给定值代入 $\\Delta_{1}$ 的表达式中：\n-   $x_1 - \\mu_1 = 0.65 - 0.50 = 0.15$\n-   $x_2 - \\mu_2 = 0.75 - 0.80 = -0.05$\n-   $\\frac{\\sigma_2}{\\sigma_1} = \\frac{0.15}{0.10} = 1.5$\n-   $\\frac{\\sigma_1}{\\sigma_2} = \\frac{0.10}{0.15} = \\frac{2}{3}$\n\n$$ \\Delta_{1} = \\frac{0.60}{2} \\left( (1.5) (1.5) (0.15) - (2.0) \\left(\\frac{2}{3}\\right) (-0.05) \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( (2.25) (0.15) - \\left(\\frac{4}{3}\\right) (-0.05) \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( 0.3375 + \\frac{0.20}{3} \\right) $$\n为保持精度，我们使用分数计算：\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{9}{4} \\cdot \\frac{15}{100} - \\frac{4}{3} \\cdot \\left(-\\frac{5}{100}\\right) \\right) $$\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{135}{400} + \\frac{20}{300} \\right) = \\frac{3}{10} \\left( \\frac{27}{80} + \\frac{1}{15} \\right) $$\n$80$ 和 $15$ 的最小公倍数是 $240$。\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{27 \\times 3}{240} + \\frac{1 \\times 16}{240} \\right) = \\frac{3}{10} \\left( \\frac{81 + 16}{240} \\right) $$\n$$ \\Delta_{1} = \\frac{3}{10} \\left( \\frac{97}{240} \\right) = \\frac{97}{800} $$\n转换为小数：\n$$ \\Delta_{1} = 0.12125 $$\n题目要求答案四舍五入到四位有效数字。\n$$ \\Delta_{1} \\approx 0.1213 $$\n$X_1$ 的依赖引起的归因偏移重新分配了部分归因。正相关（$\\rho > 0$）导致高值 $X_1$（$x_1 > \\mu_1$）的存在意味着 $X_2$ 有更高的期望值，而低值 $X_2$（$x_2  \\mu_2$）的存在意味着 $X_1$ 有更低的期望值。$\\Delta_1$ 捕获了这种复杂的相互作用。",
            "answer": "$$\\boxed{0.1213}$$"
        }
    ]
}