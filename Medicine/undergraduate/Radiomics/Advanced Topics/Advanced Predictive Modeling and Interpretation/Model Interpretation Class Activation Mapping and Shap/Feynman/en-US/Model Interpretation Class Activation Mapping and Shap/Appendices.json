{
    "hands_on_practices": [
        {
            "introduction": "Understanding how a convolutional neural network (CNN) 'sees' an image is a cornerstone of model interpretation. Class Activation Mapping (CAM) provides an intuitive way to generate heatmaps that highlight the image regions most influential for a given prediction. This first exercise breaks down the CAM procedure into its fundamental steps, allowing you to manually construct a heatmap from the final feature maps of a toy model. By working through this calculation, you will gain a concrete understanding of how high-level features are translated into a spatial attribution map. ",
            "id": "4551422",
            "problem": "In a simplified radiomics setting, consider a convolutional neural network trained to classify medical images, where the last convolutional layer produces two spatial feature maps $f_1$ and $f_2$ of size $2\\times 2$. The classifier for class $c$ is linear in the globally averaged features, with learned weights $w_1^c$ and $w_2^c$. Assume that, consistent with standard use in Class Activation Mapping (CAM), positive class evidence at a spatial location is proportional to the rectified linear combination of the feature maps at that location, using the same class weights, and that negative evidence is suppressed by rectification to emphasize the regions contributing positively to class $c$.\n\nYou are given\n$f_1=\\begin{bmatrix}1&2\\\\0&1\\end{bmatrix}$, $f_2=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}$, $w_1^c=2$, and $w_2^c=-1$.\nProceed as follows:\n\n1. From first principles of linearity and spatial equivariance of convolutional features, construct the $2\\times 2$ class activation map $M_c$ for class $c$ as described above, including rectification of negative values.\n\n2. Upsample $M_c$ to a $4\\times 4$ grid using bilinear interpolation with corner alignment (the $2\\times 2$ corners map to the $4\\times 4$ corners). Define the interpolated value at integer grid coordinates $(i,j)$ with $i,j\\in\\{0,1,2,3\\}$ by applying one-dimensional linear interpolation along one axis followed by the other, assuming uniform spacing.\n\n3. Identify the three grid coordinates in the $4\\times 4$ upsampled map with the largest interpolated values. If there is a tie at the cutoff, break ties by lexicographic order on $(i,j)$, prioritizing smaller $i$ and then smaller $j$.\n\n4. Interpret the per-pixel attribution for Shapley Additive Explanations (SHAP) at the input resolution as being proportional to the upsampled CAM and normalized to sum to $1$ across the $4\\times 4$ grid. That is, if the upsampled values are $\\{v_{ij}\\}$, define $s_{ij}=v_{ij}/\\sum_{p,q} v_{pq}$.\n\nCompute the exact value of the sum of the SHAP-like attributions over the three coordinates identified in step $3$. Express your final answer as a reduced fraction. Do not include any units.",
            "solution": "The solution proceeds in four steps as outlined in the problem statement.\n\n**Part 1: Construct the Class Activation Map ($M_c$)**\nThe class activation map $M_c$ at each spatial location $(u,v)$ is the rectified linear combination of the feature maps at that location, weighted by the class-specific weights. The formula is:\n$M_c(u,v) = \\text{ReLU}(w_1^c f_1(u,v) + w_2^c f_2(u,v))$\n\nFirst, we compute the linear combination $L = w_1^c f_1 + w_2^c f_2$:\n$L = 2 \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix} - 1 \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 0 & 2 \\end{bmatrix} - \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ -1 & 2 \\end{bmatrix}$\n\nNext, we apply the Rectified Linear Unit (ReLU) function, which sets all negative values to zero:\n$M_c = \\text{ReLU}(L) = \\begin{bmatrix} \\max(0, 2) & \\max(0, 3) \\\\ \\max(0, -1) & \\max(0, 2) \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 0 & 2 \\end{bmatrix}$\n\n**Part 2: Upsample $M_c$ to a $4\\times 4$ grid**\nWe use bilinear interpolation with corner alignment. The four corners of the $2\\times 2$ map $M_c$ correspond to the four corners of the target $4\\times 4$ map $V$. The values at the corners of $M_c$ are $M_c(0,0)=2$, $M_c(1,0)=0$, $M_c(0,1)=3$, and $M_c(1,1)=2$.\nA point $(i,j)$ in the $4\\times 4$ grid, where $i,j \\in \\{0,1,2,3\\}$, corresponds to a point $(u,v)$ in the continuous unit square via the mapping $u=i/3$ and $v=j/3$.\nThe bilinear interpolation formula is $V(u,v) = M_c(0,0)(1-u)(1-v) + M_c(1,0)u(1-v) + M_c(0,1)(1-u)v + M_c(1,1)uv$.\nSubstituting the values of $M_c$:\n$V(u,v) = 2(1-u)(1-v) + 0 \\cdot u(1-v) + 3(1-u)v + 2uv = 2 - 2u + v + uv$.\n\nWe evaluate $v_{ij} = V(i/3, j/3)$ for all $i,j$:\n$v_{ij} = 2 - \\frac{2i}{3} + \\frac{j}{3} + \\frac{ij}{9} = \\frac{18 - 6i + 3j + ij}{9}$\nThis yields the upsampled matrix $V$:\n$$ V = \\frac{1}{9} \\begin{pmatrix}\n18 & 21 & 24 & 27 \\\\\n12 & 16 & 20 & 24 \\\\\n6 & 11 & 16 & 21 \\\\\n0 & 6 & 12 & 18\n\\end{pmatrix} $$\n\n**Part 3: Identify the three largest values**\nBy inspecting the matrix of numerators, we find the largest values:\n- The largest value is $27/9=3$ at coordinate $(0,3)$.\n- The next largest value is $24/9 = 8/3$, which appears at coordinates $(0,2)$ and $(1,3)$.\nAccording to the tie-breaking rule (lexicographic order), $(0,2)$ is chosen before $(1,3)$.\nThus, the three coordinates with the largest values are $(0,3)$, $(0,2)$, and $(1,3)$.\n\n**Part 4: Compute the sum of SHAP-like attributions**\nThe attribution for pixel $(i,j)$ is $s_{ij} = v_{ij} / \\sum_{p,q} v_{pq}$. First, we find the total sum of all values in $V$.\nSum of numerators: $(18+21+24+27) + (12+16+20+24) + (6+11+16+21) + (0+6+12+18) = 90 + 72 + 54 + 36 = 252$.\nTotal sum: $\\sum v = \\frac{252}{9} = 28$.\n\nThe values for the top three coordinates are:\n- $v_{03} = 27/9 = 3$\n- $v_{02} = 24/9 = 8/3$\n- $v_{13} = 24/9 = 8/3$\n\nThe corresponding attributions are:\n- $s_{03} = \\frac{3}{28}$\n- $s_{02} = \\frac{8/3}{28} = \\frac{8}{84} = \\frac{2}{21}$\n- $s_{13} = \\frac{8/3}{28} = \\frac{8}{84} = \\frac{2}{21}$\n\nThe sum of these attributions is:\n$S = s_{03} + s_{02} + s_{13} = \\frac{3}{28} + \\frac{2}{21} + \\frac{2}{21} = \\frac{3}{28} + \\frac{4}{21}$.\nTo add these fractions, we find a common denominator, which is 84.\n$S = \\frac{3 \\times 3}{84} + \\frac{4 \\times 4}{84} = \\frac{9}{84} + \\frac{16}{84} = \\frac{25}{84}$.\nThis fraction is already reduced.",
            "answer": "$$\n\\boxed{\\frac{25}{84}}\n$$"
        },
        {
            "introduction": "While CAM helps interpret spatial information, many radiomics models rely on tabular features. For these, we turn to methods like Shapley Additive Explanations (SHAP) to understand each feature's contribution. A common mistake is to assume a feature's importance is simply its coefficient in a linear model. This exercise challenges that assumption by demonstrating how SHAP values account for not just the model's weights, but also the specific value of a feature for a given patient relative to its baseline. ",
            "id": "4551432",
            "problem": "In a radiomics study, a linear risk score is built from two quantitative features extracted from computed tomography scans. Denote the features by $X_1$ and $X_2$. The training cohort has the following properties:\n- The features are statistically independent.\n- The means are $\\mu_1 = 0$ and $\\mu_2 = 0$.\n- The standard deviations are $\\sigma_1 = 3$ and $\\sigma_2 = 1$.\n\nThe model is the linear function $f(\\mathbf{x}) = \\beta (x_1 + x_2)$ with $\\beta = 2$ and zero intercept. For model interpretation, consider Shapley Additive Explanations (SHAP), defined via the cooperative game theoretic Shapley values with the background choice given by the empirical training distribution described above. That is, for any subset $S$ of features, the coalition value is $v(S) = \\mathbb{E}\\!\\left[f(X_1, X_2)\\,\\middle|\\, X_S = x_S\\right]$, where the expectation is taken over the training distribution.\n\nConsider a particular patient whose measured features are one standard deviation above the cohort mean for each feature, so that $x_1 = \\mu_1 + \\sigma_1$ and $x_2 = \\mu_2 + \\sigma_2$.\n\nUsing only the definition of Shapley values as averages of marginal contributions over all coalitions and the stated statistical properties of the background, derive the SHAP attributions $\\phi_1$ and $\\phi_2$ for this patient. Then compute the difference $\\Delta = \\phi_1 - \\phi_2$. Express the final answer as an exact number (no rounding). The answer is unitless.",
            "solution": "The problem requires us to compute the SHAP values, $\\phi_1$ and $\\phi_2$, for a linear model with two independent features and then find their difference.\n\n**1. Define the Setup**\n- **Model:** $f(x_1, x_2) = 2(x_1 + x_2)$.\n- **Feature Distribution:** $X_1$ and $X_2$ are independent, with means $\\mathbb{E}[X_1] = \\mu_1 = 0$ and $\\mathbb{E}[X_2] = \\mu_2 = 0$.\n- **Specific Instance:** The patient has feature values $x_1 = \\mu_1 + \\sigma_1 = 0 + 3 = 3$ and $x_2 = \\mu_2 + \\sigma_2 = 0 + 1 = 1$.\n\n**2. Calculate Coalition Values**\nThe SHAP calculation relies on the value function $v(S) = \\mathbb{E}[f(X_1, X_2) | X_S = x_S]$ for all coalitions $S \\subseteq \\{X_1, X_2\\}$.\n\n- **Empty Coalition ($S=\\emptyset$):** This is the baseline or expected model output.\n$v(\\emptyset) = \\mathbb{E}[f(X_1, X_2)] = \\mathbb{E}[2(X_1 + X_2)] = 2(\\mathbb{E}[X_1] + \\mathbb{E}[X_2]) = 2(0 + 0) = 0$.\n\n- **Coalition $S=\\{X_1\\}$:** We condition on $x_1=3$ and take the expectation over $X_2$.\n$v(\\{X_1\\}) = \\mathbb{E}[f(X_1, X_2) | X_1=3] = \\mathbb{E}[2(3 + X_2)]$. Due to independence, this becomes $6 + 2\\mathbb{E}[X_2] = 6 + 2(0) = 6$.\n\n- **Coalition $S=\\{X_2\\}$:** We condition on $x_2=1$ and take the expectation over $X_1$.\n$v(\\{X_2\\}) = \\mathbb{E}[f(X_1, X_2) | X_2=1] = \\mathbb{E}[2(X_1 + 1)]$. Due to independence, this becomes $2\\mathbb{E}[X_1] + 2 = 2(0) + 2 = 2$.\n\n- **Full Coalition $S=\\{X_1, X_2\\}$:** Both features are known, so this is just the model output for the instance.\n$v(\\{X_1, X_2\\}) = f(3, 1) = 2(3 + 1) = 8$.\n\n**3. Compute SHAP Values**\nThe SHAP value for a feature is the average of its marginal contributions to all coalitions. For a two-feature model, the formula for $\\phi_i$ is $\\frac{1}{2} [ (v(\\{X_i\\}) - v(\\emptyset)) + (v(\\{X_1, X_2\\}) - v(\\{X_j\\})) ]$ where $j \\neq i$.\n\n- **SHAP value for $X_1$ ($\\phi_1$):**\n$\\phi_1 = \\frac{1}{2} [ (v(\\{X_1\\}) - v(\\emptyset)) + (v(\\{X_1, X_2\\}) - v(\\{X_2\\})) ]$\n$\\phi_1 = \\frac{1}{2} [ (6 - 0) + (8 - 2) ] = \\frac{1}{2} [6 + 6] = 6$.\n\n- **SHAP value for $X_2$ ($\\phi_2$):**\n$\\phi_2 = \\frac{1}{2} [ (v(\\{X_2\\}) - v(\\emptyset)) + (v(\\{X_1, X_2\\}) - v(\\{X_1\\})) ]$\n$\\phi_2 = \\frac{1}{2} [ (2 - 0) + (8 - 6) ] = \\frac{1}{2} [2 + 2] = 2$.\n\n(As a check, the efficiency property holds: $\\phi_1 + \\phi_2 = 6+2=8$, which equals $f(x_1, x_2) - v(\\emptyset) = 8-0=8$.)\n\n**4. Calculate the Final Difference**\nThe problem asks for the difference $\\Delta = \\phi_1 - \\phi_2$.\n$\\Delta = 6 - 2 = 4$.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Building on our understanding of SHAP, we now address a crucial real-world challenge: feature correlation. Radiomic features, such as tumor texture and shape, are often not independent. This practice explores how these dependencies can significantly alter feature attributions. You will compute SHAP values under two different assumptions—one that ignores correlations (interventional) and one that accounts for them (observational)—to quantify the impact and appreciate the importance of choosing the correct background distribution for a faithful explanation. ",
            "id": "4551492",
            "problem": "A radiomics pipeline extracts two standardized quantitative features from computed tomography of lung nodules: texture entropy $X_{1}$ and shape compactness $X_{2}$. A linear predictor of malignancy score is trained as $f(x_{1},x_{2}) = w_{1} x_{1} + w_{2} x_{2} + b$. In clinical model interpretation, Class Activation Mapping (CAM) highlights spatial regions for convolutional neural networks, whereas Shapley Additive Explanations (SHAP) allocates additive contributions of features to a model output. In tabular-feature radiomics, it is essential to distinguish interventional semantics (missing features are sampled independently from their marginals) from observational semantics (missing features are integrated out under their conditional distribution given the observed features).\n\nConsider a single patient with observed features $x_{1} = 0.65$ and $x_{2} = 0.75$. The trained linear model has parameters $w_{1} = 2.0$, $w_{2} = 1.5$, and $b = -0.2$. The feature vector $(X_{1},X_{2})$ in the training population is well-modeled by a bivariate Gaussian with means $\\mu_{1} = 0.50$, $\\mu_{2} = 0.80$, standard deviations $\\sigma_{1} = 0.10$, $\\sigma_{2} = 0.15$, and correlation $\\rho = 0.60 \\neq 0$. Assume the standard properties of multivariate Gaussian distributions and the cooperative game theory definition of SHAP values, and treat $f$ as the payoff function.\n\nStarting from first principles, compute the SHAP values for $X_{1}$ and $X_{2}$ under both interventional and observational semantics for this patient. Then, quantify the dependence-induced attribution shift for $X_{1}$ as the difference between the observational and interventional SHAP values:\n$$\\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}}.$$\nProvide only $\\Delta_{1}$ as your final answer. Round your final numerical answer to four significant figures. Express the answer as a pure number with no units.",
            "solution": "This problem requires us to calculate the shift in the SHAP value for feature $X_1$ when moving from an assumption of feature independence (interventional) to an assumption of feature dependence (observational).\n\n**1. Interventional SHAP Value ($\\phi_{1}^{\\mathrm{int}}$)**\nFor linear models with independent features, the SHAP value for a feature is its coefficient multiplied by the difference between the feature's value and its mean.\n$$ \\phi_{1}^{\\mathrm{int}} = w_1 (x_1 - \\mu_1) $$\nThis is a standard result derived from the full Shapley formula under the independence assumption, where $E[X_2|X_1=x_1]=E[X_2]=\\mu_2$.\n\n**2. Observational SHAP Value ($\\phi_{1}^{\\mathrm{obs}}$)**\nFor observational semantics, we must use the true conditional expectations, which account for the correlation $\\rho$. For a bivariate Gaussian distribution, the conditional expectation is linear:\n$$ E[X_2 | X_1=x_1] = \\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) $$\n$$ E[X_1 | X_2=x_2] = \\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) $$\nThe SHAP value $\\phi_{1}^{\\mathrm{obs}}$ is the average of two marginal contributions:\na) The contribution of $X_1$ when added to an empty set: $E[f|X_1=x_1] - E[f]$.\n$ [w_1 x_1 + w_2 E[X_2|x_1] + b] - [w_1\\mu_1 + w_2\\mu_2 + b] = w_1(x_1-\\mu_1) + w_2(E[X_2|x_1]-\\mu_2) = w_1(x_1-\\mu_1) + w_2\\rho\\frac{\\sigma_2}{\\sigma_1}(x_1-\\mu_1) $\nb) The contribution of $X_1$ when added to the set $\\{X_2\\}$: $f(x_1,x_2) - E[f|X_2=x_2]$.\n$ [w_1 x_1 + w_2 x_2 + b] - [w_1 E[X_1|x_2] + w_2 x_2 + b] = w_1(x_1 - E[X_1|x_2]) = w_1(x_1-\\mu_1) - w_1\\rho\\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2) $\n\nAveraging these two contributions gives $\\phi_{1}^{\\mathrm{obs}}$:\n$$ \\phi_{1}^{\\mathrm{obs}} = \\frac{1}{2} \\left[ \\left(w_1(x_1-\\mu_1) + w_2\\rho\\frac{\\sigma_2}{\\sigma_1}(x_1-\\mu_1)\\right) + \\left(w_1(x_1-\\mu_1) - w_1\\rho\\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2)\\right) \\right] $$\n$$ \\phi_{1}^{\\mathrm{obs}} = w_1 (x_1 - \\mu_1) + \\frac{\\rho}{2} \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) $$\n\n**3. Dependence-Induced Attribution Shift ($\\Delta_{1}$)**\nThe shift is the difference $\\Delta_{1} = \\phi_{1}^{\\mathrm{obs}} - \\phi_{1}^{\\mathrm{int}}$.\n$$ \\Delta_{1} = \\left[ w_1 (x_1 - \\mu_1) + \\frac{\\rho}{2} \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) \\right] - \\left[ w_1 (x_1 - \\mu_1) \\right] $$\n$$ \\Delta_{1} = \\frac{\\rho}{2} \\left( w_2 \\frac{\\sigma_2}{\\sigma_1} (x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2} (x_2 - \\mu_2) \\right) $$\n\n**4. Numerical Calculation**\nWe substitute the given values:\n- Parameters: $\\rho=0.60$, $w_1=2.0$, $w_2=1.5$\n- Feature values relative to mean: $x_1 - \\mu_1 = 0.65 - 0.50 = 0.15$; $x_2 - \\mu_2 = 0.75 - 0.80 = -0.05$\n- Standard deviation ratios: $\\frac{\\sigma_2}{\\sigma_1} = \\frac{0.15}{0.10} = 1.5$; $\\frac{\\sigma_1}{\\sigma_2} = \\frac{0.10}{0.15} = \\frac{2}{3}$\n\nNow, plug these into the formula for $\\Delta_{1}$:\n$$ \\Delta_{1} = \\frac{0.60}{2} \\left( (1.5) \\cdot (1.5) \\cdot (0.15) - (2.0) \\cdot \\left(\\frac{2}{3}\\right) \\cdot (-0.05) \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( 2.25 \\cdot 0.15 - \\frac{4}{3} \\cdot (-0.05) \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( 0.3375 + \\frac{0.20}{3} \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( 0.3375 + 0.06666... \\right) $$\n$$ \\Delta_{1} = 0.30 \\left( 0.4041666... \\right) = 0.12125 $$\nRounding to four significant figures, we get:\n$$ \\Delta_{1} \\approx 0.1213 $$",
            "answer": "$$\\boxed{0.1213}$$"
        }
    ]
}