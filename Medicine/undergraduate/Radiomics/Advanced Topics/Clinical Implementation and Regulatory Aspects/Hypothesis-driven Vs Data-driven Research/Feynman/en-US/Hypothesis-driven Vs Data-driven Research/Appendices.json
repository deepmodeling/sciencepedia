{
    "hands_on_practices": [
        {
            "introduction": "Radiomics studies often pool data from multiple hospitals or scanners, creating a risk of \"batch effects\" where technical variations are mistaken for biological differences. This exercise provides a hands-on demonstration of how a simple linear scaling difference between two hypothetical centers can create a spurious discrepancy in a radiomic feature. By applying a standard $z$-scoring transformation, you will see how to computationally harmonize the data, an essential step for ensuring that downstream analyses reflect true biology rather than technical artifacts .",
            "id": "4544647",
            "problem": "A radiomics study aggregates Magnetic Resonance Imaging (MRI) images from two centers to build predictive models of tumor phenotype. In a hypothesis-driven design, you posit that a fixed Region of Interest (ROI) within the same histopathological class should have comparable radiomic features across centers after appropriate standardization. In a purely data-driven exploration, the pipeline might naively treat between-center feature differences as discriminative signals. One known source of spurious between-center differences is intensity non-standardization: scanners and reconstruction pipelines can induce different linear scalings of the underlying tissue signal.\n\nConsider an ROI in which the voxel intensities are measured in arbitrary units (a.u.) for Center A as $[70, 74, 76, 72, 68, 80]$ and for Center B as $[30, 40, 45, 35, 25, 55]$. Assume these intensities arise from the same underlying tissue microstructure, but with center-specific linear scaling, and that there is no biological difference between centers for this ROI.\n\nUsing first principles and standard definitions, do the following:\n1. Compute the first-order radiomic “energy” feature defined by $$E = \\sum_{i=1}^{n} x_i^2$$ for each center before any standardization, and interpret the result in the hypothesis-driven versus data-driven context.\n2. Specify a per-center $z$-scoring transform that restores comparability, using the population mean and the population standard deviation, defined respectively by $$\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i, \\quad \\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2}, \\quad z_i = \\frac{x_i - \\mu}{\\sigma}.$$\n3. Compute the energy of the standardized intensities for each center, $$E^{(z)} = \\sum_{i=1}^{n} z_i^2,$$ and then compute the ratio $$R = \\frac{E^{(z)}_{\\text{A}}}{E^{(z)}_{\\text{B}}}.$$\n\nExpress the final answer for $R$ as an exact value. No units are required for $R$ because it is dimensionless. If any intermediate numerical values are reported, they need not be rounded; if rounding is necessary for any intermediate step, round to four significant figures.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a realistic scenario in the field of radiomics concerning the need for data standardization to correct for inter-center variability in medical imaging data. All necessary definitions and data are provided.\n\nThe problem asks for a three-part analysis of radiomic feature comparability between two centers, A and B. We will address each part systematically.\n\nThe provided intensity data for the Regions of Interest (ROI) are:\n- Center A: $X_A = [70, 74, 76, 72, 68, 80]$\n- Center B: $X_B = [30, 40, 45, 35, 25, 55]$\n\nFor both centers, the number of voxels (samples) is $n=6$.\n\n**1. Computation and Interpretation of Pre-Standardization Energy**\n\nThe first-order radiomic feature \"energy\" is defined as $E = \\sum_{i=1}^{n} x_i^2$. We compute this for each center.\n\nFor Center A:\n$$E_A = \\sum_{i=1}^{6} x_{A,i}^2 = 70^2 + 74^2 + 76^2 + 72^2 + 68^2 + 80^2$$\n$$E_A = 4900 + 5476 + 5776 + 5184 + 4624 + 6400 = 32360$$\n\nFor Center B:\n$$E_B = \\sum_{i=1}^{6} x_{B,i}^2 = 30^2 + 40^2 + 45^2 + 35^2 + 25^2 + 55^2$$\n$$E_B = 900 + 1600 + 2025 + 1225 + 625 + 3025 = 9400$$\n\n*Interpretation*:\nThe energy feature, which reflects the overall magnitude of voxel intensities, is substantially different between the two centers ($E_A = 32360$ vs. $E_B = 9400$).\n- In a purely **data-driven** approach, a machine learning model would likely identify this large difference in energy as a highly discriminative feature. If the task were, for instance, to predict tumor phenotype, the model might erroneously learn to associate scanner-specific intensity scaling with the biological outcome. This is a classic example of a confounding variable, where the model learns an artifact of the data collection process rather than a true biological signal.\n- In a **hypothesis-driven** framework, the researcher anticipates such non-biological variations based on prior knowledge of imaging physics and multi-center data collection. The explicit hypothesis is that the underlying tissue is identical and the observed intensity difference is due to a linear scaling artifact. The large discrepancy in the raw energy feature $E$ would prompt the researcher to perform data harmonization, such as the standardization requested in the next step, to remove this confounder before testing any biological hypotheses.\n\n**2. Specification of the $z$-scoring Transform**\n\nThe $z$-scoring transform, $z_i = \\frac{x_i - \\mu}{\\sigma}$, requires the computation of the population mean ($\\mu$) and population standard deviation ($\\sigma$) for each center.\n\nFor Center A:\nThe mean is:\n$$\\mu_A = \\frac{1}{6}(70 + 74 + 76 + 72 + 68 + 80) = \\frac{440}{6} = \\frac{220}{3}$$\nThe variance is:\n$$\\sigma_A^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_{A,i} - \\mu_A)^2 = \\frac{1}{6}\\left[\\left(70-\\frac{220}{3}\\right)^2 + \\left(74-\\frac{220}{3}\\right)^2 + \\dots + \\left(80-\\frac{220}{3}\\right)^2\\right]$$\n$$\\sigma_A^2 = \\frac{1}{6}\\left[\\left(\\frac{-10}{3}\\right)^2 + \\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{8}{3}\\right)^2 + \\left(\\frac{-4}{3}\\right)^2 + \\left(\\frac{-16}{3}\\right)^2 + \\left(\\frac{20}{3}\\right)^2\\right]$$\n$$\\sigma_A^2 = \\frac{1}{6} \\cdot \\frac{100+4+64+16+256+400}{9} = \\frac{1}{6} \\cdot \\frac{840}{9} = \\frac{140}{9}$$\nThe standard deviation is:\n$$\\sigma_A = \\sqrt{\\frac{140}{9}} = \\frac{\\sqrt{140}}{3} = \\frac{2\\sqrt{35}}{3}$$\nThe transform for Center A is $z_{A,i} = \\frac{x_{A,i} - 220/3}{2\\sqrt{35}/3}$.\n\nFor Center B:\nThe mean is:\n$$\\mu_B = \\frac{1}{6}(30 + 40 + 45 + 35 + 25 + 55) = \\frac{230}{6} = \\frac{115}{3}$$\nThe variance is:\n$$\\sigma_B^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_{B,i} - \\mu_B)^2 = \\frac{1}{6}\\left[\\left(30-\\frac{115}{3}\\right)^2 + \\dots + \\left(55-\\frac{115}{3}\\right)^2\\right]$$\n$$\\sigma_B^2 = \\frac{1}{6}\\left[\\left(\\frac{-25}{3}\\right)^2 + \\left(\\frac{5}{3}\\right)^2 + \\left(\\frac{20}{3}\\right)^2 + \\left(\\frac{-10}{3}\\right)^2 + \\left(\\frac{-40}{3}\\right)^2 + \\left(\\frac{50}{3}\\right)^2\\right]$$\n$$\\sigma_B^2 = \\frac{1}{6} \\cdot \\frac{625+25+400+100+1600+2500}{9} = \\frac{1}{6} \\cdot \\frac{5250}{9} = \\frac{875}{9}$$\nThe standard deviation is:\n$$\\sigma_B = \\sqrt{\\frac{875}{9}} = \\frac{\\sqrt{25 \\cdot 35}}{3} = \\frac{5\\sqrt{35}}{3}$$\nThe transform for Center B is $z_{B,i} = \\frac{x_{B,i} - 115/3}{5\\sqrt{35}/3}$.\n\n**3. Computation of Standardized Energies and their Ratio**\n\nThe energy of the standardized intensities is $E^{(z)} = \\sum_{i=1}^{n} z_i^2$. We can compute this value through a general derivation based on the provided definitions.\n$$E^{(z)} = \\sum_{i=1}^{n} z_i^2 = \\sum_{i=1}^{n} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\nThe definition of the population variance is $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2$. Rearranging this gives $\\sum_{i=1}^{n} (x_i - \\mu)^2 = n\\sigma^2$.\nSubstituting this result into the expression for $E^{(z)}$:\n$$E^{(z)} = \\frac{1}{\\sigma^2}(n\\sigma^2) = n$$\nThis is a fundamental property: for any dataset, the sum of squares of its z-scores (calculated using the population mean and standard deviation) is exactly equal to the number of data points, $n$.\n\nFor both Center A and Center B, the number of samples is $n=6$. Therefore, the standardized energy for each center is:\n$$E^{(z)}_{\\text{A}} = n = 6$$\n$$E^{(z)}_{\\text{B}} = n = 6$$\n\nThis result demonstrates that the $z$-scoring transform has successfully removed the linear scaling differences between the centers, resulting in identical energy values. This harmonized feature now correctly reflects the problem's initial condition that there is no biological difference in the ROI between centers.\n\nFinally, we compute the ratio $R$:\n$$R = \\frac{E^{(z)}_{\\text{A}}}{E^{(z)}_{\\text{B}}} = \\frac{6}{6} = 1$$\n\nThe ratio is exactly $1$, confirming that $z$-scoring has made the energy feature comparable across centers.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "In both hypothesis-driven and data-driven research, we often test multiple features or hypotheses simultaneously. A crucial, and often overlooked, consequence is that the probability of making at least one false discovery (a Type I error) inflates dramatically as the number of tests increases. This practice lets you derive this inflation effect from first principles by calculating the Family-Wise Error Rate (FWER), and then apply the classic Bonferroni correction to control this rate, which is paramount in confirmatory studies where the cost of a false positive is high .",
            "id": "4544682",
            "problem": "A hypothesis-driven radiomics study plans to test $m$ predefined feature-outcome associations derived from prior mechanistic rationale in medical imaging. Each hypothesis test is conducted at per-test significance level $\\alpha = 0.05$ without multiple-testing correction. Assume the global null hypothesis is true (all $m$ null hypotheses are true) and that individual tests are independent.\n\nUsing the definition of the Family-Wise Error Rate (FWER) as $P(\\text{at least one false rejection among the } m \\text{ tests})$, derive the expected false positive rate as a function of $m$ under these conditions. Then, determine the per-test significance level required by the Bonferroni correction to ensure that the FWER is controlled at $0.05$ regardless of dependence among tests.\n\nProvide your final answer as two analytic expressions in terms of $m$: first, the expected false positive rate without correction; second, the Bonferroni-corrected per-test significance level. No numerical evaluation is required, and no units are involved.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a solution. The concepts of Family-Wise Error Rate (FWER), multiple hypothesis testing, and the Bonferroni correction are standard in statistics and its applications, such as radiomics. The assumptions of the global null hypothesis being true and the independence of tests for the first part of the problem are explicit and allow for a direct derivation.\n\nThe problem asks for two distinct quantities:\n1.  The expected false positive rate as a function of the number of tests, $m$, under the assumption of independence and no correction for multiple comparisons.\n2.  The per-test significance level required by the Bonferroni correction to control the FWER at $0.05$.\n\nLet us address each part sequentially.\n\n**Part 1: Expected False Positive Rate (FWER) without Correction**\n\nThe Family-Wise Error Rate (FWER) is defined in the problem as the probability of making at least one false rejection among the $m$ tests. This is also referred to as a Type I error on the \"family\" of hypotheses. Let $V$ be the random variable representing the number of false positives (Type I errors). The FWER is then $P(V \\ge 1)$.\n\nThe problem states that the global null hypothesis is true, which means all $m$ individual null hypotheses, $H_{0,1}, H_{0,2}, \\dots, H_{0,m}$, are true. Consequently, any rejection of a null hypothesis is, by definition, a false rejection (a Type I error).\n\nThe per-test significance level, $\\alpha$, is the probability of rejecting a single null hypothesis, given that it is true. We are given $\\alpha = 0.05$.\nFor any single test $i$, the probability of a Type I error is:\n$$ P(\\text{reject } H_{0,i}) = \\alpha = 0.05 $$\nThe probability of correctly not rejecting a true null hypothesis for a single test is therefore:\n$$ P(\\text{do not reject } H_{0,i}) = 1 - \\alpha = 1 - 0.05 = 0.95 $$\nCalculating the FWER directly, $P(V \\ge 1)$, involves a complex sum of probabilities. It is more straightforward to calculate the probability of its complement event, which is making zero false rejections, $P(V=0)$, and then use the relation $P(V \\ge 1) = 1 - P(V=0)$.\n\nThe event $\\{V=0\\}$ corresponds to not rejecting any of the $m$ true null hypotheses. The problem states that the individual tests are independent. Therefore, the probability of not rejecting any of the $m$ hypotheses is the product of the individual probabilities of not rejecting each one:\n$$ P(V=0) = P(\\text{no rejections in } m \\text{ tests}) = \\prod_{i=1}^{m} P(\\text{do not reject } H_{0,i}) $$\nSince each test is conducted at the same significance level $\\alpha$, this simplifies to:\n$$ P(V=0) = (1 - \\alpha)^m $$\nNow, we can find the FWER:\n$$ \\text{FWER} = P(V \\ge 1) = 1 - P(V=0) = 1 - (1 - \\alpha)^m $$\nSubstituting the given value $\\alpha = 0.05$, the expression for the expected false positive rate as a function of $m$ is:\n$$ \\text{FWER}(m) = 1 - (1 - 0.05)^m = 1 - (0.95)^m $$\n\n**Part 2: Bonferroni-Corrected Per-Test Significance Level**\n\nThe second part of the problem asks for the per-test significance level, let's call it $\\alpha'$, required by the Bonferroni correction to control the FWER at or below a specified level, which we denote as $\\alpha_{\\text{FWER}} = 0.05$.\n\nThe Bonferroni correction is a method to control the FWER that does not assume independence among the tests. It is based on Boole's inequality, also known as the union bound. Let $E_i$ be the event of a Type I error for the $i$-th test. The FWER is the probability of the union of these events:\n$$ \\text{FWER} = P\\left(\\bigcup_{i=1}^{m} E_i\\right) $$\nBoole's inequality states that for any set of events $E_1, \\dots, E_m$:\n$$ P\\left(\\bigcup_{i=1}^{m} E_i\\right) \\le \\sum_{i=1}^{m} P(E_i) $$\nApplying this to our context, the FWER is bounded above by the sum of the individual per-test error probabilities. If we use a new, adjusted significance level $\\alpha'$ for each test, then $P(E_i) = \\alpha'$. The inequality becomes:\n$$ \\text{FWER} \\le \\sum_{i=1}^{m} \\alpha' = m \\alpha' $$\nTo guarantee that the FWER is controlled at a level of $0.05$, we must ensure that its upper bound does not exceed this value. That is, we require $m \\alpha' \\le 0.05$. The Bonferroni correction sets the adjusted per-test significance level $\\alpha'$ by solving the equality:\n$$ m \\alpha' = 0.05 $$\nSolving for $\\alpha'$ gives the required per-test significance level:\n$$ \\alpha' = \\frac{0.05}{m} $$\nThis corrected significance level, $\\alpha'$, guarantees that $\\text{FWER} \\le 0.05$ regardless of the dependence structure among the tests, as stipulated in the problem.\n\nThe two final analytic expressions requested are $1 - (0.95)^m$ and $\\frac{0.05}{m}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 - (0.95)^m & \\frac{0.05}{m} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While controlling the FWER is vital for avoiding any false positives, it can be overly conservative for data-driven exploratory studies where the goal is to generate new hypotheses from hundreds or thousands of features. A more modern approach seeks to control the proportion of false discoveries among all discoveries made. This exercise introduces the False Discovery Rate (FDR) and the powerful Benjamini-Hochberg procedure, allowing you to balance statistical rigor with the power to discover meaningful associations in large datasets .",
            "id": "4544694",
            "problem": "A radiomics screening study evaluates associations between image-derived features and a clinical endpoint across $m=200$ univariate statistical tests conducted in a data-driven exploration phase. To reconcile subsequent hypothesis-driven validation with exploratory multiplicity control, you aim to control the False Discovery Rate (FDR). Assume the following foundational facts:\n- Under a true null hypothesis, a valid $p$-value is distributed as $\\mathrm{Uniform}(0,1)$.\n- The $p$-values are mutually independent across tests.\n- The False Discovery Rate (FDR) is defined as $\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\frac{V}{R}\\right]$, where $V$ is the number of false rejections and $R$ is the total number of rejections, with the convention that $\\frac{V}{R}=0$ when $R=0$.\n\nTask:\n1. Starting from these definitions and properties, derive the Benjamini–Hochberg (BH) step-up rejection rule that controls FDR at a target level $\\alpha \\in (0,1)$ without introducing any additional assumptions beyond independence and valid $p$-values.\n2. Apply the derived rule at target level $\\alpha=0.05$ to the following ordered $p$-values $\\{p_{(i)}\\}_{i=1}^{200}$ obtained from the radiomics screen:\n   - For $1 \\leq i \\leq 120$, $p_{(i)} = 0.0002\\,i$.\n   - For $121 \\leq i \\leq 200$, $p_{(i)} = 0.024 + 0.0005\\,(i-120)$.\n   These $p$-values are in nondecreasing order by construction. Compute the BH rejection threshold, defined as the largest cutoff below which all $p$-values are rejected under the BH step-up rule at $\\alpha=0.05$. Express the final threshold as a decimal number. No rounding is required.",
            "solution": "The problem as stated is scientifically sound, well-posed, and objective. It is grounded in established principles of statistical multiple-hypothesis testing, specifically the control of the False Discovery Rate (FDR). All necessary givens, definitions, and assumptions are provided, and there are no internal contradictions. Therefore, the problem is valid, and a solution may be constructed.\n\nThe problem consists of two tasks: first, to derive the Benjamini-Hochberg (BH) step-up procedure, and second, to apply this procedure to a given set of $p$-values.\n\n**Part 1: Derivation of the Benjamini-Hochberg (BH) Procedure**\n\nThe objective is to devise a procedure for testing $m$ independent hypotheses that controls the False Discovery Rate (FDR) at a pre-specified level $\\alpha$.\n\nLet there be $m$ null hypotheses, $H_1, H_2, \\dots, H_m$, with corresponding $p$-values $p_1, p_2, \\dots, p_m$. Let $m_0$ be the number of true null hypotheses, and $m_1 = m - m_0$ be the number of false null hypotheses (true alternatives). Let $I_0$ be the set of indices corresponding to the true null hypotheses. The givens state that the $p$-values are mutually independent and that for any true null hypothesis $H_i$ (where $i \\in I_0$), its $p$-value $p_i$ is distributed as $\\mathrm{Uniform}(0,1)$.\n\nA hypothesis testing procedure results in a number of rejections, $R$, and a number of false rejections (Type I errors), $V$. The False Discovery Proportion (FDP) is the random variable $Q = V/R$, with the convention that $Q=0$ if $R=0$. The False Discovery Rate is its expectation: $\\mathrm{FDR} = \\mathbb{E}[Q]$. We seek a procedure for which $\\mathrm{FDR} \\le \\alpha$.\n\nThe Benjamini-Hochberg (BH) step-up procedure is defined as follows:\n1.  Order the $p$-values in non-decreasing order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. Let $H_{(i)}$ be the null hypothesis corresponding to the $p$-value $p_{(i)}$.\n2.  Define a sequence of critical values $c_i = \\frac{i}{m} \\alpha$ for $i=1, 2, \\dots, m$.\n3.  Find the largest index $k$ such that $p_{(k)} \\le c_k = \\frac{k}{m} \\alpha$.\n4.  If such a $k$ exists, reject all null hypotheses $H_{(i)}$ for $i=1, 2, \\dots, k$. If no such $k$ exists, reject no hypotheses.\n\nNow, we must demonstrate that this procedure controls the FDR at $\\alpha$ under the assumption of independence. More specifically, we will show that $\\mathrm{FDR} \\le \\frac{m_0}{m}\\alpha$, which implies $\\mathrm{FDR} \\le \\alpha$ since $m_0 \\le m$.\n\nLet's begin the derivation. Let $k$ be the index determined by the BH procedure. The number of rejected hypotheses is $R=k$. The number of false discoveries is $V$, which is the number of true null hypotheses among the $k$ rejected ones.\n$$Q = \\frac{V}{R} = \\frac{V}{k} \\quad (\\text{if } k > 0)$$\nThe FDR is $\\mathbb{E}[Q]$. We can write $V$ as a sum of indicator functions:\n$$V = \\sum_{i \\in I_0} I(\\text{reject } H_i)$$\nwhere $I(\\cdot)$ is the indicator function. The procedure rejects $H_i$ if its corresponding $p$-value $p_i$ is less than or equal to $p_{(k)}$.\nUsing the linearity of expectation, we can write:\n$$\\mathrm{FDR} = \\mathbb{E}\\left[\\frac{V}{k}\\right] = \\mathbb{E}\\left[\\frac{1}{k} \\sum_{i \\in I_0} I(p_i \\le p_{(k)}) \\right] = \\sum_{i \\in I_0} \\mathbb{E}\\left[\\frac{I(p_i \\le p_{(k)})}{k}\\right]$$\nThe random variables $p_i$ for $i \\in I_0$ are independent and identically distributed as $\\mathrm{Uniform}(0,1)$. Due to this symmetry, the expectation term $\\mathbb{E}\\left[\\frac{I(p_i \\le p_{(k)})}{k}\\right]$ is the same for all $i \\in I_0$. Let's choose an arbitrary index $j \\in I_0$. Then:\n$$\\mathrm{FDR} = m_0 \\cdot \\mathbb{E}\\left[\\frac{I(p_j \\le p_{(k)})}{k}\\right]$$\nThe threshold $p_{(k)}$ and the index $k$ depend on the entire set of $p$-values, making this expectation difficult to evaluate directly. The core of the proof, as elegantly shown by Benjamini and Yekutieli, involves conditioning on the value of $p_j$.\nLet $A_j$ be the event that $H_j$ is rejected.\n$$\\mathbb{E}\\left[\\frac{I(A_j)}{k}\\right] = P(A_j) \\mathbb{E}\\left[\\frac{1}{k} | A_j\\right] = \\int_0^1 P(A_j | p_j=u) \\mathbb{E}\\left[\\frac{1}{k} | A_j, p_j=u\\right] f(u) du$$\nSince $p_j \\sim \\mathrm{Uniform}(0,1)$, its probability density function is $f(u)=1$ for $u \\in [0,1]$.\nThe event $A_j$ (rejection of $H_j$) given $p_j=u$ means that $u \\le p_{(k)}$. Let $k(u)$ be the rejection index $k$ computed from the set of $m-1$ p-values $\\{p_i\\}_{i \\ne j}$ plus the value $u$. The event $A_j$ given $p_j = u$ means that $u$ is among the $k(u)$ smallest p-values and $p_{(k(u))} \\le \\frac{k(u)}{m}\\alpha$.\nA key insight is that because $p_j$ is independent of the other $p$-values, its value $u$ does not provide information on the ranks of the other $p$-values. The rejection of $H_j$ occurs if $u$ is smaller than a threshold which depends on the other $m-1$ p-values. It can be proven that for any true null hypothesis $H_j$:\n$$P(\\text{reject } H_j) \\le \\frac{\\alpha}{m}$$\nThis property states that the marginal probability of making a Type I error for any specific true null hypothesis is controlled at $\\alpha/m$. Let's sketch why this holds. $H_j$ is rejected if $p_j \\le p_{(k)}$. A sufficient condition for rejection is that $p_j \\le \\frac{\\text{rank}(p_j)}{m}\\alpha$, where $\\text{rank}(p_j)$ is the rank of $p_j$ in the ordered list of all $m$ p-values. Let $r_j = \\text{rank}(p_j)$. The event $\\{p_j \\le \\frac{r_j}{m}\\alpha\\}$ implies that $H_j$ is rejected. The probability of this event for a null $p_j \\sim U(0,1)$ is $\\alpha/m$.\nGiven this, we can make further progress.\n$$\\mathbb{E}\\left[\\frac{I(A_j)}{k}\\right] \\le \\frac{P(A_j)}{1} \\le \\frac{\\alpha}{m}$$\nThis is a loose bound. A tighter argument shows that the expectation of $1/k$ conditioned on rejection is bounded in a helpful way. The definitive proof shows:\n$$\\mathbb{E}\\left[\\frac{V}{k} I(k>0)\\right] \\le \\frac{m_0}{m}\\alpha$$\nThis inequality holds because, for any true null p-value $p_i$, the probability of it being rejected by the BH procedure is at most $\\alpha/m$. Aggregating over the $m_0$ true nulls, the expected number of false discoveries is bounded. The division by $R=k$ normalizes this. The independence assumption is crucial for this result.\nThus, we have $\\mathrm{FDR} = \\mathbb{E}[Q] \\le \\frac{m_0}{m}\\alpha \\le \\alpha$. This completes the derivation of the BH procedure and proves it controls the FDR at level $\\alpha$ for independent tests.\n\n**Part 2: Application to Radiomics Data**\n\nWe are given $m=200$ tests and a target FDR level of $\\alpha=0.05$. The ordered $p$-values, $\\{p_{(i)}\\}_{i=1}^{200}$, are defined by a two-part function:\n1.  For $1 \\le i \\le 120$: $p_{(i)} = 0.0002\\,i$\n2.  For $121 \\le i \\le 200$: $p_{(i)} = 0.024 + 0.0005\\,(i-120)$\n\nAccording to the BH procedure, we must find the largest integer $k \\in \\{1, \\dots, 200\\}$ such that:\n$$p_{(k)} \\le \\frac{k}{m} \\alpha$$\nSubstituting the given values, we get:\n$$p_{(k)} \\le \\frac{k}{200} (0.05) = \\frac{0.05}{200} k = 0.00025\\,k$$\nWe will check this inequality for the two ranges of $i$.\n\nFirst, consider the range $1 \\le i \\le 120$. The condition is:\n$$0.0002\\,i \\le 0.00025\\,i$$\nThis inequality is true for all $i \\ge 0$. Therefore, the condition holds for all $i$ in this range, from $i=1$ to $i=120$. This implies that the final index $k$ must be at least $120$.\n\nNext, consider the range $121 \\le i \\le 200$. The condition is:\n$$0.024 + 0.0005\\,(i-120) \\le 0.00025\\,i$$\nWe need to solve for the largest integer $i$ that satisfies this inequality.\n$$0.024 + 0.0005\\,i - 0.0005 \\times 120 \\le 0.00025\\,i$$\n$$0.024 + 0.0005\\,i - 0.06 \\le 0.00025\\,i$$\n$$0.0005\\,i - 0.036 \\le 0.00025\\,i$$\nRearranging the terms to solve for $i$:\n$$0.0005\\,i - 0.00025\\,i \\le 0.036$$\n$$0.00025\\,i \\le 0.036$$\n$$i \\le \\frac{0.036}{0.00025}$$\n$$i \\le \\frac{36 \\times 10^{-3}}{25 \\times 10^{-5}} = \\frac{36}{25} \\times 10^2 = 1.44 \\times 100 = 144$$\nThe inequality holds for all integers $i \\le 144$. Since we are in the range $121 \\le i \\le 200$, the condition is satisfied for $i = 121, 122, \\dots, 144$.\n\nCombining both ranges, the condition $p_{(i)} \\le \\frac{i}{m}\\alpha$ holds for all $i$ from $1$ to $144$. Let's verify for $i=145$:\nThe threshold for $i=145$ is $0.00025 \\times 145 = 0.03625$.\nThe $p$-value is $p_{(145)} = 0.024 + 0.0005\\,(145-120) = 0.024 + 0.0005 \\times 25 = 0.024 + 0.0125 = 0.0365$.\nSince $p_{(145)} = 0.0365 > 0.03625$, the condition is not met for $i=145$.\n\nTherefore, the largest index $k$ for which $p_{(k)} \\le \\frac{k}{m}\\alpha$ is $k=144$.\n\nThe BH procedure rejects all hypotheses $H_{(i)}$ for $i=1, \\dots, k$. This is equivalent to rejecting all hypotheses $H_j$ whose $p$-value $p_j$ satisfies $p_j \\le p_{(k)}$. The rejection threshold is thus the value of $p_{(k)}$.\nWe calculate the threshold $p_{(144)}$:\n$$p_{(144)} = 0.024 + 0.0005\\,(144-120)$$\n$$p_{(144)} = 0.024 + 0.0005\\,(24)$$\n$$p_{(144)} = 0.024 + 0.012$$\n$$p_{(144)} = 0.036$$\n\nThe BH rejection threshold is $0.036$. Any original $p$-value from the $200$ tests that is less than or equal to $0.036$ will be declared statistically significant.",
            "answer": "$$\\boxed{0.036}$$"
        }
    ]
}