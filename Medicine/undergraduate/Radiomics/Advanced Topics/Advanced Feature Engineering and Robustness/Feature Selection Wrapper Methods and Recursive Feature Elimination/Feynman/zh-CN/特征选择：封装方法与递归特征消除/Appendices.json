{
    "hands_on_practices": [
        {
            "introduction": "递归特征消除（Recursive Feature Elimination, RFE）等封装式方法的核心在于如何量化每个特征的重要性。本实践将引导您从支持向量机（SVM）的基本原理出发，推导出一种在放射组学中广泛应用的特征重要性评分。通过这个练习，您将深入理解模型系数与特征排序之间的直接数学联系，从而揭示 RFE 算法的内在机制。",
            "id": "4539669",
            "problem": "一个放射组学分类任务产生了一个包含 $n$ 个已标记样本的数据集 $\\{(x_i, y_i)\\}_{i=1}^{n}$，其中每个特征向量 $x_i \\in \\mathbb{R}^{p}$ 都在训练集上按特征进行了标准化，以使其均值为零，方差为一，并且每个标签 $y_i \\in \\{-1, +1\\}$。使用最大间隔原则、Hinge 损失和 $\\ell_{2}$ 惩罚来训练一个线性分类器，即软间隔支持向量机 (SVM)。从分离超平面的几何定义以及带有 Hinge 损失和 $\\ell_{2}$ 正则化的软间隔 SVM 原始优化公式出发，推导出已训练分类器的线性决策函数。然后，定义一种用于递归特征消除 (RFE) 的包装器方法，该方法通过在单个特征 $j$ 的系数被强制为零时正则化项的一阶变化来对特征进行排序，并将当前解固定为局部近似。在给定的放射组学特征标准化条件下，提供该特征 $j$ 的特征重要性分数的闭式解析表达式，该表达式用已训练的系数表示。你的最终答案必须是一个无单位的单一解析表达式。",
            "solution": "用户希望我解决一个问题。\n我对用户请求的分析如下：\n1. 我必须扮演一位严谨的 STEM 教授。\n2. 我必须首先验证问题陈述。\n3. 如果有效，我必须提供一个完整且有理有据的解决方案。\n4. 所有数学实体都必须使用 LaTeX 格式。\n5. 不允许使用 Markdown 标题。\n6. 最终答案必须采用特定的方框格式。\n\n### 问题验证\n\n#### 步骤 1：提取已知条件\n-   数据集：$n$ 个已标记样本 $\\{(x_i, y_i)\\}_{i=1}^{n}$。\n-   特征向量：$x_i \\in \\mathbb{R}^{p}$。\n-   特征标准化：每个特征在训练集上都被标准化，以使其均值为零，方差为一。\n-   标签：$y_i \\in \\{-1, +1\\}$。\n-   分类器：使用最大间隔原则训练的线性分类器。\n-   优化模型：带有 Hinge 损失和 $\\ell_2$ 惩罚的软间隔支持向量机 (SVM)。\n-   任务 1：从分离超平面的几何定义以及软间隔 SVM 原始优化公式出发，推导出已训练分类器的线性决策函数。\n-   任务 2：为递归特征消除 (RFE) 定义一种包装器方法，该方法使用特定的排序标准。\n-   排序标准：特征 $j$ 的分数是当特征 $j$ 的系数被强制为零时正则化项的一阶变化，并将当前解固定为局部近似。\n-   任务 3：提供该特征 $j$ 的特征重要性分数的闭式解析表达式。\n\n#### 步骤 2：使用提取的已知条件进行验证\n该问题在科学上植根于机器学习和统计学习理论领域。所提到的概念——支持向量机、Hinge 损失、$\\ell_2$ 正则化、递归特征消除和特征标准化——都是标准且定义明确的。该问题是客观的，并且有定量的规定。它要求推导一个标准结果（SVM 决策函数）和一个基于明确（尽管复杂）定义的标准的特征重要性度量。该问题是自包含的，提供了继续进行所需的所有必要定义。要求使用特征标准化是一个关键细节，它确保了最终的特征重要性分数是可比较的，这是一个精心设计问题的标志。该问题并非无足轻重，因为它要求在有限扰动的背景下仔细解释“一阶变化”这一术语，但它是一个适定问题，并能导出一个唯一的、有意义的解。没有矛盾、事实错误或伪科学主张。\n\n#### 步骤 3：结论和行动\n问题有效。我将继续进行完整的推导和解答。\n\n### 解答推导\n\n该问题包含两个主要部分：首先，推导软间隔 SVM 的线性决策函数；其次，推导一个用于递归特征消除 (RFE) 框架的特定特征重要性分数。\n\n**第一部分：线性 SVM 决策函数**\n\n线性分类器旨在使用超平面将数据点分成两类。在 $p$ 维特征空间中，超平面由满足以下方程的点集 $x$ 定义：\n$$w^T x + b = 0$$\n其中 $w \\in \\mathbb{R}^p$ 是垂直于超平面的权重向量，$b \\in \\mathbb{R}$ 是偏置项或截距项。从原点到超平面的距离是 $\\frac{|b|}{\\|w\\|}$。\n\n软间隔支持向量机旨在找到一个超平面，该超平面在容忍一些错误分类的同时，能最大化间隔（两类之间的“街道”）。这被表述为一个优化问题。带有 Hinge 损失和对权重施加 $\\ell_2$ 惩罚的软间隔 SVM 的原始公式为：\n$$ \\min_{w, b, \\xi} \\left( \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i \\right) $$\n对于每个样本 $i=1, \\dots, n$，需满足以下约束条件：\n$$ y_i(w^T x_i + b) \\ge 1 - \\xi_i $$\n$$ \\xi_i \\ge 0 $$\n这里，$\\|w\\|^2 = w^T w$ 是权重向量的欧几里得范数的平方，即 $\\ell_2$ 正则化项。最小化此项等同于最大化几何间隔，即 $\\frac{1}{\\|w\\|}$。变量 $\\xi_i \\ge 0$ 是松弛变量，它允许单个数据点位于其类别特定间隔边界的错误一侧（$0  \\xi_i \\le 1$）或被错误分类（$\\xi_i  1$）。项 $C \\sum_{i=1}^{n} \\xi_i$ 是对这些违规的惩罚，其中超参数 $C  0$ 控制着在最大化间隔和最小化训练集上的分类错误之间的权衡。这个总和是总 Hinge 损失的表达式。\n\n这个优化问题是凸的，可以通过求解找到最优系数，我们将其表示为 $w^*$ 和 $b^*$。一旦找到这些最优参数，分类器就完全训练好了。\n\n为了对一个新的、未见过的数据点 $x$ 进行分类，我们评估它落在分离超平面 $w^{*T} x + b^* = 0$ 的哪一侧。线性决策函数是超平面方程本身的输出：\n$$ g(x) = (w^*)^T x + b^* $$\n最终的分类则由该函数的符号决定，$f(x) = \\text{sign}(g(x))$。问题要求的是线性决策函数本身，即 $g(x)$。\n\n**第二部分：RFE 特征重要性分数**\n\n递归特征消除 (RFE) 是一种基于包装器的特征选择方法，其工作原理是迭代地训练模型、对特征进行排序，并消除最不重要的一个（或多个）特征。RFE 的核心是用于对特征进行排序的标准。\n\n问题将此排序标准定义为“在单个特征 $j$ 的系数被强制为零时正则化项的一阶变化，并将当前解固定为局部近似。”\n\n设从 SVM 训练得到的权重向量为 $w \\in \\mathbb{R}^p$。（为简单起见，我们省略了星号标记，但 $w$ 代表最优解）。SVM 目标函数中的正则化项是：\n$$ R(w) = \\frac{1}{2} \\|w\\|^2 = \\frac{1}{2} \\sum_{k=1}^p w_k^2 $$\n我们被要求找出将单个系数 $w_j$ 强制为零对 $R(w)$ 的影响。设 $w'$ 是新向量，其中当 $k \\ne j$ 时 $w'_k = w_k$，且 $w'_j = 0$。这可以写成 $w'_k = (1-\\delta_{jk})w_k$，其中 $\\delta_{jk}$ 是克罗内克 δ。\n\n权重向量的扰动为 $\\Delta w = w' - w$。该向量只有一个非零分量：$\\Delta w_j = 0 - w_j = -w_j$。所有其他分量 $\\Delta w_k = 0$ 当 $k \\ne j$。\n\n正则化项的变化是 $\\Delta R = R(w') - R(w) = R(w+\\Delta w) - R(w)$。“一阶变化”指的是 $R$ 在点 $w$ 附近多元泰勒级数展开中的线性项。展开式为：\n$$ \\Delta R = \\nabla R(w)^T \\Delta w + \\frac{1}{2} \\Delta w^T H_R(w) \\Delta w + \\dots $$\n其中 $\\nabla R(w)$ 是 $R$ 的梯度，$H_R(w)$ 是其海森矩阵。一阶变化，我们记为 $\\delta_1 R$，是该级数的第一项：\n$$ \\delta_1 R = \\nabla R(w)^T \\Delta w $$\n首先，我们计算正则化项 $R(w) = \\frac{1}{2} w^T w$ 的梯度：\n$$ \\nabla R(w) = w $$\n现在，我们计算梯度和扰动向量 $\\Delta w$ 的点积：\n$$ \\delta_1 R_j = w^T \\Delta w = \\sum_{k=1}^p w_k \\Delta w_k $$\n由于 $\\Delta w$ 仅在索引 $j$ 处有一个非零分量，此和简化为：\n$$ \\delta_1 R_j = w_j \\Delta w_j = w_j (-w_j) = -w_j^2 $$\n这是在消除特征 $j$ 时正则化项的一阶变化。\n\n问题要求的是“特征重要性分数”。分数通常是一个非负值，其中较小的值表示重要性较低。我们计算出的变化是负的，因为将系数强制为零必然会减小或保持 $\\|w\\|^2$ 的值。这种变化的幅度反映了该特征的影响。一个具有大绝对值系数 $w_j$ 的特征在被移除时会导致正则化项发生大的变化。因此，特征 $j$ 的重要性分数 $S_j$ 是这个一阶变化的绝对值：\n$$ S_j = |\\delta_1 R_j| = |-w_j^2| = w_j^2 $$\n然后根据此分数对特征进行排序。在 RFE 过程中，得分 $S_j$ 最小（即 $|w_j|$ 最小）的特征被认为是最不重要的，并在该次迭代中被消除。\n\n放射组学特征被标准化为零均值和单位方差这一条件至关重要。这确保了特征处于可比较的尺度上，从而使其对应的已训练系数 $w_j$ 的大小可以直接比较。如果不进行标准化，特征系数的大小会受到特征尺度的影响，使得 $w_j^2$ 成为一个有偏且不可靠的重要性度量。",
            "answer": "$$\\boxed{w_j^2}$$"
        },
        {
            "introduction": "理解了特征重要性的度量标准后，下一步便是将其应用于一个稳健的实验设计中，以进行可靠的特征选择和性能评估。本练习将探讨在封装式特征选择中嵌套交叉验证（nested cross-validation）的计算成本，这是一种避免过拟合和获得无偏性能估计的关键方法。通过计算模型训练的总次数，您将对这类方法的计算密集性有一个具体的认识。",
            "id": "4539569",
            "problem": "一项放射组学研究旨在利用从计算机断层扫描中提取的高维定量图像特征，构建一个用于病变恶性程度的二元分类器。为控制过拟合并评估泛化性能，该研究采用基于封装法（wrapper-based）的特征选择，具体使用递归特征消除（Recursive Feature Elimination, RFE）方法，并在嵌套交叉验证设计中进行评估。外部交叉验证有 $K_{\\mathrm{out}}$ 折，用于在留出的外部测试折上评估泛化能力；内部交叉验证有 $K_{\\mathrm{in}}$ 折，用于在每个外部训练分区内选择特征子集。\n\n假设采用以下协议：\n- 每当学习算法在数据集分区上进行训练时，计为一次“模型拟合”；预测和评分不计入拟合次数。\n- 在每个外部折中，一个步进式封装法搜索会检查 $E$ 个候选特征子集。对于每个候选子集，其性能通过内部 $K_{\\mathrm{in}}$ 折交叉验证进行评估，这需要在每个内部折上精确训练一个模型（即每次内部评估需要 $K_{\\mathrm{in}}$ 次拟合）。\n- 在内部搜索为给定的外部折确定最佳特征子集后，会使用该子集在整个外部训练分区上训练一个最终模型，然后在相应的留出外部测试折上进行评估。这个最终模型在每个外部折中计为额外的一次拟合。\n- 在所有外部折完成后，不执行额外的重新拟合，且推理时的预测不计入总拟合次数。\n\n从 $K$ 折交叉验证的基本定义（将数据划分为 $K$ 个不相交的折，并在每次评估中拟合模型 $K$ 次，即每个留出的折拟合一次）出发，根据上述规则，推导出一个关于 $K_{\\mathrm{out}}$、$K_{\\mathrm{in}}$ 和 $E$ 的通用表达式，用于计算完成整个嵌套协议所需的模型拟合总次数。然后，计算当 $K_{\\mathrm{out}}=5$，$K_{\\mathrm{in}}=5$ 且步进式搜索在每个外部折中执行 $E=50$ 次内部评估时的模型拟合总次数。请以无单位的单个整数形式提供最终答案。无需四舍五入。",
            "solution": "问题陈述是有效的。它具有科学依据，问题定义明确且客观。它描述了一个标准的机器学习协议（用于基于封装法的特征选择的嵌套交叉验证），并要求以模型训练操作的次数来计算总计算成本。所有术语都有定义，且逻辑是自洽的。\n\n设 $N_{\\text{fits}}$ 为整个嵌套交叉验证协议所需的模型拟合总次数。该协议由一个包含 $K_{\\mathrm{out}}$ 折的外部循环和一个包含 $K_{\\mathrm{in}}$ 折的内部循环组成。我们通过分析单个外部折内的操作，然后乘以外部折的数量 $K_{\\mathrm{out}}$，来推导拟合的总次数。\n\n考虑外部循环的单次迭代（即 $K_{\\mathrm{out}}$ 折中的一折）。数据集被划分为一个外部训练集和一个外部测试集。在此阶段，模型拟合操作仅在外部训练集上进行。\n\n在这单个外部折中，模型拟合有两个阶段：\n$1$. 用于特征选择的内部交叉验证循环。\n$2$. 为该外部折训练一个最终模型。\n\n让我们分析每个阶段的拟合次数。\n\n阶段 $1$：用于特征选择的内部交叉验证\n问题陈述中提到，一个封装法搜索会检查 $E$ 个候选特征子集。为了评估这 $E$ 个子集中每一个的性能，会在外部训练数据上执行一次内部 $K_{\\mathrm{in}}$ 折交叉验证。\n根据 $K$ 折交叉验证的定义以及题目明确的说明，单次内部 $K_{\\mathrm{in}}$ 折评估需要训练模型 $K_{\\mathrm{in}}$ 次。\n因此，评估一个候选特征子集所需的模型拟合次数为 $K_{\\mathrm{in}}$。\n由于有 $E$ 个候选子集需要评估，内部循环的模型拟合总次数为：\n$$N_{\\text{inner}} = E \\times K_{\\mathrm{in}}$$\n\n阶段 $2$：为外部折训练最终模型\n在内部循环完成并确定了最佳特征子集后，会训练一个最终模型。该模型使用选定的最优特征子集，并在*整个*外部训练分区上进行训练。问题明确指出，这计为额外的一次拟合。\n$$N_{\\text{final\\_fit}} = 1$$\n\n单个外部折的总拟合次数 $N_{\\text{per\\_outer\\_fold}}$ 是内部循环的拟合次数与最终模型拟合次数之和：\n$$N_{\\text{per\\_outer\\_fold}} = N_{\\text{inner}} + N_{\\text{final\\_fit}} = (E \\times K_{\\mathrm{in}}) + 1$$\n\n整个协议的总拟合次数\n上述过程对外部交叉验证的 $K_{\\mathrm{out}}$ 个折中的每一折都独立执行。因此，整个嵌套协议的总模型拟合次数 $N_{\\text{fits}}$ 是每折的拟合次数乘以外部折的数量：\n$$N_{\\text{fits}} = K_{\\mathrm{out}} \\times N_{\\text{per\\_outer\\_fold}}$$\n代入 $N_{\\text{per\\_outer\\_fold}}$ 的表达式，我们得到通用公式：\n$$N_{\\text{fits}} = K_{\\mathrm{out}} \\times (E \\times K_{\\mathrm{in}} + 1)$$\n\n现在，我们使用问题中给出的具体数值计算总拟合次数：$K_{\\mathrm{out}}=5$，$K_{\\mathrm{in}}=5$ 和 $E=50$。\n将这些值代入通用表达式中：\n$$N_{\\text{fits}} = 5 \\times (50 \\times 5 + 1)$$\n首先，我们计算括号内的项：\n$$50 \\times 5 + 1 = 250 + 1 = 251$$\n然后，我们完成计算：\n$$N_{\\text{fits}} = 5 \\times 251$$\n$$N_{\\text{fits}} = 1255$$\n\n因此，完成整个嵌套协议所需的模型拟合总次数为 $1255$。",
            "answer": "$$\\boxed{1255}$$"
        },
        {
            "introduction": "即使是设计精良的算法也可能在复杂的现实数据中犯错，特征选择也不例外。本实践构建了一个具有挑战性的场景，其中特征之间的高度相关性（即多重共线性）和抽样随机性会误导 RFE 算法，使其错误地移除了真正重要的特征。这个练习旨在强调批判性地分析特征选择结果的重要性，并揭示了封装式方法在某些条件下固有的不稳定性。",
            "id": "4539571",
            "problem": "一个影像组学团队正在构建一个二元恶性肿瘤分类器，但为了从第一性原理出发进行推理，他们使用平方误差下的线性回归作为封装式评估器来近似学习步骤。考虑三个标准化的影像组学特征：$x_1$ (一阶强度均值)，$x_2$ (高斯平滑强度均值)，和 $x_3$ (小波能量)。所有特征都标准化为零均值和单位方差。递归特征消除 (RFE) 程序在每一步中都按模型系数的绝对值大小对特征进行排序。封装式评估器中的模型是岭回归，系数上的惩罚项为 $\\lambda$。\n\n真实生成模型：恶性程度得分 $y$ 服从 $y = x_1 + \\varepsilon$，其中 $\\varepsilon$ 是零均值噪声，与所有特征独立，其方差为 $\\operatorname{Var}(\\varepsilon) = \\sigma_\\varepsilon^2$。在总体中，特征相关矩阵以及与 $y$ 的相关性满足：\n- 对 $i \\in \\{1,2,3\\}$，有 $\\operatorname{Var}(x_i) = 1$。\n- $\\operatorname{Corr}(x_1, x_2) = 0.95$，$\\operatorname{Corr}(x_1, x_3) = 0.70$，$\\operatorname{Corr}(x_2, x_3) = 0.95$。\n- $\\operatorname{Corr}(x_1, y) = 1$，$\\operatorname{Corr}(x_2, y) = 0.95$，$\\operatorname{Corr}(x_3, y) = 0.70$。\n\n在一个大小为 $n$ (较小) 的特定训练折上，抽样变异性产生了以下由封装式评估器使用的经验相关性：\n- $\\widehat{\\operatorname{Corr}}(x_1, y) = 0.85$，$\\widehat{\\operatorname{Corr}}(x_2, y) = 0.85$，$\\widehat{\\operatorname{Corr}}(x_3, y) = 0.87$。\n- 经验特征间相关性与其总体对应值保持接近：$\\widehat{\\operatorname{Corr}}(x_1, x_2) = 0.95$，$\\widehat{\\operatorname{Corr}}(x_1, x_3) = 0.70$，$\\widehat{\\operatorname{Corr}}(x_2, x_3) = 0.95$。\n\n假设在每个 RFE 步骤中，都在此折上拟合惩罚项为 $\\lambda = 0.30$ 的岭回归，排序标准为拟合系数的绝对值。在标准化的、大样本极限下，使用岭估计量的正规方程近似，用该折的经验矩替换 $\\Sigma_{xx}$ 和 $\\Sigma_{xy}$：\n$$\n\\hat{w} \\approx \\left(\\Sigma_{xx} + \\lambda I\\right)^{-1} \\Sigma_{xy},\n$$\n其中 $\\Sigma_{xx}$ 是 $(x_1, x_2, x_3)$ 的 $3 \\times 3$ 经验相关矩阵，$\\Sigma_{xy}$ 是与 $y$ 的经验相关性向量。在第二个 RFE 步骤中 (丢弃一个特征后)，使用相应的 $2 \\times 2$ 子矩阵和子向量。\n\n最后，为了根据真实情况 $y = x_1 + \\varepsilon$ 来判断次优性，回想一下，对于单特征线性预测器 $\\hat{y} = \\gamma x$，在新数据上的均方预测误差为\n$$\n\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + \\operatorname{Var}\\left(x_1 - \\gamma^\\star x\\right),\n$$\n其中 $\\gamma^\\star = \\operatorname{Cov}(y, x)/\\operatorname{Var}(x)$；在标准化和 $\\operatorname{Corr}(x_1, x) = \\rho$ 的条件下，这可以简化为\n$$\n\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + 1 - \\rho^2.\n$$\n\n取 $\\sigma_\\varepsilon^2 = 0.20$。\n\n哪个选项最能描述在此折上的实际 RFE 消除路径及其后果，包括最终子集在真实情况下的定量次优性？\n\nA. 首先消除 $x_2$，然后消除 $x_3$，最终剩下 $\\{x_1\\}$。没有发生排序反转；最终子集达到 $\\operatorname{MSE} = \\sigma_\\varepsilon^2 = 0.20$。\n\nB. 首先消除 $x_2$，然后消除 $x_1$，最终剩下 $\\{x_3\\}$。在移除一个冗余稳定器 ($x_2$) 后，发生了下游排序反转，最终子集的真实误差膨胀至 $\\operatorname{MSE} = \\sigma_\\varepsilon^2 + 1 - \\rho_{13}^2 = 0.20 + 1 - 0.49 = 0.71$。\n\nC. 首先消除 $x_3$，然后消除 $x_2$，最终剩下 $\\{x_1\\}$。初始排序将 $x_3$ 列为最不重要；最终子集达到 $\\operatorname{MSE} = 0.20$。\n\nD. 仅消除 $x_2$ 并停止，因为移除冗余特征在岭回归下不会引起下游排序变化；最终子集是 $\\{x_1, x_3\\}$，相对于 $\\{x_1\\}$ 没有次优性。",
            "solution": "用户希望我分析一个递归特征消除 (RFE) 过程，并确定其结果和次优性。\n\n### 步骤 1：问题验证\n\n我将首先验证问题陈述。\n\n**给定条件：**\n- **任务**：分析一个用线性回归近似的二元分类器的递归特征消除 (RFE) 过程。\n- **特征**：三个标准化特征 $x_1$、$x_2$、$x_3$，满足 $\\mathbb{E}[x_i]=0$ 和 $\\operatorname{Var}(x_i) = 1$。\n- **封装式模型**：岭回归，惩罚项 $\\lambda = 0.30$。\n- **RFE 排序标准**：拟合的岭回归系数的绝对值 $|\\hat{w}_i|$。\n- **真实模型**：$y = x_1 + \\varepsilon$，其中 $\\varepsilon$ 是零均值噪声，方差为 $\\operatorname{Var}(\\varepsilon) = \\sigma_\\varepsilon^2 = 0.20$，且 $\\varepsilon$ 与所有特征独立。\n- **总体特征相关性**：\n    - $\\operatorname{Corr}(x_1, x_2) = 0.95$\n    - $\\operatorname{Corr}(x_1, x_3) = 0.70$\n    - $\\operatorname{Corr}(x_2, x_3) = 0.95$\n- **总体特征-结果相关性** (从真实模型推导)：\n    - $\\operatorname{Corr}(x_1, y) = 1$ (这是从 $\\operatorname{Cov}(x_1, y) = \\operatorname{Cov}(x_1, x_1+\\varepsilon)=1$ 和 $\\operatorname{Var}(y) = \\operatorname{Var}(x_1+\\varepsilon)=\\operatorname{Var}(x_1)+\\operatorname{Var}(\\varepsilon)=1+\\sigma_\\varepsilon^2$ 推导出来的。所以 $\\operatorname{Corr}(x_1, y) = 1/\\sqrt{1+\\sigma_\\varepsilon^2}$。问题陈述中 $\\operatorname{Corr}(x_1, y)=1$ 意味着在此相关性计算的总体背景下 $\\sigma_\\varepsilon^2 = 0$，这存在轻微的不一致性，但我们将按照给定的相关性值进行计算，因为这是问题设置的上下文。)\n    - $\\operatorname{Corr}(x_2, y) = 0.95$\n    - $\\operatorname{Corr}(x_3, y) = 0.70$\n- **训练折上的经验相关性**：\n    - 特征间相关性与总体相同：$\\widehat{\\operatorname{Corr}}(x_1, x_2) = 0.95$、$\\widehat{\\operatorname{Corr}}(x_1, x_3) = 0.70$、$\\widehat{\\operatorname{Corr}}(x_2, x_3) = 0.95$。\n    - 由于抽样，特征-结果相关性不同：$\\widehat{\\operatorname{Corr}}(x_1, y) = 0.85$、$\\widehat{\\operatorname{Corr}}(x_2, y) = 0.85$、$\\widehat{\\operatorname{Corr}}(x_3, y) = 0.87$。\n- **岭估计量公式**：$\\hat{w} \\approx \\left(\\Sigma_{xx} + \\lambda I\\right)^{-1} \\Sigma_{xy}$，使用经验相关矩阵。\n- **MSE 公式**：对于单特征预测器 $\\hat{y}=\\gamma x$，真实预测误差为 $\\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + 1 - \\rho^2$，其中 $\\rho = \\operatorname{Corr}(x_1, x)$ 是总体相关性。\n\n**验证结论：**\n该问题陈述是统计学习中一个明确定义的理论练习。它使用了 RFE、岭回归、多重共线性以及总体参数和样本统计量之间的区别等既定概念。提供的数值和公式是一致的，并允许确定性求解。没有科学或逻辑上的矛盾会使该设置无效。在计算 $\\operatorname{Corr}(x_1, y)=1$ 时注意到的微小不一致是次要的，不影响计算步骤，因为我们被指示使用给定的相关值。问题是**有效的**。\n\n### 步骤 2：求解推导\n\nRFE 过程迭代地移除在封装式模型中具有最低绝对系数值的特征。我们将逐步追踪此过程。\n\n**RFE 步骤 1：使用特征 $\\{x_1, x_2, x_3\\}$ 的模型**\n\n岭回归系数 $\\hat{w} = (\\hat{w}_1, \\hat{w}_2, \\hat{w}_3)^T$ 使用提供的公式和经验相关矩阵进行计算。\n\n经验特征-特征相关矩阵 $\\Sigma_{xx}$ 为：\n$$ \\Sigma_{xx} = \\begin{pmatrix} 1  0.95  0.70 \\\\ 0.95  1  0.95 \\\\ 0.70  0.95  1 \\end{pmatrix} $$\n\n经验特征-结果相关向量 $\\Sigma_{xy}$ 为：\n$$ \\Sigma_{xy} = \\begin{pmatrix} 0.85 \\\\ 0.85 \\\\ 0.87 \\end{pmatrix} $$\n\n岭惩罚项为 $\\lambda = 0.30$。我们计算 $\\hat{w} = (\\Sigma_{xx} + \\lambda I)^{-1} \\Sigma_{xy}$。\n首先，我们找到要求逆的矩阵 $A = \\Sigma_{xx} + \\lambda I$：\n$$ A = \\begin{pmatrix} 1.30  0.95  0.70 \\\\ 0.95  1.30  0.95 \\\\ 0.70  0.95  1.30 \\end{pmatrix} $$\n\n$A$ 的行列式为：\n$$ \\det(A) = 1.30(1.30^2 - 0.95^2) - 0.95(1.30 \\cdot 0.95 - 0.70 \\cdot 0.95) + 0.70(0.95^2 - 1.30 \\cdot 0.70) $$\n$$ \\det(A) = 1.30(1.69 - 0.9025) - 0.95(1.235 - 0.665) + 0.70(0.9025 - 0.91) $$\n$$ \\det(A) = 1.30(0.7875) - 0.95(0.57) + 0.70(-0.0075) = 1.02375 - 0.5415 - 0.00525 = 0.477 $$\n\n逆矩阵 $A^{-1}$ 是 $\\frac{1}{\\det(A)} \\operatorname{adj}(A)$。伴随矩阵 $\\operatorname{adj}(A)$ 是代数余子式矩阵的转置。\n$$ \\operatorname{adj}(A) = \\begin{pmatrix} 0.7875  -0.57  -0.0075 \\\\ -0.57  1.20  -0.57 \\\\ -0.0075  -0.57  0.7875 \\end{pmatrix} $$\n\n现在我们计算 $\\hat{w} = A^{-1} \\Sigma_{xy}$：\n$$ \\hat{w} = \\frac{1}{0.477} \\begin{pmatrix} 0.7875  -0.57  -0.0075 \\\\ -0.57  1.20  -0.57 \\\\ -0.0075  -0.57  0.7875 \\end{pmatrix} \\begin{pmatrix} 0.85 \\\\ 0.85 \\\\ 0.87 \\end{pmatrix} $$\n$$ \\hat{w} = \\frac{1}{0.477} \\begin{pmatrix} 0.7875(0.85) - 0.57(0.85) - 0.0075(0.87) \\\\ -0.57(0.85) + 1.20(0.85) - 0.57(0.87) \\\\ -0.0075(0.85) - 0.57(0.85) + 0.7875(0.87) \\end{pmatrix} = \\frac{1}{0.477} \\begin{pmatrix} 0.669375 - 0.4845 - 0.006525 \\\\ -0.4845 + 1.02 - 0.4959 \\\\ -0.006375 - 0.4845 + 0.685125 \\end{pmatrix} $$\n$$ \\hat{w} = \\frac{1}{0.477} \\begin{pmatrix} 0.17835 \\\\ 0.03960 \\\\ 0.19425 \\end{pmatrix} \\approx \\begin{pmatrix} 0.3739 \\\\ 0.0830 \\\\ 0.4072 \\end{pmatrix} $$\n\n系数的绝对值为 $|\\hat{w}_1| \\approx 0.374$、$|\\hat{w}_2| \\approx 0.083$ 和 $|\\hat{w}_3| \\approx 0.407$。\n绝对系数最小的特征是 $x_2$。因此，**RFE 首先消除 $x_2$。**\n\n**RFE 步骤 2：使用特征 $\\{x_1, x_3\\}$ 的模型**\n\n现在，RFE 过程继续处理剩余的特征 $x_1$ 和 $x_3$。我们重新拟合岭回归模型。\n新的特征相关矩阵 $\\Sigma_{xx}^{(2)}$ 和结果相关向量 $\\Sigma_{xy}^{(2)}$ 为：\n$$ \\Sigma_{xx}^{(2)} = \\begin{pmatrix} 1  0.70 \\\\ 0.70  1 \\end{pmatrix}, \\quad \\Sigma_{xy}^{(2)} = \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} $$\n\n我们计算 $\\hat{w}^{(2)} = (\\Sigma_{xx}^{(2)} + \\lambda I)^{-1} \\Sigma_{xy}^{(2)}$。\n要求逆的矩阵 $B = \\Sigma_{xx}^{(2)} + \\lambda I$ 为：\n$$ B = \\begin{pmatrix} 1.30  0.70 \\\\ 0.70  1.30 \\end{pmatrix} $$\n\n行列式为 $\\det(B) = 1.30^2 - 0.70^2 = 1.69 - 0.49 = 1.20$。\n逆矩阵为：\n$$ B^{-1} = \\frac{1}{1.20} \\begin{pmatrix} 1.30  -0.70 \\\\ -0.70  1.30 \\end{pmatrix} $$\n\n现在我们计算新的系数 $\\hat{w}^{(2)} = (\\hat{w}_1^{(2)}, \\hat{w}_3^{(2)})^T$：\n$$ \\hat{w}^{(2)} = \\frac{1}{1.20} \\begin{pmatrix} 1.30  -0.70 \\\\ -0.70  1.30 \\end{pmatrix} \\begin{pmatrix} 0.85 \\\\ 0.87 \\end{pmatrix} $$\n$$ \\hat{w}^{(2)} = \\frac{1}{1.20} \\begin{pmatrix} 1.30(0.85) - 0.70(0.87) \\\\ -0.70(0.85) + 1.30(0.87) \\end{pmatrix} = \\frac{1}{1.20} \\begin{pmatrix} 1.105 - 0.609 \\\\ -0.595 + 1.131 \\end{pmatrix} = \\frac{1}{1.20} \\begin{pmatrix} 0.496 \\\\ 0.536 \\end{pmatrix} $$\n$$ \\hat{w}^{(2)} \\approx \\begin{pmatrix} 0.4133 \\\\ 0.4467 \\end{pmatrix} $$\n\n绝对值为 $|\\hat{w}_1^{(2)}| \\approx 0.413$ 和 $|\\hat{w}_3^{(2)}| \\approx 0.447$。\n我们有 $|\\hat{w}_1^{(2)}|  |\\hat{w}_3^{(2)}|$。系数较小的特征是 $x_1$。因此，**RFE 第二步消除 $x_1$。**\n\n最终剩下的特征是 $\\{x_3\\}$。\n\n**后果分析**\n\n真实模型是 $y = x_1 + \\varepsilon$，意味着 $x_1$ 是唯一真正具有因果关系的特征。RFE 过程被小样本上的经验相关性误导，错误地选择了 $x_3$。我们必须使用提供的 MSE 公式来量化此选择的次优性，该公式根据真实情况评估性能。\n\n基于特征 $x$ 的单特征预测器的 MSE 为：\n$$ \\operatorname{MSE}(x) = \\sigma_\\varepsilon^2 + 1 - \\rho^2 $$\n其中 $\\rho = \\operatorname{Corr}(x_1, x)$ 是真实特征 $x_1$ 和所选特征 $x$ 之间的**总体** (真实) 相关性。\n\n对于所选特征 $x_3$，我们使用总体相关性 $\\rho = \\operatorname{Corr}(x_1, x_3) = 0.70$。\n给定 $\\sigma_\\varepsilon^2 = 0.20$。\n$$ \\operatorname{MSE}(x_3) = 0.20 + 1 - (0.70)^2 = 0.20 + 1 - 0.49 = 0.20 + 0.51 = 0.71 $$\n\n作为比较，最优的单特征是 $x_1$。其 MSE 将为：\n$$ \\operatorname{MSE}(x_1) = 0.20 + 1 - \\operatorname{Corr}(x_1, x_1)^2 = 0.20 + 1 - 1^2 = 0.20 $$\n这是不可约误差 $\\sigma_\\varepsilon^2$。选择 $x_3$ 导致预测误差显著增加。\n\n### 步骤 3：逐项分析\n\n**A. 首先消除 $x_2$，然后消除 $x_3$，最终剩下 $\\{x_1\\}$。没有发生排序反转；最终子集达到 $\\operatorname{MSE} = \\sigma_\\varepsilon^2 = 0.20$。**\n- **结论：不正确。**\n- **理由：** 消除路径的第一部分是正确的 (消除 $x_2$)，但第二步是不正确的。我们的计算表明接下来消除的是 $x_1$，而不是 $x_3$。最终子集是 $\\{x_3\\}$，而不是 $\\{x_1\\}$。因此，最终的 MSE 不是 $0.20$。\n\n**B. 首先消除 $x_2$，然后消除 $x_1$，最终剩下 $\\{x_3\\}$。在移除一个冗余稳定器 ($x_2$) 后，发生了下游排序反转，最终子集的真实误差膨胀至 $\\operatorname{MSE} = \\sigma_\\varepsilon^2 + 1 - \\rho_{13}^2 = 0.20 + 1 - 0.49 = 0.71$。**\n- **结论：正确。**\n- **理由：** 此选项正确地识别了整个消除路径：先是 $x_2$，然后是 $x_1$，留下 $\\{x_3\\}$。它还正确地计算了次优特征 $x_3$ 的真实 MSE 为 $0.71$。关于“排序反转”的叙述性描述正确地捕捉了这样一种现象：贪心算法受到多重共线性 ($x_2$ 是冗余/共线的) 和抽样噪声的影响，在后续步骤中消除了真实特征 ($x_1$)。\n\n**C. 首先消除 $x_3$，然后消除 $x_2$，最终剩下 $\\{x_1\\}$。初始排序将 $x_3$ 列为最不重要；最终子集达到 $\\operatorname{MSE} = 0.20$。**\n- **结论：不正确。**\n- **理由：** 我们第一步的计算表明，$x_2$ 的绝对系数最小，而不是 $x_3$。实际上，$x_3$ 的系数最大。所描述的整个消除路径都是错误的。\n\n**D. 仅消除 $x_2$ 并停止，因为移除冗余特征在岭回归下不会引起下游排序变化；最终子集是 $\\{x_1, x_3\\}$，相对于 $\\{x_1\\}$ 没有次优性。**\n- **结论：不正确。**\n- **理由：** 此选项在多个方面都是不正确的。首先，RFE 是一个迭代过程，会一直持续到剩下预定数量的特征为止 (在此问题中默认为一个)。它不会在一步之后停止。其次，“移除冗余特征不会引起下游排序变化”的前提是错误的。移除一个特征并重新拟合模型，通常会改变系数，从而可能改变剩余特征的排序。我们的计算明确显示，$x_1$ 和 $x_3$ 的系数在步骤 1 和步骤 2 之间发生了变化。",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}