{
    "hands_on_practices": [
        {
            "introduction": "Understanding the curse of dimensionality begins with appreciating the sheer scale of high-dimensional spaces. This first exercise provides a direct and tangible calculation to demonstrate how data requirements for even simple grid-based methods grow at an astonishing exponential rate. By working through this problem , you will develop a foundational intuition for why high-dimensional data is inherently sparse.",
            "id": "2439690",
            "problem": "In a computational finance setting, suppose you are constructing a nonparametric histogram-based joint density estimator for a state vector of $d=10$ risk factors to study portfolio Value-at-Risk (VaR) dynamics. You discretize each dimension into $b=3$ equal-width bins, creating a regular grid over the joint state space. If you require the dataset to have, on average, exactly $1$ observation per grid cell, how many observations are needed? Provide your answer as an exact integer with no rounding.",
            "solution": "The problem statement is first subjected to validation.\n\nGivens are extracted verbatim:\n1.  Field: Computational finance.\n2.  Topic context: Nonparametric histogram-based joint density estimator for a state vector.\n3.  Number of dimensions (risk factors): $d=10$.\n4.  Number of bins per dimension: $b=3$.\n5.  Grid structure: A regular grid over the joint state space.\n6.  Required data density: On average, exactly $1$ observation per grid cell.\n7.  Question: How many observations are needed?\n8.  Answer format: Exact integer, no rounding.\n\nThe problem is evaluated against the validation criteria.\n-   **Scientific Grounding**: The problem is correctly framed within the context of nonparametric statistics and computational finance. The concept of discretizing a multi-dimensional space and the resulting exponential growth in data requirements (the curse of dimensionality) is a standard and fundamental topic. The premise is scientifically sound.\n-   **Well-Posedness**: The problem is well-posed. It provides all necessary parameters ($d$, $b$, and the target data density) to calculate a unique solution. The question is unambiguous.\n-   **Objectivity**: The problem is stated in objective, formal language, free of subjective or speculative content.\n\nThe problem is deemed valid as it does not violate any of the specified criteria for invalidity. It is a straightforward, well-defined quantitative problem. We may proceed with the solution.\n\nLet $d$ be the number of dimensions of the state vector, and let $b$ be the number of bins into which each dimension is discretized. From the problem statement, we have $d=10$ and $b=3$.\n\nThe histogram-based density estimator is constructed on a regular grid in a $d$-dimensional space. The total number of cells (or hypercubes) in this grid, which we shall denote as $N_{cells}$, is the product of the number of bins along each of the $d$ dimensions. As each dimension is partitioned into $b$ bins, the total number of cells is given by:\n$$\nN_{cells} = \\underbrace{b \\times b \\times \\cdots \\times b}_{d \\text{ times}} = b^d\n$$\nThe problem stipulates that the dataset must contain, on average, exactly $1$ observation per grid cell. Let $N_{obs}$ be the total number of observations required. The average number of observations per cell is the ratio of the total number of observations to the total number of cells:\n$$\n\\text{Average observations per cell} = \\frac{N_{obs}}{N_{cells}}\n$$\nAccording to the problem's requirement, this average must be equal to $1$:\n$$\n\\frac{N_{obs}}{N_{cells}} = 1\n$$\nThis implies that the required number of observations must be equal to the total number of cells in the grid:\n$$\nN_{obs} = N_{cells}\n$$\nSubstituting the expression for $N_{cells}$, we find the required number of observations as a function of $d$ and $b$:\n$$\nN_{obs} = b^d\n$$\nNow, we substitute the provided numerical values, $d=10$ and $b=3$, into this equation to find the specific number of observations needed.\n$$\nN_{obs} = 3^{10}\n$$\nWe compute the exact integer value for this expression:\n$$\n3^{10} = (3^2)^5 = 9^5 = 9 \\times 9 \\times 9 \\times 9 \\times 9 = 81 \\times 81 \\times 9 = 6561 \\times 9\n$$\nPerforming the final multiplication:\n$$\n6561 \\times 9 = 59049\n$$\nTherefore, a total of $59049$ observations are required. This result starkly illustrates the curse of dimensionality: even with a coarse discretization of only $3$ bins per dimension, the data requirement for covering a $10$-dimensional space becomes substantial.",
            "answer": "$$\n\\boxed{59049}\n$$"
        },
        {
            "introduction": "Beyond data sparsity, high-dimensional spaces possess bizarre geometric properties that defy our three-dimensional intuition. This problem  explores one of the most famous examples: the volume of a hypersphere inscribed within a hypercube. Discovering what happens to the relative volume as the dimension grows reveals why many computational methods, like rejection sampling, become infeasible in high dimensions.",
            "id": "2439712",
            "problem": "An econometrician implements a rejection sampling step inside a Monte Carlo (MC) estimator for a high-dimensional structural model. At each draw, a point is sampled uniformly from the hypercube $\\left[-r, r\\right]^{d}$ and accepted if and only if its Euclidean norm is less than or equal to $r$. The acceptance probability is equal to the ratio of the volume of the $d$-dimensional hypersphere of radius $r$ to the volume of the $d$-dimensional hypercube with side length $2 r$. For a fixed radius $r>0$, determine the limiting acceptance probability as the dimension $d \\to \\infty$. Provide your final answer as a single number. No rounding is required.",
            "solution": "The first step is to validate the problem statement. The problem asks for the limiting acceptance probability of a rejection sampling scheme in a high-dimensional space.\n\nThe givens are:\n1.  A point is sampled uniformly from the hypercube $[-r, r]^d$.\n2.  The point is accepted if its Euclidean norm is less than or equal to $r$.\n3.  The acceptance probability, $P_d$, is the ratio of the volume of the $d$-dimensional hypersphere of radius $r$ to the volume of the $d$-dimensional hypercube with side length $2r$.\n4.  The radius $r > 0$ is a fixed positive constant.\n5.  The objective is to determine the limit of $P_d$ as the dimension $d \\to \\infty$.\n\nThe problem is scientifically grounded, as it deals with standard concepts in probability theory, high-dimensional geometry, and computational statistics (Monte Carlo methods). The phenomenon it describes is a well-known example of the \"curse of dimensionality\". The problem is well-posed, providing all necessary information to derive a unique solution. The language is objective and precise. Therefore, the problem is valid and we may proceed with the solution.\n\nLet $V_{sphere}(d, r)$ denote the volume of a $d$-dimensional hypersphere of radius $r$. Let $V_{cube}(d, r)$ denote the volume of a $d$-dimensional hypercube with vertices at $(\\pm r, \\pm r, \\dots, \\pm r)$.\n\nThe side length of the hypercube $[-r, r]^d$ is $r - (-r) = 2r$. The volume of this hypercube is the product of its side lengths:\n$$ V_{cube}(d, r) = (2r)^d = 2^d r^d $$\n\nThe volume of a $d$-dimensional hypersphere (or $d$-ball) of radius $r$ is given by the formula:\n$$ V_{sphere}(d, r) = \\frac{\\pi^{d/2}}{\\Gamma\\left(\\frac{d}{2} + 1\\right)} r^d $$\nwhere $\\Gamma(z)$ is the Gamma function, a generalization of the factorial function.\n\nThe acceptance probability $P_d$ is the ratio of these two volumes:\n$$ P_d = \\frac{V_{sphere}(d, r)}{V_{cube}(d, r)} = \\frac{\\frac{\\pi^{d/2}}{\\Gamma\\left(\\frac{d}{2} + 1\\right)} r^d}{2^d r^d} $$\nThe term $r^d$ cancels, which is expected as the ratio of volumes of similarly scaled objects in the same dimension is independent of the scaling factor $r$.\n$$ P_d = \\frac{\\pi^{d/2}}{2^d \\Gamma\\left(\\frac{d}{2} + 1\\right)} $$\nTo analyze the limit as $d \\to \\infty$, we can rewrite the expression as:\n$$ P_d = \\frac{(\\sqrt{\\pi})^d}{2^d \\Gamma\\left(\\frac{d}{2} + 1\\right)} = \\frac{\\left(\\frac{\\sqrt{\\pi}}{2}\\right)^d}{\\Gamma\\left(\\frac{d}{2} + 1\\right)} $$\nWe must now evaluate the limit of this expression as $d \\to \\infty$. Let us examine the numerator and the denominator separately.\n\nFor the numerator, we have the term $\\left(\\frac{\\sqrt{\\pi}}{2}\\right)^d$. Since the value of $\\pi$ is approximately $3.14159$, $\\sqrt{\\pi}$ is approximately $1.77245$. The base of the exponent is $\\frac{\\sqrt{\\pi}}{2} \\approx 0.88622$. Since this base is a positive constant strictly less than $1$, its limit as the exponent goes to infinity is $0$:\n$$ \\lim_{d \\to \\infty} \\left(\\frac{\\sqrt{\\pi}}{2}\\right)^d = 0 $$\n\nFor the denominator, we have the term $\\Gamma\\left(\\frac{d}{2} + 1\\right)$. As $d \\to \\infty$, the argument of the Gamma function, $\\frac{d}{2} + 1$, also approaches infinity. The Gamma function $\\Gamma(z)$ has the property that $\\lim_{z \\to \\infty} \\Gamma(z) = \\infty$. Therefore:\n$$ \\lim_{d \\to \\infty} \\Gamma\\left(\\frac{d}{2} + 1\\right) = \\infty $$\n\nWe are now evaluating the limit of a fraction where the numerator approaches $0$ and the denominator approaches $\\infty$. Such a limit is unequivocally $0$.\n$$ \\lim_{d \\to \\infty} P_d = \\lim_{d \\to \\infty} \\frac{\\left(\\frac{\\sqrt{\\pi}}{2}\\right)^d}{\\Gamma\\left(\\frac{d}{2} + 1\\right)} = 0 $$\nThis result demonstrates that the acceptance probability for this rejection sampling scheme converges to zero as the dimensionality of the space increases. This means that for high dimensions, an exponentially large number of samples from the hypercube must be drawn to find one that lies within the inscribed hypersphere, rendering the method computationally infeasible. This is a classic manifestation of the curse of dimensionality. The limiting acceptance probability is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Theoretical concepts become concrete when we can see their effects through simulation. This final practice  is a hands-on coding exercise to empirically measure the impact of dimensionality on the convergence rate of a Kernel Density Estimator (KDE). By implementing the simulation and analyzing the results, you will quantify how an estimator's performance degrades as the dimension increases, translating abstract theory into observable outcomes.",
            "id": "2439662",
            "problem": "You are asked to quantify how the convergence of a multivariate Kernel Density Estimator (KDE) slows as the dimension increases, illustrating the curse of dimensionality in computational economics and finance. Consider independent and identically distributed samples $X_1,\\dots,X_n \\in \\mathbb{R}^d$ drawn from the $d$-variate standard normal distribution with density\n$$\nf_d(x) = (2\\pi)^{-d/2}\\exp\\!\\left(-\\tfrac{1}{2}\\lVert x\\rVert_2^2\\right),\n$$\nwhere $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. Define the Gaussian product kernel\n$$\nK_d(u) = (2\\pi)^{-d/2}\\exp\\!\\left(-\\tfrac{1}{2}\\lVert u\\rVert_2^2\\right),\n$$\nand the Kernel Density Estimator (KDE) with bandwidth $h>0$ as\n$$\n\\widehat{f}_{n,d,h}(x) = \\frac{1}{n h^d}\\sum_{i=1}^n K_d\\!\\left(\\frac{x - X_i}{h}\\right).\n$$\nFor each combination of sample size $n$ and dimension $d$, set the bandwidth to\n$$\nh(n,d) = n^{-1/(d+4)}.\n$$\nDefine the Monte Carlo proxy for the mean integrated squared error with respect to the true distribution (that is, the mean squared error under $X\\sim f_d$) as\n$$\n\\operatorname{MSE}_{\\text{MC}}(n,d) = \\frac{1}{Q}\\sum_{j=1}^{Q}\\left(\\widehat{f}_{n,d,h(n,d)}(Z_j) - f_d(Z_j)\\right)^2,\n$$\nwhere $Z_1,\\dots,Z_Q$ are independent draws from the $d$-variate standard normal distribution.\n\nImplement a complete, runnable program that, for the following test suite, computes $\\operatorname{MSE}_{\\text{MC}}(n,d)$ and the empirical convergence slope in log-log scale:\n\n- Test suite parameters:\n  - Dimensions $d \\in \\{\\,1,\\,3,\\,6\\,\\}$.\n  - Sample sizes $n \\in \\{\\,200,\\,800,\\,3200\\,\\}$.\n  - Number of Monte Carlo evaluation points $Q = 1024$.\n- Randomness and reproducibility:\n  - For the sample $X_1,\\dots,X_n$ in a given $(n,d)$ case, use a pseudorandom number generator seeded with the integer\n    $$\n    s_{\\text{data}}(n,d) = 10^6 + 10^4 d + n.\n    $$\n  - For the evaluation points $Z_1,\\dots,Z_Q$ in a given $d$, use a pseudorandom number generator seeded with the integer\n    $$\n    s_{\\text{eval}}(d) = 2\\cdot 10^6 + 10^4 d.\n    $$\n  - All normal random variables must be standard normal with mean $0$ and variance $1$ in each coordinate, mutually independent.\n- For each fixed $d$, compute the least-squares slope $b_d$ of the regression of $\\log \\operatorname{MSE}_{\\text{MC}}(n,d)$ on $\\log n$ over the three $n$ values in the test suite. That is, for $n\\in\\{200,800,3200\\}$, fit\n  $$\n  \\log \\operatorname{MSE}_{\\text{MC}}(n,d) \\approx a_d + b_d \\log n\n  $$\n  in the ordinary least squares sense and return the estimated slope $b_d$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in this order:\n- The nine values $\\operatorname{MSE}_{\\text{MC}}(n,d)$ for $d=1,3,6$ (in ascending order) and, within each $d$, $n=200,800,3200$ (in ascending order).\n- Followed by the three slopes $b_d$ for $d=1,3,6$ (in ascending order).\n\nThus the output must have a total of twelve floating-point numbers in the order\n$$\n\\bigl[\\operatorname{MSE}_{\\text{MC}}(200,1),\\,\\operatorname{MSE}_{\\text{MC}}(800,1),\\,\\operatorname{MSE}_{\\text{MC}}(3200,1),\\,\\operatorname{MSE}_{\\text{MC}}(200,3),\\,\\operatorname{MSE}_{\\text{MC}}(800,3),\\,\\operatorname{MSE}_{\\text{MC}}(3200,3),\\,\\operatorname{MSE}_{\\text{MC}}(200,6),\\,\\operatorname{MSE}_{\\text{MC}}(800,6),\\,\\operatorname{MSE}_{\\text{MC}}(3200,6),\\,b_1,\\,b_3,\\,b_6\\bigr].\n$$\nNo other text should be printed. Angles and physical units are not involved; all outputs must be real numbers.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **True Distribution:** $d$-variate standard normal, $f_d(x) = (2\\pi)^{-d/2}\\exp(-\\frac{1}{2}\\lVert x\\rVert_2^2)$.\n- **Data Samples:** $X_1,\\dots,X_n \\in \\mathbb{R}^d$ are i.i.d. draws from $f_d$.\n- **Kernel Function:** Gaussian product kernel, $K_d(u) = (2\\pi)^{-d/2}\\exp(-\\frac{1}{2}\\lVert u\\rVert_2^2)$.\n- **Kernel Density Estimator (KDE):** $\\widehat{f}_{n,d,h}(x) = \\frac{1}{n h^d}\\sum_{i=1}^n K_d(\\frac{x - X_i}{h})$.\n- **Bandwidth Rule:** $h(n,d) = n^{-1/(d+4)}$.\n- **Error Metric:** Monte Carlo Mean Squared Error, $\\operatorname{MSE}_{\\text{MC}}(n,d) = \\frac{1}{Q}\\sum_{j=1}^{Q}(\\widehat{f}_{n,d,h(n,d)}(Z_j) - f_d(Z_j))^2$.\n- **Evaluation Samples:** $Z_1,\\dots,Z_Q$ are i.i.d. draws from $f_d$.\n- **Parameters:**\n    - Dimensions: $d \\in \\{1, 3, 6\\}$.\n    - Sample sizes: $n \\in \\{200, 800, 3200\\}$.\n    - Evaluation size: $Q = 1024$.\n- **Random Seeds:**\n    - Data generation seed: $s_{\\text{data}}(n,d) = 10^6 + 10^4 d + n$.\n    - Evaluation data seed: $s_{\\text{eval}}(d) = 2 \\cdot 10^6 + 10^4 d$.\n- **Analysis Task:** For each $d$, compute the slope $b_d$ of the ordinary least-squares regression of $\\log \\operatorname{MSE}_{\\text{MC}}(n,d)$ on $\\log n$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed based on the established criteria.\n- **Scientifically Grounded:** The problem is a standard exercise in non-parametric statistics, specifically concerning the convergence properties of Kernel Density Estimators. The concept of the \"curse of dimensionality\" and its effect on convergence rates is a cornerstone of high-dimensional statistical theory. All definitions are standard and the setup is a classic textbook case.\n- **Well-Posed:** All necessary components are specified: distributions, estimator form, parameters ($n, d, Q$), a deterministic bandwidth selection rule, a precise error metric, and a reproducible random number generation scheme. This ensures a unique numerical solution can be obtained.\n- **Objective:** The problem is stated using precise, unambiguous mathematical language.\n\nThe problem is free of the specified flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, or unverifiable.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be provided.\n\n**Methodology**\n\nThe task is to compute the Monte Carlo Mean Squared Error, $\\operatorname{MSE}_{\\text{MC}}(n,d)$, for several combinations of sample size $n$ and dimension $d$, and then to determine the empirical convergence rate. The procedure is as follows.\n\nFor each dimension $d \\in \\{1, 3, 6\\}$:\n1.  **Generate Evaluation Points:** We first generate $Q=1024$ evaluation points $Z_1, \\dots, Z_Q$ from the $d$-variate standard normal distribution, $f_d$. The pseudorandom number generator is seeded with $s_{\\text{eval}}(d) = 2 \\cdot 10^6 + 10^4 d$ to ensure reproducibility. These points are stored in a $Q \\times d$ matrix $Z$.\n\n2.  **Compute True Density:** The true density values $f_d(Z_j)$ for $j=1, \\dots, Q$ are computed using the formula $f_d(x) = (2\\pi)^{-d/2}\\exp(-\\frac{1}{2}\\lVert x\\rVert_2^2)$. This involves calculating the squared Euclidean norm $\\lVert Z_j\\rVert_2^2$ for each point.\n\n3.  **Iterate over Sample Sizes:** For each sample size $n \\in \\{200, 800, 3200\\}$:\n    a.  **Generate Data Samples:** $n$ data points $X_1, \\dots, X_n$ are drawn from $f_d$. The generator is seeded with $s_{\\text{data}}(n,d) = 10^6 + 10^4 d + n$. These points form an $n \\times d$ matrix $X$.\n    b.  **Determine Bandwidth:** The bandwidth $h$ is calculated according to the rule $h(n,d) = n^{-1/(d+4)}$.\n    c.  **Compute KDE:** The KDE, $\\widehat{f}_{n,d,h}(x)$, must be evaluated at each point $Z_j$. The definition is:\n    $$\n    \\widehat{f}_{n,d,h}(Z_j) = \\frac{1}{n h^d}\\sum_{i=1}^n K_d\\left(\\frac{Z_j - X_i}{h}\\right)\n    $$\n    Substituting the Gaussian kernel $K_d(u) = (2\\pi)^{-d/2}\\exp(-\\frac{1}{2}\\lVert u\\rVert_2^2)$ yields:\n    $$\n    \\widehat{f}_{n,d,h}(Z_j) = \\frac{(2\\pi)^{-d/2}}{n h^d}\\sum_{i=1}^n \\exp\\left(-\\frac{1}{2h^2}\\lVert Z_j - X_i\\rVert_2^2\\right)\n    $$\n    To compute this efficiently, we first form a $Q \\times n$ matrix of squared Euclidean distances, where the entry $(j,i)$ is $\\lVert Z_j - X_i\\rVert_2^2$. This is done using the `scipy.spatial.distance.cdist` function. The exponential term is then applied element-wise, the results are summed over the index $i$ for each $j$, and finally multiplied by the constant pre-factor $\\frac{(2\\pi)^{-d/2}}{n h^d}$.\n    d.  **Compute MSE:** The $\\operatorname{MSE}_{\\text{MC}}(n,d)$ is calculated by taking the mean of the squared differences between the estimated and true densities at the evaluation points:\n    $$\n    \\operatorname{MSE}_{\\text{MC}}(n,d) = \\frac{1}{Q}\\sum_{j=1}^{Q}\\left(\\widehat{f}_{n,d,h}(Z_j) - f_d(Z_j)\\right)^2\n    $$\n4.  **Estimate Convergence Slope:** After computing the three $\\operatorname{MSE}_{\\text{MC}}$ values for a fixed $d$, we estimate the slope $b_d$ of the relationship $\\log \\operatorname{MSE}_{\\text{MC}}(n,d) \\approx a_d + b_d \\log n$. This is a standard simple linear regression problem. We define the dependent variable as $y_k = \\log \\operatorname{MSE}_{\\text{MC}}(n_k, d)$ and the independent variable as $x_k = \\log n_k$ for $n_k \\in \\{200, 800, 3200\\}$. The slope $b_d$ is found by solving the least-squares problem, for which we use the `numpy.linalg.lstsq` function.\n\nThe theoretical asymptotic mean integrated squared error (MISE) for this setup converges at a rate of $\\mathcal{O}(n^{-4/(d+4)})$. Therefore, the logarithm of the MISE is a linear function of $\\log n$ with a slope of $-\\frac{4}{d+4}$. The computed empirical slope $b_d$ is expected to approximate this theoretical value. For $d=1, 3, 6$, the theoretical slopes are $-0.8$, $-4/7 \\approx -0.571$, and $-0.4$, respectively. The decrease in the magnitude of the slope as $d$ increases is a quantitative manifestation of the curse of dimensionality: the estimator's convergence rate slows down in higher dimensions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Computes the Mean Squared Error of a Kernel Density Estimator for various\n    dimensions and sample sizes, and estimates the convergence slope to illustrate\n    the curse of dimensionality.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    dimensions = [1, 3, 6]\n    sample_sizes = [200, 800, 3200]\n    Q = 1024\n\n    all_mse_values = []\n    all_slopes = []\n\n    # Helper function for multivariate normal PDF\n    def true_density_f_d(x, d):\n        if x.ndim == 1:\n            x = x.reshape(1, -1)\n        norm_sq = np.sum(x**2, axis=1)\n        return (2 * np.pi)**(-d / 2) * np.exp(-0.5 * norm_sq)\n\n    for d in dimensions:\n        # Generate evaluation points Z for the current dimension d.\n        # This is done once per dimension.\n        s_eval = 2 * 10**6 + 10**4 * d\n        rng_eval = np.random.default_rng(s_eval)\n        Z = rng_eval.normal(loc=0, scale=1, size=(Q, d))\n\n        # Compute the true density values f_d(Z_j) at the evaluation points.\n        f_true_vals = true_density_f_d(Z, d)\n\n        mse_for_current_d = []\n        log_n_values = np.log(sample_sizes)\n\n        for n in sample_sizes:\n            # Main logic to calculate the result for one case (n, d)\n            \n            # 1. Generate data samples X\n            s_data = 10**6 + 10**4 * d + n\n            rng_data = np.random.default_rng(s_data)\n            X = rng_data.normal(loc=0, scale=1, size=(n, d))\n            \n            # 2. Calculate bandwidth h\n            h = n**(-1 / (d + 4))\n\n            # 3. Calculate KDE estimates f_hat(Z_j)\n            # Use scipy.spatial.distance.cdist for efficient computation of squared\n            # Euclidean distances between each Z_j and X_i.\n            sq_dists = cdist(Z, X, 'sqeuclidean')  # Shape (Q, n)\n            \n            # The argument to the exponential function in the kernel sum\n            kernel_exp_arg = -0.5 / (h**2) * sq_dists\n            \n            # Sum of kernel values over index i\n            sum_of_exponentials = np.sum(np.exp(kernel_exp_arg), axis=1)\n            \n            # Prefactor for the KDE formula\n            prefactor = (2 * np.pi)**(-d / 2) / (n * h**d)\n            \n            # KDE estimates at points Z_j\n            f_hat_vals = prefactor * sum_of_exponentials\n            \n            # 4. Compute the Monte Carlo proxy for MSE\n            mse = np.mean((f_hat_vals - f_true_vals)**2)\n            mse_for_current_d.append(mse)\n\n        # Append the 3 MSE values for the current dimension to the main list\n        all_mse_values.extend(mse_for_current_d)\n        \n        # 5. Compute the least-squares slope for the current dimension d\n        log_mse_values = np.log(mse_for_current_d)\n        \n        # Set up the linear system A*beta = y for regression\n        # y = log_mse_values\n        # beta = [a_d, b_d] (intercept, slope)\n        # A = [[1, log_n_1], [1, log_n_2], [1, log_n_3]]\n        A = np.vstack([np.ones_like(log_n_values), log_n_values]).T\n        \n        # Solve for the coefficients using least squares\n        coeffs = np.linalg.lstsq(A, log_mse_values, rcond=None)[0]\n        slope_b_d = coeffs[1]\n        all_slopes.append(slope_b_d)\n\n    # Combine all results into a single list for printing\n    results = all_mse_values + all_slopes\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}