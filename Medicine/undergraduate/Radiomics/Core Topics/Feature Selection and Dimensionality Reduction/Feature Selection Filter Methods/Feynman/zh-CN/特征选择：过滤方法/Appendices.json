{
    "hands_on_practices": [
        {
            "introduction": "皮尔逊相关性是评估连续特征和连续结果之间线性关系的基本过滤方法。通过从头开始实现其计算，本次实践旨在巩固您对该方法核心原理的理解，并揭示其与简单线性回归的内在联系，为构建更复杂的特征选择策略奠定基础。这项练习  将帮助您掌握如何量化和排序特征的线性关联强度。",
            "id": "5194608",
            "problem": "给定一个数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$，它表示对 $n$ 名患者测量的 $p$ 个候选特征，以及一个连续的结果向量 $y \\in \\mathbb{R}^{n}$。您的目标是设计并实现一种基于皮尔逊相关系数绝对值的单变量滤波方法用于特征选择。您必须从基本定义出发，推导出一个可实现的程序，该程序通过 $|r_j|$ 对特征进行排序，其中 $r_j$ 是特征 $j$ 与结果之间的样本皮尔逊相关性。然后，将您的程序应用于下面的测试套件，并将结果汇总到指定的单行输出中。\n\n基本定义与推导约束：\n- 从样本均值、未归一化的样本协方差和样本尺度（未归一化平方和的平方根）的定义开始。对于任何具有条目 $v_i$ 的向量 $v \\in \\mathbb{R}^{n}$，将其样本均值定义为 $\\bar{v} = \\frac{1}{n} \\sum_{i=1}^{n} v_i$。对于一对向量 $(u, v)$，将未归一化的样本协方差定义为 $\\sum_{i=1}^{n} (u_i - \\bar{u})(v_i - \\bar{v})$，将样本尺度定义为 $s_v = \\sqrt{\\sum_{i=1}^{n} (v_i - \\bar{v})^2}$。\n- 基于这些定义，推导出 $X$ 的第 $j$ 列与 $y$ 之间的样本皮尔逊相关系数 $r_j$ 的标准表达式，除了上述定义外，不引入任何额外假设。\n- 指定单变量滤波方法的完整排序程序：为每个特征 $j \\in \\{0, 1, \\dots, p-1\\}$ 计算 $r_j$，按 $|r_j|$ 的降序排序，通过较小的特征索引来确定性地打破平局，并通过定义 $r_j = 0$ 来处理 $s_{x_j} = 0$ 或 $s_y = 0$ 的边界情况。\n- 从基本原理出发，简要论证为什么这构成一种滤波方法（相对于包装方法或嵌入式方法），并解释一个将 $r_j$ 与简单线性模型拟合联系起来的数学性质，该性质支持当 $p$ 很大时此排序的可解释性。\n\n您的实现要求：\n- 实现一个函数，直接根据上述定义计算所有 $j \\in \\{0, 1, \\dots, p-1\\}$ 的 $r_j$，不使用任何用于计算相关的库程序。\n- 实现按 $|r_j|$ 排序的程序，并采用上述的平局打破规则。\n- 当 $x$ 和 $y$ 都使用样本尺度 $s_v = \\sqrt{\\sum_{i=1}^{n} (v_i - \\bar{v})^2}$ 进行标准化时，实现对皮尔逊相关性与普通最小二乘法（OLS）简单线性回归斜率之间等价性的检查。具体来说，对于具有向量 $x \\in \\mathbb{R}^{n}$ 和 $y \\in \\mathbb{R}^{n}$ 的单特征情况，构建标准化变量 $z_x = \\frac{x - \\bar{x}}{s_x}$ 和 $z_y = \\frac{y - \\bar{y}}{s_y}$，对 $z_y$ 关于 $z_x$ 拟合一个包含截距的 OLS 模型，并返回拟合斜率与使用相同定义从未标准化的 $(x, y)$ 计算出的皮尔逊相关性之间的绝对差值。报告此绝对差值。\n\n测试套件：\n- 测试用例 $1$（具有不同相关性量级的一般情况）：\n  - $X^{(1)} \\in \\mathbb{R}^{4 \\times 3}$ 和 $y^{(1)} \\in \\mathbb{R}^{4}$ 分别为\n  $$\n  X^{(1)} = \\begin{bmatrix}\n  1  2  0 \\\\\n  3  1  1 \\\\\n  2  3  2 \\\\\n  0  1  3\n  \\end{bmatrix}, \\quad\n  y^{(1)} = \\begin{bmatrix}\n  1 \\\\ 0 \\\\ 1 \\\\ 2\n  \\end{bmatrix}.\n  $$\n  将 $X^{(1)}$ 的列解释为具有从零开始的索引 $0$、$1$ 和 $2$ 的特征。\n- 测试用例 $2$（具有零方差特征和 $|r_j|$ 值出现平局的边界情况）：\n  - $X^{(2)} \\in \\mathbb{R}^{4 \\times 3}$ 和 $y^{(2)} \\in \\mathbb{R}^{4}$ 分别为\n  $$\n  X^{(2)} = \\begin{bmatrix}\n  0  0  5 \\\\\n  1  -1  5 \\\\\n  2  -2  5 \\\\\n  3  -3  5\n  \\end{bmatrix}, \\quad\n  y^{(2)} = \\begin{bmatrix}\n  0 \\\\ 1 \\\\ 2 \\\\ 3\n  \\end{bmatrix}.\n  $$\n  注意，第三个特征（索引为 $2$）具有零方差，应被赋予 $r_2 = 0$。前两个特征应在 $|r_j|$ 上产生平局，必须通过偏好较小的索引来打破。\n- 测试用例 $3$（相关性与标准化 OLS 斜率之间的等价性检查）：\n  - 设 $x^{(3)} \\in \\mathbb{R}^{5}$ 和 $y^{(3)} \\in \\mathbb{R}^{5}$ 分别为\n  $$\n  x^{(3)} = \\begin{bmatrix}\n  0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4\n  \\end{bmatrix}, \\quad\n  y^{(3)} = \\begin{bmatrix}\n  7 \\\\ 5 \\\\ 3 \\\\ 1 \\\\ -1\n  \\end{bmatrix}.\n  $$\n  计算 (i) 根据上述定义从 $(x^{(3)}, y^{(3)})$ 计算出的皮尔逊相关性与 (ii) 从 $z_y$ 对 $z_x$ 的 OLS 回归中得到的斜率之间的绝对差值，其中 $z_x = \\frac{x^{(3)} - \\overline{x^{(3)}}}{s_{x^{(3)}}}$ 和 $z_y = \\frac{y^{(3)} - \\overline{y^{(3)}}}{s_{y^{(3)}}}$，且 $s_v = \\sqrt{\\sum_{i=1}^{n} (v_i - \\bar{v})^2}$。\n\n最终输出格式：\n- 您的程序必须生成一个包含三个条目的列表的单行输出：\n  - 第一个条目是测试用例 $1$ 的排序，为一个按 $|r_j|$ 降序排列的特征索引列表（使用从零开始的索引和指定的平局打破规则）。\n  - 第二个条目是测试用例 $2$ 的排序，格式相同。\n  - 第三个条目是测试用例 $3$ 的绝对差值（一个非负实数），如上定义。\n- 具体而言，输出必须具有 $[R^{(1)}, R^{(2)}, d^{(3)}]$ 的形式，其中 $R^{(1)}$ 和 $R^{(2)}$ 是整数列表，$d^{(3)}$ 是一个浮点数。例如，一个可接受的格式是 $[[0,2,1],[0,1,2],0.0]$。\n\n关于方法论背景的说明：\n- 滤波方法使用不涉及拟合多特征预测模型的统计标准，独立于 $y$ 评估每个特征。仅为对比，包装方法通过训练和验证一个预测模型来评估特征子集以对这些子集进行评分，而嵌入式方法则在模型训练本身中包含了特征选择（例如，线性模型中的最小绝对收缩和选择算子（LASSO）正则化）。在此任务中，您无需实现包装或嵌入式方法；您的代码必须只实现按 $|r_j|$ 进行的滤波排序和测试用例 $3$ 中的标准化 OLS 斜率检查。\n\n您的程序不得从用户读取输入，并且必须按上述格式精确打印一行。",
            "solution": "该问题要求设计、推导和实现一种基于绝对皮尔逊相关系数的单变量滤波方法用于特征选择。解决方案必须根据问题陈述中给出的基本原理进行开发。\n\n首先，我们推导特征 $j$ 的样本皮尔逊相关系数 $r_j$ 的表达式。问题为任何向量 $u, v \\in \\mathbb{R}^{n}$ 提供了以下基本定义：\n- 样本均值：$\\bar{v} = \\frac{1}{n} \\sum_{i=1}^{n} v_i$。\n- 未归一化的样本协方差：$C_{uv} = \\sum_{i=1}^{n} (u_i - \\bar{u})(v_i - \\bar{v})$。\n- 样本尺度：$s_v = \\sqrt{\\sum_{i=1}^{n} (v_i - \\bar{v})^2}$。\n\n皮尔逊相关系数通常定义为两个变量的协方差除以它们标准差的乘积。使用给定的未归一化定义，两个向量 $u$ 和 $v$ 之间的样本皮尔逊相关系数 $r$ 是它们的未归一化样本协方差与它们样本尺度乘积的比率。设 $x_j$ 表示第 $j$ 个特征向量（矩阵 $X$ 的第 $j$ 列），$y$ 为结果向量。因此，$x_j$ 和 $y$ 之间的相关系数 $r_j$ 为：\n$$\nr_j = \\frac{\\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}} = \\frac{C_{x_j y}}{s_{x_j} s_y}\n$$\n此公式是通过将所提供的定义代入皮尔逊相关的标准结构直接推导出来的。问题指定了一个边界条件：如果分母为零（即 $s_{x_j} = 0$ 或 $s_y = 0$），则相关系数 $r_j$ 定义为 $0$。样本尺度为零意味着向量的方差为零（其所有元素都相同）。\n\n单变量滤波方法的完整程序如下：\n1. 对于每个特征 $j \\in \\{0, 1, \\dots, p-1\\}$，使用推导出的公式计算特征向量 $x_j$ 和结果向量 $y$ 之间的样本皮尔逊相关系数 $r_j$。\n2. 计算每个相关系数的绝对值 $|r_j|$。\n3. 按照其对应的 $|r_j|$ 值的降序对特征进行排序。\n4. 在 $|r_j|$ 值出现平局的情况下，索引 $j$ 较小的特征排名更高。这确保了排序的确定性。\n\n此程序构成了特征选择的一种滤波方法。滤波方法根据特征相对于目标变量的内在统计特性来评估和排序特征，而与任何选定的预测建模算法无关。我们的方法为每个特征单独计算一个分数（$|r_j|$），而无需训练多特征模型。这与包装方法形成对比，后者使用特定的预测模型对整个特征子集进行评分；也与嵌入式方法不同，后者将特征选择作为模型训练过程的一个组成部分（例如，LASSO 正则化）。\n\n皮尔逊相关性 $r_j$ 的一个关键数学性质支持其在特征排序中的应用，尤其是在高维（大 $p$）设置中。这个性质是它与简单普通最小二乘法（OLS）线性回归模型斜率的直接关系。对于一个关于单个特征 $x_j$ 的模型 $y = \\beta_0 + \\beta_1 x_j + \\epsilon$，斜率的 OLS 估计值为 $\\hat{\\beta}_1 = \\frac{C_{x_j y}}{s_{x_j}^2}$。这可以改写为以相关性 $r_j$ 表示的形式：\n$$\n\\hat{\\beta}_1 = \\frac{r_j s_{x_j} s_y}{s_{x_j}^2} = r_j \\frac{s_y}{s_{x_j}}\n$$\n这表明斜率与相关性成正比，但按样本尺度的比率进行了缩放。为了提供一个标准化的关联度量，我们可以考虑标准化变量之间的回归。我们将标准化变量 $z_{x_j}$ 和 $z_y$ 定义为：\n$$\nz_{x_j} = \\frac{x_j - \\bar{x}_j}{s_{x_j}}, \\quad z_y = \\frac{y - \\bar{y}}{s_y}\n$$\n根据构造，这些标准化变量的均值为 $0$，样本尺度为 $1$。我们来证明 $z_{x_j}$ 的这一点：\n- 均值：$\\bar{z}_{x_j} = \\frac{1}{n} \\sum_{i=1}^n \\frac{x_{ij} - \\bar{x}_j}{s_{x_j}} = \\frac{1}{n s_{x_j}} \\left( \\sum_{i=1}^n x_{ij} - n \\bar{x}_j \\right) = \\frac{1}{n s_{x_j}} (n \\bar{x}_j - n \\bar{x}_j) = 0$。\n- 尺度的平方：$s_{z_{x_j}}^2 = \\sum_{i=1}^n (z_{x_{ij}} - \\bar{z}_{x_j})^2 = \\sum_{i=1}^n z_{x_{ij}}^2 = \\sum_{i=1}^n \\left( \\frac{x_{ij} - \\bar{x}_j}{s_{x_j}} \\right)^2 = \\frac{1}{s_{x_j}^2} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2 = \\frac{s_{x_j}^2}{s_{x_j}^2} = 1$。\n现在，考虑一个 $z_y$ 对 $z_{x_j}$ 的包含截距的 OLS 模型：$z_y = \\beta'_0 + \\beta'_1 z_{x_j} + \\epsilon'$。斜率 $\\hat{\\beta}'_1$ 的 OLS 估计值为：\n$$\n\\hat{\\beta}'_1 = \\frac{\\sum_{i=1}^n (z_{x_{ij}} - \\bar{z}_{x_j})(z_{y_i} - \\bar{z}_y)}{\\sum_{i=1}^n (z_{x_{ij}} - \\bar{z}_{x_j})^2}\n$$\n由于 $\\bar{z}_{x_j} = 0$，$\\bar{z}_y = 0$，并且 $\\sum (z_{x_{ij}})^2 = s_{z_{x_j}}^2 = 1$，上式简化为：\n$$\n\\hat{\\beta}'_1 = \\frac{\\sum_{i=1}^n z_{x_{ij}} z_{y_i}}{1} = \\sum_{i=1}^n \\left( \\frac{x_{ij} - \\bar{x}_j}{s_{x_j}} \\right) \\left( \\frac{y_i - \\bar{y}}{s_y} \\right) = \\frac{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)(y_i - \\bar{y})}{s_{x_j} s_y} = r_j\n$$\n因此，皮尔逊相关系数 $r_j$ 完全等于标准化结果对标准化特征的简单线性回归的斜率。这提供了一个清晰的解释：$|r_j|$ 在一个通用尺度上衡量了线性关系的强度，其中特征的标准化值每变化一个单位，结果的标准化值就相应变化 $r_j$ 个单位。因此，按 $|r_j|$ 排序等同于在单变量线性情境下按其标准化效应大小的量级对特征进行排序。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the feature selection problem according to the user's specification.\n    It processes three test cases: two feature ranking tasks and one equivalence check.\n    \"\"\"\n\n    def compute_pearson_correlations(X, y):\n        \"\"\"\n        Computes Pearson correlation for each feature in X against y from first principles.\n\n        Args:\n            X (np.ndarray): Data matrix of shape (n, p).\n            y (np.ndarray): Outcome vector of shape (n,).\n\n        Returns:\n            list: A list of Pearson correlation coefficients [r_0, r_1, ..., r_{p-1}].\n        \"\"\"\n        n, p = X.shape\n        y_mean = np.mean(y)\n        y_centered = y - y_mean\n        s_y_sq = np.sum(y_centered**2)\n\n        correlations = []\n        if s_y_sq == 0.0:\n            return [0.0] * p\n\n        s_y = np.sqrt(s_y_sq)\n\n        for j in range(p):\n            x_j = X[:, j]\n            x_j_mean = np.mean(x_j)\n            x_j_centered = x_j - x_j_mean\n            s_x_j_sq = np.sum(x_j_centered**2)\n\n            if s_x_j_sq == 0.0:\n                correlations.append(0.0)\n                continue\n\n            s_x_j = np.sqrt(s_x_j_sq)\n            cov_numerator = np.sum(x_j_centered * y_centered)\n            r_j = cov_numerator / (s_x_j * s_y)\n            correlations.append(r_j)\n\n        return correlations\n\n    def rank_features(correlations):\n        \"\"\"\n        Ranks features based on the absolute value of their correlation, with tie-breaking.\n\n        Args:\n            correlations (list): List of correlation coefficients.\n\n        Returns:\n            list: A list of feature indices sorted by a descending absolute correlation.\n        \"\"\"\n        indexed_corrs = list(enumerate(correlations))\n        # Sort key: primary is descending absolute correlation, secondary is ascending index.\n        sorted_indices = sorted(indexed_corrs, key=lambda item: (-abs(item[1]), item[0]))\n        return [index for index, corr in sorted_indices]\n\n    def compute_ols_equivalence_diff(x, y):\n        \"\"\"\n        Computes the absolute difference between Pearson correlation and the slope of\n        OLS regression on standardized variables.\n\n        Args:\n            x (np.ndarray): Feature vector of shape (n,).\n            y (np.ndarray): Outcome vector of shape (n,).\n\n        Returns:\n            float: The absolute difference.\n        \"\"\"\n        n = len(x)\n\n        # (i) Compute Pearson correlation for (x, y)\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        x_c = x - x_mean\n        y_c = y - y_mean\n        s_x_sq = np.sum(x_c**2)\n        s_y_sq = np.sum(y_c**2)\n\n        if s_x_sq == 0.0 or s_y_sq == 0.0:\n            r_xy = 0.0\n        else:\n            s_x = np.sqrt(s_x_sq)\n            s_y = np.sqrt(s_y_sq)\n            cov_num = np.sum(x_c * y_c)\n            r_xy = cov_num / (s_x * s_y)\n        \n        # (ii) Compute OLS slope for standardized z_y on z_x\n        if s_x_sq == 0.0 or s_y_sq == 0.0:\n            # If variance is zero, standardization is not well-defined.\n            # In this context, implies slope is ill-defined or zero.\n            # We assume for this test case that variances are non-zero.\n            ols_slope = 0.0\n        else:\n            s_x = np.sqrt(s_x_sq)\n            s_y = np.sqrt(s_y_sq)\n            z_x = x_c / s_x\n            z_y = y_c / s_y\n            \n            # OLS slope for z_y = beta_0 + beta_1 * z_x\n            # The formulas are more stable with re-centering, though means should be zero.\n            z_x_mean = np.mean(z_x)\n            z_y_mean = np.mean(z_y)\n            z_x_c = z_x - z_x_mean\n            z_y_c = z_y - z_y_mean\n            \n            ols_slope_num = np.sum(z_x_c * z_y_c)\n            ols_slope_den = np.sum(z_x_c**2)\n            \n            if ols_slope_den == 0.0:\n                 ols_slope = 0.0\n            else:\n                 ols_slope = ols_slope_num / ols_slope_den\n\n        return abs(r_xy - ols_slope)\n\n    # Test Case 1\n    X1 = np.array([\n        [1, 2, 0],\n        [3, 1, 1],\n        [2, 3, 2],\n        [0, 1, 3]\n    ], dtype=float)\n    y1 = np.array([1, 0, 1, 2], dtype=float)\n    correlations1 = compute_pearson_correlations(X1, y1)\n    ranking1 = rank_features(correlations1)\n\n    # Test Case 2\n    X2 = np.array([\n        [0, 0, 5],\n        [1, -1, 5],\n        [2, -2, 5],\n        [3, -3, 5]\n    ], dtype=float)\n    y2 = np.array([0, 1, 2, 3], dtype=float)\n    correlations2 = compute_pearson_correlations(X2, y2)\n    ranking2 = rank_features(correlations2)\n\n    # Test Case 3\n    x3 = np.array([0, 1, 2, 3, 4], dtype=float)\n    y3 = np.array([7, 5, 3, 1, -1], dtype=float)\n    diff3 = compute_ols_equivalence_diff(x3, y3)\n\n    # Prepare final output\n    results = [\n        str(ranking1).replace(\" \", \"\"),\n        str(ranking2).replace(\" \", \"\"),\n        str(diff3)\n    ]\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "虽然线性方法很有用，但许多重要的生物学关系本质上是非线性的。本思想实验  将探讨皮尔逊相关性等线性度量的局限性，通过一个对称依赖的例子，展示了为何需要更通用的工具。您将通过推理发现，像互信息（MI）这样的度量可以捕捉到相关性分析所忽略的复杂模式。",
            "id": "4539252",
            "problem": "一项放射组学研究考虑了从肺部肿瘤的治疗前计算机断层扫描图像中提取的单个定量特征 $R$，该特征在一个包含 $n=200$ 名患者的队列中被标准化，使其分布近似为 $\\mathcal{N}(0,1)$。二元临床终点 $Y \\in \\{0,1\\}$ 表示 2 年时的局部控制情况，在本思想实验中其构造如下：存在一个阈值 $\\tau  0$，使得 $Y=\\mathbf{1}(|R|\\tau)$。假设类别流行度是非退化的（即 $\\mathbb{P}(Y=1)\\in(0,1)$），并且为此问题的目的忽略测量噪声。\n\n您必须评估用于相对于 $Y$ 对 $R$ 进行特征选择的单变量筛选方法。仅使用期望、协方差、独立性和互信息的基本定义，以及标准正态分布的对称性，论证在所描述的数据生成过程中，以下筛选器直接应用于原始特征 $R$ 时，是否预期会将 $R$ 标记为对 $Y$ 具有信息量：\n\nA. Pearson 相关性筛选器，该筛选器根据 $R$ 与编码为 $0$ 或 $1$ 的数值标签 $Y$ 之间的样本相关性的绝对值对特征进行排序。\n\nB. $2$ 样本 Student's $t$ 检验，用于比较由 $Y$ 定义的两个类别中 $R$ 的均值。\n\nC. 互信息 (MI) 筛选器，该筛选器使用混合连续-离散变量的一致估计量来估计 $I(R;Y)$，并根据估计的 MI 对特征进行排序。\n\nD. $R$ 与数值标签 $Y$ 之间的 Spearman 秩相关筛选器。\n\nE. 距离相关性筛选器，它使用的总体距离相关性当且仅当变量独立时为零。\n\n在选项 A–E 中，哪些筛选器在原则上预期能够正确识别 $R$ 对 $Y$ 具有信息量，而无需任何先前的非线性特征工程，例如将 $R$ 转换为 $R^2$？\n\n选择所有适用项。",
            "solution": "首先必须评估问题陈述的有效性。\n\n### 步骤 1：提取已知条件\n- 一个定量的放射组学特征 $R$ 是标准化的，其分布近似为 $\\mathcal{N}(0,1)$。\n- 样本大小为 $n=200$。\n- 定义了一个二元临床终点 $Y \\in \\{0,1\\}$。\n- 终点的数据生成过程为 $Y = \\mathbf{1}(|R|  \\tau)$，其中 $\\tau  0$ 为某个常数阈值，$\\mathbf{1}(\\cdot)$ 是指示函数。\n- 类别流行度是非退化的，即 $\\mathbb{P}(Y=1) \\in (0,1)$。\n- 测量噪声将被忽略。\n- 任务是评估五种用于对原始特征 $R$ 相对于 $Y$ 进行特征选择的单变量筛选方法。\n- 评估应使用基本定义和对称性。\n- 问题是，原则上哪些筛选器会识别出 $R$ 对 $Y$ 具有信息量。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据：** 该问题描述了一个简化但合理的放射组学场景，其中特征的量值（不论符号）可能与临床结果相关。所列出的统计方法是该领域的标准方法。使用标准正态分布是对标准化数据建模的常见且有效的简化。该问题基于已建立的统计学和机器学习原理。\n- **适定性：** 该问题在数学上是明确定义的。$R$ 的分布以及 $R$ 和 $Y$ 之间的函数关系都已明确说明。条件 $\\mathbb{P}(Y=1) \\in (0,1)$ 意味着对于 $R \\sim \\mathcal{N}(0,1)$，阈值 $\\tau$ 必须使得 $\\mathbb{P}(|R|  \\tau)$ 不为 $0$ 或 $1$，这对于任何有限的 $\\tau0$ 都成立。问题具体且可回答。\n- **客观性：** 语言正式、精确，且无主观内容。\n- **完整性和一致性：** 该问题是自洽的，并提供了推断统计筛选器在总体水平上行为所需的所有必要信息。没有矛盾之处。\n- **其他标准：** 该问题并非微不足道，因为它需要理解不同统计依赖性度量的局限性。它不是病态的、不切实际的（在思想实验的背景下），或有其他缺陷。\n\n### 步骤 3：结论与行动\n问题陈述是**有效的**。将推导解答。\n\n### 解答推导\n\n问题的核心是确定哪种统计度量可以检测到特定的依赖关系 $Y = \\mathbf{1}(|R|  \\tau)$，其中 $R \\sim \\mathcal{N}(0,1)$。这种关系是对称的：大的正值和大的负值 $R$ 都映射到 $Y=1$，而接近零的 $R$ 值映射到 $Y=0$。我们通过检查每个筛选器测量的总体水平属性来分析其预期行为。设 $p(r)$ 为标准正态分布的概率密度函数，它是一个偶函数：$p(-r) = p(r)$。\n\n**A. Pearson 相关性筛选器**\n\nPearson 相关系数为 $\\rho(R,Y) = \\frac{\\text{Cov}(R,Y)}{\\sigma_R \\sigma_Y}$。如果 $|\\rho(R,Y)|$ 显著不为零，筛选器将标记 $R$。协方差为 $\\text{Cov}(R,Y) = \\mathbb{E}[RY] - \\mathbb{E}[R]\\mathbb{E}[Y]$。\n给定 $R \\sim \\mathcal{N}(0,1)$，我们有 $\\mathbb{E}[R]=0$。因此，$\\text{Cov}(R,Y) = \\mathbb{E}[RY]$。\n我们计算期望 $\\mathbb{E}[RY]$：\n$$ \\mathbb{E}[RY] = \\mathbb{E}[R \\cdot \\mathbf{1}(|R|  \\tau)] = \\int_{-\\infty}^{\\infty} r \\cdot \\mathbf{1}(|r|  \\tau) \\cdot p(r) \\, dr $$\n这个积分可以分为两部分：\n$$ \\mathbb{E}[RY] = \\int_{-\\infty}^{-\\tau} r \\cdot p(r) \\, dr + \\int_{\\tau}^{\\infty} r \\cdot p(r) \\, dr $$\n被积函数 $f(r) = r \\cdot p(r)$ 是一个奇函数，因为 $r$ 是一个奇函数而 $p(r)$ 是一个偶函数。让我们明确地展示这一点。在第一个积分中令 $u = -r$：\n$$ \\int_{-\\infty}^{-\\tau} r \\cdot p(r) \\, dr = \\int_{\\infty}^{\\tau} (-u) \\cdot p(-u) \\cdot (-du) = \\int_{\\infty}^{\\tau} u \\cdot p(u) \\, du = - \\int_{\\tau}^{\\infty} u \\cdot p(u) \\, du $$\n将其代回，我们得到：\n$$ \\mathbb{E}[RY] = - \\int_{\\tau}^{\\infty} r \\cdot p(r) \\, dr + \\int_{\\tau}^{\\infty} r \\cdot p(r) \\, dr = 0 $$\n由于协方差为 $0$，总体 Pearson 相关性 $\\rho(R,Y)$ 为 $0$。基于 Pearson 相关性的筛选器旨在检测线性关系，因此原则上将无法识别 $R$ 和 $Y$ 之间的非线性对称依赖关系。\n\n对 A 的结论：**不正确**。\n\n**B. 2 样本 Student's t 检验**\n\n$t$ 检验比较由 $Y$ 定义的两个组中 $R$ 的均值。它检验原假设 $H_0: \\mathbb{E}[R|Y=0] = \\mathbb{E}[R|Y=1]$。如果拒绝该原假设，则特征 $R$ 将被标记为具有信息量。\n让我们计算条件期望：\n$$ \\mathbb{E}[R|Y=1] = \\mathbb{E}[R \\,|\\, |R|  \\tau] = \\frac{\\mathbb{E}[R \\cdot \\mathbf{1}(|R|  \\tau)]}{\\mathbb{P}(|R|  \\tau)} $$\n分子是 $\\mathbb{E}[RY]$，我们已经证明它为 $0$。由于 $\\mathbb{P}(Y=1) = \\mathbb{P}(|R|  \\tau) \\in (0,1)$，分母不为零。因此，$\\mathbb{E}[R|Y=1] = 0$。\n接下来，我们计算另一个条件期望：\n$$ \\mathbb{E}[R|Y=0] = \\mathbb{E}[R \\,|\\, |R| \\le \\tau] = \\frac{\\mathbb{E}[R \\cdot \\mathbf{1}(|R| \\le \\tau)]}{\\mathbb{P}(|R| \\le \\tau)} $$\n分子是在一个对称区间上的积分：\n$$ \\mathbb{E}[R \\cdot \\mathbf{1}(|R| \\le \\tau)] = \\int_{-\\tau}^{\\tau} r \\cdot p(r) \\, dr $$\n由于被积函数 $r \\cdot p(r)$ 是奇函数，且积分区间 $[-\\tau, \\tau]$ 关于 $0$ 对称，因此该积分为 $0$。分母 $\\mathbb{P}(|R| \\le \\tau)$ 不为零。因此，$\\mathbb{E}[R|Y=0] = 0$。\n由于 $\\mathbb{E}[R|Y=0] = \\mathbb{E}[R|Y=1] = 0$，因此 $t$ 检验的原假设在总体水平上是成立的。因此，不期望 $t$ 检验能将 $R$ 识别为有信息量的特征。\n\n对 B 的结论：**不正确**。\n\n**C. 互信息 (MI) 筛选器**\n\n互信息 $I(R;Y)$ 是衡量两个随机变量之间统计依赖性的度量。互信息的一个基本性质是 $I(R;Y) \\ge 0$，且 $I(R;Y) = 0$ 当且仅当 $R$ 和 $Y$ 是统计独立的。\n在这个问题中，$Y$ 是 $R$ 的一个确定性函数，即 $Y = \\mathbf{1}(|R|  \\tau)$。由于问题陈述类别流行度是非退化的（$\\mathbb{P}(Y=1) \\in (0,1)$），$Y$ 不是一个常数随机变量。如果 $Y$ 是 $R$ 的一个非常数函数，那么 $R$ 和 $Y$ 不可能是独立的。了解 $R$ 会提供关于 $Y$ 的信息。例如，如果我们观察到 $R=0$，我们就能确定 $Y=0$。如果我们观察到 $R = \\tau+1$，我们就能确定 $Y=1$。由于 $R$ 和 $Y$ 不是独立的，因此必然有 $I(R;Y)  0$。\n基于互信息的筛选器，使用一致估计量，将估计出一个大于零的值，从而正确地将 $R$ 标记为对预测 $Y$ 具有信息量。\n\n对 C 的结论：**正确**。\n\n**D. Spearman 秩相关筛选器**\n\nSpearman 相关性通过计算其秩次的 Pearson 相关性来衡量两个变量之间单调关系的强度。让我们分析 $R$ 的秩次与 $Y$ 的值之间的关系。低的 $R$ 值（低秩次）和高的 $R$ 值（高秩次）都导致 $Y=1$。中间的 $R$ 值（中等秩次）导致 $Y=0$。这种关系是“U形”的，并且根本上是非单调的。\n我们可以通过计算总体 Spearman 相关性来形式化这一点，也就是概率积分变换的 Pearson 相关性，$\\rho(F_R(R), F_Y(Y))$。对于混合连续-离散情况，一个更直接的方法是计算 $\\text{Cov}(\\text{rank}(R), Y)$，其中秩次由 CDF 值 $F_R(R)$ 表示。\n$\\text{Cov}(F_R(R), Y) = \\mathbb{E}[F_R(R) \\cdot Y] - \\mathbb{E}[F_R(R)]\\mathbb{E}[Y]$。\n对于任何连续变量 $R$，$F_R(R) \\sim \\text{Uniform}(0,1)$，所以 $\\mathbb{E}[F_R(R)] = 1/2$。\n我们需要 $\\mathbb{E}[F_R(R) \\cdot Y] = \\mathbb{E}[F_R(R) \\cdot \\mathbf{1}(|R|  \\tau)]$：\n$$ \\mathbb{E}[F_R(R) \\cdot Y] = \\int_{|r|\\tau} F_R(r)p(r)dr = \\int_{-\\infty}^{-\\tau} F_R(r)p(r)dr + \\int_{\\tau}^{\\infty} F_R(r)p(r)dr $$\n利用对于关于 $0$ 对称的分布，$F_R(-r) = 1 - F_R(r)$，以及 $p(r)$ 的对称性，第一个积分变为（令 $u=-r$）：\n$$ \\int_{\\infty}^{\\tau} F_R(-u)p(-u)(-du) = \\int_{\\tau}^{\\infty} (1-F_R(u))p(u)du $$\n所以，$\\mathbb{E}[F_R(R) \\cdot Y] = \\int_{\\tau}^{\\infty} (1-F_R(r))p(r)dr + \\int_{\\tau}^{\\infty} F_R(r)p(r)dr = \\int_{\\tau}^{\\infty} p(r)dr = \\mathbb{P}(R\\tau)$。\n此外，$\\mathbb{E}[Y] = \\mathbb{P}(|R|\\tau) = \\mathbb{P}(R-\\tau) + \\mathbb{P}(R\\tau) = 2\\mathbb{P}(R\\tau)$（根据对称性）。\n将这些代入协方差公式：\n$$ \\text{Cov}(F_R(R), Y) = \\mathbb{P}(R\\tau) - \\left(\\frac{1}{2}\\right) \\cdot (2\\mathbb{P}(R\\tau)) = \\mathbb{P}(R\\tau) - \\mathbb{P}(R\\tau) = 0 $$\n总体 Spearman 相关性为零。该筛选器将无法识别这种对称的、非单调的关系。\n\n对 D 的结论：**不正确**。\n\n**E. 距离相关性筛选器**\n\n问题陈述提供了距离相关性的关键属性：总体距离相关性为零当且仅当变量是统计独立的。这是相比 Pearson 相关性的一个关键优势，后者即使对于强依赖变量也可能为零（如选项 A 所示）。\n我们已经在选项 C 的分析中确定，$R$ 和 $Y$ 不是独立的，因为 $Y$ 是 $R$ 的一个非常数的确定性函数。\n由于 $R$ 和 $Y$ 不是独立的，它们的总体距离相关性必须严格大于零。因此，基于距离相关性的筛选器将计算出一个非零值，并正确地将 $R$ 识别为有信息量的特征。\n\n对 E 的结论：**正确**。",
            "answer": "$$\\boxed{CE}$$"
        },
        {
            "introduction": "在认识到互信息（MI）在检测非线性关系方面的优势后，下一步是学习如何严谨地应用它。本次实践  将指导您完成一个完整的特征选择流程，包括特征离散化，以及使用置换检验来评估观测到的MI值的统计显著性。通过这种非参数方法，您可以为特征选择做出有统计学依据的决策，而无需依赖参数假设。",
            "id": "4539256",
            "problem": "一个影像组学数据集通常包含每个受试者的量化图像衍生特征，以及每个受试者的分类结果（例如，良性与恶性）。过滤式方法使用一个有原则的统计标准，独立于任何预测模型对特征进行排序或拒绝。考虑使用互信息（MI）作为过滤标准。目标是为互信息（MI）估计构建一个基于置换的零分布，然后将经验互信息分数转换为$p$值，最后应用显著性阈值来选择特征。\n\n从以下基本概念开始。对于两个离散随机变量 $X$ 和 $Y$，其联合概率质量函数为 $p_{XY}(x,y)$，边际概率质量函数为 $p_X(x)$ 和 $p_Y(y)$，香农熵定义为\n$$\nH(X) = - \\sum_{x} p_X(x) \\log p_X(x),\n$$\n互信息（MI）定义为\n$$\nI(X;Y) = \\sum_{x} \\sum_{y} p_{XY}(x,y) \\log \\frac{p_{XY}(x,y)}{p_X(x)\\,p_Y(y)}.\n$$\n当 $X$ 和 $Y$ 作为样本 $\\{(x_i, y_i)\\}_{i=1}^N$ 被观测时，一个一致的插件估计器使用经验频率 $p_{XY}(x,y) \\approx \\frac{n_{xy}}{N}$，其中 $n_{xy}$ 是落在联合类别 $(x,y)$ 中的样本数量，类似地，$p_X(x) \\approx \\frac{n_x}{N}$ 和 $p_Y(y) \\approx \\frac{n_y}{N}$。\n\n在影像组学中，特征通常是连续的。要使用上述离散互信息的定义作为过滤器，您必须首先通过分箱对每个连续特征进行离散化。对每个特征使用等频分箱法分成 $b$ 个箱，即，计算在分数 $\\frac{1}{b}, \\frac{2}{b}, \\dots, \\frac{b-1}{b}$ 处的经验分位数来定义箱的边界，然后将每个观测到的特征值分配到其箱索引 $\\{0,1,\\dots,b-1\\}$ 中。如果一个特征的所有值都相同，则将所有观测值分配到箱索引 $0$。\n\n为了构建基于置换的MI零分布，在 $X$ 和 $Y$ 相互独立的零假设下，将观测到的类别标签 $Y$ 视为可交换的。生成标签向量的 $R$ 个随机置换，并计算每个置换后的标签向量与每个离散化特征之间的互信息，从而获得一组零分布下的互信息值。使用置换分布将给定特征的经验互信息 $I_{\\text{emp}}$ 转换为单边 $p$ 值：\n$$\np = \\frac{1 + \\sum_{r=1}^R \\mathbf{1}\\!\\left(I_r \\ge I_{\\text{emp}}\\right)}{1 + R},\n$$\n其中 $I_r$ 是在第 $r$ 次置换下计算的互信息，$\\mathbf{1}(\\cdot)$ 是指示函数。这个定义包含一个“加一”校正，以避免在有限的 $R$ 值下出现零 $p$ 值。对于给定的显著性水平 $\\alpha$，如果 $p \\le \\alpha$，则选择该特征。\n\n实现一个程序，对于下面的每个测试用例，执行以下步骤：通过等频分箱将每个特征离散化为 $b$ 个箱；计算每个离散化特征与给定二元标签之间的经验互信息；使用 $R$ 次置换构建基于置换的零分布；计算得出的 $p$ 值；并返回其 $p$ 值小于或等于 $\\alpha$ 的特征的索引。在计算互信息时使用自然对数。不涉及物理单位。不涉及角度。与 $\\alpha$ 比较时，将 $\\alpha$ 视为实数，并报告结果时不使用百分号。\n\n测试套件和参数。为保证可复现性，请使用指定的伪随机种子。所有随机抽样均来自均值为 $0$、标准差为指定值的正态分布。\n\n- 测试用例 1 (正常路径，混合信息量):\n    - 种子: $123$\n    - 样本数 $N$: $40$\n    - 特征数 $F$: $5$\n    - 标签 $Y$: 前 $20$ 个值为 $0$，后 $20$ 个为 $1$\n    - 特征:\n        - 特征 $0$: $x^{(0)}_i = y_i + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 0.15)$\n        - 特征 $1$: $x^{(1)}_i = y_i + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 0.7)$\n        - 特征 $2$: $x^{(2)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n        - 特征 $3$: 对所有 $i$，$x^{(3)}_i = 0.5$\n        - 特征 $4$: $x^{(4)}_i = 2\\,y_i + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 0.05)$\n    - 分箱数 $b$: $4$\n    - 置换次数 $R$: $600$\n    - 显著性水平 $\\alpha$: $0.05$\n- 测试用例 2 (边界情况，完美分离):\n    - 种子: $456$\n    - $N$: $30$\n    - $F$: $3$\n    - 标签 $Y$: 前 $15$ 个值为 $0$，后 $15$ 个为 $1$\n    - 特征:\n        - 特征 $0$: $x^{(0)}_i = y_i$ (精确值)\n        - 特征 $1$: $x^{(1)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n        - 特征 $2$: $x^{(2)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n    - $b$: $3$\n    - $R$: $1000$\n    - $\\alpha$: $0.01$\n- 测试用例 3 (边缘情况，小样本和常数特征):\n    - 种子: $789$\n    - $N$: $12$\n    - $F$: $4$\n    - 标签 $Y$: 前 $6$ 个值为 $0$，后 $6$ 个为 $1$\n    - 特征:\n        - 特征 $0$: $x^{(0)}_i = y_i + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 0.4)$\n        - 特征 $1$: $x^{(1)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n        - 特征 $2$: 对所有 $i$，$x^{(2)}_i = 1.7$\n        - 特征 $3$: $x^{(3)}_i = (1 - y_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 0.1)$\n    - $b$: $3$\n    - $R$: $400$\n    - $\\alpha$: $0.05$\n- 测试用例 4 (边缘情况，完全独立):\n    - 种子: $13579$\n    - $N$: $50$\n    - $F$: $4$\n    - 标签 $Y$: 每个 $y_i$ 独立地以等概率抽取为 $0$ 或 $1$\n    - 特征:\n        - 特征 $0$: $x^{(0)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n        - 特征 $1$: $x^{(1)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n        - 特征 $2$: $x^{(2)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n        - 特征 $3$: $x^{(3)}_i = \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, 1.0)$\n    - $b$: $4$\n    - $R$: $500$\n    - $\\alpha$: $0.05$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个条目是对应测试用例的所选特征索引列表（按升序排列）。例如，正确形状的输出应类似于 $[[0,2],[0],[],[]]$，但其值由上述计算确定。",
            "solution": "所提出的问题是有效的，因为它具有科学依据、定义明确且客观。它基于信息论和非参数统计的既定原则，提出了一个清晰、可形式化的任务，并为获得唯一、可验证的解提供了所有必要的数据和约束。\n\n目标是实现一种用于影像组学数据集的特征选择算法，该算法使用互信息（MI）作为过滤标准。每个特征的MI分数的显著性通过置换检验进行评估。每个测试用例的过程包括几个步骤：数据生成、特征离散化、经验MI计算、通过置换构建MI的零分布、p值计算，以及基于显著性阈值 $\\alpha$ 的最终特征选择。\n\n1.  **数据生成：** 对每个测试用例，模拟一个数据集。它包含一个大小为 $N \\times F$ 的特征矩阵 $X$ 和一个长度为 $N$ 的二元标签向量 $Y$。这里，$N$ 是样本数，$F$ 是特征数。生成过程使用特定的种子值以确保可复现性，并使用伪随机数生成器从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中产生噪声。\n\n2.  **特征离散化：** 所提供的互信息公式适用于离散随机变量。由于影像组学特征是连续的，必须首先对其进行离散化。指定的方法是等频分箱。对于给定的特征向量，我们将其值划分为 $b$ 个箱，每个箱包含大致相同数量的样本。这是通过找到 $b-1$ 个箱边界来实现的，这些边界由特征分布在分数 $\\frac{1}{b}, \\frac{2}{b}, \\dots, \\frac{b-1}{b}$ 处的经验分位数定义。然后，每个连续特征值被替换为来自 $\\{0, 1, \\dots, b-1\\}$ 的整数箱索引。对于所有值都相同的特征，会进行特殊处理；此类特征的所有样本都被分配到箱索引 $0$。这意味着该特征不携带任何信息，其熵将为 $0$。\n\n3.  **互信息估计：** 计算离散化特征 $X_d$ 和类别标签向量 $Y$ 之间的互信息。互信息量化了在已知一个变量的情况下，另一个变量不确定性的减少量。其定义为：\n    $$ I(X_d; Y) = H(X_d) + H(Y) - H(X_d, Y) $$\n    其中 $H(\\cdot)$ 表示香农熵。熵是使用观测频率的“插件”或最大似然估计器来计算的。对于一个概率质量函数为 $p_Z(z)$ 的离散变量 $Z$，其熵为 $H(Z) = - \\sum_{z} p_Z(z) \\log p_Z(z)$，使用自然对数。概率通过计数来估计：\n    -   $p_{X_d}(k) \\approx n_k / N$ 是样本落入箱 $k$ 的概率。\n    -   $p_Y(c) \\approx n_c / N$ 是样本属于类别 $c$ 的概率。\n    -   $p_{X_d,Y}(k,c) \\approx n_{kc} / N$ 是样本处于箱 $k$ 和类别 $c$ 的联合概率。\n    这里，$n_k$、$n_c$ 和 $n_{kc}$ 分别是来自大小为 $N$ 的样本数据中的相应计数。在熵的求和中，所有概率为 $0$ 的项对总和的贡献为 $0$，因为 $\\lim_{p \\to 0} p \\log p = 0$。\n\n4.  **基于置换的显著性检验：** 为了确定一个特征的观测互信息（表示为 $I_{\\text{emp}}$）是否具有统计显著性，我们将其与一个零分布进行比较。零假设 $H_0$ 是特征和类别标签是相互独立的。在 $H_0$ 下，标签向量 $Y$ 的任何置换都是等可能的。\n    通过执行以下过程 $R$ 次来经验地构建零分布：\n    -   通过随机打乱原始标签向量 $Y$ 的元素，创建一个置换后的标签向量 $Y^{(r)}$。\n    -   计算（固定的）离散化特征 $X_d$ 和置换后的标签向量 $Y^{(r)}$ 之间的互信息 $I_r$。\n    这 $R$ 个互信息值的集合 $\\{I_r\\}_{r=1}^R$ 作为零假设下互信息分布的一个经验样本。\n\n5.  **p值计算与特征选择：** 通过与零分布进行比较，将经验互信息 $I_{\\text{emp}}$ 转换为单边 p 值。p 值表示在零假设下，观测到等于或大于 $I_{\\text{emp}}$ 的互信息值的概率。其计算公式为：\n    $$ p = \\frac{1 + \\sum_{r=1}^R \\mathbf{1}\\!\\left(I_r \\ge I_{\\text{emp}}\\right)}{1 + R} $$\n    其中 $\\mathbf{1}(\\cdot)$ 是指示函数，当其参数为真时值为 $1$，否则为 $0$。分子和分母中的“加一”校正可以防止在有限置换次数 $R$ 下 p 值为 $0$，并考虑了观测数据点本身。\n    最后，如果一个特征计算出的 p 值小于或等于预定义的显著性水平 $\\alpha$，则该特征被选为显著特征。每个测试用例的最终结果是所选特征索引的有序列表。",
            "answer": "```python\nimport numpy as np\n\ndef _calculate_mi(discretized_feature, labels, num_feature_bins, num_classes):\n    \"\"\"\n    Calculates the mutual information between a discretized feature and labels.\n    Uses the formula I(X;Y) = H(X) + H(Y) - H(X,Y).\n    \"\"\"\n    N = len(labels)\n    if N == 0:\n        return 0.0\n\n    contingency_table = np.zeros((num_feature_bins, num_classes))\n    for i in range(N):\n        contingency_table[discretized_feature[i], labels[i]] += 1\n\n    p_xy = contingency_table / N\n    p_x = np.sum(p_xy, axis=1)\n    p_y = np.sum(p_xy, axis=0)\n\n    # Filter out zero probabilities to avoid log(0)\n    p_xy_nz = p_xy[p_xy > 0]\n    p_x_nz = p_x[p_x > 0]\n    p_y_nz = p_y[p_y > 0]\n\n    h_xy = -np.sum(p_xy_nz * np.log(p_xy_nz))\n    h_x = -np.sum(p_x_nz * np.log(p_x_nz))\n    h_y = -np.sum(p_y_nz * np.log(p_y_nz))\n\n    mi = h_x + h_y - h_xy\n    # Due to floating point inaccuracies, mi can sometimes be a tiny negative number.\n    return max(0.0, mi)\n\ndef _discretize_feature(feature_vec, num_bins):\n    \"\"\"\n    Discretizes a continuous feature vector using equal-frequency binning.\n    \"\"\"\n    if np.min(feature_vec) == np.max(feature_vec):\n        return np.zeros_like(feature_vec, dtype=int)\n\n    quantiles = np.linspace(1/num_bins, (num_bins - 1)/num_bins, num_bins - 1)\n    bin_edges = np.quantile(feature_vec, quantiles)\n    \n    # searchsorted finds the insertion points, which correspond to the bin indices.\n    discretized = np.searchsorted(bin_edges, feature_vec, side='right')\n    return discretized\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case for feature selection.\n    \"\"\"\n    seed = case_params['seed']\n    N = case_params['N']\n    F = case_params['F']\n    b = case_params['b']\n    R = case_params['R']\n    alpha = case_params['alpha']\n    \n    rng = np.random.default_rng(seed)\n\n    # Generate labels Y\n    if 'y_def' in case_params:\n        if case_params['y_def'] == 'balanced':\n            n_class0 = N // 2\n            n_class1 = N - n_class0\n            Y = np.array([0] * n_class0 + [1] * n_class1, dtype=int)\n        elif case_params['y_def'] == 'random':\n            Y = rng.integers(0, 2, size=N, dtype=int)\n    else:\n        # Fallback for old definition\n        Y = np.array(case_params['Y'], dtype=int)\n\n\n    # Generate features X\n    X = np.zeros((N, F))\n    for j, f_def in enumerate(case_params['features']):\n        noise_std = f_def['noise_std']\n        term = f_def['term']\n        \n        eps = rng.normal(0, noise_std, size=N) if noise_std > 0 else 0\n        \n        if term == 'y':\n            X[:, j] = Y + eps\n        elif term == '2y':\n            X[:, j] = 2 * Y + eps\n        elif term == '1-y':\n            X[:, j] = (1 - Y) + eps\n        elif term == 'const':\n            X[:, j] = f_def['val']\n        elif term == 'noise_only':\n            X[:, j] = eps\n\n    num_classes = len(np.unique(Y))\n    selected_features = []\n\n    for j in range(F):\n        feature = X[:, j]\n        \n        # 1. Discretize feature\n        discretized_feature = _discretize_feature(feature, b)\n        \n        # 2. Compute empirical MI\n        I_emp = _calculate_mi(discretized_feature, Y, b, num_classes)\n        \n        # 3. Construct null distribution via permutation\n        count_ge = 0\n        for _ in range(R):\n            perm_Y = rng.permutation(Y)\n            I_r = _calculate_mi(discretized_feature, perm_Y, b, num_classes)\n            if I_r >= I_emp:\n                count_ge += 1\n        \n        # 4. Calculate p-value\n        p_value = (1 + count_ge) / (1 + R)\n        \n        # 5. Select feature if p = alpha\n        if p_value = alpha:\n            selected_features.append(j)\n            \n    return sorted(selected_features)\n\n\ndef solve():\n    test_cases = [\n        {\n            'seed': 123, 'N': 40, 'F': 5, 'b': 4, 'R': 600, 'alpha': 0.05,\n            'y_def': 'balanced',\n            'features': [\n                {'term': 'y', 'noise_std': 0.15},\n                {'term': 'y', 'noise_std': 0.7},\n                {'term': 'noise_only', 'noise_std': 1.0},\n                {'term': 'const', 'val': 0.5, 'noise_std': 0},\n                {'term': '2y', 'noise_std': 0.05},\n            ]\n        },\n        {\n            'seed': 456, 'N': 30, 'F': 3, 'b': 3, 'R': 1000, 'alpha': 0.01,\n            'y_def': 'balanced',\n            'features': [\n                {'term': 'y', 'noise_std': 0},\n                {'term': 'noise_only', 'noise_std': 1.0},\n                {'term': 'noise_only', 'noise_std': 1.0},\n            ]\n        },\n        {\n            'seed': 789, 'N': 12, 'F': 4, 'b': 3, 'R': 400, 'alpha': 0.05,\n            'y_def': 'balanced',\n            'features': [\n                {'term': 'y', 'noise_std': 0.4},\n                {'term': 'noise_only', 'noise_std': 1.0},\n                {'term': 'const', 'val': 1.7, 'noise_std': 0},\n                {'term': '1-y', 'noise_std': 0.1},\n            ]\n        },\n        {\n            'seed': 13579, 'N': 50, 'F': 4, 'b': 4, 'R': 500, 'alpha': 0.05,\n            'y_def': 'random',\n            'features': [\n                {'term': 'noise_only', 'noise_std': 1.0},\n                {'term': 'noise_only', 'noise_std': 1.0},\n                {'term': 'noise_only', 'noise_std': 1.0},\n                {'term': 'noise_only', 'noise_std': 1.0},\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case)\n        results.append(result)\n\n    print(f\"{results}\")\n\nsolve()\n```"
        }
    ]
}