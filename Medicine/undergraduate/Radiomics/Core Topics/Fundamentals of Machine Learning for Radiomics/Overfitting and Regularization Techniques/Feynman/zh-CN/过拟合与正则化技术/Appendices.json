{
    "hands_on_practices": [
        {
            "introduction": "正则化中的惩罚参数 $\\lambda$ 不仅仅是一个可以随意调节的旋钮，它具有精确的数学含义。通过一个具体的放射组学场景，我们将运用 Karush-Kuhn-Tucker (KKT) 条件来精确计算 LASSO 模型中能将所有特征系数归零的最小 $\\lambda$ 值。这个练习将帮助你从根本上理解正则化是如何通过惩罚项来控制模型复杂度的 。",
            "id": "4553894",
            "problem": "一名放射组学研究员正在将一个连续的影像生物标志物建模为三个源自计算机断层扫描的标准化特征的线性函数：形状伸长率、纹理对比度和强度偏度。对于 $n=6$ 名患者和 $p=3$ 个特征，其设计矩阵为\n$$\nX=\\begin{pmatrix}\n-2  0  1 \\\\\n1  -2  0 \\\\\n1  0  -2 \\\\\n0  1  1 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix},\n$$\n其中 $X$ 的每一列在 $6$ 名患者中的均值为零，方差为 $1$。观测到的生物标志物值（响应向量）为\n$$\ny=\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{pmatrix}.\n$$\n考虑一个不带截距的最小二乘模型，该模型通过最小绝对收缩和选择算子（LASSO）进行正则化，即最小化凸目标函数\n$$\n\\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{1},\n$$\n其中 $\\beta\\in\\mathbb{R}^{3}$，$\\lambda\\ge 0$ 是正则化参数。使用 Karush–Kuhn–Tucker (KKT) 最优性条件，确定使零向量 $\\,\\beta=\\mathbf{0}\\,$ 成为最优解的最小惩罚值 $\\lambda$。然后，为给定的 $X$ 和 $y$ 计算其精确数值。请以单个实数形式提供最终答案。无需四舍五入。",
            "solution": "问题要求找到最小的正则化参数 $\\lambda$，使得系数向量 $\\beta = \\mathbf{0}$ 是 LASSO 目标函数的一个最优解。\n\n首先验证问题陈述的合理性和完整性。\n\n**第 1 步：提取已知条件**\n- 对于 $n=6$ 名患者和 $p=3$ 个特征，设计矩阵为：\n$$\nX=\\begin{pmatrix}\n-2  0  1 \\\\\n1  -2  0 \\\\\n1  0  -2 \\\\\n0  1  1 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}\n$$\n- 响应向量为：\n$$\ny=\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{pmatrix}\n$$\n- 需要最小化的目标函数是 LASSO 泛函：\n$$\nJ(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{1}\n$$\n其中 $\\beta \\in \\mathbb{R}^{3}$ 且 $\\lambda \\ge 0$。\n- 需要分析的条件是找到使 $\\beta = \\mathbf{0}$ 成为最优解的最小 $\\lambda$。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学依据：** 问题描述了 LASSO 回归的一个标准应用，这是统计学习和信号处理中的一项基本技术。其在放射组学中的应用是公认的。数学公式是正确的。\n- **适定性：** 问题陈述清晰，要求在精确条件下（$\\beta = \\mathbf{0}$ 的最优性）求一个特定的、可计算的量（$\\lambda$）。存在唯一的、最小的此类 $\\lambda$ 是 LASSO 问题的一个已知性质。\n- **客观性：** 问题使用精确的数学语言和符号陈述，没有任何主观或模棱两可的术语。\n- **完整性和一致性：** 提供了所有必要的数据（$X$ 和 $y$）和定义。“$X$ 的各列均值为零，单位方差”这一说法是可验证且正确的。例如，对于第一列 $x_1$，均值为 $\\frac{1}{6}(-2+1+1+0+0+0)=0$，方差为 $\\frac{1}{6}((-2)^2+1^2+1^2+0^2+0^2+0^2) = \\frac{6}{6} = 1$。类似的检查可以证实其他列也具有这些性质。问题是自洽且内部一致的。\n\n**结论：** 问题有效。\n\n**第 3 步：求解**\n需要最小化的目标函数是：\n$$\nJ(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{1}\n$$\n这个函数是两个凸函数的和：一个可微的二次项 $f(\\beta) = \\frac{1}{2}\\|y-X\\beta\\|_{2}^{2}$，和一个不可微但凸的正则化项 $g(\\beta) = \\lambda\\|\\beta\\|_{1}$。因为总目标函数 $J(\\beta)$ 是凸的，所以 Karush-Kuhn-Tucker (KKT) 条件是其最优性的充分必要条件。\n\n最优性条件表明，零向量必须是目标函数 $J(\\beta)$ 在最优点 $\\beta^*$ 处的次梯度的一个元素：\n$$\n\\mathbf{0} \\in \\partial J(\\beta^*) = \\nabla f(\\beta^*) + \\partial g(\\beta^*)\n$$\n最小二乘项 $f(\\beta)$ 的梯度是：\n$$\n\\nabla f(\\beta) = \\frac{d}{d\\beta} \\left( \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) \\right) = -X^T(y-X\\beta)\n$$\n$L_1$ 范数项 $g(\\beta)$ 的次梯度是 $\\partial g(\\beta) = \\lambda \\partial\\|\\beta\\|_{1}$。$L_1$ 范数 $\\partial\\|\\beta\\|_{1}$ 的次梯度是所有向量 $s \\in \\mathbb{R}^p$ 的集合，其中对于每个分量 $j \\in \\{1, 2, \\dots, p\\}$：\n$$\ns_j = \n\\begin{cases} \n\\text{sign}(\\beta_j)  \\text{if } \\beta_j \\neq 0 \\\\\n\\in [-1, 1]  \\text{if } \\beta_j = 0 \n\\end{cases}\n$$\n综合这些，最优解 $\\beta^*$ 的 KKT 条件是：\n$$\n-X^T(y-X\\beta^*) + \\lambda s = \\mathbf{0} \\quad \\text{for some } s \\in \\partial\\|\\beta^*\\|_{1}\n$$\n这可以改写为：\n$$\nX^T(y-X\\beta^*) = \\lambda s\n$$\n我们关心的是最优解为零向量的情况，即 $\\beta^* = \\mathbf{0}$。要使此成立，KKT 条件必须在 $\\beta = \\mathbf{0}$ 处成立。将 $\\beta^* = \\mathbf{0}$ 代入该条件得到：\n$$\nX^T(y-X\\mathbf{0}) = \\lambda s\n$$\n$$\nX^T y = \\lambda s\n$$\n在这种情况下，由于 $\\beta^* = \\mathbf{0}$，每个分量 $\\beta_j^*$ 都为 0。因此，根据次梯度的定义，向量 $s$ 的每个分量 $s_j$ 都必须满足 $s_j \\in [-1, 1]$。\n\n条件 $X^T y = \\lambda s$ 是一个方程组，每个特征 $j \\in \\{1, 2, 3\\}$ 对应一个方程：\n$$\n(X^T y)_j = \\lambda s_j\n$$\n为了使 $s_j \\in [-1, 1]$ 有解，我们必须有：\n$$\n|(X^T y)_j| = \\lambda |s_j| \\le \\lambda \\cdot 1 = \\lambda\n$$\n这个不等式 $|(X^T y)_j| \\le \\lambda$ 必须对所有的 $j \\in \\{1, 2, 3\\}$ 成立。为确保这对所有分量都成立，$\\lambda$ 必须大于或等于这些绝对值的最大值：\n$$\n\\lambda \\ge \\max_{j} |(X^T y)_j|\n$$\n这个表达式等价于 $\\lambda \\ge \\|X^T y\\|_{\\infty}$，其中 $\\|\\cdot\\|_{\\infty}$ 是最大绝对分量范数（无穷范数）。\n\n问题要求的是使 $\\beta = \\mathbf{0}$ 成为最优解的*最小* $\\lambda \\ge 0$ 值。这对应于满足导出条件的最小 $\\lambda$ 值。因此，最小的此类 $\\lambda$ 是：\n$$\n\\lambda_{\\text{min}} = \\max_{j} |(X^T y)_j| = \\|X^T y\\|_{\\infty}\n$$\n现在我们计算其数值。首先，我们计算向量 $X^T y$：\n$$\nX^T = \\begin{pmatrix}\n-2  1  1  0  0  0 \\\\\n0  -2  0  1  1  0 \\\\\n1  0  -2  1  0  0\n\\end{pmatrix}\n$$\n$$\ny = \\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3 \\\\\n0 \\\\\n-2 \\\\\n1\n\\end{pmatrix}\n$$\n$$\nX^T y = \\begin{pmatrix}\n(-2)(2) + (1)(-1) + (1)(3) + (0)(0) + (0)(-2) + (0)(1) \\\\\n(0)(2) + (-2)(-1) + (0)(3) + (1)(0) + (1)(-2) + (0)(1) \\\\\n(1)(2) + (0)(-1) + (-2)(3) + (1)(0) + (0)(-2) + (0)(1)\n\\end{pmatrix}\n$$\n$$\nX^T y = \\begin{pmatrix}\n-4 - 1 + 3 \\\\\n2 - 2 \\\\\n2 - 6\n\\end{pmatrix} = \\begin{pmatrix}\n-2 \\\\\n0 \\\\\n-4\n\\end{pmatrix}\n$$\n最后，我们计算这个结果向量的无穷范数，以找到所需的最小 $\\lambda$：\n$$\n\\lambda = \\|X^T y\\|_{\\infty} = \\max(|-2|, |0|, |-4|) = \\max(2, 0, 4)\n$$\n$$\n\\lambda = 4\n$$\n因此，使零向量成为最优解的最小惩罚值是 $4$。对于任何 $\\lambda \\ge 4$，解都保持为 $\\beta = \\mathbf{0}$。对于任何 $\\lambda  4$，至少有一个系数会变为非零。",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "在高维数据分析中，一个最常见也最危险的错误是在交叉验证之前，使用全部数据进行特征选择，这会导致“数据泄漏”并产生过于乐观的模型性能评估。本练习将引导你思考为何这种做法会引入偏差，并设计一个严谨的、能够避免数据泄漏的嵌套交叉验证流程 。这个实践的核心是训练严谨的科研思维，确保模型评估的可靠性。",
            "id": "4553958",
            "problem": "一位放射组学研究者正在处理一个高维磁共振成像特征矩阵和用于肿瘤分级的二元标签。设数据集为 $\\{(x_i,y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^p$ 且 $y_i \\in \\{0,1\\}$，并且 $p \\gg n$。该研究者使用所有 $n$ 个样本计算了全部 $p$ 个特征的单变量筛选得分（例如，相关性或互信息），一次性选出了前 $m$ 个特征，然后使用 $K$ 折交叉验证（CV）报告性能。该研究者了解 $\\ell_1$ 和 $\\ell_2$ 正则化，但在特征筛选期间没有施加惩罚。哪个选项从第一性原理出发，最好地解释了为什么这种做法会在放射组学中引发乐观偏差，并正确地勾勒出一个将特征选择和正则化嵌套在交叉验证折（fold）内的无泄漏流程？\n\nA. 使用所有 $n$ 个样本来选择特征，使得验证分区影响了特征选择图谱，因此验证风险不再是泛化风险的无偏代理；当 $p \\gg n$ 时，尤其可能出现伪关联。一个无泄漏的流程使用外部 $K$ 折交叉验证来估计性能，并且在每个训练折内部，仅使用该折的训练数据计算所有预处理（包括缩放）、执行特征选择和模型拟合，并通过在训练折上进行内部交叉验证来调整 $\\ell_1$ (lasso) 或 $\\ell_2$ (ridge) 的惩罚参数。留出的验证折使用在训练折上学到的统计量进行转换，并且从不用于选择或调优。\n\nB. 只要使用所有 $n$ 个样本对特征进行一次标准化，那么在完整数据集上选择前 $m$ 个特征就不会对交叉验证产生偏倚，因为缩放消除了依赖性；正确的流程是在所有 $n$ 个样本上进行标准化和预选，然后运行不带嵌套的 $K$ 折交叉验证。\n\nC. 乐观偏差仅在使用 $\\ell_1$ 正则化时发生，因为它执行嵌入式选择；$\\ell_2$ 正则化可以防止泄漏，因此可以接受在所有 $n$ 个样本上预选特征，然后执行不带嵌套的交叉验证岭回归。\n\nD. 乐观偏差可以通过在外部交叉验证中进行分层和置换检验来完全纠正，因此可以在交叉验证之前在所有 $n$ 个样本上选择特征；正确的流程是预选一次，然后执行带有基于置换的 $p$ 值的分层 $K$ 折交叉验证以消除估计的偏差。",
            "solution": "题目陈述描述了机器学习流程中一个常见但关键的方法学缺陷，尤其是在放射组学等高维领域。核心问题是一种数据泄漏，它会导致对模型性能的过高估计。\n\n**问题陈述验证**\n\n*   **第 1 步：提取已知信息**\n    *   数据集：$\\{(x_i,y_i)\\}_{i=1}^n$\n    *   特征向量：$x_i \\in \\mathbb{R}^p$\n    *   标签：$y_i \\in \\{0,1\\}$ (二元)\n    *   维度关系：$p \\gg n$ (高维，少样本)\n    *   研究者的流程：\n        1.  使用 $n$ 个样本的整个数据集，计算所有 $p$ 个特征的单变量筛选得分。\n        2.  基于这些得分选择前 $m$ 个特征。这一步在完整数据集上只执行一次。\n        3.  在仅包含所选 $m$ 个特征的简化数据集上，使用 $K$ 折交叉验证（CV）报告性能。\n    *   背景信息：研究者了解 $\\ell_1$ 和 $\\ell_2$ 正则化。在特征筛选步骤中不应用正则化。\n    *   问题：解释为什么这个流程会引发乐观偏差，并描述一个正确的、无泄漏的流程。\n\n*   **第 2 步：使用提取的已知信息进行验证**\n    *   问题陈述具有科学依据。所描述的场景是选择偏差的一个经典例子，这是统计学习理论中一个有据可查的现象。已知条件 $p \\gg n$ 会加剧这种偏差。\n    *   问题提得很好。它要求解释一个已知的错误并描述正确的方法论，对此存在一个标准的、公认的答案。\n    *   问题是客观的，使用精确、技术性的语言来描述一个特定的数据分析工作流程。\n    *   问题设置是完整的且内部一致的。它提供了识别方法学缺陷所需的所有必要细节。\n\n*   **第 3 步：结论和行动**\n    *   问题陈述是有效的。我将继续进行完整的推导和分析。\n\n**原理推导与正确流程**\n\n交叉验证程序试图坚持的基本原则是估计模型的泛化误差，即其在新未见数据上的预期误差。为了获得无偏估计，每一折中的测试（或验证）集必须被留出，并被视为真正未见过的数据。这意味着它不能用于模型训练过程的任何部分。“模型训练过程”不仅包括拟合最终的模型参数，还包括所有使用数据进行决策的前置步骤，如特征缩放、特征选择和超参数调优。\n\n研究者描述的流程违反了这一原则。该流程是：\n1.  特征选择：使用完整数据集 $\\{(x_i, y_i)\\}_{i=1}^n$ 来选择前 $m$ 个特征。\n2.  交叉验证：将数据（现在只有 $m$ 个特征）分成 $K$ 折并执行交叉验证。\n\n错误发生在第 1 步。通过使用所有 $n$ 个样本的标签 $y_i$ 来对特征进行评分和选择，来自整个数据集的信息——包括稍后将用于验证的数据——已经“泄漏”到了特征选择步骤中。\n\n考虑 $K$ 折交叉验证中的一折。数据被划分为训练集 $D_{train}$ 和验证集 $D_{val}$。模型在 $D_{train}$ 上进行训练。然而，构成数据空间的这些特征之所以被选中，是因为它们与*完整数据集*（包括 $D_{val}$）中的标签有很高的相关性。因此，$D_{val}$ 并非独立于特征选择过程。\n\n在特征数量 $p$ 远大于样本数量 $n$（$p \\gg n$）的高维设置中，很可能一些特征纯粹由于随机机会而与结果标签表现出强烈的伪相关性。在完整数据集上执行的特征选择步骤，会优先选择这些伪相关特征。当模型随后在验证折上进行评估时，其性能将被被人为地夸大，因为验证数据本身就参与了选择那些（碰巧）对其具有高度预测性的特征。因此，得到的交叉验证分数是对真实泛化性能的一个乐观偏倚的估计。\n\n一个正确的、无泄漏的流程确保每一折的验证数据在模型开发的任何阶段都未被看到。这是通过一个嵌套交叉验证方案实现的：\n\n1.  **外层循环（性能估计）：** 将数据集分成 $K$ 折。对于每一折 $k \\in \\{1, ..., K\\}$：\n    a. 将第 $k$ 折指定为留出验证集 $D_{val}^{(k)}$。剩下的 $K-1$ 折构成训练集 $D_{train}^{(k)}$。\n    b. **仅使用 $D_{train}^{(k)}$**，执行整个模型构建流程：\n        i. **预处理：** 仅从 $D_{train}^{(k)}$ 计算任何必要的统计量（例如，用于标准化的均值和标准差）。将学到的变换应用于 $D_{train}^{(k)}$ 和 $D_{val}^{(k)}$。\n        ii. **特征选择：** 仅使用 $D_{train}^{(k)}$ 执行特征选择程序（例如，单变量筛选）以识别一组特征。\n        iii. **超参数调优：** 如果所选模型有超参数，例如 $\\ell_1$ (Lasso) 或 $\\ell_2$ (Ridge) 回归的正则化惩罚项 $\\lambda$，则必须对它们进行调优。这通常通过一个**内层交叉验证循环**完成，该循环完全在 $D_{train}^{(k)}$ 上执行。\n        iv. **模型拟合：** 使用在步骤 (ii) 中选择的特征，在整个 $D_{train}^{(k)}$ 上训练最终模型（使用调优后的超参数）。\n    c. **评估：** 使用训练好的模型对留出的验证集 $D_{val}^{(k)}$ 进行预测，并计算性能指标（例如，准确率、AUC）。\n2.  **最终性能估计：** 将所有 $K$ 个外层折的性能指标进行汇总（例如，求平均值），以提供一个单一、稳健且无偏的模型泛化性能估计。\n\n**选项评估**\n\n*   **A. 使用所有 $n$ 个样本来选择特征，使得验证分区影响了特征选择图谱，因此验证风险不再是泛化风险的无偏代理；当 $p \\gg n$ 时，尤其可能出现伪关联。一个无泄漏的流程使用外部 $K$ 折交叉验证来估计性能，并且在每个训练折内部，仅使用该折的训练数据计算所有预处理（包括缩放）、执行特征选择和模型拟合，并通过在训练折上进行内部交叉验证来调整 $\\ell_1$ (lasso) 或 $\\ell_2$ (ridge) 的惩罚参数。留出的验证折使用在训练折上学到的统计量进行转换，并且从不用于选择或调优。**\n    此选项提供了完整而精确的解释。它正确地指出了选择过程中的数据泄漏，注意到了 $p \\gg n$ 设置的加剧效应，并准确地描述了正确的嵌套交叉验证流程，其中所有模型构建步骤（预处理、选择、调优、拟合）都局限在每一折的训练部分。\n    **结论：正确。**\n\n*   **B. 只要使用所有 $n$ 个样本对特征进行一次标准化，那么在完整数据集上选择前 $m$ 个特征就不会对交叉验证产生偏倚，因为缩放消除了依赖性；正确的流程是在所有 $n$ 个样本上进行标准化和预选，然后运行不带嵌套的 $K$ 折交叉验证。**\n    这个选项存在根本性缺陷。首先，在所有 $n$ 个样本上进行标准化本身就是一种数据泄漏。其次，更关键的是，声称缩放“消除了依赖性”从而证明特征预选是合理的，这一说法是错误的。缩放不会改变特征与结果变量之间的相关性，而这正是基于筛选的选择方法所利用的。这个选项恰恰规定了导致偏差的错误流程。\n    **结论：不正确。**\n\n*   **C. 乐观偏差仅在使用 $\\ell_1$ 正则化时发生，因为它执行嵌入式选择；$\\ell_2$ 正则化可以防止泄漏，因此可以接受在所有 $n$ 个样本上预选特征，然后执行不带嵌套的交叉验证岭回归。**\n    这个选项错误地归因了偏差的来源。问题中描述的偏差来自于*先验*的特征选择步骤，这与后续的模型选择（$\\ell_1$、$\\ell_2$ 等）无关。即使使用没有正则化的简单逻辑回归，这种偏差也同样存在。此外，$\\ell_2$ 正则化并不能“防止”在之前独立的步骤中已经发生的泄漏。建议在所有 $n$ 个样本上进行预选是错误的。\n    **结论：不正确。**\n\n*   **D. 乐观偏差可以通过在外部交叉验证中进行分层和置换检验来完全纠正，因此可以在交叉验证之前在所有 $n$ 个样本上选择特征；正确的流程是预选一次，然后执行带有基于置换的 $p$ 值的分层 $K$ 折交叉验证以消除估计的偏差。**\n    这个选项误解了分层和置换检验的作用。分层通过降低其方差来提高交叉验证估计的质量，但它不能纠正由数据泄漏引起的偏差。置换检验是用于假设检验（即，为观察到的性能生成一个 $p$ 值）的工具，而不是用于纠正有偏的性能估计。从有泄漏的流程中获得的性能估计仍然是被夸大的；置换检验只会评估这个被夸大的值是否具有统计显著性，它不会“消除”其偏差。偏差的根本原因——在完整数据集上进行预选——没有得到解决。\n    **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "正则化的核心优势在于它通过引入少量偏差来大幅降低估计量的方差，尤其是在特征高度相关时。本综合编程练习将让你亲手验证这一“偏差-方差权衡”的威力 。你将通过模拟一个含有相关特征的放射组学数据集，推导并编程比较普通最小二乘法（OLS）、岭回归和 LASSO 估计量的方差，从而直观地感受正则化如何稳定模型、对抗过拟合。",
            "id": "4553887",
            "problem": "考虑一个用于从小波分解中提取的放射组学特征的模拟线性模型。设 $X \\in \\mathbb{R}^{n \\times p}$ 表示一个特征矩阵，其列代表标准化的基于小波的放射组学特征，并根据参数为 $\\rho \\in (0,1)$ 的托普利茨结构相互关联，即总体特征协方差满足 $\\Sigma_{ij} = \\rho^{|i-j|}$。假设数据由模型 $y = X \\beta^{\\star} + \\varepsilon$ 生成，其中 $\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是一个具有稀疏非零项的固定但未知的向量，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 是方差为 $\\sigma^{2}$ 的零均值高斯噪声。$X$ 的所有列都中心化至零均值并缩放至单位 $\\ell_{2}$ 范数，响应 $y$ 也进行中心化处理。\n\n目标是刻画当特征高度相关时，无正则化的普通最小二乘估计量的方差膨胀情况，并演示在岭回归中设置正则化参数 $\\lambda  0$ 如何稳定估计量的协方差。仅使用以下基本原理：正态线性模型 $y = X \\beta^{\\star} + \\varepsilon$ 且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，将普通最小二乘估计量定义为平方误差的最小化器，将岭回归（也称为 Tikhonov 正则化）定义为带 $\\ell_{2}$ 惩罚项的平方误差的最小化器，以及将最小绝对值收缩和选择算子（LASSO）定义为带 $\\ell_{1}$ 惩罚项的平方误差的最小化器。从这些定义和高斯噪声模型出发，推导估计量的协方差，并用特征格拉姆矩阵 $X^{\\top} X$ 的谱量和噪声方差 $\\sigma^{2}$ 来表示该协方差。不要假设或使用任何不能从这些基本原理推导出的简化公式。\n\n实现一个程序，该程序：\n- 通过从一个协方差矩阵为 $\\Sigma$（其中 $\\Sigma_{ij} = \\rho^{|i-j|}$）的 $p$ 维零均值多元正态分布中抽取 $n$ 个样本，来模拟具有指定托普利茨相关结构的 $X$，然后对每个特征进行列中心化并缩放至单位 $\\ell_{2}$ 范数。\n- 设置一个固定的稀疏真实值向量 $\\beta^{\\star}$，其在索引 $\\{1,4,6,11,16\\}$ 处的非零项等于 $1$，其他位置为零（描述中使用基于1的索引；在实现中，使用基于0的索引）。\n- 使用带有噪声方差 $\\sigma^{2}$ 的模型生成 $y$。\n- 使用基于 $X^{\\top} X$ 和 $\\sigma^{2}$ 推导出的谱表达式，计算普通最小二乘估计量的精确协方差。\n- 对于给定的 $\\lambda \\ge 0$，使用基于 $X^{\\top} X$、$\\sigma^{2}$ 和 $\\lambda$ 推导出的谱表达式，计算岭估计量的精确协方差。\n- 为每个协方差报告两个标量度量：协方差的迹及其最大特征值。\n- 对于 LASSO 估计量，通过蒙特卡洛模拟来近似估计量的变异性：生成 $B$ 个独立的噪声项副本，形成 $B$ 个独立的响应 $y^{(b)} = X \\beta^{\\star} + \\varepsilon^{(b)}$，通过坐标下降法为每个副本拟合 LASSO 以获得 $\\hat{\\beta}_{\\ell_{1}}^{(b)}$，并通过对 $p$ 个坐标上的系数样本方差求平均来估计平均逐系数方差。\n\n您的程序必须评估以下参数配置的测试套件：\n- 测试 A（理想路径，高度相关特征）：$n = 120$, $p = 20$, $\\rho = 0.95$, $\\sigma^{2} = 0.25$, $\\lambda = 0.1$, LASSO 惩罚水平 $\\alpha = 0.5$, 蒙特卡洛重复次数 $B = 300$。\n- 测试 B（岭正则化的边界条件）：与测试 A 相同的 $n$, $p$, $\\rho$, 和 $\\sigma^{2}$，但 $\\lambda = 0$。\n- 测试 C（边缘情况，近奇异格拉姆矩阵）：$n = 60$, $p = 20$, $\\rho = 0.999$, $\\sigma^{2} = 0.25$, $\\lambda = 0.1$。\n- 测试 D（中度相关）：$n = 120$, $p = 20$, $\\rho = 0.5$, $\\sigma^{2} = 0.25$, $\\lambda = 0.1$。\n\n对于每个测试，计算以下布尔值输出：\n- 对于测试 A、C 和 D：输出两个布尔值，它们当且仅当 (i) 岭协方差的迹严格小于普通最小二乘协方差的迹，以及 (ii) 岭协方差的最大特征值严格小于普通最小二乘协方差的最大特征值时为 $\\text{True}$。\n- 对于测试 B：输出两个布尔值，它们当且仅当 (i) 岭协方差的迹等于普通最小二乘协方差的迹，以及 (ii) 岭协方差的最大特征值等于普通最小二乘协方差的最大特征值时为 $\\text{True}$。等价性应在 $10^{-10}$ 的数值公差内评估。\n- 额外对于测试 A：输出第三个布尔值，它当且仅当 LASSO 估计量的蒙特卡洛估计平均逐系数方差（使用指定的 $\\alpha$）严格小于普通最小二乘平均方差（计算为普通最小二乘协方差的迹除以 $p$）时为 $\\text{True}$。\n\n对所有随机性使用固定的随机种子 $s = 123$ 以确保可复现性。您的程序应生成单行输出，其中包含九个布尔结果，顺序如下：测试 A 岭迹比较，测试 A 岭最大特征值比较，测试 A LASSO 平均方差比较，测试 B 岭迹相等性，测试 B 岭最大特征值相等性，测试 C 岭迹比较，测试 C 岭最大特征值比较，测试 D 岭迹比较，测试 D 岭最大特征值比较。最终输出必须是一个用方括号括起来的逗号分隔列表，例如 $\\left[\\text{True},\\text{False},\\ldots\\right]$。不涉及角度，也不需要物理单位；所有输出都是布尔值。",
            "solution": "该问题要求推导普通最小二乘（OLS）和岭回归估计量的协方差矩阵，并在不同的数据生成参数下，使用这些结果进行计算比较。LASSO 估计量的变异性也需要通过蒙特卡洛模拟进行评估。\n\n统计模型是正态线性模型，由 $y = X \\beta^{\\star} + \\varepsilon$ 给出，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是特征矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是真实但未知的稀疏系数向量，$\\varepsilon$ 是独立同分布（i.i.d.）的高斯噪声向量，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。$X$ 的列经过中心化和缩放，具有单位 $\\ell_{2}$ 范数。\n\n### 普通最小二乘（OLS）估计量的协方差\n\nOLS 估计量，记作 $\\hat{\\beta}_{\\text{OLS}}$，被定义为最小化残差平方和（RSS）的向量 $\\beta$：\n$$ \\hat{\\beta}_{\\text{OLS}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\| y - X \\beta \\|_{2}^{2} $$\n目标函数为 $L(\\beta) = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta$。为了找到最小值，我们对 $\\beta$ 求梯度并令其为零：\n$$ \\nabla_{\\beta} L(\\beta) = -2 X^{\\top}y + 2 X^{\\top}X\\beta = 0 $$\n这得到了正规方程组：$X^{\\top}X\\hat{\\beta}_{\\text{OLS}} = X^{\\top}y$。假设格拉姆矩阵 $G = X^{\\top}X$ 是可逆的（如果 $X$ 具有满列秩，则成立），唯一解为：\n$$ \\hat{\\beta}_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y $$\n为了找到 $\\hat{\\beta}_{\\text{OLS}}$ 的协方差，我们首先用真实参数和噪声来表示该估计量。代入 $y = X \\beta^{\\star} + \\varepsilon$：\n$$ \\hat{\\beta}_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}(X \\beta^{\\star} + \\varepsilon) = (X^{\\top}X)^{-1}(X^{\\top}X)\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon = \\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon $$\n估计量的期望为 $\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = \\mathbb{E}[\\beta^{\\star} + (X^{\\top}X)^{-1}X^{\\top}\\varepsilon] = \\beta^{\\star}$，因为 $\\mathbb{E}[\\varepsilon] = 0$。这证实了 OLS 估计量是无偏的。协方差矩阵定义为 $\\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\mathbb{E}[(\\hat{\\beta}_{\\text{OLS}} - \\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}])(\\hat{\\beta}_{\\text{OLS}} - \\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}])^{\\top}]$。\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\mathbb{E}[((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)((X^{\\top}X)^{-1}X^{\\top}\\varepsilon)^{\\top}] $$\n$$ = \\mathbb{E}[(X^{\\top}X)^{-1}X^{\\top}\\varepsilon\\varepsilon^{\\top}X(X^{\\top}X)^{-1}] $$\n由于 $X$ 被认为是固定的（以其为条件），我们可以将其移到期望之外：\n$$ = (X^{\\top}X)^{-1}X^{\\top}\\mathbb{E}[\\varepsilon\\varepsilon^{\\top}]X(X^{\\top}X)^{-1} $$\n已知 $\\mathbb{E}[\\varepsilon\\varepsilon^{\\top}] = \\text{Cov}(\\varepsilon) = \\sigma^2 I_n$：\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2 (X^{\\top}X)^{-1}X^{\\top}X(X^{\\top}X)^{-1} $$\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\sigma^2 (X^{\\top}X)^{-1} $$\n设对称格拉姆矩阵 $G = X^{\\top}X$ 的谱分解为 $G = V S V^{\\top}$，其中 $S = \\text{diag}(s_1, \\dots, s_p)$ 是特征值 $s_i \\ge 0$ 组成的对角矩阵，$V$ 是相应特征向量构成的正交矩阵。则 $G^{-1} = V S^{-1} V^{\\top}$。协方差为：\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\sigma^2 V S^{-1} V^{\\top} $$\n该协方差矩阵的特征值为 $\\sigma^2/s_i$。其迹为 $\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}})) = \\sigma^2 \\sum_{i=1}^{p} \\frac{1}{s_i}$。最大特征值为 $\\lambda_{\\max}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}})) = \\sigma^2 / s_{\\min}$，其中 $s_{\\min}$ 是 $X^{\\top}X$ 的最小特征值。高的特征相关性（由大的 $\\rho$ 引起）会导致 $X^{\\top}X$ 近奇异，使得 $s_{\\min}$ 非常小，从而增大了估计量的方差。\n\n### 岭回归估计量的协方差\n\n岭回归估计量 $\\hat{\\beta}_{\\text{ridge}}$ 最小化由系数的平方 $\\ell_2$ 范数惩罚的 RSS，正则化参数为 $\\lambda  0$：\n$$ \\hat{\\beta}_{\\text{ridge}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{2}^{2} \\right) $$\n将目标函数的梯度设为零可得：\n$$ -2 X^{\\top}y + 2 X^{\\top}X\\beta + 2 \\lambda \\beta = 0 \\implies (X^{\\top}X + \\lambda I_p)\\hat{\\beta}_{\\text{ridge}} = X^{\\top}y $$\n对于 $\\lambda  0$，矩阵 $(X^{\\top}X + \\lambda I_p)$ 总是可逆的。解为：\n$$ \\hat{\\beta}_{\\text{ridge}} = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}y $$\n代入 $y = X \\beta^{\\star} + \\varepsilon$：\n$$ \\hat{\\beta}_{\\text{ridge}} = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}(X\\beta^{\\star} + \\varepsilon) = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}X\\beta^{\\star} + (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}\\varepsilon $$\n期望为 $\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}] = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}X\\beta^{\\star}$。这不等于 $\\beta^{\\star}$，所以岭估计量是有偏的。其协方差是相对于这个均值计算的：\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{ridge}}) = \\mathbb{E}[(\\hat{\\beta}_{\\text{ridge}} - \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}])(\\hat{\\beta}_{\\text{ridge}} - \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}])^{\\top}] $$\n估计量的随机部分是 $(X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}\\varepsilon$。遵循与 OLS 类似的推导：\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{ridge}}) = (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}\\mathbb{E}[\\varepsilon\\varepsilon^{\\top}]X(X^{\\top}X + \\lambda I_p)^{-1} $$\n$$ = \\sigma^2 (X^{\\top}X + \\lambda I_p)^{-1}X^{\\top}X(X^{\\top}X + \\lambda I_p)^{-1} $$\n使用谱分解 $X^{\\top}X = V S V^{\\top}$，我们有 $(X^{\\top}X + \\lambda I_p)^{-1} = V(S + \\lambda I_p)^{-1}V^{\\top}$。将此代入协方差表达式：\n$$ \\text{Cov}(\\hat{\\beta}_{\\text{ridge}}) = \\sigma^2 [V(S+\\lambda I_p)^{-1}V^{\\top}] [VSV^{\\top}] [V(S+\\lambda I_p)^{-1}V^{\\top}] $$\n$$ = \\sigma^2 V (S+\\lambda I_p)^{-1} S (S+\\lambda I_p)^{-1} V^{\\top} $$\n岭协方差矩阵的特征值为 $\\nu_i = \\sigma^2 \\frac{s_i}{(s_i+\\lambda)^2}$。其迹为 $\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{ridge}})) = \\sum_{i=1}^p \\nu_i = \\sigma^2 \\sum_{i=1}^p \\frac{s_i}{(s_i+\\lambda)^2}$。\n对于任何 $s_i  0$ 和 $\\lambda  0$，我们有 $\\frac{s_i}{(s_i+\\lambda)^2}  \\frac{1}{s_i}$。对所有 $i$求和表明 $\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{ridge}}))  \\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}}))$。同样，岭协方差的最大特征值也严格小于 OLS 协方差的最大特征值。项 $\\lambda$ 通过将 $X^{\\top}X$ 的特征值从零移开来稳定矩阵求逆，从而减小估计量的方差。\n\n### LASSO 估计量的变异性\n\nLASSO 估计量 $\\hat{\\beta}_{\\ell_1}$ 最小化带有 $\\ell_1$ 范数惩罚的 RSS：\n$$ \\hat{\\beta}_{\\ell_1} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\| y - X \\beta \\|_{2}^{2} + \\alpha \\| \\beta \\|_{1} \\right) $$\n其中 $\\| \\beta \\|_{1} = \\sum_{j=1}^p |\\beta_j|$。与 OLS 和岭回归不同，LASSO 估计量没有闭式解。其统计特性，包括其方差，分析起来更为复杂。问题规定了一种蒙特卡洛模拟方法来估计其平均变异性。这包括生成 $B$ 个独立的噪声副本 $\\varepsilon^{(b)}$，形成响应 $y^{(b)} = X \\beta^{\\star} + \\varepsilon^{(b)}$，并对每个响应使用坐标下降法计算 LASSO 估计 $\\hat{\\beta}_{\\ell_1}^{(b)}$。鉴于 $X$ 的列具有单位范数，第 $j$ 个系数的逐坐标更新规则由软阈值算子 $S_{\\alpha}(\\cdot)$ 给出：\n$$ \\beta_j \\leftarrow S_{\\alpha}(X_j^{\\top}(y - \\sum_{k \\neq j} X_k \\beta_k)) = \\text{sgn}(\\rho_j) \\max(|\\rho_j| - \\alpha, 0) $$\n其中 $\\rho_j = X_j^{\\top}(y - \\sum_{k \\neq j} X_k \\beta_k)$。在获得 $B$ 个估计向量的集合 $\\{\\hat{\\beta}_{\\ell_1}^{(b)}\\}_{b=1}^B$ 后，计算每个系数的样本方差 $\\widehat{\\text{Var}}(\\hat{\\beta}_{\\ell_1,j})$。然后，平均逐系数方差为 $\\frac{1}{p}\\sum_{j=1}^p \\widehat{\\text{Var}}(\\hat{\\beta}_{\\ell_1,j})$。将其与平均 OLS 方差 $\\frac{1}{p}\\text{Tr}(\\text{Cov}(\\hat{\\beta}_{\\text{OLS}}))$进行比较。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    RNG = np.random.default_rng(123)\n\n    test_cases = [\n        # Test A: Happy path, high correlation\n        {'n': 120, 'p': 20, 'rho': 0.95, 'sigma2': 0.25, 'lambda_': 0.1, 'alpha': 0.5, 'B': 300, 'test_id': 'A'},\n        # Test B: Boundary condition for Ridge (lambda=0)\n        {'n': 120, 'p': 20, 'rho': 0.95, 'sigma2': 0.25, 'lambda_': 0.0, 'alpha': 0.5, 'B': 300, 'test_id': 'B'},\n        # Test C: Edge case, near-singular Gram matrix\n        {'n': 60, 'p': 20, 'rho': 0.999, 'sigma2': 0.25, 'lambda_': 0.1, 'alpha': None, 'B': None, 'test_id': 'C'},\n        # Test D: Moderate correlation\n        {'n': 120, 'p': 20, 'rho': 0.5, 'sigma2': 0.25, 'lambda_': 0.1, 'alpha': None, 'B': None, 'test_id': 'D'}\n    ]\n\n    all_results = []\n    for params in test_cases:\n        results = run_test(params, RNG)\n        all_results.extend(results)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef run_test(params, rng):\n    \"\"\"\n    Executes a single test case based on the provided parameters.\n    \"\"\"\n    n, p, rho, sigma2, lambda_, alpha, B = params['n'], params['p'], params['rho'], params['sigma2'], params['lambda_'], params['alpha'], params['B']\n    test_id = params['test_id']\n\n    # 1. Simulate data.\n    # Construct the Toeplitz population covariance matrix.\n    cov_matrix = toeplitz(rho ** np.arange(p))\n    \n    # Generate feature matrix X from a multivariate normal distribution.\n    X_raw = rng.multivariate_normal(np.zeros(p), cov_matrix, n)\n    \n    # Center and scale columns of X to unit l2-norm.\n    X_centered = X_raw - np.mean(X_raw, axis=0)\n    col_norms = np.linalg.norm(X_centered, axis=0)\n    # Avoid division by zero for columns with zero norm (highly unlikely).\n    col_norms[col_norms == 0] = 1.0\n    X = X_centered / col_norms\n    \n    # Define the sparse ground-truth coefficient vector beta_star.\n    beta_star = np.zeros(p)\n    beta_star_indices = [0, 3, 5, 10, 15]  # 0-indexed for {1, 4, 6, 11, 16}.\n    beta_star[beta_star_indices] = 1.0\n    \n    # 2. Compute covariance metrics for OLS and Ridge.\n    # Compute the Gram matrix and its spectral decomposition.\n    gram_matrix = X.T @ X\n    s, V = np.linalg.eigh(gram_matrix)  # s: eigenvalues (ascending), V: eigenvectors.\n\n    # OLS covariance metrics.\n    # Eigenvalues of Cov(OLS) are sigma^2 / s_i.\n    s_inv = np.array([1.0/val if val > 1e-12 else 0 for val in s])\n    trace_ols = sigma2 * np.sum(s_inv)\n    lambda_max_ols = sigma2 * np.max(s_inv)\n\n    # Ridge covariance metrics.\n    # Eigenvalues of Cov(Ridge) are sigma^2 * s_i / (s_i + lambda)^2.\n    ridge_eigvals = sigma2 * s / ((s + lambda_)**2)\n    trace_ridge = np.sum(ridge_eigvals)\n    lambda_max_ridge = np.max(ridge_eigvals)\n\n    # 3. Perform comparisons and store boolean results.\n    results_bool = []\n    if test_id in ['A', 'C', 'D']:\n        # For lambda > 0, Ridge variance should be smaller.\n        bool1 = trace_ridge  trace_ols\n        bool2 = lambda_max_ridge  lambda_max_ols\n        results_bool.extend([bool1, bool2])\n    elif test_id == 'B':\n        # For lambda = 0, Ridge is identical to OLS.\n        tol = 1e-10\n        bool1 = abs(trace_ridge - trace_ols)  tol\n        bool2 = abs(lambda_max_ridge - lambda_max_ols)  tol\n        results_bool.extend([bool1, bool2])\n        \n    # 4. LASSO Monte Carlo simulation (for Test A only).\n    if test_id == 'A':\n        # Average variance for OLS estimator.\n        avg_var_ols = trace_ols / p\n        \n        lasso_betas = np.zeros((B, p))\n        \n        # Precompute for efficiency in coordinate descent.\n        XtX = X.T @ X\n        \n        for i in range(B):\n            # Generate new noise and response for each replicate.\n            epsilon = rng.normal(0, np.sqrt(sigma2), n)\n            y = X @ beta_star + epsilon\n            \n            # Precompute X.T @ y for the current replicate.\n            XTy = X.T @ y\n            \n            # Solve LASSO via coordinate descent.\n            beta_lasso = np.zeros(p)\n            for _ in range(100):  # Number of full cycles over coordinates.\n                max_change = 0\n                for j in range(p):\n                    beta_old_j = beta_lasso[j]\n                    # Compute argument for soft-thresholding.\n                    rho_j = XTy[j] - (XtX[j, :] @ beta_lasso - XtX[j, j] * beta_old_j)\n                    \n                    # Apply soft-thresholding.\n                    new_beta_j = np.sign(rho_j) * max(abs(rho_j) - alpha, 0)\n                    beta_lasso[j] = new_beta_j\n                    max_change = max(max_change, abs(new_beta_j - beta_old_j))\n                \n                # Check for convergence.\n                if max_change  1e-7:\n                    break\n\n            lasso_betas[i, :] = beta_lasso\n            \n        # Estimate average coefficient-wise variance from Monte Carlo samples.\n        sample_variances = np.var(lasso_betas, axis=0, ddof=1)\n        avg_var_lasso = np.mean(sample_variances)\n        \n        bool3 = avg_var_lasso  avg_var_ols\n        # Insert the LASSO comparison as the third result for Test A.\n        results_bool.insert(2, bool3)\n\n    return results_bool\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}