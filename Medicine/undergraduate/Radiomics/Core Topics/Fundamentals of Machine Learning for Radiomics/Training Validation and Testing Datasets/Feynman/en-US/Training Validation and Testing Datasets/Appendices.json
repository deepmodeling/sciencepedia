{
    "hands_on_practices": [
        {
            "introduction": "In many radiomics studies, data has a natural hierarchy, such as multiple lesions from a single patient. These lesions are not truly independent; they share a common biological context. This exercise explores the critical concept of intra-patient correlation and reveals why failing to account for it by splitting data at the lesion-level, rather than the patient-level, can lead to a dangerously optimistic estimate of model performance . Understanding this source of bias is the first step toward building truly generalizable models.",
            "id": "4568185",
            "problem": "In a radiomics study, each patient contributes multiple lesions characterized by high-dimensional image-derived features. Consider a supervised learning algorithm trained to predict a binary clinical endpoint from lesion-level radiomic features. The goal is to estimate the generalization error to unseen patients. Let the true population risk be defined as $R := \\mathbb{E}\\big[\\ell\\big(f_{\\mathcal{D}_{\\mathrm{train}}}(X^{*}), Y^{*}\\big)\\big]$, where $\\ell$ is a loss function (for example, $0$-$1$ loss), $f_{\\mathcal{D}_{\\mathrm{train}}}$ is the predictor learned on the training dataset $\\mathcal{D}_{\\mathrm{train}}$, and $(X^{*}, Y^{*})$ is an independent and identically distributed (i.i.d.) draw from the joint distribution of lesions belonging to a new patient not present in $\\mathcal{D}_{\\mathrm{train}}$.\n\nSuppose patient $i$ has a latent patient-level factor $Z_{i}$ that influences all lesions for that patient. A simple and widely used random-effects description is\n$$\nX_{ij} = g(Z_{i}) + \\varepsilon_{ij}, \\quad Y_{ij} = h(Z_{i}) + \\eta_{ij},\n$$\nwhere $j$ indexes lesions for patient $i$, $g$ and $h$ are deterministic functions, and $\\varepsilon_{ij}$ and $\\eta_{ij}$ are mean-zero residuals independent across lesions conditional on $Z_{i}$. The intra-patient correlation coefficient is defined as $\\rho := \\frac{\\mathrm{Var}(g(Z_{i}))}{\\mathrm{Var}(g(Z_{i})) + \\mathrm{Var}(\\varepsilon_{ij})}$, so that $\\rho > 0$ quantifies positive dependence among lesions from the same patient.\n\nTwo splitting strategies are considered for constructing training, validation, and testing datasets:\n- Lesion-level split: lesions are randomly partitioned, so that training, validation, and testing sets may contain different lesions from the same patient.\n- Patient-level split: patients are partitioned, ensuring that lesions from a given patient appear in only one of the training, validation, and testing sets.\n\nUsing only fundamental definitions of generalization error and the random-effects structure above, reason qualitatively about how $\\rho > 0$ affects the expected test-set error when the split is done at the lesion level, compared to the desired error to new patients. Which option best explains the direction of bias and its justification?\n\nA. With $\\rho > 0$, lesions share patient-level signal $Z_{i}$; a lesion-level split leaks $Z_{i}$ into both training and testing. Conditional on $Z_{i}$ already being represented in $\\mathcal{D}_{\\mathrm{train}}$, test lesions from the same patient are easier to predict, so the estimated test error is biased downward relative to $R$ for new patients.\n\nB. With $\\rho > 0$, correlation only inflates the variance of the error estimate; the expected error remains unchanged, so lesion-level splitting is unbiased for $R$.\n\nC. With $\\rho > 0$, models overfit to the training lesions, making test lesions harder to predict; lesion-level splitting therefore yields an upward bias in the estimated error relative to $R$.\n\nD. The bias direction cannot be determined from $\\rho$; correlation affects dependence but not the conditional distribution of errors, so the estimated error under lesion-level splitting can be biased upward or downward irrespective of $\\rho$.\n\nE. Any bias due to $\\rho > 0$ disappears under cross-validation because random folds average over dependence, making the lesion-level split unbiased for $R$.",
            "solution": "The problem asks how intra-patient correlation ($\\rho > 0$) affects the test error estimate when using a lesion-level split. The key is to understand the concept of data leakage.\n\n1.  **Source of Correlation:** The model describes a shared, patient-specific factor $Z_i$ that influences all lesions ($X_{ij}$, $Y_{ij}$) from patient $i$. This factor creates a correlation among a patient's lesions; they are not independent samples.\n\n2.  **Information Leakage:** A lesion-level split randomly assigns individual lesions to the training and test sets. This means it is highly probable that some lesions from patient $i$ will be in the training set, while other lesions from the *same patient* will be in the test set.\n\n3.  **Consequence of Leakage:** When the model trains on lesions from patient $i$, it learns to associate their features with their outcome. Because of the shared factor $Z_i$, it effectively learns to recognize the \"signature\" of patient $i$. When it later encounters other lesions from patient $i$ in the test set, it has an unfair advantage. It is not making a prediction on truly unseen data; it's recognizing a patient it has already seen.\n\n4.  **Direction of Bias:** This prior knowledge makes predicting the outcome for test lesions from patients already in the training set easier than predicting for lesions from a completely new patient. Therefore, the measured error on this \"leaky\" test set will be artificially low. This constitutes a downward bias (an optimistic estimate) relative to the true generalization error $R$, which is defined on new, unseen patients.\n\n*   **Option A** correctly identifies this entire chain of reasoning: shared signal ($Z_i$), leakage via lesion-level split, easier prediction for test lesions from the same patient, and resulting downward bias in the error estimate.\n*   **Option B** is incorrect. The correlation introduces a systematic bias, not just increased variance.\n*   **Option C** incorrectly claims the bias is upward. The leakage makes prediction easier, not harder.\n*   **Option D** is incorrect. The direction of the bias is deterministically downward due to the nature of the information leak.\n*   **Option E** is incorrect. Cross-validation with lesion-level splits simply repeats the same leakage error in every fold; averaging a biased estimate does not remove the bias.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The principle of keeping training and test data strictly separate is the bedrock of trustworthy model evaluation. When data is complex, originating from multiple scans or time points for each patient, it's easy to inadvertently \"leak\" information from the training set into the test set. This practice will guide you through the fundamental set-theoretic methods used to detect such patient-level data leakage, ensuring your evaluation is unbiased and rigorous .",
            "id": "4568150",
            "problem": "A radiomics pipeline is being developed to predict treatment response from Magnetic Resonance Imaging (MRI) volumes, with feature extraction performed on segmented regions. Each patient contributes multiple slices and possibly multiple acquisitions, and images are stored with Digital Imaging and Communications in Medicine (DICOM) metadata, including a patient identifier field. The dataset is organized for $k$-fold cross-validation, and a final hold-out test set is also reserved. To ensure rigorous evaluation, the training, validation, and test sets must be independent at the patient level, meaning that no single patientâ€™s data should appear in more than one cross-validation fold or simultaneously in training and validation/test splits.\n\nFundamental base: In statistical learning, evaluation unbiasedness requires that the training sample and the evaluation sample be drawn from disjoint sets of underlying units to avoid dependence (data leakage). In set-theoretic terms, if $\\mathcal{P}$ denotes the set of unique patient identifiers and $\\{F_1, F_2, \\dots, F_k\\}$ denotes the $k$ disjoint fold subsets of $\\mathcal{P}$, then a proper partition satisfies $F_i \\cap F_j = \\varnothing$ for all $i \\neq j$. Similarly, for a hold-out split with training set $\\mathcal{T} \\subset \\mathcal{P}$ and validation/test sets $\\mathcal{V}, \\mathcal{S} \\subset \\mathcal{P}$, independence requires $\\mathcal{T} \\cap \\mathcal{V} = \\varnothing$, $\\mathcal{T} \\cap \\mathcal{S} = \\varnothing$, and $\\mathcal{V} \\cap \\mathcal{S} = \\varnothing$. A detection procedure should determine whether these disjointness conditions are violated by using only the available patient identifier mapping and split assignments.\n\nWhich of the following procedures correctly detect inadvertent leakage at the patient level in the described setting?\n\nA. Extract the patient identifier for each image from the Digital Imaging and Communications in Medicine (DICOM) metadata, construct the set of unique patient identifiers per fold $\\{F_1, \\dots, F_k\\}$ and per split $\\mathcal{T}, \\mathcal{V}, \\mathcal{S}$, then compute all pairwise intersections $F_i \\cap F_j$ for $i \\neq j$ and $\\mathcal{T} \\cap \\mathcal{V}$, $\\mathcal{T} \\cap \\mathcal{S}$, $\\mathcal{V} \\cap \\mathcal{S}$; flag leakage if any intersection has cardinality greater than $0$.\n\nB. Verify that the number of images per fold is similar, that the class distribution is approximately balanced, and that the total count of images across folds sums to the dataset size; conclude no leakage if these checks pass.\n\nC. Compute feature vectors for all images and check whether any feature vector in one fold is exactly identical to a feature vector in another fold; if such equality is found, flag leakage.\n\nD. Hash each patient identifier to a short fixed-length code using a non-cryptographic hash and compare hashed codes across folds; if no hash collisions are observed across folds, conclude no leakage.\n\nE. For the cross-validation folds, concatenate all patient identifiers and let $U = \\bigcup_{i=1}^k F_i$. Compute $\\lvert U \\rvert$ and $\\sum_{i=1}^k \\lvert F_i \\rvert$; if $\\lvert U \\rvert = \\sum_{i=1}^k \\lvert F_i \\rvert$ and similarly $\\lvert \\mathcal{T} \\cup \\mathcal{V} \\cup \\mathcal{S} \\rvert = \\lvert \\mathcal{T} \\rvert + \\lvert \\mathcal{V} \\rvert + \\lvert \\mathcal{S} \\rvert$, conclude no leakage; otherwise flag leakage.",
            "solution": "The problem requires a procedure to verify that data splits are disjoint at the patient level. This means ensuring that the sets of unique patient identifiers for any two distinct splits (e.g., Fold 1 vs. Fold 2, or Training vs. Test) have no members in common.\n\n*   **Option A** proposes a direct and exhaustive check. It involves creating a set of unique patient IDs for each split and then systematically computing the intersection between every pair of splits that should be disjoint. If any intersection is non-empty (cardinality > 0), it correctly flags leakage. This procedure is a direct implementation of the definition of pairwise disjoint sets and is guaranteed to find any patient-level overlap.\n\n*   **Option E** proposes a mathematically equivalent but less direct check. It relies on the set-theoretic principle that the cardinality of the union of several sets equals the sum of their individual cardinalities *if and only if* they are pairwise disjoint. This is also a correct method for detecting the presence of a leak.\n\n*   **Comparison of A and E:** Both A and E are correct methods for detecting leakage. However, procedure A is diagnostically superior. If procedure E fails (i.e., the sums do not match), it only tells you that a leak exists *somewhere* among the splits. Procedure A, by checking every pair, tells you exactly *which* splits are contaminated (e.g., \"Patient P001 is in both Fold 2 and Fold 5\"). This localization is critical for debugging the data splitting code. Therefore, A represents a more complete and actionable diagnostic procedure.\n\n*   **Option B** is incorrect. Checking for balanced image counts or class distributions are good practices for stratification but do not verify patient independence. A patient could easily have images in multiple folds while the folds themselves remain balanced.\n\n*   **Option C** is incorrect. It wrongly assumes that patient leakage can be detected by identical feature vectors. Different images (e.g., different MRI slices) from the same patient will almost always produce different feature vectors.\n\n*   **Option D** is incorrect. Using non-cryptographic hashes is risky as it can lead to \"collisions\" (different patient IDs producing the same hash), which would cause the procedure to falsely flag leakage where none exists. Direct comparison of patient IDs is simple and error-free.\n\nGiven that procedure A is both correct and provides more detailed diagnostic information than E, it is the best and most thorough procedure among the choices.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A robust cross-validation strategy is essential for developing models that generalize well. This exercise synthesizes two crucial techniques: grouped splitting to respect patient-level data integrity and stratification to ensure representative sampling. You will learn how to design a complete cross-validation scheme that implements both principles, calculating the exact composition of training and validation sets for a realistic radiomics scenario .",
            "id": "4568097",
            "problem": "A radiomics study investigates whether texture features extracted from cancer lesions on Computed Tomography (CT) predict a binary clinical outcome, denoted by $y \\in \\{0,1\\}$, at the patient level. For each patient, features are extracted from $5$ lesions, and all lesions from a given patient must be kept together to avoid information leakage. There are $200$ patients in total, with $80$ patients having $y=1$ and $120$ patients having $y=0$. The model development plan uses $k$-fold Cross-Validation (CV), where Cross-Validation (CV) refers to partitioning the dataset into $k$ disjoint subsets (folds); in each CV iteration, one fold serves as the validation set and the remaining $k-1$ folds form the training set. The CV must be grouped by patient (so that all lesion-level feature vectors from any single patient are assigned to the same fold) and stratified by outcome (so that each fold contains an equal number of $y=1$ and $y=0$ patients, up to exact divisibility).\n\nStarting only from the core definitions of grouped partitioning, stratification by class labels, and $k$-fold CV, design a grouped stratified $5$-fold CV scheme that preserves patient grouping and outcome balance at the patient level. Then compute, for each CV iteration, the exact sizes of:\n- the validation set in patients and lesions,\n- the training set in patients and lesions,\n- the count of $y=1$ patients in the validation set and in the training set,\n- the count of $y=0$ patients in the validation set and in the training set.\n\nReport your final numerical answer as a single row matrix in the order\n$$\\text{(validation patients)} \\;\\&\\; \\text{(training patients)} \\;\\&\\; \\text{(validation } y=1\\text{)} \\;\\&\\; \\text{(validation } y=0\\text{)} \\;\\&\\; \\text{(training } y=1\\text{)} \\;\\&\\; \\text{(training } y=0\\text{)} \\;\\&\\; \\text{(validation lesions)} \\;\\&\\; \\text{(training lesions)}.$$\nNo rounding is required. Express the answer without units.",
            "solution": "The problem requires calculating the sizes of training and validation sets for a single iteration of a 5-fold grouped, stratified cross-validation.\n\n**1. Determine Fold Composition:**\nThe partitioning is grouped by patient and stratified by outcome.\n- Total patients: $200$\n- Number of folds ($k$): $5$\n- Patients per fold: $200 / 5 = 40$ patients.\n\nThe stratification ensures each fold has a proportional number of patients from each class:\n- Total $y=1$ patients: $80$.\n- $y=1$ patients per fold: $80 / 5 = 16$ patients.\n- Total $y=0$ patients: $120$.\n- $y=0$ patients per fold: $120 / 5 = 24$ patients.\n\nEach of the 5 folds will contain $16$ patients with $y=1$ and $24$ patients with $y=0$, for a total of $40$ patients per fold.\n\n**2. Calculate Validation Set Sizes:**\nIn each CV iteration, one fold is used as the validation set.\n- Validation patients: $1 \\text{ fold} \\times 40 \\text{ patients/fold} = 40$.\n- Validation $y=1$ patients: $1 \\text{ fold} \\times 16 \\text{ patients/fold} = 16$.\n- Validation $y=0$ patients: $1 \\text{ fold} \\times 24 \\text{ patients/fold} = 24$.\n- Validation lesions: $40 \\text{ patients} \\times 5 \\text{ lesions/patient} = 200$.\n\n**3. Calculate Training Set Sizes:**\nThe remaining $k-1 = 4$ folds are used for the training set.\n- Training patients: $4 \\text{ folds} \\times 40 \\text{ patients/fold} = 160$.\n- Training $y=1$ patients: $4 \\text{ folds} \\times 16 \\text{ patients/fold} = 64$.\n- Training $y=0$ patients: $4 \\text{ folds} \\times 24 \\text{ patients/fold} = 96$.\n- Training lesions: $160 \\text{ patients} \\times 5 \\text{ lesions/patient} = 800$.\n\n**4. Assemble Final Answer:**\nThe values are ordered as specified in the problem:\n1.  Validation patients: 40\n2.  Training patients: 160\n3.  Validation $y=1$: 16\n4.  Validation $y=0$: 24\n5.  Training $y=1$: 64\n6.  Training $y=0$: 96\n7.  Validation lesions: 200\n8.  Training lesions: 800\n\nThis results in the following matrix:",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n40 & 160 & 16 & 24 & 64 & 96 & 200 & 800\n\\end{pmatrix}\n}\n$$"
        }
    ]
}