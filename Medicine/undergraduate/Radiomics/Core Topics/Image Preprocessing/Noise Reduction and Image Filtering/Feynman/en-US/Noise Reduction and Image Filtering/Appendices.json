{
    "hands_on_practices": [
        {
            "introduction": "Gaussian filtering is a cornerstone of image processing, prized for its ability to suppress noise while preserving large-scale structures. The extent of smoothing is controlled by the standard deviation, $\\sigma$, but is often specified in practice by the more intuitive Full Width at Half Maximum (FWHM). This exercise solidifies the crucial relationship between these two parameters and provides hands-on practice in translating a desired physical smoothing effect into the correct filter settings for a digital image with specific, and often anisotropic, voxel dimensions .",
            "id": "4553384",
            "problem": "A three-dimensional Computed Tomography (CT) image in a radiomics pipeline is to be smoothed by a separable Gaussian filter to reduce high-frequency noise while preserving lesion shape characteristics. The filter acts along each Cartesian axis with identical physical blurring, and its smoothing profile per axis in one dimension is modeled by the Gaussian function $g(x) = A \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)$, where $A$ is the peak amplitude at $x=0$ and $\\sigma$ is the standard deviation. The desired blurring is specified in physical space by the Full Width at Half Maximum (FWHM), defined as the width between the two points where $g(x)$ equals half of its maximum value. \n\nStarting from the definition of the Gaussian function and the definition of Full Width at Half Maximum (FWHM), derive the analytic relationship between FWHM and $\\sigma$ without using any pre-stated shortcut formulas. Then, for a target FWHM of $W = 6\\,\\mathrm{mm}$ applied isotropically in physical space, compute the standard deviation in millimeters, and convert it to per-axis standard deviations in voxels for a CT image with voxel sizes $s_{x} = 0.8\\,\\mathrm{mm}$, $s_{y} = 0.8\\,\\mathrm{mm}$, and $s_{z} = 1.5\\,\\mathrm{mm}$. Express the final answer as a row matrix containing $(\\sigma_{x}, \\sigma_{y}, \\sigma_{z})$ in voxels, where $\\sigma_{i} = \\sigma_{\\mathrm{mm}}/s_{i}$ for $i \\in \\{x,y,z\\}$. Round each component to four significant figures. The final answer must be given in voxels.",
            "solution": "The solution proceeds in three parts: first, deriving the analytical relationship between the Full Width at Half Maximum ($W$, denoted as FWHM in the problem) and the standard deviation ($\\sigma$); second, calculating the standard deviation in physical units (millimeters) for the given $W$; and third, converting this physical standard deviation into per-axis standard deviations in voxel units.\n\n**Part 1: Derivation of the relationship between FWHM and $\\sigma$**\n\nThe one-dimensional Gaussian function is given by:\n$$g(x) = A \\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)$$\nThe maximum value of the function, $g_{\\mathrm{max}}$, occurs at the peak of the Gaussian, which is at $x=0$. Substituting $x=0$ into the equation gives:\n$$g_{\\mathrm{max}} = g(0) = A \\exp\\left(-\\frac{0^{2}}{2\\sigma^{2}}\\right) = A \\exp(0) = A$$\nThe FWHM is defined as the width between the two points $x$ where the function's value is half of its maximum. We set $g(x) = \\frac{1}{2} g_{\\mathrm{max}}$:\n$$g(x) = \\frac{A}{2}$$\nSubstituting the expression for $g(x)$, we get:\n$$A \\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right) = \\frac{A}{2}$$\nAssuming $A \\neq 0$, we can divide both sides by $A$:\n$$\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right) = \\frac{1}{2}$$\nTo solve for $x$, we take the natural logarithm ($\\ln$) of both sides:\n$$\\ln\\left[\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\right] = \\ln\\left(\\frac{1}{2}\\right)$$\n$$-\\frac{x^{2}}{2\\sigma^{2}} = -\\ln(2)$$\nMultiplying by $-1$ on both sides yields:\n$$\\frac{x^{2}}{2\\sigma^{2}} = \\ln(2)$$\nNow, we solve for $x^2$:\n$$x^{2} = 2\\sigma^{2}\\ln(2)$$\nAnd finally for $x$:\n$$x = \\pm \\sqrt{2\\sigma^{2}\\ln(2)} = \\pm \\sigma\\sqrt{2\\ln(2)}$$\nLet the two points be $x_1 = -\\sigma\\sqrt{2\\ln(2)}$ and $x_2 = +\\sigma\\sqrt{2\\ln(2)}$. The FWHM, denoted by $W$, is the distance between these two points:\n$$W = x_2 - x_1 = \\left(\\sigma\\sqrt{2\\ln(2)}\\right) - \\left(-\\sigma\\sqrt{2\\ln(2)}\\right)$$\n$$W = 2\\sigma\\sqrt{2\\ln(2)}$$\nThis is the analytic relationship between the FWHM ($W$) and the standard deviation ($\\sigma$).\n\n**Part 2: Calculation of $\\sigma$ in physical units (mm)**\n\nWe are given a target FWHM of $W = 6\\,\\mathrm{mm}$. We can rearrange the derived relationship to solve for $\\sigma$:\n$$\\sigma = \\frac{W}{2\\sqrt{2\\ln(2)}}$$\nSince the blurring is isotropic in physical space, this standard deviation, let's call it $\\sigma_{\\mathrm{mm}}$, is the same along all axes. Substituting $W = 6\\,\\mathrm{mm}$:\n$$\\sigma_{\\mathrm{mm}} = \\frac{6}{2\\sqrt{2\\ln(2)}} = \\frac{3}{\\sqrt{2\\ln(2)}}\\,\\mathrm{mm}$$\nTo proceed with numerical calculations, we use the value of $\\ln(2) \\approx 0.69314718$:\n$$\\sigma_{\\mathrm{mm}} = \\frac{3}{\\sqrt{2 \\times 0.69314718}} \\approx \\frac{3}{\\sqrt{1.38629436}} \\approx \\frac{3}{1.17741002}\\,\\mathrm{mm}$$\n$$\\sigma_{\\mathrm{mm}} \\approx 2.5481755\\,\\mathrm{mm}$$\n\n**Part 3: Conversion of $\\sigma_{\\mathrm{mm}}$ to voxel units**\n\nThe problem requires the standard deviation to be expressed in voxels for each axis, using the formula $\\sigma_{i} = \\sigma_{\\mathrm{mm}}/s_{i}$ where $s_i$ is the voxel size along axis $i$.\nThe given voxel sizes are $s_{x} = 0.8\\,\\mathrm{mm}$, $s_{y} = 0.8\\,\\mathrm{mm}$, and $s_{z} = 1.5\\,\\mathrm{mm}$.\n\nFor the x-axis:\n$$\\sigma_{x} = \\frac{\\sigma_{\\mathrm{mm}}}{s_{x}} = \\frac{2.5481755\\,\\mathrm{mm}}{0.8\\,\\mathrm{mm}} \\approx 3.185219\\,\\text{voxels}$$\nRounding to four significant figures, $\\sigma_{x} \\approx 3.185$.\n\nFor the y-axis:\n$$\\sigma_{y} = \\frac{\\sigma_{\\mathrm{mm}}}{s_{y}} = \\frac{2.5481755\\,\\mathrm{mm}}{0.8\\,\\mathrm{mm}} \\approx 3.185219\\,\\text{voxels}$$\nRounding to four significant figures, $\\sigma_{y} \\approx 3.185$.\n\nFor the z-axis:\n$$\\sigma_{z} = \\frac{\\sigma_{\\mathrm{mm}}}{s_{z}} = \\frac{2.5481755\\,\\mathrm{mm}}{1.5\\,\\mathrm{mm}} \\approx 1.698784\\,\\text{voxels}$$\nRounding to four significant figures, $\\sigma_{z} \\approx 1.699$.\n\nThe final answer is the row matrix $(\\sigma_x, \\sigma_y, \\sigma_z)$ with the computed values.",
            "answer": "$$\\boxed{\\begin{pmatrix} 3.185 & 3.185 & 1.699 \\end{pmatrix}}$$"
        },
        {
            "introduction": "When applying a filter kernel, a practical question arises at the image boundaries: what values should we assume for pixels that the kernel needs but which lie outside the image? This exercise investigates how different common \"padding\" or boundary handling strategies can introduce systematic errors, or bias, in the resulting pixel values . By analyzing a simplified model of a tumor boundary, you will quantify how these choices affect first-order features, gaining a vital appreciation for how seemingly minor implementation details can impact the reproducibility of radiomic analyses.",
            "id": "4553347",
            "problem": "In radiomics, first-order features derived from filtered images can be sensitive to how boundaries are handled during convolution-based noise reduction. Consider a one-dimensional discrete image intensity profile modeling a tumor adjacent to background, where the interior (indices $n \\geq 1$) has constant intensity $c$, and the exterior (indices $n \\leq 0$) has constant intensity $d$. A Gaussian smoothing filter is applied to reduce noise, and the local mean at the border pixel $n=1$ is taken as a representative first-order feature.\n\nLet the Gaussian kernel be discrete, symmetric, and supported on offsets $k \\in \\{-2,-1,0,1,2\\}$ with parameter $\\sigma = 1$, such that the unnormalized weight at offset $k$ is $\\exp(-k^{2}/(2\\sigma^{2}))$, and the normalized weights are $w_{k} = \\exp(-k^{2}/2)/S$, where $S = \\sum_{j=-2}^{2} \\exp(-j^{2}/2)$. The smoothed value at $n=1$ is computed by discrete convolution $y = \\sum_{k=-2}^{2} w_{k} \\, f(1+k)$.\n\nDefine the following boundary conditions used in convolution:\n- Zero padding: $f(n) = 0$ for all $n \\leq 0$.\n- Mirror (symmetric) padding: for $n \\leq 0$, reflect about the boundary so that $f(0) = f(1)$, $f(-1) = f(2)$, $f(-2) = f(3)$, and so on.\n- Replicate (nearest-neighbor) padding: for $n \\leq 0$, replicate the nearest in-bounds value so that $f(n) = f(1)$.\n\nAssume $c = 1.0$ and $d = 0.3$. Let the “true” smoothed value at $n=1$ be the convolution using the actual continuation of the scene ($f(n) = d$ for $n \\leq 0$ and $f(n) = c$ for $n \\geq 1$). For each boundary condition, define the bias of the first-order local mean feature at $n=1$ as the difference between the padded smoothed value and the true smoothed value:\n$$b_{\\text{pad}} = y_{\\text{pad}} - y_{\\text{true}}.$$\nCompute $b_{\\text{zero}}$, $b_{\\text{mirror}}$, and $b_{\\text{replicate}}$ and provide your final answers as a row matrix. Round each entry to four significant figures. No physical units are required.",
            "solution": "The problem requires the computation of the bias of a first-order feature (local mean) at a boundary pixel for three different padding schemes used in convolution-based filtering. The bias is defined as the difference between the smoothed value obtained using a given padding method and a \"true\" smoothed value.\n\nFirst, we define the components of the problem.\nThe one-dimensional discrete image intensity profile is given by:\n$$\nf_{\\text{true}}(n) = \\begin{cases} c & \\text{for } n \\geq 1 \\\\ d & \\text{for } n \\leq 0 \\end{cases}\n$$\nwhere $c=1.0$ and $d=0.3$.\n\nThe Gaussian smoothing filter uses a discrete, symmetric kernel supported on offsets $k \\in \\{-2, -1, 0, 1, 2\\}$. The normalized weights $w_k$ are given by:\n$$w_k = \\frac{\\exp(-k^2 / (2\\sigma^2))}{S}$$\nwith $\\sigma=1$, so $w_k = \\frac{\\exp(-k^2/2)}{S}$, where $S$ is the normalization constant $S = \\sum_{j=-2}^{2} \\exp(-j^2/2)$.\nThe unnormalized weights are:\n$u_0 = \\exp(0) = 1$\n$u_1 = u_{-1} = \\exp(-1^2/2) = \\exp(-0.5)$\n$u_2 = u_{-2} = \\exp(-2^2/2) = \\exp(-2)$\n\nThe normalization sum is:\n$$S = u_0 + u_1 + u_{-1} + u_2 + u_{-2} = 1 + 2\\exp(-0.5) + 2\\exp(-2)$$\nThe normalized weights are therefore:\n$w_0 = 1/S$\n$w_1 = w_{-1} = \\exp(-0.5)/S$\n$w_2 = w_{-2} = \\exp(-2)/S$\n\nThe smoothed value $y(n)$ is computed by discrete convolution. We are interested in the value at the border pixel $n=1$:\n$$y(1) = \\sum_{k=-2}^{2} w_k \\, f(1+k) = w_{-2}f(-1) + w_{-1}f(0) + w_0f(1) + w_1f(2) + w_2f(3)$$\n\nNext, we calculate the \"true\" smoothed value, $y_{\\text{true}}$, using the un-padded signal $f_{\\text{true}}(n)$. The required signal values are:\n$f_{\\text{true}}(-1) = d$\n$f_{\\text{true}}(0) = d$\n$f_{\\text{true}}(1) = c$\n$f_{\\text{true}}(2) = c$\n$f_{\\text{true}}(3) = c$\n\nSubstituting these into the convolution sum:\n$$y_{\\text{true}} = w_{-2}d + w_{-1}d + w_0c + w_1c + w_2c$$\nUsing the symmetry of the weights ($w_{-k}=w_k$):\n$$y_{\\text{true}} = w_2 d + w_1 d + w_0 c + w_1 c + w_2 c = d(w_1 + w_2) + c(w_0 + w_1 + w_2)$$\n\nNow we evaluate the smoothed value for each padding method. The bias for each method is $b_{\\text{pad}} = y_{\\text{pad}} - y_{\\text{true}}$.\n\n1.  **Zero Padding**\nThe padded signal, $f_{\\text{zero}}(n)$, is defined as $f_{\\text{zero}}(n)=0$ for $n \\leq 0$ and $f_{\\text{zero}}(n)=c$ for $n \\geq 1$.\nThe signal values needed for the convolution at $n=1$ are:\n$f_{\\text{zero}}(-1) = 0$\n$f_{\\text{zero}}(0) = 0$\n$f_{\\text{zero}}(1) = c$\n$f_{\\text{zero}}(2) = c$\n$f_{\\text{zero}}(3) = c$\n\nThe smoothed value is:\n$$y_{\\text{zero}} = w_{-2}(0) + w_{-1}(0) + w_0c + w_1c + w_2c = c(w_0 + w_1 + w_2)$$\nThe bias is:\n$$b_{\\text{zero}} = y_{\\text{zero}} - y_{\\text{true}} = c(w_0 + w_1 + w_2) - \\left[ d(w_1 + w_2) + c(w_0 + w_1 + w_2) \\right]$$\n$$b_{\\text{zero}} = -d(w_1 + w_2)$$\n\n2.  **Mirror (Symmetric) Padding**\nThe padded signal, $f_{\\text{mirror}}(n)$, is defined by reflecting about the boundary ($n=0.5$). For $n \\leq 0$, $f_{\\text{mirror}}(n) = f(1-n)$. Since $n \\leq 0$, $1-n \\geq 1$, and for any argument $m \\geq 1$, the signal value is $c$. Thus, $f_{\\text{mirror}}(n)=c$ for all $n$ within the kernel's reach.\nThe signal values needed are:\n$f_{\\text{mirror}}(-1) = f(1-(-1)) = f(2) = c$\n$f_{\\text{mirror}}(0) = f(1-0) = f(1) = c$\n$f_{\\text{mirror}}(1) = c$\n$f_{\\text{mirror}}(2) = c$\n$f_{\\text{mirror}}(3) = c$\n\nThe smoothed value is:\n$$y_{\\text{mirror}} = \\sum_{k=-2}^{2} w_k c = c \\sum_{k=-2}^{2} w_k = c \\cdot 1 = c$$\nThe bias is:\n$$b_{\\text{mirror}} = y_{\\text{mirror}} - y_{\\text{true}} = c - y_{\\text{true}}$$\n$$b_{\\text{mirror}} = c - \\left[ d(w_1 + w_2) + c(w_0 + w_1 + w_2) \\right]$$\nSince $\\sum w_k = 1$, we can write $c = c \\cdot 1 = c(w_0 + 2w_1 + 2w_2) = c(w_0 + w_1 + w_2) + c(w_1 + w_2)$.\n$$b_{\\text{mirror}} = c(w_0 + w_1 + w_2) + c(w_1 + w_2) - \\left[ d(w_1 + w_2) + c(w_0 + w_1 + w_2) \\right]$$\n$$b_{\\text{mirror}} = c(w_1 + w_2) - d(w_1 + w_2) = (c-d)(w_1 + w_2)$$\n\n3.  **Replicate (Nearest-Neighbor) Padding**\nThe padded signal, $f_{\\text{replicate}}(n)$, is defined by replicating the nearest in-bounds value. For $n \\leq 0$, $f_{\\text{replicate}}(n)=f(1)=c$. This yields the same padded values as the mirror padding for the kernel centered at $n=1$.\n$f_{\\text{replicate}}(-1) = f(1) = c$\n$f_{\\text{replicate}}(0) = f(1) = c$\nAnd so on. Thus, $y_{\\text{replicate}} = y_{\\text{mirror}} = c$.\nThe bias is therefore identical to the mirror padding case:\n$$b_{\\text{replicate}} = y_{\\text{replicate}} - y_{\\text{true}} = (c-d)(w_1 + w_2)$$\n\nFinally, we compute the numerical values.\nFirst, we find the value of $w_1+w_2$:\n$w_1+w_2 = \\frac{\\exp(-0.5)}{S} + \\frac{\\exp(-2)}{S} = \\frac{\\exp(-0.5)+\\exp(-2)}{1+2\\exp(-0.5)+2\\exp(-2)}$\nUsing numerical values:\n$\\exp(-0.5) \\approx 0.60653066$\n$\\exp(-2) \\approx 0.13533528$\n$S \\approx 1 + 2(0.60653066) + 2(0.13533528) = 1 + 1.21306132 + 0.27067056 = 2.48373188$\n$w_1+w_2 \\approx \\frac{0.60653066 + 0.13533528}{2.48373188} = \\frac{0.74186594}{2.48373188} \\approx 0.29868992$\n\nNow, we compute the biases with $c=1.0$ and $d=0.3$.\n$$b_{\\text{zero}} = -d(w_1 + w_2) = -0.3 \\times 0.29868992 \\approx -0.089606976$$\nRounding to four significant figures, $b_{\\text{zero}} \\approx -0.08961$.\n\n$$b_{\\text{mirror}} = (c-d)(w_1 + w_2) = (1.0 - 0.3) \\times 0.29868992 = 0.7 \\times 0.29868992 \\approx 0.209082944$$\nRounding to four significant figures, $b_{\\text{mirror}} \\approx 0.2091$.\n\n$$b_{\\text{replicate}} = b_{\\text{mirror}} \\approx 0.2091$$\n\nThe final answers are provided as a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.08961 & 0.2091 & 0.2091\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond applying predefined filters, a more powerful approach is to design a filter that is statistically optimal for a given imaging scenario. The Wiener filter represents the ideal linear filter for minimizing reconstruction error, provided we can estimate the statistical properties of the signal and the noise. This comprehensive practice guides you through both the theoretical derivation of the Wiener filter and its full numerical implementation, from estimating power spectra from a noisy image to applying the filter and measuring its success . This capstone exercise bridges signal processing theory with practical application, demonstrating a principled, data-driven methodology for image restoration.",
            "id": "4553371",
            "problem": "You are given a linear imaging model for a two-dimensional computed tomography (CT) slice under additive noise. Let the true image be $X[\\mathbf{r}]$, the noise be $N[\\mathbf{r}]$, and the observed image be $Y[\\mathbf{r}] = X[\\mathbf{r}] + N[\\mathbf{r}]$, where $\\mathbf{r} = (u,v)$ indexes pixel coordinates on a finite grid. Assume $X[\\mathbf{r}]$ and $N[\\mathbf{r}]$ are zero-mean, second-order stationary random fields and are uncorrelated. Let $\\mathcal{F}\\{\\cdot\\}$ denote the two-dimensional Discrete Fourier Transform (DFT), and define the two-dimensional angular frequency as $\\boldsymbol{\\omega} = (\\omega_x,\\omega_y)$. The Power Spectral Density (PSD) $S_{AA}(\\boldsymbol{\\omega})$ of a field $A[\\mathbf{r}]$ is the Fourier transform of its autocorrelation function. Under the independence and stationarity assumptions, the observation PSD satisfies $S_{YY}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})$. The goal is to recover a linear, shift-invariant estimator $\\widehat{X}[\\mathbf{r}]$ obtained by filtering $Y[\\mathbf{r}]$ with a filter whose frequency response is $H(\\boldsymbol{\\omega})$, that is, $\\mathcal{F}\\{\\widehat{X}\\}(\\boldsymbol{\\omega}) = H(\\boldsymbol{\\omega}) \\,\\mathcal{F}\\{Y\\}(\\boldsymbol{\\omega})$. You must derive from first principles the optimal $H(\\boldsymbol{\\omega})$ in the sense of minimizing the mean squared error $\\mathbb{E}\\{ \\lvert X[\\mathbf{r}] - \\widehat{X}[\\mathbf{r}] \\rvert^2 \\}$ over all linear shift-invariant estimators.\n\nYou will implement a complete numerical pipeline to:\n- Construct a synthetic CT-like phantom image $X[\\mathbf{r}]$ of size $N \\times N$ with $N = 64$ containing simple geometric structures with piecewise-constant intensities.\n- Generate noisy observations $Y[\\mathbf{r}]$ under three test cases (test suite), with known but unobserved noise realizations $N[\\mathbf{r}]$.\n- Estimate $S_{NN}(\\boldsymbol{\\omega})$ from a background-only patch assumed to contain only noise, and estimate $S_{YY}(\\boldsymbol{\\omega})$ from the full observed image.\n- Infer $S_{XX}(\\boldsymbol{\\omega})$ from $S_{YY}(\\boldsymbol{\\omega})$ and $S_{NN}(\\boldsymbol{\\omega})$ using only the given assumptions.\n- Compute the optimal $H(\\boldsymbol{\\omega})$ you derived, apply it in the frequency domain to obtain $\\widehat{X}[\\mathbf{r}]$, and quantify performance.\n\nFundamental base you may use:\n- Linearity of the Discrete Fourier Transform (DFT) and convolution, and that linear shift-invariant filtering corresponds to multiplication in the frequency domain.\n- The definition of Power Spectral Density (PSD) as the Fourier transform of the autocorrelation function for wide-sense stationary processes.\n- For uncorrelated signal and noise, $S_{YY}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})$.\n- Mean squared error optimality can be analyzed per frequency for linear shift-invariant estimators.\n\nNumerical details to implement:\n- Image size is $N \\times N$ with $N = 64$; use a fixed random seed $1337$ for reproducibility.\n- Construct $X[\\mathbf{r}]$ as follows on a zero background:\n  - A disk centered at $(32,32)$ with radius $12$ and intensity $1.00$.\n  - A vertical bar covering all rows and columns $28$ to $31$ inclusive with intensity $0.70$.\n  - A square from rows $12$ to $19$ and columns $44$ to $51$ inclusive with intensity $1.50$.\n- Define a background-only patch as the top-left $16 \\times 16$ block, that is, indices $u \\in [0,15]$, $v \\in [0,15]$, which contains only background in the clean phantom by construction.\n- Use the following test suite of three noise conditions to create $Y[\\mathbf{r}] = X[\\mathbf{r}] + N[\\mathbf{r}]$:\n  1. White Gaussian noise with standard deviation $0.10$.\n  2. Colored Gaussian noise formed by convolving white Gaussian noise (standard deviation $0.20$) with a Gaussian kernel of standard deviation $2.0$ pixels.\n  3. White Gaussian noise with standard deviation $0.02$.\n- Estimate $S_{YY}(\\boldsymbol{\\omega})$ by the periodogram of $Y[\\mathbf{r}]$ over the full image: if $F_Y = \\mathcal{F}\\{Y\\}$ on the $N \\times N$ grid, use $\\widehat{S}_{YY}(\\boldsymbol{\\omega}) = \\lvert F_Y \\rvert^2 / (N^2)$.\n- Estimate $S_{NN}(\\boldsymbol{\\omega})$ by the periodogram of the noise-only patch, zero-padded to size $N \\times N$ before applying the DFT. If the patch has size $M \\times M$ with $M = 16$, and $F_P$ is the $N \\times N$ DFT of the zero-padded patch, use $\\widehat{S}_{NN}(\\boldsymbol{\\omega}) = \\lvert F_P \\rvert^2 / (M^2)$.\n- Infer $\\widehat{S}_{XX}(\\boldsymbol{\\omega})$ from the stationarity and uncorrelatedness assumptions using only the observable quantities, and enforce nonnegativity by clamping any negative values to $0$.\n- Use a small positive regularizer $\\varepsilon = 10^{-8}$ wherever needed to avoid division by zero during implementation.\n- Apply your derived optimal $H(\\boldsymbol{\\omega})$ to $Y[\\mathbf{r}]$ in the frequency domain and invert the DFT to obtain the real-valued estimate $\\widehat{X}[\\mathbf{r}]$.\n- Quantify performance using Peak Signal-to-Noise Ratio (PSNR), defined as $\\mathrm{PSNR} = 10 \\log_{10}\\!\\left(\\dfrac{A^2}{\\mathrm{MSE}}\\right)$ where $A$ is the known peak intensity of $X[\\mathbf{r}]$ and $\\mathrm{MSE}$ is the mean squared error between $X[\\mathbf{r}]$ and the compared image. Use $A = 1.50$. Report the improvement in PSNR as a float equal to $\\mathrm{PSNR}(\\widehat{X}, X) - \\mathrm{PSNR}(Y, X)$ for each test case.\n\nAngle units do not apply. There are no physical units required. All outputs must be real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$), where each result is the PSNR improvement for the corresponding test case, rounded to $4$ decimal places.\n\nTest suite summary to implement:\n- Case $1$: white noise, standard deviation $0.10$.\n- Case $2$: colored noise via Gaussian convolution, base white noise standard deviation $0.20$, Gaussian kernel standard deviation $2.0$ pixels.\n- Case $3$: white noise, standard deviation $0.02$.\n\nYour code must be a complete, runnable program that constructs $X[\\mathbf{r}]$, generates $Y[\\mathbf{r}]$ per case, estimates the required spectra from the specified patches, derives and applies the optimal $H(\\boldsymbol{\\omega})$, and outputs the PSNR improvements in the specified single-line format.",
            "solution": "The problem requires the derivation of an optimal linear shift-invariant filter for image denoising under an additive noise model, followed by a numerical implementation and evaluation on synthetic data. This is a standard problem in signal and image processing, the solution to which is the Wiener filter.\n\n### 1. Theoretical Derivation of the Optimal Filter\n\nWe are given a linear imaging model where the observed image $Y[\\mathbf{r}]$ is the sum of the true image $X[\\mathbf{r}]$ and uncorrelated, zero-mean, second-order stationary additive noise $N[\\mathbf{r}]$:\n$$ Y[\\mathbf{r}] = X[\\mathbf{r}] + N[\\mathbf{r}] $$\nWe seek a linear shift-invariant (LSI) estimator $\\widehat{X}[\\mathbf{r}]$ of $X[\\mathbf{r}]$. An LSI filter is defined by its convolution kernel $h[\\mathbf{r}]$ or, equivalently, its frequency response $H(\\boldsymbol{\\omega})$. The restored image in the frequency domain is given by:\n$$ \\mathcal{F}\\{\\widehat{X}\\}(\\boldsymbol{\\omega}) = H(\\boldsymbol{\\omega}) \\mathcal{F}\\{Y\\}(\\boldsymbol{\\omega}) $$\nThe objective is to find the filter $H(\\boldsymbol{\\omega})$ that minimizes the mean squared error (MSE) between the true and estimated images, defined as $\\mathrm{MSE} = \\mathbb{E}\\{ |X[\\mathbf{r}] - \\widehat{X}[\\mathbf{r}]|^2 \\}$. Due to the stationarity assumption, the MSE is independent of the spatial coordinate $\\mathbf{r}$.\n\nLet $F_X(\\boldsymbol{\\omega})$, $F_Y(\\boldsymbol{\\omega})$, and $F_{\\widehat{X}}(\\boldsymbol{\\omega})$ be the Discrete Fourier Transforms (DFTs) of the respective signals. The error signal in the frequency domain is $F_E(\\boldsymbol{\\omega}) = F_X(\\boldsymbol{\\omega}) - F_{\\widehat{X}}(\\boldsymbol{\\omega})$. Substituting the filter definition, we get:\n$$ F_E(\\boldsymbol{\\omega}) = F_X(\\boldsymbol{\\omega}) - H(\\boldsymbol{\\omega}) F_Y(\\boldsymbol{\\omega}) $$\nThe MSE can be calculated as the sum (integral in the continuous case) of the Power Spectral Density (PSD) of the error signal, $S_{EE}(\\boldsymbol{\\omega})$, over all frequencies. The PSD is proportional to the expected squared magnitude of the DFT coefficients, $S_{EE}(\\boldsymbol{\\omega}) \\propto \\mathbb{E}\\{ |F_E(\\boldsymbol{\\omega})|^2 \\}$. We can minimize the total MSE by minimizing $S_{EE}(\\boldsymbol{\\omega})$ for each frequency $\\boldsymbol{\\omega}$ independently.\n$$ \\mathbb{E}\\{ |F_E(\\boldsymbol{\\omega})|^2 \\} = \\mathbb{E}\\{ |F_X(\\boldsymbol{\\omega}) - H(\\boldsymbol{\\omega}) F_Y(\\boldsymbol{\\omega})|^2 \\} $$\nExpanding the magnitude squared:\n$$ \\mathbb{E}\\{ |F_E|^2 \\} = \\mathbb{E}\\{ (F_X - H F_Y)(F_X^* - H^* F_Y^*) \\} = \\mathbb{E}\\{ |F_X|^2 \\} - H \\mathbb{E}\\{ F_Y F_X^* \\} - H^* \\mathbb{E}\\{ F_Y^* F_X \\} + |H|^2 \\mathbb{E}\\{ |F_Y|^2 \\} $$\nHere, $H$ is a function of $\\boldsymbol{\\omega}$, and $*$ denotes the complex conjugate. The expectation terms relate to the PSDs ($S_{AA}(\\boldsymbol{\\omega}) \\propto \\mathbb{E}\\{|F_A|^2\\}$) and cross-PSDs ($S_{AB}(\\boldsymbol{\\omega}) \\propto \\mathbb{E}\\{F_A F_B^*\\}$). Since $X$ and $N$ are uncorrelated and zero-mean, their cross-PSD $S_{XN}(\\boldsymbol{\\omega})$ is zero. This implies $\\mathbb{E}\\{F_X F_N^*\\} = 0$.\nThe cross-term $\\mathbb{E}\\{ F_Y F_X^* \\}$ becomes:\n$$ \\mathbb{E}\\{ F_Y F_X^* \\} = \\mathbb{E}\\{ (F_X + F_N) F_X^* \\} = \\mathbb{E}\\{ |F_X|^2 \\} + \\mathbb{E}\\{ F_N F_X^* \\} = \\mathbb{E}\\{ |F_X|^2 \\} $$\nThus, the expression for the error power at frequency $\\boldsymbol{\\omega}$ becomes proportional to:\n$$ S_{EE}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) - H(\\boldsymbol{\\omega}) S_{XX}(\\boldsymbol{\\omega}) - H^*(\\boldsymbol{\\omega}) S_{XX}(\\boldsymbol{\\omega}) + |H(\\boldsymbol{\\omega})|^2 S_{YY}(\\boldsymbol{\\omega}) $$\nTo find the optimal $H(\\boldsymbol{\\omega}) = H_R + iH_I$ that minimizes this real-valued quantity ($S_{XX}$ and $S_{YY}$ are real), we can take partial derivatives with respect to its real and imaginary parts, $H_R$ and $H_I$. Setting $\\frac{\\partial S_{EE}}{\\partial H_I} = 0$ reveals that $H_I=0$, so the optimal filter is purely real. The minimization problem reduces to differentiating with respect to $H(\\boldsymbol{\\omega})$:\n$$ \\frac{\\partial S_{EE}(\\boldsymbol{\\omega})}{\\partial H(\\boldsymbol{\\omega})} = -2 S_{XX}(\\boldsymbol{\\omega}) + 2 H(\\boldsymbol{\\omega}) S_{YY}(\\boldsymbol{\\omega}) = 0 $$\nSolving for $H(\\boldsymbol{\\omega})$ yields the optimal filter transfer function, known as the Wiener filter:\n$$ H_{optimal}(\\boldsymbol{\\omega}) = \\frac{S_{XX}(\\boldsymbol{\\omega})}{S_{YY}(\\boldsymbol{\\omega})} $$\nUsing the given additive property of PSDs for uncorrelated signals, $S_{YY}(\\boldsymbol{\\omega}) = S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})$, we can also write this as:\n$$ H_{optimal}(\\boldsymbol{\\omega}) = \\frac{S_{XX}(\\boldsymbol{\\omega})}{S_{XX}(\\boldsymbol{\\omega}) + S_{NN}(\\boldsymbol{\\omega})} $$\nThis filter intelligently attenuates frequencies where the noise power $S_{NN}$ is high relative to the signal power $S_{XX}$.\n\n### 2. Numerical Implementation\nThe implementation follows the problem's detailed instructions. A fixed random seed of $1337$ is used for reproducibility.\n\n**Phantom Construction:** A true image $X[\\mathbf{r}]$ of size $N \\times N$ with $N=64$ is created. It consists of a zero-intensity background upon which three objects are placed in order: a disk (center $(32,32)$, radius $12$, intensity $1.00$), a vertical bar (columns $28$ to $31$, intensity $0.70$), and a square (rows $12-19$, columns $44-51$, intensity $1.50$).\n\n**Noise Generation and Observation:** For each of the three test cases, a noise field $N[\\mathbf{r}]$ is generated and added to $X[\\mathbf{r}]$ to form the observed image $Y[\\mathbf{r}]$. The noise models are:\n1.  White Gaussian noise with standard deviation $\\sigma=0.10$.\n2.  Colored Gaussian noise, created by convolving white Gaussian noise ($\\sigma=0.20$) with a Gaussian kernel of standard deviation $2.0$ pixels.\n3.  White Gaussian noise with standard deviation $\\sigma=0.02$.\n\n**PSD Estimation:** The required PSDs are estimated from the observed image $Y[\\mathbf{r}]$ using periodograms.\n-   The observed image PSD, $S_{YY}(\\boldsymbol{\\omega})$, is estimated using the full $N \\times N$ image $Y[\\mathbf{r}]$:\n    $$ \\widehat{S}_{YY}(\\boldsymbol{\\omega}) = \\frac{|\\mathcal{F}\\{Y[\\mathbf{r}]\\}|^2}{N^2} $$\n-   The noise PSD, $S_{NN}(\\boldsymbol{\\omega})$, is estimated from a $M \\times M$ noise-only patch ($M=16$) from the top-left corner of $Y[\\mathbf{r}]$, where $X[\\mathbf{r}]=0$. This patch is zero-padded to $N \\times N$ before the DFT. The prescribed formula is:\n    $$ \\widehat{S}_{NN}(\\boldsymbol{\\omega}) = \\frac{|\\mathcal{F}\\{\\text{zero-padded patch}\\}|^2}{M^2} $$\n-   The signal PSD, $S_{XX}(\\boldsymbol{\\omega})$, is then inferred from the other two estimates, with a non-negativity constraint:\n    $$ \\widehat{S}_{XX}(\\boldsymbol{\\omega}) = \\max(0, \\widehat{S}_{YY}(\\boldsymbol{\\omega}) - \\widehat{S}_{NN}(\\boldsymbol{\\omega})) $$\n\n**Filtering and Restoration:** The numerical Wiener filter is constructed using the estimated PSDs and a small regularizer $\\varepsilon=10^{-8}$ to prevent division by zero:\n$$ \\widehat{H}(\\boldsymbol{\\omega}) = \\frac{\\widehat{S}_{XX}(\\boldsymbol{\\omega})}{\\widehat{S}_{YY}(\\boldsymbol{\\omega}) + \\varepsilon} $$\nThe filter is applied by multiplying it with the DFT of the observed image, $F_Y(\\boldsymbol{\\omega})$. The estimated image $\\widehat{X}[\\mathbf{r}]$ is then obtained by taking the real part of the inverse DFT of the product:\n$$ \\widehat{X}[\\mathbf{r}] = \\text{real}(\\mathcal{F}^{-1}\\{ \\widehat{H}(\\boldsymbol{\\omega}) F_Y(\\boldsymbol{\\omega}) \\}) $$\n\n**Performance Evaluation:** The performance is measured by the improvement in Peak Signal-to-Noise Ratio (PSNR), where $\\mathrm{PSNR} = 10 \\log_{10}(A^2 / \\mathrm{MSE})$ and the peak signal amplitude is given as $A=1.50$. The final reported value for each case is the difference: $\\mathrm{PSNR}(\\widehat{X}, X) - \\mathrm{PSNR}(Y, X)$. The results are rounded to $4$ decimal places.",
            "answer": "```python\nimport numpy as np\nimport scipy.ndimage\n\ndef solve():\n    \"\"\"\n    Derives and applies a Wiener filter to denoise synthetic CT images.\n    \"\"\"\n    \n    # --- GLOBAL PARAMETERS ---\n    N = 64  # Image size N x N\n    M = 16  # Noise patch size M x M\n    RANDOM_SEED = 1337\n    EPSILON = 1e-8\n    PEAK_AMPLITUDE = 1.50\n\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # --- HELPER FUNCTIONS ---\n    def create_phantom(n_size):\n        \"\"\"Constructs the synthetic CT-like phantom image.\"\"\"\n        x = np.zeros((n_size, n_size), dtype=np.float64)\n        rows, cols = np.meshgrid(np.arange(n_size), np.arange(n_size), indexing='ij')\n\n        # 1. Disk\n        center_r, center_c, radius = 32, 32, 12\n        disk_mask = (rows - center_r)**2 + (cols - center_c)**2 <= radius**2\n        x[disk_mask] = 1.00\n\n        # 2. Vertical bar\n        x[:, 28:32] = 0.70\n\n        # 3. Square\n        x[12:20, 44:52] = 1.50\n        \n        return x\n\n    def calculate_psnr(img1, img2, peak_val):\n        \"\"\"Calculates the Peak Signal-to-Noise Ratio between two images.\"\"\"\n        mse = np.mean((img1 - img2)**2)\n        if mse == 0:\n            return float('inf')\n        return 10 * np.log10(peak_val**2 / mse)\n\n    # --- MAIN PIPELINE ---\n\n    # 1. Construct the true image X\n    X_true = create_phantom(N)\n\n    # 2. Define the test suite\n    test_cases = [\n        {'type': 'white', 'std': 0.10, 'kernel_std': None},\n        {'type': 'colored', 'std': 0.20, 'kernel_std': 2.0},\n        {'type': 'white', 'std': 0.02, 'kernel_std': None}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # 3. Generate noise and the observed image Y\n        if case['type'] == 'white':\n            noise = rng.normal(loc=0, scale=case['std'], size=(N, N))\n        elif case['type'] == 'colored':\n            white_noise = rng.normal(loc=0, scale=case['std'], size=(N, N))\n            noise = scipy.ndimage.gaussian_filter(white_noise, sigma=case['kernel_std'])\n        \n        Y_observed = X_true + noise\n\n        # 4. Estimate Power Spectral Densities (PSDs)\n        # Estimate S_YY from the full image\n        F_Y = np.fft.fft2(Y_observed)\n        S_YY_hat = np.abs(F_Y)**2 / (N**2)\n\n        # Estimate S_NN from the top-left M x M noise patch\n        noise_patch = Y_observed[0:M, 0:M]\n        padded_patch = np.zeros((N, N), dtype=np.float64)\n        padded_patch[0:M, 0:M] = noise_patch\n        F_P = np.fft.fft2(padded_patch)\n        S_NN_hat = np.abs(F_P)**2 / (M**2)\n\n        # 5. Infer signal PSD S_XX\n        S_XX_hat = S_YY_hat - S_NN_hat\n        S_XX_hat = np.maximum(0, S_XX_hat) # Enforce non-negativity\n\n        # 6. Compute the optimal Wiener filter H\n        H_optimal = S_XX_hat / (S_YY_hat + EPSILON)\n\n        # 7. Apply the filter to obtain the estimated image X_hat\n        F_X_hat = H_optimal * F_Y\n        X_hat = np.real(np.fft.ifft2(F_X_hat))\n\n        # 8. Quantify performance\n        psnr_noisy = calculate_psnr(Y_observed, X_true, PEAK_AMPLITUDE)\n        psnr_filtered = calculate_psnr(X_hat, X_true, PEAK_AMPLITUDE)\n        \n        psnr_improvement = psnr_filtered - psnr_noisy\n        results.append(round(psnr_improvement, 4))\n    \n    # --- FINAL OUTPUT ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}