{
    "hands_on_practices": [
        {
            "introduction": "In biostatistics, we often work with measurements that are imperfect representations of a true biological quantity. The classical measurement error model, $W = X + U$, provides a simple yet powerful framework for thinking about this. This exercise guides you through one of the most fundamental methods for quantifying error: the test-retest study. By deriving the relationship between the test-retest correlation and the measurement error variance, you will see how a simple reliability study can reveal the proportion of observed variability that is just noise .",
            "id": "4956402",
            "problem": "A biostatistics study considers the classical measurement error model for a continuous biomarker measured in the general adult population. Let the true individual-level biomarker be denoted by $X$, and suppose a single clinic reading $W$ is observed according to $W = X + U$, where $U$ is a random measurement error with $E(U) = 0$, $\\operatorname{Var}(U) = \\sigma_{U}^{2}$, and $U$ is independent of $X$. In a test-retest substudy, a second reading $W'$ is taken on the same individual under identical conditions within a short interval such that the true value is unchanged. The second reading follows $W' = X + U'$, where $U'$ is an independent copy of $U$ and is independent of $X$ and $U$.\n\nFrom the test-retest substudy, the population test-retest correlation $\\rho_{WW'}$ is estimated from a large sample as $0.75$. From an independent large population health survey, the population variance of the single clinic reading $W$ is estimated as $\\sigma_{W}^{2} = 196$ $\\mathrm{mmHg}^{2}$.\n\nUsing only the definitions of covariance $\\operatorname{Cov}(\\cdot,\\cdot)$ and correlation, and the assumptions stated above for the classical measurement error model, derive a method-of-moments estimator for $\\sigma_{U}^{2}$ in terms of $\\rho_{WW'}$ and $\\sigma_{W}^{2}$. In your derivation, connect the test-retest correlation $\\rho_{WW'}$ to the reliability coefficient $\\lambda$ by showing that under classical error $\\lambda = \\rho_{WW'}$. Then, using the given numerical values, compute the resulting estimate of $\\sigma_{U}^{2}$. Round your final numeric answer to four significant figures. Express the variance in $\\mathrm{mmHg}^{2}$.",
            "solution": "The classical measurement error model is given by the relationship between an observed reading, $W$, a true underlying value, $X$, and a random measurement error, $U$.\nThe first reading is modeled as:\n$$W = X + U$$\nThe assumptions for the classical model are:\n1.  The mean of the measurement error is zero: $E(U) = 0$.\n2.  The variance of the measurement error is a constant: $\\operatorname{Var}(U) = \\sigma_{U}^{2}$.\n3.  The measurement error $U$ is independent of the true value $X$.\n\nFrom these assumptions, we can derive the variance of the observed reading $W$. Let $\\operatorname{Var}(X) = \\sigma_{X}^{2}$.\n$$\n\\sigma_{W}^{2} = \\operatorname{Var}(W) = \\operatorname{Var}(X + U)\n$$\nDue to the independence of $X$ and $U$, the variance of their sum is the sum of their variances:\n$$\n\\sigma_{W}^{2} = \\operatorname{Var}(X) + \\operatorname{Var}(U) = \\sigma_{X}^{2} + \\sigma_{U}^{2}\n$$\nThis equation shows that the observed variance is the sum of the true biological variance and the measurement error variance.\n\nA test-retest substudy provides a second measurement, $W'$, on the same individual. The true value $X$ is assumed to be unchanged. The model for the second reading is:\n$$W' = X + U'$$\nHere, $U'$ is the measurement error for the second reading. It is assumed to be an independent copy of $U$, meaning $E(U') = 0$, $\\operatorname{Var}(U') = \\sigma_{U}^{2}$, and $U'$ is independent of $X$ and $U$.\n\nThe problem requires us to use the test-retest correlation, $\\rho_{WW'}$. The definition of the correlation coefficient between $W$ and $W'$ is:\n$$\n\\rho_{WW'} = \\frac{\\operatorname{Cov}(W, W')}{\\sqrt{\\operatorname{Var}(W)\\operatorname{Var}(W')}}\n$$\nSince the measurements are taken under identical conditions, $\\operatorname{Var}(W') = \\operatorname{Var}(W) = \\sigma_{W}^{2}$. Therefore, the denominator is $\\sqrt{\\sigma_{W}^{2} \\cdot \\sigma_{W}^{2}} = \\sigma_{W}^{2}$.\nThe formula simplifies to:\n$$\n\\rho_{WW'} = \\frac{\\operatorname{Cov}(W, W')}{\\sigma_{W}^{2}}\n$$\nNext, we must calculate the covariance term, $\\operatorname{Cov}(W, W')$, using the model definitions $W = X + U$ and $W' = X + U'$.\n$$\n\\operatorname{Cov}(W, W') = \\operatorname{Cov}(X + U, X + U')\n$$\nUsing the bilinearity property of covariance:\n$$\n\\operatorname{Cov}(X + U, X + U') = \\operatorname{Cov}(X, X) + \\operatorname{Cov}(X, U') + \\operatorname{Cov}(U, X) + \\operatorname{Cov}(U, U')\n$$\nWe evaluate each term based on the independence assumptions:\n-   $\\operatorname{Cov}(X, X) = \\operatorname{Var}(X) = \\sigma_{X}^{2}$.\n-   $X$ and $U'$ are independent, so $\\operatorname{Cov}(X, U') = 0$.\n-   $U$ and $X$ are independent, so $\\operatorname{Cov}(U, X) = 0$.\n-   $U$ and $U'$ are independent, so $\\operatorname{Cov}(U, U') = 0$.\n\nSubstituting these results back, we find:\n$$\n\\operatorname{Cov}(W, W') = \\sigma_{X}^{2}\n$$\nNow, we substitute this into the formula for the test-retest correlation:\n$$\n\\rho_{WW'} = \\frac{\\sigma_{X}^{2}}{\\sigma_{W}^{2}}\n$$\nThe problem also asks to connect this to the reliability coefficient, $\\lambda$. In classical test theory, the reliability coefficient is defined as the proportion of the total observed variance that is due to the true score variance.\n$$\n\\lambda = \\frac{\\text{True Score Variance}}{\\text{Observed Score Variance}} = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(W)} = \\frac{\\sigma_{X}^{2}}{\\sigma_{W}^{2}}\n$$\nBy comparing this definition with our derived expression for the test-retest correlation, we see that:\n$$\n\\lambda = \\rho_{WW'}\n$$\nThis shows that under the classical measurement error model, the test-retest correlation is equivalent to the reliability coefficient.\n\nNow, we derive the method-of-moments estimator for $\\sigma_{U}^{2}$. We have a system of two equations with population moments ($\\sigma_{W}^{2}$, $\\rho_{WW'}$) and model parameters ($\\sigma_{X}^{2}$, $\\sigma_{U}^{2}$):\n1.  $\\sigma_{W}^{2} = \\sigma_{X}^{2} + \\sigma_{U}^{2}$\n2.  $\\rho_{WW'} = \\frac{\\sigma_{X}^{2}}{\\sigma_{W}^{2}}$\n\nOur goal is to express $\\sigma_{U}^{2}$ in terms of the estimable quantities $\\sigma_{W}^{2}$ and $\\rho_{WW'}$.\nFrom equation (2), we can express the unobserved true variance $\\sigma_{X}^{2}$ as:\n$$\n\\sigma_{X}^{2} = \\rho_{WW'} \\sigma_{W}^{2}\n$$\nNow, we substitute this expression for $\\sigma_{X}^{2}$ into equation (1):\n$$\n\\sigma_{W}^{2} = (\\rho_{WW'} \\sigma_{W}^{2}) + \\sigma_{U}^{2}\n$$\nFinally, we solve for $\\sigma_{U}^{2}$:\n$$\n\\sigma_{U}^{2} = \\sigma_{W}^{2} - \\rho_{WW'} \\sigma_{W}^{2}\n$$\nFactoring out $\\sigma_{W}^{2}$ gives the desired estimator:\n$$\n\\hat{\\sigma}_{U}^{2} = \\hat{\\sigma}_{W}^{2} (1 - \\hat{\\rho}_{WW'})\n$$\nwhere the hats denote estimates. The problem provides large-sample estimates, which we treat as the true population values for this calculation.\n\nThe given numerical values are:\n-   $\\rho_{WW'} = 0.75$\n-   $\\sigma_{W}^{2} = 196$ $\\mathrm{mmHg}^{2}$\n\nSubstituting these values into our derived formula for $\\sigma_{U}^{2}$:\n$$\n\\sigma_{U}^{2} = 196 \\times (1 - 0.75)\n$$\n$$\n\\sigma_{U}^{2} = 196 \\times 0.25\n$$\n$$\n\\sigma_{U}^{2} = \\frac{196}{4} = 49\n$$\nThe resulting estimate for the measurement error variance is $49$ $\\mathrm{mmHg}^{2}$. The problem requires the final answer to be rounded to four significant figures. As an exact integer, $49$ can be written as $49.00$ to meet this requirement.",
            "answer": "$$\n\\boxed{49.00}\n$$"
        },
        {
            "introduction": "While a test-retest study with two measurements is insightful, many validation studies collect multiple replicates to better characterize an instrument's precision. The Analysis of Variance (ANOVA) framework is the canonical tool for this task. In this practice, you will use the ANOVA decomposition of sums of squares to derive unbiased estimators for both the true between-subject variance ($\\sigma_{X}^{2}$) and the within-subject measurement error variance ($\\sigma_{U}^{2}$) . This skill is essential for designing and analyzing experiments aimed at validating new measurement techniques.",
            "id": "4956444",
            "problem": "A biostatistics team is studying a biomarker using repeated measurements per individual to quantify measurement error and true between-subject variability. For $i \\in \\{1,\\dots,n\\}$ subjects and $j \\in \\{1,\\dots,r\\}$ technical replicates per subject, the observed measurement is modeled as $W_{ij} = X_i + U_{ij}$, where $X_i$ is the unobserved true subject-level quantity and $U_{ij}$ is the measurement error. Assume $X_i$ are independent and identically distributed with $\\mathbb{E}[X_i] = \\mu$ and $\\mathbb{V}(X_i) = \\sigma_{X}^{2}$, and $U_{ij}$ are independent and identically distributed with $\\mathbb{E}[U_{ij}] = 0$ and $\\mathbb{V}(U_{ij}) = \\sigma_{U}^{2}$, independent of $\\{X_i\\}$. Any potential systematic bias of the instrument is modeled as an unknown constant offset $b$ that is common to all observations, and the team is interested in variance components, which should be unaffected by $b$. Using the standard framework of Analysis of Variance (ANOVA), derive unbiased estimators for the within-subject variance component $\\sigma_{U}^{2}$ and the between-subject variance component $\\sigma_{X}^{2}$ under this balanced design. Express your final estimators as closed-form analytic expressions in terms of the observed data $\\{W_{ij}\\}$, the subject-specific means $\\bar{W}_{i\\cdot} = \\frac{1}{r}\\sum_{j=1}^{r} W_{ij}$, and the grand mean $\\bar{W}_{\\cdot\\cdot} = \\frac{1}{nr}\\sum_{i=1}^{n}\\sum_{j=1}^{r} W_{ij}$. Present the final answer as a row matrix ordered as $\\left(\\hat{\\sigma}_{U}^{2}, \\hat{\\sigma}_{X}^{2}\\right)$. No numerical rounding is required; provide exact symbolic expressions.",
            "solution": "The model for the observed measurement for subject $i$ and replicate $j$ is given as $W_{ij} = X_i + U_{ij}$. A systematic bias $b$ is stated to be a constant offset for all observations, so we can write the full model as $W_{ij} = b + X_i + U_{ij}$. The random variables $X_i$ represent the true subject-level quantities and are independent and identically distributed (i.i.d.) with mean $\\mathbb{E}[X_i] = \\mu$ and variance $\\mathbb{V}(X_i) = \\sigma_{X}^{2}$. The terms $U_{ij}$ represent the measurement errors and are i.i.d. with mean $\\mathbb{E}[U_{ij}] = 0$ and variance $\\mathbb{V}(U_{ij}) = \\sigma_{U}^{2}$. The sets of variables $\\{X_i\\}$ and $\\{U_{ij}\\}$ are mutually independent.\n\nOur goal is to find unbiased estimators for the within-subject variance component, $\\sigma_{U}^{2}$, and the between-subject variance component, $\\sigma_{X}^{2}$. We use the method of moments, which in this context involves calculating the expected values of the ANOVA mean squares and equating them to the observed mean squares.\n\nThe key quantities in a one-way ANOVA are the sums of squares.\nThe Within-Subject Sum of Squares ($SSW$) is defined as:\n$$SSW = \\sum_{i=1}^{n} \\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2$$\nwhere $\\bar{W}_{i\\cdot} = \\frac{1}{r} \\sum_{j=1}^{r} W_{ij}$ is the mean for subject $i$.\n\nThe Between-Subjects Sum of Squares ($SSB$) is defined as:\n$$SSB = \\sum_{i=1}^{n} \\sum_{j=1}^{r} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2 = r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2$$\nwhere $\\bar{W}_{\\cdot\\cdot} = \\frac{1}{nr} \\sum_{i=1}^{n} \\sum_{j=1}^{r} W_{ij}$ is the grand mean.\n\nThe corresponding Mean Squares are the sums of squares divided by their degrees of freedom.\nThe Mean Square Within ($MSW$) has $n(r-1)$ degrees of freedom:\n$$MSW = \\frac{SSW}{n(r-1)}$$\nThe Mean Square Between ($MSB$) has $n-1$ degrees of freedom:\n$$MSB = \\frac{SSB}{n-1}$$\n\nTo find unbiased estimators, we first compute the expected values of $MSW$ and $MSB$.\n\nLet's analyze the term $(W_{ij} - \\bar{W}_{i\\cdot})$:\n$\\bar{W}_{i\\cdot} = \\frac{1}{r} \\sum_{j=1}^{r} (b + X_i + U_{ij}) = b + X_i + \\frac{1}{r} \\sum_{j=1}^{r} U_{ij} = b + X_i + \\bar{U}_{i\\cdot}$.\n$W_{ij} - \\bar{W}_{i\\cdot} = (b + X_i + U_{ij}) - (b + X_i + \\bar{U}_{i\\cdot}) = U_{ij} - \\bar{U}_{i\\cdot}$.\nNotice that the systematic bias $b$ and the subject-specific true value $X_i$ cancel out, meaning $SSW$ and $MSW$ are not affected by them.\n\nThe expected value of $SSW$ is:\n$$\\mathbb{E}[SSW] = \\mathbb{E}\\left[\\sum_{i=1}^{n} \\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2\\right] = \\sum_{i=1}^{n} \\mathbb{E}\\left[\\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2\\right]$$\nFor any given subject $i$, the term $\\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2$ is $(r-1)$ times the sample variance of the $r$ i.i.d. error terms $\\{U_{ij}\\}_{j=1}^r$. The expected value of the sample variance is the population variance, $\\sigma_{U}^{2}$.\nTherefore, $\\mathbb{E}\\left[\\sum_{j=1}^{r} (U_{ij} - \\bar{U}_{i\\cdot})^2\\right] = (r-1)\\sigma_{U}^{2}$.\nSumming over all $n$ subjects, we get:\n$$\\mathbb{E}[SSW] = \\sum_{i=1}^{n} (r-1)\\sigma_{U}^{2} = n(r-1)\\sigma_{U}^{2}$$\nNow, we find the expectation of $MSW$:\n$$\\mathbb{E}[MSW] = \\frac{\\mathbb{E}[SSW]}{n(r-1)} = \\frac{n(r-1)\\sigma_{U}^{2}}{n(r-1)} = \\sigma_{U}^{2}$$\nThis shows that $MSW$ is an unbiased estimator for the within-subject variance $\\sigma_{U}^{2}$.\n\nNext, we compute the expected value of $MSB$. First, consider the terms $(\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})$:\n$\\bar{W}_{\\cdot\\cdot} = \\frac{1}{n} \\sum_{i=1}^{n} \\bar{W}_{i\\cdot} = \\frac{1}{n} \\sum_{i=1}^{n} (b + X_i + \\bar{U}_{i\\cdot}) = b + \\bar{X}_{\\cdot} + \\bar{U}_{\\cdot\\cdot}$.\n$\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot} = (b + X_i + \\bar{U}_{i\\cdot}) - (b + \\bar{X}_{\\cdot} + \\bar{U}_{\\cdot\\cdot}) = (X_i - \\bar{X}_{\\cdot}) + (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot})$.\nAgain, the bias $b$ cancels. The expected value of $SSB$ is:\n$$\\mathbb{E}[SSB] = \\mathbb{E}\\left[r \\sum_{i=1}^{n} ( (X_i - \\bar{X}_{\\cdot}) + (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot}) )^2\\right]$$\nDue to the independence of $X_i$ and $U_{ij}$, the expectation of the cross-product term is zero.\n$$\\mathbb{E}[SSB] = r \\mathbb{E}\\left[\\sum_{i=1}^{n} (X_i - \\bar{X}_{\\cdot})^2\\right] + r \\mathbb{E}\\left[\\sum_{i=1}^{n} (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot})^2\\right]$$\nThe first term involves the sample variance of $\\{X_i\\}$. Since $\\mathbb{E}[\\sum_{i=1}^{n} (X_i - \\bar{X}_{\\cdot})^2] = (n-1)\\mathbb{V}(X_i) = (n-1)\\sigma_{X}^{2}$.\nThe second term involves the sample variance of $\\{\\bar{U}_{i\\cdot}\\}$. The variables $\\bar{U}_{i\\cdot}$ are i.i.d. with mean $0$ and variance $\\mathbb{V}(\\bar{U}_{i\\cdot}) = \\mathbb{V}(\\frac{1}{r}\\sum_j U_{ij}) = \\frac{1}{r^2} \\sum_j \\mathbb{V}(U_{ij}) = \\frac{r\\sigma_{U}^{2}}{r^2} = \\frac{\\sigma_{U}^{2}}{r}$.\nSo, $\\mathbb{E}[\\sum_{i=1}^{n} (\\bar{U}_{i\\cdot} - \\bar{U}_{\\cdot\\cdot})^2] = (n-1)\\mathbb{V}(\\bar{U}_{i\\cdot}) = (n-1)\\frac{\\sigma_{U}^{2}}{r}$.\nCombining these results:\n$$\\mathbb{E}[SSB] = r(n-1)\\sigma_{X}^{2} + r(n-1)\\frac{\\sigma_{U}^{2}}{r} = r(n-1)\\sigma_{X}^{2} + (n-1)\\sigma_{U}^{2}$$\nNow, we find the expectation of $MSB$:\n$$\\mathbb{E}[MSB] = \\frac{\\mathbb{E}[SSB]}{n-1} = \\frac{r(n-1)\\sigma_{X}^{2} + (n-1)\\sigma_{U}^{2}}{n-1} = r\\sigma_{X}^{2} + \\sigma_{U}^{2}$$\nWe have a system of two equations for the two unknown variance components:\n$1$. $\\mathbb{E}[MSW] = \\sigma_{U}^{2}$\n$2$. $\\mathbb{E}[MSB] = r\\sigma_{X}^{2} + \\sigma_{U}^{2}$\n\nUsing the method of moments, we substitute the observed statistics for their expectations to obtain the estimators $\\hat{\\sigma}_{U}^{2}$ and $\\hat{\\sigma}_{X}^{2}$.\nFrom equation $1$, we get the estimator for the within-subject variance:\n$$\\hat{\\sigma}_{U}^{2} = MSW = \\frac{SSW}{n(r-1)} = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)}$$\nFrom equation $2$, we substitute $\\hat{\\sigma}_{U}^{2}$ for $\\sigma_{U}^{2}$:\n$MSB = r\\hat{\\sigma}_{X}^{2} + \\hat{\\sigma}_{U}^{2} = r\\hat{\\sigma}_{X}^{2} + MSW$.\nSolving for $\\hat{\\sigma}_{X}^{2}$:\n$$r\\hat{\\sigma}_{X}^{2} = MSB - MSW$$\n$$\\hat{\\sigma}_{X}^{2} = \\frac{MSB - MSW}{r}$$\nSubstituting the expressions for $MSB$ and $MSW$:\n$$\\hat{\\sigma}_{X}^{2} = \\frac{1}{r} \\left( \\frac{SSB}{n-1} - \\frac{SSW}{n(r-1)} \\right)$$\n$$\\hat{\\sigma}_{X}^{2} = \\frac{1}{r} \\left( \\frac{r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2}{n-1} - \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} \\right)$$\nThese two expressions for $\\hat{\\sigma}_{U}^{2}$ and $\\hat{\\sigma}_{X}^{2}$ are the required unbiased estimators.\n\nThe final estimators are:\nFor the within-subject variance component $\\sigma_U^2$:\n$$\\hat{\\sigma}_{U}^{2} = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)}$$\nFor the between-subject variance component $\\sigma_X^2$:\n$$\\hat{\\sigma}_{X}^{2} = \\frac{1}{r} \\left( \\frac{r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2}{n-1} - \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} \\right)$$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} & \\frac{1}{r} \\left( \\frac{r \\sum_{i=1}^{n} (\\bar{W}_{i\\cdot} - \\bar{W}_{\\cdot\\cdot})^2}{n-1} - \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{r} (W_{ij} - \\bar{W}_{i\\cdot})^2}{n(r-1)} \\right) \\end{pmatrix} } $$"
        },
        {
            "introduction": "After learning to quantify measurement error, a crucial next question is: what are its practical consequences? This exercise tackles this question in the context of a randomized controlled trial (RCT), the gold standard for clinical evidence. You will derive the mathematical relationship showing how random error in the outcome variable reduces statistical power, forcing researchers to increase their sample size to detect a true effect . This reveals the direct economic and ethical 'cost' of imprecision in study design.",
            "id": "4956457",
            "problem": "Consider a balanced Randomized Controlled Trial (RCT) comparing a binary treatment ($X_{i} \\in \\{0,1\\}$) with equal allocation. The continuous outcome for participant $i$ follows the classical normal linear model\n$$\nY_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\varepsilon_{i},\n$$\nwhere $\\varepsilon_{i}$ are independent and identically distributed with $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$ and independent of $X_{i}$. The scientific estimand of interest is the treatment effect $\\beta_{1} = \\Delta \\neq 0$. Due to imperfect measurement, the observed outcome is subject to additive, independent measurement error:\n$$\nY_{i}^{*} = Y_{i} + V_{i}, \\quad V_{i} \\sim \\mathcal{N}(0,\\sigma_{V}^{2}),\n$$\nwith $V_{i}$ independent of $Y_{i}$ and $X_{i}$. An investigator uses the usual two-sample $t$-test (equivalently, ordinary least squares regression of $Y_{i}^{*}$ on $X_{i}$ with an intercept) to test $H_{0}:\\beta_{1}=0$ at a fixed significance level, and considers power through the noncentrality parameter of the $t$-statistic under the alternative $\\beta_{1}=\\Delta$.\n\nLet the total sample size be $n$, with $n/2$ individuals per arm. Using only the definitions of the $t$-statistic and the variance of sample means under independent, identically distributed normal errors, first derive the noncentrality parameter under $Y^{*}$ in terms of $\\Delta$, $n$, $\\sigma_{Y}^{2}$, and $\\sigma_{V}^{2}$. Then, by equating this to the noncentrality parameter obtained under the error-free outcome $Y$ (i.e., with variance $\\sigma_{Y}^{2}$), derive the sample size inflation factor $k = n^{*}/n$ required under $Y^{*}$ to maintain the same noncentrality parameter as with $Y$. Express your final answer as a single simplified analytic expression for $k$. No numerical evaluation or rounding is required, and no units should be included in the final expression.",
            "solution": "The solution proceeds in three main parts: first, we characterize the statistical properties of the observed outcome $Y_i^*$; second, we derive the noncentrality parameter (NCP) of the two-sample $t$-test based on $Y_i^*$; third, we compare this to the NCP of a hypothetical test on the true outcome $Y_i$ to find the required sample size inflation factor.\n\nThe model for the true outcome for participant $i$ is given by\n$$\nY_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})\n$$\nThe observed outcome, $Y_i^*$, is subject to an independent, additive measurement error $V_i$:\n$$\nY_{i}^{*} = Y_{i} + V_{i}, \\quad V_{i} \\sim \\mathcal{N}(0,\\sigma_{V}^{2})\n$$\nWe can substitute the first equation into the second to obtain a model for the observed outcome:\n$$\nY_{i}^{*} = (\\beta_{0} + \\beta_{1} X_{i} + \\varepsilon_{i}) + V_{i} = \\beta_{0} + \\beta_{1} X_{i} + U_{i}\n$$\nwhere $U_{i} = \\varepsilon_{i} + V_{i}$. Since $\\varepsilon_{i}$ and $V_{i}$ are independent and normally distributed, their sum $U_i$ is also normally distributed. The mean of $U_i$ is $E[U_{i}] = E[\\varepsilon_{i}] + E[V_{i}] = 0 + 0 = 0$. The variance of $U_i$ is $\\operatorname{Var}(U_{i}) = \\operatorname{Var}(\\varepsilon_{i}) + \\operatorname{Var}(V_{i}) = \\sigma_{Y}^{2} + \\sigma_{V}^{2}$, due to their independence. Let us denote the variance of the new error term as $\\sigma_{Y^{*}}^{2} = \\sigma_{Y}^{2} + \\sigma_{V}^{2}$.\nSo, the model for the observed outcome is\n$$\nY_{i}^{*} = \\beta_{0} + \\beta_{1} X_{i} + U_{i}, \\quad U_{i} \\sim \\mathcal{N}(0, \\sigma_{Y^{*}}^{2})\n$$\nThis demonstrates that additive measurement error in the outcome does not induce bias in the parameter estimates but inflates the residual variance.\n\nThe two-sample $t$-test is used to test the null hypothesis $H_{0}: \\beta_{1}=0$. This is equivalent to an ordinary least squares regression of $Y_i^*$ on $X_i$. The OLS estimator for $\\beta_1$ is $\\hat{\\beta}_1 = \\bar{Y}_1^* - \\bar{Y}_0^*$, where $\\bar{Y}_1^*$ and $\\bar{Y}_0^*$ are the sample means of the observed outcomes in the treatment ($X_i=1$) and control ($X_i=0$) arms, respectively. The trial is balanced with $n/2$ subjects per arm.\n\nUnder the alternative hypothesis $H_A: \\beta_1 = \\Delta$, the expected value of the estimator is:\n$$\nE[\\hat{\\beta}_1] = E[\\bar{Y}_1^* - \\bar{Y}_0^*] = E[\\bar{Y}_1^*] - E[\\bar{Y}_0^*]\n$$\nThe expected value of the outcome in the treatment group is $E[Y_i^* | X_i=1] = \\beta_0 + \\Delta$, and in the control group is $E[Y_i^* | X_i=0] = \\beta_0$. Thus, $E[\\hat{\\beta}_1] = (\\beta_0 + \\Delta) - \\beta_0 = \\Delta$.\n\nThe variance of the estimator is:\n$$\n\\operatorname{Var}(\\hat{\\beta}_1) = \\operatorname{Var}(\\bar{Y}_1^* - \\bar{Y}_0^*) = \\operatorname{Var}(\\bar{Y}_1^*) + \\operatorname{Var}(\\bar{Y}_0^*)\n$$\nThe variance of the sample means are $\\operatorname{Var}(\\bar{Y}_1^*) = \\frac{\\sigma_{Y^{*}}^{2}}{n/2}$ and $\\operatorname{Var}(\\bar{Y}_0^*) = \\frac{\\sigma_{Y^{*}}^{2}}{n/2}$. Therefore,\n$$\n\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{\\sigma_{Y^{*}}^{2}}{n/2} + \\frac{\\sigma_{Y^{*}}^{2}}{n/2} = \\frac{2 \\sigma_{Y^{*}}^{2}}{n/2} = \\frac{4 \\sigma_{Y^{*}}^{2}}{n}\n$$\nThe $t$-statistic is $T = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}$. Under the alternative hypothesis, this statistic follows a noncentral $t$-distribution. The noncentrality parameter (NCP), which we denote as $\\delta^*$, is given by the true mean of the numerator divided by its true standard deviation:\n$$\n\\delta^{*} = \\frac{E[\\hat{\\beta}_1]}{\\sqrt{\\operatorname{Var}(\\hat{\\beta}_1)}} = \\frac{\\Delta}{\\sqrt{\\frac{4 \\sigma_{Y^{*}}^{2}}{n}}} = \\frac{\\Delta \\sqrt{n}}{2 \\sigma_{Y^{*}}}\n$$\nSubstituting $\\sigma_{Y^{*}}^{2} = \\sigma_{Y}^{2} + \\sigma_{V}^{2}$, we arrive at the expression for the noncentrality parameter for the test using the observed data $Y^*$:\n$$\n\\delta^{*} = \\frac{\\Delta \\sqrt{n}}{2 \\sqrt{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}}\n$$\nThis concludes the first part of the derivation.\n\nFor the second part, we consider the hypothetical scenario with error-free outcomes $Y_i$. In this case, the measurement error $V_i$ is zero, so $\\sigma_V^2=0$. The variance of the outcome is simply $\\operatorname{Var}(Y_i | X_i) = \\sigma_Y^2$. The noncentrality parameter for a test on $Y_i$ with sample size $n$, which we denote $\\delta$, is obtained by setting $\\sigma_V^2=0$ in the expression for $\\delta^*$:\n$$\n\\delta = \\frac{\\Delta \\sqrt{n}}{2 \\sqrt{\\sigma_{Y}^{2}}}\n$$\nThe problem requires us to find the sample size $n^*$ needed for the study with measurement error (using outcome $Y^*$) to achieve the same noncentrality parameter $\\delta$ as the error-free study with sample size $n$. We set the NCP for the observed-data study with sample size $n^*$ equal to $\\delta$:\n$$\n\\frac{\\Delta \\sqrt{n^{*}}}{2 \\sqrt{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}} = \\frac{\\Delta \\sqrt{n}}{2 \\sqrt{\\sigma_{Y}^{2}}}\n$$\nSince $\\Delta \\neq 0$, we can cancel $\\Delta/2$ from both sides:\n$$\n\\frac{\\sqrt{n^{*}}}{\\sqrt{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}} = \\frac{\\sqrt{n}}{\\sqrt{\\sigma_{Y}^{2}}}\n$$\nSquaring both sides of the equation gives:\n$$\n\\frac{n^{*}}{\\sigma_{Y}^{2} + \\sigma_{V}^{2}} = \\frac{n}{\\sigma_{Y}^{2}}\n$$\nWe can now solve for the required sample size $n^{*}$:\n$$\nn^{*} = n \\left( \\frac{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}{\\sigma_{Y}^{2}} \\right)\n$$\nThe sample size inflation factor is $k = n^{*}/n$. Dividing the above equation by $n$ yields the final expression for $k$:\n$$\nk = \\frac{n^{*}}{n} = \\frac{\\sigma_{Y}^{2} + \\sigma_{V}^{2}}{\\sigma_{Y}^{2}} = 1 + \\frac{\\sigma_{V}^{2}}{\\sigma_{Y}^{2}}\n$$\nThis factor represents the proportional increase in sample size required to offset the loss of statistical power due to the additive measurement error in the outcome variable.",
            "answer": "$$ \\boxed{1 + \\frac{\\sigma_V^2}{\\sigma_Y^2}} $$"
        }
    ]
}