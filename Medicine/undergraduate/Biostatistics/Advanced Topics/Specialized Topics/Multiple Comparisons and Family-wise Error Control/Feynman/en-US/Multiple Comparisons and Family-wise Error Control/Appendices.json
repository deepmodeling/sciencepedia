{
    "hands_on_practices": [
        {
            "introduction": "This first exercise will ground your understanding of Family-Wise Error Rate (FWER) control using the foundational Bonferroni correction. Starting from the basic principles of probability, you will see how the simple union bound leads to a robust method for preventing an inflation of Type I errors when performing multiple tests. This practice  is essential for building a solid base before exploring more powerful, and more complex, procedures.",
            "id": "4930328",
            "problem": "A translational oncology study evaluates whether five pre-specified serum biomarkers show differential levels between cases and controls. For each biomarker, a hypothesis test yields an unadjusted $p$-value. The family of tests is to be interpreted jointly, and the investigator seeks to control the Family-Wise Error Rate (FWER) at level $\\alpha=0.05$ using the Bonferroni principle.\n\nYou are given $m=5$ unadjusted $p$-values in the order of biomarkers $1$ through $5$: $(0.003, 0.01, 0.02, 0.04, 0.06)$.\n\nStarting from the definition of Family-Wise Error Rate (FWER) as the probability of making at least one type I error among the $m$ null hypotheses, use basic probability principles to justify a Bonferroni decision rule that controls FWER at level $\\alpha$, and determine which null hypotheses are rejected. Then compute the Bonferroni-adjusted $p$-values for each biomarker.\n\nReport your final answer as a single row matrix with $10$ entries: the first five entries are the Bonferroni-adjusted $p$-values for biomarkers $1$ through $5$ (in the order given), and the next five entries are indicators of rejection at family-wise level $\\alpha$ using the Bonferroni rule, with $1$ for “reject” and $0$ for “do not reject.” No rounding is required.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of biostatistics, specifically multiple hypothesis testing. It is well-posed, with all necessary data provided, and objective, using clear and precise language.\n\nThe task is to justify the Bonferroni correction starting from the definition of the Family-Wise Error Rate (FWER), apply it to a given set of $p$-values to determine which null hypotheses are rejected, and compute the corresponding Bonferroni-adjusted $p$-values.\n\nLet there be $m$ null hypotheses, $H_{0,1}, H_{0,2}, \\dots, H_{0,m}$. For each hypothesis test $i$, we obtain a $p$-value, $p_i$. The Family-Wise Error Rate (FWER) is defined as the probability of making at least one Type I error (rejecting a true null hypothesis) among the entire family of $m$ tests. Let $E_i$ be the event of committing a Type I error for the $i$-th hypothesis. The FWER is the probability of the union of these events:\n$$\n\\text{FWER} = P\\left(\\bigcup_{i \\in I_0} E_i\\right)\n$$\nwhere $I_0$ is the index set of the true null hypotheses. To control the FWER, we consider the worst-case scenario where all null hypotheses are true, i.e., $I_0 = \\{1, 2, \\dots, m\\}$. In this case,\n$$\n\\text{FWER} = P\\left(\\bigcup_{i=1}^{m} E_i\\right)\n$$\nBy Boole's inequality (also known as the union bound), the probability of a union of events is less than or equal to the sum of their individual probabilities:\n$$\nP\\left(\\bigcup_{i=1}^{m} E_i\\right) \\leq \\sum_{i=1}^{m} P(E_i)\n$$\nA Type I error for test $i$ occurs if we reject $H_{0,i}$ when it is true. The decision rule for a single test is to reject $H_{0,i}$ if its $p$-value $p_i$ is less than or equal to some significance level, let's call it $\\alpha_{ind}$ for \"individual\". The probability of this event, given that $H_{0,i}$ is true, is the definition of the significance level of the test: $P(E_i) = P(p_i \\leq \\alpha_{ind} | H_{0,i} \\text{ is true}) = \\alpha_{ind}$.\n\nSubstituting this into the inequality, we get:\n$$\n\\text{FWER} \\leq \\sum_{i=1}^{m} \\alpha_{ind}\n$$\nThe Bonferroni method uses the same individual significance level for all tests, so $\\alpha_{ind}$ is constant for all $i=1, \\dots, m$. The inequality simplifies to:\n$$\n\\text{FWER} \\leq m \\cdot \\alpha_{ind}\n$$\nTo control the FWER at a pre-specified level $\\alpha$, we must ensure that the right-hand side of the inequality is no greater than $\\alpha$. We set $m \\cdot \\alpha_{ind} \\leq \\alpha$, which yields $\\alpha_{ind} \\leq \\frac{\\alpha}{m}$. To be as powerful as possible (i.e., to use the largest possible individual significance level), we choose $\\alpha_{ind} = \\frac{\\alpha}{m}$.\n\nThis gives the Bonferroni decision rule: reject the null hypothesis $H_{0,i}$ if its unadjusted $p$-value $p_i$ is less than or equal to the Bonferroni-corrected significance threshold $\\frac{\\alpha}{m}$. This procedure guarantees that $\\text{FWER} \\leq \\alpha$.\n\nNow, we apply this rule to the given problem.\nThe number of hypothesis tests is $m=5$.\nThe desired family-wise error rate is $\\alpha=0.05$.\nThe Bonferroni-corrected significance threshold is $\\alpha_{adj} = \\frac{\\alpha}{m} = \\frac{0.05}{5} = 0.01$.\n\nThe unadjusted $p$-values are given as $p_1=0.003$, $p_2=0.01$, $p_3=0.02$, $p_4=0.04$, and $p_5=0.06$. We compare each $p$-value to $\\alpha_{adj} = 0.01$.\n\n1.  For biomarker 1: $p_1 = 0.003$. Since $0.003 \\leq 0.01$, we reject the null hypothesis $H_{0,1}$.\n2.  For biomarker 2: $p_2 = 0.01$. Since $0.01 \\leq 0.01$, we reject the null hypothesis $H_{0,2}$.\n3.  For biomarker 3: $p_3 = 0.02$. Since $0.02 > 0.01$, we do not reject the null hypothesis $H_{0,3}$.\n4.  For biomarker 4: $p_4 = 0.04$. Since $0.04 > 0.01$, we do not reject the null hypothesis $H_{0,4}$.\n5.  For biomarker 5: $p_5 = 0.06$. Since $0.06 > 0.01$, we do not reject the null hypothesis $H_{0,5}$.\n\nThe rejection indicators ($1$ for reject, $0$ for do not reject) for biomarkers $1$ through $5$ are $(1, 1, 0, 0, 0)$.\n\nNext, we compute the Bonferroni-adjusted $p$-values, denoted $\\tilde{p}_i$. The adjusted $p$-value is defined such that one can reject $H_{0,i}$ if $\\tilde{p}_i \\leq \\alpha$. The original rejection rule is $p_i \\leq \\frac{\\alpha}{m}$. Rearranging this gives $m \\cdot p_i \\leq \\alpha$. Thus, the adjusted $p$-value $\\tilde{p}_i$ is $m \\cdot p_i$. Since a probability cannot exceed $1$, the formal definition is:\n$$\n\\tilde{p}_i = \\min(m \\cdot p_i, 1)\n$$\nWe compute this for each biomarker with $m=5$:\n\n1.  $\\tilde{p}_1 = \\min(5 \\times 0.003, 1) = \\min(0.015, 1) = 0.015$.\n2.  $\\tilde{p}_2 = \\min(5 \\times 0.01, 1) = \\min(0.05, 1) = 0.05$.\n3.  $\\tilde{p}_3 = \\min(5 \\times 0.02, 1) = \\min(0.10, 1) = 0.1$.\n4.  $\\tilde{p}_4 = \\min(5 \\times 0.04, 1) = \\min(0.20, 1) = 0.2$.\n5.  $\\tilde{p}_5 = \\min(5 \\times 0.06, 1) = \\min(0.30, 1) = 0.3$.\n\nThe adjusted $p$-values for biomarkers $1$ through $5$ are $(0.015, 0.05, 0.1, 0.2, 0.3)$. We can verify the rejection decisions using these adjusted $p$-values against $\\alpha=0.05$:\n- $\\tilde{p}_1 = 0.015 \\leq 0.05 \\implies$ Reject.\n- $\\tilde{p}_2 = 0.05 \\leq 0.05 \\implies$ Reject.\n- $\\tilde{p}_3 = 0.1 > 0.05 \\implies$ Do not reject.\n- $\\tilde{p}_4 = 0.2 > 0.05 \\implies$ Do not reject.\n- $\\tilde{p}_5 = 0.3 > 0.05 \\implies$ Do not reject.\nThis confirms the previous findings.\n\nThe final answer requires a single row matrix containing the five adjusted $p$-values followed by the five rejection indicators.\nAdjusted $p$-values: $(0.015, 0.05, 0.1, 0.2, 0.3)$.\nRejection indicators: $(1, 1, 0, 0, 0)$.\n\nThe combined row matrix is $(0.015, 0.05, 0.1, 0.2, 0.3, 1, 1, 0, 0, 0)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.015  0.05  0.1  0.2  0.3  1  1  0  0  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Many statistical procedures rely on specific assumptions, but what happens when those assumptions don't hold? This thought experiment explores the critical role of independence in multiple testing by constructing a scenario of perfectly negative dependence between two $p$-values. By analyzing the breakdown of the Šidák correction while the Bonferroni method remains valid, you will gain a crucial insight into the robustness of different FWER control strategies .",
            "id": "4930323",
            "problem": "A biostatistician tests $m=2$ null hypotheses, both true, producing valid one-test $p$-values whose null distributions are exactly $\\mathrm{Uniform}(0,1)$ marginally. To intentionally induce negative dependence across tests while preserving marginal validity, the biostatistician constructs the pair by drawing a single random variable $U \\sim \\mathrm{Uniform}(0,1)$ under the global null and setting $p_{1} = U$ and $p_{2} = 1 - U$. This construction yields perfectly negatively dependent $p$-values with correct $\\mathrm{Uniform}(0,1)$ marginals.\n\nStarting from first principles—namely, the definition of Family-Wise Error Rate (FWER) as the probability of at least one Type I error under the global null and the union bound (Bonferroni inequality) that for any events $A_{1}, A_{2}$ satisfies $\\mathbb{P}(A_{1} \\cup A_{2}) \\leq \\mathbb{P}(A_{1}) + \\mathbb{P}(A_{2})$—answer the following:\n\n1. Using only these core definitions, derive the Family-Wise Error Rate (FWER) of the Bonferroni procedure in terms of the target family-wise level $\\alpha$ when the common rejection threshold is set to $t_{\\mathrm{B}} = \\alpha/2$ for $m=2$ tests.\n\n2. Under the independence principle that defines the Šidák adjustment, determine the threshold $t_{\\mathrm{S}}$ that would control FWER at level $\\alpha$ when test statistics (or $p$-values) are independent. Express $t_{\\mathrm{S}}$ in terms of $\\alpha$ for $m=2$.\n\n3. In the negatively dependent scenario described above, compute the actual FWER achieved by using the Šidák threshold $t_{\\mathrm{S}}$ you derived in part 2, as a function of $\\alpha$. Your derivation should rely only on the definitions given and the specified joint construction of $(p_{1}, p_{2})$.\n\n4. Briefly explain the mechanism by which negative dependence in $(p_{1}, p_{2})$ violates the independence assumption underlying the Šidák equality for $\\mathbb{P}(\\text{no rejection})$, and why this can make the Šidák adjustment anti-conservative while Bonferroni continues to control FWER.\n\nCompute the numerical value of the Šidák FWER for $\\alpha = 0.05$ in this negatively dependent scenario, and report your answer as a decimal rounded to four significant figures.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard, albeit stylized, scenario in the theory of multiple comparisons to test the understanding of fundamental principles. All necessary information is provided. Therefore, the problem is valid, and a solution can be formulated.\n\nThe solution will proceed by addressing the four parts of the question in sequence.\n\nLet $m=2$ be the number of null hypotheses, both of which are true. The two $p$-values, $p_1$ and $p_2$, are constructed from a single random variable $U \\sim \\mathrm{Uniform}(0,1)$ as $p_1 = U$ and $p_2 = 1-U$. This ensures that marginally, $p_1 \\sim \\mathrm{Uniform}(0,1)$ and $p_2 \\sim \\mathrm{Uniform}(0,1)$, but they are perfectly negatively dependent. The target Family-Wise Error Rate (FWER) is $\\alpha$.\n\n**1. Family-Wise Error Rate (FWER) of the Bonferroni Procedure**\n\nThe FWER is defined as the probability of making at least one Type I error. A Type I error for test $i$ occurs if we reject the null hypothesis $H_i$ when it is true. In this problem, both nulls are true. Rejection occurs if $p_i \\le t$, where $t$ is the rejection threshold.\n\n$\\mathrm{FWER} = \\mathbb{P}(\\text{reject at least one } H_i) = \\mathbb{P}(p_1 \\le t \\cup p_2 \\le t)$\n\nThe Bonferroni procedure sets a common rejection threshold $t_{\\mathrm{B}} = \\alpha/m$. For $m=2$, this threshold is $t_{\\mathrm{B}} = \\alpha/2$.\n\nThe problem asks for a derivation using the union bound (Bonferroni inequality), which states that for any two events $A_1$ and $A_2$, $\\mathbb{P}(A_1 \\cup A_2) \\le \\mathbb{P}(A_1) + \\mathbb{P}(A_2)$.\nLet $A_i$ be the event that test $i$ is rejected, i.e., $A_i = \\{p_i \\le t_{\\mathrm{B}}\\}$.\n\nApplying the union bound to the FWER definition:\n$\\mathrm{FWER} = \\mathbb{P}(p_1 \\le \\alpha/2 \\cup p_2 \\le \\alpha/2) \\le \\mathbb{P}(p_1 \\le \\alpha/2) + \\mathbb{P}(p_2 \\le \\alpha/2)$\n\nSince both $p$-values are constructed to have a marginal $\\mathrm{Uniform}(0,1)$ distribution, for any value $x \\in [0,1]$, we have $\\mathbb{P}(p_i \\le x) = x$.\nSubstituting this into the inequality:\n$\\mathrm{FWER} \\le \\frac{\\alpha}{2} + \\frac{\\alpha}{2} = \\alpha$\n\nThis derivation demonstrates that the Bonferroni procedure controls the FWER at level $\\alpha$ (i.e., $\\mathrm{FWER} \\le \\alpha$) by design, as the inequality holds regardless of the dependence structure between the $p$-values. It is important to note that for the specific negative dependence given, the actual FWER is exactly $\\alpha$. The events $\\{p_1 \\le \\alpha/2\\}$ and $\\{p_2 \\le \\alpha/2\\}$ become $\\{U \\le \\alpha/2\\}$ and $\\{1-U \\le \\alpha/2\\} \\equiv \\{U \\ge 1-\\alpha/2\\}$. For typical $\\alpha$ (e.g., $\\alpha  1$), these events are mutually exclusive, so the probability of their union is the sum of their probabilities, yielding an exact FWER of $\\alpha/2 + \\alpha/2 = \\alpha$.\n\n**2. Šidák Adjustment Threshold**\n\nThe Šidák adjustment is derived under the assumption that the test statistics (and thus the $p$-values) are independent. The goal is to control the FWER at level $\\alpha$.\n$\\mathrm{FWER} = 1 - \\mathbb{P}(\\text{no rejections}) = \\alpha$\n\nThe event of \"no rejections\" is that all $p$-values are greater than the threshold $t_{\\mathrm{S}}$: $\\mathbb{P}(p_1  t_{\\mathrm{S}} \\text{ and } p_2  t_{\\mathrm{S}} \\text{ and } \\dots \\text{ and } p_m  t_{\\mathrm{S}})$.\nUnder the independence assumption, this probability becomes a product:\n$\\mathbb{P}(\\text{no rejections}) = \\prod_{i=1}^{m} \\mathbb{P}(p_i  t_{\\mathrm{S}})$\n\nFor $m=2$ and $p_i \\sim \\mathrm{Uniform}(0,1)$, we have $\\mathbb{P}(p_i  t_{\\mathrm{S}}) = 1 - t_{\\mathrm{S}}$.\nSo, we set the probability of no rejections to achieve the desired FWER:\n$(1 - t_{\\mathrm{S}})^{2} = 1 - \\alpha$\n\nSolving for $t_{\\mathrm{S}}$:\n$1 - t_{\\mathrm{S}} = \\sqrt{1 - \\alpha}$\n$t_{\\mathrm{S}} = 1 - \\sqrt{1 - \\alpha}$\n\nThis is the Šidák threshold for $m=2$ independent tests.\n\n**3. Actual FWER using the Šidák Threshold in the Negatively Dependent Scenario**\n\nWe now compute the actual FWER when using the threshold $t_{\\mathrm{S}} = 1 - \\sqrt{1 - \\alpha}$ with the given negatively dependent $p$-values $p_1 = U$ and $p_2 = 1-U$.\n\nThe actual FWER is:\n$\\mathrm{FWER}_{\\text{actual}} = \\mathbb{P}(p_1 \\le t_{\\mathrm{S}} \\cup p_2 \\le t_{\\mathrm{S}})$\n\nSubstituting the definitions of $p_1$ and $p_2$:\n$\\mathrm{FWER}_{\\text{actual}} = \\mathbb{P}(U \\le t_{\\mathrm{S}} \\cup 1-U \\le t_{\\mathrm{S}})$\nThe second event, $1-U \\le t_{\\mathrm{S}}$, is equivalent to $U \\ge 1-t_{\\mathrm{S}}$.\n$\\mathrm{FWER}_{\\text{actual}} = \\mathbb{P}(U \\le t_{\\mathrm{S}} \\cup U \\ge 1-t_{\\mathrm{S}})$\n\nTo evaluate this probability, we must check if the two events are mutually exclusive. The events are disjoint if $t_{\\mathrm{S}}  1 - t_{\\mathrm{S}}$, which simplifies to $2t_{\\mathrm{S}}  1$ or $t_{\\mathrm{S}}  1/2$.\nSubstituting the expression for $t_{\\mathrm{S}}$: $1 - \\sqrt{1-\\alpha}  1/2 \\implies 1/2  \\sqrt{1-\\alpha} \\implies 1/4  1-\\alpha \\implies \\alpha  3/4$. For any conventional $\\alpha$ (e.g., $\\alpha=0.05$), this condition holds.\nSince the events are mutually exclusive, the probability of their union is the sum of their individual probabilities:\n$\\mathrm{FWER}_{\\text{actual}} = \\mathbb{P}(U \\le t_{\\mathrm{S}}) + \\mathbb{P}(U \\ge 1-t_{\\mathrm{S}})$\n\nSince $U \\sim \\mathrm{Uniform}(0,1)$:\n$\\mathbb{P}(U \\le t_{\\mathrm{S}}) = t_{\\mathrm{S}}$\n$\\mathbb{P}(U \\ge 1-t_{\\mathrm{S}}) = 1 - (1-t_{\\mathrm{S}}) = t_{\\mathrm{S}}$\n\nTherefore, the actual FWER is:\n$\\mathrm{FWER}_{\\text{actual}} = t_{\\mathrm{S}} + t_{\\mathrm{S}} = 2 t_{\\mathrm{S}} = 2(1 - \\sqrt{1-\\alpha})$\n\n**4. Explanation of Šidák Procedure Failure and Bonferroni Robustness**\n\nThe Šidák adjustment's derivation critically relies on the equality $\\mathbb{P}(\\text{no rejections}) = \\prod_{i=1}^m \\mathbb{P}(p_i > t_S)$, which is only valid if the events $\\{p_i > t_S\\}$ are mutually independent.\n\nIn the given scenario, the events are $\\{p_1 > t_S\\}$ and $\\{p_2 > t_S\\}$, which are $\\{U > t_S\\}$ and $\\{1-U > t_S\\} \\equiv \\{U  1-t_S\\}$. These events are negatively dependent. The actual probability of no rejection is $\\mathbb{P}(p_1 > t_S \\text{ and } p_2 > t_S) = \\mathbb{P}(t_S  U  1-t_S)$. For $U \\sim \\mathrm{Uniform}(0,1)$, this probability is the length of the interval, $(1-t_S) - t_S = 1 - 2t_S$, assuming $t_S  1/2$.\n\nThe probability of no rejection assumed under independence is $(\\mathbb{P}(p_1 > t_S))^2 = (1-t_S)^2 = 1 - 2t_S + t_S^2$.\nComparing the actual and assumed probabilities of no rejection:\n$1 - 2t_S  1 - 2t_S + t_S^2$ for any $t_S > 0$.\n\nThis means $\\mathbb{P}(\\text{no rejections})_{\\text{actual}}  \\mathbb{P}(\\text{no rejections})_{\\text{independent}}$. The negative dependence makes it *less* likely for both tests to be non-significant simultaneously compared to the independent case. Consequently, the actual FWER, which is $1 - \\mathbb{P}(\\text{no rejections})_{\\text{actual}}$, is *greater* than the FWER under independence, $1 - \\mathbb{P}(\\text{no rejections})_{\\text{independent}} = \\alpha$. This makes the Šidák procedure anti-conservative (i.e., the true FWER exceeds the nominal level $\\alpha$).\n\nIn contrast, the Bonferroni procedure's guarantee, $\\mathrm{FWER} \\le \\alpha$, is based on the Bonferroni inequality (union bound), $\\mathbb{P}(\\cup A_i) \\le \\sum \\mathbb{P}(A_i)$. This inequality holds universally, without any assumptions about the dependence between the events $A_i$. Therefore, Bonferroni correction remains valid (i.e., controls FWER at level $\\alpha$) for any dependence structure, including negative dependence.\n\n**Numerical Calculation**\n\nFinally, we compute the numerical value for the actual FWER achieved by the Šidák threshold when $\\alpha = 0.05$ in this negatively dependent scenario.\nUsing the formula derived in part 3:\n$\\mathrm{FWER}_{\\text{actual}} = 2(1 - \\sqrt{1-\\alpha})$\nSubstituting $\\alpha = 0.05$:\n$\\mathrm{FWER}_{\\text{actual}} = 2(1 - \\sqrt{1 - 0.05}) = 2(1 - \\sqrt{0.95})$\n$\\sqrt{0.95} \\approx 0.97467943$\n$\\mathrm{FWER}_{\\text{actual}} \\approx 2(1 - 0.97467943) = 2(0.02532057) \\approx 0.05064114$\nRounding to four significant figures, the result is $0.05064$.",
            "answer": "$$\n\\boxed{0.05064}\n$$"
        },
        {
            "introduction": "Modern biostatistics often leverages computational power to solve problems where theoretical assumptions are hard to meet. This exercise introduces the max-statistic permutation test, a powerful resampling-based method for controlling FWER without relying on a specific theoretical distribution for your test statistics. You will derive the estimator for permutation-adjusted $p$-values and apply it, gaining hands-on experience with a flexible and widely used technique in contemporary research .",
            "id": "4930335",
            "problem": "You are analyzing $m$ simultaneous statistical tests (null hypotheses $H_1,\\dots,H_m$) with observed test statistics $t_1,\\dots,t_m$. To control the Family-Wise Error Rate (FWER), you will use a max-statistic permutation approach. Under the complete null hypothesis and assuming exchangeability of the test statistics under permutation of labels, the distribution of the maximum test statistic across all $m$ tests, denoted $M=\\max\\{T_1,\\dots,T_m\\}$, can be approximated by performing $B$ label permutations and recording the observed maxima $M^{(1)},\\dots,M^{(B)}$.\n\nFundamental base definitions to use:\n- The Family-Wise Error Rate (FWER) is defined as the probability of making at least one false rejection among $H_1,\\dots,H_m$: $FWER=\\mathbb{P}(\\text{at least one false rejection})$.\n- Under the complete null and exchangeability, the permutation distribution of the maximum test statistic $M$ is a valid approximation for the null distribution of the maximum.\n- For any test $j\\in\\{1,\\dots,m\\}$, the adjusted $p$-value is defined as the probability, under the null distribution of $M$, that $M$ is at least as extreme as the observed $t_j$.\n\nTasks:\n1. Starting from the definition of an adjusted $p$-value for test $j$ as the null tail probability of the maximum statistic at the threshold $t_j$, derive a computable estimator based on $B$ equally likely permutation maxima $M^{(1)},\\dots,M^{(B)}$. Your derivation must begin from the definition of probability as an expectation of an indicator function and the assumption of exchangeability under the complete null hypothesis. The estimator must be expressed without using percentages; use decimal values in $[0,1]$.\n2. Using your derived estimator, implement an algorithm that, for each $j\\in\\{1,\\dots,m\\}$, computes the adjusted $p$-value by comparing the observed $t_j$ to the permutation maxima $M^{(1)},\\dots,M^{(B)}$. Ties must be handled inclusively, meaning values equal to the threshold count as being at least as extreme.\n3. Apply your algorithm to the following test suite. Each test case provides $(m, t_1,\\dots,t_m)$ and $(B, M^{(1)},\\dots,M^{(B)})$:\n   - Test case 1 (general case): $m=4$, $(t_1,t_2,t_3,t_4)=(2.1,1.5,3.0,0.8)$; $B=10$, $(M^{(1)},\\dots,M^{(10)})=(1.9,2.2,3.1,2.8,2.4,1.7,2.0,3.0,2.6,2.3)$.\n   - Test case 2 (tie-handling): $m=3$, $(t_1,t_2,t_3)=(2.3,2.0,3.0)$; $B=3$, $(M^{(1)},M^{(2)},M^{(3)})=(2.3,2.3,2.3)$.\n   - Test case 3 (boundary $B=1$): $m=3$, $(t_1,t_2,t_3)=(1.0,2.5,3.0)$; $B=1$, $(M^{(1)})=(2.5)$.\n   - Test case 4 (negative values and edge extremes): $m=4$, $(t_1,t_2,t_3,t_4)=(-1.0,-0.3,0.0,0.5)$; $B=4$, $(M^{(1)},\\dots,M^{(4)})=(-0.5,-0.2,0.0,-0.1)$.\n4. The final output must be a single line containing the four lists of adjusted $p$-values, one list per test case, expressed as decimals, and formatted as a comma-separated list enclosed in square brackets where each inner list is itself enclosed in square brackets. For example, the output should look like $[[a_1,\\dots,a_m],[b_1,\\dots,b_m],[c_1,\\dots,c_m],[d_1,\\dots,d_m]]$, where each element is a real number in $[0,1]$.\n\nYour program must be self-contained, require no input, and print only the final formatted results as described.",
            "solution": "The problem statement has been validated and is determined to be sound, well-posed, and objective. It presents a standard task in computational statistics concerning the control of the Family-Wise Error Rate (FWER) using a permutation-based method. The provided definitions and data are consistent and sufficient for deriving and implementing a solution.\n\nThe solution proceeds in two parts. First, I will derive the computational formula for the adjusted $p$-value from fundamental statistical principles. Second, I will describe the algorithm to apply this formula to the provided data.\n\n### Part 1: Derivation of the Adjusted $p$-value Estimator\n\nThe objective is to derive a computable estimator for the adjusted $p$-value, $p_j^{\\text{adj}}$, associated with the observed test statistic $t_j$ for null hypothesis $H_j$.\n\nThe problem defines the adjusted $p$-value for test $j$ as the probability, under the complete null hypothesis $H_0^C$, that the maximum test statistic, $M = \\max\\{T_1, \\dots, T_m\\}$, is at least as extreme as the observed value $t_j$. Formally, this is written as:\n$$\np_j^{\\text{adj}} = \\mathbb{P}(M \\ge t_j \\mid H_0^C)\n$$\nHere, $T_1, \\dots, T_m$ are the random variables corresponding to the test statistics, and their joint distribution is considered under the complete null, where all $H_j$ are true.\n\nProbability can be expressed as the expectation of an indicator function. Let $\\mathbb{I}(\\cdot)$ be the indicator function, which evaluates to $1$ if its argument is true and $0$ otherwise. The definition of the adjusted $p$-value can then be rewritten as:\n$$\np_j^{\\text{adj}} = \\mathbb{E}[\\mathbb{I}(M \\ge t_j) \\mid H_0^C]\n$$\nwhere the expectation $\\mathbb{E}[\\cdot]$ is taken over the distribution of $M$ under the complete null hypothesis.\n\nIn practice, the true null distribution of $M$ is often unknown. The problem states we are to use a permutation-based approach to approximate this distribution. By performing $B$ permutations of the sample labels and re-computing the maximum test statistic for each permutation, we obtain a set of $B$ values, $\\{M^{(1)}, M^{(2)}, \\dots, M^{(B)}\\}$. Under the assumptions of the complete null and exchangeability, each $M^{(b)}$ is a draw from the true null distribution of $M$. The problem states that these are equally likely.\n\nThe principle of Monte Carlo estimation allows us to approximate an expectation by the sample mean of a function of random draws from the distribution. Therefore, we can estimate the expectation $\\mathbb{E}[\\mathbb{I}(M \\ge t_j) \\mid H_0^C]$ by taking the average of the indicator function evaluated over our $B$ permutation samples. Let $\\hat{p}_j^{\\text{adj}}$ be the estimator for $p_j^{\\text{adj}}$.\n$$\n\\hat{p}_j^{\\text{adj}} = \\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{I}(M^{(b)} \\ge t_j)\n$$\nThe summation term, $\\sum_{b=1}^{B} \\mathbb{I}(M^{(b)} \\ge t_j)$, is precisely the count of the number of permutation maxima $M^{(b)}$ that are greater than or equal to the observed test statistic $t_j$. The instruction to handle ties inclusively (\"at least as extreme\") is naturally captured by the greater-than-or-equal-to operator, $\\ge$.\n\nThus, the final computable estimator for the adjusted $p$-value is:\n$$\n\\hat{p}_j^{\\text{adj}} = \\frac{\\text{Number of } M^{(b)} \\text{ such that } M^{(b)} \\ge t_j}{B}\n$$\nThis formula provides a direct, non-parametric estimate of the adjusted $p$-value based on the empirical distribution of the maximum statistic obtained from permutations. The values are decimals in the range $[0, 1]$, as required.\n\n### Part 2: Algorithm and Application\n\nThe algorithm to compute the adjusted $p$-values for a given test case follows directly from the derived estimator.\n\nFor each test case, we are given:\n- The number of tests, $m$.\n- A vector of observed test statistics, $t = (t_1, t_2, \\dots, t_m)$.\n- The number of permutations, $B$.\n- A vector of permutation maxima, $M_{\\text{perm}} = (M^{(1)}, M^{(2)}, \\dots, M^{(B)})$.\n\nThe algorithm is as follows:\n1. Initialize an empty list, `p_adjusted`, to store the results for the $m$ tests.\n2. For each observed statistic $t_j$ where $j$ ranges from $1$ to $m$:\n    a. Initialize a counter, `count_ge`, to $0$.\n    b. Iterate through each permutation maximum $M^{(b)}$ in the vector $M_{\\text{perm}}$, where $b$ ranges from $1$ to $B$.\n    c. In each iteration, compare $M^{(b)}$ with $t_j$. If $M^{(b)} \\ge t_j$, increment `count_ge` by $1$.\n    d. After iterating through all $B$ permutation maxima, calculate the adjusted $p$-value for test $j$ as $\\hat{p}_j^{\\text{adj}} = \\frac{\\text{count\\_ge}}{B}$.\n    e. Append $\\hat{p}_j^{\\text{adj}}$ to the `p_adjusted` list.\n3. After processing all $m$ statistics, the list `p_adjusted` contains the required set of adjusted $p$-values for the test case.\n\nThis procedure is repeated for each of the four test cases provided in the problem statement. The implementation will use numerical libraries for efficient vectorized computation of the comparison and counting steps, which is computationally equivalent to the described iterative algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes adjusted p-values using a max-statistic permutation approach\n    for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: ((t_1..t_m), (M_1..M_B))\n    test_cases = [\n        # Test case 1 (general case)\n        (\n            (2.1, 1.5, 3.0, 0.8),\n            (1.9, 2.2, 3.1, 2.8, 2.4, 1.7, 2.0, 3.0, 2.6, 2.3)\n        ),\n        # Test case 2 (tie-handling)\n        (\n            (2.3, 2.0, 3.0),\n            (2.3, 2.3, 2.3)\n        ),\n        # Test case 3 (boundary B=1)\n        (\n            (1.0, 2.5, 3.0),\n            (2.5,)\n        ),\n        # Test case 4 (negative values and edge extremes)\n        (\n            (-1.0, -0.3, 0.0, 0.5),\n            (-0.5, -0.2, 0.0, -0.1)\n        )\n    ]\n\n    all_results = []\n    for t_obs_tuple, M_perm_tuple in test_cases:\n        # Convert tuples to numpy arrays for efficient computation\n        t_obs = np.array(t_obs_tuple)\n        M_perm = np.array(M_perm_tuple)\n        \n        # B is the number of permutation maxima\n        B = float(len(M_perm))\n\n        # Ensure B is not zero to avoid division by zero, though problem constraints\n        # imply B = 1.\n        if B == 0:\n            # According to the formula, if B=0, the p-value is undefined.\n            # We can represent this as NaN or handle as an error.\n            # Assuming B=1 based on the problem. \n            # For robustness, we could set p-values to 1.0 or NaN.\n            # Here we just proceed, as problem data has B = 1.\n            pass\n\n        # Reshape t_obs for broadcasting.\n        # t_obs becomes a column vector of shape (m, 1).\n        # M_perm is a row vector of shape (1, B).\n        # The comparison M_perm = t_obs[:, np.newaxis] results in a\n        # boolean matrix of shape (m, B). Each row corresponds to a t_j\n        # and contains the comparison result against all M^(b).\n        # The condition is M^(b) = t_j, which means we compare the row vector M_perm\n        # against the column vector t_obs.\n        \n        # t_obs is (m,), M_perm is (B,). Reshape t_obs to (m, 1).\n        # The comparison M_perm (B,) = t_obs[:, np.newaxis] (m, 1) broadcasts\n        # to a (m, B) boolean array.\n        comparison_matrix = M_perm = t_obs[:, np.newaxis]\n        \n        # Sum the boolean values (True=1, False=0) along axis=1 (across permutations)\n        # to get the count of M^(b) = t_j for each t_j.\n        counts = np.sum(comparison_matrix, axis=1)\n        \n        # Calculate the adjusted p-values by dividing counts by B.\n        adj_p_values = counts / B\n        \n        all_results.append(adj_p_values.tolist())\n\n    # Format the final output string exactly as specified in the problem.\n    # Example: [[a,b,c],[d,e],[f,g,h,i],[j,k]]\n    # 1. Convert each inner list of floats to a string of comma-separated values.\n    # 2. Enclose each of these strings in square brackets.\n    # 3. Join all these bracketed strings with commas.\n    # 4. Enclose the final joined string in square brackets.\n    \n    inner_strings = []\n    for res_list in all_results:\n        inner_strings.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    final_output_string = f\"[{','.join(inner_strings)}]\"\n    \n    # The final print statement must produce only the single-line format.\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}