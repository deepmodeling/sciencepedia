{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex code, it's crucial to understand how resampling methods work at a fundamental level. This exercise provides a hands-on comparison of the jackknife, an intuitive \"leave-one-out\" method, and the bootstrap, its more powerful and flexible successor. By calculating the standard error of the sample median by hand on a small dataset, you will demystify how these techniques estimate statistical variability without relying on complex theoretical formulas. This comparison highlights the core mechanics of resampling and sets the stage for more advanced computational applications .",
            "id": "4948751",
            "problem": "A biostatistics study records a small sample of biomarker levels in a cohort. Let the observed levels be the set $x=\\{3.0,\\,3.5,\\,4.0,\\,6.0,\\,6.5,\\,7.0,\\,9.0\\}$, and define the sample median as the $0.5$ quantile of the empirical distribution of $x$ (for odd sample size, the median is the middle order statistic). Using resampling-based approaches grounded in core definitions, estimate the variability of the sample median and compare jackknife and bootstrap standard errors.\n\n- First, using the leave-one-out resampling definition, compute the jackknife standard error (standard error) of the sample median based on the $n=7$ observations in $x$.\n- Second, consider the following $B=12$ ordinary nonparametric bootstrap resamples (each of size $7$, sampled with replacement from $x$):\n  - $x^{*(1)}=\\{3.0,\\,3.5,\\,4.0,\\,6.0,\\,6.5,\\,7.0,\\,9.0\\}$,\n  - $x^{*(2)}=\\{6.0,\\,6.0,\\,6.5,\\,7.0,\\,9.0,\\,3.5,\\,4.0\\}$,\n  - $x^{*(3)}=\\{9.0,\\,9.0,\\,7.0,\\,6.5,\\,6.0,\\,4.0,\\,3.5\\}$,\n  - $x^{*(4)}=\\{3.0,\\,3.0,\\,3.5,\\,4.0,\\,6.0,\\,6.0,\\,6.5\\}$,\n  - $x^{*(5)}=\\{3.0,\\,3.5,\\,3.5,\\,4.0,\\,4.0,\\,6.0,\\,6.0\\}$,\n  - $x^{*(6)}=\\{4.0,\\,4.0,\\,4.0,\\,6.0,\\,6.0,\\,6.5,\\,7.0\\}$,\n  - $x^{*(7)}=\\{7.0,\\,7.0,\\,6.5,\\,6.5,\\,6.0,\\,6.0,\\,9.0\\}$,\n  - $x^{*(8)}=\\{3.0,\\,3.5,\\,3.5,\\,4.0,\\,4.0,\\,4.0,\\,4.0\\}$,\n  - $x^{*(9)}=\\{9.0,\\,9.0,\\,7.0,\\,7.0,\\,6.5,\\,6.0,\\,6.0\\}$,\n  - $x^{*(10)}=\\{3.0,\\,3.0,\\,3.5,\\,3.5,\\,4.0,\\,4.0,\\,6.0\\}$,\n  - $x^{*(11)}=\\{6.5,\\,6.5,\\,6.5,\\,6.0,\\,6.0,\\,7.0,\\,9.0\\}$,\n  - $x^{*(12)}=\\{3.0,\\,3.0,\\,3.5,\\,4.0,\\,6.0,\\,6.5,\\,9.0\\}$.\n  Compute the bootstrap standard error of the sample median using these $12$ resamples.\n\nFinally, report the ratio $R$ of the jackknife standard error to the bootstrap standard error. Round your answer to four significant figures. Express your final answer as a pure number with no units.",
            "solution": "The user has provided a problem that requires the computation and comparison of jackknife and bootstrap standard errors for the sample median. This is a standard and well-defined problem in computational biostatistics.\n\n### Step 0: Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Givens:**\n1.  Sample data: $x=\\{3.0,\\,3.5,\\,4.0,\\,6.0,\\,6.5,\\,7.0,\\,9.0\\}$\n2.  Sample size: $n=7$\n3.  Definition of sample median for odd $n$: the middle order statistic.\n4.  Task 1: Compute jackknife standard error of the sample median.\n5.  Number of bootstrap resamples: $B=12$.\n6.  Bootstrap resamples $x^{*(1)}, \\dots, x^{*(12)}$ are provided explicitly.\n7.  Task 2: Compute bootstrap standard error of the sample median from the given resamples.\n8.  Task 3: Compute the ratio $R$ of the jackknife standard error to the bootstrap standard error.\n9.  Rounding requirement: The final answer for $R$ must be rounded to four significant figures.\n\n**Validation Criteria:**\n-   **Scientific Grounding**: The problem is grounded in the established statistical theories of jackknife and bootstrap resampling, which are fundamental methods for variance estimation. No scientific principles are violated.\n-   **Well-Posedness**: The problem is well-posed. It provides all necessary data and definitions to compute the required quantities. The solution is unique and stable.\n-   **Objectivity**: The problem is stated in objective, mathematical language with no subjective or ambiguous terms.\n\n**Verdict:**\nThe problem is valid as it is scientifically sound, well-posed, and objective. It is a standard exercise in applying resampling techniques. I will proceed with the solution.\n\n### Step 1: Preliminary Calculation - Median of the Original Sample\n\nThe original sample is $x=\\{3.0,\\,3.5,\\,4.0,\\,6.0,\\,6.5,\\,7.0,\\,9.0\\}$. The sample size is $n=7$.\nThe data is already sorted: $x_{(1)}=3.0, x_{(2)}=3.5, x_{(3)}=4.0, x_{(4)}=6.0, x_{(5)}=6.5, x_{(6)}=7.0, x_{(7)}=9.0$.\nSince $n=7$ is odd, the sample median, denoted by $\\hat{\\theta}$, is the middle value, which is the $((n+1)/2)$-th order statistic.\n$$\\text{position} = \\frac{7+1}{2} = 4$$\nThe sample median is the 4th value in the sorted list.\n$$ \\hat{\\theta} = x_{(4)} = 6.0 $$\n\n### Step 2: Jackknife Standard Error Calculation\n\nThe jackknife method involves creating $n$ new samples, each of size $n-1$, by leaving out one observation at a time. Let $x_{(i)}$ be the sample with the $i$-th observation removed, and let $\\hat{\\theta}_{(i)}$ be the median of that sample. Here, $n=7$, so each jackknife sample has size $6$. For an even-sized sample, the median is the average of the two middle elements.\n\n1.  Leave out $x_1=3.0$: $\\{3.5, 4.0, 6.0, 6.5, 7.0, 9.0\\}$. Median is $\\hat{\\theta}_{(1)} = \\frac{6.0+6.5}{2} = 6.25$.\n2.  Leave out $x_2=3.5$: $\\{3.0, 4.0, 6.0, 6.5, 7.0, 9.0\\}$. Median is $\\hat{\\theta}_{(2)} = \\frac{6.0+6.5}{2} = 6.25$.\n3.  Leave out $x_3=4.0$: $\\{3.0, 3.5, 6.0, 6.5, 7.0, 9.0\\}$. Median is $\\hat{\\theta}_{(3)} = \\frac{6.0+6.5}{2} = 6.25$.\n4.  Leave out $x_4=6.0$: $\\{3.0, 3.5, 4.0, 6.5, 7.0, 9.0\\}$. Median is $\\hat{\\theta}_{(4)} = \\frac{4.0+6.5}{2} = 5.25$.\n5.  Leave out $x_5=6.5$: $\\{3.0, 3.5, 4.0, 6.0, 7.0, 9.0\\}$. Median is $\\hat{\\theta}_{(5)} = \\frac{4.0+6.0}{2} = 5.0$.\n6.  Leave out $x_6=7.0$: $\\{3.0, 3.5, 4.0, 6.0, 6.5, 9.0\\}$. Median is $\\hat{\\theta}_{(6)} = \\frac{4.0+6.0}{2} = 5.0$.\n7.  Leave out $x_7=9.0$: $\\{3.0, 3.5, 4.0, 6.0, 6.5, 7.0\\}$. Median is $\\hat{\\theta}_{(7)} = \\frac{4.0+6.0}{2} = 5.0$.\n\nThe jackknife replicates of the median are $\\{6.25, 6.25, 6.25, 5.25, 5.0, 5.0, 5.0\\}$.\nThe jackknife standard error is given by the formula:\n$$ \\text{SE}_{\\text{jack}} = \\sqrt{\\frac{n-1}{n} \\sum_{i=1}^{n} (\\hat{\\theta}_{(i)} - \\bar{\\theta}_{(\\cdot)})^2} $$\nwhere $\\bar{\\theta}_{(\\cdot)} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{\\theta}_{(i)}$.\n\nFirst, we calculate the mean of the jackknife medians:\n$$ \\bar{\\theta}_{(\\cdot)} = \\frac{1}{7} (3 \\times 6.25 + 5.25 + 3 \\times 5.0) = \\frac{18.75 + 5.25 + 15.0}{7} = \\frac{39.0}{7} $$\nNext, we calculate the sum of squared differences:\n$$ \\sum_{i=1}^{n} (\\hat{\\theta}_{(i)} - \\bar{\\theta}_{(\\cdot)})^2 = 3 \\times \\left(6.25 - \\frac{39}{7}\\right)^2 + \\left(5.25 - \\frac{39}{7}\\right)^2 + 3 \\times \\left(5.0 - \\frac{39}{7}\\right)^2 $$\n$$ = 3 \\times \\left(\\frac{25}{4} - \\frac{39}{7}\\right)^2 + \\left(\\frac{21}{4} - \\frac{39}{7}\\right)^2 + 3 \\times \\left(5 - \\frac{39}{7}\\right)^2 $$\n$$ = 3 \\times \\left(\\frac{175-156}{28}\\right)^2 + \\left(\\frac{147-156}{28}\\right)^2 + 3 \\times \\left(\\frac{35-39}{7}\\right)^2 $$\n$$ = 3 \\times \\left(\\frac{19}{28}\\right)^2 + \\left(\\frac{-9}{28}\\right)^2 + 3 \\times \\left(\\frac{-4}{7}\\right)^2 = 3 \\times \\frac{361}{784} + \\frac{81}{784} + 3 \\times \\frac{16}{49} $$\n$$ = \\frac{1083}{784} + \\frac{81}{784} + \\frac{48 \\times 16}{49 \\times 16} = \\frac{1083 + 81 + 768}{784} = \\frac{1932}{784} = \\frac{483}{196} = \\frac{69}{28} $$\nThe jackknife variance estimate is:\n$$ \\text{Var}_{\\text{jack}} = \\frac{n-1}{n} \\sum_{i=1}^{n} (\\hat{\\theta}_{(i)} - \\bar{\\theta}_{(\\cdot)})^2 = \\frac{6}{7} \\times \\frac{69}{28} = \\frac{3 \\times 69}{7 \\times 14} = \\frac{207}{98} $$\nThe jackknife standard error is:\n$$ \\text{SE}_{\\text{jack}} = \\sqrt{\\frac{207}{98}} \\approx 1.453356 $$\n\n### Step 3: Bootstrap Standard Error Calculation\n\nWe are given $B=12$ bootstrap resamples. For each, we must find the median. Each resample has size $n=7$, so the median is the 4th order statistic.\n\n1.  $x^{*(1)}=\\{3.0,3.5,4.0,\\mathbf{6.0},6.5,7.0,9.0\\}$. $\\hat{\\theta}^{*(1)}=6.0$.\n2.  $x^{*(2)}=\\{3.5,4.0,6.0,\\mathbf{6.0},6.5,7.0,9.0\\}$. $\\hat{\\theta}^{*(2)}=6.0$.\n3.  $x^{*(3)}=\\{3.5,4.0,6.0,\\mathbf{6.5},7.0,9.0,9.0\\}$. $\\hat{\\theta}^{*(3)}=6.5$.\n4.  $x^{*(4)}=\\{3.0,3.0,3.5,\\mathbf{4.0},6.0,6.0,6.5\\}$. $\\hat{\\theta}^{*(4)}=4.0$.\n5.  $x^{*(5)}=\\{3.0,3.5,3.5,\\mathbf{4.0},4.0,6.0,6.0\\}$. $\\hat{\\theta}^{*(5)}=4.0$.\n6.  $x^{*(6)}=\\{4.0,4.0,4.0,\\mathbf{6.0},6.0,6.5,7.0\\}$. $\\hat{\\theta}^{*(6)}=6.0$.\n7.  $x^{*(7)}=\\{6.0,6.0,6.5,\\mathbf{6.5},6.5,7.0,9.0\\}$. $\\hat{\\theta}^{*(7)}=6.5$.\n8.  $x^{*(8)}=\\{3.0,3.5,3.5,\\mathbf{4.0},4.0,4.0,4.0\\}$. $\\hat{\\theta}^{*(8)}=4.0$.\n9.  $x^{*(9)}=\\{6.0,6.0,6.5,\\mathbf{7.0},7.0,9.0,9.0\\}$. $\\hat{\\theta}^{*(9)}=7.0$.\n10. $x^{*(10)}=\\{3.0,3.0,3.5,\\mathbf{3.5},4.0,4.0,6.0\\}$. $\\hat{\\theta}^{*(10)}=3.5$.\n11. $x^{*(11)}=\\{6.0,6.0,6.5,\\mathbf{6.5},6.5,7.0,9.0\\}$. $\\hat{\\theta}^{*(11)}=6.5$.\n12. $x^{*(12)}=\\{3.0,3.0,3.5,\\mathbf{4.0},6.0,6.5,9.0\\}$. $\\hat{\\theta}^{*(12)}=4.0$.\n\nThe bootstrap replicates of the median are $\\{6.0, 6.0, 6.5, 4.0, 4.0, 6.0, 6.5, 4.0, 7.0, 3.5, 6.5, 4.0\\}$.\nThe bootstrap standard error is the standard deviation of these $B$ values:\n$$ \\text{SE}_{\\text{boot}} = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\theta}^{*(b)} - \\bar{\\theta}^{*})^2} $$\nwhere $\\bar{\\theta}^{*} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\theta}^{*(b)}$.\n\nThe mean of the bootstrap medians is:\n$$ \\bar{\\theta}^{*} = \\frac{1}{12} (6.0+6.0+6.5+4.0+4.0+6.0+6.5+4.0+7.0+3.5+6.5+4.0) = \\frac{64.0}{12} = \\frac{16}{3} $$\nThe sum of squared differences is:\n$$ \\sum_{b=1}^{12} (\\hat{\\theta}^{*(b)} - \\bar{\\theta}^{*})^2 = (3.5 - \\frac{16}{3})^2 + 4 \\times (4.0 - \\frac{16}{3})^2 + 3 \\times (6.0 - \\frac{16}{3})^2 + 3 \\times (6.5 - \\frac{16}{3})^2 + (7.0 - \\frac{16}{3})^2 $$\n$$ = (\\frac{10.5-16}{3})^2 + 4(\\frac{12-16}{3})^2 + 3(\\frac{18-16}{3})^2 + 3(\\frac{19.5-16}{3})^2 + (\\frac{21-16}{3})^2 $$\n$$ = (\\frac{-5.5}{3})^2 + 4(\\frac{-4}{3})^2 + 3(\\frac{2}{3})^2 + 3(\\frac{3.5}{3})^2 + (\\frac{5}{3})^2 $$\nUsing fractions $\\frac{-11}{6}$, $\\frac{-4}{3}$, $\\frac{2}{3}$, $\\frac{7}{6}$, $\\frac{5}{3}$:\n$$ = (\\frac{121}{36}) + 4(\\frac{16}{9}) + 3(\\frac{4}{9}) + 3(\\frac{49}{36}) + (\\frac{25}{9}) $$\n$$ = \\frac{121}{36} + \\frac{64 \\times 4}{36} + \\frac{12 \\times 4}{36} + \\frac{147}{36} + \\frac{25 \\times 4}{36} = \\frac{121 + 256 + 48 + 147 + 100}{36} = \\frac{672}{36} = \\frac{56}{3} $$\nThe bootstrap variance estimate is ($B-1=11$):\n$$ \\text{Var}_{\\text{boot}} = \\frac{1}{11} \\times \\frac{56}{3} = \\frac{56}{33} $$\nThe bootstrap standard error is:\n$$ \\text{SE}_{\\text{boot}} = \\sqrt{\\frac{56}{33}} \\approx 1.302677 $$\n\n### Step 4: Ratio of Standard Errors\n\nThe ratio $R$ is defined as $\\frac{\\text{SE}_{\\text{jack}}}{\\text{SE}_{\\text{boot}}}$.\n$$ R = \\frac{\\sqrt{207/98}}{\\sqrt{56/33}} = \\sqrt{\\frac{207}{98} \\times \\frac{33}{56}} = \\sqrt{\\frac{6831}{5488}} $$\nCalculating the numerical value:\n$$ R \\approx \\sqrt{1.24471574} \\approx 1.11566829 $$\nRounding the result to four significant figures gives:\n$$ R \\approx 1.116 $$\nThis ratio indicates that for this specific dataset and small number of bootstrap resamples, the jackknife estimate of the standard error is about $11.6\\%$ larger than the bootstrap estimate.",
            "answer": "$$\\boxed{1.116}$$"
        },
        {
            "introduction": "Now that we have explored the basic mechanics, let's unlock the true potential of resampling through computation. This practice demonstrates how to implement the bootstrap in code to estimate the standard error of the sample median—a robust statistic for which standard error formulas are often complex or unreliable, especially in small samples. By applying the procedure to datasets with different characteristics, including one with a significant outlier, you will see firsthand why the bootstrap is an indispensable tool for assessing the precision of estimators in modern biostatistics .",
            "id": "4948755",
            "problem": "You are given independent and identically distributed (IID) observations of systolic blood pressure measured in millimeters of mercury (mmHg). Let the sample be denoted by $\\{X_1,\\dots,X_n\\}$ with $n=20$. The sample median $\\hat{m}$ is the central robust location estimator obtained from the order statistics of the sample, defined for even $n$ as the average of the two central order statistics. An investigator wants a robust variance estimate for $\\hat{m}$ by approximating its sampling variability using the bootstrap principle: resampling with replacement from the empirical distribution of the observed data and re-computing the median across multiple bootstrap replicates. The empirical bootstrap distribution of the medians is then used to estimate the Standard Error (SE) of $\\hat{m}$. \n\nStarting from the core definitions that:\n- The empirical distribution $\\hat{F}_n$ places probability mass $1/n$ on each observed value in $\\{X_i\\}_{i=1}^n$.\n- A bootstrap resample of size $n$ is generated by sampling with replacement from $\\{X_i\\}_{i=1}^n$ according to $\\hat{F}_n$.\n- The bootstrap estimate of the SE of the sample median is the empirical standard deviation of the set of bootstrap medians.\n\nWrite a program that:\n- For each provided test case, computes the bootstrap estimate of the SE of the sample median using $B=1000$ IID bootstrap resamples of size $n=20$ drawn with replacement from the observed data.\n- Uses a fixed pseudorandom number generator seed for reproducibility equal to $s=20231111$.\n- Reports the empirical standard deviation of the $B$ bootstrap medians as a floating-point number in mmHg.\n\nExpress all results explicitly in mmHg. The final outputs should be floats, rounded in the program to six decimal places.\n\nTest Suite (each is a list of $n=20$ systolic blood pressure values, in mmHg):\n1. General variability, typical adult values:\n   $\\{118,122,135,128,150,142,130,125,138,145,132,129,140,136,127,133,149,121,137,134\\}$\n2. Presence of a high outlier:\n   $\\{110,115,120,125,130,135,140,145,150,155,160,165,170,175,180,185,190,195,200,240\\}$\n3. Multiple ties (repeated measurements):\n   $\\{130,130,130,130,135,135,135,140,140,140,145,145,145,150,150,150,155,155,160,160\\}$\n4. Degenerate case (all equal):\n   $\\{130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130,130\\}$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each \"result\" is the bootstrap SE (mmHg) for the corresponding test case, formatted to six decimal places. Angles are not involved. No percentages are involved.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the established statistical theory of bootstrap resampling, is well-posed with all necessary parameters provided for a unique and reproducible solution, and is expressed in objective, formal language. We may therefore proceed with the solution.\n\nThe problem requires the computation of a bootstrap estimate for the standard error (SE) of the sample median for four distinct datasets of systolic blood pressure measurements. The standard error of a statistic is a measure of the variability of that statistic's sampling distribution; it quantifies the precision of the sample statistic as an estimate of the corresponding population parameter. The bootstrap is a powerful and widely used computational method for approximating this sampling distribution when its theoretical form is unknown or difficult to derive.\n\nThe fundamental principle of the bootstrap, as proposed by Efron, is to use the observed sample itself as an approximation to the true underlying data-generating distribution. The empirical distribution function, $\\hat{F}_n$, which assigns a probability mass of $1/n$ to each observed data point $X_i$ in the sample $\\{X_1, \\dots, X_n\\}$, serves as this approximation.\n\nThe procedure is as follows:\n$1$. A large number, $B$, of bootstrap samples are generated. Each bootstrap sample, denoted $\\{X_{b,1}^*, \\dots, X_{b,n}^*\\}$ for $b \\in \\{1, \\dots, B\\}$, is a random sample of size $n$ drawn *with replacement* from the original data $\\{X_1, \\dots, X_n\\}$. Drawing from the original sample with replacement is equivalent to drawing an i.i.d. sample from the empirical distribution $\\hat{F}_n$. For this problem, the sample size is $n=20$ and the number of bootstrap replicates is $B=1000$.\n\n$2$. For each bootstrap sample, the statistic of interest—in this case, the sample median—is computed. Let $\\hat{m}_b^*$ be the median of the $b$-th bootstrap sample. The sample median for a sample of even size $n$ is defined as the arithmetic mean of the two central order statistics, i.e., the values at positions $n/2$ and $(n/2)+1$ in the sorted sample. For $n=20$, this is the average of the $10$-th and $11$-th sorted values.\n\n$3$. The collection of these $B$ bootstrap medians, $\\{\\hat{m}_1^*, \\hat{m}_2^*, \\dots, \\hat{m}_B^*\\}$, constitutes an empirical approximation of the sampling distribution of the sample median $\\hat{m}$.\n\n$4$. The bootstrap estimate of the standard error of the sample median, $\\text{SE}_{\\text{boot}}(\\hat{m})$, is then calculated as the sample standard deviation of this collection of bootstrap medians:\n$$ \\text{SE}_{\\text{boot}}(\\hat{m}) = \\sqrt{ \\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{m}_b^* - \\bar{m}^*)^2 } $$\nwhere $\\bar{m}^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{m}_b^*$ is the mean of the $B$ bootstrap medians. The use of the $B-1$ denominator (Bessel's correction) is standard practice for estimating a population standard deviation from a sample, which is analogous to our current task of estimating the standard deviation of the sampling distribution from our bootstrap-generated sample of medians.\n\nTo ensure reproducibility, the entire process is executed using a pseudorandom number generator initialized with a fixed seed, $s=20231111$. This guarantees that the sequence of bootstrap samples is identical upon every execution, leading to a deterministic final result.\n\nThe algorithm to be implemented is as follows for each test case:\na. Initialize a pseudorandom number generator with the specified seed $s=20231111$. This is done only once for the entire script to ensure a continuous stream of random numbers across all test cases.\nb. For a given data sample of size $n=20$, create an array to store $B=1000$ bootstrap median values.\nc. Loop $B$ times:\n    i. Generate a bootstrap resample of size $n=20$ by choosing elements from the original sample with replacement.\n    ii. Compute the median of this resample.\n    iii. Store the computed median.\nd. After the loop, calculate the sample standard deviation of the $B$ stored medians using the formula above (i.e., with a divisor of $B-1$).\ne. Round the resulting standard error to $6$ decimal places.\nf. Repeat for all four test cases and format the results as specified.\nThis entire procedure is implemented in the provided Python code, utilizing the `numpy` library for efficient array manipulation, random sampling, and statistical calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bootstrap estimate of the standard error of the sample median\n    for four given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. General variability, typical adult values\n        [118, 122, 135, 128, 150, 142, 130, 125, 138, 145, 132, 129, 140, 136, 127, 133, 149, 121, 137, 134],\n        # 2. Presence of a high outlier\n        [110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 240],\n        # 3. Multiple ties (repeated measurements)\n        [130, 130, 130, 130, 135, 135, 135, 140, 140, 140, 145, 145, 145, 150, 150, 150, 155, 155, 160, 160],\n        # 4. Degenerate case (all equal)\n        [130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130, 130]\n    ]\n\n    # Parameters from the problem statement\n    n = 20  # Sample size\n    B = 1000 # Number of bootstrap replicates\n    seed = 20231111 # RNG seed\n\n    # Initialize the pseudorandom number generator once for reproducibility\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for data in test_cases:\n        # Convert data to a NumPy array for efficient computation\n        original_sample = np.array(data)\n        \n        # Array to store the median from each bootstrap replicate\n        bootstrap_medians = np.empty(B, dtype=np.float64)\n\n        # Main bootstrap loop\n        for i in range(B):\n            # Generate a bootstrap resample of size n by sampling with replacement\n            resample = rng.choice(original_sample, size=n, replace=True)\n            \n            # Compute and store the median of the resample\n            bootstrap_medians[i] = np.median(resample)\n\n        # Compute the bootstrap estimate of the standard error of the median.\n        # This is the empirical standard deviation of the distribution of bootstrap medians.\n        # ddof=1 is used for sample standard deviation (Bessel's correction).\n        se_median = np.std(bootstrap_medians, ddof=1)\n        \n        # Round the result to six decimal places\n        rounded_result = round(se_median, 6)\n        \n        results.append(rounded_result)\n\n    # Format the final list of results into the specified output string.\n    # The f-string format specifier ensures each float is printed with six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Resampling methods are not one-size-fits-all; they can be cleverly adapted to solve specific statistical challenges. This final practice introduces the \"wild bootstrap,\" a powerful variant designed for linear regression models where the assumption of constant error variance (homoskedasticity) is violated. You will tackle a common scenario in biostatistics where a biomarker's measurement variability increases with its level. This exercise will guide you through implementing the wild bootstrap to obtain robust standard errors for regression coefficients, demonstrating the flexibility of the resampling paradigm in real-world data analysis .",
            "id": "4948717",
            "problem": "Consider a simple linear regression model appropriate for biostatistics applications where conditional error variance increases with a biomarker level. Let observations be indexed by $i \\in \\{1,\\dots,n\\}$, with a scalar predictor (biomarker) $x_i$ and an outcome $y_i$. Assume the data are generated from the data-generating process\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\varepsilon_i,\n$$\nwhere $(\\varepsilon_i \\mid x_i)$ has conditional mean $E[\\varepsilon_i \\mid x_i] = 0$ and conditional variance $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma_0^2 \\, \\bigl(1 + \\alpha x_i\\bigr)^2$, with $\\sigma_0 > 0$ and $\\alpha \\ge 0$. The increasing variance in $x_i$ induces heteroskedasticity. The ordinary least squares estimator $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^\\top$ minimizes the sum of squared residuals $\\sum_{i=1}^n \\bigl(y_i - \\beta_0 - \\beta_1 x_i\\bigr)^2$.\n\nYou must compute heteroskedasticity-robust uncertainty via the wild bootstrap using Mammen multipliers. The wild bootstrap uses residuals $\\hat{e}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$ and generates bootstrap pseudo-responses $y_i^{\\ast} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{e}_i v_i$, where $v_i$ are independent and identically distributed across $i$ from the Mammen two-point distribution defined by\n$$\nv_i \\;=\\; \\frac{1 - \\sqrt{5}}{2} \\quad \\text{with probability} \\quad \\frac{\\sqrt{5} + 1}{2\\sqrt{5}}, \n\\qquad\nv_i \\;=\\; \\frac{1 + \\sqrt{5}}{2} \\quad \\text{with probability} \\quad \\frac{\\sqrt{5} - 1}{2\\sqrt{5}}.\n$$\nFor each bootstrap sample, refit ordinary least squares to $\\{(x_i, y_i^\\ast)\\}_{i=1}^n$ to obtain $\\hat{\\beta}^{\\ast} = (\\hat{\\beta}^{\\ast}_0, \\hat{\\beta}^{\\ast}_1)^\\top$. Repeat this for $B$ bootstrap replicates and estimate the wild bootstrap standard error for each component of $\\hat{\\beta}$ as the sample standard deviation across the $B$ bootstrap estimates. Use $B = 2000$.\n\nYour program must implement this procedure and return the wild bootstrap standard errors for the intercept and slope for multiple synthetic test datasets constructed as follows. For each test case, generate $x_i$ independently from a continuous uniform distribution on a specified interval, then generate $\\varepsilon_i$ independently from a normal distribution with mean $0$ and standard deviation $\\sigma_0 (1 + \\alpha x_i)$, and finally set $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. All randomness must be controlled by a fixed seed per test case to ensure reproducibility.\n\nUse the following test suite of four test cases, each specified by $(\\text{seed}, n, \\beta_0, \\beta_1, \\sigma_0, \\alpha, x_{\\min}, x_{\\max})$:\n- Test case $1$: $(\\,123,\\, 60,\\, 0.5,\\, 1.2,\\, 1.0,\\, 0.5,\\, 0,\\, 5\\,)$.\n- Test case $2$: $(\\,42,\\, 30,\\, -0.3,\\, 2.0,\\, 0.5,\\, 1.0,\\, 0,\\, 3\\,)$.\n- Test case $3$: $(\\,777,\\, 200,\\, 0.0,\\, 0.8,\\, 0.8,\\, 0.2,\\, 0,\\, 10\\,)$.\n- Test case $4$: $(\\,999,\\, 40,\\, 1.0,\\, -0.5,\\, 1.2,\\, 0.0,\\, 0,\\, 4\\,)$.\n\nFor each test case:\n- Set the pseudorandom number generator with the given $\\text{seed}$.\n- Draw $x_i \\sim \\mathrm{Uniform}(x_{\\min}, x_{\\max})$ independently for $i \\in \\{1,\\dots,n\\}$.\n- Draw $\\varepsilon_i \\sim \\mathcal{N}\\!\\left(0, \\bigl[\\sigma_0 (1 + \\alpha x_i)\\bigr]^2\\right)$ independently across $i$.\n- Form $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n- Fit ordinary least squares with an intercept to obtain $\\hat{\\beta}$ and residuals $\\hat{e}_i$.\n- Perform the wild bootstrap with Mammen multipliers and $B = 2000$ to obtain the bootstrap standard errors for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n\nFinal output format requirement:\n- Your program must produce a single line containing a comma-separated list enclosed in square brackets.\n- The list must contain, in order, for each test case $k \\in \\{1,2,3,4\\}$, first the wild bootstrap standard error for the intercept $\\hat{\\beta}_0$ and then for the slope $\\hat{\\beta}_1$.\n- There are thus $8$ numbers total.\n- Each number must be rounded to $6$ decimal places.\n- Example of the required formatting (with placeholders): $[\\text{se}_{0,1},\\text{se}_{1,1},\\text{se}_{0,2},\\text{se}_{1,2},\\text{se}_{0,3},\\text{se}_{1,3},\\text{se}_{0,4},\\text{se}_{1,4}]$.",
            "solution": "The problem requires the computation of heteroskedasticity-robust standard errors for the coefficients of a simple linear regression model using the wild bootstrap method with Mammen multipliers.\n\n### Step 1: Problem Validation\n\nThe first step is to validate the problem statement.\n\n#### 1.1. Extraction of Givens\nThe problem provides the following information:\n- **Model**: A simple linear regression model $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n- **Data Generating Process (DGP)**:\n    - The conditional mean of the error term is $E[\\varepsilon_i \\mid x_i] = 0$.\n    - The conditional variance of the error term is heteroskedastic: $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma_0^2 (1 + \\alpha x_i)^2$, with $\\sigma_0 > 0$ and $\\alpha \\ge 0$.\n    - For simulation, covariates $x_i$ are drawn independently from a continuous uniform distribution, $x_i \\sim \\mathrm{Uniform}(x_{\\min}, x_{\\max})$.\n    - Error terms $\\varepsilon_i$ are drawn independently from a normal distribution, $\\varepsilon_i \\sim \\mathcal{N}(0, [\\sigma_0 (1 + \\alpha x_i)]^2)$.\n- **Estimator**: The ordinary least squares (OLS) estimator $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^\\top$, which minimizes the sum of squared residuals.\n- **Uncertainty Estimation Method**: Wild bootstrap with Mammen multipliers.\n    - Residuals are calculated as $\\hat{e}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$.\n    - Bootstrap pseudo-responses are generated as $y_i^{\\ast} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i + \\hat{e}_i v_i$.\n    - The multipliers $v_i$ are drawn independently from the Mammen two-point distribution:\n        - $v_i = \\frac{1 - \\sqrt{5}}{2}$ with probability $p_1 = \\frac{\\sqrt{5} + 1}{2\\sqrt{5}}$.\n        - $v_i = \\frac{1 + \\sqrt{5}}{2}$ with probability $p_2 = \\frac{\\sqrt{5} - 1}{2\\sqrt{5}}$.\n    - For each bootstrap sample, an OLS model is refit to get $\\hat{\\beta}^{\\ast} = (\\hat{\\beta}^{\\ast}_0, \\hat{\\beta}^{\\ast}_1)^\\top$.\n    - The number of bootstrap replicates is $B = 2000$.\n    - The standard error for each coefficient is the sample standard deviation of the $B$ bootstrap estimates.\n- **Test Cases**: Four specific sets of parameters $(\\text{seed}, n, \\beta_0, \\beta_1, \\sigma_0, \\alpha, x_{\\min}, x_{\\max})$ are provided.\n    1. $(123, 60, 0.5, 1.2, 1.0, 0.5, 0, 5)$\n    2. $(42, 30, -0.3, 2.0, 0.5, 1.0, 0, 3)$\n    3. $(777, 200, 0.0, 0.8, 0.8, 0.2, 0, 10)$\n    4. $(999, 40, 1.0, -0.5, 1.2, 0.0, 0, 4)$\n- **Output Format**: A single list of $8$ numbers (SE for intercept, SE for slope for each of the $4$ cases), rounded to $6$ decimal places.\n\n#### 1.2. Validation against Criteria\n- **Scientifically Grounded**: The problem is grounded in established statistical theory. Linear regression, heteroskedasticity, and the wild bootstrap are standard topics in biostatistics and econometrics. The specified variance structure is a plausible model for phenomena where variability increases with the level of a biomarker. The Mammen distribution is a standard and theoretically justified choice for multipliers in wild bootstrap.\n- **Well-Posed**: The problem is well-posed. It provides all necessary parameters, a clear step-by-step procedure, and specific random seeds for reproducibility. This ensures that a unique and meaningful numerical solution exists.\n- **Objective**: The problem is stated in precise, objective, and mathematical language. There are no subjective or ambiguous terms.\n\nThe problem statement does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, feasible, and well-structured.\n\n#### 1.3. Verdict\nThe problem is **valid**.\n\n### Step 2: Solution Derivation\n\nThe solution involves implementing a computational statistics algorithm. The core task is to estimate the standard errors of OLS coefficients in the presence of heteroskedasticity.\n\n#### 2.1. Theoretical Framework\nThe assumed model is $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. In matrix form, this is $\\mathbf{y} = \\mathbf{X}\\beta + \\varepsilon$, where $\\mathbf{y}$ is the $n \\times 1$ vector of outcomes, $\\mathbf{X}$ is the $n \\times 2$ design matrix with a column of ones and a column of predictor values $x_i$, $\\beta = (\\beta_0, \\beta_1)^\\top$ is the vector of coefficients, and $\\varepsilon$ is the vector of errors.\n\nThe OLS estimator is given by $\\hat{\\beta} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$. Under the standard assumption of homoskedasticity (i.e., $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2$ for all $i$), the variance of $\\hat{\\beta}$ is $\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2(\\mathbf{X}^\\top\\mathbf{X})^{-1}$. However, the problem specifies heteroskedastic errors, $\\mathrm{Var}(\\varepsilon_i \\mid x_i) = \\sigma_i^2 = \\sigma_0^2 (1 + \\alpha x_i)^2$. In this case, the true variance of $\\hat{\\beta}$ is the sandwich estimator form: $\\mathrm{Var}(\\hat{\\beta}) = (\\mathbf{X}^\\top\\mathbf{X})^{-1}(\\mathbf{X}^\\top\\mathbf{\\Omega}\\mathbf{X})(\\mathbf{X}^\\top\\mathbf{X})^{-1}$, where $\\mathbf{\\Omega}$ is a diagonal matrix with diagonal entries $\\sigma_i^2$.\n\nSince $\\mathbf{\\Omega}$ is unknown, we need to estimate this variance. The wild bootstrap is a resampling method particularly suited for this. It generates bootstrap samples in a way that preserves the heteroskedasticity structure found in the original data.\n\nThe procedure is as follows:\n1.  Fit the OLS model to the original data $(\\mathbf{y}, \\mathbf{X})$ to obtain the estimates $\\hat{\\beta}$ and the residuals $\\hat{e}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$.\n2.  Generate $B$ bootstrap samples. For each bootstrap replicate $b \\in \\{1, \\dots, B\\}$:\n    a.  For each observation $i \\in \\{1, \\dots, n\\}$, draw a random value $v_i$ from a distribution with mean $0$ and variance $1$. The problem specifies the Mammen two-point distribution.\n    b.  Construct a bootstrap pseudo-response $y_i^{\\ast} = \\hat{y}_i + \\hat{e}_i v_i$. This is equivalent to creating a bootstrap error term $\\varepsilon_i^{\\ast} = \\hat{e}_i v_i$.\n    c.  Fit an OLS model using the original predictors $x_i$ and the new responses $y_i^{\\ast}$ to get a bootstrap coefficient estimate $\\hat{\\beta}^{\\ast,b}$.\n3.  The collection of bootstrap estimates $\\{\\hat{\\beta}^{\\ast,1}, \\dots, \\hat{\\beta}^{\\ast,B}\\}$ forms an empirical distribution that approximates the true sampling distribution of $\\hat{\\beta}$.\n4.  The bootstrap standard error for each coefficient is the sample standard deviation of the corresponding bootstrap estimates. For $\\hat{\\beta}_j$, the standard error is $\\mathrm{SE}(\\hat{\\beta}_j) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B (\\hat{\\beta}_j^{\\ast,b} - \\bar{\\beta}_j^{\\ast})^2}$, where $\\bar{\\beta}_j^{\\ast} = \\frac{1}{B}\\sum_{b=1}^B \\hat{\\beta}_j^{\\ast,b}$.\n\nThe choice of the Mammen distribution for $v_i$ is deliberate. It has $E[v_i] = 0$, $E[v_i^2] = 1$, and $E[v_i^3] = 1$. The first two moments ensure that in the bootstrap world, conditional on the original data, $E^{\\ast}[\\varepsilon_i^{\\ast} \\mid x_i] = \\hat{e}_i E[v_i] = 0$ and $\\mathrm{Var}^{\\ast}(\\varepsilon_i^{\\ast} \\mid x_i) = \\hat{e}_i^2 E[v_i^2] = \\hat{e}_i^2$. Since $\\hat{e}_i^2$ is an observation-specific estimate of the true error variance $\\sigma_i^2$, this procedure generates bootstrap data with a variance structure that mimics the heteroskedasticity of the original data. The third moment property helps in obtaining higher-order refinements for confidence intervals, though it is not strictly necessary for standard error estimation.\n\n#### 2.2. Algorithmic Implementation\nFor each test case defined by $(\\text{seed}, n, \\beta_0, \\beta_1, \\sigma_0, \\alpha, x_{\\min}, x_{\\max})$:\n1.  **Initialize RNG**: Set the seed of the pseudorandom number generator for reproducibility.\n2.  **Generate Data**:\n    - Draw $n$ values for the predictor $x_i$ from $\\mathrm{Uniform}(x_{\\min}, x_{\\max})$.\n    - For each $x_i$, compute the specific standard deviation $\\sigma_i = \\sigma_0 (1 + \\alpha x_i)$.\n    - Draw $n$ error terms $\\varepsilon_i$ from $\\mathcal{N}(0, \\sigma_i^2)$.\n    - Construct the outcome variable $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n3.  **Initial OLS Fit**:\n    - Construct the design matrix $\\mathbf{X}$ of size $n \\times 2$ with first column being all ones and second column being the vector of $x_i$.\n    - Compute the OLS estimate $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1)^\\top$ by solving the normal equations, for instance, using a least-squares solver on $\\mathbf{y}$ and $\\mathbf{X}$.\n    - Compute the fitted values $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\beta}$.\n    - Compute the residuals $\\hat{\\mathbf{e}} = \\mathbf{y} - \\hat{\\mathbf{y}}$.\n4.  **Wild Bootstrap Loop**:\n    - Initialize a storage matrix for bootstrap coefficients, e.g., a $B \\times 2$ NumPy array, where $B=2000$.\n    - Define the values and probabilities for the Mammen multipliers:\n        - $v_1 = (1 - \\sqrt{5})/2$, $p_1 = (\\sqrt{5} + 1)/(2\\sqrt{5})$.\n        - $v_2 = (1 + \\sqrt{5})/2$, $p_2 = 1 - p_1$.\n    - Loop $B$ times:\n        a. Generate an $n \\times 1$ vector of multipliers $\\mathbf{v}$ by drawing from the Mammen distribution.\n        b. Create the bootstrap response vector $\\mathbf{y}^{\\ast} = \\hat{\\mathbf{y}} + \\hat{\\mathbf{e}} \\odot \\mathbf{v}$, where $\\odot$ denotes element-wise multiplication.\n        c. Compute the bootstrap OLS estimate $\\hat{\\beta}^{\\ast}$ by solving the least-squares problem for $\\mathbf{y}^{\\ast}$ and $\\mathbf{X}$.\n        d. Store $\\hat{\\beta}^{\\ast}$ as a row in the storage matrix.\n5.  **Compute Standard Errors**:\n    - After the loop, calculate the sample standard deviation (with $1$ degree of freedom correction, `ddof=1`) for each column of the storage matrix. These are the wild bootstrap standard errors for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n6.  **Store and format results**: The two computed standard errors are appended to a list that aggregates results from all test cases. The final list is formatted as a string according to the problem specification.\n\nThis procedure will be repeated for all four test cases.",
            "answer": "```python\nimport numpy as np\n\ndef compute_wild_bootstrap_se(seed, n, beta0, beta1, sigma0, alpha, x_min, x_max, B=2000):\n    \"\"\"\n    Computes heteroskedasticity-robust standard errors using the wild bootstrap.\n\n    Args:\n        seed (int): The seed for the random number generator.\n        n (int): Sample size.\n        beta0 (float): True intercept.\n        beta1 (float): True slope.\n        sigma0 (float): Baseline standard deviation parameter.\n        alpha (float): Heteroskedasticity parameter.\n        x_min (float): Minimum of the uniform distribution for x.\n        x_max (float): Maximum of the uniform distribution for x.\n        B (int): Number of bootstrap replicates.\n\n    Returns:\n        tuple: A tuple containing the bootstrap standard errors for the intercept and slope.\n    \"\"\"\n    # 1. Initialize RNG and Generate Data\n    rng = np.random.default_rng(seed)\n    \n    # Generate predictors x_i\n    x = rng.uniform(x_min, x_max, size=n)\n    \n    # Generate heteroskedastic errors\n    true_sd = sigma0 * (1 + alpha * x)\n    epsilon = rng.normal(0, true_sd)\n    \n    # Generate outcomes y_i\n    y = beta0 + beta1 * x + epsilon\n    \n    # 2. Initial OLS Fit\n    # Construct design matrix X with an intercept\n    X = np.c_[np.ones(n), x]\n    \n    # Compute OLS estimates beta_hat\n    beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n    \n    # Compute fitted values and residuals\n    y_hat = X @ beta_hat\n    e_hat = y - y_hat\n    \n    # 3. Wild Bootstrap Loop\n    beta_boot_samples = np.zeros((B, 2))\n    \n    # Define Mammen multipliers\n    sqrt5 = np.sqrt(5)\n    v1 = (1 - sqrt5) / 2\n    v2 = (1 + sqrt5) / 2\n    p1 = (sqrt5 + 1) / (2 * sqrt5)\n    p2 = 1 - p1\n    mammen_values = [v1, v2]\n    mammen_probs = [p1, p2]\n    \n    for i in range(B):\n        # Generate multipliers v_i\n        v = rng.choice(mammen_values, size=n, p=mammen_probs)\n        \n        # Create bootstrap responses y_star\n        y_star = y_hat + e_hat * v\n        \n        # Fit OLS on bootstrap sample\n        beta_star = np.linalg.lstsq(X, y_star, rcond=None)[0]\n        beta_boot_samples[i, :] = beta_star\n        \n    # 4. Compute Standard Errors\n    # The sample standard deviation of the bootstrap estimates\n    se_beta0 = np.std(beta_boot_samples[:, 0], ddof=1)\n    se_beta1 = np.std(beta_boot_samples[:, 1], ddof=1)\n    \n    return se_beta0, se_beta1\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Test suite: (seed, n, beta0, beta1, sigma0, alpha, x_min, x_max)\n    test_cases = [\n        (123, 60, 0.5, 1.2, 1.0, 0.5, 0, 5),\n        (42, 30, -0.3, 2.0, 0.5, 1.0, 0, 3),\n        (777, 200, 0.0, 0.8, 0.8, 0.2, 0, 10),\n        (999, 40, 1.0, -0.5, 1.2, 0.0, 0, 4),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        seed, n, beta0, beta1, sigma0, alpha, x_min, x_max = case\n        se0, se1 = compute_wild_bootstrap_se(seed, n, beta0, beta1, sigma0, alpha, x_min, x_max)\n        all_results.extend([se0, se1])\n\n    # Format the results as required\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}