{
    "hands_on_practices": [
        {
            "introduction": "The heart of Generalized Estimating Equations lies in acknowledging the correlation among repeated measurements from the same subject. We specify this relationship using a \"working\" correlation matrix, $R$. This exercise guides you through deriving the structure for one of the most common forms for longitudinal data: the first-order autoregressive, or AR(1), matrix. By starting from the foundational principles of a time-series process, you will see how the intuitive idea that correlations weaken over time is translated into a precise mathematical form used directly in GEE analyses .",
            "id": "4913842",
            "problem": "A longitudinal cohort study measures a continuous biomarker for each subject at $4$ equally spaced clinic visits, indexed by times $t \\in \\{0,1,2,3\\}$. To fit a marginal mean model using Generalized Estimating Equations (GEE), a working correlation matrix is required to capture within-subject correlation across visits. Consider modeling the within-subject dependence using a first-order autoregressive (AR(1)) mechanism that is compatible with weak stationarity and the Markov property over equally spaced times. Assume the following fundamental conditions for a centered process: for each subject, define $X_{t} = Y_{t} - \\mu$, and suppose $X_{t}$ satisfies $X_{t} = \\rho X_{t-1} + \\epsilon_{t}$ where $|\\rho|<1$, $\\{\\epsilon_{t}\\}$ is a white noise process with mean $0$, variance $\\sigma_{\\epsilon}^{2}$, and is uncorrelated across time and with past values of $X_{t}$. \n\nUsing only these assumptions and the definition of the autocovariance function under weak stationarity, justify why an AR(1) working correlation is appropriate for equally spaced longitudinal measurements and derive the implied working correlation matrix for a single subject with $4$ visits as a function of the unknown parameter $\\rho$. Provide your final answer as a single closed-form analytic expression for the $4 \\times 4$ working correlation matrix in terms of $\\rho$. Do not round your answer.",
            "solution": "The task is to justify the appropriateness of a first-order autoregressive, AR(1), working correlation structure for equally spaced longitudinal data and to derive the corresponding $4 \\times 4$ working correlation matrix, $R(\\rho)$.\n\nFirst, we address the justification. An AR(1) model is particularly well-suited for longitudinal data collected at equally spaced time intervals for several reasons.\n1.  It posits that the measurement at a given time $t$ is a function of the measurement at the immediately preceding time $t-1$, plus random noise. This reflects the Markov property, which is a reasonable simplifying assumption for many biological or physical processes where the immediate past state is the most salient predictor of the current state.\n2.  The model implies that the correlation between two measurements decays exponentially as the time lag between them increases. For repeated measures on a subject, it is highly plausible that measurements taken closer in time (e.g., at visit $1$ and visit $2$) are more strongly correlated than measurements taken further apart in time (e.g., at visit $1$ and visit $4$). The AR(1) structure mathematically formalizes this intuitive decay.\n3.  The assumption of weak stationarity, implied by the GEE working correlation framework and the constraint $|\\rho|<1$, means that the mean, variance, and autocorrelation structure do not change over time. This provides a stable and parsimonious model for the within-subject dependence, characterized by a single parameter, $\\rho$.\n\nNext, we derive the working correlation matrix. The correlation matrix $R$ for a single subject has elements $R_{ij} = \\text{Corr}(Y_i, Y_j)$, where $Y_i$ and $Y_j$ are the outcomes at time indices $i$ and $j$ from the set $\\{0, 1, 2, 3\\}$. Let the matrix indices run from $1$ to $4$, corresponding to times $t=0, 1, 2, 3$. Thus, the element $R_{ij}$ corresponds to $\\text{Corr}(Y_{i-1}, Y_{j-1})$.\n\nGiven the centered process $X_t = Y_t - \\mu$, the covariance and variance properties of $Y_t$ are identical to those of $X_t$. Specifically, $\\text{Var}(Y_t) = \\text{Var}(X_t)$ and $\\text{Cov}(Y_i, Y_j) = \\text{Cov}(X_i, X_j)$. The correlation is given by:\n$$\n\\text{Corr}(Y_i, Y_j) = \\frac{\\text{Cov}(Y_i, Y_j)}{\\sqrt{\\text{Var}(Y_i)\\text{Var}(Y_j)}} = \\frac{\\text{Cov}(X_i, X_j)}{\\sqrt{\\text{Var}(X_i)\\text{Var}(X_j)}}\n$$\nUnder the assumption of weak stationarity, the variance is constant for all $t$, i.e., $\\text{Var}(X_t) = \\gamma(0)$, and the covariance depends only on the time lag $k = |i-j|$, i.e., $\\text{Cov}(X_i, X_j) = \\gamma(k)$. The autocorrelation function (ACF) is therefore $\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}$. Our goal is to find the form of $\\rho(k)$.\n\n1.  Derivation of the variance $\\gamma(0)$:\nFrom the model definition, $X_t = \\rho X_{t-1} + \\epsilon_t$. We compute the variance of $X_t$:\n$$\n\\text{Var}(X_t) = \\text{Var}(\\rho X_{t-1} + \\epsilon_t)\n$$\nSince $\\epsilon_t$ is uncorrelated with past values of $X_t$, including $X_{t-1}$, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(X_t) = \\text{Var}(\\rho X_{t-1}) + \\text{Var}(\\epsilon_t) = \\rho^2 \\text{Var}(X_{t-1}) + \\sigma_{\\epsilon}^2\n$$\nBy weak stationarity, $\\text{Var}(X_t) = \\text{Var}(X_{t-1}) = \\gamma(0)$. Substituting this into the equation gives:\n$$\n\\gamma(0) = \\rho^2 \\gamma(0) + \\sigma_{\\epsilon}^2\n$$\nSolving for $\\gamma(0)$:\n$$\n\\gamma(0)(1 - \\rho^2) = \\sigma_{\\epsilon}^2 \\implies \\gamma(0) = \\frac{\\sigma_{\\epsilon}^2}{1 - \\rho^2}\n$$\nThis result is valid because the problem states that $|\\rho|<1$, ensuring $1 - \\rho^2 > 0$.\n\n2.  Derivation of the autocovariance function $\\gamma(k)$ for $k > 0$:\nThe autocovariance at lag $k$ is $\\gamma(k) = \\text{Cov}(X_t, X_{t-k})$. Since the process is centered ($E[X_t] = 0$), this is $E[X_t X_{t-k}]$.\n$$\n\\gamma(k) = E[X_t X_{t-k}] = E[(\\rho X_{t-1} + \\epsilon_t) X_{t-k}]\n$$\nBy linearity of expectation:\n$$\n\\gamma(k) = \\rho E[X_{t-1} X_{t-k}] + E[\\epsilon_t X_{t-k}]\n$$\nFor any lag $k > 0$, the time point $t-k$ is in the past relative to time $t$. By assumption, the noise term $\\epsilon_t$ is uncorrelated with all past values of the process $X$, so $E[\\epsilon_t X_{t-k}] = 0$. The equation simplifies to:\n$$\n\\gamma(k) = \\rho E[X_{t-1} X_{t-k}] = \\rho \\text{Cov}(X_{t-1}, X_{t-k})\n$$\nBy stationarity, $\\text{Cov}(X_{t-1}, X_{t-k}) = \\gamma((t-1) - (t-k)) = \\gamma(k-1)$. This yields the Yule-Walker equation for an AR(1) process:\n$$\n\\gamma(k) = \\rho \\gamma(k-1)\n$$\nThis is a recursive relationship. Starting with $k=1$:\n$\\gamma(1) = \\rho \\gamma(0)$\n$\\gamma(2) = \\rho \\gamma(1) = \\rho (\\rho \\gamma(0)) = \\rho^2 \\gamma(0)$\nBy induction, for any integer lag $k \\ge 0$, the autocovariance is:\n$$\n\\gamma(k) = \\rho^k \\gamma(0)\n$$\n\n3.  Derivation of the autocorrelation function $\\rho(k)$:\nThe ACF is defined as $\\rho(k) = \\frac{\\gamma(k)}{\\gamma(0)}$. Substituting the expression for $\\gamma(k)$:\n$$\n\\rho(k) = \\frac{\\rho^k \\gamma(0)}{\\gamma(0)} = \\rho^k \\quad \\text{for } k \\ge 0\n$$\nSince $\\rho(k) = \\rho(-k)$, the general form for any integer lag $k$ is $\\rho(k) = \\rho^{|k|}$.\n\n4.  Construction of the $4 \\times 4$ working correlation matrix $R(\\rho)$:\nThe matrix has elements $R_{ij} = \\text{Corr}(Y_{i-1}, Y_{j-1}) = \\rho(|(i-1)-(j-1)|) = \\rho^{|i-j|}$, for $i,j \\in \\{1, 2, 3, 4\\}$.\nThe diagonal elements are for lag $k=0$: $R_{ii} = \\rho^{|i-i|} = \\rho^0 = 1$.\nThe off-diagonal elements are:\n$R_{12} = R_{21} = \\rho^{|1-2|} = \\rho^1 = \\rho$\n$R_{13} = R_{31} = \\rho^{|1-3|} = \\rho^2$\n$R_{14} = R_{41} = \\rho^{|1-4|} = \\rho^3$\n$R_{23} = R_{32} = \\rho^{|2-3|} = \\rho^1 = \\rho$\n$R_{24} = R_{42} = \\rho^{|2-4|} = \\rho^2$\n$R_{34} = R_{43} = \\rho^{|3-4|} = \\rho^1 = \\rho$\n\nAssembling these elements into a $4 \\times 4$ matrix gives the AR(1) working correlation matrix:\n$$\nR(\\rho) = \\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\rho^3 \\\\\n\\rho & 1 & \\rho & \\rho^2 \\\\\n\\rho^2 & \\rho & 1 & \\rho \\\\\n\\rho^3 & \\rho^2 & \\rho & 1\n\\end{pmatrix}\n$$\nThis is a symmetric Toeplitz matrix, which is characteristic of stationary time series processes.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & \\rho & \\rho^2 & \\rho^3 \\\\\n\\rho & 1 & \\rho & \\rho^2 \\\\\n\\rho^2 & \\rho & 1 & \\rho \\\\\n\\rho^3 & \\rho^2 & \\rho & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building a statistical model is only half the battle; interpreting its results is where data turns into knowledge. This practice focuses on how the choice of a link function in a GEE for binary outcomes—specifically the log versus the logit link—critically shapes the interpretation of your findings, distinguishing between risk ratios and odds ratios. You will then use fitted models to make concrete predictions, a key step in applying your analysis to real-world scenarios .",
            "id": "4913822",
            "problem": "A respiratory infection surveillance study follows $n$ households over $T$ consecutive weeks, yielding correlated binary outcomes $Y_{it} \\in \\{0,1\\}$ indicating whether household $i$ reports at least one new infection in week $t$. Because weekly infections are uncommon, empirical marginal risks are small. To account for within-household correlation while modeling the marginal mean, analysts fit two Generalized Estimating Equations (GEE) with exchangeable working correlation: one with a log link and one with a logit link. Both models use the same covariate vector $X_{it} = (1, \\text{exposure}_{it}, \\text{age}_{i}^{*})^{\\top}$, where $\\text{exposure}_{it} \\in \\{0,1\\}$ indicates whether a targeted preventive action was taken during week $t$, and $\\text{age}_{i}^{*} = (\\text{age}_{i} - 40)/10$ scales age in decades centered at $40$ years. The within-household working correlation estimate is $\\hat{\\rho} = 0.20$.\n\nFrom the log-link GEE, the estimated regression coefficients are $\\hat{\\beta}^{\\log} = (\\hat{\\beta}^{\\log}_{0}, \\hat{\\beta}^{\\log}_{1}, \\hat{\\beta}^{\\log}_{2}) = (-4.70, 0.40, 0.03)$. From the logit-link GEE, the estimated regression coefficients are $\\hat{\\beta}^{\\mathrm{logit}} = (\\hat{\\beta}^{\\mathrm{logit}}_{0}, \\hat{\\beta}^{\\mathrm{logit}}_{1}, \\hat{\\beta}^{\\mathrm{logit}}_{2}) = (-4.90, 0.70, 0.05)$.\n\nUsing only first principles for marginal models with canonical link mappings, and beginning from the definition that a GEE specifies a mean model through a link function $g$ such that $g\\!\\left(\\mu_{it}\\right) = X_{it}^{\\top}\\beta$ for the marginal mean $\\mu_{it} = \\mathbb{E}(Y_{it} \\mid X_{it})$, do the following:\n\n1. Explain, in terms of the parameters of the marginal mean model, how the exposure effect differs in interpretability under the log link versus the logit link, specifically for rare events.\n2. For a household with $\\text{exposure}_{it} = 1$ and $\\text{age}_{i} = 30$ years, compute the predicted marginal risk under each fitted GEE. Use $X_{0} = (1, 1, -1)^{\\top}$ because $\\text{age}_{i}^{*} = (30 - 40)/10 = -1$. Round both predictions to four significant figures. Report the two predictions as a two-entry row matrix, with the first entry corresponding to the log-link GEE and the second entry corresponding to the logit-link GEE. No units are required.",
            "solution": "The problem asks for two tasks based on a GEE framework for correlated binary outcomes $Y_{it}$. The marginal mean $\\mu_{it} = \\mathbb{E}(Y_{it} \\mid X_{it})$ is modeled via a link function $g$ as $g(\\mu_{it}) = X_{it}^{\\top}\\beta$. Here, $\\mu_{it}$ represents the marginal risk of infection for household $i$ in week $t$.\n\n### Part 1: Interpretation of the Exposure Effect\n\nThe interpretability of the exposure effect, captured by the coefficient $\\beta_1$ corresponding to the $\\text{exposure}_{it}$ covariate, depends directly on the chosen link function $g$.\n\n**Log Link (Relative Risk Model)**\n\nFor the first GEE, the link function is the natural logarithm, $g(\\mu_{it}) = \\ln(\\mu_{it})$. The marginal mean model is therefore:\n$$\n\\ln(\\mu_{it}) = \\beta^{\\log}_{0} + \\beta^{\\log}_{1} \\cdot \\text{exposure}_{it} + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\n$$\nExponentiating both sides yields a model for the risk itself:\n$$\n\\mu_{it} = \\exp\\left(\\beta^{\\log}_{0} + \\beta^{\\log}_{1} \\cdot \\text{exposure}_{it} + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\\right)\n$$\nTo interpret $\\beta^{\\log}_{1}$, we examine the ratio of risks for an exposed individual ($\\text{exposure}_{it} = 1$) versus an unexposed individual ($\\text{exposure}_{it} = 0$), holding all other covariates constant. This ratio is the Risk Ratio (RR).\n$$\n\\text{RR} = \\frac{\\mu_{it}(\\text{exposure}_{it}=1)}{\\mu_{it}(\\text{exposure}_{it}=0)} = \\frac{\\exp\\left(\\beta^{\\log}_{0} + \\beta^{\\log}_{1}(1) + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\\right)}{\\exp\\left(\\beta^{\\log}_{0} + \\beta^{\\log}_{1}(0) + \\beta^{\\log}_{2} \\cdot \\text{age}_{i}^{*}\\right)} = \\exp(\\beta^{\\log}_{1})\n$$\nThus, $\\exp(\\beta^{\\log}_{1})$ is the Risk Ratio. The log-link model directly estimates the log of the risk ratio. The effect measure, $\\exp(\\beta^{\\log}_{1})$, is constant across all levels of the other covariates (in this case, age).\n\n**Logit Link (Logistic Regression Model)**\n\nFor the second GEE, the link function is the logit, $g(\\mu_{it}) = \\text{logit}(\\mu_{it}) = \\ln\\left(\\frac{\\mu_{it}}{1-\\mu_{it}}\\right)$. The term $\\frac{\\mu_{it}}{1-\\mu_{it}}$ is the odds of the outcome. The marginal mean model is:\n$$\n\\ln\\left(\\frac{\\mu_{it}}{1-\\mu_{it}}\\right) = \\beta^{\\mathrm{logit}}_{0} + \\beta^{\\mathrm{logit}}_{1} \\cdot \\text{exposure}_{it} + \\beta^{\\mathrm{logit}}_{2} \\cdot \\text{age}_{i}^{*}\n$$\nTo interpret $\\beta^{\\mathrm{logit}}_{1}$, we examine the ratio of odds for an exposed individual versus an unexposed one, holding other covariates constant. This is the Odds Ratio (OR).\n$$\n\\text{OR} = \\frac{\\text{odds}(\\text{exposure}_{it}=1)}{\\text{odds}(\\text{exposure}_{it}=0)} = \\frac{\\exp\\left(\\beta^{\\mathrm{logit}}_{0} + \\beta^{\\mathrm{logit}}_{1}(1) + \\beta^{\\mathrm{logit}}_{2} \\cdot \\text{age}_{i}^{*}\\right)}{\\exp\\left(\\beta^{\\mathrm{logit}}_{0} + \\beta^{\\mathrm{logit}}_{1}(0) + \\beta^{\\mathrm{logit}}_{2} \\cdot \\text{age}_{i}^{*}\\right)} = \\exp(\\beta^{\\mathrm{logit}}_{1})\n$$\nThus, $\\exp(\\beta^{\\mathrm{logit}}_{1})$ is the Odds Ratio. The logit-link model directly estimates the log of the odds ratio. This effect measure is also constant across all levels of the other covariates.\n\n**Comparison for Rare Events**\n\nThe problem states that the infections are uncommon, meaning the marginal risk $\\mu_{it}$ is small. When $\\mu_{it} \\to 0$, the denominator in the odds calculation, $1-\\mu_{it}$, approaches $1$. Consequently, the odds approximate the risk:\n$$\n\\text{odds} = \\frac{\\mu_{it}}{1-\\mu_{it}} \\approx \\mu_{it} = \\text{risk}\n$$\nFollowing this approximation, the Odds Ratio (OR) approximates the Risk Ratio (RR):\n$$\n\\text{OR} = \\frac{\\mu_1/(1-\\mu_1)}{\\mu_0/(1-\\mu_0)} = \\frac{\\mu_1}{\\mu_0} \\cdot \\frac{1-\\mu_0}{1-\\mu_1} = \\text{RR} \\cdot \\frac{1-\\mu_0}{1-\\mu_1} \\approx \\text{RR}\n$$\nTherefore, for rare events, $\\exp(\\beta^{\\mathrm{logit}}_{1})$ (the OR) is an approximation of the RR. In contrast, $\\exp(\\beta^{\\log}_{1})$ is a direct estimate of the RR, not an approximation. The log link provides a more direct and readily interpretable measure of relative risk, which is often the quantity of primary interest in epidemiology, especially for rare outcomes.\n\n### Part 2: Prediction of Marginal Risk\n\nWe are asked to compute the predicted marginal risk, $\\hat{\\mu}$, for a household with $\\text{exposure}_{it} = 1$ and $\\text{age}_{i} = 30$ years. The scaled age is $\\text{age}_{i}^{*} = (30-40)/10 = -1$. The covariate vector is $X_{0} = (1, 1, -1)^{\\top}$.\n\n**Log-link GEE Prediction**\n\nThe estimated linear predictor, $\\hat{\\eta}^{\\log}$, is calculated using the coefficients $\\hat{\\beta}^{\\log} = (-4.70, 0.40, 0.03)^{\\top}$:\n$$\n\\hat{\\eta}^{\\log} = X_{0}^{\\top}\\hat{\\beta}^{\\log} = (1)(-4.70) + (1)(0.40) + (-1)(0.03) = -4.70 + 0.40 - 0.03 = -4.33\n$$\nThe predicted risk, $\\hat{\\mu}^{\\log}$, is found by applying the inverse link function, which is the exponential function:\n$$\n\\hat{\\mu}^{\\log} = \\exp(\\hat{\\eta}^{\\log}) = \\exp(-4.33) \\approx 0.0131682\n$$\nRounding to four significant figures, the predicted risk is $0.01317$.\n\n**Logit-link GEE Prediction**\n\nThe estimated linear predictor, $\\hat{\\eta}^{\\mathrm{logit}}$, is calculated using the coefficients $\\hat{\\beta}^{\\mathrm{logit}} = (-4.90, 0.70, 0.05)^{\\top}$:\n$$\n\\hat{\\eta}^{\\mathrm{logit}} = X_{0}^{\\top}\\hat{\\beta}^{\\mathrm{logit}} = (1)(-4.90) + (1)(0.70) + (-1)(0.05) = -4.90 + 0.70 - 0.05 = -4.25\n$$\nThe predicted risk, $\\hat{\\mu}^{\\mathrm{logit}}$, is found by applying the inverse logit (logistic) function:\n$$\n\\hat{\\mu}^{\\mathrm{logit}} = \\frac{\\exp(\\hat{\\eta}^{\\mathrm{logit}})}{1 + \\exp(\\hat{\\eta}^{\\mathrm{logit}})} = \\frac{\\exp(-4.25)}{1 + \\exp(-4.25)}\n$$\nFirst, $\\exp(-4.25) \\approx 0.0142641$. Then,\n$$\n\\hat{\\mu}^{\\mathrm{logit}} = \\frac{0.0142641}{1 + 0.0142641} = \\frac{0.0142641}{1.0142641} \\approx 0.0140635\n$$\nRounding to four significant figures, the predicted risk is $0.01406$.\n\nThe two predictions, $0.01317$ (from the log-link model) and $0.01406$ (from the logit-link model), are indeed small, which is consistent with the problem's description of rare events. The final answer requires these two values in a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.01317 & 0.01406\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world longitudinal studies are rarely complete, as participant dropout leads to missing data that can bias results if ignored. This advanced exercise moves from idealized theory to robust practice by implementing Inverse Probability Weighting (IPW) within the GEE framework. You will learn to model the missing data process itself, use it to calculate weights, and then apply those weights to obtain a more accurate estimate of the exposure effect, reflecting a complete computational workflow for modern biostatistical analysis .",
            "id": "4913867",
            "problem": "You are given a small longitudinal dataset with monotone dropout and a binary outcome, with the goal of computing Inverse Probability Weighting (IPW) weights from a logistic missingness model and then performing one iteration of a weighted Generalized Estimating Equations (GEE) update for the regression parameter vector $\\beta$ under a working independence model with a canonical link for Bernoulli data. The task must be implemented as a complete, runnable program that produces a single line of output in the format specified below.\n\nDataset specification:\n- There are $n = 4$ subjects indexed by $i \\in \\{1,2,3,4\\}$ and $T = 3$ nominal time points indexed by $t \\in \\{0,1,2\\}$.\n- Each subject has a baseline binary covariate $x_i \\in \\{0,1\\}$, given by the vector $(x_1,x_2,x_3,x_4) = (0,1,0,1)$.\n- The binary outcome $y_{it} \\in \\{0,1\\}$ is observed only when $R_{it} = 1$, where $R_{it}$ is the indicator of observation at time $t$ for subject $i$.\n- The data are listed in the following order of rows: $(i,t)$ runs through $(1,0)$, $(1,1)$, $(1,2)$, $(2,0)$, $(2,1)$, $(2,2)$, $(3,0)$, $(3,1)$, $(3,2)$, $(4,0)$, $(4,1)$, $(4,2)$. For each row:\n  - Time $t$ is as given.\n  - Baseline $x$ is $x_i$ for subject $i$, replicated across times.\n  - The observation indicator $R_{it}$ and outcome $y_{it}$ (when $R_{it}=1$) are:\n    - Subject $1$ ($x_1 = 0$): $(t,R_{1t},y_{1t})$ equals $(0,1,0)$, $(1,1,1)$, $(2,0,\\text{missing})$.\n    - Subject $2$ ($x_2 = 1$): $(t,R_{2t},y_{2t})$ equals $(0,1,1)$, $(1,1,1)$, $(2,1,1)$.\n    - Subject $3$ ($x_3 = 0$): $(t,R_{3t},y_{3t})$ equals $(0,1,0)$, $(1,0,\\text{missing})$, $(2,0,\\text{missing})$.\n    - Subject $4$ ($x_4 = 1$): $(t,R_{4t},y_{4t})$ equals $(0,1,1)$, $(1,1,0)$, $(2,0,\\text{missing})$.\n\nFoundational definitions and requirements:\n- The observation indicators $R_{it}$ are modeled using a logistic regression (Bernoulli likelihood) missingness model with linear predictor $\\eta^{(R)}_{it} = z_{it}^{\\top}\\alpha$ and probability $\\Pr(R_{it}=1 \\mid z_{it}) = \\pi_{it} = \\exp(\\eta^{(R)}_{it})/(1+\\exp(\\eta^{(R)}_{it}))$, where $z_{it}$ is a specified covariate vector for missingness modeling and $\\alpha$ is estimated by maximum likelihood.\n- The IPW weight for an observed outcome is defined as $w_{it} = 1/\\hat{\\pi}_{it}$ for $R_{it} = 1$. Rows with $R_{it} = 0$ are excluded from the GEE estimating step.\n- The marginal mean model for the binary outcome uses the canonical link for the Bernoulli distribution with a working independence correlation:\n  - $\\operatorname{logit}(\\mu_{it}) = \\eta_{it} = x^{\\top}_{it}\\beta$, where $x_{it}$ is the covariate vector for the marginal mean model and $\\beta$ is the regression parameter vector to be estimated.\n  - For Bernoulli responses, the variance function is $V(\\mu_{it}) = \\mu_{it}(1-\\mu_{it})$, and the derivative of the mean with respect to the linear predictor satisfies $d\\mu_{it}/d\\eta_{it} = \\mu_{it}(1-\\mu_{it})$ under the canonical logit link.\n- One Fisher scoring (iteratively reweighted least squares) iteration for $\\beta$ is to be performed starting from a given initial value $\\beta^{(0)}$. This update must use:\n  - Only rows with $R_{it}=1$.\n  - The IPW weights $w_{it}$ obtained from the fitted missingness model.\n  - The working independence structure.\n  - The canonical link for Bernoulli data.\n\nImplementational constraints:\n- You must compute the missingness-model maximum likelihood estimate $\\hat{\\alpha}$ by numerical optimization using Newton–Raphson or Fisher scoring on the Bernoulli log-likelihood, based on all person-time rows (both $R_{it}=1$ and $R_{it}=0$), and then compute the fitted probabilities $\\hat{\\pi}_{it}$ and IPW weights $w_{it}=1/\\hat{\\pi}_{it}$ for the rows with $R_{it}=1$.\n- Then, using the outcome-model covariate design for the observed rows, perform a single Fisher scoring or Newton step for $\\beta$ starting at $\\beta^{(0)}$.\n\nCovariate designs:\n- The marginal mean model for the binary outcome uses $x_{it} = (1, t, x_i)^{\\top}$ for every observed row.\n- The missingness model uses one of the following covariate sets for $z_{it}$, depending on the test case:\n  - Case $1$: $z_{it} = (1, t, x_i)^{\\top}$.\n  - Case $2$: $z_{it} = (1, t)^{\\top}$.\n  - Case $3$: $z_{it} = (1, t, x_i, t \\cdot x_i)^{\\top}$.\n\nTest suite:\n- For each case, perform the full procedure (fit the missingness model, compute IPW weights, and then perform one iteration of the weighted GEE update for $\\beta$) starting from the specified initial value $\\beta^{(0)}$:\n  - Case $1$: $\\beta^{(0)} = (0, 0, 0)^{\\top}$ and $z_{it} = (1, t, x_i)^{\\top}$.\n  - Case $2$: $\\beta^{(0)} = (-1, 0.5, 0.5)^{\\top}$ and $z_{it} = (1, t)^{\\top}$.\n  - Case $3$: $\\beta^{(0)} = (1, -0.5, -0.5)^{\\top}$ and $z_{it} = (1, t, x_i, t \\cdot x_i)^{\\top}$.\n\nWhat to compute and output:\n- For each case, compute the one-step updated parameter vector $\\beta^{(1)}$ as a length-$3$ vector corresponding to $(\\beta_0,\\beta_1,\\beta_2)^{\\top}$ in the outcome model $\\operatorname{logit}(\\mu_{it}) = \\beta_0 + \\beta_1 t + \\beta_2 x_i$.\n- Round each component of each $\\beta^{(1)}$ to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list representation of the updated $\\beta^{(1)}$ for the corresponding case, in order. For example, the output must have the form $[[b_{01},b_{11},b_{21}],[b_{02},b_{12},b_{22}],[b_{03},b_{13},b_{23}]]$ with no spaces anywhere in the line.\n\nNotes:\n- Use the working independence structure and assume unit dispersion for the Bernoulli variance function.\n- Use numerically stable computations: for the logistic function, ensure probabilities are kept within $(0,1)$ by appropriate clipping if necessary to avoid division by zero. All computations are unitless; no physical units apply.",
            "solution": "The user requests a one-step update for the parameter vector $\\beta$ of a marginal mean model for longitudinal binary data, adjusted for missingness using Inverse Probability Weighting (IPW) within a Generalized Estimating Equations (GEE) framework. The procedure involves two primary stages: first, estimating the missingness probabilities and constructing weights; second, using these weights in a single Fisher scoring iteration to update an initial estimate for $\\beta$.\n\n## Stage 1: Missingness Model and Inverse Probability Weighting\n\nThe problem specifies a monotone dropout pattern in the longitudinal data. The observation process for outcome $y_{it}$ is represented by the indicator $R_{it}$, where $R_{it}=1$ if the outcome is observed and $R_{it}=0$ otherwise. A logistic regression model is assumed for the probability of being observed at time $t$ for subject $i$:\n$$\n\\Pr(R_{it}=1 \\mid z_{it}) = \\pi_{it}(\\alpha) = \\frac{\\exp(z_{it}^{\\top}\\alpha)}{1+\\exp(z_{it}^{\\top}\\alpha)}\n$$\nwhere $z_{it}$ is a vector of covariates associated with missingness and $\\alpha$ is the vector of regression parameters for this model.\n\nThe parameter vector $\\alpha$ is estimated via Maximum Likelihood Estimation (MLE) using all person-time observations ($N=12$), including those with missing outcomes. The log-likelihood function for this Bernoulli model is:\n$$\n\\ell(\\alpha) = \\sum_{i=1}^{n} \\sum_{t=0}^{T-1} \\left[ R_{it} (z_{it}^{\\top}\\alpha) - \\log(1+\\exp(z_{it}^{\\top}\\alpha)) \\right]\n$$\nThe MLE $\\hat{\\alpha}$ is found by solving the score equation $S(\\alpha) = \\nabla_{\\alpha} \\ell(\\alpha) = 0$. This is done numerically using the Fisher scoring algorithm, which is equivalent to Newton-Raphson for the canonical link. The iterative update is:\n$$\n\\alpha^{(k+1)} = \\alpha^{(k)} + [I(\\alpha^{(k)})]^{-1} S(\\alpha^{(k)})\n$$\nwhere $S(\\alpha)$ is the score vector and $I(\\alpha)$ is the Fisher information matrix:\n$$\nS(\\alpha) = \\sum_{i,t} (R_{it} - \\pi_{it}(\\alpha)) z_{it} = Z^{\\top}(R - \\pi(\\alpha))\n$$\n$$\nI(\\alpha) = \\sum_{i,t} \\pi_{it}(\\alpha)(1-\\pi_{it}(\\alpha)) z_{it}z_{it}^{\\top} = Z^{\\top} W_R(\\alpha) Z\n$$\nHere, $Z$ is the design matrix for the missingness model, $R$ is the vector of observation indicators, $\\pi(\\alpha)$ is the vector of probabilities, and $W_R(\\alpha)$ is a diagonal matrix with entries $\\pi_{it}(1-\\pi_{it})$. The iteration proceeds until convergence to $\\hat{\\alpha}$.\n\nOnce $\\hat{\\alpha}$ is obtained, the estimated probabilities of being observed, $\\hat{\\pi}_{it} = \\pi_{it}(\\hat{\\alpha})$, are calculated. The IPW for each observed outcome ($R_{it}=1$) is then $w_{it} = 1/\\hat{\\pi}_{it}$.\n\n## Stage 2: Weighted GEE Update\n\nThe marginal mean of the binary outcome $y_{it}$ is modeled as:\n$$\n\\operatorname{logit}(\\mu_{it}) = \\operatorname{logit}(E[Y_{it} \\mid x_{it}]) = x_{it}^{\\top}\\beta\n$$\nwhere $x_{it} = (1, t, x_i)^{\\top}$ is the covariate vector for the outcome model and $\\beta = (\\beta_0, \\beta_1, \\beta_2)^{\\top}$ is the parameter vector of interest.\n\nThe GEE framework is used to estimate $\\beta$. To account for missing data, the standard GEE is modified by incorporating the IPW weights. For a working independence correlation structure, the IPW-GEE estimating equation is:\n$$\nU(\\beta) = \\sum_{i=1}^{n} \\sum_{t=0}^{T-1} \\frac{R_{it}}{\\hat{\\pi}_{it}} \\left(\\frac{\\partial \\mu_{it}}{\\partial \\beta}\\right)^{\\top} V(\\mu_{it})^{-1} (y_{it} - \\mu_{it}) = 0\n$$\nwhere $V(\\mu_{it}) = \\mu_{it}(1-\\mu_{it})$ is the variance of a Bernoulli response. Since the model uses the canonical logit link, we have the simplification $\\frac{\\partial \\mu_{it}}{\\partial \\beta} = \\mu_{it}(1-\\mu_{it})x_{it} = V(\\mu_{it})x_{it}$. The estimating equation thus reduces to:\n$$\nU(\\beta) = \\sum_{i, t \\text{ s.t. } R_{it}=1} w_{it} x_{it} (y_{it} - \\mu_{it}) = 0\n$$\nThe problem requires a single Fisher scoring update for $\\beta$ from a given initial value $\\beta^{(0)}$:\n$$\n\\beta^{(1)} = \\beta^{(0)} + [I(\\beta^{(0)})]^{-1} U(\\beta^{(0)})\n$$\nThe \"Hessian\" or sensitivity matrix $I(\\beta)$ in the GEE context (for a working independence model) is given by:\n$$\nI(\\beta) = \\sum_{i, t \\text{ s.t. } R_{it}=1} w_{it} \\left(\\frac{\\partial \\mu_{it}}{\\partial \\beta}\\right)^{\\top} V(\\mu_{it})^{-1} \\left(\\frac{\\partial \\mu_{it}}{\\partial \\beta}\\right) = \\sum_{i, t \\text{ s.t. } R_{it}=1} w_{it} \\mu_{it}(1-\\mu_{it}) x_{it}x_{it}^{\\top}\n$$\nThe procedure for each test case is as follows:\n1.  Construct the design matrix $Z$ for the specified missingness model.\n2.  Fit the logistic model of $R$ on $Z$ to find $\\hat{\\alpha}$.\n3.  Calculate weights $w_{it} = 1/\\hat{\\pi}_{it}$ for all observed data points.\n4.  Using the provided initial $\\beta^{(0)}$ and the observed data ($y_{it}, x_{it}$), calculate $\\mu_{it}(\\beta^{(0)})$.\n5.  Compute the score vector $U(\\beta^{(0)})$ and the sensitivity matrix $I(\\beta^{(0)})$.\n6.  Compute the one-step update $\\beta^{(1)} = \\beta^{(0)} + [I(\\beta^{(0)})]^{-1} U(\\beta^{(0)})$.\n7.  Round the components of $\\beta^{(1)}$ to $6$ decimal places.\n\nThis entire process is repeated for each of the three test cases, which differ in their specification of the missingness model covariates $z_{it}$ and the initial parameter vector $\\beta^{(0)}$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a one-step IPW-GEE update for the regression parameter vector beta\n    for three different test cases.\n    \"\"\"\n    \n    # --- Dataset Specification ---\n    # Rows correspond to (i,t) = (1,0),(1,1),(1,2), (2,0),(2,1),(2,2), etc.\n    t_all = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2], dtype=float)\n    x_i_all = np.array([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1], dtype=float)\n    R_all = np.array([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0], dtype=float)\n    y_all = np.array([0, 1, np.nan, 1, 1, 1, 0, np.nan, np.nan, 1, 0, np.nan])\n\n    # --- Test Suite ---\n    test_cases = [\n        {'beta0': np.array([0.0, 0.0, 0.0]), 'z_spec': ['1', 't', 'x']},\n        {'beta0': np.array([-1.0, 0.5, 0.5]), 'z_spec': ['1', 't']},\n        {'beta0': np.array([1.0, -0.5, -0.5]), 'z_spec': ['1', 't', 'x', 't*x']}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- Part 1: Fit Missingness Model and Compute Weights ---\n\n        # 1.1 Construct design matrix Z for the missingness model\n        z_cols = []\n        for spec in case['z_spec']:\n            if spec == '1':\n                z_cols.append(np.ones_like(t_all))\n            elif spec == 't':\n                z_cols.append(t_all)\n            elif spec == 'x':\n                z_cols.append(x_i_all)\n            elif spec == 't*x':\n                z_cols.append(t_all * x_i_all)\n        Z = np.stack(z_cols, axis=1)\n\n        # 1.2 Fit logistic regression for alpha using Fisher scoring\n        def fit_logistic_regression(X, y, max_iter=100, tol=1e-9):\n            alpha = np.zeros(X.shape[1])\n            eps = 1e-12\n            for _ in range(max_iter):\n                eta = X @ alpha\n                pi = 1 / (1 + np.exp(-eta))\n                pi = np.clip(pi, eps, 1 - eps)\n                \n                score = X.T @ (y - pi)\n                W_diag = pi * (1 - pi)\n                hessian = X.T @ (W_diag[:, np.newaxis] * X)\n                \n                try:\n                    delta = np.linalg.solve(hessian, score)\n                except np.linalg.LinAlgError:\n                    delta = np.linalg.pinv(hessian) @ score\n                \n                alpha += delta\n                if np.linalg.norm(delta) < tol:\n                    break\n            return alpha\n\n        alpha_hat = fit_logistic_regression(Z, R_all)\n\n        # 1.3 Compute IPW weights for observed data\n        pi_hat_all = 1 / (1 + np.exp(-(Z @ alpha_hat)))\n        \n        obs_idx = R_all == 1\n        pi_hat_obs = pi_hat_all[obs_idx]\n        w_obs = 1 / pi_hat_obs\n\n        # --- Part 2: One-Step Weighted GEE Update for beta ---\n\n        # 2.1 Prepare data for the GEE step (observed data only)\n        Y_obs = y_all[obs_idx]\n        t_obs = t_all[obs_idx]\n        x_i_obs = x_i_all[obs_idx]\n        \n        X_obs = np.stack([np.ones_like(t_obs), t_obs, x_i_obs], axis=1)\n        \n        beta_0 = case['beta0']\n\n        # 2.2 Perform one Fisher scoring update\n        eta_Y = X_obs @ beta_0\n        mu = 1 / (1 + np.exp(-eta_Y))\n        \n        # Calculate score vector U(beta)\n        residuals = Y_obs - mu\n        U_beta = X_obs.T @ (w_obs * residuals)\n\n        # Calculate sensitivity matrix I(beta)\n        v = mu * (1 - mu)\n        W_Y_diag = w_obs * v\n        I_beta = X_obs.T @ (W_Y_diag[:, np.newaxis] * X_obs)\n        \n        # Calculate update and new beta\n        update = np.linalg.solve(I_beta, U_beta)\n        beta_1 = beta_0 + update\n        \n        results.append(beta_1)\n\n    # --- Final Formatting and Output ---\n    final_results_list = [np.round(res, 6).tolist() for res in results]\n    print(str(final_results_list).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}