{
    "hands_on_practices": [
        {
            "introduction": "Principal Component Analysis is not just a mathematical procedure; it is a powerful tool for visualizing the underlying structure in complex datasets. The eigenvectors of the covariance matrix, which define the principal components, serve as new coordinate axes aligned with the directions of greatest variance. This first practice is designed to build your intuition by connecting the abstract concept of eigenvectors to the tangible shape of a data cloud, helping you interpret PCA plots visually. ",
            "id": "1383893",
            "problem": "A data scientist is performing Principal Component Analysis (PCA) on a large, two-dimensional dataset where each data point is represented by a vector $[x, y]^T$. After centering the data by subtracting the mean, the $2 \\times 2$ sample covariance matrix is computed. The analysis reveals that the eigenvectors of this covariance matrix are $\\mathbf{v}_1 = [1, 1]^T$ and $\\mathbf{v}_2 = [-1, 1]^T$. Furthermore, it is found that the eigenvalue corresponding to $\\mathbf{v}_1$ is substantially larger than the eigenvalue corresponding to $\\mathbf{v}_2$.\n\nBased on this information, which of the following statements provides the most plausible description of the geometric distribution of the centered data points in the $xy$-plane?\n\nA. The data points are distributed in a roughly circular cloud centered at the origin.\n\nB. The data points form an elongated cloud primarily aligned with the y-axis.\n\nC. The data points form an elongated cloud whose major axis is aligned with the line $y = -x$.\n\nD. The data points form an elongated cloud whose major axis is aligned with the line $y = x$.\n\nE. The data points are concentrated in two distinct clusters, one in the first quadrant and one in the third quadrant.",
            "solution": "Let the centered data points be $\\{\\mathbf{x}_{i}\\}_{i=1}^{n}$ with $\\mathbf{x}_{i} \\in \\mathbb{R}^{2}$ and sample covariance matrix\n$$\nS=\\frac{1}{n-1}\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{T}.\n$$\nPrincipal Component Analysis diagonalizes $S$ via eigen-decomposition. The eigenvectors $\\{\\mathbf{v}_{k}\\}$ and eigenvalues $\\{\\lambda_{k}\\}$ satisfy\n$$\nS\\mathbf{v}_{k}=\\lambda_{k}\\mathbf{v}_{k},\n$$\nwhere each $\\lambda_{k}$ equals the variance of the data projected onto direction $\\mathbf{v}_{k}$. The geometric shape of the centered point cloud is well approximated by an ellipse whose principal axes are aligned with the eigenvectors of $S$, with axis lengths proportional to $\\sqrt{\\lambda_{k}}$. The largest eigenvalue determines the major axis direction.\n\nWe are given eigenvectors $\\mathbf{v}_{1}=[1,1]^{T}$ and $\\mathbf{v}_{2}=[-1,1]^{T}$ with the eigenvalue corresponding to $\\mathbf{v}_{1}$ substantially larger than that corresponding to $\\mathbf{v}_{2}$. The direction $\\mathbf{v}_{1}=[1,1]^{T}$ corresponds to the line $y=x$ because for a direction vector $[a,b]^{T}$, the aligned line has slope $b/a$. Thus $[1,1]^{T}$ yields $y=(1/1)x=x$. Similarly, $\\mathbf{v}_{2}=[-1,1]^{T}$ corresponds to $y=(-1)x$, i.e., $y=-x$. Since $\\lambda_{1}\\gg\\lambda_{2}$, the variance is much larger along $\\mathbf{v}_{1}$, so the major axis of the data cloud aligns with $y=x$.\n\nTherefore, the most plausible description is that the centered data form an elongated cloud whose major axis is aligned with the line $y=x$, which corresponds to option D. Options A and B are inconsistent with the eigenstructure (A would require approximately equal eigenvalues; B would require a principal direction aligned with the y-axis), and option C would require the largest eigenvalue to be associated with $\\mathbf{v}_{2}$. Option E cannot be inferred from covariance alone and is not the most direct implication of the given PCA results.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "To truly master PCA, it is invaluable to perform the core calculations by hand before relying on automated software packages. This exercise demystifies the algorithm by guiding you through a complete, step-by-step analysis of a small, hypothetical gene expression dataset. By manually centering the data, computing the sample covariance matrix, and extracting the first principal component, you will gain a concrete understanding of how PCA transforms raw data into meaningful, low-dimensional representations. ",
            "id": "2416060",
            "problem": "A gene expression experiment measured log-transformed expression values for $G_1$, $G_2$, and $G_3$ across $S_1$, $S_2$, $S_3$, and $S_4$. The data matrix $X \\in \\mathbb{R}^{4 \\times 3}$ has samples as rows and genes as columns:\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\nTreat samples as independent observations and genes as variables. Using principal component analysis (PCA), compute the first principal component loading vector in gene space by:\n- column-centering $X$,\n- forming the sample covariance matrix across genes with denominator $n-1$ for $n=4$ samples, and\n- taking the unit-norm eigenvector of this covariance matrix corresponding to the largest eigenvalue.\n\nReport the loading vector as a $1 \\times 3$ row matrix ordered as $(G_1, G_2, G_3)$, with the sign chosen so that its first nonzero entry is positive. No rounding is required.",
            "solution": "We are asked to compute the first principal component loading vector in gene space using the eigen-decomposition of the sample covariance matrix across genes. Let $n=4$ be the number of samples and $p=3$ be the number of genes. The data matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows as samples and columns as genes.\n\nStep $1$: Column-centering. Compute the column means of $X$:\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\nSubtract these means from each column to obtain the centered matrix $Z$:\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\nStep $2$: Sample covariance matrix across genes. Using the denominator $n-1=3$, the sample covariance of genes is\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\nObserve that each row of $Z$ is a scalar multiple of $(1,\\,1,\\,1)$, so all three centered gene columns are identical. Compute $Z^{\\top}Z$ by noting that for any two columns $j$ and $k$, the $(j,k)$ entry equals $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$. Since all three columns are identical, every entry of $Z^{\\top}Z$ equals the sum of squares of a single centered column:\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\nTherefore,\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\nStep $3$: Eigen-decomposition. Let $J \\in \\mathbb{R}^{3 \\times 3}$ denote the all-ones matrix, i.e., $J_{jk}=1$ for all $j,k$. It is known from first principles that $J$ has rank $1$ with eigenvalues $3$ and $0$ (with multiplicity $2$). A corresponding unit-norm eigenvector for the eigenvalue $3$ is proportional to $(1,\\,1,\\,1)^{\\top}$, specifically\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nSince $S = \\frac{5}{3} J$, the eigenvalues of $S$ are $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ and $\\lambda_{2} = 0$, $\\lambda_{3} = 0$, with the same eigenvectors as $J$. Thus, the first principal component loading vector in gene space is the unit-norm eigenvector associated with $\\lambda_{1}=5$, namely $v$ as above. Choosing the sign so that the first nonzero entry is positive yields\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\nTherefore, ordered as $(G_1, G_2, G_3)$ and written as a $1 \\times 3$ row matrix, the first principal component loading vector is\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "A critical and often overlooked property of PCA is its sensitivity to the scale of the original variables. Since PCA seeks to maximize variance, variables with larger numerical ranges can dominate the analysis, even if their underlying importance is no greater. This exercise demonstrates how simply rescaling one variable can dramatically alter the direction of the principal components, emphasizing the practical importance of data standardization as a routine preprocessing step in biostatistical analysis. ",
            "id": "1946276",
            "problem": "A two-dimensional dataset is represented by a pair of centered (mean-zero) random variables, $(X_1, X_2)$. The statistical properties of this dataset are fully described by its covariance matrix $\\Sigma$, given by:\n$$\n\\Sigma = \\begin{pmatrix} 25 & 12 \\\\ 12 & 10 \\end{pmatrix}\n$$\nwhere the diagonal elements represent the variances of $X_1$ and $X_2$ respectively, and the off-diagonal elements represent their covariance.\n\nA data analyst creates a new dataset, $(Y_1, Y_2)$, by applying a linear transformation to the original data: $Y_1 = 10X_1$ and $Y_2 = X_2$. Principal Component Analysis (PCA) is then performed on this new, transformed dataset.\n\nThe first principal component is defined as the eigenvector of the covariance matrix of $(Y_1, Y_2)$ that corresponds to the largest eigenvalue. This eigenvector represents a direction in the $(Y_1, Y_2)$ plane.\n\nDetermine the angle that the first principal component vector of the transformed data makes with the positive $Y_1$-axis. The angle should be taken in the counter-clockwise direction. Express your answer in degrees, rounded to three significant figures.",
            "solution": "The first step is to determine the covariance matrix of the transformed dataset $(Y_1, Y_2)$. Let the new covariance matrix be $\\Sigma'$. The elements of $\\Sigma'$ are calculated based on the properties of variance and covariance.\n\nThe variance of $Y_1$ is:\n$$\n\\text{Var}(Y_1) = \\text{Var}(10X_1) = 10^2 \\text{Var}(X_1)\n$$\nFrom the original covariance matrix $\\Sigma$, we know that $\\text{Var}(X_1) = 25$. Therefore:\n$$\n\\text{Var}(Y_1) = 100 \\times 25 = 2500\n$$\nThe variance of $Y_2$ is unchanged:\n$$\n\\text{Var}(Y_2) = \\text{Var}(X_2) = 10\n$$\nThe covariance between $Y_1$ and $Y_2$ is:\n$$\n\\text{Cov}(Y_1, Y_2) = \\text{Cov}(10X_1, X_2) = 10 \\text{Cov}(X_1, X_2)\n$$\nFrom the original covariance matrix $\\Sigma$, we know that $\\text{Cov}(X_1, X_2) = 12$. Therefore:\n$$\n\\text{Cov}(Y_1, Y_2) = 10 \\times 12 = 120\n$$\nWith these components, the new covariance matrix $\\Sigma'$ for the transformed data is:\n$$\n\\Sigma' = \\begin{pmatrix} 2500 & 120 \\\\ 120 & 10 \\end{pmatrix}\n$$\nThe principal components are the eigenvectors of this covariance matrix. Let $\\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ be an eigenvector of $\\Sigma'$ corresponding to an eigenvalue $\\lambda$. The eigenvalue equation is $\\Sigma' \\mathbf{v} = \\lambda \\mathbf{v}$, which can be written as $(\\Sigma' - \\lambda I)\\mathbf{v} = \\mathbf{0}$.\n\nTo find the eigenvalues, we solve the characteristic equation $\\det(\\Sigma' - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 2500 - \\lambda & 120 \\\\ 120 & 10 - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(2500 - \\lambda)(10 - \\lambda) - (120)^2 = 0\n$$\n$$\n25000 - 2500\\lambda - 10\\lambda + \\lambda^2 - 14400 = 0\n$$\n$$\n\\lambda^2 - 2510\\lambda + 10600 = 0\n$$\nThe first principal component corresponds to the eigenvector associated with the largest eigenvalue, $\\lambda_1$. We can find $\\lambda_1$ using the quadratic formula:\n$$\n\\lambda_1 = \\frac{-(-2510) + \\sqrt{(-2510)^2 - 4(1)(10600)}}{2} = \\frac{2510 + \\sqrt{6300100 - 42400}}{2}\n$$\n$$\n\\lambda_1 = \\frac{2510 + \\sqrt{6257700}}{2}\n$$\nNow, we find the corresponding eigenvector $\\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ by solving $(\\Sigma' - \\lambda_1 I)\\mathbf{v} = \\mathbf{0}$. This gives a system of two linear equations. We only need one to find the direction of the vector. Using the second row:\n$$\n120v_1 + (10 - \\lambda_1)v_2 = 0\n$$\nThe slope of the eigenvector in the $(Y_1, Y_2)$ plane is $m = \\frac{v_2}{v_1}$. Rearranging the equation gives:\n$$\nm = \\frac{v_2}{v_1} = \\frac{-120}{10 - \\lambda_1} = \\frac{120}{\\lambda_1 - 10}\n$$\nSubstitute the expression for $\\lambda_1$:\n$$\n\\lambda_1 - 10 = \\frac{2510 + \\sqrt{6257700}}{2} - 10 = \\frac{2490 + \\sqrt{6257700}}{2}\n$$\nSo the slope is:\n$$\nm = \\frac{120}{\\frac{2490 + \\sqrt{6257700}}{2}} = \\frac{240}{2490 + \\sqrt{6257700}}\n$$\nNow we compute the numerical value of the slope:\n$$\n\\sqrt{6257700} \\approx 2501.5395\n$$\n$$\nm \\approx \\frac{240}{2490 + 2501.5395} = \\frac{240}{4991.5395} \\approx 0.0480836\n$$\nThe angle $\\theta$ that this vector makes with the positive $Y_1$-axis is given by the arctangent of the slope:\n$$\n\\theta = \\arctan(m) = \\arctan(0.0480836)\n$$\nTo express the angle in degrees, we convert from radians:\n$$\n\\theta_{\\text{deg}} = \\arctan(0.0480836) \\times \\frac{180}{\\pi} \\approx 2.7543^\\circ\n$$\nRounding the result to three significant figures, we get $2.75$ degrees.",
            "answer": "$$\\boxed{2.75}$$"
        }
    ]
}