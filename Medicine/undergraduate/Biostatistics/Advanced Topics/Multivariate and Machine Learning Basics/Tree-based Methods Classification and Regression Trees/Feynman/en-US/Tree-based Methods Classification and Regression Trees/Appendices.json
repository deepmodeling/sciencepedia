{
    "hands_on_practices": [
        {
            "introduction": "The power of a classification tree lies in its ability to recursively partition the data into purer, more homogeneous subsets. This process relies on a core calculation at every step: evaluating how much a potential split improves node purity. This exercise  will give you hands-on practice with the Gini impurity, a standard metric used to quantify the quality of a split in a classification setting, forming the fundamental building block of the entire tree construction algorithm.",
            "id": "4962711",
            "problem": "In a biostatistical analysis using Classification and Regression Trees (CART) for a binary outcome with classes labeled $1$ and $2$, consider a parent node containing $n_1=20$ observations from class $1$ and $n_2=30$ observations from class $2$. A candidate split partitions this parent node into a left child with counts $(15,5)$ and a right child with counts $(5,25)$ for classes $(1,2)$, respectively. Using the Gini impurity as the node impurity measure and the standard weighted reduction in impurity criterion for evaluating a split, compute the impurity decrease achieved by this split. Express your final answer as an exact fraction with no rounding.",
            "solution": "The user has provided a problem from the field of biostatistics, specifically concerning Classification and Regression Trees (CART). The task is to calculate the decrease in impurity resulting from a specific split of a parent node into two child nodes, using the Gini impurity measure.\n\n### Step 1: Problem Validation\n\nFirst, I must validate the problem statement.\n\n**Extraction of Givens:**\n*   **Methodology**: Classification and Regression Trees (CART)\n*   **Outcome**: Binary, with classes labeled $1$ and $2$.\n*   **Parent Node Composition**: Contains $n_1 = 20$ observations of class $1$ and $n_2 = 30$ observations of class $2$.\n*   **Split Definition**:\n    *   **Left Child Node Composition**: $n_{L,1} = 15$ (class $1$), $n_{L,2} = 5$ (class $2$).\n    *   **Right Child Node Composition**: $n_{R,1} = 5$ (class $1$), $n_{R,2} = 25$ (class $2$).\n*   **Impurity Measure**: Gini impurity.\n*   **Evaluation Criterion**: Weighted reduction in impurity.\n\n**Validation against Criteria:**\n1.  **Scientific Grounding**: The problem is well-grounded in the theory of decision trees, a standard topic in machine learning and biostatistics. The Gini impurity is a widely used metric for classification trees.\n2.  **Well-Posedness**: The problem provides all necessary data and definitions to compute a unique numerical answer. The request for an \"exact fraction\" is clear.\n3.  **Objectivity**: The problem is stated in precise, quantitative terms, free of any subjectivity.\n4.  **Consistency Check**:\n    *   Total observations in parent node: $N_P = n_1 + n_2 = 20 + 30 = 50$.\n    *   Total observations in left child: $N_L = n_{L,1} + n_{L,2} = 15 + 5 = 20$.\n    *   Total observations in right child: $N_R = n_{R,1} + n_{R,2} = 5 + 25 = 30$.\n    *   Conservation of observations: $N_L + N_R = 20 + 30 = 50 = N_P$.\n    *   Conservation of class 1: $n_{L,1} + n_{R,1} = 15 + 5 = 20 = n_1$.\n    *   Conservation of class 2: $n_{L,2} + n_{R,2} = 5 + 25 = 30 = n_2$.\n    The data are self-consistent.\n\n**Verdict**: The problem is valid, scientifically sound, well-posed, and internally consistent. I will now proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe impurity decrease, also known as the Gini gain, for a split is calculated as the impurity of the parent node minus the weighted average of the impurities of the child nodes.\nThe formula for the impurity decrease, $\\Delta I_{Gini}$, is:\n$$ \\Delta I_{Gini} = I_{Gini}(\\text{parent}) - \\left[ \\frac{N_L}{N_P} I_{Gini}(\\text{left}) + \\frac{N_R}{N_P} I_{Gini}(\\text{right}) \\right] $$\nwhere $I_{Gini}$ is the Gini impurity of a node, and $N_P$, $N_L$, $N_R$ are the total number of observations in the parent, left, and right nodes, respectively.\n\nThe Gini impurity for a node $m$ with $K$ classes is defined as:\n$$ I_{Gini}(m) = 1 - \\sum_{k=1}^{K} p_{mk}^2 $$\nwhere $p_{mk}$ is the proportion of observations of class $k$ in node $m$. For this binary problem ($K=2$), the formula is:\n$$ I_{Gini}(m) = 1 - (p_{m1}^2 + p_{m2}^2) $$\n\n**1. Calculate the Gini Impurity of the Parent Node ($I_{Gini}(\\text{parent})$)**\nThe parent node has $N_P = 20 + 30 = 50$ observations.\nThe proportions of the classes are:\n*   $p_{P,1} = \\frac{n_1}{N_P} = \\frac{20}{50} = \\frac{2}{5}$\n*   $p_{P,2} = \\frac{n_2}{N_P} = \\frac{30}{50} = \\frac{3}{5}$\n\nThe Gini impurity of the parent node is:\n$$ I_{Gini}(\\text{parent}) = 1 - \\left[ \\left(\\frac{2}{5}\\right)^2 + \\left(\\frac{3}{5}\\right)^2 \\right] = 1 - \\left[ \\frac{4}{25} + \\frac{9}{25} \\right] = 1 - \\frac{13}{25} = \\frac{12}{25} $$\n\n**2. Calculate the Gini Impurity of the Left Child Node ($I_{Gini}(\\text{left})$)**\nThe left child node has $N_L = 15 + 5 = 20$ observations.\nThe proportions of the classes are:\n*   $p_{L,1} = \\frac{n_{L,1}}{N_L} = \\frac{15}{20} = \\frac{3}{4}$\n*   $p_{L,2} = \\frac{n_{L,2}}{N_L} = \\frac{5}{20} = \\frac{1}{4}$\n\nThe Gini impurity of the left child node is:\n$$ I_{Gini}(\\text{left}) = 1 - \\left[ \\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 \\right] = 1 - \\left[ \\frac{9}{16} + \\frac{1}{16} \\right] = 1 - \\frac{10}{16} = 1 - \\frac{5}{8} = \\frac{3}{8} $$\n\n**3. Calculate the Gini Impurity of the Right Child Node ($I_{Gini}(\\text{right})$)**\nThe right child node has $N_R = 5 + 25 = 30$ observations.\nThe proportions of the classes are:\n*   $p_{R,1} = \\frac{n_{R,1}}{N_R} = \\frac{5}{30} = \\frac{1}{6}$\n*   $p_{R,2} = \\frac{n_{R,2}}{N_R} = \\frac{25}{30} = \\frac{5}{6}$\n\nThe Gini impurity of the right child node is:\n$$ I_{Gini}(\\text{right}) = 1 - \\left[ \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{5}{6}\\right)^2 \\right] = 1 - \\left[ \\frac{1}{36} + \\frac{25}{36} \\right] = 1 - \\frac{26}{36} = 1 - \\frac{13}{18} = \\frac{5}{18} $$\n\n**4. Calculate the Impurity Decrease ($\\Delta I_{Gini}$)**\nThe weights for the child nodes are:\n*   Weight for left child: $\\frac{N_L}{N_P} = \\frac{20}{50} = \\frac{2}{5}$\n*   Weight for right child: $\\frac{N_R}{N_P} = \\frac{30}{50} = \\frac{3}{5}$\n\nNow, substitute the calculated values into the impurity decrease formula:\n$$ \\Delta I_{Gini} = I_{Gini}(\\text{parent}) - \\left[ \\frac{N_L}{N_P} I_{Gini}(\\text{left}) + \\frac{N_R}{N_P} I_{Gini}(\\text{right}) \\right] $$\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\left(\\frac{2}{5}\\right) \\left(\\frac{3}{8}\\right) + \\left(\\frac{3}{5}\\right) \\left(\\frac{5}{18}\\right) \\right] $$\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\frac{6}{40} + \\frac{15}{90} \\right] $$\nSimplify the fractions inside the brackets:\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\frac{3}{20} + \\frac{1}{6} \\right] $$\nTo subtract the fractions, find a common denominator for $25$, $20$, and $6$. The least common multiple is $\\text{lcm}(25, 20, 6) = \\text{lcm}(5^2, 2^2 \\cdot 5, 2 \\cdot 3) = 2^2 \\cdot 3 \\cdot 5^2 = 4 \\cdot 3 \\cdot 25 = 300$.\nFirst, calculate the term in the brackets:\n$$ \\frac{3}{20} + \\frac{1}{6} = \\frac{3 \\cdot 15}{20 \\cdot 15} + \\frac{1 \\cdot 50}{6 \\cdot 50} = \\frac{45}{300} + \\frac{50}{300} = \\frac{95}{300} $$\nAlternatively, $\\text{lcm}(20, 6) = 60$.\n$$ \\frac{3}{20} + \\frac{1}{6} = \\frac{9}{60} + \\frac{10}{60} = \\frac{19}{60} $$\nNow substitute this back into the main equation:\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\frac{19}{60} $$\nUsing the common denominator $300$:\n$$ \\Delta I_{Gini} = \\frac{12 \\cdot 12}{25 \\cdot 12} - \\frac{19 \\cdot 5}{60 \\cdot 5} = \\frac{144}{300} - \\frac{95}{300} = \\frac{144 - 95}{300} $$\n$$ \\Delta I_{Gini} = \\frac{49}{300} $$\nThis fraction cannot be simplified further, as $49 = 7^2$ and $300 = 3 \\cdot 100$, and they share no common factors. The result is the exact fractional value for the impurity decrease.",
            "answer": "$$\\boxed{\\frac{49}{300}}$$"
        },
        {
            "introduction": "While classification trees deal with categorical outcomes, regression trees apply the same partitioning logic to predict continuous variables. Instead of measuring class purity, the goal becomes minimizing variance within the nodes. This exercise  demonstrates how to calculate the reduction in the sum of squared errors (SSE) from a split, which is the foundational criterion for building regression trees and the direct analogue to the Gini impurity reduction in classification.",
            "id": "4962697",
            "problem": "In a biostatistics study modeling a continuous biomarker response, a regression tree is constructed under the squared error loss to partition observations into regions that yield constant predictions. Consider a parent node containing the response values $\\{2,3,3,7,8\\}$, and suppose a candidate split sends the observations $\\{2,3,3\\}$ to the left child node and the observations $\\{7,8\\}$ to the right child node. Using the foundational principle that the least squares criterion measures fit by the within-node sum of squared errors, compute the reduction in within-node sum of squared errors achieved by this split, defined as the parent node sum of squared errors minus the sum of the child nodesâ€™ sums of squared errors. Express your final answer as an exact value (no rounding).",
            "solution": "The problem is valid as it is scientifically grounded in the principles of statistical learning, specifically the construction of regression trees. It is well-posed, with all necessary data provided to compute a unique solution, and its language is objective and precise.\n\nThe objective is to compute the reduction in impurity, measured by the sum of squared errors (SSE), achieved by a specific split in a regression tree. The reduction is defined as the SSE of the parent node minus the sum of the SSEs of the resulting child nodes.\n\nThe core principle for a regression tree under squared error loss is that the optimal constant prediction for any given node is the arithmetic mean of the response values within that node. The SSE for a node $m$ with $N_m$ observations $\\{y_1, y_2, \\dots, y_{N_m}\\}$ is calculated as:\n$$SSE_m = \\sum_{i=1}^{N_m} (y_i - \\hat{y}_m)^2$$\nwhere $\\hat{y}_m$ is the mean of the response values in node $m$:\n$$\\hat{y}_m = \\frac{1}{N_m} \\sum_{i=1}^{N_m} y_i$$\n\nFirst, we calculate the SSE for the parent node.\nLet the parent node be denoted by $P$. The response values are $Y_P = \\{2, 3, 3, 7, 8\\}$.\nThe number of observations in the parent node is $N_P = 5$.\nThe mean prediction for the parent node is:\n$$\\hat{y}_P = \\frac{2 + 3 + 3 + 7 + 8}{5} = \\frac{23}{5} = 4.6$$\nThe SSE for the parent node is:\n$$SSE_P = (2 - 4.6)^2 + (3 - 4.6)^2 + (3 - 4.6)^2 + (7 - 4.6)^2 + (8 - 4.6)^2$$\n$$SSE_P = (-2.6)^2 + (-1.6)^2 + (-1.6)^2 + (2.4)^2 + (3.4)^2$$\n$$SSE_P = 6.76 + 2.56 + 2.56 + 5.76 + 11.56 = 29.2$$\nIn exact fractional form, $SSE_P = \\frac{292}{10} = \\frac{146}{5}$.\n\nNext, we calculate the SSE for the two child nodes created by the split.\nLet the left child node be denoted by $L$. The response values are $Y_L = \\{2, 3, 3\\}$.\nThe number of observations in the left child node is $N_L = 3$.\nThe mean prediction for the left child node is:\n$$\\hat{y}_L = \\frac{2 + 3 + 3}{3} = \\frac{8}{3}$$\nThe SSE for the left child node is:\n$$SSE_L = \\left(2 - \\frac{8}{3}\\right)^2 + \\left(3 - \\frac{8}{3}\\right)^2 + \\left(3 - \\frac{8}{3}\\right)^2$$\n$$SSE_L = \\left(\\frac{6 - 8}{3}\\right)^2 + \\left(\\frac{9 - 8}{3}\\right)^2 + \\left(\\frac{9 - 8}{3}\\right)^2$$\n$$SSE_L = \\left(-\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 = \\frac{4}{9} + \\frac{1}{9} + \\frac{1}{9} = \\frac{6}{9} = \\frac{2}{3}$$\n\nLet the right child node be denoted by $R$. The response values are $Y_R = \\{7, 8\\}$.\nThe number of observations in the right child node is $N_R = 2$.\nThe mean prediction for the right child node is:\n$$\\hat{y}_R = \\frac{7 + 8}{2} = \\frac{15}{2} = 7.5$$\nThe SSE for the right child node is:\n$$SSE_R = (7 - 7.5)^2 + (8 - 7.5)^2$$\n$$SSE_R = (-0.5)^2 + (0.5)^2 = 0.25 + 0.25 = 0.5 = \\frac{1}{2}$$\n\nFinally, we compute the reduction in SSE, which is $\\Delta SSE$.\n$$\\Delta SSE = SSE_P - (SSE_L + SSE_R)$$\nSubstituting the calculated values:\n$$\\Delta SSE = \\frac{146}{5} - \\left(\\frac{2}{3} + \\frac{1}{2}\\right)$$\nFirst, we sum the child nodes' SSEs:\n$$SSE_L + SSE_R = \\frac{2}{3} + \\frac{1}{2} = \\frac{4}{6} + \\frac{3}{6} = \\frac{7}{6}$$\nNow, we perform the subtraction:\n$$\\Delta SSE = \\frac{146}{5} - \\frac{7}{6}$$\nTo subtract, we find a common denominator, which is $30$:\n$$\\Delta SSE = \\frac{146 \\times 6}{30} - \\frac{7 \\times 5}{30} = \\frac{876}{30} - \\frac{35}{30} = \\frac{876 - 35}{30} = \\frac{841}{30}$$\nThe reduction in the within-node sum of squared errors achieved by this split is $\\frac{841}{30}$.",
            "answer": "$$\\boxed{\\frac{841}{30}}$$"
        },
        {
            "introduction": "Building a tree is only half the battle; a fully grown tree often overfits the training data, capturing noise rather than true signal. To create a model that generalizes well, we must prune it back using a method like cost-complexity pruning. This exercise  walks you through applying the one-standard-error (1-SE) rule, a widely used heuristic to select a parsimonious model that performs nearly as well as the best-performing, more complex tree, providing a practical tool for navigating the crucial bias-variance trade-off.",
            "id": "4962656",
            "problem": "A biostatistics team is designing a Classification and Regression Tree (CART) model to predict $30$-day intensive care unit mortality from electronic health record features. The model is pruned using cost-complexity pruning with tuning parameter $\\alpha$, where larger values of $\\alpha$ yield simpler trees with fewer terminal nodes. To estimate the predictive risk for each $\\alpha$, the team performs $5$-fold cross-validation (CV) and records the mean cross-validated risk and its estimated standard error for each candidate value of $\\alpha \\in \\{0, 0.005, 0.01, 0.02\\}$.\n\nThe observed $5$-fold cross-validated risks are $\\{0.18, 0.16, 0.155, 0.158\\}$ and the corresponding standard errors are $\\{0.01, 0.012, 0.013, 0.014\\}$ for $\\alpha$ in the same order. Using the one-standard-error (1-SE) rule, select the value of $\\alpha$ to deploy. Report the selected value of $\\alpha$ as a single number. No rounding is required.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded question in the field of biostatistics and machine learning, with all necessary data provided and no internal contradictions. It requires the application of a standard statistical procedure, the one-standard-error rule, to select a model tuning parameter.\n\nThe task is to select the optimal value for the cost-complexity tuning parameter, $\\alpha$, for a CART model using the one-standard-error (1-SE) rule. The problem provides the results from a $5$-fold cross-validation procedure for four candidate values of $\\alpha$.\n\nThe given data are:\n-   Candidate values for $\\alpha$: $\\{0, 0.005, 0.01, 0.02\\}$\n-   Corresponding mean cross-validated risks, let's denote this as $R(\\alpha)$: $\\{0.18, 0.16, 0.155, 0.158\\}$\n-   Corresponding standard errors of the mean risk, let's denote this as $SE(\\alpha)$: $\\{0.01, 0.012, 0.013, 0.014\\}$\n\nThe one-standard-error (1-SE) rule is a heuristic used to select a parsimonious model that performs almost as well as the model with the absolute best performance. The rule consists of the following steps:\n\n1.  Identify the model that yields the minimum cross-validated risk, $R_{min}$.\n2.  Calculate a performance threshold by adding the standard error corresponding to this minimum risk to the minimum risk itself. The threshold is $R_{target} = R_{min} + SE(R_{min})$.\n3.  Select the simplest model (in this case, the model with the largest $\\alpha$) whose cross-validated risk is less than or equal to this threshold, $R_{target}$. The premise is that a larger $\\alpha$ leads to a simpler, more pruned tree, which is generally preferred to avoid overfitting.\n\nLet's apply these steps to the provided data.\n\nFirst, we organize the data pairs of $(R(\\alpha), SE(\\alpha))$ for each $\\alpha$:\n-   For $\\alpha = 0$: $R(0) = 0.18$, $SE(0) = 0.01$\n-   For $\\alpha = 0.005$: $R(0.005) = 0.16$, $SE(0.005) = 0.012$\n-   For $\\alpha = 0.01$: $R(0.01) = 0.155$, $SE(0.01) = 0.013$\n-   For $\\alpha = 0.02$: $R(0.02) = 0.158$, $SE(0.02) = 0.014$\n\n**Step 1: Find the minimum cross-validated risk.**\nBy inspecting the risk values $\\{0.18, 0.16, 0.155, 0.158\\}$, the minimum risk is:\n$$R_{min} = 0.155$$\nThis minimum risk occurs at the tuning parameter value $\\alpha = 0.01$.\n\n**Step 2: Calculate the target risk threshold.**\nThe standard error corresponding to the model with minimum risk is the one associated with $\\alpha = 0.01$, which is $SE(0.01) = 0.013$.\nThe 1-SE rule threshold is calculated as:\n$$R_{target} = R_{min} + SE(R_{min}) = 0.155 + 0.013 = 0.168$$\n\n**Step 3: Select the simplest model with risk below the threshold.**\nWe must find the model with the largest value of $\\alpha$ (simplest model) such that its risk $R(\\alpha)$ is less than or equal to $R_{target} = 0.168$. We check the candidate models in order of decreasing complexity (i.e., increasing $\\alpha$).\n\n-   For $\\alpha = 0$, the risk is $R(0) = 0.18$. Since $0.18 > 0.168$, this model is not selected.\n-   For $\\alpha = 0.005$, the risk is $R(0.005) = 0.16$. Since $0.16 \\le 0.168$, this model is a candidate.\n-   For $\\alpha = 0.01$, the risk is $R(0.01) = 0.155$. Since $0.155 \\le 0.168$, this model is a candidate.\n-   For $\\alpha = 0.02$, the risk is $R(0.02) = 0.158$. Since $0.158 \\le 0.168$, this model is a candidate.\n\nThe set of $\\alpha$ values corresponding to models that satisfy the criterion $R(\\alpha) \\le 0.168$ is $\\{0.005, 0.01, 0.02\\}$.\nThe 1-SE rule dictates selecting the simplest model from this set. In cost-complexity pruning, a larger $\\alpha$ corresponds to a higher penalty for complexity, resulting in a simpler tree. Therefore, we select the largest value of $\\alpha$ from the candidate set.\n$$\n\\alpha_{selected} = \\max\\{0.005, 0.01, 0.02\\} = 0.02\n$$\nThus, according to the one-standard-error rule, the value of $\\alpha$ to deploy is $0.02$. This choice favors a simpler model that still performs within one standard error of the best-performing model, which is desirable for better generalization to new data.",
            "answer": "$$\\boxed{0.02}$$"
        }
    ]
}