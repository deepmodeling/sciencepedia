{
    "hands_on_practices": [
        {
            "introduction": "分类树的核心在于如何选择“最佳”分割，以将数据划分为越来越“纯”的子集。为了量化一个节点的“纯度”，我们引入了不纯度度量，其中基尼不纯度（Gini impurity）是最常用的一种。通过计算每次分割带来的基尼不纯度下降量，我们可以客观地比较不同分割方案的优劣，并选择最优的一个。这个练习将带你亲手计算一次分割所带来的不纯度改善，这是理解并构建分类树算法的基础。",
            "id": "4962711",
            "problem": "在一个使用分类与回归树（CART）的生物统计分析中，对于一个类别标记为$1$和$2$的二元结果，考虑一个父节点，它包含来自类别$1$的$n_1=20$个观测值和来自类别$2$的$n_2=30$个观测值。一个候选分裂将该父节点划分为一个左子节点和一个右子节点，其类别$(1,2)$的计数分别为$(15,5)$和$(5,25)$。使用基尼不纯度作为节点不纯度的度量，并采用标准的加权不纯度减少准则来评估分裂，请计算此次分裂实现的不纯度减少量。请将您的最终答案表示为一个未经四舍五入的精确分数。",
            "solution": "用户提供了一个来自生物统计学领域的问题，具体涉及分类与回归树（CART）。任务是使用基尼不纯度度量，计算一个父节点分裂成两个子节点所导致的不纯度减少量。\n\n### 步骤 1：问题验证\n\n首先，我必须验证问题陈述。\n\n**提取已知条件：**\n*   **方法论**：分类与回归树（CART）\n*   **结果**：二元的，类别标记为$1$和$2$。\n*   **父节点构成**：包含类别$1$的$n_1 = 20$个观测值和类别$2$的$n_2 = 30$个观测值。\n*   **分裂定义**：\n    *   **左子节点构成**：$n_{L,1} = 15$（类别$1$），$n_{L,2} = 5$（类别$2$）。\n    *   **右子节点构成**：$n_{R,1} = 5$（类别$1$），$n_{R,2} = 25$（类别$2$）。\n*   **不纯度度量**：基尼不纯度。\n*   **评估准则**：加权不纯度减少。\n\n**根据标准进行验证：**\n1.  **科学依据**：该问题在决策树理论中有充分的依据，这是机器学习和生物统计学中的一个标准课题。基尼不纯度是分类树中广泛使用的度量标准。\n2.  **适定性**：该问题提供了计算唯一数值答案所需的所有数据和定义。要求“精确分数”的请求是明确的。\n3.  **客观性**：问题以精确、量化的术语陈述，没有任何主观性。\n4.  **一致性检查**：\n    *   父节点中的总观测值：$N_P = n_1 + n_2 = 20 + 30 = 50$。\n    *   左子节点中的总观测值：$N_L = n_{L,1} + n_{L,2} = 15 + 5 = 20$。\n    *   右子节点中的总观测值：$N_R = n_{R,1} + n_{R,2} = 5 + 25 = 30$。\n    *   观测值守恒：$N_L + N_R = 20 + 30 = 50 = N_P$。\n    *   类别1守恒：$n_{L,1} + n_{R,1} = 15 + 5 = 20 = n_1$。\n    *   类别2守恒：$n_{L,2} + n_{R,2} = 5 + 25 = 30 = n_2$。\n    数据是自洽的。\n\n**结论**：该问题是有效的、科学上合理的、适定的且内部一致的。我现在将继续进行解答。\n\n### 步骤 2：求解推导\n\n分裂导致的不纯度减少量（也称为基尼增益）的计算方法是：父节点的不纯度减去子节点不纯度的加权平均值。\n不纯度减少量 $\\Delta I_{Gini}$ 的公式是：\n$$ \\Delta I_{Gini} = I_{Gini}(\\text{parent}) - \\left[ \\frac{N_L}{N_P} I_{Gini}(\\text{left}) + \\frac{N_R}{N_P} I_{Gini}(\\text{right}) \\right] $$\n其中 $I_{Gini}$ 是一个节点的基尼不纯度，$N_P$、$N_L$、$N_R$ 分别是父节点、左子节点和右子节点中的总观测数。\n\n对于一个有 $K$ 个类别的节点 $m$，其基尼不纯度定义为：\n$$ I_{Gini}(m) = 1 - \\sum_{k=1}^{K} p_{mk}^2 $$\n其中 $p_{mk}$ 是节点 $m$ 中类别 $k$ 的观测值所占的比例。对于这个二元问题（$K=2$），公式为：\n$$ I_{Gini}(m) = 1 - (p_{m1}^2 + p_{m2}^2) $$\n\n**1. 计算父节点的基尼不纯度 ($I_{Gini}(\\text{parent})$)**\n父节点有 $N_P = 20 + 30 = 50$ 个观测值。\n各类别的比例是：\n*   $p_{P,1} = \\frac{n_1}{N_P} = \\frac{20}{50} = \\frac{2}{5}$\n*   $p_{P,2} = \\frac{n_2}{N_P} = \\frac{30}{50} = \\frac{3}{5}$\n\n父节点的基尼不纯度是：\n$$ I_{Gini}(\\text{parent}) = 1 - \\left[ \\left(\\frac{2}{5}\\right)^2 + \\left(\\frac{3}{5}\\right)^2 \\right] = 1 - \\left[ \\frac{4}{25} + \\frac{9}{25} \\right] = 1 - \\frac{13}{25} = \\frac{12}{25} $$\n\n**2. 计算左子节点的基尼不纯度 ($I_{Gini}(\\text{left})$)**\n左子节点有 $N_L = 15 + 5 = 20$ 个观测值。\n各类别的比例是：\n*   $p_{L,1} = \\frac{n_{L,1}}{N_L} = \\frac{15}{20} = \\frac{3}{4}$\n*   $p_{L,2} = \\frac{n_{L,2}}{N_L} = \\frac{5}{20} = \\frac{1}{4}$\n\n左子节点的基尼不纯度是：\n$$ I_{Gini}(\\text{left}) = 1 - \\left[ \\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 \\right] = 1 - \\left[ \\frac{9}{16} + \\frac{1}{16} \\right] = 1 - \\frac{10}{16} = 1 - \\frac{5}{8} = \\frac{3}{8} $$\n\n**3. 计算右子节点的基尼不纯度 ($I_{Gini}(\\text{right})$)**\n右子节点有 $N_R = 5 + 25 = 30$ 个观测值。\n各类别的比例是：\n*   $p_{R,1} = \\frac{n_{R,1}}{N_R} = \\frac{5}{30} = \\frac{1}{6}$\n*   $p_{R,2} = \\frac{n_{R,2}}{N_R} = \\frac{25}{30} = \\frac{5}{6}$\n\n右子节点的基尼不纯度是：\n$$ I_{Gini}(\\text{right}) = 1 - \\left[ \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{5}{6}\\right)^2 \\right] = 1 - \\left[ \\frac{1}{36} + \\frac{25}{36} \\right] = 1 - \\frac{26}{36} = 1 - \\frac{13}{18} = \\frac{5}{18} $$\n\n**4. 计算不纯度减少量 ($\\Delta I_{Gini}$)**\n子节点的权重是：\n*   左子节点的权重：$\\frac{N_L}{N_P} = \\frac{20}{50} = \\frac{2}{5}$\n*   右子节点的权重：$\\frac{N_R}{N_P} = \\frac{30}{50} = \\frac{3}{5}$\n\n现在，将计算出的值代入不纯度减少量的公式中：\n$$ \\Delta I_{Gini} = I_{Gini}(\\text{parent}) - \\left[ \\frac{N_L}{N_P} I_{Gini}(\\text{left}) + \\frac{N_R}{N_P} I_{Gini}(\\text{right}) \\right] $$\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\left(\\frac{2}{5}\\right) \\left(\\frac{3}{8}\\right) + \\left(\\frac{3}{5}\\right) \\left(\\frac{5}{18}\\right) \\right] $$\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\frac{6}{40} + \\frac{15}{90} \\right] $$\n简化括号内的分数：\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\left[ \\frac{3}{20} + \\frac{1}{6} \\right] $$\n为了进行分数减法，需要找到 $25$、$20$ 和 $6$ 的公分母。最小公倍数是 $\\text{lcm}(25, 20, 6) = \\text{lcm}(5^2, 2^2 \\cdot 5, 2 \\cdot 3) = 2^2 \\cdot 3 \\cdot 5^2 = 4 \\cdot 3 \\cdot 25 = 300$。\n首先，计算括号中的项：\n$$ \\frac{3}{20} + \\frac{1}{6} = \\frac{3 \\cdot 15}{20 \\cdot 15} + \\frac{1 \\cdot 50}{6 \\cdot 50} = \\frac{45}{300} + \\frac{50}{300} = \\frac{95}{300} $$\n或者，$\\text{lcm}(20, 6) = 60$。\n$$ \\frac{3}{20} + \\frac{1}{6} = \\frac{9}{60} + \\frac{10}{60} = \\frac{19}{60} $$\n现在将此结果代回主方程：\n$$ \\Delta I_{Gini} = \\frac{12}{25} - \\frac{19}{60} $$\n使用公分母 $300$：\n$$ \\Delta I_{Gini} = \\frac{12 \\cdot 12}{25 \\cdot 12} - \\frac{19 \\cdot 5}{60 \\cdot 5} = \\frac{144}{300} - \\frac{95}{300} = \\frac{144 - 95}{300} $$\n$$ \\Delta I_{Gini} = \\frac{49}{300} $$\n这个分数无法进一步简化，因为 $49 = 7^2$ 且 $300 = 3 \\cdot 100$，它们没有公因数。结果就是不纯度减少量的精确分数值。",
            "answer": "$$\\boxed{\\frac{49}{300}}$$"
        },
        {
            "introduction": "与分类树旨在降低节点“不纯度”不同，回归树处理的是连续型结果变量，其目标是最小化预测误差。对于回归树中的一个节点，最佳的预测值通常是该节点内所有观测值的均值。我们使用残差平方和（Sum of Squared Errors, SSE）来衡量该节点预测值与真实值之间的差异。因此，构建回归树的过程就是寻找能最大程度降低SSE的分割。这个练习将演示如何量化一个特定分割对SSE的减少量，让你掌握回归树构建的关键计算。",
            "id": "4962697",
            "problem": "在一项模拟连续生物标志物响应的生物统计学研究中，根据平方误差损失构建了一棵回归树，以将观测值划分到能产生恒定预测值的区域。考虑一个包含响应值 $\\{2,3,3,7,8\\}$ 的父节点，并假设一个候选分割将观测值 $\\{2,3,3\\}$ 发送到左子节点，将观测值 $\\{7,8\\}$ 发送到右子节点。使用最小二乘准则通过节点内平方误差和来衡量拟合度的基本原则，计算此次分割所实现的节点内平方误差和的减少量，该减少量定义为父节点的平方误差和减去子节点平方误差和的总和。将您的最终答案表示为精确值（不要四舍五入）。",
            "solution": "该问题是有效的，因为它在科学上基于统计学习的原理，特别是回归树的构建。问题是适定的，提供了计算唯一解所需的所有数据，并且其语言客观而精确。\n\n目标是计算在回归树中通过特定分割实现的不纯度减少量，该不纯度由平方误差和（SSE）衡量。减少量定义为父节点的SSE减去所产生的子节点的SSE之和。\n\n在平方误差损失下，回归树的核心原则是：对于任何给定节点，最优的恒定预测值是该节点内响应值的算术平均值。对于一个包含 $N_m$ 个观测值 $\\{y_1, y_2, \\dots, y_{N_m}\\}$ 的节点 $m$，其SSE计算如下：\n$$SSE_m = \\sum_{i=1}^{N_m} (y_i - \\hat{y}_m)^2$$\n其中 $\\hat{y}_m$ 是节点 $m$ 中响应值的均值：\n$$\\hat{y}_m = \\frac{1}{N_m} \\sum_{i=1}^{N_m} y_i$$\n\n首先，我们计算父节点的SSE。\n设父节点为 $P$。响应值为 $Y_P = \\{2, 3, 3, 7, 8\\}$。\n父节点中的观测数量为 $N_P = 5$。\n父节点的平均预测值为：\n$$\\hat{y}_P = \\frac{2 + 3 + 3 + 7 + 8}{5} = \\frac{23}{5} = 4.6$$\n父节点的SSE为：\n$$SSE_P = (2 - 4.6)^2 + (3 - 4.6)^2 + (3 - 4.6)^2 + (7 - 4.6)^2 + (8 - 4.6)^2$$\n$$SSE_P = (-2.6)^2 + (-1.6)^2 + (-1.6)^2 + (2.4)^2 + (3.4)^2$$\n$$SSE_P = 6.76 + 2.56 + 2.56 + 5.76 + 11.56 = 29.2$$\n用精确分数形式表示，$SSE_P = \\frac{292}{10} = \\frac{146}{5}$。\n\n接下来，我们计算由分割创建的两个子节点的SSE。\n设左子节点为 $L$。响应值为 $Y_L = \\{2, 3, 3\\}$。\n左子节点中的观测数量为 $N_L = 3$。\n左子节点的平均预测值为：\n$$\\hat{y}_L = \\frac{2 + 3 + 3}{3} = \\frac{8}{3}$$\n左子节点的SSE为：\n$$SSE_L = \\left(2 - \\frac{8}{3}\\right)^2 + \\left(3 - \\frac{8}{3}\\right)^2 + \\left(3 - \\frac{8}{3}\\right)^2$$\n$$SSE_L = \\left(\\frac{6 - 8}{3}\\right)^2 + \\left(\\frac{9 - 8}{3}\\right)^2 + \\left(\\frac{9 - 8}{3}\\right)^2$$\n$$SSE_L = \\left(-\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 = \\frac{4}{9} + \\frac{1}{9} + \\frac{1}{9} = \\frac{6}{9} = \\frac{2}{3}$$\n\n设右子节点为 $R$。响应值为 $Y_R = \\{7, 8\\}$。\n右子节点中的观测数量为 $N_R = 2$。\n右子节点的平均预测值为：\n$$\\hat{y}_R = \\frac{7 + 8}{2} = \\frac{15}{2} = 7.5$$\n右子节点的SSE为：\n$$SSE_R = (7 - 7.5)^2 + (8 - 7.5)^2$$\n$$SSE_R = (-0.5)^2 + (0.5)^2 = 0.25 + 0.25 = 0.5 = \\frac{1}{2}$$\n\n最后，我们计算SSE的减少量，即 $\\Delta SSE$。\n$$\\Delta SSE = SSE_P - (SSE_L + SSE_R)$$\n代入计算出的值：\n$$\\Delta SSE = \\frac{146}{5} - \\left(\\frac{2}{3} + \\frac{1}{2}\\right)$$\n首先，我们对子节点的SSE求和：\n$$SSE_L + SSE_R = \\frac{2}{3} + \\frac{1}{2} = \\frac{4}{6} + \\frac{3}{6} = \\frac{7}{6}$$\n现在，我们进行减法运算：\n$$\\Delta SSE = \\frac{146}{5} - \\frac{7}{6}$$\n为了进行减法，我们找到公分母，即 $30$：\n$$\\Delta SSE = \\frac{146 \\times 6}{30} - \\frac{7 \\times 5}{30} = \\frac{876}{30} - \\frac{35}{30} = \\frac{876 - 35}{30} = \\frac{841}{30}$$\n此次分割实现的节点内平方误差和的减少量为 $\\frac{841}{30}$。",
            "answer": "$$\\boxed{\\frac{841}{30}}$$"
        },
        {
            "introduction": "构建一颗决策树后，我们面临一个关键问题：这棵树是否过于复杂并对训练数据产生了过拟合？一颗完全生长的树虽然在训练集上表现完美，但在预测新数据时可能表现不佳。成本复杂性剪枝（Cost-Complexity Pruning）是一种有效避免过拟合的技术，它通过调节参数 $\\alpha$ 生成一系列从大到小的嵌套树。而“一倍标准误规则”（1-SE rule）则是在交叉验证的框架下，帮助我们从中选择一棵既有良好预测性能又不过于复杂的“最佳”树。这个练习模拟了一个在生物统计学研究中至关重要的决策过程，让你学会如何在模型的预测准确性和简洁性之间做出权衡。",
            "id": "4962656",
            "problem": "一个生物统计学团队正在设计一个分类与回归树（CART）模型，以根据电子健康记录的特征预测$30$天内重症监护室的死亡率。该模型使用成本复杂度剪枝进行修剪，调整参数为$\\alpha$，其中较大的$\\alpha$值会产生更简单、终端节点更少的树。为了估计每个$\\alpha$的预测风险，该团队执行了$5$折交叉验证（CV），并记录了每个候选$\\alpha \\in \\{0, 0.005, 0.01, 0.02\\}$值的平均交叉验证风险及其估计的标准误。\n\n观测到的$5$折交叉验证风险为$\\{0.18, 0.16, 0.155, 0.158\\}$，相应的标准误为$\\{0.01, 0.012, 0.013, 0.014\\}$，$\\alpha$的顺序与之一致。使用一倍标准误（1-SE）规则，选择要部署的$\\alpha$值。将所选的$\\alpha$值报告为单个数字。无需四舍五入。",
            "solution": "该问题是有效的。这是一个在生物统计学和机器学习领域中提法恰当、有科学依据的问题，提供了所有必要的数据且没有内部矛盾。它要求应用一个标准的统计程序，即一倍标准误规则，来选择模型的调整参数。\n\n任务是使用一倍标准误（1-SE）规则，为CART模型选择成本复杂度调整参数$\\alpha$的最优值。问题提供了针对四个候选$\\alpha$值的$5$折交叉验证程序的结果。\n\n给定的数据如下：\n-   $\\alpha$的候选值：$\\{0, 0.005, 0.01, 0.02\\}$\n-   相应的平均交叉验证风险，我们记为$R(\\alpha)$：$\\{0.18, 0.16, 0.155, 0.158\\}$\n-   相应的平均风险标准误，我们记为$SE(\\alpha)$：$\\{0.01, 0.012, 0.013, 0.014\\}$\n\n一倍标准误（1-SE）规则是一种启发式方法，用于选择一个简约模型，其性能几乎与绝对最佳性能的模型一样好。该规则包括以下步骤：\n\n1.  识别产生最小交叉验证风险$R_{min}$的模型。\n2.  通过将与此最小风险相对应的标准误加到最小风险本身来计算性能阈值。该阈值为$R_{target} = R_{min} + SE(R_{min})$。\n3.  选择交叉验证风险小于或等于此阈值$R_{target}$的最简单的模型（在本例中，即$\\alpha$值最大的模型）。其前提是，较大的$\\alpha$会导致更简单、修剪更多的树，这通常是为了避免过拟合而更受青睐。\n\n让我们将这些步骤应用于所提供的数据。\n\n首先，我们为每个$\\alpha$整理数据对$(R(\\alpha), SE(\\alpha))$：\n-   对于$\\alpha = 0$：$R(0) = 0.18$，$SE(0) = 0.01$\n-   对于$\\alpha = 0.005$：$R(0.005) = 0.16$，$SE(0.005) = 0.012$\n-   对于$\\alpha = 0.01$：$R(0.01) = 0.155$，$SE(0.01) = 0.013$\n-   对于$\\alpha = 0.02$：$R(0.02) = 0.158$，$SE(0.02) = 0.014$\n\n**步骤1：找到最小的交叉验证风险。**\n通过检查风险值$\\{0.18, 0.16, 0.155, 0.158\\}$，最小风险为：\n$$R_{min} = 0.155$$\n这个最小风险出现在调整参数值为$\\alpha = 0.01$时。\n\n**步骤2：计算目标风险阈值。**\n与最小风险模型相对应的标准误是与$\\alpha = 0.01$相关联的那个，即$SE(0.01) = 0.013$。\n1-SE规则的阈值计算如下：\n$$R_{target} = R_{min} + SE(R_{min}) = 0.155 + 0.013 = 0.168$$\n\n**步骤3：选择风险低于阈值的最简单模型。**\n我们必须找到具有最大$\\alpha$值（最简单模型）的模型，使其风险$R(\\alpha)$小于或等于$R_{target} = 0.168$。我们按照复杂度递减（即$\\alpha$递增）的顺序检查候选模型。\n\n-   对于$\\alpha = 0$，风险为$R(0) = 0.18$。由于$0.18  0.168$，不选择此模型。\n-   对于$\\alpha = 0.005$，风险为$R(0.005) = 0.16$。由于$0.16 \\le 0.168$，此模型是一个候选模型。\n-   对于$\\alpha = 0.01$，风险为$R(0.01) = 0.155$。由于$0.155 \\le 0.168$，此模型是一个候选模型。\n-   对于$\\alpha = 0.02$，风险为$R(0.02) = 0.158$。由于$0.158 \\le 0.168$，此模型是一个候选模型。\n\n满足标准$R(\\alpha) \\le 0.168$的模型的$\\alpha$值集合是$\\{0.005, 0.01, 0.02\\}$。\n1-SE规则要求从此集合中选择最简单的模型。在成本复杂度剪枝中，较大的$\\alpha$对应于对复杂度的更高惩罚，从而产生更简单的树。因此，我们从候选集合中选择最大的$\\alpha$值。\n$$\n\\alpha_{selected} = \\max\\{0.005, 0.01, 0.02\\} = 0.02\n$$\n因此，根据一倍标准误规则，要部署的$\\alpha$值为$0.02$。这个选择倾向于一个更简单的模型，该模型的性能仍在表现最佳模型的一个标准误范围内，这对于更好地泛化到新数据是可取的。",
            "answer": "$$\\boxed{0.02}$$"
        }
    ]
}