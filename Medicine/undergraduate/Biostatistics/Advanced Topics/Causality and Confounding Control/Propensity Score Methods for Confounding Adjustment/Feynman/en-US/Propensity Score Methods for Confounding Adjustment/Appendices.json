{
    "hands_on_practices": [
        {
            "introduction": "The first critical step in any analysis using propensity scores is to decide which covariates to include in the model. Simply including all available variables can be ineffective or even harmful. Causal diagrams, specifically Directed Acyclic Graphs (DAGs), offer a powerful, non-statistical framework to guide this decision. This exercise will help you apply the 'backdoor criterion' to identify confounding paths and select a sufficient set of variables for adjustment, a foundational skill for any causal analysis .",
            "id": "4943138",
            "problem": "Consider an observational study evaluating the causal effect of a binary treatment $A$ on a continuous outcome $Y$. Let $X$ denote a measured pre-treatment covariate and $U$ denote an unmeasured patient factor. Suppose the data-generating process is represented by the following directed acyclic graph (DAG): there are directed edges $U \\to A$, $U \\to X$, $X \\to A$, $X \\to Y$, and $A \\to Y$, and no other arrows. Assume standard causal identifiability conditions for propensity score methods, including positivity and consistency. Use the rules of $d$-separation on DAGs to reason about confounding, where a backdoor path from $A$ to $Y$ is any path that begins with an arrow into $A$ and remains open given the empty conditioning set. Identify all open backdoor paths from $A$ to $Y$ in this DAG and specify a minimal adjustment set composed of observed variables that blocks all such backdoor paths without blocking the causal path $A \\to Y$ or opening new paths through colliders. Your answer should be suitable for inclusion in a propensity score (PS) model.\n\nWhich option correctly lists the open backdoor paths and gives a minimal adjustment set that blocks them?\n\nA. Backdoor paths: $A \\leftarrow X \\to Y$ only; minimal adjustment set $\\{X\\}$.\n\nB. Backdoor paths: $A \\leftarrow U \\to Y$ and $A \\leftarrow X \\to Y$; minimal adjustment set $\\{U,X\\}$.\n\nC. Backdoor paths: $A \\leftarrow U \\to X \\to Y$ and $A \\leftarrow X \\to Y$; minimal adjustment set $\\{X\\}$.\n\nD. No backdoor paths; minimal adjustment set $\\varnothing$.\n\nE. Backdoor paths: $A \\leftarrow U \\to X \\to Y$ only; minimal adjustment set $\\{U\\}$.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n-   **Study Type**: Observational study.\n-   **Variables**:\n    -   $A$: Binary treatment.\n    -   $Y$: Continuous outcome.\n    -   $X$: Measured pre-treatment covariate.\n    -   $U$: Unmeasured patient factor.\n-   **Data-Generating Process (DAG)**: The directed acyclic graph contains the following directed edges:\n    1.  $U \\to A$\n    2.  $U \\to X$\n    3.  $X \\to A$\n    4.  $X \\to Y$\n    5.  $A \\to Y$\n    -   No other arrows are present.\n-   **Assumptions**: Standard causal identifiability conditions, including positivity and consistency.\n-   **Definitions**: A backdoor path from $A$ to $Y$ is defined as \"any path that begins with an arrow into $A$ and remains open given the empty conditioning set.\"\n-   **Objective**: Identify all open backdoor paths from $A$ to $Y$ and specify a minimal adjustment set composed of observed variables that blocks all such paths, suitable for a propensity score (PS) model.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is analyzed against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is well-founded in the theory of causal inference using Directed Acyclic Graphs (DAGs), a standard and rigorous framework in biostatistics, epidemiology, and computer science. The concepts of confounding, d-separation, backdoor paths, and adjustment sets are all core principles of this framework.\n-   **Well-Posed**: The problem is well-posed. The DAG structure is explicitly defined, the goal is clear, and the rules of d-separation provide a deterministic method for identifying paths and determining the effects of conditioning. A unique and meaningful solution exists.\n-   **Objective**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n-   **Complete and Consistent**: The problem provides all necessary information: the variables, their relationships (the DAG), the distinction between measured ($X$) and unmeasured ($U$) variables, and the specific goal (find paths and a minimal adjustment set of observed variables). The premises are internally consistent.\n-   **Realistic and Feasible**: The scenario represents a classic confounding structure common in observational studies, where both a measured covariate ($X$) and an unmeasured one ($U$) are common causes of treatment and outcome (or variables on the path to the outcome). The distinction between measured and unmeasured variables is a critical and realistic constraint in applied research.\n-   **Other Flaws**: The problem is not trivial, tautological, ill-posed, or unverifiable. It requires a correct application of established causal inference principles.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The analysis can proceed.\n\n## Solution Derivation\n\nThe objective is to identify all open backdoor paths from the treatment $A$ to the outcome $Y$ and then find a minimal sufficient adjustment set composed of observed variables to block these paths. The causal path of interest is the direct edge $A \\to Y$. A backdoor path is a non-causal path between $A$ and $Y$ that creates a spurious association.\n\nThe DAG is defined by the edges: $U \\to A$, $U \\to X$, $X \\to A$, $X \\to Y$, and $A \\to Y$.\n\n**1. Identification of Backdoor Paths**\n\nA backdoor path from $A$ to $Y$ is a path that starts with an arrow pointing into $A$ (i.e., $A \\leftarrow \\dots$). We must trace all such paths connecting $A$ and $Y$ that do not contain the edge $A \\to Y$. A path is \"open\" if it has no colliders, or if every collider on the path is in the conditioning set (which is currently empty).\n\n-   **Path 1**: Consider the path through the covariate $X$. The path is $A \\leftarrow X \\to Y$.\n    -   It starts with an arrow into $A$: $A \\leftarrow X$.\n    -   It connects $A$ to $Y$.\n    -   This path is a \"fork\" structure centered on $X$. There are no colliders on this path.\n    -   Therefore, $A \\leftarrow X \\to Y$ is an open backdoor path given the empty conditioning set.\n\n-   **Path 2**: Consider paths involving the unmeasured factor $U$.\n    -   A path starts with $A \\leftarrow U$. To get from $U$ to $Y$, we observe the path $U \\to X \\to Y$.\n    -   Combining these gives the path: $A \\leftarrow U \\to X \\to Y$.\n    -   It starts with an arrow into $A$: $A \\leftarrow U$.\n    -   It connects $A$ to $Y$ via $U$ and $X$.\n    -   This path is a \"chain\" structure ($U \\to X \\to Y$). There are no colliders on this path.\n    -   Therefore, $A \\leftarrow U \\to X \\to Y$ is an open backdoor path given the empty conditioning set.\n\n-   **Other potential paths**: Are there any other paths? Let's check for paths involving colliders.\n    -   The node $A$ is a collider on the path $U \\to A \\leftarrow X$. This path connects $U$ and $X$, not $A$ and $Y$. It is not a backdoor path from $A$ to $Y$.\n    -   The node $Y$ is a collider on the path $X \\to Y \\leftarrow A$, but backdoor paths cannot pass through the outcome variable.\n\nThus, there are exactly two open backdoor paths from $A$ to $Y$:\n1.  $A \\leftarrow X \\to Y$\n2.  $A \\leftarrow U \\to X \\to Y$\n\n**2. Identification of a Minimal Sufficient Adjustment Set**\n\nTo estimate the causal effect of $A$ on $Y$, we must block these two backdoor paths by conditioning on a set of observed covariates. The only observed covariate is $X$. The factor $U$ is unmeasured and cannot be included in an adjustment set for a propensity score model.\n\n-   **Blocking Path 1 ($A \\leftarrow X \\to Y$)**: This path is a fork at $X$. According to the rules of d-separation, conditioning on the fork variable $X$ blocks this path.\n-   **Blocking Path 2 ($A \\leftarrow U \\to X \\to Y$)**: This path is a chain. Conditioning on any intermediate variable in the chain blocks the path. $X$ is an intermediate variable on this path. Therefore, conditioning on $X$ also blocks this path.\n\nConditioning on the set $\\{X\\}$ is sufficient to block both open backdoor paths.\n\nNow, we must verify if $\\{X\\}$ is a minimal set and if conditioning on it induces any new problems.\n-   **Minimality**: The only smaller set is the empty set, $\\varnothing$. Conditioning on $\\varnothing$ leaves both paths open. Therefore, $\\{X\\}$ is a minimal sufficient adjustment set.\n-   **Validity of Conditioning**: We must ensure that conditioning on $X$ does not block the causal path or open a new biasing path.\n    -   $X$ is not a mediator on the causal path $A \\to Y$, so conditioning on it does not block the causal effect.\n    -   Conditioning on a variable opens a path if that variable is a collider on that path. The only collider in the system that isn't the outcome is $A$ on the path $U \\to A \\leftarrow X$. Conditioning on $X$ does not open this path (only conditioning on $A$ or its descendants would).\n    \nTherefore, $\\{X\\}$ is the correct minimal sufficient adjustment set composed of observed variables.\n\n## Option-by-Option Analysis\n\n-   **A. Backdoor paths: $A \\leftarrow X \\to Y$ only; minimal adjustment set $\\{X\\}$.**\n    -   The identified list of backdoor paths is incomplete; it omits the path $A \\leftarrow U \\to X \\to Y$. While the minimal adjustment set is correct, the premise for its selection is flawed.\n    -   **Verdict**: Incorrect.\n\n-   **B. Backdoor paths: $A \\leftarrow U \\to Y$ and $A \\leftarrow X \\to Y$; minimal adjustment set $\\{U,X\\}$.**\n    -   The path $A \\leftarrow U \\to Y$ does not exist in the specified DAG; the path from $U$ to $Y$ is mediated by $X$. The correct path is $A \\leftarrow U \\to X \\to Y$. The adjustment set $\\{U,X\\}$ is invalid because $U$ is unmeasured and cannot be conditioned upon in practice. Furthermore, the set is not minimal, as $\\{X\\}$ is sufficient.\n    -   **Verdict**: Incorrect.\n\n-   **C. Backdoor paths: $A \\leftarrow U \\to X \\to Y$ and $A \\leftarrow X \\to Y$; minimal adjustment set $\\{X\\}$.**\n    -   This option correctly identifies both open backdoor paths: $A \\leftarrow X \\to Y$ and $A \\leftarrow U \\to X \\to Y$.\n    -   It also correctly identifies $\\{X\\}$ as the minimal sufficient adjustment set consisting of observed variables that blocks both of these paths.\n    -   **Verdict**: Correct.\n\n-   **D. No backdoor paths; minimal adjustment set $\\varnothing$.**\n    -   This is incorrect. As shown in the derivation, there are two open backdoor paths creating confounding. The empty set $\\varnothing$ is not a sufficient adjustment set.\n    -   **Verdict**: Incorrect.\n\n-   **E. Backdoor paths: $A \\leftarrow U \\to X \\to Y$ only; minimal adjustment set $\\{U\\}$.**\n    -   The list of backdoor paths is incomplete; it omits the path $A \\leftarrow X \\to Y$. The suggested adjustment set $\\{U\\}$ is incorrect for two reasons: (1) $U$ is unmeasured, and (2) conditioning on $U$ alone would not block the path $A \\leftarrow X \\to Y$.\n    -   **Verdict**: Incorrect.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "After learning to identify confounders, a common impulse is to include every pre-treatment variable in the propensity score model, aiming for the best possible prediction of treatment assignment. However, not all predictors are helpful for confounding control. This practice explores the important case of an 'instrumental variable'—a variable strongly associated with treatment but not the outcome—to demonstrate a crucial trade-off between bias and variance . You will see how including such a variable, while not introducing bias, can inflate the variance of your causal estimate by creating extreme weights.",
            "id": "4943077",
            "problem": "A cohort study aims to estimate the Average Treatment Effect (ATE) of a binary treatment $T \\in \\{0,1\\}$ on a continuous outcome $Y$. Let $X \\in \\{0,1\\}$ be a confounder and $Z \\in \\{0,1\\}$ be a highly predictive variable for treatment assignment that is not a confounder of the treatment–outcome relationship. Assume $Z$ is independent of the potential outcomes given $X$, i.e., $Z \\perp Y(t) \\mid X$ for $t \\in \\{0,1\\}$. The data generating process satisfies:\n- $P(X=1)=P(Z=1)=P(X=1 \\mid Z=1)=P(X=1 \\mid Z=0)=P(Z=1 \\mid X=1)=P(Z=1 \\mid X=0)=0.5$ (i.e., $X$ and $Z$ are marginally independent and each is Bernoulli with probability $0.5$),\n- Treatment model: $P(T=1 \\mid X,Z)=\\text{expit}(\\alpha_0+\\alpha_1 X+\\alpha_2 Z)$ with $\\alpha_0=0$, $\\alpha_1=0.5$, and $\\alpha_2=3$,\n- Outcome model: $Y=\\beta_0+\\beta_1 T+\\beta_2 X+\\varepsilon$, with $\\beta_0=0$, $\\beta_1=2$, $\\beta_2=1$, and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $(X,Z,T)$ with $\\sigma^2=1$.\n\nConsider estimating the ATE using Inverse Probability of Treatment Weighting (IPTW) based on the Propensity Score (PS), where the PS is defined as $e(W)=P(T=1 \\mid W)$ for a specified set of covariates $W$. Two analyst strategies are under consideration:\n- Strategy $S_1$: Estimate PS using only the confounder, i.e., $W=X$, yielding $e_1(X)=P(T=1 \\mid X)$,\n- Strategy $S_2$: Estimate PS using both the confounder and the highly predictive non-confounder, i.e., $W=(X,Z)$, yielding $e_2(X,Z)=P(T=1 \\mid X,Z)$.\n\nUsing the above setup and the foundational definitions of ignorability $Y(t) \\perp T \\mid X$ and the balancing property of the propensity score, which statement best characterizes the impact of including $Z$ in the PS model on bias and variance of the IPTW ATE estimator, and why?\n\nA. Including $Z$ in the PS under $S_2$ reduces bias because any variable predictive of treatment must be controlled to satisfy ignorability; variance is unchanged because weights simply reflect better prediction.\n\nB. Including $Z$ in the PS under $S_2$ does not change bias relative to $S_1$ but increases variance, because $Z$ makes $e_2(X,Z)$ more extreme (closer to $0$ or $1$) without changing $Y(t) \\perp T \\mid X$, thereby inflating the variability of inverse probability weights.\n\nC. Including $Z$ in the PS under $S_2$ reduces variance, because better prediction of treatment yields weights closer to $1$, while bias may increase due to conditioning on a strong predictor of treatment.\n\nD. Including $Z$ in the PS under $S_2$ introduces bias by opening a noncausal path between $T$ and $Y$, since $Z$ is associated with $T$; variance is unaffected because the mean of the weights is fixed at $1$.",
            "solution": "## Problem Validation\n- **Scientifically Grounded**: The problem is well-grounded in causal inference theory, specifically concerning the properties of Inverse Probability of Treatment Weighting (IPTW) estimators. The concepts of confounding, instrumental variables, and the bias-variance trade-off in propensity score model specification are central to this field.\n- **Well-Posed**: The problem is well-posed, with a fully specified data-generating process (DGP), including distributions, model parameters, and relationships between variables. This allows for a deterministic analysis of bias and a qualitative analysis of variance.\n- **Objective and Complete**: The problem is stated objectively with all necessary information provided. The distinction between a confounder ($X$) and an instrument ($Z$) is clearly defined.\n- **Verdict**: The problem is valid and can be solved through direct application of causal inference principles.\n\n## Solution Derivation\n\nThe goal is to compare two strategies for estimating the Average Treatment Effect (ATE), $\\Delta = E[Y(1) - Y(0)]$, using an Inverse Probability of Treatment Weighting (IPTW) estimator. The true ATE in this model is $\\beta_1=2$. The IPTW estimator for the ATE is given by:\n$$ \\hat{\\Delta}_{\\text{IPTW}} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{T_i Y_i}{e(W_i)} - \\frac{1}{n} \\sum_{i=1}^{n} \\frac{(1-T_i) Y_i}{1 - e(W_i)} $$\nwhere $e(W) = P(T=1 \\mid W)$ is the propensity score (PS) model.\n\n**1. Bias Analysis**\n\nFor the IPTW estimator to be unbiased for the ATE, the propensity score model must be correctly specified and the conditioning set $W$ must satisfy the ignorability assumption: $Y(t) \\perp T \\mid W$ for $t \\in \\{0,1\\}$.\n\n- **Potential Outcomes**: From the outcome model $Y = \\beta_0 + \\beta_1 T + \\beta_2 X + \\varepsilon$, the potential outcomes are $Y(1) = \\beta_0 + \\beta_1 + \\beta_2 X + \\varepsilon$ and $Y(0) = \\beta_0 + \\beta_2 X + \\varepsilon$. Notice that the potential outcomes depend only on $X$ and $\\varepsilon$, not on $Z$.\n- **Confounding**: The variable $X$ is a confounder because it is a cause of the outcome ($X \\to Y$ path via $\\beta_2$) and is associated with treatment assignment ($T$ via $\\alpha_1$). Therefore, we must adjust for $X$.\n- **Strategy $S_1$ ($W=X$)**: This strategy uses the confounder set $W=\\{X\\}$. Since $X$ is the only confounder, the ignorability assumption $Y(t) \\perp T \\mid X$ holds. The PS model $e_1(X) = P(T=1 \\mid X)$ is correctly specified with respect to the DGP. Thus, the IPTW estimator based on $S_1$ is unbiased for the ATE.\n- **Strategy $S_2$ ($W=(X,Z)$)**: This strategy uses the set $W=\\{X,Z\\}$. The variable $Z$ is an instrument: it is a strong predictor of treatment ($T$ via $\\alpha_2=3$) but is independent of the potential outcomes conditional on $X$ ($Z \\perp Y(t) \\mid X$, as given and confirmed by the outcome model). Since ignorability holds for $X$, it also holds for any superset of $X$, so $Y(t) \\perp T \\mid (X,Z)$ is also true. Therefore, the IPTW estimator based on $S_2$ is also unbiased for the ATE.\n\n**Conclusion on Bias**: Both strategies yield unbiased estimators for the ATE. Including the instrument $Z$ in the propensity score model does not introduce bias.\n\n**2. Variance Analysis**\n\nThe variance of the IPTW estimator is highly sensitive to the magnitude of the weights, which are the inverse of the propensity scores. Weights become large when propensity scores are close to 0 or 1.\n\n- **Strategy $S_1$ ($W=X$)**: The PS depends only on $X$. Since $Z$ is independent of $X$, $P(T=1 \\mid X) = E[P(T=1 \\mid X,Z) \\mid X]$. The log-odds are $0.5X$.\n  - If $X=0$, $e_1(0) = \\text{expit}(0) = 0.5$. Weights are $1/0.5=2$.\n  - If $X=1$, $e_1(1) = \\text{expit}(0.5) \\approx 0.622$. Weights are $1/0.622 \\approx 1.61$ and $1/(1-0.622) \\approx 2.65$.\n- **Strategy $S_2$ ($W=(X,Z)$)**: The PS depends on both $X$ and $Z$. The log-odds are $0.5X + 3Z$.\n  - If $X=0, Z=0$, $e_2(0,0) = \\text{expit}(0) = 0.5$.\n  - If $X=1, Z=0$, $e_2(1,0) = \\text{expit}(0.5) \\approx 0.622$.\n  - If $X=0, Z=1$, $e_2(0,1) = \\text{expit}(3) \\approx 0.953$. The weight for an untreated subject is $1/(1-0.953) \\approx 21.3$.\n  - If $X=1, Z=1$, $e_2(1,1) = \\text{expit}(3.5) \\approx 0.971$. The weight for an untreated subject is $1/(1-0.971) \\approx 34.5$.\n\nAs shown, including the strong instrument $Z$ (with a large coefficient $\\alpha_2=3$) pushes the propensity scores much closer to 0 and 1 for a subset of the population. This results in some individuals receiving very large weights (e.g., 21.3 and 34.5). These extreme weights amplify the influence of a few individuals on the total estimate, leading to a substantial increase in the variance of the estimator. While including a variable that predicts the outcome (but not treatment) can decrease variance, including a variable that only predicts treatment (an instrument) but not the outcome will increase variance.\n\n**Conclusion on Variance**: Including the instrument $Z$ in the PS model under strategy $S_2$ increases the variance of the IPTW ATE estimator compared to strategy $S_1$.\n\n**3. Option-by-Option Analysis**\n\nA. **Incorrect**. Including $Z$ does not reduce bias, as $S_1$ was already unbiased. Variance is not unchanged; it increases.\nB. **Correct**. Including $Z$ does not change the (asymptotic) bias, as both strategies rely on a valid conditioning set. However, it increases variance because the strong predictive power of $Z$ for treatment alone creates more extreme propensity scores and thus more variable inverse probability weights.\nC. **Incorrect**. Including $Z$ increases, not reduces, variance. Bias does not increase (it remains zero).\nD. **Incorrect**. Including $Z$ does not introduce bias. Variance is affected.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Propensity score methods rely on the crucial assumption of positivity, or overlap: that for any given set of covariate values, there is a non-zero probability of being in either the treated or untreated group. When this assumption is violated, a common practical solution is to 'trim' individuals with extreme propensity scores (near $0$ or $1$). This exercise demonstrates that this procedure is not merely a technical fix but fundamentally alters the research question being answered . By working through a concrete example, you will learn how trimming redefines the target population and why transparently reporting this change is essential for valid scientific inference.",
            "id": "4943099",
            "problem": "An observational cohort study evaluates the effect of a binary treatment $A \\in \\{0,1\\}$ on a continuous outcome $Y$ at $6$ months. Let $\\mathbf{X}$ denote a vector of pre-treatment covariates, and define the propensity score $e(\\mathbf{X}) = \\Pr(A=1 \\mid \\mathbf{X})$. Assume the Stable Unit Treatment Value Assumption (SUTVA), consistency, conditional exchangeability $(Y^{1}, Y^{0}) \\perp A \\mid \\mathbf{X}$, and positivity on the support of $\\mathbf{X}$. The investigator estimates $\\hat{e}(\\mathbf{X})$ and decides to trim observations with extreme estimated propensity scores by discarding all units with $\\hat{e}(\\mathbf{X})  a$ or $\\hat{e}(\\mathbf{X})  1-a$, with $a = 0.05$. Suppose, for the purpose of reasoning about the estimand, that $\\hat{e}(\\mathbf{X}) \\approx e(\\mathbf{X})$.\n\nConsider a simple covariate structure in which there is a binary covariate $G \\in \\{0,1\\}$ contained in $\\mathbf{X}$, with $\\Pr(G=1) = 0.20$ and $\\Pr(G=0) = 0.80$. In the population, the propensity score satisfies $e(\\mathbf{X}) = 0.98$ whenever $G=1$ and $e(\\mathbf{X}) = 0.50$ whenever $G=0$. Let $S = \\{\\mathbf{x} : a \\le e(\\mathbf{x}) \\le 1-a\\}$ denote the overlap set induced by trimming at the threshold $a = 0.05$.\n\nUsing only the core definitions above and properties of conditional expectation, reason about how trimming alters the target population and the causal estimand. Then select the option that most accurately characterizes the post-trimming causal estimand in this setup and specifies the key elements that should be reported to communicate the trimmed estimand transparently.\n\nA. After trimming at $a=0.05$, the causal estimand becomes $E[Y^{1} - Y^{0} \\mid \\mathbf{X} \\in S]$, which in this setup reduces to $E[Y^{1} - Y^{0} \\mid G=0]$ because all units with $G=1$ are excluded ($0.98  0.95$). Transparent reporting should include the trimming threshold $a$, the definition of $S$, the proportion trimmed overall and by $A$, pre-/post-trimming covariate balance, and an interpretation that the effect generalizes to the overlap subpopulation $S$ rather than the full population.\n\nB. Trimming does not change the estimand; it remains $E[Y^{1} - Y^{0}]$ for the full population. Because trimming mainly reduces variance from extreme weights, reporting the point estimate and its $95\\%$ confidence interval is sufficient.\n\nC. Trimming changes the estimand to the average treatment effect in the treated, $E[Y^{1} - Y^{0} \\mid A=1]$, because units with $e(\\mathbf{X})$ near $1$ are retained. Transparency requires reporting $p$-values for all covariate comparisons and stating that confounding was eliminated.\n\nD. The trimmed estimand is $E[Y^{1} - Y^{0} \\mid e(\\mathbf{X})  0.95]$ (the high-propensity group), since the most informative units are those with near-certain treatment. Reporting should include only the overall trimming fraction and the post-trimming sample size.",
            "solution": "## Solution Derivation\n\nThe initial goal of the study is to estimate the population average treatment effect (ATE), which is defined as $E[Y^1 - Y^0]$. This estimand represents the average causal effect of treatment $A$ over the entire population defined by the distribution of covariates $\\mathbf{X}$.\n\nThe investigator applies a trimming procedure based on the estimated propensity score. The trimming rule is to discard subjects for whom $\\hat{e}(\\mathbf{X})  a$ or $\\hat{e}(\\mathbf{X})  1-a$. With $a=0.05$, this means subjects with $\\hat{e}(\\mathbf{X})  0.05$ or $\\hat{e}(\\mathbf{X})  0.95$ are removed from the analysis. The subsequent analysis is performed only on the subpopulation with covariate values $\\mathbf{x}$ belonging to the set $S = \\{\\mathbf{x} : 0.05 \\le e(\\mathbf{x}) \\le 0.95\\}$. The approximation $\\hat{e}(\\mathbf{X}) \\approx e(\\mathbf{X})$ allows us to use the true propensity score $e(\\mathbf{X})$ to determine which subjects are trimmed.\n\nLet's apply this rule to the specified population structure:\n1.  For the subpopulation where the covariate $G=1$, the propensity score is $e(\\mathbf{X}) = 0.98$. Since $0.98  0.95$, all individuals in this subpopulation are trimmed from the dataset.\n2.  For the subpopulation where the covariate $G=0$, the propensity score is $e(\\mathbf{X}) = 0.50$. Since $0.05 \\le 0.50 \\le 0.95$, all individuals in this subpopulation are retained for the analysis.\n\nThis means the original population, which was a mixture of individuals with $G=1$ ($20\\%$) and $G=0$ ($80\\%$), is modified. The analysis is now restricted *exclusively* to the subpopulation of individuals for whom $G=0$.\n\nThe causal estimand that can be estimated from this trimmed sample is the average treatment effect *within this new, restricted population*. The original estimand was the ATE for the full population. The new estimand is the ATE for the subpopulation defined by $\\mathbf{X} \\in S$. In our specific case, the condition $\\mathbf{X} \\in S$ is exactly equivalent to the condition $G=0$.\n\nTherefore, the post-trimming estimand is:\n$$ E[Y^1 - Y^0 \\mid \\mathbf{X} \\in S] = E[Y^1 - Y^0 \\mid G=0] $$\n\nThis new estimand is not, in general, equal to the original full-population ATE, $E[Y^1 - Y^0]$. Equality would only hold under the strong, untestable assumption that the average treatment effect is the same in the $G=0$ subpopulation and the $G=1$ subpopulation, i.e., $E[Y^1 - Y^0 \\mid G=0] = E[Y^1 - Y^0 \\mid G=1]$. Such an assumption of effect homogeneity is not given and cannot be presumed.\n\nConsequently, trimming has changed the research question. The analysis no longer addresses \"What is the effect in the full population?\" but rather \"What is the effect in the subpopulation of individuals with $G=0$?\" This is a critical distinction. Transparent reporting of the results must acknowledge this change in the target estimand and target population.\n\nGood reporting practices in this situation would include:\n*   The exact trimming rule used (the value of $a$).\n*   The definition of the resulting analysis population ($S$).\n*   The number and proportion of subjects trimmed, ideally broken down by treatment group.\n*   A comparison of the covariate distributions before and after trimming. In this case, the distribution of $G$ changes from $\\Pr(G=1)=0.20$ to $\\Pr(G=1)=0$. This is a major change and must be reported.\n*   A clear statement that the estimated effect applies to the subpopulation with good \"overlap\" in their propensity scores, and that generalization to the original full population is not guaranteed.\n\n### Option-by-Option Analysis\n\n**A. After trimming at $a=0.05$, the causal estimand becomes $E[Y^{1} - Y^{0} \\mid \\mathbf{X} \\in S]$, which in this setup reduces to $E[Y^{1} - Y^{0} \\mid G=0]$ because all units with $G=1$ are excluded ($0.98  0.95$). Transparent reporting should include the trimming threshold $a$, the definition of $S$, the proportion trimmed overall and by $A$, pre-/post-trimming covariate balance, and an interpretation that the effect generalizes to the overlap subpopulation $S$ rather than the full population.**\n\n*   This option correctly identifies that trimming changes the estimand to one conditioned on the overlap set $S$.\n*   It correctly applies the trimming rule to the given population structure, deducing that the subpopulation with $G=1$ is entirely removed and the new estimand is specific to the $G=0$ subpopulation.\n*   It provides a comprehensive and accurate list of elements required for transparent reporting, including the change in generalizability.\n*   **Verdict: Correct.**\n\n**B. Trimming does not change the estimand; it remains $E[Y^{1} - Y^{0}]$ for the full population. Because trimming mainly reduces variance from extreme weights, reporting the point estimate and its $95\\%$ confidence interval is sufficient.**\n\n*   The claim that trimming does not change the estimand is fundamentally false, as demonstrated above. The analysis is performed on a different, non-random subset of the original population.\n*   The justification that trimming is \"mainly\" for variance reduction ignores the critical impact on the target of inference and generalizability.\n*   The suggestion that reporting is sufficient with just the estimate and confidence interval promotes non-transparent and potentially misleading science.\n*   **Verdict: Incorrect.**\n\n**C. Trimming changes the estimand to the average treatment effect in the treated, $E[Y^{1} - Y^{0} \\mid A=1]$, because units with $e(\\mathbf{X})$ near $1$ are retained. Transparency requires reporting $p$-values for all covariate comparisons and stating that confounding was eliminated.**\n\n*   The claim that the estimand becomes the average treatment effect in the treated (ATT) is incorrect. The new estimand is the ATE in the subpopulation with covariate values in $S$.\n*   The reasoning \"because units with $e(\\mathbf{X})$ near $1$ are retained\" is factually wrong according to the problem statement. The rule explicitly *discards* units with $e(\\mathbf{X})  0.95$, which includes those with $e(\\mathbf{X})$ near $1$.\n*   The reporting suggestion to state \"confounding was eliminated\" is an overstatement. Propensity score methods aim to control for *observed* confounding, not eliminate all confounding (which would include unobserved confounding).\n*   **Verdict: Incorrect.**\n\n**D. The trimmed estimand is $E[Y^{1} - Y^{0} \\mid e(\\mathbf{X})  0.95]$ (the high-propensity group), since the most informative units are those with near-certain treatment. Reporting should include only the overall trimming fraction and the post-trimming sample size.**\n\n*   This option completely mischaracterizes the trimming procedure. The group with $e(\\mathbf{X})  0.95$ is the one that is *removed*, not the one that is studied.\n*   The justification is flawed. Units with extreme propensity scores are often problematic for estimation (leading to high variance of inverse probability weighted estimators) because of the lack of comparable units in the opposite treatment arm.\n*   The reporting standard suggested is minimalistic and inadequate for communicating the change in the target population and estimand.\n*   **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}