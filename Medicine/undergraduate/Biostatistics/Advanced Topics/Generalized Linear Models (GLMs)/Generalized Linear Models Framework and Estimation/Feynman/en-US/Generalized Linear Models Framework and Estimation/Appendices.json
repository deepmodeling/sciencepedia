{
    "hands_on_practices": [
        {
            "introduction": "Generalized Linear Models are typically fitted using the principle of Maximum Likelihood Estimation (MLE). This involves finding the parameter values that make the observed data most probable. This hands-on derivation  takes you under the hood of this process for the widely used logistic regression model. By explicitly deriving the log-likelihood function, its gradient (the score vector), and its Hessian matrix, you will gain a concrete understanding of the mathematical components that numerical optimization algorithms use to find the optimal model coefficients.",
            "id": "4914224",
            "problem": "A biostatistics study records binary outcomes for the presence of a specific biomarker in blood samples. For each subject $i$ with $i \\in \\{1,\\dots,n\\}$, the outcome $Y_{i} \\in \\{0,1\\}$ indicates absence or presence of the biomarker, and a single continuous covariate $X_{i} \\in \\mathbb{R}$ records a standardized exposure score. Assume subjects are independent and that conditional on $X_{i}$, $Y_{i}$ follows a Bernoulli model with mean $\\mu_{i} = \\mathbb{E}(Y_{i} \\mid X_{i})$. Consider a generalized linear model (GLM) with the canonical logit link for the Bernoulli distribution, so that the link function satisfies $\\ln\\!\\big(\\mu_{i}/(1-\\mu_{i})\\big) = \\beta_{0} + \\beta_{1} X_{i}$, where $\\beta = (\\beta_{0},\\beta_{1})^{\\top}$ is an unknown parameter vector with intercept and one slope. Starting only from the independence assumption, the Bernoulli probability mass function, and the definition of the canonical link, derive the following quantities as explicit functions of $\\beta$ and the observed data $\\{(y_{i},x_{i})\\}_{i=1}^{n}$:\n\n1. The log-likelihood $\\ell(\\beta)$.\n2. The gradient (score) vector $\\nabla \\ell(\\beta)$.\n3. The Hessian matrix $\\nabla^{2} \\ell(\\beta)$.\n\nExpress each quantity using summations over $i$ and elementary functions of $\\beta$ and the data. Your final expressions must depend only on $\\beta_{0}$, $\\beta_{1}$, $\\{x_{i}\\}_{i=1}^{n}$, and $\\{y_{i}\\}_{i=1}^{n}$, without introducing any undefined shorthand. The final answer should present these three derived expressions together as a single analytic expression. No numerical computation is required, and no rounding is needed.",
            "solution": "The problem statement is a standard derivation exercise in the context of generalized linear models (GLMs) for binary data and has been validated as scientifically grounded, well-posed, objective, and complete. There are no flaws. We proceed with the derivation of the log-likelihood, its gradient, and its Hessian.\n\nThe model is specified for $n$ independent subjects, indexed by $i=1, \\dots, n$. For each subject, we have a binary outcome $Y_i \\in \\{0, 1\\}$ and a continuous covariate $X_i \\in \\mathbb{R}$.\nThe conditional distribution of $Y_i$ given $X_i=x_i$ is a Bernoulli distribution with mean $\\mu_i = \\mathbb{E}(Y_i \\mid X_i)$. The probability mass function (PMF) for a single observation $y_i$ is:\n$$ P(Y_i = y_i \\mid X_i=x_i) = \\mu_i^{y_i} (1-\\mu_i)^{1-y_i} $$\nThe model uses a logit link function, which connects the mean $\\mu_i$ to a linear predictor $\\eta_i$:\n$$ \\eta_i = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) = \\beta_0 + \\beta_1 x_i $$\nwhere $\\beta = (\\beta_0, \\beta_1)^\\top$ is the vector of parameters.\n\nTo express the likelihood in terms of $\\beta$, we must first express $\\mu_i$ as a function of $\\eta_i$. Inverting the logit link function gives:\n$$ \\frac{\\mu_i}{1-\\mu_i} = \\exp(\\eta_i) \\implies \\mu_i = \\exp(\\eta_i) - \\mu_i \\exp(\\eta_i) \\implies \\mu_i(1+\\exp(\\eta_i)) = \\exp(\\eta_i) $$\n$$ \\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)} $$\nAlso, we find the expression for $1-\\mu_i$:\n$$ 1 - \\mu_i = 1 - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1+\\exp(\\eta_i)-\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(\\eta_i)} $$\n\nWith $\\mu_i$ defined in terms of $\\eta_i = \\beta_0 + \\beta_1 x_i$, we can now derive the required quantities. The observed data are denoted by $\\{(y_i, x_i)\\}_{i=1}^n$.\n\n**1. The Log-Likelihood Function $\\ell(\\beta)$**\n\nDue to the independence of subjects, the total likelihood $L(\\beta)$ is the product of the individual probabilities:\n$$ L(\\beta) = \\prod_{i=1}^n P(Y_i=y_i \\mid X_i=x_i) = \\prod_{i=1}^n \\mu_i^{y_i} (1-\\mu_i)^{1-y_i} $$\nThe log-likelihood $\\ell(\\beta)$ is the natural logarithm of $L(\\beta)$:\n$$ \\ell(\\beta) = \\ln(L(\\beta)) = \\sum_{i=1}^n \\ln(\\mu_i^{y_i} (1-\\mu_i)^{1-y_i}) = \\sum_{i=1}^n \\left[ y_i \\ln(\\mu_i) + (1-y_i)\\ln(1-\\mu_i) \\right] $$\nUsing $\\ln(\\mu_i) = \\ln(\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i)) = \\eta_i - \\ln(1+\\exp(\\eta_i))$ and $\\ln(1-\\mu_i) = -\\ln(1+\\exp(\\eta_i))$, we substitute these into the expression for $\\ell(\\beta)$:\n$$ \\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i (\\eta_i - \\ln(1+\\exp(\\eta_i))) + (1-y_i)(-\\ln(1+\\exp(\\eta_i))) \\right] $$\n$$ \\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i \\eta_i - y_i \\ln(1+\\exp(\\eta_i)) - \\ln(1+\\exp(\\eta_i)) + y_i \\ln(1+\\exp(\\eta_i)) \\right] $$\n$$ \\ell(\\beta) = \\sum_{i=1}^n \\left[ y_i \\eta_i - \\ln(1+\\exp(\\eta_i)) \\right] $$\nSubstituting $\\eta_i = \\beta_0 + \\beta_1 x_i$:\n$$ \\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_{i}(\\beta_{0} + \\beta_{1}x_{i}) - \\ln(1 + \\exp(\\beta_{0} + \\beta_{1}x_{i})) \\right] $$\n\n**2. The Gradient (Score) Vector $\\nabla \\ell(\\beta)$**\n\nThe gradient vector contains the partial derivatives of $\\ell(\\beta)$ with respect to $\\beta_0$ and $\\beta_1$. Let's compute the partial derivative with respect to a generic parameter $\\beta_j$, where $j \\in \\{0, 1\\}$.\nUsing the chain rule, $\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$, where $\\ell_i$ is the $i$-th term in the log-likelihood sum.\n$$ \\frac{\\partial \\ell_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i} \\left[ y_i \\eta_i - \\ln(1+\\exp(\\eta_i)) \\right] = y_i - \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = y_i - \\mu_i $$\nThe derivatives of the linear predictor are $\\frac{\\partial \\eta_i}{\\partial \\beta_0} = 1$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_1} = x_i$.\n\nFor $\\beta_0$:\n$$ \\frac{\\partial \\ell}{\\partial \\beta_0} = \\sum_{i=1}^n (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_0} = \\sum_{i=1}^n (y_i - \\mu_i) \\cdot 1 = \\sum_{i=1}^{n} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) $$\nFor $\\beta_1$:\n$$ \\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{i=1}^n (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_1} = \\sum_{i=1}^n (y_i - \\mu_i) x_i = \\sum_{i=1}^{n} x_{i} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) $$\nThe gradient vector is:\n$$ \\nabla \\ell(\\beta) = \\begin{pmatrix} \\frac{\\partial \\ell}{\\partial \\beta_0} \\\\ \\frac{\\partial \\ell}{\\partial \\beta_1} \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\\\ \\sum_{i=1}^{n} x_{i} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\end{pmatrix} $$\n\n**3. The Hessian Matrix $\\nabla^2 \\ell(\\beta)$**\n\nThe Hessian matrix consists of the second-order partial derivatives. We compute $\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}$ for $j,k \\in \\{0, 1\\}$.\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\left( \\frac{\\partial \\ell}{\\partial \\beta_j} \\right) = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^n (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\sum_{i=1}^n \\left( -\\frac{\\partial \\mu_i}{\\partial \\beta_k} \\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j} $$\nWe need the derivative of $\\mu_i$ with respect to $\\beta_k$:\n$$ \\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k} $$\nThe derivative $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$ is:\n$$ \\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{\\partial}{\\partial \\eta_i}\\left(\\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\\right) = \\frac{\\exp(\\eta_i)(1+\\exp(\\eta_i)) - \\exp(\\eta_i)\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} = \\frac{\\exp(\\eta_i)}{(1+\\exp(\\eta_i))^2} $$\nThis can be written as $\\mu_i (1-\\mu_i)$. Thus, $\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\mu_i(1-\\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}$.\nSubstituting this into the second derivative expression:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = - \\sum_{i=1}^n \\left(\\mu_i (1-\\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_k}\\right) \\frac{\\partial \\eta_i}{\\partial \\beta_j} = - \\sum_{i=1}^n \\mu_i (1-\\mu_i) \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\frac{\\partial \\eta_i}{\\partial \\beta_k} $$\nLet $x_{i0}=1$ and $x_{i1}=x_i$, so $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_{ij}$.\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = - \\sum_{i=1}^n x_{ij} x_{ik} \\mu_i(1-\\mu_i) $$\nwhere $\\mu_i(1-\\mu_i) = \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}}$.\n\nThe four elements of the Hessian matrix are:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_0^2} = - \\sum_{i=1}^n (1)^2 \\mu_i(1-\\mu_i) = -\\sum_{i=1}^{n} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_0 \\partial \\beta_1} = \\frac{\\partial^2 \\ell}{\\partial \\beta_1 \\partial \\beta_0} = - \\sum_{i=1}^n (1)(x_i) \\mu_i(1-\\mu_i) = -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_1^2} = - \\sum_{i=1}^n (x_i)^2 \\mu_i(1-\\mu_i) = -\\sum_{i=1}^{n} x_{i}^{2} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} $$\nThe Hessian matrix is therefore:\n$$ \\nabla^2 \\ell(\\beta) = \\begin{pmatrix} -\\sum_{i=1}^{n} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} & -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\\\ -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} & -\\sum_{i=1}^{n} x_{i}^{2} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\end{pmatrix} $$",
            "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_{i}(\\beta_{0} + \\beta_{1}x_{i}) - \\ln(1 + \\exp(\\beta_{0} + \\beta_{1}x_{i})) \\right] \\\\\n\\\\\n\\nabla \\ell(\\beta) = \\begin{pmatrix} \\sum_{i=1}^{n} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\\\ \\sum_{i=1}^{n} x_{i} \\left( y_{i} - \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{1+\\exp(\\beta_{0}+\\beta_{1}x_{i})} \\right) \\end{pmatrix} \\\\\n\\\\\n\\nabla^2 \\ell(\\beta) = \\begin{pmatrix} -\\sum_{i=1}^{n} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}}  -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\\\ -\\sum_{i=1}^{n} x_{i} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}}  -\\sum_{i=1}^{n} x_{i}^{2} \\frac{\\exp(\\beta_{0}+\\beta_{1}x_{i})}{(1+\\exp(\\beta_{0}+\\beta_{1}x_{i}))^{2}} \\end{pmatrix}\n\\end{aligned}\n}\n$$"
        },
        {
            "introduction": "Once a GLM has been successfully fitted, the result is a set of estimated coefficients, $\\hat{\\beta}$. The real power of the model comes from using these coefficients to interpret relationships and make predictions. This exercise  provides practical experience in this crucial step. Using the results from a hypothetical Poisson regression analysis, you will calculate the fitted incidence rates and expected event counts, reinforcing your understanding of the link function and the role of an offset for exposure time.",
            "id": "4914209",
            "problem": "A cohort study in biostatistics measures incident counts of a condition across three sites, with exposures recorded in person-years. The counts are modeled using a Poisson Generalized Linear Model (GLM), where the response at site $i$ is a count $Y_i$ assumed to follow a Poisson distribution with mean $E(Y_i)=\\mu_i$. The model uses a natural logarithm link and incorporates exposure through an offset so that the linear predictor is additive in covariates and exposure.\n\nThe covariate vector is $x_i=\\begin{pmatrix}1  a_i  z_i\\end{pmatrix}^{\\top}$, where $a_i$ is age deviation in decades from a $50$-year baseline and $z_i$ is a binary indicator for treatment. Suppose the GLM has been fit and yielded an estimated coefficient vector $\\hat{\\beta}=\\begin{pmatrix}-3.2  0.4  0.6\\end{pmatrix}^{\\top}$. For three sites, the observed covariates and exposures are:\n- Site $1$: $x_1=\\begin{pmatrix}1  1.0  0\\end{pmatrix}^{\\top}$, exposure $t_1=180$ person-years.\n- Site $2$: $x_2=\\begin{pmatrix}1  -0.5  1\\end{pmatrix}^{\\top}$, exposure $t_2=320$ person-years.\n- Site $3$: $x_3=\\begin{pmatrix}1  2.0  1\\end{pmatrix}^{\\top}$, exposure $t_3=140$ person-years.\n\nStarting from the core definitions of the Poisson distribution and the GLM framework with a logarithm link and exposure offset, derive expressions for the fitted incidence rates $\\hat{\\lambda}_i$ and the expected counts $\\hat{\\mu}_i$ in terms of $x_i$, $t_i$, and $\\hat{\\beta}$. Then compute the numerical values of $\\hat{\\lambda}_i$ and $\\hat{\\mu}_i$ for $i=1,2,3$ using the values above, and verify that the units are consistent for rates and counts. Round all numerical results to four significant figures. Express your final answer as a single row matrix containing $\\hat{\\lambda}_1$, $\\hat{\\lambda}_2$, $\\hat{\\lambda}_3$, $\\hat{\\mu}_1$, $\\hat{\\mu}_2$, $\\hat{\\mu}_3$ in that order. No units should appear in the final matrix.",
            "solution": "The problem is valid as it is scientifically grounded in standard biostatistical modeling, well-posed with all necessary information, and stated objectively. We can proceed with the solution.\n\nThe problem describes a Poisson Generalized Linear Model (GLM) for count data $Y_i$ at site $i$, where $i \\in \\{1, 2, 3\\}$. The core components of the GLM are as follows:\n$1$. The **random component** specifies the probability distribution of the response variable. Here, the count $Y_i$ is assumed to follow a Poisson distribution with mean $\\mu_i$:\n$$Y_i \\sim \\text{Poisson}(\\mu_i)$$\nThe expected value is $E[Y_i] = \\mu_i$.\n\n$2$. The **systematic component** is the linear predictor $\\eta_i$, which is a linear combination of the covariates $x_i$ and coefficients $\\beta$:\n$$\\eta_i = x_i^{\\top}\\beta$$\n\n$3$. The **link function** $g$ connects the expected value of the response $\\mu_i$ to the linear predictor $\\eta_i$. The problem states a natural logarithm link, $g(\\cdot) = \\ln(\\cdot)$. Additionally, the model incorporates exposure $t_i$ (in person-years) as an offset. In a Poisson GLM, this is formalized by modeling the rate $\\lambda_i = \\mu_i / t_i$. The link function applies to the mean, so the model equation is:\n$$g(\\mu_i) = \\eta_i + \\text{offset}_i$$\nWith a log link and a log-exposure offset, this becomes:\n$$\\ln(\\mu_i) = x_i^{\\top}\\beta + \\ln(t_i)$$\nThis structure is equivalent to modeling the logarithm of the incidence rate $\\lambda_i$ directly. To show this, we substitute $\\mu_i = \\lambda_i t_i$:\n$$\\ln(\\lambda_i t_i) = x_i^{\\top}\\beta + \\ln(t_i)$$\nUsing the property of logarithms, $\\ln(a b) = \\ln(a) + \\ln(b)$, we get:\n$$\\ln(\\lambda_i) + \\ln(t_i) = x_i^{\\top}\\beta + \\ln(t_i)$$\n$$\\ln(\\lambda_i) = x_i^{\\top}\\beta$$\nThis confirms that the model is for the log-incidence rate.\n\nThe problem provides the estimated coefficient vector $\\hat{\\beta}$. We use this to find the fitted values. The fitted linear predictor for site $i$ is $\\hat{\\eta}_i = x_i^{\\top}\\hat{\\beta}$.\n\nThe expression for the fitted incidence rate, $\\hat{\\lambda}_i$, is derived by exponentiating the fitted model for the log-rate:\n$$\\ln(\\hat{\\lambda}_i) = x_i^{\\top}\\hat{\\beta}$$\n$$\\hat{\\lambda}_i = \\exp(x_i^{\\top}\\hat{\\beta})$$\nThe unit of $\\hat{\\lambda}_i$ is events per person-year, determined by the unit of exposure $t_i$.\n\nThe expression for the fitted expected count, $\\hat{\\mu}_i$, is derived from the relationship $\\hat{\\mu}_i = \\hat{\\lambda}_i t_i$:\n$$\\hat{\\mu}_i = t_i \\hat{\\lambda}_i = t_i \\exp(x_i^{\\top}\\hat{\\beta})$$\nThe units are consistent: $\\hat{\\mu}_i$ (a dimensionless count) is the product of $t_i$ (person-years) and $\\hat{\\lambda}_i$ (events per person-year).\n\nThe given data are:\n- Estimated coefficient vector: $\\hat{\\beta}=\\begin{pmatrix}-3.2  0.4  0.6\\end{pmatrix}^{\\top}$.\n- Site $1$: $x_1=\\begin{pmatrix}1  1.0  0\\end{pmatrix}^{\\top}$, $t_1=180$ person-years.\n- Site $2$: $x_2=\\begin{pmatrix}1  -0.5  1\\end{pmatrix}^{\\top}$, $t_2=320$ person-years.\n- Site $3$: $x_3=\\begin{pmatrix}1  2.0  1\\end{pmatrix}^{\\top}$, $t_3=140$ person-years.\n\nWe now compute the numerical values for each site.\n\n**For Site 1:**\nThe linear predictor is:\n$$\\hat{\\eta}_1 = x_1^{\\top}\\hat{\\beta} = (1)(-3.2) + (1.0)(0.4) + (0)(0.6) = -3.2 + 0.4 = -2.8$$\nThe fitted incidence rate is:\n$$\\hat{\\lambda}_1 = \\exp(-2.8) \\approx 0.06081006$$\nRounding to four significant figures, $\\hat{\\lambda}_1 = 0.06081$.\nThe fitted expected count is:\n$$\\hat{\\mu}_1 = t_1 \\hat{\\lambda}_1 = 180 \\times \\exp(-2.8) \\approx 10.94581$$\nRounding to four significant figures, $\\hat{\\mu}_1 = 10.95$.\n\n**For Site 2:**\nThe linear predictor is:\n$$\\hat{\\eta}_2 = x_2^{\\top}\\hat{\\beta} = (1)(-3.2) + (-0.5)(0.4) + (1)(0.6) = -3.2 - 0.2 + 0.6 = -2.8$$\nThe fitted incidence rate is:\n$$\\hat{\\lambda}_2 = \\exp(-2.8) \\approx 0.06081006$$\nRounding to four significant figures, $\\hat{\\lambda}_2 = 0.06081$.\nThe fitted expected count is:\n$$\\hat{\\mu}_2 = t_2 \\hat{\\lambda}_2 = 320 \\times \\exp(-2.8) \\approx 19.45922$$\nRounding to four significant figures, $\\hat{\\mu}_2 = 19.46$.\n\n**For Site 3:**\nThe linear predictor is:\n$$\\hat{\\eta}_3 = x_3^{\\top}\\hat{\\beta} = (1)(-3.2) + (2.0)(0.4) + (1)(0.6) = -3.2 + 0.8 + 0.6 = -1.8$$\nThe fitted incidence rate is:\n$$\\hat{\\lambda}_3 = \\exp(-1.8) \\approx 0.16529888$$\nRounding to four significant figures, $\\hat{\\lambda}_3 = 0.1653$.\nThe fitted expected count is:\n$$\\hat{\\mu}_3 = t_3 \\hat{\\lambda}_3 = 140 \\times \\exp(-1.8) \\approx 23.14184$$\nRounding to four significant figures, $\\hat{\\mu}_3 = 23.14$.\n\nThe final results, rounded to four significant figures, are:\n$\\hat{\\lambda}_1 = 0.06081$\n$\\hat{\\lambda}_2 = 0.06081$\n$\\hat{\\lambda}_3 = 0.1653$\n$\\hat{\\mu}_1 = 10.95$\n$\\hat{\\mu}_2 = 19.46$\n$\\hat{\\mu}_3 = 23.14$\n\nThese are consolidated into a single row matrix as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.06081  0.06081  0.1653  10.95  19.46  23.14\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The GLM framework provides a versatile toolkit for modeling diverse types of data, but its components must be assembled correctly. A common point of confusion is the role of offsets and how it differs between model families, such as Poisson and binomial. This problem  challenges you to think critically about model specification. By reasoning from first principles, you will diagnose why a modeling strategy that is essential for Poisson rate models is fundamentally inappropriate for binomial proportion models, thereby sharpening your intuition for correct GLM construction.",
            "id": "4914184",
            "problem": "A biostatistics study aggregates outcomes across clinical sites. For site $i$ with covariate vector $\\mathbf{x}_i \\in \\mathbb{R}^p$, $m_i$ individuals are tested and $y_i$ test positive, where $Y_i \\sim \\text{Binomial}(m_i,p_i)$ and $p_i \\in (0,1)$ is the site-specific probability of positivity. An analyst proposes to fit a Generalized Linear Model (GLM) with a logit link and to include $\\log m_i$ as an offset in the linear predictor to “adjust for group size.”\n\nUsing the Generalized Linear Model (GLM) framework from first principles, namely that a GLM specifies (i) a response distribution from a regular exponential family, (ii) a systematic component $\\eta_i=\\mathbf{x}_i^\\top \\beta + o_i$ with known offset $o_i$, and (iii) a link function $g$ such that $g(\\mu_i)=\\eta_i$ where $\\mu_i=E(Y_i)$, determine whether adding $\\log m_i$ as an offset in a binomial logit model is appropriate. Justify your reasoning by contrasting the roles of exposure in Poisson log-linear models and the number of trials in binomial models, starting from the definitions\n- for Poisson counts: $Y_i \\sim \\text{Poisson}(\\mu_i)$ with $\\log \\mu_i=\\mathbf{x}_i^\\top \\beta + \\log t_i$ when $\\mu_i=t_i \\lambda_i$ and $t_i$ is known exposure, and\n- for grouped binomial outcomes: $Y_i \\sim \\text{Binomial}(m_i,p_i)$ with canonical link $g(p_i)=\\log\\{p_i/(1-p_i)\\}$.\n\nSelect the single best option that both explains why including $\\log m_i$ as an offset in a binomial logit model is inappropriate and correctly specifies how to model grouped binomial outcomes in this setting.\n\nA. Include $\\log m_i$ as an offset in a logistic regression on the count $y_i$ so that the model estimates the log-odds of the expected count $\\mu_i=m_i p_i$, i.e., $\\text{logit}(\\mu_i)=\\mathbf{x}_i^\\top \\beta + \\log m_i$, which properly adjusts for differing group sizes.\n\nB. Model $Y_i \\sim \\text{Binomial}(m_i,p_i)$ and link the probability with $\\text{logit}(p_i)=\\mathbf{x}_i^\\top \\beta$, without any offset in the binomial model; implement this by using the two-column response $\\text{cbind}(y_i, m_i-y_i)$ or by regressing the proportion $y_i/m_i$ with binomial weights $m_i$. The term $\\log m_i$ is an appropriate offset only in Poisson log-linear models where $\\mu_i=t_i \\lambda_i$ and $\\log \\mu_i=\\log t_i + \\mathbf{x}_i^\\top \\beta$.\n\nC. Take the transformed response $\\log(y_i/m_i)$ and fit a Gaussian linear model with identity link, adding $\\log m_i$ as an offset to stabilize the variance across sites with different $m_i$.\n\nD. Fit a Poisson regression to $y_i$ with offset $\\log m_i$ to model the event rate $\\lambda_i$, which is exactly equivalent to the binomial logit model for any $p_i \\in (0,1)$, thus justifying the offset approach in the binomial setting.",
            "solution": "The user wants a critical evaluation of a proposed statistical model within the Generalized Linear Model (GLM) framework.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   A study aggregates data from multiple clinical sites, indexed by $i$.\n-   For site $i$, the covariate vector is $\\mathbf{x}_i \\in \\mathbb{R}^p$.\n-   At site $i$, $m_i$ individuals are tested, and $y_i$ test positive.\n-   The number of positive tests, $Y_i$, is modeled as a random variable following a binomial distribution: $Y_i \\sim \\text{Binomial}(m_i, p_i)$.\n-   The site-specific probability of positivity is $p_i \\in (0,1)$.\n-   An analyst proposes a GLM with a logit link, including $\\log m_i$ as an offset in the linear predictor.\n-   The task is to determine if this proposal is appropriate, justifying the answer by contrasting the roles of exposure in Poisson models and the number of trials in binomial models.\n-   The definition provided for a Poisson model is: $Y_i \\sim \\text{Poisson}(\\mu_i)$, with the model for the mean $\\mu_i$ being $\\log \\mu_i=\\mathbf{x}_i^\\top \\beta + \\log t_i$, derived from $\\mu_i=t_i \\lambda_i$ where $t_i$ is a known exposure.\n-   The definition provided for a grouped binomial model is: $Y_i \\sim \\text{Binomial}(m_i, p_i)$ with the canonical link function being the logit, $g(p_i)=\\log\\{p_i/(1-p_i)\\}$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is set within the established mathematical and statistical framework of Generalized Linear Models. The definitions of binomial and Poisson distributions, logit and log links, and the concept of an offset are standard in biostatistics. The problem is scientifically and factually sound.\n-   **Well-Posed**: The problem is well-defined. It asks for an evaluation of a specific modeling choice ($\\log m_i$ as an offset in a binomial logit model) based on first principles of GLMs. A unique and correct conclusion can be derived from these principles.\n-   **Objective**: The problem uses precise, objective, and standard statistical terminology. There is no ambiguity or subjective language.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, scientifically grounded question about the correct application of GLM theory. Proceeding to solution.\n\n### Derivation from First Principles\n\nA Generalized Linear Model (GLM) is defined by three components:\n1.  **Random Component**: The probability distribution of the response variable, $Y_i$, which must be a member of the exponential family.\n2.  **Systematic Component**: A linear predictor, $\\eta_i$, which is a linear combination of the covariates. In its general form, it is $\\eta_i = \\mathbf{x}_i^\\top \\beta + o_i$, where $o_i$ is a known offset.\n3.  **Link Function**: A monotonic, differentiable function $g$ that relates the expected value of the response, $\\mu_i = E(Y_i)$, to the linear predictor: $g(\\mu_i) = \\eta_i$.\n\nLet us analyze the two cases mentioned in the problem: the Poisson model and the binomial model.\n\n**Case 1: Poisson Log-Linear Model**\nFor a Poisson model, the response $Y_i$ represents a count of events.\n-   **Random Component**: $Y_i \\sim \\text{Poisson}(\\mu_i)$.\n-   The expected value is $E(Y_i) = \\mu_i$.\n-   In many applications, we are interested in modeling an event *rate*, $\\lambda_i$, over a certain known 'exposure' period or volume, $t_i$. The expected count is then the rate multiplied by the exposure: $\\mu_i = t_i \\lambda_i$.\n-   The canonical link function for the Poisson distribution is the log link, $g(\\mu_i) = \\log(\\mu_i)$.\n-   The systematic component models the log of the rate: $\\log(\\lambda_i) = \\mathbf{x}_i^\\top \\beta$.\n-   Combining these elements via the link function:\n    $$ g(\\mu_i) = \\log(\\mu_i) = \\log(t_i \\lambda_i) = \\log(t_i) + \\log(\\lambda_i) $$\n    Substituting the model for the log-rate, we get the linear predictor:\n    $$ \\eta_i = \\mathbf{x}_i^\\top \\beta + \\log(t_i) $$\n-   This matches the form $\\eta_i = \\mathbf{x}_i^\\top \\beta + o_i$, where the offset $o_i$ is $\\log(t_i)$. Here, the offset is a crucial part of the model, allowing us to model the rate $\\lambda_i$ while accounting for the varying exposures $t_i$. The offset has a coefficient fixed at $1$.\n\n**Case 2: Binomial Logit-Linear Model**\nHere, the response $Y_i$ is the number of 'successes' out of $m_i$ 'trials'.\n-   **Random Component**: $Y_i \\sim \\text{Binomial}(m_i, p_i)$. The parameter we wish to model is the probability of success, $p_i$.\n-   The expected value of the count is $E(Y_i) = m_i p_i$.\n-   Unlike the Poisson case, the GLM framework for the binomial distribution models the underlying probability parameter $p_i$, not the expected count $m_i p_i$. It is more natural to think of the response as the proportion $Y_i/m_i$, for which the expectation is $E(Y_i / m_i) = p_i$. The link function is applied to this expectation.\n-   The canonical link function for the binomial distribution is the logit link: $g(p_i) = \\log \\left(\\frac{p_i}{1-p_i}\\right)$.\n-   The systematic component is a linear function of the covariates: $\\eta_i = \\mathbf{x}_i^\\top \\beta$.\n-   Combining these gives the standard logistic regression model for grouped data:\n    $$ \\log \\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^\\top \\beta $$\n-   The number of trials, $m_i$, is not part of the linear predictor. Its role is to define the distribution of the random component. Specifically, it determines the precision of the observation: the variance of the proportion $Y_i/m_i$ is $\\text{Var}(Y_i/m_i) = \\frac{p_i(1-p_i)}{m_i}$. In the maximum likelihood estimation procedure for GLMs, $m_i$ acts as a weight, indicating that proportions from sites with larger $m_i$ are more informative and should be given more weight.\n\n**Evaluating the Analyst's Proposal**\nThe analyst proposes to include $\\log m_i$ as an offset in the binomial logit model. This would mean:\n$$ \\log \\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^\\top \\beta + \\log m_i $$\nThis model is structurally incorrect. It implies that the log-odds of a positive test depend directly on the number of people tested at the site, which is conceptually flawed. For a given set of covariates $\\mathbf{x}_i$, increasing the sample size $m_i$ should not systematically change the underlying population probability $p_i$. The number of trials $m_i$ is a feature of the study design at site $i$, not an attribute of the individuals that would affect their probability of testing positive. The analyst has incorrectly conflated the role of the number of trials ($m_i$) in a binomial model with the role of exposure ($t_i$) in a Poisson model. Therefore, including $\\log m_i$ as an offset is inappropriate.\n\n### Option-by-Option Analysis\n\n**A. Include $\\log m_i$ as an offset in a logistic regression on the count $y_i$ so that the model estimates the log-odds of the expected count $\\mu_i=m_i p_i$, i.e., $\\text{logit}(\\mu_i)=\\mathbf{x}_i^\\top \\beta + \\log m_i$, which properly adjusts for differing group sizes.**\nThis proposal is mathematically invalid. The logit function, $\\text{logit}(z) = \\log(z/(1-z))$, is defined for $z \\in (0,1)$. The expected count $\\mu_i = m_i p_i$ is not a probability and will generally not be in the interval $(0,1)$. For any site with $m_i  1$, it is possible for $\\mu_i$ to be greater than $1$, in which case $\\text{logit}(\\mu_i)$ is undefined.\n**Verdict**: Incorrect.\n\n**B. Model $Y_i \\sim \\text{Binomial}(m_i,p_i)$ and link the probability with $\\text{logit}(p_i)=\\mathbf{x}_i^\\top \\beta$, without any offset in the binomial model; implement this by using the two-column response $\\text{cbind}(y_i, m_i-y_i)$ or by regressing the proportion $y_i/m_i$ with binomial weights $m_i$. The term $\\log m_i$ is an appropriate offset only in Poisson log-linear models where $\\mu_i=t_i \\lambda_i$ and $\\log \\mu_i=\\log t_i + \\mathbf{x}_i^\\top \\beta$.**\nThis option correctly describes the standard and appropriate GLM for grouped binomial data: the model is $\\text{logit}(p_i) = \\mathbf{x}_i^\\top \\beta$, with no offset. It correctly describes the two common ways to implement this model in statistical software. Crucially, it also correctly identifies the source of the confusion by pointing out that the offset structure $\\log m_i$ (or more generally, $\\log t_i$) is appropriate for Poisson log-linear models, not binomial logit models. This option fully addresses the prompt.\n**Verdict**: Correct.\n\n**C. Take the transformed response $\\log(y_i/m_i)$ and fit a Gaussian linear model with identity link, adding $\\log m_i$ as an offset to stabilize the variance across sites with different $m_i$.**\nThis describes an alternative modeling approach that abandons the GLM framework for binomial data. The log transformation of a proportion has issues (e.g., undefined for $y_i=0$) and assuming a Gaussian distribution for the transformed variable is often a poor approximation. Furthermore, adding $\\log m_i$ as an offset in the model $E[\\log(y_i/m_i)] = \\mathbf{x}_i^\\top \\beta + \\log m_i$ has no theoretical justification. Offsets affect the mean structure, whereas variance stabilization is achieved by the transformation itself or by using weights; the claim that the offset \"stabilizes the variance\" is confused.\n**Verdict**: Incorrect.\n\n**D. Fit a Poisson regression to $y_i$ with offset $\\log m_i$ to model the event rate $\\lambda_i$, which is exactly equivalent to the binomial logit model for any $p_i \\in (0,1)$, thus justifying the offset approach in the binomial setting.**\nThis option proposes approximating the binomial distribution with a Poisson distribution. While this is sometimes done (especially when $p_i$ is small), the claim that this model is \"exactly equivalent to the binomial logit model\" is false. The Poisson model with offset $\\log m_i$ is a log-linear model, i.e., $\\log(E[Y_i]/m_i) = \\mathbf{x}_i^\\top \\beta$, which implies $\\log(p_i) \\approx \\mathbf{x}_i^\\top \\beta$. The binomial logit model is $\\text{logit}(p_i) = \\mathbf{x}_i^\\top \\beta$. As $\\log(p_i) \\neq \\log(p_i/(1-p_i))$, the models are not equivalent. Furthermore, their variance structures differ: $\\text{Var}_{\\text{Bin}}(Y_i) = m_i p_i(1-p_i) \\le m_i p_i = \\text{Var}_{\\text{Pois}}(Y_i)$. The equivalence is not exact, and this does not justify the use of an offset in the true binomial model.\n**Verdict**: Incorrect.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}