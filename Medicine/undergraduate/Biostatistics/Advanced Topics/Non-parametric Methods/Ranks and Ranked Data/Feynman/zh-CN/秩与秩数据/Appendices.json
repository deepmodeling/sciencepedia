{
    "hands_on_practices": [
        {
            "introduction": "在生物统计学中，我们经常需要评估两个变量之间的关联强度。经典的Pearson相关系数是衡量线性关系的有力工具，但它对数据中的极端值（或称离群值）非常敏感。本实践将引导你通过编程模拟，直观地比较Pearson相关系数与基于秩次的Spearman相关系数的稳健性。通过人为引入非线性关系和离群值 ()，你将亲手验证秩变换如何有效地“驯服”极端数据点的影响，从而深刻理解为何在数据分布不理想时，秩方法是更可靠的选择。",
            "id": "4946608",
            "problem": "您将实施一项模拟研究，以比较Spearman等级相关系数（Spearman's $\\rho_s$）和Pearson积矩相关系数（Pearson's $\\rho$）在存在重尾噪声和点质量污染的单调非线性关系下的经验稳健性。该研究必须基于第一性原理：样本相关性的定义、等级转换，以及单调递增函数和重尾分布的基本性质。\n\n请使用以下适用于生物统计学中等级数据的基本依据：\n- 两个等长为 $n$ 的实值向量 $x$ 和 $y$ 之间的样本Pearson相关性定义为 $x$ 和 $y$ 的标准化样本协方差。\n- 两个实值向量 $x$ 和 $y$ 之间的样本Spearman相关性是在等级转换后的数据上计算的样本Pearson相关性，其中每个观测值被其在所有观测值中的等级所取代（对并列值使用平均等级）。\n- 如果对于所有 $x_1,x_2\\in\\mathbb{R}$，当 $x_1 \\le x_2$ 时均有 $g(x_1) \\le g(x_2)$，则函数 $g:\\mathbb{R}\\to\\mathbb{R}$ 是单调递增的。\n- 重尾噪声由自由度较低的学生t分布表示。\n\n您的程序必须严格按照以下步骤执行：\n1. 对于每个测试用例，按如下方式模拟一个数据集 $\\{(X_i,Y_i)\\}_{i=1}^n$ 的 $R$ 次独立重复：\n   - 对 $i=1,\\dots,n$，独立生成 $X_i \\sim \\mathcal{N}(0,1)$。\n   - 生成独立噪声 $E_i \\sim t_{\\nu}$（自由度为 $\\nu$ 的学生t分布）。将噪声乘以一个固定尺度 $s$ 以控制其幅度，得到 $s\\,E_i$。\n   - 选择一个单调递增函数 $g$，并设置干净响应为 $Y_i^{(\\mathrm{clean})}=g(X_i)+s\\,E_i$。\n   - 通过无放回均匀抽样选择 $m=\\lfloor p\\,n\\rfloor$ 个索引，并通过 $Y_i^{(\\mathrm{cont})}=Y_i^{(\\mathrm{clean})}+M\\cdot S_i$ 替换这 $m$ 个值，来创建受污染的响应 $Y_i^{(\\mathrm{cont})}$。其中，$S_i$ 是独立的Rademacher符号，以等概率取值于 $\\{-1,+1\\}$。\n2. 对于每次重复，计算：\n   - 干净数据上的样本Pearson相关性 $\\widehat{\\rho}_{P}^{(\\mathrm{clean})}$，以及受污染数据上的 $\\widehat{\\rho}_{P}^{(\\mathrm{cont})}$。\n   - 干净数据上的样本Spearman相关性 $\\widehat{\\rho}_{S}^{(\\mathrm{clean})}$，以及受污染数据上的 $\\widehat{\\rho}_{S}^{(\\mathrm{cont})}$。\n3. 对于每次重复，计算绝对变化\n   $$\\Delta_P=\\left|\\widehat{\\rho}_{P}^{(\\mathrm{cont})}-\\widehat{\\rho}_{P}^{(\\mathrm{clean})}\\right|,\\qquad \\Delta_S=\\left|\\widehat{\\rho}_{S}^{(\\mathrm{cont})}-\\widehat{\\rho}_{S}^{(\\mathrm{clean})}\\right|.$$\n4. 将这些绝对变化在 $R$ 次重复中取平均，得到该测试用例的 $\\overline{\\Delta}_P$ 和 $\\overline{\\Delta}_S$。\n5. 对于每个测试用例，返回一个布尔值，表示 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 是否成立。\n\n您的实现必须遵循的科学原理：\n- 单调递增变换保持等级不变。因此，在没有污染的情况下，Spearman相关性在边际的严格单调递增变换下是不变的，而Pearson相关性则不是。\n- 重尾噪声增加了极端值的频率和幅度，从而放大了依赖于有限二阶矩的估计量的敏感性。众所周知，与基于等级的度量相比，Pearson相关性对此类极端值更为敏感。\n- 点质量污染引入了极端离群值，主要考验的是稳健性。\n\n在所有测试用例中使用以下固定常量：\n- 重复次数 $R = 400$。\n- 噪声尺度 $s = 0.5$。\n- 污染幅度 $M = 25$。\n- 随机数生成器种子固定为您选择的某个整数，以确保可复现性。\n\n使用以下单调递增函数 $g$：\n- $g_{\\mathrm{lin}}(x)=x$。\n- $g_{\\exp}(x)=\\exp(x/2)$。\n- $g_{\\mathrm{logit}}(x)=\\frac{1}{1+\\exp(-x)}$。\n- $g_{\\mathrm{cub}}(x)=x^3$。\n\n测试套件（按此确切顺序提供结果）：\n- 案例 1：$g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.1$。\n- 案例 2：$g=g_{\\exp}$, $n=200$, $\\nu=3$, $p=0.1$。\n- 案例 3：$g=g_{\\mathrm{logit}}$, $n=200$, $\\nu=1$, $p=0.1$。\n- 案例 4：$g=g_{\\mathrm{cub}}$, $n=80$, $\\nu=3$, $p=0.3$。\n- 案例 5：$g=g_{\\mathrm{lin}}$, $n=200$, $\\nu=3$, $p=0.0$。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个包含5个布尔值的列表，按顺序对应上述5个案例。当且仅当案例 $k$ 的 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 时，第 $k$ 个布尔值必须为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n- 该行必须格式化为Python风格的列表，使用逗号分隔且无空格，例如：$[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False}]$。\n- 不应打印任何其他文本。\n\n不涉及物理单位。不涉及角度。任何比例都必须以小数形式返回，但此处的最终输出仅包含布尔值。程序必须完全自包含，不需要用户输入、外部文件或网络访问。",
            "solution": "用户要求实现一个模拟研究，以评估Pearson和Spearman相关系数的稳健性。该问题在科学上是合理的、定义明确的，并且所有参数和过程都已明确规定。因此，提供一个解决方案是恰当的。\n\n### 基于原理的设计\n\n这个问题的核心在于比较两种常见的统计关联度量方法的稳健性：Pearson积矩相关系数 $\\widehat{\\rho}_P$ 和Spearman等级相关系数 $\\widehat{\\rho}_S$。在此背景下，稳健性指的是估计量在存在偏离理想假设（如离群值或非正态误差分布）的情况下的稳定性。\n\n**1. 理论基础**\n\n**Pearson相关性 ($\\widehat{\\rho}_P$)**：样本Pearson相关系数衡量两个连续变量 $x$ 和 $y$ 之间的线性关联。对于大小为 $n$ 的样本 $\\{(x_i, y_i)\\}_{i=1}^n$，它被定义为样本协方差除以样本标准差的乘积：\n$$ \\widehat{\\rho}_P(x,y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。此公式表明 $\\widehat{\\rho}_P$ 是直接从原始数据值计算的。因此，它对离群值高度敏感。单个极端数据点可以任意地放大或缩小平方和与交叉乘积之和，对均值、方差和协方差产生不成比例的影响，从而扭曲相关性估计。\n\n**Spearman相关性 ($\\widehat{\\rho}_S$)**：样本Spearman相关系数评估两个变量之间单调关系的强度。它是一种非参数度量。其关键特征是它操作的不是原始数据，而是数据的等级。其过程包括：\n1. 将向量 $x$ 转换为其等级的向量 $R_x$。\n2. 将向量 $y$ 转换为其等级的向量 $R_y$。如果出现并列值，则分配平均等级。\n3. 计算这些等级转换后向量的Pearson相关系数：\n$$ \\widehat{\\rho}_S(x,y) = \\widehat{\\rho}_P(R_x, R_y) $$\n通过将数据转换为等级，实际数值大小的影响被消除了。一个极端的离群值仅仅被赋予最高或最低的等级（例如，$1$ 或 $n$），这个值与所有其他等级处于相同的整数尺度上。这种转换为任何单个观测值的影响设定了界限，使得 $\\widehat{\\rho}_S$ 在本质上比 $\\widehat{\\rho}_P$ 对离群值更具稳健性。\n\n**2. 模拟方法论**\n\n该模拟旨在创建能够凭经验量化这些稳健性上理论差异的数据集。\n\n**数据生成**：\n- 预测变量 $X_i$ 从标准正态分布中抽取，$X_i \\sim \\mathcal{N}(0,1)$，这是一个常见的基准。\n- 应用一个单调递增函数 $g(X_i)$ 来创建一个非线性（或对于 $g(x)=x$ 则是线性）关系。由于 $\\widehat{\\rho}_S$ 衡量单调关联，即使关系不是线性的，它也预期表现良好，这与 $\\widehat{\\rho}_P$ 不同。\n- 添加重尾噪声 $s \\cdot E_i$，其中 $E_i \\sim t_{\\nu}$。自由度 $\\nu$ 较低（例如，$\\nu=1$ 对应柯西分布，$\\nu=3$）的学生t分布比正态分布有更重的尾部，这意味着它更频繁地产生极端值。这测试了估计量对非高斯噪声的敏感性。\n- 产生的“干净”数据是 $Y_i^{(\\mathrm{clean})} = g(X_i) + s \\cdot E_i$。\n- 对一部分比例为 $p$ 的数据引入点质量污染。一个大幅度的冲击 $M \\cdot S_i$ 被加到 $m = \\lfloor p \\cdot n\\rfloor$ 个随机选择的观测值上，以创建 $Y_i^{(\\mathrm{cont})}$。这模拟了严重的测量误差或数据损坏，产生了旨在对估计量进行压力测试的极端离群值。\n\n**量化稳健性**：\n该模拟通过量化数据被污染时每个相关性估计的变化程度来衡量稳健性。对于 $R=400$ 次重复中的每一次，我们计算：\n- Pearson相关性的变化：$\\Delta_P = |\\widehat{\\rho}_{P}^{(\\mathrm{cont})} - \\widehat{\\rho}_{P}^{(\\mathrm{clean})}|$。\n- Spearman相关性的变化：$\\Delta_S = |\\widehat{\\rho}_{S}^{(\\mathrm{cont})} - \\widehat{\\rho}_{S}^{(\\mathrm{clean})}|$。\n\n较小的 $\\Delta$ 值表示更强的稳健性。通过对多次重复的这些变化进行平均，我们获得了污染对每个系数预期影响的稳定估计 $\\overline{\\Delta}_P$ 和 $\\overline{\\Delta}_S$。最终的比较 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 直接判断在指定条件下哪个估计量更稳健。\n\n**3. 实现计划**\n\n该程序将使用Python的 `numpy` 和 `scipy` 库实现。\n- 为随机数生成器（`numpy.random.default_rng`）设置一个固定的整数种子，以确保可复现性。\n- 数据生成过程精确遵循指定的步骤。将使用 `numpy.random.Generator.normal` 和 `numpy.random.Generator.standard_t` 来生成随机变量。\n- 应用污染时，首先创建干净响应向量的副本，然后使用 `numpy.random.Generator.choice` 无放回地选择索引，最后添加缩放后的Rademacher噪声。\n- 使用 `numpy.corrcoef` 计算Pearson相关性。\n- 为了根据其定义计算Spearman相关性，首先使用 `scipy.stats.rankdata`（对并列值使用 `method='average'`）获取 $X$ 和 $Y$ 的等级，然后对这些等级向量应用 `numpy.corrcoef`。\n- 主逻辑遍历五个测试用例，为每个用例执行 $R$ 次重复。在每个用例中，每次重复都会计算并累加绝对变化 $\\Delta_P$ 和 $\\Delta_S$。\n- 在一个用例的所有重复完成后，计算平均值 $\\overline{\\Delta}_P$ 和 $\\overline{\\Delta}_S$，并存储比较结果 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 的布尔值。\n- 最后，以指定格式打印所有测试用例的布尔值列表。$p=0.0$ 的特殊情况作为一个对照组；在没有污染的情况下，$\\Delta_P$ 和 $\\Delta_S$ 都应为零，使得不等式 $\\overline{\\Delta}_S \\le \\overline{\\Delta}_P$ 成立。对于所有其他情况，理论预测由于基于等级的Spearman相关性具有更优的稳健性，结果将是相似的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef solve():\n    \"\"\"\n    Implements a simulation study to compare the robustness of Pearson's and \n    Spearman's correlation coefficients under various conditions.\n    \"\"\"\n    # Define fixed constants for the simulation\n    R = 400  # Number of replicates\n    S_SCALE = 0.5  # Noise scale s\n    M_CONTAMINATION = 25.0  # Contamination magnitude M\n    SEED = 42  # Fixed seed for reproducibility\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(SEED)\n\n    # Define the monotone increasing functions\n    def g_lin(x):\n        return x\n\n    def g_exp(x):\n        return np.exp(x / 2.0)\n\n    def g_logit(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def g_cub(x):\n        return x**3\n\n    # Define the test suite\n    test_cases = [\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_lin'},\n        {'g': g_exp, 'n': 200, 'nu': 3, 'p': 0.1, 'name': 'g_exp'},\n        {'g': g_logit, 'n': 200, 'nu': 1, 'p': 0.1, 'name': 'g_logit'},\n        {'g': g_cub, 'n': 80, 'nu': 3, 'p': 0.3, 'name': 'g_cub'},\n        {'g': g_lin, 'n': 200, 'nu': 3, 'p': 0.0, 'name': 'g_lin_no_cont'},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        g, n, nu, p = case['g'], case['n'], case['nu'], case['p']\n        \n        total_delta_p = 0.0\n        total_delta_s = 0.0\n\n        for _ in range(R):\n            # Step 1: Generate data\n            # Generate X_i ~ N(0, 1)\n            x_data = rng.normal(loc=0.0, scale=1.0, size=n)\n            \n            # Generate noise E_i ~ t_nu\n            noise = rng.standard_t(df=nu, size=n)\n            \n            # Generate clean response Y_i^(clean)\n            y_clean = g(x_data) + S_SCALE * noise\n            \n            # Create contaminated response Y_i^(cont)\n            y_cont = np.copy(y_clean)\n            m = int(np.floor(p * n))\n            \n            if m > 0:\n                indices_to_contaminate = rng.choice(n, size=m, replace=False)\n                signs = rng.choice([-1, 1], size=m)\n                y_cont[indices_to_contaminate] += M_CONTAMINATION * signs\n            \n            # Step 2: Compute correlations\n            # Pearson correlations\n            rho_p_clean = np.corrcoef(x_data, y_clean)[0, 1]\n            rho_p_cont = np.corrcoef(x_data, y_cont)[0, 1]\n\n            # Spearman correlations (as Pearson on ranks)\n            rank_x = rankdata(x_data, method='average')\n            rank_y_clean = rankdata(y_clean, method='average')\n            rank_y_cont = rankdata(y_cont, method='average')\n            \n            rho_s_clean = np.corrcoef(rank_x, rank_y_clean)[0, 1]\n            rho_s_cont = np.corrcoef(rank_x, rank_y_cont)[0, 1]\n\n            # Step 3: Compute absolute changes\n            delta_p = np.abs(rho_p_cont - rho_p_clean)\n            delta_s = np.abs(rho_s_cont - rho_s_clean)\n            \n            total_delta_p += delta_p\n            total_delta_s += delta_s\n\n        # Step 4: Average absolute changes\n        avg_delta_p = total_delta_p / R\n        avg_delta_s = total_delta_s / R\n\n        # Step 5: Compare and store boolean result\n        results.append(avg_delta_s <= avg_delta_p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "从关联性分析转向假设检验，Wilcoxon秩和检验是与参数t检验相对应的常用非参数方法，用于比较两组独立样本。教科书通常直接给出其正态近似的公式，但在小样本情况下，我们可以计算出精确的p值。本练习 () 将带你从组合数学的第一性原理出发，通过实现一个递归算法来构建检验统计量的完整零分布。这个过程将揭示非参数检验p值的“神秘面纱”，让你明白其概率基础并非来自某个理论分布，而是源于对数据所有可能排列的枚举。",
            "id": "4946603",
            "problem": "考虑两组独立的连续测量数据，且无重复值。A组的Wilcoxon秩和统计量定义为：将两组的所有观测值合并，按升序排列（最小的观测值记为秩1）后，A组所有观测值秩的总和。在组标签可交换的原假设下，秩和统计量的精确抽样分布来自于将$n_1$个A标签分配给$N=n_1+n_2$个合并观测值的所有可能方式，且每种分配方式的概率均等。\n\n您的任务是，从第一性原理出发，实现一个递归来计算小样本量下Wilcoxon秩和统计量的精确双边$p$值，并将其与带连续性校正的正态近似值进行比较。推导应从秩数据的核心定义和基本概率论出发，并且不得依赖于任何预先给出的完整分布的封闭形式快捷公式。您可以使用经过充分检验的结论，例如中心极限定理下独立同分布秩和的行为。\n\n定义和要求：\n- 设$A=\\{a_1,\\dots,a_{n_1}\\}$和$B=\\{b_1,\\dots,b_{n_2}\\}$，其中$n_1\\geq 1$, $n_2\\geq 1$，且所有值均不相同。令$N=n_1+n_2$。\n- 令$W$表示观测到的A组的Wilcoxon秩和。\n- 在原假设下，$W$的精确分布是由从$\\{1,2,\\dots,N\\}$中均匀地选择$n_1$个秩并求和而引出的。使用基于此组合定义的递归来计算，对于每个整数$s$，有多少种$n_1$个秩的分配方式可以使其和等于$s$。根据这些计数，计算精确的下尾概率$P(W \\le w_{\\text{obs}})$和上尾概率$P(W \\ge w_{\\text{obs}})$，然后计算精确的双边$p$值，其公式为$2\\cdot \\min\\{P(W \\le w_{\\text{obs}}), P(W \\ge w_{\\text{obs}})\\}$，且最大不超过$1$。\n- 对于带连续性校正的正态近似，从秩的基本性质推导出$W$在原假设下的均值和方差，引用中心极限定理将$W$近似为正态分布，并通过在标准化之前从绝对偏差$|w_{\\text{obs}}-\\mu|$中减去$0.5$来应用连续性校正。使用此方法计算近似的双边$p$值。\n\n测试套件：\n实现您的程序，为以下五个测试案例计算精确的双边$p$值和带连续性校正的正态近似值。在每个案例中，数组都包含不相同的实数：\n- 案例 1：$A=[5.1,4.7,2.3]$, $B=[1.0,3.5,6.0,7.2]$。\n- 案例 2：$A=[0.5]$, $B=[1.2,2.0,3.1,4.4,5.9]$。\n- 案例 3：$A=[9.0,8.0]$, $B=[1.0,2.0,3.0]$。\n- 案例 4：$A=[2.0,4.0,5.0,7.0]$, $B=[1.0,3.0,6.0,8.0]$。\n- 案例 5：$A=[10.0,1.5,3.3]$, $B=[0.7,2.2,4.8,5.0,7.1]$。\n\n输出规范：\n- 对于每个案例，输出一个包含三个小数的列表：精确的双边$p$值、带连续性校正的正态近似双边$p$值，以及两者之间的绝对差。每个小数四舍五入到$6$位。\n- 将五个案例的结果汇总为单行，形式为一个由这三个元素的列表组成的逗号分隔列表，并用方括号括起来，中间没有空格。\n- 您的程序必须是自包含的，不接受任何输入，并且仅使用指定的运行时环境。\n\n不涉及物理单位或角度，也不应打印百分比；要求使用小数。",
            "solution": "用户要求我解决一个关于Wilcoxon秩和检验的问题，这是生物统计学中的一个常见任务。该问题有效、有科学依据且定义明确。所有定义和条件都是标准的且自洽的。任务是为给定的数据集计算精确和近似的p值。我现在开始解答。\n\n### 第一部分：Wilcoxon秩和检验的形式化\n\n设有两组独立样本，A组有$n_1$个观测值，B组有$n_2$个观测值。我们假设数据是连续的，因此没有重复值。总观测数为$N = n_1 + n_2$。\n\nWilcoxon秩和检验是一种非参数检验，用于检验原假设（$H_0$），即两个样本来自同一分布。在$H_0$下，“A”和“B”的标签是可交换的。为执行此检验，我们首先合并所有$N$个观测值，并将它们从$1$（最小）到$N$（最大）进行排序。\n\n检验统计量$W$是分配给其中一组观测值的秩的总和。问题指定$W$为A组的秩和。\n\n在$H_0$下，为A组选择任意一组$n_1$个秩的概率都是相等的。从集合$\\{1, 2, \\dots, N\\}$中选择$n_1$个秩的总方法数由二项式系数$\\binom{N}{n_1}$给出。$W$的分布是从$\\{1, 2, \\dots, N\\}$中无放回地均匀随机选择的$n_1$个整数之和的分布。\n\n### 第二部分：通过递归计算精确分布\n\n为了找到$W$的精确抽样分布，我们需要计算有多少个不同的$n_1$个秩的集合其和为特定值$s$。这是一个经典的组合问题，可以通过递归关系解决，这适合使用动态规划实现。\n\n设$C(k, i, s)$为从集合$\\{1, 2, \\dots, k\\}$中选择$i$个不同整数，使其和恰好为$s$的方法数。我们希望找到所有可能的和值$s$的分布$C(N, n_1, s)$。\n\n递归关系通过考虑最大元素$k$来导出：\n1.  **情况1：不包含整数$k$。**我们必须从$\\{1, 2, \\dots, k-1\\}$中选择$i$个整数，其和为$s$。这样做的方法数是$C(k-1, i, s)$。\n2.  **情况2：包含整数$k$。**我们必须从$\\{1, 2, \\dots, k-1\\}$中选择$i-1$个整数，其和为$s-k$。这样做的方法数是$C(k-1, i-1, s-k)$。\n\n结合这些情况，递归关系是：\n$$C(k, i, s) = C(k-1, i, s) + C(k-1, i-1, s-k)$$\n基本情况是：\n- 对于任何$k \\ge 0$，$C(k, 0, 0) = 1$（选择0个项目的方法只有一种，其和为0）。\n- 如果$i>k$、$s0$，或者$i=0$且$s \\neq 0$，则$C(k, i, s) = 0$。此外，如果$s$小于可能的最小和（$\\sum_{j=1}^i j$）或大于可能的最大和（$\\sum_{j=k-i+1}^k j$），则$C(k,i,s)=0$。\n\n一旦计算出所有相关$s$的计数$C(N, n_1, s)$，观察到特定秩和$s$的概率是：\n$$P(W=s) = \\frac{C(N, n_1, s)}{\\binom{N}{n_1}}$$\n对于一个观测到的秩和$w_{\\text{obs}}$，下尾和上尾概率是：\n$$P(W \\le w_{\\text{obs}}) = \\sum_{s \\le w_{\\text{obs}}} P(W=s)$$\n$$P(W \\ge w_{\\text{obs}}) = \\sum_{s \\ge w_{\\text{obs}}} P(W=s)$$\n精确的双边p值则定义为$2 \\cdot \\min\\{P(W \\le w_{\\text{obs}}), P(W \\ge w_{\\text{obs}})\\}$, 且最大不超过$1$。\n\n### 第三部分：带连续性校正的正态近似\n\n对于较大的样本量，中心极限定理允许我们用正态分布来近似$W$的分布。为此，我们首先需要$W$在原假设下的均值（$\\mu_W$）和方差（$\\sigma_W^2$）。\n\n统计量$W$是从秩的总体$\\{1, 2, \\dots, N\\}$中无放回地随机抽取的$n_1$个秩的和。\n这个秩总体的均值是：\n$$ E[\\text{rank}] = \\frac{1}{N} \\sum_{j=1}^{N} j = \\frac{N(N+1)}{2N} = \\frac{N+1}{2} $$\n这个秩总体的方差是：\n$$ \\sigma^2_{\\text{rank}} = E[\\text{rank}^2] - (E[\\text{rank}])^2 = \\left(\\frac{1}{N} \\sum_{j=1}^{N} j^2\\right) - \\left(\\frac{N+1}{2}\\right)^2 $$\n使用平方和公式 $\\sum_{j=1}^N j^2 = \\frac{N(N+1)(2N+1)}{6}$，我们得到：\n$$ \\sigma^2_{\\text{rank}} = \\frac{(N+1)(2N+1)}{6} - \\frac{(N+1)^2}{4} = \\frac{N+1}{12} [2(2N+1) - 3(N+1)] = \\frac{N^2-1}{12} $$\n$W$的期望是抽取的$n_1$个秩的期望之和：\n$$ \\mu_W = E[W] = E\\left[\\sum_{i=1}^{n_1} R_i\\right] = \\sum_{i=1}^{n_1} E[R_i] = n_1 E[\\text{rank}] = \\frac{n_1(N+1)}{2} $$\n从大小为$N$的总体中无放回抽样$n_1$个项目的和的方差由 $Var(\\sum R_i) = n_1 \\sigma^2_{\\text{rank}} \\frac{N-n_1}{N-1}$ 给出。代入总体方差：\n$$ \\sigma_W^2 = Var(W) = n_1 \\left(\\frac{N^2-1}{12}\\right) \\frac{N-n_1}{N-1} = n_1 \\frac{(N-1)(N+1)}{12} \\frac{n_2}{N-1} = \\frac{n_1 n_2 (N+1)}{12} $$\n$W$的零分布近似为$\\mathcal{N}(\\mu_W, \\sigma_W^2)$。由于$W$是一个离散变量，应用连续性校正以改善近似效果。校正是将观测值向均值方向调整$0.5$。标准化的检验统计量$Z_c$是：\n$$ Z_c = \\frac{|w_{\\text{obs}} - \\mu_W| - 0.5}{\\sigma_W} $$\n如果$|w_{\\text{obs}} - \\mu_W| \\le 0.5$，分子变为非正数，导致$Z_c \\le 0$。双边p值计算为$2 \\cdot P(Z > Z_c)$，其中$Z \\sim \\mathcal{N}(0,1)$是一个标准正态变量。如果$Z_c \\le 0$，这个p值将$\\ge 1$，因此取为$1$。在其他情况下，它是从标准正态分布的生存函数计算得出的。\n\n### 第四部分：测试案例的实现\n\n该解决方案用Python实现。一个辅助函数为每个测试案例计算观测到的秩和$w_{\\text{obs}}$和样本大小。$W$的精确分布使用递归公式并通过动态规划实现，以避免重复计算。从该分布中，计算出精确的p值。另一个函数计算$W$的均值和方差，并使用带连续性校正的正态近似来找到近似p值。然后计算绝对差值。所有测试案例的结果按指定格式进行格式化。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import comb\n\ndef get_wilcoxon_rank_sum(A, B):\n    \"\"\"\n    Calculates the Wilcoxon rank-sum statistic W for group A.\n    \n    Args:\n        A (list): Observations for group A.\n        B (list): Observations for group B.\n    \n    Returns:\n        tuple: (w_obs, n1, n2) where w_obs is the rank-sum for group A,\n               n1 is the size of A, and n2 is the size of B.\n    \"\"\"\n    n1 = len(A)\n    n2 = len(B)\n    \n    # Pool data and assign group labels\n    pooled_data = []\n    for x in A:\n        pooled_data.append((x, 'A'))\n    for x in B:\n        pooled_data.append((x, 'B'))\n    \n    # Sort pooled data by value to determine ranks\n    pooled_data.sort(key=lambda item: item[0])\n    \n    # Calculate rank sum for group A\n    w_obs = 0\n    for i, item in enumerate(pooled_data):\n        rank = i + 1  # Ranks are 1-based\n        if item[1] == 'A':\n            w_obs += rank\n            \n    return w_obs, n1, n2\n\nmemo_dist = {}\ndef compute_rank_sum_dist(n, k):\n    \"\"\"\n    Computes the exact distribution of the sum of n ranks chosen from {1,...,k}.\n    This is based on the recurrence C(k,i,s) = C(k-1,i,s) + C(k-1,i-1,s-k), where C\n    is the number of ways to choose i items from {1..k} to sum to s. This function\n    uses dynamic programming to find the counts for all possible sums.\n\n    Args:\n        n (int): The number of ranks to choose (n1).\n        k (int): The total number of ranks to choose from (N).\n    \n    Returns:\n        np.ndarray: An array where the index represents the sum and the value\n                    is the count of combinations yielding that sum.\n    \"\"\"\n    if (n, k) in memo_dist:\n        return memo_dist[(n, k)]\n\n    # The max possible sum for n items chosen from {1..k}\n    max_s = int(n * k - n * (n - 1) / 2)\n    \n    # DP table: dp[i][s] = # ways to choose i items with sum s from ranks {1..current_k}\n    # We use a space-optimized DP with a single 2D array.\n    dp = np.zeros((n + 1, max_s + 1), dtype=np.uint64)\n    dp[0, 0] = 1\n\n    for current_rank in range(1, k + 1):\n        # Iterate i backwards to use results from the previous 'current_rank' step\n        for i in range(min(current_rank, n), 0, -1):\n            for s in range(1, max_s + 1):\n                if s >= current_rank:\n                    dp[i, s] += dp[i - 1, s - current_rank]\n\n    dist = dp[n, :]\n    memo_dist[(n, k)] = dist\n    return dist\n\ndef get_exact_p_value(w_obs, n1, n2):\n    \"\"\"Calculates the exact two-sided p-value.\"\"\"\n    N = n1 + n2\n    \n    dist_counts = compute_rank_sum_dist(n1, N)\n    total_combinations = comb(N, n1, exact=True)\n    if total_combinations == 0: return 1.0\n    \n    # Calculate lower-tail and upper-tail probabilities\n    p_le = np.sum(dist_counts[:w_obs + 1]) / total_combinations\n    p_ge = np.sum(dist_counts[w_obs:]) / total_combinations\n    \n    p_val = 2 * min(p_le, p_ge)\n    return min(p_val, 1.0)\n\ndef get_normal_approx_p_value(w_obs, n1, n2):\n    \"\"\"Calculates the two-sided p-value using normal approximation with continuity correction.\"\"\"\n    N = n1 + n2\n    mu_W = n1 * (N + 1) / 2.0\n    var_W = n1 * n2 * (N + 1) / 12.0\n    \n    if var_W == 0:\n        return 1.0 if w_obs == mu_W else 0.0\n\n    sigma_W = np.sqrt(var_W)\n    deviation = abs(w_obs - mu_W)\n    \n    # Standardized score with continuity correction\n    z_c = (deviation - 0.5) / sigma_W\n    \n    if z_c < 0:\n        p_val_approx = 1.0\n    else:\n        # Two-sided p-value is 2 * P(Z > z_c)\n        p_val_approx = 2 * norm.sf(z_c)\n        \n    return min(p_val_approx, 1.0)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        ([5.1, 4.7, 2.3], [1.0, 3.5, 6.0, 7.2]),\n        ([0.5], [1.2, 2.0, 3.1, 4.4, 5.9]),\n        ([9.0, 8.0], [1.0, 2.0, 3.0]),\n        ([2.0, 4.0, 5.0, 7.0], [1.0, 3.0, 6.0, 8.0]),\n        ([10.0, 1.5, 3.3], [0.7, 2.2, 4.8, 5.0, 7.1]),\n    ]\n    \n    all_results = []\n\n    for A, B in test_cases:\n        w_obs, n1, n2 = get_wilcoxon_rank_sum(A, B)\n        \n        p_exact = get_exact_p_value(w_obs, n1, n2)\n        p_approx = get_normal_approx_p_value(w_obs, n1, n2)\n        abs_diff = abs(p_exact - p_approx)\n        \n        all_results.append(f\"[{p_exact:.6f},{p_approx:.6f},{abs_diff:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理想化的连续数据中，秩次是唯一的，但现实世界的生物统计数据——尤其是来自问卷调查的李克特量表（Likert scale）数据——常常包含大量相同的值，这导致了“平分秩”（tied ranks）的出现。平分秩的存在改变了检验统计量的零分布，使得前一个练习中的简单递归方法不再适用。本实践 () 将教你如何处理这一常见情况，通过精确枚举所有可能的分组情况来构建正确的零分布，并计算精确p值。掌握这项技能对于在实际研究中正确应用秩检验至关重要。",
            "id": "4946639",
            "problem": "您的任务是为秩统计量实现一种精确的、基于排列的零分布调整方法，该方法适用于结果为离散值（如李克特量表）的情况。在可交换性的零假设下，组成员身份的标签在合并后的响应多重集上是均匀随机分布的。对于离散结果，会出现秩次相同（即秩结）的情况，此时必须使用平均秩。您的任务是推导、实现并评估一种精确的枚举策略，该策略要遵循观测到的李克特变量的合并后各分类的计数。\n\n基本原理：\n- 在没有组间效应的零假设下，组标签在所有合并的观测值之间是可交换的。\n- 通过分配平均秩来处理秩结：同一类别中的所有观测值都获得它们在任意打破秩结时所占秩次的平均值。\n- 对于一个大小为 $n_1$ 的组，其威尔科克森秩和统计量 $W$ 是该组中观测值的平均秩之和。\n- 合并后的观测值总数为 $N = n_1 + n_2$，零分布下 $W$ 的均值为 $\\mu = n_1 (N + 1)/2$。\n\n您的程序必须：\n1.  通过对唯一的李克特类别进行升序排序，并为每个合并后大小为 $c_k$ 的秩结块分配平均秩 $m_k$（等于该块秩次的平均值），从而从合并数据中计算出平均秩。\n2.  将威尔科克森秩和统计量表示为 $W = \\sum_{k} x_k \\, m_k$，其中 $x_k$ 是 A 组中属于类别 $k$ 的观测值数量。向量 $(x_k)$ 的取值范围是所有满足 $0 \\le x_k \\le c_k$ 和 $\\sum_k x_k = n_1$ 的整数分配。\n3.  使用按组合计数加权的精确枚举来构建 $W$ 的精确零分布。每种分配 $(x_k)$ 对应 $\\prod_k \\binom{c_k}{x_k}$ 种不同的标签分配方式，在零假设下，每种方式都是等可能的。标签分配的总数为 $\\binom{N}{n_1}$。因此，一种分配 $(x_k)$ 的概率为 $\\left(\\prod_k \\binom{c_k}{x_k}\\right) \\big/ \\binom{N}{n_1}$。\n4.  将具有相同 $W$ 值的分配聚合起来，得到 $W$ 的精确概率质量函数，并计算：\n    -   根据实际的分组情况计算出的观测统计量 $w_{\\text{obs}}$。\n    -   左单侧 $p$ 值 $p_{\\text{left}} = \\mathbb{P}(W \\le w_{\\text{obs}})$。\n    -   右单侧 $p$ 值 $p_{\\text{right}} = \\mathbb{P}(W \\ge w_{\\text{obs}})$。\n    -   根据与均值的距离定义的双侧 $p$ 值：$p_{\\text{two}} = \\mathbb{P}(|W - \\mu| \\ge |w_{\\text{obs}} - \\mu|)$。\n5.  为避免计算中出现小数平均秩，将所有平均秩乘以因子 $2$，使 $W$ 成为一个整数值统计量 $W^{\\ast} = 2 W$，类似地，$\\mu^{\\ast} = 2 \\mu = n_1 (N + 1)$。使用这种缩放形式执行精确枚举并计算概率。\n\n测试套件：\n实现您的程序以处理以下四个测试用例。在每个用例中，A 组和 B 组都以李克特响应列表的形式给出。所有李克特值均为整数，必须从合并数据中计算平均秩。\n\n-   用例 1（存在多个秩结的理想情况）：\n    -   A 组：$[$1$,$2$,$4$,$4$,$5$]$\n    -   B 组：$[$1$,$1$,$3$,$4$,$5$,$5$,$5$]$\n\n-   用例 2（退化的全相等类别）：\n    -   A 组：$[$3$,$3$]$\n    -   B 组：$[$3$,$3$,$3$]$\n\n-   用例 3（合并数据中无秩结）：\n    -   A 组：$[$1$,$2$]$\n    -   B 组：$[$3$,$4$,$5$]$\n\n-   用例 4（样本量平衡且中间值有秩结）：\n    -   A 组：$[$2$,$2$,$3$,$4$]$\n    -   B 组：$[$1$,$2$,$3$,$3$]$\n\n所需输出：\n-   对于每个用例，输出一个列表 $[p_{\\text{two}}, p_{\\text{left}}, p_{\\text{right}}]$，其中每个条目都是一个精确到小数点后 $6$ 位的浮点数。\n-   您的程序应生成单行输出，其中包含所有用例的结果，格式为一个由各用例列表组成的逗号分隔列表，并用方括号括起来。例如：$[[p_{1,\\text{two}},p_{1,\\text{left}},p_{1,\\text{right}}],[p_{2,\\text{two}},p_{2,\\text{left}},p_{2,\\text{right}}],\\dots]$。",
            "solution": "用户提供的问题是生物统计学领域一个有效且定义明确的问题，具体涉及使用离散数据进行非参数假设检验。所有提供的信息在科学上都是合理的、自洽的，并且足以推导出唯一的解决方案。问题的核心是为威尔科克森秩和统计量实现一个精确的排列检验，同时考虑秩次相同的情况，这是统计分析中的一个标准程序。\n\n方法概述如下：\n\n1.  **数据合并与秩计算**：对于每个测试用例，将 A 组（大小为 $n_1$）和 B 组（大小为 $n_2$）的数据合并成一个大小为 $N = n_1 + n_2$ 的单一数据集。识别并排序唯一值（李克特类别）。假设有 $K$ 个唯一类别。对于每个类别 $k \\in \\{1, \\dots, K\\}$，我们计算其在合并数据中的总频率 $c_k$。\n    类别 $k$ 中的 $c_k$ 个观测值的秩占据从 $\\left(1 + \\sum_{j=1}^{k-1} c_j\\right)$ 到 $\\left(\\sum_{j=1}^{k} c_j\\right)$ 的位置。类别 $k$ 的平均秩 $m_k$ 是这些秩的平均值：\n    $$m_k = \\frac{\\left(1 + \\sum_{j=1}^{k-1} c_j\\right) + \\left(\\sum_{j=1}^{k} c_j\\right)}{2} = \\left(\\sum_{j=1}^{k-1} c_j\\right) + \\frac{c_k + 1}{2}$$\n    为避免在枚举过程中进行浮点运算，我们使用一个缩放后的平均秩 $m_k^* = 2m_k$，它保证为整数：\n    $$m_k^* = 2\\left(\\sum_{j=1}^{k-1} c_j\\right) + c_k + 1$$\n\n2.  **观测统计量与零假设参数**：A 组的威尔科克森秩和统计量 $W$ 是其观测值的秩之和。这可以用类别计数来表示。设 $x_k$ 是 A 组中属于类别 $k$ 的观测值数量。则缩放后的统计量 $W^*$ 为：\n    $$W^* = \\sum_{k=1}^{K} x_k m_k^*$$\n    我们使用提供的 A 组数据中的实际计数 $x_{k, \\text{obs}}$ 来计算该统计量的观测值 $w_{\\text{obs}}^*$。\n    在可交换性的零假设下，$W$ 分布的均值为 $\\mu = \\frac{n_1(N+1)}{2}$。缩放后的均值为 $\\mu^* = 2\\mu = n_1(N+1)$，它是一个整数。\n\n3.  **零分布的精确枚举**：关键步骤是构建在零假设下 $W^*$ 的精确概率质量函数（PMF）。零假设意味着从 $N$ 个观测值的合并集中任选 $n_1$ 个观测值构成 A 组的概率是均等的。选择 A 组的总方式数为 $\\binom{N}{n_1}$。\n    一个在计算上更高效的等价视角是，考虑将 A 组的 $n_1$ 个观测值分配到 $K$ 个类别中的所有可能方式。一个分配是一个计数向量 $(x_1, x_2, \\dots, x_K)$，满足以下约束条件：\n    $$0 \\le x_k \\le c_k \\quad \\text{for all } k=1, \\dots, K$$\n    $$\\sum_{k=1}^{K} x_k = n_1$$\n    对于每个这样的有效分配，构成 A 组的方式数是二项式系数的乘积 $\\prod_{k=1}^{K} \\binom{c_k}{x_k}$。此分配的概率为：\n    $$\\mathbb{P}(x_1, \\dots, x_K) = \\frac{\\prod_{k=1}^{K} \\binom{c_k}{x_k}}{\\binom{N}{n_1}}$$\n    我们使用递归回溯算法生成所有有效的分配向量 $(x_k)$。对于每种分配，我们计算其对应的统计量 $W^* = \\sum_k x_k m_k^*$ 及其概率。通过将产生相同 $W^*$ 值的所有分配的概率相加，构建出 $W^*$ 的完整 PMF。\n\n4.  **P值计算**：有了 $W^*$ 的完整 PMF，我们根据观测统计量 $w_{\\text{obs}}^*$ 和缩放后的均值 $\\mu^*$ 计算所需的 p 值：\n    -   **左单侧 p 值**：$p_{\\text{left}} = \\mathbb{P}(W^* \\le w_{\\text{obs}}^*) = \\sum_{w^* \\le w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **右单侧 p 值**：$p_{\\text{right}} = \\mathbb{P}(W^* \\ge w_{\\text{obs}}^*) = \\sum_{w^* \\ge w_{\\text{obs}}^*} \\mathbb{P}(W^*=w^*)$\n    -   **双侧 p 值**：这基于与均值的距离。设 $d_{\\text{obs}} = |w_{\\text{obs}}^* - \\mu^*|$。p 值是观测到至少同样极端结果的概率：\n        $$p_{\\text{two}} = \\mathbb{P}(|W^* - \\mu^*| \\ge d_{\\text{obs}}) = \\sum_{w^* \\text{ s.t. } |w^*-\\mu^*| \\ge d_{\\text{obs}}} \\mathbb{P}(W^*=w^*)$$\n    由于 $W^*$、$\\mu^*$ 和 $w_{\\text{obs}}^*$ 都是整数，这些计算是精确的，不涉及浮点数比较问题。\n\n实现将遵循此逻辑。一个递归函数将处理分配的枚举。将使用 `scipy.special.comb` 函数来计算二项式系数，以确保准确性并处理可能的大数，尽管测试用例足够小，这不成主要问题。最后，计算出的 p 值按要求四舍五入到小数点后 $6$ 位。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom collections import defaultdict\n\ndef calculate_p_values(group_a, group_b):\n    \"\"\"\n    Computes exact permutation p-values for the Wilcoxon rank-sum test with ties.\n    \n    Args:\n        group_a (list): List of observations for group A.\n        group_b (list): List of observations for group B.\n        \n    Returns:\n        list: A list containing [p_two, p_left, p_right] rounded to 6 decimal places.\n    \"\"\"\n    \n    # Step 1: Initialize data and basic parameters\n    n1 = len(group_a)\n    n2 = len(group_b)\n    N = n1 + n2\n    \n    # Handle trivial case where a group is empty\n    if n1 == 0 or n2 == 0:\n        return [1.0, 1.0, 1.0]\n\n    pooled_data = np.concatenate((group_a, group_b))\n    \n    # Step 2: Compute category counts and scaled midranks\n    unique_cats, cat_indices = np.unique(pooled_data, return_inverse=True)\n    num_cats = len(unique_cats)\n    \n    # c_k: counts of each unique category in the pooled data\n    c_k = np.bincount(cat_indices, minlength=num_cats)\n    \n    # x_k_obs: observed counts of each unique category in group A\n    group_a_cats = cat_indices[:n1]\n    x_k_obs = np.bincount(group_a_cats, minlength=num_cats)\n    \n    # m_star_k: scaled midranks for each category\n    cum_c = np.concatenate(([0], np.cumsum(c_k)))\n    m_star_k = 2 * cum_c[:-1] + c_k + 1\n\n    # Step 3: Compute observed statistic and mean\n    w_star_obs = np.sum(x_k_obs * m_star_k)\n    mu_star = n1 * (N + 1)\n\n    # Step 4: Enumerate all possible allocations and build the null distribution\n    distribution = defaultdict(float)\n    total_perms = comb(N, n1, exact=True)\n\n    allocations = []\n    def find_allocations(k_idx, n1_rem, current_alloc):\n        \"\"\"\n        Recursively find all valid allocations (x_k vectors).\n        \"\"\"\n        if k_idx == num_cats:\n            if n1_rem == 0:\n                allocations.append(current_alloc)\n            return\n\n        # Iterate through possible counts x_k for the current category\n        max_count = min(c_k[k_idx], n1_rem)\n        for x_k in range(max_count + 1):\n            find_allocations(k_idx + 1, n1_rem - x_k, current_alloc + [x_k])\n\n    find_allocations(0, n1, [])\n\n    for alloc_vec in allocations:\n        # Calculate the statistic W* for this allocation\n        w_star = np.sum(np.array(alloc_vec) * m_star_k)\n        \n        # Calculate the number of permutations for this allocation\n        prob_numerator = 1\n        for i in range(num_cats):\n            prob_numerator *= comb(c_k[i], alloc_vec[i], exact=True)\n            \n        # Add probability to the distribution map\n        probability = prob_numerator / total_perms\n        distribution[w_star] += probability\n\n    # Step 5: Calculate p-values from the null distribution\n    p_left = 0.0\n    p_right = 0.0\n    p_two = 0.0\n    \n    # d_obs is integer, as are all w_star and mu_star\n    d_obs = abs(w_star_obs - mu_star)\n\n    for w_star, prob in distribution.items():\n        if w_star <= w_star_obs:\n            p_left += prob\n        if w_star >= w_star_obs:\n            p_right += prob\n        if abs(w_star - mu_star) >= d_obs:\n            p_two += prob\n            \n    # Round results to 6 decimal places as required\n    return [round(p_two, 6), round(p_left, 6), round(p_right, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path with multiple ties)\n        {'group_a': [1, 2, 4, 4, 5], 'group_b': [1, 1, 3, 4, 5, 5, 5]},\n        \n        # Case 2 (degenerate all-equal category)\n        {'group_a': [3, 3], 'group_b': [3, 3, 3]},\n        \n        # Case 3 (no ties in the pooled data)\n        {'group_a': [1, 2], 'group_b': [3, 4, 5]},\n        \n        # Case 4 (balanced sizes with central ties)\n        {'group_a': [2, 2, 3, 4], 'group_b': [1, 2, 3, 3]}\n    ]\n    \n    results = []\n    for case in test_cases:\n        p_values = calculate_p_values(case['group_a'], case['group_b'])\n        results.append(p_values)\n        \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}