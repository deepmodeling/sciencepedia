{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's focus on the fundamental structure of the Analysis of Variance (ANOVA) table. This first exercise strips away the initial data-processing steps and provides you with the essential summary quantities directly. Your task is to use these values to complete the table and calculate the F-statistic, which is used to test the overall significance of the regression model . This practice helps solidify your understanding of the core relationships between the sums of squares, degrees of freedom, and the final test statistic.",
            "id": "1895430",
            "problem": "An agricultural scientist is studying the relationship between the amount of a newly developed nutrient supplement applied to soil and the subsequent height of a particular plant species. A simple linear regression model is fitted to the collected data to predict plant height based on the quantity of the supplement.\n\nAfter fitting the model, a statistical analysis yielded the following quantities:\n- The Total Sum of Squares ($SST$), which measures the total variance in the plant heights, was calculated to be $100.0$.\n- The Sum of Squares for Regression ($SSR$), which represents the variation in plant height explained by the regression model, was found to be $40.0$.\n- The analysis was based on a sample size such that the degrees of freedom for the error (residual) term is $df_E = 10$.\n\nBased on these results from the Analysis of Variance (ANOVA), calculate the F-statistic used to test the overall significance of the simple linear regression model. Express your answer as a number rounded to three significant figures.",
            "solution": "In one-predictor simple linear regression with an intercept, the ANOVA partitions total variability as $SST = SSR + SSE$. Thus the error sum of squares is\n$$\nSSE = SST - SSR.\n$$\nThe regression and error mean squares are defined by\n$$\nMSR = \\frac{SSR}{df_{R}}, \\quad MSE = \\frac{SSE}{df_{E}},\n$$\nand the overall F-statistic is\n$$\nF = \\frac{MSR}{MSE} = \\frac{\\frac{SSR}{df_{R}}}{\\frac{SSE}{df_{E}}}.\n$$\nFor simple linear regression, $df_{R} = 1$. Using the given values $SST = 100.0$, $SSR = 40.0$, and $df_{E} = 10$, compute\n$$\nSSE = 100.0 - 40.0 = 60.0,\n$$\n$$\nMSR = \\frac{40.0}{1} = 40.0, \\quad MSE = \\frac{60.0}{10} = 6.0,\n$$\nso\n$$\nF = \\frac{40.0}{6.0} = 6.\\overline{6}.\n$$\nRounded to three significant figures, the F-statistic is $6.67$.",
            "answer": "$$\\boxed{6.67}$$"
        },
        {
            "introduction": "Building on the foundational mechanics, this next practice challenges you to construct the ANOVA table from scratch using a raw dataset. You will calculate the Total Sum of Squares ($SST$), Regression Sum of Squares ($SSR$), and Error Sum of Squares ($SSE$) by first fitting a simple linear regression model . This hands-on process demystifies these abstract quantities by connecting them directly to the data, the fitted line, and the residuals, reinforcing the concept of variance partitioning.",
            "id": "4893782",
            "problem": "A biostatistician examines a simple linear regression of a continuous health outcome on a dietary exposure. The design matrix $X$ includes an intercept and a single continuous predictor $x$. For $n=5$ subjects, the predictor values and observed outcomes are\n$$\nx = \\begin{pmatrix}0 & 1 & 2 & 3 & 4\\end{pmatrix}, \\quad y = \\begin{pmatrix}2.2 & 3.5 & 5.3 & 6.7 & 8.0\\end{pmatrix}.\n$$\nAssume the usual fixed-design linear model $y = X\\beta + \\varepsilon$ with an intercept and one predictor, and fit the model by ordinary least squares (OLS). Using the analysis of variance (ANOVA) framework for regression, compute the Total Sum of Squares (SST), the Regression Sum of Squares (SSR), and the Error Sum of Squares (SSE) explicitly from this dataset, and verify numerically that the decomposition $\\text{SST} = \\text{SSR} + \\text{SSE}$ holds. Round your three reported sums of squares to four significant figures. Express your final answer as a row vector in the order $\\big(\\text{SST},\\,\\text{SSR},\\,\\text{SSE}\\big)$.",
            "solution": "The problem requires the computation of the components of variance from a simple linear regression analysis: Total Sum of Squares (SST), Regression Sum of Squares (SSR), and Error Sum of Squares (SSE). The model is given by $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ for $i=1, \\dots, n$.\n\nFirst, we calculate the sample means of the predictor $x$ and the outcome $y$. The sample size is $n=5$.\n\nThe sum of the predictor values is:\n$$ \\sum_{i=1}^5 x_i = 0 + 1 + 2 + 3 + 4 = 10 $$\nThe mean of the predictor is:\n$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^5 x_i = \\frac{10}{5} = 2 $$\n\nThe sum of the outcome values is:\n$$ \\sum_{i=1}^5 y_i = 2.2 + 3.5 + 5.3 + 6.7 + 8.0 = 25.7 $$\nThe mean of the outcome is:\n$$ \\bar{y} = \\frac{1}{n} \\sum_{i=1}^5 y_i = \\frac{25.7}{5} = 5.14 $$\n\n**1. Total Sum of Squares (SST)**\nSST measures the total variability in the outcome variable $y$. It is defined as the sum of the squared deviations of each observation from the sample mean $\\bar{y}$.\n$$ \\text{SST} = \\sum_{i=1}^n (y_i - \\bar{y})^2 $$\nUsing the given data:\n$$ \\text{SST} = (2.2 - 5.14)^2 + (3.5 - 5.14)^2 + (5.3 - 5.14)^2 + (6.7 - 5.14)^2 + (8.0 - 5.14)^2 $$\n$$ \\text{SST} = (-2.94)^2 + (-1.64)^2 + (0.16)^2 + (1.56)^2 + (2.86)^2 $$\n$$ \\text{SST} = 8.6436 + 2.6896 + 0.0256 + 2.4336 + 8.1796 = 21.972 $$\n\n**2. Ordinary Least Squares (OLS) Estimation**\nTo calculate SSR and SSE, we must first fit the regression line $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ by OLS. The estimators for the slope ($\\hat{\\beta}_1$) and intercept ($\\hat{\\beta}_0$) are:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nFirst, we compute $S_{xx}$ and $S_{xy}$:\n$$ S_{xx} = (0-2)^2 + (1-2)^2 + (2-2)^2 + (3-2)^2 + (4-2)^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10 $$\n$$ S_{xy} = (0-2)(2.2-5.14) + (1-2)(3.5-5.14) + (2-2)(5.3-5.14) + (3-2)(6.7-5.14) + (4-2)(8.0-5.14) $$\n$$ S_{xy} = (-2)(-2.94) + (-1)(-1.64) + (0)(0.16) + (1)(1.56) + (2)(2.86) = 5.88 + 1.64 + 0 + 1.56 + 5.72 = 14.8 $$\nNow we can compute the coefficients:\n$$ \\hat{\\beta}_1 = \\frac{14.8}{10} = 1.48 $$\n$$ \\hat{\\beta}_0 = 5.14 - (1.48)(2) = 5.14 - 2.96 = 2.18 $$\nThe fitted regression equation is $\\hat{y}_i = 2.18 + 1.48 x_i$.\n\n**3. Predicted Values and Residuals**\nUsing the fitted equation, we find the predicted values ($\\hat{y}_i$) and the residuals ($e_i = y_i - \\hat{y}_i$):\n- For $x_1=0$: $\\hat{y}_1 = 2.18 + 1.48(0) = 2.18$. Residual $e_1 = 2.2 - 2.18 = 0.02$.\n- For $x_2=1$: $\\hat{y}_2 = 2.18 + 1.48(1) = 3.66$. Residual $e_2 = 3.5 - 3.66 = -0.16$.\n- For $x_3=2$: $\\hat{y}_3 = 2.18 + 1.48(2) = 5.14$. Residual $e_3 = 5.3 - 5.14 = 0.16$.\n- For $x_4=3$: $\\hat{y}_4 = 2.18 + 1.48(3) = 6.62$. Residual $e_4 = 6.7 - 6.62 = 0.08$.\n- For $x_5=4$: $\\hat{y}_5 = 2.18 + 1.48(4) = 8.10$. Residual $e_5 = 8.0 - 8.10 = -0.10$.\n\n**4. Regression Sum of Squares (SSR)**\nSSR measures the variability explained by the regression model. It is the sum of squared deviations of the predicted values from the mean of the observed values.\n$$ \\text{SSR} = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 $$\nUsing the calculated values:\n$$ \\text{SSR} = (2.18 - 5.14)^2 + (3.66 - 5.14)^2 + (5.14 - 5.14)^2 + (6.62 - 5.14)^2 + (8.10 - 5.14)^2 $$\n$$ \\text{SSR} = (-2.96)^2 + (-1.48)^2 + 0^2 + (1.48)^2 + (2.96)^2 $$\n$$ \\text{SSR} = 8.7616 + 2.1904 + 0 + 2.1904 + 8.7616 = 21.904 $$\n\n**5. Error Sum of Squares (SSE)**\nSSE measures the variability not explained by the model (the residual variability). It is the sum of squared residuals.\n$$ \\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n e_i^2 $$\nUsing the calculated residuals:\n$$ \\text{SSE} = (0.02)^2 + (-0.16)^2 + (0.16)^2 + (0.08)^2 + (-0.10)^2 $$\n$$ \\text{SSE} = 0.0004 + 0.0256 + 0.0256 + 0.0064 + 0.0100 = 0.068 $$\n\n**6. Verification of the Decomposition**\nThe fundamental identity of ANOVA for regression is $\\text{SST} = \\text{SSR} + \\text{SSE}$. We verify this numerically with our computed values:\n$$ \\text{SSR} + \\text{SSE} = 21.904 + 0.068 = 21.972 $$\nThis value is identical to our computed SST, so the identity $\\text{SST} = \\text{SSR} + \\text{SSE}$ is confirmed.\n\n**7. Final Answer Formulation**\nThe problem requires the values of SST, SSR, and SSE rounded to four significant figures.\n- $\\text{SST} = 21.972 \\approx 21.97$\n- $\\text{SSR} = 21.904 \\approx 21.90$\n- $\\text{SSE} = 0.068 = 0.06800$ (to four significant figures)\n\nThe final answer is presented as a row vector $(\\text{SST}, \\text{SSR}, \\text{SSE})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n21.97 & 21.90 & 0.06800\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In simple linear regression, there are two common ways to test the significance of the relationship between the predictor and the response: the t-test for the slope coefficient ($\\beta_1$) and the F-test from the ANOVA table. This exercise demonstrates the intimate connection between them by asking you to compute both statistics from the same dataset . Discovering the mathematical identity $F = t^2$ provides a crucial insight, revealing the ANOVA F-test as a more general framework that encompasses the familiar t-test.",
            "id": "1895391",
            "problem": "An undergraduate student in chemical engineering is investigating the relationship between the concentration of a novel catalyst and the rate of a chemical reaction. The student performs the reaction five times, each with a different catalyst concentration, and measures the resulting reaction rate. The collected data are as follows:\n\n-   Catalyst Concentration, $x$ (in mol/L): `{1.0, 2.0, 3.0, 4.0, 5.0}`\n-   Reaction Rate, $y$ (in mol/L/s): `{2.5, 4.0, 4.8, 6.0, 7.5}`\n\nThe student proposes a simple linear regression model of the form $Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, where the errors $\\epsilon_i$ are assumed to be independent and normally distributed with mean 0 and constant variance $\\sigma^2$.\n\nYour task is to perform two key statistical tests on this model.\nFirst, calculate the t-statistic for testing the null hypothesis $H_0: \\beta_1 = 0$ against the two-sided alternative $H_a: \\beta_1 \\neq 0$.\nSecond, construct an Analysis of Variance (ANOVA) table for the regression and calculate the F-statistic.\n\nProvide the values of the t-statistic and the F-statistic. Report your answers, rounded to four significant figures, as a row matrix in the format `[t-statistic, F-statistic]`.",
            "solution": "We adopt the simple linear regression model $Y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i}$ with independent, normally distributed errors of constant variance. For simple linear regression with intercept, the least-squares slope estimator and its standard error are\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}},\\quad S_{xx}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2},\\quad S_{xy}=\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y}),\n$$\nand\n$$\n\\operatorname{SE}(\\hat{\\beta}_{1})=\\sqrt{\\frac{\\operatorname{MSE}}{S_{xx}}},\\quad \\operatorname{MSE}=\\frac{\\operatorname{SSE}}{n-2},\\quad \\operatorname{SSE}=S_{yy}-\\frac{S_{xy}^{2}}{S_{xx}},\\quad S_{yy}=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}.\n$$\nThe $t$-statistic for testing $H_{0}:\\beta_{1}=0$ against $H_{a}:\\beta_{1}\\neq 0$ is\n$$\nt=\\frac{\\hat{\\beta}_{1}}{\\operatorname{SE}(\\hat{\\beta}_{1})}.\n$$\n\nUsing the data $x=\\{1,2,3,4,5\\}$ and $y=\\{2.5,4.0,4.8,6.0,7.5\\}$ with $n=5$, compute the sample means $\\bar{x}=3$ and $\\bar{y}=\\frac{2.5+4.0+4.8+6.0+7.5}{5}=4.96$. Then\n$$\nS_{xx}=\\sum(x_{i}-\\bar{x})^{2}=10,\\quad S_{xy}=\\sum(x_{i}-\\bar{x})(y_{i}-\\bar{y})=12.\n$$\nHence the slope estimate is\n$$\n\\hat{\\beta}_{1}=\\frac{S_{xy}}{S_{xx}}=\\frac{12}{10}=1.2.\n$$\nNext, compute\n$$\nS_{yy}=\\sum(y_{i}-\\bar{y})^{2}=\\left(\\sum y_{i}^{2}\\right)-n\\bar{y}^{2}=(6.25+16+23.04+36+56.25)-5(4.96)^{2}=137.54-123.008=14.532.\n$$\nTherefore,\n$$\n\\operatorname{SSE}=S_{yy}-\\frac{S_{xy}^{2}}{S_{xx}}=14.532-\\frac{144}{10}=14.532-14.4=0.132,\\quad \\operatorname{MSE}=\\frac{\\operatorname{SSE}}{n-2}=\\frac{0.132}{3}=0.044.\n$$\nThe standard error of the slope is\n$$\n\\operatorname{SE}(\\hat{\\beta}_{1})=\\sqrt{\\frac{\\operatorname{MSE}}{S_{xx}}}=\\sqrt{\\frac{0.044}{10}}=\\sqrt{0.0044}=\\sqrt{\\frac{11}{2500}}=\\frac{\\sqrt{11}}{50}.\n$$\nThus the $t$-statistic is\n$$\nt=\\frac{\\hat{\\beta}_{1}}{\\operatorname{SE}(\\hat{\\beta}_{1})}=\\frac{1.2}{\\sqrt{11}/50}=\\frac{60}{\\sqrt{11}}\\approx 18.09\\quad(\\text{with }3\\text{ degrees of freedom}).\n$$\n\nFor the ANOVA, the regression sum of squares is\n$$\n\\operatorname{SSR}=\\frac{S_{xy}^{2}}{S_{xx}}=\\frac{144}{10}=14.4,\n$$\nwith $1$ degree of freedom, and the error sum of squares is $SSE=0.132$ with $n-2=3$ degrees of freedom. The mean squares are\n$$\n\\operatorname{MSR}=\\frac{\\operatorname{SSR}}{1}=14.4,\\quad \\operatorname{MSE}=\\frac{\\operatorname{SSE}}{3}=0.044.\n$$\nThe $F$-statistic for testing the overall regression is\n$$\nF=\\frac{\\operatorname{MSR}}{\\operatorname{MSE}}=\\frac{14.4}{0.044}=\\frac{3600}{11}\\approx 327.3.\n$$\nIn simple linear regression, $F=t^{2}$ holds, and indeed $\\left(\\frac{60}{\\sqrt{11}}\\right)^{2}=\\frac{3600}{11}$.\n\nRounded to four significant figures, the requested statistics are $t\\approx 18.09$ and $F\\approx 327.3$.",
            "answer": "$$\\boxed{\\begin{pmatrix}18.09 & 327.3\\end{pmatrix}}$$"
        }
    ]
}