{
    "hands_on_practices": [
        {
            "introduction": "多元回归分析的核心是普通最小二乘（OLS）估计。为了真正掌握其原理而不仅仅是软件操作，从第一性原理（即最小化残差平方和）出发推导估计量至关重要。本练习  将引导你运用矩阵代数和微积分知识，完整地推导出 OLS 估计量 $\\hat{\\boldsymbol{\\beta}}$，并验证其残差的一个关键性质——与设计矩阵的正交性。",
            "id": "4915381",
            "problem": "一位生物统计学家在一个包含四名成年参与者的小型试点数据集中，研究一个经对数转换的炎症标志物与两个标准化协变量之间的关联。线性多元回归模型被指定为 $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\varepsilon_{i}$，其中 $y_{i}$ 是对数转换后的标志物，$x_{1i}$ 是中心化的身体质量指数对比，$x_{2i}$ 是中心化的体力活动对比，而 $\\varepsilon_{i}$ 是误差项。普通最小二乘法 (OLS) 估计是通过在设计矩阵为满列秩的条件下，最小化残差平方和 $S(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left(y_{i} - \\beta_{0} - \\beta_{1} x_{1i} - \\beta_{2} x_{2i}\\right)^{2}$ 来定义的。观测到的设计矩阵 $X$ 和结果向量 $\\boldsymbol{y}$ 如下：\n$$\nX \\;=\\; \\begin{pmatrix}\n1  -1  -1 \\\\\n1  -1  1 \\\\\n1  1  -1 \\\\\n1  1  1\n\\end{pmatrix}, \n\\qquad\n\\boldsymbol{y} \\;=\\; \\begin{pmatrix}\n1 \\\\ 3 \\\\ 5 \\\\ 7\n\\end{pmatrix}.\n$$\n仅使用 OLS 作为 $S(\\boldsymbol{\\beta})$ 最小化者的基本定义，从第一性原理推导出估计量 $\\hat{\\boldsymbol{\\beta}}$ 并计算其精确数值。然后，通过显式计算 $X^{\\top} \\hat{\\boldsymbol{\\varepsilon}}$（其中 $\\hat{\\boldsymbol{\\varepsilon}} = \\boldsymbol{y} - X \\hat{\\boldsymbol{\\beta}}$）来验证残差正交条件，并检查每个分量是否为 $0$。以精确值（不四舍五入）表示你最终的系数估计，并将其报告为单个行向量。最终报告的值不需要单位。",
            "solution": "本题要求推导并计算一个多元线性回归模型的普通最小二乘法 (OLS) 估计量，然后验证残差正交条件。推导必须从 OLS 的基本定义开始，即最小化残差平方和 $S(\\boldsymbol{\\beta})$。\n\n模型由 $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\varepsilon_{i}$ 给出。其矩阵形式为 $\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{y}$ 是 $n \\times 1$ 的结果向量，$X$ 是 $n \\times (p+1)$ 的设计矩阵，$\\boldsymbol{\\beta}$ 是 $(p+1) \\times 1$ 的系数向量，$\\boldsymbol{\\varepsilon}$ 是 $n \\times 1$ 的误差向量。这里，$n=4$ 且 $p=2$。\n\n设计矩阵 $X$ 和结果向量 $\\boldsymbol{y}$ 已提供如下：\n$$\nX = \\begin{pmatrix}\n1  -1  -1 \\\\\n1  -1  1 \\\\\n1  1  -1 \\\\\n1  1  1\n\\end{pmatrix},\n\\qquad\n\\boldsymbol{y} = \\begin{pmatrix}\n1 \\\\ 3 \\\\ 5 \\\\ 7\n\\end{pmatrix}\n$$\n系数向量为 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^{\\top}$。\n\n残差平方和 $S(\\boldsymbol{\\beta})$ 定义为：\n$$\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}))^2\n$$\n在矩阵表示法中，这等价于残差向量 $\\boldsymbol{\\varepsilon} = \\boldsymbol{y} - X\\boldsymbol{\\beta}$ 的欧几里得范数的平方：\n$$\nS(\\boldsymbol{\\beta}) = (\\boldsymbol{y} - X\\boldsymbol{\\beta})^{\\top} (\\boldsymbol{y} - X\\boldsymbol{\\beta})\n$$\n展开此表达式得到：\n$$\nS(\\boldsymbol{\\beta}) = (\\boldsymbol{y}^{\\top} - \\boldsymbol{\\beta}^{\\top}X^{\\top}) (\\boldsymbol{y} - X\\boldsymbol{\\beta}) = \\boldsymbol{y}^{\\top}\\boldsymbol{y} - \\boldsymbol{y}^{\\top}X\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y} + \\boldsymbol{\\beta}^{\\top}X^{\\top}X\\boldsymbol{\\beta}\n$$\n由于项 $\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y}$ 是一个标量，它等于其转置，即 $(\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y})^{\\top} = \\boldsymbol{y}^{\\top}X\\boldsymbol{\\beta}$。因此，我们可以合并中间两项：\n$$\nS(\\boldsymbol{\\beta}) = \\boldsymbol{y}^{\\top}\\boldsymbol{y} - 2\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y} + \\boldsymbol{\\beta}^{\\top}X^{\\top}X\\boldsymbol{\\beta}\n$$\n为了找到最小化 $S(\\boldsymbol{\\beta})$ 的系数向量 $\\hat{\\boldsymbol{\\beta}}$，我们对 $S(\\boldsymbol{\\beta})$ 关于 $\\boldsymbol{\\beta}$ 求梯度，并将其设为零向量。使用标准的矩阵微积分求导法则，我们有：\n$$\n\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} (\\boldsymbol{y}^{\\top}\\boldsymbol{y} - 2\\boldsymbol{\\beta}^{\\top}X^{\\top}\\boldsymbol{y} + \\boldsymbol{\\beta}^{\\top}X^{\\top}X\\boldsymbol{\\beta}) = -2X^{\\top}\\boldsymbol{y} + 2X^{\\top}X\\boldsymbol{\\beta}\n$$\n将梯度设为零以求得 OLS 估计量 $\\hat{\\boldsymbol{\\beta}}$：\n$$\n-2X^{\\top}\\boldsymbol{y} + 2X^{\\top}X\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\n$$\n$$\nX^{\\top}X\\hat{\\boldsymbol{\\beta}} = X^{\\top}\\boldsymbol{y}\n$$\n这是一个线性方程组，称为正规方程组。由于题目说明设计矩阵是满列秩的，矩阵 $X^{\\top}X$ 是可逆的。因此，我们可以通过左乘 $X^{\\top}X$ 的逆矩阵来求解 $\\hat{\\boldsymbol{\\beta}}$：\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^{\\top}X)^{-1}X^{\\top}\\boldsymbol{y}\n$$\n这就完成了从第一性原理推导 OLS 估计量的过程。\n\n接下来，我们计算数值。首先，我们计算 $X^{\\top}X$：\n$$\nX^{\\top}X = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix} = \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix} = 4I_{3}\n$$\n计算表明 $X$ 的列是正交的，这将 $X^{\\top}X$ 简化为一个对角矩阵。其逆矩阵很容易计算：\n$$\n(X^{\\top}X)^{-1} = (4I_{3})^{-1} = \\frac{1}{4}I_{3} = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix}\n$$\n现在，我们计算 $X^{\\top}\\boldsymbol{y}$：\n$$\nX^{\\top}\\boldsymbol{y} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1+3+5+7 \\\\ -1-3+5+7 \\\\ -1+3-5+7 \\end{pmatrix} = \\begin{pmatrix} 16 \\\\ 8 \\\\ 4 \\end{pmatrix}\n$$\n最后，我们计算 $\\hat{\\boldsymbol{\\beta}}$：\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^{\\top}X)^{-1}X^{\\top}\\boldsymbol{y} = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 16 \\\\ 8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{16}{4} \\\\ \\frac{8}{4} \\\\ \\frac{4}{4} \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n所以，估计的系数为 $\\hat{\\beta}_0 = 4$，$\\hat{\\beta}_1 = 2$，和 $\\hat{\\beta}_2 = 1$。\n\n最后一项任务是验证残差正交条件 $X^{\\top}\\hat{\\boldsymbol{\\varepsilon}} = \\mathbf{0}$，其中 $\\hat{\\boldsymbol{\\varepsilon}} = \\boldsymbol{y} - X\\hat{\\boldsymbol{\\beta}}$。首先，我们计算预测值向量 $\\hat{\\boldsymbol{y}} = X\\hat{\\boldsymbol{\\beta}}$：\n$$\n\\hat{\\boldsymbol{y}} = X\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(4) - 1(2) - 1(1) \\\\ 1(4) - 1(2) + 1(1) \\\\ 1(4) + 1(2) - 1(1) \\\\ 1(4) + 1(2) + 1(1) \\end{pmatrix} = \\begin{pmatrix} 4-2-1 \\\\ 4-2+1 \\\\ 4+2-1 \\\\ 4+2+1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix}\n$$\n接下来，我们计算残差向量 $\\hat{\\boldsymbol{\\varepsilon}}$：\n$$\n\\hat{\\boldsymbol{\\varepsilon}} = \\boldsymbol{y} - \\hat{\\boldsymbol{y}} = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\mathbf{0}_{4 \\times 1}\n$$\n残差向量是零向量，这表明模型对数据实现了完美拟合。这是一个由问题中提供的特定数值引起的特殊情况。\n\n现在我们验证正交条件：\n$$\nX^{\\top}\\hat{\\boldsymbol{\\varepsilon}} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(0) + 1(0) + 1(0) + 1(0) \\\\ -1(0) - 1(0) + 1(0) + 1(0) \\\\ -1(0) + 1(0) - 1(0) + 1(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n所得向量 $X^{\\top}\\hat{\\boldsymbol{\\varepsilon}}$ 的每个分量都是 $0$。这明确地验证了残差正交条件，该条件是由推导出 $\\hat{\\boldsymbol{\\beta}}$ 的正规方程组直接得出的结论。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4  2  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在简单的线性模型中，回归系数的解释似乎直截了当。然而，当模型包含交互项（如 $x_j x_k$）时，一个预测变量的效应就不再是恒定的，而是依赖于另一个变量的取值。本练习  将通过推导一个变量的“条件效应”，清晰地揭示这种依赖关系，这是正确解释复杂但更贴近现实的回归模型的关键技能。",
            "id": "4915325",
            "problem": "一个生物统计学团队正在建模一种循环细胞因子的预期对数浓度（记为 $y$），该浓度是每位参与者测量的两种连续暴露量：$x_{j}$（例如，标准化剂量）和 $x_{k}$（例如，标准化体力活动）的函数，且两者之间存在交互作用。他们拟合了一个包含交互项的多元线性回归模型，该模型可能还包括不涉及 $x_{j}$ 或 $x_{k}$ 的其他协变量。对于一个普通参与者，其模型为\n$$\ny \\;=\\; \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\;+\\; \\varepsilon,\n$$\n其中 $\\varepsilon$ 是一个均值为零且独立于预测变量的误差项，$\\mathcal{S}$ 是除 $j$ 和 $k$ 之外的其他协变量的索引。在一般线性模型的假设下，给定预测变量时，$y$ 的条件期望是移除 $\\varepsilon$ 后得到的回归函数。\n\n仅使用 (i) 线性模型下条件期望的定义和 (ii) 在保持其他参数固定的情况下偏导数的定义，推导在固定 $x_{k}$ 时，$x_{j}$ 对预期结果的条件效应，即，计算\n$$\n\\frac{\\partial\\, E\\!\\left[y \\,\\middle|\\, x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}} \\right]}{\\partial x_{j}}\n$$\n并以 $\\beta_{j}$、$\\beta_{k}$、$\\beta_{jk}$、$x_{j}$ 和 $x_{k}$ 的闭式代数表达式形式表示。\n\n你的最终答案必须是单个符号表达式。不得包含单位。不得提供不等式或方程式。不要四舍五入。",
            "solution": "目标是计算 $y$ 的条件期望关于预测变量 $x_{j}$ 的偏导数。推导过程按照问题陈述的要求分两步进行。\n\n首先，我们确定给定所有预测变量时 $y$ 的条件期望。给定的模型是：\n$$\ny \\;=\\; \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\;+\\; \\varepsilon\n$$\n给定所有预测变量的集合（我们记为 $\\mathbf{X} = \\{x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}}\\}$）时，$y$ 的条件期望是：\n$$\nE\\!\\left[y \\,\\middle|\\, \\mathbf{X} \\right] \\;=\\; E\\!\\left[ \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\;+\\; \\varepsilon \\,\\middle|\\, \\mathbf{X} \\right]\n$$\n根据期望算子的线性性质，我们可以写出：\n$$\nE\\!\\left[y \\,\\middle|\\, \\mathbf{X} \\right] \\;=\\; E\\!\\left[ \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\,\\middle|\\, \\mathbf{X} \\right] \\;+\\; E\\!\\left[ \\varepsilon \\,\\middle|\\, \\mathbf{X} \\right]\n$$\n整个回归函数由常数（$\\beta$ 系数）和给定的预测变量值组成，因此在期望运算中被视为常数。也就是说，对于函数 $g(\\mathbf{X})$，$E[g(\\mathbf{X})|\\mathbf{X}] = g(\\mathbf{X})$。因此：\n$$\nE\\!\\left[ \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\,\\middle|\\, \\mathbf{X} \\right] \\;=\\; \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell}\n$$\n此外，问题陈述指出误差项 $\\varepsilon$ 独立于预测变量且均值为零。$\\varepsilon$ 与 $\\mathbf{X}$ 的独立性意味着 $E[\\varepsilon | \\mathbf{X}] = E[\\varepsilon]$。由于 $E[\\varepsilon] = 0$，因此 $E[\\varepsilon | \\mathbf{X}] = 0$。\n\n将这些结果代回，我们得到 $y$ 的条件期望表达式：\n$$\nE\\!\\left[y \\,\\middle|\\, x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}} \\right] \\;=\\; \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell}\n$$\n这样就完成了所需推导的第一部分。\n\n其次，我们计算该条件期望关于 $x_j$ 的偏导数，同时保持所有其他预测变量（$x_k$ 和 $\\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}}$）固定。\n$$\n\\frac{\\partial}{\\partial x_{j}} E\\!\\left[y \\,\\middle|\\, x_{j}, x_{k}, \\{x_{\\ell}\\}_{\\ell \\in \\mathcal{S}} \\right] \\;=\\; \\frac{\\partial}{\\partial x_{j}} \\left( \\beta_{0} \\;+\\; \\beta_{j}\\,x_{j} \\;+\\; \\beta_{k}\\,x_{k} \\;+\\; \\beta_{jk}\\,x_{j}\\,x_{k} \\;+\\; \\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell} \\right)\n$$\n我们应用求导的加法法则，对表达式逐项关于 $x_j$求导：\n$$\n\\frac{\\partial}{\\partial x_{j}} (\\beta_{0}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} (\\beta_{j}\\,x_{j}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} (\\beta_{k}\\,x_{k}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} (\\beta_{jk}\\,x_{j}\\,x_{k}) \\;+\\; \\frac{\\partial}{\\partial x_{j}} \\left(\\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell}\\right)\n$$\n我们计算每一项的导数：\n- 系数 $\\beta_0$ 是一个常数，所以其导数为零：$\\frac{\\partial}{\\partial x_{j}} (\\beta_{0}) = 0$。\n- 对于项 $\\beta_{j}\\,x_{j}$，其关于 $x_j$ 的导数是常数系数 $\\beta_j$：$\\frac{\\partial}{\\partial x_{j}} (\\beta_{j}\\,x_{j}) = \\beta_j$。\n- 对于项 $\\beta_{k}\\,x_{k}$，在对 $x_j$ 求导时，$\\beta_k$ 和 $x_k$ 都被视为常数，所以该项的导数为零：$\\frac{\\partial}{\\partial x_{j}} (\\beta_{k}\\,x_{k}) = 0$。\n- 对于交互项 $\\beta_{jk}\\,x_{j}\\,x_{k}$，系数 $\\beta_{jk}$ 和变量 $x_k$ 被视为常数。因此，其导数为 $\\frac{\\partial}{\\partial x_{j}} ((\\beta_{jk}\\,x_{k}) x_{j}) = \\beta_{jk}\\,x_{k}$。\n- 对于求和项 $\\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell}$，索引集 $\\mathcal{S}$ 与 $\\{j, k\\}$ 不同。因此，对于每个 $\\ell \\in \\mathcal{S}$，在对 $x_j$ 求偏导时 $x_\\ell$ 保持不变。整个和的导数为零：$\\frac{\\partial}{\\partial x_{j}} \\left(\\sum_{\\ell \\in \\mathcal{S}} \\beta_{\\ell}\\,x_{\\ell}\\right) = \\sum_{\\ell \\in \\mathcal{S}} \\frac{\\partial}{\\partial x_{j}} (\\beta_{\\ell}\\,x_{\\ell}) = \\sum_{\\ell \\in \\mathcal{S}} 0 = 0$。\n\n将这些结果相加，得到 $x_{j}$ 的条件效应的最终表达式：\n$$\n\\frac{\\partial E[y | \\mathbf{X}]}{\\partial x_{j}} = 0 + \\beta_{j} + 0 + \\beta_{jk}\\,x_{k} + 0 = \\beta_{j} + \\beta_{jk}\\,x_{k}\n$$\n该表达式表示，在给定 $x_k$ 的特定值和所有其他协变量的情况下，$x_j$ 每增加一个单位，$y$ 的期望值的变化量。它表明，在存在交互项的情况下，一个预测变量的效应不是恒定的，而是线性地依赖于另一个预测变量的水平。",
            "answer": "$$\\boxed{\\beta_{j} + \\beta_{jk}x_{k}}$$"
        },
        {
            "introduction": "在回归分析中，并非所有的数据点都具有相同的重要性；某些数据点对回归结果的潜在影响远大于其他点。杠杆值 (leverage) 是衡量这种潜在影响力的一个关键指标，它完全由数据点在预测变量空间中的位置决定。本练习  提供了一个计算杠杆值并识别“高杠杆点”的实践机会，这是回归诊断中不可或缺的一步，有助于我们评估模型的稳健性。",
            "id": "4915371",
            "problem": "一个生物统计学团队使用多元线性回归研究生物标志物如何响应临床协变量。该模型包含一个截距和两个预测变量：一个二元指示变量（某种状况存在时编码为 $+1$，不存在时编码为 $-1$），以及一个中心化的连续实验室测量值。对于 $n=8$ 名患者，设计矩阵 $\\mathbf{X}$ 如下所示\n$$\n\\mathbf{X} \\;=\\;\n\\begin{pmatrix}\n1  1  16 \\\\\n1  1  -8 \\\\\n1  1  -4 \\\\\n1  1  -4 \\\\\n1  -1  0 \\\\\n1  -1  0 \\\\\n1  -1  0 \\\\\n1  -1  0\n\\end{pmatrix}.\n$$\n参数数量（包括截距）为 $p=3$。计算 $i=1,\\dots,8$ 的杠杆值 $h_{ii}$，并根据经验法则阈值 $2p/n$ 确定哪些是高杠杆观测值。最终答案仅报告高杠杆观测值的数量。无需四舍五入；提供确切的计数。",
            "solution": "第 $i$ 个观测值的杠杆值 $h_{ii}$ 是帽子矩阵 $\\mathbf{H}$ 的第 $i$ 个对角元素。帽子矩阵定义为：\n$$\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\n$$\n其中 $\\mathbf{X}$ 是 $n \\times p$ 的设计矩阵。观测值 $i$ 的杠杆值可以直接计算为：\n$$\nh_{ii} = \\mathbf{x}_i^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_i\n$$\n其中 $\\mathbf{x}_i^T$ 是 $\\mathbf{X}$ 的第 $i$ 行。\n\n首先，我们计算矩阵 $\\mathbf{X}^T\\mathbf{X}$。$\\mathbf{X}$ 的转置是：\n$$\n\\mathbf{X}^T \\;=\\;\n\\begin{pmatrix}\n1  1  1  1  1  1  1  1 \\\\\n1  1  1  1  -1  -1  -1  -1 \\\\\n16  -8  -4  -4  0  0  0  0\n\\end{pmatrix}\n$$\n$\\mathbf{X}$ 的列是正交的。设这些列为 $\\mathbf{c}_0$、$\\mathbf{c}_1$ 和 $\\mathbf{c}_2$。我们计算内积：\n$$\n\\mathbf{c}_0^T\\mathbf{c}_0 = \\sum_{i=1}^{8} 1^2 = 8\n$$\n$$\n\\mathbf{c}_1^T\\mathbf{c}_1 = 4 \\times 1^2 + 4 \\times (-1)^2 = 8\n$$\n$$\n\\mathbf{c}_2^T\\mathbf{c}_2 = 16^2 + (-8)^2 + (-4)^2 + (-4)^2 + 4 \\times 0^2 = 256 + 64 + 16 + 16 = 352\n$$\n非对角元素为：\n$$\n\\mathbf{c}_0^T\\mathbf{c}_1 = 1(1) + 1(1) + 1(1) + 1(1) + 1(-1) + 1(-1) + 1(-1) + 1(-1) = 4 - 4 = 0\n$$\n$$\n\\mathbf{c}_0^T\\mathbf{c}_2 = 1(16) + 1(-8) + 1(-4) + 1(-4) + 4 \\times 1(0) = 16 - 8 - 4 - 4 = 0\n$$\n$$\n\\mathbf{c}_1^T\\mathbf{c}_2 = 1(16) + 1(-8) + 1(-4) + 1(-4) + (-1)(0) + (-1)(0) + (-1)(0) + (-1)(0) = 16 - 8 - 4 - 4 = 0\n$$\n由于这些列是正交的，$\\mathbf{X}^T\\mathbf{X}$ 是一个对角矩阵：\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 8  0  0 \\\\ 0  8  0 \\\\ 0  0  352 \\end{pmatrix}\n$$\n其逆矩阵 $(\\mathbf{X}^T\\mathbf{X})^{-1}$ 也是一个对角矩阵，对角元素为原对角元素的倒数：\n$$\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\begin{pmatrix} \\frac{1}{8}  0  0 \\\\ 0  \\frac{1}{8}  0 \\\\ 0  0  \\frac{1}{352} \\end{pmatrix}\n$$\n现在我们可以为每个观测值 $i$ 计算杠杆值 $h_{ii} = \\mathbf{x}_i^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_i$。对于行向量 $\\mathbf{x}_i^T = (x_{i0}, x_{i1}, x_{i2})$，杠杆值为：\n$$\nh_{ii} = \\frac{x_{i0}^2}{8} + \\frac{x_{i1}^2}{8} + \\frac{x_{i2}^2}{352}\n$$\n由于所有观测值的 $x_{i0}=1$，这可以简化为：\n$$\nh_{ii} = \\frac{1}{8} + \\frac{x_{i1}^2}{8} + \\frac{x_{i2}^2}{352}\n$$\n我们为 $\\mathbf{X}$ 的不同行计算 $h_{ii}$：\n-   对于 $i=1$，$\\mathbf{x}_1^T = (1, 1, 16)$：\n    $h_{11} = \\frac{1}{8} + \\frac{1^2}{8} + \\frac{16^2}{352} = \\frac{2}{8} + \\frac{256}{352} = \\frac{1}{4} + \\frac{8 \\times 32}{11 \\times 32} = \\frac{1}{4} + \\frac{8}{11} = \\frac{11 + 32}{44} = \\frac{43}{44}$。\n-   对于 $i=2$，$\\mathbf{x}_2^T = (1, 1, -8)$：\n    $h_{22} = \\frac{1}{8} + \\frac{1^2}{8} + \\frac{(-8)^2}{352} = \\frac{1}{4} + \\frac{64}{352} = \\frac{1}{4} + \\frac{2 \\times 32}{11 \\times 32} = \\frac{1}{4} + \\frac{2}{11} = \\frac{11 + 8}{44} = \\frac{19}{44}$。\n-   对于 $i=3, 4$，$\\mathbf{x}_{3,4}^T = (1, 1, -4)$：\n    $h_{33} = h_{44} = \\frac{1}{8} + \\frac{1^2}{8} + \\frac{(-4)^2}{352} = \\frac{1}{4} + \\frac{16}{352} = \\frac{1}{4} + \\frac{16}{22 \\times 16} = \\frac{1}{4} + \\frac{1}{22} = \\frac{11 + 2}{44} = \\frac{13}{44}$。\n-   对于 $i=5, 6, 7, 8$，$\\mathbf{x}_{5-8}^T = (1, -1, 0)$：\n    $h_{55} = \\dots = h_{88} = \\frac{1}{8} + \\frac{(-1)^2}{8} + \\frac{0^2}{352} = \\frac{1}{8} + \\frac{1}{8} = \\frac{2}{8} = \\frac{1}{4} = \\frac{11}{44}$。\n\n杠杆值的集合是 $\\{\\frac{43}{44}, \\frac{19}{44}, \\frac{13}{44}, \\frac{13}{44}, \\frac{11}{44}, \\frac{11}{44}, \\frac{11}{44}, \\frac{11}{44}\\}$。\n作为检验，杠杆值之和必须等于 $p$：\n$$\n\\sum_{i=1}^{8} h_{ii} = \\frac{43}{44} + \\frac{19}{44} + 2 \\left( \\frac{13}{44} \\right) + 4 \\left( \\frac{11}{44} \\right) = \\frac{43 + 19 + 26 + 44}{44} = \\frac{132}{44} = 3\n$$\n这与 $p=3$ 相符，表明计算是正确的。\n\n高杠杆点的经验法则阈值是 $2p/n$。当 $p=3$ 且 $n=8$ 时，阈值为：\n$$\n\\frac{2p}{n} = \\frac{2 \\times 3}{8} = \\frac{6}{8} = \\frac{3}{4}\n$$\n为了将杠杆值与此阈值进行比较，我们可以将阈值表示为分母为 $44$ 的形式：$\\frac{3}{4} = \\frac{33}{44}$。\n我们现在确定哪些观测值的 $h_{ii} > \\frac{33}{44}$：\n-   $h_{11} = \\frac{43}{44}$。由于 $43 > 33$，因此 $h_{11} > \\frac{33}{44}$。这是一个高杠杆观测值。\n-   $h_{22} = \\frac{19}{44}$。由于 $19  33$，这不是一个高杠杆观测值。\n-   $h_{33} = h_{44} = \\frac{13}{44}$。由于 $13  33$，这些不是高杠杆观测值。\n-   $h_{55}$ 至 $h_{88} = \\frac{11}{44}$。由于 $11  33$，这些不是高杠杆观测值。\n\n只有一个观测值 $i=1$ 满足高杠杆点的标准。高杠杆观测值的数量是 $1$。",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}