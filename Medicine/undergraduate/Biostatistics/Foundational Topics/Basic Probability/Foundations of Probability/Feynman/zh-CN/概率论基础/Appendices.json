{
    "hands_on_practices": [
        {
            "introduction": "在生物统计学中，一项核心技能是根据新证据更新我们的信念。本练习将通过一个涉及多种疾病和诊断测试的真实临床场景，指导您运用贝叶斯定理和全概率定律，在给定测试结果的情况下计算患有某种特定疾病的概率。这是医学决策中的一项基本任务，它展示了如何将先验知识与观察数据相结合，以得出更精确的结论。",
            "id": "4913399",
            "problem": "一项针对因急性下呼吸道感染入院的成年患者的医院监测研究确定，以该综合征入院为条件，存在以下三种病因中的一种：甲型流感 ($A$)、呼吸道合胞病毒 ($B$) 或细菌性肺炎 ($C$)。根据该入院人群的历史患病率，先验概率为 $P(A)=0.24$、$P(B)=0.31$ 和 $P(C)=0.45$。\n\n每位入院患者都接受两种诊断评估：\n1. 一种分子呼吸道病原体检测（基于聚合酶链式反应 (PCR)），其对病毒靶标返回阳性 $(M^{+})$ 或阴性 $(M^{-})$ 结果。\n2. 一种血清降钙素原测试，在临床验证的阈值下进行二分类，其返回高 $(P^{+})$ 或低 $(P^{-})$ 结果。\n\n从独立的临床验证数据中，已知以下条件概率：\n- 给定 $A$：$P(M^{+}\\mid A)=0.94$ 和 $P(P^{+}\\mid A)=0.28$。\n- 给定 $B$：$P(M^{+}\\mid B)=0.88$ 和 $P(P^{+}\\mid B)=0.35$。\n- 给定 $C$：$P(M^{+}\\mid C)=0.17$ 和 $P(P^{+}\\mid C)=0.81$。\n\n可以合理地假设，在真实病因的条件下，两种测试结果是独立的。\n\n仅使用概率和条件概率的核心定义，以及上述独立性假设，完成以下任务：\n- 推导由疾病 $\\{A,B,C\\}$ 和测试结果对 $\\{(M^{+},P^{+}), (M^{+},P^{-}), (M^{-},P^{+}), (M^{-},P^{-})\\}$ 组成的样本空间上的完整联合概率表。\n- 然后，计算在两次测试均为阳性的情况下，入院患者患有细菌性肺炎 $(C)$ 的后验概率，即 $P(C\\mid M^{+},P^{+})$。\n\n将最终数值答案表示为四舍五入到四位有效数字的小数。不要使用百分号。",
            "solution": "此问题被判定为有效。\n**步骤1：提取已知条件：** 问题提供了一组互斥且穷尽的病因 $\\{A, B, C\\}$，其先验概率为 $P(A)=0.24$，$P(B)=0.31$ 和 $P(C)=0.45$。它指定了两种具有二元结果 $(M^{+}, M^{-})$ 和 $(P^{+}, P^{-})$ 的诊断测试。它提供了每种病因的阳性测试结果的条件概率：$P(M^{+}\\mid A)=0.94$，$P(P^{+}\\mid A)=0.28$；$P(M^{+}\\mid B)=0.88$，$P(P^{+}\\mid B)=0.35$；$P(M^{+}\\mid C)=0.17$，$P(P^{+}\\mid C)=0.81$。陈述了一个关键假设，即在给定病因的情况下，测试是条件独立的。任务是推导联合概率表并计算一个特定的后验概率 $P(C\\mid M^{+},P^{+})$，四舍五入到四位有效数字。\n**步骤2：使用提取的已知条件进行验证：** 该问题具有科学依据，描述了临床诊断和流行病学中的一个标准情景。问题是适定的，因为提供了所有必要的概率，先验概率之和为1，任务明确，可以导出一个唯一的解。语言客观而精确。该问题是基础概率论，特别是全概率定律和贝叶斯定理的直接应用，并非无意义或不适定的。满足有效问题的所有条件。\n\n解题过程如下。设 $E$ 为病因的随机变量，取值于 $\\{A, B, C\\}$。设 $M$ 和 $P$ 分别为分子检测和降钙素原测试结果的随机变量。\n\n给定的先验概率是：\n$$P(A) = 0.24$$\n$$P(B) = 0.31$$\n$$P(C) = 0.45$$\n这些是互斥且穷尽的，因为 $0.24 + 0.31 + 0.45 = 1$。\n\n给定的阳性测试结果的条件概率是：\n- 对于病因 $A$：$P(M^{+} \\mid A) = 0.94$ 和 $P(P^{+} \\mid A) = 0.28$。\n- 对于病因 $B$：$P(M^{+} \\mid B) = 0.88$ 和 $P(P^{+} \\mid B) = 0.35$。\n- 对于病因 $C$：$P(M^{+} \\mid C) = 0.17$ 和 $P(P^{+} \\mid C) = 0.81$。\n\n问题陈述，在真实病因的条件下，两种测试结果是独立的。这可以对任何病因 $E \\in \\{A, B, C\\}$ 和测试结果 $M_i \\in \\{M^{+}, M^{-}\\}, P_j \\in \\{P^{+}, P^{-}\\}$ 表示为：\n$$P(M_i, P_j \\mid E) = P(M_i \\mid E) P(P_j \\mid E)$$\n\n第一个任务是推导 $P(E, M, P)$ 的完整联合概率表。联合概率使用链式法则求得：$P(E, M, P) = P(M, P \\mid E)P(E)$。应用条件独立性假设，这变为：\n$$P(E, M, P) = P(M \\mid E) P(P \\mid E) P(E)$$\n\n首先，我们使用补集法则 $P(\\text{event}^c) = 1 - P(\\text{event})$ 来计算阴性测试结果的条件概率：\n- $P(M^{-} \\mid A) = 1 - P(M^{+} \\mid A) = 1 - 0.94 = 0.06$\n- $P(P^{-} \\mid A) = 1 - P(P^{+} \\mid A) = 1 - 0.28 = 0.72$\n- $P(M^{-} \\mid B) = 1 - P(M^{+} \\mid B) = 1 - 0.88 = 0.12$\n- $P(P^{-} \\mid B) = 1 - P(P^{+} \\mid B) = 1 - 0.35 = 0.65$\n- $P(M^{-} \\mid C) = 1 - P(M^{+} \\mid C) = 1 - 0.17 = 0.83$\n- $P(P^{-} \\mid C) = 1 - P(P^{+} \\mid C) = 1 - 0.81 = 0.19$\n\n现在我们可以计算所有 $3 \\times 2 \\times 2 = 12$ 个联合概率：\n\n对于病因 $A$（$P(A) = 0.24$）：\n- $P(A, M^{+}, P^{+}) = P(M^{+} \\mid A) P(P^{+} \\mid A) P(A) = 0.94 \\times 0.28 \\times 0.24 = 0.063168$\n- $P(A, M^{+}, P^{-}) = P(M^{+} \\mid A) P(P^{-} \\mid A) P(A) = 0.94 \\times 0.72 \\times 0.24 = 0.162432$\n- $P(A, M^{-}, P^{+}) = P(M^{-} \\mid A) P(P^{+} \\mid A) P(A) = 0.06 \\times 0.28 \\times 0.24 = 0.004032$\n- $P(A, M^{-}, P^{-}) = P(M^{-} \\mid A) P(P^{-} \\mid A) P(A) = 0.06 \\times 0.72 \\times 0.24 = 0.010368$\n\n对于病因 $B$（$P(B) = 0.31$）：\n- $P(B, M^{+}, P^{+}) = P(M^{+} \\mid B) P(P^{+} \\mid B) P(B) = 0.88 \\times 0.35 \\times 0.31 = 0.095480$\n- $P(B, M^{+}, P^{-}) = P(M^{+} \\mid B) P(P^{-} \\mid B) P(B) = 0.88 \\times 0.65 \\times 0.31 = 0.177320$\n- $P(B, M^{-}, P^{+}) = P(M^{-} \\mid B) P(P^{+} \\mid B) P(B) = 0.12 \\times 0.35 \\times 0.31 = 0.013020$\n- $P(B, M^{-}, P^{-}) = P(M^{-} \\mid B) P(P^{-} \\mid B) P(B) = 0.12 \\times 0.65 \\times 0.31 = 0.024180$\n\n对于病因 $C$（$P(C) = 0.45$）：\n- $P(C, M^{+}, P^{+}) = P(M^{+} \\mid C) P(P^{+} \\mid C) P(C) = 0.17 \\times 0.81 \\times 0.45 = 0.061965$\n- $P(C, M^{+}, P^{-}) = P(M^{+} \\mid C) P(P^{-} \\mid C) P(C) = 0.17 \\times 0.19 \\times 0.45 = 0.014535$\n- $P(C, M^{-}, P^{+}) = P(M^{-} \\mid C) P(P^{+} \\mid C) P(C) = 0.83 \\times 0.81 \\times 0.45 = 0.302535$\n- $P(C, M^{-}, P^{-}) = P(M^{-} \\mid C) P(P^{-} \\mid C) P(C) = 0.83 \\times 0.19 \\times 0.45 = 0.070965$\n\n完整的联合概率表如下：\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{病因 } E  \\text{结果 } (M,P)  \\text{联合概率 } P(E,M,P) \\\\\n\\hline\n\\hline\nA  (M^{+}, P^{+})  0.063168 \\\\\nA  (M^{+}, P^{-})  0.162432 \\\\\nA  (M^{-}, P^{+})  0.004032 \\\\\nA  (M^{-}, P^{-})  0.010368 \\\\\n\\hline\nB  (M^{+}, P^{+})  0.095480 \\\\\nB  (M^{+}, P^{-})  0.177320 \\\\\nB  (M^{-}, P^{+})  0.013020 \\\\\nB  (M^{-}, P^{-})  0.024180 \\\\\n\\hline\nC  (M^{+}, P^{+})  0.061965 \\\\\nC  (M^{+}, P^{-})  0.014535 \\\\\nC  (M^{-}, P^{+})  0.302535 \\\\\nC  (M^{-}, P^{-})  0.070965 \\\\\n\\hline\n\\end{array}\n$$\n\n第二个任务是计算后验概率 $P(C \\mid M^{+}, P^{+})$。使用条件概率的定义，这也是贝叶斯定理的基础：\n$$P(C \\mid M^{+}, P^{+}) = \\frac{P(C, M^{+}, P^{+})}{P(M^{+}, P^{+})}$$\n分子是我们已经计算过的联合概率：\n$$P(C, M^{+}, P^{+}) = 0.061965$$\n分母 $P(M^{+}, P^{+})$ 是观察到结果 $(M^{+}, P^{+})$ 的边际概率。它是根据全概率定律，将此结果在所有可能病因上的联合概率相加得到的：\n$$P(M^{+}, P^{+}) = P(A, M^{+}, P^{+}) + P(B, M^{+}, P^{+}) + P(C, M^{+}, P^{+})$$\n使用我们计算出的值：\n$$P(M^{+}, P^{+}) = 0.063168 + 0.095480 + 0.061965 = 0.220613$$\n现在我们可以计算后验概率：\n$$P(C \\mid M^{+}, P^{+}) = \\frac{0.061965}{0.220613} \\approx 0.2808775$$\n问题要求答案四舍五入到四位有效数字。第五位有效数字是7，所以我们将第四位向上取整。\n$$P(C \\mid M^{+}, P^{+}) \\approx 0.2809$$\n这就是在分子检测阳性和降钙素原水平高的情况下，患者患有细菌性肺炎的后验概率。",
            "answer": "$$\\boxed{0.2809}$$"
        },
        {
            "introduction": "独立性是概率论中的一个基石概念，但其内涵比初看起来更为精妙。本练习旨在探讨成对独立与相互独立这两个概念之间的关键区别，前者指任意两个事件相互独立，而后者则是一个更强的条件。通过构建一个具体的反例，您将亲身体会到，为何从成对独立直接推断相互独立是一个常见的逻辑陷阱，并加深对独立性本质的理解。",
            "id": "4913391",
            "problem": "一项生物样本库研究考虑了两个不连锁的双等位基因座，记为 $L_A$ 和 $L_B$，在一个大的随机交配群体中，每个基因座的次要等位基因频率均为 $0.5$。对于一个随机抽样的个体，定义指示变量 $X_A=\\mathbf{1}\\{\\text{在}L_A\\text{处存在次要等位基因}\\}$ 和 $X_B=\\mathbf{1}\\{\\text{在}L_B\\text{处存在次要等位基因}\\}$。假设标准模型成立，即不连锁的基因座之间相互独立，因此 $X_A$ 和 $X_B$ 是独立的，且均服从 $\\text{Bernoulli}(0.5)$ 分布。定义第三个指示变量 $X_C=\\mathbf{1}\\{\\text{在}L_A\\text{或}L_B\\text{中恰好一个位置存在次要等位基因}\\}$，即 $X_C=\\mathbf{1}\\{X_A\\neq X_B\\}$。使用概率论的基本定义（柯尔莫哥洛夫公理）、事件独立性和指示变量，判断 $X_A$、$X_B$ 和 $X_C$ 是否两两独立以及是否相互独立。选择唯一能够正确描述其独立性结构并用有效的数值结果来支持该描述的最佳选项。\n\nA. $X_A$、$X_B$ 和 $X_C$ 是相互独立的；例如，$P(X_A=1,X_B=1,X_C=1)=\\left(\\tfrac{1}{2}\\right)^3=\\tfrac{1}{8}$。\n\nB. $X_A$、$X_B$ 和 $X_C$ 是两两独立但不是相互独立的；具体来说，$P(X_A=1,X_B=1,X_C=1)=0$ 而 $P(X_A=1)P(X_B=1)P(X_C=1)=\\tfrac{1}{8}$。\n\nC. $X_A$ 和 $X_C$ 不独立，因为 $P(X_C=1\\mid X_A=1)=1$。\n\nD. $X_A$、$X_B$ 和 $X_C$ 之间的两两独立性仅当 $P(X_A=1)=P(X_B=1)=\\tfrac{1}{4}$ 时成立。",
            "solution": "在进行求解之前，对问题陈述的有效性进行严格评估。\n\n**步骤1：提取已知条件**\n- 两个不连锁的双等位基因座，$L_A$ 和 $L_B$。\n- 在一个大的随机交配群体中，每个基因座的次要等位基因频率为 $0.5$。\n- $X_A = \\mathbf{1}\\{\\text{在}L_A\\text{处存在次要等位基因}\\}$。\n- $X_B = \\mathbf{1}\\{\\text{在}L_B\\text{处存在次要等位基因}\\}$。\n- 明确假设1：$X_A$ 和 $X_B$ 独立。\n- 明确假设2：$X_A \\sim \\text{Bernoulli}(0.5)$ 且 $X_B \\sim \\text{Bernoulli}(0.5)$。\n- $X_C = \\mathbf{1}\\{\\text{在}L_A\\text{或}L_B\\text{中恰好一个位置存在次要等位基因}\\}$，其定义为 $X_C = \\mathbf{1}\\{X_A \\neq X_B\\}$。\n\n**步骤2：使用提取的已知条件进行验证**\n问题提出了一个生物统计学情景，然后给出了明确的数学假设。生物学背景和数学假设之间存在潜在的不一致。在一个处于哈代-温伯格平衡的二倍体生物中，若次要等位基因频率为 $p=0.5$，则个体拥有至少一个次要等位基因的概率为 $P(\\text{基因型为Aa或aa}) = 2p(1-p) + p^2 = 2(0.5)(0.5) + (0.5)^2 = 0.5 + 0.25 = 0.75$。这意味着 $P(X_A=1) = 0.75$。然而，问题明确指出要“假设...$X_A$ 和 $X_B$...每个都服从 $\\text{Bernoulli}(0.5)$ 分布”。在抽象问题求解的背景下，明确的假设优先于从情景背景中得出的推论。因此，该问题是关于三个随机变量 $X_A$、$X_B$ 和 $X_C$ 的一个定义明确的概率论问题，它们的性质由这些假设定义。该问题在概率论领域内有科学依据，是适定的、客观的，并且不包含其他逻辑缺陷。\n\n**步骤3：结论和行动**\n该问题作为一个数学练习是有效的。将基于给定的明确假设进行求解。\n\n**推导**\n\n我们已知 $X_A$ 和 $X_B$ 是独立的随机变量，且 $P(X_A=1) = P(X_A=0) = 0.5$ 以及 $P(X_B=1) = P(X_B=0) = 0.5$。第三个变量是 $X_C = \\mathbf{1}\\{X_A \\neq X_B\\}$。这可以写成 $X_C = |X_A - X_B|$ 或 $X_C = (X_A + X_B) \\pmod 2$。\n\n首先，我们确定 $X_C$ 的概率分布。事件 $\\{X_C=1\\}$ 发生当且仅当 $\\{X_A=1, X_B=0\\}$ 或 $\\{X_A=0, X_B=1\\}$ 发生。这些是互斥事件。\n$$P(X_C=1) = P(\\{X_A=1, X_B=0\\} \\cup \\{X_A=0, X_B=1\\})$$\n$$P(X_C=1) = P(X_A=1, X_B=0) + P(X_A=0, X_B=1)$$\n根据 $X_A$ 和 $X_B$ 的独立性：\n$$P(X_C=1) = P(X_A=1)P(X_B=0) + P(X_A=0)P(X_B=1)$$\n$$P(X_C=1) = (0.5)(0.5) + (0.5)(0.5) = 0.25 + 0.25 = 0.5$$\n因此，$X_C$ 也是一个参数为 $0.5$ 的伯努利随机变量，所以 $P(X_C=0) = 1 - P(X_C=1) = 0.5$。\n\n接下来，我们检查两两独立性。\n1.  **$X_A$ 和 $X_B$ 的独立性**：这在问题陈述中已给出。\n\n2.  **$X_A$ 和 $X_C$ 的独立性**：我们必须检查是否对于所有 $i, j \\in \\{0, 1\\}$，都有 $P(X_A=i, X_C=j) = P(X_A=i)P(X_C=j)$。我们来检查 $i=1, j=1$ 的情况。\n    $$P(X_A=1, X_C=1) = P(X_A=1 \\text{ and } X_A \\neq X_B)$$\n    这等价于事件 $\\{X_A=1, X_B=0\\}$。\n    $$P(X_A=1, X_B=0) = P(X_A=1)P(X_B=0) = (0.5)(0.5) = 0.25$$\n    边际概率的乘积是：\n    $$P(X_A=1)P(X_C=1) = (0.5)(0.5) = 0.25$$\n    由于 $P(X_A=1, X_C=1) = P(X_A=1)P(X_C=1)$，这个条件成立。为了完全严谨，必须检查所有四种组合，但由于对称性，它们都成立。例如，$P(X_A=0, X_C=1) = P(X_A=0, X_B=1) = (0.5)(0.5) = 0.25$，并且 $P(X_A=0)P(X_C=1) = (0.5)(0.5) = 0.25$。因此，$X_A$ 和 $X_C$ 是独立的。\n\n3.  **$X_B$ 和 $X_C$ 的独立性**：$X_C$ 的定义相对于 $X_A$ 和 $X_B$ 是对称的。由于 $X_A$ 和 $X_B$ 具有相同的分布，用于证明 $X_A$ 和 $X_C$ 独立性的论证同样适用于 $X_B$ 和 $X_C$。因此，$X_B$ 和 $X_C$ 是独立的。\n\n关于两两独立性的结论：$X_A$、$X_B$ 和 $X_C$ 是两两独立的。\n\n最后，我们检查相互独立性。要满足相互独立性，必须对所有 $i, j, k \\in \\{0, 1\\}$，都有 $P(X_A=i, X_B=j, X_C=k) = P(X_A=i)P(X_B=j)P(X_C=k)$。\n我们来考察事件 $\\{X_A=1, X_B=1, X_C=1\\}$。\n这个联合事件的概率是：\n$$P(X_A=1, X_B=1, X_C=1) = P(X_A=1, X_B=1 \\text{ and } \\mathbf{1}\\{X_A \\neq X_B\\}=1)$$\n如果 $X_A=1$ 且 $X_B=1$，那么 $X_A=X_B$，这意味着 $\\mathbf{1}\\{X_A \\neq X_B\\}=0$。因此，$X_C$ 必须是 $0$。事件 $\\{X_A=1, X_B=1, X_C=1\\}$ 是一个不可能事件，所以其概率为 $0$。\n$$P(X_A=1, X_B=1, X_C=1) = 0$$\n现在，我们计算边际概率的乘积：\n$$P(X_A=1)P(X_B=1)P(X_C=1) = (0.5)(0.5)(0.5) = \\left(\\frac{1}{2}\\right)^3 = \\frac{1}{8}$$\n由于 $0 \\neq \\frac{1}{8}$，相互独立性的条件不满足。\n$$P(X_A=1, X_B=1, X_C=1) \\neq P(X_A=1)P(X_B=1)P(X_C=1)$$\n关于相互独立性的结论：$X_A$、$X_B$ 和 $X_C$ 不是相互独立的。这是因为知道任意两个变量的值就可以确定第三个变量的值。例如，如果 $X_A=i$ 且 $X_B=j$，那么 $X_C$ 就固定为 $|i-j|$。\n\n**逐项分析选项**\n\nA. $X_A$、$X_B$ 和 $X_C$ 是相互独立的；例如，$P(X_A=1,X_B=1,X_C=1)=\\left(\\tfrac{1}{2}\\right)^3=\\tfrac{1}{8}$。\n这是**不正确**的。这些变量不是相互独立的。具体的数值声明也是错误的；根据推导，$P(X_A=1,X_B=1,X_C=1)=0$，而不是 $\\frac{1}{8}$。\n\nB. $X_A$、$X_B$ 和 $X_C$ 是两两独立但不是相互独立的；具体来说，$P(X_A=1,X_B=1,X_C=1)=0$ 而 $P(X_A=1)P(X_B=1)P(X_C=1)=\\tfrac{1}{8}$。\n这是**正确**的。对独立性结构的描述与我们推导出的结果完全一致。所提供的数值例子是证明缺乏相互独立性的正确理由。\n\nC. $X_A$ 和 $X_C$ 不独立，因为 $P(X_C=1\\mid X_A=1)=1$。\n这是**不正确**的。如上所示，$X_A$ 和 $X_C$ 是独立的。其支持理由在事实上也是错误的。我们可以计算条件概率：\n$$P(X_C=1 \\mid X_A=1) = \\frac{P(X_C=1, X_A=1)}{P(X_A=1)} = \\frac{0.25}{0.5} = 0.5$$\n由于 $P(X_C=1 \\mid X_A=1) = 0.5 = P(X_C=1)$，这证实了它们的独立性，直接与该选项的说法相矛盾。\n\nD. $X_A$、$X_B$ 和 $X_C$ 之间的两两独立性仅当 $P(X_A=1)=P(X_B=1)=\\tfrac{1}{4}$ 时成立。\n这是**不正确**的。我们已经证明了在给定情况 $P(X_A=1)=P(X_B=1)=\\frac{1}{2}$ 下，两两独立性是成立的。一般来说，对于独立同分布的 $X_A, X_B \\sim \\text{Bernoulli}(p)$，与 $X_C = X_A \\oplus X_B$ 的两两独立性当且仅当 $p=\\frac{1}{2}$ 时成立（对于非平凡情况 $p \\in (0,1)$）。该选项中陈述的条件 $p=\\frac{1}{4}$ 是错误的。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "理论与实践的结合是生物统计学研究的特色，而计算机模拟是连接两者的重要桥梁。在处理真实数据或进行模拟研究时，我们常常需要检验数据是否符合诸如独立性这样的理论假设。本编码练习将向您展示如何通过生成合成数据、计算经验概率，并将其与理论预测进行比较，来从经验上检验随机变量的独立性，从而将抽象的概率定义应用于具体的数据分析任务中。",
            "id": "4913365",
            "problem": "考虑两个二元随机变量 $X \\in \\{0,1\\}$ 和 $Y \\in \\{0,1\\}$，它们分别代表在一项生物统计模拟研究的每次试验中记录的两种不同生物标志物的存在（1）或不存在（0）。理论基础是概率空间 $(\\Omega, \\mathcal{F}, \\mathbb{P})$ 上的概率公理化定义、随机变量独立性的定义以及基于频率的概率估计方法。独立性的定义为：$X$ 与 $Y$ 独立，当且仅当对于所有 $x \\in \\{0,1\\}$ 和 $y \\in \\{0,1\\}$，等式 $\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\,\\mathbb{P}(Y=y)$ 成立。\n\n您将通过重复试验生成合成数据，从生成的数据中估计经验概率，并通过证明联合概率与边际概率乘积在指定容差范围内的相等性来验证独立性。对每组参数，模拟 $N$ 次独立同分布试验，生成配对 $(X_t, Y_t)$，其中 $t = 1, 2, \\ldots, N$。令经验联合概率为\n$$\n\\hat{p}_{ij} = \\frac{1}{N}\\sum_{t=1}^{N} \\mathbf{1}\\{X_t=i, Y_t=j\\}, \\quad i,j \\in \\{0,1\\},\n$$\n令经验边际概率为\n$$\n\\hat{p}_{i\\cdot} = \\sum_{j=0}^{1}\\hat{p}_{ij} \\quad \\text{and} \\quad \\hat{p}_{\\cdot j} = \\sum_{i=0}^{1}\\hat{p}_{ij}.\n$$\n定义验证统计量\n$$\nD = \\max_{i \\in \\{0,1\\},\\, j \\in \\{0,1\\}} \\left| \\hat{p}_{ij} - \\hat{p}_{i\\cdot}\\,\\hat{p}_{\\cdot j} \\right|.\n$$\n对于用户指定的容差 $\\varepsilon > 0$，当且仅当 $D \\le \\varepsilon$ 时，判定独立性得到验证。\n\n模拟模型：\n- 独立伯努利边际：对每个 $t$，独立地从 $X_t \\sim \\text{Bernoulli}(p_X)$ 和 $Y_t \\sim \\text{Bernoulli}(p_Y)$ 中抽样。\n- 给定边际和依赖参数的指定联合分布：使用以下公式构建联合概率表\n$$\np_{11} = p_X p_Y + c,\\quad\np_{10} = p_X(1 - p_Y) - c,\\quad\np_{01} = (1 - p_X)p_Y - c,\\quad\np_{00} = (1 - p_X)(1 - p_Y) + c,\n$$\n受限于所有 $p_{ij} \\ge 0$ 且 $\\sum_{i,j} p_{ij} = 1$ 的约束。当 $c \\neq 0$ 且约束条件成立时，这将产生一个具有固定边际 $p_X$ 和 $p_Y$ 的相关模型。\n\n您的任务是实现一个程序，对下方测试套件中的每组参数，执行模拟，计算 $D$，并输出一个布尔值，指示独立性是否在给定的 $\\varepsilon$ 容差内得到验证。程序必须是自包含的，并且不得读取任何输入。\n\n随机数生成器 (RNG) 必须为每个测试用例设置种子以确保可复现性。请使用一个能在 $N$ 次试验中产生独立抽样并遵循指定模型的 RNG。\n\n测试套件参数集（每个测试用例是一个参数元组）：\n1. $(\\text{模型}=\\text{独立},\\, N=200000,\\, p_X=0.3,\\, p_Y=0.6,\\, \\varepsilon=0.002,\\, \\text{种子}=12345)$\n2. $(\\text{模型}=\\text{相关},\\, N=200000,\\, p_X=0.3,\\, p_Y=0.6,\\, c=0.05,\\, \\varepsilon=0.002,\\, \\text{种子}=54321)$\n3. $(\\text{模型}=\\text{独立},\\, N=500,\\, p_X=0.3,\\, p_Y=0.6,\\, \\varepsilon=0.06,\\, \\text{种子}=111)$\n4. $(\\text{模型}=\\text{独立},\\, N=50000,\\, p_X=0.02,\\, p_Y=0.03,\\, \\varepsilon=0.001,\\, \\text{种子}=222)$\n5. $(\\text{模型}=\\text{相关},\\, N=200000,\\, p_X=0.02,\\, p_Y=0.03,\\, c=0.0004,\\, \\varepsilon=0.0003,\\, \\text{种子}=333)$\n\n注释：\n- 对于相关模型，参数 $c$ 必须满足\n$$\n- \\min\\left\\{p_X(1-p_Y),\\, (1-p_X)p_Y \\right\\} \\le c \\le \\min\\left\\{p_X p_Y,\\, (1-p_X)(1-p_Y) \\right\\}\n$$\n以保持所有 $p_{ij}$ 的非负性。\n- 输出为布尔值。没有物理单位，没有角度，也没有百分比；所有概率必须作为 $[0,1]$ 区间内的小数处理。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表（例如，$[result_1,result_2,result_3,result_4,result_5]$），其中每个 $result_k$ 是一个布尔值，指示在第 $k$ 组参数下独立性是否得到验证。",
            "solution": "该问题要求通过蒙特卡洛模拟来验证两个二元随机变量 $X, Y \\in \\{0,1\\}$ 之间的统计独立性。整个过程基于概率论的公理化基础，并采用标准的统计估计技术。\n\n这个问题的理论基础是两个随机变量的独立性定义。在给定的概率空间 $(\\Omega, \\mathcal{F}, \\mathbb{P})$ 上，两个随机变量 $X$ 和 $Y$ 被认为是独立的，当且仅当它们的联合概率分布函数等于其边际分布函数的乘积。对于离散二元变量，此条件简化为：\n$$\n\\mathbb{P}(X=i, Y=j) = \\mathbb{P}(X=i)\\,\\mathbb{P}(Y=j) \\quad \\forall i,j \\in \\{0,1\\}\n$$\n令 $p_{ij} = \\mathbb{P}(X=i, Y=j)$ 表示联合概率，令 $p_{i\\cdot} = \\mathbb{P}(X=i)$ 和 $p_{\\cdot j} = \\mathbb{P}(Y=j)$ 表示边际概率。因此，独立性条件为对所有 $i,j$ 都有 $p_{ij} = p_{i\\cdot} p_{\\cdot j}$。\n\n在模拟研究中，真实概率 $\\{p_{ij}\\}$ 由数据生成模型定义。我们抽取一个大小为 $N$ 的独立同分布样本对 $(X_t, Y_t)_{t=1}^N$。从这个样本中，我们使用基于频率的估计量来估计真实概率。经验联合概率 $\\hat{p}_{ij}$ 计算为结果为 $(i,j)$ 的试验所占的比例：\n$$\n\\hat{p}_{ij} = \\frac{1}{N} \\sum_{t=1}^{N} \\mathbf{1}\\{X_t=i, Y_t=j\\}\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。随后，根据全概率定律，通过对经验联合概率求和来计算经验边际概率 $\\hat{p}_{i\\cdot}$ 和 $\\hat{p}_{\\cdot j}$：\n$$\n\\hat{p}_{i\\cdot} = \\sum_{j=0}^{1} \\hat{p}_{ij} = \\hat{p}_{i0} + \\hat{p}_{i1} \\quad \\text{and} \\quad \\hat{p}_{\\cdot j} = \\sum_{i=0}^{1} \\hat{p}_{ij} = \\hat{p}_{0j} + \\hat{p}_{1j}\n$$\n由于有限的样本量 $N$，抽样变异性将导致估计概率偏离其真实值。因此，即使底层数据生成过程是完全独立的，我们也不期望等式 $\\hat{p}_{ij} = \\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j}$ 精确成立。相反，我们评估与该等式的偏差是否足够小。验证统计量 $D$ 定义为所有四种可能结果上的最大绝对偏差：\n$$\nD = \\max_{i,j \\in \\{0,1\\}} |\\hat{p}_{ij} - \\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j}|\n$$\n如果这个最大偏差小于或等于指定的容差 $\\varepsilon$，即 $D \\le \\varepsilon$，则认为独立性得到“验证”。\n\n问题指定了两种模拟模型：\n1.  **独立模型**：$X_t$ 和 $Y_t$ 的样本分别从参数为 $p_X$ 和 $p_Y$ 的伯努利分布中独立抽取。根据构造，真实的联合概率为 $p_{ij} = p_{i\\cdot}p_{\\cdot j}$，因此任何测量到的偏差 $D > 0$ 都纯粹由抽样误差引起。\n2.  **相关模型**：使用边际概率 $p_X$、$p_Y$ 和一个依赖参数 $c$ 来构造联合概率分布。联合概率由以下公式给出：\n    $p_{11} = p_X p_Y + c$，\n    $p_{10} = p_X(1-p_Y) - c$，\n    $p_{01} = (1-p_X)p_Y - c$，\n    $p_{00} = (1-p_X)(1-p_Y) + c$。\n    这种构造保留了边际概率，即 $p_{1\\cdot} = p_{11}+p_{10} = p_X$ 且 $p_{\\cdot 1} = p_{11}+p_{01} = p_Y$。参数 $c$ 等于 $X$ 和 $Y$ 之间的协方差，$\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] = p_{11} - p_X p_Y = c$。当 $c \\neq 0$ 时，变量是相关的，理论偏差为 $|c|$。\n\n算法实现按个例进行。对每个测试用例，都为随机数生成器设置种子以保证可复现性。\n-   数据生成：对于独立模型，生成两个独立的随机数数组。对于相关模型，首先计算四个联合概率 $\\{p_{00}, p_{01}, p_{10}, p_{11}\\}$。这些概率定义了四种结果 $\\{(0,0), (0,1), (1,0), (1,1)\\}$ 上的一个分类分布。从该分布中抽取 $N$ 个样本。采用向量化方法将抽样的类别索引（从 0 到 3）映射回相应的 $(X,Y)$ 对。\n-   $D$的计算：根据生成的 $X$ 和 $Y$ 数据高效地构建一个 $2 \\times 2$ 的列联表（计数）。该表通过除以 $N$ 进行归一化，以产生经验联合概率矩阵 $\\hat{P} = (\\hat{p}_{ij})$。通过分别对 $\\hat{P}$ 的行和列求和，得到边际概率向量 $\\hat{p}_{i\\cdot}$ 和 $\\hat{p}_{\\cdot j}$。边际概率乘积矩阵 $(\\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j})$ 通过这些向量的外积计算得出。最后，通过找到 $\\hat{P}$ 与边际乘积矩阵之间差的绝对值矩阵中的最大元素来获得 $D$。将得到的 $D$ 与 $\\varepsilon$ 进行比较，以产生最终的布尔值输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for each test case\n    and verifying independence based on the specified criterion.\n    \"\"\"\n\n    test_cases = [\n        ('independent', 200000, 0.3, 0.6, 0.002, 12345),\n        ('dependent', 200000, 0.3, 0.6, 0.05, 0.002, 54321),\n        ('independent', 500, 0.3, 0.6, 0.06, 111),\n        ('independent', 50000, 0.02, 0.03, 0.001, 222),\n        ('dependent', 200000, 0.02, 0.03, 0.0004, 0.0003, 333)\n    ]\n\n    results = []\n\n    def _calculate_verification(X, Y, N, epsilon):\n        \"\"\"\n        Common logic to calculate the verification statistic D and check against epsilon.\n        \"\"\"\n        # Create a 2x2 contingency table of counts\n        counts = np.zeros((2, 2), dtype=np.int64)\n        np.add.at(counts, (X, Y), 1)\n\n        # Calculate empirical joint probabilities\n        p_hat_ij = counts / N\n\n        # Calculate empirical marginal probabilities\n        p_hat_i_dot = np.sum(p_hat_ij, axis=1)\n        p_hat_dot_j = np.sum(p_hat_ij, axis=0)\n\n        # Calculate the matrix of product-of-marginals\n        p_hat_prod = np.outer(p_hat_i_dot, p_hat_dot_j)\n\n        # Calculate the verification statistic D\n        D = np.max(np.abs(p_hat_ij - p_hat_prod))\n\n        # Check if D is within the tolerance epsilon\n        return D = epsilon\n\n    for case in test_cases:\n        model = case[0]\n        seed = case[-1]\n        epsilon = case[-2]\n        N = case[1]\n        p_X = case[2]\n        p_Y = case[3]\n        \n        # Initialize the random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        if model == 'independent':\n            # Generate X and Y from independent Bernoulli distributions\n            X = (rng.random(N)  p_X).astype(np.int8)\n            Y = (rng.random(N)  p_Y).astype(np.int8)\n        \n        elif model == 'dependent':\n            c = case[4]\n            \n            # Calculate the joint probability distribution\n            p11 = p_X * p_Y + c\n            p10 = p_X * (1 - p_Y) - c\n            p01 = (1 - p_X) * p_Y - c\n            p00 = 1.0 - p11 - p10 - p01\n            \n            # Probabilities for outcomes (0,0), (0,1), (1,0), (1,1)\n            probs = [p00, p01, p10, p11]\n            \n            # Draw N samples from the categorical distribution\n            choices = rng.choice(4, size=N, p=probs)\n            \n            # Map choice indices to (X, Y) pairs efficiently\n            # choice 0 -> (0,0), 1 -> (0,1), 2 -> (1,0), 3 -> (1,1)\n            X = (choices // 2).astype(np.int8)\n            Y = (choices % 2).astype(np.int8)\n\n        # Calculate the result for the current case\n        result = _calculate_verification(X, Y, N, epsilon)\n        results.append(result.item())\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}