{
    "hands_on_practices": [
        {
            "introduction": "The axioms of probability provide the theoretical foundation, but their true power is revealed when we use them to build models for real-world phenomena. This first practice challenges you to do just that by constructing a probability space for one of the most common scenarios in biostatistics: a series of independent patient outcomes. By deriving the famous binomial probability formula from first principles, you will gain a deep appreciation for how foundational rules about independence and additivity give rise to powerful predictive tools. ",
            "id": "4942639",
            "problem": "A randomized clinical study enrolls $n$ patients to receive a new therapy. For each patient $i \\in \\{1,\\dots,n\\}$, define a binary outcome $X_{i}$ where $X_{i}=1$ denotes a clinically validated response and $X_{i}=0$ denotes no response. Assume each patient has the same response risk $p \\in (0,1)$ and that outcomes across patients are independent in the sense of product construction from first principles.\n\nUsing only the Kolmogorov axioms of probability (nonnegativity, normalization, countable additivity) and the definition of independence, construct a probability space that represents the $n$ independent and identically distributed Bernoulli risks. Explicitly specify the sample space $\\Omega$, the sigma-algebra $\\mathcal{F}$, and the probability measure $\\mathbb{P}$ so that the coordinate outcomes are independent and identically distributed with $\\mathbb{P}(X_{i}=1)=p$ and $\\mathbb{P}(X_{i}=0)=1-p$ for each $i \\in \\{1,\\dots,n\\}$. Then, derive from these axioms and counting arguments the probability that there are exactly $k$ responses, where $k \\in \\{0,1,\\dots,n\\}$.\n\nExpress your final answer as a single closed-form analytic expression in terms of $n$, $k$, and $p$. No rounding is required.",
            "solution": "The problem statement is a well-posed and fundamental exercise in probability theory, specifically the construction of a product probability space for a sequence of independent and identically distributed (i.i.d.) Bernoulli trials. It is scientifically sound, internally consistent, and contains all necessary information to derive the requested probability. We therefore proceed with the solution.\n\nThe objective is to first construct a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$ that models the outcomes of $n$ independent patients, each with a binary response. We must then use this construction and foundational principles to derive the probability of observing exactly $k$ responses.\n\n**Step 1: Construction of the Probability Space $(\\Omega, \\mathcal{F}, \\mathbb{P})$**\n\nA probability space is a triple consisting of a sample space $\\Omega$, a sigma-algebra of events $\\mathcal{F}$, and a probability measure $\\mathbb{P}$.\n\n**1. The Sample Space $\\Omega$**\nThe outcome of the study for $n$ patients is a sequence of length $n$, where the $i$-th element represents the outcome for patient $i$. Since each outcome $X_i$ is binary ($1$ for response, $0$ for no response), an elementary outcome of the entire experiment is a tuple $\\omega = (\\omega_1, \\omega_2, \\dots, \\omega_n)$ where each $\\omega_i \\in \\{0, 1\\}$. The sample space $\\Omega$ is the set of all such possible sequences. This is the $n$-fold Cartesian product of the set $\\{0, 1\\}$ with itself.\n$$ \\Omega = \\{0, 1\\}^n = \\{(\\omega_1, \\omega_2, \\dots, \\omega_n) \\mid \\omega_i \\in \\{0, 1\\} \\text{ for } i=1, \\dots, n\\} $$\nThis is a finite sample space with $|\\Omega| = 2^n$ elements.\n\n**2. The Sigma-Algebra $\\mathcal{F}$**\nAn event is a subset of $\\Omega$. The collection of all events must form a sigma-algebra $\\mathcal{F}$. For a finite sample space such as $\\Omega$, the most comprehensive and standard choice for the sigma-algebra is the power set of $\\Omega$, denoted $\\mathcal{P}(\\Omega)$. The power set is the set of all subsets of $\\Omega$.\n$$ \\mathcal{F} = \\mathcal{P}(\\Omega) $$\nThis choice trivially satisfies the axioms of a sigma-algebra:\n   - $\\Omega \\in \\mathcal{F}$ (since $\\Omega \\subseteq \\Omega$).\n   - If $A \\in \\mathcal{F}$, then its complement $A^c = \\Omega \\setminus A$ is also a subset of $\\Omega$ and thus $A^c \\in \\mathcal{F}$.\n   - If $A_1, A_2, \\dots$ is any (finite or countable) collection of sets in $\\mathcal{F}$, their union $\\bigcup_i A_i$ is also a subset of $\\Omega$ and thus is in $\\mathcal{F}$.\n\n**3. The Probability Measure $\\mathbb{P}$**\nThe probability measure $\\mathbb{P}: \\mathcal{F} \\to [0, 1]$ assigns a probability to every event in $\\mathcal{F}$. To satisfy the Kolmogorov axioms and the problem's conditions of independence and identical distribution, we first define the probability of each elementary outcome $\\omega \\in \\Omega$.\n\nThe problem states that the patient outcomes are independent and that for each patient $i$, the probability of a response is $\\mathbb{P}(X_i=1) = p$ and of no response is $\\mathbb{P}(X_i=0) = 1-p$. An elementary outcome $\\omega = (\\omega_1, \\dots, \\omega_n)$ represents the joint occurrence of $n$ individual outcomes. The event $\\{\\omega\\}$ is the intersection of the events $E_i = \\{\\text{outcome of patient } i \\text{ is } \\omega_i\\}$ for $i=1, \\dots, n$.\n\nBy the definition of independence, the probability of the intersection of independent events is the product of their individual probabilities.\n$$ \\mathbb{P}(\\{\\omega\\}) = \\mathbb{P}(\\text{outcome 1 is } \\omega_1, \\dots, \\text{outcome } n \\text{ is } \\omega_n) = \\prod_{i=1}^{n} \\mathbb{P}(\\text{outcome } i \\text{ is } \\omega_i) $$\nWe are given $\\mathbb{P}(\\text{outcome } i \\text{ is } 1) = p$ and $\\mathbb{P}(\\text{outcome } i \\text{ is } 0) = 1-p$. This can be written compactly as $\\mathbb{P}(\\text{outcome } i \\text{ is } \\omega_i) = p^{\\omega_i} (1-p)^{1-\\omega_i}$, since if $\\omega_i=1$, this is $p^1(1-p)^0=p$, and if $\\omega_i=0$, this is $p^0(1-p)^1=1-p$.\n\nTherefore, for an elementary outcome $\\omega = (\\omega_1, \\dots, \\omega_n)$, its probability is:\n$$ \\mathbb{P}(\\{\\omega\\}) = \\prod_{i=1}^{n} p^{\\omega_i} (1-p)^{1-\\omega_i} = p^{\\sum_{i=1}^n \\omega_i} (1-p)^{\\sum_{i=1}^n (1-\\omega_i)} = p^{\\sum_{i=1}^n \\omega_i} (1-p)^{n - \\sum_{i=1}^n \\omega_i} $$\nLet $s(\\omega) = \\sum_{i=1}^n \\omega_i$ be the number of responses (ones) in the sequence $\\omega$. Then the probability of this specific sequence is:\n$$ \\mathbb{P}(\\{\\omega\\}) = p^{s(\\omega)} (1-p)^{n-s(\\omega)} $$\nFor any event $A \\in \\mathcal{F}$, which is a set of elementary outcomes, the third Kolmogorov axiom (countable additivity, which for a finite space simplifies to finite additivity) dictates that its probability is the sum of the probabilities of the elementary outcomes it contains:\n$$ \\mathbb{P}(A) = \\sum_{\\omega \\in A} \\mathbb{P}(\\{\\omega\\}) $$\nWe must verify that this measure satisfies the first two Kolmogorov axioms.\n- **Axiom 1 (Nonnegativity):** Since $p \\in (0, 1)$, we have $p > 0$ and $1-p > 0$. Thus, for any $\\omega$, $\\mathbb{P}(\\{\\omega\\}) = p^{s(\\omega)} (1-p)^{n-s(\\omega)} \\ge 0$. The probability of any event $A$ is a sum of these non-negative terms, so $\\mathbb{P}(A) \\ge 0$.\n- **Axiom 2 (Normalization):** We must show $\\mathbb{P}(\\Omega)=1$.\n$$ \\mathbb{P}(\\Omega) = \\sum_{\\omega \\in \\Omega} \\mathbb{P}(\\{\\omega\\}) = \\sum_{\\omega \\in \\Omega} p^{s(\\omega)} (1-p)^{n-s(\\omega)} $$\nWe can group the elementary outcomes by the number of responses, $k = s(\\omega)$. For a fixed $k \\in \\{0, 1, \\dots, n\\}$, the number of sequences $\\omega$ with exactly $k$ responses is the number of ways to choose $k$ positions for the '1's out of $n$ positions, which is given by the binomial coefficient $\\binom{n}{k}$. Each such sequence has the same probability $p^k(1-p)^{n-k}$.\n$$ \\mathbb{P}(\\Omega) = \\sum_{k=0}^{n} \\left( \\sum_{\\omega: s(\\omega)=k} p^k (1-p)^{n-k} \\right) = \\sum_{k=0}^{n} \\binom{n}{k} p^k (1-p)^{n-k} $$\nThis sum is the binomial expansion of $(p + (1-p))^n$.\n$$ \\mathbb{P}(\\Omega) = (p + (1-p))^n = 1^n = 1 $$\nThe normalization axiom is satisfied. The probability space is now fully and correctly constructed.\n\n**Step 2: Derivation of the Probability of Exactly $k$ Responses**\n\nWe are asked to find the probability of the event that there are exactly $k$ responses. Let us denote this event by $E_k$. In the formalism of our sample space, this event is the set of all elementary outcomes $\\omega$ that contain exactly $k$ ones.\n$$ E_k = \\{\\omega \\in \\Omega \\mid s(\\omega) = \\sum_{i=1}^n \\omega_i = k\\} $$\nSince $E_k \\subseteq \\Omega$, $E_k$ is an event in our sigma-algebra $\\mathcal{F}$. Using the additivity property of the probability measure, its probability is the sum of the probabilities of its constituent elementary outcomes:\n$$ \\mathbb{P}(E_k) = \\sum_{\\omega \\in E_k} \\mathbb{P}(\\{\\omega\\}) $$\nAs established in Step 1, for any single outcome $\\omega \\in E_k$, the number of responses is $s(\\omega)=k$. The probability of this specific outcome is:\n$$ \\mathbb{P}(\\{\\omega\\}) = p^k (1-p)^{n-k} $$\nCrucially, every elementary outcome in the event $E_k$ has the exact same probability. The sum therefore simplifies to the number of such outcomes multiplied by this common probability value:\n$$ \\mathbb{P}(E_k) = |E_k| \\times p^k (1-p)^{n-k} $$\nThe final task is to determine $|E_k|$, the number of elements in $E_k$. This is a purely combinatorial problem: we need to count the number of distinct binary sequences of length $n$ that have exactly $k$ ones (and consequently, $n-k$ zeros). This is equivalent to choosing $k$ positions for the ones from the $n$ available positions. The number of ways to do this is given by the binomial coefficient \"n choose k\":\n$$ |E_k| = \\binom{n}{k} = \\frac{n!}{k!(n-k)!} $$\nSubstituting this counting result into the probability expression, we obtain the probability of observing exactly $k$ responses:\n$$ \\mathbb{P}(E_k) = \\binom{n}{k} p^k (1-p)^{n-k} $$\nThis result, known as the probability mass function of the binomial distribution, has been derived from first principles as required.",
            "answer": "$$ \\boxed{\\binom{n}{k} p^k (1-p)^{n-k}} $$"
        },
        {
            "introduction": "The concept of independence seems simple, but it harbors important subtleties. It is tempting to assume that if events are independent in pairs, they must be independent as a whole group, but this is not always true. This exercise uses a hypothetical but clear scenario involving clinical test results to construct a classic counterexample, forcing you to distinguish between pairwise and joint independence and appreciate why the latter is a much stronger condition. ",
            "id": "4942611",
            "problem": "In a cohort study of an emerging viral infection, each participant is tested at two visits using polymerase chain reaction (PCR). Let the baseline test result be encoded by a binary variable $X \\in \\{0,1\\}$, where $X=1$ denotes PCR-positive and $X=0$ denotes PCR-negative. Let the follow-up test result be encoded by a binary variable $Y \\in \\{0,1\\}$ defined analogously. Assume the following simplified modeling assumptions to isolate probability structure: \n- Across participants, the baseline and follow-up PCR positivity are exchangeable with $P(X=1)=P(Y=1)=\\frac{1}{2}$.\n- Conditional on the underlying latent infection dynamics, $X$ and $Y$ are independent and the four joint outcomes $(X,Y) \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$ are equally likely.\n\nDefine the three events\n- $A=\\{X=1\\}$, \n- $B=\\{Y=1\\}$, \n- $C=\\{X \\neq Y\\}$ (the event that the participantâ€™s PCR status changes between visits, corresponding to either clearance or incident detection).\n\nStarting only from the foundational axioms of probability (Kolmogorov axioms) and the definitions of independence of events, construct the sample space and the probability measure for this model, compute $P(A)$, $P(B)$, $P(C)$, $P(A \\cap B)$, $P(A \\cap C)$, $P(B \\cap C)$, and $P(A \\cap B \\cap C)$, and use these quantities to determine whether $A$, $B$, and $C$ are pairwise independent and whether they are jointly independent.\n\nProvide the value of $P(A \\cap B \\cap C)$ as your final answer. No rounding is required. Express all probabilities as exact fractions or decimals.",
            "solution": "The experiment consists of observing the pair of test results $(X,Y)$. The set of all possible outcomes, the sample space $\\Omega$, is given by:\n$$\n\\Omega = \\{(0,0), (0,1), (1,0), (1,1)\\}\n$$\nThe problem states that these four elementary outcomes are equally likely. According to the axioms of probability, the sum of probabilities of all outcomes must be $1$. Therefore, the probability measure $\\mathbb{P}$ assigns a probability of $\\frac{1}{4}$ to each elementary outcome:\n$$\n\\mathbb{P}(\\{(0,0)\\}) = \\mathbb{P}(\\{(0,1)\\}) = \\mathbb{P}(\\{(1,0)\\}) = \\mathbb{P}(\\{(1,1)\\}) = \\frac{1}{4}\n$$\nNow, we define the events $A$, $B$, and $C$ as subsets of the sample space $\\Omega$.\n-   Event $A = \\{X=1\\}$ corresponds to the outcomes where the first element is $1$.\n    $$\n    A = \\{(1,0), (1,1)\\}\n    $$\n-   Event $B = \\{Y=1\\}$ corresponds to the outcomes where the second element is $1$.\n    $$\n    B = \\{(0,1), (1,1)\\}\n    $$\n-   Event $C = \\{X \\neq Y\\}$ corresponds to the outcomes where the two elements are different.\n    $$\n    C = \\{(0,1), (1,0)\\}\n    $$\nNext, we compute the probabilities of these events by summing the probabilities of their constituent elementary outcomes.\n$$\n\\mathbb{P}(A) = \\mathbb{P}(\\{(1,0)\\}) + \\mathbb{P}(\\{(1,1)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\n$$\n\\mathbb{P}(B) = \\mathbb{P}(\\{(0,1)\\}) + \\mathbb{P}(\\{(1,1)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\n$$\n\\mathbb{P}(C) = \\mathbb{P}(\\{(0,1)\\}) + \\mathbb{P}(\\{(1,0)\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\n$$\nTo check for independence, we must compute the probabilities of the intersections of these events.\n-   $A \\cap B = \\{(1,0), (1,1)\\} \\cap \\{(0,1), (1,1)\\} = \\{(1,1)\\}$.\n    $$\n    \\mathbb{P}(A \\cap B) = \\mathbb{P}(\\{(1,1)\\}) = \\frac{1}{4}\n    $$\n-   $A \\cap C = \\{(1,0), (1,1)\\} \\cap \\{(0,1), (1,0)\\} = \\{(1,0)\\}$.\n    $$\n    \\mathbb{P}(A \\cap C) = \\mathbb{P}(\\{(1,0)\\}) = \\frac{1}{4}\n    $$\n-   $B \\cap C = \\{(0,1), (1,1)\\} \\cap \\{(0,1), (1,0)\\} = \\{(0,1)\\}$.\n    $$\n    \\mathbb{P}(B \\cap C) = \\mathbb{P}(\\{(0,1)\\}) = \\frac{1}{4}\n    $$\nNow we check for pairwise independence. Two events $E_1$ and $E_2$ are independent if $\\mathbb{P}(E_1 \\cap E_2) = \\mathbb{P}(E_1)\\mathbb{P}(E_2)$.\n-   For $A$ and $B$: $\\mathbb{P}(A)\\mathbb{P}(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. Since $\\mathbb{P}(A \\cap B) = \\frac{1}{4}$, events $A$ and $B$ are independent.\n-   For $A$ and $C$: $\\mathbb{P}(A)\\mathbb{P}(C) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. Since $\\mathbb{P}(A \\cap C) = \\frac{1}{4}$, events $A$ and $C$ are independent.\n-   For $B$ and $C$: $\\mathbb{P}(B)\\mathbb{P}(C) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. Since $\\mathbb{P}(B \\cap C) = \\frac{1}{4}$, events $B$ and $C$ are independent.\nThus, the events $A$, $B$, and $C$ are pairwise independent.\n\nFor the events to be jointly (or mutually) independent, they must be pairwise independent and also satisfy the condition $\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C)$. We calculate the intersection $A \\cap B \\cap C$:\n$$\nA \\cap B \\cap C = (A \\cap B) \\cap C = \\{(1,1)\\} \\cap \\{(0,1), (1,0)\\} = \\emptyset\n$$\nThe intersection of the three events is the empty set. The probability of the empty set is zero.\n$$\n\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(\\emptyset) = 0\n$$\nNow we compare this to the product of the individual probabilities:\n$$\n\\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}\n$$\nSince $\\mathbb{P}(A \\cap B \\cap C) = 0$ but $\\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C) = \\frac{1}{8}$, we have:\n$$\n\\mathbb{P}(A \\cap B \\cap C) \\neq \\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C)\n$$\nTherefore, the events $A$, $B$, and $C$ are not jointly independent, despite being pairwise independent.\n\nThe problem asks for the value of $\\mathbb{P}(A \\cap B \\cap C)$, which we have calculated to be $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "While independence is a convenient assumption, many biostatistical datasets feature outcomes that are correlated. This advanced exercise introduces a more flexible concept called exchangeability, where the joint probability of outcomes is unaffected by their order. You will explore how unobserved heterogeneity, modeled here as a latent variable, can lead to outcomes that are exchangeable but not independent, providing a crucial insight into the structure of hierarchical and mixture models used throughout biostatistics. ",
            "id": "4942635",
            "problem": "In a multi-center clinical study of a binary outcome, consider a latent-site mixture to model unexplained heterogeneity across centers. Let $Z \\in \\{0,1\\}$ be an unobserved site-level indicator with $\\mathbb{P}(Z=1)=q$ and $\\mathbb{P}(Z=0)=1-q$, where $0<q<1$. Conditional on $Z=z$, the responses of $n$ distinct patients, $Y_{1},\\dots,Y_{n}$, are independent and identically distributed Bernoulli random variables with success probability $p_{z}$, where $p_{1}=\\alpha$ and $p_{0}=\\beta$, with $0<\\alpha,\\beta<1$ and $\\alpha \\neq \\beta$. Assume $n \\geq 2$ and that all patients in the sample belong to the same latent site (that is, they share the same $Z$).\n\nStarting only from the Kolmogorov axioms, the definition of exchangeability (invariance of the joint distribution under index permutations), the definition of independence (factorization of the joint distribution into marginals), and the law of total probability, provide a formal argument that the joint distribution of $(Y_{1},\\dots,Y_{n})$ is exchangeable but, unless $q \\in \\{0,1\\}$ or $\\alpha=\\beta$, the variables $Y_{i}$ are not mutually independent. Use conditioning on $Z$ to show that all patients have the same marginal success probability but that correlations persist unconditionally. Explicitly demonstrate for $n=2$ that $\\mathbb{P}(Y_{1}=1, Y_{2}=1) \\neq \\mathbb{P}(Y_{1}=1)\\,\\mathbb{P}(Y_{2}=1)$ when $0<q<1$ and $\\alpha \\neq \\beta$.\n\nFinally, for the specific parameter values $q=\\frac{2}{5}$, $\\alpha=\\frac{4}{5}$, and $\\beta=\\frac{1}{5}$, compute the correlation coefficient between $Y_{1}$ and $Y_{2}$ implied by this mixture model. Express your final answer as a reduced fraction with no rounding. Your final answer should be this correlation coefficient only.",
            "solution": "The problem statement is a well-posed and scientifically grounded exercise in probability theory, specifically regarding mixture models and exchangeable random variables. All provided parameters and conditions are consistent and sufficient for a complete solution. Therefore, the problem is valid.\n\nLet the set of random variables be $(Y_{1}, \\dots, Y_{n})$, representing binary outcomes for $n$ patients. The outcomes are conditioned on a latent site-level indicator $Z \\in \\{0, 1\\}$.\nThe given probabilities are:\n$\\mathbb{P}(Z=1) = q$ and $\\mathbb{P}(Z=0) = 1-q$, with $0<q<1$.\nConditional on $Z=z$, the variables $Y_i$ are independent and identically distributed (i.i.d.) Bernoulli random variables with success probability $p_z$.\nThe success probabilities are given by $p_1 = \\alpha$ and $p_0 = \\beta$, with $0 < \\alpha, \\beta < 1$ and $\\alpha \\neq \\beta$.\n\nFirst, we will establish that the sequence of random variables $(Y_{1}, \\dots, Y_{n})$ is exchangeable. A sequence is exchangeable if its joint probability distribution is invariant under any permutation of its indices. Let $(y_1, \\dots, y_n)$ be a particular realization of the outcomes, where each $y_i \\in \\{0, 1\\}$. We compute the joint probability mass function (PMF) $\\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n)$ using the law of total probability, conditioning on the latent variable $Z$.\n$$ \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n) = \\sum_{z \\in \\{0,1\\}} \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n | Z=z) \\mathbb{P}(Z=z) $$\n$$ = \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n | Z=1) \\mathbb{P}(Z=1) + \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n | Z=0) \\mathbb{P}(Z=0) $$\nGiven $Z=z$, the variables $Y_i$ are i.i.d. Bernoulli($p_z$). Therefore, the joint conditional PMF is the product of the marginal conditional PMFs:\n$$ \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n | Z=z) = \\prod_{i=1}^{n} \\mathbb{P}(Y_i=y_i | Z=z) $$\nFor a single Bernoulli trial, $\\mathbb{P}(Y_i=y_i | Z=z) = p_z^{y_i} (1-p_z)^{1-y_i}$. Thus, the product becomes:\n$$ \\prod_{i=1}^{n} p_z^{y_i} (1-p_z)^{1-y_i} = p_z^{\\sum y_i} (1-p_z)^{\\sum (1-y_i)} = p_z^{\\sum y_i} (1-p_z)^{n-\\sum y_i} $$\nLet $k = \\sum_{i=1}^{n} y_i$ be the total number of successes. The conditional joint PMF depends only on $k$ and $n$.\nSubstituting $p_1 = \\alpha$ and $p_0 = \\beta$:\n$$ \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n | Z=1) = \\alpha^k (1-\\alpha)^{n-k} $$\n$$ \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n | Z=0) = \\beta^k (1-\\beta)^{n-k} $$\nSubstituting these into the law of total probability expression:\n$$ \\mathbb{P}(Y_{1}=y_1, \\dots, Y_{n}=y_n) = q \\alpha^k (1-\\alpha)^{n-k} + (1-q) \\beta^k (1-\\beta)^{n-k} $$\nThis expression for the joint PMF depends on the sequence $(y_1, \\dots, y_n)$ only through its sum $k = \\sum_{i=1}^{n} y_i$. Let $\\pi$ be any permutation of the indices $\\{1, \\dots, n\\}$. The sum of the permuted sequence $(y_{\\pi(1)}, \\dots, y_{\\pi(n)})$ is identical to the sum of the original sequence. Therefore, the joint probability is invariant under permutation of the indices, which is the definition of exchangeability.\n\nNext, we demonstrate that the variables $Y_i$ are not mutually independent, unless $q \\in \\{0,1\\}$ or $\\alpha=\\beta$. First, we compute the marginal success probability for any $Y_i$. Due to exchangeability, all $Y_i$ are identically distributed, so $\\mathbb{P}(Y_i=1)$ is the same for all $i$. Let's compute it for $Y_1$ using the law of total probability:\n$$ \\mathbb{P}(Y_1=1) = \\mathbb{P}(Y_1=1 | Z=1)\\mathbb{P}(Z=1) + \\mathbb{P}(Y_1=1 | Z=0)\\mathbb{P}(Z=0) $$\n$$ \\mathbb{P}(Y_1=1) = \\alpha q + \\beta(1-q) $$\nLet's denote this common marginal success probability by $p_{\\text{marg}} = q\\alpha + (1-q)\\beta$. If the variables were independent, the joint probability of any set of outcomes would be the product of their marginal probabilities.\nLet's test this for $n=2$, as requested, by comparing $\\mathbb{P}(Y_1=1, Y_2=1)$ with $\\mathbb{P}(Y_1=1)\\mathbb{P}(Y_2=1)$.\nFor independence, we would need $\\mathbb{P}(Y_1=1, Y_2=1) = p_{\\text{marg}}^2$.\nLet's calculate $\\mathbb{P}(Y_1=1, Y_2=1)$ from our joint PMF formula with $n=2$ and $y_1=y_2=1$, so $k=2$:\n$$ \\mathbb{P}(Y_1=1, Y_2=1) = q \\alpha^2 (1-\\alpha)^{0} + (1-q) \\beta^2 (1-\\beta)^{0} = q\\alpha^2 + (1-q)\\beta^2 $$\nThe independence condition is:\n$$ q\\alpha^2 + (1-q)\\beta^2 = (q\\alpha + (1-q)\\beta)^2 $$\nExpanding the right-hand side:\n$$ (q\\alpha + (1-q)\\beta)^2 = q^2\\alpha^2 + 2q(1-q)\\alpha\\beta + (1-q)^2\\beta^2 $$\nSetting the two expressions equal:\n$$ q\\alpha^2 + (1-q)\\beta^2 = q^2\\alpha^2 + 2q(1-q)\\alpha\\beta + (1-q)^2\\beta^2 $$\nRearranging the terms to one side:\n$$ (q-q^2)\\alpha^2 + ((1-q)-(1-q)^2)\\beta^2 - 2q(1-q)\\alpha\\beta = 0 $$\n$$ q(1-q)\\alpha^2 + (1-q)(1-(1-q))\\beta^2 - 2q(1-q)\\alpha\\beta = 0 $$\n$$ q(1-q)\\alpha^2 + q(1-q)\\beta^2 - 2q(1-q)\\alpha\\beta = 0 $$\nFactoring out the common term $q(1-q)$:\n$$ q(1-q)(\\alpha^2 - 2\\alpha\\beta + \\beta^2) = 0 $$\n$$ q(1-q)(\\alpha-\\beta)^2 = 0 $$\nThis equality holds if and only if $q=0$, $q=1$, or $\\alpha=\\beta$. The problem states that $0<q<1$ and $\\alpha \\neq \\beta$. Under these conditions, each of the three terms $q$, $(1-q)$, and $(\\alpha-\\beta)^2$ is strictly positive. Therefore, their product is strictly positive, and the equality does not hold.\nThis demonstrates that $\\mathbb{P}(Y_1=1, Y_2=1) \\neq \\mathbb{P}(Y_1=1)\\mathbb{P}(Y_2=1)$, proving that $Y_1$ and $Y_2$ are not independent. The non-zero difference $\\mathbb{P}(Y_1=1, Y_2=1) - \\mathbb{P}(Y_1=1)\\mathbb{P}(Y_2=1)$ is the covariance, which shows that correlations persist unconditionally.\n\nFinally, we compute the correlation coefficient $\\rho(Y_1, Y_2)$ for the given parameter values $q=\\frac{2}{5}$, $\\alpha=\\frac{4}{5}$, and $\\beta=\\frac{1}{5}$.\nThe correlation coefficient is defined as:\n$$ \\rho(Y_1, Y_2) = \\frac{\\text{Cov}(Y_1, Y_2)}{\\sqrt{\\text{Var}(Y_1) \\text{Var}(Y_2)}} $$\nSince the variables are identically distributed, $\\text{Var}(Y_1) = \\text{Var}(Y_2)$, so the denominator is $\\text{Var}(Y_1)$.\n\nThe covariance is $\\text{Cov}(Y_1, Y_2) = \\mathbb{E}[Y_1 Y_2] - \\mathbb{E}[Y_1]\\mathbb{E}[Y_2]$.\nFor Bernoulli variables, $\\mathbb{E}[Y_1 Y_2] = \\mathbb{P}(Y_1=1, Y_2=1)$ and $\\mathbb{E}[Y_i]=\\mathbb{P}(Y_i=1)$.\n$$ \\text{Cov}(Y_1, Y_2) = \\mathbb{P}(Y_1=1, Y_2=1) - p_{\\text{marg}}^2 $$\nFrom our previous derivation, this is exactly the term $q(1-q)(\\alpha-\\beta)^2$.\nLet's compute its value:\n$q = \\frac{2}{5}$, so $1-q = \\frac{3}{5}$.\n$\\alpha = \\frac{4}{5}$ and $\\beta = \\frac{1}{5}$.\n$$ \\text{Cov}(Y_1, Y_2) = \\left(\\frac{2}{5}\\right)\\left(\\frac{3}{5}\\right)\\left(\\frac{4}{5} - \\frac{1}{5}\\right)^2 = \\frac{6}{25}\\left(\\frac{3}{5}\\right)^2 = \\frac{6}{25} \\cdot \\frac{9}{25} = \\frac{54}{625} $$\n\nThe variance of a Bernoulli variable is $\\text{Var}(Y_1) = p_{\\text{marg}}(1-p_{\\text{marg}})$.\nFirst, we compute $p_{\\text{marg}}$:\n$$ p_{\\text{marg}} = q\\alpha + (1-q)\\beta = \\left(\\frac{2}{5}\\right)\\left(\\frac{4}{5}\\right) + \\left(\\frac{3}{5}\\right)\\left(\\frac{1}{5}\\right) = \\frac{8}{25} + \\frac{3}{25} = \\frac{11}{25} $$\nNow, we compute the variance:\n$$ \\text{Var}(Y_1) = p_{\\text{marg}}(1-p_{\\text{marg}}) = \\frac{11}{25}\\left(1 - \\frac{11}{25}\\right) = \\frac{11}{25}\\left(\\frac{14}{25}\\right) = \\frac{154}{625} $$\n\nNow, we can compute the correlation coefficient:\n$$ \\rho(Y_1, Y_2) = \\frac{\\text{Cov}(Y_1, Y_2)}{\\text{Var}(Y_1)} = \\frac{54/625}{154/625} = \\frac{54}{154} $$\nTo express this as a reduced fraction, we find the greatest common divisor of the numerator and denominator. Both are divisible by $2$:\n$$ \\frac{54}{154} = \\frac{2 \\times 27}{2 \\times 77} = \\frac{27}{77} $$\nThe prime factorization of $27$ is $3^3$ and of $77$ is $7 \\times 11$. There are no common factors, so the fraction is fully reduced.",
            "answer": "$$\\boxed{\\frac{27}{77}}$$"
        }
    ]
}