{
    "hands_on_practices": [
        {
            "introduction": "Our journey into hands-on practice begins with the bedrock of probability theory: the axioms. This first exercise focuses on how mutually exclusive and exhaustive events partition a sample space, meaning every possible outcome belongs to exactly one event. By manipulating these fundamental rules, you will see how the probability of one part of the partition directly determines the probability of the rest .",
            "id": "55",
            "problem": "Consider a random experiment with a sample space $S$. Let $A$ and $B$ be two possible outcomes or sets of outcomes (events) within this sample space. The events $A$ and $B$ are defined to be **mutually exclusive**, which means they cannot both occur simultaneously (i.e., their intersection is the empty set, $A \\cap B = \\emptyset$).\n\nLet $p_A$ be the probability of event $A$ occurring, and $p_B$ be the probability of event $B$ occurring.\n$$\nP(A) = p_A\n$$\n$$\nP(B) = p_B\n$$\n\nFurthermore, let $C$ be the event that **neither** event $A$ **nor** event $B$ occurs. The probability of this event is given as $p_C$.\n$$\nP(C) = p_C\n$$\nThe three events $A$, $B$, and $C$ are collectively exhaustive, meaning that their union constitutes the entire sample space $S$.\n\nUsing only the fundamental axioms of probability, derive an expression for the probability of the union of events $A$ and $B$, denoted as $P(A \\cup B)$, solely in terms of the given probability $p_C$.",
            "solution": "By the axioms of probability for mutually exclusive and collectively exhaustive events $A,B,C$, we have\n$$\nP(A\\cup B\\cup C)=1.\n$$\nSince $A,B,C$ are pairwise disjoint,\n$$\nP(A\\cup B\\cup C)=P(A)+P(B)+P(C)=p_A+p_B+p_C.\n$$\nHence\n$$\np_A+p_B+p_C=1.\n$$\nAlso, because $A$ and $B$ are mutually exclusive,\n$$\nP(A\\cup B)=P(A)+P(B)=p_A+p_B.\n$$\nEliminating $p_A+p_B$ using the exhaustive sum,\n$$\nP(A\\cup B)=1-p_C.\n$$",
            "answer": "$$\\boxed{1 - p_C}$$"
        },
        {
            "introduction": "A frequent point of confusion in probability is the distinction between mutually exclusive and independent events. Mutually exclusive events cannot occur together, making them highly dependent, whereas independent events have no influence on one another. This problem  presents a realistic biostatistical scenario, challenging you to use concrete data to calculate marginal and joint probabilities and test these two crucial concepts from first principles.",
            "id": "4931655",
            "problem": "A biostatistics team is studying the relationship between a binary laboratory biomarker $M$ (positive $M^+$, negative $M^-$) and a binary clinical outcome $Y$ (improved $Y^+$, not improved $Y^-$) in a cohort. The sample space $\\Omega$ of a randomly selected participant consists of the four joint outcomes $\\{(M^+,Y^+),(M^+,Y^-),(M^-,Y^+),(M^-,Y^-)\\}$, with probabilities assigned to each joint outcome. For each option below, a complete probability model on $\\Omega$ is given along with event definitions for $A$ and $B$. Select all options in which $A$ and $B$ are independent and not mutually exclusive. Independence, mutual exclusivity, and exhaustiveness must be assessed from first principles using the provided probability models.\n\nOption A:\n- Joint probabilities: $P(M^+,Y^+)=0.20$, $P(M^+,Y^-)=0.20$, $P(M^-,Y^+)=0.30$, $P(M^-,Y^-)=0.30$.\n- Events: $A=\\{M^+\\}$, $B=\\{Y^+\\}$.\n\nOption B:\n- Joint probabilities: $P(M^+,Y^+)=0.10$, $P(M^+,Y^-)=0.30$, $P(M^-,Y^+)=0.40$, $P(M^-,Y^-)=0.20$.\n- Events: $A=\\{M^+\\}$, $B=\\{Y^+\\}$.\n\nOption C:\n- Joint probabilities: $P(M^+,Y^+)=0.20$, $P(M^+,Y^-)=0.20$, $P(M^-,Y^+)=0.30$, $P(M^-,Y^-)=0.30$.\n- Events: $A=\\{M^+\\}$, $B=\\{M^-\\}$.\n\nOption D:\n- Joint probabilities: $P(M^+,Y^+)=0.20$, $P(M^+,Y^-)=0.20$, $P(M^-,Y^+)=0.30$, $P(M^-,Y^-)=0.30$.\n- Events: $A=\\{M^+\\cup Y^+\\}$, $B=\\{Y^+\\}$.\n\nOption E:\n- Joint probabilities: $P(M^+,Y^+)=0.30$, $P(M^+,Y^-)=0.30$, $P(M^-,Y^+)=0.20$, $P(M^-,Y^-)=0.20$.\n- Events: $A=\\{M^+\\}$, $B=\\{Y^+\\}$.",
            "solution": "The problem statement will first be validated for scientific soundness, clarity, and completeness.\n\n### Step 1: Extract Givens\n\n-   **Sample Space**: The sample space $\\Omega$ for a randomly selected participant consists of four joint outcomes: $\\Omega = \\{(M^+,Y^+),(M^+,Y^-),(M^-,Y^+),(M^-,Y^-)\\}$.\n-   **Biomarker**: A binary laboratory biomarker $M$ with states positive ($M^+$) and negative ($M^-$).\n-   **Clinical Outcome**: A binary clinical outcome $Y$ with states improved ($Y^+$) and not improved ($Y^-$).\n-   **Task**: For each option, a complete probability model is given. We must select all options where the defined events $A$ and $B$ are both independent and not mutually exclusive.\n-   **Required Analysis**: The assessment must be based on first principles of probability theory.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is grounded in elementary probability theory and its application in biostatistics, a standard and legitimate scientific field. The concepts of sample space, events, joint probability, marginal probability, independence, and mutual exclusivity are fundamental to probability. The scenario is a common one in clinical studies.\n-   **Well-Posed**: The problem is well-posed. For each option, a complete probability distribution on the sample space is provided (the sum of probabilities is $1$), and the events $A$ and $B$ are unambiguously defined. The question asks for a determination based on these definitions, which leads to a unique answer for each option.\n-   **Objective**: The problem is stated in precise, objective language. The criteria for independence ($P(A \\cap B) = P(A)P(B)$) and mutual exclusivity ($A \\cap B = \\emptyset$, implying $P(A \\cap B)=0$) are standard mathematical definitions.\n\nThe problem does not violate any of the invalidity criteria. It is a straightforward problem in applied probability.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. Proceeding to the solution.\n\n### Derivation of Solution\n\nThe problem requires us to identify all options where events $A$ and $B$ are simultaneously:\n1.  **Independent**: Two events $A$ and $B$ are independent if and only if $P(A \\cap B) = P(A)P(B)$.\n2.  **Not Mutually Exclusive**: Two events $A$ and $B$ are not mutually exclusive if their intersection is non-empty, i.e., $A \\cap B \\neq \\emptyset$. For a valid probability model where all elementary outcomes have non-zero probability, this is equivalent to $P(A \\cap B)  0$.\n\nWe will analyze each option against these two criteria. The event $\\{M^+\\}$ corresponds to the set of outcomes $\\{(M^+, Y^+), (M^+, Y^-)\\}$, and the event $\\{Y^+\\}$ corresponds to $\\{(M^+, Y^+), (M^-, Y^+)\\}$. Probabilities of such events (marginal probabilities) are found by summing the probabilities of their constituent elementary outcomes.\n\n**Option A**\n-   Joint probabilities: $P(M^+,Y^+)=0.20$, $P(M^+,Y^-)=0.20$, $P(M^-,Y^+)=0.30$, $P(M^-,Y^-)=0.30$. The sum is $0.20+0.20+0.30+0.30=1.00$.\n-   Events: $A=\\{M^+\\}$, $B=\\{Y^+\\}$.\n-   The elementary outcomes for each event are:\n    -   $A = \\{(M^+, Y^+), (M^+, Y^-)\\}$.\n    -   $B = \\{(M^+, Y^+), (M^-, Y^+)\\}$.\n-   The intersection of the events is $A \\cap B = \\{(M^+, Y^+)\\}$.\n-   The probability of the intersection is $P(A \\cap B) = P(M^+, Y^+) = 0.20$.\n-   Since $P(A \\cap B) = 0.20  0$, the events are **not mutually exclusive**.\n-   Now, we check for independence. We calculate the marginal probabilities $P(A)$ and $P(B)$:\n    -   $P(A) = P(M^+) = P(M^+, Y^+) + P(M^+, Y^-) = 0.20 + 0.20 = 0.40$.\n    -   $P(B) = P(Y^+) = P(M^+, Y^+) + P(M^-, Y^+) = 0.20 + 0.30 = 0.50$.\n-   We compute the product $P(A)P(B) = 0.40 \\times 0.50 = 0.20$.\n-   Comparing with $P(A \\cap B)$, we see that $P(A \\cap B) = 0.20$ and $P(A)P(B) = 0.20$. Since $P(A \\cap B) = P(A)P(B)$, the events are **independent**.\n-   Verdict: Option A satisfies both conditions (independent and not mutually exclusive). **Correct**.\n\n**Option B**\n-   Joint probabilities: $P(M^+,Y^+)=0.10$, $P(M^+,Y^-)=0.30$, $P(M^-,Y^+)=0.40$, $P(M^-,Y^-)=0.20$. The sum is $0.10+0.30+0.40+0.20=1.00$.\n-   Events: $A=\\{M^+\\}$, $B=\\{Y^+\\}$.\n-   The intersection is $A \\cap B = \\{(M^+, Y^+)\\}$.\n-   The probability of the intersection is $P(A \\cap B) = P(M^+, Y^+) = 0.10$.\n-   Since $P(A \\cap B) = 0.10  0$, the events are **not mutually exclusive**.\n-   Now, we check for independence:\n    -   $P(A) = P(M^+) = P(M^+, Y^+) + P(M^+, Y^-) = 0.10 + 0.30 = 0.40$.\n    -   $P(B) = P(Y^+) = P(M^+, Y^+) + P(M^-, Y^+) = 0.10 + 0.40 = 0.50$.\n-   We compute the product $P(A)P(B) = 0.40 \\times 0.50 = 0.20$.\n-   Comparing with $P(A \\cap B)$, we see that $P(A \\cap B) = 0.10$ and $P(A)P(B) = 0.20$. Since $P(A \\cap B) \\neq P(A)P(B)$, the events are **not independent**.\n-   Verdict: Option B fails the independence condition. **Incorrect**.\n\n**Option C**\n-   Joint probabilities: Same as Option A. $P(M^+,Y^+)=0.20$, $P(M^+,Y^-)=0.20$, $P(M^-,Y^+)=0.30$, $P(M^-,Y^-)=0.30$.\n-   Events: $A=\\{M^+\\}$, $B=\\{M^-\\}$.\n-   The events are defined as:\n    -   $A = \\{(M^+, Y^+), (M^+, Y^-)\\}$.\n    -   $B = \\{(M^-, Y^+), (M^-, Y^-)\\}$.\n-   The intersection of these two sets is empty: $A \\cap B = \\emptyset$.\n-   Therefore, the events are **mutually exclusive**.\n-   The problem requires events that are *not* mutually exclusive.\n-   Verdict: Option C fails the \"not mutually exclusive\" condition. **Incorrect**.\n\n**Option D**\n-   Joint probabilities: Same as Option A. $P(M^+,Y^+)=0.20$, $P(M^+,Y^-)=0.20$, $P(M^-,Y^+)=0.30$, $P(M^-,Y^-)=0.30$.\n-   Events: $A=\\{M^+\\cup Y^+\\}$, $B=\\{Y^+\\}$.\n-   The elementary outcomes for each event are:\n    -   $A = \\{M^+\\} \\cup \\{Y^+\\} = \\{(M^+,Y^+), (M^+,Y^-)\\} \\cup \\{(M^+,Y^+), (M^-,Y^+)\\} = \\{(M^+,Y^+), (M^+,Y^-), (M^-,Y^+)\\}$.\n    -   $B = \\{Y^+\\} = \\{(M^+,Y^+), (M^-,Y^+)\\}$.\n-   The intersection is $A \\cap B = B = \\{(M^+,Y^+), (M^-,Y^+)\\}$.\n-   The probability of the intersection is $P(A \\cap B) = P(B) = P(Y^+) = P(M^+,Y^+) + P(M^-,Y^+) = 0.20 + 0.30 = 0.50$.\n-   Since $P(A \\cap B) = 0.50  0$, the events are **not mutually exclusive**.\n-   Now, we check for independence. We need $P(A)$ and $P(B)$. We already have $P(B)=0.50$.\n    -   $P(A) = P(\\{(M^+,Y^+), (M^+,Y^-), (M^-,Y^+)\\}) = P(M^+,Y^+) + P(M^+,Y^-) + P(M^-,Y^+) = 0.20 + 0.20 + 0.30 = 0.70$.\n-   We compute the product $P(A)P(B) = 0.70 \\times 0.50 = 0.35$.\n-   Comparing with $P(A \\cap B)$, we see that $P(A \\cap B) = 0.50$ and $P(A)P(B) = 0.35$. Since $P(A \\cap B) \\neq P(A)P(B)$, the events are **not independent**. (Note: When one event $B$ is a proper subset of another event $A$, i.e., $B \\subset A$, they can only be independent if $P(B)0$ and $P(A)=1$, or if $P(B)=0$. Here, $P(A)=0.70 \\neq 1$ and $P(B)=0.50  0$, so they cannot be independent.)\n-   Verdict: Option D fails the independence condition. **Incorrect**.\n\n**Option E**\n-   Joint probabilities: $P(M^+,Y^+)=0.30$, $P(M^+,Y^-)=0.30$, $P(M^-,Y^+)=0.20$, $P(M^-,Y^-)=0.20$. The sum is $0.30+0.30+0.20+0.20=1.00$.\n-   Events: $A=\\{M^+\\}$, $B=\\{Y^+\\}$.\n-   The intersection is $A \\cap B = \\{(M^+, Y^+)\\}$.\n-   The probability of the intersection is $P(A \\cap B) = P(M^+, Y^+) = 0.30$.\n-   Since $P(A \\cap B) = 0.30  0$, the events are **not mutually exclusive**.\n-   Now, we check for independence:\n    -   $P(A) = P(M^+) = P(M^+, Y^+) + P(M^+, Y^-) = 0.30 + 0.30 = 0.60$.\n    -   $P(B) = P(Y^+) = P(M^+, Y^+) + P(M^-, Y^+) = 0.30 + 0.20 = 0.50$.\n-   We compute the product $P(A)P(B) = 0.60 \\times 0.50 = 0.30$.\n-   Comparing with $P(A \\cap B)$, we see that $P(A \\cap B) = 0.30$ and $P(A)P(B) = 0.30$. Since $P(A \\cap B) = P(A)P(B)$, the events are **independent**.\n-   Verdict: Option E satisfies both conditions (independent and not mutually exclusive). **Correct**.\n\nIn summary, Options A and E both describe scenarios where the events $A$ and $B$ are independent and not mutually exclusive.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "We now scale up from simple binary events to the more realistic scenario of classifying outcomes into multiple categories, a common task in biostatistics. When these categories are mutually exclusive and exhaustive, they form the basis for the multinomial distribution, a cornerstone of categorical data analysis. This advanced practice  guides you through deriving the probability estimates for such a model from first principles and applying them to a hypothetical patient dataset, bridging the gap from theory to statistical modeling.",
            "id": "4931638",
            "problem": "A hospital surveillance study classifies each patient into exactly one of $K$ disease categories that are mutually exclusive and exhaustive, meaning that for any single patient, exactly one of the $K$ categories occurs and no two categories occur simultaneously. Suppose patients are enrolled independently and identically distributed (i.i.d.) with respect to their disease category, and let $\\mathbf{p} = (p_{1}, \\dots, p_{K})$ denote the (unknown) vector of category probabilities with $\\sum_{k=1}^{K} p_{k} = 1$ and $p_{k} \\in (0,1)$ for all $k$.\n\nFrom first principles of probability (the axioms of probability and the definition of mutually exclusive and exhaustive events), a count vector $\\mathbf{X} = (X_{1}, \\dots, X_{K})$ over $n$ independent patients can be modeled using a well-tested distribution appropriate for multivariate counts constrained to sum to $n$. Formulate this model explicitly and, using the likelihood principle and constrained optimization, derive the maximum likelihood estimate (MLE) of each $p_{k}$ in terms of the observed counts.\n\nApply your derivation to the following data from $n = 320$ patients classified into $K = 5$ categories:\n- Infectious diseases: $X_{1} = 124$\n- Cardiovascular diseases: $X_{2} = 89$\n- Respiratory diseases: $X_{3} = 57$\n- Neoplasms: $X_{4} = 38$\n- Other diagnoses: $X_{5} = 12$\n\nExpress the final estimated probabilities as decimals rounded to four significant figures, ordered as $(p_{1}, p_{2}, p_{3}, p_{4}, p_{5})$ corresponding to the categories listed above.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of probability and statistical inference, specifically concerning categorical data analysis, a core topic in biostatistics. The problem is well-posed, providing all necessary data and a clear objective: deriving and applying the maximum likelihood estimator for the parameters of a multinomial distribution. The provided data are self-consistent, as the sum of the counts $124 + 89 + 57 + 38 + 12 = 320$ equals the total number of patients $n$. The problem is objective and free of ambiguity.\n\nThe problem describes a process of classifying $n$ independent patients into one of $K$ mutually exclusive and exhaustive categories. This is a classic multinomial experiment setup.\nLet $\\mathbf{X} = (X_1, X_2, \\dots, X_K)$ be the random vector of counts for each of the $K$ categories, where $\\sum_{k=1}^{K} X_k = n$.\nLet $\\mathbf{p} = (p_1, p_2, \\dots, p_K)$ be the vector of probabilities for a single patient falling into each category, where $p_k  0$ for all $k$ and $\\sum_{k=1}^{K} p_k = 1$.\n\nSince the patients are enrolled independently and identically distributed (i.i.d.), the count vector $\\mathbf{X}$ follows a Multinomial distribution. The probability mass function (PMF) for observing a specific count vector $\\mathbf{x} = (x_1, x_2, \\dots, x_K)$ is given by:\n$$ P(\\mathbf{X} = \\mathbf{x} | \\mathbf{p}) = \\frac{n!}{x_1! x_2! \\dots x_K!} \\prod_{k=1}^{K} p_k^{x_k} $$\nwhere $\\sum_{k=1}^{K} x_k = n$.\n\nThe maximum likelihood estimate (MLE) of $\\mathbf{p}$ is the value of $\\mathbf{p}$ that maximizes the likelihood function $L(\\mathbf{p} | \\mathbf{x})$, which is numerically equivalent to the PMF but viewed as a function of the parameters $\\mathbf{p}$ for fixed observed data $\\mathbf{x}$.\n$$ L(\\mathbf{p}; \\mathbf{x}) = \\frac{n!}{\\prod_{k=1}^{K} x_k!} \\prod_{k=1}^{K} p_k^{x_k} $$\nTo simplify the maximization, we work with the log-likelihood function, $\\ell(\\mathbf{p}; \\mathbf{x}) = \\ln(L(\\mathbf{p}; \\mathbf{x}))$. Maximizing the log-likelihood is equivalent to maximizing the likelihood because the logarithm is a monotonically increasing function.\n$$ \\ell(\\mathbf{p}; \\mathbf{x}) = \\ln\\left(\\frac{n!}{\\prod_{k=1}^{K} x_k!}\\right) + \\ln\\left(\\prod_{k=1}^{K} p_k^{x_k}\\right) = \\ln(C) + \\sum_{k=1}^{K} x_k \\ln(p_k) $$\nwhere $C$ is the multinomial coefficient, which is a constant with respect to $\\mathbf{p}$.\n\nWe must maximize $\\ell(\\mathbf{p}; \\mathbf{x})$ subject to the constraint that the probabilities sum to one: $g(\\mathbf{p}) = \\sum_{k=1}^{K} p_k - 1 = 0$. This is a constrained optimization problem that can be solved using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}(\\mathbf{p}, \\lambda)$ is:\n$$ \\mathcal{L}(\\mathbf{p}, \\lambda) = \\ell(\\mathbf{p}; \\mathbf{x}) - \\lambda g(\\mathbf{p}) = \\left( \\ln(C) + \\sum_{k=1}^{K} x_k \\ln(p_k) \\right) - \\lambda \\left(\\sum_{k=1}^{K} p_k - 1\\right) $$\nTo find the critical points, we take the partial derivative of $\\mathcal{L}$ with respect to each $p_j$ (for $j = 1, \\dots, K$) and the Lagrange multiplier $\\lambda$, and set them to zero.\n$$ \\frac{\\partial \\mathcal{L}}{\\partial p_j} = \\frac{x_j}{p_j} - \\lambda = 0 $$\nThis gives us $x_j = \\lambda p_j$, or $p_j = \\frac{x_j}{\\lambda}$, for each $j = 1, \\dots, K$.\n\nThe partial derivative with respect to $\\lambda$ recovers the constraint:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -\\left(\\sum_{k=1}^{K} p_k - 1\\right) = 0 \\implies \\sum_{k=1}^{K} p_k = 1 $$\nNow, we substitute the expression $p_k = x_k/\\lambda$ into the constraint equation:\n$$ \\sum_{k=1}^{K} \\frac{x_k}{\\lambda} = 1 $$\n$$ \\frac{1}{\\lambda} \\sum_{k=1}^{K} x_k = 1 $$\nGiven that the total number of observations is $n = \\sum_{k=1}^{K} x_k$, we find the value of the multiplier $\\lambda$:\n$$ \\frac{n}{\\lambda} = 1 \\implies \\lambda = n $$\nSubstituting this value of $\\lambda$ back into the expression for $p_j$, we obtain the maximum likelihood estimator $\\hat{p}_j$:\n$$ \\hat{p}_j = \\frac{x_j}{n} $$\nThus, the MLE for each category probability is its sample proportion.\n\nNow, we apply this result to the given data:\nTotal patients, $n = 320$.\nNumber of categories, $K = 5$.\nObserved counts:\n- Infectious diseases: $x_1 = 124$\n- Cardiovascular diseases: $x_2 = 89$\n- Respiratory diseases: $x_3 = 57$\n- Neoplasms: $x_4 = 38$\n- Other diagnoses: $x_5 = 12$\n\nWe calculate the estimated probabilities $\\hat{p}_k = x_k/n$ and round to four significant figures:\n- $\\hat{p}_1 = \\frac{124}{320} = 0.3875$. This value is exact and already has four significant figures.\n- $\\hat{p}_2 = \\frac{89}{320} = 0.278125$. Rounded to four significant figures, this is $0.2781$.\n- $\\hat{p}_3 = \\frac{57}{320} = 0.178125$. Rounded to four significant figures, this is $0.1781$.\n- $\\hat{p}_4 = \\frac{38}{320} = 0.11875$. Rounded to four significant figures, this is $0.1188$.\n- $\\hat{p}_5 = \\frac{12}{320} = 0.0375$. To express this with four significant figures, we add trailing zeros: $0.03750$.\n\nThe final vector of estimated probabilities is $(\\hat{p}_1, \\hat{p}_2, \\hat{p}_3, \\hat{p}_4, \\hat{p}_5)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.3875  0.2781  0.1781  0.1188  0.03750 \\end{pmatrix}}\n$$"
        }
    ]
}