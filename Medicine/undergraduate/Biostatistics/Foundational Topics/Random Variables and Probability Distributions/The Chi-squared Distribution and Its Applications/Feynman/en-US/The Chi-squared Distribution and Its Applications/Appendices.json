{
    "hands_on_practices": [
        {
            "introduction": "A deep understanding of any probability distribution begins with its fundamental properties, such as its mean and variance. This exercise  provides a straightforward way to explore how the variance of a chi-squared random variable behaves under scaling. By working through this problem, you will apply basic variance rules to see how normalizing the statistic by its degrees of freedom, a common procedure, impacts its spread.",
            "id": "2330",
            "problem": "Let $X$ be a random variable following a chi-squared distribution with $k$ degrees of freedom, denoted as $X \\sim \\chi^2(k)$. The chi-squared distribution is a continuous probability distribution with applications in statistical inference.\n\nFor a random variable $X \\sim \\chi^2(k)$, it is given that its expected value (mean) is $\\mathbb{E}[X] = k$ and its variance is $\\text{Var}(X) = 2k$.\n\nConsider a new random variable $Y$ that is created by normalizing $X$ by its degrees of freedom:\n$$\nY = \\frac{X}{k}\n$$\nYour task is to derive an expression for the variance of the random variable $Y$, denoted as $\\text{Var}(Y)$, using the provided properties of $X$. Express your final answer in terms of the parameter $k$.",
            "solution": "We have $Y = \\frac{X}{k}$.  Using the property of variance under scalar multiplication,\n$$\n\\mathrm{Var}(aX) = a^2\\,\\mathrm{Var}(X),\n$$\nwith $a = \\frac{1}{k}$, it follows that\n$$\n\\mathrm{Var}\\bigl(Y\\bigr)\n= \\mathrm{Var}\\Bigl(\\frac{X}{k}\\Bigr)\n= \\frac{1}{k^2}\\,\\mathrm{Var}(X).\n$$\nSince $\\mathrm{Var}(X)=2k$, we obtain\n$$\n\\mathrm{Var}(Y)\n= \\frac{1}{k^2}\\,(2k)\n= \\frac{2}{k}.\n$$",
            "answer": "$$\\boxed{\\frac{2}{k}}$$"
        },
        {
            "introduction": "The chi-squared distribution is central to making inferences about population variance using the sample variance, $S^2$. This practice  investigates a critical practical question: how sensitive is this classical estimator to outliers in the data? By deriving and applying the influence function, you will quantitatively measure this sensitivity and gain insight into why robust statistical methods are essential in biostatistical analysis.",
            "id": "4958322",
            "problem": "A biostatistics laboratory analyzes a continuous biomarker whose baseline distribution is well-approximated by a normal distribution. Let $X$ denote an individual measurement with distribution $F$, mean $\\mu$, and variance $\\sigma^{2}$. Consider the variance functional $T(F) = \\mathrm{Var}_{F}(X)$. A classical estimator of $\\sigma^{2}$ is the unbiased sample variance $S^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2}$, for which the statistic $(n-1) S^{2} / \\sigma^{2}$ follows a chi-squared distribution with $n-1$ degrees of freedom under normality.\n\nTo study the effect of outliers, consider the contaminated distribution $F_{\\varepsilon} = (1 - \\varepsilon) F + \\varepsilon \\Delta_{z}$, where $0 < \\varepsilon \\ll 1$ and $\\Delta_{z}$ is a point mass at $z$. The influence function at $z$ of a statistical functional $T$ at $F$ is defined by\n$$\n\\mathrm{IF}(z; T, F) \\equiv \\lim_{\\varepsilon \\to 0^{+}} \\frac{T\\big((1 - \\varepsilon) F + \\varepsilon \\Delta_{z}\\big) - T(F)}{\\varepsilon}.\n$$\nStarting only from the definition of variance $T(F) = \\mathbb{E}_{F}[X^{2}] - \\big(\\mathbb{E}_{F}[X]\\big)^{2}$ and the definition of the influence function above, do the following:\n\n1) Derive a closed-form expression for $\\mathrm{IF}(z; T, F)$ in terms of $z$, $\\mu$, and $\\sigma^{2}$.\n\n2) Specialize your result to the baseline normal model $F = \\mathcal{N}(0, 1)$, and obtain the first-order approximation\n$$\nT(F_{\\varepsilon}) \\approx T(F) + \\varepsilon \\,\\mathrm{IF}(z; T, F)\n$$\nfor a single contamination at $z = c$.\n\n3) Connect your result to the chi-squared framework as follows. Let $n = 20$, and define $Q = (n-1) S^{2}$ under the baseline scale $\\sigma^{2} = 1$. Use the fact that $\\mathbb{E}[S^{2}] = \\mathrm{Var}(X)$ for any distribution with finite second moment to write a first-order approximation to $\\mathbb{E}[Q]$ under the contamination model with $\\varepsilon = 0.01$ and $c = 10$.\n\nReport the resulting numerical value of the approximation to $\\mathbb{E}[Q]$, rounded to four significant figures.\n\nBriefly comment, in words, on why the median absolute deviation (MAD), appropriately scaled to estimate $\\sigma$ under normality, is a more robust alternative in this setting, but do not compute it.\n\nYour final answer must be a single real number.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is rooted in a standard framework of robust statistics, specifically the use of influence functions to analyze the sensitivity of statistical estimators to contamination. All definitions, constants, and tasks are clearly specified, forming a self-contained and logically consistent problem. There are no contradictions, ambiguities, or factual errors. We may therefore proceed with a full solution.\n\nThe solution is presented in three parts, followed by a brief commentary, as requested by the problem statement.\n\n### Part 1: Derivation of the Influence Function for the Variance Functional\n\nThe variance functional is given by $T(F) = \\mathrm{Var}_{F}(X)$. We start from its definition in terms of expectations:\n$$\nT(F) = \\mathbb{E}_{F}[X^{2}] - \\big(\\mathbb{E}_{F}[X]\\big)^{2}\n$$\nThe contaminated distribution is $F_{\\varepsilon} = (1 - \\varepsilon) F + \\varepsilon \\Delta_{z}$. To find $T(F_{\\varepsilon})$, we first need to compute the first two moments with respect to $F_{\\varepsilon}$.\n\nThe expectation of $X$ under $F_{\\varepsilon}$ is:\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X] = \\int x \\, dF_{\\varepsilon}(x) = (1 - \\varepsilon) \\int x \\, dF(x) + \\varepsilon \\int x \\, d\\Delta_{z}(x)\n$$\nBy definition, $\\int x \\, dF(x) = \\mathbb{E}_{F}[X] = \\mu$ and $\\int x \\, d\\Delta_{z}(x) = z$. Thus,\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X] = (1 - \\varepsilon)\\mu + \\varepsilon z\n$$\n\nThe expectation of $X^{2}$ under $F_{\\varepsilon}$ is:\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X^{2}] = \\int x^{2} \\, dF_{\\varepsilon}(x) = (1 - \\varepsilon) \\int x^{2} \\, dF(x) + \\varepsilon \\int x^{2} \\, d\\Delta_{z}(x)\n$$\nBy definition, $\\int x^{2} \\, dF(x) = \\mathbb{E}_{F}[X^{2}]$ and $\\int x^{2} \\, d\\Delta_{z}(x) = z^{2}$. We know that $\\sigma^{2} = \\mathbb{E}_{F}[X^{2}] - \\mu^{2}$, which implies $\\mathbb{E}_{F}[X^{2}] = \\sigma^{2} + \\mu^{2}$. Therefore,\n$$\n\\mathbb{E}_{F_{\\varepsilon}}[X^{2}] = (1 - \\varepsilon)(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\n$$\n\nNow we can write down $T(F_{\\varepsilon})$:\n$$\nT(F_{\\varepsilon}) = \\mathbb{E}_{F_{\\varepsilon}}[X^{2}] - \\big(\\mathbb{E}_{F_{\\varepsilon}}[X]\\big)^{2} = \\left[(1 - \\varepsilon)(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\\right] - \\left[(1 - \\varepsilon)\\mu + \\varepsilon z\\right]^{2}\n$$\nWe expand this expression, keeping terms up to the first order in $\\varepsilon$, since a limit as $\\varepsilon \\to 0$ will be taken:\n$$\n(1 - \\varepsilon)\\mu + \\varepsilon z = \\mu + \\varepsilon(z - \\mu)\n$$\n$$\n\\left[(1 - \\varepsilon)\\mu + \\varepsilon z\\right]^{2} = \\left[\\mu + \\varepsilon(z - \\mu)\\right]^{2} = \\mu^{2} + 2\\varepsilon\\mu(z-\\mu) + O(\\varepsilon^{2})\n$$\nThe first term for $T(F_{\\varepsilon})$ is:\n$$\n(1 - \\varepsilon)(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2} = \\sigma^{2} + \\mu^{2} - \\varepsilon(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\n$$\nCombining these, we get:\n$$\nT(F_{\\varepsilon}) = \\left[\\sigma^{2} + \\mu^{2} - \\varepsilon(\\sigma^{2} + \\mu^{2}) + \\varepsilon z^{2}\\right] - \\left[\\mu^{2} + 2\\varepsilon\\mu(z-\\mu) + O(\\varepsilon^{2})\\right]\n$$\n$$\nT(F_{\\varepsilon}) = (\\sigma^{2} + \\mu^{2} - \\mu^{2}) + \\varepsilon(-(\\sigma^{2} + \\mu^{2}) + z^{2} - 2\\mu(z-\\mu)) + O(\\varepsilon^{2})\n$$\n$$\nT(F_{\\varepsilon}) = \\sigma^{2} + \\varepsilon(- \\sigma^{2} - \\mu^{2} + z^{2} - 2\\mu z + 2\\mu^{2}) + O(\\varepsilon^{2})\n$$\n$$\nT(F_{\\varepsilon}) = \\sigma^{2} + \\varepsilon(z^{2} - 2\\mu z + \\mu^{2} - \\sigma^{2}) + O(\\varepsilon^{2})\n$$\nRecognizing the perfect square, we have:\n$$\nT(F_{\\varepsilon}) = \\sigma^{2} + \\varepsilon \\big( (z-\\mu)^{2} - \\sigma^{2} \\big) + O(\\varepsilon^{2})\n$$\nNow we apply the definition of the influence function, noting that $T(F) = \\sigma^{2}$:\n$$\n\\mathrm{IF}(z; T, F) = \\lim_{\\varepsilon \\to 0^{+}} \\frac{T(F_{\\varepsilon}) - T(F)}{\\varepsilon} = \\lim_{\\varepsilon \\to 0^{+}} \\frac{\\left[\\sigma^{2} + \\varepsilon \\big( (z-\\mu)^{2} - \\sigma^{2} \\big) + O(\\varepsilon^{2})\\right] - \\sigma^{2}}{\\varepsilon}\n$$\n$$\n\\mathrm{IF}(z; T, F) = \\lim_{\\varepsilon \\to 0^{+}} \\frac{\\varepsilon \\big( (z-\\mu)^{2} - \\sigma^{2} \\big) + O(\\varepsilon^{2})}{\\varepsilon} = \\lim_{\\varepsilon \\to 0^{+}} \\left[ (z-\\mu)^{2} - \\sigma^{2} + O(\\varepsilon) \\right]\n$$\nThis gives the closed-form expression for the influence function of the variance:\n$$\n\\mathrm{IF}(z; T, F) = (z-\\mu)^{2} - \\sigma^{2}\n$$\n\n### Part 2: Specialization and First-Order Approximation\n\nFor the baseline normal model $F = \\mathcal{N}(0, 1)$, the parameters are $\\mu = 0$ and $\\sigma^{2} = 1$. Substituting these into the general expression for the influence function, we obtain:\n$$\n\\mathrm{IF}(z; T, \\mathcal{N}(0, 1)) = (z - 0)^{2} - 1 = z^{2} - 1\n$$\nThe first-order approximation for the functional $T(F_{\\varepsilon})$ is given by $T(F_{\\varepsilon}) \\approx T(F) + \\varepsilon \\, \\mathrm{IF}(z; T, F)$. For a single contamination at $z = c$, this becomes:\n$$\nT(F_{\\varepsilon}) \\approx T(\\mathcal{N}(0, 1)) + \\varepsilon \\, \\mathrm{IF}(c; T, \\mathcal{N}(0, 1))\n$$\nSubstituting the specific values $T(\\mathcal{N}(0, 1)) = \\sigma^2 = 1$ and the derived influence function, we get:\n$$\nT(F_{\\varepsilon}) \\approx 1 + \\varepsilon (c^{2} - 1)\n$$\n\n### Part 3: Connection to the Chi-Squared Framework and Calculation\n\nWe are given $n=20$ and the statistic $Q = (n-1) S^{2}$. We need to find a first-order approximation to $\\mathbb{E}[Q]$ under the contamination model. Using the property of linearity of expectation and the given fact that $\\mathbb{E}[S^{2}] = \\mathrm{Var}(X)$ for any distribution with a finite second moment, we can write:\n$$\n\\mathbb{E}[Q] = \\mathbb{E}[(n-1)S^{2}] = (n-1) \\mathbb{E}[S^{2}]\n$$\nUnder the contaminated distribution $F_{\\varepsilon}$, the expectation of the sample variance is the true variance of that distribution, $\\mathrm{Var}_{F_{\\varepsilon}}(X) = T(F_{\\varepsilon})$.\n$$\n\\mathbb{E}[Q] = (n-1) T(F_{\\varepsilon})\n$$\nUsing the first-order approximation for $T(F_{\\varepsilon})$ derived in Part 2, we have:\n$$\n\\mathbb{E}[Q] \\approx (n-1) \\left[ T(F) + \\varepsilon \\, \\mathrm{IF}(c; T, F) \\right]\n$$\nThe problem specifies a baseline scale of $\\sigma^{2} = 1$, and the context implies a baseline mean of $\\mu=0$ (from Part 2). Thus, $T(F) = 1$ and $\\mathrm{IF}(c; T, F) = c^2 - 1$.\nThe approximation for $\\mathbb{E}[Q]$ is:\n$$\n\\mathbb{E}[Q] \\approx (n-1) [1 + \\varepsilon (c^{2} - 1)]\n$$\nWe now substitute the numerical values provided: $n=20$, $\\varepsilon=0.01$, and $c=10$.\n$$\n\\mathbb{E}[Q] \\approx (20-1) [1 + 0.01 (10^{2} - 1)]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 [1 + 0.01 (100 - 1)]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 [1 + 0.01 \\times 99]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 [1 + 0.99]\n$$\n$$\n\\mathbb{E}[Q] \\approx 19 \\times 1.99\n$$\n$$\n\\mathbb{E}[Q] \\approx 37.81\n$$\nThe baseline expectation of $Q$ under the $\\mathcal{N}(0,1)$ model is $\\mathbb{E}[Q] = n-1=19$, since $Q/\\sigma^2 \\sim \\chi^2_{n-1}$. The calculation shows that a $1\\%$ contamination at $10$ standard deviations nearly doubles the expected value of $Q$, highlighting the extreme sensitivity of the sample variance to outliers.\nThe result, $37.81$, is already given to four significant figures.\n\n### Commentary on the Robustness of MAD\n\nThe median absolute deviation (MAD) is a more robust estimator of scale than the sample variance. This is because it is based on medians, which are themselves robust estimators of location. The influence function of an estimator quantifies the effect of a single outlier on the estimate. For the variance, we found $\\mathrm{IF}(z; T, F) = (z-\\mu)^{2} - \\sigma^{2}$, which is unbounded as $z \\to \\infty$. This means a single, arbitrarily large outlier can have an arbitrarily large effect on the sample variance. In contrast, the influence function for the median is bounded. Consequently, the influence function for the MAD is also bounded. A bounded influence function implies that the effect of a single outlier is limited, regardless of its magnitude. This property makes the MAD a robust statistic, meaning it is insensitive to a small fraction of gross errors or outliers in the data.",
            "answer": "$$\\boxed{37.81}$$"
        },
        {
            "introduction": "Perhaps the most famous application of the chi-squared distribution is in goodness-of-fit testing, where Pearson's test statistic is known to converge to a $\\chi^2$ distribution as the sample size grows. This hands-on computational exercise  allows you to witness this fundamental theorem of statistics in action. By simulating data and calculating the test statistic repeatedly, you will see its empirical distribution get closer to the theoretical chi-squared curve, providing a tangible understanding of an abstract concept.",
            "id": "2405617",
            "problem": "You will study the asymptotic null distribution of the Pearson chi-square goodness-of-fit test statistic via simulation and quantify its convergence to the theoretical chi-square distribution. Consider a categorical model with $K$ categories and a probability vector $\\mathbf{p} = (p_{1},\\ldots,p_{K})$ where each $p_{i} \\in (0,1)$ and $\\sum_{i=1}^{K} p_{i} = 1$. For a sample of size $n$, let $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ denote the category counts, assumed to follow a multinomial distribution under the null hypothesis, and let $\\mathbf{E} = (E_{1},\\ldots,E_{K})$ be the expected counts with $E_{i} = n p_{i}$ for each $i \\in \\{1,\\ldots,K\\}$. Define the Pearson chi-square statistic\n$$\nQ_{n} \\;=\\; \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\nUnder the null hypothesis with fixed $K$ and all $p_{i} \\in (0,1)$, it is known that as $n \\to \\infty$, the distribution of $Q_{n}$ converges to the chi-square distribution with $K-1$ degrees of freedom.\n\nYour task is to produce a deterministic simulation-based program that, for each specified test case, approximates the distribution of $Q_{n}$ using Monte Carlo replication and then reports the Kolmogorov-Smirnov distance between the empirical distribution of $Q_{n}$ and the theoretical chi-square distribution with $K-1$ degrees of freedom. The Kolmogorov-Smirnov (KS) distance is the supremum of the absolute difference between two Cumulative Distribution Functions (CDFs):\n$$\nD \\;=\\; \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) \\;-\\; F_{\\chi^{2}_{K-1}}(x) \\right|.\n$$\nYour program must be deterministic by controlling pseudorandomness with the provided seed in each test case.\n\nUse the following test suite. For each test case, you are given $(K,\\mathbf{p},n,R,s)$ where $K$ is the number of categories, $\\mathbf{p}$ is the probability vector, $n$ is the sample size, $R$ is the number of Monte Carlo replications, and $s$ is the seed for the pseudorandom number generator.\n\n- Test case $1$ (general case with moderate $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 50$\n  - $R = 20000$\n  - $s = 12345$\n- Test case $2$ (same model, larger $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 200$\n  - $R = 20000$\n  - $s = 12345$\n- Test case $3$ (same model, much larger $n$):\n  - $K = 5$\n  - $\\mathbf{p} = (0.1,\\,0.2,\\,0.3,\\,0.25,\\,0.15)$\n  - $n = 2000$\n  - $R = 20000$\n  - $s = 12345$\n- Test case $4$ (edge case with small $n$ and many categories relative to $n$):\n  - $K = 8$\n  - $\\mathbf{p} = (1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8,\\,1/8)$\n  - $n = 16$\n  - $R = 20000$\n  - $s = 67890$\n- Test case $5$ (higher dimension model):\n  - $K = 20$\n  - $\\mathbf{p} = (1/20,\\,\\ldots,\\,1/20)$\n  - $n = 1000$\n  - $R = 15000$\n  - $s = 13579$\n\nYour program must compute, for each test case, a single real number equal to the Kolmogorov-Smirnov distance $D$ between the empirical distribution of $Q_{n}$ (based on $R$ replications) and the chi-square distribution with $K-1$ degrees of freedom. The answers must be real numbers rounded to exactly $6$ decimal places.\n\nFinal output format: Your program should produce a single line containing the results for the five test cases as a comma-separated list enclosed in square brackets (for example, $[d_{1},d_{2},d_{3},d_{4},d_{5}]$), where each $d_{j}$ is the rounded Kolmogorov-Smirnov distance for test case $j$.",
            "solution": "The problem posed is a well-defined exercise in computational statistics and is deemed valid. It is scientifically grounded in established statistical theory, namely Pearson's chi-square test and its asymptotic properties. The parameters for each test case are complete, consistent, and allow for a unique, verifiable solution. The task requires a numerical investigation of a fundamental limit theorem, which is a standard and meaningful procedure in quantitative disciplines.\n\nThe solution proceeds as follows. We address the problem of quantifying the convergence of the Pearson chi-square statistic, $Q_{n}$, to its asymptotic $\\chi^2$ distribution.\n\nFirst, we establish the theoretical foundation. Under the null hypothesis, the observed counts $\\mathbf{O} = (O_{1},\\ldots,O_{K})$ for a sample of size $n$ drawn from a categorical distribution with $K$ categories and probability vector $\\mathbf{p}=(p_1, \\ldots, p_K)$ follow a multinomial distribution, denoted $\\text{Multinomial}(n, \\mathbf{p})$. The expected count for category $i$ is $E_i=np_i$. The Pearson chi-square statistic is given by\n$$\nQ_{n} = \\sum_{i=1}^{K} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}.\n$$\nA fundamental result in statistics, Pearson's theorem, states that as the sample size $n$ approaches infinity, the distribution of $Q_n$ converges to a chi-square distribution with $K-1$ degrees of freedom, denoted $\\chi^2_{K-1}$. This convergence is the subject of our investigation. The rate of this convergence depends on $n$, $K$, and the specific probabilities $p_i$. The approximation is generally considered reliable when all expected counts $E_i$ are sufficiently large, typically $E_i \\ge 5$.\n\nTo numerically evaluate the quality of this approximation for finite $n$, we employ a Monte Carlo simulation. For each test case, defined by the parameters $(K, \\mathbf{p}, n, R, s)$, the procedure is as follows:\n\n1.  **Initialization**: We fix the seed of the pseudorandom number generator to the specified value $s$. This ensures that the simulation is deterministic and its results are perfectly reproducible.\n\n2.  **Simulation of $Q_n$**: We generate $R$ independent samples of the statistic $Q_n$. This is achieved by performing $R$ replications of the following steps:\n    a. Draw a vector of observed counts $\\mathbf{O} = (O_1, \\ldots, O_K)$ from the $\\text{Multinomial}(n, \\mathbf{p})$ distribution. Using a vectorized implementation, we can generate all $R$ vectors of counts in a single operation, resulting in an $R \\times K$ matrix of observed counts.\n    b. For each of the $R$ vectors of observed counts, we compute the corresponding value of $Q_n$. The expected counts $\\mathbf{E} = n\\mathbf{p}$ are constant across all replications. This calculation is also vectorized to efficiently compute an array of $R$ values of the $Q_n$ statistic.\n\n3.  **Quantification of Distributional Distance**: After generating $R$ samples of $Q_n$, we obtain an empirical Cumulative Distribution Function (CDF), denoted $\\widehat{F}_{Q_n}(x)$. We must measure the \"distance\" between this empirical CDF and the theoretical CDF of the target distribution, $F_{\\chi^2_{K-1}}(x)$. The problem specifies the use of the Kolmogorov-Smirnov (KS) distance, defined as the supremum of the absolute difference between the two CDFs:\n    $$\n    D = \\sup_{x \\in \\mathbb{R}} \\left| \\widehat{F}_{Q_{n}}(x) - F_{\\chi^{2}_{K-1}}(x) \\right|.\n    $$\n    A smaller value of $D$ indicates a better fit between the empirical distribution of $Q_n$ and the theoretical $\\chi^2_{K-1}$ distribution. This computation is performed using the `kstest` function from the `scipy.stats` library, which directly compares the generated sample of $Q_n$ values against the theoretical chi-square distribution with $K-1$ degrees of freedom.\n\nThis entire procedure is applied to each of the five test cases. The final output for each case is the computed KS distance $D$, rounded to $6$ decimal places. The results from the test cases illustrate the properties of the asymptotic approximation: the distance $D$ is expected to decrease as $n$ increases (Cases $1-3$), and the approximation is expected to be poor when expected cell counts are small (Case $4$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Computes the Kolmogorov-Smirnov distance between the empirical distribution\n    of the Pearson chi-square statistic and the theoretical chi-square\n    distribution for a suite of test cases.\n    \"\"\"\n\n    def compute_ks_distance(K, p_vec, n, R, seed):\n        \"\"\"\n        Runs a Monte Carlo simulation to compute the KS distance for one test case.\n\n        Args:\n            K (int): Number of categories.\n            p_vec (list or np.ndarray): Probability vector.\n            n (int): Sample size.\n            R (int): Number of Monte Carlo replications.\n            seed (int): Seed for the pseudorandom number generator.\n\n        Returns:\n            float: The computed KS distance, rounded to 6 decimal places.\n        \"\"\"\n        # 1. Initialize the pseudorandom number generator for deterministic results.\n        rng = np.random.default_rng(seed)\n\n        # 2. Define model parameters and calculate theoretical expected counts.\n        p_vec = np.array(p_vec)\n        expected_counts = n * p_vec\n        df = K - 1  # Degrees of freedom for the chi-square distribution.\n\n        # 3. Generate R sets of observed counts from the multinomial distribution.\n        # This is a vectorized operation, creating an (R, K) array.\n        observed_counts = rng.multinomial(n, p_vec, size=R)\n\n        # 4. Calculate the Pearson chi-square statistic for each of the R replicates.\n        # This calculation is also vectorized for efficiency.\n        # We sum over the K categories (axis=1).\n        q_n_samples = np.sum((observed_counts - expected_counts)**2 / expected_counts, axis=1)\n\n        # 5. Compute the Kolmogorov-Smirnov statistic.\n        # This compares the empirical distribution of the simulated Q_n values\n        # against the theoretical chi-square CDF with K-1 degrees of freedom.\n        ks_statistic, _ = kstest(q_n_samples, 'chi2', args=(df,), N=R)\n\n        # 6. Return the result rounded to the specified precision.\n        return round(ks_statistic, 6)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (K, p_vector, n, R, seed)\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 50, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 200, 20000, 12345),\n        (5, [0.1, 0.2, 0.3, 0.25, 0.15], 2000, 20000, 12345),\n        (8, [1/8] * 8, 16, 20000, 67890),\n        (20, [1/20] * 20, 1000, 15000, 13579)\n    ]\n\n    results = []\n    for case in test_cases:\n        K, p, n, R, s = case\n        result = compute_ks_distance(K, p, n, R, s)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}