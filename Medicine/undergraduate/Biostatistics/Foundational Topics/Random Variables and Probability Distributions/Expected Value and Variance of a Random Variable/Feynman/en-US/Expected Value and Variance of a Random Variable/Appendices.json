{
    "hands_on_practices": [
        {
            "introduction": "The concept of expected value is a cornerstone of probability, representing the long-run average outcome of a random process. This exercise provides a straightforward scenario to solidify your understanding of how to calculate the expected value for discrete random variables. By working with non-standard dice, you will practice applying the fundamental definition of expectation and utilize one of its most elegant properties: the linearity of expectation, which simplifies the analysis of combined random events. ",
            "id": "1916150",
            "problem": "A game designer is creating a new board game that uses two distinct, non-standard six-sided dice, referred to as Die A and Die B. Both dice are fair, meaning that for each die, any of its six faces is equally likely to land facing up after a roll. The set of integers on the six faces of Die A is $\\{0, 1, 1, 2, 4, 5\\}$. The set of integers on the six faces of Die B is $\\{2, 3, 4, 5, 6, 6\\}$. If a player rolls both dice simultaneously, what is the expected value of the sum of the numbers that appear on their top faces?",
            "solution": "Let $X$ be the random variable representing the numerical outcome of rolling Die A, and let $Y$ be the random variable representing the numerical outcome of rolling Die B. We are asked to find the expected value of the sum of these two random variables, which is denoted as $E[X+Y]$.\n\nA fundamental property of expected values is the linearity of expectation. This property states that for any two random variables $X$ and $Y$, the expected value of their sum is equal to the sum of their individual expected values:\n$$E[X+Y] = E[X] + E[Y]$$\nThis holds true regardless of whether the variables are independent or not. In this case, the rolls of the two dice are independent events.\n\nFirst, we calculate the expected value of the outcome of Die A, $E[X]$. The expected value of a discrete random variable is calculated by summing the product of each possible value and its probability. Since Die A is a fair six-sided die, the probability of any specific face landing up is $\\frac{1}{6}$. The values on the faces are $\\{0, 1, 1, 2, 4, 5\\}$.\nThe expected value $E[X]$ is the average of these values:\n$$E[X] = 0 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6}$$\n$$E[X] = \\frac{1}{6} (0 + 1 + 1 + 2 + 4 + 5)$$\n$$E[X] = \\frac{13}{6}$$\n\nNext, we calculate the expected value of the outcome of Die B, $E[Y]$. The values on the faces of Die B are $\\{2, 3, 4, 5, 6, 6\\}$. Since Die B is also a fair six-sided die, the probability of any face landing up is $\\frac{1}{6}$.\nThe expected value $E[Y]$ is the average of these values:\n$$E[Y] = 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6}$$\n$$E[Y] = \\frac{1}{6} (2 + 3 + 4 + 5 + 6 + 6)$$\n$$E[Y] = \\frac{26}{6}$$\n$$E[Y] = \\frac{13}{3}$$\n\nFinally, we use the linearity of expectation to find the expected sum $E[X+Y]$:\n$$E[X+Y] = E[X] + E[Y]$$\n$$E[X+Y] = \\frac{13}{6} + \\frac{26}{6}$$\n$$E[X+Y] = \\frac{13 + 26}{6}$$\n$$E[X+Y] = \\frac{39}{6}$$\nTo simplify the fraction, we divide the numerator and the denominator by their greatest common divisor, which is 3:\n$$E[X+Y] = \\frac{39 \\div 3}{6 \\div 3} = \\frac{13}{2}$$\n\nThus, the expected sum of the numbers shown on the top faces of the two dice is $\\frac{13}{2}$.",
            "answer": "$$\\boxed{\\frac{13}{2}}$$"
        },
        {
            "introduction": "Moving from discrete outcomes to continuous ones, this problem challenges you to apply the concepts of mean and variance to a continuous random variable. Here, you will model the position of a randomly chosen point, a classic application of the uniform distribution. This practice is essential for learning how to use integration to compute the central tendency ($E[D]$) and the spread ($\\operatorname{Var}(D)$) of a continuous probability distribution. ",
            "id": "1374161",
            "problem": "A point is chosen uniformly at random along a straight, rigid wire of total length $L$. Let the random variable $D$ represent the distance from this chosen point to the exact midpoint of the wire.\n\nDetermine the mean (expected value) and the variance of the random variable $D$. Your final answer should be presented as a $1 \\times 2$ matrix of the form $\\begin{pmatrix} \\text{Mean} & \\text{Variance} \\end{pmatrix}$, expressed in terms of $L$.",
            "solution": "Let the wire be centered at the origin, and let $X$ denote the coordinate of the chosen point along the wire. Since the point is chosen uniformly along the wire of length $L$, we have $X \\sim \\text{Uniform}([-L/2, L/2])$ with probability density\n$$\nf_{X}(x) = \\begin{cases}\n\\frac{1}{L}, & -\\frac{L}{2} \\leq x \\leq \\frac{L}{2},\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe distance from the midpoint is $D = |X|$. For $d \\in [0, L/2]$, the transformation $x \\mapsto d = |x|$ has two preimages $x = d$ and $x = -d$, each with derivative $1$. Thus, by the change-of-variables formula for transformations of random variables,\n$$\nf_{D}(d) = f_{X}(d) + f_{X}(-d) = \\frac{1}{L} + \\frac{1}{L} = \\frac{2}{L}, \\quad 0 \\leq d \\leq \\frac{L}{2},\n$$\nand $f_{D}(d) = 0$ otherwise. Hence $D$ is uniformly distributed on $[0, L/2]$.\n\nThe mean of $D$ is\n$$\n\\mathbb{E}[D] = \\int_{0}^{L/2} d \\cdot \\frac{2}{L} \\,\\mathrm{d}d = \\frac{2}{L} \\left[ \\frac{d^{2}}{2} \\right]_{0}^{L/2} = \\frac{1}{L} \\cdot \\frac{L^{2}}{4} = \\frac{L}{4}.\n$$\nThe second moment is\n$$\n\\mathbb{E}[D^{2}] = \\int_{0}^{L/2} d^{2} \\cdot \\frac{2}{L} \\,\\mathrm{d}d = \\frac{2}{L} \\left[ \\frac{d^{3}}{3} \\right]_{0}^{L/2} = \\frac{2}{L} \\cdot \\frac{1}{3} \\cdot \\frac{L^{3}}{8} = \\frac{L^{2}}{12}.\n$$\nTherefore, the variance is\n$$\n\\operatorname{Var}(D) = \\mathbb{E}[D^{2}] - \\left(\\mathbb{E}[D]\\right)^{2} = \\frac{L^{2}}{12} - \\left(\\frac{L}{4}\\right)^{2} = \\frac{L^{2}}{12} - \\frac{L^{2}}{16} = \\frac{L^{2}}{48}.\n$$\nThus the requested $1 \\times 2$ matrix is $\\begin{pmatrix} \\mathbb{E}[D] & \\operatorname{Var}(D) \\end{pmatrix} = \\begin{pmatrix} \\frac{L}{4} & \\frac{L^{2}}{48} \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{L}{4} & \\frac{L^{2}}{48} \\end{pmatrix}}$$"
        },
        {
            "introduction": "This advanced problem connects the theoretical concepts of expectation and variance directly to the practice of biostatistical modeling. You will explore a fundamental principle known as the Law of Total Variance, which elegantly partitions the overall variability in an outcome into explained and unexplained components. By applying this law in a logistic regression context, you will see how statisticians dissect the sources of variation in a health outcome, a critical skill for interpreting sophisticated data analyses. ",
            "id": "4911570",
            "problem": "In a prospective cohort study, let $Y \\in \\{0,1\\}$ denote whether an individual develops a particular condition within one year ($Y=1$) or not ($Y=0$). Let $X$ be a binary biomarker indicator, where $X=1$ denotes a high expression of a certain protein and $X=0$ denotes low expression. Suppose the conditional risk of the condition is modeled via logistic regression, which is a special case of the Generalized Linear Model (GLM), so that the logit link satisfies\n$$\n\\ln\\!\\left(\\frac{P(Y=1 \\mid X)}{1 - P(Y=1 \\mid X)}\\right) = \\alpha + \\beta X.\n$$\nAssume $P(X=1)=0.35$ and $P(X=0)=0.65$, with $\\alpha=-1.2$ and $\\beta=2.0$. Define $p(X) = P(Y=1 \\mid X)$.\n\nTasks:\n1. Starting only from the definitions of expected value and variance, and the basic properties of conditional expectation, derive an expression for the unconditional variance $\\operatorname{Var}(Y)$ in terms of $p(X)$, an expectation operator over the distribution of $X$, and a variance operator over the distribution of $X$.\n2. Using the given model and distribution of $X$, compute the numerical value of $\\operatorname{Var}(Y)$.\n3. Briefly interpret the two additive components in your derived expression in the context of logistic regression, distinguishing within-covariate uncertainty from between-covariate heterogeneity.\n\nRound your final numerical value in Task 2 to four significant figures. Express the final answer as a pure number without any units.",
            "solution": "### Task 1: Derivation of the Unconditional Variance $\\operatorname{Var}(Y)$\nThe derivation begins with the Law of Total Variance, a fundamental theorem in probability theory. For any two random variables $Y$ and $X$, the variance of $Y$ can be decomposed as:\n$$\n\\operatorname{Var}(Y) = E[\\operatorname{Var}(Y \\mid X)] + \\operatorname{Var}(E[Y \\mid X])\n$$\nIn this context, the outer expectation $E[\\cdot]$ and variance $\\operatorname{Var}(\\cdot)$ operators are taken with respect to the distribution of the random variable $X$. To be explicit, we can write them as $E_X[\\cdot]$ and $\\operatorname{Var}_X(\\cdot)$.\n\nWe now identify the inner conditional terms based on the problem's definitions.\nThe outcome $Y$ conditional on $X$ is a binary variable, so it follows a Bernoulli distribution. For a fixed value of $X$, the probability of success ($Y=1$) is given by $P(Y=1 \\mid X)$. The problem defines this probability as $p(X)$.\n$$\np(X) = P(Y=1 \\mid X)\n$$\nFor a Bernoulli random variable with success probability $p$, the expected value is $p$ and the variance is $p(1-p)$. Therefore, the conditional expectation and conditional variance of $Y$ given $X$ are:\n1.  Conditional Expectation: $E[Y \\mid X] = p(X)$.\n2.  Conditional Variance: $\\operatorname{Var}(Y \\mid X) = p(X)(1 - p(X))$.\n\nSubstituting these expressions back into the Law of Total Variance formula gives the desired expression for the unconditional variance $\\operatorname{Var}(Y)$:\n$$\n\\operatorname{Var}(Y) = E_X[p(X)(1 - p(X))] + \\operatorname{Var}_X(p(X))\n$$\nThis expression decomposes the total variance of the outcome $Y$ into two components, described further in Task 3.\n\n### Task 2: Numerical Computation of $\\operatorname{Var}(Y)$\nTo compute the numerical value, we first need to determine the values of $p(X)$ for $X=0$ and $X=1$. The logistic regression model is given by $\\ln(\\frac{p(X)}{1-p(X)}) = \\alpha + \\beta X$. Solving for $p(X)$ yields the logistic function:\n$$\np(X) = \\frac{\\exp(\\alpha + \\beta X)}{1 + \\exp(\\alpha + \\beta X)}\n$$\nUsing the given parameters $\\alpha = -1.2$ and $\\beta = 2.0$:\n- For $X=0$:\n$$\np(0) = P(Y=1 \\mid X=0) = \\frac{\\exp(-1.2 + 2.0 \\cdot 0)}{1 + \\exp(-1.2)} = \\frac{\\exp(-1.2)}{1 + \\exp(-1.2)} \\approx 0.231475\n$$\n- For $X=1$:\n$$\np(1) = P(Y=1 \\mid X=1) = \\frac{\\exp(-1.2 + 2.0 \\cdot 1)}{1 + \\exp(-1.2 + 2.0)} = \\frac{\\exp(0.8)}{1 + \\exp(0.8)} \\approx 0.689974\n$$\nNow we compute the two terms in the variance decomposition. The random variable $p(X)$ takes value $p(0)$ with probability $P(X=0)=0.65$ and value $p(1)$ with probability $P(X=1)=0.35$.\n\n**First term: $E_X[p(X)(1 - p(X))]$**\nThis is the expected value of the conditional variance.\n$$\nE_X[p(X)(1-p(X))] = p(0)(1-p(0)) \\cdot P(X=0) + p(1)(1-p(1)) \\cdot P(X=1)\n$$\nWe calculate the components:\n- $p(0)(1-p(0)) \\approx 0.231475 \\cdot (1 - 0.231475) \\approx 0.177895$\n- $p(1)(1-p(1)) \\approx 0.689974 \\cdot (1 - 0.689974) \\approx 0.213909$\nSubstituting these values:\n$$\nE_X[p(X)(1-p(X))] \\approx (0.177895)(0.65) + (0.213909)(0.35) \\approx 0.115632 + 0.074868 = 0.190500\n$$\n\n**Second term: $\\operatorname{Var}_X(p(X))$**\nThis is the variance of the conditional expectation. We use the formula $\\operatorname{Var}(Z) = E[Z^2] - (E[Z])^2$.\nFirst, we compute the expectation of $p(X)$:\n$$\nE_X[p(X)] = p(0) \\cdot P(X=0) + p(1) \\cdot P(X=1)\n$$\n$$\nE_X[p(X)] \\approx (0.231475)(0.65) + (0.689974)(0.35) \\approx 0.150459 + 0.241491 = 0.391950\n$$\nNext, we compute the expectation of $(p(X))^2$:\n$$\nE_X[(p(X))^2] = (p(0))^2 \\cdot P(X=0) + (p(1))^2 \\cdot P(X=1)\n$$\n$$\nE_X[(p(X))^2] \\approx (0.231475)^2(0.65) + (0.689974)^2(0.35) \\approx (0.053581)(0.65) + (0.476065)(0.35) \\approx 0.034828 + 0.166623 = 0.201451\n$$\nNow, we calculate the variance:\n$$\n\\operatorname{Var}_X(p(X)) = E_X[(p(X))^2] - (E_X[p(X)])^2 \\approx 0.201451 - (0.391950)^2 \\approx 0.201451 - 0.153625 = 0.047826\n$$\n\n**Total Variance $\\operatorname{Var}(Y)$**\nFinally, we sum the two components:\n$$\n\\operatorname{Var}(Y) = E_X[p(X)(1 - p(X))] + \\operatorname{Var}_X(p(X)) \\approx 0.190500 + 0.047826 = 0.238326\n$$\nRounding to four significant figures, the numerical value is $0.2383$.\n\n### Task 3: Interpretation of the Variance Components\nThe decomposition $\\operatorname{Var}(Y) = E_X[p(X)(1 - p(X))] + \\operatorname{Var}_X(p(X))$ provides insight into the sources of variation in the outcome $Y$.\n\n1.  **First Component: $E_X[p(X)(1 - p(X))]$**\n    This term, $E[\\operatorname{Var}(Y \\mid X)]$, represents the average **within-covariate uncertainty**. For any given subgroup of the population with a fixed biomarker status $X=x$, the outcome $Y$ is still a random variable with variance $p(x)(1-p(x))$. This variability is inherent to the Bernoulli nature of the outcome and cannot be explained by the biomarker $X$. It is the irreducible randomness or noise that remains even after we have stratified the population by the predictor. The operator $E_X[\\cdot]$ computes the weighted average of this uncertainty across the different biomarker groups ($X=0$ and $X=1$). This term is often referred to as the \"unexplained variance\".\n\n2.  **Second Component: $\\operatorname{Var}_X(p(X))$**\n    This term, $\\operatorname{Var}(E[Y \\mid X])$, represents the **between-covariate heterogeneity**. The conditional expectation $E[Y \\mid X] = p(X)$ is the risk of the condition, which is a function of the biomarker $X$. This term measures how much this predicted risk varies across the population due to the variability in the biomarker $X$. If the biomarker had no effect on the outcome (i.e., $\\beta=0$), then $p(X)$ would be constant, and this variance component would be zero. Therefore, this component quantifies the portion of the total variance in $Y$ that is systematically \"explained by\" the logistic regression model through the predictor $X$. It captures the variation in $Y$ that arises because individuals have different risk levels associated with their different biomarker statuses.",
            "answer": "$$\\boxed{0.2383}$$"
        }
    ]
}