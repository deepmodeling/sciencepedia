{
    "hands_on_practices": [
        {
            "introduction": "Real-world biostatistical data often defy simple classification as purely discrete or continuous. This exercise introduces a mixed random variable, which is common in scenarios like survival analysis with a \"cure\" fraction, where some subjects respond to treatment immediately while others respond over time. By working through this problem , you will apply the fundamental definition of expectation to this hybrid distribution, a crucial skill for correctly analyzing sophisticated biostatistical models.",
            "id": "4944773",
            "problem": "A biostatistics team is studying the time to immediate biochemical response in a cohort where some individuals respond at baseline while others respond later. Let $X$ denote the time to response (in hours). Empirically, there is a point mass at $0$ representing immediate response with probability $p$, and a continuous component for delayed response on $(0,\\infty)$ with probability density function (PDF) $(1-p)\\lambda \\exp(-\\lambda x)$, where $\\lambda0$ is a rate parameter. Thus, $X$ is a mixed random variable with a probability mass function (PMF) at $0$ equal to $p$ and a PDF on $(0,\\infty)$ equal to $(1-p)\\lambda \\exp(-\\lambda x)$. Assume $0 \\leq p \\leq 1$ and $\\lambda0$.\n\nStarting from the fundamental definition of expectation as a Lebesgue integral with respect to the distribution of $X$ (i.e., $E[g(X)] = \\int g(x)\\,\\mathrm{d}F_X(x)$), and using only well-established facts about integration and exponentials, compute the exact closed-form expressions for $E[X]$ and $E[|X|]$ in terms of $p$ and $\\lambda$. Then, discuss the integrability conditions for $X$ under the Lebesgue definition (that is, when $E[|X|]$ is finite) in terms of the parameters $p$ and $\\lambda$, and provide clear reasoning tied to the structure of the mixed distribution.\n\nExpress the final results for $E[X]$ and $E[|X|]$ as exact algebraic expressions in $p$ and $\\lambda$. No numerical rounding is required. Do not include units in your final expressions, but you may refer to units in your explanation.",
            "solution": "The problem statement is assessed to be valid as it is scientifically grounded, self-contained, and well-posed. It describes a mixed random variable, a standard concept in probability theory and biostatistics, and asks for the derivation of its expectation and a discussion of its integrability, which are well-defined mathematical tasks.\n\nThe random variable $X$, representing the time to response, has a mixed distribution. Its probability measure is composed of a discrete part and a continuous part.\n1.  A discrete point mass at $X=0$ with probability $P(X=0) = p$.\n2.  A continuous distribution on the interval $(0, \\infty)$ with a probability density function given by $f_c(x) = (1-p)\\lambda \\exp(-\\lambda x)$ for $x0$.\n\nThe total probability is the sum of the probability of the discrete event and the integral of the continuous density over its support:\n$$P(X=0) + \\int_{0}^{\\infty} f_c(x) \\, \\mathrm{d}x = p + \\int_0^\\infty (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThe integral evaluates to:\n$$(1-p)\\lambda \\int_0^\\infty \\exp(-\\lambda x) \\, \\mathrm{d}x = (1-p)\\lambda \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda x) \\right]_0^\\infty = (1-p) \\left( \\lim_{x\\to\\infty} (-\\exp(-\\lambda x)) - (-\\exp(0)) \\right)$$\nSince $\\lambda  0$, $\\lim_{x\\to\\infty} \\exp(-\\lambda x) = 0$. Thus, the integral is $(1-p)(0 - (-1)) = 1-p$.\nThe total probability is $p + (1-p) = 1$, which confirms the validity of the distribution.\n\nThe problem requires computing the expectation of a function $g(X)$ starting from the fundamental definition $E[g(X)] = \\int g(x)\\,\\mathrm{d}F_X(x)$, where $F_X(x)$ is the cumulative distribution function of $X$. For a mixed distribution like this, the Lebesgue-Stieltjes integral separates into a sum over the discrete mass points and a standard Riemann integral over the continuous domain.\n$$E[g(X)] = g(0)P(X=0) + \\int_0^\\infty g(x) f_c(x) \\, \\mathrm{d}x$$\n$$E[g(X)] = g(0) \\cdot p + \\int_0^\\infty g(x) (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\n\nFirst, we compute the expectation of $X$, denoted $E[X]$. This corresponds to the case where $g(x) = x$.\n$$E[X] = (0) \\cdot p + \\int_0^\\infty x (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\n$$E[X] = (1-p)\\lambda \\int_0^\\infty x \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nTo evaluate the integral, we use integration by parts, $\\int u \\, \\mathrm{d}v = uv - \\int v \\, \\mathrm{d}u$. Let $u=x$ and $\\mathrm{d}v = \\exp(-\\lambda x) \\, \\mathrm{d}x$. Then $\\mathrm{d}u = \\mathrm{d}x$ and $v = -\\frac{1}{\\lambda}\\exp(-\\lambda x)$.\n$$\\int_0^\\infty x \\exp(-\\lambda x) \\, \\mathrm{d}x = \\left[ x \\left(-\\frac{1}{\\lambda}\\exp(-\\lambda x)\\right) \\right]_0^\\infty - \\int_0^\\infty \\left(-\\frac{1}{\\lambda}\\exp(-\\lambda x)\\right) \\, \\mathrm{d}x$$\n$$= \\left[ -\\frac{x}{\\lambda}\\exp(-\\lambda x) \\right]_0^\\infty + \\frac{1}{\\lambda} \\int_0^\\infty \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThe first term is evaluated at the limits: $\\lim_{x\\to\\infty} \\left(-\\frac{x}{\\lambda}\\exp(-\\lambda x)\\right) - 0$. The limit as $x\\to\\infty$ is $0$, which can be verified using L'HÃ´pital's rule on $\\frac{x}{\\exp(\\lambda x)}$. The second term is $\\frac{1}{\\lambda}$ times the integral of an exponential PDF (without its rate constant), which we already know evaluates to $\\frac{1}{\\lambda}$.\n$$\\int_0^\\infty x \\exp(-\\lambda x) \\, \\mathrm{d}x = 0 + \\frac{1}{\\lambda} \\left[ -\\frac{1}{\\lambda}\\exp(-\\lambda x) \\right]_0^\\infty = \\frac{1}{\\lambda^2} (-\\exp(-\\infty) - (-\\exp(0))) = \\frac{1}{\\lambda^2}(0 - (-1)) = \\frac{1}{\\lambda^2}$$\nSubstituting this result back into the expression for $E[X]$:\n$$E[X] = (1-p)\\lambda \\left(\\frac{1}{\\lambda^2}\\right) = \\frac{1-p}{\\lambda}$$\n\nNext, we compute $E[|X|]$, which corresponds to the case $g(x) = |x|$.\n$$E[|X|] = |0| \\cdot p + \\int_0^\\infty |x| (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThe support of the random variable $X$ is the set $\\{0\\} \\cup (0, \\infty)$, where all values are non-negative. Therefore, for any value $x$ that $X$ can take, $|x|=x$.\nThis implies that $|X| = X$, and consequently, their expectations must be equal.\n$$E[|X|] = E[X] = \\frac{1-p}{\\lambda}$$\nTo be rigorous, we show this from the integral definition:\n$$E[|X|] = 0 \\cdot p + \\int_0^\\infty x (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$$\nThis is precisely the same expression as for $E[X]$, yielding the same result.\n$$E[|X|] = \\frac{1-p}{\\lambda}$$\n\nFinally, we discuss the integrability conditions for $X$ under the Lebesgue definition. A random variable $X$ is defined as integrable if its expectation exists and is finite, which is equivalent to the condition $E[|X|]  \\infty$.\nFrom our calculation, we have:\n$$E[|X|] = \\frac{1-p}{\\lambda}$$\nThe problem provides the constraints on the parameters: $0 \\le p \\le 1$ and $\\lambda  0$.\nUnder these constraints:\n- The numerator, $1-p$, is in the range $[0, 1]$.\n- The denominator, $\\lambda$, is strictly positive, so $\\lambda \\in (0, \\infty)$.\nThe ratio $\\frac{1-p}{\\lambda}$ is therefore always non-negative and finite. The only way it could be infinite is if $\\lambda=0$, but this is explicitly excluded by the problem statement.\nTherefore, $E[|X|]$ is always finite for any valid choice of parameters $p$ and $\\lambda$.\n\nThe reason for this guaranteed integrability is rooted in the structure of the distribution. The total expectation $E[|X|]$ is the sum of contributions from the discrete and continuous parts. The discrete part at $x=0$ contributes $|0| \\cdot p = 0$, which is finite. The continuous part's contribution is determined by the integral $\\int_0^\\infty x (1-p)\\lambda \\exp(-\\lambda x) \\, \\mathrm{d}x$. The convergence of this integral depends on the tail behavior of the density. The term $\\exp(-\\lambda x)$ decays to zero much faster than the term $x$ grows to infinity, for any $\\lambda  0$. This rapid exponential decay ensures that the integral is finite. Thus, the random variable $X$ is always integrable given the specified parameter ranges.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1-p}{\\lambda}  \\frac{1-p}{\\lambda} \\end{pmatrix}}$$"
        },
        {
            "introduction": "A central task in biostatistics is to estimate the parameters of a theoretical distribution using real-world sample data. This is often complicated by practical limitations like right-censoring, where we only have partial information for some subjects in a study. This practice  offers a quintessential biostatistical experience: deriving and applying the maximum likelihood estimator (MLE) for an exponential survival model, demonstrating how the likelihood function elegantly incorporates both complete and incomplete observations to make efficient use of all available information.",
            "id": "4944733",
            "problem": "A clinical study follows $n=12$ patients from enrollment to either a biomedical event of interest or administrative right-censoring. Assume the time-to-event for each patient is a realization from an independent and identically distributed (i.i.d.) exponential random variable with rate parameter $\\lambda0$, and that right-censoring is non-informative. For patient $i$, let $t_i$ denote the observed follow-up time (in months), and let $\\delta_i$ be the observed event indicator, where $\\delta_i=1$ if the event occurred at time $t_i$ and $\\delta_i=0$ if the observation was right-censored at time $t_i$. The observed data are:\n- Follow-up times (months): $2.1, 3.5, 5.2, 1.8, 4.0, 6.7, 3.3, 7.5, 2.9, 5.0, 8.2, 4.6$.\n- Event indicators: $1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0$.\n\nStarting from first principles of likelihood for independent continuous-time survival data with right-censoring, write the full likelihood in terms of $\\lambda$, the observed times $\\{t_i\\}$, and event indicators $\\{\\delta_i\\}$. Differentiate the log-likelihood to obtain the score equation, solve for the maximum likelihood estimator of the rate parameter $\\lambda$, and then compute its numerical value for the dataset above. Round your final numerical answer to four significant figures. Express the final rate in per month.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Number of patients: $n=12$.\n- The time-to-event for each patient is a realization from an independent and identically distributed (i.i.d.) exponential random variable with rate parameter $\\lambda  0$.\n- Right-censoring is non-informative.\n- For patient $i$, $t_i$ is the observed follow-up time (in months).\n- For patient $i$, $\\delta_i$ is the observed event indicator, where $\\delta_i=1$ for an event and $\\delta_i=0$ for right-censoring.\n- Observed follow-up times $\\{t_i\\}_{i=1}^{12}$: $2.1, 3.5, 5.2, 1.8, 4.0, 6.7, 3.3, 7.5, 2.9, 5.0, 8.2, 4.6$.\n- Observed event indicators $\\{\\delta_i\\}_{i=1}^{12}$: $1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the exponential distribution and the principle of maximum likelihood for right-censored data, which are standard and fundamental concepts in biostatistics and survival analysis. The model is scientifically sound.\n- **Well-Posed:** The problem provides all necessary data and clear definitions. The objective is to find the maximum likelihood estimator (MLE) for a well-defined parameter of a standard distribution, which has a unique and meaningful solution.\n- **Objective:** The problem is stated with precise, quantitative, and unbiased language.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe time-to-event $T$ for a patient is assumed to follow an exponential distribution with rate parameter $\\lambda  0$. The probability density function (PDF) is given by:\n$$f(t|\\lambda) = \\lambda \\exp(-\\lambda t), \\quad t \\ge 0$$\nThe corresponding survival function, $S(t|\\lambda)$, which is the probability that the event has not occurred by time $t$, is:\n$$S(t|\\lambda) = P(T  t) = \\int_{t}^{\\infty} \\lambda \\exp(-\\lambda u) du = \\exp(-\\lambda t)$$\nThe likelihood function is constructed based on the contributions from each of the $n=12$ independent patients. The contribution of patient $i$ depends on whether their event was observed or their observation was right-censored.\n\nFor a patient $i$ who experiences the event at time $t_i$ (i.e., $\\delta_i=1$), their contribution to the likelihood is the probability density of the event occurring at that specific time, which is $f(t_i|\\lambda)$.\n\nFor a patient $i$ who is right-censored at time $t_i$ (i.e., $\\delta_i=0$), we only know that their true event time is greater than $t_i$. Their contribution to the likelihood is the probability of this, which is given by the survival function $S(t_i|\\lambda)$.\n\nWe can write the likelihood contribution for a single patient $i$ in a general form using the event indicator $\\delta_i$:\n$$L_i(\\lambda) = [f(t_i|\\lambda)]^{\\delta_i} [S(t_i|\\lambda)]^{1-\\delta_i}$$\nSubstituting the expressions for the exponential PDF and survival function:\n$$L_i(\\lambda) = [\\lambda \\exp(-\\lambda t_i)]^{\\delta_i} [\\exp(-\\lambda t_i)]^{1-\\delta_i}$$\nThis expression can be simplified by combining the exponential terms:\n$$L_i(\\lambda) = \\lambda^{\\delta_i} \\exp(-\\lambda t_i \\delta_i) \\exp(-\\lambda t_i (1-\\delta_i)) = \\lambda^{\\delta_i} \\exp(-\\lambda t_i (\\delta_i + 1 - \\delta_i)) = \\lambda^{\\delta_i} \\exp(-\\lambda t_i)$$\nSince the observations are independent, the total likelihood function $L(\\lambda)$ for the entire sample of $n$ patients is the product of the individual likelihood contributions:\n$$L(\\lambda) = \\prod_{i=1}^{n} L_i(\\lambda) = \\prod_{i=1}^{n} \\left(\\lambda^{\\delta_i} \\exp(-\\lambda t_i)\\right)$$\nThis simplifies to:\n$$L(\\lambda) = \\left(\\prod_{i=1}^{n} \\lambda^{\\delta_i}\\right) \\left(\\prod_{i=1}^{n} \\exp(-\\lambda t_i)\\right) = \\lambda^{\\sum_{i=1}^{n} \\delta_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)$$\nTo find the maximum likelihood estimator (MLE) for $\\lambda$, it is mathematically more convenient to maximize the log-likelihood function, $\\ell(\\lambda) = \\ln(L(\\lambda))$:\n$$\\ell(\\lambda) = \\ln\\left(\\lambda^{\\sum_{i=1}^{n} \\delta_i} \\exp\\left(-\\lambda \\sum_{i=1}^{n} t_i\\right)\\right)$$\n$$\\ell(\\lambda) = \\left(\\sum_{i=1}^{n} \\delta_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i$$\nTo find the value of $\\lambda$ that maximizes $\\ell(\\lambda)$, we differentiate $\\ell(\\lambda)$ with respect to $\\lambda$ and set the result to zero. This derivative is known as the score function, $U(\\lambda)$.\n$$U(\\lambda) = \\frac{d\\ell}{d\\lambda} = \\frac{d}{d\\lambda}\\left[\\left(\\sum_{i=1}^{n} \\delta_i\\right) \\ln(\\lambda) - \\lambda \\sum_{i=1}^{n} t_i\\right] = \\frac{\\sum_{i=1}^{n} \\delta_i}{\\lambda} - \\sum_{i=1}^{n} t_i$$\nThe score equation is $U(\\hat{\\lambda}) = 0$, where $\\hat{\\lambda}$ denotes the MLE:\n$$\\frac{\\sum_{i=1}^{n} \\delta_i}{\\hat{\\lambda}} - \\sum_{i=1}^{n} t_i = 0$$\nSolving for $\\hat{\\lambda}$:\n$$\\hat{\\lambda} = \\frac{\\sum_{i=1}^{n} \\delta_i}{\\sum_{i=1}^{n} t_i}$$\nThe MLE for the rate parameter is the total number of observed events divided by the total person-time of follow-up.\n\nNow, we compute the numerical value of $\\hat{\\lambda}$ using the provided data.\nFirst, we calculate the total number of events, $d = \\sum_{i=1}^{12} \\delta_i$:\n$$d = 1+1+0+1+0+1+1+0+1+1+0+0 = 7$$\nNext, we calculate the total person-time of follow-up, $T = \\sum_{i=1}^{12} t_i$:\n$$T = 2.1+3.5+5.2+1.8+4.0+6.7+3.3+7.5+2.9+5.0+8.2+4.6 = 54.8 \\text{ months}$$\nFinally, we compute the estimate $\\hat{\\lambda}$:\n$$\\hat{\\lambda} = \\frac{d}{T} = \\frac{7}{54.8} \\approx 0.127737226...$$\nThe problem requires the numerical answer to be rounded to four significant figures.\n$$\\hat{\\lambda} \\approx 0.1277$$\nThe units of the rate are events per month.",
            "answer": "$$\\boxed{0.1277}$$"
        },
        {
            "introduction": "Beyond describing individual variables, biostatistics is deeply concerned with modeling the relationships between them, such as the link between two different clinical biomarkers. The bivariate normal distribution provides a classic and powerful framework for modeling such dependencies. This hands-on derivation  will guide you through calculating conditional expectation and variance, revealing the mathematical foundations of linear regression and illustrating how observing one variable, $Y$, can sharpen our predictions and reduce our uncertainty about another, $X$.",
            "id": "4944729",
            "problem": "A clinical cohort study models two continuous biomarkers for cardiovascular risk: systolic blood pressure, denoted by the random variable $X$, and total serum cholesterol, denoted by the random variable $Y$. Investigators posit that $(X,Y)$ follows a bivariate normal distribution with marginal distributions $X \\sim \\mathcal{N}(\\mu_{X},\\sigma_{X}^{2})$ and $Y \\sim \\mathcal{N}(\\mu_{Y},\\sigma_{Y}^{2})$, and correlation $\\rho \\in (-1,1)$. The covariance between $X$ and $Y$ is $\\mathrm{Cov}(X,Y) = \\rho\\,\\sigma_{X}\\sigma_{Y}$. Using only fundamental properties of the multivariate normal distribution, the definition of conditional expectation and conditional variance, and the structure of the joint density induced by the covariance matrix, derive closed-form expressions for the conditional expectation $E[X \\mid Y=y]$ and the conditional variance $\\mathrm{Var}(X \\mid Y=y)$ as functions of $\\mu_{X}$, $\\mu_{Y}$, $\\sigma_{X}$, $\\sigma_{Y}$, $\\rho$, and the observed value $y$. Express your final answer as two closed-form expressions arranged in a single row matrix using the $\\mathrm{pmatrix}$ environment. No rounding is required.",
            "solution": "The foundational structure is that $(X,Y)$ is bivariate normal, meaning any linear combination of $X$ and $Y$ is normally distributed, and the joint density is determined by the mean vector and covariance matrix. Let the mean vector be $(\\mu_{X},\\mu_{Y})$ and the covariance matrix be\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\sigma_{X}^{2}  \\rho\\,\\sigma_{X}\\sigma_{Y} \\\\\n\\rho\\,\\sigma_{X}\\sigma_{Y}  \\sigma_{Y}^{2}\n\\end{pmatrix}.\n$$\nA standard and well-tested representation of the bivariate normal distribution constructs $(X,Y)$ from independent standard normal variables. Let $U$ and $V$ be independent standard normal random variables, each distributed as $\\mathcal{N}(0,1)$. Define\n$$\nX \\;=\\; \\mu_{X} + \\sigma_{X} U,\n\\qquad\nY \\;=\\; \\mu_{Y} + \\sigma_{Y}\\big(\\rho\\,U + \\sqrt{1-\\rho^{2}}\\,V\\big).\n$$\nThis representation reproduces the specified marginal variances, means, and covariance, since\n$$\nE[X] = \\mu_{X},\\quad E[Y] = \\mu_{Y},\\quad \\mathrm{Var}(X) = \\sigma_{X}^{2},\\quad \\mathrm{Var}(Y) = \\sigma_{Y}^{2},\n$$\nand\n$$\n\\mathrm{Cov}(X,Y) = \\sigma_{X}\\sigma_{Y}\\,\\mathrm{Cov}\\big(U,\\,\\rho\\,U + \\sqrt{1-\\rho^{2}}\\,V\\big) = \\sigma_{X}\\sigma_{Y}\\,\\rho.\n$$\nTo compute $E[X \\mid Y=y]$ and $\\mathrm{Var}(X \\mid Y=y)$, we observe that conditioning on $Y=y$ is equivalent to conditioning on\n$$\nW \\;=\\; \\rho\\,U + \\sqrt{1-\\rho^{2}}\\,V \\;=\\; \\frac{Y - \\mu_{Y}}{\\sigma_{Y}}.\n$$\nThe pair $(U,W)$ is bivariate normal with means $E[U]=0$, $E[W]=0$, variances $\\mathrm{Var}(U)=1$, $\\mathrm{Var}(W)=1$, and correlation $\\rho$ because\n$$\n\\mathrm{Cov}(U,W) = \\mathrm{Cov}\\big(U,\\,\\rho\\,U + \\sqrt{1-\\rho^{2}}\\,V\\big) = \\rho.\n$$\nWe derive the conditional distribution of $U$ given $W=w$ from first principles via the joint density of a bivariate normal. The joint density of $(U,W)$ is\n$$\nf_{U,W}(u,w) \\;=\\; \\frac{1}{2\\pi\\sqrt{1-\\rho^{2}}}\\,\\exp\\!\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(u^{2} - 2\\rho\\,u\\,w + w^{2}\\right)\\right).\n$$\nThe conditional density $f_{U\\mid W}(u\\mid w)$ is proportional to $f_{U,W}(u,w)$ in $u$ for fixed $w$. Ignoring factors that do not depend on $u$, we have\n$$\nf_{U\\mid W}(u\\mid w) \\;\\propto\\; \\exp\\!\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(u^{2} - 2\\rho\\,u\\,w\\right)\\right).\n$$\nComplete the square in $u$:\n$$\nu^{2} - 2\\rho\\,u\\,w \\;=\\; (u - \\rho\\,w)^{2} - \\rho^{2}w^{2}.\n$$\nHence,\n$$\nf_{U\\mid W}(u\\mid w) \\;\\propto\\; \\exp\\!\\left(-\\frac{1}{2(1-\\rho^{2})}(u - \\rho\\,w)^{2}\\right),\n$$\nwhich identifies $U\\mid W=w$ as normally distributed with mean $\\rho\\,w$ and variance $1-\\rho^{2}$:\n$$\nU \\mid W=w \\;\\sim\\; \\mathcal{N}\\!\\left(\\rho\\,w,\\;1-\\rho^{2}\\right).\n$$\nSubstituting back $w = (y - \\mu_{Y})/\\sigma_{Y}$ and using $X = \\mu_{X} + \\sigma_{X} U$, we obtain\n$$\nE[X \\mid Y=y] \\;=\\; \\mu_{X} + \\sigma_{X}\\,E[U \\mid W = (y - \\mu_{Y})/\\sigma_{Y}] \\;=\\; \\mu_{X} + \\sigma_{X}\\,\\rho\\,\\frac{y - \\mu_{Y}}{\\sigma_{Y}},\n$$\nand\n$$\n\\mathrm{Var}(X \\mid Y=y) \\;=\\; \\sigma_{X}^{2}\\,\\mathrm{Var}\\!\\left(U \\mid W = \\frac{y - \\mu_{Y}}{\\sigma_{Y}}\\right) \\;=\\; \\sigma_{X}^{2}\\,(1 - \\rho^{2}).\n$$\nThese closed-form expressions depend only on $(\\mu_{X},\\mu_{Y},\\sigma_{X},\\sigma_{Y},\\rho)$ and the observed value $y$. They are consistent with the general property that, for a multivariate normal distribution, the conditional distribution is normal with a mean that is affine in the conditioning variable and a variance that does not depend on the specific conditioned value.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\mu_{X} + \\rho\\,\\frac{\\sigma_{X}}{\\sigma_{Y}}\\,(y - \\mu_{Y})  \\sigma_{X}^{2}\\,(1 - \\rho^{2})\\end{pmatrix}}$$"
        }
    ]
}