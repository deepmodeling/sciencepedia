{
    "hands_on_practices": [
        {
            "introduction": "One of the most critical applications of statistical power occurs during the design phase of a study. Before collecting any data, researchers must ensure their experiment has a reasonable chance of detecting a scientifically meaningful effect if one truly exists. This exercise  guides you through the fundamental derivation of a sample size formula, directly connecting the desired power ($1-\\beta$), significance level ($\\alpha$), effect size ($\\delta$), and data variability ($\\sigma$) to the required number of participants.",
            "id": "4992656",
            "problem": "A two-arm Randomized Controlled Trial (RCT) is planned to compare an antihypertensive drug against placebo with equal allocation. The primary endpoint is the change in systolic blood pressure, measured in millimeters of mercury. Investigators will use a two-sided hypothesis test of the difference in population means with type I error rate $\\alpha = 0.05$. Assume individual patient outcomes within each arm are independent and identically distributed, approximately normal, with a common and known standard deviation $\\sigma = 10$ millimeters of mercury. The clinically meaningful difference to detect is a true mean reduction in the treatment arm relative to placebo of $\\delta = 4$ millimeters of mercury. The design target is statistical power $1 - \\beta = 0.80$.\n\nStarting from the definitions of type I error, type II error, and statistical power, and the distribution of the standardized difference in sample means under the null and under a fixed alternative, derive an expression for the required per-arm sample size for a two-sided test with equal allocation. Then compute the minimal integer number of participants per arm required to attain power at least $0.80$ for $\\delta = 4$, $\\sigma = 10$, and $\\alpha = 0.05$. Provide the final answer as a single integer giving the required per-arm sample size.",
            "solution": "Let $\\mu_1$ and $\\mu_2$ be the population mean changes in systolic blood pressure for the placebo and treatment arms, respectively. The sample sizes are equal, $n_1 = n_2 = n$. The outcomes are assumed to be normally distributed with a common known standard deviation $\\sigma$. The hypotheses for a two-sided test are:\n$$ H_0: \\mu_1 - \\mu_2 = 0 $$\n$$ H_a: \\mu_1 - \\mu_2 \\neq 0 $$\n\nThe test statistic is based on the difference in sample means, $\\bar{X}_1 - \\bar{X}_2$. The sampling distribution of this difference is $\\bar{X}_1 - \\bar{X}_2 \\sim N(\\mu_1 - \\mu_2, \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n}) = N(\\mu_1 - \\mu_2, \\frac{2\\sigma^2}{n})$.\nUnder the null hypothesis $H_0$, the standardized test statistic is:\n$$ Z = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{2\\sigma^2}{n}}} $$\nwhich follows a standard normal distribution, $Z \\sim N(0, 1)$.\n\nA Type I error occurs if we reject $H_0$ when it is true. For a two-sided test at a significance level $\\alpha$, we reject $H_0$ if $|Z| > z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the upper $(1-\\alpha/2)$-quantile of the standard normal distribution.\n\nA Type II error occurs if we fail to reject $H_0$ when it is false. Statistical power, $1-\\beta$, is the probability of correctly rejecting $H_0$ when the alternative hypothesis $H_a$ is true. We calculate power for a specific alternative, namely that the true difference is the clinically meaningful difference $\\delta$. So, we assume $\\mu_1 - \\mu_2 = \\delta$.\n\nUnder this alternative, the sampling distribution is $\\bar{X}_1 - \\bar{X}_2 \\sim N(\\delta, \\frac{2\\sigma^2}{n})$. Power is the probability that the observed difference falls in the rejection region. Assuming $\\delta > 0$, the power is dominated by detections in the upper tail. The standard approximation for power leads to the following relationship:\n$$ \\frac{\\delta\\sqrt{n}}{\\sigma\\sqrt{2}} \\approx z_{1-\\alpha/2} + z_{1-\\beta} $$\nwhere $z_{1-\\beta}$ is the upper $(1-\\beta)$-quantile of the standard normal distribution.\nRearranging to solve for the sample size $n$ gives the desired expression for the per-arm sample size:\n$$ n = \\frac{2\\sigma^2(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nNow, we compute the minimal integer sample size for the given parameters:\n- Type I error rate $\\alpha = 0.05$.\n- Statistical power $1 - \\beta = 0.80$, so the Type II error rate is $\\beta = 0.20$.\n- Common standard deviation $\\sigma = 10$ mmHg.\n- Clinically meaningful difference $\\delta = 4$ mmHg.\n\nFirst, we find the required quantiles from the standard normal distribution:\n- For a two-sided test with $\\alpha = 0.05$, we need the upper $1 - \\alpha/2 = 1 - 0.025 = 0.975$ quantile: $z_{0.975} \\approx 1.960$.\n- For a power of $0.80$, we need the upper $1 - \\beta = 0.80$ quantile: $z_{0.80} \\approx 0.842$.\n\nSubstituting these values into the derived formula for $n$:\n$$ n = \\frac{2(10)^2(1.960 + 0.842)^2}{(4)^2} $$\n$$ n = \\frac{2 \\times 100 \\times (2.802)^2}{16} $$\n$$ n = \\frac{200 \\times 7.851204}{16} $$\n$$ n \\approx 98.14 $$\nSince the number of participants must be an integer, and the power must be *at least* $0.80$, we must round the calculated value of $n$ up to the next whole number. Choosing $n=98$ would result in a power slightly below the target of $0.80$. Therefore, the minimal integer number of participants required per arm is $99$.",
            "answer": "$$ \\boxed{99} $$"
        },
        {
            "introduction": "While designing powerful studies is the ideal, researchers often face constraints that lead to small sample sizes, particularly in exploratory research. Interpreting the results of such studies requires careful statistical reasoning to avoid drawing incorrect conclusions. This problem  presents a common scenario—a non-significant result from an experiment with very few replicates—challenging you to correctly apply the concepts of statistical power and Type II error, and to avoid the logical fallacy of equating non-significance with the proven absence of an effect.",
            "id": "2410288",
            "problem": "A computational biology team uses Ribonucleic Acid sequencing (RNA-seq) to test whether a transcription factor knockdown changes the mean normalized expression of a target gene relative to an untreated control. They collect biological replicates from two independent groups (knockdown and control), with $n_{\\text{KD}}=4$ and $n_{\\text{CTRL}}=4$ replicates, respectively. They perform a pre-specified two-sided test comparing group means at significance level $\\alpha=0.05$ and obtain a $p$-value of $p=0.12$. Based on this non-significant result, a team member writes in the draft: “there is no effect.”\n\nWhich option both correctly states the null and alternative hypotheses at the population level for this analysis and gives the most accurate reason why the team cannot conclude “there is no effect”?\n\nA. $H_0$: the difference in population mean expression is $0$; $H_A$: the difference in population mean expression is $\\neq 0$. Because $p=0.12 > \\alpha=0.05$, the test fails to reject $H_0$. With small $n$, statistical power is low (higher probability of Type II error), so a non-significant result is not evidence that the true effect is exactly $0$.\n\nB. $H_0$: there is a true effect; $H_A$: there is no true effect. Because $p=0.12 > \\alpha=0.05$, we accept $H_0$ and conclude there is no effect.\n\nC. $H_0$: the sample means are equal; $H_A$: the sample means differ. Because the sample size is small, the Type I error rate is inflated, so we cannot trust the test and should assume there is no effect.\n\nD. $H_0$: the difference in population medians is $0$; $H_A$: the difference in population medians is $>0$. Given a prior expectation of up-regulation, a two-sided test proving non-significance implies no effect.\n\nE. $H_0$: the difference in population mean expression is $0$; $H_A$: the difference in population mean expression is $\\neq 0$. Since $p=0.12$, there is a $12\\%$ chance the null hypothesis is true; this large probability indicates there is likely no effect.",
            "solution": "The problem asks for the correct formulation of hypotheses and the correct interpretation of a non-significant result from a study with a very small sample size.\n\n1.  **Formulating Hypotheses:** Hypothesis tests make inferences about population parameters, not sample statistics. The team is comparing the *mean* expression levels between the knockdown and control populations. The null hypothesis ($H_0$) is the statement of no effect, and the alternative hypothesis ($H_A$) is the statement of an effect. For a two-sided test:\n    *   $H_0$: The difference in population mean expression is zero ($\\mu_{\\text{KD}} - \\mu_{\\text{CTRL}} = 0$).\n    *   $H_A$: The difference in population mean expression is not zero ($\\mu_{\\text{KD}} - \\mu_{\\text{CTRL}} \\neq 0$).\n\n2.  **Interpreting the Result:** The study found a $p$-value of $0.12$. The significance level was set at $\\alpha=0.05$.\n    *   Since $p=0.12 > \\alpha=0.05$, the result is not statistically significant.\n    *   The correct statistical conclusion is to \"fail to reject the null hypothesis.\"\n    *   It is a logical fallacy to \"accept the null hypothesis\" or to conclude that the null hypothesis is true. Absence of evidence is not evidence of absence.\n\n3.  **The Role of Power:** Statistical power is the probability of detecting a true effect. Power is heavily dependent on sample size. With extremely small sample sizes ($n=4$ in each group), this study has very low power. This means it has a high probability of committing a Type II error—failing to detect a real effect if one exists. A non-significant result in a low-power study is inconclusive: it could be that there is no effect, or it could be that there is a real effect that the study was too small to detect.\n\n**Evaluating the Options:**\n\n*   **A:** This option correctly states the hypotheses for a two-sided test on population means. It correctly interprets the result as a \"failure to reject $H_0$\" because $p > \\alpha$. Most importantly, it correctly identifies that low power due to small sample size ($n$) means a non-significant result is not proof of \"no effect.\" This is the most accurate and complete explanation.\n*   **B:** Incorrectly reverses the null and alternative hypotheses. The null hypothesis should be the statement of \"no effect.\" It also makes the logical error of \"accepting $H_0$.\"\n*   **C:** Incorrectly states the hypotheses in terms of *sample* means. Hypothesis tests are about populations. It also incorrectly claims that small sample size inflates the Type I error rate; in fact, it inflates the Type II error rate (i.e., reduces power).\n*   **D:** Incorrectly formulates the hypotheses about medians (not means) and as a one-sided test (not two-sided). It also incorrectly claims that non-significance \"proves\" no effect.\n*   **E:** States the hypotheses correctly, but makes a critical error in interpreting the $p$-value. A $p$-value of $0.12$ is *not* the probability that the null hypothesis is true. This is a common and serious misinterpretation.\n\nTherefore, option A provides the only correct and thorough reasoning.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Scientific truth is rarely established by a single study; it is built through a cumulative process of discovery and independent replication. This advanced case study  delves into a realistic scenario from genomics where a highly significant finding fails to replicate in a different population. Analyzing this situation requires you to synthesize your understanding of Type I and Type II errors, the multiple factors that influence statistical power, and the real-world possibility of true biological differences between study groups.",
            "id": "2438780",
            "problem": "A Genome-Wide Association Study (GWAS) of a binary disease in European-ancestry participants discovers a single-nucleotide polymorphism (SNP) with per-allele association that is genome-wide significant. The discovery cohort has $n_{\\text{case},1} = 30{,}000$ cases and $n_{\\text{control},1} = 70{,}000$ controls. The index SNP has European risk-allele frequency $p_{\\text{EUR}} = 0.30$, estimated odds ratio $\\widehat{\\mathrm{OR}}_1 = 1.08$, and a two-sided $p$-value $p_1 = 2\\times 10^{-9}$ (which is below the conventional threshold $5\\times 10^{-8}$ for genome-wide significance). An independent replication is attempted in East Asian ancestry with $n_{\\text{case},2} = 5{,}000$ and $n_{\\text{control},2} = 5{,}000$, testing the same index SNP or its best available proxy, which has squared correlation $r^2 = 0.80$ with the index SNP in Europeans. In the East Asian sample, the risk-allele frequency is $p_{\\text{EAS}} = 0.10$. The replication uses a two-sided threshold $p_2 < 0.05$ for this single pre-specified SNP and reports $\\widehat{\\mathrm{OR}}_2 = 1.05$ with $p_2 = 0.12$ (not significant). Both studies perform standard quality control and adjust for principal components of ancestry.\n\nUsing only fundamental definitions about statistical errors and widely accepted properties of statistical power in association testing, reason about why a genome-wide significant discovery can fail to replicate, and how to interpret such a failure in terms of Type I and Type II errors. Select all statements that are correct in this scenario.\n\nA. The failure to replicate implies the discovery was a Type I error, because a true association would have yielded $p_2 < 0.05$ in the replication.\n\nB. The replication may have committed a Type II error: smaller $n_{\\text{case},2}$ and $n_{\\text{control},2}$, lower $p_{\\text{EAS}}$, and imperfect linkage disequilibrium between the tested proxy and the causal variant reduce power, so non-significance can occur even if the association is real.\n\nC. If the causal effect is present in Europeans but absent or materially weaker in East Asians, then the null hypothesis may be true in the replication cohort; in that case, non-significance does not constitute a Type II error with respect to the replication’s hypothesis test.\n\nD. Achieving $p_1 < 5\\times 10^{-8}$ in discovery eliminates the possibility that the discovery was a Type I error.\n\nE. Despite $p_1 = 2\\times 10^{-9}$, the discovery could still be a Type I error; the genome-wide threshold controls but does not abolish the probability of false positives across many tested variants, and residual confounding could inflate test statistics.\n\nF. If, instead of $p_2 < 0.05$, the replication required $p_2 < 5\\times 10^{-8}$ for this single pre-specified SNP, the probability of a Type II error would increase substantially relative to using $p_2 < 0.05$.",
            "solution": "This problem requires an analysis of why a statistically significant finding in a discovery study might not replicate in a follow-up study. We must evaluate each statement based on the principles of hypothesis testing, statistical power, and common issues in genetic association studies.\n\n**Analysis of Statements:**\n\n**A: Incorrect.** A failure to replicate does not definitively prove the original finding was a Type I error (a false positive). The replication study might simply be underpowered to detect a true, but small, effect. Statistical tests are probabilistic; a true association does not guarantee a significant p-value in every study.\n\n**B: Correct.** This statement accurately lists several key reasons why the replication study has substantially lower statistical power than the discovery study:\n1.  **Smaller Sample Size:** The replication has 10,000 participants, while the discovery had 100,000.\n2.  **Lower Allele Frequency:** The risk allele is much rarer in the replication cohort ($p_{\\text{EAS}} = 0.10$) than in the discovery cohort ($p_{\\text{EUR}} = 0.30$). For a fixed sample size and effect size, power is lower for rarer variants. The effective sample size, proportional to $n \\cdot p(1-p)$, is dramatically smaller.\n3.  **Imperfect Proxy:** The replication used a proxy SNP with $r^2=0.80$. This means that even if the original SNP were causal, the power to detect its effect via the proxy is reduced by approximately 20%.\nBecause of this severely reduced power, the replication study could easily fail to detect a real association, which would be a Type II error.\n\n**C: Correct.** This statement raises the valid possibility of genuine biological heterogeneity. The genetic effect of a SNP can differ across ancestry groups due to different genetic backgrounds or gene-environment interactions. If the SNP has no effect in the East Asian population (i.e., the true odds ratio is 1), then the null hypothesis is true for the replication study. In this case, failing to find a significant result ($p_2=0.12$) is the correct outcome. A Type II error can only occur when the null hypothesis is false. Thus, if $H_0$ is true in the replication cohort, the non-significant result is not a Type II error.\n\n**D: Incorrect.** A p-value, no matter how small, does not \"eliminate\" the possibility of a Type I error. The genome-wide significance threshold ($5 \\times 10^{-8}$) is designed to make Type I errors rare across the millions of tests in a GWAS, but it does not make them impossible for any single test.\n\n**E: Correct.** This is the proper interpretation. A p-value of $2 \\times 10^{-9}$ makes a Type I error very unlikely, but not impossible. The stringent threshold reduces but does not eliminate the chance of a false positive. Furthermore, the statement correctly points out another potential cause of false positives: residual confounding (e.g., from subtle population stratification not fully captured by adjustments) that can inflate test statistics and create spurious associations.\n\n**F: Correct.** There is an inherent trade-off between the Type I error rate ($\\alpha$) and the Type II error rate ($\\beta$). If we make our criterion for significance more stringent (i.e., decrease $\\alpha$), we make it harder to reject the null hypothesis. This directly decreases the power ($1-\\beta$) of the test and therefore increases the probability of a Type II error ($\\beta$). Changing the significance threshold from a conventional $0.05$ to a genome-wide $5 \\times 10^{-8}$ would dramatically increase the burden of proof, thereby substantially increasing the chance of missing a true effect.\n\nIn summary, statements B, C, E, and F are all correct applications of statistical principles to this scenario.",
            "answer": "$$\\boxed{BCEF}$$"
        }
    ]
}