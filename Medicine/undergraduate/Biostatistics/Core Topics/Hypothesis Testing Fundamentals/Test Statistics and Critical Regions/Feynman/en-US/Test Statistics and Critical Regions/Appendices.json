{
    "hands_on_practices": [
        {
            "introduction": "Understanding how a hypothesis test works begins with deriving its components from scratch. This first practice challenges you to construct the rejection rule for a one-sided Z-test, a cornerstone of statistical inference, using only fundamental distributional facts. By deriving the critical value and the test's power function, you will gain a deep, foundational understanding of how we control error rates and evaluate test performance .",
            "id": "4956821",
            "problem": "A biostatistics research team is designing a one-sided hypothesis test for the mean concentration of a biomarker measured by a calibrated assay. For a single study arm, the observations are modeled as independent and identically distributed random variables $X_1,\\dots,X_n$ from a normal distribution with mean $\\mu$ and known variance $\\sigma^2$, written $X_i \\sim N(\\mu,\\sigma^2)$. The scientific question is whether the true mean exceeds a clinically established reference $\\mu_0$. The team adopts the hypotheses $H_0:\\mu=\\mu_0$ versus $H_1:\\mu>\\mu_0$ and requires a test that has exact size $\\alpha$, where $0<\\alpha<1$ is prespecified.\n\nUsing only the following foundational facts:\n- If $X_1,\\dots,X_n$ are independent and identically distributed as $N(\\mu,\\sigma^2)$ with known $\\sigma^2$, then the sample mean $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n X_i$ is normally distributed with mean $\\mu$ and variance $\\sigma^2/n$.\n- For any real-valued constant $a$, if $Y\\sim N(\\mu_Y,\\sigma_Y^2)$, then $(Y-\\mu_Y)/\\sigma_Y$ has the standard normal distribution with cumulative distribution function $\\Phi$ and corresponding quantile function $\\Phi^{-1}$.\n\nDerive, from first principles, a rejection rule of exact size $\\alpha$ for testing $H_0:\\mu=\\mu_0$ versus $H_1:\\mu>\\mu_0$ expressed as a threshold on $\\overline{X}$, and then derive the power function $\\pi(\\mu)$, defined as $\\pi(\\mu)=\\mathbb{P}_{\\mu}(\\text{reject }H_0)$ as a function of the true mean $\\mu$. Express your final results in terms of $n$, $\\sigma$, $\\mu_0$, $\\alpha$, $\\Phi$, and $\\Phi^{-1}$. Do not provide any numerical approximations.\n\nYour final answer must be a single composite expression that simultaneously reports the critical value on $\\overline{X}$ and the power function $\\pi(\\mu)$, written as a row matrix. No rounding is required and no units apply.",
            "solution": "The problem as stated is a standard, well-posed problem in introductory mathematical statistics and biostatistics. It is scientifically grounded, internally consistent, and contains all necessary information to derive the requested quantities. The problem is therefore deemed valid and a full solution is warranted.\n\nThe goal is to first derive a rejection rule of exact size $\\alpha$ for the one-sided hypothesis test $H_0: \\mu = \\mu_0$ versus $H_1: \\mu > \\mu_0$, and then to derive the power function $\\pi(\\mu)$.\n\n**Part 1: Derivation of the Rejection Rule**\n\nThe size of a test, denoted by $\\alpha$, is the probability of a Type I error. This is the probability of rejecting the null hypothesis $H_0$ when it is, in fact, true. The problem requires this size to be exactly $\\alpha$.\nA rejection rule for the one-sided alternative $H_1: \\mu > \\mu_0$ will naturally be of the form \"reject $H_0$ if the sample mean $\\overline{X}$ is large\". We formalize this by defining a rejection region $\\mathcal{R} = \\{\\overline{X} > c\\}$, where $c$ is a critical value we must determine.\n\nThe size constraint is mathematically expressed as:\n$$\n\\mathbb{P}(\\text{reject } H_0 \\mid H_0 \\text{ is true}) = \\alpha\n$$\nSubstituting our rejection rule and the condition for $H_0$ being true ($\\mu = \\mu_0$), we get:\n$$\n\\mathbb{P}_{\\mu_0}(\\overline{X} > c) = \\alpha\n$$\nThe subscript $\\mu_0$ indicates that the probability is calculated under the distribution where the true mean is $\\mu_0$.\n\nAccording to the provided foundational facts, when the true mean is $\\mu_0$, the sample mean $\\overline{X}$ is normally distributed with mean $\\mu_0$ and variance $\\sigma^2/n$. That is, $\\overline{X} \\sim N(\\mu_0, \\sigma^2/n)$.\n\nTo evaluate the probability and solve for $c$, we standardize the random variable $\\overline{X}$. Under $H_0$, the standardized statistic is:\n$$\nZ = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}}\n$$\nThis statistic $Z$ follows the standard normal distribution, $N(0,1)$. We can transform the inequality $\\overline{X} > c$ into an equivalent inequality for $Z$:\n$$\n\\overline{X} > c \\quad \\iff \\quad \\overline{X} - \\mu_0 > c - \\mu_0 \\quad \\iff \\quad \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}} > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\n$$\nThus, the size constraint becomes:\n$$\n\\mathbb{P}\\left(Z > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\\right) = \\alpha\n$$\nFor a standard normal variable $Z$ with cumulative distribution function (CDF) $\\Phi$, we know that $\\mathbb{P}(Z > k) = 1 - \\mathbb{P}(Z \\le k) = 1 - \\Phi(k)$. The value $k$ for which this probability equals $\\alpha$ is a quantile of the standard normal distribution. Specifically, it is the upper $\\alpha$-quantile, which we can denote as $z_{\\alpha}$. This value is defined by the property $\\mathbb{P}(Z \\le z_{\\alpha}) = 1-\\alpha$. Using the provided quantile function notation $\\Phi^{-1}$, this is $z_{\\alpha} = \\Phi^{-1}(1-\\alpha)$.\n\nBy equating the argument of the probability with this quantile, we get:\n$$\n\\frac{c - \\mu_0}{\\sigma/\\sqrt{n}} = \\Phi^{-1}(1-\\alpha)\n$$\nWe now solve for the critical value $c$, which is the threshold on $\\overline{X}$:\n$$\nc = \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\n$$\nThis is the first part of the answer. The rejection rule is to reject $H_0$ if a calculated sample mean $\\overline{X}$ is greater than this value $c$.\n\n**Part 2: Derivation of the Power Function**\n\nThe power of a test, $\\pi(\\mu)$, is the probability of correctly rejecting the null hypothesis when the true mean is $\\mu$, where $\\mu$ is a value specified by the alternative hypothesis (i.e., $\\mu > \\mu_0$). The power function is defined for any value of $\\mu$:\n$$\n\\pi(\\mu) = \\mathbb{P}_{\\mu}(\\text{reject } H_0)\n$$\nUsing the rejection rule derived above, this becomes:\n$$\n\\pi(\\mu) = \\mathbb{P}_{\\mu}\\left(\\overline{X} > \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\\right)\n$$\nTo evaluate this probability, we must consider the distribution of $\\overline{X}$ when the true mean is $\\mu$. From the given facts, this distribution is $\\overline{X} \\sim N(\\mu, \\sigma^2/n)$. We standardize $\\overline{X}$ with respect to this true distribution:\n$$\nZ' = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n$$\nWe now re-express the inequality in our power calculation in terms of this new standard normal variable $Z'$.\n$$\n\\overline{X} > \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\n$$\nSubtract $\\mu$ from both sides and divide by the standard error $\\sigma/\\sqrt{n}$:\n$$\n\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} > \\frac{\\left(\\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)\\right) - \\mu}{\\sigma/\\sqrt{n}}\n$$\nThe left side is $Z'$. We simplify the right side:\n$$\nZ' > \\frac{\\mu_0 - \\mu}{\\sigma/\\sqrt{n}} + \\frac{\\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)}{\\sigma/\\sqrt{n}}\n$$\n$$\nZ' > \\frac{\\mu_0 - \\mu}{\\sigma/\\sqrt{n}} + \\Phi^{-1}(1-\\alpha)\n$$\nThe power function is the probability of this event occurring:\n$$\n\\pi(\\mu) = \\mathbb{P}\\left(Z' > \\Phi^{-1}(1-\\alpha) + \\frac{\\mu_0 - \\mu}{\\sigma/\\sqrt{n}}\\right)\n$$\nThis can be rewritten as:\n$$\n\\pi(\\mu) = \\mathbb{P}\\left(Z' > \\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)\n$$\nUsing the property that for a standard normal variable $Z'$, $\\mathbb{P}(Z' > k) = 1 - \\Phi(k)$, we obtain the final expression for the power function:\n$$\n\\pi(\\mu) = 1 - \\Phi\\left(\\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)\n$$\nThis is the second part of the answer, expressed solely in terms of the required parameters.\n\nThe final answer requires a composite expression for the critical value on $\\overline{X}$ and the power function $\\pi(\\mu)$.\nThe critical value is $c = \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha)$.\nThe power function is $\\pi(\\mu) = 1 - \\Phi\\left(\\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mu_0 + \\frac{\\sigma}{\\sqrt{n}} \\Phi^{-1}(1-\\alpha) & 1 - \\Phi\\left(\\Phi^{-1}(1-\\alpha) - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}\\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Theoretical knowledge of test statistics and critical regions has profound practical implications for designing effective studies. This exercise bridges that gap by asking you to determine the minimum sample size needed to achieve a desired level of statistical power for a clinically meaningful effect. This type of calculation is a critical step in planning clinical trials and other research, ensuring that a study is large enough to detect a true effect if one exists .",
            "id": "4956820",
            "problem": "A clinical trial plans to evaluate whether a new intervention increases the mean concentration of a circulating biomarker relative to a historical reference mean. Assume that observations $X_{1}, X_{2}, \\dots, X_{n}$ are independent and identically distributed as normal with mean $\\mu$ and known variance $\\sigma^{2}$. The null hypothesis is $H_{0}: \\mu \\le \\mu_{0}$ against the one-sided alternative $H_{1}: \\mu > \\mu_{0}$. The test will reject $H_{0}$ if the sample mean $\\bar{X}$ exceeds a cutoff $c$. The design requirements are a type I error rate $\\alpha = 0.025$ and power $0.9$ at the clinically meaningful effect $\\mu = \\mu_{0} + \\delta$.\n\nUse the following scientifically plausible parameters: reference mean $\\mu_{0} = 10$ mg/dL, clinically meaningful increase $\\delta = 3$ mg/dL, and known standard deviation $\\sigma = 8$ mg/dL. Determine the minimal integer sample size $n$ such that the test with rejection region $\\bar{X} > c$ has size $\\alpha$ and power $0.9$ at $\\mu = \\mu_{0} + \\delta$. Then define the corresponding critical cutoff $c$ for the sample mean. Express the cutoff in mg/dL and round the cutoff to four significant figures.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of biostatistics, specifically hypothesis testing and power analysis for a one-sided Z-test. The problem is well-posed, objective, and provides a complete and consistent set of parameters, allowing for a unique solution.\n\nThe problem requires the determination of the minimum integer sample size, $n$, and the corresponding critical cutoff value, $c$, for a hypothesis test concerning a population mean.\n\nThe observations $X_{1}, X_{2}, \\dots, X_{n}$ are independent and identically distributed (i.i.d.) from a normal distribution with mean $\\mu$ and known variance $\\sigma^{2}$. The sample mean, $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, is therefore also normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^{2}}{n}$. We write this as $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^{2}}{n})$.\n\nThe hypotheses are:\nNull hypothesis $H_{0}: \\mu \\le \\mu_{0}$\nAlternative hypothesis $H_{1}: \\mu > \\mu_{0}$\n\nThe test rejects $H_{0}$ if $\\bar{X} > c$. The given parameters are:\nReference mean: $\\mu_{0} = 10 \\text{ mg/dL}$\nStandard deviation: $\\sigma = 8 \\text{ mg/dL}$\nClinically meaningful increase: $\\delta = 3 \\text{ mg/dL}$, which defines the specific alternative mean $\\mu_1 = \\mu_{0} + \\delta = 10 + 3 = 13 \\text{ mg/dL}$.\nType I error rate (size): $\\alpha = 0.025$\nPower at $\\mu_1$: $1-\\beta = 0.9$, which implies a Type II error rate of $\\beta = 0.1$.\n\nFirst, we formulate the constraint on the Type I error rate. The size of the test, $\\alpha$, is the maximum probability of rejecting $H_0$ when $H_0$ is true. This maximum occurs at the boundary of the null hypothesis region, i.e., when $\\mu = \\mu_{0}$.\n$$ \\alpha = P(\\text{Reject } H_0 \\mid \\mu = \\mu_0) = P(\\bar{X} > c \\mid \\mu = \\mu_0) $$\nTo evaluate this probability, we standardize the sample mean $\\bar{X}$. Under the condition $\\mu = \\mu_0$, the statistic $Z = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}$ follows a standard normal distribution, $N(0,1)$.\n$$ \\alpha = P\\left(\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}} > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\\right) = P\\left(Z > \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}}\\right) $$\nLet $z_{\\alpha}$ be the upper $\\alpha$-quantile of the standard normal distribution, defined by $P(Z > z_{\\alpha}) = \\alpha$. We can then write:\n$$ \\frac{c - \\mu_0}{\\sigma/\\sqrt{n}} = z_{\\alpha} $$\nThis gives our first equation relating $c$ and $n$:\n$$ c = \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} $$\n\nSecond, we formulate the constraint on the power of the test. The power is the probability of rejecting $H_0$ when the alternative hypothesis is true. We are given that the power must be $1-\\beta = 0.9$ at the specific alternative mean $\\mu_1 = \\mu_0 + \\delta$.\n$$ 1 - \\beta = P(\\text{Reject } H_0 \\mid \\mu = \\mu_1) = P(\\bar{X} > c \\mid \\mu = \\mu_1) $$\nUnder the condition $\\mu = \\mu_1$, the statistic $\\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}}$ follows a standard normal distribution, $N(0,1)$.\n$$ 1 - \\beta = P\\left(\\frac{\\bar{X} - \\mu_1}{\\sigma/\\sqrt{n}} > \\frac{c - \\mu_1}{\\sigma/\\sqrt{n}}\\right) = P\\left(Z > \\frac{c - \\mu_1}{\\sigma/\\sqrt{n}}\\right) $$\nLet $z_{\\beta}$ be the upper $\\beta$-quantile of the standard normal distribution, defined by $P(Z > z_{\\beta}) = \\beta$. The probability $P(Z > x) = 1-\\beta$ implies $x = -z_{\\beta}$. Thus, we have:\n$$ \\frac{c - \\mu_1}{\\sigma/\\sqrt{n}} = -z_{\\beta} $$\nThis gives our second equation relating $c$ and $n$:\n$$ c = \\mu_1 - z_{\\beta} \\frac{\\sigma}{\\sqrt{n}} $$\n\nWe now have a system of two equations for the two unknowns, $n$ and $c$. We can solve for $n$ by equating the two expressions for $c$:\n$$ \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} = \\mu_1 - z_{\\beta} \\frac{\\sigma}{\\sqrt{n}} $$\nRearranging the terms to solve for $\\sqrt{n}$:\n$$ (z_{\\alpha} + z_{\\beta}) \\frac{\\sigma}{\\sqrt{n}} = \\mu_1 - \\mu_0 $$\nRecognizing that $\\mu_1 - \\mu_0 = \\delta$:\n$$ \\sqrt{n} = \\frac{(z_{\\alpha} + z_{\\beta})\\sigma}{\\delta} $$\nSquaring both sides gives the formula for the required sample size $n$:\n$$ n = \\left( \\frac{(z_{\\alpha} + z_{\\beta})\\sigma}{\\delta} \\right)^2 $$\n\nWe now substitute the given numerical values.\nFor $\\alpha=0.025$, the corresponding upper quantile is $z_{0.025} \\approx 1.95996$.\nFor a power of $0.9$, $\\beta=0.1$, and the corresponding upper quantile is $z_{0.1} \\approx 1.28155$.\nThe remaining parameters are $\\sigma=8$ and $\\delta=3$.\n$$ n = \\left( \\frac{(1.95996 + 1.28155) \\times 8}{3} \\right)^2 $$\n$$ n = \\left( \\frac{3.24151 \\times 8}{3} \\right)^2 = \\left( \\frac{25.93208}{3} \\right)^2 \\approx (8.644027)^2 \\approx 74.7192 $$\nSince the sample size $n$ must be an integer and the power requirement is a minimum, we must round the calculated value of $n$ up to the next whole number to ensure the power is at least $0.9$.\nTherefore, the minimal integer sample size is $n=75$.\n\nWith $n=75$, we can now determine the critical cutoff $c$. The cutoff is defined by the Type I error rate constraint. Using the first equation derived:\n$$ c = \\mu_0 + z_{\\alpha} \\frac{\\sigma}{\\sqrt{n}} $$\nSubstituting the values $\\mu_0=10$, $z_{0.025} \\approx 1.95996$, $\\sigma=8$, and $n=75$:\n$$ c = 10 + 1.95996 \\times \\frac{8}{\\sqrt{75}} $$\n$$ c = 10 + 1.95996 \\times \\frac{8}{5\\sqrt{3}} \\approx 10 + 1.95996 \\times 0.9237604 $$\n$$ c \\approx 10 + 1.81055 $$\n$$ c \\approx 11.81055 $$\nThe problem requires rounding the cutoff $c$ to four significant figures.\n$$ c \\approx 11.81 \\text{ mg/dL} $$\n\nThe results are the sample size $n=75$ and the critical cutoff $c=11.81$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 75 & 11.81 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While tests for continuous data allow for precise control of the Type I error rate, discrete data like counts present unique challenges. This practice explores how to construct a most powerful test for a binomial proportion, demonstrating why it may be impossible to achieve a target size $\\alpha$ exactly without a randomized decision rule. By working through this problem, you will see how the Neyman-Pearson framework is adapted for discrete distributions, a crucial concept in fields like epidemiology and genomics .",
            "id": "4956779",
            "problem": "A clinical trial monitors an immunological response by counting the number of responders in a small cohort. Suppose the count of responders $X$ in a cohort of size $n=5$ is modeled as $X \\sim \\text{Binomial}(n,p)$ with responder probability $p$. You wish to test the null hypothesis of no improvement, the null hypothesis $H_0: p=p_0$, against the one-sided alternative hypothesis $H_1: p>p_0$. Let the baseline probability be $p_0=\\frac{1}{2}$, and target a size (Type I error probability) $\\alpha=\\frac{1}{10}$. Using the Neymanâ€“Pearson (NP) framework and the definition of a most powerful test at size $\\alpha$, construct the critical region that rejects $H_0$ at size exactly $\\alpha$. Work from first principles: start with the likelihood ratio ordering implied by the NP lemma for simple hypotheses and extend the argument to the one-sided composite alternative by monotone likelihood ratio reasoning. Because the sample space is discrete for small $n$, you must justify whether randomization is needed at the boundary to achieve exact size, and if so, compute the boundary and the exact randomization probability.\n\nConcretely, determine the smallest integer cutoff $c$ such that the NP most powerful test rejects for $X \\geq c$ and, if necessary, randomizes at $X=c-1$ to achieve exact size $\\alpha$. Report the cutoff $c$ and the randomization probability $\\gamma$ used at $X=c-1$. Express your final answer as a row matrix $\\begin{pmatrix} c & \\gamma \\end{pmatrix}$, with $\\gamma$ given as an exact reduced fraction. Do not round.",
            "solution": "The problem requires the construction of a most powerful test of size $\\alpha = \\frac{1}{10}$ for the hypothesis $H_0: p = p_0 = \\frac{1}{2}$ against the composite alternative $H_1: p > \\frac{1}{2}$. The data consists of a single observation $X$ from a Binomial distribution, $X \\sim \\text{Binomial}(n, p)$, with $n=5$.\n\nThe Neyman-Pearson (NP) framework is the foundation for this analysis. The NP lemma provides the most powerful test for simple hypotheses. For a simple null $H_0: p=p_0$ versus a simple alternative $H_1': p=p_1$ where $p_1 > p_0$, the most powerful test rejects $H_0$ for large values of the likelihood ratio, $\\frac{L(p_1|x)}{L(p_0|x)}$. The likelihood function for a binomial observation $X=x$ is given by its probability mass function (PMF):\n$$ L(p|x) = P(X=x; p) = \\binom{n}{x} p^x (1-p)^{n-x} $$\nThe likelihood ratio is:\n$$ \\frac{L(p_1|x)}{L(p_0|x)} = \\frac{\\binom{n}{x} p_1^x (1-p_1)^{n-x}}{\\binom{n}{x} p_0^x (1-p_0)^{n-x}} = \\left(\\frac{p_1}{p_0}\\right)^x \\left(\\frac{1-p_1}{1-p_0}\\right)^{n-x} = \\left(\\frac{1-p_1}{1-p_0}\\right)^n \\left[ \\frac{p_1(1-p_0)}{p_0(1-p_1)} \\right]^x $$\nFor any $p_1 > p_0$, the term $\\frac{p_1(1-p_0)}{p_0(1-p_1)}$ is greater than $1$. Therefore, the likelihood ratio is a monotonically increasing function of the statistic $X$. This property is known as the monotone likelihood ratio (MLR) in the statistic $X$. By the Karlin-Rubin theorem, which extends the NP lemma to certain composite hypotheses, the test that rejects $H_0$ for large values of $X$ is the Uniformly Most Powerful (UMP) test for $H_0: p \\le p_0$ versus $H_1: p > p_0$. This also applies to our specific problem of $H_0: p=p_0$ vs $H_1: p>p_0$.\n\nThe critical region of the test is therefore of the form $\\{x : x \\ge c\\}$ or, to achieve an exact size $\\alpha$, a randomized test may be required. The test is defined by a function $\\phi(x)$, the probability of rejecting $H_0$ given $X=x$. Based on the MLR property, the UMP test has the form:\n$$ \\phi(x) = \\begin{cases} 1 & \\text{if } x > k \\\\ \\gamma & \\text{if } x = k \\\\ 0 & \\text{if } x < k \\end{cases} $$\nfor some integer cutoff $k$ and randomization probability $\\gamma \\in [0, 1]$. The problem statement uses a slightly different but equivalent notation where the critical region is defined by an integer cutoff $c$ and a randomization probability $\\gamma$ at the boundary point $c-1$. The test rejects $H_0$ if $X \\ge c$, and rejects with probability $\\gamma$ if $X = c-1$.\n\nThe size of the test is the probability of a Type I error, which must be exactly $\\alpha$.\n$$ \\alpha = E_{p_0}[\\phi(X)] = \\sum_{x=0}^{n} \\phi(x) P(X=x | p=p_0) $$\nUsing the problem's specified structure, this becomes:\n$$ \\alpha = P_{p_0}(X \\ge c) + \\gamma \\cdot P_{p_0}(X = c-1) $$\nWe are given $n=5$, $p_0 = \\frac{1}{2}$, and $\\alpha = \\frac{1}{10}$. Under the null hypothesis, $X \\sim \\text{Binomial}(5, \\frac{1}{2})$. The PMF is:\n$$ P(X=k | p_0=\\frac{1}{2}) = \\binom{5}{k} \\left(\\frac{1}{2}\\right)^k \\left(\\frac{1}{2}\\right)^{5-k} = \\frac{1}{32} \\binom{5}{k} $$\nThe probabilities for each possible value of $X$ are:\n$P(X=0) = \\binom{5}{0}/32 = 1/32$\n$P(X=1) = \\binom{5}{1}/32 = 5/32$\n$P(X=2) = \\binom{5}{2}/32 = 10/32$\n$P(X=3) = \\binom{5}{3}/32 = 10/32$\n$P(X=4) = \\binom{5}{4}/32 = 5/32$\n$P(X=5) = \\binom{5}{5}/32 = 1/32$\n\nWe need to find the integer $c$ and the probability $\\gamma$. The cutoff $c$ is the smallest integer such that $P_{p_0}(X \\ge c) \\le \\alpha$. Let's compute the right-tail cumulative probabilities:\n$P_{p_0}(X \\ge 5) = P(X=5) = \\frac{1}{32}$.\n$P_{p_0}(X \\ge 4) = P(X=4) + P(X=5) = \\frac{5}{32} + \\frac{1}{32} = \\frac{6}{32}$.\n\nThe target size is $\\alpha = \\frac{1}{10} = \\frac{3.2}{32}$.\nWe observe that $P_{p_0}(X \\ge 5) = \\frac{1}{32} < \\frac{3.2}{32}$ and $P_{p_0}(X \\ge 4) = \\frac{6}{32} > \\frac{3.2}{32}$.\nA non-randomized test that rejects for $X \\ge 5$ has size $\\frac{1}{32}$, which is too small. A test that rejects for $X \\ge 4$ has size $\\frac{6}{32}$, which is too large. Therefore, to achieve the exact size $\\alpha=\\frac{1}{10}$, we must use randomization.\n\nThe structure of the test dictates that we reject with certainty for the most extreme outcomes. The cutoff for certain rejection will be $c=5$. We will then randomize at the boundary point $c-1=4$.\nThe size equation is:\n$$ \\alpha = P_{p_0}(X \\ge c) + \\gamma \\cdot P_{p_0}(X = c-1) $$\nSubstituting the values we have determined: $c=5$, $\\alpha=\\frac{1}{10}$.\n$$ \\frac{1}{10} = P_{p_0}(X \\geq 5) + \\gamma \\cdot P_{p_0}(X = 4) $$\nUsing the calculated probabilities:\n$$ \\frac{1}{10} = \\frac{1}{32} + \\gamma \\cdot \\frac{5}{32} $$\nWe now solve for the randomization probability $\\gamma$:\n$$ \\gamma \\cdot \\frac{5}{32} = \\frac{1}{10} - \\frac{1}{32} $$\nTo subtract the fractions, we find a common denominator, which is $160$:\n$$ \\gamma \\cdot \\frac{5}{32} = \\frac{16}{160} - \\frac{5}{160} = \\frac{11}{160} $$\n$$ \\gamma = \\frac{11}{160} \\cdot \\frac{32}{5} $$\nSimplifying the expression:\n$$ \\gamma = \\frac{11 \\cdot 32}{5 \\cdot 32 \\cdot 5} = \\frac{11}{25} $$\nThe value $\\gamma = \\frac{11}{25}$ is between $0$ and $1$, as required for a probability.\n\nThus, the most powerful test of size exactly $\\alpha=\\frac{1}{10}$ is to reject $H_0$ if $X \\ge 5$, and if $X=4$, to reject $H_0$ with probability $\\frac{11}{25}$. The cutoff for certain rejection is $c=5$, and the randomization probability at $X=c-1=4$ is $\\gamma=\\frac{11}{25}$.\n\nThe final answer is composed of the integer cutoff $c$ and the randomization probability $\\gamma$.\n$c=5$\n$\\gamma=\\frac{11}{25}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & \\frac{11}{25} \\end{pmatrix}}\n$$"
        }
    ]
}