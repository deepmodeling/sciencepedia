{
    "hands_on_practices": [
        {
            "introduction": "估计标准误 $\\hat{\\sigma}$ 不仅是衡量模型整体拟合优度的单一指标，更是检验模型基本假设的重要基准。本练习将挑战你像统计学家一样思考，通过将不同数据子集的残差离散程度与整体 $\\hat{\\sigma}$ 进行比较，设计出一种稳健的规则来诊断异方差性（即误差方差非恒定）问题。这个过程将加深你对模型诊断图中视觉模式背后统计原理的理解 。",
            "id": "4953188",
            "problem": "一位研究者使用普通最小二乘法（OLS）拟合了一个线性回归模型，以在一个包含 $n=90$ 名参与者的队列中，将一个连续的生物标志物响应 $y$ 与 $p=3$ 个预测变量（包括一个截距项）关联起来。在用于连续结果的生物统计建模的经典 OLS 假设下，误差 $\\varepsilon_i$ 独立同分布，均值为 $0$，方差为常数 $\\sigma^2$。研究者通过绘制残差 $e_i$ 相对于拟合值 $\\hat{y}_i$ 的图来评估模型的充分性。估计的总体标准误差（残差标准误）定义为 $\\hat{\\sigma}=\\sqrt{\\mathrm{RSS}/(n-p)}$，其中 $\\mathrm{RSS}=\\sum_{i=1}^n e_i^2$ 是残差平方和。为了从残差图中诊断可能的异方差性，分析师将拟合值 $\\hat{y}_i$ 分成三个大小相等（每组 $m=30$）的组：低（$L$）、中（$M$）和高（$H$），并分别计算每组内的样本残差标准差 $s_L$、$s_M$ 和 $s_H$。下列哪个诊断规则通过比较不同拟合值组之间的残差离散程度，最恰当地利用 $\\hat{\\sigma}$ 来检测异方差性，并在大约 $5\\%$ 的显著性水平上用 $\\hat{\\sigma}$ 来量化阈值？\n\nA. 如果高拟合值组的残差离散程度相对于 $\\hat{\\sigma}$ 同时较大，并且低拟合值组的残差离散程度相对于 $\\hat{\\sigma}$ 同时较小，即如果 $s_H>1.21\\,\\hat{\\sigma}$ 且 $s_L<0.80\\,\\hat{\\sigma}$，则标记为异方差；这个联合条件意味着一个大的离散程度比率 $s_H^2/s_L^2\\gtrsim 2.29$。\n\nB. 只要 $s_H>1.05\\,\\hat{\\sigma}$，就标记为异方差，不论其他组的残差离散程度如何。\n\nC. 如果中间组的平均绝对残差超过估计的总体标准误差，即如果 $\\frac{1}{m}\\sum_{i\\in M}|e_i|>\\hat{\\sigma}$，则标记为异方差。\n\nD. 如果高组和低组之间的残差离散程度差异超过估计的总体标准误差的十分之一，即如果 $s_H-s_L>0.10\\,\\hat{\\sigma}$，则标记为异方差。\n\nE. 如果高拟合值组中的最大残差超过估计的总体标准误差的三倍，即如果 $\\max_{i\\in H}|e_i|>3\\,\\hat{\\sigma}$，则标记为异方差。",
            "solution": "问题陈述具有科学依据、提法得当且客观。它描述了回归诊断中的一个标准程序（检查异方差性），并要求基于已建立的统计学原理来评估几种启发式规则。所提供的数据（$n=90$, $p=3$, $m=30$）是充分且一致的。该问题是有效的。\n\n核心任务是在给定选项中确定最适合用于诊断异方差性的规则，其中“适合”意味着符合统计学原理，并校准到大约 $5\\%$ 的显著性水平。原假设 ($H_0$) 是同方差性，即误差方差 $\\sigma^2$ 在所有拟合值 $\\hat{y}_i$ 的水平上是恒定的。备择假设 ($H_a$) 是异方差性，特别是一种方差随拟合值增加而增加的模式，即 $\\sigma_L^2  \\sigma_M^2  \\sigma_H^2$。\n\n比较两个总体方差的标准统计方法是 F 检验，它考察样本方差的比率。在此背景下，最相关的比较是在低拟合值组（$L$）和高拟合值组（$H$）之间进行的。样本方差分别为 $s_L^2$ 和 $s_H^2$。F 统计量将是 $F = s_H^2 / s_L^2$。\n\n每组的样本方差是根据 $m=30$ 个残差计算的。每个估计的自由度大约为 $\\nu = m-1 = 29$。（精确的自由度要稍微复杂一些，因为残差不是完全独立的，但对于一个启发式规则来说，这是一个标准且合理的近似）。在 $H_0$ 下，比率 $F = s_H^2 / s_L^2$ 服从自由度为 $(\\nu_H, \\nu_L) = (29, 29)$ 的 F 分布。\n\n为了在 $5\\%$ 的显著性水平上检验备择假设 $H_a: \\sigma_H^2 > \\sigma_L^2$，我们会使用单尾检验。临界值为 $F_{0.05, 29, 29} \\approx 1.85$。如果我们进行双尾检验（即检验任何差异，$\\sigma_H^2 \\neq \\sigma_L^2$），我们会在 $5\\%/2 = 2.5\\%$ 水平上找到上临界值，即 $F_{0.025, 29, 29} \\approx 2.15$。如果一个诊断规则的拒绝阈值对应于此范围或更大的 F 统计量，则认为该规则是合适的。\n\n现在我将基于这些原则评估每个选项。\n\n**A. 如果高拟合值组的残差离散程度相对于 $\\hat{\\sigma}$ 同时较大，并且低拟合值组的残差离散程度相对于 $\\hat{\\sigma}$ 同时较小，即如果 $s_H1.21\\,\\hat{\\sigma}$ 且 $s_L0.80\\,\\hat{\\sigma}$，则标记为异方差；这个联合条件意味着一个大的离散程度比率 $s_H^2/s_L^2\\gtrsim 2.29$。**\n该规则专门针对常见的异方差性“扇形散开”模式。让我们验证其含义。如果 $s_H  1.21\\,\\hat{\\sigma}$ 且 $s_L  0.80\\,\\hat{\\sigma}$，那么它们的方差之比是 $s_H^2 / s_L^2  (1.21\\,\\hat{\\sigma})^2 / (0.80\\,\\hat{\\sigma})^2 = (1.21/0.80)^2 = (1.5125)^2 \\approx 2.2877$。所述值 2.29 是正确的。\n我们将这个隐含的 F 统计量 2.29 与我们的临界值进行比较。由于 $2.29  F_{0.025, 29, 29} \\approx 2.15$，这个条件表明在 5%（双尾）甚至 2.5%（单尾）水平上，方差存在统计学上的显著差异。对 $s_H$ 和 $s_L$ 相对于总体估计 $\\hat{\\sigma}$ 的联合条件是将检验操作化的明智方法，并确保大的比率是由我们感兴趣的特定模式引起的。该规则符合统计学原理，并与指定的显著性水平一致。\n**结论：正确**\n\n**B. 只要 $s_H1.05\\,\\hat{\\sigma}$，就标记为异方差，不论其他组的残差离散程度如何。**\n这个规则在 $s_H^2  (1.05)^2\\,\\hat{\\sigma}^2 \\approx 1.10\\,\\hat{\\sigma}^2$ 时标记异方差。在原假设下，$s_H^2$ 和 $\\hat{\\sigma}^2$ 都是相同方差 $\\sigma^2$ 的估计量，因此它们的比率预期接近 1。阈值 1.10 非常接近 1。用于比较 $s_H^2$ 与 $\\hat{\\sigma}^2$ 的正确检验统计量是 $s_H^2/\\hat{\\sigma}^2$，如果我们将其分布近似为 $F_{m-1, n-p} = F_{29, 87}$，其 5% 单侧临界值约为 1.56。阈值 1.10 远低于此临界值，这意味着该规则将具有非常高的一类错误率（错误地检测到异方差性的概率）。这不是一个在 5% 显著性水平上的检验。\n**结论：不正确**\n\n**C. 如果中间组的平均绝对残差超过估计的总体标准误差，即如果 $\\frac{1}{m}\\sum_{i\\in M}|e_i|\\hat{\\sigma}$，则标记为异方差。**\n该规则比较了两种不同的离散程度度量：子组的平均绝对偏差（MAD）与整个样本的均方根误差（RMSE 或 $\\hat{\\sigma}$）。对于均值为 0、标准差为 $\\sigma$ 的正态分布，绝对残差的期望值为 $E[|e|]=\\sigma\\sqrt{2/\\pi}\\approx 0.798\\sigma$。因此，在同方差性下，我们期望样本 MAD 大约为 $0.8\\hat{\\sigma}$。因此，条件 $\\frac{1}{m}\\sum_{i\\in M}|e_i|\\hat{\\sigma}$ 是一个极其严格的标准，即使在严重的异方差性下也不太可能满足。这等同于期望 $0.8\\sigma  \\sigma$，而这是错误的。此外，关注中间组对于检测方差的单调趋势是低效的。\n**结论：不正确**\n\n**D. 如果高组和低组之间的残差离散程度差异超过估计的总体标准误差的十分之一，即如果 $s_H-s_L0.10\\,\\hat{\\sigma}$，则标记为异方差。**\n方差的假设检验基于比率（F检验），而不是差异。两个样本标准差之间差异的分布不直观，并且依赖于潜在的方差本身，这并不理想。此外，阈值 $0.10\\,\\hat{\\sigma}$ 在统计上是不合理的。在 $H_0$ 下，差异 $s_H-s_L$ 的标准差可以近似为 $\\sigma\\sqrt{1/(2(m-1)) + 1/(2(m-1))} = \\sigma/\\sqrt{m-1} = \\sigma/\\sqrt{29} \\approx 0.186\\sigma$。$0.10\\hat{\\sigma}$ 的阈值仅距离期望的差异 0 约半个标准误差，这将导致非常高的一类错误率。5% 的显著性水平将需要一个更接近 $1.645 \\times 0.186\\hat{\\sigma} \\approx 0.31\\hat{\\sigma}$ 的阈值。\n**结论：不正确**\n\n**E. 如果高拟合值组中的最大残差超过估计的总体标准误差的三倍，即如果 $\\max_{i\\in H}|e_i|3\\,\\hat{\\sigma}$，则标记为异方差。**\n这个诊断规则是为检测单个离群值而设计的，而不是异方差性。异方差性是一组残差离散程度的系统性变化，而离群值是具有大残差的单个点。一组的方差可以显著增加，而不会产生任何大于 $3\\hat{\\sigma}$ 的单个离群值。相反，即使真实的误差方差是恒定的，单个大的离群值也可以触发此规则。该规则滥用了用于离群值检测的工具来诊断非恒定方差的问题。\n**结论：不正确**\n\n结论：选项 A是唯一一个基于合理的统计学原理（通过比率，类似于F检验）来比较方差的规则，它被指定用于一种常见的异方差模式，并使用一个与大约5%的常规显著性水平相对应的定量阈值。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在模型拟合中，一个常见的陷阱是“过拟合”，这会导致样本内估计标准误 $\\hat{\\sigma}$ 对模型的真实预测能力过于乐观。本编程练习将指导你实现一个非参数自助法（bootstrap）程序，以估计一个更切实际的样本外预测误差。通过对比样本内和样本外误差，你将掌握评估模型泛化能力的关键技能 。",
            "id": "4953185",
            "problem": "您将实现一个完整的、可运行的程序，在一个生物统计预测情境下，量化并对比线性回归的两种变异性概念：样本内残差标准误和通过非参数自助法估计的样本外预测残差标准误。请从基本的线性模型和抽样定义出发，避免使用任何预先给出的目标量的快捷公式。您的实现必须是自包含且可复现的。\n\n考虑带截距的普通最小二乘线性模型，其中响应向量 $y \\in \\mathbb{R}^n$ 与一个包含截距列的设计矩阵 $X_{\\text{tilde}} \\in \\mathbb{R}^{n \\times q}$ 相关，其中 $q = p + 1$ 包括 $p$ 个预测变量和截距。该模型假设 $Y = X_{\\text{tilde}} \\beta + \\varepsilon$，误差项独立，均值为 $0$，方差为常数 $\\sigma^2$。设 $\\hat{\\beta}$ 是通过最小化残差平方和拟合的普通最小二乘估计量，设拟合值为 $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。将样本内残差标准误定义为基于残差自由度的误差方差无偏估计量的平方根。\n\n将样本外预测残差标准误定义为来自相同数据生成机制的新观测值的期望均方预测误差的平方根。由于此预测损失的有限样本分布依赖于未知总体和估计变异性，您必须根据以下程序，使用带袋外评估的成对非参数自助法来估计它。\n\n您必须从第一性原理实现以下算法步骤。\n\n- 样本内残差标准误。\n  1. 对完整样本拟合带截距的普通最小二乘模型。\n  2. 计算残差向量和残差自由度，即 $n - q$，其中 $q$ 是包括截距在内的拟合系数的数量。\n  3. 根据残差构建误差方差的无偏估计量，并取其平方根以获得样本内残差标准误。\n\n- 样本外自助法预测残差标准误（带袋外评估的成对自助法）。\n  1. 对于指定数量的自助法重复次数 $B$，重复以下步骤：\n     a. 通过从 $\\{1,\\dots,n\\}$ 中有放回地抽取 $n$ 个索引来生成一个自助样本。\n     b. 对自助样本拟合带截距的普通最小二乘模型。\n     c. 将本次重复中未被选入自助样本的索引识别为袋外集。\n     d. 对于每个袋外索引 $i$，计算袋外预测误差 $y_i - \\hat{y}_i^{(b)}$，其中 $\\hat{y}_i^{(b)}$ 是在自助样本上拟合的模型得出的预测值。\n  2. 聚合所有重复中的所有袋外平方预测误差，然后除以袋外预测的总数，以获得袋外均方误差。\n  3. 取其平方根以获得自助法预测残差标准误。\n\n您的程序必须为每个测试用例计算以下三个量：\n- 样本内残差标准误，\n- 自助法预测残差标准误，\n- 这两个值之间的差值（自助法预测值减去样本内值）。\n\n所有量都必须是浮点数。\n\n测试套件的数据生成必须遵循此处完整指定的、可复现的设计。对于每个测试用- 例，您必须：\n- 使用现代伪随机数生成器将随机种子设置为给定值。\n- 生成 $X \\in \\mathbb{R}^{n \\times p}$，其元素为独立的标准正态分布项。\n- 构建 $X_{\\text{tilde}} = [\\mathbf{1}, X]$，其中 $\\mathbf{1}$ 是长度为 $n$ 的全一列向量。\n- 根据 $Y = \\beta_0 \\mathbf{1} + X \\beta_{\\text{slopes}} + \\varepsilon$ 生成 $Y$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 在观测间独立，所有参数如下文指定。\n\n需要精确实现的测试套件：\n- 案例 1：seed $= 1729$, $n = 80$, $p = 3$, $\\beta_0 = 0.7$, 斜率向量 $\\beta_{\\text{slopes}} = [1.5, -2.0, 0.5]$, $\\sigma = 1.2$, $B = 500$。\n- 案例 2：seed $= 2021$, $n = 25$, $p = 5$, $\\beta_0 = -1.0$, 斜率向量 $\\beta_{\\text{slopes}} = [0.6, -0.4, 1.2, 0.0, 0.3]$, $\\sigma = 0.8$, $B = 600$。\n- 案例 3：seed $= 777$, $n = 15$, $p = 8$, $\\beta_0 = 1.0$, 斜率向量 $\\beta_{\\text{slopes}} = [0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]$, $\\sigma = 1.5$, $B = 800$。\n\n实现要求：\n- 所有拟合均使用带截距的普通最小二乘法。如果自助样本是秩亏的，使用能够产生最小化残差平方和的拟合值的最小二乘解（例如，通过 Moore–Penrose 伪逆方法）。\n- 自助法袋外聚合必须对每个袋外预测同等计数。如果某次重复没有袋外观测值，则它对聚合结果没有贡献。\n- 必须通过使用指定的种子来确保数值稳定性和可复现性。\n\n最终输出格式：\n- 对于每个测试用例，按上述顺序输出一个包含三个浮点数的列表。\n- 在最终输出中，将每个浮点数四舍五入到恰好 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，形式为由方括号括起来的、包含三个元素列表的逗号分隔列表。例如：$[[a\\_1,b\\_1,c\\_1],[a\\_2,b\\_2,c\\_2],[a\\_3,b\\_3,c\\_3]]$，其中每个 $a\\_j, b\\_j, c\\_j$ 都是四舍五入到 $6$ 位小数的浮点数。",
            "solution": "本解决方案提出了一个计算框架，用以量化和比较回归模型变异性的两种不同度量：样本内残差标准误和通过非参数自助法估计的样本外预测残差标准误。该分析基于普通最小二乘（OLS）线性回归和基于重抽样的模型验证原则。\n\n### 线性模型与普通最小二乘估计\n\n我们考虑标准线性回归模型，该模型假定响应向量 $y \\in \\mathbb{R}^n$ 与一组 $p$ 个预测变量之间存在线性关系。模型表示为：\n$$\nY = X_{\\text{tilde}} \\beta + \\varepsilon\n$$\n其中：\n- $Y$ 是 $n \\times 1$ 的响应随机向量。\n- $X_{\\text{tilde}}$ 是 $n \\times q$ 的设计矩阵，包括一个用于截距的前导全一列和 $p$ 个用于预测变量的列。因此，$q = p + 1$。\n- $\\beta$ 是 $q \\times 1$ 的未知真实系数向量，其中 $\\beta = [\\beta_0, \\beta_1, \\dots, \\beta_p]^T$。\n- $\\varepsilon$ 是 $n \\times 1$ 的未观测到的随机误差向量，并假设 $\\varepsilon_i$ 是独立同分布的，均值 $E[\\varepsilon_i] = 0$，方差恒为 $Var(\\varepsilon_i) = \\sigma^2$。\n\n普通最小二乘（OLS）估计量，记为 $\\hat{\\beta}$，是通过最小化残差平方和（RSS）得到的。$\\hat{\\beta}$ 的 OLS 解由正规方程给出：\n$$\n\\hat{\\beta} = (X_{\\text{tilde}}^T X_{\\text{tilde}})^{-1} X_{\\text{tilde}}^T y\n$$\n在 $X_{\\text{tilde}}$ 不是满列秩的情况下（即其列线性相关），矩阵 $X_{\\text{tilde}}^T X_{\\text{tilde}}$ 是奇异的，无法求逆。这种情况可能在自助样本中发生。在这种情况下，使用 Moore-Penrose 伪逆 $(X_{\\text{tilde}}^T X_{\\text{tilde}})^{\\dagger}$ 可获得一个最小二乘解，该解能最小化残差的欧几里得范数 $\\|y - X_{\\text{tilde}}\\beta\\|_2$。\n\n一旦计算出 $\\hat{\\beta}$，拟合值为 $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。\n\n### 样本内残差标准误（RSE）\n\n样本内 RSE 是对误差项 $\\varepsilon$ 的标准差 $\\sigma$ 的一个估计。它量化了基于用于拟合模型相同的数据，观测数据点与拟合回归线之间的典型偏差。\n\n1.  **残差平方和（RSS）**：这是观测值与拟合值之间差异的平方和：\n    $$\n    RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = e^T e\n    $$\n\n2.  **无偏方差估计量**：为了获得误差方差 $\\sigma^2$ 的一个无偏估计量，RSS 被残差自由度 $df = n - q$ 除。自由度减少 $q$ 是为了解释从数据中估计出的 $q$ 个系数。\n    $$\n    \\hat{\\sigma}^2 = \\frac{RSS}{n - q}\n    $$\n\n3.  **残差标准误**：RSE 是这个无偏方差估计量的平方根：\n    $$\n    RSE_{\\text{in-sample}} = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{e^T e}{n - q}}\n    $$\n样本内 RSE 通常是对模型在新的、未见过的数据上的预测误差的一个乐观（即向下偏置）估计，因为模型已经过优化以最小化在训练样本上的误差。\n\n### 自助法预测残差标准误\n\n为了获得对模型预测性能更现实的估计，我们使用一种重抽样方法，该方法模拟将模型拟合到新数据的过程。带袋外（OOB）评估的成对非参数自助法是一种计算密集但功能强大的技术。它估计了在模型训练中未使用的数据点上的期望预测误差。\n\n对于 $B$ 次自助法重复，算法流程如下：\n\n1.  **自助抽样**：对于每次重复 $b \\in \\{1, \\dots, B\\}$，通过从原始索引集 $\\{1, \\dots, n\\}$ 中有放回地抽取索引来创建一个大小为 $n$ 的自助样本。设所选索引集为 $I^{(b)}$。自助数据为 $(X_{\\text{tilde}}^{(b)}, y^{(b)})$，对应于这些索引。\n\n2.  **模型拟合**：将一个 OLS 模型拟合到自助样本 $(X_{\\text{tilde}}^{(b)}, y^{(b)})$ 上，得到一个系数向量 $\\hat{\\beta}^{(b)}$。\n\n3.  **袋外（OOB）评估**：重复 $b$ 的 OOB 样本由其索引未在 $I^{(b)}$ 中被选中的原始数据点组成。设 OOB 索引集为 $OOB^{(b)} = \\{1, \\dots, n\\} \\setminus I^{(b)}$。\n    对于每个索引 $i \\in OOB^{(b)}$，使用在自助样本上拟合的模型来预测响应：\n    $$\n    \\hat{y}_i^{(b)} = \\tilde{x}_i^T \\hat{\\beta}^{(b)}\n    $$\n    其中 $\\tilde{x}_i^T$ 是原始设计矩阵 $X_{\\text{tilde}}$ 的第 $i$ 行。此 OOB 点的平方预测误差为 $(y_i - \\hat{y}_i^{(b)})^2$。\n\n4.  **聚合**：袋外均方误差（MSE$_{\\text{OOB}}$）是通过对所有 $B$ 次重复中所有 OOB 集计算出的所有平方预测误差求平均值来计算的。\n    $$\n    \\text{MSE}_{\\text{OOB}} = \\frac{\\sum_{b=1}^{B} \\sum_{i \\in OOB^{(b)}} (y_i - \\hat{y}_i^{(b)})^2}{\\sum_{b=1}^{B} |OOB^{(b)}|}\n    $$\n    其中 $|OOB^{(b)}|$ 是重复 $b$ 的袋外观测数量。\n\n5.  **自助法预测 RSE**：MSE$_{\\text{OOB}}$ 的平方根给出了通过自助法估计的预测残差标准误。\n    $$\n    RSE_{\\text{pred}} = \\sqrt{\\text{MSE}_{\\text{OOB}}}\n    $$\n该值提供了对模型在新数据上性能的一个偏差较小的估计。\n\n### 比较\n差值 $\\Delta = RSE_{\\text{pred}} - RSE_{\\text{in-sample}}$ 可作为“乐观偏差”的度量。一个较大的正值表示过拟合程度更高，即模型在其训练数据上的表现显著优于其在新数据上的预期表现。当预测变量数量 $p$ 相对于样本量 $n$ 较大时，这种差异在高维设置中尤为明显。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and contrasts in-sample and bootstrap out-of-bag predictive \n    residual standard errors for linear regression models.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"seed\": 1729, \"n\": 80, \"p\": 3, \"beta_0\": 0.7, \n            \"beta_slopes\": np.array([1.5, -2.0, 0.5]), \"sigma\": 1.2, \"B\": 500\n        },\n        {\n            \"seed\": 2021, \"n\": 25, \"p\": 5, \"beta_0\": -1.0, \n            \"beta_slopes\": np.array([0.6, -0.4, 1.2, 0.0, 0.3]), \"sigma\": 0.8, \"B\": 600\n        },\n        {\n            \"seed\": 777, \"n\": 15, \"p\": 8, \"beta_0\": 1.0, \n            \"beta_slopes\": np.array([0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]), \"sigma\": 1.5, \"B\": 800\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Unpack parameters\n        seed = case[\"seed\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n        beta_0 = case[\"beta_0\"]\n        beta_slopes = case[\"beta_slopes\"]\n        sigma = case[\"sigma\"]\n        B = case[\"B\"]\n        \n        # Initialize a modern pseudorandom number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X = rng.normal(size=(n, p))\n        X_tilde = np.hstack((np.ones((n, 1)), X))\n        beta_full = np.concatenate(([beta_0], beta_slopes))\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = X_tilde @ beta_full + epsilon\n\n        # --- In-sample residual standard error ---\n        q = p + 1\n        # np.linalg.lstsq handles potential rank-deficiency and returns RSS\n        _, rss_array, _, _ = np.linalg.lstsq(X_tilde, y, rcond=None)\n        rss = rss_array[0]\n        df = n - q\n        \n        # Handle case where df = 0 (more predictors than samples)\n        # In this situation, the in-sample error is 0, and RSE is undefined or 0.\n        if df  0:\n            mse_in_sample = rss / df\n            rse_in_sample = np.sqrt(mse_in_sample)\n        else:\n            rse_in_sample = 0.0\n\n\n        # --- Out-of-sample bootstrap predictive residual standard error ---\n        total_oob_sq_err = 0.0\n        total_oob_count = 0\n        all_indices = np.arange(n)\n\n        for _ in range(B):\n            # Draw a bootstrap sample of indices\n            boot_indices = rng.choice(all_indices, size=n, replace=True)\n            \n            # Identify out-of-bag (OOB) indices\n            # unique() is important as bootstrap samples may have duplicates\n            oob_indices = np.setdiff1d(all_indices, np.unique(boot_indices), assume_unique=True)\n\n            if len(oob_indices) == 0:\n                continue\n            \n            # Create bootstrap and OOB data sets\n            X_boot, y_boot = X_tilde[boot_indices], y[boot_indices]\n            X_oob, y_oob = X_tilde[oob_indices], y[oob_indices]\n\n            # Fit OLS model on bootstrap sample\n            # This automatically uses a pseudoinverse for rank-deficient matrices\n            beta_boot, _, _, _ = np.linalg.lstsq(X_boot, y_boot, rcond=None)\n\n            # Predict on OOB data\n            y_pred_oob = X_oob @ beta_boot\n\n            # Accumulate squared errors and counts\n            total_oob_sq_err += np.sum((y_oob - y_pred_oob)**2)\n            total_oob_count += len(oob_indices)\n        \n        # Calculate OOB MSE and the predictive RSE\n        if total_oob_count  0:\n            mse_oob = total_oob_sq_err / total_oob_count\n            rse_pred = np.sqrt(mse_oob)\n        else:\n            # Fallback if no OOB samples were ever generated (highly unlikely)\n            rse_pred = np.nan\n\n        # Calculate the difference\n        difference = rse_pred - rse_in_sample\n\n        results.append([rse_in_sample, rse_pred, difference])\n\n    # Format the final output string as per requirements\n    output_parts = []\n    for res in results:\n        # rounding to exactly 6 decimal places\n        part = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_str = f\"[{','.join(output_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界的数据往往不完美，缺失值是生物统计分析中一个常见且棘手的挑战。本高级编程练习将引导你推导并实现期望最大化（EM）算法，以便在响应变量存在缺失数据的情况下，计算估计标准误 $\\hat{\\sigma}$ 的最大似然估计。这项实践将让你掌握一种在数据不完整时进行原则性统计推断的强大技术 。",
            "id": "4953183",
            "problem": "考虑具有独立误差的正态线性模型。令 $y \\in \\mathbb{R}^{n}$ 表示响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 为满列秩的设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 为回归系数，$\\sigma^{2} \\in \\mathbb{R}_{+}$ 为误差方差。假设抽样模型为 $y \\mid X, \\beta, \\sigma^{2} \\sim \\mathcal{N}(X \\beta, \\sigma^{2} I_{n})$，其中 $I_{n}$ 是 $n \\times n$ 的单位矩阵。响应的一个子集是完全随机缺失的，并由非数值 $\\mathrm{NaN}$ 表示。任务是推导并实现一个期望最大化（EM, Expectation-Maximization）过程，以在存在缺失响应的情况下，计算残差标准差（通常称为估计的标准误）的最大似然估计 $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^{2}}$。\n\n您的推导必须从以下基础开始：\n- 多元正态模型的完全数据对数似然以及 EM 算法的定义（在给定观测数据和当前参数的条件下，最大化缺失数据的条件分布下的期望完全数据对数似然）。\n- 最小二乘法的线性代数恒等式，以及对于 $z \\sim \\mathcal{N}(\\mu, \\tau^{2})$，对任意实数 $a$ 有 $\\mathbb{E}\\left[(z - a)^{2}\\right] = \\tau^{2} + (\\mu - a)^{2}$ 这一事实。\n\n您必须：\n- 在此存在缺失响应的设置中，于 EM 框架内推导 $\\beta$ 和 $\\sigma^{2}$ 的更新方程。您的推导应明确说明对缺失响应的期望如何进入每次迭代中最大化的目标函数，并应以用 $X$、 $y$ 的观测分量、当前迭代值 $(\\beta^{(t)}, \\sigma^{2\\,(t)})$ 以及缺失响应数量等计数表示的 $\\beta$ 和 $\\sigma^{2}$ 的显式更新规则作结。\n- 解释在收敛时如何从 $\\hat{\\sigma}^{2}$ 获得估计的标准误 $\\hat{\\sigma}$。\n- 解释在没有缺失响应时，这些更新如何简化为通常的完全数据最大似然估计量。\n\n然后，实现一个程序，该程序执行 EM 迭代直至收敛，从 $\\beta^{(0)} = \\mathbf{0}_{p}$ 和 $\\sigma^{2\\,(0)} = 1$ 开始，使用以下收敛准则：当 $\\sigma^{2}$ 的相对变化和 $\\beta$ 的相对变化（欧几里得范数）的最大值小于 $10^{-10}$ 时，或当执行了 $10{,}000$ 次迭代时停止，以先发生者为准。如果问题从观测数据中无法识别（例如，零个观测响应或导致观测数据正规方程奇异的设计），您的实现应为该测试用例的 $\\hat{\\sigma}$ 返回非数值 $\\mathrm{NaN}$。\n\n测试套件：\n- 案例 1（带有一个缺失响应的简单线性模型）：\n  - $$ X^{(1)} = \\begin{bmatrix} 1  0 \\\\\\\\ 1  1 \\\\\\\\ 1  2 \\\\\\\\ 1  3 \\\\\\\\ 1  4 \\end{bmatrix} $$\n  - $$ y^{(1)} = \\begin{bmatrix} 2.1 \\\\\\\\ 2.4 \\\\\\\\ 3.2 \\\\\\\\ \\mathrm{NaN} \\\\\\\\ 3.9 \\end{bmatrix} $$\n- 案例 2（无缺失响应）：\n  - $$ X^{(2)} = \\begin{bmatrix} 1  -2 \\\\\\\\ 1  -1 \\\\\\\\ 1  0 \\\\\\\\ 1  1 \\\\\\\\ 1  2 \\end{bmatrix} $$\n  - $$ y^{(2)} = \\begin{bmatrix} -1.5 \\\\\\\\ -0.4 \\\\\\\\ 0.3 \\\\\\\\ 1.6 \\\\\\\\ 2.5 \\end{bmatrix} $$\n- 案例 3（带有两个预测变量和一个缺失响应）：\n  - $$ X^{(3)} = \\begin{bmatrix} 1  0  1 \\\\\\\\ 1  1  2 \\\\\\\\ 1  2  0 \\\\\\\\ 1  3  1 \\\\\\\\ 1  4  3 \\\\\\\\ 1  5  5 \\end{bmatrix} $$\n  - $$ y^{(3)} = \\begin{bmatrix} 0.8 \\\\\\\\ 1.2 \\\\\\\\ 1.95 \\\\\\\\ \\mathrm{NaN} \\\\\\\\ 2.3 \\\\\\\\ 2.55 \\end{bmatrix} $$\n- 案例 4（边界情况，观测响应上的残差为零，得到 $\\hat{\\sigma} = 0$）：\n  - $$ X^{(4)} = \\begin{bmatrix} 1  0 \\\\\\\\ 1  1 \\\\\\\\ 1  2 \\end{bmatrix} $$\n  - $$ y^{(4)} = \\begin{bmatrix} 1.0 \\\\\\\\ 3.0 \\\\\\\\ \\mathrm{NaN} \\end{bmatrix} $$\n\n您的程序必须：\n- 实现您推导的 EM 更新，以估计每个案例的 $\\hat{\\sigma}$。\n- 使用上述确切的测试用例，直接嵌入程序中（无输入）。\n- 生成单行输出，其中包含用方括号括起来的四个 $\\hat{\\sigma}$ 值的逗号分隔列表，每个值四舍五入到六位小数，例如 $\\left[0.123456,1.234568,0.000000,2.000000\\right]$。\n\n每个测试用例的最终输出必须是浮点数。不涉及物理单位或角度单位。",
            "solution": "经评估，用户提供的问题是有效的。这是一个计算统计学中定义明确的问题，基于最大似然估计和期望最大化（EM）算法的既定原则。所有术语都得到了正式定义，设置是自洽和一致的，测试用例结构良好。\n\n### EM 算法的推导\n\n目标是为正态线性模型 $y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I_n)$ 找到参数 $\\theta = (\\beta, \\sigma^2)$ 的最大似然估计（MLE），其中响应向量 $y$ 的某些分量是缺失的。我们将观测到的分量表示为 $y_{obs}$，缺失的分量表示为 $y_{mis}$。EM 算法提供了一个迭代过程，通过重复最大化完全数据对数似然的期望来最大化观测数据的对数似然 $\\ell(\\theta; y_{obs}) = \\log p(y_{obs}|\\theta)$。\n\n完全数据向量是 $y \\in \\mathbb{R}^n$，划分为 $y_{obs}$ 和 $y_{mis}$。令 $O$ 和 $M$ 分别为观测数据和缺失数据的索引集。总观测数为 $n = |O| + |M| = n_{obs} + n_{mis}$。\n\n完全数据对数似然，忽略与 $\\theta$ 无关的常数，为：\n$$ \\ell(\\theta; y) = -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} (y - X\\beta)^T(y - X\\beta) $$\n$$ \\ell(\\theta; y) = -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 $$\n其中 $x_i^T$ 是设计矩阵 $X$ 的第 $i$ 行。\n\nEM 算法包括两个步骤：期望步骤（E-step）和最大化步骤（M-step）。\n\n#### E 步骤\n在 E 步骤中，在第 $t$ 次迭代时，我们计算在给定观测数据 $y_{obs}$ 和当前参数估计 $\\theta^{(t)} = (\\beta^{(t)}, \\sigma^{2,(t)})$ 的条件下，完全数据对数似然的期望。该函数表示为 $Q(\\theta | \\theta^{(t)})$。\n$$ Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{y_{mis} | y_{obs}, \\theta^{(t)}}[\\ell(\\theta; y)] $$\n代入对数似然表达式：\n$$ Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{y_{mis} | y_{obs}, \\theta^{(t)}} \\left[ -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 \\right] $$\n项 $-\\frac{n}{2} \\log(\\sigma^2)$ 相对于期望是常数。我们只需要计算平方和项的期望。\n$$ Q(\\theta | \\theta^{(t)}) = -\\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathbb{E}_{y_{mis} | y_{obs}, \\theta^{(t)}} \\left[ \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 \\right] $$\n求和可以分为对观测索引和缺失索引的求和：\n$$ \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 = \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} (y_i - x_i^T\\beta)^2 $$\n第一项只涉及观测数据，在期望中被视为常数。第二项的期望是关于 $y_{mis}$ 的分布计算的。由于模型假设误差独立，$y_i$ 在给定 $\\beta$ 和 $\\sigma^2$ 的条件下是独立的。因此，$y_{mis}$ 的分布独立于 $y_{obs}$。\n$$ y_i | \\theta^{(t)} \\sim \\mathcal{N}(x_i^T\\beta^{(t)}, \\sigma^{2,(t)}) \\quad \\text{for } i \\in M $$\n我们需要计算对于 $i \\in M$ 的 $\\mathbb{E}_{y_i|\\theta^{(t)}}[(y_i - x_i^T\\beta)^2]$。使用给定的恒等式 $\\mathbb{E}[(z - a)^2] = \\text{Var}(z) + (\\mathbb{E}[z] - a)^2$，其中 $z=y_i$，$\\mathbb{E}[z] = x_i^T\\beta^{(t)}$，$\\text{Var}(z) = \\sigma^{2,(t)}$，以及 $a=x_i^T\\beta$，我们得到：\n$$ \\mathbb{E}_{y_i|\\theta^{(t)}}[(y_i - x_i^T\\beta)^2] = \\sigma^{2,(t)} + (x_i^T\\beta^{(t)} - x_i^T\\beta)^2 $$\n将此代回 $Q(\\theta|\\theta^{(t)})$ 的表达式中：\n$$ Q(\\theta | \\theta^{(t)}) = -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left[ \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} \\left(\\sigma^{2,(t)} + (x_i^T\\beta^{(t)} - x_i^T\\beta)^2\\right) \\right] $$\n\n#### M 步骤\n在 M 步骤中，我们找到使 $Q(\\theta | \\theta^{(t)})$ 最大化的参数 $\\theta^{(t+1)} = (\\beta^{(t+1)}, \\sigma^{2,(t+1)})$。这通过对 $\\beta$ 和 $\\sigma^2$ 求导并令其为零来实现。\n\n**$\\beta$ 的更新**：\n为了找到 $\\beta^{(t+1)}$，我们最小化 $Q$ 中依赖于 $\\beta$ 的项：\n$$ \\arg\\min_{\\beta} \\left[ \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta)^2 \\right] $$\n我们定义一个“补全”的响应向量 $y^{(t)}$，其中缺失值用它们当前的条件期望填充：\n$$ y_i^{(t)} = \\begin{cases} y_i  \\text{if } i \\in O \\\\ \\mathbb{E}[y_i | y_{obs}, \\theta^{(t)}] = x_i^T \\beta^{(t)}  \\text{if } i \\in M \\end{cases} $$\n为 $\\beta$ 最小化的表达式变为：\n$$ \\sum_{i \\in O} (y_i^{(t)} - x_i^T\\beta)^2 + \\sum_{i \\in M} (y_i^{(t)} - x_i^T\\beta)^2 = \\sum_{i=1}^n (y_i^{(t)} - x_i^T\\beta)^2 = (y^{(t)} - X\\beta)^T(y^{(t)} - X\\beta) $$\n这是一个标准的普通最小二乘（OLS）问题。解为：\n$$ \\beta^{(t+1)} = (X^T X)^{-1} X^T y^{(t)} $$\n由于问题陈述 $X$ 具有满列秩，这一步保证是明确定义的。\n\n**$\\sigma^2$ 的更新**：\n为了找到 $\\sigma^{2,(t+1)}$，我们固定 $\\beta = \\beta^{(t+1)}$ 并关于 $\\sigma^2$ 最大化 $Q$。令 $S(\\beta) = \\sum_{i \\in O} (y_i - x_i^T\\beta)^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta)^2$。\n我们最大化：\n$$ -\\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\left[ S(\\beta^{(t+1)}) + n_{mis}\\sigma^{2,(t)} \\right] $$\n关于 $\\sigma^2$ 求导并令其为零，得到：\n$$ -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\left[ S(\\beta^{(t+1)}) + n_{mis}\\sigma^{2,(t)} \\right] = 0 $$\n$$ \\sigma^{2,(t+1)} = \\frac{1}{n} \\left[ S(\\beta^{(t+1)}) + n_{mis}\\sigma^{2,(t)} \\right] $$\n我们使用 $y^{(t)}$ 的定义来展开 $S(\\beta^{(t+1)})$：\n$S(\\beta^{(t+1)}) = (y^{(t)} - X\\beta^{(t+1)})^T(y^{(t)} - X\\beta^{(t+1)})$ 即 $\\sum_{i \\in O} (y_i - x_i^T\\beta^{(t+1)})^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta^{(t+1)})^2$。\n因此，$\\sigma^2$ 的更新规则是：\n$$ \\sigma^{2,(t+1)} = \\frac{1}{n} \\left[ \\sum_{i \\in O} (y_i - x_i^T\\beta^{(t+1)})^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta^{(t+1)})^2 + n_{mis}\\sigma^{2,(t)} \\right] $$\n\n### EM 算法总结\n1.  **初始化**：设置初始值 $\\beta^{(0)} = \\mathbf{0}_p$ 和 $\\sigma^{2,(0)} = 1$。\n2.  **迭代**：对于 $t = 0, 1, 2, \\dots$ 直至收敛：\n    a.  **E 步骤**：通过填补缺失值来构造补全的响应向量 $y^{(t)}$：\n        $y_i^{(t)} = y_i$ 对于 $i \\in O$，以及 $y_i^{(t)} = x_i^T\\beta^{(t)}$ 对于 $i \\in M$。\n    b.  **M 步骤**：更新参数估计：\n        -   $\\beta^{(t+1)} = (X^T X)^{-1} X^T y^{(t)}$\n        -   $\\sigma^{2,(t+1)} = \\frac{1}{n} \\left( \\sum_{i \\in O} (y_i - x_i^T\\beta^{(t+1)})^2 + \\sum_{i \\in M} (x_i^T\\beta^{(t)} - x_i^T\\beta^{(t+1)})^2 + n_{mis}\\sigma^{2,(t)} \\right)$\n3.  **收敛**：当参数 $(\\beta, \\sigma^2)$ 稳定时，算法收敛。令 $(\\hat{\\beta}, \\hat{\\sigma}^2)$ 为收敛时的估计值。残差标准差的最终估计为 $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$。\n\n### 简化为完全数据情况\n如果没有缺失响应（$n_{mis} = 0$），则对所有 $t$ 都有 $y^{(t)} = y$。E 步骤是平凡的。\nM 步骤的更新变为：\n-   $\\beta^{(1)} = (X^T X)^{-1} X^T y = \\hat{\\beta}_{MLE}$。该值在所有后续迭代中保持不变。\n-   $\\sigma^{2,(1)} = \\frac{1}{n} \\left( \\sum_{i \\in O} (y_i - x_i^T\\hat{\\beta}_{MLE})^2 + 0 + 0 \\right) = \\frac{1}{n} (y - X\\hat{\\beta}_{MLE})^T(y - X\\hat{\\beta}_{MLE}) = \\hat{\\sigma}^2_{MLE}$。\n算法在一次迭代中收敛到具有完全数据的正态线性模型的标准 MLE。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def run_em_for_sigma(X, y, tol=1e-10, max_iter=10000):\n        \"\"\"\n        Implements the EM algorithm to estimate the residual standard deviation.\n\n        Args:\n            X (np.ndarray): The design matrix of shape (n, p).\n            y (np.ndarray): The response vector of shape (n,), possibly containing NaNs.\n            tol (float): The convergence tolerance.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            float: The estimated residual standard deviation (sigma).\n        \"\"\"\n        n, p = X.shape\n\n        # Identify observed and missing data\n        obs_mask = ~np.isnan(y)\n        mis_mask = np.isnan(y)\n        obs_idx = np.where(obs_mask)[0]\n        mis_idx = np.where(mis_mask)[0]\n        n_obs = len(obs_idx)\n        n_mis = len(mis_idx)\n\n        # Handle non-identifiable cases\n        if n_obs == 0:\n            return np.nan\n        \n        X_obs = X[obs_idx, :]\n        # Check if the observed-data normal equations are singular\n        if np.linalg.matrix_rank(X_obs)  p:\n            return np.nan\n\n        # Pre-compute parts that are constant through iterations\n        XTX = X.T @ X\n        XTX_inv = np.linalg.inv(XTX)\n        y_obs = y[obs_idx]\n\n        # Initialization\n        beta = np.zeros(p)\n        sigma2 = 1.0\n\n        for _ in range(max_iter):\n            beta_old = beta\n            sigma2_old = sigma2\n\n            # E-step: Construct the completed response vector y_imputed\n            # by filling missing values with their conditional expectation\n            y_imputed = np.copy(y)\n            y_imputed[mis_idx] = X[mis_idx, :] @ beta_old\n\n            # M-step: Update parameters\n            # Update beta\n            beta = XTX_inv @ X.T @ y_imputed\n\n            # Update sigma^2\n            # Term 1: Sum of squared residuals for observed data\n            res_obs = y_obs - X_obs @ beta\n            term1 = np.sum(res_obs**2)\n\n            # Term 2: Contribution from the uncertainty in imputed values\n            # This is sum_{i in M} (E[y_i | old] - E[y_i | new])^2,\n            # where E[y_i] = x_i^T beta\n            imputed_change = X[mis_idx, :] @ beta_old - X[mis_idx, :] @ beta\n            term2 = np.sum(imputed_change**2)\n            \n            # Term 3: Contribution from the variance of imputed values\n            term3 = n_mis * sigma2_old\n            \n            sigma2 = (term1 + term2 + term3) / n\n            \n            # Convergence check\n            # Use small epsilon to avoid division by zero\n            eps = 1e-12\n            rel_change_beta = np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + eps)\n            rel_change_sigma2 = np.abs(sigma2 - sigma2_old) / (sigma2_old + eps)\n            \n            if max(rel_change_beta, rel_change_sigma2)  tol:\n                break\n        \n        # At convergence, the final estimate is sqrt(sigma^2)\n        return np.sqrt(sigma2)\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([2.1, 2.4, 3.2, np.nan, 3.9]),\n        },\n        {\n            \"X\": np.array([[1, -2], [1, -1], [1, 0], [1, 1], [1, 2]]),\n            \"y\": np.array([-1.5, -0.4, 0.3, 1.6, 2.5]),\n        },\n        {\n            \"X\": np.array([[1, 0, 1], [1, 1, 2], [1, 2, 0], [1, 3, 1], [1, 4, 3], [1, 5, 5]]),\n            \"y\": np.array([0.8, 1.2, 1.95, np.nan, 2.3, 2.55]),\n        },\n        {\n            \"X\": np.array([[1, 0], [1, 1], [1, 2]]),\n            \"y\": np.array([1.0, 3.0, np.nan]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        sigma_hat = run_em_for_sigma(X, y)\n        results.append(sigma_hat)\n\n    # Format the final output string\n    # Round to six decimal places\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}