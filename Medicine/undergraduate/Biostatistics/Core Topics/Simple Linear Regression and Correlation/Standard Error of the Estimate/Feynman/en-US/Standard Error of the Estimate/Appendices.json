{
    "hands_on_practices": [
        {
            "introduction": "A primary use of the standard error of the estimate, $\\hat{\\sigma}$, is in model diagnostics. This practice challenges you to move beyond simply looking at a residual plot and to think critically about how to formalize a diagnostic rule for heteroscedasticity, or non-constant error variance . By evaluating different potential rules, you will develop a deeper intuition for what constitutes a statistically significant pattern versus random noise, using $\\hat{\\sigma}$ as your fundamental yardstick.",
            "id": "4953188",
            "problem": "An investigator fits a linear regression model using Ordinary Least Squares (OLS) to relate a continuous biomarker response $y$ to $p=3$ predictors (including an intercept) in a cohort of $n=90$ participants. Under the classical OLS assumptions for biostatistical modeling of continuous outcomes, errors $\\varepsilon_i$ are independent and identically distributed with mean $0$ and constant variance $\\sigma^2$, and the residuals $e_i$ are visualized against the fitted values $\\hat{y}_i$ to assess model adequacy. The overall standard error of the estimate (residual standard error) is defined as $\\hat{\\sigma}=\\sqrt{\\mathrm{RSS}/(n-p)}$, where $\\mathrm{RSS}=\\sum_{i=1}^n e_i^2$ is the residual sum of squares. To diagnose possible heteroscedasticity from the residual plot, the analyst bins the fitted values $\\hat{y}_i$ into three equal-sized groups of $m=30$ each: low ($L$), middle ($M$), and high ($H$), and within each group computes the sample residual standard deviation $s_L$, $s_M$, and $s_H$, respectively. Which of the following diagnostic rules most appropriately leverages $\\hat{\\sigma}$ to detect heteroscedasticity by comparing residual spreads across fitted-value groups and quantifies thresholds in terms of $\\hat{\\sigma}$ at approximately a $5\\%$ significance level?\n\nA. Flag heteroscedasticity if the high-fitted group’s residual spread is simultaneously large relative to $\\hat{\\sigma}$ and the low-fitted group’s residual spread is small relative to $\\hat{\\sigma}$, namely if $s_H>1.21\\,\\hat{\\sigma}$ and $s_L<0.80\\,\\hat{\\sigma}$; this joint condition implies a large spread ratio $s_H^2/s_L^2\\gtrsim 2.29$.\n\nB. Flag heteroscedasticity whenever $s_H>1.05\\,\\hat{\\sigma}$, regardless of the residual spread in the other groups.\n\nC. Flag heteroscedasticity if the average absolute residual in the middle group exceeds the overall standard error of the estimate, that is if $\\frac{1}{m}\\sum_{i\\in M}|e_i|>\\hat{\\sigma}$.\n\nD. Flag heteroscedasticity if the difference in residual spreads between the high and low groups exceeds one-tenth of the overall standard error of the estimate, that is if $s_H-s_L>0.10\\,\\hat{\\sigma}$.\n\nE. Flag heteroscedasticity if the maximum residual in the high-fitted group exceeds three times the overall standard error of the estimate, that is if $\\max_{i\\in H}|e_i|>3\\,\\hat{\\sigma}$.",
            "solution": "The user wants me to act as a STEM professor and solve a biostatistics problem.\n\nMy process will be:\n1.  **Parse the request**: I need to validate the problem statement first, then solve it and evaluate the options. The output must be in a specific XML format with strict LaTeX formatting.\n\n2.  **Problem Validation**:\n    *   **Step 1: Extract Givens**:\n        *   Model: Linear regression using Ordinary Least Squares (OLS).\n        *   Response variable: $y$, a continuous biomarker.\n        *   Number of predictors: $p=3$ (including an intercept).\n        *   Sample size: $n=90$.\n        *   OLS assumptions: Errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.) with mean $0$ and constant variance $\\sigma^2$.\n        *   Residuals: $e_i$.\n        *   Residual Sum of Squares (RSS): $\\mathrm{RSS}=\\sum_{i=1}^n e_i^2$.\n        *   Standard error of the estimate (residual standard error): $\\hat{\\sigma}=\\sqrt{\\mathrm{RSS}/(n-p)}$.\n        *   Diagnostic method: Bin fitted values $\\hat{y}_i$ into three equal-sized groups of $m=30$ each: low ($L$), middle ($M$), and high ($H$).\n        *   Group-wise statistics: Sample residual standard deviations $s_L$, $s_M$, and $s_H$ are computed for each group.\n        *   Question: Which diagnostic rule most appropriately leverages $\\hat{\\sigma}$ to detect heteroscedasticity by comparing residual spreads across fitted-value groups and quantifies thresholds in terms of $\\hat{\\sigma}$ at approximately a $5\\%$ significance level?\n\n    *   **Step 2: Validate Using Extracted Givens**:\n        *   **Scientifically Grounded**: The problem describes a standard procedure in regression diagnostics, specifically checking for heteroscedasticity (non-constant variance of errors). The tests mentioned, like comparing residual variances across groups of fitted values, are related to established statistical tests like the Goldfeld-Quandt test or the Breusch-Pagan test, although simplified for this context. The binning strategy is a common informal graphical check. The quantities $\\hat{\\sigma}$, $s_L$, $s_H$ are standard statistical estimates. The problem is well-grounded in statistics and biostatistics.\n        *   **Well-Posed**: The problem asks to identify the *most appropriate* diagnostic rule among the given options. This implies there is a correct statistical principle to apply. The question is about identifying a rule that corresponds to a known statistical test or a reasonable heuristic based on sound statistical theory. The problem is structured to have a unique best answer by asking for a rule that corresponds to a certain significance level (~5%).\n        *   **Objective**: The language is precise and technical. No subjective statements.\n        *   No scientific unsoundness.\n        *   The problem is formalizable. It's directly about biostatistics.\n        *   The setup is complete. We have $n$, $p$, and the group size $m$. This is enough information to reason about the statistical properties of the estimators.\n        *   No unrealistic or infeasible conditions.\n        *   The problem is not ill-posed. It asks to evaluate specific criteria, which can be done using statistical theory.\n        *   Not pseudo-profound or trivial. It requires knowledge of hypothesis testing for variances.\n        *   It is verifiable through statistical theory.\n\n    *   **Step 3: Verdict and Action**: The problem is **valid**. I will proceed with the solution.\n\n3.  **Solution Derivation**:\n    *   The core of the problem is to find a statistical test for heteroscedasticity that compares variances between groups of residuals. The null hypothesis ($H_0$) is homoscedasticity, meaning $\\sigma_L^2 = \\sigma_M^2 = \\sigma_H^2 = \\sigma^2$. The alternative hypothesis ($H_a$) is heteroscedasticity, where the variances are not all equal, often with a specific pattern like increasing variance with fitted values ($\\sigma_L^2 < \\sigma_M^2 < \\sigma_H^2$).\n    *   The quantities $s_L^2, s_M^2, s_H^2$ are sample variances calculated for the residuals in each group. Under the null hypothesis, and assuming the residuals within each group are approximately normally distributed, the statistics $(m-k_g)s_g^2/\\sigma^2$ follow a chi-squared distribution, where $s_g^2 = \\frac{1}{m-k_g}\\sum_{i \\in g} e_i^2$ and $k_g$ is a small number related to degrees of freedom lost within the group. The problem defines $s_g$ as the \"sample residual standard deviation\". Let's assume this is the standard definition, so $s_g^2 = \\frac{1}{m-1} \\sum_{i \\in g} (e_i - \\bar{e}_g)^2$. However, in regression diagnostics, it's common to use $s_g^2 = \\frac{1}{m}\\sum_{i \\in g} e_i^2$ or $\\frac{1}{m-k_{sub}}\\sum e_i^2$ for some small $k_{sub}$. Let's assume the degrees of freedom for each group is approximately $m=30$.\n    *   A common test for comparing two variances is the F-test. The test statistic is the ratio of the sample variances, $F = s_H^2 / s_L^2$. Under $H_0$, this statistic follows an F-distribution with degrees of freedom $(\\nu_H, \\nu_L)$. The degrees of freedom for each sample variance estimate would be approximately $m-1 = 29$.\n    *   Let's analyze the question's premise: \"quantifies thresholds in terms of $\\hat{\\sigma}$ at approximately a $5\\%$ significance level\". This suggests we're not just doing a simple F-test between two groups, but relating the group SDs ($s_L, s_H$) to the overall SD ($\\hat{\\sigma}$).\n    *   The overall variance estimator is $\\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n-p} = \\frac{\\sum_{i=1}^{90} e_i^2}{90-3} = \\frac{\\sum_{i=1}^{90} e_i^2}{87}$.\n    *   The RSS can be decomposed into the sum of squared residuals from each group: $\\mathrm{RSS} = \\sum_{i \\in L} e_i^2 + \\sum_{i \\in M} e_i^2 + \\sum_{i \\in H} e_i^2$.\n    *   The sample variance within a group $g$ is denoted $s_g$. How is it calculated? \"sample residual standard deviation\". Usually, this means $s_g^2 = \\frac{1}{m-1} \\sum_{i \\in g} (e_i - \\bar{e}_g)^2$. In regression, the mean of all residuals is zero, $\\bar{e} = 0$. However, the mean of residuals within a specific bin of fitted values, $\\bar{e}_g$, is not guaranteed to be zero. Let's assume for simplicity, it's close to zero. Then $s_g^2 \\approx \\frac{1}{m-1} \\sum_{i \\in g} e_i^2$. The problem gives $m=30$.\n    *   Under $H_0: \\sigma_L^2 = \\sigma_H^2 = \\sigma^2$. The quantities $(m-1)s_L^2/\\sigma^2$ and $(m-1)s_H^2/\\sigma^2$ are approximately $\\chi^2_{m-1}$ distributed. Thus, $s_L^2$ and $s_H^2$ are estimators of $\\sigma^2$.\n    *   The statistic $\\hat{\\sigma}^2 = \\mathrm{RSS}/(n-p)$ is also an estimator of $\\sigma^2$. It is based on $n-p=87$ degrees of freedom.\n    *   The problem is essentially setting up a simplified version of more formal tests. The Goldfeld-Quandt test specifically orders the data, splits it into (typically two) groups, and computes the ratio of RSS from separate regressions on each group. This problem simplifies by binning residuals from a single regression.\n    *   Let's look at the options. They set thresholds on $s_H/\\hat{\\sigma}$ and $s_L/\\hat{\\sigma}$. This is a comparison of a group-specific standard deviation to the pooled standard deviation.\n    *   Let's analyze the statistic $s_g^2 / \\hat{\\sigma}^2$. This is a ratio of two variance estimators.\n    *   $s_g^2$ is based on approximately $m-1 = 29$ degrees of freedom.\n    *   $\\hat{\\sigma}^2$ is based on $n-p = 87$ degrees of freedom.\n    *   The statistic $F = s_g^2 / \\hat{\\sigma}^2$ does not follow a simple F-distribution because $s_g^2$ and $\\hat{\\sigma}^2$ are not independent. $\\hat{\\sigma}^2$ is calculated using all residuals, including those in group $g$.\n    *   However, let's consider the spirit of the question. It is proposing a heuristic diagnostic rule. We need to find the one that is most statistically sound, corresponding to a \"5% significance level\".\n    *   Let's evaluate the options.\n\n    *   **Option A**: Flag if $s_H>1.21\\,\\hat{\\sigma}$ and $s_L<0.80\\,\\hat{\\sigma}$.\n        *   This checks for a specific pattern of heteroscedasticity: increasing variance with $\\hat{y}$. The variance is high for high $\\hat{y}$ and low for low $\\hat{y}$. This is a very common pattern.\n        *   The condition is joint: $s_H^2 > (1.21)^2 \\hat{\\sigma}^2 \\approx 1.464 \\hat{\\sigma}^2$ and $s_L^2 < (0.80)^2 \\hat{\\sigma}^2 = 0.64 \\hat{\\sigma}^2$.\n        *   It also states \"this joint condition implies a large spread ratio $s_H^2/s_L^2\\gtrsim 2.29$\". Let's check this: $s_H^2 / s_L^2 > (1.464 \\hat{\\sigma}^2) / (0.64 \\hat{\\sigma}^2) = 1.464 / 0.64 \\approx 2.2875$. So the implication is correct.\n        *   Is this ratio statistically significant at the 5% level? This ratio $s_H^2/s_L^2$ would be compared to a critical value from an F-distribution. The degrees of freedom are approximately $(m-1, m-1) = (29, 29)$.\n        *   Let's find the critical value $F_{crit}$ for an F-test with $\\nu_1=29, \\nu_2=29$ degrees of freedom. Heteroscedasticity is usually a two-sided concern (variance could increase or decrease), but in the context of high vs low fitted values, a one-sided test $H_a: \\sigma_H^2 > \\sigma_L^2$ is common. So we look for the upper $5\\%$ critical value.\n        *   Using an F-distribution calculator or table for $\\nu_1=29, \\nu_2=29$ at $\\alpha=0.05$ (one-tailed), we get $F_{0.05, 29, 29} \\approx 1.85$.\n        *   What if it's a two-tailed test? We'd use $\\alpha/2 = 0.025$. The upper critical value $F_{0.025, 29, 29} \\approx 2.15$.\n        *   The value $2.29$ is greater than both $1.85$ and $2.15$. So a ratio of this magnitude would indeed be significant at the 5% level.\n        *   But the rule isn't just about the ratio. It's about the individual comparisons to $\\hat{\\sigma}$.\n        *   Could $s_H/\\hat{\\sigma}$ and $s_L/\\hat{\\sigma}$ have a known distribution? Perhaps related to the F-distribution. Let $RSS_g = \\sum_{i \\in g} e_i^2$. Let's assume $s_g^2 \\approx RSS_g/m$. Then $s_H^2 / s_L^2 \\approx (RSS_H/m) / (RSS_L/m) = RSS_H/RSS_L$. Under $H_0$, $RSS_H/\\sigma^2 \\sim \\chi^2_m$ and $RSS_L/\\sigma^2 \\sim \\chi^2_m$ (approximately, since residuals are not exactly the errors, and their sum of squares doesn't perfectly follow a chi-squared distribution, but it's a good approximation). Since the bins are created from one regression, the RSS terms are not independent. The Goldfeld-Quandt test actually runs separate regressions, making the RSS terms independent. This is a simplification.\n        *   Let's assume independence for a moment. Then $(RSS_H/m) / (RSS_L/m)$ follows an $F_{m,m}$ distribution. For $m=30$, the critical F-value at $\\alpha=0.05$ (one-tail) is $F_{0.05, 30, 30} \\approx 1.84$. At $\\alpha=0.025$ (one-tail for a two-tailed test), $F_{0.025, 30, 30} \\approx 2.13$. The value of $2.29$ is indeed in the rejection region for a test at the 5% level.\n        *   But where do the numbers $1.21$ and $0.80$ come from? This is the key. They seem to define a specific region in the $(s_L, s_H)$ plane.\n        *   Let's think about the distribution of $s_g^2 / \\hat{\\sigma}^2$ under homoscedasticity.\n            Let $S_g = (m-1)s_g^2/\\sigma^2 \\sim \\chi^2_{m-1}$ and $S_{total} = (n-p)\\hat{\\sigma}^2/\\sigma^2 \\sim \\chi^2_{n-p}$.\n            $S_g$ and $S_{total}$ are not independent.\n            Let $RSS_g = \\sum_{i \\in g} e_i^2$. Then $s_g^2 \\approx RSS_g/(m-1)$.\n            $\\hat{\\sigma}^2 = (RSS_L+RSS_M+RSS_H)/(n-p)$.\n            $s_H^2/\\hat{\\sigma}^2 \\approx \\frac{RSS_H/(m-1)}{(RSS_L+RSS_M+RSS_H)/(n-p)}$.\n            Under $H_0$, we expect $E[RSS_g] \\approx (m-1)\\sigma^2$. (More accurately, it's slightly less due to the projection matrix properties, but let's go with this approximation). So we'd expect $RSS_L \\approx RSS_M \\approx RSS_H$.\n            Then $E[\\hat{\\sigma}^2] \\approx \\frac{3(m-1)\\sigma^2}{n-p} = \\frac{3 \\times 29}{87} \\sigma^2 = \\frac{87}{87} \\sigma^2 = \\sigma^2$. This is consistent.\n            And we expect $E[s_H^2] \\approx \\sigma^2$. So we expect the ratio $s_H/\\hat{\\sigma}$ to be around $1$.\n            The question is about the quantiles of the distribution of $s_H/\\hat{\\sigma}$ and $s_L/\\hat{\\sigma}$.\n            Let's reconsider the ratio test $F = s_H^2/s_L^2$. This is the basis for the Goldfeld-Quandt test and is a standard way to compare two variances. The critical value for a one-tailed test at $\\alpha=0.05$ with $(29, 29)$ degrees of freedom is $F_{crit} \\approx 1.85$.\n            The rule in A says $s_H^2/s_L^2 \\gtrsim 2.29$. This corresponds to a p-value less than $0.05$. For $F_{29,29}$, a value of $2.29$ corresponds to a p-value of about $0.015$. So it's significant at the 5% level.\n            But again, where do $1.21$ and $0.80$ come from?\n            Perhaps they are from the distribution of $\\sqrt{\\chi^2_{\\nu_1}/\\nu_1} / \\sqrt{\\chi^2_{\\nu_2}/\\nu_2}$? This would be $\\sqrt{F_{\\nu_1, \\nu_2}}$.\n            What test has a critical value around $1.21$?\n            Maybe it's related to the Hartford test, which tests for heteroscedasticity by regressing absolute residuals on fitted values. Or White's test. Or Breusch-Pagan. The problem describes a specific ad-hoc binned approach, similar to Goldfeld-Quandt.\n            Let's consider another possibility. What if the individual ratios $s_H/\\hat{\\sigma}$ are being tested?\n            The statistic is $(m-1) s_H^2 / (\\sigma^2)$ is $\\chi^2_{m-1}$. The statistic $(n-p)\\hat{\\sigma}^2 / (\\sigma^2)$ is $\\chi^2_{n-p}$.\n            The ratio of two independent chi-squared variables divided by their degrees of freedom is an F-distribution.\n            $F = \\frac{s_H^2/\\sigma^2}{\\hat{\\sigma}^2/\\sigma^2} = \\frac{s_H^2}{\\hat{\\sigma}^2}$. But they are not independent.\n            However, for large $n$, maybe the dependence is weak. If we *assume* independence, $s_H^2/\\hat{\\sigma}^2$ would follow an F-distribution with $(m-1, n-p) = (29, 87)$ degrees of freedom.\n            The upper 2.5% point of $F_{29, 87}$ is about $1.72$. The upper 5% point is about $1.56$.\n            So, we would flag heteroscedasticity if $s_H^2/\\hat{\\sigma}^2 > 1.72$ (two-sided-like test, one side of it) or $s_H^2/\\hat{\\sigma}^2 > 1.56$ (one-sided test).\n            This would mean $s_H/\\hat{\\sigma} > \\sqrt{1.72} \\approx 1.31$ or $s_H/\\hat{\\sigma} > \\sqrt{1.56} \\approx 1.25$.\n            The value $1.21$ in Option A is close to $1.25$.\n            So, $s_H > 1.21 \\hat{\\sigma}$ could correspond to $s_H^2 > 1.46 \\hat{\\sigma}^2$, which is close to the critical value $F_{0.05, 29, 87} \\approx 1.56$. It's a bit lower.\n            What about the lower bound? $s_L < 0.80 \\hat{\\sigma}$ means $s_L^2/\\hat{\\sigma}^2 < 0.64$.\n            The lower critical point for $F_{29, 87}$ at $\\alpha=0.05$ is $1/F_{0.05, 87, 29}$. $F_{0.05, 87, 29} \\approx 1.75$. So the lower critical point is $1/1.75 \\approx 0.57$.\n            The value $0.64$ is close to this.\n            The test in A seems to be a joint test, that $s_H$ is significantly large *and* $s_L$ is significantly small. This is a powerful way to detect fanning-out heteroscedasticity. Each individual condition ($s_H > 1.21 \\hat{\\sigma}$, $s_L < 0.80 \\hat{\\sigma}$) might correspond to a significance level larger than 5% (e.g. 10% or 15%), but the joint event has a smaller probability, which might be around 5%.\n            Let's re-check the F critical values. Using a calculator, $F_{0.05, 29, 87}$ is $1.563$. $\\sqrt{1.563} \\approx 1.25$. So $s_H > 1.25 \\hat{\\sigma}$ would be a one-sided test at 5% significance level, assuming independence. The value $1.21$ is quite close.\n            For the lower end, the critical value would be $F_{0.95, 29, 87} = 1/F_{0.05, 87, 29}$. Using a calculator, $F_{0.05, 87, 29} \\approx 1.75$. So the lower 5% critical value is $1/1.75 \\approx 0.57$. This means we'd check if $s_L^2/\\hat{\\sigma}^2 < 0.57$, or $s_L/\\hat{\\sigma} < \\sqrt{0.57} \\approx 0.755$. The value $0.80$ in Option A is close to this.\n            So, option A combines two conditions, each of which is close to being significant at a one-sided 5% level (if we ignore dependence). The requirement for both to be true ($s_H$ is 'large' and $s_L$ is 'small') makes sense for detecting increasing variance. The calculation of the implied ratio $s_H^2/s_L^2 \\gtrsim 2.29$ is correct, and this value is significant for an F-test on two independent variances of size 30.\n            This option seems statistically plausible, representing a well-constructed heuristic based on formal testing principles.\n\n    *   **Option B**: Flag heteroscedasticity whenever $s_H>1.05\\,\\hat{\\sigma}$, regardless of the residual spread in the other groups.\n        *   This rule flags heteroscedasticity if $s_H^2 > (1.05)^2 \\hat{\\sigma}^2 = 1.1025 \\hat{\\sigma}^2$.\n        *   As calculated before, the 5% critical value for $s_H^2/\\hat{\\sigma}^2$ (assuming independence, pretending it's an F-test) would be around $1.56$.\n        *   The value $1.1025$ is very low. This test would have a very high Type I error rate. Under homoscedasticity, we would expect $s_H \\approx \\hat{\\sigma}$. So $s_H/\\hat{\\sigma}$ will fluctuate around 1. A threshold of $1.05$ is too close to 1 to be a meaningful check at the 5% significance level.\n        *   Consider the distribution of $s_H^2/\\hat{\\sigma}^2$. It's a ratio of variances. Let's see what p-value $F=1.1025$ corresponds to for $F_{29, 87}$. It would be very high, probably around 35-40%. So this is not a 5% level test.\n        *   Therefore, this rule is inappropriate. It's too sensitive.\n\n    *   **Option C**: Flag heteroscedasticity if the average absolute residual in the middle group exceeds the overall standard error of the estimate, that is if $\\frac{1}{m}\\sum_{i\\in M}|e_i|>\\hat{\\sigma}$.\n        *   This compares the mean absolute deviation (MAD) in one group to the root mean squared error (RMSE) for the whole dataset.\n        *   Let's relate MAD to the standard deviation. For a normal distribution $N(0, \\sigma^2)$, the expected value of the absolute value is $E[|X|] = \\sigma \\sqrt{2/\\pi}$.\n        *   So, under homoscedasticity, we expect $E\\left[\\frac{1}{m}\\sum_{i\\in M}|e_i|\\right] \\approx \\sigma \\sqrt{2/\\pi}$.\n        *   The statistic in the rule is $\\frac{1}{m}\\sum_{i\\in M}|e_i|$, which is an estimator of $E[|e|] \\approx \\sigma \\sqrt{2/\\pi}$.\n        *   The rule is to compare this to $\\hat{\\sigma}$, which is an estimator of $\\sigma$.\n        *   So the rule is roughly equivalent to checking if $\\sigma \\sqrt{2/\\pi} > \\sigma$.\n        *   $\\sqrt{2/\\pi} \\approx \\sqrt{0.6366} \\approx 0.798$.\n        *   So the rule is to check if $\\approx 0.8\\sigma > \\sigma$, which is never true.\n        *   This means the average absolute residual is almost always *less* than the standard deviation. So this rule would essentially never flag heteroscedasticity.\n        *   The condition $\\frac{1}{m}\\sum_{i\\in M}|e_i|>\\hat{\\sigma}$ is very unlikely to be met even with severe heteroscedasticity.\n        *   Furthermore, this rule looks at the middle group ($M$), which is the least informative for detecting a monotonic trend in variance. We should be comparing the extremes ($L$ and $H$).\n        *   This option is statistically unsound.\n\n    *   **Option D**: Flag heteroscedasticity if the difference in residual spreads between the high and low groups exceeds one-tenth of the overall standard error of the estimate, that is if $s_H-s_L>0.10\\,\\hat{\\sigma}$.\n        *   This proposes a check on the difference of standard deviations, $s_H-s_L$.\n        *   Tests for equality of variance are typically based on ratios ($F$-test), not differences. The distribution of a difference of sample standard deviations is complex and not standard.\n        *   Let's consider the scale. Is $0.10 \\hat{\\sigma}$ a reasonable threshold?\n        *   Under $H_0$, both $s_H$ and $s_L$ are estimates of $\\sigma$, and they will fluctuate around it. Their difference $s_H - s_L$ will fluctuate around 0. The standard error of a sample standard deviation $s$ (from a normal distribution) is approximately $\\sigma/\\sqrt{2(m-1)}$.\n        *   Here $m = 30$, so the standard error of $s_H$ is approx $\\sigma/\\sqrt{2(29)} = \\sigma/\\sqrt{58} \\approx 0.13 \\sigma$.\n        *   The variance of the difference, assuming independence, is $Var(s_H - s_L) = Var(s_H) + Var(s_L) \\approx (\\sigma^2/58) + (\\sigma^2/58) = 2\\sigma^2/58 = \\sigma^2/29$.\n        *   The standard deviation of the difference is $\\sqrt{\\sigma^2/29} = \\sigma/\\sqrt{29} \\approx 0.186 \\sigma$.\n        *   So, a difference of $s_H - s_L$ of about $0.1 \\sigma$ is only about half a standard deviation away from the expected mean of 0.\n        *   Using a normal approximation for the difference, a 5% one-sided test would require a critical value of $1.645$ standard deviations. So we would need $s_H - s_L > 1.645 \\times (0.186 \\sigma) \\approx 0.306 \\sigma$.\n        *   The threshold $0.10 \\hat{\\sigma}$ is far too small. This rule would be overly sensitive and lead to a very high Type I error rate.\n        *   Using differences of standard deviations is also not a standard or robust method. Ratios are preferred as they are scale-invariant.\n\n    *   **Option E**: Flag heteroscedasticity if the maximum residual in the high-fitted group exceeds three times the overall standard error of the estimate, that is if $\\max_{i\\in H}|e_i|>3\\,\\hat{\\sigma}$.\n        *   This is a rule for detecting outliers, not heteroscedasticity.\n        *   An outlier is an individual point that is far from the fitted regression line. Heteroscedasticity is a systematic change in the spread of the residuals.\n        *   While a group with higher variance is more likely to produce a large residual, this rule only uses information from a single data point ($\\max_{i\\in H}|e_i|$). A good test for heteroscedasticity should use an aggregate measure of spread from the group, like the sample variance or standard deviation, which involves all points in the group.\n        *   The rule of thumb $|e_i| > 3\\hat{\\sigma}$ is indeed used to flag potential outliers. The Bonferroni correction is often applied when checking for the maximum of many residuals. For $n=90$ residuals, the probability of seeing at least one residual with $|e_i/\\hat{\\sigma}|>3$ can be significant even under homoscedasticity. If we look just at the group $H$ with $m=30$ points, the chance is smaller but still non-negligible.\n        *   The primary flaw is that this is a test for an outlier, not a systematic change in variance. A single outlier could trigger this rule even if the variance is constant. Conversely, a modest increase in variance across all 30 points in group H might not produce any single residual larger than $3\\hat{\\sigma}$, but would be clear evidence of heteroscedasticity.\n        *   This rule is not appropriate for its stated purpose of detecting heteroscedasticity.\n\n    *   **Revisiting Option A**:\n        *   This option proposes a rule that is specific to a fanning-out pattern of heteroscedasticity, which is very common.\n        *   It uses summary statistics ($s_H$, $s_L$) that incorporate information from all residuals in the extreme groups.\n        *   It correctly identifies that the ratio $s_H^2/s_L^2$ is the key quantity and calculates the implied threshold correctly.\n        *   It correctly notes that a ratio of $2.29$ is large. We verified that this ratio is statistically significant at a conventional level ($\\alpha=0.05$).\n        *   Let's check the numbers again. If we are doing a two-sided F-test for $s_H^2/s_L^2$ at $\\alpha=0.05$, the upper critical value is $F_{0.025, 29, 29} \\approx 2.15$. The value $2.29$ is in the rejection region.\n        *   The question asks for a rule that \"quantifies thresholds ... at approximately a 5% significance level\".\n        *   The implied F-statistic from option A is $2.29$. The p-value for $F_{29,29} > 2.29$ is approximately $0.014$ (one-tailed). This is significant at the 5% and 2.5% levels. So this rule is consistent with a 5% significance level.\n        *   What is the role of the individual conditions $s_H > 1.21 \\hat{\\sigma}$ and $s_L < 0.80 \\hat{\\sigma}$?\n        *   They give a 'mechanism' for the large ratio. It's not just any large ratio; it's one caused by $s_H$ being larger than the average spread and $s_L$ being smaller. This precisely describes the fanning-out pattern.\n        *   Could there be a test based on $(s_H - s_L)/\\hat{\\sigma}$ or something similar? We dismissed this in Option D because the distribution is complicated.\n        *   Let's check the numbers again. If we are doing a two-sided F-test for $s_H^2/s_L^2$ at $\\alpha=0.05$, the upper critical value is $F_{0.025, 29, 29} \\approx 2.15$. The value $2.29$ is in the rejection region.\n        *   The question asks for a rule that \"quantifies thresholds ... at approximately a 5% significance level\".\n        *   The implied F-statistic from option A is $2.29$. The p-value for an F-distribution with $(29, 29)$ degrees of freedom to exceed $2.29$ is approximately $0.014$ (one-tailed). This is significant at the $5\\%$ level. So this rule is consistent with a $5\\%$ significance level.\n        *   The individual thresholds are also plausible as heuristics, as their squares ($1.46$ and $0.64$) are near the 10% critical points of an approximate F-test against the pooled variance. The joint condition makes the test more specific and likely brings the significance level to the target of approximately $5\\%$.\n        *   Comparison of Option A to the others clearly shows its superiority. It is the only option that is based on sound statistical principles for variance comparison and plausible numerical thresholds.\n\n4.  **Final Polish of the Solution Write-up**:\n    *   Start with the problem validation. State it's valid.\n    *   Introduce the principle of testing for heteroscedasticity: comparing variances of residuals across different levels of fitted values. The null hypothesis is homoscedasticity ($\\sigma_L^2=\\sigma_M^2=\\sigma_H^2=\\sigma^2$).\n    *   The standard test for comparing two variances is the F-test, which uses the ratio of sample variances, $F = s_H^2 / s_L^2$.\n    *   Under $H_0$, and approximating residuals in bins as independent samples, this statistic follows an F-distribution with $(m-1, m-1) = (29, 29)$ degrees of freedom.\n    *   The one-sided 5% critical value $F_{0.05, 29, 29}$ is approximately $1.85$. The two-sided 5% critical value (i.e., upper 2.5% point) $F_{0.025, 29, 29}$ is approximately $2.15$.\n    *   Now analyze each option.\n        *   **A**: The joint condition implies $s_H^2/s_L^2 > (1.21/0.80)^2 \\approx 2.29$. This value of $2.29$ exceeds the critical value $F_{0.025, 29, 29} \\approx 2.15$. Therefore, observing such a large ratio would lead to rejection of the null hypothesis of equal variances at a significance level of $5\\%$ (two-sided) or $2.5\\%$ (one-sided). The condition is well-formulated to detect an increasing trend in variance ($s_H$ large, $s_L$ small). The individual thresholds, $s_H > 1.21\\hat{\\sigma}$ and $s_L < 0.80\\hat{\\sigma}$, link the group spreads to the overall estimated spread, which is a reasonable heuristic. This rule is statistically sound and aligns with the 5% significance level criterion.\n        *   **B**: The rule is $s_H > 1.05\\hat{\\sigma}$, or $s_H^2/\\hat{\\sigma}^2 > 1.1025$. Under $H_0$, both $s_H$ and $\\hat{\\sigma}$ are estimators of $\\sigma$, so their ratio should be close to $1$. A threshold of $1.05$ is far too low to indicate a statistically significant departure from $1$ at the $5\\%$ level. Using an approximate $F_{29, 87}$ distribution, the 5% critical value for $s_H^2/\\hat{\\sigma}^2$ would be about $1.56$. Thus, this rule has a very high Type I error rate.\n        *   **C**: This rule compares the mean absolute deviation (MAD) in the middle group to the overall residual standard error $\\hat{\\sigma}$. For normally distributed residuals with standard deviation $\\sigma$, the expected MAD is $E[|e|] = \\sigma \\sqrt{2/\\pi} \\approx 0.8\\sigma$. The condition is effectively checking if $0.8\\hat{\\sigma} > \\hat{\\sigma}$, which is illogical. This rule would almost never be triggered. Also, it focuses on the middle group, which is less sensitive for detecting trends.\n        *   **D**: This rule uses the difference $s_H - s_L > 0.10\\hat{\\sigma}$. Variance tests are based on ratios, not differences, because ratios are scale-invariant and have well-known distributions (F-dist). The distribution of the difference of standard deviations is complicated. More importantly, the threshold $0.10\\hat{\\sigma}$ is too small. The standard error of the difference $s_H - s_L$ under $H_0$ is approximately $0.186\\sigma$. A threshold of $0.10\\sigma$ is only about half a standard error from zero, leading to a very high rate of false positives.\n        *   **E**: This rule, $\\max_{i\\in H}|e_i|>3\\,\\hat{\\sigma}$, is a classic rule of thumb for identifying individual outliers, not for diagnosing heteroscedasticity, which is a systematic property of the error distribution. A group of residuals can have a higher variance without any single residual being a \"3-sigma\" outlier. Conversely, one outlier can exist even if the variance is constant. This rule is a misapplication of an outlier detection tool to a variance problem.\n    *   Conclusion: Option A provides the most statistically appropriate diagnostic rule. It is based on a ratio of variances, consistent with standard F-tests, and the stated thresholds correspond to a detection criterion at approximately the 5% significance level.\n\n5.  **Final Answer Formulation**:\n    *   The single correct option is A.\n    *   The final answer should be `$$\\boxed{A}$$`.\n\nEverything appears to be in order. I will now generate the output based on this thought process.\n---\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a standard procedure in regression diagnostics (checking for heteroscedasticity) and asks to evaluate several heuristic rules based on established statistical principles. The provided data ($n=90$, $p=3$, $m=30$) are sufficient and consistent. The problem is valid.\n\nThe central task is to identify the most appropriate diagnostic rule for heteroscedasticity among the given options, where \"appropriate\" implies being statistically principled and calibrated to roughly a $5\\%$ significance level. The null hypothesis ($H_0$) is homoscedasticity, i.e., the error variance $\\sigma^2$ is constant across all levels of the fitted values $\\hat{y}_i$. The alternative hypothesis ($H_a$) is heteroscedasticity, specifically a pattern where the variance increases with the fitted value, meaning $\\sigma_L^2 < \\sigma_M^2 < \\sigma_H^2$.\n\nA standard statistical method for comparing the variances of two populations is the F-test, which examines the ratio of the sample variances. In this context, the most relevant comparison is between the low-fitted group ($L$) and the high-fitted group ($H$). The sample variances are $s_L^2$ and $s_H^2$. The F-statistic would be $F = s_H^2 / s_L^2$.\n\nThe sample variance for each group is calculated from $m=30$ residuals. The degrees of freedom for each estimate is approximately $\\nu = m-1 = 29$. (The exact degrees of freedom are slightly more complex because the residuals are not fully independent, but this is a standard and reasonable approximation for a heuristic rule). Under $H_0$, the ratio $F = s_H^2 / s_L^2$ follows an F-distribution with $(\\nu_H, \\nu_L) = (29, 29)$ degrees of freedom.\n\nTo test at a $5\\%$ significance level for the alternative $H_a: \\sigma_H^2 > \\sigma_L^2$, we would use a one-tailed test. The critical value is $F_{0.05, 29, 29} \\approx 1.85$. If we were conducting a two-tailed test (i.e., testing for any difference, $\\sigma_H^2 \\neq \\sigma_L^2$), we would find the upper critical value at the $5\\%/2 = 2.5\\%$ level, which is $F_{0.025, 29, 29} \\approx 2.15$. A diagnostic rule is considered appropriate if its rejection threshold corresponds to an F-statistic in this range or larger.\n\nI will now evaluate each option based on these principles.\n\n**A. Flag heteroscedasticity if the high-fitted group’s residual spread is simultaneously large relative to $\\hat{\\sigma}$ and the low-fitted group’s residual spread is small relative to $\\hat{\\sigma}$, namely if $s_H>1.21\\,\\hat{\\sigma}$ and $s_L<0.80\\,\\hat{\\sigma}$; this joint condition implies a large spread ratio $s_H^2/s_L^2\\gtrsim 2.29$.**\nThis rule specifically targets the common \"fanning out\" pattern of heteroscedasticity. Let's verify the implication. If $s_H > 1.21\\,\\hat{\\sigma}$ and $s_L < 0.80\\,\\hat{\\sigma}$, then the ratio of their variances is $s_H^2 / s_L^2 > (1.21\\,\\hat{\\sigma})^2 / (0.80\\,\\hat{\\sigma})^2 = (1.21/0.80)^2 = (1.5125)^2 \\approx 2.2877$. The stated value of $2.29$ is correct.\nWe compare this implied F-statistic of $2.29$ to our critical values. Since $2.29 > F_{0.025, 29, 29} \\approx 2.15$, this condition indicates a statistically significant difference in variances at a $5\\%$ (two-tailed) or even $2.5\\%$ (one-tailed) level. The joint condition on $s_H$ and $s_L$ relative to the overall estimate $\\hat{\\sigma}$ is a sensible way to operationalize the test and ensure the large ratio is due to the specific pattern of interest. This rule is statistically principled and consistent with the specified significance level.\n**Verdict: Correct**\n\n**B. Flag heteroscedasticity whenever $s_H>1.05\\,\\hat{\\sigma}$, regardless of the residual spread in the other groups.**\nThis rule flags heteroscedasticity if $s_H^2 > (1.05)^2\\,\\hat{\\sigma}^2 \\approx 1.10\\,\\hat{\\sigma}^2$. Under the null hypothesis, both $s_H^2$ and $\\hat{\\sigma}^2$ are estimators of the same variance $\\sigma^2$, so their ratio is expected to be near $1$. A threshold of $1.10$ is very close to $1$. The proper test statistic for comparing $s_H^2$ to $\\hat{\\sigma}^2$ would be $s_H^2_H/\\hat{\\sigma}^2$, which, if we approximate its distribution as $F_{m-1, n-p} = F_{29, 87}$, has a $5\\%$ one-sided critical value of approximately $1.56$. The threshold of $1.10$ is far below this critical value, implying this rule would have a very high Type I error rate (probability of falsely detecting heteroscedasticity). It is not a test at the $5\\%$ significance level.\n**Verdict: Incorrect**\n\n**C. Flag heteroscedasticity if the average absolute residual in the middle group exceeds the overall standard error of the estimate, that is if $\\frac{1}{m}\\sum_{i\\in M}|e_i|>\\hat{\\sigma}$.**\nThis rule compares two different measures of spread: the mean absolute deviation (MAD) for a subgroup versus the root mean squared error (RMSE or $\\hat{\\sigma}$) for the entire sample. For a normal distribution with mean $0$ and standard deviation $\\sigma$, the expected value of an absolute residual is $E[|e|]=\\sigma\\sqrt{2/\\pi}\\approx 0.798\\sigma$. Thus, under homoscedasticity, we expect the sample MAD to be approximately $0.8\\hat{\\sigma}$. The condition $\\frac{1}{m}\\sum_{i\\in M}|e_i|>\\hat{\\sigma}$ is therefore an extremely stringent criterion that is unlikely to be met even under severe heteroscedasticity. It is equivalent to expecting $0.8\\sigma > \\sigma$, which is false. Furthermore, focusing on the middle group is inefficient for detecting a monotonic trend in variance.\n**Verdict: Incorrect**\n\n**D. Flag heteroscedasticity if the difference in residual spreads between the high and low groups exceeds one-tenth of the overall standard error of the estimate, that is if $s_H-s_L>0.10\\,\\hat{\\sigma}$.**\nHypothesis tests for variances are based on ratios ($F$-test), not differences. The distribution of the difference between two sample standard deviations is not straightforward and depends on the underlying variance itself, which is not ideal. Moreover, the threshold $0.10\\,\\hat{\\sigma}$ is not statistically justified. Under $H_0$, the standard deviation of the difference $s_H-s_L$ can be approximated as $\\sigma\\sqrt{1/(2(m-1)) + 1/(2(m-1))} = \\sigma/\\sqrt{m-1} = \\sigma/\\sqrt{29} \\approx 0.186\\sigma$. A threshold of $0.10\\hat{\\sigma}$ is only about half a standard error away from the expected difference of $0$, which would lead to a very high Type I error rate. A $5\\%$ significance level would require a threshold closer to $1.645 \\times 0.186\\hat{\\sigma} \\approx 0.31\\hat{\\sigma}$.\n**Verdict: Incorrect**\n\n**E. Flag heteroscedasticity if the maximum residual in the high-fitted group exceeds three times the overall standard error of the estimate, that is if $\\max_{i\\in H}|e_i|>3\\,\\hat{\\sigma}$.**\nThis diagnostic rule is designed to detect individual outliers, not heteroscedasticity. Heteroscedasticity is a systematic change in the spread of a group of residuals, whereas an outlier is a single point with a large residual. A group's variance can increase significantly without producing any single outlier greater than $3\\hat{\\sigma}$. Conversely, a single large outlier can trigger this rule even if the true error variance is constant. This rule misuses a tool for outlier detection to diagnose a problem of non-constant variance.\n**Verdict: Incorrect**\n\nConclusion: Option A is the only rule that is based on sound statistical principles for comparing variances (via a ratio, analogous to an F-test), is specified for a common pattern of heteroscedasticity, and uses a quantitative threshold that corresponds to a conventional significance level of approximately $5\\%$.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Beyond visual inspection, we can quantitatively assess model assumptions like the normality of errors. This exercise guides you through the process of using $\\hat{\\sigma}$ to test if the number of large residuals in your dataset is consistent with what you'd expect under a normal distribution . This practice is a classic example of how to derive a simple diagnostic test from first principles to evaluate the adequacy of your model.",
            "id": "4953182",
            "problem": "A biostatistics team fits an Ordinary Least Squares (OLS) multiple linear regression to model log-transformed C-reactive protein in a cohort of $n = 200$ adults using age, body mass index, and sex as predictors. The standard error of the estimate, denoted $\\hat{\\sigma}$, computed from the residuals $e_{i}$ of the fitted model, equals $\\hat{\\sigma} = 0.62$ on the log scale. Under the standard linear model assumptions, the residuals are independent and identically distributed with mean $0$ and variance $\\sigma^{2}$, and $\\hat{\\sigma}$ is a consistent estimator of $\\sigma$. In the observed data, $O = 15$ residuals satisfy $|e_{i}| > 2\\hat{\\sigma}$.\n\nStarting from the definitions of residuals and the cumulative distribution function (CDF) of the standard normal distribution, and without assuming any specific numeric tail probability, proceed as follows:\n\n1) Using only the normality and independence assumptions for residuals and the fact that $\\hat{\\sigma}$ estimates $\\sigma$, derive an exact expression for the expected number $E$ of residuals with $|e_{i}| > 2\\hat{\\sigma}$ in terms of the standard normal CDF.\n\n2) Model the indicator of the event $\\{|e_{i}| > 2\\hat{\\sigma}\\}$ as a Bernoulli random variable and derive the sampling variance of the count of exceedances. From this, construct the standardized deviation\n$$\nZ \\;=\\; \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}},\n$$\nwhere $X$ is the count of residuals with $|e_{i}| > 2\\hat{\\sigma}$.\n\n3) Evaluate $Z$ numerically under the normality assumption. Round your answer to four significant figures. Express your final answer as a pure number with no units.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in standard statistical theory, internally consistent, and well-posed under the explicitly stated assumptions. The problem provides all necessary data to proceed with a unique solution. We will now derive the solution in three parts as requested.\n\nThe core of the problem is to evaluate whether the observed number of large residuals, $O=15$, is consistent with the assumption that the residuals follow a normal distribution. We are given a sample of size $n=200$, and the standard error of the estimate is $\\hat{\\sigma} = 0.62$. The residuals $e_i$ are assumed to be independent and identically distributed (i.i.d.) as $N(0, \\sigma^2)$, where $\\hat{\\sigma}$ is a consistent estimator of $\\sigma$.\n\nFirst, we derive an exact expression for the expected number, $E$, of residuals whose absolute value exceeds twice the standard error of the estimate. Let $I_i$ be an indicator random variable for the event $|e_i| > 2\\hat{\\sigma}$, for $i=1, 2, \\dots, n$. That is,\n$$ I_i = \\begin{cases} 1 & \\text{if } |e_i| > 2\\hat{\\sigma} \\\\ 0 & \\text{if } |e_i| \\le 2\\hat{\\sigma} \\end{cases} $$\nThe total number of such residuals is a random variable $X = \\sum_{i=1}^{n} I_i$. The expected number of exceedances, $E$, is the expectation of $X$. By the linearity of expectation,\n$$ E = E[X] = E\\left[\\sum_{i=1}^{n} I_i\\right] = \\sum_{i=1}^{n} E[I_i] $$\nThe expectation of an indicator variable is the probability of the event it indicates. Let this probability be $p$.\n$$ E[I_i] = P(|e_i| > 2\\hat{\\sigma}) = p $$\nSince the residuals are assumed to be identically distributed, this probability $p$ is the same for all $i$. Therefore, the expected number is $E = np$.\n\nTo calculate $p$, we use the normality assumption, $e_i \\sim N(0, \\sigma^2)$. Dividing by $\\sigma$, we get that $e_i/\\sigma$ follows a standard normal distribution, $N(0, 1)$. The problem states that $\\hat{\\sigma}$ is a consistent estimator of $\\sigma$. For a large sample size ($n=200$), we can approximate $\\sigma$ with its estimate $\\hat{\\sigma}$ to calculate the probability.\n$$ p = P(|e_i| > 2\\hat{\\sigma}) \\approx P\\left(\\left|\\frac{e_i}{\\sigma}\\right| > 2\\right) $$\nLet $Z_{std}$ be a random variable with the standard normal distribution, $Z_{std} \\sim N(0, 1)$. Let $\\Phi(z)$ be its cumulative distribution function (CDF), i.e., $\\Phi(z) = P(Z_{std} \\le z)$. The probability $p$ can be expressed as:\n$$ p = P(|Z_{std}| > 2) = P(Z_{std} > 2) + P(Z_{std} < -2) $$\nUsing the properties of the CDF and the symmetry of the normal distribution about $0$, we have:\n$$ P(Z_{std} > 2) = 1 - P(Z_{std} \\le 2) = 1 - \\Phi(2) $$\n$$ P(Z_{std} < -2) = \\Phi(-2) = 1 - \\Phi(2) $$\nTherefore, the probability $p$ is:\n$$ p = [1 - \\Phi(2)] + [1 - \\Phi(2)] = 2(1 - \\Phi(2)) $$\nSubstituting this expression for $p$ into the formula for $E$, we obtain the exact expression for the expected number of exceedances in terms of $n$ and the standard normal CDF:\n$$ E = np = 2n(1 - \\Phi(2)) $$\n\nSecond, we model the count of exceedances and derive the standardized deviation $Z$. The indicator variables $I_i$ are Bernoulli random variables with success probability $p = 2(1 - \\Phi(2))$. Since the residuals $e_i$ are assumed to be independent, the indicator variables $I_i$ are also independent. The count of exceedances, $X = \\sum_{i=1}^{n} I_i$, is the sum of $n$ i.i.d. Bernoulli trials. Thus, $X$ follows a binomial distribution, $X \\sim \\text{Bin}(n, p)$.\nThe variance of a binomial random variable is given by $\\operatorname{Var}(X) = np(1-p)$. Substituting the expression for $p$:\n$$ \\operatorname{Var}(X) = n [2(1 - \\Phi(2))] [1 - 2(1 - \\Phi(2))] = 2n(1 - \\Phi(2))(2\\Phi(2) - 1) $$\nThe standardized deviation $Z$ is defined as the difference between the observed count $O$ and the expected count $E$, divided by the standard deviation of the count.\n$$ Z = \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}} $$\nSubstituting the derived expressions for $E$ and $\\operatorname{Var}(X)$, we have:\n$$ Z = \\frac{O - 2n(1 - \\Phi(2))}{\\sqrt{2n(1 - \\Phi(2))(2\\Phi(2) - 1)}} $$\n\nThird, we evaluate $Z$ numerically. The given values are $n=200$ for the sample size and $O=15$ for the observed number of exceedances. We use the standard value for the normal CDF at $z=2$, which is $\\Phi(2) \\approx 0.97724987$.\nFirst, we calculate the probability $p$:\n$$ p = 2(1 - \\Phi(2)) \\approx 2(1 - 0.97724987) = 2(0.02275013) = 0.04550026 $$\nNext, we calculate the expected number of exceedances, $E$:\n$$ E = np \\approx 200 \\times 0.04550026 = 9.100052 $$\nThen, we calculate the variance of the count, $\\operatorname{Var}(X)$:\n$$ \\operatorname{Var}(X) = np(1-p) \\approx 9.100052 \\times (1 - 0.04550026) = 9.100052 \\times 0.95449974 \\approx 8.68594 $$\nThe standard deviation is the square root of the variance:\n$$ \\sqrt{\\operatorname{Var}(X)} \\approx \\sqrt{8.68594} \\approx 2.947192 $$\nFinally, we compute the value of the $Z$ statistic:\n$$ Z = \\frac{O - E}{\\sqrt{\\operatorname{Var}(X)}} \\approx \\frac{15 - 9.100052}{2.947192} = \\frac{5.899948}{2.947192} \\approx 2.00190 $$\nRounding the result to four significant figures, as requested, gives $Z = 2.002$. This value indicates that the observed number of residuals with absolute values greater than $2\\hat{\\sigma}$ is approximately $2$ standard deviations above what would be expected under the assumption of normality.",
            "answer": "$$\\boxed{2.002}$$"
        },
        {
            "introduction": "A model's fit to the data used for training, summarized by the in-sample $\\hat{\\sigma}$, can be misleadingly optimistic. This hands-on coding practice demonstrates how to quantify this optimism by comparing the in-sample error to a more robust estimate of out-of-sample predictive error using the bootstrap method . Completing this exercise will provide you with a practical tool for assessing a model's real-world performance and understanding the problem of overfitting.",
            "id": "4953185",
            "problem": "You will implement a complete, runnable program that quantifies and contrasts two notions of variability for linear regression in a biostatistical prediction setting: an in-sample residual standard error and an out-of-sample predictive residual standard error estimated by a nonparametric bootstrap. Work from the fundamental linear model and sampling definitions, and avoid using any pre-stated shortcut formulas for the target quantities. Your implementation must be self-contained and reproducible.\n\nConsider the ordinary least squares linear model with intercept, where a response vector $y \\in \\mathbb{R}^n$ relates to a design matrix with an intercept column $X_{\\text{tilde}} \\in \\mathbb{R}^{n \\times q}$, where $q = p + 1$ includes $p$ predictors and the intercept. The model assumes $Y = X_{\\text{tilde}} \\beta + \\varepsilon$ with independent errors of mean $0$ and constant variance $\\sigma^2$. Let $\\hat{\\beta}$ be the ordinary least squares estimator, fitted by minimizing the sum of squared residuals, and let fitted values be $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$ and residuals be $e = y - \\hat{y}$. Define the in-sample residual standard error as the square root of an unbiased estimator of the error variance based on the residual degrees of freedom.\n\nDefine the out-of-sample predictive residual standard error as the square root of the expected mean squared prediction error for new observations from the same data-generating mechanism. Because the finite-sample distribution of this predictive loss depends on the unknown population and on estimation variability, you must estimate it using a nonparametric bootstrap-of-pairs with out-of-bag evaluation, according to the following procedure.\n\nYou must implement the following algorithmic steps from first principles.\n\n- In-sample residual standard error.\n  1. Fit the ordinary least squares model with an intercept to the full sample.\n  2. Compute the vector of residuals and the residual degrees of freedom as $n - q$, where $q$ is the number of fitted coefficients including the intercept.\n  3. Form the unbiased estimator of the error variance from the residuals and take its square root to obtain the in-sample residual standard error.\n\n- Out-of-sample bootstrap predictive residual standard error (bootstrap-of-pairs with out-of-bag evaluation).\n  1. For a specified number of bootstrap replicates $B$, repeat:\n     a. Draw a bootstrap sample by sampling $n$ indices with replacement from $\\{1,\\dots,n\\}$.\n     b. Fit the ordinary least squares model with an intercept to the bootstrap sample.\n     c. Identify the out-of-bag set for this replicate as those indices in $\\{1,\\dots,n\\}$ not selected in the bootstrap sample.\n     d. For each out-of-bag index $i$, compute the out-of-bag prediction error as $y_i - \\hat{y}_i^{(b)}$, where $\\hat{y}_i^{(b)}$ is the prediction from the model fitted on the bootstrap sample.\n  2. Aggregate all out-of-bag squared prediction errors across all replicates and divide by the total number of out-of-bag predictions to obtain the out-of-bag mean squared error.\n  3. Take the square root to obtain the bootstrap predictive residual standard error.\n\nYour program must compute, for each test case, the following three quantities:\n- the in-sample residual standard error,\n- the bootstrap predictive residual standard error,\n- the difference between these two values (bootstrap predictive minus in-sample).\n\nAll quantities must be floats.\n\nData generation for the test suite must follow a reproducible design that is fully specified here. For each test case, you must:\n- Set the random seed to the given value using a modern pseudorandom number generator.\n- Generate $X \\in \\mathbb{R}^{n \\times p}$ with independent standard normal entries.\n- Form $X_{\\text{tilde}} = [\\mathbf{1}, X]$, where $\\mathbf{1}$ is a column of ones of length $n$.\n- Generate $Y$ according to $Y = \\beta_0 \\mathbf{1} + X \\beta_{\\text{slopes}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ independently across observations, with all parameters specified below.\n\nTest suite to implement exactly:\n- Case $1$: seed $= 1729$, $n = 80$, $p = 3$, $\\beta_0 = 0.7$, slope vector $\\beta_{\\text{slopes}} = [1.5, -2.0, 0.5]$, $\\sigma = 1.2$, $B = 500$.\n- Case $2$: seed $= 2021$, $n = 25$, $p = 5$, $\\beta_0 = -1.0$, slope vector $\\beta_{\\text{slopes}} = [0.6, -0.4, 1.2, 0.0, 0.3]$, $\\sigma = 0.8$, $B = 600$.\n- Case $3$: seed $= 777$, $n = 15$, $p = 8$, $\\beta_0 = 1.0$, slope vector $\\beta_{\\text{slopes}} = [0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]$, $\\sigma = 1.5$, $B = 800$.\n\nImplementation requirements:\n- Use ordinary least squares with an intercept for all fits. If a bootstrap sample is rank-deficient, use a least-squares solution that produces fitted values minimizing the sum of squared residuals (for example, via a Moore–Penrose pseudoinverse approach).\n- The bootstrap out-of-bag aggregation must count each out-of-bag prediction equally. If a replicate has zero out-of-bag observations, it contributes nothing to the aggregate.\n- Numerical stability and reproducibility must be ensured by using the stated seeds.\n\nFinal output format:\n- For each test case, output a list with three floats in the order specified above.\n- Round each float to exactly $6$ decimal places in the final output.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of the three-element lists, enclosed in square brackets. For example: $[[a\\_1,b\\_1,c\\_1],[a\\_2,b\\_2,c\\_2],[a\\_3,b\\_3,c\\_3]]$, where each $a\\_j$, $b\\_j$, $c\\_j$ is a float rounded to $6$ decimals.",
            "solution": "This solution presents a computational framework to quantify and compare two distinct measures of regression model variability: the in-sample residual standard error and an out-of-sample predictive residual standard error estimated via a non-parametric bootstrap. The analysis is grounded in the principles of ordinary least squares (OLS) linear regression and resampling-based model validation.\n\n### The Linear Model and Ordinary Least Squares Estimation\n\nWe consider the standard linear regression model, which posits a linear relationship between a response vector $y \\in \\mathbb{R}^n$ and a set of $p$ predictor variables. The model is expressed as:\n$$\nY = X_{\\text{tilde}} \\beta + \\varepsilon\n$$\nwhere:\n- $Y$ is the $n \\times 1$ random vector of responses.\n- $X_{\\text{tilde}}$ is the $n \\times q$ design matrix, which includes a leading column of ones for the intercept and $p$ columns for the predictors. Thus, $q = p + 1$.\n- $\\beta$ is the $q \\times 1$ vector of unknown true coefficients, with $\\beta = [\\beta_0, \\beta_1, \\dots, \\beta_p]^T$.\n- $\\varepsilon$ is the $n \\times 1$ vector of unobserved random errors, with the assumption that $\\varepsilon_i$ are independent and identically distributed with mean $E[\\varepsilon_i] = 0$ and constant variance $Var(\\varepsilon_i) = \\sigma^2$.\n\nThe ordinary least squares (OLS) estimator, denoted $\\hat{\\beta}$, is found by minimizing the residual sum of squares (RSS). The OLS solution for $\\hat{\\beta}$ is given by the normal equations:\n$$\n\\hat{\\beta} = (X_{\\text{tilde}}^T X_{\\text{tilde}})^{-1} X_{\\text{tilde}}^T y\n$$\nIn cases where $X_{\\text{tilde}}$ is not of full column rank (i.e., its columns are linearly dependent), the matrix $X_{\\text{tilde}}^T X_{\\text{tilde}}$ is singular and cannot be inverted. This can occur in bootstrap samples. In such scenarios, a least-squares solution is obtained using the Moore-Penrose pseudoinverse, $(X_{\\text{tilde}}^T X_{\\text{tilde}})^{\\dagger}$, which provides a solution that minimizes the Euclidean norm of the residuals, $\\|y - X_{\\text{tilde}}\\beta\\|_2$.\n\nOnce $\\hat{\\beta}$ is computed, the fitted values are $\\hat{y} = X_{\\text{tilde}} \\hat{\\beta}$, and the residuals are $e = y - \\hat{y}$.\n\n### In-Sample Residual Standard Error (RSE)\n\nThe in-sample RSE is an estimate of $\\sigma$, the standard deviation of the error term $\\varepsilon$. It quantifies the typical deviation of the observed data points from the fitted regression line based on the same data used to fit the model.\n\n1.  **Residual Sum of Squares (RSS)**: This is the sum of the squared differences between the observed and fitted values:\n    $$\n    RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = e^T e\n    $$\n\n2.  **Unbiased Variance Estimator**: To obtain an unbiased estimator of the error variance $\\sigma^2$, the RSS is divided by the residual degrees of freedom, $df = n - q$. The degrees of freedom are reduced by $q$ to account for the $q$ coefficients estimated from the data.\n    $$\n    \\hat{\\sigma}^2 = \\frac{RSS}{n - q}\n    $$\n\n3.  **Residual Standard Error**: The RSE is the square root of this unbiased variance estimator:\n    $$\n    RSE_{\\text{in-sample}} = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{e^T e}{n - q}}\n    $$\nThe in-sample RSE is often an optimistic (i.e., downwardly biased) estimate of the model's predictive error on new, unseen data, as the model has been optimized to minimize error on the training sample.\n\n### Bootstrap Predictive Residual Standard Error\n\nTo obtain a more realistic estimate of a model's predictive performance, we use a resampling method that simulates the process of fitting the model to new data. The non-parametric bootstrap-of-pairs with out-of-bag (OOB) evaluation is a computationally intensive but powerful technique for this purpose. It estimates the expected prediction error on data points not used in a model's training.\n\nThe algorithm proceeds as follows for $B$ bootstrap replicates:\n\n1.  **Bootstrap Sampling**: For each replicate $b \\in \\{1, \\dots, B\\}$, a bootstrap sample of size $n$ is created by drawing indices with replacement from the original set of indices $\\{1, \\dots, n\\}$. Let the set of chosen indices be $I^{(b)}$. The bootstrap data is $(X_{\\text{tilde}}^{(b)}, y^{(b)})$, corresponding to these indices.\n\n2.  **Model Fitting**: An OLS model is fitted to the bootstrap sample $(X_{\\text{tilde}}^{(b)}, y^{(b)})$, yielding a coefficient vector $\\hat{\\beta}^{(b)}$.\n\n3.  **Out-of-Bag (OOB) Evaluation**: The OOB sample for replicate $b$ consists of the original data points whose indices were not selected in $I^{(b)}$. Let the set of OOB indices be $OOB^{(b)} = \\{1, \\dots, n\\} \\setminus I^{(b)}$.\n    For each index $i \\in OOB^{(b)}$, the model fitted on the bootstrap sample is used to predict the response:\n    $$\n    \\hat{y}_i^{(b)} = \\tilde{x}_i^T \\hat{\\beta}^{(b)}\n    $$\n    where $\\tilde{x}_i^T$ is the $i$-th row of the original design matrix $X_{\\text{tilde}}$. The squared prediction error for this OOB point is $(y_i - \\hat{y}_i^{(b)})^2$.\n\n4.  **Aggregation**: The out-of-bag Mean Squared Error (MSE$_{\\text{OOB}}$) is calculated by averaging all squared prediction errors computed across all OOB sets from all $B$ replicates.\n    $$\n    \\text{MSE}_{\\text{OOB}} = \\frac{\\sum_{b=1}^{B} \\sum_{i \\in OOB^{(b)}} (y_i - \\hat{y}_i^{(b)})^2}{\\sum_{b=1}^{B} |OOB^{(b)}|}\n    $$\n    where $|OOB^{(b)}|$ is the number of out-of-bag observations for replicate $b$.\n\n5.  **Bootstrap Predictive RSE**: The square root of the MSE$_{\\text{OOB}}$ gives the bootstrap-estimated predictive residual standard error.\n    $$\n    RSE_{\\text{pred}} = \\sqrt{\\text{MSE}_{\\text{OOB}}}\n    $$\nThis value provides a less biased estimate of the model's performance on new data.\n\n### Comparison\nThe difference, $\\Delta = RSE_{\\text{pred}} - RSE_{\\text{in-sample}}$, serves as a measure of \"optimism bias.\" A larger positive value indicates a greater degree of overfitting, where the model performs substantially better on the data it was trained on compared to its expected performance on new data. This discrepancy is particularly pronounced in high-dimensional settings where the number of predictors $p$ is large relative to the sample size $n$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and contrasts in-sample and bootstrap out-of-bag predictive \n    residual standard errors for linear regression models.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"seed\": 1729, \"n\": 80, \"p\": 3, \"beta_0\": 0.7, \n            \"beta_slopes\": np.array([1.5, -2.0, 0.5]), \"sigma\": 1.2, \"B\": 500\n        },\n        {\n            \"seed\": 2021, \"n\": 25, \"p\": 5, \"beta_0\": -1.0, \n            \"beta_slopes\": np.array([0.6, -0.4, 1.2, 0.0, 0.3]), \"sigma\": 0.8, \"B\": 600\n        },\n        {\n            \"seed\": 777, \"n\": 15, \"p\": 8, \"beta_0\": 1.0, \n            \"beta_slopes\": np.array([0.5, -0.5, 0.8, 0.0, 0.3, -0.2, 0.4, 0.1]), \"sigma\": 1.5, \"B\": 800\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Unpack parameters\n        seed = case[\"seed\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n        beta_0 = case[\"beta_0\"]\n        beta_slopes = case[\"beta_slopes\"]\n        sigma = case[\"sigma\"]\n        B = case[\"B\"]\n        \n        # Initialize a modern pseudorandom number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        X = rng.normal(size=(n, p))\n        X_tilde = np.hstack((np.ones((n, 1)), X))\n        beta_full = np.concatenate(([beta_0], beta_slopes))\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = X_tilde @ beta_full + epsilon\n\n        # --- In-sample residual standard error ---\n        q = p + 1\n        # np.linalg.lstsq handles potential rank-deficiency and returns RSS\n        _, rss_array, _, _ = np.linalg.lstsq(X_tilde, y, rcond=None)\n        rss = rss_array[0]\n        df = n - q\n        \n        # Handle case where df <= 0 (more predictors than samples)\n        # In this situation, the in-sample error is 0, and RSE is undefined or 0.\n        if df > 0:\n            mse_in_sample = rss / df\n            rse_in_sample = np.sqrt(mse_in_sample)\n        else:\n            rse_in_sample = 0.0\n\n\n        # --- Out-of-sample bootstrap predictive residual standard error ---\n        total_oob_sq_err = 0.0\n        total_oob_count = 0\n        all_indices = np.arange(n)\n\n        for _ in range(B):\n            # Draw a bootstrap sample of indices\n            boot_indices = rng.choice(all_indices, size=n, replace=True)\n            \n            # Identify out-of-bag (OOB) indices\n            # unique() is important as bootstrap samples may have duplicates\n            oob_indices = np.setdiff1d(all_indices, np.unique(boot_indices), assume_unique=True)\n\n            if len(oob_indices) == 0:\n                continue\n            \n            # Create bootstrap and OOB data sets\n            X_boot, y_boot = X_tilde[boot_indices], y[boot_indices]\n            X_oob, y_oob = X_tilde[oob_indices], y[oob_indices]\n\n            # Fit OLS model on bootstrap sample\n            # This automatically uses a pseudoinverse for rank-deficient matrices\n            beta_boot, _, _, _ = np.linalg.lstsq(X_boot, y_boot, rcond=None)\n\n            # Predict on OOB data\n            y_pred_oob = X_oob @ beta_boot\n\n            # Accumulate squared errors and counts\n            total_oob_sq_err += np.sum((y_oob - y_pred_oob)**2)\n            total_oob_count += len(oob_indices)\n        \n        # Calculate OOB MSE and the predictive RSE\n        if total_oob_count > 0:\n            mse_oob = total_oob_sq_err / total_oob_count\n            rse_pred = np.sqrt(mse_oob)\n        else:\n            # Fallback if no OOB samples were ever generated (highly unlikely)\n            rse_pred = np.nan\n\n        # Calculate the difference\n        difference = rse_pred - rse_in_sample\n\n        results.append([rse_in_sample, rse_pred, difference])\n\n    # Format the final output string as per requirements\n    output_parts = []\n    for res in results:\n        # rounding to exactly 6 decimal places\n        part = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_str = f\"[{','.join(output_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}