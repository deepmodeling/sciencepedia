{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of making inferences about a population correlation, $\\rho$, is the ability to construct a confidence interval from a sample correlation, $r$. This practice guides you through the fundamental mechanics of using the Fisher Z-transformation to achieve this . You will not only apply the standard procedure but also explore a bias-corrected version, highlighting the nature of this method as a powerful approximation that can be refined, especially when dealing with smaller sample sizes.",
            "id": "4915699",
            "problem": "A clinical study investigates the association between a continuous biomarker and a continuous clinical severity score measured simultaneously on a sample of $n=12$ independent patients. Assume the paired measurements arise from a bivariate normal distribution with unknown population correlation $\\rho$. The observed sample Pearson correlation is $r=0.70$. Using the framework of large-sample inference for correlations based on Fisher's $z$ transformation and its small-sample bias properties:\n\n1. Construct a $95\\%$ confidence interval (CI) for $\\rho$ using the standard Fisher $z$ approach.\n2. Construct a $95\\%$ CI for $\\rho$ using a bias-corrected Fisher $z$ approach that removes the first-order small-sample bias in the transformed scale.\n3. Compute the widths of both CIs and determine whether the plausible true correlation value $\\rho_0=0.65$ would be contained in each CI.\n4. Report, as a single number, the ratio of the width of the bias-corrected CI to the width of the standard CI. Round your final reported ratio to four significant figures.",
            "solution": "The problem requires the construction and comparison of two types of confidence intervals for a population correlation coefficient, $\\rho$, based on a sample correlation, $r$, from a sample of size $n$. The underlying theory is Fisher's $z$-transformation, which stabilizes the variance and transforms the sampling distribution of the correlation coefficient to be approximately normal.\n\nThe data and parameters provided are:\n- Sample size: $n=12$\n- Sample Pearson correlation: $r=0.70$\n- Confidence level: $95\\%$, which corresponds to a standard normal critical value $z_{\\alpha/2} = z_{0.025}$ for a two-sided interval. The value is $z_{0.025} \\approx 1.95996$.\n- A hypothetical true correlation for evaluation: $\\rho_0=0.65$.\n\n**1. Standard 95% Confidence Interval for $\\rho$**\n\nThe standard approach uses Fisher's $z$-transformation, defined as $z = \\arctanh(r)$. The transformed variable $z$ is approximately normally distributed with mean $\\zeta = \\arctanh(\\rho)$ and standard deviation (which we call the standard error) $\\sigma_z = \\frac{1}{\\sqrt{n-3}}$.\n\nFirst, we compute the transformed sample correlation $z$:\n$$z = \\arctanh(r) = \\arctanh(0.70) = \\frac{1}{2} \\ln\\left(\\frac{1+0.70}{1-0.70}\\right) = \\frac{1}{2} \\ln\\left(\\frac{1.7}{0.3}\\right) \\approx 0.86730$$\n\nNext, we compute the standard error, $SE(z)$:\n$$SE(z) = \\frac{1}{\\sqrt{n-3}} = \\frac{1}{\\sqrt{12-3}} = \\frac{1}{\\sqrt{9}} = \\frac{1}{3}$$\n\nThe $95\\%$ confidence interval for the transformed population parameter $\\zeta$ is given by $z \\pm z_{0.025} \\cdot SE(z)$.\n$$CI_{\\zeta} = 0.86730 \\pm 1.95996 \\times \\frac{1}{3} = 0.86730 \\pm 0.65332$$\nThis gives the interval for $\\zeta$ as $[0.21398, 1.52062]$.\n\nTo obtain the confidence interval for $\\rho$, we apply the inverse transformation, $\\rho = \\tanh(\\zeta)$, to the lower and upper bounds of $CI_{\\zeta}$.\n$$ \\rho_L = \\tanh(0.21398) \\approx 0.21102 $$\n$$ \\rho_U = \\tanh(1.52062) \\approx 0.90897 $$\nThe standard $95\\%$ CI for $\\rho$ is approximately $[0.2110, 0.9090]$.\n\n**2. Bias-Corrected 95% Confidence Interval for $\\rho$**\n\nFisher's $z$-transformation has a small-sample bias. The expected value of $z$ is approximately $E[z] \\approx \\zeta + \\frac{\\rho}{2(n-1)}$. To correct for this, we can subtract an estimate of the bias term from our sample $z$. The bias-corrected estimate of $\\zeta$ is $z_{adj} = z - \\frac{r}{2(n-1)}$.\n\nFirst, we calculate the estimated bias:\n$$ \\text{Bias}(z) \\approx \\frac{r}{2(n-1)} = \\frac{0.70}{2(12-1)} = \\frac{0.70}{22} \\approx 0.03182 $$\n\nNext, we find the adjusted $z$-value:\n$$ z_{adj} = 0.86730 - 0.03182 = 0.83548 $$\n\nThe confidence interval for $\\zeta$ is now centered at $z_{adj}$, using the same standard error and margin of error:\n$$ CI_{\\zeta, corr} = z_{adj} \\pm z_{0.025} \\cdot SE(z) = 0.83548 \\pm 0.65332 $$\nThis gives the bias-corrected interval for $\\zeta$ as $[0.18216, 1.48880]$.\n\nWe transform these bounds back to the $\\rho$ scale:\n$$ \\rho_{L, corr} = \\tanh(0.18216) \\approx 0.18025 $$\n$$ \\rho_{U, corr} = \\tanh(1.48880) \\approx 0.90302 $$\nThe bias-corrected $95\\%$ CI for $\\rho$ is approximately $[0.1803, 0.9030]$.\n\n**3. CI Widths and Containment of $\\rho_0 = 0.65$**\n\nWe compute the widths of both confidence intervals.\nThe width of the standard CI is:\n$$ W_{std} = \\rho_U - \\rho_L = 0.90897 - 0.21102 = 0.69795 $$\n\nThe width of the bias-corrected CI is:\n$$ W_{corr} = \\rho_{U, corr} - \\rho_{L, corr} = 0.90302 - 0.18025 = 0.72277 $$\n\nThe correction shifts the center of the interval on the $z$-scale closer to $0$. Due to the non-linear nature of the $\\tanh$ function (it is less steep near $z=0$), this shift results in a wider confidence interval on the $\\rho$-scale.\n\nNext, we check if $\\rho_0=0.65$ is contained in each interval.\n- Standard CI: $[0.2110, 0.9090]$. Since $0.2110 \\le 0.65 \\le 0.9090$, $\\rho_0$ is contained in the standard CI.\n- Bias-corrected CI: $[0.1803, 0.9030]$. Since $0.1803 \\le 0.65 \\le 0.9030$, $\\rho_0$ is also contained in the bias-corrected CI.\n\n**4. Ratio of CI Widths**\n\nFinally, we compute the ratio of the width of the bias-corrected CI to that of the standard CI.\n$$ \\text{Ratio} = \\frac{W_{corr}}{W_{std}} = \\frac{0.72277}{0.69795} \\approx 1.03556 $$\n\nRounding to four significant figures, the ratio is $1.036$.",
            "answer": "$$\n\\boxed{1.036}\n$$"
        },
        {
            "introduction": "While the Fisher Z-transformation provides an excellent approximation for inference, it's crucial to understand its behavior relative to other methods. This exercise provides a valuable comparative analysis, pitting the approximate Z-transformation method against an exact interval derived from inverting the Student's $t$-test for the special case where $\\rho=0$ . By examining how the confidence interval widths compare across different sample sizes, you will gain a deeper intuition for the regimes where the Fisher Z approximation is not only valid but also highly efficient.",
            "id": "4915692",
            "problem": "A biostatistician is studying the relationship between two continuous biomarkers measured on paired subjects, and assumes a bivariate normal model for the joint distribution of the two biomarkers. From a sample of size $n$ pairs, the sample correlation is observed to be $r=0.0$. The goal is to construct a two-sided $100(1-\\alpha)\\%$ confidence interval for the population correlation $\\rho$ using two approaches: (i) Fisher’s hyperbolic arctangent transformation and (ii) directly inverting the Student’s $t$ test for the null hypothesis $H_{0}:\\rho=0$.\n\nStarting from fundamental sampling-distribution facts under the bivariate normal model, derive the expression for the confidence interval half-width (and hence total width) as a function of $n$ for each method when $r=0.0$ and $\\alpha=0.05$. Using these expressions, evaluate and compare the two widths for representative sample sizes $n=4$, $n=6$, and $n=20$. Based on these comparisons and the large-sample behavior of each method, select the statement that best characterizes the regime in which each method is preferable.\n\nChoose one option:\n\nA. For all $n\\geq 4$, Fisher’s $z$ interval is narrower than the interval obtained by directly inverting the $t$ test at $r=0$, so Fisher’s $z$ is preferable for all $n$.\n\nB. For very small sample sizes (e.g., $n\\leq 6$), directly inverting the $t$ test for $H_{0}:\\rho=0$ yields an equal or narrower interval than Fisher’s $z$ at $r=0$, but for moderate to large $n$ (e.g., $n\\geq 7$), Fisher’s $z$ yields a narrower interval and is preferable.\n\nC. The widths depend only on the observed $r$ and not on $n$; at $r=0$ both methods have the same width for any $n$, so the choice of method is arbitrary.\n\nD. The interval from directly inverting the $t$ test is uniformly narrower than Fisher’s $z$ across all $n$, making it preferable in general.\n\nE. Fisher’s $z$ produces an exact confidence interval with guaranteed nominal coverage for any $n$, hence it is always preferable regardless of $n$.",
            "solution": "The objective is to compare the widths of two-sided $95\\%$ confidence intervals for the population correlation $\\rho$ when the observed sample correlation is $r=0.0$, using two different methods. The comparison will be based on the half-widths of the intervals derived for each method.\n\n#### Method (i): Fisher's Hyperbolic Arctangent ($z$) Transformation\n\nFisher's $z$-transformation is defined as:\n$$ z = \\text{arctanh}(r) = \\frac{1}{2} \\ln \\left(\\frac{1+r}{1-r}\\right) $$\nFor a sample of size $n$ from a bivariate normal distribution, the transformed variable $z$ is approximately normally distributed with mean $\\zeta = \\text{arctanh}(\\rho)$ and standard deviation (standard error) $\\sigma_z \\approx \\frac{1}{\\sqrt{n-3}}$. This approximation is valid for $n>3$.\n\nA $100(1-\\alpha)\\%$ confidence interval for $\\zeta$ is given by:\n$$ z \\pm z_{\\alpha/2} \\sigma_z \\approx \\text{arctanh}(r) \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n-3}} $$\nwhere $z_{\\alpha/2}$ is the upper $\\alpha/2$ quantile of the standard normal distribution. For a $95\\%$ CI, $\\alpha=0.05$ and $z_{0.025} \\approx 1.96$.\n\nGiven $r=0.0$, the observed transformed value is $z = \\text{arctanh}(0) = 0$. The CI for $\\zeta$ is:\n$$ \\left[ -\\frac{z_{0.025}}{\\sqrt{n-3}}, \\frac{z_{0.025}}{\\sqrt{n-3}} \\right] $$\nTo obtain the CI for $\\rho$, we apply the inverse transformation, $\\rho = \\tanh(\\zeta)$:\n$$ \\text{CI}_z(\\rho) = \\left[ \\tanh\\left(-\\frac{z_{0.025}}{\\sqrt{n-3}}\\right), \\tanh\\left(\\frac{z_{0.025}}{\\sqrt{n-3}}\\right) \\right] $$\nSince $\\tanh(-x) = -\\tanh(x)$, the interval is symmetric about $0$. The half-width, which we denote $h_z$, is:\n$$ h_z = \\tanh\\left(\\frac{z_{0.025}}{\\sqrt{n-3}}\\right) $$\n\n#### Method (ii): Inverting the Student’s $t$-test for $H_0: \\rho=0$\n\nThis method constructs an *exact* confidence interval for $\\rho$ by inverting the hypothesis test for $H_0: \\rho = \\rho_0$. When $r=0.0$, the exact CI is symmetric around $0$. Its endpoints are the critical values of $r$ for rejecting the null hypothesis $H_0: \\rho=0$ at a significance level $\\alpha$.\n\nThe test statistic for $H_0: \\rho=0$ is $T = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}$, which follows a Student's $t$-distribution with $\\nu=n-2$ degrees of freedom. We find the critical value of $|r|$, let's call it $r_c$, by setting $|T|$ equal to $t_{\\alpha/2, n-2}$:\n$$ \\frac{r_c\\sqrt{n-2}}{\\sqrt{1-r_c^2}} = t_{\\alpha/2, n-2} $$\nSolving for $r_c$:\n$$ r_c = \\frac{t_{\\alpha/2, n-2}}{\\sqrt{n-2 + t_{\\alpha/2, n-2}^2}} $$\nFor $r=0$, the exact $95\\%$ CI for $\\rho$ is $[-r_c, r_c]$. The half-width, denoted $h_t$, is:\n$$ h_t = \\frac{t_{0.025, n-2}}{\\sqrt{n-2 + t_{0.025, n-2}^2}} $$\n\n#### Numerical Comparison\n\nWe now compare the half-widths $h_z$ and $h_t$ for $\\alpha=0.05$ ($z_{0.025} \\approx 1.95996$) at sample sizes $n=4, 6, 20$.\n\n**For $n=4$**:\n-   $df = n-2 = 2$. From $t$-tables, $t_{0.025, 2} \\approx 4.3027$.\n-   $h_t = \\frac{4.3027}{\\sqrt{2 + (4.3027)^2}} = \\frac{4.3027}{\\sqrt{20.5132}} \\approx 0.9500$.\n-   $h_z = \\tanh\\left(\\frac{1.96}{\\sqrt{4-3}}\\right) = \\tanh(1.96) \\approx 0.9611$.\n-   For $n=4$, $h_t < h_z$. The exact interval is narrower.\n\n**For $n=6$**:\n-   $df = n-2 = 4$. From $t$-tables, $t_{0.025, 4} \\approx 2.7764$.\n-   $h_t = \\frac{2.7764}{\\sqrt{4 + (2.7764)^2}} = \\frac{2.7764}{\\sqrt{11.7084}} \\approx 0.8114$.\n-   $h_z = \\tanh\\left(\\frac{1.96}{\\sqrt{6-3}}\\right) = \\tanh\\left(\\frac{1.96}{\\sqrt{3}}\\right) \\approx \\tanh(1.1316) \\approx 0.8116$.\n-   For $n=6$, $h_t \\approx h_z$, with the exact interval being marginally narrower.\n\n**For $n=20$**:\n-   $df = n-2 = 18$. From $t$-tables, $t_{0.025, 18} \\approx 2.1009$.\n-   $h_t = \\frac{2.1009}{\\sqrt{18 + (2.1009)^2}} = \\frac{2.1009}{\\sqrt{22.4138}} \\approx 0.4438$.\n-   $h_z = \\tanh\\left(\\frac{1.96}{\\sqrt{20-3}}\\right) = \\tanh\\left(\\frac{1.96}{\\sqrt{17}}\\right) \\approx \\tanh(0.4754) \\approx 0.4425$.\n-   For $n=20$, $h_z < h_t$. The Fisher's $z$ interval is narrower.\n\nA check at $n=7$ shows that $h_z < h_t$, indicating the crossover point occurs between $n=6$ and $n=7$. For very small sample sizes ($n \\le 6$), the exact interval is narrower. For moderate to large sample sizes ($n \\ge 7$), the approximate Fisher's $z$ interval becomes narrower.\n\n### Evaluation of Options\n\nA. For all $n\\geq 4$, Fisher’s $z$ interval is narrower...\n**Incorrect**. Our calculations show this is false for $n=4$ and $n=6$.\n\nB. For very small sample sizes (e.g., $n\\leq 6$), directly inverting the $t$ test... yields an equal or narrower interval... but for moderate to large $n$ (e.g., $n\\geq 7$), Fisher’s $z$ yields a narrower interval...\n**Correct**. This statement perfectly aligns with our detailed calculations.\n\nC. The widths depend only on the observed $r$ and not on $n$...\n**Incorrect**. The formulae for $h_z$ and $h_t$ clearly show a dependence on $n$.\n\nD. The interval from directly inverting the $t$ test is uniformly narrower...\n**Incorrect**. Our calculations show this is false for $n \\ge 7$.\n\nE. Fisher’s $z$ produces an exact confidence interval...\n**Incorrect**. The premise is false; the Fisher's $z$ interval is approximate, while the one derived from inverting the test is exact.\n\nTherefore, option B is the only one consistent with the derivations.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "A critical assumption underlying the Fisher Z-transformation is that the data originate from a bivariate normal distribution, a condition often not met in real-world research. This final practice addresses this important challenge, exploring a principled strategy to adapt our methods for non-normal data . By transforming the marginal distributions to be approximately normal while preserving the underlying dependence structure, we can justifiably apply the Fisher Z-transformation, making it a more robust and widely applicable tool in the biostatistician's toolkit.",
            "id": "4915681",
            "problem": "A biostatistics team studies the association between two serum biomarkers, denoted by $X$ and $Y$, measured in $n=120$ independent patients. Exploratory analysis shows that each marginal distribution is continuous, strictly positive, and markedly right-skewed with heavy tails. A scatterplot suggests a monotone increasing association but with non-elliptical contours due to the skewness. The investigators want to report a $95\\%$ confidence interval for the Pearson correlation of $X$ and $Y$ using the classical Fisher $z$ approach, but they are concerned that the usual large-sample normal approximation underlying this approach relies on approximate bivariate normality, which is questionable here.\n\nStarting from fundamental facts that (i) the Pearson correlation measures linear association and is not invariant to nonlinear marginal transformations, (ii) Fisher’s $z$-based inference is derived under the bivariate normal model where the Fisher-transformed sample correlation has an approximately normal distribution with variance depending only on $n$, and (iii) strictly monotone marginal transformations preserve the rank-based dependence structure while altering higher moments and marginal shapes, choose the single most appropriate transformation strategy that would justify using Fisher’s $z$-based inference in this non-normal setting. Your choice must be based on a principle that preserves the underlying monotone dependence while rendering the transformed marginals approximately normal, so that the assumptions behind Fisher’s $z$ are approximately satisfied.\n\nWhich option best accomplishes this aim and provides the correct justification?\n\nA. Apply a rank-based inverse normal transformation to each marginal (marginal normal scores), then compute the Pearson correlation on the transformed variables and apply Fisher’s $z$ to that correlation. Justification: strictly monotone, rank-based marginal transformations preserve the copula (dependence structure). Under a Gaussian copula model, the transformed pair has approximately bivariate normal margins and elliptical dependence, so Fisher’s $z$ inference with variance approximately $1/(n-3)$ is appropriate.\n\nB. Standardize $X$ and $Y$ to have sample mean $0$ and sample variance $1$, then compute the Pearson correlation and apply Fisher’s $z$. Justification: standardization, together with the Central Limit Theorem, makes the data approximately normal, which legitimizes Fisher’s $z$.\n\nC. Replace Pearson’s correlation with Spearman’s rank correlation to reduce sensitivity to marginal non-normality, then apply Fisher’s $z$ transformation to the Spearman correlation. Justification: ranks are robust to skewness and Fisher’s $z$ applies to any correlation measure.\n\nD. Apply the same strictly increasing nonlinear transformation to both $X$ and $Y$ (for example, a logarithm) until the histograms look visually symmetric, then compute the Pearson correlation and apply Fisher’s $z$. Justification: any monotone transformation preserves correlation, so making the histograms look symmetric is sufficient for Fisher’s $z$ to be valid.\n\nE. Keep the data on the original scale, compute the Pearson correlation, but use a nonparametric bootstrap to estimate the sampling distribution of the Fisher $z$ statistic. Justification: the bootstrap makes the Fisher $z$ statistic approximately normal regardless of marginal distributions, so the usual Fisher interval is valid.",
            "solution": "The problem asks for the most appropriate transformation strategy to justify the use of Fisher's $z$-transformation for constructing a confidence interval for the Pearson correlation between two biomarkers, $X$ and $Y$. The data consists of $n=120$ pairs, and the marginal distributions are known to be non-normal (continuous, strictly positive, right-skewed, heavy-tailed).\n\nFirst, let us recall the fundamentals of the Fisher $z$-transformation. Given a sample Pearson correlation coefficient $r$ calculated from a sample of size $n$ drawn from a bivariate normal distribution with population correlation $\\rho$, the Fisher $z$-transformation is defined as:\n$$z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) = \\text{arctanh}(r)$$\nThe key result is that, under the assumption of bivariate normality, the sampling distribution of $z$ is approximately normal, even for small $n$:\n$$z \\sim \\mathcal{N}\\left( \\text{arctanh}(\\rho), \\frac{1}{n-3} \\right)$$\nThis allows for the construction of a $(1-\\alpha) \\times 100\\%$ confidence interval for $\\text{arctanh}(\\rho)$ as $z \\pm z_{\\alpha/2} \\frac{1}{\\sqrt{n-3}}$, which can then be back-transformed to obtain a confidence interval for $\\rho$.\n\nThe critical issue here is that the data for $X$ and $Y$ are explicitly stated to be non-normal, which invalidates the primary assumption underlying this procedure. The goal is therefore to find a transformation strategy that makes the data conform as closely as possible to the bivariate normal assumption, while preserving the underlying dependence structure.\n\nAccording to Sklar's theorem, any continuous multivariate distribution can be decomposed into its marginal distributions and a copula, which describes the dependence structure between the variables. A strictly monotone transformation applied to the marginals of a distribution does not change the copula. Thus, we can change the marginal distributions to be of a more desirable form (e.g., normal) while preserving the intrinsic dependence structure.\n\nThe problem states that there is a \"monotone increasing association\". We seek a transformation that preserves this rank-based dependence structure while inducing normality on the marginals. A powerful method to achieve this is the rank-based inverse normal transformation, also known as obtaining normal scores. For each variable, say $X$, the observations $x_1, x_2, \\dots, x_n$ are replaced by their normal scores. A common version of this transformation is $x'_i = \\Phi^{-1}\\left(\\frac{\\text{rank}(x_i)}{n+1}\\right)$, where $\\Phi^{-1}$ is the quantile function (inverse CDF) of the standard normal distribution, and the denominator is adjusted to $n+1$ to avoid evaluating at $1$. After this transformation, the new variables, say $X'$ and $Y'$, will have marginal distributions that are, by construction, approximately standard normal.\n\nIf the original dependence structure (copula) of $(X, Y)$ was a Gaussian copula, then the transformed variables $(X', Y')$ will have an approximately bivariate normal distribution. This is the ideal scenario for applying Fisher's $z$-transformation. We can then compute the Pearson correlation of the transformed data, $r' = \\text{corr}(X', Y')$, and proceed with the classical Fisher $z$ inference, as its assumptions are now approximately met.\n\nWith this theoretical framework, we now evaluate the given options.\n\nA. Apply a rank-based inverse normal transformation to each marginal (marginal normal scores), then compute the Pearson correlation on the transformed variables and apply Fisher’s $z$ to that correlation. Justification: strictly monotone, rank-based marginal transformations preserve the copula (dependence structure). Under a Gaussian copula model, the transformed pair has approximately bivariate normal margins and elliptical dependence, so Fisher’s $z$ inference with variance approximately $1/(n-3)$ is appropriate.\nThis option precisely describes the procedure derived above. The transformation induces approximate marginal normality. The justification correctly states that this rank-based monotone transformation preserves the copula. It correctly notes that if the underlying copula is Gaussian, the transformed data become approximately bivariate normal, which is the exact condition required for the classical Fisher $z$ approach to be valid. The variance of approximately $1/(n-3)$ is then appropriate. This is a theoretically sound and statistically principled approach.\nVerdict: **Correct**.\n\nB. Standardize $X$ and $Y$ to have sample mean $0$ and sample variance $1$, then compute the Pearson correlation and apply Fisher’s $z$. Justification: standardization, together with the Central Limit Theorem, makes the data approximately normal, which legitimizes Fisher’s $z$.\nThis option is fundamentally flawed. Standardization is a linear transformation, $X_{\\text{std}} = (X - \\mu_X)/\\sigma_X$. A linear transformation does not alter the shape of a distribution; a right-skewed distribution remains right-skewed after standardization. The justification invokes the Central Limit Theorem (CLT), which states that the distribution of the *sample mean* tends to normality as the sample size increases. The CLT says nothing about the distribution of the data itself. Therefore, this procedure fails to address the non-normality of the marginals.\nVerdict: **Incorrect**.\n\nC. Replace Pearson’s correlation with Spearman’s rank correlation to reduce sensitivity to marginal non-normality, then apply Fisher’s $z$ transformation to the Spearman correlation. Justification: ranks are robust to skewness and Fisher’s $z$ applies to any correlation measure.\nWhile switching to Spearman's rank correlation, $r_S$, is a valid robust strategy, the justification for the subsequent step is false. The Fisher $z$-transformation and its associated variance formula, $1/(n-3)$, were specifically derived for the Pearson correlation coefficient $r$ under the assumption of bivariate normality. It does not apply universally to any correlation measure. While a similar variance-stabilizing transformation exists for $r_S$, its variance is approximately $1.06/(n-3)$, not $1/(n-3)$. Applying the classical Fisher machinery directly to $r_S$ is incorrect.\nVerdict: **Incorrect**.\n\nD. Apply the same strictly increasing nonlinear transformation to both $X$ and $Y$ (for example, a logarithm) until the histograms look visually symmetric, then compute the Pearson correlation and apply Fisher’s $z$. Justification: any monotone transformation preserves correlation, so making the histograms look symmetric is sufficient for Fisher’s $z$ to be valid.\nThis option contains two significant errors in its justification. First, \"any monotone transformation preserves correlation\" is false. Pearson correlation measures *linear* association and is not invariant under nonlinear transformations; i.e., in general, $\\text{corr}(g(X), g(Y)) \\ne \\text{corr}(X,Y)$ for a nonlinear function $g$. It is rank-based correlations that are preserved. Second, \"making the histograms look symmetric is sufficient\" is also false. Achieving normal marginals is a necessary, but not sufficient, condition for bivariate normality. The dependence structure (copula) might still be non-Gaussian. The approach in Option A is more principled than an ad-hoc visual approach, and the justification here is factually incorrect.\nVerdict: **Incorrect**.\n\nE. Keep the data on the original scale, compute the Pearson correlation, but use a nonparametric bootstrap to estimate the sampling distribution of the Fisher $z$ statistic. Justification: the bootstrap makes the Fisher $z$ statistic approximately normal regardless of marginal distributions, so the usual Fisher interval is valid.\nThis option describes a valid alternative inferential procedure (bootstrapping), but it does not fulfill the stated task, which is to find a transformation strategy that *justifies using the classical Fisher's z-based inference*. Moreover, the justification provided is misleading. The bootstrap does not \"make\" the statistic normal; it provides an empirical approximation of the statistic's true sampling distribution, whatever shape it may have. One would then typically use the percentiles of this empirical distribution to form a confidence interval (e.g., a percentile bootstrap interval), which can be asymmetric and is distinct from the \"usual Fisher interval\" that relies on the symmetric normal approximation. The bootstrap is a way to bypass the assumptions of the classical method, not a way to satisfy them.\nVerdict: **Incorrect**.\n\nIn conclusion, Option A presents the only method and justification that is fully consistent with modern statistical theory for handling this specific problem. It correctly identifies a principled transformation that directly addresses the violation of the normality assumption, thereby legitimizing the subsequent use of the classical Fisher $z$ procedure.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}