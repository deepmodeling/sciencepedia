{
    "hands_on_practices": [
        {
            "introduction": "In regression analysis, not all data points exert the same pull on the fitted line. This exercise demystifies the concept of \"leverage,\" a measure of an observation's potential to influence the regression model based solely on its position in the predictor space. By deriving the formula for leverage from first principles in a simple linear regression setting , you will gain a concrete understanding of why data points with more extreme predictor values act as stronger levers on the model's fit.",
            "id": "4949183",
            "problem": "A biostatistics study models a continuous biomarker response as a linear function of a continuous dose, with an intercept. Specifically, for observations indexed by $i \\in \\{1,\\dots,n\\}$, the model is $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$, where the errors $\\varepsilon_{i}$ satisfy the usual ordinary least squares (OLS) assumptions used to justify projection properties of fitted values. Let the $n \\times 2$ design matrix be $X = \\begin{pmatrix} 1 & x_{1} \\\\ \\vdots & \\vdots \\\\ 1 & x_{n} \\end{pmatrix}$ and let the projection (hat) matrix be the unique linear operator that maps observed responses to their fitted values under OLS. The diagonal elements of this projection matrix are the leverages, denoted $h_{ii}$, which quantify the potential of observation $i$ to influence its own fitted value.\n\nStarting only from standard linear model definitions and linear algebra facts for $2 \\times 2$ matrices, derive a closed-form expression for the leverage $h_{ii}$ in terms of $n$, $x_{i}$, $\\bar{x}$, and $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$, where $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_{j}$. Your derivation should make explicit any intermediate algebraic steps needed to eliminate nuisance sums in favor of $\\bar{x}$ and $\\sum_{j=1}^{n} (x_{j} - \\bar{x})^{2}$.\n\nAdditionally, explain qualitatively, based on your derivation, how the spacing of the design points $\\{x_{j}\\}_{j=1}^{n}$ determines the potential influence of an observation, and why more extreme $x_{i}$ values relative to $\\bar{x}$ have higher leverage.\n\nProvide, as your final answer, the simplified analytic expression for $h_{ii}$. No numerical rounding is required, and no units are involved.",
            "solution": "The problem requires the derivation of the leverage, $h_{ii}$, for a simple linear regression model and a qualitative explanation of its properties. The derivation must begin from fundamental definitions in linear model theory.\n\nThe simple linear regression model is given by $y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\varepsilon_{i}$. In matrix form, this is $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of responses, $X$ is the $n \\times 2$ design matrix, $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}$ is the vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the vector of errors.\n\nThe design matrix $X$ is given as:\n$$\nX = \\begin{pmatrix} 1 & x_{1} \\\\ 1 & x_{2} \\\\ \\vdots & \\vdots \\\\ 1 & x_{n} \\end{pmatrix}\n$$\nThe vector of ordinary least squares (OLS) fitted values, $\\hat{\\mathbf{y}}$, is obtained by projecting the observed response vector $\\mathbf{y}$ onto the column space of $X$. The projection matrix, or hat matrix, is denoted by $H$ and is defined as:\n$$\nH = X(X^\\top X)^{-1} X^\\top\n$$\nThe leverage $h_{ii}$ of the $i$-th observation is the $i$-th diagonal element of $H$. It can be computed as $h_{ii} = \\mathbf{x}_i^\\top (X^\\top X)^{-1} \\mathbf{x}_i$, where $\\mathbf{x}_i^\\top = \\begin{pmatrix} 1 & x_i \\end{pmatrix}$ is the $i$-th row of the design matrix $X$.\n\nOur derivation proceeds in several steps.\n\n**Step 1: Compute the matrix $X^\\top X$.**\nThe matrix $X^\\top X$ is a $2 \\times 2$ symmetric matrix.\n$$\nX^\\top X = \\begin{pmatrix} 1 & 1 & \\dots & 1 \\\\ x_1 & x_2 & \\dots & x_n \\end{pmatrix} \\begin{pmatrix} 1 & x_{1} \\\\ 1 & x_{2} \\\\ \\vdots & \\vdots \\\\ 1 & x_{n} \\end{pmatrix} = \\begin{pmatrix} \\sum_{j=1}^{n} 1 & \\sum_{j=1}^{n} x_j \\\\ \\sum_{j=1}^{n} x_j & \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\nUsing the definition of the sample mean, $\\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_j$, we can write $\\sum_{j=1}^{n} x_j = n\\bar{x}$.\nThus,\n$$\nX^\\top X = \\begin{pmatrix} n & n\\bar{x} \\\\ n\\bar{x} & \\sum_{j=1}^{n} x_j^2 \\end{pmatrix}\n$$\n\n**Step 2: Compute the determinant of $X^\\top X$ and its inverse.**\nThe determinant of $X^\\top X$ is:\n$$\n\\det(X^\\top X) = n \\left(\\sum_{j=1}^{n} x_j^2\\right) - (n\\bar{x})^2 = n \\sum_{j=1}^{n} x_j^2 - n^2\\bar{x}^2\n$$\nTo express this in the required terms, we use the definition of the sum of squared deviations from the mean, which we denote as $S_{xx}$:\n$$\nS_{xx} = \\sum_{j=1}^{n} (x_j - \\bar{x})^2 = \\sum_{j=1}^{n} (x_j^2 - 2x_j\\bar{x} + \\bar{x}^2) = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}\\left(\\sum_{j=1}^{n} x_j\\right) + \\sum_{j=1}^{n} \\bar{x}^2\n$$\nSubstituting $\\sum_{j=1}^{n} x_j = n\\bar{x}$, we get:\n$$\nS_{xx} = \\left(\\sum_{j=1}^{n} x_j^2\\right) - 2\\bar{x}(n\\bar{x}) + n\\bar{x}^2 = \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2\n$$\nTherefore, the determinant can be simplified:\n$$\n\\det(X^\\top X) = n \\left( \\left(\\sum_{j=1}^{n} x_j^2\\right) - n\\bar{x}^2 \\right) = n S_{xx} = n \\sum_{j=1}^{n} (x_j - \\bar{x})^2\n$$\nNow, we find the inverse of the $2 \\times 2$ matrix $X^\\top X$ using the formula for the inverse of a general $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$:\n$$\n(X^\\top X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} \\sum_{j=1}^{n} x_j^2 & -n\\bar{x} \\\\ -n\\bar{x} & n \\end{pmatrix}\n$$\nTo simplify this expression, we substitute $\\sum_{j=1}^{n} x_j^2 = S_{xx} + n\\bar{x}^2$:\n$$\n(X^\\top X)^{-1} = \\frac{1}{n S_{xx}} \\begin{pmatrix} S_{xx} + n\\bar{x}^2 & -n\\bar{x} \\\\ -n\\bar{x} & n \\end{pmatrix} = \\begin{pmatrix} \\frac{S_{xx} + n\\bar{x}^2}{n S_{xx}} & -\\frac{n\\bar{x}}{n S_{xx}} \\\\ -\\frac{n\\bar{x}}{n S_{xx}} & \\frac{n}{n S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} & -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}} & \\frac{1}{S_{xx}} \\end{pmatrix}\n$$\n\n**Step 3: Compute the leverage $h_{ii}$.**\nUsing the formula $h_{ii} = \\mathbf{x}_i^\\top (X^\\top X)^{-1} \\mathbf{x}_i$ with $\\mathbf{x}_i^\\top = \\begin{pmatrix} 1 & x_i \\end{pmatrix}$:\n$$\nh_{ii} = \\begin{pmatrix} 1 & x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} & -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}} & \\frac{1}{S_{xx}} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}\n$$\nFirst, multiply the row vector by the matrix:\n$$\n\\begin{pmatrix} 1 & x_i \\end{pmatrix} \\begin{pmatrix} \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} & -\\frac{\\bar{x}}{S_{xx}} \\\\ -\\frac{\\bar{x}}{S_{xx}} & \\frac{1}{S_{xx}} \\end{pmatrix} = \\begin{pmatrix} \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right) & \\left(-\\frac{\\bar{x}}{S_{xx}} + \\frac{x_i}{S_{xx}}\\right) \\end{pmatrix}\n$$\nNext, multiply this resulting $1 \\times 2$ vector by the column vector $\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}$:\n$$\nh_{ii} = \\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{x_i \\bar{x}}{S_{xx}}\\right) \\cdot 1 + \\left(\\frac{x_i - \\bar{x}}{S_{xx}}\\right) \\cdot x_i\n$$\n$$\nh_{ii} = \\frac{1}{n} + \\frac{\\bar{x}^2 - x_i \\bar{x} + x_i^2 - \\bar{x} x_i}{S_{xx}} = \\frac{1}{n} + \\frac{x_i^2 - 2x_i\\bar{x} + \\bar{x}^2}{S_{xx}}\n$$\nRecognizing the numerator of the second term as $(x_i - \\bar{x})^2$, we arrive at the final expression:\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\n$$\nSubstituting back the definition of $S_{xx}$:\n$$\nh_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}\n$$\nThis is the required closed-form expression for the leverage of observation $i$.\n\n**Qualitative Explanation of Leverage**\n\nThe derived expression for leverage, $h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$, provides direct insight into the factors that determine an observation's potential influence on its own fitted value. The leverage is composed of two non-negative terms:\n\n$1$. The term $\\frac{1}{n}$ represents a baseline component of leverage that is uniform for all observations. It only depends on the sample size $n$. As the number of observations increases, this baseline influence decreases, reflecting the reduced importance of any single point in a larger dataset.\n\n$2$. The second term, $\\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}$, represents the component of leverage due to the position of the predictor value $x_i$ relative to the other predictor values.\n   - The numerator, $(x_i - \\bar{x})^2$, is the squared distance of $x_i$ from the center of the predictor data, $\\bar{x}$. This term shows that as an observation's predictor value becomes more extreme (i.e., farther from the mean $\\bar{x}$), its leverage increases quadratically. An observation with a large value of $|x_i - \\bar{x}|$ is called a high-leverage point. Such points act like long lever arms, giving them greater potential to pivot the fitted regression line.\n   - The denominator, $\\sum_{j=1}^{n} (x_j - \\bar{x})^2$, is the sum of squared deviations of all predictor values. It quantifies the total spread or variability of the design points $\\{x_j\\}$. This term sets the scale by which the \"extremeness\" of a given point $x_i$ is judged.\n     - If the design points are widely spaced, the denominator will be large. This reduces the magnitude of the positional leverage component for all points, meaning the regression line is more robustly \"anchored\" by the dispersed data, and the influence of any single point is diminished.\n     - Conversely, if the design points are tightly clustered, the denominator will be small. This amplifies the positional leverage component. In such a scenario, an observation that is only moderately distant from $\\bar{x}$ can have very high leverage because it is an outlier relative to the narrow distribution of the remaining data.\n\nIn conclusion, an observation's leverage is high if its predictor value is far from the mean of all predictor values, particularly when the majority of other predictor values are not widely spread out. This gives a precise, quantitative basis for the intuitive idea that remote data points in the predictor space are the most influential in a regression analysis.",
            "answer": "$$\n\\boxed{\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n} (x_j - \\bar{x})^2}}\n$$"
        },
        {
            "introduction": "While leverage quantifies a point's *potential* for influence, Cook's distance measures its *actual* impact on the estimated coefficients. This metric assesses how much the entire regression model changes when a single data point is removed. This practice guides you through the derivation of a famous formula for Cook's distance , which elegantly shows that influence is a product of both leverage and the size of the residual, providing a powerful diagnostic tool.",
            "id": "4949167",
            "problem": "A biostatistician fits an ordinary least squares (OLS) linear regression model with response vector $y \\in \\mathbb{R}^{n}$ and full-rank design matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n &gt; p \\ge 2$. Assume the classical linear model $y = X\\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Let $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y$ be the OLS estimator, $\\hat{y} = X \\hat{\\beta}$ the fitted values, and $e = y - \\hat{y}$ the residual vector. Define the hat matrix $H = X (X^{\\top} X)^{-1} X^{\\top}$, and let $h_{ii}$ denote its $i$-th diagonal element (the leverage of observation $i$). Let $s^{2} = \\frac{1}{n-p} \\|y - X \\hat{\\beta}\\|_{2}^{2}$ be the mean squared error. The internally studentized residual for observation $i$ is defined as $t_{i} = \\frac{e_{i}}{s \\sqrt{1 - h_{ii}}}$. For the leave-one-out fit that omits observation $i$, denote its coefficient estimator by $\\hat{\\beta}_{(i)}$. Cook’s distance for observation $i$ is defined by\n$$\nD_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}}.\n$$\nStarting only from these definitions and standard linear algebra identities for rank-one updates, derive an explicit expression for $D_{i}$ written solely in terms of $p$, $t_{i}$, and $h_{ii}$. Then, based on your derivation, explain briefly how leverage $h_{ii}$ and residual magnitude (as captured by $t_{i}$) each influence $D_{i}$.\n\nProvide your final answer as the closed-form analytical expression for $D_{i}$ in terms of $p$, $t_{i}$, and $h_{ii}$. No numerical evaluation is required.",
            "solution": "The problem is well-posed, scientifically grounded in the theory of linear models, and contains all necessary information for a unique analytical solution. The definitions and premises are standard and internally consistent.\n\nThe objective is to derive an expression for Cook's distance, $D_{i}$, in terms of the number of predictors $p$, the internally studentized residual $t_{i}$, and the leverage $h_{ii}$. The provided definition of Cook's distance for observation $i$ is:\n$$ D_{i} \\;=\\; \\frac{\\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)^{\\top} X^{\\top} X \\left(\\hat{\\beta} - \\hat{\\beta}_{(i)}\\right)}{p\\, s^{2}} $$\nThe core of the derivation is to find a simplified expression for the change in the coefficient vector, $\\hat{\\beta} - \\hat{\\beta}_{(i)}$, that results from omitting the $i$-th observation. The leave-one-out estimator, $\\hat{\\beta}_{(i)}$, is calculated using the data matrices $X_{(i)}$ and $y_{(i)}$, which are formed by removing the $i$-th row from $X$ and $y$, respectively. Let $x_i^{\\top}$ be the $i$-th row of $X$. The required matrix products for the leave-one-out estimator are rank-$1$ modifications of the full-data products:\n$$ X_{(i)}^{\\top} X_{(i)} = X^{\\top}X - x_i x_i^{\\top} $$\n$$ X_{(i)}^{\\top} y_{(i)} = X^{\\top}y - x_i y_i $$\nTo compute $\\hat{\\beta}_{(i)} = (X_{(i)}^{\\top} X_{(i)})^{-1} X_{(i)}^{\\top} y_{(i)}$, we first need the inverse of $X_{(i)}^{\\top} X_{(i)}$. We use the Sherman-Morrison-Woodbury formula for a rank-$1$ update, which states $(A - uv^{\\top})^{-1} = A^{-1} + \\frac{A^{-1}uv^{\\top}A^{-1}}{1 - v^{\\top}A^{-1}u}$. Setting $A = X^{\\top}X$ and $u = v = x_i$, we get:\n$$ (X^{\\top}X - x_i x_i^{\\top})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - x_i^{\\top}(X^{\\top}X)^{-1}x_i} $$\nThe term in the denominator, $x_i^{\\top}(X^{\\top}X)^{-1}x_i$, corresponds to the $i$-th diagonal element of the hat matrix $H = X(X^{\\top}X)^{-1}X^{\\top}$, which is the leverage $h_{ii}$. The inverse is therefore:\n$$ (X_{(i)}^{\\top} X_{(i)})^{-1} = (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} $$\nNow, we can express $\\hat{\\beta}_{(i)}$:\n$$ \\hat{\\beta}_{(i)} = \\left( (X^{\\top}X)^{-1} + \\frac{(X^{\\top}X)^{-1}x_i x_i^{\\top}(X^{\\top}X)^{-1}}{1 - h_{ii}} \\right) (X^{\\top}y - x_i y_i) $$\nExpanding this expression yields:\n$$ \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}(X^{\\top}y) - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} \\left( x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y - x_i^{\\top}(X^{\\top}X)^{-1}x_i y_i \\right) $$\nWe identify several terms: $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$, the $i$-th fitted value $\\hat{y}_i = x_i^{\\top}\\hat{\\beta} = x_i^{\\top}(X^{\\top}X)^{-1}X^{\\top}y$, and $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$. Substituting these gives:\n$$ \\hat{\\beta}_{(i)} = \\hat{\\beta} - (X^{\\top}X)^{-1}x_i y_i + \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) $$\nRearranging to find the difference vector $\\hat{\\beta} - \\hat{\\beta}_{(i)}$:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = (X^{\\top}X)^{-1}x_i y_i - \\frac{(X^{\\top}X)^{-1}x_i}{1-h_{ii}} (\\hat{y}_i - h_{ii} y_i) = (X^{\\top}X)^{-1}x_i \\left( y_i - \\frac{\\hat{y}_i - h_{ii} y_i}{1 - h_{ii}} \\right) $$\nThe term in the parenthesis simplifies:\n$$ \\frac{y_i(1 - h_{ii}) - (\\hat{y}_i - h_{ii} y_i)}{1 - h_{ii}} = \\frac{y_i - y_i h_{ii} - \\hat{y}_i + y_i h_{ii}}{1 - h_{ii}} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} = \\frac{e_i}{1-h_{ii}} $$\nwhere $e_i = y_i - \\hat{y}_i$ is the $i$-th residual. This leads to the key result for the change in coefficients:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{(X^{\\top}X)^{-1}x_i e_i}{1 - h_{ii}} $$\nNext, we substitute this into the numerator of Cook's distance:\n$$ (\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)}) = \\left( \\frac{e_i}{1-h_{ii}} x_i^{\\top}(X^{\\top}X)^{-1} \\right) (X^{\\top}X) \\left( \\frac{(X^{\\top}X)^{-1}x_i e_i}{1-h_{ii}} \\right) $$\n$$ = \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 x_i^{\\top} \\left( (X^{\\top}X)^{-1} (X^{\\top}X) (X^{\\top}X)^{-1} \\right) x_i = \\frac{e_i^2}{(1-h_{ii})^2} x_i^{\\top} (X^{\\top}X)^{-1} x_i $$\nUsing $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$, the numerator becomes $\\frac{e_i^2 h_{ii}}{(1-h_{ii})^2}$. Substituting this into the definition of $D_i$:\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2} \\right] $$\nThe final step is to incorporate the internally studentized residual $t_{i} = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}$. Squaring this gives $t_i^2 = \\frac{e_i^2}{s^2(1-h_{ii})}$, from which we get $e_i^2 = t_i^2 s^2 (1 - h_{ii})$. Substituting this expression for $e_i^2$:\n$$ D_i = \\frac{1}{p s^2} \\left[ \\frac{t_i^2 s^2 (1 - h_{ii}) h_{ii}}{(1-h_{ii})^2} \\right] $$\nCanceling $s^2$ and a factor of $(1-h_{ii})$ yields the final expression for Cook's distance:\n$$ D_i = \\frac{t_i^2}{p} \\frac{h_{ii}}{1-h_{ii}} $$\nThis derived expression illuminates the nature of an observation's influence. The influence, measured by $D_i$, is a product of two factors: one related to the residual size ($t_i^2$) and one related to the observation's leverage ($h_{ii}$).\nThe term $t_i^2$ shows that $D_i$ is proportional to the squared studentized residual. Points that are far from the regression hyperplane (outliers in the response space) will have large $|t_i|$ values, contributing to a larger $D_i$.\nThe term $\\frac{h_{ii}}{1-h_{ii}}$ captures the effect of leverage. Leverage $h_{ii}$ quantifies how far an observation's predictor values $x_i$ are from the center of the predictor space. The function $\\frac{h_{ii}}{1-h_{ii}}$ increases non-linearly as leverage $h_{ii}$ approaches $1$. This means that high-leverage points act as a strong multiplier on the residual term.\nIn summary, an observation is influential if it has a large residual, high leverage, or both. A high-leverage point has the potential for high influence; this potential is realized if the point also has a non-negligible residual. Conversely, a point with low leverage must have a very large residual to be influential.",
            "answer": "$$\\boxed{\\frac{t_{i}^{2}}{p} \\frac{h_{ii}}{1 - h_{ii}}}$$"
        },
        {
            "introduction": "A key goal of residual analysis is to identify outliers, but the tool we use matters. This conceptual problem asks you to compare the effectiveness of two different kinds of standardized residuals—internally and externally studentized—for detecting an aberrant observation. By analyzing their statistical properties, you will uncover the critical diagnostic phenomenon known as \"masking,\" where an outlier can inflate variance estimates and hide itself. Understanding this distinction  is essential for robust outlier detection in practice.",
            "id": "4949136",
            "problem": "A biostatistician fits a linear model $y = X\\beta + \\varepsilon$ to study the relationship between a continuous biomarker response $y$ and a set of covariates $X$ (including an intercept), where $\\varepsilon$ are independent errors with mean $0$ and variance $\\sigma^{2}$. Ordinary least squares (OLS) yields the fitted values $\\hat{y}$ and residuals $e = y - \\hat{y}$. Let $H = X(X^{\\top}X)^{-1}X^{\\top}$ be the hat matrix with diagonal elements $h_{ii}$, and recall that under standard OLS assumptions, $\\operatorname{Var}(e_{i}) = \\sigma^{2}(1 - h_{ii})$.\n\nTo screen for outliers, the analyst considers two standardized residual statistics at observation $i$: the internally studentized statistic $t_{i}$ that uses the full-data mean squared error (MSE) estimate of $\\sigma^{2}$, and the externally studentized (deleted) statistic $t_{i}^{*}$ that uses the leave-one-out MSE computed after removing observation $i$. Both statistics adjust for leverage through $h_{ii}$ but differ in how the variance of the error is estimated.\n\nSuppose exactly one observation, at index $i$, is aberrant in the sense that its error $\\varepsilon_{i}$ is shifted so that the corresponding residual $e_{i}$ has magnitude much larger than typical, while all other errors $\\varepsilon_{j}$ for $j \\neq i$ follow the modeling assumptions. At a fixed significance level for an outlier test based on these statistics, which statement best characterizes the relative power (probability of correctly flagging the aberrant observation) of $t_{i}$ versus $t_{i}^{*}$ in terms of their variance estimation differences?\n\nChoose the single best option.\n\nA. When only one observation is aberrant, $t_{i}^{*}$ typically has greater power than $t_{i}$ because the leave-one-out variance estimate excludes the aberrant observation and is not inflated, whereas the full-data variance estimate is inflated by the aberrant residual, reducing the magnitude of $t_{i}$ and causing masking.\n\nB. When only one observation is aberrant, $t_{i}$ and $t_{i}^{*}$ have equal power because both adjust for leverage via $h_{ii}$, and thus scale by the same effective variance.\n\nC. $t_{i}$ has greater power than $t_{i}^{*}$ because using all $n$ observations yields a smaller full-data variance estimate than the leave-one-out estimate, making $t_{i}$ larger in magnitude.\n\nD. $t_{i}^{*}$ has lower power than $t_{i}$ because removing the observation reduces degrees of freedom, increasing the leave-one-out variance estimate enough to offset any masking effect.",
            "solution": "The problem statement is a well-posed question in the domain of regression diagnostics, a standard topic in biostatistics and statistics in general. All terms are standard and the setup is clear and internally consistent. It describes a classical scenario for comparing outlier detection statistics. Thus, the problem is valid.\n\nThe core of the problem is to compare the statistical power of two different types of studentized residuals for detecting a single outlier. Let us formally define the quantities involved.\n\nThe linear model is given by $y = X\\beta + \\varepsilon$, where the errors $\\varepsilon_i$ are independent with $\\mathrm{E}[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$. After fitting the model using ordinary least squares (OLS), we obtain the residuals $e = y - \\hat{y}$. The variance of the $i$-th residual is $\\operatorname{Var}(e_i) = \\sigma^2(1 - h_{ii})$, where $h_{ii}$ is the $i$-th diagonal element of the hat matrix $H = X(X^{\\top}X)^{-1}X^{\\top}$.\n\nThe two statistics for screening the $i$-th observation as an outlier are:\n1.  The **internally studentized residual**, $t_i$: This statistic uses the full-data estimate of the error variance, $\\hat{\\sigma}^2$, which is the Mean Squared Error (MSE).\n    $$ \\hat{\\sigma}^2 = \\text{MSE} = \\frac{\\sum_{j=1}^{n} e_j^2}{n-p} = \\frac{SSE}{n-p} $$\n    where $n$ is the number of observations and $p$ is the number of parameters in the model (i.e., the number of columns of $X$). The statistic is:\n    $$ t_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}} $$\n\n2.  The **externally studentized (or deleted) residual**, $t_i^*$: This statistic uses a leave-one-out estimate of the error variance, $\\hat{\\sigma}_{(i)}^2$. This estimate is calculated by fitting the regression model to the data with the $i$-th observation removed.\n    $$ \\hat{\\sigma}_{(i)}^2 = \\text{MSE}_{(i)} = \\frac{SSE_{(i)}}{n-1-p} $$\n    where $SSE_{(i)}$ is the residual sum of squares from the model fit without observation $i$. The statistic is:\n    $$ t_i^* = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1-h_{ii}}} $$\n    Note that both statistics use the same raw residual $e_i$ in the numerator and the same leverage adjustment $\\sqrt{1-h_{ii}}$ in the denominator. The sole difference lies in the variance estimate: $\\hat{\\sigma}$ versus $\\hat{\\sigma}_{(i)}$.\n\nThe problem posits a specific scenario: exactly one observation, at index $i$, is aberrant, such that its residual $e_i$ has a magnitude \"much larger than typical\". This means $e_i^2$ is very large compared to the other squared residuals $e_j^2$ for $j \\neq i$.\n\nLet us analyze the relationship between the two variance estimates, $\\hat{\\sigma}^2$ and $\\hat{\\sigma}_{(i)}^2$. There is a well-known identity that connects the full-data sum of squared errors ($SSE$) with the leave-one-out sum of squared errors ($SSE_{(i)}$):\n$$ SSE = SSE_{(i)} + \\frac{e_i^2}{1-h_{ii}} $$\nThis identity shows that the full-data $SSE$ is composed of the $SSE$ from the data without observation $i$, plus a term that is directly proportional to the squared residual of that observation, amplified by its leverage (since $0 < h_{ii} < 1$).\n\nNow consider the variance estimates:\n- Full-data estimate: $\\hat{\\sigma}^2 = \\frac{SSE}{n-p} = \\frac{SSE_{(i)} + e_i^2/(1-h_{ii})}{n-p}$\n- Leave-one-out estimate: $\\hat{\\sigma}_{(i)}^2 = \\frac{SSE_{(i)}}{n-p-1}$\n\nSince observation $i$ is an outlier, $e_i^2$ is large. This large value of $e_i^2$ contributes to the numerator of $\\hat{\\sigma}^2$, causing it to be an inflated estimate of the true error variance $\\sigma^2$. The outlier effectively \"pulls\" the variance estimate towards itself. In contrast, $\\hat{\\sigma}_{(i)}^2$ is calculated from the remaining $n-1$ observations, which, by assumption, are not aberrant. Therefore, $\\hat{\\sigma}_{(i)}^2$ is an un-inflated and more accurate estimate of the true variance $\\sigma^2$. The large value of $e_i^2$ makes $SSE \\gg SSE_{(i)}$, and this effect typically far outweighs the small difference in the degrees of freedom ($n-p$ vs. $n-p-1$). Consequently, $\\hat{\\sigma}^2$ will be significantly larger than $\\hat{\\sigma}_{(i)}^2$.\n\nNow, we compare the magnitudes of the two test statistics:\n$$ |t_i| = \\frac{|e_i|}{\\hat{\\sigma}\\sqrt{1-h_{ii}}} \\quad \\text{and} \\quad |t_i^*| = \\frac{|e_i|}{\\hat{\\sigma}_{(i)}\\sqrt{1-h_{ii}}} $$\nSince the numerators are identical and we have established that $\\hat{\\sigma} > \\hat{\\sigma}_{(i)}$, it follows that the denominator of $|t_i|$ is larger than the denominator of $|t_i^*|$. Therefore:\n$$ |t_i| < |t_i^*| $$\nThis phenomenon is known as **masking**. The single outlier inflates the full-data variance estimate, which in turn reduces the magnitude of its own internally studentized residual, making it less likely to be flagged as an outlier. The externally studentized residual, $t_i^*$, avoids this problem by using a variance estimate that is not contaminated by the outlier itself.\n\nThe **power** of an outlier test refers to its ability to correctly detect a true outlier. For a given significance level, an observation is flagged if its test statistic exceeds a critical value. Since $|t_i^*|$ is larger than $|t_i|$, it is more likely to exceed any given critical value. Thus, the test based on $t_i^*$ has greater power to detect the single aberrant observation.\n\nUnder the null hypothesis that there is no outlier and the errors are normally distributed, $t_i^*$ follows a Student's t-distribution with $n-p-1$ degrees of freedom. This provides a direct basis for formal hypothesis testing.\n\nNow, we evaluate each option:\n\n**A. When only one observation is aberrant, $t_{i}^{*}$ typically has greater power than $t_{i}$ because the leave-one-out variance estimate excludes the aberrant observation and is not inflated, whereas the full-data variance estimate is inflated by the aberrant residual, reducing the magnitude of $t_{i}$ and causing masking.**\nThis statement is perfectly aligned with our derivation. It correctly identifies that $t_i^*$ has greater power, correctly attributes this to the non-inflated nature of the leave-one-out variance estimate versus the inflated full-data estimate, and correctly describes the consequence as a reduction in the magnitude of $t_i$ (masking). This option is **Correct**.\n\n**B. When only one observation is aberrant, $t_{i}$ and $t_{i}^{*}$ have equal power because both adjust for leverage via $h_{ii}$, and thus scale by the same effective variance.**\nThis is incorrect. While both statistics adjust for leverage, they critically differ in their variance estimation component ($\\hat{\\sigma}$ vs. $\\hat{\\sigma}_{(i)}$). They do not scale by the same effective variance in the presence of an outlier. Thus, their powers are not equal. This option is **Incorrect**.\n\n**C. $t_{i}$ has greater power than $t_{i}^{*}$ because using all $n$ observations yields a smaller full-data variance estimate than the leave-one-out estimate, making $t_{i}$ larger in magnitude.**\nThis statement makes two errors. First, it claims $t_i$ has greater power, which is the opposite of the truth. Second, its reasoning is flawed: in the presence of an outlier, the full-data variance estimate $\\hat{\\sigma}^2$ is typically *larger*, not smaller, than the leave-one-out estimate $\\hat{\\sigma}_{(i)}^2$. This makes $|t_i|$ smaller, not larger. This option is **Incorrect**.\n\n**D. $t_{i}^{*}$ has lower power than $t_{i}$ because removing the observation reduces degrees of freedom, increasing the leave-one-out variance estimate enough to offset any masking effect.**\nThis statement incorrectly claims $t_i^*$ has lower power. The reasoning provided is also flawed. While the degrees of freedom for the leave-one-out estimate are indeed lower ($n-p-1$ vs. $n-p$), this effect is minor compared to the major reduction in the sum of squared errors ($SSE_{(i)}$ vs. $SSE$) from removing the outlier's large contribution. The leave-one-out variance estimate is therefore smaller, not larger, than the full-data estimate. This option is **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}