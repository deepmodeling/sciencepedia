{
    "hands_on_practices": [
        {
            "introduction": "Before diving into diagnostic plots and tests, it is crucial to understand precisely what we are assessing. A common point of confusion is the 'normality' assumption in linear regression. This first exercise  challenges you to distinguish between the distribution of the outcome variable, $Y$, and the distribution of the model's errors, $\\varepsilon$. By constructing a scenario where the model assumptions hold perfectly yet the outcome is non-normal, you will solidify your understanding of this fundamental concept.",
            "id": "4894614",
            "problem": "A biostatistician is analyzing an outcome $Y$ (e.g., a continuous biomarker) as a function of a single predictor $X$ (e.g., a binary genotype indicator or a continuous exposure), using the simple linear regression model. The modeling goal is to check linearity, homoscedasticity, and normality in a way that supports valid inference about the regression slope.\n\nFrom first principles for the classical linear model, the core assumptions are that the conditional mean of $Y$ given $X$ is linear, the conditional variance of $Y$ given $X$ is constant, and the errors are conditionally independent and identically distributed with a specified distribution. In particular, let the model be $Y=\\beta_0+\\beta_1 X+\\varepsilon$, with $\\mathbb{E}(\\varepsilon\\mid X)=0$ and $\\operatorname{Var}(\\varepsilon\\mid X)=\\sigma^2$; when a normality assumption is made, it is that $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$ and is independent of $X$.\n\nSelect all statements that are correct. Your choices should reflect a correct distinction between the normality of errors and the normality of $Y$, and should identify a concrete case where $Y$ is non-normal even though the errors are normal conditional on $X$, as well as appropriate assessments of linearity and homoscedasticity.\n\nA. The normality assumption in the classical linear model refers to the conditional distribution of the errors $\\varepsilon=Y-\\mathbb{E}(Y\\mid X)$ (equivalently, of $Y\\mid X$ about its mean), not to the marginal distribution of $Y$.\n\nB. If $X\\in\\{0,1\\}$ with $\\mathbb{P}(X=1)=0.5$, $\\beta_1\\neq 0$, and $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X$, then $Y$ is marginally normal with variance $\\sigma^2+0.25\\,\\beta_1^2$.\n\nC. If $X\\in\\{0,1\\}$ with $\\mathbb{P}(X=1)=p\\in(0,1)$, $\\beta_1\\neq 0$, and $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X$, then $Y$ has a two-component normal mixture marginal distribution that is generally non-normal; nevertheless the conditional errors are normal and homoscedastic.\n\nD. Homoscedasticity in the linear model means that $\\operatorname{Var}(Y\\mid X=x)$ is constant in $x$, so a residuals-versus-fitted plot should show no systematic fanning or curvature if the assumption holds.\n\nE. To assess the normality assumption that underlies $t$-tests and confidence intervals in a linear regression, it is sufficient to examine a quantile–quantile (Q–Q) plot and perform a Shapiro–Wilk test on the raw response $Y$, because if $Y$ is normal then the errors must be normal.\n\nF. In the model $Y=\\beta_0+\\beta_1 X+\\varepsilon$ with $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$, if $X$ is continuous and non-normal (for example, $X\\sim \\operatorname{Uniform}[0,1]$) and independent of $\\varepsilon$, then the marginal distribution of $Y$ is generally non-normal even though the conditional errors are normal.\n\nChoose all that apply.",
            "solution": "The problem statement provides a correct and standard definition of the simple linear regression model and its underlying assumptions. Let the model be $Y=\\beta_0+\\beta_1 X+\\varepsilon$. The core assumptions for inference are:\n1.  **Linearity**: $\\mathbb{E}(Y \\mid X=x) = \\beta_0 + \\beta_1 x$. This is equivalent to stating $\\mathbb{E}(\\varepsilon \\mid X) = 0$.\n2.  **Homoscedasticity**: $\\operatorname{Var}(Y \\mid X=x) = \\sigma^2$ for all $x$. This is equivalent to stating $\\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^2$, a constant.\n3.  **Normality of Errors**: The errors are normally distributed, conditional on $X$. That is, $\\varepsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2)$. This implies $Y \\mid X \\sim \\mathcal{N}(\\beta_0 + \\beta_1 X, \\sigma^2)$.\n4.  **Independence**: The errors $\\varepsilon_i$ for different observations are independent.\n\nThe problem is scientifically grounded, well-posed, and objective. We can proceed to evaluate each statement based on these principles.\n\n**A. The normality assumption in the classical linear model refers to the conditional distribution of the errors $\\varepsilon=Y-\\mathbb{E}(Y\\mid X)$ (equivalently, of $Y\\mid X$ about its mean), not to the marginal distribution of $Y$.**\n\nThe model is $Y = \\mathbb{E}(Y \\mid X) + \\varepsilon$. By definition, the error term is $\\varepsilon = Y - \\mathbb{E}(Y \\mid X)$. The normality assumption, essential for the validity of $t$-tests and confidence intervals in small samples, is specified for this error term. Specifically, it is assumed that for any given value of the predictor $X$, the errors are drawn from a normal distribution with mean $0$ and some variance $\\sigma^2$. This is precisely the conditional distribution $\\varepsilon \\mid X$.\n\nEquivalently, since $Y \\mid X = \\mathbb{E}(Y \\mid X) + (\\varepsilon \\mid X)$, and $\\mathbb{E}(Y \\mid X)$ is fixed for a given $X$, the conditional distribution of $Y$ given $X$ is a Normal distribution shifted by its mean, $\\mathbb{E}(Y \\mid X)$. So, $Y \\mid X \\sim \\mathcal{N}(\\mathbb{E}(Y \\mid X), \\sigma^2)$.\n\nThe assumption does not pertain to the marginal distribution of $Y$ (the distribution of $Y$ ignoring $X$), which, as we will see, is often non-normal. This statement is a correct and fundamental articulation of the normality assumption.\n\nVerdict: **Correct**.\n\n**B. If $X\\in\\{0,1\\}$ with $\\mathbb{P}(X=1)=0.5$, $\\beta_1\\neq 0$, and $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X$, then $Y$ is marginally normal with variance $\\sigma^2+0.25\\,\\beta_1^2$.**\n\nLet's determine the marginal distribution and variance of $Y$.\nThe distribution of $Y$ is a mixture of its conditional distributions.\n-   If $X=0$ (with probability $0.5$), then $Y \\mid (X=0) = \\beta_0 + \\varepsilon \\sim \\mathcal{N}(\\beta_0, \\sigma^2)$.\n-   If $X=1$ (with probability $0.5$), then $Y \\mid (X=1) = \\beta_0 + \\beta_1 + \\varepsilon \\sim \\mathcal{N}(\\beta_0+\\beta_1, \\sigma^2)$.\n\nThe marginal probability density function of $Y$ is given by the law of total probability:\n$f_Y(y) = f_{Y \\mid X}(y \\mid 0)\\mathbb{P}(X=0) + f_{Y \\mid X}(y \\mid 1)\\mathbb{P}(X=1)$\n$f_Y(y) = 0.5 \\cdot \\phi(y; \\beta_0, \\sigma^2) + 0.5 \\cdot \\phi(y; \\beta_0+\\beta_1, \\sigma^2)$, where $\\phi(y; \\mu, \\sigma^2)$ is the normal PDF.\nThis is a mixture of two normal distributions with different means (since $\\beta_1 \\neq 0$). A mixture of two distinct normal distributions is not a normal distribution. Therefore, the claim that \"$Y$ is marginally normal\" is false.\n\nNow let's compute the marginal variance of $Y$ using the law of total variance: $\\operatorname{Var}(Y) = \\mathbb{E}[\\operatorname{Var}(Y \\mid X)] + \\operatorname{Var}[\\mathbb{E}(Y \\mid X)]$.\n-   The conditional variance is $\\operatorname{Var}(Y \\mid X) = \\operatorname{Var}(\\beta_0 + \\beta_1 X + \\varepsilon \\mid X) = \\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^2$. This is constant, so $\\mathbb{E}[\\operatorname{Var}(Y \\mid X)] = \\sigma^2$.\n-   The conditional expectation is $\\mathbb{E}(Y \\mid X) = \\beta_0 + \\beta_1 X$.\n-   We need the variance of this term: $\\operatorname{Var}[\\mathbb{E}(Y \\mid X)] = \\operatorname{Var}(\\beta_0 + \\beta_1 X) = \\beta_1^2 \\operatorname{Var}(X)$.\n-   For $X \\sim \\text{Bernoulli}(p=0.5)$, $\\operatorname{Var}(X) = p(1-p) = 0.5(1-0.5) = 0.25$.\n-   So, $\\operatorname{Var}[\\mathbb{E}(Y \\mid X)] = \\beta_1^2 (0.25) = 0.25 \\beta_1^2$.\n-   Putting it together: $\\operatorname{Var}(Y) = \\sigma^2 + 0.25 \\beta_1^2$.\n\nThe calculation of the variance is correct, but the statement claims that $Y$ is marginally normal, which is incorrect. A statement must be entirely correct.\n\nVerdict: **Incorrect**.\n\n**C. If $X\\in\\{0,1\\}$ with $\\mathbb{P}(X=1)=p\\in(0,1)$, $\\beta_1\\neq 0$, and $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X$, then $Y$ has a two-component normal mixture marginal distribution that is generally non-normal; nevertheless the conditional errors are normal and homoscedastic.**\n\nThis statement generalizes the setup from B.\n-   The marginal distribution of $Y$ is a mixture of $\\mathcal{N}(\\beta_0, \\sigma^2)$ (with weight $1-p$) and $\\mathcal{N}(\\beta_0+\\beta_1, \\sigma^2)$ (with weight $p$). Since $p \\in (0,1)$ and $\\beta_1 \\neq 0$, the two components are distinct and both have non-zero weight. Such a mixture distribution is not normal. So, the first part of the statement is correct.\n-   The problem states $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$. This directly implies that the conditional errors are normal.\n-   The variance of the conditional errors is $\\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^2$. Since $\\sigma^2$ is a constant that does not depend on the value of $X$, the errors are homoscedastic.\n\nAll parts of this statement are correct. It accurately describes a scenario where the regression model assumptions hold, yet the marginal distribution of the outcome is not normal.\n\nVerdict: **Correct**.\n\n**D. Homoscedasticity in the linear model means that $\\operatorname{Var}(Y\\mid X=x)$ is constant in $x$, so a residuals-versus-fitted plot should show no systematic fanning or curvature if the assumption holds.**\n\nThe first part of the statement, \"Homoscedasticity ... means that $\\operatorname{Var}(Y\\mid X=x)$ is constant in $x$\", is the correct definition.\nThe second part is an implication: \"so a residuals-versus-fitted plot should show no systematic fanning or curvature if the assumption holds.\"\nLet's analyze the components of the diagnostic plot.\n-   **Fanning**: A \"fanning\" or \"cone\" shape in the residuals-versus-fitted plot implies that the variance of the residuals changes with the level of the fitted values. The absence of fanning is the graphical evidence for homoscedasticity. So, if homoscedasticity holds, we expect no systematic fanning.\n-   **Curvature**: A \"curvature\" or any non-random pattern in the mean of the residuals across the fitted values indicates that the linearity assumption is violated ($\\mathbb{E}(Y \\mid X)$ is not linear in $X$).\nThe statement claims that if homoscedasticity holds, there should be *no curvature*. This is false. A model can be homoscedastic while violating the linearity assumption. For example, if the true model is $Y = \\beta_0 + \\beta_1 X^2 + \\varepsilon$ with $\\operatorname{Var}(\\varepsilon)=\\sigma^2$, and we incorrectly fit $Y = \\alpha_0 + \\alpha_1 X + \\eta$, the errors are homoscedastic but the residual plot will show a distinct parabolic curve. Homoscedasticity does not imply linearity. Therefore, the presence of homoscedasticity does not guarantee an absence of curvature in the residual plot.\n\nVerdict: **Incorrect**.\n\n**E. To assess the normality assumption that underlies $t$-tests and confidence intervals in a linear regression, it is sufficient to examine a quantile–quantile (Q–Q) plot and perform a Shapiro–Wilk test on the raw response $Y$, because if $Y$ is normal then the errors must be normal.**\n\nThe normality assumption for inference in linear regression pertains to the errors, $\\varepsilon$, not the response variable, $Y$. The correct procedure is to assess the normality of the estimated residuals, $\\hat{\\varepsilon}_i = y_i - \\hat{y}_i$, for instance, using a Q-Q plot or Shapiro-Wilk test on these residuals. Testing the raw response $Y$ is fundamentally incorrect, as demonstrated in options C and F. The marginal distribution of $Y$ is often non-normal even when the model assumptions are perfectly met.\n\nThe justification provided, \"because if $Y$ is normal then the errors must be normal\", is also flawed and irrelevant. The relationship is $\\varepsilon = Y - (\\beta_0 + \\beta_1 X)$. Whether $\\varepsilon$ is normal depends on the distributions of both $Y$ and $X$. For example, if $Y \\sim \\mathcal{N}$ and $X \\sim \\text{Bernoulli}$, then $\\varepsilon$ would be a mixture of two normals, which is not normal. The implication is not generally true. But more importantly, the premise of testing $Y$ is wrong to begin with.\n\nVerdict: **Incorrect**.\n\n**F. In the model $Y=\\beta_0+\\beta_1 X+\\varepsilon$ with $\\varepsilon\\mid X\\sim \\mathcal{N}(0,\\sigma^2)$, if $X$ is continuous and non-normal (for example, $X\\sim \\operatorname{Uniform}[0,1]$) and independent of $\\varepsilon$, then the marginal distribution of $Y$ is generally non-normal even though the conditional errors are normal.**\n\nThis statement presents another concrete counterexample to the notion that the outcome $Y$ must be normal.\n-   The assumption $\\varepsilon \\mid X \\sim \\mathcal{N}(0,\\sigma^2)$ is given, and since $\\varepsilon$ is also independent of $X$, this is equivalent to $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$. The conditional errors are indeed normal.\n-   The variable $Y$ is the sum of two independent random variables: $\\varepsilon$ and $(\\beta_0 + \\beta_1 X)$.\n-   Let's take the example $X \\sim \\operatorname{Uniform}[0,1]$. Then, assuming $\\beta_1 \\neq 0$, the variable $(\\beta_0 + \\beta_1 X)$ follows a uniform distribution.\n-   The sum of a normally distributed variable and an independently, uniformly distributed variable is found by convolving their probability density functions. A well-known result from probability theory (related to Cramér's decomposition theorem) states that the sum of two independent random variables is normal only if both original variables were normal.\n-   Since the uniform distribution is not normal, the sum $Y = (\\beta_0 + \\beta_1 X) + \\varepsilon$ will not be normally distributed.\nThis provides a valid example showing that the model assumptions can hold (including normal errors) while the marginal distribution of $Y$ is non-normal.\n\nVerdict: **Correct**.\n\nFinal summary of correct options: A, C, F.",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "Once we are clear on the assumptions, the next step is learning to detect violations using diagnostic plots. This practice  focuses on the linearity assumption by asking you to think like a modeler creating a 'counterexample.' You will explore how a component-plus-residual plot, a powerful diagnostic tool, visually reveals a specific type of model misspecification—a quadratic relationship—that a simple linear model would miss.",
            "id": "4894615",
            "problem": "A biostatistician is assessing whether a simple linear regression of a continuous response $Y$ on a single continuous predictor $X$ is appropriate for a new dataset. The goal is to construct a counterexample in which the conditional mean is quadratic, that is, $E(Y \\mid X = x) = \\beta_{0} + \\beta_{1} x + \\beta_{2} x^{2}$ with $\\beta_{2} \\neq 0$, while the noise is independent, homoscedastic, and Gaussian, so that only the linearity assumption is violated. The biostatistician plans to evaluate a component-plus-residual plot for $X$ after fitting a simple linear regression of $Y$ on $X$.\n\nSelect all options that correctly specify such a data-generating process and correctly describe how the component-plus-residual plot would reveal the quadratic departure from linearity when a simple linear regression is fitted.\n\nA. $X \\sim \\mathrm{Uniform}(-3, 3)$; $Y = 1 - 0.5\\,X + 1.0\\,X^{2} + \\varepsilon$, with $\\varepsilon \\mid X \\sim \\mathcal{N}(0, 1)$ independent of $X$. When a simple linear regression of $Y$ on $X$ is fitted, the component-plus-residual plot versus $X$ will display a pronounced U-shaped curvature because the underlying mean is quadratic with a positive $X^{2}$ coefficient.\n\nB. $X \\sim \\mathrm{Uniform}(-3, 3)$; $Y = 1 + 0.5\\,X + \\varepsilon$, with $\\varepsilon \\mid X \\sim \\mathcal{N}(0, 1 + X^{2})$ independent of $X$. When a simple linear regression of $Y$ on $X$ is fitted, the component-plus-residual plot versus $X$ will display curvature due to the heteroscedasticity.\n\nC. $X \\sim \\mathcal{N}(0, 1)$; $Y = 2 + 1.5\\,X - 0.8\\,X^{2} + \\varepsilon$, with $\\varepsilon \\mid X \\sim \\mathcal{N}(0, 0.5^{2})$ independent of $X$. When a simple linear regression of $Y$ on $X$ is fitted, the component-plus-residual plot versus $X$ will show an inverted U-shaped curvature reflecting the negative $X^{2}$ coefficient.\n\nD. $X \\sim \\mathrm{Uniform}(-3, 3)$; $Y = 1 - 0.5\\,X + 1.0\\,X^{2} + \\varepsilon$, with $\\varepsilon \\mid X \\sim \\mathrm{Laplace}(0, 1)$ independent of $X$. When a simple linear regression of $Y$ on $X$ is fitted, the component-plus-residual plot versus $X$ will appear essentially linear, but a quantile-quantile plot of the residuals will reveal the non-Normal errors.\n\nE. $X \\sim \\mathcal{N}(0, 1)$; $Y = 2 + 1.5\\,X + \\varepsilon$, with $\\varepsilon \\mid X \\sim \\mathcal{N}(0, 1)$ independent of $X$. When a simple linear regression of $Y$ on $X$ is fitted, the component-plus-residual plot versus $X$ will show a U-shaped curvature because the true mean is linear and the curvature arises from random noise.",
            "solution": "We begin from the standard assumptions and definitions for simple linear regression of $Y$ on $X$:\n\n- Linearity of the conditional mean: $E(Y \\mid X = x) = \\beta_{0} + \\beta_{1} x$.\n- Homoscedasticity: $\\mathrm{Var}(Y \\mid X = x) = \\sigma^{2}$ for all $x$.\n- Normality of errors: $Y \\mid X = x \\sim \\mathcal{N}(\\beta_{0} + \\beta_{1} x, \\sigma^{2})$.\n\nA counterexample to linearity, while preserving homoscedasticity and Normality, is any data-generating process of the form\n$$\nY = \\gamma_{0} + \\gamma_{1} X + \\gamma_{2} X^{2} + \\varepsilon, \\quad \\varepsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^{2}) \\text{ independent of } X,\n$$\nwith $\\gamma_{2} \\neq 0$. This ensures $E(Y \\mid X = x) = \\gamma_{0} + \\gamma_{1} x + \\gamma_{2} x^{2}$ (quadratic), $\\mathrm{Var}(Y \\mid X = x) = \\sigma^{2}$ (constant), and Gaussian noise.\n\nTo see how a component-plus-residual plot reveals the quadratic departure, recall the definition of the component-plus-residual (also called partial residual) for the predictor $X$ in a simple regression:\n- Fit the ordinary least squares model $Y = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X + e$ and compute residuals $r_{i} = y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} x_{i}$.\n- The component-plus-residual values are $r_{i} + \\hat{\\beta}_{1} x_{i}$ plotted against $x_{i}$.\n\nIn simple linear regression with a single predictor, we can write\n$$\nr_{i} + \\hat{\\beta}_{1} x_{i} = \\left(y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} x_{i}\\right) + \\hat{\\beta}_{1} x_{i} = y_{i} - \\hat{\\beta}_{0}.\n$$\nTaking conditional expectation given $X = x$ under the quadratic truth,\n$$\nE\\!\\left[r + \\hat{\\beta}_{1} X \\mid X = x\\right] = E\\!\\left[Y - \\hat{\\beta}_{0} \\mid X = x\\right] = \\gamma_{0} + \\gamma_{1} x + \\gamma_{2} x^{2} - \\hat{\\beta}_{0},\n$$\nwhich is a quadratic function of $x$ when $\\gamma_{2} \\neq 0$. Therefore, as $x$ varies, the expected component-plus-residual traces a curved (parabolic) pattern. The sign of $\\gamma_{2}$ determines whether the curvature is U-shaped ($\\gamma_{2} > 0$) or inverted U-shaped ($\\gamma_{2} < 0$). This provides a principled graphical diagnostic for a quadratic departure from linearity, while homoscedasticity arises from the constant variance $\\sigma^{2}$ of $\\varepsilon$, and Normality from the Gaussian $\\varepsilon$.\n\nNow we evaluate each option.\n\nOption A:\n- Data-generating process: $X \\sim \\mathrm{Uniform}(-3, 3)$; $Y = 1 - 0.5\\,X + 1.0\\,X^{2} + \\varepsilon$, $\\varepsilon \\mid X \\sim \\mathcal{N}(0, 1)$ independent of $X$.\n- Then $E(Y \\mid X = x) = 1 - 0.5\\,x + 1.0\\,x^{2}$ with a strictly positive quadratic term, $\\mathrm{Var}(Y \\mid X = x) = 1$ (constant), and $\\varepsilon$ is Gaussian. Thus linearity is violated, homoscedasticity and Normality hold.\n- From the derivation, the component-plus-residual plotted against $x$ has an expected quadratic shape; with a positive quadratic coefficient it is U-shaped.\nVerdict: Correct.\n\nOption B:\n- Data-generating process: $Y = 1 + 0.5\\,X + \\varepsilon$ with $\\varepsilon \\mid X \\sim \\mathcal{N}(0, 1 + X^{2})$.\n- Here $E(Y \\mid X = x) = 1 + 0.5\\,x$ is linear (no quadratic term), but the variance increases with $x^{2}$, violating homoscedasticity.\n- The component-plus-residual plot has expected value linear in $x$, so it does not display systematic curvature due to the mean. Heteroscedasticity typically appears as changing vertical spread (a funnel), not curvature in the mean pattern.\n- The claim that curvature arises due to heteroscedasticity is incorrect as a description of the expected mean pattern in the component-plus-residual plot.\nVerdict: Incorrect.\n\nOption C:\n- Data-generating process: $X \\sim \\mathcal{N}(0, 1)$; $Y = 2 + 1.5\\,X - 0.8\\,X^{2} + \\varepsilon$, with $\\varepsilon \\mid X \\sim \\mathcal{N}(0, 0.5^{2})$ independent of $X$.\n- Then $E(Y \\mid X = x) = 2 + 1.5\\,x - 0.8\\,x^{2}$ with a negative quadratic term, constant variance $0.5^{2}$, and Gaussian noise. Linearity is violated, homoscedasticity and Normality hold.\n- The component-plus-residual plot versus $x$ has an expected inverted parabolic shape (opening downward), consistent with the negative quadratic term.\nVerdict: Correct.\n\nOption D:\n- Data-generating process: $Y = 1 - 0.5\\,X + 1.0\\,X^{2} + \\varepsilon$ with $\\varepsilon \\mid X \\sim \\mathrm{Laplace}(0, 1)$.\n- Although $E(Y \\mid X = x)$ is quadratic (violating linearity), the noise is not Gaussian, violating the stated Normality requirement.\n- Furthermore, the assertion that the component-plus-residual plot will appear essentially linear is incorrect; as shown above, the expected component-plus-residual is quadratic in $x$ whenever the conditional mean is quadratic, regardless of heavy tails.\nVerdict: Incorrect.\n\nOption E:\n- Data-generating process: $Y = 2 + 1.5\\,X + \\varepsilon$ with $\\varepsilon$ Gaussian and homoscedastic.\n- Here $E(Y \\mid X = x)$ is linear; there is no quadratic departure. The expected component-plus-residual is linear, and any apparent curvature would be random noise, not a systematic diagnostic signal.\n- The statement that a U-shaped curvature will appear because the true mean is linear is incorrect.\nVerdict: Incorrect.\n\nTherefore, the options that correctly construct the required counterexample and correctly state how component-plus-residual plots reveal the quadratic departure are A and C.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Diagnostic checks are not just an academic exercise; they have real-world consequences for the validity of our scientific conclusions. This final practice problem  places you in a realistic biostatistical setting with a small sample size where multiple assumptions appear to be violated. Your task is to move from diagnosis to remedy, choosing the most appropriate method to construct a reliable confidence interval when the standard assumptions for OLS inference do not hold.",
            "id": "4894646",
            "problem": "A clinical researcher investigates the linear association between urinary albumin-to-creatinine ratio $Y$ (mg/g) and systolic blood pressure $X$ (mmHg) among patients with type $2$ diabetes. The researcher fits the simple linear regression model $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$ for $i = 1, \\dots, n$, where $n = 12$, $E[\\varepsilon_i] = 0$, and $\\text{Var}(\\varepsilon_i)$ is assumed constant under the classical model. The aim is to estimate a $95\\%$ confidence interval for the slope $\\beta_1$. Standard practice often constructs such intervals by relying on residual normality and homoscedasticity diagnostics in Ordinary Least Squares (OLS). However, in this dataset, diagnostic plots show the following: the residuals-versus-fitted plot exhibits a funnel shape (variance increasing with fitted values), the Quantile–Quantile (Q–Q) plot indicates a heavy right tail, and a Shapiro–Wilk (SW) test yields a $p$-value of $0.12$ (not rejecting normality, but with limited power due to small $n$). Furthermore, a preliminary attempt to analyze $\\log(Y)$ did not resolve the issues: residuals remained right-skewed, and the residuals-versus-fitted plot showed mild curvature, raising concerns about linearity and homoscedasticity even on the transformed scale.\n\nFrom first principles, confidence intervals derived under normal-error assumptions rely on distributional results that may not hold under small $n$ with skewed, non-constant variance residuals. The researcher must choose a method to obtain a more reliable interval estimate for $\\beta_1$ under these conditions.\n\nWhich approach is most appropriate to remedy the misleading reliance on residual normality and homoscedasticity for the confidence interval of $\\beta_1$ in this setting?\n\nA. Proceed with the standard $t$-based confidence interval from OLS, arguing that the Shapiro–Wilk test did not reject normality.\n\nB. Apply a logarithmic transformation to $Y$, refit the linear model, and construct the usual $t$-based interval on the transformed scale after rechecking diagnostics.\n\nC. Construct a nonparametric bootstrap confidence interval for $\\beta_1$ by resampling the observed pairs $(X_i, Y_i)$ with replacement to approximate the sampling distribution of the slope, and use the empirical quantiles (e.g., percentile or Bias-Corrected and Accelerated (BCa)) of the bootstrap slopes to form the $95\\%$ confidence interval.\n\nD. Remove the two largest residual observations to restore symmetry and recompute the standard OLS $t$-based interval.\n\nE. Rely on large-sample asymptotic normality of the OLS slope estimator and use a normal-based interval despite $n = 12$ and the observed diagnostics.",
            "solution": "The scenario involves a simple linear regression model $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$ with $n = 12$. The classical OLS confidence interval for $\\beta_1$ relies on specific assumptions: linearity of the mean function, independence of errors $\\varepsilon_i$, homoscedasticity $\\text{Var}(\\varepsilon_i) = \\sigma^2$ for all $i$, and normality $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Under these assumptions, the OLS slope estimator $\\hat{\\beta}_1$ can be written as a linear combination of the responses $Y_i$, specifically $\\hat{\\beta}_1 = \\sum_{i=1}^n w_i Y_i$ with deterministic weights $w_i$ that depend on the observed $X_i$ and satisfy $\\sum_{i=1}^n w_i = 0$. If $\\varepsilon_i$ are independent and identically distributed as normal, then $Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i$ implies that $\\hat{\\beta}_1$ is normally distributed, and the usual $t$-based confidence interval has exact coverage when the error variance is estimated from the residuals.\n\nWhen normality and homoscedasticity are violated, particularly with small $n$, these distributional results break down. Specifically:\n\n1. If $\\varepsilon_i$ are skewed, the distribution of $\\hat{\\beta}_1 = \\sum_i w_i \\varepsilon_i + \\beta_1$ inherits skewness from $\\varepsilon_i$ and may deviate materially from normality when $n$ is small. The Student’s $t$ pivotal quantity is not exactly $t$-distributed without normal errors.\n\n2. If $\\text{Var}(\\varepsilon_i)$ increases with the mean (heteroscedasticity), the variance of $\\hat{\\beta}_1$ depends on the pattern of $X_i$ and $\\text{Var}(\\varepsilon_i)$, invalidating the usual standard error formula derived under constant variance. Small $n$ limits the reliability of asymptotic approximations.\n\n3. Diagnostic tests like the Shapiro–Wilk test yielding $p = 0.12$ are inconclusive under $n = 12$; lack of rejection does not validate normality, and visual diagnostics indicate meaningful skewness and a funnel-shaped residual plot, suggesting heteroscedasticity and possible nonlinearity.\n\nA principle-based remedy is to approximate the sampling distribution of $\\hat{\\beta}_1$ empirically without imposing normality or homoscedasticity. The nonparametric bootstrap of pairs $(X_i, Y_i)$ does this by resampling the observed data structure, preserving the joint distribution, including any relationship between the mean and variance (heteroscedasticity), and the observed skewness. The steps are:\n\n1. Compute the OLS slope $\\hat{\\beta}_1$ from the original sample.\n\n2. For each bootstrap iteration $b = 1, \\dots, B$ (with $B$ large, such as $B = 2000$), draw $n$ pairs $(X_i^*, Y_i^*)$ with replacement from the original $n$ pairs $(X_i, Y_i)$.\n\n3. Fit the same linear regression to the bootstrap sample and record the slope $\\hat{\\beta}_1^{*(b)}$.\n\n4. The empirical distribution of $\\{\\hat{\\beta}_1^{*(b)}\\}_{b=1}^B$ approximates the sampling distribution of $\\hat{\\beta}_1$ under the data-generating mechanism. Use its quantiles to form a percentile $95\\%$ confidence interval, or apply Bias-Corrected and Accelerated (BCa) adjustments to correct for bias and skewness.\n\nThis approach does not rely on normal residuals, can adapt to skewed errors and non-constant variance, and is suitable for small $n$ while acknowledging the limitations inherent in very small samples. In contrast, transformations and parametric fixes may still leave residual issues unaddressed, as observed in the scenario.\n\nOption-by-option analysis:\n\nA. Proceeding with the standard $t$-based interval from OLS assumes residual normality and homoscedasticity. The Shapiro–Wilk $p$-value of $0.12$ does not confirm normality with $n = 12$, and the diagnostics show right-skew and a funnel shape, directly violating assumptions required for valid $t$-based coverage. This approach risks undercoverage or miscalibration. Verdict: Incorrect.\n\nB. A logarithmic transformation of $Y$ is a common technique to reduce right-skew and stabilize variance when the error structure is multiplicative. However, the problem states that this attempt did not resolve skewness and introduced mild curvature, indicating lingering departures from linearity and homoscedasticity even on the transformed scale. Relying on $t$-based intervals after an insufficiently effective transformation remains problematic, especially at $n = 12$. Verdict: Incorrect in this specific scenario.\n\nC. The nonparametric bootstrap of pairs $(X_i, Y_i)$ resamples the joint data, preserving the dependence structure between $X$ and $Y$, the skewness, and the heteroscedasticity pattern. Constructing percentile or Bias-Corrected and Accelerated (BCa) intervals from the empirical distribution of bootstrap slopes provides an interval that does not rely on residual normality or constant variance and directly addresses the concern of misleading normality assumptions in small samples. Verdict: Correct.\n\nD. Deleting the two largest residual observations to force symmetry is an ad hoc form of outlier removal that introduces selection bias, invalidates inference by conditioning on the data twice, and typically understates variability. It does not constitute a principled remedy for skewness or heteroscedasticity. Verdict: Incorrect.\n\nE. Asymptotic normality of $\\hat{\\beta}_1$ holds under broad conditions as $n \\to \\infty$, but with $n = 12$ and evident skewness and heteroscedasticity, large-sample approximations are unreliable, potentially yielding miscalibrated intervals. Verdict: Incorrect.\n\nTherefore, the most appropriate remedy in this setting is to use a nonparametric bootstrap of pairs to construct a confidence interval for $\\beta_1$ that does not rely on residual normality or homoscedasticity assumptions and accounts for the observed skewness and small $n$.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}