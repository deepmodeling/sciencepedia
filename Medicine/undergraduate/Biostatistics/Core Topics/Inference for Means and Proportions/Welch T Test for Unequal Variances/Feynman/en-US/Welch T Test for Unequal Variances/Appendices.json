{
    "hands_on_practices": [
        {
            "introduction": "Before we can confidently use a statistical test, we must understand its mechanics. This first practice dives into the heart of the Welch $t$-test: the Welch-Satterthwaite approximation for the degrees of freedom, $\\nu$. By exploring how $\\nu$ changes when group variances and sample sizes are unbalanced, you will gain a deeper intuition for how the test quantifies uncertainty arising from heteroscedasticity. ",
            "id": "4966248",
            "problem": "A study compares the mean serum concentration of a biomarker between two independent patient groups receiving two formulations of the same drug. The two groups are known to be heteroscedastic, with unequal population variances. You are tasked with quantifying how the imbalance in sample sizes and the ratio of sample variances affect the approximate degrees of freedom used in the unequal-variance two-sample Student’s $t$-test (commonly called Welch’s $t$-test).\n\nConsider the following two scenarios, each with group $1$ and group $2$:\n\n- Scenario A (high variance in the smaller group): group $1$ has sample size $n_{1} = 15$ and sample variance $s_{1}^{2} = 25$; group $2$ has sample size $n_{2} = 65$ and sample variance $s_{2}^{2} = 9$.\n- Scenario B (high variance in the larger group): group $1$ has sample size $n_{1} = 15$ and sample variance $s_{1}^{2} = 9$; group $2$ has sample size $n_{2} = 65$ and sample variance $s_{2}^{2} = 25$.\n\nStarting from first principles appropriate for biostatistics, namely the independence of samples, the sampling distribution properties of the sample mean, and the sampling distribution properties of the sample variance, derive the approximate degrees of freedom $\\nu$ used in Welch’s $t$-test. Then, compute $\\nu$ for Scenario A and Scenario B, and report the ratio\n$$\nR \\;=\\; \\frac{\\nu_{\\text{A}}}{\\nu_{\\text{B}}}.\n$$\nRound your final numerical answer for $R$ to four significant figures. No units are required.\n\nIn your reasoning, explain why smaller values of $\\nu$ indicate greater uncertainty arising from heteroscedasticity and sample-size imbalance, but do not use any shortcut formulas stated without derivation.",
            "solution": "The problem asks for the derivation of the approximate degrees of freedom, $\\nu$, for the Welch's $t$-test, and to compute its value for two scenarios to understand the impact of sample size imbalance and heteroscedasticity.\n\nThe Welch's $t$-test is used to test the hypothesis of equal means between two independent samples, $\\bar{X}_1$ and $\\bar{X}_2$, drawn from populations with means $\\mu_1$ and $\\mu_2$ and variances $\\sigma_1^2$ and $\\sigma_2^2$, where it is not assumed that $\\sigma_1^2 = \\sigma_2^2$. The samples have sizes $n_1$ and $n_2$, and sample variances $s_1^2$ and $s_2^2$.\n\nUnder the null hypothesis $H_0: \\mu_1 = \\mu_2$, the Welch's $t$-statistic is given by:\n$$\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n$$\nThe numerator, $\\bar{X}_1 - \\bar{X}_2$, assuming the populations are normally distributed or by the Central Limit Theorem for large sample sizes, follows a normal distribution with mean $\\mu_1 - \\mu_2$ and variance $\\text{Var}(\\bar{X}_1 - \\bar{X}_2) = \\text{Var}(\\bar{X}_1) + \\text{Var}(\\bar{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$.\nThe denominator is the standard error of the difference between the means, where the unknown population variances $\\sigma_1^2$ and $\\sigma_2^2$ are estimated by the sample variances $s_1^2$ and $s_2^2$.\n\nThe statistic $t$ is approximately distributed as a Student's $t$-distribution with an effective number of degrees of freedom, $\\nu$. To find $\\nu$, we use the Welch-Satterthwaite approximation. This method approximates the distribution of a linear combination of independent sample variances.\n\nLet the random variable for the squared standard error be $Y = \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}$. The key idea is to approximate the distribution of $Y$ by a scaled chi-squared distribution, $c\\chi^2_{\\nu}$, by matching the first two moments (mean and variance).\n\nFirst, we determine the mean and variance of $Y$.\nThe sample variance $s_i^2$ is an unbiased estimator of the population variance $\\sigma_i^2$, so $E[s_i^2] = \\sigma_i^2$. The expectation of $Y$ is:\n$$\nE[Y] = E\\left[\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right] = \\frac{E[s_1^2]}{n_1} + \\frac{E[s_2^2]}{n_2} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n$$\nNext, we find the variance of $Y$. Assuming the underlying populations are normal, the quantity $\\frac{(n_i-1)s_i^2}{\\sigma_i^2}$ follows a chi-squared distribution with $n_i-1$ degrees of freedom, $\\chi^2_{n_i-1}$. The variance of a $\\chi^2_k$ distribution is $2k$.\n$$\n\\text{Var}\\left(\\frac{(n_i-1)s_i^2}{\\sigma_i^2}\\right) = 2(n_i-1)\n$$\nUsing the property $\\text{Var}(aX) = a^2\\text{Var}(X)$, we can find the variance of $s_i^2$:\n$$\n\\left(\\frac{n_i-1}{\\sigma_i^2}\\right)^2 \\text{Var}(s_i^2) = 2(n_i-1) \\implies \\text{Var}(s_i^2) = \\frac{2(\\sigma_i^2)^2}{n_i-1}\n$$\nSince the two samples are independent, $s_1^2$ and $s_2^2$ are independent random variables. Therefore, the variance of their sum is the sum of their variances:\n$$\n\\text{Var}(Y) = \\text{Var}\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right) = \\frac{\\text{Var}(s_1^2)}{n_1^2} + \\frac{\\text{Var}(s_2^2)}{n_2^2} = \\frac{1}{n_1^2}\\left(\\frac{2(\\sigma_1^2)^2}{n_1-1}\\right) + \\frac{1}{n_2^2}\\left(\\frac{2(\\sigma_2^2)^2}{n_2-1}\\right)\n$$\n\nNow, we match these moments to those of a scaled chi-squared variable, $Z = c\\chi^2_{\\nu}$.\nThe mean and variance of $Z$ are:\n$$\nE[Z] = E[c\\chi^2_{\\nu}] = c E[\\chi^2_{\\nu}] = c\\nu\n$$\n$$\n\\text{Var}(Z) = \\text{Var}[c\\chi^2_{\\nu}] = c^2 \\text{Var}[\\chi^2_{\\nu}] = c^2(2\\nu)\n$$\nEquating the means, $E[Y] = E[Z]$:\n$$\n\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} = c\\nu\n$$\nEquating the variances, $\\text{Var}(Y) = \\text{Var}(Z)$:\n$$\n\\frac{2(\\sigma_1^2)^2}{n_1^2(n_1-1)} + \\frac{2(\\sigma_2^2)^2}{n_2^2(n_2-1)} = 2c^2\\nu\n$$\nFrom the mean equation, we have $c = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)$. Substituting this into the variance equation:\n$$\n\\frac{(\\sigma_1^2)^2}{n_1^2(n_1-1)} + \\frac{(\\sigma_2^2)^2}{n_2^2(n_2-1)} = c^2\\nu = \\frac{1}{\\nu^2}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2 \\nu = \\frac{1}{\\nu}\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2\n$$\nSolving for $\\nu$, we obtain the Welch-Satterthwaite equation for degrees of freedom:\n$$\n\\nu = \\frac{\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2}{\\frac{(\\sigma_1^2/n_1)^2}{n_1-1} + \\frac{(\\sigma_2^2/n_2)^2}{n_2-1}}\n$$\nIn practice, the population variances $\\sigma_1^2$ and $\\sigma_2^2$ are unknown. We substitute their sample estimates $s_1^2$ and $s_2^2$ to obtain the final form for the approximate degrees of freedom:\n$$\n\\nu \\approx \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\n$$\nThe value of $\\nu$ represents the effective amount of information available to estimate the variance of the difference in means. A smaller value of $\\nu$ corresponds to a $t$-distribution with heavier tails, which reflects greater uncertainty. This greater uncertainty requires a larger observed $t$-statistic to achieve statistical significance. From the formula, it can be seen that the denominator term $\\frac{(s_i^2/n_i)^2}{n_i-1}$ is the variance of the variance estimate for group $i$. If a group with a small sample size $n_i$ (and thus a small $n_i-1$ in the denominator) also has a large variance $s_i^2$, its contribution to the overall uncertainty becomes disproportionately large. This combination results in a smaller value of $\\nu$, penalizing the test for the higher uncertainty.\n\nNow we compute $\\nu$ for the two scenarios.\n\nScenario A: $n_1 = 15$, $s_1^2 = 25$; $n_2 = 65$, $s_2^2 = 9$. The smaller group has the larger variance.\nThe terms for the variance components are:\n$v_1 = \\frac{s_1^2}{n_1} = \\frac{25}{15} = \\frac{5}{3}$\n$v_2 = \\frac{s_2^2}{n_2} = \\frac{9}{65}$\nThe degrees of freedom $\\nu_A$ are:\n$$\n\\nu_A = \\frac{\\left(\\frac{5}{3} + \\frac{9}{65}\\right)^2}{\\frac{(5/3)^2}{15-1} + \\frac{(9/65)^2}{65-1}} = \\frac{\\left(\\frac{325+27}{195}\\right)^2}{\\frac{25/9}{14} + \\frac{81/4225}{64}} = \\frac{(352/195)^2}{\\frac{25}{126} + \\frac{81}{270400}}\n$$\nNumerically:\n$$\n\\nu_A \\approx \\frac{(1.66667 + 0.13846)^2}{\\frac{(1.66667)^2}{14} + \\frac{(0.13846)^2}{64}} \\approx \\frac{(1.80513)^2}{0.198413 + 0.0002996} \\approx \\frac{3.25849}{0.1987126} \\approx 16.398\n$$\n\nScenario B: $n_1 = 15$, $s_1^2 = 9$; $n_2 = 65$, $s_2^2 = 25$. The larger group has the larger variance.\nThe terms for the variance components are:\n$v_1 = \\frac{s_1^2}{n_1} = \\frac{9}{15} = \\frac{3}{5}$\n$v_2 = \\frac{s_2^2}{n_2} = \\frac{25}{65} = \\frac{5}{13}$\nThe degrees of freedom $\\nu_B$ are:\n$$\n\\nu_B = \\frac{\\left(\\frac{3}{5} + \\frac{5}{13}\\right)^2}{\\frac{(3/5)^2}{15-1} + \\frac{(5/13)^2}{65-1}} = \\frac{\\left(\\frac{39+25}{65}\\right)^2}{\\frac{9/25}{14} + \\frac{25/169}{64}} = \\frac{(64/65)^2}{\\frac{9}{350} + \\frac{25}{10816}}\n$$\nNumerically:\n$$\n\\nu_B \\approx \\frac{(0.6 + 0.38462)^2}{\\frac{(0.6)^2}{14} + \\frac{(0.38462)^2}{64}} \\approx \\frac{(0.98462)^2}{0.025714 + 0.002311} \\approx \\frac{0.96947}{0.028025} \\approx 34.595\n$$\nThe value of $\\nu_A$ is much smaller than $\\nu_B$, confirming that placing high variance in the small sample group leads to greater overall uncertainty and thus fewer effective degrees of freedom.\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\nu_A}{\\nu_B} = \\frac{16.39829}{34.59544} \\approx 0.473998\n$$\nRounding to four significant figures, we get $R \\approx 0.4740$.",
            "answer": "$$\\boxed{0.4740}$$"
        },
        {
            "introduction": "Real-world data is rarely perfect, and outliers can significantly distort statistical results, particularly the sample variance. This exercise explores the practical challenge of an outlier within the context of the Welch $t$-test. You will investigate how an extreme value can inflate the standard error of the mean difference, affect the test's power, and learn to apply diagnostic tools to identify such influential points. ",
            "id": "4966276",
            "problem": "A biostatistics study compares a continuous biomarker between two independent groups using Welch’s two-sample $t$-test. Group $C$ (control) has $n_C=24$, sample mean $\\bar{X}_C=4.1$, and sample variance $S_C^2=1.44$. Group $T$ (treatment) has $n_T=12$. An exploratory plot reveals one extremely high value in Group $T$. With all Group $T$ observations included, the sample mean and variance are $\\bar{X}_T=5.2$ and $S_T^2=6.76$. Temporarily excluding the suspected outlier yields $\\bar{X}_{T,(-)}=4.8$, $S_{T,(-)}^2=3.24$, and $n_{T,(-)}=11$. You plan to proceed with Welch’s two-sample $t$-test, which uses group-specific variability to form the denominator (the standard error (SE)) of the test statistic and adjusts the degrees of freedom to account for unequal variances.\n\nBased on first principles for sampling variability of independent sample means and how sample variance is computed, select all statements that are correct about how an outlier in the higher-variance group can affect the test and about diagnostics suited to detect such influence on $S_k^2$ and the standard error.\n\nA. In Welch’s two-sample $t$-test, an outlier that inflates the high-variance group’s $S_k^2$ will increase the estimated standard error of the mean difference, typically decreasing the magnitude of the test statistic and reducing power.\n\nB. In Welch’s two-sample $t$-test, the degrees of freedom are unaffected by changes in group variances, so an outlier does not influence them.\n\nC. A case-deletion check that recomputes the group variance and the test’s standard error with and without each observation is a direct diagnostic for identifying points that disproportionately inflate the denominator of the test statistic.\n\nD. Within-group quantile–quantile plots and robust scale estimates such as the Median Absolute Deviation (MAD) can help distinguish variance inflation due to a few outliers from generally high dispersion in that group.\n\nE. Levene’s test based on means will always flag a single outlier as variance heterogeneity, while the Brown–Forsythe variant based on medians will never do so.\n\nF. Computing within-group externally studentized residuals and comparing the Welch standard error under case deletion for each observation are diagnostics that directly assess an observation’s impact on the denominator of the Welch test statistic.\n\nSelect all that apply.",
            "solution": "The user wants a critical validation of the problem statement, followed by a full solution deriving the correct answer and evaluating each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following information for a biostatistics study using a Welch’s two-sample $t$-test:\n-   Two independent groups: Group C (control) and Group T (treatment).\n-   Group C: sample size $n_C=24$, sample mean $\\bar{X}_C=4.1$, sample variance $S_C^2=1.44$.\n-   Group T: sample size $n_T=12$.\n-   An outlier is noted in Group T.\n-   Group T (with outlier): sample mean $\\bar{X}_T=5.2$, sample variance $S_T^2=6.76$.\n-   Group T (without outlier): sample size $n_{T,(-)}=11$, sample mean $\\bar{X}_{T,(-)}=4.8$, sample variance $S_{T,(-)}^2=3.24$.\n-   The premise is that Welch's $t$-test uses group-specific variability for the standard error and adjusts degrees of freedom for unequal variances.\n-   The question is to select all correct statements regarding the effect of an outlier in the higher-variance group and the diagnostics for its influence.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is firmly grounded in standard statistical theory. Welch's $t$-test, the concepts of mean, variance, outliers, and diagnostic checks (case deletion, robust estimators) are all fundamental topics in biostatistics and statistics in general.\n2.  **Well-Posed:** The problem is well-posed. It presents a clear scenario and asks for an evaluation of several conceptual statements based on established statistical principles. A definite set of correct/incorrect answers can be determined.\n3.  **Objective:** The language is clear, precise, and objective. It presents numerical data and describes statistical procedures without subjectivity.\n4.  **Consistency Check:** The provided data is consistent. An outlier is expected to increase both the mean and the variance of its group. For Group T, removing the outlier decreases the mean from $\\bar{X}_T=5.2$ to $\\bar{X}_{T,(-)}=4.8$ and drastically decreases the variance from $S_T^2=6.76$ to $S_{T,(-)}^2=3.24$. This is entirely consistent with the properties of a high-value outlier. The problem setup is sound.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, self-consistent, and presents a well-posed question. I will proceed with the solution.\n\n### Derivation of Solution\n\nThe core of the problem relates to the components of the Welch's two-sample $t$-test.\n\nThe Welch's $t$-statistic is given by:\n$$ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{SE_{\\bar{X}_1 - \\bar{X}_2}} = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}} $$\nwhere $\\bar{X}_k$, $S_k^2$, and $n_k$ are the sample mean, sample variance, and sample size for group $k \\in \\{1, 2\\}$, respectively. The term in the denominator is the standard error (SE) of the difference in means.\n\nThe degrees of freedom ($\\nu$) are approximated by the Welch–Satterthwaite equation:\n$$ \\nu \\approx \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}} $$\nThese formulae show that both the test statistic $t$ and the degrees of freedom $\\nu$ are functions of the sample variances $S_1^2$ and $S_2^2$. An outlier that affects a sample variance will therefore affect both of these quantities.\n\n### Option-by-Option Analysis\n\n**A. In Welch’s two-sample $t$-test, an outlier that inflates the high-variance group’s $S_k^2$ will increase the estimated standard error of the mean difference, typically decreasing the magnitude of the test statistic and reducing power.**\n\n-   **Effect on Standard Error (SE):** The SE is $SE = \\sqrt{\\frac{S_C^2}{n_C} + \\frac{S_T^2}{n_T}}$. If an outlier in Group T inflates $S_T^2$, the term $\\frac{S_T^2}{n_T}$ increases, which in turn increases the value under the square root, thus increasing the SE. This part of the statement is correct. From the problem data, the SE increases from $\\sqrt{\\frac{1.44}{24} + \\frac{3.24}{11}} \\approx 0.595$ to $\\sqrt{\\frac{1.44}{24} + \\frac{6.76}{12}} \\approx 0.790$.\n-   **Effect on Test Statistic $|t|$:** The magnitude of the test statistic is $|t| = \\frac{|\\bar{X}_T - \\bar{X}_C|}{SE}$. An outlier affects both the numerator (by changing $\\bar{X}_T$) and the denominator (by changing $S_T^2$ and thus the $SE$). The influence of a single point on the sample variance is related to its squared deviation from the mean, whereas its influence on the mean is linear. For an extreme outlier, the quadratic effect on variance often dominates the linear effect on the mean. This means the denominator ($SE$) can increase proportionally more than the numerator ($|\\bar{X}_T - \\bar{X}_C|$), leading to a decrease in the magnitude of $t$. The word \"typically\" correctly qualifies this, as it is a general tendency, not an absolute rule. (The data in the problem provides a counterexample where $|t|$ increases from $\\approx 1.18$ to $\\approx 1.39$, but this does not invalidate the general principle qualified by \"typically\").\n-   **Effect on Power:** Statistical power is the probability of correctly rejecting a false null hypothesis. A smaller test statistic magnitude leads to a larger p-value, making it harder to reject the null hypothesis and thus reducing power.\n-   The statement is a correct description of the typical effects of an outlier.\n\n**Verdict: Correct**\n\n**B. In Welch’s two-sample $t$-test, the degrees of freedom are unaffected by changes in group variances, so an outlier does not influence them.**\n\n-   The Welch–Satterthwaite formula for degrees of freedom, $\\nu$, explicitly includes the sample variances $S_1^2$ and $S_2^2$:\n$$ \\nu \\approx \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}} $$\n-   Since an outlier changes the sample variance $S_k^2$ of its group, it must also change the calculated value of $\\nu$.\n-   For the given data, without the outlier, $\\nu \\approx \\frac{(\\frac{1.44}{24} + \\frac{3.24}{11})^2}{\\frac{(\\frac{1.44}{24})^2}{23} + \\frac{(\\frac{3.24}{11})^2}{10}} \\approx 14.23$.\n-   With the outlier, $\\nu \\approx \\frac{(\\frac{1.44}{24} + \\frac{6.76}{12})^2}{\\frac{(\\frac{1.44}{24})^2}{23} + \\frac{(\\frac{6.76}{12})^2}{11}} \\approx 13.34$.\n-   The degrees of freedom are clearly affected. The statement is factually incorrect.\n\n**Verdict: Incorrect**\n\n**C. A case-deletion check that recomputes the group variance and the test’s standard error with and without each observation is a direct diagnostic for identifying points that disproportionately inflate the denominator of the test statistic.**\n\n-   The denominator of the Welch's $t$-statistic is the standard error, $SE = \\sqrt{\\frac{S_C^2}{n_C} + \\frac{S_T^2}{n_T}}$.\n-   A case-deletion diagnostic involves removing one observation at a time and recalculating a statistic of interest.\n-   By recomputing the group variance $S_T^2$ (and consequently the overall $SE$) after deleting each observation from Group T, one can calculate the change in the $SE$.\n-   An observation that causes a large drop in the $SE$ upon its removal is, by definition, a point that has a large influence on (and inflates) the $SE$. This procedure is a direct measurement of each observation's influence on the denominator. The statement accurately describes this valid diagnostic technique.\n\n**Verdict: Correct**\n\n**D. Within-group quantile–quantile plots and robust scale estimates such as the Median Absolute Deviation (MAD) can help distinguish variance inflation due to a few outliers from generally high dispersion in that group.**\n\n-   **Q-Q plots:** A quantile-quantile plot for a group's data against a normal distribution will show most points falling on a straight line, with a few points deviating sharply at the ends if there are outliers. In contrast, if the data comes from a distribution with inherently high dispersion (e.g., a heavy-tailed distribution), the plot will show a systematic, non-linear pattern (e.g., S-shaped). This makes Q-Q plots an effective visual tool for this distinction.\n-   **Robust scale estimates:** The Median Absolute Deviation ($MAD$) is an estimator of scale that is resistant to outliers, unlike the standard deviation $S$. By comparing $S$ to a scaled version of the $MAD$ (e.g., $1.4826 \\times MAD$ for consistency with the standard deviation under normality), one can diagnose the source of high variance. A large discrepancy (S >> scaled MAD) suggests outlier-driven variance, while comparable values suggest that the high dispersion is a property of the bulk of the data.\n-   Both tools are standard and appropriate for the stated purpose.\n\n**Verdict: Correct**\n\n**E. Levene’s test based on means will always flag a single outlier as variance heterogeneity, while the Brown–Forsythe variant based on medians will never do so.**\n\n-   This statement uses the absolute terms \"always\" and \"never,\" which is highly suspect for statistical tests that are probabilistic.\n-   **Levene's test based on means:** This test is sensitive to outliers because it uses the group mean, which is non-robust. A single outlier can inflate the absolute deviations from the mean, making the test more likely to find significant variance heterogeneity. However, it will not \"always\" do so; if the outlier is not sufficiently extreme or the sample size is large, the test may not be significant at a given $\\alpha$ level.\n-   **Brown–Forsythe test based on medians:** This variant is more robust because it uses the group median, which is resistant to outliers. It is therefore less likely to be triggered by a single outlier. However, it will not \"never\" do so. A sufficiently extreme outlier will create one very large absolute deviation from the median, which could be enough to make the test significant. The test is robust, not immune.\n-   The use of \"always\" and \"never\" makes the statement factually incorrect.\n\n**Verdict: Incorrect**\n\n**F. Computing within-group externally studentized residuals and comparing the Welch standard error under case deletion for each observation are diagnostics that directly assess an observation’s impact on the denominator of the Welch test statistic.**\n\n-   This statement proposes two diagnostic methods.\n-   **Comparing the Welch standard error under case deletion:** As established in the analysis of option C, this is a direct and valid method for quantifying an observation's impact on the $SE$, which is the denominator.\n-   **Computing within-group externally studentized residuals:** An externally studentized residual for an observation $X_j$ is calculated as $t_j = \\frac{X_j - \\bar{X}_{(-j)}}{S_{(-j)}}$, where the mean and standard deviation are computed from the data excluding observation $j$. This metric is designed to identify outliers by assessing how far a point is from the rest of the data, scaled by a standard deviation estimate that is not influenced by the point itself. Outliers are the primary source of point-wise influence on variance. Therefore, identifying outliers with studentized residuals is a standard and direct way to flag points that are likely to have a large impact on the group variance $S_k^2$, and thus on the denominator of the Welch test statistic.\n-   Both diagnostics listed are standard tools used to assess the impact of individual observations on model parameters and test statistics, specifically in relation to variance and outliers. The statement is correct.\n\n**Verdict: Correct**",
            "answer": "$$\\boxed{ACDF}$$"
        },
        {
            "introduction": "A critical task in biostatistics is designing studies with sufficient statistical power to detect clinically meaningful effects. This final practice challenges you to perform a power and sample size calculation for a planned study that will use the Welch $t$-test. You will learn to determine the optimal sample sizes for two groups when their variances are expected to be unequal, a common and crucial step in planning robust and efficient research. ",
            "id": "4966295",
            "problem": "A biostatistics team plans a two-arm randomized study comparing the mean reduction in fasting plasma glucose between a new diet (group $1$) and a standard diet (group $2$). From a pilot study, they anticipate population standard deviations of approximately $\\sigma_{1}=8$ and $\\sigma_{2}=12$ (in the same glucose units for both groups). They wish to design the study to detect a true mean difference of $\\Delta=\\mu_{1}-\\mu_{2}=5$ in favor of the new diet. The primary analysis will use the unequal-variance two-sample $t$-test (Welch’s $t$-test) at a two-sided type I error rate $\\alpha=0.05$. Assume that the per-participant costs are equal in both groups and that the study will be powered against a difference of exactly $\\Delta$.\n\nStarting from first principles appropriate for this context (distribution of sample means, construction of the Welch test statistic, and large-sample approximations to its distribution under alternatives), determine the minimal integer sample sizes $(n_{1},n_{2})$ that achieve power $1-\\beta=0.90$ under the Welch framework. Specifically, derive the allocation rule that minimizes total sample size for a given detection threshold, obtain a continuous solution for $(n_{1},n_{2})$ from this rule, and then verify and adjust to the smallest integers that still attain at least the target power when using the Welch test with the Satterthwaite degrees-of-freedom approximation. Report your final answer as the ordered pair $(n_{1},n_{2})$ with no units. There is no rounding by significant figures; instead, choose the smallest integers that meet the power requirement.",
            "solution": "The problem statement is a well-posed and scientifically grounded exercise in biostatistical power analysis for a two-sample Welch's $t$-test with unequal variances. All necessary parameters are provided, and the objective is clearly defined. The problem is valid.\n\nThe objective is to find the minimal integer sample sizes, $n_1$ and $n_2$, for two groups that achieve a statistical power of $1-\\beta = 0.90$ for detecting a true mean difference of $\\Delta = \\mu_1 - \\mu_2 = 5$ using a two-sided Welch's $t$-test at a significance level of $\\alpha = 0.05$. The population standard deviations are given as $\\sigma_1 = 8$ and $\\sigma_2 = 12$.\n\nThe Welch's t-statistic is given by:\n$$\nT = \\frac{\\bar{X}_1 - \\bar{X}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n$$\nwhere $\\bar{X}_i$ are the sample means and $s_i^2$ are the sample variances. Under the null hypothesis $H_0: \\mu_1 - \\mu_2 = 0$, this statistic follows an approximate Student's $t$-distribution with $\\nu$ degrees of freedom, where $\\nu$ is estimated by the Satterthwaite-Welch equation.\n\nFor study planning, we can make a large-sample approximation where the $t$-distribution is approximated by a standard normal distribution, and sample variances $s_i^2$ are replaced by the anticipated population variances $\\sigma_i^2$. The standard error of the difference in means is $SE = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}$.\n\nThe two-sided test rejects $H_0$ if $|\\frac{\\bar{X}_1 - \\bar{X}_2}{SE}| > z_{\\alpha/2}$, where $z_q$ is the upper $q$ quantile of the standard normal distribution. For $\\alpha = 0.05$, we have $z_{\\alpha/2} = z_{0.025} \\approx 1.95996$.\n\nThe power, $1-\\beta$, is the probability of rejecting $H_0$ given that the alternative hypothesis $H_a: \\mu_1 - \\mu_2 = \\Delta$ is true. For $\\Delta > 0$, the power is dominated by the probability of exceeding the upper critical value:\n$$\n1-\\beta \\approx P\\left( \\frac{\\bar{X}_1 - \\bar{X}_2}{SE} > z_{\\alpha/2} \\mid \\mu_1 - \\mu_2 = \\Delta \\right)\n$$\nStandardizing the variable under $H_a$:\n$$\n1-\\beta \\approx P\\left( \\frac{(\\bar{X}_1 - \\bar{X}_2) - \\Delta}{SE} > z_{\\alpha/2} - \\frac{\\Delta}{SE} \\right) = P\\left( Z > z_{\\alpha/2} - \\frac{\\Delta}{SE} \\right)\n$$\nwhere $Z \\sim N(0,1)$. To achieve power $1-\\beta$, we require $z_{\\alpha/2} - \\frac{\\Delta}{SE} \\le -z_{\\beta}$, which gives the sample size condition:\n$$\n\\frac{\\Delta}{SE} \\ge z_{\\alpha/2} + z_{\\beta}\n$$\nFor a power of $1-\\beta=0.90$, we have $\\beta=0.10$, and $z_{\\beta} = z_{0.10} \\approx 1.28155$.\nThus, $z_{\\alpha/2} + z_{\\beta} \\approx 1.95996 + 1.28155 = 3.24151$.\nThe constraint is $\\left( \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} \\right) \\le \\left( \\frac{\\Delta}{z_{\\alpha/2} + z_{\\beta}} \\right)^2$.\n\nTo minimize the total sample size $N = n_1 + n_2$ subject to this constraint, we derive the optimal allocation ratio. We must minimize $n_1+n_2$ for a fixed value of the variance term $V = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$. Using the method of Lagrange multipliers or substitution, the total sample size is minimized when the allocation ratio is:\n$$\n\\frac{n_1}{n_2} = \\frac{\\sigma_1}{\\sigma_2}\n$$\nFor the given parameters, $\\frac{n_1}{n_2} = \\frac{8}{12} = \\frac{2}{3}$. Let $n_1 = \\frac{2}{3}n_2$. Substituting this into the power equation:\n$$\n\\frac{\\Delta^2}{(z_{\\alpha/2} + z_{\\beta})^2} \\ge \\frac{\\sigma_1^2}{(2/3)n_2} + \\frac{\\sigma_2^2}{n_2} = \\frac{1}{n_2}\\left(\\frac{3}{2}\\sigma_1^2 + \\sigma_2^2\\right)\n$$\nSolving for $n_2$:\n$$\nn_2 \\ge \\frac{(z_{\\alpha/2} + z_{\\beta})^2}{\\Delta^2} \\left(\\frac{3}{2}\\sigma_1^2 + \\sigma_2^2\\right)\n$$\nUsing the given values: $\\Delta=5$, $\\sigma_1=8$, $\\sigma_2=12$:\n$$\nn_2 \\ge \\frac{(3.24151)^2}{5^2} \\left(\\frac{3}{2}(8^2) + 12^2\\right) = \\frac{10.5073}{25} \\left(96 + 144\\right) = 0.420292 \\times 240 \\approx 100.87\n$$\nThen, $n_1 = \\frac{2}{3} n_2 \\ge \\frac{2}{3}(100.87) \\approx 67.25$.\nThe continuous solution is approximately $(n_1, n_2) = (67.25, 100.87)$, for a total size of $N \\approx 168.12$.\n\nThis large-sample approximation must be refined by using the more precise Welch-Satterthwaite formulation, which uses a $t$-distribution. The power condition becomes:\n$$\n\\frac{\\Delta}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\ge t_{\\alpha/2, \\nu} + z_{\\beta}\n$$\nwhere $\\nu$ is the Satterthwaite degrees of freedom, estimated for planning as:\n$$\n\\nu = \\frac{\\left(\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\right)^2}{\\frac{(\\sigma_1^2/n_1)^2}{n_1-1} + \\frac{(\\sigma_2^2/n_2)^2}{n_2-1}}\n$$\nWe must find the minimal integers $(n_1, n_2)$ that satisfy this condition, starting with candidates near the continuous solution. The minimal total integer size must be at least $N = \\lceil 168.12 \\rceil = 169$. The optimal allocation for $N=169$ is approximately $n_1=169 \\times (2/5) = 67.6$ and $n_2=169 \\times (3/5) = 101.4$. The closest integer pair is $(n_1, n_2) = (68, 101)$.\n\nLet's test the pair $(n_1, n_2)=(68, 101)$:\n$V_1 = \\frac{\\sigma_1^2}{n_1} = \\frac{64}{68} = \\frac{16}{17} \\approx 0.941176$\n$V_2 = \\frac{\\sigma_2^2}{n_2} = \\frac{144}{101} \\approx 1.425743$\n$SE = \\sqrt{V_1 + V_2} \\approx \\sqrt{2.366919} \\approx 1.538479$\nThe non-centrality parameter is $\\frac{\\Delta}{SE} \\approx \\frac{5}{1.538479} \\approx 3.25000$.\nThe degrees of freedom are:\n$$\n\\nu = \\frac{(2.366919)^2}{\\frac{(16/17)^2}{67} + \\frac{(144/101)^2}{100}} \\approx \\frac{5.60253}{0.013221 + 0.020327} \\approx 166.99\n$$\nWe use the floor, $\\nu=166$. The critical value is $t_{0.025, 166} \\approx 1.97424$.\nWe check if $\\frac{\\Delta}{SE} - t_{\\alpha/2, \\nu} \\ge z_{\\beta}$:\n$3.25000 - 1.97424 = 1.27576$.\nSince $1.27576 < z_{0.10} \\approx 1.28155$, the power is insufficient. The power is $\\Phi(1.27576) \\approx 0.899$.\n\nThis indicates that a total sample size of $N=169$ is not sufficient. We proceed to $N=170$. The optimal allocation for $N=170$ is $n_1 = 170 \\times \\frac{2}{5} = 68$ and $n_2 = 170 \\times \\frac{3}{5} = 102$. This pair $(68, 102)$ exactly matches the optimal allocation ratio $\\frac{68}{102} = \\frac{2}{3}$.\n\nLet's test the pair $(n_1, n_2) = (68, 102)$:\n$V_1 = \\frac{\\sigma_1^2}{n_1} = \\frac{64}{68} = \\frac{16}{17}$\n$V_2 = \\frac{\\sigma_2^2}{n_2} = \\frac{144}{102} = \\frac{24}{17}$\n$SE = \\sqrt{\\frac{16}{17} + \\frac{24}{17}} = \\sqrt{\\frac{40}{17}} \\approx 1.533930$\nThe non-centrality parameter is $\\frac{\\Delta}{SE} = \\frac{5}{\\sqrt{40/17}} \\approx 3.25965$.\nThe degrees of freedom are:\n$$\n\\nu = \\frac{(40/17)^2}{\\frac{(16/17)^2}{67} + \\frac{(24/17)^2}{101}} = \\frac{1600/289}{\\frac{256}{289 \\cdot 67} + \\frac{576}{289 \\cdot 101}} = \\frac{1600}{\\frac{256}{67} + \\frac{576}{101}} \\approx \\frac{1600}{3.8209 + 5.7030} \\approx 167.99\n$$\nWe use the floor, $\\nu=167$. The critical value is $t_{0.025, 167} \\approx 1.97413$.\nWe check if $\\frac{\\Delta}{SE} - t_{\\alpha/2, \\nu} \\ge z_{\\beta}$:\n$3.25965 - 1.97413 = 1.28552$.\nSince $1.28552 > z_{0.10} \\approx 1.28155$, the power is sufficient. The power is $\\Phi(1.28552) \\approx 0.9007$.\n\nSince a total sample size of $N=169$ is insufficient, the minimum total sample size is $N=170$. The pair $(n_1, n_2) = (68, 102)$ achieves the required power and perfectly matches the optimal allocation rule. Therefore, it represents the minimal integer sample sizes that satisfy the problem's constraints.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 68 & 102 \\end{pmatrix}}\n$$"
        }
    ]
}