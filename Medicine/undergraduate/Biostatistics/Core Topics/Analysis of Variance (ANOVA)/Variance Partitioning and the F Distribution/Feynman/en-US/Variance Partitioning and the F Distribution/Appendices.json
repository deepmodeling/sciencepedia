{
    "hands_on_practices": [
        {
            "introduction": "The best way to truly grasp a statistical method is to build it from the ground up. This first exercise challenges you to do just that for the Analysis of Variance (ANOVA). By writing a program to compute the $F$-statistic from raw data, you will demystify the ANOVA table and see firsthand how the total variability in a dataset is partitioned into its fundamental components: the variation *between* groups and the variation *within* groups . This practice will solidify your understanding of sums of squares, mean squares, and their role in hypothesis testing.",
            "id": "4965592",
            "problem": "You are to write a complete program that constructs a one-way Analysis of Variance (ANOVA) table from raw data using first principles of variance partitioning and then reports only the computed $F$-statistic for each provided dataset. Your implementation must not call any built-in ANOVA routines; it must compute all quantities from core definitions of averages and sums of squared deviations. Begin from the following fundamental base: the definitions of sample means and sums of squared deviations, the identity that total variation partitions additively into within-group and between-group components, and the independence and homoscedasticity assumptions of the one-way ANOVA model under which the $F$-statistic has an $F$ distribution. Specifically, use only the following principles as starting points:\n- For any finite set of real-valued observations $\\{y\\}$, the sample mean is $\\bar{y} = \\frac{1}{n}\\sum_{j=1}^{n} y_{j}$ and the total sum of squared deviations is $\\sum_{j=1}^{n} (y_{j} - \\bar{y})^{2}$.\n- In a one-way grouping with $k$ groups and group sizes $n_{i}$, the total sum of squares partitions as $SS_{T} = SS_{W} + SS_{B}$, where $SS_{W}$ is the within-groups sum of squares computed from deviations to group means and $SS_{B}$ is the between-groups sum of squares computed from deviations of group means to the overall mean, weighted by group sizes.\n- Under the one-way ANOVA model with independent, normally distributed errors of equal variance across groups, the ratio of mean squares $F = MS_{B}/MS_{W}$ follows an $F$ distribution with numerator degrees of freedom $k-1$ and denominator degrees of freedom $N - k$ under the null hypothesis that all group means are equal, where $N = \\sum_{i=1}^{k} n_{i}$.\n\nYour program must, for each dataset:\n1. Compute each group mean $\\bar{y}_{i}$, the overall mean $\\bar{y}$, the within-groups sum of squares $SS_{W} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(y_{ij} - \\bar{y}_{i})^{2}$, the total sum of squares $SS_{T} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_{i}}(y_{ij} - \\bar{y})^{2}$, and the between-groups sum of squares $SS_{B} = SS_{T} - SS_{W}$ (thus verifying the variance partitioning identity).\n2. Compute the degrees of freedom $df_{B} = k - 1$ and $df_{W} = N - k$, the mean squares $MS_{B} = SS_{B}/df_{B}$ and $MS_{W} = SS_{W}/df_{W}$, and the $F$-statistic $F = MS_{B}/MS_{W}$.\n3. For numerical robustness, if $MS_{W} = 0$, define $F$ as $+\\infty$ if $SS_{B} > 0$, and define $F$ as $\\mathrm{nan}$ if also $SS_{B} = 0$.\n\nTest Suite:\nUse exactly the following four datasets, each represented as a list of groups, where each group is a list of real numbers.\n\n- Test case A (balanced, clear between-group separation): $k = 3$, group data\n  - Group $1$: $[4.1, 4.3, 4.2, 4.0]$\n  - Group $2$: $[5.0, 5.1, 4.9, 5.2]$\n  - Group $3$: $[6.0, 5.8, 6.2, 5.9]$\n- Test case B (no between-group variation; all group means exactly equal): $k = 3$, group data\n  - Group $1$: $[10.0, 12.0, 8.0]$\n  - Group $2$: $[9.0, 10.0, 11.0]$\n  - Group $3$: $[7.0, 10.0, 13.0]$\n- Test case C (small and unequal sample sizes): $k = 2$, group data\n  - Group $1$: $[2.0, 2.1]$\n  - Group $2$: $[2.5, 2.4, 2.6]$\n- Test case D (unequal group sizes with one group shifted): $k = 4$, group data\n  - Group $1$: $[15.0, 16.0, 14.0]$\n  - Group $2$: $[15.0, 15.5]$\n  - Group $3$: $[14.8, 15.2, 15.0, 15.1]$\n  - Group $4$: $[18.0, 17.5, 18.2]$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output only the $F$-statistic as a floating-point number rounded to $6$ decimal places, in the order A, B, C, D. For example, the final output must look like\n$[\\text{FA},\\text{FB},\\text{FC},\\text{FD}]$\nwith no additional text or whitespace beyond what is necessary to separate the numbers with commas.\n\nNo physical units are involved in this problem, so do not report any units. Do not read any input; the program must be fully self-contained and reproducible as specified above.",
            "solution": "The problem requires the implementation of a one-way Analysis of Variance (ANOVA) from first principles to compute the $F$-statistic for several datasets. The solution is derived by systematically applying the definitions of variance components as specified.\n\nThe core principle of one-way ANOVA is the partitioning of total variation in a dataset into variation between groups and variation within groups. Let the data consist of $k$ groups. The $i$-th group (for $i \\in \\{1, 2, \\dots, k\\}$) contains $n_i$ observations, denoted by $y_{ij}$ where $j \\in \\{1, 2, \\dots, n_i\\}$. The total number of observations in the dataset is $N = \\sum_{i=1}^{k} n_i$.\n\nThe algorithmic process for calculating the $F$-statistic for a given dataset proceeds as follows:\n\n1.  **Calculation of Means**:\n    First, we compute the necessary sample means.\n    -   The mean of each group $i$, denoted $\\bar{y}_i$, is calculated as:\n        $$ \\bar{y}_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} y_{ij} $$\n    -   The overall mean of all observations, denoted $\\bar{y}$, is calculated as:\n        $$ \\bar{y} = \\frac{1}{N} \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} y_{ij} $$\n\n2.  **Calculation of Sums of Squares (SS)**:\n    The variance partitioning is performed by calculating the sums of squared deviations.\n    -   The **Total Sum of Squares** ($SS_T$) measures the total variation of all data points around the overall mean. It is defined as:\n        $$ SS_T = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y})^2 $$\n    -   The **Within-Groups Sum of Squares** ($SS_W$), also known as the error sum of squares, measures the variation of data points around their respective group means. It is calculated by summing the squared deviations within each group and then summing these results across all groups:\n        $$ SS_W = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (y_{ij} - \\bar{y}_i)^2 $$\n    -   The **Between-Groups Sum of Squares** ($SS_B$) measures the variation of the group means around the overall mean, weighted by group size. As per the problem's directive, this quantity is computed using the fundamental partitioning identity of ANOVA:\n        $$ SS_B = SS_T - SS_W $$\n        This identity, $SS_T = SS_B + SS_W$, is a cornerstone of ANOVA, stating that total variation is the sum of between-group and within-group variations.\n\n3.  **Calculation of Degrees of Freedom (df)**:\n    Each sum of squares term is associated with a number of degrees of freedom.\n    -   The degrees of freedom for $SS_B$ are $df_B = k - 1$, corresponding to the $k$ group means minus $1$ constraint (the overall mean).\n    -   The degrees of freedom for $SS_W$ are $df_W = N - k$, corresponding to the $N$ total observations minus the $k$ group means that were calculated from them.\n\n4.  **Calculation of Mean Squares (MS)**:\n    The mean squares are the sums of squares normalized by their respective degrees of freedom, representing average variation.\n    -   The **Mean Square Between Groups** is:\n        $$ MS_B = \\frac{SS_B}{df_B} $$\n    -   The **Mean Square Within Groups** is:\n        $$ MS_W = \\frac{SS_W}{df_W} $$\n    $MS_B$ represents the variance between the groups, while $MS_W$ represents the pooled variance within the groups.\n\n5.  **Calculation of the F-Statistic**:\n    The $F$-statistic is the ratio of the between-group variance to the within-group variance.\n    $$ F = \\frac{MS_B}{MS_W} $$\n    A large $F$-value suggests that the variation between groups is significantly larger than the variation within groups, providing evidence against the null hypothesis that all group means are equal.\n\n6.  **Handling of Special Cases**:\n    The problem specifies rules for robustness when the denominator, $MS_W$, is zero. This occurs if and only if $SS_W=0$, which happens when all observations within each group are identical.\n    -   If $MS_W = 0$ (i.e., $SS_W = 0$) and $SS_B > 0$, the variation between groups is non-zero while the variation within is zero. This implies infinitely strong evidence for a difference, so $F$ is defined as $+\\infty$.\n    -   If $MS_W = 0$ (i.e., $SS_W = 0$) and $SS_B = 0$, all observations in the entire dataset are identical. In this case, the F-statistic is indeterminate, and is defined as Not-a-Number ($\\mathrm{nan}$).\n\nThe implementation translates these steps into a function that processes each dataset, calculates all intermediate quantities ($N$, $k$, means, $SS_T$, $SS_W$, $SS_B$, $df_B$, $df_W$, $MS_B$, $MS_W$), and returns the final $F$-statistic, including the specified handling for special cases. The function is then called for each test case, and the results are formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef compute_f_statistic(data: list[list[float]]) -> float:\n    \"\"\"\n    Computes the F-statistic for a one-way ANOVA from first principles.\n\n    Args:\n        data: A list of lists, where each inner list represents a group's data.\n\n    Returns:\n        The calculated F-statistic as a float.\n    \"\"\"\n    # 1. Compute counts and consolidate data into a single array.\n    k = len(data)\n    if k <= 1:\n        # F-statistic is not well-defined for a single group or no groups.\n        return math.nan\n\n    all_obs_list = [obs for group in data for obs in group]\n    N = len(all_obs_list)\n    \n    if N <= k:\n        # This implies at least one group is empty or all groups have 1 obs,\n        # leading to df_w <= 0.\n        if N == k and all(len(g)==1 for g in data):\n             # SS_W will be 0. Need to check SSB\n             ss_b_check = np.var(all_obs_list) * N\n             if ss_b_check > 0: return float('inf')\n             else: return float('nan')\n        return math.nan\n\n    all_obs_np = np.array(all_obs_list, dtype=np.float64)\n\n    # 2. Compute the group means and the overall mean.\n    overall_mean = np.mean(all_obs_np)\n    group_means = [np.mean(np.array(g, dtype=np.float64)) for g in data]\n\n    # 3. Compute the Sums of Squares (SS).\n    # SS_T: Total Sum of Squares\n    ss_t = np.sum((all_obs_np - overall_mean)**2)\n\n    # SS_W: Within-Groups Sum of Squares\n    ss_w = 0.0\n    for i in range(k):\n        group_data = np.array(data[i], dtype=np.float64)\n        ss_w += np.sum((group_data - group_means[i])**2)\n    \n    # SS_B: Between-Groups Sum of Squares, derived from the partitioning identity.\n    ss_b = ss_t - ss_w\n    \n    # Numpy's float precision can sometimes make a very small positive number\n    # slightly negative. We correct this for ss_b, which must be non-negative.\n    if ss_b < 0 and np.isclose(ss_b, 0):\n        ss_b = 0.0\n\n    # 4. Compute degrees of freedom.\n    df_b = k - 1\n    df_w = N - k\n\n    # 5. Handle special cases as per problem description (based on SS_W).\n    # MS_W = 0 if and only if SS_W = 0.\n    if np.isclose(ss_w, 0):\n        ss_w = 0.0\n\n    if ss_w == 0:\n        if ss_b > 0:\n            return float('inf')\n        else: # ss_b is also 0\n            return float('nan')\n\n    # 6. Compute Mean Squares (MS) and the F-statistic.\n    ms_b = ss_b / df_b\n    ms_w = ss_w / df_w\n    \n    f_statistic = ms_b / ms_w\n    \n    return f_statistic\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A\n        [[4.1, 4.3, 4.2, 4.0], [5.0, 5.1, 4.9, 5.2], [6.0, 5.8, 6.2, 5.9]],\n        # Test case B\n        [[10.0, 12.0, 8.0], [9.0, 10.0, 11.0], [7.0, 10.0, 13.0]],\n        # Test case C\n        [[2.0, 2.1], [2.5, 2.4, 2.6]],\n        # Test case D\n        [[15.0, 16.0, 14.0], [15.0, 15.5], [14.8, 15.2, 15.0, 15.1], [18.0, 17.5, 18.2]]\n    ]\n\n    results = []\n    for case in test_cases:\n        f_value = compute_f_statistic(case)\n        results.append(f_value)\n\n    # Final print statement in the exact required format.\n    # The format specifier {:.6f} correctly handles regular floats, 'inf', and 'nan'.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having learned the mechanics of ANOVA, we now explore the elegant geometry that underpins it. This practice delves into the concept of orthogonality, a critical property of well-balanced experimental designs . You will learn to think about design factors as vectors in a high-dimensional space and verify that the statistical independence of sums of squares is a direct consequence of the geometric orthogonality of these vectors. This exercise provides a powerful intuition for why experimental design is so crucial for clear, unambiguous results.",
            "id": "4965573",
            "problem": "You are given a linear model context where a response vector is modeled by additive effects of two design factors. The goal is to check orthogonality of design factors by computing pairwise inner products of centered columns, and to relate orthogonality to the independence of sums of squares components and their Fisher-Snedecor $F$ distribution (defined as the ratio of two scaled chi-square variables). Work entirely from first principles of linear models and projections in Euclidean space, without using pre-specified analysis of variance formulas.\n\nDefinitions and assumptions to use as a base:\n- Let the response vector be $y \\in \\mathbb{R}^n$. Let $X_A \\in \\mathbb{R}^{n \\times p_A}$ and $X_B \\in \\mathbb{R}^{n \\times p_B}$ be design matrices encoding two categorical factors $A$ and $B$ using any full-rank contrast coding of the main effects. Let $\\mathbf{1} \\in \\mathbb{R}^n$ be the intercept column (all ones).\n- Columns may be centered by subtracting their column means to remove the intercept direction for inner product checks.\n- Use the orthogonal projection onto a column space $S$, defined as the Euclidean projection onto $S$. Treat projection operators as symmetric idempotent linear maps on $\\mathbb{R}^n$.\n- Use ranks of design matrices to compute degrees of freedom for the effects and the error.\n- Under the standard linear model with independent identically distributed Gaussian error, quadratic forms induced by orthogonal projections onto mutually orthogonal subspaces are independent. When divided by their degrees of freedom, the ratio of an effect sum of squares to the error mean square follows the Fisher-Snedecor $F$ distribution.\n\nYour program must:\n- For each test case, build effect-coded design matrices for factors $A$ and $B$ with $L_A - 1$ and $L_B - 1$ columns respectively, where $L_A$ and $L_B$ are the numbers of levels. Use a deterministic effect coding scheme where, for $L-1$ columns, entries are $+1$ for membership in each non-reference level and $-1$ for membership in the reference level, and $0$ otherwise. Then center each column by subtracting its mean.\n- Compute pairwise inner products between every centered column of $X_A$ and every centered column of $X_B$. Also compute the maximum absolute normalized inner product (absolute correlation) across all pairs. Declare the orthogonality check as true if this maximum absolute normalized inner product is less than a tolerance $\\tau$.\n- Construct the intercept projector onto $\\mathrm{span}(\\mathbf{1})$ and the projectors onto $\\mathrm{span}([\\mathbf{1}, X_A])$ and $\\mathrm{span}([\\mathbf{1}, X_B])$ using the Moore-Penrose pseudoinverse, and form $Q_A$ and $Q_B$ as the unique effect projectors for $A$ and $B$ after removing the intercept (difference of nested projectors). Compute $SSE$ using the projector onto the full model $\\mathrm{span}([\\mathbf{1}, X_A, X_B])$.\n- Compute the degrees of freedom $df_A$, $df_B$, and $df_e$ using matrix ranks of the nested design matrices.\n- Compute the $F$ statistic for each factor as the ratio of its mean square to the error mean square, and its tail probability under the Fisher-Snedecor $F$ distribution. Declare the independence condition as true if and only if the unique effect projectors $Q_A$ and $Q_B$ are orthogonal (their product is numerically near the zero operator under the tolerance $\\tau$).\n- Use a strict tolerance $\\tau = 10^{-10}$ for all orthogonality checks.\n\nTest Suite:\nImplement the above for the following three cases. For all cases, let the deterministic residual be $r_i = 0.1 \\times ((i \\bmod 3) - 1)$ for index $i$ starting at $0$. The mean and main effects are $ \\mu = 10$, $ \\alpha = [-2, 0, 2]$ for factor $A$ levels, and $\\beta = [-1, 1]$ for factor $B$ levels. The response is defined as $y_i = \\mu + \\alpha[A_i] + \\beta[B_i] + r_i$.\n\n- Case $1$ (balanced $3 \\times 2$ design, two replicates per cell; total $n=12$):\n  - $A = [\\,0,0,1,1,2,2,0,0,1,1,2,2\\,]$\n  - $B = [\\,0,1,0,1,0,1,0,1,0,1,0,1\\,]$\n\n- Case $2$ (unbalanced crossing; total $n=11$):\n  - $A = [\\,0,0,0,1,1,2,2,2,2,2,1\\,]$\n  - $B = [\\,0,1,0,0,1,0,1,0,1,0,1\\,]$\n\n- Case $3$ (nested/overlapping factors; $B$ is a binary collapse of $A$ where $B=1$ if $A=2$ and $B=0$ otherwise; balanced $n=12$):\n  - $A = [\\,0,0,1,1,2,2,0,0,1,1,2,2\\,]$\n  - $B = [\\,0,0,0,0,1,1,0,0,0,0,1,1\\,]$\n\nFor each case, output the following in order:\n$[$\n- the orthogonality check for centered columns (a boolean),\n- the independence condition check for unique effect projectors $Q_A$ and $Q_B$ (a boolean),\n- the maximum absolute normalized inner product across factor columns (a float),\n- the $F$ statistic for factor $A$ (a float),\n- the tail probability for factor $A$ under the Fisher-Snedecor $F$ distribution (a float),\n- the $F$ statistic for factor $B$ (a float),\n- the tail probability for factor $B$ under the Fisher-Snedecor $F$ distribution (a float)\n$]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is the list for a case in the exact order above. For example, the output should look like $[ \\text{case1\\_list}, \\text{case2\\_list}, \\text{case3\\_list} ]$.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of linear statistical models, is mathematically well-posed, and all terms and procedures are objectively and completely defined. I will now proceed with the solution.\n\nThe problem requires an analysis of a two-factor linear model from first principles, using the geometric interpretation of linear regression in terms of vector projections. We will construct the necessary matrices and operators to partition the variance of a response vector $y$ and to test the significance of the two factors, $A$ and $B$.\n\n**1. Linear Model and Vector Space Formulation**\n\nThe given linear model is $y_i = \\mu + \\alpha[A_i] + \\beta[B_i] + r_i$. In vector form, this can be written as $y = X\\theta + \\epsilon$, where $y \\in \\mathbb{R}^n$ is the response vector and $\\epsilon$ is the error vector. The design matrix $X$ and parameter vector $\\theta$ encapsulate the model structure. Specifically, we can represent the model as:\n$$\ny = \\mathbf{1}\\mu + X_A\\alpha^* + X_B\\beta^* + \\epsilon\n$$\nwhere $\\mathbf{1}$ is the $n \\times 1$ vector of ones for the intercept $\\mu$, $X_A \\in \\mathbb{R}^{n \\times (L_A-1)}$ and $X_B \\in \\mathbb{R}^{n \\times (L_B-1)}$ are the design matrices for the main effects of factors $A$ (with $L_A$ levels) and $B$ (with $L_B$ levels), and $\\alpha^*$ and $\\beta^*$ are vectors of effect coefficients.\n\n**2. Projection Operators and Sums of Squares**\n\nThe core of this analysis is the orthogonal projection of the response vector $y$ onto various subspaces of $\\mathbb{R}^n$. A projection onto the column space $\\mathcal{C}(Z)$ of a matrix $Z$ is performed by a projection operator $P_Z$. If $Z$ has full column rank, $P_Z = Z(Z^T Z)^{-1}Z^T$. More generally, using the Moore-Penrose pseudoinverse $Z^\\dagger$, the projector is $P_Z = Z Z^\\dagger$. Projection operators are symmetric ($P_Z^T = P_Z$) and idempotent ($P_Z^2 = P_Z$).\n\nThe sum of squares ($SS$) associated with a set of predictors $Z$ is the squared Euclidean norm of the projection of $y$ onto $\\mathcal{C}(Z)$:\n$$\nSS(Z) = \\|P_Z y\\|^2 = y^T P_Z^T P_Z y = y^T P_Z y\n$$\n\nThe problem requires us to compute specific sums of squares. We define the following subspaces and their projectors:\n- Intercept only: Subspace $S_1 = \\mathrm{span}(\\mathbf{1})$, Projector $P_1$.\n- Intercept and Factor A: Subspace $S_{1,A} = \\mathrm{span}([\\mathbf{1}, X_A])$, Projector $P_{1,A}$.\n- Intercept and Factor B: Subspace $S_{1,B} = \\mathrm{span}([\\mathbf{1}, X_B])$, Projector $P_{1,B}$.\n- Full model (A+B): Subspace $S_{1,A,B} = \\mathrm{span}([\\mathbf{1}, X_A, X_B])$, Projector $P_{1,A,B}$.\n\nThe unique contribution of a factor is assessed by the additional variance it explains. The problem defines the unique effect projector for factor $A$ as $Q_A = P_{1,A} - P_1$, and for factor $B$ as $Q_B = P_{1,B} - P_1$. These are also projection operators. $Q_A$ projects onto the subspace of $\\mathcal{C}([\\mathbf{1}, X_A])$ that is orthogonal to $\\mathcal{C}(\\mathbf{1})$. This subspace is precisely the column space of the centered design matrix, $\\mathcal{C}(\\tilde{X}_A)$, where $\\tilde{X}_A = (I - P_1)X_A$.\n\nThe sums of squares for the factors and the error are then:\n- Sum of Squares for A (given intercept): $SS_A = \\|Q_A y\\|^2 = y^T Q_A y$.\n- Sum of Squares for B (given intercept): $SS_B = \\|Q_B y\\|^2 = y^T Q_B y$.\n- Sum of Squares for Error: $SSE = \\|y - P_{1,A,B} y\\|^2 = \\|(I - P_{1,A,B})y\\|^2 = y^T (I - P_{1,A,B}) y$.\n\n**3. Degrees of Freedom and the F-statistic**\n\nThe degrees of freedom ($df$) for any sum of squares term $y^T P y$ is the rank of the projection matrix $P$, which is also its trace, $df_P = \\mathrm{rank}(P) = \\mathrm{tr}(P)$.\n- $df_A = \\mathrm{rank}(Q_A) = \\mathrm{rank}(P_{1,A}) - \\mathrm{rank}(P_1) = L_A - 1$.\n- $df_B = \\mathrm{rank}(Q_B) = \\mathrm{rank}(P_{1,B}) - \\mathrm{rank}(P_1) = L_B - 1$.\n- $df_e = \\mathrm{rank}(I - P_{1,A,B}) = n - \\mathrm{rank}([\\mathbf{1}, X_A, X_B])$.\n\nA mean square ($MS$) is a sum of squares divided by its degrees of freedom, $MS = SS/df$. The F-statistic for testing the significance of a factor is the ratio of its mean square to the mean square for error ($MSE$):\n- $F_A = \\frac{MS_A}{MSE} = \\frac{SS_A / df_A}{SSE / df_e}$\n- $F_B = \\frac{MS_B}{MSE} = \\frac{SS_B / df_B}{SSE / df_e}$\n\nUnder the null hypothesis that a factor has no effect and assuming i.i.d. Gaussian errors, this statistic follows a Fisher-Snedecor F-distribution, $F \\sim F(df_{effect}, df_e)$.\n\n**4. Orthogonality and Independence**\n\nTwo concepts of orthogonality are tested:\n- **Design Orthogonality**: Factors $A$ and $B$ are orthogonal if their centered design matrices are orthogonal, i.e., $\\tilde{X}_A^T \\tilde{X}_B = \\mathbf{0}$. This means that every column of $\\tilde{X}_A$ is orthogonal to every column of $\\tilde{X}_B$. This is tested by computing all pairwise inner products and checking if they are numerically zero. A balanced design (equal number of observations in each cell) guarantees this property.\n- **Projector Orthogonality (Independence of SS)**: The sums of squares $SS_A$ and $SS_B$ are statistically independent if their corresponding projection matrices are orthogonal, i.e., $Q_A Q_B = \\mathbf{0}$. As established, $Q_A$ is the projector onto $\\mathcal{C}(\\tilde{X}_A)$ and $Q_B$ is the projector onto $\\mathcal{C}(\\tilde{X}_B)$. The condition $Q_A Q_B = \\mathbf{0}$ is equivalent to the orthogonality of these two subspaces, which in turn is equivalent to $\\tilde{X}_A^T \\tilde{X}_B = \\mathbf{0}$.\n\nTherefore, the check on pairwise inner products of centered columns and the check on the orthogonality of projectors $Q_A$ and $Q_B$ are mathematically equivalent. We expect them to yield the same Boolean result in all cases.\n\n**5. Implementation Steps**\n\nFor each test case, we perform the following calculations:\n1.  Generate the response vector $y$ of length $n$ using the given model parameters and residual function.\n2.  Construct the effect-coded design matrices $X_A$ (size $n \\times (L_A-1)$) and $X_B$ (size $n \\times (L_B-1)$). We use an effect-coding scheme where the last level is a reference level.\n3.  Center $X_A$ and $X_B$ by subtracting column means to get $\\tilde{X}_A$ and $\\tilde{X}_B$.\n4.  Compute all pairwise inner products between columns of $\\tilde{X}_A$ and $\\tilde{X}_B$. Normalize them to get correlations and find the maximum absolute value. Check if this value is less than a tolerance $\\tau=10^{-10}$ to test design orthogonality.\n5.  Construct the projection matrices $P_1$, $P_{1,A}$, $P_{1,B}$, and $P_{1,A,B}$ using the pseudoinverse method.\n6.  Compute the unique effect projectors $Q_A = P_{1,A} - P_1$ and $Q_B = P_{1,B} - P_1$.\n7.  Check for projector orthogonality by computing the Frobenius norm of their product, $\\|Q_A Q_B\\|_F$, and comparing it to $\\tau$.\n8.  Calculate $SS_A$, $SS_B$, and $SSE$ using the projectors and $y$.\n9.  Calculate degrees of freedom $df_A$, $df_B$, and $df_e$ from matrix ranks.\n10. Compute $F_A$, $F_B$, and their corresponding tail probabilities (p-values) using the F-distribution from `scipy.stats`.\n11. Collect and format the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the validation and analysis for all test cases.\n    \"\"\"\n    \n    # Define parameters common to all test cases\n    mu = 10.0\n    alpha = np.array([-2.0, 0.0, 2.0])\n    beta = np.array([-1.0, 1.0])\n    r_func = lambda i: 0.1 * ((i % 3) - 1)\n    tau = 1e-10\n\n    # Define the test cases\n    test_cases = [\n        # Case 1: Balanced 3x2 design, n=12\n        {'A': np.array([0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2]),\n         'B': np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])},\n        \n        # Case 2: Unbalanced crossing, n=11\n        {'A': np.array([0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 1]),\n         'B': np.array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1])},\n        \n        # Case 3: Nested/overlapping factors, n=12\n        {'A': np.array([0, 0, 1, 1, 2, 2, 0, 0, 1, 1, 2, 2]),\n         'B': np.array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1])}\n    ]\n\n    all_results = []\n    for case_data in test_cases:\n        A_levels = case_data['A']\n        B_levels = case_data['B']\n        \n        # Generate response vector y\n        n = len(A_levels)\n        residuals = np.array([r_func(i) for i in range(n)])\n        y = mu + alpha[A_levels] + beta[B_levels] + residuals\n        \n        # --- Start Analysis ---\n\n        def build_design_matrix(levels, num_levels):\n            \"\"\"Builds an effect-coded design matrix.\"\"\"\n            n_obs = len(levels)\n            ref_level = num_levels - 1\n            X = np.zeros((n_obs, num_levels - 1))\n            for i in range(n_obs):\n                level = levels[i]\n                if level == ref_level:\n                    X[i, :] = -1.0\n                else:\n                    X[i, level] = 1.0\n            return X\n\n        LA = len(np.unique(A_levels))\n        LB = len(np.unique(B_levels))\n        \n        XA = build_design_matrix(A_levels, LA)\n        XB = build_design_matrix(B_levels, LB)\n\n        # 1. Orthogonality check of centered columns\n        XA_c = XA - XA.mean(axis=0)\n        XB_c = XB - XB.mean(axis=0)\n        \n        max_abs_corr = 0.0\n        if XA_c.shape[1] > 0 and XB_c.shape[1] > 0:\n            for i in range(XA_c.shape[1]):\n                col_A = XA_c[:, i]\n                norm_A = np.linalg.norm(col_A)\n                if norm_A < tau: continue\n                for j in range(XB_c.shape[1]):\n                    col_B = XB_c[:, j]\n                    norm_B = np.linalg.norm(col_B)\n                    if norm_B < tau: continue\n                    \n                    inner_product = np.dot(col_A, col_B)\n                    correlation = inner_product / (norm_A * norm_B)\n                    max_abs_corr = max(max_abs_corr, abs(correlation))\n\n        orthogonality_check = max_abs_corr < tau\n\n        # 2. Construct projectors and check independence\n        def make_projector(Z):\n            return Z @ np.linalg.pinv(Z)\n\n        one = np.ones((n, 1))\n        X_1 = one\n        X_1A = np.hstack([one, XA])\n        X_1B = np.hstack([one, XB])\n        \n        # Handle potential linear dependencies in full model for Case 3\n        # For rank calculation, we must be careful with stacking.\n        # If B is a function of A, hstack([1, X_A, X_B]) will be rank-deficient.\n        # np.linalg.matrix_rank will handle this.\n        X_1AB = np.hstack([one, XA, XB])\n\n        P_1 = make_projector(X_1)\n        P_1A = make_projector(X_1A)\n        P_1B = make_projector(X_1B)\n        P_1AB = make_projector(X_1AB)\n        \n        QA = P_1A - P_1\n        QB = P_1B - P_1\n        \n        independence_check = np.linalg.norm(QA @ QB, 'fro') < tau\n\n        # 3. Compute DFs, SS, and F-statistics\n        df_A = XA.shape[1]\n        df_B = XB.shape[1]\n        df_e = n - np.linalg.matrix_rank(X_1AB)\n\n        SSA = y.T @ QA @ y\n        SSB = y.T @ QB @ y\n        SSE = y.T @ (np.identity(n) - P_1AB) @ y\n        \n        MSA = SSA / df_A if df_A > 0 else 0\n        MSB = SSB / df_B if df_B > 0 else 0\n        MSE = SSE / df_e if df_e > 0 else np.nan\n\n        if MSE > 0 and MSA >= 0 and df_A > 0:\n            F_A = MSA / MSE\n            p_A = f.sf(F_A, df_A, df_e)\n        else:\n            F_A = np.nan\n            p_A = np.nan\n\n        if MSE > 0 and MSB >= 0 and df_B > 0:\n            F_B = MSB / MSE\n            p_B = f.sf(F_B, df_B, df_e)\n        else:\n            F_B = np.nan\n            p_B = np.nan\n            \n        case_results = [\n            orthogonality_check,\n            independence_check,\n            float(max_abs_corr),\n            float(F_A),\n            float(p_A),\n            float(F_B),\n            float(p_B)\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The format required is a string representation of a list of lists.\n    # str(list) in Python produces '[item1, item2]', so joining str(r) for each result list r works.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a solid theoretical foundation, we can now turn to one of the most practical applications of the F-distribution: planning for discovery. Before collecting any data, a researcher must ask, \"How large a sample do I need to detect a meaningful effect?\" This exercise guides you in building a power analysis tool to answer that very question . By working with the noncentral F-distribution, which models the behavior of the F-statistic when an effect is truly present, you will learn to calculate the sample size required to ensure your experiment has a high probability of success.",
            "id": "4965568",
            "problem": "You are tasked with constructing a program that performs a power analysis for a balanced one-way fixed-effects Analysis of Variance (ANOVA). The program must determine the minimal integer per-group sample size required to achieve a specified probability of correctly rejecting the null hypothesis, under assumed normality and a common within-group variance. The approach must explicitly compare a central critical threshold to an inverse survival quantile from the noncentral F distribution.\n\nThe foundational setting is a balanced one-way fixed-effects ANOVA model with $k$ groups, per-group sample size $n$, and total sample size $N = k n$. Group $i$ has population mean $\\mu_i$ and all groups share a common error variance $\\sigma^2$. The hypothesis test targets $H_0$ that all group means are equal versus the fixed-effects alternative that not all $\\mu_i$ are equal. The error probability threshold is the significance level $\\alpha \\in (0,1)$, and the desired probability of rejecting $H_0$ under the fixed-effects alternative is the power level $1 - \\beta \\in (0,1)$.\n\nYour program must:\n- Assume a balanced design with equal $n$ across all groups.\n- Compute the minimal integer per-group sample size $n \\geq 2$ that achieves at least the specified power level for the provided group means and common within-group variance. If the specified means imply no difference (i.e., all group means are equal in the sense required by the model), the program must output $-1$ for that test case to indicate that the target power is unattainable at any finite sample size.\n- Use the central F distribution to determine the critical threshold at tail probability $\\alpha$ for degrees of freedom $df_1 = k - 1$ and $df_2 = N - k$, and use the noncentral F inverse survival quantile at the specified power with the appropriate noncentrality parameter under the fixed-effects alternative to assess whether the power requirement is met.\n- Ensure the computed per-group sample size is the smallest integer satisfying the requirement.\n\nThe program must implement a monotone search strategy over $n$ (for example, bracketing followed by binary search), and it must base the decision logic on a comparison between the central F critical threshold and the noncentral F inverse survival quantile at the target power.\n\nThere are no physical units involved in this computation. All outputs must be integers. If the target power is unattainable for the provided parameters (which occurs if the group means are all equal), output $-1$.\n\nTest Suite:\nEvaluate your program on the following four cases. In each case, the input is the list of group means, the common within-group variance, the significance level, and the desired power.\n\n- Case $1$: means $[0.0, 0.5, 1.0]$, variance $\\sigma^2 = 1.0$, significance $\\alpha = 0.05$, desired power $1 - \\beta = 0.8$.\n- Case $2$: means $[0.0, 0.2]$, variance $\\sigma^2 = 1.0$, significance $\\alpha = 0.05$, desired power $1 - \\beta = 0.9$.\n- Case $3$: means $[1.0, 1.0, 1.0, 1.0]$, variance $\\sigma^2 = 1.0$, significance $\\alpha = 0.05$, desired power $1 - \\beta = 0.8$.\n- Case $4$: means $[-1.0, -0.5, 0.0, 0.5, 1.0]$, variance $\\sigma^2 = 0.5$, significance $\\alpha = 0.01$, desired power $1 - \\beta = 0.95$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the integer results for the four cases as a comma-separated list enclosed in square brackets, in the same order as the cases above. For example, the output format must be like $[n_1,n_2,n_3,n_4]$ where each $n_i$ is the computed minimal per-group sample size for Case $i$, or $-1$ if unattainable.",
            "solution": "The user-provided problem has been rigorously validated and is determined to be **valid**. It is a well-posed, scientifically sound problem in the domain of biostatistical power analysis. All necessary parameters are specified, the terminology is precise, and the objective is clear. The problem asks for the implementation of a standard, albeit non-trivial, statistical procedure.\n\nHerein, a complete, reasoned solution is provided.\n\n### Theoretical Foundation of One-Way ANOVA Power Analysis\n\nThe problem is situated in the context of a one-way fixed-effects Analysis of Variance (ANOVA). The statistical model for an observation $Y_{ij}$ from group $i \\in \\{1, \\dots, k\\}$ and subject $j \\in \\{1, \\dots, n\\}$ is:\n$$\nY_{ij} = \\mu_i + \\epsilon_{ij}\n$$\nwhere $\\mu_i$ is the true mean of group $i$, and the error terms $\\epsilon_{ij}$ are assumed to be independent and identically distributed normal random variables, $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$, with a common variance $\\sigma^2$ across all $k$ groups. The design is balanced, meaning each group has the same sample size, $n$. The total sample size is $N = kn$.\n\nThe hypothesis test for ANOVA is:\n-   Null Hypothesis $H_0$: All group means are equal, i.e., $\\mu_1 = \\mu_2 = \\dots = \\mu_k$.\n-   Alternative Hypothesis $H_1$: At least one group mean is different from the others.\n\nThe test statistic is the ratio of the Mean Square Between groups ($MSB$) to the Mean Square Within groups ($MSW$):\n$$\nF = \\frac{MSB}{MSW} = \\frac{SSB / (k-1)}{SSW / (N-k)}\n$$\nThe distributions of this F-statistic under the null and alternative hypotheses are central to power analysis:\n1.  Under $H_0$, the statistic $F$ follows a **central F-distribution** with $df_1 = k-1$ and $df_2 = N-k$ degrees of freedom. We denote this as $F \\sim F(df_1, df_2)$.\n2.  Under $H_1$, the statistic $F$ follows a **noncentral F-distribution** with the same degrees of freedom and a noncentrality parameter (NCP), $\\lambda$. We denote this as $F \\sim F(df_1, df_2, \\lambda)$.\n\nThe noncentrality parameter $\\lambda$ quantifies the degree to which the null hypothesis is false. For a balanced one-way ANOVA, it is defined as:\n$$\n\\lambda = \\frac{n \\sum_{i=1}^{k} (\\mu_i - \\bar{\\mu})^2}{\\sigma^2}\n$$\nwhere $\\bar{\\mu} = \\frac{1}{k} \\sum_{i=1}^{k} \\mu_i$ is the grand mean of the population means.\n\n### Power Calculation and Computational Strategy\n\nStatistical power ($1-\\beta$) is the probability of correctly rejecting a false null hypothesis. We reject $H_0$ if the observed F-statistic, $F_{obs}$, exceeds a critical value, $F_{crit}$. This critical value is determined by the significance level $\\alpha$. Specifically, $F_{crit}$ is the value for which the probability of observing an F-statistic greater than it, under $H_0$, is exactly $\\alpha$. This corresponds to the upper $\\alpha$-quantile of the central F-distribution:\n$$\nF_{crit} = F_{\\text{isf}}(\\alpha; df_1, df_2)\n$$\nwhere $F_{\\text{isf}}$ is the inverse survival function (or quantile point function for the upper tail) of the central F-distribution.\n\nThe power is then the probability of this rejection event occurring when $H_1$ is true:\n$$\n\\text{Power} = P(F > F_{crit} | H_1)\n$$\nSince under $H_1$ the F-statistic follows a noncentral F-distribution, the power is calculated using the survival function (SF) of this distribution:\n$$\n\\text{Power} = \\text{SF}_{ncf}(F_{crit}; df_1, df_2, \\lambda)\n$$\nThe goal is to find the minimum integer per-group sample size $n \\geq 2$ that satisfies:\n$$\n\\text{Power} \\geq 1-\\beta\n$$\nwhere $1-\\beta$ is the desired power level.\n\nThe problem specifies a particular method for this check, which involves comparing the central critical threshold to an inverse survival quantile from the noncentral F-distribution. This is an elegant reformulation of the power condition. The condition $\\text{SF}_{ncf}(F_{crit}; df_1, df_2, \\lambda) \\geq 1-\\beta$ is equivalent to:\n$$\nF_{crit} \\leq F_{ncf, \\text{isf}}(1-\\beta; df_1, df_2, \\lambda)\n$$\nThis equivalence holds because the survival function is a monotonically decreasing function, so applying its inverse (the inverse survival function) to both sides of the inequality reverses the inequality sign. The right-hand side of this inequality is precisely the \"inverse survival quantile from the noncentral F distribution at the specified power,\" as required.\n\n### Search Algorithm for Minimal Sample Size $n$\n\nThe core of the problem is to find the smallest integer $n \\geq 2$ that satisfies the inequality above. Critically, both sides of the inequality depend on $n$:\n-   $df_2(n) = k(n-1)$\n-   $\\lambda(n) = n \\cdot C$, where the constant $C = \\frac{\\sum (\\mu_i - \\bar{\\mu})^2}{\\sigma^2}$ depends only on the given problem parameters.\n\nAn analysis of the inequality's dependence on $n$ reveals its monotonic nature:\n-   The left-hand side, $F_{crit}(n) = F_{\\text{isf}}(\\alpha; k-1, k(n-1))$, is a **decreasing** function of $n$, as increasing the denominator degrees of freedom $df_2$ makes the central F-distribution less spread out.\n-   The right-hand side, $F_{ncf, \\text{isf}}(1-\\beta; k-1, k(n-1), \\lambda(n))$, is an **increasing** function of $n$. This is because both an increase in $df_2$ and a linear increase in the NCP $\\lambda$ shift the noncentral F-distribution to the right, increasing its quantiles.\n\nSince a decreasing function is being compared to an increasing function, the condition will be met for all $n$ above some threshold. This structure makes the problem amenable to an efficient search.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Calculate $k$ from the list of means. Compute the sum of squared deviations of means, $\\sum_{i=1}^{k} (\\mu_i - \\bar{\\mu})^2$. If this sum is zero (or numerically indistinguishable from zero), it implies $\\lambda=0$ for all $n$. In this scenario, the power is always equal to the significance level $\\alpha$. If $\\alpha$ is less than the target power $1-\\beta$, the goal is unattainable. The program must return $-1$ as stipulated.\n2.  **Search for $n$**: We seek the smallest integer $n \\geq 2$ satisfying the power condition.\n    -   A robust approach is to first establish a search range $[n_{low}, n_{high}]$ where the condition is not met at $n_{low}$ and is met at $n_{high}$. This can be done by starting with a low value (e.g., $n_{low}=2$) and exponentially increasing a test value until the power requirement is met, which then becomes $n_{high}$.\n    -   Once this bracket is established, a **binary search** is performed within the range $[n_{low}, n_{high}]$ to efficiently pinpoint the smallest integer $n$ for which the power condition holds. This strategy is efficient and guaranteed to find the minimal integer solution due to the established monotonicity.\n\nThe final Python implementation will use `numpy` for numerical calculations and `scipy.stats.f.isf` and `scipy.stats.ncf.isf` to compute the required statistical quantiles, directly embodying the described logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f, ncf\n\ndef compute_min_n_power_anova(\n    means: list[float], variance: float, alpha: float, power: float\n) -> int:\n    \"\"\"\n    Computes the minimal integer per-group sample size for a one-way ANOVA.\n\n    Args:\n        means: A list of population group means.\n        variance: The common within-group variance (sigma^2).\n        alpha: The significance level (Type I error probability).\n        power: The desired statistical power (1 - beta).\n\n    Returns:\n        The minimal integer sample size n per group (n >= 2), or -1 if\n        the target power is unattainable (i.e., all group means are equal).\n    \"\"\"\n\n    k = len(means)\n    if k < 2:\n        # ANOVA requires at least 2 groups.\n        # This case is not in the test suite but is a logical check.\n        return -1\n\n    # Calculate the sum of squared deviations of group means from the grand mean\n    mu_array = np.array(means)\n    grand_mean = np.mean(mu_array)\n    sum_sq_dev = np.sum((mu_array - grand_mean) ** 2)\n\n    # If all means are equal, the noncentrality parameter is always 0.\n    # Power will be equal to alpha, so target power is unattainable.\n    if np.isclose(sum_sq_dev, 0.0):\n        return -1\n\n    effect_size_term = sum_sq_dev / variance\n\n    def is_power_sufficient(n: int) -> bool:\n        \"\"\"\n        Checks if a given sample size n achieves the target power.\n        The check is based on the problem's specified comparison.\n        \"\"\"\n        if n < 2:\n            return False\n\n        df1 = k - 1\n        df2 = k * (n - 1)\n        \n        # Noncentrality parameter lambda\n        ncp = n * effect_size_term\n\n        # Central F critical threshold for significance level alpha\n        f_crit = f.isf(alpha, df1, df2)\n\n        # Inverse survival quantile from the noncentral F distribution at the target power\n        f_power_quantile = ncf.isf(power, df1, df2, ncp)\n        \n        # The condition Power >= target_power is equivalent to F_crit <= F_power_quantile\n        return f_crit <= f_power_quantile\n\n    # Initial check at the lower bound n=2\n    if is_power_sufficient(2):\n        return 2\n\n    # --- Bracketing phase to find a search range [n_low, n_high] ---\n    n_low = 2\n    n_high = 4\n    # Set a practical limit to prevent potential infinite loops with extreme parameters\n    MAX_N_BRACKET = 1000000  \n    while not is_power_sufficient(n_high):\n        n_low = n_high\n        n_high *= 2\n        if n_high > MAX_N_BRACKET:\n            # Power goal is practically unattainable\n            return -1\n\n    # --- Binary search phase to find the minimal integer n ---\n    min_n = n_high\n    while n_low <= n_high:\n        n_mid = n_low + (n_high - n_low) // 2\n        if n_mid < 2:  # Ensure sample size is at least 2\n            n_low = n_mid + 1\n            continue\n\n        if is_power_sufficient(n_mid):\n            min_n = n_mid      # n_mid is a potential answer, try for smaller n\n            n_high = n_mid - 1\n        else:\n            n_low = n_mid + 1  # n_mid is too small, need larger n\n            \n    return min_n\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # format: (means, variance, alpha, power)\n    test_cases = [\n        ([0.0, 0.5, 1.0], 1.0, 0.05, 0.8),\n        ([0.0, 0.2], 1.0, 0.05, 0.9),\n        ([1.0, 1.0, 1.0, 1.0], 1.0, 0.05, 0.8),\n        ([-1.0, -0.5, 0.0, 0.5, 1.0], 0.5, 0.01, 0.95),\n    ]\n\n    results = []\n    for case in test_cases:\n        means, variance, alpha, power = case\n        result = compute_min_n_power_anova(means, variance, alpha, power)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}