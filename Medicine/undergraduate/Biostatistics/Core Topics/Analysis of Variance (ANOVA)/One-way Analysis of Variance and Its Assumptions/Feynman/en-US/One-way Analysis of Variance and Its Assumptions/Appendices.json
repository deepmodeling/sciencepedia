{
    "hands_on_practices": [
        {
            "introduction": "Choosing the right statistical tool is a fundamental skill in biostatistics. This choice often involves a trade-off between the power of a test and the strictness of its assumptions. This exercise explores the classic decision between the parametric one-way ANOVA and its non-parametric counterpart, the Kruskal-Wallis test, challenging you to identify the specific conditions under which ANOVA provides the most statistical power to detect differences between groups . Understanding this is the first step toward correctly applying ANOVA in practice.",
            "id": "1961647",
            "problem": "A biostatistician is analyzing data from a clinical trial designed to compare the efficacy of three new drug formulations ($A$, $B$, and $C$) against a placebo. The measurement of efficacy is a continuous variable representing a specific biomarker level in the blood. The researcher has four independent groups of patients, one for each formulation and one for the placebo. The goal is to determine if there is a statistically significant difference in the mean biomarker level among the four groups.\n\nThe statistician is considering two different statistical procedures for this comparison: the one-way Analysis of Variance (ANOVA) test and the Kruskal-Wallis test. The statistical power of a hypothesis test is defined as the probability of correctly rejecting the null hypothesis when it is in fact false. The choice of test can significantly impact the power of the analysis.\n\nUnder which of the following circumstances would the one-way ANOVA test be the more powerful choice compared to the Kruskal-Wallis test for this analysis?\n\nA. When the biomarker data for each group contains significant outliers.\n\nB. When the sample sizes for the four groups are substantially different.\n\nC. When the number of groups to be compared is large (e.g., more than five).\n\nD. When the biomarker data within each of the four groups are found to be measured on an ordinal scale.\n\nE. When the populations of biomarker levels from which the patient groups are sampled can be assumed to be normally distributed and to have equal variances.",
            "solution": "Define the null and alternative hypotheses for four independent groups (three drug formulations and placebo). Let $\\mu_{A}, \\mu_{B}, \\mu_{C}, \\mu_{P}$ denote the population means of the biomarker for groups $A, B, C$, and placebo, respectively. The null hypothesis is\n$$\nH_{0}:\\ \\mu_{A}=\\mu_{B}=\\mu_{C}=\\mu_{P},\n$$\nwith the alternative that at least one mean differs.\n\nA one-way ANOVA F-test is a parametric procedure derived under the model that, within each group, observations are independent and identically distributed as normal with a common variance, and the groups are independent. Under the assumptions of normality and homoscedasticity (equal variances), and when the effect of interest is a difference in means, the F-test uses the full metric information in the data and is known to be more powerful than rank-based nonparametric competitors. In particular, under Gaussian errors with equal variances, the parametric F-test is the most powerful invariant test for detecting mean differences across groups.\n\nThe Kruskal-Wallis test is a rank-based, distribution-free procedure that tests for differences in central tendency across groups without assuming normality. It is robust to outliers and non-normality and is appropriate when measurements are ordinal or when the normality/equal-variance assumptions are doubtful. However, because it relies on ranks rather than the raw metric values, it typically sacrifices power relative to ANOVA when the parametric assumptions hold.\n\nEvaluate each option in light of power:\n- Option A (significant outliers): Outliers indicate violations of normality or heavy tails; Kruskal-Wallis is more robust and often more powerful than ANOVA in such cases. Thus ANOVA would not be the more powerful choice.\n- Option B (substantially different sample sizes): Unequal sample sizes alone do not make ANOVA more powerful than Kruskal-Wallis. If variances are equal and normality holds, ANOVA remains valid and typically more powerful; if variances are unequal, ANOVA can lose validity and power. The mere presence of unbalanced group sizes is not a general condition under which ANOVA is more powerful than Kruskal-Wallis.\n- Option C (large number of groups): The relative power between ANOVA and Kruskal-Wallis is driven by distributional assumptions and scale of measurement, not simply by the count of groups. Having more groups does not intrinsically favor ANOVA over Kruskal-Wallis in terms of power.\n- Option D (ordinal scale): ANOVA assumes interval-scale measurements and normally distributed errors; with ordinal data, Kruskal-Wallis is appropriate and typically more powerful because ANOVA’s assumptions are violated.\n- Option E (normal populations with equal variances): This is precisely the setting where the ANOVA F-test is most efficient; it leverages full metric information and is more powerful than Kruskal-Wallis for detecting mean differences.\n\nTherefore, the one-way ANOVA test is the more powerful choice under the assumption of normality with equal variances across groups, which corresponds to option E.",
            "answer": "$$\\boxed{E}$$"
        },
        {
            "introduction": "A significant F-test in an ANOVA tells us that at least one group mean is different, but it doesn't specify which ones or how. To answer more targeted scientific questions, we use linear contrasts. This practice problem guides you through the process of constructing and interpreting a confidence interval for a specific contrast, allowing you to test a focused hypothesis about how different drug doses compare . Mastering contrasts allows you to move beyond the global test and extract precise, meaningful conclusions from your data.",
            "id": "4919608",
            "problem": "A randomized clinical trial evaluates the effect of three doses of an anti-inflammatory drug versus placebo on the reduction in C-reactive protein (CRP) after $8$ weeks. The four arms are placebo, low dose, medium dose, and high dose. The one-way analysis of variance (ANOVA) model assumes independent normally distributed errors with a common variance. The sample sizes are $n_{\\text{placebo}} = 28$, $n_{\\text{low}} = 30$, $n_{\\text{medium}} = 32$, and $n_{\\text{high}} = 26$. The sample means of CRP reduction (in $\\mathrm{mg/L}$) are $\\bar{y}_{\\text{placebo}} = 0.7$, $\\bar{y}_{\\text{low}} = 2.9$, $\\bar{y}_{\\text{medium}} = 3.5$, and $\\bar{y}_{\\text{high}} = 4.8$. The pooled within-group variance estimate from the ANOVA, denoted $\\hat{\\sigma}^2$, is $1.44$.\n\nConsider the contrast that compares the high dose to the average of the low and medium doses, defined at the population level by $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$. Under the one-way ANOVA assumptions and using the pooled variance estimate, construct the two-sided $95\\%$ confidence interval for $\\psi$ based on the residual degrees of freedom. Report the lower bound of this confidence interval in $\\mathrm{mg/L}$, rounded to four significant figures. Briefly interpret what the sign of the entire interval implies for clinical inference about the superiority of the high dose relative to the average of the low and medium doses.",
            "solution": "The problem requires the construction of a $95\\%$ confidence interval for a specific linear contrast of population means in the context of a one-way analysis of variance (ANOVA).\n\nFirst, we validate the problem statement.\nThe problem provides the following givens:\n-   Number of groups, $k=4$.\n-   Group sample sizes: $n_{\\text{placebo}} = 28$, $n_{\\text{low}} = 30$, $n_{\\text{medium}} = 32$, and $n_{\\text{high}} = 26$.\n-   Group sample means: $\\bar{y}_{\\text{placebo}} = 0.7$, $\\bar{y}_{\\text{low}} = 2.9$, $\\bar{y}_{\\text{medium}} = 3.5$, and $\\bar{y}_{\\text{high}} = 4.8$.\n-   The pooled within-group variance estimate (Mean Squared Error, MSE): $\\hat{\\sigma}^2 = 1.44$.\n-   The population contrast of interest: $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$.\n-   The confidence level is $95\\%$, which corresponds to $\\alpha = 0.05$.\n\nThe problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is a standard biostatistical problem. Therefore, the problem is deemed valid.\n\nThe general form of a two-sided $(1-\\alpha) \\times 100\\%$ confidence interval for a linear contrast $\\psi = \\sum_{i=1}^{k} c_i \\mu_i$ is given by:\n$$ \\hat{\\psi} \\pm t_{\\alpha/2, df} \\cdot SE(\\hat{\\psi}) $$\nwhere $\\hat{\\psi}$ is the sample estimate of the contrast, $t_{\\alpha/2, df}$ is the critical value from the t-distribution with $df$ degrees of freedom, and $SE(\\hat{\\psi})$ is the standard error of the estimate.\n\nWe proceed by calculating each component.\n\n1.  **Estimate of the contrast, $\\hat{\\psi}$**:\n    The coefficients for the contrast $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\mu_{\\text{low}} - \\frac{1}{2}\\mu_{\\text{medium}}$ are $c_{\\text{high}}=1$, $c_{\\text{low}}=-1/2$, $c_{\\text{medium}}=-1/2$, and $c_{\\text{placebo}}=0$.\n    The sample estimate $\\hat{\\psi}$ is calculated by substituting the sample means for the population means:\n    $$ \\hat{\\psi} = \\bar{y}_{\\text{high}} - \\frac{1}{2}\\left(\\bar{y}_{\\text{low}} + \\bar{y}_{\\text{medium}}\\right) $$\n    Substituting the given values:\n    $$ \\hat{\\psi} = 4.8 - \\frac{1}{2}(2.9 + 3.5) = 4.8 - \\frac{1}{2}(6.4) = 4.8 - 3.2 = 1.6 $$\n\n2.  **Degrees of Freedom, $df$**:\n    The degrees of freedom for the pooled variance estimate (the residual degrees of freedom) are given by $df = N - k$, where $N$ is the total sample size and $k$ is the number of groups.\n    The total sample size is $N = n_{\\text{placebo}} + n_{\\text{low}} + n_{\\text{medium}} + n_{\\text{high}} = 28 + 30 + 32 + 26 = 116$.\n    The number of groups is $k=4$.\n    Therefore, the degrees of freedom are:\n    $$ df = 116 - 4 = 112 $$\n\n3.  **Standard Error of the estimate, $SE(\\hat{\\psi})$**:\n    The variance of the estimated contrast, $\\widehat{\\text{Var}}(\\hat{\\psi})$, is calculated using the pooled variance estimate $\\hat{\\sigma}^2$ and the sample sizes of the groups involved in the contrast.\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) = \\hat{\\sigma}^2 \\sum_{i=1}^{k} \\frac{c_i^2}{n_i} $$\n    For our specific contrast:\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) = \\hat{\\sigma}^2 \\left( \\frac{c_{\\text{high}}^2}{n_{\\text{high}}} + \\frac{c_{\\text{low}}^2}{n_{\\text{low}}} + \\frac{c_{\\text{medium}}^2}{n_{\\text{medium}}} \\right) = \\hat{\\sigma}^2 \\left( \\frac{1^2}{n_{\\text{high}}} + \\frac{(-1/2)^2}{n_{\\text{low}}} + \\frac{(-1/2)^2}{n_{\\text{medium}}} \\right) $$\n    Substituting the given values:\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) = 1.44 \\left( \\frac{1}{26} + \\frac{1/4}{30} + \\frac{1/4}{32} \\right) = 1.44 \\left( \\frac{1}{26} + \\frac{1}{120} + \\frac{1}{128} \\right) $$\n    Calculating the terms inside the parentheses:\n    $$ \\frac{1}{26} \\approx 0.03846154, \\quad \\frac{1}{120} \\approx 0.00833333, \\quad \\frac{1}{128} = 0.0078125 $$\n    $$ \\sum \\frac{c_i^2}{n_i} \\approx 0.03846154 + 0.00833333 + 0.0078125 = 0.05460737 $$\n    $$ \\widehat{\\text{Var}}(\\hat{\\psi}) \\approx 1.44 \\times 0.05460737 \\approx 0.07863461 $$\n    The standard error is the square root of the variance:\n    $$ SE(\\hat{\\psi}) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\psi})} \\approx \\sqrt{0.07863461} \\approx 0.2804186 $$\n\n4.  **Critical value, $t_{\\alpha/2, df}$**:\n    For a $95\\%$ confidence interval, $\\alpha=0.05$, so we need the critical value $t_{0.025, 112}$. Using statistical software or a detailed t-distribution table for $df=112$, the value is approximately:\n    $$ t_{0.025, 112} \\approx 1.98137 $$\n\n5.  **Confidence Interval Construction**:\n    Now we calculate the margin of error (ME):\n    $$ ME = t_{0.025, 112} \\cdot SE(\\hat{\\psi}) \\approx 1.98137 \\times 0.2804186 \\approx 0.555627 $$\n    The $95\\%$ confidence interval is $\\hat{\\psi} \\pm ME$:\n    $$ CI_{95\\%} = 1.6 \\pm 0.555627 $$\n    Lower bound: $L = 1.6 - 0.555627 = 1.044373$\n    Upper bound: $U = 1.6 + 0.555627 = 2.155627$\n    The confidence interval is approximately $(1.044, 2.156)$.\n\n6.  **Final Answer and Rounding**:\n    The problem asks for the lower bound of the confidence interval, rounded to four significant figures.\n    The calculated lower bound is $1.044373$. The first four significant figures are $1$, $0$, $4$, and $4$. The fifth significant digit is $3$, which is less than $5$, so we round down.\n    The lower bound rounded to four significant figures is $1.044$.\n\n7.  **Interpretation of the Confidence Interval**:\n    The $95\\%$ confidence interval for the contrast $\\psi = \\mu_{\\text{high}} - \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$ is approximately $(1.044, 2.156)$. Since the entire interval is positive (the lower bound $1.044$ is greater than $0$), we can be $95\\%$ confident that the true value of the contrast $\\psi$ is positive. A positive value for $\\psi$ implies that $\\mu_{\\text{high}} > \\frac{1}{2}\\left(\\mu_{\\text{low}} + \\mu_{\\text{medium}}\\right)$.\n    Clinically, this provides statistically significant evidence that the mean reduction in C-reactive protein for the high-dose group is greater than the average of the mean reductions for the low- and medium-dose groups. This suggests superiority of the high dose compared to the average effect of the lower, active doses.",
            "answer": "$$ \\boxed{1.044} $$"
        },
        {
            "introduction": "The theoretical elegance of ANOVA meets the messy reality of clinical research in this final exercise. The validity of our statistical inferences depends entirely on whether the data meets the test's underlying assumptions. This problem presents a realistic scenario where a protocol deviation introduces non-random missing data, a common challenge in applied research . By working through this thought experiment, you will develop a deeper understanding of how real-world events can systematically bias results and invalidate the assumptions of even the most standard statistical procedures.",
            "id": "4821638",
            "problem": "A multicenter randomized controlled trial compares three perioperative hydration protocols for reducing systolic blood pressure at 24 hours post-surgery. Let groups be indexed by $j \\in \\{1,2,3\\}$, and within each group let participants be indexed by $i$. The outcome for participant $i$ in group $j$ is modeled as\n$$\nY_{ij} \\;=\\; \\mu_j \\;+\\; \\frac{\\gamma}{2}\\,\\Big(2\\,\\mathbf{1}\\{T_{ij}=\\text{Afternoon}\\}-1\\Big) \\;+\\; \\varepsilon_{ij},\n$$\nwhere $\\mu_j$ is the protocol-specific mean, $\\gamma>0$ encodes a diurnal effect common to all groups, $T_{ij}\\in\\{\\text{Morning},\\text{Afternoon}\\}$ is the time-of-day of the measurement, $\\varepsilon_{ij} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0,\\sigma^2)$, and the error terms are independent across all participants and groups. The diurnal term implies that, holding protocol fixed, measurements taken in the afternoon are on average $\\gamma/2$ higher and those in the morning are on average $\\gamma/2$ lower, relative to the group mean $\\mu_j$.\n\nPlanned scheduling is as follows: in groups $j=1$ and $j=3$, each participant’s measurement time is independently assigned Morning or Afternoon with equal probability $1/2$; in group $j=2$, the clinic intended to schedule all measurements in the Afternoon.\n\nDue to a protocol deviation at the group $j=2$ clinic, a cold chain failure led to cancellation of all Afternoon measurement slots on the study day, and participants were told they could be measured only in the Morning if they could return; those who could not return in the Morning were not measured. The time-of-day variable $T_{ij}$ was not recorded in the analysis dataset. Define the missingness indicator $R_{ij}$ by $R_{ij}=1$ if $Y_{ij}$ is observed and $R_{ij}=0$ if missing. In practice, $R_{ij}=1$ with probability $1$ for all participants in groups $j=1$ and $j=3$, and $R_{i2}=1$ if and only if $T_{i2}=\\text{Morning}$.\n\nAn investigator plans to fit a one-way Analysis of Variance (ANOVA) with a single factor “protocol group” to the observed outcomes, using listwise deletion (complete cases only). Assume the classical one-way ANOVA model conditions on the observed data: within each group $j$, the residuals are independent and identically distributed as $\\mathcal{N}(0,\\sigma^2)$, and across groups the residual distributions share the same variance $\\sigma^2$.\n\nBased on the definitions of Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR), and the foundational distributional assumptions underlying the one-way ANOVA $F$ test under the null hypothesis $H_0:\\mu_1=\\mu_2=\\mu_3$, which of the following statements is most accurate about this scenario and the validity of the one-way ANOVA using listwise deletion?\n\nA. The missingness mechanism is Missing Completely At Random (MCAR), so listwise deletion yields unbiased group means and the $F$ statistic has its nominal central $F$ distribution under $H_0$.\n\nB. The missingness mechanism is Missing At Random (MAR) because it depends only on observed group membership; therefore, listwise deletion in the one-way ANOVA remains valid.\n\nC. The missingness mechanism is not Missing Completely At Random relative to the analysis dataset because $R_{ij}$ depends on an unrecorded time-of-day variable that is associated with $Y_{ij}$; consequently, complete-case one-way ANOVA on group alone can produce biased group means and an $F$ statistic whose null distribution is not the nominal central $F$, invalidating nominal Type I error control.\n\nD. Regardless of the missingness mechanism, as long as normality and equal variances hold within observed groups, listwise deletion guarantees valid one-way ANOVA inference at the nominal level.",
            "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n-   **Model for outcome:** The outcome for participant $i$ in group $j \\in \\{1,2,3\\}$ is modeled as $Y_{ij} = \\mu_j + \\frac{\\gamma}{2}\\,(2\\,\\mathbf{1}\\{T_{ij}=\\text{Afternoon}\\}-1) + \\varepsilon_{ij}$.\n-   **Parameters and variables:**\n    -   $\\mu_j$: protocol-specific mean for group $j$.\n    -   $\\gamma$: a positive constant ($\\gamma > 0$) encoding a diurnal effect.\n    -   $T_{ij}$: time-of-day of measurement, either 'Morning' or 'Afternoon'.\n    -   $\\varepsilon_{ij}$: independent and identically distributed random errors, $\\varepsilon_{ij} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0,\\sigma^2)$.\n-   **Interpretation of diurnal term:** Measurements in the afternoon are on average $\\gamma/2$ higher and in the morning are $\\gamma/2$ lower than the group mean $\\mu_j$.\n-   **Planned scheduling:**\n    -   Groups $j=1, 3$: $P(T_{ij}=\\text{Morning}) = P(T_{ij}=\\text{Afternoon}) = 1/2$.\n    -   Group $j=2$: All measurements were intended to be in the Afternoon.\n-   **Protocol deviation and missingness:**\n    -   In group $j=2$, afternoon slots were cancelled.\n    -   The missingness indicator is $R_{ij}$, with $R_{ij}=1$ if $Y_{ij}$ is observed and $R_{ij}=0$ if missing.\n    -   For groups $j=1, 3$: $P(R_{ij}=1) = 1$ (no missing data).\n    -   For group $j=2$: $R_{i2}=1$ if and only if $T_{i2}=\\text{Morning}$.\n-   **Analysis dataset:** The time-of-day variable $T_{ij}$ is not recorded. The dataset for analysis contains only group identifiers and observed outcomes.\n-   **Proposed analysis:** A one-way Analysis of Variance (ANOVA) with a single factor \"protocol group\" on the complete cases (listwise deletion).\n-   **Question context:** Evaluation of the scenario in terms of missing data definitions (MCAR, MAR, MNAR) and validity of the one-way ANOVA F-test under the null hypothesis $H_0: \\mu_1=\\mu_2=\\mu_3$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem describes a realistic scenario in a multicenter clinical trial, involving common issues like protocol deviations, missing data, and confounding variables (diurnal effect). The statistical concepts (ANOVA, MCAR/MAR/MNAR, distributional assumptions) are standard in biostatistics and medical research. The setup is scientifically and mathematically sound.\n-   **Well-Posed:** The problem provides a clear model, a specific data collection process (including the deviation), a proposed analysis, and asks a specific question about the validity of that analysis. The information is sufficient to determine the missing data mechanism and evaluate its consequences. A unique and meaningful solution exists.\n-   **Objective:** The language is precise and quantitative. There are no subjective or opinion-based statements.\n-   **Completeness and Consistency:** The problem provides all necessary information. The initial plan for group $2$ (all afternoon) and the subsequent deviation (only morning observed) are not contradictory; they describe a change in protocol which is central to the problem. All terms are well-defined.\n\nThe problem statement is valid. It is a well-constructed question in applied statistics.\n\n### Step 3: Derivation and Option Analysis\n\n**1. Characterization of the Missing Data Mechanism**\n\nThe definitions of missing data mechanisms relate the probability of missingness to the data values themselves. Let $Y$ be the complete data matrix.\n-   **Missing Completely At Random (MCAR):** The probability of missingness does not depend on any values in $Y$, observed or unobserved.\n-   **Missing At Random (MAR):** The probability of missingness depends only on the observed values in $Y$.\n-   **Missing Not At Random (MNAR):** The probability of missingness depends on the unobserved (missing) values, even after accounting for the observed values.\n\nIn this problem, data are missing only in group $j=2$. The rule is that an observation $Y_{i2}$ is missing ($R_{i2}=0$) if and only if the measurement was to occur in the afternoon ($T_{i2}=\\text{Afternoon}$).\n\nThe outcome $Y_{ij}$ is a function of the time-of-day $T_{ij}$. Specifically,\n-   If $T_{ij}=\\text{Morning}$, $Y_{ij} = \\mu_j - \\gamma/2 + \\varepsilon_{ij}$.\n-   If $T_{ij}=\\text{Afternoon}$, $Y_{ij} = \\mu_j + \\gamma/2 + \\varepsilon_{ij}$.\n\nSince $\\gamma>0$, afternoon measurements are systematically higher than morning measurements, holding all else constant. The reason for missingness in group $2$ is that the measurement time was 'Afternoon'. This means that participants whose outcomes *would have been systematically higher* are the ones who are missing from the dataset. The variable $T_{ij}$ that governs missingness is not recorded in the analysis dataset, and it directly influences the value of $Y_{ij}$. The probability of missingness depends on the unobserved value of $Y_{ij}$ (or more directly, on the unobserved covariate $T_{ij}$ which determines part of $Y_{ij}$'s value). This is the definition of MNAR. The mechanism is not MCAR because missingness is related to the outcome. It is not MAR because the variable driving missingness ($T_{ij}$) is unobserved by the analyst.\n\n**2. Consequences for Complete-Case One-Way ANOVA**\n\nThe proposed analysis is a one-way ANOVA on the observed data (listwise deletion). Let us examine the properties of the data that enter this analysis.\n\n-   **Expected Group Means for Observed Data:**\n    -   For groups $j=1$ and $j=3$, there is no missing data. The time-of-day is assigned with probability $1/2$. The expected outcome is:\n        $$E[Y_{ij}] = E[\\mu_j + \\frac{\\gamma}{2}(2\\,\\mathbf{1}\\{T_{ij}=\\text{Afternoon}\\}-1)] = \\mu_j + \\frac{\\gamma}{2}(2 \\cdot \\frac{1}{2} - 1) = \\mu_j$$\n        The sample means $\\bar{Y}_1$ and $\\bar{Y}_3$ are unbiased estimators of $\\mu_1$ and $\\mu_3$.\n    -   For group $j=2$, data are observed *only if* $T_{i2}=\\text{Morning}$. We are calculating the expectation conditional on the data being observed ($R_{i2}=1$).\n        $$E[Y_{i2} | R_{i2}=1] = E[Y_{i2} | T_{i2}=\\text{Morning}]$$\n        $$E[Y_{i2} | T_{i2}=\\text{Morning}] = E[\\mu_2 + \\frac{\\gamma}{2}(2\\,\\mathbf{1}\\{T_{i2}=\\text{Afternoon}\\}-1) + \\varepsilon_{i2} | T_{i2}=\\text{Morning}]$$\n        Since $T_{i2}=\\text{Morning}$, $\\mathbf{1}\\{T_{i2}=\\text{Afternoon}\\} = 0$.\n        $$E[Y_{i2, \\text{observed}}] = E[\\mu_2 - \\gamma/2 + \\varepsilon_{i2}] = \\mu_2 - \\gamma/2$$\n        The sample mean for the observed data in group $2$, $\\bar{Y}_{2, \\text{obs}}$, is a biased estimator of $\\mu_2$. It systematically underestimates $\\mu_2$ by $\\gamma/2$.\n\n-   **Impact on ANOVA F-test under the Null Hypothesis $H_0: \\mu_1=\\mu_2=\\mu_3=\\mu$**\n    Under $H_0$, the true means are equal. However, the expected values of the observed sample means are:\n    $$E[\\bar{Y}_{1, \\text{obs}}] = \\mu$$\n    $$E[\\bar{Y}_{2, \\text{obs}}] = \\mu - \\gamma/2$$\n    $$E[\\bar{Y}_{3, \\text{obs}}] = \\mu$$\n    Because these expected values are not equal (since $\\gamma>0$), the between-group sum of squares (SSB) will be systematically inflated. Even when $H_0$ is true, there will be apparent differences between the group means due to the missing data mechanism. Consequently, the F-statistic, $F = \\text{MSB} / \\text{MSW}$, will be systematically larger than expected under the null. Its null distribution will not be the nominal central $F$-distribution, but rather a non-central $F$-distribution. This invalidates the test, leading to an inflated Type I error rate (i.e., too many false rejections of the null hypothesis).\n\n-   **Evaluation of ANOVA Assumptions on Observed Data**\n    -   **Homoscedasticity (Equal Variances):**\n        -   For groups $j=1,3$, the variance of the observed outcomes is:\n            $\\text{Var}(Y_{ij}) = \\text{Var}(\\frac{\\gamma}{2}(2\\,\\mathbf{1}\\{T_{ij}\\!=\\!\\text{Aft.}\\}\\!-\\!1) + \\varepsilon_{ij}) = \\text{Var}(\\frac{\\gamma}{2}Z_i) + \\text{Var}(\\varepsilon_{ij})$ where $Z_i$ is a random variable taking values $+1$ and $-1$ with probability $1/2$. $\\text{Var}(Z_i)=1$. So, $\\text{Var}(Y_{ij}) = (\\gamma/2)^2 + \\sigma^2$.\n        -   For group $j=2$, the observed outcomes are $Y_{i2, \\text{obs}} = \\mu_2 - \\gamma/2 + \\varepsilon_{i2}$. The variance is:\n            $\\text{Var}(Y_{i2, \\text{obs}}) = \\text{Var}(\\mu_2 - \\gamma/2 + \\varepsilon_{i2}) = \\text{Var}(\\varepsilon_{i2}) = \\sigma^2$.\n        -   Since $\\gamma>0$, $\\sigma^2 + \\gamma^2/4 \\neq \\sigma^2$. The assumption of equal variances across the groups of observed data is violated.\n    -   **Normality:**\n        -   For groups $j=1,3$, the outcome $Y_{ij}$ follows a mixture of two normal distributions: $0.5 \\cdot \\mathcal{N}(\\mu_j - \\gamma/2, \\sigma^2) + 0.5 \\cdot \\mathcal{N}(\\mu_j + \\gamma/2, \\sigma^2)$. A mixture of normals is not, in general, a normal distribution.\n        -   For group $j=2$, the observed outcome $Y_{i2, \\text{obs}}$ follows a normal distribution $\\mathcal{N}(\\mu_2 - \\gamma/2, \\sigma^2)$.\n        -   The normality assumption is violated for the observed data in groups $1$ and $3$.\n\nBoth the biased estimation and the violation of fundamental ANOVA assumptions (homoscedasticity and normality) render the proposed complete-case analysis invalid.\n\n### Option-by-Option Analysis\n\n**A. The missingness mechanism is Missing Completely At Random (MCAR), so listwise deletion yields unbiased group means and the $F$ statistic has its nominal central $F$ distribution under $H_0$.**\nThis statement is incorrect on all counts. The mechanism is MNAR, not MCAR, as missingness is tied to the outcome value. Listwise deletion yields a biased mean for group $2$. The F-statistic will not follow its nominal null distribution due to both the biased mean and violated variance assumptions.\n**Verdict: Incorrect.**\n\n**B. The missingness mechanism is Missing At Random (MAR) because it depends only on observed group membership; therefore, listwise deletion in the one-way ANOVA remains valid.**\nThe claim that the mechanism is MAR is incorrect. Missingness depends on group membership *and* the unobserved time-of-day, which is intrinsically linked to the outcome value, making it MNAR. Furthermore, even if the mechanism were MAR, listwise deletion is not guaranteed to be valid; it often introduces bias unless the analysis model includes the variables that predict missingness.\n**Verdict: Incorrect.**\n\n**C. The missingness mechanism is not Missing Completely At Random relative to the analysis dataset because $R_{ij}$ depends on an unrecorded time-of-day variable that is associated with $Y_{ij}$; consequently, complete-case one-way ANOVA on group alone can produce biased group means and an $F$ statistic whose null distribution is not the nominal central $F$, invalidating nominal Type I error control.**\nThis statement is entirely accurate. It correctly states the mechanism is not MCAR and identifies the root cause: the dependence of missingness ($R_{ij}$) on an unrecorded variable ($T_{ij}$) that is part of the outcome model for $Y_{ij}$. It correctly deduces the consequences: biased group means from the complete-case analysis, and an invalid F-test because its null distribution is no longer the nominal central-F, which invalidates Type I error control.\n**Verdict: Correct.**\n\n**D. Regardless of the missingness mechanism, as long as normality and equal variances hold within observed groups, listwise deletion guarantees valid one-way ANOVA inference at the nominal level.**\nThis statement is incorrect for multiple reasons. First, the premise is false; as derived above, neither normality (in groups 1 & 3) nor equal variances hold across the observed groups. Second, the conclusion is a fundamental misunderstanding of missing data theory. The validity of an analysis depends critically on the missingness mechanism. Listwise deletion is not a universally valid procedure and can introduce substantial bias under MAR and MNAR, invalidating inference even if other assumptions were to hold.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}