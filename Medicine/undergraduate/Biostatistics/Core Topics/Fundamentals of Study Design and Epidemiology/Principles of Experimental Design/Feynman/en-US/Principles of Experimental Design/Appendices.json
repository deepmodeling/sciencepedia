{
    "hands_on_practices": [
        {
            "introduction": "A successful experiment begins long before the first data point is collected. This practice guides you through one of the most critical planning stages: determining the appropriate sample size. By deriving the sample size formula for comparing two means from first principles, you will gain a deep, practical understanding of the fundamental trade-offs between statistical power, the desired significance level $\\alpha$, the variability of the outcome $\\sigma^2$, and the magnitude of the effect $\\delta$ you aim to detect . This exercise is essential for designing studies that are not only statistically valid but also ethically and financially efficient.",
            "id": "4941140",
            "problem": "A biostatistician is planning a randomized two-arm study to compare the mean of a continuous outcome under Treatment A and Treatment B. Outcomes in arm A are independent and identically distributed as $N(\\mu_{A}, \\sigma^{2})$ and in arm B as $N(\\mu_{B}, \\sigma^{2})$, with the same variance $\\sigma^{2}$ in each arm. The allocation is equal, with $n$ subjects per arm. The primary analysis will use a two-sided pooled two-sample $t$-test of the null hypothesis $H_{0}: \\mu_{A} - \\mu_{B} = 0$ at significance level $\\alpha$, against the two-sided alternative $H_{1}: \\mu_{A} - \\mu_{B} \\neq 0$. The design target is to have power $1 - \\beta$ to detect a prespecified difference $\\delta = \\mu_{A} - \\mu_{B}$ under the alternative, where $\\delta \\neq 0$.\n\nStarting from first principles about sampling distributions and the definition of Type I error and power, and using a scientifically standard large-sample planning approximation for the $t$-test based on the normal distribution, derive a closed-form analytic expression for the required per-arm sample size $n$ in terms of $\\sigma^{2}$, $\\alpha$, $\\beta$, and $\\delta$. Explicitly state the assumptions used at each step, including distributional assumptions, independence, equal variances, equal allocation, and any large-sample approximations. Define the standard normal quantile $z_{p}$ such that $\\Pr(Z \\leq z_{p}) = p$ for $Z \\sim N(0,1)$, and express your answer using this notation.\n\nExpress the final answer as a single closed-form analytic expression for $n$ without rounding. No physical units apply to $n$ in this problem. Do not provide any numerical evaluation.",
            "solution": "The user wants to solve the problem: A biostatistician is planning a randomized two-arm study to compare the mean of a continuous outcome under Treatment A and Treatment B. Outcomes in arm A are independent and identically distributed as $N(\\mu_{A}, \\sigma^{2})$ and in arm B as $N(\\mu_{B}, \\sigma^{2})$, with the same variance $\\sigma^{2}$ in each arm. The allocation is equal, with $n$ subjects per arm. The primary analysis will use a two-sided pooled two-sample $t$-test of the null hypothesis $H_{0}: \\mu_{A} - \\mu_{B} = 0$ at significance level $\\alpha$, against the two-sided alternative $H_{1}: \\mu_{A} - \\mu_{B} \\neq 0$. The design target is to have power $1 - \\beta$ to detect a prespecified difference $\\delta = \\mu_{A} - \\mu_{B}$ under the alternative, where $\\delta \\neq 0$.\n\nStarting from first principles about sampling distributions and the definition of Type I error and power, and using a scientifically standard large-sample planning approximation for the $t$-test based on the normal distribution, derive a closed-form analytic expression for the required per-arm sample size $n$ in terms of $\\sigma^{2}$, $\\alpha$, $\\beta$, and $\\delta$. Explicitly state the assumptions used at each step, including distributional assumptions, independence, equal variances, equal allocation, and any large-sample approximations. Define the standard normal quantile $z_{p}$ such that $\\Pr(Z \\leq z_{p}) = p$ for $Z \\sim N(0,1)$, and express your answer using this notation.\n\nExpress the final answer as a single closed-form analytic expression for $n$ without rounding. No physical units apply to $n$ in this problem. Do not provide any numerical evaluation.\n\n### Step 1: Extract Givens\n- Study design: Randomized two-arm study (Treatment A, Treatment B).\n- Outcome data in arm A: Independent and identically distributed (i.i.d.) from a normal distribution $N(\\mu_{A}, \\sigma^{2})$.\n- Outcome data in arm B: i.i.d. from a normal distribution $N(\\mu_{B}, \\sigma^{2})$.\n- Variance: Common variance $\\sigma^{2}$ for both arms.\n- Allocation: Equal sample sizes, $n$ subjects per arm.\n- Analysis method: Two-sided pooled two-sample $t$-test.\n- Null Hypothesis $H_{0}$: $\\mu_{A} - \\mu_{B} = 0$.\n- Alternative Hypothesis $H_{1}$: $\\mu_{A} - \\mu_{B} \\neq 0$.\n- Significance level (Type I error probability): $\\alpha$.\n- Power (probability of correctly rejecting $H_{0}$): $1 - \\beta$.\n- Type II error probability: $\\beta$.\n- Effect size to detect: $\\delta = \\mu_{A} - \\mu_{B}$, with $\\delta \\neq 0$.\n- Required approximation: Large-sample planning approximation based on the normal distribution.\n- Notation: $z_{p}$ is the standard normal quantile where $\\Pr(Z \\leq z_{p}) = p$ for $Z \\sim N(0,1)$.\n- Objective: Derive a closed-form expression for the per-arm sample size $n$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a classic and fundamental exercise in biostatistical experimental design.\n- **Scientifically Grounded**: The premises are based on standard statistical theory, including normal distributions, hypothesis testing, and power analysis. The large-sample approximation of a $t$-test by a $z$-test is a standard, scientifically-accepted technique for sample size planning.\n- **Well-Posed**: The problem provides all necessary parameters ($\\alpha$, $\\beta$, $\\delta$, $\\sigma^2$) and assumptions to uniquely derive the requested formula for $n$.\n- **Objective**: The problem is stated using precise, unambiguous statistical language.\n\nThe problem does not exhibit any of the listed flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, pseudo-profound, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete derivation of the per-arm sample size $n$ will be provided.\n\n### Derivation\nWe begin by formalizing the problem based on the provided assumptions.\n\n**1. Assumptions:**\n- Let $X_{A,i}$ for $i=1, \\dots, n$ be the i.i.d. outcomes from arm A, where $X_{A,i} \\sim N(\\mu_A, \\sigma^2)$.\n- Let $X_{B,i}$ for $i=1, \\dots, n$ be the i.i.d. outcomes from arm B, where $X_{B,i} \\sim N(\\mu_B, \\sigma^2)$.\n- The two samples are independent of each other.\n- The population variance $\\sigma^2$ is assumed to be known for planning purposes, which is the essence of the large-sample approximation for the $t$-test.\n\n**2. Sampling Distribution of the Difference in Means:**\nLet $\\bar{X}_A = \\frac{1}{n} \\sum_{i=1}^{n} X_{A,i}$ and $\\bar{X}_B = \\frac{1}{n} \\sum_{i=1}^{n} X_{B,i}$ be the sample means for arm A and arm B, respectively.\nFrom the properties of the normal distribution, the sampling distributions of the means are:\n$\\bar{X}_A \\sim N\\left(\\mu_A, \\frac{\\sigma^2}{n}\\right)$\n$\\bar{X}_B \\sim N\\left(\\mu_B, \\frac{\\sigma^2}{n}\\right)$\n\nThe difference in sample means is $\\bar{X}_A - \\bar{X}_B$. Since the samples are independent, the distribution of this difference is also normal.\nThe mean of the difference is $E[\\bar{X}_A - \\bar{X}_B] = E[\\bar{X}_A] - E[\\bar{X}_B] = \\mu_A - \\mu_B$.\nThe variance of the difference is $Var(\\bar{X}_A - \\bar{X}_B) = Var(\\bar{X}_A) + Var(\\bar{X}_B) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n}$.\nThus, the sampling distribution of the difference in means is:\n$$(\\bar{X}_A - \\bar{X}_B) \\sim N\\left(\\mu_A - \\mu_B, \\frac{2\\sigma^2}{n}\\right)$$\n\n**3. The Test Statistic and Rejection Region:**\nUnder the large-sample approximation where $\\sigma^2$ is treated as known, the test statistic is:\n$$Z_{\\text{stat}} = \\frac{(\\bar{X}_A - \\bar{X}_B) - (\\mu_A - \\mu_B)_0}{\\sqrt{\\frac{2\\sigma^2}{n}}}$$\nUnder the null hypothesis $H_0: \\mu_A - \\mu_B = 0$, the value $(\\mu_A - \\mu_B)_0$ is $0$. The statistic becomes:\n$$Z_{\\text{stat}} = \\frac{\\bar{X}_A - \\bar{X}_B}{\\sigma\\sqrt{\\frac{2}{n}}}$$\nUnder $H_0$, this statistic follows a standard normal distribution, $Z_{\\text{stat}} \\sim N(0,1)$.\n\nFor a two-sided test at a significance level of $\\alpha$, we reject $H_0$ if the observed value of $|Z_{\\text{stat}}|$ is greater than the critical value $z_{1-\\alpha/2}$. The rejection region is defined by $|Z_{\\text{stat}}| > z_{1-\\alpha/2}$. The quantile $z_{1-\\alpha/2}$ is chosen such that $\\Pr(Z > z_{1-\\alpha/2}) = \\alpha/2$ for $Z \\sim N(0,1)$.\n\n**4. Power Calculation:**\nPower is the probability of rejecting $H_0$ when the alternative hypothesis $H_1$ is true. The power is $1-\\beta$. We are designing the study to detect a specific difference $\\delta = \\mu_A - \\mu_B$.\nUnder $H_1$, the true mean of $(\\bar{X}_A - \\bar{X}_B)$ is $\\delta$. The test statistic $Z_{\\text{stat}}$ is no longer centered at $0$. Its distribution under $H_1$ is:\n$$E[Z_{\\text{stat}}] = E\\left[\\frac{\\bar{X}_A - \\bar{X}_B}{\\sigma\\sqrt{\\frac{2}{n}}}\\right] = \\frac{E[\\bar{X}_A - \\bar{X}_B]}{\\sigma\\sqrt{\\frac{2}{n}}} = \\frac{\\delta}{\\sigma\\sqrt{\\frac{2}{n}}}$$\n$$Var(Z_{\\text{stat}}) = Var\\left(\\frac{\\bar{X}_A - \\bar{X}_B}{\\sigma\\sqrt{\\frac{2}{n}}}\\right) = \\frac{Var(\\bar{X}_A - \\bar{X}_B)}{(\\sigma\\sqrt{\\frac{2}{n}})^2} = \\frac{2\\sigma^2/n}{2\\sigma^2/n} = 1$$\nSo, under $H_1$, $Z_{\\text{stat}} \\sim N\\left(\\frac{\\delta}{\\sigma\\sqrt{2/n}}, 1\\right)$.\n\nPower is the probability of the test statistic falling into the rejection region, given $H_1$ is true:\n$$1 - \\beta = \\Pr(\\text{Reject } H_0 | H_1 \\text{ is true}) = \\Pr\\left(|Z_{\\text{stat}}| > z_{1-\\alpha/2} | \\mu_A - \\mu_B = \\delta\\right)$$\nThis expands to:\n$$1 - \\beta = \\Pr\\left(Z_{\\text{stat}} > z_{1-\\alpha/2} | H_1\\right) + \\Pr\\left(Z_{\\text{stat}} < -z_{1-\\alpha/2} | H_1\\right)$$\nWithout loss of generality, assume $\\delta > 0$. The mean of $Z_{\\text{stat}}$ under $H_1$ is positive, so the distribution is shifted to the right. The probability $\\Pr\\left(Z_{\\text{stat}} < -z_{1-\\alpha/2} | H_1\\right)$ becomes negligible for typical power levels (e.g., $> 0.5$). Thus, the power is dominated by the first term:\n$$1 - \\beta \\approx \\Pr\\left(Z_{\\text{stat}} > z_{1-\\alpha/2} | H_1\\right)$$\nTo evaluate this probability, we standardize $Z_{\\text{stat}}$ by subtracting its mean under $H_1$:\n$$1 - \\beta = \\Pr\\left(Z_{\\text{stat}} - \\frac{\\delta}{\\sigma\\sqrt{2/n}} > z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right)$$\nLet $W = Z_{\\text{stat}} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}$. Under $H_1$, $W \\sim N(0,1)$.\n$$1 - \\beta = \\Pr\\left(W > z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}\\right)$$\nBy the definition of power $1-\\beta$, the quantile for the standard normal distribution $W$ that has a cumulative probability of $\\beta$ to its left is $z_\\beta$. This is equivalent to having a probability of $1-\\beta$ to its right. Therefore, the argument of the probability function must be equal to $z_\\beta$:\n$$z_{\\beta} = z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}$$\nIf we had assumed $\\delta < 0$, the dominant term for power would be $\\Pr(Z_{\\text{stat}} < -z_{1-\\alpha/2})$ leading to $z_{1-\\beta} = -z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}}$. Both cases are handled by using $|\\delta|$ and ensuring the signs are consistent. Re-expressing the first case using $|\\delta|$ for $\\delta>0$: $z_\\beta = z_{1-\\alpha/2} - \\frac{|\\delta|}{\\sigma\\sqrt{2/n}}$.\nUsing the symmetry property of the standard normal quantiles, $z_{\\beta} = -z_{1-\\beta}$, we have:\n$$-z_{1-\\beta} = z_{1-\\alpha/2} - \\frac{|\\delta|}{\\sigma\\sqrt{2/n}}$$\n\n**5. Solving for Sample Size n:**\nWe now solve the equation for the per-arm sample size $n$.\n$$z_{1-\\alpha/2} + z_{1-\\beta} = \\frac{|\\delta|}{\\sigma\\sqrt{2/n}}$$\nRearranging the terms to isolate $n$:\n$$z_{1-\\alpha/2} + z_{1-\\beta} = \\frac{|\\delta|\\sqrt{n}}{\\sigma\\sqrt{2}}$$\n$$\\sqrt{n} = \\frac{\\sigma\\sqrt{2}(z_{1-\\alpha/2} + z_{1-\\beta})}{|\\delta|}$$\nSquaring both sides gives the expression for $n$:\n$$n = \\left(\\frac{\\sigma\\sqrt{2}(z_{1-\\alpha/2} + z_{1-\\beta})}{|\\delta|}\\right)^2$$\n$$n = \\frac{2\\sigma^2(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2}$$\nSince $\\delta$ is squared, the absolute value is no longer necessary. This is the final closed-form expression for the required sample size per arm.",
            "answer": "$$\n\\boxed{\\frac{2\\sigma^{2}(z_{1-\\alpha/2} + z_{1-\\beta})^{2}}{\\delta^{2}}}\n$$"
        },
        {
            "introduction": "Once an experiment is complete, the focus shifts from design to analysis and interpretation. This exercise challenges you to move beyond simple calculations to consider the causal meaning behind your results. By computing various measures of association—the risk difference, risk ratio, and odds ratio—you will learn to articulate which of these estimands most directly represents the average causal effect targeted by a randomized controlled trial . This practice is crucial for connecting the statistical output to the core principles of the potential outcomes framework, reinforcing why randomization is the bedrock of causal inference.",
            "id": "4941285",
            "problem": "A clinical team conducts a randomized controlled trial (RCT) to evaluate a prophylactic intervention intended to reduce the risk of a laboratory-confirmed binary outcome (infection within $30$ days). Participants are randomized to either the intervention arm or the control arm under complete randomization with perfect adherence and no loss to follow-up. Let $Y$ denote the binary outcome ($Y=1$ if infected; $Y=0$ otherwise). The potential outcomes are $Y(1)$ under intervention and $Y(0)$ under control, and the Stable Unit Treatment Value Assumption (SUTVA) holds. The realized sample sizes and event counts are: intervention arm $n_{1}=184$ with $x_{1}=28$ infections, and control arm $n_{0}=176$ with $x_{0}=44$ infections.\n\nUsing foundational definitions of risk, odds, and design-based causal inference:\n- Compute the observed risks $p_{1}$ and $p_{0}$ in the two arms,\n- Compute the risk difference $p_{1}-p_{0}$, the risk ratio $p_{1}/p_{0}$, and the odds ratio $\\frac{p_{1}/(1-p_{1})}{p_{0}/(1-p_{0})}$,\n- Starting from the potential outcomes framework and the principle of randomized allocation, explain which of these estimands most directly aligns with the randomized design’s target causal effect and why, making explicit the causal quantity the design identifies.\n\nReport the risk difference $p_{1}-p_{0}$ as your final numeric answer, rounded to four significant figures and expressed as a decimal (do not use a percentage sign). No other quantities should be reported in the final answer box.",
            "solution": "The problem is valid as it presents a complete, consistent, and scientifically grounded scenario in biostatistics and causal inference. All data and assumptions required for the solution are explicitly provided.\n\nThe task is to compute several measures of effect from the provided data of a randomized controlled trial (RCT) and to explain which measure most directly represents the causal quantity targeted by the design.\n\nFirst, we compute the observed risks (proportions of infections) in the intervention and control arms. Let $p_1$ be the risk in the intervention arm and $p_0$ be the risk in the control arm. The givens are:\n- Intervention arm: sample size $n_1 = 184$, number of infections $x_1 = 28$.\n- Control arm: sample size $n_0 = 176$, number of infections $x_0 = 44$.\n\nThe observed risks are calculated as the number of events divided by the number of participants in each arm:\n$$p_1 = \\frac{x_1}{n_1} = \\frac{28}{184}$$\n$$p_0 = \\frac{x_0}{n_0} = \\frac{44}{176} = \\frac{1}{4} = 0.25$$\n\nNow, we compute the three requested estimands: the risk difference ($RD$), the risk ratio ($RR$), and the odds ratio ($OR$).\n\n1.  **Risk Difference ($RD$)**:\n    $$RD = p_1 - p_0 = \\frac{28}{184} - \\frac{44}{176} = \\frac{28}{184} - \\frac{1}{4} = \\frac{7 \\times 4 - 46 \\times 1}{46 \\times 4} = \\frac{28-46}{184} = \\frac{-18}{184} = -\\frac{9}{92}$$\n    As a decimal, $RD \\approx -0.097826...$\n\n2.  **Risk Ratio ($RR$)**:\n    $$RR = \\frac{p_1}{p_0} = \\frac{28/184}{1/4} = \\frac{28 \\times 4}{184} = \\frac{112}{184} = \\frac{28}{46} = \\frac{14}{23}$$\n    As a decimal, $RR \\approx 0.608695...$\n\n3.  **Odds Ratio ($OR$)**:\n    The odds of infection in the intervention arm are $o_1 = \\frac{p_1}{1-p_1}$.\n    $$1-p_1 = 1 - \\frac{28}{184} = \\frac{184-28}{184} = \\frac{156}{184}$$\n    $$o_1 = \\frac{28/184}{156/184} = \\frac{28}{156} = \\frac{7}{39}$$\n    The odds of infection in the control arm are $o_0 = \\frac{p_0}{1-p_0}$.\n    $$1-p_0 = 1 - \\frac{1}{4} = \\frac{3}{4}$$\n    $$o_0 = \\frac{1/4}{3/4} = \\frac{1}{3}$$\n    The odds ratio is the ratio of these odds:\n    $$OR = \\frac{o_1}{o_0} = \\frac{7/39}{1/3} = \\frac{7 \\times 3}{39} = \\frac{21}{39} = \\frac{7}{13}$$\n    As a decimal, $OR \\approx 0.538461...$\n\nThe second part of the task is to explain which of these estimands most directly aligns with the causal effect targeted by a randomized design, within the potential outcomes framework.\n\nThe primary causal estimand of interest in many contexts is the Average Causal Effect (ACE), also known as the Average Treatment Effect (ATE). For an individual $i$, the causal effect of the intervention is defined as the difference between their potential outcomes: $Y_i(1) - Y_i(0)$, where $Y_i(1)$ is the outcome if they receive the intervention and $Y_i(0)$ is the outcome if they are in the control group. The fundamental problem of causal inference is that we can only observe one of these potential outcomes for any given individual.\n\nThe ATE is the expectation of this individual-level causal effect over the entire population:\n$$ATE = E[Y(1) - Y(0)]$$\nBy the linearity of expectation, the ATE can be written as:\n$$ATE = E[Y(1)] - E[Y(0)]$$\nSince the outcome $Y$ is a binary variable ($0$ or $1$), its expectation is equal to the probability that it takes the value $1$. Therefore:\n$$E[Y(1)] = P(Y(1)=1)$$\n$$E[Y(0)] = P(Y(0)=1)$$\nSo, the ATE is the difference in the probabilities of the outcome under the two conditions, had the entire population been exposed to each:\n$$ATE = P(Y(1)=1) - P(Y(0)=1)$$\nThis is the causal risk difference.\n\nThe key feature of a randomized controlled trial is that the treatment assignment, let's call it $Z$ (where $Z=1$ for intervention, $Z=0$ for control), is independent of the potential outcomes $(Y(1), Y(0))$. This is the property of exchangeability, formally written as $Z \\perp \\{Y(1), Y(0)\\}$. This property, guaranteed by randomization, allows us to identify the ATE from observed data.\n\nThe observed risk in the intervention arm, $p_1$, is an estimate of the conditional probability $P(Y=1 | Z=1)$. Under the potential outcomes framework and the Stable Unit Treatment Value Assumption (SUTVA), the observed outcome is $Y = Y(1)$ for those with $Z=1$. Thus:\n$$P(Y=1 | Z=1) = P(Y(1)=1 | Z=1)$$\nBecause of randomization ($Z \\perp Y(1)$), a person's potential outcome under treatment is independent of whether they were actually assigned to the treatment group. Therefore:\n$$P(Y(1)=1 | Z=1) = P(Y(1)=1)$$\nThis means the observed risk in the intervention group, $p_1$, is a consistent and unbiased estimator of the population-level risk if everyone were treated, $P(Y(1)=1)$.\n\nSimilarly, for the control arm:\n$$P(Y=1 | Z=0) = P(Y(0)=1 | Z=0) = P(Y(0)=1)$$\nThe observed risk in the control group, $p_0$, is a consistent and unbiased estimator of the population-level risk if no one were treated, $P(Y(0)=1)$.\n\nGiven these two results, the observed risk difference, $p_1 - p_0$, is a consistent and unbiased estimator of the Average Treatment Effect:\n$$E[p_1 - p_0] = E[p_1] - E[p_0] = P(Y(1)=1) - P(Y(0)=1) = ATE$$\nThe risk difference directly corresponds to the ATE, which is defined on an additive scale and represents the average of the individual-level causal effects, $E[Y_i(1) - Y_i(0)]$.\n\nThe risk ratio and odds ratio are also valid causal estimands, representing the causal effect on multiplicative scales. The causal risk ratio is $\\frac{P(Y(1)=1)}{P(Y(0)=1)}$ and the causal odds ratio is $\\frac{P(Y(1)=1)/P(Y(1)=0)}{P(Y(0)=1)/P(Y(0)=0)}$. While randomization allows for their unbiased estimation (on the log scale, for instance), they are non-linear transformations of the probabilities. They are not the average of individual-level effects. For instance, the risk ratio is the ratio of averages, $\\frac{E[Y(1)]}{E[Y(0)]}$, not the average of ratios, $E[\\frac{Y(1)}{Y(0)}]$. For this reason, the risk difference is considered the most direct and fundamental measure of the average causal effect targeted by a randomized design, as it preserves the additive structure from the individual level to the population average. It quantifies the absolute change in average risk attributable to the intervention.\n\nThe final answer requested is the numerical value of the risk difference, $p_1 - p_0$, rounded to four significant figures.\n$$p_1 - p_0 = -\\frac{9}{92} \\approx -0.09782608...$$\nRounding to four significant figures yields $-0.09783$.",
            "answer": "$$ \\boxed{-0.09783} $$"
        },
        {
            "introduction": "While randomization ensures that treatment groups are comparable on average, we can often improve the precision of our effect estimates by accounting for baseline differences between subjects. This hands-on problem demonstrates the powerful technique of Analysis of Covariance (ANCOVA), a prespecified adjustment method widely used in clinical trials. By calculating a covariate-adjusted treatment effect from summary statistics, you will see how incorporating baseline information can lead to a more powerful and precise analysis, and you will clarify the surprisingly minimal assumptions required for this method to yield an unbiased estimate of the causal effect in a randomized setting .",
            "id": "4941235",
            "problem": "A randomized controlled trial evaluates a new antihypertensive drug versus placebo on end-of-study systolic blood pressure measured in millimeters of mercury (mmHg). The trial enrolls $n=200$ participants with simple $1{:}1$ randomization to treatment indicator $T \\in \\{0,1\\}$, where $T=1$ denotes active treatment and $T=0$ denotes placebo. Before observing outcomes, the analysis plan prespecifies covariate adjustment by linear analysis of covariance (ANCOVA) with two baseline covariates: baseline systolic blood pressure $B$ (mmHg) and age $A$ (years). The prespecified analysis model is the linear regression\n$$\nY \\;=\\; \\beta_{0} \\;+\\; \\beta_{T}\\,T \\;+\\; \\beta_{B}\\,B \\;+\\; \\beta_{A}\\,A \\;+\\; \\varepsilon,\n$$\nwhere $Y$ is end-of-study systolic blood pressure (mmHg) and $\\varepsilon$ is an error term. The prespecified plan further states that ordinary least squares will be used, an intercept will be included, and for computational transparency the analysis will be presented on mean-centered variables, that is, the regression of $Y - \\bar{Y}$ on $T-\\bar{T}$, $B-\\bar{B}$, and $A-\\bar{A}$ without an explicit intercept.\n\nYou are given the following empirically observed, mean-centered sample summaries across all $n=200$ participants:\n- $\\operatorname{Var}(T) = 0.25$,\n- $\\operatorname{Cov}(T,Y) = -1.5$,\n- $\\operatorname{Var}(B) = 225$, $\\operatorname{Var}(A) = 100$, $\\operatorname{Cov}(B,A) = 60$,\n- $\\operatorname{Cov}(B,Y) = 192$, $\\operatorname{Cov}(A,Y) = 32$,\n- $\\operatorname{Cov}(T,B) = 0.9$, $\\operatorname{Cov}(T,A) = -0.5$.\n\nAll variances and covariances are the usual mean-centered, finite-sample (dividing by $n$) covariances. Assume the Stable Unit Treatment Value Assumption (SUTVA) and that randomization is implemented without interference or noncompliance.\n\nUsing only these summaries and fundamental definitions of ordinary least squares, compute the prespecified covariate-adjusted treatment effect estimator $\\hat{\\beta}_{T}$ under the linear model above. State clearly the assumptions under which $\\hat{\\beta}_{T}$ is unbiased for the average causal effect in expectation over the randomization.\n\nRound your final numerical answer for $\\hat{\\beta}_{T}$ to four significant figures and express it in millimeters of mercury (mmHg).",
            "solution": "The problem requires the computation of the ordinary least squares (OLS) estimator for the treatment effect, $\\hat{\\beta}_{T}$, in a prespecified analysis of covariance (ANCOVA) model, and a statement of the assumptions under which this estimator is unbiased for the average causal effect.\n\nFirst, the validity of the problem is established. All necessary data, in the form of sample variances and covariances, are provided. The values are internally consistent; for instance, the stated variance of the treatment indicator, $\\operatorname{Var}(T) = 0.25$, corresponds exactly to a $1{:}1$ randomization in a sample of any size ($n_1=n_0$, so $\\bar{T}=0.5$, and $\\operatorname{Var}(T)=p(1-p)=0.5 \\times 0.5 = 0.25$). The problem is scientifically grounded in the statistical theory of randomized controlled trials and linear models, and is well-posed. Thus, we may proceed with the solution.\n\nThe prespecified analysis model is the linear regression:\n$$\nY \\;=\\; \\beta_{0} \\;+\\; \\beta_{T}\\,T \\;+\\; \\beta_{B}\\,B \\;+\\; \\beta_{A}\\,A \\;+\\; \\varepsilon\n$$\nThe problem states that the analysis will be performed on mean-centered variables. Let $y_c = Y - \\bar{Y}$, $t_c = T - \\bar{T}$, $b_c = B - \\bar{B}$, and $a_c = A - \\bar{A}$. The regression model on centered variables omits the intercept:\n$$\ny_c \\;=\\; \\beta_{T}\\,t_c \\;+\\; \\beta_{B}\\,b_c \\;+\\; \\beta_{A}\\,a_c \\;+\\; \\varepsilon\n$$\nThe OLS estimator for the coefficient vector $\\boldsymbol{\\beta} = [\\beta_T, \\beta_B, \\beta_A]^T$ is given by the solution to the normal equations. In matrix form, using sample covariance matrices, this is expressed as:\n$$\n\\hat{\\boldsymbol{\\beta}} \\;=\\; S_{XX}^{-1} S_{Xy}\n$$\nwhere $S_{XX}$ is the sample covariance matrix of the predictors $(T, B, A)$ and $S_{Xy}$ is the vector of sample covariances between the predictors and the outcome $Y$. The problem specifies that all variances and covariances are computed with a divisor of $n$.\n\nFrom the given summary statistics, we construct the matrices. The covariance matrix of the predictors is:\n$$\nS_{XX} = \\begin{pmatrix}\n\\operatorname{Var}(T) & \\operatorname{Cov}(T,B) & \\operatorname{Cov}(T,A) \\\\\n\\operatorname{Cov}(B,T) & \\operatorname{Var}(B) & \\operatorname{Cov}(B,A) \\\\\n\\operatorname{Cov}(A,T) & \\operatorname{Cov}(A,B) & \\operatorname{Var}(A)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.25 & 0.9 & -0.5 \\\\\n0.9 & 225 & 60 \\\\\n-0.5 & 60 & 100\n\\end{pmatrix}\n$$\nThe vector of covariances between predictors and the outcome is:\n$$\nS_{Xy} = \\begin{pmatrix}\n\\operatorname{Cov}(T,Y) \\\\\n\\operatorname{Cov}(B,Y) \\\\\n\\operatorname{Cov}(A,Y)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1.5 \\\\\n192 \\\\\n32\n\\end{pmatrix}\n$$\nWe need to solve the system $S_{XX} \\hat{\\boldsymbol{\\beta}} = S_{Xy}$ for $\\hat{\\boldsymbol{\\beta}}$. We can do this by finding the inverse of $S_{XX}$. The inverse is $S_{XX}^{-1} = \\frac{1}{\\det(S_{XX})} \\operatorname{adj}(S_{XX})$.\n\nFirst, we calculate the determinant of $S_{XX}$:\n$$\n\\begin{align*}\n\\det(S_{XX}) &= 0.25(225 \\cdot 100 - 60 \\cdot 60) - 0.9(0.9 \\cdot 100 - 60(-0.5)) + (-0.5)(0.9 \\cdot 60 - 225(-0.5)) \\\\\n&= 0.25(22500 - 3600) - 0.9(90 + 30) - 0.5(54 + 112.5) \\\\\n&= 0.25(18900) - 0.9(120) - 0.5(166.5) \\\\\n&= 4725 - 108 - 83.25 \\\\\n&= 4533.75\n\\end{align*}\n$$\nNow, we find the entries of the adjugate matrix, $\\operatorname{adj}(S_{XX})$, which is the transpose of the cofactor matrix. We only need the first row of $S_{XX}^{-1}$ to compute $\\hat{\\beta}_T$. The first row of the inverse matrix is composed of the cofactors of the first column of $S_{XX}$, divided by the determinant.\nThe cofactors are:\n$$\nC_{11} = (225 \\cdot 100 - 60 \\cdot 60) = 18900 \\\\\nC_{21} = -(0.9 \\cdot 100 - (-0.5) \\cdot 60) = -(90 + 30) = -120 \\\\\nC_{31} = (0.9 \\cdot 60 - (-0.5) \\cdot 225) = 54 + 112.5 = 166.5\n$$\nThe vector of OLS estimates is $\\hat{\\boldsymbol{\\beta}} = S_{XX}^{-1} S_{Xy}$. The first component, $\\hat{\\beta}_T$, is:\n$$\n\\hat{\\beta}_{T} = \\frac{1}{\\det(S_{XX})} (C_{11} \\cdot \\operatorname{Cov}(T,Y) + C_{21} \\cdot \\operatorname{Cov}(B,Y) + C_{31} \\cdot \\operatorname{Cov}(A,Y))\n$$\nSubstituting the values:\n$$\n\\begin{align*}\n\\hat{\\beta}_{T} &= \\frac{1}{4533.75} [18900(-1.5) + (-120)(192) + 166.5(32)] \\\\\n&= \\frac{1}{4533.75} [-28350 - 23040 + 5328] \\\\\n&= \\frac{1}{4533.75} [-51390 + 5328] \\\\\n&= \\frac{-46062}{4533.75} \\\\\n&\\approx -10.159845...\n\\end{align*}\n$$\nRounding to four significant figures, we get $\\hat{\\beta}_{T} = -10.16$.\n\nThe second part of the question asks for the assumptions under which $\\hat{\\beta}_{T}$ is unbiased for the average causal effect (ACE), $E[Y(1) - Y(0)]$, in expectation over the randomization. The context of a randomized controlled trial is crucial. In this setting, the expectation is taken over the random assignment of the treatment vector $T$, holding the potential outcomes $\\{Y_i(0), Y_i(1)\\}$ and covariates $\\{B_i, A_i\\}$ for all $n=200$ participants fixed.\n\nA fundamental result in the theory of randomized experiments (e.g., Freedman, 2008; Lin, 2013) is that for any pre-specified set of covariates, the OLS estimator of the treatment coefficient in an ANCOVA model is an unbiased estimator of the sample average causal effect, $\\frac{1}{n}\\sum_{i=1}^n (Y_i(1) - Y_i(0))$, in expectation over the randomization. This result holds regardless of whether the linear model is correctly specified. That is, it does not require linearity of the associations between the outcome and covariates, nor does it require a constant treatment effect (i.e., no treatment-by-covariate interactions).\n\nTherefore, the only assumptions required for $E_{\\text{rand}}[\\hat{\\beta}_T] = \\text{ACE}_{\\text{sample}}$ are the fundamental assumptions that define a valid randomized experiment, which were explicitly provided or implied in the problem statement:\n1.  **Randomization**: The treatment $T$ was assigned randomly, making it statistically independent of the potential outcomes and baseline covariates.\n2.  **Stable Unit Treatment Value Assumption (SUTVA)**: This assumption comprises two parts:\n    *   No interference: The potential outcomes of any participant are unaffected by the treatment assignment of any other participant.\n    *   Consistency: The observed outcome for a participant corresponds to their potential outcome under the treatment they actually received (i.e., there are no hidden variations of the treatment or placebo).\n3.  **No noncompliance**: All participants adhered to their assigned treatment.\n\nGiven that these assumptions hold (as stated in the problem), the ANCOVA estimator $\\hat{\\beta}_T$ is unbiased for the sample ACE. If the trial participants are also considered a random sample from a larger population, then $\\hat{\\beta}_T$ is also unbiased for the population ACE.",
            "answer": "$$\\boxed{-10.16}$$"
        }
    ]
}