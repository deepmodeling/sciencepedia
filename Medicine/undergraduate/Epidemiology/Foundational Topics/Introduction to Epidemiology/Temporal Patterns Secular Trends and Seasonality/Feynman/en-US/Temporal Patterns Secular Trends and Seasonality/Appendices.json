{
    "hands_on_practices": [
        {
            "introduction": "When we examine disease rates over many years or decades, we are often looking for a secular trend—a long-term, underlying change in incidence. However, a major challenge is that the age structure of the population itself may change over time, and since disease risk often varies with age, this can create a misleading trend. This exercise guides you through the fundamental epidemiological technique of direct age standardization, allowing you to isolate the true secular trend by removing the confounding effect of demographic shifts .",
            "id": "4642214",
            "problem": "A population-based disease surveillance program seeks to quantify secular trends in incidence over long time spans while removing confounding due to changes in age structure. The program will use direct age standardization across two decades for multiple data sets, each with a fixed standard population. Starting from fundamental definitions in epidemiology and basic properties of weighted averages, derive an algorithm that, for each data set, calculates the age-standardized incidence rate for each of two decades and the secular change between them.\n\nUse the following foundational base without introducing any shortcut formulas in the problem statement. For an age group index $i \\in \\{1,\\dots,k\\}$ and time index $t \\in \\{t_1,t_2\\}$:\n- The age-specific incidence rate is defined by $r_{i,t} = c_{i,t}/N_{i,t}$, where $c_{i,t}$ is the number of incident cases and $N_{i,t}$ is the person-time or population-at-risk for the same age group and decade.\n- A fixed standard population provides weights through normalization: use a nonnegative vector of counts $S = (S_1,\\dots,S_k)$ to construct normalized weights $w_i$ that sum to $1$.\n- Express final age-standardized rates in per-$10^5$ person-years. Let $m = 10^5$ be the multiplier.\n\nYour task is to:\n1. Derive, from these definitions, how to compute the direct age-standardized incidence rate for each of two decades, using the same fixed standard population for both decades to isolate secular trends.\n2. Define the secular change as an absolute difference between the age-standardized incidence rates of the two decades.\n3. Implement a program that computes, for each test case, the triple consisting of the two age-standardized rates and their absolute difference, all expressed in per-$10^5$ person-years and rounded to $2$ decimal places.\n\nScientific realism: The data sets below are internally consistent and plausible for an infectious or chronic disease surveillance program. Seasonality is not considered because the aggregation spans decades; the focus is on secular trends. All age groups within each test case share the same fixed standard population used for both decades in that case.\n\nUnits: All rates must be reported in per-$10^5$ person-years.\n\nAngle units: Not applicable.\n\nPercentages: Not applicable.\n\nTest suite and parameter specification (each vector is ordered as $(G_1,G_2,G_3,G_4,G_5)$ for five age groups):\n- Test case $1$ (happy path; moderate aging and moderate incidence change):\n  - Standard population $S^{(1)} = (300000, 320000, 200000, 140000, 40000)$.\n  - Decade $t_1$: cases $C^{(1)}_{t_1} = (180, 450, 720, 980, 500)$, populations $N^{(1)}_{t_1} = (200000, 180000, 120000, 70000, 20000)$.\n  - Decade $t_2$: cases $C^{(1)}_{t_2} = (160, 400, 800, 1100, 600)$, populations $N^{(1)}_{t_2} = (180000, 160000, 130000, 90000, 30000)$.\n- Test case $2$ (edge case; zero cases in a young age group, substantial changes in older groups):\n  - Standard population $S^{(2)} = (400000, 300000, 180000, 100000, 20000)$.\n  - Decade $t_1$: cases $C^{(2)}_{t_1} = (0, 200, 500, 600, 150)$, populations $N^{(2)}_{t_1} = (250000, 200000, 100000, 40000, 5000)$.\n  - Decade $t_2$: cases $C^{(2)}_{t_2} = (0, 210, 600, 700, 350)$, populations $N^{(2)}_{t_2} = (240000, 220000, 120000, 60000, 10000)$.\n- Test case $3$ (boundary; large age-structure shift across decades but constant age-specific rates):\n  - Standard population $S^{(3)} = (500000, 250000, 150000, 80000, 20000)$.\n  - Decade $t_1$: cases $C^{(3)}_{t_1} = (50, 100, 400, 1000, 2000)$, populations $N^{(3)}_{t_1} = (100000, 100000, 100000, 100000, 100000)$.\n  - Decade $t_2$: cases $C^{(3)}_{t_2} = (200, 200, 480, 600, 400)$, populations $N^{(3)}_{t_2} = (400000, 200000, 120000, 60000, 20000)$.\n- Test case $4$ (small populations; check numerical stability and rounding with rare events):\n  - Standard population $S^{(4)} = (200000, 300000, 250000, 200000, 50000)$.\n  - Decade $t_1$: cases $C^{(4)}_{t_1} = (2, 9, 15, 30, 12)$, populations $N^{(4)}_{t_1} = (20000, 30000, 25000, 20000, 5000)$.\n  - Decade $t_2$: cases $C^{(4)}_{t_2} = (3, 12, 20, 45, 25)$, populations $N^{(4)}_{t_2} = (20000, 30000, 25000, 20000, 5000)$.\n\nProgram output format:\n- For each test case, compute three values in order: the age-standardized incidence rate for decade $t_1$, the age-standardized incidence rate for decade $t_2$, and the absolute secular change $|R_{t_2} - R_{t_1}|$, all in per-$10^5$ person-years and rounded to $2$ decimal places.\n- Aggregate the results of all test cases into a single flat list in the order of the test cases: $[R^{(1)}_{t_1}, R^{(1)}_{t_2}, |R^{(1)}_{t_2} - R^{(1)}_{t_1}|, R^{(2)}_{t_1}, R^{(2)}_{t_2}, |R^{(2)}_{t_2} - R^{(2)}_{t_1}|, R^{(3)}_{t_1}, R^{(3)}_{t_2}, |R^{(3)}_{t_2} - R^{(3)}_{t_1}|, R^{(4)}_{t_1}, R^{(4)}_{t_2}, |R^{(4)}_{t_2} - R^{(4)}_{t_1}|]$.\n- Your program should produce a single line of output containing this list as a comma-separated sequence enclosed in square brackets, for example $[x_1,x_2,\\dots,x_{12}]$.",
            "solution": "The problem requires the derivation and implementation of an algorithm to calculate direct age-standardized incidence rates for two distinct time periods (decades) and the secular change between them. This method is fundamental in epidemiology for comparing rates across time or populations while controlling for the confounding effect of differing age structures. The derivation will proceed from the foundational definitions provided.\n\nLet the index for age groups be $i \\in \\{1, 2, \\dots, k\\}$, and the index for the two decades be $t \\in \\{t_1, t_2\\}$. The given quantities for each test case are:\n- $C_{t} = (c_{1,t}, c_{2,t}, \\dots, c_{k,t})$: A vector of incident case counts for a given decade $t$.\n- $N_{t} = (N_{1,t}, N_{2,t}, \\dots, N_{k,t})$: A vector of person-time or population-at-risk counts for the corresponding age groups and decade.\n- $S = (S_1, S_2, \\dots, S_k)$: A vector of population counts for a fixed standard population.\n- $m = 10^5$: A multiplier to express rates per $10^5$ person-years.\n\n**1. Derivation of Age-Specific Incidence Rates**\n\nThe foundational measure of disease frequency in a specific subpopulation is the age-specific incidence rate. As defined in the problem, for age group $i$ and decade $t$, this rate, denoted $r_{i,t}$, is the ratio of new cases to the total person-time at risk.\n$$r_{i,t} = \\frac{c_{i,t}}{N_{i,t}}$$\nThis calculation is performed for each age group $i$ and for each of the two decades, $t_1$ and $t_2$.\n\n**2. Derivation of Standardized Weights**\n\nThe core of direct standardization is the application of a common age structure to the populations being compared. This common structure is provided by the standard population $S$. We must derive a set of normalized weights, $w_i$, that represent the proportion of the standard population in each age group $i$.\n\nThe total size of the standard population is the sum of its age-specific counts:\n$$S_{total} = \\sum_{j=1}^{k} S_j$$\nThe weight for age group $i$, $w_i$, is the proportion of this total population that belongs to group $i$:\n$$w_i = \\frac{S_i}{S_{total}} = \\frac{S_i}{\\sum_{j=1}^{k} S_j}$$\nBy this construction, the weights are non-negative (since $S_i \\ge 0$) and sum to unity, a necessary property for a weighted average:\n$$\\sum_{i=1}^{k} w_i = \\sum_{i=1}^{k} \\frac{S_i}{\\sum_{j=1}^{k} S_j} = \\frac{\\sum_{i=1}^{k} S_i}{\\sum_{j=1}^{k} S_j} = 1$$\n\n**3. Derivation of the Age-Standardized Incidence Rate (ASR)**\n\nThe direct age-standardized incidence rate for a given decade $t$, which we denote $R_t$, is a weighted average of the age-specific incidence rates $r_{i,t}$. The weights used are the standard population weights $w_i$ derived above. This procedure yields the overall rate that would be observed in the study population if it had the same age structure as the standard population.\n\nThe raw standardized rate, $R_{t, \\text{raw}}$, is computed as:\n$$R_{t, \\text{raw}} = \\sum_{i=1}^{k} w_i \\cdot r_{i,t}$$\nSubstituting the previously derived expressions for $w_i$ and $r_{i,t}$:\n$$R_{t, \\text{raw}} = \\sum_{i=1}^{k} \\left( \\frac{S_i}{\\sum_{j=1}^{k} S_j} \\right) \\cdot \\left( \\frac{c_{i,t}}{N_{i,t}} \\right)$$\nThe problem requires the final rates to be expressed per $m = 10^5$ person-years. Therefore, we scale the raw rate by this multiplier:\n$$R_t = m \\cdot R_{t, \\text{raw}} = 10^5 \\cdot \\sum_{i=1}^{k} \\left( \\frac{S_i}{\\sum_{j=1}^{k} S_j} \\right) \\left( \\frac{c_{i,t}}{N_{i,t}} \\right)$$\nThis calculation is performed for both decade $t_1$ and decade $t_2$, using the respective case and population counts ($c_{i,t_1}, N_{i,t_1}$ and $c_{i,t_2}, N_{i,t_2}$), but crucially, using the *same* standard population weights $w_i$ for both. This ensures that any observed difference between $R_{t_1}$ and $R_{t_2}$ is not due to changes in the age composition of the study populations over the two decades.\n\n**4. Definition of Secular Change**\n\nSecular change quantifies the trend in incidence over the long term. As specified, we define it as the absolute difference between the age-standardized rates of the two decades:\n$$\\Delta R = |R_{t_2} - R_{t_1}|$$\nThis value represents the magnitude of the change in the age-adjusted incidence rate between decade $t_1$ and $t_2$, expressed per $10^5$ person-years.\n\n**5. Algorithmic Summary**\n\nThe complete algorithm to be implemented is as follows:\nFor each test case provided:\n1.  Receive the input vectors for the standard population $S$, cases $C_{t_1}$ and $C_{t_2}$, and study populations $N_{t_1}$ and $N_{t_2}$.\n2.  Calculate the total standard population $S_{total} = \\sum_{i=1}^{k} S_i$.\n3.  Calculate the vector of standard weights $W = (w_1, \\dots, w_k)$ where $w_i = S_i / S_{total}$.\n4.  For decade $t_1$:\n    a. Calculate the vector of age-specific rates $R'_{t_1} = (r_{1,t_1}, \\dots, r_{k,t_1})$ where $r_{i,t_1} = c_{i,t_1} / N_{i,t_1}$.\n    b. Calculate the standardized rate $R_{t_1} = 10^5 \\cdot \\sum_{i=1}^{k} w_i \\cdot r_{i,t_1}$.\n5.  For decade $t_2$:\n    a. Calculate the vector of age-specific rates $R'_{t_2} = (r_{1,t_2}, \\dots, r_{k,t_2})$ where $r_{i,t_2} = c_{i,t_2} / N_{i,t_2}$.\n    b. Calculate the standardized rate $R_{t_2} = 10^5 \\cdot \\sum_{i=1}^{k} w_i \\cdot r_{i,t_2}$.\n6.  Calculate the secular change $\\Delta R = |R_{t_2} - R_{t_1}|$.\n7.  Round the three computed values, $R_{t_1}$, $R_{t_2}$, and $\\Delta R$, to two decimal places.\n8.  Store the resulting triplet of values.\n\nThis procedure will be executed for all test cases, and the results will be aggregated into a single flat list for output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an algorithm for direct age standardization to compute\n    age-standardized incidence rates and their secular change for multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        (\n            (300000, 320000, 200000, 140000, 40000),  # Standard population S^(1)\n            (180, 450, 720, 980, 500),                # Cases C^(1)_t1\n            (200000, 180000, 120000, 70000, 20000),  # Population N^(1)_t1\n            (160, 400, 800, 1100, 600),                # Cases C^(1)_t2\n            (180000, 160000, 130000, 90000, 30000)   # Population N^(1)_t2\n        ),\n        # Test case 2\n        (\n            (400000, 300000, 180000, 100000, 20000),  # Standard population S^(2)\n            (0, 200, 500, 600, 150),                   # Cases C^(2)_t1\n            (250000, 200000, 100000, 40000, 5000),   # Population N^(2)_t1\n            (0, 210, 600, 700, 350),                   # Cases C^(2)_t2\n            (240000, 220000, 120000, 60000, 10000)   # Population N^(2)_t2\n        ),\n        # Test case 3\n        (\n            (500000, 250000, 150000, 80000, 20000),  # Standard population S^(3)\n            (50, 100, 400, 1000, 2000),                # Cases C^(3)_t1\n            (100000, 100000, 100000, 100000, 100000), # Population N^(3)_t1\n            (200, 200, 480, 600, 400),                 # Cases C^(3)_t2\n            (400000, 200000, 120000, 60000, 20000)    # Population N^(3)_t2\n        ),\n        # Test case 4\n        (\n            (200000, 300000, 250000, 200000, 50000),  # Standard population S^(4)\n            (2, 9, 15, 30, 12),                       # Cases C^(4)_t1\n            (20000, 30000, 25000, 20000, 5000),     # Population N^(4)_t1\n            (3, 12, 20, 45, 25),                      # Cases C^(4)_t2\n            (20000, 30000, 25000, 20000, 5000)      # Population N^(4)_t2\n        )\n    ]\n\n    results = []\n    multiplier = 1e5\n\n    for s_pop, c_t1, n_t1, c_t2, n_t2 in test_cases:\n        # Convert tuples to numpy arrays for vectorized operations, ensuring float division.\n        s_pop_arr = np.array(s_pop, dtype=float)\n        c_t1_arr = np.array(c_t1, dtype=float)\n        n_t1_arr = np.array(n_t1, dtype=float)\n        c_t2_arr = np.array(c_t2, dtype=float)\n        n_t2_arr = np.array(n_t2, dtype=float)\n\n        # Step 1: Calculate standard weights\n        total_s_pop = np.sum(s_pop_arr)\n        weights = s_pop_arr / total_s_pop\n\n        # Step 2: Calculate age-specific rates for each decade\n        age_specific_rates_t1 = c_t1_arr / n_t1_arr\n        age_specific_rates_t2 = c_t2_arr / n_t2_arr\n\n        # Step 3: Calculate age-standardized rates (ASR)\n        # The ASR is the weighted average of age-specific rates, scaled by the multiplier.\n        # np.sum(weights * rates) is equivalent to the dot product.\n        asr_t1 = np.sum(weights * age_specific_rates_t1) * multiplier\n        asr_t2 = np.sum(weights * age_specific_rates_t2) * multiplier\n\n        # Step 4: Calculate secular change\n        secular_change = np.abs(asr_t2 - asr_t1)\n\n        # Step 5: Round results to 2 decimal places\n        asr_t1_rounded = np.round(asr_t1, 2)\n        asr_t2_rounded = np.round(asr_t2, 2)\n        secular_change_rounded = np.round(secular_change, 2)\n\n        results.extend([asr_t1_rounded, asr_t2_rounded, secular_change_rounded])\n\n    # Format results to always show two decimal places and create the final output string\n    formatted_results = [f\"{x:.2f}\" for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond long-term secular trends, many infectious diseases exhibit strong seasonal patterns, with cases peaking during certain times of the year. A common way to model this is with a multiplicative decomposition, where a seasonal component scales the underlying trend. In this practice, you will calculate a normalized seasonal index, a powerful tool that quantifies the relative strength of seasonality for each month, helping us understand, for example, just how much more common an infection is in winter compared to summer .",
            "id": "4642118",
            "problem": "A city health department monitors monthly counts of a respiratory infection over $5$ consecutive years. Let the observed count at time $t$ be denoted by $Y_t$, and suppose the time series admits a multiplicative decomposition into secular (long-term) trend, seasonality, and remainder as $Y_t = T_t \\times S_{m(t)} \\times R_t$, where $T_t$ is a slowly varying component, $S_{m(t)}$ is the month-specific seasonal component for month $m(t) \\in \\{1,2,\\dots,12\\}$, and $R_t$ is a remainder term with mean $1$ over time. Assume the secular trend is approximately stable over the $5$-year window so that month-specific averages are meaningful estimators of relative seasonal levels.\n\nYou are provided the arithmetic mean of the monthly counts across the $5$ years for each calendar month:\n- January: $450$\n- February: $400$\n- March: $300$\n- April: $300$\n- May: $250$\n- June: $180$\n- July: $200$\n- August: $225$\n- September: $240$\n- October: $360$\n- November: $375$\n- December: $500$\n\nUsing only the definitions above and the principle that over one full seasonal cycle the seasonal component should be neutral in aggregate, derive a month-specific seasonal index estimator based on dividing each month’s mean by an overall mean across all months and years and then imposing a multiplicative neutrality constraint so that the $12$ monthly indices have a product equal to $1$. Then, compute the normalized seasonal index for February using the data given. Provide your final answer as a unitless decimal rounded to four significant figures.",
            "solution": "The multiplicative decomposition $Y_t = T_t \\times S_{m(t)} \\times R_t$ posits a month-specific seasonal multiplier $S_{m}$ that scales the secular level $T_t$. A natural plug-in estimator of relative seasonality for month $m$ is to compare the mean for that month to an overall mean across all months and years. Let $\\bar{Y}_m$ denote the arithmetic mean of $Y_t$ restricted to month $m$ over the $5$ years, and let $\\bar{Y}$ denote the overall mean across all months and years. Because each month contributes the same number of observations ($5$), $\\bar{Y}$ equals the arithmetic mean of the $12$ month-specific means:\n$$\n\\bar{Y} = \\frac{1}{12}\\sum_{m=1}^{12} \\bar{Y}_m.\n$$\nAn unnormalized relative seasonal factor for month $m$ is then\n$$\ns_m = \\frac{\\bar{Y}_m}{\\bar{Y}}.\n$$\nUnder the multiplicative framework, a standard neutrality requirement over one full cycle is that the product of the normalized seasonal indices equals $1$:\n$$\n\\prod_{m=1}^{12} I_m = 1.\n$$\nTo meet this constraint, we normalize by the geometric mean of the unnormalized factors. Define\n$$\nG = \\left( \\prod_{m=1}^{12} s_m \\right)^{\\frac{1}{12}},\n$$\nand set\n$$\nI_m = \\frac{s_m}{G}.\n$$\nBy construction,\n$$\n\\prod_{m=1}^{12} I_m = \\frac{\\prod_{m=1}^{12} s_m}{G^{12}} = 1.\n$$\n\nWe now compute the quantities for the provided data. First, compute the overall mean $\\bar{Y}$ as the arithmetic mean of the $12$ monthly means:\n\n$$\n\\sum_{m=1}^{12} \\bar{Y}_m = 450 + 400 + 300 + 300 + 250 + 180 + 200 + 225 + 240 + 360 + 375 + 500 = 3780,\n$$\n\nso\n\n$$\n\\bar{Y} = \\frac{3780}{12} = 315.\n$$\n\nThe unnormalized factor for February is\n\n$$\ns_{\\text{Feb}} = \\frac{\\bar{Y}_{\\text{Feb}}}{\\bar{Y}} = \\frac{400}{315}.\n$$\n\n\nNext, compute the normalizing factor $G$:\n\n$$\nG = \\left( \\prod_{m=1}^{12} \\frac{\\bar{Y}_m}{\\bar{Y}} \\right)^{\\frac{1}{12}}\n= \\left( \\frac{\\prod_{m=1}^{12} \\bar{Y}_m}{\\bar{Y}^{12}} \\right)^{\\frac{1}{12}}\n= \\frac{\\left( \\prod_{m=1}^{12} \\bar{Y}_m \\right)^{\\frac{1}{12}}}{\\bar{Y}}.\n$$\n\nObserve that the monthly means can be paired to reveal their product structure:\n- $(450, 200)$: $450 \\times 200 = 90{,}000 = 300^2$,\n- $(400, 225)$: $400 \\times 225 = 90{,}000 = 300^2$,\n- $(360, 250)$: $360 \\times 250 = 90{,}000 = 300^2$,\n- $(375, 240)$: $375 \\times 240 = 90{,}000 = 300^2$,\n- $(300, 300)$: $300 \\times 300 = 90{,}000 = 300^2$,\n- $(500, 180)$: $500 \\times 180 = 90{,}000 = 300^2$.\nThus,\n\n$$\n\\prod_{m=1}^{12} \\bar{Y}_m = (300^2)^6 = 300^{12}.\n$$\n\nTherefore,\n\n$$\nG = \\frac{(300^{12})^{\\frac{1}{12}}}{315} = \\frac{300}{315} = \\frac{20}{21}.\n$$\n\n\nThe normalized seasonal index for February is\n\n$$\nI_{\\text{Feb}} = \\frac{s_{\\text{Feb}}}{G} = \\frac{\\frac{400}{315}}{\\frac{300}{315}} = \\frac{400}{300} = \\frac{4}{3}.\n$$\n\nExpressed as a decimal and rounded to four significant figures,\n\n$$\nI_{\\text{Feb}} \\approx 1.333.\n$$\n\n\nInterpretation: Under a multiplicative seasonal structure with neutral product over the $12$ months, the normalized seasonal index $I_{\\text{Feb}} \\approx 1.333$ indicates that, after controlling for any stable secular level over the $5$-year window, February’s expected count is about $1.333$ times the neutral monthly level (index equal to $1$), reflecting a relative elevation consistent with winter seasonality in respiratory infections. The index is dimensionless and conveys a multiplicative seasonal effect size.",
            "answer": "$$\\boxed{1.333}$$"
        },
        {
            "introduction": "After decomposing a time series into trend and seasonal components, how can we be sure our model has done a good job? The key is to examine the residuals—the part of the data that the model can't explain. If the model is a good fit, the residuals should be unpredictable white noise; if they show patterns, it signals that our model is misspecified. This exercise will teach you how to perform a residual analysis, using autocorrelation to diagnose specific problems like uncaptured seasonality or an ill-fitting trend, and how to choose an appropriate remedy .",
            "id": "4642205",
            "problem": "You are given a set of residual time series $R_t$ that result from a Seasonal-Trend decomposition using Loess (STL) applied to epidemiologic surveillance data. Under a well-specified decomposition, residuals should approximate a white-noise process, that is, have approximately zero linear autocorrelation at non-zero lags. Your task is to evaluate whether each residual series shows evidence of autocorrelation consistent with underfitting of the seasonal or trend components and to algorithmically propose a remedy.\n\nBase facts and definitions to use:\n- A stationary white-noise series has zero autocorrelation at all non-zero lags.\n- The sample autocorrelation at lag $k$ compares the covariance between $R_t$ and $R_{t+k}$ to the variance of $R_t$.\n- A standard portmanteau test for residual autocorrelation aggregates the contributions of autocorrelation at multiple lags to test the null hypothesis that all autocorrelations up to a specified maximum lag are zero.\n\nYour program must, for each test case defined below, do all of the following in strictly numerical terms:\n1) Compute the sample autocorrelation values $r_k$ for lags $k \\in \\{1,2,\\dots,L\\}$ from the series $R_t$ of length $n$.\n2) Perform a standard portmanteau test of no autocorrelation up to lag $L$ at significance level $\\alpha$. Use any accepted portmanteau formulation consistent with the base facts above and report a boolean indicating whether autocorrelation is detected.\n3) Identify the set of lags $k$ such that $|r_k|$ is individually significant at level $\\alpha$ using a large-sample normal approximation for $r_k$ with variance approximately $1/n$. Concretely, declare lag $k$ significant if $|r_k| > z_{1-\\alpha/2}/\\sqrt{n}$, where $z_{1-\\alpha/2}$ is the two-sided normal critical value.\n4) From the pattern of significant autocorrelations, propose a numeric remedy code according to the following decision rules that reflect common STL tuning actions:\n   - Code $0$: No action. Use if there is no evidence of autocorrelation based on the portmanteau test and there are no individually significant lags.\n   - Code $2$: Decrease seasonal smoothing window size (increase seasonal flexibility). Use if there are significant autocorrelations at one or more seasonal multiples $k \\in \\{m,2m,3m,\\dots\\}$ with average sign positive.\n   - Code $3$: Increase trend smoothing window size (make the trend smoother). Use if there are significant negative short-lag autocorrelations, in particular if lag $k=1$ is significant with $r_1 < 0$.\n   - Code $4$: Decrease trend smoothing window size (make the trend more flexible). Use if there are significant positive short-lag autocorrelations, in particular if lag $k=1$ is significant with $r_1 > 0$.\n   - Code $5$: Consider adding an autoregressive moving-average error model (allow residual autocorrelation explicitly). Use if many lags are significant without aligning to seasonal multiples and without the specific short-lag patterns above. For concreteness, apply this if the count of significant lags is at least $\\max\\{5,\\lfloor 0.25L \\rfloor\\}$ and neither of the short-lag rules nor the seasonal-multiple rule applies.\n\nIf multiple rules could apply, prioritize in this order: seasonal multiples (code $2$), then short-lag $k=1$ positive (code $4$), then short-lag $k=1$ negative (code $3$), then broad residual autocorrelation (code $5$), else code $0$.\n\nTest suite. For each case, $t$ starts at $1$ and increments by $1$ to the specified $n$. Use the given seasonal period $m$, maximum lag $L$, and significance level $\\alpha$. All computations are unitless.\n\n- Case A (seasonal underfitting pattern):\n  - $n=120$, $m=12$, $L=24$, $\\alpha=0.05$.\n  - $R_t = \\sin(2\\pi t/12)$ for $t \\in \\{1,\\dots,120\\}$.\n\n- Case B (short-lag negative autocorrelation):\n  - $n=100$, $m=12$, $L=20$, $\\alpha=0.05$.\n  - $R_t = (-0.8)^t$ for $t \\in \\{1,\\dots,100\\}$.\n\n- Case C (leftover slow trend):\n  - $n=200$, $m=12$, $L=24$, $\\alpha=0.05$.\n  - $R_t = \\sin(2\\pi t/200)$ for $t \\in \\{1,\\dots,200\\}$.\n\n- Case D (broad structured residual autocorrelation not aligned with $m$):\n  - $n=140$, $m=12$, $L=24$, $\\alpha=0.05$.\n  - $R_t = \\sin(2\\pi t/7) + 0.5\\sin(2\\pi t/5)$ for $t \\in \\{1,\\dots,140\\}$.\n\n- Case E (near-white deterministic residuals via a linear congruential generator, boundary/no-action case):\n  - $n=200$, $m=12$, $L=10$, $\\alpha=0.05$.\n  - Define integers with modulus $M=10007$, multiplier $a=48271$, increment $c=0$, seed $x_1=1$, and recurrence $x_{t+1} = (a x_t + c) \\bmod M$ for $t \\in \\{1,\\dots,199\\}$. Let $U_t = x_t/M$. Define $R_t = (U_t - \\overline{U})/s_U$, where $\\overline{U}$ is the sample mean of $U_t$ and $s_U$ is the sample standard deviation of $U_t$ over $t \\in \\{1,\\dots,200\\}$.\n\nYour program should produce a single line of output containing the results for the five cases as a comma-separated list enclosed in square brackets. For each case, output a list with three elements: \n- a boolean indicating whether the portmanteau test rejects the null of no autocorrelation at level $\\alpha$,\n- a list of all individually significant lags (integers from $1$ to $L$) in ascending order determined by the normal approximation rule,\n- the integer remedy code as defined above.\n\nFor example, the final output must have the form \n\"[[bool,[k_1,k_2,...],code],[bool,[...],code],[...],[...],[...]]\" \nwith no extra text.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of time series analysis, specifically the diagnostic checking of residuals in epidemiological modeling. The problem is well-posed, with all necessary parameters, data definitions, and logical rules for decision-making provided. The tasks are objective, quantitative, and computationally verifiable.\n\nThe objective is to analyze a given residual time series $R_t$ for patterns of autocorrelation that would suggest shortcomings in a Seasonal-Trend decomposition using Loess (STL). This involves computing the sample autocorrelation function (ACF), performing a portmanteau test for overall significance, identifying individually significant lags, and recommending a corrective action based on a specified rule-based system.\n\nThe analytical procedure for each time series of length $n$ is as follows:\n\nFirst, we define the time series for each test case. The problem provides five distinct cases, generating series $R_t$ for $t \\in \\{1, 2, \\dots, n\\}$.\n\nSecond, we compute the sample autocorrelation function (ACF), $r_k$, for lags $k \\in \\{1, 2, \\dots, L\\}$. The ACF at lag $k$ measures the correlation between the time series and a lagged version of itself. For a series $R_t$ with sample mean $\\bar{R}$, the sample autocorrelation is defined as:\n$$\nr_k = \\frac{\\sum_{t=1}^{n-k} (R_t - \\bar{R})(R_{t+k} - \\bar{R})}{\\sum_{t=1}^{n} (R_t - \\bar{R})^2}\n$$\nThese values are the basis for all subsequent tests.\n\nThird, we perform a Ljung-Box portmanteau test to assess the overall significance of autocorrelation across multiple lags. This test evaluates the null hypothesis $H_0$ that the first $L$ autocorrelation coefficients of the series are all zero, i.e., $H_0: \\rho_1 = \\rho_2 = \\dots = \\rho_L = 0$. The Ljung-Box Q-statistic is calculated as:\n$$\nQ(L) = n(n+2) \\sum_{k=1}^{L} \\frac{r_k^2}{n-k}\n$$\nUnder the null hypothesis, $Q(L)$ follows a chi-squared distribution with $L$ degrees of freedom, denoted $\\chi^2(L)$. We reject $H_0$ at a significance level $\\alpha$ if the computed p-value, $P(\\chi^2(L) > Q(L))$, is less than $\\alpha$. The result is a boolean value indicating rejection.\n\nFourth, we identify individual lags $k$ where the autocorrelation $r_k$ is statistically significant. For a large sample size $n$, $r_k$ under the null hypothesis ($\\rho_k = 0$) is approximately normally distributed with mean $0$ and variance $1/n$. Therefore, we declare lag $k$ significant at level $\\alpha$ if its absolute value exceeds a critical threshold based on the standard normal distribution:\n$$\n|r_k| > \\frac{z_{1-\\alpha/2}}{\\sqrt{n}}\n$$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For $\\alpha = 0.05$, $z_{0.975} \\approx 1.96$. A list of all such significant lags from $1$ to $L$ will be compiled.\n\nFifth, we apply a prioritized, rule-based algorithm to propose a numerical remedy code. The rules are evaluated in the specified order:\n1.  **Code $2$ (Seasonal underfitting):** This is triggered if there are significant autocorrelations at seasonal lags, i.e., for $k$ in the set $\\{m, 2m, \\dots\\}$ where $m$ is the seasonal period, and the average sign of these significant seasonal autocorrelations is positive.\n2.  **Code $4$ (Trend too smooth):** This is triggered if the previous rule does not apply and the autocorrelation at lag $k=1$ is significant and positive ($r_1 > 0$), suggesting an underfitted (insufficiently flexible) trend.\n3.  **Code $3$ (Trend too flexible):** This is triggered if the previous rules do not apply and the autocorrelation at lag $k=1$ is significant and negative ($r_1 < 0$), suggesting an overfitted (too flexible) trend.\n4.  **Code $5$ (General residual structure):** This is triggered if none of the above rules apply and there is a large number of significant lags, specifically if the count is at least $\\max\\{5, \\lfloor 0.25L \\rfloor\\}$. This suggests a more complex correlation structure that may require an autoregressive moving-average (ARMA) model for the residuals.\n5.  **Code $0$ (No action):** If none of the conditions for codes $2, 4, 3$, or $5$ are met, this code is assigned, indicating that the residuals are sufficiently close to white noise.\n\nThis complete procedure is applied to each of the five test cases defined in the problem statement. The generation of the series for Case E involves a linear congruential generator $x_{t+1} = (a x_t + c) \\pmod M$ with seed $x_1=1$, multiplier $a=48271$, increment $c=0$ and modulus $M=10007$, followed by standardization of the sequence $U_t = x_t / M$. All calculations are performed numerically to produce the specified output format for each case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2, norm\n\ndef analyze_series(R_t, n, m, L, alpha):\n    \"\"\"\n    Analyzes a time series for autocorrelation and proposes a remedy.\n    \n    Args:\n        R_t (np.ndarray): The time series data.\n        n (int): Length of the time series.\n        m (int): Seasonal period.\n        L (int): Maximum lag for tests.\n        alpha (float): Significance level.\n\n    Returns:\n        list: [portmanteau_rejection (bool), significant_lags (list), remedy_code (int)]\n    \"\"\"\n    \n    # Step 1: Compute sample autocorrelation function (ACF) r_k for k=1,...,L\n    mean_R = np.mean(R_t)\n    centered_R = R_t - mean_R\n    var_R = np.sum(centered_R**2)\n    \n    r = np.zeros(L)\n    for k in range(1, L + 1):\n        # Numerator: sum of cross-products of lagged series\n        numerator = np.sum(centered_R[:n-k] * centered_R[k:])\n        r[k-1] = numerator / var_R\n\n    # Step 2: Perform Ljung-Box portmanteau test\n    q_statistic = 0\n    for k in range(1, L + 1):\n        q_statistic += (r[k-1]**2) / (n - k)\n    q_statistic *= n * (n + 2)\n    \n    p_value = chi2.sf(q_statistic, df=L)\n    portmanteau_rejection = p_value < alpha\n\n    # Step 3: Identify individually significant lags\n    critical_value = norm.ppf(1 - alpha / 2) / np.sqrt(n)\n    significant_lags = [k for k in range(1, L + 1) if np.abs(r[k-1]) > critical_value]\n\n    # Step 4: Propose a numeric remedy code based on prioritized rules\n    remedy_code = 0  # Default: No action\n    \n    # Rule for Code 2 (seasonal underfitting)\n    seasonal_lags_significant = [k for k in significant_lags if k > 0 and k % m == 0]\n    if seasonal_lags_significant:\n        avg_seasonal_r = np.mean([r[k-1] for k in seasonal_lags_significant])\n        if avg_seasonal_r > 0:\n            remedy_code = 2\n            return [portmanteau_rejection, significant_lags, remedy_code]\n\n    # Rule for Code 4 (trend too smooth)\n    if 1 in significant_lags and r[0] > 0:\n        remedy_code = 4\n        return [portmanteau_rejection, significant_lags, remedy_code]\n\n    # Rule for Code 3 (trend too flexible)\n    if 1 in significant_lags and r[0] < 0:\n        remedy_code = 3\n        return [portmanteau_rejection, significant_lags, remedy_code]\n        \n    # Rule for Code 5 (broad residual autocorrelation)\n    min_significant_count = max(5, L // 4)\n    if len(significant_lags) >= min_significant_count:\n        remedy_code = 5\n        return [portmanteau_rejection, significant_lags, remedy_code]\n\n    # Rule for Code 0 (default if no other rules apply)\n    # The problem description for code 0 is a justification, the priority\n    # list implies it's the fallback. We return the default code 0.\n    return [portmanteau_rejection, significant_lags, remedy_code]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases_defs = {\n        'A': {'n': 120, 'm': 12, 'L': 24, 'alpha': 0.05, 'type': 'A'},\n        'B': {'n': 100, 'm': 12, 'L': 20, 'alpha': 0.05, 'type': 'B'},\n        'C': {'n': 200, 'm': 12, 'L': 24, 'alpha': 0.05, 'type': 'C'},\n        'D': {'n': 140, 'm': 12, 'L': 24, 'alpha': 0.05, 'type': 'D'},\n        'E': {'n': 200, 'm': 12, 'L': 10, 'alpha': 0.05, 'type': 'E'},\n    }\n\n    results = []\n\n    for case_id in ['A', 'B', 'C', 'D', 'E']:\n        params = test_cases_defs[case_id]\n        n, m, L, alpha = params['n'], params['m'], params['L'], params['alpha']\n        \n        R_t = None\n        t = np.arange(1, n + 1)\n\n        if params['type'] == 'A':\n            R_t = np.sin(2 * np.pi * t / 12)\n        elif params['type'] == 'B':\n            R_t = (-0.8)**t\n        elif params['type'] == 'C':\n            R_t = np.sin(2 * np.pi * t / 200)\n        elif params['type'] == 'D':\n            R_t = np.sin(2 * np.pi * t / 7) + 0.5 * np.sin(2 * np.pi * t / 5)\n        elif params['type'] == 'E':\n            M = 10007\n            a = 48271\n            c = 0\n            x = np.zeros(n, dtype=np.int64)\n            x[0] = 1\n            for i in range(1, n):\n                x[i] = (a * x[i-1] + c) % M\n            \n            U_t = x / M\n            mean_U = np.mean(U_t)\n            std_U = np.std(U_t)\n            R_t = (U_t - mean_U) / std_U\n        \n        result_tuple = analyze_series(R_t, n, m, L, alpha)\n        results.append(result_tuple)\n\n    # Format output precisely as requested\n    formatted_results = []\n    for res in results:\n        bool_str = str(res[0])\n        list_str = str(res[1])\n        code_str = str(res[2])\n        formatted_results.append(f\"[{bool_str},{list_str},{code_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```"
        }
    ]
}