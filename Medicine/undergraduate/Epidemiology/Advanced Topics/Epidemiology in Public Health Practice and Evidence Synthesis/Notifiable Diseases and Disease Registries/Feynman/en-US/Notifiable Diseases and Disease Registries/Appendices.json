{
    "hands_on_practices": [
        {
            "introduction": "The incidence rates published by disease registries are fundamental to public health, yet they are subject to measurement error. This practical exercise will guide you through calculating the bias in an incidence estimate that arises from the operational realities of surveillance, namely imperfect laboratory tests and incomplete testing of suspected cases. By working through this scenario , you will gain a quantitative understanding of how diagnostic sensitivity ($Se$) and specificity ($Sp$) directly impact the accuracy of headline public health figures.",
            "id": "4614601",
            "problem": "A nationally notifiable respiratory infection is tracked in the Disease Registry (DR). Surveillance uses a clinical case definition to identify suspected cases, which are categorized as \"probable\" if laboratory confirmation is missing or negative, and \"confirmed\" if laboratory testing is positive. The laboratory test is a Polymerase Chain Reaction (PCR) assay with imperfect diagnostic performance. The registry’s annual incidence estimate is computed using only confirmed cases divided by the total person-time at risk.\n\nIn a given year, the population at risk contributes $5{,}000{,}000$ person-years. There are $1{,}500$ true incident infections that all meet the clinical case definition and are captured as suspected by the surveillance system. In addition, clinical evaluation yields $1{,}000$ suspected individuals who do not have the infection (non-diseased suspects). A fraction $f = 0.80$ of all suspected individuals undergo laboratory testing. The PCR assay has sensitivity $Se = 0.92$ and specificity $Sp = 0.98$, where sensitivity is defined as the probability that the test is positive given true infection, and specificity is defined as the probability that the test is negative given no infection.\n\nUsing only these foundational definitions and the incidence concept of cases per person-time, derive an analytic expression for the bias in the registry’s incidence estimate attributable to misclassification between probable and confirmed categories caused by imperfect sensitivity and specificity and incomplete testing, where bias is defined as $I_{\\text{obs}} - I_{\\text{true}}$, with $I_{\\text{obs}}$ the observed incidence based on confirmed cases and $I_{\\text{true}}$ the true incidence based on true infections. Then, evaluate this bias numerically for the parameters given. Express your final numeric answer in cases per $100{,}000$ person-years and round your answer to three significant figures.",
            "solution": "The problem statement is scientifically grounded, well-posed, and contains all necessary information for a unique solution. The parameters and definitions provided are standard in the field of epidemiology. Therefore, the problem is valid, and we proceed with the solution.\n\nThe objective is to derive an expression for the bias in the incidence estimate and then calculate its numerical value. The bias is defined as the difference between the observed incidence ($I_{\\text{obs}}$) and the true incidence ($I_{\\text{true}}$).\n\nFirst, let us define the variables from the problem statement:\n- The number of true incident infections that are captured as suspected cases: $D^+ = 1{,}500$.\n- The number of non-diseased individuals captured as suspected cases: $D^- = 1{,}000$.\n- The total person-time at risk: $PT = 5{,}000{,}000$ person-years.\n- The fraction of suspected individuals who undergo laboratory testing: $f = 0.80$.\n- The sensitivity of the PCR assay: $Se = P(T^+|D^+) = 0.92$.\n- The specificity of the PCR assay: $Sp = P(T^-|D^-) = 0.98$.\n\nThe true incidence, $I_{\\text{true}}$, is the number of true incident infections divided by the total person-time at risk.\n$$I_{\\text{true}} = \\frac{D^+}{PT}$$\n\nThe observed incidence, $I_{\\text{obs}}$, is based on the number of \"confirmed\" cases. A case is confirmed if a suspected individual has a positive laboratory test ($T^+$). The total number of confirmed cases, which we denote as $C_{\\text{conf}}$, is the sum of true positives (TP) and false positives (FP) from the pool of tested individuals.\n\nThe total number of suspected cases is $D^+ + D^-$.\nThe number of truly infected individuals who are tested is $f \\times D^+$.\nThe number of non-infected individuals who are tested is $f \\times D^-$.\n\nThe number of true positives (TP) is the number of infected individuals who are tested and have a positive result.\n$$TP = (f \\cdot D^+) \\cdot Se$$\n\nThe number of false positives (FP) is the number of non-infected individuals who are tested and have a positive result. The probability of a positive test in a non-infected individual is $P(T^+|D^-) = 1 - P(T^-|D^-) = 1 - Sp$.\n$$FP = (f \\cdot D^-) \\cdot (1 - Sp)$$\n\nThe total number of confirmed cases is the sum of these two groups.\n$$C_{\\text{conf}} = TP + FP = f \\cdot D^+ \\cdot Se + f \\cdot D^- \\cdot (1 - Sp)$$\n\nThe observed incidence, $I_{\\text{obs}}$, is the number of confirmed cases divided by the total person-time at risk.\n$$I_{\\text{obs}} = \\frac{C_{\\text{conf}}}{PT} = \\frac{f \\cdot D^+ \\cdot Se + f \\cdot D^- \\cdot (1 - Sp)}{PT}$$\n\nThe bias is defined as $Bias = I_{\\text{obs}} - I_{\\text{true}}$. Substituting the expressions for $I_{\\text{obs}}$ and $I_{\\text{true}}$, we obtain the analytic expression for the bias:\n$$Bias = \\frac{f \\cdot D^+ \\cdot Se + f \\cdot D^- \\cdot (1 - Sp)}{PT} - \\frac{D^+}{PT}$$\nThis expression can be simplified to:\n$$Bias = \\frac{1}{PT} \\left[ f \\cdot D^+ \\cdot Se + f \\cdot D^- \\cdot (1 - Sp) - D^+ \\right]$$\nThis is the required analytic expression for the bias.\n\nNow, we evaluate this expression numerically using the given parameters.\n$D^+ = 1500$, $D^- = 1000$, $PT = 5{,}000{,}000$, $f = 0.80$, $Se = 0.92$, and $Sp = 0.98$.\n\nFirst, we calculate the total number of confirmed cases, $C_{\\text{conf}}$:\n$$C_{\\text{conf}} = (0.80 \\times 1500 \\times 0.92) + (0.80 \\times 1000 \\times (1 - 0.98))$$\n$$C_{\\text{conf}} = (1200 \\times 0.92) + (800 \\times 0.02)$$\n$$C_{\\text{conf}} = 1104 + 16$$\n$$C_{\\text{conf}} = 1120$$\n\nThe registry counts $1120$ confirmed cases, whereas the true number of incident cases is $1500$. The difference in case counts is $1120 - 1500 = -380$. This indicates a net under-ascertainment of cases.\n\nThe bias in incidence is this difference in counts divided by the person-time.\n$$Bias = \\frac{C_{\\text{conf}} - D^+}{PT} = \\frac{1120 - 1500}{5{,}000{,}000} = \\frac{-380}{5{,}000{,}000} \\text{ cases per person-year}$$\n$$Bias = -0.000076 \\text{ cases per person-year}$$\n\nThe problem requires the answer to be expressed in cases per $100{,}000$ person-years. We convert the units:\n$$Bias_{\\text{per } 100k} = \\frac{-380}{5{,}000{,}000} \\times 100{,}000$$\n$$Bias_{\\text{per } 100k} = -380 \\times \\frac{100{,}000}{5{,}000{,}000} = -380 \\times \\frac{1}{50}$$\n$$Bias_{\\text{per } 100k} = -7.6$$\n\nFinally, we round the result to three significant figures.\n$$Bias_{\\text{per } 100k} = -7.60$$\nThe negative sign indicates that the registry's incidence estimate is lower than the true incidence, representing an underestimation.",
            "answer": "$$\\boxed{-7.60}$$"
        },
        {
            "introduction": "A central challenge for any disease registry is under-ascertainment—the fact that no single data source captures every case. This problem introduces capture-recapture analysis, a classic epidemiological tool used to estimate the true number of cases by comparing two or more incomplete lists . You will derive a bias-corrected estimator and, importantly, explore what happens when the core assumption of independence is violated, a common issue in real-world surveillance systems.",
            "id": "4614599",
            "problem": "A national surveillance team aims to estimate the total number of cases $N$ of a notifiable infectious disease during a calendar year using two independent sources: a statutory Notifiable Disease Reporting System (source $1$) and a Hospital Discharge Registry (source $2$). Assume the population is closed to case entry and removal during the study period, that case ascertainment by each source is without replacement, and that case-linkage across sources is perfect. Let $n_1$ denote the number of cases captured by source $1$, $n_2$ the number captured by source $2$, and $m$ the number appearing in both sources.\n\nStarting from the core definitions of two-source capture–recapture under independence, and using the fact that, conditional on $N$, $n_1$, and $n_2$, the overlap $m$ follows a hypergeometric distribution, derive a small-sample bias-reduced estimator for $N$ based on $(n_1,n_2,m)$, without assuming large-sample approximations. Then, consider departure from independence between sources modeled by a multiplicative dependence factor $\\phi$ defined by $p_{12} = \\phi\\, p_1 p_2$, where $p_1$ and $p_2$ are the marginal capture probabilities for sources $1$ and $2$, respectively, and $p_{12}$ is the joint capture probability. Using first principles and asymptotic (large $N$) reasoning, determine the closed-form expression for the asymptotic multiplicative bias of the bias-reduced estimator derived earlier, defined as the limit of $\\mathbb{E}[\\hat{N}]/N$ as $N \\to \\infty$, under the dependence model given by $\\phi$.\n\nExpress the final asymptotic multiplicative bias as a single closed-form analytic expression in terms of $\\phi$. No numerical rounding is required. No units are required for the final expression.",
            "solution": "This problem consists of two parts. First, we must derive a small-sample bias-reduced estimator for the total population size $N$ using a two-source capture-recapture model under the assumption of independence. Second, we must calculate the asymptotic multiplicative bias of this estimator when the independence assumption is violated, and the dependence is modeled by a factor $\\phi$.\n\n### Part 1: Derivation of the Bias-Reduced Estimator\n\nThe problem states that conditional on the total number of cases $N$ and the number of cases captured by source 1, $n_1$, the number of cases captured by source 2, $n_2$, can be seen as a sample of size $n_2$ drawn without replacement from the total population of $N$ cases. Within this population of $N$ cases, $n_1$ are \"marked\" (i.e., also in source 1). The number of cases found in both sources, $m$, is therefore the number of marked cases in the sample of size $n_2$. This is precisely the setup for a hypergeometric distribution.\n\nThe probability mass function for $m$, conditional on $N$, $n_1$, and $n_2$, is given by:\n$$ P(m | N, n_1, n_2) = \\frac{\\binom{n_1}{m} \\binom{N-n_1}{n_2-m}}{\\binom{N}{n_2}} $$\nThe expected value of $m$ is $\\mathbb{E}[m] = \\frac{n_1 n_2}{N}$. Replacing the expectation $\\mathbb{E}[m]$ with its observed value $m$ and solving for $N$ yields the maximum likelihood estimator (MLE), also known as the Lincoln-Petersen estimator, $\\hat{N}_{LP} = \\frac{n_1 n_2}{m}$. This estimator is known to be biased, particularly for small sample sizes, and it is undefined if $m=0$.\n\nTo derive a bias-reduced estimator, we follow the work of Chapman (1951), who proposed the estimator $\\hat{N}_{C} = \\frac{(n_1+1)(n_2+1)}{m+1} - 1$. This form is motivated by finding a function of the observations whose expectation is related to $N$ in a simple way. Let us demonstrate that this estimator is nearly unbiased by calculating its expectation. We are interested in $\\mathbb{E}[\\hat{N}_{C}]$. It is more direct to evaluate $\\mathbb{E}[\\hat{N}_{C}+1]$.\n\n$$ \\mathbb{E}[\\hat{N}_{C}+1] = \\mathbb{E}\\left[\\frac{(n_1+1)(n_2+1)}{m+1}\\right] = (n_1+1)(n_2+1)\\mathbb{E}\\left[\\frac{1}{m+1}\\right] $$\nThe expectation of $\\frac{1}{m+1}$ is calculated over the distribution of $m$:\n$$ \\mathbb{E}\\left[\\frac{1}{m+1}\\right] = \\sum_{k} \\frac{1}{k+1} P(m=k) = \\sum_{k} \\frac{1}{k+1} \\frac{\\binom{n_1}{k} \\binom{N-n_1}{n_2-k}}{\\binom{N}{n_2}} $$\nwhere the sum is over the support of the distribution. We use the identity $\\frac{1}{k+1}\\binom{n}{k} = \\frac{1}{n+1}\\binom{n+1}{k+1}$. Applying this to $\\frac{1}{k+1}\\binom{n_1}{k}$:\n$$ \\mathbb{E}\\left[\\frac{1}{m+1}\\right] = \\frac{1}{\\binom{N}{n_2}} \\sum_{k} \\frac{1}{n_1+1} \\binom{n_1+1}{k+1} \\binom{N-n_1}{n_2-k} $$\n$$ = \\frac{1}{(n_1+1)\\binom{N}{n_2}} \\sum_{k} \\binom{n_1+1}{k+1} \\binom{N-n_1}{n_2-k} $$\nLet $j=k+1$. The summation becomes:\n$$ \\sum_{j} \\binom{n_1+1}{j} \\binom{N-n_1}{n_2-(j-1)} = \\sum_{j} \\binom{n_1+1}{j} \\binom{N-n_1}{(n_2+1)-j} $$\nThis sum is in the form of Vandermonde's Identity, $\\sum_{j} \\binom{r}{j}\\binom{s}{c-j} = \\binom{r+s}{c}$, with $r=n_1+1$, $s=N-n_1$, and $c=n_2+1$. Therefore, the sum is equal to:\n$$ \\binom{(n_1+1) + (N-n_1)}{n_2+1} = \\binom{N+1}{n_2+1} $$\nSubstituting this back into the expression for the expectation:\n$$ \\mathbb{E}\\left[\\frac{1}{m+1}\\right] = \\frac{1}{(n_1+1)\\binom{N}{n_2}} \\binom{N+1}{n_2+1} $$\nWe can simplify the ratio of the binomial coefficients:\n$$ \\frac{\\binom{N+1}{n_2+1}}{\\binom{N}{n_2}} = \\frac{(N+1)!}{(n_2+1)!(N-n_2)!} \\cdot \\frac{n_2!(N-n_2)!}{N!} = \\frac{N+1}{n_2+1} $$\nThis gives:\n$$ \\mathbb{E}\\left[\\frac{1}{m+1}\\right] = \\frac{1}{n_1+1} \\cdot \\frac{N+1}{n_2+1} $$\nNow we can compute $\\mathbb{E}[\\hat{N}_{C}+1]$:\n$$ \\mathbb{E}[\\hat{N}_{C}+1] = (n_1+1)(n_2+1) \\left( \\frac{1}{n_1+1} \\frac{N+1}{n_2+1} \\right) = N+1 $$\nThus, $\\mathbb{E}[\\hat{N}_{C}] = N$. This result holds exactly if the summation is over all possible values, which requires $n_1+n_2 \\geq N$. In practice, this condition may not be met, but the estimator still has a much-reduced bias compared to the MLE. Therefore, the small-sample bias-reduced estimator, which we will call $\\hat{N}_{BR}$, is:\n$$ \\hat{N}_{BR} = \\frac{(n_1+1)(n_2+1)}{m+1} - 1 $$\n\n### Part 2: Asymptotic Multiplicative Bias under Dependence\n\nWe now consider the case where the two sources are not independent. The dependence is modeled by the relationship $p_{12} = \\phi\\, p_1 p_2$, where $p_1$ and $p_2$ are the marginal capture probabilities, $p_{12}$ is the joint capture probability, and $\\phi$ is a multiplicative dependence factor. If $\\phi=1$, the sources are independent. If $\\phi>1$, there is positive dependence (cases in one source are more likely to be in the other). If $\\phi<1$, there is negative dependence.\n\nThe problem asks for the asymptotic multiplicative bias of the estimator $\\hat{N}_{BR}$ derived above. This bias is defined as $\\lim_{N \\to \\infty} \\frac{\\mathbb{E}[\\hat{N}_{BR}]}{N}$.\n\nThe \"asymptotic (large $N$) reasoning\" implies that as $N \\to \\infty$, the observed counts $n_1$, $n_2$, and $m$ converge to their expected values. By the Law of Large Numbers, for large $N$:\n$n_1/N \\to p_1 \\implies n_1 \\approx N p_1$\n$n_2/N \\to p_2 \\implies n_2 \\approx N p_2$\n$m/N \\to p_{12} \\implies m \\approx N p_{12} = N \\phi p_1 p_2$\n\nThe expectation of a ratio of random variables is complex. However, for large $N$, the estimator $\\hat{N}_{BR}$ itself will be close to the value obtained by substituting the expected values of $n_1$, $n_2$, and $m$ into its formula. The asymptotic expectation of $\\hat{N}_{BR}$ can thus be approximated by this value. This is a standard approach in large-sample theory, where the limit of the expectation is taken to be the expectation of the limit.\n\n$$ \\mathbb{E}[\\hat{N}_{BR}] \\approx \\frac{(\\mathbb{E}[n_1]+1)(\\mathbb{E}[n_2]+1)}{\\mathbb{E}[m]+1} - 1 $$\nSubstituting the large-$N$ expectations:\n$$ \\mathbb{E}[\\hat{N}_{BR}] \\approx \\frac{(N p_1 + 1)(N p_2 + 1)}{N \\phi p_1 p_2 + 1} - 1 $$\nNow we can compute the asymptotic multiplicative bias:\n$$ B = \\lim_{N \\to \\infty} \\frac{\\mathbb{E}[\\hat{N}_{BR}]}{N} = \\lim_{N \\to \\infty} \\frac{1}{N} \\left[ \\frac{(N p_1 + 1)(N p_2 + 1)}{N \\phi p_1 p_2 + 1} - 1 \\right] $$\nLet's evaluate the limit of each term separately.\n$$ \\lim_{N \\to \\infty} \\frac{-1}{N} = 0 $$\nThe main part of the limit is:\n$$ \\lim_{N \\to \\infty} \\frac{1}{N} \\left[ \\frac{(N p_1 + 1)(N p_2 + 1)}{N \\phi p_1 p_2 + 1} \\right] = \\lim_{N \\to \\infty} \\frac{N^2 p_1 p_2 + N p_1 + N p_2 + 1}{N(N \\phi p_1 p_2 + 1)} $$\n$$ = \\lim_{N \\to \\infty} \\frac{N^2 p_1 p_2 + N(p_1+p_2) + 1}{N^2 \\phi p_1 p_2 + N} $$\nTo evaluate this limit, we divide both the numerator and the denominator by the highest power of $N$, which is $N^2$:\n$$ B = \\lim_{N \\to \\infty} \\frac{\\frac{N^2 p_1 p_2}{N^2} + \\frac{N(p_1+p_2)}{N^2} + \\frac{1}{N^2}}{\\frac{N^2 \\phi p_1 p_2}{N^2} + \\frac{N}{N^2}} = \\lim_{N \\to \\infty} \\frac{p_1 p_2 + \\frac{p_1+p_2}{N} + \\frac{1}{N^2}}{\\phi p_1 p_2 + \\frac{1}{N}} $$\nAs $N \\to \\infty$, all terms with $N$ in the denominator go to $0$:\n$$ B = \\frac{p_1 p_2 + 0 + 0}{\\phi p_1 p_2 + 0} = \\frac{p_1 p_2}{\\phi p_1 p_2} $$\nAssuming $p_1 > 0$ and $p_2 > 0$, we can cancel the term $p_1 p_2$:\n$$ B = \\frac{1}{\\phi} $$\nThe asymptotic multiplicative bias of the bias-reduced estimator, when the true relationship between the sources is $p_{12} = \\phi p_1 p_2$, is $1/\\phi$. This result is independent of the marginal capture probabilities $p_1$ and $p_2$.",
            "answer": "$$\\boxed{\\frac{1}{\\phi}}$$"
        },
        {
            "introduction": "Beyond simply counting cases, modern disease registries are used for dynamic, real-time surveillance to detect outbreaks. This hands-on coding exercise places you in the role of a surveillance epidemiologist, tasked with building a statistical model to forecast the expected number of daily cases based on trends and seasonal patterns . You will then learn to compute standardized residuals to identify statistically significant \"spikes\" in case counts that could signal a new outbreak, forming the basis of an automated alert system.",
            "id": "4614619",
            "problem": "A health department maintains a daily registry of a notifiable infectious disease. Let $y_t$ denote the observed count on day $t$ for $t \\in \\{1,2,\\dots,N\\}$. Assume $y_t \\mid \\mu_t \\sim \\mathrm{Poisson}(\\mu_t)$ with canonical log link, where the systematic component models a seasonal pattern using harmonic (sine and cosine) terms with known period $P$ (in days) and up to $K$ harmonics, optionally including a linear time trend. Specifically, define the design matrix $X \\in \\mathbb{R}^{N \\times p}$ row-wise as follows for each day $t$:\n- An intercept term equal to $1$.\n- An optional linear time trend term equal to $t$ if included.\n- For each harmonic $k \\in \\{1,\\dots,K\\}$, the terms $\\sin\\!\\big(2\\pi k t / P\\big)$ and $\\cos\\!\\big(2\\pi k t / P\\big)$.\n\nLet $\\beta \\in \\mathbb{R}^p$ be the regression coefficients, and $\\eta_t = x_t^\\top \\beta$ the linear predictor for row vector $x_t$ of $X$. The expected count obeys $\\mu_t = \\exp(\\eta_t)$. The likelihood for independent observations is\n$$\nL(\\beta) = \\prod_{t=1}^{N} \\frac{\\mu_t^{y_t} e^{-\\mu_t}}{y_t!}, \\quad \\text{and} \\quad \\ell(\\beta) = \\sum_{t=1}^{N} \\left( y_t \\log \\mu_t - \\mu_t - \\log(y_t!) \\right),\n$$\nwith $\\log \\mu_t = \\eta_t = x_t^\\top \\beta$. Estimation proceeds by maximum likelihood using the iteratively reweighted least squares algorithm from generalized linear model theory. After obtaining the maximum likelihood estimate $\\hat{\\beta}$, define $\\hat{\\mu}_t = \\exp(x_t^\\top \\hat{\\beta})$. Let $W = \\mathrm{diag}(\\hat{\\mu}_1,\\dots,\\hat{\\mu}_N)$ denote the diagonal weight matrix at convergence, and define the “hat” matrix\n$$\nH = W^{1/2} X \\left( X^\\top W X \\right)^{-1} X^\\top W^{1/2}.\n$$\nLet $h_t$ denote the $t$-th diagonal element of $H$. The leverage-adjusted standardized Pearson residual for day $t$ is\n$$\nr_t^\\ast = \\frac{y_t - \\hat{\\mu}_t}{\\sqrt{\\hat{\\mu}_t \\, (1 - h_t)}}.\n$$\nUnder standard regularity conditions for generalized linear models and correct model specification, $r_t^\\ast$ is approximately standard normal for large $N$. For prospective outbreak signal detection on a target day $t^\\ast$, a one-sided exceedance rule declares a signal if $r_{t^\\ast}^\\ast > z$, where $z$ is a pre-specified threshold (e.g., $z = 2.5$ corresponding to an upper-tail event under approximate normality).\n\nTask. Write a complete program that, for each test case, constructs the design matrix $X$ according to the specified $P$, $K$, and trend inclusion flag; fits the Poisson generalized linear model by maximum likelihood via iteratively reweighted least squares; computes $\\hat{\\mu}_t$, the hat diagonal $h_t$, and the leverage-adjusted standardized Pearson residual $r_t^\\ast$ for the specified target day $t^\\ast$; and returns both $\\hat{\\mu}_{t^\\ast}$ and $r_{t^\\ast}^\\ast$, along with a boolean outbreak signal indicator equal to $\\mathrm{True}$ if $r_{t^\\ast}^\\ast > z$ and $\\mathrm{False}$ otherwise. Report floating-point results rounded to $6$ decimal places.\n\nUse the following test suite of deterministic cases that mimic daily notifiable disease counts with weekly seasonality:\n\n- Case A (happy path with a clear spike): $N = 30$, $P = 7$, $K = 1$, include linear trend, target day $t^\\ast = 28$, threshold $z = 2.5$, and observed daily counts\n$$\n\\{7,9,10,11,12,11,9,7,9,10,11,12,11,9,7,9,10,11,12,11,9,7,9,10,11,12,11,30,7,9\\}.\n$$\n\n- Case B (longer series without a spike): $N = 35$, $P = 7$, $K = 2$, exclude linear trend, target day $t^\\^\\ast = 35$, threshold $z = 2.5$, and observed daily counts\n$$\n\\{4,5,6,6,6,5,4,4,5,6,6,6,5,4,4,5,6,6,6,5,4,4,5,6,6,6,5,4,4,5,6,6,6,5,4\\}.\n$$\n\n- Case C (small counts with zeros as a boundary condition): $N = 14$, $P = 7$, $K = 1$, include linear trend, target day $t^\\ast = 14$, threshold $z = 2.5$, and observed daily counts\n$$\n\\{0,1,0,2,1,3,1,0,1,0,2,1,3,1\\}.\n$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of the form $[\\hat{\\mu}_{t^\\ast}, r_{t^\\ast}^\\ast, \\text{signal}]$. The floating-point values must be rounded to $6$ decimal places, and the boolean must be unquoted. For example, an output with three test cases should look like\n$$\n[[\\hat{\\mu}_A, r_A^\\ast, \\mathrm{True/False}],[\\hat{\\mu}_B, r_B^\\ast, \\mathrm{True/False}],[\\hat{\\mu}_C, r_C^\\ast, \\mathrm{True/False}]].\n$$\nNo units are required for counts; report dimensionless values for residuals. Angles inside trigonometric functions are in radians by construction via the $2\\pi$ factor, and no user-supplied angle units are needed. All computations must be deterministic and self-contained within the program.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It describes a standard epidemiological surveillance task using established statistical methods. We proceed with a complete solution.\n\nThe solution implements an algorithm for prospective outbreak detection based on a Poisson generalized linear model (GLM). The methodology involves four main stages: model specification, parameter estimation, residual calculation, and signal detection.\n\n1.  **Model Specification and Design Matrix Construction**\nThe observed daily disease counts, $y_t$ for day $t \\in \\{1, 2, \\dots, N\\}$, are assumed to follow a Poisson distribution, $y_t \\sim \\mathrm{Poisson}(\\mu_t)$. The expected count $\\mu_t$ is related to a set of predictors through a log-linear model:\n$$\n\\log(\\mu_t) = \\eta_t = x_t^\\top \\beta\n$$\nHere, $\\eta_t$ is the linear predictor, $x_t$ is a row vector of predictors for day $t$, and $\\beta$ is a vector of regression coefficients. The design matrix $X \\in \\mathbb{R}^{N \\times p}$ is constructed by stacking the row vectors $x_t$. Each $x_t$ is composed of:\n- An intercept term, which is always $1$.\n- An optional linear time trend, with value $t$.\n- $K$ pairs of harmonic terms to model seasonality with period $P$. For each harmonic $k \\in \\{1, \\dots, K\\}$, the terms are $\\sin(2\\pi k t / P)$ and $\\cos(2\\pi k t / P)$.\nThe dimension $p$ of the coefficient vector $\\beta$ is therefore $p = 1 + \\mathbb{I}(\\text{trend}) + 2K$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n\n2.  **Parameter Estimation via Iteratively Reweighted Least Squares (IRLS)**\nThe unknown coefficients $\\beta$ are estimated by maximizing the Poisson log-likelihood function:\n$$\n\\ell(\\beta) = \\sum_{t=1}^{N} \\left( y_t \\eta_t - \\exp(\\eta_t) - \\log(y_t!) \\right)\n$$\nThis maximization is performed using the Iteratively Reweighted Least Squares (IRLS) algorithm. For a Poisson model with the canonical log link, IRLS is equivalent to the Newton-Raphson method. Starting with an initial guess for $\\beta$ (e.g., $\\beta^{(0)} = \\mathbf{0}$), the coefficients are updated iteratively:\n$$\n\\beta^{(i+1)} = \\beta^{(i)} + \\left( X^\\top W^{(i)} X \\right)^{-1} X^\\top (y - \\mu^{(i)})\n$$\nwhere:\n- $\\mu^{(i)} = \\exp(X\\beta^{(i)})$ are the estimated mean counts at iteration $i$.\n- $W^{(i)} = \\mathrm{diag}(\\mu_1^{(i)}, \\dots, \\mu_N^{(i)})$ is a diagonal matrix of weights.\nThe term $(X^\\top W^{(i)} X)$ is the Fisher information matrix. The iteration continues until the change in $\\beta$ falls below a predefined tolerance, yielding the maximum likelihood estimate $\\hat{\\beta}$.\n\n3.  **Leverage and Residual Calculation**\nUpon convergence at $\\hat{\\beta}$, the final fitted values are $\\hat{\\mu} = \\exp(X\\hat{\\beta})$, and the final weight matrix is $W = \\mathrm{diag}(\\hat{\\mu})$. The influence of each observation on its own fitted value is quantified by its leverage, which is the corresponding diagonal element $h_t$ of the hat matrix $H$:\n$$\nH = W^{1/2} X \\left( X^\\top W X \\right)^{-1} X^\\top W^{1/2}\n$$\nA computationally efficient method to find the diagonal elements without forming the full $N \\times N$ matrix $H$ is:\n$$\nh_t = H_{tt} = \\hat{\\mu}_t x_t \\left( X^\\top W X \\right)^{-1} x_t^\\top\n$$\nWith these quantities, we compute the leverage-adjusted standardized Pearson residual for each observation. The Pearson residual, $r_t^P = (y_t - \\hat{\\mu}_t) / \\sqrt{\\hat{\\mu}_t}$, is standardized by its approximate standard deviation, $\\sqrt{1 - h_t}$, to yield:\n$$\nr_t^\\ast = \\frac{y_t - \\hat{\\mu}_t}{\\sqrt{\\hat{\\mu}_t (1 - h_t)}}\n$$\nUnder correct model specification, these residuals are approximately distributed as standard normal variables for large $N$.\n\n4.  **Outbreak Detection**\nFor a specific target day $t^\\ast$, the surveillance algorithm computes the residual $r_{t^\\ast}^\\ast$. An outbreak signal is declared if this value exceeds a pre-specified critical threshold $z$, which corresponds to an upper-tail quantile of the standard normal distribution. The condition for an alert is:\n$$\nr_{t^\\ast}^\\ast > z\n$$\nThe program implements these steps for each test case, reporting the estimated mean $\\hat{\\mu}_{t^\\ast}$, the residual $r_{t^\\ast}^\\ast$, and the boolean signal indicator.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are required for this solution.\n\ndef fit_poisson_glm(y, X, max_iter=50, tol=1e-8):\n    \"\"\"\n    Fits a Poisson GLM using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        y (np.ndarray): The observed counts (response variable).\n        X (np.ndarray): The design matrix.\n        max_iter (int): Maximum number of iterations for the IRLS algorithm.\n        tol (float): Tolerance for convergence based on the change in coefficients.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The estimated regression coefficients (beta_hat).\n            - np.ndarray: The fitted mean values (mu_hat).\n    \"\"\"\n    n_params = X.shape[1]\n    beta = np.zeros(n_params)\n\n    for _ in range(max_iter):\n        eta = X @ beta\n        mu = np.exp(eta)\n        \n        W = np.diag(mu)\n        \n        # Newton-Raphson update step: beta_new = beta + (X'WX)^-1 * X'(y-mu)\n        score = X.T @ (y - mu)\n        fisher_info = X.T @ W @ X\n        \n        try:\n            # np.linalg.solve is numerically more stable than inverting the matrix\n            delta = np.linalg.solve(fisher_info, score)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if matrix is singular or ill-conditioned\n            delta = np.linalg.pinv(fisher_info) @ score\n            \n        beta_new = beta + delta\n        \n        # Check for convergence\n        if np.sum((beta_new - beta)**2) < tol:\n            beta = beta_new\n            break\n        \n        beta = beta_new\n    \n    # After convergence, calculate final mean from the final beta\n    final_eta = X @ beta\n    final_mu = np.exp(final_eta)\n    \n    return beta, final_mu\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"y\": [7,9,10,11,12,11,9,7,9,10,11,12,11,9,7,9,10,11,12,11,9,7,9,10,11,12,11,30,7,9],\n            \"P\": 7, \"K\": 1, \"include_trend\": True, \"t_star\": 28, \"z\": 2.5\n        },\n        {\n            \"y\": [4,5,6,6,6,5,4,4,5,6,6,6,5,4,4,5,6,6,6,5,4,4,5,6,6,6,5,4,4,5,6,6,6,5,4],\n            \"P\": 7, \"K\": 2, \"include_trend\": False, \"t_star\": 35, \"z\": 2.5\n        },\n        {\n            \"y\": [0,1,0,2,1,3,1,0,1,0,2,1,3,1],\n            \"P\": 7, \"K\": 1, \"include_trend\": True, \"t_star\": 14, \"z\": 2.5\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        y_obs = case[\"y\"]\n        P, K, include_trend, t_star, z = case[\"P\"], case[\"K\"], case[\"include_trend\"], case[\"t_star\"], case[\"z\"]\n\n        y = np.array(y_obs, dtype=np.float64)\n        N = len(y)\n        t = np.arange(1, N + 1)\n\n        # 1. Construct the design matrix X\n        columns = [np.ones(N)] # Intercept\n        if include_trend:\n            columns.append(t)\n        for k in range(1, K + 1):\n            angle = 2 * np.pi * k * t / P\n            columns.append(np.sin(angle))\n            columns.append(np.cos(angle))\n        X = np.column_stack(columns)\n\n        # 2. Fit the Poisson GLM\n        beta_hat, mu_hat = fit_poisson_glm(y, X)\n        W_hat = np.diag(mu_hat)\n        \n        # 3. Compute hat matrix diagonals (leverages)\n        fisher_info_final = X.T @ W_hat @ X\n        try:\n            info_inv = np.linalg.inv(fisher_info_final)\n        except np.linalg.LinAlgError:\n            info_inv = np.linalg.pinv(fisher_info_final)\n        \n        # Vectorized calculation of h_t = mu_t * x_t * (X'WX)^-1 * x_t'\n        leverage_component = np.sum((X @ info_inv) * X, axis=1)\n        h_diagonals = mu_hat * leverage_component\n\n        # 4. Calculate residual and signal for the target day t_star\n        idx_star = t_star - 1\n        \n        y_t_star = y[idx_star]\n        mu_t_star_hat = mu_hat[idx_star]\n        h_t_star = h_diagonals[idx_star]\n        \n        # Avoid division by zero or sqrt of negative if h_t_star >= 1\n        denominator_val = mu_t_star_hat * (1 - h_t_star)\n        if denominator_val > 0:\n            denominator = np.sqrt(denominator_val)\n            r_t_star_adj = (y_t_star - mu_t_star_hat) / denominator\n        elif y_t_star == mu_t_star_hat : # Denominator is 0, but so is numerator\n            r_t_star_adj = 0.0\n        else: # Denominator is 0, numerator is not. Residual is infinite.\n            r_t_star_adj = np.inf if (y_t_star > mu_t_star_hat) else -np.inf\n\n        signal = r_t_star_adj > z\n        \n        all_results.append([\n            round(mu_t_star_hat, 6),\n            round(r_t_star_adj, 6),\n            bool(signal)\n        ])\n\n    # Final print statement in the exact required format.\n    output_parts = []\n    for res in all_results:\n        # Format as \"[val1,val2,BoolVal]\" without spaces\n        formatted_part = f\"[{res[0]},{res[1]},{str(res[2])}]\"\n        output_parts.append(formatted_part)\n    \n    final_string = f\"[{','.join(output_parts)}]\"\n    print(final_string)\n\nsolve()\n```"
        }
    ]
}