{
    "hands_on_practices": [
        {
            "introduction": "A core task in epidemiology is to estimate the rate of disease occurrence and quantify the uncertainty around that estimate. This exercise grounds the widely used Poisson model in its fundamental probabilistic assumptions. By working from first principles, you will not only derive the formula for an incidence rate estimator but also its variance and standard error, providing a clear understanding of how we measure the impact of random error on our estimates .",
            "id": "4626594",
            "problem": "A cohort study follows a stable population at risk for a rare, acute outcome. Over a fixed observation window that contributes a total of $T=500$ person-years, the study observes $x=10$ incident cases. Investigators posit that, under conditions of rare events, independence across disjoint time intervals, and a constant underlying hazard within the window, the event-count process can be approximated by a Poisson process with constant incidence rate $\\lambda$ (in cases per person-year). \n\nUsing only fundamental probabilistic properties of the Poisson process and standard definitions in epidemiology, do the following:\n- Provide a justification for modeling the count $X$ of incident cases as Poisson with mean $\\lambda T$ under the stated conditions, thereby motivating the estimator $\\hat{\\lambda}=X/T$ of the rate.\n- From first principles, derive $\\operatorname{Var}(\\hat{\\lambda})$ and the corresponding standard error $\\operatorname{SE}(\\hat{\\lambda})$ in terms of $\\lambda$ and $T$.\n- Then, using the plug-in principle that replaces the unknown $\\lambda$ by its maximum likelihood estimate (MLE) $\\hat{\\lambda}$, compute the numerical values of $\\operatorname{Var}(\\hat{\\lambda})$ and $\\operatorname{SE}(\\hat{\\lambda})$ for the given data.\n\nUse cases per person-year as the unit for rates; the variance has the corresponding squared unit. Round your numerical answers for $\\operatorname{Var}(\\hat{\\lambda})$ and $\\operatorname{SE}(\\hat{\\lambda})$ to four significant figures.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of epidemiology and probability theory, specifically the application of the Poisson process to model rare event counts. The problem is well-posed, objective, and contains all necessary information for a unique solution without any internal contradictions. We may therefore proceed with the solution.\n\nThe problem is divided into three parts: first, a justification of the Poisson model and the rate estimator; second, a first-principles derivation of the estimator's variance and standard error; and third, a numerical calculation based on the provided data.\n\nPart 1: Justification of the Poisson Model and the Estimator $\\hat{\\lambda}$\n\nThe modeling of incident cases using a Poisson process rests on a set of fundamental assumptions about the nature of the event-generating process over the total exposure time, $T$. The total person-time, $T$, can be conceptualized as a long observation interval. We can partition this interval into a very large number, $n$, of small, disjoint subintervals of length $\\Delta t = \\frac{T}{n}$.\n\nThe problem states three key conditions:\n1.  Independence across disjoint time intervals: The occurrence of an event in one subinterval does not affect the probability of an event in any other subinterval.\n2.  Constant underlying hazard: The instantaneous risk of an event, the hazard rate $\\lambda$, is constant throughout the observation window.\n3.  Rare events: The outcome is rare, which implies that the probability of an event in a small subinterval $\\Delta t$ is small.\n\nUnder these conditions, for a sufficiently small $\\Delta t$, the probability of observing exactly one event in that subinterval is approximately proportional to its length, $p = P(\\text{1 event in } \\Delta t) \\approx \\lambda \\Delta t$. The probability of observing more than one event is negligible, i.e., $P(\\text{more than 1 event in } \\Delta t) = o(\\Delta t)$. The probability of observing zero events is therefore $P(\\text{0 events in } \\Delta t) \\approx 1 - \\lambda \\Delta t$.\n\nEach of the $n$ subintervals can be treated as an independent Bernoulli trial, where \"success\" is the occurrence of one case. The total number of cases, $X$, over the entire period $T$ is the sum of the outcomes of these $n$ trials. Therefore, $X$ follows a binomial distribution, $X \\sim \\text{Binomial}(n, p)$, where $n$ is the number of trials and $p \\approx \\lambda \\Delta t$ is the probability of success in each trial.\n\nThe expected number of cases is $E[X] = np = n(\\lambda \\Delta t) = n\\left(\\lambda \\frac{T}{n}\\right) = \\lambda T$.\n\nThe Poisson distribution arises as the limit of the binomial distribution as the number of trials $n$ goes to infinity ($n \\to \\infty$) and the probability of success $p$ goes to zero ($p \\to 0$) in such a way that their product, the mean $np = \\lambda T$, remains constant. In our epidemiological context, this corresponds to making the subintervals $\\Delta t$ infinitesimally small. This limiting process is known as the Poisson limit theorem.\n\nThus, the count of incident cases $X$ over a total person-time $T$ is well-approximated by a Poisson distribution with mean (and parameter) $\\mu = \\lambda T$. The probability mass function for observing $k$ cases is:\n$$\nP(X=k) = \\frac{(\\lambda T)^k e^{-\\lambda T}}{k!}\n$$\nThis justifies modeling the random variable $X$ as $X \\sim \\text{Poisson}(\\lambda T)$.\n\nTo motivate the estimator $\\hat{\\lambda} = \\frac{X}{T}$, we can use the method of maximum likelihood. The likelihood function for observing a particular count $x$ is the probability mass function evaluated at $x$, viewed as a function of the parameter $\\lambda$:\n$$\nL(\\lambda | x) = P(X=x) = \\frac{(\\lambda T)^x e^{-\\lambda T}}{x!}\n$$\nFor maximization, it is more convenient to work with the log-likelihood function, $\\ln L(\\lambda | x)$:\n$$\n\\ln L(\\lambda | x) = \\ln\\left(\\frac{(\\lambda T)^x e^{-\\lambda T}}{x!}\\right) = x \\ln(\\lambda T) - \\lambda T - \\ln(x!) = x \\ln(\\lambda) + x \\ln(T) - \\lambda T - \\ln(x!)\n$$\nTo find the value of $\\lambda$ that maximizes this function, we take the first derivative with respect to $\\lambda$ and set it to zero:\n$$\n\\frac{d}{d\\lambda} \\ln L(\\lambda | x) = \\frac{x}{\\lambda} - T = 0\n$$\nSolving for $\\lambda$ yields the maximum likelihood estimator (MLE), denoted $\\hat{\\lambda}$:\n$$\n\\hat{\\lambda} = \\frac{x}{T}\n$$\nReplacing the observed value $x$ with the random variable $X$ gives the estimator as a function of the data, $\\hat{\\lambda} = \\frac{X}{T}$. This estimator is intuitive, representing the observed frequency of events per unit of exposure time.\n\nPart 2: Derivation of $\\operatorname{Var}(\\hat{\\lambda})$ and $\\operatorname{SE}(\\hat{\\lambda})$\n\nThe estimator for the incidence rate is $\\hat{\\lambda} = \\frac{X}{T}$. To derive its variance, we use the properties of the variance operator. Since $T$ is a fixed, non-random quantity (total person-years), we have:\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\operatorname{Var}\\left(\\frac{X}{T}\\right)\n$$\nUsing the variance property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ for a constant $a$ and random variable $Y$, we get:\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\left(\\frac{1}{T}\\right)^2 \\operatorname{Var}(X) = \\frac{1}{T^2} \\operatorname{Var}(X)\n$$\nA fundamental property of the Poisson distribution is that its variance is equal to its mean. Since $X \\sim \\text{Poisson}(\\lambda T)$, its mean is $E[X] = \\lambda T$ and its variance is $\\operatorname{Var}(X) = \\lambda T$. Substituting this into the equation for $\\operatorname{Var}(\\hat{\\lambda})$:\n$$\n\\operatorname{Var}(\\hat{\\lambda}) = \\frac{1}{T^2}(\\lambda T) = \\frac{\\lambda}{T}\n$$\nThe standard error of an estimator, $\\operatorname{SE}(\\hat{\\lambda})$, is defined as the square root of its variance:\n$$\n\\operatorname{SE}(\\hat{\\lambda}) = \\sqrt{\\operatorname{Var}(\\hat{\\lambda})} = \\sqrt{\\frac{\\lambda}{T}}\n$$\nThese expressions for the variance and standard error depend on the true, unknown rate $\\lambda$.\n\nPart 3: Numerical Computation\n\nThe problem provides the following data: total person-years $T = 500$ and observed number of cases $x = 10$.\nFirst, we compute the point estimate of the incidence rate $\\lambda$ using the MLE formula:\n$$\n\\hat{\\lambda} = \\frac{x}{T} = \\frac{10}{500} = \\frac{1}{50} = 0.02 \\text{ cases per person-year}\n$$\nTo compute a numerical value for the variance and standard error, we use the \"plug-in principle\" as instructed. This involves substituting the unknown parameter $\\lambda$ with its estimate $\\hat{\\lambda}$ in the derived formulas. This gives us the *estimated* variance and *estimated* standard error.\n\nThe estimated variance, denoted $\\widehat{\\operatorname{Var}}(\\hat{\\lambda})$, is:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\lambda}) = \\frac{\\hat{\\lambda}}{T} = \\frac{x/T}{T} = \\frac{x}{T^2}\n$$\nSubstituting the given numerical values:\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\lambda}) = \\frac{10}{500^2} = \\frac{10}{250000} = \\frac{1}{25000} = 0.00004\n$$\nThe units are (cases per person-year)$^2$. Rounding to four significant figures, this is $0.00004000$.\n\nThe estimated standard error, denoted $\\widehat{\\operatorname{SE}}(\\hat{\\lambda})$, is the square root of the estimated variance:\n$$\n\\widehat{\\operatorname{SE}}(\\hat{\\lambda}) = \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\lambda})} = \\sqrt{\\frac{x}{T^2}} = \\frac{\\sqrt{x}}{T}\n$$\nSubstituting the numerical values:\n$$\n\\widehat{\\operatorname{SE}}(\\hat{\\lambda}) = \\frac{\\sqrt{10}}{500} \\approx \\frac{3.16227766}{500} \\approx 0.006324555\n$$\nRounding this result to four significant figures, we get $0.006325$. The units are cases per person-year.\n\nThe final numerical results for the estimated variance and standard error of $\\hat{\\lambda}$ are $0.00004000$ and $0.006325$, respectively.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 4.000 \\times 10^{-5} & 6.325 \\times 10^{-3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While large-sample statistical methods are powerful, their validity hinges on certain assumptions that are not always met in practice, especially in studies with small sample sizes or rare outcomes. This problem presents a common scenario where asymptotic approximations fail, forcing a critical evaluation of standard tools. You will learn to identify these situations and justify the use of an exact alternative, ensuring robust and valid statistical inference even when data is sparse .",
            "id": "4626605",
            "problem": "A small prospective cohort study recruited two groups with fixed sizes: an exposed group of $n_{E}=5$ individuals and an unexposed group of $n_{U}=5$ individuals, for a total of $n=10$. After follow-up, the observed $2\\times 2$ table of disease status by exposure was:\n- Exposed: cases $0$, non-cases $5$.\n- Unexposed: cases $4$, non-cases $1$.\n\nInvestigators wish to test the null hypothesis of no association between exposure and disease. From first principles, argue whether asymptotic normal approximations are appropriate in this setting and, if not, identify an exact alternative and the rationale for its validity. Choose the single best option.\n\nA. Asymptotic normality of common estimators (for example, the difference in proportions or the log odds ratio) relies on conditions such as $n p (1-p)$ being sufficiently large in each group. Here, cell counts are very small and one cell is $0$, placing the estimator at a boundary where the sampling distribution is highly discrete and skewed, violating normal approximation assumptions. An exact test that conditions on the fixed margins—Fisher’s exact test—is appropriate because under the null and fixed margins the cell count follows a hypergeometric distribution, yielding exact tail probabilities and correct type I error control.\n\nB. Asymptotic normal approximations are acceptable as long as the total sample size satisfies $n\\ge 10$, so a large-sample $z$-test for the difference in proportions is preferred. Exact tests are unnecessarily conservative in this setting.\n\nC. Adding $0.5$ to all cells to avoid zeros, then using a Wald test for the log odds ratio, guarantees nominal coverage and valid type I error even with very small counts, so an exact alternative is not needed.\n\nD. Because the exposed group has $0$ cases, it contributes no information. A one-sample exact binomial test on the unexposed group’s case count alone (ignoring the exposed group) is valid and more powerful than conditioning on margins as in Fisher’s exact test.",
            "solution": "The user wants a critical analysis of a statistical problem in epidemiology, focusing on the choice between asymptotic and exact hypothesis tests.\n\n### Step 1: Extract Givens\n- Study type: Prospective cohort study.\n- Group sizes (fixed): Exposed group, $n_{E}=5$; Unexposed group, $n_{U}=5$.\n- Total sample size: $n=10$.\n- Observed data in a $2\\times 2$ table:\n  - Exposed group: 0 cases, 5 non-cases.\n  - Unexposed group: 4 cases, 1 non-case.\n- Hypothesis to test: Null hypothesis of no association between exposure and disease.\n- Question: Evaluate the appropriateness of asymptotic normal approximations and identify a valid exact alternative with its rationale.\n\nThe $2\\times 2$ contingency table is as follows:\n$$\n\\begin{array}{c|cc|c}\n & \\text{Cases} & \\text{Non-cases} & \\text{Total} \\\\\n\\hline\n\\text{Exposed} & a = 0 & b = 5 & n_E = 5 \\\\\n\\text{Unexposed} & c = 4 & d = 1 & n_U = 5 \\\\\n\\hline\n\\text{Total} & m_1 = 4 & m_2 = 6 & N = 10\n\\end{array}\n$$\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a standard scenario in biostatistics and epidemiology.\n- **Scientifically Grounded:** The setup of a prospective cohort study, the construction of a $2\\times 2$ table, and the testing of the null hypothesis of no association are fundamental concepts in epidemiology. The distinction between asymptotic and exact tests is a critical topic in statistical inference. The problem is based on established statistical principles.\n- **Well-Posed:** The problem provides a complete dataset ($n_E=5, n_U=5$, and the number of cases in each group) and poses a clear, answerable question regarding the choice of statistical methodology. A unique, correct answer can be derived from first principles of statistics.\n- **Objective:** The problem is stated in objective, quantitative terms. It is free from ambiguity and subjective claims.\n- **Other Flaws:** The problem is not incomplete, contradictory, unrealistic (small pilot studies are common), ill-posed, or trivial. It presents a classic example where large-sample theory fails, which is a core concept.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The solution process will proceed.\n\n### Principle-Based Derivation and Solution\n\nThe primary task is to assess the validity of statistical tests that rely on asymptotic normal approximations for the given data. These tests include the Pearson chi-squared test and z-tests for the difference in proportions or the log odds ratio.\n\n**1. Appropriateness of Asymptotic Normal Approximations**\n\nThe validity of these large-sample tests depends on the Central Limit Theorem, which states that the sampling distribution of an estimator (like a sample proportion or a log odds ratio) approaches a normal distribution as the sample size grows. For this approximation to be reliable, the sample size must be \"sufficiently large.\"\n\nIn the context of contingency tables, a common rule of thumb for the Pearson chi-squared test is that all expected cell counts should be at least 5. Let's calculate the expected cell counts under the null hypothesis ($H_0$) of no association. Under $H_0$, the probability of disease is the same in both groups. The overall proportion of cases is $\\hat{p} = \\frac{\\text{Total Cases}}{\\text{Total N}} = \\frac{4}{10} = 0.4$.\n\nThe expected counts ($E_{ij}$) are:\n- Expected Cases, Exposed: $E_{11} = n_E \\times \\hat{p} = 5 \\times 0.4 = 2$.\n- Expected Non-cases, Exposed: $E_{12} = n_E \\times (1-\\hat{p}) = 5 \\times 0.6 = 3$.\n- Expected Cases, Unexposed: $E_{21} = n_U \\times \\hat{p} = 5 \\times 0.4 = 2$.\n- Expected Non-cases, Unexposed: $E_{22} = n_U \\times (1-\\hat{p}) = 5 \\times 0.6 = 3$.\n\nAll expected cell counts (2 and 3) are less than the conventional threshold of 5. This strongly indicates that the chi-squared distribution, and by extension the normal distribution, will be a poor approximation to the true sampling distribution of the test statistic.\n\nFurthermore, the data is sparse, with an observed count of 0 in the exposed cases cell. This creates a boundary condition. For estimators like the log odds ratio, $\\ln(\\text{OR}) = \\ln(\\frac{ad}{bc})$, the point estimate is undefined because $\\ln(\\frac{0 \\times 1}{5 \\times 4}) = \\ln(0)$. Any test based on such an estimator and its assumed normality will be problematic. The true sampling distribution in such a case is highly discrete and skewed, which is fundamentally different from the continuous, symmetric normal distribution.\n\nTherefore, from first principles, asymptotic normal approximations are not appropriate for this dataset.\n\n**2. Identification of an Exact Alternative**\n\nWhen asymptotic methods are invalid, an \"exact\" test is required. An exact test calculates the p-value by summing the probabilities of the observed outcome and all more extreme outcomes, based on the exact discrete probability distribution of the data under the null hypothesis. This ensures that the Type I error rate is controlled at or below the nominal significance level $\\alpha$.\n\nFor a $2\\times 2$ table, the standard exact test is **Fisher's exact test**.\n\n**Rationale for Fisher's Exact Test:**\nThe test is derived by conditioning on the marginal totals of the table being fixed. In this prospective cohort study, the row totals ($n_E=5$ and $n_U=5$) were fixed by design. Fisher's test adds the condition that the column totals (total cases $m_1=4$ and total non-cases $m_2=6$) are also considered fixed for the purpose of inference.\n\nUnder the null hypothesis of no association between the exposure and the disease, and given these fixed margins, the probability of observing any specific arrangement of cell counts ($a, b, c, d$) is given by the **hypergeometric distribution**:\n$$P(a | n_E, n_U, m_1, m_2) = \\frac{\\binom{n_E}{a} \\binom{n_U}{m_1-a}}{\\binom{N}{m_1}}$$\nwhere $a$ is the number of cases in the exposed group.\n\nFor the observed table, the probability is:\n$$P(a=0) = \\frac{\\binom{5}{0} \\binom{5}{4}}{\\binom{10}{4}} = \\frac{1 \\times 5}{\\frac{10 \\cdot 9 \\cdot 8 \\cdot 7}{4 \\cdot 3 \\cdot 2 \\cdot 1}} = \\frac{5}{210} \\approx 0.0238$$\n\nThe p-value is then calculated by summing the probabilities of all tables that are as extreme as or more extreme than the observed table. Since $a=0$ is the most extreme result possible in the direction of a protective effect (or lack of a risk effect), the one-sided p-value is simply $P(a=0) \\approx 0.0238$.\n\nThis procedure does not rely on any large-sample approximations. It provides an exact probability, ensuring valid inference and correct Type I error control, which is precisely what is needed in situations with small sample sizes and sparse data.\n\n### Option-by-Option Analysis\n\n**A. Asymptotic normality of common estimators (for example, the difference in proportions or the log odds ratio) relies on conditions such as $n p (1-p)$ being sufficiently large in each group. Here, cell counts are very small and one cell is 0, placing the estimator at a boundary where the sampling distribution is highly discrete and skewed, violating normal approximation assumptions. An exact test that conditions on the fixed margins—Fisher’s exact test—is appropriate because under the null and fixed margins the cell count follows a hypergeometric distribution, yielding exact tail probabilities and correct type I error control.**\n- This option correctly identifies the failure of large-sample assumptions, citing both the small counts and the zero cell causing a boundary problem with a skewed, discrete distribution. It correctly identifies Fisher's exact test as the appropriate alternative. Crucially, it provides the correct theoretical justification: conditioning on the margins, the cell count follows a hypergeometric distribution under the null hypothesis, which allows for exact probability calculations and proper Type I error control. This aligns perfectly with our derivation.\n- **Verdict: Correct**\n\n**B. Asymptotic normal approximations are acceptable as long as the total sample size satisfies $n\\ge 10$, so a large-sample $z$-test for the difference in proportions is preferred. Exact tests are unnecessarily conservative in this setting.**\n- This statement is incorrect. A simple threshold on the total sample size like $n \\ge 10$ is not a sufficient condition for the validity of asymptotic approximations. The distribution of data within the cells is paramount. The current problem, with $n=10$, is a textbook counterexample. The claim that exact tests are \"unnecessarily conservative\" is misleading; in this context, the alternative z-test is invalid and would have an uncontrolled Type I error rate. The conservatism of the exact test is a far lesser issue than the invalidity of the asymptotic test.\n- **Verdict: Incorrect**\n\n**C. Adding 0.5 to all cells to avoid zeros, then using a Wald test for the log odds ratio, guarantees nominal coverage and valid type I error even with very small counts, so an exact alternative is not needed.**\n- Adding a continuity correction like 0.5 (a form of pseudo-count) is a technique to handle zero cells and sometimes improve the performance of asymptotic methods. However, it does not \"guarantee\" nominal coverage or valid Type I error, especially in a case with such extremely small sample sizes. It is still an approximation, and its performance can be poor. The claim that an exact test is \"not needed\" is false; the exact test is the gold-standard method for ensuring validity in this scenario.\n- **Verdict: Incorrect**\n\n**D. Because the exposed group has 0 cases, it contributes no information. A one-sample exact binomial test on the unexposed group’s case count alone (ignoring the exposed group) is valid and more powerful than conditioning on margins as in Fisher’s exact test.**\n- This option is fundamentally flawed. The observation of 0 cases out of 5 in the exposed group is highly relevant information, especially when contrasted with the 4 cases out of 5 in the unexposed group. Discarding the exposed group's data is scientifically and statistically indefensible. A one-sample test on the unexposed group cannot test for an *association* between exposure and outcome; it can only test a hypothesis about the baseline incidence in that single group. The entire purpose of the study is comparison, which this approach abandons.\n- **Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Random error does more than just widen confidence intervals; it can also produce systematic artifacts in data that may be misinterpreted as real effects. This practice explores the counterintuitive phenomenon of \"regression to the mean,\" which often occurs in screening programs or studies that select subjects based on measurements subject to error. Understanding this concept is crucial for correctly interpreting changes in follow-up measurements and avoiding erroneous conclusions about intervention effectiveness .",
            "id": "4626616",
            "problem": "A hypertension screening program recruits individuals using a single screening measurement of systolic blood pressure (SBP). Let the observed screening SBP be denoted by $X$, the unobserved stable true SBP by $T$, and the follow-up SBP measured one month later by $Y$. Assume a classical measurement error model with $X = T + E$ and $Y = T + E'$, where $E$ and $E'$ are independent, mean-zero errors that are also independent of $T$. In the target population, the screening measurements $X$ are approximately normally distributed with mean $\\mu_{X} = 130$ mmHg and standard deviation $\\sigma_{X} = 20$ mmHg. The single-occasion reliability of SBP is $r = 0.6$, defined as $r = \\mathrm{Var}(T)/\\mathrm{Var}(X)$. The cohort is selected if the screening measurement satisfies $X \\ge c$ with $c = 160$ mmHg. Assume there is no systematic change in true SBP between screening and follow-up, so that the expected true SBP is stable over time in the absence of intervention.\n\nUsing only the core definitions of reliability and properties of conditional expectations for the normal distribution, derive an expression for $\\mathbb{E}[Y \\mid X \\ge c]$ in terms of $r$, $\\mu_{X}$, $\\sigma_{X}$, and $c$, and then compute its numerical value for the parameters given above. Your final numeric answer must be rounded to $4$ significant figures and expressed in mmHg.",
            "solution": "We begin from the classical measurement error model and the definition of reliability. Let $T$ denote the stable true systolic blood pressure (SBP) with mean $\\mu_{T}$ and variance $\\sigma_{T}^{2}$, and let $E$ and $E'$ be independent, mean-zero errors with variances $\\sigma_{E}^{2}$ and $\\sigma_{E'}^{2}$, respectively, independent of $T$. The screening and follow-up measurements are $X = T + E$ and $Y = T + E'$. The reliability of a single screening measurement is defined as\n$$\nr \\equiv \\frac{\\mathrm{Var}(T)}{\\mathrm{Var}(X)} = \\frac{\\sigma_{T}^{2}}{\\sigma_{T}^{2} + \\sigma_{E}^{2}}.\n$$\nBecause $E$ has mean zero and is independent of $T$, the screening mean satisfies $\\mu_{X} = \\mathbb{E}[X] = \\mathbb{E}[T] + \\mathbb{E}[E] = \\mu_{T}$, so $\\mu_{T} = \\mu_{X}$.\n\nNext, we derive $\\mathbb{E}[T \\mid X]$. Since $X = T + E$ with $T$ independent of $E$ and $(T, X)$ jointly normal, the conditional expectation is linear with slope equal to the regression coefficient\n$$\n\\beta_{TX} = \\frac{\\mathrm{Cov}(T, X)}{\\mathrm{Var}(X)} = \\frac{\\mathrm{Var}(T)}{\\mathrm{Var}(X)} = r,\n$$\nand intercept $\\mu_{T} - r \\mu_{X} = \\mu_{X} - r \\mu_{X} = (1 - r)\\mu_{X}$. Therefore,\n$$\n\\mathbb{E}[T \\mid X = x] = \\mu_{T} + r (x - \\mu_{X}) = \\mu_{X} + r (x - \\mu_{X}).\n$$\nBecause $Y = T + E'$ with $\\mathbb{E}[E' \\mid X] = 0$ and $E'$ independent of $X$, we have\n$$\n\\mathbb{E}[Y \\mid X] = \\mathbb{E}[T \\mid X] + \\mathbb{E}[E' \\mid X] = \\mu_{X} + r (X - \\mu_{X}).\n$$\nTaking the conditional expectation over the selected set $\\{X \\ge c\\}$ yields\n$$\n\\mathbb{E}[Y \\mid X \\ge c] = \\mathbb{E}\\big[\\mathbb{E}[Y \\mid X] \\,\\big|\\, X \\ge c\\big] = \\mu_{X} + r\\big(\\mathbb{E}[X \\mid X \\ge c] - \\mu_{X}\\big).\n$$\nThus, the problem reduces to computing $\\mathbb{E}[X \\mid X \\ge c]$ for a normally distributed $X \\sim \\mathcal{N}(\\mu_{X}, \\sigma_{X}^{2})$. Let $Z = \\frac{X - \\mu_{X}}{\\sigma_{X}} \\sim \\mathcal{N}(0, 1)$ and $\\alpha = \\frac{c - \\mu_{X}}{\\sigma_{X}}$. Using the definition of conditional expectation for a truncated normal,\n$$\n\\mathbb{E}[Z \\mid Z \\ge \\alpha] = \\frac{\\int_{\\alpha}^{\\infty} z \\,\\varphi(z) \\, dz}{\\int_{\\alpha}^{\\infty} \\varphi(z)\\, dz} = \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)},\n$$\nwhere $\\varphi$ and $\\Phi$ denote the standard normal probability density function (PDF) and cumulative distribution function (CDF), respectively, with $\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right)$. Therefore,\n$$\n\\mathbb{E}[X \\mid X \\ge c] = \\mu_{X} + \\sigma_{X} \\cdot \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)}, \\quad \\alpha = \\frac{c - \\mu_{X}}{\\sigma_{X}}.\n$$\nSubstituting this into the expression for $\\mathbb{E}[Y \\mid X \\ge c]$ gives\n$$\n\\mathbb{E}[Y \\mid X \\ge c] = \\mu_{X} + r\\left(\\mu_{X} + \\sigma_{X} \\cdot \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)} - \\mu_{X}\\right) = \\mu_{X} + r \\sigma_{X} \\cdot \\frac{\\varphi(\\alpha)}{1 - \\Phi(\\alpha)}.\n$$\nNow plug in the given values $\\mu_{X} = 130$, $\\sigma_{X} = 20$, $c = 160$, and $r = 0.6$ (all numerical values in mmHg except $r$ which is unitless). First compute\n$$\n\\alpha = \\frac{c - \\mu_{X}}{\\sigma_{X}} = \\frac{160 - 130}{20} = 1.5.\n$$\nEvaluate the standard normal PDF and tail probability at $\\alpha = 1.5$:\n$$\n\\varphi(1.5) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{1.5^{2}}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-1.125) \\approx 0.1295175957,\n$$\n$$\n1 - \\Phi(1.5) \\approx 0.0668072013.\n$$\nHence,\n$$\n\\frac{\\varphi(1.5)}{1 - \\Phi(1.5)} \\approx \\frac{0.1295175957}{0.0668072013} \\approx 1.938537037.\n$$\nThen,\n$$\n\\mathbb{E}[X \\mid X \\ge 160] \\approx 130 + 20 \\times 1.938537037 \\approx 130 + 38.77074074 \\approx 168.7707407,\n$$\nand finally\n$$\n\\mathbb{E}[Y \\mid X \\ge 160] \\approx 130 + 0.6 \\times (168.7707407 - 130) = 130 + 0.6 \\times 38.7707407 \\approx 130 + 23.2624444 \\approx 153.2624444.\n$$\nRounded to $4$ significant figures and expressed in mmHg, the expected follow-up mean is $153.3$ mmHg.",
            "answer": "$$\\boxed{153.3}$$"
        }
    ]
}