{
    "hands_on_practices": [
        {
            "introduction": "To truly understand generalized linear models, it's best to start with the simplest possible case. For a binary outcome, this is the intercept-only logistic regression model, which essentially models the overall probability of the outcome in the study population. This foundational exercise  will guide you through deriving the model's single parameter, $\\beta_0$, from first principles, revealing its direct interpretation as the sample log-odds of the outcome.",
            "id": "4595227",
            "problem": "A cross-sectional study recruits $n=100$ individuals from a community and records a binary infection outcome for each person, where $Y_{i}=1$ indicates an infected individual and $Y_{i}=0$ otherwise. The outcomes are assumed independent and follow a Bernoulli distribution with common probability of infection $p$. Investigators fit an intercept-only generalized linear model (GLM) using the Bernoulli distribution and the logit link, so that the linear predictor is $\\eta=\\beta_{0}$ and the mean is $p=\\Pr(Y_{i}=1)$ with $g(p)=\\ln\\!\\big(p/(1-p)\\big)$. The data show $y=30$ infections out of $n=100$.\n\nStarting from the independence assumption and the Bernoulli likelihood, derive the maximum likelihood estimator for $p$, and then use the link function to obtain the maximum likelihood estimator $\\hat{\\beta}_{0}$. Provide an epidemiological interpretation of $\\hat{\\beta}_{0}$ in terms of log-odds. Give your final answer for $\\hat{\\beta}_{0}$ as an exact analytic expression using natural logarithms. Do not round.",
            "solution": "The problem asks for the maximum likelihood estimator (MLE) for the intercept $\\beta_0$ in an intercept-only logistic regression model, and its epidemiological interpretation.\n\n### 1. Derive the MLE for the Probability `p`\n\nThe data consists of $n=100$ independent Bernoulli trials $Y_i$, where $y=30$ of them are successes ($Y_i=1$). The total number of successes, $\\sum Y_i = y$, follows a binomial distribution with probability mass function (likelihood):\n$$ L(p; y) = \\binom{n}{y} p^y (1-p)^{n-y} $$\nTo find the MLE for $p$, we maximize this likelihood. It is easier to maximize the log-likelihood function, $\\ell(p) = \\ln L(p)$:\n$$ \\ell(p) = \\ln\\binom{n}{y} + y\\ln(p) + (n-y)\\ln(1-p) $$\nWe take the derivative with respect to $p$ and set it to zero:\n$$ \\frac{d\\ell}{dp} = \\frac{y}{p} - \\frac{n-y}{1-p} = 0 $$\nSolving for $p$:\n$$ \\frac{y}{p} = \\frac{n-y}{1-p} \\implies y(1-p) = p(n-y) \\implies y - yp = np - yp \\implies y = np $$\nThus, the MLE for $p$ is:\n$$ \\hat{p} = \\frac{y}{n} $$\nFor the given data, $y=30$ and $n=100$, so the MLE of the probability of infection is:\n$$ \\hat{p} = \\frac{30}{100} = 0.3 $$\n\n### 2. Derive the MLE for `β₀`\n\nThe model specifies a logit link function, which connects the mean of the outcome, $p$, to the linear predictor, $\\eta = \\beta_0$:\n$$ g(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 $$\nThe quantity $\\frac{p}{1-p}$ is the odds of the outcome. Therefore, $\\beta_0$ represents the log-odds of the outcome.\n\nDue to the invariance property of maximum likelihood estimators, the MLE of a function of a parameter is that same function applied to the MLE of the parameter. Therefore, the MLE for $\\beta_0$, denoted $\\hat{\\beta}_0$, is:\n$$ \\hat{\\beta}_0 = g(\\hat{p}) = \\ln\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right) $$\nSubstituting the value of $\\hat{p} = 0.3$:\n$$ \\hat{\\beta}_0 = \\ln\\left(\\frac{0.3}{1-0.3}\\right) = \\ln\\left(\\frac{0.3}{0.7}\\right) = \\ln\\left(\\frac{3}{7}\\right) $$\nThis is the exact analytic expression for the MLE of the intercept.\n\n### 3. Epidemiological Interpretation\n\nThe parameter $\\beta_0$ is the log-odds of infection. The MLE $\\hat{\\beta}_0 = \\ln(3/7)$ is the estimated log-odds of infection in the sampled community, based on the observed data. It represents the logarithm of the ratio of the probability of being infected to the probability of not being infected.",
            "answer": "$$\n\\boxed{\\ln\\left(\\frac{3}{7}\\right)}\n$$"
        },
        {
            "introduction": "Building upon the intercept-only model, the real power of logistic regression emerges when we add predictor variables to assess associations. A key task in epidemiology is to quantify the strength of these associations using measures like the odds ratio and risk ratio. This practice  demonstrates how to calculate and, crucially, distinguish between these two important measures from the coefficients of a fitted logistic model, a skill essential for correctly interpreting and communicating research findings.",
            "id": "4595204",
            "problem": "A cohort study investigates the association between a binary exposure $X \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$. The outcome is modeled with a generalized linear model (GLM) with a logit link, so that for each $x \\in \\{0,1\\}$ the risk satisfies $\\operatorname{logit}(p(x))=\\beta_{0}+\\beta_{1} x$, where $p(x)=\\Pr(Y=1 \\mid X=x)$ and $\\operatorname{logit}(p)=\\ln\\!\\big(p/(1-p)\\big)$. Suppose that when unexposed the baseline risk is $p(0)=p_{0}=0.2$, and that the fitted logit coefficient for the exposure is $\\beta_{1}=0.8$. Using only the definitions of odds, odds ratio, and the inverse-logit transformation, compute:\n- the exposure odds ratio comparing $X=1$ to $X=0$ under this model, and\n- the risk ratio comparing $p(1)$ to $p(0)$ under this model.\n\nReport both numerical values rounded to four significant figures. Express your final answer as a row matrix in the order (odds ratio, risk ratio). No units are required.",
            "solution": "The problem requires the computation of the odds ratio and the risk ratio from a given logistic regression model for a binary exposure and a binary outcome. The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution.\n\nThe model for the risk, $p(x)=\\Pr(Y=1 \\mid X=x)$, is given by a generalized linear model with a logit link function:\n$$ \\operatorname{logit}(p(x)) = \\beta_{0}+\\beta_{1} x $$\nwhere $x \\in \\{0,1\\}$. The logit function is defined as $\\operatorname{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$. The given parameters are the baseline risk $p(0) = 0.2$ and the logit coefficient for the exposure, $\\beta_{1} = 0.8$.\n\n### Part 1: Calculation of the Odds Ratio (OR)\n\nThe odds of an event for a given risk $p$ are defined as the ratio of the probability of the event occurring to the probability of it not occurring. For an exposure level $x$, the odds are:\n$$ O(x) = \\frac{p(x)}{1-p(x)} $$\nThe odds ratio ($OR$) compares the odds of the outcome in the exposed group ($X=1$) to the odds in the unexposed group ($X=0$):\n$$ OR = \\frac{O(1)}{O(0)} = \\frac{p(1)/(1-p(1))}{p(0)/(1-p(0))} $$\nFrom the definition of the logit function, we can see that $\\operatorname{logit}(p(x))$ is the natural logarithm of the odds, $O(x)$:\n$$ \\operatorname{logit}(p(x)) = \\ln\\left(\\frac{p(x)}{1-p(x)}\\right) = \\ln(O(x)) $$\nThe given logistic model can therefore be written in terms of the log-odds:\n$$ \\ln(O(x)) = \\beta_{0}+\\beta_{1} x $$\nFor the exposed group ($x=1$), the log-odds are:\n$$ \\ln(O(1)) = \\beta_{0}+\\beta_{1}(1) = \\beta_{0}+\\beta_{1} $$\nFor the unexposed group ($x=0$), the log-odds are:\n$$ \\ln(O(0)) = \\beta_{0}+\\beta_{1}(0) = \\beta_{0} $$\nThe natural logarithm of the odds ratio can be found by taking the difference between the log-odds of the two groups:\n$$ \\ln(OR) = \\ln\\left(\\frac{O(1)}{O(0)}\\right) = \\ln(O(1)) - \\ln(O(0)) $$\nSubstituting the expressions from the model:\n$$ \\ln(OR) = (\\beta_{0}+\\beta_{1}) - \\beta_{0} = \\beta_{1} $$\nThis demonstrates a fundamental property of logistic regression: the coefficient $\\beta_1$ for a binary exposure variable is the log-odds ratio. To find the odds ratio itself, we exponentiate the coefficient $\\beta_1$:\n$$ OR = \\exp(\\beta_{1}) $$\nGiven that $\\beta_{1} = 0.8$, the odds ratio is:\n$$ OR = \\exp(0.8) \\approx 2.2255409 $$\nRounding to four significant figures, we get:\n$$ OR \\approx 2.226 $$\n\n### Part 2: Calculation of the Risk Ratio (RR)\n\nThe risk ratio ($RR$), also known as the relative risk, is the ratio of the risk of the outcome in the exposed group to the risk in the unexposed group:\n$$ RR = \\frac{p(1)}{p(0)} $$\nWe are given the baseline risk $p(0) = 0.2$. To compute the risk ratio, we must first determine the risk in the exposed group, $p(1)$.\n\nWe can find the value of the intercept parameter $\\beta_0$ using the given baseline risk $p(0)$. As shown before, $\\ln(O(0)) = \\beta_0$.\n$$ \\beta_{0} = \\ln(O(0)) = \\ln\\left(\\frac{p(0)}{1-p(0)}\\right) = \\ln\\left(\\frac{0.2}{1-0.2}\\right) = \\ln\\left(\\frac{0.2}{0.8}\\right) = \\ln(0.25) $$\nNow, we can find the log-odds for the exposed group, $\\ln(O(1))$:\n$$ \\ln(O(1)) = \\beta_{0} + \\beta_{1} = \\ln(0.25) + 0.8 $$\nTo find the risk $p(1)$ from the log-odds $\\ln(O(1))$, we use the inverse-logit transformation, which is requested by the problem statement. If $\\eta = \\operatorname{logit}(p) = \\ln(p/(1-p))$, then solving for $p$ gives $p = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}$.\nLet $\\eta_1 = \\ln(O(1)) = \\ln(0.25) + 0.8$. The risk $p(1)$ is:\n$$ p(1) = \\frac{\\exp(\\eta_1)}{1+\\exp(\\eta_1)} = \\frac{\\exp(\\ln(0.25) + 0.8)}{1+\\exp(\\ln(0.25) + 0.8)} $$\nUsing the property $\\exp(a+b) = \\exp(a)\\exp(b)$, we have:\n$$ p(1) = \\frac{\\exp(\\ln(0.25)) \\cdot \\exp(0.8)}{1 + \\exp(\\ln(0.25)) \\cdot \\exp(0.8)} = \\frac{0.25 \\cdot \\exp(0.8)}{1 + 0.25 \\cdot \\exp(0.8)} $$\nSubstituting the numerical value $\\exp(0.8) \\approx 2.2255409$:\n$$ p(1) \\approx \\frac{0.25 \\times 2.2255409}{1 + 0.25 \\times 2.2255409} = \\frac{0.5563852}{1.5563852} \\approx 0.3574883 $$\nNow we can compute the risk ratio:\n$$ RR = \\frac{p(1)}{p(0)} \\approx \\frac{0.3574883}{0.2} \\approx 1.7874415 $$\nRounding to four significant figures, we get:\n$$ RR \\approx 1.787 $$\nThe two requested values are the odds ratio, approximately $2.226$, and the risk ratio, approximately $1.787$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 2.226 & 1.787 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While we can interpret the parameters of a fitted model, how does a computer actually find the optimal values for the coefficients? This exercise  demystifies the 'black box' of the model-fitting process by taking you under the hood. You will perform the first computational step of the Iteratively Reweighted Least Squares (IRLS) algorithm, the standard method for fitting GLMs, by calculating the initial 'working responses' and 'weights' for a simple dataset.",
            "id": "4595198",
            "problem": "An epidemiologist is fitting a generalized linear model for a binary disease outcome using a logistic mean-variance structure. Consider independent Bernoulli outcomes $Y_i \\in \\{0,1\\}$ with a single standardized predictor $x_i$ and the canonical logit link. The model is specified without an intercept as $g(\\mu_i) = \\eta_i = \\beta x_i$, where $g(\\mu) = \\ln\\!\\big(\\mu/(1-\\mu)\\big)$, $\\mu_i = \\mathbb{E}[Y_i \\mid x_i]$, and $\\beta \\in \\mathbb{R}$ is the coefficient to be estimated. The two observations are $(x_1 = 1, y_1 = 1)$ and $(x_2 = -1, y_2 = 0)$. Using the core definitions of generalized linear models, the Bernoulli likelihood, the canonical link, and Fisher scoring, derive the working response $z$ and the diagonal weights $w$ used by Iteratively Reweighted Least Squares (IRLS) for the first iteration starting from $\\beta^{(0)} = 0$. Then compute their numerical values for these data at $\\beta^{(0)}$.\n\nProvide the final result as a single row matrix containing, in order, $z_1$, $z_2$, $w_1$, $w_2$. No rounding is required.",
            "solution": "The problem asks for the components of the first iteration of the Iteratively Reweighted Least Squares (IRLS) algorithm for a simple logistic regression model. Specifically, we need to find the working responses $z_1, z_2$ and the weights $w_1, w_2$ at the starting value $\\beta^{(0)} = 0$.\n\n### 1. General Formulas for IRLS\n\nFor a Generalized Linear Model, the IRLS algorithm iteratively solves a weighted least squares problem to find the maximum likelihood estimates of the coefficients $\\boldsymbol{\\beta}$. The update at each step involves a working response vector $\\mathbf{z}$ and a diagonal weight matrix $\\mathbf{W}$. The components for each observation $i$ are defined as:\n- **Working response:** $z_i = \\eta_i + (y_i - \\mu_i) \\frac{d\\eta_i}{d\\mu_i}$\n- **Weight:** $w_i = \\left[ \\left(\\frac{d\\eta_i}{d\\mu_i}\\right)^2 \\text{Var}(Y_i) \\right]^{-1}$\n\nwhere $\\eta_i$ is the linear predictor, $\\mu_i$ is the mean of $Y_i$, and $y_i$ is the observed outcome.\n\n### 2. Specialization for the Logistic Model (Canonical Link)\n\nFor a logistic model, the outcome $Y_i$ follows a Bernoulli distribution, and the link function is the canonical logit link.\n- **Distribution:** $Y_i \\sim \\text{Bernoulli}(\\mu_i)$\n- **Mean:** $\\mathbb{E}[Y_i] = \\mu_i$\n- **Variance:** $\\text{Var}(Y_i) = \\mu_i(1-\\mu_i)$\n- **Link function:** $\\eta_i = g(\\mu_i) = \\ln\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)$\n\nWe need the derivative of the link function with respect to the mean:\n$$ \\frac{d\\eta_i}{d\\mu_i} = \\frac{d}{d\\mu_i} \\left[ \\ln(\\mu_i) - \\ln(1-\\mu_i) \\right] = \\frac{1}{\\mu_i} + \\frac{1}{1-\\mu_i} = \\frac{1}{\\mu_i(1-\\mu_i)} $$\nSubstituting this into the weight formula gives a simple result for the canonical link:\n$$ w_i = \\left[ \\left(\\frac{1}{\\mu_i(1-\\mu_i)}\\right)^2 \\mu_i(1-\\mu_i) \\right]^{-1} = \\left[ \\frac{1}{\\mu_i(1-\\mu_i)} \\right]^{-1} = \\mu_i(1-\\mu_i) $$\nSo, the weights are simply the variance of the outcome.\n\nThe working response simplifies to:\n$$ z_i = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i(1-\\mu_i)} $$\n\n### 3. First Iteration Calculation\n\nWe start the algorithm at iteration $t=0$ with the initial guess $\\beta^{(0)} = 0$.\n\n**Step 3.1: Calculate initial linear predictors ($\\eta_i^{(0)}$) and means ($\\mu_i^{(0)}$)**\nThe model is $\\eta_i = \\beta x_i$. With $\\beta^{(0)} = 0$:\n- For observation 1 ($x_1=1$): $\\eta_1^{(0)} = \\beta^{(0)} x_1 = 0 \\times 1 = 0$.\n- For observation 2 ($x_2=-1$): $\\eta_2^{(0)} = \\beta^{(0)} x_2 = 0 \\times (-1) = 0$.\n\nThe mean $\\mu_i$ is found by inverting the logit link: $\\mu_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{1}{1+\\exp(-\\eta_i)}$.\nSince $\\eta_1^{(0)} = \\eta_2^{(0)} = 0$, the initial predicted mean for both observations is:\n$$ \\mu_1^{(0)} = \\mu_2^{(0)} = \\frac{1}{1+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2} $$\n\n**Step 3.2: Calculate initial weights ($w_i$)**\nUsing the formula $w_i = \\mu_i(1-\\mu_i)$:\n- $w_1 = \\mu_1^{(0)}(1-\\mu_1^{(0)}) = \\frac{1}{2}\\left(1-\\frac{1}{2}\\right) = \\frac{1}{4}$.\n- $w_2 = \\mu_2^{(0)}(1-\\mu_2^{(0)}) = \\frac{1}{2}\\left(1-\\frac{1}{2}\\right) = \\frac{1}{4}$.\n\n**Step 3.3: Calculate initial working responses ($z_i$)**\nUsing the formula $z_i = \\eta_i + \\frac{y_i - \\mu_i}{w_i}$:\n- For observation 1 ($y_1=1$): $z_1 = \\eta_1^{(0)} + \\frac{y_1 - \\mu_1^{(0)}}{w_1} = 0 + \\frac{1 - 1/2}{1/4} = \\frac{1/2}{1/4} = 2$.\n- For observation 2 ($y_2=0$): $z_2 = \\eta_2^{(0)} + \\frac{y_2 - \\mu_2^{(0)}}{w_2} = 0 + \\frac{0 - 1/2}{1/4} = \\frac{-1/2}{1/4} = -2$.\n\n### 4. Final Result\nThe requested numerical values for the first iteration, in the order $(z_1, z_2, w_1, w_2)$, are $(2, -2, 1/4, 1/4)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & -2 & \\frac{1}{4} & \\frac{1}{4}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}