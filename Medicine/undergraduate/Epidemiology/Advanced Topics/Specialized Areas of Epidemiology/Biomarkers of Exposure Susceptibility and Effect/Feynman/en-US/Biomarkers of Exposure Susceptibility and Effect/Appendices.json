{
    "hands_on_practices": [
        {
            "introduction": "A biomarker might show high sensitivity and specificity in a controlled lab setting, but how well does it actually work when applied to a whole community? This practice delves into this critical question, showing how the prevalence of an exposure in a population can drastically alter the meaning of a positive test result. By applying Bayes' theorem, you will calculate the positive and negative predictive values to understand a biomarker's real-world performance and the risk of false positives in low-prevalence scenarios .",
            "id": "4573524",
            "problem": "A public health laboratory evaluates a urinary cotinine biomarker as an indicator of recent tobacco smoke exposure. In a validation study against a gold standard, the biomarker’s sensitivity and specificity were estimated. The laboratory now plans to deploy the biomarker in a community respiratory cohort where the true prevalence of recent exposure is low. Using foundational probability definitions and Bayes’ theorem, you will derive predictive values for this biomarker and interpret their epidemiologic implications in the low-prevalence setting.\n\nLet $D$ denote the event “truly exposed (recent tobacco smoke exposure present),” and let $\\bar{D}$ denote “truly unexposed.” Let $T$ denote “biomarker positive,” and $\\bar{T}$ denote “biomarker negative.” Sensitivity is defined as $P(T \\mid D)$, specificity as $P(\\bar{T} \\mid \\bar{D})$, and exposure prevalence as $P(D)$. Positive predictive value (PPV) is defined as $P(D \\mid T)$ and negative predictive value (NPV) as $P(\\bar{D} \\mid \\bar{T})$.\n\nTasks:\n1. Starting from the above definitions and Bayes’ theorem, derive closed-form expressions for the positive predictive value $PPV$ and the negative predictive value $NPV$ in terms of sensitivity $Se$, specificity $Sp$, and exposure prevalence $\\pi$, where $Se = P(T \\mid D)$, $Sp = P(\\bar{T} \\mid \\bar{D})$, and $\\pi = P(D)$.\n2. For a biomarker with sensitivity $Se = 0.94$, specificity $Sp = 0.96$, and exposure prevalence $\\pi = 0.05$ in the target cohort, compute $PPV$ and $NPV$.\n3. Briefly explain, using your derived expressions, the epidemiologic implications of low prevalence for the interpretation of biomarker-positive results. In particular, comment on the proportion of biomarker-positive results that are false positives when $\\pi$ is small.\n\nExpress your numerical answers for $PPV$ and $NPV$ as decimals, rounded to four significant figures. Do not use the percentage sign.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of probability theory and its application to epidemiology, is well-posed with all necessary information provided, and is expressed in objective, formal language. The problem presents a standard, non-trivial exercise in biostatistics.\n\n**Task 1: Derivation of PPV and NPV**\n\nWe are asked to derive expressions for the positive predictive value ($PPV$) and negative predictive value ($NPV$) in terms of sensitivity ($Se$), specificity ($Sp$), and prevalence ($\\pi$).\n\nThe given definitions are:\n- Sensitivity: $Se = P(T \\mid D)$\n- Specificity: $Sp = P(\\bar{T} \\mid \\bar{D})$\n- Prevalence: $\\pi = P(D)$\n- Positive Predictive Value: $PPV = P(D \\mid T)$\n- Negative Predictive Value: $NPV = P(\\bar{D} \\mid \\bar{T})$\n\nFrom these, we can also state complementary probabilities:\n- $P(\\bar{D}) = 1 - P(D) = 1 - \\pi$\n- False Positive Rate: $P(T \\mid \\bar{D}) = 1 - P(\\bar{T} \\mid \\bar{D}) = 1 - Sp$\n- False Negative Rate: $P(\\bar{T} \\mid D) = 1 - P(T \\mid D) = 1 - Se$\n\n**Derivation of PPV:**\nWe begin with the definition of $PPV$ and apply Bayes' theorem:\n$$PPV = P(D \\mid T) = \\frac{P(T \\mid D) P(D)}{P(T)}$$\nThe denominator, $P(T)$, is the overall probability of a positive test result. It can be expanded using the law of total probability:\n$$P(T) = P(T \\mid D)P(D) + P(T \\mid \\bar{D})P(\\bar{D})$$\nSubstituting the defined terms ($Se$, $Sp$, $\\pi$) into this expansion:\n$$P(T) = (Se)(\\pi) + (1 - Sp)(1 - \\pi)$$\nThe numerator of the Bayes' theorem expression is $P(T \\mid D) P(D) = (Se)(\\pi)$.\nSubstituting these into the expression for $PPV$ yields the closed-form expression:\n$$PPV = \\frac{Se \\cdot \\pi}{(Se \\cdot \\pi) + (1 - Sp)(1 - \\pi)}$$\n\n**Derivation of NPV:**\nWe begin with the definition of $NPV$ and apply Bayes' theorem:\n$$NPV = P(\\bar{D} \\mid \\bar{T}) = \\frac{P(\\bar{T} \\mid \\bar{D}) P(\\bar{D})}{P(\\bar{T})}$$\nThe denominator, $P(\\bar{T})$, is the overall probability of a negative test result. It can be expanded using the law of total probability:\n$$P(\\bar{T}) = P(\\bar{T} \\mid \\bar{D})P(\\bar{D}) + P(\\bar{T} \\mid D)P(D)$$\nSubstituting the defined terms into this expansion:\n$$P(\\bar{T}) = (Sp)(1 - \\pi) + (1 - Se)(\\pi)$$\nThe numerator of the Bayes' theorem expression is $P(\\bar{T} \\mid \\bar{D}) P(\\bar{D}) = (Sp)(1 - \\pi)$.\nSubstituting these into the expression for $NPV$ yields the closed-form expression:\n$$NPV = \\frac{Sp \\cdot (1 - \\pi)}{(Sp \\cdot (1 - \\pi)) + (1 - Se)\\pi}$$\n\n**Task 2: Calculation of PPV and NPV**\n\nWe are given the following values:\n- $Se = 0.94$\n- $Sp = 0.96$\n- $\\pi = 0.05$\n\nUsing the derived formula for $PPV$:\n$$PPV = \\frac{(0.94)(0.05)}{(0.94)(0.05) + (1 - 0.96)(1 - 0.05)} = \\frac{0.047}{0.047 + (0.04)(0.95)} = \\frac{0.047}{0.047 + 0.038} = \\frac{0.047}{0.085}$$\n$$PPV \\approx 0.552941...$$\nRounding to four significant figures, $PPV = 0.5529$.\n\nUsing the derived formula for $NPV$:\n$$NPV = \\frac{(0.96)(1 - 0.05)}{(0.96)(1 - 0.05) + (1 - 0.94)(0.05)} = \\frac{(0.96)(0.95)}{(0.96)(0.95) + (0.06)(0.05)} = \\frac{0.912}{0.912 + 0.003} = \\frac{0.912}{0.915}$$\n$$NPV \\approx 0.996721...$$\nRounding to four significant figures, $NPV = 0.9967$.\n\n**Task 3: Epidemiologic Implications of Low Prevalence**\n\nThe derived expression for $PPV$ is:\n$$PPV = \\frac{Se \\cdot \\pi}{(Se \\cdot \\pi) + (1 - Sp)(1 - \\pi)}$$\nIn this formula, the denominator $P(T)$ is the sum of the proportion of true positives, $(Se \\cdot \\pi)$, and the proportion of false positives, $(1 - Sp)(1 - \\pi)$, in the overall population. The proportion of positive biomarker results that are false positives is given by $P(\\bar{D} \\mid T) = 1 - PPV$.\n\nWhen the prevalence $\\pi$ is low (i.e., $\\pi \\to 0$), the term $(1 - \\pi)$ approaches $1$. The numerator of the $PPV$ expression, $Se \\cdot \\pi$, becomes very small. The denominator term for false positives, $(1 - Sp)(1 - \\pi)$, will dominate the term for true positives, $Se \\cdot \\pi$, unless the specificity $Sp$ is exceptionally close to $1$.\n\nConsider the ratio of false positives to true positives among all positive tests:\n$$\\frac{\\text{False Positives}}{\\text{True Positives}} = \\frac{P(T \\mid \\bar{D})P(\\bar{D})}{P(T \\mid D)P(D)} = \\frac{(1 - Sp)(1 - \\pi)}{Se \\cdot \\pi}$$\nAs $\\pi$ becomes smaller, this ratio grows larger. This signifies that even for a test with high specificity (e.g., $Sp = 0.96$, so $1-Sp = 0.04$), when applied to a large population of unexposed individuals (as is a consequence of low prevalence), the absolute number of false positives can be comparable to, or even exceed, the absolute number of true positives.\n\nIn this specific problem, with $\\pi = 0.05$, the $PPV$ is only $0.5529$. This means that $1 - 0.5529 = 0.4471$, or approximately $44.7\\%$ of all positive results are false positives. If the prevalence were even lower, say $\\pi = 0.01$, the $PPV$ would drop to approximately $0.19$, implying that over $80\\%$ of positive results would be false positives.\n\nThe epidemiologic implication is critical: in low-prevalence settings, a positive test result from a single biomarker, even one with high sensitivity and specificity, has a low probability of indicating true exposure. A substantial proportion of positive results will be incorrect (false positives), making the biomarker unreliable for individual case confirmation without further verification. Its utility may be greater for population-level surveillance rather than individual diagnosis.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5529  0.9967 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Many biological markers, like metabolites in urine, are not perfectly stable and fluctuate from day to day. This inherent variability acts as a form of measurement error, which can make a true association between an exposure and a health outcome appear weaker than it really is. This phenomenon, known as attenuation or regression dilution bias, is a fundamental challenge in epidemiology, and this exercise will guide you through deriving and calculating its precise impact on a study's results .",
            "id": "4573601",
            "problem": "A cohort study of ambient benzene exposure uses a urinary metabolite as a biomarker of exposure and high-sensitivity C-reactive protein as a biomarker of effect. A validation sub-study collected repeated urine samples on each participant to quantify variability in the exposure biomarker. Under the classical additive measurement error model, each observed biomarker measurement $X^{\\ast}$ on a given day satisfies $X^{\\ast} = X + U$, where $X$ is the participant’s long-term average exposure biomarker level and $U$ is within-person short-term variability plus analytical noise, independent of $X$ and of the outcome given $X$. The validation sub-study estimates a between-person variance $\\sigma_{b}^{2}$ and a within-person variance $\\sigma_{w}^{2}$ on the log-transformed exposure biomarker scale. Suppose the estimates are $\\sigma_{b}^{2} = 0.8$ and $\\sigma_{w}^{2} = 1.2$, and that the true linear relation between the biomarker of effect $Y$ and the true exposure biomarker $X$ is $Y = \\alpha + \\beta X + \\varepsilon$ with $\\beta = 0.5$ for a $1$-unit increase in $X$, where $\\varepsilon$ is independent of $X$ and $U$.\n\nUsing only variance and covariance definitions and the assumptions stated above, derive from first principles the expected slope when regressing $Y$ on a single-occasion exposure biomarker measurement $X^{\\ast}$ and compute its numerical value. Round your answer to four significant figures. Express the final coefficient as a pure number with no unit.",
            "solution": "The problem asks for the derivation of the expected slope when regressing a biomarker of effect, $Y$, on a single-occasion measurement of a biomarker of exposure, $X^{\\ast}$. This observed slope, which we shall denote as $\\beta^{\\ast}$, is defined in a simple linear regression by the ratio of the covariance between the outcome and the predictor to the variance of the predictor.\n\n$$\n\\beta^{\\ast} = \\frac{\\mathrm{Cov}(Y, X^{\\ast})}{\\mathrm{Var}(X^{\\ast})}\n$$\n\nThe problem provides the following models and parameters:\n1.  The true relationship between the outcome $Y$ and the true long-term average exposure biomarker level $X$ is given by the linear model:\n    $$Y = \\alpha + \\beta X + \\varepsilon$$\n    where $\\beta = 0.5$, and $\\varepsilon$ is a random error term independent of $X$.\n\n2.  The observed biomarker measurement $X^{\\ast}$ is related to the true level $X$ via the classical additive measurement error model:\n    $$X^{\\ast} = X + U$$\n    where $U$ is the measurement error, representing within-person variability and analytical noise.\n\n3.  The variances of the true exposure levels and the measurement error are given as the between-person variance and within-person variance, respectively:\n    -   $\\mathrm{Var}(X) = \\sigma_{b}^{2} = 0.8$\n    -   $\\mathrm{Var}(U) = \\sigma_{w}^{2} = 1.2$\n\n4.  Key independence assumptions are stated: $X$, $U$, and $\\varepsilon$ are mutually independent. Specifically, $U$ is independent of $X$, and $\\varepsilon$ is independent of both $X$ and $U$.\n\nOur derivation proceeds in two parts: calculating the denominator $\\mathrm{Var}(X^{\\ast})$ and the numerator $\\mathrm{Cov}(Y, X^{\\ast})$.\n\nFirst, we calculate the variance of the observed measurement, $\\mathrm{Var}(X^{\\ast})$. Using the model $X^{\\ast} = X + U$ and the property that the variance of a sum of independent random variables is the sum of their variances:\n$$\n\\mathrm{Var}(X^{\\ast}) = \\mathrm{Var}(X + U)\n$$\nGiven that $X$ and $U$ are independent, we have:\n$$\n\\mathrm{Var}(X^{\\ast}) = \\mathrm{Var}(X) + \\mathrm{Var}(U) = \\sigma_{b}^{2} + \\sigma_{w}^{2}\n$$\n\nSecond, we calculate the covariance between the outcome $Y$ and the observed measurement $X^{\\ast}$, $\\mathrm{Cov}(Y, X^{\\ast})$. We substitute the expressions for $Y$ and $X^{\\ast}$:\n$$\n\\mathrm{Cov}(Y, X^{\\ast}) = \\mathrm{Cov}(\\alpha + \\beta X + \\varepsilon, X + U)\n$$\nUsing the bilinearity property of covariance:\n$$\n\\mathrm{Cov}(Y, X^{\\ast}) = \\mathrm{Cov}(\\alpha, X+U) + \\mathrm{Cov}(\\beta X, X+U) + \\mathrm{Cov}(\\varepsilon, X+U)\n$$\nLet's evaluate each term:\n-   $\\mathrm{Cov}(\\alpha, X+U) = 0$, since $\\alpha$ is a constant.\n-   $\\mathrm{Cov}(\\beta X, X+U) = \\beta \\mathrm{Cov}(X, X+U) = \\beta (\\mathrm{Cov}(X, X) + \\mathrm{Cov}(X, U))$.\n    -   $\\mathrm{Cov}(X, X) = \\mathrm{Var}(X) = \\sigma_{b}^{2}$.\n    -   $\\mathrm{Cov}(X, U) = 0$, because $X$ and $U$ are independent.\n    -   Therefore, $\\mathrm{Cov}(\\beta X, X+U) = \\beta \\sigma_{b}^{2}$.\n-   $\\mathrm{Cov}(\\varepsilon, X+U) = \\mathrm{Cov}(\\varepsilon, X) + \\mathrm{Cov}(\\varepsilon, U)$.\n    -   $\\mathrm{Cov}(\\varepsilon, X) = 0$, because $\\varepsilon$ and $X$ are independent.\n    -   $\\mathrm{Cov}(\\varepsilon, U) = 0$, because $\\varepsilon$ and $U$ are independent.\n    -   Therefore, $\\mathrm{Cov}(\\varepsilon, X+U) = 0$.\n\nCombining these results, the numerator becomes:\n$$\n\\mathrm{Cov}(Y, X^{\\ast}) = 0 + \\beta \\sigma_{b}^{2} + 0 = \\beta \\sigma_{b}^{2}\n$$\n\nNow, we can assemble the expression for the observed slope $\\beta^{\\ast}$ by substituting the derived expressions for the numerator and the denominator:\n$$\n\\beta^{\\ast} = \\frac{\\beta \\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{w}^{2}}\n$$\nThis relationship shows that the observed slope $\\beta^{\\ast}$ is an attenuated version of the true slope $\\beta$. The attenuation factor, $\\frac{\\sigma_{b}^{2}}{\\sigma_{b}^{2} + \\sigma_{w}^{2}}$, is known as the reliability ratio or intraclass correlation coefficient.\n\nFinally, we compute the numerical value of $\\beta^{\\ast}$ using the given values: $\\beta = 0.5$, $\\sigma_{b}^{2} = 0.8$, and $\\sigma_{w}^{2} = 1.2$.\n$$\n\\beta^{\\ast} = \\frac{(0.5)(0.8)}{0.8 + 1.2} = \\frac{0.4}{2.0} = 0.2\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\beta^{\\ast} = 0.2000\n$$",
            "answer": "$$\\boxed{0.2000}$$"
        },
        {
            "introduction": "In statistical analysis, the instinct to adjust for potential confounders is strong, but what if adjusting for a variable could actually introduce bias? This exercise explores a counter-intuitive pitfall in epidemiology known as collider-stratification bias, where controlling for a variable that is a common effect of two other factors can create a spurious statistical association between them. Through a hands-on calculation, you will see how adjusting for a seemingly relevant biomarker can distort the true effect of an exposure on an outcome, a critical lesson for designing valid analyses .",
            "id": "4573566",
            "problem": "Consider a study of an environmental exposure where investigators measure a stress hormone biomarker that is influenced by both the exposure and an unmeasured susceptibility factor. Let $X$ denote exposure, $U$ denote an unmeasured susceptibility factor, $C$ denote the biomarker (for example, a cortisol-like stress hormone), and $Y$ denote a health effect outcome. Suppose the data-generating mechanism is linear with independent, mean-zero exogenous components. Specifically, assume $X$ and $U$ are independent with $\\operatorname{Var}(X)=1$ and $\\operatorname{Var}(U)=1$, and let\n$$C = \\alpha_{X} X + \\alpha_{U} U + \\varepsilon_{C}, \\quad Y = \\beta_{X} X + \\beta_{U} U + \\varepsilon_{Y},$$\nwhere $\\varepsilon_{C}$ and $\\varepsilon_{Y}$ are independent mean-zero errors, independent of $X$ and $U$, with $\\operatorname{Var}(\\varepsilon_{C})=1$ and $\\operatorname{Var}(\\varepsilon_{Y})=1$. Take $\\alpha_{X}=0.8$, $\\alpha_{U}=1.0$, $\\beta_{X}=0.5$, and $\\beta_{U}=0.6$.\n\nIn a Directed Acyclic Graph (DAG) representation, $C$ is a collider on the path $X \\rightarrow C \\leftarrow U \\rightarrow Y$. Using only core definitions of covariance, variance, and linear regression (i.e., the normal equations that define population regression coefficients), derive from first principles the population coefficient on $X$ when regressing $Y$ on both $X$ and $C$, and compute the bias induced by including $C$ as an adjustment variable. Define the bias as $b_{X|C} - \\beta_{X}$, where $b_{X|C}$ is the coefficient of $X$ in the population linear regression of $Y$ on $X$ and $C$.\n\nRound your final numeric answer for the bias to four significant figures. No physical units are involved in this calculation.",
            "solution": "The problem asks for the derivation of the bias induced by adjusting for a collider variable in a linear regression model. We are given a system of linear structural equations representing a data-generating process and are tasked with calculating the population coefficient for an exposure variable $X$ in a regression of an outcome $Y$ on both $X$ and a biomarker $C$. The bias is the difference between this coefficient and the true causal effect of $X$ on $Y$.\n\nThe specified data-generating mechanism is:\n$$C = \\alpha_{X} X + \\alpha_{U} U + \\varepsilon_{C}$$\n$$Y = \\beta_{X} X + \\beta_{U} U + \\varepsilon_{Y}$$\n\nThe variables $X$, $U$, $\\varepsilon_{C}$, and $\\varepsilon_{Y}$ are all mutually independent with mean zero. Their variances are given as $\\operatorname{Var}(X)=1$, $\\operatorname{Var}(U)=1$, $\\operatorname{Var}(\\varepsilon_{C})=1$, and $\\operatorname{Var}(\\varepsilon_{Y})=1$. The coefficients are given as $\\alpha_{X}=0.8$, $\\alpha_{U}=1.0$, $\\beta_{X}=0.5$, and $\\beta_{U}=0.6$. The true causal effect of $X$ on $Y$ is represented by the coefficient $\\beta_{X}$.\n\nWe consider the population linear regression of $Y$ on $X$ and $C$:\n$$E[Y | X, C] = b_{0} + b_{X|C} X + b_{C|X} C$$\nThe coefficients $b_{X|C}$ and $b_{C|X}$ are determined by the normal equations, which can be expressed in matrix form. Let the vector of predictors be $\\mathbf{Z} = \\begin{pmatrix} X \\\\ C \\end{pmatrix}$. The vector of regression coefficients $\\mathbf{b} = \\begin{pmatrix} b_{X|C} \\\\ b_{C|X} \\end{pmatrix}$ is given by:\n$$\\mathbf{b} = \\Sigma_{\\mathbf{Z}\\mathbf{Z}}^{-1} \\Sigma_{\\mathbf{Z}Y}$$\nwhere $\\Sigma_{\\mathbf{Z}\\mathbf{Z}}$ is the covariance matrix of the predictors and $\\Sigma_{\\mathbf{Z}Y}$ is the vector of covariances between the predictors and the outcome variable $Y$.\n$$\\Sigma_{\\mathbf{Z}\\mathbf{Z}} = \\begin{pmatrix} \\operatorname{Var}(X)  \\operatorname{Cov}(X, C) \\\\ \\operatorname{Cov}(C, X)  \\operatorname{Var}(C) \\end{pmatrix}, \\quad \\Sigma_{\\mathbf{Z}Y} = \\begin{pmatrix} \\operatorname{Cov}(X, Y) \\\\ \\operatorname{Cov}(C, Y) \\end{pmatrix}$$\n\nThe derivation proceeds by first calculating the required variance and covariance terms from their definitions and the given structural equations. Since all fundamental variables have mean zero, any linear combination of them also has mean zero. Therefore, for any two variables $A$ and $B$ in our model, $\\operatorname{Cov}(A, B) = E[AB]$.\n\n1.  **$\\operatorname{Var}(X)$**: This is given as $\\operatorname{Var}(X) = 1$.\n\n2.  **$\\operatorname{Cov}(X, C)$**:\n    $$\\operatorname{Cov}(X, C) = \\operatorname{Cov}(X, \\alpha_{X} X + \\alpha_{U} U + \\varepsilon_{C})$$\n    By linearity of covariance and independence of $X$, $U$, and $\\varepsilon_{C}$:\n    $$\\operatorname{Cov}(X, C) = \\alpha_{X} \\operatorname{Cov}(X, X) + \\alpha_{U} \\operatorname{Cov}(X, U) + \\operatorname{Cov}(X, \\varepsilon_{C}) = \\alpha_{X} \\operatorname{Var}(X) + 0 + 0$$\n    $$\\operatorname{Cov}(X, C) = 0.8 \\times 1 = 0.8$$\n\n3.  **$\\operatorname{Var}(C)$**:\n    $$\\operatorname{Var}(C) = \\operatorname{Var}(\\alpha_{X} X + \\alpha_{U} U + \\varepsilon_{C})$$\n    Due to the independence of $X$, $U$, and $\\varepsilon_{C}$, the variance of the sum is the sum of the variances:\n    $$\\operatorname{Var(C)} = \\operatorname{Var}(\\alpha_{X} X) + \\operatorname{Var}(\\alpha_{U} U) + \\operatorname{Var}(\\varepsilon_{C}) = \\alpha_{X}^{2} \\operatorname{Var}(X) + \\alpha_{U}^{2} \\operatorname{Var}(U) + \\operatorname{Var}(\\varepsilon_{C})$$\n    $$\\operatorname{Var}(C) = (0.8)^{2}(1) + (1.0)^{2}(1) + 1 = 0.64 + 1.0 + 1.0 = 2.64$$\n\n4.  **$\\operatorname{Cov}(X, Y)$**:\n    $$\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(X, \\beta_{X} X + \\beta_{U} U + \\varepsilon_{Y})$$\n    By linearity of covariance and independence of $X$, $U$, and $\\varepsilon_{Y}$:\n    $$\\operatorname{Cov}(X, Y) = \\beta_{X} \\operatorname{Cov}(X, X) + \\beta_{U} \\operatorname{Cov}(X, U) + \\operatorname{Cov}(X, \\varepsilon_{Y}) = \\beta_{X} \\operatorname{Var}(X) + 0 + 0$$\n    $$\\operatorname{Cov}(X, Y) = 0.5 \\times 1 = 0.5$$\n\n5.  **$\\operatorname{Cov}(C, Y)$**:\n    $$\\operatorname{Cov}(C, Y) = \\operatorname{Cov}(\\alpha_{X} X + \\alpha_{U} U + \\varepsilon_{C}, \\beta_{X} X + \\beta_{U} U + \\varepsilon_{Y})$$\n    Expanding this using bilinearity of covariance and noting that covariances of independent variables are zero:\n    $$\\operatorname{Cov}(C, Y) = \\operatorname{Cov}(\\alpha_{X}X, \\beta_{X}X) + \\operatorname{Cov}(\\alpha_{U}U, \\beta_{U}U)$$\n    $$= \\alpha_{X}\\beta_{X}\\operatorname{Var}(X) + \\alpha_{U}\\beta_{U}\\operatorname{Var}(U)$$\n    $$\\operatorname{Cov}(C, Y) = (0.8)(0.5)(1) + (1.0)(0.6)(1) = 0.4 + 0.6 = 1.0$$\n\nNow we construct the matrices:\n$$\\Sigma_{\\mathbf{Z}\\mathbf{Z}} = \\begin{pmatrix} 1  0.8 \\\\ 0.8  2.64 \\end{pmatrix}, \\quad \\Sigma_{\\mathbf{Z}Y} = \\begin{pmatrix} 0.5 \\\\ 1.0 \\end{pmatrix}$$\n\nTo find the coefficient vector $\\mathbf{b}$, we first compute the inverse of $\\Sigma_{\\mathbf{Z}\\mathbf{Z}}$:\n$$\\det(\\Sigma_{\\mathbf{Z}\\mathbf{Z}}) = (1)(2.64) - (0.8)(0.8) = 2.64 - 0.64 = 2.0$$\n$$\\Sigma_{\\mathbf{Z}\\mathbf{Z}}^{-1} = \\frac{1}{\\det(\\Sigma_{\\mathbf{Z}\\mathbf{Z}})} \\begin{pmatrix} 2.64  -0.8 \\\\ -0.8  1 \\end{pmatrix} = \\frac{1}{2.0} \\begin{pmatrix} 2.64  -0.8 \\\\ -0.8  1 \\end{pmatrix} = \\begin{pmatrix} 1.32  -0.4 \\\\ -0.4  0.5 \\end{pmatrix}$$\n\nNow we solve for $\\mathbf{b}$:\n$$\\mathbf{b} = \\begin{pmatrix} b_{X|C} \\\\ b_{C|X} \\end{pmatrix} = \\Sigma_{\\mathbf{Z}\\mathbf{Z}}^{-1} \\Sigma_{\\mathbf{Z}Y} = \\begin{pmatrix} 1.32  -0.4 \\\\ -0.4  0.5 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 1.0 \\end{pmatrix}$$\n$$= \\begin{pmatrix} (1.32)(0.5) + (-0.4)(1.0) \\\\ (-0.4)(0.5) + (0.5)(1.0) \\end{pmatrix} = \\begin{pmatrix} 0.66 - 0.4 \\\\ -0.2 + 0.5 \\end{pmatrix} = \\begin{pmatrix} 0.26 \\\\ 0.3 \\end{pmatrix}$$\nThe population coefficient on $X$ when regressing $Y$ on both $X$ and $C$ is the first element of this vector:\n$$b_{X|C} = 0.26$$\n\nThe problem defines bias as the difference between this estimated coefficient and the true causal effect, $\\beta_{X}$.\n$$\\text{Bias} = b_{X|C} - \\beta_{X}$$\nThe true causal effect is given as $\\beta_{X} = 0.5$.\n$$\\text{Bias} = 0.26 - 0.5 = -0.24$$\nThis non-zero bias is the collider-stratification bias, which arises from conditioning on $C$, a common effect (collider) of the exposure $X$ and the unmeasured confounder $U$.\n\nThe final numeric answer for the bias must be rounded to four significant figures. The calculated bias is exactly $-0.24$. Expressed to four significant figures, this is $-0.2400$.",
            "answer": "$$\\boxed{-0.2400}$$"
        }
    ]
}