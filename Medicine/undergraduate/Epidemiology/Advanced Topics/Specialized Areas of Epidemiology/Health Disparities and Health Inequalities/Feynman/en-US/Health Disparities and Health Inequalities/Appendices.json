{
    "hands_on_practices": [
        {
            "introduction": "A foundational task in studying health inequalities is comparing outcomes like mortality between populations. However, raw comparisons can be misleading if the groups have different underlying structures, such as age distributions. This exercise introduces indirect standardization, a core epidemiological method to create a more equitable comparison. By calculating the Standardized Mortality Ratio (SMR), you will learn to quantify the excess mortality in a specific community relative to a national standard, providing a powerful initial measure of a health disparity . This practice emphasizes that the SMR is a starting point for inquiry, prompting a deeper investigation into the complex factors behind the observed differences.",
            "id": "4595794",
            "problem": "A public health team is assessing mortality in a disadvantaged urban neighborhood cluster that experiences elevated chronic disease burden and limited access to preventive care. To study health inequalities, they decide to perform indirect age standardization against a national standard. Assume a one-year observation period with complete follow-up and negligible in- and out-migration, so that each resident contributes approximately $1$ person-year. The national standard age-specific mortality rates are:\n\n- Ages $0$–$44$: $1.2$ per $1{,}000$ person-years\n- Ages $45$–$64$: $5.6$ per $1{,}000$ person-years\n- Ages $65$–$74$: $14.0$ per $1{,}000$ person-years\n- Ages $75$–$84$: $40.0$ per $1{,}000$ person-years\n- Ages $\\ge 85$: $110.0$ per $1{,}000$ person-years\n\nThe disadvantaged group's age distribution at baseline is:\n\n- Ages $0$–$44$: $12{,}500$ residents\n- Ages $45$–$64$: $8{,}400$ residents\n- Ages $65$–$74$: $2{,}900$ residents\n- Ages $75$–$84$: $1{,}100$ residents\n- Ages $\\ge 85$: $270$ residents\n\nAcross the year, the disadvantaged group experiences $223$ observed deaths in total (all causes). Using only fundamental definitions of mortality, indirect standardization, and ratio measures, compute the standardized mortality ratio for the disadvantaged group relative to the national standard. Express the final standardized mortality ratio as a unitless decimal and round your answer to four significant figures.\n\nThen, explain the interpretational pitfalls of the standardized mortality ratio when the disadvantaged group's underlying health needs differ systematically from the standard population (for example, due to higher baseline morbidity, environmental exposures, or care access barriers). Your explanation should focus on why the ratio might conflate need with performance or other structural factors, and what assumptions are necessary for fair comparison.\n\nNote: Standardized Mortality Ratio (SMR) should be interpreted as a dimensionless ratio comparing observed deaths to the expected number of deaths under the standard rates.",
            "solution": "The problem requires the calculation of the Standardized Mortality Ratio (SMR) for a disadvantaged group and an explanation of the interpretational pitfalls of this measure in the context of health disparities. The solution proceeds in two parts: first, the quantitative calculation of the SMR, and second, a qualitative analysis of its assumptions and limitations.\n\n**Part 1: Calculation of the Standardized Mortality Ratio (SMR)**\n\nThe Standardized Mortality Ratio is defined as the ratio of the total number of observed deaths in the study population to the total number of expected deaths. The expected deaths are calculated by applying the age-specific mortality rates of a standard population to the age structure of the study population.\n\nThe formula for the SMR is:\n$$ \\text{SMR} = \\frac{\\text{Observed Deaths}}{\\text{Expected Deaths}} = \\frac{D_O}{D_E} $$\n\nThe total number of observed deaths, $D_O$, is given as $223$.\n\nThe total number of expected deaths, $D_E$, is the sum of the expected deaths in each age stratum $i$. The expected deaths for a given stratum, $d_{E,i}$, are calculated by multiplying the number of people in that stratum in the study population, $N_i$, by the corresponding age-specific mortality rate from the standard population, $M_i$.\n$$ D_E = \\sum_i d_{E,i} = \\sum_i (N_i \\times M_i) $$\n\nWe are given the following data:\n\nStudy Population ($N_i$):\n- Ages $0$–$44$: $N_1 = 12,500$\n- Ages $45$–$64$: $N_2 = 8,400$\n- Ages $65$–$74$: $N_3 = 2,900$\n- Ages $75$–$84$: $N_4 = 1,100$\n- Ages $\\ge 85$: $N_5 = 270$\n\nStandard Mortality Rates ($M_i$), converted from per $1,000$ person-years to a decimal rate:\n- Ages $0$–$44$: $M_1 = 1.2 / 1000 = 0.0012$\n- Ages $45$–$64$: $M_2 = 5.6 / 1000 = 0.0056$\n- Ages $65$–$74$: $M_3 = 14.0 / 1000 = 0.0140$\n- Ages $75–84$: $M_4 = 40.0 / 1000 = 0.0400$\n- Ages $\\ge 85$: $M_5 = 110.0 / 1000 = 0.1100$\n\nNow, we calculate the expected deaths for each age stratum:\n- Stratum $1$ (Ages $0$–$44$): $d_{E,1} = N_1 \\times M_1 = 12,500 \\times 0.0012 = 15.0$\n- Stratum $2$ (Ages $45$–$64$): $d_{E,2} = N_2 \\times M_2 = 8,400 \\times 0.0056 = 47.04$\n- Stratum $3$ (Ages $65$–$74$): $d_{E,3} = N_3 \\times M_3 = 2,900 \\times 0.0140 = 40.6$\n- Stratum $4$ (Ages $75$–$84$): $d_{E,4} = N_4 \\times M_4 = 1,100 \\times 0.0400 = 44.0$\n- Stratum $5$ (Ages $\\ge 85$): $d_{E,5} = N_5 \\times M_5 = 270 \\times 0.1100 = 29.7$\n\nThe total number of expected deaths, $D_E$, is the sum of these values:\n$$ D_E = 15.0 + 47.04 + 40.6 + 44.0 + 29.7 = 176.34 $$\n\nWith $D_O = 223$ and $D_E = 176.34$, we can now compute the SMR:\n$$ \\text{SMR} = \\frac{D_O}{D_E} = \\frac{223}{176.34} \\approx 1.2645967... $$\n\nRounding to four significant figures, we get:\n$$ \\text{SMR} \\approx 1.265 $$\nAn SMR of $1.265$ indicates that the disadvantaged group experienced approximately $26.5\\%$ more deaths than would have been expected if they had the same age-specific mortality rates as the national standard population.\n\n**Part 2: Interpretational Pitfalls of the SMR**\n\nThe calculation of the SMR is predicated on a critical, often unstated, assumption: that the standard population's age-specific mortality rates ($M_i$) are the appropriate rates to expect in the study population, once age has been accounted for. For the comparison to be fair, one must assume that, within each age stratum, the study and standard populations are otherwise comparable in terms of underlying health status and risk. This is known as the assumption of comparability of age-specific rate ratios across the populations.\n\nIn the context of health disparities, this assumption is fundamentally violated. The problem statement itself specifies that the study group is a \"disadvantaged urban neighborhood\" with \"elevated chronic disease burden and limited access to preventive care.\" This leads to significant interpretational pitfalls:\n\n1.  **Conflation of Need, Structural Factors, and Performance:** An elevated SMR does not cleanly isolate a single cause. The excess mortality ($26.5\\%$ in this case) is an aggregate effect of multiple, entangled factors:\n    *   **Higher Health Needs:** The disadvantaged population likely has a higher prevalence and severity of morbidity (illness) at any given age compared to the national average. For example, a $50$-year-old in this group may have a health profile more akin to a $60$-year-old in the standard population. Applying the standard mortality rate for $50$-year-olds to this group underestimates the \"expected\" mortality based on their actual health status.\n    *   **Structural Disadvantages:** Factors such as higher exposure to environmental pollution, chronic stress from socioeconomic hardship, food deserts, and unsafe housing can directly increase mortality risk, independent of healthcare access or quality. These factors are not present to the same degree in the standard population, and the SMR does not adjust for them.\n    *   **Healthcare System Performance:** The \"limited access to preventive care\" and likely lower quality of curative care contribute to a higher case-fatality rate for many conditions. This is a failure of the healthcare system to meet the population's needs.\n\n2.  **The Misleading Nature of \"Expected\" Deaths:** The term \"expected deaths\" is a technical one. It does not represent a realistic expectation for a population with a high burden of disease. It represents a hypothetical scenario: \"What if this population, despite its disadvantages, magically had the mortality risk of the healthier, better-resourced standard population?\" The SMR measures the gap between the observed reality and this counterfactual ideal. Attributing the entire gap to a single cause, such as \"poor hospital quality,\" is a form of ecological fallacy. It ignores the fact that the population was sicker to begin with.\n\nFor the SMR to be a fair comparison of, for instance, the quality of healthcare between two regions, it is necessary to assume that the populations have a similar underlying burden of disease and risk factor profile, differing only in the healthcare they receive. In the study of health inequalities, this assumption is invalid by definition. The SMR is therefore a useful tool for *quantifying* the magnitude of an inequality, but it is not a tool for *explaining* its causes without further information. It demonstrates that a problem exists but does not, by itself, diagnose the etiology of the problem.",
            "answer": "$$ \\boxed{1.265} $$"
        },
        {
            "introduction": "The accuracy of our conclusions about health disparities rests on the quality of our data. This practice confronts a critical real-world challenge: what happens when our measurement tools, like diagnostic tests, perform differently across the very groups we are studying? This phenomenon, known as differential misclassification, can significantly distort our perception of health inequalities. Through this exercise, you will quantify how biased measurement accuracy can systematically alter the observed risk difference and risk ratio, two fundamental measures of disparity . Mastering this concept is crucial for critically evaluating evidence and understanding the methodological rigor required in health equity research.",
            "id": "4595786",
            "problem": "An urban cohort study investigates a binary health outcome $Y$ (uncontrolled hypertension over $12$ months) across two racial or ethnic groups, $A$ and $B$, to assess health disparities. Let $p_{A} = P(Y=1 \\mid A)$ and $p_{B} = P(Y=1 \\mid B)$ denote the true risks. The disparity is summarized by the risk difference $RD = p_{A} - p_{B}$ and the risk ratio $RR = \\frac{p_{A}}{p_{B}}$. In the survey, the outcome is ascertained by an algorithm that exhibits differential misclassification across groups due to differences in care access and measurement fidelity. For group $g \\in \\{A,B\\}$, let the sensitivity be $Se_{g} = P(Y^{*}=1 \\mid Y=1, g)$ and the specificity be $Sp_{g} = P(Y^{*}=0 \\mid Y=0, g)$, where $Y^{*}$ is the misclassified outcome recorded by the algorithm. Assume nondifferential error within groups (the algorithm’s $Se_{g}$ and $Sp_{g}$ do not depend on covariates) and independent misclassification conditional on the true outcome.\n\nGiven clinically adjudicated true risks $p_{A} = 0.15$ and $p_{B} = 0.10$, and algorithm performance $Se_{A} = 0.85$, $Sp_{A} = 0.95$, $Se_{B} = 0.70$, and $Sp_{B} = 0.98$, do the following:\n\n1. Using only the fundamental definitions of sensitivity and specificity, derive an expression for the observed risk in each group, $p^{*}_{g} = P(Y^{*}=1 \\mid g)$, as a function of $p_{g}$, $Se_{g}$, and $Sp_{g}$.\n\n2. Use your derived expression to obtain the observed disparity measures $RD^{*} = p^{*}_{A} - p^{*}_{B}$ and $RR^{*} = \\frac{p^{*}_{A}}{p^{*}_{B}}$.\n\n3. Derive the bias in each disparity measure due to differential misclassification, defined additively as $RD^{*} - RD$ and $RR^{*} - RR$. Determine the direction of bias for each measure (whether the observed measure overestimates or underestimates the true measure), and compute the magnitude of the bias.\n\nReport the two numeric bias quantities $RD^{*} - RD$ and $RR^{*} - RR$, rounded to four significant figures. Express the numbers as decimals with no units.",
            "solution": "The solution proceeds in three parts as requested.\n\n### Part 1: Derivation of Observed Risk $p^{*}_{g}$\nThe observed risk for a group $g$, denoted as $p^{*}_{g}$, is the probability of a positive test result, $P(Y^{*}=1 \\mid g)$. We can derive an expression for this quantity using the law of total probability, by conditioning on the true disease status $Y$.\n\nFor any group $g \\in \\{A, B\\}$, the population is partitioned into those who truly have the condition ($Y=1$) and those who do not ($Y=0$). Therefore, we can write:\n$$\np^{*}_{g} = P(Y^{*}=1 \\mid g) = P(Y^{*}=1, Y=1 \\mid g) + P(Y^{*}=1, Y=0 \\mid g)\n$$\nUsing the definition of conditional probability, $P(X,Z \\mid W) = P(X \\mid Z,W)P(Z \\mid W)$, we can expand each term:\n$$\np^{*}_{g} = P(Y^{*}=1 \\mid Y=1, g) P(Y=1 \\mid g) + P(Y^{*}=1 \\mid Y=0, g) P(Y=0 \\mid g)\n$$\nWe recognize the terms in this expression from the problem statement:\n-   $P(Y=1 \\mid g)$ is the true risk, $p_{g}$.\n-   $P(Y=0 \\mid g) = 1 - P(Y=1 \\mid g) = 1 - p_{g}$.\n-   $P(Y^{*}=1 \\mid Y=1, g)$ is the sensitivity of the test for group $g$, $Se_{g}$.\n-   $P(Y^{*}=1 \\mid Y=0, g)$ is the false positive rate. This is related to the specificity, $Sp_{g} = P(Y^{*}=0 \\mid Y=0, g)$, by the relation $P(Y^{*}=1 \\mid Y=0, g) = 1 - Sp_{g}$.\n\nSubstituting these definitions into the equation yields the desired expression for the observed risk:\n$$\np^{*}_{g} = (Se_{g} \\cdot p_{g}) + (1 - Sp_{g}) \\cdot (1 - p_{g})\n$$\nThis formula shows that the observed prevalence is a sum of the true positives detected by the test and the false positives generated by the test.\n\n### Part 2: Calculation of Observed Disparity Measures $RD^{*}$ and $RR^{*}$\nFirst, we calculate the true disparity measures, $RD$ and $RR$, using the given true risks, $p_{A}=0.15$ and $p_{B}=0.10$.\n$$\nRD = p_{A} - p_{B} = 0.15 - 0.10 = 0.05\n$$\n$$\nRR = \\frac{p_{A}}{p_{B}} = \\frac{0.15}{0.10} = 1.5\n$$\nNext, we apply the formula derived in Part 1 to compute the observed risks $p^{*}_{A}$ and $p^{*}_{B}$ using the given sensitivities and specificities.\n\nFor group $A$: $p_{A} = 0.15$, $Se_{A} = 0.85$, $Sp_{A} = 0.95$.\n$$\np^{*}_{A} = (Se_{A} \\cdot p_{A}) + (1 - Sp_{A}) \\cdot (1 - p_{A})\n$$\n$$\np^{*}_{A} = (0.85 \\cdot 0.15) + (1 - 0.95) \\cdot (1 - 0.15)\n$$\n$$\np^{*}_{A} = (0.1275) + (0.05) \\cdot (0.85) = 0.1275 + 0.0425 = 0.1700\n$$\nFor group $B$: $p_{B} = 0.10$, $Se_{B} = 0.70$, $Sp_{B} = 0.98$.\n$$\np^{*}_{B} = (Se_{B} \\cdot p_{B}) + (1 - Sp_{B}) \\cdot (1 - p_{B})\n$$\n$$\np^{*}_{B} = (0.70 \\cdot 0.10) + (1 - 0.98) \\cdot (1 - 0.10)\n$$\n$$\np^{*}_{B} = (0.07) + (0.02) \\cdot (0.90) = 0.07 + 0.018 = 0.0880\n$$\nNow, we can compute the observed disparity measures, $RD^{*}$ and $RR^{*}$.\n$$\nRD^{*} = p^{*}_{A} - p^{*}_{B} = 0.1700 - 0.0880 = 0.0820\n$$\n$$\nRR^{*} = \\frac{p^{*}_{A}}{p^{*}_{B}} = \\frac{0.1700}{0.0880} \\approx 1.931818...\n$$\n\n### Part 3: Calculation of Bias\nThe additive bias for each measure is the difference between the observed measure and the true measure.\n\nFor the Risk Difference ($RD$):\n$$\n\\text{Bias}_{RD} = RD^{*} - RD = 0.0820 - 0.05 = 0.0320\n$$\nSince the bias is positive, the differential misclassification leads to an **overestimation** of the true risk difference. The magnitude of the bias, rounded to four significant figures, is $0.03200$.\n\nFor the Risk Ratio ($RR$):\n$$\n\\text{Bias}_{RR} = RR^{*} - RR = \\frac{0.1700}{0.0880} - 1.5 = \\frac{170}{88} - \\frac{3}{2} = \\frac{85}{44} - \\frac{66}{44} = \\frac{19}{44}\n$$\n$$\n\\text{Bias}_{RR} \\approx 0.431818...\n$$\nSince this bias is also positive, the differential misclassification also leads to an **overestimation** of the true risk ratio. The magnitude of the bias, rounded to four significant figures, is $0.4318$.\n\nThe two numeric bias quantities are $0.03200$ for the risk difference and $0.4318$ for the risk ratio.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.03200 & 0.4318 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "As algorithms become integral to clinical decision-making, epidemiologists must be equipped to evaluate their impact on health equity. This exercise moves from traditional metrics to the cutting-edge field of algorithmic fairness. You will assess a risk stratification tool using two key fairness criteria: Equalized Odds, which scrutinizes error rates, and Predictive Parity, which ensures a positive prediction has the same meaning for all groups . This practice demonstrates the inherent mathematical trade-offs between different definitions of fairness, particularly when disease prevalence varies by population—a common feature of health disparities.",
            "id": "4595755",
            "problem": "You are evaluating a binary risk stratification algorithm for a chronic disease in two demographic groups, labeled Group A and Group B. For each group, you are given counts from a $2 \\times 2$ contingency table comparing the algorithm’s predicted label $\\hat{Y} \\in \\{0,1\\}$ with the true outcome $Y \\in \\{0,1\\}$. Let $TP$ be the count of true positives, $FP$ the count of false positives, $TN$ the count of true negatives, and $FN$ the count of false negatives.\n\nYour task is to implement a program that, for each test case, computes across the two groups:\n- The positive class prevalence (base rate) $\\pi_g = \\dfrac{TP_g + FN_g}{TP_g + FP_g + TN_g + FN_g}$ for group $g \\in \\{\\text{A},\\text{B}\\}$.\n- The True Positive Rate (TPR) $=\\dfrac{TP_g}{TP_g + FN_g}$, the False Positive Rate (FPR) $=\\dfrac{FP_g}{FP_g + TN_g}$, and the Positive Predictive Value (PPV) $=\\dfrac{TP_g}{TP_g + FP_g}$, computed separately for each group.\n- A boolean indicating whether Equalized Odds (EO) holds between the two groups, defined as both $|\\text{TPR}_{\\text{A}} - \\text{TPR}_{\\text{B}}| \\le \\epsilon$ and $|\\text{FPR}_{\\text{A}} - \\text{FPR}_{\\text{B}}| \\le \\epsilon$, with tolerance $\\epsilon = 10^{-6}$.\n- A boolean indicating whether Predictive Parity (PP) holds between the two groups, defined as $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}| \\le \\epsilon$, with tolerance $\\epsilon = 10^{-6}$.\n\nHandling undefined quantities:\n- If a denominator is zero for a rate in a group, that rate is considered undefined in that group. For the parity checks, treat two undefined rates (one in each group for the same metric) as equal; if one rate is undefined and the other is defined, treat them as not equal. For the summary differences requested below, if both PPVs are undefined, set the absolute PPV difference to $0.0$.\n\nFor each test case, your program must output a list with four entries in the following order:\n- A boolean for EO holding ($\\text{True}$ or $\\text{False}$).\n- A boolean for PP holding ($\\text{True}$ or $\\text{False}$).\n- The absolute difference in base rates $|\\pi_{\\text{A}} - \\pi_{\\text{B}}|$, rounded to six decimal places.\n- The absolute difference in PPVs $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|$, rounded to six decimal places, with the undefined-undefined case set to $0.0$ as specified above.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these per-case lists, enclosed in square brackets, for example: $[[\\text{True},\\text{False},0.125000,0.031250],[\\dots]]$. All fractional results must be expressed as decimals; do not use a percentage sign.\n\nUse the following test suite of four cases. Each case provides the eight nonnegative integers $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}},TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}})$:\n\n- Case 1 (identical error rates but different base rates; expect Equalized Odds to hold and Predictive Parity to fail):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (80, 20, 80, 20)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (40, 30, 120, 10)$\n\n- Case 2 (equal PPVs but different error rates; expect Predictive Parity to hold and Equalized Odds to fail):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (50, 50, 50, 50)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (30, 30, 120, 20)$\n\n- Case 3 (degenerate always-negative classifier in both groups; treat both PPVs as undefined and equal, so both parities hold):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (0, 0, 90, 10)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (0, 0, 80, 20)$\n\n- Case 4 (same base rates and same error rates; expect both parities to hold):\n  - $(TP_{\\text{A}},FP_{\\text{A}},TN_{\\text{A}},FN_{\\text{A}}) = (42, 28, 112, 18)$\n  - $(TP_{\\text{B}},FP_{\\text{B}},TN_{\\text{B}},FN_{\\text{B}}) = (21, 14, 56, 9)$\n\nYour program must hard-code the above test cases in this order and the tolerance $\\epsilon = 10^{-6}$. It must output a single line: a list of four sublists (one per case) in the exact order of the cases, where each sublist is $[\\text{EO},\\text{PP},|\\pi_{\\text{A}} - \\pi_{\\text{B}}|,|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|]$ with the two floating-point entries rounded to six decimal places as specified.",
            "solution": "The analysis proceeds by first defining the necessary quantities based on the provided counts from a $2 \\times 2$ contingency table for each group $g \\in \\{\\text{A}, \\text{B}\\}$: True Positives ($TP_g$), False Positives ($FP_g$), True Negatives ($TN_g$), and False Negatives ($FN_g$).\n\nThe core quantities to be calculated for each group $g$ are:\n1.  **Total Condition Positive**: The number of individuals who truly have the condition.\n    $$P_g = TP_g + FN_g$$\n2.  **Total Condition Negative**: The number of individuals who truly do not have the condition.\n    $$N'_g = FP_g + TN_g$$\n3.  **Total Predicted Positive**: The number of individuals predicted by the algorithm to have the condition.\n    $$\\hat{P}_g = TP_g + FP_g$$\n4.  **Total Population**: The total number of individuals in the group.\n    $$N_{total, g} = TP_g + FP_g + TN_g + FN_g = P_g + N'_g$$\n\nFrom these, we can define the required rates for each group $g$:\n-   **Positive Class Prevalence (Base Rate)**, $\\pi_g$: The proportion of individuals in the group who have the condition.\n    $$\\pi_g = \\frac{P_g}{N_{total, g}} = \\frac{TP_g + FN_g}{TP_g + FP_g + TN_g + FN_g}$$\n-   **True Positive Rate (TPR)**, also known as sensitivity or recall: The proportion of individuals with the condition who are correctly identified by the algorithm.\n    $$\\text{TPR}_g = \\frac{TP_g}{P_g} = \\frac{TP_g}{TP_g + FN_g}$$\n    This is undefined if the denominator $P_g = 0$.\n-   **False Positive Rate (FPR)**: The proportion of individuals without the condition who are incorrectly identified by the algorithm.\n    $$\\text{FPR}_g = \\frac{FP_g}{N'_g} = \\frac{FP_g}{FP_g + TN_g}$$\n    This is undefined if the denominator $N'_g = 0$.\n-   **Positive Predictive Value (PPV)**, also known as precision: The proportion of individuals predicted to have the condition who truly have it.\n    $$\\text{PPV}_g = \\frac{TP_g}{\\hat{P}_g} = \\frac{TP_g}{TP_g + FP_g}$$\n    This is undefined if the denominator $\\hat{P}_g = 0$.\n\nNext, we evaluate the fairness criteria between Group A and Group B, using a tolerance $\\epsilon = 10^{-6}$.\n\n-   **Equalized Odds (EO)**: This criterion is met if the classifier has equal TPR and FPR across both groups. A special handling for undefined rates is required: two undefined rates (e.g., $\\text{TPR}_\\text{A}$ and $\\text{TPR}_\\text{B}$) are considered equal, while a defined rate and an undefined rate are considered not equal. Formally, EO holds if both of the following conditions are met:\n    1.  The TPRs are considered equal: $|\\text{TPR}_{\\text{A}} - \\text{TPR}_{\\text{B}}| \\le \\epsilon$, or both rates are undefined.\n    2.  The FPRs are considered equal: $|\\text{FPR}_{\\text{A}} - \\text{FPR}_{\\text{B}}| \\le \\epsilon$, or both rates are undefined.\n\n-   **Predictive Parity (PP)**: This criterion is met if the classifier has equal PPV across both groups, subject to the same handling of undefined rates. Formally, PP holds if:\n    1.  The PPVs are considered equal: $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}| \\le \\epsilon$, or both rates are undefined.\n\nFinally, for each test case, we must assemble a list containing four values:\n1.  A boolean value indicating if EO holds ($\\text{True}$ or $\\text{False}$).\n2.  A boolean value indicating if PP holds ($\\text{True}$ or $\\text{False}$).\n3.  The absolute difference in base rates, $|\\pi_{\\text{A}} - \\pi_{\\text{B}}|$.\n4.  The absolute difference in PPVs, $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|$. According to the problem specification, if both $\\text{PPV}_{\\text{A}}$ and $\\text{PPV}_{\\text{B}}$ are undefined, this difference is to be set to $0.0$.\n\nThe overall algorithm is as follows:\n1.  For each test case, retrieve the eight input values: $(TP_{\\text{A}}, FP_{\\text{A}}, TN_{\\text{A}}, FN_{\\text{A}}, TP_{\\text{B}}, FP_{\\text{B}}, TN_{\\text{B}}, FN_{\\text{B}})$.\n2.  Define a procedure that takes $(TP, FP, TN, FN)$ as input and calculates the rates $(\\pi, \\text{TPR}, \\text{FPR}, \\text{PPV})$, returning a special value (e.g., `None` in Python) for any rate where the denominator is zero.\n3.  Apply this procedure to the data for Group A and Group B to obtain their respective rates.\n4.  Implement a comparison function that takes two rates and the tolerance $\\epsilon$ and returns `True` if they are considered equal according to the problem's rules (i.e., absolute difference is within tolerance, or both are undefined).\n5.  Use this comparison function to check for EO (comparing TPRs and FPRs) and PP (comparing PPVs).\n6.  Calculate the absolute difference in base rates, $|\\pi_{\\text{A}} - \\pi_{\\text{B}}|$.\n7.  Calculate the absolute difference in PPVs, $|\\text{PPV}_{\\text{A}} - \\text{PPV}_{\\text{B}}|$, applying the special rule for the case where both are undefined.\n8.  Format the four resulting values into a list, rounding the floating-point numbers to six decimal places.\n9.  Collect the lists from all test cases into a final list and format it into the required string output.\n\n**Example Walkthrough: Case 1**\n-   Group A: $(TP_{\\text{A}}, FP_{\\text{A}}, TN_{\\text{A}}, FN_{\\text{A}}) = (80, 20, 80, 20)$\n    -   $P_A = 80+20=100$, $N'_A = 20+80=100$, $\\hat{P}_A = 80+20=100$, $N_{total,A} = 200$.\n    -   $\\pi_A = 100/200 = 0.5$, $\\text{TPR}_A = 80/100 = 0.8$, $\\text{FPR}_A = 20/100 = 0.2$, $\\text{PPV}_A = 80/100=0.8$.\n-   Group B: $(TP_{\\text{B}}, FP_{\\text{B}}, TN_{\\text{B}}, FN_{\\text{B}}) = (40, 30, 120, 10)$\n    -   $P_B = 40+10=50$, $N'_B = 30+120=150$, $\\hat{P}_B = 40+30=70$, $N_{total,B} = 200$.\n    -   $\\pi_B = 50/200 = 0.25$, $\\text{TPR}_B = 40/50 = 0.8$, $\\text{FPR}_B = 30/150 = 0.2$, $\\text{PPV}_B = 40/70 \\approx 0.571429$.\n-   **EO Check**: $|\\text{TPR}_A - \\text{TPR}_B| = |0.8 - 0.8| = 0 \\le \\epsilon$. $|\\text{FPR}_A - \\text{FPR}_B| = |0.2 - 0.2| = 0 \\le \\epsilon$. Both pass. EO is $\\text{True}$.\n-   **PP Check**: $|\\text{PPV}_A - \\text{PPV}_B| = |0.8 - 40/70| \\approx 0.228571 > \\epsilon$. Fails. PP is $\\text{False}$.\n-   **Differences**:\n    -   $|\\pi_A - \\pi_B| = |0.5 - 0.25| = 0.25$.\n    -   $|\\text{PPV}_A - \\text{PPV}_B| \\approx 0.228571$.\n-   **Result**: $[\\text{True}, \\text{False}, 0.250000, 0.228571]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted.\n\ndef solve():\n    \"\"\"\n    Computes fairness metrics and differences for a binary classifier\n    across two groups for a given suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (80, 20, 80, 20, 40, 30, 120, 10),\n        # Case 2\n        (50, 50, 50, 50, 30, 30, 120, 20),\n        # Case 3\n        (0, 0, 90, 10, 0, 0, 80, 20),\n        # Case 4\n        (42, 28, 112, 18, 21, 14, 56, 9),\n    ]\n\n    epsilon = 1e-6\n    all_results = []\n\n    def calculate_metrics(tp, fp, tn, fn):\n        \"\"\"\n        Calculates prevalence and performance metrics. Returns None for undefined rates.\n        \"\"\"\n        total_pop = tp + fp + tn + fn\n        \n        # Denominators\n        p_cond = tp + fn      # Condition Positive\n        n_cond = fp + tn      # Condition Negative\n        p_pred = tp + fp      # Predicted Positive\n\n        pi = p_cond / total_pop if total_pop > 0 else 0.0\n        tpr = tp / p_cond if p_cond > 0 else None\n        fpr = fp / n_cond if n_cond > 0 else None\n        ppv = tp / p_pred if p_pred > 0 else None\n\n        return pi, tpr, fpr, ppv\n\n    def are_rates_equal(rate_a, rate_b, tol):\n        \"\"\"\n        Compares two rates according to the problem's rules for undefined values.\n        \"\"\"\n        if rate_a is None and rate_b is None:\n            return True\n        if rate_a is None or rate_b is None:\n            return False\n        return abs(rate_a - rate_b) = tol\n\n    for case in test_cases:\n        tp_a, fp_a, tn_a, fn_a, tp_b, fp_b, tn_b, fn_b = case\n\n        pi_a, tpr_a, fpr_a, ppv_a = calculate_metrics(tp_a, fp_a, tn_a, fn_a)\n        pi_b, tpr_b, fpr_b, ppv_b = calculate_metrics(tp_b, fp_b, tn_b, fn_b)\n\n        # Evaluate Equalized Odds (EO)\n        eo_holds = are_rates_equal(tpr_a, tpr_b, epsilon) and \\\n                   are_rates_equal(fpr_a, fpr_b, epsilon)\n\n        # Evaluate Predictive Parity (PP)\n        pp_holds = are_rates_equal(ppv_a, ppv_b, epsilon)\n\n        # Calculate absolute difference in base rates\n        pi_diff = abs(pi_a - pi_b)\n\n        # Calculate absolute difference in PPVs with special handling\n        if ppv_a is None and ppv_b is None:\n            ppv_diff = 0.0\n        # The case where one is None and the other is not is not in the test data,\n        # so a simple calculation is sufficient for the valid test cases.\n        else:\n            ppv_diff = abs(ppv_a - ppv_b)\n\n        # Append formatted results for the current case\n        all_results.append([eo_holds, pp_holds, pi_diff, ppv_diff])\n\n    # Format the final output string exactly as required, avoiding extra spaces.\n    formatted_results = []\n    for res in all_results:\n        # Format: [bool,bool,float,float] without spaces\n        # Python's str(True) is 'True', which is correct.\n        # f-string formatting handles rounding to 6 decimal places.\n        formatted_str = f\"[{res[0]},{res[1]},{res[2]:.6f},{res[3]:.6f}]\"\n        formatted_results.append(formatted_str)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}