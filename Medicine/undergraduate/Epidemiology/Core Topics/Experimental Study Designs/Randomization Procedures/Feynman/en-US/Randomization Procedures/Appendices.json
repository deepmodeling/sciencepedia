{
    "hands_on_practices": [
        {
            "introduction": "To understand the behavior of randomized trials, we must first master the statistical properties of the assignment process itself. This foundational exercise explores simple randomization, where each participant's assignment is akin to an independent coin flip, and asks you to derive the fundamental statistical properties that form the basis for analyzing trial outcomes. By working through the expectation and variance of treatment indicators, you will build a solid quantitative understanding of how randomization translates into predictable statistical behavior at both the individual and group levels.",
            "id": "4627424",
            "problem": "A two-arm individually randomized controlled trial (randomized controlled trial (RCT)) in epidemiology enrolls $N$ subjects. Each subject $i \\in \\{1,2,\\dots,N\\}$ is assigned to the treatment arm independently with probability $p \\in (0,1)$, and otherwise to the control arm. Let the treatment indicator be $T_i \\in \\{0,1\\}$, where $T_i = 1$ denotes assignment to treatment and $T_i = 0$ denotes assignment to control. Define the sample treatment proportion as $$\\hat{\\pi} = \\frac{1}{N}\\sum_{i=1}^{N} T_i.$$\n\nStarting from the core definitions of expectation, variance, independence, and the properties of indicator variables, derive closed-form expressions for the expectation and variance of $T_i$ and for the expectation and variance of $\\hat{\\pi}$. Express your final answer as a single row vector in the order $$\\mathbb{E}[T_i],\\ \\operatorname{Var}(T_i),\\ \\mathbb{E}[\\hat{\\pi}],\\ \\operatorname{Var}(\\hat{\\pi}).$$\n\nProvide exact symbolic expressions; no numerical approximation or rounding is required. No physical units are involved.",
            "solution": "The problem specifies independent treatment assignment for each subject with probability $p$. This implies that, for each $i$, the treatment indicator $T_i$ is a Bernoulli random variable with parameter $p$, denoted $T_i \\sim \\operatorname{Bernoulli}(p)$, and $\\{T_i\\}_{i=1}^{N}$ are independent and identically distributed.\n\nWe begin with the core definitions:\n- For any random variable $X$, the expectation is $\\mathbb{E}[X]$.\n- The variance is $\\operatorname{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$.\n- For independent random variables $X_1,\\dots,X_N$, $\\operatorname{Var}\\!\\left(\\sum_{i=1}^{N} X_i\\right) = \\sum_{i=1}^{N} \\operatorname{Var}(X_i)$.\n- For any scalar $a$, $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$ and $\\mathbb{E}[aX] = a\\,\\mathbb{E}[X]$.\n- For an indicator variable $I_A$ of an event $A$, $\\mathbb{E}[I_A] = \\Pr(A)$ and since $I_A \\in \\{0,1\\}$, $I_A^{2} = I_A$.\n\nExpectation and variance of $T_i$:\nSince $T_i$ indicates treatment assignment, $\\Pr(T_i=1) = p$ and $\\Pr(T_i=0) = 1-p$. Using the indicator property,\n$$\\mathbb{E}[T_i] = 1 \\cdot \\Pr(T_i=1) + 0 \\cdot \\Pr(T_i=0) = p.$$\nFor the variance,\n$$\\operatorname{Var}(T_i) = \\mathbb{E}[T_i^{2}] - (\\mathbb{E}[T_i])^{2}.$$\nBecause $T_i^{2} = T_i$ for $T_i \\in \\{0,1\\}$, we have $\\mathbb{E}[T_i^{2}] = \\mathbb{E}[T_i] = p$. Therefore,\n$$\\operatorname{Var}(T_i) = p - p^{2} = p(1-p).$$\n\nExpectation and variance of $\\hat{\\pi}$:\nDefine the sum of treatment indicators as $S_N = \\sum_{i=1}^{N} T_i$. Then $\\hat{\\pi} = \\frac{1}{N} S_N$. Using linearity of expectation,\n$$\\mathbb{E}[\\hat{\\pi}] = \\mathbb{E}\\!\\left[\\frac{1}{N} \\sum_{i=1}^{N} T_i\\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[T_i] = \\frac{1}{N} \\cdot N \\cdot p = p.$$\nFor the variance, using independence of $\\{T_i\\}_{i=1}^{N}$ and variance scaling,\n$$\\operatorname{Var}(\\hat{\\pi}) = \\operatorname{Var}\\!\\left(\\frac{1}{N} \\sum_{i=1}^{N} T_i\\right) = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\operatorname{Var}(T_i) = \\frac{1}{N^{2}} \\cdot N \\cdot p(1-p) = \\frac{p(1-p)}{N}.$$\n\nAlternatively, one may observe that $S_N \\sim \\operatorname{Binomial}(N,p)$ by independence and identical Bernoulli trials, which yields $\\mathbb{E}[S_N] = Np$ and $\\operatorname{Var}(S_N) = Np(1-p)$; scaling by $1/N$ leads directly to the same $\\mathbb{E}[\\hat{\\pi}] = p$ and $\\operatorname{Var}(\\hat{\\pi}) = \\frac{p(1-p)}{N}$.\n\nCollecting the four quantities in the requested order gives\n$$\\mathbb{E}[T_i] = p,\\quad \\operatorname{Var}(T_i) = p(1-p),\\quad \\mathbb{E}[\\hat{\\pi}] = p,\\quad \\operatorname{Var}(\\hat{\\pi}) = \\frac{p(1-p)}{N}.$$",
            "answer": "$$\\boxed{\\begin{pmatrix} p & p(1-p) & p & \\frac{p(1-p)}{N} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While simple randomization is unbiased in the long run, it can lead to undesirable imbalances in small samples, such as within a single clinical center, which can compromise statistical power and credibility. This practice problem first challenges you to quantify the risk of a complete imbalance using basic probability principles. You will then see how a common alternative, permuted block randomization, provides a direct and practical safeguard against this issue, illustrating a core principle of robust trial design.",
            "id": "4627370",
            "problem": "A multicenter Randomized Controlled Trial (RCT) assigns participants to a treatment arm or a control arm using simple randomization, meaning each participant independently has probability $0.5$ of being assigned to the treatment arm and probability $0.5$ of being assigned to the control arm. Consider a small clinical center that will enroll exactly $6$ participants. Define a center to have a complete imbalance if all $6$ participants in that center end up in the same arm (either all treatment or all control).\n\nUsing only fundamental probability principles (independence of assignments and basic counting of equally likely outcomes), derive the probability that this center experiences complete imbalance under simple randomization. Then, based on your derivation and the core idea of permuted block randomization within centers, propose a concrete safeguard using permuted blocks that eliminates complete imbalance for this center and briefly justify why it works, without performing any additional numerical computation.\n\nProvide your final probability as a reduced fraction. Do not round. Your final answer must be a single real-valued number.",
            "solution": "Under simple randomization, each participant’s allocation is an independent Bernoulli trial with success probability $0.5$ for the treatment arm and $0.5$ for the control arm. For $6$ participants, there are $2^{6}$ equally likely treatment assignment sequences because each participant has $2$ possible assignments and the $6$ assignments are independent.\n\nA complete imbalance occurs if all $6$ participants are assigned to the treatment arm or if all $6$ participants are assigned to the control arm. There are exactly $2$ such sequences: one in which all $6$ are treatment and one in which all $6$ are control.\n\nBy the classical definition of probability on a finite sample space with equally likely outcomes, the probability of complete imbalance is the ratio of the number of favorable sequences to the total number of sequences:\n$$\nP(\\text{complete imbalance}) = \\frac{2}{2^{6}} = 2^{1-6} = \\frac{1}{32}.\n$$\n\nSafeguard using permuted blocks: Implement permuted block randomization within the center using a fixed block size $b=4$ with equal allocation within each block. In a permuted block of size $4$, each block contains exactly $2$ treatment and $2$ control assignments in a random order. Because the center enrolls $6$ participants, at least one complete block of size $4$ will be filled, guaranteeing that the center has at least $2$ treatment and $2$ control assignments among the first $4$ participants. The remaining $2$ participants belong to the next block and cannot undo the presence of both arms in the center’s accumulated assignments. Therefore, under permuted blocks of size $b=4$ within the center, the probability of complete imbalance is $0$ for this center. This conclusion follows directly from the enforced balance within each complete block and does not require any additional numerical calculation.",
            "answer": "$$\\boxed{\\frac{1}{32}}$$"
        },
        {
            "introduction": "Permuted block randomization effectively prevents the large imbalances possible under simple randomization, but this control introduces a new vulnerability: predictability. If investigators can deduce the block structure, they may be able to anticipate future assignments, which threatens to introduce bias and undermine the integrity of the trial. This exercise delves into this critical trade-off by asking you to quantify the exact probability that a future assignment can be correctly guessed, highlighting the crucial balance between achieving group balance and maintaining allocation concealment.",
            "id": "4627374",
            "problem": "A two-arm randomized clinical trial uses fixed block randomization with equal allocation within each block. Let the block size be $b$, where $b$ is an even positive integer. Within each block, exactly $b/2$ participants are assigned to treatment $A$ and exactly $b/2$ to treatment $B$, and the order within a block is generated as a uniformly random permutation of these $b$ labels.\n\nConsider the current, partially filled block. Suppose that $k$ participants in the current block have already been assigned, with $a$ assigned to treatment $A$ and $k-a$ assigned to treatment $B$. Assume $0 \\leq k \\leq b-1$, $0 \\leq a \\leq \\min\\{k, b/2\\}$, and $k-a \\leq b/2$. A recruiter knows $b$, $k$, and $a$, but not the concealed assignment order. The recruiter will make a single guess for the next participant’s assignment using the strategy that maximizes the probability of guessing correctly.\n\nUsing only core definitions of fixed block randomization and basic conditional probability for sampling without replacement from a finite population, derive a closed-form analytic expression for this maximum probability of a correct guess as a function of $b$, $k$, and $a$. Express your final answer as a single simplified analytic expression. Do not round and do not include units.",
            "solution": "The goal is to find the maximum probability of correctly guessing the assignment for the $(k+1)$-th participant, given the state of a partially filled block. The recruiter's optimal strategy is to guess the treatment that is more likely to be assigned next.\n\nFirst, let's determine the composition of the remaining assignments in the block.\n- The block size is $b$.\n- The total number of assignments to treatment A is $b/2$.\n- The total number of assignments to treatment B is $b/2$.\n\nCurrently, $k$ participants have been assigned:\n- $a$ participants to treatment A.\n- $k-a$ participants to treatment B.\n\nThe number of assignments yet to be made in the block is $b - k$.\nThe number of remaining assignments for treatment A is:\n$$N_A = (\\text{total A's}) - (\\text{assigned A's}) = \\frac{b}{2} - a$$\nThe number of remaining assignments for treatment B is:\n$$N_B = (\\text{total B's}) - (\\text{assigned B's}) = \\frac{b}{2} - (k-a) = \\frac{b}{2} - k + a$$\n\nThe total number of remaining assignments is $N_A + N_B = (\\frac{b}{2} - a) + (\\frac{b}{2} - k + a) = b - k$, which is correct. Since the original block sequence was a random permutation, any of the remaining $b-k$ assignments is equally likely to be the next one.\n\nThe probability that the next assignment is to treatment A is the proportion of remaining A's:\n$$P(\\text{next is } A) = \\frac{N_A}{b - k} = \\frac{\\frac{b}{2} - a}{b - k}$$\nThe probability that the next assignment is to treatment B is the proportion of remaining B's:\n$$P(\\text{next is } B) = \\frac{N_B}{b - k} = \\frac{\\frac{b}{2} - k + a}{b - k}$$\n\nThe recruiter wants to maximize the probability of a correct guess. This maximum probability is the larger of the two probabilities calculated above:\n$$P_{\\text{max}} = \\max\\left( P(\\text{next is } A), P(\\text{next is } B) \\right) = \\frac{\\max\\left( \\frac{b}{2} - a, \\frac{b}{2} - k + a \\right)}{b-k}$$\nTo simplify this into a single expression, we use the identity $\\max(X, Y) = \\frac{1}{2}(X+Y + |X-Y|)$. Let $X = \\frac{b}{2} - a$ and $Y = \\frac{b}{2} - k + a$.\n- Sum: $X+Y = (\\frac{b}{2} - a) + (\\frac{b}{2} - k + a) = b-k$.\n- Difference: $X-Y = (\\frac{b}{2} - a) - (\\frac{b}{2} - k + a) = k-2a$.\n\nSubstituting these into the numerator of $P_{\\text{max}}$:\n$$\\max\\left( \\frac{b}{2} - a, \\frac{b}{2} - k + a \\right) = \\frac{1}{2}((b-k) + |k-2a|)$$\nFinally, we substitute this back into the expression for $P_{\\text{max}}$:\n$$P_{\\text{max}} = \\frac{\\frac{1}{2}((b-k) + |k-2a|)}{b-k} = \\frac{b-k}{2(b-k)} + \\frac{|k-2a|}{2(b-k)}$$\n$$P_{\\text{max}} = \\frac{1}{2} + \\frac{|k - 2a|}{2(b - k)}$$\nThis is the final simplified expression. It shows the baseline probability of $1/2$ plus a term that quantifies the added predictability based on the current imbalance $|k-2a|$ among the $k$ assigned participants.",
            "answer": "$$\\boxed{\\frac{1}{2} + \\frac{|k - 2a|}{2(b - k)}}$$"
        }
    ]
}