{
    "hands_on_practices": [
        {
            "introduction": "An epidemic curve's shape can hint at the underlying transmission dynamics. While a simple, homogeneous transmission process might produce a smooth, single-peaked curve, real-world events like super-spreading can create a much more volatile pattern. This exercise  introduces a statistical tool to formally test whether the observed daily counts are more variable than expected under a simple Poisson model, providing quantitative evidence for the presence of overdispersion.",
            "id": "4590076",
            "problem": "An outbreak of a respiratory infection occurs on a university campus. The epidemic curve (histogram of incident cases in equal-duration daily bins) shows two conspicuous peaks and a long tail. Investigators suspect that super-spreading events have occurred, producing a curve with multiple modes and heavy tails. Using the definition of an epidemic curve as daily counts and the well-tested property of a homogeneous-count Poisson process that each bin has independent increments with equal mean and variance, and using the Central Limit Theorem (CLT) approximation for sums of standardized residuals, derive from first principles a goodness-of-fit statistic based solely on the curve’s bin counts that quantifies overdispersion relative to the Poisson assumption. Then, compute its numerical value for the following $12$ consecutive daily counts:\n$(x_1,\\dots,x_{12})=(2,3,4,5,22,7,5,4,3,18,6,5)$.\nRound your final numeric answer to four significant figures and express it as a dimensionless number.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of epidemiology and statistics, well-posed with sufficient and consistent information, and objectively phrased. The task is a standard exercise in biostatistics: deriving and calculating a goodness-of-fit statistic to test for overdispersion in count data against a Poisson model.\n\nThe objective is to derive a goodness-of-fit statistic that quantifies overdispersion in daily case counts, relative to the assumption of a homogeneous Poisson process. Let the sequence of daily case counts be denoted by $(x_1, x_2, \\dots, x_N)$, where $N$ is the total number of days (bins). In this problem, $N=12$.\n\nThe null hypothesis, $H_0$, is that the counts are generated by a homogeneous Poisson process. This implies that the counts $x_i$ are realizations of $N$ independent and identically distributed (i.i.d.) random variables, $X_1, X_2, \\dots, X_N$, each following a Poisson distribution with a single, constant rate parameter $\\lambda$. We write this as $X_i \\sim \\text{Pois}(\\lambda)$ for $i=1, \\dots, N$.\n\nA fundamental property of the Poisson distribution is that its mean and variance are equal, i.e., $E[X_i] = \\lambda$ and $\\text{Var}(X_i) = \\lambda$. Overdispersion occurs when the observed variance in the data is significantly greater than the mean, which would constitute evidence against the Poisson model.\n\nThe problem directs us to derive the statistic from first principles using standardized residuals and the Central Limit Theorem (CLT). For a single observation $x_i$, the standardized residual under the true model would be:\n$$ Z_i = \\frac{x_i - E[X_i]}{\\sqrt{\\text{Var}(X_i)}} = \\frac{x_i - \\lambda}{\\sqrt{\\lambda}} $$\nUnder $H_0$, the random variables $Z_i$ would have a mean of $0$ and a variance of $1$.\n\nHowever, the parameter $\\lambda$ is unknown and must be estimated from the data. The maximum likelihood estimator (and method of moments estimator) for $\\lambda$ is the sample mean:\n$$ \\hat{\\lambda} = \\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\nWe substitute this estimate $\\bar{x}$ into the expression for the standardized residuals. A goodness-of-fit statistic can be constructed by summing the squares of these estimated standardized residuals. This statistic is the Pearson chi-squared statistic for a Poisson model, often referred to as the Poisson dispersion statistic or index of dispersion statistic. It is defined as:\n$$ D = \\sum_{i=1}^{N} \\left( \\frac{x_i - \\bar{x}}{\\sqrt{\\bar{x}}} \\right)^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}{\\bar{x}} $$\nThis statistic, $D$, measures the total squared deviation of the observations from the estimated mean, scaled by the estimated mean itself. As the numerator and denominator have the same units (counts), the statistic $D$ is a dimensionless quantity. Under the null hypothesis, for sufficiently large $N$ and $\\lambda$, $D$ follows approximately a chi-squared distribution with $N-1$ degrees of freedom. A large value of $D$ relative to $N-1$ suggests that the variance in the data is larger than the mean, indicating overdispersion.\n\nWe now compute the numerical value of this statistic for the given data.\nThe data are the $N=12$ consecutive daily counts:\n$$ (x_1, \\dots, x_{12}) = (2, 3, 4, 5, 22, 7, 5, 4, 3, 18, 6, 5) $$\nFirst, we calculate the sample mean, $\\bar{x}$:\n$$ \\sum_{i=1}^{12} x_i = 2+3+4+5+22+7+5+4+3+18+6+5 = 84 $$\n$$ \\bar{x} = \\frac{84}{12} = 7 $$\nNext, we calculate the sum of the squared deviations from the mean, $\\sum_{i=1}^{12} (x_i - \\bar{x})^2$:\n\\begin{align*}\n\\sum_{i=1}^{12} (x_i - 7)^2 = (2-7)^2 + (3-7)^2 + (4-7)^2 + (5-7)^2 + (22-7)^2 + (7-7)^2 \\\\\n\\quad + (5-7)^2 + (4-7)^2 + (3-7)^2 + (18-7)^2 + (6-7)^2 + (5-7)^2 \\\\\n= (-5)^2 + (-4)^2 + (-3)^2 + (-2)^2 + (15)^2 + (0)^2 \\\\\n\\quad + (-2)^2 + (-3)^2 + (-4)^2 + (11)^2 + (-1)^2 + (-2)^2 \\\\\n= 25 + 16 + 9 + 4 + 225 + 0 + 4 + 9 + 16 + 121 + 1 + 4 \\\\\n= 434\n\\end{align*}\nFinally, we compute the statistic $D$:\n$$ D = \\frac{\\sum_{i=1}^{12} (x_i - \\bar{x})^2}{\\bar{x}} = \\frac{434}{7} = 62 $$\nThe problem requires the answer to be rounded to four significant figures. The exact value is $62$, which expressed to four significant figures is $62.00$.",
            "answer": "$$\\boxed{62.00}$$"
        },
        {
            "introduction": "The bars on an epidemic curve represent observed counts, which are estimates of the true, underlying incidence. This practice  explores how to quantify the uncertainty around these daily estimates by constructing confidence intervals. By comparing intervals from a simple Poisson model to those from a Negative Binomial model, you will directly observe how accounting for overdispersion provides a more realistic—and often much wider—assessment of statistical uncertainty.",
            "id": "4590085",
            "problem": "An acute outbreak in a closed residential community is monitored with an epidemic curve constructed from daily incident case counts, with a bin width of $1$ day. Consider the statistical task of constructing uncertainty bands for individual daily bars on the epidemic curve. Suppose that the data-generating mechanism for a single day’s count is either a Poisson count model or a Negative Binomial count model with a known overdispersion (size) parameter.\n\nStarting from foundational definitions and well-tested facts about count distributions, do the following.\n\n1. Define a two-sided $95\\%$ Confidence Interval (CI) for the expected incident count per day under each of the following assumptions:\n   - Poisson count model with mean $ \\mu $, where the daily count $ X $ has $ \\mathbb{E}[X] = \\mu $ and $ \\mathrm{Var}(X) = \\mu $.\n   - Negative Binomial count model with mean $ \\mu $ and known size parameter $ k  0 $, where the daily count $ X $ has $ \\mathbb{E}[X] = \\mu $ and $ \\mathrm{Var}(X) = \\mu + \\mu^{2}/k $.\n   Use asymptotic normal approximation arguments justified by distributional properties of $ X $ and state clearly any plug-in steps or approximations you use.\n\n2. For a specific day on the epidemic curve, the observed count is $ X = 49 $. Under the Poisson assumption and under the Negative Binomial assumption with size parameter $ k = 10 $, compute the upper bound of the approximate two-sided $95\\%$ CI for $ \\mu $ in each case, using the standard normal quantile $ z_{0.975} = 1.96 $. Then compute the difference between these two upper bounds (Negative Binomial upper bound minus Poisson upper bound).\n\n3. Briefly discuss how these CIs would be visually presented on the epidemic curve and the interpretive implications of the differences in interval width between the Poisson and Negative Binomial assumptions, focusing on how overdispersion manifests in the visualization.\n\nRound the final numerical difference to four significant figures, and express it in incident counts (a pure count unit with no additional physical dimension).",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on standard statistical models used in epidemiology, namely the Poisson and Negative Binomial distributions for count data. The parameters and relationships are correctly defined. The questions are specific, and all necessary data for calculation ($X=49$, $k=10$, $z_{0.975}=1.96$) are provided. The problem is a valid exercise in applied biostatistics.\n\n### Part 1: Confidence Interval Definitions\n\nA two-sided $(1-\\alpha) \\times 100\\%$ confidence interval (CI) for a parameter $\\theta$ can be constructed based on an estimator $\\hat{\\theta}$ which is asymptotically normally distributed. If $\\hat{\\theta} \\approx N(\\theta, \\mathrm{Var}(\\hat{\\theta}))$, then an approximate CI is given by $\\hat{\\theta} \\pm z_{1-\\alpha/2} \\sqrt{\\widehat{\\mathrm{Var}}(\\hat{\\theta})}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution and $\\widehat{\\mathrm{Var}}(\\hat{\\theta})$ is a consistent estimate of the variance of the estimator. For a $95\\%$ CI, $\\alpha=0.05$ and $z_{1-\\alpha/2} = z_{0.975}$.\n\nFor a single day's observation of an incident count $X$, the most direct estimator for the expected count $\\mu$ is the observation itself, so we set $\\hat{\\mu} = X$. The validity of the normal approximation relies on the expected count $\\mu$ being sufficiently large. The variance of our estimator is $\\mathrm{Var}(\\hat{\\mu}) = \\mathrm{Var}(X)$. To construct the CI, we need to estimate this variance. A common approach is the \"plug-in\" method, where we substitute the estimated mean $\\hat{\\mu}=X$ into the expression for the variance.\n\n**1. Poisson Count Model**\n\nUnder the Poisson model, the daily count $X$ is assumed to follow a Poisson distribution with mean $\\mu$. The key properties are:\n$$\n\\mathbb{E}[X] = \\mu\n$$\n$$\n\\mathrm{Var}(X) = \\mu\n$$\nOur estimator for $\\mu$ is $\\hat{\\mu} = X$. The variance of this estimator is $\\mathrm{Var}(X) = \\mu$. We estimate this variance by plugging in our estimate for $\\mu$:\n$$\n\\widehat{\\mathrm{Var}}(X) = \\hat{\\mu} = X\n$$\nTherefore, the approximate two-sided $95\\%$ CI for $\\mu$ under the Poisson assumption is:\n$$\nCI_{\\text{Poisson}} = X \\pm z_{0.975} \\sqrt{X}\n$$\n\n**2. Negative Binomial Count Model**\n\nUnder the Negative Binomial model, with a known size parameter $k  0$, the daily count $X$ has the following properties:\n$$\n\\mathbb{E}[X] = \\mu\n$$\n$$\n\\mathrm{Var}(X) = \\mu + \\frac{\\mu^2}{k}\n$$\nThe term $\\mu^2/k$ accounts for overdispersion, which is the excess variance compared to a Poisson model. Again, we use $\\hat{\\mu} = X$ as our estimator for $\\mu$. The variance of the estimator is $\\mathrm{Var}(X) = \\mu + \\mu^2/k$. Using the plug-in method, we estimate this variance by substituting $X$ for $\\mu$:\n$$\n\\widehat{\\mathrm{Var}}(X) = X + \\frac{X^2}{k}\n$$\nTherefore, the approximate two-sided $95\\%$ CI for $\\mu$ under the Negative Binomial assumption is:\n$$\nCI_{\\text{NB}} = X \\pm z_{0.975} \\sqrt{X + \\frac{X^2}{k}}\n$$\n\n### Part 2: Numerical Calculation\n\nWe are given the observed count $X = 49$ for a specific day. The standard normal quantile for a $95\\%$ CI is $z_{0.975} = 1.96$. For the Negative Binomial model, the size parameter is given as $k = 10$.\n\n**1. Poisson Upper Bound**\n\nThe upper bound of the $95\\%$ CI for $\\mu$ under the Poisson assumption, denoted $U_{\\text{Poisson}}$, is:\n$$\nU_{\\text{Poisson}} = X + z_{0.975} \\sqrt{X}\n$$\nSubstituting the given values:\n$$\nU_{\\text{Poisson}} = 49 + 1.96 \\sqrt{49} = 49 + 1.96 \\times 7 = 49 + 13.72 = 62.72\n$$\n\n**2. Negative Binomial Upper Bound**\n\nThe upper bound of the $95\\%$ CI for $\\mu$ under the Negative Binomial assumption, denoted $U_{\\text{NB}}$, is:\n$$\nU_{\\text{NB}} = X + z_{0.975} \\sqrt{X + \\frac{X^2}{k}}\n$$\nSubstituting the given values:\n$$\nU_{\\text{NB}} = 49 + 1.96 \\sqrt{49 + \\frac{49^2}{10}} = 49 + 1.96 \\sqrt{49 + \\frac{2401}{10}}\n$$\n$$\nU_{\\text{NB}} = 49 + 1.96 \\sqrt{49 + 240.1} = 49 + 1.96 \\sqrt{289.1}\n$$\nCalculating the numerical value:\n$$\nU_{\\text{NB}} \\approx 49 + 1.96 \\times 17.00294 = 49 + 33.32576... \\approx 82.32576\n$$\n\n**3. Difference between Upper Bounds**\n\nThe difference is $U_{\\text{NB}} - U_{\\text{Poisson}}$:\n$$\n\\text{Difference} = 82.32576... - 62.72 = 19.60576...\n$$\nRounding to four significant figures, the difference is $19.61$.\n\n### Part 3: Discussion of Visualization and Interpretation\n\n**Visual Presentation:**\nOn an epidemic curve, which is typically a bar chart showing incident case counts over time (e.g., daily), these confidence intervals would be visualized as vertical error bars centered on each bar. The height of the bar represents the observed count $X$ for that day. The error bar would extend from the lower bound of the CI to the upper bound. For a given day with count $X=49$, the Poisson error bar would span the interval $[49 - 13.72, 49 + 13.72] = [35.28, 62.72]$, while the Negative Binomial error bar would span the interval $[49 - 33.33, 49 + 33.33] \\approx [15.67, 82.33]$.\n\n**Interpretive Implications of Overdispersion:**\nThe core difference between the two models lies in the variance. The variance of the Negative Binomial distribution, $\\mathrm{Var}(X) = \\mu + \\mu^2/k$, is strictly greater than the variance of the Poisson distribution, $\\mathrm{Var}(X) = \\mu$, for any $\\mu  0$ and finite $k  0$. This phenomenon is known as overdispersion, indicating more variability in the data than expected under a simple Poisson process.\n\nThis greater variance directly translates into wider confidence intervals. As calculated, the width of the $95\\%$ CI for $\\mu$ is approximately $2 \\times 13.72 = 27.44$ counts for the Poisson model, but approximately $2 \\times 33.33 = 66.66$ counts for the Negative Binomial model.\n\nVisually, the error bars under the Negative Binomial assumption will be substantially longer than those under the Poisson assumption. This visual difference carries a critical interpretive implication: it reflects a greater degree of uncertainty about the true underlying daily incidence rate $\\mu$. Overdispersion in epidemic data often arises from real-world biological and social phenomena, such as superspreading events where a few individuals cause a large number of secondary infections, or clustering of cases within households or social networks. Such heterogeneity violates the Poisson assumption that every individual has an equal and independent probability of becoming a case in a given time interval.\n\nBy using a Negative Binomial model and displaying the corresponding wider error bars, an epidemiologist more honestly represents the true uncertainty in the daily counts. Relying on a Poisson model when overdispersion is present would lead to erroneously narrow confidence intervals, creating a false sense of precision and potentially underestimating the plausible range of the true incidence rate. This could lead to flawed public health assessments and decisions. The Negative Binomial model provides a more conservative and often more realistic quantification of statistical uncertainty in the presence of transmission heterogeneity.",
            "answer": "$$\n\\boxed{19.61}\n$$"
        },
        {
            "introduction": "Identifying the \"peak\" of an epidemic is crucial for public health planning, but random noise in daily reports makes the exact timing uncertain. This hands-on coding exercise  demonstrates how to use a powerful computational technique called parametric bootstrapping to quantify this uncertainty. By simulating thousands of possible epidemic curves based on the observed data, you will construct a confidence interval for the peak's timing, moving beyond a single point estimate to a more robust statistical range.",
            "id": "4590034",
            "problem": "You are given incident counts over equally spaced time intervals for several hypothetical outbreaks. An epidemic curve is defined as a sequence of incident counts per time interval, where each count is modeled as an independent realization of a counting process. To quantify uncertainty in peak timing, your task is to construct a Confidence Interval (CI) for the peak time using bootstrap resampling of incident counts. For clarity, define peak time as the earliest time index at which the maximum count is observed.\n\nBase assumptions and definitions:\n- Incident counts per interval are modeled as realizations of independent Poisson random variables, a commonly used model for count data. Let the observed count at time index $t$ be $c_t$, where $t$ ranges from $0$ to $T - 1$ and $T$ is the number of intervals in the epidemic curve.\n- The peak time estimator is the earliest index at which the maximum of the observed counts occurs, expressed as an integer index where the first interval is $0$.\n- A Bootstrap Confidence Interval (BCI) is constructed by repeatedly resampling synthetic incident counts and recomputing the peak time for each resample. Use the percentile method: the lower bound is the empirical $q_{\\ell}$ quantile and the upper bound is the empirical $q_u$ quantile of the bootstrap distribution of peak times.\n\nYou must write a complete, runnable program that, for each test case, computes:\n- The observed peak time index (earliest index of the maximum observed count).\n- The lower and upper bounds of a percentile BCI for the peak time using parametric bootstrap resampling where, for each interval $t$, bootstrap counts are independently drawn from a Poisson distribution with mean equal to the observed count $c_t$.\n\nScientific and computational requirements:\n- Use $B$ bootstrap replicates.\n- Use quantile probabilities $q_{\\ell}$ and $q_u$, where $q_{\\ell} \\in (0,1)$ and $q_u \\in (0,1)$ with $q_{\\ell} lt; q_u$, expressed as decimals (not with a percentage sign).\n- Break ties in peak identification by selecting the earliest index among those attaining the maximum.\n- Time units are intervals indexed by integers, which you must report as integer indices. Express the final peak time estimates and bounds as integer indices (unit: time intervals, with the first interval labeled $0$).\n\nTest suite:\nFor each test case, you are provided a tuple consisting of $(\\text{counts}, B, q_{\\ell}, q_u, s)$ where $\\text{counts}$ is the list of incident counts per interval, $B$ is the number of bootstrap replicates, $q_{\\ell}$ is the lower quantile probability, $q_u$ is the upper quantile probability, and $s$ is the random seed for reproducibility.\n\n- Test case $1$ (unimodal curve):\n  - $\\text{counts} = [\\,0,\\,1,\\,3,\\,7,\\,12,\\,20,\\,27,\\,23,\\,15,\\,9,\\,5,\\,3,\\,2,\\,1,\\,0\\,]$\n  - $B = 4000$\n  - $q_{\\ell} = 0.025$\n  - $q_u = 0.975$\n  - $s = 12345$\n\n- Test case $2$ (two equal peaks, tie handled by earliest index):\n  - $\\text{counts} = [\\,0,\\,2,\\,5,\\,9,\\,12,\\,15,\\,15,\\,12,\\,9,\\,5,\\,2,\\,0\\,]$\n  - $B = 4000$\n  - $q_{\\ell} = 0.025$\n  - $q_u = 0.975$\n  - $s = 67890$\n\n- Test case $3$ (flat zeros, boundary case):\n  - $\\text{counts} = [\\,0,\\,0,\\,0,\\,0,\\,0\\,]$\n  - $B = 4000$\n  - $q_{\\ell} = 0.025$\n  - $q_u = 0.975$\n  - $s = 13579$\n\nFinal output specification:\n- For each test case, output a list of three integers $[\\,\\hat{t}_{\\text{peak}},\\,L,\\,U\\,]$, where $\\hat{t}_{\\text{peak}}$ is the observed peak index, $L$ is the lower bound of the percentile BCI, and $U$ is the upper bound of the percentile BCI.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case represented by its own bracketed list (e.g., $[\\,[\\,a,b,c\\,],\\,[\\,d,e,f\\,],\\,[\\,g,h,i\\,]\\,]$). All reported values must be integers in the specified unit (time intervals), with the first interval labeled $0$.",
            "solution": "The problem is well-defined, scientifically sound, and computationally feasible. All necessary data, parameters, and definitions are provided. The assumptions, such as modeling incident counts as independent Poisson random variables and using a parametric bootstrap for confidence interval estimation, are standard and appropriate within the field of epidemiology. The problem is therefore deemed **valid**.\n\nThe task is to compute the observed peak time and a percentile-based Bootstrap Confidence Interval (BCI) for the peak time of several epidemic curves. The peak time is defined as the earliest time index at which the maximum incident count occurs.\n\n**1. Theoretical Framework**\n\nLet the sequence of observed incident counts be denoted by $\\mathbf{c} = (c_0, c_1, \\ldots, c_{T-1})$, where $c_t$ is the count at time index $t$ and $T$ is the total number of time intervals.\n\n**Observed Peak Time Estimator ($\\hat{t}_{\\text{peak}}$):**\nThe peak time estimator, $\\hat{t}_{\\text{peak}}$, is defined as the earliest time index corresponding to the maximum observed count. Mathematically, this is expressed as:\n$$\n\\hat{t}_{\\text{peak}} = \\min \\{ t \\in \\{0, 1, \\ldots, T-1\\} \\mid c_t = \\max_{i=0}^{T-1} c_i \\}\n$$\nThis is equivalent to the `argmax` function in many numerical libraries, which returns the index of the first occurrence of the maximum value.\n\n**Parametric Bootstrap for Uncertainty Estimation:**\nThe problem specifies a parametric bootstrap procedure to quantify the uncertainty in the $\\hat{t}_{\\text{peak}}$ estimate. This involves the following assumptions and steps:\n\n- **Model Assumption:** Each observed count $c_t$ is treated as a realization of an independent Poisson random variable, $C_t \\sim \\text{Poisson}(\\lambda_t)$.\n- **Parameter Estimation:** The rate parameter $\\lambda_t$ of the Poisson distribution for each time $t$ is estimated by the observed count $c_t$. That is, $\\hat{\\lambda}_t = c_t$.\n- **Resampling:** A large number, $B$, of \"bootstrap replicates\" or synthetic epidemic curves are generated. For each replicate $b \\in \\{1, 2, \\ldots, B\\}$, a new sequence of counts $\\mathbf{c}^{*(b)} = (c_0^{*(b)}, c_1^{*(b)}, \\ldots, c_{T-1}^{*(b)})$ is created. Each synthetic count $c_t^{*(b)}$ is an independent random draw from the estimated distribution:\n$$\nc_t^{*(b)} \\sim \\text{Poisson}(\\hat{\\lambda}_t) = \\text{Poisson}(c_t)\n$$\n- **Bootstrap Distribution of the Estimator:** For each synthetic curve $\\mathbf{c}^{*(b)}$, the peak time estimator is re-calculated, yielding a bootstrap peak time $\\hat{t}_{\\text{peak}}^{*(b)}$.\n$$\n\\hat{t}_{\\text{peak}}^{*(b)} = \\min \\{ t \\in \\{0, 1, \\ldots, T-1\\} \\mid c_t^{*(b)} = \\max_{i=0}^{T-1} c_i^{*(b)} \\}\n$$\nThe collection of these $B$ values, $\\{\\hat{t}_{\\text{peak}}^{*(1)}, \\hat{t}_{\\text{peak}}^{*(2)}, \\ldots, \\hat{t}_{\\text{peak}}^{*(B)}\\}$, forms the empirical bootstrap distribution of the peak time.\n\n**Percentile Confidence Interval:**\nThe $(1 - (\\alpha_{\\ell} + \\alpha_u)) \\times 100\\%$ percentile BCI is constructed by taking the empirical quantiles of the sorted bootstrap distribution. Given lower and upper quantile probabilities $q_{\\ell}$ and $q_u$ (where, for a symmetric CI, $q_{\\ell} = \\alpha/2$ and $q_u = 1 - \\alpha/2$), the confidence interval bounds $[L, U]$ are:\n- $L$: The $q_{\\ell}$-th empirical quantile of the bootstrap distribution $\\{\\hat{t}_{\\text{peak}}^*\\}$.\n- $U$: The $q_u$-th empirical quantile of the bootstrap distribution $\\{\\hat{t}_{\\text{peak}}^*\\}$.\n\nThe final values for $\\hat{t}_{\\text{peak}}$, $L$, and $U$ must be reported as integers, as they represent time indices.\n\n**2. Algorithmic Implementation**\n\nFor each test case specified by $(\\text{counts}, B, q_{\\ell}, q_u, s)$:\n\n1.  **Initialization:** Set the random number generator seed to $s$ for reproducibility. Convert the input `counts` list to a NumPy array, let's call it $\\mathbf{c}_{\\text{obs}}$. Let $T$ be the length of this array.\n\n2.  **Calculate Observed Peak Time:** Compute $\\hat{t}_{\\text{peak}}$ from $\\mathbf{c}_{\\text{obs}}$ using `numpy.argmax()`, which directly implements the required definition (earliest index of the maximum).\n\n3.  **Bootstrap Loop:**\n    a. Initialize an empty list or array, `bootstrap_peak_times`, to store the peak time from each replicate.\n    b. Repeat $B$ times:\n        i. Generate a synthetic count vector $\\mathbf{c}^*$ of length $T$. This is done by drawing from Poisson distributions in a vectorized manner, where the $t$-th element is drawn from $\\text{Poisson}(c_{\\text{obs}, t})$. In NumPy, this is `rng.poisson(c_obs)`.\n        ii. Calculate the peak time for this synthetic vector, $\\hat{t}_{\\text{peak}}^* = \\text{np.argmax}(\\mathbf{c}^*)$.\n        iii. Append $\\hat{t}_{\\text{peak}}^*$ to the `bootstrap_peak_times` list.\n\n4.  **Determine Confidence Bounds:**\n    a. After the loop, convert `bootstrap_peak_times` to a NumPy array.\n    b. Calculate the lower bound $L$ and upper bound $U$ by computing the $q_{\\ell}$ and $q_u$ quantiles of the `bootstrap_peak_times` array. The `numpy.quantile()` function is used for this.\n    c. Convert the resulting quantile values, which may be floats due to interpolation, into integers as required by the problem statement. Truncation via casting to `int` is a direct and standard way to satisfy this.\n\n5.  **Format Output:** Store the triplet $[\\hat{t}_{\\text{peak}}, L, U]$ for the current test case. After processing all cases, format the collected results into the specified final string.\n\nThis procedure will be applied to each test case provided. The use of a fixed random seed $s$ ensures that the stochastic bootstrap process yields a deterministic and verifiable answer for each case. For the test case with all zero counts, the Poisson rate parameter $\\lambda$ is $0$ for all time points. A Poisson distribution with $\\lambda=0$ only produces the value $0$. Thus, every bootstrap replicate will also be a vector of zeros, and its peak time will always be $0$, leading to a confidence interval of $[0, 0]$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the observed peak time and a percentile-based Bootstrap Confidence Interval (BCI)\n    for the peak time of epidemic curves.\n    \"\"\"\n    \n    # Test suite: (counts, B, q_l, q_u, s)\n    test_cases = [\n        # Test case 1 (unimodal curve)\n        (\n            [0, 1, 3, 7, 12, 20, 27, 23, 15, 9, 5, 3, 2, 1, 0],\n            4000, 0.025, 0.975, 12345\n        ),\n        # Test case 2 (two equal peaks, tie handled by earliest index)\n        (\n            [0, 2, 5, 9, 12, 15, 15, 12, 9, 5, 2, 0],\n            4000, 0.025, 0.975, 67890\n        ),\n        # Test case 3 (flat zeros, boundary case)\n        (\n            [0, 0, 0, 0, 0],\n            4000, 0.025, 0.975, 13579\n        )\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        counts, B, q_l, q_u, s = case\n        \n        # Convert counts to a NumPy array\n        observed_counts = np.array(counts, dtype=np.int64)\n\n        # Step 1: Calculate the observed peak time (earliest index of max)\n        observed_peak_time = np.argmax(observed_counts)\n\n        # Step 2: Set up the random number generator for reproducibility\n        rng = np.random.default_rng(s)\n\n        # Step 3: Perform parametric bootstrap resampling\n        bootstrap_peak_times = np.zeros(B, dtype=np.int64)\n        \n        # In the edge case where all observed counts are 0, the Poisson parameter is 0.\n        # A Poisson(0) distribution always yields 0. Thus, every bootstrap sample will be\n        # all zeros, and the argmax will always be 0. We can optimize this.\n        if np.all(observed_counts == 0):\n            # bootstrap_peak_times is already initialized to all zeros.\n            pass\n        else:\n            for i in range(B):\n                # Generate one synthetic epidemic curve\n                # Each new count is drawn from a Poisson distribution with mean = observed count\n                synthetic_counts = rng.poisson(lam=observed_counts)\n                \n                # Calculate the peak time for the synthetic curve\n                bootstrap_peak_times[i] = np.argmax(synthetic_counts)\n\n        # Step 4: Construct the percentile confidence interval\n        # Calculate the lower and upper quantiles of the bootstrap distribution\n        # The problem requires integer indices as output. We cast the float result\n        # of the quantile function to int.\n        lower_bound = int(np.quantile(bootstrap_peak_times, q_l))\n        upper_bound = int(np.quantile(bootstrap_peak_times, q_u))\n        \n        # Step 5: Collect results for this case\n        result = [int(observed_peak_time), lower_bound, upper_bound]\n        all_results.append(result)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}