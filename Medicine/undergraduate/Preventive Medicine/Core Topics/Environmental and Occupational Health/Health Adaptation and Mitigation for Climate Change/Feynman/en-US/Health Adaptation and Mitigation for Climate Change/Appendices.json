{
    "hands_on_practices": [
        {
            "introduction": "Before we can design effective adaptation strategies, we must first quantify the health burden of climate-related hazards. This practice demonstrates a fundamental technique in environmental epidemiology: calculating the mortality attributable to an environmental exposure like heat. By applying a temperature-mortality risk function to a distribution of temperatures, you will learn how to estimate the impact of heat on a population and, crucially, how to measure the life-saving benefits of interventions that reduce heat exposure and population vulnerability .",
            "id": "4529533",
            "problem": "A metropolitan health department is quantifying heat-related mortality to inform preventive medicine strategies for climate change adaptation and mitigation. Consider a summer period of $90$ days. Let the baseline daily mortality in the absence of heat stress at a reference temperature $T_0=22~^{\\circ}\\mathrm{C}$ be $M_0=40$ deaths/day. The exposure–response function for temperature is the Relative Risk (RR), defined for a given daily mean temperature $T$ as $RR(T)=\\exp\\!\\big(\\beta\\,(T-T_0)\\big)$, where $\\beta$ is a temperature–mortality slope parameter estimated from epidemiological time-series analyses that adequately control for confounding.\n\nTwo scenarios are observed:\n\n- Pre-intervention (current climate and vulnerability): daily mean temperature $T$ follows a normal distribution with mean $\\mu_{\\mathrm{pre}}=29~^{\\circ}\\mathrm{C}$ and standard deviation $\\sigma_{\\mathrm{pre}}=3~^{\\circ}\\mathrm{C}$, and the estimated slope is $\\beta_{\\mathrm{pre}}=0.02~\\mathrm{per~^{\\circ}\\!C}$.\n\n- Post-intervention (adaptation and mitigation in place): urban greening and cooling centers reduce the urban heat island intensity and population vulnerability such that the daily mean temperature $T$ follows a normal distribution with mean $\\mu_{\\mathrm{post}}=27.5~^{\\circ}\\mathrm{C}$ and standard deviation $\\sigma_{\\mathrm{post}}=2.5~^{\\circ}\\mathrm{C}$, and the slope is reduced to $\\beta_{\\mathrm{post}}=0.016~\\mathrm{per~^{\\circ}\\!C}$.\n\nUsing only fundamental definitions of Relative Risk and expected mortality under exposure distributions, derive from first principles an expression for the expected total attributable mortality over the $90$-day period in each scenario, treating $M_0$ as constant across days and interpreting $T_0$ as the counterfactual reference exposure. Then compute the change in attributable mortality, defined as the post-intervention total attributable mortality minus the pre-intervention total attributable mortality. Round your final numerical answer to three significant figures. Express the final answer in deaths. In one sentence, interpret the causal meaning of the computed change under the assumption that $RR(T)$ represents a causal effect of temperature on mortality conditional on appropriately controlled confounders.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, and complete. All necessary parameters and functional forms are provided, and the scenario is a standard application of environmental epidemiological risk assessment. The problem is therefore deemed valid and a solution will be derived.\n\nThe task is to compute the change in total attributable mortality between a pre-intervention and a post-intervention scenario over a $D=90$-day period.\n\nFirst, we establish the definitions from first principles.\nThe total daily mortality at a given temperature $T$ is the product of the baseline mortality $M_0$ and the Relative Risk $RR(T)$:\n$$ M(T) = M_0 \\cdot RR(T) $$\nThe problem defines the Relative Risk function as:\n$$ RR(T) = \\exp(\\beta(T-T_0)) $$\nwhere $T_0$ is the reference temperature at which $RR(T_0)=1$.\n\nAttributable mortality on a given day is the number of deaths that would not have occurred if the temperature had been at the reference level $T_0$. This is the difference between the total mortality at temperature $T$ and the baseline mortality:\n$$ M_{\\mathrm{attr}}(T) = M(T) - M_0 = M_0 \\cdot RR(T) - M_0 = M_0 \\left( RR(T) - 1 \\right) $$\n\nIn each scenario, the daily mean temperature $T$ is not a fixed value but a random variable following a normal distribution, $T \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Therefore, to find the daily attributable mortality, we must compute the expected value of $M_{\\mathrm{attr}}(T)$:\n$$ E[M_{\\mathrm{attr}}(T)] = E[M_0 (RR(T) - 1)] $$\nSince $M_0$ is a constant, by the linearity of expectation:\n$$ E[M_{\\mathrm{attr}}(T)] = M_0 (E[RR(T)] - 1) $$\n\nThe core of the problem is to calculate the expected Relative Risk, $E[RR(T)]$.\n$$ E[RR(T)] = E[\\exp(\\beta(T - T_0))] = E[\\exp(\\beta T - \\beta T_0)] $$\nUsing the property $\\exp(a+b) = \\exp(a)\\exp(b)$, and since $\\beta$ and $T_0$ are constants:\n$$ E[RR(T)] = E[\\exp(\\beta T) \\cdot \\exp(-\\beta T_0)] = \\exp(-\\beta T_0) E[\\exp(\\beta T)] $$\nThe term $E[\\exp(\\beta T)]$ is, by definition, the moment-generating function (MGF) of the random variable $T$, evaluated at the point $\\beta$. The MGF for a normally distributed random variable $T \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is given by $M_T(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$.\nSubstituting $t=\\beta$, we get:\n$$ E[\\exp(\\beta T)] = \\exp\\left(\\mu \\beta + \\frac{1}{2}\\sigma^2 \\beta^2\\right) $$\nNow, we substitute this back into the expression for $E[RR(T)]$:\n$$ E[RR(T)] = \\exp(-\\beta T_0) \\cdot \\exp\\left(\\mu \\beta + \\frac{1}{2}\\sigma^2 \\beta^2\\right) = \\exp\\left(\\mu\\beta + \\frac{1}{2}\\sigma^2 \\beta^2 - \\beta T_0\\right) $$\n$$ E[RR(T)] = \\exp\\left(\\beta(\\mu - T_0) + \\frac{1}{2}\\beta^2\\sigma^2\\right) $$\n\nThe expected total attributable mortality over the $D$-day period is the daily expected attributable mortality multiplied by the number of days, $D$:\n$$ M_{\\mathrm{attr, total}} = D \\cdot E[M_{\\mathrm{attr}}(T)] = D \\cdot M_0 \\left( E[RR(T)] - 1 \\right) $$\nSubstituting the expression for $E[RR(T)]$, we obtain the general formula for the total attributable mortality for a given scenario:\n$$ M_{\\mathrm{attr, total}} = D \\cdot M_0 \\left[ \\exp\\left(\\beta(\\mu - T_0) + \\frac{1}{2}\\beta^2\\sigma^2\\right) - 1 \\right] $$\nThis is the expression derived from first principles.\n\nWe now apply this formula to the two scenarios provided.\nGeneral parameters: $D=90$, $M_0=40$, $T_0=22$.\n\nFor the pre-intervention scenario:\n$\\mu_{\\mathrm{pre}} = 29$, $\\sigma_{\\mathrm{pre}} = 3$, $\\beta_{\\mathrm{pre}} = 0.02$.\nFirst, calculate the exponent for the exponential function:\n$$ \\text{Exponent}_{\\mathrm{pre}} = \\beta_{\\mathrm{pre}}(\\mu_{\\mathrm{pre}} - T_0) + \\frac{1}{2}\\beta_{\\mathrm{pre}}^2\\sigma_{\\mathrm{pre}}^2 $$\n$$ \\text{Exponent}_{\\mathrm{pre}} = 0.02(29 - 22) + \\frac{1}{2}(0.02)^2(3)^2 $$\n$$ \\text{Exponent}_{\\mathrm{pre}} = 0.02(7) + \\frac{1}{2}(0.0004)(9) = 0.14 + 0.0018 = 0.1418 $$\nNow, calculate the total attributable mortality for the pre-intervention period:\n$$ M_{\\mathrm{attr, total, pre}} = 90 \\cdot 40 \\left[ \\exp(0.1418) - 1 \\right] $$\n$$ M_{\\mathrm{attr, total, pre}} = 3600 \\left[ \\exp(0.1418) - 1 \\right] \\approx 3600(1.152344 - 1) = 3600(0.152344) \\approx 548.438 $$\n\nFor the post-intervention scenario:\n$\\mu_{\\mathrm{post}} = 27.5$, $\\sigma_{\\mathrm{post}} = 2.5$, $\\beta_{\\mathrm{post}} = 0.016$.\nCalculate the exponent for the exponential function:\n$$ \\text{Exponent}_{\\mathrm{post}} = \\beta_{\\mathrm{post}}(\\mu_{\\mathrm{post}} - T_0) + \\frac{1}{2}\\beta_{\\mathrm{post}}^2\\sigma_{\\mathrm{post}}^2 $$\n$$ \\text{Exponent}_{\\mathrm{post}} = 0.016(27.5 - 22) + \\frac{1}{2}(0.016)^2(2.5)^2 $$\n$$ \\text{Exponent}_{\\mathrm{post}} = 0.016(5.5) + \\frac{1}{2}(0.000256)(6.25) = 0.088 + 0.0008 = 0.0888 $$\nNow, calculate the total attributable mortality for the post-intervention period:\n$$ M_{\\mathrm{attr, total, post}} = 90 \\cdot 40 \\left[ \\exp(0.0888) - 1 \\right] $$\n$$ M_{\\mathrm{attr, total, post}} = 3600 \\left[ \\exp(0.0888) - 1 \\right] \\approx 3600(1.092864 - 1) = 3600(0.092864) \\approx 334.310 $$\n\nThe change in attributable mortality is the post-intervention total minus the pre-intervention total:\n$$ \\Delta M_{\\mathrm{attr, total}} = M_{\\mathrm{attr, total, post}} - M_{\\mathrm{attr, total, pre}} $$\n$$ \\Delta M_{\\mathrm{attr, total}} \\approx 334.310 - 548.438 = -214.128 $$\nRounding the final answer to three significant figures gives $-214$.\n\nThe causal interpretation of the computed change, as requested, is as follows: Assuming causality, the adaptation and mitigation interventions are expected to avert approximately $214$ deaths attributable to heat over the $90$-day summer period.",
            "answer": "$$ \\boxed{-214} $$"
        },
        {
            "introduction": "Knowing the overall risk is the first step; acting on it effectively is the next. This exercise moves from risk assessment to risk management by tackling the challenge of designing a heat-health warning system. You will use decision theory to formalize the trade-off between the cost of a false alarm (unnecessary action) and a missed event (failure to act), deriving an optimal, data-driven temperature threshold for triggering public health actions . This practice bridges the gap between epidemiological findings and operational public health decisions.",
            "id": "4529547",
            "problem": "A city’s preventive medicine department is designing a trigger for its Heat Health Action Plan to adapt to increasing extreme heat under climate change. Define the binary health outcome $H \\in \\{0,1\\}$ where $H=1$ denotes that tomorrow’s heat exposure leads to a clinically significant surge in heat-related emergency department visits, and $H=0$ denotes no surge. The department can issue a preemptive heat-health action $A \\in \\{0,1\\}$, where $A=1$ denotes triggering the action today (e.g., opening cooling centers, outreach, and alerts), and $A=0$ denotes not triggering.\n\nFor this decision, the department adopts a loss function that balances false alarms and missed events: a false alarm incurs a loss of $C_{F}$ when $A=1$ and $H=0$, and a missed event incurs a loss of $C_{M}$ when $A=0$ and $H=1$. Correct decisions incur zero loss. The forecasting system provides a calibrated probability of a surge conditional on the forecasted maximum Heat Index (HI), denoted $Y$ (in degrees Celsius), via a logistic model\n$$\np(Y) \\equiv \\mathbb{P}(H=1 \\mid Y) = \\frac{1}{1+\\exp\\!\\big(-(a + b Y)\\big)}.\n$$\nAssume the model parameters are $a=-16$ and $b=0.4$, the false alarm loss is $C_{F} = 1.2 \\times 10^{5}$, and the missed event loss is $C_{M} = 9.0 \\times 10^{5}$.\n\nStarting from the principle of minimizing expected loss for a binary action under uncertainty, derive the optimal posterior-probability decision threshold $p^{\\star}$ in terms of $C_{F}$ and $C_{M}$, then translate this into an optimal Heat Index cutoff $Y^{\\star}$ that satisfies $p(Y^{\\star})=p^{\\star}$. Compute the numerical value of $Y^{\\star}$ using the provided parameters. Round your final numerical answer to four significant figures and express it in degrees Celsius.",
            "solution": "The decision problem is to choose $A \\in \\{0,1\\}$ to minimize expected loss given the posterior probability $p = \\mathbb{P}(H=1 \\mid Y)$. The loss function is\n$$\nL(A,H) = \n\\begin{cases}\nC_{F}, & \\text{if } A=1,\\, H=0 \\\\\nC_{M}, & \\text{if } A=0,\\, H=1 \\\\\n0, & \\text{if } A=H,\n\\end{cases}\n$$\nwhich captures the preventive medicine trade-off between false alarms and missed events.\n\nGiven $p=\\mathbb{P}(H=1 \\mid Y)$, the expected loss if we trigger ($A=1$) is\n$$\n\\mathbb{E}[L \\mid A=1, Y] = \\mathbb{P}(H=0 \\mid Y)\\, C_{F} + \\mathbb{P}(H=1 \\mid Y)\\, 0 = (1-p)\\, C_{F}.\n$$\nThe expected loss if we do not trigger ($A=0$) is\n$$\n\\mathbb{E}[L \\mid A=0, Y] = \\mathbb{P}(H=1 \\mid Y)\\, C_{M} + \\mathbb{P}(H=0 \\mid Y)\\, 0 = p\\, C_{M}.\n$$\nThe optimal action minimizes expected loss, so we choose $A=1$ when\n$$\n(1-p)\\, C_{F} < p\\, C_{M}.\n$$\nSolving $(1-p)\\, C_{F} < p\\, C_{M}$ for $p$ gives\n$$\nC_{F} - p\\, C_{F} < p\\, C_{M}\n\\quad \\Rightarrow \\quad\nC_{F} < p\\,(C_{F} + C_{M})\n\\quad \\Rightarrow \\quad\np > \\frac{C_{F}}{C_{F} + C_{M}}.\n$$\nThus, the optimal posterior-probability threshold is\n$$\np^{\\star} = \\frac{C_{F}}{C_{F} + C_{M}}.\n$$\n\nTo convert this to a threshold on the Heat Index, we use the monotonic logistic relationship between $p$ and $Y$:\n$$\np(Y) = \\frac{1}{1+\\exp\\!\\big(-(a + b Y)\\big)}.\n$$\nThe inverse-logit (log-odds) transformation yields\n$$\n\\ln\\!\\left(\\frac{p(Y)}{1-p(Y)}\\right) = a + b Y.\n$$\nSetting $p(Y^{\\star}) = p^{\\star}$ gives\n$$\na + b Y^{\\star} = \\ln\\!\\left(\\frac{p^{\\star}}{1-p^{\\star}}\\right),\n$$\nand hence\n$$\nY^{\\star} = \\frac{\\ln\\!\\left(\\frac{p^{\\star}}{1-p^{\\star}}\\right) - a}{b}.\n$$\n\nNow compute $p^{\\star}$ with the given losses $C_{F} = 1.2 \\times 10^{5}$ and $C_{M} = 9.0 \\times 10^{5}$:\n$$\np^{\\star} = \\frac{1.2 \\times 10^{5}}{1.2 \\times 10^{5} + 9.0 \\times 10^{5}}\n= \\frac{1.2}{10.2}\n= \\frac{12}{102}\n= \\frac{2}{17}.\n$$\nTherefore\n$$\n\\frac{p^{\\star}}{1-p^{\\star}} = \\frac{\\frac{2}{17}}{1 - \\frac{2}{17}} \n= \\frac{\\frac{2}{17}}{\\frac{15}{17}} \n= \\frac{2}{15}.\n$$\nCompute the log-odds:\n$$\n\\ln\\!\\left(\\frac{p^{\\star}}{1-p^{\\star}}\\right) = \\ln\\!\\left(\\frac{2}{15}\\right).\n$$\nSubstitute $a=-16$ and $b=0.4$:\n$$\nY^{\\star} = \\frac{\\ln\\!\\left(\\frac{2}{15}\\right) - (-16)}{0.4}\n= \\frac{\\ln\\!\\left(\\frac{2}{15}\\right) + 16}{0.4}.\n$$\nNumerically, $\\ln\\!\\left(\\frac{2}{15}\\right) \\approx -2.0149030206$, so\n$$\nY^{\\star} \\approx \\frac{-2.0149030206 + 16}{0.4}\n= \\frac{13.9850969794}{0.4}\n= 34.9627424485.\n$$\nRounded to four significant figures and expressed in degrees Celsius, the optimal Heat Index cutoff is $34.96$.",
            "answer": "$$\\boxed{34.96}$$"
        },
        {
            "introduction": "Health risks from climate change are not distributed equally; some communities are far more vulnerable than others due to social, economic, and demographic factors. This final practice equips you with a powerful statistical method, Principal Component Analysis (PCA), to synthesize multiple vulnerability indicators into a single, robust index. By constructing this index, you will learn how to move beyond single-risk factors to create a holistic map of vulnerability, enabling health agencies to target adaptation resources and interventions with precision and equity .",
            "id": "4529526",
            "problem": "A public health agency seeks a mathematically principled composite index to prioritize climate-related health adaptation in communities. You will formalize a composite vulnerability index based on four indicators: fraction of population aged at least 65 years, prevalence of comorbidities, poverty rate, and housing quality score. The agency will use this index to rank geographic units (for example, census tracts) for preventive medicine interventions. Your task is to derive, justify, and implement a composite index using Principal Component Analysis (PCA) as a variance-maximizing linear combination of standardized indicators, then compute the index for the provided test suite.\n\nUse a fundamental base appropriate to preventive medicine and quantitative population health: linear algebra definitions for covariance and orthogonal projections, and statistical standardization. Specifically, ground your derivation in the following well-tested facts: principal components are orthonormal directions that maximize sample variance subject to orthogonality constraints; standardization by zero-mean, unit-variance removes scale effects so each indicator contributes comparably; and covariance matrices of standardized variables capture shared variability structure. Do not invoke shortcut formulas; instead, derive from the definitions of variance, covariance, and orthonormal eigen-decomposition.\n\nData, units, and transformations:\n- Indicators are measured per unit as follows:\n  - Fraction aged at least 65 years, as a decimal in $[0,1]$.\n  - Prevalence of comorbidities (for example, at least two chronic conditions), as a decimal in $[0,1]$.\n  - Poverty rate, as a decimal in $[0,1]$.\n  - Housing quality score, a dimensionless index on $[0,100]$ where higher values denote better quality.\n- To align indicator directions so that higher values reflect higher vulnerability, transform the housing quality score to a poor-housing fraction $p_h$ via $p_h = 1 - q/100$, where $q$ is the housing quality score. This makes $p_h \\in [0,1]$ with higher values denoting worse housing.\n\nAlgorithmic requirements:\n- For each test case, assemble a data matrix with columns ordered as aged at least 65 years, comorbidity prevalence, poverty rate, and poor-housing fraction $p_h$.\n- Standardize each column to zero mean and unit variance using $z$-scores so that each standardized indicator has mean $0$ and standard deviation $1$. If an indicator is constant across units (zero variance), treat its standardized values as $0$ to avoid undefined division, reflecting that it contributes no variance to the index.\n- Compute the sample covariance matrix of the standardized data. Derive the first principal component as the unit eigenvector associated with the largest eigenvalue of this covariance matrix, which corresponds to the direction of maximal variance in the standardized indicator space.\n- Orient the first principal component so that increasing standardized indicators in the vulnerability direction yields a larger index value. Concretely, if the sum of the entries of the first principal component is negative, multiply the component by $-1$ to ensure positive alignment.\n- Define the composite vulnerability index for each unit as the projection (score) onto the oriented first principal component, yielding a dimensionless real-valued index per unit.\n\nOutput requirements:\n- Express all indicator inputs as decimals or scores as specified; no percentages with a percent sign are permitted. The final composite index is dimensionless and should be output as real numbers rounded to $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the vulnerability index values per unit for the corresponding test case (for example, $[[v_{1,1},v_{1,2}], [v_{2,1},v_{2,2},v_{2,3}], [v_{3,1}]]$).\n\nTest suite:\n- Test Case $1$ (general, $5$ units):\n  - Fraction aged at least 65 years: $[0.15,0.22,0.08,0.18,0.30]$.\n  - Comorbidity prevalence: $[0.30,0.45,0.25,0.35,0.50]$.\n  - Poverty rate: $[0.12,0.20,0.10,0.18,0.25]$.\n  - Housing quality score: $[70,55,80,65,40]$.\n- Test Case $2$ (boundary with a constant indicator, $4$ units):\n  - Fraction aged at least 65 years: $[0.12,0.18,0.20,0.15]$.\n  - Comorbidity prevalence: $[0.25,0.30,0.28,0.27]$.\n  - Poverty rate (constant): $[0.15,0.15,0.15,0.15]$.\n  - Housing quality score: $[60,60,45,75]$.\n- Test Case $3$ (edge case with extremes, $2$ units):\n  - Fraction aged at least 65 years: $[0.05,0.40]$.\n  - Comorbidity prevalence: $[0.10,0.60]$.\n  - Poverty rate: $[0.05,0.35]$.\n  - Housing quality score: $[90,35]$.\n\nYour program must compute the composite vulnerability index for each unit within each test case according to the algorithm above and print a single line containing $3$ lists (one per test case), each listing the unit-level index values rounded to $4$ decimal places, in the exact format described.",
            "solution": "The problem posed is formally valid. It is scientifically grounded in established statistical methods, well-posed with a clear and complete set of instructions, and its terms are objective and mathematically precise. We may therefore proceed with a principled derivation of the solution.\n\nThe objective is to construct a composite vulnerability index, denoted $I$, for a set of $n$ geographic units based on $m=4$ indicators. The index is formulated as a linear combination of these indicators, with weights determined by Principal Component Analysis (PCA) to capture the maximum possible variance in the data. This approach reduces the multidimensional indicator space to a single dimension that best explains the differences in vulnerability among the units.\n\nFirst, we represent the data for the $n$ units and $m$ indicators as a data matrix $X \\in \\mathbb{R}^{n \\times m}$. The columns of $X$ correspond to the indicators. Let $x_{ij}$ be the value of the $j$-th indicator for the $i$-th unit. The indicators are, in order: fraction of population aged at least 65 years, prevalence of comorbidities, poverty rate, and a transformed housing quality score.\n\nA critical preliminary step is to ensure that all indicators are directionally aligned. Higher values for age, comorbidity, and poverty already signify higher vulnerability. The housing quality score, $q \\in [0, 100]$, is inversely related to vulnerability (higher score means better housing). We transform it into a poor-housing fraction, $p_h \\in [0, 1]$, using the prescribed formula:\n$$ p_h = 1 - \\frac{q}{100} $$\nNow, a higher $p_h$ corresponds to higher vulnerability, consistent with the other indicators. The four columns of our data matrix $X$, which we denote by the vectors $x_1, x_2, x_3, x_4$, are now all positively associated with the latent concept of vulnerability.\n\nThe indicators are measured on different scales, and their variances may differ substantially. To prevent indicators with larger variance from dominating the analysis, we standardize each indicator to have a mean of $0$ and a standard deviation of $1$. This ensures that each indicator contributes to the composite index on an equal footing in terms of variance. For each indicator column $x_j$ of $X$, we compute its sample mean $\\mu_j$ and sample standard deviation $\\sigma_j$ (using $n-1$ in the denominator for an unbiased estimate of variance):\n$$ \\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij} $$\n$$ \\sigma_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2} $$\nThe standardized value, or $z$-score, for each observation is then:\n$$ z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j} $$\nIn the event that an indicator is constant across all units, its standard deviation $\\sigma_j$ will be $0$. In this case, the indicator provides no information about the variation between units. As specified, we define its standardized values $z_{ij}$ to be $0$ for all $i$. Let $Z$ be the resulting $n \\times m$ matrix of standardized data.\n\nThe core of PCA lies in the analysis of the covariance structure of the data. We compute the sample covariance matrix $C$ of the standardized data matrix $Z$. For data where each column has a mean of $0$, this is given by:\n$$ C = \\frac{1}{n-1} Z^T Z $$\nSince the columns of $Z$ are standardized to have unit variance (unless $\\sigma_j=0$), the diagonal elements of $C$ are $1$ (or $0$), and the off-diagonal elements $C_{jk}$ are the Pearson correlation coefficients between indicators $j$ and $k$. Thus, $C$ is the correlation matrix of the original indicators.\n\nPCA finds the direction in the $m$-dimensional indicator space that maximizes the variance of the data when projected onto it. This direction is given by a unit vector of weights, $w \\in \\mathbb{R}^m$ (where $w^T w = 1$), which defines a linear combination of the standardized indicators. The resulting scores for each unit form a vector $I = Zw$. The variance of these scores is:\n$$ \\text{Var}(I) = \\text{Var}(Zw) = \\frac{1}{n-1} (Zw)^T (Zw) = w^T \\left( \\frac{1}{n-1} Z^T Z \\right) w = w^T C w $$\nWe seek to maximize this quantity, $w^T C w$, subject to the constraint that $w$ is a unit vector, $w^T w = 1$. From the principles of linear algebra, specifically the spectral theorem for symmetric matrices, this maximum value is the largest eigenvalue of the matrix $C$, and the vector $w$ that achieves this maximum is the corresponding eigenvector. This eigenvector is the first principal component, which we denote $w_{PC1}$.\n\nThe eigendecomposition of the covariance matrix $C$ yields a set of eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_m \\ge 0$ and a corresponding set of orthonormal eigenvectors $w_1, w_2, \\dots, w_m$. We select $w_1$, the eigenvector associated with the largest eigenvalue $\\lambda_1$, as our vector of weights.\n\nAn eigenvector is only defined up to its sign. To ensure a consistent interpretation of the index, we must orient $w_1$. Since all indicators were constructed such that higher values imply greater vulnerability, we expect the weights in $w_1$ to be predominantly positive, so that an increase in any indicator contributes positively to the overall index. The prescribed orientation rule is to check the sum of the elements of $w_1$. If this sum is negative, we multiply the eigenvector by $-1$. Let this oriented vector be $w_{orient}$.\n$$ w_{orient} = \\begin{cases} w_1 & \\text{if } \\sum_{j=1}^m (w_1)_j \\ge 0 \\\\ -w_1 & \\text{if } \\sum_{j=1}^m (w_1)_j < 0 \\end{cases} $$\nFinally, the composite vulnerability index for each unit $i$, denoted $I_i$, is its score along this principal component. This score is calculated as the projection of the unit's standardized data vector (row $i$ of $Z$, denoted $z_i^T$) onto the oriented principal component vector $w_{orient}$. This is equivalent to their dot product:\n$$ I_i = z_i^T \\cdot w_{orient} = \\sum_{j=1}^m z_{ij} (w_{orient})_j $$\nIn matrix notation, the vector of all index values is $I = Z w_{orient}$. These scores are dimensionless real numbers, where a higher value indicates greater composite vulnerability, providing a rational basis for prioritizing public health interventions. The implementation will now proceed by applying these derived steps to the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.linalg import eigh\n\ndef compute_vulnerability_index(age: list, comorb: list, pov: list, housing_q: list) -> list:\n    \"\"\"\n    Computes a composite vulnerability index using Principal Component Analysis.\n\n    Args:\n        age: List of fractions of population aged at least 65 years.\n        comorb: List of prevalence of comorbidities.\n        pov: List of poverty rates.\n        housing_q: List of housing quality scores.\n\n    Returns:\n         A list of vulnerability index scores for each unit, rounded to 4 decimal places.\n    \"\"\"\n    # 1. Assemble and transform the data matrix\n    # Transform housing quality score (q) to poor-housing fraction (p_h)\n    ph = [1.0 - q / 100.0 for q in housing_q]\n    \n    # Create the data matrix X with n units (rows) and m indicators (columns)\n    X = np.array([age, comorb, pov, ph], dtype=float).T\n    n, m = X.shape\n\n    if n <= 1:\n        # Cannot compute standard deviation for a single unit\n        return [0.0] * n\n\n    # 2. Standardize each indicator column to zero mean and unit variance\n    means = np.mean(X, axis=0)\n    # Use ddof=1 for sample standard deviation\n    stds = np.std(X, axis=0, ddof=1)\n    \n    Z = np.zeros_like(X, dtype=float)\n    # A small tolerance to handle floating point inaccuracies near zero\n    zero_std_tolerance = 1e-9\n    \n    for j in range(m):\n        if stds[j] > zero_std_tolerance:\n            Z[:, j] = (X[:, j] - means[j]) / stds[j]\n        # If std is zero, the standardized column remains all zeros, as specified.\n\n    # 3. Compute the sample covariance matrix of the standardized data\n    # np.cov with rowvar=False computes the covariance between columns.\n    # ddof=1 is the default and correct for sample covariance.\n    C = np.cov(Z, rowvar=False, ddof=1)\n\n    # 4. Perform eigendecomposition of the covariance matrix\n    # eigh is used for symmetric matrices and returns eigenvalues in ascending order.\n    eigenvalues, eigenvectors = eigh(C)\n\n    # 5. The first principal component is the eigenvector with the largest eigenvalue\n    # This corresponds to the last eigenvector returned by eigh.\n    pc1 = eigenvectors[:, -1]\n\n    # 6. Orient the principal component for consistent interpretation\n    # If the sum of its elements is negative, flip its sign.\n    if np.sum(pc1) < 0:\n        pc1 = -pc1\n    \n    w_oriented = pc1\n\n    # 7. Define the index as the projection onto the oriented principal component\n    # This is the dot product of each unit's standardized data with the component vector.\n    scores = Z @ w_oriented\n    \n    # Round results to 4 decimal places as per output requirements\n    return np.round(scores, 4).tolist()\n\ndef solve():\n    \"\"\"\n    Runs the vulnerability index calculation for the defined test suite\n    and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (general, 5 units)\n        {\n            \"age\": [0.15, 0.22, 0.08, 0.18, 0.30],\n            \"comorb\": [0.30, 0.45, 0.25, 0.35, 0.50],\n            \"pov\": [0.12, 0.20, 0.10, 0.18, 0.25],\n            \"housing_q\": [70, 55, 80, 65, 40]\n        },\n        # Test Case 2 (boundary with a constant indicator, 4 units)\n        {\n            \"age\": [0.12, 0.18, 0.20, 0.15],\n            \"comorb\": [0.25, 0.30, 0.28, 0.27],\n            \"pov\": [0.15, 0.15, 0.15, 0.15],\n            \"housing_q\": [60, 60, 45, 75]\n        },\n        # Test Case 3 (edge case with extremes, 2 units)\n        {\n            \"age\": [0.05, 0.40],\n            \"comorb\": [0.10, 0.60],\n            \"pov\": [0.05, 0.35],\n            \"housing_q\": [90, 35]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_vulnerability_index(\n            age=case[\"age\"],\n            comorb=case[\"comorb\"],\n            pov=case[\"pov\"],\n            housing_q=case[\"housing_q\"]\n        )\n        all_results.append(result)\n\n    # Format the final output string to exactly match the problem specification:\n    # Example: [[v1_1,v1_2],[v2_1,v2_2,v2_3]]\n    inner_parts = []\n    for res_list in all_results:\n        inner_parts.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    final_output_string = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output_string)\n\nsolve()\n\n```"
        }
    ]
}