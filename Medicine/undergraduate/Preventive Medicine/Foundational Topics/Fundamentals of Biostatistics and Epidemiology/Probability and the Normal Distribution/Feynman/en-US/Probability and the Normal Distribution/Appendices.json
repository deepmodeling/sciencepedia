{
    "hands_on_practices": [
        {
            "introduction": "A core task in preventive medicine is to estimate population-level health metrics, like the average blood glucose level, from a manageable sample. This exercise guides you through the process of constructing a confidence interval, a fundamental tool for quantifying the precision of such an estimate when the true population variance is unknown . Mastering this practice is essential for interpreting public health data and understanding the statistical foundation of surveillance reports.",
            "id": "4563667",
            "problem": "A community preventive medicine program conducts public health surveillance by periodically screening fasting plasma glucose among adults. In one screening round, a simple random sample of $n = 25$ adults is obtained under a design intended to approximate independent and identically distributed (iid) draws from a population. Let the individual measurements be $X_{1}, X_{2}, \\dots, X_{n}$, where each $X_{i}$ is modeled as iid Normal with common mean $\\mu$ and variance $\\sigma^{2}$, that is $X_{i} \\sim \\text{iid } N(\\mu, \\sigma^{2})$, and $\\sigma^{2}$ is unknown. The sample yields a sample mean of $\\bar{x} = 102$ milligrams per deciliter (mg/dL) and a sample standard deviation of $s = 18$ mg/dL.\n\nUsing only core definitions and well-tested facts about sampling distributions under the Normal model, first derive an analytic expression for the $100(1-\\alpha)\\%$ confidence interval for $\\mu$ when $\\sigma^{2}$ is unknown. Then, specialize to $\\alpha = 0.05$ and compute the numerical endpoints of the $95\\%$ confidence interval for $\\mu$ using the given data. Round your numerical endpoints to four significant figures. Express the final interval endpoints in mg/dL.\n\nFinally, interpret the coverage property of this interval in the surveillance context: explain, in terms of repeated rounds of such surveillance conducted under the same design and population conditions, what it means for the interval procedure to have $95\\%$ coverage.",
            "solution": "The problem statement is evaluated to be valid. It is a scientifically grounded, well-posed, and objective problem from the field of biostatistics. All necessary data ($n=25$, $\\bar{x}=102$, $s=18$) and conditions (iid Normal model, unknown variance) are provided to derive and compute a confidence interval for the population mean $\\mu$. The tasks are clearly specified: derive the general formula, compute the specific $95\\%$ interval, and interpret the result.\n\nFirst, we derive an analytic expression for the $100(1-\\alpha)\\%$ confidence interval for the population mean $\\mu$ when the population variance $\\sigma^2$ is unknown.\nThe model states that the individual measurements $X_1, X_2, \\dots, X_n$ are independent and identically distributed (iid) draws from a Normal distribution with mean $\\mu$ and variance $\\sigma^2$, denoted as $X_i \\sim N(\\mu, \\sigma^2)$. The sample size is $n$.\nThe sample mean is $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. Due to the properties of the Normal distribution, the sample mean $\\bar{X}$ is also normally distributed with mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$, i.e., $\\bar{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})$.\n\nSince $\\sigma^2$ is unknown, it must be estimated from the sample. The unbiased estimator for $\\sigma^2$ is the sample variance, $S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$. The sample standard deviation is $S = \\sqrt{S^2}$.\n\nA pivotal quantity is a function of the sample data and the unknown parameter whose distribution does not depend on the parameter. For a Normal sample with unknown variance, the appropriate pivotal quantity is the t-statistic:\n$$\nT = \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}}\n$$\nBy a fundamental result in statistics (often related to Cochran's theorem), this quantity $T$ follows a Student's t-distribution with $\\nu = n-1$ degrees of freedom. We denote this as $T \\sim t_{n-1}$.\n\nTo construct a $100(1-\\alpha)\\%$ confidence interval, we find a critical value $t_{n-1, \\alpha/2}$ from the t-distribution such that the probability of a value falling in the tails is $\\alpha$. Specifically, $P(T > t_{n-1, \\alpha/2}) = \\alpha/2$. Due to the symmetry of the t-distribution about $0$, we have:\n$$\nP(-t_{n-1, \\alpha/2} < T < t_{n-1, \\alpha/2}) = 1 - \\alpha\n$$\nSubstituting the expression for $T$:\n$$\nP\\left(-t_{n-1, \\alpha/2} < \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}} < t_{n-1, \\alpha/2}\\right) = 1 - \\alpha\n$$\nWe now rearrange the inequalities to isolate the parameter $\\mu$:\n$$\n-t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} < \\bar{X} - \\mu < t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}\n$$\nSubtracting $\\bar{X}$ from all parts:\n$$\n-\\bar{X} - t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} < -\\mu < -\\bar{X} + t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}\n$$\nMultiplying by $-1$ and reversing the direction of the inequalities:\n$$\n\\bar{X} + t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} > \\mu > \\bar{X} - t_{n-1, \\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}\n$$\nThis gives the analytical expression for the $100(1-\\alpha)\\%$ confidence interval for $\\mu$:\n$$\n\\left( \\bar{X} - t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}}, \\bar{X} + t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}} \\right)\n$$\nOr more compactly, $\\bar{X} \\pm t_{n-1, \\alpha/2} \\frac{S}{\\sqrt{n}}$.\n\nNext, we specialize this result for the given data and compute the $95\\%$ confidence interval.\nThe given values are:\nSample size $n = 25$.\nSample mean $\\bar{x} = 102$ mg/dL.\nSample standard deviation $s = 18$ mg/dL.\nThe confidence level is $95\\%$, so $1-\\alpha = 0.95$, which implies $\\alpha = 0.05$ and $\\alpha/2 = 0.025$.\nThe degrees of freedom are $\\nu = n-1 = 25 - 1 = 24$.\n\nWe need the critical value $t_{\\nu, \\alpha/2} = t_{24, 0.025}$. From a standard t-distribution table or statistical software, this value is approximately $2.064$.\n$t_{24, 0.025} \\approx 2.064$.\n\nThe standard error of the mean is calculated as:\n$$\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}} = \\frac{18}{\\sqrt{25}} = \\frac{18}{5} = 3.6\n$$\nThe margin of error (ME) is:\n$$\nME = t_{24, 0.025} \\times SE(\\bar{x}) \\approx 2.064 \\times 3.6 = 7.4304\n$$\nThe endpoints of the confidence interval are:\nLower bound: $\\bar{x} - ME = 102 - 7.4304 = 94.5696$.\nUpper bound: $\\bar{x} + ME = 102 + 7.4304 = 109.4304$.\n\nThe problem requires rounding the endpoints to four significant figures.\nFor the lower bound, $94.5696$, the four significant figures are $9$, $4$, $5$, and $6$. The next digit is $9$, so we round up: $94.57$.\nFor the upper bound, $109.4304$, the four significant figures are $1$, $0$, $9$, and $4$. The next digit is $3$, so we do not round up: $109.4$.\nThe numerical $95\\%$ confidence interval for $\\mu$ is $(94.57, 109.4)$ mg/dL.\n\nFinally, we interpret the coverage property of this interval procedure.\nThe $95\\%$ confidence level refers to the long-run performance of the method, not to a single calculated interval. In the context of the public health surveillance program, the interpretation is as follows: If this screening process (drawing a simple random sample of $n=25$ adults and computing the $95\\%$ confidence interval for the mean fasting glucose) were to be repeated a very large number of times under identical population conditions, then approximately $95\\%$ of the intervals so constructed would contain the true, unknown population mean glucose level, $\\mu$. The other $5\\%$ of the intervals would fail to capture $\\mu$. It is incorrect to state that there is a $95\\%$ probability that the specific interval $(94.57, 109.4)$ contains $\\mu$; once computed, this interval either does or does not contain $\\mu$. The probability is attached to the procedure of generating the interval, not to the outcome of a single instance.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n94.57 & 109.4\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Statistical models, including those based on the Normal distribution, are powerful but rely on assumptions that real-world data may violate. This practice presents a realistic scenario from a clinical trial where measurement anomalies lead to heavy-tailed data, challenging the standard two-sample $t$-test . By analyzing the consequences of this deviation from normality, you will develop a critical understanding of a test's limitations and the importance of robust alternatives.",
            "id": "4563643",
            "problem": "A community-based randomized controlled trial (RCT) in preventive medicine evaluates a lifestyle intervention intended to reduce systolic blood pressure. Let the outcome be the change in systolic blood pressure from baseline to $6$ months, denoted $X$ in the control arm and $Y$ in the intervention arm. Because of occasional measurement anomalies and rare clinical events, the marginal distribution of individual changes is not exactly Normal. A plausible heavy-tailed, symmetric contamination model for each arm is:\nwith probability $0.95$, an individual’s change is distributed as $\\mathcal{N}(\\mu, \\sigma^{2})$; with probability $0.05$, it is distributed as $\\mathcal{N}(\\mu, 25\\sigma^{2})$. Assume independent and identically distributed sampling within and across arms. Suppose the trial enrolls $n_{1} = 40$ participants in the control arm and $n_{2} = 40$ in the intervention arm, and the primary analysis targets a two-sample mean comparison at significance level $\\alpha = 0.05$, two-sided.\n\nStarting from core definitions and well-tested results in probability, including the Central Limit Theorem (CLT) for independent, identically distributed random variables with finite variance, and the exact finite-sample Student’s $t$-distribution under Normality, analyze how the stated heavy-tailed contamination affects the validity (type I error calibration) and power of the classical pooled two-sample $t$-test for mean differences. Then, propose a methodologically sound alternative test that achieves robustness to outliers while admitting large-sample Normal calibration.\n\nWhich of the following statements are correct?\n\nA. Under the null hypothesis $\\mu_{X} = \\mu_{Y}$, the pooled two-sample $t$-test maintains exact type I error $\\alpha$ for any independent and identically distributed sample with finite variance because the CLT guarantees its $t$-statistic has a Student’s $t$ distribution.\n\nB. In the contamination model, the population variance of the outcome in each arm is $2.20\\,\\sigma^{2}$; therefore, relative to a purely Normal baseline with variance $\\sigma^{2}$, the standard error of the difference in sample means increases by a factor of $\\sqrt{2.20}$, which reduces the noncentrality parameter at fixed effect size and hence power.\n\nC. A two-sample $20\\%$ trimmed mean test using a Winsorized variance estimate produces a Studentized statistic that is asymptotically standard Normal under finite-variance independent sampling, enabling large-sample Normal calibration and improved robustness to outliers.\n\nD. For symmetric heavy-tailed distributions, the classical pooled two-sample $t$-test remains the uniformly most powerful unbiased test for mean differences at any sample size.\n\nE. A permutation test of the difference in means yields a null distribution that is asymptotically Normal regardless of whether the outcome variance is finite, so Normal calibration is always appropriate.",
            "solution": "The problem statement has been critically validated and found to be sound.\n\nFirst, let's analyze the properties of the contamination model. Let $Z$ be the outcome variable for a participant in either arm. Its distribution is a mixture of two Normal distributions. Let $C$ be an indicator variable, where $C=1$ if the observation comes from the contaminating distribution (with probability $p=0.05$) and $C=0$ otherwise (with probability $1-p=0.95$).\nThe model is:\n$Z | C=0 \\sim \\mathcal{N}(\\mu, \\sigma^{2})$\n$Z | C=1 \\sim \\mathcal{N}(\\mu, 25\\sigma^{2})$\n\nThe mean of $Z$ is found using the law of total expectation:\n$$E[Z] = E[E[Z|C]] = P(C=0)E[Z|C=0] + P(C=1)E[Z|C=1]$$\n$$E[Z] = (0.95)\\mu + (0.05)\\mu = \\mu$$\nThe mean of the outcome is unchanged by the contamination.\n\nThe variance of $Z$ is found using the law of total variance:\n$$\\text{Var}(Z) = E[\\text{Var}(Z|C)] + \\text{Var}(E[Z|C])$$\nFirst term:\n$$E[\\text{Var}(Z|C)] = P(C=0)\\text{Var}(Z|C=0) + P(C=1)\\text{Var}(Z|C=1)$$\n$$E[\\text{Var}(Z|C)] = (0.95)\\sigma^{2} + (0.05)(25\\sigma^{2}) = 0.95\\sigma^{2} + 1.25\\sigma^{2} = 2.20\\sigma^{2}$$\nSecond term:\n$E[Z|C]$ is always $\\mu$, regardless of $C$. Therefore, it is a constant.\n$$\\text{Var}(E[Z|C]) = \\text{Var}(\\mu) = 0$$\nSo, the total variance is:\n$$\\text{Var}(Z) = 2.20\\sigma^{2} + 0 = 2.20\\sigma^{2}$$\nThis distribution is symmetric because it is a mixture of two symmetric distributions, $\\mathcal{N}(\\mu, \\sigma^2)$ and $\\mathcal{N}(\\mu, 25\\sigma^2)$, with the same center of symmetry $\\mu$. However, it is not a Normal distribution; it is a heavy-tailed (leptokurtic) distribution.\n\nThe classical pooled two-sample $t$-test statistic under the null hypothesis $H_0: \\mu_X = \\mu_Y$ is:\n$$T = \\frac{\\bar{X} - \\bar{Y}}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$$\nwhere $S_p^2$ is the pooled sample variance. For this statistic to follow an exact Student's $t$-distribution with $n_1+n_2-2$ degrees of freedom, the data from each group must be i.i.d. samples from a Normal distribution with equal variances. In this problem, the Normality assumption is violated.\n\nNow, let's evaluate each statement.\n\nA. Under the null hypothesis $\\mu_{X} = \\mu_{Y}$, the pooled two-sample $t$-test maintains exact type I error $\\alpha$ for any independent and identically distributed sample with finite variance because the CLT guarantees its $t$-statistic has a Student’s $t$ distribution.\n\nThis statement is incorrect for multiple reasons. Firstly, the Student's $t$-distribution is the exact finite-sample null distribution for the $t$-statistic *only when the underlying data are normally distributed*. The problem explicitly states that the distribution is a non-Normal mixture. Secondly, the Central Limit Theorem (CLT) guarantees that the distribution of the sample mean (and thus the difference in sample means, $\\bar{X}-\\bar{Y}$) approaches a Normal distribution as the sample size grows. By Slutsky's theorem, the $t$-statistic itself approaches a standard Normal distribution (not a Student's $t$-distribution) for large samples. For finite samples from a non-Normal distribution, the distribution of the $t$-statistic is not exactly a Student's $t$-distribution, and the type I error rate is not guaranteed to be exactly $\\alpha$. While for symmetric distributions like this one, the approximation can be reasonable, the claim of \"exact\" for \"any\" i.i.d distribution is false.\nVerdict: **Incorrect**.\n\nB. In the contamination model, the population variance of the outcome in each arm is $2.20\\,\\sigma^{2}$; therefore, relative to a purely Normal baseline with variance $\\sigma^{2}$, the standard error of the difference in sample means increases by a factor of $\\sqrt{2.20}$, which reduces the noncentrality parameter at fixed effect size and hence power.\n\nAs derived above, the variance of the outcome in each arm is indeed $2.20\\sigma^2$. The standard error (SE) of the difference in sample means is $\\text{SE}(\\bar{X}-\\bar{Y}) = \\sqrt{\\frac{\\text{Var}(X)}{n_1} + \\frac{\\text{Var}(Y)}{n_2}}$. Assuming the same parameters for both arms, $\\text{Var}(X) = \\text{Var}(Y) = 2.20\\sigma^2$. With equal sample sizes $n_1=n_2=n=40$, the SE is $\\sqrt{\\frac{2 \\cdot 2.20\\sigma^2}{n}}$. For a purely Normal baseline, the variance would be $\\sigma^2$ and the SE would be $\\sqrt{\\frac{2\\sigma^2}{n}}$. The ratio of the standard errors is $\\frac{\\sqrt{2.20 \\cdot (2\\sigma^2/n)}}{\\sqrt{2\\sigma^2/n}} = \\sqrt{2.20}$. The statement is correct that the SE increases by a factor of $\\sqrt{2.20}$. The power of a test depends on the noncentrality parameter, which for a $t$-test is proportional to the effect size divided by the standard error: $\\delta = \\frac{\\mu_X - \\mu_Y}{\\text{SE}(\\bar{X}-\\bar{Y})}$. Since the SE is in the denominator, increasing it by a factor of $\\sqrt{2.20}$ will decrease the noncentrality parameter by the same factor for a fixed effect size $\\mu_X - \\mu_Y$. Power is a monotonically increasing function of the magnitude of the noncentrality parameter, so a smaller noncentrality parameter results in lower power. The entire statement is logically and mathematically correct.\nVerdict: **Correct**.\n\nC. A two-sample $20\\%$ trimmed mean test using a Winsorized variance estimate produces a Studentized statistic that is asymptotically standard Normal under finite-variance independent sampling, enabling large-sample Normal calibration and improved robustness to outliers.\n\nThis statement describes a standard robust statistical procedure. A trimmed mean is calculated by removing a fixed percentage of the smallest and largest observations and then averaging the remaining ones. This makes the estimator less sensitive to outliers, which is precisely the problem introduced by the contaminating distribution. For i.i.d. samples from a distribution with a finite variance (which our mixture model has), a generalized Central Limit Theorem for trimmed means shows that they are asymptotically Normally distributed. The \"Winsorized variance\" is a specific estimator designed to be a consistent estimator of the asymptotic variance of the trimmed mean. A Studentized statistic is formed by taking the estimator (difference in trimmed means), subtracting the true value under the null, and dividing by its estimated standard error (which uses the Winsorized variance). By Slutsky's theorem, this Studentized statistic is asymptotically standard Normal, i.e., its distribution converges to $\\mathcal{N}(0,1)$ as the sample size tends to infinity. This allows for \"large-sample Normal calibration,\" meaning p-values can be calculated using the standard Normal distribution. The statement is a completely accurate description of this robust testing procedure and its properties.\nVerdict: **Correct**.\n\nD. For symmetric heavy-tailed distributions, the classical pooled two-sample $t$-test remains the uniformly most powerful unbiased test for mean differences at any sample size.\n\nThis statement is false. The property of being the uniformly most powerful unbiased (UMPU) test is a very strong optimality condition that the two-sample $t$-test possesses under the strict assumptions of i.i.d. sampling from Normal distributions with equal variance. When the Normality assumption is violated, even if the distribution is symmetric, the $t$-test loses this optimality property. For heavy-tailed distributions, the variance of the sample mean is large, and the sample variance estimator is highly variable, both of which degrade the power of the $t$-test. Robust tests, such as the trimmed mean test (Option C) or rank-based tests (like the Wilcoxon rank-sum test), are typically more powerful than the $t$-test in such situations.\nVerdict: **Incorrect**.\n\nE. A permutation test of the difference in means yields a null distribution that is asymptotically Normal regardless of whether the outcome variance is finite, so Normal calibration is always appropriate.\n\nThis statement is incorrect. While permutation tests are a powerful non-parametric tool that provides exact p-values for finite samples without assuming a specific distribution, their large-sample (asymptotic) properties do have requirements. The permutation central limit theorems (e.g., by Hoeffding, 1951) state that the permutation distribution of a standardized test statistic (like the difference in means) converges to a Normal distribution, *provided that certain conditions on the moments of the underlying population hold*. A key condition is that the population variance must be finite. If the variance is infinite (e.g., if sampling from a Cauchy distribution), the permutation distribution of the sample mean does not converge to a Normal distribution, and using a Normal approximation (\"Normal calibration\") for the p-value would be incorrect. The claim that this holds \"regardless of whether the outcome variance is finite\" is false.\nVerdict: **Incorrect**.\n\nIn summary, statements B and C are correct.",
            "answer": "$$\\boxed{BC}$$"
        },
        {
            "introduction": "Modern public health research often relies on simulation to evaluate complex strategies or understand the interplay of multiple risk factors. This hands-on coding exercise demonstrates how to generate correlated data that mimics real-world health variables, such as blood pressure and cholesterol, using the multivariate normal distribution . You will implement the Cholesky decomposition method, a cornerstone of statistical computing, to create realistic datasets for a simulated study.",
            "id": "4563690",
            "problem": "A public health research group is preparing a Monte Carlo study to evaluate a screening strategy in a large cohort. To realistically reflect joint variation in continuous risk factors that are approximately Gaussian after standard transformation (for example, systolic blood pressure, body mass index, and low-density lipoprotein cholesterol), they need to simulate correlated normal samples with prespecified mean vector and covariance matrix. The goal is to implement a principled algorithm, grounded in first principles, that generates multivariate normal samples with a given mean vector and covariance matrix, and then validate the empirical mean vector and covariance matrix against their targets using tolerances justified from sampling variability.\n\nFundamental base for the problem:\n- Definition of the multivariate normal distribution and the property that linear transformations of independent standard normal variables produce general multivariate normal variables.\n- Properties of covariance matrices, including symmetry and positive definiteness, and the existence and uniqueness of a lower-triangular factor for such matrices that can be used to construct the required linear transformation.\n- Sampling variability of empirical moments for independent and identically distributed draws from a multivariate normal distribution, including the variance of the sample mean and the variance structure of the maximum-likelihood covariance estimator when the true mean is known.\n\nYour task is to write a complete, runnable program that, for each provided test case, performs the following steps in a purely mathematical and algorithmic manner:\n\n1. Input specification is fixed within the program (no external input). For each test case, you are given:\n   - A dimension $d$ implied by the length of a mean vector $\\boldsymbol{\\mu} \\in \\mathbb{R}^d$.\n   - A symmetric covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ that is intended to be positive definite.\n   - A sample size $N \\in \\mathbb{N}$.\n   - A pseudo-random seed $s \\in \\mathbb{N}$.\n\n2. Validate that $\\boldsymbol{\\Sigma}$ is symmetric and positive definite. If $\\boldsymbol{\\Sigma}$ is not symmetric or is not positive definite, the test case result must be $[\\text{False},\\text{False}]$.\n\n3. Construct a transformation that maps independent standard normal draws to a multivariate normal draw with mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\Sigma}$ using the unique lower-triangular factor guaranteed by positive definiteness. Generate $N$ independent samples using a reproducible pseudo-random number generator initialized to seed $s$.\n\n4. Compute the empirical mean vector $\\widehat{\\boldsymbol{\\mu}} \\in \\mathbb{R}^d$ and the maximum-likelihood covariance estimate $\\widehat{\\boldsymbol{\\Sigma}} \\in \\mathbb{R}^{d \\times d}$ using centering at the known target mean $\\boldsymbol{\\mu}$.\n\n5. Define tolerance thresholds that reflect sampling variability under the multivariate normal model:\n   - For the mean, for each component $j \\in \\{1,\\dots,d\\}$, define the tolerance\n     $$\\tau^{(\\mu)}_j = k_{\\mu} \\sqrt{\\frac{\\boldsymbol{\\Sigma}_{jj}}{N}},$$\n     where $k_{\\mu} \\in \\mathbb{R}$ is a fixed multiplier. Use $k_{\\mu} = 4.0$.\n   - For the covariance, for each element $(i,j)$, define the tolerance\n     $$\\tau^{(\\Sigma)}_{ij} = k_{\\Sigma} \\sqrt{\\frac{\\boldsymbol{\\Sigma}_{ij}^2 + \\boldsymbol{\\Sigma}_{ii}\\boldsymbol{\\Sigma}_{jj}}{N}},$$\n     where $k_{\\Sigma} \\in \\mathbb{R}$ is a fixed multiplier. Use $k_{\\Sigma} = 4.0$.\n\n6. Determine two booleans for each test case:\n   - A mean-acceptance boolean that is $\\text{True}$ if and only if $|\\widehat{\\mu}_j - \\mu_j| \\le \\tau^{(\\mu)}_j$ for all $j \\in \\{1,\\dots,d\\}$.\n   - A covariance-acceptance boolean that is $\\text{True}$ if and only if $|\\widehat{\\Sigma}_{ij} - \\Sigma_{ij}| \\le \\tau^{(\\Sigma)}_{ij}$ for all $i,j \\in \\{1,\\dots,d\\}$.\n\n7. The final program output must be a single line containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list in square brackets containing the two booleans described in step $6$. For example, the format is\n   $$[\\,[b_{1,1},b_{1,2}],\\,[b_{2,1},b_{2,2}],\\,[b_{3,1},b_{3,2}],\\,[b_{4,1},b_{4,2}]\\,],$$\n   printed without spaces, where each $b_{k,\\ell}$ is either $\\text{True}$ or $\\text{False}$.\n\nTest suite:\n- Case $1$ (identity covariance, zero mean):\n  - $\\boldsymbol{\\mu} = [\\,0,\\,0,\\,0\\,]$.\n  - Correlation matrix $\\mathbf{C}_1 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ and standard deviations $\\boldsymbol{\\sigma} = [\\,1,\\,1,\\,1\\,]$, so $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\boldsymbol{\\sigma})\\,\\mathbf{C}_1\\,\\mathrm{diag}(\\boldsymbol{\\sigma})$ equals the identity.\n  - $N = 50000$.\n  - $s = 12345$.\n\n- Case $2$ (realistic moderate correlations):\n  - $\\boldsymbol{\\mu} = [\\,120,\\,27,\\,130\\,]$.\n  - Correlation matrix $\\mathbf{C}_2 = \\begin{bmatrix} 1 & 0.4 & -0.2 \\\\ 0.4 & 1 & 0.3 \\\\ -0.2 & 0.3 & 1 \\end{bmatrix}$ and standard deviations $\\boldsymbol{\\sigma} = [\\,12,\\,4,\\,25\\,]$, so $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\boldsymbol{\\sigma})\\,\\mathbf{C}_2\\,\\mathrm{diag}(\\boldsymbol{\\sigma})$.\n  - $N = 60000$.\n  - $s = 2468$.\n\n- Case $3$ (near-singular high correlation):\n  - $\\boldsymbol{\\mu} = [\\,50,\\,50,\\,50\\,]$.\n  - Correlation matrix with constant off-diagonal $r$: $\\mathbf{C}_3 = \\begin{bmatrix} 1 & r & r \\\\ r & 1 & r \\\\ r & r & 1 \\end{bmatrix}$ with $r = 0.99$ and standard deviations $\\boldsymbol{\\sigma} = [\\,10,\\,10,\\,10\\,]$, so $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\boldsymbol{\\sigma})\\,\\mathbf{C}_3\\,\\mathrm{diag}(\\boldsymbol{\\sigma})$.\n  - $N = 80000$.\n  - $s = 13579$.\n\n- Case $4$ (negative and mixed correlations):\n  - $\\boldsymbol{\\mu} = [\\,110,\\,23,\\,95\\,]$.\n  - Correlation matrix $\\mathbf{C}_4 = \\begin{bmatrix} 1 & -0.4 & 0.1 \\\\ -0.4 & 1 & 0.3 \\\\ 0.1 & 0.3 & 1 \\end{bmatrix}$ and standard deviations $\\boldsymbol{\\sigma} = [\\,8,\\,6,\\,5\\,]$, so $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\boldsymbol{\\sigma})\\,\\mathbf{C}_4\\,\\mathrm{diag}(\\boldsymbol{\\sigma})$.\n  - $N = 70000$.\n  - $s = 97531$.\n\nImplementation constraints:\n- You must use a pseudorandom generator with a fixed seed per case to ensure reproducibility.\n- Center the covariance estimate at the known target $\\boldsymbol{\\mu}$ rather than the sample mean to align with the maximum-likelihood estimator under known mean.\n- No external inputs or files; all parameters are defined within the program.\n- The final output must be printed exactly as a single line in the format described in step $7$, with no spaces.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\,[\\text{True},\\text{True}],\\,[\\text{True},\\text{False}]\\,]$), printed without spaces.",
            "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the theory of multivariate statistics, is well-posed with a clear objective and sufficient data, and is expressed in objective, formal language. The task is to implement and validate a standard algorithm for generating samples from a multivariate normal distribution.\n\n### Theoretical Framework\n\nThe generation of samples from a general multivariate normal distribution is predicated on a fundamental property of this distribution. A $d$-dimensional random vector $\\mathbf{X}$ is said to follow a multivariate normal distribution with mean vector $\\boldsymbol{\\mu} \\in \\mathbb{R}^d$ and covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$, denoted $\\mathbf{X} \\sim \\mathcal{N}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, if for any constant vector $\\mathbf{a} \\in \\mathbb{R}^d$, the linear combination $\\mathbf{a}^T\\mathbf{X}$ is a univariate normal random variable.\n\nThe core principle for generating such samples is based on the affine transformation property. Let $\\mathbf{Z}$ be a $d$-dimensional random vector whose components are independent and identically distributed standard normal random variables, i.e., $Z_i \\sim \\mathcal{N}(0, 1)$ for $i=1, \\dots, d$. The mean of $\\mathbf{Z}$ is $\\mathbb{E}[\\mathbf{Z}] = \\mathbf{0}$ and its covariance matrix is $\\mathbb{E}[\\mathbf{Z}\\mathbf{Z}^T] = \\mathbf{I}$, the identity matrix.\n\nConsider a linear transformation of $\\mathbf{Z}$ of the form:\n$$ \\mathbf{X} = \\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{Z} $$\nwhere $\\mathbf{L}$ is a $d \\times d$ matrix. The mean of $\\mathbf{X}$ is:\n$$ \\mathbb{E}[\\mathbf{X}] = \\mathbb{E}[\\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{Z}] = \\boldsymbol{\\mu} + \\mathbf{L}\\mathbb{E}[\\mathbf{Z}] = \\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{0} = \\boldsymbol{\\mu} $$\nThe covariance matrix of $\\mathbf{X}$ is:\n$$ \\mathrm{Cov}(\\mathbf{X}) = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu})(\\mathbf{X} - \\boldsymbol{\\mu})^T] = \\mathbb{E}[(\\mathbf{L}\\mathbf{Z})(\\mathbf{L}\\mathbf{Z})^T] = \\mathbb{E}[\\mathbf{L}\\mathbf{Z}\\mathbf{Z}^T\\mathbf{L}^T] = \\mathbf{L}\\mathbb{E}[\\mathbf{Z}\\mathbf{Z}^T]\\mathbf{L}^T = \\mathbf{L}\\mathbf{I}\\mathbf{L}^T = \\mathbf{L}\\mathbf{L}^T $$\nTo generate samples $\\mathbf{X}$ with a target covariance matrix $\\boldsymbol{\\Sigma}$, we must find a matrix $\\mathbf{L}$ such that $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^T$.\n\n### Cholesky Decomposition\n\nA standard and computationally efficient method to find such a matrix $\\mathbf{L}$ is the Cholesky decomposition. A covariance matrix $\\boldsymbol{\\Sigma}$ must be symmetric and positive definite. A real, symmetric matrix $\\boldsymbol{\\Sigma}$ is positive definite if and only if it has a unique decomposition into the product of a lower-triangular matrix $\\mathbf{L}$ with positive diagonal entries and its transpose $\\mathbf{L}^T$:\n$$ \\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^T $$\nThis decomposition provides the required transformation matrix $\\mathbf{L}$. The existence of this unique decomposition is also a constructive test for positive definiteness. If the Cholesky decomposition algorithm succeeds, the matrix is positive definite; if it fails, the matrix is not.\n\n### Algorithm and Implementation\n\nThe solution proceeds through the following steps for each test case.\n\n1.  **Input Specification and Validation**: The given target mean vector $\\boldsymbol{\\mu}$, correlation matrix $\\mathbf{C}$, vector of standard deviations $\\boldsymbol{\\sigma}$, sample size $N$, and seed $s$ are defined. The covariance matrix $\\boldsymbol{\\Sigma}$ is constructed via the relation $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\boldsymbol{\\sigma})\\,\\mathbf{C}\\,\\mathrm{diag}(\\boldsymbol{\\sigma})$. The matrix $\\boldsymbol{\\Sigma}$ is first validated for symmetry, i.e., we check if $\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^T$. Subsequently, its positive definiteness is confirmed by attempting to compute its Cholesky decomposition. If $\\boldsymbol{\\Sigma}$ is not symmetric or the decomposition fails, the matrix is not positive definite, and the result for the test case is recorded as $[\\text{False}, \\text{False}]$.\n\n2.  **Sample Generation**:\n    - A pseudo-random number generator is initialized with the specified seed $s$ for reproducibility.\n    - A matrix of $N \\times d$ independent standard normal samples, $\\mathbf{Z}_s$, is generated. Each row of $\\mathbf{Z}_s$ represents a single draw $\\mathbf{z}^T$ from the $d$-dimensional standard normal distribution.\n    - The Cholesky factor $\\mathbf{L}$ is computed from the valid covariance matrix $\\boldsymbol{\\Sigma}$.\n    - The $N$ samples from $\\mathcal{N}_d(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ are generated using the transformation. Let $\\mathbf{X}$ be the $N \\times d$ matrix of generated samples. The $i$-th row of $\\mathbf{X}$, denoted $\\mathbf{x}_i^T$, is computed as $\\mathbf{x}_i^T = \\boldsymbol{\\mu}^T + \\mathbf{z}_i^T \\mathbf{L}^T$. In matrix notation, this is $\\mathbf{X} = \\mathbf{1}\\boldsymbol{\\mu}^T + \\mathbf{Z}_s \\mathbf{L}^T$, where $\\mathbf{1}$ is a column vector of ones of length $N$.\n\n3.  **Empirical Moment Estimation**:\n    - The empirical mean vector $\\widehat{\\boldsymbol{\\mu}}$ is computed by averaging the sample vectors:\n      $$ \\widehat{\\boldsymbol{\\mu}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i $$\n    - The maximum-likelihood estimate of the covariance matrix, given the known population mean $\\boldsymbol{\\mu}$, is computed as:\n      $$ \\widehat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{x}_i - \\boldsymbol{\\mu})(\\mathbf{x}_i - \\boldsymbol{\\mu})^T $$\n\n4.  **Tolerance Calculation and Validation**:\n    - **Mean Validation**: For each component $j \\in \\{1,\\dots,d\\}$, the absolute difference $|\\widehat{\\mu}_j - \\mu_j|$ is compared against a tolerance $\\tau_j^{(\\mu)}$. This tolerance is proportional to the standard deviation of the sample mean estimator $\\widehat{\\mu}_j$, which is $\\sqrt{\\boldsymbol{\\Sigma}_{jj}/N}$. The tolerance is set to:\n      $$ \\tau_j^{(\\mu)} = k_{\\mu} \\sqrt{\\frac{\\boldsymbol{\\Sigma}_{jj}}{N}} $$\n      with $k_{\\mu} = 4.0$. The mean-acceptance boolean is $\\text{True}$ if $|\\widehat{\\mu}_j - \\mu_j| \\le \\tau_j^{(\\mu)}$ holds for all $j$.\n\n    - **Covariance Validation**: For each element $(i,j)$, the absolute difference $|\\widehat{\\Sigma}_{ij} - \\Sigma_{ij}|$ is compared against a tolerance $\\tau_{ij}^{(\\Sigma)}$. This tolerance is proportional to the standard deviation of the estimator $\\widehat{\\Sigma}_{ij}$, which for a normal distribution with known mean is $\\sqrt{(\\boldsymbol{\\Sigma}_{ij}^2 + \\boldsymbol{\\Sigma}_{ii}\\boldsymbol{\\Sigma}_{jj})/N}$. The tolerance is set to:\n      $$ \\tau_{ij}^{(\\Sigma)} = k_{\\Sigma} \\sqrt{\\frac{\\boldsymbol{\\Sigma}_{ij}^2 + \\boldsymbol{\\Sigma}_{ii}\\boldsymbol{\\Sigma}_{jj}}{N}} $$\n      with $k_{\\Sigma} = 4.0$. The covariance-acceptance boolean is $\\text{True}$ if $|\\widehat{\\Sigma}_{ij} - \\Sigma_{ij}| \\le \\tau_{ij}^{(\\Sigma)}$ holds for all pairs $(i, j)$.\n\nThe multiplier $k=4.0$ implies that the check is for whether the empirical estimate falls within $4$ standard deviations of its expected value. By Chebyshev's inequality, the probability of failure for any single check is less than $1/k^2 = 1/16$, and for a Gaussian estimator, it is much smaller (approximately $6 \\times 10^{-5}$). This makes the test stringent but accounts for expected sampling variability.\n\nThe final output is a list containing the pair of booleans $[\\text{mean-acceptance}, \\text{covariance-acceptance}]$ for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multivariate normal simulation and validation problem.\n    \"\"\"\n    test_cases = [\n        # Case 1: identity covariance, zero mean\n        {'mu': np.array([0., 0., 0.]),\n         'C': np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]),\n         'sigma_vec': np.array([1., 1., 1.]),\n         'N': 50000,\n         's': 12345},\n        # Case 2: realistic moderate correlations\n        {'mu': np.array([120., 27., 130.]),\n         'C': np.array([[1., 0.4, -0.2], [0.4, 1., 0.3], [-0.2, 0.3, 1.]]),\n         'sigma_vec': np.array([12., 4., 25.]),\n         'N': 60000,\n         's': 2468},\n        # Case 3: near-singular high correlation\n        {'mu': np.array([50., 50., 50.]),\n         'C': np.array([[1., 0.99, 0.99], [0.99, 1., 0.99], [0.99, 0.99, 1.]]),\n         'sigma_vec': np.array([10., 10., 10.]),\n         'N': 80000,\n         's': 13579},\n        # Case 4: negative and mixed correlations\n        {'mu': np.array([110., 23., 95.]),\n         'C': np.array([[1., -0.4, 0.1], [-0.4, 1., 0.3], [0.1, 0.3, 1.]]),\n         'sigma_vec': np.array([8., 6., 5.]),\n         'N': 70000,\n         's': 97531},\n    ]\n\n    results = []\n    k_mu = 4.0\n    k_Sigma = 4.0\n\n    for case in test_cases:\n        mu = case['mu']\n        C = case['C']\n        sigma_vec = case['sigma_vec']\n        N = case['N']\n        s = case['s']\n\n        # 1. Construct covariance matrix Sigma\n        D = np.diag(sigma_vec)\n        Sigma = D @ C @ D\n        \n        # 2. Validate Sigma: must be symmetric and positive definite\n        # Check symmetry\n        if not np.allclose(Sigma, Sigma.T):\n            results.append([False, False])\n            continue\n        \n        # Check positive definiteness by attempting Cholesky decomposition\n        try:\n            L = np.linalg.cholesky(Sigma)\n        except np.linalg.LinAlgError:\n            results.append([False, False])\n            continue\n\n        # 3. Generate samples\n        d = len(mu)\n        rng = np.random.default_rng(seed=s)\n        Z = rng.standard_normal(size=(N, d))\n        # X = mu + Z @ L.T\n        X = mu + Z.dot(L.T)\n\n        # 4. Compute empirical statistics\n        # Empirical mean\n        mu_hat = np.mean(X, axis=0)\n        \n        # Empirical covariance (MLE with known mean mu)\n        X_centered_known_mean = X - mu\n        # Sigma_hat = (X_centered_known_mean.T @ X_centered_known_mean) / N\n        Sigma_hat = np.cov(X_centered_known_mean, rowvar=False, ddof=0)\n        \n        # 5. Define tolerance thresholds\n        # Mean tolerance\n        tol_mu = k_mu * np.sqrt(np.diag(Sigma) / N)\n        \n        # Covariance tolerance\n        Sigma_ii = np.diag(Sigma).reshape(-1, 1)\n        Sigma_jj = np.diag(Sigma).reshape(1, -1)\n        Sigma_ii_jj = Sigma_ii @ Sigma_jj\n        tol_Sigma = k_Sigma * np.sqrt((np.square(Sigma) + Sigma_ii_jj) / N)\n\n        # 6. Determine acceptance booleans\n        mean_accepted = np.all(np.abs(mu_hat - mu) <= tol_mu)\n        cov_accepted = np.all(np.abs(Sigma_hat - Sigma) <= tol_Sigma)\n        \n        results.append([bool(mean_accepted), bool(cov_accepted)])\n\n    # 7. Format final output\n    formatted_results = [str(r).replace(' ', '') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}