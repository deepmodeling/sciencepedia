{
    "hands_on_practices": [
        {
            "introduction": "A core principle in histology is that a cell's structure reflects its function. This exercise () challenges you to apply this principle by distinguishing an active, antibody-secreting plasma cell from its quiescent memory B cell counterpart. By correlating morphological features like the perinuclear 'hof' and cytoplasmic basophilia with the underlying cellular machinery, you will solidify your understanding of the protein secretion pathway.",
            "id": "4909636",
            "problem": "A histology laboratory is comparing two populations of B lymphocyte derivatives within a human lymph node section: one population consists of terminal antibody-secreting cells, and the other consists of antigen-experienced but non-secreting cells poised for rapid reactivation. The laboratory aims to distinguish these populations using morphometric features in routine hematoxylin and eosin stained sections and immunohistochemical markers, and to rationalize organelle differences from first principles of protein biosynthesis and trafficking. Use the following foundational base: the Central Dogma of Molecular Biology (deoxyribonucleic acid to ribonucleic acid to protein), the role of the rough endoplasmic reticulum (RER) in co-translational translocation of secretory proteins, and the function of the Golgi apparatus in post-translational processing and export. In routine sections, basophilia reflects ribonucleic acid-rich cytoplasm, and a perinuclear \"hof\" reflects the position of the Golgi apparatus. In immunohistochemistry, plasma cells typically express syndecan-1 (cluster of differentiation, CD138) and downregulate B cell antigens such as CD20, whereas memory B cells commonly retain B-lineage antigens including CD20 and CD27.\n\nWhich option most accurately integrates morphometric features and marker expression to differentiate plasma cells from memory B cells and correctly explains, from the above principles, why the RER is expanded in plasma cells?\n\nA. Plasma cells are identified by an eccentric nucleus with coarse \"clock-face\" chromatin, abundant basophilic cytoplasm, and a conspicuous perinuclear hof corresponding to the Golgi apparatus; they are CD138 positive and typically lack CD20. Memory B cells have a round, centrally placed nucleus, scant cytoplasm, and retain CD20 and CD27. RER expansion in plasma cells arises because immunoglobulin polypeptides, bearing signal peptides, undergo co-translational translocation into the RER and high secretory throughput activates quality-control pathways that increase protein-folding capacity, thereby enlarging the RER to meet sustained antibody export demands.\n\nB. Plasma cells are small cells with a central nucleus, minimal cytoplasm, and no perinuclear hof; they are CD27 and CD20 positive. Memory B cells have eccentric nuclei and perinuclear clearing. RER expansion in plasma cells is primarily driven by mitochondrial biogenesis to fuel class-switch recombination.\n\nC. Plasma cells are CD138 positive but their perinuclear hof represents lysosome clustering for antigen processing; RER expansion occurs because plasma cells act as professional phagocytes that increase endocytosis rather than protein secretion. Memory B cells are CD138 negative and have abundant basophilic cytoplasm.\n\nD. Memory B cells are defined by strong CD138 expression and a prominent perinuclear hof reflecting active secretion, whereas plasma cells are CD20 positive, small, and have scant cytoplasm. RER expansion in plasma cells reflects increased synthesis of cytoskeletal proteins for motility rather than secretory proteins.\n\nE. Plasma cells exhibit an eccentric nucleus, perinuclear hof, and CD138 positivity, and memory B cells retain CD20 and CD27; however, the principal driver of RER expansion in plasma cells is increased lipid synthesis for membrane biogenesis in the smooth endoplasmic reticulum rather than elevated protein export.",
            "solution": "The problem statement is valid. It is scientifically grounded in established principles of immunology, cell biology, and histology. The terms are well-defined, the premises are factually correct, and the question is well-posed, requiring the integration of multiple layers of biological knowledge to identify the most accurate description among the given options.\n\nThe problem asks to differentiate between two cell populations—terminal antibody-secreting plasma cells and quiescent, antigen-experienced memory B cells—using morphology, immunohistochemistry, and an explanation for organelle differences derived from first principles.\n\nLet us derive the expected characteristics of each cell type from the provided foundational base.\n\n**1. Functional Role and\nConsequences for Protein Synthesis Machinery**\n\n*   **Plasma Cells:** Defined as \"terminal antibody-secreting cells\". Their primary function is the mass production and export of proteins (immunoglobulins/antibodies). According to the central dogma (DNA $\\rightarrow$ RNA $\\rightarrow$ protein) and the principles of protein trafficking, secreted proteins are synthesized on ribosomes attached to the rough endoplasmic reticulum (RER). The nascent immunoglobulin polypeptides contain a signal peptide that directs them into the RER lumen via co-translational translocation.\n*   To sustain high-rate secretion, a plasma cell must possess an extensive protein synthesis and export apparatus. This necessitates:\n    1.  An expanded **Rough Endoplasmic Reticulum (RER)** to accommodate the massive flux of protein synthesis, folding, and assembly.\n    2.  A large, highly active **Golgi apparatus** to further process, sort, and package the antibodies into secretory vesicles for exocytosis.\n*   **Memory B Cells:** Defined as \"non-secreting cells poised for rapid reactivation\". They are in a relatively quiescent state and are not actively secreting antibodies. Therefore, they do not require an extensive protein synthesis or export machinery. Their RER and Golgi apparatus would be correspondingly small and undeveloped compared to a plasma cell.\n\n**2. Histological Morphology (Hematoxylin & Eosin Stain)**\n\n*   **Plasma Cells:**\n    *   **Cytoplasm:** The vast network of RER is studded with ribosomes, which are rich in ribosomal ribonucleic acid (rRNA). The problem states that basophilia reflects RNA-rich cytoplasm. Consequently, plasma cells exhibit abundant, intensely **basophilic** (blue/purple) cytoplasm.\n    *   **Golgi Apparatus:** The large Golgi apparatus, required for protein processing, is located near the nucleus. As stated, it creates a pale, unstained area in H&E sections known as a **perinuclear hof** or halo.\n    *   **Nucleus:** Due to the large cytoplasmic volume, the nucleus is typically pushed to one side, resulting in an **eccentric** position. As a terminally differentiated cell, its chromatin is condensed, often clumping at the periphery of the nucleus, creating a characteristic **\"clock-face\"** or \"cartwheel\" pattern.\n*   **Memory B Cells:**\n    *   **Cytoplasm:** As quiescent lymphocytes with minimal RER, they have a **scant** or minimal amount of cytoplasm, which is not strongly basophilic.\n    *   **Golgi Apparatus:** Their Golgi is small, so no perinuclear hof is visible.\n    *   **Nucleus:** The nucleus is typically large relative to the cell size, round, and **centrally located**. The chromatin is condensed (as it is a quiescent cell) but does not typically show the classic clock-face pattern of a plasma cell.\n\n**3. Immunohistochemical (IHC) Markers**\n\nThe problem explicitly provides these markers:\n*   **Plasma cells:** Express syndecan-1 (cluster of differentiation, CD138) and lose expression of pan-B cell markers like CD20.\n*   **Memory B cells:** Retain B-lineage antigens, including CD20 and CD27.\n\n**Summary of Differentiating Features:**\n\n| Feature | Plasma Cell | Memory B Cell |\n| :--- | :--- | :--- |\n| **Function** | Antibody Secretion | Quiescent Memory |\n| **Cytoplasm** | Abundant, Basophilic | Scant |\n| **Nucleus** | Eccentric, \"Clock-face\" | Central, Round |\n| **Perinuclear Hof** | Prominent | Absent |\n| **RER/Golgi** | Extensive / Large | Minimal / Small |\n| **IHC Markers** | CD138+, CD20- | CD20+, CD27+ |\n\nNow we evaluate each option against this rigorously derived profile.\n\n**Option A:** Plasma cells are identified by an eccentric nucleus with coarse \"clock-face\" chromatin, abundant basophilic cytoplasm, and a conspicuous perinuclear hof corresponding to the Golgi apparatus; they are CD138 positive and typically lack CD20. Memory B cells have a round, centrally placed nucleus, scant cytoplasm, and retain CD20 and CD27. RER expansion in plasma cells arises because immunoglobulin polypeptides, bearing signal peptides, undergo co-translational translocation into the RER and high secretory throughput activates quality-control pathways that increase protein-folding capacity, thereby enlarging the RER to meet sustained antibody export demands.\n*   **Analysis:** This option perfectly matches our derived summary. The descriptions of plasma cell morphology (eccentric nucleus, clock-face chromatin, basophilic cytoplasm, hof), plasma cell IHC (CD138+, CD20-), memory B cell morphology (central nucleus, scant cytoplasm), and memory B cell IHC (CD20+, CD27+) are all correct. The explanation for RER expansion is also excellent, correctly linking the function (secretion) to the structure (expanded RER) via the fundamental principles of co-translational translocation of signal-peptide-bearing proteins. The mention of activating quality-control pathways to increase folding capacity is a sophisticated and accurate detail corresponding to the unfolded protein response (UPR), which is essential for managing high protein loads in the ER.\n*   **Verdict: Correct.**\n\n**Option B:** Plasma cells are small cells with a central nucleus, minimal cytoplasm, and no perinuclear hof; they are CD27 and CD20 positive. Memory B cells have eccentric nuclei and perinuclear clearing. RER expansion in plasma cells is primarily driven by mitochondrial biogenesis to fuel class-switch recombination.\n*   **Analysis:** This option incorrectly swaps the morphological and IHC characteristics of plasma cells and memory B cells. The explanation for RER expansion is also incorrect; class-switch recombination occurs in B cells prior to terminal differentiation into plasma cells, and while protein synthesis is energy-intensive, the primary driver for RER expansion is the demand for secretory protein synthesis, not mitochondrial biogenesis.\n*   **Verdict: Incorrect.**\n\n**Option C:** Plasma cells are CD138 positive but their perinuclear hof represents lysosome clustering for antigen processing; RER expansion occurs because plasma cells act as professional phagocytes that increase endocytosis rather than protein secretion. Memory B cells are CD138 negative and have abundant basophilic cytoplasm.\n*   **Analysis:** This option contains multiple fundamental errors. It misidentifies the perinuclear hof (it is the Golgi, not lysosomes). It fundamentally misrepresents the function of a plasma cell as a phagocyte instead of a secretory cell. It incorrectly ascribes abundant basophilic cytoplasm to memory B cells.\n*   **Verdict: Incorrect.**\n\n**Option D:** Memory B cells are defined by strong CD138 expression and a prominent perinuclear hof reflecting active secretion, whereas plasma cells are CD20 positive, small, and have scant cytoplasm. RER expansion in plasma cells reflects increased synthesis of cytoskeletal proteins for motility rather than secretory proteins.\n*   **Analysis:** This option, like B, inverts the identities of the two cell types. The explanation for RER expansion is also flawed. Cytoskeletal proteins are synthesized on free ribosomes, not the RER, and the dominant function driving RER expansion in plasma cells is antibody secretion, not motility.\n*   **Verdict: Incorrect.**\n\n**Option E:** Plasma cells exhibit an eccentric nucleus, perinuclear hof, and CD138 positivity, and memory B cells retain CD20 and CD27; however, the principal driver of RER expansion in plasma cells is increased lipid synthesis for membrane biogenesis in the smooth endoplasmic reticulum rather than elevated protein export.\n*   **Analysis:** This option begins by correctly stating the morphological and IHC features. However, the explanation for RER expansion is incorrect. While the endoplasmic reticulum system as a whole expands, and the smooth endoplasmic reticulum (SER) is involved in lipid synthesis for membranes, the defining feature and name of the *rough* endoplasmic reticulum (RER) relate to its ribosome-studded appearance and its primary role in synthesizing secretory and membrane proteins. The massive expansion of the RER in plasma cells is driven directly by the demand for protein export, not by lipid synthesis, which is an SER function. The explanation misattributes the cause.\n*   **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Histological analysis is not just about qualitative description; it is increasingly a quantitative science. This practice () takes you into the realm of quantitative hematopathology by tasking you with the calculation of the Myeloid-to-Erythroid (M:E) ratio from bone marrow cell counts. You will apply statistical principles to not only estimate this crucial diagnostic parameter but also to construct a confidence interval, reflecting the precision of your assessment.",
            "id": "4909669",
            "problem": "In a bone marrow core biopsy from an adult patient without known hematologic disease, a hematopathologist performs systematic high-power field counts to assess granulopoietic activity. Across representative fields, the observer identifies a total of $n = 500$ nucleated precursors, classifying each as either myeloid or erythroid based on standard histologic criteria. The counts are $m = 360$ myeloid precursors and $e = 140$ erythroid precursors. Assume that each observed precursor is an independent draw from a stable underlying population in which the probability that a randomly sampled precursor is myeloid is $p$, so that the myeloid count $m$ is modeled by a binomial distribution with parameters $(n, p)$, and the erythroid count is $e = n - m$. The myeloid-to-erythroid ratio (M:E) is defined as $R = \\frac{p}{1 - p}$.\n\nUsing only statistically and biologically well-tested facts and core definitions applicable to histologic cell classification, and without invoking any ad hoc shortcuts, do the following:\n\n- Derive an estimator $\\hat{R}$ of the M:E ratio from the observed counts.\n- Construct a two-sided confidence interval for $p$ at confidence level $0.95$ that has good coverage for binomial data, and then map this interval to a confidence interval for $R$ via the transformation $R = \\frac{p}{1 - p}$ justified by monotonicity. Let $z$ denote the standard normal quantile corresponding to confidence level $0.95$.\n- Report the final numerical results for the point estimate $\\hat{R}$ and the lower and upper endpoints of the confidence interval for $R$ rounded to four significant figures. Express your final result as a row vector $(\\hat{R}, R_{\\mathrm{L}}, R_{\\mathrm{U}})$. No units are required because $R$ is dimensionless.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- Total number of nucleated precursors observed: $n = 500$.\n- Number of myeloid precursors observed: $m = 360$.\n- Number of erythroid precursors observed: $e = 140$.\n- The sum is consistent: $m + e = 360 + 140 = 500 = n$.\n- The myeloid count $m$ is modeled as a draw from a binomial distribution, $m \\sim B(n, p)$, where $p$ is the true underlying proportion of myeloid precursors.\n- The myeloid-to-erythroid ratio (M:E) is defined as $R = \\frac{p}{1-p}$.\n- The confidence level for the interval is specified as $0.95$.\n- Notation $z$ is used for the standard normal quantile corresponding to the confidence level $0.95$.\n- The requested output is a row vector $(\\hat{R}, R_{\\mathrm{L}}, R_{\\mathrm{U}})$, where $\\hat{R}$ is the point estimate of $R$, and $(R_{\\mathrm{L}}, R_{\\mathrm{U}})$ are the lower and upper bounds of the confidence interval for $R$, with all values rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding:** The M:E ratio is a standard metric in hematopathology, used to assess hematopoiesis in bone marrow. The binomial distribution is the correct and standard model for count data representing successes (e.g., myeloid cells) in a fixed number of independent trials (e.g., total cells counted). The specified numerical values ($m=360$, $e=140$) yield a sample M:E ratio of $360/140 \\approx 2.57:1$, which is within the typical physiological range for an adult (approximately $2:1$ to $4:1$). The methods requested (estimation and confidence interval construction) are fundamental applications of biostatistics.\n- **Well-Posedness:** The problem provides all necessary data ($n$, $m$), a clear statistical model, and specific deliverables (a point estimate and a confidence interval). The request for a confidence interval with \"good coverage\" is a standard directive in applied statistics, pointing towards robust methods like the Wilson score interval, which avoids the known deficiencies of simpler approximations.\n- **Objectivity:** The problem is stated in precise, formal language without subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous solution can be constructed.\n\n### Solution Derivation\n\n**1. Point Estimator for the M:E Ratio, $\\hat{R}$**\n\nThe parameter of interest is the true proportion of myeloid precursors, $p$. The count of myeloid precursors, $m$, follows a binomial distribution $B(n, p)$. The maximum likelihood estimator (MLE) for a binomial proportion $p$ is the sample proportion:\n$$\n\\hat{p} = \\frac{m}{n}\n$$\nThe M:E ratio is defined as a function of $p$:\n$$\nR(p) = \\frac{p}{1-p}\n$$\nBy the invariance property of maximum likelihood estimators, the MLE for $R$ is found by applying the function $R$ to the MLE for $p$:\n$$\n\\hat{R} = R(\\hat{p}) = \\frac{\\hat{p}}{1-\\hat{p}}\n$$\nSubstituting $\\hat{p} = \\frac{m}{n}$:\n$$\n\\hat{R} = \\frac{m/n}{1 - m/n} = \\frac{m/n}{(n-m)/n} = \\frac{m}{n-m}\n$$\nGiven that the number of erythroid precursors is $e = n - m$, the estimator is simply the ratio of the observed counts:\n$$\n\\hat{R} = \\frac{m}{e}\n$$\nUsing the provided data, $m=360$ and $e=140$:\n$$\n\\hat{R} = \\frac{360}{140} = \\frac{18}{7} \\approx 2.571428...\n$$\n\n**2. Confidence Interval for the Proportion, $p$**\n\nThe problem requires a confidence interval with good coverage properties. The standard Wald interval performs poorly for proportions, especially as $p$ approaches $0$ or $1$. The Wilson score interval is a superior alternative derived by inverting the score test, and it is suitable here.\n\nThe two-sided $95\\%$ confidence interval for $p$ is given by the set of values $p_0$ for which the score test of the null hypothesis $H_0: p = p_0$ is not rejected at the $\\alpha = 0.05$ significance level. This corresponds to finding the roots of the equation:\n$$\n\\frac{(\\hat{p} - p)^2}{p(1-p)/n} = z_{\\alpha/2}^2\n$$\nFor a $0.95$ confidence level, $\\alpha = 0.05$, so $\\alpha/2 = 0.025$. The corresponding standard normal quantile is $z = z_{0.025} \\approx 1.96$.\nRearranging the equation yields a quadratic equation in $p$:\n$$\n(n+z^2)p^2 - (2n\\hat{p} + z^2)p + n\\hat{p}^2 = 0\n$$\nSolving for $p$ using the quadratic formula gives the lower ($p_L$) and upper ($p_U$) bounds of the Wilson score interval:\n$$\n(p_L, p_U) = \\frac{\\hat{p} + \\frac{z^2}{2n}}{1 + \\frac{z^2}{n}} \\pm \\frac{z}{1 + \\frac{z^2}{n}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z^2}{4n^2}}\n$$\nWe have $n=500$, $m=360$, so $\\hat{p} = \\frac{360}{500} = 0.72$. The quantile is $z \\approx 1.96$.\nLet's compute the terms:\n$$\n\\frac{z^2}{n} = \\frac{1.96^2}{500} = \\frac{3.8416}{500} = 0.0076832\n$$\nThe denominator is $1 + \\frac{z^2}{n} = 1.0076832$.\nThe adjusted center of the interval is:\n$$\n\\tilde{p} = \\frac{\\hat{p} + z^2/(2n)}{1 + z^2/n} = \\frac{0.72 + 3.8416/1000}{1.0076832} = \\frac{0.7238416}{1.0076832} \\approx 0.718306\n$$\nThe half-width (margin of error) is:\n$$\nW = \\frac{z}{1+z^2/n} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z^2}{4n^2}} = \\frac{1.96}{1.0076832} \\sqrt{\\frac{0.72 \\times 0.28}{500} + \\frac{3.8416}{4 \\times 500^2}}\n$$\n$$\nW = \\frac{1.96}{1.0076832} \\sqrt{0.0004032 + 0.0000038416} = \\frac{1.96}{1.0076832} \\sqrt{0.0004070416}\n$$\n$$\nW \\approx 1.94503 \\times 0.02017527 \\approx 0.039243\n$$\nThe confidence interval for $p$ is therefore:\n$$\np_L = \\tilde{p} - W \\approx 0.718306 - 0.039243 = 0.679063\n$$\n$$\np_U = \\tilde{p} + W \\approx 0.718306 + 0.039243 = 0.757549\n$$\n\n**3. Confidence Interval for the M:E Ratio, $R$**\n\nThe function $R(p) = \\frac{p}{1-p}$ relates the proportion $p$ to the M:E ratio $R$. The derivative of this function is $R'(p) = \\frac{1}{(1-p)^2}$. Since $p \\in (0, 1)$, $R'(p)$ is always positive, meaning $R(p)$ is a strictly monotonic increasing function of $p$.\nThis monotonicity allows us to transform the confidence interval for $p$, $(p_L, p_U)$, directly into a confidence interval for $R$:\n$$\nR_L = R(p_L) = \\frac{p_L}{1-p_L}\n$$\n$$\nR_U = R(p_U) = \\frac{p_U}{1-p_U}\n$$\nUsing the bounds for $p$ calculated above:\n$$\nR_L = \\frac{0.679063}{1 - 0.679063} = \\frac{0.679063}{0.320937} \\approx 2.11584...\n$$\n$$\nR_U = \\frac{0.757549}{1 - 0.757549} = \\frac{0.757549}{0.242451} \\approx 3.12450...\n$$\n\n**4. Final Numerical Results**\n\nThe problem asks for the point estimate $\\hat{R}$ and the confidence interval bounds $(R_L, R_U)$ rounded to four significant figures.\n- Point Estimate: $\\hat{R} = \\frac{18}{7} \\approx 2.571428... \\to 2.571$\n- Lower Bound: $R_L \\approx 2.11584... \\to 2.116$\n- Upper Bound: $R_U \\approx 3.12450... \\to 3.125$\n\nThe final result is the row vector $(\\hat{R}, R_{\\mathrm{L}}, R_{\\mathrm{U}})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.571 & 2.116 & 3.125\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "How can the trained eye of a histologist be translated into an automated algorithm? This advanced practice () explores this question by focusing on the maturation of neutrophils, a key process in the immune response. You will learn how a subtle morphological feature—nuclear lobulation—can be quantified and used to build a computational classifier that distinguishes band from segmented neutrophils using principles of Bayesian decision theory.",
            "id": "4909684",
            "problem": "An image-based assessment of granulocytic nuclear morphology can be summarized by a single scalar nuclear curvature metric, denoted $\\,\\kappa\\,$. Assume a pipeline has segmented nuclei and extracted $\\,\\kappa\\,$ per cell. To distinguish band neutrophils (less lobulated) from segmented neutrophils (more lobulated), posit the following statistically grounded model: conditioned on the true class, the measured curvature $\\,\\kappa\\,$ is a Gaussian random variable (i.e., normally distributed) due to segmentation noise and biological variability. Specifically, let $\\,\\kappa \\mid B \\sim \\mathcal{N}(\\mu_B,\\sigma_B^2)\\,$ for the band class $\\,B\\,$ and $\\,\\kappa \\mid S \\sim \\mathcal{N}(\\mu_S,\\sigma_S^2)\\,$ for the segmented class $\\,S\\,$, with class priors $\\,\\pi_B\\,$ and $\\,\\pi_S\\,$ where $\\,\\pi_B + \\pi_S = 1\\,$ and $\\,\\pi_B>0\\,$, $\\,\\pi_S>0\\,$. Consider a single-threshold classifier that assigns a cell to class $\\,S\\,$ if $\\,\\kappa \\ge \\kappa^\\star\\,$ and to class $\\,B\\,$ otherwise.\n\nUsing only the fundamental definitions of the Gaussian probability density function and Bayes decision theory under $\\,0$-$1\\,$ loss with known class priors, derive from first principles the expression characterizing the optimal threshold curvature $\\,\\kappa^\\star\\,$ that minimizes total misclassification risk. Your derivation must start from the definition of total misclassification risk as a function of a decision threshold and proceed by taking a derivative and setting it to zero to find candidate optima. You must handle both the equal-variance and unequal-variance cases in a unified way. If multiple candidate thresholds arise, specify how to select the one that truly minimizes the total misclassification risk.\n\nThen implement a program that, given $\\,(\\mu_B,\\sigma_B,\\mu_S,\\sigma_S,\\pi_B,\\pi_S)\\,$, computes $\\,\\kappa^\\star\\,$ by:\n- Forming the necessary equation(s) implied by your derivation.\n- Solving for all real candidate thresholds.\n- Evaluating the total misclassification risk at each candidate and returning the minimizing $\\,\\kappa^\\star\\,$.\n\nYour program must evaluate the following test suite and output all results on a single line as a comma-separated Python list of floats rounded to exactly $6$ decimal places, in the order shown. No units are required.\n\nTest suite (each tuple is $(\\mu_B,\\sigma_B,\\mu_S,\\sigma_S,\\pi_B,\\pi_S)$):\n- Case $1$: $(0.8,\\,0.4,\\,1.6,\\,0.4,\\,0.5,\\,0.5)$\n- Case $2$: $(1.0,\\,0.3,\\,1.4,\\,0.3,\\,0.7,\\,0.3)$\n- Case $3$: $(0.9,\\,0.25,\\,1.5,\\,0.45,\\,0.5,\\,0.5)$\n- Case $4$: $(1.2,\\,0.2,\\,1.6,\\,0.8,\\,0.4,\\,0.6)$\n\nFinal output format:\n- Your program should produce a single line of output containing the thresholds as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3,x_4]$), with each value rounded to $6$ decimal places.",
            "solution": "The problem requires the derivation of the optimal decision threshold $\\kappa^\\star$ for classifying leukocytes based on a nuclear curvature metric $\\kappa$. The classification is between band neutrophils ($B$) and segmented neutrophils ($S$). The framework is Bayesian decision theory with a $0$-$1$ loss function, meaning the objective is to minimize the total probability of misclassification. The measured curvature $\\kappa$ is modeled by a Gaussian distribution conditioned on the true class: $\\kappa \\mid B \\sim \\mathcal{N}(\\mu_B, \\sigma_B^2)$ and $\\kappa \\mid S \\sim \\mathcal{N}(\\mu_S, \\sigma_S^2)$, with class priors $\\pi_B$ and $\\pi_S$. The classifier is a single-threshold type: a cell is assigned to class $S$ if $\\kappa \\ge \\kappa^\\star$ and to class $B$ if $\\kappa < \\kappa^\\star$.\n\nThe derivation proceeds from first principles.\n\nFirst, we define the total misclassification risk, $R(\\kappa^\\star)$, as a function of the threshold $\\kappa^\\star$. This risk is the sum of the probabilities of the two possible types of errors:\n1.  A cell of true class $B$ is classified as $S$. This occurs when $\\kappa \\ge \\kappa^\\star$. The probability of this event is $P(\\kappa \\ge \\kappa^\\star \\mid B)\\pi_B$.\n2.  A cell of true class $S$ is classified as $B$. This occurs when $\\kappa < \\kappa^\\star$. The probability of this event is $P(\\kappa < \\kappa^\\star \\mid S)\\pi_S$.\n\nThe total risk is the sum of these probabilities:\n$$R(\\kappa^\\star) = \\pi_B P(\\kappa \\ge \\kappa^\\star \\mid B) + \\pi_S P(\\kappa < \\kappa^\\star \\mid S)$$\n\nLet $p(\\kappa \\mid B)$ and $p(\\kappa \\mid S)$ denote the probability density functions (PDFs) of the Gaussian distributions for classes $B$ and $S$, respectively. The risk can be expressed in terms of integrals of these PDFs:\n$$R(\\kappa^\\star) = \\pi_B \\int_{\\kappa^\\star}^{\\infty} p(\\kappa \\mid B) \\, d\\kappa + \\pi_S \\int_{-\\infty}^{\\kappa^\\star} p(\\kappa \\mid S) \\, d\\kappa$$\n\nTo find the threshold $\\kappa^\\star$ that minimizes $R$, we differentiate $R(\\kappa^\\star)$ with respect to $\\kappa^\\star$ and set the derivative to zero. Using the Fundamental Theorem of Calculus, which states $\\frac{d}{dx} \\int_a^x f(t) dt = f(x)$ and $\\frac{d}{dx} \\int_x^b f(t) dt = -f(x)$, we get:\n$$\\frac{dR}{d\\kappa^\\star} = \\pi_B \\frac{d}{d\\kappa^\\star}\\left(\\int_{\\kappa^\\star}^{\\infty} p(\\kappa \\mid B) \\, d\\kappa\\right) + \\pi_S \\frac{d}{d\\kappa^\\star}\\left(\\int_{-\\infty}^{\\kappa^\\star} p(\\kappa \\mid S) \\, d\\kappa\\right)$$\n$$\\frac{dR}{d\\kappa^\\star} = -\\pi_B p(\\kappa^\\star \\mid B) + \\pi_S p(\\kappa^\\star \\mid S)$$\n\nSetting the derivative to zero gives the condition for a critical point of the risk function:\n$$-\\pi_B p(\\kappa^\\star \\mid B) + \\pi_S p(\\kappa^\\star \\mid S) = 0$$\n$$\\pi_B p(\\kappa^\\star \\mid B) = \\pi_S p(\\kappa^\\star \\mid S)$$\nThis equation signifies that at an optimal threshold, the scaled likelihoods of the two classes must be equal.\n\nNext, we substitute the Gaussian PDF formula, $p(\\kappa \\mid C) = \\frac{1}{\\sqrt{2\\pi}\\sigma_C} \\exp\\left(-\\frac{(\\kappa - \\mu_C)^2}{2\\sigma_C^2}\\right)$ for a class $C \\in \\{B, S\\}$:\n$$\\pi_B \\frac{1}{\\sqrt{2\\pi}\\sigma_B} \\exp\\left(-\\frac{(\\kappa^\\star - \\mu_B)^2}{2\\sigma_B^2}\\right) = \\pi_S \\frac{1}{\\sqrt{2\\pi}\\sigma_S} \\exp\\left(-\\frac{(\\kappa^\\star - \\mu_S)^2}{2\\sigma_S^2}\\right)$$\n\nTo solve for $\\kappa^\\star$, we take the natural logarithm of both sides:\n$$\\ln\\left(\\frac{\\pi_B}{\\sigma_B}\\right) - \\frac{(\\kappa^\\star - \\mu_B)^2}{2\\sigma_B^2} = \\ln\\left(\\frac{\\pi_S}{\\sigma_S}\\right) - \\frac{(\\kappa^\\star - \\mu_S)^2}{2\\sigma_S^2}$$\n\nRearranging the terms isolates $\\kappa^\\star$:\n$$\\frac{(\\kappa^\\star - \\mu_S)^2}{2\\sigma_S^2} - \\frac{(\\kappa^\\star - \\mu_B)^2}{2\\sigma_B^2} = \\ln\\left(\\frac{\\pi_S}{\\sigma_S}\\right) - \\ln\\left(\\frac{\\pi_B}{\\sigma_B}\\right) = \\ln\\left(\\frac{\\pi_S \\sigma_B}{\\pi_B \\sigma_S}\\right)$$\n\nExpanding the squared terms and clearing the denominators by multiplying by $2\\sigma_B^2\\sigma_S^2$ leads to:\n$$\\sigma_B^2((\\kappa^\\star)^2 - 2\\kappa^\\star\\mu_S + \\mu_S^2) - \\sigma_S^2((\\kappa^\\star)^2 - 2\\kappa^\\star\\mu_B + \\mu_B^2) = -2\\sigma_B^2\\sigma_S^2 \\ln\\left(\\frac{\\pi_B \\sigma_S}{\\pi_S \\sigma_B}\\right)$$\n\nCollecting terms by powers of $\\kappa^\\star$ reveals a quadratic equation of the form $A(\\kappa^\\star)^2 + B\\kappa^\\star + C = 0$:\n$$(\\sigma_B^2 - \\sigma_S^2)(\\kappa^\\star)^2 + (2\\mu_B\\sigma_S^2 - 2\\mu_S\\sigma_B^2)\\kappa^\\star + (\\mu_S^2\\sigma_B^2 - \\mu_B^2\\sigma_S^2 - 2\\sigma_B^2\\sigma_S^2 \\ln\\left(\\frac{\\pi_B \\sigma_S}{\\pi_S \\sigma_B}\\right)) = 0$$\nNote that we have absorbed the negative sign on the right-hand side into the logarithm.\nThe coefficients are:\n$$A = \\sigma_S^2 - \\sigma_B^2$$\n$$B = 2(\\mu_S\\sigma_B^2 - \\mu_B\\sigma_S^2)$$\n$$C = \\mu_B^2\\sigma_S^2 - \\mu_S^2\\sigma_B^2 - 2\\sigma_B^2\\sigma_S^2 \\ln\\left(\\frac{\\pi_B\\sigma_S}{\\pi_S\\sigma_B}\\right)$$\n\nWe now analyze the two cases for the variances:\n\n1.  **Equal Variances ($\\sigma_B = \\sigma_S = \\sigma$):**\n    In this case, the coefficient $A = \\sigma^2 - \\sigma^2 = 0$, and the equation becomes linear: $B\\kappa^\\star + C = 0$.\n    $B = 2(\\mu_S\\sigma^2 - \\mu_B\\sigma^2) = 2\\sigma^2(\\mu_S - \\mu_B)$.\n    $C = \\mu_B^2\\sigma^2 - \\mu_S^2\\sigma^2 - 2\\sigma^4 \\ln\\left(\\frac{\\pi_B}{\\pi_S}\\right) = \\sigma^2(\\mu_B^2 - \\mu_S^2) - 2\\sigma^4 \\ln\\left(\\frac{\\pi_B}{\\pi_S}\\right)$.\n    The solution is $\\kappa^\\star = -C/B$:\n    $$\\kappa^\\star = -\\frac{\\sigma^2(\\mu_B - \\mu_S)(\\mu_B + \\mu_S) - 2\\sigma^4 \\ln(\\pi_B/\\pi_S)}{2\\sigma^2(\\mu_S - \\mu_B)} = \\frac{(\\mu_B + \\mu_S)}{2} + \\frac{\\sigma^2}{\\mu_S-\\mu_B}\\ln\\left(\\frac{\\pi_B}{\\pi_S}\\right)$$\n    This provides a unique candidate for the optimal threshold.\n\n2.  **Unequal Variances ($\\sigma_B \\neq \\sigma_S$):**\n    Here, $A \\neq 0$, and we must solve the full quadratic equation $A(\\kappa^\\star)^2 + B\\kappa^\\star + C = 0$. The solutions are given by the quadratic formula:\n    $$\\kappa^\\star = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$$\n    Depending on the discriminant $\\Delta = B^2 - 4AC$, there can be zero, one, or two real solutions for $\\kappa^\\star$. These are the critical points of the risk function.\n\nWhen one or more candidate thresholds are found, we must determine which one truly minimizes the total misclassification risk. The critical points found by setting the derivative to zero can be local minima, maxima, or inflection points. The optimal threshold is the one that corresponds to the global minimum of the risk function. To find it, we evaluate the risk $R(\\kappa^\\star)$ at each of the real candidates and select the one yielding the smallest risk value. The risk function can be computed using the Gaussian cumulative distribution function (CDF), denoted $\\Phi(z)$.\n$$R(\\kappa^\\star) = \\pi_B \\left(1 - \\Phi\\left(\\frac{\\kappa^\\star - \\mu_B}{\\sigma_B}\\right)\\right) + \\pi_S \\Phi\\left(\\frac{\\kappa^\\star - \\mu_S}{\\sigma_S}\\right)$$\nThis procedure guarantees finding the optimal single threshold $\\kappa^\\star$ under the problem's constraints. The following program implements this derivation to solve for $\\kappa^\\star$ for the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves for the optimal classification threshold for a series of test cases.\n    \"\"\"\n    \n    # Each tuple is (mu_B, sigma_B, mu_S, sigma_S, pi_B, pi_S)\n    test_cases = [\n        (0.8, 0.4, 1.6, 0.4, 0.5, 0.5),\n        (1.0, 0.3, 1.4, 0.3, 0.7, 0.3),\n        (0.9, 0.25, 1.5, 0.45, 0.5, 0.5),\n        (1.2, 0.2, 1.6, 0.8, 0.4, 0.6),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_B, sigma_B, mu_S, sigma_S, pi_B, pi_S = case\n        k_opt = find_optimal_threshold(mu_B, sigma_B, mu_S, sigma_S, pi_B, pi_S)\n        results.append(k_opt)\n\n    # Format the final output as a comma-separated list of floats\n    # rounded to 6 decimal places, enclosed in brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef find_optimal_threshold(mu_B, sigma_B, mu_S, sigma_S, pi_B, pi_S):\n    \"\"\"\n    Calculates the optimal single threshold k_star that minimizes total misclassification risk.\n\n    Args:\n        mu_B (float): Mean for class B.\n        sigma_B (float): Standard deviation for class B.\n        mu_S (float): Mean for class S.\n        sigma_S (float): Standard deviation for class S.\n        pi_B (float): Prior probability for class B.\n        pi_S (float): Prior probability for class S.\n\n    Returns:\n        float: The optimal threshold k_star.\n    \"\"\"\n    sigma_B_sq = sigma_B**2\n    sigma_S_sq = sigma_S**2\n\n    # Case 1: Equal variances\n    if np.isclose(sigma_B, sigma_S):\n        # The equation for the threshold is linear.\n        # This is expected to have a unique minimum.\n        if np.isclose(mu_S, mu_B):\n            # A degenerate case, but handle it. If means are same, decision boundary depends on priors.\n            # If priors are same, any threshold between means works, e.g., the mean itself.\n            return mu_B\n        \n        log_term = np.log(pi_B / pi_S)\n        k_star = (mu_B + mu_S) / 2.0 + (sigma_B_sq / (mu_S - mu_B)) * log_term\n        return k_star\n\n    # Case 2: Unequal variances -> solve a quadratic equation A*k^2 + B*k + C = 0\n    A = sigma_S_sq - sigma_B_sq\n    B = 2 * (mu_S * sigma_B_sq - mu_B * sigma_S_sq)\n    \n    # The term inside the logarithm can be zero or negative if priors are zero, but the problem states pi_B, pi_S > 0.\n    log_arg = (pi_B * sigma_S) / (pi_S * sigma_B)\n    log_val = np.log(log_arg)\n    \n    C = mu_B**2 * sigma_S_sq - mu_S**2 * sigma_B_sq - 2 * sigma_B_sq * sigma_S_sq * log_val\n    \n    discriminant = B**2 - 4 * A * C\n    \n    candidates = []\n    if discriminant >= 0:\n        sqrt_discriminant = np.sqrt(discriminant)\n        k1 = (-B - sqrt_discriminant) / (2 * A)\n        k2 = (-B + sqrt_discriminant) / (2 * A)\n        candidates.append(k1)\n        if not np.isclose(discriminant, 0):\n            candidates.append(k2)\n    \n    if not candidates:\n        # If no real solutions, the minimum risk occurs at +/- infinity,\n        # meaning one class is always chosen over the other. \n        # The minimal risk is min(pi_B, pi_S).\n        # This problem context implies a finite threshold exists.\n        # This case is not expected for the given test data.\n        # A robust implementation might return np.nan # Placeholder for an unhandled edge case\n        risk_b = pi_S # Risk if we always classify as B\n        risk_s = pi_B # Risk if we always classify as S\n        return -np.inf if risk_b  risk_s else np.inf\n\n\n    # Evaluate risk at each candidate to find the true minimum\n    risks = [calculate_risk(k, mu_B, sigma_B, mu_S, sigma_S, pi_B, pi_S) for k in candidates]\n    \n    min_risk_index = np.argmin(risks)\n    return candidates[min_risk_index]\n\ndef calculate_risk(k_star, mu_B, sigma_B, mu_S, sigma_S, pi_B, pi_S):\n    \"\"\"\n    Computes the total misclassification risk for a given threshold k_star.\n    \"\"\"\n    def phi(z):\n        # Gaussian CDF using the error function erf\n        return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n\n    # Standardized variables\n    z_B = (k_star - mu_B) / sigma_B\n    z_S = (k_star - mu_S) / sigma_S\n    \n    # Risk = P(classify S | B) * P(B) + P(classify B | S) * P(S)\n    # P(classify S | B) = P(k = k_star | B) = 1 - CDF(k_star) for B\n    # P(classify B | S) = P(k  k_star | S) = CDF(k_star) for S\n    risk = pi_B * (1.0 - phi(z_B)) + pi_S * phi(z_S)\n    return risk\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}