{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of any Health Information Exchange (HIE) is its ability to accurately link records for the same patient across different healthcare organizations. This exercise delves into the Fellegi-Sunter model, a probabilistic framework that uses Bayesian decision theory to weigh evidence from various demographic fields. By working through this problem, you will gain hands-on experience in building the core logic of a patient matching service, translating theoretical probabilities into a concrete classification of match, non-match, or clerical review .",
            "id": "4841814",
            "problem": "A Health Information Exchange (HIE) network must probabilistically link patient records received from two hospital systems. The network uses the Fellegi–Sunter (FS) probabilistic record linkage framework based on Bayes’ decision theory, where field comparisons yield an evidence score via a likelihood ratio constructed from the probability of agreement given a true match (the $m$-probability) and the probability of agreement given a true non-match (the $u$-probability). Assume conditional independence of field comparisons given match status.\n\nStarting from Bayes’ theorem and the definition of the likelihood ratio, derive the two decision thresholds for three-way classification (automatic match, clerical review, automatic non-match) that achieve fixed posterior probability criteria: automatically declare a match when the posterior probability of being a true match is at least $q_{u}$, automatically declare a non-match when the posterior probability is at most $q_{\\ell}$, and otherwise send the pair for clerical review.\n\nThen, for a blocked candidate set with prior match probability $p=0.01$ and these posterior probability criteria $q_{u}=0.95$ and $q_{\\ell}=0.05$, compute the thresholds numerically.\n\nFinally, consider a single candidate pair with the following five fields and their respective $m$- and $u$-probabilities of agreement:\n- First name exact agreement: $m_{1}=0.88$, $u_{1}=0.03$.\n- Last name exact agreement: $m_{2}=0.95$, $u_{2}=0.01$.\n- Date of birth exact agreement: $m_{3}=0.97$, $u_{3}=0.002$.\n- Gender agreement: $m_{4}=0.99$, $u_{4}=0.50$.\n- Five-digit zip code agreement: $m_{5}=0.90$, $u_{5}=0.08$.\n\nSuppose the observed comparison outcomes for this pair are agreement on fields $1$, $2$, $4$, and $5$, and a disagreement on field $3$. Using the FS model, compute the total log-likelihood ratio weight $W=\\ln(L)$ for this pair, where $L$ is the likelihood ratio formed by multiplying the appropriate per-field ratios for the observed agreements and disagreements. Determine the classification of the pair relative to the derived thresholds within your working. Round your final answer for $W$ to four significant figures. Express the final answer as a pure number with no units.",
            "solution": "The problem requires the derivation of decision thresholds for a three-way classification in probabilistic record linkage, followed by a numerical calculation for a specific case. The framework is the Fellegi-Sunter model, which is based on Bayesian decision theory.\n\nFirst, we derive the decision thresholds. Let $M$ be the event that a candidate pair of records is a true match, and $U$ be the event that it is a true non-match ($U = M^c$). Let $C$ represent the vector of comparison outcomes for the fields being compared. The prior probability of a match in a blocked candidate set is given as $p = P(M)$, so the prior probability of a non-match is $P(U) = 1-p$.\n\nAccording to Bayes' theorem, the posterior probability of a match given the comparison outcomes $C$ is:\n$$P(M|C) = \\frac{P(C|M) P(M)}{P(C)}$$\nThe denominator, $P(C)$, can be expanded using the law of total probability:\n$$P(C) = P(C|M)P(M) + P(C|U)P(U)$$\nSubstituting this into the expression for the posterior probability gives:\n$$P(M|C) = \\frac{P(C|M) P(M)}{P(C|M)P(M) + P(C|U)P(U)}$$\nThe likelihood ratio, $L$, is defined as the ratio of the probability of observing the comparison outcomes given a true match to the probability of observing them given a true non-match:\n$$L = \\frac{P(C|M)}{P(C|U)}$$\nTo express the posterior probability in terms of the likelihood ratio $L$ and the prior probability $p$, we divide the numerator and denominator of the posterior probability expression by $P(C|U)$:\n$$P(M|C) = \\frac{\\frac{P(C|M)}{P(C|U)} P(M)}{\\frac{P(C|M)}{P(C|U)} P(M) + P(U)} = \\frac{Lp}{Lp + (1-p)}$$\nThe problem specifies two posterior probability criteria for classification: an upper threshold $q_u$ and a lower threshold $q_{\\ell}$.\nAn automatic match is declared if $P(M|C) \\ge q_u$. We solve for the corresponding threshold on $L$, which we will call $T_u$.\n$$\\frac{Lp}{Lp + (1-p)} \\ge q_u$$\n$$Lp \\ge q_u(Lp + 1-p)$$\n$$Lp \\ge q_u Lp + q_u(1-p)$$\n$$Lp(1-q_u) \\ge q_u(1-p)$$\n$$L \\ge \\frac{q_u}{1-q_u} \\frac{1-p}{p}$$\nThus, the likelihood ratio threshold for an automatic match is $T_u = \\frac{q_u}{1-q_u} \\frac{1-p}{p}$.\n\nAn automatic non-match is declared if $P(M|C) \\le q_{\\ell}$. We solve for the corresponding threshold on $L$, which we will call $T_\\ell$.\n$$\\frac{Lp}{Lp + (1-p)} \\le q_{\\ell}$$\n$$Lp \\le q_{\\ell}(Lp + 1-p)$$\n$$Lp(1-q_{\\ell}) \\le q_{\\ell}(1-p)$$\n$$L \\le \\frac{q_{\\ell}}{1-q_{\\ell}} \\frac{1-p}{p}$$\nThus, the likelihood ratio threshold for an automatic non-match is $T_\\ell = \\frac{q_{\\ell}}{1-q_{\\ell}} \\frac{1-p}{p}$.\n\nThe problem uses the log-likelihood ratio weight, $W = \\ln(L)$. The decision rules are based on thresholds for $W$:\n- Automatic match if $W \\ge W_u$, where $W_u = \\ln(T_u) = \\ln\\left(\\frac{q_u}{1-q_u} \\frac{1-p}{p}\\right)$.\n- Automatic non-match if $W \\le W_\\ell$, where $W_\\ell = \\ln(T_\\ell) = \\ln\\left(\\frac{q_{\\ell}}{1-q_{\\ell}} \\frac{1-p}{p}\\right)$.\n- Clerical review if $W_\\ell  W  W_u$.\n\nNext, we compute these thresholds numerically using the given values: prior match probability $p=0.01$, and posterior probability criteria $q_u=0.95$ and $q_\\ell=0.05$.\nThe prior odds ratio term is $\\frac{1-p}{p} = \\frac{1-0.01}{0.01} = \\frac{0.99}{0.01} = 99$.\nThe upper threshold for the log-likelihood ratio is:\n$$W_u = \\ln\\left(\\frac{0.95}{1-0.95} \\cdot 99\\right) = \\ln\\left(\\frac{0.95}{0.05} \\cdot 99\\right) = \\ln(19 \\cdot 99) = \\ln(1881) \\approx 7.5396$$\nThe lower threshold for the log-likelihood ratio is:\n$$W_\\ell = \\ln\\left(\\frac{0.05}{1-0.05} \\cdot 99\\right) = \\ln\\left(\\frac{0.05}{0.95} \\cdot 99\\right) = \\ln\\left(\\frac{1}{19} \\cdot 99\\right) = \\ln\\left(\\frac{99}{19}\\right) \\approx 1.6507$$\nSo, the decision rules are: match if $W \\ge 7.5396$, non-match if $W \\le 1.6507$, and review if $1.6507  W  7.5396$.\n\nFinally, we compute the total log-likelihood ratio weight $W$ for the given candidate pair. The Fellegi-Sunter model assumes conditional independence of field comparisons given the match status. This means the total likelihood ratio $L$ is the product of the individual field likelihood ratios, $L = \\prod_{i=1}^k L_i$. Consequently, the total log-likelihood weight $W$ is the sum of the individual field weights, $W = \\sum_{i=1}^k w_i$, where $w_i = \\ln(L_i)$.\n\nFor a field $i$ where the comparison results in agreement, the likelihood ratio is $L_i = \\frac{m_i}{u_i}$, and the weight is $w_i = \\ln(\\frac{m_i}{u_i})$.\nFor a field $i$ where the comparison results in disagreement, the likelihood ratio is $L_i = \\frac{1-m_i}{1-u_i}$, and the weight is $w_i = \\ln(\\frac{1-m_i}{1-u_i})$.\n\nThe specific pair has agreement on fields $1$, $2$, $4$, $5$ and disagreement on field $3$. We calculate the weight for each field:\n- Field 1 (agreement): $m_1=0.88$, $u_1=0.03$. $w_1 = \\ln\\left(\\frac{0.88}{0.03}\\right) \\approx 3.3787$.\n- Field 2 (agreement): $m_2=0.95$, $u_2=0.01$. $w_2 = \\ln\\left(\\frac{0.95}{0.01}\\right) = \\ln(95) \\approx 4.5539$.\n- Field 3 (disagreement): $m_3=0.97$, $u_3=0.002$. $w_3 = \\ln\\left(\\frac{1-0.97}{1-0.002}\\right) = \\ln\\left(\\frac{0.03}{0.998}\\right) \\approx -3.5046$.\n- Field 4 (agreement): $m_4=0.99$, $u_4=0.50$. $w_4 = \\ln\\left(\\frac{0.99}{0.50}\\right) = \\ln(1.98) \\approx 0.6831$.\n- Field 5 (agreement): $m_5=0.90$, $u_5=0.08$. $w_5 = \\ln\\left(\\frac{0.90}{0.08}\\right) = \\ln(11.25) \\approx 2.4204$.\n\nThe total log-likelihood ratio weight $W$ is the sum of these individual weights:\n$$W = w_1 + w_2 + w_3 + w_4 + w_5$$\n$$W \\approx 3.3787 + 4.5539 - 3.5046 + 0.6831 + 2.4204 = 7.5315$$\nTo classify the pair, we compare this value to the calculated thresholds:\n$W_\\ell \\approx 1.6507$ and $W_u \\approx 7.5396$.\nSince $1.6507  7.5315  7.5396$, we have $W_\\ell  W  W_u$.\nTherefore, the pair falls into the clerical review category.\n\nThe problem asks for the final answer to be the numerical value of $W$, rounded to four significant figures.\n$W \\approx 7.5315$. Rounding to four significant figures gives $7.532$.",
            "answer": "$$\\boxed{7.532}$$"
        },
        {
            "introduction": "Developing a patient matching algorithm is only the first step; we must also rigorously evaluate its performance to ensure patient safety and data integrity. This practice introduces the standard metrics used for this purpose: precision, recall, and the $F_1$ score, all derived from the confusion matrix. Calculating these values will allow you to quantitatively analyze the crucial trade-off between incorrectly linking different patients (a high-risk false positive) and failing to link records for the same patient (a false negative), which is a central challenge in HIE operations .",
            "id": "4841823",
            "problem": "A regional Health Information Exchange (HIE) operates a probabilistic patient matching algorithm that classifies candidate Electronic Health Record (EHR) pairs as either \"match\" or \"non-match.\" On a validation dataset of 10,000 candidate record pairs with adjudicated ground truth, the algorithm’s outcomes relative to the ground truth are summarized by the following confusion matrix counts: true positives $TP = 850$, false positives $FP = 350$, false negatives $FN = 150$, and true negatives $TN = 8,650$. Starting only from the foundational definitions of the confusion matrix and the interpretation of precision and recall in binary classification, derive the expressions for precision, recall, and the $F_1$ score in terms of $TP$, $FP$, and $FN$, and compute these values for the algorithm described. Then, explain the trade-offs between precision and recall in patient matching within Health Information Exchange models, focusing on how adjusting a similarity threshold impacts $TP$, $FP$, and $FN$, and the potential downstream effects on data quality and patient safety in cross-organizational exchange. Report the final numerical answer as the computed $F_1$ score for this dataset, rounded to four significant figures. No units are required.",
            "solution": "The problem requires the derivation of key binary classification metrics from first principles, their calculation for a given dataset, and a qualitative explanation of their implications in the specific context of Health Information Exchange (HIE) patient matching.\n\nFirst, we define the terms based on the confusion matrix components provided:\n-   A \"positive\" instance refers to a candidate record pair that is a true match.\n-   A \"negative\" instance refers to a candidate record pair that is a true non-match.\n-   True Positives ($TP$): The number of true matches correctly classified as a \"match\" by the algorithm. Given as $TP = 850$.\n-   False Positives ($FP$): The number of true non-matches incorrectly classified as a \"match\". This is a Type I error. Given as $FP = 350$.\n-   False Negatives ($FN$): The number of true matches incorrectly classified as a \"non-match\". This is a Type II error. Given as $FN = 150$.\n-   True Negatives ($TN$): The number of true non-matches correctly classified as a \"non-match\". Given as $TN = 8650$.\nThe total number of pairs is $TP+FP+FN+TN = 850 + 350 + 150 + 8650 = 10000$, which matches the provided dataset size.\n\nNow, we derive the expressions for precision, recall, and the $F_1$ score.\n\nPrecision, also known as the Positive Predictive Value ($PPV$), measures the accuracy of the positive predictions. It answers the question: \"Of all the pairs that the algorithm classified as a match, what fraction were actually matches?\"\nThe total number of pairs classified as a match is the sum of true positives and false positives, $TP + FP$. The number of these that are correct is $TP$.\nTherefore, the expression for precision is:\n$$\n\\text{Precision} = \\frac{TP}{TP + FP}\n$$\n\nRecall, also known as sensitivity or the True Positive Rate ($TPR$), measures the completeness of the positive predictions. It answers the question: \"Of all the pairs that were actually matches, what fraction did the algorithm correctly identify?\"\nThe total number of actual matches in the dataset is the sum of the true positives (which were correctly identified) and the false negatives (which were missed), $TP + FN$. The number of these that the algorithm correctly identified is $TP$.\nTherefore, the expression for recall is:\n$$\n\\text{Recall} = \\frac{TP}{TP + FN}\n$$\n\nThe $F_1$ score is defined as the harmonic mean of precision and recall. It serves as a single metric that balances both concerns. The harmonic mean of two numbers, $A$ and $B$, is given by $\\frac{2AB}{A+B}$. Substituting precision and recall for $A$ and $B$:\n$$\nF_1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\nSubstituting the expressions in terms of $TP$, $FP$, and $FN$:\n$$\nF_1 = \\frac{2 \\cdot \\left(\\frac{TP}{TP + FP}\\right) \\cdot \\left(\\frac{TP}{TP + FN}\\right)}{\\left(\\frac{TP}{TP + FP}\\right) + \\left(\\frac{TP}{TP + FN}\\right)}\n$$\nTo simplify, we find a common denominator for the sum in the denominator of the main fraction:\n$$\nF_1 = \\frac{\\frac{2 \\cdot TP^2}{(TP + FP)(TP + FN)}}{\\frac{TP(TP + FN) + TP(TP + FP)}{(TP + FP)(TP + FN)}} = \\frac{2 \\cdot TP^2}{TP(TP + FN) + TP(TP + FP)}\n$$\nFactoring out $TP$ from the denominator:\n$$\nF_1 = \\frac{2 \\cdot TP^2}{TP((TP + FN) + (TP + FP))} = \\frac{2 \\cdot TP}{2TP + FP + FN}\n$$\nThis final expression is a convenient form for calculation.\n\nNext, we compute these values for the given algorithm.\nGiven: $TP = 850$, $FP = 350$, $FN = 150$.\n\nPrecision:\n$$\n\\text{Precision} = \\frac{850}{850 + 350} = \\frac{850}{1200} = \\frac{85}{120} = \\frac{17}{24} \\approx 0.7083\n$$\n\nRecall:\n$$\n\\text{Recall} = \\frac{850}{850 + 150} = \\frac{850}{1000} = 0.85\n$$\n\n$F_1$ Score:\nUsing the simplified formula derived above:\n$$\nF_1 = \\frac{2 \\cdot TP}{2TP + FP + FN} = \\frac{2 \\cdot 850}{2 \\cdot 850 + 350 + 150} = \\frac{1700}{1700 + 500} = \\frac{1700}{2200} = \\frac{17}{22}\n$$\nConverting this to a decimal and rounding to four significant figures:\n$$\nF_1 = \\frac{17}{22} \\approx 0.772727... \\approx 0.7727\n$$\n\nFinally, we explain the trade-offs between precision and recall in the context of HIE patient matching. Probabilistic patient matching algorithms typically calculate a similarity score between record pairs and classify them based on a predefined threshold. The choice of this threshold directly governs the trade-off.\n\n-   **Impact of Adjusting the Similarity Threshold**:\n    -   **Raising the threshold** makes the algorithm more stringent. It requires a higher degree of similarity to declare a \"match\". This leads to a decrease in both $TP$ (as some borderline true matches are missed) and $FP$ (as more non-matches are correctly rejected). Consequently, $FN$ increases and $TN$ increases. The result is typically an **increase in precision** (fewer false links) and a **decrease in recall** (more missed true links).\n    -   **Lowering the threshold** makes the algorithm more lenient. It declares a \"match\" with less evidence. This leads to an increase in both $TP$ (as more true matches are found) and $FP$ (as more non-matches are incorrectly linked). Consequently, $FN$ decreases and $TN$ decreases. The result is a **decrease in precision** (more false links) and an **increase in recall** (fewer missed true links).\n\n-   **Downstream Effects on Data Quality and Patient Safety**:\n    -   **High-Precision, Low-Recall Scenario (High Threshold)**: The primary benefit is high data integrity for linked records. There is a low probability of creating an \"overlay,\" which is the merging of records from two distinct patients. An overlay is a critical patient safety failure, as a clinician might make a life-threatening decision based on another patient's allergies, medications, or lab results. The downside is record fragmentation. With high $FN$, many of a single patient's records remain unlinked across the HIE. This undermines the purpose of the HIE, leading to an incomplete patient view, which can cause redundant testing, missed diagnoses, or failure to identify trends.\n    -   **Low-Precision, High-Recall Scenario (Low Threshold)**: The primary benefit is a more comprehensive, longitudinal patient record, as most true matches are correctly linked (low $FN$). This fulfills the main objective of the HIE. However, the high rate of false positives ($FP$) poses a severe risk to patient safety and data quality. The creation of numerous overlays pollutes the database, erodes physician trust in the system, and can directly lead to medical errors. The operational cost of manually identifying and resolving these incorrect links is also prohibitive.\n\nThe fundamental trade-off in HIE is between patient safety risks. A false positive (overlay) can lead to direct, immediate harm. A false negative (fragmented record) is an error of omission that can also lead to harm, but often less directly. For this reason, patient matching systems in production environments are almost always tuned to favor extremely high precision at the expense of recall. The challenge is to improve the underlying algorithm to increase recall without sacrificing the necessary high level of precision.",
            "answer": "$$\n\\boxed{0.7727}\n$$"
        },
        {
            "introduction": "The ultimate goal of successfully matching and linking patient records is to create a more comprehensive longitudinal health record. This exercise applies fundamental probability principles to model and quantify the completeness of a dataset, such as a patient's medication list, aggregated from multiple independent sources. Through this practice, you will learn to assess the value added by an HIE and understand the important concept of diminishing marginal returns when incorporating additional data feeds into the system .",
            "id": "4841791",
            "problem": "In a query-based Health Information Exchange (HIE) model, a patient's medication list is aggregated from multiple independent sources. For any single true medication on the patient's list, define the completeness of the aggregated list as the probability that the medication is present in the union of all sources. Assume the following fundamental base: (i) event independence across sources for whether a given medication is captured, and (ii) completeness is the fraction of true items captured, which for a single medication equals the probability it is captured by at least one source. Let the per-source capture probabilities be $p_{1}, p_{2}, \\ldots, p_{n}$, where source $i$ captures a given true medication independently with probability $p_{i}$ and misses it with probability $1 - p_{i}$.\n\nStarting from these definitions, derive a closed-form analytic expression for the expected completeness across $n$ sources. Then, for a specific Health Information Exchange (HIE) aggregation that draws medication data from three sources—Electronic Health Record (EHR), Pharmacy Claims (PC), and a regional HIE repository—with capture probabilities $p_{1} = 0.52$, $p_{2} = 0.43$, and $p_{3} = 0.31$, compute the expected completeness for the patient's medication list across these three sources. Next, derive from first principles the marginal expected gain in completeness when adding a fourth independent source with capture probability $p_{4}$, and evaluate this marginal expected gain when the fourth source is an electronic prescribing (eRx) network with $p_{4} = 0.27$.\n\nExpress both the expected completeness across the initial three sources and the marginal expected gain from adding the eRx source as decimal fractions, and round your answers to four significant figures. Do not use a percentage sign.",
            "solution": "### Derivation and Calculation\nThe problem asks for two quantities: the expected completeness for $n=3$ sources, and the marginal gain in completeness when adding a fourth source.\n\nFirst, we derive the general closed-form expression for the expected completeness, which we denote as $C_n$, for $n$ independent sources. The term \"expected completeness\" for a single medication is synonymous with the probability of its capture. A medication is successfully aggregated if it is captured by at least one source. This is the event of the union of individual capture events. Let $A_i$ be the event that source $i$ captures the medication, with probability $P(A_i) = p_i$. The completeness is the probability of the union of these events:\n$$C_n = P(A_1 \\cup A_2 \\cup \\dots \\cup A_n)$$\nCalculating the probability of a union directly is complex. It is more straightforward to use the complement rule. The complement of \"at least one source captures the medication\" is \"all sources miss the medication\". The event that source $i$ misses the medication is $A_i^c$, with probability $P(A_i^c) = 1 - p_i$.\nThe completeness is therefore:\n$$C_n = 1 - P(\\text{all sources miss the medication})$$\n$$C_n = 1 - P(A_1^c \\cap A_2^c \\cap \\dots \\cap A_n^c)$$\nThe problem states that the capture events are independent across sources. This independence extends to their complements. Therefore, the probability of the intersection of these complement events is the product of their individual probabilities:\n$$P(A_1^c \\cap A_2^c \\cap \\dots \\cap A_n^c) = P(A_1^c) P(A_2^c) \\dots P(A_n^c) = \\prod_{i=1}^{n} P(A_i^c)$$\nSubstituting $P(A_i^c) = 1 - p_i$, we obtain the probability that all sources miss the medication:\n$$\\prod_{i=1}^{n} (1 - p_i)$$\nThus, the closed-form analytic expression for the expected completeness across $n$ sources is:\n$$C_n = 1 - \\prod_{i=1}^{n} (1 - p_i)$$\nThis completes the first part of the derivation.\n\nNext, we compute the expected completeness for the three specified sources with capture probabilities $p_{1} = 0.52$, $p_{2} = 0.43$, and $p_{3} = 0.31$.\nUsing the derived formula for $n=3$:\n$$C_3 = 1 - (1 - p_1)(1 - p_2)(1 - p_3)$$\nSubstituting the given values:\n$$C_3 = 1 - (1 - 0.52)(1 - 0.43)(1 - 0.31)$$\n$$C_3 = 1 - (0.48)(0.57)(0.69)$$\nFirst, we compute the product of the miss probabilities:\n$$(0.48)(0.57) = 0.2736$$\n$$(0.2736)(0.69) = 0.188784$$\nNow, substitute this back into the expression for $C_3$:\n$$C_3 = 1 - 0.188784 = 0.811216$$\nRounding to four significant figures gives $0.8112$.\n\nSecond, we derive an expression for the marginal expected gain in completeness when adding a fourth source with capture probability $p_4$. The marginal gain, which we denote as $\\Delta C_4$, is the difference between the completeness with four sources, $C_4$, and the completeness with three sources, $C_3$.\n$$\\Delta C_4 = C_4 - C_3$$\nUsing our general formula:\n$$C_3 = 1 - \\prod_{i=1}^{3} (1 - p_i)$$\n$$C_4 = 1 - \\prod_{i=1}^{4} (1 - p_i) = 1 - (1 - p_4) \\prod_{i=1}^{3} (1 - p_i)$$\nNow we compute the difference:\n$$\\Delta C_4 = \\left(1 - (1 - p_4) \\prod_{i=1}^{3} (1 - p_i)\\right) - \\left(1 - \\prod_{i=1}^{3} (1 - p_i)\\right)$$\n$$\\Delta C_4 = 1 - \\prod_{i=1}^{3} (1 - p_i) + p_4 \\prod_{i=1}^{3} (1 - p_i) - 1 + \\prod_{i=1}^{3} (1 - p_i)$$\nThe terms cancel, leaving:\n$$\\Delta C_4 = p_4 \\prod_{i=1}^{3} (1 - p_i)$$\nThis result is intuitive: the gain from the fourth source occurs only if all of the first three sources miss the medication (an event with probability $\\prod_{i=1}^{3} (1-p_i)$) AND the fourth source captures it (an event with probability $p_4$).\n\nFinally, we evaluate this marginal gain for $p_{4} = 0.27$. We have already calculated the product term:\n$$\\prod_{i=1}^{3} (1 - p_i) = 0.188784$$\nSubstituting the values:\n$$\\Delta C_4 = (0.27) (0.188784)$$\n$$\\Delta C_4 = 0.05097168$$\nRounding to four significant figures gives $0.05097$.\nThe two requested values are the expected completeness for the three sources ($C_3$) and the marginal expected gain from the fourth source ($\\Delta C_4$).",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8112  0.05097 \\end{pmatrix}}\n$$"
        }
    ]
}