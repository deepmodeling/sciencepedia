{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the fundamental differences between structured and unstructured clinical data, there is no substitute for hands-on implementation. This first practice exercise guides you through building two distinct systems for identifying patients with a specific condition, hyperkalemia. You will construct a precise, rule-based query operating on structured laboratory values and a relevance-based information retrieval query using the standard Okapi BM25 algorithm on free-text clinical notes . By implementing both approaches, you will gain a concrete understanding of the deterministic logic of structured data retrieval versus the probabilistic and tunable nature of searching through unstructured text.",
            "id": "4857063",
            "problem": "You are given a small, synthetic Electronic Health Record (EHR) dataset containing both structured laboratory measurements and unstructured clinical notes for a set of patients. The task is to formalize two retrieval strategies to identify patients with hyperkalemia: one operating on structured laboratory data, and one operating on unstructured clinical notes using Information Retrieval (IR) scoring. Your program must implement both strategies from first principles and compute comparative metrics across a test suite of parameter settings.\n\nFundamental base and core definitions:\n1. Structured clinical data are typed fields in relational form. A laboratory record is a tuple $(p, c, v, u, t)$ where $p$ is a patient identifier, $c$ is a test code, $v$ is the numeric test value, $u$ is the unit, and $t$ is the time index. For serum potassium, use test code $c = \\text{\"K\"}$ and unit $u = \\text{\"mmol/L\"}$. Hyperkalemia is defined as serum potassium strictly greater than a threshold $T$ measured in $\\text{mmol/L}$.\n2. Unstructured clinical data are free-text notes. Treat each patient’s concatenated notes as one document $D$. Information Retrieval (IR) scoring is based on well-tested components: Term Frequency (TF), Inverse Document Frequency (IDF), and document length normalization. The Okapi BM25 ranking function combines these components with saturation parameter $k_1$ and length normalization parameter $b$. Your implementation must compute scores consistent with these components, without assuming any external resources.\n3. Inverse Document Frequency (IDF) is a function increasing in corpus size $N$ and decreasing in document frequency $n_t$ for term $t$. Use an IDF form that depends only on $N$ and $n_t$ and is monotone in these variables. Term Frequency (TF) is the count $f(t, D)$ of term $t$ in document $D$. Document length $|D|$ is the number of tokens in $D$. Average document length $\\overline{|D|}$ is the corpus average. BM25 uses $k_1$ to saturate TF and $b$ to normalize by document length. The score for a document is the sum over query terms of component contributions determined by $f(t,D)$, $N$, $n_t$, $|D|$, $\\overline{|D|}$, $k_1$, and $b$.\n\nDataset:\n- Patients: $p \\in \\{101, 102, 103, 104, 105\\}$.\n\nStructured laboratory records $(p, c, v, u, t)$:\n- $(101, \\text{\"K\"}, 4.0, \\text{\"mmol/L\"}, 2)$, $(101, \\text{\"K\"}, 5.6, \\text{\"mmol/L\"}, 5)$, $(101, \\text{\"K\"}, 5.4, \\text{\"mmol/L\"}, 40)$.\n- $(102, \\text{\"K\"}, 5.5, \\text{\"mmol/L\"}, 3)$, $(102, \\text{\"K\"}, 5.7, \\text{\"mmol/L\"}, 60)$.\n- $(103, \\text{\"K\"}, 6.0, \\text{\"mmol/L\"}, 10)$.\n- $(104, \\text{\"K\"}, 5.3, \\text{\"mmol/L\"}, 4)$, $(104, \\text{\"K\"}, 5.9, \\text{\"mmol/L\"}, 200)$.\n- $(105, \\text{\"K\"}, \\text{no record})$.\n\nUnstructured clinical notes (one document per patient, tokenization is case-insensitive with punctuation removed):\n- Patient $101$: $\\text{``patient has hyperkalemia and elevated potassium k 5.6 mmol l''}$.\n- Patient $102$: $\\text{``potassium borderline high k 5.5 monitor''}$.\n- Patient $103$: $\\text{``no evidence of hyperkalemia potassium normal''}$.\n- Patient $104$: $\\text{``electrolyte panel shows mildly elevated k consider causes''}$.\n- Patient $105$: $\\text{``history of hyperkalemia due to chronic kidney disease recurrent episodes''}$.\n\nYour program must:\n1. Implement a structured query that returns the sorted list of patient identifiers $p$ such that there exists a laboratory record with $c = \\text{\"K\"}$, $u = \\text{\"mmol/L\"}$, time $t$ in a closed window $[t_{\\min}, t_{\\max}]$, and value $v$ strictly greater than threshold $T$ measured in $\\text{mmol/L}$.\n2. Implement a BM25-style IR scoring method that:\n   - Tokenizes each document to compute $f(t, D)$, $|D|$, and $\\overline{|D|}$ across the corpus.\n   - Computes an IDF value that increases when $N$ increases and decreases when $n_t$ increases.\n   - Computes a per-document BM25 score for a query term list $Q$ as a sum of term contributions with parameters $k_1$ and $b$.\n   - Returns the sorted list of patient identifiers $p$ whose BM25 score is greater than or equal to a threshold $\\tau$ (dimensionless).\n3. For each test case, compute the overlap count $|S \\cap I|$ and the Jaccard index $J = \\dfrac{|S \\cap I|}{|S \\cup I|}$ as a float rounded to four decimal places, where $S$ is the structured set and $I$ is the IR set.\n\nUnits and expressions:\n- The hyperkalemia threshold $T$ must be treated in $\\text{mmol/L}$.\n- All angles are irrelevant to this problem.\n- Express the Jaccard index as a decimal number.\n\nTest suite:\nCompute results for the following parameter sets, each producing one result item containing four elements: the structured patient list, the IR patient list, the overlap count, and the Jaccard index.\n1. $T = 5.5$ (in $\\text{mmol/L}$), $(t_{\\min}, t_{\\max}) = (0, 30)$, $k_1 = 1.5$, $b = 0.75$, $\\tau = 1.0$, $Q = [\\text{``hyperkalemia''}, \\text{``potassium''}]$.\n2. $T = 5.5$ (in $\\text{mmol/L}$), $(t_{\\min}, t_{\\max}) = (0, 7)$, $k_1 = 1.2$, $b = 0.75$, $\\tau = 2.0$, $Q = [\\text{``hyperkalemia''}]$.\n3. $T = 5.5$ (in $\\text{mmol/L}$), $(t_{\\min}, t_{\\max}) = (0, 365)$, $k_1 = 0.9$, $b = 1.0$, $\\tau = 0.5$, $Q = [\\text{``elevated''}, \\text{``potassium''}]$.\n4. $T = 5.5$ (in $\\text{mmol/L}$), $(t_{\\min}, t_{\\max}) = (0, 30)$, $k_1 = 0.1$, $b = 0.0$, $\\tau = 0.1$, $Q = [\\text{``hyperkalemia''}]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order described above. For example, a valid output shape is $\\text{``[[\\dots],[\\dots],[\\dots],[\\dots]]''}$, and there must be no spaces in the printed output.",
            "solution": "The problem statement has been critically validated and is deemed to be valid. It is scientifically grounded in the principles of medical informatics and information retrieval, is well-posed with a complete and consistent setup, and is expressed in objective, formalizable language. The task requires the implementation of two distinct data retrieval strategies—one based on structured queries and another on unstructured text analysis using a standard algorithm—and their quantitative comparison. While the problem does not specify the exact formula for Inverse Document Frequency (IDF), the reference to \"Okapi BM25\" strongly implies the use of its canonical formulation. This is a reasonable and standard interpretation in the field. All data and parameters are provided to ensure a unique and verifiable solution exists.\n\nThe solution will be developed by first formalizing the data and the two retrieval methodologies, and then implementing these methodologies to compute the required comparative metrics for the given test suite.\n\n**1. Data Formalization**\n\nThe provided dataset consists of two components for a set of patients $P = \\{101, 102, 103, 104, 105\\}$.\n\n- **Structured Laboratory Data**: A set of tuples $(p, c, v, u, t)$, where $p \\in P$ is the patient identifier, $c$ is the test code, $v$ is the numeric value, $u$ is the unit, and $t$ is the time index. For this problem, we are concerned with serum potassium, identified by $c = \\text{\"K\"}$ and $u = \\text{\"mmol/L\"}$.\n\n- **Unstructured Clinical Notes**: A collection of free-text documents $\\{D_p\\}_{p \\in P}$, where each $D_p$ represents the concatenated notes for patient $p$. To process this text, a tokenization procedure is applied: the text is converted to lowercase, split into tokens based on whitespace, and leading/trailing punctuation is removed from each token.\n\n**2. Structured Retrieval Strategy**\n\nThis strategy identifies patients with laboratory evidence of hyperkalemia. A patient $p$ is included in the resulting set $S$ if there exists at least one laboratory record $(p', c, v, u, t)$ such that:\n1. The patient identifier matches: $p' = p$.\n2. The test code identifies serum potassium: $c = \\text{\"K\"}$.\n3. The value $v$ exceeds a given threshold $T$: $v > T$.\n4. The time of measurement $t$ falls within a specified closed interval $[t_{\\min}, t_{\\max}]$: $t_{\\min} \\le t \\le t_{\\max}$.\n\nThe final output $S$ is the sorted list of unique patient identifiers satisfying these conditions.\n\n**3. Unstructured Retrieval Strategy (Okapi BM25)**\n\nThis strategy identifies patients based on the relevance of their clinical notes to a query, as measured by the Okapi BM25 scoring function. The total corpus size is $N = |P| = 5$.\n\nFirst, the corpus of notes is preprocessed. For each document $D_p$, we compute its length $|D_p|$ (the number of tokens). The average document length over the corpus is $\\overline{|D|} = \\frac{1}{N} \\sum_{p \\in P} |D_p|$.\n\nFor a given query $Q$, which is a list of terms, the BM25 score for a document $D$ is calculated as the sum of contributions from each query term $t \\in Q$:\n$$\n\\text{Score}(D, Q) = \\sum_{t \\in Q} \\text{IDF}(t) \\cdot \\frac{f(t, D) \\cdot (k_1 + 1)}{f(t, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|D|}{\\overline{|D|}}\\right)}\n$$\nThe components of this formula are defined as follows:\n\n- $f(t, D)$: The Term Frequency (TF) of term $t$ in document $D$, i.e., the number of times $t$ appears in the tokenized document.\n\n- $\\text{IDF}(t)$: The Inverse Document Frequency of term $t$. As per the standard BM25 algorithm, this is calculated as:\n$$\n\\text{IDF}(t) = \\ln\\left(\\frac{N - n_t + 0.5}{n_t + 0.5} + 1\\right)\n$$\nwhere $n_t$ is the document frequency of term $t$, i.e., the number of documents in the corpus containing the term $t$. This formula correctly reflects the required property of increasing with $N$ and decreasing with $n_t$.\n\n- $k_1$: A saturation parameter, typically between $1.2$ and $2.0$. It controls how quickly the TF term's contribution saturates.\n\n- $b$: A length normalization parameter, typically around $0.75$. When $b=0$, document length is ignored. When $b=1$, the effect of document length is fully normalized.\n\nA patient $p$ is included in the resulting set $I$ if the BM25 score for their document $D_p$ is greater than or equal to a specified threshold $\\tau$: $\\text{Score}(D_p, Q) \\ge \\tau$. The final output $I$ is the sorted list of unique patient identifiers satisfying this condition.\n\n**4. Comparative Metrics**\n\nTo compare the two retrieval strategies, the following metrics are computed for each test case, where $S$ and $I$ are the sets of patient IDs returned by the structured and IR methods, respectively:\n\n- **Overlap Count**: The number of patients identified by both methods, given by the cardinality of the intersection of the two sets: $|S \\cap I|$.\n\n- **Jaccard Index**: A measure of similarity between the two sets, defined as the size of the intersection divided by the size of the union:\n$$\nJ(S, I) = \\frac{|S \\cap I|}{|S \\cup I|}\n$$\nThe value of $J$ is a float between $0$ and $1$, inclusive, and will be reported rounded to four decimal places. If both sets are empty, their union is also empty, and the Jaccard index is defined as $1.0$ (although some definitions set it to $0$; here we consider the case where this would be $0/0$, but the problem constraints make this case unlikely; if $S, I$ are empty $J=0/0$ is undefined, but if they are equal and empty $J$ can be seen as $1$, let's follow the standard CS path where empty set union is empty, so size is 0, so if intersection is also 0, this gives $0/0$, which most libraries like `sklearn` define as $0.0$). For non-empty sets, where $S \\cup I$ is also non-empty, division by zero is not a concern. If one set is empty and other not, $J=0$. If both are empty, $S=I$, so $J=1$. But standard is to return $0$ for $|S \\cup I|=0$. We will return $0.0$ if the union is empty.\n\n**5. Execution Logic**\n\nThe program will iterate through the provided test suite. For each set of parameters $(T, t_{\\min}, t_{\\max}, k_1, b, \\tau, Q)$, it will:\n1. Execute the structured query to generate the set $S$.\n2. Execute the IR query to generate the set $I$.\n3. Compute the overlap count and Jaccard index.\n4. Collate these four results ($S$, $I$, overlap, Jaccard index) into a list.\nFinally, all results from the test suite will be aggregated into a single list and formatted into the specified string output.",
            "answer": "```python\nimport numpy as np\nimport collections\nimport re\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, implementing structured and unstructured\n    retrieval strategies for identifying hyperkalemia and computing comparative metrics.\n    \"\"\"\n    #\n    # --- Data Definition ---\n    #\n    patient_ids = [101, 102, 103, 104, 105]\n    \n    # Structured laboratory records: (p, c, v, u, t)\n    lab_records = [\n        (101, \"K\", 4.0, \"mmol/L\", 2), (101, \"K\", 5.6, \"mmol/L\", 5), (101, \"K\", 5.4, \"mmol/L\", 40),\n        (102, \"K\", 5.5, \"mmol/L\", 3), (102, \"K\", 5.7, \"mmol/L\", 60),\n        (103, \"K\", 6.0, \"mmol/L\", 10),\n        (104, \"K\", 5.3, \"mmol/L\", 4), (104, \"K\", 5.9, \"mmol/L\", 200),\n    ]\n\n    # Unstructured clinical notes\n    notes = {\n        101: \"patient has hyperkalemia and elevated potassium k 5.6 mmol l\",\n        102: \"potassium borderline high k 5.5 monitor\",\n        103: \"no evidence of hyperkalemia potassium normal\",\n        104: \"electrolyte panel shows mildly elevated k consider causes\",\n        105: \"history of hyperkalemia due to chronic kidney disease recurrent episodes\",\n    }\n\n    #\n    # --- Test Suite ---\n    #\n    test_cases = [\n        {'T': 5.5, 't_min': 0, 't_max': 30, 'k1': 1.5, 'b': 0.75, 'tau': 1.0, 'Q': [\"hyperkalemia\", \"potassium\"]},\n        {'T': 5.5, 't_min': 0, 't_max': 7, 'k1': 1.2, 'b': 0.75, 'tau': 2.0, 'Q': [\"hyperkalemia\"]},\n        {'T': 5.5, 't_min': 0, 't_max': 365, 'k1': 0.9, 'b': 1.0, 'tau': 0.5, 'Q': [\"elevated\", \"potassium\"]},\n        {'T': 5.5, 't_min': 0, 't_max': 30, 'k1': 0.1, 'b': 0.0, 'tau': 0.1, 'Q': [\"hyperkalemia\"]},\n    ]\n\n    #\n    # --- Helper Functions ---\n    #\n\n    def tokenize(text):\n        \"\"\"Tokenizes text: lowercase, split by space, strip punctuation.\"\"\"\n        return [re.sub(r'^[^\\w\\s]+|[^\\w\\s]+$', '', token) for token in text.lower().split()]\n\n    def structured_query(records, T, t_min, t_max):\n        \"\"\"Performs a structured query on lab records.\"\"\"\n        found_pids = set()\n        for p, c, v, u, t in records:\n            if c == \"K\" and u == \"mmol/L\" and v > T and t_min <= t <= t_max:\n                found_pids.add(p)\n        return sorted(list(found_pids))\n\n    def ir_query(notes_data, pids, Q, k1, b, tau):\n        \"\"\"Performs an IR query using Okapi BM25.\"\"\"\n        \n        # 1. Preprocessing and Corpus Statistics\n        corpus = {p: tokenize(notes_data.get(p, '')) for p in pids}\n        doc_lengths = {p: len(tokens) for p, tokens in corpus.items()}\n        N = float(len(pids))\n        avg_doc_length = sum(doc_lengths.values()) / N if N > 0 else 0.0\n\n        tf = {p: collections.Counter(tokens) for p, tokens in corpus.items()}\n        \n        # 2. IDF calculation\n        doc_freq = collections.defaultdict(int)\n        for p in pids:\n            for term in set(corpus[p]):\n                if term in Q:\n                    doc_freq[term] += 1\n\n        idf = {}\n        for term in Q:\n            n_t = doc_freq[term]\n            idf_val = np.log(((N - n_t + 0.5) / (n_t + 0.5)) + 1)\n            idf[term] = idf_val\n\n        # 3. BM25 Score Calculation\n        scores = {}\n        for p in pids:\n            score = 0.0\n            doc_len = doc_lengths[p]\n            for term in Q:\n                if term in tf[p]:\n                    f_td = tf[p][term]\n                    \n                    numerator = f_td * (k1 + 1)\n                    denominator = f_td + k1 * (1 - b + b * (doc_len / avg_doc_length))\n                    \n                    tf_component = numerator / denominator\n                    score += idf[term] * tf_component\n            scores[p] = score\n        \n        # 4. Filtering and Sorting\n        result_pids = set()\n        for p, score in scores.items():\n            if score >= tau:\n                result_pids.add(p)\n                \n        return sorted(list(result_pids))\n\n    def calculate_metrics(S, I):\n        \"\"\"Calculates overlap count and Jaccard index.\"\"\"\n        set_S = set(S)\n        set_I = set(I)\n        \n        intersection = set_S.intersection(set_I)\n        union = set_S.union(set_I)\n        \n        overlap_count = len(intersection)\n        \n        if not union:\n            jaccard_index = 0.0\n        else:\n            jaccard_index = float(overlap_count) / len(union)\n        \n        return overlap_count, round(jaccard_index, 4)\n\n    #\n    # --- Main Execution Loop ---\n    #\n    all_results = []\n    for case in test_cases:\n        # Structured query\n        S = structured_query(lab_records, case['T'], case['t_min'], case['t_max'])\n        \n        # IR query\n        I = ir_query(notes, patient_ids, case['Q'], case['k1'], case['b'], case['tau'])\n\n        # Metrics\n        overlap, jaccard = calculate_metrics(S, I)\n        \n        # Format Jaccard to 4 decimal places string\n        jaccard_str = f\"{jaccard:.4f}\"\n        \n        # Instead of direct conversion of float, use the formatted string\n        # Convert the float to string in the final formatting\n        result_item_raw = [S, I, overlap, jaccard]\n        \n        # Manual string construction to match format exactly.\n        s_str = str(result_item_raw[0]).replace(\" \", \"\")\n        i_str = str(result_item_raw[1]).replace(\" \", \"\")\n        overlap_str = str(result_item_raw[2])\n        \n        all_results.append(f\"[{s_str},{i_str},{overlap_str},{jaccard_str}]\")\n\n    # Final print statement\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we have methods for extracting information, how do we measure how well they work? This practice moves from implementation to evaluation, providing a quantitative framework for comparing the performance of structured data against an automated system for unstructured text. You will use a confusion matrix—a cornerstone of classification performance analysis—to calculate fundamental metrics like precision, recall, and the F1-score for a Natural Language Processing (NLP) pipeline . By contrasting these with the accuracy of a structured data field, you will learn to critically assess the unique error profiles and trade-offs inherent in each data type.",
            "id": "4857088",
            "problem": "A clinical natural language processing (NLP) pipeline is evaluated for extracting the concept “documented chronic kidney disease” from unstructured clinical notes, using expert chart review as the reference standard. For the same concept, the Electronic Health Record (EHR) contains a structured problem-list field. The evaluation is run on $N = 500$ patient encounters. The confusion matrix counts for the unstructured concept extraction are $TP = 190$, $FP = 35$, $TN = 245$, and $FN = 30$. Separately, the confusion matrix counts for the structured field are $TP_{s} = 180$, $FP_{s} = 20$, $TN_{s} = 260$, and $FN_{s} = 40$. Using first principles definitions of classification metrics based on these counts, compute the precision, recall, specificity, and $F1$ for the unstructured extraction, and the accuracy for the structured field. Then, define the signed difference $\\Delta$ between the unstructured extraction $F1$ and the structured field accuracy as $\\Delta = F1_{\\text{unstructured}} - \\text{accuracy}_{\\text{structured}}$. Report $\\Delta$ as a decimal and round your final answer to four significant figures.",
            "solution": "The problem requires the calculation and comparison of performance metrics for two different methods of identifying a clinical concept (\"documented chronic kidney disease\") from patient encounter data. The first method is an unstructured data extraction pipeline using natural language processing (NLP), and the second is a structured data field from the Electronic Health Record (EHR). The evaluation is performed against a reference standard (expert chart review) for a sample of $N=500$ encounters.\n\nTo begin, we must first validate the problem statement.\n\nThe givens are:\nTotal encounters, $N = 500$.\n\nFor unstructured concept extraction:\nTrue Positives, $TP = 190$.\nFalse Positives, $FP = 35$.\nTrue Negatives, $TN = 245$.\nFalse Negatives, $FN = 30$.\n\nFor the structured problem-list field:\nTrue Positives, $TP_s = 180$.\nFalse Positives, $FP_s = 20$.\nTrue Negatives, $TN_s = 260$.\nFalse Negatives, $FN_s = 40$.\n\nThe problem asks for several calculations:\n1.  Precision, recall, specificity, and $F1$ score for the unstructured extraction.\n2.  Accuracy for the structured field.\n3.  The signed difference $\\Delta = F1_{\\text{unstructured}} - \\text{accuracy}_{\\text{structured}}$.\n\nBefore proceeding, we verify the internal consistency of the provided data.\nFor the unstructured data, the sum of the confusion matrix cells is $TP + FP + TN + FN = 190 + 35 + 245 + 30 = 500$, which matches the total number of encounters $N$.\nFor the structured data, the sum is $TP_s + FP_s + TN_s + FN_s = 180 + 20 + 260 + 40 = 500$, which also matches $N$.\nFurthermore, the number of \"true positive\" cases in the reference standard must be consistent between the two evaluations.\nFor the unstructured evaluation, the number of positives in the reference is $P = TP + FN = 190 + 30 = 220$.\nFor the structured evaluation, the number of positives is $P_s = TP_s + FN_s = 180 + 40 = 220$.\nThe number of \"true negative\" cases must also be consistent.\nFor the unstructured evaluation, the number of negatives is $N_{neg} = TN + FP = 245 + 35 = 280$.\nFor the structured evaluation, the number of negatives is $N_{s,neg} = TN_s + FP_s = 260 + 20 = 280$.\nSince $P = P_s$ and $N_{neg} = N_{s,neg}$, and the total encounters $N$ is consistent, the problem is well-defined, self-contained, and scientifically grounded in standard classification performance evaluation. The problem is deemed valid.\n\nWe now proceed with the calculations based on their first-principles definitions.\n\nFor the unstructured data extraction:\nThe total number of predicted positive cases is $TP + FP = 190 + 35 = 225$.\nThe total number of actual positive cases is $TP + FN = 190 + 30 = 220$.\nThe total number of actual negative cases is $TN + FP = 245 + 35 = 280$.\n\nPrecision, which measures the proportion of correctly predicted positives among all predicted positives, is defined as:\n$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\nSubstituting the values for the unstructured extraction:\n$$ \\text{Precision}_{\\text{unstructured}} = \\frac{190}{190 + 35} = \\frac{190}{225} $$\n\nRecall (or Sensitivity), which measures the proportion of correctly identified actual positives, is defined as:\n$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\nSubstituting the values:\n$$ \\text{Recall}_{\\text{unstructured}} = \\frac{190}{190 + 30} = \\frac{190}{220} $$\n\nSpecificity, which measures the proportion of correctly identified actual negatives, is defined as:\n$$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\nSubstituting the values:\n$$ \\text{Specificity}_{\\text{unstructured}} = \\frac{245}{245 + 35} = \\frac{245}{280} $$\n\nThe $F1$ score is the harmonic mean of precision and recall. It is defined as:\n$$ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nA more direct formula using the confusion matrix counts is:\n$$ F1 = \\frac{2TP}{2TP + FP + FN} $$\nSubstituting the values for the unstructured extraction:\n$$ F1_{\\text{unstructured}} = \\frac{2 \\times 190}{2 \\times 190 + 35 + 30} = \\frac{380}{380 + 65} = \\frac{380}{445} $$\n\nNext, we calculate the accuracy for the structured data field.\nAccuracy measures the proportion of all correct predictions (both true positives and true negatives) among the total number of cases. It is defined as:\n$$ \\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN} = \\frac{TP + TN}{N} $$\nUsing the counts for the structured field ($TP_s, TN_s$) and the total number of encounters $N$:\n$$ \\text{accuracy}_{\\text{structured}} = \\frac{TP_s + TN_s}{N} = \\frac{180 + 260}{500} = \\frac{440}{500} $$\n\nFinally, we are asked to compute the signed difference $\\Delta$:\n$$ \\Delta = F1_{\\text{unstructured}} - \\text{accuracy}_{\\text{structured}} $$\nSubstituting the fractional forms we derived:\n$$ \\Delta = \\frac{380}{445} - \\frac{440}{500} $$\nTo compute the final value, we convert these fractions to their decimal representations.\n$$ F1_{\\text{unstructured}} = \\frac{380}{445} \\approx 0.85393258... $$\n$$ \\text{accuracy}_{\\text{structured}} = \\frac{440}{500} = 0.88 $$\nNow, we compute the difference:\n$$ \\Delta \\approx 0.85393258 - 0.88 = -0.02606741... $$\nThe problem requires this result to be rounded to four significant figures. The first non-zero digit is $2$ in the hundredths place. The four significant figures are $2$, $6$, $0$, and $6$. The fifth significant figure is $7$. Since $7 \\geq 5$, we round up the fourth significant figure.\n$$ \\Delta \\approx -0.02607 $$\nThis is the final numerical answer.",
            "answer": "$$\\boxed{-0.02607}$$"
        },
        {
            "introduction": "Beyond building and evaluating data extraction systems, a medical informaticist must understand how the choice of data source impacts downstream research. This final exercise explores the profound consequences of data quality on scientific conclusions, focusing on a common challenge in epidemiology: measurement error. You will derive from first principles how using an imperfect measure of exposure from unstructured notes—a phenomenon known as nondifferential misclassification—can systematically bias the results of a study . This practice illuminates how such errors often lead to an attenuation, or underestimation, of the true risk, highlighting the critical link between data quality and the validity of scientific evidence.",
            "id": "4857070",
            "problem": "A hospital informatics team is estimating the causal effect of a binary exposure $E \\in \\{0,1\\}$ (e.g., chronic medication use) on a binary outcome $D \\in \\{0,1\\}$ (e.g., an adverse event) using an Electronic Health Record (EHR). The exposure can be ascertained either from structured fields (treated as a gold-standard measurement of the true exposure) or from clinician notes using Natural Language Processing (NLP). Assume the following are all true and scientifically well-calibrated for the population under study:\n\n- The true exposure prevalence is $P(E=1)=\\pi$, with $\\pi=0.30$.\n- The true risk among the unexposed is $P(D=1 \\mid E=0)=p_{0}$, with $p_{0}=0.10$.\n- The true risk ratio comparing exposed to unexposed is $\\mathrm{RR}_{\\text{true}}=r$, with $r=2.0$, so that $P(D=1 \\mid E=1)=p_{1}=r\\,p_{0}$.\n- The NLP note-extracted exposure $E^{\\ast}$ is a nondifferentially misclassified measurement of $E$ relative to the structured fields, with sensitivity $Se=P(E^{\\ast}=1 \\mid E=1)=0.80$ and specificity $Sp=P(E^{\\ast}=0 \\mid E=0)=0.90$. Nondifferential misclassification means that errors in $E^{\\ast}$ do not depend on $D$ given $E$.\n\nStarting only from the fundamental definitions of risk, risk ratio, the law of total probability, and Bayes’ theorem, derive a closed-form expression for the observed risk ratio when exposure is defined by the notes variable $E^{\\ast}$,\n$$\n\\mathrm{RR}_{\\text{notes}}=\\frac{P(D=1 \\mid E^{\\ast}=1)}{P(D=1 \\mid E^{\\ast}=0)},\n$$\nas a function of $Se$, $Sp$, $\\pi$, $p_{0}$, and $r$. Then substitute the given numerical values to compute $\\mathrm{RR}_{\\text{notes}}$ and the attenuation factor\n$$\nA=\\frac{\\mathrm{RR}_{\\text{notes}}}{\\mathrm{RR}_{\\text{true}}}.\n$$\nReport $A$ as your final answer. Round your final numeric answer to four significant figures. Express the answer as a pure number (unitless).",
            "solution": "The problem is valid as it is scientifically grounded in the principles of epidemiology and biostatistics, is well-posed with all necessary information provided, and is free of ambiguities or contradictions.\n\nThe objective is to derive an expression for the observed risk ratio, $\\mathrm{RR}_{\\text{notes}}$, based on a misclassified exposure measurement $E^{\\ast}$, and then to compute the attenuation factor $A$. The true risk ratio is given by $\\mathrm{RR}_{\\text{true}} = r = \\frac{P(D=1|E=1)}{P(D=1|E=0)} = \\frac{p_1}{p_0}$. The observed risk ratio is defined as:\n$$ \\mathrm{RR}_{\\text{notes}} = \\frac{P(D=1 \\mid E^{\\ast}=1)}{P(D=1 \\mid E^{\\ast}=0)} $$\nWe will derive expressions for the numerator and the denominator of $\\mathrm{RR}_{\\text{notes}}$ separately, starting from fundamental principles.\n\nFirst, let's derive the expression for the numerator, $P(D=1 \\mid E^{\\ast}=1)$.\nUsing the definition of conditional probability, we have:\n$$ P(D=1 \\mid E^{\\ast}=1) = \\frac{P(D=1, E^{\\ast}=1)}{P(E^{\\ast}=1)} $$\nWe find the denominator, $P(E^{\\ast}=1)$, by applying the law of total probability, conditioning on the true exposure status $E \\in \\{0,1\\}$:\n$$ P(E^{\\ast}=1) = P(E^{\\ast}=1 \\mid E=1)P(E=1) + P(E^{\\ast}=1 \\mid E=0)P(E=0) $$\nUsing the provided definitions: $P(E=1) = \\pi$, $P(E=0) = 1-\\pi$, $P(E^{\\ast}=1 \\mid E=1) = Se$ (sensitivity), and $P(E^{\\ast}=0 \\mid E=0) = Sp$ (specificity), which implies $P(E^{\\ast}=1 \\mid E=0) = 1 - Sp$.\nSubstituting these into the equation for $P(E^{\\ast}=1)$ gives:\n$$ P(E^{\\ast}=1) = Se \\cdot \\pi + (1-Sp)(1-\\pi) $$\nNext, we find the numerator, $P(D=1, E^{\\ast}=1)$, again using the law of total probability, conditioning on $E$:\n$$ P(D=1, E^{\\ast}=1) = P(D=1, E^{\\ast}=1 \\mid E=1)P(E=1) + P(D=1, E^{\\ast}=1 \\mid E=0)P(E=0) $$\nUsing the chain rule for probability, $P(A,B|C) = P(A|B,C)P(B|C)$, this becomes:\n$$ P(D=1, E^{\\ast}=1) = P(D=1 \\mid E^{\\ast}=1, E=1)P(E^{\\ast}=1 \\mid E=1)P(E=1) + P(D=1 \\mid E^{\\ast}=1, E=0)P(E^{\\ast}=1 \\mid E=0)P(E=0) $$\nThe problem states that $E^{\\ast}$ is a nondifferentially misclassified measurement of $E$ with respect to $D$. This means $E^{\\ast}$ is conditionally independent of $D$ given $E$, which implies $P(D=1 \\mid E, E^{\\ast}) = P(D=1 \\mid E)$. Applying this assumption:\n$$ P(D=1 \\mid E^{\\ast}=1, E=1) = P(D=1 \\mid E=1) = p_1 $$\n$$ P(D=1 \\mid E^{\\ast}=1, E=0) = P(D=1 \\mid E=0) = p_0 $$\nSubstituting these and the other definitions into the expression for $P(D=1, E^{\\ast}=1)$:\n$$ P(D=1, E^{\\ast}=1) = p_1 \\cdot Se \\cdot \\pi + p_0 \\cdot (1-Sp) \\cdot (1-\\pi) $$\nGiven $p_1 = r \\cdot p_0$, we can factor out $p_0$:\n$$ P(D=1, E^{\\ast}=1) = p_0 [r \\cdot Se \\cdot \\pi + (1-Sp)(1-\\pi)] $$\nCombining the parts for $P(D=1 \\mid E^{\\ast}=1)$:\n$$ P(D=1 \\mid E^{\\ast}=1) = \\frac{p_0 [r \\cdot Se \\cdot \\pi + (1-Sp)(1-\\pi)]}{Se \\cdot \\pi + (1-Sp)(1-\\pi)} $$\n\nSecond, we derive the expression for the denominator of $\\mathrm{RR}_{\\text{notes}}$, $P(D=1 \\mid E^{\\ast}=0)$.\nFollowing a similar procedure, starting with the definition of conditional probability:\n$$ P(D=1 \\mid E^{\\ast}=0) = \\frac{P(D=1, E^{\\ast}=0)}{P(E^{\\ast}=0)} $$\nThe denominator is $P(E^{\\ast}=0) = 1 - P(E^{\\ast}=1)$, or by the law of total probability:\n$$ P(E^{\\ast}=0) = P(E^{\\ast}=0 \\mid E=1)P(E=1) + P(E^{\\ast}=0 \\mid E=0)P(E=0) $$\nUsing $P(E^{\\ast}=0 \\mid E=1) = 1-Se$ and $P(E^{\\ast}=0 \\mid E=0) = Sp$:\n$$ P(E^{\\ast}=0) = (1-Se)\\pi + Sp(1-\\pi) $$\nThe numerator, $P(D=1, E^{\\ast}=0)$, is found by conditioning on $E$:\n$$ P(D=1, E^{\\ast}=0) = P(D=1 \\mid E^{\\ast}=0, E=1)P(E^{\\ast}=0 \\mid E=1)P(E=1) + P(D=1 \\mid E^{\\ast}=0, E=0)P(E^{\\ast}=0 \\mid E=0)P(E=0) $$\nApplying the nondifferential misclassification assumption, $P(D=1 \\mid E, E^{\\ast}) = P(D=1 \\mid E)$, we get:\n$$ P(D=1, E^{\\ast}=0) = p_1 \\cdot (1-Se) \\cdot \\pi + p_0 \\cdot Sp \\cdot (1-\\pi) $$\nFactoring out $p_0$ using $p_1 = r \\cdot p_0$:\n$$ P(D=1, E^{\\ast}=0) = p_0 [r(1-Se)\\pi + Sp(1-\\pi)] $$\nCombining the parts for $P(D=1 \\mid E^{\\ast}=0)$:\n$$ P(D=1 \\mid E^{\\ast}=0) = \\frac{p_0 [r(1-Se)\\pi + Sp(1-\\pi)]}{(1-Se)\\pi + Sp(1-\\pi)} $$\n\nNow, we assemble the final expression for $\\mathrm{RR}_{\\text{notes}}$:\n$$ \\mathrm{RR}_{\\text{notes}} = \\frac{P(D=1 \\mid E^{\\ast}=1)}{P(D=1 \\mid E^{\\ast}=0)} = \\frac{\\frac{p_0 [r \\cdot Se \\cdot \\pi + (1-Sp)(1-\\pi)]}{Se \\cdot \\pi + (1-Sp)(1-\\pi)}}{\\frac{p_0 [r(1-Se)\\pi + Sp(1-\\pi)]}{(1-Se)\\pi + Sp(1-\\pi)}} $$\nThe baseline risk $p_0$ cancels, yielding the closed-form expression as a function of $r, Se, Sp,$ and $\\pi$:\n$$ \\mathrm{RR}_{\\text{notes}} = \\frac{r \\cdot Se \\cdot \\pi + (1-Sp)(1-\\pi)}{r(1-Se)\\pi + Sp(1-\\pi)} \\cdot \\frac{(1-Se)\\pi + Sp(1-\\pi)}{Se \\cdot \\pi + (1-Sp)(1-\\pi)} $$\nThis is the required symbolic derivation.\n\nNext, we substitute the given numerical values:\n$r=2.0$, $\\pi=0.30$, $Se=0.80$, $Sp=0.90$.\nFrom these, we derive: $1-\\pi = 0.70$, $1-Se = 0.20$, $1-Sp = 0.10$.\n\nLet's compute the four main components of the $\\mathrm{RR}_{\\text{notes}}$ expression:\n- $P(E^{\\ast}=1) = Se \\cdot \\pi + (1-Sp)(1-\\pi) = (0.80)(0.30) + (0.10)(0.70) = 0.24 + 0.07 = 0.31$\n- $P(E^{\\ast}=0) = (1-Se)\\pi + Sp(1-\\pi) = (0.20)(0.30) + (0.90)(0.70) = 0.06 + 0.63 = 0.69$\n- Risk in the observed-exposed group numerator: $r \\cdot Se \\cdot \\pi + (1-Sp)(1-\\pi) = (2.0)(0.80)(0.30) + (0.10)(0.70) = 0.48 + 0.07 = 0.55$\n- Risk in the observed-unexposed group numerator: $r(1-Se)\\pi + Sp(1-\\pi) = (2.0)(0.20)(0.30) + (0.90)(0.70) = 0.12 + 0.63 = 0.75$\n\nPlugging these into the simplified $\\mathrm{RR}_{\\text{notes}}$ formula:\n$$ \\mathrm{RR}_{\\text{notes}} = \\frac{0.55}{0.75} \\cdot \\frac{0.69}{0.31} = \\frac{0.3795}{0.2325} \\approx 1.632258... $$\nThe true risk ratio is $\\mathrm{RR}_{\\text{true}} = r = 2.0$.\nThe attenuation factor $A$ is the ratio of the observed risk ratio to the true risk ratio:\n$$ A = \\frac{\\mathrm{RR}_{\\text{notes}}}{\\mathrm{RR}_{\\text{true}}} = \\frac{1.632258...}{2.0} = 0.816129... $$\nRounding the result to four significant figures, we get $A = 0.8161$.",
            "answer": "$$\n\\boxed{0.8161}\n$$"
        }
    ]
}