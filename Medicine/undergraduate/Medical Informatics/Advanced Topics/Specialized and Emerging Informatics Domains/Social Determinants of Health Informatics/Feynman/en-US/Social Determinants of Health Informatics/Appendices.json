{
    "hands_on_practices": [
        {
            "introduction": "One of the central goals in studying Social Determinants of Health (SDOH) is to move beyond correlation and estimate the *causal* impact of social factors on health outcomes. Observational data from sources like Electronic Health Records (EHR) are rife with confounding variables; for instance, are poor outcomes caused by neighborhood conditions, or by the fact that sicker individuals tend to live in those neighborhoods? This exercise  will guide you through the practical application of the backdoor adjustment formula, a foundational technique in causal inference, to compute an adjusted causal effect from stratified data and isolate the impact of an SDOH exposure.",
            "id": "4855871",
            "problem": "A health system integrates Electronic Health Record (EHR) data with Social Determinants of Health (SDOH) measures to study the effect of living in a high Neighborhood Deprivation Index (NDI) area on 30-day hospital readmission after heart failure discharge. Let the binary exposure be $A \\in \\{0,1\\}$, where $A=1$ indicates residence in a high NDI census tract and $A=0$ indicates residence in a low NDI census tract. Let the binary outcome be $Y \\in \\{0,1\\}$, where $Y=1$ denotes a 30-day readmission. Consider two measured covariates assembled by the health informatics pipeline: clinical severity $S \\in \\{0,1\\}$, with $S=1$ denoting high severity and $S=0$ denoting low severity, and income category $I \\in \\{0,1\\}$, with $I=1$ denoting low income and $I=0$ denoting medium or high income. Define $Z=(S,I)$ with support on the four strata $z_{1}=(1,1)$, $z_{2}=(1,0)$, $z_{3}=(0,1)$, and $z_{4}=(0,0)$.\n\nA causal directed acyclic graph (DAG) for this setting has arrows $Z \\to A$, $Z \\to Y$, and $A \\to Y$, and investigators have validated that the measured covariate set $Z$ blocks all backdoor paths between $A$ and $Y$ in the DAG. Assume the standard identifiability conditions from causal inference and medical informatics data integration hold: conditional exchangeability $Y^{a} \\perp A \\mid Z$ for $a \\in \\{0,1\\}$, consistency $Y=Y^{A}$, and positivity $\\Pr(A=a \\mid Z=z)>0$ for all strata $z$ with $\\Pr(Z=z)>0$.\n\nFrom the integrated data warehouse, the following quantities have been estimated for each stratum $z_{k}$:\n- For $z_{1}=(S=1,I=1)$: $\\Pr(Y=1 \\mid A=1,Z=z_{1})=0.30$, $\\Pr(Y=1 \\mid A=0,Z=z_{1})=0.24$, and $\\Pr(Z=z_{1})=0.20$.\n- For $z_{2}=(S=1,I=0)$: $\\Pr(Y=1 \\mid A=1,Z=z_{2})=0.25$, $\\Pr(Y=1 \\mid A=0,Z=z_{2})=0.22$, and $\\Pr(Z=z_{2})=0.25$.\n- For $z_{3}=(S=0,I=1)$: $\\Pr(Y=1 \\mid A=1,Z=z_{3})=0.15$, $\\Pr(Y=1 \\mid A=0,Z=z_{3})=0.12$, and $\\Pr(Z=z_{3})=0.30$.\n- For $z_{4}=(S=0,I=0)$: $\\Pr(Y=1 \\mid A=1,Z=z_{4})=0.10$, $\\Pr(Y=1 \\mid A=0,Z=z_{4})=0.08$, and $\\Pr(Z=z_{4})=0.25$.\n\nUsing the backdoor criterion and the foundational definitions of causal effects, compute the adjusted average causal risk difference of $A=1$ versus $A=0$ on $Y$, expressed as the difference in interventional risks. Provide the final answer as a unitless decimal rounded to four significant figures.",
            "solution": "The objective is to compute the average causal risk difference (ACRD) of exposure $A$ on outcome $Y$. The exposure $A$ represents residence in a high ($A=1$) versus low ($A=0$) Neighborhood Deprivation Index (NDI) area. The outcome $Y$ is 30-day hospital readmission ($Y=1$) versus no readmission ($Y=0$). The set of covariates is $Z=(S,I)$, where $S$ is clinical severity and $I$ is income category.\n\nThe target causal estimand is the average causal risk difference, defined as the difference in the expected potential outcomes (or interventional risks) under exposure versus no exposure:\n$$\n\\text{ACRD} = E[Y^1] - E[Y^0]\n$$\nwhere $Y^a$ is the potential outcome for an individual if their exposure level were set to $a \\in \\{0,1\\}$. Since $Y$ is a binary variable, the expectation is equivalent to the probability of the event, so we can write this as:\n$$\n\\text{ACRD} = \\Pr(Y^1=1) - \\Pr(Y^0=1)\n$$\nThe problem states that the necessary identifiability conditions hold. Specifically, the covariate set $Z$ is sufficient to control for confounding, as it blocks all backdoor paths between $A$ and $Y$. This is formally expressed by the conditional exchangeability assumption, $Y^a \\perp A \\mid Z$. Along with consistency ($Y=Y^A$) and positivity ($\\Pr(A=a \\mid Z=z) > 0$), this allows us to identify the interventional risks $\\Pr(Y^a=1)$ from the observational data using the backdoor adjustment formula, also known as standardization or the g-formula.\n\nThe formula for the interventional risk, $\\Pr(Y^a=1)$, is given by standardizing over the distribution of the confounding variables $Z$:\n$$\n\\Pr(Y^a=1) = \\sum_{z} \\Pr(Y=1 \\mid A=a, Z=z) \\Pr(Z=z)\n$$\nThe summation is over all possible strata of $Z$. In this problem, we have four strata: $z_{1}=(1,1)$, $z_{2}=(1,0)$, $z_{3}=(0,1)$, and $z_{4}=(0,0)$. The problem provides all the necessary quantities: the stratum-specific conditional probabilities of the outcome, $\\Pr(Y=1 \\mid A=a, Z=z_k)$, and the marginal probabilities of each stratum, $\\Pr(Z=z_k)$.\n\nFirst, we calculate the interventional risk under exposure, $\\Pr(Y^1=1)$, by setting $a=1$:\n$$\n\\Pr(Y^1=1) = \\sum_{k=1}^{4} \\Pr(Y=1 \\mid A=1, Z=z_k) \\Pr(Z=z_k)\n$$\nSubstituting the provided values for each stratum:\n- For $z_{1}$: $\\Pr(Y=1 \\mid A=1, Z=z_{1}) = 0.30$ and $\\Pr(Z=z_{1}) = 0.20$.\n- For $z_{2}$: $\\Pr(Y=1 \\mid A=1, Z=z_{2}) = 0.25$ and $\\Pr(Z=z_{2}) = 0.25$.\n- For $z_{3}$: $\\Pr(Y=1 \\mid A=1, Z=z_{3}) = 0.15$ and $\\Pr(Z=z_{3}) = 0.30$.\n- For $z_{4}$: $\\Pr(Y=1 \\mid A=1, Z=z_{4}) = 0.10$ and $\\Pr(Z=z_{4}) = 0.25$.\n\nThe calculation is:\n$$\n\\Pr(Y^1=1) = (0.30)(0.20) + (0.25)(0.25) + (0.15)(0.30) + (0.10)(0.25)\n$$\n$$\n\\Pr(Y^1=1) = 0.0600 + 0.0625 + 0.0450 + 0.0250 = 0.1925\n$$\nThis value represents the 30-day readmission risk in the population if everyone lived in a high NDI area, after adjusting for clinical severity and income.\n\nNext, we calculate the interventional risk under no exposure, $\\Pr(Y^0=1)$, by setting $a=0$:\n$$\n\\Pr(Y^0=1) = \\sum_{k=1}^{4} \\Pr(Y=1 \\mid A=0, Z=z_k) \\Pr(Z=z_k)\n$$\nSubstituting the provided values for each stratum:\n- For $z_{1}$: $\\Pr(Y=1 \\mid A=0, Z=z_{1}) = 0.24$ and $\\Pr(Z=z_{1}) = 0.20$.\n- For $z_{2}$: $\\Pr(Y=1 \\mid A=0, Z=z_{2}) = 0.22$ and $\\Pr(Z=z_{2}) = 0.25$.\n- For $z_{3}$: $\\Pr(Y=1 \\mid A=0, Z=z_{3}) = 0.12$ and $\\Pr(Z=z_{3}) = 0.30$.\n- For $z_{4}$: $\\Pr(Y=1 \\mid A=0, Z=z_{4}) = 0.08$ and $\\Pr(Z=z_{4}) = 0.25$.\n\nThe calculation is:\n$$\n\\Pr(Y^0=1) = (0.24)(0.20) + (0.22)(0.25) + (0.12)(0.30) + (0.08)(0.25)\n$$\n$$\n\\Pr(Y^0=1) = 0.0480 + 0.0550 + 0.0360 + 0.0200 = 0.1590\n$$\nThis value represents the 30-day readmission risk in the population if everyone lived in a low NDI area, after adjusting for clinical severity and income.\n\nFinally, we compute the average causal risk difference by subtracting the interventional risk under no exposure from the interventional risk under exposure:\n$$\n\\text{ACRD} = \\Pr(Y^1=1) - \\Pr(Y^0=1) = 0.1925 - 0.1590 = 0.0335\n$$\nThe problem requires the final answer to be rounded to four significant figures. The calculated value $0.0335$ has three significant figures. To express it with four significant figures, we add a trailing zero, which is significant in this context.\n$$\n\\text{ACRD} = 0.03350\n$$\nThis result indicates that living in a high NDI area causally increases the absolute risk of 30-day hospital readmission by $3.350$ percentage points, after accounting for confounding by clinical severity and income level.",
            "answer": "$$\\boxed{0.03350}$$"
        },
        {
            "introduction": "Much of the rich data on Social Determinants of Health is not found in structured database fields but is buried within clinicians' free-text notes. Natural Language Processing (NLP) models are powerful tools for unlocking this information at scale, but their output must be rigorously validated before use. This practice  challenges you to step into the role of a data scientist and evaluate the performance of an NLP extractor by calculating fundamental metrics from a confusion matrix, a core competency for anyone deploying machine learning in healthcare.",
            "id": "4855893",
            "problem": "A health system deploys a Natural Language Processing (NLP) pipeline to extract the presence of a Social Determinants of Health (SDOH) factor, specifically “housing instability,” from clinical notes. A reference standard corpus of $3{,}000$ de-identified notes is created by human annotators. In this corpus, the true condition “housing instability present” is recorded for $1{,}200$ notes, and “housing instability absent” for $1{,}800$ notes. When benchmarked against the reference standard, the NLP extractor’s outputs yield the following confusion matrix counts: true positives $TP = 936$, false positives $FP = 144$, false negatives $FN = 264$, and true negatives $TN = 1{,}656$.\n\nUsing only the fundamental definitions of event counts in a confusion matrix and their induced conditional rate concepts, compute the following performance metrics for the extractor: sensitivity, specificity, positive predictive value (PPV), and F1 score. Express each as a decimal in the closed interval $[0,1]$. Round each value to four significant figures. Provide your final answer as a single row matrix in the order $(\\text{sensitivity}, \\text{specificity}, \\text{PPV}, \\text{F1 score})$, with no units and no percentage signs.",
            "solution": "The condition \"housing instability present\" corresponds to the positive class, and \"housing instability absent\" corresponds to the negative class. The provided counts from the confusion matrix are:\n- True Positives ($TP$): The number of cases correctly identified as positive. $TP = 936$.\n- False Positives ($FP$): The number of cases incorrectly identified as positive. $FP = 144$.\n- False Negatives ($FN$): The number of cases incorrectly identified as negative. $FN = 264$.\n- True Negatives ($TN$): The number of cases correctly identified as negative. $TN = 1{,}656$.\n\nFrom these fundamental counts, we can verify the total number of actual positive and negative instances in the reference standard corpus.\n- Total actual positives ($P$): $P = TP + FN = 936 + 264 = 1{,}200$. This matches the given value.\n- Total actual negatives ($N$): $N = TN + FP = 1{,}656 + 144 = 1{,}800$. This matches the given value.\n- Total instances: $P + N = 1{,}200 + 1{,}800 = 3{,}000$. This also matches the total corpus size.\n\nThe performance metrics are computed as follows:\n\n1.  **Sensitivity (Recall or True Positive Rate)**: This metric measures the proportion of actual positives that are correctly identified by the extractor. It is defined as the ratio of true positives to the total number of actual positives.\n    $$ \\text{Sensitivity} = \\frac{TP}{TP + FN} $$\n    Substituting the given values:\n    $$ \\text{Sensitivity} = \\frac{936}{936 + 264} = \\frac{936}{1{,}200} = 0.78 $$\n    Rounding to four significant figures, we get $0.7800$.\n\n2.  **Specificity (True Negative Rate)**: This metric measures the proportion of actual negatives that are correctly identified. It is defined as the ratio of true negatives to the total number of actual negatives.\n    $$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\n    Substituting the given values:\n    $$ \\text{Specificity} = \\frac{1{,}656}{1{,}656 + 144} = \\frac{1{,}656}{1{,}800} = 0.92 $$\n    Rounding to four significant figures, we get $0.9200$.\n\n3.  **Positive Predictive Value (PPV or Precision)**: This metric measures the proportion of positive predictions that are in fact correct. It is defined as the ratio of true positives to the total number of predicted positives.\n    $$ \\text{PPV} = \\frac{TP}{TP + FP} $$\n    Substituting the given values:\n    $$ \\text{PPV} = \\frac{936}{936 + 144} = \\frac{936}{1{,}080} \\approx 0.86666... $$\n    Rounding to four significant figures, we get $0.8667$.\n\n4.  **F1 Score**: This metric is the harmonic mean of Precision (PPV) and Recall (Sensitivity). It provides a single score that balances both concerns. The formula is:\n    $$ \\text{F1 Score} = 2 \\times \\frac{\\text{PPV} \\times \\text{Sensitivity}}{\\text{PPV} + \\text{Sensitivity}} $$\n    Alternatively, it can be calculated directly from the confusion matrix counts:\n    $$ \\text{F1 Score} = \\frac{2TP}{2TP + FP + FN} $$\n    Using this direct formula:\n    $$ \\text{F1 Score} = \\frac{2 \\times 936}{(2 \\times 936) + 144 + 264} = \\frac{1{,}872}{1{,}872 + 408} = \\frac{1{,}872}{2{,}280} \\approx 0.82105... $$\n    Rounding to four significant figures, we get $0.8211$.\n\nThe calculated values, rounded to four significant figures, are: Sensitivity = $0.7800$, Specificity = $0.9200$, PPV = $0.8667$, and F1 Score = $0.8211$. These will be presented in the specified matrix format.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.7800 & 0.9200 & 0.8667 & 0.8211\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "As algorithmic tools are integrated into clinical workflows to guide decisions from resource allocation to risk stratification, it is ethically imperative to ensure they do not perpetuate or amplify existing health disparities linked to SDOH. A model that seems accurate overall can still be deeply inequitable, systematically disadvantaging certain populations. In this hands-on exercise , you will perform a fundamental fairness audit by calculating the disparate impact ratio and its confidence interval, providing a quantitative method to test whether a model's benefits and burdens are being distributed equitably across social groups.",
            "id": "4855851",
            "problem": "A hospital system deploys a risk-scoring model in its Electronic Health Record (EHR) to prioritize proactive outreach for hypertension care management. To evaluate algorithmic fairness related to a key social determinant of health (neighborhood socioeconomic status), patients are partitioned by a binary group indicator $G$, where $G=1$ denotes residency in low-income, high-deprivation areas (as measured by the Area Deprivation Index), and $G=0$ denotes residency in higher-income, low-deprivation areas. Let $\\hat{Y}=1$ denote that the model flags a patient for outreach. The disparate impact ratio is defined as $\\text{DI}=\\frac{P(\\hat{Y}=1\\mid G=1)}{P(\\hat{Y}=1\\mid G=0)}$. The $0.8$ rule is commonly used as a practical fairness check: values of $\\text{DI}$ below $0.8$ suggest potential disparate impact against $G=1$.\n\nFrom one month of EHR logs, the following counts were observed:\n- In the $G=1$ group: $n_1=400$ patients, with $x_1=88$ flagged ($\\hat{Y}=1$).\n- In the $G=0$ group: $n_0=500$ patients, with $x_0=150$ flagged ($\\hat{Y}=1$).\n\nUsing the frequency definitions of probability, construct point estimates of the group-wise positive flag rates and the disparate impact ratio. Then, under large-sample assumptions justified by the Central Limit Theorem for sample proportions and the delta method applied to the log-transformed ratio, derive a two-sided confidence interval at confidence level $0.95$ for $\\text{DI}$. Test the $0.8$ rule by assessing whether $0.8$ lies within the derived interval and interpret the result.\n\nProvide, as your final numeric answer, the lower bound of the $0.95$ confidence interval for $\\text{DI}$. Round your final numeric answer to three significant figures. Express the final answer as a unitless decimal.",
            "solution": "Let $p_1 = P(\\hat{Y}=1 \\mid G=1)$ be the probability that a patient from the low-income group ($G=1$) is flagged for outreach, and let $p_0 = P(\\hat{Y}=1 \\mid G=0)$ be the corresponding probability for the higher-income group ($G=0$). The disparate impact ratio is defined as $\\text{DI} = \\frac{p_1}{p_0}$.\n\nThe problem provides the following data:\nFor group $G=1$: sample size $n_1 = 400$, number flagged $x_1 = 88$.\nFor group $G=0$: sample size $n_0 = 500$, number flagged $x_0 = 150$.\n\nFirst, we compute the point estimates for the group-wise positive flag rates using the frequency definition of probability. The maximum likelihood estimate for a binomial proportion $p$ is the sample proportion $\\hat{p} = \\frac{x}{n}$.\nThe point estimate for $p_1$ is:\n$$ \\hat{p}_1 = \\frac{x_1}{n_1} = \\frac{88}{400} = 0.22 $$\nThe point estimate for $p_0$ is:\n$$ \\hat{p}_0 = \\frac{x_0}{n_0} = \\frac{150}{500} = 0.30 $$\nThe point estimate for the disparate impact ratio, $\\widehat{\\text{DI}}$, is the ratio of these estimated proportions:\n$$ \\widehat{\\text{DI}} = \\frac{\\hat{p}_1}{\\hat{p}_0} = \\frac{0.22}{0.30} = \\frac{11}{15} \\approx 0.7333 $$\n\nTo construct a confidence interval for $\\text{DI}$, we use the method specified: application of the delta method to the log-transformed ratio. This approach is preferred as the distribution of $\\ln(\\widehat{\\text{DI}})$ is typically more symmetric and closer to a normal distribution than that of $\\widehat{\\text{DI}}$.\nLet $\\theta = \\ln(\\text{DI}) = \\ln(p_1) - \\ln(p_0)$. The estimator for $\\theta$ is $\\hat{\\theta} = \\ln(\\widehat{\\text{DI}}) = \\ln(\\hat{p}_1) - \\ln(\\hat{p}_0)$.\n\nUnder the large-sample assumption, the sample proportions $\\hat{p}_1$ and $\\hat{p}_0$ are approximately normally distributed. Since the two groups represent independent samples, the variance of $\\hat{\\theta}$ is the sum of the variances of its terms:\n$$ \\text{Var}(\\hat{\\theta}) = \\text{Var}(\\ln(\\hat{p}_1)) + \\text{Var}(\\ln(\\hat{p}_0)) $$\nWe use the delta method to find the variance of a function of a random variable. For a function $f(X)$, $\\text{Var}(f(X)) \\approx [f'(E[X])]^2\\text{Var}(X)$. Here, $f(p) = \\ln(p)$, so $f'(p) = \\frac{1}{p}$. The variance of a sample proportion $\\hat{p}$ is $\\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n}$.\nApplying this, we get:\n$$ \\text{Var}(\\ln(\\hat{p})) \\approx \\left(\\frac{1}{p}\\right)^2 \\frac{p(1-p)}{n} = \\frac{1-p}{np} $$\nThus, the variance of $\\hat{\\theta}$ is approximately:\n$$ \\text{Var}(\\hat{\\theta}) \\approx \\frac{1-p_1}{n_1 p_1} + \\frac{1-p_0}{n_0 p_0} $$\nWe estimate this variance by substituting the sample proportions for the true proportions, which gives the squared standard error of $\\hat{\\theta}$:\n$$ \\widehat{\\text{SE}}(\\hat{\\theta})^2 = \\frac{1-\\hat{p}_1}{n_1 \\hat{p}_1} + \\frac{1-\\hat{p}_0}{n_0 \\hat{p}_0} = \\frac{1-\\hat{p}_1}{x_1} + \\frac{1-\\hat{p}_0}{x_0} $$\nSubstituting the numerical values:\n$$ \\widehat{\\text{SE}}(\\hat{\\theta})^2 = \\frac{1-0.22}{88} + \\frac{1-0.30}{150} = \\frac{0.78}{88} + \\frac{0.70}{150} $$\n$$ \\widehat{\\text{SE}}(\\hat{\\theta})^2 \\approx 0.00886363... + 0.00466666... = 0.0135303... $$\nThe standard error is the square root of this value:\n$$ \\widehat{\\text{SE}}(\\hat{\\theta}) = \\sqrt{0.0135303...} \\approx 0.11632 $$\nThe point estimate for $\\theta$ is:\n$$ \\hat{\\theta} = \\ln(\\widehat{\\text{DI}}) = \\ln\\left(\\frac{11}{15}\\right) \\approx -0.30945 $$\nA two-sided confidence interval for $\\theta$ at a confidence level of $1-\\alpha = 0.95$ is given by $\\hat{\\theta} \\pm z_{\\alpha/2} \\widehat{\\text{SE}}(\\hat{\\theta})$. For $\\alpha=0.05$, the critical value is $z_{0.025} = 1.96$.\nThe margin of error is $E = 1.96 \\times 0.11632 \\approx 0.22799$.\nThe $95\\%$ confidence interval for $\\theta = \\ln(\\text{DI})$ is:\n$$ [-0.30945 - 0.22799, -0.30945 + 0.22799] = [-0.53744, -0.08146] $$\nTo obtain the confidence interval for $\\text{DI}$, we exponentiate the endpoints of the interval for $\\theta$:\n$$ \\text{CI}_{\\text{DI}} = [\\exp(-0.53744), \\exp(-0.08146)] \\approx [0.58428, 0.92176] $$\nRounding to three significant figures, the $95\\%$ confidence interval for $\\text{DI}$ is $[0.584, 0.922]$.\n\nThe final steps are to test the $0.8$ rule and provide the lower bound.\nThe $0.8$ rule suggests potential disparate impact if $\\text{DI} < 0.8$. Our point estimate is $\\widehat{\\text{DI}} \\approx 0.733$, which is below the $0.8$ threshold. However, the threshold value of $0.8$ is contained within the $95\\%$ confidence interval $[0.584, 0.922]$. This means that we cannot conclude with $95\\%$ confidence that the true disparate impact ratio is less than $0.8$. The observed difference from the $0.8$ threshold is not statistically significant at the $\\alpha=0.05$ level. The data are consistent with a true $\\text{DI}$ that is either below or above $0.8$.\n\nThe problem asks for the lower bound of the $0.95$ confidence interval for $\\text{DI}$, rounded to three significant figures.\nThe calculated lower bound is approximately $0.58428$. Rounding to three significant figures gives $0.584$.",
            "answer": "$$\\boxed{0.584}$$"
        }
    ]
}