{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of precision medicine is the analysis of high-dimensional 'omics' data to discover novel biomarkers. However, testing thousands of genes simultaneously dramatically increases the likelihood of finding false positives due to random chance, a challenge known as the multiple testing problem. This exercise will demonstrate how to apply a fundamental statistical method, the Benjamini-Hochberg procedure, to control the False Discovery Rate (FDR) and identify statistically robust candidate genes from a large set of tests .",
            "id": "4852814",
            "problem": "A precision oncology study uses Ribonucleic Acid sequencing (RNA-seq) to profile gene expression in patient tumors to identify genes whose expression levels are associated with response to a targeted therapy. Testing each gene independently yields many hypothesis tests. In this high-dimensional setting, explain why performing many hypothesis tests increases the chance of making at least one false positive, and why controlling the False Discovery Rate (FDR) is often preferable to controlling the Family-Wise Error Rate (FWER) when prioritizing candidate biomarkers for follow-up. Then, using a standard step-up approach that controls the FDR at a target level $q$ under independence, determine the data-driven $p$-value cutoff (that is, the largest $p$-value among the rejected hypotheses) for declaring statistical significance.\n\nAssume the study tests $m = 18$ genes and obtains the following set of $p$-values (unsorted): $\\{0.062,\\ 0.021,\\ 0.0087,\\ 0.055,\\ 0.0012,\\ 0.026,\\ 0.0168,\\ 0.047,\\ 0.039,\\ 0.0125,\\ 0.0102,\\ 0.091,\\ 0.0025,\\ 0.0006,\\ 0.006,\\ 0.033,\\ 0.078,\\ 0.0041\\}$. Control the FDR at level $q = 0.05$ using the Benjamini–Hochberg (BH) procedure and report the resulting $p$-value cutoff as a pure number with no units. No rounding is required.",
            "solution": "The problem requires a two-part explanation followed by a calculation.\n\nFirst, an explanation of why performing many hypothesis tests increases the chance of making at least one false positive.\nLet $\\alpha$ be the significance level for a single hypothesis test, which represents the probability of a Type I error (a false positive) when the null hypothesis is true. A typical value is $\\alpha = 0.05$. The probability of correctly not rejecting a true null hypothesis is therefore $1 - \\alpha$. If we conduct $m$ independent hypothesis tests, the probability of making no Type I errors across all of them is $(1 - \\alpha)^m$. The probability of making at least one Type I error, known as the Family-Wise Error Rate (FWER), is the complement of this event:\n$$\n\\text{FWER} = 1 - (1 - \\alpha)^m\n$$\nFor the given problem, with $m = 18$ genes and a per-test significance level of $\\alpha = 0.05$, the FWER without any correction would be:\n$$\n\\text{FWER} = 1 - (1 - 0.05)^{18} = 1 - (0.95)^{18} \\approx 1 - 0.3972 = 0.6028\n$$\nThis means there is over a $60\\%$ chance of observing at least one false positive result, a drastic inflation from the desired $5\\%$ level. As $m$ grows large, as is common in genomics where thousands or tens of thousands of genes are tested, this probability rapidly approaches $1$, making false positives virtually certain.\n\nSecond, an explanation of why controlling the False Discovery Rate (FDR) is often preferable to controlling the FWER in biomarker discovery.\nFWER control aims to make the probability of even a single false positive across all tests very low (e.g., $\\le \\alpha$). This is a very stringent criterion. Methods that control the FWER, such as the Bonferroni correction (which uses a significance threshold of $\\alpha/m$), are highly conservative. While they effectively reduce false positives, they also severely reduce statistical power, meaning they are much more likely to miss true effects (i.e., they increase the rate of false negatives). In an exploratory setting like biomarker discovery, the primary goal is to generate a list of promising candidates for further, more rigorous (and often expensive) validation. The cost of a false negative (missing a potentially life-saving biomarker) can be very high.\n\nFDR control, in contrast, is less stringent. The FDR is defined as the expected proportion of rejected hypotheses that are actually false rejections (false positives).\n$$\n\\text{FDR} = E\\left[\\frac{V}{R} | R > 0\\right]P(R>0)\n$$\nwhere $V$ is the number of false positives (erroneously rejected true nulls) and $R$ is the total number of rejected nulls. Controlling the FDR at a level $q$ (e.g., $q = 0.05$) means that, on average, no more than $5\\%$ of the genes declared significant are expected to be false positives. This approach tolerates a small fraction of false positives within the list of discoveries in exchange for substantially greater power to detect true effects. This trade-off is often desirable in high-dimensional discovery science, as it generates a richer candidate list for follow-up studies, which are designed to then filter out the false positives.\n\nFinally, we apply the Benjamini–Hochberg (BH) procedure to find the data-driven p-value cutoff. The procedure controls the FDR at level $q = 0.05$ for $m = 18$ independent tests.\n\nStep 1: Sort the $m=18$ p-values in ascending order, denoted as $P_{(i)}$ for $i=1, \\dots, 18$.\nThe provided set is $\\{0.062, 0.021, 0.0087, 0.055, 0.0012, 0.026, 0.0168, 0.047, 0.039, 0.0125, 0.0102, 0.091, 0.0025, 0.0006, 0.006, 0.033, 0.078, 0.0041\\}$.\nThe sorted p-values $P_{(i)}$ are:\n$P_{(1)} = 0.0006$\n$P_{(2)} = 0.0012$\n$P_{(3)} = 0.0025$\n$P_{(4)} = 0.0041$\n$P_{(5)} = 0.006$\n$P_{(6)} = 0.0087$\n$P_{(7)} = 0.0102$\n$P_{(8)} = 0.0125$\n$P_{(9)} = 0.0168$\n$P_{(10)} = 0.021$\n$P_{(11)} = 0.026$\n$P_{(12)} = 0.033$\n$P_{(13)} = 0.039$\n$P_{(14)} = 0.047$\n$P_{(15)} = 0.055$\n$P_{(16)} = 0.062$\n$P_{(17)} = 0.078$\n$P_{(18)} = 0.091$\n\nStep 2: Find the largest index $k$ such that $P_{(k)} \\le \\frac{k}{m}q$.\nHere, $m=18$ and $q=0.05$. The condition is $P_{(k)} \\le \\frac{k}{18}(0.05)$. We check this condition for each $i$:\nFor $i=1$: $P_{(1)} = 0.0006 \\le \\frac{1}{18}(0.05) \\approx 0.00278$. The condition holds.\nFor $i=2$: $P_{(2)} = 0.0012 \\le \\frac{2}{18}(0.05) \\approx 0.00556$. The condition holds.\nFor $i=3$: $P_{(3)} = 0.0025 \\le \\frac{3}{18}(0.05) \\approx 0.00833$. The condition holds.\nFor $i=4$: $P_{(4)} = 0.0041 \\le \\frac{4}{18}(0.05) \\approx 0.01111$. The condition holds.\nFor $i=5$: $P_{(5)} = 0.006 \\le \\frac{5}{18}(0.05) \\approx 0.01389$. The condition holds.\nFor $i=6$: $P_{(6)} = 0.0087 \\le \\frac{6}{18}(0.05) \\approx 0.01667$. The condition holds.\nFor $i=7$: $P_{(7)} = 0.0102 \\le \\frac{7}{18}(0.05) \\approx 0.01944$. The condition holds.\nFor $i=8$: $P_{(8)} = 0.0125 \\le \\frac{8}{18}(0.05) \\approx 0.02222$. The condition holds.\nFor $i=9$: $P_{(9)} = 0.0168 \\le \\frac{9}{18}(0.05) = 0.025$. The condition holds.\nFor $i=10$: $P_{(10)} = 0.021 \\le \\frac{10}{18}(0.05) \\approx 0.02778$. The condition holds.\nFor $i=11$: $P_{(11)} = 0.026 \\le \\frac{11}{18}(0.05) \\approx 0.03056$. The condition holds.\nFor $i=12$: $P_{(12)} = 0.033 \\le \\frac{12}{18}(0.05) \\approx 0.03333$. The condition holds.\nFor $i=13$: $P_{(13)} = 0.039 > \\frac{13}{18}(0.05) \\approx 0.03611$. The condition fails.\n\nThe largest index $k$ for which the condition holds is $k = 12$.\n\nStep 3: Determine the cutoff.\nThe BH procedure rejects all null hypotheses $H_{(i)}$ for $i = 1, \\dots, k$. In this case, we reject the null hypotheses for the $12$ smallest p-values. The data-driven p-value cutoff for declaring significance is the largest p-value among those that are rejected, which is $P_{(k)}$.\nTherefore, the cutoff is $P_{(12)} = 0.033$. Any original p-value less than or equal to this value is declared statistically significant.",
            "answer": "$$\\boxed{0.033}$$"
        },
        {
            "introduction": "Once biomarkers are identified, they are often used to build clinical prediction models that estimate an individual's risk for a specific outcome. Simply building a model is not enough; we must rigorously evaluate its performance to ensure it is both accurate and reliable for clinical use. This practice problem provides hands-on experience in calculating three of the most important metrics for model evaluation: the Area Under the Curve (AUC) for discrimination, the calibration intercept and slope for assessing probabilistic accuracy, and the Brier score for overall performance .",
            "id": "4852815",
            "problem": "A clinical decision support system produces individualized risk estimates for a binary outcome (for example, the occurrence of an adverse drug reaction within $30$ days) to inform precision medicine and personalized healthcare. For $n=6$ patients, the model outputs predicted probabilities $\\hat{p}_i$ and the observed outcomes $y_i \\in \\{0,1\\}$ as follows: patient $1$: $\\hat{p}_1=0.12$, $y_1=0$; patient $2$: $\\hat{p}_2=0.18$, $y_2=0$; patient $3$: $\\hat{p}_3=0.33$, $y_3=1$; patient $4$: $\\hat{p}_4=0.52$, $y_4=0$; patient $5$: $\\hat{p}_5=0.61$, $y_5=1$; patient $6$: $\\hat{p}_6=0.85$, $y_6=1$.\n\nUsing foundational definitions appropriate to medical informatics evaluation of probabilistic risk models:\n\n- Discrimination is quantified by the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), defined empirically as the probability that a randomly chosen case (with $y=1$) has a higher predicted risk than a randomly chosen control (with $y=0$), with ties contributing one-half.\n- Calibration is summarized by an intercept and slope obtained from the least-squares linear calibration model under squared-error loss, where the best linear projection of $y$ onto $1$ and $\\hat{p}$ minimizes $\\sum_{i=1}^{n}(y_i - \\alpha - \\beta \\hat{p}_i)^{2}$. The calibration intercept $\\alpha$ and calibration slope $\\beta$ are the coefficients of this projection.\n- Overall performance under squared-error loss is measured by the Brier score, defined as $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{p}_i - y_i)^{2}$.\n\nStarting from these definitions and first principles (probability of concordance for AUC, normal equations for least squares calibration, and mean squared error for the Brier score), compute the AUC, the calibration intercept $\\alpha$, the calibration slope $\\beta$, and the Brier score for the given data. Round each metric to four significant figures. Report your final answer as a single row matrix containing, in order, the AUC, the calibration intercept, the calibration slope, and the Brier score. No units are required.",
            "solution": "The problem provides data for $n=6$ patients, consisting of predicted probabilities $\\hat{p}_i$ from a risk model and observed binary outcomes $y_i$. The task is to compute three performance metrics: the Area Under the ROC Curve (AUC) for discrimination, the calibration intercept $\\alpha$ and slope $\\beta$ for calibration, and the Brier score for overall performance. The calculations must be based on the provided definitions and first principles.\n\nThe given data are:\n- Patient $1$: $\\hat{p}_1=0.12$, $y_1=0$\n- Patient $2$: $\\hat{p}_2=0.18$, $y_2=0$\n- Patient $3$: $\\hat{p}_3=0.33$, $y_3=1$\n- Patient $4$: $\\hat{p}_4=0.52$, $y_4=0$\n- Patient $5$: $\\hat{p}_5=0.61$, $y_5=1$\n- Patient $6$: $\\hat{p}_6=0.85$, $y_6=1$\n\nFirst, we identify the cases (patients with the event, $y_i=1$) and controls (patients without the event, $y_i=0$).\n- Cases: Patients $3, 5, 6$. Their predicted risks are $\\{\\hat{p}_3, \\hat{p}_5, \\hat{p}_6\\} = \\{0.33, 0.61, 0.85\\}$. The number of cases is $n_1=3$.\n- Controls: Patients $1, 2, 4$. Their predicted risks are $\\{\\hat{p}_1, \\hat{p}_2, \\hat{p}_4\\} = \\{0.12, 0.18, 0.52\\}$. The number of controls is $n_0=3$.\n\n**1. Area Under the ROC Curve (AUC)**\n\nThe AUC is defined as the probability that a randomly chosen case has a higher predicted risk than a randomly chosen control, with ties contributing $0.5$. This is equivalent to calculating the Wilcoxon-Mann-Whitney U statistic. We form all possible pairs of (case, control) and compare their predicted risks. The total number of pairs is $n_1 \\times n_0 = 3 \\times 3 = 9$.\n\nWe tabulate the comparisons:\n1.  Pair $(\\hat{p}_3, \\hat{p}_1) = (0.33, 0.12)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n2.  Pair $(\\hat{p}_3, \\hat{p}_2) = (0.33, 0.18)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n3.  Pair $(\\hat{p}_3, \\hat{p}_4) = (0.33, 0.52)$: $\\hat{p}_{\\text{case}}  \\hat{p}_{\\text{control}}$. Score = $0$.\n4.  Pair $(\\hat{p}_5, \\hat{p}_1) = (0.61, 0.12)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n5.  Pair $(\\hat{p}_5, \\hat{p}_2) = (0.61, 0.18)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n6.  Pair $(\\hat{p}_5, \\hat{p}_4) = (0.61, 0.52)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n7.  Pair $(\\hat{p}_6, \\hat{p}_1) = (0.85, 0.12)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n8.  Pair $(\\hat{p}_6, \\hat{p}_2) = (0.85, 0.18)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n9.  Pair $(\\hat{p}_6, \\hat{p}_4) = (0.85, 0.52)$: $\\hat{p}_{\\text{case}} > \\hat{p}_{\\text{control}}$. Score = $1$.\n\nThere are no ties in predicted risk between cases and controls. The total score is the sum of scores for all pairs:\n$$ \\text{Total Score} = 1+1+0+1+1+1+1+1+1 = 8 $$\nThe AUC is the total score divided by the total number of pairs:\n$$ \\text{AUC} = \\frac{8}{9} \\approx 0.88888... $$\nRounding to four significant figures, we get $\\text{AUC} = 0.8889$.\n\n**2. Calibration Intercept ($\\alpha$) and Slope ($\\beta$)**\n\nThe calibration intercept $\\alpha$ and slope $\\beta$ are determined by a simple linear regression of the observed outcomes $y_i$ on the predicted probabilities $\\hat{p}_i$. We aim to find $\\alpha$ and $\\beta$ that minimize the sum of squared errors $S = \\sum_{i=1}^{n}(y_i - \\alpha - \\beta \\hat{p}_i)^{2}$. The solution is given by the normal equations, leading to the standard formulas for least-squares estimates:\n$$ \\beta = \\frac{\\sum_{i=1}^{n}(\\hat{p}_i - \\bar{p})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(\\hat{p}_i - \\bar{p})^2} = \\frac{n\\sum_{i=1}^{n}\\hat{p}_iy_i - (\\sum_{i=1}^{n}\\hat{p}_i)(\\sum_{i=1}^{n}y_i)}{n\\sum_{i=1}^{n}\\hat{p}_i^2 - (\\sum_{i=1}^{n}\\hat{p}_i)^2} $$\n$$ \\alpha = \\bar{y} - \\beta\\bar{p} $$\nwhere $\\bar{p}$ is the mean of the predicted probabilities and $\\bar{y}$ is the mean of the outcomes.\n\nWe compute the necessary sums with $n=6$:\n- $\\sum y_i = 0+0+1+0+1+1 = 3$\n- $\\sum \\hat{p}_i = 0.12+0.18+0.33+0.52+0.61+0.85 = 2.61$\n- $\\sum \\hat{p}_iy_i = (0.12)(0)+(0.18)(0)+(0.33)(1)+(0.52)(0)+(0.61)(1)+(0.85)(1) = 0.33+0.61+0.85 = 1.79$\n- $\\sum \\hat{p}_i^2 = 0.12^2+0.18^2+0.33^2+0.52^2+0.61^2+0.85^2 = 0.0144+0.0324+0.1089+0.2704+0.3721+0.7225 = 1.5207$\n\nNow we can compute $\\beta$:\n$$ \\beta = \\frac{6(1.79) - (2.61)(3)}{6(1.5207) - (2.61)^2} = \\frac{10.74 - 7.83}{9.1242 - 6.8121} = \\frac{2.91}{2.3121} \\approx 1.258596... $$\nRounding to four significant figures gives $\\beta = 1.259$.\n\nNext, we compute the means $\\bar{y}$ and $\\bar{p}$:\n- $\\bar{y} = \\frac{\\sum y_i}{n} = \\frac{3}{6} = 0.5$\n- $\\bar{p} = \\frac{\\sum \\hat{p}_i}{n} = \\frac{2.61}{6} = 0.435$\n\nThen, we calculate $\\alpha$:\n$$ \\alpha = \\bar{y} - \\beta\\bar{p} \\approx 0.5 - (1.258596...)(0.435) = 0.5 - 0.547489... = -0.047489... $$\nRounding to four significant figures gives $\\alpha = -0.04749$.\n\n**3. Brier Score**\n\nThe Brier score is the mean squared error between the predicted probabilities and the observed outcomes. It is defined as:\n$$ \\text{Brier Score} = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{p}_i - y_i)^2 $$\nWe calculate the squared error for each patient:\n- $(\\hat{p}_1-y_1)^2 = (0.12-0)^2 = 0.0144$\n- $(\\hat{p}_2-y_2)^2 = (0.18-0)^2 = 0.0324$\n- $(\\hat{p}_3-y_3)^2 = (0.33-1)^2 = (-0.67)^2 = 0.4489$\n- $(\\hat{p}_4-y_4)^2 = (0.52-0)^2 = 0.2704$\n- $(\\hat{p}_5-y_5)^2 = (0.61-1)^2 = (-0.39)^2 = 0.1521$\n- $(\\hat{p}_6-y_6)^2 = (0.85-1)^2 = (-0.15)^2 = 0.0225$\n\nThe sum of these squared errors is:\n$$ \\sum_{i=1}^{6}(\\hat{p}_i - y_i)^2 = 0.0144+0.0324+0.4489+0.2704+0.1521+0.0225 = 0.9407 $$\nThe Brier score is the mean of this sum:\n$$ \\text{Brier Score} = \\frac{0.9407}{6} \\approx 0.1567833... $$\nRounding to four significant figures, we get $\\text{Brier Score} = 0.1568$.\n\nIn summary, the computed metrics rounded to four significant figures are:\n- $\\text{AUC} = 0.8889$\n- Calibration Intercept $\\alpha = -0.04749$\n- Calibration Slope $\\beta = 1.259$\n- Brier Score $= 0.1568$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.8889  -0.04749  1.259  0.1568\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Even a highly accurate prediction model may not be implemented in practice if the costs outweigh the benefits. The final step in translating a discovery into a clinical tool often involves a health economic evaluation to determine its real-world value and feasibility. This exercise walks through a practical cost-effectiveness scenario where you will calculate the Number Needed to Genotype (NNG) to prevent an adverse event and determine the break-even cost for a pharmacogenomic test, providing insight into the critical decision-making process for implementing precision medicine strategies .",
            "id": "4852807",
            "problem": "In a precision medicine program for drug safety, a pharmacogenomic (PGx) test is used to identify patients with a genetic variant that increases the risk of a serious adverse event when treated with the standard therapy. The clinical and economic context is as follows. The genotype prevalence among indicated patients is $0.12$. Under standard therapy without testing, the adverse event probability is $0.10$ for carriers and $0.03$ for non-carriers. If tested, identified carriers are offered an alternative therapy that reduces their adverse event probability multiplicatively by a risk ratio of $0.50$ relative to the carrier risk under standard therapy. Non-carriers receive the standard therapy. Assume the PGx test has perfect analytic validity and clinical specificity and sensitivity, and that a fraction $0.80$ of identified carriers actually receive the alternative therapy in routine care. There is no change in adverse event probability for non-carriers, and there are no additional harms from the alternative therapy aside from cost. The incremental drug cost of the alternative therapy relative to the standard therapy, incurred only for carriers who switch therapies, is $\\$150$ per patient. The downstream cost attributable to experiencing the serious adverse event (inclusive of acute care and follow-up) is $\\$25{,}000$ per event. \n\nUsing only the fundamental definitions of probability, risk ratio, and absolute risk change, do the following in order:\n\n1) Derive, symbolically, the absolute risk reduction per genotyped patient attributable to implementing genotype-guided therapy under the assumptions above, and compute the number needed to genotype to avert one adverse event in this setting.\n\n2) Define a simple feasibility criterion for population-level implementation: the expected incremental cost per genotyped patient should not be positive. Under this criterion, derive an expression for the break-even per-person test cost that makes the expected incremental cost exactly zero, and compute its value given the data above.\n\nExpress the final answer as the break-even test cost in United States dollars. Round your final answer to three significant figures. State no units inside the final boxed answer.",
            "solution": "This problem requires a cost-effectiveness analysis of a pharmacogenomic testing program. We will first derive the absolute risk reduction per patient genotyped and then use that to find the break-even cost for the test.\n\nLet's define the given parameters symbolically:\n-   Prevalence of the genetic variant (carrier status): $p_G = P(G+) = 0.12$. The prevalence of non-carriers is $P(G-) = 1 - p_G = 0.88$.\n-   Risk of adverse event (AE) for a carrier under standard therapy: $R_{S|G+} = P(AE|G+, S) = 0.10$.\n-   Risk of AE for a non-carrier under standard therapy: $R_{S|G-} = P(AE|G-, S) = 0.03$.\n-   Risk ratio for the alternative therapy in carriers: $RR = 0.50$.\n-   Fraction of carriers who switch to alternative therapy: $f_s = 0.80$.\n-   Incremental cost of the alternative drug: $C_{alt} = \\$150$.\n-   Cost of an adverse event: $C_{AE} = \\$25,000$.\n-   The test is assumed to have perfect diagnostic accuracy.\n\n**Part 1: Absolute Risk Reduction and Number Needed to Genotype (NNG)**\n\nThe absolute risk reduction (ARR) per genotyped patient is the difference in the expected probability of an adverse event between the standard care scenario (no testing) and the genotype-guided scenario. The change in care only affects carriers who switch therapy. Therefore, the reduction in risk only occurs in this subgroup. The symbolic expression for the absolute risk reduction per genotyped patient is:\n$$ARR = (\\text{prevalence of carriers}) \\times (\\text{fraction who switch}) \\times (\\text{baseline risk for carriers}) \\times (\\text{relative risk reduction})$$\n$$ARR = p_G \\cdot f_s \\cdot R_{S|G+} \\cdot (1 - RR)$$\nThis expression is intuitive: The overall risk reduction is realized only in the sub-population of carriers ($p_G$) who actually switch therapy ($f_s$), and is proportional to their baseline risk ($R_{S|G+}$) and the fractional risk reduction from the new therapy ($(1-RR)$).\n\nUsing the given values:\n$$ARR = (0.12) \\cdot (0.80) \\cdot (0.10) \\cdot (1 - 0.50)$$\n$$ARR = (0.12) \\cdot (0.80) \\cdot (0.10) \\cdot (0.50) = 0.0048$$\n\nThe number needed to genotype (NNG) to avert one adverse event is the reciprocal of the ARR:\n$$NNG = \\frac{1}{ARR} = \\frac{1}{0.0048} \\approx 208.33$$\nThus, approximately 209 patients must be genotyped to prevent one adverse event.\n\n**Part 2: Break-Even Test Cost**\n\nThe feasibility criterion states that the expected incremental cost per genotyped patient, $\\Delta E[C]$, should be zero at the break-even point. This incremental cost includes the cost of the test, the extra cost for the alternative drug, and the savings from averted adverse events. Let $C_{test}$ be the per-person cost of the genotype test. The equation for the incremental cost is:\n$$ \\Delta E[C] = C_{test} + (\\text{incremental drug costs}) - (\\text{savings from averted AEs}) = 0 $$\n$$ C_{test} + (p_G \\cdot f_s \\cdot C_{alt}) - (ARR \\cdot C_{AE}) = 0 $$\nSolving for the break-even test cost, $C_{test}$:\n$$C_{test} = ARR \\cdot C_{AE} - p_G \\cdot f_s \\cdot C_{alt}$$\nThis shows that the affordable test cost is equal to the healthcare savings from averted adverse events minus the additional expenditure on the alternative therapy.\n\nSubstituting the numerical values:\n-   Savings from averted AEs: $ARR \\cdot C_{AE} = 0.0048 \\cdot \\$25,000 = \\$120$\n-   Additional drug expenditure: $p_G \\cdot f_s \\cdot C_{alt} = 0.12 \\cdot 0.80 \\cdot \\$150 = 0.096 \\cdot \\$150 = \\$14.40$\n\nThe break-even test cost is:\n$$C_{test} = \\$120 - \\$14.40 = \\$105.60$$\nThe problem asks to round the final answer to three significant figures. Rounding $105.60$ to three significant figures gives $106$.",
            "answer": "$$\\boxed{106}$$"
        }
    ]
}