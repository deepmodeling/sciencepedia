{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in patient safety is that many adverse events go unreported, creating a gap in our understanding of the true frequency of harm. This exercise introduces the capture-recapture method, a powerful statistical technique, to estimate the total number of events by comparing two independent reporting sources. By working through this problem, you will learn how to look beyond the observed data to quantify the hidden scale of underreporting .",
            "id": "4852033",
            "problem": "A tertiary hospital uses two independent adverse event reporting sources for patient safety monitoring: a rule-based trigger tool within the Electronic Health Record (EHR) and a voluntary incident reporting system. Over one quarter, the trigger tool recorded $n_1 = 120$ adverse events and the incident reporting system recorded $n_2 = 150$ adverse events. Upon de-duplication, there were $m = 60$ events that appeared in both sources.\n\nAssume a closed population of adverse events over the quarter, equal catchability across events, and source independence, meaning the probability that an event is captured by both sources equals the product of the probabilities that it is captured by each source individually. Using these assumptions and starting from the definitions of capture probabilities and counts, derive the two-source capture-recapture estimator for the total number of adverse events in the quarter and compute its numerical value for the given data.\n\nDefine the underreporting fraction as the proportion of adverse events missing from both sources relative to the estimated total, that is, the fraction of the estimated total number of adverse events that were not observed in either source. Compute this underreporting fraction for the given data.\n\nExpress the final underreporting fraction as a decimal number and round your answer to four significant figures.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established statistical method of capture-recapture analysis, a standard technique in fields like epidemiology and ecology, and its application to adverse event reporting is a recognized use case in medical informatics. The problem is well-posed, providing all necessary data ($n_1, n_2, m$) and clear assumptions (closed population, equal catchability, source independence) to derive a unique and meaningful solution. The language is objective and the data are internally consistent.\n\nLet $N$ represent the true total number of adverse events in the closed population during the quarter. This is the unknown quantity to be estimated.\nLet $n_1$ be the number of events captured by the first source (the EHR trigger tool), so $n_1 = 120$.\nLet $n_2$ be the number of events captured by the second source (the voluntary incident reporting system), so $n_2 = 150$.\nLet $m$ be the number of events captured by both sources, so $m = 60$.\n\nLet $p_1$ be the probability that a given adverse event is captured by source $1$.\nLet $p_2$ be the probability that a given adverse event is captured by source $2$.\nThe assumption of equal catchability means $p_1$ and $p_2$ are constant for all events.\n\nThe number of events captured by each source can be related to the total population size $N$ and the capture probabilities. The expected number of events captured are:\n$$E[n_1] = Np_1$$\n$$E[n_2] = Np_2$$\n\nThe assumption of source independence states that the probability of an event being captured by both sources is the product of the individual capture probabilities, $p_1 p_2$. Therefore, the expected number of events captured by both sources is:\n$$E[m] = N p_1 p_2$$\n\nWe can derive an estimator for $N$ using the method of moments. From the equations above, we can express $p_1$ and $p_2$ as:\n$$p_1 = \\frac{E[n_1]}{N} \\quad \\text{and} \\quad p_2 = \\frac{E[n_2]}{N}$$\nSubstituting these into the equation for $E[m]$:\n$$E[m] = N \\left( \\frac{E[n_1]}{N} \\right) \\left( \\frac{E[n_2]}{N} \\right) = \\frac{E[n_1] E[n_2]}{N}$$\nRearranging to solve for $N$, we get:\n$$N = \\frac{E[n_1] E[n_2]}{E[m]}$$\nBy replacing the expected values with the observed sample counts ($n_1$, $n_2$, and $m$), we obtain the two-source capture-recapture estimator for $N$, denoted as $\\hat{N}$:\n$$\\hat{N} = \\frac{n_1 n_2}{m}$$\nThis is also known as the Lincoln-Petersen estimator.\n\nNow, we compute the numerical value of $\\hat{N}$ using the provided data:\n$$\\hat{N} = \\frac{120 \\times 150}{60} = \\frac{18000}{60} = 300$$\nThe estimated total number of adverse events is $300$.\n\nNext, we are asked to compute the underreporting fraction, which is defined as the proportion of adverse events missing from both sources relative to the estimated total. This corresponds to the probability that an event is missed by source $1$ AND missed by source $2$.\nThe probability of an event being missed by source $1$ is $(1 - p_1)$.\nThe probability of an event being missed by source $2$ is $(1 - p_2)$.\nDue to the independence assumption, the probability of being missed by both sources is the product of these individual probabilities:\n$$P(\\text{missed by both}) = (1 - p_1)(1 - p_2)$$\nThis quantity is the underreporting fraction, which we denote $f_U$. We estimate it by substituting estimators for $p_1$ and $p_2$.\n\nWe can estimate $p_1$ and $p_2$ from the data. The proportion of events from the second sample ($n_2$) that were also captured by the first source ($m$) provides an estimate for $p_1$.\n$$\\hat{p}_1 = \\frac{m}{n_2}$$\nSimilarly, the proportion of events from the first sample ($n_1$) that were also captured by the second source ($m$) provides an estimate for $p_2$.\n$$\\hat{p}_2 = \\frac{m}{n_1}$$\nSubstituting these estimators into the expression for the underreporting fraction gives our estimator $\\hat{f}_U$:\n$$\\hat{f}_U = (1 - \\hat{p}_1)(1 - \\hat{p}_2) = \\left(1 - \\frac{m}{n_2}\\right)\\left(1 - \\frac{m}{n_1}\\right)$$\nUsing the given numerical values:\n$$\\hat{p}_1 = \\frac{60}{150} = \\frac{2}{5} = 0.4$$\n$$\\hat{p}_2 = \\frac{60}{120} = \\frac{1}{2} = 0.5$$\nNow we compute the underreporting fraction:\n$$\\hat{f}_U = (1 - 0.4)(1 - 0.5) = (0.6)(0.5) = 0.3$$\n\nAlternatively, we can calculate the number of events missed by both sources. The total number of unique events observed is $n_1 + n_2 - m = 120 + 150 - 60 = 210$.\nThe estimated number of unobserved events, $U$, is the estimated total minus the observed total:\n$$U = \\hat{N} - (n_1 + n_2 - m) = 300 - 210 = 90$$\nThe underreporting fraction is then the ratio of unobserved events to the estimated total:\n$$\\hat{f}_U = \\frac{U}{\\hat{N}} = \\frac{90}{300} = 0.3$$\nBoth methods yield the same result. The problem requires the answer as a decimal number rounded to four significant figures. The value $0.3$ expressed to four significant figures is $0.3000$.",
            "answer": "$$\\boxed{0.3000}$$"
        },
        {
            "introduction": "To improve event detection, hospitals are increasingly using automated systems that scan electronic health records, but no system is perfect. This practice challenges you to evaluate the performance of a simple classifier using the fundamental metrics of sensitivity and specificity. Mastering these concepts is essential for understanding how well a detection tool can distinguish true adverse events from irrelevant information .",
            "id": "4852083",
            "problem": "A hospital deploys a simple keyword-based rule system to flag mentions of a specific medication-related adverse event in Electronic Health Record (EHR) notes. In a test set of $500$ notes, independent adjudication (the gold standard) identified $50$ notes that truly contain the adverse event. The classifier flagged $65$ notes as positive; of these flagged notes, $45$ were truly positive, and $20$ were false positives. Using only the standard definitions of sensitivity and specificity relative to the gold standard, compute both metrics for this classifier. Express each metric as a reduced fraction. Provide your final answer as a row matrix $\\begin{pmatrix}\\text{sensitivity} & \\text{specificity}\\end{pmatrix}$. No rounding is required. Then, in one to two sentences, explain how adding clinical negation handling in Natural Language Processing (NLP) would be expected to change these metrics if it primarily corrects phrases such as “no evidence of X” and “denies X” without altering recognition of affirmed events. Your explanation does not affect the numerical answer you submit.",
            "solution": "The problem is well-posed and valid. It provides a self-contained and consistent set of data for a standard binary classification evaluation task. All necessary data points are provided, and they are internally consistent. The concepts of sensitivity and specificity are fundamental, objective, and scientifically grounded metrics in statistics and medical informatics.\n\nTo solve the problem, we first establish the components of the confusion matrix. Let $N$ be the total number of notes, $P$ be the number of notes with the condition present (true cases), and $N_{neg}$ be the number of notes with the condition absent (true negatives).\n\nFrom the givens:\nTotal notes, $N = 500$.\nTotal notes with the true adverse event, $P = 50$.\nTherefore, the total number of notes without the adverse event is $N_{neg} = N - P = 500 - 50 = 450$.\n\nThe classifier's performance is described by the following:\nTotal notes flagged as positive (Predicted Positives) = $65$.\nOf these, the number of correctly flagged notes, or True Positives ($TP$), is $TP = 45$.\nThe number of incorrectly flagged notes, or False Positives ($FP$), is $FP = 20$.\nWe can verify the consistency of the data: $TP + FP = 45 + 20 = 65$, which matches the total number of notes flagged as positive.\n\nNow we can determine the remaining two components of the confusion matrix:\nFalse Negatives ($FN$) are the notes that truly contain the adverse event but were not flagged by the classifier. This is the difference between the total true cases and the true positives.\n$$FN = P - TP = 50 - 45 = 5$$\n\nTrue Negatives ($TN$) are the notes that do not contain the adverse event and were correctly not flagged by the classifier. This is the difference between the total non-event notes and the false positives.\n$$TN = N_{neg} - FP = 450 - 20 = 430$$\n\nWe can summarize these values in a contingency table:\n\\begin{array}{c|c|c|c}\n & \\text{Condition Positive} & \\text{Condition Negative} & \\text{Total} \\\\\n\\hline\n\\text{Predicted Positive} & TP = 45 & FP = 20 & 65 \\\\\n\\hline\n\\text{Predicted Negative} & FN = 5 & TN = 430 & 435 \\\\\n\\hline\n\\text{Total} & 50 & 450 & 500 \\\\\n\\end{array}\n\nNow, we compute sensitivity and specificity using their standard definitions.\n\nSensitivity, also known as the True Positive Rate ($TPR$), is the proportion of actual positives that are correctly identified. The formula is:\n$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\nSubstituting the values we found:\n$$\\text{Sensitivity} = \\frac{45}{45 + 5} = \\frac{45}{50}$$\nReducing this fraction to its simplest form:\n$$\\text{Sensitivity} = \\frac{9 \\times 5}{10 \\times 5} = \\frac{9}{10}$$\n\nSpecificity, also known as the True Negative Rate ($TNR$), is the proportion of actual negatives that are correctly identified. The formula is:\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\nSubstituting the values we found:\n$$\\text{Specificity} = \\frac{430}{430 + 20} = \\frac{430}{450}$$\nReducing this fraction to its simplest form by dividing the numerator and denominator by $10$:\n$$\\text{Specificity} = \\frac{43}{45}$$\nThe number $43$ is prime, so this fraction cannot be reduced further.\n\nThe final answer is to be presented as a row matrix $\\begin{pmatrix}\\text{sensitivity} & \\text{specificity}\\end{pmatrix}$.\n$$\\begin{pmatrix} \\frac{9}{10} & \\frac{43}{45} \\end{pmatrix}$$\n\nRegarding the final explanatory question: Adding NLP for negation handling would primarily correct false positives by identifying negated mentions of the event, thereby increasing the number of true negatives. This change would be expected to increase the classifier's specificity while leaving its sensitivity unchanged, as the number of true positives and false negatives would not be affected.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{10} & \\frac{43}{45} \\end{pmatrix}}$$"
        },
        {
            "introduction": "A classifier might be very accurate in theory, but how useful is it in practice, especially when searching for rare events? This exercise delves into the concept of Positive Predictive Value ($PPV$), which answers the crucial question: \"If the system flags an event, what is the probability that it's a real event?\" You will see how an event's low prevalence can dramatically impact a system's real-world reliability .",
            "id": "4852094",
            "problem": "A large hospital system deploys an Electronic Health Record (EHR)-based adverse event classifier to flag possible inpatient adverse drug events for safety review. Let the true adverse event status be a binary event, and let the classifier output be a binary test result. Positive Predictive Value ($PPV$) is defined as the probability that an adverse event truly occurred given a positive classifier result. Sensitivity is defined as the probability of a positive classifier result given that an adverse event truly occurred. Specificity is defined as the probability of a negative classifier result given that an adverse event did not occur. Prevalence is defined as the prior probability that an adverse event truly occurred in the monitored population.\n\nUsing only the formal definitions of sensitivity, specificity, prevalence, and basic probability laws, including the law of total probability and Bayes’ theorem as foundational tools, do the following:\n\n1) Derive an analytic expression for the Positive Predictive Value $PPV$ in terms of sensitivity $s$, specificity $c$, and prevalence $p$.\n\n2) Using your derived expression, compute $PPV$ for sensitivity $s=0.85$, specificity $c=0.95$, and prevalence $p=0.02$. Round your final numerical result to four significant figures. Express your final answer as a decimal (do not use a percentage sign).",
            "solution": "The problem is subjected to validation and is found to be valid. It is scientifically grounded in the principles of probability theory and its application to medical diagnostics, a core topic in medical informatics. The problem is well-posed, objective, and contains all necessary information for a complete, unique solution. There are no contradictions, ambiguities, or unrealistic assumptions. We may therefore proceed with the solution.\n\nLet $A$ denote the event that a true adverse event has occurred, and let $A^c$ denote the complementary event that no adverse event has occurred.\nLet $T^+$ denote the event that the classifier yields a positive result, and let $T^-$ denote the event that the classifier yields a negative result.\n\nThe given definitions can be formally expressed as probabilities conditional on these events:\n- Prevalence, $p$: The prior probability of an adverse event.\n$$p = P(A)$$\n- Sensitivity, $s$: The probability of a positive test result given that an adverse event has occurred (True Positive Rate).\n$$s = P(T^+ | A)$$\n- Specificity, $c$: The probability of a negative test result given that an adverse event has not occurred (True Negative Rate).\n$$c = P(T^- | A^c)$$\n\nThe problem asks for an expression for the Positive Predictive Value ($PPV$), which is defined as the probability that an adverse event has truly occurred given that the classifier result is positive.\n$$PPV = P(A | T^+)$$\n\n**1) Derivation of the analytic expression for PPV**\n\nWe begin by applying Bayes' theorem to the definition of $PPV$:\n$$P(A | T^+) = \\frac{P(T^+ | A) P(A)}{P(T^+)}$$\nThe numerator consists of terms that are directly given in the problem statement:\n- $P(T^+ | A)$ is the sensitivity, $s$.\n- $P(A)$ is the prevalence, $p$.\n\nThus, the numerator is $s \\cdot p$.\n\nThe denominator, $P(T^+)$, is the overall probability of a positive test result. This term is not directly given and must be expressed in terms of $s$, $c$, and $p$. To do this, we use the law of total probability, conditioning on the true event status ($A$ or $A^c$):\n$$P(T^+) = P(T^+ | A) P(A) + P(T^+ | A^c) P(A^c)$$\nLet's analyze each term in this expansion:\n- $P(T^+ | A) P(A)$: This is the probability of a true positive. We already know $P(T^+ | A) = s$ and $P(A) = p$. So, this term is $s \\cdot p$.\n- $P(T^+ | A^c) P(A^c)$: This is the probability of a false positive. We need to find expressions for $P(T^+ | A^c)$ and $P(A^c)$.\n    - $P(A^c)$ is the probability that an adverse event does not occur. Since $A$ and $A^c$ are complementary events, their probabilities sum to $1$.\n    $$P(A^c) = 1 - P(A) = 1 - p$$\n    - $P(T^+ | A^c)$ is the probability of a positive test result given that no adverse event occurred (False Positive Rate). We are given the specificity, $c = P(T^- | A^c)$, which is the probability of a *negative* test given no event. Since the test result is binary (either positive, $T^+$, or negative, $T^-$), the probabilities of these outcomes for a given condition must sum to $1$.\n    $$P(T^+ | A^c) + P(T^- | A^c) = 1$$\n    Therefore, the False Positive Rate is:\n    $$P(T^+ | A^c) = 1 - P(T^- | A^c) = 1 - c$$\n    Combining these, the probability of a false positive is $(1-c)(1-p)$.\n\nSubstituting these components back into the law of total probability expression for $P(T^+)$:\n$$P(T^+) = (s \\cdot p) + (1-c)(1-p)$$\nNow, we can substitute the expanded form of $P(T^+)$ back into the Bayes' theorem expression for $PPV$:\n$$PPV = \\frac{s \\cdot p}{s \\cdot p + (1-c)(1-p)}$$\nThis is the required analytic expression for the Positive Predictive Value in terms of sensitivity ($s$), specificity ($c$), and prevalence ($p$).\n\n**2) Computation of PPV**\n\nWe are given the following values:\n- Sensitivity, $s = 0.85$\n- Specificity, $c = 0.95$\n- Prevalence, $p = 0.02$\n\nWe substitute these numerical values into the derived expression for $PPV$.\n\nFirst, calculate the numerator (the probability of a true positive):\n$$\\text{Numerator} = s \\cdot p = 0.85 \\times 0.02 = 0.017$$\n\nNext, calculate the components of the denominator.\nThe first term of the denominator is the same as the numerator: $s \\cdot p = 0.017$.\nThe second term of the denominator is the probability of a false positive:\n$$(1-c)(1-p) = (1 - 0.95) \\times (1 - 0.02) = 0.05 \\times 0.98 = 0.049$$\n\nNow, calculate the full denominator (the total probability of a positive test):\n$$\\text{Denominator} = s \\cdot p + (1-c)(1-p) = 0.017 + 0.049 = 0.066$$\n\nFinally, compute the $PPV$:\n$$PPV = \\frac{\\text{Numerator}}{\\text{Denominator}} = \\frac{0.017}{0.066}$$\n$$PPV \\approx 0.25757575...$$\nThe problem requires the result to be rounded to four significant figures. The first four significant figures are $2$, $5$, $7$, $5$. The fifth digit is $7$, which is greater than or equal to $5$, so we round up the fourth significant digit.\n$$PPV \\approx 0.2576$$",
            "answer": "$$\\boxed{0.2576}$$"
        }
    ]
}