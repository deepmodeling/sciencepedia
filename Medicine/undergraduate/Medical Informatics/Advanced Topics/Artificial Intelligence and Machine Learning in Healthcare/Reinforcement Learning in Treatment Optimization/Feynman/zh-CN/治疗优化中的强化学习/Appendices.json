{
    "hands_on_practices": [
        {
            "introduction": "在临床决策中，我们常常面临一个两难的困境：是应该坚持当前看起来最优的治疗方案（利用），还是尝试其他可能效果更好但尚不确定的方案（探索）？此练习将此“探索-利用”困境简化为一个多臂老虎机问题，这是一种常用于模拟单次决策优化的强化学习模型。通过应用置信上界（UCB）算法，您将学习如何根据现有数据，在利用已知最佳选择和探索未知潜在更优选择之间做出智能化的权衡。",
            "id": "4855011",
            "problem": "一个医院的处方集委员会正在试用强化学习（RL）来优化一个为期一天的滴定方案中的抗高血压药物剂量。该场景被建模为一个随机多臂老虎机问题，有 $3$ 种剂量行动（索引为 $i \\in \\{1,2,3\\}$）。每个行动产生的奖励等于 $24$ 小时内收缩压的归一化改善值，该值被缩放到 $[0,1]$ 区间内。在当前决策时刻，根据先前患者的数据，得到的经验平均奖励估计值和观测次数如下：行动 $1$ 的经验平均奖励 $\\hat{\\mu}_{1} = 0.62$，观测次数 $n_{1} = 40$；行动 $2$ 的经验平均奖励 $\\hat{\\mu}_{2} = 0.66$，观测次数 $n_{2} = 20$；行动 $3$ 的经验平均奖励 $\\hat{\\mu}_{3} = 0.58$，观测次数 $n_{3} = 10$。设当前时间索引为 $t = \\sum_{i=1}^{3} n_{i}$。\n\n假设奖励是独立同分布（i.i.d.）的，并且界于 $[0,1]$ 区间内。使用霍夫丁不等式（Hoeffding’s inequality）作为基本统计原理，为每个行动的未知真实平均奖励 $\\mu_{i}$ 构建一个高概率上界，并采用置信度调度 $\\delta(t) = t^{-2}$ 来控制探索。基于这些基础，为每个行动推导一个置信上界，并根据置信上界（UCB）原则来决定下一步应选择哪个行动。如果出现平局，选择索引最小的行动。\n\n将最终答案表述为所选行动的索引，以一个不带单位的数字表示。最终答案无需四舍五入。",
            "solution": "首先将对问题的科学合理性、清晰度和完整性进行验证。\n\n### 步骤 1：提取已知条件\n- 行动（臂）的数量：$3$，索引为 $i \\in \\{1, 2, 3\\}$。\n- 奖励：收缩压的归一化改善值，缩放到 $[0, 1]$ 区间内。\n- 行动 $1$ 的经验数据：经验平均奖励 $\\hat{\\mu}_{1} = 0.62$，观测次数 $n_{1} = 40$。\n- 行动 $2$ 的经验数据：经验平均奖励 $\\hat{\\mu}_{2} = 0.66$，观测次数 $n_{2} = 20$。\n- 行动 $3$ 的经验数据：经验平均奖励 $\\hat{\\mu}_{3} = 0.58$，观测次数 $n_{3} = 10$。\n- 当前时间索引：$t = \\sum_{i=1}^{3} n_{i}$。\n- 统计假设：奖励是独立同分布（i.i.d.）的，并且界于 $[0, 1]$ 区间内。\n- 基本原理：霍夫丁不等式 (Hoeffding's inequality)。\n- 置信度调度：$\\delta(t) = t^{-2}$。\n- 决策规则：选择具有最高置信上界（UCB）的行动。\n- 平局打破规则：如果出现平局，选择索引最小的行动。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据充分：** 该问题牢固地建立在强化学习的成熟理论，特别是多臂老虎机问题之上。使用基于霍夫丁不等式（Hoeffding's inequality）的置信上界（UCB）算法是一种经典方法。医疗背景是此类优化算法的一个现实应用领域。\n- **问题定义明确：** 该问题提供了所有必要的数据（$\\hat{\\mu}_i$, $n_i$）、一个明确的目标（使用 UCB 原则选择下一个行动）、一种构建置信区间的具体方法（使用 $\\delta(t)=t^{-2}$ 的霍夫丁不等式），以及一个平局打破规则。这种结构确保了存在一个唯一的、有意义的解。\n- **客观性：** 问题以精确、量化且无偏见的语言陈述。\n- **完整性与一致性：** 问题是自洽的。所提供的数据和条件足以推导出解。没有内部矛盾。\n\n### 步骤 3：结论与行动\n问题有效。将提供一个完整的、有理有据的解。\n\n### 解题过程\n该问题要求根据置信上界（UCB）原则选择下一个行动。每个行动 $i$ 的 UCB 是其经验平均奖励 $\\hat{\\mu}_i$ 与一个量化该估计不确定性的置信项（或探索奖励）之和。选择具有最高 UCB 的行动。\n\n置信项由霍夫丁不等式（Hoeffding's inequality）推导而来。对于行动 $i$ 的一组 $n_i$ 个独立同分布的、界于 $[0, 1]$ 的奖励 $R_j$，霍夫丁不等式为真实平均奖励 $\\mu_i$ 提供了一个单边界：\n$$ P(\\mu_i > \\hat{\\mu}_i + \\epsilon) \\le \\exp(-2n_i \\epsilon^2) $$\n其中 $\\hat{\\mu}_i$ 是经验平均值，$\\epsilon > 0$ 是置信区间的宽度。\n\n问题指定使用置信度调度 $\\delta(t) = t^{-2}$ 来控制真实平均值超过上界的概率。我们将不等式的右侧设为 $\\delta(t)$：\n$$ \\exp(-2n_i \\epsilon^2) = \\delta(t) = t^{-2} $$\n我们求解 $\\epsilon$ 以找到探索奖励：\n$$ -2n_i \\epsilon^2 = \\ln(t^{-2}) $$\n$$ -2n_i \\epsilon^2 = -2\\ln(t) $$\n$$ \\epsilon^2 = \\frac{\\ln(t)}{n_i} $$\n$$ \\epsilon = \\sqrt{\\frac{\\ln(t)}{n_i}} $$\n这一项代表我们对 $\\mu_i$ 估计的置信度。因此，行动 $i$ 在时间 $t$ 的 UCB 为：\n$$ UCB_i(t) = \\hat{\\mu}_i + \\sqrt{\\frac{\\ln(t)}{n_i}} $$\n\n首先，我们计算当前的时间索引 $t$：\n$$ t = n_1 + n_2 + n_3 = 40 + 20 + 10 = 70 $$\n现在，我们使用给定的数据计算三个行动中每一个的 UCB。\n\n对于行动 $1$：\n$$ UCB_1 = \\hat{\\mu}_1 + \\sqrt{\\frac{\\ln(t)}{n_1}} = 0.62 + \\sqrt{\\frac{\\ln(70)}{40}} $$\n对于行动 $2$：\n$$ UCB_2 = \\hat{\\mu}_2 + \\sqrt{\\frac{\\ln(t)}{n_2}} = 0.66 + \\sqrt{\\frac{\\ln(70)}{20}} $$\n对于行动 $3$：\n$$ UCB_3 = \\hat{\\mu}_3 + \\sqrt{\\frac{\\ln(t)}{n_3}} = 0.58 + \\sqrt{\\frac{\\ln(70)}{10}} $$\n为确定选择哪个行动，我们必须找到 $\\{UCB_1, UCB_2, UCB_3\\}$ 中的最大值。设基础探索项为 $B = \\sqrt{\\frac{\\ln(70)}{40}}$。由于 $t=70 > 1$，所以 $\\ln(70) > 0$，因此 $B > 0$。我们可以用 $B$ 来表示其他的探索项：\n$$ \\sqrt{\\frac{\\ln(70)}{20}} = \\sqrt{2 \\cdot \\frac{\\ln(70)}{40}} = \\sqrt{2}B $$\n$$ \\sqrt{\\frac{\\ln(70)}{10}} = \\sqrt{4 \\cdot \\frac{\\ln(70)}{40}} = 2B $$\nUCB 表达式变为：\n$$ UCB_1 = 0.62 + B $$\n$$ UCB_2 = 0.66 + \\sqrt{2}B $$\n$$ UCB_3 = 0.58 + 2B $$\n\n接下来，我们比较这些值。\n1.  比较 $UCB_2$ 和 $UCB_1$：\n    $$ UCB_2 - UCB_1 = (0.66 + \\sqrt{2}B) - (0.62 + B) = 0.04 + (\\sqrt{2} - 1)B $$\n    由于 $\\sqrt{2} \\approx 1.414 > 1$，所以 $(\\sqrt{2}-1) > 0$。又因为 $B>0$，所以整个表达式为正。因此，$UCB_2 > UCB_1$。\n\n2.  比较 $UCB_3$ 和 $UCB_2$：\n    $$ UCB_3 - UCB_2 = (0.58 + 2B) - (0.66 + \\sqrt{2}B) = (2 - \\sqrt{2})B - 0.08 $$\n    为了确定这个表达式的符号，我们必须检查 $(2 - \\sqrt{2})B$ 是否大于 $0.08$。代入 $B = \\sqrt{\\frac{\\ln(70)}{40}}$：\n    $$ (2 - \\sqrt{2})\\sqrt{\\frac{\\ln(70)}{40}} > 0.08 $$\n    由于两边都是正数，我们可以将它们平方而不改变不等号的方向：\n    $$ (2 - \\sqrt{2})^2 \\frac{\\ln(70)}{40} > (0.08)^2 = 0.0064 $$\n    $$ (4 - 4\\sqrt{2} + 2)\\ln(70) > 40 \\times 0.0064 $$\n    $$ (6 - 4\\sqrt{2})\\ln(70) > 0.256 $$\n    我们可以使用已知的近似值来评估左侧。我们知道 $1.414 < \\sqrt{2} < 1.415$ 并且 $e^4 \\approx 54.6$, $e^{4.25} \\approx 70.1$。所以 $\\ln(70)$ 略小于 $4.25$。我们使用 $\\ln(70) > 4.2$ 和 $\\sqrt{2} < 1.415$。\n    $$ 6 - 4\\sqrt{2} > 6 - 4(1.415) = 6 - 5.66 = 0.34 $$\n    所以，左侧大于 $0.34 \\times 4.2 = 1.428$。\n    因为 $1.428 > 0.256$，所以不等式成立。因此，$UCB_3 - UCB_2 > 0$，这意味着 $UCB_3 > UCB_2$。\n\n通过比较，我们确定了顺序 $UCB_3 > UCB_2 > UCB_1$。UCB 原则要求选择使置信上界最大化的行动。这对应于行动 $3$。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "许多医疗决策并非一次性完成，而是一个涉及多步骤、贯穿整个治疗过程的序列。马尔可夫决策过程（MDP）为这种序贯决策问题提供了强大的数学框架。本练习构建了一个完整的临床场景，其中患者的状态（如“危重”、“稳定”）和治疗干预（如“积极”、“保守”）的效果均被精确定义，您需要运用动态规划的核心思想，计算出能最大化长期期望回报的最优治疗策略。",
            "id": "4855005",
            "problem": "一位临床医生正在为急症护理环境中的序贯治疗设计一种强化学习（RL）策略。将该问题建模为一个带折扣的有限马尔可夫决策过程（MDP），包含状态、动作、转移概率和一个具有临床动机的奖励函数。状态空间包含 $3$ 个健康状态：$s_{1}$（危重）、$s_{2}$（稳定）和 $s_{3}$（出院）。动作空间包含在 $s_{1}$ 和 $s_{2}$ 中可用的 $2$ 种治疗选择：$a_{1}$（积极）和 $a_{2}$（保守）。状态 $s_{3}$ 是一个吸收态，其唯一的被动动作使状态保持不变。\n\n对于 $s_{1}$（危重）状态，转移动态如下：\n- 如果选择动作 $a_{1}$（积极）：$\\mathbb{P}(s_{1} \\rightarrow s_{1}) = 0.2$，$\\mathbb{P}(s_{1} \\rightarrow s_{2}) = 0.6$，$\\mathbb{P}(s_{1} \\rightarrow s_{3}) = 0.2$。\n- 如果选择动作 $a_{2}$（保守）：$\\mathbb{P}(s_{1} \\rightarrow s_{1}) = 0.5$，$\\mathbb{P}(s_{1} \\rightarrow s_{2}) = 0.4$，$\\mathbb{P}(s_{1} \\rightarrow s_{3}) = 0.1$。\n\n对于 $s_{2}$（稳定）状态：\n- 如果选择动作 $a_{1}$（积极）：$\\mathbb{P}(s_{2} \\rightarrow s_{1}) = 0.1$，$\\mathbb{P}(s_{2} \\rightarrow s_{2}) = 0.2$，$\\mathbb{P}(s_{2} \\rightarrow s_{3}) = 0.7$。\n- 如果选择动作 $a_{2}$（保守）：$\\mathbb{P}(s_{2} \\rightarrow s_{1}) = 0.05$，$\\mathbb{P}(s_{2} \\rightarrow s_{2}) = 0.45$，$\\mathbb{P}(s_{2} \\rightarrow s_{3}) = 0.5$。\n\n状态 $s_{3}$（出院）是吸收态：$\\mathbb{P}(s_{3} \\rightarrow s_{3}) = 1$。\n\n单步奖励 $r(s,a,s')$ 被定义为对临床结果和治疗副作用进行编码。在动作 $a \\in \\{a_{1}, a_{2}\\}$ 下，从一个非吸收态 $s \\in \\{s_{1}, s_{2}\\}$ 转移到 $s' \\in \\{s_{1}, s_{2}, s_{3}\\}$ 时，奖励为：\n- 结果部分：如果 $s' = s_{3}$（出院），奖励 $+10$；如果 $s' = s_{2}$（稳定），奖励 $+2$；如果 $s' = s_{1}$（危重），奖励 $-3$。\n- 动作成本：对于 $a = a_{1}$（积极），成本为 $-2$；对于 $a = a_{2}$（保守），成本为 $-0.5$。\n\n对于从吸收态 $s_{3}$ 开始的转移，所有奖励均为 $0$。折扣因子为 $\\gamma = 0.9$。\n\n从时间 $t=0$ 的初始状态 $s_{1}$（危重）开始，确定能够最大化预期折扣奖励总和的最优静态确定性策略，并计算 $s_{1}$ 对应的最优状态值，记为 $V^{\\ast}(s_{1})$。\n\n您的推导应基于马尔可夫决策过程和强化学习的基本原理，从预期折扣回报的定义和最优性原理开始。在计算 $V^{\\ast}(s_{1})$ 之前，您必须明确证明您所选择的最优策略的合理性。\n\n答案规范：\n- 仅报告最优预期折扣值 $V^{\\ast}(s_{1})$，形式为一个实数。\n- 将您的最终数值答案四舍五入到四位有效数字。\n- 在最终数值答案中不要包含单位。",
            "solution": "该问题描述了一个带折扣的有限马尔可夫决策过程（MDP）。我们被要求找到最优状态值 $V^{\\ast}(s_1)$。最优状态值函数 $V^{\\ast}(s)$ 是贝尔曼最优方程的唯一解：\n$$V^{\\ast}(s) = \\max_{a \\in \\mathcal{A}(s)} \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s' | s, a) \\left[ R(s, a, s') + \\gamma V^{\\ast}(s') \\right]$$\n其中 $R(s,a,s')$ 是单步奖励，$\\gamma$ 是折扣因子。\n状态空间为 $\\mathcal{S} = \\{s_1, s_2, s_3\\}$，动作空间为 $\\mathcal{A}(s_1)=\\mathcal{A}(s_2)=\\{a_1, a_2\\}$。状态 $s_3$ 是吸收态，所以它的值是到达那里时获得的即时奖励，从 $s_3$ 出发的奖励为 0。因此，$V^{\\ast}(s_3) = 0$。\n\n让我们定义状态-动作值函数（Q-函数）如下：\n$$Q^{\\ast}(s,a) = \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s' | s, a) \\left[ R(s, a, s') + \\gamma V^{\\ast}(s') \\right]$$\n那么，$V^{\\ast}(s) = \\max_{a} Q^{\\ast}(s,a)$。单步奖励 $R(s,a,s')$ 是结果部分和动作成本的总和。让我们首先计算每个状态-动作对的预期即时奖励。设 $r(s,a) = \\sum_{s'} \\mathbb{P}(s'|s,a) R(s,a,s')$。\n$R(s, a, s') = R_{out}(s') + R_{act}(a)$。\n$r(s_1, a_1) = 0.2(-3-2) + 0.6(2-2) + 0.2(10-2) = 0.2(-5) + 0.6(0) + 0.2(8) = -1 + 1.6 = 0.6$。\n$r(s_1, a_2) = 0.5(-3-0.5) + 0.4(2-0.5) + 0.1(10-0.5) = 0.5(-3.5) + 0.4(1.5) + 0.1(9.5) = -1.75 + 0.6 + 0.95 = -0.2$。\n$r(s_2, a_1) = 0.1(-3-2) + 0.2(2-2) + 0.7(10-2) = 0.1(-5) + 0.2(0) + 0.7(8) = -0.5 + 5.6 = 5.1$。\n$r(s_2, a_2) = 0.05(-3-0.5) + 0.45(2-0.5) + 0.5(10-0.5) = 0.05(-3.5) + 0.45(1.5) + 0.5(9.5) = -0.175 + 0.675 + 4.75 = 5.25$。\n\n现在，关于 $V^{\\ast}(s_1)$ 和 $V^{\\ast}(s_2)$ 的贝尔曼方程是：\n$V^{\\ast}(s_1) = \\max \\{ r(s_1, a_1) + \\gamma \\sum_{s'} \\mathbb{P}(s'|s_1,a_1)V^{\\ast}(s'), \\quad r(s_1, a_2) + \\gamma \\sum_{s'} \\mathbb{P}(s'|s_1,a_2)V^{\\ast}(s') \\}$\n$V^{\\ast}(s_2) = \\max \\{ r(s_2, a_1) + \\gamma \\sum_{s'} \\mathbb{P}(s'|s_2,a_1)V^{\\ast}(s'), \\quad r(s_2, a_2) + \\gamma \\sum_{s'} \\mathbb{P}(s'|s_2,a_2)V^{\\ast}(s') \\}$\n\n当 $\\gamma=0.9$ 且 $V^{\\ast}(s_3)=0$ 时：\n$V^{\\ast}(s_1) = \\max \\{ 0.6 + 0.9(0.2V^{\\ast}(s_1) + 0.6V^{\\ast}(s_2)), \\quad -0.2 + 0.9(0.5V^{\\ast}(s_1) + 0.4V^{\\ast}(s_2)) \\}$\n$V^{\\ast}(s_1) = \\max \\{ 0.6 + 0.18V^{\\ast}(s_1) + 0.54V^{\\ast}(s_2), \\quad -0.2 + 0.45V^{\\ast}(s_1) + 0.36V^{\\ast}(s_2) \\}$\n$V^{\\ast}(s_2) = \\max \\{ 5.1 + 0.9(0.1V^{\\ast}(s_1) + 0.2V^{\\ast}(s_2)), \\quad 5.25 + 0.9(0.05V^{\\ast}(s_1) + 0.45V^{\\ast}(s_2)) \\}$\n$V^{\\ast}(s_2) = \\max \\{ 5.1 + 0.09V^{\\ast}(s_1) + 0.18V^{\\ast}(s_2), \\quad 5.25 + 0.045V^{\\ast}(s_1) + 0.405V^{\\ast}(s_2) \\}$\n\n我们可以使用价值迭代来解这个方程组。让我们初始化 $V_0(s_1)=0, V_0(s_2)=0$。\n$k=1$:\n$V_1(s_1) = \\max\\{0.6, -0.2\\} = 0.6$（选择 $a_1$）\n$V_1(s_2) = \\max\\{5.1, 5.25\\} = 5.25$（选择 $a_2$）\n所以初始的策略猜测是 $\\pi_1(s_1)=a_1, \\pi_1(s_2)=a_2$。\n\n让我们假设这是最优策略，并为该策略的价值函数 $V^\\pi$ 求解相应的线性方程组。这是策略迭代中的策略评估步骤。\n设 $V_1=V^{\\pi}(s_1)$ 且 $V_2=V^{\\pi}(s_2)$。\n对于 $\\pi(s_1) = a_1$：$V_1 = 0.6 + 0.18V_1 + 0.54V_2 \\implies 0.82V_1 - 0.54V_2 = 0.6$\n对于 $\\pi(s_2) = a_2$：$V_2 = 5.25 + 0.045V_1 + 0.405V_2 \\implies -0.045V_1 + 0.595V_2 = 5.25$\n\n这是一个由两个线性方程组成的方程组：\n1) $0.82V_1 - 0.54V_2 = 0.6$\n2) $-0.045V_1 + 0.595V_2 = 5.25$\n\n由(2)得，$V_2 = \\frac{5.25 + 0.045V_1}{0.595}$。代入(1)：\n$0.82V_1 - 0.54 \\left( \\frac{5.25 + 0.045V_1}{0.595} \\right) = 0.6$\n乘以 $0.595$:\n$0.82 \\times 0.595 V_1 - 0.54(5.25 + 0.045V_1) = 0.6 \\times 0.595$\n$0.4879V_1 - 2.835 - 0.0243V_1 = 0.357$\n$0.4636V_1 = 3.192$\n$V_1 = \\frac{3.192}{0.4636} \\approx 6.8852459$\n\n现在求 $V_2$:\n$V_2 = \\frac{5.25 + 0.045(6.8852459)}{0.595} \\approx \\frac{5.25 + 0.309836}{0.595} \\approx \\frac{5.559836}{0.595} \\approx 9.34426$\n\n现在，我们必须证明这个策略 $\\pi(s_1)=a_1, \\pi(s_2)=a_2$ 确实是最优的。我们通过检查策略改进条件来做到这一点：对于每个状态 $s$，动作 $\\pi(s)$ 是否仍然是最大化 Q 值的那个？\n$V^{\\pi}(s_1) \\approx 6.885$, $V^{\\pi}(s_2) \\approx 9.344$。\n对于 $s_1$：\n$Q^{\\pi}(s_1, a_1) = 0.6 + 0.18(6.885) + 0.54(9.344) \\approx 0.6 + 1.2393 + 5.0458 = 6.8851 \\approx V^{\\pi}(s_1)$ (如预期)\n$Q^{\\pi}(s_1, a_2) = -0.2 + 0.45(6.885) + 0.36(9.344) \\approx -0.2 + 3.0983 + 3.3638 = 6.2621$\n由于 $Q^{\\pi}(s_1, a_1) > Q^{\\pi}(s_1, a_2)$，选择 $\\pi(s_1)=a_1$ 是正确的。\n\n对于 $s_2$：\n$Q^{\\pi}(s_2, a_1) = 5.1 + 0.09(6.885) + 0.18(9.344) \\approx 5.1 + 0.61965 + 1.68192 = 7.40157$\n$Q^{\\pi}(s_2, a_2) = 5.25 + 0.045(6.885) + 0.405(9.344) \\approx 5.25 + 0.3098 + 3.7843 = 9.3441 \\approx V^{\\pi}(s_2)$ (如预期)\n由于 $Q^{\\pi}(s_2, a_2) > Q^{\\pi}(s_2, a_1)$，选择 $\\pi(s_2)=a_2$ 是正确的。\n\n策略 $\\pi = \\{a_1, a_2\\}$ 是稳定的，因此是最优的。$s_1$ 的最优状态值为 $V^{\\ast}(s_1) = V_1 \\approx 6.8852459$。\n四舍五入到四位有效数字得到 $6.885$。",
            "answer": "$$\\boxed{6.885}$$"
        },
        {
            "introduction": "在前一个练习中，我们假设了治疗效果的模型是完全已知的，但这在现实世界中很少发生。一个更实际且至关重要的问题是：我们能否仅利用已有的电子健康记录（EHR）数据，在不进行实际临床试验的情况下，评估一个全新治疗策略的潜在效果？这个问题引出了“离策略评估”（Off-Policy Evaluation, OPE）的概念。本练习将指导您使用双重稳健（Doubly Robust, DR）估计器，这是一种结合了结果建模和重要性加权的先进技术，旨在从历史数据中可靠地估计新策略的价值，为医疗AI的安全部署提供关键依据。",
            "id": "4855025",
            "problem": "考虑一个在医学信息学中用于治疗优化的强化学习（RL）单步治疗决策问题。临床背景是基于一个二元风险组指标 $x \\in \\{0,1\\}$ 的脓毒症抗生素选择，其中 $x=1$ 表示高风险，$x=0$ 表示低风险。一项离策略评估（OPE）任务是使用由行为策略 $b$ 生成的回顾性电子健康记录（EHR）数据，来估计一个确定性目标策略 $\\pi$ 下的预期归一化临床改善。给定一个包含 $n=5$ 个独立患者事件的小数据集 $\\{(x_i,a_i,y_i)\\}_{i=1}^{5}$，其中 $x_i$ 是风险组，$a_i \\in \\{\\mathrm{T0},\\mathrm{T1}\\}$ 是所选的抗生素，$y_i \\in [0,1]$ 是观察到的归一化改善。确定性目标策略规定，对于高风险患者 $\\pi(\\mathrm{T1}\\mid x=1)=1$，对于低风险患者 $\\pi(\\mathrm{T0}\\mid x=0)=1$，并且 $\\pi(\\mathrm{T0}\\mid x=1)=\\pi(\\mathrm{T1}\\mid x=0)=0$。\n\n您还有一个从历史数据中训练出来的结果回归模型 $m(x,a)$，它提供以下动作价值预测：\n- 对于高风险 $x=1$：$m(1,\\mathrm{T1})=0.75$, $m(1,\\mathrm{T0})=0.55$。\n- 对于低风险 $x=0$：$m(0,\\mathrm{T0})=0.68$, $m(0,\\mathrm{T1})=0.52$。\n\n提供了以下事件和预先计算的重要性权重 $w_i=\\pi(a_i\\mid x_i)/b(a_i\\mid x_i)$：\n- 事件 $i=1$：$(x_1=1,\\ a_1=\\mathrm{T1},\\ y_1=0.80)$，权重 $w_1=\\frac{10}{7}$。\n- 事件 $i=2$：$(x_2=1,\\ a_2=\\mathrm{T0},\\ y_2=0.50)$，权重 $w_2=0$。\n- 事件 $i=3$：$(x_3=0,\\ a_3=\\mathrm{T0},\\ y_3=0.70)$，权重 $w_3=\\frac{5}{3}$。\n- 事件 $i=4$：$(x_4=0,\\ a_4=\\mathrm{T1},\\ y_4=0.40)$，权重 $w_4=0$。\n- 事件 $i=5$：$(x_5=1,\\ a_5=\\mathrm{T1},\\ y_5=0.60)$，权重 $w_5=\\frac{10}{7}$。\n\n仅使用策略价值、重要性采样和全期望定律的基本定义，推导目标策略价值 $V(\\pi)$ 的单步双重稳健（DR）估计量的表达式，然后根据给定数据集计算其经验估计值。清晰地说明您计算出的值。将您的最终数值答案四舍五入到四位有效数字。将您的答案表示为一个无单位的实数。",
            "solution": "用户要求在一个单步离策略评估设置中，推导并计算目标策略价值 $V(\\pi)$ 的双重稳健（DR）估计量。\n\n首先，我们将推导双重稳健估计量的一般形式。确定性目标策略 $\\pi$ 的价值是当根据该策略选择动作时所获得的预期奖励。它可以通过对状态 $x$ 的分布和动作价值函数 $Q(x,a) = \\mathbb{E}[R \\mid X=x, A=a]$ 应用全期望定律来定义，其中 $R$ 是奖励：\n$$V(\\pi) = \\mathbb{E}_{x \\sim p(x)}[Q(x, \\pi(x))]$$\n这里，$p(x)$ 是状态的边际概率分布。给定一个结果回归模型 $m(x,a)$，它是真实动作价值函数 $Q(x,a)$ 的一个近似。\n\nDR 估计量将直接方法（使用结果模型）与重要性采样（使用行为策略）相结合。让我们从一个关键的恒等式开始。考虑以下期望，该期望是针对数据生成（行为）分布计算的，其中状态 $x$ 从 $p(x)$ 中采样，动作 $a$ 从行为策略 $b(a|x)$ 中采样：\n$$\\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] = \\mathbb{E}_{x \\sim p(x)} \\left[ \\sum_{a} b(a|x) \\frac{\\pi(a|x)}{b(a|x)} (\\mathbb{E}[R|x,a] - m(x,a)) \\right]$$\n这里，$R$ 是奖励随机变量，其条件期望是 $Q(x,a)$。简化求和：\n$$= \\mathbb{E}_{x \\sim p(x)} \\left[ \\sum_{a} \\pi(a|x) (Q(x,a) - m(x,a)) \\right]$$\n对于问题中给定的确定性策略 $\\pi$，当 $a=\\pi(x)$ 时 $\\pi(a|x)$ 为 $1$，否则为 $0$。因此，求和可以简化为单项：\n$$= \\mathbb{E}_{x \\sim p(x)} [Q(x, \\pi(x)) - m(x, \\pi(x))]$$\n认识到 $\\mathbb{E}_{x}[Q(x, \\pi(x))] = V(\\pi)$，我们有：\n$$\\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] = V(\\pi) - \\mathbb{E}_{x \\sim p(x)}[m(x, \\pi(x))]$$\n重新整理此方程以求解 $V(\\pi)$，我们得到一个表达式，其经验平均值将构成我们的估计量：\n$$V(\\pi) = \\mathbb{E}_{x, a \\sim b} \\left[ \\frac{\\pi(a|x)}{b(a|x)} (R - m(x,a)) \\right] + \\mathbb{E}_{x \\sim p(x)}[m(x, \\pi(x))]$$\n经验双重稳健估计量 $\\hat{V}_{DR}(\\pi)$ 是该表达式的样本均值近似，基于在行为策略 $b$ 下收集的 $n$ 个样本 $\\{(x_i, a_i, y_i)\\}_{i=1}^n$ 的数据集计算得出。令 $w_i = \\frac{\\pi(a_i|x_i)}{b(a_i|x_i)}$ 为第 $i$ 个样本的重要性权重。估计量为：\n$$\\hat{V}_{DR}(\\pi) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ w_i (y_i - m(x_i, a_i)) + m(x_i, \\pi(x_i)) \\right]$$\n该估计量是“双重稳健”的，因为如果结果模型是正确的（即 $m(x,a) = Q(x,a)$）或者用于计算权重 $w_i$ 的倾向性模型是正确的，它就能提供 $V(\\pi)$ 的无偏估计。\n\n现在，我们将为给定的包含 $n=5$ 个患者事件的数据集计算此估计量的值。\n目标策略 $\\pi$ 是：$\\pi(x=1) = \\mathrm{T1}$ 且 $\\pi(x=0) = \\mathrm{T0}$。\n结果模型 $m(x, a)$ 的预测值为：\n$m(1, \\mathrm{T1}) = 0.75$, $m(1, \\mathrm{T0}) = 0.55$。\n$m(0, \\mathrm{T0}) = 0.68$, $m(0, \\mathrm{T1}) = 0.52$。\n\n我们为每个事件 $i \\in \\{1, 2, 3, 4, 5\\}$ 计算求和内的项 $T_i = w_i (y_i - m(x_i, a_i)) + m(x_i, \\pi(x_i))$。\n\n-   **事件 $i=1$**：$(x_1=1, a_1=\\mathrm{T1}, y_1=0.80), w_1=\\frac{10}{7}$。\n    策略 $\\pi$ 对 $x_1=1$ 指定的动作为 $\\pi(1)=\\mathrm{T1}$。\n    $T_1 = w_1 (y_1 - m(x_1, a_1)) + m(x_1, \\pi(x_1)) = \\frac{10}{7} (0.80 - m(1, \\mathrm{T1})) + m(1, \\mathrm{T1})$\n    $T_1 = \\frac{10}{7} (0.80 - 0.75) + 0.75 = \\frac{10}{7} (0.05) + 0.75 = \\frac{0.5}{7} + 0.75 = \\frac{1}{14} + \\frac{3}{4} = \\frac{2+21}{28} = \\frac{23}{28}$。\n\n-   **事件 $i=2$**：$(x_2=1, a_2=\\mathrm{T0}, y_2=0.50), w_2=0$。\n    策略 $\\pi$ 对 $x_2=1$ 指定的动作为 $\\pi(1)=\\mathrm{T1}$。\n    $T_2 = w_2 (y_2 - m(x_2, a_2)) + m(x_2, \\pi(x_2)) = 0 \\cdot (0.50 - m(1, \\mathrm{T0})) + m(1, \\mathrm{T1})$\n    $T_2 = 0 + 0.75 = 0.75 = \\frac{3}{4}$。\n\n-   **事件 $i=3$**：$(x_3=0, a_3=\\mathrm{T0}, y_3=0.70), w_3=\\frac{5}{3}$。\n    策略 $\\pi$ 对 $x_3=0$ 指定的动作为 $\\pi(0)=\\mathrm{T0}$。\n    $T_3 = w_3 (y_3 - m(x_3, a_3)) + m(x_3, \\pi(x_3)) = \\frac{5}{3} (0.70 - m(0, \\mathrm{T0})) + m(0, \\mathrm{T0})$\n    $T_3 = \\frac{5}{3} (0.70 - 0.68) + 0.68 = \\frac{5}{3} (0.02) + 0.68 = \\frac{0.1}{3} + 0.68 = \\frac{1}{30} + \\frac{68}{100} = \\frac{1}{30} + \\frac{17}{25} = \\frac{5+102}{150} = \\frac{107}{150}$。\n\n-   **事件 $i=4$**：$(x_4=0, a_4=\\mathrm{T1}, y_4=0.40), w_4=0$。\n    策略 $\\pi$ 对 $x_4=0$ 指定的动作为 $\\pi(0)=\\mathrm{T0}$。\n    $T_4 = w_4 (y_4 - m(x_4, a_4)) + m(x_4, \\pi(x_4)) = 0 \\cdot (0.40 - m(0, \\mathrm{T1})) + m(0, \\mathrm{T0})$\n    $T_4 = 0 + 0.68 = \\frac{17}{25}$。\n\n-   **事件 $i=5$**：$(x_5=1, a_5=\\mathrm{T1}, y_5=0.60), w_5=\\frac{10}{7}$。\n    策略 $\\pi$ 对 $x_5=1$ 指定的动作为 $\\pi(1)=\\mathrm{T1}$。\n    $T_5 = w_5 (y_5 - m(x_5, a_5)) + m(x_5, \\pi(x_5)) = \\frac{10}{7} (0.60 - m(1, \\mathrm{T1})) + m(1, \\mathrm{T1})$\n    $T_5 = \\frac{10}{7} (0.60 - 0.75) + 0.75 = \\frac{10}{7} (-0.15) + 0.75 = \\frac{-1.5}{7} + 0.75 = -\\frac{3}{14} + \\frac{3}{4} = \\frac{-6+21}{28} = \\frac{15}{28}$。\n\n现在，我们将这些项相加：\n$\\sum_{i=1}^{5} T_i = \\frac{23}{28} + \\frac{3}{4} + \\frac{107}{150} + \\frac{17}{25} + \\frac{15}{28}$\n$\\sum_{i=1}^{5} T_i = \\left(\\frac{23}{28} + \\frac{15}{28}\\right) + \\frac{3}{4} + \\left(\\frac{107}{150} + \\frac{17 \\cdot 6}{25 \\cdot 6}\\right)$\n$\\sum_{i=1}^{5} T_i = \\frac{38}{28} + \\frac{3}{4} + \\left(\\frac{107}{150} + \\frac{102}{150}\\right)$\n$\\sum_{i=1}^{5} T_i = \\frac{19}{14} + \\frac{3}{4} + \\frac{209}{150}$\n为了将这些分数相加，我们找到一个公分母。$14=2 \\cdot 7$、$4=2^2$ 和 $150=2 \\cdot 3 \\cdot 5^2$ 的最小公倍数是 $2^2 \\cdot 3 \\cdot 5^2 \\cdot 7 = 4 \\cdot 3 \\cdot 25 \\cdot 7 = 2100$。\n$\\sum_{i=1}^{5} T_i = \\frac{19 \\cdot 150}{2100} + \\frac{3 \\cdot 525}{2100} + \\frac{209 \\cdot 14}{2100}$\n$\\sum_{i=1}^{5} T_i = \\frac{2850}{2100} + \\frac{1575}{2100} + \\frac{2926}{2100} = \\frac{2850 + 1575 + 2926}{2100} = \\frac{7351}{2100}$\n\n最后，我们通过将总和除以 $n=5$ 来计算估计量 $\\hat{V}_{DR}(\\pi)$：\n$\\hat{V}_{DR}(\\pi) = \\frac{1}{5} \\left( \\frac{7351}{2100} \\right) = \\frac{7351}{10500}$\n其小数值为 $0.700095238...$。\n四舍五入到四位有效数字，我们看第五位有效数字。由于 $9 \\ge 5$，我们将第四位数字向上取整。\n$\\hat{V}_{DR}(\\pi) \\approx 0.7001$。",
            "answer": "$$\\boxed{0.7001}$$"
        }
    ]
}