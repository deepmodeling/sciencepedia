{
    "hands_on_practices": [
        {
            "introduction": "The journey into processing clinical text begins with normalization, a set of techniques used to reduce textual variation and standardize input. However, in the high-stakes clinical domain, seemingly benign operations like lowercasing or removing punctuation can have dangerous consequences, altering dosages or confusing medical terms. This practice challenges you to think critically about these preprocessing steps, evaluating which normalization techniques are unsafe and why, building a foundational understanding of data cleaning in a medical context .",
            "id": "4841496",
            "problem": "A clinical Natural Language Processing (NLP) pipeline is being designed to preprocess free-text notes before downstream extraction of medication names and physiological units. Let a normalization function be any deterministic mapping $f:\\Sigma^* \\rightarrow \\Sigma^*$ from strings over an alphabet $\\Sigma$ to strings over the same alphabet that is intended to reduce superficial variability while preserving the underlying clinical semantics. A normalization operation is unsafe for clinical units and medication names if applying $f$ indiscriminately can transform a string into a different one that plausibly denotes a different entity, dose, or measurement than originally intended, or makes a true entity indistinguishable from a different concept. Consider the following normalization operations, formalized as deterministic functions:\n- Lowercasing: $f_{\\text{lower}}$ maps uppercase letters to their lowercase forms.\n- Punctuation stripping: $f_{\\text{punct}}$ removes all characters belonging to a designated punctuation set (for example, hyphens, slashes, periods).\n- Unicode normalization: $f_{\\text{unicode}}$ maps strings to a Unicode normalization form such as Normalization Form Canonical Composition (NFC) or Normalization Form Compatibility Composition (NFKC), using the Unicode Standard’s equivalence classes to ensure canonically equivalent strings are identical.\n- De-duplication: $f_{\\text{dedup}}$ collapses any maximal run of consecutive identical characters to a single instance of that character.\n\nThe pipeline must preserve critical distinctions needed to correctly identify entities such as International Unit (IU), milligram (mg), millimeters of mercury (mmHg), and brand or formulation cues (for example, Extended Release (ER) suffixes). Which of the following statements correctly identify normalization operations that are unsafe when applied indiscriminately to clinical units and medication names? Select all that apply.\n\nA. Lowercasing is unsafe because it collapses case-sensitive distinctions such as the analyte “Mg” (magnesium) into the unit “mg” (milligram), and may also blur formulation cues like “ER” in “metoprolol ER” by making them indistinguishable from unrelated lowercase tokens.\n\nB. Unconditional punctuation stripping is unsafe because it removes hyphens, slashes, and decimal points that encode meaning, as in “metoprolol-ER” (formulation), “NovoLog Mix $70/30$” (ratio), and “$0.5$ mg” (dose), thereby changing or obscuring the intended medication or unit.\n\nC. Unicode normalization to NFC or NFKC is universally safe for clinical units and medication names; it cannot change semantics because it only merges canonically or compatibly equivalent code points.\n\nD. Character de-duplication is unsafe because it alters units and abbreviations that rely on repeated letters, such as “mmHg” becoming “mHg” and “SSRI” becoming “SRI,” thereby changing or obscuring the intended unit or class.\n\nE. Unicode normalization that performs compatibility decomposition followed by aggressive American Standard Code for Information Interchange (ASCII) folding is unsafe because symbols such as the micro sign “µ” or Greek small letter mu “μ” in “μg” may be flattened to “ug,” increasing ambiguity and risking misinterpretation of microgram doses.\n\nChoose all that apply.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A normalization function is a deterministic mapping $f:\\Sigma^* \\rightarrow \\Sigma^*$.\n- An operation $f$ is defined as \"unsafe\" if its indiscriminate application can transform a string into one denoting a different entity, dose, or measurement, or make a true entity indistinguishable from a different concept.\n- The context is a clinical Natural Language Processing (NLP) pipeline for preprocessing free-text notes.\n- The pipeline must preserve distinctions for entities like International Unit (IU), milligram (mg), millimeters of mercury (mmHg), and medication formulation cues like Extended Release (ER).\n- The normalization operations to be evaluated are:\n    - Lowercasing: $f_{\\text{lower}}$\n    - Punctuation stripping: $f_{\\text{punct}}$\n    - Unicode normalization: $f_{\\text{unicode}}$ (to NFC or NFKC)\n    - De-duplication: $f_{\\text{dedup}}$ (collapses consecutive identical characters)\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the established fields of natural language processing and medical informatics. The described operations ($f_{\\text{lower}}$, $f_{\\text{punct}}$, etc.) are standard text preprocessing techniques. The clinical examples (Mg vs. mg, mmHg, medication names) are factually correct and represent real-world challenges in clinical text processing. The premise is scientifically sound.\n- **Well-Posed**: The problem provides clear definitions for a \"normalization function\" and an \"unsafe\" operation. It asks for an evaluation of given statements against these definitions. The task is specific and allows for a definite set of correct answers.\n- **Objective**: The problem is stated in precise, formal, and objective language. The functions are described deterministically. The examples are factual and not subject to opinion.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and contains sufficient information to proceed with a solution.\n\n### Analysis of Options\n\nThe task is to identify which statements correctly describe a normalization operation as unsafe according to the provided definition.\n\n**A. Lowercasing: $f_{\\text{lower}}$**\nThis statement claims that lowercasing is unsafe. It provides two examples:\n$1$. The symbol for the element magnesium is \"Mg\", while the symbol for the unit milligram is \"mg\". Applying the lowercasing function yields $f_{\\text{lower}}(\\text{\"Mg\"}) = \\text{\"mg\"}$. This makes the elemental analyte indistinguishable from the unit of mass, which is a critical semantic distinction in a clinical context (e.g., a lab result for serum magnesium versus a medication dosage).\n$2$. For a medication like \"metoprolol ER\", where \"ER\" signifies \"Extended Release\", $f_{\\text{lower}}(\\text{\"metoprolol ER\"}) = \\text{\"metoprolol er\"}$. While \"er\" might still be interpretable, the standardized, capitalized form \"ER\" is a stronger, less ambiguous signal for the drug formulation. The loss of case can increase ambiguity.\nThe first example, \"Mg\" vs. \"mg\", is a definitive case where applying $f_{\\text{lower}}$ makes a \"true entity indistinguishable from a different concept,\" perfectly matching the definition of an unsafe operation.\n*Verdict on A*: **Correct**.\n\n**B. Unconditional punctuation stripping: $f_{\\text{punct}}$**\nThis statement claims that unconditional punctuation stripping is unsafe. It provides several examples of meaningful punctuation:\n$1$. In \"metoprolol-ER\", the hyphen connects the drug name to its formulation. $f_{\\text{punct}}(\\text{\"metoprolol-ER\"}) = \\text{\"metoprololER\"}$. This may complicate tokenization and pattern matching.\n$2$. In \"NovoLog Mix $70/30$\", the slash separates the components of an insulin mixture ratio. $f_{\\text{punct}}(\\text{\"NovoLog Mix 70/30\"}) = \\text{\"NovoLog Mix 7030\"}$. This transforms a ratio into a four-digit number, completely obscuring the original meaning.\n$3$. In \"$0.5$ mg\", the period is a decimal point indicating a dose. $f_{\\text{punct}}(\\text{\"0.5 mg\"}) = \\text{\"05 mg\"}$. This changes the dose from one-half of a milligram to five milligrams, a $10$-fold increase, which could have catastrophic clinical consequences.\nThese examples, particularly the last two, demonstrate that $f_{\\text{punct}}$ can transform a string into one that \"denotes a different... dose, or measurement than originally intended.\"\n*Verdict on B*: **Correct**.\n\n**C. Unicode normalization: $f_{\\text{unicode}}$**\nThis statement claims that Unicode normalization to NFC or NFKC is \"universally safe\". To be universally safe, it must not cause any loss of semantic distinction in any plausible clinical scenario.\n- NFC (Normalization Form Canonical Composition) combines characters and their diacritics into precomposed forms (e.g., `e` + `´` $\\rightarrow$ `é`). This is generally considered semantically lossless, as it deals with canonical equivalences.\n- NFKC (Normalization Form Compatibility Composition) is more aggressive. It first applies compatibility decompositions. For instance, a ligature like `ﬁ` (U+FB01) would be decomposed to `f` and `i`. A superscript like `²` (U+00B2) would be mapped to the digit `2` (U+0032). The Roman numeral `Ⅳ` (U+2163) may be mapped to `IV`. These transformations do not preserve all semantic nuances. A character and its compatibility equivalent are, by definition of the Unicode standard, not necessarily interchangeable in all contexts. For example, changing a superscript in a chemical formula or a mathematical expression could alter its meaning.\nThe claim of \"universal safety\" for both NFC and NFKC is an exceptionally strong claim. Since NFKC is explicitly designed to break some semantic distinctions for the sake of compatibility (e.g., for searching or collation), it cannot be considered universally safe. The potential for NFKC to alter meaning makes this statement false.\n*Verdict on C*: **Incorrect**.\n\n**D. Character de-duplication: $f_{\\text{dedup}}$**\nThis statement claims that de-duplicating consecutive characters is unsafe. It provides two examples:\n$1$. The unit for blood pressure is \"mmHg\" (millimeters of mercury). The function $f_{\\text{dedup}}$ would transform this: $f_{\\text{dedup}}(\\text{\"mmHg\"}) = \\text{\"mHg\"}$. The resulting string \"mHg\" is not a standard representation and obscures the intended unit, which relies on the double \"m\".\n$2$. The acronym \"SSRI\" stands for Selective Serotonin Reuptake Inhibitor. The function $f_{\\text{dedup}}$ would transform this: $f_{\\text{dedup}}(\\text{\"SSRI\"}) = \\text{\"SRI\"}$. While \"SRI\" (Serotonin Reuptake Inhibitor) is a related concept, the transformation loses specificity.\nThe \"mmHg\" example is a clear case where a standard unit representation is corrupted, matching the definition of an unsafe operation.\n*Verdict on D*: **Correct**.\n\n**E. Unicode normalization with ASCII folding**\nThis statement claims that a process of compatibility decomposition followed by ASCII folding is unsafe.\nThe process is described as mapping symbols like the micro sign \"µ\" (U+00B5) or Greek small letter mu \"μ\" (U+03BC) to \"u\", resulting in \"μg\" (microgram) becoming \"ug\".\nThe abbreviation \"ug\" for microgram is on the \"List of Error-Prone Abbreviations, Symbols, and Dose Designations\" by the Institute for Safe Medication Practices (ISMP) precisely because it is easily mistaken for \"mg\" (milligram), which can lead to a $1000$-fold dosing error. The recommended, unambiguous abbreviation is \"mcg\".\nAn NLP operation that systematically converts a correct representation (\"μg\") into a known dangerous one (\"ug\") \"increase[s] ambiguity and risk[s] misinterpretation,\" making it unsafe by the problem's definition.\n*Verdict on E*: **Correct**.",
            "answer": "$$\\boxed{ABDE}$$"
        },
        {
            "introduction": "Once text is cleaned, the next step is to represent it in a way that a computer can understand and analyze. The Term Frequency-Inverse Document Frequency (TF-IDF) algorithm is a cornerstone of information retrieval and text mining, providing a numerical weight to each word that reflects its importance in a document relative to a collection of documents. This hands-on exercise will guide you through the calculation of TF-IDF from scratch, reinforcing how this powerful technique helps systems identify and rank relevant clinical documents .",
            "id": "4841475",
            "problem": "A clinical search module is being developed to index hospital progress notes for querying terms relevant to respiratory conditions. Consider a corpus of $N$ clinical documents where $N=5$, and the clinical term $t=$ \"pneumonia\" appears with raw counts across the documents given by the vector $\\left[2,0,1,0,1\\right]$, ordered by document identifiers $d=1,2,3,4,5$. The number of documents in which the term \"pneumonia\" appears at least once, also known as the document frequency, is $\\mathrm{df}=3$. Starting from the fundamental definitions of term frequency and inverse document frequency in information retrieval for natural language processing, treat term frequency as the raw count $c(t,d)$ and define inverse document frequency as the natural logarithm of the ratio of the corpus size to the document frequency. Using the standard multiplicative Term Frequency–Inverse Document Frequency (TF-IDF) weighting scheme, compute the TF-IDF value for each document and rank the documents from highest to lowest TF-IDF. Finally, determine the sum of the TF-IDF values across all $N$ documents and express your final answer as a single closed-form analytic expression. Report only the sum; no rounding is required and no units should be included.",
            "solution": "The problem is validated against the required criteria.\n\n**Step 1: Extract Givens**\n-   A corpus of $N$ clinical documents, where $N=5$.\n-   A clinical term, $t=$ \"pneumonia\".\n-   The raw counts of the term $t$ across documents $d=1,2,3,4,5$ are given by the vector $\\begin{pmatrix} 2  0  1  0  1 \\end{pmatrix}$.\n-   The document frequency of the term, $\\mathrm{df}$, is given as $\\mathrm{df}=3$.\n-   The definition of term frequency, $\\mathrm{tf}(t,d)$, is the raw count $c(t,d)$.\n-   The definition of inverse document frequency, $\\mathrm{idf}(t)$, is the natural logarithm of the ratio of the corpus size to the document frequency, i.e., $\\mathrm{idf}(t) = \\ln\\left(\\frac{N}{\\mathrm{df}}\\right)$.\n-   The weighting scheme is the standard multiplicative Term Frequency–Inverse Document Frequency (TF-IDF), i.e., $\\mathrm{tfidf}(t,d) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t)$.\n-   The final objective is to compute the sum of the TF-IDF values across all $N$ documents.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem is well-grounded in the fundamental principles of Natural Language Processing (NLP) and Information Retrieval (IR), which are core components of medical informatics. Term Frequency–Inverse Document Frequency (TF-IDF) is a standard and well-established statistical measure used to evaluate the importance of a word in a document in a collection or corpus. The provided definitions for TF (as raw count) and IDF (using the natural logarithm) are common and valid formulations.\n-   **Well-Posed and Consistent**: The problem is well-posed, providing all necessary data and definitions for a unique solution. The givens are internally consistent. The raw count vector $\\begin{pmatrix} 2  0  1  0  1 \\end{pmatrix}$ shows that the term \"pneumonia\" appears in documents $1$, $3$, and $5$. This means the term appears in $3$ documents. This matches the provided document frequency, $\\mathrm{df}=3$.\n-   **Objectivity**: The problem is stated in objective, precise language, free of any subjective or opinion-based claims.\n-   **Completeness and Feasibility**: The problem is self-contained and requires no external information. The scenario is a simplified but realistic application in clinical text processing.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be derived.\n\n**Solution Derivation**\n\nThe problem requires the calculation of the sum of TF-IDF scores for the term $t=$ \"pneumonia\" across a corpus of $N=5$ documents.\n\nFirst, we calculate the Inverse Document Frequency (IDF) for the term $t$. The problem defines IDF as:\n$$\n\\mathrm{idf}(t) = \\ln\\left(\\frac{N}{\\mathrm{df}}\\right)\n$$\nWe are given the total number of documents $N=5$ and the document frequency of the term $\\mathrm{df}=3$. Substituting these values into the formula:\n$$\n\\mathrm{idf}(t) = \\ln\\left(\\frac{5}{3}\\right)\n$$\nThis value is constant for the term $t$ across all documents in the corpus.\n\nNext, we calculate the Term Frequency (TF) for the term $t$ in each document $d$. The problem states that the term frequency is to be treated as the raw count, $c(t,d)$. The raw counts are provided as a vector for documents $d=1,2,3,4,5$:\n-   $c(t,1) = 2$\n-   $c(t,2) = 0$\n-   $c(t,3) = 1$\n-   $c(t,4) = 0$\n-   $c(t,5) = 1$\nTherefore, the TF values for each document are $\\mathrm{tf}(t,1)=2$, $\\mathrm{tf}(t,2)=0$, $\\mathrm{tf}(t,3)=1$, $\\mathrm{tf}(t,4)=0$, and $\\mathrm{tf}(t,5)=1$.\n\nNow, we compute the TF-IDF score for each document using the multiplicative definition, $\\mathrm{tfidf}(t,d) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t)$.\n-   For document $d=1$: $\\mathrm{tfidf}(t,1) = \\mathrm{tf}(t,1) \\times \\mathrm{idf}(t) = 2 \\times \\ln\\left(\\frac{5}{3}\\right) = 2 \\ln\\left(\\frac{5}{3}\\right)$\n-   For document $d=2$: $\\mathrm{tfidf}(t,2) = \\mathrm{tf}(t,2) \\times \\mathrm{idf}(t) = 0 \\times \\ln\\left(\\frac{5}{3}\\right) = 0$\n-   For document $d=3$: $\\mathrm{tfidf}(t,3) = \\mathrm{tf}(t,3) \\times \\mathrm{idf}(t) = 1 \\times \\ln\\left(\\frac{5}{3}\\right) = \\ln\\left(\\frac{5}{3}\\right)$\n-   For document $d=4$: $\\mathrm{tfidf}(t,4) = \\mathrm{tf}(t,4) \\times \\mathrm{idf}(t) = 0 \\times \\ln\\left(\\frac{5}{3}\\right) = 0$\n-   For document $d=5$: $\\mathrm{tfidf}(t,5) = \\mathrm{tf}(t,5) \\times \\mathrm{idf}(t) = 1 \\times \\ln\\left(\\frac{5}{3}\\right) = \\ln\\left(\\frac{5}{3}\\right)$\n\nThe problem also asks to rank the documents from highest to lowest TF-IDF. The value $\\ln(\\frac{5}{3})  0$ since $\\frac{5}{3}  1$. The ranking is therefore determined by the magnitude of the raw counts:\n-   Rank 1: Document $1$ (TF-IDF = $2 \\ln(\\frac{5}{3})$)\n-   Rank 2 (tie): Document $3$ and Document $5$ (TF-IDF = $\\ln(\\frac{5}{3})$)\n-   Rank 3 (tie): Document $2$ and Document $4$ (TF-IDF = $0$)\nThe ranking is $d_1  (d_3=d_5)  (d_2=d_4)$.\n\nFinally, we must determine the sum of the TF-IDF values across all $N$ documents. Let this sum be $S$.\n$$\nS = \\sum_{d=1}^{5} \\mathrm{tfidf}(t,d)\n$$\n$$\nS = \\mathrm{tfidf}(t,1) + \\mathrm{tfidf}(t,2) + \\mathrm{tfidf}(t,3) + \\mathrm{tfidf}(t,4) + \\mathrm{tfidf}(t,5)\n$$\nSubstituting the calculated values:\n$$\nS = 2 \\ln\\left(\\frac{5}{3}\\right) + 0 + \\ln\\left(\\frac{5}{3}\\right) + 0 + \\ln\\left(\\frac{5}{3}\\right)\n$$\nWe can factor out the constant $\\mathrm{idf}(t) = \\ln(\\frac{5}{3})$ term:\n$$\nS = \\left(2 + 0 + 1 + 0 + 1\\right) \\ln\\left(\\frac{5}{3}\\right)\n$$\nThe sum of the coefficients (which are the raw counts) is $2+0+1+0+1 = 4$.\nThus, the sum is:\n$$\nS = 4 \\ln\\left(\\frac{5}{3}\\right)\n$$\nThis is the required closed-form analytic expression for the sum.",
            "answer": "$$\\boxed{4 \\ln\\left(\\frac{5}{3}\\right)}$$"
        },
        {
            "introduction": "Building a clinical NLP model to extract information, such as identifying medical problems in a note, is only half the battle; we must also rigorously evaluate its performance. This final practice delves into the critical task of evaluation for Named Entity Recognition (NER), a core clinical NLP task. By calculating and contrasting token-level and span-level $F_1$-scores, you will uncover the subtle but crucial differences in evaluation methodologies and understand why a high score in one metric does not always guarantee practical, real-world utility .",
            "id": "4841429",
            "problem": "A clinical named entity recognition system is evaluated on a single sentence tokenized into $10$ tokens: $\\text{Patient}$ $(1)$, $\\text{has}$ $(2)$, $\\text{chest}$ $(3)$, $\\text{pain}$ $(4)$, $\\text{and}$ $(5)$, $\\text{shortness}$ $(6)$, $\\text{of}$ $(7)$, $\\text{breath}$ $(8)$, $\\text{denies}$ $(9)$, $\\text{fever}$ $(10)$. The gold labels use the Begin–Inside–Outside (BIO) scheme for the clinical problem entity type, where $\\text{B-P}$ denotes the beginning of a problem span, $\\text{I-P}$ denotes the inside of a problem span, and $\\text{O}$ denotes a token outside any problem span. The gold BIO tags for the tokens are:\n$(1)\\ \\text{O},\\ (2)\\ \\text{O},\\ (3)\\ \\text{B-P},\\ (4)\\ \\text{I-P},\\ (5)\\ \\text{O},\\ (6)\\ \\text{B-P},\\ (7)\\ \\text{I-P},\\ (8)\\ \\text{I-P},\\ (9)\\ \\text{O},\\ (10)\\ \\text{B-P}$.\nThe system’s predicted BIO tags are:\n$(1)\\ \\text{B-P},\\ (2)\\ \\text{I-P},\\ (3)\\ \\text{B-P},\\ (4)\\ \\text{I-P},\\ (5)\\ \\text{O},\\ (6)\\ \\text{B-P},\\ (7)\\ \\text{I-P},\\ (8)\\ \\text{O},\\ (9)\\ \\text{O},\\ (10)\\ \\text{B-P}$.\n\nAssume the following evaluation conventions grounded in standard information retrieval definitions:\n- Token-level evaluation treats BIO tag assignment as a single-label multi-class decision per token, and the token-level $F_{1}$-score is the micro-averaged harmonic mean of token-level precision and token-level recall across all BIO classes over all $10$ tokens.\n- Span-level evaluation treats each contiguous problem span as a unit, and a predicted span is counted as correct only if its boundaries and entity type exactly match a gold span. The span-level $F_{1}$-score is computed from span-level precision and span-level recall over all spans in this sentence.\n\nIt is known that $7$ tokens have their predicted BIO tag exactly matching the gold tag, while only $2$ predicted spans exactly match gold spans. Using only the definitions above as the fundamental base, compute:\n- the token-level micro-averaged $F_{1}$-score over the $10$ tokens, and\n- the span-level exact-match $F_{1}$-score over predicted and gold spans.\n\nRound both $F_{1}$-scores to four significant figures. Express the final answers as decimals without any units.",
            "solution": "The problem is evaluated and found to be valid. It is scientifically grounded in the principles of natural language processing evaluation, is well-posed with sufficient and consistent data, and is expressed in objective language. The provided summary statistics (\"$7$ tokens have their predicted BIO tag exactly matching the gold tag\" and \"$2$ predicted spans exactly match gold spans\") are consistent with the raw data, confirming the self-consistency of the problem statement. We may therefore proceed with the solution.\n\nThe problem asks for the computation of two distinct metrics: a token-level micro-averaged $F_{1}$-score and a span-level exact-match $F_{1}$-score. We will address each in turn.\n\n### Part 1: Token-Level Micro-Averaged $F_{1}$-Score\n\nThe evaluation is at the token level, where each of the $N=10$ tokens is assigned a single BIO tag from the set $\\{\\text{B-P, I-P, O}\\}$. This is a single-label multi-class classification task. The token-level $F_{1}$-score is defined as the micro-averaged harmonic mean of precision and recall.\n\nIn micro-averaging, we sum the true positives ($TP$), false positives ($FP$), and false negatives ($FN$) for each class before calculating the metrics. Let the set of classes be $C = \\{\\text{B-P, I-P, O}\\}$.\nThe micro-averaged precision ($P_{micro}$) is given by:\n$$ P_{micro} = \\frac{\\sum_{c \\in C} TP_c}{\\sum_{c \\in C} (TP_c + FP_c)} $$\nThe micro-averaged recall ($R_{micro}$) is given by:\n$$ R_{micro} = \\frac{\\sum_{c \\in C} TP_c}{\\sum_{c \\in C} (TP_c + FN_c)} $$\n\nFor a single-label classification task over a fixed number of items (tokens), the total number of predictions is equal to the total number of ground truth labels, which is the total number of tokens, $N$.\nThe denominator of $P_{micro}$, $\\sum_{c \\in C} (TP_c + FP_c)$, represents the total number of predictions made by the system, which is $N=10$.\nThe denominator of $R_{micro}$, $\\sum_{c \\in C} (TP_c + FN_c)$, represents the total number of gold standard labels, which is also $N=10$.\nThe numerator, $\\sum_{c \\in C} TP_c$, is the sum of correctly classified tokens across all classes. This is simply the total number of tokens for which the predicted tag matches the gold tag.\n\nThe problem statement explicitly provides this number: \"$7$ tokens have their predicted BIO tag exactly matching the gold tag\".\nLet $N_{correct} = 7$ be the number of correctly classified tokens and $N_{total} = 10$ be the total number of tokens.\n\nThus, we have:\n$$ P_{micro} = \\frac{N_{correct}}{N_{total}} = \\frac{7}{10} = 0.7 $$\n$$ R_{micro} = \\frac{N_{correct}}{N_{total}} = \\frac{7}{10} = 0.7 $$\n\nFor any multi-class classification problem where each item is assigned exactly one label, the micro-averaged precision, recall, and $F_{1}$-score are all equal to the overall accuracy.\nThe token-level micro-averaged $F_{1}$-score ($F_{1,token}$) is the harmonic mean of $P_{micro}$ and $R_{micro}$:\n$$ F_{1,token} = 2 \\times \\frac{P_{micro} \\times R_{micro}}{P_{micro} + R_{micro}} $$\nSince $P_{micro} = R_{micro}$, we have $F_{1,token} = P_{micro} = R_{micro}$.\n$$ F_{1,token} = 0.7 $$\nRounding to four significant figures as required, we get $0.7000$.\n\n### Part 2: Span-Level Exact-Match $F_{1}$-Score\n\nFor the span-level evaluation, we first identify the contiguous problem spans from both the gold and predicted BIO tags. A span is a sequence of tokens starting with a `B-P` tag, followed by zero or more `I-P` tags.\n\n**Gold Spans (Ground Truth):**\nThe gold tags are: O, O, B-P, I-P, O, B-P, I-P, I-P, O, B-P.\n- `chest pain`: Tokens $3$ (`B-P`) and $4$ (`I-P`).\n- `shortness of breath`: Tokens $6$ (`B-P`), $7$ (`I-P`), and $8$ (`I-P`).\n- `fever`: Token $10$ (`B-P`).\nSo, there are $3$ gold spans. Let $N_{gold} = 3$.\n\n**Predicted Spans (System Output):**\nThe predicted tags are: B-P, I-P, B-P, I-P, O, B-P, I-P, O, O, B-P.\n- `Patient has`: Tokens $1$ (`B-P`) and $2$ (`I-P`).\n- `chest pain`: Tokens $3$ (`B-P`) and $4$ (`I-P`).\n- `shortness of`: Tokens $6$ (`B-P`) and $7$ (`I-P`).\n- `fever`: Token $10$ (`B-P`).\nSo, there are $4$ predicted spans. Let $N_{predicted} = 4$.\n\nNow we calculate the number of true positives ($TP$), false positives ($FP$), and false negatives ($FN$) for the spans. A predicted span is a true positive if it matches a gold span exactly in terms of both boundaries and entity type.\n\n- **True Positives ($TP$)**: The number of predicted spans that exactly match a gold span.\n  - Predicted `chest pain` (tokens $3$-$4$) matches gold `chest pain` (tokens $3$-$4$). This is $1$ TP.\n  - Predicted `fever` (token $10$) matches gold `fever` (token $10$). This is a second TP.\n  - The other two predicted spans do not match any gold span exactly.\n  - Thus, $TP = 2$. This is consistent with the problem statement: \"$2$ predicted spans exactly match gold spans\".\n\n- **False Positives ($FP$)**: The number of predicted spans that are not true positives.\n  - $FP = N_{predicted} - TP = 4 - 2 = 2$.\n  - The false positive spans are `Patient has` (tokens $1$-$2$) and `shortness of` (tokens $6$-$7$).\n\n- **False Negatives ($FN$)**: The number of gold spans that are not matched by any predicted span.\n  - $FN = N_{gold} - TP = 3 - 2 = 1$.\n  - The false negative span is `shortness of breath` (tokens $6$-$8$). The system's prediction `shortness of` (tokens 6-7) is a partial match, but under the exact-match criterion, it does not count as a correct prediction, and the gold span is considered missed.\n\nWith these values, we can compute span-level precision ($P_{span}$) and recall ($R_{span}$).\n$$ P_{span} = \\frac{TP}{TP + FP} = \\frac{2}{2 + 2} = \\frac{2}{4} = \\frac{1}{2} = 0.5 $$\n$$ R_{span} = \\frac{TP}{TP + FN} = \\frac{2}{2 + 1} = \\frac{2}{3} $$\n\nThe span-level $F_{1}$-score ($F_{1,span}$) is the harmonic mean of $P_{span}$ and $R_{span}$:\n$$ F_{1,span} = 2 \\times \\frac{P_{span} \\times R_{span}}{P_{span} + R_{span}} = 2 \\times \\frac{\\frac{1}{2} \\times \\frac{2}{3}}{\\frac{1}{2} + \\frac{2}{3}} $$\n$$ F_{1,span} = 2 \\times \\frac{\\frac{1}{3}}{\\frac{3}{6} + \\frac{4}{6}} = 2 \\times \\frac{\\frac{1}{3}}{\\frac{7}{6}} = 2 \\times \\frac{1}{3} \\times \\frac{6}{7} = \\frac{4}{7} $$\nTo express this as a decimal rounded to four significant figures:\n$$ F_{1,span} = \\frac{4}{7} \\approx 0.57142857... $$\nRounding gives $0.5714$.\n\nThe two computed values are:\n- Token-level micro-averaged $F_{1}$-score: $0.7000$\n- Span-level exact-match $F_{1}$-score: $0.5714$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0.7000  0.5714 \\end{pmatrix} } $$"
        }
    ]
}