{
    "hands_on_practices": [
        {
            "introduction": "放射组学（Radiomics）从医学图像中提取定量特征，但这些特征并非空中楼阁，而是直接源于体素（voxel）数据。本练习旨在揭开这一过程的神秘面纱，重点关注纹理分析的基石——灰度共生矩阵（Grey-Level Co-occurrence Matrix, GLCM）。通过为一个小的、假设性的三维区域手动计算 GLCM 及其派生特征（对比度 $C$ 和熵 $H$），您将对图像纹理如何被量化获得具体而深刻的理解 。",
            "id": "4834547",
            "problem": "一个三维感兴趣区域 (ROI) 在一个由整数坐标 $(x,y,z)$ 索引的体素网格上被离散化为三个灰度级 $\\{1,2,3\\}$，其中 $x \\in \\{1,2\\}$，$y \\in \\{1,2\\}$，$z \\in \\{1,2\\}$。该 ROI 由两个轴向切片（$z=1$ 和 $z=2$）组成，每个切片大小为 $2 \\times 2$，其灰度级如下（行对应于 $y$ 的增加，列对应于 $x$ 的增加）：\n$$\nz=1:\\quad \\begin{bmatrix} 1  2 \\\\ 2  1 \\end{bmatrix}, \\qquad\nz=2:\\quad \\begin{bmatrix} 2  3 \\\\ 1  2 \\end{bmatrix}.\n$$\n使用放射组学中灰度共生矩阵 (GLCM) 的标准定义，通过计算沿 13 个唯一的三维方向（根据“第一个非零分量为正”的规则获得）的共生次数，为此 ROI 构建一个距离 $d=1$ 的聚合对称 GLCM。具体来说，方向集是\n$$\n\\{(1,0,0),(0,1,0),(0,0,1),(1,1,0),(1,-1,0),(1,0,1),(1,0,-1),(0,1,1),(0,1,-1),(1,1,1),(1,1,-1),(1,-1,1),(1,-1,-1)\\}.\n$$\n对于 ROI 内沿上述任一方向 $\\mathbf{v}$ 且距离 $d=1$ 的每个体素对 $(\\mathbf{p},\\mathbf{p}+\\mathbf{v})$，设 $i$ 为 $\\mathbf{p}$ 处的灰度级，$j$ 为 $\\mathbf{p}+\\mathbf{v}$ 处的灰度级。对于每个这样的体素对，将 $(i,j)$ 和 $(j,i)$ 的计数都加 1，以形成对称的 GLCM。在累积了所有方向的计数后，通过除以总计数来归一化 GLCM，得到概率 $P(i,j)$，使得 $\\sum_{i}\\sum_{j} P(i,j)=1$。\n\n仅使用 GLCM 的核心定义和特征公式，计算：\n- 对比度\n$$\nC = \\sum_{i=1}^{3}\\sum_{j=1}^{3} (i-j)^{2}\\, P(i,j),\n$$\n- 和熵\n$$\nH = -\\sum_{i=1}^{3}\\sum_{j=1}^{3} P(i,j)\\, \\ln\\!\\big(P(i,j)\\big),\n$$\n其中 $\\ln$ 表示自然对数，且根据连续性，当 $P(i,j)=0$ 时，该项的贡献为 $0$。\n\n将您的最终答案表示为一个无单位的行向量 $\\big(C\\ \\ H\\big)$，并将两个值四舍五入到四位有效数字。",
            "solution": "该问题要求我们为一个给定的三维感兴趣区域 (ROI) 计算对比度和熵这两个放射组学特征。该过程涉及构建一个灰度共生矩阵 (GLCM)，对其进行归一化，然后应用特征定义。\n\n### 步骤 1：问题验证\n\n首先根据所需标准对问题进行验证。\n\n*   **已知条件提取**：\n    *   ROI：一个 $2 \\times 2 \\times 2$ 的体素网格，坐标为 $(x,y,z)$，其中 $x,y,z \\in \\{1,2\\}$。\n    *   灰度级：$G = \\{1, 2, 3\\}$。\n    *   ROI 数据：设 $V(x,y,z)$ 为坐标 $(x,y,z)$ 处的灰度级。\n        *   切片 $z=1$：$V(1,1,1)=1$, $V(2,1,1)=2$, $V(1,2,1)=2$, $V(2,2,1)=1$。\n        *   切片 $z=2$：$V(1,1,2)=2$, $V(2,1,2)=3$, $V(1,2,2)=1$, $V(2,2,2)=2$。\n    *   GLCM 参数：\n        *   距离 $d=1$ 通过指定的 13 个方向向量 $\\mathbf{v}$ 集合来解释：\n        $\\{(1,0,0),(0,1,0),(0,0,1),(1,1,0),(1,-1,0),(1,0,1),(1,0,-1),(0,1,1),(0,1,-1),(1,1,1),(1,1,-1),(1,-1,1),(1,-1,-1)\\}$.\n        *   对称化：对于找到的每个共生对 $(i,j)$，GLCM 中 $(i,j)$ 和 $(j,i)$ 的计数都增加 1。\n        *   归一化：最终的 GLCM 除以其所有条目的总和。\n    *   特征公式：\n        *   对比度：$C = \\sum_{i=1}^{3}\\sum_{j=1}^{3} (i-j)^{2}\\, P(i,j)$\n        *   熵：$H = -\\sum_{i=1}^{3}\\sum_{j=1}^{3} P(i,j)\\, \\ln\\!\\big(P(i,j)\\big)$\n\n*   **验证结论**：此问题是**有效的**。它在科学上基于放射组学的原理，问题提出得很好，提供了所有必要信息和明确的指令，并且其表述是客观的。该设置是一个标准的、尽管规模较小的纹理分析计算。\n\n### 步骤 2：共生矩阵的构建\n\n我们系统地遍历 ROI 中的 8 个体素 $\\mathbf{p}$ 和 13 个方向向量 $\\mathbf{v}$。对于每对 $(\\mathbf{p}, \\mathbf{v})$，我们检查相邻体素 $\\mathbf{q} = \\mathbf{p} + \\mathbf{v}$ 是否在 ROI 边界内。如果是，我们记录灰度级对 $(i,j) = (V(\\mathbf{p}), V(\\mathbf{q}))$。\n\n这 8 个体素及其灰度级为：\n$V(1,1,1)=1, V(2,1,1)=2, V(1,2,1)=2, V(2,2,1)=1$\n$V(1,1,2)=2, V(2,1,2)=3, V(1,2,2)=1, V(2,2,2)=2$\n\n为每个方向找到的共生对是：\n*   $\\mathbf{v}=(1,0,0)$: $(1,2), (2,1), (2,3), (1,2)$\n*   $\\mathbf{v}=(0,1,0)$: $(1,2), (2,1), (2,1), (3,2)$\n*   $\\mathbf{v}=(0,0,1)$: $(1,2), (2,3), (2,1), (1,2)$\n*   $\\mathbf{v}=(1,1,0)$: $(1,1), (2,2)$\n*   $\\mathbf{v}=(1,-1,0)$: $(2,2), (1,3)$\n*   $\\mathbf{v}=(1,0,1)$: $(1,3), (2,2)$\n*   $\\mathbf{v}=(1,0,-1)$: $(2,2), (1,1)$\n*   $\\mathbf{v}=(0,1,1)$: $(1,1), (2,2)$\n*   $\\mathbf{v}=(0,1,-1)$: $(2,2), (3,1)$\n*   $\\mathbf{v}=(1,1,1)$: $(1,2)$\n*   $\\mathbf{v}=(1,1,-1)$: $(2,1)$\n*   $\\mathbf{v}=(1,-1,1)$: $(2,3)$\n*   $\\mathbf{v}=(1,-1,-1)$: $(1,2)$\n\n总共找到了 28 个有效的共生对。统计这些对可以得到非对称矩阵 $M$ 的计数，其中 $M_{ij}$ 是对 $(i,j)$ 出现的次数：\n*   $(1,1)$ 的计数：$3$\n*   $(1,2)$ 的计数：$7$\n*   $(1,3)$ 的计数：$2$\n*   $(2,1)$ 的计数：$5$\n*   $(2,2)$ 的计数：$6$\n*   $(2,3)$ 的计数：$3$\n*   $(3,1)$ 的计数：$1$\n*   $(3,2)$ 的计数：$1$\n*   $(3,3)$ 的计数：$0$\n\n这给出了矩阵 $M$：\n$$\nM = \\begin{pmatrix} 3  7  2 \\\\ 5  6  3 \\\\ 1  1  0 \\end{pmatrix}\n$$\n问题说明，对于找到的每一对，我们都将 $(i,j)$ 和 $(j,i)$ 的计数加一。这等同于通过将 $M$ 与其转置 $M^T$ 相加来创建一个对称矩阵 $S$。\n$$\nS = M + M^T = \\begin{pmatrix} 3  7  2 \\\\ 5  6  3 \\\\ 1  1  0 \\end{pmatrix} + \\begin{pmatrix} 3  5  1 \\\\ 7  6  1 \\\\ 2  3  0 \\end{pmatrix} = \\begin{pmatrix} 6  12  3 \\\\ 12  12  4 \\\\ 3  4  0 \\end{pmatrix}\n$$\n$S$ 中的总计数 $N_{total}$ 是其所有元素的和：\n$N_{total} = 6+12+3+12+12+4+3+4+0 = 56$。这正如预期的那样，是共生对数量（28）的 2 倍。\n\n### 步骤 3：归一化与特征计算\n\n归一化 GLCM $P$ 是通过将 $S$ 的每个元素除以 $N_{total}=56$ 得到的：\n$$\nP = \\frac{1}{56} S = \\frac{1}{56} \\begin{pmatrix} 6  12  3 \\\\ 12  12  4 \\\\ 3  4  0 \\end{pmatrix} = \\begin{pmatrix} 6/56  12/56  3/56 \\\\ 12/56  12/56  4/56 \\\\ 3/56  4/56  0 \\end{pmatrix}\n$$\n\n**对比度 (C)**\n对比度使用公式 $C = \\sum_{i,j} (i-j)^2 P(i,j)$ 计算。\n$$\nC = (1-1)^2 P(1,1) + (1-2)^2 P(1,2) + (1-3)^2 P(1,3) + \\dots\n$$\n我们可以根据 $(i-j)^2$ 的值对各项进行分组：\n*   对于对角线元素，$(i-j)^2 = 0$。\n*   对于 $P(1,2), P(2,1), P(2,3), P(3,2)$，$(i-j)^2 = 1$。\n*   对于 $P(1,3), P(3,1)$，$(i-j)^2 = 4$。\n\n$C = 1^2 \\cdot \\left(P(1,2) + P(2,1) + P(2,3) + P(3,2)\\right) + 2^2 \\cdot \\left(P(1,3) + P(3,1)\\right)$\n$C = 1 \\cdot \\left(\\frac{12}{56} + \\frac{12}{56} + \\frac{4}{56} + \\frac{4}{56}\\right) + 4 \\cdot \\left(\\frac{3}{56} + \\frac{3}{56}\\right)$\n$C = \\frac{32}{56} + 4 \\cdot \\frac{6}{56} = \\frac{32}{56} + \\frac{24}{56} = \\frac{56}{56} = 1$\n所以，对比度 $C$ 精确为 $1$。\n\n**熵 (H)**\n熵使用公式 $H = -\\sum_{i,j} P(i,j) \\ln(P(i,j))$ 计算，约定 $0 \\ln(0) = 0$。\n矩阵 $P$ 中的非零概率为：\n$P(1,1) = 6/56$\n$P(1,2) = P(2,1) = P(2,2) = 12/56$\n$P(1,3) = P(3,1) = 3/56$\n$P(2,3) = P(3,2) = 4/56$\n\n$H = - \\left[ P(1,1)\\ln P(1,1) + 3 \\cdot P(1,2)\\ln P(1,2) + 2 \\cdot P(1,3)\\ln P(1,3) + 2 \\cdot P(2,3)\\ln P(2,3) \\right]$\n$H = - \\left[ \\frac{6}{56}\\ln\\left(\\frac{6}{56}\\right) + 3\\frac{12}{56}\\ln\\left(\\frac{12}{56}\\right) + 2\\frac{3}{56}\\ln\\left(\\frac{3}{56}\\right) + 2\\frac{4}{56}\\ln\\left(\\frac{4}{56}\\right) \\right]$\n$H = - \\frac{1}{56} \\left[ 6\\ln\\left(\\frac{6}{56}\\right) + 36\\ln\\left(\\frac{12}{56}\\right) + 6\\ln\\left(\\frac{3}{56}\\right) + 8\\ln\\left(\\frac{4}{56}\\right) \\right]$\n使用属性 $\\ln(a/b) = \\ln(a)-\\ln(b)$，并合并各项：\n$H = - \\frac{1}{56} [ (6\\ln 6 + 36\\ln 12 + 6\\ln 3 + 8\\ln 4) - (6+36+6+8)\\ln 56 ]$\n$H = - \\frac{1}{56} [ (6\\ln 6 + 36\\ln 12 + 6\\ln 3 + 8\\ln 4) - 56\\ln 56 ]$\n$H = \\ln(56) - \\frac{1}{56} (6\\ln 6 + 36\\ln 12 + 6\\ln 3 + 8\\ln 4)$\n计算数值：\n$\\ln(56) \\approx 4.02535$\n第二项：\n$\\frac{1}{56} (6\\ln 6 + 36\\ln 12 + 6\\ln 3 + 8\\ln 4) \\approx \\frac{1}{56}(6(1.79176) + 36(2.48491) + 6(1.09861) + 8(1.38629))$\n$\\approx \\frac{1}{56}(10.75056 + 89.45676 + 6.59166 + 11.09032) \\approx \\frac{117.8893}{56} \\approx 2.10517$\n$H \\approx 4.02535 - 2.10517 \\approx 1.92018$\n\n### 步骤 4：最终答案\n\n问题要求将结果四舍五入到四位有效数字。\n$C = 1.000$\n$H \\approx 1.920$\n\n最终答案表示为行向量 $(C \\ \\ H)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.000  1.920\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在训练用于医学图像分割的深度学习模型时，一个主要障碍是严重的类别不平衡问题，即目标病灶只占图像的一小部分。在这种情况下，标准的损失函数很容易被海量的背景区域所主导，从而无法有效学习目标特征。焦点损失（Focal Loss）函数通过动态地重新加权每个体素的损失贡献来解决这一问题，它迫使模型专注于“难”的、被错误分类的样本，而不是“易”的、已正确分类的样本。本练习将探讨焦点损失的工作机制，特别是其聚焦参数 $\\gamma$ 如何调节这种侧重程度 ，理解这一机制对于在真实的医学应用中有效训练鲁棒的分割模型至关重要。",
            "id": "4834610",
            "problem": "在一项关于磁共振成像（MRI）的体素级病灶分割任务中，一个卷积神经网络（CNN）为每个体素输出一个预测概率 $p$，表示该体素属于病灶类。令 $y \\in \\{0,1\\}$ 表示真实标签，并定义 $p_t$ 为真实类别的预测概率，即如果 $y=1$，则 $p_t = p$；如果 $y=0$，则 $p_t = 1-p$。单个体素的标准二元交叉熵为 $-\\log p_t$。为了解决严重的类别不平衡问题并进行难例挖掘，一种广泛使用的改进方法引入了可调聚焦参数 $\\gamma \\ge 0$ 和类别权重 $\\alpha \\in (0,1)$，通过一个因子来缩放交叉熵。当 $p_t \\to 1$ 时，该因子减小；当 $p_t \\to 0$ 时，该因子保持较高水平，从而降低分类良好体素的权重，并强调分类错误体素的权重。考虑两个病灶体素（因此它们共享相同的类别权重 $\\alpha$）：体素 $v_1$ 的 $p_t = 0.2$（难例），体素 $v_2$ 的 $p_t = 0.95$（易例）。假设对数为自然对数。关于在这个修改后的损失函数中，增加 $\\gamma$ 对分类错误体素相对强调程度的影响，以下哪些陈述是正确的？选择所有适用项。\n\nA. 当 $\\gamma = 2$ 时，乘以交叉熵的调制因子对于 $p_t = 0.2$ 的情况比对于 $p_t = 0.95$ 的情况要大。\n\nB. 当 $\\gamma = 0$ 时，修改后的损失函数简化为权重为 $\\alpha$ 的类别加权交叉熵，并且不存在依赖于 $p_t$ 的聚焦效应。\n\nC. 增加 $\\gamma$ 会以相同的乘法常数减少易例和难例体素的损失贡献，从而保持它们的比率不变。\n\nD. 对于任何固定的 $\\gamma > 0$，难例体素的损失与易例体素的损失之比与 $\\alpha$ 无关，并且随着 $\\gamma$ 的增加而增加。",
            "solution": "用户希望我验证并解决一个关于医学图像分割中使用的修改版交叉熵损失函数性质的问题。\n\n### 问题验证\n\n**步骤1：提取已知信息**\n\n*   **任务：** 磁共振成像（MRI）上的体素级病灶分割。\n*   **模型输出：** 每个体素属于病灶类的预测概率 $p$。\n*   **真实标签：** $y \\in \\{0, 1\\}$。\n*   **真实类别的预测概率：** $p_t$ 定义为，如果 $y=1$，则 $p_t = p$；如果 $y=0$，则 $p_t = 1-p$。\n*   **标准二元交叉熵（BCE）：** 对于单个体素，损失为 $-\\log p_t$。\n*   **修改后的损失函数：** 标准BCE被修改以解决类别不平衡问题并进行难例挖掘。它引入了聚焦参数 $\\gamma \\ge 0$ 和类别权重 $\\alpha \\in (0,1)$。交叉熵通过一个因子进行缩放，该因子在 $p_t \\to 1$ 时减小，在 $p_t \\to 0$ 时保持较高水平。\n*   **体素数据：** 考虑两个病灶体素，意味着它们的 $y=1$。它们共享相同的类别权重 $\\alpha$。\n    *   体素 $v_1$（难例）：$p_t = 0.2$。我们将其表示为 $p_{t,1}$。\n    *   体素 $v_2$（易例）：$p_t = 0.95$。我们将其表示为 $p_{t,2}$。\n*   **对数类型：** 使用自然对数。\n\n**步骤2：使用提取的已知信息进行验证**\n\n1.  **科学依据：** 该问题描述了Focal Loss，这是深度学习中一个开创性且广泛使用的损失函数，尤其适用于物体检测和分割等存在严重类别不平衡的任务。它在医学成像，特别是MRI病灶分割中的应用，是一个标准且活跃的研究领域。所有概念——交叉熵、类别不平衡、难例挖掘、CNN——都是机器学习和医学信息学领域的基础。该问题具有科学合理性。\n2.  **定义明确：** 问题清晰地描述了修改后的损失函数。“通过一个因子来缩放交叉熵，当 $p_t \\to 1$ 时，该因子减小；当 $p_t \\to 0$ 时，该因子保持较高水平” 这句话明确指向Focal Loss中使用的调制因子 $(1-p_t)^\\gamma$。结合类别权重 $\\alpha$，损失函数得到了明确的定义。给定的 $p_t$ 和参数 $\\gamma$ 的具体数值使得进行具体的数学分析成为可能。问题精确，并允许一组唯一的正确答案。\n3.  **客观性：** 问题使用精确、客观和技术性的术语进行陈述。它没有歧义、主观性或观点性。\n\n**步骤3：结论与行动**\n\n问题陈述是有效的。它科学合理、定义明确且客观。我将继续进行解答。\n\n### 解题推导\n\n问题描述了一个用于二元分类的修改版损失函数，它是Focal Loss的一个变体。单个样本的标准二元交叉熵是 $CE(p_t) = -\\log(p_t)$。\n\n问题陈述了两个修改点：\n1.  引入了类别权重 $\\alpha \\in (0,1)$。由于体素属于病灶类，其权重为 $\\alpha$。这得到了 $\\alpha$ 加权的交叉熵：$-\\alpha \\log(p_t)$。\n2.  引入了一个调制因子来缩放交叉熵。该因子依赖于聚焦参数 $\\gamma \\ge 0$，并被描述为当 $p_t \\to 1$ 时减小，当 $p_t \\to 0$ 时保持较高水平。这唯一定义了 $(1-p_t)^\\gamma$ 这一项。\n\n综合这些，单个体素的修改后损失函数 $L$ 为：\n$$L(p_t) = -\\alpha (1-p_t)^\\gamma \\log(p_t)$$\n这适用于病灶类。我们已知两个病灶体素，因此它们的类别权重都是 $\\alpha$。\n\n这两个体素是：\n*   难例体素 $v_1$：$p_{t,1} = 0.2$。其损失为 $L_1 = -\\alpha (1 - 0.2)^\\gamma \\log(0.2) = -\\alpha (0.8)^\\gamma \\log(0.2)$。\n*   易例体素 $v_2$：$p_{t,2} = 0.95$。其损失为 $L_2 = -\\alpha (1 - 0.95)^\\gamma \\log(0.95) = -\\alpha (0.05)^\\gamma \\log(0.95)$。\n\n我们现在来分析每个陈述。\n\n**A. 当 $\\gamma = 2$ 时，乘以交叉熵的调制因子对于 $p_t = 0.2$ 的情况比对于 $p_t = 0.95$ 的情况要大。**\n\n标准交叉熵是 $-\\log(p_t)$。完整的缩放因子是 $\\alpha (1-p_t)^\\gamma$。“调制因子” 指的是负责聚焦效应的部分，即 $(1-p_t)^\\gamma$。让我们将这个因子表示为 $M(p_t) = (1-p_t)^\\gamma$。我们需要比较当 $\\gamma = 2$ 时 $M(p_{t,1})$ 和 $M(p_{t,2})$ 的大小。\n\n对于 $p_{t,1} = 0.2$ 的难例体素 $v_1$：\n$$M(0.2) = (1 - 0.2)^2 = (0.8)^2 = 0.64$$\n\n对于 $p_{t,2} = 0.95$ 的易例体素 $v_2$：\n$$M(0.95) = (1 - 0.95)^2 = (0.05)^2 = 0.0025$$\n\n比较这两个值，我们发现 $0.64 > 0.0025$。因此，对于 $p_t = 0.2$ 的情况，调制因子比对于 $p_t = 0.95$ 的情况要大。这展示了Focal Loss所期望的行为：它降低了分类良好（易例）样本的贡献。\n\n**A的结论：正确**\n\n**B. 当 $\\gamma = 0$ 时，修改后的损失函数简化为权重为 $\\alpha$ 的类别加权交叉熵，并且不存在依赖于 $p_t$ 的聚焦效应。**\n\n让我们将 $\\gamma = 0$ 代入损失函数 $L(p_t) = -\\alpha (1-p_t)^\\gamma \\log(p_t)$。\n对于任何 $p_t \\in [0,1)$，我们有 $1-p_t > 0$，所以 $(1-p_t)^0 = 1$。（对于边缘情况 $p_t=1$，无论如何损失都为0）。\n损失函数变为：\n$$L(p_t) = -\\alpha (1-p_t)^0 \\log(p_t) = -\\alpha (1) \\log(p_t) = -\\alpha \\log(p_t)$$\n这正是对于病灶类，权重为 $\\alpha$ 的类别加权交叉熵的定义。\n\n“聚焦效应”是由调制因子 $(1-p_t)^\\gamma$ 产生的。当 $\\gamma=0$ 时，该因子变为 $1$，这是一个常数，不再依赖于 $p_t$。因此，它不会根据样本分类的好坏来选择性地缩放损失。该类的所有样本都由相同的常数 $\\alpha$ 缩放。聚焦效应不存在。\n\n**B的结论：正确**\n\n**C. 增加 $\\gamma$ 会以相同的乘法常数减少易例和难例体素的损失贡献，从而保持它们的比率不变。**\n\n让我们分析增加 $\\gamma$ 对损失 $L_1$ 和 $L_2$ 的影响。\n任何体素的损失为 $L(\\gamma) = C \\cdot b^\\gamma$，其中 $C = -\\alpha \\log(p_t)$ 且 $b = 1-p_t$。对于我们的两个体素，$p_t \\in (0,1)$，所以 $b \\in (0,1)$ 且 $C > 0$。随着 $\\gamma$ 的增加，$b^\\gamma$ 减小，因此损失 $L$ 减小。因此，增加 $\\gamma$ 确实会减少两个体素的损失贡献。\n\n现在，我们来检查减少量是否是相同的乘法常数。假设 $\\gamma$ 增加到 $\\gamma' = \\gamma + \\Delta\\gamma$，其中 $\\Delta\\gamma > 0$。\n难例体素的新损失是 $L_1' = L_1(\\gamma+\\Delta\\gamma) = -\\alpha (0.8)^{\\gamma+\\Delta\\gamma} \\log(0.2) = L_1(\\gamma) \\cdot (0.8)^{\\Delta\\gamma}$。\n易例体素的新损失是 $L_2' = L_2(\\gamma+\\Delta\\gamma) = -\\alpha (0.05)^{\\gamma+\\Delta\\gamma} \\log(0.95) = L_2(\\gamma) \\cdot (0.05)^{\\Delta\\gamma}$。\n\n减少的乘法常数分别为 $(0.8)^{\\Delta\\gamma}$ 和 $(0.05)^{\\Delta\\gamma}$。由于 $0.8 \\neq 0.05$，这些常数不相同。\n\n此外，我们来考察损失的比率：\n$$ \\frac{L_1}{L_2} = \\frac{-\\alpha (0.8)^\\gamma \\log(0.2)}{-\\alpha (0.05)^\\gamma \\log(0.95)} = \\left(\\frac{0.8}{0.05}\\right)^\\gamma \\frac{\\log(0.2)}{\\log(0.95)} = (16)^\\gamma \\frac{\\log(0.2)}{\\log(0.95)} $$\n由于底数 $16$ 被提升到 $\\gamma$ 次方，该比率显然是 $\\gamma$ 的函数，并且当 $\\gamma$ 改变时不会保持不变。因此，该陈述在两个方面都是错误的。\n\n**C的结论：不正确**\n\n**D. 对于任何固定的 $\\gamma > 0$，难例体素的损失与易例体素的损失之比与 $\\alpha$ 无关，并且随着 $\\gamma$ 的增加而增加。**\n\n这个陈述包含两个部分。\n第一部分：该比率与 $\\alpha$ 无关。\n让我们计算难例体素损失（$L_1$）与易例体素损失（$L_2$）的比率：\n$$ R_\\gamma = \\frac{L_1}{L_2} = \\frac{-\\alpha (1-p_{t,1})^\\gamma \\log(p_{t,1})}{-\\alpha (1-p_{t,2})^\\gamma \\log(p_{t,2})} = \\frac{(1-p_{t,1})^\\gamma \\log(p_{t,1})}{(1-p_{t,2})^\\gamma \\log(p_{t,2})} $$\n类别权重 $\\alpha$ 在分子和分母中被消去。因此，该比率确实与 $\\alpha$ 无关。\n\n第二部分：该比率随着 $\\gamma$ 的增加而增加。\n让我们将比率 $R_\\gamma$ 作为 $\\gamma$ 的函数进行分析：\n$$ R_\\gamma = \\left(\\frac{1-p_{t,1}}{1-p_{t,2}}\\right)^\\gamma \\left(\\frac{\\log(p_{t,1})}{\\log(p_{t,2})}\\right) $$\n代入数值 $p_{t,1}=0.2$ 和 $p_{t,2}=0.95$：\n$$ R_\\gamma = \\left(\\frac{1-0.2}{1-0.95}\\right)^\\gamma \\left(\\frac{\\log(0.2)}{\\log(0.95)}\\right) = \\left(\\frac{0.8}{0.05}\\right)^\\gamma \\left(\\frac{\\log(0.2)}{\\log(0.95)}\\right) = (16)^\\gamma \\left(\\frac{\\log(0.2)}{\\log(0.95)}\\right) $$\n令 $C = \\frac{\\log(0.2)}{\\log(0.95)}$。由于 $p_t \\in (0,1)$，$\\log(p_t)$ 是负数。所以，$C$ 是两个负数之比，因此是正数。\n该比率为 $R_\\gamma = C \\cdot (16)^\\gamma$。\n要确定 $R_\\gamma$ 是否随 $\\gamma$ 增加而增加，我们可以考察它关于 $\\gamma$ 的导数：\n$$ \\frac{dR_\\gamma}{d\\gamma} = C \\cdot \\frac{d}{d\\gamma}(16^\\gamma) = C \\cdot 16^\\gamma \\log(16) $$\n由于 $C>0$，对于任何实数 $\\gamma$ 都有 $16^\\gamma>0$，并且 $\\log(16)>0$，因此导数 $\\frac{dR_\\gamma}{d\\gamma}$ 严格为正。这意味着 $R_\\gamma$ 是 $\\gamma$ 的一个严格递增函数。这就是“聚焦”特性的数学基础：增加 $\\gamma$ 会增加难例损失与易例损失的比率，从而将更多的相对权重放在纠正难例上。\n\n陈述的两个部分都为真。\n\n**D的结论：正确**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "设计一个强大的三维深度学习模型（如3D U-Net）只是成功的一半，另一半则是确保它能够在现有硬件上进行训练，而图形处理单元（GPU）的显存（VRAM）通常是最关键的瓶颈。本练习将引导您完成一项切实的工程任务：估算训练一个3D U-Net所需的显存。这包括计算模型参数、它们的梯度，以及最关键的——在前向传播过程中存储的中间激活值所消耗的内存。通过完成这项计算 ，您将对三维模型的巨大资源需求建立起直观认识，并学会识别内存消耗的主要来源，这对于在实际硬件约束下设计和部署深度学习解决方案是不可或缺的技能。",
            "id": "4834626",
            "problem": "一个由放射组学驱动的用于计算机断层扫描 (CT) 的三维肿瘤分割流程，使用一个三维 U-Net 从一个大小为 $256 \\times 256 \\times 256$ 体素的单通道体积感兴趣区域 (ROI) 生成一个二值掩码。其架构规定如下：编码器有 $5$ 个分辨率级别，每个级别有两个三维卷积，使用 $3 \\times 3 \\times 3$ 的卷积核、步幅为 $1$ 以及保持空间维度的填充。输入层的基础特征通道数为 $C_0 = 32$，并且在每个下采样步骤后通道数翻倍：$\\{32, 64, 128, 256, 512\\}$。级别之间的下采样通过大小为 $2 \\times 2 \\times 2$ 且步幅为 $2$ 的最大池化来执行。解码器与编码器对称：每个级别以一个三维转置卷积（卷积核大小 $2 \\times 2 \\times 2$，步幅 $2$）开始，以进行上采样并将通道数减半，然后与编码器的跳跃连接进行拼接，再接两个三维卷积，其卷积核大小为 $3 \\times 3 \\times 3$，步幅为 $1$，并使用保持空间维度的填充。最终的输出层是一个 $1 \\times 1 \\times 1$ 的卷积，将 $32$ 个通道映射到 $1$ 个输出通道。所有的激活值和参数都以单精度浮点（$32$位）格式存储，每个元素占用 $4$ 字节。训练使用不带动量的随机梯度下降 (SGD)：存储权重及其梯度，忽略优化器的辅助状态。假设所有中间卷积输出都被保留，直到在反向传播中计算出它们的梯度，并且在基础估算中忽略框架工作区和内核启动开销。\n\n使用张量所需内存等于其元素数量乘以每元素字节数的基本定义，以及对于一个空间网格大小为 $D \\times H \\times W$、输入通道为 $C_{\\mathrm{in}}$、输出通道为 $C_{\\mathrm{out}}$、卷积核大小为 $k \\times k \\times k$ 的三维卷积，其浮点运算次数等于 $2 \\cdot D \\cdot H \\cdot W \\cdot C_{\\mathrm{in}} \\cdot C_{\\mathrm{out}} \\cdot k^3$，执行以下操作：\n\n1.  从第一性原理出发，推导一个在训练期间对于批量大小 $B$ 所需的激活内存的通用表达式，该表达式是编码器和解码器中所有卷积层以及最终输出层输出的总和。结果以吉字节 (GB) 为单位表示，使用 $1\\,\\mathrm{GB} = 10^9$ 字节的约定。\n2.  在当前架构规范下，数值计算（不四舍五入）批量大小 $B = 1$ 时的总激活内存。然后计算参数内存和参数梯度内存，并将它们相加，以获得以吉字节为单位的基础视频随机存取存储器 (VRAM) 估算总量。\n3.  一个图形处理器 (GPU) 拥有 $12$ GB 的 VRAM。为了考虑基础估算中未包含的运行时开销（框架工作区、CUDA cudnn 缓冲区、非模型分配），在与硬件限制比较之前，将基础 VRAM 估算值乘以一个保守的 $1.1$ 的裕量因子。确定在此裕量下能适应 $12$ GB VRAM 限制的最大整数批量大小 $B_{\\max}$。提供 $B_{\\max}$ 作为你的最终答案。\n4.  简要论证至少两种可以在不减小输入空间大小的情况下，将训练适应于 $12$ GB VRAM 限制的策略。\n\nVRAM 数量以吉字节表示。中间内存估算无需四舍五入。最终答案必须是单个整数 $B_{\\max}$。",
            "solution": "在尝试任何解决方案之前，先对问题进行验证。\n\n### 步骤 1: 提取已知条件\n- **模型**：一个用于肿瘤分割的三维 U-Net。\n- **输入**：一个大小为 $256 \\times 256 \\times 256$ 体素的单通道体积感兴趣区域 (ROI)。\n- **数据精度**：单精度浮点（32位），每个元素占用 $4$ 字节。\n- **编码器架构**：\n    - $5$ 个分辨率级别。\n    - 每个级别有 $2$ 个三维卷积。\n    - 卷积核大小：$3 \\times 3 \\times 3$，步幅 $1$，带保持空间维度的填充。\n    - 基础特征通道数：$C_0 = 32$。\n    - 通道数递增：$\\{32, 64, 128, 256, 512\\}$。\n    - 下采样：大小为 $2 \\times 2 \\times 2$ 且步幅为 $2$ 的最大池化。\n- **解码器架构**：\n    - 与编码器结构对称。\n    - 上采样：使用卷积核大小为 $2 \\times 2 \\times 2$、步幅为 $2$ 的三维转置卷积，通道数减半。\n    - 随后与编码器的跳跃连接进行拼接。\n    - 随后是 $2$ 个三维卷积，卷积核大小为 $3 \\times 3 \\times 3$，步幅 $1$，带保持空间维度的填充。\n- **最终层**：一个 $1 \\times 1 \\times 1$ 的卷积，将 $32$ 个通道映射到 $1$ 个输出通道。\n- **训练内存假设**：\n    - 批量大小用 $B$ 表示。\n    - 训练使用不带动量的随机梯度下降 (SGD)。\n    - 内存存储权重及其梯度。优化器辅助状态被忽略。\n    - 所有中间卷积输出都被保留以用于反向传播。\n    - 框架开销在基础估算中被忽略。\n- **定义**：\n    - 张量内存 = (元素数量) $\\times$ (每元素字节数)。\n    - $1\\,\\mathrm{GB} = 10^9$ 字节。\n- **硬件与约束**：\n    - GPU 显存 (VRAM)：$12$ GB。\n    - 裕量因子：$1.1$。\n- **任务**：\n    1. 推导批量大小为 $B$ 的激活内存的通用表达式。\n    2. 计算 $B = 1$ 时的基础 VRAM 总量。\n    3. 确定最大整数批量大小 $B_{\\max}$。\n    4. 论证两种在 VRAM 限制内进行训练的策略。\n\n### 步骤 2: 使用提取的已知条件进行验证\n该问题具有科学依据，描述了一种标准的深度学习架构（3D U-Net）和一个常见的工程任务（计算内存需求）。输入维度和通道数等参数对于医学成像应用是合理的。这是一个定义明确的问题，提供了进行所需计算的所有必要定义、常数和架构细节。语言客观且正式。它不违反任何科学原则，不基于错误的前提，也不包含任何矛盾。所需的计算并非微不足道，代表了部署深度学习模型时的一个现实挑战。\n\n### 步骤 3: 结论与行动\n问题被认为是**有效的**。将提供完整的解决方案。\n\n### 解题推导\n\n我们首先为 U-Net 架构定义符号。\n设 $i \\in \\{0, 1, 2, 3, 4\\}$ 为编码器中分辨率级别的索引。\n级别 $i$ 的空间维度为 $S_i$。输入大小为 $S_0 = 256$。通过步幅为 2 的最大池化，$S_i = \\frac{S_0}{2^i} = \\frac{256}{2^i}$。\n级别 $i$ 的输出通道数为 $C_i$。基础通道数为 $C_0=32$，每个级别通道数翻倍，因此 $C_i = C_0 \\cdot 2^i = 32 \\cdot 2^i$。\n单个元素（32位浮点数）的内存为 $M_{elem} = 4$ 字节。\n激活内存是反向传播所需的所有中间层输出所需的存储空间。\n\n**1. 激活内存的通用表达式**\n\n批量大小为 $B$ 的总激活内存 $M_{act}(B)$ 是编码器、解码器和最终层激活内存的总和。\n\n**编码器激活内存 ($M_{enc}$)**\n在每个级别 $i \\in \\{0, 1, 2, 3, 4\\}$，有两个卷积层。由于填充保持了空间维度，两个层都产生大小为 $B \\times S_i \\times S_i \\times S_i \\times C_i$ 的输出张量。\n级别 $i$ 的两个卷积输出的内存为 $2 \\cdot B \\cdot S_i^3 \\cdot C_i \\cdot M_{elem}$。\n编码器总激活内存是所有 $5$ 个级别的总和：\n$$M_{enc}(B) = \\sum_{i=0}^{4} 2 \\cdot B \\cdot S_i^3 \\cdot C_i \\cdot M_{elem}$$\n\n**解码器激活内存 ($M_{dec}$)**\n解码器有 $4$ 个级别，对应于编码器级别 $i \\in \\{0, 1, 2, 3\\}$。在每个级别，它执行一次上采样（转置卷积）、一次拼接和两次常规卷积。我们必须存储每个卷积操作的输出。\n在解码器级别 $j \\in \\{0, 1, 2, 3\\}$，过程如下：\n1.  通过转置卷积进行上采样：输出张量的维度为 $B \\times S_j^3 \\times C_j$。内存：$B \\cdot S_j^3 \\cdot C_j \\cdot M_{elem}$。\n2.  与来自编码器级别 $j$ 的跳跃连接（其维度为 $B \\times S_j^3 \\times C_j$）进行拼接。\n3.  随后的两个卷积都产生维度为 $B \\times S_j^3 \\times C_j$ 的输出张量。两者的内存为：$2 \\cdot B \\cdot S_j^3 \\cdot C_j \\cdot M_{elem}$。\n因此，对于 $4$ 个解码器级别中的每一个，我们存储一个转置卷积和两个常规卷积的输出，它们的大小均为 $B \\times S_j^3 \\times C_j$。\n每个解码器级别 $j$ 的总内存为 $3 \\cdot B \\cdot S_j^3 \\cdot C_j \\cdot M_{elem}$。\n解码器总激活内存是这 $4$ 个级别的总和：\n$$M_{dec}(B) = \\sum_{j=0}^{3} 3 \\cdot B \\cdot S_j^3 \\cdot C_j \\cdot M_{elem}$$\n\n**最终层激活内存 ($M_{final}$)**\n最终的 $1 \\times 1 \\times 1$ 卷积接收最后一个解码器级别的输出（大小为 $B \\times S_0^3 \\times C_0$）并将其映射到单个通道。输出张量的维度为 $B \\times S_0^3 \\times 1$。\n$$M_{final}(B) = B \\cdot S_0^3 \\cdot 1 \\cdot M_{elem}$$\n\n**总激活内存 ($M_{act}$)**\n$$M_{act}(B) = M_{enc}(B) + M_{dec}(B) + M_{final}(B)$$\n$$M_{act}(B) = \\left( \\sum_{i=0}^{4} 2 S_i^3 C_i \\right) B \\cdot M_{elem} + \\left( \\sum_{j=0}^{3} 3 S_j^3 C_j \\right) B \\cdot M_{elem} + (S_0^3) B \\cdot M_{elem}$$\n我们可以合并公共索引 $i, j \\in \\{0, 1, 2, 3\\}$ 上的求和。\n$$M_{act}(B) = B \\cdot M_{elem} \\left( \\sum_{i=0}^{3} (2 S_i^3 C_i + 3 S_i^3 C_i) + 2 S_4^3 C_4 + S_0^3 \\right)$$\n$$M_{act}(B) = B \\cdot M_{elem} \\left( \\sum_{i=0}^{3} 5 S_i^3 C_i + 2 S_4^3 C_4 + S_0^3 \\right)$$\n我们代入 $S_i = S_0 / 2^i$ 和 $C_i = C_0 \\cdot 2^i$。乘积 $S_i^3 C_i = (S_0/2^i)^3 (C_0 \\cdot 2^i) = (S_0^3 C_0) / 4^i$。\n$$M_{act}(B) = B \\cdot M_{elem} \\left( 5 S_0^3 C_0 \\sum_{i=0}^{3} \\frac{1}{4^i} + 2 \\frac{S_0^3 C_0}{4^4} + S_0^3 \\right)$$\n等比数列的和为 $\\sum_{i=0}^{3} (1/4)^i = \\frac{1-(1/4)^4}{1-1/4} = \\frac{1-1/256}{3/4} = \\frac{255/256}{3/4} = \\frac{85}{64}$。\n$$M_{act}(B) = B \\cdot M_{elem} \\cdot S_0^3 \\left( 5 C_0 \\frac{85}{64} + \\frac{2 C_0}{256} + 1 \\right)$$\n代入 $S_0 = 256$, $C_0 = 32$, $M_{elem} = 4$ 字节，并转换为 GB ($1\\,\\mathrm{GB} = 10^9$ 字节):\n$$M_{act}(B) = \\frac{B \\cdot 4 \\cdot 256^3}{10^9} \\left( 5 \\cdot 32 \\cdot \\frac{85}{64} + \\frac{2 \\cdot 32}{256} + 1 \\right)$$\n$$M_{act}(B) = \\frac{B \\cdot 4 \\cdot 16777216}{10^9} \\left( \\frac{5 \\cdot 32 \\cdot 85}{64} + \\frac{64}{256} + 1 \\right) = \\frac{B \\cdot 67108864}{10^9} \\left( 212.5 + 0.25 + 1 \\right)$$\n$$M_{act}(B) = \\frac{B \\cdot 67108864 \\cdot 213.75}{10^9} = B \\cdot \\frac{14344519680}{10^9} = B \\cdot 14.34451968 \\,\\mathrm{GB}$$\n\n**2. B=1 时的基础 VRAM 总量**\n\n对于 $B=1$，激活内存为 $M_{act}(1) = 14.34451968\\,\\mathrm{GB}$。\n\n接下来，我们计算参数 ($M_{params}$) 及其梯度 ($M_{grads}$) 的内存。\n$M_{total}(B) = M_{act}(B) + M_{params} + M_{grads}$。由于使用不带动量的 SGD，因此 $M_{grads} = M_{params}$。\n一个卷积层（包括偏置）的参数数量为 $N = C_{in} \\cdot C_{out} \\cdot k_D \\cdot k_H \\cdot k_W + C_{out}$。\n\n**参数计算：**\n- **编码器 ($k=3$):**\n    - L0: $(1 \\cdot 32 \\cdot 3^3 + 32) + (32 \\cdot 32 \\cdot 3^3 + 32) = 28576$\n    - L1: $(32 \\cdot 64 \\cdot 3^3 + 64) + (64 \\cdot 64 \\cdot 3^3 + 64) = 166016$\n    - L2: $(64 \\cdot 128 \\cdot 3^3 + 128) + (128 \\cdot 128 \\cdot 3^3 + 128) = 663808$\n    - L3: $(128 \\cdot 256 \\cdot 3^3 + 256) + (256 \\cdot 256 \\cdot 3^3 + 256) = 2654720$\n    - L4: $(256 \\cdot 512 \\cdot 3^3 + 512) + (512 \\cdot 512 \\cdot 3^3 + 512) = 10617856$\n    - $N_{enc} = 14130976$ 个参数。\n- **解码器：**\n    - L3: 转置卷积 ($k=2$, $C_{in}=512, C_{out}=256$): $512 \\cdot 256 \\cdot 2^3 + 256 = 1048832$。卷积1 ($k=3$, $C_{in}=512, C_{out}=256$): $512 \\cdot 256 \\cdot 3^3 + 256 = 3539200$。卷积2 ($k=3$, $C_{in}=256, C_{out}=256$): $256 \\cdot 256 \\cdot 3^3 + 256 = 1769728$。总计: $6357760$。\n    - L2: ($C_{up,in}=256, C_{out}=128$), ($C_{c1,in}=256, C_{out}=128$), ($C_{c2,in}=128, C_{out}=128$)。总计: $262272 + 884864 + 442496 = 1589632$。\n    - L1: ($C_{up,in}=128, C_{out}=64$), ($C_{c1,in}=128, C_{out}=64$), ($C_{c2,in}=64, C_{out}=64$)。总计: $65600 + 221248 + 110656 = 397504$。\n    - L0: ($C_{up,in}=64, C_{out}=32$), ($C_{c1,in}=64, C_{out}=32$), ($C_{c2,in}=32, C_{out}=32$)。总计: $16416 + 55328 + 27680 = 99424$。\n    - $N_{dec} = 8444320$ 个参数。\n- **最终层：** $1 \\times 1 \\times 1$ 卷积 ($C_{in}=32, C_{out}=1$): $32 \\cdot 1 \\cdot 1^3 + 1 = 33$ 个参数。\n- **总参数：** $N_{total} = N_{enc} + N_{dec} + N_{final} = 14130976 + 8444320 + 33 = 22575329$。\n- **参数和梯度内存：**\n    $M_{params} + M_{grads} = 2 \\cdot N_{total} \\cdot M_{elem} = 2 \\cdot 22575329 \\cdot 4 = 180602632$ 字节。\n    以 GB 为单位: $180602632 / 10^9 = 0.180602632\\,\\mathrm{GB}$。\n- **B=1 时的基础 VRAM 总量：**\n    $M_{total}(1) = M_{act}(1) + (M_{params} + M_{grads}) / 10^9 = 14.34451968 + 0.180602632 = 14.525122312\\,\\mathrm{GB}$。\n\n**3. 最大整数批量大小 ($B_{\\max}$)**\n\n总分配内存必须在 GPU 的 VRAM 限制内，包括 $1.1$ 的裕量因子。\n设 $V_{lim} = 12\\,\\mathrm{GB} = 12 \\times 10^9$ 字节。\n约束条件为：$1.1 \\cdot M_{total}(B) \\le V_{lim}$。\n$$1.1 \\cdot (B \\cdot M_{act, B=1} + (M_{params} + M_{grads})) \\le 12 \\times 10^9$$\n$$1.1 \\cdot (B \\cdot 14344519680 + 180602632) \\le 12 \\times 10^9$$\n$$B \\cdot 14344519680 + 180602632 \\le \\frac{12 \\times 10^9}{1.1} \\approx 10909090909.09$$\n$$B \\cdot 14344519680 \\le 10909090909.09 - 180602632 = 10728488277.09$$\n$$B \\le \\frac{10728488277.09}{14344519680} \\approx 0.7479$$\n最大整数批量大小 $B$ 必须小于或等于 $0.7479$。因此，最大整数批量大小为 $B_{\\max} = 0$。这表明在给定条件下，使用指定的硬件，批量大小为 1 或更大时训练是不可行的。\n\n**4. VRAM 缩减策略**\n\n为了在不减小输入空间大小的情况下将训练过程适应 $12$ GB VRAM 限制，可以采用多种策略。以下是两种：\n\n1.  **混合精度训练**：问题指定所有张量都使用单精度（32位）浮点数。通过切换到混合精度训练，该训练主要使用半精度（16位）浮点数（例如，`FP16`），激活值、参数和梯度的内存占用可以减少近一半。由于激活内存是 VRAM 的主要消耗者（本例中超过98%），这种减少将是巨大的。批量大小为 $B=1$ 的内存将减少到约 $14.53 / 2 \\approx 7.27$ GB，即使有 $1.1 \\times$ 的裕量（$1.1 \\cdot 7.27 \\approx 7.99$ GB），也能够轻松地适应 12 GB 的限制。\n\n2.  **梯度检查点（激活重计算）**：VRAM 的主要成本是存储前向传播中的所有中间激活值，以供反向传播使用。梯度检查点是一种以计算成本换取内存的技术。它不存储所有激活值，而是只保存一个策略性选择的子集（检查点）。在反向传播期间，未存储的用于梯度计算的激活值会从最近的检查点即时重新计算。这可以显著减少激活内存需求，通常从与层数成线性关系 $O(L)$ 减少到大约 $O(\\sqrt{L})$，代价是一次额外的前向传播。考虑到这个3D模型中激活值极高的内存消耗，这种方法将非常有效。d",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}