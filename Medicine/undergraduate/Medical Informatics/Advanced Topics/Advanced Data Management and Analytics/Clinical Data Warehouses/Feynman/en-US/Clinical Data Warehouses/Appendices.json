{
    "hands_on_practices": [
        {
            "introduction": "Modern Clinical Data Warehouses (CDWs) are increasingly built on cloud platforms, offering scalability and flexibility. However, this power comes with a responsibility to manage operational costs effectively. This first practice  provides a foundational exercise in cloud cost estimation, breaking down the total expense into its core components: data storage and computational processing. By working through this realistic scenario, you will develop a practical understanding of how resource consumption translates directly into financial costs, a vital skill for anyone involved in managing or planning healthcare data systems.",
            "id": "4826390",
            "problem": "A hospital maintains a Clinical Data Warehouse (CDW) that stores longitudinal electronic health record data and derived analytics artifacts in a public cloud. The CDW uses a linear unit-cost model, where the monthly storage charge is proportional to the provisioned data volume and the compute charge for Extract-Transform-Load (ETL) pipelines is proportional to cumulative node-hours. One node-hour is defined as one compute node running for one hour; cumulative usage sums over all nodes and durations.\n\nAssume the following for a single calendar month:\n- Provisioned storage volume is $200$ terabytes (TB).\n- The cloud provider’s storage price is $\\$23$ per TB-month.\n- ETL runs on a $10$-node cluster, and the cloud provider’s compute price is $\\$2$ per node-hour.\n- The ETL workload consumes a total of $300$ node-hours over the month.\n\nUsing the linear cost framework implied by unit pricing, determine the total monthly operating cost of the CDW by summing the storage and compute components. Express your answer in United States Dollars (USD). No rounding is required; provide the exact value.",
            "solution": "The problem statement is evaluated for validity prior to attempting a solution.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Provisioned storage volume: $200$ terabytes (TB).\n- Cloud provider’s storage price: $\\$23$ per TB-month.\n- ETL runs on a: $10$-node cluster.\n- Cloud provider’s compute price: $\\$2$ per node-hour.\n- ETL workload consumption: $300$ node-hours over the month.\n- Cost model: Linear unit-cost.\n- Objective: Determine the total monthly operating cost in United States Dollars (USD).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required criteria for validity.\n- **Scientifically Grounded:** The problem is based on standard, realistic cost models for public cloud infrastructure services. The concepts of per-unit pricing for storage (dollars per terabyte-month) and compute (dollars per node-hour) are fundamental principles in cloud economics. The application context, a Clinical Data Warehouse (CDW), is a typical use case for such services. The numerical values provided are plausible.\n- **Well-Posed:** The problem is self-contained, providing all necessary data and definitions to calculate the total cost. The objective is clearly stated. The data are internally consistent. The cluster size ($10$ nodes) is extraneous information, as the total billing metric, cumulative node-hours ($300$), is explicitly given. This does not constitute a contradiction but rather a slight over-specification that does not impede a unique solution.\n- **Objective:** The problem is phrased using clear, precise, and unbiased language. It employs standard industry terminology, leaving no room for subjective interpretation.\n\nThe problem does not exhibit any flaws that would render it invalid. It is scientifically sound, well-posed, and objective.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be constructed.\n\n**Solution Derivation**\n\nThe problem asks for the total monthly operating cost of the Clinical Data Warehouse (CDW). According to the specified linear unit-cost model, the total cost $C_{\\text{total}}$ is the sum of the storage cost, $C_{\\text{storage}}$, and the compute cost, $C_{\\text{compute}}$.\n$$\nC_{\\text{total}} = C_{\\text{storage}} + C_{\\text{compute}}\n$$\nWe will calculate each component separately.\n\nFirst, we determine the monthly storage cost, $C_{\\text{storage}}$. This cost is the product of the provisioned storage volume and the unit storage price.\nLet $V$ be the provisioned storage volume and $P_{\\text{storage}}$ be the unit price for storage.\nThe given values are:\n- $V = 200 \\text{ TB}$\n- $P_{\\text{storage}} = 23 \\text{ USD per TB-month}$\n\nThe storage cost is calculated as:\n$$\nC_{\\text{storage}} = V \\cdot P_{\\text{storage}}\n$$\nSubstituting the numerical values:\n$$\nC_{\\text{storage}} = 200 \\text{ TB} \\times 23 \\frac{\\text{USD}}{\\text{TB-month}} = 4600 \\text{ USD}\n$$\n\nNext, we determine the monthly compute cost for the Extract-Transform-Load (ETL) pipelines, $C_{\\text{compute}}$. This cost is the product of the total compute consumption in node-hours and the unit price per node-hour.\nLet $U_{\\text{compute}}$ be the total compute usage and $P_{\\text{compute}}$ be the unit price for compute.\nThe given values are:\n- $U_{\\text{compute}} = 300 \\text{ node-hours}$\n- $P_{\\text{compute}} = 2 \\text{ USD per node-hour}$\n\nThe compute cost is calculated as:\n$$\nC_{\\text{compute}} = U_{\\text{compute}} \\cdot P_{\\text{compute}}\n$$\nSubstituting the numerical values:\n$$\nC_{\\text{compute}} = 300 \\text{ node-hours} \\times 2 \\frac{\\text{USD}}{\\text{node-hour}} = 600 \\text{ USD}\n$$\nThe information that the ETL runs on a $10$-node cluster is consistent with the total consumption but is not required for this calculation, as the cumulative billing unit (node-hours) is already provided.\n\nFinally, the total monthly operating cost, $C_{\\text{total}}$, is the sum of the storage and compute costs.\n$$\nC_{\\text{total}} = C_{\\text{storage}} + C_{\\text{compute}} = 4600 \\text{ USD} + 600 \\text{ USD}\n$$\n$$\nC_{\\text{total}} = 5200 \\text{ USD}\n$$\nThe problem requires the exact value in United States Dollars. The total cost is $5200$ USD.",
            "answer": "$$\n\\boxed{5200}\n$$"
        },
        {
            "introduction": "A patient's medical history is a story that unfolds over time, and a key function of a CDW is to capture this story accurately. This practice  delves into a core data warehousing technique known as Slowly Changing Dimensions (SCD) Type 2, which is used to maintain a full history of changes to data. You will step into the shoes of a data analyst to determine a patient's insurance status on a specific date, learning how to navigate historical records and resolve data conflicts that arise in real-world systems.",
            "id": "4826408",
            "problem": "A Clinical Data Warehouse (CDW) maintains a Slowly Changing Dimension Type 2 (SCD Type 2) dimension for payer insurance coverage. Each row represents a version of a patient’s insurance attributes, with business-effective validity captured by a half-open interval $\\left[\\text{effective\\_start}, \\text{effective\\_end}\\right)$, where $\\text{effective\\_end}$ may be open-ended. The Clinical Data Warehouse enforces homogeneity of natural keys and aims for non-overlapping business-valid intervals per natural key, but real-world ingestion defects may introduce overlapping intervals that must be reconciled during “as-of” queries.\n\nFundamental facts you may assume:\n- In SCD Type 2, the business-valid interval is modeled as $\\left[\\text{effective\\_start}, \\text{effective\\_end}\\right)$, meaning $\\text{effective\\_start} \\leq t < \\text{effective\\_end}$ indicates validity at time $t$.\n- Open-ended validity is represented by $\\text{effective\\_end} = +\\infty$.\n- The “as-of” operator returns the single row that is business-valid at time $t$.\n\nConsider one patient with natural key $k = 98765$. The dimension has the following rows (each bullet is one row):\n- Surrogate key $s = 101$, version $v = 1$, $\\text{effective\\_start} = 2023\\text{-}01\\text{-}01$, $\\text{effective\\_end} = 2023\\text{-}12\\text{-}31$, plan name “Medicaid”, plan code $3$, $\\text{etl\\_load\\_ts} = 2023\\text{-}01\\text{-}02\\ \\text{T}\\ 04{:}00$.\n- Surrogate key $s = 102$, version $v = 2$, $\\text{effective\\_start} = 2024\\text{-}01\\text{-}01$, $\\text{effective\\_end} = +\\infty$, plan name “Commercial A”, plan code $1$, $\\text{etl\\_load\\_ts} = 2024\\text{-}01\\text{-}02\\ \\text{T}\\ 05{:}00$.\n- Surrogate key $s = 135$, version $v = 3$, $\\text{effective\\_start} = 2024\\text{-}03\\text{-}15$, $\\text{effective\\_end} = +\\infty$, plan name “Commercial A”, plan code $1$, $\\text{etl\\_load\\_ts} = 2024\\text{-}03\\text{-}16\\ \\text{T}\\ 05{:}52$.\n\nLet the as-of time be $t = 2024\\text{-}06\\text{-}15$. Define the requested output as a single integer code $c^{\\ast}$ that equals the plan code of the patient’s insurance in force at time $t$. If no row is valid at time $t$, define $c^{\\ast} = 0$.\n\nTasks:\n- Using only the foundational SCD Type 2 semantics stated above, determine $c^{\\ast}$ at $t = 2024\\text{-}06\\text{-}15$.\n- In your reasoning, explicitly justify how overlapping intervals are handled in a principled, deterministic manner grounded in SCD Type 2 semantics and data governance, and show that your approach yields a unique $c^{\\ast}$ for this data.\n\nProvide the final value of $c^{\\ast}$ as a single integer. No rounding is needed, and no units are required.",
            "solution": "The problem is deemed valid. It is a well-posed problem within the domain of data warehousing and medical informatics. It is scientifically grounded in the established principles of Slowly Changing Dimensions (SCD), is internally consistent, and contains the necessary information to derive a unique, deterministic solution. The core task involves resolving an ambiguity—overlapping business-valid time intervals—by applying a standard data governance principle, which is a common and practical challenge in this field.\n\nThe problem requires determining the correct insurance plan code, denoted as $c^{\\ast}$, for a patient with natural key $k = 98765$ at the specific \"as-of\" time $t = 2024\\text{-}06\\text{-}15$. The business validity of each record is defined by the half-open interval $[\\text{effective\\_start}, \\text{effective\\_end})$. A record is considered valid at time $t$ if the condition $\\text{effective\\_start} \\leq t < \\text{effective\\_end}$ is met.\n\nWe are given three records for the patient with natural key $k = 98765$:\n\\begin{enumerate}\n    \\item Row $1$: surrogate key $s_1 = 101$, version $v_1 = 1$. Interval $[\\text{start}_1, \\text{end}_1) = [2023\\text{-}01\\text{-}01, 2023\\text{-}12\\text{-}31)$. Plan code $c_1 = 3$. ETL timestamp $\\text{ts}_1 = 2023\\text{-}01\\text{-}02\\ \\text{T}\\ 04{:}00$.\n    \\item Row $2$: surrogate key $s_2 = 102$, version $v_2 = 2$. Interval $[\\text{start}_2, \\text{end}_2) = [2024\\text{-}01\\text{-}01, +\\infty)$. Plan code $c_2 = 1$. ETL timestamp $\\text{ts}_2 = 2024\\text{-}01\\text{-}02\\ \\text{T}\\ 05{:}00$.\n    \\item Row $3$: surrogate key $s_3 = 135$, version $v_3 = 3$. Interval $[\\text{start}_3, \\text{end}_3) = [2024\\text{-}03\\text{-}15, +\\infty)$. Plan code $c_3 = 1$. ETL timestamp $\\text{ts}_3 = 2024\\text{-}03\\text{-}16\\ \\text{T}\\ 05{:}52$.\n\\end{enumerate}\n\nThe first step is to identify the set of rows, $R_{\\text{valid}}$, that are valid at the as-of time $t = 2024\\text{-}06\\text{-}15$. We test the validity condition for each row.\n\nFor Row $1$:\nThe interval is $[2023\\text{-}01\\text{-}01, 2023\\text{-}12\\text{-}31)$. The condition is $2023\\text{-}01\\text{-}01 \\leq 2024\\text{-}06\\text{-}15 < 2023\\text{-}12\\text{-}31$. This is false, because $2024\\text{-}06\\text{-}15$ is not less than $2023\\text{-}12\\text{-}31$. Thus, Row $1$ is not valid at time $t$.\n\nFor Row $2$:\nThe interval is $[2024\\text{-}01\\text{-}01, +\\infty)$. The condition is $2024\\text{-}01\\text{-}01 \\leq 2024\\text{-}06\\text{-}15 < +\\infty$. This is true. Thus, Row $2$ is a candidate for the valid record at time $t$.\n\nFor Row $3$:\nThe interval is $[2024\\text{-}03\\text{-}15, +\\infty)$. The condition is $2024\\text{-}03\\text{-}15 \\leq 2024\\text{-}06\\text{-}15 < +\\infty$. This is true. Thus, Row $3$ is also a candidate for the valid record at time $t$.\n\nWe have identified two valid rows, $R_{\\text{valid}} = \\{\\text{Row } 2, \\text{Row } 3\\}$, for the as-of time $t$. This presents a conflict, as the definition of the \"as-of\" operator states that it returns a *single* row. The problem statement anticipates this, noting that \"ingestion defects may introduce overlapping intervals that must be reconciled\". A principled, deterministic resolution is required.\n\nIn data warehousing and data governance, when multiple records claim validity for the same business time due to data quality issues, the conflict is resolved by selecting the record that represents the most current or authoritative information. The ETL load timestamp, $\\text{etl\\_load\\_ts}$, provides the necessary metadata for this resolution. It records the system time when the information was ingested into the warehouse. A later timestamp signifies more recent information, which is assumed to supersede any earlier, conflicting information. This \"last write wins\" strategy is a standard principle for maintaining data integrity.\n\nTherefore, the deterministic rule for selecting the unique valid row $r^{\\ast}$ from the set of candidates $R_{\\text{valid}}$ is to choose the one with the maximum $\\text{etl\\_load\\_ts}$.\nFormally, $r^{\\ast} = \\arg\\max_{r_i \\in R_{\\text{valid}}}(\\text{ts}_i)$.\n\nLet us apply this rule to the two candidate rows:\n\\begin{itemize}\n    \\item Row $2$: $\\text{ts}_2 = 2024\\text{-}01\\text{-}02\\ \\text{T}\\ 05{:}00$\n    \\item Row $3$: $\\text{ts}_3 = 2024\\text{-}03\\text{-}16\\ \\text{T}\\ 05{:}52$\n\\end{itemize}\nComparing the timestamps, we find that $\\text{ts}_3 > \\text{ts}_2$. This means that the information in Row $3$ was loaded into the Clinical Data Warehouse after the information in Row $2$. Consequently, Row $3$ is the authoritative record for the period where their effective intervals overlap. Other metadata such as the version number ($v_3 > v_2$) and the surrogate key ($s_3 > s_2$) are consistent with this conclusion, but the `etl_load_ts` is the most direct and reliable arbiter.\n\nThe uniquely determined valid row is Row $3$. The problem asks for the plan code, $c^{\\ast}$, from this row. The plan code for Row $3$ is given as $c_3 = 1$.\n\nTherefore, the patient's insurance plan code in force at time $t = 2024\\text{-}06\\text{-}15$ is $1$.\nIf no row had been found valid, the answer would be $c^{\\ast} = 0$ per the problem definition, but this is not the case here.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "The vast potential of CDWs for research comes with a profound ethical and legal obligation: protecting patient privacy. Before data can be shared for secondary use, it must be de-identified to minimize the risk of revealing patient identities. This exercise  introduces you to $k$-anonymity, a fundamental privacy model. By analyzing a hypothetical dataset, you will learn how to identify records that fail to meet a given privacy threshold, providing a tangible method for quantifying and mitigating re-identification risk.",
            "id": "4826387",
            "problem": "A Clinical Data Warehouse (CDW) aggregates patient-level data from multiple clinical systems. In a de-identified extract built for secondary use, the data steward treats the quasi-identifier set $\\{\\text{age}, \\text{sex}, \\text{three-digit ZIP code}\\}$ as the attributes used to form equivalence classes. By definition, $k$-anonymity requires that every record’s equivalence class (under these quasi-identifiers) contains at least $k$ records. Consider a CDW extract with $100{,}000$ records partitioned into $500$ equivalence classes under these quasi-identifiers. The class-size histogram is as follows: $320$ classes have size $220$, $150$ classes have size $196$, $20$ classes have size $8$, and $10$ classes have size $4$. Using only the core definitions of $k$-anonymity and equivalence classes, first reason from first principles to characterize which records violate $k$-anonymity for $k=10$ and why. Then, define $R(k)$ as the total number of records in the dataset that belong to equivalence classes that fail to meet $k$-anonymity, and compute $R(10)$ for the given histogram. Express your final answer as a single integer.",
            "solution": "The problem requires an analysis of a dataset's compliance with the principle of $k$-anonymity. We begin by establishing the fundamental definitions.\n\nAn equivalence class is a set of records that are indistinguishable from one another with respect to a defined set of attributes known as quasi-identifiers. In this problem, the quasi-identifier set is specified as $\\{\\text{age}, \\text{sex}, \\text{three-digit ZIP code}\\}$. All records within a single equivalence class share the same values for these three attributes.\n\nThe principle of $k$-anonymity states that a dataset is $k$-anonymous if every record in the dataset belongs to an equivalence class containing at least $k$ records. Let $s$ denote the size (i.e., the number of records) of an arbitrary equivalence class. For a dataset to satisfy $k$-anonymity, the condition $s \\ge k$ must hold for all equivalence classes in the dataset.\n\nA record violates $k$-anonymity if it is a member of an equivalence class for which the size $s$ is less than $k$. The question specifies a value of $k=10$. Therefore, any record belonging to an equivalence class of size $s < 10$ is considered to violate $10$-anonymity. These records are more susceptible to re-identification because they can only be linked to a small group of individuals.\n\nThe problem provides a histogram of equivalence class sizes for a dataset containing a total of $N=100,000$ records partitioned into $500$ classes:\n1. There are $n_1 = 320$ classes of size $s_1 = 220$.\n2. There are $n_2 = 150$ classes of size $s_2 = 196$.\n3. There are $n_3 = 20$ classes of size $s_3 = 8$.\n4. There are $n_4 = 10$ classes of size $s_4 = 4$.\n\nWe must now characterize which records violate $10$-anonymity by comparing each class size $s_i$ to the required minimum size $k=10$.\n\nFor the first group of classes, the size is $s_1 = 220$. Since $220 \\ge 10$, these $320$ classes satisfy the $10$-anonymity criterion. The records within these classes are not in violation.\n\nFor the second group of classes, the size is $s_2 = 196$. Since $196 \\ge 10$, these $150$ classes also satisfy the $10$-anonymity criterion. The records within these classes are not in violation.\n\nFor the third group of classes, the size is $s_3 = 8$. Since $8 < 10$, these $20$ classes fail to meet the $10$-anonymity criterion. Consequently, all records belonging to these classes are in violation of $10$-anonymity.\n\nFor the fourth group of classes, the size is $s_4 = 4$. Since $4 < 10$, these $10$ classes also fail to meet the $10$-anonymity criterion. All records belonging to these classes are in violation of $10$-anonymity.\n\nThe problem defines $R(k)$ as the total number of records in the dataset that belong to equivalence classes that fail to meet $k$-anonymity. Based on our analysis, the classes that fail to meet $10$-anonymity are those with sizes $s_3=8$ and $s_4=4$. To compute $R(10)$, we must sum the number of records in all such violating classes.\n\nThe total number of records in the $n_3 = 20$ classes of size $s_3=8$ is:\n$$N_{\\text{violating}, 1} = n_3 \\times s_3 = 20 \\times 8 = 160$$\n\nThe total number of records in the $n_4 = 10$ classes of size $s_4=4$ is:\n$$N_{\\text{violating}, 2} = n_4 \\times s_4 = 10 \\times 4 = 40$$\n\nThe total number of records $R(10)$ that violate $10$-anonymity is the sum of the records in all violating classes:\n$$R(10) = N_{\\text{violating}, 1} + N_{\\text{violating}, 2} = 160 + 40 = 200$$\n\nThus, there are a total of $200$ records in the dataset that belong to equivalence classes whose sizes are smaller than $10$.",
            "answer": "$$\n\\boxed{200}\n$$"
        }
    ]
}