{
    "hands_on_practices": [
        {
            "introduction": "Before health data can be used for secondary purposes like research, it must often be de-identified to protect patient privacy in accordance with regulations like the Health Insurance Portability and Accountability Act (HIPAA). This practice introduces $k$-anonymity, a fundamental technique for preventing re-identification by ensuring that any individual's record is indistinguishable from at least $k-1$ other records based on quasi-identifiers. In this exercise , you will apply this concept by determining the necessary level of data generalization to make a sample dataset compliant, a common task that balances privacy protection with data utility.",
            "id": "4853649",
            "problem": "A health system maintains patient visit records that include five-digit United States Postal Service ZIP codes. For primary use in patient care, retaining full ZIP codes aids operations. For secondary use in health services research subject to the Health Insurance Portability and Accountability Act (HIPAA), the data custodian imposes the privacy requirement of $k$-anonymity on quasi-identifiers. The quasi-identifier here is the ZIP code. Under $k$-anonymity, each equivalence class of records formed by indistinguishable quasi-identifier values must contain at least $k$ records. To satisfy this, the custodian allows only hierarchical generalization of ZIP codes to a shared prefix of length $L$, where $L \\in \\{1,2,3,4,5\\}$, and replaces the remaining digits by a wildcard. No record suppression is permitted.\n\nYou are given a secondary-use dataset with the following counts per five-digit ZIP code (each bullet gives a ZIP code and the number of records in that ZIP):\n\n- $02138$: $2$\n- $02139$: $2$\n- $02141$: $1$\n- $02212$: $1$\n- $02215$: $3$\n- $02301$: $2$\n- $02302$: $1$\n- $03060$: $2$\n- $03062$: $1$\n- $03063$: $2$\n- $03101$: $1$\n- $03102$: $1$\n- $04001$: $3$\n- $04002$: $2$\n\nAssume $k=5$. When generalizing to a prefix length $L$, an equivalence class is defined by the shared first $L$ digits; for example, $L=3$ forms classes like $021***$, $022***$, and $023***$, each containing the sum of records whose ZIP codes begin with those three digits. Determine the minimal integer $L$ such that all equivalence classes induced by the first $L$ digits contain at least $k$ records, without suppressing any records. Express your answer as an integer with no units. No rounding is required.",
            "solution": "The problem requires us to determine the minimal integer prefix length $L$ for which a dataset of patient visit records, identified by five-digit ZIP codes, satisfies $k$-anonymity with $k=5$. The anonymization technique is hierarchical generalization, where ZIP codes are truncated to their first $L$ digits. An equivalence class consists of all records whose ZIP codes share the same $L$-digit prefix. The condition is that every such equivalence class must contain at least $k$ records. No records can be suppressed.\n\nFirst, let's formalize the problem and clarify the objective. The set of possible prefix lengths is $L \\in \\{1, 2, 3, 4, 5\\}$. We are given a dataset of counts for specific five-digit ZIP codes. Let $C(z)$ be the number of records for a given ZIP code $z$. For a given prefix length $L$, an equivalence class $E_p$ is defined by a prefix $p$ of length $L$. The size of this class is the sum of counts of all ZIP codes $z$ that start with the prefix $p$, which we denote as $|E_p|$. The $k$-anonymity requirement is that for a chosen $L$, $|E_p| \\ge k$ for all existing prefixes $p$ of length $L$.\n\nThe problem asks for the \"minimal integer $L$\". This phrasing presents a potential ambiguity. If $P(L)$ is the property that $k$-anonymity is satisfied for prefix length $L$, and if $P(L')$ holds for some $L'$, then it will also hold for any $L < L'$. This is because an equivalence class for prefix length $L$ is a union of one or more equivalence classes for length $L'$, and thus its size will be greater than or equal to the size of any of its constituent $L'$-classes. This implies that the set of valid $L$ values is of the form $\\{1, 2, \\dots, L_{\\max}\\}$. A literal interpretation of \"minimal integer $L$\" would mean finding the minimum of this set, which would be $L=1$ (provided the total number of records is at least $k$). This solution, while mathematically literal, would represent the maximum possible generalization and information loss, which runs counter to the practical goal of data utility in research.\n\nIn the context of data privacy and utility, the objective is to apply the minimal necessary generalization to meet the privacy requirement, thereby preserving as much information as possible. Minimal generalization corresponds to the largest prefix length $L$ that satisfies the condition. Therefore, we interpret the problem's objective as finding the maximal integer $L$ for which the $k$-anonymity constraint is met. We are thus seeking to find $\\max \\{L \\in \\{1, 2, 3, 4, 5\\} \\mid \\forall p, |E_p| \\ge k \\}$.\n\nWe are given $k=5$ and the following record counts:\n- $C(02138)=2$\n- $C(02139)=2$\n- $C(02141)=1$\n- $C(02212)=1$\n- $C(02215)=3$\n- $C(02301)=2$\n- $C(02302)=1$\n- $C(03060)=2$\n- $C(03062)=1$\n- $C(03063)=2$\n- $C(03101)=1$\n- $C(03102)=1$\n- $C(04001)=3$\n- $C(04002)=2$\n\nWe will test values of $L$ starting from $L=5$ downwards.\n\n**Case 1: $L=5$ (no generalization)**\nThe equivalence classes are the individual five-digit ZIP codes. The sizes of the classes are the counts themselves: $\\{2, 2, 1, 1, 3, 2, 1, 2, 1, 2, 1, 1, 3, 2\\}$. The minimum size is $1$. Since $1 < k=5$, this level is not compliant.\n\n**Case 2: $L=4$ (generalization to four-digit prefixes)**\nWe group records by the first four digits of their ZIP code.\n- $E_{0213}$: $C(02138) + C(02139) = 2 + 2 = 4$.\nSince $|E_{0213}| = 4 < 5$, this level is not compliant. We do not need to check the other equivalence classes.\n\n**Case 3: $L=3$ (generalization to three-digit prefixes)**\nWe group records by the first three digits.\n- $E_{021}$: $C(02138) + C(02139) + C(02141) = 2 + 2 + 1 = 5$. This class is compliant.\n- $E_{022}$: $C(02212) + C(02215) = 1 + 3 = 4$. This class is not compliant, as $4 < 5$.\nTherefore, $L=3$ is not a valid level of generalization.\n\n**Case 4: $L=2$ (generalization to two-digit prefixes)**\nWe group records by the first two digits.\n- $E_{02}$: This class includes all ZIP codes starting with '02'. We can sum the sizes of the $L=3$ classes that form this group:\n$|E_{02}| = |E_{021}| + |E_{022}| + |E_{023}| = (2+2+1) + (1+3) + (2+1) = 5 + 4 + 3 = 12$.\nSince $12 \\ge 5$, this class is compliant.\n- $E_{03}$: This class includes all ZIP codes starting with '03'.\n$|E_{03}| = |E_{030}| + |E_{031}| = (2+1+2) + (1+1) = 5 + 2 = 7$.\nSince $7 \\ge 5$, this class is compliant.\n- $E_{04}$: This class includes all ZIP codes starting with '04'.\n$|E_{04}| = C(04001) + C(04002) = 3 + 2 = 5$.\nSince $5 \\ge 5$, this class is compliant.\n\nAll equivalence classes for $L=2$ have sizes $\\{12, 7, 5\\}$. The minimum size is $5$, which satisfies the condition $|E_p| \\ge 5$. Thus, $L=2$ is a valid level of generalization.\n\nSince we are seeking the maximal $L$ that satisfies the condition and have found that $L=3$ fails while $L=2$ succeeds, $L=2$ is the solution under our interpretation. Any smaller value of $L$ (i.e., $L=1$) would also be compliant but would represent an unnecessary loss of data utility.\n\nThe minimal integer $L$ that corresponds to the minimal required level of generalization is $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Data collected for one purpose, such as billing, carries the imprint of that context, including its incentives and limitations. When this data is repurposed for research—a classic example of secondary use—a 'context collapse' can occur, leading to biased conclusions if the original data-generating process is ignored. This exercise  challenges you to evaluate a realistic scenario involving diagnostic codes, exposing how and why naive prevalence estimates can fail and requiring you to identify robust methods to correct for these biases.",
            "id": "4853664",
            "problem": "A health system has a dataset of International Classification of Diseases, Tenth Revision (ICD-10) codes collected primarily for reimbursement (primary use). The institution plans to repurpose these codes to estimate the prevalence of chronic kidney disease (CKD) among adult patients (secondary use). Over the last year, the system recorded $N = 50{,}000$ adult patients with at least one encounter; of these, $C = 4{,}100$ had at least one CKD ICD-10 code (codes from the $N18.x$ family).\n\nA validation study was conducted in one nephrology-heavy clinical division. The study reviewed $n = 1{,}000$ randomly sampled patients from that division, applying a gold-standard CKD definition based on sustained estimated glomerular filtration rate (eGFR) thresholds. The study found $180$ CKD cases by the gold standard; $126$ of these had a CKD ICD-10 code. Among the $820$ non-CKD patients, $82$ had a CKD ICD-10 code.\n\nAdditional context: A risk-adjustment reimbursement initiative started mid-year, instructing coders to increase capture of chronic conditions, producing approximately a $20\\%$ increase in CKD coding rates after the policy change. Approximately $20\\%$ of visits for uninsured patients are recorded in the clinical system but are not submitted for claims coding, and thus may lack ICD-10 codes.\n\nAssume that CKD status is relatively stable over a year, and that ICD-10 code assignment can be modeled as a binary classifier with sensitivity $Se$ and specificity $Sp$ that may vary by clinical setting.\n\nWhich of the following statements best evaluates whether using $C/N$ to infer CKD prevalence leads to context collapse and proposes appropriate mitigations? Select all that apply.\n\nA. Because ICD-10 is standardized, $C/N$ is an unbiased estimator of CKD prevalence across the health system regardless of coding incentives or missing claims, so no adjustment or validation is necessary.\n\nB. Using validation-derived $Se \\approx 0.70$ and $Sp \\approx 0.90$, naive use of $C/N$ often overestimates CKD prevalence when false positives are nontrivial; however, the observed $C/N = 0.082$ is inconsistent with the validation setting, indicating failure of transportability across clinical contexts and sampling frames. Appropriate mitigations include stratified or site-specific validation, recalibration of $Se$ and $Sp$, combining ICD-10 codes with laboratory data in a computable phenotype, and adjustments for uninsured encounters and time-varying coding policies.\n\nC. Since $Se \\approx 0.70$, the primary issue is undercounting true CKD; therefore, a sufficient correction is to multiply $C/N$ by $1/Se$ to estimate the true prevalence, without considering $Sp$ or selection processes.\n\nD. Reusing reimbursement-oriented ICD-10 codes to infer disease prevalence is a secondary use vulnerable to context collapse because the data-generating process is driven by billing incentives, coverage, and policy changes rather than clinical truth. Mitigations include external validation against gold-standard clinical criteria, computable phenotyping that incorporates laboratory measurements and longitudinal evidence, modeling time-stratified coding changes, and explicit treatment of missing claims among uninsured patients; $C/N$ alone is insufficient for unbiased prevalence estimation.",
            "solution": "The problem statement is evaluated for validity prior to providing a solution.\n\n### Step 1: Extract Givens\n- Total adult patients with at least one encounter: $N = 50{,}000$.\n- Patients with at least one CKD ICD-10 code (N18.x family): $C = 4{,}100$.\n- Primary use of data: Reimbursement.\n- Secondary use of data: Estimate prevalence of chronic kidney disease (CKD).\n- Validation study sample size: $n = 1{,}000$ (from a single nephrology-heavy clinical division).\n- Validation study results (based on gold-standard eGFR definition):\n    - True CKD cases found: $180$.\n    - Non-CKD cases found: $820$.\n    - True positives (TP, true CKD with code): $126$.\n    - False positives (FP, not CKD but with code): $82$.\n- Additional context:\n    - A mid-year reimbursement initiative increased CKD coding rates by approximately $20\\%$.\n    - Approximately $20\\%$ of visits for uninsured patients are not submitted for claims, potentially lacking ICD-10 codes.\n- Assumptions:\n    - CKD status is stable over the year.\n    - ICD-10 code assignment acts as a binary classifier with sensitivity ($Se$) and specificity ($Sp$).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within medical informatics and epidemiology, using established concepts like ICD-10 codes, prevalence, sensitivity, specificity, gold-standard validation, and the secondary use of health data. The scenario of \"context collapse\"—where data generated for one purpose (billing) is repurposed for another (research) without accounting for the original context's biases—is a central and well-defined problem in the field. The provided data are numerically consistent and plausible for a health system setting. The problem is well-posed, asking for an evaluation of statements about this scenario, which requires applying principles of epidemiology and data science to the given information. The statement is objective and free of scientific flaws or contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A full analysis and solution will be provided.\n\n### Derivation\nThe core of the problem is to evaluate the naive estimator of prevalence, $P_{naive} = C/N$, and assess statements regarding its validity and potential corrections.\n\n1.  **Naive Prevalence Estimation:**\n    The observed proportion of patients with a CKD ICD-10 code in the entire health system is:\n    $$P_{\\text{obs}} = \\frac{C}{N} = \\frac{4{,}100}{50{,}000} = 0.082$$\n\n2.  **Performance Metrics from Validation Study:**\n    The validation study was conducted on a sample of $n=1{,}000$ patients from a \"nephrology-heavy\" division. This is a selected, non-representative sample. From this sample, we can calculate the performance of the ICD-10 code as a classifier *within that specific context*.\n    -   True Positives ($TP$) = $126$.\n    -   Total positives (true CKD cases) in the sample = $180$.\n    -   False Negatives ($FN$) = Total Positives - $TP = 180 - 126 = 54$.\n    -   False Positives ($FP$) = $82$.\n    -   Total negatives (non-CKD cases) in the sample = $820$.\n    -   True Negatives ($TN$) = Total Negatives - $FP = 820 - 82 = 738$.\n\n    The sensitivity ($Se$) and specificity ($Sp$) in the validation sample are:\n    $$Se_{\\text{val}} = \\frac{TP}{TP + FN} = \\frac{126}{180} = 0.70$$\n    $$Sp_{\\text{val}} = \\frac{TN}{TN + FP} = \\frac{738}{820} = 0.90$$\n\n3.  **Relationship Between Observed and True Prevalence:**\n    The observed proportion of positive tests ($P_{\\text{obs}}$) is related to the true prevalence ($P$) and the classifier's performance ($Se$, $Sp$) by the formula:\n    $$P_{\\text{obs}} = (Se \\times P) + ((1 - Sp) \\times (1 - P))$$\n    The naive estimator $C/N$ is an estimate of $P_{\\text{obs}}$, not the true prevalence $P$. Using $C/N$ as an estimate of $P$ implicitly assumes $Se=1$ and $Sp=1$, which is incorrect.\n\n4.  **Testing for Transportability (Context Collapse):**\n    A critical question is whether the $Se_{\\text{val}}$ and $Sp_{\\text{val}}$ calculated from the specialized nephrology clinic sample can be applied (i.e., are \"transportable\") to the general patient population. We can test this by assuming they *are* transportable and seeing if the numbers are consistent. We set $P_{\\text{obs}}$ to the system-wide value of $0.082$ and use $Se_{\\text{val}}=0.70$ and $Sp_{\\text{val}}=0.90$ to solve for the implied true prevalence $P$:\n    $$0.082 = (0.70 \\times P) + ((1 - 0.90) \\times (1 - P))$$\n    $$0.082 = 0.70 P + 0.10(1 - P)$$\n    $$0.082 = 0.70 P + 0.10 - 0.10 P$$\n    $$0.082 - 0.10 = 0.60 P$$\n    $$-0.018 = 0.60 P$$\n    $$P = -\\frac{0.018}{0.60} = -0.03$$\n    A prevalence cannot be negative. This mathematical contradiction proves that the performance characteristics ($Se=0.70$, $Sp=0.90$) of the ICD-10 codes in the nephrology-heavy validation sample are **not applicable** to the general health system population. This is a classic example of model or metric failure due to lack of transportability, which is a key aspect of context collapse. The data-generating process (coding) is different in the specialized clinic versus the general population.\n\n5.  **Other Contextual Factors:**\n    The problem statement explicitly notes other factors contributing to context collapse:\n    -   **Time-Varying Policy:** A mid-year initiative increased CKD coding. This means $Se$ is not constant over the measurement period, biasing the overall count.\n    -   **Missing Data/Coverage Bias:** Uninsured patients are less likely to have claims-based ICD codes, leading to a systematic undercounting of cases in this subpopulation.\n\n### Option-by-Option Analysis\n\n**A. Because ICD-10 is standardized, $C/N$ is an unbiased estimator of CKD prevalence across the health system regardless of coding incentives or missing claims, so no adjustment or validation is necessary.**\nThis statement is fundamentally flawed. Standardization of codes does not imply standardized or accurate *application* of codes. The use of codes for billing is known to be imperfect for clinical surveillance, exhibiting both false positives and false negatives ($Se < 1$ and $Sp < 1$). The data clearly shows this. Therefore, $C/N$ is a biased estimator of true prevalence. The contextual factors mentioned (incentives, missing claims) are primary sources of this bias.\n**Verdict: Incorrect.**\n\n**B. Using validation-derived $Se \\approx 0.70$ and $Sp \\approx 0.90$, naive use of $C/N$ often overestimates CKD prevalence when false positives are nontrivial; however, the observed $C/N = 0.082$ is inconsistent with the validation setting, indicating failure of transportability across clinical contexts and sampling frames. Appropriate mitigations include stratified or site-specific validation, recalibration of $Se$ and $Sp$, combining ICD-10 codes with laboratory data in a computable phenotype, and adjustments for uninsured encounters and time-varying coding policies.**\nThis statement contains several components, all of which are correct.\n- The calculation of $Se \\approx 0.70$ and $Sp \\approx 0.90$ from the validation data is accurate.\n- The claim that nontrivial false positives (here, $1-Sp = 0.10$) can lead to overestimation is a known principle, especially when true prevalence is low.\n- The statement correctly identifies the inconsistency between the system-wide $C/N=0.082$ and the validation set's performance metrics, correctly diagnosing this as a failure of transportability. Our calculation resulting in a negative prevalence provides definitive proof of this point.\n- The proposed mitigations (stratified validation, recalibration, computable phenotyping with lab data, and adjustments for policy/coverage effects) are the standard, state-of-the-art methods to address the identified problems of selection bias, context collapse, and data quality issues.\n**Verdict: Correct.**\n\n**C. Since $Se \\approx 0.70$, the primary issue is undercounting true CKD; therefore, a sufficient correction is to multiply $C/N$ by $1/Se$ to estimate the true prevalence, without considering $Sp$ or selection processes.**\nThis statement proposes an overly simplistic correction. The formula for this correction would be $P_{est} = P_{obs}/Se$. This is only valid if there are no false positives, i.e., $Sp=1$. The validation data shows $Sp=0.90$, indicating a significant number of false positives that must be accounted for. Ignoring $Sp$, and more importantly, ignoring the transportability failure, the selection bias of the validation sample, and other contextual factors, makes this proposed correction invalid and insufficient.\n**Verdict: Incorrect.**\n\n**D. Reusing reimbursement-oriented ICD-10 codes to infer disease prevalence is a secondary use vulnerable to context collapse because the data-generating process is driven by billing incentives, coverage, and policy changes rather than clinical truth. Mitigations include external validation against gold-standard clinical criteria, computable phenotyping that incorporates laboratory measurements and longitudinal evidence, modeling time-stratified coding changes, and explicit treatment of missing claims among uninsured patients; $C/N$ alone is insufficient for unbiased prevalence estimation.**\nThis statement provides a high-level, conceptually perfect summary of the problem.\n- It correctly identifies the scenario as a secondary use of data.\n- It accurately defines \"context collapse\" in this setting as a misalignment between the data-generating process (driven by billing, policy) and the analytical goal (estimating clinical truth).\n- It correctly identifies that $C/N$ alone is an insufficient estimator.\n- The list of mitigations is comprehensive and appropriate, covering validation, building more robust phenotypes (e.g., with lab data), and modeling the specific biases mentioned in the prompt (temporal changes, missing data). This statement provides the theoretical framework that explains the quantitative findings discussed in option B.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "Electronic Health Record (EHR) data is a direct reflection of clinical workflow, including its interruptions, delays, and documentation habits, which can appear as artifacts like illogical timestamps or extreme outliers. Naively using this raw data for secondary analysis, such as measuring time-to-treatment, can produce highly misleading results about clinical performance. This practical exercise  will guide you through implementing a robust statistical procedure to detect and correct these workflow-induced data quality issues, transforming messy operational data into a more reliable resource for research.",
            "id": "4853698",
            "problem": "You are given paired timestamp data from an Electronic Health Record (EHR) system, consisting of medication order times and medication administration times measured in minutes from a common index (for example, arrival to the emergency department). In medical informatics, the primary use of health data is the direct support of patient care and operations (for example, placing orders and scanning medications), while the secondary use of health data refers to downstream analysis for quality improvement, research, and policy evaluation. When conducting secondary time-to-treatment analyses using EHR timestamps, workflow-induced documentation lags can bias the estimates of treatment timeliness.\n\nStarting from core definitions and well-tested statistical facts:\n\n- Let the order time be denoted by $O_i$ and the administration time by $A_i$ for patient $i$.\n- Define the recorded lag for patient $i$ as $L_i = A_i - O_i$. Clinically, $L_i$ contains the true operational delay plus documentation latency.\n- Define the robust central tendency using the median $M = \\mathrm{median}(L_i)$ and robust dispersion via the Median Absolute Deviation (MAD), $\\mathrm{MAD} = \\mathrm{median}(|L_i - M|)$.\n- A robust upper threshold for detecting unusually large lags is $U = M + \\tau \\cdot s \\cdot \\mathrm{MAD}$, where $s = 1.4826$ is the scale factor that makes $\\mathrm{MAD}$ a consistent estimator of the standard deviation under normality, and $\\tau$ is a chosen multiple. Use $\\tau = 3$.\n- If $\\mathrm{MAD} = 0$, set $U = M + \\delta$ with $\\delta = 1$ to avoid degeneracy.\n- A negative lag $L_i < 0$ indicates a documentation inversion (for example, back-entry or mis-sequencing), not a clinical impossibility, and should be flagged as workflow-induced.\n- Estimate a baseline operational lag $B$ as the median of the nonnegative lags: $B = \\mathrm{median}(\\{L_i \\mid L_i \\ge 0\\})$. If there are no nonnegative lags, set $B = \\max(0, M)$.\n\nDefine a correction procedure for secondary analysis that attempts to remove documentation-induced bias in the recorded lags:\n\n- If $L_i < 0$, set the corrected lag $L'_i = B$.\n- If $L_i > U$, set $L'_i = U$.\n- Otherwise, set $L'_i = L_i$.\n\nFor each test case, compute the following metrics:\n\n1. The detection fraction $r$ as the fraction (expressed as a decimal) of lags flagged as workflow-induced, where a lag is flagged if $L_i < 0$ or $L_i > U$.\n2. The mean bias $\\Delta = \\overline{L} - \\overline{L'}$, where $\\overline{L}$ is the arithmetic mean of the recorded lags, and $\\overline{L'}$ is the arithmetic mean of the corrected lags. Report $\\Delta$ in minutes, expressed as a decimal rounded to three decimals.\n3. The change in compliance with a target time-to-treatment threshold $T$ minutes, defined as $p' - p$, where $p$ is the fraction (expressed as a decimal) of patients with $L_i \\le T$, and $p'$ is the fraction with $L'_i \\le T$. Use $T = 30$ minutes. Report the change as a decimal rounded to three decimals.\n\nYour program must implement the above definitions and correction logic, and then apply them to the following test suite of order and administration times (all times are in minutes). For each test case, treat the input as two equal-length lists of $O_i$ and $A_i$:\n\n- Test case $1$ (happy path, small variability, no negatives):\n  - Orders $O$: $(5, 15, 30, 45, 60, 75)$\n  - Administrations $A$: $(17, 28, 40, 59, 71, 84)$\n- Test case $2$ (documentation inversions present):\n  - Orders $O$: $(10, 20, 30, 40, 50)$\n  - Administrations $A$: $(18, 15, 35, 45, 53)$\n- Test case $3$ (boundary case, zero lags):\n  - Orders $O$: $(0, 10, 20)$\n  - Administrations $A$: $(0, 10, 20)$\n- Test case $4$ (heavy upper tail, mixed extremes):\n  - Orders $O$: $(5, 10, 20, 40, 80, 100, 120)$\n  - Administrations $A$: $(15, 22, 110, 55, 91, 300, 129)$\n- Test case $5$ (single pair, degenerate dispersion):\n  - Orders $O$: $(5)$\n  - Administrations $A$: $(50)$\n\nAngle units are not applicable. Physical units are minutes; report all time-derived outputs in minutes as decimals. Percentages must be expressed as decimals. Your program should produce a single line of output containing the results for the test cases as a list of lists, each inner list being $[r, \\Delta, p' - p]$, with each value rounded to three decimals. For example: $[[r_1,\\Delta_1,c_1],[r_2,\\Delta_2,c_2],\\dots]$.",
            "solution": "The problem requires the implementation of a statistical procedure to identify and correct for workflow-induced biases in Electronic Health Record (EHR) time-to-treatment data. We are asked to compute three specific metrics that quantify the impact of this correction: the fraction of data points flagged as biased, the average change in the recorded lag times, and the change in compliance with a clinical performance target.\n\nThe procedure is based on robust statistics, which are less sensitive to outliers than classical methods. Let us systematically detail the required calculations.\n\nFirst, for each patient $i$, we are given an order time $O_i$ and an administration time $A_i$. The primary variable of interest is the recorded lag, $L_i$, defined as the difference:\n$$L_i = A_i - O_i$$\n\nThe core of the detection algorithm relies on comparing individual lags $L_i$ to the overall distribution of lags. We first establish a measure of central tendency using the median of the lags, $M$:\n$$M = \\mathrm{median}(\\{L_i\\})$$\n\nNext, we establish a measure of dispersion using the Median Absolute Deviation (MAD), which is the median of the absolute differences between each lag and the central tendency $M$:\n$$\\mathrm{MAD} = \\mathrm{median}(\\{|L_i - M|\\})$$\n\nWith these robust statistics, we define an upper threshold, $U$, to identify unusually large positive lags. This threshold is set at a multiple, $\\tau$, of the scaled MAD above the median. The scale factor, $s=1.4826$, is used to make the MAD a consistent estimator of the standard deviation for normally distributed data. The problem specifies using $\\tau=3$.\n$$U = M + \\tau \\cdot s \\cdot \\mathrm{MAD}$$\n\nA special case arises when the data has no variability, resulting in $\\mathrm{MAD} = 0$. In this degenerate case, the threshold is defined by adding a small constant, $\\delta=1$, to the median:\n$$U = M + \\delta \\quad (\\text{if } \\mathrm{MAD} = 0)$$\n\nA lag $L_i$ is flagged as workflow-induced if it is either negative ($L_i < 0$), indicating a documentation inversion, or if it exceeds the upper threshold ($L_i > U$).\n\nOnce the flagging criteria are established, a correction procedure is applied to generate a new set of lags, $L'_i$. The correction logic is as follows:\n1.  If a lag is negative ($L_i < 0$), it is an artifact. It is replaced by a baseline operational lag, $B$. This baseline is estimated as the median of all non-negative recorded lags: $B = \\mathrm{median}(\\{L_j \\mid L_j \\ge 0\\})$. If no non-negative lags exist, $B$ is defined as $B = \\max(0, M)$. Thus, if $L_i < 0$, then $L'_i = B$.\n2.  If a lag exceeds the upper threshold ($L_i > U$), it is considered an outlier. It is capped at the threshold value. Thus, if $L_i > U$, then $L'_i = U$.\n3.  If a lag is not flagged ($0 \\le L_i \\le U$), it is considered valid and remains unchanged. Thus, if $0 \\le L_i \\le U$, then $L'_i = L_i$.\n\nAfter applying this correction to all lags, we compute the specified metrics for each test case. Let $N$ be the total number of patients/lags.\n\n1.  **Detection Fraction, $r$**: This is the fraction of lags that were flagged. Let $N_{flagged}$ be the count of lags where $L_i < 0$ or $L_i > U$.\n    $$r = \\frac{N_{flagged}}{N}$$\n\n2.  **Mean Bias, $\\Delta$**: This metric quantifies the average magnitude of the correction. It is the difference between the arithmetic mean of the original lags, $\\overline{L}$, and the arithmetic mean of the corrected lags, $\\overline{L'}$.\n    $$\\overline{L} = \\frac{1}{N} \\sum_{i=1}^{N} L_i$$\n    $$\\overline{L'} = \\frac{1}{N} \\sum_{i=1}^{N} L'_i$$\n    $$\\Delta = \\overline{L} - \\overline{L'}$$\n    This value is reported in minutes, rounded to three decimal places.\n\n3.  **Change in Compliance, $p' - p$**: This measures the impact of the correction on a key performance indicator. Given a target time-to-treatment threshold, $T=30$ minutes, we calculate the proportion of patients meeting this target before and after correction. Let $p$ be the proportion for original lags and $p'$ for corrected lags.\n    $$p = \\frac{\\text{count}(L_i \\le T)}{N}$$\n    $$p' = \\frac{\\text{count}(L'_i \\le T)}{N}$$\n    The change is then $(p' - p)$, reported as a decimal rounded to three places.\n\nThe implementation will process each test case according to these sequential definitions to produce the final results.",
            "answer": "[[0.0, 0.0, 0.0], [0.2, -2.0, 0.0], [0.0, 0.0, 0.0], [0.286, 34.188, 0.286], [0.0, 0.0, 0.0]]"
        }
    ]
}