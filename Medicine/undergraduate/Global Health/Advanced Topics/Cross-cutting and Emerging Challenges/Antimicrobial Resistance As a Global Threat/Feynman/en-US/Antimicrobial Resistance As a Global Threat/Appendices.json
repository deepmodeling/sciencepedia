{
    "hands_on_practices": [
        {
            "introduction": "To manage the global threat of antimicrobial resistance (AMR), we must first be able to measure its primary driver: the volume of antibiotics used by a population. This exercise introduces the standard metric for this measurement, the Defined Daily Doses per $1{,}000$ inhabitants per day (DID), used by organizations like the World Health Organization. By calculating and interpreting the DID from a hypothetical national dataset , you will gain a practical understanding of how public health agencies monitor and compare antibiotic consumption patterns across different regions, a crucial first step in designing effective stewardship programs.",
            "id": "4968800",
            "problem": "Antimicrobial Resistance (AMR) is driven in part by the intensity of antibiotic consumption in populations. Surveillance systems report consumption using the metric Defined Daily Doses per $1{,}000$ inhabitants per day (DID), where Defined Daily Dose (DDD) is the average maintenance dose per day for a drug in adults. Starting from the core definitions that (i) population-level consumption is an aggregate of $ \\text{DDD} $ over a period, and (ii) comparability across populations and time requires normalization by the number of inhabitants and by days in the period, derive from first principles the expression needed to compute DID. A country reports an annual total of $365{,}000$ DDD of systemic antibacterials in a population of $1{,}000{,}000$ inhabitants over a non-leap year of $365$ days. Compute the DID and interpret the consumption category according to these thresholds: low if DID  5, moderate if DID >= 5 and  15, and high if DID >= 15. Express the final value in DDD per $1{,}000$ inhabitants per day. No rounding is necessary; provide the exact value.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- **Metric to derive and compute**: Defined Daily Doses per $1{,}000$ inhabitants per day (DID).\n- **Definition of DDD**: The average maintenance dose per day for a drug in adults.\n- **Principle for derivation (i)**: Population-level consumption is an aggregate of DDD over a period.\n- **Principle for derivation (ii)**: Comparability across populations and time requires normalization by the number of inhabitants and by days in the period.\n- **Data for calculation**:\n  - Total consumption of systemic antibacterials: $365,000$ DDD.\n  - Population size: $1,000,000$ inhabitants.\n  - Time period: $365$ days (non-leap year).\n- **Interpretation thresholds for DID**:\n  - Low:  5\n  - Moderate: >= 5 and  15\n  - High: >= 15\n- **Output requirement**: The exact value of DID.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. The metric DID (Defined Daily Doses per $1,000$ inhabitants per day) is a standard unit of measurement for drug consumption, widely used by international health organizations such as the World Health Organization (WHO) and the European Centre for Disease Prevention and Control (ECDC) for surveillance of antimicrobial consumption. The premises for its derivation—aggregation and normalization—are fundamental to epidemiology and public health statistics. The problem is well-posed, providing all necessary data for a unique solution. The language is objective and the data are consistent. The problem is therefore deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed by first deriving the formula for DID from the stated first principles and then applying it to the provided data.\n\n### Derivation of the DID Formula\nLet $C_{total}$ represent the total consumption of a drug, measured as an aggregate number of Defined Daily Doses (DDD), over a specific time period.\nLet $P$ be the total number of inhabitants in the population.\nLet $T$ be the duration of the observation period in days.\n\nAccording to the provided principles, the metric must be normalized by both the population size and the number of days to ensure comparability.\n\nFirst, we calculate the average consumption per day for the entire population by dividing the total consumption by the number of days in the period:\n$$\n\\text{Consumption per day} = \\frac{C_{total}}{T}\n$$\nThis quantity has units of DDD per day.\n\nNext, to obtain the average consumption per inhabitant per day, we normalize this by the population size:\n$$\n\\text{Consumption per inhabitant per day} = \\frac{C_{total}}{P \\times T}\n$$\nThis quantity has units of DDD per inhabitant per day.\n\nThe definition of DID requires this rate to be expressed per $1,000$ inhabitants, not per single inhabitant. Therefore, we must multiply the per-inhabitant rate by a scaling factor of $1,000$:\n$$\n\\text{DID} = \\left( \\frac{C_{total}}{P \\times T} \\right) \\times 1000\n$$\nThis is the general expression for calculating DID, derived from the first principles of aggregation and normalization as requested.\n\n### Calculation of DID\nThe problem provides the following values:\n- Total annual consumption, $C_{total} = 365000$ DDD.\n- Population size, $P = 1000000$ inhabitants.\n- Time period, $T = 365$ days.\n\nSubstituting these values into the derived formula:\n$$\n\\text{DID} = \\left( \\frac{365000}{1000000 \\times 365} \\right) \\times 1000\n$$\nWe can simplify the expression. First, we compute the fraction:\n$$\n\\frac{365000}{365} = 1000\n$$\nSubstituting this back into the equation for DID:\n$$\n\\text{DID} = \\left( \\frac{1000}{1000000} \\right) \\times 1000\n$$\nThe fraction simplifies to:\n$$\n\\frac{1000}{1000000} = \\frac{1}{1000} = 0.001\n$$\nFinally, we multiply by the scaling factor:\n$$\n\\text{DID} = 0.001 \\times 1000 = 1\n$$\nThe calculated value is $1$ DID, which has units of DDD per $1,000$ inhabitants per day.\n\n### Interpretation of the Result\nThe problem requires an interpretation of the consumption category based on the following thresholds: low if DID  5, moderate if DID >= 5 and  15, and high if DID >= 15.\nOur calculated value is $\\text{DID} = 1$. Since $1  5$, the antimicrobial consumption in this country is categorized as **low**.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "After quantifying antibiotic consumption, the next critical task is to measure its direct consequence: the prevalence of resistant bacteria. Effective surveillance forms the bedrock of any AMR response, but it must be designed with statistical rigor to be both reliable and efficient. This practice explores the statistical foundation of surveillance design by having you calculate the minimum sample size required to estimate a resistance proportion with a desired level of precision . Working through this scenario will give you insight into the practical decisions public health institutes make to ensure their data on AMR trends is scientifically valid.",
            "id": "4968790",
            "problem": "A national public health institute is designing an Antimicrobial Resistance (AMR) sentinel surveillance study to estimate, in a single year, the proportion of bloodstream isolates of a priority pathogen that are resistant to a specified last-line antimicrobial. Let the true resistance proportion be denoted by $p$, and suppose a recent pilot study suggests $p \\approx 0.2$. The institute requires that the two-sided $95\\%$ confidence interval for the estimated resistance proportion have a margin-of-error no larger than $\\pm d$ with $d = 0.05$. Assume isolates are independent and identically distributed Bernoulli outcomes (resistant versus not resistant), and that a normal approximation to the sampling distribution of the sample proportion is appropriate for planning.\n\nStarting from the definition of the sample proportion and its large-sample normal approximation, and the standard construction of a two-sided confidence interval based on a normal quantile $z_{0.975}$, derive an expression for the minimum sample size $n$ that guarantees the margin-of-error requirement under the planning value $p=0.2$. Then, evaluate this expression numerically using $z_{0.975} = 1.96$ and $d = 0.05$.\n\nReport the required sample size as the smallest integer $n$ that meets the margin-of-error requirement, expressed as a count of isolates. Do not use a percentage in your final answer.",
            "solution": "The surveillance objective is to estimate a binary outcome proportion, so we consider $X_{1}, X_{2}, \\dots, X_{n}$ independent and identically distributed Bernoulli random variables with success probability $p$, where $X_{i} = 1$ indicates resistance and $X_{i} = 0$ indicates susceptibility. The sample proportion is\n$$\n\\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}.\n$$\nBy the Central Limit Theorem and the properties of Bernoulli random variables, for large $n$,\n$$\n\\hat{p} \\approx \\mathcal{N}\\!\\left(p, \\frac{p(1-p)}{n}\\right).\n$$\nA two-sided $95\\%$ confidence interval for $p$ based on the Wald method and the normal quantile $z_{0.975}$ is\n$$\n\\hat{p} \\pm z_{0.975} \\sqrt{\\frac{p(1-p)}{n}}.\n$$\nThe margin-of-error, denoted $M$, is defined as the half-width of this interval:\n$$\nM = z_{0.975} \\sqrt{\\frac{p(1-p)}{n}}.\n$$\nTo guarantee that the margin-of-error does not exceed a specified planning threshold $d$, we require\n$$\nM \\leq d \\quad \\Longrightarrow \\quad z_{0.975} \\sqrt{\\frac{p(1-p)}{n}} \\leq d.\n$$\nSolving this inequality for $n$ proceeds by isolating $n$:\n$$\n\\sqrt{\\frac{p(1-p)}{n}} \\leq \\frac{d}{z_{0.975}} \\quad \\Longrightarrow \\quad \\frac{p(1-p)}{n} \\leq \\frac{d^{2}}{z_{0.975}^{2}} \\quad \\Longrightarrow \\quad n \\geq \\frac{z_{0.975}^{2}\\, p(1-p)}{d^{2}}.\n$$\nThus, the planning expression for the minimum required sample size is\n$$\nn_{\\min} = \\frac{z_{0.975}^{2}\\, p(1-p)}{d^{2}}.\n$$\nUsing the provided values $z_{0.975} = 1.96$, $p = 0.2$, and $d = 0.05$, we compute\n$$\nz_{0.975}^{2} = (1.96)^{2} = 3.8416,\n$$\n$$\np(1-p) = 0.2 \\times 0.8 = 0.16,\n$$\n$$\nd^{2} = (0.05)^{2} = 0.0025.\n$$\nSubstituting,\n$$\nn_{\\min} = \\frac{3.8416 \\times 0.16}{0.0025} = \\frac{0.614656}{0.0025} = 245.8624.\n$$\nBecause $n$ must be an integer count of isolates and the inequality requires $n \\geq 245.8624$, the smallest integer satisfying the margin-of-error requirement is\n$$\nn = \\lceil 245.8624 \\rceil = 246.\n$$",
            "answer": "$$\\boxed{246}$$"
        },
        {
            "introduction": "Measuring consumption and resistance is vital, but the ultimate goal is to intervene effectively to control the spread of AMR. When a new policy, such as an antibiotic restriction, is implemented, how can we determine if it actually caused a change in resistance rates? This advanced exercise introduces a powerful method from epidemiology called difference-in-differences (DiD), which allows us to estimate the causal effect of an intervention by comparing changes over time between a group that received the policy and a control group that did not. By applying the DiD framework to a hypothetical policy scenario , you will learn to think critically about evaluating public health interventions and the key assumptions required to move from correlation to credible causal inference.",
            "id": "4503349",
            "problem": "You are given monthly resistance-rate data for two groups of clinical sites: a treated group that adopted an antibiotic restriction policy, and a control group that did not. Resistance rate is a unitless proportion bounded between $0$ and $1$. Assume the observed resistance rate $Y_{g,t}$ for group $g \\in \\{\\text{treated}, \\text{control}\\}$ at month index $t$ is a realization of an underlying data-generating process consistent with the potential outcomes framework: for each group $g$ and time $t$, there exist potential outcomes $Y_{g,t}(d)$ for treatment status $d \\in \\{0,1\\}$, and the observed outcome satisfies $Y_{g,t} = Y_{g,t}(D_{g,t})$, where $D_{g,t}$ is the treatment indicator. Under the Stable Unit Treatment Value Assumption (SUTVA), no site’s outcome is affected by another site’s treatment status, and the treatment has a single version per group. Consider a two-period setting aggregated into pre-policy months and post-policy months, and suppose an additive structure of group and time shocks is a suitable approximation in the absence of treatment.\n\nStarting from the above fundamental base—potential outcomes, consistency, SUTVA, and additive group and time shocks—you must:\n\n- Derive and implement an estimator for the average treatment effect on the treated (ATT) using a difference-in-differences logic without relying on any shortcut formulas given in the statement. The estimator should use the available pre-policy and post-policy grouped data to produce a causal effect estimate in the unit of resistance rate (as a decimal proportion).\n\n- Assess the parallel trends assumption by using only the pre-policy data: for each group, fit a simple linear model of the form $Y_{g,t} = \\alpha_g + \\beta_g t + \\varepsilon_{g,t}$ over the pre-policy months, where $t$ is an integer month index starting at $0$ for the first pre-policy month, and test the null hypothesis $H_0: \\beta_{\\text{treated}} - \\beta_{\\text{control}} = 0$ with a two-sided significance level of $\\alpha = 0.05$. Your test must use a principled variance estimate for the slope difference based on the residual variance from the fitted lines and must account for potentially unequal variances via a Welch–Satterthwaite approximation to the degrees of freedom.\n\n- Assess whether causal inference is credible by jointly considering: (i) whether the parallel trends test supports $H_0$ at the specified $\\alpha$ level, (ii) a provided spillover factor $s \\in [0,1]$ representing potential cross-group interference (with smaller values indicating weaker spillovers), and (iii) a binary indicator $c \\in \\{0,1\\}$ of major compositional shifts between pre-policy and post-policy periods that would violate comparability. Declare causal inference credible only if the parallel trends assumption is supported, the spillover factor $s$ is at most $0.2$, and $c = 0$.\n\nYour program must compute, for each test case, the following outputs:\n- The estimated effect as a decimal proportion, rounded to $4$ decimal places.\n- A boolean indicating whether the parallel trends hypothesis test does not reject $H_0$ at $\\alpha = 0.05$.\n- A boolean indicating whether causal inference is credible given the above criteria.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a bracketed triple $[ \\text{effect}, \\text{parallel\\_ok}, \\text{inference\\_ok} ]$.\n\nTest suite (all resistance rates are decimal proportions):\n\nCase $1$ (general case with plausible effect and supported assumptions):\n- Treated pre-policy months: $[0.25, 0.251, 0.249, 0.252, 0.253, 0.254]$\n- Treated post-policy months: $[0.205, 0.206, 0.204, 0.207, 0.208, 0.209]$\n- Control pre-policy months: $[0.26, 0.261, 0.259, 0.262, 0.263, 0.264]$\n- Control post-policy months: $[0.265, 0.266, 0.264, 0.267, 0.268, 0.269]$\n- Spillover factor $s = 0.10$\n- Compositional shift indicator $c = 0$\n\nCase $2$ (violation of parallel trends in pre-policy data):\n- Treated pre-policy months: $[0.30, 0.301, 0.299, 0.300, 0.302, 0.301]$\n- Treated post-policy months: $[0.275, 0.276, 0.274, 0.275, 0.277, 0.276]$\n- Control pre-policy months: $[0.28, 0.284, 0.288, 0.292, 0.296, 0.300]$\n- Control post-policy months: $[0.305, 0.309, 0.313, 0.317, 0.321, 0.325]$\n- Spillover factor $s = 0.05$\n- Compositional shift indicator $c = 0$\n\nCase $3$ (no apparent effect but high spillovers):\n- Treated pre-policy months: $[0.22, 0.221, 0.223, 0.224, 0.225, 0.226]$\n- Treated post-policy months: $[0.226, 0.227, 0.228, 0.229, 0.230, 0.231]$\n- Control pre-policy months: $[0.23, 0.231, 0.232, 0.233, 0.234, 0.235]$\n- Control post-policy months: $[0.236, 0.237, 0.238, 0.239, 0.240, 0.241]$\n- Spillover factor $s = 0.60$\n- Compositional shift indicator $c = 0$\n\nCase $4$ (edge case with minimal pre-policy months and compositional shift):\n- Treated pre-policy months: $[0.27, 0.270, 0.271]$\n- Treated post-policy months: $[0.240, 0.241, 0.242]$\n- Control pre-policy months: $[0.27, 0.270, 0.271]$\n- Control post-policy months: $[0.272, 0.273, 0.274]$\n- Spillover factor $s = 0.15$\n- Compositional shift indicator $c = 1$\n\nFinal output format requirement:\nYour program should produce a single line containing a list of the four bracketed triples for the above cases, in order: $[[e_1, p_1, i_1],[e_2, p_2, i_2],[e_3, p_3, i_3],[e_4, p_4, i_4]]$, where $e_k$ is the effect rounded to $4$ decimal places, $p_k$ is the parallel trends boolean, and $i_k$ is the causal inference boolean.",
            "solution": "The user-provided problem has been assessed and is valid. It is scientifically grounded in established principles of causal inference and statistics, well-posed with a clear objective and sufficient data, and free from subjective or unverifiable claims. The solution proceeds in three stages: derivation of the causal estimator, formulation of the statistical test for the parallel trends assumption, and statement of the criteria for credible inference.\n\n### 1. Derivation of the Average Treatment Effect on the Treated (ATT) Estimator\n\nThe problem is framed within the potential outcomes framework. Let $Y_{g,t}$ be the observed resistance rate for group $g$ in time period $t$. The groups are $g \\in \\{T, C\\}$, where $T$ denotes the treated group and $C$ denotes the control group. The time periods are $t \\in \\{pre, post\\}$, representing the pre-policy and post-policy eras.\n\nUnder this framework, for each unit (group-time observation), there exists a pair of potential outcomes: $Y_{g,t}(1)$ if the unit is exposed to the treatment (the antibiotic restriction policy) and $Y_{g,t}(0)$ if the unit is not exposed. The treatment is implemented for the treated group in the post-policy period only. The treatment indicator, $D_{g,t}$, is therefore:\n- $D_{T, post} = 1$\n- $D_{T, pre} = 0$\n- $D_{C, post} = 0$\n- $D_{C, pre} = 0$\n\nThe consistency assumption, part of the Stable Unit Treatment Value Assumption (SUTVA), links potential outcomes to observed outcomes: $Y_{g,t} = Y_{g,t}(D_{g,t})$. This gives:\n- $\\bar{Y}_{T, post} = E[Y_{T, post}(1)]$ (Observed mean for treated group, post-policy)\n- $\\bar{Y}_{T, pre} = E[Y_{T, pre}(0)]$ (Observed mean for treated group, pre-policy)\n- $\\bar{Y}_{C, post} = E[Y_{C, post}(0)]$ (Observed mean for control group, post-policy)\n- $\\bar{Y}_{C, pre} = E[Y_{C, pre}(0)]$ (Observed mean for control group, pre-policy)\n\nThe parameter of interest is the Average Treatment Effect on the Treated (ATT), which is the causal effect of the policy on the group that received it. In the post-policy period, this is defined as:\n$$\nATT = E[Y_{T, post}(1) - Y_{T, post}(0)]\n$$\nThe first term, $E[Y_{T, post}(1)]$, is the expected outcome for the treated group under treatment, which is observed. We can estimate it with the sample mean $\\bar{Y}_{T, post}$. The second term, $E[Y_{T, post}(0)]$, is the counterfactual: the expected outcome for the treated group in the post-policy period had they *not* received the treatment. This is unobservable.\n\nTo estimate this counterfactual, we invoke the key identifying assumption of difference-in-differences (DiD): the parallel trends assumption. The problem states this as an \"additive structure of group and time shocks... in the absence of treatment.\" This implies that the expected change in the outcome over time for the treated group, had it not been treated, would have been the same as the observed change in the control group. Formally:\n$$\nE[Y_{T, post}(0) - Y_{T, pre}(0)] = E[Y_{C, post}(0) - Y_{C, pre}(0)]\n$$\nWe can rearrange this to solve for the unobservable counterfactual term:\n$$\nE[Y_{T, post}(0)] = E[Y_{T, pre}(0)] + \\left( E[Y_{C, post}(0)] - E[Y_{C, pre}(0)] \\right)\n$$\nNow, we substitute this expression back into the definition of ATT:\n$$\nATT = E[Y_{T, post}(1)] - \\left( E[Y_{T, pre}(0)] + E[Y_{C, post}(0)] - E[Y_{C, pre}(0)] \\right)\n$$\nRearranging the terms gives the DiD structure:\n$$\nATT = \\left( E[Y_{T, post}(1)] - E[Y_{T, pre}(0)] \\right) - \\left( E[Y_{C, post}(0)] - E[Y_{C, pre}(0)] \\right)\n$$\nThis is the difference in the change over time for the treated group minus the change over time for the control group. Each expectation is estimated by its corresponding sample mean from the data. The DiD estimator, $\\widehat{ATT}$, is:\n$$\n\\widehat{ATT} = (\\bar{Y}_{T, post} - \\bar{Y}_{T, pre}) - (\\bar{Y}_{C, post} - \\bar{Y}_{C, pre})\n$$\nThis estimator will be implemented to compute the effect size.\n\n### 2. Parallel Trends Hypothesis Test\n\nAn explicit test of the parallel trends assumption is not possible because it involves the counterfactual. However, we can test for parallel trends in the pre-policy period, which provides supportive evidence for the plausibility of the assumption. The problem specifies fitting a simple linear model to the pre-policy data for each group:\n$$\nY_{g,t} = \\alpha_g + \\beta_g t + \\varepsilon_{g,t}\n$$\nwhere $g \\in \\{T, C\\}$, $t$ is the month index starting at $t=0$, and $\\varepsilon_{g,t}$ is a random error term. The slope parameter $\\beta_g$ represents the trend for group $g$. The parallel trends assumption implies that the trends should be equal, i.e., $\\beta_T = \\beta_C$. We test the null hypothesis $H_0: \\beta_T - \\beta_C = 0$ against the two-sided alternative $H_1: \\beta_T - \\beta_C \\neq 0$.\n\nFor each group $g$, we estimate the slope $\\beta_g$ using Ordinary Least Squares (OLS). The estimator $\\hat{\\beta}_g$ and its variance are:\n$$\n\\hat{\\beta}_g = \\frac{\\sum_{i=1}^{N_g} (t_i - \\bar{t}_g)(Y_{g,i} - \\bar{Y}_g)}{\\sum_{i=1}^{N_g} (t_i - \\bar{t}_g)^2}\n$$\n$$\n\\widehat{Var}(\\hat{\\beta}_g) = \\frac{\\hat{\\sigma}_g^2}{\\sum_{i=1}^{N_g} (t_i - \\bar{t}_g)^2} \\quad \\text{where} \\quad \\hat{\\sigma}_g^2 = \\frac{1}{N_g - 2} \\sum_{i=1}^{N_g} (Y_{g,i} - \\hat{Y}_{g,i})^2\n$$\nHere, $N_g$ is the number of pre-policy months for group $g$, and $\\hat{Y}_{g,i}$ are the fitted values from the regression.\n\nThe test for the difference in slopes uses a t-statistic:\n$$\nT = \\frac{(\\hat{\\beta}_T - \\hat{\\beta}_C) - 0}{SE(\\hat{\\beta}_T - \\hat{\\beta}_C)} = \\frac{\\hat{\\beta}_T - \\hat{\\beta}_C}{\\sqrt{\\widehat{Var}(\\hat{\\beta}_T) + \\widehat{Var}(\\hat{\\beta}_C)}}\n$$\nSince the residual variances $\\sigma_T^2$ and $\\sigma_C^2$ may not be equal, we do not pool them. The degrees of freedom for the t-distribution are approximated using the Welch-Satterthwaite equation:\n$$\n\\nu \\approx \\frac{\\left( \\widehat{Var}(\\hat{\\beta}_T) + \\widehat{Var}(\\hat{\\beta}_C) \\right)^2}{\\frac{\\left(\\widehat{Var}(\\hat{\\beta}_T)\\right)^2}{N_T - 2} + \\frac{\\left(\\widehat{Var}(\\hat{\\beta}_C)\\right)^2}{N_C - 2}}\n$$\nThe null hypothesis $H_0$ is not rejected if the calculated p-value for the test statistic $T$ with $\\nu$ degrees of freedom is greater than or equal to the significance level $\\alpha = 0.05$.\n\n### 3. Credibility of Causal Inference\n\nThe credibility of the causal inference derived from this analysis depends on three conditions being met simultaneously:\n1.  **Parallel Trends Support**: The hypothesis test described above must not reject the null hypothesis of equal pre-policy trends at the $\\alpha=0.05$ significance level.\n2.  **Limited Spillovers**: The SUTVA assumption, particularly the \"no interference between units\" clause, must be plausible. The provided spillover factor $s$ is a proxy for this, with a threshold of $s \\le 0.2$ required for credibility.\n3.  **Stable Composition**: The groups must be comparable over time. A major compositional shift, indicated by $c=1$, would violate this. Thus, we require $c=0$.\n\nInference is declared credible only if all three conditions are satisfied.\n\nThe Python implementation will carry out these three sets of calculations for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    # Test suite data as provided in the problem statement.\n    test_cases = [\n        {\n            \"treated_pre\": [0.25, 0.251, 0.249, 0.252, 0.253, 0.254],\n            \"treated_post\": [0.205, 0.206, 0.204, 0.207, 0.208, 0.209],\n            \"control_pre\": [0.26, 0.261, 0.259, 0.262, 0.263, 0.264],\n            \"control_post\": [0.265, 0.266, 0.264, 0.267, 0.268, 0.269],\n            \"spillover\": 0.10,\n            \"composition_shift\": 0,\n        },\n        {\n            \"treated_pre\": [0.30, 0.301, 0.299, 0.300, 0.302, 0.301],\n            \"treated_post\": [0.275, 0.276, 0.274, 0.275, 0.277, 0.276],\n            \"control_pre\": [0.28, 0.284, 0.288, 0.292, 0.296, 0.300],\n            \"control_post\": [0.305, 0.309, 0.313, 0.317, 0.321, 0.325],\n            \"spillover\": 0.05,\n            \"composition_shift\": 0,\n        },\n        {\n            \"treated_pre\": [0.22, 0.221, 0.223, 0.224, 0.225, 0.226],\n            \"treated_post\": [0.226, 0.227, 0.228, 0.229, 0.230, 0.231],\n            \"control_pre\": [0.23, 0.231, 0.232, 0.233, 0.234, 0.235],\n            \"control_post\": [0.236, 0.237, 0.238, 0.239, 0.240, 0.241],\n            \"spillover\": 0.60,\n            \"composition_shift\": 0,\n        },\n        {\n            \"treated_pre\": [0.27, 0.270, 0.271],\n            \"treated_post\": [0.240, 0.241, 0.242],\n            \"control_pre\": [0.27, 0.270, 0.271],\n            \"control_post\": [0.272, 0.273, 0.274],\n            \"spillover\": 0.15,\n            \"composition_shift\": 1,\n        },\n    ]\n\n    results = []\n    for case_data in test_cases:\n        result_triple = _solve_one_case(case_data)\n        results.append(result_triple)\n    \n    # Format the final output string exactly as required.\n    # The string representation of a Python list of lists is the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _fit_linear_model(y_pre):\n    \"\"\"\n    Fits a simple linear model Y = alpha + beta*t + epsilon to pre-policy data.\n    \n    Args:\n        y_pre (list or np.ndarray): The observed resistance rates for the pre-policy period.\n\n    Returns:\n        tuple: A tuple containing:\n            - slope (float): The estimated slope (beta_hat).\n            - var_slope (float): The estimated variance of the slope.\n            - n (int): The number of observations.\n    \"\"\"\n    n = len(y_pre)\n    # The model requires at least 3 points to estimate residual variance with 2 parameters.\n    # The problem provides cases with n>=3.\n    if n = 2:\n        return np.nan, np.nan, n\n\n    t_vec = np.arange(n)\n    y_vec = np.array(y_pre)\n\n    # Fit a polynomial of degree 1 (a line) using np.polyfit for robustness.\n    slope, intercept = np.polyfit(t_vec, y_vec, 1)\n    \n    # Calculate residuals to estimate variance.\n    y_pred = intercept + slope * t_vec\n    residuals = y_vec - y_pred\n    \n    # Residual variance (sigma_hat^2) with N-k-1 degrees of freedom (here N-2).\n    df = n - 2\n    residual_variance = np.sum(residuals**2) / df if df > 0 else 0\n    \n    # Variance of the slope estimator.\n    t_mean = np.mean(t_vec)\n    ss_t = np.sum((t_vec - t_mean)**2)\n    var_slope = residual_variance / ss_t if ss_t > 0 else np.inf\n    \n    return slope, var_slope, n\n\ndef _solve_one_case(case_data):\n    \"\"\"\n    Computes the three required outputs for a single test case.\n    \"\"\"\n    t_pre = np.array(case_data[\"treated_pre\"])\n    t_post = np.array(case_data[\"treated_post\"])\n    c_pre = np.array(case_data[\"control_pre\"])\n    c_post = np.array(case_data[\"control_post\"])\n    s = case_data[\"spillover\"]\n    c = case_data[\"composition_shift\"]\n    \n    # --- Part 1: Estimate ATT using Difference-in-Differences ---\n    mean_t_pre = np.mean(t_pre)\n    mean_t_post = np.mean(t_post)\n    mean_c_pre = np.mean(c_pre)\n    mean_c_post = np.mean(c_post)\n    \n    # ATT_hat = (Y_T_post - Y_T_pre) - (Y_C_post - Y_C_pre)\n    att_estimate = (mean_t_post - mean_t_pre) - (mean_c_post - mean_c_pre)\n    effect = round(att_estimate, 4)\n\n    # --- Part 2: Assess Parallel Trends Assumption ---\n    beta_t, var_beta_t, n_t = _fit_linear_model(t_pre)\n    beta_c, var_beta_c, n_c = _fit_linear_model(c_pre)\n    \n    # Perform Welch's t-test for the difference in pre-policy slopes\n    beta_diff = beta_t - beta_c\n    se_diff = np.sqrt(var_beta_t + var_beta_c)\n    \n    # Handle cases with perfect fits (zero variance) or identical trends.\n    if se_diff == 0:\n        # If std error is 0, slopes must be deterministic. If they are also equal,\n        # there is no evidence against H0. If not equal, infinite evidence against.\n        t_stat = 0.0 if beta_diff == 0 else np.inf\n    else:\n        t_stat = beta_diff / se_diff\n\n    # Welch-Satterthwaite approximation for degrees of freedom\n    df_t = n_t - 2\n    df_c = n_c - 2\n    \n    # Avoid division by zero if df is 0 or variances are 0.\n    ws_dof_den = ((var_beta_t**2) / df_t) + ((var_beta_c**2) / df_c) if (df_t > 0 and df_c > 0) else 0\n\n    if ws_dof_den == 0:\n        # This case is approached if variances are zero. Assume high DoF as there's no variability to penalize.\n        ws_dof = float('inf') \n    else:\n        ws_dof_num = (var_beta_t + var_beta_c)**2\n        ws_dof = ws_dof_num / ws_dof_den\n\n    # Two-sided test at alpha = 0.05\n    alpha = 0.05\n    p_value = 2 * t.sf(np.abs(t_stat), df=ws_dof)\n    \n    parallel_ok = p_value >= alpha\n\n    # --- Part 3: Assess Causal Inference Credibility ---\n    is_s_ok = s = 0.2\n    is_c_ok = c == 0\n    \n    inference_ok = parallel_ok and is_s_ok and is_c_ok\n    \n    return [effect, parallel_ok, inference_ok]\n\nsolve()\n```"
        }
    ]
}