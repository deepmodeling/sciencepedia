{
    "hands_on_practices": [
        {
            "introduction": "Before an mHealth intervention can be scaled up, its effectiveness must be rigorously proven, often through a Randomized Controlled Trial (RCT). This exercise guides you through the crucial process of calculating the required sample size for a *cluster* RCT, where groups of individuals (like clinics) are randomized instead of the individuals themselves. By deriving the formula that accounts for the intracluster correlation coefficient ($\\rho$), you will gain a fundamental understanding of the 'design effect' and how it impacts the statistical power of studies in real-world public health settings .",
            "id": "4973577",
            "problem": "A Ministry of Health is evaluating a Mobile Health (mHealth) decision-support application for hypertension management in primary care clinics across a middle-income country. They plan a two-arm, parallel, cluster Randomized Controlled Trial (RCT) with equal numbers of clinics per arm and equal cluster size. The primary outcome is the change in systolic blood pressure measured in millimeters of mercury, modeled as a continuous variable. Within each clinic, individual outcomes are positively correlated due to shared providers and local workflow; this correlation is captured by the intracluster correlation coefficient (ICC), denoted by $\\rho$.\n\nAssume the following fundamental base:\n- Individuals within the same clinic have identical marginal variance $\\sigma^{2}$ and pairwise covariance $\\rho \\sigma^{2}$; outcomes from different clinics are independent.\n- The difference in arm means is tested using a two-sided normal approximation with significance level $\\alpha$ and power $1 - \\beta$.\n- Let $m$ denote the number of individuals per clinic and $J$ the number of clinics per arm. Let $\\delta$ be the true difference in arm means to be detected.\n\nTasks:\n1. Starting from the variance definition for sums of correlated random variables and the normal approximation for a two-sample mean difference test, derive the sample size formula for the required number of clinics per arm, $J$, as a function of $\\sigma^{2}$, $\\rho$, $m$, $\\delta$, $\\alpha$, and $\\beta$. Your derivation must explicitly show how the intracluster correlation coefficient inflates the variance via the design effect induced by clustering.\n2. Using the derived formula, compute the required number of clinics per arm for the following scientifically plausible parameters for this mHealth evaluation:\n   - Two-sided significance level $\\alpha = 0.05$ and power $1 - \\beta = 0.9$,\n   - Standard deviation $\\sigma = 12$,\n   - Intracluster correlation coefficient $\\rho = 0.02$,\n   - Equal cluster size $m = 35$,\n   - Target mean difference $\\delta = 4$.\nReport the required $J$ as the smallest integer greater than or equal to the computed value. Provide the final answer as a single number with no units in the box.",
            "solution": "We model the outcome for individual $i$ in clinic (cluster) $j$ in arm $a \\in \\{T, C\\}$ as $Y_{a, j i}$, with $\\mathbb{E}[Y_{a, j i}] = \\mu_{a}$ and $\\operatorname{Var}(Y_{a, j i}) = \\sigma^{2}$. For any two distinct individuals $i \\neq i'$ in the same clinic $j$, we assume $\\operatorname{Cov}(Y_{a, j i}, Y_{a, j i'}) = \\rho \\sigma^{2}$, and for individuals in different clinics, covariance is $0$. Let there be $J$ clinics per arm and $m$ individuals per clinic, so the arm-level sample size is $n = J m$ individuals per arm.\n\nDefine the arm mean as\n$$\n\\bar{Y}_{a} = \\frac{1}{J m} \\sum_{j=1}^{J} \\sum_{i=1}^{m} Y_{a, j i}.\n$$\nWe first derive $\\operatorname{Var}(\\bar{Y}_{a})$ under clustering. Using the variance of a sum,\n$$\n\\operatorname{Var}\\!\\left(\\sum_{j=1}^{J} \\sum_{i=1}^{m} Y_{a, j i}\\right)\n= \\sum_{j=1}^{J} \\left[ \\sum_{i=1}^{m} \\operatorname{Var}(Y_{a, j i}) + \\sum_{\\substack{i, i' = 1 \\\\ i \\neq i'}}^{m} \\operatorname{Cov}(Y_{a, j i}, Y_{a, j i'}) \\right].\n$$\nInside each clinic $j$, there are $m$ variance terms $\\sigma^{2}$ and $m(m-1)$ pairwise covariance terms $\\rho \\sigma^{2}$. Therefore,\n$$\n\\operatorname{Var}\\!\\left(\\sum_{j=1}^{J} \\sum_{i=1}^{m} Y_{a, j i}\\right)\n= \\sum_{j=1}^{J} \\left[ m \\sigma^{2} + m (m-1) \\rho \\sigma^{2} \\right]\n= J m \\sigma^{2} \\left[ 1 + (m-1) \\rho \\right].\n$$\nDividing by $(J m)^{2}$ gives the variance of the arm mean,\n$$\n\\operatorname{Var}(\\bar{Y}_{a}) = \\frac{\\sigma^{2} \\left[ 1 + (m-1) \\rho \\right]}{J m}.\n$$\nThe difference in arm means $\\bar{Y}_{T} - \\bar{Y}_{C}$ has variance\n$$\n\\operatorname{Var}(\\bar{Y}_{T} - \\bar{Y}_{C})\n= \\operatorname{Var}(\\bar{Y}_{T}) + \\operatorname{Var}(\\bar{Y}_{C})\n= \\frac{2 \\sigma^{2} \\left[ 1 + (m-1) \\rho \\right]}{J m},\n$$\nusing independence between arms.\n\nFor a two-sided test at significance level $\\alpha$ with power $1 - \\beta$, the normal approximation requires the detectable difference $\\delta$ to satisfy\n$$\n\\delta = \\left( z_{1 - \\alpha/2} + z_{1 - \\beta} \\right) \\sqrt{ \\operatorname{Var}(\\bar{Y}_{T} - \\bar{Y}_{C}) },\n$$\nwhere $z_{p}$ is the $p$th quantile of the standard normal distribution. Substituting the variance,\n$$\n\\delta = \\left( z_{1 - \\alpha/2} + z_{1 - \\beta} \\right) \\sqrt{ \\frac{2 \\sigma^{2} \\left[ 1 + (m-1) \\rho \\right]}{J m} }.\n$$\nSolving for $J$,\n$$\nJ = \\frac{2 \\sigma^{2} \\left[ 1 + (m-1) \\rho \\right] \\left( z_{1 - \\alpha/2} + z_{1 - \\beta} \\right)^{2} }{ m \\, \\delta^{2} }.\n$$\nThis expression reveals the classical design effect\n$$\n\\text{Design effect} = 1 + (m-1) \\rho,\n$$\nmultiplying the variance relative to individual randomization. Equivalently, one may write $J = \\left( \\text{DE} \\times n_{\\text{indiv, per arm}} \\right) / m$, where $n_{\\text{indiv, per arm}} = \\frac{2 \\sigma^{2} \\left( z_{1 - \\alpha/2} + z_{1 - \\beta} \\right)^{2}}{\\delta^{2}}$ is the per-arm sample size under individual randomization, and $\\text{DE} = 1 + (m-1)\\rho$.\n\nWe now compute $J$ for the given parameters. With $\\alpha = 0.05$ (two-sided) and $1 - \\beta = 0.9$, we have $z_{1 - \\alpha/2} = z_{0.975} \\approx 1.96$ and $z_{1 - \\beta} = z_{0.9} \\approx 1.2816$. Let $\\sigma = 12$ so $\\sigma^{2} = 144$, $\\rho = 0.02$, $m = 35$, and $\\delta = 4$ so $\\delta^{2} = 16$.\n\nCompute the design effect:\n$$\n\\text{DE} = 1 + (m-1)\\rho = 1 + 34 \\times 0.02 = 1 + 0.68 = 1.68.\n$$\nCompute the quantile sum and its square:\n$$\nz_{1 - \\alpha/2} + z_{1 - \\beta} \\approx 1.96 + 1.2816 = 3.2416, \\quad (3.2416)^{2} \\approx 10.50797056.\n$$\nSubstitute into the formula:\n$$\nJ = \\frac{2 \\cdot 144 \\cdot 1.68 \\cdot 10.50797056}{35 \\cdot 16}.\n$$\nFirst, compute the numerator factor without $m$ and $\\delta^{2}$:\n$$\n2 \\cdot 144 \\cdot 10.50797056 = 288 \\cdot 10.50797056 \\approx 3026.293533,\n$$\nthen multiply by $\\text{DE} = 1.68$:\n$$\n3026.293533 \\times 1.68 \\approx 5084.179138.\n$$\nDivide by $m \\delta^{2} = 35 \\cdot 16 = 560$:\n$$\nJ \\approx \\frac{5084.179138}{560} \\approx 9.079.\n$$\nThe required number of clinics per arm is the smallest integer greater than or equal to $9.079$, which is $10$.",
            "answer": "$$\\boxed{10}$$"
        },
        {
            "introduction": "The promise of mHealth often relies on continuous monitoring, which presents a significant engineering challenge: how to power a device for weeks or months on a small battery. This practice places you in the role of an mHealth engineer designing a wearable sensor for long-term fever surveillance. You will apply the principles of duty-cycling and the Nyquist-Shannon Sampling Theorem to find the optimal balance between collecting high-fidelity data and conserving energy to ensure the device lasts for the entire study duration .",
            "id": "4973574",
            "problem": "A low-cost wearable for fever surveillance in a global health study must monitor skin temperature continuously over a $30$-day period using a duty-cycled sampling strategy to conserve battery. The wearable samples uniformly every $\\Delta t$ seconds. Each sample requires the temperature sensor and microcontroller to be powered for a fixed stabilization-and-read window of $\\tau$ seconds, during which the device draws a constant active current. Outside that window, the device remains in a low-power idle state. There is no wireless transmission during the study period; data are stored locally and uploaded after the study. Assume an ideal battery model with negligible self-discharge and constant voltage so that charge in milliampere-hours is conserved.\n\nYou are given the following scientifically plausible parameters:\n\n- Battery capacity is $300 \\text{ mAh}$.\n- Study duration is $30$ days.\n- Idle current draw is $0.05 \\text{ mA}$ whenever the device is not sampling.\n- Active current draw during each sampling window is $4.00 \\text{ mA}$.\n- Each sampling window duration is $\\tau = 2.00 \\text{ s}$.\n- The target temperature signal to be reconstructed is empirically band-limited to $B = \\frac{1}{1800} \\text{ Hz}$.\n\nUsing only fundamental definitions of average current under duty cycling and the Sampling Theorem (Shannon-Nyquist), do the following:\n\n1. Derive the inequality that constrains $\\Delta t$ so that the device completes the full $30$-day study without exhausting the battery. Express this constraint in terms of $\\tau$, the idle and active currents, the battery capacity, and the total study time.\n2. Derive the data-fidelity constraint on $\\Delta t$ implied by the Sampling Theorem for perfect reconstruction of a band-limited signal of bandwidth $B$ under uniform sampling.\n3. Define the optimal sensing schedule as the choice of $\\Delta t$ that maximizes data fidelity (i.e., minimizes $\\Delta t$) while still guaranteeing completion of the $30$-day study and respecting the fidelity constraint. Compute the optimal uniform sampling interval $\\Delta t^{\\ast}$ in seconds.\n\nExpress your final numerical answer for $\\Delta t^{\\ast}$ in seconds and round to four significant figures. Do not include units in your final answer box.",
            "solution": "The problem statement has been validated and is found to be scientifically grounded, well-posed, and objective. All provided parameters are physically plausible for a low-power wearable device, and the problem structure is self-contained and logically consistent. There are no contradictions, ambiguities, or violations of fundamental principles. We may therefore proceed with a formal solution.\n\nThe problem asks for three derivations related to determining the optimal sampling interval, $\\Delta t$, for a wearable temperature sensor. We will address each part in sequence.\n\nLet us define the given parameters symbolically:\n- Battery capacity: $Q_{cap} = 300 \\text{ mAh}$\n- Total study duration: $T_{study} = 30 \\text{ days}$\n- Idle current: $I_{idle} = 0.05 \\text{ mA}$\n- Active current: $I_{active} = 4.00 \\text{ mA}$\n- Sampling window duration: $\\tau = 2.00 \\text{ s}$\n- Signal bandwidth: $B = \\frac{1}{1800} \\text{ Hz}$\n\n**1. Battery Life Constraint**\n\nTo ensure the device operates for the entire study duration, the total charge consumed must not exceed the battery's capacity. The device operates on a duty cycle with period $\\Delta t$. In each cycle, the device is in an active state for a duration $\\tau$ and in an idle state for a duration $\\Delta t - \\tau$.\n\nThe average current draw, $I_{avg}$, can be calculated by averaging the current over one cycle:\n$$I_{avg} = \\frac{(I_{active} \\times \\tau) + (I_{idle} \\times (\\Delta t - \\tau))}{\\Delta t}$$\nThis expression can be rearranged as:\n$$I_{avg} = \\frac{I_{active}\\tau + I_{idle}\\Delta t - I_{idle}\\tau}{\\Delta t} = I_{idle} + (I_{active} - I_{idle})\\frac{\\tau}{\\Delta t}$$\nFor the device to last the entire study duration $T_{study}$, the total charge consumed, which is $I_{avg} \\times T_{study}$, must be less than or equal to the battery capacity $Q_{cap}$.\n$$I_{avg} \\times T_{study} \\le Q_{cap}$$\nHere, we must be consistent with units. Let currents be in milliamperes (mA), $Q_{cap}$ in milliampere-hours (mAh), and $T_{study}$ in hours (h). The ratio $\\tau/\\Delta t$ is dimensionless as both are in seconds.\n\nFirst, convert the study duration to hours:\n$$T_{study} = 30 \\text{ days} \\times 24 \\frac{\\text{h}}{\\text{day}} = 720 \\text{ h}$$\nThe maximum permissible average current is thus $I_{avg,max} = \\frac{Q_{cap}}{T_{study}}$.\nSubstituting the expression for $I_{avg}$ into the energy constraint:\n$$\\left(I_{idle} + (I_{active} - I_{idle})\\frac{\\tau}{\\Delta t}\\right) \\times T_{study} \\le Q_{cap}$$\nDividing by $T_{study}$:\n$$I_{idle} + (I_{active} - I_{idle})\\frac{\\tau}{\\Delta t} \\le \\frac{Q_{cap}}{T_{study}}$$\nTo derive the constraint on $\\Delta t$, we isolate the term containing it:\n$$(I_{active} - I_{idle})\\frac{\\tau}{\\Delta t} \\le \\frac{Q_{cap}}{T_{study}} - I_{idle}$$\nSince $I_{active} > I_{idle}$ and $\\tau > 0$, the term $(I_{active} - I_{idle})\\tau$ is a positive constant. For a solution to exist, the right-hand side must also be positive, which means the device must draw less current on average than the idle current, a condition met here. We can thus take the reciprocal of both sides, which reverses the inequality:\n$$\\frac{\\Delta t}{(I_{active} - I_{idle})\\tau} \\ge \\frac{1}{\\frac{Q_{cap}}{T_{study}} - I_{idle}}$$\nFinally, we arrive at the inequality for $\\Delta t$ based on battery life:\n$$\\Delta t \\ge \\frac{(I_{active} - I_{idle})\\tau}{\\frac{Q_{cap}}{T_{study}} - I_{idle}}$$\nThis is the first required derivation. This inequality sets a minimum value for the sampling interval, $\\Delta t_{min,battery}$, below which the battery will be exhausted before the study concludes.\n\n**2. Data Fidelity Constraint**\n\nThe second constraint comes from the Nyquist-Shannon Sampling Theorem. The theorem states that to allow for perfect reconstruction of a continuous-time signal that is band-limited to a frequency $B$, the sampling frequency, $f_s$, must be at least twice the bandwidth.\n$$f_s \\ge 2B$$\nThe sampling frequency $f_s$ is the reciprocal of the uniform sampling interval $\\Delta t$:\n$$f_s = \\frac{1}{\\Delta t}$$\nSubstituting this into the Nyquist criterion gives the constraint on $\\Delta t$:\n$$\\frac{1}{\\Delta t} \\ge 2B$$\nSolving for $\\Delta t$ by taking the reciprocal of both sides (which reverses the inequality):\n$$\\Delta t \\le \\frac{1}{2B}$$\nThis is the second required derivation. This inequality sets a maximum value for the sampling interval, $\\Delta t_{max,fidelity}$, above which aliasing will occur, preventing perfect reconstruction of the signal.\n\n**3. Optimal Uniform Sampling Interval, $\\Delta t^{\\ast}$**\n\nThe problem defines the optimal sensing schedule as the choice of $\\Delta t$ that maximizes data fidelity while satisfying both the battery and fidelity constraints. Maximizing data fidelity is explicitly equated to minimizing the sampling interval $\\Delta t$.\n\nThe two constraints define a valid range for $\\Delta t$:\n$$\\frac{(I_{active} - I_{idle})\\tau}{\\frac{Q_{cap}}{T_{study}} - I_{idle}} \\le \\Delta t \\le \\frac{1}{2B}$$\nLet $\\Delta t_{min,battery} = \\frac{(I_{active} - I_{idle})\\tau}{\\frac{Q_{cap}}{T_{study}} - I_{idle}}$ and $\\Delta t_{max,fidelity} = \\frac{1}{2B}$.\nThe goal is to find the minimum possible $\\Delta t$ within the valid range $[\\Delta t_{min,battery}, \\Delta t_{max,fidelity}]$, provided this interval is non-empty (i.e., $\\Delta t_{min,battery} \\le \\Delta t_{max,fidelity}$). The optimal value, $\\Delta t^{\\ast}$, that minimizes $\\Delta t$ is therefore the lower bound of this interval:\n$$\\Delta t^{\\ast} = \\Delta t_{min,battery} = \\frac{(I_{active} - I_{idle})\\tau}{\\frac{Q_{cap}}{T_{study}} - I_{idle}}$$\nWe now compute the numerical values for the bounds to verify feasibility and find the answer.\n\nUsing the given parameters:\n- $\\Delta t_{max,fidelity} = \\frac{1}{2B} = \\frac{1}{2 \\times \\frac{1}{1800} \\text{ Hz}} = \\frac{1800}{2} \\text{ s} = 900 \\text{ s}$\n\n- For $\\Delta t_{min,battery}$:\n  The maximum average current sustainable is $\\frac{Q_{cap}}{T_{study}} = \\frac{300 \\text{ mAh}}{720 \\text{ h}} = \\frac{5}{12} \\text{ mA}$.\n  The numerator is $(I_{active} - I_{idle})\\tau = (4.00 \\text{ mA} - 0.05 \\text{ mA}) \\times 2.00 \\text{ s} = 3.95 \\text{ mA} \\times 2.00 \\text{ s} = 7.90 \\text{ mA} \\cdot \\text{s}$.\n  The denominator is $\\frac{Q_{cap}}{T_{study}} - I_{idle} = \\frac{5}{12} \\text{ mA} - 0.05 \\text{ mA} = \\left(\\frac{5}{12} - \\frac{1}{20}\\right) \\text{ mA} = \\left(\\frac{25 - 3}{60}\\right) \\text{ mA} = \\frac{22}{60} \\text{ mA} = \\frac{11}{30} \\text{ mA}$.\n\n  Therefore,\n  $$\\Delta t^{\\ast} = \\frac{7.90 \\text{ mA} \\cdot \\text{s}}{\\frac{11}{30} \\text{ mA}} = \\frac{7.90 \\times 30}{11} \\text{ s} = \\frac{237}{11} \\text{ s}$$\n\nA valid solution exists because $\\Delta t^{\\ast} = \\frac{237}{11} \\text{ s} \\approx 21.55 \\text{ s}$, which is less than $\\Delta t_{max,fidelity} = 900 \\text{ s}$. The optimal sampling interval is the smallest possible time that satisfies the energy constraint.\n\nThe final numerical value for $\\Delta t^{\\ast}$ is:\n$$\\Delta t^{\\ast} = \\frac{237}{11} \\text{ s} \\approx 21.545454... \\text{ s}$$\nRounding to four significant figures as requested, we get $21.55$ seconds.",
            "answer": "$$\\boxed{21.55}$$"
        },
        {
            "introduction": "A predictive model is only as valuable as the decisions it supports. This problem tackles the critical final step of deploying a machine learning model in a clinical setting: choosing a decision threshold that maximizes clinical utility. Starting from the principles of Bayes decision theory, you will derive an optimal risk score threshold that maximizes the Net Benefit, a metric that explicitly balances the benefit of true positives against the harm of false positives . This exercise demonstrates how to translate a model's probabilistic output into a concrete, clinically-informed action.",
            "id": "4973582",
            "problem": "A Mobile Health (mHealth) application deploys a binary risk model on smartphones to triage patients for an acute event within a $7$-day window based on wearable sensor data. The model outputs a calibrated log-odds score $z \\in \\mathbb{R}$ for the event, where larger $z$ indicates higher risk. The population event prevalence is $\\pi$, and the developer faces class imbalance because the event is rare.\n\nTo address imbalance during model development, the team considers a weighted empirical risk with class weights. At deployment, a clinical stakeholder specifies a decision-analytic preference through a decision curve: acting on a positive prediction yields net benefit that trades off false positives against true positives according to a threshold probability $p_{t}$, with net benefit defined as\n$$\n\\mathrm{NB}(p_{t}; \\text{policy}) \\equiv \\frac{\\mathrm{TP}}{N} - \\frac{p_{t}}{1-p_{t}} \\cdot \\frac{\\mathrm{FP}}{N},\n$$\nwhere $N$ is the cohort size, $\\mathrm{TP}$ is the number of true positives, and $\\mathrm{FP}$ is the number of false positives. Assume perfect probability calibration of the model scores and that the decision maker fixes $p_{t}$ in advance.\n\nAssume that, conditional on the true class $Y \\in \\{0,1\\}$, the deployed score $z$ follows a Gaussian distribution with equal variance,\n$$\nz \\mid (Y=1) \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2}) \\quad \\text{and} \\quad z \\mid (Y=0) \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2}),\n$$\nwith $\\mu_{1} > \\mu_{0}$. Let the triage policy be a deterministic threshold on $z$: predict positive if and only if $z \\ge z_{\\ast}$.\n\nTasks:\n1. Starting from the definitions of expected loss and Bayes decision theory, formalize class imbalance handling via weighted misclassification costs and show that maximizing $\\mathrm{NB}(p_{t}; z_{\\ast})$ over thresholds $z_{\\ast}$ is equivalent to minimizing an expected cost with false negative cost equal to $1$ and false positive cost equal to $w \\equiv \\frac{p_{t}}{1-p_{t}}$. Derive, from first principles, the optimal likelihood-ratio threshold and express the corresponding optimal threshold on $z$ in terms of $\\mu_{0}$, $\\mu_{1}$, \\sigma^{2}, \\pi$, and $p_{t}$.\n2. For a deployment in which the population prevalence is $\\pi = 0.04$, the stakeholder sets $p_{t} = 0.1$, and the score distributions are parameterized by $\\mu_{1} = 1.2$, $\\mu_{0} = -0.3$, and $\\sigma = 0.9$, compute the optimal score threshold $z_{\\ast}$ that maximizes net benefit at the specified decision preference.\n\nRound your final numerical answer for $z_{\\ast}$ to four significant figures. No units are required.",
            "solution": "The user-provided problem is validated as sound and well-posed. It is grounded in standard statistical decision theory and its application to medical diagnostics, a core topic in digital health. All necessary information is provided, and the problem is internally consistent. The tasks are clearly defined and lead to a unique, meaningful solution.\n\n### Part 1: Derivation of the Optimal Threshold\n\nThe first task is to formalize the equivalence between maximizing the Net Benefit, $\\mathrm{NB}(p_{t})$, and minimizing a weighted expected misclassification cost, and then to derive the optimal decision threshold $z_{\\ast}$.\n\nLet $Y \\in \\{0, 1\\}$ be the true class label, where $Y=1$ denotes the occurrence of the event and $Y=0$ denotes its absence. The population prevalence of the event is $P(Y=1) = \\pi$. The decision policy is to predict an event if the score $z$ exceeds a threshold $z_{\\ast}$.\n\nThe Net Benefit is defined as:\n$$\n\\mathrm{NB}(p_{t}; z_{\\ast}) \\equiv \\frac{\\mathrm{TP}(z_{\\ast})}{N} - \\frac{p_{t}}{1-p_{t}} \\cdot \\frac{\\mathrm{FP}(z_{\\ast})}{N}\n$$\nwhere $N$ is the total number of individuals, $\\mathrm{TP}(z_{\\ast})$ is the number of true positives, and $\\mathrm{FP}(z_{\\ast})$ is the number of false positives for a given threshold $z_{\\ast}$.\n\nWe can express $\\mathrm{TP}(z_{\\ast})$ and $\\mathrm{FP}(z_{\\ast})$ in terms of the true positive rate ($\\mathrm{TPR}$) and false positive rate ($\\mathrm{FPR}$):\n$\\mathrm{TP}(z_{\\ast}) = N \\cdot \\pi \\cdot \\mathrm{TPR}(z_{\\ast})$, where $\\mathrm{TPR}(z_{\\ast}) = P(z \\ge z_{\\ast} \\mid Y=1)$.\n$\\mathrm{FP}(z_{\\ast}) = N \\cdot (1-\\pi) \\cdot \\mathrm{FPR}(z_{\\ast})$, where $\\mathrm{FPR}(z_{\\ast}) = P(z \\ge z_{\\ast} \\mid Y=0)$.\n\nSubstituting these into the $\\mathrm{NB}$ equation gives:\n$$\n\\mathrm{NB}(p_{t}; z_{\\ast}) = \\frac{N \\pi \\mathrm{TPR}(z_{\\ast})}{N} - \\frac{p_{t}}{1-p_{t}} \\frac{N (1-\\pi) \\mathrm{FPR}(z_{\\ast})}{N} = \\pi \\mathrm{TPR}(z_{\\ast}) - \\frac{p_{t}}{1-p_{t}} (1-\\pi) \\mathrm{FPR}(z_{\\ast})\n$$\n\nNow, consider the expected cost (or risk) of misclassification for a decision rule. Let $C_{FN}$ be the cost of a false negative (predicting $0$ when $Y=1$) and $C_{FP}$ be the cost of a false positive (predicting $1$ when $Y=0$). The costs of true positives and true negatives are assumed to be $0$. The expected cost, $E[\\text{Cost}]$, is:\n$$\nE[\\text{Cost}(z_{\\ast})] = C_{FP} P(\\text{predict } 1, Y=0) + C_{FN} P(\\text{predict } 0, Y=1)\n$$\nUsing conditional probabilities:\n$$\nE[\\text{Cost}(z_{\\ast})] = C_{FP} P(z \\ge z_{\\ast} \\mid Y=0) P(Y=0) + C_{FN} P(z < z_{\\ast} \\mid Y=1) P(Y=1)\n$$\nLet FNR be the false negative rate, $\\mathrm{FNR}(z_{\\ast}) = P(z < z_{\\ast} \\mid Y=1) = 1 - \\mathrm{TPR}(z_{\\ast})$.\nThe expected cost is:\n$$\nE[\\text{Cost}(z_{\\ast})] = C_{FP} (1-\\pi) \\mathrm{FPR}(z_{\\ast}) + C_{FN} \\pi \\mathrm{FNR}(z_{\\ast})\n$$\n$$\nE[\\text{Cost}(z_{\\ast})] = C_{FP} (1-\\pi) \\mathrm{FPR}(z_{\\ast}) + C_{FN} \\pi (1 - \\mathrm{TPR}(z_{\\ast}))\n$$\nThe problem asks to show equivalence with $C_{FN}=1$ and $C_{FP} = w \\equiv \\frac{p_{t}}{1-p_{t}}$. Substituting these costs:\n$$\nE[\\text{Cost}(z_{\\ast})] = \\frac{p_{t}}{1-p_{t}} (1-\\pi) \\mathrm{FPR}(z_{\\ast}) + 1 \\cdot \\pi (1 - \\mathrm{TPR}(z_{\\ast}))\n$$\n$$\nE[\\text{Cost}(z_{\\ast})] = \\pi - \\left( \\pi \\mathrm{TPR}(z_{\\ast}) - \\frac{p_{t}}{1-p_{t}} (1-\\pi) \\mathrm{FPR}(z_{\\ast}) \\right)\n$$\n$$\nE[\\text{Cost}(z_{\\ast})] = \\pi - \\mathrm{NB}(p_{t}; z_{\\ast})\n$$\nSince $\\pi$ is a constant with respect to the choice of threshold $z_{\\ast}$, minimizing the expected cost $E[\\text{Cost}(z_{\\ast})]$ is equivalent to maximizing the Net Benefit $\\mathrm{NB}(p_{t}; z_{\\ast})$. This establishes the required equivalence.\n\nNext, we derive the optimal threshold $z_{\\ast}$ from first principles. According to Bayes decision theory, the optimal rule is to choose the class that minimizes the posterior expected cost. For any given score $z$, we predict class $1$ if the expected cost of doing so is less than the expected cost of predicting class $0$.\n\n$\\text{Cost}(\\text{predict } 1 \\mid z) = 0 \\cdot P(Y=1 \\mid z) + C_{FP} \\cdot P(Y=0 \\mid z) = C_{FP} P(Y=0 \\mid z)$\n$\\text{Cost}(\\text{predict } 0 \\mid z) = C_{FN} \\cdot P(Y=1 \\mid z) + 0 \\cdot P(Y=0 \\mid z) = C_{FN} P(Y=1 \\mid z)$\n\nPredict class $1$ if $\\text{Cost}(\\text{predict } 1 \\mid z) < \\text{Cost}(\\text{predict } 0 \\mid z)$:\n$$\nC_{FP} P(Y=0 \\mid z) < C_{FN} P(Y=1 \\mid z)\n$$\nUsing Bayes' rule, $P(Y=k \\mid z) = \\frac{p(z \\mid Y=k) P(Y=k)}{p(z)}$, this becomes:\n$$\nC_{FP} p(z \\mid Y=0) (1-\\pi) < C_{FN} p(z \\mid Y=1) \\pi\n$$\nRearranging gives the likelihood-ratio test: predict class $1$ if\n$$\n\\frac{p(z \\mid Y=1)}{p(z \\mid Y=0)} > \\frac{C_{FP} (1-\\pi)}{C_{FN} \\pi}\n$$\nThe optimal threshold $z_{\\ast}$ is found where equality holds. Substituting $C_{FN}=1$ and $C_{FP} = \\frac{p_{t}}{1-p_{t}}$:\n$$\n\\frac{p(z_{\\ast} \\mid Y=1)}{p(z_{\\ast} \\mid Y=0)} = \\frac{\\frac{p_{t}}{1-p_{t}} (1-\\pi)}{1 \\cdot \\pi} = \\frac{p_t}{1-p_t} \\frac{1-\\pi}{\\pi}\n$$\nThe problem states that the class-conditional distributions of the score $z$ are Gaussian with equal variance:\n$p(z \\mid Y=1) = \\mathcal{N}(\\mu_1, \\sigma^2)$ and $p(z \\mid Y=0) = \\mathcal{N}(\\mu_0, \\sigma^2)$. The likelihood ratio is:\n$$\n\\frac{p(z \\mid Y=1)}{p(z \\mid Y=0)} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu_1)^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu_0)^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{(z-\\mu_0)^2 - (z-\\mu_1)^2}{2\\sigma^2}\\right)\n$$\nThe exponent simplifies to:\n$(z^2 - 2z\\mu_0 + \\mu_0^2) - (z^2 - 2z\\mu_1 + \\mu_1^2) = 2z(\\mu_1 - \\mu_0) - (\\mu_1^2 - \\mu_0^2) = (\\mu_1 - \\mu_0)(2z - (\\mu_1 + \\mu_0))$.\nTaking the natural logarithm of the threshold equation:\n$$\n\\ln\\left( \\frac{p(z_{\\ast} \\mid Y=1)}{p(z_{\\ast} \\mid Y=0)} \\right) = \\frac{(\\mu_1 - \\mu_0)(2z_{\\ast} - (\\mu_1 + \\mu_0))}{2\\sigma^2} = \\ln\\left(\\frac{p_t}{1-p_t} \\frac{1-\\pi}{\\pi}\\right)\n$$\nWe solve for $z_{\\ast}$:\n$$\n2z_{\\ast} - (\\mu_1 + \\mu_0) = \\frac{2\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{p_t(1-\\pi)}{\\pi(1-p_t)}\\right)\n$$\n$$\n2z_{\\ast} = (\\mu_1 + \\mu_0) + \\frac{2\\sigma^2}{\\mu_1 - \\mu_0} \\left[ \\ln\\left(\\frac{p_t}{1-p_t}\\right) - \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) \\right]\n$$\n$$\nz_{\\ast} = \\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\left[ \\ln\\left(\\frac{p_t}{1-p_t}\\right) - \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) \\right]\n$$\nThis is the optimal threshold on the score $z$ in terms of the given parameters.\n\n### Part 2: Numerical Calculation\n\nThe second task is to compute the value of $z_{\\ast}$ given the specific parameters: $\\pi = 0.04$, $p_t = 0.1$, $\\mu_1 = 1.2$, $\\mu_0 = -0.3$, and $\\sigma = 0.9$.\n\nFirst, we calculate the components of the formula for $z_{\\ast}$:\n$\\mu_0 + \\mu_1 = -0.3 + 1.2 = 0.9$\n$\\mu_1 - \\mu_0 = 1.2 - (-0.3) = 1.5$\n$\\sigma^2 = (0.9)^2 = 0.81$\nThe midpoint term is $\\frac{\\mu_0 + \\mu_1}{2} = \\frac{0.9}{2} = 0.45$.\n\nNext, we calculate the logarithmic terms:\nThe odds for the decision threshold $p_t$ is $\\frac{p_t}{1-p_t} = \\frac{0.1}{1-0.1} = \\frac{0.1}{0.9} = \\frac{1}{9}$.\nThe odds for the prevalence $\\pi$ is $\\frac{\\pi}{1-\\pi} = \\frac{0.04}{1-0.04} = \\frac{0.04}{0.96} = \\frac{4}{96} = \\frac{1}{24}$.\n\nThe difference of the log-odds is:\n$$\n\\ln\\left(\\frac{p_t}{1-p_t}\\right) - \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\ln\\left(\\frac{1}{9}\\right) - \\ln\\left(\\frac{1}{24}\\right) = \\ln\\left(\\frac{1/9}{1/24}\\right) = \\ln\\left(\\frac{24}{9}\\right) = \\ln\\left(\\frac{8}{3}\\right)\n$$\n\nNow, we compute the adjustment term:\n$$\n\\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{8}{3}\\right) = \\frac{0.81}{1.5} \\ln\\left(\\frac{8}{3}\\right) = 0.54 \\ln\\left(\\frac{8}{3}\\right)\n$$\nUsing the value $\\ln(8/3) \\approx 0.98082925$:\n$$\n0.54 \\times 0.98082925 \\approx 0.529647795\n$$\n\nFinally, we combine the terms to find $z_{\\ast}$:\n$$\nz_{\\ast} = 0.45 + 0.529647795 = 0.979647795\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\nz_{\\ast} \\approx 0.9796\n$$",
            "answer": "$$\\boxed{0.9796}$$"
        }
    ]
}