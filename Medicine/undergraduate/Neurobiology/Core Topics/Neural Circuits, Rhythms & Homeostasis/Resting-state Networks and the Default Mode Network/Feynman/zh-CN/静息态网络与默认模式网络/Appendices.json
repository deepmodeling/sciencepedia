{
    "hands_on_practices": [
        {
            "introduction": "在我们能够探究大脑网络（如默认模式网络）的内在连通性之前，我们必须首先解决一个关键的挑战：从我们感兴趣的神经信号中分离出各种噪声源。功能磁共振成像（fMRI）时间序列数据极易受到头部运动等非神经因素的干扰。本练习将通过一个亲身实践的编程任务，指导您使用通用线性模型（GLM）来构建一个包含头部运动参数及其时间导数的“干扰项”设计矩阵，并从一个合成的感兴趣区域（ROI）时间序列中回归掉这些干扰信号，从而“净化”信号 。这个过程是几乎所有功能连接性分析的必要预处理步骤，它让您能够从根本上理解数据去噪的原理。",
            "id": "5056395",
            "problem": "给定一段来自功能性磁共振成像 (fMRI) 数据中默认模式网络 (DMN) 的某个感兴趣区域 (ROI) 的时间序列，以及一组刚体运动参数及其时间导数。在静息态分析中，运动会干扰与静息态网络相关的内在神经波动的估计。您的任务是构建一个回归设计矩阵以捕捉与运动相关的混淆因素，然后计算近似该 ROI 内在神经信号的残差时间序列。请使用基于第一性原理的、规范的线性建模方法。\n\n推导和实现应基于以下基本事实：\n- 观测数据的一般线性模型为 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^T$ 是观测到的时间序列，$X \\in \\mathbb{R}^{T \\times p}$ 是回归量的设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数，$\\varepsilon$ 是误差项。\n- 普通最小二乘法 (OLS) 通过最小化误差平方和来估计系数，并得出 $\\hat{\\beta} = X^{+} y$，其中 $X^{+}$ 表示 Moore–Penrose 伪逆，这在存在秩亏的情况下是一致的。\n- 残差由 $r = y - X\\hat{\\beta}$ 给出，在 OLS 下，残差与 $X$ 的列空间正交。\n\n通过串联以下各列来构建设计矩阵 $X$：\n1. 刚体运动参数 $M \\in \\mathbb{R}^{T \\times 6}$（三个平移和三个旋转），按列排列。\n2. 一阶时间导数 $D \\in \\mathbb{R}^{T \\times 6}$，其中当 $t \\geq 1$ 时，$D_t = M_t - M_{t-1}$，且 $D_0$ 为零向量。\n3. 一列全为 1 的截距项。\n4. 一列线性趋势项，对于 $t = 0, 1, \\dots, T-1$，其值为 $t - \\bar{t}$，其中 $\\bar{t}$ 是索引向量的均值。\n\n您将根据上述 $X$ 计算 ROI 时间序列 $y$ 的残差 $r$，然后使用皮尔逊相关性评估 $r$ 在多大程度上近似于内在神经成分 $n$（一个给定的合成基准真相）。使用皮尔逊相关性的定义：\n$$\n\\rho(r,n) = \\frac{\\sum_{t=0}^{T-1} (r_t - \\bar{r})(n_t - \\bar{n})}{\\sqrt{\\sum_{t=0}^{T-1} (r_t - \\bar{r})^2} \\sqrt{\\sum_{t=0}^{T-1} (n_t - \\bar{n})^2}},\n$$\n其中 $\\bar{r}$ 和 $\\bar{n}$ 分别是 $r$ 和 $n$ 的样本均值。\n\n单位：在构建正弦波时使用秒作为时间单位，并以赫兹指定频率。重复时间 (TR) 以秒为单位给出。正弦波相位的角度单位是弧度。\n\n实现一个程序，对以下测试套件执行上述步骤，该套件包括一个一般情况、一个边界条件和一个秩亏的边缘情况。所有信号都是确定性的，并逐点定义，以确保可复现性。\n\n令 $t = 0, 1, \\dots, T-1$ 并为方便起见定义 $u_t = t/T$。对于所有情况，将导数 $D_0$ 设置为零向量。\n\n测试用例 1（一般情况，TR $= 2\\,\\mathrm{s}$，$T = 200$）：\n- 神经基准真相：\n$$\nn_t = \\sin(2\\pi \\cdot 0.03 \\cdot \\mathrm{TR} \\cdot t) + 0.5 \\sin(2\\pi \\cdot 0.05 \\cdot \\mathrm{TR} \\cdot t + \\pi/4).\n$$\n- 运动参数 $M$ 的列 $(m_1,\\dots,m_6)$：\n$$\n\\begin{aligned}\nm_{1,t} = 0.5 \\sin(2\\pi \\cdot 0.2 \\cdot \\mathrm{TR} \\cdot t) + 0.1 u_t, \\\\\nm_{2,t} = 0.4 \\cos(2\\pi \\cdot 0.25 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{3,t} = 0.3 \\sin(2\\pi \\cdot 0.15 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{4,t} = 0.5 u_t, \\\\\nm_{5,t} = 0.2 \\sin(2\\pi \\cdot 0.05 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{6,t} = 0.25 \\cos(2\\pi \\cdot 0.07 \\cdot \\mathrm{TR} \\cdot t).\n\\end{aligned}\n$$\n- 混淆因素的系数：$b = [0.9, -0.5, 0.3, 0.6, -0.4, 0.2]$, $c = [0.5, 0.1, -0.2, 0.3, 0.0, -0.1]$。\n- ROI 时间序列：\n$$\ny_t = n_t + \\sum_{j=1}^{6} b_j m_{j,t} + \\sum_{j=1}^{6} c_j d_{j,t} + 0.1 \\sin(2\\pi \\cdot 0.4 \\cdot \\mathrm{TR} \\cdot t),\n$$\n其中 $d_{j,t}$ 是时间点 $t$ 的第 $j$ 个导数列。\n\n测试用例 2（零运动的边界条件，TR $= 0.8\\,\\mathrm{s}$，$T = 60$）：\n- 神经基准真相：\n$$\nn_t = \\sin(2\\pi \\cdot 0.04 \\cdot \\mathrm{TR} \\cdot t) + 0.6 \\cos(2\\pi \\cdot 0.02 \\cdot \\mathrm{TR} \\cdot t).\n$$\n- 运动参数：对于所有 $j$ 和 $t$，$m_{j,t} = 0$。\n- ROI 时间序列（包括一个微小的高频分量和一个线性漂移）：\n$$\ny_t = n_t + 0.05 \\sin(2\\pi \\cdot 0.3 \\cdot \\mathrm{TR} \\cdot t) + 0.2 u_t.\n$$\n\n测试用例 3（秩亏的边缘情况，TR $= 1.5\\,\\mathrm{s}$，$T = 120$）：\n- 神经基准真相：\n$$\nn_t = \\sin(2\\pi \\cdot 0.025 \\cdot \\mathrm{TR} \\cdot t).\n$$\n- 引入共线性的运动参数 $M$ 的列 $(m_1,\\dots,m_6)$：\n$$\n\\begin{aligned}\nm_{1,t} = \\sin(2\\pi \\cdot 0.1 \\cdot \\mathrm{TR} \\cdot t), \\\\\nm_{2,t} = m_{1,t}, \\\\\nm_{3,t} = 0.3 \\sin(2\\pi \\cdot 0.1 \\cdot \\mathrm{TR} \\cdot t + \\pi/3), \\\\\nm_{4,t} = 0.5 u_t, \\\\\nm_{5,t} = 0.5 u_t, \\\\\nm_{6,t} = \\cos(2\\pi \\cdot 0.2 \\cdot \\mathrm{TR} \\cdot t).\n\\end{aligned}\n$$\n- 混淆因素的系数：$b = [0.5, -0.5, 0.4, 0.3, -0.3, 0.2]$, $c = [0.2, -0.2, 0.1, 0.1, -0.1, 0.0]$。\n- ROI 时间序列：\n$$\ny_t = n_t + \\sum_{j=1}^{6} b_j m_{j,t} + \\sum_{j=1}^{6} c_j d_{j,t}.\n$$\n\n实现要求：\n- 使用后向差分计算 $D$，并将 $D_0$ 设置为零。\n- 通过串联 $M$、$D$、一列全为 1 的截距项和中心化的线性趋势 $(t - \\bar{t})$ 来构建 $X$。\n- 使用 $X$ 的 Moore–Penrose 伪逆估计 $\\hat{\\beta}$，并计算残差 $r = y - X\\hat{\\beta}$。\n- 对每个测试用例计算皮尔逊相关性 $\\rho(r,n)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个相关性值四舍五入到六位小数（例如，“[0.987654,0.912345,0.998765]”）。不应打印任何其他文本。",
            "solution": "该问题要求实现一种基于线性回归的信号处理技术，以从合成的功能性磁共振成像 (fMRI) 时间序列中移除与运动相关的及其他噪声伪影。首先将确立此方法的有效性，然后基于线性代数和统计学的基本原理，给出一个详细的、分步的解决方案。\n\n### 问题验证\n\n**第 1 步：提取给定信息**\n问题提供了以下定义和数据：\n- **通用线性模型 (GLM):** $y = X\\beta + \\varepsilon$\n- **普通最小二乘 (OLS) 解:** $\\hat{\\beta} = X^{+} y$，其中 $X^{+}$ 是 Moore–Penrose 伪逆。\n- **残差:** $r = y - X\\hat{\\beta}$\n- **设计矩阵 $X$:** 由代表以下各项的列串联而成：\n    1. 刚体运动参数 $M \\in \\mathbb{R}^{T \\times 6}$。\n    2. 时间导数 $D \\in \\mathbb{R}^{T \\times 6}$，其中当 $t \\geq 1$ 时 $D_t = M_t - M_{t-1}$，且 $D_0 = \\mathbf{0}$。\n    3. 一个截距列（全为 1 的向量）。\n    4. 一个去均值的线性趋势列，$(t - \\bar{t})$。\n- **评估指标:** 皮尔逊相关系数 $\\rho(r,n)$。\n- **三个测试用例:** 每个用例都指定了时间点数 $T$、重复时间 TR，以及用于基准真相神经信号 $n_t$、运动参数 $M$ 和观测到的 ROI 时间序列 $y_t$ 的确定性公式。这些用例旨在测试一般场景、边界条件（零运动）和边缘情况（秩亏设计矩阵）。\n\n**第 2 步：使用提取的给定信息进行验证**\n根据所需标准对问题陈述进行严格评估，得出以下结论：\n1.  **科学基础：** 该问题牢固地植根于 fMRI 数据分析的标准方法论。使用 GLM 对噪声信号进行建模和回归去除——包括“Friston 24 参数模型”扩展族（运动、导数，有时还有它们的平方）和低频漂移（截距和线性趋势）——是静息态 fMRI 预处理的基石。所指定的数学工具，即 OLS 和 Moore–Penrose 伪逆，是解决此类模型的正确和标准方法，尤其是在存在共线性回归量的情况下。该问题在科学上和数学上都是合理的。\n2.  **适定性：** 该问题是明确的适定问题。所有信号和参数都由明确的数学方程定义，计算过程也以算法方式指定。这确保了每个测试用例都存在唯一且稳定的数值解。包含一个秩亏的情况，并要求使用伪逆，表明了对潜在数值问题及其正确解决方案的深入理解。\n3.  **客观性：** 问题陈述使用精确、客观的语言编写，没有任何主观或推测性主张。所有组成部分都进行了定量定义。\n4.  **完整性和一致性：** 提供了所有必要信息，包括参数（$T$、TR）、信号定义和程序步骤。测试用例内部一致，并且从给定的框架中逻辑地推导出来。\n5.  **可行性：** 参数值（TR、T、信号频率）对于 fMRI 研究是现实的。计算任务是可行的，并且可以使用标准数值库直接实现。\n\n**第 3 步：结论**\n该问题是**有效的**。它代表了一个在计算神经科学和信号处理领域中公式完善、科学相关且可验证的练习。\n\n### 原则性解决方案\n\n核心原则是通过对已知的噪声源方差进行建模和移除，来分离出感兴趣的信号。这是通过一个线性模型框架完成的。\n\n**1. 用设计矩阵建模噪声信号**\n观测到的 fMRI 时间序列 $y \\in \\mathbb{R}^T$ 被假定为真实神经信号、各种噪声源（混淆因素）和测量误差的线性混合。GLM 模型 $y = X\\beta + \\varepsilon$ 为此假设提供了数学结构。关键是构建一个设计矩阵 $X \\in \\mathbb{R}^{T \\times p}$，其列（回归量）代表了可疑噪声信号的时间过程。\n在这个问题中，设计矩阵 $X$ 被构建用来模拟四种类型的混淆因素：\n- **刚体运动 ($M$)：** 头部运动是 fMRI 中非神经方差的主要来源。$M$ 的 6 列对每个时间点的平移和旋转位移进行建模。\n- **运动的时间导数 ($D$)：** 运动参数的后向差分 $D_t = M_t - M_{t-1}$ 被包含进来，以捕捉更复杂的、时间上偏移的运动效应。\n- **直流偏移（截距）：** 包含一列全为 1 的项来模拟信号的平均水平，该平均水平没有生理学解释。\n- **线性漂移（趋势）：** 包含一个代表线性趋势的列 $t-\\bar{t}$，以解释随时间缓慢发生的扫描仪引起的漂移。\n完整的设计矩阵通过串联这些分量形成：$X = [M, D, \\mathbf{1}, (t-\\bar{t})]$。对于此问题，其维度为 $T \\times 14$（$M$ 占 6 列，$D$ 占 6 列，截距占 1 列，趋势占 1 列）。\n\n**2. 估计和移除噪声信号**\n目标是找出这些噪声回归量对观测信号 $y$ 的贡献。普通最小二乘法 (OLS) 找到一组系数 $\\hat{\\beta} \\in \\mathbb{R}^p$，使得观测信号 $y$ 与模型信号 $X\\beta$ 之间的平方差之和最小化。解由以下公式给出：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n在实践中，矩阵 $X^T X$ 可能是病态的或奇异的（不可逆），特别是当某些回归量高度相关（共线性）时。问题在测试用例 3 中明确地创造了这种情况。Moore-Penrose 伪逆 $X^{+}$ 提供了一个稳定且通用的解，即使对于秩亏矩阵也有效：\n$$\n\\hat{\\beta} = X^{+} y\n$$\n一旦估计出 $\\hat{\\beta}$，我们就可以计算出由我们的噪声回归量所解释的信号分量：$\\hat{y} = X\\hat{\\beta}$。\n\n**3. 计算残差**\n残差时间序列 $r$ 是从原始观测中减去建模的噪声分量后剩下的部分：\n$$\nr = y - \\hat{y} = y - X\\hat{\\beta}\n$$\n根据构造，残差向量 $r$ 与 $X$ 的列空间正交 ($X^T r = \\mathbf{0}$)。这意味着我们将 $y$ 投影到了与混淆因素空间正交的子空间上，从而有效地“清洗”了信号中任何可以被 $X$ 的列线性解释的方差。这个残差序列 $r$ 是我们对真实潜在神经信号的估计，其中可能还包含未建模的噪声和测量误差 $\\varepsilon$。\n\n**4. 评估去噪性能**\n为了量化残差序列 $r$ 恢复已知基准真相神经信号 $n$ 的程度，我们计算皮尔逊相关系数 $\\rho(r, n)$。\n$$\n\\rho(r,n) = \\frac{\\text{cov}(r, n)}{\\sigma_r \\sigma_n} = \\frac{\\sum_{t=0}^{T-1} (r_t - \\bar{r})(n_t - \\bar{n})}{\\sqrt{\\sum_{t=0}^{T-1} (r_t - \\bar{r})^2} \\sqrt{\\sum_{t=0}^{T-1} (n_t - \\bar{n})^2}}\n$$\n接近 1 的相关性表明去噪过程成功地移除了混淆因素，留下的残差信号与真实神经活动的时间动态非常匹配。\n\n实现将通过对指定的三个测试用例分别执行这四个步骤来进行。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per the instructions.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for fMRI signal denoising.\n    \"\"\"\n\n    def process_case(TR, T, n_func, m_func, y_func_extras):\n        \"\"\"\n        Processes a single test case for fMRI signal denoising.\n\n        Args:\n            TR (float): Repetition time in seconds.\n            T (int): Number of time points.\n            n_func (callable): Function to generate the neural ground truth signal n(t).\n            m_func (callable): Function to generate the motion parameters matrix M(t).\n            y_func_extras (callable): Function to generate extra components of the ROI signal y(t).\n\n        Returns:\n            float: The Pearson correlation between the residual signal r and the neural signal n.\n        \"\"\"\n        # 1. Generate time vectors and base signals\n        t = np.arange(T)\n        u = t / T\n\n        # Generate neural ground truth\n        n = n_func(t, TR, u)\n\n        # Generate motion parameters M\n        M = m_func(t, TR, u)\n        \n        # Generate temporal derivatives D\n        D = np.zeros_like(M)\n        if T > 1:\n            D[1:] = M[1:] - M[:-1]\n\n        # Generate the observed ROI time series y\n        y_confound_M = np.sum(y_func_extras[\"b\"] * M, axis=1) if \"b\" in y_func_extras else 0\n        y_confound_D = np.sum(y_func_extras[\"c\"] * D, axis=1) if \"c\" in y_func_extras else 0\n        y_extra_terms = y_func_extras[\"extra_func\"](t, TR, u) if \"extra_func\" in y_func_extras else 0\n        \n        y = n + y_confound_M + y_confound_D + y_extra_terms\n\n        # 2. Construct the design matrix X\n        # Intercept column\n        intercept = np.ones((T, 1))\n        # Linear trend column (demeaned)\n        t_bar = np.mean(t)\n        linear_trend = (t - t_bar).reshape(-1, 1)\n\n        # Concatenate all regressors\n        # X will have T rows and 6 (M) + 6 (D) + 1 (intercept) + 1 (trend) = 14 columns\n        X = np.c_[M, D, intercept, linear_trend]\n\n        # 3. Perform OLS regression and compute residuals\n        # Use Moore-Penrose pseudo-inverse for stability and to handle rank deficiency\n        X_pinv = np.linalg.pinv(X)\n        beta_hat = X_pinv @ y\n        \n        # Residuals are the \"cleaned\" signal\n        r = y - X @ beta_hat\n        \n        # 4. Evaluate performance using Pearson correlation\n        # np.corrcoef returns a 2x2 matrix, the off-diagonal element is the correlation\n        correlation = np.corrcoef(r, n)[0, 1]\n        \n        return correlation\n\n    # Test Case 1: General case\n    case1_params = {\n        \"TR\": 2.0, \"T\": 200,\n        \"n_func\": lambda t, TR, u: np.sin(2 * np.pi * 0.03 * TR * t) + 0.5 * np.sin(2 * np.pi * 0.05 * TR * t + np.pi / 4),\n        \"m_func\": lambda t, TR, u: np.c_[\n            0.5 * np.sin(2 * np.pi * 0.2 * TR * t) + 0.1 * u,\n            0.4 * np.cos(2 * np.pi * 0.25 * TR * t),\n            0.3 * np.sin(2 * np.pi * 0.15 * TR * t),\n            0.5 * u,\n            0.2 * np.sin(2 * np.pi * 0.05 * TR * t),\n            0.25 * np.cos(2 * np.pi * 0.07 * TR * t)\n        ],\n        \"y_func_extras\": {\n            \"b\": np.array([0.9, -0.5, 0.3, 0.6, -0.4, 0.2]),\n            \"c\": np.array([0.5, 0.1, -0.2, 0.3, 0.0, -0.1]),\n            \"extra_func\": lambda t, TR, u: 0.1 * np.sin(2 * np.pi * 0.4 * TR * t)\n        }\n    }\n\n    # Test Case 2: Boundary condition (zero motion)\n    case2_params = {\n        \"TR\": 0.8, \"T\": 60,\n        \"n_func\": lambda t, TR, u: np.sin(2 * np.pi * 0.04 * TR * t) + 0.6 * np.cos(2 * np.pi * 0.02 * TR * t),\n        \"m_func\": lambda t, TR, u: np.zeros((len(t), 6)),\n        \"y_func_extras\": {\n            \"extra_func\": lambda t, TR, u: 0.05 * np.sin(2 * np.pi * 0.3 * TR * t) + 0.2 * u\n        }\n    }\n\n    # Test Case 3: Edge case (rank deficiency)\n    case3_params = {\n        \"TR\": 1.5, \"T\": 120,\n        \"n_func\": lambda t, TR, u: np.sin(2 * np.pi * 0.025 * TR * t),\n        \"m_func\": lambda t, TR, u: np.c_[\n            np.sin(2 * np.pi * 0.1 * TR * t),\n            np.sin(2 * np.pi * 0.1 * TR * t), # Collinear with m1\n            0.3 * np.sin(2 * np.pi * 0.1 * TR * t + np.pi/3),\n            0.5 * u,\n            0.5 * u, # Collinear with m4\n            np.cos(2 * np.pi * 0.2 * TR * t)\n        ],\n        \"y_func_extras\": {\n            \"b\": np.array([0.5, -0.5, 0.4, 0.3, -0.3, 0.2]),\n            \"c\": np.array([0.2, -0.2, 0.1, 0.1, -0.1, 0.0])\n        }\n    }\n\n    test_cases = [case1_params, case2_params, case3_params]\n    results = []\n    for case in test_cases:\n        result = process_case(case[\"TR\"], case[\"T\"], case[\"n_func\"], case[\"m_func\"], case[\"y_func_extras\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{res:.6f}' for res in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "静息态功能磁共振成像（fMRI）研究中最引人注目且最具争议的发现之一是默认模式网络（DMN）与背侧注意网络（DAN）之间的“反相关性”。然而，一种被称为全局信号回归（GSR）的预处理技术，虽然能有效去除广泛的生理噪声，却在数学上可能人为地引入或增强这种反相关性。本练习  旨在让您直面这一方法学上的核心辩题，您将使用基础的偏相关公式来预测GSR对相关性的影响，并将其与经过GSR处理后的“真实”数据进行比较。通过这种方式，您可以定量地评估观察到的DMN-DAN反相关性在多大程度上可以被解释为神经活动的真实特征，而非仅仅是数据处理的副产品。",
            "id": "5056380",
            "problem": "给定对称的逐区域皮尔逊相关矩阵，这些矩阵代表一组大脑区域的静息态功能性磁共振成像时间序列，其对角线元素等于 $1.00$。背景是关于静息态网络的研究，重点关注默认模式网络（DMN）和背侧注意网络（DAN）。主要目的是评估通过全局信号回归（GSR）去除共享方差对观察到的 DMN 和 DAN 之间反相关的影响。具体来说，您必须计算 GSR 后观察到的 DMN-DAN 反相关是否超过了纯粹由去除全局共享分量所预期的程度。\n\n基本原理和定义：\n- 皮尔逊相关性量化了标准化时间序列之间的线性关联。设 $X_i$ 表示区域 $i$ 的标准化时间序列。未经 GSR 的相关矩阵为 $R^{(0)}$，其元素为 $r^{(0)}_{ij} = \\mathrm{corr}(X_i, X_j)$。\n- 全局信号 $G$ 定义为所有 $N$ 个区域时间序列的时间平均值：$G = \\frac{1}{N}\\sum_{k=1}^{N} X_k$。在线性回归下，去除全局信号等同于计算控制了 $G$ 的偏相关。\n- 对于任何区域 $i$，其与全局信号的协方差为 $\\mathrm{cov}(X_i, G) = \\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}$，全局信号的方差为 $\\mathrm{var}(G) = \\frac{1}{N^2}\\sum_{a=1}^{N}\\sum_{b=1}^{N} r^{(0)}_{ab}$。因此，区域 $i$ 与全局信号之间的相关性为\n$$\nr_{iG} = \\frac{\\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}}{\\sqrt{\\mathrm{var}(G)}}.\n$$\n- 去除全局信号后，区域 $i$ 和 $j$ 之间预期的成对偏相关为\n$$\nr^{\\mathrm{pred}}_{ij} = \\frac{r^{(0)}_{ij} - r_{iG}\\, r_{jG}}{\\sqrt{\\left(1 - r_{iG}^2\\right)\\left(1 - r_{jG}^2\\right)}}.\n$$\n该量代表了仅因去除可归因于全局信号的共享方差而预期的相关性。\n\n给定：\n- $R^{(0)}$：未经 GSR 的相关矩阵。\n- $R^{(1)}$：经过 GSR 的经验相关矩阵。\n- 网络索引集：默认模式网络（DMN）索引和背侧注意网络（DAN）索引，均为 $\\{0,1,\\dots,N-1\\}$ 的不相交子集。\n\n您的任务：\n1. 计算差分矩阵 $\\Delta = R^{(1)} - R^{(0)}$。\n2. 仅使用 $R^{(0)}$，计算所有 $i$ 的 $r_{iG}$，然后根据上述公式计算预测的偏相关矩阵 $R^{\\mathrm{pred}}$，其元素为 $r^{\\mathrm{pred}}_{ij}$。\n3. 对于所有 DMN-DAN 对 $(i,j)$（其中 $i$ 在 DMN 中，$j$ 在 DAN 中），评估观察到的相关性 $r^{(1)}_{ij}$ 是否比仅基于共享方差去除的预测 $r^{\\mathrm{pred}}_{ij}$ 更负。将此评估量化为满足 $r^{(1)}_{ij}  r^{\\mathrm{pred}}_{ij}$ 的 DMN-DAN 对的比例（以小数形式表示）。\n\n您的程序必须处理以下测试套件。每个测试用例给出 $N$、DMN 索引、DAN 索引、$R^{(0)}$ 和 $R^{(1)}$。所有数值条目均为无单位的相关值。\n\n测试用例 1：\n- $N = 6$。\n- DMN 索引：$[0,1,2]$。\n- DAN 索引：$[3,4,5]$。\n- $R^{(0)}$:\n  $$\n  \\begin{bmatrix}\n  1.00  0.45  0.40  -0.12  -0.15  -0.10 \\\\\n  0.45  1.00  0.42  -0.18  -0.16  -0.14 \\\\\n  0.40  0.42  1.00  -0.11  -0.13  -0.12 \\\\\n  -0.12  -0.18  -0.11  1.00  0.48  0.44 \\\\\n  -0.15  -0.16  -0.13  0.48  1.00  0.46 \\\\\n  -0.10  -0.14  -0.12  0.44  0.46  1.00\n  \\end{bmatrix}\n  $$\n- $R^{(1)}$:\n  $$\n  \\begin{bmatrix}\n  1.00  0.47  0.45  -0.25  -0.28  -0.22 \\\\\n  0.47  1.00  0.46  -0.30  -0.29  -0.27 \\\\\n  0.45  0.46  1.00  -0.24  -0.26  -0.25 \\\\\n  -0.25  -0.30  -0.24  1.00  0.50  0.47 \\\\\n  -0.28  -0.29  -0.26  0.50  1.00  0.49 \\\\\n  -0.22  -0.27  -0.25  0.47  0.49  1.00\n  \\end{bmatrix}\n  $$\n\n测试用例 2：\n- $N = 6$。\n- DMN 索引：$[0,1,2]$。\n- DAN 索引：$[3,4,5]$。\n- $R^{(0)}$:\n  $$\n  \\begin{bmatrix}\n  1.00  0.30  0.28  -0.02  0.00  0.01 \\\\\n  0.30  1.00  0.32  -0.01  0.02  0.00 \\\\\n  0.28  0.32  1.00  0.00  -0.01  0.02 \\\\\n  -0.02  -0.01  0.00  1.00  0.31  0.29 \\\\\n  0.00  0.02  -0.01  0.31  1.00  0.33 \\\\\n  0.01  0.00  0.02  0.29  0.33  1.00\n  \\end{bmatrix}\n  $$\n- $R^{(1)}$:\n  $$\n  \\begin{bmatrix}\n  1.00  0.29  0.27  -0.02  -0.01  0.00 \\\\\n  0.29  1.00  0.31  -0.02  0.01  -0.01 \\\\\n  0.27  0.31  1.00  0.00  -0.02  0.01 \\\\\n  -0.02  -0.02  0.00  1.00  0.30  0.28 \\\\\n  -0.01  0.01  -0.02  0.30  1.00  0.32 \\\\\n  0.00  -0.01  0.01  0.28  0.32  1.00\n  \\end{bmatrix}\n  $$\n\n测试用例 3：\n- $N = 8$。\n- DMN 索引：$[0,1,2,3]$。\n- DAN 索引：$[4,5,6,7]$。\n- $R^{(0)}$:\n  $$\n  \\begin{bmatrix}\n  1.00  0.55  0.50  0.52  -0.05  -0.04  -0.06  -0.05 \\\\\n  0.55  1.00  0.53  0.54  -0.06  -0.05  -0.07  -0.06 \\\\\n  0.50  0.53  1.00  0.51  -0.04  -0.03  -0.05  -0.04 \\\\\n  0.52  0.54  0.51  1.00  -0.05  -0.04  -0.06  -0.05 \\\\\n  -0.05  -0.06  -0.04  -0.05  1.00  0.56  0.54  0.52 \\\\\n  -0.04  -0.05  -0.03  -0.04  0.56  1.00  0.55  0.53 \\\\\n  -0.06  -0.07  -0.05  -0.06  0.54  0.55  1.00  0.57 \\\\\n  -0.05  -0.06  -0.04  -0.05  0.52  0.53  0.57  1.00\n  \\end{bmatrix}\n  $$\n- $R^{(1)}$:\n  $$\n  \\begin{bmatrix}\n  1.00  0.58  0.54  0.56  -0.22  -0.21  -0.23  -0.22 \\\\\n  0.58  1.00  0.57  0.58  -0.24  -0.23  -0.25  -0.24 \\\\\n  0.54  0.57  1.00  0.55  -0.20  -0.19  -0.21  -0.20 \\\\\n  0.56  0.58  0.55  1.00  -0.23  -0.22  -0.24  -0.23 \\\\\n  -0.22  -0.24  -0.20  -0.23  1.00  0.60  0.58  0.57 \\\\\n  -0.21  -0.23  -0.19  -0.22  0.60  1.00  0.59  0.58 \\\\\n  -0.23  -0.25  -0.21  -0.24  0.58  0.59  1.00  0.61 \\\\\n  -0.22  -0.24  -0.20  -0.23  0.57  0.58  0.61  1.00\n  \\end{bmatrix}\n  $$\n\n输出规范：\n- 对于每个测试用例，计算 DMN-DAN 对中满足 $r^{(1)}_{ij}  r^{\\mathrm{pred}}_{ij}$ 的比例，并将该比例四舍五入到三位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如，对于三个测试用例，输出为 $[0.125,0.000,1.000]$。\n\n不涉及物理单位或角度。将任何分数结果表示为小数，不要使用百分号。输入已嵌入到您的程序中；不使用用户输入或外部文件。",
            "solution": "该问题是有效的，因为它在科学上基于神经影像数据分析的原理，在数学上是适定问题，并为其解决提供了完整且一致的数据和定义。该分析直接解决了一个功能连接研究中已知的关于全局信号回归（GSR）影响的方法学问题。\n\n该解决方案通过为每个测试用例实现问题陈述中描述的计算来推进。总体目标是确定默认模式网络（DMN）和背侧注意网络（DAN）之间的区域对中，经验观察到的 GSR 后相关性 $r^{(1)}_{ij}$ 比纯粹由共同全局信号的统计去除所预测的相关性 $r^{\\mathrm{pred}}_{ij}$ 更负的比例。\n\n每个测试用例的逻辑步骤如下：\n\n1.  **解析输入**：对于每个测试用例，我们都给定了区域数 $N$、DMN 和 DAN 的索引集、GSR 前的相关矩阵 $R^{(0)}$，以及 GSR 后的经验相关矩阵 $R^{(1)}$。\n\n2.  **计算预测相关性 $R^{\\mathrm{pred}}$**：任务的核心是仅使用 GSR 前的矩阵 $R^{(0)}$ 来计算矩阵 $R^{\\mathrm{pred}}$。这涉及基于所提供公式的三个子步骤。\n\n    a.  **计算全局信号的方差 $\\mathrm{var}(G)$**：全局信号 $G$ 是所有区域时间序列的平均值。由于时间序列是标准化的，因此可以从相关矩阵 $R^{(0)}$ 计算 $G$ 的方差：\n        $$\n        \\mathrm{var}(G) = \\frac{1}{N^2}\\sum_{a=1}^{N}\\sum_{b=1}^{N} r^{(0)}_{ab}\n        $$\n        这通过将矩阵 $R^{(0)}$ 的所有元素相加并除以 $N^2$ 来实现。\n\n    b.  **计算区域与全局信号的相关性 $r_{iG}$**：对于每个区域 $i$，计算其与全局信号的相关性 $r_{iG}$。这需要区域时间序列 $X_i$ 和全局信号 $G$ 之间的协方差，由 $\\mathrm{cov}(X_i, G) = \\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}$ 给出。这是 $R^{(0)}$ 的第 $i$ 行（或列）的平均值。然后，相关性为：\n        $$\n        r_{iG} = \\frac{\\mathrm{cov}(X_i, G)}{\\sqrt{\\mathrm{var}(G)}} = \\frac{\\frac{1}{N}\\sum_{k=1}^{N} r^{(0)}_{ik}}{\\sqrt{\\mathrm{var}(G)}}\n        $$\n        对从 $0$ 到 $N-1$ 的每个区域 $i$ 执行此计算，从而得到一个 $r_{iG}$ 值的向量。\n\n    c.  **计算预测的偏相关矩阵 $R^{\\mathrm{pred}}$**：使用 $r_{iG}$ 值，通过标准的偏相关公式计算每对区域 $(i,j)$ 的预测偏相关 $r^{\\mathrm{pred}}_{ij}$，该公式模拟了回归掉全局信号 $G$ 的效果：\n        $$\n        r^{\\mathrm{pred}}_{ij} = \\frac{r^{(0)}_{ij} - r_{iG}\\, r_{jG}}{\\sqrt{\\left(1 - r_{iG}^2\\right)\\left(1 - r_{jG}^2\\right)}}\n        $$\n        该公式应用于所有对 $(i,j)$ 以构建完整的预测矩阵 $R^{\\mathrm{pred}}$。对角线元素为 $1$。\n\n3.  **量化 DMN-DAN 反相关差异**：最后一步是比较感兴趣的特定网络对的经验性 GSR 后相关性与预测相关性。\n    \n    a.  **识别 DMN-DAN 对**：我们遍历所有区域对 $(i, j)$，使得区域 $i$ 在 DMN 索引集中，区域 $j$ 在 DAN 索引集中。\n\n    b.  **应用条件**：对于每个 DMN-DAN 对 $(i, j)$，我们测试条件 $r^{(1)}_{ij}  r^{\\mathrm{pred}}_{ij}$。如果经验观察到的相关性比 GSR 统计伪影预测的相关性更负（即数值更小），则此条件为真。\n\n    c.  **计算比例**：维护一个计数器，记录满足条件的对数。然后将此计数除以 DMN-DAN 对的总数，即 DMN 中区域数与 DAN 中区域数的乘积。\n        $$\n        \\text{fraction} = \\frac{\\left| \\left\\{ (i,j) \\mid i \\in \\text{DMN}, j \\in \\text{DAN}, \\text{ and } r^{(1)}_{ij}  r^{\\mathrm{pred}}_{ij} \\right\\} \\right|}{\\left| \\text{DMN} \\right| \\times \\left| \\text{DAN} \\right|}\n        $$\n\n4.  **格式化输出**：每个测试用例得到的分数四舍五入到三位小数。最终输出是这些分数的逗号分隔列表，并用方括号括起来。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Processes a suite of test cases to analyze the effect of Global Signal Regression (GSR)\n    on DMN-DAN anticorrelations in fMRI data.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"N\": 6,\n            \"dmn_indices\": [0, 1, 2],\n            \"dan_indices\": [3, 4, 5],\n            \"R0\": [\n                [1.00, 0.45, 0.40, -0.12, -0.15, -0.10],\n                [0.45, 1.00, 0.42, -0.18, -0.16, -0.14],\n                [0.40, 0.42, 1.00, -0.11, -0.13, -0.12],\n                [-0.12, -0.18, -0.11, 1.00, 0.48, 0.44],\n                [-0.15, -0.16, -0.13, 0.48, 1.00, 0.46],\n                [-0.10, -0.14, -0.12, 0.44, 0.46, 1.00]\n            ],\n            \"R1\": [\n                [1.00, 0.47, 0.45, -0.25, -0.28, -0.22],\n                [0.47, 1.00, 0.46, -0.30, -0.29, -0.27],\n                [0.45, 0.46, 1.00, -0.24, -0.26, -0.25],\n                [-0.25, -0.30, -0.24, 1.00, 0.50, 0.47],\n                [-0.28, -0.29, -0.26, 0.50, 1.00, 0.49],\n                [-0.22, -0.27, -0.25, 0.47, 0.49, 1.00]\n            ]\n        },\n        {\n            \"N\": 6,\n            \"dmn_indices\": [0, 1, 2],\n            \"dan_indices\": [3, 4, 5],\n            \"R0\": [\n                [1.00, 0.30, 0.28, -0.02, 0.00, 0.01],\n                [0.30, 1.00, 0.32, -0.01, 0.02, 0.00],\n                [0.28, 0.32, 1.00, 0.00, -0.01, 0.02],\n                [-0.02, -0.01, 0.00, 1.00, 0.31, 0.29],\n                [0.00, 0.02, -0.01, 0.31, 1.00, 0.33],\n                [0.01, 0.00, 0.02, 0.29, 0.33, 1.00]\n            ],\n            \"R1\": [\n                [1.00, 0.29, 0.27, -0.02, -0.01, 0.00],\n                [0.29, 1.00, 0.31, -0.02, 0.01, -0.01],\n                [0.27, 0.31, 1.00, 0.00, -0.02, 0.01],\n                [-0.02, -0.02, 0.00, 1.00, 0.30, 0.28],\n                [-0.01, 0.01, -0.02, 0.30, 1.00, 0.32],\n                [0.00, -0.01, 0.01, 0.28, 0.32, 1.00]\n            ]\n        },\n        {\n            \"N\": 8,\n            \"dmn_indices\": [0, 1, 2, 3],\n            \"dan_indices\": [4, 5, 6, 7],\n            \"R0\": [\n                [1.00, 0.55, 0.50, 0.52, -0.05, -0.04, -0.06, -0.05],\n                [0.55, 1.00, 0.53, 0.54, -0.06, -0.05, -0.07, -0.06],\n                [0.50, 0.53, 1.00, 0.51, -0.04, -0.03, -0.05, -0.04],\n                [0.52, 0.54, 0.51, 1.00, -0.05, -0.04, -0.06, -0.05],\n                [-0.05, -0.06, -0.04, -0.05, 1.00, 0.56, 0.54, 0.52],\n                [-0.04, -0.05, -0.03, -0.04, 0.56, 1.00, 0.55, 0.53],\n                [-0.06, -0.07, -0.05, -0.06, 0.54, 0.55, 1.00, 0.57],\n                [-0.05, -0.06, -0.04, -0.05, 0.52, 0.53, 0.57, 1.00]\n            ],\n            \"R1\": [\n                [1.00, 0.58, 0.54, 0.56, -0.22, -0.21, -0.23, -0.22],\n                [0.58, 1.00, 0.57, 0.58, -0.24, -0.23, -0.25, -0.24],\n                [0.54, 0.57, 1.00, 0.55, -0.20, -0.19, -0.21, -0.20],\n                [0.56, 0.58, 0.55, 1.00, -0.23, -0.22, -0.24, -0.23],\n                [-0.22, -0.24, -0.20, -0.23, 1.00, 0.60, 0.58, 0.57],\n                [-0.21, -0.23, -0.19, -0.22, 0.60, 1.00, 0.59, 0.58],\n                [-0.23, -0.25, -0.21, -0.24, 0.58, 0.59, 1.00, 0.61],\n                [-0.22, -0.24, -0.20, -0.23, 0.57, 0.58, 0.61, 1.00]\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        dmn_indices = case[\"dmn_indices\"]\n        dan_indices = case[\"dan_indices\"]\n        R0 = np.array(case[\"R0\"])\n        R1 = np.array(case[\"R1\"])\n\n        # Calculate variance of the global signal G\n        var_G = np.sum(R0) / (N**2)\n        sqrt_var_G = np.sqrt(var_G)\n        \n        # Calculate correlation of each region i with the global signal G\n        r_iG = np.zeros(N)\n        if sqrt_var_G > 0:\n            cov_Xi_G = np.sum(R0, axis=1) / N\n            r_iG = cov_Xi_G / sqrt_var_G\n\n        # Calculate the predicted partial correlation matrix R_pred\n        R_pred = np.zeros_like(R0)\n        for i in range(N):\n            for j in range(N):\n                if i == j:\n                    R_pred[i, j] = 1.0\n                    continue\n                \n                # Denominator for the partial correlation formula\n                den_sqrt_term = (1 - r_iG[i]**2) * (1 - r_iG[j]**2)\n                \n                # Handle potential numerical instability if correlations are perfect\n                if den_sqrt_term == 0:\n                    # This case implies r_iG or r_jG is 1, partial correlation is ill-defined.\n                    # With valid correlation matrices, this should not happen. We set to NaN.\n                    R_pred[i, j] = np.nan\n                    continue\n\n                denominator = np.sqrt(den_sqrt_term)\n                numerator = R0[i, j] - r_iG[i] * r_iG[j]\n                R_pred[i, j] = numerator / denominator\n\n        # Count DMN-DAN pairs where observed correlation is more negative than predicted\n        count_exceeding = 0\n        total_pairs = len(dmn_indices) * len(dan_indices)\n\n        for i in dmn_indices:\n            for j in dan_indices:\n                if R1[i, j]  R_pred[i, j]:\n                    count_exceeding += 1\n\n        # Calculate the final fraction\n        if total_pairs > 0:\n            fraction = count_exceeding / total_pairs\n        else:\n            fraction = 0.0\n\n        results.append(f\"{fraction:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们为什么要研究大脑在“休息”时做什么？一个强有力的答案是，静息态下的大脑网络活动模式能够预测个体在执行认知任务时的行为表现。本练习  将引导您构建一个连接大脑与行为的预测模型，具体来说，您将利用任务开始前测量的默认模式网络（DMN）内部的功能连接性强度，来预测被试在后续任务中反应时间的变异性。您将首先对一组“训练”数据拟合一个线性回归模型，然后用这个模型来预测全新“测试”数据的行为表现，这体现了将神经影像学发现转化为具有预测能力的生物标志物的核心思想。",
            "id": "5056143",
            "problem": "您的任务是推导并实现一个预测模型，该模型使用任务前的默认模式网络 (DMN) 连接性来预测在面向外部任务中的表现，该表现操作化为反应时间 (RT) 的变异性。默认模式网络 (DMN) 是一组在静息状态下表现出相干活动的大脑区域；任务前更强的 DMN 连接性通常与面向外部任务表现中更高的变异性相关，这是由于内向聚焦的加工过程所致。从神经生物学和统计学的基本定义出发，构建一个模型，其中标量 DMN 连接性指数通过普通最小二乘线性回归预测 RT 变异性，并将其应用于指定的测试套件。\n\n基本原理与定义：\n- 两个区域间的静息态功能连接性由它们在静息状态下的血氧水平依赖时间序列计算出的皮尔逊相关系数 $r$ 定义。\n- 为了从相关系数中获得一个正态分布的汇总指数，使用 Fisher $z$ 变换：$z = \\operatorname{atanh}(r)$。\n- 给定 $N$ 个 DMN 区域，有 $M = N(N-1)/2$ 个唯一的成对相关性 $\\{r_{ij}\\}$（上三角部分），参与者 $i$ 的 DMN 连接性指数定义为 Fisher $z$ 变换后成对相关性的算术平均值：\n$$\nC_i = \\frac{1}{M} \\sum_{(j,k)\\in \\mathcal{U}} \\operatorname{atanh}(r_{jk}^{(i)}),\n$$\n其中 $\\mathcal{U}$ 表示上三角索引对的集合，且 $r_{jk}^{(i)} \\in (-1,1)$。\n- 参与者 $i$ 的反应时间变异性定义为其在面向外部任务中记录的逐次试验反应时间 $\\{t_{i\\ell}\\}_{\\ell=1}^n$ 的总体标准差（分母为 $n$）：\n$$\nV_i = \\sqrt{\\frac{1}{n}\\sum_{\\ell=1}^n \\left(t_{i\\ell} - \\bar{t}_i\\right)^2},\n\\quad \\text{with} \\quad\n\\bar{t}_i = \\frac{1}{n} \\sum_{\\ell=1}^n t_{i\\ell}.\n$$\n- 预测模型是一个将连接性与变异性联系起来的线性回归：\n$$\n\\hat{V} = a + b C,\n$$\n其参数 $(a,b)$ 的选择旨在最小化训练集上的残差平方和。普通最小二乘解为：\n$$\nb = \\frac{\\sum_{i=1}^n (C_i - \\bar{C})(V_i - \\bar{V})}{\\sum_{i=1}^n (C_i - \\bar{C})^2},\n\\quad\na = \\bar{V} - b\\,\\bar{C},\n$$\n其中 $\\bar{C}$ 和 $\\bar{V}$ 是 $\\{C_i\\}$ 和 $\\{V_i\\}$ 的样本均值。\n\n训练数据说明：\n- 考虑 $N=4$ 个 DMN 区域，因此每个参与者有 $M=6$ 个唯一的成对相关性。对于每个训练参与者 $i\\in\\{1,\\dots,8\\}$，提供的六个非对角线相关性 $\\{r^{(i)}\\}$ 如下（每个列表是有序的，但对于求均值而言顺序无关紧要）：\n1. 参与者 1: $[\\,0.15,\\,0.20,\\,0.10,\\,0.12,\\,0.18,\\,0.16\\,]$\n2. 参与者 2: $[\\,0.25,\\,0.28,\\,0.22,\\,0.24,\\,0.26,\\,0.27\\,]$\n3. 参与者 3: $[\\,0.35,\\,0.33,\\,0.32,\\,0.36,\\,0.37,\\,0.34\\,]$\n4. 参与者 4: $[\\,0.45,\\,0.48,\\,0.42,\\,0.44,\\,0.46,\\,0.47\\,]$\n5. 参与者 5: $[\\,0.55,\\,0.52,\\,0.56,\\,0.58,\\,0.54,\\,0.57\\,]$\n6. 参与者 6: $[\\,0.65,\\,0.62,\\,0.63,\\,0.66,\\,0.67,\\,0.64\\,]$\n7. 参与者 7: $[\\,0.70,\\,0.72,\\,0.68,\\,0.69,\\,0.71,\\,0.73\\,]$\n8. 参与者 8: $[\\,0.75,\\,0.78,\\,0.74,\\,0.76,\\,0.77,\\,0.79\\,]$\n- 对于每个训练参与者 $i$，逐次试验的反应时间 $\\{t_{i\\ell}\\}$ 由一个基础模式向量 $p$ 和一个振幅 $A_i$（单位：秒）确定性地构建。设基础模式为\n$$\np = [\\,-0.3,\\,-0.2,\\,-0.1,\\,0.0,\\,0.1,\\,0.2,\\,0.3,\\,-0.1,\\,0.1,\\,0.0,\\,-0.2,\\,0.2\\,],\n$$\n并定义\n$$\nt_{i\\ell} = 0.6 + A_i\\,p_\\ell \\quad \\text{秒},\n$$\n对于 $\\ell=1,\\dots,12$。使用以下振幅（单位：秒）：\n$$\nA_1=0.16,\\; A_2=0.20,\\; A_3=0.24,\\; A_4=0.28,\\; A_5=0.32,\\; A_6=0.36,\\; A_7=0.40,\\; A_8=0.44.\n$$\n这些振幅在人类反应时间的科学合理范围内产生递增的变异性幅度。\n\n模型拟合任务：\n- 通过对 Fisher $z$ 变换后的相关性求平均，计算每个训练参与者的 $C_i$。\n- 将每个训练参与者的 $V_i$ 计算为 $t_{i\\ell}$ 的总体标准差。\n- 使用上述普通最小二乘公式拟合线性回归，以获得 $a$ 和 $b$。\n\n预测与测试套件：\n- 使用拟合的 $(a,b)$，为三个新的参与者（测试案例）预测 RT 变异性 $\\hat{V}$（单位：秒），这些参与者仅由其六个非对角线 DMN 相关系数指定：\n1. 测试案例 1 (中等连接性): $[\\,0.60,\\,0.58,\\,0.62,\\,0.59,\\,0.61,\\,0.60\\,]$\n2. 测试案例 2 (低连接性边界): $[\\,0.05,\\,0.08,\\,0.02,\\,0.04,\\,0.06,\\,0.03\\,]$\n3. 测试案例 3 (高连接性边缘): $[\\,0.82,\\,0.80,\\,0.83,\\,0.81,\\,0.84,\\,0.82\\,]$\n\n要求输出：\n- 您的程序应生成单行输出，其中包含三个测试案例的预测 RT 变异性值（单位：秒），每个值四舍五入到六位小数，形式为逗号分隔并用方括号括起来的列表，例如：$[0.052345,0.031000,0.078999]$。",
            "solution": "该问题要求基于任务前的默认模式网络 (DMN) 连接性构建一个预测反应时间 (RT) 变异性的模型。该模型将是一个简单的线性回归，使用一组参与者的数据进行训练，然后用于对一组新的测试案例进行预测。该过程涉及几个步骤：为每位参与者计算 DMN 连接性指数，确定其 RT 变异性，拟合回归模型，最后应用模型进行预测。\n\n分析主要分四个阶段进行：\n1.  为 8 位训练参与者中的每一位计算自变量（DMN 连接性指数，$C_i$）。\n2.  为每位训练参与者计算因变量（RT 变异性，$V_i$）。\n3.  估计普通最小二乘 (OLS) 线性回归模型的参数。\n4.  使用拟合的模型为 3 个测试案例预测 RT 变异性。\n\n**步骤 1：计算 DMN 连接性指数 ($C_i$)**\n\n对于每位参与者 $i$，DMN 连接性指数 $C_i$ 定义为 DMN 内 $M$ 个唯一成对连接的 Fisher $z$ 变换后皮尔逊相关系数的算术平均值。当有 $N=4$ 个区域时，存在 $M = N(N-1)/2 = 4(3)/2 = 6$ 个唯一配对。公式为：\n$$\nC_i = \\frac{1}{6} \\sum_{(j,k)\\in \\mathcal{U}} \\operatorname{atanh}(r_{jk}^{(i)})\n$$\n其中 $\\{r_{jk}^{(i)}\\}$ 是参与者 $i$ 的 6 个相关系数。函数 $\\operatorname{atanh}(r)$ 是 Fisher $z$ 变换。\n\n使用为 8 位训练参与者提供的相关数据，我们计算出以下连接性指数：\n- 参与者 1: $r^{(1)} = [\\,0.15,\\,0.20,\\,0.10,\\,0.12,\\,0.18,\\,0.16\\,] \\implies C_1 \\approx 0.153018$\n- 参与者 2: $r^{(2)} = [\\,0.25,\\,0.28,\\,0.22,\\,0.24,\\,0.26,\\,0.27\\,] \\implies C_2 \\approx 0.259096$\n- 参与者 3: $r^{(3)} = [\\,0.35,\\,0.33,\\,0.32,\\,0.36,\\,0.37,\\,0.34\\,] \\implies C_3 \\approx 0.359877$\n- 参与者 4: $r^{(4)} = [\\,0.45,\\,0.48,\\,0.42,\\,0.44,\\,0.46,\\,0.47\\,] \\implies C_4 \\approx 0.489166$\n- 参与者 5: $r^{(5)} = [\\,0.55,\\,0.52,\\,0.56,\\,0.58,\\,0.54,\\,0.57\\,] \\implies C_5 \\approx 0.623617$\n- 参与者 6: $r^{(6)} = [\\,0.65,\\,0.62,\\,0.63,\\,0.66,\\,0.67,\\,0.64\\,] \\implies C_6 \\approx 0.767243$\n- 参与者 7: $r^{(7)} = [\\,0.70,\\,0.72,\\,0.68,\\,0.69,\\,0.71,\\,0.73\\,] \\implies C_7 \\approx 0.877983$\n- 参与者 8: $r^{(8)} = [\\,0.75,\\,0.78,\\,0.74,\\,0.76,\\,0.77,\\,0.79\\,] \\implies C_8 \\approx 1.009470$\n\n这 8 个值构成了回归的自变量集合 $\\{C_i\\}_{i=1}^8$。\n\n**步骤 2：计算反应时间变异性 ($V_i$)**\n\nRT 变异性 $V_i$ 是参与者 $n=12$ 次反应时间 $\\{t_{i\\ell}\\}$ 的总体标准差。这些时间由公式 $t_{i\\ell} = 0.6 + A_i p_\\ell$ 生成，其中 $A_i$ 是特定于参与者的振幅，$p$ 是一个基础模式向量。\n\n$V_i$ 的公式为：\n$$\nV_i = \\sqrt{\\frac{1}{n}\\sum_{\\ell=1}^n \\left(t_{i\\ell} - \\bar{t}_i\\right)^2}\n$$\n我们可以简化这个计算。首先，求出平均反应时间 $\\bar{t}_i$：\n$$\n\\bar{t}_i = \\frac{1}{n} \\sum_{\\ell=1}^n t_{i\\ell} = \\frac{1}{n} \\sum_{\\ell=1}^n (0.6 + A_i p_\\ell) = 0.6 + A_i \\left(\\frac{1}{n} \\sum_{\\ell=1}^n p_\\ell\\right) = 0.6 + A_i \\bar{p}\n$$\n那么，与均值的偏差为 $t_{i\\ell} - \\bar{t}_i = (0.6 + A_i p_\\ell) - (0.6 + A_i \\bar{p}) = A_i(p_\\ell - \\bar{p})$。\n将此代入方差公式：\n$$\nV_i^2 = \\frac{1}{n}\\sum_{\\ell=1}^n \\left(A_i(p_\\ell - \\bar{p})\\right)^2 = A_i^2 \\left(\\frac{1}{n}\\sum_{\\ell=1}^n (p_\\ell - \\bar{p})^2\\right) = A_i^2 \\sigma_p^2\n$$\n其中 $\\sigma_p$ 是基础模式 $p$ 的总体标准差。由于所有振幅 $A_i$ 都是正数，所以 $V_i = A_i \\sigma_p$。\n\n让我们为给定的模式 $p = [\\,-0.3,\\,-0.2,\\,-0.1,\\,0.0,\\,0.1,\\,0.2,\\,0.3,\\,-0.1,\\,0.1,\\,0.0,\\,-0.2,\\,0.2\\,]$ 计算 $\\sigma_p$。\n元素之和为 $\\sum p_\\ell = 0$，所以均值 $\\bar{p}=0$。\n平方和为 $\\sum p_\\ell^2 = (-0.3)^2 + (-0.2)^2 + (-0.1)^2 + 0.0^2 + 0.1^2 + 0.2^2 + 0.3^2 + (-0.1)^2 + 0.1^2 + 0.0^2 + (-0.2)^2 + 0.2^2 = 0.09+0.04+0.01+0+0.01+0.04+0.09+0.01+0.01+0+0.04+0.04 = 0.38$。\n所以，$\\sum p_\\ell^2 = 0.38$。总体方差为 $\\sigma_p^2 = \\frac{1}{n} \\sum p_\\ell^2 = 0.38 / 12$。\n标准差为 $\\sigma_p = \\sqrt{0.38 / 12} \\approx 0.177951$。\n\n现在，我们使用每位训练参与者指定的振幅 $A_i$ 来计算 $V_i = A_i \\sigma_p$：\n- $A_1=0.16 \\implies V_1 \\approx 0.028472$\n- $A_2=0.20 \\implies V_2 \\approx 0.035590$\n- $A_3=0.24 \\implies V_3 \\approx 0.042708$\n- $A_4=0.28 \\implies V_4 \\approx 0.049826$\n- $A_5=0.32 \\implies V_5 \\approx 0.056944$\n- $A_6=0.36 \\implies V_6 \\approx 0.064062$\n- $A_7=0.40 \\implies V_7 \\approx 0.071181$\n- $A_8=0.44 \\implies V_8 \\approx 0.078299$\n\n这 8 个值构成了因变量集合 $\\{V_i\\}_{i=1}^8$。\n\n**步骤 3：拟合线性回归模型**\n\n模型为 $\\hat{V} = a + b C$。OLS 参数 $(a,b)$ 使用以下公式计算：\n$$\nb = \\frac{\\sum_{i=1}^8 (C_i - \\bar{C})(V_i - \\bar{V})}{\\sum_{i=1}^8 (C_i - \\bar{C})^2}, \\quad a = \\bar{V} - b\\,\\bar{C}\n$$\n首先，我们计算训练数据向量 $\\{C_i\\}$ 和 $\\{V_i\\}$ 的均值：\n- $\\bar{C} = \\frac{1}{8} \\sum C_i \\approx 0.567434$\n- $\\bar{V} = \\frac{1}{8} \\sum V_i \\approx 0.053385$\n\n接下来，我们计算斜率 $b$ 的分子（协方差项）和分母（方差项）：\n- $\\sum (C_i - \\bar{C})(V_i - \\bar{V}) \\approx 0.005117$\n- $\\sum (C_i - \\bar{C})^2 \\approx 0.088001$\n\n斜率为 $b = 0.005117 / 0.088001 \\approx 0.058146$。\n截距为 $a = \\bar{V} - b\\bar{C} \\approx 0.053385 - (0.058146 \\times 0.567434) \\approx 0.020412$。\n\n最终的预测模型近似为 $\\hat{V} = 0.020412 + 0.058146 C$。\n\n**步骤 4：对测试案例进行预测**\n\n我们现在使用这个模型来为 3 个测试案例预测 RT 变异性。对于每个案例，我们首先根据提供的相关性计算其连接性指数 $C_{test}$，然后将其代入回归方程。\n\n- **测试案例 1**: $r_{test,1} = [\\,0.60,\\,0.58,\\,0.62,\\,0.59,\\,0.61,\\,0.60\\,]$\n  - $C_{test,1} = \\frac{1}{6} \\sum \\operatorname{atanh}(r_{test,1}) \\approx 0.693383$\n  - $\\hat{V}_{test,1} = a + b C_{test,1} \\approx 0.020412 + (0.058146 \\times 0.693383) \\approx 0.060714$\n\n- **测试案例 2**: $r_{test,2} = [\\,0.05,\\,0.08,\\,0.02,\\,0.04,\\,0.06,\\,0.03\\,]$\n  - $C_{test,2} = \\frac{1}{6} \\sum \\operatorname{atanh}(r_{test,2}) \\approx 0.046726$\n  - $\\hat{V}_{test,2} = a + b C_{test,2} \\approx 0.020412 + (0.058146 \\times 0.046726) \\approx 0.023129$\n\n- **测试案例 3**: $r_{test,3} = [\\,0.82,\\,0.80,\\,0.83,\\,0.81,\\,0.84,\\,0.82\\,]$\n  - $C_{test,3} = \\frac{1}{6} \\sum \\operatorname{atanh}(r_{test,3}) \\approx 1.158098$\n  - $\\hat{V}_{test,3} = a + b C_{test,3} \\approx 0.020412 + (0.058146 \\times 1.158098) \\approx 0.087799$\n\n对于测试案例 1、2 和 3，最终预测的 RT 变异性值（四舍五入到六位小数）分别约为 $0.060714$ 秒、$0.023129$ 秒和 $0.087799$ 秒。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements a predictive model for RT variability from DMN connectivity.\n    \"\"\"\n\n    # --- Training Data Specification ---\n    # Correlation coefficients for N=8 training participants (M=6 pairs each)\n    training_correlations = [\n        [0.15, 0.20, 0.10, 0.12, 0.18, 0.16],  # P1\n        [0.25, 0.28, 0.22, 0.24, 0.26, 0.27],  # P2\n        [0.35, 0.33, 0.32, 0.36, 0.37, 0.34],  # P3\n        [0.45, 0.48, 0.42, 0.44, 0.46, 0.47],  # P4\n        [0.55, 0.52, 0.56, 0.58, 0.54, 0.57],  # P5\n        [0.65, 0.62, 0.63, 0.66, 0.67, 0.64],  # P6\n        [0.70, 0.72, 0.68, 0.69, 0.71, 0.73],  # P7\n        [0.75, 0.78, 0.74, 0.76, 0.77, 0.79],  # P8\n    ]\n\n    # Amplitudes for generating reaction times\n    training_amplitudes = [0.16, 0.20, 0.24, 0.28, 0.32, 0.36, 0.40, 0.44]\n\n    # Base pattern for reaction time trials\n    base_pattern = np.array([-0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, -0.1, 0.1, 0.0, -0.2, 0.2])\n\n    # --- Test Suite Data ---\n    test_cases_correlations = [\n        [0.60, 0.58, 0.62, 0.59, 0.61, 0.60],  # Test case 1\n        [0.05, 0.08, 0.02, 0.04, 0.06, 0.03],  # Test case 2\n        [0.82, 0.80, 0.83, 0.81, 0.84, 0.82],  # Test case 3\n    ]\n\n    # === Step 1: Compute training variables C_i and V_i ===\n\n    # Compute DMN connectivity index (C_i) for each training participant\n    # C_i is the mean of the Fisher z-transformed correlations.\n    C_train = np.array([\n        np.mean(np.arctanh(r_set)) for r_set in training_correlations\n    ])\n\n    # Compute RT variability (V_i) for each training participant\n    # V_i is the population standard deviation of reaction times.\n    # As derived, V_i = A_i * population_std(base_pattern).\n    # numpy.std calculates population std by default (ddof=0).\n    sigma_p = np.std(base_pattern)\n    V_train = np.array(training_amplitudes) * sigma_p\n\n    # === Step 2: Fit the linear regression model ===\n    # Model: V_hat = a + b*C\n    # Using ordinary least squares (OLS) formulas.\n\n    # Calculate means of the training data vectors\n    C_mean = np.mean(C_train)\n    V_mean = np.mean(V_train)\n\n    # Calculate the slope (b)\n    numerator = np.sum((C_train - C_mean) * (V_train - V_mean))\n    denominator = np.sum((C_train - C_mean)**2)\n    b = numerator / denominator\n\n    # Calculate the intercept (a)\n    a = V_mean - b * C_mean\n\n    # === Step 3: Make predictions for test cases ===\n    \n    # Compute DMN connectivity index (C_test) for each test case\n    C_test = np.array([\n        np.mean(np.arctanh(r_set)) for r_set in test_cases_correlations\n    ])\n    \n    # Predict RT variability (V_hat) using the fitted model\n    V_predicted = a + b * C_test\n\n    # --- Format and print the final output ---\n    results = [f\"{val:.6f}\" for val in V_predicted]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}