{
    "hands_on_practices": [
        {
            "introduction": "The theoretical framework of Reward Prediction Error (RPE) is grounded in a precise mathematical formula from temporal-difference (TD) learning. This first exercise provides a foundational practice in calculating the TD error, $\\delta_t$. By working through a scenario where an immediate reward is absent, you will gain a deeper intuition for how RPE signals are driven not just by what you get, but by how your future prospects change. ",
            "id": "5001000",
            "problem": "In the basal ganglia, phasic activity of dopaminergic neurons in the Substantia nigra pars compacta (SNc) is widely modeled as reporting a reward prediction error that trains striatal circuits for action selection. Consider an actor-critic formulation in which the critic maintains a state-value function $V(s)$ and learning is driven by the temporal-difference error $\\delta_t$. Starting from the definition of the state-value function as the expected discounted return, $V(s_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k} \\mid s_t\\right]$, and the Bellman consistency condition $V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$, derive the expression for the sample temporal-difference error $\\delta_t$ in terms of $r_t$, $\\gamma$, $V(s_t)$, and $V(s_{t+1})$. Then, for a single trial in which $r_t = 0$, $\\gamma = 0.9$, $V(s_t) = 0.5$, and $V(s_{t+1}) = 0.6$, compute the numerical value of $\\delta_t$. Briefly explain whether, under the standard interpretation linking $\\delta_t$ to phasic dopaminergic signaling in the striatum, dopamine should increase or decrease relative to baseline for this transition.\n\nReport only the numerical value of $\\delta_t$ as a pure number (arbitrary units). Do not include units and do not round.",
            "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- Definition of the state-value function: $V(s_t) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k} \\mid s_t\\right]$\n- Bellman consistency condition: $V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$\n- The temporal-difference error is denoted by $\\delta_t$.\n- Task 1: Derive the expression for the sample temporal-difference error $\\delta_t$ in terms of $r_t$, $\\gamma$, $V(s_t)$, and $V(s_{t+1})$.\n- Task 2: For a single trial, compute the numerical value of $\\delta_t$ given the values:\n  - $r_t = 0$\n  - $\\gamma = 0.9$\n  - $V(s_t) = 0.5$\n  - $V(s_{t+1}) = 0.6$\n- Task 3: Briefly explain the implication for phasic dopaminergic signaling.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is based on the standard temporal-difference (TD) learning framework, a cornerstone of modern reinforcement learning and computational neuroscience. The definitions of the state-value function and the Bellman equation are correct. The premise that phasic dopamine activity reports a reward prediction error ($\\delta_t$) is a central, well-established, and empirically supported hypothesis in neurobiology. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly stated. It asks for a standard derivation followed by a numerical calculation using provided, complete data. The existence of a unique, stable, and meaningful solution is guaranteed.\n- **Objective**: The problem uses precise, standard terminology from the field and is free of subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically grounded, well-posed, and objective. There are no contradictions, missing information, or violations of fundamental principles. A complete solution will be provided.\n\n### Derivation and Calculation\nThe starting point for deriving the temporal-difference (TD) error is the Bellman consistency condition for the state-value function, $V(s_t)$:\n$$V(s_t) = \\mathbb{E}\\left[r_t + \\gamma V(s_{t+1}) \\mid s_t\\right]$$\nThis equation asserts that the value of the current state, $s_t$, is the expectation of the sum of the immediate reward, $r_t$, and the discounted value of the subsequent state, $s_{t+1}$. The expectation $\\mathbb{E}[\\cdot]$ is taken over the possible rewards and next states, given the current state $s_t$.\n\nIn a real-world scenario or a simulation, an agent experiences a single trajectory, not an expectation over all possible trajectories. For a specific transition at time $t$, the agent moves from state $s_t$ to $s_{t+1}$ and receives a reward $r_t$. From this single sample, we can construct an improved, or updated, estimate of $V(s_t)$. This estimate is the \"TD target\", and it is formed by taking the quantity inside the expectation from the Bellman equation for this specific instance:\n$$ \\text{TD Target} = r_t + \\gamma V(s_{t+1}) $$\nThe term $V(s_{t+1})$ is the current estimate of the value of the next state. The TD target combines the actually received reward $r_t$ with this existing estimate, providing a more informed valuation of $s_t$ than the old value $V(s_t)$.\n\nThe temporal-difference error, $\\delta_t$, is defined as the difference between this new, sample-based estimate (the TD target) and the old estimate, $V(s_t)$.\n$$ \\delta_t = (\\text{TD Target}) - V(s_t) $$\nSubstituting the expression for the TD target, we arrive at the standard formula for the sample temporal-difference error:\n$$ \\delta_t = \\left( r_t + \\gamma V(s_{t+1}) \\right) - V(s_t) $$\nThis expression represents the error in the prediction $V(s_t)$ when compared to the outcome realized one time-step later.\n\nThe problem provides the following values for a single trial:\n- Reward: $r_t = 0$\n- Discount factor: $\\gamma = 0.9$\n- Value of current state: $V(s_t) = 0.5$\n- Value of next state: $V(s_{t+1}) = 0.6$\n\nWe substitute these numerical values into the derived expression for $\\delta_t$:\n$$ \\delta_t = (0 + 0.9 \\times 0.6) - 0.5 $$\nFirst, we compute the product inside the parentheses:\n$$ 0.9 \\times 0.6 = 0.54 $$\nNow, we substitute this result back into the equation for $\\delta_t$:\n$$ \\delta_t = 0.54 - 0.5 $$\n$$ \\delta_t = 0.04 $$\nThe numerical value of the temporal-difference error for this transition is $0.04$.\n\nFinally, we address the link to dopaminergic signaling. The reward prediction error hypothesis posits that phasic changes in dopamine neuron firing encode $\\delta_t$.\n- A positive prediction error ($\\delta_t > 0$) signifies that the outcome was better than expected. This is associated with a phasic increase (a burst) of dopamine neuron firing.\n- A negative prediction error ($\\delta_t  0$) signifies that the outcome was worse than expected. This is associated with a phasic decrease (a pause) in dopamine neuron firing below the baseline tonic rate.\n- A zero prediction error ($\\delta_t = 0$) signifies that the outcome was exactly as expected, resulting in no change to the tonic firing rate.\n\nIn this case, $\\delta_t = 0.04$, which is a positive value. This indicates a positive reward prediction error. Despite the immediate reward being zero ($r_t=0$), the agent transitioned to a state of higher value ($V(s_{t+1}) = 0.6$) than was fully anticipated by its initial state value ($V(s_t) = 0.5$). The discounted value of the next state, $0.9 \\times 0.6 = 0.54$, is greater than the initial value of $0.5$. The outcome was \"better than expected\". Therefore, this event should trigger a phasic increase in dopamine release in the striatum.",
            "answer": "$$\n\\boxed{0.04}\n$$"
        },
        {
            "introduction": "With the ability to calculate RPE, we can now use it to explain complex behavioral phenomena that were once puzzling to learning theorists. This practice explores 'blocking' and 'unblocking,' two classic effects in Pavlovian conditioning. You will see how the presence or absence of a prediction error signal determines whether a new association is learned, demonstrating the powerful explanatory scope of the RPE hypothesis. ",
            "id": "5058233",
            "problem": "A classical Pavlovian conditioning experiment is conducted in two phases with cues and rewards that are standard in the study of dopaminergic neuron signaling. In Phase $1$, a single cue $A$ is repeatedly paired with a reward of magnitude $R_1$ in dimensionless normalized units such that training reaches asymptote. Empirically and theoretically, at asymptote the expected value encoded for cue $A$ equals the delivered reward, that is $V(A)=R_1$. In Phase $2$, the compound cue $AB$ is presented under two separate conditions. In Condition $B_{\\text{lock}}$ (blocking), the delivered reward remains $R_1$ when $AB$ is presented. In Condition $U_{\\text{nblock}}$ (unblocking), the delivered reward is increased to $R_2R_1$ when $AB$ is presented. Assume the expected value for a compound cue is the additive sum of associative strengths, so $V(AB)=V(A)+V(B)$, and the novel cue $B$ initially has $V(B)=0$ at the start of Phase $2$ because it has never been paired with reward. Take the delivered rewards to be $R_1=1.0$ and $R_2=1.4$ in dimensionless normalized units.\n\nDopaminergic neurons encode the reward prediction error, defined at the time of reward delivery as the difference between obtained reward and expected reward. Using this as the fundamental base, derive the predicted dopamine signals at the moment of reward delivery for Condition $B_{\\text{lock}}$ and Condition $U_{\\text{nblock}}$ by computing the reward prediction errors from first principles. Report your final answer as a row matrix containing the two real numbers corresponding to the reward prediction errors for $B_{\\text{lock}}$ and $U_{\\text{nblock}}$, respectively, in dimensionless normalized units. No rounding is required.",
            "solution": "The problem is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- In Phase $1$, cue $A$ is paired with reward $R_1$ until asymptote.\n- At asymptote, the expected value of cue $A$ is $V(A) = R_1$.\n- In Phase $2$, a compound cue $AB$ is presented.\n- The initial value of the novel cue $B$ at the start of Phase $2$ is $V(B) = 0$.\n- The expected value for the compound cue $AB$ is the additive sum of associative strengths: $V(AB) = V(A) + V(B)$.\n- In Condition $B_{\\text{lock}}$ (blocking), the delivered reward for cue $AB$ is $R_1$.\n- In Condition $U_{\\text{nblock}}$ (unblocking), the delivered reward for cue $AB$ is $R_2$.\n- The relationship between the rewards is $R_2  R_1$.\n- The specific numerical values for the rewards are $R_1 = 1.0$ and $R_2 = 1.4$.\n- The definition of the reward prediction error ($\\delta$) is the difference between the obtained reward ($R$) and the expected reward ($V$): $\\delta = R - V$.\n- The task is to compute the reward prediction errors for Condition $B_{\\text{lock}}$ and Condition $U_{\\text{nblock}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated for validity.\n- **Scientifically Grounded:** The problem is a classic formulation in computational neuroscience, specifically relating to reinforcement learning models of dopaminergic function, such as the Rescorla-Wagner model. The concepts of associative value ($V$), compound cues, the blocking phenomenon, and reward prediction error ($\\delta$) are cornerstones of this field. The stated relationship $\\delta = R - V$ is the canonical definition of reward prediction error.\n- **Well-Posed:** The problem is fully self-contained. It provides all necessary definitions ($V(AB) = V(A) + V(B)$, $\\delta = R - V$), initial conditions ($V(A) = R_1$ from Phase $1$, $V(B) = 0$ at the start of Phase $2$), and parameters ($R_1$, $R_2$) to compute a unique solution for the two specified conditions.\n- **Objective:** The problem is stated in precise, formal language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the listed invalidity flaws. It is scientifically sound, well-posed, and objective.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\nThe core of the problem is to calculate the reward prediction error, denoted here by $\\delta$, at the moment of reward delivery for two distinct experimental conditions. The fundamental equation given is:\n$$\n\\delta = R_{\\text{obtained}} - V_{\\text{expected}}\n$$\nwhere $R_{\\text{obtained}}$ is the actual reward delivered and $V_{\\text{expected}}$ is the total expected reward based on the cues present.\n\nAt the start of Phase $2$, the system has been trained such that the value of cue $A$ has reached an asymptote where it equals the reward with which it was paired.\n$$\nV(A) = R_1\n$$\nCue $B$ is novel, meaning it has no prior association with reward.\n$$\nV(B) = 0\n$$\nIn Phase $2$, the compound cue $AB$ is presented. According to the problem statement, the expected value of this compound cue is the sum of the individual values of its components.\n$$\nV_{\\text{expected}} = V(AB) = V(A) + V(B)\n$$\nSubstituting the known values for $V(A)$ and $V(B)$ at the beginning of Phase $2$:\n$$\nV_{\\text{expected}} = R_1 + 0 = R_1\n$$\nThis expected value, $V_{\\text{expected}} = R_1$, is the same for both conditions, $B_{\\text{lock}}$ and $U_{\\text{nblock}}$, because the expectation is formed based on prior learning before the new reward is delivered.\n\nNow, we compute the reward prediction error for each condition separately.\n\n**Condition $B_{\\text{lock}}$ (blocking):**\nIn this condition, the delivered reward, $R_{\\text{obtained}}$, is $R_1$.\nThe reward prediction error, $\\delta_{\\text{lock}}$, is:\n$$\n\\delta_{\\text{lock}} = R_{\\text{obtained}} - V_{\\text{expected}} = R_1 - R_1 = 0\n$$\nThis result, a prediction error of zero, is the central feature of the blocking effect. Because the reward is fully predicted by the pre-trained cue $A$, there is no \"surprise\" in the outcome. Consequently, no new learning about cue $B$ occurs. The dopaminergic signal is predicted to show no change from its baseline firing rate.\n\n**Condition $U_{\\text{nblock}}$ (unblocking):**\nIn this condition, the delivered reward, $R_{\\text{obtained}}$, is increased to $R_2$.\nThe reward prediction error, $\\delta_{\\text{unblock}}$, is:\n$$\n\\delta_{\\text{unblock}} = R_{\\text{obtained}} - V_{\\text{expected}} = R_2 - R_1\n$$\nThis is a positive prediction error because the problem states $R_2  R_1$. This \"surprise\" signal indicates that the outcome was better than expected. This positive error signal is thought to drive learning, leading to an increase in the associative strength of the cues present (in this case, cue $B$, since $V(A)$ is already at its asymptote with respect to $R_1$). The dopaminergic signal is predicted to show a phasic burst of activity.\n\nFinally, we substitute the provided numerical values, $R_1 = 1.0$ and $R_2 = 1.4$.\n\nFor Condition $B_{\\text{lock}}$:\n$$\n\\delta_{\\text{lock}} = 0\n$$\n\nFor Condition $U_{\\text{nblock}}$:\n$$\n\\delta_{\\text{unblock}} = R_2 - R_1 = 1.4 - 1.0 = 0.4\n$$\n\nThe final answer is to be reported as a row matrix containing these two values, $[\\delta_{\\text{lock}}, \\delta_{\\text{unblock}}]$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  0.4 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The final piece of the puzzle is understanding how the brain uses the RPE signal to physically implement learning. This practice introduces the 'three-factor learning rule,' a widely accepted model for how the RPE, broadcast by dopamine, modifies synaptic connections. By calculating the change in synaptic weight, you will connect the abstract computational signal of RPE to the concrete biological process of synaptic plasticity. ",
            "id": "5058243",
            "problem": "In a classical conditioning paradigm, dopaminergic neurons in the midbrain broadcast a Reward Prediction Error (RPE) signal, denoted $\\delta_t$, that reports the instantaneous difference between received and expected reward. A corticostriatal synapse onto a medium spiny neuron maintains an eligibility trace, denoted $e_t$, which reflects recent pre- and postsynaptic activity and decays over time. Empirical and theoretical work supports a three-factor learning rule: synaptic weight change at time $t$, denoted $\\Delta w$, depends on a local eligibility factor, a global modulatory teaching signal, and a learning rate. Assume the following foundational principles:\n- If either the eligibility trace or the teaching signal is zero, there is no plasticity at that moment.\n- Near the operating point, the synaptic update can be modeled by the lowest-order (linear) interaction between the eligibility trace and the teaching signal.\n- A positive learning rate $\\alpha$ scales the overall magnitude of change.\n\nGiven that at a particular time $t$, the eligibility trace is $e_t = 0.4$, the Reward Prediction Error is $\\delta_t = 0.2$, and the learning rate is $\\alpha = 0.05$ (all quantities are unitless), do the following:\n1. From the principles above, write down the simplest linear expression for $\\Delta w$ consistent with the three-factor rule.\n2. Using the provided values, compute the numerical value of $\\Delta w$.\n3. Based on the sign of $\\Delta w$, state whether the synapse should undergo potentiation or depression at this time.\n\nReport the numerical value of $\\Delta w$ as your final answer. Express the final answer as an exact decimal (no rounding needed) and note that the synaptic weight is unitless. Provide your reasoning and interpretation separately from the final numeric answer.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of computational neuroscience, specifically the theory of reinforcement learning as applied to synaptic plasticity. The problem is well-posed, objective, and contains all necessary information to derive a unique solution without contradiction.\n\nThe problem asks for three components: the derivation of the learning rule, the calculation of the synaptic weight change $\\Delta w$, and the interpretation of this change.\n\n1.  **Formulation of the Synaptic Update Rule**\n\nThe task is to find the simplest mathematical expression for the synaptic weight change, $\\Delta w$, consistent with the three-factor learning rule described. The three factors are:\n-   The learning rate, $\\alpha$.\n-   The eligibility trace, $e_t$.\n-   The teaching signal, which is the Reward Prediction Error (RPE), $\\delta_t$.\n\nThe problem provides three foundational principles to guide the formulation:\n-   Principle 1: If either the eligibility trace ($e_t$) or the teaching signal ($\\delta_t$) is zero, there is no plasticity. Mathematically, if $e_t = 0$ or $\\delta_t = 0$, then $\\Delta w = 0$.\n-   Principle 2: The update rule is modeled by the \"lowest-order (linear) interaction\" between the eligibility trace $e_t$ and the teaching signal $\\delta_t$. In a Taylor expansion of a function of two variables $f(x, y)$ around $(0,0)$, the lowest-order interaction term is of the form $c \\cdot x \\cdot y$. Higher-order interactions would be terms like $x^2 y$, $x y^2$, etc. Thus, the interaction between $e_t$ and $\\delta_t$ implies a term proportional to their product, $e_t \\delta_t$. This product form also satisfies Principle 1, as the expression becomes zero if either $e_t$ or $\\delta_t$ is zero. An additive form, such as $A e_t + B \\delta_t$, would not satisfy Principle 1 for arbitrary non-zero values of the other variable unless the coefficients $A$ and $B$ were zero.\n-   Principle 3: A positive learning rate $\\alpha$ scales the overall magnitude of the change. This implies that $\\alpha$ acts as a constant of proportionality.\n\nCombining these principles, the synaptic weight change $\\Delta w$ is directly proportional to the product of the learning rate, the eligibility trace, and the reward prediction error. The simplest expression that satisfies all three conditions is:\n$$\n\\Delta w = \\alpha \\cdot e_t \\cdot \\delta_t\n$$\n\nThis equation is a cornerstone of reinforcement learning models of the basal ganglia.\n\n2.  **Calculation of the Numerical Value of $\\Delta w$**\n\nThe problem provides the following values at a specific time $t$:\n-   Learning rate: $\\alpha = 0.05$\n-   Eligibility trace: $e_t = 0.4$\n-   Reward Prediction Error: $\\delta_t = 0.2$\n\nSubstituting these values into the derived equation for $\\Delta w$:\n$$\n\\Delta w = (0.05) \\cdot (0.4) \\cdot (0.2)\n$$\n\nThe calculation proceeds as follows:\n$$\n\\Delta w = (0.020) \\cdot (0.2)\n$$\n$$\n\\Delta w = 0.004\n$$\nAll quantities are given as unitless, so the resulting synaptic weight change is also unitless.\n\n3.  **Interpretation of the Sign of $\\Delta w$**\n\nThe term $\\Delta w$ represents the change in the strength, or weight, of the synapse. The sign of $\\Delta w$ determines the direction of plasticity.\n-   If $\\Delta w  0$, the synaptic weight increases. This strengthening of the synapse is known as **long-term potentiation (LTP)**.\n-   If $\\Delta w  0$, the synaptic weight decreases. This weakening of the synapse is known as **long-term depression (LTD)**.\n-   If $\\Delta w = 0$, there is no change in synaptic weight.\n\nIn this case, the calculated value is $\\Delta w = 0.004$, which is a positive number. Therefore, the synapse should undergo potentiation.\n\nThis result is consistent with the theory of reinforcement learning. The positive reward prediction error ($\\delta_t = 0.2  0$) indicates that the outcome was better than expected. The learning rule acts to strengthen the recently active synapses (indicated by a non-zero eligibility trace, $e_t=0.4$) to make the behavior that led to this positive outcome more likely in the future.",
            "answer": "$$\n\\boxed{0.004}\n$$"
        }
    ]
}