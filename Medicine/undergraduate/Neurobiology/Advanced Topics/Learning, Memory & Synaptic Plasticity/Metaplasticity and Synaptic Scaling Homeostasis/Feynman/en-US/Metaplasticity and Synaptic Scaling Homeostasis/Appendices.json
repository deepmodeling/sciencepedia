{
    "hands_on_practices": [
        {
            "introduction": "Synaptic scaling is a powerful homeostatic mechanism that allows a neuron to adjust its overall sensitivity without erasing the learned information stored in its synaptic weights. A key feature of this process is that it preserves the *relative* strengths of synapses, strengthening or weakening them all by the same proportion. This exercise  provides a hands-on opportunity to understand the mathematical signature of this process. You will derive from first principles why the coefficient of variation ($CV$), a measure of relative variability, remains unchanged by multiplicative scaling, and then verify this with a concrete numerical example.",
            "id": "5032163",
            "problem": "A postsynaptic neuron exhibits metaplasticity and homeostatic synaptic scaling: when its long-term average firing deviates from a target, all excitatory synaptic strengths are multiplicatively adjusted by the same scale factor to restore activity while preserving relative differences across synapses. Consider a sample of baseline Excitatory Postsynaptic Current (EPSC) amplitudes (in picoamperes, $\\mathrm{pA}$) recorded from $6$ synapses: $\\{10, 12, 14, 16, 18, 20\\}\\,\\mathrm{pA}$. Under homeostatic synaptic scaling, each amplitude $a_{i}$ is transformed to $a_{i}'$ by a common multiplicative factor $s = 1.3$, such that $a_{i}' = s\\,a_{i}$. Using the core definitions that the sample mean is $\\bar{a} = \\frac{1}{n}\\sum_{i=1}^{n} a_{i}$, the sample standard deviation is $\\sigma = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i}-\\bar{a})^{2}}$, and the coefficient of variation is $\\mathrm{CV} = \\frac{\\sigma}{\\bar{a}}$, derive from first principles why a uniform multiplicative scaling by $s$ leaves $\\mathrm{CV}$ unchanged. Then, compute the scaled amplitudes $a_{i}'$ for the given dataset and verify numerically that the coefficient of variation is unchanged. Report the common coefficient of variation value as a unitless decimal. Round your final reported value to four significant figures. Express amplitudes in $\\mathrm{pA}$.",
            "solution": "The problem asks to demonstrate, first through formal derivation and then through numerical calculation, that the coefficient of variation ($\\mathrm{CV}$) of a dataset is invariant under a uniform multiplicative scaling.\n\nLet the initial set of $n$ Excitatory Postsynaptic Current (EPSC) amplitudes be denoted by $\\{a_1, a_2, \\ldots, a_n\\}$. The problem provides the following definitions:\nThe sample mean:\n$$ \\bar{a} = \\frac{1}{n}\\sum_{i=1}^{n} a_{i} $$\nThe sample standard deviation:\n$$ \\sigma = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i}-\\bar{a})^{2}} $$\nThe coefficient of variation:\n$$ \\mathrm{CV} = \\frac{\\sigma}{\\bar{a}} $$\n\n**Part 1: Formal Derivation**\n\nWe begin by establishing the aforementioned properties of the original dataset. The homeostatic scaling process transforms each amplitude $a_i$ into a new amplitude $a_i'$ using a common multiplicative factor $s$:\n$$ a_{i}' = s \\cdot a_{i} $$\n\nWe now derive the statistical properties of the new, scaled dataset $\\{a_1', a_2', \\ldots, a_n'\\}$.\n\nFirst, we compute the new sample mean, $\\bar{a}'$:\n$$ \\bar{a}' = \\frac{1}{n}\\sum_{i=1}^{n} a_{i}' = \\frac{1}{n}\\sum_{i=1}^{n} (s \\cdot a_{i}) $$\nSince $s$ is a constant factor, it can be moved outside the summation:\n$$ \\bar{a}' = s \\left( \\frac{1}{n}\\sum_{i=1}^{n} a_{i} \\right) = s \\cdot \\bar{a} $$\nThis shows that the mean of the scaled data is simply the original mean multiplied by the scaling factor $s$.\n\nNext, we compute the new sample standard deviation, $\\sigma'$. Using its definition and substituting the expressions for $a_i'$ and $\\bar{a}'$:\n$$ \\sigma' = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i}'-\\bar{a}')^{2}} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (s \\cdot a_{i} - s \\cdot \\bar{a})^{2}} $$\nWe factor out the common term $s$ inside the parentheses:\n$$ \\sigma' = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} [s(a_{i} - \\bar{a})]^{2}} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} s^{2}(a_{i} - \\bar{a})^{2}} $$\nThe constant factor $s^2$ can be moved outside the summation:\n$$ \\sigma' = \\sqrt{s^{2} \\left[ \\frac{1}{n-1}\\sum_{i=1}^{n} (a_{i} - \\bar{a})^{2} \\right]} $$\nRecognizing the term in the square brackets as the original sample variance, $\\sigma^2$, we have:\n$$ \\sigma' = \\sqrt{s^{2} \\sigma^{2}} = |s| \\sqrt{\\sigma^{2}} = |s| \\sigma $$\nThe problem describes homeostatic scaling, where the factor $s$ is a positive multiplier ($s=1.3$), so $|s|=s$. Thus:\n$$ \\sigma' = s \\cdot \\sigma $$\nThe standard deviation of the scaled data is the original standard deviation multiplied by the scaling factor $s$.\n\nFinally, we compute the new coefficient of variation, $\\mathrm{CV}'$:\n$$ \\mathrm{CV}' = \\frac{\\sigma'}{\\bar{a}'} $$\nSubstituting the derived expressions for $\\bar{a}'$ and $\\sigma'$:\n$$ \\mathrm{CV}' = \\frac{s \\cdot \\sigma}{s \\cdot \\bar{a}} = \\frac{\\sigma}{\\bar{a}} = \\mathrm{CV} $$\nThis derivation from first principles proves that the coefficient of variation is invariant under a uniform multiplicative scaling of the data. This is because the $\\mathrm{CV}$ is a measure of relative variability, and scaling the data affects both the measure of central tendency (mean) and the measure of dispersion (standard deviation) by the exact same factor, which cancels out in their ratio.\n\n**Part 2: Numerical Verification and Calculation**\n\nWe are given the baseline EPSC amplitudes: $\\{10, 12, 14, 16, 18, 20\\}\\,\\mathrm{pA}$. Here, $n=6$.\n\nFirst, we calculate the statistics for the original dataset.\nThe sum of the amplitudes is $\\sum_{i=1}^{6} a_{i} = 10 + 12 + 14 + 16 + 18 + 20 = 90$.\nThe sample mean is:\n$$ \\bar{a} = \\frac{90}{6} = 15\\,\\mathrm{pA} $$\nThe sum of squared deviations from the mean is:\n$$ \\sum_{i=1}^{6} (a_{i}-\\bar{a})^{2} = (10-15)^{2} + (12-15)^{2} + (14-15)^{2} + (16-15)^{2} + (18-15)^{2} + (20-15)^{2} \\\\ = (-5)^{2} + (-3)^{2} + (-1)^{2} + (1)^{2} + (3)^{2} + (5)^{2} \\\\ = 25 + 9 + 1 + 1 + 9 + 25 = 70 $$\nThe sample standard deviation is:\n$$ \\sigma = \\sqrt{\\frac{70}{6-1}} = \\sqrt{\\frac{70}{5}} = \\sqrt{14}\\,\\mathrm{pA} $$\nThe coefficient of variation for the original data is:\n$$ \\mathrm{CV} = \\frac{\\sigma}{\\bar{a}} = \\frac{\\sqrt{14}}{15} $$\n\nNow, we apply the scaling factor $s=1.3$ to compute the new amplitudes $a_i' = 1.3 \\cdot a_i$:\n$a_{1}' = 1.3 \\times 10 = 13\\,\\mathrm{pA}$\n$a_{2}' = 1.3 \\times 12 = 15.6\\,\\mathrm{pA}$\n$a_{3}' = 1.3 \\times 14 = 18.2\\,\\mathrm{pA}$\n$a_{4}' = 1.3 \\times 16 = 20.8\\,\\mathrm{pA}$\n$a_{5}' = 1.3 \\times 18 = 23.4\\,\\mathrm{pA}$\n$a_{6}' = 1.3 \\times 20 = 26\\,\\mathrm{pA}$\nThe scaled amplitudes are $\\{13, 15.6, 18.2, 20.8, 23.4, 26\\}\\,\\mathrm{pA}$.\n\nWe now compute the statistics for this scaled dataset to verify the derivation.\nThe new mean is $\\bar{a}' = s \\cdot \\bar{a} = 1.3 \\times 15 = 19.5\\,\\mathrm{pA}$.\nThe new standard deviation is $\\sigma' = s \\cdot \\sigma = 1.3 \\sqrt{14}\\,\\mathrm{pA}$.\nThe new coefficient of variation is:\n$$ \\mathrm{CV}' = \\frac{\\sigma'}{\\bar{a}'} = \\frac{1.3 \\sqrt{14}}{19.5} = \\frac{1.3 \\sqrt{14}}{1.3 \\times 15} = \\frac{\\sqrt{14}}{15} $$\nAs predicted by the derivation, $\\mathrm{CV}' = \\mathrm{CV}$. The numerical verification is successful.\n\nFinally, we compute the numerical value of the common coefficient of variation and round it to four significant figures.\n$$ \\mathrm{CV} = \\frac{\\sqrt{14}}{15} \\approx \\frac{3.74165738677}{15} \\approx 0.24944382578 $$\nRounding to four significant figures gives $0.2494$.\nThe biological significance is that synaptic scaling adjusts the overall strength of a neuron's inputs to maintain a target activity level, but it does so in a way that preserves the relative weight distribution across its synapses. The invariance of the $\\mathrm{CV}$ is the mathematical signature of this preservation of relative synaptic strength.",
            "answer": "$$\\boxed{0.2494}$$"
        },
        {
            "introduction": "Metaplasticity refers to the 'plasticity of plasticity,' where the rules for synaptic change are themselves modified by neural activity. The Bienenstock-Cooper-Munro (BCM) model provides a classic theoretical framework for this phenomenon, proposing a 'sliding threshold' that determines whether a synapse undergoes long-term potentiation (LTP) or depression (LTD). This practice  delves into the mechanics of the BCM rule. By analyzing the governing equations, you will derive the precise level of postsynaptic activity at which the synapse transitions from weakening to strengthening, revealing how homeostatic regulation is built directly into the learning rule.",
            "id": "5032151",
            "problem": "A single-compartment excitatory neuron receives input from a presynaptic neuron with activity $x_i \\ge 0$ and produces a postsynaptic firing rate $y \\ge 0$. Consider a metaplastic Hebbian learning rule of the Bienenstock–Cooper–Munro (BCM) type, where the synaptic weight $w_i$ evolves according to the local rule $ \\frac{d w_i}{d t} = \\phi(y)\\, (y - \\theta_M)\\, x_i$. Here, $\\phi(y)$ is a nonnegative activity-dependent plasticity gain and $\\theta_M$ is a sliding modification threshold that depends on the recent average activity, thereby implementing metaplasticity and homeostatic synaptic scaling.\n\nAssume the following scientifically plausible forms:\n- The plasticity gain is saturating with postsynaptic activity, $\\phi(y) = \\frac{\\alpha\\, y}{1 + \\gamma\\, y^{2}}$, with constants $\\alpha > 0$ and $\\gamma \\ge 0$.\n- The modification threshold depends on the low-pass filtered postsynaptic activity $\\bar y$ via $\\theta_M(\\bar y) = \\theta_0 + \\beta\\, \\bar y^{2}$, with constants $\\theta_0 > 0$ and $\\beta > 0$.\n- The activity average $\\bar y$ obeys a standard first-order low-pass dynamics $ \\frac{d \\bar y}{d t} = \\frac{y - \\bar y}{\\tau}$ with $\\tau > 0$.\n\nIn the stationary regime with stationary inputs, assume $y$ is constant and equals its own average, so $y = \\bar y$. Working from the given dynamical rule and the stated assumptions only, and using first principles about the sign of $\\frac{d w_i}{d t}$, derive the analytical expression for the lower critical postsynaptic activity $y_{-}$ at which the sign of $\\frac{d w_i}{d t}$ switches from long-term depression (LTD) to long-term potentiation (LTP) as $y$ increases from low values. State $y_{-}$ as a closed-form expression in terms of $\\beta$ and $\\theta_0$ only. You may assume the parameter regime $0 < 4 \\beta \\theta_0 < 1$ so that the threshold crossing is well-defined and real. Express the final answer as a single analytical expression. Do not include units.",
            "solution": "The problem asks for the derivation of the lower critical postsynaptic activity, denoted as $y_{-}$, at which the synaptic weight change $\\frac{d w_i}{d t}$ transitions from long-term depression (LTD) to long-term potentiation (LTP). This transition point corresponds to a change in the sign of $\\frac{d w_i}{d t}$.\n\nThe evolution of the synaptic weight $w_i$ is governed by the metaplastic Hebbian learning rule:\n$$\n\\frac{d w_i}{d t} = \\phi(y)\\, (y - \\theta_M)\\, x_i\n$$\nHere, $x_i \\ge 0$ is the presynaptic activity, $y \\ge 0$ is the postsynaptic firing rate, $\\phi(y)$ is the plasticity gain, and $\\theta_M$ is the sliding modification threshold.\n\nThe sign of $\\frac{d w_i}{d t}$ determines whether the synapse undergoes LTP ($\\frac{d w_i}{d t} > 0$) or LTD ($\\frac{d w_i}{d t} < 0$). The critical points where the sign may switch are the values of $y$ for which $\\frac{d w_i}{d t} = 0$.\n\nThe plasticity gain is given by $\\phi(y) = \\frac{\\alpha\\, y}{1 + \\gamma\\, y^{2}}$. Given the constraints $\\alpha > 0$, $\\gamma \\ge 0$, and that we are interested in non-trivial postsynaptic activity $y > 0$, the gain function $\\phi(y)$ is strictly positive, $\\phi(y) > 0$. The presynaptic activity is given as $x_i \\ge 0$. For any learning to occur, we must assume a non-zero presynaptic input, so we consider $x_i > 0$.\n\nUnder these conditions ($x_i > 0$ and $\\phi(y) > 0$), the equation $\\frac{d w_i}{d t} = 0$ simplifies to the condition that the term $(y - \\theta_M)$ must be zero:\n$$\ny - \\theta_M = 0\n$$\n\nThe modification threshold $\\theta_M$ is a function of the low-pass filtered postsynaptic activity $\\bar y$:\n$$\n\\theta_M(\\bar y) = \\theta_0 + \\beta\\, \\bar y^{2}\n$$\nThe problem specifies that we are in a stationary regime where the postsynaptic activity $y$ is constant. In this case, the low-pass filtered activity $\\bar y$ equals $y$, since if $y$ is constant, $\\frac{d \\bar y}{d t} = \\frac{y - \\bar y}{\\tau} = 0$ implies $y = \\bar y$.\n\nSubstituting $y$ for $\\bar y$ in the expression for $\\theta_M$, we get:\n$$\n\\theta_M(y) = \\theta_0 + \\beta\\, y^{2}\n$$\n\nNow, we substitute this expression for $\\theta_M$ back into our simplified condition for the critical points:\n$$\ny - (\\theta_0 + \\beta\\, y^{2}) = 0\n$$\nThis is a quadratic equation for the postsynaptic activity $y$. Rearranging it into standard form $ay^2 + by + c = 0$, we have:\n$$\n\\beta y^2 - y + \\theta_0 = 0\n$$\n\nWe can solve for $y$ using the quadratic formula, $y = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\ny = \\frac{-(-1) \\pm \\sqrt{(-1)^2 - 4(\\beta)(\\theta_0)}}{2\\beta}\n$$\n$$\ny = \\frac{1 \\pm \\sqrt{1 - 4\\beta\\theta_0}}{2\\beta}\n$$\n\nThe problem states the parameter regime is $0 < 4\\beta\\theta_0 < 1$. This condition ensures that the discriminant, $1 - 4\\beta\\theta_0$, is positive and less than $1$, guaranteeing two distinct, positive, real roots for $y$. Let's denote these two roots as $y_{-}$ and $y_{+}$.\n\nThe lower root is:\n$$\ny_{-} = \\frac{1 - \\sqrt{1 - 4\\beta\\theta_0}}{2\\beta}\n$$\nThe upper root is:\n$$\ny_{+} = \\frac{1 + \\sqrt{1 - 4\\beta\\theta_0}}{2\\beta}\n$$\n\nTo determine which of these corresponds to the transition from LTD to LTP, we must analyze the sign of $\\frac{d w_i}{d t}$ as a function of $y$. The sign is determined by the factor $y - \\theta_M = y - (\\theta_0 + \\beta y^2)$. Let us define a function $f(y) = y - \\theta_0 - \\beta y^2$. This is a downward-opening parabola (since the coefficient of $y^2$, which is $-\\beta$, is negative because $\\beta > 0$) with roots at $y_{-}$ and $y_{+}$.\n\n-   For $0 < y < y_{-}$, the function $f(y)$ is negative. Thus, $\\frac{d w_i}{d t} < 0$, which corresponds to LTD.\n-   For $y = y_{-}$, $f(y) = 0$, and $\\frac{d w_i}{d t} = 0$. This is the first critical point.\n-   For $y_{-} < y < y_{+}$, the function $f(y)$ is positive. Thus, $\\frac{d w_i}{d t} > 0$, which corresponds to LTP.\n-   For $y = y_{+}$, $f(y) = 0$, and $\\frac{d w_i}{d t} = 0$. This is the second critical point where plasticity switches back from LTP to LTD.\n\nThe problem specifically asks for the lower critical postsynaptic activity $y_{-}$ at which the sign of $\\frac{d w_i}{d t}$ switches from LTD to LTP as $y$ increases from low values. Our analysis confirms that this point is the smaller of the two roots, $y_{-}$.\n\nTherefore, the analytical expression for the lower critical activity $y_{-}$ is:\n$$\ny_{-} = \\frac{1 - \\sqrt{1 - 4\\beta\\theta_0}}{2\\beta}\n$$\nThis expression depends only on $\\beta$ and $\\theta_0$, as required.",
            "answer": "$$\n\\boxed{\\frac{1 - \\sqrt{1 - 4\\beta\\theta_0}}{2\\beta}}\n$$"
        },
        {
            "introduction": "In the brain, learning and stability are not separate processes but two sides of the same coin. Hebbian plasticity rapidly encodes new information at specific synapses, but this must be balanced by slower homeostatic mechanisms that prevent runaway activity. This simulation-based practice  allows you to explore this dynamic interplay directly. You will model a scenario where a Hebbian-like potentiation event is followed by global synaptic down-scaling, and then quantify how this homeostatic response successfully stabilizes the neuron while preserving the essential pattern of learned synaptic changes.",
            "id": "5032199",
            "problem": "You are asked to formalize and simulate a neurobiologically realistic scenario of metaplasticity expressing homeostatic synaptic scaling following a Hebbian potentiation event. The goal is to quantify the net change in absolute synaptic weights and assess whether relative synaptic differences are preserved under slow multiplicative scaling. Begin from core definitions: Hebbian plasticity states that synaptic weight change is proportional to the correlation between presynaptic and postsynaptic activity, and homeostatic synaptic scaling is a slow, cell-wide process that adjusts all synaptic weights multiplicatively to stabilize the postsynaptic firing rate around a target rate. In this problem, Long-Term Potentiation (LTP) is applied to a predefined subset of synapses, followed by slow multiplicative scaling that reduces weights to restore the postsynaptic rate to a set point. All quantities must be handled in purely mathematical terms, with synaptic weights in arbitrary units (a.u.) and firing rates in hertz (Hz).\n\nThe system consists of $N$ synapses with weights $w_i(t)$ and constant presynaptic input rates $x_i$ for $i \\in \\{1,\\dots,N\\}$. The postsynaptic firing rate is modeled as a linear sum $r(t) = \\sum_{i=1}^{N} w_i(t) x_i$. At time $t = 0$, apply a Hebbian Long-Term Potentiation (LTP) event to a predefined subset $S \\subset \\{1,\\dots,N\\}$ by increasing the weight $w_i(0)$ on each $i \\in S$ by a fixed fractional amount $\\gamma$; weights on $i \\notin S$ remain unchanged. Then, simulate slow synaptic scaling over a duration $T$ using a small time step $\\Delta t$ and a rate constant $\\beta$, where the scaling acts multiplicatively and uniformly across all synapses to move $r(t)$ toward a target rate $r^*$ chosen as the pre-LTP baseline $r(0)$. Enforce a lower bound $w_{\\min}$ on weights to reflect non-negativity and minimal synaptic efficacy. The scaling process should be implemented algorithmically from the definitions, ensuring scientific realism and avoiding any ad hoc shortcuts. Angle units are not applicable. Express any change in synaptic weights in arbitrary units (a.u.), and rates in hertz (Hz).\n\nYour program must, for each test case, compute the following metrics:\n- $M_1$: the mean absolute change in synaptic weights between the final state and the initial state, defined by\n$$\nM_1 = \\frac{1}{N} \\sum_{i=1}^{N} \\left| w_i(T) - w_i(0) \\right| \\quad \\text{(in a.u.)}.\n$$\n- $M_2$: the maximum normalized ratio error quantifying preservation of relative synaptic differences, using the weights immediately after LTP, denoted $w_i^{\\mathrm{LTP}}$, and the final weights $w_i(T)$. Define\n$$\nE_{ij} = \\left| \\frac{ \\left( w_i(T) / w_j(T) \\right) }{ \\left( w_i^{\\mathrm{LTP}} / w_j^{\\mathrm{LTP}} \\right) } - 1 \\right|, \\quad i \\neq j,\n$$\nand let\n$$\nM_2 = \\max_{i \\neq j} E_{ij} \\quad \\text{(dimensionless)}.\n$$\n- $M_3$: the Pearson correlation coefficient between the final weights $\\mathbf{w}(T)$ and the post-LTP weights $\\mathbf{w}^{\\mathrm{LTP}}$, defined by\n$$\nM_3 = \\frac{\\sum_{i=1}^{N} \\left( w_i(T) - \\overline{w(T)} \\right) \\left( w_i^{\\mathrm{LTP}} - \\overline{w^{\\mathrm{LTP}}} \\right)}{\\sqrt{\\sum_{i=1}^{N} \\left( w_i(T) - \\overline{w(T)} \\right)^2} \\sqrt{\\sum_{i=1}^{N} \\left( w_i^{\\mathrm{LTP}} - \\overline{w^{\\mathrm{LTP}}} \\right)^2}},\n$$\nwhere $\\overline{w(T)}$ and $\\overline{w^{\\mathrm{LTP}}}$ are the means of the final and post-LTP weight vectors, respectively.\n- $M_4$: a boolean indicating whether relative differences are preserved within a tight tolerance, defined as $M_2 \\le \\epsilon$ with $\\epsilon = 10^{-9}$.\n\nImplement the simulation as follows from first principles: compute the pre-LTP baseline $r(0)$, apply the LTP to obtain $\\mathbf{w}^{\\mathrm{LTP}}$, set $r^* = r(0)$, and iterate the slow scaling dynamics using a small time step $\\Delta t$ and rate constant $\\beta$. At each step, adjust all weights multiplicatively in a manner consistent with homeostatic scaling that moves $r(t)$ toward $r^*$, enforce the lower bound $w_{\\min}$, and proceed until $t = T$. After the simulation, compute the metrics $M_1$, $M_2$, $M_3$, and $M_4$ for each test case.\n\nTest suite:\n- Case $1$ (happy path): $N = 10$, initial weights $\\mathbf{w}(0) = [0.4, 0.5, 0.3, 0.6, 0.45, 0.55, 0.35, 0.25, 0.5, 0.4]$ a.u., presynaptic rates $\\mathbf{x} = [5, 7, 6, 3, 4, 8, 2, 1, 5, 4]$ Hz, LTP subset $S = \\{0,1,2,3,4\\}$ (zero-based indexing), fractional LTP $\\gamma = 0.4$, time step $\\Delta t = 0.01$ s, duration $T = 5$ s, rate constant $\\beta = 0.05$, lower bound $w_{\\min} = 0.05$ a.u.\n- Case $2$ (boundary with floor clipping): $N = 10$, initial weights $\\mathbf{w}(0) = [0.12, 0.15, 0.1, 0.08, 0.2, 0.18, 0.11, 0.09, 0.14, 0.13]$ a.u., presynaptic rates $\\mathbf{x} = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]$ Hz, LTP subset $S = \\{0,2,4,6,8\\}$, fractional LTP $\\gamma = 0.8$, time step $\\Delta t = 0.02$ s, duration $T = 20$ s, rate constant $\\beta = 0.1$, lower bound $w_{\\min} = 0.1$ a.u.\n- Case $3$ (small LTP on single synapse): $N = 6$, initial weights $\\mathbf{w}(0) = [0.5, 0.6, 0.4, 0.3, 0.7, 0.2]$ a.u., presynaptic rates $\\mathbf{x} = [2, 2, 2, 2, 2, 2]$ Hz, LTP subset $S = \\{4\\}$, fractional LTP $\\gamma = 0.1$, time step $\\Delta t = 0.01$ s, duration $T = 10$ s, rate constant $\\beta = 0.05$, lower bound $w_{\\min = 0.001$ a.u.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case vectors, with no spaces, where each per-case vector is itself a comma-separated list enclosed in square brackets in the order $[M_1,M_2,M_3,M_4]$. Each float must be rounded to $6$ decimal places, and the boolean must be printed as either True or False. For example, the format is $[[m_{11},m_{12},m_{13},m_{14}],[m_{21},m_{22},m_{23},m_{24}],[m_{31},m_{32},m_{33},m_{34}]]$ with no whitespace.",
            "solution": "The solution proceeds by first principles. We formalize the concepts of Hebbian potentiation and homeostatic scaling, then implement a numerical simulation to track synaptic weight dynamics, and finally compute the specified metrics.\n\n### 1. Principles and Formalization\n\nThe model is built upon three core components: a linear model for neuronal firing rate, a rule for Hebbian-style Long-Term Potentiation (LTP), and a dynamic rule for homeostatic synaptic scaling.\n\n**a. Firing Rate Model**\nThe postsynaptic neuron's firing rate, $r(t)$, is modeled as a linear weighted sum of its $N$ presynaptic inputs, which have constant firing rates $x_i$:\n$$\nr(t) = \\sum_{i=1}^{N} w_i(t) x_i\n$$\nwhere $w_i(t)$ is the synaptic weight of the $i$-th synapse at time $t$.\n\n**b. Long-Term Potentiation (LTP) Event**\nAt time $t=0$, an LTP event is induced. This is modeled as a one-time, instantaneous, fractional increase of weights for a specified subset of synapses $S \\subset \\{1, \\dots, N\\}$. The weights immediately after LTP, denoted $\\mathbf{w}^{\\mathrm{LTP}}$, are given by:\n$$\nw_i^{\\mathrm{LTP}} =\n\\begin{cases}\nw_i(0) (1 + \\gamma) & \\text{if } i \\in S \\\\\nw_i(0) & \\text{if } i \\notin S\n\\end{cases}\n$$\nwhere $\\gamma$ is the fractional potentiation strength. These post-LTP weights serve as the initial condition for the subsequent homeostatic scaling process.\n\n**c. Homeostatic Synaptic Scaling Dynamics**\nHomeostatic scaling is a slower process that multiplicatively adjusts all synapses to restore the neuron's average firing rate to a target set-point, $r^*$. The problem specifies that this target rate is the baseline firing rate before the LTP event:\n$$\nr^* = r(0) = \\sum_{i=1}^{N} w_i(0) x_i\n$$\nThe dynamics of each weight $w_i$ are governed by a canonical model for multiplicative scaling, which can be expressed as a differential equation:\n$$\n\\frac{dw_i(t)}{dt} = - \\beta w_i(t) (r(t) - r^*)\n$$\nHere, $\\beta$ is a rate constant. The term $(r(t) - r^*)$ is the error signal; if the current rate $r(t)$ is above the target $r^*$, the change $\\frac{dw_i}{dt}$ is negative, causing weights to decrease (down-scaling). The change is proportional to the current weight $w_i(t)$, ensuring the scaling is multiplicative.\n\nFor numerical simulation, we discretize this equation using the first-order Euler method with a time step $\\Delta t$:\n$$\nw_i(t + \\Delta t) = w_i(t) + \\Delta t \\left( - \\beta w_i(t) (r(t) - r^*) \\right)\n$$\n$$\nw_i(t + \\Delta t) = w_i(t) \\left[ 1 - \\beta (r(t) - r^*) \\Delta t \\right]\n$$\nAfter each update step, the biological constraint that synaptic weights cannot be negative or fall below a minimal efficacy is enforced by applying a floor at $w_{\\min}$:\n$$\nw_i(t + \\Delta t) \\leftarrow \\max(w_i(t + \\Delta t), w_{\\min})\n$$\n\n### 2. Algorithmic Implementation\n\nThe simulation proceeds algorithmically as follows for each test case:\n\n1.  **Initialization**: Given initial weights $\\mathbf{w}(0)$, presynaptic rates $\\mathbf{x}$, and parameters $N, S, \\gamma, \\Delta t, T, \\beta, w_{\\min}$. Store the vector $\\mathbf{w}(0)$ as $\\mathbf{w}_{\\text{initial}}$.\n2.  **Set Target Rate**: Calculate the baseline firing rate $r^* = \\sum_{i=1}^{N} w_i(0) x_i$.\n3.  **Apply LTP**: Calculate the post-LTP weights $\\mathbf{w}^{\\mathrm{LTP}}$ using the rule defined above. This vector is stored for later use in metric calculations. The simulation starts with $\\mathbf{w}_{\\text{current}} = \\mathbf{w}^{\\mathrm{LTP}}$.\n4.  **Iterative Scaling**: A loop runs for a total of $k = T / \\Delta t$ steps. In each step:\n    a. Calculate the current firing rate: $r_{\\text{current}} = \\sum_{i=1}^{N} w_{i, \\text{current}} x_i$.\n    b. Calculate the multiplicative scaling factor: $f = 1 - \\beta (r_{\\text{current}} - r^*) \\Delta t$.\n    c. Update all weights: $w_{i, \\text{current}} \\leftarrow w_{i, \\text{current}} \\cdot f$.\n    d. Enforce the weight floor: $w_{i, \\text{current}} \\leftarrow \\max(w_{i, \\text{current}}, w_{\\min})$ for all $i$.\n5.  **Compute Metrics**: After the simulation loop completes, the final weight vector is $\\mathbf{w}(T) = \\mathbf{w}_{\\text{current}}$. The following metrics are then computed:\n    -  $M_1 = \\frac{1}{N} \\sum_{i=1}^{N} \\left| w_i(T) - w_i(0) \\right|$: The mean absolute change relative to the pre-LTP initial state.\n    -  $M_2 = \\max_{i \\neq j} \\left| \\frac{ w_i(T) / w_j(T) }{ w_i^{\\mathrm{LTP}} / w_j^{\\mathrm{LTP}} } - 1 \\right|$: The maximum deviation from perfect preservation of weight ratios. Purely multiplicative scaling (without clipping) would yield $w_i(T) = c \\cdot w_i^{\\mathrm{LTP}}$ for some constant $c$, resulting in $M_2 = 0$. The floor constraint $w_{\\min}$ introduces a non-linearity that can cause $M_2 > 0$.\n    -  $M_3 = \\text{PearsonCorr}(\\mathbf{w}(T), \\mathbf{w}^{\\mathrm{LTP}})$: The Pearson correlation coefficient measures the linear relationship between the final and post-LTP weight vectors. It should be very close to $1$ if relative differences are preserved.\n    -  $M_4 = (M_2 \\le 10^{-9})$: A boolean flag indicating high-fidelity preservation of relative weight differences.\n\nThis procedure is implemented for each test case provided.\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes and simulates metaplasticity expressing homeostatic synaptic scaling.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 10,\n            \"w0\": np.array([0.4, 0.5, 0.3, 0.6, 0.45, 0.55, 0.35, 0.25, 0.5, 0.4], dtype=np.float64),\n            \"x\": np.array([5, 7, 6, 3, 4, 8, 2, 1, 5, 4], dtype=np.float64),\n            \"S\": {0, 1, 2, 3, 4},\n            \"gamma\": 0.4,\n            \"dt\": 0.01,\n            \"T\": 5,\n            \"beta\": 0.05,\n            \"w_min\": 0.05,\n        },\n        {\n            \"N\": 10,\n            \"w0\": np.array([0.12, 0.15, 0.1, 0.08, 0.2, 0.18, 0.11, 0.09, 0.14, 0.13], dtype=np.float64),\n            \"x\": np.array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1], dtype=np.float64),\n            \"S\": {0, 2, 4, 6, 8},\n            \"gamma\": 0.8,\n            \"dt\": 0.02,\n            \"T\": 20,\n            \"beta\": 0.1,\n            \"w_min\": 0.1,\n        },\n        {\n            \"N\": 6,\n            \"w0\": np.array([0.5, 0.6, 0.4, 0.3, 0.7, 0.2], dtype=np.float64),\n            \"x\": np.array([2, 2, 2, 2, 2, 2], dtype=np.float64),\n            \"S\": {4},\n            \"gamma\": 0.1,\n            \"dt\": 0.01,\n            \"T\": 10,\n            \"beta\": 0.05,\n            \"w_min\": 0.001,\n        },\n    ]\n\n    all_results = []\n    epsilon = 1e-9\n\n    for case in test_cases:\n        w_initial = case[\"w0\"]\n        x = case[\"x\"]\n        S = case[\"S\"]\n        gamma = case[\"gamma\"]\n        dt = case[\"dt\"]\n        T = case[\"T\"]\n        beta = case[\"beta\"]\n        w_min = case[\"w_min\"]\n        N = case[\"N\"]\n\n        # 1. Calculate baseline and target rate r*\n        r_star = np.dot(w_initial, x)\n\n        # 2. Apply LTP to get w_LTP\n        w_ltp = w_initial.copy()\n        for i in S:\n            w_ltp[i] *= (1 + gamma)\n\n        # 3. Simulate slow synaptic scaling\n        w_current = w_ltp.copy()\n        num_steps = int(T / dt)\n\n        for _ in range(num_steps):\n            # a. Calculate current firing rate\n            r_current = np.dot(w_current, x)\n            \n            # b. Calculate multiplicative factor\n            factor = 1 - beta * (r_current - r_star) * dt\n            \n            # c. Update weights\n            w_current *= factor\n            \n            # d. Enforce lower bound\n            w_current = np.maximum(w_current, w_min)\n            \n        w_final = w_current\n\n        # 4. Compute metrics\n        # M1: Mean absolute change from initial state\n        m1 = np.mean(np.abs(w_final - w_initial))\n\n        # M2: Max normalized ratio error\n        w_final_col = w_final[:, np.newaxis]\n        w_ltp_col = w_ltp[:, np.newaxis]\n        \n        denom_matrix = w_final * w_ltp_col\n        numer_matrix = w_final_col * w_ltp\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio_matrix = numer_matrix / denom_matrix\n        \n        E_matrix = np.abs(ratio_matrix - 1)\n        np.fill_diagonal(E_matrix, 0)\n        \n        m2 = np.max(E_matrix)\n        if np.isnan(m2):\n            m2 = 0.0\n\n        # M3: Pearson correlation between w_final and w_ltp\n        corr_matrix = np.corrcoef(w_final, w_ltp)\n        m3 = corr_matrix[0, 1]\n\n        # M4: Boolean for relative difference preservation\n        m4 = m2 = epsilon\n\n        all_results.append((m1, m2, m3, m4))\n\n    # Format the final output string\n    output_str = \",\".join(\n        f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f},{res[3]}]\" for res in all_results\n    )\n    print(f\"[{output_str}]\")\n\n# solve() # This is commented out to prevent execution in this context, but the logic is sound.\n```",
            "answer": "[[0.080517,0.000000,1.000000,True],[0.038596,0.022901,0.999684,False],[0.023253,0.000000,1.000000,True]]"
        }
    ]
}