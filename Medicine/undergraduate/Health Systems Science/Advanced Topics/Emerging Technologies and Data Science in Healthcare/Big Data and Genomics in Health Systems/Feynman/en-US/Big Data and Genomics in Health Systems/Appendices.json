{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in any large-scale genomic study is determining the required sample size. This practice explores the statistical principles behind study design, particularly when searching for rare pathogenic variants. By working through this problem, you will derive the relationship between an allele's frequency in a population, the desired probability of detecting it, and the number of individuals you need to screen, which is a foundational skill for planning cost-effective and powerful genomic research. ",
            "id": "4361957",
            "problem": "A large integrated health system is planning a genomic screening program using Next-Generation Sequencing (NGS) across $N$ unrelated, diploid individuals to detect at least one copy of a rare pathogenic allele with population allele frequency $p$, where $0<p<1$. Assume the following foundational conditions: the population is large and randomly mating, so that chromosomes sampled across individuals can be treated as independent draws; each individual contributes two alleles, for a total of $2N$ allele draws; and the sequencing assay has perfect analytic sensitivity and specificity with uniform coverage, so detection is equivalent to the allele being present in at least one of the $2N$ draws.\n\nStarting from the basic rules of probability for independent Bernoulli trials and the relationship between the probability of at least one success and the probability of zero successes, derive the smallest integer sample size $N$ such that the probability of observing at least one copy of the allele is at least $0.95$. Express your final result as a single closed-form analytic expression in terms of $p$ only, using natural logarithms. Also, briefly explain what this implies about the residual risk of missing the variant in a finite sample when the allele is rare.\n\nYour final answer must be a single closed-form analytic expression. Do not include any units. Do not provide a numerical approximation.",
            "solution": "The problem requires the derivation of the smallest integer sample size, $N$, needed to detect at least one copy of a pathogenic allele with a given probability. The problem is well-posed and scientifically grounded in the principles of probability and population genetics.\n\nLet $p$ be the population frequency of the pathogenic allele. The problem states that we are screening $N$ unrelated, diploid individuals. Each individual carries two alleles at the locus of interest. Assuming random mating and a large population, the alleles can be treated as independent draws from the population. Therefore, the total number of alleles sampled is $2N$.\n\nThe process of observing alleles can be modeled as a series of $2N$ independent Bernoulli trials. For each trial (i.e., for each allele sampled), a \"success\" is defined as observing the pathogenic allele. The probability of success in a single trial is $p$. The probability of \"failure\" (observing the non-pathogenic, or wild-type, allele) is $1-p$.\n\nLet $X$ be the random variable representing the number of pathogenic alleles observed in the sample of $2N$ alleles. We are interested in the event that at least one copy is observed, which corresponds to $X \\ge 1$. The problem requires that the probability of this event is at least $0.95$:\n$$P(X \\ge 1) \\ge 0.95$$\n\nIt is more straightforward to calculate the probability of the complementary event, which is observing zero copies of the pathogenic allele ($X=0$). The relationship between these probabilities is:\n$$P(X \\ge 1) = 1 - P(X=0)$$\n\nSubstituting this into the inequality gives:\n$$1 - P(X=0) \\ge 0.95$$\n$$P(X=0) \\le 1 - 0.95$$\n$$P(X=0) \\le 0.05$$\n\nThe event $X=0$ means that all $2N$ independent Bernoulli trials result in a failure. The probability of a single failure is $1-p$. Since the trials are independent, the probability of $2N$ consecutive failures is the product of their individual probabilities:\n$$P(X=0) = (1-p)^{2N}$$\n\nNow, we substitute this expression back into the inequality:\n$$(1-p)^{2N} \\le 0.05$$\n\nTo solve for $N$, we take the natural logarithm of both sides. The natural logarithm function, $\\ln(x)$, is a strictly increasing function, so it preserves the direction of the inequality.\n$$\\ln\\left((1-p)^{2N}\\right) \\le \\ln(0.05)$$\n\nUsing the logarithm power rule, $\\ln(a^b) = b \\ln(a)$, we get:\n$$2N \\ln(1-p) \\le \\ln(0.05)$$\n\nTo isolate $N$, we must divide by $2 \\ln(1-p)$. It is critical to consider the sign of $\\ln(1-p)$. Since the allele frequency $p$ is given to be in the range $0 < p < 1$, the term $1-p$ is in the range $0 < 1-p < 1$. The natural logarithm of any number between $0$ and $1$ is negative. Therefore, $\\ln(1-p) < 0$. When dividing both sides of an inequality by a negative number, the direction of the inequality must be reversed:\n$$N \\ge \\frac{\\ln(0.05)}{2 \\ln(1-p)}$$\n\nThis inequality gives the condition that the sample size $N$ must satisfy. The problem asks for the *smallest integer* sample size $N$ that fulfills this condition. This is obtained by taking the ceiling of the right-hand side of the inequality. The ceiling function, $\\lceil x \\rceil$, gives the smallest integer greater than or equal to $x$.\nThus, the smallest integer sample size $N$ is:\n$$N = \\left\\lceil \\frac{\\ln(0.05)}{2 \\ln(1-p)} \\right\\rceil$$\n\nRegarding the second part of the question, this result has a significant implication for the residual risk of missing a rare variant. The residual risk is the probability of not detecting the allele, which is $P(X=0) = (1-p)^{2N}$. For a rare allele, $p$ is very small ($p \\ll 1$). In this regime, we can use the Taylor series approximation for the natural logarithm, $\\ln(1-x) \\approx -x$ for small $x$. Applying this, $\\ln(1-p) \\approx -p$. The condition on $N$ becomes approximately:\n$$N \\ge \\frac{\\ln(0.05)}{2(-p)} = \\frac{-\\ln(0.05)}{2p}$$\nSince $\\ln(0.05) \\approx -2.996$, this simplifies to $N \\ge \\frac{1.498}{p}$. This shows that the required sample size $N$ is inversely proportional to the allele frequency $p$. As an allele becomes rarer (as $p \\to 0$), the sample size required to detect it with a fixed level of confidence grows towards infinity. This means that for any finite sample size, there is always a non-zero, and potentially substantial, residual risk of failing to detect an extremely rare variant. This is a fundamental limitation of sampling-based screening strategies in genomics.",
            "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(0.05)}{2 \\ln(1-p)} \\right\\rceil}$$"
        },
        {
            "introduction": "Once genomic data is generated, assessing its quality is paramount. This exercise introduces the Phred quality score ($Q$), the universal standard for quantifying the probability of error in sequenced DNA bases. You will learn to connect these scores to the expected number of incorrect base calls in a whole-genome sequencing run, a critical calculation for any health system analyst planning downstream validation and clinical reporting workflows. ",
            "id": "4361966",
            "problem": "A national precision health initiative is using Next-Generation Sequencing (NGS) to generate whole-genome data for clinical decision support in a population cohort. The genome size under analysis is $3 \\times 10^{9}$ bases per individual. Health systems analysts need to estimate the expected burden of erroneous base calls to plan downstream validation and quality control workflows. The Phred quality score definition is $Q = -10 \\log_{10}(p_{\\text{err}})$, where $p_{\\text{err}}$ is the probability that a base call is incorrect. In a typical sequencing run, $0.85$ of all base calls have quality $Q=30$ and the remaining $0.15$ have quality $Q=20$. Starting from the core definition of expectation for Bernoulli trials and the Phred score definition above, derive a formula that connects the distribution of Phred scores to the expected number of errors across the $3 \\times 10^{9}$ bases, and compute the expected number of erroneous base calls for this run. Express your final answer as a count in scientific notation rounded to three significant figures.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All data required for a solution are provided, the definitions (e.g., Phred score) are standard in the field of genomics, and the premises are consistent and physically realistic. The problem asks for a derivation and a calculation, which is a standard format for a quantitative scientific question. Therefore, I will proceed with a full solution.\n\nThe primary goal is to find the expected number of erroneous base calls, denoted as $E[X]$, in a genome sequence of total size $N$. Let us model the correctness of each base call as an independent Bernoulli trial. For each base $i$ from $1$ to $N$, we can define a random variable $X_i$ such that:\n$X_i = 1$ if the base call is an error (with probability $p_{\\text{err},i}$).\n$X_i = 0$ if the base call is correct (with probability $1 - p_{\\text{err},i}$).\n\nThe expected value of this random variable is $E[X_i] = 1 \\cdot p_{\\text{err},i} + 0 \\cdot (1 - p_{\\text{err},i}) = p_{\\text{err},i}$.\n\nThe total number of errors, $X$, across the entire sequence of $N$ bases is the sum of these individual random variables:\n$$X = \\sum_{i=1}^{N} X_i$$\n\nBy the linearity of expectation, the expected total number of errors is the sum of the individual expectations:\n$$E[X] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i] = \\sum_{i=1}^{N} p_{\\text{err},i}$$\n\nThe problem states that the base calls are partitioned into groups based on their Phred quality score, $Q$. Let there be $k$ such groups, where each group $k$ is characterized by a quality score $Q_k$ and comprises a fraction $f_k$ of the total bases. Within each group, all bases have the same probability of error, $p_{\\text{err},k}$. The total number of bases in group $k$ is $N_k = f_k N$.\n\nThe total sum of error probabilities can be restructured as a sum over these groups:\n$$E[X] = \\sum_{k} \\sum_{i \\in \\text{group } k} p_{\\text{err},i} = \\sum_{k} (N_k \\cdot p_{\\text{err},k})$$\nSubstituting $N_k = f_k N$:\n$$E[X] = \\sum_{k} (f_k N \\cdot p_{\\text{err},k}) = N \\sum_{k} (f_k \\cdot p_{\\text{err},k})$$\n\nThe connection between the Phred score $Q_k$ and the error probability $p_{\\text{err},k}$ is given by the definition:\n$$Q_k = -10 \\log_{10}(p_{\\text{err},k})$$\nTo find $p_{\\text{err},k}$ as a function of $Q_k$, we rearrange this equation:\n$$-\\frac{Q_k}{10} = \\log_{10}(p_{\\text{err},k})$$\n$$p_{\\text{err},k} = 10^{-Q_k/10}$$\n\nSubstituting this expression for $p_{\\text{err},k}$ into our equation for $E[X]$ gives the general formula that connects the distribution of Phred scores to the expected number of errors:\n$$E[X] = N \\sum_{k} f_k 10^{-Q_k/10}$$\n\nNow, we apply this derived formula to the specific data provided in the problem. The total number of bases is $N = 3 \\times 10^9$. The bases are partitioned into two groups:\nGroup 1: fraction $f_1 = 0.85$ with quality score $Q_1 = 30$.\nGroup 2: fraction $f_2 = 0.15$ with quality score $Q_2 = 20$.\n\nFirst, we calculate the error probabilities for each group:\nFor Group 1:\n$$p_{\\text{err},1} = 10^{-Q_1/10} = 10^{-30/10} = 10^{-3} = 0.001$$\nFor Group 2:\n$$p_{\\text{err},2} = 10^{-Q_2/10} = 10^{-20/10} = 10^{-2} = 0.01$$\n\nNow we use the formula for the expected number of errors with $k=2$:\n$$E[X] = N (f_1 p_{\\text{err},1} + f_2 p_{\\text{err},2})$$\nSubstituting the numerical values:\n$$E[X] = (3 \\times 10^9) \\left( (0.85)(10^{-3}) + (0.15)(10^{-2}) \\right)$$\n$$E[X] = (3 \\times 10^9) \\left( 0.00085 + 0.0015 \\right)$$\n$$E[X] = (3 \\times 10^9) \\left( 0.00235 \\right)$$\nTo perform the final multiplication, we express $0.00235$ in scientific notation as $2.35 \\times 10^{-3}$:\n$$E[X] = (3 \\times 10^9) \\times (2.35 \\times 10^{-3})$$\n$$E[X] = (3 \\times 2.35) \\times 10^{(9-3)}$$\n$$E[X] = 7.05 \\times 10^6$$\n\nThe expected number of erroneous base calls for this run is $7.05 \\times 10^6$. The result is already expressed in scientific notation with three significant figures as requested.",
            "answer": "$$\\boxed{7.05 \\times 10^6}$$"
        },
        {
            "introduction": "The ultimate goal of many genomic initiatives is to build predictive models for clinical outcomes. This presents a major challenge when the number of features (e.g., genetic variants) vastly outnumbers the patients ($p \\gg n$), creating a high risk of overfitting. This practice guides you through the design of a nested cross-validation scheme, the gold standard for obtaining an unbiased estimate of a model's performance by rigorously separating data used for feature selection, hyperparameter tuning, and final evaluation. ",
            "id": "4361973",
            "problem": "A large integrated health system links Electronic Health Record (EHR) data with genotyping arrays to predict the risk of a statin-associated adverse drug reaction within one year of initiation. You are given a cohort of $N=1{,}200$ adult patients, with a binary outcome prevalence of approximately $25\\%$. For each patient, there are $p=50{,}000$ Single Nucleotide Polymorphism (SNP) features coded as minor allele counts and $q=20$ curated clinical covariates. Genotyping occurred in $3$ batches. You plan to train a penalized logistic regression with an elastic-net penalty, tuning hyperparameters $\\alpha$ and $\\lambda$, and you will also perform feature selection that includes variance filtering and univariate association filtering with False Discovery Rate (FDR) control. Your goal is to estimate generalization performance while strictly preventing data leakage during both feature selection and hyperparameter tuning.\n\nUsing only fundamental principles about generalization error estimation, overfitting when $p \\gg n$, and the definition of data leakage (any use of information from the test data influencing model training or selection), choose the nested cross-validation (CV) scheme that correctly prevents leakage and provides a defensible justification for the inner and outer split sizes in this genomic setting.\n\nWhich option is best?\n\nA. Use outer $K=5$ stratified by outcome and genotyping batch, and inner $L=5$ on the outer-training portion. In each inner split, fit all preprocessing steps (standardization), variance filtering, and univariate FDR-based filtering using only the inner-training fold, then tune $\\alpha$ and $\\lambda$ via grid search within the inner loop. Select the best pipeline by inner CV and evaluate it once on the untouched outer test fold. Average performance across outer folds. Justification: With $N=1{,}200$, each outer test fold is about $N/K \\approx 240$ patients; each inner training fold has about $N \\cdot \\frac{K-1}{K} \\cdot \\frac{L-1}{L} \\approx 768$ patients, and each inner validation fold about $N \\cdot \\frac{K-1}{K} \\cdot \\frac{1}{L} \\approx 192$ patients. Given outcome prevalence $25\\%$, this yields adequate positive cases per fold and balances variance of the outer estimate against computational burden.\n\nB. First perform univariate association tests with outcome on the entire dataset to pre-filter SNPs to the top $1{,}000$ features, then run nested CV with outer $K=5$ and inner $L=5$ to tune $\\alpha$ and $\\lambda$ only. Justification: Pre-filtering once on all data reduces dimensionality and stabilizes inner CV, and the outer CV then provides an unbiased performance estimate.\n\nC. Use outer $K=10$ and inner $L=10$ to maximize stability. Standardize all features once using the global mean and variance computed across the full dataset before any splitting, then run inner CV to tune $\\alpha$ and $\\lambda$ and evaluate on the outer folds. Justification: Larger $K$ and $L$ reduce variance of estimates, and global standardization avoids scale drift across folds.\n\nD. Perform a single $80/20$ split into training and test. Within the $80\\%$ training set, use $L=5$ inner CV for feature selection and hyperparameter tuning. If inner performance saturates, tweak thresholds and retune using the $20\\%$ test set as a sanity check for early stopping. Justification: A larger training set ($80\\%$) increases power for feature selection in $p \\gg n$, and the $20\\%$ set ensures external validity while allowing pragmatic early stopping to avoid overfitting.\n\nOnly one option is fully correct under the constraints above. Select it.",
            "solution": "The problem requires an evaluation of different validation strategies for a high-dimensional ($p \\gg n$) classification task, with the explicit goal of obtaining a reliable estimate of generalization performance while strictly preventing data leakage.\n\n### Foundational Principles\n\n1.  **Generalization Error**: The primary objective is to estimate the performance of a model-building *procedure* on new, unseen data. This procedure includes all steps that involve learning from data: feature scaling, feature selection, and model parameter/hyperparameter estimation.\n2.  **Data Leakage**: This critical error occurs when information from the test set (data reserved for final performance evaluation) is used, corrected or not, during the training or selection of the model. This leads to an optimistically biased performance estimate, as the model has been inadvertently tuned to the test data. To prevent leakage, the test set must be used only once, for the final evaluation of a model that was built without any knowledge of it.\n3.  **High-Dimensional Data ($p \\gg n$)**: The given problem has $p=50,020$ predictors ($50,000$ SNPs + $20$ clinical) and $N=1,200$ samples. In such a scenario, the risk of overfitting is extremely high. A model can easily find spurious correlations in the training data that do not generalize. Therefore, rigorous validation is paramount.\n4.  **Nested Cross-Validation (CV)**: This is the standard methodology for estimating generalization error when the modeling pipeline itself involves hyperparameter tuning or any other data-driven model selection choices (e.g., feature selection).\n    *   The **outer loop** partitions the data into $K$ folds to generate $K$ independent test sets. Its purpose is to produce an averaged, stable estimate of the generalization error.\n    *   The **inner loop** operates *only* on the training data from the outer loop. Its purpose is to select the optimal hyperparameters (here, $\\alpha$ and $\\lambda$) for the model.\n    *   Crucially, the entire pipeline, including any feature preprocessing or selection that depends on the data (e.g., fitting scalers, running univariate tests for filtering), must be re-executed for each fold of the outer loop, using only that fold's training data.\n\n### Option-by-Option Analysis\n\n**A. Use outer $K=5$ stratified by outcome and genotyping batch, and inner $L=5$ on the outer-training portion. In each inner split, fit all preprocessing steps (standardization), variance filtering, and univariate FDR-based filtering using only the inner-training fold, then tune $\\alpha$ and $\\lambda$ via grid search within the inner loop. Select the best pipeline by inner CV and evaluate it once on the untouched outer test fold. Average performance across outer folds. Justification: With $N=1{,}200$, each outer test fold is about $N/K \\approx 240$ patients; each inner training fold has about $N \\cdot \\frac{K-1}{K} \\cdot \\frac{L-1}{L} \\approx 768$ patients, and each inner validation fold about $N \\cdot \\frac{K-1}{K} \\cdot \\frac{1}{L} \\approx 192$ patients. Given outcome prevalence $25\\%$, this yields adequate positive cases per fold and balances variance of the outer estimate against computational burden.**\n\nThis option describes a correct and rigorous nested cross-validation procedure.\n1.  **Leakage Prevention**: It correctly specifies that all data-dependent steps (preprocessing, filtering, hyperparameter tuning) are enclosed within the validation structure. The \"untouched outer test fold\" is only used for final evaluation in each outer loop iteration. This structure strictly prevents data leakage.\n2.  **Stratification**: Stratifying by outcome and genotyping batch are best practices. Stratifying by the outcome prevalence of $25\\%$ ensures that each fold has a representative number of positive and negative cases, which is crucial for stable model training and evaluation. Stratifying by batch helps ensure the model is robust to technical artifacts from the genotyping process.\n3.  **Justification**: The provided justification is sound. It correctly calculates the sample sizes for the various folds:\n    *   Outer train set size: $N \\cdot (K-1)/K = 1,200 \\cdot 4/5 = 960$.\n    *   Outer test set size: $N/K = 1,200/5 = 240$.\n    *   Inner train set size: $960 \\cdot (L-1)/L = 960 \\cdot 4/5 = 768$.\n    *   Inner validation set size: $960/L = 960/5 = 192$.\n    With a $25\\%$ prevalence, the number of positive cases in the smallest training partition (inner training set) is $768 \\cdot 0.25 = 192$, and in the smallest validation partition (inner validation set) is $192 \\cdot 0.25 = 48$. These are adequate sample sizes to avoid severe instability in model fitting and evaluation. The choice of $K=5$ and $L=5$ represents a standard and reasonable balance between computational cost and the stability of the performance estimate.\n\n**Verdict: Correct.**\n\n**B. First perform univariate association tests with outcome on the entire dataset to pre-filter SNPs to the top $1{,}000$ features, then run nested CV with outer $K=5$ and inner $L=5$ to tune $\\alpha$ and $\\lambda$ only. Justification: Pre-filtering once on all data reduces dimensionality and stabilizes inner CV, and the outer CV then provides an unbiased performance estimate.**\n\nThis option describes a common but fundamentally flawed procedure. The initial feature filtering step uses the entire dataset, including the outcome labels. This means that information from the samples that will later be used as test data has \"leaked\" into the feature selection process. The features are selected because they show an association across the full dataset. Consequently, the cross-validated performance will be optimistically biased, as the model is evaluated on data that has already been used to make a key modeling decision. The justification's claim that this provides an \"unbiased performance estimate\" is false. This is a canonical example of data leakage.\n\n**Verdict: Incorrect.**\n\n**C. Use outer $K=10$ and inner $L=10$ to maximize stability. Standardize all features once using the global mean and variance computed across the full dataset before any splitting, then run inner CV to tune $\\alpha$ and $\\lambda$ and evaluate on the outer folds. Justification: Larger $K$ and $L$ reduce variance of estimates, and global standardization avoids scale drift across folds.**\n\nThis procedure also commits data leakage, albeit in a more subtle way than option B. By standardizing all features using the mean and variance calculated from the *full dataset*, information about the distribution of the test data (its mean and variance) is incorporated into the training data before the model is built. A rigorously correct procedure must learn the standardization parameters (mean and variance) *only from the training portion* of each CV fold and then apply those same parameters to scale the test portion of that fold. While this form of leakage is sometimes less severe than using outcome labels, it is still a violation of the principle of strict separation and can lead to biased performance estimates. The justification about avoiding \"scale drift\" is a misunderstanding of the goal; the model's performance on new data (which may have such drift) is precisely what we want to estimate.\n\n**Verdict: Incorrect.**\n\n**D. Perform a single $80/20$ split into training and test. Within the $80\\%$ training set, use $L=5$ inner CV for feature selection and hyperparameter tuning. If inner performance saturates, tweak thresholds and retune using the $20\\%$ test set as a sanity check for early stopping. Justification: A larger training set ($80\\%$) increases power for feature selection in $p \\gg n$, and the $20\\%$ set ensures external validity while allowing pragmatic early stopping to avoid overfitting.**\n\nThis option explicitly recommends misusing the test set. A test set is for a single, final evaluation of the frozen, finalized model. Using the test set to \"tweak thresholds and retune\" or as a \"sanity check for early stopping\" means the test set is no longer an independent holdout set. It is being used to guide the modeling process, thereby becoming part of the training/validation loop. Any performance metric reported on this $20\\%$ set will be optimistically biased. This is a form of manual data leakage. Furthermore, a single train/test split provides a performance estimate with higher variance (i.e., less reliable) than a $K$-fold cross-validation approach.\n\n**Verdict: Incorrect.**\n\n### Conclusion\n\nOnly option A describes a methodologically sound procedure that correctly implements nested cross-validation to prevent data leakage in a complex, high-dimensional modeling pipeline. The other three options all contain critical flaws that lead to biased estimation of generalization performance.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}