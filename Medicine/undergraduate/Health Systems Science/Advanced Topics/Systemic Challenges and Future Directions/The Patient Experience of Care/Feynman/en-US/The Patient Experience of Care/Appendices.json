{
    "hands_on_practices": [
        {
            "introduction": "Analyzing the patient experience often begins with understanding patient narratives from interviews or open-ended survey questions. To ensure this qualitative data is interpreted consistently, researchers must establish inter-rater reliability. This practice  introduces you to Cohen’s kappa ($ \\kappa $), a fundamental statistical tool for measuring agreement between two coders that accounts for agreement occurring by chance. Mastering this method is a crucial first step in demonstrating the rigor and trustworthiness of any qualitative analysis of patient feedback.",
            "id": "4400341",
            "problem": "A research team in health systems science is studying the patient experience of care by analyzing qualitative interview data. Two independent coders analyze the same set of $20$ patient interview transcripts and apply a binary code indicating the presence ($1$) or absence ($0$) of a specific theme (for example, perceived respect from clinicians). To assess inter-rater reliability (IRR), the team plans to use Cohen’s kappa, a chance-corrected agreement statistic for two raters with categorical data. You are asked to select the best description of how to compute Cohen’s kappa for this binary code from these $20$ transcripts and to interpret a resulting value of $\\kappa = 0.72$ in terms of agreement quality for this coding task.\n\nWhich option most accurately specifies a correct computational procedure starting from first principles of observed versus chance agreement using the raters’ marginal proportions, and provides a scientifically sound interpretation of $\\kappa = 0.72$ in this context?\n\nA. Construct a $2 \\times 2$ table of joint decisions with counts $n_{11}, n_{10}, n_{01}, n_{00}$, where $n_{11}$ is the number of transcripts both coders marked as present and $n_{00}$ is the number both marked as absent. Compute observed agreement $p_o = (n_{11} + n_{00})/n$, expected agreement by chance $p_e = \\left(\\frac{n_{1+}}{n}\\right)\\left(\\frac{n_{+1}}{n}\\right) + \\left(\\frac{n_{0+}}{n}\\right)\\left(\\frac{n_{+0}}{n}\\right)$ using each rater’s marginal proportions, and then compute $\\kappa = \\dfrac{p_o - p_e}{1 - p_e}$. Interpret $\\kappa = 0.72$ as substantial beyond-chance agreement for this binary code on $20$ transcripts, indicating reliable coding, while noting that extreme prevalence or bias could affect $\\kappa$.\n\nB. Calculate simple percent agreement $p_o$ only by summing matches and dividing by $20$; there is no need to account for chance because two trained coders are unlikely to guess. Interpret $0.72$ agreement as poor because it is less than $0.80$, which is the minimum acceptable threshold.\n\nC. Assume each category (present, absent) has equal probability $0.5$ for each coder, so the expected agreement by chance is $0.5^2 + 0.5^2 = 0.5$. Then compute $\\kappa$ from $p_o$ and $p_e = 0.5$. Interpret $\\kappa = 0.72$ as almost perfect agreement.\n\nD. Compute Cohen’s kappa by calculating the Pearson correlation coefficient between the two binary code vectors. Interpret a correlation of $0.72$ as strong agreement because it is close to $1$.\n\nE. Compute expected agreement by chance as $p_e = \\left(\\dfrac{n_{1+} + n_{+1}}{2n}\\right)^2 + \\left(1 - \\dfrac{n_{1+} + n_{+1}}{2n}\\right)^2$ using the average prevalence across raters, then compute $\\kappa = \\dfrac{p_o - p_e}{1 - p_e}$. Interpret $\\kappa = 0.72$ as only moderate agreement because the sample size is $20$.",
            "solution": "The problem statement is first validated for scientific soundness, clarity, and completeness.\n\n### Step 1: Extract Givens\n-   **Field of Study**: Health systems science, specifically the patient experience of care.\n-   **Data Source**: Qualitative interview data from $n=20$ patient interview transcripts.\n-   **Raters**: Two independent coders.\n-   **Coding Scheme**: A binary code indicating the presence ($1$) or absence ($0$) of a specific theme.\n-   **Statistical Measure**: Cohen’s kappa ($\\kappa$) to assess inter-rater reliability (IRR).\n-   **Value for Interpretation**: A resulting value of $\\kappa = 0.72$.\n-   **Task**: Select the option that (1) correctly specifies the computation of Cohen's kappa from first principles using the raters' marginal proportions, and (2) provides a scientifically sound interpretation of $\\kappa = 0.72$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against standard validation criteria.\n-   **Scientifically Grounded**: The problem describes a standard and appropriate application of a well-established statistical method, Cohen's kappa, for assessing inter-rater reliability in the context of qualitative data analysis. This is a common practice in social sciences, health sciences, and related fields. The premises are factually sound.\n-   **Well-Posed**: The problem is clearly structured. It asks for the correct computational procedure and a standard interpretation of a specific statistical value. This allows for a unique, correct answer to be identified among the options.\n-   **Objective**: The language is precise and unbiased. The scenario presented is a typical research situation, free of subjective claims.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is scientifically sound, well-posed, and objective. It provides a sufficient basis for a rigorous statistical analysis. The solution process will now proceed.\n\n### Derivation of Cohen's Kappa and Interpretation\n\nCohen's kappa ($\\kappa$) is a statistic that measures inter-rater agreement for categorical items, while correcting for agreement that would be expected by chance. The general formula is:\n$$ \\kappa = \\frac{p_o - p_e}{1 - p_e} $$\nwhere $p_o$ is the observed proportional agreement, and $p_e$ is the hypothetical probability of chance agreement.\n\nFor a binary coding task (e.g., presence/$1$ vs. absence/$0$) with two raters, the data can be organized in a $2 \\times 2$ contingency table with counts $n_{ij}$, where $i$ is the rating of Coder 1 and $j$ is the rating of Coder 2. Let $n$ be the total number of items coded, which is $20$ in this case.\n\n|           | Coder 2: Present (1) | Coder 2: Absent (0) | Row Total |\n| :-------- | :------------------: | :-----------------: | :-------: |\n| Coder 1: Present (1) |       $n_{11}$       |       $n_{10}$        |  $n_{1+}$   |\n| Coder 1: Absent (0)  |       $n_{01}$       |       $n_{00}$        |  $n_{0+}$   |\n| Column Total |       $n_{+1}$       |       $n_{+0}$        |    $n$    |\n\n1.  **Observed Agreement ($p_o$)**: This is the proportion of items on which the two coders agree. Agreement occurs when both code 'present' ($n_{11}$) or both code 'absent' ($n_{00}$).\n    $$ p_o = \\frac{\\text{Number of agreements}}{\\text{Total items}} = \\frac{n_{11} + n_{00}}{n} $$\n\n2.  **Expected Agreement by Chance ($p_e$)**: This is the probability of the coders agreeing by chance, calculated based on their individual (marginal) frequencies of using each code.\n    -   Proportion of 'present' codes by Coder 1: $p_{1+} = n_{1+}/n$\n    -   Proportion of 'absent' codes by Coder 1: $p_{0+} = n_{0+}/n$\n    -   Proportion of 'present' codes by Coder 2: $p_{+1} = n_{+1}/n$\n    -   Proportion of 'absent' codes by Coder 2: $p_{+0} = n_{+0}/n$\n\n    The chance probability of both coding 'present' is the product of their individual probabilities: $p_{1+} \\times p_{+1}$.\n    The chance probability of both coding 'absent' is the product of their individual probabilities: $p_{0+} \\times p_{+0}$.\n    The total expected agreement by chance is the sum of these possibilities:\n    $$ p_e = (p_{1+} \\times p_{+1}) + (p_{0+} \\times p_{+0}) = \\left(\\frac{n_{1+}}{n}\\right)\\left(\\frac{n_{+1}}{n}\\right) + \\left(\\frac{n_{0+}}{n}\\right)\\left(\\frac{n_{+0}}{n}\\right) $$\n\n3.  **Interpretation of $\\kappa = 0.72$**: The interpretation of kappa values is subject to convention, but widely cited benchmarks (e.g., Landis & Koch, 1977) provide a useful guide:\n    -   $< 0.00$: Poor\n    -   $0.00 - 0.20$: Slight\n    -   $0.21 - 0.40$: Fair\n    -   $0.41 - 0.60$: Moderate\n    -   $0.61 - 0.80$: Substantial\n    -   $0.81 - 1.00$: Almost perfect\n\n    Thus, a kappa value of $\\kappa = 0.72$ is typically interpreted as \"substantial\" agreement. It is also important to consider the limitations of kappa, namely its sensitivity to the prevalence of the codes (the \"prevalence paradox\") and the degree of asymmetry in the raters' marginal totals (the \"bias paradox\"). Acknowledging these factors indicates a more complete understanding.\n\n### Evaluation of Options\n\n**A. Construct a $2 \\times 2$ table of joint decisions with counts $n_{11}, n_{10}, n_{01}, n_{00}$, where $n_{11}$ is the number of transcripts both coders marked as present and $n_{00}$ is the number both marked as absent. Compute observed agreement $p_o = (n_{11} + n_{00})/n$, expected agreement by chance $p_e = \\left(\\frac{n_{1+}}{n}\\right)\\left(\\frac{n_{+1}}{n}\\right) + \\left(\\frac{n_{0+}}{n}\\right)\\left(\\frac{n_{+0}}{n}\\right)$ using each rater’s marginal proportions, and then compute $\\kappa = \\dfrac{p_o - p_e}{1 - p_e}$. Interpret $\\kappa = 0.72$ as substantial beyond-chance agreement for this binary code on $20$ transcripts, indicating reliable coding, while noting that extreme prevalence or bias could affect $\\kappa$.**\n-   **Computation**: The computational procedure described is precisely correct. It correctly defines the contingency table, the formula for observed agreement $p_o$, and the formula for chance-expected agreement $p_e$ based on the product of the marginal proportions for each rater. The final formula for $\\kappa$ is correct.\n-   **Interpretation**: The interpretation of $\\kappa=0.72$ as \"substantial beyond-chance agreement\" is consistent with standard guidelines. The mention of potential effects from extreme prevalence or bias is a critical and sophisticated point, demonstrating a thorough understanding of the statistic's properties.\n-   **Verdict**: **Correct**.\n\n**B. Calculate simple percent agreement $p_o$ only by summing matches and dividing by $20$; there is no need to account for chance because two trained coders are unlikely to guess. Interpret $0.72$ agreement as poor because it is less than $0.80$, which is the minimum acceptable threshold.**\n-   **Computation**: This is fundamentally incorrect. The entire purpose of Cohen's kappa is to improve upon simple percent agreement by accounting for chance. Ignoring the chance correction ($p_e$) is a failure to calculate kappa. The rationale that trained coders do not guess misunderstands what \"chance\" means in this context; it refers to the base rates of code usage, not random guessing.\n-   **Interpretation**: Interpreting a value of $0.72$ as \"poor\" is an unusually harsh judgment and not standard. Moreover, the number given ($0.72$) is the value of $\\kappa$, not $p_o$, which the option suggests calculating. The option is internally inconsistent.\n-   **Verdict**: **Incorrect**.\n\n**C. Assume each category (present, absent) has equal probability $0.5$ for each coder, so the expected agreement by chance is $0.5^2 + 0.5^2 = 0.5$. Then compute $\\kappa$ from $p_o$ and $p_e = 0.5$. Interpret $\\kappa = 0.72$ as almost perfect agreement.**\n-   **Computation**: This is incorrect. The chance agreement $p_e$ in Cohen's kappa must be calculated from the *observed marginal probabilities* derived from the raters' actual data. Assuming a priori probabilities of $0.5$ is not how Cohen's kappa is computed unless, by sheer coincidence, both raters happened to use both codes exactly $50\\%$ of the time.\n-   **Interpretation**: Interpreting $\\kappa = 0.72$ as \"almost perfect agreement\" is an overstatement. Standard scales place this in the \"substantial\" range.\n-   **Verdict**: **Incorrect**.\n\n**D. Compute Cohen’s kappa by calculating the Pearson correlation coefficient between the two binary code vectors. Interpret a correlation of $0.72$ as strong agreement because it is close to $1$.**\n-   **Computation**: This is incorrect. Cohen's kappa and the Pearson correlation coefficient (which for binary data is the phi coefficient, $\\phi$) are different statistics. While related, they are not interchangeable. Kappa is a measure of agreement, whereas correlation is a measure of association. They yield identical values only in the specific case where the marginal distributions of the two raters are identical (i.e., no bias). In general, $\\kappa \\neq \\phi$. The problem explicitly asks for Cohen's kappa.\n-   **Interpretation**: While a correlation of $0.72$ might be considered strong, the premise of the calculation is erroneous.\n-   **Verdict**: **Incorrect**.\n\n**E. Compute expected agreement by chance as $p_e = \\left(\\dfrac{n_{1+} + n_{+1}}{2n}\\right)^2 + \\left(1 - \\dfrac{n_{1+} + n_{+1}}{2n}\\right)^2$ using the average prevalence across raters, then compute $\\kappa = \\dfrac{p_o - p_e}{1 - p_e}$. Interpret $\\kappa = 0.72$ as only moderate agreement because the sample size is $20$.**\n-   **Computation**: The formula for $p_e$ provided here is not for Cohen's kappa. This formula calculates chance agreement based on the average prevalence of codes across both raters. This is the method used for Scott's Pi ($\\pi$), a different IRR statistic. Cohen's kappa maintains the distinction between the two raters' marginals.\n-   **Interpretation**: The interpretation of $\\kappa=0.72$ as \"only moderate\" is a possible but less common labeling. More importantly, attributing this interpretation to the small sample size is flawed logic. A small sample size ($n=20$) affects the *precision* of the kappa estimate (i.e., its confidence interval will be wide), but it does not change the interpretation of the point estimate itself.\n-   **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once patient experience data is collected, health systems face the challenge of using it to make fair and meaningful evaluations. Simply comparing raw scores between clinicians can be misleading due to different patient populations or random variation. This exercise  demonstrates how to use a standard statistical transformation, the $z$-score, to benchmark individual performance against a larger population. This hands-on practice is essential for anyone involved in healthcare quality improvement, as it provides a robust method for identifying and acting on performance data.",
            "id": "4400356",
            "problem": "A health system is evaluating the patient experience of care using a standardized communication domain score on a $1$ to $5$ scale for individual clinicians. To compare clinicians fairly against a national benchmark, the system standardizes each score to a $z$-score relative to the benchmark distribution, which is approximately normal with population mean $\\mu = 3.5$ and population standard deviation $\\sigma = 0.3$. By statistical definition, a $z$-score for an observed value $x$ is given by the standardized transformation that measures the number of population standard deviations the observation is from the population mean.\n\nThe system uses a policy to categorize each clinician’s performance based on the standardized score:\n- Exceeds expectations if $z > 1.0$,\n- Meets expectations if $-1.0 \\leq z \\leq 1.0$,\n- Needs improvement if $z < -1.0$.\n\nFor team-based incentives, the system assigns a group-level incentive multiplier $M$ defined as the baseline $1$ plus $0.02$ for each clinician categorized as “Exceeds expectations,” plus $0.01$ for each clinician categorized as “Meets expectations,” and minus $0.015$ for each clinician categorized as “Needs improvement.”\n\nA team has five clinicians with patient experience scores $3.4$, $3.7$, $3.8$, $3.2$, and $3.9$. Using only the definition of the $z$-score and the categorization rules above, compute the resulting group incentive multiplier $M$ as a decimal. Round your final answer to four significant figures. Do not include any units in your answer.",
            "solution": "The problem has been validated and is deemed sound. It is self-contained, scientifically grounded in basic statistical principles, and well-posed. All necessary data and definitions are provided for a unique, verifiable solution.\n\nThe problem requires the calculation of a group incentive multiplier, $M$, for a team of five clinicians. The calculation is based on their individual patient experience scores, which are standardized relative to a national benchmark.\n\nFirst, we must formalize the definition of the $z$-score. The problem states that a $z$-score \"measures the number of population standard deviations the observation is from the population mean.\" For an observed value $x$, a population mean $\\mu$, and a population standard deviation $\\sigma$, this corresponds to the well-established formula:\n$$z = \\frac{x - \\mu}{\\sigma}$$\nThe problem provides the population parameters for the benchmark distribution: $\\mu = 3.5$ and $\\sigma = 0.3$. The team consists of $5$ clinicians with the following scores: $x_1 = 3.4$, $x_2 = 3.7$, $x_3 = 3.8$, $x_4 = 3.2$, and $x_5 = 3.9$.\n\nWe will calculate the $z$-score for each clinician and then categorize their performance according to the given rules:\n- Exceeds expectations: $z > 1.0$\n- Meets expectations: $-1.0 \\leq z \\leq 1.0$\n- Needs improvement: $z < -1.0$\n\nFor Clinician 1 with score $x_1 = 3.4$:\n$$z_1 = \\frac{3.4 - 3.5}{0.3} = \\frac{-0.1}{0.3} = -\\frac{1}{3} \\approx -0.333$$\nSince $-1.0 \\leq z_1 \\leq 1.0$, this clinician \"Meets expectations.\"\n\nFor Clinician 2 with score $x_2 = 3.7$:\n$$z_2 = \\frac{3.7 - 3.5}{0.3} = \\frac{0.2}{0.3} = \\frac{2}{3} \\approx 0.667$$\nSince $-1.0 \\leq z_2 \\leq 1.0$, this clinician \"Meets expectations.\"\n\nFor Clinician 3 with score $x_3 = 3.8$:\n$$z_3 = \\frac{3.8 - 3.5}{0.3} = \\frac{0.3}{0.3} = 1.0$$\nSince $-1.0 \\leq z_3 \\leq 1.0$, this clinician \"Meets expectations.\" The upper boundary is inclusive for this category.\n\nFor Clinician 4 with score $x_4 = 3.2$:\n$$z_4 = \\frac{3.2 - 3.5}{0.3} = \\frac{-0.3}{0.3} = -1.0$$\nSince $-1.0 \\leq z_4 \\leq 1.0$, this clinician \"Meets expectations.\" The lower boundary is inclusive for this category.\n\nFor Clinician 5 with score $x_5 = 3.9$:\n$$z_5 = \\frac{3.9 - 3.5}{0.3} = \\frac{0.4}{0.3} = \\frac{4}{3} \\approx 1.333$$\nSince $z_5 > 1.0$, this clinician \"Exceeds expectations.\"\n\nNext, we count the number of clinicians in each performance category. Let $N_{ex}$ be the number of clinicians who exceed expectations, $N_{me}$ be the number who meet expectations, and $N_{ni}$ be the number who need improvement.\n- $N_{ex} = 1$\n- $N_{me} = 4$\n- $N_{ni} = 0$\n\nThe group incentive multiplier $M$ is defined as:\n$$M = 1 + (0.02 \\times N_{ex}) + (0.01 \\times N_{me}) - (0.015 \\times N_{ni})$$\nSubstituting the counts we found:\n$$M = 1 + (0.02 \\times 1) + (0.01 \\times 4) - (0.015 \\times 0)$$\n$$M = 1 + 0.02 + 0.04 - 0$$\n$$M = 1.06$$\nThe problem requires the final answer to be rounded to four significant figures. The number $1.06$ has three significant figures. To express it with four significant figures, we add a trailing zero.\n$$M = 1.060$$\nThis is the final group incentive multiplier.",
            "answer": "$$\\boxed{1.060}$$"
        },
        {
            "introduction": "The ultimate goal of measuring the patient experience is to improve it. When a health system implements a new program, such as communication training for clinicians, how can it determine if the program actually caused an improvement? This capstone problem  introduces the difference-in-differences (DiD) method, a powerful quasi-experimental technique used to estimate the causal effect of an intervention. By learning to apply the DiD framework, you gain a critical skill for moving beyond simple pre-post comparisons and rigorously evaluating the real-world impact of quality improvement initiatives.",
            "id": "4400358",
            "problem": "A regional health system implements a patient-centered communication training for clinicians in one hospital wing (treatment group) while another comparable wing does not receive the training (control group). Patient experience of care is measured using a standardized instrument such as the Consumer Assessment of Healthcare Providers and Systems (CAHPS), reported on a $5$-point scale, where higher values reflect better patient experience. The mean scores are observed for both groups in the period before and after the intervention: treatment pre-intervention $\\bar{Y}_{T,\\text{pre}}=3.2$, treatment post-intervention $\\bar{Y}_{T,\\text{post}}=3.8$, control pre-intervention $\\bar{Y}_{C,\\text{pre}}=3.1$, and control post-intervention $\\bar{Y}_{C,\\text{post}}=3.3$.\n\nStarting from the potential outcomes definition of the average treatment effect (ATE), $ATE = \\mathbb{E}[Y(1) - Y(0)]$, and the parallel trends assumption that the change in the untreated potential outcome over time would have been the same in the treatment and control groups absent the intervention, derive the difference-in-differences estimand from first principles and then compute its numerical value using the provided pre/post means. Clearly state any assumptions needed to justify identification. Express the final effect in points on the $5$-point scale as an exact value (do not round).",
            "solution": "This problem is valid as it is scientifically grounded in the principles of causal inference, specifically the difference-in-differences (DiD) method, and provides a well-posed question with sufficient, consistent data. The terminology and scenario are standard in health systems science and econometrics.\n\nThe objective is to derive the difference-in-differences (DiD) estimand from first principles using the potential outcomes framework and then to compute its value.\n\nLet $D_i$ be an indicator variable where $D_i=1$ if an individual $i$ is in the treatment group (the hospital wing receiving the training) and $D_i=0$ if in the control group. Let $t$ be a time indicator, with $t=0$ for the pre-intervention period and $t=1$ for the post-intervention period.\n\nIn the potential outcomes framework, for each individual $i$ in the post-intervention period ($t=1$), there are two potential outcomes:\n- $Y_i(1)$: the outcome if the individual (or their clinician) had received the treatment.\n- $Y_i(0)$: the outcome if the individual (or their clinician) had not received the treatment.\n\nThe problem statement refers to the Average Treatment Effect (ATE), $\\mathbb{E}[Y(1) - Y(0)]$. However, the DiD design with a non-randomly assigned treatment group identifies the Average Treatment Effect on the Treated (ATT), which is the average effect of the treatment for those who actually received it. The ATT is formally defined as:\n$$ \\text{ATT} = \\mathbb{E}[Y(1) - Y(0) | D=1] $$\nWe can separate this into two components:\n$$ \\text{ATT} = \\mathbb{E}[Y(1) | D=1] - \\mathbb{E}[Y(0) | D=1] $$\nThe first term, $\\mathbb{E}[Y(1) | D=1]$, is the expected outcome for the treatment group in the post-intervention period, given that they received the treatment. This is observable from the data. We can estimate this with the sample mean of the treatment group in the post-period, $\\bar{Y}_{T,\\text{post}}$.\n$$ \\mathbb{E}[Y(1) | D=1] = \\mathbb{E}[Y_{\\text{post}} | D=1] $$\nThe second term, $\\mathbb{E}[Y(0) | D=1]$, is the expected outcome for the treatment group in the post-intervention period *had they not received the treatment*. This is the fundamental problem of causal inference: this quantity is a counterfactual and is not directly observable.\n\nTo identify this counterfactual term, we must invoke an assumption. The problem states the key identifying assumption for the DiD estimator: the **parallel trends assumption**. This assumption states that, in the absence of treatment, the average change in the outcome for the treatment group would have been the same as the average change in the outcome for the control group. Formally, using potential outcomes in both pre- ($t=0$) and post- ($t=1$) periods:\n$$ \\mathbb{E}[Y_{\\text{post}}(0) - Y_{\\text{pre}}(0) | D=1] = \\mathbb{E}[Y_{\\text{post}}(0) - Y_{\\text{pre}}(0) | D=0] $$\nIn the pre-intervention period ($t=0$), no group has received the treatment. Therefore, the observed outcome is the untreated potential outcome for everyone: $Y_{\\text{pre}} = Y_{\\text{pre}}(0)$. Likewise, for the control group in the post-period, their observed outcome is their untreated potential outcome: $Y_{\\text{post}}|_{D=0} = Y_{\\text{post}}(0)|_{D=0}$. We can thus rewrite the parallel trends assumption using observable outcomes for the control group and for all groups in the pre-period:\n$$ \\mathbb{E}[Y_{\\text{post}}(0) | D=1] - \\mathbb{E}[Y_{\\text{pre}} | D=1] = \\mathbb{E}[Y_{\\text{post}} | D=0] - \\mathbb{E}[Y_{\\text{pre}} | D=0] $$\nNow, we can solve for our unobservable counterfactual, $\\mathbb{E}[Y(0) | D=1] = \\mathbb{E}[Y_{\\text{post}}(0) | D=1]$:\n$$ \\mathbb{E}[Y_{\\text{post}}(0) | D=1] = \\mathbb{E}[Y_{\\text{pre}} | D=1] + \\left( \\mathbb{E}[Y_{\\text{post}} | D=0] - \\mathbb{E}[Y_{\\text{pre}} | D=0] \\right) $$\nThis equation shows that the counterfactual outcome for the treated group can be estimated by taking their pre-intervention outcome and adding the change observed in the control group over the same period.\n\nNow, we substitute this expression back into the definition of ATT:\n$$ \\text{ATT} = \\mathbb{E}[Y_{\\text{post}} | D=1] - \\left( \\mathbb{E}[Y_{\\text{pre}} | D=1] + \\mathbb{E}[Y_{\\text{post}} | D=0] - \\mathbb{E}[Y_{\\text{pre}} | D=0] \\right) $$\nRearranging the terms yields the difference-in-differences estimand:\n$$ \\text{ATT} = \\left( \\mathbb{E}[Y_{\\text{post}} | D=1] - \\mathbb{E}[Y_{\\text{pre}} | D=1] \\right) - \\left( \\mathbb{E}[Y_{\\text{post}} | D=0] - \\mathbb{E}[Y_{\\text{pre}} | D=0] \\right) $$\nThis expression is the \"difference in differences\": the difference in the average outcome for the treatment group before and after the intervention, minus the difference in the average outcome for the control group over the same time period.\n\nAn additional key assumption, usually implicit, is the Stable Unit Treatment Value Assumption (SUTVA), which posits that there are no spillover effects between the treatment and control groups and that the treatment is uniform for all in the treatment group.\n\nTo compute the numerical value, we use the provided sample means as estimates for the population expectations:\n$\\bar{Y}_{T,\\text{post}} = \\mathbb{E}[Y_{\\text{post}} | D=1] = 3.8$\n$\\bar{Y}_{T,\\text{pre}} = \\mathbb{E}[Y_{\\text{pre}} | D=1] = 3.2$\n$\\bar{Y}_{C,\\text{post}} = \\mathbb{E}[Y_{\\text{post}} | D=0] = 3.3$\n$\\bar{Y}_{C,\\text{pre}} = \\mathbb{E}[Y_{\\text{pre}} | D=0] = 3.1$\n\nThe DiD estimate, $\\hat{\\delta}_{\\text{DiD}}$, is:\n$$ \\hat{\\delta}_{\\text{DiD}} = (\\bar{Y}_{T,\\text{post}} - \\bar{Y}_{T,\\text{pre}}) - (\\bar{Y}_{C,\\text{post}} - \\bar{Y}_{C,\\text{pre}}) $$\nPlugging in the given values:\n$$ \\hat{\\delta}_{\\text{DiD}} = (3.8 - 3.2) - (3.3 - 3.1) $$\nFirst, calculate the change for the treatment group:\n$$ \\Delta_T = 3.8 - 3.2 = 0.6 $$\nNext, calculate the change for the control group:\n$$ \\Delta_C = 3.3 - 3.1 = 0.2 $$\nFinally, calculate the difference of these changes:\n$$ \\hat{\\delta}_{\\text{DiD}} = 0.6 - 0.2 = 0.4 $$\nThe estimated average treatment effect on the treated is an increase of $0.4$ points on the $5$-point scale.",
            "answer": "$$\n\\boxed{0.4}\n$$"
        }
    ]
}