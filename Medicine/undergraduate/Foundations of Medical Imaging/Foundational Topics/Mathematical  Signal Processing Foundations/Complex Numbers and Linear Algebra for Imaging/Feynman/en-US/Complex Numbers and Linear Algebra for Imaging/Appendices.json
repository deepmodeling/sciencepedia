{
    "hands_on_practices": [
        {
            "introduction": "The Fourier Transform is the cornerstone of Magnetic Resonance Imaging, providing the mathematical link between the spatial information of an image and its measured frequency data, known as k-space. A critical property of the unitary Fourier Transform is that it conserves energy, a principle formalized by Parseval's theorem. This exercise  allows you to computationally verify this fundamental law, building confidence that the total signal power—a crucial physical quantity—remains unchanged when we move between the image and k-space domains.",
            "id": "4870067",
            "problem": "You are to write a complete, runnable program that tests the preservation of energy between the image domain and the frequency domain (commonly called k-space in Magnetic Resonance Imaging (MRI)) using the Discrete Fourier Transform (DFT). This task verifies the equality predicted by the foundational principle known as Parseval's theorem under a unitary transform.\n\nBegin from the following fundamental base that is standard in the foundations of medical imaging:\n\n1. The complex-valued signal or image is modeled as a finite array of samples, either one-dimensional or two-dimensional. For a one-dimensional signal of length $N$, write $s[n] \\in \\mathbb{C}$ for $n \\in \\{0,1,\\dots,N-1\\}$; for a two-dimensional array of size $M \\times N$, write $s[m,n] \\in \\mathbb{C}$ for $m \\in \\{0,1,\\dots,M-1\\}$ and $n \\in \\{0,1,\\dots,N-1\\}$.\n\n2. The unitary Discrete Fourier Transform (DFT) is defined by\n$$\n\\hat{s}[k] = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1} s[n] \\, e^{-i 2\\pi \\frac{n k}{N}}, \\quad k \\in \\{0,1,\\dots,N-1\\},\n$$\nfor the one-dimensional case, and\n$$\n\\hat{s}[u,v] = \\frac{1}{\\sqrt{MN}} \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} s[m,n] \\, e^{-i 2\\pi \\left( \\frac{m u}{M} + \\frac{n v}{N} \\right)}, \\quad (u,v) \\in \\{0,1,\\dots,M-1\\} \\times \\{0,1,\\dots,N-1\\},\n$$\nfor the two-dimensional case, where $i$ is the imaginary unit and all angles are in radians.\n\n3. The energy of a complex array in the image domain is defined by\n$$\nE_{\\text{image}} = \\sum_{n} |s[n]|^2 \\quad \\text{(one-dimensional)}, \\qquad E_{\\text{image}} = \\sum_{m} \\sum_{n} |s[m,n]|^2 \\quad \\text{(two-dimensional)},\n$$\nand the energy in k-space is defined by\n$$\nE_{\\text{k}} = \\sum_{k} |\\hat{s}[k]|^2 \\quad \\text{(one-dimensional)}, \\qquad E_{\\text{k}} = \\sum_{u} \\sum_{v} |\\hat{s}[u,v]|^2 \\quad \\text{(two-dimensional)}.\n$$\n\nYour program must implement the following:\n\n- Use a unitary Fast Fourier Transform (FFT) implementation that corresponds to the above DFT definitions (that is, orthonormal normalization).\n- For each provided test case, compute $E_{\\text{image}}$ and $E_{\\text{k}}$ and verify if $|E_{\\text{image}} - E_{\\text{k}}| \\le \\varepsilon$, with tolerance $\\varepsilon = 10^{-12}$.\n- Produce as output a single line containing a comma-separated list of boolean values enclosed in square brackets, where each boolean corresponds to whether the energy equality holds within tolerance for the respective test case. For example, the output must be of the form $[b_1,b_2,b_3,b_4]$, where each $b_j$ is either $\\text{True}$ or $\\text{False}$ without quotes.\n\nNo physical units are involved in this problem; treat all quantities as dimensionless. All angles in any trigonometric functions must be in radians.\n\nTest Suite:\n\nCompute the above for the following signals/images:\n\n1. One-dimensional complex signal of length $7$:\n   - $s[0] = 1 + 2i$\n   - $s[1] = -0.5 + 0.25i$\n   - $s[2] = 0 + 0i$\n   - $s[3] = 3 - 4i$\n   - $s[4] = -2 + 0i$\n   - $s[5] = 0.1 + 0.2i$\n   - $s[6] = -1.5 - 0.5i$\n\n2. Two-dimensional complex image of size $8 \\times 8$ defined by the analytic formula\n   $$\n   s[m,n] = \\exp\\!\\big(-\\alpha \\big((m - m_0)^2 + (n - n_0)^2\\big)\\big) + i \\, \\beta \\, \\sin\\!\\Big(2\\pi \\Big(\\frac{u_0 m}{M} + \\frac{v_0 n}{N}\\Big)\\Big),\n   $$\n   with parameters $M = 8$, $N = 8$, $\\alpha = 0.3$, $\\beta = 0.2$, $m_0 = 2.3$, $n_0 = 5.5$, $u_0 = 1.5$, $v_0 = 2.0$. All trigonometric angles are in radians.\n\n3. One-dimensional complex signal of length $16$ that is identically zero:\n   - $s[n] = 0 + 0i$ for all $n \\in \\{0,1,\\dots,15\\}$.\n\n4. Two-dimensional complex image of size $10 \\times 12$ containing a single complex impulse:\n   - $s[p,q] = 3 - 4i$ at $(p,q) = (2,7)$, and $s[m,n] = 0 + 0i$ for all other $(m,n)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[b_1,b_2,b_3,b_4]$). Each $b_j$ must be a boolean value indicating whether $|E_{\\text{image}} - E_{\\text{k}}| \\le 10^{-12}$ for the corresponding test case.",
            "solution": "The problem requires the verification of Parseval's theorem for the unitary Discrete Fourier Transform (DFT) across several test cases. This theorem is a fundamental principle in Fourier analysis and signal processing, establishing that the energy of a signal is conserved under the unitary DFT. In medical imaging, particularly Magnetic Resonance Imaging (MRI), this corresponds to the equality of energy between the image domain and the frequency domain, or k-space.\n\nThe energy of a discrete, complex-valued signal $s$ is defined as the sum of the squared magnitudes of its samples. This is equivalent to the squared $L_2$-norm of the signal. For a one-dimensional signal $s[n]$ of length $N$, the image-domain energy is:\n$$\nE_{\\text{image}} = \\sum_{n=0}^{N-1} |s[n]|^2\n$$\nFor a two-dimensional signal $s[m,n]$ of size $M \\times N$, the energy is:\n$$\nE_{\\text{image}} = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} |s[m,n]|^2\n$$\n\nThe problem defines a unitary DFT. The transform of a signal $s$ results in a k-space signal $\\hat{s}$. For the one-dimensional case, the transform is:\n$$\n\\hat{s}[k] = \\frac{1}{\\sqrt{N}} \\sum_{n=0}^{N-1} s[n] \\, e^{-i 2\\pi \\frac{n k}{N}}\n$$\nFor the two-dimensional case, it is:\n$$\n\\hat{s}[u,v] = \\frac{1}{\\sqrt{MN}} \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} s[m,n] \\, e^{-i 2\\pi \\left( \\frac{m u}{M} + \\frac{n v}{N} \\right)}\n$$\nThe normalization factors $\\frac{1}{\\sqrt{N}}$ and $\\frac{1}{\\sqrt{MN}}$ ensure the transform is unitary.\n\nThe corresponding energy in k-space is calculated analogously to the image-domain energy:\n$$\nE_{\\text{k}} = \\sum_{k=0}^{N-1} |\\hat{s}[k]|^2 \\quad \\text{(one-dimensional)}\n$$\n$$\nE_{\\text{k}} = \\sum_{u=0}^{M-1} \\sum_{v=0}^{N-1} |\\hat{s}[u,v]|^2 \\quad \\text{(two-dimensional)}\n$$\n\nParseval's theorem for the unitary DFT states that the energy is preserved, i.e., $E_{\\text{image}} = E_{\\text{k}}$. Our task is to numerically verify this equality for four distinct signals. Due to the nature of floating-point arithmetic, we will not expect exact equality but will test if the absolute difference is within a small tolerance $\\varepsilon = 10^{-12}$:\n$$\n|E_{\\text{image}} - E_{\\text{k}}| \\le \\varepsilon\n$$\n\nThe computational procedure for each test case is as follows:\n1.  Construct the input signal $s$ as a NumPy array of complex numbers.\n2.  Calculate the image-domain energy $E_{\\text{image}}$ by summing the squared absolute values of the elements of $s$. This can be computed via `numpy.sum(numpy.abs(s)**2)`.\n3.  Compute the k-space representation $\\hat{s}$ using the Fast Fourier Transform (FFT) algorithm. To match the problem's unitary definition, the `norm=\"ortho\"` option in NumPy's FFT functions (`numpy.fft.fft` for 1D, `numpy.fft.fft2` for 2D) must be used. This applies the correct orthonormal scaling.\n4.  Calculate the k-space energy $E_{\\text{k}}$ by summing the squared absolute values of the elements of $\\hat{s}$.\n5.  Compare $E_{\\text{image}}$ and $E_{\\text{k}}$ using the specified tolerance $\\varepsilon$. The result is a boolean value indicating whether the energy conservation principle holds.\n\nThe four test cases are implemented as follows:\n-   **Case 1:** A one-dimensional complex signal of length $7$ is created directly from the provided list of values.\n-   **Case 2:** A two-dimensional complex image of size $8 \\times 8$ is generated programmatically. We create two grids of indices, $m \\in \\{0, \\dots, 7\\}$ and $n \\in \\{0, \\dots, 7\\}$, and apply the given analytic formula to each coordinate pair $(m,n)$.\n-   **Case 3:** A one-dimensional complex signal of length $16$ is created as an array of zeros. This serves as a trivial base case where both energies are expected to be exactly $0$.\n-   **Case 4:** A two-dimensional complex image of size $10 \\times 12$ is created as an array of zeros, with a single non-zero value (a complex impulse) at coordinates $(2,7)$.\n\nThe program will execute this procedure for all four cases and report the boolean results in the specified format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Verifies Parseval's theorem for the unitary DFT on four test cases.\n    \"\"\"\n    \n    tolerance = 1e-12\n    results = []\n\n    # --- Test Case 1: 1D complex signal of length 7 ---\n    s1 = np.array([\n        1 + 2j,\n        -0.5 + 0.25j,\n        0 + 0j,\n        3 - 4j,\n        -2 + 0j,\n        0.1 + 0.2j,\n        -1.5 - 0.5j\n    ], dtype=np.complex128)\n    \n    # --- Test Case 2: 2D complex image of size 8x8 from formula ---\n    M2, N2 = 8, 8\n    alpha = 0.3\n    beta = 0.2\n    m0, n0 = 2.3, 5.5\n    u0, v0 = 1.5, 2.0\n    \n    m_indices, n_indices = np.meshgrid(np.arange(M2), np.arange(N2), indexing='ij')\n    \n    term1 = np.exp(-alpha * ((m_indices - m0)**2 + (n_indices - n0)**2))\n    term2 = 1j * beta * np.sin(2 * np.pi * (u0 * m_indices / M2 + v0 * n_indices / N2))\n    s2 = term1 + term2\n\n    # --- Test Case 3: 1D complex signal of length 16 (all zeros) ---\n    s3 = np.zeros(16, dtype=np.complex128)\n\n    # --- Test Case 4: 2D complex image 10x12 with a single impulse ---\n    M4, N4 = 10, 12\n    s4 = np.zeros((M4, N4), dtype=np.complex128)\n    s4[2, 7] = 3 - 4j\n    \n    signals = [s1, s2, s3, s4]\n\n    for s in signals:\n        # Calculate image domain energy\n        e_image = np.sum(np.abs(s)**2)\n\n        # Compute k-space representation using unitary FFT\n        if s.ndim == 1:\n            s_hat = np.fft.fft(s, norm=\"ortho\")\n        else: # s.ndim == 2\n            s_hat = np.fft.fft2(s, norm=\"ortho\")\n        \n        # Calculate k-space energy\n        e_k = np.sum(np.abs(s_hat)**2)\n        \n        # Verify if energy is preserved within tolerance\n        is_preserved = np.abs(e_image - e_k) <= tolerance\n        results.append(is_preserved)\n\n    # Format the final output string\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While the Discrete Fourier Transform (DFT) is a powerful tool, its application to finite-length signals, as is always the case in practice, can introduce artifacts. This exercise  explores one of the most common issues: spectral leakage, where energy from a signal's true frequency \"leaks\" into adjacent frequency bins, potentially obscuring important details. You will implement and compare the effect of different window functions to see firsthand how this leakage can be quantified and effectively mitigated, a crucial step in robust signal analysis.",
            "id": "4870083",
            "problem": "Consider a finite-length, uniformly sampled real-valued discrete-time signal $x[n]$ generated by sampling a single-tone sinusoid. Let the sampling frequency be $f_s$ in hertz, the number of samples be $N$, and the tone frequency be $f_0$ in hertz. The samples are defined for integer indices $n$ with $0 \\le n \\le N-1$ by the expression $x[n] = A \\sin\\left(2\\pi \\frac{f_0}{f_s} n\\right)$, where $A$ is a real amplitude constant. Suppose the spectrum is computed using the Fast Fourier Transform (FFT), which is an efficient algorithm for computing the Discrete Fourier Transform (DFT). The DFT of a length-$N$ sequence $x[n]$ is given by $X[k] = \\sum_{n=0}^{N-1} x[n] e^{-j 2\\pi \\frac{k n}{N}}$ for integer frequency bin indices $k$ with $0 \\le k \\le N-1$, where $j$ denotes the imaginary unit.\n\nIn practical imaging and signal analysis, truncating a continuous signal to a finite interval introduces spectral leakage. Windowing is used to mitigate leakage by multiplying the signal by a deterministic sequence $w[n]$ prior to applying the FFT. This multiplication in the discrete-time domain corresponds to convolution in the discrete-frequency domain. Consider two windows:\n- The rectangular window $w_{\\mathrm{rect}}[n] = 1$ for $0 \\le n \\le N-1$.\n- The Hann window $w_{\\mathrm{hann}}[n] = \\frac{1}{2} - \\frac{1}{2} \\cos\\left(2\\pi \\frac{n}{N-1}\\right)$ for $0 \\le n \\le N-1$.\n\nDefine the windowed spectrum as $X_w[k] = \\sum_{n=0}^{N-1} \\big(x[n] w[n]\\big) e^{-j 2\\pi \\frac{k n}{N}}$ and its energy distribution across bins as $E[k] = \\left|X_w[k]\\right|^2$. Let $k_0$ be the index that maximizes $E[k]$ over $0 \\le k \\le N-1$ (the dominant bin). For a real-valued sinusoid, the conjugate peak occurs at index $k_1 = (N - k_0) \\bmod N$. For a user-specified nonnegative integer mainlobe half-width $m$, define the mainlobe support $S$ as the union of all indices within $m$ bins (inclusive) of both $k_0$ and $k_1$, with wrap-around modulo $N$. Formally, $S = \\left\\{(k_0 + \\delta) \\bmod N \\mid -m \\le \\delta \\le m \\right\\} \\cup \\left\\{(k_1 + \\delta) \\bmod N \\mid -m \\le \\delta \\le m \\right\\}$. The leakage ratio is defined by\n$$\n\\lambda = \\frac{\\sum_{k \\notin S} E[k]}{\\sum_{k=0}^{N-1} E[k]}.\n$$\nThis ratio quantifies the fraction of spectral energy outside the mainlobe regions associated with the dominant positive and negative frequency components.\n\nTask: For each test case specified below, generate the finite sinusoid $x[n]$ using the angle argument in radians, compute the FFT-based spectrum with and without windowing (rectangular and Hann windows), evaluate the leakage ratio $\\lambda$ for each case, and report the comparison as three floats per test case: the leakage ratio without windowing (rectangular), the leakage ratio with Hann windowing, and the difference between them (rectangular minus Hann). All frequencies are in hertz, and angles in the sinusoid definition must be in radians.\n\nTest Suite (each case specifies $(N, f_s, f_0, A, m)$):\n- Case $1$: $(256, 1024, 128, 1, 0)$, a tone exactly aligned with a DFT bin and zero half-width mainlobe.\n- Case $2$: $(256, 1024, 130, 1, 1)$, a tone slightly off-bin to demonstrate leakage and mitigation by windowing.\n- Case $3$: $(256, 1024, 510, 1, 1)$, a tone near the Nyquist frequency to test edge behavior.\n- Case $4$: $(64, 128, 20.5, 1, 1)$, shorter record with off-bin tone to stress coarse resolution.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list corresponds to one test case and must be a list of three floats in the order described above. For example, a valid output format is\n$[\\,[\\lambda_{\\mathrm{rect},1},\\lambda_{\\mathrm{hann},1},\\Delta_1],\\,[\\lambda_{\\mathrm{rect},2},\\lambda_{\\mathrm{hann},2},\\Delta_2],\\,[\\lambda_{\\mathrm{rect},3},\\lambda_{\\mathrm{hann},3},\\Delta_3],\\,[\\lambda_{\\mathrm{rect},4},\\lambda_{\\mathrm{hann},4},\\Delta_4]\\,]$.",
            "solution": "The problem is scientifically grounded, well-posed, objective, and internally consistent. All necessary parameters, definitions, and functions are provided to allow for a unique and meaningful solution. The problem statement adheres to the standard principles of digital signal processing. Therefore, a solution will be provided.\n\nThe core task is to analyze spectral leakage of a discrete-time sinusoidal signal and to quantify the effect of windowing. The Discrete Fourier Transform (DFT), typically computed via the Fast Fourier Transform (FFT), is the primary tool for spectral analysis. A key assumption of the DFT is that the finite-length signal segment $x[n]$ of length $N$ represents one period of an infinitely periodic signal. If the signal $x[n]$ contains a non-integer number of cycles over its duration, this periodicity assumption creates a sharp discontinuity at the boundary where the signal \"wraps around\". This artifact introduces spurious frequency components into the spectrum, a phenomenon known as spectral leakage, where energy from the true frequency \"leaks\" into other frequency bins.\n\nWindowing is a technique to mitigate spectral leakage. It involves multiplying the signal $x[n]$ by a window function $w[n]$ that smoothly tapers to zero or near-zero at its endpoints. This tapering reduces the boundary discontinuity, thereby suppressing the sidelobes in the frequency domain at the cost of widening the mainlobe. The problem contrasts the rectangular window, $w_{\\mathrm{rect}}[n] = 1$ for all $n$ (equivalent to no windowing), with the Hann window, $w_{\\mathrm{hann}}[n] = \\frac{1}{2} - \\frac{1}{2} \\cos\\left(2\\pi \\frac{n}{N-1}\\right)$, which is a common choice for leakage suppression.\n\nThe leakage ratio, $\\lambda$, is defined to quantify this effect:\n$$\n\\lambda = \\frac{\\sum_{k \\notin S} E[k]}{\\sum_{k=0}^{N-1} E[k]}\n$$\nwhere $E[k]$ is the energy in frequency bin $k$, and $S$ is the set of indices corresponding to the main spectral peaks (mainlobes). This ratio measures the fraction of total spectral energy that lies outside the mainlobes.\n\nThe solution is derived by following a systematic computational procedure for each test case specified by the parameters $(N, f_s, f_0, A, m)$.\n\n1.  **Signal Generation**: For each test case, the discrete-time signal $x[n]$ is generated for integer indices $n$ from $0$ to $N-1$ according to the formula:\n    $$\n    x[n] = A \\sin\\left(2\\pi \\frac{f_0}{f_s} n\\right)\n    $$\n\n2.  **Window Application**: Two versions of the signal are prepared.\n    -   Rectangular windowing: $x_{\\mathrm{rect}}[n] = x[n] \\cdot w_{\\mathrm{rect}}[n] = x[n]$, since $w_{\\mathrm{rect}}[n] = 1$.\n    -   Hann windowing: $x_{\\mathrm{hann}}[n] = x[n] \\cdot w_{\\mathrm{hann}}[n]$, where $w_{\\mathrm{hann}}[n] = \\frac{1}{2} - \\frac{1}{2} \\cos\\left(2\\pi \\frac{n}{N-1}\\right)$.\n\n3.  **Spectral Computation**: The DFT of each windowed signal is computed using the FFT algorithm.\n    -   $X_{\\mathrm{rect}}[k] = \\sum_{n=0}^{N-1} x_{\\mathrm{rect}}[n] e^{-j 2\\pi \\frac{k n}{N}}$\n    -   $X_{\\mathrm{hann}}[k] = \\sum_{n=0}^{N-1} x_{\\mathrm{hann}}[n] e^{-j 2\\pi \\frac{k n}{N}}$\n    where $j$ is the imaginary unit and $k$ is the frequency bin index from $0$ to $N-1$.\n\n4.  **Energy Spectrum**: The energy distribution across frequency bins is calculated as the squared magnitude of the complex spectrum.\n    -   $E_{\\mathrm{rect}}[k] = \\left|X_{\\mathrm{rect}}[k]\\right|^2$\n    -   $E_{\\mathrm{hann}}[k] = \\left|X_{\\mathrm{hann}}[k]\\right|^2$\n\n5.  **Leakage Ratio Calculation**: For each energy spectrum ($E_{\\mathrm{rect}}$ and $E_{\\mathrm{hann}}$), the leakage ratio $\\lambda$ is computed.\n    -   The total energy $E_{\\text{total}} = \\sum_{k=0}^{N-1} E[k]$ is calculated. By Parseval's theorem, this is proportional to the sum of squared signal values.\n    -   The index of the dominant spectral peak, $k_0$, is found: $k_0 = \\arg\\max_{k} E[k]$.\n    -   For a real-valued input signal, the spectrum is conjugate symmetric, meaning $|X[k]| = |X[(N-k) \\bmod N]|$. The conjugate peak index is thus $k_1 = (N - k_0) \\bmod N$.\n    -   The mainlobe support set $S$ is constructed based on the mainlobe half-width $m$. It comprises all indices within $m$ bins of $k_0$ and $k_1$, including wrap-around behavior (modulo $N$).\n    $$\n    S = \\left\\{(k_0 + \\delta) \\bmod N \\mid -m \\le \\delta \\le m \\right\\} \\cup \\left\\{(k_1 + \\delta) \\bmod N \\mid -m \\le \\delta \\le m \\right\\}\n    $$\n    -   The energy in the mainlobes, $E_{\\text{mainlobe}} = \\sum_{k \\in S} E[k]$, is summed.\n    -   The leakage energy is the total energy minus the mainlobe energy: $E_{\\text{leakage}} = E_{\\text{total}} - E_{\\text{mainlobe}}$.\n    -   The leakage ratio is then $\\lambda = E_{\\text{leakage}} / E_{\\text{total}}$, provided $E_{\\text{total}} > 0$.\n\n6.  **Result Consolidation**: For each test case, the three required values are calculated and stored: the leakage ratio for the rectangular window ($\\lambda_{\\mathrm{rect}}$), the leakage ratio for the Hann window ($\\lambda_{\\mathrm{hann}}$), and their difference ($\\Delta\\lambda = \\lambda_{\\mathrm{rect}} - \\lambda_{\\mathrm{hann}}$). The final output is an aggregation of these results for all test cases.\n\nFor cases where the signal frequency $f_0$ corresponds to an integer number of cycles within the $N$ samples (i.e., $N \\cdot f_0/f_s$ is an integer), as in Case $1$, the rectangular window will exhibit virtually zero leakage. In contrast, for off-bin frequencies (e.g., Cases $2, 3, 4$), the rectangular window produces significant leakage, which the Hann window is expected to mitigate effectively, resulting in a substantially smaller $\\lambda_{\\mathrm{hann}}$ and a large positive difference $\\Delta\\lambda$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.fft import fft\n\ndef solve():\n    \"\"\"\n    Solves the spectral leakage analysis problem for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, fs, f0, A, m)\n    test_cases = [\n        (256, 1024, 128, 1, 0),    # Case 1: Tone on-bin\n        (256, 1024, 130, 1, 1),    # Case 2: Tone off-bin\n        (256, 1024, 510, 1, 1),    # Case 3: Tone near Nyquist\n        (64, 128, 20.5, 1, 1),     # Case 4: Shorter record, off-bin\n    ]\n\n    results = []\n    \n    def calculate_leakage_ratio(E, N, m):\n        \"\"\"\n        Calculates the leakage ratio for a given energy spectrum.\n\n        Args:\n            E (np.ndarray): The energy spectrum |X[k]|^2.\n            N (int): The number of samples.\n            m (int): The mainlobe half-width.\n\n        Returns:\n            float: The leakage ratio lambda.\n        \"\"\"\n        total_energy = np.sum(E)\n        if total_energy == 0:\n            return 0.0\n\n        # Find the dominant frequency bin index\n        k0 = np.argmax(E)\n        \n        # Find the conjugate peak index\n        k1 = (N - k0) % N\n        \n        # Construct the mainlobe support set S\n        mainlobe_indices = set()\n        for delta in range(-m, m + 1):\n            mainlobe_indices.add((k0 + delta) % N)\n            mainlobe_indices.add((k1 + delta) % N)\n        \n        # Sum the energy in the mainlobes\n        mainlobe_energy = np.sum(E[list(mainlobe_indices)])\n        \n        # The leakage energy is the total energy minus the mainlobe energy\n        leakage_energy = total_energy - mainlobe_energy\n        \n        # The leakage ratio is the fraction of energy outside the mainlobes\n        ratio = leakage_energy / total_energy\n        return ratio\n\n    for case in test_cases:\n        N, fs, f0, A, m = case\n\n        # 1. Generate the signal\n        n = np.arange(N)\n        x = A * np.sin(2 * np.pi * f0 / fs * n)\n\n        # 2. Define and apply windows\n        # Rectangular window (effectively no windowing)\n        x_rect = x\n\n        # Hann window\n        w_hann = 0.5 - 0.5 * np.cos(2 * np.pi * n / (N - 1))\n        x_hann = x * w_hann\n        \n        # 3. Compute FFT-based spectra\n        X_rect = fft(x_rect)\n        X_hann = fft(x_hann)\n\n        # 4. Compute energy spectra\n        E_rect = np.abs(X_rect)**2\n        E_hann = np.abs(X_hann)**2\n        \n        # 5. Calculate leakage ratios\n        lambda_rect = calculate_leakage_ratio(E_rect, N, m)\n        lambda_hann = calculate_leakage_ratio(E_hann, N, m)\n        \n        # 6. Calculate the difference\n        delta_lambda = lambda_rect - lambda_hann\n        \n        results.append([lambda_rect, lambda_hann, delta_lambda])\n\n    # Format the final output string as a list of lists.\n    result_strings = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Image reconstruction is fundamentally an inverse problem: we aim to recover an image $x$ from measurements $y$ related by a system model $A$ in the equation $y = Ax + e$. When this problem is ill-conditioned, directly inverting the system matrix $A$ can drastically amplify noise, rendering the reconstruction useless. This practice  introduces regularization through Truncated Singular Value Decomposition (TSVD), a powerful technique that balances the tradeoff between solution accuracy (bias) and noise amplification (variance), providing a hands-on look at how linear algebra can be used to find stable and meaningful solutions to real-world imaging challenges.",
            "id": "4870009",
            "problem": "Consider a linear inverse problem in a complex-valued imaging system where measurements are modeled as $y \\in \\mathbb{C}^m$, an unknown image parameter vector $x \\in \\mathbb{C}^n$, a known system matrix $A \\in \\mathbb{C}^{m \\times n}$, and additive zero-mean complex noise $e \\in \\mathbb{C}^m$ with covariance $\\sigma^2 I_m$. The forward model is $y = A x + e$. Let the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^{H}$, where $U \\in \\mathbb{C}^{m \\times m}$ and $V \\in \\mathbb{C}^{n \\times n}$ are unitary, $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative singular values $\\{\\sigma_i\\}$, and $H$ denotes the conjugate transpose. For a truncation level $k$ with $0 \\leq k \\leq r$ where $r$ is the numerical rank of $A$, the truncated SVD estimator is defined as $x_k = V_k \\Sigma_k^{-1} U_k^{H} y$, where $U_k \\in \\mathbb{C}^{m \\times k}$ contains the first $k$ left singular vectors, $V_k \\in \\mathbb{C}^{n \\times k}$ contains the first $k$ right singular vectors, and $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ contains the first $k$ singular values on its diagonal.\n\nStarting from the definitions above and the linear estimator framework, derive expressions for the bias, variance, and the expected squared reconstruction error $E[\\|x_k - x\\|_2^2]$ of the truncated SVD estimator as functions of the truncation level $k$, the singular values $\\{\\sigma_i\\}$, the right singular vectors $V_k$, the unknown vector $x$, and the noise variance $\\sigma^2$. Your derivation must be based on the properties of unitary matrices, projections, and linear estimators for complex-valued random vectors with covariance $\\sigma^2 I_m$.\n\nThen, implement a complete program that, for each test case listed below, computes:\n- For every truncation level $k$ from $0$ to $r$:\n    - The squared bias term $\\| (I_n - V_k V_k^{H}) x \\|_2^2$.\n    - The variance term $\\sigma^2 \\sum_{i=1}^{k} \\sigma_i^{-2}$.\n    - The expected squared error $E[\\|x_k - x\\|_2^2]$ as the sum of the squared bias and the variance term.\n- The truncation level $k^\\star$ (choose the smallest $k$ in case of a tie) that minimizes the expected squared error.\n- The expected squared error at $k^\\star$, along with the squared bias and variance terms at $k^\\star$.\n\nAll vector norms must be the Euclidean $2$-norm, and all matrix transposes involving complex quantities must use the conjugate transpose operation. There are no physical units required in this problem. Angles are not used in this problem.\n\nTest suite (each case provides $A$, $x$, and $\\sigma^2$):\n\n- Case $1$ (diagonal, well-conditioned):\n    - $m = n = 5$,\n    - $A = \\mathrm{diag}([1.0, 0.9, 0.8, 0.7, 0.6])$,\n    - $x = [1.0, -0.5, 0.7, -0.3, 0.2]^T$,\n    - $\\sigma^2 = 0.0001$.\n\n- Case $2$ (diagonal, ill-conditioned):\n    - $m = n = 6$,\n    - $A = \\mathrm{diag}([5.0, 3.0, 1.5, 0.6, 0.3, 0.1])$,\n    - $x = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]^T$,\n    - $\\sigma^2 = 0.02$.\n\n- Case $3$ (partial discrete Fourier transform, complex-valued, underdetermined):\n    - $m = 6$, $n = 8$,\n    - Let $F \\in \\mathbb{C}^{8 \\times 8}$ be the unitary discrete Fourier transform matrix with entries $F_{j\\ell} = \\frac{1}{\\sqrt{8}} \\exp\\left(-\\mathrm{i} \\frac{2\\pi}{8} j \\ell\\right)$ for $j,\\ell \\in \\{0,1,\\dots,7\\}$.\n    - $A$ is formed by taking the first $6$ rows of $F$ (i.e., $A \\in \\mathbb{C}^{6 \\times 8}$ is the row-restricted unitary matrix).\n    - $x = [1.0 + 0.0\\mathrm{i}, 0.0 + 1.0\\mathrm{i}, -1.0 + 0.0\\mathrm{i}, 0.0 - 1.0\\mathrm{i}, 0.5 + 0.0\\mathrm{i}, -0.5 + 0.0\\mathrm{i}, 0.0 + 0.25\\mathrm{i}, -0.25 + 0.0\\mathrm{i}]^T$,\n    - $\\sigma^2 = 0.1$.\n\nProgram requirements:\n- For each test case, compute the truncation-level-dependent quantities described above, determine $k^\\star$, and report, in the specified output format, the tuple $(k^\\star, E[\\|x_{k^\\star} - x\\|_2^2], \\| (I_n - V_{k^\\star} V_{k^\\star}^{H}) x \\|_2^2, \\sigma^2 \\sum_{i=1}^{k^\\star} \\sigma_i^{-2})$.\n- Floating-point numbers in the final output must be rounded to $6$ decimal places.\n- The final output format must be a single line containing a comma-separated list enclosed in square brackets, concatenating the four values for each test case in order. Concretely, the output must be of the form $[k_1^\\star,\\ \\text{mse}_1,\\ \\text{bias}_1,\\ \\text{var}_1,\\ k_2^\\star,\\ \\text{mse}_2,\\ \\text{bias}_2,\\ \\text{var}_2,\\ k_3^\\star,\\ \\text{mse}_3,\\ \\text{bias}_3,\\ \\text{var}_3]$, where $k_j^\\star$ is an integer and each of the other values is a float rounded to $6$ decimal places.\n\nYour derivation must avoid shortcut formulas and proceed from the definitions of SVD, linear estimators, unitary projections, and covariance in complex-valued spaces. Ensure scientific realism and internal consistency. The program must be self-contained and reproducible without any external input.",
            "solution": "The problem requires the derivation of the bias, variance, and expected squared error for the truncated Singular Value Decomposition (TSVD) estimator in the context of a linear inverse problem, followed by a numerical implementation.\n\nThe linear model is given by $y = Ax + e$, where $y \\in \\mathbb{C}^m$ are the measurements, $x \\in \\mathbb{C}^n$ is the unknown vector, $A \\in \\mathbb{C}^{m \\times n}$ is the system matrix, and $e \\in \\mathbb{C}^m$ is zero-mean complex additive noise with covariance $E[ee^H] = \\sigma^2 I_m$.\n\nThe Singular Value Decomposition (SVD) of $A$ is $A = U \\Sigma V^H$, where $U$ and $V$ are unitary matrices and $\\Sigma$ is a real-valued $m \\times n$ diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, where $r = \\text{rank}(A)$. We can express $A$ as a sum of outer products: $A = \\sum_{i=1}^r \\sigma_i u_i v_i^H$, where $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\nThe TSVD estimator for a truncation level $k$ ($0 \\le k \\le r$) is defined as $x_k = V_k \\Sigma_k^{-1} U_k^H y$. Here, $V_k$ consists of the first $k$ columns of $V$, $U_k$ consists of the first $k$ columns of $U$, and $\\Sigma_k$ is a $k \\times k$ diagonal matrix with the first $k$ singular values $\\{\\sigma_1, \\dots, \\sigma_k\\}$. The estimator can also be written using the pseudoinverse of the rank-$k$ approximation of $A$, $A_k = U_k \\Sigma_k V_k^H$, as $x_k = A_k^\\dagger y$.\n\n### Derivation of the Bias\n\nThe bias of an estimator $\\hat{\\theta}$ is defined as $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. For the TSVD estimator $x_k$, the bias is $B(x_k) = E[x_k] - x$.\n\nFirst, we compute the expected value of the estimator, $E[x_k]$:\n$$E[x_k] = E[V_k \\Sigma_k^{-1} U_k^H y]$$\nSince $V_k$, $\\Sigma_k$, and $U_k$ are deterministic, we can move the expectation operator inside:\n$$E[x_k] = V_k \\Sigma_k^{-1} U_k^H E[y]$$\nThe expected value of the measurements $y$ is:\n$$E[y] = E[Ax + e] = A E[x] + E[e]$$\nSince $x$ is a deterministic (but unknown) vector and the noise $e$ is zero-mean ($E[e]=0$), we have $E[y] = Ax$.\nSubstituting this back into the expression for $E[x_k]$:\n$$E[x_k] = V_k \\Sigma_k^{-1} U_k^H (Ax)$$\nNow, we substitute the SVD of $A$:\n$$E[x_k] = (V_k \\Sigma_k^{-1} U_k^H) (U \\Sigma V^H) x$$\nThe term $U_k^H U$ represents the product of the first $k$ columns of $U$ (transposed and conjugated) with the full matrix $U$. Since the columns of $U$ are orthonormal, $u_i^H u_j = \\delta_{ij}$, the resulting $k \\times m$ matrix is $[I_k, 0]$. So, $U_k^H A = U_k^H (U \\Sigma V^H) = [I_k, 0] \\Sigma V^H$. The product $[I_k, 0] \\Sigma$ selects the top $k$ rows of $\\Sigma$, which we can write as $[\\Sigma_k, 0] V^H$.\nLet's use a more direct approach. The product $A_k^\\dagger A = (V_k \\Sigma_k^{-1} U_k^H)(U \\Sigma V^H)$ simplifies nicely.\n$$A_k^\\dagger A = V_k \\Sigma_k^{-1} (U_k^H U) \\Sigma V^H$$\nSince $U_k^H U = [I_k, 0]$, $U_k^H U \\Sigma$ is a $k \\times n$ matrix whose first $k$ columns form $\\Sigma_k$ and the rest are zero. That is, $U_k^H U \\Sigma V^H = (\\sum_{i=1}^k e_i \\sigma_i v_i^H)$, where $e_i$ are standard basis vectors. A simpler way is to note that $A_k^\\dagger A = V_k \\Sigma_k^{-1} U_k^H \\sum_{i=1}^r \\sigma_i u_i v_i^H = V_k \\Sigma_k^{-1} \\sum_{i=1}^k \\sigma_i (U_k^H u_i) v_i^H = V_k \\Sigma_k^{-1} \\sum_{i=1}^k \\sigma_i e_i v_i^H = V_k \\Sigma_k^{-1} \\Sigma_k V_k^H = V_k V_k^H$.\nThus, the expected value of the estimator is:\n$$E[x_k] = (V_k V_k^H) x$$\nThe matrix $P_k = V_k V_k^H$ is the orthogonal projector onto the subspace spanned by the first $k$ right singular vectors.\nThe bias vector is therefore:\n$$B(x_k) = E[x_k] - x = V_k V_k^H x - x = (V_k V_k^H - I_n) x$$\nThe squared bias is the squared Euclidean norm of this vector:\n$$\\|B(x_k)\\|_2^2 = \\|(V_k V_k^H - I_n) x\\|_2^2$$\nSince $(I_n - V_k V_k^H)$ is an orthogonal projector, this is equivalent to $\\|(I_n - V_k V_k^H)x\\|_2^2$. The vector $x$ can be expanded in the basis of right singular vectors $\\{v_i\\}_{i=1}^n$ as $x = \\sum_{i=1}^n (v_i^H x) v_i$. The projection $V_k V_k^H x = \\sum_{i=1}^k v_i v_i^H x = \\sum_{i=1}^k (v_i^H x) v_i$.\nThe bias vector is $(V_k V_k^H - I_n)x = -\\sum_{i=k+1}^n (v_i^H x) v_i$. Due to the orthonormality of the $\\{v_i\\}$, the squared norm is:\n$$\\|B(x_k)\\|_2^2 = \\sum_{i=k+1}^n |v_i^H x|^2$$\n\n### Derivation of the Variance\n\nThe variance of a vector estimator is the trace of its covariance matrix, $\\text{Var}(x_k) = \\text{Tr}(\\text{Cov}(x_k))$. The covariance matrix is $\\text{Cov}(x_k) = E[(x_k - E[x_k])(x_k - E[x_k])^H]$.\nFirst, we find the random component of the estimator:\n$$x_k - E[x_k] = V_k \\Sigma_k^{-1} U_k^H y - V_k \\Sigma_k^{-1} U_k^H (Ax) = V_k \\Sigma_k^{-1} U_k^H (y - Ax) = V_k \\Sigma_k^{-1} U_k^H e$$\nNow, we compute the covariance matrix:\n$$\\text{Cov}(x_k) = E[(V_k \\Sigma_k^{-1} U_k^H e) (V_k \\Sigma_k^{-1} U_k^H e)^H] = E[V_k \\Sigma_k^{-1} U_k^H e e^H U_k \\Sigma_k^{-1} V_k^H]$$\nMoving the deterministic matrices outside the expectation:\n$$\\text{Cov}(x_k) = V_k \\Sigma_k^{-1} U_k^H E[e e^H] U_k \\Sigma_k^{-1} V_k^H$$\nSubstituting $E[e e^H] = \\sigma^2 I_m$:\n$$\\text{Cov}(x_k) = V_k \\Sigma_k^{-1} U_k^H (\\sigma^2 I_m) U_k \\Sigma_k^{-1} V_k^H = \\sigma^2 V_k \\Sigma_k^{-1} (U_k^H U_k) \\Sigma_k^{-1} V_k^H$$\nSince $U_k$ has orthonormal columns, $U_k^H U_k = I_k$. The singular values are real, so $\\Sigma_k^{-1}$ is self-adjoint.\n$$\\text{Cov}(x_k) = \\sigma^2 V_k \\Sigma_k^{-1} I_k \\Sigma_k^{-1} V_k^H = \\sigma^2 V_k \\Sigma_k^{-2} V_k^H$$\nThe variance is the trace of this matrix. Using the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$:\n$$\\text{Var}(x_k) = \\text{Tr}(\\sigma^2 V_k \\Sigma_k^{-2} V_k^H) = \\sigma^2 \\text{Tr}(\\Sigma_k^{-2} V_k^H V_k)$$\nSince $V_k$ has orthonormal columns, $V_k^H V_k = I_k$.\n$$\\text{Var}(x_k) = \\sigma^2 \\text{Tr}(\\Sigma_k^{-2})$$\nAs $\\Sigma_k^{-2}$ is a diagonal matrix with entries $\\sigma_i^{-2}$ for $i=1, \\dots, k$, the trace is the sum of its diagonal elements:\n$$\\text{Var}(x_k) = \\sigma^2 \\sum_{i=1}^k \\frac{1}{\\sigma_i^2}$$\n\n### Derivation of the Expected Squared Error (MSE)\n\nThe expected squared error, or Mean Squared Error (MSE), is $E[\\|x_k - x\\|_2^2]$. It can be decomposed into bias and variance components:\n$$E[\\|x_k - x\\|_2^2] = E[\\|(x_k - E[x_k]) + (E[x_k] - x)\\|_2^2]$$\nLet the zero-mean random component be $\\tilde{x}_k = x_k - E[x_k]$ and the deterministic bias vector be $B_k = E[x_k] - x$.\n$$E[\\|\\tilde{x}_k + B_k\\|_2^2] = E[(\\tilde{x}_k + B_k)^H (\\tilde{x}_k + B_k)] = E[\\tilde{x}_k^H \\tilde{x}_k + \\tilde{x}_k^H B_k + B_k^H \\tilde{x}_k + B_k^H B_k]$$\nBy linearity of expectation, and since $E[\\tilde{x}_k] = 0$:\n$$E[\\|\\tilde{x}_k + B_k\\|_2^2] = E[\\tilde{x}_k^H \\tilde{x}_k] + E[\\tilde{x}_k^H]B_k + B_k^H E[\\tilde{x}_k] + B_k^H B_k = E[\\|\\tilde{x}_k\\|_2^2] + \\|B_k\\|_2^2$$\nThe first term is the expected norm of the zero-mean component, which is the variance:\n$$E[\\|\\tilde{x}_k\\|_2^2] = E[\\|x_k - E[x_k]\\|_2^2] = E[\\text{Tr}((x_k-E[x_k])(x_k-E[x_k])^H)] = \\text{Tr}(\\text{Cov}(x_k)) = \\text{Var}(x_k)$$\nThe second term is the squared norm of the bias vector.\nTherefore, the MSE is the sum of the variance and the squared bias:\n$$E[\\|x_k - x\\|_2^2] = \\text{Var}(x_k) + \\|B(x_k)\\|_2^2$$\nSubstituting the derived expressions:\n$$E[\\|x_k - x\\|_2^2] = \\sigma^2 \\sum_{i=1}^k \\frac{1}{\\sigma_i^2} + \\sum_{i=k+1}^n |v_i^H x|^2$$\nThis final expression reveals the fundamental bias-variance tradeoff. As the truncation level $k$ increases, the bias term (error from truncating the solution space) decreases, while the variance term (error from noise amplification) increases, particularly when small singular values $\\sigma_i$ are included in the inversion. The optimal $k^\\star$ balances these two opposing effects to minimize the total expected error.\nFor the special case $k=0$, the estimator is $x_0 = 0$. The variance sum is empty and thus $0$. The bias sum runs from $i=1$ to $n$, giving $\\sum_{i=1}^n |v_i^H x|^2 = \\|x\\|_2^2$. The MSE is $\\|x\\|_2^2$, which is correct since $E[\\|0-x\\|_2^2] = \\|x\\|_2^2$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import dft\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def _compute_metrics_for_case(A, x, sigma_sq):\n        \"\"\"\n        Computes the optimal truncation k and associated metrics for a single case.\n        \"\"\"\n        m, n = A.shape\n        \n        # Perform SVD\n        # full_matrices=True is important to get the full V matrix for n-dim space\n        U, s, Vh = np.linalg.svd(A, full_matrices=True)\n        \n        # Determine numerical rank r\n        # The problem defines k up to r, so we consider only non-zero singular values.\n        # A small tolerance is used for numerical stability.\n        # Although the problems are designed to have clear ranks, this is robust.\n        r = np.sum(s > 1e-12)\n\n        k_values = range(r + 1)\n        mses = []\n        biases = []\n        variances = []\n        \n        # Pre-compute coefficients |v_i^H x|^2\n        # Vh contains rows v_i^H.\n        v_coeffs_sq = np.abs(Vh @ x)**2\n\n        for k in k_values:\n            if k == 0:\n                # For k=0, estimator is x0=0. Variance is 0. Bias^2 is ||x||^2.\n                # bias_sq = np.sum_{i=1..n} |v_i^H x|^2\n                bias_sq = np.sum(v_coeffs_sq)\n                var = 0.0\n            else:\n                # Bias term: sum_{i=k+1..n} |v_i^H x|^2\n                # In 0-based indexing, this is sum over elements from index k onwards.\n                bias_sq = np.sum(v_coeffs_sq[k:])\n                \n                # Variance term: sigma^2 * sum_{i=1..k} 1/sigma_i^2\n                # s contains singular values, s[:k] are the first k values\n                var = sigma_sq * np.sum(1.0 / s[:k]**2)\n\n            mse = bias_sq + var\n            mses.append(mse)\n            biases.append(bias_sq)\n            variances.append(var)\n\n        # Find the optimal truncation level k_star (smallest k in case of a tie)\n        k_star = np.argmin(mses)\n        \n        # Get metrics for k_star\n        min_mse = mses[k_star]\n        bias_at_k_star = biases[k_star]\n        var_at_k_star = variances[k_star]\n\n        return k_star, min_mse, bias_at_k_star, var_at_k_star\n\n    # --- Test Cases ---\n    \n    # Case 1: diagonal, well-conditioned\n    A1 = np.diag([1.0, 0.9, 0.8, 0.7, 0.6])\n    x1 = np.array([1.0, -0.5, 0.7, -0.3, 0.2])\n    sigma_sq1 = 0.0001\n\n    # Case 2: diagonal, ill-conditioned\n    A2 = np.diag([5.0, 3.0, 1.5, 0.6, 0.3, 0.1])\n    x2 = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n    sigma_sq2 = 0.02\n\n    # Case 3: partial discrete Fourier transform, complex\n    F = dft(8, scale='sqrtn')\n    A3 = F[:6, :]\n    x3 = np.array([1.0 + 0.0j, 0.0 + 1.0j, -1.0 + 0.0j, 0.0 - 1.0j, \n                   0.5 + 0.0j, -0.5 + 0.0j, 0.0 + 0.25j, -0.25 + 0.0j])\n    sigma_sq3 = 0.1\n\n    test_cases = [\n        (A1, x1, sigma_sq1),\n        (A2, x2, sigma_sq2),\n        (A3, x3, sigma_sq3),\n    ]\n\n    all_results = []\n    for A, x, sigma_sq in test_cases:\n        k_star, mse, bias_sq, var = _compute_metrics_for_case(A, x, sigma_sq)\n        all_results.append(str(k_star))\n        all_results.append(f\"{mse:.6f}\")\n        all_results.append(f\"{bias_sq:.6f}\")\n        all_results.append(f\"{var:.6f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}