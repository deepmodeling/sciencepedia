{
    "hands_on_practices": [
        {
            "introduction": "Before we can reconstruct a SPECT image, we must first build a mathematical model of the image acquisition process itself. This is known as the \"forward problem\": if we know the distribution of the radiotracer in the body, can we predict the data our scanner will measure? This exercise  walks you through this fundamental step, using a simplified linear model to calculate the expected photon counts based on a known activity distribution, system geometry, and background noise. Understanding this forward model is the essential first step toward solving the \"inverse problem\" of image reconstruction.",
            "id": "4927199",
            "problem": "Consider Single Photon Emission Computed Tomography (SPECT), where the measured projection data are modeled as random counts whose expectation is determined by the system response to the underlying activity distribution. In a discrete two-voxel, two-bin setting, let the system matrix $A$ describe the contribution of each voxel to each projection bin, the activity vector $x$ encode the voxel activities, and the additive term $r$ represent mean background counts (including scatter and electronic noise). Adopt the standard forward expectation model for SPECT that combines linear system response and additive background at the level of expected counts. With \n$A=\\begin{bmatrix}0.5 & 0.2 \\\\ 0.3 & 0.4\\end{bmatrix}$, \n$x=\\begin{bmatrix}10 \\\\ 4\\end{bmatrix}$ (becquerel, Bq), and \n$r=\\begin{bmatrix}1 \\\\ 2\\end{bmatrix}$ (counts), compute the mean projection vector $\\bar{y}$ in counts. Then briefly interpret the contribution of each term to each entry of $\\bar{y}$ in terms of voxel contributions and background. Express your final numeric answer in counts and do not round.",
            "solution": "The user wants me to solve the problem by following the instructions.\n1. ### **Problem Validation**\nThe problem must be validated before a solution is attempted.\n\n#### **Step 1: Extract Givens**\n- **Topic**: Single Photon Emission Computed Tomography (SPECT) image reconstruction.\n- **Model**: A discrete two-voxel, two-bin setting. The forward expectation model for mean projection counts $\\bar{y}$ is given as a combination of linear system response and additive background.\n- **System Matrix**: $A = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix}$. The elements $A_{ij}$ represent the contribution of voxel $j$ to projection bin $i$.\n- **Activity Vector**: $x = \\begin{bmatrix} 10 \\\\ 4 \\end{bmatrix}$ (in units of becquerel, Bq).\n- **Mean Background Vector**: $r = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$ (in units of counts).\n- **Task**: Compute the mean projection vector $\\bar{y}$ (in counts) and interpret the contribution of each term.\n\n#### **Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically grounded. The model for the mean projection data, $\\bar{y} = Ax + r$, is a standard and fundamental linear approximation used in the field of medical imaging, particularly for SPECT and PET reconstruction. It correctly represents the physical process where photons from an activity distribution ($x$) are detected by a system (with response $A$) and are corrupted by additive background signals like scatter and random coincidences ($r$).\n- **Well-Posedness**: The problem is well-posed. It provides all necessary numerical data ($A$, $x$, $r$) and a clear definition of the model to be used. The calculation is a straightforward matrix-vector multiplication and addition, which yields a unique and meaningful solution.\n- **Objectivity**: The problem is stated objectively, using standard terminology from the field of medical imaging. All quantities are defined precisely.\n- **Completeness and Consistency**: The problem is self-contained. While there is a slight ambiguity in units (Bq for activity vs. counts for projections and background), it is standard in such simplified problems to assume that the system matrix $A$ incorporates any necessary conversion factors (e.g., acquisition time, detector efficiency, geometric factors) to ensure dimensional consistency. The model is therefore applied as given, with the result being in counts.\n- **Other Flaws**: The problem does not violate any other criteria. It is not unrealistic (for a didactic example), ill-posed, trivial, or unverifiable.\n\n#### **Step 3: Verdict and Action**\nThe problem is **valid**. A full solution will be provided.\n\n2. ### **Solution**\nThe problem describes the standard linear forward model for the expectation of SPECT projection data. The mean projection vector, $\\bar{y}$, is the sum of the expected counts originating from the tracer distribution within the patient and the expected counts from background sources. This relationship is expressed as:\n$$ \\bar{y} = Ax + r $$\nwhere $A$ is the system matrix, $x$ is the activity vector, and $r$ is the mean background vector.\n\nWe are given the following values:\nThe system matrix $A$:\n$$ A = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} $$\nThe activity vector $x$:\n$$ x = \\begin{bmatrix} 10 \\\\ 4 \\end{bmatrix} $$\nThe mean background vector $r$:\n$$ r = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$\n\nFirst, we compute the contribution from the activity distribution, which is the product $Ax$:\n$$ Ax = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix} \\begin{bmatrix} 10 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} (0.5)(10) + (0.2)(4) \\\\ (0.3)(10) + (0.4)(4) \\end{bmatrix} $$\n$$ Ax = \\begin{bmatrix} 5 + 0.8 \\\\ 3 + 1.6 \\end{bmatrix} = \\begin{bmatrix} 5.8 \\\\ 4.6 \\end{bmatrix} $$\nThis vector represents the mean counts detected in each bin originating from the two voxels, before considering background.\n\nNext, we add the mean background counts $r$ to find the total mean projection vector $\\bar{y}$:\n$$ \\bar{y} = Ax + r = \\begin{bmatrix} 5.8 \\\\ 4.6 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5.8 + 1 \\\\ 4.6 + 2 \\end{bmatrix} = \\begin{bmatrix} 6.8 \\\\ 6.6 \\end{bmatrix} $$\nThus, the mean projection vector is $\\bar{y} = \\begin{bmatrix} 6.8 \\\\ 6.6 \\end{bmatrix}$ counts.\n\nThe interpretation of the contributions to each entry of $\\bar{y}$ is as follows:\nLet the components of the mean projection vector be $\\bar{y} = \\begin{bmatrix} \\bar{y}_1 \\\\ \\bar{y}_2 \\end{bmatrix}$.\n\nFor the first projection bin, $\\bar{y}_1 = 6.8$ counts:\nThe total mean count is a sum of three terms:\n1.  Contribution from the first voxel (activity $x_1 = 10$) to the first bin: $A_{11}x_1 = 0.5 \\times 10 = 5.0$ counts.\n2.  Contribution from the second voxel (activity $x_2 = 4$) to the first bin: $A_{12}x_2 = 0.2 \\times 4 = 0.8$ counts.\n3.  Contribution from background to the first bin: $r_1 = 1$ count.\nThe sum is $\\bar{y}_1 = 5.0 + 0.8 + 1 = 6.8$ counts.\n\nFor the second projection bin, $\\bar{y}_2 = 6.6$ counts:\nThe total mean count is a sum of three terms:\n1.  Contribution from the first voxel (activity $x_1 = 10$) to the second bin: $A_{21}x_1 = 0.3 \\times 10 = 3.0$ counts.\n2.  Contribution from the second voxel (activity $x_2 = 4$) to the second bin: $A_{22}x_2 = 0.4 \\times 4 = 1.6$ counts.\n3.  Contribution from background to the second bin: $r_2 = 2$ counts.\nThe sum is $\\bar{y}_2 = 3.0 + 1.6 + 2 = 6.6$ counts.\nThe system matrix $A$ shows that the first voxel contributes more strongly to the first bin, while the second voxel contributes more strongly to the second bin, which is typical for a detector geometry where each bin is positioned closer to its corresponding voxel.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 6.8 \\\\ 6.6 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Image reconstruction aims to reverse the data acquisition process, turning projection data back into an image of the tracer distribution. A foundational operation in this process is backprojection, which essentially \"smears\" the detected counts from each projection bin back across the image space along the paths from which they could have originated. In this practice , you will compute the backprojection, mathematically represented by applying the transpose of the system matrix ($A^T$), to gain a practical understanding of this crucial component used in nearly all iterative reconstruction algorithms.",
            "id": "4927237",
            "problem": "A Single Photon Emission Computed Tomography (SPECT) acquisition can be modeled in discrete form by a linear system where each measured line integral is the sum of voxel activities weighted by the corresponding path lengths through those voxels. Consider a toy case with a one-dimensional, two-voxel activity image represented by a vector $\\boldsymbol{x} \\in \\mathbb{R}^{2}$, and two projection rays. The measured sinogram data are $\\boldsymbol{y} \\in \\mathbb{R}^{2}$ with $\\boldsymbol{y} = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}$, and the path-length system matrix is\n$$\nA = L = \\begin{bmatrix}\n1 & 0.5 \\\\\n0.2 & 1.2\n\\end{bmatrix}.\n$$\nAssume no attenuation, no scatter, and unit detection sensitivity, so that the discrete forward model is $\\boldsymbol{y} = A \\boldsymbol{x}$ in normalized units. The adjoint (backprojection) operator with respect to the standard Euclidean inner product is defined as the unique linear operator $A^{T}$ satisfying $\\langle A \\boldsymbol{x}, \\boldsymbol{z} \\rangle = \\langle \\boldsymbol{x}, A^{T} \\boldsymbol{z} \\rangle$ for all $\\boldsymbol{x}, \\boldsymbol{z} \\in \\mathbb{R}^{2}$. Using only the path-length weights in $A$, compute the backprojection of the data, namely $A^{T} \\boldsymbol{y}$.\n\nProvide your final answer as a two-component vector of exact real numbers. Do not round. Report the numerical values without units (assume normalized arbitrary units).",
            "solution": "The problem asks for the computation of the backprojection of a given sinogram data vector, $\\boldsymbol{y}$, using a specified system matrix, $A$. The context is a simplified discrete model of Single Photon Emission Computed Tomography (SPECT).\n\nThe forward model is given by the linear system of equations:\n$$\n\\boldsymbol{y} = A \\boldsymbol{x}\n$$\nwhere $\\boldsymbol{x} \\in \\mathbb{R}^{2}$ is the unknown activity distribution (the image), $\\boldsymbol{y} \\in \\mathbb{R}^{2}$ is the measured projection data (the sinogram), and $A \\in \\mathbb{R}^{2 \\times 2}$ is the system matrix that maps the image space to the projection space. This operation is known as the forward projection.\n\nThe problem provides the following specific values:\nThe system matrix, which represents the path lengths of the rays through the voxels, is:\n$$\nA = \\begin{bmatrix}\n1 & 0.5 \\\\\n0.2 & 1.2\n\\end{bmatrix}\n$$\nThe measured data vector is:\n$$\n\\boldsymbol{y} = \\begin{bmatrix}\n5 \\\\\n7\n\\end{bmatrix}\n$$\nThe backprojection operation is defined as the application of the adjoint operator of $A$ to the data vector $\\boldsymbol{y}$. The problem states that the adjoint operator, denoted $A^{T}$, is defined with respect to the standard Euclidean inner product $\\langle \\cdot, \\cdot \\rangle$. For any real matrix $A$, the unique operator satisfying $\\langle A \\boldsymbol{u}, \\boldsymbol{v} \\rangle = \\langle \\boldsymbol{u}, A^{T} \\boldsymbol{v} \\rangle$ for all vectors $\\boldsymbol{u}, \\boldsymbol{v}$ of appropriate dimensions is the matrix transpose of $A$. Therefore, the backprojection operator in this case is simply the transpose of the system matrix $A$.\n\nThe transpose of $A$, denoted $A^{T}$, is obtained by interchanging the rows and columns of $A$:\n$$\nA^{T} = \\begin{bmatrix}\n1 & 0.2 \\\\\n0.5 & 1.2\n\\end{bmatrix}\n$$\nThe backprojection is the computation of the vector $A^{T} \\boldsymbol{y}$. This operation maps the data from the projection space back into the image space. It is a fundamental step in many iterative reconstruction algorithms, such as filtered backprojection or MLEM (Maximum Likelihood Expectation Maximization).\n\nWe now compute the product $A^{T} \\boldsymbol{y}$:\n$$\nA^{T} \\boldsymbol{y} = \\begin{bmatrix}\n1 & 0.2 \\\\\n0.5 & 1.2\n\\end{bmatrix}\n\\begin{bmatrix}\n5 \\\\\n7\n\\end{bmatrix}\n$$\nThe resulting vector has two components. The first component is calculated as:\n$$\n(A^{T} \\boldsymbol{y})_1 = (1 \\times 5) + (0.2 \\times 7) = 5 + 1.4 = 6.4\n$$\nThe second component is calculated as:\n$$\n(A^{T} \\boldsymbol{y})_2 = (0.5 \\times 5) + (1.2 \\times 7) = 2.5 + 8.4 = 10.9\n$$\nThus, the backprojection of the data $\\boldsymbol{y}$ is the vector:\n$$\nA^{T} \\boldsymbol{y} = \\begin{bmatrix}\n6.4 \\\\\n10.9\n\\end{bmatrix}\n$$\nThese values are exact real numbers as required.",
            "answer": "$$\\boxed{\\begin{pmatrix} 6.4 \\\\ 10.9 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While backprojection provides a rough initial image, a more sophisticated approach is needed to achieve an accurate reconstruction that honors the statistical nature of photon counting. The Maximum-Likelihood Expectation-Maximization (ML-EM) algorithm is a cornerstone iterative method that accomplishes this by refining an image estimate to best match the measured data according to the Poisson statistics of radioactive decay. This exercise  guides you through a single, complete iteration of the ML-EM algorithm, demonstrating how the forward projection and a weighted backprojection are combined to update and improve the image estimate.",
            "id": "4927224",
            "problem": "In Single Photon Emission Computed Tomography (SPECT), a standard binned projection model assumes that each detector bin count $y_i$ is a realization of a Poisson random variable with mean $\\lambda_i = \\sum_{j} A_{ij} x_j + r_i$, where $A_{ij}$ is a known system matrix encoding the probability that an emission from voxel $j$ is detected in bin $i$, $x_j \\ge 0$ is the unknown activity in voxel $j$, and $r_i \\ge 0$ is the known mean background count in bin $i$. Under this model, the maximum likelihood estimate can be sought with the Expectation-Maximization framework, yielding a multiplicative fixed-point iteration that preserves nonnegativity.\n\nStarting from the Poisson data model, the definition of the log-likelihood for independent Poisson measurements, and the Expectation-Maximization construction with appropriate latent variables, derive the fixed-point multiplicative update appropriate for Maximum-Likelihood Expectation-Maximization (ML-EM) under known $A$ and $r$. Then, using your derived update, perform exactly one iteration starting from the given initialization to compute $x^{1}$ for the following numerically specified SPECT system:\n$$\nA=\\begin{bmatrix}0.4 & 0.2\\\\ 0.1 & 0.5\\end{bmatrix},\\quad\ny=\\begin{bmatrix}20\\\\ 10\\end{bmatrix},\\quad\nr=\\begin{bmatrix}0\\\\ 0\\end{bmatrix},\\quad\nx^{0}=\\begin{bmatrix}5\\\\ 5\\end{bmatrix}.\n$$\nReport the two components of $x^{1}$ as exact values in counts (no rounding). Express the final numerical result for $x^{1}$ in counts.",
            "solution": "The problem is valid. It is a standard question in medical image reconstruction that is scientifically grounded, well-posed, and contains all necessary information for a unique solution.\n\nThe problem is divided into two parts: first, to derive the Maximum-Likelihood Expectation-Maximization (ML-EM) update rule for SPECT reconstruction, and second, to apply this rule for one iteration with given numerical data.\n\n### Part 1: Derivation of the ML-EM Update Rule\n\nThe problem states the statistical model for the measured projection data. For each detector bin $i$, the count $y_i$ is a realization of an independent Poisson random variable with mean $\\lambda_i$. The mean is given by the linear model:\n$$\n\\lambda_i = \\sum_{j=1}^{N} A_{ij} x_j + r_i\n$$\nwhere $x_j$ is the unknown activity in voxel $j$ ($j=1, \\dots, N$), $A_{ij}$ is the system matrix element representing the probability that an emission from voxel $j$ is detected in bin $i$, and $r_i$ is the known mean of background events (e.g., scatter) in bin $i$. The goal is to find the activity vector $x = (x_1, \\dots, x_N)^T$ that maximizes the likelihood of observing the measurement vector $y = (y_1, \\dots, y_M)^T$.\n\nThe log-likelihood function $L(x)$ for independent Poisson measurements is:\n$$\nL(x) = \\ln P(y|x) = \\sum_{i=1}^{M} \\ln \\left( \\frac{\\exp(-\\lambda_i) \\lambda_i^{y_i}}{y_i!} \\right) = \\sum_{i=1}^{M} \\left( -\\lambda_i + y_i \\ln(\\lambda_i) - \\ln(y_i!) \\right)\n$$\nSubstituting the expression for $\\lambda_i$ and dropping the term $\\ln(y_i!)$ which is constant with respect to $x$:\n$$\nL(x) \\propto \\sum_{i=1}^{M} \\left( - \\left(\\sum_{j=1}^{N} A_{ij} x_j + r_i\\right) + y_i \\ln\\left(\\sum_{j=1}^{N} A_{ij} x_j + r_i\\right) \\right)\n$$\nDirect maximization of this function is difficult due to the logarithm of a sum. The Expectation-Maximization (EM) algorithm circumvents this by introducing latent variables, which constitute the \"complete data\".\n\nLet the complete data be the set of variables $\\{z_{ij}\\}$, where $z_{ij}$ is the number of photons originating from voxel $j$ and detected in bin $i$. For the background, we can think of it as originating from a separate source, contributing $w_i$ counts to bin $i$.\nThe observed, or \"incomplete\", data $y_i$ is the sum of these contributions:\n$$\ny_i = \\sum_{j=1}^{N} z_{ij} + w_i\n$$\nAccording to the Poisson model, $z_{ij}$ and $w_i$ are independent Poisson random variables with means $E[z_{ij}] = A_{ij} x_j$ and $E[w_i] = r_i$. The sum of independent Poisson variables is also Poisson, so $y_i$ is Poisson with mean $\\sum_j E[z_{ij}] + E[w_i] = \\sum_j A_{ij} x_j + r_i$, which is consistent with the problem statement.\n\nThe log-likelihood of the complete data $\\{z_{ij}, w_i\\}$, given $x$, is:\n$$\nL_c(x) = \\ln P(\\{z_{ij}, w_i\\}|x) = \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\ln P(z_{ij}|x) + \\sum_{i=1}^{M} \\ln P(w_i)\n$$\nAs the term for $w_i$ does not depend on $x$, it can be ignored during maximization.\n$$\nL_c(x) \\propto \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\left( -A_{ij}x_j + z_{ij}\\ln(A_{ij}x_j) - \\ln(z_{ij}!) \\right)\n$$\nThe EM algorithm proceeds in two steps:\n\n**E-Step (Expectation):**\nWe compute the expectation of the complete-data log-likelihood, conditioned on the observed data $y$ and the current estimate of the parameters $x^{(k)}$. This is the function $Q(x | x^{(k)})$:\n$$\nQ(x | x^{(k)}) = E[L_c(x) | y, x^{(k)}] = \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\left( -A_{ij}x_j + E[z_{ij}|y, x^{(k)}]\\ln(A_{ij}x_j) \\right) + \\text{const.}\n$$\nThe key is to compute the conditional expectation $E[z_{ij}|y, x^{(k)}]$. Given that the sum of independent Poisson variables $\\{z_{i1}, \\dots, z_{iN}, w_i\\}$ is $y_i$, their joint distribution conditioned on the sum is multinomial. The expectation of any one component, say $z_{ij}$, is the total count $y_i$ multiplied by the ratio of its mean to the sum of all means:\n$$\nE[z_{ij}|y_i, x^{(k)}] = y_i \\frac{E[z_{ij}|x^{(k)}]}{\\sum_{l=1}^{N}E[z_{il}|x^{(k)}] + E[w_i]} = y_i \\frac{A_{ij} x_j^{(k)}}{\\sum_{l=1}^{N} A_{il} x_l^{(k)} + r_i}\n$$\nLet's denote $\\hat{z}_{ij}^{(k)} = E[z_{ij}|y, x^{(k)}]$.\n\n**M-Step (Maximization):**\nWe find the next estimate $x^{(k+1)}$ by maximizing $Q(x | x^{(k)})$ with respect to $x$.\n$$\nx^{(k+1)} = \\arg\\max_{x \\ge 0} Q(x | x^{(k)})\n$$\nWe take the partial derivative of $Q(x|x^{(k)})$ with respect to each $x_j$ and set it to zero.\n$$\n\\frac{\\partial Q(x|x^{(k)})}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\sum_{i=1}^{M} \\sum_{l=1}^{N} \\left( -A_{il}x_l + \\hat{z}_{il}^{(k)}\\ln(A_{il}x_l) \\right)\n$$\nThe derivative is non-zero only for terms where $l=j$:\n$$\n\\frac{\\partial Q}{\\partial x_j} = \\sum_{i=1}^{M} \\left( -A_{ij} + \\frac{\\hat{z}_{ij}^{(k)}}{x_j} \\right)\n$$\nSetting this to $0$:\n$$\n\\sum_{i=1}^{M} \\frac{\\hat{z}_{ij}^{(k)}}{x_j} = \\sum_{i=1}^{M} A_{ij} \\implies x_j = \\frac{\\sum_{i=1}^{M} \\hat{z}_{ij}^{(k)}}{\\sum_{i=1}^{M} A_{ij}}\n$$\nThis gives the update rule for $x_j^{(k+1)}$:\n$$\nx_j^{(k+1)} = \\frac{1}{\\sum_{i=1}^{M} A_{ij}} \\sum_{i=1}^{M} \\left( y_i \\frac{A_{ij} x_j^{(k)}}{\\sum_{l=1}^{N} A_{il} x_l^{(k)} + r_i} \\right)\n$$\nThis can be rearranged into the standard multiplicative form:\n$$\nx_j^{(k+1)} = x_j^{(k)} \\frac{\\sum_{i=1}^{M} A_{ij} \\frac{y_i}{\\sum_{l=1}^{N} A_{il} x_l^{(k)} + r_i}}{\\sum_{i=1}^{M} A_{ij}}\n$$\nThis is the fixed-point multiplicative update rule for ML-EM. The term $\\sum_{i=1}^{M} A_{ij}$ is often denoted as the sensitivity $s_j$ of voxel $j$.\n\n### Part 2: Numerical Calculation for One Iteration\n\nWe are given the following numerical values:\n$$\nA=\\begin{bmatrix}0.4 & 0.2\\\\ 0.1 & 0.5\\end{bmatrix},\\quad\ny=\\begin{bmatrix}20\\\\ 10\\end{bmatrix},\\quad\nr=\\begin{bmatrix}0\\\\ 0\\end{bmatrix},\\quad\nx^{(0)}=\\begin{bmatrix}5\\\\ 5\\end{bmatrix}\n$$\nWe want to compute $x^{(1)} = \\begin{bmatrix} x_1^{(1)} \\\\ x_2^{(1)} \\end{bmatrix}$.\n\nThe number of voxels is $N=2$ and the number of detector bins is $M=2$. The update rule for $j=1, 2$ is:\n$$\nx_j^{(1)} = x_j^{(0)} \\frac{\\sum_{i=1}^{2} A_{ij} \\frac{y_i}{\\sum_{l=1}^{2} A_{il} x_l^{(0)} + r_i}}{\\sum_{i=1}^{2} A_{ij}}\n$$\nWith $r=0$, this simplifies to:\n$$\nx_j^{(1)} = x_j^{(0)} \\frac{\\sum_{i=1}^{2} A_{ij} \\frac{y_i}{(Ax^{(0)})_i}}{\\sum_{i=1}^{2} A_{ij}}\n$$\n\n**1. Calculate the forward projection $\\lambda^{(0)} = Ax^{(0)}$:**\n$$\n\\lambda^{(0)} = \\begin{bmatrix}0.4 & 0.2\\\\ 0.1 & 0.5\\end{bmatrix} \\begin{bmatrix}5\\\\ 5\\end{bmatrix} = \\begin{bmatrix}0.4(5) + 0.2(5) \\\\ 0.1(5) + 0.5(5)\\end{bmatrix} = \\begin{bmatrix}2 + 1 \\\\ 0.5 + 2.5\\end{bmatrix} = \\begin{bmatrix}3\\\\ 3\\end{bmatrix}\n$$\n\n**2. Calculate the correction ratios $y_i / \\lambda_i^{(0)}$:**\n$$\n\\frac{y_1}{\\lambda_1^{(0)}} = \\frac{20}{3}\n$$\n$$\n\\frac{y_2}{\\lambda_2^{(0)}} = \\frac{10}{3}\n$$\n\n**3. Calculate the back-projection term in the numerator of the update factor:**\nThis is the term $b_j = \\sum_{i=1}^{2} A_{ij} \\frac{y_i}{\\lambda_i^{(0)}}$. In matrix form, this is $A^T (y./\\lambda^{(0)})$.\nFor $j=1$:\n$$\nb_1 = A_{11}\\frac{y_1}{\\lambda_1^{(0)}} + A_{21}\\frac{y_2}{\\lambda_2^{(0)}} = (0.4)\\left(\\frac{20}{3}\\right) + (0.1)\\left(\\frac{10}{3}\\right) = \\frac{8}{3} + \\frac{1}{3} = \\frac{9}{3} = 3\n$$\nFor $j=2$:\n$$\nb_2 = A_{12}\\frac{y_1}{\\lambda_1^{(0)}} + A_{22}\\frac{y_2}{\\lambda_2^{(0)}} = (0.2)\\left(\\frac{20}{3}\\right) + (0.5)\\left(\\frac{10}{3}\\right) = \\frac{4}{3} + \\frac{5}{3} = \\frac{9}{3} = 3\n$$\n\n**4. Calculate the sensitivities $s_j = \\sum_{i=1}^{2} A_{ij}$:**\nFor $j=1$:\n$$\ns_1 = A_{11} + A_{21} = 0.4 + 0.1 = 0.5\n$$\nFor $j=2$:\n$$\ns_2 = A_{12} + A_{22} = 0.2 + 0.5 = 0.7\n$$\n\n**5. Apply the multiplicative update to find $x^{(1)}$:**\n$$\nx_j^{(1)} = x_j^{(0)} \\frac{b_j}{s_j}\n$$\nFor $j=1$:\n$$\nx_1^{(1)} = x_1^{(0)} \\frac{b_1}{s_1} = 5 \\cdot \\frac{3}{0.5} = 5 \\cdot 6 = 30\n$$\nFor $j=2$:\n$$\nx_2^{(1)} = x_2^{(0)} \\frac{b_2}{s_2} = 5 \\cdot \\frac{3}{0.7} = \\frac{15}{7/10} = \\frac{150}{7}\n$$\n\nThus, the updated activity vector after one iteration is $x^{(1)} = \\begin{bmatrix} 30 \\\\ 150/7 \\end{bmatrix}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 30 \\\\ \\frac{150}{7} \\end{pmatrix}}\n$$"
        }
    ]
}