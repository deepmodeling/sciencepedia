{
    "hands_on_practices": [
        {
            "introduction": "The quality of a CT reconstruction depends critically on how we model measurement uncertainty. Since X-ray detection is a quantum process, the photon counts at the detector follow Poisson statistics, meaning that measurements with fewer photons are inherently noisier. This practice  guides you through deriving the statistical weights from this physical principle, leading to a Weighted Least Squares (WLS) formulation that more accurately reflects the data fidelity.",
            "id": "4895909",
            "problem": "A parallel-beam transmission Computed Tomography (CT) system measures photon counts at the detector. Assume each ray measurement is a random variable following Poisson statistics due to quantum noise. Let the incident photon flux be $I_0$ photons per detector integration, and for a uniform slab with constant linear attenuation coefficient $\\mu$ and ray path length $l$, the Beer–Lambert law gives the mean transmitted flux as $I_0 \\exp(-l \\mu)$. The measured detector count $n$ on a ray is modeled as a Poisson random variable with mean $I_0 \\exp(-l \\mu)$. Consider the log-transformed measurement $y = -\\ln\\!\\big(n/I_0\\big)$.\n\nStarting from these foundations:\n- the Beer–Lambert law, and\n- the Poisson statistics of photon counting with mean equal to the transmitted flux,\n\nperform the following tasks:\n1. Using a first-order approximation for the transformation $y = -\\ln\\!\\big(n/I_0\\big)$ about the mean count, derive an expression for the approximate variance of $y$ in terms of the Poisson mean, and identify the optimal weighting for a Weighted Least Squares (WLS) formulation that models $y$ as a noisy linear observation of $l \\mu$.\n2. For a uniform $\\mu$ and a set of independent rays indexed by $i$ with known path lengths $l_i$, log-transformed measurements $y_i$, and weights derived from part 1, derive the closed-form WLS estimator for $\\mu$.\n3. Using the following data from three independent rays, compute the numerical value of the WLS estimate of $\\mu$:\n   - Incident flux $I_0$ is the same for all rays and equals $5.0 \\times 10^{5}$ photons per detector integration.\n   - Path lengths are $l_1 = 30$ mm, $l_2 = 50$ mm, and $l_3 = 80$ mm.\n   - Measured counts are $n_1 = 450000$, $n_2 = 420000$, and $n_3 = 380000$.\n   - Use the log-transform $y_i = -\\ln\\!\\big(n_i/I_0\\big)$ and the plug-in approximation that replaces the Poisson mean in the weights by the measured counts.\n\nExpress the final numerical estimate of $\\mu$ in $\\mathrm{mm}^{-1}$ and round your answer to four significant figures.",
            "solution": "**1. Approximate Variance of $y$ and Optimal WLS Weight**\n\nThe log-transformed measurement is given by $y = g(n) = -\\ln(n/I_0)$, where $n$ is a Poisson random variable with mean $\\lambda = \\mathbb{E}[n] = I_0 \\exp(-l \\mu)$. The variance of a Poisson random variable is equal to its mean, so $\\mathrm{Var}(n) = \\lambda$.\n\nTo find the approximate variance of the transformed variable $y$, we use a first-order Taylor series expansion of $g(n)$ around the mean $\\lambda$. This is commonly known as the delta method. The variance is approximated as:\n$$\n\\mathrm{Var}(y) = \\mathrm{Var}(g(n)) \\approx \\left[ g'(\\mathbb{E}[n]) \\right]^2 \\mathrm{Var}(n)\n$$\nFirst, we find the derivative of $g(n)$ with respect to $n$:\n$$\ng(n) = -\\ln(n) + \\ln(I_0)\n$$\n$$\ng'(n) = \\frac{d}{dn} \\left( -\\ln(n) + \\ln(I_0) \\right) = -\\frac{1}{n}\n$$\nNext, we evaluate this derivative at the mean of $n$, which is $\\lambda$:\n$$\ng'(\\lambda) = -\\frac{1}{\\lambda}\n$$\nSubstituting this into the variance approximation formula, along with $\\mathrm{Var}(n) = \\lambda$:\n$$\n\\mathrm{Var}(y) \\approx \\left( -\\frac{1}{\\lambda} \\right)^2 \\lambda = \\frac{1}{\\lambda^2} \\lambda = \\frac{1}{\\lambda}\n$$\nSubstituting the expression for $\\lambda$:\n$$\n\\mathrm{Var}(y) \\approx \\frac{1}{I_0 \\exp(-l \\mu)} = \\frac{\\exp(l \\mu)}{I_0}\n$$\nIn a Weighted Least Squares (WLS) regression, the optimal weights are the reciprocal of the measurement variances. The model is $y \\approx l\\mu$, which is a noisy linear observation of the sinogram value $p = l\\mu$. The weight $w$ for the measurement $y$ is:\n$$\nw = \\frac{1}{\\mathrm{Var}(y)} \\approx \\lambda = I_0 \\exp(-l \\mu)\n$$\nThus, the approximate variance of $y$ is $1/ \\lambda$, and the optimal WLS weight is $\\lambda$.\n\n**2. Closed-Form WLS Estimator for $\\mu$**\n\nWe have a set of independent measurements $\\{y_i\\}$ for $i=1, 2, ..., N$. The model for each measurement is:\n$$\ny_i \\approx l_i \\mu\n$$\nThis can be written as a linear model $y_i = l_i \\mu + \\epsilon_i$, where $\\epsilon_i$ is a zero-mean noise term with variance $\\mathrm{Var}(y_i) \\approx 1/w_i$. The WLS estimator for $\\mu$, denoted $\\hat{\\mu}$, is the value that minimizes the weighted sum of squared residuals, $S(\\mu)$:\n$$\nS(\\mu) = \\sum_{i=1}^{N} w_i (y_i - l_i \\mu)^2\n$$\nTo find the minimum, we differentiate $S(\\mu)$ with respect to $\\mu$ and set the result to zero:\n$$\n\\frac{dS}{d\\mu} = \\sum_{i=1}^{N} w_i \\cdot 2(y_i - l_i \\mu) \\cdot (-l_i) = -2 \\sum_{i=1}^{N} w_i l_i (y_i - l_i \\mu)\n$$\nSetting $\\frac{dS}{d\\mu} = 0$:\n$$\n\\sum_{i=1}^{N} w_i l_i (y_i - l_i \\mu) = 0\n$$\n$$\n\\sum_{i=1}^{N} w_i l_i y_i - \\sum_{i=1}^{N} w_i l_i^2 \\mu = 0\n$$\nSolving for $\\mu$:\n$$\n\\mu \\left( \\sum_{i=1}^{N} w_i l_i^2 \\right) = \\sum_{i=1}^{N} w_i l_i y_i\n$$\nThe closed-form WLS estimator for $\\mu$ is therefore:\n$$\n\\hat{\\mu}_{\\text{WLS}} = \\frac{\\sum_{i=1}^{N} w_i l_i y_i}{\\sum_{i=1}^{N} w_i l_i^2}\n$$\nAs derived in Part 1, the optimal weights are $w_i = \\lambda_i = I_0 \\exp(-l_i \\mu)$. Since these weights depend on the unknown parameter $\\mu$, a practical solution involves a \"plug-in\" approximation, where the unknown mean count $\\lambda_i$ is replaced by its measured value $n_i$. The problem statement explicitly requires this approximation, so we use $w_i = n_i$.\n\n**3. Numerical Computation of the WLS Estimate for $\\mu$**\n\nWe are given the following data for $N=3$ rays:\n- $I_0 = 5.0 \\times 10^5$\n- $l_1 = 30$ mm, $l_2 = 50$ mm, $l_3 = 80$ mm\n- $n_1 = 450000$, $n_2 = 420000$, $n_3 = 380000$\n\nFirst, we calculate the log-transformed measurements $y_i = -\\ln(n_i/I_0)$:\n$$\ny_1 = -\\ln\\left(\\frac{450000}{500000}\\right) = -\\ln(0.9) \\approx 0.1053605\n$$\n$$\ny_2 = -\\ln\\left(\\frac{420000}{500000}\\right) = -\\ln(0.84) \\approx 0.1743534\n$$\n$$\ny_3 = -\\ln\\left(\\frac{380000}{500000}\\right) = -\\ln(0.76) \\approx 0.2744368\n$$\nWe use the plug-in weights $w_i = n_i$:\n- $w_1 = 450000$\n- $w_2 = 420000$\n- $w_3 = 380000$\n\nNow, we compute the numerator and denominator of the WLS estimator formula:\n$$\n\\hat{\\mu} = \\frac{n_1 l_1 y_1 + n_2 l_2 y_2 + n_3 l_3 y_3}{n_1 l_1^2 + n_2 l_2^2 + n_3 l_3^2}\n$$\n\nNumerator: $\\sum_{i=1}^{3} n_i l_i y_i$\n$$\n(450000)(30)(0.1053605) \\approx 1422366.75\n$$\n$$\n(420000)(50)(0.1743534) \\approx 3661421.4\n$$\n$$\n(380000)(80)(0.2744368) \\approx 8342858.88\n$$\nNumerator Sum $\\approx 1422366.75 + 3661421.4 + 8342858.88 = 13426647.03$\n\nDenominator: $\\sum_{i=1}^{3} n_i l_i^2$\n$$\n(450000)(30^2) = (450000)(900) = 405,000,000\n$$\n$$\n(420000)(50^2) = (420000)(2500) = 1,050,000,000\n$$\n$$\n(380000)(80^2) = (380000)(6400) = 2,432,000,000\n$$\nDenominator Sum = $405,000,000 + 1,050,000,000 + 2,432,000,000 = 3,887,000,000$\n\nFinally, we compute the estimate for $\\hat{\\mu}$:\n$$\n\\hat{\\mu} = \\frac{13426647.03}{3887000000} \\approx 0.003454244... \\; \\mathrm{mm}^{-1}\n$$\nRounding the result to four significant figures, we get:\n$$\n\\hat{\\mu} \\approx 0.003454 \\; \\mathrm{mm}^{-1}\n$$",
            "answer": "$$\n\\boxed{0.003454}\n$$"
        },
        {
            "introduction": "Solving the least-squares problem directly can be problematic, as small amounts of noise in the measurements can lead to large, unphysical artifacts in the reconstructed image. This exercise  introduces Tikhonov regularization, a foundational method for stabilizing these ill-posed inverse problems. By adding a penalty on the solution's roughness, you will compute a reconstruction that balances data consistency with a plausible image structure.",
            "id": "4895893",
            "problem": "Consider a simplified parallel-beam Computed Tomography (CT) forward model with two unknown, spatially uniform pixel attenuation coefficients $x \\in \\mathbb{R}^{2}$ representing a $2 \\times 1$ object with column pixels. Three rays are measured: ray $1$ traverses only pixel $1$ for a path length of $1\\,\\mathrm{cm}$, ray $2$ traverses only pixel $2$ for a path length of $1\\,\\mathrm{cm}$, and ray $3$ traverses both pixels, each for a path length of $1\\,\\mathrm{cm}$. After logarithmic conversion of detected photon counts to line integrals, the measured data vector is $b \\in \\mathbb{R}^{3}$ with components $b_{1} = 0.10$, $b_{2} = 0.20$, and $b_{3} = 0.25$. The system matrix $A \\in \\mathbb{R}^{3 \\times 2}$ corresponds to these path lengths and is given implicitly by the geometry described above. Assume independent and identically distributed Gaussian noise across rays so the weighting matrix for Weighted Least Squares (WLS) is $W = I_{3}$. To stabilize the reconstruction, apply quadratic Tikhonov regularization with a first-order finite-difference operator $L \\in \\mathbb{R}^{1 \\times 2}$ defined by $L = \\begin{bmatrix}1 & -1\\end{bmatrix}$ and regularization parameter $\\lambda = 0.50$.\n\nStarting from the linear forward model $b = A x + n$ and the WLS Tikhonov objective\n$$\nJ(x) = \\frac{1}{2}\\,\\|W^{1/2}(A x - b)\\|_{2}^{2} + \\frac{\\lambda}{2}\\,\\|L x\\|_{2}^{2},\n$$\nderive the condition that the minimizer must satisfy, then compute the minimizer $x^{\\star}$ using the provided $A$, $b$, $W$, $L$, and $\\lambda$. Express the final reconstructed attenuation coefficients in reciprocal centimeters ($\\mathrm{cm}^{-1}$). Provide your final answer as a single row vector in the $\\begin{pmatrix}\\cdot & \\cdot\\end{pmatrix}$ format. No rounding is required; report the exact values.",
            "solution": "The objective is to find the vector $x \\in \\mathbb{R}^2$ that minimizes the Tikhonov-regularized weighted least squares objective function:\n$$\nJ(x) = \\frac{1}{2}\\,\\|W^{1/2}(A x - b)\\|_{2}^{2} + \\frac{\\lambda}{2}\\,\\|L x\\|_{2}^{2}\n$$\nThe squared $L_2$-norm can be expressed using the transpose: $\\|v\\|_2^2 = v^T v$.\n$$\nJ(x) = \\frac{1}{2}\\,(A x - b)^T W (A x - b) + \\frac{\\lambda}{2}\\,(L x)^T (L x)\n$$\nExpanding the terms, we get:\n$$\nJ(x) = \\frac{1}{2}\\,(x^T A^T - b^T) W (A x - b) + \\frac{\\lambda}{2}\\, x^T L^T L x\n$$\n$$\nJ(x) = \\frac{1}{2}\\,(x^T A^T W A x - x^T A^T W b - b^T W A x + b^T W b) + \\frac{\\lambda}{2}\\, x^T L^T L x\n$$\nSince $x^T A^T W b$ is a scalar, it equals its transpose, $(x^T A^T W b)^T = b^T W^T A x$. As $W$ is a weighting matrix, it is symmetric ($W^T = W$), so $b^T W A x = x^T A^T W b$.\n$$\nJ(x) = \\frac{1}{2}\\,x^T (A^T W A) x - x^T (A^T W b) + \\frac{1}{2}\\,b^T W b + \\frac{1}{2}\\, x^T (\\lambda L^T L) x\n$$\nThis is a quadratic function of $x$. To find the minimum, we compute the gradient of $J(x)$ with respect to $x$ and set it to the zero vector.\n$$\n\\nabla_{x} J(x) = \\frac{1}{2}\\,(2 A^T W A x) - A^T W b + \\frac{1}{2}\\,(2 \\lambda L^T L x)\n$$\n$$\n\\nabla_{x} J(x) = A^T W A x - A^T W b + \\lambda L^T L x\n$$\nSetting the gradient to zero gives the condition for the minimizer $x^{\\star}$:\n$$\nA^T W A x^{\\star} + \\lambda L^T L x^{\\star} = A^T W b\n$$\nFactoring out $x^{\\star}$, we get the normal equations for Tikhonov-regularized WLS:\n$$\n(A^T W A + \\lambda L^T L) x^{\\star} = A^T W b\n$$\nThe solution is thus:\n$$\nx^{\\star} = (A^T W A + \\lambda L^T L)^{-1} A^T W b\n$$\n\nNow, we substitute the given values. First, we construct the system matrix $A$ from the provided geometry for $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$:\n-   Ray 1: $1 \\cdot x_1 + 0 \\cdot x_2 = b_1$. This gives the first row of $A$ as $\\begin{bmatrix} 1 & 0 \\end{bmatrix}$.\n-   Ray 2: $0 \\cdot x_1 + 1 \\cdot x_2 = b_2$. This gives the second row as $\\begin{bmatrix} 0 & 1 \\end{bmatrix}$.\n-   Ray 3: $1 \\cdot x_1 + 1 \\cdot x_2 = b_3$. This gives the third row as $\\begin{bmatrix} 1 & 1 \\end{bmatrix}$.\nSo, the system matrix is:\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\nThe data vector is $b = \\begin{pmatrix} 0.10 \\\\ 0.20 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{5} \\\\ \\frac{1}{4} \\end{pmatrix}$.\nThe weighting matrix is $W = I_3$, so the formula simplifies to:\n$$\nx^{\\star} = (A^T A + \\lambda L^T L)^{-1} A^T b\n$$\nWe compute the required matrices:\n$A^T = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$\n$A^T A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+0+1 & 0+0+1 \\\\ 0+0+1 & 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n$L = \\begin{bmatrix} 1 & -1 \\end{bmatrix}$, so $L^T = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n$L^T L = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{bmatrix} 1 & -1 \\end{bmatrix} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$\nThe regularization parameter is $\\lambda = 0.50 = \\frac{1}{2}$.\nNow, we compute the term $(A^T A + \\lambda L^T L)$:\n$$\nA^T A + \\lambda L^T L = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{5}{2} \\end{pmatrix}\n$$\nNext, we compute its inverse. The determinant is $\\det(\\cdot) = (\\frac{5}{2})(\\frac{5}{2}) - (\\frac{1}{2})(\\frac{1}{2}) = \\frac{25}{4} - \\frac{1}{4} = \\frac{24}{4} = 6$.\nThe inverse is:\n$$\n(A^T A + \\lambda L^T L)^{-1} = \\frac{1}{6} \\begin{pmatrix} \\frac{5}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{12} & -\\frac{1}{12} \\\\ -\\frac{1}{12} & \\frac{5}{12} \\end{pmatrix}\n$$\nNext, we compute the term $A^T b$:\n$$\nA^T b = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{5} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1(\\frac{1}{10}) + 0(\\frac{1}{5}) + 1(\\frac{1}{4}) \\\\ 0(\\frac{1}{10}) + 1(\\frac{1}{5}) + 1(\\frac{1}{4}) \\end{pmatrix} = \\begin{pmatrix} \\frac{2+5}{20} \\\\ \\frac{4+5}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{20} \\\\ \\frac{9}{20} \\end{pmatrix}\n$$\nFinally, we compute the solution $x^{\\star}$:\n$$\nx^{\\star} = (A^T A + \\lambda L^T L)^{-1} (A^T b) = \\begin{pmatrix} \\frac{5}{12} & -\\frac{1}{12} \\\\ -\\frac{1}{12} & \\frac{5}{12} \\end{pmatrix} \\begin{pmatrix} \\frac{7}{20} \\\\ \\frac{9}{20} \\end{pmatrix}\n$$\n$$\nx^{\\star} = \\begin{pmatrix} \\frac{5}{12} \\cdot \\frac{7}{20} - \\frac{1}{12} \\cdot \\frac{9}{20} \\\\ -\\frac{1}{12} \\cdot \\frac{7}{20} + \\frac{5}{12} \\cdot \\frac{9}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{35 - 9}{240} \\\\ \\frac{-7 + 45}{240} \\end{pmatrix} = \\begin{pmatrix} \\frac{26}{240} \\\\ \\frac{38}{240} \\end{pmatrix}\n$$\nSimplifying the fractions:\n$$\nx^{\\star} = \\begin{pmatrix} x_1^{\\star} \\\\ x_2^{\\star} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{120} \\\\ \\frac{19}{120} \\end{pmatrix}\n$$\nThe reconstructed attenuation coefficients are $x_1^{\\star} = \\frac{13}{120}\\,\\mathrm{cm}^{-1}$ and $x_2^{\\star} = \\frac{19}{120}\\,\\mathrm{cm}^{-1}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{13}{120} & \\frac{19}{120} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Modern iterative reconstruction often goes beyond simple smoothness priors to leverage insights from compressed sensing, such as the observation that images are sparse in a wavelet domain. This advanced practice  explores how to enforce this sparsity using $\\ell_1$ regularization, a technique renowned for preserving sharp edges while suppressing noise. You will derive the powerful proximal gradient algorithms (ISTA and FISTA) that are essential for solving these non-smooth optimization problems.",
            "id": "4913497",
            "problem": "In a parallel-beam computed tomography (CT) system, the continuous Radon transform is discretized to give a linear forward model. Let the unknown image be represented by a vector $\\boldsymbol{x} \\in \\mathbb{R}^{n}$, and let the system matrix $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ encode line integrals along rays. Measured data are $\\boldsymbol{b} \\in \\mathbb{R}^{m}$. To incorporate the prior knowledge that the image is sparse in an orthonormal wavelet basis, consider an orthonormal wavelet transform $\\boldsymbol{W} \\in \\mathbb{R}^{n \\times n}$ (so that $\\boldsymbol{W}^{\\top}\\boldsymbol{W}=\\boldsymbol{I}$) and the convex optimization problem\n$$\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2} \\ + \\ \\lambda \\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1},\n$$\nwhere $\\lambda>0$ is a regularization parameter. \n\nStarting only from the following fundamental bases:\n(i) the linear forward model of tomography,\n(ii) the definition of the gradient of a smooth function, and\n(iii) the definition of the proximal operator of a closed, proper, convex function,\nperform the following:\n\n1) Explain why the term $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ promotes sparsity of wavelet coefficients and how this relates to suppressing noise-like structures in CT reconstruction.\n\n2) Derive the updates for the Iterative Shrinkage-Thresholding Algorithm (ISTA) for the above problem by applying proximal gradient descent to the smooth data-fidelity term and the non-smooth wavelet $\\ell_{1}$ term. Assume that the step size is chosen as $t \\in (0, 2/L)$, where $L$ is a Lipschitz constant of the gradient of the data-fidelity term.\n\n3) Derive the acceleration for the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), clearly specifying the momentum sequence and the extrapolated iterates.\n\n4) Evaluate the derived updates for the concrete $2$-dimensional test case with\n$$\n\\boldsymbol{A}=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad\n\\boldsymbol{W}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}, \\quad\n\\lambda=\\frac{3}{5}, \\quad \\boldsymbol{x}^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix},\n$$\nand\n$$\n\\boldsymbol{b}=\\boldsymbol{W}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix}.\n$$\nUse the canonical choice $t=1/L$, where $L$ is the Lipschitz constant of the gradient of the data-fidelity term for this case. Compute the first ISTA iterate $\\boldsymbol{x}^{1}_{\\text{ISTA}}$ and the second FISTA iterate $\\boldsymbol{x}^{2}_{\\text{FISTA}}$ starting from $\\boldsymbol{x}^{0}$. Provide your final numerical result as a single row vector containing the four entries $\\big(x^{1}_{\\text{ISTA},1}, x^{1}_{\\text{ISTA},2}, x^{2}_{\\text{FISTA},1}, x^{2}_{\\text{FISTA},2}\\big)$. No rounding is required; report exact values.",
            "solution": "The optimization problem is\n$$\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^{n}} \\ F(\\boldsymbol{x}) \\quad \\text{where} \\quad F(\\boldsymbol{x}) = \\underbrace{\\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2}}_{f(\\boldsymbol{x})} \\ + \\ \\underbrace{\\lambda \\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}}_{g(\\boldsymbol{x})}.\n$$\nThis is a composite convex optimization problem where $f(\\boldsymbol{x})$ is a smooth, convex data-fidelity term, and $g(\\boldsymbol{x})$ is a non-smooth, convex regularization term.\n\n**1) Sparsity Promotion and Noise Suppression**\n\nThe term $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ serves as a regularization penalty. The vector $\\boldsymbol{\\alpha} = \\boldsymbol{W}\\boldsymbol{x}$ represents the coefficients of the image $\\boldsymbol{x}$ in the orthonormal wavelet basis $\\boldsymbol{W}$. The $\\ell_1$-norm, $\\|\\boldsymbol{\\alpha}\\|_1 = \\sum_{i} |\\alpha_i|$, is being minimized, scaled by the regularization parameter $\\lambda > 0$.\n\nThe $\\ell_1$-norm is a convex proxy for the non-convex $\\ell_0$ pseudo-norm, which counts the number of non-zero elements in a vector. Minimizing the $\\ell_1$-norm is known to promote sparsity, meaning it encourages solutions where many of the coefficients $\\alpha_i$ are exactly zero. Geometrically, the level sets of the $\\ell_1$-norm are hyper-diamonds (in $\\mathbb{R}^2$, a square rotated by $45^\\circ$), which have sharp \"corners\" lying on the coordinate axes. When seeking a minimum of the total objective function $F(\\boldsymbol{x})$, the solution is often found at these corners, where one or more coefficients are zero.\n\nIn the context of CT imaging, this is highly effective due to two empirical facts:\ni. Natural images, while not sparse in the pixel domain, are often sparse or \"compressible\" in a wavelet basis. This means their structure can be accurately represented by a small number of large-magnitude wavelet coefficients, while the rest are close to zero.\nii. Measurement noise in CT, when transformed into the wavelet domain, tends to distribute its energy more or less uniformly across all coefficients, resulting in a large number of small-magnitude coefficients.\n\nBy minimizing the $\\ell_1$-norm of the wavelet coefficients, the optimization process preferentially shrinks small coefficients to zero, while preserving the large coefficients. This has the effect of eliminating the contributions from noise (which manifest as small coefficients) while retaining the essential structural information of the image (captured by large coefficients). Thus, the term $\\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_{1}$ acts as a powerful prior that suppresses noise-like structures and encourages reconstructions that are consistent with the expected characteristics of natural images.\n\n**2) Derivation of the Iterative Shrinkage-Thresholding Algorithm (ISTA)**\n\nISTA is an instance of the proximal gradient descent algorithm, designed for problems of the form $\\min_{\\boldsymbol{x}} f(\\boldsymbol{x}) + g(\\boldsymbol{x})$. The iterative update is:\n$$\n\\boldsymbol{x}^{k+1} = \\text{prox}_{t g}(\\boldsymbol{x}^k - t \\nabla f(\\boldsymbol{x}^k)),\n$$\nwhere $t > 0$ is a step size.\n\nFirst, we compute the gradient of the smooth term $f(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}\\|_{2}^{2} = \\frac{1}{2}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b})^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b})$.\n$$\n\\nabla f(\\boldsymbol{x}) = \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b}).\n$$\nThe gradient descent step on $f(\\boldsymbol{x})$ is $\\boldsymbol{x}^k - t \\nabla f(\\boldsymbol{x}^k)$.\n\nNext, we derive the proximal operator of $g(\\boldsymbol{x}) = \\lambda\\|\\boldsymbol{W}\\boldsymbol{x}\\|_1$. The proximal operator is defined as:\n$$\n\\text{prox}_{t g}(\\boldsymbol{y}) = \\arg\\min_{\\boldsymbol{z} \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|\\boldsymbol{z}-\\boldsymbol{y}\\|_{2}^{2} + t\\lambda\\|\\boldsymbol{W}\\boldsymbol{z}\\|_1 \\right).\n$$\nSince $\\boldsymbol{W}$ is an orthonormal matrix, $\\boldsymbol{W}^{\\top}\\boldsymbol{W} = \\boldsymbol{I}$, and it preserves the Euclidean norm, i.e., $\\|\\boldsymbol{u}\\|_2 = \\|\\boldsymbol{W}\\boldsymbol{u}\\|_2$. Let $\\boldsymbol{\\alpha} = \\boldsymbol{W}\\boldsymbol{z}$ and $\\boldsymbol{\\beta} = \\boldsymbol{W}\\boldsymbol{y}$. Then $\\boldsymbol{z} = \\boldsymbol{W}^{\\top}\\boldsymbol{\\alpha}$, and $\\|\\boldsymbol{z}-\\boldsymbol{y}\\|_2^2 = \\|\\boldsymbol{W}^{\\top}(\\boldsymbol{\\alpha}-\\boldsymbol{\\beta})\\|_2^2 = \\|\\boldsymbol{\\alpha}-\\boldsymbol{\\beta}\\|_2^2$. The minimization problem can be rewritten in terms of $\\boldsymbol{\\alpha}$:\n$$\n\\arg\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|\\boldsymbol{\\alpha}-\\boldsymbol{\\beta}\\|_2}^{2} + t\\lambda\\|\\boldsymbol{\\alpha}\\|_1 \\right).\n$$\nThis is the definition of the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding operator $S_{t\\lambda}(\\cdot)$. The solution is $\\boldsymbol{\\alpha}^{*} = S_{t\\lambda}(\\boldsymbol{\\beta})$. The soft-thresholding operator is applied element-wise: $(S_{\\tau}(u))_i = \\text{sign}(u_i)\\max(|u_i|-\\tau, 0)$.\n\nTransforming back to the original variable $\\boldsymbol{z}$, we get $\\boldsymbol{z}^{*} = \\boldsymbol{W}^{\\top}\\boldsymbol{\\alpha}^{*} = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{\\beta}) = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{W}\\boldsymbol{y})$.\nCombining these parts, the ISTA update is a two-step process:\n1. Gradient step: $\\boldsymbol{y}^k = \\boldsymbol{x}^k - t \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}^k-\\boldsymbol{b})$\n2. Proximal step: $\\boldsymbol{x}^{k+1} = \\boldsymbol{W}^{\\top}S_{t\\lambda}(\\boldsymbol{W}\\boldsymbol{y}^k)$\n\n**3) Derivation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)**\n\nFISTA accelerates ISTA by incorporating a momentum term. It maintains an auxiliary sequence of extrapolated points $\\boldsymbol{z}^k$. The algorithm proceeds as follows:\nInitialize: Choose $\\boldsymbol{x}^0 \\in \\mathbb{R}^n$, set $\\boldsymbol{z}^0 = \\boldsymbol{x}^0$, and $\\theta_0=1$.\nFor $k=0, 1, 2, \\dots$:\n1. Perform the gradient and proximal steps at the extrapolated point $\\boldsymbol{z}^k$:\n   a. Gradient step: $\\boldsymbol{y}^k = \\boldsymbol{z}^k - t_k \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{z}^k - \\boldsymbol{b})$\n   b. Proximal step: $\\boldsymbol{x}^{k+1} = \\boldsymbol{W}^{\\top}S_{t_k\\lambda}(\\boldsymbol{W}\\boldsymbol{y}^k)$\n2. Update the momentum parameter $\\theta_k$:\n   $$\n   \\theta_{k+1} = \\frac{1 + \\sqrt{1 + 4\\theta_k^2}}{2}\n   $$\n3. Compute the next extrapolated point $\\boldsymbol{z}^{k+1}$ via a linear combination of the current and previous iterates:\n   $$\n   \\boldsymbol{z}^{k+1} = \\boldsymbol{x}^{k+1} + \\frac{\\theta_k - 1}{\\theta_{k+1}}(\\boldsymbol{x}^{k+1} - \\boldsymbol{x}^k)\n   $$\nThe step size $t_k$ is typically constant, $t_k=t \\in (0, 2/L)$, where $L$ is the Lipschitz constant of $\\nabla f$.\n\n**4) Evaluation for the Concrete Test Case**\n\nWe are given:\n$\\boldsymbol{A}=\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} = \\boldsymbol{I}$,\n$\\boldsymbol{W}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}$,\n$\\lambda=\\frac{3}{5}$,\n$\\boldsymbol{x}^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$,\nand $\\boldsymbol{b}=\\boldsymbol{W}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}\\begin{pmatrix}1 \\\\ \\frac{1}{5}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1+\\frac{1}{5} \\\\ 1-\\frac{1}{5}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\\frac{6}{5} \\\\ \\frac{4}{5}\\end{pmatrix}$.\n\nThe Lipschitz constant of $\\nabla f(\\boldsymbol{x})=\\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}-\\boldsymbol{b})$ is $L = \\|\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\|_2$. Here, $\\boldsymbol{A}=\\boldsymbol{I}$, so $\\boldsymbol{A}^{\\top}\\boldsymbol{A} = \\boldsymbol{I}^{\\top}\\boldsymbol{I} = \\boldsymbol{I}$. The spectral norm of the identity matrix is $1$, so $L=1$.\nThe step size is given as $t=1/L = 1/1 = 1$.\nThe soft-thresholding parameter is $\\tau = t\\lambda = 1 \\cdot \\frac{3}{5} = \\frac{3}{5}$.\n\nNote that $\\boldsymbol{W}$ is symmetric ($\\boldsymbol{W}^{\\top}=\\boldsymbol{W}$) and also satisfies $\\boldsymbol{W}^2 = \\frac{1}{2}\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix}2 & 0 \\\\ 0 & 2\\end{pmatrix} = \\boldsymbol{I}$.\n\n**Computation of $\\boldsymbol{x}^{1}_{\\text{ISTA}}$:**\nStarting with $\\boldsymbol{x}^0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$.\n1. Gradient step:\n   $\\boldsymbol{y}^0 = \\boldsymbol{x}^0 - t \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}^0 - \\boldsymbol{b}) = \\begin{pmatrix}0\\\\0\\end{pmatrix} - 1 \\cdot \\boldsymbol{I}^{\\top}(\\boldsymbol{I}\\begin{pmatrix}0\\\\0\\end{pmatrix} - \\boldsymbol{b}) = -(-\\boldsymbol{b}) = \\boldsymbol{b}$.\n2. Proximal step:\n   $\\boldsymbol{x}^1_{\\text{ISTA}} = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^0) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b})$.\n   First, we compute the argument of the thresholding function:\n   $\\boldsymbol{W}\\boldsymbol{b} = \\boldsymbol{W}\\left(\\boldsymbol{W}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix}\\right) = \\boldsymbol{W}^2 \\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\boldsymbol{I}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix}$.\n   Next, apply soft-thresholding with $\\tau=3/5$:\n   $S_{3/5}\\begin{pmatrix}1 \\\\ 1/5\\end{pmatrix} = \\begin{pmatrix} \\text{sign}(1)\\max(|1|-\\frac{3}{5}, 0) \\\\ \\text{sign}(\\frac{1}{5})\\max(|\\frac{1}{5}|-\\frac{3}{5}, 0) \\end{pmatrix} = \\begin{pmatrix} \\max(\\frac{2}{5}, 0) \\\\ \\max(-\\frac{2}{5}, 0) \\end{pmatrix} = \\begin{pmatrix}\\frac{2}{5} \\\\ 0\\end{pmatrix}$.\n   Finally, transform back:\n   $\\boldsymbol{x}^1_{\\text{ISTA}} = \\boldsymbol{W}\\begin{pmatrix}\\frac{2}{5} \\\\ 0\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}\\begin{pmatrix}\\frac{2}{5} \\\\ 0\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\\frac{2}{5} \\\\ \\frac{2}{5}\\end{pmatrix} = \\frac{2}{5\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n   So, $\\boldsymbol{x}^{1}_{\\text{ISTA}} = \\begin{pmatrix} \\frac{\\sqrt{2}}{5} \\\\ \\frac{\\sqrt{2}}{5} \\end{pmatrix}$.\n\n**Computation of $\\boldsymbol{x}^{2}_{\\text{FISTA}}$:**\nInitialize: $\\boldsymbol{x}^0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$, $\\boldsymbol{z}^0 = \\boldsymbol{x}^0$, $\\theta_0=1$. Step size $t=1$.\n\n*Iteration 1 (k=0):*\n1. Gradient and Proximal steps at $\\boldsymbol{z}^0$:\n   $\\boldsymbol{y}^0 = \\boldsymbol{z}^0 - t\\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{z}^0-\\boldsymbol{b}) = \\begin{pmatrix}0\\\\0\\end{pmatrix} - 1\\cdot(\\begin{pmatrix}0\\\\0\\end{pmatrix}-\\boldsymbol{b}) = \\boldsymbol{b}$.\n   $\\boldsymbol{x}^1 = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^0) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{x}^1_{\\text{ISTA}} = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n2. Update $\\theta_0$:\n   $\\theta_1 = \\frac{1+\\sqrt{1+4\\theta_0^2}}{2} = \\frac{1+\\sqrt{5}}{2}$.\n3. Update $\\boldsymbol{z}^0$:\n   $\\boldsymbol{z}^1 = \\boldsymbol{x}^1 + \\frac{\\theta_0-1}{\\theta_1}(\\boldsymbol{x}^1-\\boldsymbol{x}^0) = \\boldsymbol{x}^1 + \\frac{1-1}{\\theta_1}(\\boldsymbol{x}^1-\\boldsymbol{x}^0) = \\boldsymbol{x}^1$.\n   So, $\\boldsymbol{z}^1 = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n\n*Iteration 2 (k=1):*\n1. Gradient and Proximal steps at $\\boldsymbol{z}^1$:\n   $\\boldsymbol{y}^1 = \\boldsymbol{z}^1 - t\\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{z}^1-\\boldsymbol{b}) = \\boldsymbol{z}^1 - 1\\cdot(\\boldsymbol{z}^1-\\boldsymbol{b}) = \\boldsymbol{b}$.\n   The argument for the proximal operator is again $\\boldsymbol{b}$.\n   $\\boldsymbol{x}^2 = \\boldsymbol{W}^{\\top}S_{\\tau}(\\boldsymbol{W}\\boldsymbol{y}^1) = \\boldsymbol{W}S_{3/5}(\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{x}^1 = \\frac{\\sqrt{2}}{5}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n   So, $\\boldsymbol{x}^2_{\\text{FISTA}} = \\begin{pmatrix} \\frac{\\sqrt{2}}{5} \\\\ \\frac{\\sqrt{2}}{5} \\end{pmatrix}$.\n\nIn this specific case, the algorithm converges in one step. The FISTA iterate $\\boldsymbol{x}^2$ is therefore the same as $\\boldsymbol{x}^1$.\n\nThe final result is the row vector $\\big(x^{1}_{\\text{ISTA},1}, x^{1}_{\\text{ISTA},2}, x^{2}_{\\text{FISTA},1}, x^{2}_{\\text{FISTA},2}\\big)$.\n$x^{1}_{\\text{ISTA},1} = \\frac{\\sqrt{2}}{5}$, $x^{1}_{\\text{ISTA},2} = \\frac{\\sqrt{2}}{5}$.\n$x^{2}_{\\text{FISTA},1} = \\frac{\\sqrt{2}}{5}$, $x^{2}_{\\text{FISTA},2} = \\frac{\\sqrt{2}}{5}$.\nThe final vector is $\\left(\\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{2}}{5} & \\frac{\\sqrt{2}}{5} & \\frac{\\sqrt{2}}{5} & \\frac{\\sqrt{2}}{5}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}