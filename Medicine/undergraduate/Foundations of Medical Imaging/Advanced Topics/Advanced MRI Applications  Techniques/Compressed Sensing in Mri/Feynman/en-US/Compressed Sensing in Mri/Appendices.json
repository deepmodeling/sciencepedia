{
    "hands_on_practices": [
        {
            "introduction": "Compressed sensing reconstruction algorithms work by promoting sparsity in a transform domain, such as the wavelet domain. This practice focuses on the core mathematical operation that achieves this: soft-thresholding. Since MRI signals are inherently complex-valued, you will derive the complex soft-thresholding operator from the fundamental definition of a proximal operator and implement it to verify its essential, phase-preserving nature .",
            "id": "4870640",
            "problem": "You are to implement and analyze complex-valued soft-thresholding for wavelet coefficients in the context of Magnetic Resonance Imaging (MRI), using principles from compressed sensing. Consider that MRI images and their wavelet coefficients are complex-valued due to quadrature detection and phase-encoding. Compressed Sensing (CS) promotes sparsity in a transform domain, here the wavelet domain, by penalizing the sum of magnitudes of coefficients.\n\nFundamental base:\n- A complex number is written as $z = r e^{i \\phi}$ where $r = |z| \\geq 0$ is the magnitude and $\\phi = \\arg(z) \\in (-\\pi,\\pi]$ is the phase, and $i = \\sqrt{-1}$ is the imaginary unit.\n- The complex $\\ell_1$ norm of a vector $x \\in \\mathbb{C}^n$ is $\\|x\\|_1 = \\sum_{k=1}^n |x_k|$.\n- The proximal operator of a function $g$ at $y$ is defined by $\\mathrm{prox}_g(y) = \\arg\\min_x \\left( \\tfrac{1}{2}\\|x-y\\|_2^2 + g(x) \\right)$, a well-tested concept in convex optimization frequently used in CS MRI.\n\nTask:\n- Starting from the above base, derive the complex-valued soft-thresholding operator applied coefficient-wise as the proximal operator of the complex $\\ell_1$ norm. Your derivation must justify why the operator preserves phase while shrinking magnitudes to promote sparsity in the complex domain.\n- Implement a program that applies complex-valued soft-thresholding to given arrays of complex numbers representing wavelet coefficients. The program must compute, for each test case, the maximum absolute phase difference in radians between the original coefficients and the thresholded coefficients among those coefficients that remain nonzero after thresholding. If all thresholded coefficients are zero, return $0$ for that case. Angles must be measured in radians.\n\nInput specification (embedded in the program):\n- You will be given a test suite consisting of arrays of complex coefficients and nonnegative thresholds $\\tau$.\n- Apply coefficient-wise complex soft-thresholding with threshold $\\tau$ to each array.\n\nTest suite to implement:\n- Case $1$ (happy path): coefficients $\\left[ 1 + i,\\; -0.5 + 2 i,\\; 0.3 - 0.4 i,\\; 0 + 0 i \\right]$ with threshold $\\tau = 0.5$.\n- Case $2$ (boundary condition equals threshold): coefficients $\\left[ 0.5 + 0 i,\\; 0 - 0.5 i,\\; 0.3535533905932738 + 0.3535533905932738 i \\right]$ with threshold $\\tau = 0.5$.\n- Case $3$ (zero threshold): coefficients $\\left[ -2 + 3 i,\\; 4 + 0 i,\\; 0 + 5 i,\\; -1 - 1 i \\right]$ with threshold $\\tau = 0$.\n- Case $4$ (high threshold causing all zeros): coefficients $\\left[ 1 + 2 i,\\; -3 + 4 i,\\; 5 - 6 i \\right]$ with threshold $\\tau = 10$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a single real number (a float) equal to the maximum absolute phase difference in radians as defined above. The final output must be of the form $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$.\n\nScientific realism requirements:\n- Treat the wavelet transform as a linear, unitary transform on complex images; the thresholding acts on the complex wavelet coefficients directly.\n- Justify rigorously in your derivation why preserving phase while shrinking magnitudes arises from minimizing a convex objective grounded in the proximal operator definition, not from heuristic shortcuts.\n\nAnswer format:\n- Provide a complete derivation and explanation.\n- Provide runnable code that internally defines and runs the test suite.\n- The final printed output must be a single line in the exact format specified above.",
            "solution": "The problem requires the derivation of the complex-valued soft-thresholding operator and its implementation to analyze its phase-preserving properties. This operator is fundamental to many compressed sensing reconstruction algorithms, including those used in MRI, where sparsity is promoted in a transform domain (e.g., wavelet) to enable undersampled acquisition.\n\nThe derivation begins with the definition of the proximal operator. For a given function $g$, its proximal operator at a point $y$ is defined as:\n$$\n\\mathrm{prox}_g(y) = \\arg\\min_x \\left( \\frac{1}{2}\\|x-y\\|_2^2 + g(x) \\right)\n$$\nIn the context of compressed sensing, the function $g(x)$ is typically a scaled $\\ell_1$ norm used to promote sparsity. For a vector of complex coefficients $x \\in \\mathbb{C}^n$, we use $g(x) = \\tau \\|x\\|_1 = \\tau \\sum_{k=1}^n |x_k|$, where $\\tau \\geq 0$ is a non-negative threshold parameter. The objective function to minimize is:\n$$\nJ(x) = \\frac{1}{2}\\|x-y\\|_2^2 + \\tau \\|x\\|_1 = \\frac{1}{2} \\sum_{k=1}^n |x_k - y_k|^2 + \\tau \\sum_{k=1}^n |x_k|\n$$\nSince the objective function is a sum of terms, each involving only a single coefficient pair $(x_k, y_k)$, the minimization is separable. We can find the optimal vector $x$ by minimizing each component's contribution to the total objective independently. Therefore, for each coefficient $k$, we solve the following minimization problem over the complex variable $x \\in \\mathbb{C}$, given the complex measurement $y \\in \\mathbb{C}$:\n$$\n\\hat{x} = \\arg\\min_{x \\in \\mathbb{C}} \\left( \\frac{1}{2} |x - y|^2 + \\tau |x| \\right)\n$$\nLet us represent the complex variables $x$ and $y$ in polar coordinates: $x = r_x e^{i\\phi_x}$ and $y = r_y e^{i\\phi_y}$, where $r_x = |x|$, $\\phi_x = \\arg(x)$, $r_y = |y|$, and $\\phi_y = \\arg(y)$. The objective function becomes:\n$$\nj(r_x, \\phi_x) = \\frac{1}{2} |r_x e^{i\\phi_x} - r_y e^{i\\phi_y}|^2 + \\tau r_x\n$$\nExpanding the squared magnitude term:\n$$\n|r_x e^{i\\phi_x} - r_y e^{i\\phi_y}|^2 = (r_x e^{i\\phi_x} - r_y e^{i\\phi_y})(r_x e^{-i\\phi_x} - r_y e^{-i\\phi_y}) = r_x^2 - 2 r_x r_y \\cos(\\phi_x - \\phi_y) + r_y^2\n$$\nThe objective function is thus:\n$$\nj(r_x, \\phi_x) = \\frac{1}{2} (r_x^2 - 2 r_x r_y \\cos(\\phi_x - \\phi_y) + r_y^2) + \\tau r_x\n$$\nWe seek to minimize this function with respect to the magnitude $r_x \\geq 0$ and the phase $\\phi_x \\in (-\\pi, \\pi]$. If $y = 0$, then $r_y=0$, and the objective becomes $\\frac{1}{2}r_x^2 + \\tau r_x$, which is clearly minimized at $r_x = 0$, implying $\\hat{x} = 0$.\n\nIf $y \\neq 0$ (so $r_y > 0$), we can first minimize with respect to the phase $\\phi_x$. The only term dependent on $\\phi_x$ is $-2 r_x r_y \\cos(\\phi_x - \\phi_y)$. To minimize the objective $j$, this term must be minimized, which is equivalent to maximizing $\\cos(\\phi_x - \\phi_y)$. The maximum value of the cosine function is $1$, which occurs when its argument is $0$ (or any integer multiple of $2\\pi$). This implies that the optimal phase $\\phi_x$ must be equal to the phase of the input coefficient, $\\phi_y$. This is a crucial result: the proximal operator of the complex $\\ell_1$ norm preserves the phase of the input coefficient, $\\arg(\\hat{x}) = \\arg(y)$.\n\nHaving established that $\\phi_x = \\phi_y$, we can substitute $\\cos(\\phi_x - \\phi_y) = 1$ into the objective function, which now depends only on the magnitude $r_x$:\n$$\nj(r_x) = \\frac{1}{2} (r_x^2 - 2 r_x r_y + r_y^2) + \\tau r_x = \\frac{1}{2} (r_x - r_y)^2 + \\tau r_x\n$$\nThis is the objective function for real-valued soft-thresholding, which we must minimize for $r_x \\geq 0$. This function is convex but not differentiable at $r_x = 0$. We use subgradient calculus. The subdifferential $\\partial j(r_x)$ is given by:\n$$\n\\partial j(r_x) = (r_x - r_y) + \\tau \\cdot \\partial r_x\n$$\nwhere $\\partial r_x$ is the subdifferential of the absolute value function for a non-negative variable. The minimum occurs when $0 \\in \\partial j(r_x)$.\n\nCase 1: The solution is at $r_x > 0$. Here, $\\partial r_x = \\{1\\}$, so we set the derivative to zero: $r_x - r_y + \\tau = 0 \\implies r_x = r_y - \\tau$. Since we assumed $r_x > 0$, this solution is valid only if $r_y - \\tau > 0$, i.e., $r_y > \\tau$.\n\nCase 2: The solution is at $r_x = 0$. The subdifferential is $\\partial r_x = [0, 1]$ for a non-negative variable (or more properly, the subdifferential of $\\tau |x|$ where $x$ can be positive or negative is $\\tau [-1, 1]$, and for a minimizer at $x=0$, subgradient optimality requires $y \\in \\tau [-1, 1]$ which is $|y| \\leq \\tau$). Let's re-verify with the one-sided derivative for $r_x \\ge 0$. The optimality condition at a boundary point $r_x=0$ is that the directional derivative is non-negative. $j'(0^+) = -r_y + \\tau$. For $r_x=0$ to be the minimum, we must have $j'(0^+) \\geq 0$, which means $-r_y + \\tau \\geq 0$, or $r_y \\leq \\tau$.\n\nCombining these two cases, the optimal magnitude $r_x=|\\hat{x}|$ is:\n$$\n|\\hat{x}| = \\begin{cases} r_y - \\tau, & \\text{if } r_y > \\tau \\\\ 0, & \\text{if } r_y \\leq \\tau \\end{cases}\n$$\nThis can be written compactly as $|\\hat{x}| = \\max(0, r_y - \\tau) = (|y| - \\tau)_+$.\n\nCombining the results for magnitude and phase, the complex soft-thresholding operator is:\n$$\n\\hat{x} = |\\hat{x}| e^{i\\phi_x} = \\max(0, |y| - \\tau) e^{i\\arg(y)}\n$$\nFor $y \\neq 0$, we can write $e^{i\\arg(y)} = y/|y|$. The operator can thus be expressed as:\n$$\n\\hat{x} = \\max(0, |y| - \\tau) \\frac{y}{|y|} = \\left(1 - \\frac{\\tau}{|y|}\\right)_+ y\n$$\nThis derivation rigorously shows that for any coefficient $y$ whose magnitude $|y|$ is greater than the threshold $\\tau$, the resulting coefficient $\\hat{x}$ is non-zero and has a phase identical to that of $y$. Its magnitude is shrunk by $\\tau$. If $|y| \\leq \\tau$, the coefficient is set to zero. This selective shrinkage of small coefficients is what promotes sparsity.\n\nThe computational task is to implement this operator and compute the maximum absolute phase difference between the original and thresholded coefficients for the subset of coefficients that remain non-zero. Based on the derivation, this phase difference must be identically zero in theory. The numerical computation serves as a verification of this fundamental property. For any test case where all coefficients are thresholded to zero, the maximum phase difference is defined to be $0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements complex soft-thresholding and computes the maximum phase difference\n    for a suite of test cases, as per the problem description.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([1 + 1j, -0.5 + 2j, 0.3 - 0.4j, 0 + 0j], dtype=complex), 0.5),\n        (np.array([0.5 + 0j, 0 - 0.5j, 0.3535533905932738 + 0.3535533905932738j], dtype=complex), 0.5),\n        (np.array([-2 + 3j, 4 + 0j, 0 + 5j, -1 - 1j], dtype=complex), 0.0),\n        (np.array([1 + 2j, -3 + 4j, 5 - 6j], dtype=complex), 10.0),\n    ]\n\n    results = []\n\n    for y, tau in test_cases:\n        # Complex soft-thresholding operator: S_tau(y)\n        # S_tau(y_k) = max(0, |y_k| - tau) * (y_k / |y_k|)\n        # This is numerically robustly implemented as:\n        # x = y * scale, where scale = max(0, |y|-tau) / |y|\n        \n        magnitudes = np.abs(y)\n        \n        # Calculate the shrunk magnitudes.\n        shrunk_magnitudes = np.maximum(0, magnitudes - tau)\n        \n        # Calculate the thresholded coefficients.\n        # We use np.divide to handle division by zero safely.\n        # Where magnitude is 0, the numerator shrunk_magnitudes is also 0 (since tau>=0).\n        # We specify out=np.zeros_like(...) so that 0/0 results in 0.\n        scale = np.divide(shrunk_magnitudes, magnitudes, \n                          out=np.zeros_like(magnitudes, dtype=float), \n                          where=(magnitudes != 0))\n        \n        x = y * scale\n\n        # Identify the coefficients that remain non-zero after thresholding.\n        # A small tolerance is used for floating point comparisons.\n        nonzero_mask = np.abs(x) > 1e-15\n\n        if not np.any(nonzero_mask):\n            # If all thresholded coefficients are zero, the max phase difference is 0.\n            max_phase_diff = 0.0\n        else:\n            # Select the original and thresholded coefficients that are non-zero.\n            y_nonzero = y[nonzero_mask]\n            x_nonzero = x[nonzero_mask]\n            \n            # Calculate the phase difference.\n            # np.angle(z2 * np.conj(z1)) gives angle(z2) - angle(z1) in (-pi, pi].\n            # This is numerically more stable than np.angle(z2) - np.angle(z1).\n            phase_diffs = np.angle(x_nonzero * np.conj(y_nonzero))\n            \n            # Find the maximum absolute phase difference.\n            max_phase_diff = np.max(np.abs(phase_diffs))\n            \n        results.append(max_phase_diff)\n\n    # Format the results into the required output string.\n    # The str() conversion ensures standard float representation.\n    # The derived theory predicts all results will be 0.0\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A key insight of compressed sensing is that randomized, incoherent undersampling is far more effective than structured, periodic undersampling. This is because random sampling converts aliasing artifacts from structured \"ghosts\" into low-level, noise-like interference that sparsity-promoting algorithms can easily remove. In this hands-on coding exercise, you will construct both structured and random sampling masks, compute their corresponding point spread functions, and use a quantitative metric to prove that randomness leads to less coherent aliasing .",
            "id": "4870620",
            "problem": "You are given a task grounded in the relationship between sampling in the frequency domain and aliasing in the image domain for Magnetic Resonance Imaging (MRI). Use the Discrete Fourier Transform (DFT) framework as follows.\n\nFundamental base: In two-dimensional imaging, a discrete object on an image grid, when transformed to the frequency domain by the two-dimensional Discrete Fourier Transform (DFT), yields its frequency representation. Undersampling in the frequency domain corresponds to multiplying the fully sampled frequency data by a binary sampling mask. In the image domain, this multiplication corresponds to convolution with the inverse DFT of the sampling mask, which is the point spread function (PSF). Hence, for a sampling mask, the aliasing artifacts in the image domain are characterized by the PSF. Structured, periodic sampling typically produces coherent aliasing (ghosts), while randomized sampling produces incoherent, noise-like artifacts. These are standard, well-tested facts in the foundations of medical imaging and compressed sensing.\n\nObjective: Write a program that constructs two types of frequency-domain sampling masks on a two-dimensional grid of size $N \\times N$ (with $N$ given), along the phase-encode direction (the $k_y$ axis), and compares the coherence of the aliasing ghosts induced by each mask using a PSF-based metric.\n\nDefinitions to use:\n- The two-dimensional inverse DFT (denoted $\\mathrm{IDFT}_2$) of a sampling mask yields the point spread function $p(\\mathbf{r})$, where $\\mathbf{r}$ indexes image-domain pixels. You may implement $\\mathrm{IDFT}_2$ with any normalization, as the metric below is defined by a ratio that cancels consistent scaling.\n- The magnitude of the PSF is $|p(\\mathbf{r})|$.\n- Let $p_0$ be the central pixel of the shifted PSF (i.e., after applying a shift so that the zero-frequency component is centered), and let $\\mathcal{R}$ denote the set of all pixels except the center. Define the coherence ratio of the aliasing ghosts for a mask as\n$$\nC \\;=\\; \\frac{\\max_{\\mathbf{r} \\in \\mathcal{R}} |p(\\mathbf{r})|}{\\operatorname{mean}_{\\mathbf{r} \\in \\mathcal{R}} |p(\\mathbf{r})|}.\n$$\nIf $|p(\\mathbf{r})|$ is identically zero on $\\mathcal{R}$, define $C = 0$.\n\nSampling masks to construct for each test case $(N, R)$:\n- Randomly shifted Cartesian mask: Choose a random integer shift $s$ uniformly from $\\{0, 1, \\dots, R-1\\}$, and select all phase-encode lines $k_y$ such that $(k_y - s) \\bmod R = 0$. For each selected $k_y$, include the entire line across all $k_x$ (i.e., set those rows in the mask to $1$, others to $0$). This yields a periodic set of lines with a random cyclic shift.\n- Fully random mask with matched sampling density: Let $M$ be the number of lines selected by the randomly shifted Cartesian mask for the given $(N, R)$ realization. Independently choose a set of $M$ distinct $k_y$ indices uniformly at random from $\\{0,1,\\dots,N-1\\}$, and include those full lines across all $k_x$.\n\nFor each mask, compute the PSF $p(\\mathbf{r})$ as the two-dimensional inverse DFT of the mask, apply a frequency shift so that the central lobe is at the center, exclude the center pixel, and compute the coherence ratio $C$ as above.\n\nComparison task: For each test case $(N, R)$, compute:\n- $C_{\\mathrm{shift}}$: the coherence ratio for the randomly shifted Cartesian mask.\n- $C_{\\mathrm{rand}}$: the coherence ratio for the fully random mask with matched line count.\n\nProduce a boolean result per test case indicating whether $C_{\\mathrm{rand}} < C_{\\mathrm{shift}}$.\n\nRandomness and reproducibility:\n- Use a fixed random seed $s_0 = 2025$ for the base generator.\n- For test case index $i$ starting at $0$, use seed $s_0 + i$ to generate both the shift $s$ and the random line subset for that case, ensuring reproducibility.\n\nTest suite:\n- Case $1$: $(N, R) = (64, 4)$.\n- Case $2$: $(N, R) = (64, 8)$.\n- Case $3$: $(N, R) = (64, 1)$.\n- Case $4$: $(N, R) = (60, 7)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True}]$.\n- The result for each test case is the boolean indicating whether $C_{\\mathrm{rand}} < C_{\\mathrm{shift}}$ for that case, in the same order as the test suite.\n\nNo physical units are involved in this problem. Angles are not part of the computation. All numerical outputs are booleans as specified. The program must be self-contained and require no input.",
            "solution": "The problem requires an algorithmic comparison of aliasing artifact coherence resulting from two distinct k-space undersampling strategies in Magnetic Resonance Imaging (MRI). The foundation of this analysis is the Fourier relationship between the k-space sampling mask and the image-domain point spread function (PSF). The PSF, defined as the inverse Discrete Fourier Transform (IDFT) of the sampling mask, describes how a single point in the object spreads out in the reconstructed image. The structure of the PSF's sidelobes, which manifest as aliasing artifacts or \"ghosts,\" is dictated by the geometry of the sampling mask. A periodic, structured mask is expected to produce a PSF with sparse, high-amplitude sidelobes (coherent aliasing), while a randomized mask is expected to produce a PSF with numerous, low-amplitude, noise-like sidelobes (incoherent aliasing). This problem quantifies this coherence using a peak-to-average ratio of the PSF's sidelobe magnitudes.\n\nThe procedure is executed for each test case, which is defined by a grid-size parameter $N$ and a sampling rate parameter $R$. A unique, reproducible random seed is used for each case.\n\n**Step 1: Mask Generation**\n\nFor a given test case $(N, R)$ and its corresponding random seed, two $N \\times N$ binary sampling masks are constructed. Sampling is performed along the phase-encode direction, represented by the $k_y$ axis, meaning entire rows of the k-space grid are either fully sampled or fully skipped.\n\n1.  **Randomly Shifted Cartesian Mask ($M_{\\mathrm{shift}}$):** This mask simulates a periodically undersampled acquisition with a random starting position. A random integer shift, $s$, is chosen uniformly from the set $\\{0, 1, \\dots, R-1\\}$. A phase-encode line at index $k_y$ (where $k_y \\in \\{0, 1, \\dots, N-1\\}$) is selected if and only if it satisfies the condition $(k_y - s) \\bmod R = 0$. For every selected $k_y$, the corresponding row in the mask $M_{\\mathrm{shift}}$ is set to $1$; all other entries are $0$.\n\n2.  **Fully Random Mask ($M_{\\mathrm{rand}}$):** This mask is designed to have the same sampling density (i.e., the same total number of sampled lines) as the Cartesian mask, but with the lines chosen randomly. First, the number of lines, $M$, selected in $M_{\\mathrm{shift}}$ is counted. Then, a new set of $M$ distinct line indices is chosen uniformly at random from $\\{0, 1, \\dots, N-1\\}$. The mask $M_{\\mathrm{rand}}$ is constructed by setting the rows corresponding to these randomly chosen indices to $1$.\n\n**Step 2: Point Spread Function and Coherence Ratio Calculation**\n\nFor each mask, its corresponding PSF and coherence ratio are computed.\n\n1.  **Point Spread Function (PSF):** The two-dimensional PSF, $p(\\mathbf{r})$, is the inverse Discrete Fourier Transform of the k-space mask. To facilitate analysis of its structure, the PSF is shifted such that its main lobe, corresponding to the zero-frequency component of k-space, is located at the center of the $N \\times N$ image grid. This is accomplished computationally by applying a two-dimensional inverse Fast Fourier Transform ($\\mathrm{IDFT}_2$) followed by a circular shift (`fftshift`).\n\n2.  **Coherence Ratio ($C$):** The coherence ratio is a metric designed to quantify the \"peakiness\" of the aliasing artifacts relative to their average intensity. Let $|p(\\mathbf{r})|$ be the magnitude of the shifted PSF. The central pixel, $p_0$, represents the main lobe and is excluded from the analysis of artifacts. Let $\\mathcal{R}$ be the set of all non-central pixel locations. The coherence ratio is defined as:\n    $$\n    C \\;=\\; \\frac{\\max_{\\mathbf{r} \\in \\mathcal{R}} |p(\\mathbf{r})|}{\\operatorname{mean}_{\\mathbf{r} \\in \\mathcal{R}} |p(\\mathbf{r})|}\n    $$\n    In the specific case where a mask is fully sampled (e.g., for $R=1$), the PSF is a perfect delta function, meaning $|p(\\mathbf{r})| = 0$ for all $\\mathbf{r} \\in \\mathcal{R}$. In this scenario, the expression for $C$ becomes an indeterminate form $0/0$. The problem prescribes that in this case, $C$ shall be defined as $0$.\n\n**Step 3: Comparison and Final Output**\n\nThe core of the task is to compare the coherence of the artifacts produced by the two sampling schemes. For each test case $(N, R)$:\n\n1.  The coherence ratio for the randomly shifted Cartesian mask, $C_{\\mathrm{shift}}$, is calculated.\n2.  The coherence ratio for the fully random mask, $C_{\\mathrm{rand}}$, is calculated.\n3.  The boolean expression $C_{\\mathrm{rand}} < C_{\\mathrm{shift}}$ is evaluated. This tests the hypothesis that random sampling leads to less coherent (more noise-like) artifacts than structured, periodic sampling.\n\nThis procedure is repeated for all cases in the test suite, using a base random seed $s_0 = 2025$ and incrementing it for each subsequent test case ($s_0+i$ for case index $i$) to ensure reproducibility. The final output is a list of these boolean results. For the edge case $(N=64, R=1)$, both masks correspond to full sampling, yielding $C_{\\mathrm{shift}} = 0$ and $C_{\\mathrm{rand}} = 0$. The comparison $0 < 0$ evaluates to False. For all other cases, where $R > 1$, we expect $C_{\\mathrm{rand}} < C_{\\mathrm{shift}}$ to be True, confirming a fundamental principle of compressed sensing.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_coherence_ratio(mask):\n    \"\"\"\n    Computes the PSF and its coherence ratio for a given k-space mask.\n\n    Args:\n        mask (np.ndarray): A 2D numpy array representing the k-space sampling mask.\n\n    Returns:\n        float: The coherence ratio C.\n    \"\"\"\n    N, _ = mask.shape\n\n    # 1. Compute the PSF as the inverse DFT of the mask, then center it.\n    psf = np.fft.ifft2(mask)\n    psf_shifted = np.fft.fftshift(psf)\n    psf_mag = np.abs(psf_shifted)\n\n    # 2. Extract sidelobes by excluding the central pixel.\n    center_y, center_x = N // 2, N // 2\n    \n    # Create a boolean mask for the region of interest R (all pixels except the center).\n    roi_mask = np.ones((N, N), dtype=bool)\n    roi_mask[center_y, center_x] = False\n    sidelobes = psf_mag[roi_mask]\n\n    # 3. Handle the special case of zero sidelobes (full sampling).\n    # This also handles the potential division by zero.\n    if np.all(sidelobes == 0):\n        return 0.0\n\n    # 4. Compute the coherence ratio.\n    max_sidelobe = np.max(sidelobes)\n    mean_sidelobe = np.mean(sidelobes)\n    \n    C = max_sidelobe / mean_sidelobe\n    return C\n\ndef calculate_comparison(N, R, seed):\n    \"\"\"\n    Generates two masks, computes their coherence ratios, and compares them.\n\n    Args:\n        N (int): The grid size (N x N).\n        R (int): The Cartesian sampling rate parameter.\n        seed (int): The random seed for this test case.\n\n    Returns:\n        bool: The result of C_rand  C_shift.\n    \"\"\"\n    # Initialize a random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # --- Mask 1: Randomly Shifted Cartesian ---\n    mask_shift = np.zeros((N, N), dtype=float)\n    s = rng.integers(R)\n    ky_indices_shift = [ky for ky in range(N) if (ky - s) % R == 0]\n    mask_shift[ky_indices_shift, :] = 1.0\n\n    # --- Mask 2: Fully Random with Matched Density ---\n    M = len(ky_indices_shift)\n    mask_rand = np.zeros((N, N), dtype=float)\n    # Choose M distinct ky indices uniformly at random.\n    ky_indices_rand = rng.choice(N, size=M, replace=False)\n    mask_rand[ky_indices_rand, :] = 1.0\n\n    # --- Compute Coherence Ratios ---\n    C_shift = compute_coherence_ratio(mask_shift)\n    C_rand = compute_coherence_ratio(mask_rand)\n\n    # --- Perform Comparison ---\n    return C_rand  C_shift\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (64, 4),  # Case 1\n        (64, 8),  # Case 2\n        (64, 1),  # Case 3\n        (60, 7),  # Case 4\n    ]\n\n    base_seed = 2025\n    results = []\n\n    for i, (N, R) in enumerate(test_cases):\n        seed = base_seed + i\n        result = calculate_comparison(N, R, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Python's str(bool) outputs 'True' or 'False' which is the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While uniform random sampling is effective, we can design even smarter sampling patterns by acknowledging that most of an image's energy is concentrated in the center of k-space. This practice explores variable-density sampling, a strategy that preferentially acquires more samples at low spatial frequencies. Through a direct calculation, you will determine the expected number of samples within a central region of k-space, providing a concrete understanding of how a sampling probability density function translates into a physically meaningful acquisition pattern .",
            "id": "4870674",
            "problem": "A two-dimensional Magnetic Resonance Imaging (MRI) experiment uses compressed sensing with variable-density random sampling over a circular $k$-space support. The fully sampled $k$-space support is the disk $\\{ \\boldsymbol{k} \\in \\mathbb{R}^{2} : \\| \\boldsymbol{k} \\| \\leq K \\}$ of radius $K > 0$. The sampling strategy draws $N_{\\text{acq}}$ acquisition locations independently from a continuous probability density function $p(\\boldsymbol{k})$ that is radially symmetric and favors low spatial frequencies. The density is specified by\n$$\np(\\boldsymbol{k}) \\propto \\frac{1}{1 + \\left( \\frac{\\| \\boldsymbol{k} \\|}{k_{c}} \\right)^{2}}, \\quad \\text{for } \\| \\boldsymbol{k} \\| \\leq K,\n$$\nand $p(\\boldsymbol{k}) = 0$ otherwise, where $k_{c} > 0$ is a characteristic knee parameter and the proportionality constant is chosen so that $p$ integrates to $1$ over the disk.\n\nLet the fully sampled acquisition contain $M$ distinct $k$-space locations within the disk, and let the target acceleration factor be $R > 1$, so that the number of acquired locations is $N_{\\text{acq}} = M / R$. For a given radius $r$ with $0 \\leq r \\leq K$, compute the exact closed-form expression for the expected number of acquired samples that lie within the subdisk $\\{ \\boldsymbol{k} : \\| \\boldsymbol{k} \\| \\leq r \\}$, expressed in terms of $M$, $R$, $r$, $k_{c}$, and $K$.\n\nYour final answer must be a single closed-form analytical expression. No units are required.",
            "solution": "The problem asks for the expected number of acquired $k$-space samples that fall within a subdisk of radius $r$. Let this quantity be denoted by $E[N_r]$.\n\nThe $N_{\\text{acq}}$ samples are drawn independently from the probability density function $p(\\boldsymbol{k})$. Let $X_i$ be an indicator random variable for the $i$-th acquired sample, where $i = 1, \\dots, N_{\\text{acq}}$. We define $X_i = 1$ if the $i$-th sample $\\boldsymbol{k}_i$ falls within the subdisk $D_r=\\{ \\boldsymbol{k} : \\| \\boldsymbol{k} \\| \\leq r \\}$, and $X_i = 0$ otherwise. The total number of samples within $D_r$ is $N_r = \\sum_{i=1}^{N_{\\text{acq}}} X_i$.\n\nBy the linearity of expectation, the expected number of samples in $D_r$ is:\n$$\nE[N_r] = E\\left[\\sum_{i=1}^{N_{\\text{acq}}} X_i\\right] = \\sum_{i=1}^{N_{\\text{acq}}} E[X_i]\n$$\nThe expectation of an indicator variable is the probability of the event it indicates. For any sample $i$, this probability is the same and is given by the integral of the PDF $p(\\boldsymbol{k})$ over the subdisk $D_r$. Let us denote this probability as $P_r$.\n$$\nP_r = \\text{Prob}(\\boldsymbol{k}_i \\in D_r) = \\int_{D_r} p(\\boldsymbol{k}) d^2\\boldsymbol{k}\n$$\nThus, $E[X_i] = P_r$ for all $i$, and the expected number of samples is:\n$$\nE[N_r] = \\sum_{i=1}^{N_{\\text{acq}}} P_r = N_{\\text{acq}} P_r = \\frac{M}{R} P_r\n$$\nTo find $E[N_r]$, we must first determine the normalized PDF $p(\\boldsymbol{k})$ and then compute the probability $P_r$.\n\n**1. Normalization of the Probability Density Function**\nThe PDF is given as $p(\\boldsymbol{k}) = C \\cdot \\left(1 + \\left( \\frac{\\| \\boldsymbol{k} \\|}{k_{c}} \\right)^{2}\\right)^{-1}$ for $\\| \\boldsymbol{k} \\| \\leq K$, where $C$ is a normalization constant. The constant $C$ is determined by the condition $\\int_{D_K} p(\\boldsymbol{k}) d^2\\boldsymbol{k} = 1$. Since the integrand is radially symmetric, we convert to polar coordinates, where $\\boldsymbol{k} = (k_r, \\theta)$, $\\| \\boldsymbol{k} \\| = k_r$, and the area element is $d^2\\boldsymbol{k} = k_r dk_r d\\theta$.\n$$\n\\int_{0}^{2\\pi} \\int_{0}^{K} C \\frac{1}{1 + \\left(\\frac{k_r}{k_{c}}\\right)^{2}} k_r dk_r d\\theta = 1\n$$\nThe integral over $\\theta$ from $0$ to $2\\pi$ yields a factor of $2\\pi$.\n$$\n2\\pi C \\int_{0}^{K} \\frac{k_r}{1 + \\frac{k_r^2}{k_c^2}} dk_r = 1\n$$\nTo solve the integral, we use the substitution $u = 1 + \\frac{k_r^2}{k_c^2}$. The differential is $du = \\frac{2k_r}{k_c^2} dk_r$, which implies $k_r dk_r = \\frac{k_c^2}{2} du$. The limits of integration change from $k_r=0$ to $u=1$ and from $k_r=K$ to $u=1 + (K/k_c)^2$.\n$$\n\\int_{0}^{K} \\frac{k_r}{1 + \\frac{k_r^2}{k_c^2}} dk_r = \\int_{1}^{1 + (K/k_c)^2} \\frac{1}{u} \\left(\\frac{k_c^2}{2} du\\right) = \\frac{k_c^2}{2} [\\ln(u)]_{1}^{1 + (K/k_c)^2}\n$$\n$$\n= \\frac{k_c^2}{2} \\left[ \\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right) - \\ln(1) \\right] = \\frac{k_c^2}{2} \\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right)\n$$\nSubstituting this back into the normalization condition:\n$$\n2\\pi C \\left[ \\frac{k_c^2}{2} \\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right) \\right] = 1 \\implies \\pi C k_c^2 \\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right) = 1\n$$\nSolving for $C$:\n$$\nC = \\frac{1}{\\pi k_c^2 \\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right)}\n$$\n\n**2. Calculation of the Probability $P_r$**\nNow we compute the probability $P_r$ that a sample lies within the subdisk $D_r$ of radius $r$.\n$$\nP_r = \\int_{D_r} p(\\boldsymbol{k}) d^2\\boldsymbol{k} = \\int_{0}^{2\\pi} \\int_{0}^{r} C \\frac{1}{1 + \\left(\\frac{k_r}{k_{c}}\\right)^{2}} k_r dk_r d\\theta\n$$\nThis integral is identical in form to the normalization integral but with the upper limit for a radius of $r$ instead of $K$. Using the same integration procedure, we find:\n$$\nP_r = 2\\pi C \\left[ \\frac{k_c^2}{2} \\ln\\left(1 + \\left(\\frac{r}{k_c}\\right)^2\\right) \\right] = \\pi C k_c^2 \\ln\\left(1 + \\left(\\frac{r}{k_c}\\right)^2\\right)\n$$\nNow, we substitute the expression for the normalization constant $C$:\n$$\nP_r = \\frac{1}{\\pi k_c^2 \\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right)} \\cdot \\pi k_c^2 \\ln\\left(1 + \\left(\\frac{r}{k_c}\\right)^2\\right)\n$$\nThe terms $\\pi k_c^2$ cancel, yielding the elegant result:\n$$\nP_r = \\frac{\\ln\\left(1 + \\left(\\frac{r}{k_c}\\right)^2\\right)}{\\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right)}\n$$\nThis expression is valid for $0 \\le r \\le K$. It correctly gives $P_r=0$ for $r=0$ and $P_r=1$ for $r=K$.\n\n**3. Final Expression for the Expected Number of Samples**\nFinally, we compute the expected number of samples $E[N_r]$ by substituting the expression for $P_r$ into the equation $E[N_r] = (M/R) P_r$.\n$$\nE[N_r] = \\frac{M}{R} \\cdot \\frac{\\ln\\left(1 + \\left(\\frac{r}{k_c}\\right)^2\\right)}{\\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right)}\n$$\nThis is the final closed-form expression for the expected number of acquired samples within the subdisk of radius $r$, expressed in terms of the given parameters $M, R, r, k_c$, and $K$.",
            "answer": "$$\n\\boxed{\\frac{M}{R} \\frac{\\ln\\left(1 + \\left(\\frac{r}{k_c}\\right)^2\\right)}{\\ln\\left(1 + \\left(\\frac{K}{k_c}\\right)^2\\right)}}\n$$"
        }
    ]
}