{
    "hands_on_practices": [
        {
            "introduction": "Medical imaging archives and teleradiology systems handle immense volumes of data, making efficient data compression a necessity for storage and network transmission. However, in medicine, the integrity of the diagnostic information is paramount, meaning compression cannot compromise diagnostic accuracy. This practice places you in the role of an imaging informatics specialist tasked with selecting a compression strategy, forcing a critical evaluation of the trade-offs between lossless and lossy methods. By analyzing a hypothetical but realistic clinical scenario , you will learn to connect abstract technical metrics like the Peak Signal-to-Noise Ratio ($PSNR$) to concrete diagnostic requirements, such as detecting low-contrast lesions or subtle microcalcifications.",
            "id": "4894579",
            "problem": "A hospital is standardizing its medical imaging archive and teleradiology pipeline across two modalities: chest computed tomography (CT) and screening mammography. Each chest CT study has approximately $512 \\times 512 \\times 300$ voxels stored at $12$-bit depth. Each mammogram has approximately $4096 \\times 4096$ pixels at $14$-bit depth. The imaging informatics team must choose compression configurations that satisfy clinical constraints for primary diagnostic interpretation while managing storage and network demands.\n\nThree candidate configurations are considered:\n- Configuration X: Joint Photographic Experts Group Lossless Standard (JPEG-LS) in strictly lossless mode, achieving an average compression ratio of approximately $2.0{:}1$ across both modalities.\n- Configuration Y: Joint Photographic Experts Group $2000$ (JPEG2000) in irreversible (lossy) mode at approximately $0.5$ bits per pixel for CT and $0.8$ bits per pixel for mammography. On a validation set, the measured Peak Signal-to-Noise Ratio (PSNR) is approximately $46$ dB for CT and $42$ dB for mammography. Radiologists note weak wavelet ringing near sharp edges and microcalcifications, but no obvious blocking artifacts.\n- Configuration Z: JPEG2000 in reversible (lossless) mode using an integer wavelet transform, achieving an average compression ratio of approximately $1.8{:}1$.\n\nClinical constraints given by the radiology leadership:\n- For CT, primary diagnostic reads must preserve quantitative Hounsfield unit measurements (assume the stored $12$-bit range maps $1$ stored gray level to approximately $1$ Hounsfield unit) and must not degrade detectability of low-contrast liver lesions on the order of $5$ Hounsfield units above background.\n- For mammography, primary diagnostic reads must not introduce artifacts that could mimic or obscure microcalcifications on the order of $0.2$ mm, and any compression must be either strictly lossless or demonstrably “visually lossless” with artifact profiles that do not alter detection performance.\n\nStarting from first principles and core definitions:\n- Lossless compression exactly preserves pixel values under decompression, whereas lossy compression is not exactly invertible and introduces distortion.\n- Peak Signal-to-Noise Ratio (PSNR) is defined from the mean squared error (MSE) and the maximum possible pixel value in the image’s bit depth.\n\nAssume the maximum possible pixel value equals $4095$ for $12$-bit CT and $16383$ for $14$-bit mammography. Using these definitions and the scenario above, which option best justifies a safe, clinically appropriate choice for primary diagnostic interpretation across both modalities while correctly distinguishing lossless versus lossy methods and appropriately interpreting PSNR and artifact profiles?\n\nA. Prefer JPEG-LS (lossless) for CT primary reads to preserve exact Hounsfield units, and avoid irreversible JPEG2000 for CT even at PSNR around $46$ dB because the induced mean squared error may be on the order of tens of gray levels (Hounsfield units), potentially degrading $5$ Hounsfield unit lesion detectability; for mammography, reject irreversible JPEG2000 at PSNR around $42$ dB if wavelet ringing could mimic microcalcifications, and use a strictly lossless option (JPEG-LS or reversible JPEG2000). PSNR alone is insufficient to guarantee diagnostic safety; artifact profiles must be evaluated.\n\nB. Choose irreversible JPEG2000 for both CT and mammography because JPEG2000 is always lossy, and any PSNR above $40$ dB guarantees visually lossless performance; artifact profiles at such PSNR are negligible, so they can be ignored in clinical evaluation.\n\nC. Compression acceptability is modality independent, so a PSNR threshold of $30$ dB is sufficient for diagnostic imaging across CT and mammography; therefore, irreversible JPEG2000 at the given bit rates is safe for primary reads, and lossless methods are unnecessary.\n\nD. Lossless JPEG-LS at approximately $2{:}1$ is never justified for primary interpretation since the storage savings are small; at PSNR around $46$ dB the root-mean-square error must be below $1$ gray level, ensuring that $5$ Hounsfield unit lesion detectability is preserved, and wavelet ringing cannot plausibly confound microcalcification detection in mammography; therefore, choose irreversible JPEG2000 for both modalities.",
            "solution": "The user wants me to act as a meticulous STEM professor to solve a problem about medical image compression.\n\n### Step 1: Extract Givens\n- **Chest CT Data:**\n  - Dimensions: $512 \\times 512 \\times 300$ voxels\n  - Bit depth: $12$-bit\n- **Screening Mammography Data:**\n  - Dimensions: $4096 \\times 4096$ pixels\n  - Bit depth: $14$-bit\n- **Configuration X (JPEG-LS Lossless):**\n  - Compression Type: Strictly lossless\n  - Average Compression Ratio: $\\approx 2.0{:}1$\n- **Configuration Y (JPEG2000 Irreversible):**\n  - Compression Type: Lossy\n  - CT Bit Rate: $\\approx 0.5$ bits per pixel\n  - Mammography Bit Rate: $\\approx 0.8$ bits per pixel\n  - CT PSNR: $\\approx 46$ dB\n  - Mammography PSNR: $\\approx 42$ dB\n  - Artifacts: Weak wavelet ringing near sharp edges and microcalcifications; no obvious blocking artifacts.\n- **Configuration Z (JPEG2000 Reversible):**\n  - Compression Type: Lossless (using integer wavelet transform)\n  - Average Compression Ratio: $\\approx 1.8{:}1$\n- **Clinical Constraints (CT):**\n  - Must preserve quantitative Hounsfield unit (HU) measurements.\n  - Assumption: $1$ stored gray level $\\approx 1$ HU.\n  - Must not degrade detectability of low-contrast lesions of $\\approx 5$ HU.\n- **Clinical Constraints (Mammography):**\n  - Must not introduce artifacts that mimic or obscure microcalcifications ($\\approx 0.2$ mm).\n  - Compression must be strictly lossless or \"visually lossless\" with non-altering artifact profiles.\n- **First Principles & Assumptions:**\n  - Lossless compression is perfectly invertible; lossy compression is not.\n  - Peak Signal-to-Noise Ratio (PSNR) is defined as $PSNR = 10 \\log_{10} \\left( \\frac{MAX_I^2}{MSE} \\right)$, where $MSE$ is the mean squared error and $MAX_I$ is the maximum possible pixel value.\n  - For $12$-bit CT, $MAX_I = 2^{12} - 1 = 4095$.\n  - For $14$-bit mammography, $MAX_I = 2^{14} - 1 = 16383$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a realistic scenario in medical imaging informatics, involving a trade-off analysis between different image compression standards (JPEG-LS, JPEG2000), modes (lossless, lossy), and their impact on diagnostic quality for specific clinical tasks in CT and mammography.\n\n- **Scientifically Grounded:** The technical specifications (bit depths, image dimensions, compression ratios, PSNR values, bit rates) are plausible and consistent with real-world applications. The compression standards mentioned are widely used in medicine. The clinical constraints (HU preservation in CT, microcalcification detection in mammography) are standard and critical considerations. The provided definitions of lossless/lossy compression and PSNR are correct.\n- **Well-Posed:** The problem provides a clear set of configurations and constraints and asks for an evaluation to determine the most appropriate choice. The question is structured to test the understanding of the interplay between technical metrics and clinical requirements.\n- **Objective:** The language is technical and unbiased. It presents data and constraints without subjective framing.\n- **Completeness:** The problem is self-contained. It provides all necessary data (PSNR values, max pixel values, clinical needs) to perform a quantitative and qualitative analysis of the options.\n\nThe problem does not violate any of the invalidity criteria. It is a scientifically sound, well-posed, and objective problem rooted in a real-world engineering and clinical challenge.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed with the detailed analysis and solution.\n\n### Principle-Based Derivation\n\nThe core of the problem is to determine which compression configuration is safe for primary diagnostic interpretation, given the stringent clinical constraints.\n\n1.  **Analyze Lossless vs. Lossy Compression in Context:**\n    - **Lossless Compression (Configurations X and Z):** By definition, lossless compression schemes (JPEG-LS and reversible JPEG2000) guarantee that the decompressed image is bit-for-bit identical to the original. This means that for CT, every Hounsfield unit value is perfectly preserved. For mammography, no artifacts of any kind are introduced. Therefore, both Configuration X and Configuration Z are inherently safe and satisfy the clinical constraints for both modalities. The choice between them would depend on secondary factors like compression efficiency (X is slightly better at $2.0{:}1$ vs. Z at $1.8{:}1$) or implementation details, but both are clinically acceptable.\n\n2.  **Analyze Lossy Compression (Configuration Y) and its Clinical Impact:**\n    - **General Principle:** Lossy compression introduces errors. Its acceptability for primary diagnosis is not guaranteed and must be rigorously proven for each specific clinical task. Global quality metrics like PSNR can be misleading.\n    - **CT Analysis:** The clinical constraint is the preservation of quantitative HU values and the detection of $5$ HU low-contrast lesions. We must assess if the error introduced by Configuration Y is compatible with this constraint. We use the given PSNR of $46$ dB.\n        The relationship between PSNR and Mean Squared Error (MSE) is:\n        $$PSNR = 10 \\log_{10} \\left( \\frac{MAX_I^2}{MSE} \\right)$$\n        For the $12$-bit CT data, $MAX_I = 4095$. We can solve for the MSE:\n        $$46 = 10 \\log_{10} \\left( \\frac{4095^2}{MSE_{CT}} \\right)$$\n        $$4.6 = \\log_{10} \\left( \\frac{16769025}{MSE_{CT}} \\right)$$\n        $$10^{4.6} = \\frac{16769025}{MSE_{CT}}$$\n        $$MSE_{CT} = \\frac{16769025}{10^{4.6}} \\approx \\frac{16769025}{39810.7} \\approx 421.2$$\n        The Root Mean Squared Error (RMSE) is the square root of the MSE:\n        $$RMSE_{CT} = \\sqrt{MSE_{CT}} \\approx \\sqrt{421.2} \\approx 20.5$$\n        Assuming $1$ gray level corresponds to $1$ HU, the RMSE is approximately $20.5$ HU. The RMSE represents the standard deviation of the compression errors. This means that, on average, the error in pixel values is on the order of $20.5$ HU. An average error of this magnitude is four times larger than the $5$ HU signal of the low-contrast lesion that must be detected. Such a high level of noise would unequivocally degrade, and likely obliterate, the visibility of such lesions. Therefore, Configuration Y is unacceptable for primary diagnostic CT reads under the given constraints.\n\n    - **Mammography Analysis:** The clinical constraint is to avoid introducing artifacts that mimic or obscure microcalcifications. The problem explicitly states that radiologists observed \"weak wavelet ringing near sharp edges and microcalcifications\" with Configuration Y. This is a direct violation of the clinical constraint. Wavelet-based compression schemes like JPEG2000 are known to produce ringing artifacts around high-frequency features. Microcalcifications are high-frequency features. Ringing can either mask a true microcalcification or create a \"pseudo-calcification,\" leading to false negatives or false positives, respectively. Even though the PSNR is high ($42$ dB), the specific nature (profile) of the artifact makes it diagnostically unacceptable for this task. The general principle that PSNR alone is insufficient to guarantee diagnostic safety is paramount here.\n\n**Conclusion:** Based on the analysis, only lossless compression methods (Configurations X and Z) are appropriate and safe for primary diagnosis for both modalities given the strict clinical requirements. Lossy compression (Configuration Y) fails the test for CT due to unacceptable quantitative error and for mammography due to the presence of diagnostically confounding artifacts.\n\n### Option-by-Option Analysis\n\n**A. Prefer JPEG-LS (lossless) for CT primary reads to preserve exact Hounsfield units, and avoid irreversible JPEG2000 for CT even at PSNR around $46$ dB because the induced mean squared error may be on the order of tens of gray levels (Hounsfield units), potentially degrading $5$ Hounsfield unit lesion detectability; for mammography, reject irreversible JPEG2000 at PSNR around $42$ dB if wavelet ringing could mimic microcalcifications, and use a strictly lossless option (JPEG-LS or reversible JPEG2000). PSNR alone is insufficient to guarantee diagnostic safety; artifact profiles must be evaluated.**\n- **Evaluation:** This statement is fully consistent with our derivation.\n  - It correctly prioritizes lossless compression for quantitative CT.\n  - It correctly deduces that a $46$ dB PSNR in this context leads to an error (RMSE of $\\approx 20.5$ HU, which is 'on the order of tens of gray levels') that is too large for detecting a $5$ HU lesion.\n  - It correctly rejects lossy JPEG2000 for mammography based on the specific artifact profile (wavelet ringing) and its potential to interfere with microcalcification detection.\n  - It correctly advocates for a strictly lossless option for mammography.\n  - It correctly states the principle that PSNR is insufficient and artifact profiles are critical.\n- **Verdict:** **Correct**.\n\n**B. Choose irreversible JPEG2000 for both CT and mammography because JPEG2000 is always lossy, and any PSNR above $40$ dB guarantees visually lossless performance; artifact profiles at such PSNR are negligible, so they can be ignored in clinical evaluation.**\n- **Evaluation:** This statement contains multiple falsehoods.\n  - \"JPEG2000 is always lossy\" is factually incorrect. Configuration Z explicitly describes JPEG2000 in a reversible (lossless) mode.\n  - \"any PSNR above $40$ dB guarantees visually lossless performance\" is a dangerous and incorrect overgeneralization. As calculated, $46$ dB leads to significant quantitative error in CT, and 'visually lossless' is task-dependent.\n  - \"artifact profiles...are negligible\" is directly contradicted by the problem statement, which notes that radiologists observed wavelet ringing.\n- **Verdict:** **Incorrect**.\n\n**C. Compression acceptability is modality independent, so a PSNR threshold of $30$ dB is sufficient for diagnostic imaging across CT and mammography; therefore, irreversible JPEG2000 at the given bit rates is safe for primary reads, and lossless methods are unnecessary.**\n- **Evaluation:** This statement is based on flawed premises.\n  - \"Compression acceptability is modality independent\" is fundamentally wrong. Clinical requirements are specific to the imaging modality and the diagnostic question.\n  - \"a PSNR threshold of $30$ dB is sufficient\" is an arbitrary, unsourced, and dangerously low value for primary diagnosis.\n  - The conclusion that irreversible JPEG2000 is safe and lossless methods are unnecessary is incorrect, as demonstrated by the analysis above.\n- **Verdict:** **Incorrect**.\n\n**D. Lossless JPEG-LS at approximately $2{:}1$ is never justified for primary interpretation since the storage savings are small; at PSNR around $46$ dB the root-mean-square error must be below $1$ gray level, ensuring that $5$ Hounsfield unit lesion detectability is preserved, and wavelet ringing cannot plausibly confound microcalcification detection in mammography; therefore, choose irreversible JPEG2000 for both modalities.**\n- **Evaluation:** This statement contains a critical factual error and flawed reasoning.\n  - The claim that $2{:}1$ lossless compression is \"never justified\" is a poor cost-benefit argument that wrongly dismisses the paramount importance of diagnostic integrity.\n  - The statement that at $46$ dB PSNR, \"the root-mean-square error must be below $1$ gray level\" is factually incorrect. Our calculation showed the RMSE is $\\approx 20.5$ gray levels. An RMSE below $1$ would require a PSNR $> 72$ dB.\n  - The assurance that $5$ HU detectability is preserved is based on this false premise.\n  - The claim that \"wavelet ringing cannot plausibly confound\" detection is an unsubstantiated assertion that contradicts the radiologists' observations.\n- **Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The era of \"big data\" in medical imaging is characterized by the move from qualitative visual assessment to quantitative analysis. Radiomics, the process of extracting large numbers of quantitative features from medical images, aims to uncover biomarkers that can aid in diagnosis, prognosis, and treatment response prediction. For any such feature to be a reliable biomarker, it must be stable and reproducible. This exercise  provides a hands-on opportunity to assess the inter-scan variability of a radiomics feature by calculating its coefficient of variation ($CV$), a fundamental statistical measure of repeatability.",
            "id": "4894554",
            "problem": "A radiomics feature such as normalized texture entropy is repeatedly measured from a single patient under identical acquisition and reconstruction protocols to assess inter-scan variability. Let the repeated measures be $n=6$ independent and identically distributed draws from a stationary process, with observed values $x_{1}=0.982$, $x_{2}=0.996$, $x_{3}=1.009$, $x_{4}=0.991$, $x_{5}=1.015$, and $x_{6}=1.003$. Starting from the fundamental definitions of the sample mean $\\bar{x}$ and the unbiased sample variance $s^{2}$, derive and compute the coefficient of variation for these repeated measures as a single closed-form expression in radicals and integers, and report it exactly (do not convert to a decimal). Then, based on first principles and widely used practice in quantitative imaging, briefly justify what numerical threshold for the coefficient of variation might be considered acceptable for clinical biomarker repeatability and whether the computed variability meets such a threshold. The final answer must be a single closed-form analytic expression for the coefficient of variation without units.",
            "solution": "The problem requires the derivation and computation of the coefficient of variation ($CV$) for a set of repeated measurements of a radiomics feature, and a subsequent discussion of its acceptability in a clinical context.\n\nFirst, we validate the problem statement.\nThe problem is well-defined, providing a set of $n=6$ numerical data points: $x_{1}=0.982$, $x_{2}=0.996$, $x_{3}=1.009$, $x_{4}=0.991$, $x_{5}=1.015$, and $x_{6}=1.003$. It asks for the calculation of a standard statistical quantity, the coefficient of variation, based on fundamental definitions. The context of radiomics feature repeatability is scientifically grounded and relevant to the field of quantitative medical imaging. There are no contradictions, missing data, or scientifically unsound premises. Therefore, the problem is deemed valid.\n\nThe coefficient of variation ($CV$) is defined as the ratio of the sample standard deviation ($s$) to the absolute value of the sample mean ($\\bar{x}$):\n$$CV = \\frac{s}{|\\bar{x}|}$$\n\nWe begin by computing the sample mean, $\\bar{x}$, from the given data. The sample mean is defined as:\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\nGiven $n=6$, the sum of the observed values is:\n$$\\sum_{i=1}^{6} x_i = 0.982 + 0.996 + 1.009 + 0.991 + 1.015 + 1.003 = 5.996$$\nTo maintain exact precision as requested, we work with fractions. The sum can be written as $\\frac{5996}{1000}$.\nThe sample mean is therefore:\n$$\\bar{x} = \\frac{1}{6} \\left(\\frac{5996}{1000}\\right) = \\frac{5996}{6000} = \\frac{1499 \\times 4}{1500 \\times 4} = \\frac{1499}{1500}$$\nSince $\\bar{x}$ is positive, $|\\bar{x}| = \\bar{x}$.\n\nNext, we compute the unbiased sample variance, $s^2$, defined as:\n$$s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\nWith $n=6$, the denominator is $n-1=5$. We must calculate the sum of the squared deviations from the mean, $\\sum_{i=1}^{6} (x_i - \\bar{x})^2$. We will perform these calculations using fractions to preserve exactness. We use a common denominator of $3000$ for the data points $x_i$ and the mean $\\bar{x} = \\frac{2998}{3000}$.\n\nThe deviations $(x_i - \\bar{x})$ are:\n$x_1 - \\bar{x} = \\frac{2946}{3000} - \\frac{2998}{3000} = -\\frac{52}{3000}$\n$x_2 - \\bar{x} = \\frac{2988}{3000} - \\frac{2998}{3000} = -\\frac{10}{3000}$\n$x_3 - \\bar{x} = \\frac{3027}{3000} - \\frac{2998}{3000} = \\frac{29}{3000}$\n$x_4 - \\bar{x} = \\frac{2973}{3000} - \\frac{2998}{3000} = -\\frac{25}{3000}$\n$x_5 - \\bar{x} = \\frac{3045}{3000} - \\frac{2998}{3000} = \\frac{47}{3000}$\n$x_6 - \\bar{x} = \\frac{3009}{3000} - \\frac{2998}{3000} = \\frac{11}{3000}$\n\nThe sum of squared deviations is:\n$$\\sum_{i=1}^{6} (x_i - \\bar{x})^2 = \\left(-\\frac{52}{3000}\\right)^2 + \\left(-\\frac{10}{3000}\\right)^2 + \\left(\\frac{29}{3000}\\right)^2 + \\left(-\\frac{25}{3000}\\right)^2 + \\left(\\frac{47}{3000}\\right)^2 + \\left(\\frac{11}{3000}\\right)^2$$\n$$\\sum_{i=1}^{6} (x_i - \\bar{x})^2 = \\frac{1}{3000^2} \\left((-52)^2 + (-10)^2 + 29^2 + (-25)^2 + 47^2 + 11^2\\right)$$\n$$\\sum_{i=1}^{6} (x_i - \\bar{x})^2 = \\frac{1}{9000000} \\left(2704 + 100 + 841 + 625 + 2209 + 121\\right) = \\frac{6600}{9000000} = \\frac{66}{90000} = \\frac{11}{15000}$$\nNow we can compute the unbiased sample variance:\n$$s^2 = \\frac{1}{5} \\left(\\frac{11}{15000}\\right) = \\frac{11}{75000}$$\n\nThe sample standard deviation, $s$, is the square root of the variance:\n$$s = \\sqrt{\\frac{11}{75000}} = \\frac{\\sqrt{11}}{\\sqrt{75000}} = \\frac{\\sqrt{11}}{\\sqrt{2500 \\times 30}} = \\frac{\\sqrt{11}}{50\\sqrt{30}}$$\nTo rationalize the denominator, we multiply the numerator and denominator by $\\sqrt{30}$:\n$$s = \\frac{\\sqrt{11} \\sqrt{30}}{50\\sqrt{30}\\sqrt{30}} = \\frac{\\sqrt{330}}{50 \\times 30} = \\frac{\\sqrt{330}}{1500}$$\n\nFinally, we compute the coefficient of variation:\n$$CV = \\frac{s}{\\bar{x}} = \\frac{\\frac{\\sqrt{330}}{1500}}{\\frac{1499}{1500}} = \\frac{\\sqrt{330}}{1499}$$\nThis is the single closed-form expression for the coefficient of variation.\n\nRegarding the second part of the question, in quantitative imaging, a biomarker's utility is critically dependent on its repeatability. The coefficient of variation is a key metric for this assessment. While there is no universal threshold, a common goalpost, advocated by bodies like the Quantitative Imaging Biomarkers Alliance (QIBA), is for the within-subject coefficient of variation ($wCV$) to be sufficiently small relative to the biological or pathological changes the biomarker is intended to measure. For many applications, particularly in oncology where treatment response may involve changes of $20\\%$ to $30\\%$, a $wCV$ of less than $10\\%$ to $15\\%$ is often considered acceptable or good. A value above $20\\%$ would generally be deemed poor.\nThe computed value is $CV = \\frac{\\sqrt{330}}{1499} \\approx \\frac{18.166}{1499} \\approx 0.0121$, which corresponds to $1.21\\%$. This level of variability is exceptionally low and would be considered excellent, indicating high repeatability for the radiomics feature under the tested conditions. It comfortably meets any reasonable threshold for clinical biomarker qualification.",
            "answer": "$$\\boxed{\\frac{\\sqrt{330}}{1499}}$$"
        },
        {
            "introduction": "Machine learning and artificial intelligence systems are becoming integral to modern radiology, offering powerful tools for tasks like disease detection and workflow prioritization. However, evaluating these models requires a nuanced understanding of performance metrics, as simple accuracy can be dangerously misleading, especially in medicine where diseases are often rare. This practice  delves into the foundational tools of classifier evaluation, including Receiver Operating Characteristic ($ROC$) and Precision-Recall ($PR$) curves. You will explore from first principles why and when one type of curve may be more informative than another, a critical skill for anyone looking to deploy or interpret AI in a clinical setting.",
            "id": "4894610",
            "problem": "A hospital is deploying a machine learning system to prioritize follow-up for suspected pulmonary embolism in computed tomography pulmonary angiography exams. The system outputs a continuous score for each case that is intended to reflect the likelihood of disease. Let the binary label be $Y \\in \\{0,1\\}$, the score be $S \\in [0,1]$, the operating threshold be $\\tau \\in [0,1]$, and the predicted label be $\\hat{Y}(\\tau) = \\mathbb{1}\\{S \\ge \\tau\\}$. The disease prevalence in the screening population is $\\pi = P(Y=1)$, which is small. The imaging informatics team considers three families of evaluation curves: Receiver Operating Characteristic (ROC), Precision–Recall (PR), and calibration (reliability) curves. They also summarize discrimination with Area Under the Curve (AUC).\n\nFrom first principles in binary decision theory, choose all statements that are correct about what these curves are, what they measure, and under what conditions PR curves are more informative than ROC for rare disease detection.\n\nA. The Receiver Operating Characteristic (ROC) curve is the locus of points $(\\text{TPR}(\\tau), \\text{FPR}(\\tau))$ as $\\tau$ varies, where $\\text{TPR}(\\tau) = P(\\hat{Y}(\\tau) = 1 \\mid Y = 1)$ and $\\text{FPR}(\\tau) = P(\\hat{Y}(\\tau) = 1 \\mid Y = 0)$. Because these are conditional probabilities on $Y$, the ROC coordinates do not explicitly depend on the prevalence $\\pi$.\n\nB. The Area Under the Curve (AUC) associated with the Receiver Operating Characteristic (ROC) equals the probability that a randomly chosen positive case has a higher score than a randomly chosen negative case, that is $P(S_{+} > S_{-})$, where $S_{+}$ and $S_{-}$ denote independent draws from the score distributions for $Y=1$ and $Y=0$, respectively.\n\nC. The Precision–Recall (PR) curve plots $\\text{Precision}(\\tau)$ versus $\\text{Recall}(\\tau)$ as $\\tau$ varies, where $\\text{Precision}(\\tau) = P(Y=1 \\mid \\hat{Y}(\\tau)=1)$ and $\\text{Recall}(\\tau) = \\text{TPR}(\\tau)$. For a non-informative ranking that yields $\\text{TPR}(\\tau) = \\text{FPR}(\\tau)$ at all thresholds, the expected precision equals the prevalence $\\pi$. When $\\pi$ is small (rare disease), PR curves are more informative than ROC because precision explicitly reflects the impact of prevalence on positive predictive value, which is critical for clinical utility.\n\nD. A calibration (reliability) curve assesses whether predicted probabilities are numerically consistent with empirical event frequencies. It plots binned average predicted probability $\\hat{p}$ versus the observed fraction $f$ of positives in each bin; perfect calibration lies on the identity line $f = \\hat{p}$.\n\nE. In rare disease detection, ROC curves become misleading because $\\text{TPR}(\\tau)$ numerically depends on the prevalence $\\pi$, which causes ROC performance to inflate or deflate as $\\pi$ changes even if the score distributions remain the same.\n\nF. A high Area Under the Curve (AUC) guarantees that a model is well calibrated across the entire range of predicted probabilities.\n\nG. Precision is invariant to prevalence $\\pi$, so Precision–Recall (PR) curves are less useful than Receiver Operating Characteristic (ROC) curves when classes are imbalanced.",
            "solution": "The problem asks us to evaluate several statements concerning the evaluation of a binary classification model for a rare disease, based on first principles of decision theory. The model outputs a continuous score $S \\in [0,1]$ for a true binary label $Y \\in \\{0,1\\}$, where $Y=1$ indicates disease. The prevalence of the disease is $\\pi = P(Y=1)$, and it is small. A predicted label $\\hat{Y}(\\tau) = \\mathbb{1}\\{S \\ge \\tau\\}$ is obtained by thresholding the score $S$ at a level $\\tau$.\n\nWe will define the key metrics from first principles and then analyze each statement.\n\nThe fundamental quantities are the True Positive Rate (TPR), also known as Recall or Sensitivity, and the False Positive Rate (FPR).\n- True Positive Rate: $\\text{TPR}(\\tau) = P(\\hat{Y}(\\tau) = 1 \\mid Y = 1) = P(S \\ge \\tau \\mid Y = 1)$. This is the fraction of actual positives that are correctly identified as positive.\n- False Positive Rate: $\\text{FPR}(\\tau) = P(\\hat{Y}(\\tau) = 1 \\mid Y = 0) = P(S \\ge \\tau \\mid Y = 0)$. This is the fraction of actual negatives that are incorrectly identified as positive. $1 - \\text{FPR}(\\tau)$ is the Specificity.\n\nAnother crucial metric is Precision, also known as Positive Predictive Value (PPV).\n- Precision: $\\text{Precision}(\\tau) = P(Y=1 \\mid \\hat{Y}(\\tau) = 1) = P(Y=1 \\mid S \\ge \\tau)$. This is the fraction of positive predictions that are actually correct.\n\nUsing Bayes' theorem, we can express Precision in terms of TPR, FPR, and prevalence $\\pi$:\n$$ \\text{Precision}(\\tau) = \\frac{P(S \\ge \\tau \\mid Y=1) P(Y=1)}{P(S \\ge \\tau)} $$\nThe denominator is the overall probability of a positive prediction, which can be expanded using the law of total probability:\n$$ P(S \\ge \\tau) = P(S \\ge \\tau \\mid Y=1)P(Y=1) + P(S \\ge \\tau \\mid Y=0)P(Y=0) $$\n$$ P(S \\ge \\tau) = \\text{TPR}(\\tau)\\pi + \\text{FPR}(\\tau)(1-\\pi) $$\nSubstituting this back into the expression for Precision gives:\n$$ \\text{Precision}(\\tau) = \\frac{\\text{TPR}(\\tau)\\pi}{\\text{TPR}(\\tau)\\pi + \\text{FPR}(\\tau)(1-\\pi)} $$\n\nWith these definitions, we can now evaluate each statement.\n\n**A. The Receiver Operating Characteristic (ROC) curve is the locus of points $(\\text{TPR}(\\tau), \\text{FPR}(\\tau))$ as $\\tau$ varies, where $\\text{TPR}(\\tau) = P(\\hat{Y}(\\tau) = 1 \\mid Y = 1)$ and $\\text{FPR}(\\tau) = P(\\hat{Y}(\\tau) = 1 \\mid Y = 0)$. Because these are conditional probabilities on $Y$, the ROC coordinates do not explicitly depend on the prevalence $\\pi$.**\n- The statement first provides the correct definition of the ROC curve as a plot of $\\text{TPR}$ vs. $\\text{FPR}$ for all possible thresholds $\\tau$.\n- The definitions given for TPR and FPR are $\\text{TPR}(\\tau) = P(S \\ge \\tau \\mid Y=1)$ and $\\text{FPR}(\\tau) = P(S \\ge \\tau \\mid Y=0)$.\n- These probabilities are conditioned on the true class label, $Y=1$ or $Y=0$. Therefore, their values depend only on the distribution of the score $S$ within the positive class and the negative class, respectively.\n- They do not depend on the relative proportions of the positive and negative classes in the overall population, which is the prevalence $\\pi = P(Y=1)$.\n- Therefore, the coordinates of the ROC curve are indeed independent of prevalence.\n- **Verdict: Correct.**\n\n**B. The Area Under the Curve (AUC) associated with the Receiver Operating Characteristic (ROC) equals the probability that a randomly chosen positive case has a higher score than a randomly chosen negative case, that is $P(S_{+} > S_{-})$, where $S_{+}$ and $S_{-}$ denote independent draws from the score distributions for $Y=1$ and $Y=0$, respectively.**\n- Geometrically, the Area Under the ROC curve (AUC-ROC) is given by $\\int_0^1 \\text{TPR}(x) dx$, where the x-axis is the $\\text{FPR}$.\n- Let $f_1(s)$ and $f_0(s)$ be the probability density functions of the score $S$ for the positive ($Y=1$) and negative ($Y=0$) classes, respectively.\n- $\\text{TPR}(\\tau) = \\int_{\\tau}^{\\infty} f_1(s) ds$ and $\\text{FPR}(\\tau) = \\int_{\\tau}^{\\infty} f_0(s) ds$.\n- The AUC can be calculated by integrating $\\text{TPR}$ with respect to $\\text{FPR}$. As $\\tau$ goes from $\\infty$ to $-\\infty$, $\\text{FPR}$ goes from $0$ to $1$. Using $\\tau$ as the integration variable, we have $d(\\text{FPR}(\\tau)) = -f_0(\\tau)d\\tau$.\n- $\\text{AUC} = \\int_{x=0}^{1} \\text{TPR} \\,d\\text{FPR} = \\int_{\\tau=\\infty}^{-\\infty} \\text{TPR}(\\tau) (-f_0(\\tau)d\\tau) = \\int_{-\\infty}^{\\infty} \\text{TPR}(\\tau) f_0(\\tau) d\\tau$.\n- Now consider the probability $P(S_{+} > S_{-})$, where $S_{+} \\sim f_1(s)$ and $S_{-} \\sim f_0(s)$ are independent. We can compute this by conditioning on the value of $S_{-}$:\n$$ P(S_{+} > S_{-}) = \\int_{-\\infty}^{\\infty} P(S_{+} > s_{-} \\mid S_{-}=s_{-}) f_0(s_{-}) ds_{-} $$\n- Since $S_{+}$ is independent of $S_{-}$, $P(S_{+} > s_{-} \\mid S_{-}=s_{-}) = P(S_{+} > s_{-}) = \\int_{s_{-}}^{\\infty} f_1(s) ds = \\text{TPR}(s_{-})$.\n- Substituting this back, we get $P(S_{+} > S_{-}) = \\int_{-\\infty}^{\\infty} \\text{TPR}(s_{-}) f_0(s_{-}) ds_{-}$.\n- This is precisely the expression for the AUC. This interpretation is a standard result from non-parametric statistics (related to the Wilcoxon-Mann-Whitney test).\n- **Verdict: Correct.**\n\n**C. The Precision–Recall (PR) curve plots $\\text{Precision}(\\tau)$ versus $\\text{Recall}(\\tau)$ as $\\tau$ varies, where $\\text{Precision}(\\tau) = P(Y=1 \\mid \\hat{Y}(\\tau)=1)$ and $\\text{Recall}(\\tau) = \\text{TPR}(\\tau)$. For a non-informative ranking that yields $\\text{TPR}(\\tau) = \\text{FPR}(\\tau)$ at all thresholds, the expected precision equals the prevalence $\\pi$. When $\\pi$ is small (rare disease), PR curves are more informative than ROC for clinical utility because precision explicitly reflects the impact of prevalence on positive predictive value.**\n- The definitions of the PR curve, Precision, and Recall are all correct.\n- Consider a non-informative classifier for which the score provides no information about the class, meaning $\\text{TPR}(\\tau) = \\text{FPR}(\\tau)$ for any $\\tau$. Using the formula for precision derived earlier:\n$$ \\text{Precision}(\\tau) = \\frac{\\text{TPR}(\\tau)\\pi}{\\text{TPR}(\\tau)\\pi + \\text{FPR}(\\tau)(1-\\pi)} $$\n- Substituting $\\text{FPR}(\\tau) = \\text{TPR}(\\tau)$:\n$$ \\text{Precision}(\\tau) = \\frac{\\text{TPR}(\\tau)\\pi}{\\text{TPR}(\\tau)\\pi + \\text{TPR}(\\tau)(1-\\pi)} = \\frac{\\text{TPR}(\\tau)\\pi}{\\text{TPR}(\\tau)(\\pi + 1 - \\pi)} = \\frac{\\text{TPR}(\\tau)\\pi}{\\text{TPR}(\\tau)} = \\pi $$\n- So, for a random classifier, the precision is constant and equal to the prevalence $\\pi$. This serves as the baseline for PR curves. This part is correct.\n- For a rare disease, $\\pi$ is small. The precision formula shows that even if TPR is high and FPR is low, a small $\\pi$ can lead to a very low precision. For instance, if $\\pi = 0.01$, $\\text{TPR} = 0.9$, $\\text{FPR} = 0.1$, the precision is $\\frac{0.9 \\times 0.01}{0.9 \\times 0.01 + 0.1 \\times 0.99} = \\frac{0.009}{0.009 + 0.099} \\approx 0.083$. A positive test result is correct only $8.3\\%$ of the time. The ROC curve, being insensitive to $\\pi$, would show the same ($\\text{TPR}, \\text{FPR}$) point regardless of prevalence and might suggest excellent performance. The PR curve, however, would visually demonstrate this poor positive predictive value. Since clinical utility is often tied to the reliability of a positive finding (i.e., precision), the PR curve is indeed more informative in this context.\n- **Verdict: Correct.**\n\n**D. A calibration (reliability) curve assesses whether predicted probabilities are numerically consistent with empirical event frequencies. It plots binned average predicted probability $\\hat{p}$ versus the observed fraction $f$ of positives in each bin; perfect calibration lies on the identity line $f = \\hat{p}$.**\n- This statement describes model calibration. A model is calibrated if its predicted probability corresponds to the true frequency of the event. For example, for the set of all cases where the model predicts a probability of $0.8$, we would expect $80\\%$ of them to be actual positives.\n- The standard method to visualize calibration is a reliability diagram or calibration curve. One divides the range of predicted probabilities (scores) into bins (e.g., $[0, 0.1), [0.1, 0.2), \\dots$). For each bin, one computes the average predicted probability of the cases that fall into it (the x-coordinate, $\\hat{p}$) and the actual fraction of positive cases in that bin (the y-coordinate, $f$).\n- For a perfectly calibrated model, the observed fraction $f$ will equal the average predicted probability $\\hat{p}$ for every bin. Thus, the points $(\\hat{p}, f)$ will lie on the line of identity $y=x$.\n- The statement gives an accurate and standard description of a calibration curve.\n- **Verdict: Correct.**\n\n**E. In rare disease detection, ROC curves become misleading because $\\text{TPR}(\\tau)$ numerically depends on the prevalence $\\pi$, which causes ROC performance to inflate or deflate as $\\pi$ changes even if the score distributions remain the same.**\n- This statement claims that ROC curves are misleading for a specific reason: that $\\text{TPR}(\\tau)$ depends on prevalence $\\pi$.\n- As established in the analysis of option A, $\\text{TPR}(\\tau) = P(S \\ge \\tau \\mid Y = 1)$. By definition, this is a conditional probability that is independent of the marginal probability $P(Y=1) = \\pi$.\n- The premise of the statement is factually incorrect. The coordinates of the ROC curve, TPR and FPR, are invariant to changes in class prevalence.\n- While it is true that ROC curves can be misleading in rare disease settings, it is for a different reason: their insensitivity to prevalence masks the poor positive predictive value (precision) that often occurs in these scenarios. The reason provided in the statement is false.\n- **Verdict: Incorrect.**\n\n**F. A high Area Under the Curve (AUC) guarantees that a model is well calibrated across the entire range of predicted probabilities.**\n- AUC is a measure of discrimination, i.e., the model's ability to rank positive instances higher than negative instances. Calibration is a measure of probabilistic accuracy, i.e., whether the score values themselves correspond to true probabilities.\n- These two properties are distinct. A model can have perfect discrimination (AUC=$1.0$) but be poorly calibrated.\n- For example, consider a dataset and a model that assigns a score of $S=0.7$ to all positive instances ($Y=1$) and a score of $S=0.6$ to all negative instances ($Y=0$). This model has perfect ranking ability; any threshold $\\tau \\in (0.6, 0.7)$ will perfectly separate the classes. The ROC curve will go from $(0,0)$ to $(0,1)$ to $(1,1)$, yielding an AUC of $1.0$.\n- However, the model is not calibrated. When it predicts a score of $0.7$, the true probability of being positive is $1.0$, not $0.7$. When it predicts $0.6$, the true probability is $0.0$, not $0.6$.\n- More generally, applying any strictly increasing monotonic function to the scores of a model will preserve the ranking and thus leave the AUC unchanged, but it will alter the score values and thus destroy calibration (unless the function is the identity).\n- **Verdict: Incorrect.**\n\n**G. Precision is invariant to prevalence $\\pi$, so Precision–Recall (PR) curves are less useful than Receiver Operating Characteristic (ROC) curves when classes are imbalanced.**\n- The first part of the statement claims that precision is invariant to prevalence $\\pi$. From our foundational formula:\n$$ \\text{Precision}(\\tau) = \\frac{\\text{TPR}(\\tau)\\pi}{\\text{TPR}(\\tau)\\pi + \\text{FPR}(\\tau)(1-\\pi)} $$\n- This expression clearly shows that precision is a function of prevalence $\\pi$. This dependence is strong, especially when $\\pi$ is small, as demonstrated in the analysis for option C. Therefore, the premise is false.\n- The second part of the statement draws a conclusion from this false premise. In fact, the opposite conclusion is true. It is precisely *because* precision depends on prevalence that PR curves are considered more informative and useful than ROC curves for imbalanced classification problems. They directly visualize the impact of class imbalance on the model's performance in terms of positive predictions.\n- **Verdict: Incorrect.**\n\nBased on the analysis, statements A, B, C, and D are correct.",
            "answer": "$$\\boxed{ABCD}$$"
        }
    ]
}