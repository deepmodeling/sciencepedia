{
    "hands_on_practices": [
        {
            "introduction": "许多基于模型的迭代重建方法的核心在于求解一个大规模线性方程组。这项实践练习  将指导你实现共轭梯度（CG）算法，这是一个解决此类任务的强大而高效的求解器。你还将探索预处理技术，这是在实际场景中加速收敛的关键技术。",
            "id": "4900883",
            "problem": "考虑在基于模型和统计迭代重建的计算机断层扫描 (CT) 中出现的二次惩罚加权最小二乘 (PWLS) 子问题。目标函数由下式给出\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\,\\lVert \\mathbf{y} - A\\mathbf{x} \\rVert_{W}^2 \\ + \\ \\frac{1}{2}\\,\\beta\\,\\lVert C\\mathbf{x} \\rVert_2^2,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是将图像系数映射到线积分的系统矩阵，$\\mathbf{y} \\in \\mathbb{R}^m$ 是测量向量，$W \\in \\mathbb{R}^{m \\times m}$ 是一个对角加权矩阵，其非负元素反映了测量方差，$C \\in \\mathbb{R}^{p \\times n}$ 是一个线性正则化算子，$\\beta \\in \\mathbb{R}_{>0}$ 是正则化参数。加权范数定义为 $\\lVert \\mathbf{v} \\rVert_W^2 = \\mathbf{v}^{\\top} W \\mathbf{v}$。\n\n从凸二次最小化的基本定义以及对称正定 (SPD) 矩阵的性质出发，推导最小化子的正规方程，并证明解 $\\mathbf{x}^\\star$ 满足\n$$\nH\\,\\mathbf{x}^\\star = \\mathbf{b},\n$$\n其中\n$$\nH = A^{\\top} W A + \\beta\\, C^{\\top} C, \\qquad \\mathbf{b} = A^{\\top} W \\mathbf{y}.\n$$\n设计一个共轭梯度 (CG) 方案来求解线性系统 $H\\,\\mathbf{x} = \\mathbf{b}$，要求仅使用与 $A$、$W$、$C$ 和 $C^{\\top}$ 的矩阵向量乘积，而不显式地构造 $H$。讨论并实现使用对角矩阵的左预处理\n$$\nM = \\operatorname{diag}\\!\\left(A^{\\top} W A + \\beta\\, C^{\\top} C\\right),\n$$\n并使用 $M^{-1}$ 作为预处理器。解释为什么 $M$ 是一个有效的预处理器以及它如何近似 $H$。\n\n您的程序必须实现无预处理的 CG 和对角预处理的 CG 以求解系统 $H\\,\\mathbf{x} = \\mathbf{b}$，并满足以下规范：\n- 初始迭代点必须为 $\\mathbf{x}_0 = \\mathbf{0}$。\n- 对残差使用欧几里得范数。\n- 停止准则必须是相对残差阈值\n$$\n\\frac{\\lVert \\mathbf{r}_k \\rVert_2}{\\lVert \\mathbf{r}_0 \\rVert_2} \\leq \\varepsilon,\n$$\n其中 $\\varepsilon = 10^{-8}$，$\\mathbf{r}_k = \\mathbf{b} - H\\mathbf{x}_k$。\n- 最大迭代次数必须为 $5n$。\n\n通过以下方式实现矩阵向量乘积\n$$\nH\\mathbf{v} = A^{\\top}\\left(W(A\\mathbf{v})\\right) + \\beta\\, C^{\\top}(C\\mathbf{v}),\n$$\n并通过以下方式实现对角预处理器\n$$\n\\operatorname{diag}(H)_j = \\sum_{i=1}^{m} W_{ii}\\,A_{ij}^2 + \\beta \\sum_{k=1}^{p} C_{kj}^2,\n$$\n并约定任何零对角线元素都将被一个小的正常数替换，以避免在 $M^{-1}$ 中出现除以零的情况。\n\n测试套件：\n为保证可复现性，请使用以下四个测试用例。在每种情况下，构造 $A$、$W$、$C$、$\\beta$、一个真实向量 $\\mathbf{x}_{\\text{true}}$ 以及测量值 $\\mathbf{y} = A\\mathbf{x}_{\\text{true}}$。然后设置 $\\mathbf{b} = A^{\\top} W \\mathbf{y}$。\n\n- 情况 1 (一般良态)：$n=6$, $m=12$。使用以 1 为种子的伪随机正态生成器生成 $A$ 和 $\\mathbf{x}_{\\text{true}}$。生成对角矩阵 $W$，其元素从 $[0.5, 2.0]$ 均匀抽取。将 $C$ 设置为 $\\mathbb{R}^n$ 上大小为 $(n-1)\\times n$ 的一阶有限差分算子，并设置 $\\beta = 0.1$。\n\n- 情况 2 (通过零权重实现部分数据缺失)：$n=6$, $m=12$。使用以 2 为种子的伪随机正态生成器生成 $A$ 和 $\\mathbf{x}_{\\text{true}}$。在 $[0.5, 2.0]$ 上均匀生成初始对角矩阵 $W$ 的元素，然后将索引 $2$、$3$、$6$ 和 $9$（使用从零开始的索引）处的四个元素设置为零。设置 $C = I_n$（$n \\times n$ 单位矩阵）和 $\\beta = 0.5$。\n\n- 情况 3 (病态正向模型)：$n=6$, $m=10$。使用以 3 为种子的伪随机正态生成器生成一个向量 $\\mathbf{u} \\in \\mathbb{R}^m$；构造 $A$，使其第一列等于 $\\mathbf{u}$，后续每列等于 $\\mathbf{u} + 10^{-4}\\,\\mathbf{v}_j$，其中每个 $\\mathbf{v}_j$ 从标准正态分布中独立同分布地抽取。在 $[0.8, 1.2]$ 上均匀抽取 $W$ 的对角元素。将 $C$ 设置为 $\\mathbb{R}^n$ 上的一阶有限差分算子，并设置 $\\beta = 10^{-4}$。\n\n- 情况 4 (强正则化)：$n=8$, $m=12$。使用以 4 为种子的伪随机正态生成器生成 $A$ 和 $\\mathbf{x}_{\\text{true}}$。在 $[0.2, 0.5]$ 上均匀生成 $W$ 的对角元素。将 $C$ 设置为 $\\mathbb{R}^n$ 上的一阶有限差分算子，并设置 $\\beta = 100$。\n\n角度单位不适用。输出中没有物理单位。\n\n输出规范：\n对于每个测试用例，运行无预处理的 CG 和对角预处理的 CG，并记录达到停止准则所需的迭代次数。最终程序输出必须是单行，包含一个由方括号括起来的逗号分隔列表，其中按以下顺序包含八个整数：\n$$\n[\\text{iters\\_unpre\\_case1},\\text{iters\\_pre\\_case1},\\text{iters\\_unpre\\_case2},\\text{iters\\_pre\\_case2},\\text{iters\\_unpre\\_case3},\\text{iters\\_pre\\_case3},\\text{iters\\_unpre\\_case4},\\text{iters\\_pre\\_case4}]。\n$$\n您的程序必须是自包含的、确定性的，并且其唯一的输出必须是这一个单行。",
            "solution": "问题陈述已经过验证，被认为是合理、完整且适定的。我们可以开始推导和求解。\n\n该问题要求解一个在医学影像重建中常见的惩罚加权最小二乘 (PWLS) 最小化问题。目标函数由下式给出：\n$$\nf(\\mathbf{x}) = \\frac{1}{2}\\,\\lVert \\mathbf{y} - A\\mathbf{x} \\rVert_{W}^2 + \\frac{1}{2}\\,\\beta\\,\\lVert C\\mathbf{x} \\rVert_2^2\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^n$ 是待估计的图像系数向量，$\\mathbf{y} \\in \\mathbb{R}^m$ 是测量向量，$A \\in \\mathbb{R}^{m \\times n}$ 是系统矩阵，$W \\in \\mathbb{R}^{m \\times m}$ 是一个对角加权矩阵，其非负元素反映了测量方差，$C \\in \\mathbb{R}^{p \\times n}$ 是一个正则化算子，$\\beta \\in \\mathbb{R}_{>0}$ 是正则化参数。加权范数定义为 $\\lVert \\mathbf{v} \\rVert_W^2 = \\mathbf{v}^{\\top} W \\mathbf{v}$。\n\n**1. 正规方程的推导**\n\n函数 $f(\\mathbf{x})$ 是关于 $\\mathbf{x}$ 的二次函数。为了找到最小化 $f(\\mathbf{x})$ 的向量 $\\mathbf{x}^\\star$，我们必须找到 $f(\\mathbf{x})$ 相对于 $\\mathbf{x}$ 的梯度为零的点。首先，我们展开目标函数：\n$$\nf(\\mathbf{x}) = \\frac{1}{2} (\\mathbf{y} - A\\mathbf{x})^{\\top} W (\\mathbf{y} - A\\mathbf{x}) + \\frac{1}{2} \\beta (C\\mathbf{x})^{\\top}(C\\mathbf{x})\n$$\n展开第一项：\n$$\n(\\mathbf{y} - A\\mathbf{x})^{\\top} W (\\mathbf{y} - A\\mathbf{x}) = (\\mathbf{y}^{\\top} - \\mathbf{x}^{\\top}A^{\\top})W(\\mathbf{y} - A\\mathbf{x}) = \\mathbf{y}^{\\top}W\\mathbf{y} - \\mathbf{y}^{\\top}WA\\mathbf{x} - \\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y} + \\mathbf{x}^{\\top}A^{\\top}WA\\mathbf{x}\n$$\n由于 $\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y}$ 是一个标量，它等于其转置 $(\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y})^{\\top} = \\mathbf{y}^{\\top}W^{\\top}A\\mathbf{x}$。由于 $W$ 是对角矩阵，所以它是对称的 ($W=W^\\top$)，因此 $\\mathbf{x}^{\\top}A^{\\top}W\\mathbf{y} = \\mathbf{y}^{\\top}WA\\mathbf{x}$。于是该项变为：\n$$\n\\mathbf{y}^{\\top}W\\mathbf{y} - 2\\mathbf{y}^{\\top}WA\\mathbf{x} + \\mathbf{x}^{\\top}A^{\\top}WA\\mathbf{x}\n$$\n$f(\\mathbf{x})$ 的第二项是 $\\frac{1}{2} \\beta \\mathbf{x}^{\\top}C^{\\top}C\\mathbf{x}$。将所有部分组合起来，目标函数为：\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\top} (A^{\\top}WA + \\beta C^{\\top}C) \\mathbf{x} - (A^{\\top}W\\mathbf{y})^{\\top}\\mathbf{x} + \\frac{1}{2}\\mathbf{y}^{\\top}W\\mathbf{y}\n$$\n这是一个标准的二次型 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x} + \\text{常数}$，其中：\n$$\nH = A^{\\top}WA + \\beta C^{\\top}C\n$$\n$$\n\\mathbf{b} = A^{\\top}W\\mathbf{y}\n$$\n$f(\\mathbf{x})$ 相对于 $\\mathbf{x}$ 的梯度是 $\\nabla_{\\mathbf{x}}f(\\mathbf{x}) = H\\mathbf{x} - \\mathbf{b}$。矩阵 $H$ 是对称的，因为 $W$ 是对称的，所以 $(A^{\\top}WA)^{\\top} = A^{\\top}W^{\\top}A = A^{\\top}WA$，并且 $(C^{\\top}C)^{\\top} = C^{\\top}C$。$f(\\mathbf{x})$ 的海森矩阵是 $\\nabla^2_{\\mathbf{x}}f(\\mathbf{x}) = H$。由于 $W$ 具有非负元素，$A^{\\top}WA$ 是半正定的。类似地，$C^{\\top}C$ 也是半正定的。由于 $\\beta > 0$，它们的和 $H$ 也是半正定的。在通常情况下，当 $\\ker(A) \\cap \\ker(C) = \\{\\mathbf{0}\\}$ 时，$H$ 是正定的，这使得 $f(\\mathbf{x})$ 是严格凸的，并确保存在唯一的最小化子。\n\n将梯度设为零以找到最小值 $\\mathbf{x}^\\star$：\n$$\n\\nabla_{\\mathbf{x}}f(\\mathbf{x}^\\star) = H \\mathbf{x}^\\star - \\mathbf{b} = \\mathbf{0}\n$$\n这就得到了称为正规方程的线性方程组：\n$$\nH \\mathbf{x}^\\star = \\mathbf{b}\n$$\n这证实了问题陈述中提供的结构。\n\n**2. 共轭梯度法与预处理**\n\n共轭梯度 (CG) 算法是一种迭代方法，非常适用于求解像 $H\\mathbf{x} = \\mathbf{b}$ 这样的大型、稀疏、对称正定 (SPD) 线性系统。其一个关键优势是它仅需要一个函数来计算矩阵向量乘积 $H\\mathbf{v}$，从而避免了矩阵 $H$ 的显式构造和存储。该乘积实现为：\n$$\nH\\mathbf{v} = (A^{\\top}WA + \\beta C^{\\top}C)\\mathbf{v} = A^{\\top}(W(A\\mathbf{v})) + \\beta C^{\\top}(C\\mathbf{v})\n$$\nCG 的收敛速度取决于 $H$ 的条件数。预处理是一种将原系统转换为具有更优条件数的系统的技术。我们使用一个预处理器矩阵 $M$ 来近似 $H$，并且该矩阵易于求逆。预处理共轭梯度 (PCG) 方法修改了标准 CG 算法以整合 $M^{-1}$。\n\n所选的预处理器是对角矩阵 $M = \\operatorname{diag}(H)$。这是一种 Jacobi 预处理形式。\n- **作为预处理器的有效性**：要使 $M$ 成为一个有效的预处理器，它必须是对称且正定的。$M$ 是对角矩阵，因此是对称的。由于 $H$ 是 SPD 的（在温和条件下），其所有对角元素 $H_{jj} = \\mathbf{e}_j^\\top H \\mathbf{e}_j$ 均为正，其中 $\\mathbf{e}_j$ 是第 $j$ 个标准基向量。因此，$M$ 是正定的。如果任何对角元素为零，它将被一个小的正常数替换，以保持可逆性和正定性。\n- **对 H 的近似**：$M$ 通过保留 $H$ 的对角元素而舍弃所有非对角信息来近似 $H$。这是一个简单但通常有效的策略，特别是当 $H$ 具有一定程度的对角占优时。\n- **可逆性**：由于是对角矩阵，$M^{-1}$ 的计算非常简单；其对角元素是 $M$ 对角元素的倒数。\n\n$H = A^{\\top}WA + \\beta C^{\\top}C$ 的第 $j$ 个对角元素计算如下：\n$$\nM_{jj} = H_{jj} = (A^{\\top}WA)_{jj} + \\beta (C^{\\top}C)_{jj}\n$$\n项 $(A^{\\top}WA)_{jj}$ 为 $\\sum_{i=1}^m (A^\\top)_{ji} (WA)_{ij} = \\sum_{i=1}^m A_{ij} \\sum_{k=1}^m W_{ik} A_{kj}$。由于 $W$ 是对角的（当 $i \\neq k$ 时 $W_{ik}=0$），这简化为 $\\sum_{i=1}^m A_{ij} W_{ii} A_{ij} = \\sum_{i=1}^m W_{ii} A_{ij}^2$。\n项 $(C^{\\top}C)_{jj}$ 为 $\\sum_{k=1}^p (C^\\top)_{jk} C_{kj} = \\sum_{k=1}^p C_{kj}^2$。\n将这些结合起来，就得到了指定的公式：\n$$\nM_{jj} = \\sum_{i=1}^{m} W_{ii}\\,A_{ij}^2 + \\beta \\sum_{k=1}^{p} C_{kj}^2\n$$\n\n实现将遵循标准的 PCG 算法，其中步骤 $M\\mathbf{z}_k = \\mathbf{r}_k$ 通过逐元素除法 $\\mathbf{z}_k = \\mathbf{r}_k ./ \\operatorname{diag}(M)$ 来求解。\n\n算法将使用初始猜测 $\\mathbf{x}_0 = \\mathbf{0}$、相对残差停止准则 $\\lVert \\mathbf{r}_k \\rVert_2 / \\lVert \\mathbf{r}_0 \\rVert_2 \\leq 10^{-8}$ 和最大迭代次数 $5n$ 来实现。",
            "answer": "```python\nimport numpy as np\n\ndef H_matvec(v, A, W_diag, C, beta):\n    \"\"\"Computes the matrix-vector product H*v without forming H explicitly.\"\"\"\n    # H*v = (A.T @ W @ A + beta * C.T @ C) @ v\n    #     = A.T @ (W @ (A @ v)) + beta * C.T @ (C @ v)\n    # Since W is diagonal, W @ u is just W_diag * u\n    return A.T @ (W_diag * (A @ v)) + beta * (C.T @ (C @ v))\n\ndef compute_M_diag(A, W_diag, C, beta):\n    \"\"\"Computes the diagonal of H.\"\"\"\n    # diag(H)_j = sum_i(W_ii * A_ij^2) + beta * sum_k(C_kj^2)\n    # Vectorized computation:\n    # Part 1: diag(A.T @ W @ A) = einsum('i,ij->j', W_diag, A**2)\n    # Part 2: diag(C.T @ C) = sum(C**2, axis=0)\n    diag_H = np.einsum('i,ij->j', W_diag, A**2) + beta * np.sum(C**2, axis=0)\n    \n    # Replace zero entries with a small positive constant to ensure invertibility\n    diag_H[diag_H == 0] = 1e-12\n    return diag_H\n\ndef cg_unpreconditioned(A, W_diag, C, beta, b, tol=1e-8, max_iter=None):\n    \"\"\"Implements the unpreconditioned Conjugate Gradient algorithm.\"\"\"\n    n = b.shape[0]\n    if max_iter is None:\n        max_iter = 5 * n\n\n    x = np.zeros(n)\n    r = b - H_matvec(x, A, W_diag, C, beta)\n    p = r.copy()\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n\n    rs_old = np.dot(r, r)\n\n    for k in range(max_iter):\n        Hp = H_matvec(p, A, W_diag, C, beta)\n        alpha = rs_old / np.dot(p, Hp)\n        \n        x = x + alpha * p\n        r = r - alpha * Hp\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm = tol:\n            return k + 1\n\n        rs_new = np.dot(r, r)\n        gamma = rs_new / rs_old\n        p = r + gamma * p\n        rs_old = rs_new\n        \n    return max_iter\n\ndef cg_preconditioned(A, W_diag, C, beta, b, tol=1e-8, max_iter=None):\n    \"\"\"Implements the diagonally preconditioned Conjugate Gradient algorithm.\"\"\"\n    n = b.shape[0]\n    if max_iter is None:\n        max_iter = 5 * n\n\n    M_diag = compute_M_diag(A, W_diag, C, beta)\n    \n    x = np.zeros(n)\n    r = b - H_matvec(x, A, W_diag, C, beta)\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n\n    z = r / M_diag\n    p = z.copy()\n    rz_old = np.dot(r, z)\n\n    for k in range(max_iter):\n        Hp = H_matvec(p, A, W_diag, C, beta)\n        alpha = rz_old / np.dot(p, Hp)\n        \n        x = x + alpha * p\n        r = r - alpha * Hp\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm = tol:\n            return k + 1\n\n        z = r / M_diag\n        rz_new = np.dot(r, z)\n        gamma = rz_new / rz_old\n        p = z + gamma * p\n        rz_old = rz_new\n        \n    return max_iter\n\ndef setup_case(case_params):\n    \"\"\"Sets up the matrices and vectors for a given test case.\"\"\"\n    n, m, beta, seed, case_type = case_params\n    rng = np.random.default_rng(seed)\n\n    # Generate A and x_true\n    if case_type == 'ill-conditioned':\n        A = np.zeros((m, n))\n        u = rng.normal(size=m)\n        A[:, 0] = u\n        for j in range(1, n):\n            v_j = rng.normal(size=m)\n            A[:, j] = u + 1e-4 * v_j\n        x_true = rng.normal(size=n)\n    else:\n        A = rng.normal(size=(m, n))\n        x_true = rng.normal(size=n)\n\n    # Generate W\n    if case_type == 'general':\n        W_diag = rng.uniform(0.5, 2.0, size=m)\n    elif case_type == 'missing-data':\n        W_diag = rng.uniform(0.5, 2.0, size=m)\n        W_diag[[2, 3, 6, 9]] = 0.0\n    elif case_type == 'ill-conditioned':\n        W_diag = rng.uniform(0.8, 1.2, size=m)\n    elif case_type == 'strong-reg':\n        W_diag = rng.uniform(0.2, 0.5, size=m)\n    else:\n        raise ValueError(\"Unknown case type\")\n\n    # Generate C\n    if case_type == 'missing-data':\n        C = np.eye(n)\n    else: # first-order finite difference operator\n        C = np.eye(n - 1, n, k=1) - np.eye(n - 1, n)\n\n    y = A @ x_true\n    b = A.T @ (W_diag * y)\n    \n    return A, W_diag, C, beta, b\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # n, m, beta, seed, case_type\n        (6, 12, 0.1, 1, 'general'),\n        (6, 12, 0.5, 2, 'missing-data'),\n        (6, 10, 1e-4, 3, 'ill-conditioned'),\n        (8, 12, 100.0, 4, 'strong-reg')\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        n = params[0]\n        max_iter = 5 * n\n        A, W_diag, C, beta, b = setup_case(params)\n        \n        iters_unpre = cg_unpreconditioned(A, W_diag, C, beta, b, max_iter=max_iter)\n        iters_pre = cg_preconditioned(A, W_diag, C, beta, b, max_iter=max_iter)\n        \n        results.extend([iters_unpre, iters_pre])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "虽然标准的迭代求解器很稳健，但对于 CT 中的海量数据集来说可能速度很慢。有序子集（OS）方法是一种广泛使用的技术，可以显著加速收敛。这项练习  提供了一个难得的机会来“窥探内部”，通过数值计算具体地展示 OS 算法的一个关键现象——极限环——从而让你对重建速度和数学精度之间的权衡有更深刻的理解。",
            "id": "4900879",
            "problem": "考虑一个简化的计算机断层扫描（CT）逆问题，该问题针对一个双参数图像模型 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 和三条理想化的投影射线。数据保真度由最小二乘目标函数建模\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\| A \\mathbf{x} - \\mathbf{b} \\|_{2}^{2},\n$$\n其中 $A \\in \\mathbb{R}^{3 \\times 2}$ 堆叠了单位范数射线方向，$\\mathbf{b} \\in \\mathbb{R}^{3}$ 是测量的线积分向量。设三条射线方向为\n$$\n\\mathbf{u}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad \\mathbf{u}_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, \\quad \\mathbf{u}_{3} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\n$$\n并将测量矩阵和数据划分为两个有序子集，用于有序子集（OS）方法（在每一轮中先处理第一个子集，然后处理第二个子集）：\n$$\nA_{1} = \\begin{pmatrix} \\mathbf{u}_{1}^{\\top} \\\\ \\mathbf{u}_{2}^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}, \\quad \\mathbf{b}_{1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\qquad\nA_{2} = \\begin{pmatrix} \\mathbf{u}_{3}^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 2}, \\quad b_{2} = \\frac{1}{2}.\n$$\n因此 $A = \\begin{pmatrix} A_{1} \\\\ A_{2} \\end{pmatrix}$ 且 $\\mathbf{b} = \\begin{pmatrix} \\mathbf{b}_{1} \\\\ b_{2} \\end{pmatrix}$。考虑使用恒定步长 $\\alpha = \\frac{1}{2}$ 的 OS 梯度下降法，对于第 $k$ 轮，从 $\\mathbf{x}_{k,1}$ 开始，执行以下操作\n$$\n\\mathbf{x}_{k,2} = \\mathbf{x}_{k,1} - \\alpha \\, A_{1}^{\\top} \\big( A_{1}\\mathbf{x}_{k,1} - \\mathbf{b}_{1} \\big), \\qquad\n\\mathbf{x}_{k,3} = \\mathbf{x}_{k,2} - \\alpha \\, A_{2}^{\\top} \\big( A_{2}\\mathbf{x}_{k,2} - b_{2} \\big),\n$$\n轮更新为 $\\mathbf{x}_{k+1,1} \\equiv \\mathbf{x}_{k+1} := \\mathbf{x}_{k,3}$。\n\n您将需要证明 $f$ 是强凸的，推导出得到的仿射轮映射，并证明在稳态下，$\\mathbf{x}_{k,1}$ 和 $\\mathbf{x}_{k,2}$ 之间存在一个非平凡的两点极限环。具体来说：\n\n- 从上述定义出发，通过分析海森矩阵 $H := A^{\\top}A$ 来验证 $f$ 的强凸性。\n- 用 $H_{1} := A_{1}^{\\top}A_{1}$、$H_{2} := A_{2}^{\\top}A_{2}$、$\\mathbf{g}_{1} := A_{1}^{\\top}\\mathbf{b}_{1}$ 和 $\\mathbf{g}_{2} := A_{2}^{\\top} b_{2}$ 表示，推导出仿射轮算子 $\\mathbf{x}_{k+1} = T \\, \\mathbf{x}_{k} + \\mathbf{h}$。\n- 求解轮算子的不动点 $\\mathbf{x}_{\\mathrm{OS}}$，并证明子集内迭代收敛到一个稳定的两点环 $\\mathbf{x}_{\\mathrm{OS}} \\leftrightarrow \\mathbf{y}_{\\mathrm{OS}}$，其中 $\\mathbf{y}_{\\mathrm{OS}} := \\mathbf{x}_{\\mathrm{OS}} - \\alpha\\big(H_{1}\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_{1}\\big)$。\n- 根据子集偏差解释为什么 $\\big(H_{1}\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_{1}\\big) \\neq \\mathbf{0}$，以及这如何导致非零的周期振幅。\n- 对于给定的数值实例，计算稳态周期振幅的欧几里得范数 $\\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_{2}$。请将最终答案以单个实数的形式给出。无需四舍五入。该量为无量纲量。",
            "solution": "该问题是有效的，因为它在医学成像迭代算法领域提出了一个适定且有科学依据的问题。我们将进行完整的推导和求解。\n\n该问题要求对用于简化计算机断层扫描（CT）逆问题的有序子集（OS）梯度下降算法进行分析。我们将按顺序解决问题的每个部分。\n\n首先，我们验证目标函数 $f(\\mathbf{x}) = \\frac{1}{2} \\| A \\mathbf{x} - \\mathbf{b} \\|_{2}^{2}$ 的强凸性。如果一个函数的海森矩阵是正定的，那么该函数就是强凸的。该目标函数是一个二次型：\n$$f(\\mathbf{x}) = \\frac{1}{2} (A\\mathbf{x} - \\mathbf{b})^{\\top}(A\\mathbf{x} - \\mathbf{b}) = \\frac{1}{2} (\\mathbf{x}^{\\top}A^{\\top}A\\mathbf{x} - 2\\mathbf{b}^{\\top}A\\mathbf{x} + \\mathbf{b}^{\\top}\\mathbf{b})$$\n$f$ 相对于 $\\mathbf{x}$ 的梯度是 $\\nabla f(\\mathbf{x}) = A^{\\top}A\\mathbf{x} - A^{\\top}\\mathbf{b}$。\n海森矩阵是二阶导数，即 $H = \\nabla^2 f(\\mathbf{x}) = A^{\\top}A$。\n我们从其子集 $A_1$ 和 $A_2$ 构建完整的系统矩阵 $A$。\n$$A_1 = \\begin{pmatrix} \\mathbf{u}_{1}^{\\top} \\\\ \\mathbf{u}_{2}^{\\top} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} \\mathbf{u}_{3}^{\\top} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n$$A = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n现在，我们计算海森矩阵 $H$：\n$$H = A^{\\top}A = \\begin{pmatrix} 1  0  \\frac{1}{\\sqrt{2}} \\\\ 0  1  \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1\\cdot1 + 0\\cdot0 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}}  1\\cdot0 + 0\\cdot1 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}} \\\\ 0\\cdot1 + 1\\cdot0 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}}  0\\cdot0 + 1\\cdot1 + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix}$$\n为了检查正定性，我们通过求解 $\\det(H - \\lambda I) = 0$ 来找到 $H$ 的特征值 $\\lambda$：\n$$\\det \\begin{pmatrix} \\frac{3}{2} - \\lambda  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{3}{2} - \\lambda \\end{pmatrix} = \\left(\\frac{3}{2} - \\lambda\\right)^2 - \\left(\\frac{1}{2}\\right)^2 = 0$$\n$$ \\left(\\frac{3}{2} - \\lambda - \\frac{1}{2}\\right) \\left(\\frac{3}{2} - \\lambda + \\frac{1}{2}\\right) = 0 $$\n$$ (1-\\lambda)(2-\\lambda) = 0 $$\n特征值为 $\\lambda_1 = 1$ 和 $\\lambda_2 = 2$。由于两个特征值都严格为正，海森矩阵 $H$ 是正定的，因此目标函数 $f(\\mathbf{x})$ 是强凸的。\n\n接下来，我们推导仿射轮算子 $\\mathbf{x}_{k+1} = T \\mathbf{x}_{k} + \\mathbf{h}$。从 $\\mathbf{x}_k \\equiv \\mathbf{x}_{k,1}$ 开始的一轮更新由以下两个连续步骤给出：\n$$ \\mathbf{x}_{k,2} = \\mathbf{x}_{k,1} - \\alpha A_{1}^{\\top} ( A_{1}\\mathbf{x}_{k,1} - \\mathbf{b}_{1} ) $$\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_{k,2} - \\alpha A_{2}^{\\top} ( A_{2}\\mathbf{x}_{k,2} - b_{2} ) $$\n使用定义 $H_i = A_i^{\\top}A_i$ 和 $\\mathbf{g}_1 = A_1^{\\top}\\mathbf{b}_1$, $\\mathbf{g}_2 = A_2^{\\top}b_2$，更新公式为：\n$$ \\mathbf{x}_{k,2} = (I - \\alpha H_1) \\mathbf{x}_{k,1} + \\alpha \\mathbf{g}_1 $$\n$$ \\mathbf{x}_{k+1} = (I - \\alpha H_2) \\mathbf{x}_{k,2} + \\alpha \\mathbf{g}_2 $$\n将 $\\mathbf{x}_{k,2}$ 的表达式代入第二个方程：\n$$ \\mathbf{x}_{k+1} = (I - \\alpha H_2) \\left[ (I - \\alpha H_1) \\mathbf{x}_{k,1} + \\alpha \\mathbf{g}_1 \\right] + \\alpha \\mathbf{g}_2 $$\n$$ \\mathbf{x}_{k+1} = (I - \\alpha H_2)(I - \\alpha H_1) \\mathbf{x}_{k,1} + \\alpha(I - \\alpha H_2)\\mathbf{g}_1 + \\alpha \\mathbf{g}_2 $$\n这是一个仿射映射 $\\mathbf{x}_{k+1} = T\\mathbf{x}_k + \\mathbf{h}$，其算子 $T$ 和偏移量 $\\mathbf{h}$ 由以下公式给出：\n$$ T = (I - \\alpha H_2)(I - \\alpha H_1) $$\n$$ \\mathbf{h} = \\alpha(I - \\alpha H_2)\\mathbf{g}_1 + \\alpha \\mathbf{g}_2 $$\n\n在稳态下，轮初迭代序列收敛到一个不动点 $\\mathbf{x}_k \\to \\mathbf{x}_{\\mathrm{OS}}$。子集内迭代则形成一个极限环。设当 $k \\to \\infty$ 时，$\\mathbf{x}_{k,1} \\to \\mathbf{x}_{\\mathrm{OS}}$ 且 $\\mathbf{x}_{k,2} \\to \\mathbf{y}_{\\mathrm{OS}}$。稳态更新变为：\n$$ \\mathbf{y}_{\\mathrm{OS}} = \\mathbf{x}_{\\mathrm{OS}} - \\alpha (H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1) $$\n$$ \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{y}_{\\mathrm{OS}} - \\alpha (H_2 \\mathbf{y}_{\\mathrm{OS}} - \\mathbf{g}_2) $$\n这两个方程定义了两点极限环 $\\mathbf{x}_{\\mathrm{OS}} \\leftrightarrow \\mathbf{y}_{\\mathrm{OS}}$。通过将第一个方程代入第二个方程，可以找到轮算子的不动点 $\\mathbf{x}_{\\mathrm{OS}}$，得到 $ (H_1 + H_2 - \\alpha H_2 H_1) \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{g}_1 + \\mathbf{g}_2 - \\alpha H_2 \\mathbf{g}_1 $。这证实了 $\\mathbf{x}_{\\mathrm{OS}}$ 是算子 $T$ 的一个不动点。\n\n极限环的振幅是非零的，$\\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_2 \\neq 0$，因为在环中的任何一点，子集目标函数的梯度不会同时为零。周期振幅向量是 $\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}} = -\\alpha (H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1)$。其范数为零当且仅当 $H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1 = \\mathbf{0}$，这意味着第一个子集的成本函数在 $\\mathbf{x}_{\\mathrm{OS}}$ 处的梯度为零。如果这是真的，那么 $\\mathbf{y}_{\\mathrm{OS}} = \\mathbf{x}_{\\mathrm{OS}}$。将此代入第二个稳态方程将意味着 $H_2 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_2 = \\mathbf{0}$。因此，一个零振幅周期（收敛到单一点）要求该点同时使两个子集成本函数的梯度都为零。这种情况只在存在一个点 $\\mathbf{x}^*$ 使得 $H_1\\mathbf{x}^*=\\mathbf{g}_1$ 和 $H_2\\mathbf{x}^*=\\mathbf{g}_2$ 时才会发生。这等价于 $\\mathbf{x}^*$ 是完整问题的最小二乘解 $\\mathbf{x}_{\\mathrm{LS}}$，并且该解恰好也最小化了每个子集的成本函数。通常情况下，跨子集的数据是不一致的，这意味着 $f_1(\\mathbf{x})$ 的最小化子与 $f_2(\\mathbf{x})$ 的最小化子不同。这被称为子集偏差。因此，OS 算法收敛到一个围绕点 $\\mathbf{x}_{\\mathrm{OS}}$ 的极限环，该点不是真正的最小二乘解，并且在该点处子集梯度非零。非零梯度 $H_1\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1$ 是非零周期振幅的直接原因。\n\n最后，我们计算给定实例的稳态周期振幅的欧几里得范数 $\\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_2$。\n我们有 $\\alpha = 1/2$。我们首先计算必要的矩阵和向量：\n$$ H_1 = A_1^{\\top} A_1 = I^{\\top}I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} $$\n$$ \\mathbf{g}_1 = A_1^{\\top} \\mathbf{b}_1 = I \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n$$ H_2 = A_2^{\\top} A_2 = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\end{pmatrix}\\right) = \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} $$\n$$ \\mathbf{g}_2 = A_2^{\\top} b_2 = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\frac{1}{2} = \\frac{1}{2\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n我们使用 $(H_1 + H_2 - \\alpha H_2 H_1) \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{g}_1 + \\mathbf{g}_2 - \\alpha H_2 \\mathbf{g}_1$ 求解不动点 $\\mathbf{x}_{\\mathrm{OS}}$。\n项 $\\alpha H_2 \\mathbf{g}_1$ 简化为：\n$$ \\alpha H_2 \\mathbf{g}_1 = \\frac{1}{2} \\left( \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 1-1 \\\\ 1-1 \\end{pmatrix} = \\mathbf{0} $$\n求解 $\\mathbf{x}_{\\mathrm{OS}}$ 的系统变为 $(H_1 + H_2 - \\alpha H_2 H_1) \\mathbf{x}_{\\mathrm{OS}} = \\mathbf{g}_1 + \\mathbf{g}_2$。\n左侧的矩阵是：\n$$ H_1 + H_2 - \\frac{1}{2} H_2 I = \\begin{pmatrix} \\frac{3}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} - \\frac{1}{2} \\left( \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{3}{2}-\\frac{1}{4}  \\frac{1}{2}-\\frac{1}{4} \\\\ \\frac{1}{2}-\\frac{1}{4}  \\frac{3}{2}-\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{4}  \\frac{1}{4} \\\\ \\frac{1}{4}  \\frac{5}{4} \\end{pmatrix} $$\n右侧的向量是：\n$$ \\mathbf{g}_1 + \\mathbf{g}_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\frac{1}{2\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{4} \\\\ -1 + \\frac{\\sqrt{2}}{4} \\end{pmatrix} $$\n我们求解 $\\frac{1}{4}\\begin{pmatrix} 5  1 \\\\ 1  5 \\end{pmatrix} \\mathbf{x}_{\\mathrm{OS}} = \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{4} \\\\ -1 + \\frac{\\sqrt{2}}{4} \\end{pmatrix}$，即 $\\begin{pmatrix} 5  1 \\\\ 1  5 \\end{pmatrix} \\mathbf{x}_{\\mathrm{OS}} = \\begin{pmatrix} 4 + \\sqrt{2} \\\\ -4 + \\sqrt{2} \\end{pmatrix}$。\n$\\begin{pmatrix} 5  1 \\\\ 1  5 \\end{pmatrix}$ 的逆矩阵是 $\\frac{1}{24}\\begin{pmatrix} 5  -1 \\\\ -1  5 \\end{pmatrix}$。\n$$ \\mathbf{x}_{\\mathrm{OS}} = \\frac{1}{24} \\begin{pmatrix} 5  -1 \\\\ -1  5 \\end{pmatrix} \\begin{pmatrix} 4 + \\sqrt{2} \\\\ -4 + \\sqrt{2} \\end{pmatrix} = \\frac{1}{24} \\begin{pmatrix} 5(4+\\sqrt{2}) - (-4+\\sqrt{2}) \\\\ -(4+\\sqrt{2}) + 5(-4+\\sqrt{2}) \\end{pmatrix} = \\frac{1}{24} \\begin{pmatrix} 24 + 4\\sqrt{2} \\\\ -24 + 4\\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{6} \\\\ -1 + \\frac{\\sqrt{2}}{6} \\end{pmatrix} $$\n周期振幅向量是 $\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}} = -\\alpha (H_1 \\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1)$。由于 $H_1=I$：\n$$ \\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}} = -\\frac{1}{2} (\\mathbf{x}_{\\mathrm{OS}} - \\mathbf{g}_1) = -\\frac{1}{2} \\left( \\begin{pmatrix} 1 + \\frac{\\sqrt{2}}{6} \\\\ -1 + \\frac{\\sqrt{2}}{6} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = -\\frac{1}{2} \\begin{pmatrix} \\frac{\\sqrt{2}}{6} \\\\ \\frac{\\sqrt{2}}{6} \\end{pmatrix} = \\begin{pmatrix} -\\frac{\\sqrt{2}}{12} \\\\ -\\frac{\\sqrt{2}}{12} \\end{pmatrix} $$\n该向量的欧几里得范数是：\n$$ \\|\\mathbf{y}_{\\mathrm{OS}} - \\mathbf{x}_{\\mathrm{OS}}\\|_{2} = \\sqrt{\\left(-\\frac{\\sqrt{2}}{12}\\right)^2 + \\left(-\\frac{\\sqrt{2}}{12}\\right)^2} = \\sqrt{\\frac{2}{144} + \\frac{2}{144}} = \\sqrt{\\frac{4}{144}} = \\sqrt{\\frac{1}{36}} = \\frac{1}{6} $$\n稳态周期振幅的欧几里得范数为 $\\frac{1}{6}$。",
            "answer": "$$\\boxed{\\frac{1}{6}}$$"
        },
        {
            "introduction": "二次正则化是有效的，但像总变分（TV）这样的非光滑正则化方法通常能更好地保留图像的锐利边缘，从而获得更优的图像质量。解决包含 TV 正则化的优化问题需要一套不同的算法工具。这项“纸上谈兵”的练习  将挑战你正确地构建近端梯度法——一种用于最小化此类不可微目标函数的基础算法——从而加深你对现代优化技术的理解。",
            "id": "4900910",
            "problem": "一个透射计算机断层扫描（CT）系统通过一个作用于未知线性衰减图像 $x \\in \\mathbb{R}^{n}$ 的线性系统矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的线积分进行建模。对于足够高的光子计数，透射光子的泊松测量统计在经过对数变换后，可以由一个高斯模型 $b \\approx A x + \\varepsilon$ 来近似，其中噪声 $\\varepsilon$ 为零均值，其逆协方差矩阵为 $W \\succ 0$。在最大后验（MAP）公式中，图像估计 $x$ 最小化一个形式为 $F(x) = f(x) + g(x)$ 的目标函数，其中数据保真项 $f$ 是高斯近似下的负对数似然，而正则化项 $g$ 是凸的且可能非光滑。考虑以下情况\n$$\nf(x) = \\frac{1}{2} \\left\\| W^{1/2} (A x - b) \\right\\|_{2}^{2}, \\qquad g(x) = \\lambda \\| D x \\|_{1},\n$$\n其中 $D$ 是一个实现各向异性总变分的离散有限差分算子，而 $\\lambda  0$ 是一个正则化权重。当 $f$ 可微且其梯度是Lipschitz连续的，并且 $g$ 是凸的（但不一定可微）时，可以应用近端梯度法。\n\n哪个选项正确且精确地指明了以下两者：\n- 步长 $\\tau  0$ 的近端算子 $\\operatorname{prox}_{\\tau g}(z)$ 的定义，以及\n- 针对上述CT MAP问题的相应近端梯度更新，包括正确的梯度表达式和关于 $\\nabla f$ 的Lipschitz常数的有效步长条件？\n\n选择唯一最佳选项。\n\nA. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} W (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} W A \\|_{2} }$ 的 $\\tau$。\n\nB. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ \\tau \\, g(u) + \\frac{1}{2} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} W (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} + \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau  \\frac{2}{ \\| A^{\\top} W A \\|_{2} }$ 的 $\\tau$。\n\nC. 不需要近端算子，因为 $g$ 是可微的；使用\n$$\nx^{k+1} = x^{k} - \\tau \\big( A^{\\top} W (A x^{k} - b) + \\lambda \\, D^{\\top} D x^{k} \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} W A + \\lambda D^{\\top} D \\|_{2} }$ 的 $\\tau$。\n\nD. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} W^{-1} (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W^{-1} (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} W^{-1} A \\|_{2} }$ 的 $\\tau$。\n\nE. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} A \\|_{2} }$ 的 $\\tau$。",
            "solution": "用户希望我验证并解决一个关于在CT图像重建中针对特定MAP目标函数的近端梯度法的问题。\n\n### 第1步：提取已知条件\n- **目标函数：** 目标是找到一个图像估计 $x$ 以最小化 $F(x) = f(x) + g(x)$。\n- **数据保真项 ($f$):** $f(x) = \\frac{1}{2} \\| W^{1/2} (A x - b) \\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是系统矩阵，$x \\in \\mathbb{R}^{n}$ 是图像，$b \\in \\mathbb{R}^{m}$ 代表测量值，而 $W \\succ 0$ 是噪声的逆协方差矩阵。\n- **正则化项 ($g$):** $g(x) = \\lambda \\| D x \\|_{1}$，其中 $D$ 是一个有限差分算子且 $\\lambda  0$。\n- **优化方法：** 近端梯度法。\n- **性质：** $f$ 是可微的，其梯度是Lipschitz连续的，而 $g$ 是凸的但可能非光滑。\n\n### 第2步：使用提取的已知条件进行验证\n1.  **科学依据：** 该问题坚实地植根于计算医学成像和反问题领域。目标函数是CT中基于模型的迭代重建（MBIR）的标准公式，它结合了加权最小二乘数据保真项（对透射数据的对数似然在计数值较高时的一个有效近似）和总变分（TV）正则化器。近端梯度法是解决此类优化问题的标准且合适算法。该问题在科学上和数学上都是合理的。\n2.  **适定性：** 该问题是适定的。目标函数是凸的，它是一个凸二次函数 $f(x)$ 和一个凸函数 $g(x)$ 的和。问题要求的是一个标准优化算法的正确定义和应用，该算法基于已建立的理论，具有唯一、正确的表述。\n3.  **客观性和清晰度：** 该问题使用精确的数学符号和术语进行陈述。没有歧义或主观陈述。\n\n**结论：** 问题有效。我将继续进行解答。\n\n### 推导\n\n该问题要求为最小化 $F(x) = f(x) + g(x)$ 构建近端梯度法的更新步骤。\n\n**1. 近端梯度法更新规则**\n近端梯度法是一种用于最小化形如 $f(x) + g(x)$ 的复合函数的迭代算法。在每次迭代 $k$ 中，它对光滑部分 $f(x)$ 执行一个梯度下降步骤，然后应用对应于非光滑部分 $g(x)$ 的近端算子。更新规则是：\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\left(x^k - \\tau \\nabla f(x^k)\\right)\n$$\n其中 $\\tau  0$ 是步长。\n\n**2. 近端算子的定义**\n近端算子，记为 $\\operatorname{prox}_{\\tau g}(z)$，被定义为以下最小化问题的解：\n$$\n\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}\n$$\n该算子找到一个点 $u$，这个点是在接近输入点 $z$（项 $\\|u-z\\|_2^2$）和最小化函数 $g(u)$ 之间的一个权衡。参数 $\\tau$ 控制着这个权衡。\n\n**3. 数据保真项的梯度，$\\nabla f(x)$**\n数据保真项是 $f(x) = \\frac{1}{2} \\| W^{1/2} (A x - b) \\|_{2}^{2}$。我们可以将平方欧几里得范数重写为内积：\n$$\nf(x) = \\frac{1}{2} \\left\\langle W^{1/2} (A x - b), W^{1/2} (A x - b) \\right\\rangle\n$$\n由于 $W$ 是一个正定（因此也是对称）矩阵，其平方根 $W^{1/2}$ 也是对称的，即 $(W^{1/2})^{\\top} = W^{1/2}$。\n$$\nf(x) = \\frac{1}{2} (A x - b)^{\\top} (W^{1/2})^{\\top} W^{1/2} (A x - b) = \\frac{1}{2} (A x - b)^{\\top} W (A x - b)\n$$\n为了求梯度，我们可以使用标准的矩阵微积分法则。对于函数 $h(x) = \\frac{1}{2} (Ax-b)^{\\top}Q(Ax-b)$，如果 $Q$ 是对称的，其梯度为 $\\nabla h(x) = A^{\\top}Q(Ax-b)$。在我们的例子中，$Q=W$，它是对称的。\n因此，$f(x)$ 的梯度是：\n$$\n\\nabla f(x) = A^{\\top} W (A x - b)\n$$\n\n**4. 步长条件**\n如果步长 $\\tau$ 选择得当，近端梯度法的收敛性是可以保证的。该条件取决于梯度 $\\nabla f(x)$ 的Lipschitz常数 $L$。如果对所有的 $x_1, x_2$ 都满足 $\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 \\le L \\|x_1 - x_2\\|_2$，则梯度 $\\nabla f(x)$ 是Lipschitz连续的，常数为 $L$。\n为了找到 $L$，我们计算 $f(x)$ 的Hessian矩阵，也就是 $\\nabla f(x)$ 的雅可比矩阵：\n$$\n\\nabla f(x) = A^{\\top} W A x - A^{\\top} W b\n$$\n$$\n\\nabla^2 f(x) = \\frac{\\partial}{\\partial x} (A^{\\top} W A x - A^{\\top} W b) = A^{\\top} W A\n$$\nLipschitz常数 $L$ 是Hessian矩阵的最大特征值（谱范数）。由于 $A^{\\top}W A$ 是一个对称半正定矩阵，它的谱范数就是它的最大特征值。\n$$\nL = \\| A^{\\top} W A \\|_{2}\n$$\n对于任何满足 $0  \\tau  \\frac{2}{L}$ 的步长 $\\tau$，近端梯度法都保证收敛。一个更严格的条件 $0  \\tau \\le \\frac{1}{L}$ 能确保目标函数值在每一步都单调递减，并且也是一个有效的收敛条件。\n$$\n0  \\tau \\le \\frac{1}{\\| A^{\\top} W A \\|_{2}}\n$$\n\n**正确组成部分的总结：**\n- **近端算子定义：** $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$\n- **梯度表达式：** $\\nabla f(x) = A^{\\top} W (A x - b)$\n- **更新规则：** $x^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W (A x^{k} - b) \\big)$\n- **步长条件：** $0  \\tau \\le \\frac{1}{ \\| A^{\\top} W A \\|_{2} }$ 或 $0  \\tau  \\frac{2}{ \\| A^{\\top} W A \\|_{2} }$ 都是有效的。\n\n### 逐项分析选项\n\n**A. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} W (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} W A \\|_{2} }$ 的 $\\tau$。**\n\n- 近端算子的定义是正确的。\n- 梯度 $\\nabla f(x)$ 的表达式是正确的。\n- 更新规则正确地将梯度下降步骤（注意减号）与近端算子结合起来。\n- 步长条件 $0  \\tau \\le \\frac{1}{L}$ 是一个有效且常用的收敛条件。\n所有组成部分都是正确且一致的。\n\n结论：**正确**。\n\n**B. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ \\tau \\, g(u) + \\frac{1}{2} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} W (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} + \\tau \\, A^{\\top} W (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau  \\frac{2}{ \\| A^{\\top} W A \\|_{2} }$ 的 $\\tau$。**\n\n- 近端算子的定义使用了一种替代的（但也是有效的）符号约定，其中算子应用于函数 $\\tau g$。\n- 更新规则在梯度项上使用了“+”号，即 $x^k + \\tau \\nabla f(x^k)$。这是一个梯度*上升*步骤，它会试图最大化 $f(x)$，而不是最小化它。对于一个最小化问题，这在根本上是错误的。\n\n结论：**不正确**。\n\n**C. 不需要近端算子，因为 $g$ 是可微的；使用\n$$\nx^{k+1} = x^{k} - \\tau \\big( A^{\\top} W (A x^{k} - b) + \\lambda \\, D^{\\top} D x^{k} \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} W A + \\lambda D^{\\top} D \\|_{2} }$ 的 $\\tau$。**\n\n- “$g(x) = \\lambda \\|Dx\\|_1$ 是可微的”这个前提是错误的。L1范数在其参数为零时是不可微的。\n- 项 $\\lambda D^{\\top} D x^k$ 不是 $g(x)$ 的梯度。它是 $\\frac{\\lambda}{2}\\|Dx\\|_2^2$ 的梯度，这是一个不同的正则化器（图像梯度上的吉洪诺夫正则化）。所提出的更新是针对一个不同问题的。\n\n结论：**不正确**。\n\n**D. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} W^{-1} (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} W^{-1} (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} W^{-1} A \\|_{2} }$ 的 $\\tau$。**\n\n- 梯度的表达式 $\\nabla f(x) = A^{\\top} W^{-1} (A x - b)$ 是不正确的。它应该是 $A^{\\top} W (A x - b)$。该梯度表达式对应于一个不同的数据保真项 $\\frac{1}{2} (A x - b)^{\\top} W^{-1} (A x - b)$，这与问题陈述中给出的不符。\n\n结论：**不正确**。\n\n**E. $\\operatorname{prox}_{\\tau g}(z) = \\arg\\min_{u} \\left\\{ g(u) + \\frac{1}{2 \\tau} \\| u - z \\|_{2}^{2} \\right\\}$，且当 $\\nabla f(x) = A^{\\top} (A x - b)$ 时，更新为\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\big( x^{k} - \\tau \\, A^{\\top} (A x^{k} - b) \\big),\n$$\n对于任何满足 $0  \\tau \\le \\frac{1}{ \\| A^{\\top} A \\|_{2} }$ 的 $\\tau$。**\n\n- 梯度的表达式 $\\nabla f(x) = A^{\\top} (A x - b)$ 是不正确的。这个表达式仅对非加权最小二乘法有效，这对应于设置 $W=I$（单位矩阵）。问题陈述中说明 $W \\succ 0$ 是一个通用的逆协方差矩阵。\n\n结论：**不正确**。\n\n基于详细分析，只有选项A为给定的CT重建问题提供了完全正确且一致的近端梯度法公式。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}