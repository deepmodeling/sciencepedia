{
    "hands_on_practices": [
        {
            "introduction": "在整合复杂的多组学数据集之前，我们必须首先确保每个数据集都具有高品质。本练习聚焦于任何分析流程中至关重要的第一步：质量控制（QC）。你将学习如何为RNA测序和蛋白质组学数据计算标准的QC指标，并运用稳健的统计方法来识别低质量样本，掌握这项技能对于避免“垃圾进，垃圾出”的问题至关重要，并能确保任何下游多组学分析结果的可靠性。",
            "id": "5062582",
            "problem": "给定来自 $n$ 个样本的核糖核酸测序 (RNA-seq) 和蛋白质组学数据。RNA-seq 数据汇总为一个元基因体覆盖度矩阵 $X \\in \\mathbb{N}^{n \\times p}$，其中每一行对应一个样本，每一列 $j \\in \\{1,\\dots,p\\}$ 对应从 $5'$ 端到 $3'$ 端沿聚合基因体的第 $j$ 个区间中映射到的读数计数。蛋白质组学数据以强度矩阵 $Y \\in \\mathbb{R}^{n \\times q}$ 的形式提供，其中每一行对应一个样本，每一列对应一个肽强度，严格为正的值表示肽段被鉴定出。您的任务是计算每个样本的质量指标，并使用稳健统计方法建立用于标记异常值的阈值。\n\n定义：\n- 样本 $i$ 的单位样本 RNA-seq 文库大小为 $LS_i = \\sum_{j=1}^{p} X_{i,j}$。\n- 在比例 $\\alpha \\in (0,1)$ 下，样本 $i$ 的单位样本 $5'$ 偏好为 $B_i = \\dfrac{\\sum_{j=1}^{k} X_{i,j}}{\\sum_{j=1}^{p} X_{i,j}}$，其中 $k = \\lfloor \\alpha p \\rfloor$。如果 $\\sum_{j=1}^{p} X_{i,j} = 0$，则按惯例定义 $B_i = 0$。将 $B_i$ 表示为小数。\n- 样本 $i$ 的单位样本已鉴定肽段数量为 $P_i = \\#\\{j \\in \\{1,\\dots,q\\} : Y_{i,j} > 0\\}$。\n\n用于异常值检测的稳健阈值：\n- 对于跨样本的指标向量 $v \\in \\mathbb{R}^{n}$（例如，$v = (LS_1,\\dots,LS_n)$，$v = (B_1,\\dots,B_n)$ 或 $v = (P_1,\\dots,P_n)$），计算中位数 $m = \\mathrm{median}(v)$ 和中位数绝对偏差 (MAD)，其定义为 $\\mathrm{MAD} = \\mathrm{median}(|v - m|)$。使用正态一致性常数 $c = 1.4826$ 形成一个稳健尺度 $s = c \\cdot \\mathrm{MAD}$。如果 $s = 0$，则退而使用四分位距 (IQR)，其由四分位数 $Q_1$ 和 $Q_3$ 定义为 $\\mathrm{IQR} = Q_3 - Q_1$，并设置 $s = \\mathrm{IQR}/1.349$。如果 $s$ 仍然为 $0$，则设置 $s = 0$。\n- 使用用户指定的稳健乘数 $z > 0$，将下阈值和上阈值定义为 $T_{\\mathrm{low}} = m - z s$ 和 $T_{\\mathrm{high}} = m + z s$。\n- 当 $LS_i  T_{\\mathrm{low}}$ 和 $P_i  T_{\\mathrm{low}}$ 时，分别标记文库大小和已鉴定肽段数量的低异常值。\n- 当 $B_i  T_{\\mathrm{high}}$ 时，标记 $5'$ 偏好的高异常值。\n\n基本原理：\n- 测序和蛋白质组学测量产生对独立事件进行聚合的计数和强度数据，其中计数的总和和比率提供了有意义的样本级指标。使用中位数和中位数绝对偏差 (MAD) 的稳健统计方法在存在异常值和非高斯噪声的情况下提供了稳定的阈值设定，而当 MAD 为零时，四分位距 (IQR) 提供了一个备用尺度。\n\n实现一个程序，为每个样本计算 $LS_i$、$B_i$ 和 $P_i$，根据上述规则为每个指标推导出 $T_{\\mathrm{low}}$ 和 $T_{\\mathrm{high}}$，并为每个样本返回以下布尔标记：\n- 低 RNA-seq 文库大小，\n- 高 $5'$ 偏好，\n- 低已鉴定肽段数量。\n\n使用以下测试套件。对于每种情况，$n$、$p$ 和 $q$ 由 $X$ 和 $Y$ 的形状隐式给出。\n\n情况 1：参数 $\\alpha = 0.2$，$z = 2$，其中\n- $X^{(1)}$ 行：\n  - 样本 1：$[100,95,90,85,80,75,70,65,60,55]$\n  - 样本 2：$[300,300,10,10,10,10,10,10,10,10]$\n  - 样本 3：$[5,5,5,5,5,5,5,5,5,5]$\n- $Y^{(1)}$ 行：\n  - 样本 1：$[12000,9000,8000,5000,3000,2000,1000,500]$\n  - 样本 2：$[100,50,25,10,5,1,0.5,0]$\n  - 样本 3：$[0,0,0,0,0,0,0,0]$\n\n情况 2：参数 $\\alpha = 0.2$，$z = 3$，其中\n- $X^{(2)}$ 行：\n  - 样本 1：$[50,50,50,50,50,50,50,50,50,50]$\n  - 样本 2：$[50,50,50,50,50,50,50,50,50,50]$\n  - 样本 3：$[50,50,50,50,50,50,50,50,50,50]$\n- $Y^{(2)}$ 行：\n  - 样本 1：$[1,1,1,1,0,0]$\n  - 样本 2：$[1,1,1,1,0,0]$\n  - 样本 3：$[1,1,1,1,0,0]$\n\n情况 3：参数 $\\alpha = 0.2$，$z = 0.5$，其中\n- $X^{(3)}$ 行：\n  - 样本 1：$[40,38,36,34,32,30,28,26,24,22]$\n  - 样本 2：$[0,0,0,0,0,0,0,0,0,0]$\n  - 样本 3：$[10,10,10,10,10,10,10,10,10,10]$\n- $Y^{(3)}$ 行：\n  - 样本 1：$[10,9,0,0,0]$\n  - 样本 2：$[0,0,0,0,0]$\n  - 样本 3：$[5,4,3,2,1]$\n\n角度单位不适用。没有物理单位。诸如 $B_i$ 之类的比例必须表示为小数。您的程序应生成单行输出，其中包含每个案例的三个布尔列表，顺序为 $[$文库大小低值标记, $5'$ 偏好高值标记, 肽段计数低值标记$]$，这三个案例的结果聚合为一个用方括号括起来的逗号分隔列表，例如 $[[[b_{1},\\dots],[b_{1},\\dots],[b_{1},\\dots]],[[...]],[[...]]]$，其中每个 $b_{i}$ 为 $\\mathrm{True}$ 或 $\\mathrm{False}$。",
            "solution": "该问题已经过验证，被认为是合理的。它在科学上基于标准的生物信息学质量控制程序，在数学上定义清晰、约束明确，表述客观。所有必要的测试用例数据和参数均已提供。\n\n任务是根据多组学数据（RNA-seq 和蛋白质组学）为 $n$ 个样本中的每一个计算三个质量控制指标，为这些指标建立稳健的统计阈值，并标记出作为异常值的样本。\n\n每个测试用例的解决方案分三个主要阶段进行：\n1.  计算每个样本的指标：文库大小 ($LS_i$)、$5'$ 偏好 ($B_i$) 和肽段计数 ($P_i$)。\n2.  为每个指标向量推导稳健的异常值阈值 ($T_{\\mathrm{low}}$, $T_{\\mathrm{high}}$)。\n3.  应用阈值标记异常样本。\n\n每个样本 $i \\in \\{1, \\dots, n\\}$ 的输入数据包括一个 RNA-seq 读数计数的行向量 $X_{i,:} \\in \\mathbb{N}^{p}$ 和一个肽强度的行向量 $Y_{i,:} \\in \\mathbb{R}^{q}$。\n\n**1. 单位样本指标计算**\n\n对于每个样本 $i$，我们根据提供的定义计算以下指标：\n- **RNA-seq 文库大小 ($LS_i$)**：样本 RNA-seq 图谱中的读数总数。\n$$LS_i = \\sum_{j=1}^{p} X_{i,j}$$\n- **$5'$ 偏好 ($B_i$)**：落入聚合基因体初始区间的读数比例，这可能表明存在降解或不完全的反转录。给定一个比例 $\\alpha \\in (0,1)$，我们首先确定区间的数量 $k = \\lfloor \\alpha p \\rfloor$。\n$$B_i = \\begin{cases} \\frac{\\sum_{j=1}^{k} X_{i,j}}{\\sum_{j=1}^{p} X_{i,j}}  \\text{if } \\sum_{j=1}^{p} X_{i,j}  0 \\\\ 0  \\text{if } \\sum_{j=1}^{p} X_{i,j} = 0 \\end{cases}$$\n- **已鉴定肽段数量 ($P_i$)**：具有正强度信号的肽段计数，反映了蛋白质组学测量的深度。\n$$P_i = \\#\\{j \\in \\{1, \\dots, q\\} : Y_{i,j}  0\\}$$\n\n**2. 稳健阈值推导**\n\n对于三个指标向量 $v = (LS_1, \\dots, LS_n)$、$v = (B_1, \\dots, B_n)$ 和 $v = (P_1, \\dots, P_n)$ 中的每一个，我们计算异常值阈值。该方法使用稳健统计，与基于均值和标准差的经典方法相比，它对异常值的存在不那么敏感。\n\n-   首先，我们计算指标向量 $v$ 的中位数 $m$：\n    $$m = \\mathrm{median}(v)$$\n-   接下来，我们计算中位数绝对偏差 (MAD)，即与中位数偏差的绝对值的中位数：\n    $$\\mathrm{MAD} = \\mathrm{median}(|v - m|) = \\mathrm{median}(|v_1 - m|, |v_2 - m|, \\dots, |v_n - m|)$$\n-   然后计算尺度的稳健估计值 $s$。主要方法使用 MAD，通过一个常数 $c = 1.4826$ 进行缩放，使其与正态分布数据的标准差具有可比性。\n    $$s = c \\cdot \\mathrm{MAD}$$\n-   为 $\\mathrm{MAD} = 0$ 的情况定义了一个备用机制，这种情况发生在至少一半的数据点相同时。在这种情况下，使用四分位距 (IQR)。\n    $$Q_1 = v \\text{ 的第 } 25 \\text{ 百分位数}$$\n    $$Q_3 = v \\text{ 的第 } 75 \\text{ 百分位数}$$\n    $$\\mathrm{IQR} = Q_3 - Q_1$$\n    然后将尺度设置为 $s = \\mathrm{IQR} / 1.349$。除数 $1.349$ 将 IQR 与正态分布的标准差关联起来。如果此计算结果也为 $s=0$，则将尺度设置为 $s=0$。\n-   最后，使用稳健尺度 $s$ 和用户指定的乘数 $z  0$，将下阈值和上阈值定义为：\n    $$T_{\\mathrm{low}} = m - z s$$\n    $$T_{\\mathrm{high}} = m + z s$$\n\n**3. 异常值标记**\n\n根据这些阈值对样本进行标记：\n-   **低文库大小**：如果 $LS_i  T_{\\mathrm{low}, LS}$，则标记样本 $i$。\n-   **高 $5'$ 偏好**：如果 $B_i  T_{\\mathrm{high}, B}$，则标记样本 $i$。\n-   **低肽段计数**：如果 $P_i  T_{\\mathrm{low}, P}$，则标记样本 $i$。\n\n---\n\n**测试用例执行**\n\n**情况 1: $\\alpha = 0.2, z = 2$**\n-   $n=3, p=10, q=8$。$k = \\lfloor 0.2 \\times 10 \\rfloor = 2$。\n-   **指标**：\n    -   $LS = [\\sum X_{1,j}, \\sum X_{2,j}, \\sum X_{3,j}] = [775, 680, 50]$。\n    -   $B = [195/775, 600/680, 10/50] \\approx [0.2516, 0.8824, 0.2]$。\n    -   $P = [8, 7, 0]$。\n-   **阈值与标记**：\n    -   $v=LS$：$m=680$，$\\mathrm{MAD}=95$，$s = 1.4826 \\times 95 \\approx 140.85$。$T_{\\mathrm{low}} = 680 - 2 \\times 140.85 = 398.3$。$LS_3=50  398.3$。标记：`[False, False, True]`。\n    -   $v=B$：$m \\approx 0.2516$，$\\mathrm{MAD} \\approx |0.2 - 0.2516| = 0.0516$。$s = 1.4826 \\times 0.0516 \\approx 0.0765$。$T_{\\mathrm{high}} = 0.2516 + 2 \\times 0.0765 \\approx 0.4046$。$B_2 \\approx 0.8824  0.4046$。标记：`[False, True, False]`。\n    -   $v=P$：$m=7$，$\\mathrm{MAD}=1$，$s = 1.4826 \\times 1 = 1.4826$。$T_{\\mathrm{low}} = 7 - 2 \\times 1.4826 = 4.0348$。$P_3=0  4.0348$。标记：`[False, False, True]`。\n-   **结果**：`[[False, False, True], [False, True, False], [False, False, True]]`\n\n**情况 2: $\\alpha = 0.2, z = 3$**\n-   $n=3, p=10, q=6$。$k = \\lfloor 0.2 \\times 10 \\rfloor = 2$。所有样本都相同。\n-   **指标**：\n    -   $LS = [500, 500, 500]$。\n    -   $B = [0.2, 0.2, 0.2]$。\n    -   $P = [4, 4, 4]$。\n-   **阈值与标记**：\n    -   对于所有三个指标，值向量 $v$ 的所有元素都相同。这导致 $\\mathrm{MAD}=0$ 和 $\\mathrm{IQR}=0$。因此，尺度 $s=0$。\n    -   $T_{\\mathrm{low}} = m - z \\cdot 0 = m$ 且 $T_{\\mathrm{high}} = m + z \\cdot 0 = m$。\n    -   由于所有 $v_i=m$，异常值条件 $v_i  m$ 和 $v_i  m$ 永远不会满足。\n    -   所有标记均为 `False`。\n-   **结果**：`[[False, False, False], [False, False, False], [False, False, False]]`\n\n**情况 3: $\\alpha = 0.2, z = 0.5$**\n-   $n=3, p=10, q=5$。$k = \\lfloor 0.2 \\times 10 \\rfloor = 2$。\n-   **指标**：\n    -   $LS = [310, 0, 100]$。\n    -   $B = [78/310, 0, 20/100] \\approx [0.2516, 0.0, 0.2]$。\n    -   $P = [2, 0, 5]$。\n-   **阈值与标记**：\n    -   $v=LS$：$m=100$，$\\mathrm{MAD}=100$，$s = 1.4826 \\times 100 = 148.26$。$T_{\\mathrm{low}} = 100 - 0.5 \\times 148.26 = 25.87$。$LS_2=0  25.87$。标记：`[False, True, False]`。\n    -   $v=B$：$m=0.2$，$\\mathrm{MAD} = |0.2516-0.2| \\approx 0.0516$。$s = 1.4826 \\times 0.0516 \\approx 0.0765$。$T_{\\mathrm{high}} = 0.2 + 0.5 \\times 0.0765 \\approx 0.2383$。$B_1 \\approx 0.2516  0.2383$。标记：`[True, False, False]`。\n    -   $v=P$：$m=2$，$\\mathrm{MAD}=2$，$s = 1.4826 \\times 2 = 2.9652$。$T_{\\mathrm{low}} = 2 - 0.5 \\times 2.9652 = 0.5174$。$P_2=0  0.5174$。标记：`[False, True, False]`。\n-   **结果**：`[[False, True, False], [True, False, False], [False, True, False]]`\n\n最终的实现将以编程方式执行这些计算。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"matrices\": {\n                \"X\": np.array([\n                    [100, 95, 90, 85, 80, 75, 70, 65, 60, 55],\n                    [300, 300, 10, 10, 10, 10, 10, 10, 10, 10],\n                    [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n                ], dtype=np.int64),\n                \"Y\": np.array([\n                    [12000, 9000, 8000, 5000, 3000, 2000, 1000, 500],\n                    [100, 50, 25, 10, 5, 1, 0.5, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0]\n                ], dtype=np.float64)\n            },\n            \"params\": {\"alpha\": 0.2, \"z\": 2.0}\n        },\n        {\n            \"matrices\": {\n                \"X\": np.array([\n                    [50, 50, 50, 50, 50, 50, 50, 50, 50, 50],\n                    [50, 50, 50, 50, 50, 50, 50, 50, 50, 50],\n                    [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n                ], dtype=np.int64),\n                \"Y\": np.array([\n                    [1, 1, 1, 1, 0, 0],\n                    [1, 1, 1, 1, 0, 0],\n                    [1, 1, 1, 1, 0, 0]\n                ], dtype=np.float64)\n            },\n            \"params\": {\"alpha\": 0.2, \"z\": 3.0}\n        },\n        {\n            \"matrices\": {\n                \"X\": np.array([\n                    [40, 38, 36, 34, 32, 30, 28, 26, 24, 22],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n                ], dtype=np.int64),\n                \"Y\": np.array([\n                    [10, 9, 0, 0, 0],\n                    [0, 0, 0, 0, 0],\n                    [5, 4, 3, 2, 1]\n                ], dtype=np.float64)\n            },\n            \"params\": {\"alpha\": 0.2, \"z\": 0.5}\n        }\n    ]\n\n    all_case_results = []\n    for case in test_cases:\n        X = case[\"matrices\"][\"X\"]\n        Y = case[\"matrices\"][\"Y\"]\n        alpha = case[\"params\"][\"alpha\"]\n        z = case[\"params\"][\"z\"]\n        \n        case_result = analyze_multi_omics(X, Y, alpha, z)\n        all_case_results.append(case_result)\n    \n    # Format the final output string as specified\n    case_strings = [f\"[{','.join(map(str, res))}]\" for res in all_case_results]\n    final_output_string = f\"[{','.join(case_strings)}]\"\n    print(final_output_string)\n\n\ndef get_robust_thresholds(v, z):\n    \"\"\"\n    Computes robust lower and upper thresholds for a given metric vector.\n    \"\"\"\n    if v.size == 0:\n        return 0.0, 0.0\n        \n    m = np.median(v)\n    \n    # Calculate MAD and the robust scale s\n    c = 1.4826\n    mad = np.median(np.abs(v - m))\n    s = c * mad\n    \n    # Fallback to IQR if MAD-based scale is zero\n    if s == 0:\n        q1 = np.quantile(v, 0.25)\n        q3 = np.quantile(v, 0.75)\n        iqr = q3 - q1\n        s = iqr / 1.349\n    \n    # Final thresholds\n    t_low = m - z * s\n    t_high = m + z * s\n    \n    return t_low, t_high\n\ndef analyze_multi_omics(X, Y, alpha, z):\n    \"\"\"\n    Performs the full quality control analysis for one case.\n    \"\"\"\n    n, p = X.shape\n    q = Y.shape[1]\n\n    # 1. Calculate per-sample metrics\n    \n    # RNA-seq Library Size (LS)\n    ls_vec = np.sum(X, axis=1)\n\n    # RNA-seq 5' Bias (B)\n    k = int(np.floor(alpha * p))\n    b_vec = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        if ls_vec[i]  0:\n            b_vec[i] = np.sum(X[i, :k]) / ls_vec[i]\n        else:\n            b_vec[i] = 0.0\n            \n    # Number of Identified Peptides (P)\n    p_vec = np.sum(Y  0, axis=1)\n\n    # 2. Derive thresholds and flag outliers\n    \n    # For Library Size (LS) - flag low outliers\n    t_low_ls, _ = get_robust_thresholds(ls_vec, z)\n    ls_flags = (ls_vec  t_low_ls).tolist()\n\n    # For 5' Bias (B) - flag high outliers\n    _, t_high_b = get_robust_thresholds(b_vec, z)\n    b_flags = (b_vec  t_high_b).tolist()\n    \n    # For Peptide Count (P) - flag low outliers\n    t_low_p, _ = get_robust_thresholds(p_vec, z)\n    p_flags = (p_vec  t_low_p).tolist()\n\n    return [ls_flags, b_flags, p_flags]\n\nsolve()\n```"
        },
        {
            "introduction": "多组学的一个关键目标是通过基因表达等中间分子性状，将遗传变异与疾病联系起来。本练习将介绍一种实现这一目标的强大方法：全转录组关联研究（TWAS）。通过整合全基因组关联研究（GWAS）数据与表达数量性状位点（eQTL）信息，你将学习如何检验那些基因的遗传预测表达水平是否与某种疾病相关，从而在一种用于筛选疾病相关基因的前沿技术方面获得动手经验。",
            "id": "5062552",
            "problem": "您将处理一个为高级本科生理解医学遗传学而设计的综合性多组学整合场景。目标是通过整合表达数量性状基因座 (eQTL) 权重和全基因组关联研究 (GWAS) 的单核苷酸多态性 (SNP) 摘要统计数据，并考虑连锁不平衡 (LD)，来实现一个简化的转录组范围关联研究 (TWAS)。然后，您将计算跨独立队列和跨组织的重现性指标。\n\n基本原理和定义：分子生物学中心法则指出，脱氧核糖核酸 (DNA) 转录为核糖核酸 (RNA)，然后翻译成蛋白质。表达数量性状基因座 (eQTL) 将遗传变异与基因表达关联起来。在线性预测模型中，带有源自 eQTL 权重的 SNP 的遗传变异可用于预测组织中的基因表达。GWAS 摘要统计数据提供标准化的 SNP 水平与表型的关联。跨 SNP 的连锁不平衡 (LD) 引入了相关性，在形成 SNP 水平统计数据的线性聚合时必须将其纳入考虑。在正态性的标准假设下，标准化的基因水平关联统计量可以通过其由 LD 引起的方差来适当地归一化 SNP 水平 GWAS 统计量的线性组合而得出。统计显著性使用标准正态分布进行评估。\n\n您的任务全部以纯数学和计算术语表示，如下所示：\n1. 对于每个基因、组织和队列，使用从 eQTL 导出的 SNP 权重构建的基因表达线性预测器，整合 GWAS SNP 水平的标准化统计数据和 LD 协方差矩阵来归一化统计量，从而推导并计算标准化的 TWAS 检验统计量。\n2. 使用标准正态分布将得到的标准化统计量转换为双侧 p 值。如果 p 值严格小于显著性阈值 $\\alpha$，则认为该基因-组织-队列的关联是显著的。\n3. 按如下方式计算每个组织的跨队列重复率（以小数表示）：使用队列 $1$ 作为参考队列。对于组织 $t$，在队列 $1$ 中显著的基因集合中，统计有多少基因在队列 $2$ 中也显著，并且在两个队列中 TWAS 统计量的符号相同。该重复率是这个计数除以队列 $1$ 中显著基因的数量。如果队列 $1$ 中没有显著基因，则该组织的重复率定义为 $0.0$。\n4. 计算每个队列内的跨组织重现性，即在同一队列内，组织 $1$ 和组织 $2$ 的 TWAS 统计量在所有基因间的皮尔逊相关性。如果某队列中任一组织的 TWAS 统计量在所有基因间的标准差为零，则为避免未定义行为，该队列的相关性定义为 $0.0$。\n5. 对每个基因和组织，使用按队列样本量的平方根加权的 Stouffer 方法，进行跨队列的固定效应荟萃分析。将同一基因和组织的队列水平标准化 TWAS 统计量合并为单个荟萃分析统计量，转换为双侧 p 值，并在阈值 $\\alpha$ 下判断荟萃分析的显著性。以在组织 $1$ 荟萃分析中显著的基因集为参考，计算跨组织荟萃分析的重复率（以小数表示），统计其中在组织 $2$ 荟萃分析中也显著且合并统计量符号相同的基因数量，然后除以参考集的大小。如果参考集为空，则此重复率定义为 $0.0$。\n\n单位：不涉及物理单位或角度。所有比率必须以小数表示。最终输出中所有报告的值必须是数字。\n\n为确保数值稳健性而采用的算法约定：\n- 如果由于权重向量为零，某个基因-组织的 LD 归一化项等于 $0$，则为避免除以零，将标准化的 TWAS 统计量定义为 $0.0$。\n- 使用从标准正态分布计算出的双侧 p 值，即 $p = 2 \\cdot \\Phi(-|Z|)$，其中 $Z$ 是标准化的 TWAS 统计量，$\\Phi$ 是标准正态分布的累积分布函数。\n- 对于皮尔逊相关性，如果任一向量的标准差为零，则返回 $0.0$。\n\n具有指定参数值的测试套件：\n案例 $1$：\n- 组织：$2$ 个组织，标记为 $T1$ 和 $T2$。\n- 队列：$2$ 个队列，标记为 $C1$ 和 $C2$，样本量为 $N_{1} = 5000$ 和 $N_{2} = 7000$。\n- 显著性阈值：$\\alpha = 0.05$。\n- 基因：$3$ 个基因，标记为 $g1$、$g2$ 和 $g3$。\n- 每个组织和基因的 eQTL 衍生权重：\n  - 组织 $T1$：$g1: [0.6, -0.2]$, $g2: [0.5, 0.0, -0.3]$, $g3: [0.0]$。\n  - 组织 $T2$：$g1: [0.3, 0.1]$, $g2: [0.2, 0.4, 0.1]$, $g3: [0.5]$。\n- 每个基因的 LD 协方差矩阵：\n  - $g1$: $\\Sigma = \\begin{bmatrix} 1.0  0.2 \\\\ 0.2  1.0 \\end{bmatrix}$。\n  - $g2$: $\\Sigma = \\operatorname{diag}(1.0, 1.0, 1.0)$。\n  - $g3$: $\\Sigma = [1.0]$。\n- 每个队列的 GWAS 标准化 SNP 水平统计量 $z$：\n  - 队列 $C1$：$g1: [1.2, -0.8]$, $g2: [0.1, 3.5, -2.8]$, $g3: [-0.5]$。\n  - 队列 $C2$：$g1: [1.0, -0.6]$, $g2: [0.2, 3.0, -2.0]$, $g3: [-0.3]$。\n\n案例 $2$：\n- 组织：$2$ 个组织，标记为 $T1$ 和 $T2$。\n- 队列：$2$ 个队列，标记为 $C1$ 和 $C2$，样本量为 $N_{1} = 1000$ 和 $N_{2} = 1000$。\n- 显著性阈值：$\\alpha = 0.05$。\n- 基因：$2$ 个基因，标记为 $g1$ 和 $g2$。\n- 每个组织和基因的 eQTL 衍生权重：\n  - 组织 $T1$：$g1: [0.0, 0.0]$, $g2: [-0.5, 0.3]$。\n  - 组织 $T2$：$g1: [0.1, 0.1]$, $g2: [0.5, -0.3]$。\n- 每个基因的 LD 协方差矩阵：\n  - $g1$: $\\Sigma = \\begin{bmatrix} 1.0  0.5 \\\\ 0.5  1.0 \\end{bmatrix}$。\n  - $g2$: $\\Sigma = \\begin{bmatrix} 1.0  0.5 \\\\ 0.5  1.0 \\end{bmatrix}$。\n- 每个队列的 GWAS 标准化 SNP 水平统计量 $z$：\n  - 队列 $C1$：$g1: [2.0, -2.0]$, $g2: [2.4, 0.8]$。\n  - 队列 $C2$：$g1: [1.5, -1.5]$, $g2: [-1.8, -0.4]$。\n\n最终输出格式要求：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。对于案例 $1$，按以下顺序生成 $5$ 个浮点值：组织 $T1$ 的重复率、组织 $T2$ 的重复率、队列 $C1$ 内的跨组织皮尔逊相关性、队列 $C2$ 内的跨组织皮尔逊相关性以及跨组织荟萃分析重复率。对于案例 $2$，按相同顺序生成相同的 $5$ 个值。将这两组合并成一个列表，总共包含 $10$ 个值。例如，输出必须类似于 $[v_1,v_2,v_3,v_4,v_5,v_6,v_7,v_8,v_9,v_{10}]$，其中每个 $v_i$ 是您的程序针对相应指标和案例计算的浮点数。\n\n您的实现不得读取任何输入，必须仅依赖于上面提供的参数。所有比率都必须是小数。问题中不出现单位或角度。",
            "solution": "所呈现的问题是医学遗传学领域中一个明确定义的计算任务，具体来说是转录组范围关联研究 (TWAS) 的简化模拟。它在科学上基于既定原则，提供了一套完整且一致的数据和定义，并要求计算具体的、客观的指标。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n解决方案的结构遵循问题陈述中概述的五个任务。对于每个基因 $g$、组织 $t$ 和队列 $c$，我们将计算必要的统计数据。设 $W_{g,t}$ 为 eQTL 衍生的 SNP 权重向量，$z_{g,c}$ 为 GWAS 标准化的 SNP 水平统计量向量，$\\Sigma_g$ 为与基因 $g$ 相关的 SNP 的 LD 协方差矩阵。队列 $c \\in \\{1, 2\\}$ 的样本量表示为 $N_c$。显著性阈值为 $\\alpha$。\n\n**1. 标准化 TWAS 检验统计量 ($Z_{g,t,c}$)**\n\n标准化的 TWAS 检验统计量 $Z_{g,t,c}$ 是通过取 GWAS SNP 统计量的线性组合（由它们对基因表达的 eQTL 效应加权），并用其在零假设下的标准差对该总和进行归一化而得出的。未归一化的统计量是点积 $W_{g,t}^T z_{g,c}$。由 SNP 之间的相关结构 (LD) 引起的该线性组合的方差由二次型 $W_{g,t}^T \\Sigma_g W_{g,t}$ 给出。因此，标准化的统计量为：\n\n$$ Z_{g,t,c} = \\frac{W_{g,t}^T z_{g,c}}{\\sqrt{W_{g,t}^T \\Sigma_g W_{g,t}}} $$\n\n根据问题的算法约定，如果分母为 $0$（当 $W_{g,t}$ 是零向量时会发生），则 $Z_{g,t,c}$ 定义为 $0.0$。\n\n**2. 双侧 p 值 ($p_{g,t,c}$)**\n\n在无关联的零假设下，标准化统计量 $Z_{g,t,c}$ 服从标准正态分布 $\\mathcal{N}(0, 1)$。双侧 p 值是观察到等于或比计算出的检验统计量更极端的检验统计量的概率。其计算方式如下：\n\n$$ p_{g,t,c} = 2 \\cdot \\Phi(-|Z_{g,t,c}|) $$\n\n其中 $\\Phi$ 是标准正态分布的累积分布函数 (CDF)。如果 $p_{g,t,c}  \\alpha$，则认为该关联具有统计显著性。\n\n**3. 跨队列重复率 ($R_t$)**\n\n该指标评估两个独立队列之间显著发现的一致性。队列 $1$ 作为参考。对于每个组织 $t$，我们首先确定在队列 $1$ 中显著的基因集合，表示为 $S_{t,C1} = \\{g \\mid p_{g,t,C1}  \\alpha \\}$。这类基因的数量为 $N_{sig,t,C1} = |S_{t,C1}|$。\n\n然后，我们统计这个集合中，在队列 $2$ 中也显著且关联方向相同的基因数量（即它们的 $Z$ 分数符号相同）。这个计数为 $N_{rep,t} = |\\{g \\in S_{t,C1} \\mid p_{g,t,C2}  \\alpha \\text{ and } \\operatorname{sign}(Z_{g,t,C1}) = \\operatorname{sign}(Z_{g,t,C2}) \\}|$。\n\n组织 $t$ 的重复率是比率：\n\n$$ R_t = \\frac{N_{rep,t}}{N_{sig,t,C1}} $$\n\n如果 $N_{sig,t,C1} = 0$，则重复率 $R_t$ 定义为 $0.0$。\n\n**4. 跨组织重现性 ($\\rho_c$)**\n\n该指标衡量同一队列内两个不同组织之间基因水平关联信号的相关性。对于每个队列 $c$，我们为每个组织构建一个跨所有基因的 Z 分数向量：\n$\\mathbf{Z}_{T1, c} = (Z_{g1,T1,c}, Z_{g2,T1,c}, \\dots, Z_{G,T1,c})$\n$\\mathbf{Z}_{T2, c} = (Z_{g1,T2,c}, Z_{g2,T2,c}, \\dots, Z_{G,T2,c})$\n其中 $G$ 是基因总数。\n\n重现性是这两个向量之间的皮尔逊相关系数：\n\n$$ \\rho_c = \\operatorname{Corr}(\\mathbf{Z}_{T1, c}, \\mathbf{Z}_{T2, c}) $$\n\n根据规定，如果任一向量的标准差为 $0$，则 $\\rho_c$ 定义为 $0.0$。\n\n**5. 荟萃分析与荟萃重复率 ($R_{meta}$)**\n\n固定效应荟萃分析结合了两个队列的结果，以便为每个基因-组织对获得一个更具统计功效的单一关联估计。我们使用 Stouffer 方法，通过每个队列样本量的平方根 $w_c = \\sqrt{N_c}$ 对其 Z 分数进行加权。组合后的 Z 分数为：\n\n$$ Z_{meta,g,t} = \\frac{\\sum_{c=1}^{2} w_c Z_{g,t,c}}{\\sqrt{\\sum_{c=1}^{2} w_c^2}} = \\frac{\\sqrt{N_1} Z_{g,t,C1} + \\sqrt{N_2} Z_{g,t,C2}}{\\sqrt{N_1 + N_2}} $$\n\n根据这些荟萃分析 Z 分数，我们计算 p 值并如步骤 $2$ 中所述确定显著性。\n\n最后，我们计算跨组织荟萃分析重复率 $R_{meta}$。这类似于跨队列重复率，但应用于荟萃分析的结果。组织 $1$ 是参考。我们找到在组织 $1$ 荟萃分析中显著的基因集 $S_{meta,T1} = \\{ g \\mid p_{meta,g,T1}  \\alpha \\}$。然后，我们统计其中有多少基因在组织 $2$ 荟萃分析中也显著，并且 $Z_{meta}$ 的符号相同。该重复率是此计数与参考集大小 $|S_{meta,T1}|$ 的比率。如果参考集为空，则该重复率为 $0.0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main solver function that processes the test cases and prints the final results.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"tissues\": [\"T1\", \"T2\"],\n            \"cohorts\": [\"C1\", \"C2\"],\n            \"n_samples\": {\"C1\": 5000, \"C2\": 7000},\n            \"alpha\": 0.05,\n            \"genes\": [\"g1\", \"g2\", \"g3\"],\n            \"weights\": {\n                \"g1\": {\"T1\": np.array([0.6, -0.2]), \"T2\": np.array([0.3, 0.1])},\n                \"g2\": {\"T1\": np.array([0.5, 0.0, -0.3]), \"T2\": np.array([0.2, 0.4, 0.1])},\n                \"g3\": {\"T1\": np.array([0.0]), \"T2\": np.array([0.5])},\n            },\n            \"ld_matrices\": {\n                \"g1\": np.array([[1.0, 0.2], [0.2, 1.0]]),\n                \"g2\": np.diag([1.0, 1.0, 1.0]),\n                \"g3\": np.array([[1.0]]),\n            },\n            \"gwas_stats\": {\n                \"g1\": {\"C1\": np.array([1.2, -0.8]), \"C2\": np.array([1.0, -0.6])},\n                \"g2\": {\"C1\": np.array([0.1, 3.5, -2.8]), \"C2\": np.array([0.2, 3.0, -2.0])},\n                \"g3\": {\"C1\": np.array([-0.5]), \"C2\": np.array([-0.3])},\n            },\n        },\n        {\n            \"tissues\": [\"T1\", \"T2\"],\n            \"cohorts\": [\"C1\", \"C2\"],\n            \"n_samples\": {\"C1\": 1000, \"C2\": 1000},\n            \"alpha\": 0.05,\n            \"genes\": [\"g1\", \"g2\"],\n            \"weights\": {\n                \"g1\": {\"T1\": np.array([0.0, 0.0]), \"T2\": np.array([0.1, 0.1])},\n                \"g2\": {\"T1\": np.array([-0.5, 0.3]), \"T2\": np.array([0.5, -0.3])},\n            },\n            \"ld_matrices\": {\n                \"g1\": np.array([[1.0, 0.5], [0.5, 1.0]]),\n                \"g2\": np.array([[1.0, 0.5], [0.5, 1.0]]),\n            },\n            \"gwas_stats\": {\n                \"g1\": {\"C1\": np.array([2.0, -2.0]), \"C2\": np.array([1.5, -1.5])},\n                \"g2\": {\"C1\": np.array([2.4, 0.8]), \"C2\": np.array([-1.8, -0.4])},\n            },\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = process_case(case)\n        all_results.extend(results)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case, computing all required metrics.\n    \"\"\"\n    alpha = params[\"alpha\"]\n    tissues = params[\"tissues\"]\n    cohorts = params[\"cohorts\"]\n    genes = params[\"genes\"]\n    n_samples = params[\"n_samples\"]\n\n    # --- Task 1  2: Compute Z-scores and p-values ---\n    twas_stats = {}  # {gene: {tissue: {cohort: {'z': ..., 'p': ...}}}}\n    for g in genes:\n        twas_stats[g] = {}\n        for t in tissues:\n            twas_stats[g][t] = {}\n            for c in cohorts:\n                W = params[\"weights\"][g][t]\n                Z_snp = params[\"gwas_stats\"][g][c]\n                Sigma = params[\"ld_matrices\"][g]\n                \n                # Calculate normalization term\n                norm_term_sq = W.T @ Sigma @ W\n\n                if norm_term_sq == 0:\n                    z_score = 0.0\n                else:\n                    numerator = W.T @ Z_snp\n                    z_score = numerator / np.sqrt(norm_term_sq)\n                \n                p_value = 2 * norm.cdf(-np.abs(z_score))\n                twas_stats[g][t][c] = {\"z\": z_score, \"p\": p_value}\n\n    # --- Task 3: Cross-cohort replication rate ---\n    # Reference cohort is C1 (the first in the list)\n    ref_cohort = cohorts[0]\n    comp_cohort = cohorts[1]\n    replication_rates = []\n    for t in tissues:\n        sig_in_ref = {g for g in genes if twas_stats[g][t][ref_cohort][\"p\"]  alpha}\n        n_sig_ref = len(sig_in_ref)\n        \n        if n_sig_ref == 0:\n            replication_rates.append(0.0)\n            continue\n            \n        n_replicated = 0\n        for g in sig_in_ref:\n            is_sig_in_comp = twas_stats[g][t][comp_cohort][\"p\"]  alpha\n            z_ref = twas_stats[g][t][ref_cohort][\"z\"]\n            z_comp = twas_stats[g][t][comp_cohort][\"z\"]\n            signs_match = np.sign(z_ref) == np.sign(z_comp)\n            \n            if is_sig_in_comp and signs_match:\n                n_replicated += 1\n        \n        replication_rates.append(n_replicated / n_sig_ref)\n\n    # --- Task 4: Cross-tissue Pearson correlation ---\n    ref_tissue = tissues[0]\n    comp_tissue = tissues[1]\n    correlations = []\n    for c in cohorts:\n        z_vec_t1 = np.array([twas_stats[g][ref_tissue][c][\"z\"] for g in genes])\n        z_vec_t2 = np.array([twas_stats[g][comp_tissue][c][\"z\"] for g in genes])\n\n        if np.std(z_vec_t1) == 0 or np.std(z_vec_t2) == 0:\n            correlations.append(0.0)\n        else:\n            corr_matrix = np.corrcoef(z_vec_t1, z_vec_t2)\n            correlations.append(corr_matrix[0, 1])\n\n    # --- Task 5: Meta-analysis and meta-replication rate ---\n    meta_stats = {} # {gene: {tissue: {'z': ..., 'p': ...}}}\n    w1 = np.sqrt(n_samples[cohorts[0]])\n    w2 = np.sqrt(n_samples[cohorts[1]])\n    meta_norm_term = np.sqrt(w1**2 + w2**2)\n    \n    for g in genes:\n        meta_stats[g] = {}\n        for t in tissues:\n            z1 = twas_stats[g][t][cohorts[0]][\"z\"]\n            z2 = twas_stats[g][t][cohorts[1]][\"z\"]\n            \n            meta_z = (w1 * z1 + w2 * z2) / meta_norm_term\n            meta_p = 2 * norm.cdf(-np.abs(meta_z))\n            meta_stats[g][t] = {\"z\": meta_z, \"p\": meta_p}\n\n    # Meta-replication\n    sig_meta_t1 = {g for g in genes if meta_stats[g][ref_tissue][\"p\"]  alpha}\n    n_sig_meta_t1 = len(sig_meta_t1)\n    \n    meta_replication_rate = 0.0\n    if n_sig_meta_t1  0:\n        n_meta_replicated = 0\n        for g in sig_meta_t1:\n            is_sig_meta_t2 = meta_stats[g][comp_tissue][\"p\"]  alpha\n            z_meta_t1 = meta_stats[g][ref_tissue][\"z\"]\n            z_meta_t2 = meta_stats[g][comp_tissue][\"z\"]\n            signs_match = np.sign(z_meta_t1) == np.sign(z_meta_t2)\n            \n            if is_sig_meta_t2 and signs_match:\n                n_meta_replicated += 1\n        meta_replication_rate = n_meta_replicated / n_sig_meta_t1\n        \n    case_results = [\n        replication_rates[0],  # rep rate T1\n        replication_rates[1],  # rep rate T2\n        correlations[0],       # corr C1\n        correlations[1],       # corr C2\n        meta_replication_rate\n    ]\n    return case_results\n\nsolve()\n```"
        },
        {
            "introduction": "当一个遗传区域同时与疾病（来自GWAS）和基因表达（来自eQTL）相关时，我们很容易假设它们共享一个共同的致病变异。然而，由于复杂的连锁不平衡模式，情况并非总是如此，而本练习将通过统计共定位的方法来应对这一挑战。你将实践一个贝叶斯框架，用以计算GWAS信号和eQTL信号真正共存于同一因果变异的概率，这个练习将培养你在遗传研究中对因果关系的批判性思维，并介绍灵敏度分析的重要概念。",
            "id": "5062585",
            "problem": "给定一个简化的、有原则的框架，用于在单个基因组位点上，使用全基因组关联研究 (GWAS) 和表达数量性状位点 (eQTL) 信号执行统计共定位，并量化其对贝叶斯先验选择以及连锁不平衡 (LD) 参考面板差异的敏感性。其基本出发点是：包括贝叶斯定理在内的概率法则、标准化回归统计量的高斯似然，以及连锁不平衡作为单核苷酸多态性 (SNP) 之间相关性结构的定义。您必须从这些原则中推导出一个可实现的算法，并计算两种性状共享相同因果变异的后验概率。\n\n从以下基本原则和定义开始：\n- 假设一个位点内有 $M$ 个 SNP，索引为 $j \\in \\{1,\\dots,M\\}$。对于每个性状，边际 $z$ 分数向量（记为 $\\mathbf{z}_{\\mathrm{marg}} \\in \\mathbb{R}^{M}$）是通过将性状分别对每个 SNP 进行线性回归得到的，其中基因型已标准化为单位方差。在单位点因果变异近似和标准化基因型下，边际 $z$ 分数向量的期望值满足 $\\mathbb{E}[\\mathbf{z}_{\\mathrm{marg}}] = \\mathbf{R}\\,\\mathbf{z}_{\\mathrm{causal}}$，其中 $\\mathbf{R} \\in \\mathbb{R}^{M \\times M}$ 是 LD 相关性矩阵，$\\mathbf{z}_{\\mathrm{causal}} \\in \\mathbb{R}^{M}$ 是一个稀疏向量，其非零项对应于因果 SNP。\n- 根据多元正态回归和线性代数的性质，可通过求解线性系统 $\\mathbf{R}\\,\\widehat{\\mathbf{z}}_{\\mathrm{causal}} = \\mathbf{z}_{\\mathrm{marg}}$ 来获得假定因果 $z$ 分数向量的估计量，即当 $\\mathbf{R}$ 可逆时，$\\widehat{\\mathbf{z}}_{\\mathrm{causal}} = \\mathbf{R}^{-1}\\,\\mathbf{z}_{\\mathrm{marg}}$。此步骤考虑了 LD 结构，并将取决于所选的 LD 参考面板。\n- 对于每个 SNP $j$，假设在备择假设下，真实标准化效应大小 $\\beta_{j}$ 服从高斯先验 $\\beta_{j} \\sim \\mathcal{N}(0, W)$，而在原假设下为位于 $0$ 的点质量。在给定 $\\beta_{j}$ 的条件下，对于观测到的标准化效应（等同于 $z$ 分数）使用高斯似然，比较备择假设与原假设的单位点贝叶斯因子是 Wakefield 近似贝叶斯因子：\n$$\n\\mathrm{ABF}(z_{j}; W) \\;=\\; \\sqrt{\\frac{1}{1 + W}} \\;\\exp\\!\\left(\\frac{1}{2}\\,\\frac{W}{1+W}\\,z_{j}^{2}\\right),\n$$\n其中 $z_{j}$ 是标准化效应（此处我们使用解卷积后的估计值 $z_{j} = \\widehat{z}_{\\mathrm{causal},j}$），$W$ 是备择假设下标准化效应大小的先验方差。\n- 为两种性状（性状 1 为 GWAS，性状 2 为 eQTL）定义以下假设：$H_{0}$（两种性状均无关联），$H_{1}$（仅性状 1 有关联），$H_{2}$（仅性状 2 有关联），$H_{3}$（两种性状均有关联，但由不同的因果变异驱动），以及 $H_{4}$（两种性状均有关联，且由相同的因果变异驱动）。假设单位点先验概率 $p_{1}$ 表示仅性状 1 具有因果性，$p_{2}$ 表示仅性状 2 具有因果性，$p_{12}$ 表示在同一 SNP 上具有共享因果性。在稀有因果性近似下，每个假设的未归一化后验权重为：\n$$\nw_{0} \\;=\\; 1,\n$$\n$$\nw_{1} \\;=\\; \\sum_{j=1}^{M} p_{1}\\,\\mathrm{ABF}_{1,j},\n\\quad\nw_{2} \\;=\\; \\sum_{j=1}^{M} p_{2}\\,\\mathrm{ABF}_{2,j},\n$$\n$$\nw_{3} \\;=\\; \\sum_{\\substack{j=1\\\\k=1\\\\j\\neq k}}^{M} p_{1}\\,p_{2}\\,\\mathrm{ABF}_{1,j}\\,\\mathrm{ABF}_{2,k},\n\\quad\nw_{4} \\;=\\; \\sum_{j=1}^{M} p_{12}\\,\\mathrm{ABF}_{1,j}\\,\\mathrm{ABF}_{2,j},\n$$\n其中 $\\mathrm{ABF}_{1,j}$ 和 $\\mathrm{ABF}_{2,j}$ 分别是性状 1 和性状 2 的单位点贝叶斯因子，由相应的解卷积 $z$ 分数计算得出。$H_{4}$ 的后验概率则为\n$$\n\\mathrm{PP}_{4} \\;=\\; \\frac{w_{4}}{w_{0} + w_{1} + w_{2} + w_{3} + w_{4}}.\n$$\n\n您的任务是严格按照规定实现上述模型，并在一个包含不同先验选择和 LD 参考面板的小型测试套件上计算 $\\mathrm{PP}_{4}$。\n\n使用以下数值上指定且科学上合理的测试输入。\n\n- SNP 数量: $M = 5$。\n\n- LD 参考面板 A (矩阵 $\\mathbf{R}_{A}$):\n$$\n\\begin{bmatrix}\n1  0.2  0.1  0  0 \\\\\n0.2  1  0.3  0.1  0 \\\\\n0.1  0.3  1  0.25  0.05 \\\\\n0  0.1  0.25  1  0.2 \\\\\n0  0  0.05  0.2  1\n\\end{bmatrix}\n$$\n\n- LD 参考面板 B (矩阵 $\\mathbf{R}_{B}$):\n$$\n\\begin{bmatrix}\n1  0.15  0.05  0  0 \\\\\n0.15  1  0.25  0.05  0 \\\\\n0.05  0.25  1  0.2  0.1 \\\\\n0  0.05  0.2  1  0.25 \\\\\n0  0  0.1  0.25  1\n\\end{bmatrix}\n$$\n\n- 标准化效应的先验方差（两种性状相同）: $W = 0.2$。\n\n- 先验集 A: $p_{1} = 1 \\times 10^{-4}$， $p_{2} = 1 \\times 10^{-4}$， $p_{12} = 1 \\times 10^{-5}$。\n\n- 先验集 B: $p_{1} = 1 \\times 10^{-4}$， $p_{2} = 1 \\times 10^{-4}$， $p_{12} = 5 \\times 10^{-5}$。\n\n- 测试位点和观测到的边际 $z$ 分数（性状 1 为 GWAS，性状 2 为 eQTL）。每个向量按 SNP 索引 $j \\in \\{1,2,3,4,5\\}$ 排序。\n    - 位点 1 (强共享信号):\n        - $\\mathbf{z}^{(1)}_{\\mathrm{GWAS}} = [\\,0.6,\\,1.8,\\,6.0,\\,1.5,\\,0.3\\,]$\n        - $\\mathbf{z}^{(1)}_{\\mathrm{eQTL}} = [\\,0.5,\\,1.5,\\,5.0,\\,1.25,\\,0.25\\,]$\n    - 位点 2 (跨性状的不同信号):\n        - $\\mathbf{z}^{(2)}_{\\mathrm{GWAS}} = [\\,1.0,\\,5.0,\\,1.5,\\,0.5,\\,0.0\\,]$\n        - $\\mathbf{z}^{(2)}_{\\mathrm{eQTL}} = [\\,0.0,\\,0.5,\\,1.25,\\,5.0,\\,1.0\\,]$\n    - 位点 3 (有 GWAS 信号但无 eQTL 支持):\n        - $\\mathbf{z}^{(3)}_{\\mathrm{GWAS}} = [\\,0.6,\\,1.8,\\,6.0,\\,1.5,\\,0.3\\,]$\n        - $\\mathbf{z}^{(3)}_{\\mathrm{eQTL}} = [\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$\n\n计算要求：\n- 对每个位点、每个 LD 参考面板（$\\mathbf{R}_{A}$ 和 $\\mathbf{R}_{B}$）以及每个先验集（A 和 B），通过以下步骤计算 $\\mathrm{PP}_{4}$：\n    $1)$ 使用所选的 $\\mathbf{R}$ 对两种性状的边际 $z$ 分数进行解卷积，以获得每个性状的 $\\widehat{\\mathbf{z}}_{\\mathrm{causal}}$，\n    $2)$ 使用给定的 $W$ 计算单位点近似贝叶斯因子，\n    $3)$ 使用指定的单位点先验形成未归一化的权重 $w_{0},\\dots,w_{4}$，以及\n    $4)$ 进行归一化以获得 $\\mathrm{PP}_{4}$。\n\n测试套件和输出规范：\n- 按以下固定顺序评估 12 个计算：\n    $1)$ 位点 1, $\\mathbf{R}_{A}$, 先验 A;\n    $2)$ 位点 1, $\\mathbf{R}_{A}$, 先验 B;\n    $3)$ 位点 1, $\\mathbf{R}_{B}$, 先验 A;\n    $4)$ 位点 1, $\\mathbf{R}_{B}$, 先验 B;\n    $5)$ 位点 2, $\\mathbf{R}_{A}$, 先验 A;\n    $6)$ 位点 2, $\\mathbf{R}_{A}$, 先验 B;\n    $7)$ 位点 2, $\\mathbf{R}_{B}$, 先验 A;\n    $8)$ 位点 2, $\\mathbf{R}_{B}$, 先验 B;\n    $9)$ 位点 3, $\\mathbf{R}_{A}$, 先验 A;\n    $10)$ 位点 3, $\\mathbf{R}_{A}$, 先验 B;\n    $11)$ 位点 3, $\\mathbf{R}_{B}$, 先验 A;\n    $12)$ 位点 3, $\\mathbf{R}_{B}$, 先验 B.\n- 程序必须生成单行输出，其中包含 12 个 $\\mathrm{PP}_{4}$ 值，形式为方括号括起来的逗号分隔列表，例如 $[x_{1},x_{2},\\dots,x_{12}]$，其中每个 $x_{i}$ 是一个十进制形式的浮点数。不允许使用百分比；所有量都必须是 $[0,1]$ 范围内的十进制数。\n\n您的实现必须是一个完整的、可运行的程序，该程序不读取任何输入，并严格按照指定的顺序和格式写入一行结果。",
            "solution": "用户提供了一个问题，要求实现一个统计共定位模型，以计算全基因组关联研究 (GWAS) 信号和表达数量性状位点 (eQTL) 信号共享一个因果变异 ($H_4$) 的后验概率。\n\n### 步骤 1：问题验证\n\n首先对问题的有效性进行严格评估。\n\n#### 提取已知条件\n从问题陈述中逐字提取已知条件：\n-   **模型参数**：SNP 数量 $M=5$。标准化效应大小的先验方差 $W=0.2$。\n-   **LD 矩阵**：$\\mathbf{R}_A$ 和 $\\mathbf{R}_B$，均为 $5 \\times 5$ 矩阵，表示连锁不平衡相关结构。\n-   **先验集**：\n    -   先验 A: $p_1 = 1 \\times 10^{-4}$, $p_2 = 1 \\times 10^{-4}$, $p_{12} = 1 \\times 10^{-5}$。\n    -   先验 B: $p_1 = 1 \\times 10^{-4}$, $p_2 = 1 \\times 10^{-4}$, $p_{12} = 5 \\times 10^{-5}$。\n-   **观测数据（边际 z 分数）**：\n    -   位点 1: $\\mathbf{z}^{(1)}_{\\mathrm{GWAS}} = [0.6, 1.8, 6.0, 1.5, 0.3]$, $\\mathbf{z}^{(1)}_{\\mathrm{eQTL}} = [0.5, 1.5, 5.0, 1.25, 0.25]$。\n    -   位点 2: $\\mathbf{z}^{(2)}_{\\mathrm{GWAS}} = [1.0, 5.0, 1.5, 0.5, 0.0]$, $\\mathbf{z}^{(2)}_{\\mathrm{eQTL}} = [0.0, 0.5, 1.25, 5.0, 1.0]$。\n    -   位点 3: $\\mathbf{z}^{(3)}_{\\mathrm{GWAS}} = [0.6, 1.8, 6.0, 1.5, 0.3]$, $\\mathbf{z}^{(3)}_{\\mathrm{eQTL}} = [0.0, 0.0, 0.0, 0.0, 0.0]$。\n-   **核心方程**：\n    1.  因果 z 分数估计：$\\widehat{\\mathbf{z}}_{\\mathrm{causal}} = \\mathbf{R}^{-1}\\,\\mathbf{z}_{\\mathrm{marg}}$。\n    2.  近似贝叶斯因子：$\\mathrm{ABF}(z_j; W) = \\sqrt{\\frac{1}{1+W}}\\,\\exp(\\frac{1}{2}\\,\\frac{W}{1+W}\\,z_j^2)$，其中 $z_j = \\widehat{z}_{\\mathrm{causal},j}$。\n    3.  未归一化的后验权重：$w_0, w_1, w_2, w_3, w_4$ 的表达式。\n    4.  $H_4$ 的后验概率：$\\mathrm{PP}_4 = w_4 / \\sum_{i=0}^4 w_i$。\n-   **计算任务**：为 12 个指定的位点、LD 面板和先验集组合计算 $\\mathrm{PP}_4$。\n\n#### 验证结论\n-   **有科学依据**：该问题描述了 `coloc` 统计框架的一个简化版本，这是一种在统计遗传学中用于整合 GWAS 和 eQTL 数据的标准且被广泛接受的方法。其基本原理——贝叶斯定理、回归系数的多元正态理论以及近似贝叶斯因子 (ABF) 的使用——都是该领域的标准方法。\n-   **适定性**：所有必要的数据、方程和常数都已提供。LD 矩阵 $\\mathbf{R}_A$ 和 $\\mathbf{R}_B$ 是对称正定的（它们的行列式分别约为 $0.741$ 和 $0.823$），这确保了它们是可逆的，并且 $\\widehat{\\mathbf{z}}_{\\mathrm{causal}}$ 的唯一解存在。指令清晰明确，可为每个指定案例带来唯一的数值结果。\n-   **客观性**：该问题以精确的数学语言陈述，没有主观性或歧义。\n\n该问题被认为是**有效的**，因为它是自洽的、科学上合理的并且是适定的。\n\n### 步骤 2：求解推导与实现\n解决方案涉及对 12 个测试案例中的每一个系统地应用所提供的公式。每个案例的算法流程如下。\n\n1.  **选择输入**：对于每个案例，选择特定的边际 z 分数向量 ($\\mathbf{z}_{\\mathrm{GWAS}}, \\mathbf{z}_{\\mathrm{eQTL}}$)、LD 矩阵 ($\\mathbf{R}$) 和先验概率 ($p_1, p_2, p_{12}$)。\n\n2.  **解卷积 z 分数以估计因果效应**：LD 结构掩盖了真实的因果信号。为了解决这个问题，通过对 LD 矩阵求逆来“解卷积”边际 z 分数。这是一种统计精细定位的形式。\n    $$ \\widehat{\\mathbf{z}}_{\\mathrm{causal},1} = \\mathbf{R}^{-1}\\mathbf{z}_{\\mathrm{GWAS}} $$\n    $$ \\widehat{\\mathbf{z}}_{\\mathrm{causal},2} = \\mathbf{R}^{-1}\\mathbf{z}_{\\mathrm{eQTL}} $$\n    此处，$\\widehat{\\mathbf{z}}_{\\mathrm{causal},1}$ 和 $\\widehat{\\mathbf{z}}_{\\mathrm{causal},2}$ 分别是性状 1 (GWAS) 和性状 2 (eQTL) 的估计因果 z 分数向量。\n\n3.  **计算近似贝叶斯因子 (ABF)**：对于每个 SNP $j \\in \\{1, \\dots, M\\}$ 和每个性状，计算一个 ABF。ABF 量化了该 SNP 处存在非零效应的证据。计算使用估计的因果 z 分数向量的分量。\n    令 $c_1 = \\sqrt{\\frac{1}{1+W}}$ 和 $c_2 = \\frac{1}{2}\\frac{W}{1+W}$。每个性状的 ABF 按元素计算：\n    $$ \\mathrm{ABF}_{1,j} = c_1 \\exp(c_2 (\\widehat{z}_{\\mathrm{causal},1,j})^2) $$\n    $$ \\mathrm{ABF}_{2,j} = c_1 \\exp(c_2 (\\widehat{z}_{\\mathrm{causal},2,j})^2) $$\n\n4.  **计算汇总统计量和未归一化后验权重**：将单位点 ABF 进行汇总，以形成假设权重所需的总和。\n    $$ S_1 = \\sum_{j=1}^{M} \\mathrm{ABF}_{1,j} $$\n    $$ S_2 = \\sum_{j=1}^{M} \\mathrm{ABF}_{2,j} $$\n    $$ S_{12} = \\sum_{j=1}^{M} \\mathrm{ABF}_{1,j} \\mathrm{ABF}_{2,j} $$\n    然后使用这些总和来计算五个假设中每一个的未归一化后验权重，如问题陈述中定义：\n    $$ w_0 = 1 $$\n    $$ w_1 = p_1 S_1 $$\n    $$ w_2 = p_2 S_2 $$\n    $$ w_3 = p_1 p_2 (S_1 S_2 - S_{12}) $$\n    $$ w_4 = p_{12} S_{12} $$\n\n5.  **计算 $H_4$ 的后验概率**：将权重通过它们的总和进行归一化，以获得后验概率。所需量 $\\mathrm{PP}_4$ 是假设 $H_4$ 的归一化权重。\n    $$ \\mathrm{PP}_4 = \\frac{w_4}{w_0 + w_1 + w_2 + w_3 + w_4} $$\n\n该过程被实现并针对 12 个指定的测试案例进行循环，并收集最终的 $\\mathrm{PP}_4$ 值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified statistical colocalization model and computes the posterior probability of a shared causal variant (PP4) for a suite of 12 test cases.\n    \"\"\"\n    \n    # ------------------ Define Givens from Problem Statement ------------------\n    \n    # Number of SNPs\n    M = 5\n    \n    # Prior variance for standardized effects\n    W = 0.2\n    \n    # LD reference panels\n    R_A = np.array([\n        [1.0, 0.2, 0.1, 0.0, 0.0],\n        [0.2, 1.0, 0.3, 0.1, 0.0],\n        [0.1, 0.3, 1.0, 0.25, 0.05],\n        [0.0, 0.1, 0.25, 1.0, 0.2],\n        [0.0, 0.0, 0.05, 0.2, 1.0]\n    ])\n    \n    R_B = np.array([\n        [1.0, 0.15, 0.05, 0.0, 0.0],\n        [0.15, 1.0, 0.25, 0.05, 0.0],\n        [0.05, 0.25, 1.0, 0.2, 0.1],\n        [0.0, 0.05, 0.2, 1.0, 0.25],\n        [0.0, 0.0, 0.1, 0.25, 1.0]\n    ])\n    \n    # Prior sets\n    priors_A = {'p1': 1e-4, 'p2': 1e-4, 'p12': 1e-5}\n    priors_B = {'p1': 1e-4, 'p2': 1e-4, 'p12': 5e-5}\n    \n    # Test loci and observed marginal z-scores\n    locus1_z_gwas = np.array([0.6, 1.8, 6.0, 1.5, 0.3])\n    locus1_z_eqtl = np.array([0.5, 1.5, 5.0, 1.25, 0.25])\n    \n    locus2_z_gwas = np.array([1.0, 5.0, 1.5, 0.5, 0.0])\n    locus2_z_eqtl = np.array([0.0, 0.5, 1.25, 5.0, 1.0])\n    \n    locus3_z_gwas = np.array([0.6, 1.8, 6.0, 1.5, 0.3])\n    locus3_z_eqtl = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n\n    # ------------------ Setup Test Suite ------------------\n    \n    test_cases = [\n        # Locus 1\n        (locus1_z_gwas, locus1_z_eqtl, R_A, priors_A),\n        (locus1_z_gwas, locus1_z_eqtl, R_A, priors_B),\n        (locus1_z_gwas, locus1_z_eqtl, R_B, priors_A),\n        (locus1_z_gwas, locus1_z_eqtl, R_B, priors_B),\n        # Locus 2\n        (locus2_z_gwas, locus2_z_eqtl, R_A, priors_A),\n        (locus2_z_gwas, locus2_z_eqtl, R_A, priors_B),\n        (locus2_z_gwas, locus2_z_eqtl, R_B, priors_A),\n        (locus2_z_gwas, locus2_z_eqtl, R_B, priors_B),\n        # Locus 3\n        (locus3_z_gwas, locus3_z_eqtl, R_A, priors_A),\n        (locus3_z_gwas, locus3_z_eqtl, R_A, priors_B),\n        (locus3_z_gwas, locus3_z_eqtl, R_B, priors_A),\n        (locus3_z_gwas, locus3_z_eqtl, R_B, priors_B),\n    ]\n\n    results = []\n    \n    # Pre-calculate ABF constants\n    c1 = np.sqrt(1 / (1 + W))\n    c2 = 0.5 * W / (1 + W)\n    \n    # Pre-calculate matrix inverses\n    inv_R_A = np.linalg.inv(R_A)\n    inv_R_B = np.linalg.inv(R_B)\n    \n    # Map LD matrices to their pre-computed inverses\n    inv_R_map = {id(R_A): inv_R_A, id(R_B): inv_R_B}\n\n    for z_gwas, z_eqtl, R, priors in test_cases:\n        \n        # Step 1: Deconvolve z-scores\n        inv_R = inv_R_map[id(R)]\n        z_causal_gwas = inv_R @ z_gwas\n        z_causal_eqtl = inv_R @ z_eqtl\n        \n        # Step 2: Compute Approximate Bayes Factors (ABF)\n        abf1 = c1 * np.exp(c2 * z_causal_gwas**2)\n        abf2 = c1 * np.exp(c2 * z_causal_eqtl**2)\n        \n        # Step 3: Compute aggregate statistics and unnormalized posterior weights\n        S1 = np.sum(abf1)\n        S2 = np.sum(abf2)\n        S12 = np.sum(abf1 * abf2)\n        \n        p1 = priors['p1']\n        p2 = priors['p2']\n        p12 = priors['p12']\n        \n        w0 = 1.0\n        w1 = p1 * S1\n        w2 = p2 * S2\n        w3 = p1 * p2 * (S1 * S2 - S12)\n        w4 = p12 * S12\n        \n        # Step 4: Calculate the Posterior Probability for H4\n        total_w = w0 + w1 + w2 + w3 + w4\n        pp4 = w4 / total_w if total_w > 0 else 0.0\n        \n        results.append(pp4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}