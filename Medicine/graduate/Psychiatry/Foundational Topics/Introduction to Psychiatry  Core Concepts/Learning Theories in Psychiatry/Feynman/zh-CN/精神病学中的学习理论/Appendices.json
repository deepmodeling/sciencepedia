{
    "hands_on_practices": [
        {
            "introduction": "要掌握精神病学中的学习理论，理解 Rescorla-Wagner 模型是第一步。该模型用一个简洁的数学公式描述了生物体如何根据“意外”或预测误差来学习线索与结果之间的关联。本练习将带你亲手计算在一个复合线索试验中，不同线索的联结强度 $V$ 是如何变化的，这是理解更复杂学习现象的基础。",
            "id": "4721764",
            "problem": "一位治疗师在对惊恐障碍实施暴露疗法时，使用一个正式的联结学习框架来追踪患者在面对先前已获得对内感受性痛苦的预测价值的复合线索时，其条件反应如何变化。考虑一个单一的强化复合试验，其中两个线索，$A$（一种内感受）和 $B$（一种外感受性情境），被一同呈现，并伴随着被视为非条件刺激的惊恐反应。在该试验之前，线索的联结强度为 $V_{A}=0.4$ 和 $V_{B}=0.2$。治疗师为线索指定了特定的显著性参数 $\\alpha_{A}=0.3$ 和 $\\alpha_{B}=0.1$，并对非条件刺激使用共同的学习率参数 $\\beta=0.5$。该试验中非条件刺激的强化量为 $\\lambda=1$。使用预测误差原理和 Rescorla–Wagner 联结学习框架，从第一性原理推导出更新规则，并计算此次复合试验中联结强度的变化量 $\\Delta V_{A}$ 和 $\\Delta V_{B}$。将你的最终答案 $(\\Delta V_{A}, \\Delta V_{B})$ 表示为一个行矩阵。无需四舍五入。",
            "solution": "Rescorla-Wagner 模型假定，在一次学习试验中，一个刺激的联结强度变化量（$\\Delta V$）取决于实际结果与预期结果之间的差异。这种差异被称为预测误差。某个特定试验中出现的单个线索 $i$ 的联结强度更新规则，便是从这一原理推导出来的。\n\n联结强度的变化量 $\\Delta V_i$ 与线索 $i$ 的显著性（用 $\\alpha_i$ 表示）以及与非条件刺激（US）相关的学习率参数（用 $\\beta$ 表示）成正比。关键在于，这一变化是由预测误差项 $(\\lambda - V_{total})$ 驱动的，其中 $\\lambda$ 是非条件刺激所能支持的联结强度的渐近水平，而 $V_{total}$ 是该试验中出现的所有线索的联结强度之和。\n\nRescorla-Wagner 模型对线索 $i$ 的通用更新规则形式如下：\n$$\n\\Delta V_i = \\alpha_i \\beta (\\lambda - V_{total})\n$$\n\n在本问题中，我们考虑的是一个复合试验，其中两个线索 $A$ 和 $B$ 被同时呈现。因此，该试验中的总联结强度 $V_{total}$ 是出现线索的各自强度之和：\n$$\nV_{total} = V_A + V_B\n$$\n这个总和代表了对非条件刺激出现的总预期。强化由 $\\lambda$ 给出，即惊恐反应的量级。\n\n将 $V_{total}$ 的表达式代入通用更新规则，我们得到线索 $A$ 的联结强度变化量（$\\Delta V_A$）和线索 $B$ 的联结强度变化量（$\\Delta V_B$）的具体方程：\n$$\n\\Delta V_A = \\alpha_A \\beta (\\lambda - (V_A + V_B))\n$$\n$$\n\\Delta V_B = \\alpha_B \\beta (\\lambda - (V_A + V_B))\n$$\n注意，预测误差项 $(\\lambda - (V_A + V_B))$ 对两个线索是相同的，因为它们是同一个复合刺激事件的一部分。它们强度上的差异性变化源于它们不同的显著性值 $\\alpha_A$ 和 $\\alpha_B$。\n\n问题提供了以下数值：\n- 线索 $A$ 的初始联结强度：$V_A = 0.4$\n- 线索 $B$ 的初始联结强度：$V_B = 0.2$\n- 线索 $A$ 的显著性：$\\alpha_A = 0.3$\n- 线索 $B$ 的显著性：$\\alpha_B = 0.1$\n- 非条件刺激的学习率：$\\beta = 0.5$\n- 强化量：$\\lambda = 1$\n\n首先，我们计算强化试验前复合线索 $AB$ 的总联结强度：\n$$\nV_{total} = V_A + V_B = 0.4 + 0.2 = 0.6\n$$\n接下来，我们计算预测误差：\n$$\n\\text{预测误差} = \\lambda - V_{total} = 1 - 0.6 = 0.4\n$$\n正的预测误差表明结果比预期的要强，这导致了出现线索的联结强度增加（兴奋性条件反射）。\n\n现在，我们可以计算每个线索的联结强度变化量。\n对于线索 $A$：\n$$\n\\Delta V_A = \\alpha_A \\beta (\\lambda - V_{total}) = (0.3)(0.5)(0.4)\n$$\n$$\n\\Delta V_A = (0.15)(0.4) = 0.06\n$$\n对于线索 $B$：\n$$\n\\Delta V_B = \\alpha_B \\beta (\\lambda - V_{total}) = (0.1)(0.5)(0.4)\n$$\n$$\n\\Delta V_B = (0.05)(0.4) = 0.02\n$$\n因此，该复合试验中联结强度的变化量为 $\\Delta V_A = 0.06$ 和 $\\Delta V_B = 0.02$。结果以行矩阵 $(\\Delta V_A, \\Delta V_B)$ 的形式呈现。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.06  0.02\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Rescorla-Wagner 模型的强大之处在于它能预测一些看似不直观但临床意义重大的现象，例如“阻断效应”。本练习将该模型应用于一个创伤后应激障碍（PTSD）的临床场景，帮助你理解为何患者有时难以学习到新的创伤相关线索，并思考如何通过设计干预措施来克服这一学习障碍。通过这个实践，你将看到理论模型如何直接指导治疗策略的设计 。",
            "id": "4721800",
            "problem": "一名患有创伤后应激障碍（PTSD）的患者报告称，在接触到城市紧急事件的提示物时会感到极度痛苦。确定了三种与创伤相关的条件刺激（CS）：警报声（$A$）、柴油废气的气味（$B$）和闪烁的蓝光（$C$）。非条件刺激（US）是治疗师引导下对指征性创伤中最糟糕时刻的想象性重现。治疗师采用控制性暴露治疗，系统地改变想象性非条件刺激的强度和条件刺激组合的构成，以研究和利用联结学习机制。\n\n在治疗前的历史学习中，警报声（$A$）与创伤记忆之间已形成主导性联结，使得在归一化标度上，$A$ 的联结强度已接近渐近线，当想象性非条件刺激强度为 $\\lambda = 1.0$ 时，有 $V_A \\approx 1.0$。在基线水平，其他线索的联结强度可忽略不计，$V_B \\approx 0$ 且 $V_C \\approx 0$。各条件刺激的凸显性参数在不同治疗阶段保持稳定，其中 $A$ 的凸显性中等（$\\alpha_A = 0.4$），$B$ 的凸显性较低（$\\alpha_B = 0.3$），$C$ 的凸显性最低（$\\alpha_C = 0.2$）。非条件刺激的加工参数由治疗节奏设定，并在各阶段保持恒定（$\\beta_{\\mathrm{US}} = 0.5$）。\n\n治疗师计划在四个阶段中进行以下操作：\n\n- 阶段 $1$：进行十次复合刺激阶段，同时呈现 $A$ 和 $B$（$AB$），同时诱发与基线强度相同的想象性非条件刺激，$\\lambda = 1.0$。\n- 阶段 $2$：进行十次复合刺激阶段，同时呈现 $A$ 和 $C$（$AC$），并有意将想象性非条件刺激的强度增加到 $\\lambda' = 1.4$ 以引发意外（surprise）。\n- 阶段 $3$：对 $A$ 进行消退处理，即在没有非条件刺激的情况下重复呈现 $A$，直到 $V_A$ 降低至约 $0.6$。\n- 阶段 $4$：进行数次治疗，单独呈现 $B$ 和非条件刺激（不存在 $A$ 或 $C$），此时 $\\lambda = 1.0$。\n\n利用适用于精神病学中经典条件反射的预测误差驱动的联结学习原理，判断在阶段 $1$ 中哪个线索会表现出阻断效应，并指出在所描述的各阶段中，哪一项单一操作最能直接解除对 $B$ 的学习阻断（即，由于预测误差增加而导致 $V_B$ 产生正向变化）。选择正确陈述阶段 $1$ 的阻断结果以及对 $B$ 最有效的解除阻断设计的选项。\n\nA. 在阶段 $1$ 中，$B$ 将被 $A$ 阻断，因为当 $V_A \\approx 1.0$ 时，复合刺激 $AB$ 在 $\\lambda = 1.0$ 的情况下不会增加意外感。在阶段 $2$ 中，$C$ 将获得联结强度，因为尽管 $A$ 占主导地位，将非条件刺激增加到 $\\lambda' = 1.4$ 仍会产生预测误差。对 $B$ 最直接的解除阻断方法是，首先通过消退（阶段 $3$）降低 $V_A$，然后单独呈现 $B$ 和非条件刺激（阶段 $4$），这样在 $B$–US 试验中 $\\sum V$ 会很低，从而使 $V_B$ 增加。\n\nB. 在阶段 $1$ 中，$B$ 不会被阻断，因为即使 $A$ 占主导地位，$\\alpha_B = 0.3$ 的凸显性也足以进行学习。在 $\\lambda = 1.0$ 的条件下持续进行 $AB$ 阶段会逐渐解除对 $B$ 的阻断，无需额外操作，并且在阶段 $2$ 中增加非条件刺激是不必要的。\n\nC. 在阶段 $1$ 中，无论非条件刺激强度如何变化，只要 $A$ 存在，$B$ 和 $C$ 都会被阻断。为了解除对 $B$ 的阻断，应在 $ABD$ 复合刺激中引入一个新的线索 $D$，因为向复合刺激中添加新颖性比移除 $A$ 或改变 $\\lambda$ 更能增加关于 $B$ 的学习。\n\nD. 在阶段 $1$ 中，$B$ 将被阻断，而在阶段 $2$ 中，$C$ 也将被阻断，因为 $A$ 的联结强度仍然很高。对 $B$ 的最佳解除阻断方法是将想象性非条件刺激的强度降低到基线以下（例如，降至 $\\lambda = 0.6$），这样 $B$ 可以在没有压倒性恐惧反应的情况下学习；对 $A$ 进行消退是不必要的。",
            "solution": "该问题遵循预测误差驱动的联结学习原理，其形式化模型为 Rescorla-Wagner 模型。在任何给定的学习试验中，条件刺激（CS）$i$ 的联结强度变化量 $\\Delta V_i$ 由以下方程决定：\n$$ \\Delta V_i = \\alpha_i \\beta (\\lambda - V_{total}) $$\n其中 $\\alpha_i$ 是条件刺激 $i$ 的凸显性，$\\beta$ 是与非条件刺激（US）相关的速率参数，$\\lambda$ 是非条件刺激的大小，而 $V_{total}$ 是该试验中出现的所有条件刺激的联结强度之和，即 $\\sum V_j$。项 $(\\lambda - V_{total})$ 代表预测误差。\n\n问题提供了以下初始条件和参数：\n- 初始联结强度：$V_A \\approx 1.0$, $V_B \\approx 0$, $V_C \\approx 0$。\n- 基线非条件刺激大小：$\\lambda = 1.0$。\n- 凸显性参数：$\\alpha_A = 0.4$, $\\alpha_B = 0.3$, $\\alpha_C = 0.2$。\n- 非条件刺激加工参数：$\\beta_{\\mathrm{US}} = 0.5$。\n\n**阶段 $1$ 分析：阻断效应**\n在阶段 $1$ 中，复合刺激 $AB$ 与强度为 $\\lambda = 1.0$ 的非条件刺激一同呈现。在此阶段开始时，复合刺激的总联结强度为 $V_{total} = V_A + V_B \\approx 1.0 + 0 = 1.0$。\n因此，预测误差为：\n$$ \\lambda - V_{total} \\approx 1.0 - 1.0 = 0 $$\n在此阶段的第一次试验中，线索 $B$ 的联结强度变化为：\n$$ \\Delta V_B = \\alpha_B \\beta (\\lambda - (V_A + V_B)) \\approx (0.3)(0.5)(1.0 - (1.0 + 0)) = 0 $$\n由于预测误差约等于 $0$，线索 $B$ 不会发生新的学习。线索 $A$ 预先存在的联结强度完全预测了非条件刺激，从而“阻断”了线索 $B$ 获得联结强度。这证实了在阶段 $1$ 中，$B$ 将被 $A$ 阻断。\n\n**解除阻断机制分析**\n解除阻断的核心原理是在本会发生阻断的情况下制造一个非零的预测误差。该问题描述了两种实现此目的的不同方法。\n\n1.  **增加非条件刺激强度（阶段 $2$）：** 在阶段 $2$ 中，复合刺激 $AC$ 与强度增加的非条件刺激配对，$\\lambda' = 1.4$。初始总联结强度为 $V_{total} = V_A + V_C \\approx 1.0 + 0 = 1.0$。预测误差变为：\n    $$ \\lambda' - V_{total} \\approx 1.4 - 1.0 = 0.4 $$\n    这个正向的预测误差（“意外”）驱动了所有在场线索的新学习。对于线索 $C$，其强度变化为：\n    $$ \\Delta V_C = \\alpha_C \\beta (\\lambda' - (V_A + V_C)) \\approx (0.2)(0.5)(0.4) = 0.04 $$\n    因此，$V_C$ 将会增加。这展示了通过增加非条件刺激强度来解除阻断。\n\n2.  **降低阻断物强度（阶段 $3$ 和 $4$）：** 此策略旨在促成关于线索 $B$ 的学习。\n    - **阶段 $3$（对 $A$ 进行消退）：** 单独呈现线索 $A$，不伴随非条件刺激（$\\lambda = 0$）。预测误差为 $\\lambda - V_A = 0 - V_A$。这个负向预测误差导致 $V_A$ 下降。该阶段持续进行，直到 $V_A$ 降至约 $0.6$。\n    - **阶段 $4$（$B$ 的学习获得）：** 随后，在线索 $B$ 单独呈现的同时，给予原始强度的非条件刺激，$\\lambda = 1.0$。在此阶段开始时，$V_B \\approx 0$。总联结强度即为 $V_B$。预测误差为：\n      $$ \\lambda - V_B \\approx 1.0 - 0 = 1.0 $$\n      $B$ 的联结强度变化为：\n      $$ \\Delta V_B = \\alpha_B \\beta (\\lambda - V_B) \\approx (0.3)(0.5)(1.0 - 0) = 0.15 $$\n      这个大的正向变化表明，线索 $B$ 现在将很容易获得联结强度。这个过程，包括对阻断物进行消退，然后用目标线索进行条件反射试验，是克服阻断效应的有效方法，它通过确保目标线索与非条件刺激配对时有大的预测误差，从而让之前被阻断的线索获得强度。\n\n问题要求确定阶段 $1$ 中的阻断效应，以及最直接解除对 $B$ 的学习阻断的操作。根据我们的分析，在阶段 $1$ 中 $B$ 被阻断。问题中描述的解除对 $B$ 的学习阻断的操作是阶段 $3$ 和阶段 $4$ 的序列。\n\n**选项评估：**\n\n-   **A.** 该选项指出，在阶段 $1$ 中，$B$ 被 $A$ 阻断，因为复合刺激 $AB$ 没有产生意外。这是正确的。它正确地解释了在阶段 $2$ 中，将 $\\lambda$ 增加到 $1.4$ 解除了对 $C$ 的学习阻断。最后，它准确地描述了阶段 $3$ 和阶段 $4$ 程序的逻辑，作为解除对 $B$ 阻断的直接方法：通过降低 $V_A$，然后将 $B$ 与非条件刺激配对，确保在 $B$-US 试验中有高预测误差。该选项与我们的推导完全一致。**正确**。\n\n-   **B.** 该选项错误地声称，由于其凸显性 $\\alpha_B = 0.3$，$B$ 不会被阻断。Rescorla-Wagner 模型规定，学习是凸显性和预测误差的乘积。如果预测误差为零，无论凸显性如何，学习都为零。**错误**。\n\n-   **C.** 该选项犯了几个错误。它错误地指出 $C$ 在阶段 $1$ 被阻断（实际上 $C$ 并未出现）。它错误地声称非条件刺激强度的变化不影响阻断。阶段 $2$ 就是一个直接的反例。在标准的 Rescorla-Wagner 模型中，添加一个新线索 $D$ 的建议不是一个有效的解除阻断方法，因为添加一个 $V_D=0$ 的线索并不会改变预测误差。**错误**。\n\n-   **D.** 该选项正确地指出 $B$ 在阶段 $1$ 被阻断，但错误地声称 $C$ 在阶段 $2$ 也被阻断。如分析所示，$C$ 在阶段 $2$ 被解除了阻断。此外，它提议降低非条件刺激强度（例如，降至 $\\lambda=0.6$）来解除对 $B$ 的阻断。这将产生一个负的预测误差 $(\\lambda - (V_A+V_B)) \\approx 0.6 - 1.0 = -0.4$，这会导致抑制性条件作用（$V_B$ 的减少），而不是兴奋性条件作用。**错误**。\n\n因此，选项 A 提供了根据指定学习模型对这些现象的唯一准确和完整的描述。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "除了经典的联结学习，现代计算精神病学广泛使用强化学习模型来理解决策和动机。时间差分（TD）学习是其中的核心框架，它模拟了生物体如何根据即时奖励和对未来的预期来逐步更新其对不同状态“价值”的评估。本练习将引导你计算一个关键变量——“奖励预测误差” $\\delta_t$，这个概念不仅是学习的核心驱动力，还被认为与大脑中多巴胺的活动密切相关，从而搭建起从行为到神经生物学的桥梁。",
            "id": "4721727",
            "problem": "一名患有重度抑郁症的患者参与了一项基于强化的决策任务，该任务被建模为马尔可夫决策过程 (MDP)。临床医生使用时间差分 (TD) 学习来量化患者在折扣累积回报和贝尔曼一致性的假设下，如何在每次试验中更新状态值。在试验 $t$ 中，患者处于状态 $s_t$，当前的状态值估计为 $V(s_t) = 0.5$。在选择一个动作后，患者收到即时结果 $r_t = 1$，并转移到状态 $s_{t+1}$，其状态值估计为 $V(s_{t+1}) = 0.6$。学习率为 $\\alpha = 0.2$，折扣因子为 $\\gamma = 0.9$。利用期望折扣回报的基本定义和贝尔曼一致性原则，推导观测回报与当前估计之间的基于样本的差异（奖励预测误差），并计算对 $V(s_t)$ 的单步 TD 更新。报告奖励预测误差的数值和 $V(s_t)$ 的更新值估计。此外，从患者学习（趋近与回避）和精神病理学模型中合理的神经调质信号传导方面，解释奖励预测误差符号的临床意义。将最终数值答案表示为一个行矩阵，其中包含奖励预测误差，后跟更新后的值估计。无需四舍五入。",
            "solution": "该问题要求应用单步时间差分 (TD) 学习算法，通常称为 TD(0)。该算法根据单次转移的经验来更新状态 $V(s_t)$ 的值估计，该经验由当前状态 $s_t$、收到的奖励 $r_t$ 和下一个状态 $s_{t+1}$ 组成。\n\n其基本原理是贝尔曼一致性，即一个状态的价值应该等于期望的即时奖励加上期望的下一个状态的折扣价值。TD 算法使用此期望的一个样本来更新其当前估计。基于样本的 $s_t$ 真实价值的估计被称为“TD 目标”。对于单步前瞻，TD 目标定义为：\n$$\n\\text{TD Target} = r_t + \\gamma V(s_{t+1})\n$$\n这个表达式代表了从这单步经验中观察到的总折扣回报。\n\n这个新的、基于经验的估计与旧的价值估计之间的差异就是奖励预测误差 (RPE)，用 $\\delta_t$ 表示。RPE 量化了结果的“意外”程度。其形式化定义为：\n$$\n\\delta_t = (\\text{TD Target}) - V(s_t) = [r_t + \\gamma V(s_{t+1})] - V(s_t)\n$$\n使用问题陈述中提供的值 ($r_t = 1$, $\\gamma = 0.9$, $V(s_{t+1}) = 0.6$, $V(s_t) = 0.5$)，我们可以计算 RPE 的数值：\n$$\n\\delta_t = [1 + (0.9)(0.6)] - 0.5\n$$\n$$\n\\delta_t = [1 + 0.54] - 0.5\n$$\n$$\n\\delta_t = 1.54 - 0.5\n$$\n$$\n\\delta_t = 1.04\n$$\n第一个要求的值，即奖励预测误差，是 $1.04$。\n\nTD 更新规则通过将原始值估计 $V(s_t)$ 部分地移向 TD 目标来修改它。此调整的大小由学习率 $\\alpha$ 控制。更新后的值，我们可以表示为 $V_{\\text{new}}(s_t)$，计算如下：\n$$\nV_{\\text{new}}(s_t) = V(s_t) + \\alpha \\delta_t\n$$\n代入已知值 ($\\alpha = 0.2$) 和计算出的 RPE：\n$$\nV_{\\text{new}}(s_t) = 0.5 + (0.2)(1.04)\n$$\n$$\nV_{\\text{new}}(s_t) = 0.5 + 0.208\n$$\n$$\nV_{\\text{new}}(s_t) = 0.708\n$$\n第二个要求的值，即 $V(s_t)$ 的更新值估计，是 $0.708$。\n\n**临床解释**\n\n奖励预测误差 ($\\delta_t$) 的符号对其解释至关重要。在这里，$\\delta_t = 1.04$，是正数。\n\n1.  **在患者学习中的意义**：正的 RPE 表明结果“好于预期”。由价值函数 $V$ 代表的患者的世界模型，低估了处于状态 $s_t$ 的价值。体验到的回报 (TD 目标为 1.54) 显著高于预期的回报 ($V(s_t)$ 为 0.5)。这个正向的意外信号用于强化导致此有利结果的行为（在状态 $s_t$ 中选择的动作）。它促进“趋近”行为，增加了患者将来在面对状态 $s_t$ 时做出类似选择的概率。状态 $s_t$ 的价值被向上更新（从 $0.5$ 到 $0.708$），反映了这一新的学习过程。\n\n2.  **可能的神经调质信号传导**：在计算精神病学中，RPE 被假设由中脑多巴胺能神经元的相位性放电编码。如此处计算出的正 RPE，对应于目标脑区（如纹状体和前额叶皮层）中多巴胺的相位性爆发式释放。这种多巴胺信号被认为是学习积极结果和激励未来目标导向行为的神经生物学基础。在重度抑郁症的背景下，一个关键的假说（快感缺乏）是该奖励系统功能减弱。在该患者的任务表现中观察到强健的正 RPE 可能表明，奖励学习回路的至少这一部分是有反应的，为评估治疗效果或疾病进展提供了一个量化指标。相反，在许多试验中，对于积极结果，持续出现低于预期的 RPE 可能成为快感缺乏的生物标志物。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.04  0.708 \\end{pmatrix}}\n$$"
        }
    ]
}