{
    "hands_on_practices": [
        {
            "introduction": "The old adage 'garbage in, garbage out' is especially true in multi-omics analysis. Before embarking on complex integration, it is crucial to ensure the quality of each dataset by identifying and handling outlier samples that could skew the results. This first practice  guides you through building an automated quality control pipeline, employing robust statistical methods like Principal Component Analysis (PCA) leverage scores and Median Absolute Deviation (MAD) to flag problematic samples across different omic layers.",
            "id": "5033985",
            "problem": "You are tasked with constructing a quality control pipeline for multi-omics integration in translational medicine that detects outlier samples across omics using Principal Component Analysis (PCA) leverage scores and Median Absolute Deviation (MAD) thresholds applied to key quality metrics. The pipeline must be implemented as a complete, runnable program that takes no input and produces a single line of output. The program must compute outlier indices for a provided test suite.\n\nFoundational base and definitions to use:\n- Let an omics data matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$ with $n$ samples (rows) and $p$ features (columns). Each column of $X$ must be standardized to zero mean and unit variance before subsequent computations.\n- Principal Component Analysis (PCA) can be defined via the Singular Value Decomposition (SVD), $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{p \\times r}$ have orthonormal columns, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is diagonal with nonnegative singular values, and $r = \\operatorname{rank}(X)$.\n- For a chosen number of components $k$, with $k \\leq p$, define the projection onto the top-$k$ left singular subspace by $U_k \\in \\mathbb{R}^{n \\times k}$, the first $k$ columns of $U$ if $r \\geq k$, and by $U_k \\in \\mathbb{R}^{n \\times r}$ otherwise. The PCA leverage for sample $i$ is the diagonal element of the projection matrix onto this subspace, $h_i = \\sum_{j=1}^{\\min(k, r)} U_{ij}^2$.\n- The projection matrix $H_k = U_k U_k^\\top$ is symmetric and idempotent, and satisfies $\\operatorname{trace}(H_k) = \\min(k, r)$. Therefore, the mean leverage equals $\\operatorname{trace}(H_k)/n = \\min(k, r)/n$. A leverage-based outlier can be defined by comparing $h_i$ to a threshold $\\alpha \\cdot \\min(k, r)/n$, with amplification factor $\\alpha > 1$.\n- For a quality metric vector $q \\in \\mathbb{R}^n$, define $\\operatorname{median}(q)$, the median absolute deviations $d_i = |q_i - \\operatorname{median}(q)|$, $\\operatorname{MAD}(q) = \\operatorname{median}(d)$, and the robust scaled deviation $z_i = d_i / (c \\cdot \\operatorname{MAD}(q))$ with $c = 1.4826$ (the consistency constant under normality). A metric-based outlier is any $i$ for which $z_i$ exceeds a threshold $t$.\n- Integrated across omics: given $O$ omics matrices $\\{X^{(o)}\\}_{o=1}^O$, define for each sample $i$ the count of leverage exceedances across omics, $C^{\\text{lev}}_i$, and the count of metric exceedances across the provided metrics, $C^{\\text{met}}_i$. A sample $i$ is flagged as an integrated outlier if either $C^{\\text{lev}}_i \\geq \\theta$ or $C^{\\text{met}}_i \\geq \\phi$, for given integer thresholds $\\theta$ and $\\phi$.\n\nImplementation requirements:\n- Standardize each omics matrix column-wise to zero mean and unit variance; if a column has zero variance, treat its standardized values as all zeros.\n- Compute PCA leverage using the SVD-based definition above with $U_k$ and $h_i$.\n- Compute quality metric outliers using the MAD-based robust deviations. If $\\operatorname{MAD}(q) = 0$, define no outliers for that metric unless any $q_i \\neq \\operatorname{median}(q)$; under that latter condition, treat those with $q_i \\neq \\operatorname{median}(q)$ as outliers.\n- Use $0$-based indexing for reporting outlier sample indices.\n\nTest suite:\n- Test Case $1$ (general multi-omics with a clear outlier in one omic and metrics):\n    - Number of samples: $n = 6$; number of omics: $O = 3$; components $k = 2$; leverage amplification $\\alpha = 2.0$; leverage count threshold $\\theta = 1$; metric threshold $t = 3.0$; metric count threshold $\\phi = 1$.\n    - Genomics matrix $X^{(1)}$ ($6 \\times 4$):\n      $$\n      \\begin{bmatrix}\n      0.5 & 1.0 & -0.3 & 0.2 \\\\\n      0.6 & 0.8 & -0.1 & 0.0 \\\\\n      0.4 & 1.1 & -0.2 & 0.1 \\\\\n      0.5 & 0.9 & -0.3 & 0.2 \\\\\n      0.6 & 1.0 & -0.2 & 0.3 \\\\\n      0.5 & 0.95 & -0.25 & 0.15\n      \\end{bmatrix}\n      $$\n    - Transcriptomics matrix $X^{(2)}$ ($6 \\times 5$):\n      $$\n      \\begin{bmatrix}\n      10 & 12 & 9 & 11 & 10 \\\\\n      11 & 12 & 9 & 10 & 11 \\\\\n      10 & 11 & 10 & 11 & 10 \\\\\n      10 & 12 & 9 & 11 & 10 \\\\\n      11 & 11 & 9 & 10 & 12 \\\\\n      10 & 12 & 10 & 11 & 11\n      \\end{bmatrix}\n      $$\n    - Proteomics matrix $X^{(3)}$ ($6 \\times 3$):\n      $$\n      \\begin{bmatrix}\n      0 & 0 & 1 \\\\\n      0 & 0 & 1 \\\\\n      0 & 0 & 1 \\\\\n      10 & 10 & 1 \\\\\n      0 & 0 & 1 \\\\\n      0 & 0 & 1\n      \\end{bmatrix}\n      $$\n    - Quality metrics:\n      - Metric $1$ (e.g., RNA quality): $[8.0,\\, 8.2,\\, 7.9,\\, 3.0,\\, 8.1,\\, 8.0]$\n      - Metric $2$ (e.g., library size): $[5{,}000{,}000,\\, 5{,}100{,}000,\\, 4{,}900{,}000,\\, 1{,}000{,}000,\\, 5{,}200{,}000,\\, 5{,}000{,}000]$\n    - Expected behavior: a single integrated outlier corresponding to the outlying proteomics sample and metrics.\n- Test Case $2$ (boundary condition: zero-variance features and no metric deviations):\n    - $n = 4$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 1$, $t = 3.0$, $\\phi = 1$.\n    - Omics matrices $$X^{(1)} = X^{(2)} = \\begin{bmatrix}1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\end{bmatrix}$$\n    - Quality metrics:\n      - Metric $1$: $[10,\\, 10,\\, 10,\\, 10]$\n      - Metric $2$: $[100,\\, 100,\\, 100,\\, 100]$\n    - Expected behavior: no integrated outliers.\n- Test Case $3$ (edge case: outliers only via metrics):\n    - $n = 5$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 2$, $t = 3.0$, $\\phi = 2$.\n    - Omics matrices:\n      - $$X^{(1)} = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\\\ 1.1 & 1.9 \\\\ 0.9 & 2.1 \\\\ 1 & 2 \\end{bmatrix}$$\n      - $$X^{(2)} = \\begin{bmatrix} 5 & 5 \\\\ 5 & 5 \\\\ 5.1 & 4.9 \\\\ 4.9 & 5.1 \\\\ 5 & 5 \\end{bmatrix}$$\n    - Quality metrics:\n      - Metric $1$: $[0.1,\\, 5.0,\\, 0.2,\\, 0.1,\\, 0.0]$\n      - Metric $2$: $[100,\\, 1000,\\, 105,\\, 98,\\, 97]$\n    - Expected behavior: a single integrated outlier via metrics (second sample).\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a list of $0$-based indices of samples flagged as integrated outliers for the corresponding test case. For example, an output of the form $[[i\\_1, i\\_2],[\\,], [j]]$ is valid. The final output must be printed exactly as a single Python list literal of lists on one line, with no additional text.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in established statistical methods (Principal Component Analysis leverage, Median Absolute Deviation), well-posed with a deterministic algorithm, objective in its language, and complete in its specification of data and parameters. The problem is a formalizable and relevant task in the quality control of multi-omics data within translational medicine. There are no contradictions, no violations of scientific principles, and no ambiguities that would prevent a unique, verifiable solution.\n\nHere follows a complete, reasoned solution.\n\n### **Algorithmic Procedure**\n\nThe quality control pipeline identifies outlier samples by integrating two distinct types of statistical measures across multiple omics datasets and quality metric vectors: PCA-based leverage scores and MAD-based robust deviations. The procedure for a given test case is as follows.\n\n**1. Initialization**\nLet $n$ be the number of samples. Two integer vectors are initialized to store outlier counts for each sample:\n- $C^{\\text{lev}} \\in \\mathbb{Z}^n$, initialized to zeros, to count leverage-based outlier flags.\n- $C^{\\text{met}} \\in \\mathbb{Z}^n$, initialized to zeros, to count metric-based outlier flags.\n\n**2. Leverage-Based Outlier Detection (per omics dataset)**\nFor each of the $O$ omics data matrices $X^{(o)} \\in \\mathbb{R}^{n \\times p}$, where $o \\in \\{1, \\dots, O\\}$:\n\n**a. Standardization:**\nEach column $j$ of $X^{(o)}$ is standardized to have a mean of $0$ and a standard deviation of $1$. Let $X^{(o)}_{:,j}$ be the $j$-th column. Its mean $\\mu_j$ and standard deviation $\\sigma_j$ are computed. The standardized column $X'^{(o)}_{:,j}$ is given by:\n$$\nX'^{(o)}_{i,j} = \\begin{cases} (X^{(o)}_{i,j} - \\mu_j) / \\sigma_j & \\text{if } \\sigma_j > 0 \\\\ 0 & \\text{if } \\sigma_j = 0 \\end{cases}\n$$\nThis creates the standardized matrix $X'^{(o)}$.\n\n**b. Singular Value Decomposition (SVD):**\nThe SVD of the standardized matrix is computed: $X'^{(o)} = U^{(o)} \\Sigma^{(o)} (V^{(o)})^\\top$. Here, $U^{(o)} \\in \\mathbb{R}^{n \\times n}$ is the matrix of left singular vectors.\n\n**c. Rank and Component Selection:**\nThe rank of the matrix, $r^{(o)} = \\operatorname{rank}(X'^{(o)})$, is determined. This corresponds to the number of non-zero singular values. The number of principal components to consider is $k' = \\min(k, r^{(o)})$, where $k$ is the user-specified number of components. We define $U_{k'}^{(o)}$ as the matrix containing the first $k'$ columns of $U^{(o)}$.\n\n**d. Leverage Calculation:**\nThe leverage score $h_i^{(o)}$ for each sample $i \\in \\{1, \\dots, n\\}$ is the sum of the squares of the elements in the $i$-th row of $U_{k'}^{(o)}$:\n$$\nh_i^{(o)} = \\sum_{j=1}^{k'} (U^{(o)}_{ij})^2\n$$\n\n**e. Outlier Flagging:**\nA leverage threshold $\\tau_{\\text{lev}}^{(o)}$ is calculated based on the mean leverage:\n$$\n\\tau_{\\text{lev}}^{(o)} = \\frac{\\alpha \\cdot k'}{n}\n$$\nwhere $\\alpha$ is the given amplification factor. If a sample's leverage $h_i^{(o)}$ exceeds this threshold, its leverage outlier count $C^{\\text{lev}}_i$ is incremented:\n$$\n\\text{if } h_i^{(o)} > \\tau_{\\text{lev}}^{(o)}, \\text{ then } C^{\\text{lev}}_i \\leftarrow C^{\\text{lev}}_i + 1\n$$\n\n**3. Metric-Based Outlier Detection (per quality metric)**\nFor each provided quality metric vector $q \\in \\mathbb{R}^n$:\n\n**a. Median and Median Absolute Deviation (MAD):**\nThe median of the metric, $m = \\operatorname{median}(q)$, is calculated. The absolute deviations from the median, $d_i = |q_i - m|$, are computed for all samples. The MAD is the median of these absolute deviations: $\\operatorname{MAD}(q) = \\operatorname{median}(d)$.\n\n**b. Outlier Flagging:**\nThe method for flagging outliers depends on the value of $\\operatorname{MAD}(q)$:\n- If $\\operatorname{MAD}(q) > 0$: The robust scaled deviation $z_i$ is calculated for each sample:\n  $$\n  z_i = \\frac{d_i}{c \\cdot \\operatorname{MAD}(q)} = \\frac{|q_i - \\operatorname{median}(q)|}{1.4826 \\cdot \\operatorname{MAD}(q)}\n  $$\n  If $z_i$ exceeds the metric threshold $t$, the sample's metric outlier count $C^{\\text{met}}_i$ is incremented.\n- If $\\operatorname{MAD}(q) = 0$: Any sample $i$ for which the metric value $q_i$ is not equal to the median $m$ is considered an outlier. For each such sample, $C^{\\text{met}}_i$ is incremented. If all $q_i$ are equal to the median, no outliers are flagged for this metric.\n\n**4. Integrated Outlier Identification**\nAfter processing all omics matrices and quality metrics, the final outlier status of each sample $i$ is determined by comparing its accumulated counts $C^{\\text{lev}}_i$ and $C^{\\text{met}}_i$ with the respective integer thresholds $\\theta$ and $\\phi$:\n$$\n\\text{Sample } i \\text{ is an integrated outlier if } (C^{\\text{lev}}_i \\geq \\theta) \\lor (C^{\\text{met}}_i \\geq \\phi)\n$$\nThe 0-based indices of all samples satisfying this condition are collected as the final result for the test case.\n\n---\n### **Application to Test Cases**\n\n**Test Case 1**\n- Parameters: $n=6, O=3, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$.\n- **Leverage:**\n  - For $X^{(1)}$ and $X^{(2)}$, the data points are relatively homogeneous. After standardization, the SVD yields leverage scores where no single sample dominates. The ranks are $r^{(1)}=4$ and $r^{(2)}=5$. Then $k' = \\min(2,4)=2$ and $k'=\\min(2,5)=2$. The thresholds are $\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 6 \\approx 0.667$ and $\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 6 \\approx 0.667$. All calculated $h_i^{(1)}$ and $h_i^{(2)}$ values are found to be below this threshold.\n  - For $X^{(3)}$, the 4th sample (index $3$) is anomalous: $[10, 10, 1]$. After standardization, the matrix rank is $r^{(3)}=1$. Thus, $k'=\\min(2,1)=1$. The leverage threshold is $\\tau_{\\text{lev}}^{(3)} = 2.0 \\cdot 1 / 6 \\approx 0.333$. The leverage scores are approximately $h^{(3)} = [0.038, 0.038, 0.038, 0.769, 0.038, 0.038]$. Only $h_3^{(3)} \\approx 0.769 > 0.333$.\n  - The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 1, 0, 0]$.\n- **Metrics:**\n  - Metric 1: $q = [8.0, 8.2, 7.9, 3.0, 8.1, 8.0]$. The median is $8.0$. Deviations are $[0.0, 0.2, 0.1, 5.0, 0.1, 0.0]$. The MAD is $0.1$. The z-score for sample 4 (index 3) is $z_3 = 5.0 / (1.4826 \\cdot 0.1) \\approx 33.72 > 3.0$.\n  - Metric 2: $q = [5\\text{e}6, 5.1\\text{e}6, 4.9\\text{e}6, 1\\text{e}6, 5.2\\text{e}6, 5\\text{e}6]$. The median is $5\\text{e}6$. MAD is $1\\text{e}5$. The z-score for sample 4 is $z_3 = 4\\text{e}6 / (1.4826 \\cdot 1\\text{e}5) \\approx 26.98 > 3.0$.\n  - The metric counts are $C^{\\text{met}} = [0, 0, 0, 2, 0, 0]$.\n- **Integration:**\n  - For sample 4 (index 3): $C^{\\text{lev}}_3=1 \\geq \\theta=1$ and $C^{\\text{met}}_3=2 \\geq \\phi=1$. The sample is an outlier.\n  - For all other samples $i \\neq 3$, $C^{\\text{lev}}_i=0 < 1$ and $C^{\\text{met}}_i=0 < 1$. They are not outliers.\n- **Result:** `[3]`\n\n**Test Case 2**\n- Parameters: $n=4, O=2, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$.\n- **Leverage:**\n  - Both $X^{(1)}$ and $X^{(2)}$ consist of identical rows. Each column has a standard deviation of $0$.\n  - Per the rule, both standardized matrices $X'^{(1)}$ and $X'^{(2)}$ are zero matrices.\n  - The rank of a zero matrix is $r=0$. Thus $k'=\\min(2, 0)=0$.\n  - The leverage scores $h_i$ are all $0$, and the threshold $\\tau_{\\text{lev}}$ is also $0$. The condition $h_i > \\tau_{\\text{lev}}$ is never met.\n  - The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 0]$.\n- **Metrics:**\n  - Both metric vectors, $[10, 10, 10, 10]$ and $[100, 100, 100, 100]$, consist of identical values.\n  - For both, the median is the constant value, all deviations are $0$, and thus the MAD is $0$.\n  - The special case for $\\operatorname{MAD}=0$ applies. Since no $q_i$ differs from the median, no outliers are flagged.\n  - The metric counts are $C^{\\text{met}} = [0, 0, 0, 0]$.\n- **Integration:**\n  - For all samples, $C^{\\text{lev}}_i=0$ and $C^{\\text{met}}_i=0$. The conditions $C^{\\text{lev}}_i \\geq 1$ or $C^{\\text{met}}_i \\geq 1$ are never met.\n- **Result:** `[]`\n\n**Test Case 3**\n- Parameters: $n=5, O=2, k=2, \\alpha=2.0, \\theta=2, t=3.0, \\phi=2$.\n- **Leverage:**\n  - The matrices $X^{(1)}$ and $X^{(2)}$ consist of very similar samples.\n  - For $X^{(1)}$, $r^{(1)}=2, k'=2$. $\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 5 = 0.8$. All $h_i^{(1)}$ are below this threshold.\n  - For $X^{(2)}$, $r^{(2)}=2, k'=2$. $\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 5 = 0.8$. All $h_i^{(2)}$ are below this threshold.\n  - No sample is flagged as a leverage outlier in any omic. The leverage counts are $C^{\\text{lev}} = [0, 0, 0, 0, 0]$.\n- **Metrics:**\n  - Metric 1: $q = [0.1, 5.0, 0.2, 0.1, 0.0]$. Median is $0.1$. MAD is $0.1$. The z-score for sample 2 (index 1) is $z_1 = 4.9 / (1.4826 \\cdot 0.1) \\approx 33.05 > 3.0$. Sample 2 is an outlier.\n  - Metric 2: $q = [100, 1000, 105, 98, 97]$. Median is $100$. MAD is $3$. The z-score for sample 2 is $z_1 = 900 / (1.4826 \\cdot 3) \\approx 202.41 > 3.0$. Sample 2 is an outlier.\n  - The metric counts are $C^{\\text{met}} = [0, 2, 0, 0, 0]$.\n- **Integration:**\n  - The thresholds are high: $\\theta=2, \\phi=2$.\n  - For sample 2 (index 1): $C^{\\text{lev}}_1=0 < \\theta=2$, but $C^{\\text{met}}_1=2 \\geq \\phi=2$. The sample is an outlier.\n  - For all other samples, counts are too low.\n- **Result:** `[1]`",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run the multi-omics outlier detection pipeline on a predefined test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"params\": {\"n\": 6, \"O\": 3, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([\n                    [0.5, 1.0, -0.3, 0.2],\n                    [0.6, 0.8, -0.1, 0.0],\n                    [0.4, 1.1, -0.2, 0.1],\n                    [0.5, 0.9, -0.3, 0.2],\n                    [0.6, 1.0, -0.2, 0.3],\n                    [0.5, 0.95, -0.25, 0.15]\n                ]),\n                np.array([\n                    [10, 12, 9, 11, 10],\n                    [11, 12, 9, 10, 11],\n                    [10, 11, 10, 11, 10],\n                    [10, 12, 9, 11, 10],\n                    [11, 11, 9, 10, 12],\n                    [10, 12, 10, 11, 11]\n                ]),\n                np.array([\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [10, 10, 1],\n                    [0, 0, 1],\n                    [0, 0, 1]\n                ])\n            ],\n            \"quality_metrics\": [\n                np.array([8.0, 8.2, 7.9, 3.0, 8.1, 8.0]),\n                np.array([5_000_000, 5_100_000, 4_900_000, 1_000_000, 5_200_000, 5_000_000])\n            ]\n        },\n        # Test Case 2\n        {\n            \"params\": {\"n\": 4, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]),\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]])\n            ],\n            \"quality_metrics\": [\n                np.array([10, 10, 10, 10]),\n                np.array([100, 100, 100, 100])\n            ]\n        },\n        # Test Case 3\n        {\n            \"params\": {\"n\": 5, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 2, \"t\": 3.0, \"phi\": 2},\n            \"omics_data\": [\n                np.array([[1, 2], [1, 2], [1.1, 1.9], [0.9, 2.1], [1, 2]]),\n                np.array([[5, 5], [5, 5], [5.1, 4.9], [4.9, 5.1], [5, 5]])\n            ],\n            \"quality_metrics\": [\n                np.array([0.1, 5.0, 0.2, 0.1, 0.0]),\n                np.array([100, 1000, 105, 98, 97])\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        params = case[\"params\"]\n        n = params[\"n\"]\n        k = params[\"k\"]\n        alpha = params[\"alpha\"]\n        theta = params[\"theta\"]\n        t = params[\"t\"]\n        phi = params[\"phi\"]\n        \n        c_lev = np.zeros(n, dtype=int)\n        c_met = np.zeros(n, dtype=int)\n\n        # Leverage-based outlier detection\n        for X in case[\"omics_data\"]:\n            # Standardize matrix\n            mean = np.mean(X, axis=0)\n            std = np.std(X, axis=0)\n            X_std = np.zeros_like(X, dtype=float)\n            for j in range(X.shape[1]):\n                if std[j] > 1e-9: # Use tolerance for float comparison\n                    X_std[:, j] = (X[:, j] - mean[j]) / std[j]\n            \n            # SVD and rank\n            try:\n                U, s, Vh = svd(X_std, full_matrices=False)\n                # Rank is number of singular values greater than a tolerance\n                rank = np.linalg.matrix_rank(X_std)\n            except np.linalg.LinAlgError:\n                # SVD can fail on some ill-conditioned matrices, though unlikely here\n                continue\n\n            num_components = min(k, rank)\n            if num_components == 0:\n                continue\n\n            # Leverage calculation\n            U_k = U[:, :num_components]\n            h = np.sum(U_k**2, axis=1)\n            \n            # Leverage outlier flagging\n            leverage_threshold = alpha * num_components / n\n            c_lev[h > leverage_threshold] += 1\n\n        # Metric-based outlier detection\n        c_consistency = 1.4826\n        for q in case[\"quality_metrics\"]:\n            median_q = np.median(q)\n            deviations = np.abs(q - median_q)\n            mad_q = np.median(deviations)\n\n            if mad_q > 1e-9: # Use tolerance for float comparison\n                z_scores = deviations / (c_consistency * mad_q)\n                c_met[z_scores > t] += 1\n            else:\n                # Special case for MAD == 0\n                c_met[q != median_q] += 1\n        \n        # Integrated outlier identification\n        integrated_outliers = np.where((c_lev >= theta) | (c_met >= phi))[0].tolist()\n        results.append(integrated_outliers)\n\n    # Final print statement must produce the exact single-line format.\n    # The format `f\"[{','.join(map(str, results))}]\"` is explicitly requested,\n    # resulting in a string like '[[3],[],[1]]'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once data quality is assured, we can proceed to integration. One powerful paradigm is to shift the focus from features to patients, constructing a network of patient-to-patient similarities for each omic type and then fusing these networks. This hands-on exercise  provides a step-by-step implementation of Similarity Network Fusion (SNF), a state-of-the-art algorithm that iteratively refines these networks to reveal a single, robust patient similarity structure reinforced across all omics.",
            "id": "5034014",
            "problem": "Implement a complete, self-contained program that constructs a multi-omics Similarity Network Fusion (SNF) pipeline to integrate patient-patient similarity networks across multiple omics and returns, for each provided test case, a single fused affinity matrix. The algorithmic steps must be derived from fundamental and well-tested principles: distance-based similarity, Gaussian kernels, nearest-neighbor graphs, stochastic normalization, and iterative message passing on graphs with a convergence criterion. Your implementation must strictly follow the procedure and definitions below and produce outputs exactly in the specified format.\n\nYou are given, per test case, a set of omics-specific patient feature matrices and scalar hyperparameters. Let there be $M$ omics (data views), each represented by a real-valued matrix $X^{(m)} \\in \\mathbb{R}^{N \\times p_m}$ for $m \\in \\{1,\\dots,M\\}$, where $N$ is the number of patients and $p_m$ is the number of features in omic $m$. The goal is to produce a single fused affinity matrix $F \\in \\mathbb{R}^{N \\times N}$ that integrates similarities across all omics, using iterative message passing until convergence or a maximum number of iterations is reached.\n\nBase definitions and required steps, which must be implemented exactly as stated:\n\n1. Z-scoring per omic and per feature:\n   - For each omic $m$ and each feature (column) $j \\in \\{1,\\dots,p_m\\}$, compute the mean $\\mu_{m,j}$ and standard deviation $\\sigma_{m,j}$ across patients (rows). Transform $X^{(m)}_{:,j}$ to $\\tilde{X}^{(m)}_{:,j}$ via $\\tilde{X}^{(m)}_{i,j} = \\frac{X^{(m)}_{i,j} - \\mu_{m,j}}{\\sigma_{m,j}}$. If $\\sigma_{m,j} = 0$, set $\\tilde{X}^{(m)}_{:,j}$ to the zero vector.\n   - This is justified by the widely used standardization in statistics to remove scale differences, ensuring comparability across features and omics.\n\n2. Pairwise squared Euclidean distance within each omic:\n   - For each omic $m$, compute $D^{(m)} \\in \\mathbb{R}^{N \\times N}$ with entries $D^{(m)}_{ij} = \\sum_{l=1}^{p_m} \\left(\\tilde{X}^{(m)}_{i,l} - \\tilde{X}^{(m)}_{j,l}\\right)^2$.\n\n3. Gaussian kernel similarity:\n   - Given a positive bandwidth parameter $\\sigma > 0$ (provided per test case), define $G^{(m)} \\in \\mathbb{R}^{N \\times N}$ by\n     $$G^{(m)}_{ij} = \\exp\\left( - \\frac{D^{(m)}_{ij}}{2 \\sigma^2} \\right).$$\n   - Set $G^{(m)}_{ii} = 1$ for all $i$.\n\n4. $k$-nearest neighbor sparsification and symmetrization:\n   - Let $k$ be a positive integer with $1 \\le k \\le N-1$ (provided per test case). For each row $i$ in $G^{(m)}$, identify the indices of the $k$ largest off-diagonal entries $G^{(m)}_{ij}$ with $j \\neq i$; in case of ties, break ties by smaller index $j$ first. Create a sparse matrix $S^{(m)}_{\\text{mask}}$ that keeps only these $k$ neighbors per row (off-diagonals) and zeros out the rest of the off-diagonal entries. Then symmetrize:\n     $$W^{(m)} = \\frac{S^{(m)}_{\\text{mask}} + \\left(S^{(m)}_{\\text{mask}}\\right)^{\\top}}{2},$$\n     and finally set the diagonal by $W^{(m)}_{ii} = 1$ for all $i$.\n\n5. Row-stochastic normalization:\n   - Convert $W^{(m)}$ to a row-stochastic matrix $P^{(m,0)}$ by normalizing rows to sum to $1$:\n     $$P^{(m,0)}_{ij} = \\frac{W^{(m)}_{ij}}{\\sum_{j'=1}^{N} W^{(m)}_{ij'}}.$$\n     If a row sum is $0$, replace that row by the uniform distribution over all columns, i.e., $P^{(m,0)}_{ij} = \\frac{1}{N}$ for all $j$.\n\n6. Fixed neighbor-constrained propagation matrices:\n   - From $P^{(m,0)}$, construct fixed propagation matrices $S^{(m)}$ by zeroing all but the same $k$ off-diagonal nearest neighbors per row (based on the magnitudes in $P^{(m,0)}$, ties broken by smaller index) and then normalizing each row to sum to $1$:\n     $$S^{(m)}_{ij} = \\begin{cases}\n     \\frac{P^{(m,0)}_{ij}}{\\sum_{j' \\in \\mathcal{N}^{(m)}_k(i)} P^{(m,0)}_{ij'}} & \\text{if } j \\in \\mathcal{N}^{(m)}_k(i),\\ j \\neq i,\\\\\n     0 & \\text{otherwise,}\n     \\end{cases}$$\n     where $\\mathcal{N}^{(m)}_k(i)$ is the index set of the $k$ selected off-diagonal neighbors of $i$ in $P^{(m,0)}$. If the denominator is $0$, set $S^{(m)}_{ij} = \\frac{1}{N}$ for all $j$.\n\n7. Iterative message passing (fusion):\n   - For $t = 0,1,2,\\dots$, update each view as\n     $$P^{(m,t+1)} = S^{(m)} \\left( \\frac{1}{M-1} \\sum_{\\substack{v=1 \\\\ v \\ne m}}^{M} P^{(v,t)} \\right) \\left(S^{(m)}\\right)^{\\top},$$\n     followed by row-stochastic normalization of $P^{(m,t+1)}$ as in step $5$.\n   - For $M = 1$, skip this iterative update and define the fused matrix as the single view after step $5$.\n\n8. Convergence criterion and termination:\n   - Define the change at iteration $t$ as\n     $$\\Delta^{(t)} = \\max_{m \\in \\{1,\\dots,M\\}} \\left\\| P^{(m,t+1)} - P^{(m,t)} \\right\\|_F,$$\n     where $\\|\\cdot\\|_F$ is the Frobenius norm. Given a tolerance $\\varepsilon > 0$ and a maximum number of iterations $T_{\\max} \\in \\mathbb{N}$ (both provided), stop the iteration at the first $t$ such that $\\Delta^{(t)} \\le \\varepsilon$ or when $t+1 = T_{\\max}$, whichever occurs first.\n\n9. Fused matrix:\n   - After termination, define\n     $$F = \\frac{1}{M} \\sum_{m=1}^{M} P^{(m,t_{\\text{final}})}.$$\n     Row-normalize $F$ to be row-stochastic as in step $5$.\n\n10. Required output per test case:\n    - Flatten $F$ in row-major order into a list of $N \\times N$ real numbers. Round each entry to exactly $6$ decimal places (fixed-point, no scientific notation).\n\nProgram input is fixed within your code and must consist of the following test suite. For each test case, you must implement the above steps using the given omics matrices and hyperparameters $(k, \\sigma, \\varepsilon, T_{\\max})$:\n\n- Test case $1$ (happy path):\n  - $N = 4$, $M = 2$, $k = 2$, $\\sigma = 1.0$, $\\varepsilon = 1\\times 10^{-6}$, $T_{\\max} = 50$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{4 \\times 2}$ with rows\n    $[0.0, 1.0]$, $[0.2, 0.9]$, $[3.0, 3.5]$, $[3.2, 3.6]$.\n  - Omic $2$: $X^{(2)} \\in \\mathbb{R}^{4 \\times 2}$ with rows\n    $[0.1, 1.1]$, $[0.0, 1.0]$, $[3.1, 3.4]$, $[3.3, 3.7]$.\n\n- Test case $2$ (single-omic edge case):\n  - $N = 4$, $M = 1$, $k = 1$, $\\sigma = 0.7$, $\\varepsilon = 1\\times 10^{-6}$, $T_{\\max} = 10$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{4 \\times 3}$ with rows\n    $[1.0, 0.0, 0.5]$, $[0.9, 0.1, 0.4]$, $[3.0, 3.5, 3.7]$, $[3.1, 3.6, 3.8]$.\n\n- Test case $3$ (early-stop via large tolerance):\n  - $N = 5$, $M = 2$, $k = 2$, $\\sigma = 1.5$, $\\varepsilon = 10^{9}$, $T_{\\max} = 20$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{5 \\times 2}$ with rows\n    $[0.0, 0.0]$, $[0.1, -0.1]$, $[5.0, 5.0]$, $[5.1, 4.9]$, $[2.5, 2.5]$.\n  - Omic $2$: $X^{(2)} \\in \\mathbb{R}^{5 \\times 2}$ with rows\n    $[0.2, -0.2]$, $[0.0, 0.0]$, $[4.9, 5.1]$, $[5.2, 5.0]$, $[2.4, 2.6]$.\n\n- Test case $4$ (boundary on iterations):\n  - $N = 3$, $M = 3$, $k = 2$, $\\sigma = 1.0$, $\\varepsilon = 1\\times 10^{-12}$, $T_{\\max} = 1$.\n  - Omic $1$: $X^{(1)} \\in \\mathbb{R}^{3 \\times 2}$ with rows\n    $[0.0, 0.0]$, $[1.0, 1.0]$, $[2.0, 2.0]$.\n  - Omic $2$: $X^{(2)} \\in \\mathbb{R}^{3 \\times 2}$ with rows\n    $[0.0, 0.1]$, $[1.0, 1.1]$, $[1.9, 2.1]$.\n  - Omic $3$: $X^{(3)} \\in \\mathbb{R}^{3 \\times 3}$ with rows\n    $[0.0, 1.0, 2.0]$, $[1.0, 2.0, 3.0]$, $[2.0, 3.0, 4.0]$.\n\nYour program must compute the fused matrix $F$ for each test case following steps $1$ through $9$ above, then flatten and round $F$ as specified in step $10$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\big[$result\\_1,result\\_2,result\\_3,result\\_4$\\big]$). Here each result must itself be a list of $N \\times N$ floating-point numbers (flattened fused matrix) rounded to exactly $6$ decimal places, printed in fixed-point notation without scientific notation. No extra text or lines are permitted.",
            "solution": "The user has provided a valid problem statement. It is scientifically grounded, well-posed, objective, and complete. The problem requires the implementation of the Similarity Network Fusion (SNF) algorithm, a state-of-the-art method in bioinformatics for integrating multi-omics data. The procedure is based on established principles from statistics, numerical analysis, and graph theory. The provided test cases are well-defined and allow for a deterministic verification of the implementation. I will now proceed with a detailed solution.\n\nThe solution involves a step-by-step implementation of the specified SNF pipeline. The core idea is to convert each omics dataset into a patient-patient similarity network and then iteratively fuse these networks to produce a single, comprehensive network that captures similarities reinforced across multiple data types.\n\n### Algorithmic Design and Principles\n\nThe implementation is designed as a modular function that executes the prescribed sequence of operations.\n\n1.  **Data Standardization (Z-scoring):**\n    For each omic data type $m \\in \\{1,\\dots,M\\}$, represented by a matrix $X^{(m)} \\in \\mathbb{R}^{N \\times p_m}$, we first standardize the data. Standardization is a critical preprocessing step to ensure that features with larger numerical ranges do not disproportionately influence downstream distance calculations. For each feature (column) $j$, we compute its mean $\\mu_{m,j}$ and standard deviation $\\sigma_{m,j}$ over the $N$ patients. Each entry is then transformed according to the Z-score formula:\n    $$ \\tilde{X}^{(m)}_{i,j} = \\frac{X^{(m)}_{i,j} - \\mu_{m,j}}{\\sigma_{m,j}} $$\n    In the special case where a feature is constant across all patients, its standard deviation $\\sigma_{m,j}$ is $0$. To prevent division by zero and render the feature non-informative (as it has no variance), the corresponding transformed feature vector $\\tilde{X}^{(m)}_{:,j}$ is set to a vector of zeros, as per the problem specification.\n\n2.  **Pairwise Similarity Network Construction:**\n    Following standardization, we transform each omic's feature-based representation into a patient-patient similarity network. This is a two-stage process. First, for each omic $m$, we compute the pairwise squared Euclidean distance matrix $D^{(m)} \\in \\mathbb{R}^{N \\times N}$, where each element $D^{(m)}_{ij}$ measures the dissimilarity between patient $i$ and patient $j$ in the standardized feature space:\n    $$ D^{(m)}_{ij} = \\sum_{l=1}^{p_m} \\left(\\tilde{X}^{(m)}_{i,l} - \\tilde{X}^{(m)}_{j,l}\\right)^2 $$\n    Next, these distances are converted into affinities using a Gaussian similarity kernel, also known as a radial basis function (RBF) kernel. This creates the affinity matrix $G^{(m)} \\in \\mathbb{R}^{N \\times N}$:\n    $$ G^{(m)}_{ij} = \\exp\\left( - \\frac{D^{(m)}_{ij}}{2 \\sigma^2} \\right) $$\n    The hyperparameter $\\sigma$ is the kernel bandwidth, which controls how rapidly affinity decays with distance. This function maps distances (from $0$ to $\\infty$) to similarities (from $1$ to $0$). The diagonal elements $G^{(m)}_{ii}$ are explicitly set to $1$, representing maximal self-similarity.\n\n3.  **Network Sparsification and Symmetrization:**\n    Real-world networks are often sparse. To reflect this and to denoise the network by removing weak, likely spurious connections, we retain only the connections to the $k$-nearest neighbors for each patient. For each patient $i$, we identify the $k$ other patients $j$ corresponding to the $k$ largest affinity values $G^{(m)}_{ij}$ (where $j \\neq i$). Ties are broken by preferring the neighbor with the smaller index $j$. This results in a directed k-NN graph. Since similarity is inherently a symmetric concept, this graph is symmetrized to produce an undirected weighted adjacency matrix $W^{(m)}$:\n    $$ W^{(m)} = \\frac{S^{(m)}_{\\text{mask}} + \\left(S^{(m)}_{\\text{mask}}\\right)^{\\top}}{2} $$\n    Here, $S^{(m)}_{\\text{mask}}$ is a matrix that retains the affinity values for the $k$ nearest neighbors in each row of $G^{(m)}$ and is zero otherwise. The diagonal elements $W^{(m)}_{ii}$ are set to $1$.\n\n4.  **Stochastic Normalization and Propagation Matrix Construction:**\n    The affinity matrices $W^{(m)}$ are converted into row-stochastic matrices $P^{(m,0)}$, where each row sums to $1$. This is achieved by dividing each element by its row sum:\n    $$ P^{(m,0)}_{ij} = \\frac{W^{(m)}_{ij}}{\\sum_{j'=1}^{N} W^{(m)}_{ij'}} $$\n    If a row sum is zero (i.e., a patient has no neighbors), it is replaced by a uniform distribution, $P^{(m,0)}_{ij} = 1/N$. These matrices can be interpreted as transition probability matrices for a random walk on the graph.\n    In parallel, we construct fixed propagation matrices $S^{(m)}$. These are sparse, row-stochastic matrices that encode the local neighborhood structure of each network. $S^{(m)}$ is derived from $P^{(m,0)}$ by retaining, for each row $i$, only the entries corresponding to its $k$-nearest neighbors (as determined by the values in $P^{(m,0)}$) and re-normalizing these entries to sum to $1$. This matrix $S^{(m)}$ will act as a fixed filter to propagate information during the fusion process.\n\n5.  **Iterative Network Fusion:**\n    The fusion is achieved through an iterative cross-network diffusion process, a form of message passing. At each iteration $t$, each network $P^{(m,t)}$ is updated based on the information from all other networks. The update rule is:\n    $$ P^{(m,t+1)} = S^{(m)} \\left( \\frac{1}{M-1} \\sum_{\\substack{v=1 \\\\ v \\ne m}}^{M} P^{(v,t)} \\right) \\left(S^{(m)}\\right)^{\\top} $$\n    The term in the parenthesis is the average of all other networks at iteration $t$. This average network is then \"filtered\" through the local structure of network $m$ via pre- and post-multiplication by $S^{(m)}$ and its transpose. This step effectively reinforces similarities that are supported by multiple data sources, causing the networks to become progressively more similar. After each update, $P^{(m,t+1)}$ is re-normalized to be row-stochastic. This process is skipped for the single-omic case ($M=1$).\n\n6.  **Convergence and Final Fusion:**\n    The iterative process continues until the networks stabilize or a maximum number of iterations, $T_{\\max}$, is reached. Stabilization is assessed by measuring the change between consecutive iterations using the Frobenius norm. The process terminates at iteration $t$ if the maximum change across all networks falls below a tolerance $\\varepsilon$:\n    $$ \\Delta^{(t)} = \\max_{m \\in \\{1,\\dots,M\\}} \\left\\| P^{(m,t+1)} - P^{(m,t)} \\right\\|_F \\le \\varepsilon $$\n    Upon termination at $t_{\\text{final}}$, the final fused network $F$ is computed by averaging the converged networks:\n    $$ F = \\frac{1}{M} \\sum_{m=1}^{M} P^{(m,t_{\\text{final}})} $$\n    This final matrix is also row-normalized to yield the final patient-patient similarity network. The resulting matrix $F$ represents a consolidated view of patient similarity, integrating evidence from all $M$ omics.\n\n7.  **Output Formatting:**\n    The final fused matrix $F$ is flattened into a one-dimensional list in row-major order, and each element is rounded to $6$ decimal places as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the SNF pipeline, printing the final results.\n    \"\"\"\n\n    def normalize_rows(matrix):\n        \"\"\"\n        Row-normalizes a matrix to be row-stochastic.\n        If a row sum is 0, it is replaced with a uniform distribution.\n        \"\"\"\n        N = matrix.shape[0]\n        row_sums = matrix.sum(axis=1, keepdims=True)\n        normalized_matrix = matrix.copy()\n        \n        non_zero_rows = row_sums.flatten() > 0\n        if np.any(non_zero_rows):\n            normalized_matrix[non_zero_rows, :] /= row_sums[non_zero_rows]\n        \n        zero_rows = ~non_zero_rows\n        if np.any(zero_rows):\n            normalized_matrix[zero_rows, :] = 1.0 / N\n            \n        return normalized_matrix\n\n    def get_k_nearest_neighbor_mask(A, k):\n        \"\"\"\n        Determines the k-nearest-neighbor mask for a given affinity matrix.\n        Tie-breaking is done by smaller column index.\n        \"\"\"\n        N = A.shape[0]\n        mask = np.zeros((N, N), dtype=bool)\n\n        for i in range(N):\n            row_values = A[i, :]\n            # Get off-diagonal elements and their original indices\n            off_diag_indices = np.arange(N) != i\n            off_diag_vals = row_values[off_diag_indices]\n            off_diag_j_indices = np.arange(N)[off_diag_indices]\n\n            # Use lexsort for stable sorting: sort by -value (desc), then index (asc)\n            sorted_indices = np.lexsort((off_diag_j_indices, -off_diag_vals))\n            \n            # Identify the top k original column indices\n            top_k_j_indices = off_diag_j_indices[sorted_indices[:k]]\n            mask[i, top_k_j_indices] = True\n            \n        return mask\n\n    def run_snf_for_case(X_list, k, sigma, eps, T_max):\n        \"\"\"\n        Executes the complete Similarity Network Fusion (SNF) pipeline for a single test case.\n        \"\"\"\n        N = X_list[0].shape[0]\n        M = len(X_list)\n\n        # Step 1: Z-scoring per omic and per feature\n        X_std_list = []\n        for X_m in X_list:\n            mean = np.mean(X_m, axis=0)\n            std = np.std(X_m, axis=0)\n            X_tilde_m = np.zeros_like(X_m, dtype=float)\n            \n            non_zero_std_mask = std > 0\n            if np.any(non_zero_std_mask):\n                X_tilde_m[:, non_zero_std_mask] = (X_m[:, non_zero_std_mask] - mean[non_zero_std_mask]) / std[non_zero_std_mask]\n            \n            X_std_list.append(X_tilde_m)\n\n        # Step 2: Pairwise squared Euclidean distance\n        D_list = [squareform(pdist(X_std, metric='sqeuclidean')) for X_std in X_std_list]\n\n        # Step 3: Gaussian kernel similarity\n        G_list = []\n        for D_m in D_list:\n            G_m = np.exp(-D_m / (2 * sigma**2))\n            np.fill_diagonal(G_m, 1)\n            G_list.append(G_m)\n\n        # Step 4: k-nearest neighbor sparsification and symmetrization\n        W_list = []\n        for G_m in G_list:\n            knn_mask = get_k_nearest_neighbor_mask(G_m, k)\n            S_mask_matrix = np.zeros_like(G_m)\n            S_mask_matrix[knn_mask] = G_m[knn_mask]\n            \n            W_m = (S_mask_matrix + S_mask_matrix.T) / 2\n            np.fill_diagonal(W_m, 1)\n            W_list.append(W_m)\n\n        # Step 5: Row-stochastic normalization\n        P0_list = [normalize_rows(W_m) for W_m in W_list]\n\n        if M == 1:\n            # For M=1, result is the single normalized network\n            final_fused_matrix = normalize_rows(P0_list[0])\n            flat_rounded = [f\"{x:.6f}\" for x in final_fused_matrix.flatten()]\n            return f\"[{','.join(flat_rounded)}]\"\n\n        # Step 6: Fixed neighbor-constrained propagation matrices\n        S_list = []\n        for P0_m in P0_list:\n            S_m = np.zeros_like(P0_m)\n            knn_mask = get_k_nearest_neighbor_mask(P0_m, k)\n            for i in range(N):\n                neighbor_indices = np.where(knn_mask[i, :])[0]\n                denominator = P0_m[i, neighbor_indices].sum()\n                if denominator > 0:\n                    S_m[i, neighbor_indices] = P0_m[i, neighbor_indices] / denominator\n                else:\n                    S_m[i, :] = 1.0 / N\n            S_list.append(S_m)\n            \n        # Step 7 & 8: Iterative message passing and convergence\n        P_current = [p.copy() for p in P0_list]\n        for t in range(T_max):\n            P_next = [np.zeros_like(p) for p in P_current]\n            sum_P_current = sum(P_current) if M > 1 else P_current[0]\n            \n            for m in range(M):\n                avg_others = (sum_P_current - P_current[m]) / (M - 1)\n                P_next_m_unnorm = S_list[m] @ avg_others @ S_list[m].T\n                P_next[m] = normalize_rows(P_next_m_unnorm)\n\n            max_diff = 0\n            for m in range(M):\n                diff = np.linalg.norm(P_next[m] - P_current[m], 'fro')\n                max_diff = max(max_diff, diff)\n            \n            P_current = P_next\n            \n            if max_diff <= eps:\n                break\n\n        # Step 9: Fused matrix\n        F_unnorm = sum(P_current) / M\n        final_fused_matrix = normalize_rows(F_unnorm)\n\n        # Step 10: Required output formatting\n        flat_rounded = [f\"{x:.6f}\" for x in final_fused_matrix.flatten()]\n        return f\"[{','.join(flat_rounded)}]\"\n\n    test_cases = [\n        {\n            \"X_list\": [\n                np.array([[0.0, 1.0], [0.2, 0.9], [3.0, 3.5], [3.2, 3.6]], dtype=float),\n                np.array([[0.1, 1.1], [0.0, 1.0], [3.1, 3.4], [3.3, 3.7]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.0, \"eps\": 1e-6, \"T_max\": 50\n        },\n        {\n            \"X_list\": [\n                np.array([[1.0, 0.0, 0.5], [0.9, 0.1, 0.4], [3.0, 3.5, 3.7], [3.1, 3.6, 3.8]], dtype=float)\n            ],\n            \"k\": 1, \"sigma\": 0.7, \"eps\": 1e-6, \"T_max\": 10\n        },\n        {\n            \"X_list\": [\n                np.array([[0.0, 0.0], [0.1, -0.1], [5.0, 5.0], [5.1, 4.9], [2.5, 2.5]], dtype=float),\n                np.array([[0.2, -0.2], [0.0, 0.0], [4.9, 5.1], [5.2, 5.0], [2.4, 2.6]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.5, \"eps\": 1e9, \"T_max\": 20\n        },\n        {\n            \"X_list\": [\n                np.array([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]], dtype=float),\n                np.array([[0.0, 0.1], [1.0, 1.1], [1.9, 2.1]], dtype=float),\n                np.array([[0.0, 1.0, 2.0], [1.0, 2.0, 3.0], [2.0, 3.0, 4.0]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.0, \"eps\": 1e-12, \"T_max\": 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result_str = run_snf_for_case(\n            case[\"X_list\"], case[\"k\"], case[\"sigma\"], case[\"eps\"], case[\"T_max\"]\n        )\n        results.append(result_str)\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building a predictive model from multi-omics data often involves tuning hyperparameters to optimize performance, but how do we get an honest estimate of how well the final, tuned model will perform on new data? This final practice  addresses this critical question by exploring nested cross-validation, the gold-standard procedure for obtaining an unbiased performance estimate. You will analyze why simpler approaches can fall prey to 'information leakage', leading to dangerously optimistic conclusions about a model's utility.",
            "id": "5033979",
            "problem": "A translational medicine team is building a prognostic model for response to immunotherapy in a cohort of $500$ patients profiled with messenger ribonucleic acid (mRNA) transcriptomics and proteomics. For patient $i$, the multi-omics measurements are $x_{i}^{(\\mathrm{RNA})} \\in \\mathbb{R}^{p_{\\mathrm{RNA}}}$ and $x_{i}^{(\\mathrm{Prot})} \\in \\mathbb{R}^{p_{\\mathrm{Prot}}}$, and the binary clinical outcome is $y_{i} \\in \\{0,1\\}$. The joint sample $\\mathcal{D} = \\{(x_{i}^{(\\mathrm{RNA})}, x_{i}^{(\\mathrm{Prot})}, y_{i})\\}_{i=1}^{n}$ is assumed independent and identically distributed from an unknown law $P_{X,Y}$, where $n = 500$, $p_{\\mathrm{RNA}} = 20000$, and $p_{\\mathrm{Prot}} = 3000$. The team plans to use a regularized predictor $f_{\\lambda}$ with hyperparameters $\\lambda$ and a supervised feature selection operator $\\mathcal{S}$ that, given data, returns a subset of features across both omics based on their association with $y$ (for example, top scores by absolute correlation with $y$ or differential expression).\n\nThey want to estimate the generalization risk $R(f_{\\lambda^{*}}) = \\mathbb{E}_{(X,Y) \\sim P_{X,Y}}[L(Y, f_{\\lambda^{*}}(X))]$, where $L$ is a bounded loss and $\\lambda^{*}$ is chosen by hyperparameter tuning. They consider Nested Cross-Validation (CV), which involves an outer CV loop to estimate performance and an inner CV loop to tune $\\lambda$ and any data-dependent steps. The inner loop should operate only on the outer training data and never on the outer test data.\n\nA colleague proposes to perform feature selection $\\mathcal{S}$ once on the full dataset $\\mathcal{D}$ prior to any splitting, then run an outer $K_{\\mathrm{out}}$-fold CV with an inner $K_{\\mathrm{in}}$-fold CV for tuning $\\lambda$ on each outer training split, using the globally preselected features from $\\mathcal{S}(\\mathcal{D})$ in all folds.\n\nWhich option most accurately defines nested CV for multi-omics model tuning and evaluation and correctly explains, from first principles, why information leakage occurs and biases the estimated risk when supervised feature selection $\\mathcal{S}$ is performed before splitting into inner and outer folds?\n\nA. Nested CV partitions $\\mathcal{D}$ into $K_{\\mathrm{out}}$ outer folds. For each outer fold $t$, the outer training set $\\mathcal{D}_{-t}$ is further partitioned into $K_{\\mathrm{in}}$ inner folds. All data-dependent operations, including feature selection $\\mathcal{S}$, normalization, and hyperparameter tuning of $f_{\\lambda}$, are fit exclusively on $\\mathcal{D}_{-t}$ using the inner loop to select $\\lambda^{*}$; a final model $f_{\\lambda^{*}}$ is trained on $\\mathcal{D}_{-t}$ and evaluated on the held-out outer test set $\\mathcal{T}_{t}$. The nested estimator $\\widehat{R}_{\\mathrm{nested}} = \\frac{1}{K_{\\mathrm{out}}}\\sum_{t=1}^{K_{\\mathrm{out}}} \\frac{1}{|\\mathcal{T}_{t}|}\\sum_{(x,y)\\in \\mathcal{T}_{t}} L(y, f_{\\lambda^{*}}(x))$ approximates $R(f_{\\lambda^{*}})$ because $f_{\\lambda^{*}}$ and the preprocessing pipeline are measurable functions of $\\mathcal{D}_{-t}$ only, rendering them independent of outcomes in $\\mathcal{T}_{t}$. If $\\mathcal{S}$ is computed once on $\\mathcal{D}$ before splitting, then for each $t$, $f_{\\lambda^{*}}$ depends on $\\mathcal{S}(\\mathcal{D})$, which in turn depends on $\\mathcal{T}_{t}$ through $y$; this breaks the independence between $f_{\\lambda^{*}}$ and $Y$ in $\\mathcal{T}_{t}$ and induces optimistic bias in $\\widehat{R}_{\\mathrm{nested}}$ because test outcomes influenced feature choice, a data snooping effect.\n\nB. Nested CV first uses the inner $K_{\\mathrm{in}}$-fold CV across $\\mathcal{D}$ to choose $\\lambda^{*}$, then uses outer $K_{\\mathrm{out}}$-fold CV (with the same $\\lambda^{*}$) to report performance. Performing $\\mathcal{S}$ on $\\mathcal{D}$ before splitting does not leak information because the same set of features is applied uniformly to both training and test splits, ensuring consistency.\n\nC. Nested CV draws $K_{\\mathrm{out}}$ outer folds and reuses those exact fold indices inside the inner loop to avoid distributional changes between inner and outer splits. Leakage only occurs when the hyperparameter tuning procedure directly accesses the outer test labels $y$; global supervised $\\mathcal{S}$ applied prior to splitting does not leak if the inner loop later re-optimizes $\\lambda$.\n\nD. Nested CV requires that feature selection be performed globally on the entire dataset to reduce variance of the evaluator, then the inner loop tunes $\\lambda$ within the selected feature space using resampling from the same folds as the outer loop. Leakage arises in multi-omics primarily when the omics modalities are highly correlated, not from pre-splitting $\\mathcal{S}$ itself.\n\nE. Nested CV for multi-omics early integration concatenates $x^{(\\mathrm{RNA})}$ and $x^{(\\mathrm{Prot})}$ first, then performs a single $K$-fold CV for tuning and evaluation simultaneously on the same partitions. Because early integration treats features uniformly, applying a global $\\mathcal{S}(\\mathcal{D})$ before CV cannot leak information; any bias must come from overfitting $\\lambda$, not from feature selection.",
            "solution": "The problem statement is assessed for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Cohort size:** $n = 500$ patients.\n- **Data per patient $i$:**\n    - mRNA transcriptomics: $x_{i}^{(\\mathrm{RNA})} \\in \\mathbb{R}^{p_{\\mathrm{RNA}}}$.\n    - Proteomics: $x_{i}^{(\\mathrm{Prot})} \\in \\mathbb{R}^{p_{\\mathrm{Prot}}}$.\n    - Clinical outcome: $y_{i} \\in \\{0,1\\}$.\n- **Feature dimensions:** $p_{\\mathrm{RNA}} = 20000$, $p_{\\mathrm{Prot}} = 3000$.\n- **Dataset:** $\\mathcal{D} = \\{(x_{i}^{(\\mathrm{RNA})}, x_{i}^{(\\mathrm{Prot})}, y_{i})\\}_{i=1}^{n}$.\n- **Data generating process:** The sample $\\mathcal{D}$ is assumed independent and identically distributed (i.i.d.) from an unknown probability law $P_{X,Y}$.\n- **Predictor:** A regularized predictor $f_{\\lambda}$ with hyperparameters $\\lambda$.\n- **Feature selection:** A supervised operator $\\mathcal{S}$ that selects a subset of features based on their association with the outcome $y$.\n- **Objective:** Estimate the generalization risk $R(f_{\\lambda^{*}}) = \\mathbb{E}_{(X,Y) \\sim P_{X,Y}}[L(Y, f_{\\lambda^{*}}(X))]$, where $L$ is a bounded loss function and $\\lambda^{*}$ is selected via hyperparameter tuning.\n- **Proposed evaluation method:** Nested Cross-Validation (CV), defined as an outer loop for performance estimation and an inner loop for tuning $\\lambda$ and other data-dependent steps. The inner loop must only use the outer training data.\n- **Colleague's flawed proposal:** Perform supervised feature selection $\\mathcal{S}$ once on the full dataset $\\mathcal{D}$ *before* any CV splitting. Then, run a nested CV procedure on the pre-selected features.\n- **Question:** Identify the option that correctly defines nested CV and explains, from first principles, the information leakage and bias caused by the colleague's proposal.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly grounded in statistical learning theory and its application to bioinformatics and translational medicine. The scenario describes a standard high-dimension, low-sample-size ($p = p_{\\mathrm{RNA}} + p_{\\mathrm{Prot}} = 23000 \\gg n = 500$) classification problem. Nested cross-validation is the standard, state-of-the-art procedure for obtaining a nearly unbiased estimate of generalization error for a model whose hyperparameters are tuned via a data-driven process. The concept of information leakage from supervised feature selection is a critical and well-established topic in machine learning methodology. The problem is scientifically sound.\n- **Well-Posed:** The problem is well-posed. It asks for the correct definition of a standard procedure (nested CV) and the correct explanation of a common methodological fallacy (information leakage). A unique and correct answer can be determined based on established principles.\n- **Objective:** The problem is stated using formal mathematical notation and precise, objective language. There are no subjective or ambiguous terms.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is scientifically sound, well-posed, and objective, describing a realistic and important methodological challenge in computational biology. I will now proceed with the derivation and evaluation of options.\n\n### Principle-Based Derivation\nThe primary goal is to estimate the generalization risk, $R(f_{\\lambda^{*}}) = \\mathbb{E}_{(X,Y) \\sim P_{X,Y}}[L(Y, f_{\\lambda^{*}}(X))]$. This is the expected performance of a model on new, unseen data drawn from the same distribution $P_{X,Y}$. The model-building process itself involves data-dependent steps: supervised feature selection via $\\mathcal{S}$ and hyperparameter selection for $\\lambda$. Let us denote the entire model-building pipeline, which includes feature selection and hyperparameter tuning, by $\\mathcal{A}$. Given a training dataset $\\mathcal{D}_{\\text{train}}$, this pipeline produces a specific predictor, $f = \\mathcal{A}(\\mathcal{D}_{\\text{train}})$. We want to estimate the expected performance of the *pipeline* applied to our full dataset $\\mathcal{D}$.\n\nThe fundamental principle of performance evaluation is that the test data must remain completely unseen during the entire model-building process. An estimator for the risk, for instance, $\\widehat{R} = \\frac{1}{|\\mathcal{T}|} \\sum_{(x,y) \\in \\mathcal{T}} L(y, f(x))$ computed on a test set $\\mathcal{T}$, is an unbiased estimate of the true risk of the specific model $f$ only if the data points in $\\mathcal{T}$ are i.i.d. and independent of the data used to construct $f$. Since $f = \\mathcal{A}(\\mathcal{D}_{\\text{train}})$, this means $\\mathcal{T}$ and $\\mathcal{D}_{\\text{train}}$ must be disjoint, and $f$ must be statistically independent of the outcomes $Y$ in the test set $\\mathcal{T}$.\n\n**Correct Nested Cross-Validation:**\nThis procedure is designed to respect the principle of data separation for both hyperparameter tuning and final performance estimation.\n1.  **Outer Loop (Performance Estimation):** The full dataset $\\mathcal{D}$ is partitioned into $K_{\\mathrm{out}}$ disjoint folds. For each fold $t \\in \\{1, \\dots, K_{\\mathrm{out}}\\}$, we designate fold $t$ as the outer test set $\\mathcal{T}_t$ and the remaining $K_{\\mathrm{out}}-1$ folds as the outer training set $\\mathcal{D}_{-t}$.\n2.  **Inner Pipeline (Model Building on $\\mathcal{D}_{-t}$):** For each outer fold $t$, the *entire model-building pipeline* is applied *only* to $\\mathcal{D}_{-t}$:\n    a. **Supervised Feature Selection:** The operator $\\mathcal{S}$ is applied to $\\mathcal{D}_{-t}$ to get a feature set $S_t = \\mathcal{S}(\\mathcal{D}_{-t})$.\n    b. **Inner Loop (Hyperparameter Tuning):** To select the best hyperparameter $\\lambda$ for a model trained on data with features $S_t$, we cannot use the outer test set $\\mathcal{T}_t$. Therefore, we perform another cross-validation *within* $\\mathcal{D}_{-t}$. $\\mathcal{D}_{-t}$ is partitioned into $K_{\\mathrm{in}}$ inner folds. For each candidate hyperparameter $\\lambda_j$, a model is trained on $K_{\\mathrm{in}}-1$ inner folds and validated on the held-out inner fold. The average validation performance across the $K_{\\mathrm{in}}$ inner folds is computed for $\\lambda_j$. The hyperparameter that yields the best average performance is selected as $\\lambda^{*}_t$.\n    c. **Final Model for Fold $t$:** A model $f_{t, \\lambda^{*}_t}$ is trained on the *entire* outer training set $\\mathcal{D}_{-t}$ using the selected features $S_t$ and the optimal hyperparameter $\\lambda^{*}_t$.\n3.  **Evaluation:** The model $f_{t, \\lambda^{*}_t}$ is evaluated on the held-out outer test set $\\mathcal{T}_t$, which it has never seen in any form. The error for fold $t$ is calculated.\n4.  **Final Risk Estimate:** The nested CV estimate of the generalization risk is the average of the errors across all $K_{\\mathrm{out}}$ outer folds: $\\widehat{R}_{\\mathrm{nested}} = \\frac{1}{K_{\\mathrm{out}}}\\sum_{t=1}^{K_{\\mathrm{out}}} \\text{Error}(\\mathcal{T}_t)$. This provides a nearly unbiased estimate of the true generalization performance of the entire pipeline $\\mathcal{A}$.\n\n**Analysis of the Colleague's Flawed Proposal:**\nThe proposal suggests performing supervised feature selection $\\mathcal{S}(\\mathcal{D})$ once, *before* any partitioning. Let the resulting features be $S_{\\text{global}}$.\n- When the outer loop considers fold $t$, the training set is $\\mathcal{D}_{-t}$ and the test set is $\\mathcal{T}_t$.\n- The model will be trained on $\\mathcal{D}_{-t}$ using features $S_{\\text{global}}$.\n- However, $S_{\\text{global}}$ was derived from $\\mathcal{S}(\\mathcal{D}) = \\mathcal{S}(\\mathcal{D}_{-t} \\cup \\mathcal{T}_t)$.\n- Because $\\mathcal{S}$ is a *supervised* operator, it uses the outcome labels $y$ to make its selection. This means that the labels $\\{y_i | (x_i, y_i) \\in \\mathcal{T}_t\\}$ from the test set were used to choose $S_{\\text{global}}$.\n- Consequently, the feature set $S_{\\text{global}}$ has been selected with prior knowledge of the test set's labels. The features are \"pre-screened\" to be predictive of the outcomes in $\\mathcal{T}_t$.\n- This violates the core principle of evaluation. The model trained for fold $t$, $f_t$, is now a function of $S_{\\text{global}}$, which is itself a function of $\\mathcal{T}_t$. Thus, $f_t$ is no longer statistically independent of the test data $\\mathcal{T}_t$.\n- Evaluating $f_t$ on $\\mathcal{T}_t$ will yield an optimistically biased (i.e., better than reality) performance estimate. This phenomenon is known as \"information leakage\" or \"data snooping.\" The model has \"snooped\" at the test set answers during the feature selection step.\n\n### Option-by-Option Analysis\n\n**A. Nested CV partitions $\\mathcal{D}$ into $K_{\\mathrm{out}}$ outer folds. For each outer fold $t$, the outer training set $\\mathcal{D}_{-t}$ is further partitioned into $K_{\\mathrm{in}}$ inner folds. All data-dependent operations, including feature selection $\\mathcal{S}$, normalization, and hyperparameter tuning of $f_{\\lambda}$, are fit exclusively on $\\mathcal{D}_{-t}$ using the inner loop to select $\\lambda^{*}$; a final model $f_{\\lambda^{*}}$ is trained on $\\mathcal{D}_{-t}$ and evaluated on the held-out outer test set $\\mathcal{T}_{t}$. The nested estimator $\\widehat{R}_{\\mathrm{nested}} = \\frac{1}{K_{\\mathrm{out}}}\\sum_{t=1}^{K_{\\mathrm{out}}} \\frac{1}{|\\mathcal{T}_{t}|}\\sum_{(x,y)\\in \\mathcal{T}_{t}} L(y, f_{\\lambda^{*}}(x))$ approximates $R(f_{\\lambda^{*}})$ because $f_{\\lambda^{*}}$ and the preprocessing pipeline are measurable functions of $\\mathcal{D}_{-t}$ only, rendering them independent of outcomes in $\\mathcal{T}_{t}$. If $\\mathcal{S}$ is computed once on $\\mathcal{D}$ before splitting, then for each $t$, $f_{\\lambda^{*}}$ depends on $\\mathcal{S}(\\mathcal{D})$, which in turn depends on $\\mathcal{T}_{t}$ through $y$; this breaks the independence between $f_{\\lambda^{*}}$ and $Y$ in $\\mathcal{T}_{t}$ and induces optimistic bias in $\\widehat{R}_{\\mathrm{nested}}$ because test outcomes influenced feature choice, a data snooping effect.**\n- This option provides a complete and accurate description of the correct nested CV procedure. It correctly states that *all* data-dependent steps must be encapsulated within the outer training loop.\n- The justification for why this worksindependence between the model function and the test setis statistically correct.\n- The explanation of the flaw in the colleague's proposal is also perfect. It correctly identifies that using $\\mathcal{S}(\\mathcal{D})$ makes the model building for fold $t$ dependent on the test set $\\mathcal{T}_t$, breaking independence and causing optimistic bias via data snooping.\n- **Verdict: Correct**\n\n**B. Nested CV first uses the inner $K_{\\mathrm{in}}$-fold CV across $\\mathcal{D}$ to choose $\\lambda^{*}$, then uses outer $K_{\\mathrm{out}}$-fold CV (with the same $\\lambda^{*}$) to report performance. Performing $\\mathcal{S}$ on $\\mathcal{D}$ before splitting does not leak information because the same set of features is applied uniformly to both training and test splits, ensuring consistency.**\n- The description of nested CV is incorrect. It confuses the order and purpose of the loops. Tuning $\\lambda^*$ on $\\mathcal{D}$ and then evaluating on folds of $\\mathcal{D}$ is itself a flawed procedure that leaks information.\n- The claim that performing supervised $\\mathcal{S}$ on $\\mathcal{D}$ does not leak information is fundamentally false. The \"consistency\" of applying the same features does not mitigate the bias introduced by selecting them using test set labels.\n- **Verdict: Incorrect**\n\n**C. Nested CV draws $K_{\\mathrm{out}}$ outer folds and reuses those exact fold indices inside the inner loop to avoid distributional changes between inner and outer splits. Leakage only occurs when the hyperparameter tuning procedure directly accesses the outer test labels $y$; global supervised $\\mathcal{S}$ applied prior to splitting does not leak if the inner loop later re-optimizes $\\lambda$.**\n- The description of reusing fold indices is conceptually flawed and not how nested CV is performed. The inner loop partitions the outer training set.\n- The claim that leakage *only* occurs if the hyperparameter tuning sees the test labels is dangerously wrong. It incorrectly exonerates supervised feature selection performed on the full dataset, which is a classic and severe form of information leakage. Re-optimizing $\\lambda$ within a biased feature space does not correct the bias.\n- **Verdict: Incorrect**\n\n**D. Nested CV requires that feature selection be performed globally on the entire dataset to reduce variance of the evaluator, then the inner loop tunes $\\lambda$ within the selected feature space using resampling from the same folds as the outer loop. Leakage arises in multi-omics primarily when the omics modalities are highly correlated, not from pre-splitting $\\mathcal{S}$ itself.**\n- This statement advocates for the flawed procedure. While global feature selection may reduce variance *of the feature set*, it introduces a substantial optimistic *bias* in the performance estimate, which is a more critical error. The primary goal of nested CV is to obtain a low-bias estimate.\n- The claim that leakage is primarily from inter-omic correlation and not from the pre-splitting selection step is a misdirection. The leakage is a direct consequence of the evaluation methodology, not the data's inherent structure.\n- **Verdict: Incorrect**\n\n**E. Nested CV for multi-omics early integration concatenates $x^{(\\mathrm{RNA})}$ and $x^{(\\mathrm{Prot})}$ first, then performs a single $K$-fold CV for tuning and evaluation simultaneously on the same partitions. This describes standard (non-nested) CV, not nested CV. Using the same partitions for tuning and evaluation leads to an optimistic performance estimate.**\n- The claim that early integration or uniform treatment of features prevents leakage from global supervised selection is false. The leakage is caused by using the outcome variable $y$ from the whole dataset $\\mathcal{D}$ to select features, regardless of how those features were constructed or combined.\n- **Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}