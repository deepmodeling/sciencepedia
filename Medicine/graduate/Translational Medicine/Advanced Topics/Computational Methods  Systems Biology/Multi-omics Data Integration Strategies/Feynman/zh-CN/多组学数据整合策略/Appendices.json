{
    "hands_on_practices": [
        {
            "introduction": "在任何多组学项目中，首要步骤是确保数据的完整性。一个常见且关键的问题是样本标签错误，即来自不同组学的数据文件被错误地分配给了同一个病人。本实践旨在解决跨不同组学数据集验证样本同一性的关键任务。您将基于群体遗传学原理，通过评估基因型的一致性，实现一个概率模型，以判断全基因组测序（WGS）和RNA测序（RNA-seq）数据是否真正来源于同一个体 。这项动手练习强调了在进行任何整合分析之前，进行严格的前期质量控制的重要性。",
            "id": "5034034",
            "problem": "您的任务是设计并实现一个程序化决策规则，通过评估全基因组测序（WGS）和RNA测序（RNA-seq）变异检出之间的基因型一致性，来解决患者标识符不匹配的问题。您必须从群体遗传学和概率推断的基础原理出发，不得使用简化的快捷方式。生成模型如下。\n\n基本原理：\n- 假设Hardy–Weinberg平衡。对于一个次要等位基因频率为 $p$ 的双等位单核苷酸变异，其真实的二倍体基因型 $t \\in \\{0,1,2\\}$（次要等位基因的数量）分布如下：$P(t=0)=(1-p)^2$，$P(t=1)=2p(1-p)$，$P(t=2)=p^2$。\n- 对于每种测序方法，假设一个对称的基因分型错误模型：给定真实基因型 $t$，观察到的基因型 $g \\in \\{0,1,2\\}$ 等于 $t$ 的概率为 $1-e$，否则为其他两种基因型之一的概率各为 $e/2$，其中 $e$ 是该测序方法特有的错误率。将WGS的错误率表示为 $e_w$，RNA-seq的错误率表示为 $e_r$。\n- 在不同变异位点之间，以其真实基因型为条件，观测结果是相互独立的。\n\n匹配与不匹配假设：\n- 在匹配假设 $\\mathcal{M}$ 下，给定变异位点的WGS和RNA-seq观测值来自于同一个体，共享同一个潜在的真实基因型 $t$；以 $t$ 为条件，这两个观测到的基因型是源于同一个 $t$ 的独立生成事件，各自具有相应的错误率。\n- 在不匹配假设 $\\mathcal{U}$ 下，WGS和RNA-seq的观测值来自于从同一群体中抽样的独立个体；等价地，它们的真实基因型是从给定 $p$ 的Hardy–Weinberg分布中独立抽取的，并且每个观测值是根据其自身的真实基因型和相应的测序错误率生成的。\n\n决策框架：\n- 设匹配假设的先验概率为 $\\pi_{\\mathcal{M}} \\in (0,1)$，因此先验几率为 $O_0 = \\pi_{\\mathcal{M}}/(1-\\pi_{\\mathcal{M}})$。\n- 对于一组变异，定义总对数似然比 $\\Lambda$ 为所有变异位点的对数似然比之和。每个位点的对数似然比是基于上述模型，在 $\\mathcal{M}$ 与 $\\mathcal{U}$ 假设下观察到该基因型对的似然比。\n- 后验几率为 $O_{\\text{post}} = O_0 \\cdot \\exp(\\Lambda)$，匹配的后验概率为 $\\pi_{\\text{post}} = O_{\\text{post}}/(1+O_{\\text{post}})$。\n- 通过要求 $\\pi_{\\text{post}} \\ge \\tau$ 来设定一个一致性阈值，其中 $\\tau \\in (0,1)$ 是一个指定的目标水平。\n\n您的程序必须：\n1. 对于每个测试用例，使用上述生成性定义从第一性原理计算总对数似然比 $\\Lambda$，然后计算后验概率 $\\pi_{\\text{post}}$，最后输出一个布尔决策，指明是否 $\\pi_{\\text{post}} \\ge \\tau$。\n2. 在所有对数计算中使用自然对数（以 $e$ 为底）。\n3. 将所有浮点数输出四舍五入到 $6$ 位小数。\n\n测试套件：\n- 案例A（高一致性的“理想路径”）：$G_w=[0,1,2,1,0,1,2,0]$, $G_r=[0,1,2,1,0,1,1,0]$, $p=[0.05,0.10,0.20,0.15,0.30,0.10,0.25,0.05]$, $e_w=0.01$, $e_r=0.05$, $\\pi_{\\mathcal{M}}=0.5$, $\\tau=0.95$。\n- 案例B（明显不匹配）：$G_w=[0,0,1,2,1,0,2,1,0,2]$, $G_r=[2,1,0,0,2,1,0,1,2,0]$, $p=[0.20,0.40,0.30,0.10,0.25,0.35,0.15,0.05,0.50,0.45]$, $e_w=0.01$, $e_r=0.05$, $\\pi_{\\mathcal{M}}=0.5$, $\\tau=0.95$。\n- 案例C（边界条件，变异少，错误率中等）：$G_w=[0,1,1,2]$, $G_r=[0,1,0,2]$, $p=[0.30,0.10,0.10,0.40]$, $e_w=0.02$, $e_r=0.08$, $\\pi_{\\mathcal{M}}=0.5$, $\\tau=0.90$。\n\n计算定义：\n- 对于每个变异索引 $i$，令 $p_i$ 表示次要等位基因频率，$g_{w,i}$ 表示WGS基因型，$g_{r,i}$ 表示RNA-seq基因型。\n- 定义 $P_{\\mathcal{M},i}$ 为在 $\\mathcal{M}$ 假设下观测到 $(g_{w,i}, g_{r,i})$ 的概率，该概率通过使用Hardy–Weinberg先验和生成模型对单一真实基因型进行边缘化得到。\n- 定义 $P_{\\mathcal{U},i}$ 为在 $\\mathcal{U}$ 假设下观测到 $(g_{w,i}, g_{r,i})$ 的概率，该概率通过将每个观测基因型在Hardy–Weinberg先验和各自测序方法的生成模型下的边际概率相乘得到。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表。对于每个测试用例，附加总对数似然比 $\\Lambda$（四舍五入到 $6$ 位小数），后跟阈值的布尔决策（其中 $True$ 表示 $\\pi_{\\text{post}} \\ge \\tau$）。因此，总输出应采用 $[\\Lambda_A,\\text{decision}_A,\\Lambda_B,\\text{decision}_B,\\Lambda_C,\\text{decision}_C]$ 的形式，不含任何额外文本。",
            "solution": "该问题要求设计并实现一个程序化决策规则，通过比较全基因组测序（WGS）和RNA测序（RNA-seq）数据来确定患者样本的一致性。该解决方案基于一个贝叶斯框架，用于评估两个样本来自同一个体（$\\mathcal{M}$，匹配假设）与来自两个不同个体（$\\mathcal{U}$，不匹配假设）的后验概率。实现将完全基于所提供的群体遗传学和概率建模的第一性原理。\n\n解决方案的核心是计算一组遗传变异的总对数似然比 $\\Lambda$。该量度衡量了观测到的基因型数据支持匹配假设相对于不匹配假设的证据权重。\n\n**1. 生成模型规范**\n\n概率模型由三个部分定义：\n\n- **Hardy-Weinberg平衡（HWE）先验：** 对于一个给定的次要等位基因频率为 $p$ 的双等位变异，其真实二倍体基因型 $t \\in \\{0, 1, 2\\}$（表示次要等位基因的数量）从一个三项分布中抽取，概率如下：\n  $$P(t=0) = (1-p)^2$$\n  $$P(t=1) = 2p(1-p)$$\n  $$P(t=2) = p^2$$\n\n- **对称基因分型错误模型：** 在给定真实基因型 $t$ 的情况下，观测到基因型 $g \\in \\{0, 1, 2\\}$ 的概率由特定于测序方法的错误率 $e$ 定义。条件概率 $P(g|t, e)$ 为：\n  $$P(g|t, e) = \\begin{cases} 1-e  \\text{if } g = t \\\\ e/2  \\text{if } g \\neq t \\end{cases}$$\n  我们将WGS和RNA-seq的错误率分别表示为 $e_w$ 和 $e_r$。\n\n- **假设条件下的似然：** 对于单个变异，观测到的WGS基因型 $g_w$ 和RNA-seq基因型 $g_r$ 在两种假设下具有不同的联合概率：\n  - **匹配假设 ($\\mathcal{M}$):** 基因型 $g_w$ 和 $g_r$ 来源于一个单一、共享的真实基因型 $t$。在以 $t$ 为条件下，观测是独立的。\n  - **不匹配假设 ($\\mathcal{U}$):** 基因型 $g_w$ 和 $g_r$ 来源于两个独立的真实基因型 $t_w$ 和 $t_r$，每个都从HWE分布中抽取。\n\n**2. 每个变异似然的推导**\n\n对于每个次要等位基因频率为 $p_i$ 且观测基因型为 $(g_{w,i}, g_{r,i})$ 的变异 $i$，我们推导在每种假设下的似然。为清晰起见，在下面的推导中省略了下标 $i$。\n\n- **$\\mathcal{M}$ 假设下的似然：** 概率 $P_{\\mathcal{M}}(g_w, g_r)$ 是通过对单一潜在真实基因型 $t$ 进行边缘化得到的：\n  $$P_{\\mathcal{M}}(g_w, g_r) = \\sum_{t=0}^{2} P(g_w, g_r, t | \\mathcal{M})$$\n  通过应用链式法则以及在给定 $t$ 时 $g_w$ 和 $g_r$ 的条件独立性：\n  $$P_{\\mathcal{M}}(g_w, g_r) = \\sum_{t=0}^{2} P(g_w | t, e_w) P(g_r | t, e_r) P(t)$$\n  其中 $P(t)$ 是给定MAF $p$ 的HWE先验概率。\n\n- **$\\mathcal{U}$ 假设下的似然：** 概率 $P_{\\mathcal{U}}(g_w, g_r)$ 是每个观测值的边际概率的乘积，这是由于该假设下的独立性假定：\n  $$P_{\\mathcal{U}}(g_w, g_r) = P(g_w | \\mathcal{U}) P(g_r | \\mathcal{U})$$\n  每个边际概率是通过对其自身的潜在真实基因型（$t_w$ 或 $t_r$）进行积分计算的，该基因型是从HWE分布中抽取的：\n  $$P(g_w | \\mathcal{U}) = \\sum_{t_w=0}^{2} P(g_w | t_w, e_w) P(t_w)$$\n  $$P(g_r | \\mathcal{U}) = \\sum_{t_r=0}^{2} P(g_r | t_r, e_r) P(t_r)$$\n  因此，\n  $$P_{\\mathcal{U}}(g_w, g_r) = \\left( \\sum_{t_w=0}^{2} P(g_w | t_w, e_w) P(t_w) \\right) \\left( \\sum_{t_r=0}^{2} P(g_r | t_r, e_r) P(t_r) \\right)$$\n\n**3. 贝叶斯推断与决策框架**\n\n决策过程使用贝叶斯更新来聚合所有变异的证据。\n\n- **对数似然比：** 对于每个变异 $i$，我们计算对数似然比（使用自然对数 $\\log$）：\n  $$\\lambda_i = \\log \\left( \\frac{P_{\\mathcal{M},i}(g_{w,i}, g_{r,i})}{P_{\\mathcal{U},i}(g_{w,i}, g_{r,i})} \\right)$$\n  总对数似然比 $\\Lambda$ 是所有变异的总和，假设它们相互独立：\n  $$\\Lambda = \\sum_i \\lambda_i$$\n\n- **后验概率计算：** 匹配的先验信念由 $\\pi_{\\mathcal{M}}$ 给出，对应于先验几率 $O_0 = \\pi_{\\mathcal{M}} / (1 - \\pi_{\\mathcal{M}})$。来自数据的证据（体现在似然比 $\\exp(\\Lambda)$ 中）将先验几率更新为后验几率：\n  $$O_{\\text{post}} = O_0 \\cdot \\exp(\\Lambda)$$\n  然后将后验几率转换回匹配的后验概率 $\\pi_{\\text{post}}$：\n  $$\\pi_{\\text{post}} = \\frac{O_{\\text{post}}}{1 + O_{\\text{post}}}$$\n\n- **决策规则：** 如果后验概率达到或超过指定的置信度阈值 $\\tau$，则判定为匹配：\n  $$\\text{Decision} = (\\pi_{\\text{post}} \\ge \\tau)$$\n\n程序将为每个测试用例实现这些计算，得出总对数似然比 $\\Lambda$ 和最终的布尔决策。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the genotype concordance problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"Gw\": [0, 1, 2, 1, 0, 1, 2, 0],\n            \"Gr\": [0, 1, 2, 1, 0, 1, 1, 0],\n            \"p\": [0.05, 0.10, 0.20, 0.15, 0.30, 0.10, 0.25, 0.05],\n            \"ew\": 0.01,\n            \"er\": 0.05,\n            \"pi_M\": 0.5,\n            \"tau\": 0.95,\n        },\n        {\n            \"id\": \"B\",\n            \"Gw\": [0, 0, 1, 2, 1, 0, 2, 1, 0, 2],\n            \"Gr\": [2, 1, 0, 0, 2, 1, 0, 1, 2, 0],\n            \"p\": [0.20, 0.40, 0.30, 0.10, 0.25, 0.35, 0.15, 0.05, 0.50, 0.45],\n            \"ew\": 0.01,\n            \"er\": 0.05,\n            \"pi_M\": 0.5,\n            \"tau\": 0.95,\n        },\n        {\n            \"id\": \"C\",\n            \"Gw\": [0, 1, 1, 2],\n            \"Gr\": [0, 1, 0, 2],\n            \"p\": [0.30, 0.10, 0.10, 0.40],\n            \"ew\": 0.02,\n            \"er\": 0.08,\n            \"pi_M\": 0.5,\n            \"tau\": 0.90,\n        },\n    ]\n\n    results = []\n    \n    def get_p_g_given_t(obs_g, true_t, error_rate):\n        \"\"\"Calculates P(g|t) based on the symmetric error model.\"\"\"\n        if obs_g == true_t:\n            return 1.0 - error_rate\n        else:\n            return error_rate / 2.0\n\n    for case in test_cases:\n        Gw = case[\"Gw\"]\n        Gr = case[\"Gr\"]\n        p_list = case[\"p\"]\n        ew = case[\"ew\"]\n        er = case[\"er\"]\n        pi_M = case[\"pi_M\"]\n        tau = case[\"tau\"]\n\n        total_log_likelihood_ratio = 0.0\n\n        for gw, gr, p in zip(Gw, Gr, p_list):\n            # Hardy-Weinberg prior probabilities for t in {0, 1, 2}\n            p_t_vec = np.array([(1-p)**2, 2*p*(1-p), p**2])\n            \n            p_M = 0.0\n            p_gw_marginal = 0.0\n            p_gr_marginal = 0.0\n\n            for t_idx in range(3):\n                # P(g_w | t) and P(g_r | t)\n                p_gw_given_t = get_p_g_given_t(gw, t_idx, ew)\n                p_gr_given_t = get_p_g_given_t(gr, t_idx, er)\n                \n                # Contribution to P_M (match hypothesis)\n                p_M += p_gw_given_t * p_gr_given_t * p_t_vec[t_idx]\n                \n                # Contribution to marginals for P_U (mismatch hypothesis)\n                p_gw_marginal += p_gw_given_t * p_t_vec[t_idx]\n                p_gr_marginal += p_gr_given_t * p_t_vec[t_idx]\n\n            p_U = p_gw_marginal * p_gr_marginal\n\n            variant_llr = np.log(p_M / p_U)\n            total_log_likelihood_ratio += variant_llr\n\n        # Calculate posterior probability\n        prior_odds = pi_M / (1.0 - pi_M)\n        posterior_odds = prior_odds * np.exp(total_log_likelihood_ratio)\n        posterior_prob = posterior_odds / (1.0 + posterior_odds)\n\n        # Make decision\n        decision = posterior_prob >= tau\n\n        # Append results\n        results.append(f\"{total_log_likelihood_ratio:.6f}\")\n        results.append(str(decision))\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在确认样本身份无误后，下一个质量控制关卡是检测每个组学层中的技术异常样本。本练习将指导您构建一个QC流程，该流程使用主成分分析（PCA）的杠杆分数和稳健的统计指标，系统地识别跨多个组学数据集的异常样本 。掌握这项技能对于确保多组学整合分析结果的可靠性和有效性至关重要。",
            "id": "5033985",
            "problem": "您的任务是为转化医学中的多组学整合构建一个质量控制流程，该流程使用主成分分析 (PCA) 的杠杆分数和应用于关键质量指标的中位数绝对偏差 (MAD) 阈值来检测跨组学的异常样本。该流程必须实现为一个完整、可运行的程序，不接受任何输入，并产生单行输出。程序必须为提供的测试套件计算异常样本的索引。\n\n使用的基础和定义：\n- 令组学数据矩阵表示为 $X \\in \\mathbb{R}^{n \\times p}$，其中有 $n$ 个样本（行）和 $p$ 个特征（列）。在进行后续计算之前， $X$ 的每一列都必须标准化为零均值和单位方差。\n- 主成分分析 (PCA) 可以通过奇异值分解 (SVD) 定义，$X = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{p \\times r}$ 具有标准正交列，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是具有非负奇异值的对角矩阵，且 $r = \\operatorname{rank}(X)$。\n- 对于选定的主成分数 $k$（$k \\leq p$），定义到前 $k$ 个左奇异子空间的投影由 $U_k \\in \\mathbb{R}^{n \\times k}$ 表示，如果 $r \\geq k$，则 $U_k$ 是 $U$ 的前 $k$ 列，否则为 $U_k \\in \\mathbb{R}^{n \\times r}$。样本 $i$ 的 PCA 杠杆值是到该子空间的投影矩阵的对角元素，$h_i = \\sum_{j=1}^{\\min(k, r)} U_{ij}^2$。\n- 投影矩阵 $H_k = U_k U_k^\\top$ 是对称且幂等的，并满足 $\\operatorname{trace}(H_k) = \\min(k, r)$。因此，平均杠杆值等于 $\\operatorname{trace}(H_k)/n = \\min(k, r)/n$。基于杠杆值的异常值可以通过将 $h_i$ 与阈值 $\\alpha \\cdot \\min(k, r)/n$ 进行比较来定义，其中放大因子 $\\alpha > 1$。\n- 对于一个质量指标向量 $q \\in \\mathbb{R}^n$，定义 $\\operatorname{median}(q)$，中位数绝对偏差 $d_i = |q_i - \\operatorname{median}(q)|$，$\\operatorname{MAD}(q) = \\operatorname{median}(d)$，以及稳健缩放偏差 $z_i = d_i / (c \\cdot \\operatorname{MAD}(q))$，其中 $c = 1.4826$（正态性下的一致性常数）。基于指标的异常值是任何满足 $z_i$ 超过阈值 $t$ 的样本 $i$。\n- 跨组学整合：给定 $O$ 个组学矩阵 $\\{X^{(o)}\\}_{o=1}^O$，为每个样本 $i$ 定义跨组学的杠杆值超标计数 $C^{\\text{lev}}_i$ 和跨所提供指标的指标超标计数 $C^{\\text{met}}_i$。如果对于给定的整数阈值 $\\theta$ 和 $\\phi$，满足 $C^{\\text{lev}}_i \\geq \\theta$ 或 $C^{\\text{met}}_i \\geq \\phi$ 中任一条件，则样本 $i$ 被标记为整合异常值。\n\n实现要求：\n- 按列将每个组学矩阵标准化为零均值和单位方差；如果某一列的方差为零，则将其标准化值全部视为零。\n- 使用上述基于 SVD 的定义，通过 $U_k$ 和 $h_i$ 计算 PCA 杠杆值。\n- 使用基于 MAD 的稳健偏差计算质量指标异常值。如果 $\\operatorname{MAD}(q) = 0$，则该指标没有异常值，除非存在任何 $q_i \\neq \\operatorname{median}(q)$；在后一种情况下，将那些 $q_i \\neq \\operatorname{median}(q)$ 的样本视为异常值。\n- 报告异常样本索引时使用基于 0 的索引。\n\n测试套件：\n- 测试用例 1（一般多组学情况，在一个组学和指标中存在明显异常值）：\n    - 样本数：$n = 6$；组学数：$O = 3$；主成分数 $k = 2$；杠杆放大因子 $\\alpha = 2.0$；杠杆计数阈值 $\\theta = 1$；指标阈值 $t = 3.0$；指标计数阈值 $\\phi = 1$。\n    - 基因组学矩阵 $X^{(1)}$ ($6 \\times 4$):\n      $\n      \\begin{bmatrix}\n      0.5  1.0  -0.3  0.2 \\\\\n      0.6  0.8  -0.1  0.0 \\\\\n      0.4  1.1  -0.2  0.1 \\\\\n      0.5  0.9  -0.3  0.2 \\\\\n      0.6  1.0  -0.2  0.3 \\\\\n      0.5  0.95  -0.25  0.15\n      \\end{bmatrix}\n      $\n    - 转录组学矩阵 $X^{(2)}$ ($6 \\times 5$):\n      $\n      \\begin{bmatrix}\n      10  12  9  11  10 \\\\\n      11  12  9  10  11 \\\\\n      10  11  10  11  10 \\\\\n      10  12  9  11  10 \\\\\n      11  11  9  10  12 \\\\\n      10  12  10  11  11\n      \\end{bmatrix}\n      $\n    - 蛋白质组学矩阵 $X^{(3)}$ ($6 \\times 3$):\n      $\n      \\begin{bmatrix}\n      0  0  1 \\\\\n      0  0  1 \\\\\n      0  0  1 \\\\\n      10  10  1 \\\\\n      0  0  1 \\\\\n      0  0  1\n      \\end{bmatrix}\n      $\n    - 质量指标：\n      - 指标 1（例如，RNA 质量）：$[8.0,\\, 8.2,\\, 7.9,\\, 3.0,\\, 8.1,\\, 8.0]$\n      - 指标 2（例如，文库大小）：$[5{,}000{,}000,\\, 5{,}100{,}000,\\, 4{,}900{,}000,\\, 1{,}000{,}000,\\, 5{,}200{,}000,\\, 5{,}000{,}000]$\n    - 预期行为：一个整合异常值，对应于蛋白质组学样本和指标中的异常。\n- 测试用例 2（边界条件：零方差特征和无指标偏差）：\n    - $n = 4$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 1$, $t = 3.0$, $\\phi = 1$。\n    - 组学矩阵 $X^{(1)} = X^{(2)} = \\begin{bmatrix}1  2  3 \\\\ 1  2  3 \\\\ 1  2  3 \\\\ 1  2  3 \\end{bmatrix}$\n    - 质量指标：\n      - 指标 1：$[10,\\, 10,\\, 10,\\, 10]$\n      - 指标 2：$[100,\\, 100,\\, 100,\\, 100]$\n    - 预期行为：无整合异常值。\n- 测试用例 3（边缘情况：仅通过指标产生异常值）：\n    - $n = 5$, $O = 2$, $k = 2$, $\\alpha = 2.0$, $\\theta = 2$, $t = 3.0$, $\\phi = 2$。\n    - 组学矩阵：\n      - $X^{(1)} = \\begin{bmatrix} 1  2 \\\\ 1  2 \\\\ 1.1  1.9 \\\\ 0.9  2.1 \\\\ 1  2 \\end{bmatrix}$\n      - $X^{(2)} = \\begin{bmatrix} 5  5 \\\\ 5  5 \\\\ 5.1  4.9 \\\\ 4.9  5.1 \\\\ 5  5 \\end{bmatrix}$\n    - 质量指标：\n      - 指标 1：$[0.1,\\, 5.0,\\, 0.2,\\, 0.1,\\, 0.0]$\n      - 指标 2：$[100,\\, 1000,\\, 105,\\, 98,\\, 97]$\n    - 预期行为：通过指标产生一个整合异常值（第二个样本）。\n输出规格：\n- 您的程序应生成单行输出，其中包含结果，格式为方括号内以逗号分隔的列表。每个结果都必须是对应测试用例中被标记为整合异常值的样本的基于 0 的索引列表。例如，形式为 $[[i\\_1, i\\_2],[\\,], [j]]$ 的输出是有效的。最终输出必须在一行上精确打印为单个 Python 列表字面量的列表，不得包含任何额外文本。",
            "solution": "问题陈述已经过验证，被认为是有效的。它在科学上基于已建立的统计方法（主成分分析杠杆值，中位数绝对偏差），通过确定性算法进行了良好定义，语言客观，并且在数据和参数的规范方面是完整的。该问题是转化医学中多组学数据质量控制领域一个可形式化且相关的任务。不存在矛盾、不违反科学原理，也没有会妨碍产生唯一、可验证解的歧义。\n\n以下是一个完整的、附带推理的解决方案。\n\n### **算法流程**\n\n该质量控制流程通过整合两种不同类型的统计度量来识别异常样本，这些度量跨越多个组学数据集和质量指标向量：基于 PCA 的杠杆分数和基于 MAD 的稳健偏差。针对给定测试用例的流程如下。\n\n**1. 初始化**\n设 $n$ 为样本数量。初始化两个整数向量用于存储每个样本的异常值计数：\n- $C^{\\text{lev}} \\in \\mathbb{Z}^n$，初始化为零，用于统计基于杠杆值的异常标志。\n- $C^{\\text{met}} \\in \\mathbb{Z}^n$，初始化为零，用于统计基于指标的异常标志。\n\n**2. 基于杠杆值的异常检测（每个组学数据集）**\n对于 $O$ 个组学数据矩阵中的每一个 $X^{(o)} \\in \\mathbb{R}^{n \\times p}$，其中 $o \\in \\{1, \\dots, O\\}$：\n\n**a. 标准化：**\n$X^{(o)}$ 的每一列 $j$ 都被标准化为均值为 $0$、标准差为 $1$。设 $X^{(o)}_{:,j}$ 为第 $j$ 列。计算其均值 $\\mu_j$ 和标准差 $\\sigma_j$。标准化后的列 $X'^{(o)}_{:,j}$ 由下式给出：\n$$\nX'^{(o)}_{i,j} = \\begin{cases} (X^{(o)}_{i,j} - \\mu_j) / \\sigma_j  \\text{if } \\sigma_j > 0 \\\\ 0  \\text{if } \\sigma_j = 0 \\end{cases}\n$$\n这就创建了标准化矩阵 $X'^{(o)}$。\n\n**b. 奇异值分解 (SVD)：**\n计算标准化矩阵的 SVD：$X'^{(o)} = U^{(o)} \\Sigma^{(o)} (V^{(o)})^\\top$。这里，$U^{(o)} \\in \\mathbb{R}^{n \\times n}$ 是左奇异向量矩阵。\n\n**c. 秩和主成分选择：**\n确定矩阵的秩，$r^{(o)} = \\operatorname{rank}(X'^{(o)})$。这对应于非零奇异值的数量。要考虑的主成分数量为 $k' = \\min(k, r^{(o)})$，其中 $k$ 是用户指定的主成分数。我们将 $U_{k'}^{(o)}$ 定义为包含 $U^{(o)}$ 前 $k'$ 列的矩阵。\n\n**d. 杠杆值计算：**\n对于每个样本 $i \\in \\{1, \\dots, n\\}$，其杠杆分数 $h_i^{(o)}$ 是 $U_{k'}^{(o)}$ 第 $i$ 行元素平方和：\n$$\nh_i^{(o)} = \\sum_{j=1}^{k'} (U^{(o)}_{ij})^2\n$$\n\n**e. 异常值标记：**\n基于平均杠杆值计算杠杆阈值 $\\tau_{\\text{lev}}^{(o)}$：\n$$\n\\tau_{\\text{lev}}^{(o)} = \\frac{\\alpha \\cdot k'}{n}\n$$\n其中 $\\alpha$ 是给定的放大因子。如果一个样本的杠杆值 $h_i^{(o)}$ 超过此阈值，则其杠杆异常计数 $C^{\\text{lev}}_i$ 增加：\n$$\n\\text{if } h_i^{(o)} > \\tau_{\\text{lev}}^{(o)}, \\text{ then } C^{\\text{lev}}_i \\leftarrow C^{\\text{lev}}_i + 1\n$$\n\n**3. 基于指标的异常检测（每个质量指标）**\n对于每个提供的质量指标向量 $q \\in \\mathbb{R}^n$：\n\n**a. 中位数和中位数绝对偏差 (MAD)：**\n计算指标的中位数 $m = \\operatorname{median}(q)$。计算所有样本与中位数的绝对偏差 $d_i = |q_i - m|$。MAD 是这些绝对偏差的中位数：$\\operatorname{MAD}(q) = \\operatorname{median}(d)$。\n\n**b. 异常值标记：**\n标记异常值的方法取决于 $\\operatorname{MAD}(q)$ 的值：\n- 如果 $\\operatorname{MAD}(q) > 0$：为每个样本计算稳健缩放偏差 $z_i$：\n  $$\n  z_i = \\frac{d_i}{c \\cdot \\operatorname{MAD}(q)} = \\frac{|q_i - \\operatorname{median}(q)|}{1.4826 \\cdot \\operatorname{MAD}(q)}\n  $$\n  如果 $z_i$ 超过指标阈值 $t$，则该样本的指标异常计数 $C^{\\text{met}}_i$ 增加。\n- 如果 $\\operatorname{MAD}(q) = 0$：任何指标值 $q_i$ 不等于中位数 $m$ 的样本 $i$ 都被视为异常值。对于每个此类样本，$C^{\\text{met}}_i$ 增加。如果所有的 $q_i$ 都等于中位数，则此指标不标记任何异常值。\n\n**4. 整合异常值识别**\n处理完所有组学矩阵和质量指标后，通过将每个样本 $i$ 的累积计数 $C^{\\text{lev}}_i$ 和 $C^{\\text{met}}_i$ 与各自的整数阈值 $\\theta$ 和 $\\phi$ 进行比较，来确定其最终的异常状态：\n$$\n\\text{样本 } i \\text{ 是整合异常值，如果 } (C^{\\text{lev}}_i \\geq \\theta) \\lor (C^{\\text{met}}_i \\geq \\phi)\n$$\n满足此条件的所有样本的基于 0 的索引被收集起来，作为该测试用例的最终结果。\n\n---\n### **应用于测试用例**\n\n**测试用例 1**\n- 参数：$n=6, O=3, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$。\n- **杠杆值：**\n  - 对于 $X^{(1)}$ 和 $X^{(2)}$，数据点相对同质。标准化后，SVD 产生的杠杆分数中没有单个样本占主导地位。秩分别为 $r^{(1)}=4$ 和 $r^{(2)}=5$。则 $k' = \\min(2,4)=2$ 和 $k'=\\min(2,5)=2$。阈值为 $\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 6 \\approx 0.667$ 和 $\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 6 \\approx 0.667$。所有计算出的 $h_i^{(1)}$ 和 $h_i^{(2)}$ 值都低于此阈值。\n  - 对于 $X^{(3)}$，第 4 个样本（索引 3）是异常的：$[10, 10, 1]$。标准化后，矩阵的秩为 $r^{(3)}=1$。因此，$k'=\\min(2,1)=1$。杠杆阈值为 $\\tau_{\\text{lev}}^{(3)} = 2.0 \\cdot 1 / 6 \\approx 0.333$。杠杆分数约为 $h^{(3)} = [0.038, 0.038, 0.038, 0.769, 0.038, 0.038]$。只有 $h_3^{(3)} \\approx 0.769 > 0.333$。\n  - 杠杆计数为 $C^{\\text{lev}} = [0, 0, 0, 1, 0, 0]$。\n- **指标：**\n  - 指标 1：$q = [8.0, 8.2, 7.9, 3.0, 8.1, 8.0]$。中位数为 $8.0$。偏差为 $[0.0, 0.2, 0.1, 5.0, 0.1, 0.0]$。MAD 为 $0.1$。样本 4（索引 3）的 z-score 为 $z_3 = 5.0 / (1.4826 \\cdot 0.1) \\approx 33.72 > 3.0$。\n  - 指标 2：$q = [5\\text{e}6, 5.1\\text{e}6, 4.9\\text{e}6, 1\\text{e}6, 5.2\\text{e}6, 5\\text{e}6]$。中位数为 $5\\text{e}6$。MAD 为 $1\\text{e}5$。样本 4 的 z-score 为 $z_3 = 4\\text{e}6 / (1.4826 \\cdot 1\\text{e}5) \\approx 26.98 > 3.0$。\n  - 指标计数为 $C^{\\text{met}} = [0, 0, 0, 2, 0, 0]$。\n- **整合：**\n  - 对于样本 4（索引 3）：$C^{\\text{lev}}_3=1 \\geq \\theta=1$ 且 $C^{\\text{met}}_3=2 \\geq \\phi=1$。该样本是异常值。\n  - 对于所有其他样本 $i \\neq 3$，$C^{\\text{lev}}_i=0  1$ 且 $C^{\\text{met}}_i=0  1$。它们不是异常值。\n- **结果：** `[3]`\n\n**测试用例 2**\n- 参数：$n=4, O=2, k=2, \\alpha=2.0, \\theta=1, t=3.0, \\phi=1$。\n- **杠杆值：**\n  - $X^{(1)}$ 和 $X^{(2)}$ 都由相同的行组成。每列的标准差为 $0$。\n  - 根据规则，两个标准化矩阵 $X'^{(1)}$ 和 $X'^{(2)}$ 都是零矩阵。\n  - 零矩阵的秩为 $r=0$。因此 $k'=\\min(2, 0)=0$。\n  - 杠杆分数 $h_i$ 全为 $0$，阈值 $\\tau_{\\text{lev}}$ 也为 $0$。条件 $h_i  \\tau_{\\text{lev}}$ 永远不会满足。\n  - 杠杆计数为 $C^{\\text{lev}} = [0, 0, 0, 0]$。\n- **指标：**\n  - 两个指标向量 $[10, 10, 10, 10]$ 和 $[100, 100, 100, 100]$ 都由相同的值组成。\n  - 对于两者，中位数都是该常数值，所有偏差都为 $0$，因此 MAD 也为 $0$。\n  - 适用 $\\operatorname{MAD}=0$ 的特殊情况。由于没有 $q_i$ 与中位数不同，因此不标记异常值。\n  - 指标计数为 $C^{\\text{met}} = [0, 0, 0, 0]$。\n- **整合：**\n  - 对于所有样本，$C^{\\text{lev}}_i=0$ 且 $C^{\\text{met}}_i=0$。条件 $C^{\\text{lev}}_i \\geq 1$ 或 $C^{\\text{met}}_i \\geq 1$ 永远不会满足。\n- **结果：** `[]`\n\n**测试用例 3**\n- 参数：$n=5, O=2, k=2, \\alpha=2.0, \\theta=2, t=3.0, \\phi=2$。\n- **杠杆值：**\n  - 矩阵 $X^{(1)}$ 和 $X^{(2)}$ 由非常相似的样本组成。\n  - 对于 $X^{(1)}$，$r^{(1)}=2, k'=2$。$\\tau_{\\text{lev}}^{(1)} = 2.0 \\cdot 2 / 5 = 0.8$。所有的 $h_i^{(1)}$ 都低于此阈值。\n  - 对于 $X^{(2)}$，$r^{(2)}=2, k'=2$。$\\tau_{\\text{lev}}^{(2)} = 2.0 \\cdot 2 / 5 = 0.8$。所有的 $h_i^{(2)}$ 都低于此阈值。\n  - 在任何组学数据中都没有样本被标记为杠杆异常值。杠杆计数为 $C^{\\text{lev}} = [0, 0, 0, 0, 0]$。\n- **指标：**\n  - 指标 1：$q = [0.1, 5.0, 0.2, 0.1, 0.0]$。中位数为 $0.1$。MAD 为 $0.1$。样本 2（索引 1）的 z-score 为 $z_1 = 4.9 / (1.4826 \\cdot 0.1) \\approx 33.05  3.0$。样本 2 是一个异常值。\n  - 指标 2：$q = [100, 1000, 105, 98, 97]$。中位数为 $100$。MAD 为 $3$。样本 2 的 z-score 为 $z_1 = 900 / (1.4826 \\cdot 3) \\approx 202.41  3.0$。样本 2 是一个异常值。\n  - 指标计数为 $C^{\\text{met}} = [0, 2, 0, 0, 0]$。\n- **整合：**\n  - 阈值很高：$\\theta=2, \\phi=2$。\n  - 对于样本 2（索引 1）：$C^{\\textlev}_1=0  \\theta=2$，但 $C^{\\text{met}}_1=2 \\geq \\phi=2$。该样本是异常值。\n  - 对于所有其他样本，计数都太低。\n- **结果：** `[1]`",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run the multi-omics outlier detection pipeline on a predefined test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"params\": {\"n\": 6, \"O\": 3, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([\n                    [0.5, 1.0, -0.3, 0.2],\n                    [0.6, 0.8, -0.1, 0.0],\n                    [0.4, 1.1, -0.2, 0.1],\n                    [0.5, 0.9, -0.3, 0.2],\n                    [0.6, 1.0, -0.2, 0.3],\n                    [0.5, 0.95, -0.25, 0.15]\n                ]),\n                np.array([\n                    [10, 12, 9, 11, 10],\n                    [11, 12, 9, 10, 11],\n                    [10, 11, 10, 11, 10],\n                    [10, 12, 9, 11, 10],\n                    [11, 11, 9, 10, 12],\n                    [10, 12, 10, 11, 11]\n                ]),\n                np.array([\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [0, 0, 1],\n                    [10, 10, 1],\n                    [0, 0, 1],\n                    [0, 0, 1]\n                ])\n            ],\n            \"quality_metrics\": [\n                np.array([8.0, 8.2, 7.9, 3.0, 8.1, 8.0]),\n                np.array([5_000_000, 5_100_000, 4_900_000, 1_000_000, 5_200_000, 5_000_000])\n            ]\n        },\n        # Test Case 2\n        {\n            \"params\": {\"n\": 4, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 1, \"t\": 3.0, \"phi\": 1},\n            \"omics_data\": [\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]),\n                np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]])\n            ],\n            \"quality_metrics\": [\n                np.array([10, 10, 10, 10]),\n                np.array([100, 100, 100, 100])\n            ]\n        },\n        # Test Case 3\n        {\n            \"params\": {\"n\": 5, \"O\": 2, \"k\": 2, \"alpha\": 2.0, \"theta\": 2, \"t\": 3.0, \"phi\": 2},\n            \"omics_data\": [\n                np.array([[1, 2], [1, 2], [1.1, 1.9], [0.9, 2.1], [1, 2]]),\n                np.array([[5, 5], [5, 5], [5.1, 4.9], [4.9, 5.1], [5, 5]])\n            ],\n            \"quality_metrics\": [\n                np.array([0.1, 5.0, 0.2, 0.1, 0.0]),\n                np.array([100, 1000, 105, 98, 97])\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        params = case[\"params\"]\n        n = params[\"n\"]\n        k = params[\"k\"]\n        alpha = params[\"alpha\"]\n        theta = params[\"theta\"]\n        t = params[\"t\"]\n        phi = params[\"phi\"]\n        \n        c_lev = np.zeros(n, dtype=int)\n        c_met = np.zeros(n, dtype=int)\n\n        # Leverage-based outlier detection\n        for X in case[\"omics_data\"]:\n            # Standardize matrix\n            mean = np.mean(X, axis=0)\n            std = np.std(X, axis=0)\n            X_std = np.zeros_like(X, dtype=float)\n            for j in range(X.shape[1]):\n                if std[j]  1e-9: # Use tolerance for float comparison\n                    X_std[:, j] = (X[:, j] - mean[j]) / std[j]\n            \n            # SVD and rank\n            try:\n                # Rank is number of singular values greater than a tolerance\n                rank = np.linalg.matrix_rank(X_std)\n                if rank == 0:\n                    continue\n                U, s, Vh = svd(X_std, full_matrices=False)\n            except np.linalg.LinAlgError:\n                continue\n\n            num_components = min(k, rank)\n            if num_components == 0:\n                continue\n\n            # Leverage calculation\n            U_k = U[:, :num_components]\n            h = np.sum(U_k**2, axis=1)\n            \n            # Leverage outlier flagging\n            leverage_threshold = alpha * num_components / n\n            c_lev[h  leverage_threshold] += 1\n\n        # Metric-based outlier detection\n        c_consistency = 1.4826\n        for q in case[\"quality_metrics\"]:\n            median_q = np.median(q)\n            deviations = np.abs(q - median_q)\n            mad_q = np.median(deviations)\n\n            if mad_q  1e-9: # Use tolerance for float comparison\n                z_scores = deviations / (c_consistency * mad_q)\n                c_met[z_scores  t] += 1\n            else:\n                # Special case for MAD == 0\n                c_met[q != median_q] += 1\n        \n        # Integrated outlier identification\n        integrated_outliers = np.where((c_lev = theta) | (c_met = phi))[0].tolist()\n        results.append(integrated_outliers)\n    \n    # Using str() on a list gives a valid Python literal representation.\n    # Joining them with commas and wrapping in [] gives the final format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "手握经过验证的高质量数据，您现在可以开始进行整合分析了。本实践将深入探讨相似性网络融合（SNF），这是一种强大的整合方法，它通过将各类数据各自的病人-病人相似性网络融合成一个单一的、全面的网络来实现数据整合 。通过从基本步骤——包括距离计算、核函数变换和迭代网络融合——开始实现SNF算法，您将对如何综合多组学信息以揭示一致的病患亚群获得深刻的实践理解。",
            "id": "5034014",
            "problem": "实现一个完整、独立的程序，构建一个多组学相似性网络融合 (SNF) 流程，以整合跨多个组学的患者-患者相似性网络，并为每个提供的测试用例返回一个单一的融合亲和矩阵。算法步骤必须源自基本且经过充分检验的原则：基于距离的相似性、高斯核、最近邻图、随机归一化以及带有收敛准则的图上迭代消息传递。您的实现必须严格遵循下述过程和定义，并以指定格式精确地生成输出。\n\n对于每个测试用例，您将获得一组特定组学的患者特征矩阵和标量超参数。假设有 $M$ 个组学（数据视图），每个由一个实值矩阵 $X^{(m)} \\in \\mathbb{R}^{N \\times p_m}$ 表示，其中 $m \\in \\{1,\\dots,M\\}$，$N$ 是患者数量，$p_m$ 是组学 $m$ 中的特征数量。目标是通过迭代消息传递，直到收敛或达到最大迭代次数，生成一个单一的融合亲和矩阵 $F \\in \\mathbb{R}^{N \\times N}$，该矩阵整合了所有组学的相似性。\n\n必须完全按照所述实现的基本定义和必需步骤：\n\n1.  每个组学和每个特征的Z-score标准化：\n    -   对于每个组学 $m$ 和每个特征（列）$j \\in \\{1,\\dots,p_m\\}$，计算患者（行）的平均值 $\\mu_{m,j}$ 和标准差 $\\sigma_{m,j}$。通过 $\\tilde{X}^{(m)}_{i,j} = \\frac{X^{(m)}_{i,j} - \\mu_{m,j}}{\\sigma_{m,j}}$ 将 $X^{(m)}_{:,j}$ 转换为 $\\tilde{X}^{(m)}_{:,j}$。如果 $\\sigma_{m,j} = 0$，则将 $\\tilde{X}^{(m)}_{:,j}$ 设置为零向量。\n    -   其理由是统计学中广泛使用标准化来消除尺度差异，确保特征和组学之间的可比性。\n\n2.  每个组学内的逐对平方欧几里得距离：\n    -   对于每个组学 $m$，计算 $D^{(m)} \\in \\mathbb{R}^{N \\times N}$，其条目为 $D^{(m)}_{ij} = \\sum_{l=1}^{p_m} \\left(\\tilde{X}^{(m)}_{i,l} - \\tilde{X}^{(m)}_{j,l}\\right)^2$。\n\n3.  高斯核相似性：\n    -   给定一个正带宽参数 $\\sigma  0$（每个测试用例提供），通过以下公式定义 $G^{(m)} \\in \\mathbb{R}^{N \\times N}$\n        $$G^{(m)}_{ij} = \\exp\\left( - \\frac{D^{(m)}_{ij}}{2 \\sigma^2} \\right)。$$\n    -   对所有 $i$，设置 $G^{(m)}_{ii} = 1$。\n\n4.  $k$-最近邻稀疏化和对称化：\n    -   设 $k$ 为一个正整数，满足 $1 \\le k \\le N-1$（每个测试用例提供）。对于 $G^{(m)}$ 中的每一行 $i$，识别出 $k$ 个最大的非对角线元素 $G^{(m)}_{ij}$（其中 $j \\neq i$）的索引；如果出现平局，则优先选择较小的索引 $j$。创建一个稀疏矩阵 $S^{(m)}_{\\text{mask}}$，该矩阵仅保留每行的这 $k$ 个邻居（非对角线），并将其余非对角线元素置零。然后进行对称化：\n        $$W^{(m)} = \\frac{S^{(m)}_{\\text{mask}} + \\left(S^{(m)}_{\\text{mask}}\\right)^{\\top}}{2}，$$\n        最后，对所有 $i$，通过设置 $W^{(m)}_{ii} = 1$ 来确定对角线。\n\n5.  行随机归一化：\n    -   通过将行归一化使其和为 $1$，将 $W^{(m)}$ 转换为行随机矩阵 $P^{(m,0)}$：\n        $$P^{(m,0)}_{ij} = \\frac{W^{(m)}_{ij}}{\\sum_{j'=1}^{N} W^{(m)}_{ij'}}。$$\n        如果行和为 $0$，则用所有列上的均匀分布替换该行，即，对所有 $j$，$P^{(m,0)}_{ij} = \\frac{1}{N}$。\n\n6.  固定的邻居约束传播矩阵：\n    -   从 $P^{(m,0)}$ 出发，通过将每行除了相同的 $k$ 个非对角线最近邻（基于 $P^{(m,0)}$ 中的值大小，平局由较小索引打破）之外的所有元素置零，然后将每行归一化使其和为 $1$，来构建固定的传播矩阵 $S^{(m)}$：\n        $$S^{(m)}_{ij} = \\begin{cases}\n        \\frac{P^{(m,0)}_{ij}}{\\sum_{j' \\in \\mathcal{N}^{(m)}_k(i)} P^{(m,0)}_{ij'}}  \\text{若 } j \\in \\mathcal{N}^{(m)}_k(i),\\ j \\neq i,\\\\\n        0  \\text{否则,}\n        \\end{cases}$$\n        其中 $\\mathcal{N}^{(m)}_k(i)$ 是在 $P^{(m,0)}$ 中 $i$ 的 $k$ 个选定非对角线邻居的索引集。如果分母为 $0$，则对所有 $j$ 设置 $S^{(m)}_{ij} = \\frac{1}{N}$。\n\n7.  迭代消息传递（融合）：\n    -   对于 $t = 0,1,2,\\dots$，按如下方式更新每个视图\n        $$P^{(m,t+1)} = S^{(m)} \\left( \\frac{1}{M-1} \\sum_{\\substack{v=1 \\\\ v \\ne m}}^{M} P^{(v,t)} \\right) \\left(S^{(m)}\\right)^{\\top}，$$\n        之后如步骤5所示对 $P^{(m,t+1)}$ 进行行随机归一化。\n    -   对于 $M = 1$ 的情况，跳过此迭代更新，并将融合矩阵定义为步骤5之后的单一视图。\n\n8.  收敛准则与终止：\n    -   将在迭代 $t$ 时的变化定义为\n        $$\\Delta^{(t)} = \\max_{m \\in \\{1,\\dots,M\\}} \\left\\| P^{(m,t+1)} - P^{(m,t)} \\right\\|_F，$$\n        其中 $\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数。给定一个容差 $\\varepsilon  0$ 和一个最大迭代次数 $T_{\\max} \\in \\mathbb{N}$（两者均提供），在第一个满足 $\\Delta^{(t)} \\le \\varepsilon$ 的 $t$ 或当 $t+1 = T_{\\max}$ 时停止迭代，以先发生者为准。\n\n9.  融合矩阵：\n    -   终止后，定义\n        $$F = \\frac{1}{M} \\sum_{m=1}^{M} P^{(m,t_{\\text{final}})}。$$\n        如步骤5所示，将 $F$ 进行行归一化，使其成为行随机矩阵。\n\n10. 每个测试用例的所需输出：\n    -   将 $F$ 以行主序展平成一个包含 $N \\times N$ 个实数的列表。将每个条目四舍五入到恰好 $6$ 位小数（定点表示法，无科学记数法）。\n\n程序输入固定在您的代码中，并且必须包含以下测试套件。对于每个测试用例，您必须使用给定的组学矩阵和超参数 $(k,\\ \\sigma,\\ \\varepsilon,\\ T_{\\max})$ 来实现上述步骤1到9：\n\n-   测试用例 $1$ (正常路径):\n    -   $N = 4$, $M = 2$, $k = 2$, $\\sigma = 1.0$, $\\varepsilon = 1\\times 10^{-6}$, $T_{\\max} = 50$。\n    -   组学 $1$: $X^{(1)} \\in \\mathbb{R}^{4 \\times 2}$，行向量为\n        $[0.0, 1.0]$, $[0.2, 0.9]$, $[3.0, 3.5]$, $[3.2, 3.6]$。\n    -   组学 $2$: $X^{(2)} \\in \\mathbb{R}^{4 \\times 2}$，行向量为\n        $[0.1, 1.1]$, $[0.0, 1.0]$, $[3.1, 3.4]$, $[3.3, 3.7]$。\n\n-   测试用例 $2$ (单组学边缘情况):\n    -   $N = 4$, $M = 1$, $k = 1$, $\\sigma = 0.7$, $\\varepsilon = 1\\times 10^{-6}$, $T_{\\max} = 10$。\n    -   组学 $1$: $X^{(1)} \\in \\mathbb{R}^{4 \\times 3}$，行向量为\n        $[1.0, 0.0, 0.5]$, $[0.9, 0.1, 0.4]$, $[3.0, 3.5, 3.7]$, $[3.1, 3.6, 3.8]$。\n\n-   测试用例 $3$ (通过大容差提前停止):\n    -   $N = 5$, $M = 2$, $k = 2$, $\\sigma = 1.5$, $\\varepsilon = 10^{9}$, $T_{\\max} = 20$。\n    -   组学 $1$: $X^{(1)} \\in \\mathbb{R}^{5 \\times 2}$，行向量为\n        $[0.0, 0.0]$, $[0.1, -0.1]$, $[5.0, 5.0]$, $[5.1, 4.9]$, $[2.5, 2.5]$。\n    -   组学 $2$: $X^{(2)} \\in \\mathbb{R}^{5 \\times 2}$，行向量为\n        $[0.2, -0.2]$, $[0.0, 0.0]$, $[4.9, 5.1]$, $[5.2, 5.0]$, $[2.4, 2.6]$。\n\n-   测试用例 $4$ (迭代次数边界):\n    -   $N = 3$, $M = 3$, $k = 2$, $\\sigma = 1.0$, $\\varepsilon = 1\\times 10^{-12}$, $T_{\\max} = 1$。\n    -   组学 $1$: $X^{(1)} \\in \\mathbb{R}^{3 \\times 2}$，行向量为\n        $[0.0, 0.0]$, $[1.0, 1.0]$, $[2.0, 2.0]$。\n    -   组学 $2$: $X^{(2)} \\in \\mathbb{R}^{3 \\times 2}$，行向量为\n        $[0.0, 0.1]$, $[1.0, 1.1]$, $[1.9, 2.1]$。\n    -   组学 $3$: $X^{(3)} \\in \\mathbb{R}^{3 \\times 3}$，行向量为\n        $[0.0, 1.0, 2.0]$, $[1.0, 2.0, 3.0]$, $[2.0, 3.0, 4.0]$。\n\n您的程序必须遵循上述步骤 $1$ 到 $9$ 计算每个测试用例的融合矩阵 $F$，然后按照步骤 $10$ 中的规定对 $F$ 进行展平和四舍五入。\n\n最终输出格式要求：\n-   您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，$\\big[$result\\_1,result\\_2,result\\_3,result\\_4$\\big]$）。这里的每个结果本身必须是一个包含 $N \\times N$ 个浮点数的列表（展平的融合矩阵），四舍五入到恰好 $6$ 位小数，并以定点表示法打印，不使用科学记数法。不允许有多余的文本或行。",
            "solution": "用户提供了一个有效的问题陈述。它具有科学依据、问题明确、客观且完整。该问题要求实现相似性网络融合（SNF）算法，这是一种生物信息学中用于整合多组学数据的先进方法。该过程基于统计学、数值分析和图论的既定原则。提供的测试用例定义明确，允许对实现进行确定性验证。我现在将进行详细的解答。\n\n解决方案涉及对指定的SNF流程进行分步实现。其核心思想是将每个组学数据集转换为患者-患者相似性网络，然后迭代地融合这些网络，以生成一个能够捕捉跨多种数据类型所增强的相似性的单一、全面的网络。\n\n### 算法设计与原则\n\n实现被设计为一个模块化函数，执行规定的操作序列。\n\n1.  **数据标准化 (Z-score)：**\n    对于每个组学数据类型 $m \\in \\{1,\\dots,M\\}$，由矩阵 $X^{(m)} \\in \\mathbb{R}^{N \\times p_m}$ 表示，我们首先对数据进行标准化。标准化是一个关键的预处理步骤，以确保具有较大数据范围的特征不会对下游的距离计算产生不成比例的影响。对于每个特征（列）$j$，我们计算其在 $N$ 个患者上的平均值 $\\mu_{m,j}$ 和标准差 $\\sigma_{m,j}$。然后根据Z-score公式对每个条目进行转换：\n    $$ \\tilde{X}^{(m)}_{i,j} = \\frac{X^{(m)}_{i,j} - \\mu_{m,j}}{\\sigma_{m,j}} $$\n    在特殊情况下，当一个特征在所有患者中都是恒定的，其标准差 $\\sigma_{m,j}$ 为 $0$。为防止除以零并使该特征无信息（因为它没有方差），根据问题规范，相应的转换后特征向量 $\\tilde{X}^{(m)}_{:,j}$ 被设置为零向量。\n\n2.  **逐对相似性网络构建：**\n    标准化之后，我们将每个组学的基于特征的表示转换为患者-患者相似性网络。这是一个两阶段的过程。首先，对于每个组学 $m$，我们计算逐对平方欧几里得距离矩阵 $D^{(m)} \\in \\mathbb{R}^{N \\times N}$，其中每个元素 $D^{(m)}_{ij}$ 衡量患者 $i$ 和患者 $j$ 在标准化特征空间中的不相似性：\n    $$ D^{(m)}_{ij} = \\sum_{l=1}^{p_m} \\left(\\tilde{X}^{(m)}_{i,l} - \\tilde{X}^{(m)}_{j,l}\\right)^2 $$\n    接下来，使用高斯相似性核（也称为径向基函数（RBF）核）将这些距离转换为亲和度。这创建了亲和矩阵 $G^{(m)} \\in \\mathbb{R}^{N \\times N}$：\n    $$ G^{(m)}_{ij} = \\exp\\left( - \\frac{D^{(m)}_{ij}}{2 \\sigma^2} \\right) $$\n    超参数 $\\sigma$ 是核带宽，它控制亲和度随距离衰减的速度。此函数将距离（从 $0$ 到 $\\infty$）映射到相似度（从 $1$ 到 $0$）。对角线元素 $G^{(m)}_{ii}$ 被明确设置为 $1$，表示最大的自我相似性。\n\n3.  **网络稀疏化与对称化：**\n    真实世界的网络通常是稀疏的。为了反映这一点并通过移除微弱的、可能是伪造的连接来对网络进行去噪，我们只保留每个患者与其 $k$ 个最近邻的连接。对于每个患者 $i$，我们识别出与 $k$ 个最大亲和度值 $G^{(m)}_{ij}$（其中 $j \\neq i$）相对应的其他 $k$ 个患者 $j$。平局通过优先选择较小编号的邻居 $j$ 来解决。这将产生一个有向k-NN图。由于相似性本质上是一个对称概念，该图被对称化以产生一个无向加权邻接矩阵 $W^{(m)}$：\n    $$ W^{(m)} = \\frac{S^{(m)}_{\\text{mask}} + \\left(S^{(m)}_{\\text{mask}}\\right)^{\\top}}{2} $$\n    这里，$S^{(m)}_{\\text{mask}}$ 是一个矩阵，它保留了 $G^{(m)}$ 每行中 $k$ 个最近邻的亲和度值，而在其他位置为零。对角线元素 $W^{(m)}_{ii}$ 被设置为 $1$。\n\n4.  **随机归一化与传播矩阵构建：**\n    亲和矩阵 $W^{(m)}$ 被转换为行随机矩阵 $P^{(m,0)}$，其中每行之和为 $1$。这是通过将每个元素除以其行和来实现的：\n    $$ P^{(m,0)}_{ij} = \\frac{W^{(m)}_{ij}}{\\sum_{j'=1}^{N} W^{(m)}_{ij'}} $$\n    如果行和为零（即，一个患者没有邻居），它将被替换为均匀分布，$P^{(m,0)}_{ij} = 1/N$。这些矩阵可以解释为图上随机游走的转移概率矩阵。\n    同时，我们构建固定的传播矩阵 $S^{(m)}$。这些是稀疏的行随机矩阵，编码了每个网络的局部邻域结构。$S^{(m)}$ 是从 $P^{(m,0)}$ 派生出来的，方法是为每行 $i$ 仅保留对应于其 $k$-最近邻（由 $P^{(m,0)}$ 中的值确定）的条目，并将这些条目重新归一化使其和为 $1$。在融合过程中，这个矩阵 $S^{(m)}$ 将作为一个固定的滤波器来传播信息。\n\n5.  **迭代网络融合：**\n    融合是通过一个迭代的跨网络扩散过程实现的，这是一种消息传递的形式。在每次迭代 $t$ 中，每个网络 $P^{(m,t)}$ 都基于所有其他网络的信息进行更新。更新规则是：\n    $$ P^{(m,t+1)} = S^{(m)} \\left( \\frac{1}{M-1} \\sum_{\\substack{v=1 \\\\ v \\ne m}}^{M} P^{(v,t)} \\right) \\left(S^{(m)}\\right)^{\\top} $$\n    括号中的项是迭代 $t$ 时所有其他网络的平均值。这个平均网络然后通过与 $S^{(m)}$ 及其转置进行前后相乘，被网络 $m$ 的局部结构“过滤”。这一步有效地增强了由多个数据源支持的相似性，使网络逐渐变得更加相似。每次更新后，$P^{(m,t+1)}$ 被重新归一化为行随机矩阵。对于单组学情况（$M=1$），此过程将被跳过。\n\n6.  **收敛与最终融合：**\n    迭代过程持续进行，直到网络稳定或达到最大迭代次数 $T_{\\max}$。稳定性通过使用弗罗贝尼乌斯范数测量连续迭代之间的变化来评估。如果所有网络的最大变化低于容差 $\\varepsilon$，则过程在迭代 $t$ 终止：\n    $$ \\Delta^{(t)} = \\max_{m \\in \\{1,\\dots,M\\}} \\left\\| P^{(m,t+1)} - P^{(m,t)} \\right\\|_F \\le \\varepsilon $$\n    在 $t_{\\text{final}}$ 终止时，通过对收敛的网络进行平均来计算最终的融合网络 $F$：\n    $$ F = \\frac{1}{M} \\sum_{m=1}^{M} P^{(m,t_{\\text{final}})} $$\n    这个最终矩阵也经过行归一化，以产生最终的患者-患者相似性网络。最终得到的矩阵 $F$ 代表了患者相似性的一个综合视图，整合了来自所有 $M$ 个组学的证据。\n\n7.  **输出格式化：**\n    最终的融合矩阵 $F$ 按行主序被展平为一维列表，并且每个元素都按要求四舍五入到6位小数。",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the SNF pipeline, printing the final results.\n    \"\"\"\n\n    def normalize_rows(matrix):\n        \"\"\"\n        Row-normalizes a matrix to be row-stochastic.\n        If a row sum is 0, it is replaced with a uniform distribution.\n        \"\"\"\n        N = matrix.shape[0]\n        row_sums = matrix.sum(axis=1, keepdims=True)\n        normalized_matrix = matrix.copy()\n        \n        non_zero_rows = row_sums.flatten()  1e-12\n        if np.any(non_zero_rows):\n            normalized_matrix[non_zero_rows, :] /= row_sums[non_zero_rows]\n        \n        zero_rows = ~non_zero_rows\n        if np.any(zero_rows):\n            normalized_matrix[zero_rows, :] = 1.0 / N\n            \n        return normalized_matrix\n\n    def get_k_nearest_neighbor_mask(A, k):\n        \"\"\"\n        Determines the k-nearest-neighbor mask for a given affinity matrix.\n        Tie-breaking is done by smaller column index.\n        \"\"\"\n        N = A.shape[0]\n        mask = np.zeros((N, N), dtype=bool)\n\n        for i in range(N):\n            row_values = A[i, :]\n            # Get off-diagonal elements and their original indices\n            off_diag_indices = np.arange(N) != i\n            off_diag_vals = row_values[off_diag_indices]\n            off_diag_j_indices = np.arange(N)[off_diag_indices]\n\n            # Use lexsort for stable sorting: sort by -value (desc), then index (asc)\n            sorted_indices = np.lexsort((off_diag_j_indices, -off_diag_vals))\n            \n            # Identify the top k original column indices\n            top_k_j_indices = off_diag_j_indices[sorted_indices[:k]]\n            mask[i, top_k_j_indices] = True\n            \n        return mask\n\n    def run_snf_for_case(X_list, k, sigma, eps, T_max):\n        \"\"\"\n        Executes the complete Similarity Network Fusion (SNF) pipeline for a single test case.\n        \"\"\"\n        N = X_list[0].shape[0]\n        M = len(X_list)\n\n        # Step 1: Z-scoring per omic and per feature\n        X_std_list = []\n        for X_m in X_list:\n            mean = np.mean(X_m, axis=0)\n            std = np.std(X_m, axis=0)\n            X_tilde_m = np.zeros_like(X_m, dtype=float)\n            \n            non_zero_std_mask = std  1e-12\n            if np.any(non_zero_std_mask):\n                X_tilde_m[:, non_zero_std_mask] = (X_m[:, non_zero_std_mask] - mean[non_zero_std_mask]) / std[non_zero_std_mask]\n            \n            X_std_list.append(X_tilde_m)\n\n        # Step 2: Pairwise squared Euclidean distance\n        D_list = [squareform(pdist(X_std, metric='sqeuclidean')) for X_std in X_std_list]\n\n        # Step 3: Gaussian kernel similarity\n        G_list = []\n        for D_m in D_list:\n            G_m = np.exp(-D_m / (2 * sigma**2))\n            np.fill_diagonal(G_m, 1)\n            G_list.append(G_m)\n\n        # Step 4: k-nearest neighbor sparsification and symmetrization\n        W_list = []\n        for G_m in G_list:\n            knn_mask = get_k_nearest_neighbor_mask(G_m, k)\n            S_mask_matrix = np.zeros_like(G_m)\n            S_mask_matrix[knn_mask] = G_m[knn_mask]\n            \n            W_m = (S_mask_matrix + S_mask_matrix.T) / 2\n            np.fill_diagonal(W_m, 1)\n            W_list.append(W_m)\n\n        # Step 5: Row-stochastic normalization\n        P0_list = [normalize_rows(W_m) for W_m in W_list]\n\n        if M == 1:\n            # For M=1, result is the single normalized network\n            final_fused_matrix = P0_list[0]\n            flat_rounded = [f\"{x:.6f}\" for x in final_fused_matrix.flatten()]\n            return f\"[{','.join(flat_rounded)}]\"\n\n        # Step 6: Fixed neighbor-constrained propagation matrices\n        S_list = []\n        for P0_m in P0_list:\n            S_m = np.zeros_like(P0_m)\n            knn_mask = get_k_nearest_neighbor_mask(P0_m, k)\n            for i in range(N):\n                neighbor_indices = np.where(knn_mask[i, :])[0]\n                denominator = P0_m[i, neighbor_indices].sum()\n                if denominator  1e-12:\n                    S_m[i, neighbor_indices] = P0_m[i, neighbor_indices] / denominator\n                else:\n                    S_m[i, :] = 1.0 / N\n            S_list.append(S_m)\n            \n        # Step 7  8: Iterative message passing and convergence\n        P_current = [p.copy() for p in P0_list]\n        for t in range(T_max):\n            P_next = [np.zeros_like(p) for p in P_current]\n            sum_P_current = sum(P_current)\n            \n            for m in range(M):\n                avg_others = (sum_P_current - P_current[m]) / (M - 1)\n                P_next_m_unnorm = S_list[m] @ avg_others @ S_list[m].T\n                P_next[m] = normalize_rows(P_next_m_unnorm)\n\n            max_diff = 0\n            for m in range(M):\n                diff = np.linalg.norm(P_next[m] - P_current[m], 'fro')\n                max_diff = max(max_diff, diff)\n            \n            P_current = P_next\n            \n            if max_diff = eps:\n                break\n\n        # Step 9: Fused matrix\n        F_unnorm = sum(P_current) / M\n        final_fused_matrix = normalize_rows(F_unnorm)\n\n        # Step 10: Required output formatting\n        flat_rounded = [f\"{x:.6f}\" for x in final_fused_matrix.flatten()]\n        return f\"[{','.join(flat_rounded)}]\"\n\n    test_cases = [\n        {\n            \"X_list\": [\n                np.array([[0.0, 1.0], [0.2, 0.9], [3.0, 3.5], [3.2, 3.6]], dtype=float),\n                np.array([[0.1, 1.1], [0.0, 1.0], [3.1, 3.4], [3.3, 3.7]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.0, \"eps\": 1e-6, \"T_max\": 50\n        },\n        {\n            \"X_list\": [\n                np.array([[1.0, 0.0, 0.5], [0.9, 0.1, 0.4], [3.0, 3.5, 3.7], [3.1, 3.6, 3.8]], dtype=float)\n            ],\n            \"k\": 1, \"sigma\": 0.7, \"eps\": 1e-6, \"T_max\": 10\n        },\n        {\n            \"X_list\": [\n                np.array([[0.0, 0.0], [0.1, -0.1], [5.0, 5.0], [5.1, 4.9], [2.5, 2.5]], dtype=float),\n                np.array([[0.2, -0.2], [0.0, 0.0], [4.9, 5.1], [5.2, 5.0], [2.4, 2.6]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.5, \"eps\": 1e9, \"T_max\": 20\n        },\n        {\n            \"X_list\": [\n                np.array([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]], dtype=float),\n                np.array([[0.0, 0.1], [1.0, 1.1], [1.9, 2.1]], dtype=float),\n                np.array([[0.0, 1.0, 2.0], [1.0, 2.0, 3.0], [2.0, 3.0, 4.0]], dtype=float)\n            ],\n            \"k\": 2, \"sigma\": 1.0, \"eps\": 1e-12, \"T_max\": 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result_str = run_snf_for_case(\n            case[\"X_list\"], case[\"k\"], case[\"sigma\"], case[\"eps\"], case[\"T_max\"]\n        )\n        results.append(result_str)\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}