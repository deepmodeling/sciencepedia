{
    "hands_on_practices": [
        {
            "introduction": "The discovery of novel therapeutic targets often begins with genomics. This practice simulates a core proteogenomics workflow, translating tumor-specific genetic variants into a personalized protein database. By implementing the steps from DNA to peptide, you will gain hands-on experience in generating the very search space where neoantigens and aberrant phosphopeptides—the prime candidates for next-generation therapies—are found. ",
            "id": "5023010",
            "problem": "You are to implement a self-contained computational workflow that transforms simplified tumor variant representations from Variant Call Format (VCF)-like and Gene Transfer Format (GTF)-like annotations into a personalized protein database and quantifies immunoproteomics- and phosphoproteomics-relevant observables. The workflow must begin from principles derived from the Central Dogma of Molecular Biology (deoxyribonucleic acid to ribonucleic acid to protein), deterministic codon translation, and protease cleavage specificity, and must be expressed in purely algorithmic terms without reliance on external files or networks.\n\nFundamental bases and definitions:\n- Central Dogma: coding deoxyribonucleic acid (DNA) is interpreted in contiguous triplets called codons to yield an amino acid sequence. There exist well-established codon-to-amino-acid mappings specified by the Standard Genetic Code.\n- Open reading frame: translation proceeds from the first base of the coding deoxyribonucleic acid in steps of codons; stop codons terminate translation. For this task, all coding sequences provided are in-frame and contain no stop codons unless a frameshift is introduced by an insertion or deletion.\n- Protease specificity: trypsin cleaves after lysine and arginine residues, denoted by amino acids $K$ and $R$, respectively, except when the next residue is proline, denoted by $P$. Formally, if the protein sequence is $P = p_1 p_2 \\dots p_n$, a cleavage occurs after position $i$ if and only if $p_i \\in \\{K,R\\}$ and either $i = n$ or $p_{i+1} \\neq P$.\n- Peptide length filter: only peptides with length between $8$ and $11$ residues inclusive are retained.\n- Phosphosite motif surrogate: a peptide is marked as containing a potential proline-directed phosphorylation motif if it contains the substring $SP$ or $TP$ (serine-proline or threonine-proline).\n\nVariant event definitions to be modeled:\n- Single-nucleotide variant (SNV): change a single base in the coding DNA at a specified $0$-based index.\n- Insertion or deletion (indel): replace a specified reference substring at a $0$-based position with an alternate substring (the alternate may be empty for deletions). All positions are $0$-based indices on the coding DNA string.\n- Gene fusion: construct a chimeric coding DNA by taking the prefix of one transcript up to a specified left breakpoint base index and concatenating it with the suffix of a second transcript starting at a specified right start base index. Both indices are $0$-based and must be multiples of $3$ to preserve the reading frame.\n- Alternative splicing (exon skipping): generate an isoform by removing specified exons (given by $1$-based indices) from the transcript’s exon list and concatenating the remaining exons in order.\n\nReference data:\n- Coding DNA sequences for transcripts (concatenated as contiguous strings, all in-frame):\n    - Transcript $T1$:\n      ATGGCTGTTCTGCAATCTGGTGTTAAATCTGCTCTGGAAACTCAAAATGTTCGTACTCTGGTTCTACTACTGCTGGTCGT\n    - Transcript $T2$:\n      ATGCTGGCTGAAGAAGTTGTTAAAGCTAATCTGACTGTTTCTGCTCGT\n    - Transcript $T3$ exons (for exon-skipping events):\n        - Exon $1$: ATGGCTCAAGTTGCT\n        - Exon $2$: TCTGCTAAAACT\n        - Exon $3$: GTTTCTGCTGGTCGT\n      The full reference $T3$ coding DNA is the concatenation of exons $1$–$3$ in order.\n\nAssumptions and constraints:\n- All transcript coding DNA is uppercase and uses only $\\{A,C,G,T\\}$.\n- All provided breakpoints for fusions are multiples of $3$.\n- Translation uses the Standard Genetic Code; translation stops at the first stop codon if one arises due to a frameshift.\n- Trypsin cleavage follows the rule defined above; empty peptide segments are discarded before length filtering.\n- A personalized protein database is the set of unique protein sequences generated by applying the specified events to the reference inputs for each test case. Reference peptides are computed from the unmodified proteins of $T1$, $T2$, and $T3$.\n\nYour program must implement the following operations:\n- Translate coding DNA to protein by mapping each codon to its amino acid; stop translation at the first stop codon if encountered.\n- Apply SNVs, indels, fusions, and alternative splicing as defined to produce variant coding DNA, then translate to variant proteins.\n- Tryptically digest both reference and variant proteins, then retain only peptides with length between $8$ and $11$ inclusive.\n- Compute the following per test case:\n    $1.$ The number of distinct variant proteins in the personalized database (an integer).\n    $2.$ The number of distinct tryptic peptides (length $8$–$11$ inclusive) derived from the personalized database (an integer).\n    $3.$ The number of neo-peptides, defined as variant peptides not present among the corresponding reference peptides from $T1$, $T2$, and $T3$ (an integer).\n    $4.$ The number of neo-peptides that contain at least one $SP$ or $TP$ motif (an integer).\n\nTest suite:\nImplement the workflow on the following test cases. All positions are $0$-based for bases and $1$-based for exon indices.\n\n- Test case $1$ (single-nucleotide variant):\n    - Events: a single SNV on $T1$ at base index $30$, changing reference base $G$ to alternate base $C$.\n\n- Test case $2$ (in-frame deletion):\n    - Events: a single indel on $T2$ deleting the substring $GAA$ at base index $9$ (i.e., three-base deletion preserving the frame).\n\n- Test case $3$ (gene fusion):\n    - Events: a fusion between $T1$ and $T2$ formed by taking the prefix of $T1$ up to base index $36$ and the suffix of $T2$ starting at base index $9$, concatenated in-frame.\n\n- Test case $4$ (alternative splicing, exon skip):\n    - Events: an alternative splicing event on $T3$ that skips exon index $2$ (exon indices are $1$-based).\n\n- Test case $5$ (combined events):\n    - Events: apply all of the above events jointly (the SNV on $T1$, the in-frame deletion on $T2$, the $T1$–$T2$ fusion, and the $T3$ exon $2$ skip). The personalized database contains all unique variant proteins arising from these events.\n\nOutput specification:\n- For each test case in the order given above, compute the list $[n_{\\mathrm{var\\_proteins}}, n_{\\mathrm{var\\_peptides}}, n_{\\mathrm{neo}}, n_{\\mathrm{neo\\_motif}}]$ where all entries are integers as defined above.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists enclosed in square brackets, for example, $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\dots]$ with no spaces inserted.\n\nAngle units and physical units are not applicable. All outputs are integers as defined. The correctness of your program will be assessed on the exact aggregated list it prints for the test suite above.",
            "solution": "The user has provided a computational problem simulating a bioinformatics workflow for personalized proteomics. I will first validate the problem statement and then, if valid, provide a reasoned solution and implementation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Reference Data Structures:**\n    -   Transcript $T1$ coding DNA (cDNA): `ATGGCTGTTCTGCAATCTGGTGTTAAATCTGCTCTGGAAACTCAAAATGTTCGTACTCTGGTTCTACTACTGCTGGTCGT`\n    -   Transcript $T2$ cDNA: `ATGCTGGCTGAAGAAGTTGTTAAAGCTAATCTGACTGTTTCTGCTCGT`\n    -   Transcript $T3$ exons:\n        -   Exon $1$: `ATGGCTCAAGTTGCT`\n        -   Exon $2$: `TCTGCTAAAACT`\n        -   Exon $3$: `GTTTCTGCTGGTCGT`\n-   **Biological & Chemical Rules:**\n    -   **Translation:** Standard Genetic Code is used to translate cDNA codons into amino acids. Translation terminates at the first stop codon.\n    -   **Proteolysis:** Trypsin cleaves a protein sequence $P = p_1 p_2 \\dots p_n$ after a residue $p_i$ if and only if $p_i \\in \\{K,R\\}$ (lysine or arginine) and ($i = n$ or $p_{i+1} \\neq P$ (proline)).\n    -   **Peptide Filtering:** Only peptides with length between $8$ and $11$ amino acids, inclusive, are considered.\n    -   **Phosphosite Motif:** A peptide is marked if it contains the substring `SP` or `TP`.\n-   **Genetic Event Definitions:**\n    -   **SNV (Single-Nucleotide Variant):** A single base is changed at a specified $0$-based index in the cDNA.\n    -   **Indel (Insertion/Deletion):** A substring at a $0$-based index in the cDNA is replaced by an alternate substring.\n    -   **Gene Fusion:** A chimeric cDNA is formed by concatenating a prefix from one transcript and a suffix from another. Breakpoints are $0$-based and multiples of $3$.\n    -   **Alternative Splicing (Exon Skipping):** An isoform is generated by omitting specified exons (given by $1$-based indices) from a transcript's exon list.\n-   **Test Suite & Output:**\n    1.  **Case 1 (SNV):** On $T1$, at index $30$, change reference base to `C`.\n    2.  **Case 2 (Indel):** On $T2$, delete substring `GAA` at index $9$.\n    3.  **Case 3 (Fusion):** Fuse $T1$ (prefix up to index $36$) with $T2$ (suffix from index $9$).\n    4.  **Case 4 (Splicing):** On $T3$, skip exon $2$ ($1$-based index).\n    5.  **Case 5 (Combined):** Apply all events from cases 1-4. The personalized database is the set of all unique variant proteins produced by these events.\n-   **Required Metrics per Test Case:** The program must compute a list of four integers: $[n_{\\mathrm{var\\_proteins}}, n_{\\mathrm{var\\_peptides}}, n_{\\mathrm{neo}}, n_{\\mathrm{neo\\_motif}}]$.\n    -   $n_{\\mathrm{var\\_proteins}}$: Number of unique variant protein sequences.\n    -   $n_{\\mathrm{var\\_peptides}}$: Number of unique, length-filtered tryptic peptides from the variant proteins.\n    -   $n_{\\mathrm{neo}}$: Number of variant peptides not present in the set of reference peptides from unmodified $T1$, $T2$, and $T3$.\n    -   $n_{\\mathrm{neo\\_motif}}$: Number of neo-peptides containing an `SP` or `TP` motif.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem is a well-founded simulation based on fundamental principles of molecular biology, including the Central Dogma, codon translation, and enzymatic protein digestion. While simplified, it does not violate scientific facts. It accurately models common types of genetic variants.\n-   **Well-Posed:** The problem is deterministic and well-posed. Inputs, operational rules, and desired outputs are defined with mathematical and algorithmic precision. All necessary data are provided, and the constraints (e.g., fusion breakpoints being multiples of $3$) ensure that the operations are well-defined (i.e., preserve the reading frame).\n-   **Objective:** The problem statement is objective, using formal, unambiguous technical language devoid of subjective interpretation.\n\nThe problem does not exhibit any invalidity flaws. It is a formalizable and solvable computational exercise directly relevant to the specified topic domain of immunoproteomics and phosphoproteomics.\n\n**Step 3: Verdict and Action**\n\n-   **Verdict:** The problem is **valid**.\n-   **Action:** Proceed to the solution.\n\n### Solution Design\n\nThe computational workflow will be implemented by breaking it down into a series of modular functions, each corresponding to a core biological process. The overall design follows a principle-based approach, starting from the reference genetic material and progressively applying transformations as defined in the problem.\n\n**1. Data Initialization and Constants:**\nFirst, we establish the constants of our model: the reference DNA sequences for transcripts $T1$, $T2$, and the exons of $T3$, as well as the Standard Genetic Code mapping codons to amino acids. A special character, `*`, is used to denote a stop codon.\n\n**2. Core Biological Process Functions:**\n-   **`translate(dna_seq)`:** This function implements the Central Dogma's translation step. It iterates through the input DNA sequence, taking three bases (a codon) at a time. Each codon is mapped to its corresponding amino acid using the genetic code dictionary. If a stop codon (`*`) is encountered, translation is terminated, and the protein sequence synthesized so far is returned. This correctly models the biological process of translation termination.\n-   **`digest(protein_seq)`:** This function simulates proteolytic cleavage by trypsin. It iterates through the protein sequence and identifies cleavage sites according to the rule: cleave after an amino acid $p_i \\in \\{K, R\\}$ unless the subsequent amino acid $p_{i+1}$ is $P$. A cleavage also occurs if $p_i$ is the C-terminal (last) residue. The function returns a list of resulting peptide fragments.\n-   **`filter_peptides(peptides)`:** This utility function applies the specified length filter, retaining only those peptides whose length is between $8$ and $11$ residues, inclusive.\n\n**3. Variant Generation and Analysis Workflow:**\nThe main logic orchestrates these functions to solve each test case.\n-   **Reference Peptide Set Calculation:** Before processing the test cases, a comprehensive set of reference peptides is generated. The reference cDNA for $T1$, $T2$, and $T3$ (concatenated from its exons) are each translated, digested, and filtered. The resulting peptides are collected into a single set, `ref_peptides`, which serves as the baseline for identifying neo-peptides.\n-   **Event-Driven Variant Generation:** For each test case, we process a list of specified genetic events. A helper function, `apply_events`, is designed to interpret each event type (SNV, indel, fusion, splicing) and apply the corresponding transformation to the appropriate reference DNA sequence, producing one or more variant DNA sequences. These are collected into a set to handle any identical DNA sequences that might arise.\n-   **Personalized Proteome and Peptidome Analysis:** The set of variant DNA sequences is translated into a set of unique variant proteins, forming the \"personalized protein database.\" The cardinality of this set gives $n_{\\mathrm{var\\_proteins}}$. Each unique variant protein is then digested and filtered to generate a set of \"variant peptides.\" The size of this set is $n_{\\mathrm{var\\_peptides}}$.\n-   **Neo-peptide and Motif Quantification:** The set of neo-peptides is computed by taking the set-theoretic difference between the variant peptide set and the reference peptide set. The size of the resulting set is $n_{\\mathrm{neo}}$. Finally, each neo-peptide is scanned for the presence of `'SP'` or `'TP'` substrings to count $n_{\\mathrm{neo\\_motif}}$.\n\n**4. Test Case Execution:**\nThe workflow is executed for each of the five test cases defined in the problem. The four computed metrics ($n_{\\mathrm{var\\_proteins}}, n_{\\mathrm{var\\_peptides}}, n_{\\mathrm{neo}}, n_{\\mathrm{neo\\_motif}}$) for each case are collected. The final output is formatted into a single line as a JSON-like array of arrays, with no whitespace, as per the specification. This structured, step-by-step process ensures that each biological rule and computational requirement from the problem statement is correctly and robustly implemented.",
            "answer": "```python\n# The final answer must be a single, complete, standalone program.\n# The code must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are permitted.\n\ndef solve():\n    \"\"\"\n    Implements a self-contained computational workflow to simulate a personalized\n    proteogenomics analysis, from variant DNA to immunologically relevant observables.\n    \"\"\"\n    \n    # 1. Define fundamental constants and reference data\n    GENETIC_CODE = {\n        'AAA': 'K', 'AAC': 'N', 'AAG': 'K', 'AAT': 'N',\n        'ACA': 'T', 'ACC': 'T', 'ACG': 'T', 'ACT': 'T',\n        'AGA': 'R', 'AGC': 'S', 'AGG': 'R', 'AGT': 'S',\n        'ATA': 'I', 'ATC': 'I', 'ATG': 'M', 'ATT': 'I',\n        'CAA': 'Q', 'CAC': 'H', 'CAG': 'Q', 'CAT': 'H',\n        'CCA': 'P', 'CCC': 'P', 'CCG': 'P', 'CCT': 'P',\n        'CGA': 'R', 'CGC': 'R', 'CGG': 'R', 'CGT': 'R',\n        'CTA': 'L', 'CTC': 'L', 'CTG': 'L', 'CTT': 'L',\n        'GAA': 'E', 'GAC': 'D', 'GAG': 'E', 'GAT': 'D',\n        'GCA': 'A', 'GCC': 'A', 'GCG': 'A', 'GCT': 'A',\n        'GGA': 'G', 'GGC': 'G', 'GGG': 'G', 'GGT': 'G',\n        'GTA': 'V', 'GTC': 'V', 'GTG': 'V', 'GTT': 'V',\n        'TAA': '*', 'TAC': 'Y', 'TAG': '*', 'TAT': 'Y',\n        'TCA': 'S', 'TCC': 'S', 'TCG': 'S', 'TCT': 'S',\n        'TGA': '*', 'TGC': 'C', 'TGG': 'W', 'TGT': 'C',\n        'TTA': 'L', 'TTC': 'F', 'TTG': 'L', 'TTT': 'F'\n    }\n\n    REFS = {\n        'T1': \"ATGGCTGTTCTGCAATCTGGTGTTAAATCTGCTCTGGAAACTCAAAATGTTCGTACTCTGGTTCTACTACTGCTGGTCGT\",\n        'T2': \"ATGCTGGCTGAAGAAGTTGTTAAAGCTAATCTGACTGTTTCTGCTCGT\",\n        'T3_exons': [\n            \"ATGGCTCAAGTTGCT\",\n            \"TCTGCTAAAACT\",\n            \"GTTTCTGCTGGTCGT\",\n        ]\n    }\n    REFS['T3'] = \"\".join(REFS['T3_exons'])\n\n    # 2. Define core biological process functions\n    def translate(dna_seq):\n        \"\"\"Translates a DNA sequence into a protein sequence.\"\"\"\n        protein = []\n        for i in range(0, len(dna_seq) - len(dna_seq) % 3, 3):\n            codon = dna_seq[i:i+3]\n            aa = GENETIC_CODE.get(codon, '')\n            if aa == '*':\n                break\n            protein.append(aa)\n        return \"\".join(protein)\n\n    def digest(protein_seq):\n        \"\"\"Digests a protein with trypsin.\"\"\"\n        peptides = []\n        start = 0\n        for i, aa in enumerate(protein_seq):\n            if aa in ('K', 'R'):\n                if i == len(protein_seq) - 1 or protein_seq[i+1] != 'P':\n                    peptide = protein_seq[start:i+1]\n                    if peptide:\n                        peptides.append(peptide)\n                    start = i + 1\n        \n        if start < len(protein_seq):\n            final_peptide = protein_seq[start:]\n            if final_peptide:\n                peptides.append(final_peptide)\n        return peptides\n\n    def filter_peptides(peptides, min_len=8, max_len=11):\n        \"\"\"Filters peptides by length.\"\"\"\n        return {p for p in peptides if min_len <= len(p) <= max_len}\n\n    # 3. Pre-calculate the reference peptidome\n    ref_proteins = [translate(REFS[tid]) for tid in ['T1', 'T2', 'T3']]\n    ref_peptides = set()\n    for prot in ref_proteins:\n        peps = digest(prot)\n        ref_peptides.update(filter_peptides(peps))\n\n    # 4. Define variant events for each test case\n    test_cases = [\n        # Case 1: SNV\n        [('snv', 'T1', 30, 'C')],\n        # Case 2: Indel\n        [('indel', 'T2', 9, 'GAA', '')],\n        # Case 3: Fusion\n        [('fusion', 'T1', 36, 'T2', 9)],\n        # Case 4: Splicing\n        [('splice', 'T3', {2})],\n        # Case 5: Combined\n        [\n            ('snv', 'T1', 30, 'C'),\n            ('indel', 'T2', 9, 'GAA', ''),\n            ('fusion', 'T1', 36, 'T2', 9),\n            ('splice', 'T3', {2})\n        ]\n    ]\n\n    # 5. Process each test case\n    final_results = []\n    for events in test_cases:\n        variant_dnas = set()\n        for event in events:\n            etype = event[0]\n            if etype == 'snv':\n                _, t_id, pos, alt = event\n                dna = REFS[t_id]\n                var_dna = dna[:pos] + alt + dna[pos+1:]\n                variant_dnas.add(var_dna)\n            elif etype == 'indel':\n                _, t_id, pos, ref_sub, alt_sub = event\n                dna = REFS[t_id]\n                var_dna = dna[:pos] + alt_sub + dna[pos+len(ref_sub):]\n                variant_dnas.add(var_dna)\n            elif etype == 'fusion':\n                _, t1_id, bp1, t2_id, bp2 = event\n                dna1 = REFS[t1_id]\n                dna2 = REFS[t2_id]\n                var_dna = dna1[:bp1] + dna2[bp2:]\n                variant_dnas.add(var_dna)\n            elif etype == 'splice':\n                _, t_id, skip_indices = event\n                exons = REFS[f\"{t_id}_exons\"]\n                kept_exons = [exon for i, exon in enumerate(exons, 1) if i not in skip_indices]\n                var_dna = \"\".join(kept_exons)\n                variant_dnas.add(var_dna)\n\n        # Generate personalized proteome and peptidome\n        variant_proteins = {translate(dna) for dna in variant_dnas}\n        \n        variant_peptides = set()\n        for prot in variant_proteins:\n            peps = digest(prot)\n            variant_peptides.update(filter_peptides(peps))\n\n        # Compute metrics\n        n_var_proteins = len(variant_proteins)\n        n_var_peptides = len(variant_peptides)\n        \n        neo_peptides = variant_peptides - ref_peptides\n        n_neo = len(neo_peptides)\n        \n        n_neo_motif = sum(1 for p in neo_peptides if 'SP' in p or 'TP' in p)\n        \n        final_results.append([n_var_proteins, n_var_peptides, n_neo, n_neo_motif])\n    \n    # 6. Print the final aggregated output in the required format\n    # The format must be a list of lists, stringified without spaces.\n    results_str = \",\".join([str(res).replace(\" \", \"\") for res in final_results])\n    print(f\"[{results_str}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "A successful target discovery study hinges not only on identifying potential targets but also on designing experiments with enough statistical power to validate them. This exercise guides you through the process of calculating the required sample size for a phosphoproteomics experiment. By deriving the formula from first principles, you will develop a deep understanding of the interplay between effect size, variance, and statistical confidence, a crucial skill for planning robust and efficient translational research. ",
            "id": "5022986",
            "problem": "A translational phosphoproteomics study is being planned to discover actionable targets by contrasting phosphosite abundance between a disease cohort and a matched control cohort. Quantitative measurements (for example, from tandem mass spectrometry) are log-transformed base 2 to stabilize variance, so that a log base 2 fold-change of $1$ corresponds to a doubling in abundance. A pilot study performed under the same sample preparation and acquisition conditions provides a pooled within-group variance estimate on the log base 2 scale of $s_{p}^{2} = 0.50$ for a candidate phosphosite.\n\nYou will design a two-arm study with equal per-group sample sizes, using a two-sample $t$-test framework to detect a true difference in group means of $\\delta = 1$ on the log base 2 scale. Assume independent samples, equal variances across groups, and that the sampling distribution of the difference of sample means is well approximated by a Gaussian distribution due to the Central Limit Theorem. The significance level is two-sided $\\alpha = 0.05$, and the target statistical power is $0.80$.\n\nStarting from the definitions of Type I and Type II error and the sampling distribution of the difference of means under the null and alternative, derive the Gaussian-approximate per-group sample size $n$ needed to achieve the specified power at the specified $\\alpha$ without invoking any pre-memorized sample size formulas. Then, using the pilot variance estimate $s_{p}^{2}$ as the common variance in the approximation, compute the numerical value of $n$.\n\nUse the following rounding requirements: round any standard normal quantiles to four significant figures, and report the final per-group sample size $n$ to four significant figures. Do not round $n$ to an integer; report it as a real number.",
            "solution": "The user has provided a problem that requires the derivation of a sample size formula and its subsequent numerical evaluation.\n\n### Step 1: Extract Givens\n-   **Study Design**: Two-arm study, disease vs. control.\n-   **Data Transformation**: Log-transformed base $2$.\n-   **Statistical Test**: Two-sample $t$-test framework, with a Gaussian (normal) approximation.\n-   **Sample Sizes**: Equal per-group sample sizes, $n_1 = n_2 = n$.\n-   **Assumptions**: Independent samples, equal variances.\n-   **Variance Estimate**: Pooled within-group variance on log base $2$ scale, $s_{p}^{2} = 0.50$. This will be used as the common variance $\\sigma^2$.\n-   **Effect Size**: True difference in group means to detect, $\\delta = 1$.\n-   **Significance Level**: Two-sided $\\alpha = 0.05$.\n-   **Statistical Power**: Target power is $1-\\beta = 0.80$.\n-   **Rounding**: Standard normal quantiles to four significant figures; final sample size $n$ to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is well-grounded in biostatistics and experimental design. Power analysis for a two-sample test is a standard procedure. The context of phosphoproteomics is realistic, as is the use of log-transformation for mass-spectrometry data. The specified parameters ($\\alpha$, power, $\\delta$, $\\sigma^2$) are typical for such studies.\n-   **Well-Posed**: The problem is well-posed. It clearly defines the objective: derive the sample size formula from first principles and then calculate its value. All necessary parameters and assumptions are provided. A unique, meaningful solution exists.\n-   **Objective**: The problem is stated in precise, objective language, free of bias or subjective claims.\n\nThe problem does not violate any of the specified invalidity criteria. It is scientifically sound, formalizable, complete, realistic, well-posed, and non-trivial.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Derivation of the Sample Size Formula\nLet $\\mu_1$ and $\\mu_2$ be the true population means of the log-transformed phosphosite abundance for the disease and control groups, respectively. Let $\\bar{X}_1$ and $\\bar{X}_2$ be the corresponding sample means from samples of size $n$. The common population variance is $\\sigma^2$, which we will estimate with $s_p^2$. The difference to detect is $\\delta = \\mu_1 - \\mu_2$.\n\nThe null and alternative hypotheses for the two-sided test are:\n$$H_0: \\mu_1 - \\mu_2 = 0$$\n$$H_A: \\mu_1 - \\mu_2 \\neq 0$$\n\nThe test statistic is based on the difference of the sample means, $\\bar{X}_1 - \\bar{X}_2$. Due to the independence of the samples and the assumption of equal variance $\\sigma^2$, the variance of this difference is:\n$$\\text{Var}(\\bar{X}_1 - \\bar{X}_2) = \\text{Var}(\\bar{X}_1) + \\text{Var}(\\bar{X}_2) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n}$$\nThe standard error of the difference is $SE = \\sqrt{\\frac{2\\sigma^2}{n}}$.\n\nThe problem specifies using a Gaussian approximation. Therefore, the sampling distribution of the difference in means is approximated as:\n$$\\bar{X}_1 - \\bar{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{2\\sigma^2}{n}\\right)$$\n\n**Condition for Type I Error ($\\alpha$)**\nUnder the null hypothesis ($H_0$), $\\mu_1 - \\mu_2 = 0$. The sampling distribution is $\\bar{X}_1 - \\bar{X}_2 \\sim N(0, \\frac{2\\sigma^2}{n})$. A Type I error occurs if we reject $H_0$ when it is true. For a two-sided test with significance level $\\alpha$, we reject $H_0$ if the observed difference is sufficiently far from $0$. The rejection region is defined by critical values that cut off $\\alpha/2$ in each tail of the standard normal distribution. Let $z_{\\alpha/2}$ be the standard normal quantile such that $P(Z > z_{\\alpha/2}) = \\alpha/2$, where $Z$ is a standard normal random variable.\n\nWe reject $H_0$ if $|\\bar{X}_1 - \\bar{X}_2| > C$, where $C$ is the critical value of the difference. To find $C$, we standardize:\n$$\\frac{C - 0}{\\sqrt{2\\sigma^2/n}} = z_{\\alpha/2}$$\n$$C = z_{\\alpha/2} \\sqrt{\\frac{2\\sigma^2}{n}}$$\nSo, we reject $H_0$ if $\\bar{X}_1 - \\bar{X}_2 > z_{\\alpha/2} \\sqrt{\\frac{2\\sigma^2}{n}}$ or if $\\bar{X}_1 - \\bar{X}_2 < -z_{\\alpha/2} \\sqrt{\\frac{2\\sigma^2}{n}}$.\n\n**Condition for Statistical Power ($1-\\beta$)**\nStatistical power is the probability of correctly rejecting $H_0$ when the alternative hypothesis ($H_A$) is true. We evaluate power for a specific alternative, $\\mu_1 - \\mu_2 = \\delta$. Under this alternative, the sampling distribution is $\\bar{X}_1 - \\bar{X}_2 \\sim N(\\delta, \\frac{2\\sigma^2}{n})$.\n\nFor $\\delta > 0$, the probability of falling into the lower rejection region ($\\bar{X}_1 - \\bar{X}_2 < -C$) is negligible. Thus, the power can be approximated by considering only the upper rejection region:\n$$\\text{Power} = 1 - \\beta \\approx P\\left(\\bar{X}_1 - \\bar{X}_2 > C \\mid \\mu_1 - \\mu_2 = \\delta\\right)$$\nTo calculate this probability, we standardize the critical value $C$ with respect to the distribution under $H_A$:\n$$1 - \\beta = P\\left( \\frac{(\\bar{X}_1 - \\bar{X}_2) - \\delta}{\\sqrt{2\\sigma^2/n}} > \\frac{C - \\delta}{\\sqrt{2\\sigma^2/n}} \\right)$$\nThe term on the left inside the probability is a standard normal variable $Z$. Let $z_\\beta$ be the standard normal quantile such that $P(Z > z_\\beta) = \\beta$. Then $P(Z > -z_\\beta) = 1-\\beta$. Thus, we must have:\n$$-z_\\beta = \\frac{C - \\delta}{\\sqrt{2\\sigma^2/n}}$$\n\nNow, we substitute the expression for $C$ from the Type I error condition:\n$$-z_\\beta = \\frac{z_{\\alpha/2} \\sqrt{\\frac{2\\sigma^2}{n}} - \\delta}{\\sqrt{2\\sigma^2/n}} = z_{\\alpha/2} - \\frac{\\delta}{\\sqrt{\\frac{2\\sigma^2}{n}}}$$\nRearranging to solve for the term containing $n$:\n$$\\frac{\\delta}{\\sqrt{\\frac{2\\sigma^2}{n}}} = z_{\\alpha/2} + z_\\beta$$\nSquaring both sides:\n$$\\frac{\\delta^2}{\\frac{2\\sigma^2}{n}} = (z_{\\alpha/2} + z_\\beta)^2$$\n$$\\frac{n\\delta^2}{2\\sigma^2} = (z_{\\alpha/2} + z_\\beta)^2$$\nFinally, solving for the per-group sample size $n$:\n$$n = \\frac{2\\sigma^2(z_{\\alpha/2} + z_\\beta)^2}{\\delta^2}$$\nThis is the general formula for the per-group sample size in a two-sample test under the Gaussian approximation, derived from first principles.\n\n### Numerical Calculation\nWe now substitute the given values into the derived formula.\n-   $\\sigma^2 \\approx s_{p}^{2} = 0.50$\n-   $\\delta = 1$\n-   $\\alpha = 0.05 \\implies \\alpha/2 = 0.025$\n-   $1-\\beta = 0.80 \\implies \\beta = 0.20$\n\nWe need to find the standard normal quantiles $z_{0.025}$ and $z_{0.20}$. The problem requires rounding these to four significant figures.\n-   $z_{\\alpha/2} = z_{0.025}$ is the value for which the cumulative probability is $1-0.025 = 0.975$. From a standard normal table or calculator, this value is approximately $1.95996$. Rounded to four significant figures, $z_{0.025} = 1.960$.\n-   $z_\\beta = z_{0.20}$ is the value for which the cumulative probability is $1-0.20 = 0.80$. From a standard normal table or calculator, this value is approximately $0.84162$. Rounded to four significant figures, $z_{0.20} = 0.8416$.\n\nSubstituting these values into the sample size formula:\n$$n = \\frac{2(0.50)(1.960 + 0.8416)^2}{(1)^2}$$\n$$n = \\frac{1 \\times (2.8016)^2}{1}$$\n$$n = (2.8016)^2$$\n$$n = 7.84896256$$\n\nThe problem requires reporting the final value for $n$ to four significant figures.\n$$n \\approx 7.849$$\nThis represents the non-integer per-group sample size required to achieve the specified power. In a practical setting, one would round this up to the next integer (i.e., $8$ participants per group). However, the problem explicitly requests the real number result.",
            "answer": "$$\n\\boxed{7.849}\n$$"
        },
        {
            "introduction": "In high-throughput mass spectrometry, confidently identifying peptides from complex spectra is paramount. This practice addresses the critical task of controlling the False Discovery Rate (FDR) in immunopeptidomics, where peptides do not follow standard protease rules. You will evaluate why common methods fail and learn to justify a tailored decoy strategy that ensures the validity of your peptide identifications, a non-negotiable step for nominating credible therapeutic targets. ",
            "id": "5022983",
            "problem": "In a translational mass spectrometry pipeline for immunopeptidomics, endogenous Major Histocompatibility Complex (MHC) class I ligands are identified without protease digestion, resulting in nontryptic peptides with length distribution centered around $8$–$11$ amino acids and allele-specific anchor residue enrichment (for example, position $2$ and the C-terminus). A laboratory currently uses a protein-level reversed-sequence decoy database constrained to tryptic peptides for False Discovery Rate (FDR) estimation via the Target-Decoy Approach (TDA). You are asked to assess whether this decoy strategy satisfies the foundational requirement for valid TDA and to propose a tailored decoy strategy that avoids composition bias while accounting for immunopeptidome-specific properties.\n\nUse the following fundamental bases:\n- Central Dogma of Molecular Biology (DNA $\\rightarrow$ RNA $\\rightarrow$ protein), proteasomal processing, and MHC presentation: endogenous peptides are generated by proteasomal cleavage and trimming, not by exogenous protease rules; their termini and sequence motifs reflect MHC binding constraints rather than protease specificity.\n- In peptide-spectrum matching with a score function $s$, the TDA requires that the score distribution under the null hypothesis (no true match) be equivalent between target and decoy candidates. Let $f_{T}(s\\mid H_{0})$ and $f_{D}(s\\mid H_{0})$ denote the null score densities for target and decoy sets, respectively. A necessary condition for valid FDR control via TDA is $f_{T}(s\\mid H_{0}) = f_{D}(s\\mid H_{0})$ for all relevant $s$.\n- Fragmentation-based scoring functions depend on peptide properties such as length $L$ and composition vector $\\mathbf{c}$ (counts of each residue), which affect the expected number of matched fragment ions. In a simplified null model, the expected score can be approximated as $\\mathbb{E}[s\\mid H_{0},L,\\mathbf{c}] = p(\\mathbf{c})\\cdot (L-1)$ where $p(\\mathbf{c})$ aggregates composition-dependent fragmentation propensities and spectrum peak density.\n\nConsider the following statements and proposed decoy schemes:\n\nA. Continue using reversed protein sequence decoys cut into tryptic peptides with Lysine/Arginine C-termini, because the equal-chance assumption of TDA depends only on database size, not on peptide biophysical properties.\n\nB. Generate decoy peptides by independent and identically distributed (i.i.d.) sampling of residues according to global proteome amino acid frequencies, matching only the length $L$ of each target peptide, thereby removing MHC motif biases present in targets.\n\nC. For each target peptide, generate $k$ decoys by a constrained random permutation of its residues that preserves the exact amino acid multiset $\\mathbf{c}$ and precursor mass-to-charge properties, disallows identity and near-identity (for example, no single transposition neighbors), leaves N- and C-termini unconstrained by protease rules, and adds a mild constraint to displace canonical allele-specific anchors away from their typical positions while keeping $\\mathbf{c}$ unchanged. Decoys are stratified by modification state (for example, phosphorylation) so that the modification count and sites are permuted but preserved in total.\n\nD. Use allele-specific Position Specific Scoring Matrices (PSSMs) to sample decoys that preserve position-specific residue frequencies (including anchors) and the length distribution, ensuring that decoys resemble true binders as closely as possible.\n\nWhich option best justifies the need for tailored decoys in immunopeptidomics and proposes a decoy scheme that avoids composition bias while maintaining the validity condition $f_{T}(s\\mid H_{0}) = f_{D}(s\\mid H_{0})$? Select the single best choice.",
            "solution": "The problem asks for an assessment of a decoy database strategy for False Discovery Rate (FDR) estimation in immunopeptidomics and to identify the best alternative strategy among the given options. The core of the problem lies in satisfying the foundational requirement of the Target-Decoy Approach (TDA), which is that the score distribution of incorrect peptide-spectrum matches (PSMs) must be the same for the target and decoy databases.\n\n**Problem Statement Validation**\n\n*   **Step 1: Extract Givens**\n    *   **Context:** Translational mass spectrometry for immunopeptidomics.\n    *   **Target Peptides:** Endogenous Major Histocompatibility Complex (MHC) class I ligands.\n    *   **Target Peptide Properties:**\n        *   Generated without protease digestion (nontryptic).\n        *   Length distribution centered around $8$–$11$ amino acids.\n        *   Exhibit allele-specific anchor residue enrichment (e.g., at position $2$ and the C-terminus).\n    *   **Current Decoy Strategy:** A protein-level reversed-sequence decoy database, constrained to tryptic peptides (C-terminal Lysine/Arginine).\n    *   **Task:** Assess the current decoy strategy and propose a tailored one that avoids composition bias.\n    *   **Fundamental Basis 1 (Biology):** MHC ligands are products of proteasomal processing, not exogenous proteases; their sequence features are dictated by MHC binding constraints.\n    *   **Fundamental Basis 2 (TDA Statistics):** A valid TDA requires that the null score probability density functions for target and decoy searches are identical: $f_{T}(s\\mid H_{0}) = f_{D}(s\\mid H_{0})$, where $H_{0}$ is the null hypothesis of an incorrect match.\n    *   **Fundamental Basis 3 (Scoring Model):** Peptide-spectrum matching scores, $s$, depend on peptide properties like length $L$ and amino acid composition $\\mathbf{c}$. A simplified null model for the expected score is given: $\\mathbb{E}[s\\mid H_{0},L,\\mathbf{c}] = p(\\mathbf{c})\\cdot (L-1)$.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   **Scientific Groundedness:** The problem statement is scientifically sound. It accurately describes the unique characteristics of the immunopeptidome (nontryptic nature, specific length range, anchor motifs) which distinguish it from typical shotgun proteomics samples. The statistical foundation of TDA ($f_{T}(s\\mid H_{0}) = f_{D}(s\\mid H_{0})$) is correctly stated and is a cornerstone of proteomics bioinformatics. The relationship between peptide properties (composition, length) and fragmentation-based scores is also a well-established principle in mass spectrometry.\n    *   **Well-Posedness:** The problem is well-posed. It presents a realistic scenario, provides the necessary theoretical background (biological and statistical), and asks for a critical evaluation of methods based on these principles. The goal is clear: find the decoy strategy that best satisfies the TDA's core assumption in this specific context.\n    *   **Objectivity:** The problem is stated in objective, technical language.\n\n*   **Step 3: Verdict and Action**\n    *   The problem statement is valid. It is scientifically accurate, well-posed, and free of ambiguity or contradiction. It presents a non-trivial challenge that is highly relevant to the field of proteomics. I will proceed to solve the problem.\n\n**Solution Derivation**\n\nThe fundamental issue is a mismatch between the properties of the target peptides (MHC ligands) and the decoy peptides generated by the current strategy.\n\n1.  **Target Peptides:** Nontryptic, short ($8$–$11$ aa), and possess a biased amino acid composition due to MHC anchor motifs. The C-terminal residue, for instance, is determined by the MHC allele's binding pocket, not by a protease's cutting rule.\n2.  **Current Decoy Peptides:** Tryptic, meaning they are computationally generated by cleaving reversed protein sequences after every Lysine ($K$) or Arginine ($R$), except when followed by Proline. This imposes a strong constraint that almost all decoy peptides will have a C-terminal $K$ or $R$.\n3.  **Violation of TDA Assumption:** The MS/MS-based score $s$ is sensitive to peptide properties. As given, the expected score under the null hypothesis depends on composition $\\mathbf{c}$ and length $L$. The target and decoy peptide populations have systematically different compositions. For example, the C-terminal residue distribution is completely different. This leads to distinct distributions of the composition vector $\\mathbf{c}_{target}$ and $\\mathbf{c}_{decoy}$. Consequently, the expected null scores will differ, and the null score distributions will not be equal: $f_{T}(s\\mid H_{0}) \\neq f_{D}(s\\mid H_{0})$. This invalidates the FDR estimation. A valid decoy strategy must generate decoys whose properties, under the null hypothesis, closely mirror those of the target peptides.\n\n**Evaluation of Options**\n\n*   **A. Continue using reversed protein sequence decoys cut into tryptic peptides...**\n    This option claims that TDA validity depends only on database size, not biophysical properties. This is fundamentally false. As established above and stated in the problem's premises, the score distributions for null matches must be identical. Since scores depend on peptide properties which influence fragmentation (e.g., presence of basic residues like $K$ and $R$ at the C-terminus affects fragmentation patterns and charge states), a systematic difference in these properties between targets (nontryptic) and decoys (tryptic) violates the TDA assumption.\n    **Verdict: Incorrect.**\n\n*   **B. Generate decoy peptides by i.i.d. sampling of residues according to global proteome amino acid frequencies...**\n    This method generates nontryptic decoys and can match the length distribution, which is an improvement. However, it samples residues from global proteome frequencies. The target immunopeptidome is not a random sample from the proteome; it is a highly selected subset enriched for specific residues that mediate MHC binding. Therefore, the amino acid composition $\\mathbf{c}$ of target peptides will systematically differ from that of decoys generated by this method. This composition bias will, according to the provided scoring model, lead to different null score distributions ($f_{T}(s\\mid H_{0}) \\neq f_{D}(s\\mid H_{0})$), thus violating the TDA assumption.\n    **Verdict: Incorrect.**\n\n*   **C. For each target peptide, generate $k$ decoys by a constrained random permutation of its residues...**\n    This is a \"peptide shuffling\" approach. By permuting the amino acids of a given target peptide to create a decoy, this method ensures that the decoy has the *exact same* length ($L$), amino acid composition ($\\mathbf{c}$), and precursor mass as the target. This is the most direct way to eliminate composition bias. For any pair of a target and its shuffled decoy, the term $p(\\mathbf{c})$ in the simplified scoring model $\\mathbb{E}[s\\mid H_{0},L,\\mathbf{c}] = p(\\mathbf{c})\\cdot (L-1)$ is identical. This ensures that, on average, a null match to a target peptide and a null match to its decoy counterpart will have the same score, thereby satisfying $f_{T}(s\\mid H_{0}) = f_{D}(s\\mid H_{0})$ with high fidelity. The proposed constraints (disallowing identity, displacing anchors) are sophisticated refinements that make the decoy set an even better model of the null hypothesis by ensuring decoys do not accidentally mimic true binding motifs. The handling of post-translational modifications (PTMs) by preserving their count is also correct. This method is perfectly tailored to the specific challenges of immunopeptidomics.\n    **Verdict: Correct.**\n\n*   **D. Use allele-specific Position Specific Scoring Matrices (PSSMs) to sample decoys that preserve position-specific residue frequencies (including anchors)...**\n    This method generates decoys that intentionally mimic the key features of *true* MHC ligands (the $H_1$ or alternative hypothesis), including the anchor motifs. The purpose of a decoy database is to model the score distribution of *incorrectly* matched peptides (the $H_0$ or null hypothesis). By making the decoys look as much like true positives as possible, the score distributions of targets and decoys will become difficult to distinguish. This collapses the separation between the null and alternative distributions, leading to a severe loss of statistical power and an inability to identify true Positives with any confidence. This approach misunderstands the fundamental purpose of the decoy set in TDA.\n    **Verdict: Incorrect.**\n\n**Conclusion**\n\nOption C provides the most robust and theoretically sound solution. It justifies the need for a tailored approach by proposing a method (peptide shuffling) that directly addresses the unique properties of the immunopeptidome (nontryptic nature, biased composition) and maintains the validity of the TDA by eliminating composition bias.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}