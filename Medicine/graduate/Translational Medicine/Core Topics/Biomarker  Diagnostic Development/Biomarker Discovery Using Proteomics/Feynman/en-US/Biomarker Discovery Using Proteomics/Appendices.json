{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any proteomics study is the confident identification of peptides from the millions of tandem mass spectra generated. This exercise tackles the critical first step in the data analysis pipeline: how to distinguish true peptide-spectrum matches from the vast number of random, incorrect matches. You will implement the gold-standard target-decoy strategy to estimate and control the False Discovery Rate (FDR), transforming raw search engine scores into statistically meaningful q-values. This practice is essential for building a reliable foundation upon which all subsequent biological interpretation and biomarker validation rests .",
            "id": "4994733",
            "problem": "You are given arrays of raw peptide-spectrum match (PSM) scores produced by a tandem mass spectrometry search engine for two classes: targets (putative real peptide identifications) and decoys (synthetic or shuffled sequences designed to emulate incorrect matches). The goal is to design a principled calibration procedure that transforms raw scores into significance values using the decoy null distribution, and then to compute minimal false discovery rates (q-values) to determine identification thresholds.\n\nUse the following fundamental base in your derivation and algorithm:\n\n- In the target-decoy framework, incorrect matches are assumed to be distributed like decoys under the null. Let the decoy scores be sampled from a null distribution. For any raw score $s$, define the calibrated $p$-value via the empirical decoy tail probability:\n$$\np(s) = \\frac{\\#\\{\\text{decoy scores } \\ge s\\}}{n_D}\n$$\nwhere $n_D$ is the total number of decoy observations. This is an empirical, distribution-free calibration that maps heterogeneous raw scores onto a common significance scale.\n\n- For a significance threshold $t$ applied to calibrated $p$-values, define $D(t)$ as the number of decoys with $p$-value less than or equal to $t$ and $T(t)$ as the number of targets with $p$-value less than or equal to $t$. The estimated false discovery rate at threshold $t$ is\n$$\n\\widehat{\\mathrm{FDR}}(t) = \\frac{D(t)}{T(t)} \\, .\n$$\n\n- For a given target PSM with calibrated $p$-value $p_i$, its $q$-value is the minimal false discovery rate at which it would be accepted. If $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$ denote the sorted target $p$-values, then for the target at rank $k$:\n$$\nq_{(k)} = \\min_{j \\ge k} \\widehat{\\mathrm{FDR}}(p_{(j)}) \\, .\n$$\nThis enforces monotonicity of the $q$-values with respect to acceptance thresholds. All proportions and thresholds must be expressed as decimals (for example, use $0.05$ and not a percentage sign).\n\nYour task is to implement this calibration and $q$-value computation in a single program. The program must:\n\n1. Accept no input; instead, use the embedded test suite provided below.\n2. For each test case, compute calibrated $p$-values for targets and decoys using the empirical decoy tail probability $p(s)$ as defined above.\n3. Compute target $q$-values using the definition above, with $\\widehat{\\mathrm{FDR}}(t) = D(t)/T(t)$ and no additional continuity correction.\n4. For each specified identification threshold $\\alpha$ (expressed as a decimal), count the number of target PSMs whose $q$-value is less than or equal to $\\alpha$.\n5. Aggregate these counts across all test cases and thresholds into a single flat list in a predefined order and print it exactly once.\n\nTest suite parameter values:\n\n- Test Case $1$ (general case):\n    - Target scores: $[2.1, 3.0, 1.5, 2.7, 3.5, 0.9, 1.2]$\n    - Decoy scores: $[0.5, 1.0, 1.4, 2.0, 2.3, 2.6, 2.9, 3.2]$\n    - Thresholds $\\alpha$: $[0.05, 0.10]$\n\n- Test Case $2$ (boundary case with very strong targets):\n    - Target scores: $[4.0, 3.8, 3.6, 3.5]$\n    - Decoy scores: $[0.2, 0.3, 0.4, 0.1, 0.25]$\n    - Thresholds $\\alpha$: $[0.01, 0.05, 0.10]$\n\n- Test Case $3$ (edge case with ties between target and decoy scores):\n    - Target scores: $[1.0, 1.0, 0.5]$\n    - Decoy scores: $[1.0, 0.5, 0.5]$\n    - Thresholds $\\alpha$: $[0.20]$\n\nOutput specification:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be a flat sequence of integers giving, in order, the counts of accepted targets at each threshold for Test Case $1$ (in the order given), followed by Test Case $2$ (in the order given), followed by Test Case $3$ (in the order given). For example, the output must have the form $[\\text{tc1\\_a1},\\text{tc1\\_a2},\\text{tc2\\_a1},\\text{tc2\\_a2},\\text{tc2\\_a3},\\text{tc3\\_a1}]$.",
            "solution": "The objective is to transform raw peptide-spectrum match (PSM) scores into calibrated significance values and then determine minimal false discovery rates ($q$-values) for setting identification thresholds. The scientific foundation rests on the target-decoy strategy, which treats decoy matches as a faithful empirical surrogate for the null distribution of incorrect matches. This permits nonparametric calibration and control of discoveries.\n\nPrinciple $1$: Calibration via the empirical null distribution. Let the decoy scores be independent draws from the null distribution of incorrect matches. For any raw score $s$, the empirical tail probability under the decoy distribution gives a calibrated $p$-value:\n$$\np(s) = \\frac{\\#\\{\\text{decoy scores } \\ge s\\}}{n_D} \\, .\n$$\nThis mapping is monotone decreasing in $s$ and converts arbitrary raw scores into a common significance scale. Under the null, $p(s)$ is approximately uniform on $[0,1]$ because it is an empirical cumulative distribution function (by the probability integral transform), and under the alternative (true targets), $p(s)$ concentrates near $0$.\n\nPrinciple $2$: Estimating the false discovery rate (FDR). For a threshold $t$ on calibrated $p$-values, define\n$$\nD(t) = \\#\\{\\text{decoy } p \\le t\\}, \\quad T(t) = \\#\\{\\text{target } p \\le t\\} \\, .\n$$\nAssuming a balanced search and that the decoy $p$-values represent the null, the proportion of false discoveries among accepted targets at threshold $t$ is estimated by\n$$\n\\widehat{\\mathrm{FDR}}(t) = \\frac{D(t)}{T(t)} \\, .\n$$\nThis estimator is an empirical ratio of accepted nulls to accepted targets. We do not introduce a continuity correction here; thus $\\widehat{\\mathrm{FDR}}(t)$ can be $0$ if no decoys are accepted.\n\nPrinciple $3$: $q$-values as minimal FDR. The $q$-value for a given target PSM is defined as the minimal false discovery rate at which it would be accepted. Let the sorted target $p$-values be $p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(m)}$. At each rank $k$, consider the acceptance threshold $t = p_{(k)}$ and compute\n$$\n\\widehat{\\mathrm{FDR}}(p_{(k)}) = \\frac{D(p_{(k)})}{T(p_{(k)})} = \\frac{D(p_{(k)})}{k} \\, .\n$$\nTo enforce monotonicity (that is, $q$-values should be non-decreasing with decreasing stringency), define\n$$\nq_{(k)} = \\min_{j \\ge k} \\widehat{\\mathrm{FDR}}(p_{(j)}) \\, .\n$$\nThis is the empirical analogue of the minimal FDR at or above the PSM’s own acceptance threshold.\n\nAlgorithmic design:\n\n- Compute calibrated $p$-values for both targets and decoys using the empirical decoy tail probability $p(s)$. This step performs score calibration, transforming heterogeneous raw scores onto a unified significance scale, justified by the empirical null modeled by decoys.\n\n- Sort target $p$-values in ascending order to form ranks. For each sorted target threshold $p_{(k)}$, compute $D(p_{(k)})$ using the decoy $p$-values and set $T(p_{(k)}) = k$. The estimated false discovery rates at these thresholds are $\\widehat{\\mathrm{FDR}}(p_{(k)}) = D(p_{(k)})/k$.\n\n- Compute the suffix minimum over the sequence $\\{\\widehat{\\mathrm{FDR}}(p_{(k)})\\}_{k=1}^m$ to obtain monotone $q$-values: $q_{(k)} = \\min_{j \\ge k} \\widehat{\\mathrm{FDR}}(p_{(j)})$. Map these back to the original target ordering.\n\n- For each identification threshold $\\alpha$ (expressed in decimals), count the number of target $q$-values satisfying $q_i \\le \\alpha$.\n\nTest suite coverage rationale:\n\n- Test Case $1$ exercises typical overlap between target and decoy scores, producing a mix of small and large $p$-values and ensuring nontrivial $D(t)$ and $T(t)$ ratios.\n\n- Test Case $2$ represents a boundary scenario in which targets are far stronger than decoys; here the empirical decoy tail probabilities for targets are $0$, and $\\widehat{\\mathrm{FDR}}(t)$ can be $0$, validating that the algorithm accepts many targets at stringent $\\alpha$.\n\n- Test Case $3$ introduces ties between target and decoy scores to test robustness in computing tail probabilities and cumulative counts, ensuring consistent handling of equality through the $\\ge$ and $\\le$ definitions.\n\nFinal output format:\n\n- Produce a single line with a flat list of integers giving, in order, the counts of accepted targets for Test Case $1$ thresholds, then Test Case $2$ thresholds, then Test Case $3$ thresholds, exactly as $[\\text{tc1\\_a1},\\text{tc1\\_a2},\\text{tc2\\_a1},\\text{tc2\\_a2},\\text{tc2\\_a3},\\text{tc3\\_a1}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef empirical_p_values(scores, decoy_scores):\n    \"\"\"\n    Compute calibrated p-values for given raw scores using the empirical decoy tail.\n    p(s) = #{decoy = s} / n_decoy\n    \"\"\"\n    decoy_scores = np.asarray(decoy_scores, dtype=float)\n    n_decoy = decoy_scores.size\n    # Sort decoys for efficient tail counting using binary search\n    decoy_sorted = np.sort(decoy_scores)\n    # For each s, count how many decoys are = s using searchsorted on ascending array\n    # Number = s = n_decoy - index of first element  (s - epsilon)\n    # We use 'left' on s to get index of first = s, then tail count is n_decoy - idx.\n    def tail_count(s):\n        idx = np.searchsorted(decoy_sorted, s, side='left')\n        return n_decoy - idx\n    # Vectorize tail_count over scores\n    scores = np.asarray(scores, dtype=float)\n    tail_counts = np.array([tail_count(s) for s in scores], dtype=float)\n    pvals = tail_counts / float(n_decoy)\n    return pvals\n\ndef target_decoy_q_values(target_pvals, decoy_pvals):\n    \"\"\"\n    Compute q-values for targets using the target-decoy FDR estimator:\n    FDR(p_(k)) = D(p_(k)) / k where D counts decoy p-values = threshold.\n    q_(k) = min_{j=k} FDR(p_(j)) to enforce monotonicity.\n    \"\"\"\n    target_pvals = np.asarray(target_pvals, dtype=float)\n    decoy_pvals = np.asarray(decoy_pvals, dtype=float)\n\n    # Sort target p-values ascending and keep indices to map back\n    sort_idx = np.argsort(target_pvals)\n    sorted_target_p = target_pvals[sort_idx]\n\n    # Pre-sort decoy p-values ascending for cumulative counting\n    decoy_sorted_p = np.sort(decoy_pvals)\n\n    # For each sorted target threshold t, compute D(t) = # decoy p = t using searchsorted with 'right'\n    D = np.searchsorted(decoy_sorted_p, sorted_target_p, side='right').astype(float)\n    # T(k) = k for ranks 1..m\n    m = sorted_target_p.size\n    T = np.arange(1, m + 1, dtype=float)\n\n    # Estimated FDR at each threshold\n    fdr = D / T\n\n    # Compute suffix minimum to get monotone q-values in sorted order\n    # q_sorted[k] = min_{j=k} fdr[j]\n    # Use cumulative minimum on reversed array, then reverse back\n    rev_cummin = np.minimum.accumulate(fdr[::-1])\n    q_sorted = rev_cummin[::-1]\n\n    # Map back to original order\n    q_vals = np.empty_like(q_sorted)\n    q_vals[sort_idx] = q_sorted\n\n    return q_vals\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"target_scores\": [2.1, 3.0, 1.5, 2.7, 3.5, 0.9, 1.2],\n            \"decoy_scores\":  [0.5, 1.0, 1.4, 2.0, 2.3, 2.6, 2.9, 3.2],\n            \"alphas\":        [0.05, 0.10],\n        },\n        # Test Case 2\n        {\n            \"target_scores\": [4.0, 3.8, 3.6, 3.5],\n            \"decoy_scores\":  [0.2, 0.3, 0.4, 0.1, 0.25],\n            \"alphas\":        [0.01, 0.05, 0.10],\n        },\n        # Test Case 3\n        {\n            \"target_scores\": [1.0, 1.0, 0.5],\n            \"decoy_scores\":  [1.0, 0.5, 0.5],\n            \"alphas\":        [0.20],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        target_scores = case[\"target_scores\"]\n        decoy_scores = case[\"decoy_scores\"]\n        alphas = case[\"alphas\"]\n\n        # Step 1: Calibrate scores to p-values using empirical decoy tail\n        target_p = empirical_p_values(target_scores, decoy_scores)\n        decoy_p = empirical_p_values(decoy_scores, decoy_scores)\n\n        # Step 2: Compute q-values for targets using target-decoy FDR estimator\n        q_vals = target_decoy_q_values(target_p, decoy_p)\n\n        # Step 3: For each alpha threshold, count accepted targets (q = alpha)\n        for alpha in alphas:\n            count = int(np.sum(q_vals = float(alpha)))\n            results.append(count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a peptide has been confidently identified, its inherent physicochemical properties govern its behavior within the mass spectrometer. This exercise bridges the gap between fundamental biochemistry and the practicalities of instrumentation by focusing on a peptide's charge state. You will apply the Henderson-Hasselbalch equation to predict the average charge of a peptide at a specific pH, a key parameter that influences its ionization efficiency, selection for fragmentation, and ultimately its detection and quantification in a Liquid Chromatography-Mass Spectrometry (LC-MS) experiment .",
            "id": "4994707",
            "problem": "In a translational medicine biomarker discovery workflow using Liquid Chromatography–Mass Spectrometry (LC–MS), the charge state distribution of tryptic peptides critically impacts precursor selection and fragmentation efficiency. Consider a candidate plasma biomarker peptide that, after enzymatic digestion, yields a peptide with $n_{K} = 2$ lysine residues, $n_{R} = 1$ arginine residue, $n_{H} = 3$ histidine residues, and an unmodified free $N$-terminus ($n_{N} = 1$). Assume each protonatable site behaves independently and can accept at most one proton, with no cooperative effects and negligible ionic strength. The solvent prior to ionization is a near-physiological buffer at $\\mathrm{pH} = 7.40$. Use the following acid dissociation constants (expressed as $\\mathrm{p}K_a$) for the protonatable groups: lysine side chain $\\mathrm{p}K_{a,K} = 10.50$, arginine side chain $\\mathrm{p}K_{a,R} = 12.50$, histidine side chain $\\mathrm{p}K_{a,H} = 6.00$, and peptide $N$-terminus $\\mathrm{p}K_{a,N} = 8.00$.\n\nStarting from the definitions of acidity and basicity and the relationship between $\\mathrm{p}H$, $\\mathrm{p}K_a$, and the base-to-conjugate-acid concentration ratio (Henderson–Hasselbalch), derive the fractional protonation $\\,\\alpha_i\\,$ for a generic protonatable site $i$ and then compute the expected positive-mode charge state $q_{\\mathrm{avg}}$ of this peptide at $\\mathrm{p}H = 7.40$ by summing the expected number of protons bound across all protonatable basic sites:\n$$q_{\\mathrm{avg}} = n_{K}\\,\\alpha_{K} + n_{R}\\,\\alpha_{R} + n_{H}\\,\\alpha_{H} + n_{N}\\,\\alpha_{N}.$$\nProvide the final expected charge state as a dimensionless number. Round your answer to four significant figures.",
            "solution": "The problem is first critically validated according to the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Number of lysine residues: $n_{K} = 2$\n- Number of arginine residues: $n_{R} = 1$\n- Number of histidine residues: $n_{H} = 3$\n- Number of unmodified free N-termini: $n_{N} = 1$\n- Solvent pH: $\\mathrm{pH} = 7.40$\n- Lysine side chain pKa: $\\mathrm{p}K_{a,K} = 10.50$\n- Arginine side chain pKa: $\\mathrm{p}K_{a,R} = 12.50$\n- Histidine side chain pKa: $\\mathrm{p}K_{a,H} = 6.00$\n- Peptide N-terminus pKa: $\\mathrm{p}K_{a,N} = 8.00$\n- Assumptions: Independent protonatable sites, each site accepts at most one proton, no cooperative effects, negligible ionic strength.\n- Formula for average charge: $q_{\\mathrm{avg}} = n_{K}\\,\\alpha_{K} + n_{R}\\,\\alpha_{R} + n_{H}\\,\\alpha_{H} + n_{N}\\,\\alpha_{N}$\n- Task: Derive the fractional protonation $\\alpha_i$ and compute $q_{\\mathrm{avg}}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly based on the principles of acid-base chemistry, specifically the Henderson-Hasselbalch equation, and its application to calculating the isoelectric point and charge state of peptides, which is a standard and fundamental concept in biochemistry and proteomics. The provided $\\mathrm{p}K_a$ values are realistic for amino acid residues in a peptide chain. The context of biomarker discovery using LC-MS is appropriate and scientifically sound.\n- **Well-Posed:** The problem is well-posed. It provides all necessary data (number of each type of residue, their respective $\\mathrm{p}K_a$ values, and the solvent $\\mathrm{pH}$) and a clear objective. The assumptions of independence and single protonation per site ensure that a unique and stable solution can be determined.\n- **Objective:** The problem is stated in precise, objective language, free of any subjective or opinion-based claims.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is scientifically sound, well-posed, and objective. The solution process may proceed.\n\n### Solution Derivation\n\nThe problem requires the derivation of the fractional protonation $\\alpha_i$ for a generic basic site and then the calculation of the total expected charge state $q_{\\mathrm{avg}}$ of the peptide.\n\n**1. Derivation of Fractional Protonation ($\\alpha_i$)**\n\nConsider a generic monoprotic basic site, $B$, in equilibrium with its conjugate acid, $BH^{+}$. The equilibrium represents the dissociation of the protonated form:\n$$BH^{+} \\rightleftharpoons B + H^{+}$$\nThe acid dissociation constant, $K_a$, for this equilibrium is defined as:\n$$K_a = \\frac{[B][H^{+}]}{[BH^{+}]}$$\nBy definition, $\\mathrm{p}K_a = -\\log_{10}(K_a)$ and $\\mathrm{pH} = -\\log_{10}([H^{+}])$. Taking the negative base-$10$ logarithm of the $K_a$ expression yields:\n$$-\\log_{10}(K_a) = -\\log_{10}\\left(\\frac{[B][H^{+}]}{[BH^{+}]}\\right)$$\n$$\\mathrm{p}K_a = -\\log_{10}([H^{+}]) - \\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right)$$\n$$\\mathrm{p}K_a = \\mathrm{pH} - \\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right)$$\nThis rearranges to the Henderson-Hasselbalch equation:\n$$\\mathrm{pH} = \\mathrm{p}K_a + \\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right)$$\nThe fractional protonation, $\\alpha_i$, for a site $i$ is the fraction of the molecules that are in the protonated state, $BH^{+}$:\n$$\\alpha_i = \\frac{[BH^{+}]}{[B] + [BH^{+}]}$$\nFrom the Henderson-Hasselbalch equation, we can express the ratio of the base to the acid form:\n$$\\log_{10}\\left(\\frac{[B]}{[BH^{+}]}\\right) = \\mathrm{pH} - \\mathrm{p}K_a$$\n$$ \\frac{[B]}{[BH^{+}]} = 10^{\\mathrm{pH} - \\mathrm{p}K_a} $$\n$$ [B] = [BH^{+}] \\cdot 10^{\\mathrm{pH} - \\mathrm{p}K_a} $$\nSubstituting this expression for $[B]$ into the equation for $\\alpha_i$:\n$$\\alpha_i = \\frac{[BH^{+}]}{([BH^{+}] \\cdot 10^{\\mathrm{pH} - \\mathrm{p}K_a}) + [BH^{+}]}$$\nFactoring out $[BH^{+}]$ from the denominator and canceling the term gives the final expression for fractional protonation:\n$$\\alpha_i = \\frac{1}{10^{\\mathrm{pH} - \\mathrm{p}K_a} + 1}$$\n\n**2. Calculation of Specific Fractional Protonations**\n\nUsing the derived formula and the given values ($\\mathrm{pH} = 7.40$), we calculate the fractional protonation for each type of basic site.\n\n- **Lysine (K):** $\\mathrm{p}K_{a,K} = 10.50$\n$$\\alpha_{K} = \\frac{1}{10^{7.40 - 10.50} + 1} = \\frac{1}{10^{-3.10} + 1} \\approx \\frac{1}{0.0007943 + 1} \\approx 0.9992062$$\n\n- **Arginine (R):** $\\mathrm{p}K_{a,R} = 12.50$\n$$\\alpha_{R} = \\frac{1}{10^{7.40 - 12.50} + 1} = \\frac{1}{10^{-5.10} + 1} \\approx \\frac{1}{0.000007943 + 1} \\approx 0.9999921$$\n\n- **Histidine (H):** $\\mathrm{p}K_{a,H} = 6.00$\n$$\\alpha_{H} = \\frac{1}{10^{7.40 - 6.00} + 1} = \\frac{1}{10^{1.40} + 1} \\approx \\frac{1}{25.11886 + 1} \\approx 0.0382864$$\n\n- **N-terminus (N):** $\\mathrm{p}K_{a,N} = 8.00$\n$$\\alpha_{N} = \\frac{1}{10^{7.40 - 8.00} + 1} = \\frac{1}{10^{-0.60} + 1} \\approx \\frac{1}{0.2511886 + 1} \\approx 0.7992440$$\n\n**3. Calculation of the Expected Charge State ($q_{\\mathrm{avg}}$)**\n\nThe total expected charge state is the sum of the expected number of protons bound across all protonatable sites. This is calculated using the formula provided in the problem statement and the numbers of each residue: $n_{K} = 2$, $n_{R} = 1$, $n_{H} = 3$, $n_{N} = 1$.\n\n$$q_{\\mathrm{avg}} = n_{K}\\,\\alpha_{K} + n_{R}\\,\\alpha_{R} + n_{H}\\,\\alpha_{H} + n_{N}\\,\\alpha_{N}$$\n$$q_{\\mathrm{avg}} = (2)(\\alpha_{K}) + (1)(\\alpha_{R}) + (3)(\\alpha_{H}) + (1)(\\alpha_{N})$$\nSubstituting the calculated values for each $\\alpha_i$:\n$$q_{\\mathrm{avg}} \\approx (2)(0.9992062) + (1)(0.9999921) + (3)(0.0382864) + (1)(0.7992440)$$\n$$q_{\\mathrm{avg}} \\approx 1.9984124 + 0.9999921 + 0.1148592 + 0.7992440$$\n$$q_{\\mathrm{avg}} \\approx 3.9125077$$\nThe problem requires the answer to be rounded to four significant figures.\n$$q_{\\mathrm{avg}} \\approx 3.913$$\nThe expected positive-mode charge state of the peptide at $\\mathrm{pH} = 7.40$ is approximately $+3.913$.",
            "answer": "$$\\boxed{3.913}$$"
        },
        {
            "introduction": "The ultimate goal of a biomarker is to accurately classify individuals into distinct clinical groups, such as 'disease present' versus 'disease absent'. This practice introduces a robust and standard methodology for evaluating this capability: Receiver Operating Characteristic (ROC) analysis. You will learn to construct an ROC curve from a set of classifier scores, calculate the Area Under the Curve (AUC) to quantify overall discriminatory power, and delve into the crucial distinction between a model's discrimination and its calibration. Mastering this technique provides a powerful, threshold-independent framework for assessing and comparing the performance of candidate biomarkers .",
            "id": "4994751",
            "problem": "A translational proteomics team is validating a candidate biomarker panel measured by Liquid Chromatography–Tandem Mass Spectrometry (LC-MS/MS) to discriminate a clinically defined disease state from controls. A probabilistic classifier trained on peptide intensity features outputs ranked predicted probabilities for blinded validation samples. For the validation set, the known clinical status and model output probabilities are:\n\n- Disease present (positives): $\\{0.93,\\,0.85,\\,0.77,\\,0.52,\\,0.40,\\,0.30\\}$.\n- Disease absent (negatives): $\\{0.88,\\,0.68,\\,0.35,\\,0.25,\\,0.15,\\,0.05\\}$.\n\nUsing the foundational definitions that the True Positive Rate (TPR, sensitivity) is $\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$ and the False Positive Rate (FPR, $1-$specificity) is $\\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{FP}+\\mathrm{TN}}$, and that the Receiver Operating Characteristic (ROC) curve is the parametric plot of $\\left(\\mathrm{FPR}(\\tau),\\,\\mathrm{TPR}(\\tau)\\right)$ as the decision threshold $\\tau$ on the predicted probability is swept from high to low, perform the following:\n\n- Derive the ROC curve from first principles by sweeping $\\tau$ across the set of unique model outputs, and explicitly enumerate the sequence of $\\left(\\mathrm{FPR},\\,\\mathrm{TPR}\\right)$ points.\n- Compute the Area Under the Curve (AUC) as the integral of $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ over $\\mathrm{FPR}\\in[0,1]$, evaluating it exactly for the given data.\n\nAdditionally, for clinical decision-making, briefly interpret discrimination versus calibration in this context. Assume a hypothetical recalibration of predicted probabilities via the monotonic mapping $p' = \\sigma\\!\\left(a + b\\,\\sigma^{-1}(p)\\right)$ with $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$, $a = 0$, and $b = 2$. Discuss, without performing any new numerical computation, whether this recalibration would change the ROC curve or the AUC, and explain the implications for threshold selection under unequal misclassification costs.\n\nExpress the final AUC as an exact fraction or a decimal number. Do not include any units and do not use a percentage sign. If you choose a decimal, you may report it exactly without rounding requirements.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It provides sufficient data and clear definitions to derive a unique solution for the requested calculations and a well-reasoned discussion for the conceptual parts. The problem is a standard exercise in the evaluation of binary classifiers, a core task in translational medicine and biomarker discovery.\n\n### Part 1: Derivation of the ROC Curve\n\nThe Receiver Operating Characteristic (ROC) curve is a plot of the True Positive Rate (TPR) against the False Positive Rate (FPR) at various decision threshold settings.\n\nThe given data are:\n-   Probabilities for the disease-present (positive) class, $P$: $\\{0.93, 0.85, 0.77, 0.52, 0.40, 0.30\\}$. The total number of positives is $N_P = 6$.\n-   Probabilities for the disease-absent (negative) class, $N$: $\\{0.88, 0.68, 0.35, 0.25, 0.15, 0.05\\}$. The total number of negatives is $N_N = 6$.\n\nThe TPR and FPR are defined as:\n$$ \\mathrm{TPR} = \\frac{\\mathrm{TP}}{N_P} \\quad \\text{and} \\quad \\mathrm{FPR} = \\frac{\\mathrm{FP}}{N_N} $$\nwhere TP (True Positives) is the number of positive samples with a score greater than or equal to the threshold $\\tau$, and FP (False Positives) is the number of negative samples with a score $\\ge \\tau$.\n\nTo construct the ROC curve, we consider each unique score value as a potential threshold. We sort all unique scores in descending order:\n$S = \\{0.93, 0.88, 0.85, 0.77, 0.68, 0.52, 0.40, 0.35, 0.30, 0.25, 0.15, 0.05\\}$.\n\nWe sweep the threshold $\\tau$ from a value greater than the maximum score (e.g., $\\tau=1$) down to a value less than or equal to the minimum score.\n\n1.  For $\\tau  0.93$: No sample is classified as positive. $\\mathrm{TP}=0$, $\\mathrm{FP}=0$. Thus, $(\\mathrm{FPR}, \\mathrm{TPR}) = (0, 0)$. This is the starting point of the ROC curve.\n\n2.  As we lower the threshold past each score, we update TP or FP. Each time the threshold crosses a positive sample's score, TP increases by $1$, and TPR increases by $1/N_P = 1/6$. Each time it crosses a negative sample's score, FP increases by $1$, and FPR increases by $1/N_N = 1/6$.\n\nThe sequence of points $(\\mathrm{FPR}, \\mathrm{TPR})$ is generated as the threshold is lowered:\n-   $\\tau = 0.93$: A positive score is crossed. TP becomes $1$. Point: $(\\frac{0}{6}, \\frac{1}{6}) = (0, \\frac{1}{6})$.\n-   $\\tau = 0.88$: A negative score is crossed. FP becomes $1$. Point: $(\\frac{1}{6}, \\frac{1}{6})$.\n-   $\\tau = 0.85$: A positive score is crossed. TP becomes $2$. Point: $(\\frac{1}{6}, \\frac{2}{6}) = (\\frac{1}{6}, \\frac{1}{3})$.\n-   $\\tau = 0.77$: A positive score is crossed. TP becomes $3$. Point: $(\\frac{1}{6}, \\frac{3}{6}) = (\\frac{1}{6}, \\frac{1}{2})$.\n-   $\\tau = 0.68$: A negative score is crossed. FP becomes $2$. Point: $(\\frac{2}{6}, \\frac{3}{6}) = (\\frac{1}{3}, \\frac{1}{2})$.\n-   $\\tau = 0.52$: A positive score is crossed. TP becomes $4$. Point: $(\\frac{2}{6}, \\frac{4}{6}) = (\\frac{1}{3}, \\frac{2}{3})$.\n-   $\\tau = 0.40$: A positive score is crossed. TP becomes $5$. Point: $(\\frac{2}{6}, \\frac{5}{6}) = (\\frac{1}{3}, \\frac{5}{6})$.\n-   $\\tau = 0.35$: A negative score is crossed. FP becomes $3$. Point: $(\\frac{3}{6}, \\frac{5}{6}) = (\\frac{1}{2}, \\frac{5}{6})$.\n-   $\\tau = 0.30$: A positive score is crossed. TP becomes $6$. Point: $(\\frac{3}{6}, \\frac{6}{6}) = (\\frac{1}{2}, 1)$.\n-   $\\tau = 0.25$: A negative score is crossed. FP becomes $4$. Point: $(\\frac{4}{6}, \\frac{6}{6}) = (\\frac{2}{3}, 1)$.\n-   $\\tau = 0.15$: A negative score is crossed. FP becomes $5$. Point: $(\\frac{5}{6}, \\frac{6}{6}) = (\\frac{5}{6}, 1)$.\n-   $\\tau = 0.05$: A negative score is crossed. FP becomes $6$. Point: $(\\frac{6}{6}, \\frac{6}{6}) = (1, 1)$. This is the final point.\n\nThe explicit sequence of $(\\mathrm{FPR}, \\mathrm{TPR})$ vertices for the ROC curve, starting from $(0,0)$ and ending at $(1,1)$, is:\n$$ (0,0) \\to (0, \\frac{1}{6}) \\to (\\frac{1}{6}, \\frac{1}{6}) \\to (\\frac{1}{6}, \\frac{1}{3}) \\to (\\frac{1}{6}, \\frac{1}{2}) \\to (\\frac{1}{3}, \\frac{1}{2}) \\to (\\frac{1}{3}, \\frac{2}{3}) \\to (\\frac{1}{3}, \\frac{5}{6}) \\to (\\frac{1}{2}, \\frac{5}{6}) \\to (\\frac{1}{2}, 1) \\to (\\frac{2}{3}, 1) \\to (\\frac{5}{6}, 1) \\to (1,1) $$\n\n### Part 2: Computation of the Area Under the Curve (AUC)\n\nThe AUC is the integral of the ROC curve, $\\int_0^1 \\mathrm{TPR}(\\mathrm{FPR}) \\, d\\mathrm{FPR}$. For an empirical ROC curve constructed from discrete data, this integral is calculated by summing the areas of the trapezoids formed by the vertices. The area of a trapezoid defined by points $(x_{i-1}, y_{i-1})$ and $(x_i, y_i)$ is $\\frac{1}{2}(y_{i-1} + y_i)(x_i - x_{i-1})$.\n\nThe ROC curve consists of horizontal and vertical segments. The area is accumulated only by the horizontal segments where $\\mathrm{FPR}$ changes.\n-   From $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=1/6$, $\\mathrm{TPR}$ is constant at $1/6$. Area = $(\\frac{1}{6} - 0) \\times \\frac{1}{6} = \\frac{1}{36}$. (This assumes step-like behavior, an alternative is the trapezoidal rule which we will use).\n\nUsing the trapezoidal rule on the vertices:\n-   Points $(\\frac{1}{6}, \\frac{1}{6})$ to $(\\frac{1}{6}, \\frac{1}{2})$: These are vertical segments where $\\Delta\\mathrm{FPR}=0$, contributing no area.\n-   Points $(\\frac{1}{6}, \\frac{1}{2})$ to $(\\frac{1}{3}, \\frac{1}{2})$: horizontal segment. Area = $\\frac{1}{2}(\\frac{1}{2}+\\frac{1}{2})(\\frac{1}{3}-\\frac{1}{6}) = \\frac{1}{2}(1)(\\frac{1}{6}) = \\frac{1}{12}$.\n-   Points $(\\frac{1}{3}, \\frac{1}{2})$ to $(\\frac{1}{3}, \\frac{5}{6})$: vertical segment, zero area.\n-   And so on.\n\nA more direct method is to sum the areas of rectangles under the curve.\n-   Area$_1$: width $\\frac{1}{6}$, height $\\frac{1}{6}$. Area = $\\frac{1}{36}$.\n-   Area$_2$: width $\\frac{1}{6}$, height $\\frac{3}{6}$. Area = $(\\frac{2}{6}-\\frac{1}{6}) \\times \\frac{3}{6} = \\frac{1}{6} \\times \\frac{3}{6} = \\frac{3}{36}$.\n-   Area$_3$: width $\\frac{1}{6}$, height $\\frac{5}{6}$. Area = $(\\frac{3}{6}-\\frac{2}{6}) \\times \\frac{5}{6} = \\frac{1}{6} \\times \\frac{5}{6} = \\frac{5}{36}$.\n-   Area$_4$: width $\\frac{3}{6}$, height $1$. Area = $(1-\\frac{3}{6}) \\times 1 = \\frac{3}{6} = \\frac{18}{36}$.\nTotal Area = $\\frac{1}{36} + \\frac{3}{36} + \\frac{5}{36} + \\frac{18}{36} = \\frac{27}{36}$.\n\nAn equivalent and robust method is using the Wilcoxon-Mann-Whitney U statistic formulation, where the AUC is the probability that a randomly chosen positive sample has a higher score than a randomly chosen negative sample, $P(S_P  S_N)$. There are $N_P \\times N_N = 6 \\times 6 = 36$ possible pairs of (positive, negative) samples.\nWe count the number of pairs $(p, n)$ where $p \\in P$, $n \\in N$, and $p  n$.\n-   For $p=0.93$: $6$ pairs ($0.93 $ all $6$ negative scores).\n-   For $p=0.85$: $5$ pairs ($0.85 $ all negative scores except $0.88$).\n-   For $p=0.77$: $5$ pairs ($0.77 $ all negative scores except $0.88$).\n-   For $p=0.52$: $4$ pairs ($0.52  \\{0.35, 0.25, 0.15, 0.05\\}$).\n-   For $p=0.40$: $4$ pairs ($0.40  \\{0.35, 0.25, 0.15, 0.05\\}$).\n-   For $p=0.30$: $3$ pairs ($0.30  \\{0.25, 0.15, 0.05\\}$).\nThe total number of concordant pairs is $6 + 5 + 5 + 4 + 4 + 3 = 27$.\nThere are no ties between positive and negative scores.\nThe AUC is therefore:\n$$ \\mathrm{AUC} = \\frac{27}{36} = \\frac{3}{4} = 0.75 $$\n\n### Part 3: Interpretation of Discrimination and Calibration\n\n-   **Discrimination** refers to a model's ability to distinguish between classes. A model with good discrimination assigns higher scores to positive instances than to negative instances. The ROC curve and its associated AUC are pure measures of discrimination. They are constructed based on the rank ordering of scores. An AUC of $1.0$ represents perfect discrimination, while an AUC of $0.5$ represents a model with no better-than-random discrimination.\n\n-   **Calibration** refers to the degree to which a model's predicted probabilities reflect the true likelihood of an event. A model is perfectly calibrated if, among all instances where it predicts a probability of $p$, the true fraction of positive instances is also $p$. Calibration is about the absolute meaning of the predicted values, not just their rank. A model can have excellent discrimination (high AUC) but be poorly calibrated (e.g., systematically over- or under-confident).\n\nIn summary, discrimination is about separating classes, while calibration is about the trustworthiness of the probability scores themselves.\n\n### Part 4: Effect of Recalibration\n\nThe proposed recalibration is a monotonic mapping $p' = \\sigma(a + b\\,\\sigma^{-1}(p))$. With $a=0$ and $b=2$, the mapping is $p' = \\sigma(2\\,\\sigma^{-1}(p))$. The sigmoid function $\\sigma(x) = (1+\\exp(-x))^{-1}$ and its inverse, the logit function $\\sigma^{-1}(p) = \\ln(p/(1-p))$, are both strictly monotonically increasing. Since $b = 2  0$, the transformation $x \\mapsto 2x$ is also strictly increasing. The composition of strictly monotonically increasing functions is itself strictly monotonically increasing.\n\n-   **Effect on ROC Curve and AUC**: The ROC curve and AUC are measures of discrimination and depend only on the rank ordering of the scores. Since the recalibration function $p \\mapsto p'$ is strictly monotonic, it preserves the rank order of the predicted probabilities. If $p_1  p_2$, then $p'_1  p'_2$. Consequently, the ROC curve will be **identical** to the original one, and the AUC will be **unchanged**. The recalibration improves calibration without affecting discrimination.\n\n-   **Implications for Threshold Selection**: Clinical decisions often involve unequal costs for different types of misclassification (False Positives vs. False Negatives). The optimal decision threshold $\\tau^*$ depends on these costs. For example, to minimize expected cost, one might classify as positive if the predicted probability $p$ exceeds a threshold related to the cost ratio, e.g., $\\tau^* = \\frac{C_{FP}}{C_{FP} + C_{FN}}$, where $C_{FP}$ and $C_{FN}$ are the costs of a false positive and false negative, respectively. This formula assumes the probabilities are well-calibrated. If the original probabilities $p$ are not calibrated, applying a theoretically derived threshold is incorrect. Recalibration aims to make the probabilities $p'$ reliable. By using the recalibrated probabilities $p'$, a decision-maker can apply a threshold derived from cost-benefit analysis with greater confidence that it will lead to optimal outcomes. Thus, while recalibration does not change the overall discriminatory power (AUC), it is crucial for making well-informed, cost-sensitive decisions at the individual level by ensuring the probabilities used for thresholding are meaningful.",
            "answer": "$$\\boxed{0.75}$$"
        }
    ]
}