{
    "hands_on_practices": [
        {
            "introduction": "To truly master differential expression analysis, it's essential to look \"under the hood\" of common software packages. This practice demystifies the core engine of tools like DESeq2 by having you implement the fitting procedure for a Negative Binomial Generalized Linear Model (NB-GLM) . By coding the Iteratively Reweighted Least Squares (IRLS) algorithm and performing a Wald test, you will gain a first-principles understanding of how coefficients and their statistical significance are derived from raw count data.",
            "id": "4333087",
            "problem": "You are tasked with building a program that, for a single gene measured across two clinical groups, fits a Negative Binomial Generalized Linear Model (NB-GLM) and performs a Wald test on the group coefficient to assess differential expression. This lies at the core of precision medicine and genomic diagnostics where Ribonucleic Acid (RNA) sequencing counts are modeled using appropriate count distributions and group effects are inferred. The program must adhere to the following precise modeling and statistical framework and produce results in the specified output format.\n\nFundamental base and definitions:\n- Negative Binomial (NB) model with mean-variance relationship consistent with biological count data. Use the parameterization where the variance is $$\\operatorname{Var}(Y_i) = \\mu_i + \\phi \\mu_i^2,$$ where $Y_i$ is the count for observation $i$, $\\mu_i$ is the mean, and $\\phi$ is the dispersion parameter, with the equivalent \"size\" parameter $r$ defined by $$r = \\frac{1}{\\phi}.$$ Under this parameterization, the variance can also be written as $$\\operatorname{Var}(Y_i) = \\mu_i + \\frac{\\mu_i^2}{r}.$$\n- Generalized Linear Model (GLM) with a log link and an offset (a known exposure or library size factor). The mean satisfies $$\\log(\\mu_i) = \\log(o_i) + \\mathbf{x}_i^\\top \\boldsymbol{\\beta},$$ where $o_i$ is the offset for observation $i$, $\\mathbf{x}_i$ is the design vector, and $\\boldsymbol{\\beta}$ is the coefficient vector.\n- Two-group design: use $$\\mathbf{x}_i = [1, g_i]^\\top,$$ where $g_i \\in \\{0, 1\\}$ indicates group membership. The coefficient $\\beta_0$ is the intercept, and $\\beta_1$ is the group effect (the parameter of interest for differential expression).\n- Maximum Likelihood Estimation (MLE): Fit $\\boldsymbol{\\beta}$ by maximizing the NB log-likelihood subject to the above link and offset structure.\n- Wald test for the group coefficient: If $\\widehat{\\beta}_1$ is the maximum likelihood estimate and $\\operatorname{se}(\\widehat{\\beta}_1)$ is its estimated standard error from the Fisher information, then the Wald statistic is $$W = \\left(\\frac{\\widehat{\\beta}_1}{\\operatorname{se}(\\widehat{\\beta}_1)}\\right)^2,$$ and the corresponding two-sided $p$-value must be computed using the standard normal approximation.\n\nYour program must implement from the above definitions and principles the fitting of the NB-GLM with the log link and offsets, and then compute the Wald statistic $W$ and the two-sided $p$-value for $\\beta_1$.\n\nInput is embedded in the program as a test suite of four cases. For each case, you are provided a vector of counts $y$, a vector of offsets $o$, a binary group indicator vector $g$, and a dispersion $\\phi$. All vectors are aligned by observation index. Each case should be treated independently. No physical units are involved, and all outputs must be numeric. The test suite is:\n\n- Case $1$ (general case with moderate counts and heterogeneous offsets):\n  - $y = [14, 9, 11, 10, 8, 23, 29, 31, 18, 27]$\n  - $o = [1.10, 0.90, 1.05, 1.00, 0.95, 1.20, 1.30, 0.85, 1.10, 0.90]$\n  - $g = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]$\n  - $\\phi = 0.25$\n- Case $2$ (edge case with zeros and low counts):\n  - $y = [0, 0, 1, 0, 2, 3, 4, 0, 5, 2]$\n  - $o = [1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]$\n  - $g = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]$\n  - $\\phi = 0.50$\n- Case $3$ (high counts with heterogeneous offsets suggesting normalization effects):\n  - $y = [50, 60, 55, 48, 52, 58]$\n  - $o = [2.00, 1.80, 2.20, 3.00, 2.50, 3.50]$\n  - $g = [0, 0, 0, 1, 1, 1]$\n  - $\\phi = 0.10$\n- Case $4$ (near-null effect with small sample size and high dispersion):\n  - $y = [7, 8, 6, 9]$\n  - $o = [1.00, 1.00, 1.00, 1.00]$\n  - $g = [0, 0, 1, 1]$\n  - $\\phi = 1.00$\n\nYour program must, for each case, fit the NB-GLM under the above definitions, compute the Wald statistic $W$ and the two-sided $p$-value (expressed as a decimal), and then print a single line containing the results for all four cases in order, formatted as a comma-separated list enclosed in square brackets, where each element is the pair $[W, p]$ for one case. For example, the printed output format must be $$[[W_1, p_1],[W_2, p_2],[W_3, p_3],[W_4, p_4]].$$",
            "solution": "The problem requires the implementation of a Negative Binomial Generalized Linear Model (NB-GLM) to test for differential gene expression between two groups. The problem is well-posed, scientifically grounded in established biostatistical methods, and provides all necessary information for a unique solution. We will proceed with a full solution.\n\nOur objective is to fit the coefficients $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^\\top$ of the model and perform a Wald test on the group coefficient $\\widehat{\\beta}_1$.\n\nThe model is defined by two components:\n1.  The distribution of the count data $Y_i$ for each observation $i$, which is a Negative Binomial (NB) distribution with mean $\\mu_i$ and dispersion $\\phi$. The variance is given by the relation $\\operatorname{Var}(Y_i) = \\mu_i + \\phi \\mu_i^2$. The size parameter $r$ of the NB distribution is related to the dispersion by $r=1/\\phi$.\n2.  The relationship between the mean $\\mu_i$ and the model's linear predictor $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$. This is given by a log link function, incorporating an offset term $o_i$ (typically representing library size or exposure):\n    $$ \\log(\\mu_i) = \\log(o_i) + \\eta_i \\implies \\mu_i = o_i \\exp(\\eta_i) = o_i \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}) $$\n    The design vector for a two-group comparison is $\\mathbf{x}_i = [1, g_i]^\\top$, where $g_i \\in \\{0, 1\\}$ is the group indicator. Thus, the linear predictor is $\\eta_i = \\beta_0 + \\beta_1 g_i$.\n\nThe coefficient $\\beta_1$ represents the log-fold change in mean expression for group $1$ relative to group $0$. A test of the hypothesis $H_0: \\beta_1 = 0$ is a test for differential expression.\n\nThe coefficients $\\boldsymbol{\\beta}$ are estimated by maximizing the log-likelihood of the NB-GLM. The log-likelihood function for the entire dataset $\\mathbf{y} = [y_1, \\dots, y_N]^\\top$ is the sum of individual log-likelihoods:\n$$ \\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\ell_i(\\boldsymbol{\\beta}) $$\nwhere $\\ell_i$ is the log-probability of observing $y_i$ given $\\mu_i(\\boldsymbol{\\beta})$ and $\\phi$. The NB probability mass function parameterized by mean $\\mu$ and size $r=1/\\phi$ is:\n$$ P(Y=y | \\mu, r) = \\frac{\\Gamma(y+r)}{\\Gamma(y+1)\\Gamma(r)} \\left(\\frac{r}{r+\\mu}\\right)^r \\left(\\frac{\\mu}{r+\\mu}\\right)^y $$\nThe log-likelihood for observation $i$, ignoring terms constant with respect to $\\boldsymbol{\\beta}$, is:\n$$ \\ell_i(\\boldsymbol{\\beta}) \\propto y_i \\log(\\mu_i) - (y_i+r) \\log(\\mu_i+r) $$\nSubstituting $\\mu_i = o_i \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})$, we obtain the function to be maximized with respect to $\\boldsymbol{\\beta}$.\n\nThis maximization is performed numerically using the Iteratively Reweighted Least Squares (IRLS) algorithm, which is equivalent to a Newton-Raphson optimization. The IRLS algorithm iteratively solves a weighted least squares problem. At each iteration $t$:\n1.  Calculate the current linear predictors $\\eta^{(t)} = X \\boldsymbol{\\beta}^{(t)}$ and means $\\mu^{(t)}_i = o_i \\exp(\\eta_i^{(t)})$.\n2.  Calculate the working dependent variables:\n    $$ z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}} $$\n3.  Calculate the weights:\n    $$ w_i^{(t)} = \\frac{(\\partial \\mu_i / \\partial \\eta_i)^2}{\\operatorname{Var}(Y_i)} = \\frac{(\\mu_i^{(t)})^2}{\\mu_i^{(t)} + \\phi (\\mu_i^{(t)})^2} = \\frac{\\mu_i^{(t)}}{1 + \\phi \\mu_i^{(t)}} $$\n4.  Update the coefficients by solving the weighted least squares problem:\n    $$ \\boldsymbol{\\beta}^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} \\mathbf{z}^{(t)} $$\n    where $W^{(t)}$ is the diagonal matrix of weights $w_i^{(t)}$.\n\nThis process is repeated until the change in $\\boldsymbol{\\beta}$ is below a small tolerance. The initial value for $\\beta_1$ is set to $0$, and for $\\beta_0$ to $\\log(\\sum y_i / \\sum o_i)$.\n\nUpon convergence, we obtain the Maximum Likelihood Estimate (MLE) $\\widehat{\\boldsymbol{\\beta}}$. The asymptotic covariance matrix of $\\widehat{\\boldsymbol{\\beta}}$ is estimated by the inverse of the Fisher information matrix, evaluated at $\\widehat{\\boldsymbol{\\beta}}$:\n$$ \\operatorname{Cov}(\\widehat{\\boldsymbol{\\beta}}) = I(\\widehat{\\boldsymbol{\\beta}})^{-1} = (X^\\top \\widehat{W} X)^{-1} $$\nwhere $\\widehat{W}$ is the diagonal weight matrix computed using the final mean estimates $\\widehat{\\mu}_i$.\n\nThe standard error of the group coefficient estimate, $\\operatorname{se}(\\widehat{\\beta}_1)$, is the square root of the second diagonal element of this covariance matrix (corresponding to $\\beta_1$).\n\nThe Wald test statistic for the hypothesis $H_0: \\beta_1 = 0$ is then calculated as:\n$$ W = \\left( \\frac{\\widehat{\\beta}_1}{\\operatorname{se}(\\widehat{\\beta}_1)} \\right)^2 $$\nUnder the null hypothesis, $W$ follows a chi-squared distribution with $1$ degree of freedom ($\\chi^2_1$). The two-sided $p$-value is the probability of observing a statistic at least as extreme as $W$:\n$$ p \\text{-value} = P(\\chi^2_1 \\ge W) $$\nThis procedure is applied independently to each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the NB-GLM analysis for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([14, 9, 11, 10, 8, 23, 29, 31, 18, 27]),\n            np.array([1.10, 0.90, 1.05, 1.00, 0.95, 1.20, 1.30, 0.85, 1.10, 0.90]),\n            np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]),\n            0.25\n        ),\n        # Case 2\n        (\n            np.array([0, 0, 1, 0, 2, 3, 4, 0, 5, 2]),\n            np.array([1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00]),\n            np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]),\n            0.50\n        ),\n        # Case 3\n        (\n            np.array([50, 60, 55, 48, 52, 58]),\n            np.array([2.00, 1.80, 2.20, 3.00, 2.50, 3.50]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            0.10\n        ),\n        # Case 4\n        (\n            np.array([7, 8, 6, 9]),\n            np.array([1.00, 1.00, 1.00, 1.00]),\n            np.array([0, 0, 1, 1]),\n            1.00\n        )\n    ]\n\n    results = []\n    for y, o, g, phi in test_cases:\n        W, p = fit_nb_glm_and_test(y, o, g, phi)\n        results.append([W, p])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef fit_nb_glm_and_test(y, o, g, phi):\n    \"\"\"\n    Fits a Negative Binomial GLM and performs a Wald test on the group coefficient.\n\n    Args:\n        y (np.ndarray): Count data vector.\n        o (np.ndarray): Offset vector.\n        g (np.ndarray): Group indicator vector (0 or 1).\n        phi (float): Dispersion parameter.\n\n    Returns:\n        tuple[float, float]: A tuple containing the Wald statistic W and the p-value.\n    \"\"\"\n    # 1. Construct the design matrix X\n    # Column 0 is the intercept, Column 1 is the group effect.\n    X = np.stack([np.ones_like(g), g], axis=1)\n    \n    # 2. Initialize coefficients for IRLS\n    # Initialize beta_1 to 0 (no group effect)\n    # Initialize beta_0 to the log of the overall mean rate\n    sum_y = np.sum(y)\n    if sum_y == 0:\n        # Avoid log(0) if all counts are zero\n        beta0_init = -10 \n    else:\n        beta0_init = np.log(sum_y / np.sum(o))\n    beta = np.array([beta0_init, 0.0])\n\n    # 3. Perform Iteratively Reweighted Least Squares (IRLS)\n    max_iter = 50\n    tolerance = 1e-8\n    for i in range(max_iter):\n        eta = X @ beta\n        mu = o * np.exp(eta)\n        \n        # Calculate weights for WLS\n        weights = mu / (1 + phi * mu)\n        W_diag = np.diag(weights)\n        \n        # Calculate working dependent variable z\n        z = eta + (y - mu) / mu\n\n        # Store old beta for convergence check\n        beta_old = beta.copy()\n\n        # Update beta by solving the weighted least squares problem\n        # beta_new = inv(X.T @ W @ X) @ X.T @ W @ z\n        try:\n            XT_W = X.T @ W_diag\n            XT_W_X = XT_W @ X\n            XT_W_z = XT_W @ z\n            beta = np.linalg.solve(XT_W_X, XT_W_z)\n        except np.linalg.LinAlgError:\n            # If matrix is singular, use a pseudo-inverse\n            beta = np.linalg.pinv(XT_W_X) @ XT_W_z\n            \n        # Check for convergence\n        if np.sum(np.abs(beta - beta_old))  tolerance:\n            break\n\n    # 4. Perform Wald test on the group coefficient (beta_1)\n    # Get the final MLE estimates\n    beta_hat = beta\n    beta1_hat = beta_hat[1]\n    \n    # Recalculate final mu and weights at MLE\n    eta_hat = X @ beta_hat\n    mu_hat = o * np.exp(eta_hat)\n    weights_hat = mu_hat / (1 + phi * mu_hat)\n    W_hat_diag = np.diag(weights_hat)\n    \n    # Calculate Fisher Information Matrix I = X^T W X\n    fisher_info = X.T @ W_hat_diag @ X\n    \n    try:\n        # Covariance matrix is the inverse of the Fisher Information\n        cov_beta = np.linalg.inv(fisher_info)\n        # Variance of beta_1 is the second diagonal element\n        var_beta1 = cov_beta[1, 1]\n        \n        # Standard error of beta1_hat\n        se_beta1 = np.sqrt(var_beta1)\n        \n        if se_beta1  1e-10: # Avoid division by zero\n            wald_stat = 0.0\n        else:\n            # Wald statistic W = (beta1_hat / se_beta1)^2\n            wald_stat = (beta1_hat / se_beta1)**2\n            \n        # p-value from chi-squared distribution with 1 degree of freedom\n        p_value = chi2.sf(wald_stat, df=1)\n\n    except np.linalg.LinAlgError:\n        # If Fisher information matrix is singular, test is inconclusive.\n        # This can happen with sparse data or separation.\n        wald_stat = np.nan\n        p_value = np.nan\n\n    return wald_stat, p_value\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "A correctly specified model is the foundation of valid statistical inference. This simulation exercise provides a powerful, hands-on demonstration of the dangers of omitted-variable bias in genomic analysis . By simulating data with a strong batch effect and comparing a naive analysis to a correctly adjusted one, you will see firsthand how unmodeled technical artifacts can generate a flood of false discoveries, reinforcing the non-negotiable importance of accounting for confounding variables.",
            "id": "2385475",
            "problem": "Write a complete, runnable program that simulates gene expression data under a linear model with a strong batch effect and evaluates the impact of omitting the batch covariate during differential gene expression analysis. Your program must implement the following scientifically grounded and widely accepted base principles without relying on any shortcut formulas provided in the problem statement.\n\nBase principles to use:\n- Gene expression on an appropriate transformed scale (for example, a logarithmic scale) can be modeled by a linear model with Gaussian noise. For gene index $g$ and sample index $i$, let $y_{g i}$ denote the expression. The model includes a gene-specific baseline term, a condition term, a batch term, and an independent noise term.\n- Ordinary Least Squares (OLS) under the Gaussian noise assumption yields unbiased estimators in correctly specified linear models and provides test statistics for hypotheses on model coefficients that follow the Student's $t$-distribution under the null.\n- Multiple hypothesis testing across many genes should be controlled using the False Discovery Rate (FDR). Use the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$.\n\nSimulation design and hypothesis testing:\n- All genes share the same generative model structure and are simulated under the global null of no true differential expression between the two biological conditions. Specifically, for every gene $g$, the condition effect is $0$; the batch effect is shared across genes as a fixed shift between two batches; the baseline level may vary by gene; and the noise is independent and identically distributed Gaussian across samples within a gene, with constant variance.\n- The program must estimate, for each gene, the two-sided $p$-value for the null hypothesis that the condition coefficient is zero using:\n  1. A misspecified model that omits the batch covariate (condition-only analysis).\n  2. A correctly specified model that includes both condition and batch covariates (condition-plus-batch analysis).\n- Apply the Benjamini–Hochberg (BH) procedure at target level $q = 0.05$ across all genes in each analysis to obtain the number of discoveries (these are all false discoveries because the data are simulated under the global null).\n\nScientific rationale to be demonstrated:\n- When the condition and batch covariates are correlated (confounding) and the batch effect is strong, omitting the batch covariate induces bias in the estimated condition effect and inflates false discoveries, whereas including the batch covariate removes the bias and controls false discoveries near the target FDR level.\n- When the condition and batch covariates are orthogonal, omitting the batch covariate does not induce bias in the estimated condition effect; false discoveries should be close to the target FDR level even without adjustment.\n- As a boundary case, when the batch effect is zero, including or omitting the batch covariate should yield similar behavior.\n\nTest suite:\nSimulate $m = 2000$ genes in each case. Use independent baseline levels $\\mu_g \\sim \\mathcal{N}(0, 1)$ for $g = 1, \\dots, m$. For each case, generate independent Gaussian noise $\\varepsilon_{g i} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.5$.\n\nEncode the binary condition covariate as $x_i \\in \\{0, 1\\}$ for the two conditions, and the binary batch covariate as $z_i \\in \\{0, 1\\}$ for the two batches. The batch shift is a constant $\\gamma$ added for $z_i = 1$ and $0$ for $z_i = 0$. The true condition effect is $0$ for all genes. Use the following three cases, each with a specified sample composition across condition and batch:\n- Case $1$ (high confounding, strong batch): batch $1$ has $19$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $19$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $2$ (orthogonal design, strong batch): batch $1$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; batch $2$ has $10$ samples from condition $0$ and $10$ samples from condition $1$; $\\gamma = 1.5$; total samples $n = 40$.\n- Case $3$ (high confounding, no batch): batch $1$ has $9$ samples from condition $0$ and $1$ sample from condition $1$; batch $2$ has $1$ sample from condition $0$ and $9$ samples from condition $1$; $\\gamma = 0.0$; total samples $n = 20$.\n\nRandomness and reproducibility:\n- Use a fixed random seed of $20240513$ to initialize the generator for Case $1$. For Cases $2$ and $3$, you must add the case index to the base seed (that is, $20240513 + 2$ and $20240513 + 3$) to ensure independence across cases while retaining reproducibility.\n\nStatistical analysis requirements:\n- For each gene and each case, compute the two-sided $p$-value testing the null hypothesis that the condition coefficient equals $0$ using:\n  1. A condition-only linear model with an intercept.\n  2. A condition-plus-batch linear model with an intercept.\n- Use the Benjamini–Hochberg (BH) procedure at level $q = 0.05$ on the set of $m$ $p$-values within each analysis to determine the number of discoveries (rejections). Since all nulls are true by construction, every discovery is a false positive. Report the numbers of discoveries for both analyses.\n\nRequired final output format:\n- Your program should produce a single line of output containing $6$ comma-separated integers enclosed in square brackets, in the following order: $[\\text{case1\\_naive}, \\text{case1\\_adjusted}, \\text{case2\\_naive}, \\text{case2\\_adjusted}, \\text{case3\\_naive}, \\text{case3\\_adjusted}]$, where “naive” denotes the condition-only analysis and “adjusted” denotes the condition-plus-batch analysis.\n\nNo external inputs:\n- The program must be entirely self-contained and must not read any input or files or access any network resources. All numerical values must be hard coded as specified above.\n\nAngle units and physical units:\n- No angles or physical units are involved.\n\nAnswer type:\n- Each of the $6$ reported values must be an integer.\n\nYour goal is to implement the simulation and analysis so that the output demonstrates, through these test cases, how omitting a strong, confounded batch effect inflates false discoveries in differential gene expression analysis, while including the batch indicator restores valid inference.",
            "solution": "The problem presented is a valid, well-posed exercise in computational statistics, designed to demonstrate a critical principle in the analysis of high-throughput biological data: the danger of omitted-variable bias. The specific context is differential gene expression analysis, where unmeasured or unmodeled technical factors, such as experimental batches, can confound the biological signals of interest. The problem is scientifically grounded, using standard linear models, Ordinary Least Squares (OLS), Student's $t$-tests, and the Benjamini-Hochberg (BH) procedure for False Discovery Rate (FDR) control, which are foundational methods in the field. The simulation parameters are clearly specified, ensuring the problem is self-contained and reproducible. We shall proceed with the solution.\n\nThe core of the problem lies in the general linear model, which we write in matrix form for a single gene as $Y = X\\beta + \\epsilon$. Here, $Y$ is an $n \\times 1$ vector of expression measurements for $n$ samples, $X$ is an $n \\times p$ design matrix encoding the experimental covariates for the $p$ parameters, $\\beta$ is a $p \\times 1$ vector of coefficients to be estimated, and $\\epsilon$ is an $n \\times 1$ vector of independent and identically distributed error terms, assumed to follow a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$.\n\nThe data are simulated under a \"global null\" scenario, meaning there is no true differential expression. For each of $m=2000$ genes, indexed by $g$, and $n$ samples, indexed by $i$, the expression $y_{gi}$ is generated according to the true model:\n$$y_{gi} = \\mu_g + \\gamma z_i + \\varepsilon_{gi}$$\nwhere $\\mu_g \\sim \\mathcal{N}(0, 1)$ is the gene-specific baseline expression, $\\gamma$ is the magnitude of the batch effect, $z_i \\in \\{0, 1\\}$ is the batch indicator covariate, and $\\varepsilon_{gi} \\sim \\mathcal{N}(0, \\sigma^2)$ is the noise term with $\\sigma=0.5$. The true coefficient for the biological condition is zero.\n\nWe will analyze these simulated data using two different models for each gene:\n\n1.  A **misspecified or \"naive\" model**, which omits the batch covariate:\n    $y_{gi} = \\beta_{g0} + \\beta_{g1} x_i + e_{gi}$. The design matrix $X_{\\text{naive}}$ has two columns: an intercept (a vector of ones) and the condition covariate vector $x$. We test the null hypothesis $H_0: \\beta_{g1} = 0$.\n\n2.  A **correctly specified or \"adjusted\" model**, which includes the batch covariate:\n    $y_{gi} = \\beta'_{g0} + \\beta'_{g1} x_i + \\beta'_{g2} z_i + e'_{gi}$. The design matrix $X_{\\text{adj}}$ has three columns: an intercept, the condition vector $x$, and the batch vector $z$. We test the null hypothesis $H_0: \\beta'_{g1} = 0$.\n\nFor both models, the coefficients $\\beta$ are estimated using Ordinary Least Squares (OLS), for which the solution is $\\hat{\\beta} = (X^T X)^{-1}X^T Y$. This can be solved efficiently for all $m$ genes simultaneously.\n\nTo test the significance of the condition coefficient $\\hat{\\beta}_1$ (where the subscript $1$ denotes the coefficient for the condition covariate $x$), we compute the Student's $t$-statistic:\n$$t = \\frac{\\hat{\\beta}_1}{\\text{SE}(\\hat{\\beta}_1)}$$\nThe standard error, $\\text{SE}(\\hat{\\beta}_1)$, is the square root of the estimated variance of the coefficient estimator. The variance-covariance matrix of the estimators is given by $\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}$. We estimate the unknown error variance $\\sigma^2$ with its unbiased estimator, the mean squared error:\n$$\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{\\text{RSS}}{n-p}$$\nwhere $\\hat{y}_i$ are the fitted values from the model, $\\text{RSS}$ is the residual sum of squares, $n$ is the number of samples, and $p$ is the number of parameters in the model (the number of columns in $X$). Let $C = (X^T X)^{-1}$. The specific variance for $\\hat{\\beta}_1$ is $\\hat{\\sigma}^2 C_{11}$ (assuming the condition covariate is the second column of $X$, indexed by $1$). The standard error is thus $\\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\hat{\\sigma}^2 C_{11}}$. Under the null hypothesis, this $t$-statistic follows a Student's $t$-distribution with $n-p$ degrees of freedom. From this distribution, we compute a two-sided $p$-value for each gene.\n\nSince we are performing $m=2000$ hypothesis tests, one for each gene, we must correct for multiple testing to control the number of false discoveries. We will use the Benjamini-Hochberg (BH) procedure at a target False Discovery Rate (FDR) of $q = 0.05$. The procedure is as follows:\n1.  Sort the $m$ $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n2.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m} q$.\n3.  If such a $k$ exists, reject the null hypotheses for all tests corresponding to $p_{(1)}, \\dots, p_{(k)}$. The number of discoveries is $k$. If no such $k$ exists, no hypotheses are rejected, and the number of discoveries is $0$.\n\nWe will implement this entire process for the three specified cases, which differ in their experimental design (confounding vs. orthogonal) and the strength of the batch effect.\n\n-   **Case 1 (Confounding):** The condition and batch covariates are strongly correlated. When the batch effect $\\gamma$ is large, omitting the batch covariate $z$ will lead to a biased estimate of the condition coefficient $\\beta_1$. This bias, known as omitted-variable bias, is proportional to the true batch effect and the correlation between the batch and condition covariates. This bias will systematically shift the estimated condition effects away from zero, leading to a large number of small $p$-values and a massive inflation of false discoveries in the naive analysis. The adjusted model, by accounting for $z$, will remove this bias and provide valid inference, with the number of false discoveries properly controlled near the expected level.\n\n-   **Case 2 (Orthogonal):** The condition and batch covariates are uncorrelated (orthogonal design). In this scenario, the omitted-variable bias term is zero. Therefore, even when a strong batch effect is present, the estimate of the condition coefficient in the naive model remains unbiased. Both a naive and an adjusted analysis are expected to control the false discovery rate correctly.\n\n-   **Case 3 (Confounding, No Batch Effect):** The design is confounded as in Case 1, but the batch effect size $\\gamma$ is zero. The omitted-variable bias is proportional to $\\gamma$, so if $\\gamma=0$, there is no bias. Both the naive and adjusted models should perform correctly, similar to Case 2.\n\nThe program will systematically execute these simulations, perform both naive and adjusted analyses for each case, apply the BH procedure, and report the resulting number of false discoveries, thereby quantitatively demonstrating these fundamental statistical principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation suite and print the final results.\n    \"\"\"\n\n    # Global parameters\n    m = 2000  # Number of genes\n    sigma = 0.5  # Noise standard deviation\n    q_level = 0.05  # Target FDR level for BH procedure\n\n    # Case 1: High confounding, strong batch\n    case1_params = {\n        'n_cond0_batch1': 19, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 19\n    }\n    case1_gamma = 1.5\n    case1_seed = 20240513\n    \n    # Case 2: Orthogonal design, strong batch\n    case2_params = {\n        'n_cond0_batch1': 10, 'n_cond1_batch1': 10,\n        'n_cond0_batch2': 10, 'n_cond1_batch2': 10\n    }\n    case2_gamma = 1.5\n    case2_seed = 20240513 + 2\n\n    # Case 3: High confounding, no batch effect\n    case3_params = {\n        'n_cond0_batch1': 9, 'n_cond1_batch1': 1,\n        'n_cond0_batch2': 1, 'n_cond1_batch2': 9\n    }\n    case3_gamma = 0.0\n    case3_seed = 20240513 + 3\n    \n    test_cases = [\n        (case1_params, case1_gamma, m, sigma, q_level, case1_seed),\n        (case2_params, case2_gamma, m, sigma, q_level, case2_seed),\n        (case3_params, case3_gamma, m, sigma, q_level, case3_seed),\n    ]\n\n    results = []\n    for params in test_cases:\n        naive_discoveries, adjusted_discoveries = run_simulation(*params)\n        results.extend([naive_discoveries, adjusted_discoveries])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(case_params, gamma, m, sigma, q, seed):\n    \"\"\"\n    Runs a single simulation case.\n    \n    Generates data, performs naive and adjusted analyses, and returns the number of discoveries for each.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Construct design vectors (x for condition, z for batch)\n    n01 = case_params['n_cond0_batch1']\n    n11 = case_params['n_cond1_batch1']\n    n02 = case_params['n_cond0_batch2']\n    n12 = case_params['n_cond1_batch2']\n    \n    n_batch1 = n01 + n11\n    n_batch2 = n02 + n12\n    n = n_batch1 + n_batch2\n\n    x = np.array([0]*n01 + [1]*n11 + [0]*n02 + [1]*n12)\n    z = np.array([0]*n_batch1 + [1]*n_batch2)\n\n    # 2. Generate gene expression data\n    # True model: y = mu + gamma*z + noise\n    mu_g = np.random.normal(0, 1, size=(m, 1))\n    noise = np.random.normal(0, sigma, size=(m, n))\n    Y = mu_g + gamma * z[np.newaxis, :] + noise\n\n    # 3. Define design matrices\n    X_naive = np.vstack([np.ones(n), x]).T\n    X_adjusted = np.vstack([np.ones(n), x, z]).T\n\n    # 4. Perform analyses and get discovery counts\n    naive_discoveries = perform_analysis(Y, X_naive, q)\n    adjusted_discoveries = perform_analysis(Y, X_adjusted, q)\n\n    return naive_discoveries, adjusted_discoveries\n\ndef perform_analysis(Y, X, q):\n    \"\"\"\n    Performs OLS regression and multiple testing correction for a set of genes.\n    \"\"\"\n    n, p = X.shape # n = samples, p = parameters\n    m = Y.shape[0] # m = genes\n    \n    # 1. Fit linear model for all genes at once using np.linalg.lstsq\n    # Y is (m, n), X is (n, p). We need to solve X @ B.T = Y.T for B.\n    # B will be (m, p). lstsq returns coefficients as (p, m).\n    beta_hat, rss_per_gene, _, _ = np.linalg.lstsq(X, Y.T, rcond=None)\n\n    # 2. Calculate t-statistics for the condition coefficient (at index 1)\n    df = n - p\n    sigma_sq_hat = rss_per_gene / df\n    \n    # The variance of beta_hat is diag(inv(X'X)) * sigma_hat^2\n    # We are interested in the coefficient for the condition 'x', which is at index 1\n    C = np.linalg.inv(X.T @ X)\n    se_beta1 = np.sqrt(sigma_sq_hat * C[1, 1])\n\n    # Avoid division by zero if standard error is somehow zero\n    # This should not happen in this problem's setup\n    t_stats = np.zeros(m)\n    valid_se = se_beta1 > 0\n    t_stats[valid_se] = beta_hat[1, valid_se] / se_beta1[valid_se]\n    \n    # 3. Calculate two-sided p-values\n    p_values = 2 * t.sf(np.abs(t_stats), df=df)\n\n    # 4. Apply Benjamini-Hochberg procedure\n    num_discoveries = bh_procedure(p_values, q)\n    \n    return num_discoveries\n\ndef bh_procedure(p_values, q):\n    \"\"\"\n    Applies the Benjamini-Hochberg procedure to control FDR.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return 0\n        \n    p_values_sorted = np.sort(p_values)\n    ranks = np.arange(1, m + 1)\n    thresholds = (ranks / m) * q\n    \n    # Find all p-values that are below the BH threshold\n    significant_mask = p_values_sorted = thresholds\n    \n    if np.any(significant_mask):\n        # The number of discoveries is the rank of the last p-value\n        # that is below its threshold\n        k = np.max(np.where(significant_mask))\n        num_discoveries = k + 1\n    else:\n        num_discoveries = 0\n        \n    return num_discoveries\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Beyond identifying simple up- or down-regulation, a key goal of precision medicine is to understand how treatment effects vary across different populations. This practice bridges the gap between a nuanced biological question and a precise statistical hypothesis . You will derive the specific contrast vector needed to test for a differential treatment effect between sexes within a GLM, learning a crucial skill for designing and interpreting complex interaction studies.",
            "id": "4333021",
            "problem": "In a precision medicine study, you perform differential gene expression analysis of Ribonucleic Acid sequencing (RNA-seq) counts using a Negative Binomial Generalized Linear Model (GLM) with a log link and a sample-specific offset for library size. The design includes a binary treatment factor $T$ with levels $T=\\text{control}$ and $T=\\text{drug}$, a binary sex factor $\\text{Sex}$ with levels $\\text{Sex}=\\text{female}$ and $\\text{Sex}=\\text{male}$, and a three-level batch factor $\\text{Batch} \\in \\{ \\text{B}_1, \\text{B}_2, \\text{B}_3 \\}$ as a nuisance variable. The GLM linear predictor for a single gene is specified as\n$$\n\\eta_i \\equiv \\ln(\\mu_i) \\;=\\; \\beta_0 \\;+\\; \\beta_T \\, x_{i,T} \\;+\\; \\beta_S \\, x_{i,S} \\;+\\; \\beta_{TS} \\, (x_{i,T}\\,x_{i,S}) \\;+\\; \\beta_{B1} \\, x_{i,B1} \\;+\\; \\beta_{B2} \\, x_{i,B2} \\;+\\; \\ln(s_i),\n$$\nwhere $\\mu_i$ is the mean count for sample $i$, $s_i$ is its known library-size scaling factor (offset), and the columns are coded as follows:\n- Sum-to-zero coding for $T$: $x_{i,T} = +1$ if $T=\\text{drug}$ and $x_{i,T} = -1$ if $T=\\text{control}$.\n- Sum-to-zero coding for $\\text{Sex}$: $x_{i,S} = +1$ if $\\text{Sex}=\\text{male}$ and $x_{i,S} = -1$ if $\\text{Sex}=\\text{female}$.\n- Interaction column $x_{i,T}x_{i,S}$ is the elementwise product of the $T$ and $\\text{Sex}$ codes.\n- Sum-to-zero coding for $\\text{Batch}$ with baseline-free two columns $(x_{i,B1}, x_{i,B2})$ corresponding to contrasts among $\\text{B}_1,\\text{B}_2,\\text{B}_3$ (you may treat these as nuisance regressors that do not involve $T$ or $\\text{Sex}$).\n\nLet the regression coefficient vector be ordered as\n$$\n\\boldsymbol{\\beta} \\;=\\; \\begin{pmatrix} \\beta_0  \\beta_T  \\beta_S  \\beta_{TS}  \\beta_{B1}  \\beta_{B2} \\end{pmatrix}^{\\top}.\n$$\nDefine the sex-specific treatment effects on the log scale as the difference in the linear predictor between $T=\\text{drug}$ and $T=\\text{control}$ within each sex. Using only the stated GLM structure, factor codings, and the definition of the linear predictor, derive a single contrast row vector $\\mathbf{c} \\in \\mathbb{R}^{6}$, aligned with the coefficient order above, such that $\\mathbf{c}^{\\top}\\boldsymbol{\\beta}$ equals the difference between the male treatment effect and the female treatment effect (on the log scale). Provide the contrast that yields exactly that difference, not a scaled multiple of it. Express your final contrast as a $1 \\times 6$ row vector. No numerical rounding is required and no units should be reported.",
            "solution": "The problem is valid as it is scientifically grounded in standard statistical methodology for genomic data analysis (Negative Binomial GLM), is well-posed with a unique solution derivable from the provided information, and is stated using objective, formal language. It is free from any scientific flaws, ambiguities, or contradictions.\n\nThe objective is to find a contrast row vector $\\mathbf{c}$ such that the linear combination $\\mathbf{c}^{\\top}\\boldsymbol{\\beta}$ is equal to the difference between the male-specific treatment effect and the female-specific treatment effect, on the log scale.\n\nThe linear predictor for a sample $i$ is given by:\n$$\n\\eta_i = \\ln(\\mu_i) = \\beta_0 + \\beta_T x_{i,T} + \\beta_S x_{i,S} + \\beta_{TS} (x_{i,T}x_{i,S}) + \\beta_{B1} x_{i,B1} + \\beta_{B2} x_{i,B2} + \\ln(s_i)\n$$\nThe terms $\\beta_{B1} x_{i,B1} + \\beta_{B2} x_{i,B2}$ relate to the batch effect and the term $\\ln(s_i)$ is the offset for library size. When we calculate treatment effects, we are comparing expected log-means for different groups. These comparisons are made assuming the same batch and library size, so these terms will cancel out in the subtractions. Therefore, for our derivation, we only need to consider the part of the predictor involving the intercept and the factors of interest, $T$ and $\\text{Sex}$:\n$$\n\\eta'(x_T, x_S) = \\beta_0 + \\beta_T x_T + \\beta_S x_S + \\beta_{TS} (x_T x_S)\n$$\nThe coding for the factors is given as:\n- Treatment $T$: $x_T = +1$ for drug, $x_T = -1$ for control.\n- Sex: $x_S = +1$ for male, $x_S = -1$ for female.\n\nFirst, we determine the sex-specific treatment effects. The treatment effect is defined as the difference in the linear predictor between $T=\\text{drug}$ and $T=\\text{control}$.\n\nLet's calculate the male treatment effect (MTE). For males, $x_S = +1$.\nThe predictor for a male on the drug is $\\eta'(\\text{drug, male}) = \\eta'(x_T=+1, x_S=+1)$:\n$$\n\\eta'(+1, +1) = \\beta_0 + \\beta_T(+1) + \\beta_S(+1) + \\beta_{TS}(+1)(+1) = \\beta_0 + \\beta_T + \\beta_S + \\beta_{TS}\n$$\nThe predictor for a male on the control is $\\eta'(\\text{control, male}) = \\eta'(x_T=-1, x_S=+1)$:\n$$\n\\eta'(-1, +1) = \\beta_0 + \\beta_T(-1) + \\beta_S(+1) + \\beta_{TS}(-1)(+1) = \\beta_0 - \\beta_T + \\beta_S - \\beta_{TS}\n$$\nThe male treatment effect is the difference:\n$$\n\\text{MTE} = \\eta'(+1, +1) - \\eta'(-1, +1) = (\\beta_0 + \\beta_T + \\beta_S + \\beta_{TS}) - (\\beta_0 - \\beta_T + \\beta_S - \\beta_{TS}) = 2\\beta_T + 2\\beta_{TS}\n$$\nNext, we calculate the female treatment effect (FTE). For females, $x_S = -1$.\nThe predictor for a female on the drug is $\\eta'(\\text{drug, female}) = \\eta'(x_T=+1, x_S=-1)$:\n$$\n\\eta'(+1, -1) = \\beta_0 + \\beta_T(+1) + \\beta_S(-1) + \\beta_{TS}(+1)(-1) = \\beta_0 + \\beta_T - \\beta_S - \\beta_{TS}\n$$\nThe predictor for a female on the control is $\\eta'(\\text{control, female}) = \\eta'(x_T=-1, x_S=-1)$:\n$$\n\\eta'(-1, -1) = \\beta_0 + \\beta_T(-1) + \\beta_S(-1) + \\beta_{TS}(-1)(-1) = \\beta_0 - \\beta_T - \\beta_S + \\beta_{TS}\n$$\nThe female treatment effect is the difference:\n$$\n\\text{FTE} = \\eta'(+1, -1) - \\eta'(-1, -1) = (\\beta_0 + \\beta_T - \\beta_S - \\beta_{TS}) - (\\beta_0 - \\beta_T - \\beta_S + \\beta_{TS}) = 2\\beta_T - 2\\beta_{TS}\n$$\nThe problem asks for the quantity corresponding to the difference between the male treatment effect and the female treatment effect:\n$$\n\\text{Difference} = \\text{MTE} - \\text{FTE} = (2\\beta_T + 2\\beta_{TS}) - (2\\beta_T - 2\\beta_{TS}) = 4\\beta_{TS}\n$$\nThis quantity is the interaction effect, representing how the treatment effect changes between sexes. We want to find the contrast row vector $\\mathbf{c}$ such that $\\mathbf{c}^{\\top}\\boldsymbol{\\beta}$ equals this difference. The coefficient vector is given as $\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0  \\beta_T  \\beta_S  \\beta_{TS}  \\beta_{B1}  \\beta_{B2} \\end{pmatrix}^{\\top}$.\n\nLet the contrast row vector be $\\mathbf{c} = \\begin{pmatrix} c_0  c_T  c_S  c_{TS}  c_{B1}  c_{B2} \\end{pmatrix}$.\nThe problem states the desired linear combination is $\\mathbf{c}^{\\top}\\boldsymbol{\\beta}$. Assuming a notational inconsistency and that $\\mathbf{c}$ is the row vector, the linear combination is $\\mathbf{c}\\boldsymbol{\\beta}$. If we strictly follow the notation where $\\mathbf{c}$ is a column vector, then the vector we seek is the transpose of $\\mathbf{c}$. In either interpretation, the elements of the row vector are the coefficients of the linear combination.\n$$\n\\mathbf{c}\\boldsymbol{\\beta} = c_0\\beta_0 + c_T\\beta_T + c_S\\beta_S + c_{TS}\\beta_{TS} + c_{B1}\\beta_{B1} + c_{B2}\\beta_{B2}\n$$\nWe require this to be equal to $4\\beta_{TS}$.\n$$\nc_0\\beta_0 + c_T\\beta_T + c_S\\beta_S + c_{TS}\\beta_{TS} + c_{B1}\\beta_{B1} + c_{B2}\\beta_{B2} = 0\\cdot\\beta_0 + 0\\cdot\\beta_T + 0\\cdot\\beta_S + 4\\cdot\\beta_{TS} + 0\\cdot\\beta_{B1} + 0\\cdot\\beta_{B2}\n$$\nBy comparing the coefficients of each $\\beta_j$, we can identify the elements of the contrast vector $\\mathbf{c}$:\n$c_0 = 0$\n$c_T = 0$\n$c_S = 0$\n$c_{TS} = 4$\n$c_{B1} = 0$\n$c_{B2} = 0$\n\nTherefore, the contrast row vector is $\\begin{pmatrix} 0  0  0  4  0  0 \\end{pmatrix}$. This vector isolates $4$ times the interaction coefficient $\\beta_{TS}$, which, under sum-to-zero coding, represents the difference in treatment effects between the two sexes.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  0  0  4  0  0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}