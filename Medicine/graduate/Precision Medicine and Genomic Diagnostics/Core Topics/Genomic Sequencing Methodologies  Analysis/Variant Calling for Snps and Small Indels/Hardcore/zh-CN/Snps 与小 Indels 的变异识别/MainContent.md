## 引言
在[精准医疗](@entry_id:152668)和基因组研究的时代，准确识别个体DNA序列中的[单核苷酸多态性](@entry_id:173601)（SNP）和小片段插入缺失（Indel）是解锁遗传信息、理解疾病机制和指导个性化治疗的基石。然而，从海量的原始测[序数](@entry_id:150084)据中区分真实的遗传变异与普遍存在的测序错误和技术假象，是一个巨大的计算挑战。本文旨在系统性地剖析这一挑战，并提供一个从理论到实践的完整学习路径，以弥合基础统计原理与复杂生物学应用之间的知识鸿沟。

在接下来的章节中，我们将踏上一段深入的探索之旅。首先，在“原理与机制”部分，我们将揭示[变异识别](@entry_id:177461)背后的核心概率框架，从[贝叶斯定理](@entry_id:151040)的基础出发，详细拆解现代[变异识别](@entry_id:177461)算法（如GATK HaplotypeCaller）的内部工作流程。接着，在“应用与跨学科连接”部分，我们将展示这些理论如何在[癌症基因组学](@entry_id:143632)、药物基因组学等前沿领域中发挥关键作用，并应对各种真实世界的复杂情况。最后，通过“动手实践”环节，您将有机会亲手应用所学知识，通过解决具体的计算问题，来巩固和深化对[变异识别](@entry_id:177461)技术的理解。

## 原理与机制

本章将深入探讨用于检测[单核苷酸多态性 (SNP)](@entry_id:269310) 和小片段[插入缺失](@entry_id:173062) (Indel) 的变异识别技术背后的核心原理与关键机制。我们将从支撑[变异识别](@entry_id:177461)的统计学基础出发，逐步解析从原始测序数据到最终高质量变异检出的完整计算流程。本章内容将涵盖基因型似然的构建、群体遗传学[先验信息](@entry_id:753750)的整合、两种主流变异识别范式（基于堆积和基于单倍型）的对比、现代[变异识别](@entry_id:177461)算法的内部机制，以及对结果的精确解读和质量控制。

### 变异识别的核心概率框架

在根本上，变异识别是一个统计推断问题。其核心目标是根据观察到的测序数据 $D$（即比对到基因组特定位点的所有序列读段），推断出该样本在该位点最有可能的真实基因型 $G$。这一推断过程的数学基石是[贝叶斯定理](@entry_id:151040)，它将后验概率、似然和先验概率联系在一起：

$$P(G | D) \propto P(D | G) \times P(G)$$

这个表达式的含义是，给定观测数据后某个特定基因型的后验概率（$P(G | D)$），正比于该基因型的似然（$P(D | G)$）与该基因型的[先验概率](@entry_id:275634)（$P(G)$）的乘积。变异识别算法的本质就是为每个可能的基因型计算该乘积，并通常将具有最高后验概率的基因型作为最终的判断。下面，我们将分别解析似然和先验这两个关键组成部分。

#### 基因型似然模型：$P(D|G)$

**基因型似然**（genotype likelihood）是[变异识别](@entry_id:177461)[概率模型](@entry_id:265150)的核心。它回答了这样一个问题：“如果我们假设样本的真实基因型是 $G$，那么观察到我们手中这些测序数据 $D$ 的概率是多少？”

为了构建一个基因型似然模型，我们通常从一些基本假设出发。首先，我们假设覆盖某个特定位点的每一条测序读段（read）都是一次独立的观测。其次，我们假设测序错误以一个已知的概率发生，该概率通常由测序仪给出的碱基[质量分数](@entry_id:161575)（Phred quality score）$Q$ 来量化，即错误率 $\epsilon = 10^{-Q/10}$。

在一个[二倍体](@entry_id:268054)、双等位基因的位点上，我们考虑三种可能的基因型：纯合参考型（RR）、杂合型（RA）和纯合变异型（AA）。我们可以为每条读段建立一个发射模型，然后将所有读段的概率相乘，得到整个数据集的[联合似然](@entry_id:750952)。

具体来说，假设一个位点被 $n$ 条读段覆盖，数据集 $D = \{o_i\}_{i=1}^n$ 代表这 $n$ 条读段在该位点的碱基观测结果。我们用一个[指示变量](@entry_id:266428) $r_i$ 来表示观测结果，如果第 $i$ 条读段观测到参考等位基因 $R$，则 $r_i = 1$；如果观测到变异等位基因 $A$，则 $r_i = 0$。同时，假设对于SNP，从一个真实碱基到某个特定错误碱基的[错误概率](@entry_id:267618)为 $\alpha \epsilon_i$（通常 $\alpha = 1/3$）。

根据这些原则，我们可以推导出针对不同基因型的[似然函数](@entry_id:141927) ：

1.  **纯合参考型 (G = RR)**：如果真实基因型是 RR，那么所有读段都应源于参考等位基因。任何观测到的变异等位基因 $A$ 都必定是测序错误。因此，观测到参考碱基的概率是 $1 - \epsilon_i$，观测到变异碱基的概率是 $\alpha \epsilon_i$。对于整个数据集的似然是所有独立读段概率的乘积：
    $$P(D | \text{RR}) = \prod_{i=1}^n \left[(1 - \epsilon_i)^{r_i} \cdot (\alpha \epsilon_i)^{1 - r_i}\right]$$

2.  **纯合变异型 (G = AA)**：同理，如果真实基因型是 AA，任何观测到的参考等位基因 $R$ 都必定是测序错误。其[似然函数](@entry_id:141927)为：
    $$P(D | \text{AA}) = \prod_{i=1}^n \left[(\alpha \epsilon_i)^{r_i} \cdot (1 - \epsilon_i)^{1 - r_i}\right]$$

3.  **杂合型 (G = RA)**：如果真实基因型是 RA，那么一条读段可能源于携带 $R$ 的染色体，也可能源于携带 $A$ 的染色体。在没有等位基因偏向（allelic bias）的情况下，我们假设这两种情况的概率各为 $0.5$。因此，根据[全概率公式](@entry_id:194231)，观测到任意一条读段的概率是两种情况的加权平均。整个数据集的似然为：
    $$P(D | \text{RA}) = \prod_{i=1}^n \left\{ \frac{1}{2}\left[(1 - \epsilon_i)^{r_i} (\alpha \epsilon_i)^{1 - r_i}\right] + \frac{1}{2}\left[(\alpha \epsilon_i)^{r_i} (1 - \epsilon_i)^{1 - r_i}\right] \right\}$$

这个似然模型构成了变异识别的数学基础，它将原始的碱基质量分数和观测计数，转化为了对不同基因型假设的支持程度的量化度量。

#### 基因型先验模型：$P(G)$

**基因型[先验概率](@entry_id:275634)**（genotype prior）代表了在观测到任何测序数据之前，我们对某个位点出现特定基因型的预期。这种预期可以基于广泛的[群体遗传学](@entry_id:146344)知识，使我们的推断不仅仅依赖于当前单个样本的数据，从而提高在低质量或低深度数据下的准确性。

一个标准且广泛应用的先验模型是基于**哈迪-温伯格平衡**（Hardy-Weinberg Equilibrium, HWE）的。HWE 原理指出，在一个大的、[随机交配](@entry_id:149892)的、没有选择、突变或迁移影响的群体中，等位基因频率和基因型频率将保持稳定。如果一个变异等位基因 $A$ 在群体中的频率为 $p$，那么参考等位基因 $R$ 的频率则为 $1-p$。根据HWE，三种基因型的先验概率可以推导如下 ：

-   $P(\text{RR}) = (1-p)^2$
-   $P(\text{RA}) = 2p(1-p)$
-   $P(\text{AA}) = p^2$

这里的[等位基因频率](@entry_id:146872) $p$ 可以从大型参考数据库（如 gnomAD）中获得，或者在对一个群组进行联合分析时动态估计。

#### 结合似然与先验：[贝叶斯推断](@entry_id:146958)实战

通过将上述的基因型似然 $P(D|G)$ 和HWE先验 $P(G)$ 相结合，我们就可以计算每个基因型的（未归一化的）后验概率。最终，具有最高后验概率的基因型即为**[最大后验概率](@entry_id:268939)**（Maximum A Posteriori, MAP）估计。

例如，在一个具体的场景中 ，假设一个位点总测序深度 $n = 30$，其中观测到变异等位基因的读段数 $k = 3$，测序错误率 $\epsilon = 0.01$，该变异在群体中的频率 $p = 0.2$。通过应用前述公式，我们可以分别计算 RR、RA 和 AA 三种基因型的后验概率。计算结果会显示，尽管观测到了 3 条变异读段，但由于杂合子（RA）的似然值相对于纯合参考（RR）的似然值来说不够高，并且群体频率 $p$ 较低导致 RR 的[先验概率](@entry_id:275634)远大于 RA，最终 RR 基因型的后验概率会是最高的。这说明，在证据不强的情况下，[先验信息](@entry_id:753750)会“拉动”结果朝向更常见的基因型，从而有效过滤掉一部分由测序噪音引起的[假阳性](@entry_id:635878)。

### 从原始比对到变异检出：两种范式

构建了核心[概率模型](@entry_id:265150)后，下一个问题是：模型中的“数据”$D$——即每个位点的等位基因计数——从何而来？这引出了两种主流的变异识别策略：基于堆积（pileup-based）的方法和基于单倍型（haplotype-based）的方法。

#### 基于堆积（Pileup-Based）的方法

这是一种早期且直观的方法。它首先将所有测序读段通过比对软件（aligner）与[参考基因组](@entry_id:269221)进行比对。然后，对于基因组的每一个坐标位置，该方法简单地“堆积”所有覆盖此处的读段，并统计支持参考等位基因和各种变异等位基因的读段数量。这些计数，结合碱基[质量分数](@entry_id:161575)，直接被输入前述的似然模型中，逐个位点独立地进行基因型判断。

这种方法的优点是简单、快速。然而，其根本缺陷在于“逐点审视”的局限性。它无法利用一个读段上相邻位置之间的关联信息。当一个区域存在复杂的变异（如SNP和[Indel](@entry_id:173062)紧邻）时，比对软件本身可能会产生大量模糊和错误的排列，导致证据被分散。

一个典型的例子是  中的场景一：在一个G碱基的同聚物（homopolymer）区域旁边，同时存在一个 2bp 的缺失和一个 SNP。由于同聚物的存在，比对软件在放置这个 2bp 缺失的位置上会产生歧义，一些读段可能将缺失放在同聚物的开端，另一些则放在中间。这导致在任何一个单一位置上，支持缺失的证据都被分散了，不足以让 pileup-based 的[变异识别](@entry_id:177461)算法做出高[置信度](@entry_id:267904)的判断。同时，附近SNP的信号也可能因为[比对质量](@entry_id:170584)下降而减弱。最终，这种方法可能会漏掉这个缺失，或者给出一个低质量的SNP检出。

#### 基于单倍型（Haplotype-Based）的方法：一种更强大的范式

为了克服 pileup-based 方法的局限性，现代[变异识别](@entry_id:177461)工具（如 GATK HaplotypeCaller）普遍采用基于单倍型的方法。这种方法不再孤立地看待每个位点，而是对一小段基因组区域（称为“活跃区域”，active region）进行联合分析。其核心步骤包括：

1.  **识别活跃区域**：通过初步扫描比对文件，寻找存在变异信号（如碱基错配、[Indel](@entry_id:173062)）的区域。
2.  **局部[从头组装](@entry_id:172264)**：在活跃区域内，算法放弃原始的比对结果，将所有覆盖该区域的读段进行局部**[从头组装](@entry_id:172264)**（de novo assembly），构建出几条最有可能的候选**单倍型**（haplotype）序列。这些单倍型代表了该区域在样本中可能存在的真实DNA序列片段。
3.  **读段重比对与似然计算**：将所有读段与这些候选单倍型（而不仅仅是参考基因组）进行重比对，并计算每条读段源于每条候选单倍型的似然。
4.  **基因型判断**：最后，根据所有读段对各个单倍型的支持度，综合推断出该区域的基因型。

回到  中的复杂变异场景，基于单倍型的方法展现了其强大之处。它会在局部组装步骤中，从读段数据里直接构建出两条主要的候选单倍型：一条是参考序列，另一条是同时包含“2bp缺失”和“SNP”的变异序列。然后，当所有读段被重新比对到这两条候选单倍型上时，证据就被完美地分开了：约一半的读段会与变异单倍型完美匹配，另一半则与参考单倍型[完美匹配](@entry_id:273916)。这样，原本被分散的证据得到了有效整合，使得算法能够高置信度地检出一个复杂的联合变异。

值得注意的是，在简单的情况下，例如一个位于非重复区域的孤立SNP （场景二），两种方法都能表现良好。这凸显了基于单倍型的方法的优势主要体现在处理复杂和具有挑战性的基因组区域上。

### 基于单倍型的[变异识别](@entry_id:177461)机制剖析

鉴于基于单倍型的方法在现代基因组学中的核心地位，我们将深入剖析其关键技术环节。

#### 利用[de Bruijn图](@entry_id:263552)进行局部单倍型组装

在活跃区域内构建候选单倍型的核心技术是基于 **de Bruijn 图**的局部组装。这个过程大致如下 ：

1.  **[k-mer](@entry_id:166084)化**：算法将活跃区域内的所有读段分解成长度为 $k$ 的短序列，称为 **k-mer**。
2.  **图构建**：每个在数据中出现次数足够多（超过某个噪音阈值）的 $(k-1)$-mer 被视为图中的一个节点。如果数据中存在一个 $k$-mer，其前 $(k-1)$ 个碱基和后 $(k-1)$ 个碱基分别对应图中的节点 A 和节点 B，则在 A 和 B 之间画一条有向边。
3.  **路径寻找**：基因组中的变异（SNP或Indel）会在[de Bruijn图](@entry_id:263552)中产生“[分叉](@entry_id:270606)”或“气泡”结构。候选单倍型就是通过遍历图中从起点到终点的不同路径而得到的序列。

$k$ 值的选择是一个关键的权衡。较小的 $k$ 值可以增加对包含错误的读段的容忍度，因为一个 $k$-mer 完全正确的概率是 $(1-\epsilon)^k$，这个值随 $k$ 的减小而增大。这有助于从含有少量测序错误的读段中恢复出变异信号，从而提高敏感性。然而，过小的 $k$ 值会降低序列的特异性，尤其是在重复序列区域，可能导致图中产生大量纠缠不清的复杂结构，使单倍型的解析变得困难甚至不可能 。

#### 利用[配对隐马尔可夫模型](@entry_id:162687)（[Pair-HMM](@entry_id:162687)）评估单倍型

当候选单倍型被构建出来后，算法需要一个精确的方法来评估每条原始读段对每条候选单倍型的支持程度，即计算似然值 $P(\text{read} | \text{haplotype})$。这个任务由**[配对隐马尔可夫模型](@entry_id:162687)**（[Pair-HMM](@entry_id:162687)）完成。

[Pair-HMM](@entry_id:162687) 是一个生成式概率模型，它将两条序列（这里是读段和单倍型）的比对视为一个通过三种[隐藏状态](@entry_id:634361)的路径 ：

-   **匹配/错配状态 (M)**：读段的一个碱基与单倍型的一个碱基对齐。
-   **插入状态 (I)**：读段的一个碱基与单倍型中的一个缺口对齐（即读段相对于单倍型有插入）。
-   **删除状态 (D)**：单倍型的一个碱基与读段中的一个缺口对齐（即读段相对于单倍型有缺失）。

模型的**转移概率**（transition probabilities）定义了从一个状态转移到另一个状态的概率（例如，从匹配状态M进入插入状态I的概率对应于“打开一个插入缺口”的罚分）。**发射概率**（emission probabilities）则定义了在某个状态下生成特定观测（例如，在M状态下，当单倍型碱基为C而读段碱基为T时，发射概率由碱基[质量分数](@entry_id:161575)决定）。

通过动态规划算法（如[前向算法](@entry_id:165467)），[Pair-HMM](@entry_id:162687)可以计算出读段和单倍型之间所有可能比对路径的概率之和，从而得到一个总的似然值 $P(\text{read} | \text{haplotype})$。

这个严谨的概率框架也是解决**参考偏见**（reference bias）问题的关键。参考偏见是指标准比对算法在面对包含Indel的读段时，可能会因为打开缺口的罚分（gap penalty）设置不当，而倾向于将Indel错误地比对为一系列碱基错配，从而使支持变异的证据丢失 。[Pair-HMM](@entry_id:162687)通过一个统一的、对称的概率模型（例如，设置$p(M \to I) = p(M \to D)$）来评估所有可能性，并进行局部重比对，能够以一种更公平和数学上更严谨的方式在“比对为[Indel](@entry_id:173062)”和“比对为错配”之间做出选择，从而显著减少参考偏见。

### 提升准确性与功效：高级主题

为了在精准医疗等要求极高的场景中获得可靠的变异检出结果，研究者们还发展了多种高级策略来进一步提升[变异识别](@entry_id:177461)的准确性和统计功效。

#### 借助群体数据：联合基因分型

当处理一个包含多个样本的队列研究时，我们可以利用**联合基因分型**（joint genotyping）技术来“借用”群体信息，以增强对单个样本（尤其是低深度样本）的判断力。

其核心思想是在贝叶斯框架中，使用从整个队列中动态估计出的等位基因频率来更新每个样本的先验概率 $P(G)$ 。想象一个场景：在分析一个100人的队列时，我们在某个位点发现99个高深度样本中有50个是杂合子。这表明该变异在该队列中是相当常见的（等位基因频率 $\hat{p} \approx 0.25$）。现在，我们来分析最后一个深度非常低（仅4x）的样本S。在这个位点，样本S有1条支持变异的读段。如果单独分析样本S，由于数据太弱，而我们通常假设未知变异是罕见的（例如，先验频率 $p_0=10^{-3}$），那么该变异很可能被当作测序噪音而被忽略，后验概率会非常低。

然而，在联合分型中，我们使用从群体中估计出的高频率 $\hat{p} \approx 0.25$ 来为样本S构建先验概率。这个强烈的“该变异很常见”的[先验信息](@entry_id:753750)，会极大地提升杂合[子基](@entry_id:152709)因型的后验概率，足以让算法在微弱的读段证据下也能自信地检出这个杂合变异 。这生动地展示了联合分析如何通过信息共享来拯救在单样本分析中可能丢失的变异。

#### 系统性测序错误的建模

测序错误并非完全随机。除了**随机错误**（stochastic errors）外，还存在大量**系统性错误**（systematic errors），它们是与特定实验条件或序列上下文相关的、可重现的偏见。例如，在[Illumina测序](@entry_id:171043)平台上，错误率会随着测序轮次（cycle）的增加而上升，并且某些特定的[核酸](@entry_id:164998)序列模式（如GGT三联体）之后更容易出现特定类型的错误。

这些系统性错误是[假阳性](@entry_id:635878)变异检出的主要来源。一个先进的[变异识别](@entry_id:177461)流程必须能够识别并对这些错误进行建模。其核心思想是，不再假设一个恒定的错误率 $\epsilon$，而是构建一个更精细的错误模型，其中错误率是多个协变量（如测序轮次、序列上下文、碱基[质量分数](@entry_id:161575)等）的函数 。

例如，我们可以使用一个[广义线性模型](@entry_id:171019)（如逻辑回归）来建模[错误概率](@entry_id:267618) $p_{c,k}$，其中 $c$ 代表测序轮次，$k$ 代表序列上下文：
$$\operatorname{logit}(p_{c,k}) = \alpha + \beta \cdot c + \gamma_k$$
这个模型可以从基因组中大量被认为是纯合参考的位点的数据中学习得到。一旦模型训练完成，它就可以用来预测在任何给定位置的预期系统性错误率。这个过程的本质就是**碱基质量分数重校准**（Base Quality Score Recalibration, BQSR）。

在进行变异识别时，算法就可以利用这个校准过的、考虑了系统性偏差的错误率。例如，在一个长的同聚物区域，我们知道[Indel](@entry_id:173062)的错误率会显著升高。通过建立一个依赖于同聚物长度 $L$ 的错误模型 $e(L)$ ，即使我们观察到一定数量的支持Indel的读段，我们也可以通过[贝叶斯因子](@entry_id:143567)（Bayes factor）等统计工具来判断，这些观测是由极高的背景噪音引起的，还是真正代表了一个低频变异。这种精细的错误建模对于区分真实信号和高频伪影至关重要。

### 解读[变异识别](@entry_id:177461)结果

[变异识别](@entry_id:177461)流程的最后一步是生成结果文件，并正确地解读其中的信息。

#### 变异检出格式（VCF）：描述变异的语言

**VCF (Variant Call Format)** 是存储变异信息的标准文件格式。理解其中的关键字段对于评估变异的质量至关重要。在一个典型的VCF文件中，针对每个样本的信息（`FORMAT`列）包含以下重要字段 ：

-   **AD (Allele Depth)**：等位基因深度。记录了支持参考等位基因和各个变异等位基因的过滤后读段数量。例如 `28,12` 表示28条读段支持参考等位基因，12条支持变异等位基因。
-   **AF (Allele Fraction)**：[等位基因频率](@entry_id:146872)。对于单个样本，它通常是由AD计算出的变异等位基因所占的比例。例如，对于`AD=28,12`，AF为 $12 / (28+12) = 0.3$。
-   **PL (Phred-scaled Likelihoods)**：经过Phred标度（$-10 \log_{10}(L)$）的基因型似然。这是一个包含三个值的字段，分别对应纯合参考、杂合和纯合变异基因型。为了便于比较，这些值经过归一化，使得最可能的基因型的PL值为0。例如 `40,0,20` 表示杂合基因型最可能，其原始似然最高；纯合参考基因型的PL值为40，意味着其似然比杂合子低$10^{4}$倍。
-   **GQ (Genotype Quality)**：基因型质量。表示对所给出的基因型判断的[置信度](@entry_id:267904)。它被定义为“该基因型判断是错误的”这一事件的Phred标度概率。一个非常好的近似计算方法是，G[Q值](@entry_id:265045)约等于次优基因型的PL值。在`PL=40,0,20`的例子中，次优基因型的PL是20，因此GQ值约为20。
-   **MQ (Mapping Quality)**：[比对质量](@entry_id:170584)。这是一个在`INFO`列的位点级别注释，总结了所有覆盖该位点的读段的[比对质量](@entry_id:170584)。通常使用[均方根](@entry_id:263605)（Root Mean Square, RMS）作为聚合统计量。

#### 规范化表示：[Indel](@entry_id:173062)表示的唯一性

在重复序列区域，一个[Indel](@entry_id:173062)的表示方式可能存在歧义。例如，在一个 `AAAAA` 的同聚物中删除两个A，可以有多种等效的表示方式。为了确保变异表示的唯一性和可比性，VCF规范要求对[Indel](@entry_id:173062)进行**归一化**（normalization）。这通常包括两个步骤 ：

1.  **左对齐 (Left-alignment)**：将[Indel](@entry_id:173062)尽可能地向序列的左侧（坐标减小的方向）移动，直到不能再移动为止，同时保持其所代表的单倍型序列不变。
2.  **简约化 (Parsimony)**：修剪`REF`和`ALT`等位基因序列前后共有的碱基，直到只剩下[VCF格式](@entry_id:756453)所必需的一个锚定碱基。

经过归一化的[Indel](@entry_id:173062)具有唯一的、最简约的表示形式。例如，一个在 `chr1` 上 `2005-2009` 位点 `AAAAA` 同聚物中的2bp `AA` 缺失，其正确的、经过左对齐和归一化的VCF表示应为 `POS=2004, REF=TAA, ALT=T` 。规范化是确保不同研究或不同分析流程产生的VCF文件能够进行有意义比较的基础。

#### 检出的不对称性：存在与缺失的证明

最后，我们需要理解一个微妙但至关重要的统计学概念：检出变异的存在与自信地断言变异的缺失，在统计学上是不对称的 。

-   **检出变异**：相当于找到了足够的“积极证据”来拒绝“该位点是纯合参考”这一零假设。这是一个针对特定[备择假设](@entry_id:167270)（如杂合子）的显著性检验。
-   **断言纯合参考**：这不仅仅是“未能找到变异的证据”。一个高置信度的纯合参考检出，是在声明我们有足够的**[统计功效](@entry_id:197129)**（statistical power）来排除所有合理的变异可能性（包括各种SNP和[Indel](@entry_id:173062)）。这是一种“证明缺失”的行为。

这种功效完全依赖于测序深度 $n$。在深度很低的情况下（例如 $n=5$），即使所有读段都与参考基因组一致，我们也无法自信地排除杂合子的可能性，因为我们可能只是运气不好，没有抽样到来自变异染色体的读段。相反，在深度很高的情况下（例如 $n=100$），如果所有读段都与参考一致，我们就有非常强的证据来断言该位点确实是纯合参考。

因此，对一个变异的检出报告，无论是“存在”还是“缺失”，都必须结合其质量和深度信息来综合解读。这也解释了为什么在任何测序实验中，总有一部分基因组区域因为深度不足或质量太差而处于“无法判断”的状态。