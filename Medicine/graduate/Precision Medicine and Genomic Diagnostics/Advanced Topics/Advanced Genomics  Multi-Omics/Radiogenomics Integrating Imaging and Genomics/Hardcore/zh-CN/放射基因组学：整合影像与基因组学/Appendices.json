{
    "hands_on_practices": [
        {
            "introduction": "在构建预测模型之前，一个常见的首要步骤是探索整合后数据集的整体结构。本练习演示了一种使用主成分分析（PCA）整合来自不同模态（放射组学和基因组学）数据块的基础技术。通过对数据块进行标准化和能量均衡加权，我们可以进行联合降维，以识别跨越影像和基因组特征的主要变异模式，这有助于理解患者异质性的主要来源。",
            "id": "4374287",
            "problem": "您会获得代表同一患者队列的放射组学特征和基因组学特征的配对数据矩阵，这是一个放射基因组学整合场景。令 $X_r \\in \\mathbb{R}^{n \\times p_r}$ 表示放射组学数据块，$X_g \\in \\mathbb{R}^{n \\times p_g}$ 表示基因组学数据块，其中有 $n$ 个患者，$p_r$ 个放射组学特征和 $p_g$ 个基因组学特征。任务是设计一个程序，执行基于特定原理的分块预处理和降维，然后使用主成分分析 (PCA) 来量化与整合相关的指标。\n\n基本原理：\n- Z-score 标准化：对于任何特征向量 $x \\in \\mathbb{R}^n$，其标准化形式为 $z = (x - \\bar{x}) / s_x$，其中 $\\bar{x}$ 是均值，$s_x$ 是以自由度为 $1$ 计算的样本标准差。\n- 弗罗贝尼乌斯范数：对于任何矩阵 $A \\in \\mathbb{R}^{n \\times p}$，其范数为 $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^p a_{ij}^2}$。\n- 奇异值分解 (SVD)：对于任何实数矩阵 $Z \\in \\mathbb{R}^{n \\times p}$，存在一个正交分解 $Z = U S V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times m}$ 的列是标准正交的，$V \\in \\mathbb{R}^{p \\times m}$ 的列是标准正交的，$S \\in \\mathbb{R}^{m \\times m}$ 是一个对角矩阵，其非负对角元满足 $s_1 \\ge s_2 \\ge \\dots \\ge s_m \\ge 0$，且 $m = \\min(n, p)$。\n\n程序必须根据这些原理实现以下步骤：\n1. 对每个数据块 $X_r$ 和 $X_g$，跨患者进行逐特征的中心化和 Z-score 标准化，以获得 $Z_r$ 和 $Z_g$。如果任何特征的方差为零 ($s_x = 0$)，则认为它不携带可标准化的信号，在中心化后保持其值不变，不进行会导致除以零的缩放（即，在中心化后将其标准化条目设置为零）。\n2. 使用弗罗贝尼乌斯范数执行数据块缩放以均衡总数据块能量：计算 $w_r = 1/\\|Z_r\\|_F$（如果 $\\|Z_r\\|_F > 0$，否则 $w_r = 1$）和 $w_g = 1/\\|Z_g\\|_F$（如果 $\\|Z_g\\|_F > 0$，否则 $w_g = 1$）。通过水平拼接，形成拼接的、经过数据块缩放的矩阵 $Z = [\\, w_r Z_r \\;\\; w_g Z_g \\,] \\in \\mathbb{R}^{n \\times (p_r + p_g)}$。\n3. 计算 $Z$ 的奇异值分解 (SVD)：$Z = U S V^\\top$。使用此分解计算以下量：\n   - 前 $k$ 个主成分的累计解释方差比：$$R_k = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^m s_j^2},$$ 其中 $m = \\min(n, p_r + p_g)$。\n   - 第一个主成分的数据块载荷能量：令 $v_1 \\in \\mathbb{R}^{p_r + p_g}$ 为第一个右奇异向量（$V$ 的第一列，或等效于 $V^\\top$ 的第一行的共轭转置）。定义 $$B_r = \\sum_{i=1}^{p_r} v_{1,i}^2 \\quad \\text{和} \\quad B_g = \\sum_{i=p_r+1}^{p_r+p_g} v_{1,i}^2.$$ 根据 $V$ 的标准正交性，$B_r + B_g = 1$，并且每个值都是介于 $0$ 和 $1$ 之间的小数，表示放射组学与基因组学数据块对第一个主成分方向的相对贡献。\n   - 使用前 $k$ 个主成分的相对重构误差：令 $U_k \\in \\mathbb{R}^{n \\times k}$、$S_k \\in \\mathbb{R}^{k \\times k}$ 和 $V_k \\in \\mathbb{R}^{(p_r+p_g) \\times k}$ 表示截断后的因子。秩-$k$ 重构为 $Z_k = U_k S_k V_k^\\top$。相对重构误差为 $$E_k = \\frac{\\|Z - Z_k\\|_F}{\\|Z\\|_F}。$$\n\n您的程序必须为每个测试用例输出一个列表 $[R_k, B_r, B_g, E_k]$，其中每个值都四舍五入到六位小数，并表示为小数（而非百分比）。最终输出必须是一行，包含这些按测试用例排列的列表的列表，格式为用方括号括起来的逗号分隔列表；例如，$\\big[ [ 0.123456, 0.400000, 0.600000, 0.250000 ], [ \\dots ] \\big]$。\n\n测试套件：\n提供以下四种情况的结果，它们分别涵盖了一般情况、特征数量不平衡的情况、共线性边界情况和零方差边界情况。对于每种情况，都指定了整数 $k$。\n\n情况 (a)：$n = 6$，$p_r = 3$，$p_g = 4$，$k = 2$，其中\n$$\nX_r =\n\\begin{bmatrix}\n1  2  -1 \\\\\n2  3  -1 \\\\\n3  5  -2 \\\\\n4  6  -3 \\\\\n5  8  -4 \\\\\n6  9  -4\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n2  1  0  1 \\\\\n4  1  -1  2 \\\\\n5  2  1  3 \\\\\n8  3  0  4 \\\\\n10  4  1  5 \\\\\n12  4  2  6\n\\end{bmatrix}.\n$$\n\n情况 (b)：$n = 6$，$p_r = 2$，$p_g = 8$，$k = 3$，其中\n$$\nX_r =\n\\begin{bmatrix}\n1  100 \\\\\n2  90 \\\\\n1  110 \\\\\n3  95 \\\\\n2  105 \\\\\n4  100\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n0  1  0  1  2  3  0  5 \\\\\n1  2  1  2  2  3  1  6 \\\\\n0  1  1  1  3  4  2  5 \\\\\n2  3  2  3  2  4  3  7 \\\\\n1  2  3  2  3  5  4  8 \\\\\n3  4  3  4  4  6  5  9\n\\end{bmatrix}.\n$$\n\n情况 (c)：$n = 5$，$p_r = 3$，$p_g = 3$，$k = 1$，其中 $X_r$ 中存在共线性，\n$$\nX_r =\n\\begin{bmatrix}\n1  2  -1 \\\\\n2  4  -2 \\\\\n3  6  -3 \\\\\n4  8  -4 \\\\\n5  10  -5\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n0  1  0 \\\\\n1  1  -1 \\\\\n1  2  0 \\\\\n2  3  1 \\\\\n3  5  2\n\\end{bmatrix}.\n$$\n\n情况 (d)：$n = 4$，$p_r = 3$，$p_g = 2$，$k = 1$，其中存在零方差特征，\n$$\nX_r =\n\\begin{bmatrix}\n1  2  5 \\\\\n2  4  5 \\\\\n3  6  5 \\\\\n4  8  5\n\\end{bmatrix},\n\\quad\nX_g =\n\\begin{bmatrix}\n0  0 \\\\\n0  0 \\\\\n1  0 \\\\\n1  0\n\\end{bmatrix}.\n$$\n\n角度单位不适用。不涉及物理单位。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例由 $[R_k, B_r, B_g, E_k]$ 表示，并四舍五入到六位小数，例如 $\\big[ [ 0.123456, 0.400000, 0.600000, 0.250000 ], [ \\dots ] \\big]$。",
            "solution": "用户提供了一个问题陈述，要求对配对的放射组学和基因组学数据矩阵进行计算分析。我将首先根据指定标准验证该问题陈述。\n\n### 第 1 步：提取已知信息\n\n- **数据块**：一个放射组学矩阵 $X_r \\in \\mathbb{R}^{n \\times p_r}$ 和一个基因组学矩阵 $X_g \\in \\mathbb{R}^{n \\times p_g}$，其中 $n$ 是患者数量，$p_r$ 是放射组学特征的数量，$p_g$ 是基因组学特征的数量。\n- **基本定义**：\n    - **Z-score 标准化**：对于一个特征向量 $x \\in \\mathbb{R}^n$，其标准化向量为 $z = (x - \\bar{x}) / s_x$，其中 $\\bar{x}$ 是均值，$s_x$ 是使用 $n-1$ 作为分母（自由度为 1）计算的样本标准差。\n    - **弗罗贝尼乌斯范数**：对于一个矩阵 $A \\in \\mathbb{R}^{n \\times p}$，其范数为 $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^p a_{ij}^2}$。\n    - **奇异值分解 (SVD)**：对于一个实数矩阵 $Z \\in \\mathbb{R}^{n \\times p}$，其 SVD 为 $Z = U S V^\\top$，其属性如问题中所述。\n- **计算步骤**：\n    1.  **标准化**：将 $X_r$ 和 $X_g$ 的列进行标准化，得到 $Z_r$ 和 $Z_g$。对于方差为零的特征，中心化和缩放后的列变为零向量。\n    2.  **数据块缩放**：计算权重 $w_r = 1/\\|Z_r\\|_F$ 和 $w_g = 1/\\|Z_g\\|_F$（如果范数非零，否则权重为 1）。形成拼接矩阵 $Z = [\\, w_r Z_r \\;\\; w_g Z_g \\,]$。\n    3.  **SVD 和指标计算**：计算 $Z$ 的 SVD。由此计算：\n        - 累计解释方差比：$R_k = (\\sum_{j=1}^k s_j^2) / (\\sum_{j=1}^m s_j^2)$，其中 $m = \\min(n, p_r + p_g)$。\n        - 数据块载荷能量：$B_r = \\sum_{i=1}^{p_r} v_{1,i}^2$ 和 $B_g = \\sum_{i=p_r+1}^{p_r+p_g} v_{1,i}^2$，使用第一个右奇异向量 $v_1$。\n        - 相对重构误差：$E_k = \\|Z - Z_k\\|_F / \\|Z\\|_F$，对于秩-k 近似 $Z_k$。\n- **测试用例**：提供了四个具体用例，包括矩阵 $X_r$、$X_g$ 和一个整数 $k$。\n- **输出格式**：一个单行字符串，表示一个列表的列表，其中每个内部列表包含 $[R_k, B_r, B_g, E_k]$，其值四舍五入到六位小数。\n\n### 第 2 步：使用提取的已知信息进行验证\n\n- **科学依据**：该问题位于放射基因组学领域，并采用标准的、成熟的统计技术进行数据整合，包括 Z-score 标准化、分块归一化和通过 SVD 实现的主成分分析 (PCA)。所有定义和提议的指标在数学和概念上都是合理的。\n- **定义明确**：问题定义清晰，包含所有必要的数据（$X_r, X_g, k$）和明确的、逐步的指令。操作序列为每个测试用例导向一套唯一的、可计算的指标。\n- **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n- **完整性和一致性**：指令是完整的且内部一致。对零方差特征的特殊处理已明确规定，防止了除零错误并确保了程序的鲁棒性。\n- **现实性和可行性**：提供的测试用例使用小型的、可计算处理的合成矩阵，同时代表了真实数据中遇到的结构性挑战（例如，共线性、零方差）。\n- **结构和清晰度**：问题结构良好，定义清晰，逻辑流程从数据预处理到分析和指标提取，一气呵成。\n\n### 第 3 步：结论和行动\n\n问题陈述有效。这是一个定义明确、有科学依据的计算任务。我将继续进行解答。\n\n### 解答\n\n解答需要系统地实现指定的多步骤过程。这个过程的核心是一种应用于两个数据块的共识主成分分析 (PCA)，这是数据融合任务（如放射基因组学）中的常见策略。\n\n**1. 逐特征标准化**\n\n对于每个数据块 $X_r$ 和 $X_g$，每个特征（列）都独立进行标准化。对于给定的特征向量 $x \\in \\mathbb{R}^n$，标准化将其转换为 $z$，使其均值为 $0$，样本方差为 $1$。公式为：\n$$\nz = \\frac{x - \\bar{x}}{s_x}\n$$\n其中 $\\bar{x}$ 是 $x$ 的均值，$s_x$ 是样本标准差，用自由度 $n-1$ 计算。此步骤确保所有特征，无论其原始单位或尺度如何，对每个数据块内总方差的初始计算贡献相等。\n\n一个关键的边界情况是当一个特征的方差为零时（即 $s_x = 0$）。在这种情况下，特征列是恒定的。问题正确地指定了处理方式：中心化的列（一个零向量）被用作标准化的列。这正确地反映了一个恒定特征不提供关于患者间方差的任何信息。此操作产生标准化的矩阵 $Z_r$ 和 $Z_g$。\n\n**2. 数据块能量均衡**\n\n在对每个数据块内的特征进行标准化之后，这些数据块本身可能具有不同的“总能量”，通过其弗罗贝尼乌斯范数的平方 $\\|Z_r\\|_F^2$ 和 $\\|Z_g\\|_F^2$ 来量化。特征的数量（$p_r, p_g$）直接影响此能量，因为一个标准化特征向量的平方和是 $n-1$。因此，$\\|Z_r\\|_F^2 = p_r^* (n-1)$，其中 $p_r^*$ 是 $X_r$ 中非恒定特征的数量。\n\n为了防止特征更多的块在联合分析中占主导地位，我们进行数据块缩放。我们计算缩放权重 $w_r = 1 / \\|Z_r\\|_F$ 和 $w_g = 1 / \\|Z_g\\|_F$。应用这些权重后，我们得到缩放后的数据块 $w_r Z_r$ 和 $w_g Z_g$。现在，每个缩放后的数据块的弗罗贝尼乌斯范数均为 $1$，确保它们对拼接矩阵的总方差贡献相等。最终用于分析的矩阵通过水平拼接形成：\n$$\nZ = [\\, w_r Z_r \\;\\; w_g Z_g \\,]\n$$\n这个组合矩阵的弗罗贝尼乌斯范数平方为 $\\|Z\\|_F^2 = \\|w_r Z_r\\|_F^2 + \\|w_g Z_g\\|_F^2 = 1 + 1 = 2$。\n\n**3. 奇异值分解 (SVD)**\n\n通过计算矩阵 $Z$ 的 SVD 来执行 PCA：$Z = U S V^\\top$。$U S$ 的列是主成分（得分），$V$ 的列是主轴（载荷）。$S$ 的对角线元素是奇异值 $s_j$。数据中的总方差与奇异值平方和 $\\sum_j s_j^2$ 成正比，该值等于 $\\|Z\\|_F^2$。\n\n**4. 指标计算**\n\nSVD 提供了计算所需指标的所有必要元素。\n\n- **累计解释方差比 ($R_k$)**：由第 $j$ 个主成分解释的方差与 $s_j^2$ 成正比。前 $k$ 个主成分的累计解释方差比是它们占总方差的比例：\n$$\nR_k = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^{\\min(n, p_r+p_g)} s_j^2}\n$$\n\n- **数据块载荷能量 ($B_r, B_g$)**：第一个主轴 $v_1$（$V$ 的第一列）是一个长度为 $p_r + p_g$ 的载荷向量。它定义了高维特征空间中方差最大的方向。$v_1$ 的前 $p_r$ 个元素对应于放射组学特征，接下来的 $p_g$ 个元素对应于基因组学特征。这些子向量的平方模表示每个数据块对定义此主方向的贡献。我们计算：\n$$\nB_r = \\sum_{i=1}^{p_r} v_{1,i}^2 \\quad \\text{和} \\quad B_g = \\sum_{i=p_r+1}^{p_r+p_g} v_{1,i}^2\n$$\n由于 $v_1$ 是一个单位向量，可以保证 $B_r + B_g = 1$。这些指标量化了放射组学与基因组学在协变的主要模式中的相对重要性。\n\n- **相对重构误差 ($E_k$)**：埃卡特-杨-米尔斯基定理 (Eckart-Young-Mirsky theorem) 指出，矩阵 $Z$ 的最佳秩-$k$ 近似是 $Z_k = U_k S_k V_k^\\top$，其中因子是截断的 SVD 分量。该定理还给出了误差矩阵的范数：$\\|Z - Z_k\\|_F^2 = \\sum_{j=k+1}^m s_j^2$。因此，相对重构误差是：\n$$\nE_k = \\frac{\\|Z - Z_k\\|_F}{\\|Z\\|_F} = \\frac{\\sqrt{\\sum_{j=k+1}^m s_j^2}}{\\sqrt{\\sum_{j=1}^m s_j^2}}\n$$\n这等同于 $\\sqrt{1 - R_k}$，一旦知道了 $R_k$，就提供了一种计算效率高的方法来找到误差。\n\n以下程序为给定的测试用例实现了这整个工作流程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef standardize(X):\n    \"\"\"\n    Performs Z-score standardization on the columns of matrix X.\n    Uses sample standard deviation (ddof=1).\n    Columns with zero variance are set to zero.\n    \"\"\"\n    # Ensure input is float for calculations\n    X_float = X.astype(float)\n    \n    # Center the data\n    mean = np.mean(X_float, axis=0)\n    X_centered = X_float - mean\n    \n    # Calculate sample standard deviation\n    std_dev = np.std(X_float, axis=0, ddof=1)\n    \n    # Standardize, handling zero-variance columns\n    Z = np.zeros_like(X_float)\n    non_zero_std_mask = std_dev > 1e-15 # Use a tolerance for floating point comparison\n    \n    # Use np.divide for safe division\n    if np.any(non_zero_std_mask):\n        Z[:, non_zero_std_mask] = np.divide(\n            X_centered[:, non_zero_std_mask],\n            std_dev[non_zero_std_mask]\n        )\n        \n    return Z\n\ndef solve():\n    \"\"\"\n    Main function to run the radiogenomics integration analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"k\": 2,\n            \"Xr\": np.array([\n                [1, 2, -1], [2, 3, -1], [3, 5, -2], \n                [4, 6, -3], [5, 8, -4], [6, 9, -4]\n            ]),\n            \"Xg\": np.array([\n                [2, 1, 0, 1], [4, 1, -1, 2], [5, 2, 1, 3],\n                [8, 3, 0, 4], [10, 4, 1, 5], [12, 4, 2, 6]\n            ])\n        },\n        {\n            \"k\": 3,\n            \"Xr\": np.array([\n                [1, 100], [2, 90], [1, 110], \n                [3, 95], [2, 105], [4, 100]\n            ]),\n            \"Xg\": np.array([\n                [0, 1, 0, 1, 2, 3, 0, 5], [1, 2, 1, 2, 2, 3, 1, 6],\n                [0, 1, 1, 1, 3, 4, 2, 5], [2, 3, 2, 3, 2, 4, 3, 7],\n                [1, 2, 3, 2, 3, 5, 4, 8], [3, 4, 3, 4, 4, 6, 5, 9]\n            ])\n        },\n        {\n            \"k\": 1,\n            \"Xr\": np.array([\n                [1, 2, -1], [2, 4, -2], [3, 6, -3], \n                [4, 8, -4], [5, 10, -5]\n            ]),\n            \"Xg\": np.array([\n                [0, 1, 0], [1, 1, -1], [1, 2, 0], \n                [2, 3, 1], [3, 5, 2]\n            ])\n        },\n        {\n            \"k\": 1,\n            \"Xr\": np.array([\n                [1, 2, 5], [2, 4, 5], \n                [3, 6, 5], [4, 8, 5]\n            ]),\n            \"Xg\": np.array([\n                [0, 0], [0, 0], \n                [1, 0], [1, 0]\n            ])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        Xr, Xg, k = case[\"Xr\"], case[\"Xg\"], case[\"k\"]\n        pr = Xr.shape[1]\n\n        # Step 1: Standardization\n        Zr = standardize(Xr)\n        Zg = standardize(Xg)\n\n        # Step 2: Block Scaling\n        norm_Zr = np.linalg.norm(Zr, 'fro')\n        norm_Zg = np.linalg.norm(Zg, 'fro')\n\n        wr = 1.0 / norm_Zr if norm_Zr > 0 else 1.0\n        wg = 1.0 / norm_Zg if norm_Zg > 0 else 1.0\n\n        Z = np.concatenate((wr * Zr, wg * Zg), axis=1)\n\n        # Step 3: SVD and Metrics\n        # Use full_matrices=False for efficiency\n        U, s, Vt = np.linalg.svd(Z, full_matrices=False)\n        \n        s_squared = s**2\n        total_variance = np.sum(s_squared)\n\n        if total_variance > 1e-15:\n            # R_k: Cumulative explained variance ratio\n            Rk = np.sum(s_squared[:k]) / total_variance\n\n            # B_r, B_g: Block loading energy\n            v1 = Vt[0, :]\n            Br = np.sum(v1[:pr]**2)\n            Bg = np.sum(v1[pr:]**2)\n            \n            # E_k: Relative reconstruction error\n            Ek = np.sqrt(1 - Rk)\n        else:\n            # Handle case of zero-variance matrix Z\n            Rk = 0.0\n            # If v1 is not well-defined, split contribution by number of features\n            total_p = Z.shape[1]\n            Br = pr / total_p if total_p > 0 else 0.5\n            Bg = 1.0 - Br\n            # Error of approximating 0 with 0 is 0\n            Ek = 0.0\n\n        all_results.append([Rk, Br, Bg, Ek])\n\n    # Format the final output string as specified\n    # e.g., [[val1,val2,...],[val1,val2,...]]\n    output_lists = []\n    for result_list in all_results:\n        # Format each number to have 6 decimal places.\n        inner_list_str = ','.join([f\"{v:.6f}\" for v in result_list])\n        output_lists.append(f\"[{inner_list_str}]\")\n    \n    final_output_str = f\"[{','.join(output_lists)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "当使用来自不同来源（如影像和基因组学）的特征构建预测模型时，数据预处理是一个至关重要但常被忽视的步骤。本练习探讨了特征缩放如何与岭回归等正则化模型相互作用。它突显了各向同性惩罚（如岭回归中的惩罚）并非尺度不变的，不进行特征标准化可能会导致模型无意中对数值较大的特征施加更重的惩罚，从而产生偏差。",
            "id": "4374286",
            "problem": "您正在研究放射基因组学，整合磁共振成像 (MRI) 特征与下一代测序 (NGS) 衍生的基因组特征，以预测一个连续的肿瘤侵袭性指数。考虑一个带有截距的线性模型，该模型根据每个样本的两个特征预测侵袭性指数：一个源自影像的放射组学纹理特征和一个源自基因组的表达谱。建模假设是响应由一个带有加性高斯噪声的线性模型生成，并且我们通过最小化增广了对非截距系数的各向同性平方惩罚项的经验平方误差来估计参数。\n\n基本原理：\n- 带截距的线性模型定义为 $y_i = b + \\mathbf{x}_i^\\top \\mathbf{w} + \\varepsilon_i$，其中 $\\varepsilon_i$ 是均值为零、方差有限的独立高斯噪声项，$\\mathbf{x}_i \\in \\mathbb{R}^2$ 是样本 $i$ 的特征向量，$b \\in \\mathbb{R}$ 是截距，$\\mathbf{w} \\in \\mathbb{R}^2$ 是斜率系数。\n- 惩罚最小二乘法（Tikhonov 回归或岭回归）通过最小化经验风险 $J(b,\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - b - \\mathbf{x}_i^\\top \\mathbf{w})^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2$ 来估计 $(b, \\mathbf{w})$，其中 $\\lambda \\ge 0$ 是正则化强度。\n- 特征标准化通过 $\\tilde{x}_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}$ 转换特征，其中 $\\mu_j$ 是特征 $j$ 在 $n$ 个样本中的均值，$s_j$ 是其标准差。\n\n目标：\n在原始特征空间中，分析使用和不使用特征标准化的岭回归对拟合系数的影响。具体来说，对于每个测试用例，您必须计算未缩放的岭回归解的斜率系数与将标准化岭回归解代数转换回原始特征空间后的斜率系数之间的差值的欧几里得范数。这样可以分离出统一的各向同性正则化与特征缩放相互作用的影响。\n\n需要实现的算法任务：\n1. 对于给定的数据集，通过将一个全为 1 的列与原始特征矩阵 $X \\in \\mathbb{R}^{n \\times 2}$ 拼接，构造设计矩阵 $Z \\in \\mathbb{R}^{n \\times 3}$，即 $Z = [\\mathbf{1}, X]$。\n2. 求解带有未惩罚截距的惩罚正规方程组：$(\\frac{1}{n} Z^\\top Z + \\lambda P)\\,\\theta = \\frac{1}{n} Z^\\top y$，其中 $\\theta = \\begin{bmatrix} b \\\\ \\mathbf{w} \\end{bmatrix}$ 且 $P = \\mathrm{diag}(0,1,1)$。\n3. 通过减去每个特征的均值 $\\mu_j$ 并除以每个特征的标准差 $s_j$ 来标准化特征以获得 $\\tilde{X}$。构造 $\\tilde{Z} = [\\mathbf{1}, \\tilde{X}]$ 并求解 $(\\frac{1}{n} \\tilde{Z}^\\top \\tilde{Z} + \\lambda P)\\,\\tilde{\\theta} = \\frac{1}{n} \\tilde{Z}^\\top y$ 以获得 $\\tilde{\\theta} = \\begin{bmatrix} \\tilde{b} \\\\ \\tilde{\\mathbf{w}} \\end{bmatrix}$。\n4. 使用标准化所蕴含的仿射变换将标准化解转换回原始特征空间：$b_{\\mathrm{raw}}^{(\\mathrm{std})} = \\tilde{b} - \\sum_{j=1}^2 \\tilde{w}_j \\frac{\\mu_j}{s_j}$ 和 $\\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})} = \\left[ \\frac{\\tilde{w}_1}{s_1}, \\frac{\\tilde{w}_2}{s_2} \\right]^\\top$。\n5. 计算 $\\mathbb{R}^2$ 中的欧几里得范数 $\\Delta = \\lVert \\mathbf{w}_{\\mathrm{raw}} - \\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})} \\rVert_2$，其中 $\\mathbf{w}_{\\mathrm{raw}}$ 是步骤 2 中得到的斜率向量。\n\n注意：当 $\\lambda = 0$ 时，两种方法在反向转换后应完全一致，因为标准化是普通最小二乘法的可逆重参数化。对于 $\\lambda > 0$，特征缩放会与各向同性惩罚项相互作用，通常导致原始空间系数的差异。\n\n测试套件：\n对于下面的每个案例，$X$ 有两列，第一列是 MRI 衍生的纹理强度摘要（无量纲，任意单位），第二列是基因组表达谱得分（无量纲），$y$ 是侵袭性指数（无量纲）。所有数字都已明确给出。\n\n- 案例 1（平衡的特征尺度，中等正则化）：\n$$\nX = \\begin{bmatrix}\n12.0  0.80 \\\\\n15.0  1.10 \\\\\n9.0  0.50 \\\\\n14.0  1.00 \\\\\n11.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.10 \\\\\n1.20 \\\\\n1.90 \\\\\n1.60\n\\end{bmatrix}, \\quad\n\\lambda = 0.50.\n$$\n\n- 案例 2（高度不平衡的尺度：影像特征数量级大，基因组特征数量级小，正则化强度相同）：\n$$\nX = \\begin{bmatrix}\n800.0  0.80 \\\\\n1200.0  1.10 \\\\\n950.0  0.50 \\\\\n1100.0  1.00 \\\\\n1000.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.30 \\\\\n1.70 \\\\\n2.10 \\\\\n1.90\n\\end{bmatrix}, \\quad\n\\lambda = 0.50.\n$$\n\n- 案例 3（与案例 2 相同，强正则化）：\n$$\nX = \\begin{bmatrix}\n800.0  0.80 \\\\\n1200.0  1.10 \\\\\n950.0  0.50 \\\\\n1100.0  1.00 \\\\\n1000.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.30 \\\\\n1.70 \\\\\n2.10 \\\\\n1.90\n\\end{bmatrix}, \\quad\n\\lambda = 50.00.\n$$\n\n- 案例 4（与案例 2 相同，无正则化；边界条件）：\n$$\nX = \\begin{bmatrix}\n800.0  0.80 \\\\\n1200.0  1.10 \\\\\n950.0  0.50 \\\\\n1100.0  1.00 \\\\\n1000.0  0.90\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.50 \\\\\n2.30 \\\\\n1.70 \\\\\n2.10 \\\\\n1.90\n\\end{bmatrix}, \\quad\n\\lambda = 0.00.\n$$\n\n- 案例 5（基因组特征近乎恒定且影像特征紧密聚集，中等正则化）：\n$$\nX = \\begin{bmatrix}\n1000.0  0.95 \\\\\n990.0  0.96 \\\\\n1010.0  0.94 \\\\\n1005.0  0.95 \\\\\n995.0  0.96\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.80 \\\\\n1.82 \\\\\n1.78 \\\\\n1.81 \\\\\n1.79\n\\end{bmatrix}, \\quad\n\\lambda = 0.50.\n$$\n\n要求的计算和输出：\n- 对于每个案例，计算上面定义的标量 $\\Delta$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_k$ 是案例 $k$ 按照所列顺序计算出的 $\\Delta$ 的浮点值。",
            "solution": "该问题被评估为有效。它在科学上基于正则化线性回归的原理，这是统计学和机器学习中的一个基本主题。其表述在数学上是精确、自洽且适定的，提供了计算唯一、可验证解所需的所有数据和算法步骤。放射基因组学的背景为分析惩罚模型中特征缩放效应提供了一个现实的应用领域。\n\n目标是量化在线性模型中使用岭回归时，对原始特征与对标准化特征进行回归所估计出的斜率系数的差异。这种差异的产生是因为各向同性惩罚项 $\\lambda \\lVert \\mathbf{w} \\rVert_2^2$ 对特征的缩放不是不变的，而未惩罚的最小二乘损失函数是。\n\n线性模型由 $y_i = b + \\mathbf{x}_i^\\top \\mathbf{w} + \\varepsilon_i$ 给出，其中 $y_i \\in \\mathbb{R}$ 是样本 $i$ 的响应，$\\mathbf{x}_i \\in \\mathbb{R}^2$ 是特征向量，$b \\in \\mathbb{R}$ 是截距，$\\mathbf{w} \\in \\mathbb{R}^2$ 是斜率系数。对于 $n$ 个样本，其矩阵形式为 $y = \\mathbf{1}_n b + X \\mathbf{w} + \\varepsilon$，可以写成 $y = Z \\theta + \\varepsilon$，其中 $Z = [\\mathbf{1}_n, X]$ 是 $n \\times (p+1)$ 的设计矩阵（$p=2$ 个特征），$\\theta = [b, \\mathbf{w}^\\top]^\\top$ 是 $(p+1) \\times 1$ 的系数向量。\n\n系数通过最小化惩罚最小二乘目标函数来估计：\n$$\nJ(b, \\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - b - \\mathbf{x}_i^\\top \\mathbf{w})^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\n$$\n截距 $b$ 不受惩罚。该目标函数可以表示为矩阵形式：\n$$\nJ(\\theta) = \\frac{1}{n} (y - Z\\theta)^\\top(y - Z\\theta) + \\lambda \\theta^\\top P \\theta\n$$\n其中 $P = \\mathrm{diag}(0, 1, 1)$ 是一个 $3 \\times 3$ 矩阵，仅对 $\\theta$ 中的斜率系数施加惩罚。\n\n将梯度 $\\nabla_\\theta J(\\theta)$ 设为零，得到正规方程组：\n$$\n\\left(\\frac{1}{n} Z^\\top Z + \\lambda P\\right) \\theta = \\frac{1}{n} Z^\\top y\n$$\n求解该方程组可以找到给定数据集 $(X, y)$ 和正则化强度 $\\lambda$ 下的估计系数。\n\n我们将通过两种不同的程序计算斜率系数 $\\mathbf{w}$，然后进行比较。\n\n**程序 1：对原始特征进行岭回归**\n\n1.  使用原始特征矩阵 $X$ 构造设计矩阵 $Z = [\\mathbf{1}_n, X]$。\n2.  求解正规方程组以获得 $\\theta_{\\mathrm{raw}} = [b_{\\mathrm{raw}}, \\mathbf{w}_{\\mathrm{raw}}^\\top]^\\top$：\n    $$\n    \\left(\\frac{1}{n} Z^\\top Z + \\lambda P\\right) \\theta_{\\mathrm{raw}} = \\frac{1}{n} Z^\\top y\n    $$\n3.  得到的斜率向量为 $\\mathbf{w}_{\\mathrm{raw}}$。\n\n**程序 2：对标准化特征进行岭回归并进行反向转换**\n\n1.  标准化特征矩阵 $X$。对于每个特征列 $j \\in \\{1, 2\\}$，计算其均值 $\\mu_j$ 和标准差 $s_j$：\n    $$\n    \\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}, \\quad s_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\n    $$\n    标准化后的特征为 $\\tilde{x}_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}$。令 $\\tilde{X}$ 为标准化特征矩阵。\n2.  为标准化问题构造设计矩阵 $\\tilde{Z} = [\\mathbf{1}_n, \\tilde{X}]$。\n3.  求解相应的正规方程组，以获得标准化空间中的系数 $\\tilde{\\theta} = [\\tilde{b}, \\tilde{\\mathbf{w}}^\\top]^\\top$：\n    $$\n    \\left(\\frac{1}{n} \\tilde{Z}^\\top \\tilde{Z} + \\lambda P\\right) \\tilde{\\theta} = \\frac{1}{n} \\tilde{Z}^\\top y\n    $$\n4.  解 $\\tilde{\\theta}$ 为模型 $y \\approx \\tilde{b} + \\tilde{X}\\tilde{\\mathbf{w}}$ 提供了系数。为了将其与 $\\mathbf{w}_{\\mathrm{raw}}$ 进行比较，我们必须将它们转换回原始特征空间。标准化预测变量和原始预测变量之间的关系意味着对斜率系数进行以下转换：\n    $$\n    w_{\\mathrm{raw}, j}^{(\\mathrm{std})} = \\frac{\\tilde{w}_j}{s_j}\n    $$\n    这就得到了向量 $\\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})}$。\n\n**最终比较**\n\n通过计算在原始特征空间中得到的两个斜率向量之差的欧几里得范数，可以分离出标准化的影响：\n$$\n\\Delta = \\lVert \\mathbf{w}_{\\mathrm{raw}} - \\mathbf{w}_{\\mathrm{raw}}^{(\\mathrm{std})} \\rVert_2\n$$\n将对指定的五个测试用例中的每一个执行此计算。对于 $\\lambda=0$ 的情况，我们期望 $\\Delta \\approx 0$，因为普通最小二乘法对预测变量的仿射变换是等变的。对于 $\\lambda > 0$，特征尺度（$s_1$ 与 $s_2$）的差异将与各向同性惩罚项相互作用，导致 $\\Delta > 0$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of feature standardization on ridge regression coefficients.\n    \n    For each test case, this function computes two sets of ridge regression coefficients:\n    1. On the raw, unscaled features.\n    2. On standardized (z-scored) features, which are then algebraically transformed back\n       to the original feature space.\n    \n    It then calculates the Euclidean norm of the difference between the slope coefficients\n    from these two procedures.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: balanced feature scales, moderate regularization\n        {\n            \"X\": np.array([\n                [12.0, 0.80], [15.0, 1.10], [9.0, 0.50], [14.0, 1.00], [11.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.10, 1.20, 1.90, 1.60]),\n            \"lambda\": 0.50\n        },\n        # Case 2: highly unbalanced scales, moderate regularization\n        {\n            \"X\": np.array([\n                [800.0, 0.80], [1200.0, 1.10], [950.0, 0.50], [1100.0, 1.00], [1000.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.30, 1.70, 2.10, 1.90]),\n            \"lambda\": 0.50\n        },\n        # Case 3: same as Case 2, strong regularization\n        {\n            \"X\": np.array([\n                [800.0, 0.80], [1200.0, 1.10], [950.0, 0.50], [1100.0, 1.00], [1000.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.30, 1.70, 2.10, 1.90]),\n            \"lambda\": 50.00\n        },\n        # Case 4: same as Case 2, no regularization (OLS)\n        {\n            \"X\": np.array([\n                [800.0, 0.80], [1200.0, 1.10], [950.0, 0.50], [1100.0, 1.00], [1000.0, 0.90]\n            ]),\n            \"y\": np.array([1.50, 2.30, 1.70, 2.10, 1.90]),\n            \"lambda\": 0.00\n        },\n        # Case 5: near-constant signatures, moderate regularization\n        {\n            \"X\": np.array([\n                [1000.0, 0.95], [990.0, 0.96], [1010.0, 0.94], [1005.0, 0.95], [995.0, 0.96]\n            ]),\n            \"y\": np.array([1.80, 1.82, 1.78, 1.81, 1.79]),\n            \"lambda\": 0.50\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        lambda_val = case[\"lambda\"]\n        \n        n, p = X.shape\n        \n        # --- Procedure 1: Ridge Regression on Raw Features ---\n        \n        # Construct design matrix Z = [1, X]\n        Z_raw = np.hstack([np.ones((n, 1)), X])\n        \n        # Penalty matrix P = diag(0, 1, ..., 1)\n        P = np.diag([0.0] + [1.0] * p)\n        \n        # Left-hand side of normal equations: (1/n * Z'Z + lambda * P)\n        A_raw = (1/n) * (Z_raw.T @ Z_raw) + lambda_val * P\n        \n        # Right-hand side of normal equations: (1/n * Z'y)\n        b_raw = (1/n) * (Z_raw.T @ y)\n\n        # Solve for coefficients theta_raw = [b_raw, w_raw]\n        theta_raw = np.linalg.solve(A_raw, b_raw)\n        w_raw = theta_raw[1:]\n\n        # --- Procedure 2: Ridge Regression on Standardized Features ---\n\n        # Standardize features (using population std dev, i.e., ddof=0)\n        mu = np.mean(X, axis=0)\n        s = np.std(X, axis=0)\n        \n        # Handle cases where a feature is constant (s_j = 0) to avoid division by zero.\n        # Although not present in test data, this is good practice.\n        # We replace s_j=0 with 1 to avoid scaling, as the feature is constant anyway.\n        s_safe = np.where(s == 0, 1, s)\n        X_std = (X - mu) / s_safe\n\n        # Construct standardized design matrix\n        Z_std = np.hstack([np.ones((n, 1)), X_std])\n        \n        # Solve the normal equations for the standardized problem\n        A_std = (1/n) * (Z_std.T @ Z_std) + lambda_val * P\n        b_std = (1/n) * (Z_std.T @ y)\n        theta_std = np.linalg.solve(A_std, b_std)\n        w_tilde = theta_std[1:]\n        \n        # --- Back-transform and Compare ---\n        \n        # Convert standardized slope coefficients back to raw feature space\n        w_raw_from_std = w_tilde / s_safe\n        \n        # Compute the Euclidean norm of the difference\n        delta = np.linalg.norm(w_raw - w_raw_from_std)\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在放射基因组学研究中，我们常常需要检验数千个特征与某个临床或基因组结果之间的关联，这带来了严重的多重检验问题。本练习介绍了一种强大的非参数方法——置换检验（permutation testing），用以计算在所有特征中观察到的最强关联的校正后p值。这项实践演示了如何构建一个经验零分布来评估研究发现的统计显著性，从而有效避免因检验大量特征而产生的伪关联。",
            "id": "4557637",
            "problem": "考虑一个放射基因组学（radiogenomics）的场景，其中一组定量影像特征与同一组受试者的单个基因组测量向量相关联。令 $X \\in \\mathbb{R}^{n \\times p}$ 表示影像特征矩阵，其中 $n$ 是受试者数量，$p$ 是特征数量；令 $y \\in \\mathbb{R}^{n}$ 表示这些受试者的基因组测量向量。目标是使用基于零假设下可交换性原理的置换检验，评估基因组测量与任何单个影像特征之间的最大关联的显著性。\n\n基本依据和假设：\n- 在影像特征与基因组测量之间没有关联的零假设下，$y$ 的索引在受试者之间是可交换的。也就是说，$y$ 在受试者之间的任何置换都是等可能的，因此，通过随机置换 $y$ 并重新计算统计量，可以近似得到在 $X$ 和 $y$ 之间计算的任何关联统计量的分布。\n- 一个特征向量 $x \\in \\mathbb{R}^{n}$ 与基因组向量 $y \\in \\mathbb{R}^{n}$ 之间的关联通过皮尔逊相关系数（Pearson Correlation Coefficient, PCC）来衡量，对于非退化数据，其定义为\n$$\n\\rho(x,y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}},\n$$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 表示样本均值。如果 $\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} = 0$ 或 $\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} = 0$，则定义 $\\rho(x,y) = 0$ 以避免除以零。\n- 定义跨特征的最大关联统计量为\n$$\nT_{\\mathrm{obs}} = \\max_{1 \\le j \\le p} \\left|\\rho\\left(X_{\\cdot j}, y\\right)\\right|,\n$$\n其中 $X_{\\cdot j}$ 是 $X$ 的第 $j$ 列。\n\n置换检验程序：\n- 生成 $M$ 个对 $y$ 的索引 $\\{1,2,\\ldots,n\\}$ 的随机置换，每个置换都使用固定的种子 $s$ 进行均匀随机抽样，以保证可复现性。\n- 对于每个置换 $b \\in \\{1,2,\\ldots,M\\}$，计算\n$$\nT^{(b)} = \\max_{1 \\le j \\le p} \\left|\\rho\\left(X_{\\cdot j}, y^{(b)}\\right)\\right|,\n$$\n其中 $y^{(b)}$ 是根据第 $b$ 次随机置换对其条目进行置换后的 $y$。\n- 使用加一校正计算经验 $p$ 值，以确保即使在 $T_{\\mathrm{obs}}$ 极值情况下也有效：\n$$\np = \\frac{1 + \\sum_{b=1}^{M} \\mathbf{1}\\left\\{T^{(b)} \\ge T_{\\mathrm{obs}}\\right\\}}{1 + M}.\n$$\n\n您的任务是实现一个程序，对下面测试套件中的每个测试用例，使用上述程序计算 $T_{\\mathrm{obs}}$ 的经验 $p$ 值。程序必须根据任一向量方差为零时 $\\rho(x,y)=0$ 的规则，处理零方差特征或零方差基因组向量。\n\n测试套件（每个用例指定 $n$、$p$、$X$、$y$、置换次数 $M$ 和种子 $s$）：\n- 用例 1（正常路径）：\n  - $n = 8$, $p = 5$, $M = 1000$, $s = 42$.\n  - $y = [\\, 0.7,\\ -1.2,\\ 0.3,\\ 0.5,\\ -0.4,\\ 1.1,\\ -0.8,\\ 0.0 \\,]$.\n  - $X$ 的列为\n    - $X_{\\cdot 1} = [\\, 1.0,\\ 0.5,\\ -0.2,\\ 0.4,\\ 0.0,\\ 0.9,\\ -0.5,\\ -0.1 \\,]$,\n    - $X_{\\cdot 2} = [\\, 0.2,\\ -0.1,\\ 0.3,\\ 0.1,\\ -0.3,\\ 0.4,\\ -0.2,\\ 0.0 \\,]$,\n    - $X_{\\cdot 3} = [\\, 2.0,\\ 1.5,\\ -1.0,\\ 1.2,\\ -0.5,\\ 1.8,\\ -1.3,\\ -0.2 \\,]$,\n    - $X_{\\cdot 4} = [\\, 0.5,\\ -0.6,\\ 0.4,\\ 0.7,\\ -0.5,\\ 0.9,\\ -0.8,\\ 0.1 \\,]$,\n    - $X_{\\cdot 5} = [\\, -0.3,\\ 0.1,\\ -0.4,\\ 0.2,\\ 0.0,\\ -0.1,\\ 0.3,\\ 0.5 \\,]$.\n- 用例 2（存在零方差特征）：\n  - $n = 6$, $p = 4$, $M = 500$, $s = 123$.\n  - $y = [\\, 1.0,\\ 0.9,\\ -0.8,\\ 0.7,\\ -0.6,\\ 0.5 \\,]$.\n  - $X$ 的列为\n    - $X_{\\cdot 1} = [\\, 2.0,\\ 1.8,\\ -1.6,\\ 1.4,\\ -1.2,\\ 1.0 \\,]$,\n    - $X_{\\cdot 2} = [\\, 0.0,\\ 0.0,\\ 0.0,\\ 0.0,\\ 0.0,\\ 0.0 \\,]$,\n    - $X_{\\cdot 3} = [\\, -0.5,\\ -0.2,\\ 0.0,\\ 0.1,\\ 0.3,\\ 0.4 \\,]$,\n    - $X_{\\cdot 4} = [\\, 0.6,\\ -0.7,\\ 0.8,\\ -0.9,\\ 1.0,\\ -1.1 \\,]$.\n- 用例 3（最大特征相关性存在并列）：\n  - $n = 7$, $p = 3$, $M = 400$, $s = 2023$.\n  - $y = [\\, 0.2,\\ 0.4,\\ 0.6,\\ 0.8,\\ -0.1,\\ -0.3,\\ -0.5 \\,]$.\n  - $X$ 的列为\n    - $X_{\\cdot 1} = [\\, 1.0,\\ 2.0,\\ 3.0,\\ 4.0,\\ -0.5,\\ -1.0,\\ -1.5 \\,]$,\n    - $X_{\\cdot 2} = [\\, 1.0,\\ 2.0,\\ 3.0,\\ 4.0,\\ -0.5,\\ -1.0,\\ -1.5 \\,]$,\n    - $X_{\\cdot 3} = [\\, 0.5,\\ -0.2,\\ 0.1,\\ -0.3,\\ 0.2,\\ -0.1,\\ 0.0 \\,]$.\n- 用例 4（恒定基因组向量边界情况）：\n  - $n = 5$, $p = 3$, $M = 200$, $s = 7$.\n  - $y = [\\, 1.0,\\ 1.0,\\ 1.0,\\ 1.0,\\ 1.0 \\,]$.\n  - $X$ 的列为\n    - $X_{\\cdot 1} = [\\, 0.3,\\ -0.2,\\ 0.1,\\ 0.0,\\ 0.2 \\,]$,\n    - $X_{\\cdot 2} = [\\, -0.1,\\ 0.2,\\ -0.3,\\ 0.4,\\ -0.5 \\,]$,\n    - $X_{\\cdot 3} = [\\, 0.5,\\ 0.5,\\ 0.5,\\ 0.5,\\ 0.5 \\,]$.\n\n算法和输出要求：\n- 对每个测试用例，实现上文定义的 $T_{\\mathrm{obs}}$ 和经验 $p$ 值 $p$ 的计算，使用确切的 $M$ 次置换，并通过确定性地初始化伪随机数生成器来使用指定的种子 $s$。\n- 为了数值稳定性和正确性，在计算 $\\rho(x,y)$ 时，请使用所提供的精确定义，并在任一分母为 $0$ 时，赋值 $\\rho(x,y) = 0$。\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为四个 $p$ 值的逗号分隔列表，保留 $6$ 位小数，并用方括号括起来，按给定用例的顺序排列（例如，$[0.123456,0.654321,0.000001,1.000000]$）。",
            "solution": "该问题是有效的。它在计算统计学领域，特别是在放射基因组学领域，提出了一个清晰、有科学依据且定义明确的任务。所提供的方法——一种用于评估最大皮尔逊相关性显著性的置换检验——是生物信息学中处理多重比较的标准而稳健的技术。所有必要的数据、参数和边界条件都已明确定义，从而可以得到唯一且可验证的解。\n\n解决方案通过为每个测试用例实现指定的置换检验程序来展开。此程序的核心基于零假设（$H_0$）下的可交换性原理。$H_0$ 假设影像特征 $X \\in \\mathbb{R}^{n \\times p}$ 与基因组测量向量 $y \\in \\mathbb{R}^{n}$ 之间没有关联。如果 $H_0$ 为真，那么观测到的受试者数据配对仅仅是 $n!$ 种等可能排列中的一种。因此，我们可以通过反复打乱 $y$ 的元素并重新计算统计量来模拟 $H_0$ 下任何检验统计量的分布。\n\n所选的检验统计量是所有 $p$ 个特征中的最大绝对皮尔逊相关系数：\n$$\nT = \\max_{1 \\le j \\le p} \\left|\\rho\\left(X_{\\cdot j}, y\\right)\\right|\n$$\n其中 $X_{\\cdot j}$ 是 $X$ 的第 $j$ 列。该统计量捕捉了数据中最强的线性关联，无论其方向（正或负），其在置换下的分布为校正 $p$ 个特征间多重比较的单次假设检验提供了基础。皮尔逊相关系数 $\\rho(x,y)$ 定义为：\n$$\n\\rho(x,y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n$$\n如规定，如果任一输入向量的方差为零（即，分母中的相应项为零），则定义 $\\rho(x,y)$ 为 $0$。\n\n算法实现对每个测试用例遵循以下步骤：\n\n1.  **计算观测统计量 ($T_{\\mathrm{obs}}$)**：第一步是根据原始未置换的数据计算检验统计量。为提高效率，所有 $p$ 个特征列与向量 $y$ 的皮尔逊相关系数以向量化的方式计算。通过减去各自的均值，对特征矩阵 $X$ 和向量 $y$ 进行中心化。所有特征的相关性公式的分子可以通过转置的中心化特征矩阵与中心化的基因组向量之间的矩阵-向量乘积来同时计算。分母也以向量方式计算。执行检查以确定 $X$ 中是否存在零方差列或 $y$ 向量是否为零方差，以应用 $\\rho=0$ 规则。观测统计量 $T_{\\mathrm{obs}}$ 随后是这些计算出的相关性绝对值的最大值。\n\n2.  **生成零分布**：执行一个循环 $M$ 次，以近似 $H_0$ 下检验统计量的分布。在每次迭代 $b \\in \\{1, 2, \\ldots, M\\}$ 中：\n    a. 通过随机置换原始向量 $y$ 的元素，生成一个新的基因组向量 $y^{(b)}$。使用指定的种子 $s$ 初始化伪随机数生成器以确保可复现性。\n    b. 对此置换后的数据集计算检验统计量 $T^{(b)} = \\max_{1 \\le j \\le p} |\\rho(X_{\\cdot j}, y^{(b)})|$，使用与计算 $T_{\\mathrm{obs}}$ 相同的向量化程序。\n\n3.  **计算经验 p 值**：经验 $p$ 值是在零假设下观测到至少与 $T_{\\mathrm{obs}}$ 一样极端的检验统计量的估计概率。通过将 $T_{\\mathrm{obs}}$ 与来自零分布的 $M$ 个统计量集合进行比较来计算。统计满足 $T^{(b)} \\ge T_{\\mathrm{obs}}$ 的置换次数。最终的 $p$ 值使用“加一”校正计算：\n    $$\n    p = \\frac{1 + \\sum_{b=1}^{M} \\mathbf{1}\\left\\{T^{(b)} \\ge T_{\\mathrm{obs}}\\right\\}}{1 + M}\n    $$\n    其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。这种校正可以防止 $p$ 值为 $0$，并提供一个更保守和稳定的估计。\n\n将这整个程序应用于四个测试用例中的每一个，这些用例旨在测试实现在标准用例、存在零方差特征的用例、存在并列最大相关性的用例以及存在零方差基因组向量的边界用例上的正确性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the radiogenomics permutation testing problem for the given test suite.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        (\n            np.array([\n                [1.0, 0.2, 2.0, 0.5, -0.3], [0.5, -0.1, 1.5, -0.6, 0.1],\n                [-0.2, 0.3, -1.0, 0.4, -0.4], [0.4, 0.1, 1.2, 0.7, 0.2],\n                [0.0, -0.3, -0.5, -0.5, 0.0], [0.9, 0.4, 1.8, 0.9, -0.1],\n                [-0.5, -0.2, -1.3, -0.8, 0.3], [-0.1, 0.0, -0.2, 0.1, 0.5]\n            ]),\n            np.array([0.7, -1.2, 0.3, 0.5, -0.4, 1.1, -0.8, 0.0]),\n            1000, 42\n        ),\n        # Case 2 (zero-variance feature present)\n        (\n            np.array([\n                [2.0, 0.0, -0.5, 0.6], [1.8, 0.0, -0.2, -0.7],\n                [-1.6, 0.0, 0.0, 0.8], [1.4, 0.0, 0.1, -0.9],\n                [-1.2, 0.0, 0.3, 1.0], [1.0, 0.0, 0.4, -1.1]\n            ]),\n            np.array([1.0, 0.9, -0.8, 0.7, -0.6, 0.5]),\n            500, 123\n        ),\n        # Case 3 (tie in maximal feature correlation)\n        (\n            np.array([\n                [1.0, 1.0, 0.5], [2.0, 2.0, -0.2], [3.0, 3.0, 0.1],\n                [4.0, 4.0, -0.3], [-0.5, -0.5, 0.2], [-1.0, -1.0, -0.1],\n                [-1.5, -1.5, 0.0]\n            ]),\n            np.array([0.2, 0.4, 0.6, 0.8, -0.1, -0.3, -0.5]),\n            400, 2023\n        ),\n        # Case 4 (constant genomic vector boundary)\n        (\n            np.array([\n                [0.3, -0.1, 0.5], [-0.2, 0.2, 0.5], [0.1, -0.3, 0.5],\n                [0.0, 0.4, 0.5], [0.2, -0.5, 0.5]\n            ]),\n            np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            200, 7\n        )\n    ]\n    \n    def pearson_corr_matrix(X, y):\n        \"\"\"\n        Computes Pearson correlation between each column of X and vector y.\n        Handles zero-variance cases according to the problem specification.\n        \"\"\"\n        n, p = X.shape\n        \n        # Center y\n        y_mean = np.mean(y)\n        y_centered = y - y_mean\n        ss_y = np.sum(y_centered**2)\n\n        # If y has zero variance, all correlations are 0\n        if ss_y == 0:\n            return np.zeros(p)\n\n        # Center X column-wise\n        X_mean = np.mean(X, axis=0)\n        X_centered = X - X_mean\n        ss_X = np.sum(X_centered**2, axis=0)\n        \n        # Numerator: dot product of centered vectors (covariance term)\n        # transpose X_centered (p, n) and dot with y_centered (n,) -> (p,)\n        numerator = X_centered.T @ y_centered\n\n        # Denominator: product of standard deviations\n        denominator = np.sqrt(ss_X * ss_y)\n\n        # Initialize correlations to 0, which is the default for zero-variance cases\n        correlations = np.zeros(p)\n        \n        # Create a mask for columns with non-zero variance\n        non_zero_denom_mask = denominator > 0\n        \n        # Calculate correlation only for non-zero variance columns to avoid division by zero\n        correlations[non_zero_denom_mask] = numerator[non_zero_denom_mask] / denominator[non_zero_denom_mask]\n        \n        return correlations\n\n    results = []\n    for X, y, M, s in test_cases:\n        # Step 1: Compute the observed maximal association statistic\n        obs_corrs = pearson_corr_matrix(X, y)\n        T_obs = np.max(np.abs(obs_corrs))\n\n        # Step 2: Generate permutation distribution\n        rng = np.random.default_rng(seed=s)\n        num_exceeding = 0\n        \n        for _ in range(M):\n            # Permute the genomic vector y\n            y_permuted = rng.permutation(y)\n            \n            # Compute maximal correlation for the permuted data\n            perm_corrs = pearson_corr_matrix(X, y_permuted)\n            T_perm = np.max(np.abs(perm_corrs))\n            \n            # Check if the permuted statistic is greater than or equal to the observed one\n            if T_perm >= T_obs:\n                num_exceeding += 1\n                \n        # Step 3: Compute the p-value with the +1 correction\n        p_value = (1 + num_exceeding) / (1 + M)\n        results.append(f\"{p_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}