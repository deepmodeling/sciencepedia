{
    "hands_on_practices": [
        {
            "introduction": "A Manhattan plot visualizes millions of statistical tests across the genome, but how do we decide which signals are noteworthy? With so many tests, the conventional $p$-value threshold of $0.05$ would lead to an overwhelming number of false positives. This first exercise guides you through the fundamental task of setting a statistically rigorous significance threshold for a genome-wide study by applying the Bonferroni correction, a method designed to control the family-wise error rate. Calculating this threshold is the first step in interpreting any Manhattan plot and determining which genetic variants warrant further investigation .",
            "id": "4353047",
            "problem": "A clinical genomics team is conducting an exome-wide rare variant association study to inform precision medicine in oncology. They test a subset of exonic single-nucleotide variants across the genome, totaling $m=200{,}000$ hypothesis tests for variant-phenotype associations. Under the null hypothesis for each test, the $p$-values are super-uniform on the unit interval due to valid calibration, and the team wants to control the Family-Wise Error Rate (FWER) at $\\alpha=0.05$ when deciding which associations to prioritize for functional validation.\n\nThe standard exome-wide visualization is a Manhattan plot where the $y$-axis records $-\\log_{10}(p)$ for each variant across chromosomes, supplemented by a quantile-quantile (QQ) plot to assess calibration and potential inflation. To maintain FWER control at level $\\alpha$, the team will annotate a horizontal line on the Manhattan plot corresponding to the Bonferroni-corrected per-test decision threshold.\n\nStarting from the fundamental definition of FWER as the probability of at least one false positive across the $m$ tests, and using well-tested probabilistic bounds under multiple testing, derive the Bonferroni per-test $p$-value threshold and then compute the corresponding horizontal annotation level on the Manhattan plot, expressed as $-\\log_{10}$ of that threshold. Round your final $-\\log_{10}$ value to six significant figures. Provide only the final $-\\log_{10}$ threshold value as your answer.",
            "solution": "The problem is valid as it presents a standard, well-defined task in statistical genetics that is scientifically grounded, objective, and contains all necessary information for a unique solution.\n\nThe objective is to determine the significance threshold on the $-\\log_{10}(p)$ scale for a Manhattan plot, such that the Family-Wise Error Rate (FWER) is controlled at a specified level $\\alpha$ for a given number of hypothesis tests $m$. The method stipulated is the Bonferroni correction.\n\nFirst, we define the Family-Wise Error Rate (FWER). For a family of $m$ hypothesis tests, the FWER is the probability of making at least one Type I error (a false positive) among all tests. Let $V$ be the random variable representing the number of true null hypotheses that are incorrectly rejected. The FWER is then defined as:\n$$\n\\text{FWER} = P(V \\ge 1)\n$$\n\nLet $H_{0_1}, H_{0_2}, \\dots, H_{0_m}$ be the null hypotheses for the $m$ tests, and let $p_1, p_2, \\dots, p_m$ be the corresponding $p$-values. Let $I_0$ be the set of indices for which the null hypothesis is true. A Type I error occurs for test $i \\in I_0$ if we reject $H_{0_i}$. The decision rule is to reject $H_{0_i}$ if its $p$-value $p_i$ is less than or equal to some per-test significance threshold, which we denote as $\\alpha_{BT}$.\n\nThe event $\\{V \\ge 1\\}$ is the union of the events that a true null hypothesis $H_{0_i}$ (where $i \\in I_0$) is rejected. This can be written as:\n$$\n\\text{FWER} = P\\left( \\bigcup_{i \\in I_0} \\{p_i \\le \\alpha_{BT}\\} \\right)\n$$\n\nTo control this probability, we use Boole's inequality, also known as the union bound. This inequality states that for any collection of events, the probability of their union is less than or equal to the sum of their individual probabilities.\n$$\nP\\left( \\bigcup_{i \\in I_0} \\{p_i \\le \\alpha_{BT}\\} \\right) \\le \\sum_{i \\in I_0} P(p_i \\le \\alpha_{BT})\n$$\n\nUnder a true null hypothesis $H_{0_i}$, the $p$-value $p_i$ follows a uniform distribution on the interval $[0, 1]$. The problem states the $p$-values are \"super-uniform\", which means $P(p_i \\le x) \\le x$ for any $x \\in [0, 1]$. This condition is more conservative and strengthens the inequality. For the purpose of finding a general bound, we consider the worst-case scenario allowed, which is the uniform distribution, where $P(p_i \\le \\alpha_{BT}) = \\alpha_{BT}$. Therefore:\n$$\n\\sum_{i \\in I_0} P(p_i \\le \\alpha_{BT}) \\le \\sum_{i \\in I_0} \\alpha_{BT}\n$$\n\nLet $m_0$ be the number of true null hypotheses, so $m_0 = |I_0|$. The sum becomes $m_0 \\alpha_{BT}$. Combining the inequalities, we have:\n$$\n\\text{FWER} \\le m_0 \\alpha_{BT}\n$$\n\nIn a typical experimental setting, the number of true nulls $m_0$ is unknown. To ensure the FWER is controlled under any condition, we must take the most conservative (largest) possible value for $m_0$, which is the total number of tests, $m$. So, $m_0 \\le m$. This leads to the well-known Bonferroni bound:\n$$\n\\text{FWER} \\le m \\alpha_{BT}\n$$\n\nTo control the FWER at a desired level $\\alpha$, we enforce the condition that this upper bound is equal to $\\alpha$:\n$$\nm \\alpha_{BT} = \\alpha\n$$\nSolving for the per-test threshold $\\alpha_{BT}$ gives the Bonferroni correction formula:\n$$\n\\alpha_{BT} = \\frac{\\alpha}{m}\n$$\n\nNow, we substitute the given values into this formula.\nThe total number of hypothesis tests is $m = 200,000$.\nThe desired FWER level is $\\alpha = 0.05$.\n\nThe Bonferroni-corrected per-test $p$-value threshold is:\n$$\n\\alpha_{BT} = \\frac{0.05}{200,000} = \\frac{5 \\times 10^{-2}}{2 \\times 10^5} = 2.5 \\times 10^{-7}\n$$\n\nThe Manhattan plot displays the results on a $-\\log_{10}$ scale. The horizontal line corresponding to this significance threshold will be at a $y$-value of $-\\log_{10}(\\alpha_{BT})$.\n$$\ny_{\\text{threshold}} = -\\log_{10}(\\alpha_{BT}) = -\\log_{10}(2.5 \\times 10^{-7})\n$$\n\nUsing the properties of logarithms, specifically $\\log(ab) = \\log(a) + \\log(b)$ and $\\log(10^k) = k$:\n$$\ny_{\\text{threshold}} = -(\\log_{10}(2.5) + \\log_{10}(10^{-7}))\n$$\n$$\ny_{\\text{threshold}} = -(\\log_{10}(2.5) - 7)\n$$\n$$\ny_{\\text{threshold}} = 7 - \\log_{10}(2.5)\n$$\n\nNow, we compute the numerical value:\n$$\n\\log_{10}(2.5) \\approx 0.39794000867...\n$$\n$$\ny_{\\text{threshold}} \\approx 7 - 0.39794000867 = 6.60205999133...\n$$\n\nThe problem requires rounding the final value to six significant figures. The digits are $6, 6, 0, 2, 0, 5$. The seventh digit is $9$, so we round up the sixth digit.\n$$\ny_{\\text{threshold}} \\approx 6.60206\n$$\nThis is the required annotation level for the Manhattan plot.",
            "answer": "$$\\boxed{6.60206}$$"
        },
        {
            "introduction": "Before declaring any genetic association as significant, we must first ensure our statistical tests are well-calibrated. The Quantile-Quantile (QQ) plot serves as a crucial diagnostic tool for this purpose, comparing the observed distribution of our test statistics to what we'd expect under the null hypothesis of no association. This practice demystifies the genomic inflation factor, $\\lambda_{GC}$, a key metric that summarizes the QQ plot into a single number, by having you calculate it from a small set of test statistics. Understanding how to compute and interpret $\\lambda_{GC}$ is essential for identifying potential issues like population stratification that can lead to spurious findings .",
            "id": "4353249",
            "problem": "A cohort-wide case-control analysis in a precision oncology Genome-Wide Association Study (GWAS) evaluates single nucleotide variant associations with a pharmacogenomic phenotype. For a Manhattan plot and Quantile-Quantile (QQ) plot sanity check, you wish to quantify potential test statistic inflation using the Genomic Control (GC) inflation factor. Under the null hypothesis of no association, single-variant $z$-scores are approximately distributed as a standard normal, and the corresponding squared $z$-scores follow a chi-square distribution with one degree of freedom. The GC inflation factor $\\lambda_{GC}$ is defined as the ratio of the observed median of the chi-square statistics to the null median. In the absence of inflation, the null median of a chi-square distribution with one degree of freedom equals approximately $0.4549$. \n\nYou are given the following vector of $z$-scores for a subset of independent variants assayed in the GWAS: \n$$[\\,0.00,\\,-0.15,\\,0.25,\\,-0.35,\\,0.45,\\,-0.55,\\,0.65,\\,-0.68,\\,0.71,\\,-0.75,\\,0.85,\\,-1.20,\\,1.50,\\,-2.00,\\,2.50,\\,-3.00,\\,3.50\\,].$$\n\nStarting from first principles of the null distribution for $z$-scores, compute the GC inflation factor $\\lambda_{GC}$ by first transforming each $z$-score to a chi-square statistic with one degree of freedom via squaring, then taking the median of these chi-square values, and finally dividing by $0.4549$. Round your answer to $4$ significant figures. Express the final value as a pure number without units.",
            "solution": "Under the null hypothesis of no association, the single-variant test statistic $z$ is approximately distributed as $\\mathcal{N}(0,1)$, the standard normal distribution. A fundamental property of the standard normal is that $z^{2}$ is distributed as a chi-square random variable with one degree of freedom, denoted $\\chi^{2}_{1}$. The Quantile-Quantile (QQ) plot compares observed quantiles of the test statistics to expected quantiles under $\\chi^{2}_{1}$, and the Genomic Control (GC) inflation factor $\\lambda_{GC}$ quantifies inflation by comparing the observed median of $\\chi^{2}_{1}$ statistics to the theoretical null median.\n\nThe GC inflation factor is defined as\n$$\\lambda_{GC} \\equiv \\frac{\\mathrm{median}\\big(\\{z_{i}^{2}\\}\\big)}{m_{0}},$$\nwhere $m_{0}$ is the theoretical median of $\\chi^{2}_{1}$, equal to $0.4549$.\n\nWe are given $n=17$ $z$-scores:\n$$[\\,0.00,\\,-0.15,\\,0.25,\\,-0.35,\\,0.45,\\,-0.55,\\,0.65,\\,-0.68,\\,0.71,\\,-0.75,\\,0.85,\\,-1.20,\\,1.50,\\,-2.00,\\,2.50,\\,-3.00,\\,3.50\\,].$$\n\nFirst, square each $z$-score to obtain the observed $\\chi^{2}_{1}$ statistics:\n- $0.00^{2} = 0.0000$\n- $(-0.15)^{2} = 0.0225$\n- $0.25^{2} = 0.0625$\n- $(-0.35)^{2} = 0.1225$\n- $0.45^{2} = 0.2025$\n- $(-0.55)^{2} = 0.3025$\n- $0.65^{2} = 0.4225$\n- $(-0.68)^{2} = 0.4624$\n- $0.71^{2} = 0.5041$\n- $(-0.75)^{2} = 0.5625$\n- $0.85^{2} = 0.7225$\n- $(-1.20)^{2} = 1.4400$\n- $1.50^{2} = 2.2500$\n- $(-2.00)^{2} = 4.0000$\n- $2.50^{2} = 6.2500$\n- $(-3.00)^{2} = 9.0000$\n- $3.50^{2} = 12.2500$\n\nOrder these values in ascending order (they are already in ascending order after squaring given the construction by increasing absolute value):\n$$[\\,0.0000,\\,0.0225,\\,0.0625,\\,0.1225,\\,0.2025,\\,0.3025,\\,0.4225,\\,0.4624,\\,0.5041,\\,0.5625,\\,0.7225,\\,1.4400,\\,2.2500,\\,4.0000,\\,6.2500,\\,9.0000,\\,12.2500\\,].$$\n\nFor an odd number $n=17$, the sample median is the $\\frac{n+1}{2} = 9$-th ordered value. Thus,\n$$\\mathrm{median}\\big(\\{z_{i}^{2}\\}\\big) = 0.5041.$$\n\nApply the GC definition with $m_{0} = 0.4549$:\n$$\\lambda_{GC} = \\frac{0.5041}{0.4549}.$$\n\nNote that $0.5041 = \\frac{5041}{10000}$ and $0.4549 = \\frac{4549}{10000}$, hence\n$$\\lambda_{GC} = \\frac{\\frac{5041}{10000}}{\\frac{4549}{10000}} = \\frac{5041}{4549}.$$\n\nCompute the numerical value and round to $4$ significant figures:\n$$\\lambda_{GC} \\approx 1.1081556...\\quad \\Rightarrow \\quad \\lambda_{GC} \\text{ (to }4\\text{ significant figures)} = 1.108.$$",
            "answer": "$$\\boxed{1.108}$$"
        },
        {
            "introduction": "Observing genomic inflation (a $\\lambda_{GC} \\gt 1.0$) presents a critical dilemma: is it the result of systemic bias like cryptic relatedness, or is it the signature of true, widespread polygenic signal? Answering this question correctly is vital for the validity of a study. This problem moves into advanced interpretation, challenging you to use the outputs of Linkage Disequilibrium Score Regression (LDSC)—a powerful technique that can distinguish between confounding and polygenicity—to make an informed judgment. This exercise simulates a real-world diagnostic scenario where you must correctly interpret these advanced metrics to guide subsequent analytical decisions .",
            "id": "4353250",
            "problem": "A pharmacogenomic Genome-Wide Association Study (GWAS) of warfarin maintenance dose in a hospital-based biobank enrolled patients across multiple ancestries, totaling $N = 120{,}000$ unrelated individuals after standard quality control and adjustment for $20$ ancestry principal components, age, sex, recruitment site, and genotyping batch. The study reports a Manhattan plot with numerous sub-genome-wide peaks dispersed across the genome and a Quantile-Quantile (QQ) plot showing early deviation from the null line. The genomic inflation factor (genomic control), $\\lambda_{\\mathrm{GC}}$, computed at the median test statistic, is $1.20$. Linkage Disequilibrium Score Regression (LDSC) yields an intercept of $1.02$ (standard error $0.01$) and a positive slope.\n\nUsing the core definitions that, under the global null, single-nucleotide polymorphism (SNP)-level test statistics approximately follow a $\\chi^2$ distribution with $1$ degree of freedom and should align with the QQ-plot null line, that the genomic control factor $\\lambda_{\\mathrm{GC}}$ summarizes global inflation relative to the median of the null, and that the LDSC intercept estimates the confounding-induced component of inflation whereas the slope captures polygenic signal correlated with Linkage Disequilibrium (LD), infer the most likely primary source of the observed inflation and recommend the most appropriate corrective action for valid inference in the context of precision medicine and genomic diagnostics.\n\nChoose the best option.\n\nA. The inflation is predominantly due to residual population stratification. Apply classical genomic control by dividing all $\\chi^2$ statistics by $\\lambda_{\\mathrm{GC}} = 1.20$, then reassess associations at the standard threshold.\n\nB. The inflation is largely due to true polygenic signal given the near-null LDSC intercept. Do not apply wholesale genomic control using $\\lambda_{\\mathrm{GC}}$; at most, adjust by the LDSC intercept (divide $\\chi^2$ by $1.02$) to account for minimal confounding, prefer linear mixed models to further absorb structure, and report LDSC estimates alongside results.\n\nC. The inflation is primarily caused by cryptic relatedness. Remove all related individuals until $\\lambda_{\\mathrm{GC}}$ is approximately $1.00$ and repeat the association scan.\n\nD. The inflation arises from technical batch artifacts that escaped covariate adjustment. Correct by quantile-normalizing $p$-values to the null and lower the genome-wide threshold to $5 \\times 10^{-9}$ to offset residual bias.\n\nE. The inflation indicates systematic bias irrespective of source. Keep the current analysis but increase the genome-wide significance threshold from $5 \\times 10^{-8}$ to $5 \\times 10^{-9}$ to compensate for the elevated false positive rate.",
            "solution": "The problem requires interpreting a set of GWAS quality control metrics to distinguish between true polygenic signal and confounding bias. The key is to understand how the genomic inflation factor ($\\lambda_{\\mathrm{GC}}$) and the outputs of LD Score Regression (LDSC) provide complementary information.\n\n1.  **Interpreting the Metrics:**\n    *   **Genomic Inflation Factor ($\\lambda_{\\mathrm{GC}} = 1.20$):** This value is significantly elevated, indicating that the observed test statistics are inflated genome-wide compared to the null expectation. This inflation can be caused by true polygenicity (many variants with small, real effects) or by confounding factors like population stratification or cryptic relatedness.\n    *   **LDSC Intercept ($1.02$, SE $0.01$):** LDSC is designed to separate these two sources of inflation. The intercept of the LDSC regression captures inflation that is independent of local linkage disequilibrium (LD) patterns. This is the signature of confounding bias. An intercept of $1.0$ represents no confounding. Here, the intercept is $1.02$, which is only marginally elevated and very close to the null expectation of $1.0$. This indicates that confounding factors like population stratification or cryptic relatedness are *not* the primary drivers of the observed inflation.\n    *   **LDSC Slope (Positive):** The slope of the LDSC regression captures inflation that correlates with the LD score of each SNP. A positive slope is the classic signature of true polygenicity, as it shows that SNPs in regions of higher LD (which tag more genetic variation) have stronger association signals on average.\n    *   **Conclusion from Metrics:** The combination of a high $\\lambda_{\\mathrm{GC}}$ and a near-null LDSC intercept strongly suggests that the observed inflation is predominantly due to true, widespread polygenic signal, not confounding bias.\n\n2.  **Evaluating the Options:**\n\n    *   **A. Apply classical genomic control:** This is incorrect. Applying a correction based on $\\lambda_{\\mathrm{GC}}=1.20$ would be overly conservative because it fails to distinguish between polygenicity and confounding, and would thus erase the true genetic signal we aim to detect.\n    *   **B. Inflation is due to polygenic signal:** This aligns perfectly with our interpretation of the LDSC results. The recommended actions—avoiding harsh genomic control, using methods like linear mixed models that properly account for genetic structure, and reporting LDSC results for transparency—are all consistent with modern best practices for analyzing large-scale GWAS of polygenic traits.\n    *   **C. Inflation is due to cryptic relatedness:** This is incorrect. Significant cryptic relatedness would inflate the LDSC intercept, which is not observed here.\n    *   **D. Inflation is due to batch artifacts:** This is incorrect for the same reason as C. Technical artifacts are a form of confounding that would inflate the LDSC intercept. The proposed correction (quantile-normalization) is statistically invalid.\n    *   **E. Increase the significance threshold:** This is a crude and inadequate fix. It does not address the underlying statistical model and would lead to a loss of power without properly correcting the test statistics for the minimal confounding that does exist.\n\nTherefore, the evidence points overwhelmingly to true polygenicity as the source of inflation, making option B the correct diagnosis and recommendation.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}