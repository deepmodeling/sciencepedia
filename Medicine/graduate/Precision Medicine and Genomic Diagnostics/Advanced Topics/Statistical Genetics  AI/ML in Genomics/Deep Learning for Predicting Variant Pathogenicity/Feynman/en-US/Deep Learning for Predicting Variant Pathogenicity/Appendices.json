{
    "hands_on_practices": [
        {
            "introduction": "A deep learning model's output, often a probability score, is only the first step in making a diagnostic call. To translate this score into a definitive classification like \"pathogenic\" or \"benign,\" we must establish a decision threshold. This exercise  delves into the crucial task of selecting an optimal threshold by analytically deriving the point that maximizes the $F_1$ score, a metric that balances the trade-off between precision and recall.",
            "id": "4330573",
            "problem": "In a precision medicine pipeline for genomic diagnostics, a deep learning classifier outputs a calibrated posterior probability $p \\in [0,1]$ that a given single-nucleotide variant is truly pathogenic, conditional on observed molecular features. Consider a held-out evaluation cohort that is balanced in composition, with true pathogenic variants and true benign variants present in equal proportions. Let $Y \\in \\{1,0\\}$ denote the true class ($Y=1$ pathogenic, $Y=0$ benign), and suppose the model’s calibrated scores have the following class-conditional distributions:\n- For pathogenic variants ($Y=1$), $p \\mid Y=1 \\sim \\operatorname{Beta}(2,1)$.\n- For benign variants ($Y=0$), $p \\mid Y=0 \\sim \\operatorname{Beta}(1,2)$.\n\nAssume the Beta distribution has the density $f(p; a,b) = \\frac{1}{B(a,b)} p^{a-1} (1-p)^{b-1}$ for $0 \\leq p \\leq 1$, with $B(a,b)$ the Beta function. Also assume calibration means that for any decision threshold $\\tau \\in [0,1]$, the following foundational definitions hold:\n- The true positive rate (recall) is $\\operatorname{TPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=1)$.\n- The false positive rate is $\\operatorname{FPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=0)$.\n- The prevalence is $\\pi = \\mathbb{P}(Y=1)$, which here equals $\\pi = \\frac{1}{2}$ by the balanced cohort design.\n- The precision (positive predictive value) at threshold $\\tau$ is $\\operatorname{PPV}(\\tau) = \\mathbb{P}(Y=1 \\mid p \\geq \\tau)$ obtained by Bayes’ rule from the above probabilities and $\\pi$.\n- The $F_{1}$ score at $\\tau$ is the harmonic mean of precision and recall, $F_{1}(\\tau) = \\frac{2 \\,\\operatorname{PPV}(\\tau)\\,\\operatorname{TPR}(\\tau)}{\\operatorname{PPV}(\\tau) + \\operatorname{TPR}(\\tau)}$.\n\nThe decision rule for variant triage is to call a variant “pathogenic” if and only if $p \\geq \\tau$. Using only these foundational definitions and the stated score distributions, derive from first principles the operating threshold $\\tau^{\\star}$ that maximizes the $F_{1}$ score on this cohort. Provide the final result as a single exact analytic expression for $\\tau^{\\star}$ with no units. If you choose to express a numerical approximation, also include the exact form; do not round unless you explicitly provide both, and do not include any units. The final answer must be a single number or a single closed-form expression.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and internally consistent. All provided definitions and distributions are standard in statistics and machine learning, and the context of genomic diagnostics is appropriate. The distributions for the classifier scores are not only valid but also satisfy the condition of perfect calibration for the given balanced cohort, which is an elegant property. Therefore, the problem is valid, and we may proceed to a solution.\n\nThe objective is to find the operating threshold $\\tau^{\\star} \\in [0,1]$ that maximizes the $F_{1}$ score. The $F_{1}$ score is defined as:\n$$F_{1}(\\tau) = \\frac{2 \\,\\operatorname{PPV}(\\tau)\\,\\operatorname{TPR}(\\tau)}{\\operatorname{PPV}(\\tau) + \\operatorname{TPR}(\\tau)}$$\nWhere $\\operatorname{PPV}(\\tau)$ is the precision (positive predictive value) and $\\operatorname{TPR}(\\tau)$ is the true positive rate (recall) at the decision threshold $\\tau$.\n\nFirst, we derive an expression for the true positive rate, $\\operatorname{TPR}(\\tau)$. By definition, $\\operatorname{TPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=1)$. The scores for pathogenic variants ($Y=1$) are distributed as $p \\mid Y=1 \\sim \\operatorname{Beta}(2,1)$.\nThe probability density function (PDF) for a $\\operatorname{Beta}(a,b)$ distribution is $f(p; a,b) = \\frac{1}{B(a,b)} p^{a-1} (1-p)^{b-1}$.\nThe Beta function $B(a,b)$ is given by $\\int_{0}^{1} x^{a-1}(1-x)^{b-1} dx$.\nFor $a=2$ and $b=1$, $B(2,1) = \\int_{0}^{1} p^{2-1}(1-p)^{1-1} dp = \\int_{0}^{1} p \\,dp = [\\frac{p^2}{2}]_{0}^{1} = \\frac{1}{2}$.\nThus, the PDF for pathogenic variants is $f(p \\mid Y=1) = \\frac{p^{1}}{1/2} = 2p$ for $p \\in [0,1]$.\nWe can now calculate $\\operatorname{TPR}(\\tau)$:\n$$\\operatorname{TPR}(\\tau) = \\int_{\\tau}^{1} f(p \\mid Y=1) \\,dp = \\int_{\\tau}^{1} 2p \\,dp = [p^2]_{\\tau}^{1} = 1 - \\tau^2$$\n\nNext, we derive an expression for the precision, $\\operatorname{PPV}(\\tau) = \\mathbb{P}(Y=1 \\mid p \\geq \\tau)$. Using Bayes' rule and the law of total probability:\n$$\\operatorname{PPV}(\\tau) = \\frac{\\mathbb{P}(p \\geq \\tau \\mid Y=1)\\mathbb{P}(Y=1)}{\\mathbb{P}(p \\geq \\tau \\mid Y=1)\\mathbbP(Y=1) + \\mathbb{P}(p \\geq \\tau \\mid Y=0)\\mathbb{P}(Y=0)}$$\nThis is equivalent to:\n$$\\operatorname{PPV}(\\tau) = \\frac{\\operatorname{TPR}(\\tau)\\pi}{\\operatorname{TPR}(\\tau)\\pi + \\operatorname{FPR}(\\tau)(1-\\pi)}$$\nWe are given the prevalence $\\pi = \\mathbb{P}(Y=1) = \\frac{1}{2}$. We need to compute the false positive rate, $\\operatorname{FPR}(\\tau) = \\mathbb{P}(p \\geq \\tau \\mid Y=0)$.\nThe scores for benign variants ($Y=0$) are distributed as $p \\mid Y=0 \\sim \\operatorname{Beta}(1,2)$.\nFor $a=1$ and $b=2$, $B(1,2) = \\int_{0}^{1} p^{1-1}(1-p)^{2-1} dp = \\int_{0}^{1} (1-p) \\,dp = [p - \\frac{p^2}{2}]_{0}^{1} = 1 - \\frac{1}{2} = \\frac{1}{2}$.\nThe PDF for benign variants is $f(p \\mid Y=0) = \\frac{(1-p)^{1}}{1/2} = 2(1-p)$ for $p \\in [0,1]$.\nWe can now calculate $\\operatorname{FPR}(\\tau)$:\n$$\\operatorname{FPR}(\\tau) = \\int_{\\tau}^{1} f(p \\mid Y=0) \\,dp = \\int_{\\tau}^{1} 2(1-p) \\,dp = 2[p - \\frac{p^2}{2}]_{\\tau}^{1} = 2\\left( (1-\\frac{1}{2}) - (\\tau - \\frac{\\tau^2}{2}) \\right) = 2(\\frac{1}{2} - \\tau + \\frac{\\tau^2}{2}) = 1 - 2\\tau + \\tau^2 = (1-\\tau)^2$$\nNow, we substitute $\\operatorname{TPR}(\\tau)$, $\\operatorname{FPR}(\\tau)$, and $\\pi$ into the expression for $\\operatorname{PPV}(\\tau)$:\n$$\\operatorname{PPV}(\\tau) = \\frac{(1-\\tau^2)(\\frac{1}{2})}{(1-\\tau^2)(\\frac{1}{2}) + (1-\\tau)^2(\\frac{1}{2})} = \\frac{1-\\tau^2}{1-\\tau^2 + (1-\\tau)^2}$$\nFor $\\tau \\neq 1$, we can factor $1-\\tau^2 = (1-\\tau)(1+\\tau)$ and $(1-\\tau)^2 = (1-\\tau)(1-\\tau)$:\n$$\\operatorname{PPV}(\\tau) = \\frac{(1-\\tau)(1+\\tau)}{(1-\\tau)(1+\\tau) + (1-\\tau)(1-\\tau)} = \\frac{1+\\tau}{(1+\\tau) + (1-\\tau)} = \\frac{1+\\tau}{2}$$\n\nNow we can write $F_1(\\tau)$ entirely as a function of $\\tau$:\n$$F_{1}(\\tau) = \\frac{2 \\left(\\frac{1+\\tau}{2}\\right)(1-\\tau^2)}{\\frac{1+\\tau}{2} + (1-\\tau^2)} = \\frac{(1+\\tau)(1-\\tau^2)}{\\frac{1+\\tau+2(1-\\tau^2)}{2}} = \\frac{2(1+\\tau)(1-\\tau^2)}{1+\\tau+2-2\\tau^2} = \\frac{2(1+\\tau)(1-\\tau)(1+\\tau)}{-2\\tau^2+\\tau+3}$$\nThe denominator can be factored as $-2\\tau^2+\\tau+3 = (3-2\\tau)(1+\\tau)$. For $\\tau \\in [0,1)$, $1+\\tau \\neq 0$, so we can simplify:\n$$F_{1}(\\tau) = \\frac{2(1+\\tau)(1-\\tau)(1+\\tau)}{(3-2\\tau)(1+\\tau)} = \\frac{2(1-\\tau)(1+\\tau)}{3-2\\tau} = \\frac{2(1-\\tau^2)}{3-2\\tau}$$\nTo find the maximum, we compute the derivative of $F_{1}(\\tau)$ with respect to $\\tau$ and set it to $0$. Using the quotient rule:\n$$\\frac{dF_1}{d\\tau} = 2 \\frac{(-2\\tau)(3-2\\tau) - (1-\\tau^2)(-2)}{(3-2\\tau)^2} = 2 \\frac{-6\\tau+4\\tau^2+2-2\\tau^2}{(3-2\\tau)^2} = \\frac{2(2\\tau^2-6\\tau+2)}{(3-2\\tau)^2} = \\frac{4(\\tau^2-3\\tau+1)}{(3-2\\tau)^2}$$\nFor the derivative to be zero, the numerator must be zero:\n$$\\tau^2 - 3\\tau + 1 = 0$$\nWe solve this quadratic equation for $\\tau$:\n$$\\tau = \\frac{-(-3) \\pm \\sqrt{(-3)^2 - 4(1)(1)}}{2(1)} = \\frac{3 \\pm \\sqrt{9-4}}{2} = \\frac{3 \\pm \\sqrt{5}}{2}$$\nThis gives two potential solutions: $\\tau_1 = \\frac{3 + \\sqrt{5}}{2}$ and $\\tau_2 = \\frac{3 - \\sqrt{5}}{2}$.\nThe value of $\\sqrt{5}$ is approximately $2.236$.\n$\\tau_1 \\approx \\frac{3+2.236}{2} \\approx 2.618$. This value is outside the valid domain for a probability threshold, $[0,1]$.\n$\\tau_2 \\approx \\frac{3-2.236}{2} \\approx 0.382$. This value is within the domain $[0,1]$.\nTherefore, the only critical point within the valid range is $\\tau^{\\star} = \\frac{3 - \\sqrt{5}}{2}$.\n\nTo confirm this is a maximum, we can examine the sign of the first derivative. The denominator $(3-2\\tau)^2$ is always positive for $\\tau \\neq 3/2$. The sign of the derivative is determined by the numerator, $g(\\tau) = \\tau^2-3\\tau+1$, which is an upward-opening parabola. For $\\tau < \\tau_2$, $g(\\tau) > 0$ (e.g., $g(0)=1$), so $F_1(\\tau)$ is increasing. For $\\tau > \\tau_2$ (but less than $\\tau_1$), $g(\\tau) < 0$, so $F_1(\\tau)$ is decreasing. This confirms that $\\tau^{\\star} = \\frac{3 - \\sqrt{5}}{2}$ corresponds to a local maximum. Since it is the only critical point in the interval $[0,1)$, and $F_1(0)=2/3 \\approx 0.667$ and $F_1(1)=0$, this local maximum is the global maximum on the interval.\n\nThe optimal threshold $\\tau^{\\star}$ that maximizes the $F_1$ score is $\\frac{3 - \\sqrt{5}}{2}$.",
            "answer": "$$\\boxed{\\frac{3 - \\sqrt{5}}{2}}$$"
        },
        {
            "introduction": "Predicting variant pathogenicity often involves integrating evidence from multiple, independent data sources or models. Simply averaging scores is rarely optimal; a more principled approach is needed to weigh each source by its reliability. This practice  guides you through implementing a robust evidence aggregator based on the Gauss–Markov theorem, using inverse-variance weighting to combine calibrated log-odds from different modalities into a single, statistically optimal posterior probability.",
            "id": "4330543",
            "problem": "You are tasked to design and implement a principled aggregator for integrating multimodal evidence from independent deep learning models that estimate the pathogenicity of genomic variants. The goal is to produce a single posterior probability of pathogenicity for each variant by combining several modality-specific outputs in a way that is consistent with probability theory and optimal estimation.\n\nStart from a context-appropriate fundamental base:\n- The definition of posterior probability from Bayes' theorem: for a binary label $Y \\in \\{0,1\\}$ and evidence $x$, the posterior odds satisfy $$\\frac{\\mathbb{P}(Y=1 \\mid x)}{\\mathbb{P}(Y=0 \\mid x)} = \\frac{\\pi}{1-\\pi} \\cdot \\Lambda(x),$$ where $\\pi \\in (0,1)$ is the prior probability of pathogenicity and $\\Lambda(x)$ is the likelihood ratio contributed by the observed evidence.\n- The property that the logit transform of a probability is its log-odds: $$\\operatorname{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right).$$\n- The Gauss–Markov theorem (best linear unbiased estimator): given independent, unbiased estimators with known variances for a common latent quantity, the inverse-variance weighted average minimizes mean squared error among linear unbiased estimators.\n\nAssume the following modality model. You have $M$ modalities indexed by $m \\in \\{1,\\dots,M\\}$. Each modality produces an estimated probability $p_m \\in (0,1)$ that a variant is pathogenic based solely on its modality-specific evidence, under an implicit neutral prior of $0.5$. After passing through a calibration step with a positive temperature $t_m > 0$, the calibrated modality-specific log-odds is $$z_m = \\frac{\\operatorname{logit}(p_m)}{t_m}.$$ Assume that each $z_m$ is an unbiased noisy estimate of a common latent log-likelihood ratio $\\ell^\\star$ for the variant, i.e., $$z_m = \\ell^\\star + \\varepsilon_m,$$ where the noise terms satisfy $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma_m^2)$ and are independent across $m$, with known standard deviations $\\sigma_m > 0$. Under these assumptions, the inverse-variance weighting implied by the Gauss–Markov theorem yields a single integrated estimate of the latent log-likelihood ratio. Combining this integrated estimate with a specified prior probability $\\pi$ via Bayes' theorem produces the final posterior probability of pathogenicity.\n\nYour task is to implement a program that:\n- Takes a fixed test suite of parameter sets.\n- For each parameter set, computes the integrated posterior probability of pathogenicity as a decimal in $(0,1)$, using the principled approach derived from the base assumptions above.\n- Handles the numerical boundary conditions when any $p_m$ equals $0$ or $1$ by clipping $p_m$ to $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon = 10^{-12}$ before applying the logit transform.\n\nPrecisely, for each variant with parameters:\n- Prior probability of pathogenicity $\\pi \\in (0,1)$.\n- Modality probability outputs $\\{p_m\\}_{m=1}^M$, each in $(0,1)$, which must be treated as decimals without a percentage sign.\n- Calibration temperatures $\\{t_m\\}_{m=1}^M$, each in $\\mathbb{R}_{>0}$.\n- Modality noise standard deviations $\\{\\sigma_m\\}_{m=1}^M$, each in $\\mathbb{R}_{>0}$.\n\nYou must produce a single posterior probability $\\hat{p} \\in (0,1)$ per variant as follows, expressed purely in decimal units with no percent sign:\n- Use the base principles above to derive the integrated estimator of the latent log-likelihood ratio and then the posterior probability via Bayes' theorem. Do not introduce any heuristic combination rule that contradicts the base assumptions.\n\nTest suite:\nUse exactly the following six test cases, each specified by the tuple $(\\pi, \\{p_m\\}_{m=1}^M, \\{t_m\\}_{m=1}^M, \\{\\sigma_m\\}_{m=1}^M)$ with $M=3$:\n1. $(\\pi = 0.01, \\{p_m\\} = \\{0.8, 0.75, 0.6\\}, \\{t_m\\} = \\{1.2, 1.0, 1.5\\}, \\{\\sigma_m\\} = \\{0.6, 0.4, 0.8\\})$,\n2. $(\\pi = 0.5, \\{p_m\\} = \\{0.99, 0.02, 0.01\\}, \\{t_m\\} = \\{1.0, 1.1, 1.3\\}, \\{\\sigma_m\\} = \\{0.5, 0.3, 0.3\\})$,\n3. $(\\pi = 0.001, \\{p_m\\} = \\{1.0, 0.0, 0.5\\}, \\{t_m\\} = \\{1.0, 1.0, 1.0\\}, \\{\\sigma_m\\} = \\{0.2, 0.2, 1.0\\})$,\n4. $(\\pi = 0.9, \\{p_m\\} = \\{0.5, 0.5, 0.5\\}, \\{t_m\\} = \\{1.0, 1.0, 1.0\\}, \\{\\sigma_m\\} = \\{10.0, 10.0, 10.0\\})$,\n5. $(\\pi = 0.05, \\{p_m\\} = \\{0.7, 0.4, 0.65\\}, \\{t_m\\} = \\{2.0, 0.8, 1.4\\}, \\{\\sigma_m\\} = \\{1.5, 0.6, 0.9\\})$,\n6. $(\\pi = 0.2, \\{p_m\\} = \\{0.51, 0.49, 0.5\\}, \\{t_m\\} = \\{1.3, 1.3, 1.3\\}, \\{\\sigma_m\\} = \\{0.1, 5.0, 10.0\\})$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the six posterior probabilities for the six test cases as a comma-separated list enclosed in square brackets. Each probability must be formatted as a decimal rounded to six digits after the decimal point. For example, an output line should look like $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$ where each $\\text{result}_i$ is a decimal in $(0,1)$.",
            "solution": "We begin by formalizing the multimodal integration problem as the estimation of a latent log-likelihood ratio from multiple independent, noisy estimates produced by modality-specific deep learning models.\n\nLet $Y \\in \\{0,1\\}$ denote the binary pathogenicity label. Let the prior probability of pathogenicity be $\\pi \\in (0,1)$, so the prior odds are $\\frac{\\pi}{1-\\pi}$ and the prior log-odds are $$\\eta = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).$$ Consider $M$ modalities indexed by $m \\in \\{1,\\dots,M\\}$. Each modality produces an output probability $p_m \\in (0,1)$ which, under an implicit neutral prior of $0.5$, reflects the modality-specific posterior belief using only its evidence. The logit transform of this probability is an estimate of a modality-specific log-odds. To correct for calibration discrepancies, we apply temperature scaling with $t_m > 0$, yielding the calibrated modality log-odds\n$$\nz_m = \\frac{\\operatorname{logit}(p_m)}{t_m} = \\frac{1}{t_m} \\log\\left(\\frac{p_m}{1-p_m}\\right).\n$$\nWe posit the model\n$$\nz_m = \\ell^\\star + \\varepsilon_m,\n$$\nwhere $\\ell^\\star$ is the latent log-likelihood ratio contributed by the evidence across modalities, and $\\varepsilon_m \\sim \\mathcal{N}(0,\\sigma_m^2)$ are independent noise terms with known standard deviations $\\sigma_m > 0$. The independence and Gaussian noise assumptions are well tested in statistical modeling for aggregating unbiased estimates, and they allow us to apply the Gauss–Markov theorem.\n\nBy the Gauss–Markov theorem, among linear unbiased estimators of $\\ell^\\star$, the inverse-variance weighted average minimizes the mean squared error. Define weights\n$$\nw_m = \\frac{1}{\\sigma_m^2}.\n$$\nThe best linear unbiased estimator of $\\ell^\\star$ is then\n$$\n\\hat{\\ell} = \\frac{\\sum_{m=1}^M w_m z_m}{\\sum_{m=1}^M w_m}.\n$$\nThis $\\hat{\\ell}$ aggregates modality-specific calibrated log-odds into a single integrated log-likelihood ratio estimate.\n\nBayes' theorem connects likelihood ratios with posterior odds. The posterior odds given the joint evidence $x$ are\n$$\n\\frac{\\mathbb{P}(Y=1 \\mid x)}{\\mathbb{P}(Y=0 \\mid x)} = \\frac{\\pi}{1-\\pi} \\cdot \\Lambda(x),\n$$\nand taking logs gives the posterior log-odds\n$$\n\\log\\left(\\frac{\\mathbb{P}(Y=1 \\mid x)}{\\mathbb{P}(Y=0 \\mid x)}\\right) = \\eta + \\log \\Lambda(x).\n$$\nUnder our model, $\\hat{\\ell}$ serves as an estimator of $\\log \\Lambda(x)$. Therefore, the integrated posterior log-odds is\n$$\n\\hat{z} = \\eta + \\hat{\\ell}.\n$$\nFinally, we map $\\hat{z}$ back to a probability using the logistic function (the inverse of the logit),\n$$\n\\hat{p} = \\frac{1}{1 + \\exp(-\\hat{z})}.\n$$\n\nNumerical stability:\nThe logit function is undefined at $p_m = 0$ or $p_m = 1$. To ensure numerical stability while respecting the probabilistic semantics, we clip each $p_m$ to the interval $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon = 10^{-12}$ before applying the logit transform. This clipping preserves the intended extremal behavior while avoiding infinities in computation.\n\nAlgorithmic steps for each test case:\n1. Read $\\pi$, $\\{p_m\\}_{m=1}^M$, $\\{t_m\\}_{m=1}^M$, and $\\{\\sigma_m\\}_{m=1}^M$.\n2. Compute the prior log-odds $\\eta = \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$.\n3. For each modality $m$, clip $p_m$ to $[\\varepsilon,1-\\varepsilon]$ with $\\varepsilon = 10^{-12}$, then compute $$z_m = \\frac{1}{t_m} \\log\\left(\\frac{p_m}{1-p_m}\\right).$$\n4. Compute weights $w_m = \\frac{1}{\\sigma_m^2}$ and the integrated estimator $$\\hat{\\ell} = \\frac{\\sum_{m=1}^M w_m z_m}{\\sum_{m=1}^M w_m}.$$\n5. Compute the integrated posterior log-odds $\\hat{z} = \\eta + \\hat{\\ell}$ and the posterior probability $$\\hat{p} = \\frac{1}{1 + \\exp(-\\hat{z})}.$$\n6. Round $\\hat{p}$ to six digits after the decimal point and output.\n\nEdge cases in the provided test suite:\n- When $\\pi$ is very small (e.g., $\\pi = 0.001$), even strongly conflicting modalities may cancel in $\\hat{\\ell}$, and the posterior is dominated by the prior, yielding $\\hat{p}$ close to $\\pi$.\n- When all modalities produce $p_m = 0.5$ and have large $\\sigma_m$, the integrated evidence is uninformative, and the output $\\hat{p}$ tends toward the prior $\\pi$.\n- When one modality is highly reliable (small $\\sigma_m$) and others are unreliable (large $\\sigma_m$), the integrated estimate appropriately emphasizes the reliable modality.\n\nThe program implements these steps for the six specified cases and prints a single line containing the six results as a comma-separated list enclosed in square brackets. Each result is a decimal in $(0,1)$ rounded to six digits after the decimal point, satisfying the final output specification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef clip_probabilities(p_array, eps=1e-12):\n    # Clip probabilities to avoid logit singularities at 0 and 1.\n    return np.clip(p_array, eps, 1.0 - eps)\n\ndef logit(p):\n    # Compute logit safely: log(p/(1-p))\n    return np.log(p) - np.log1p(-p)\n\ndef integrate_multimodal(pi, p_list, t_list, sigma_list, eps=1e-12):\n    # Convert inputs to numpy arrays\n    p = clip_probabilities(np.array(p_list, dtype=float), eps=eps)\n    t = np.array(t_list, dtype=float)\n    sigma = np.array(sigma_list, dtype=float)\n\n    # Prior log-odds\n    prior_log_odds = np.log(pi) - np.log(1.0 - pi)\n\n    # Calibrated modality log-odds\n    z = logit(p) / t\n\n    # Inverse-variance weights\n    w = 1.0 / (sigma ** 2)\n\n    # Integrated latent log-likelihood ratio via inverse-variance weighted average\n    ell_hat = np.sum(w * z) / np.sum(w)\n\n    # Posterior log-odds and probability\n    z_total = prior_log_odds + ell_hat\n    p_hat = 1.0 / (1.0 + np.exp(-z_total))\n    return p_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (pi, p_list, t_list, sigma_list)\n        (0.01, [0.8, 0.75, 0.6], [1.2, 1.0, 1.5], [0.6, 0.4, 0.8]),\n        (0.5, [0.99, 0.02, 0.01], [1.0, 1.1, 1.3], [0.5, 0.3, 0.3]),\n        (0.001, [1.0, 0.0, 0.5], [1.0, 1.0, 1.0], [0.2, 0.2, 1.0]),\n        (0.9, [0.5, 0.5, 0.5], [1.0, 1.0, 1.0], [10.0, 10.0, 10.0]),\n        (0.05, [0.7, 0.4, 0.65], [2.0, 0.8, 1.4], [1.5, 0.6, 0.9]),\n        (0.2, [0.51, 0.49, 0.5], [1.3, 1.3, 1.3], [0.1, 5.0, 10.0]),\n    ]\n\n    results = []\n    for case in test_cases:\n        pi, p_list, t_list, sigma_list = case\n        result = integrate_multimodal(pi, p_list, t_list, sigma_list, eps=1e-12)\n        # Format to six decimal places\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In clinical settings, conveying the uncertainty of a prediction is as important as the prediction itself. Instead of a single \"best guess,\" it is often more responsible to provide a set of possible labels. This exercise  introduces class-conditional conformal prediction, a powerful, distribution-free framework for constructing prediction sets that are guaranteed to contain the true variant status with a pre-specified probability, offering a rigorous way to handle diagnostic ambiguity.",
            "id": "4330594",
            "problem": "A deep learning classifier for genomic variant interpretation outputs, for each variant, a pair of estimated class probabilities for benign and pathogenic status. Let the two labels be $0$ for benign and $1$ for pathogenic. Consider the following construction of a prediction set for a test variant using class-conditional split inductive conformal prediction. The foundational base is the assumption of exchangeability (equivalently, independence and identical distribution in the calibration and test phases) between the calibration data and the test variant, and the monotonicity of nonconformity scores with respect to error risk. Specifically, suppose a model provides an estimated probability vector $\\hat{p}(x) = (\\hat{p}_0(x), \\hat{p}_1(x))$ for any variant $x$, with $\\hat{p}_0(x) + \\hat{p}_1(x) = 1$. Define a nonconformity score for any candidate label $y \\in \\{0,1\\}$ as $A(x,y) = 1 - \\hat{p}_y(x)$, which is larger when the classifier is less confident in label $y$. For each class $y \\in \\{0,1\\}$, collect calibration scores only from calibration variants whose true label is $y$; denote the multiset of these scores by $\\mathcal{S}_y$. For a test variant $x^{\\ast}$, compute a class-specific conformity assessment by comparing $A(x^{\\ast}, y)$ to the empirical distribution of $\\mathcal{S}_y$. A decision set at target miscoverage level $\\alpha \\in (0,1)$ is obtained by including all labels $y$ whose tail probability under $\\mathcal{S}_y$ is sufficiently large to control miscoverage.\n\nYour task is to derive, from the exchangeability assumption and the definition of $A(x,y)$ above, a finite-sample valid, non-randomized class-conditional decision rule that yields a prediction set $\\Gamma_{\\alpha}(x^{\\ast}) \\subseteq \\{0,1\\}$ with marginal coverage at least $1 - \\alpha$, and to implement it. You must adhere to the following constraints and clarifications.\n\n- Fundamental base to use: exchangeability of calibration and test instances, the definition of the nonconformity score $A(x,y) = 1 - \\hat{p}_y(x)$, and the ranking-based validity of inductive conformal methods under exchangeability. Do not assume any asymptotics or parametric likelihood models beyond these foundations.\n- You must express the decision rule purely in terms of counts over $\\mathcal{S}_y$ and the test score $A(x^{\\ast}, y)$, using only operations justified by the exchangeability-based rank argument.\n- The rule must be class-conditional, meaning calibration scores for each candidate label $y$ are compared only to $\\mathcal{S}_y$.\n- All candidate labels $y$ satisfying the rule must be included in the final decision set $\\Gamma_{\\alpha}(x^{\\ast})$, and no other labels may be included.\n\nUse the following test suite. In each case, you are given:\n- Calibration arrays for each class: for benign ($y=0$), a list of values that are equal to $\\hat{p}_0(x_i)$ for calibration variants with true label $0$; for pathogenic ($y=1$), a list of values that are equal to $\\hat{p}_1(x_i)$ for calibration variants with true label $1$. You must convert each of these to nonconformity scores by applying $A(x_i,y) = 1 - \\hat{p}_y(x_i)$.\n- A test variant’s estimated probability $\\hat{p}_1(x^{\\ast})$ for pathogenic; by conservation, $\\hat{p}_0(x^{\\ast}) = 1 - \\hat{p}_1(x^{\\ast})$.\n- A target miscoverage $\\alpha$.\n\nConstruct $\\Gamma_{\\alpha}(x^{\\ast})$ as above for each case. Encode the final decision set as a list of integers in ascending order, where benign is encoded as $0$ and pathogenic as $1$.\n\nTest cases:\n\n- Case $1$ (happy path): benign calibration $\\{\\hat{p}_0\\} = [0.85, 0.75, 0.65, 0.55, 0.50]$, pathogenic calibration $\\{\\hat{p}_1\\} = [0.92, 0.80, 0.70, 0.60]$, test $\\hat{p}_1(x^{\\ast}) = 0.88$, target $\\alpha = 0.20$.\n- Case $2$ (boundary tie case): benign calibration $\\{\\hat{p}_0\\} = [0.70]$, pathogenic calibration $\\{\\hat{p}_1\\} = [0.70]$, test $\\hat{p}_1(x^{\\ast}) = 0.60$, target $\\alpha = 0.50$.\n- Case $3$ (ambiguous inclusion): benign calibration $\\{\\hat{p}_0\\} = [0.52, 0.62]$, pathogenic calibration $\\{\\hat{p}_1\\} = [0.55, 0.65]$, test $\\hat{p}_1(x^{\\ast}) = 0.58$, target $\\alpha = 0.25$.\n\nYour program must compute the three decision sets in the order listed and produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each decision set itself printed as a bracketed, comma-separated list of integers in ascending order and with no spaces anywhere. For example, a valid output format for three results is $[[],[],[]]$, but without any spaces. Therefore, your program’s output must look like $[[\\dots],[\\dots],[\\dots]]$, where each inner bracket contains zero, one, or two integers chosen from $\\{0,1\\}$.",
            "solution": "The problem requires the derivation and implementation of a class-conditional split inductive conformal prediction rule. The derivation must be based on the principle of exchangeability.\n\nLet the two classes be $y \\in \\{0, 1\\}$, representing benign and pathogenic variants, respectively. A machine learning model provides an estimated probability vector $\\hat{p}(x) = (\\hat{p}_0(x), \\hat{p}_1(x))$ for any variant $x$, where $\\hat{p}_0(x) + \\hat{p}_1(x) = 1$. The nonconformity score for a data point $(x,y)$ with respect to its true label $y$ is defined as $A(x,y) = 1 - \\hat{p}_y(x)$. This score measures how \"unusual\" the observation is, with higher scores indicating greater nonconformity.\n\nThe procedure is class-conditional and operates in a split-inductive setting. For each class $y \\in \\{0,1\\}$, we are given a calibration set $\\mathcal{C}_y$ of examples whose true label is $y$. Let $n_y = |\\mathcal{C}_y|$ be the number of calibration examples for class $y$. The multiset of nonconformity scores for class $y$ is computed as $\\mathcal{S}_y = \\{A(x_i, y_i) | (x_i, y_i) \\in \\mathcal{C}_y \\text{ and } y_i=y\\}$.\n\nThe fundamental principle of inductive conformal prediction is based on exchangeability. For a new test point $x^{\\ast}$, we wish to determine if a candidate label $y$ should be included in its prediction set. We form a hypothesis that the true label of $x^{\\ast}$ is $y$. If this hypothesis is true, then the set of $n_y+1$ data points, consisting of the $n_y$ calibration points for class $y$ and the test point $(x^{\\ast}, y)$, is an exchangeable sequence. Consequently, the sequence of their nonconformity scores, $\\{s_{y,1}, \\dots, s_{y,n_y}, s_y^{\\ast}\\}$, where $s_{y,i} = A(x_i, y)$ for calibration points and $s_y^{\\ast} = A(x^{\\ast}, y)$ for the test point, is also a sequence of exchangeable random variables.\n\nUnder exchangeability, the rank of the test score $s_y^{\\ast}$ within the combined set of $n_y+1$ scores is uniformly distributed on the integers $\\{1, 2, \\dots, n_y+1\\}$. This property allows us to define a \"p-value\" for the hypothesis that the true label of $x^{\\ast}$ is $y$. A non-randomized p-value, which provides finite-sample validity, is defined as the proportion of scores in the combined set that are at least as non-conforming as the test score.\n\nLet $k_y$ be the number of calibration scores for class $y$ that are greater than or equal to the test score for that class hypothesis:\n$$k_y = |\\{s \\in \\mathcal{S}_y : s \\ge s_y^{\\ast}\\}| = |\\{s \\in \\mathcal{S}_y : s \\ge A(x^{\\ast}, y)\\}|$$\nThe p-value for the hypothesis that the true label of $x^{\\ast}$ is $y$, denoted $\\pi_y(x^{\\ast})$, is given by:\n$$\\pi_y(x^{\\ast}) = \\frac{k_y + 1}{n_y + 1}$$\nThe term $k_y$ counts the number of calibration scores as non-conforming as the test score, and the '`$+1$`' in the numerator accounts for the test score itself.\n\nThe conformal prediction set, $\\Gamma_{\\alpha}(x^{\\ast})$, is constructed by including all labels $y$ that are not \"too surprising\" at a given significance level $\\alpha \\in (0,1)$. A label is considered not too surprising if its p-value is greater than $\\alpha$. The decision rule is therefore:\n$$\\text{Include } y \\text{ in } \\Gamma_{\\alpha}(x^{\\ast}) \\iff \\pi_y(x^{\\ast}) > \\alpha$$\nSubstituting the expression for the p-value, we obtain the final rule:\n$$\\text{Include } y \\text{ in } \\Gamma_{\\alpha}(x^{\\ast}) \\iff \\frac{k_y + 1}{n_y + 1} > \\alpha$$\nThis inequality can also be written as $k_y + 1 > \\alpha(n_y + 1)$.\n\nThis procedure is performed independently for each candidate label $y \\in \\{0, 1\\}$.\nFor $y=0$ (benign):\n1.  Compute the calibration scores: $\\mathcal{S}_0 = \\{1 - \\hat{p}_0(x_i) \\text{ for each } x_i \\text{ in the benign calibration set}\\}$. Let $n_0=|\\mathcal{S}_0|$.\n2.  Compute the test score: $s_0^{\\ast} = A(x^{\\ast}, 0) = 1 - \\hat{p}_0(x^{\\ast}) = 1 - (1 - \\hat{p}_1(x^{\\ast})) = \\hat{p}_1(x^{\\ast})$.\n3.  Count $k_0 = |\\{s \\in \\mathcal{S}_0 : s \\ge s_0^{\\ast}\\}|$.\n4.  Include $0$ in $\\Gamma_{\\alpha}(x^{\\ast})$ if $\\frac{k_0 + 1}{n_0 + 1} > \\alpha$.\n\nFor $y=1$ (pathogenic):\n1.  Compute the calibration scores: $\\mathcal{S}_1 = \\{1 - \\hat{p}_1(x_i) \\text{ for each } x_i \\text{ in the pathogenic calibration set}\\}$. Let $n_1=|\\mathcal{S}_1|$.\n2.  Compute the test score: $s_1^{\\ast} = A(x^{\\ast}, 1) = 1 - \\hat{p}_1(x^{\\ast})$.\n3.  Count $k_1 = |\\{s \\in \\mathcal{S}_1 : s \\ge s_1^{\\ast}\\}|$.\n4.  Include $1$ in $\\Gamma_{\\alpha}(x^{\\ast})$ if $\\frac{k_1 + 1}{n_1 + 1} > \\alpha$.\n\nThe resulting set $\\Gamma_{\\alpha}(x^{\\ast})$ is the union of all labels that satisfy their respective conditions. This construction guarantees marginal coverage, i.e., for any class $y$, the probability of including the true label in the prediction set is at least $1-\\alpha$, i.e., $P(y \\in \\Gamma_{\\alpha}(X) | Y=y) \\ge 1-\\alpha$.",
            "answer": "```python\nimport numpy as np\n\ndef compute_prediction_set(calib_p0, calib_p1, test_p1, alpha):\n    \"\"\"\n    Computes the class-conditional conformal prediction set.\n\n    Args:\n        calib_p0 (list): List of p_0 probabilities for benign calibration set.\n        calib_p1 (list): List of p_1 probabilities for pathogenic calibration set.\n        test_p1 (float): The p_1 probability for the test variant.\n        alpha (float): The target miscoverage level.\n\n    Returns:\n        list: The prediction set, a list of integers (0 or 1).\n    \"\"\"\n    prediction_set = []\n\n    # ----------- Label 0 (Benign) -----------\n    n0 = len(calib_p0)\n    if n0 > 0:\n        # Nonconformity score for calibration is 1 - p_0\n        scores_0 = 1 - np.array(calib_p0)\n        \n        # Test score for hypothesis y=0 is A(x*, 0) = 1 - p_0(x*) = p_1(x*)\n        test_score_0 = test_p1\n        \n        # Count calibration scores >= test score\n        k0 = np.sum(scores_0 >= test_score_0)\n        \n        # p-value check\n        p_value_0 = (k0 + 1) / (n0 + 1)\n        if p_value_0 > alpha:\n            prediction_set.append(0)\n    # If n0 is 0, p-value is 1/(0+1)=1, which is > alpha. The rule includes the label.\n    # However, problem cases have n_y > 0. The definition is upheld for n_y=0.\n    elif n0 == 0:\n         # p-value = 1/(0+1) = 1. Since alpha is in (0,1), 1 > alpha is always true.\n         prediction_set.append(0)\n\n\n    # ----------- Label 1 (Pathogenic) -----------\n    n1 = len(calib_p1)\n    if n1 > 0:\n        # Nonconformity score for calibration is 1 - p_1\n        scores_1 = 1 - np.array(calib_p1)\n        \n        # Test score for hypothesis y=1 is A(x*, 1) = 1 - p_1(x*)\n        test_score_1 = 1 - test_p1\n        \n        # Count calibration scores >= test score\n        k1 = np.sum(scores_1 >= test_score_1)\n        \n        # p-value check\n        p_value_1 = (k1 + 1) / (n1 + 1)\n        if p_value_1 > alpha:\n            prediction_set.append(1)\n    elif n1 == 0:\n        prediction_set.append(1)\n        \n    prediction_set.sort()\n    return prediction_set\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'calib_p0': [0.85, 0.75, 0.65, 0.55, 0.50],\n            'calib_p1': [0.92, 0.80, 0.70, 0.60],\n            'test_p1': 0.88,\n            'alpha': 0.20\n        },\n        {\n            'calib_p0': [0.70],\n            'calib_p1': [0.70],\n            'test_p1': 0.60,\n            'alpha': 0.50\n        },\n        {\n            'calib_p0': [0.52, 0.62],\n            'calib_p1': [0.55, 0.65],\n            'test_p1': 0.58,\n            'alpha': 0.25\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_prediction_set(\n            case['calib_p0'],\n            case['calib_p1'],\n            case['test_p1'],\n            case['alpha']\n        )\n        results.append(str(result).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}