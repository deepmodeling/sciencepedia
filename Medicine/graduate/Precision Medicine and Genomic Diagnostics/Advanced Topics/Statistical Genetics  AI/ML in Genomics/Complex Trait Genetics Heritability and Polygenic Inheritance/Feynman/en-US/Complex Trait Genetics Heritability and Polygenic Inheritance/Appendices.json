{
    "hands_on_practices": [
        {
            "introduction": "Understanding heritability begins with the foundational methods of quantitative genetics. This practice explores the classical approach of estimating narrow-sense heritability ($h^2$) by examining the resemblance between relatives, specifically through parent-offspring regression . By deriving the relationship between the regression slope and $h^2$, you will solidify your grasp of this core concept and appreciate how it connects theoretical genetic variance to observable phenotypic data. Furthermore, this exercise introduces the critical real-world complication of measurement error, demonstrating how it attenuates statistical estimates and why accounting for it is essential for accurate scientific inference.",
            "id": "4328507",
            "problem": "In a population-based biobank study targeting precision medicine and genomic diagnostics, consider a complex quantitative trait such as adult systolic blood pressure. Assume an additive polygenic architecture with random mating and no assortative mating, and ignore dominance, epistasis, shared environmental covariance between generations, and gene–environment correlation. Let the narrow-sense heritability be defined as $h^{2} = V_{A} / V_{P}$, where $V_{A}$ is the additive genetic variance and $V_{P}$ is the total phenotypic variance.\n\nA mid-parent–offspring regression is performed using the observed mid-parent phenotype as predictor and the offspring phenotype as response. Let $Y_{f}^{\\mathrm{true}}$ and $Y_{m}^{\\mathrm{true}}$ denote the true parental phenotypes, and $Y_{f}^{\\mathrm{obs}} = Y_{f}^{\\mathrm{true}} + e_{f}$, $Y_{m}^{\\mathrm{obs}} = Y_{m}^{\\mathrm{true}} + e_{m}$ their observed values, where $e_{f}$ and $e_{m}$ are independent classical measurement errors with zero mean and variance $\\sigma_{e}^{2}$, independent of all true phenotypes. The true mid-parent is $M^{\\mathrm{true}} = \\frac{Y_{f}^{\\mathrm{true}} + Y_{m}^{\\mathrm{true}}}{2}$ and the observed mid-parent is $M^{\\mathrm{obs}} = \\frac{Y_{f}^{\\mathrm{obs}} + Y_{m}^{\\mathrm{obs}}}{2} = M^{\\mathrm{true}} + \\varepsilon_{M}$ with $\\varepsilon_{M} = \\frac{e_{f} + e_{m}}{2}$. The offspring phenotype $Y_{o}$ is measured with negligible error. In the dataset, the ordinary least squares slope of the offspring on the observed mid-parent is $b = 0.45$. Independent test–retest studies establish that the per-parent test–retest reliability (defined as $\\mathrm{Var}(Y^{\\mathrm{true}})/\\mathrm{Var}(Y^{\\mathrm{obs}})$) is $r_{p} = 0.80$.\n\nStarting from core quantitative genetics definitions and properties of covariance in pedigrees, derive the relationship between the regression slope of offspring on the true mid-parent and $h^{2}$, and then derive how classical measurement error in the parents attenuates the observed slope. Using these derivations, compute the measurement-error-corrected narrow-sense heritability $h^{2}$ for this dataset, reporting a single number. No rounding is required.",
            "solution": "The problem requires the calculation of the narrow-sense heritability, $h^2$, corrected for measurement error in the parental phenotypes. The solution will proceed in three stages as requested by the prompt: first, deriving the relationship between the true mid-parent–offspring regression slope and $h^2$; second, deriving the attenuation of this slope by measurement error; and third, applying these derivations to compute $h^2$ from the provided data.\n\nThe fundamental model for a quantitative trait phenotype, ignoring dominance and epistasis as specified, is $Y^{\\mathrm{true}} = A + E_{u}$, where $A$ is the additive genetic value and $E_{u}$ is the unique environmental contribution. The total phenotypic variance is $V_P = \\mathrm{Var}(Y^{\\mathrm{true}}) = \\mathrm{Var}(A) + \\mathrm{Var}(E_{u}) = V_A + V_E$. The narrow-sense heritability is defined as $h^2 = V_A / V_P$.\n\n**1. Regression of Offspring Phenotype on True Mid-Parent Phenotype**\n\nThe slope of a simple linear regression of a response variable $Y$ on a predictor variable $X$ is given by $\\beta_{Y|X} = \\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(X)}$. Here, the response is the offspring phenotype, $Y_o$, and the predictor is the true mid-parent phenotype, $M^{\\mathrm{true}} = \\frac{Y_{f}^{\\mathrm{true}} + Y_{m}^{\\mathrm{true}}}{2}$. The offspring's phenotype is measured with negligible error, so we can denote it as $Y_o$. The slope, which we shall call $\\beta^{\\mathrm{true}}$, is:\n$$\n\\beta^{\\mathrm{true}} = \\frac{\\mathrm{Cov}(Y_o, M^{\\mathrm{true}})}{\\mathrm{Var}(M^{\\mathrm{true}})}\n$$\nWe must compute the covariance in the numerator and the variance in the denominator.\n\nNumerator: $\\mathrm{Cov}(Y_o, M^{\\mathrm{true}})$\nUsing the definition of $M^{\\mathrm{true}}$ and properties of covariance:\n$$\n\\mathrm{Cov}(Y_o, M^{\\mathrm{true}}) = \\mathrm{Cov}\\left(Y_o, \\frac{Y_{f}^{\\mathrm{true}} + Y_{m}^{\\mathrm{true}}}{2}\\right) = \\frac{1}{2} \\left[ \\mathrm{Cov}(Y_o, Y_{f}^{\\mathrm{true}}) + \\mathrm{Cov}(Y_o, Y_{m}^{\\mathrm{true}}) \\right]\n$$\nThe covariance between an offspring and one parent (e.g., the father) is $\\mathrm{Cov}(Y_o, Y_{f}^{\\mathrm{true}})$. Under the problem's assumptions (no shared environment between generations, purely additive genetics), this covariance is equal to the covariance of their additive genetic values: $\\mathrm{Cov}(Y_o, Y_{f}^{\\mathrm{true}}) = \\mathrm{Cov}(A_o, A_f)$. A child receives a random half of its alleles from each parent, so the covariance of additive genetic values between a parent and offspring is $\\frac{1}{2}V_A$. Thus:\n$$\n\\mathrm{Cov}(Y_o, Y_{f}^{\\mathrm{true}}) = \\mathrm{Cov}(Y_o, Y_{m}^{\\mathrm{true}}) = \\frac{1}{2}V_A\n$$\nSubstituting this into the expression for the covariance with the mid-parent:\n$$\n\\mathrm{Cov}(Y_o, M^{\\mathrm{true}}) = \\frac{1}{2} \\left( \\frac{1}{2}V_A + \\frac{1}{2}V_A \\right) = \\frac{1}{2}V_A\n$$\n\nDenominator: $\\mathrm{Var}(M^{\\mathrm{true}})$\nUsing the definition of $M^{\\mathrm{true}}$ and properties of variance:\n$$\n\\mathrm{Var}(M^{\\mathrm{true}}) = \\mathrm{Var}\\left(\\frac{Y_{f}^{\\mathrm{true}} + Y_{m}^{\\mathrm{true}}}{2}\\right) = \\frac{1}{4} \\mathrm{Var}(Y_{f}^{\\mathrm{true}} + Y_{m}^{\\mathrm{true}}) = \\frac{1}{4} \\left[ \\mathrm{Var}(Y_{f}^{\\mathrm{true}}) + \\mathrm{Var}(Y_{m}^{\\mathrm{true}}) + 2\\mathrm{Cov}(Y_{f}^{\\mathrm{true}}, Y_{m}^{\\mathrm{true}}) \\right]\n$$\nThe parents are drawn from the same population, so $\\mathrm{Var}(Y_{f}^{\\mathrm{true}}) = \\mathrm{Var}(Y_{m}^{\\mathrm{true}}) = V_P$. The problem specifies random mating and no assortative mating, which implies that the phenotypes of the parents are uncorrelated: $\\mathrm{Cov}(Y_{f}^{\\mathrm{true}}, Y_{m}^{\\mathrm{true}}) = 0$. Therefore:\n$$\n\\mathrm{Var}(M^{\\mathrm{true}}) = \\frac{1}{4} (V_P + V_P + 0) = \\frac{2V_P}{4} = \\frac{1}{2}V_P\n$$\n\nCombining the numerator and denominator, the true regression slope is:\n$$\n\\beta^{\\mathrm{true}} = \\frac{\\frac{1}{2}V_A}{\\frac{1}{2}V_P} = \\frac{V_A}{V_P} = h^2\n$$\nThis establishes the classic quantitative genetics result that the regression slope of offspring phenotype on the true mid-parent phenotype is equal to the narrow-sense heritability.\n\n**2. Attenuation of the Slope by Measurement Error**\n\nThe observed regression is of $Y_o$ on the observed mid-parent phenotype, $M^{\\mathrm{obs}} = \\frac{Y_{f}^{\\mathrm{obs}} + Y_{m}^{\\mathrm{obs}}}{2}$. The observed slope, given as $b=0.45$, is:\n$$\nb = \\frac{\\mathrm{Cov}(Y_o, M^{\\mathrm{obs}})}{\\mathrm{Var}(M^{\\mathrm{obs}})}\n$$\nWe analyze the effect of measurement error on the numerator and denominator separately. The observed phenotypes are $Y_{f}^{\\mathrm{obs}} = Y_{f}^{\\mathrm{true}} + e_{f}$ and $Y_{m}^{\\mathrm{obs}} = Y_{m}^{\\mathrm{true}} + e_{m}$. The observed mid-parent is $M^{\\mathrm{obs}} = M^{\\mathrm{true}} + \\varepsilon_M$, where $\\varepsilon_M = \\frac{e_f + e_m}{2}$.\n\nNumerator: $\\mathrm{Cov}(Y_o, M^{\\mathrm{obs}})$\n$$\n\\mathrm{Cov}(Y_o, M^{\\mathrm{obs}}) = \\mathrm{Cov}(Y_o, M^{\\mathrm{true}} + \\varepsilon_M) = \\mathrm{Cov}(Y_o, M^{\\mathrm{true}}) + \\mathrm{Cov}(Y_o, \\varepsilon_M)\n$$\nThe measurement errors $e_f$ and $e_m$ are independent of all true phenotypes, including $Y_o$. Therefore, their average, $\\varepsilon_M$, is also independent of $Y_o$, which means $\\mathrm{Cov}(Y_o, \\varepsilon_M) = 0$. The numerator is unaffected by the measurement error:\n$$\n\\mathrm{Cov}(Y_o, M^{\\mathrm{obs}}) = \\mathrm{Cov}(Y_o, M^{\\mathrm{true}}) = \\frac{1}{2}V_A\n$$\n\nDenominator: $\\mathrm{Var}(M^{\\mathrm{obs}})$\n$$\n\\mathrm{Var}(M^{\\mathrm{obs}}) = \\mathrm{Var}(M^{\\mathrm{true}} + \\varepsilon_M) = \\mathrm{Var}(M^{\\mathrm{true}}) + \\mathrm{Var}(\\varepsilon_M) + 2\\mathrm{Cov}(M^{\\mathrm{true}}, \\varepsilon_M)\n$$\nSince the errors are independent of true phenotypes, $\\mathrm{Cov}(M^{\\mathrm{true}}, \\varepsilon_M) = 0$. The variance of the error term $\\varepsilon_M$ is:\n$$\n\\mathrm{Var}(\\varepsilon_M) = \\mathrm{Var}\\left(\\frac{e_f + e_m}{2}\\right) = \\frac{1}{4}\\mathrm{Var}(e_f + e_m)\n$$\nAs $e_f$ and $e_m$ are independent with variance $\\sigma_e^2$, $\\mathrm{Var}(e_f + e_m) = \\mathrm{Var}(e_f) + \\mathrm{Var}(e_m) = \\sigma_e^2 + \\sigma_e^2 = 2\\sigma_e^2$.\n$$\n\\mathrm{Var}(\\varepsilon_M) = \\frac{1}{4}(2\\sigma_e^2) = \\frac{1}{2}\\sigma_e^2\n$$\nThe total variance of the observed mid-parent is:\n$$\n\\mathrm{Var}(M^{\\mathrm{obs}}) = \\mathrm{Var}(M^{\\mathrm{true}}) + \\mathrm{Var}(\\varepsilon_M) = \\frac{1}{2}V_P + \\frac{1}{2}\\sigma_e^2 = \\frac{1}{2}(V_P + \\sigma_e^2)\n$$\nThe observed regression slope $b$ is therefore:\n$$\nb = \\frac{\\frac{1}{2}V_A}{\\frac{1}{2}(V_P + \\sigma_e^2)} = \\frac{V_A}{V_P + \\sigma_e^2}\n$$\nThis can be rewritten in terms of $h^2$:\n$$\nb = \\frac{V_A/V_P}{(V_P + \\sigma_e^2)/V_P} = \\frac{h^2}{1 + \\sigma_e^2/V_P}\n$$\nThis expression shows the attenuation of the true slope $h^2$ by a factor related to the measurement error variance.\n\n**3. Computation of the Corrected Heritability**\n\nTo solve for $h^2$, we must find the value of the attenuation factor. The problem provides the per-parent test-retest reliability, $r_p = 0.80$, defined as:\n$$\nr_p = \\frac{\\mathrm{Var}(Y^{\\mathrm{true}})}{\\mathrm{Var}(Y^{\\mathrm{obs}})}\n$$\nFor a single parent, $\\mathrm{Var}(Y^{\\mathrm{true}}) = V_P$. The observed variance is $\\mathrm{Var}(Y^{\\mathrm{obs}}) = \\mathrm{Var}(Y^{\\mathrm{true}} + e) = \\mathrm{Var}(Y^{\\mathrm{true}}) + \\mathrm{Var}(e) = V_P + \\sigma_e^2$. Thus,\n$$\nr_p = \\frac{V_P}{V_P + \\sigma_e^2}\n$$\nNow, let us examine the attenuation factor in our expression for the observed slope $b$:\n$$\nb = \\frac{V_A}{V_P + \\sigma_e^2} = \\left(\\frac{V_A}{V_P}\\right) \\left(\\frac{V_P}{V_P + \\sigma_e^2}\\right) = h^2 \\cdot r_p\n$$\nThe relationship is remarkably simple: the observed slope is the product of the narrow-sense heritability and the reliability of a single parent's measurement. This arises because the process of creating the mid-parent value scales both the signal variance ($\\mathrm{Var}(M^{\\mathrm{true}}) = V_P/2$) and the noise variance ($\\mathrm{Var}(\\varepsilon_M) = \\sigma_e^2/2$) by the same factor of $1/2$, leaving their ratio, and thus the reliability, unchanged. The reliability of the mid-parent predictor is $r_M = \\frac{\\mathrm{Var}(M^{\\mathrm{true}})}{\\mathrm{Var}(M^{\\mathrm{obs}})} = \\frac{V_P/2}{(V_P + \\sigma_e^2)/2} = \\frac{V_P}{V_P + \\sigma_e^2} = r_p$.\n\nWe are given $b = 0.45$ and $r_p = 0.80$. We can now solve for $h^2$:\n$$\nh^2 = \\frac{b}{r_p} = \\frac{0.45}{0.80}\n$$\n$$\nh^2 = \\frac{45/100}{80/100} = \\frac{45}{80} = \\frac{9}{16}\n$$\nConverting this fraction to a decimal gives the final numerical answer.\n$$\nh^2 = 0.5625\n$$",
            "answer": "$$\\boxed{0.5625}$$"
        },
        {
            "introduction": "While heritability provides a picture of the total genetic influence on a trait, polygenic inheritance is built from the small effects of many individual loci. This exercise drills down to this fundamental level, asking you to quantify the proportion of trait variance explained by a single SNP . You will derive the direct link between a variant's allele frequency ($p$) and its per-allele effect size ($\\beta$) and its contribution to population-level variation. The problem also pivots to the analysis of binary disease outcomes, introducing the indispensable liability-threshold model and explaining how case-control study designs necessitate careful interpretation of effect sizes.",
            "id": "4328538",
            "problem": "A study in precision medicine evaluates the variance explained by a single nucleotide polymorphism (SNP) in a quantitative endophenotype relevant to a complex disease. Consider a large, randomly mating population in Hardy–Weinberg equilibrium. Let the SNP minor allele frequency be denoted by $p$, and let the genotype for an individual be coded as $X \\in \\{0,1,2\\}$ representing the minor allele count. Assume an additive genetic model for the observed quantitative phenotype,\n$$\nY \\;=\\; \\mu \\;+\\; \\beta X \\;+\\; \\varepsilon,\n$$\nwhere $\\mu$ is a constant, $\\beta$ is the per-allele effect on the observed scale, and $\\varepsilon$ is a residual term with $E[\\varepsilon]=0$ and independent of $X$. The total phenotypic variance in the population is $V_{P} = \\operatorname{Var}(Y)$, which includes both genetic and non-genetic sources of variation.\n\nStarting only from the definitions above, derive an expression for the proportion of variance in $Y$ explained by the SNP, $R^{2}$, in terms of $p$, $\\beta$, and $V_{P}$. Then evaluate it under the following scientifically realistic parameter values:\n- $p = 0.30$,\n- $\\beta = 0.10$,\n- $V_{P} = 1.00$.\n\nRound your numerical answer to four significant figures and express it as a decimal (no percentage sign).\n\nIn addition, the same SNP is later analyzed in a genome-wide association study (GWAS) using a case–control design for a binary disease outcome. Let disease liability $L$ follow a standard normal distribution, and suppose disease occurs if and only if $L$ exceeds a threshold $t$ chosen such that the population prevalence is $K$, so $t = \\Phi^{-1}(1-K)$, where $\\Phi$ is the standard normal cumulative distribution function and $\\phi$ is its probability density function. The GWAS uses an ascertained sample with case fraction $s$ that differs from $K$ (for concreteness, suppose $K = 0.05$ and $s = 0.50$). Explain, using the liability-threshold framework and first principles of variance and regression, why the per-allele effect estimated on the observed binary scale in an ascertained case–control sample is not directly comparable to the population effect on the liability scale, and show the form of the scaling factor that relates observed-scale variance explained to liability-scale variance explained. Do not provide numerical values for the scaling factor; instead, write it in symbolic form using $K$, $s$, and $t$.\n\nYour final submitted answer must be the single numerical value of $R^{2}$ for the quantitative phenotype under the given $p$, $\\beta$, and $V_{P}$, rounded to four significant figures.",
            "solution": "The problem asks for two distinct analyses. First, to derive and calculate the proportion of phenotypic variance ($R^2$) explained by a single nucleotide polymorphism (SNP) for a quantitative trait under an additive model. Second, to explain the relationship between effect sizes on the observed and liability scales in a case-control study of a binary trait.\n\n**Part 1: Variance Explained in a Quantitative Trait**\n\nThe proportion of variance in the phenotype $Y$ explained by the SNP, denoted as $R^2$, is the ratio of the genetic variance due to the SNP, $V_G$, to the total phenotypic variance, $V_P$.\n$$\nR^2 = \\frac{V_G}{V_P}\n$$\nThe phenotype is given by the additive model $Y = \\mu + \\beta X + \\varepsilon$, where $X$ is the genotype coded as the number of minor alleles and $\\varepsilon$ is an independent residual term. The total phenotypic variance is therefore:\n$$\nV_P = \\operatorname{Var}(Y) = \\operatorname{Var}(\\mu + \\beta X + \\varepsilon)\n$$\nSince $\\mu$ is a constant and $X$ and $\\varepsilon$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\beta X) + \\operatorname{Var}(\\varepsilon) = \\beta^2 \\operatorname{Var}(X) + \\operatorname{Var}(\\varepsilon)\n$$\nThe genetic variance component, $V_G$, is the part of the variance attributable to the genotype $X$, which is $\\operatorname{Var}(\\beta X)$.\n$$\nV_G = \\operatorname{Var}(\\beta X) = \\beta^2 \\operatorname{Var}(X)\n$$\nThus, the expression for $R^2$ is:\n$$\nR^2 = \\frac{\\beta^2 \\operatorname{Var}(X)}{V_P}\n$$\nTo complete this expression, we must find the variance of the genotype variable, $\\operatorname{Var}(X)$. The population is in Hardy–Weinberg equilibrium (HWE) with a minor allele frequency of $p$. The frequencies of the genotypes $X=0$, $X=1$, and $X=2$ are, respectively:\n\\begin{itemize}\n    \\item $P(X=0) = (1-p)^2$\n    \\item $P(X=1) = 2p(1-p)$\n    \\item $P(X=2) = p^2$\n\\end{itemize}\nThe variance of $X$ is given by $\\operatorname{Var}(X) = E[X^2] - (E[X])^2$. First, we compute the expected value of $X$:\n$$\nE[X] = \\sum_{i=0}^{2} i \\cdot P(X=i) = (0)(1-p)^2 + (1)(2p(1-p)) + (2)(p^2)\n$$\n$$\nE[X] = 2p - 2p^2 + 2p^2 = 2p\n$$\nNext, we compute the expected value of $X^2$:\n$$\nE[X^2] = \\sum_{i=0}^{2} i^2 \\cdot P(X=i) = (0)^2(1-p)^2 + (1)^2(2p(1-p)) + (2)^2(p^2)\n$$\n$$\nE[X^2] = 2p(1-p) + 4p^2 = 2p - 2p^2 + 4p^2 = 2p + 2p^2\n$$\nNow, we can find the variance of $X$:\n$$\n\\operatorname{Var}(X) = E[X^2] - (E[X])^2 = (2p + 2p^2) - (2p)^2 = 2p + 2p^2 - 4p^2 = 2p - 2p^2\n$$\n$$\n\\operatorname{Var}(X) = 2p(1-p)\n$$\nThis is a standard result for the variance of the allele count at a biallelic locus in HWE. Substituting this into the expression for $R^2$, we obtain the general formula:\n$$\nR^2 = \\frac{2p(1-p)\\beta^2}{V_P}\n$$\nNow, we evaluate this expression using the provided parameter values: $p = 0.30$, $\\beta = 0.10$, and $V_P = 1.00$.\n$$\nR^2 = \\frac{2(0.30)(1-0.30)(0.10)^2}{1.00}\n$$\n$$\nR^2 = \\frac{2(0.30)(0.70)(0.01)}{1.00}\n$$\n$$\nR^2 = (0.60)(0.70)(0.01) = 0.42 \\times 0.01 = 0.0042\n$$\nRounding to four significant figures, the result is $0.004200$.\n\n**Part 2: Case-Control Study and Liability Scale**\n\nIn the context of a genome-wide association study (GWAS) for a binary disease outcome, the effect of a SNP is typically estimated on the observed scale of case-control status (coded as $1$ for cases, $0$ for controls). This estimated effect is not directly comparable to the SNP's true effect on the underlying, unobserved liability scale for two principal reasons: the change of scale and ascertainment bias.\n\n1.  **Liability-Threshold Model and Change of Scale**: The model posits a continuous liability $L$, which is standard normally distributed in the population ($L \\sim N(0,1)$). A person is a case if their liability exceeds a threshold $t$. The SNP's true effect, which we might call $\\beta_{liability}$, is a linear effect on $L$. However, a GWAS using logistic regression estimates an odds ratio (or a log-odds ratio, $\\beta_{observed}$), which describes the effect on the binary outcome. The transformation from the continuous liability scale to the binary disease scale is non-linear, defined by the thresholding operation $I(L>t)$. The relationship between an effect on the liability scale and the effect on the outcome probability (and thus the odds ratio) depends on the value of the standard normal probability density function, $\\phi(\\cdot)$, at the threshold $t$.\n\n2.  **Ascertainment Bias**: A case-control study is not a random sample of the population. By design, cases (who have high liability, $L>t$) and controls (who have lower liability, $L \\le t$) are oversampled relative to their population prevalences. The proportion of cases in the sample, $s$, is typically much larger than the population disease prevalence, $K$ (e.g., $s=0.50$ vs. $K=0.05$). This ascertainment process enriches for risk alleles in the case group and protective alleles in the control group, altering the allele frequencies and the distribution of liability within the sample compared to the general population. Consequently, an effect size estimated within this ascertained sample does not reflect the effect size in the general population without correction.\n\nDue to these factors, the variance explained on the observed 0/1 scale in a case-control sample ($R^2_{obs}$) is not equal to the variance explained on the liability scale in the population ($R^2_{liab}$). However, a scaling factor can be derived from the principles of truncation of the normal distribution to convert between these two quantities. The relationship is given by:\n$$\nR^2_{liab} \\approx R^2_{obs} \\times C\n$$\nwhere $C$ is the scaling factor. The established form of this factor, which transforms the proportion of variance on the observed scale to that on the liability scale, is:\n$$\nC = \\frac{K^2(1-K)^2}{s(1-s)\\phi(t)^2}\n$$\nHere, $K$ is the population prevalence, $s$ is the case fraction in the sample, and $\\phi(t)$ is the height of the standard normal probability density function at the liability threshold $t$, where $t=\\Phi^{-1}(1-K)$. This factor correctly accounts for both the change of scale and the case-control ascertainment scheme.\n\nThe final answer required is the numerical result from Part 1.",
            "answer": "$$\\boxed{0.004200}$$"
        },
        {
            "introduction": "Modern genetics has largely moved from pedigree-based studies to population-based genome-wide association studies (GWAS). This practice brings you to the forefront of contemporary methods by introducing Linkage Disequilibrium (LD) Score regression—a powerful technique for estimating SNP-based heritability ($h^2_{\\text{SNP}}$) directly from GWAS summary statistics . You will derive its core equation to understand how the method cleverly leverages patterns of correlation between genetic variants (LD) to disentangle true, genome-wide polygenic signals from confounding factors. This exercise provides hands-on experience with a state-of-the-art tool that has revolutionized our understanding of the genetic architecture of complex traits.",
            "id": "4328524",
            "problem": "A research team conducts a large-scale Genome-Wide Association Study (GWAS) of a continuous trait with unit variance under the additive polygenic model. Genotypes are standardized so that each Single-Nucleotide Polymorphism (SNP) genotype has mean zero and variance one, and the phenotype is standardized to have mean zero and variance one. The team plans to estimate Single-Nucleotide Polymorphism heritability, denoted $h^{2}_{\\text{SNP}}$, using Linkage Disequilibrium (LD) Score regression (LDSC), where Linkage Disequilibrium (LD) Score $\\ell_{j}$ for SNP $j$ is defined as the sum of squared correlations with all other genotyped SNPs. Assume the following standard conditions for LD Score regression: additive genetic effects, small and approximately independent SNP effect sizes across the genome, effect sizes independent of local LD, negligible confounding such that the regression intercept equals the null expectation, and homogeneous sampling such that the sampling variance contributes its canonical null term.\n\nStarting from the additive model and the definition of $h^{2}_{\\text{SNP}}$ as the proportion of phenotypic variance explained by all genotyped SNPs, derive the expression linking the expected mean of the association $\\chi^{2}$ statistics across SNPs to $h^{2}_{\\text{SNP}}$, the sample size $N$, the number of SNPs $M$, and the average LD score $\\bar{\\ell}$. Then, using the provided summary statistics from the GWAS meta-analysis—mean $\\chi^{2}$ statistic $\\overline{\\chi^{2}} = 1.30$, sample size $N = 200{,}000$, average LD score $\\bar{\\ell} = 100$, and number of SNPs $M = 1{,}000{,}000$—compute $h^{2}_{\\text{SNP}}$ under the stated assumptions.\n\nExpress your final answer as a decimal fraction. No rounding is required beyond exact arithmetic.",
            "solution": "The task is twofold: first, to derive the theoretical relationship between the mean chi-squared ($\\chi^2$) statistic, SNP heritability ($h^{2}_{\\text{SNP}}$), sample size ($N$), number of SNPs ($M$), and the average LD score ($\\bar{\\ell}$); second, to compute $h^{2}_{\\text{SNP}}$ using the provided data.\n\n**Part 1: Derivation of the LD Score Regression Equation**\n\nWe begin with the standard additive linear model for a phenotype $y$ in a sample of $N$ individuals. The phenotype for individual $i$ is given by:\n$$\ny_i = \\sum_{j=1}^{M} X_{ij} \\beta_j + \\epsilon_i\n$$\nwhere $M$ is the number of genotyped SNPs, $X_{ij}$ is the standardized genotype of individual $i$ at SNP $j$, $\\beta_j$ is the true additive genetic effect of SNP $j$, and $\\epsilon_i$ is the non-genetic component (environmental effects and noise) for individual $i$.\n\nThe problem states that both the phenotype and genotypes are standardized. This implies:\n- Phenotype: $\\mathbb{E}[y_i] = 0$ and $\\text{Var}(y) = 1$.\n- Genotype: For each SNP $j$, $\\mathbb{E}[X_{ij}] = 0$ and $\\text{Var}(X_j) = 1$ over the $N$ individuals.\n\nThe SNP heritability, $h^{2}_{\\text{SNP}}$, is defined as the proportion of phenotypic variance explained by all genotyped SNPs. Since $\\text{Var}(y)=1$, we have:\n$$\nh^{2}_{\\text{SNP}} = \\text{Var}\\left(\\sum_{j=1}^{M} X_{ij} \\beta_j\\right)\n$$\nThe core assumption of the polygenic model used in LDSC is that the effect sizes $\\beta_j$ are random variables drawn independently from a distribution with mean $0$ and variance $\\mathbb{E}[\\beta_j^2] = \\frac{h^{2}_{\\text{SNP}}}{M}$. This ensures that, if all SNPs were in linkage equilibrium (uncorrelated), the total genetic variance would be $\\sum_{j=1}^{M} \\text{Var}(\\beta_j) \\text{Var}(X_j) = M \\times \\frac{h^{2}_{\\text{SNP}}}{M} \\times 1 = h^{2}_{\\text{SNP}}$.\n\nIn a GWAS, we estimate the marginal effect of each SNP $j$ using simple linear regression. For standardized variables, the estimated effect size, $\\hat{\\beta}_j$, is the sample correlation between the genotype $X_j$ and the phenotype $y$:\n$$\n\\hat{\\beta}_j = \\frac{1}{N} \\sum_{i=1}^{N} X_{ij} y_i\n$$\nSubstituting the true model for $y_i$:\n$$\n\\hat{\\beta}_j = \\frac{1}{N} \\sum_{i=1}^{N} X_{ij} \\left( \\sum_{k=1}^{M} X_{ik} \\beta_k + \\epsilon_i \\right) = \\sum_{k=1}^{M} \\left( \\frac{1}{N} \\sum_{i=1}^{N} X_{ij} X_{ik} \\right) \\beta_k + \\frac{1}{N} \\sum_{i=1}^{N} X_{ij} \\epsilon_i\n$$\nThe term $\\frac{1}{N} \\sum_{i=1}^{N} X_{ij} X_{ik}$ is the sample correlation between SNP $j$ and SNP $k$, denoted by $r_{jk}$. The second term represents sampling noise. So, we can write:\n$$\n\\hat{\\beta}_j = \\sum_{k=1}^{M} r_{jk} \\beta_k + e_j\n$$\nwhere $e_j = \\frac{1}{N} \\sum_{i=1}^{N} X_{ij} \\epsilon_i$.\n\nThe association test statistic for SNP $j$ is typically a z-score, $Z_j = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)}$. For a large sample size $N$ and standardized variables, the standard error of the effect estimate is approximately $\\text{SE}(\\hat{\\beta}_j) \\approx \\frac{1}{\\sqrt{N}}$.\nThe $\\chi^2$ statistic is the square of the z-score: $\\chi^2_j = Z_j^2 \\approx N \\hat{\\beta}_j^2$.\n$$\n\\chi^2_j \\approx N \\left( \\sum_{k=1}^{M} r_{jk} \\beta_k + e_j \\right)^2 = N \\left( \\sum_{k=1}^{M} r_{jk} \\beta_k \\right)^2 + 2N e_j \\left( \\sum_{k=1}^{M} r_{jk} \\beta_k \\right) + N e_j^2\n$$\nWe are interested in the expected value of this statistic, $\\mathbb{E}[\\chi^2_j]$, where the expectation is taken over the distribution of true effects $\\beta_k$ and the sampling of individuals (which determines $e_j$).\n$$\n\\mathbb{E}[\\chi^2_j] = \\mathbb{E}\\left[N \\left( \\sum_{k=1}^{M} r_{jk} \\beta_k \\right)^2\\right] + 2N \\mathbb{E}\\left[e_j \\left( \\sum_{k=1}^{M} r_{jk} \\beta_k \\right)\\right] + \\mathbb{E}[N e_j^2]\n$$\nThe genetic effects $\\beta_k$ are assumed to be independent of the non-genetic component $\\epsilon_i$, and thus independent of the sampling noise $e_j$. Therefore, the cross-term is zero: $\\mathbb{E}[e_j (\\dots)] = \\mathbb{E}[e_j]\\mathbb{E}[(\\dots)] = 0$.\n\nLet's evaluate the remaining two terms.\nThe first term is the contribution from genetics and LD:\n$$\n\\mathbb{E}\\left[N \\left( \\sum_{k=1}^{M} r_{jk} \\beta_k \\right)^2\\right] = N \\mathbb{E}\\left[ \\left(\\sum_k r_{jk} \\beta_k\\right) \\left(\\sum_m r_{jm} \\beta_m\\right) \\right] = N \\sum_{k,m} r_{jk} r_{jm} \\mathbb{E}[\\beta_k \\beta_m]\n$$\nSince effect sizes of different SNPs are independent, $\\mathbb{E}[\\beta_k \\beta_m] = 0$ for $k \\neq m$. Thus, we only consider terms where $k = m$. Using $\\mathbb{E}[\\beta_k^2] = \\frac{h^{2}_{\\text{SNP}}}{M}$:\n$$\nN \\sum_{k=1}^{M} r_{jk}^2 \\mathbb{E}[\\beta_k^2] = N \\sum_{k=1}^{M} r_{jk}^2 \\left(\\frac{h^{2}_{\\text{SNP}}}{M}\\right) = N \\frac{h^{2}_{\\text{SNP}}}{M} \\sum_{k=1}^{M} r_{jk}^2\n$$\nThe LD score for SNP $j$ is defined as $\\ell_j = \\sum_{k=1}^{M} r_{jk}^2$. So the term becomes $N \\frac{h^{2}_{\\text{SNP}}}{M} \\ell_j$.\n\nThe second term is the contribution from sampling variance, $\\mathbb{E}[N e_j^2]$. The problem specifies \"homogeneous sampling such that the sampling variance contributes its canonical null term.\" Under the null hypothesis of no genetic effects ($\\beta_j = 0$ for all $j$), the $\\chi^2$ statistic for a single test follows a $\\chi^2$ distribution with one degree of freedom, which has an expected value of $1$. This contribution to the expected $\\chi^2$ statistic from sampling noise is therefore $1$.\n\nCombining the terms, the expected $\\chi^2$ statistic for a single SNP $j$ is:\n$$\n\\mathbb{E}[\\chi^2_j] = N \\frac{h^{2}_{\\text{SNP}}}{M} \\ell_j + 1\n$$\nThis equation forms the basis of LD Score regression, where one regresses the observed $\\chi^2_j$ statistics against their pre-computed LD scores $\\ell_j$. The problem asks for the relationship involving the *mean* $\\chi^2$ statistic, $\\overline{\\chi^2}$. We find this by averaging the above expression over all $M$ SNPs:\n$$\n\\mathbb{E}[\\overline{\\chi^2}] = \\mathbb{E}\\left[\\frac{1}{M}\\sum_{j=1}^{M} \\chi^2_j\\right] = \\frac{1}{M}\\sum_{j=1}^{M} \\mathbb{E}[\\chi^2_j] = \\frac{1}{M}\\sum_{j=1}^{M} \\left(N \\frac{h^{2}_{\\text{SNP}}}{M} \\ell_j + 1\\right)\n$$\n$$\n\\mathbb{E}[\\overline{\\chi^2}] = \\frac{N h^{2}_{\\text{SNP}}}{M^2} \\sum_{j=1}^{M} \\ell_j + \\frac{1}{M}\\sum_{j=1}^{M} 1 = \\frac{N h^{2}_{\\text{SNP}}}{M} \\left(\\frac{1}{M}\\sum_{j=1}^{M} \\ell_j\\right) + 1\n$$\nRecognizing that $\\bar{\\ell} = \\frac{1}{M}\\sum_{j=1}^{M} \\ell_j$ is the average LD score, we arrive at the final derived expression:\n$$\n\\mathbb{E}[\\overline{\\chi^2}] = \\frac{N h^{2}_{\\text{SNP}}}{M} \\bar{\\ell} + 1\n$$\n\n**Part 2: Calculation of $h^{2}_{\\text{SNP}}$**\n\nWe use the observed mean $\\chi^2$ statistic, $\\overline{\\chi^2} = 1.30$, as our estimate for its expectation, $\\mathbb{E}[\\overline{\\chi^2}]$. We can now solve for $h^{2}_{\\text{SNP}}$:\n$$\n\\overline{\\chi^2} = \\frac{N \\bar{\\ell}}{M} h^{2}_{\\text{SNP}} + 1\n$$\n$$\n\\overline{\\chi^2} - 1 = \\frac{N \\bar{\\ell}}{M} h^{2}_{\\text{SNP}}\n$$\n$$\nh^{2}_{\\text{SNP}} = \\frac{M (\\overline{\\chi^2} - 1)}{N \\bar{\\ell}}\n$$\nThe provided summary statistics are:\n- $\\overline{\\chi^2} = 1.30$\n- $N = 200{,}000 = 2 \\times 10^5$\n- $\\bar{\\ell} = 100 = 10^2$\n- $M = 1{,}000{,}000 = 10^6$\n\nSubstituting these values into the expression for $h^{2}_{\\text{SNP}}$:\n$$\nh^{2}_{\\text{SNP}} = \\frac{1{,}000{,}000 \\times (1.30 - 1)}{200{,}000 \\times 100}\n$$\n$$\nh^{2}_{\\text{SNP}} = \\frac{10^6 \\times 0.30}{(2 \\times 10^5) \\times 10^2}\n$$\n$$\nh^{2}_{\\text{SNP}} = \\frac{0.30 \\times 10^6}{2 \\times 10^7}\n$$\n$$\nh^{2}_{\\text{SNP}} = \\frac{0.30}{2 \\times 10^{7-6}} = \\frac{0.30}{2 \\times 10^1} = \\frac{0.30}{20}\n$$\n$$\nh^{2}_{\\text{SNP}} = 0.015\n$$\nThe estimated SNP heritability is $0.015$, or $1.5\\%$.",
            "answer": "$$\n\\boxed{0.015}\n$$"
        }
    ]
}