{
    "hands_on_practices": [
        {
            "introduction": "Before embarking on a costly Genome-Wide Association Study (GWAS), it is essential to perform power calculations to ensure the study is adequately sized to detect genetic associations of interest. This exercise  guides you through the derivation of a sample size formula from first principles, using the asymptotic properties of the logistic regression Wald test. Mastering this theoretical foundation is crucial for designing effective genetic studies and interpreting their results within the context of statistical power.",
            "id": "4370897",
            "problem": "A population-scale biobank plans a genome-wide association study (Genome-Wide Association Study (GWAS)) for a binary disease outcome analyzed with logistic regression under an additive genetic model. Genotypes at a single biallelic locus are coded as $g \\in \\{0,1,2\\}$ minor alleles. Assume Hardy–Weinberg equilibrium (HWE) at minor allele frequency (minor allele frequency (MAF)) $p$, independence of individuals, and that a nested case-control sample will be assembled with an equal number of cases and controls, so the case fraction in the analysis dataset is $c = 1/2$. The true per-allele odds ratio is $\\mathrm{OR}$, so the log-odds regression coefficient is $\\beta = \\ln(\\mathrm{OR})$. The biobank will adopt a two-sided genome-wide significance threshold $\\alpha = 5 \\times 10^{-8}$ and target power $0.8$ for the Wald test of $H_{0} : \\beta = 0$ versus $H_{1} : \\beta \\neq 0$.\n\nStarting from first principles for logistic regression, the asymptotic normality of the maximum likelihood estimator, and the definition of Fisher information, derive from these bases an approximation to the non-centrality parameter for the Wald test and use it to obtain a closed-form expression for the total sample size $N$ (cases plus controls) required to achieve the specified significance level and power. You may use the fact that under HWE the variance of $g$ under the additive coding is a function of $p$ and that the Wald statistic is asymptotically standard normal under the null and normal with non-zero mean under the alternative. Express your final answer as a single analytic expression in terms of $p$ and $\\mathrm{OR}$, using the standard normal cumulative distribution function $\\Phi(\\cdot)$ and its quantile function $\\Phi^{-1}(\\cdot)$. No numerical rounding is required.",
            "solution": "We model the binary outcome $Y \\in \\{0,1\\}$ with logistic regression under an additive genetic effect:\n$$\n\\operatorname{logit}\\big(\\Pr(Y=1 \\mid g)\\big) \\;=\\; \\alpha_{0} + \\beta g,\n$$\nwhere $\\beta = \\ln(\\mathrm{OR})$ is the per-allele log-odds effect. The Wald test for $H_{0}:\\beta=0$ uses the statistic\n$$\nW \\;=\\; \\frac{\\hat{\\beta}}{\\operatorname{SE}(\\hat{\\beta})},\n$$\nwhere $\\hat{\\beta}$ is the maximum likelihood estimator (MLE) and $\\operatorname{SE}(\\hat{\\beta})$ is its standard error. By well-tested asymptotic theory of MLEs, $\\hat{\\beta}$ is approximately normal with mean $\\beta$ and variance equal to the inverse of the Fisher information for $\\beta$,\n$$\n\\hat{\\beta} \\;\\dot{\\sim}\\; \\mathcal{N}\\!\\left(\\beta, \\left[I(\\beta)\\right]^{-1}\\right),\n$$\nso that\n$$\nW \\;\\dot{\\sim}\\; \\mathcal{N}\\!\\left(\\delta,\\,1\\right), \\quad \\text{with} \\quad \\delta \\;=\\; \\frac{\\beta}{\\operatorname{SE}(\\hat{\\beta})} \\;=\\; \\beta \\sqrt{I(\\beta)}.\n$$\n\nWe now approximate $I(\\beta)$ for logistic regression with a single predictor $g$ and an intercept. The Fisher information matrix for $(\\alpha_{0}, \\beta)$ in logistic regression can be written as $X^{\\top} W X$, where $X$ is the $N \\times 2$ design matrix with columns $(1, g)$ and $W$ is the diagonal matrix with entries $w_{i} = \\pi_{i}(1-\\pi_{i})$, where $\\pi_{i} = \\Pr(Y_{i}=1 \\mid g_{i}) = \\operatorname{logit}^{-1}(\\alpha_{0} + \\beta g_{i})$. The information for $\\beta$ is the $(2,2)$ element of $X^{\\top} W X$ after accounting for the intercept, which can be expressed (using standard results for regression with an intercept) as\n$$\nI(\\beta) \\;\\approx\\; \\sum_{i=1}^{N} w_{i} \\left(g_{i} - \\bar{g}\\right)^{2},\n$$\nwhere $\\bar{g}$ is the sample mean of $g$. Under a nested case-control design with case fraction $c$ and small to moderate $\\beta$, we approximate $w_{i} \\approx c(1-c)$ uniformly, reflecting the Bernoulli variance of the outcome at the sampled case fraction. Taking expectation over the genotype distribution then gives\n$$\nI(\\beta) \\;\\approx\\; N \\, c(1-c) \\, \\operatorname{Var}(g).\n$$\n\nUnder Hardy–Weinberg equilibrium at minor allele frequency $p$, the genotype $g$ under additive coding has probabilities $\\Pr(g=0)=(1-p)^{2}$, $\\Pr(g=1)=2p(1-p)$, and $\\Pr(g=2)=p^{2}$. One can compute\n$$\n\\mathbb{E}[g] \\;=\\; 2p, \\quad \\mathbb{E}[g^{2}] \\;=\\; 2p(1-p) + 4p^{2},\n$$\nso\n$$\n\\operatorname{Var}(g) \\;=\\; \\mathbb{E}[g^{2}] - \\big(\\mathbb{E}[g]\\big)^{2} \\;=\\; \\big(2p(1-p) + 4p^{2}\\big) - 4p^{2} \\;=\\; 2p(1-p).\n$$\n\nCombining these, the non-centrality parameter for the Wald statistic is\n$$\n\\delta \\;=\\; \\beta \\sqrt{I(\\beta)} \\;\\approx\\; \\beta \\sqrt{N \\, c(1-c) \\, 2p(1-p)}.\n$$\nWith $c = 1/2$ for the planned equal case-control sampling,\n$$\nc(1-c) \\;=\\; \\frac{1}{4}, \\quad \\text{so} \\quad \\delta \\;\\approx\\; \\beta \\sqrt{N \\cdot \\frac{1}{4} \\cdot 2p(1-p)} \\;=\\; \\beta \\sqrt{N \\cdot \\frac{1}{2} p(1-p)}.\n$$\n\nFor a two-sided Wald test at significance level $\\alpha$, the rejection threshold is $|W|  z_{1-\\alpha/2}$, where $z_{q} = \\Phi^{-1}(q)$ and $\\Phi$ is the standard normal cumulative distribution function. Under $H_{1}$, $W \\dot{\\sim} \\mathcal{N}(\\delta,1)$. The power $1 - \\beta_{\\text{pow}}$ (here $0.8$) satisfies the standard normal approximation\n$$\n1 - \\beta_{\\text{pow}} \\;\\approx\\; \\Pr\\!\\left(|Z|  z_{1-\\alpha/2} \\,\\big|\\, Z \\sim \\mathcal{N}(\\delta,1)\\right),\n$$\nwhich, by symmetry and monotonicity of $\\Phi$, is achieved approximately when\n$$\n\\delta \\;\\approx\\; z_{1-\\alpha/2} + z_{1-\\beta_{\\text{pow}}}.\n$$\nEquating the expressions for $\\delta$ and solving for $N$ gives\n$$\n\\beta \\sqrt{N \\cdot \\frac{1}{2} p(1-p)} \\;=\\; z_{1-\\alpha/2} + z_{1-\\beta_{\\text{pow}}}\n\\quad \\Longrightarrow \\quad\nN \\;=\\; \\frac{\\big(z_{1-\\alpha/2} + z_{1-\\beta_{\\text{pow}}}\\big)^{2}}{\\frac{1}{2} p(1-p) \\beta^{2}}.\n$$\nSubstituting $\\beta = \\ln(\\mathrm{OR})$, $\\alpha = 5 \\times 10^{-8}$ (so $z_{1-\\alpha/2} = \\Phi^{-1}(1 - 2.5 \\times 10^{-8})$), and $1 - \\beta_{\\text{pow}} = 0.8$ (so $z_{1-\\beta_{\\text{pow}}} = \\Phi^{-1}(0.8)$), we obtain the required total sample size:\n$$\nN \\;=\\; \\frac{\\big(\\,\\Phi^{-1}(1 - 2.5 \\times 10^{-8}) + \\Phi^{-1}(0.8)\\,\\big)^{2}}{\\frac{1}{2} \\, p(1-p) \\, \\big(\\ln(\\mathrm{OR})\\big)^{2}}\n\\;=\\;\n\\frac{2 \\, \\big(\\,\\Phi^{-1}(1 - 2.5 \\times 10^{-8}) + \\Phi^{-1}(0.8)\\,\\big)^{2}}{p(1-p) \\, \\big(\\ln(\\mathrm{OR})\\big)^{2}}.\n$$\nThis is a closed-form analytic expression in terms of $p$ and $\\mathrm{OR}$, with the specified $\\alpha$ and power, derived from the logistic regression Fisher information and the asymptotic distribution of the Wald test.",
            "answer": "$$\\boxed{\\frac{2\\left(\\Phi^{-1}\\!\\left(1-2.5\\times 10^{-8}\\right)+\\Phi^{-1}\\!\\left(0.8\\right)\\right)^{2}}{p\\left(1-p\\right)\\left(\\ln\\!\\left(\\mathrm{OR}\\right)\\right)^{2}}}$$"
        },
        {
            "introduction": "Maintaining data integrity is a critical challenge in high-throughput sequencing. A common technical artifact is sample contamination, which can introduce noise and lead to false conclusions if not properly identified and managed. In this practical exercise , you will implement a real-world quality control method to estimate sample contamination levels from sequencing data. By modeling allele counts with a binomial mixture model and deriving confidence intervals, you will learn to quantify uncertainty and make data-driven decisions about sample inclusion, a vital skill for robust biobank analysis.",
            "id": "4370879",
            "problem": "A national-scale population biobank sequences many participants and monitors sample contamination in high-throughput pipelines by analyzing allele balance at sites that are homozygous in the index sample. Consider a curated panel of biallelic Single Nucleotide Polymorphisms (SNPs) selected to have high expected heterozygosity in the population. For a given sample, across this panel, define the pooled minor-allele read count $X$ and the pooled total read count $D$ over all sites that are homozygous for the index sample (so any observed minor allele reads arise from sequencing error or contamination). Assume the following fundamental base:\n\n- Each read is an independent Bernoulli trial, reflecting standard random sampling of molecules in short-read sequencing, with a per-read minor-allele probability that depends on whether the read originates from the index sample or a contaminant.\n- Let the contamination fraction be $c \\in [0,1]$, meaning the probability that any given read derives from a contaminant genome is $c$, and from the index genome is $1 - c$.\n- Let the per-base sequencing error rate be $\\epsilon \\in (0, 0.5)$, assumed known from calibration on control data.\n- Let $h \\in [0,1]$ denote the heterozygosity probability at the chosen SNP panel for a randomly drawn contaminant genome in the biobank population, assumed known from population-scale allele frequency data. At a heterozygous contaminant genotype, the expected minor-allele probability per read is $0.5$; at a homozygous contaminant genotype at these sites, minor-allele reads arise only from error at rate $\\epsilon$.\n\nUnder these assumptions, the per-read minor-allele probability is a two-component mixture at the read level: with probability $1-c$ a read comes from the index genome and contributes a minor allele with probability $\\epsilon$, and with probability $c$ a read comes from the contaminant and contributes a minor allele according to the population heterozygosity structure of the panel. By independence of reads, the pooled minor-allele count $X$ over $D$ reads satisfies a Binomial law with a success parameter that is an affine function of $c$. Using only these base assumptions, derive from first principles:\n\n1) The pooled likelihood for $c$ given $X$ and $D$ under the Binomial model implied by the two-component read-level mixture, and the Maximum Likelihood Estimate (MLE) of $c$ expressed in terms of the observed pooled minor-allele fraction $X/D$, $\\epsilon$, and $h$. The estimate must be clipped to the interval $[0,1]$.\n\n2) A two-sided equal-tailed $100(1-\\alpha)\\%$ confidence interval for the pooled Binomial minor-allele probability parameter, using the exact Clopper–Pearson method, and its transformation into a confidence interval for $c$ by inverting the affine mapping. Clip the transformed bounds to $[0,1]$.\n\n3) An exclusion rule suitable for population-scale biobank quality control: exclude a sample if and only if the point estimate of contamination $c$ is greater than or equal to a specified threshold $\\tau$ (express all contamination values as decimal fractions, not percentages).\n\nImplement a program that, for a fixed panel and quality-control configuration, computes for each test sample:\n- the contamination point estimate $\\hat{c}$,\n- the lower and upper bounds of the two-sided Clopper–Pearson $100(1-\\alpha)\\%$ confidence interval for $c$,\n- the exclusion decision as a boolean under the rule above.\n\nUse the following fixed parameters, justified by a high-quality short-read pipeline and a high-heterozygosity SNP panel curated from the biobank:\n- sequencing error rate $\\epsilon = 0.002$,\n- panel heterozygosity probability $h = 0.5$,\n- confidence level $1-\\alpha = 0.95$ (that is, $\\alpha = 0.05$),\n- exclusion threshold $\\tau = 0.03$.\n\nTest suite. For each test case, you are given $(D, X)$ as pooled totals over all homozygous sites for the index sample:\n- Case A (typical low contamination): $D = 150000$, $X = 487$.\n- Case B (near the exclusion threshold): $D = 120000$, $X = 1136$.\n- Case C (high contamination): $D = 100000$, $X = 2192$.\n- Case D (no contamination with sampling fluctuation): $D = 180000$, $X = 355$.\n- Case E (borderline at lower coverage): $D = 5000$, $X = 45$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order A through E and is itself a list of four elements: $[\\hat{c}, c_{\\text{low}}, c_{\\text{high}}, \\text{exclude}]$. The three contamination values must be rounded to exactly $6$ decimal places, and the boolean must be a language-native boolean. For example, an output for two hypothetical cases would look like $[[0.012345,0.010000,0.015000,False],[0.045678,0.043000,0.048500,True]]$.",
            "solution": "The problem is deemed valid as it is scientifically grounded in standard models of sequencing data, is well-posed with all necessary information, and is expressed objectively. We proceed to the solution.\n\nThe solution is derived in three parts, following the problem statement. First, we derive the Maximum Likelihood Estimate (MLE) for the contamination fraction $c$. Second, we derive the confidence interval for $c$. Third, we specify the exclusion rule.\n\n### Part 1: Derivation of the Contamination Estimator ($\\hat{c}$)\n\nThe core of the model is to determine the probability, let's call it $p$, that a single read at a site homozygous in the index sample will show a minor allele. This probability $p$ is the success parameter of a Binomial distribution governing the pooled minor-allele count $X$ over a total of $D$ reads.\n\nThe probablity $p$ is a mixture, conditioned on the origin of the read (index sample or contaminant).\nLet $c$ be the contamination fraction. A read originates from the contaminant with probability $c$ and from the index sample with probability $1-c$.\nLet $\\epsilon$ be the per-base sequencing error rate.\nLet $h$ be the probability that a contaminant is heterozygous at a given panel SNP.\n\nThe probability of observing a minor allele in a single read, $p$, is given by the law of total probability:\n$$p(c) = P(\\text{minor allele} | \\text{read from index}) P(\\text{read from index}) + P(\\text{minor allele} | \\text{read from contaminant}) P(\\text{read from contaminant})$$\n\n1.  If the read is from the index sample (with probability $1-c$): The index sample is homozygous for the major allele at the selected sites. A minor allele can only be observed due to a sequencing error. Thus, $P(\\text{minor allele} | \\text{read from index}) = \\epsilon$.\n\n2.  If the read is from the contaminant (with probability $c$): The probability of observing a minor allele depends on the contaminant's genotype.\n    -   The contaminant is heterozygous (e.g., A/T at an A/A index site) with probability $h$. In this case, the probability of sampling the minor allele is $0.5$.\n    -   The contaminant is homozygous for the same major allele as the index (e.g., A/A at an A/A index site) with probability $1-h$. In this case, a minor allele is observed only due to sequencing error, with probability $\\epsilon$.\n    Therefore, the probability of a minor allele, given the read is from the contaminant, is:\n    $$P(\\text{minor allele} | \\text{read from contaminant}) = h \\cdot 0.5 + (1-h) \\cdot \\epsilon$$\n\nCombining these terms, the overall minor-allele probability $p$ is:\n$$p(c) = \\epsilon (1-c) + \\left( 0.5h + \\epsilon(1-h) \\right) c$$\nExpanding and simplifying the expression:\n$$p(c) = \\epsilon - \\epsilon c + 0.5hc + \\epsilon c - h\\epsilon c$$\n$$p(c) = \\epsilon + c(0.5h - h\\epsilon)$$\n$$p(c) = \\epsilon + ch(0.5 - \\epsilon)$$\nThis confirms that $p$ is an affine function of $c$. Let the constant slope be $A = h(0.5 - \\epsilon)$. The relationship is $p(c) = \\epsilon + Ac$.\n\nThe number of minor-allele reads $X$ in a total of $D$ reads is modeled as a binomial random variable:\n$$X \\sim \\text{Binomial}(D, p(c))$$\nThe likelihood function for $c$ given the data $(X, D)$ is:\n$$L(c; X, D) = \\binom{D}{X} [p(c)]^X [1 - p(c)]^{D-X}$$\nTo find the Maximum Likelihood Estimate (MLE) of $c$, we maximize the log-likelihood $\\ell(c) = \\log L(c)$. The MLE for a binomial proportion $p$ is famously $\\hat{p} = X/D$. We can show this by differentiating the log-likelihood with respect to $p$ (and by extension, $c$):\n$$\\frac{d\\ell}{dc} = \\frac{d\\ell}{dp} \\frac{dp}{dc} = \\left( \\frac{X}{p} - \\frac{D-X}{1-p} \\right) A$$\nSetting the derivative to zero (and assuming $A \\neq 0$), we find that the term in the parenthesis must be zero, which gives $\\hat{p} = X/D$.\n\nSubstituting this into our affine relationship for $p(c)$:\n$$\\hat{p} = \\epsilon + \\hat{c}h(0.5 - \\epsilon)$$\nSolving for the MLE $\\hat{c}$:\n$$\\hat{c} = \\frac{\\hat{p} - \\epsilon}{h(0.5 - \\epsilon)} = \\frac{X/D - \\epsilon}{h(0.5 - \\epsilon)}$$\nSince $c$ is a fraction, its estimate must lie in the interval $[0,1]$. We enforce this by clipping the estimate:\n$$\\hat{c}_{\\text{clipped}} = \\max\\left(0, \\min\\left(1, \\frac{X/D - \\epsilon}{h(0.5 - \\epsilon)}\\right)\\right)$$\n\n### Part 2: Derivation of the Confidence Interval for $c$\n\nWe first compute a $100(1-\\alpha)\\%$ confidence interval for the binomial proportion $p$, denoted $[p_{\\text{low}}, p_{\\text{high}}]$, using the exact Clopper-Pearson method. This method defines the interval bounds by inverting the binomial test.\n-   The lower bound $p_{\\text{low}}$ is the value of $p$ such that the probability of observing $X$ or more successes is $\\alpha/2$:\n    $$P(Y \\ge X | Y \\sim \\text{Binomial}(D, p_{\\text{low}})) = \\sum_{k=X}^{D} \\binom{D}{k} p_{\\text{low}}^k (1-p_{\\text{low}})^{D-k} = \\frac{\\alpha}{2}$$\n-   The upper bound $p_{\\text{high}}$ is the value of $p$ such that the probability of observing $X$ or fewer successes is $\\alpha/2$:\n    $$P(Y \\le X | Y \\sim \\text{Binomial}(D, p_{\\text{high}})) = \\sum_{k=0}^{X} \\binom{D}{k} p_{\\text{high}}^k (1-p_{\\text{high}})^{D-k} = \\frac{\\alpha}{2}$$\n\nThese equations can be solved using the quantiles of the Beta distribution.\n-   $p_{\\text{low}} = B(\\frac{\\alpha}{2}; X, D-X+1)$, where $B(q; a, b)$ is the $q$-th quantile of the Beta distribution with shape parameters $a$ and $b$. If $X=0$, $p_{\\text{low}}=0$.\n-   $p_{\\text{high}} = B(1-\\frac{\\alpha}{2}; X+1, D-X)$. If $X=D$, $p_{\\text{high}}=1$.\n\nNext, we transform this interval for $p$ into a confidence interval for $c$. The relationship $c(p) = \\frac{p - \\epsilon}{h(0.5 - \\epsilon)}$ is monotonically increasing in $p$, because the denominator $h(0.5 - \\epsilon)$ is positive for the given parameters ($h=0.5 > 0$ and $\\epsilon=0.002  0.5$).\nTherefore, the confidence interval for $c$, $[c_{\\text{low}}, c_{\\text{high}}]$, is obtained by applying the transformation to the bounds of the interval for $p$:\n$$c_{\\text{low}} = \\frac{p_{\\text{low}} - \\epsilon}{h(0.5 - \\epsilon)}$$\n$$c_{\\text{high}} = \\frac{p_{\\text{high}} - \\epsilon}{h(0.5 - \\epsilon)}$$\nAs with the point estimate, these bounds are clipped to the valid range $[0, 1]$:\n$$c_{\\text{low, clipped}} = \\max(0, \\min(1, c_{\\text{low}}))$$\n$$c_{\\text{high, clipped}} = \\max(0, \\min(1, c_{\\text{high}}))$$\n\n### Part 3: Exclusion Rule\n\nThe quality control rule is to exclude a sample if its point estimate of contamination, $\\hat{c}$, is greater than or equal to a specified threshold $\\tau$.\n$$\\text{exclude} = (\\hat{c} \\ge \\tau)$$\n\nUsing the fixed parameters:\n-   $\\epsilon = 0.002$\n-   $h = 0.5$\n-   $1-\\alpha = 0.95 \\implies \\alpha = 0.05 \\implies \\alpha/2 = 0.025$\n-   $\\tau = 0.03$\n\nThe denominator for the transformations is $h(0.5 - \\epsilon) = 0.5(0.5 - 0.002) = 0.249$.\n\nThe derived formulas are implemented to process the test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Solves the contamination estimation problem for a series of test cases.\n    \"\"\"\n\n    # Fixed parameters from the problem statement\n    epsilon = 0.002  # sequencing error rate\n    h = 0.5          # panel heterozygosity probability\n    alpha = 0.05     # for 95% confidence interval\n    tau = 0.03       # exclusion threshold\n\n    # Test cases: (D, X)\n    # D: pooled total read count\n    # X: pooled minor-allele read count\n    test_cases = [\n        (150000, 487),   # Case A\n        (120000, 1136),  # Case B\n        (100000, 2192),  # Case C\n        (180000, 355),   # Case D\n        (5000, 45)       # Case E\n    ]\n\n    # Pre-calculate the constant denominator for the transformation\n    # c = (p - epsilon) / denom\n    denom = h * (0.5 - epsilon)\n\n    results_as_strings = []\n    for D, X in test_cases:\n        # 1. Calculate the Maximum Likelihood Estimate (MLE) of c\n        p_hat = X / D\n        c_hat_raw = (p_hat - epsilon) / denom\n        c_hat = max(0.0, min(1.0, c_hat_raw))\n\n        # 2. Calculate the Clopper-Pearson confidence interval for c\n\n        # Calculate CI for the binomial proportion p\n        if X == 0:\n            p_low = 0.0\n        else:\n            p_low = beta.ppf(alpha / 2, X, D - X + 1)\n        \n        if X == D:\n            p_high = 1.0\n        else:\n            p_high = beta.ppf(1 - alpha / 2, X + 1, D - X)\n\n        # Transform the CI for p to a CI for c\n        c_low_raw = (p_low - epsilon) / denom\n        c_high_raw = (p_high - epsilon) / denom\n        \n        # Clip the CI bounds to the [0, 1] interval\n        c_low = max(0.0, min(1.0, c_low_raw))\n        c_high = max(0.0, min(1.0, c_high_raw))\n\n        # 3. Apply the exclusion rule\n        exclude = c_hat = tau\n\n        # Format the results for this case\n        result_string = (\n            f\"[{c_hat:.6f},\"\n            f\"{c_low:.6f},\"\n            f\"{c_high:.6f},\"\n            f\"{exclude}]\"\n        )\n        results_as_strings.append(result_string)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Population-scale biobanks often aggregate samples from diverse ancestral backgrounds, creating population structure that can confound genetic association studies. This hands-on coding task  will walk you through the entire workflow of using Principal Component Analysis (PCA) to dissect this structure. You will simulate genotype data, apply the standard Hardy-Weinberg equilibrium-based scaling, and use Singular Value Decomposition (SVD) to quantify the major axes of genetic variation, providing a foundational skill for nearly all analyses of large-scale genomic data.",
            "id": "4370918",
            "problem": "A population-scale biobank collects genotype dosages from many individuals across many genomic variants. Consider an admixed cohort comprising multiple continental ancestry groups. The cohort is represented by a genotype matrix with samples as rows and variants as columns. Each entry is the count of the alternative allele per variant per sample and takes values in $\\{0,1,2\\}$. Principal component analysis (PCA) on standardized genotype data is commonly used to capture population structure, which can confound downstream precision medicine analyses if not accounted for. Your task is to compute principal components via singular value decomposition and quantify the fraction of total variance explained by the top components in several scenarios.\n\nUse the following fundamental base:\n- Genotype dosage $g_{ij} \\in \\{0,1,2\\}$ is the count of the alternative allele for sample $i$ and variant $j$.\n- The allele frequency for variant $j$ across all samples is $p_j = \\frac{1}{2n} \\sum_{i=1}^{n} g_{ij}$, where $n$ is the number of samples.\n- Under Hardy–Weinberg equilibrium, the expected variance of the genotype dosage for variant $j$ is $2 p_j (1 - p_j)$, which is widely used to standardize genotype columns for PCA.\n- Let the standardized data matrix be $X \\in \\mathbb{R}^{n \\times m}$ with entries $x_{ij} = \\frac{g_{ij} - 2 p_j}{\\sqrt{2 p_j (1 - p_j)}}$ for all variants with $0  p_j  1$. Variants with $p_j \\in \\{0,1\\}$ have zero variance and must be excluded from PCA.\n- Principal component analysis on $X$ can be obtained via the singular value decomposition $X = U \\Sigma V^{\\top}$, where $\\Sigma$ has nonnegative singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$.\n\nFrom these bases, derive how to compute the fraction of variance explained by each principal component from the singular values, and implement it algorithmically. You must:\n1. Construct synthetic genotype matrices according to specified parameters for an admixed cohort with continental ancestries. For each ancestry group $g$, generate each genotype $g_{ij}$ by drawing from a binomial distribution with $2$ trials and success probability equal to the group-specific allele frequency $p_{j}^{(g)}$, using a fixed pseudorandom seed per test case to ensure deterministic outputs.\n2. Standardize the genotype matrix using the Hardy–Weinberg scaling described above, removing monomorphic variants with $p_j \\in \\{0,1\\}$.\n3. Compute the singular value decomposition of the standardized matrix and compute the fractions of total variance explained by the top $k$ principal components. Express each fraction as a decimal rounded to four places (no percentage sign).\n\nTest Suite and Parameters:\nImplement the following three test cases. Each case is defined by group sizes, variant count, ancestry-informative marker (AIM) blocks with group-specific allele frequencies, a neutral allele frequency shared across groups for the remaining variants, a pseudorandom seed, and $k$ (the number of top components to report).\n\n- Case 1 (general admixed two-ancestry cohort):\n    - Number of groups: $2$ with group sizes $[100, 100]$.\n    - Number of variants: $500$.\n    - AIM blocks: one block with count $80$ and group-specific allele frequencies $[0.1, 0.9]$ for the two groups, respectively.\n    - Neutral allele frequency for remaining variants: $0.3$ in both groups.\n    - Seed: $123$.\n    - Report top $k = 3$ components.\n\n- Case 2 (boundary case: single ancestry group, minimal structure):\n    - Number of groups: $1$ with group sizes $[120]$.\n    - Number of variants: $400$.\n    - AIM blocks: none (i.e., all variants have the same allele frequency in the single group).\n    - Neutral allele frequency for all variants: $0.3$.\n    - Seed: $456$.\n    - Report top $k = 3$ components.\n\n- Case 3 (three-ancestry cohort with two orthogonal AIM blocks):\n    - Number of groups: $3$ with group sizes $[80, 80, 40]$.\n    - Number of variants: $600$.\n    - AIM blocks: two blocks:\n        - Block 1: count $30$ with group-specific allele frequencies $[0.1, 0.9, 0.9]$.\n        - Block 2: count $30$ with group-specific allele frequencies $[0.9, 0.1, 0.9]$.\n    - Neutral allele frequency for remaining variants: $0.3$ in all groups.\n    - Seed: $789$.\n    - Report top $k = 3$ components.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to a test case and must be a bracketed, comma-separated list of the top $k$ variance-explained fractions, rounded to four decimal places, with no spaces. For example: \"[[0.3210,0.1205,0.0800],[0.0155,0.0132,0.0121],[0.2501,0.1700,0.1200]]\".",
            "solution": "The problem requires the implementation of a standard population genetics workflow to analyze simulated genotype data. This involves generating synthetic genotypes for admixed populations, standardizing the data, performing principal component analysis (PCA) via singular value decomposition (SVD), and quantifying the population structure by calculating the fraction of total variance explained by the leading principal components.\n\nFirst, we establish the theoretical foundation for calculating the fraction of variance explained from singular values. Let the standardized genotype matrix be $X \\in \\mathbb{R}^{n \\times m'}$, where $n$ is the number of samples and $m'$ is the number of polymorphic variants (i.e., variants with allele frequency $p_j$ such that $0  p_j  1$). The entries of this matrix are given by $x_{ij} = \\frac{g_{ij} - 2 p_j}{\\sqrt{2 p_j (1 - p_j)}}$. By construction, each column of $X$ is centered to have a mean of $0$ and standardized.\n\nThe total variance in the data is a measure of the total spread of the data points. In the context of PCA, it is defined as the trace of the sample covariance matrix, $\\frac{1}{n-1}X^{\\top}X$. The total sum of squares, which is proportional to the total variance, can be computed from the squared Frobenius norm of the matrix $X$:\n$$ \\|X\\|_F^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{m'} x_{ij}^2 $$\nThe singular value decomposition of $X$ is given by $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m' \\times m'}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times m'}$ is a rectangular diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r  0$ on its diagonal, where $r$ is the rank of $X$.\n\nA key property of SVD is that the squared Frobenius norm of a matrix is equal to the sum of its squared singular values:\n$$ \\|X\\|_F^2 = \\text{Tr}(X^{\\top}X) = \\text{Tr}((U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top})) = \\text{Tr}(V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top}) = \\text{Tr}(V\\Sigma^{\\top}\\Sigma V^{\\top}) $$\nUsing the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$, we have:\n$$ \\text{Tr}(V(\\Sigma^{\\top}\\Sigma) V^{\\top}) = \\text{Tr}((\\Sigma^{\\top}\\Sigma)V^{\\top}V) = \\text{Tr}(\\Sigma^{\\top}\\Sigma) = \\sum_{k=1}^{r} \\sigma_k^2 $$\nThe variance captured by the $k$-th principal component is proportional to the square of the $k$-th singular value, $\\sigma_k^2$. Therefore, the fraction of the total variance explained by the $k$-th principal component is the ratio of $\\sigma_k^2$ to the total sum of squared singular values.\n$$ \\text{Fraction of Variance (PC}_k\\text{)} = \\frac{\\sigma_k^2}{\\sum_{l=1}^{r} \\sigma_l^2} $$\nThis formula provides a direct method to compute the desired quantities from the singular values of the standardized genotype matrix.\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n1.  **Data Simulation**: A genotype matrix $G \\in \\mathbb{R}^{n \\times m}$ is constructed, where $n$ is the total number of samples across all groups and $m$ is the total number of variants.\n    - An allele frequency matrix of size $m \\times (\\text{number of groups})$ is created, populated with frequencies from the specified Ancestry-Informative Marker (AIM) blocks and the neutral background frequency.\n    - A mapping from each sample index $i \\in \\{1, \\dots, n\\}$ to its corresponding ancestry group $g$ is established.\n    - For each sample $i$ in group $g$ and each variant $j$, a genotype dosage $g_{ij}$ is drawn from a binomial distribution, $g_{ij} \\sim \\text{Binomial}(2, p_{j}^{(g)})$, where $p_{j}^{(g)}$ is the allele frequency for variant $j$ in group $g$. A fixed pseudorandom seed ensures reproducibility for each case.\n2.  **Data Standardization**:\n    - The overall allele frequency $p_j$ for each variant $j$ is calculated across all $n$ samples using the formula $p_j = \\frac{1}{2n} \\sum_{i=1}^{n} g_{ij}$.\n    - Variants that are monomorphic (i.e., $p_j = 0$ or $p_j = 1$) are identified and excluded from the matrix $G$, resulting in a new matrix $G'$ of size $n \\times m'$, where $m' \\le m$.\n    - The matrix $G'$ is standardized column-wise to produce the matrix $X$ with entries $x_{ij} = \\frac{g'_{ij} - 2 p'_j}{\\sqrt{2 p'_j (1 - p'_j)}}$, where $p'_j$ are the frequencies for the polymorphic variants.\n3.  **SVD and Variance Calculation**:\n    - The singular value decomposition is computed for the standardized matrix $X$ to obtain the singular values, $\\sigma_k$. We only need the singular values, not the full $U$ and $V$ matrices.\n    - The squared singular values, $\\sigma_k^2$, are calculated. The total variance is computed as their sum, $V_{\\text{total}} = \\sum_k \\sigma_k^2$.\n    - The fraction of variance explained by each of the top $k$ principal components is computed as $\\sigma_k^2 / V_{\\text{total}}$.\n4.  **Reporting**: The resulting fractions for the top $k$ components are collected and rounded to four decimal places as specified. This process is repeated for each test case provided.\n\nThis principled approach ensures a correct and robust implementation that directly follows from the mathematical definitions of PCA and SVD.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n\n    def _solve_case(group_sizes, num_variants, aim_blocks, neutral_af, seed, k):\n        \"\"\"\n        Solves a single test case for genotype simulation and PCA variance analysis.\n        \"\"\"\n        # Initialize a random number generator with the specified seed for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        num_groups = len(group_sizes)\n        n_total_samples = sum(group_sizes)\n        m_total_variants = num_variants\n\n        # 1. Construct allele frequency matrix (p_jg): variants x groups\n        p_variants_groups = np.zeros((m_total_variants, num_groups))\n        \n        current_variant_idx = 0\n        if aim_blocks:\n            for block in aim_blocks:\n                count, freqs = block['count'], block['freqs']\n                p_variants_groups[current_variant_idx:current_variant_idx + count, :] = freqs\n                current_variant_idx += count\n        \n        # Fill remaining variants with the neutral allele frequency\n        if current_variant_idx  m_total_variants:\n            p_variants_groups[current_variant_idx:, :] = neutral_af\n            \n        # 2. Generate genotype matrix G\n        # Create a map from sample index to group index\n        sample_to_group_map = np.repeat(np.arange(num_groups), group_sizes)\n        \n        # Generate genotypes g_ij ~ Binomial(2, p_j^(g))\n        G = np.zeros((n_total_samples, m_total_variants), dtype=np.float64)\n        for i in range(n_total_samples):\n            group_idx = sample_to_group_map[i]\n            # Get all allele frequencies for the variants for that sample's group\n            allele_freqs_for_sample = p_variants_groups[:, group_idx]\n            G[i, :] = rng.binomial(2, p=allele_freqs_for_sample)\n            \n        # 3. Standardize the genotype matrix\n        # Calculate overall allele frequencies p_j for each variant\n        # Use np.sum(G, axis=0, dtype=np.float64) to prevent potential overflow with integer types\n        p_j = np.sum(G, axis=0) / (2 * n_total_samples)\n        \n        # Identify and filter out monomorphic variants (where p_j is 0 or 1)\n        is_polymorphic = (p_j  0)  (p_j  1)\n        \n        G_poly = G[:, is_polymorphic]\n        p_j_poly = p_j[is_polymorphic]\n        \n        # If no polymorphic variants remain, variance is zero.\n        if G_poly.shape[1] == 0:\n            return [0.0] * k\n            \n        # Standardize G_poly to create matrix X\n        mean_g = 2 * p_j_poly\n        std_g = np.sqrt(2 * p_j_poly * (1 - p_j_poly))\n        \n        X = (G_poly - mean_g) / std_g\n        \n        # 4. Compute SVD and calculate variance fractions\n        # We only need the singular values, so compute_uv=False is efficient.\n        singular_values = svd(X, compute_uv=False)\n        \n        # The number of non-zero singular values cannot exceed min(n, m')\n        num_components = len(singular_values)\n        if num_components == 0:\n            return [0.0] * k\n\n        squared_sv = singular_values**2\n        total_variance = np.sum(squared_sv)\n\n        if total_variance == 0:\n            return [0.0] * k\n            \n        variance_fractions = squared_sv / total_variance\n        \n        # 5. Format results for the top k components\n        top_k_fractions = list(variance_fractions[:k])\n        \n        # Pad with zeros if the rank of the matrix is less than k\n        if len(top_k_fractions)  k:\n            top_k_fractions.extend([0.0] * (k - len(top_k_fractions)))\n            \n        return top_k_fractions\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"group_sizes\": [100, 100], \"num_variants\": 500,\n            \"aim_blocks\": [{'count': 80, 'freqs': [0.1, 0.9]}],\n            \"neutral_af\": 0.3, \"seed\": 123, \"k\": 3\n        },\n        # Case 2\n        {\n            \"group_sizes\": [120], \"num_variants\": 400,\n            \"aim_blocks\": [],\n            \"neutral_af\": 0.3, \"seed\": 456, \"k\": 3\n        },\n        # Case 3\n        {\n            \"group_sizes\": [80, 80, 40], \"num_variants\": 600,\n            \"aim_blocks\": [\n                {'count': 30, 'freqs': [0.1, 0.9, 0.9]},\n                {'count': 30, 'freqs': [0.9, 0.1, 0.9]}\n            ],\n            \"neutral_af\": 0.3, \"seed\": 789, \"k\": 3\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = _solve_case(**params)\n        all_results.append(result)\n\n    # Format the final output string according to the specified format.\n    # \"[[res1_1,res1_2,...],[res2_1,res2_2,...],...]\"\n    # with each fraction rounded to four decimal places.\n    formatted_case_results = []\n    for res_list in all_results:\n        # Format each number to exactly four decimal places\n        formatted_list_str = [f\"{x:.4f}\" for x in res_list]\n        # Join numbers with commas and enclose in brackets\n        formatted_case_results.append(f\"[{','.join(formatted_list_str)}]\")\n    \n    # Join all case results and enclose in the final brackets\n    final_output_string = f\"[{','.join(formatted_case_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}