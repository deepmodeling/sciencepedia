{
    "hands_on_practices": [
        {
            "introduction": "在任何大规模基因组测序项目中，数据的质量都是至关重要的。样本之间的交叉污染可能会引入虚假的遗传信号，从而严重影响研究结果的可靠性。本实践将引导你通过分析在目标样本中本应为纯合的位点上的等位基因平衡，建立一个用于估算和标记样本污染水平的量化模型，这是所有下游基因组分析开始前不可或缺的第一步质量控制 。",
            "id": "4370879",
            "problem": "一个国家级的人群生物样本库对大量参与者进行测序，并通过分析索引样本中纯合位点的等位基因平衡来监控高通量流程中的样本污染。考虑一个经过筛选的双等位基因单核苷酸多态性（SNP）面板，该面板的选择标准是在人群中具有高预期杂合度。对于给定样本，在该面板中，将在索引样本所有纯合位点上合并的次要等位基因读取计数定义为 $X$，合并的总读取计数定义为 $D$（因此，任何观察到的次要等位基因读取都源于测序错误或污染）。假设以下基本前提：\n\n- 每次读取都是一次独立的伯努利试验，反映了短读长测序中分子的标准随机抽样，其单次读取的次要等位基因概率取决于读取是源自索引样本还是污染源。\n- 设污染比例为 $c \\in [0,1]$，表示任何给定的读取源自污染源基因组的概率为 $c$，源自索引基因组的概率为 $1 - c$。\n- 设单碱基测序错误率为 $\\epsilon \\in (0, 0.5)$，并假定该值通过对对照数据的校准已知。\n- 设 $h \\in [0,1]$ 表示在生物样本库人群中随机抽取的污染源基因组在所选SNP面板上的杂合度概率，并假定该值根据人群规模的等位基因频率数据已知。在杂合的污染源基因型处，每次读取的预期次要等位基因概率为 $0.5$；在这些位点上的纯合污染源基因型处，次要等位基因读取仅由错误产生，错误率为 $\\epsilon$。\n\n在这些假设下，单次读取的次要等位基因概率在读取层面是一个双组分混合模型：读取有 $1-c$ 的概率来自索引样本，并以 $\\epsilon$ 的概率贡献一个次要等位基因；有 $c$ 的概率来自污染源，并根据面板在人群中的杂合度结构贡献一个次要等位基因。根据读取的独立性，$D$ 次读取中合并的次要等位基因计数 $X$ 服从二项分布，其成功参数是 $c$ 的一个仿射函数。仅使用这些基本假设，从第一性原理推导：\n\n1) 在双组分读取层面混合模型所隐含的二项模型下，给定 $X$ 和 $D$ 时 $c$ 的合并似然函数，以及用观测到的合并次要等位基因比例 $X/D$、$\\epsilon$ 和 $h$ 表示的 $c$ 的最大似然估计（MLE）。该估计值必须被裁剪到区间 $[0,1]$ 内。\n\n2) 使用精确的Clopper–Pearson方法，为合并的二项分布次要等位基因概率参数计算一个双边等尾 $100(1-\\alpha)\\%$ 置信区间，并通过反转仿射映射将其转换为 $c$ 的置信区间。将转换后的界限裁剪到 $[0,1]$ 内。\n\n3) 一个适用于人群规模生物样本库质量控制的排除规则：当且仅当污染的点估计值 $c$ 大于或等于指定的阈值 $\\tau$ 时，排除一个样本（所有污染值均以十进制小数而非百分比表示）。\n\n实现一个程序，对于固定的面板和质量控制配置，为每个测试样本计算：\n- 污染点估计值 $\\hat{c}$，\n- $c$ 的双边Clopper–Pearson $100(1-\\alpha)\\%$ 置信区间的下界和上界，\n- 根据上述规则得出的排除决策（布尔值）。\n\n使用以下由高质量短读长流程和从生物样本库中筛选出的高杂合度SNP面板确定的固定参数：\n- 测序错误率 $\\epsilon = 0.002$，\n- 面板杂合度概率 $h = 0.5$，\n- 置信水平 $1-\\alpha = 0.95$（即 $\\alpha = 0.05$），\n- 排除阈值 $\\tau = 0.03$。\n\n测试套件。对于每个测试用例，给定 $(D, X)$ 作为索引样本所有纯合位点的合并总数：\n- 案例A（典型的低污染）：$D = 150000$, $X = 487$。\n- 案例B（接近排除阈值）：$D = 120000$, $X = 1136$。\n- 案例C（高污染）：$D = 100000$, $X = 2192$。\n- 案例D（无污染但有抽样波动）：$D = 180000$, $X = 355$。\n- 案例E（覆盖度较低的临界情况）：$D = 5000$, $X = 45$。\n\n最终输出格式。您的程序应生成一行输出，包含一个用方括号括起来的逗号分隔列表，其中每个元素对应从A到E的一个测试用例，并且本身是一个包含四个元素的列表：[$\\hat{c}$, $c_{\\text{low}}$, $c_{\\text{high}}$, exclude]。三个污染值必须四舍五入到恰好 $6$ 位小数，布尔值必须是语言原生的布尔值。例如，两个假设案例的输出可能如下所示：$[[0.012345,0.010000,0.015000,False],[0.045678,0.043000,0.048500,True]]$。",
            "solution": "该问题被认为是有效的，因为它在科学上基于测序数据的标准模型，定义明确且包含所有必要信息，并且表述客观。我们着手进行求解。\n\n解答过程遵循问题陈述，分为三个部分。首先，我们推导污染比例 $c$ 的最大似然估计（MLE）。其次，我们推导 $c$ 的置信区间。第三，我们明确排除规则。\n\n### 第1部分：污染估计量（$\\hat{c}$）的推导\n\n该模型的核心是确定在索引样本的纯合位点上，单次读取显示次要等位基因的概率（我们称之为 $p$）。这个概率 $p$ 是一个二项分布的成功参数，该分布控制着总共 $D$ 次读取中合并的次要等位基因计数 $X$。\n\n概率 $p$ 是一个混合概率，取决于读取的来源（索引样本或污染源）。\n设 $c$ 为污染比例。一次读取有 $c$ 的概率来自污染源，有 $1-c$ 的概率来自索引样本。\n设 $\\epsilon$ 为单碱基测序错误率。\n设 $h$ 为污染源在给定SNP面板位点上为杂合子的概率。\n\n根据全概率定律，单次读取中观察到次要等位基因的概率 $p$ 为：\n$$p(c) = P(\\text{次要等位基因} | \\text{来自索引样本的读取}) P(\\text{来自索引样本的读取}) + P(\\text{次要等位基因} | \\text{来自污染源的读取}) P(\\text{来自污染源的读取})$$\n\n1.  如果读取来自索引样本（概率为 $1-c$）：索引样本在选定位点上对于主要等位基因是纯合的。只有在发生测序错误时才能观察到次要等位基因。因此，$P(\\text{次要等位基因} | \\text{来自索引样本的读取}) = \\epsilon$。\n\n2.  如果读取来自污染源（概率为 $c$）：观察到次要等位基因的概率取决于污染源的基因型。\n    -   污染源为杂合子（例如，在A/A索引位点上为A/T）的概率为 $h$。在这种情况下，抽样到次要等位基因的概率是 $0.5$。\n    -   污染源与索引样本一样，对于相同的主要等位基因为纯合子（例如，在A/A索引位点上为A/A）的概率为 $1-h$。在这种情况下，只有因测序错误才能观察到次要等位基因，概率为 $\\epsilon$。\n    因此，给定读取来自污染源，观察到次要等位基因的概率为：\n    $$P(\\text{次要等位基因} | \\text{来自污染源的读取}) = h \\cdot 0.5 + (1-h) \\cdot \\epsilon$$\n\n综合这些项，总的次要等位基因概率 $p$ 为：\n$$p(c) = \\epsilon (1-c) + \\left( 0.5h + \\epsilon(1-h) \\right) c$$\n展开并简化表达式：\n$$p(c) = \\epsilon - \\epsilon c + 0.5hc + \\epsilon c - h\\epsilon c$$\n$$p(c) = \\epsilon + c(0.5h - h\\epsilon)$$\n$$p(c) = \\epsilon + ch(0.5 - \\epsilon)$$\n这证实了 $p$ 是 $c$ 的一个仿射函数。设常数斜率为 $A = h(0.5 - \\epsilon)$。关系式为 $p(c) = \\epsilon + Ac$。\n\n在总共 $D$ 次读取中，次要等位基因的读取次数 $X$ 被建模为一个二项随机变量：\n$$X \\sim \\text{Binomial}(D, p(c))$$\n给定数据 $(X, D)$，$c$ 的似然函数为：\n$$L(c; X, D) = \\binom{D}{X} [p(c)]^X [1 - p(c)]^{D-X}$$\n为了找到 $c$ 的最大似然估计（MLE），我们最大化对数似然函数 $\\ell(c) = \\log L(c)$。众所周知，二项比例 $p$ 的MLE是 $\\hat{p} = X/D$。我们可以通过对对数似然函数关于 $p$（并由此对 $c$）求导来证明这一点：\n$$\\frac{d\\ell}{dc} = \\frac{d\\ell}{dp} \\frac{dp}{dc} = \\left( \\frac{X}{p} - \\frac{D-X}{1-p} \\right) A$$\n将导数设为零（并假设 $A \\neq 0$），我们发现括号中的项必须为零，这得到 $\\hat{p} = X/D$。\n\n将此结果代入我们的 $p(c)$ 仿射关系式中：\n$$\\hat{p} = \\epsilon + \\hat{c}h(0.5 - \\epsilon)$$\n求解MLE $\\hat{c}$：\n$$\\hat{c} = \\frac{\\hat{p} - \\epsilon}{h(0.5 - \\epsilon)} = \\frac{X/D - \\epsilon}{h(0.5 - \\epsilon)}$$\n由于 $c$ 是一个比例，其估计值必须位于区间 $[0,1]$ 内。我们通过裁剪估计值来强制满足这一条件：\n$$\\hat{c}_{\\text{clipped}} = \\max\\left(0, \\min\\left(1, \\frac{X/D - \\epsilon}{h(0.5 - \\epsilon)}\\right)\\right)$$\n\n### 第2部分：$c$ 的置信区间的推导\n\n我们首先使用精确的Clopper-Pearson方法为二项比例 $p$ 计算一个 $100(1-\\alpha)\\%$ 的置信区间，记为 $[p_{\\text{low}}, p_{\\text{high}}]$。该方法通过反转二项检验来定义区间界限。\n-   下界 $p_{\\text{low}}$ 是这样一个 $p$ 值，使得观察到 $X$ 次或更多成功的概率为 $\\alpha/2$：\n    $$P(Y \\ge X | Y \\sim \\text{Binomial}(D, p_{\\text{low}})) = \\sum_{k=X}^{D} \\binom{D}{k} p_{\\text{low}}^k (1-p_{\\text{low}})^{D-k} = \\frac{\\alpha}{2}$$\n-   上界 $p_{\\text{high}}$ 是这样一个 $p$ 值，使得观察到 $X$ 次或更少成功的概率为 $\\alpha/2$：\n    $$P(Y \\le X | Y \\sim \\text{Binomial}(D, p_{\\text{high}})) = \\sum_{k=0}^{X} \\binom{D}{k} p_{\\text{high}}^k (1-p_{\\text{high}})^{D-k} = \\frac{\\alpha}{2}$$\n\n这些方程可以使用贝塔分布的分位数来求解。\n-   $p_{\\text{low}} = B(\\frac{\\alpha}{2}; X, D-X+1)$，其中 $B(q; a, b)$ 是形状参数为 $a$ 和 $b$ 的贝塔分布的 $q$-分位数。如果 $X=0$，$p_{\\text{low}}=0$。\n-   $p_{\\text{high}} = B(1-\\frac{\\alpha}{2}; X+1, D-X)$。如果 $X=D$，$p_{\\text{high}}=1$。\n\n接下来，我们将这个 $p$ 的置信区间转换为 $c$ 的置信区间。关系式 $c(p) = \\frac{p - \\epsilon}{h(0.5 - \\epsilon)}$ 是关于 $p$ 单调递增的，因为对于给定的参数（$h=0.5  0$ 且 $\\epsilon=0.002  0.5$），分母 $h(0.5 - \\epsilon)$ 为正。\n因此，$c$ 的置信区间 $[c_{\\text{low}}, c_{\\text{high}}]$ 可通过将该变换应用于 $p$ 的区间界限来获得：\n$$c_{\\text{low}} = \\frac{p_{\\text{low}} - \\epsilon}{h(0.5 - \\epsilon)}$$\n$$c_{\\text{high}} = \\frac{p_{\\text{high}} - \\epsilon}{h(0.5 - \\epsilon)}$$\n与点估计一样，这些界限也被裁剪到有效范围 $[0, 1]$ 内：\n$$c_{\\text{low, clipped}} = \\max(0, \\min(1, c_{\\text{low}}))$$\n$$c_{\\text high, clipped}} = \\max(0, \\min(1, c_{\\text{high}}))$$\n\n### 第3部分：排除规则\n\n质量控制规则是：如果样本的污染点估计值 $\\hat{c}$ 大于或等于指定的阈值 $\\tau$，则排除该样本。\n$$\\text{排除} = (\\hat{c} \\ge \\tau)$$\n\n使用固定参数：\n-   $\\epsilon = 0.002$\n-   $h = 0.5$\n-   $1-\\alpha = 0.95 \\implies \\alpha = 0.05 \\implies \\alpha/2 = 0.025$\n-   $\\tau = 0.03$\n\n用于转换的分母是 $h(0.5 - \\epsilon) = 0.5(0.5 - 0.002) = 0.249$。\n\n使用推导出的公式来处理这些测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Solves the contamination estimation problem for a series of test cases.\n    \"\"\"\n\n    # Fixed parameters from the problem statement\n    epsilon = 0.002  # sequencing error rate\n    h = 0.5          # panel heterozygosity probability\n    alpha = 0.05     # for 95% confidence interval\n    tau = 0.03       # exclusion threshold\n\n    # Test cases: (D, X)\n    # D: pooled total read count\n    # X: pooled minor-allele read count\n    test_cases = [\n        (150000, 487),   # Case A\n        (120000, 1136),  # Case B\n        (100000, 2192),  # Case C\n        (180000, 355),   # Case D\n        (5000, 45)       # Case E\n    ]\n\n    # Pre-calculate the constant denominator for the transformation\n    # c = (p - epsilon) / denom\n    denom = h * (0.5 - epsilon)\n\n    results_as_strings = []\n    for D, X in test_cases:\n        # 1. Calculate the Maximum Likelihood Estimate (MLE) of c\n        p_hat = X / D\n        c_hat_raw = (p_hat - epsilon) / denom\n        c_hat = max(0.0, min(1.0, c_hat_raw))\n\n        # 2. Calculate the Clopper-Pearson confidence interval for c\n\n        # Calculate CI for the binomial proportion p\n        if X == 0:\n            p_low = 0.0\n        else:\n            p_low = beta.ppf(alpha / 2, X, D - X + 1)\n        \n        if X == D:\n            p_high = 1.0\n        else:\n            p_high = beta.ppf(1 - alpha / 2, X + 1, D - X)\n\n        # Transform the CI for p to a CI for c\n        c_low_raw = (p_low - epsilon) / denom\n        c_high_raw = (p_high - epsilon) / denom\n        \n        # Clip the CI bounds to the [0, 1] interval\n        c_low = max(0.0, min(1.0, c_low_raw))\n        c_high = max(0.0, min(1.0, c_high_raw))\n\n        # 3. Apply the exclusion rule\n        exclude = c_hat >= tau\n\n        # Format the results for this case\n        result_string = (\n            f\"[{c_hat:.6f},\"\n            f\"{c_low:.6f},\"\n            f\"{c_high:.6f},\"\n            f\"{exclude}]\"\n        )\n        results_as_strings.append(result_string)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在确保了单个样本的质量之后，下一步是评估遗传变异本身在群体水平上的可靠性。哈迪-温伯格平衡（Hardy-Weinberg Equilibrium, HWE）是群体遗传学的核心原则，显著偏离HWE的变异位点往往暗示着基因分型错误或其他技术偏差。这项实践将指导你实现标准的卡方检验来评估HWE，这是在进行关联分析等下游研究之前筛选变异位点的关键技能 。",
            "id": "4370919",
            "problem": "给定一组人群组，每个代表了某个群体规模生物样本库中单个双等位基因位点的祖源分层。对于每个组，您会获得变异等位基因的基因型计数，分别编码为 $0$、$1$ 或 $2$ 个拷贝。将观察到的计数表示为 $\\left(n_{0}, n_{1}, n_{2}\\right)$，其中 $n_{0}$ 是拥有 $0$ 个变异等位基因的个体数量，$n_{1}$ 是拥有 $1$ 个变异等位基因的个体数量，$n_{2}$ 是拥有 $2$ 个变异等位基因的个体数量。令 $N = n_{0} + n_{1} + n_{2}$ 为该组中非缺失基因型的总数。您必须计算变异等位基因的特定祖源等位基因频率，并在每个组内检验哈迪-温伯格平衡 (Hardy-Weinberg equilibrium, HWE)，以检测分层偏差和潜在的基因型检出问题。\n\n使用以下基本原理：\n- 在此位点上，满足随机交配且无选择、迁移或突变的条件下，哈迪-温伯格定律表明，如果变异等位基因频率为 $p$，参考等位基因频率为 $q = 1 - p$，则拥有 $2$、$1$ 和 $0$ 个变异等位基因的基因型频率分别为 $p^{2}$、$2 p q$ 和 $q^{2}$。\n- 组内变异等位基因频率的最大似然估计量为 $\\hat{p} = \\dfrac{n_{1} + 2 n_{2}}{2 N}$。\n- 用于 HWE 的标准失拟检验使用皮尔逊卡方统计量，该统计量比较观察到的基因型计数与期望计数 $N q^{2}$、$2 N p q$ 和 $N p^{2}$。在 HWE 的原假设和大样本正则性条件下，该检验统计量渐近服从自由度 $\\nu = 1$ 的卡方分布。\n\n您的任务是实现一个程序，该程序能够：\n- 对每个组，使用自由度 $\\nu = 1$ 的皮尔逊卡方检验计算变异等位基因频率 $\\hat{p}$ 和 HWE 的 $p$ 值。\n- 定义用于显著性判断的决策阈值 $\\alpha = 10^{-3}$。\n- 为每个测试用例标记两种情况：\n  1. `any_deviation`：如果测试用例中至少有一个组的 $p$ 值严格小于 $\\alpha$，则为 true。\n  2. `stratified_deviation`：如果存在至少一个组的 $p$ 值严格小于 $\\alpha$，且至少有另一个组的 $p$ 值大于或等于 $\\alpha$，则为 true。如果 $N > 0$ 的组少于两个，则将 `stratified_deviation` 设置为 false。\n- 以科学上合理的方式处理边界情况：\n  - 如果某个组的 $N = 0$，则定义 $\\hat{p} = 0$ 且 HWE 的 $p$ 值为 $1.0$。\n  - 在计算卡方统计量时，仅包括期望计数严格为正的项；如果期望计数为 $0$，则跳过该项。这与在拟合模型下某个单元格的期望概率为零时从似然渐近理论推导出的结论是一致的。\n\n测试套件包含以下四个测试用例，每个测试用例由一个祖源组列表组成，其中每个组由三元组 $\\left[n_{0}, n_{1}, n_{2}\\right]$ 给出：\n- 测试用例 1：两个组\n  - 组 A：$\\left[490, 420, 90\\right]$\n  - 组 B：$\\left[560, 280, 160\\right]$\n- 测试用例 2：两个组\n  - 组 A：$\\left[500, 0, 0\\right]$\n  - 组 B：$\\left[405, 90, 5\\right]$\n- 测试用例 3：两个组\n  - 组 A：$\\left[27, 3, 0\\right]$\n  - 组 B：$\\left[16, 8, 1\\right]$\n- 测试用例 4：两个组\n  - 组 A：$\\left[0, 0, 0\\right]$\n  - 组 B：$\\left[100, 600, 100\\right]$\n\n对于每个测试用例，您的程序必须生成一个列表，按顺序包含：\n- 各组的变异等位基因频率列表 $\\left[\\hat{p}_{1}, \\hat{p}_{2}, \\dots \\right]$，以浮点数形式表示。\n- 各组的 HWE $p$ 值列表 $\\left[p\\text{-value}_{1}, p\\text{-value}_{2}, \\dots \\right]$，以浮点数形式表示。\n- 布尔标志 `any_deviation`。\n- 布尔标志 `stratified_deviation`。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，列表中的每个元素按上文给出的确切顺序对应一个测试用例。每个元素本身必须是按所述结构组织的列表。例如，最外层结构必须类似于 $\\left[\\text{test\\_result}_{1}, \\text{test\\_result}_{2}, \\text{test\\_result}_{3}, \\text{test\\_result}_{4}\\right]$，其中每个 $\\text{test\\_result}_{i}$ 都具有所述形式。不涉及任何物理单位或角度单位。所有比例和概率必须表示为小数，而不是百分比。程序没有输入；它必须使用嵌入的测试套件。",
            "solution": "此问题经评估为有效。它在科学上基于群体遗传学原理，特别是哈迪-温伯格平衡 (Hardy-Weinberg equilibrium, HWE) 定律。该问题提法恰当，具有清晰的定义、数据和目标。它是客观、自包含的，并且可以形式化为计算算法。提供的常数和公式，例如等位基因频率的最大似然估计量和使用自由度为 $\\nu=1$ 的皮尔逊卡方检验，对于此类分析是标准且正确的。边界情况得到了明确和恰当的处理。\n\n解决方案通过实现一个算法来分析多个测试用例中的多个种群群体的基因型计数。对于每个组，我们计算变异等位基因频率并检验与 HWE 的偏离。然后将这些结果汇总，以推导出用于统计显著性的测试用例级别标志。\n\n### 单个种群群体的算法\n\n对于每个由 $0$、$1$ 和 $2$ 个变异等位基因拷贝的观察基因型计数 $(n_0, n_1, n_2)$ 定义的组，我们执行以下步骤：\n\n1.  **计算样本量**：具有非缺失基因型的个体总数为 $N = n_0 + n_1 + n_2$。\n\n2.  **处理零样本量**：按规定，如果 $N=0$，则该组不提供任何信息。变异等位基因频率 $\\hat{p}$ 定义为 $0$，HWE 的 $p$ 值定义为 $1.0$。\n\n3.  **计算等位基因频率**：如果 $N  0$，则通过等位基因计数法计算变异等位基因频率的最大似然估计 (MLE) $\\hat{p}$：\n    $$ \\hat{p} = \\frac{n_1 + 2n_2}{2N} $$\n    参考等位基因的频率则为 $\\hat{q} = 1 - \\hat{p}$。\n\n4.  **计算期望基因型计数**：在 HWE 的原假设下，期望基因型计数由估计的等位基因频率推导得出：\n    -   $0$ 个变异等位基因的期望计数：$E_0 = N \\hat{q}^2$\n    -   $1$ 个变异等位基因的期望计数：$E_1 = 2N\\hat{p}\\hat{q}$\n    -   $2$ 个变异等位基因的期望计数：$E_2 = N \\hat{p}^2$\n\n5.  **计算皮尔逊卡方统计量**：使用皮尔逊 $\\chi^2$ 统计量进行失拟检验，该统计量将观察计数 $(O_0, O_1, O_2) = (n_0, n_1, n_2)$ 与期望计数 $(E_0, E_1, E_2)$ 进行比较：\n    $$ \\chi^2 = \\sum_{i=0}^{2} \\frac{(O_i - E_i)^2}{E_i} $$\n    根据问题说明，仅当对应的期望计数 $E_i$ 严格为正时，才将项包含在总和中。如果 $\\hat{p}=0$ 或 $\\hat{p}=1$，则样本是单态的，观察计数与期望计数完全匹配（例如，观察值为 $(N, 0, 0)$ vs. 期望值为 $(N, 0, 0)$），$\\chi^2$ 统计量为 $0$，数据处于完美的 HWE 状态。\n\n6.  **计算 p 值**：在原假设下，$\\chi^2$ 统计量渐近服从自由度为 $\\nu = 3 - 1 - 1 = 1$ 的卡方分布（3 个基因型类别减去 1 个总数约束，再减去 1 个估计参数 $\\hat{p}$）。$p$ 值是在原假设为真的前提下，观察到至少与计算出的检验统计量一样极端的检验统计量的概率。这对应于在计算出的统计量处求值的 $\\chi^2(\\nu=1)$ 分布的生存函数 (1 - CDF)。\n\n### 整个测试用例的算法\n\n对于每个由一个或多个种群群体组成的测试用例，在单独分析每个组后，执行以下步骤：\n\n1.  **收集各组结果**：将计算出的变异等位基因频率 $[\\hat{p}_1, \\hat{p}_2, \\dots]$ 和 HWE $p$ 值 $[p\\text{-value}_1, p\\text{-value}_2, \\dots]$ 收集到单独的列表中。\n\n2.  **定义显著性阈值**：HWE 检验的显著性水平设置为 $\\alpha = 10^{-3}$。\n\n3.  **确定 `any_deviation` 标志**：如果任何计算出的组的 $p$ 值严格小于 $\\alpha$，则此布尔标志设置为 `true`。否则，设置为 `false`。\n    $$ \\text{any\\_deviation} = \\exists i : p\\text{-value}_i  \\alpha $$\n\n4.  **确定 `stratified_deviation` 标志**：此标志指示 HWE 偏差是否存在于部分而非全部种群分层中。其逻辑由两个条件定义：\n    a. 首先，我们计算具有非零样本量 ($N  0$) 的组的数量。如果此计数小于 $2$，则将 `stratified_deviation` 设置为 `false`，因为少于两个信息性组无法评估分层。\n    b. 如果有两个或更多信息性组，当且仅当存在至少一个组的 $p$ 值严格小于 $\\alpha$ 并且至少有另一个组的 $p$ 值大于或等于 $\\alpha$ 时，该标志才设置为 `true`。\n\n### 实现\n该算法使用 Python 实现，利用 `numpy` 库进行数值运算，并使用 `scipy.stats.chi2` 从卡方分布计算 $p$ 值。最终输出是一个表示结果列表的单个字符串，每个测试用例对应一个结果元素，并按照指定要求精确格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Computes ancestry-specific allele frequencies and tests for Hardy-Weinberg\n    Equilibrium (HWE) for a set of population groups.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        [[490, 420, 90], [560, 280, 160]],\n        # Test case 2\n        [[500, 0, 0], [405, 90, 5]],\n        # Test case 3\n        [[27, 3, 0], [16, 8, 1]],\n        # Test case 4\n        [[0, 0, 0], [100, 600, 100]],\n    ]\n\n    alpha = 1e-3\n    all_results = []\n\n    for case in test_cases:\n        p_hats = []\n        p_values = []\n        groups_with_N_gt_0 = 0\n\n        for group_counts in case:\n            n0, n1, n2 = group_counts\n            N = n0 + n1 + n2\n\n            if N == 0:\n                p_hat = 0.0\n                p_val = 1.0\n            else:\n                groups_with_N_gt_0 += 1\n                p_hat = (n1 + 2 * n2) / (2 * N)\n                \n                # For monomorphic populations (p_hat=0 or p_hat=1), there is no\n                # deviation from HWE. The chi-squared statistic is 0.\n                if p_hat == 0.0 or p_hat == 1.0:\n                    chi2_stat = 0.0\n                else:\n                    q_hat = 1.0 - p_hat\n                    \n                    # Expected counts under HWE\n                    E = np.array([N * q_hat**2, N * 2 * p_hat * q_hat, N * p_hat**2])\n                    O = np.array([n0, n1, n2], dtype=float)\n                    \n                    # Pearson's chi-squared statistic\n                    chi2_stat = 0.0\n                    if E[0] > 0:\n                        chi2_stat += (O[0] - E[0])**2 / E[0]\n                    if E[1] > 0:\n                        chi2_stat += (O[1] - E[1])**2 / E[1]\n                    if E[2] > 0:\n                        chi2_stat += (O[2] - E[2])**2 / E[2]\n                \n                # p-value from chi-squared distribution with 1 degree of freedom\n                p_val = chi2.sf(chi2_stat, df=1)\n\n            p_hats.append(p_hat)\n            p_values.append(p_val)\n\n        # Flag for any deviation from HWE\n        any_dev = any(p  alpha for p in p_values)\n        \n        # Flag for stratified deviation\n        strat_dev = False\n        if groups_with_N_gt_0 >= 2:\n            has_low_p = any(p  alpha for p in p_values)\n            has_high_p = any(p >= alpha for p in p_values)\n            if has_low_p and has_high_p:\n               strat_dev = True\n        \n        # Assemble the final result for the test case\n        case_result = [p_hats, p_values, any_dev, strat_dev]\n        all_results.append(case_result)\n\n    # Custom string formatting to match problem requirements\n    def custom_format(obj):\n        if isinstance(obj, bool):\n            return str(obj)\n        if isinstance(obj, list):\n            return f\"[{','.join(map(custom_format, obj))}]\"\n        if isinstance(obj, float):\n            return f\"{obj:.17g}\" # High precision for scientific values\n        return str(obj)\n\n    results_str_list = [custom_format(res) for res in all_results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "拥有高质量的样本和变异数据后，我们便可以开始探索数据中蕴含的生物学信息。在大型生物样本库中，理解群体的遗传背景和结构对于避免关联研究中的混淆偏倚至关重要。本练习将展示如何应用主成分分析（Principal Component Analysis, PCA）这一核心技术，从高维度的基因型数据中解析出群体结构，为后续的精准医学研究奠定基础 。",
            "id": "4370918",
            "problem": "一个群体规模的生物样本库收集了大量个体在众多基因组变异位点上的基因型剂量。考虑一个由多个大陆祖源群体组成的混合队列。该队列由一个基因型矩阵表示，其中行代表样本，列代表变异位点。每个条目是每个样本中每个变异位点的替代等位基因计数，取值为 $\\{0,1,2\\}$。对标准化基因型数据进行主成分分析（PCA）通常用于捕捉群体结构，如果不对其进行校正，可能会混淆下游的精准医疗分析。您的任务是通过奇异值分解计算主成分，并量化在几种情景下，由顶层主成分解释的总方差比例。\n\n使用以下基本依据：\n- 基因型剂量 $g_{ij} \\in \\{0,1,2\\}$ 是样本 $i$ 和变异位点 $j$ 的替代等位基因计数。\n- 所有样本中变异位点 $j$ 的等位基因频率为 $p_j = \\frac{1}{2n} \\sum_{i=1}^{n} g_{ij}$，其中 $n$ 是样本数量。\n- 在哈迪-温伯格平衡下，变异位点 $j$ 的基因型剂量的预期方差为 $2 p_j (1 - p_j)$，这被广泛用于PCA中对基因型列进行标准化。\n- 设标准化数据矩阵为 $X \\in \\mathbb{R}^{n \\times m'}$，其条目为 $x_{ij} = \\frac{g_{ij} - 2 p_j}{\\sqrt{2 p_j (1 - p_j)}}$，适用于所有 $0  p_j  1$ 的变异位点。$p_j \\in \\{0,1\\}$ 的变异位点方差为零，必须从PCA中排除。\n- 对 $X$ 的主成分分析可通过奇异值分解 $X = U \\Sigma V^{\\top}$ 获得，其中 $\\Sigma$ 具有非负奇异值 $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$。\n\n基于这些依据，推导出如何从奇异值计算每个主成分解释的方差比例，并以算法形式实现。您必须：\n1. 根据指定参数为具有大陆祖源的混合队列构建合成基因型矩阵。对于每个祖源群体 $g$，通过从二项分布中抽样来生成每个基因型 $g_{ij}$，试验次数为 $2$，成功概率等于特定群体的等位基因频率 $p_{j}^{(g)}$，每个测试用例使用固定的伪随机种子以确保确定性输出。\n2. 使用上述哈迪-温伯格标度法对基因型矩阵进行标准化，并移除单态性变异（$p_j \\in \\{0,1\\}$）。\n3. 计算标准化矩阵的奇异值分解，并计算前 $k$ 个主成分解释的总方差比例。将每个比例表示为四舍五入到四位小数的十进制数（不带百分号）。\n\n测试套件与参数：\n实现以下三个测试用例。每个案例由群体规模、变异位点数量、具有特定群体等位基因频率的祖源信息标记（AIM）区块、剩余变异位点在各群体间共享的中性等位基因频率、一个伪随机种子和 $k$（要报告的顶层主成分数量）定义。\n\n- 案例1（通用的双祖源混合队列）：\n    - 群体数量：$2$ 个，群体规模为 $[100, 100]$。\n    - 变异位点数量：$500$。\n    - AIM区块：一个区块，数量为 $80$，两个群体各自的等位基因频率为 $[0.1, 0.9]$。\n    - 剩余变异位点的中性等位基因频率：两个群体均为 $0.3$。\n    - 种子：$123$。\n    - 报告前 $k = 3$ 个主成分。\n\n- 案例2（边界情况：单一祖源群体，结构极小）：\n    - 群体数量：$1$ 个，群体规模为 $[120]$。\n    - 变异位点数量：$400$。\n    - AIM区块：无（即，所有变异位点在单一群体中具有相同的等位基因频率）。\n    - 所有变异位点的中性等位基因频率：$0.3$。\n    - 种子：$456$。\n    - 报告前 $k = 3$ 个主成分。\n\n- 案例3（具有两个正交AIM区块的三祖源队列）：\n    - 群体数量：$3$ 个，群体规模为 $[80, 80, 40]$。\n    - 变异位点数量：$600$。\n    - AIM区块：两个区块：\n        - 区块1：数量 $30$，特定群体等位基因频率为 $[0.1, 0.9, 0.9]$。\n        - 区块2：数量 $30$，特定群体等位基因频率为 $[0.9, 0.1, 0.9]$。\n    - 剩余变异位点的中性等位基因频率：所有群体均为 $0.3$。\n    - 种子：$789$。\n    - 报告前 $k = 3$ 个主成分。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个元素对应一个测试用例，并且必须是一个由方括号括起来、逗号分隔的前 $k$ 个方差解释比例的列表，四舍五入到四位小数，无空格。例如：\"[[0.3210,0.1205,0.0800],[0.0155,0.0132,0.0121],[0.2501,0.1700,0.1200]]\"。",
            "solution": "该问题要求实现一个标准群体遗传学工作流程来分析模拟的基因型数据。这包括为混合群体生成合成基因型、标准化数据、通过奇异值分解（SVD）执行主成分分析（PCA），以及通过计算前导主成分解释的总方差比例来量化群体结构。\n\n首先，我们建立从奇异值计算所解释方差比例的理论基础。设标准化基因型矩阵为 $X \\in \\mathbb{R}^{n \\times m'}$，其中 $n$ 是样本数量，$m'$ 是多态性变异（即等位基因频率 $p_j$ 满足 $0  p_j  1$ 的变异位点）的数量。该矩阵的条目由 $x_{ij} = \\frac{g_{ij} - 2 p_j}{\\sqrt{2 p_j (1 - p_j)}}$ 给出。根据构造， $X$ 的每一列都经过中心化（均值为 $0$）和标准化处理。\n\n数据中的总方差是数据点总离散程度的度量。在PCA的背景下，它被定义为样本协方差矩阵 $\\frac{1}{n-1}X^{\\top}X$ 的迹。与总方差成比例的总平方和，可以从矩阵 $X$ 的弗罗贝尼乌斯范数的平方计算得出：\n$$ \\|X\\|_F^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{m'} x_{ij}^2 $$\n$X$ 的奇异值分解由 $X = U \\Sigma V^{\\top}$ 给出，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{m' \\times m'}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times m'}$ 是一个矩形对角矩阵，其对角线上有非负奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$，其中 $r$ 是 $X$ 的秩。\n\nSVD的一个关键性质是，矩阵的弗罗贝尼乌斯范数的平方等于其奇异值平方的和：\n$$ \\|X\\|_F^2 = \\text{Tr}(X^{\\top}X) = \\text{Tr}((U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top})) = \\text{Tr}(V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top}) = \\text{Tr}(V\\Sigma^{\\top}\\Sigma V^{\\top}) $$\n利用迹的循环性质 $\\text{Tr}(ABC) = \\text{Tr}(CAB)$，我们有：\n$$ \\text{Tr}(V(\\Sigma^{\\top}\\Sigma) V^{\\top}) = \\text{Tr}((\\Sigma^{\\top}\\Sigma)V^{\\top}V) = \\text{Tr}(\\Sigma^{\\top}\\Sigma) = \\sum_{k=1}^{r} \\sigma_k^2 $$\n第 $k$ 个主成分所捕获的方差与第 $k$ 个奇异值的平方 $\\sigma_k^2$ 成正比。因此，第 $k$ 个主成分所解释的总方差比例，是 $\\sigma_k^2$ 与所有奇异值平方和的比值。\n$$ \\text{方差比例(PC}_k\\text{)} = \\frac{\\sigma_k^2}{\\sum_{l=1}^{r} \\sigma_l^2} $$\n这个公式提供了一种从标准化基因型矩阵的奇异值直接计算所需量值的方法。\n\n解决每个测试用例的算法流程如下：\n1.  **数据模拟**：构建一个基因型矩阵 $G \\in \\mathbb{R}^{n \\times m}$，其中 $n$ 是所有群体的总样本数，$m$ 是总变异位点数。\n    - 创建一个大小为 $m \\times (\\text{群体数量})$ 的等位基因频率矩阵，用指定的祖源信息标记（AIM）区块的频率和中性背景频率填充。\n    - 建立从每个样本索引 $i \\in \\{1, \\dots, n\\}$ 到其对应祖源群体 $g$ 的映射。\n    - 对于群体 $g$ 中的每个样本 $i$ 和每个变异位点 $j$，从一个二项分布 $g_{ij} \\sim \\text{Binomial}(2, p_{j}^{(g)})$ 中抽取一个基因型剂量 $g_{ij}$，其中 $p_{j}^{(g)}$ 是变异位点 $j$ 在群体 $g$ 中的等位基因频率。固定的伪随机种子确保了每个案例的可复现性。\n2.  **数据标准化**：\n    - 使用公式 $p_j = \\frac{1}{2n} \\sum_{i=1}^{n} g_{ij}$ 计算每个变异位点 $j$ 在所有 $n$ 个样本中的总体等位基因频率 $p_j$。\n    - 识别并从矩阵 $G$ 中排除单态性变异（即 $p_j = 0$ 或 $p_j = 1$），得到一个大小为 $n \\times m'$ 的新矩阵 $G'$，其中 $m' \\le m$。\n    - 按列对矩阵 $G'$ 进行标准化，以生成矩阵 $X$，其条目为 $x_{ij} = \\frac{g'_{ij} - 2 p'_j}{\\sqrt{2 p'_j (1 - p'_j)}}$，其中 $p'_j$ 是多态性变异的频率。\n3.  **SVD与方差计算**：\n    - 对标准化矩阵 $X$ 进行奇异值分解以获得奇异值 $\\sigma_k$。我们只需要奇异值，而不需要完整的 $U$ 和 $V$ 矩阵。\n    - 计算奇异值的平方 $\\sigma_k^2$。总方差计算为它们的和，$V_{\\text{total}} = \\sum_k \\sigma_k^2$。\n    - 计算前 $k$ 个主成分中每个主成分解释的方差比例，即 $\\sigma_k^2 / V_{\\text{total}}$。\n4.  **报告**：收集前 $k$ 个主成分的所得比例，并按规定四舍五入到四位小数。对每个提供的测试用例重复此过程。\n\n这种基于原理的方法确保了实现的正确性和稳健性，它直接遵循PCA和SVD的数学定义。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n\n    def _solve_case(group_sizes, num_variants, aim_blocks, neutral_af, seed, k):\n        \"\"\"\n        Solves a single test case for genotype simulation and PCA variance analysis.\n        \"\"\"\n        # Initialize a random number generator with the specified seed for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        num_groups = len(group_sizes)\n        n_total_samples = sum(group_sizes)\n        m_total_variants = num_variants\n\n        # 1. Construct allele frequency matrix (p_jg): variants x groups\n        p_variants_groups = np.zeros((m_total_variants, num_groups))\n        \n        current_variant_idx = 0\n        if aim_blocks:\n            for block in aim_blocks:\n                count, freqs = block['count'], block['freqs']\n                p_variants_groups[current_variant_idx:current_variant_idx + count, :] = freqs\n                current_variant_idx += count\n        \n        # Fill remaining variants with the neutral allele frequency\n        if current_variant_idx  m_total_variants:\n            p_variants_groups[current_variant_idx:, :] = neutral_af\n            \n        # 2. Generate genotype matrix G\n        # Create a map from sample index to group index\n        sample_to_group_map = np.repeat(np.arange(num_groups), group_sizes)\n        \n        # Generate genotypes g_ij ~ Binomial(2, p_j^(g))\n        G = np.zeros((n_total_samples, m_total_variants), dtype=np.float64)\n        for i in range(n_total_samples):\n            group_idx = sample_to_group_map[i]\n            # Get all allele frequencies for the variants for that sample's group\n            allele_freqs_for_sample = p_variants_groups[:, group_idx]\n            G[i, :] = rng.binomial(2, p=allele_freqs_for_sample)\n            \n        # 3. Standardize the genotype matrix\n        # Calculate overall allele frequencies p_j for each variant\n        # Use np.sum(G, axis=0, dtype=np.float64) to prevent potential overflow with integer types\n        p_j = np.sum(G, axis=0) / (2 * n_total_samples)\n        \n        # Identify and filter out monomorphic variants (where p_j is 0 or 1)\n        is_polymorphic = (p_j > 0)  (p_j  1)\n        \n        G_poly = G[:, is_polymorphic]\n        p_j_poly = p_j[is_polymorphic]\n        \n        # If no polymorphic variants remain, variance is zero.\n        if G_poly.shape[1] == 0:\n            return [0.0] * k\n            \n        # Standardize G_poly to create matrix X\n        mean_g = 2 * p_j_poly\n        std_g = np.sqrt(2 * p_j_poly * (1 - p_j_poly))\n        \n        X = (G_poly - mean_g) / std_g\n        \n        # 4. Compute SVD and calculate variance fractions\n        # We only need the singular values, so compute_uv=False is efficient.\n        singular_values = svd(X, compute_uv=False)\n        \n        # The number of non-zero singular values cannot exceed min(n, m')\n        num_components = len(singular_values)\n        if num_components == 0:\n            return [0.0] * k\n\n        squared_sv = singular_values**2\n        total_variance = np.sum(squared_sv)\n\n        if total_variance == 0:\n            return [0.0] * k\n            \n        variance_fractions = squared_sv / total_variance\n        \n        # 5. Format results for the top k components\n        top_k_fractions = list(variance_fractions[:k])\n        \n        # Pad with zeros if the rank of the matrix is less than k\n        if len(top_k_fractions)  k:\n            top_k_fractions.extend([0.0] * (k - len(top_k_fractions)))\n            \n        return top_k_fractions\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"group_sizes\": [100, 100], \"num_variants\": 500,\n            \"aim_blocks\": [{'count': 80, 'freqs': [0.1, 0.9]}],\n            \"neutral_af\": 0.3, \"seed\": 123, \"k\": 3\n        },\n        # Case 2\n        {\n            \"group_sizes\": [120], \"num_variants\": 400,\n            \"aim_blocks\": [],\n            \"neutral_af\": 0.3, \"seed\": 456, \"k\": 3\n        },\n        # Case 3\n        {\n            \"group_sizes\": [80, 80, 40], \"num_variants\": 600,\n            \"aim_blocks\": [\n                {'count': 30, 'freqs': [0.1, 0.9, 0.9]},\n                {'count': 30, 'freqs': [0.9, 0.1, 0.9]}\n            ],\n            \"neutral_af\": 0.3, \"seed\": 789, \"k\": 3\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = _solve_case(**params)\n        all_results.append(result)\n\n    # Format the final output string according to the specified format.\n    # \"[[res1_1,res1_2,...],[res2_1,res2_2,...],...]\"\n    # with each fraction rounded to four decimal places.\n    formatted_case_results = []\n    for res_list in all_results:\n        # Format each number to exactly four decimal places\n        formatted_list_str = [f\"{x:.4f}\" for x in res_list]\n        # Join numbers with commas and enclose in brackets\n        formatted_case_results.append(f\"[{','.join(formatted_list_str)}]\")\n    \n    # Join all case results and enclose in the final brackets\n    final_output_string = f\"[{','.join(formatted_case_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}