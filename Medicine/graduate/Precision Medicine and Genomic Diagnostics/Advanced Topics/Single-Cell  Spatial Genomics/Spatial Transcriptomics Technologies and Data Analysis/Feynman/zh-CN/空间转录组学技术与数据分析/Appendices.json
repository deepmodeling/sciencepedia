{
    "hands_on_practices": [
        {
            "introduction": "在分析空间转录组学数据时，一个基础性的挑战是原始基因表达计数受到测序深度等技术因素的强烈影响。因此，标准化的目的是消除这些技术差异，使得在不同空间位置上的基因表达水平具有可比性。本练习将指导您实现并比较几种关键的标准化方法，从简单的每百万计数（Counts Per Million, CPM）缩放到基于模型的sctransform，从而深入理解不同方法如何影响数据特征，例如基因表达的方差。",
            "id": "4385438",
            "problem": "给定代表空间转录组学测量的、按点计算的总计数和基因-点计数矩阵。设有 $G$ 个基因，索引为 $g \\in \\{1,\\dots,G\\}$，以及 $S$ 个空间点，索引为 $s \\in \\{1,\\dots,S\\}$。基因 $g$ 在点 $s$ 观测到的整数计数表示为 $x_{gs}$，点 $s$ 的总计数（文库大小）表示为 $c_s$。您必须计算三种归一化方法，并比较每种归一化方法下基因间的方差。\n\n基本基础和定义：\n- 在基于计数的转录组学中，观测到的计数是由潜在丰度和抽样深度驱动的非负整数。每个点的总计数 $c_s$ 作为暴露标量。计数数据的一个广泛接受的随机模型是负二项 (NB) 分布，其中 $x_{gs}$ 以均值 $\\mu_{gs}$ 和过度离散参数 $\\theta_g$ 建模，使得方差为 $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$。在具有对数连接函数的广义线性模型 (GLM) 中的偏移量公式设定 $\\log(\\mu_{gs}) = \\beta_{0g} + \\log(c_s)$，这意味着对于基因特异性比率 $p_g = e^{\\beta_{0g}}$，有 $\\mu_{gs} = c_s \\cdot p_g$。\n- 每百万计数 (CPM) 归一化将计数重新缩放到百万分之几的基础上，以调整文库大小。具体来说，对于每个点 $s$，基因 $g$ 的 CPM 归一化值为 $n_{gs}^{\\mathrm{CPM}} = 10^6 \\cdot x_{gs} / c_s$（如果 $c_s > 0$）和 $n_{gs}^{\\mathrm{CPM}} = 0$（如果 $c_s = 0$）。\n- 带有伪计数的自然对数变换，记为“log1p”，定义为 $y_{gs}^{\\log 1\\mathrm{p}} = \\log\\left(1 + n_{gs}^{\\mathrm{CPM}}\\right)$。\n- 单细胞变换 (sctransform) 残差源自以文库大小为偏移量的 NB-GLM。在 $\\mu_{gs} = c_s \\cdot p_g$ 和过度离散参数 $\\theta_g$ 的条件下，皮尔逊残差定义为\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}},  \\text{if } \\mu_{gs} > 0,\\\\\n0,  \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\n为了确定 $\\theta_g$，使用一个经过充分检验的事实：在正确指定的 NB 模型下，皮尔逊残差平方的期望值约等于 $1$。求解 $\\theta_g > 0$ 使得\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1,\n$$\n其中 $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$。如果即使在 $\\theta_g \\to \\infty$（泊松极限）时，也没有解能得到大于或等于 $1$ 的值，则将 $\\theta_g$ 设置为一个大值（例如，$\\theta_g = 10^8$）。如果任何点 $s$ 的 $c_s = 0$，则为该点定义 $n_{gs}^{\\mathrm{CPM}} = 0$ 和 $\\mu_{gs} = 0$。\n\n任务：\n1. 实现 CPM 归一化，计算所有基因和点的 $n_{gs}^{\\mathrm{CPM}}$。\n2. 对 CPM 值实现 log1p 变换，计算所有基因和点的 $y_{gs}^{\\log 1\\mathrm{p}}$。\n3. 实现带偏移量的 NB-GLM，计算 sctransform 皮尔逊残差 $r_{gs}$：\n   - 在偏移量模型下，通过最大似然估计 $p_g$ 为 $p_g = \\left(\\sum_{s=1}^{S} x_{gs}\\right)\\Big/\\left(\\sum_{s : c_s > 0} c_s\\right)$。\n   - 对所有点设置 $\\mu_{gs} = c_s \\cdot p_g$。\n   - 使用上述方程，通过对 $\\theta_g \\in (0, +\\infty)$ 进行稳健的一维求根方法（例如，在足够大的区间上使用二分法）求解 $\\theta_g$。\n   - 计算 $r_{gs}$。\n4. 对于每种归一化，计算每个基因在各点间的总体方差。对于任意向量 $z_{g\\cdot} = (z_{g1},\\dots,z_{gS})$，总体方差定义为\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\\right)^2.\n$$\n\n角度单位不适用。物理单位是计数；除了定义的 CPM 外，不需要进行单位转换。\n\n测试套件：\n- 测试用例 1：\n  - $S = 5$, $G = 4$,\n  - $c = [5000, 7000, 6000, 8000, 5500]$,\n  - $X = \\begin{bmatrix}\n  50  70  60  80  55 \\\\\n  200  150  180  220  170 \\\\\n  0  5  10  15  0 \\\\\n  1000  1400  1100  1600  1200\n  \\end{bmatrix}$.\n- 测试用例 2：\n  - $S = 3$, $G = 3$,\n  - $c = [0, 3000, 4000]$,\n  - $X = \\begin{bmatrix}\n  0  30  40 \\\\\n  0  300  400 \\\\\n  0  0  10\n  \\end{bmatrix}$.\n- 测试用例 3：\n  - $S = 2$, $G = 5$,\n  - $c = [10000, 15000]$,\n  - $X = \\begin{bmatrix}\n  0  0 \\\\\n  4000  6000 \\\\\n  100  300 \\\\\n  5000  7000 \\\\\n  10  10\n  \\end{bmatrix}$.\n\n输出规格：\n- 对于每个测试用例，生成一个包含三个列表的列表：\n  1. $n_{gs}^{\\mathrm{CPM}}$ 在各点间的逐基因总体方差列表（每个基因一个浮点数）。\n  2. $y_{gs}^{\\log 1\\mathrm{p}}$ 在各点间的逐基因总体方差列表（每个基因一个浮点数）。\n  3. $r_{gs}$ 在各点间的逐基因总体方差列表（每个基因一个浮点数）。\n- 您的程序应生成一行输出，其中包含所有测试用例的结果，形式为用方括号括起来的逗号分隔列表，每个浮点数四舍五入到六位小数。最终格式必须是：\n\"[[list_for_test_case_1],[list_for_test_case_2],[list_for_test_case_3]]\",\n其中每个 list_for_test_case_k 本身为 \"[[var_CPM_gene_1,...,var_CPM_gene_G],[var_log1p_gene_1,...,var_log1p_gene_G],[var_sctransform_gene_1,...,var_sctransform_gene_G]]\"。",
            "solution": "该问题要求实现并比较三种不同的空间转录组学计数数据归一化方法。这些方法是每百万计数 (CPM)、CPM 值的对数变换 (log1p) 以及来自负二项广义线性模型 (sctransform) 的皮尔逊残差。对于每种方法，都需要计算每个基因在空间点上的总体方差。\n\n该问题已经过验证，被认为是科学上合理、定义明确且客观的。它基于单细胞和空间转录组学中使用的既定统计和计算方法。所有必要的数据、模型和程序都已明确定义，从而可以得到唯一且可验证的解。\n\n将通过按规定实现每个归一化步骤，然后计算每个基因的总体方差来开发解决方案。\n\n设给定数据为基因-点计数矩阵 $X = \\{x_{gs}\\}$（对于 $g \\in \\{1,\\dots,G\\}$ 个基因和 $s \\in \\{1,\\dots,S\\}$ 个点）以及总点计数（文库大小）向量 $c = \\{c_s\\}$。\n\n**1. 归一化方法**\n\n**a. 每百万计数 (CPM) 归一化**\n\n该方法通过每个点的总计数 $c_s$ 对原始计数 $x_{gs}$ 进行归一化，并将结果乘以一百万。这调整了各点之间测序深度的差异。公式为：\n$$\nn_{gs}^{\\mathrm{CPM}} = \n\\begin{cases}\n\\dfrac{x_{gs}}{c_s} \\times 10^6,  \\text{if } c_s > 0, \\\\\n0,  \\text{if } c_s = 0.\n\\end{cases}\n$$\n得到的矩阵 $N^{\\mathrm{CPM}} = \\{n_{gs}^{\\mathrm{CPM}}\\}$ 表示了假设每个点的总计数都为一百万时的计数。\n\n**b. 对数变换 (log1p)**\n\n基因组学中的计数数据通常是高度偏斜的。应用对数变换有助于稳定方差，使数据分布更对称，这对于许多下游统计分析是有益的。在取自然对数之前加上一个伪计数 $1$，以避免零计数（即 $\\log(0)$）的问题。应用于 CPM 值的公式是：\n$$\ny_{gs}^{\\log 1\\mathrm{p}} = \\log(1 + n_{gs}^{\\mathrm{CPM}})\n$$\n得到的矩阵是 $Y^{\\log 1\\mathrm{p}} = \\{y_{gs}^{\\log 1\\mathrm{p}}\\}$。\n\n**c. Sctransform 皮尔逊残差**\n\n该方法基于更复杂的计数数据统计模型。它使用负二项 (NB) 分布对计数 $x_{gs}$ 进行建模，并将文库大小 $c_s$ 作为暴露项。均值 $\\mu_{gs}$ 被建模为与文库大小成正比：$\\mu_{gs} = p_g c_s$，其中 $p_g$ 是基因特异性的相对丰度参数。NB 分布的方差由 $\\operatorname{Var}(x_{gs}) = \\mu_{gs} + \\mu_{gs}^2 / \\theta_g$ 给出，其中 $\\theta_g$ 是基因特异性的过度离散参数。目标是计算皮尔逊残差，这些是标准化的值，在完美的模型拟合下，其均值应为 $0$，方差应为 $1$。\n\n对于每个基因 $g$，流程如下：\n\n**I. 估计比率参数 $p_g$：**\n参数 $p_g$ 使用最大似然估计，对于此模型，这简化为总基因计数与总有效文库大小的比率：\n$$\np_g = \\frac{\\sum_{s=1}^{S} x_{gs}}{\\sum_{s: c_s > 0} c_s}\n$$\n如果 $\\sum x_{gs} = 0$，则 $p_g=0$，所有期望值 $\\mu_{gs}$ 均为 $0$，所有残差均为 $0$，方差也为 $0$。\n\n**II. 估计过度离散参数 $\\theta_g$：**\n参数 $\\theta_g$ 控制方差超出均值的程度。它通过求解以下方程来估计，该方程将平均皮尔逊残差平方设为其期望值 $1$：\n$$\n\\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs} + \\mu_{gs}^2/\\theta_g} = 1\n$$\n其中 $\\mathcal{I}_g = \\{ s \\,|\\, \\mu_{gs} > 0 \\}$。使用数值求根算法求解该方程以获得 $\\theta_g > 0$。左侧是 $\\theta_g$ 的单调递增函数。我们首先在极限 $\\theta_g \\to \\infty$（泊松情况）下评估该函数：\n$$\nL_{\\infty} = \\frac{1}{|\\mathcal{I}_g|}\\sum_{s \\in \\mathcal{I}_g} \\frac{\\left(x_{gs} - \\mu_{gs}\\right)^2}{\\mu_{gs}}\n$$\n如果 $L_{\\infty} < 1$，则数据相对于泊松模型是欠离散的，不存在正的有限 $\\theta_g$ 解。在这种情况下，如规定，我们将 $\\theta_g$ 设置为一个大值，$\\theta_g = 10^8$。否则，存在唯一的根 $\\theta_g$，可以使用一维求解器（如布伦特方法）在合适的区间（例如 $(0, 10^{12}]$）上找到。\n\n**III. 计算皮尔逊残差 $r_{gs}$：**\n有了 $p_g$（以及因此的 $\\mu_{gs}$）和 $\\theta_g$ 的估计值，皮尔逊残差计算如下：\n$$\nr_{gs} = \n\\begin{cases}\n\\dfrac{x_{gs} - \\mu_{gs}}{\\sqrt{\\mu_{gs} + \\mu_{gs}^2/\\theta_g}},  \\text{if } \\mu_{gs} > 0, \\\\\n0,  \\text{if } \\mu_{gs} = 0.\n\\end{cases}\n$$\n残差矩阵为 $R = \\{r_{gs}\\}$。\n\n**2. 方差计算**\n\n对于每个基因 $g$ 和三个结果数据矩阵（$N^{\\mathrm{CPM}}$、$Y^{\\log 1\\mathrm{p}}$ 和 $R$）中的每一个，计算在 $S$ 个点上的总体方差。对于基因 $g$ 的变换值的一般向量 $z_{g\\cdot} = (z_{g1}, \\dots, z_{gS})$，总体方差为：\n$$\n\\operatorname{Var}_{\\text{pop}}(z_{g\\cdot}) = \\frac{1}{S} \\sum_{s=1}^{S} \\left(z_{gs} - \\bar{z}_g\\right)^2, \\quad \\text{其中 } \\bar{z}_g = \\frac{1}{S}\\sum_{t=1}^{S} z_{gt}\n$$\n此计算将对每个基因执行，为每个测试用例产生三个方差列表，对应于三种归一化方法。\n\n该实现将处理每个测试用例，将这些步骤应用于每个基因，并根据指定的输出格式格式化生成的方差列表。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\nimport collections\n\n# The problem specifies numpy and scipy as permitted libraries.\n# The phrase \"No other libraries outside the Python standard library are permitted\"\n# is interpreted as excluding libraries *not* on the supplied list.\n\ndef sctransform_gene(x_g, c):\n    \"\"\"\n    Computes sctransform residuals and their variance for a single gene.\n    \n    Args:\n        x_g (np.ndarray): Count vector for one gene (shape S).\n        c (np.ndarray): Library size vector (shape S).\n\n    Returns:\n        float: The population variance of the sctransform residuals.\n    \"\"\"\n    S = len(c)\n    \n    # If gene is not expressed at all, residuals are all zero, variance is zero.\n    if np.sum(x_g) == 0:\n        return 0.0\n\n    # Identify effective spots (where library size > 0)\n    c_mask = c > 0\n    c_eff = c[c_mask]\n    \n    # If no spots have counts, variance is zero.\n    if len(c_eff) == 0:\n        return 0.0\n        \n    x_eff = x_g[c_mask]\n    sum_c_eff = np.sum(c_eff)\n\n    # Estimate gene-specific rate parameter p_g\n    p_g = np.sum(x_g) / sum_c_eff\n\n    # Compute expected counts mu_gs\n    mu = c * p_g\n\n    # Filter for spots where mu > 0 for theta estimation (I_g)\n    mu_eff = mu[c_mask]\n    \n    # Numerically stabilize division by mu_eff in case of underflow\n    mu_eff_reg = mu_eff + 1e-12\n    \n    # Check for under-dispersion by evaluating the limit as theta -> infinity\n    # which corresponds to the Poisson model.\n    limit_term = np.sum((x_eff - mu_eff)**2 / mu_eff_reg)\n    \n    # Average squared Pearson residual for the Poisson model\n    limit_lhs = limit_term / len(c_eff)\n\n    if limit_lhs  1:\n        # Under-dispersed case: no solution for theta > 0. Set to a large value.\n        theta_g = 1e8\n    else:\n        # Over-dispersed case: find theta using root-finding.\n        def objective(theta):\n            # Objective function to find the root for theta\n            # Target: average squared Pearson residual = 1\n            if theta = 0: return -np.inf\n            var_nb = mu_eff + mu_eff**2 / theta\n            # Prevent division by zero if var_nb is somehow zero\n            var_nb[var_nb == 0] = 1e-12 \n            sum_sq_pearson = np.sum((x_eff - mu_eff)**2 / var_nb)\n            return sum_sq_pearson / len(c_eff) - 1\n\n        try:\n            # Brent's method is a robust and efficient root-finding algorithm.\n            # A large interval for theta is used.\n            theta_g = brentq(objective, a=1e-8, b=1e12, xtol=1e-6, rtol=1e-6)\n        except ValueError:\n            # If brentq fails (e.g., endpoints have same sign unexpectedly),\n            # fall back to the safe high-theta value.\n            theta_g = 1e8\n\n    # Compute Pearson residuals\n    residuals = np.zeros(S, dtype=float)\n    mu_pos_mask = mu > 0\n    \n    if np.any(mu_pos_mask):\n        var_nb = mu[mu_pos_mask] + mu[mu_pos_mask]**2 / theta_g\n        # Prevent sqrt of zero or negative values due to floating point inaccuracies\n        var_nb[var_nb = 0] = 1e-12\n        residuals[mu_pos_mask] = (x_g[mu_pos_mask] - mu[mu_pos_mask]) / np.sqrt(var_nb)\n\n    return np.var(residuals)\n\ndef process_case(X, c):\n    \"\"\"\n    Processes one test case: computes variances for all three normalizations.\n    \n    Args:\n        X (list of lists): GxS matrix of gene counts.\n        c (list): Vector of S library sizes.\n\n    Returns:\n        tuple: Three lists of floats (cpm_vars, log1p_vars, sct_vars).\n    \"\"\"\n    X = np.array(X, dtype=float)\n    c = np.array(c, dtype=float)\n    G, S = X.shape\n\n    # --- CPM and log1p Normalizations ---\n    # Create a regularized c vector to avoid division by zero, then correct.\n    c_reg = np.copy(c)\n    zero_c_mask = c == 0\n    c_reg[zero_c_mask] = 1.0 \n    \n    cpm = 1e6 * X / c_reg[:, np.newaxis].T\n    cpm[:, zero_c_mask] = 0.0 # Enforce 0 for spots with 0 library size\n    \n    log1p_cpm = np.log1p(cpm)\n\n    # Calculate population variance (ddof=0 is default) per gene (axis=1)\n    cpm_vars = list(np.var(cpm, axis=1))\n    log1p_vars = list(np.var(log1p_cpm, axis=1))\n    \n    # --- sctransform Normalization ---\n    sct_vars = []\n    for g in range(G):\n        var = sctransform_gene(X[g, :], c)\n        sct_vars.append(var)\n        \n    return cpm_vars, log1p_vars, sct_vars\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"c\": [5000, 7000, 6000, 8000, 5500],\n            \"X\": [\n                [50, 70, 60, 80, 55],\n                [200, 150, 180, 220, 170],\n                [0, 5, 10, 15, 0],\n                [1000, 1400, 1100, 1600, 1200]\n            ]\n        },\n        {\n            \"c\": [0, 3000, 4000],\n            \"X\": [\n                [0, 30, 40],\n                [0, 300, 400],\n                [0, 0, 10]\n            ]\n        },\n        {\n            \"c\": [10000, 15000],\n            \"X\": [\n                [0, 0],\n                [4000, 6000],\n                [100, 300],\n                [5000, 7000],\n                [10, 10]\n            ]\n        }\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        cpm_vars, log1p_vars, sct_vars = process_case(case[\"X\"], case[\"c\"])\n        \n        cpm_str = f\"[{','.join(f'{v:.6f}' for v in cpm_vars)}]\"\n        log1p_str = f\"[{','.join(f'{v:.6f}' for v in log1p_vars)}]\"\n        sct_str = f\"[{','.join(f'{v:.6f}' for v in sct_vars)}]\"\n        \n        case_result_str = f\"[{cpm_str},{log1p_str},{sct_str}]\"\n        all_results_str.append(case_result_str)\n\n    # Format the final output string exactly as specified.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "空间转录组学的核心在于其空间维度信息，而利用这些信息的关键一步是构建一个能反映组织空间结构的网络图。$k$-最近邻（$k$-Nearest Neighbors, $k$-NN）图是一种常用方法，它通过连接空间上邻近的细胞或区域来定义局部邻域。本练习将让您亲手实践如何根据空间坐标构建$k$-NN图，并探索关键参数$k$的选择如何影响图的拓扑结构，这是进行空间聚类、数据平滑和细胞通讯分析等高级任务的先决条件。",
            "id": "4385418",
            "problem": "您正在分析空间转录组学的空间点布局，以构建用于下游任务（如空间平滑和区域分割）的邻域图。给定一个有限的二维点坐标集和目标平均度，您必须确定一个整数邻域参数 $k$，用于构建一个$k$-最近邻（k-NN）图。当有向的$k$-最近邻图通过无向并集对称化后，该图的平均度应与目标平均度最佳匹配。然后，您必须验证生成的度分布的良定图论属性。\n\n使用以下基本基础：\n- 距离是$\\mathbb{R}^2$中的欧几里得距离：对于位置分别为$\\mathbf{x}_i$和$\\mathbf{x}_j$的点$i$和$j$，其距离为 $d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2 = \\sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2}$。\n- 如果节点$j$在所有$j \\neq i$的$d(i,j)$值中属于最小的$k$个之一，则存在从节点$i$到节点$j$的有向$k$-最近邻（k-NN）关系。\n- 要从有向$k$-最近邻图获得一个无向图，请使用并集对称化：当且仅当$j$在$i$的$k$个最近邻居列表中，或者$i$在$j$的$k$个最近邻居列表中时，$i$和$j$之间存在一条无向边。\n- 在无向简单图中，节点的度是与其关联的边的数量。根据握手引理，$\\sum_{i=1}^{N} \\deg(i) = 2|E|$。\n\n距离相等情况下邻居选择的确定性要求：\n- 当距离相等时，优先选择较小的节点索引来打破平局。也就是说，为了给节点$i$选择$k$个邻居，需要将候选者$j \\neq i$按序对$(d(i,j), j)$进行字典升序排序，并取前$k$个。\n\n任务：\n- 对于每个测试用例，给定$N$个点的坐标和目标平均度$\\bar{d}_{\\text{target}}$，在整数$k \\in \\{1, 2, \\dots, N-1\\}$的范围内进行搜索，以构建通过并集对称化的无向$k$-最近邻图，并计算其平均度$\\bar{d}_k$。选择使$|\\bar{d}_k - \\bar{d}_{\\text{target}}|$最小化的$k^\\star$。如果多个$k$值达到相同的最小绝对偏差，则选择其中最小的$k$。\n- 验证生成的度分布的两个属性：\n  1. 精确的握手一致性：$\\left(\\sum_{i=1}^{N} \\deg(i)\\right) = 2|E|$。\n  2. 与目标的接近程度：$|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$，其中容差$\\theta$固定为$\\theta = 0.25$。\n\n对于每个测试用例，以列表$[k^\\star, \\bar{d}_{k^\\star}, \\text{handshake\\_ok}, \\text{within\\_tolerance}]$的形式生成结果，其中$k^\\star$是一个整数，$\\bar{d}_{k^\\star}$是一个实数（浮点数），两个验证标志是布尔值。\n\n测试套件：\n- 案例 A（线性阵列，理想路径）：\n  - 坐标：$(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(3.0, 0.0)$, $(4.0, 0.0)$, $(5.0, 0.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 2.0$。\n- 案例 B（正方形，边界接近完备）：\n  - 坐标：$(0.0, 0.0)$, $(1.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 3.0$。\n- 案例 C（网格，内部结构）：\n  - 坐标：$(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(0.0, 1.0)$, $(1.0, 1.0)$, $(2.0, 1.0)$, $(0.0, 2.0)$, $(1.0, 2.0)$, $(2.0, 2.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 4.0$。\n- 案例 D（重复点，处理等值情况的边界案例）：\n  - 坐标：$(0.0, 0.0)$, $(0.0, 0.0)$, $(1.0, 0.0)$, $(2.0, 0.0)$, $(2.0, 0.0)$。\n  - 目标平均度：$\\bar{d}_{\\text{target}} = 2.0$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例的结果本身也是一个列表，格式为$[k^\\star,\\bar{d}_{k^\\star},\\text{handshake\\_ok},\\text{within\\_tolerance}]$。例如：$[[1,2.0,True,True],[\\dots]]$。\n\n此任务不涉及物理单位或角度。所有计算都是无单位的。容差$\\theta = 0.25$统一应用于所有案例。图是无权和简单的（无自环，无重边）。",
            "solution": "用户提供了一个定义明确的计算问题，该问题位于空间转录组学数据分析领域。任务是确定一个最佳邻域参数$k$，用于构建一个能最好地逼近目标平均度的$k$-最近邻（k-NN）图，并验证生成图的某些属性。\n\n该问题按如下方式进行验证：\n-   科学依据：该问题基于计算几何和图论中的既定方法（欧几里得距离、k-NN图、图的度、握手引理），并应用于空间基因组学中的一个相关问题。所有前提在科学和数学上都是合理的。\n-   良定性：问题明确无歧义。输入被清晰地指定。构建图的过程，包括一个确定性的等值处理规则，都有明确的定义。选择最佳参数$k^\\star$的优化标准也是明确的（最小化与目标的绝对偏差，并用最小的$k$作为等值处理方式），确保了唯一解的存在。\n-   客观性：问题陈述是形式化和定量的，不含主观性语言。\n-   完整性：问题提供了所有必要的信息，包括测试用例的坐标、每个用例的目标平均度以及容差参数$\\theta$的值。\n\n该问题被认为是有效的，可以制定解决方案。\n\n解决方案通过实现指定的图构建和搜索算法来展开。对于每个测试用例，给定$\\mathbb{R}^2$中的一组$N$个点坐标$\\{\\mathbf{x}_i\\}_{i=1}^N$和目标平均度$\\bar{d}_{\\text{target}}$，算法会遍历邻域参数$k \\in \\{1, 2, \\dots, N-1\\}$的所有可能整数值。\n\n对于每个$k$值，我们构建一个图$G_k = (V, E_k)$，其中$V$是$N$个点的集合。构建过程包括两个主要步骤：\n1.  有向k-NN图构建：对每个点$i$，我们确定其$k$个最近邻的集合，记为$\\text{NN}_k(i)$。这是通过计算点$i$到所有其他点$j \\neq i$的欧几里得距离$d(i,j) = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2$来实现的。然后，候选点$j$根据字典序对$(d(i,j), j)$进行升序排序。此排序列表中的前$k$个点构成了$\\text{NN}_k(i)$。此步骤为所有$j \\in \\text{NN}_k(i)$定义了一组有向边$(i, j)$。\n\n2.  通过并集对称化：当且仅当点$j$是点$i$的$k$-最近邻，或者点$i$是点$j$的$k$-最近邻时，一条无向边$\\{i, j\\}$才被包含在边集$E_k$中。形式上，$E_k = \\{\\{i, j\\} \\mid j \\in \\text{NN}_k(i) \\lor i \\in \\text{NN}_k(j)\\}$。这种构造保证了图是简单的无向图，因为自环是不允许的，并且基于集合的$E_k$定义隐式地处理了重复边。\n\n一旦图$G_k$构建完成，我们计算其属性。节点$i$的度$\\deg_k(i)$是与该节点关联的边的数量。图的平均度则为$\\bar{d}_k = \\frac{1}{N} \\sum_{i=1}^{N} \\deg_k(i)$。\n\n在为所有可能的$k$值计算出$\\bar{d}_k$之后，我们选择能最小化绝对偏差$|\\bar{d}_k - \\bar{d}_{\\text{target}}|$的最优参数$k^\\star$。如果出现平局，即多个$k$值产生相同的最小偏差，则根据问题规范选择这些$k$值中最小的一个。\n\n最后，对最优图$G_{k^\\star}$执行两个验证检查：\n1.  握手一致性：我们验证度的总和等于边数的两倍：$\\sum_{i=1}^{N} \\deg_{k^\\star}(i) = 2|E_{k^\\star}|$。这是任何无向图的基本属性（握手引理），可作为对图数据结构实现正确性的健全性检查。度的总和必须是一个偶数。\n2.  与目标的接近程度：我们检查所达到的平均度$\\bar{d}_{k^\\star}$是否在目标值的指定容差$\\theta=0.25$范围内：$|\\bar{d}_{k^\\star} - \\bar{d}_{\\text{target}}| \\le \\theta$。\n\n该算法使用Python实现，并利用`numpy`库来高效地进行成对距离矩阵的向量化计算。核心逻辑被封装在一个处理单个测试用例的函数中。一个主脚本定义了测试套件并遍历每个用例，收集结果并将其格式化为指定的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A (linear array, happy path)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [2.0, 0.0], \n                [3.0, 0.0], [4.0, 0.0], [5.0, 0.0]\n            ]),\n            \"target_avg_degree\": 2.0\n        },\n        {\n            \"name\": \"Case B (square, boundary near completeness)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]\n            ]),\n            \"target_avg_degree\": 3.0\n        },\n        {\n            \"name\": \"Case C (grid, interior structure)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n                [0.0, 1.0], [1.0, 1.0], [2.0, 1.0],\n                [0.0, 2.0], [1.0, 2.0], [2.0, 2.0]\n            ]),\n            \"target_avg_degree\": 4.0\n        },\n        {\n            \"name\": \"Case D (duplicates, tie-handling edge case)\",\n            \"coords\": np.array([\n                [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], \n                [2.0, 0.0], [2.0, 0.0]\n            ]),\n            \"target_avg_degree\": 2.0\n        }\n    ]\n\n    theta = 0.25\n    results = []\n    \n    for case in test_cases:\n        coords = case[\"coords\"]\n        target_avg_degree = case[\"target_avg_degree\"]\n        result = analyze_graph_topology(coords, target_avg_degree, theta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\ndef analyze_graph_topology(coords, target_avg_degree, theta):\n    \"\"\"\n    For a given set of coordinates and target average degree, finds the optimal k\n    and performs the required verifications.\n    \"\"\"\n    n_spots = coords.shape[0]\n    \n    # Pre-calculate all pairwise Euclidean distances efficiently.\n    # coords[:, np.newaxis, :] gives shape (N, 1, 2)\n    # coords[np.newaxis, :, :] gives shape (1, N, 2)\n    # Broadcasting computes the difference for all pairs.\n    diffs = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n    dist_matrix = np.sqrt(np.sum(diffs**2, axis=-1))\n    \n    k_results = []\n\n    # Search over all possible k values from 1 to N-1\n    for k in range(1, n_spots):\n        # Adjacency list representation using sets for automatic duplicate edge handling\n        adj = [set() for _ in range(n_spots)]\n        \n        # Determine directed k-NN and build undirected graph via union symmetrization\n        for i in range(n_spots):\n            # Form a list of (distance, index) tuples for sorting\n            neighbors_with_dist = []\n            for j in range(n_spots):\n                if i == j:\n                    continue\n                neighbors_with_dist.append((dist_matrix[i, j], j))\n            \n            # Sort by distance, then by index for tie-breaking\n            neighbors_with_dist.sort()\n            \n            # Extract the k-nearest neighbors\n            k_nearest_indices = [neighbor[1] for neighbor in neighbors_with_dist[:k]]\n            \n            # Add undirected edges to the graph\n            for neighbor_idx in k_nearest_indices:\n                adj[i].add(neighbor_idx)\n                adj[neighbor_idx].add(i)\n        \n        # Calculate degrees and average degree for this k\n        degrees = [len(s) for s in adj]\n        sum_of_degrees = sum(degrees)\n        avg_degree = sum_of_degrees / n_spots\n        \n        # Store results for this k\n        deviation = abs(avg_degree - target_avg_degree)\n        k_results.append({\n            'k': k, 'avg_degree': avg_degree, \n            'deviation': deviation, 'sum_of_degrees': sum_of_degrees\n        })\n\n    # Select the best k based on minimum deviation, with k as a tie-breaker\n    best_result = sorted(k_results, key=lambda x: (x['deviation'], x['k']))[0]\n    \n    k_star = best_result['k']\n    d_k_star = best_result['avg_degree']\n    \n    # Perform the final verification steps\n    # 1. Handshake consistency: sum of degrees must be an even integer.\n    sum_of_degrees_star = best_result['sum_of_degrees']\n    # sum_of_degrees is an integer by construction. The handshake lemma implies it must\n    # be even for any undirected graph. This check verifies implementation correctness.\n    handshake_ok = (sum_of_degrees_star % 2 == 0)\n    \n    # 2. Proximity to target: absolute deviation must be within tolerance.\n    within_tolerance = (best_result['deviation'] = theta)\n    \n    return [k_star, d_k_star, handshake_ok, within_tolerance]\n\nsolve()\n```"
        },
        {
            "introduction": "在获得了标准化的空间表达数据后，一个更深层次的科学问题是量化我们观察到的表达变异的来源：它究竟是源于真实的生物学差异，还是实验过程中的技术噪音？本练习将引导您应用一个分层随机效应模型，这是一个强大的统计工具，用以将总方差分解为不同层级的组分，例如患者间的生物学变异 $\\sigma_b^2$ 和样本间的技术变异 $\\sigma_t^2$。通过计算组内相关系数（Intraclass Correlation Coefficient, ICC），您将学会如何定量评估实验的可重复性以及不同变异来源的相对重要性。",
            "id": "4385420",
            "problem": "考虑一个空间转录组学实验，其中每个患者有重复的切片。设 $y_{p,s,i}$ 表示在患者 $p$ 的切片 $s$ 上的斑点 $i$ 处测量的单个基因的对数归一化表达量。假设一个分层正态随机效应模型，用于一个具有 $P$ 个患者、每个患者 $S$ 个切片、每个切片 $I$ 个斑点的平衡设计，其中\n$$\ny_{p,s,i} = \\mu + b_p + t_{p,s} + e_{p,s,i},\n$$\n其中，$b_p \\sim \\mathcal{N}(0,\\sigma_b^2)$ 代表患者间的生物学变异，$t_{p,s} \\sim \\mathcal{N}(0,\\sigma_t^2)$ 代表同一患者内重复切片间的技术变异，而 $e_{p,s,i} \\sim \\mathcal{N}(0,\\sigma_e^2)$ 代表同一切片内斑点间的残差测量变异。所有随机效应相互独立，且设计是平衡的，因此每个患者恰好有 $S$ 个切片，每个切片恰好有 $I$ 个斑点。\n\n仅从随机效应的独立性、独立项求和下的方差可加性以及样本均值和无偏样本方差的定义出发，推导出一个矩估计法程序来估计方差分量 $\\sigma_b^2$、$\\sigma_t^2$ 和 $\\sigma_e^2$。使用这些方差分量估计值计算两个组内相关系数 (ICC)：患者水平 ICC（针对斑点水平测量值），定义为从同一患者（但可能来自不同切片）随机抽取的两个斑点水平测量值之间的相关性；以及切片水平 ICC（针对斑点水平测量值），定义为从同一切片随机抽取的两个斑点水平测量值之间的相关性。ICC 必须以小数形式表示。\n\n科学真实性约束：\n- 方差分量 $\\sigma_b^2$、$\\sigma_t^2$ 和 $\\sigma_e^2$ 是非负的；如果任何矩估计法估计值因抽样波动而为负，则在计算 ICC 之前将其设置为 $0$。\n- 患者水平的 ICC 必须反映可归因于同一患者内观测值共享的随机效应的总斑点水平方差的比例，而切片水平的 ICC 必须反映可归因于同一切片内观测值共享的随机效应的总斑点水平方差的比例。\n\n测试套件（每个案例都是一个平衡的嵌套数据集，其中 $P=2$，$S=2$，$I=3$；数值为对数归一化的任意单位）：\n- 案例 1（一般情况，中等生物学变异，中度残差变异，可忽略的切片变异）：\n  - 患者 1，切片 1：$\\{2.00, 2.20, 1.80\\}$\n  - 患者 1，切片 2：$\\{2.10, 2.30, 1.90\\}$\n  - 患者 2，切片 1：$\\{3.00, 3.10, 2.90\\}$\n  - 患者 2，切片 2：$\\{3.20, 3.00, 3.10\\}$\n- 案例 2（边缘情况，技术变异占主导，低生物学变异，中度残差变异）：\n  - 患者 1，切片 1：$\\{2.10, 2.00, 2.20\\}$\n  - 患者 1，切片 2：$\\{3.10, 3.00, 3.20\\}$\n  - 患者 2，切片 1：$\\{2.05, 2.15, 2.10\\}$\n  - 患者 2，切片 2：$\\{3.05, 2.95, 3.15\\}$\n- 案例 3（边界情况，高生物学变异，可忽略的技术和残差变异）：\n  - 患者 1，切片 1：$\\{1.00, 1.00, 1.00\\}$\n  - 患者 1，切片 2：$\\{1.00, 1.00, 1.00\\}$\n  - 患者 2，切片 1：$\\{4.00, 4.00, 4.00\\}$\n  - 患者 2，切片 2：$\\{4.00, 4.00, 4.00\\}$\n\n每个测试案例的所需输出：\n- 一个包含两个浮点数的列表：$[\\text{ICC}_\\text{patient}, \\text{ICC}_\\text{section}]$，其中两个浮点数都四舍五入到小数点后 $4$ 位。\n\n您的程序应生成单行输出，其中包含所有三个案例的结果，格式为一个用方括号括起来的逗号分隔列表，其中每个元素是上述的单个案例列表（例如，`[[a_1,b_1],[a_2,b_2],[a_3,b_3]]`，但使用实际数值）。",
            "solution": "该问题要求推导一个矩估计法程序，以估计嵌套随机效应模型中的方差分量，并随后计算两种类型的组内相关系数 (ICC)。\n\n### 第 1 步：问题验证\n\n**1.1. 提取已知条件**\n\n- **模型：** $y_{p,s,i} = \\mu + b_p + t_{p,s} + e_{p,s,i}$，适用于患者 $p \\in \\{1, \\dots, P\\}$，切片 $s \\in \\{1, \\dots, S\\}$，斑点 $i \\in \\{1, \\dots, I\\}$。\n- **随机效应：**\n    - 患者效应：$b_p \\sim \\mathcal{N}(0, \\sigma_b^2)$。\n    - 患者内切片效应：$t_{p,s} \\sim \\mathcal{N}(0, \\sigma_t^2)$。\n    - 残差误差：$e_{p,s,i} \\sim \\mathcal{N}(0, \\sigma_e^2)$。\n- **假设：** 所有随机效应相互独立。设计是平衡的。\n- **定义：**\n    - 患者水平ICC：从同一患者随机抽取的两个斑点水平测量值之间的相关性。\n    - 切片水平ICC：从同一切片随机抽取的两个斑点水平测量值之间的相关性。\n- **约束条件：**\n    - 方差分量估计值必须为非负。如果估计值为负，则将其设置为 $0$。\n    - ICC 定义必须反映可归因于共享随机效应的总方差比例。\n- **测试数据：** 提供了三个测试案例，其中 $P=2$, $S=2$, $I=3$。\n- **所需输出：** 对每个案例，一个包含两个浮点数的列表 $[\\text{ICC}_\\text{patient}, \\text{ICC}_\\text{section}]$，四舍五入到小数点后 $4$ 位。\n\n**1.2. 使用提取的已知条件进行验证**\n\n根据验证标准对问题进行评估：\n\n- **科学依据：** 分层线性模型（或嵌套随机效应模型）是生物科学（包括基因组学）中用于分析结构化实验中变异来源的标准和基础统计工具。矩估计法是一种经典的参数估计统计技术。该问题在科学上是合理的。\n- **适定性：** 问题被明确地规定。平衡的设计和定义的模型结构确保了矩估计法估计量是适定的。截断负方差估计值的指令解决了估计过程中的潜在模糊性，从而得到唯一解。\n- **客观性：** 问题陈述是形式化和数学化的，使用了精确且无歧义的术语。\n\n该问题没有表现出任何诸如科学不合理、非形式化、不完整、矛盾或不切实际的条件等缺陷。这是一个应用于相关生物学背景的标准、可解的数理统计问题。\n\n**1.3. 结论与行动**\n\n该问题是**有效的**。将提供一个完整、合理的解决方案。\n\n### 第 2 步：估计量和 ICC 的推导\n\n解决方案分为两个主要部分：首先，推导方差分量 $\\sigma_b^2$、$\\sigma_t^2$ 和 $\\sigma_e^2$ 的矩估计法估计量；其次，基于这些分量推导 ICC 的公式。\n\n**2.1. 方差分量的矩估计法估计量**\n\n我们使用方差分析 (ANOVA) 框架，对于此模型，它等同于矩估计法。我们定义与层次结构中每个级别相对应的平方和 (SS)，并求出它们的期望值。\n\n模型为 $y_{p,s,i} = \\mu + b_p + t_{p,s} + e_{p,s,i}$。让我们定义样本均值：\n- 切片均值：$\\bar{y}_{p,s,.} = \\frac{1}{I} \\sum_{i=1}^{I} y_{p,s,i}$\n- 患者均值：$\\bar{y}_{p,.,.} = \\frac{1}{S} \\sum_{s=1}^{S} \\bar{y}_{p,s,.} = \\frac{1}{SI} \\sum_{s=1}^{S} \\sum_{i=1}^{I} y_{p,s,i}$\n- 总均值：$\\bar{y}_{.,.,.} = \\frac{1}{P} \\sum_{p=1}^{P} \\bar{y}_{p,.,.} = \\frac{1}{PSI} \\sum_{p=1}^{P} \\sum_{s=1}^{S} \\sum_{i=1}^{I} y_{p,s,i}$\n\n总平方和可以分解为三个分量：\n\n1.  **切片内平方和（或误差平方和，SSE）：** 它衡量同一切片内斑点的变异性。\n    $$SSE = \\sum_{p=1}^{P} \\sum_{s=1}^{S} \\sum_{i=1}^{I} (y_{p,s,i} - \\bar{y}_{p,s,.})^2$$\n    相应的**均方误差 (MSE)** 为 $MSE = \\frac{SSE}{P S (I-1)}$。其期望值为：\n    $$E[MSE] = \\sigma_e^2$$\n\n2.  **患者内切片平方和 (SST)：** 它衡量同一患者内切片均值的变异性。\n    $$SST = I \\sum_{p=1}^{P} \\sum_{s=1}^{S} (\\bar{y}_{p,s,.} - \\bar{y}_{p,.,.})^2$$\n    相应的**切片均方 (MST)** 为 $MST = \\frac{SST}{P (S-1)}$。其期望值为：\n    $$E[MST] = \\sigma_e^2 + I \\sigma_t^2$$\n\n3.  **患者间平方和 (SSB)：** 它衡量患者均值围绕总均值的变异性。\n    $$SSB = SI \\sum_{p=1}^{P} (\\bar{y}_{p,.,.} - \\bar{y}_{.,.,.})^2$$\n    相应的**患者间均方 (MSB)** 为 $MSB = \\frac{SSB}{P-1}$。其期望值为：\n    $$E[MSB] = \\sigma_e^2 + I \\sigma_t^2 + SI \\sigma_b^2$$\n\n通过将观察到的均方与其期望值相等，我们得到一个方程组：\n1.  $MSE = \\hat{\\sigma}_e^2$\n2.  $MST = \\hat{\\sigma}_e^2 + I \\hat{\\sigma}_t^2$\n3.  $MSB = \\hat{\\sigma}_e^2 + I \\hat{\\sigma}_t^2 + SI \\hat{\\sigma}_b^2$\n\n解此方程组，得到方差分量估计量 ($\\hat{\\sigma}^2$)：\n$$ \\hat{\\sigma}_e^2 = MSE $$\n$$ \\hat{\\sigma}_t^2 = \\frac{MST - MSE}{I} $$\n$$ \\hat{\\sigma}_b^2 = \\frac{MSB - MST}{SI} $$\n\n根据问题的约束，任何负的估计值都被截断为零。设最终的非负估计值用上标星号表示：\n$$ \\hat{\\sigma}_e^{2*} = MSE \\quad (\\text{注意：} MSE \\ge 0) $$\n$$ \\hat{\\sigma}_t^{2*} = \\max\\left(0, \\frac{MST - MSE}{I}\\right) $$\n$$ \\hat{\\sigma}_b^{2*} = \\max\\left(0, \\frac{MSB - MST}{SI}\\right) $$\n\n**2.2. 组内相关系数 (ICC)**\n\nICC 定义为两次测量之间的相关性，对于一个平稳过程，这可以简化为两次测量之间的协方差与单次测量的总方差之比。\n单个观测值 $y_{p,s,i}$ 的总方差为：\n$$ \\text{Var}(y_{p,s,i}) = \\text{Var}(\\mu + b_p + t_{p,s} + e_{p,s,i}) = \\text{Var}(b_p) + \\text{Var}(t_{p,s}) + \\text{Var}(e_{p,s,i}) = \\sigma_b^2 + \\sigma_t^2 + \\sigma_e^2 $$\n\n1.  **患者水平 ICC ($\\text{ICC}_\\text{patient}$):** 这是来自同一患者 $p$ 但可能来自不同切片和斑点的两个观测值 $y_{p,s,i}$ 和 $y_{p,s',i'}$ 之间的相关性。\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s',i'}) = \\text{Cov}(\\mu + b_p + t_{p,s} + e_{p,s,i}, \\mu + b_p + t_{p,s'} + e_{p,s',i'}) $$\n    由于所有随机效应都是独立的，因此只有共享效应的协方差才非零。唯一保证共享的效应是患者效应 $b_p$。\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s',i'}) = \\text{Var}(b_p) = \\sigma_b^2 $$\n    因此，患者水平的 ICC 是：\n    $$ \\text{ICC}_\\text{patient} = \\frac{\\text{Cov}(y_{p,s,i}, y_{p,s',i'})}{\\text{Var}(y_{p,s,i})} = \\frac{\\sigma_b^2}{\\sigma_b^2 + \\sigma_t^2 + \\sigma_e^2} $$\n\n2.  **切片水平 ICC ($\\text{ICC}_\\text{section}$):** 这是来自同一切片 $(p,s)$ 但不同斑点 ($i \\neq i'$) 的两个观测值 $y_{p,s,i}$ 和 $y_{p,s,i'}$ 之间的相关性。\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s,i'}) = \\text{Cov}(\\mu + b_p + t_{p,s} + e_{p,s,i}, \\mu + b_p + t_{p,s} + e_{p,s,i'}) $$\n    共享的随机效应是患者效应 $b_p$ 和切片效应 $t_{p,s}$。\n    $$ \\text{Cov}(y_{p,s,i}, y_{p,s,i'}) = \\text{Var}(b_p) + \\text{Var}(t_{p,s}) = \\sigma_b^2 + \\sigma_t^2 $$\n    因此，切片水平的 ICC 是：\n    $$ \\text{ICC}_\\text{section} = \\frac{\\text{Cov}(y_{p,s,i}, y_{p,s,i'})}{\\text{Var}(y_{p,s,i})} = \\frac{\\sigma_b^2 + \\sigma_t^2}{\\sigma_b^2 + \\sigma_t^2 + \\sigma_e^2} $$\n\n使用截断后的估计值，ICC 估计量为：\n$$ \\widehat{\\text{ICC}}_\\text{patient} = \\frac{\\hat{\\sigma}_b^{2*}}{\\hat{\\sigma}_b^{2*} + \\hat{\\sigma}_t^{2*} + \\hat{\\sigma}_e^{2*}} $$\n$$ \\widehat{\\text{ICC}}_\\text{section} = \\frac{\\hat{\\sigma}_b^{2*} + \\hat{\\sigma}_t^{2*}}{\\hat{\\sigma}_b^{2*} + \\hat{\\sigma}_t^{2*} + \\hat{\\sigma}_e^{2*}} $$\n如果总估计方差为零，则所有分量方差也必须为零，并且 ICC 取为零。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_iccs(data, P, S, I):\n    \"\"\"\n    Calculates variance components and ICCs for a balanced nested design.\n\n    Args:\n        data (np.ndarray): A PxSxI array of measurements.\n        P (int): Number of patients.\n        S (int): Number of sections per patient.\n        I (int): Number of spots per section.\n\n    Returns:\n        list: A list containing [ICC_patient, ICC_section] rounded to 4 decimals.\n    \"\"\"\n    # Reshape data into a convenient PxSxI format if not already\n    data = np.asarray(data).reshape((P, S, I))\n\n    # Calculate means\n    mean_sections = np.mean(data, axis=2)  # shape (P, S)\n    mean_patients = np.mean(mean_sections, axis=1)  # shape (P,)\n    grand_mean = np.mean(mean_patients)\n\n    # Calculate Sums of Squares (SS)\n    # SSB: Sum of Squares Between patients\n    ssb_term = (mean_patients - grand_mean)**2\n    SSB = S * I * np.sum(ssb_term)\n\n    # SST: Sum of Squares for Sections within patients\n    sst_term = (mean_sections - mean_patients[:, np.newaxis])**2\n    SST = I * np.sum(sst_term)\n\n    # SSE: Sum of Squares Error (Within sections)\n    sse_term = (data - mean_sections[:, :, np.newaxis])**2\n    SSE = np.sum(sse_term)\n\n    # Calculate Degrees of Freedom (df)\n    df_B = P - 1\n    df_T = P * (S - 1)\n    df_E = P * S * (I - 1)\n\n    # Calculate Mean Squares (MS)\n    # Handle df=0 case for robustness, though not expected with given test cases\n    MSB = SSB / df_B if df_B > 0 else 0.0\n    MST = SST / df_T if df_T > 0 else 0.0\n    MSE = SSE / df_E if df_E > 0 else 0.0\n\n    # Estimate variance components\n    sigma_e2_hat = MSE\n    sigma_t2_hat = (MST - MSE) / I\n    sigma_b2_hat = (MSB - MST) / (S * I)\n\n    # Truncate negative estimates to 0\n    sigma_e2_star = max(0, sigma_e2_hat)\n    sigma_t2_star = max(0, sigma_t2_hat)\n    sigma_b2_star = max(0, sigma_b2_hat)\n\n    # Calculate total variance\n    total_variance = sigma_b2_star + sigma_t2_star + sigma_e2_star\n\n    # Calculate ICCs\n    if total_variance == 0:\n        # If all data points are identical, variance is 0, correlation is undefined.\n        # Treat as 0, reflecting no correlated variability.\n        icc_patient = 0.0\n        icc_section = 0.0\n    else:\n        icc_patient = sigma_b2_star / total_variance\n        icc_section = (sigma_b2_star + sigma_t2_star) / total_variance\n\n    return [f\"{icc_patient:.4f}\", f\"{icc_section:.4f}\"]\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test suite data\n    # Each case is a balanced nested dataset with P=2, S=2, I=3\n    test_cases = [\n        # Case 1\n        [\n            [[2.00, 2.20, 1.80], [2.10, 2.30, 1.90]],  # Patient 1\n            [[3.00, 3.10, 2.90], [3.20, 3.00, 3.10]]   # Patient 2\n        ],\n        # Case 2\n        [\n            [[2.10, 2.00, 2.20], [3.10, 3.00, 3.20]],  # Patient 1\n            [[2.05, 2.15, 2.10], [3.05, 2.95, 3.15]]   # Patient 2\n        ],\n        # Case 3\n        [\n            [[1.00, 1.00, 1.00], [1.00, 1.00, 1.00]],  # Patient 1\n            [[4.00, 4.00, 4.00], [4.00, 4.00, 4.00]]   # Patient 2\n        ]\n    ]\n\n    P, S, I = 2, 2, 3\n    results = []\n    for case_data in test_cases:\n        iccs = calculate_iccs(case_data, P, S, I)\n        # Convert string floats to actual floats for list representation\n        results.append([float(x) for x in iccs])\n\n    # Final print statement must produce a list of lists format without spaces.\n    # str(results) produces \"[...]\", and .replace(' ', '') removes spaces.\n    final_output = str(results).replace(' ', '')\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}