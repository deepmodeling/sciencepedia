{
    "hands_on_practices": [
        {
            "introduction": "A theoretical model is only as useful as our ability to simulate it. This first practice tackles the fundamental task of translating the continuous-time Ornstein-Uhlenbeck SDE into a discrete-time algorithm suitable for computer simulation. By deriving the Euler-Maruyama scheme, you will gain insight into how stochastic processes are numerically integrated and, crucially, how the choice of simulation time step affects the accuracy of your model's long-term statistical properties .",
            "id": "4006881",
            "problem": "A single-compartment neuron receives synaptic input whose net fluctuation is modeled as an Ornstein–Uhlenbeck (OU) process. Let $X_{t}$ denote the synaptic variable, assumed to follow the stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\frac{1}{\\tau}\\,\\big(X_{t}-\\mu\\big)\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $\\tau>0$ is the correlation time, $\\mu$ is the mean level, $\\sigma>0$ is the noise intensity, and $W_{t}$ is standard Brownian motion (Wiener process). Starting only from the definitions of a stochastic differential equation (SDE), the Itō increment properties of Brownian motion, and the general Euler–Maruyama (EM) construction principle, derive a time-stepping scheme that advances $X_{t}$ over a fixed step $\\,\\Delta t>0\\,$ using a normal random variable at each step.\n\nThen, view the resulting discrete-time scheme as a first-order autoregressive process and, without invoking any closed-form exact solution for the continuous OU process, derive the stationary variance of the discrete-time scheme as a function of $\\Delta t$, $\\tau$, and $\\sigma$. Next, derive the stationary variance of the continuous-time OU process directly from its moment equation. Define the dimensionless step size $h \\equiv \\Delta t/\\tau$ and the relative stationary-variance error\n$$\n\\varepsilon_{\\mathrm{rel}}(h) \\;\\equiv\\; \\frac{\\operatorname{Var}_{\\mathrm{disc}}(h)-\\operatorname{Var}_{\\mathrm{cont}}}{\\operatorname{Var}_{\\mathrm{cont}}}.\n$$\nImpose the accuracy requirement $\\varepsilon_{\\mathrm{rel}}(h) \\le \\epsilon$ for a prescribed tolerance $\\,\\epsilon \\in (0,1)\\,$, and determine the largest admissible dimensionless step size $h_{\\max}$ as a closed-form expression in terms of $\\epsilon$. Provide your final answer as the analytic expression for $h_{\\max}$. No numerical evaluation is required, and you should not report any units for your final expression.",
            "solution": "The problem is to first derive a discrete-time approximation for an Ornstein-Uhlenbeck (OU) process using the Euler-Maruyama method, then analyze the stationary variance of this discrete scheme and its continuous-time counterpart, and finally determine an accuracy constraint on the time step. The problem is well-posed and scientifically sound, allowing for a complete derivation.\n\nThe Ornstein-Uhlenbeck process is described by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} = -\\frac{1}{\\tau}(X_{t}-\\mu)\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $\\tau > 0$, $\\mu$, and $\\sigma > 0$ are constants, and $W_t$ is a standard Wiener process.\n\nFirst, we derive the time-stepping scheme. The Euler-Maruyama method approximates the SDE over a small, finite time step $\\Delta t > 0$. We discretize time as $t_n = n \\Delta t$ and let $X_n \\equiv X_{t_n}$. The differential equation is approximated by the difference equation:\n$$\nX_{n+1} - X_n \\approx -\\frac{1}{\\tau}(X_n - \\mu)\\Delta t + \\sigma (W_{t_{n+1}} - W_{t_n})\n$$\nThe increment of the Wiener process, $\\Delta W_n \\equiv W_{t_{n+1}} - W_{t_n}$, is a normally distributed random variable with mean $0$ and variance $\\Delta t$. We can thus write $\\Delta W_n = \\sqrt{\\Delta t} Z_n$, where $Z_n$ is a standard normal random variable, $Z_n \\sim \\mathcal{N}(0,1)$. Substituting this into the difference equation gives the time-stepping scheme:\n$$\nX_{n+1} = X_n - \\frac{\\Delta t}{\\tau}(X_n - \\mu) + \\sigma \\sqrt{\\Delta t} Z_n\n$$\nRearranging the terms, we get:\n$$\nX_{n+1} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)X_n + \\frac{\\Delta t}{\\tau}\\mu + \\sigma \\sqrt{\\Delta t} Z_n\n$$\nThis is the desired first-order autoregressive, AR($1$), process.\n\nNext, we derive the stationary variance of this discrete-time scheme, which we denote as $\\operatorname{Var}_{\\mathrm{disc}}$. A stationary process has a time-independent mean and variance. Let $\\mathbb{E}[X_n] = \\mu_{\\mathrm{disc}}$ and $\\operatorname{Var}(X_n) = \\sigma^2_{\\mathrm{disc}}$ for all $n$ in the stationary state. Taking the expectation of the scheme, and using $\\mathbb{E}[Z_n]=0$:\n$$\n\\mathbb{E}[X_{n+1}] = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)\\mathbb{E}[X_n] + \\frac{\\Delta t}{\\tau}\\mu\n$$\n$$\n\\mu_{\\mathrm{disc}} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)\\mu_{\\mathrm{disc}} + \\frac{\\Delta t}{\\tau}\\mu\n$$\nSolving for $\\mu_{\\mathrm{disc}}$ yields $\\mu_{\\mathrm{disc}} = \\mu$. The stationary mean of the discrete process matches the mean of the continuous process.\n\nTo find the stationary variance, we compute the variance of the scheme. Since $X_n$ and the subsequent noise term $Z_n$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(X_{n+1}) = \\operatorname{Var}\\left(\\left(1 - \\frac{\\Delta t}{\\tau}\\right)X_n + \\sigma \\sqrt{\\Delta t} Z_n\\right)\n$$\n$$\n\\operatorname{Var}(X_{n+1}) = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2 \\operatorname{Var}(X_n) + (\\sigma \\sqrt{\\Delta t})^2 \\operatorname{Var}(Z_n)\n$$\nIn the stationary state, $\\operatorname{Var}(X_{n+1}) = \\operatorname{Var}(X_n) = \\sigma^2_{\\mathrm{disc}}$, and since $\\operatorname{Var}(Z_n)=1$:\n$$\n\\sigma^2_{\\mathrm{disc}} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2 \\sigma^2_{\\mathrm{disc}} + \\sigma^2 \\Delta t\n$$\nSolving for $\\sigma^2_{\\mathrm{disc}}$:\n$$\n\\sigma^2_{\\mathrm{disc}}\\left[1 - \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2\\right] = \\sigma^2 \\Delta t\n$$\n$$\n\\sigma^2_{\\mathrm{disc}}\\left[1 - \\left(1 - 2\\frac{\\Delta t}{\\tau} + \\frac{(\\Delta t)^2}{\\tau^2}\\right)\\right] = \\sigma^2 \\Delta t\n$$\n$$\n\\sigma^2_{\\mathrm{disc}}\\left[2\\frac{\\Delta t}{\\tau} - \\frac{(\\Delta t)^2}{\\tau^2}\\right] = \\sigma^2 \\Delta t\n$$\nAssuming $\\Delta t \\neq 0$ and the process is stable (which requires $2\\frac{\\Delta t}{\\tau} - \\frac{(\\Delta t)^2}{\\tau^2} > 0$, or $\\Delta t < 2\\tau$), we can divide to find:\n$$\n\\sigma^2_{\\mathrm{disc}} = \\frac{\\sigma^2 \\Delta t}{\\frac{\\Delta t}{\\tau}\\left(2 - \\frac{\\Delta t}{\\tau}\\right)} = \\frac{\\sigma^2 \\tau}{2 - \\frac{\\Delta t}{\\tau}}\n$$\nSo, the stationary variance of the discrete scheme is $\\operatorname{Var}_{\\mathrm{disc}} = \\frac{\\sigma^2 \\tau}{2 - \\Delta t/\\tau}$.\n\nNow, we derive the stationary variance of the continuous-time OU process, $\\operatorname{Var}_{\\mathrm{cont}}$. Let $Y_t = X_t - \\mu$. The SDE for $Y_t$ is $\\mathrm{d}Y_t = -\\frac{1}{\\tau}Y_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t$. The variance of $X_t$ is equal to the variance of $Y_t$. We derive the differential equation for the second moment $m_2(t) = \\mathbb{E}[Y_t^2]$. Using Itō's lemma for $f(Y) = Y^2$, we have $\\mathrm{d}f(Y_t) = f'(Y_t)\\mathrm{d}Y_t + \\frac{1}{2}f''(Y_t)(\\mathrm{d}Y_t)^2$. Here, $f'(Y) = 2Y$ and $f''(Y) = 2$. The quadratic variation is $(\\mathrm{d}Y_t)^2 = \\sigma^2 \\mathrm{d}t$.\n$$\n\\mathrm{d}(Y_t^2) = 2Y_t \\left(-\\frac{1}{\\tau}Y_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t\\right) + \\frac{1}{2}(2)(\\sigma^2 \\mathrm{d}t) = \\left(-\\frac{2}{\\tau}Y_t^2 + \\sigma^2\\right)\\mathrm{d}t + 2\\sigma Y_t \\mathrm{d}W_t\n$$\nTaking the expectation, and noting that the expectation of the Itō integral term is zero ($\\mathbb{E}[2\\sigma \\int Y_s \\mathrm{d}W_s] = 0$):\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}[Y_t^2] = -\\frac{2}{\\tau}\\mathbb{E}[Y_t^2] + \\sigma^2\n$$\nIn the stationary state, the time derivative is zero, so $\\frac{\\mathrm{d}m_2}{\\mathrm{d}t} = 0$. Let $m_{2,\\mathrm{ss}}$ be the stationary second moment.\n$$\n0 = -\\frac{2}{\\tau}m_{2,\\mathrm{ss}} + \\sigma^2 \\quad \\implies \\quad m_{2,\\mathrm{ss}} = \\frac{\\sigma^2 \\tau}{2}\n$$\nSince the stationary mean of $Y_t$ is $0$, its stationary variance is $\\operatorname{Var}_{\\mathrm{cont}} = m_{2,\\mathrm{ss}} = \\frac{\\sigma^2 \\tau}{2}$.\n\nWe now define the dimensionless step size $h \\equiv \\Delta t / \\tau$ and the relative stationary-variance error $\\varepsilon_{\\mathrm{rel}}(h)$.\n$$\n\\operatorname{Var}_{\\mathrm{disc}}(h) = \\frac{\\sigma^2 \\tau}{2 - h}\n$$\n$$\n\\varepsilon_{\\mathrm{rel}}(h) = \\frac{\\operatorname{Var}_{\\mathrm{disc}}(h)-\\operatorname{Var}_{\\mathrm{cont}}}{\\operatorname{Var}_{\\mathrm{cont}}} = \\frac{\\frac{\\sigma^2 \\tau}{2-h} - \\frac{\\sigma^2 \\tau}{2}}{\\frac{\\sigma^2 \\tau}{2}}\n$$\nCanceling the term $\\frac{\\sigma^2 \\tau}{2}$:\n$$\n\\varepsilon_{\\mathrm{rel}}(h) = \\frac{\\frac{2}{2-h} - 1}{1} = \\frac{2 - (2-h)}{2-h} = \\frac{h}{2-h}\n$$\nFinally, we impose the accuracy requirement $\\varepsilon_{\\mathrm{rel}}(h) \\le \\epsilon$ for a given tolerance $\\epsilon \\in (0,1)$ and find the largest admissible step size $h_{\\max}$.\n$$\n\\frac{h}{2-h} \\le \\epsilon\n$$\nFor the discrete process to be stable, we must have $h < 2$, so the denominator $2-h$ is positive. We can multiply both sides by $2-h$ without changing the inequality's direction.\n$$\nh \\le \\epsilon(2-h) = 2\\epsilon - \\epsilon h\n$$\n$$\nh + \\epsilon h \\le 2\\epsilon\n$$\n$$\nh(1+\\epsilon) \\le 2\\epsilon\n$$\nSince $1+\\epsilon > 0$, we can divide to get:\n$$\nh \\le \\frac{2\\epsilon}{1+\\epsilon}\n$$\nThe function $\\varepsilon_{\\mathrm{rel}}(h)$ is monotonically increasing for $h \\in [0, 2)$. Therefore, the inequality holds for all $h$ up to the maximum value where equality is achieved. The largest admissible dimensionless step size is:\n$$\nh_{\\max} = \\frac{2\\epsilon}{1+\\epsilon}\n$$",
            "answer": "$$\n\\boxed{\\frac{2\\epsilon}{1+\\epsilon}}\n$$"
        },
        {
            "introduction": "While the standard OU process is a powerful abstraction, biophysical quantities like synaptic conductance are often constrained; for instance, they cannot be negative. This exercise challenges you to adapt the basic OU model to incorporate such a physical reality by introducing a reflecting boundary at zero. Through this practice, you will learn how to modify an SDE to enforce constraints and analyze the consequences for the process's stationary statistics using the Fokker-Planck equation .",
            "id": "4006914",
            "problem": "A common stochastic model for a synaptic conductance is an Ornstein-Uhlenbeck (OU) process that relaxes toward a baseline conductance while being driven by thermal-like fluctuations. Let the unconstrained synaptic conductance process be defined by the stochastic differential equation\n$$\n\\mathrm{d}g(t) \\;=\\; -\\frac{g(t)-\\mu}{\\tau}\\,\\mathrm{d}t \\;+\\; \\sqrt{2D}\\,\\mathrm{d}W_t,\n$$\nwhere $g(t)$ is the conductance, $\\mu$ is the baseline conductance, $\\tau$ is the relaxation time constant, $D$ is a diffusion strength, and $W_t$ is a standard Wiener process (Brownian motion). In biophysically realistic settings, conductance cannot be negative, so a mechanism is required to ensure $g(t)\\ge 0$.\n\n1. Propose a minimal modification of the above stochastic differential equation that enforces a perfectly reflecting boundary at $g=0$ (that is, sample paths are constrained to $g(t)\\ge 0$ and are instantaneously reflected at $g=0$). Clearly specify any additional term(s) and their defining properties.\n\n2. Using the stationary Fokker–Planck equation with a no-flux boundary condition at $g=0$ and appropriate decay at $g\\to\\infty$, derive the stationary probability density on $[0,\\infty)$ for the reflected process. Then compute, in closed form, the stationary mean and stationary variance of $g$ for the reflected process. Express your final formulas in terms of $\\mu$, $\\tau$, $D$, and the standard normal probability density function and cumulative distribution function, denoted $\\phi$ and $\\Phi$, respectively, where\n$$\n\\phi(x)\\;=\\;\\frac{1}{\\sqrt{2\\pi}}\\,\\exp\\!\\left(-\\frac{x^{2}}{2}\\right), \n\\qquad\n\\Phi(x)\\;=\\;\\int_{-\\infty}^{x}\\phi(u)\\,\\mathrm{d}u.\n$$\n\n3. Briefly state how the stationary mean and variance of the reflected process compare to those of the unconstrained OU process.\n\nProvide your final answer as the pair consisting of the stationary mean and the stationary variance for the reflected process. No numerical evaluation is required; provide closed-form expressions only. Do not include units. You do not need to round your answer.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a standard, solvable problem in the field of stochastic processes applied to computational neuroscience.\n\n**1. Modified Stochastic Differential Equation with a Reflecting Boundary**\n\nThe unconstrained Ornstein-Uhlenbeck (OU) process is given by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}g(t) \\;=\\; -\\frac{g(t)-\\mu}{\\tau}\\,\\mathrm{d}t \\;+\\; \\sqrt{2D}\\,\\mathrm{d}W_t\n$$\nTo enforce a perfectly reflecting boundary at $g=0$ such that the process is constrained to $g(t) \\ge 0$, we introduce a minimal modification in the form of a local time term, denoted $L_t$. The SDE for the reflected process becomes:\n$$\n\\mathrm{d}g(t) \\;=\\; -\\frac{g(t)-\\mu}{\\tau}\\,\\mathrm{d}t \\;+\\; \\sqrt{2D}\\,\\mathrm{d}W_t \\;+\\; \\mathrm{d}L_t\n$$\nThe additional term $L_t$ is the local time of the process at the boundary $g=0$. It is a continuous, non-decreasing process that starts at $L_0=0$ and only increases when the process $g(t)$ is at the boundary. This can be formally stated as:\n$$\nL_t = \\int_0^t I_{\\{g(s)=0\\}}\\,\\mathrm{d}L_s\n$$\nwhere $I_{\\{g(s)=0\\}}$ is the indicator function which is $1$ if $g(s)=0$ and $0$ otherwise. This term provides an instantaneous \"push\" in the positive direction whenever the trajectory hits the boundary at $g=0$, preventing it from becoming negative.\n\n**2. Stationary Probability Density, Mean, and Variance**\n\nThe stationary probability density function, $p(g)$, for a one-dimensional diffusion process described by $\\mathrm{d}g(t) = A(g)\\,\\mathrm{d}t + B(g)\\,\\mathrm{d}W_t$ satisfies the stationary Fokker–Planck equation. This equation states that the probability flux, $J(g)$, must be constant. The flux is defined as:\n$$\nJ(g) = A(g)p(g) - \\frac{1}{2}\\frac{\\mathrm{d}}{\\mathrm{d}g}\\left[B(g)^2 p(g)\\right]\n$$\nFor the given OU process, the drift coefficient is $A(g) = -\\frac{g-\\mu}{\\tau}$ and the diffusion coefficient is constant, $B(g)^2 = 2D$. The flux is therefore:\n$$\nJ(g) = -\\frac{g-\\mu}{\\tau}p(g) - D\\frac{\\mathrm{d}p(g)}{\\mathrm{d}g}\n$$\nThe boundary conditions for the reflected process on $[0, \\infty)$ are:\n1.  A no-flux condition at the reflecting boundary $g=0$, which means $J(0) = 0$.\n2.  The probability density must vanish at infinity, i.e., $p(g) \\to 0$ as $g \\to \\infty$. This implies $J(\\infty) = 0$.\n\nSince the flux is constant and is zero at the boundaries, it must be zero everywhere: $J(g)=0$ for all $g \\ge 0$. This gives us a first-order ordinary differential equation for $p(g)$:\n$$\n-\\frac{g-\\mu}{\\tau}p(g) - D\\frac{\\mathrm{d}p(g)}{\\mathrm{d}g} = 0\n$$\n$$\n\\frac{\\mathrm{d}p}{p} = -\\frac{g-\\mu}{D\\tau}\\,\\mathrm{d}g\n$$\nIntegrating both sides yields:\n$$\n\\ln p(g) = -\\int \\frac{g-\\mu}{D\\tau}\\,\\mathrm{d}g = -\\frac{(g-\\mu)^2}{2D\\tau} + C_0\n$$\n$$\np(g) = C \\exp\\left(-\\frac{(g-\\mu)^2}{2D\\tau}\\right)\n$$\nwhere $C$ is a normalization constant. Let's define the variance of the unconstrained process as $\\sigma^2 = D\\tau$. Then the density is $p(g) = C \\exp\\left(-\\frac{(g-\\mu)^2}{2\\sigma^2}\\right)$ for $g \\ge 0$. The constant $C$ is determined by the normalization condition $\\int_0^\\infty p(g)\\,\\mathrm{d}g = 1$.\n$$\n\\int_0^\\infty C \\exp\\left(-\\frac{(g-\\mu)^2}{2\\sigma^2}\\right)\\,\\mathrm{d}g = 1\n$$\nWe perform a change of variables to $u = (g-\\mu)/\\sigma$. Then $\\mathrm{d}g = \\sigma\\,\\mathrm{d}u$. The lower limit of integration becomes $g=0 \\implies u = -\\mu/\\sigma$. The upper limit is $g=\\infty \\implies u=\\infty$.\n$$\nC\\sigma \\int_{-\\mu/\\sigma}^\\infty \\exp\\left(-\\frac{u^2}{2}\\right)\\,\\mathrm{d}u = 1\n$$\nThe integral can be expressed using the standard normal cumulative distribution function $\\Phi(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}}\\exp(-u^2/2)\\,\\mathrm{d}u$.\n$$\n\\int_{-\\mu/\\sigma}^\\infty \\exp\\left(-\\frac{u^2}{2}\\right)\\,\\mathrm{d}u = \\sqrt{2\\pi}\\left[\\Phi(\\infty) - \\Phi(-\\mu/\\sigma)\\right] = \\sqrt{2\\pi}\\left[1 - \\Phi(-\\mu/\\sigma)\\right]\n$$\nUsing the identity $\\Phi(x) + \\Phi(-x) = 1$, this becomes $\\sqrt{2\\pi}\\Phi(\\mu/\\sigma)$.\nSo, $C\\sigma\\sqrt{2\\pi}\\Phi(\\mu/\\sigma) = 1$, which gives $C = \\frac{1}{\\sigma\\sqrt{2\\pi}\\Phi(\\mu/\\sigma)}$.\nThe stationary probability density for the reflected process is a truncated normal distribution:\n$$\np(g) = \\frac{1}{\\sigma\\sqrt{2\\pi}\\Phi(\\mu/\\sigma)} \\exp\\left(-\\frac{(g-\\mu)^2}{2\\sigma^2}\\right) = \\frac{\\phi((g-\\mu)/\\sigma)}{\\sigma\\Phi(\\mu/\\sigma)}, \\quad \\text{for } g \\ge 0\n$$\n\n**Stationary Mean ($E_r[g]$):**\n$$\nE_r[g] = \\int_0^\\infty g\\,p(g)\\,\\mathrm{d}g = \\int_0^\\infty (g-\\mu)p(g)\\,\\mathrm{d}g + \\mu\\int_0^\\infty p(g)\\,\\mathrm{d}g = E_r[g-\\mu] + \\mu\n$$\nLet $z = \\mu/\\sigma$. We calculate $E_r[g-\\mu]$:\n$$\nE_r[g-\\mu] = \\int_0^\\infty (g-\\mu) \\frac{\\phi((g-\\mu)/\\sigma)}{\\sigma\\Phi(z)}\\,\\mathrm{d}g\n$$\nUsing the substitution $u=(g-\\mu)/\\sigma$, this becomes:\n$$\nE_r[g-\\mu] = \\int_{-z}^\\infty \\sigma u \\frac{\\phi(u)}{\\Phi(z)}\\,\\mathrm{d}u = \\frac{\\sigma}{\\Phi(z)} \\int_{-z}^\\infty u \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{u^2}{2}\\right)\\,\\mathrm{d}u\n$$\nThe integral evaluates to $\\frac{1}{\\sqrt{2\\pi}}\\left[-\\exp(-u^2/2)\\right]_{-z}^\\infty = \\frac{1}{\\sqrt{2\\pi}}\\exp(-(-z)^2/2) = \\phi(z)$.\nTherefore, $E_r[g-\\mu] = \\sigma \\frac{\\phi(z)}{\\Phi(z)}$.\nThe stationary mean is:\n$$\nE_r[g] = \\mu + \\sigma \\frac{\\phi(z)}{\\Phi(z)} = \\mu + \\sqrt{D\\tau} \\frac{\\phi(\\mu/\\sqrt{D\\tau})}{\\Phi(\\mu/\\sqrt{D\\tau})}\n$$\n\n**Stationary Variance ($Var_r[g]$):**\nThe variance is given by $\\mathrm{Var}_r[g] = E_r[g^2] - (E_r[g])^2$. A known result for a normal distribution truncated on $[a,\\infty)$ is that its variance is $\\sigma^2\\left[1 - \\lambda(\\alpha)( \\lambda(\\alpha) - \\alpha)\\right]$, where $\\alpha=(a-\\mu)/\\sigma$ and $\\lambda(\\alpha)=\\phi(\\alpha)/(1-\\Phi(\\alpha))$.\nFor our case, $a=0$, so $\\alpha = -\\mu/\\sigma = -z$. The term $1-\\Phi(\\alpha) = 1-\\Phi(-z) = \\Phi(z)$. Thus $\\lambda(-z) = \\phi(-z)/\\Phi(z) = \\phi(z)/\\Phi(z)$.\nThe variance is then:\n$$\n\\mathrm{Var}_r[g] = \\sigma^2 \\left[ 1 - \\frac{\\phi(z)}{\\Phi(z)} \\left( \\frac{\\phi(z)}{\\Phi(z)} - (-z) \\right) \\right] = \\sigma^2 \\left[ 1 - \\frac{z\\phi(z)}{\\Phi(z)} - \\left(\\frac{\\phi(z)}{\\Phi(z)}\\right)^2 \\right]\n$$\nSubstituting $\\sigma^2 = D\\tau$ and $z=\\mu/\\sqrt{D\\tau}$, we get:\n$$\n\\mathrm{Var}_r[g] = D\\tau \\left( 1 - \\frac{\\mu}{\\sqrt{D\\tau}}\\frac{\\phi(\\mu/\\sqrt{D\\tau})}{\\Phi(\\mu/\\sqrt{D\\tau})} - \\left(\\frac{\\phi(\\mu/\\sqrt{D\\tau})}{\\Phi(\\mu/\\sqrt{D\\tau})}\\right)^2 \\right)\n$$\n\n**3. Comparison with the Unconstrained Process**\n\nFor the standard (unconstrained) OU process, the stationary distribution is a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, where $\\sigma^2=D\\tau$.\n-   **Stationary Mean:** $E_u[g] = \\mu$.\n-   **Stationary Variance:** $\\mathrm{Var}_u[g] = D\\tau$.\n\nComparing the two processes:\n-   **Mean:** The term $\\sqrt{D\\tau} \\frac{\\phi(z)}{\\Phi(z)}$ is strictly positive for any finite $z$. Therefore, $E_r[g] = \\mu + (\\text{positive term}) > \\mu = E_u[g]$. The mean of the reflected process is always greater than that of the unconstrained process. This is because the reflecting boundary prevents excursions into negative values, pushing the probability mass to higher values and thus increasing the average.\n-   **Variance:** The term $1 - z\\frac{\\phi(z)}{\\Phi(z)} - (\\frac{\\phi(z)}{\\Phi(z)})^2$ is less than $1$, since the subtracted terms are positive. Therefore, $\\mathrm{Var}_r[g] < D\\tau = \\mathrm{Var}_u[g]$. The variance of the reflected process is always less than that of the unconstrained process. The reflecting boundary truncates the state space, reducing the overall spread of the distribution.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mu + \\sqrt{D\\tau} \\frac{\\phi\\left(\\frac{\\mu}{\\sqrt{D\\tau}}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sqrt{D\\tau}}\\right)} & D\\tau \\left( 1 - \\frac{\\mu}{\\sqrt{D\\tau}}\\frac{\\phi\\left(\\frac{\\mu}{\\sqrt{D\\tau}}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sqrt{D\\tau}}\\right)} - \\left(\\frac{\\phi\\left(\\frac{\\mu}{\\sqrt{D\\tau}}\\right)}{\\Phi\\left(\\frac{\\mu}{\\sqrt{D\\tau}}\\right)}\\right)^2 \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The ultimate test of a model is its connection to experimental data. This final practice bridges the gap between the theoretical OU process and messy, real-world measurements, which are often noisy and probabilistic. You will play the role of a computational neuroscientist, deriving estimators to infer the hidden parameters of a latent synaptic conductance process from observable statistics, accounting for factors like stochastic vesicle release and recording noise .",
            "id": "4006846",
            "problem": "You are given a modeling task in the context of synaptic noise represented by an Ornstein-Uhlenbeck process in brain modeling and computational neuroscience. Miniature Excitatory Postsynaptic Currents (mEPSCs) are modeled as arising from a latent synaptic conductance process with recording noise. Use first-principles to derive an estimator for the Ornstein-Uhlenbeck parameters based on observable distributional statistics and release probability.\n\nStart from the following base and assumptions:\n\n- The latent synaptic conductance $g(t)$ obeys the Ornstein-Uhlenbeck stochastic differential equation\n$$\n\\mathrm{d}g(t) = -\\frac{1}{\\tau}\\left(g(t)-\\mu\\right)\\mathrm{d}t + \\sigma\\,\\mathrm{d}W(t),\n$$\nwhere $W(t)$ is a standard Wiener process (Brownian motion), and $\\mu$, $\\sigma$, $\\tau$ are constant parameters. The steady-state distribution of $g(t)$ is Gaussian with mean $\\mu$ and variance that depends on $\\sigma$ and $\\tau$.\n- At each trial, a release event occurs with probability $p$ independently across trials. Let $I\\in\\{0,1\\}$ be the Bernoulli indicator with $\\mathbb{P}(I=1)=p$. Conditional on $I=1$, the observed mEPSC amplitude is modeled as $X=G+\\varepsilon$, and conditional on $I=0$, as $X=\\varepsilon$, where $G$ is a draw from the steady-state of the Ornstein-Uhlenbeck process and $\\varepsilon$ is independent additive recording noise.\n- The recording noise $\\varepsilon$ is modeled as Gaussian with zero mean and known variance $v_n$, independent across trials and independent of $G$.\n- Among trials with $I=1$, the amplitudes on consecutive trials separated by inter-trial interval $\\Delta t$ exhibit correlation due to the autocorrelation of the latent Ornstein-Uhlenbeck process and additive noise.\n\nYou are provided, for each test case:\n- The release probability $p$ (unitless, expressed as a decimal).\n- The recording noise variance $v_n$ in picoampere-squared (pA$^2$).\n- The inter-trial interval $\\Delta t$ in milliseconds (ms).\n- The observed amplitude mean $m_{\\mathrm{obs}}$ in picoamperes (pA) across all trials.\n- The observed amplitude variance $s^2_{\\mathrm{obs}}$ in picoampere-squared (pA$^2$) across all trials.\n- The observed conditional Pearson correlation $r_{\\mathrm{obs}}$ (unitless) between amplitudes of two consecutive trials, computed only on pairs where both trials had a release event ($I=1$ in both).\n\nYour task:\n1. Derive, from the stated assumptions and the Ornstein-Uhlenbeck steady-state properties and independence structure, expressions linking the observable statistics $m_{\\mathrm{obs}}$, $s^2_{\\mathrm{obs}}$, $r_{\\mathrm{obs}}$ to the parameters $\\mu$, $\\sigma$, and $\\tau$, explicitly accounting for the mixture due to stochastic release and the additive recording noise.\n2. Use those derived relations to compute parameter estimates $(\\widehat{\\mu},\\widehat{\\sigma},\\widehat{\\tau})$ from $(p,v_n,\\Delta t,m_{\\mathrm{obs}},s^2_{\\mathrm{obs}},r_{\\mathrm{obs}})$.\n\nUnits and output:\n- Report $\\widehat{\\mu}$ in picoamperes (pA).\n- Report $\\widehat{\\sigma}$ in picoamperes per square root millisecond (pA$/\\sqrt{\\mathrm{ms}}$).\n- Report $\\widehat{\\tau}$ in milliseconds (ms).\n- For numerical output, do not include unit strings; values must be floats rounded to six decimal places.\n\nTest suite:\nCompute $(\\widehat{\\mu},\\widehat{\\sigma},\\widehat{\\tau})$ for each of the following parameter sets. All quantities are given in appropriate units as specified above, with numbers written in decimal form.\n\n- Case 1: $p=0.35$, $v_n=9$, $\\Delta t=2$, $m_{\\mathrm{obs}}=4.2$, $s^2_{\\mathrm{obs}}=58.56$, $r_{\\mathrm{obs}}=0.603108$.\n- Case 2: $p=0.05$, $v_n=4$, $\\Delta t=5$, $m_{\\mathrm{obs}}=0.4$, $s^2_{\\mathrm{obs}}=9.29$, $r_{\\mathrm{obs}}=0.557019$.\n- Case 3: $p=0.95$, $v_n=1$, $\\Delta t=1$, $m_{\\mathrm{obs}}=4.75$, $s^2_{\\mathrm{obs}}=7.8875$, $r_{\\mathrm{obs}}=0.614169$.\n- Case 4: $p=0.4$, $v_n=100$, $\\Delta t=4$, $m_{\\mathrm{obs}}=6.0$, $s^2_{\\mathrm{obs}}=211.6$, $r_{\\mathrm{obs}}=0.358000$.\n- Case 5: $p=0.5$, $v_n=10$, $\\Delta t=20$, $m_{\\mathrm{obs}}=3.5$, $s^2_{\\mathrm{obs}}=27.25$, $r_{\\mathrm{obs}}=0.183940$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of three floats $[\\widehat{\\mu},\\widehat{\\sigma},\\widehat{\\tau}]$ in the order specified, for the five cases above. For example, the final output format must be like\n$[[\\widehat{\\mu}_1,\\widehat{\\sigma}_1,\\widehat{\\tau}_1],[\\widehat{\\mu}_2,\\widehat{\\sigma}_2,\\dots]]$\nwith all values rounded to six decimal places and no unit strings included.",
            "solution": "The task is to derive estimators for the parameters $(\\mu, \\sigma, \\tau)$ of an Ornstein-Uhlenbeck (OU) process from a set of observable statistics. The derivation proceeds from first principles based on the provided model assumptions.\n\nThe latent synaptic conductance $g(t)$ is described by the Ornstein-Uhlenbeck stochastic differential equation:\n$$\n\\mathrm{d}g(t) = -\\frac{1}{\\tau}\\left(g(t)-\\mu\\right)\\mathrm{d}t + \\sigma\\,\\mathrm{d}W(t)\n$$\nThis process is foundational. Its key statistical properties in the steady state are:\n1.  The stationary distribution of $g(t)$, which we denote by the random variable $G$, is a Gaussian distribution, $G \\sim \\mathcal{N}(\\mu, v_g)$.\n2.  The mean of this distribution is $\\mathbb{E}[G] = \\mu$.\n3.  The variance, $v_g = \\mathrm{Var}(G)$, is a function of the process parameters: $v_g = \\frac{\\sigma^2 \\tau}{2}$.\n4.  The autocorrelation function for a time lag $\\Delta t$ is $\\mathrm{Corr}(g(t), g(t+\\Delta t)) = e^{-\\Delta t/\\tau}$, which gives the autocovariance as $\\mathrm{Cov}(g(t), g(t+\\Delta t)) = \\mathrm{Var}(G) e^{-\\Delta t/\\tau} = v_g e^{-\\Delta t/\\tau}$.\n\nThe observed mEPSC amplitude, $X$, is modeled as a mixture reflecting stochastic synaptic release. A release occurs with probability $p$, indicated by a Bernoulli variable $I \\sim \\mathrm{Bernoulli}(p)$. The observed amplitude is $X = G + \\varepsilon$ if a release occurs ($I=1$), and $X = \\varepsilon$ if it fails ($I=0$). The recording noise $\\varepsilon$ is independent of the signal $G$ and follows a Gaussian distribution $\\varepsilon \\sim \\mathcal{N}(0, v_n)$. This can be written compactly as $X = I \\cdot G + \\varepsilon$.\n\nWe derive the estimators by relating the theoretical moments of $X$ to the observed statistics ($m_{\\mathrm{obs}}$, $s^2_{\\mathrm{obs}}$, $r_{\\mathrm{obs}}$).\n\n**Step 1: Derivation of the Estimator for Mean Conductance $\\mu$**\n\nWe relate the observed mean amplitude $m_{\\mathrm{obs}}$ to $\\mu$. Using the law of total expectation on $X = I \\cdot G + \\varepsilon$:\n$$\nm_{\\mathrm{obs}} = \\mathbb{E}[X] = \\mathbb{E}[I \\cdot G + \\varepsilon]\n$$\nDue to the independence of $I$, $G$, and $\\varepsilon$, and $\\mathbb{E}[\\varepsilon]=0$:\n$$\nm_{\\mathrm{obs}} = \\mathbb{E}[I] \\mathbb{E}[G] + \\mathbb{E}[\\varepsilon] = p \\cdot \\mu + 0 = p\\mu\n$$\nThis provides a direct relationship. Solving for $\\mu$, we get the estimator:\n$$\n\\widehat{\\mu} = \\frac{m_{\\mathrm{obs}}}{p}\n$$\n\n**Step 2: Derivation for the Latent Variance $v_g$**\n\nWe relate the observed variance $s^2_{\\mathrm{obs}}$ to the latent variance $v_g$. Using the law of total variance for $X$:\n$$\ns^2_{\\mathrm{obs}} = \\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X|I)] + \\mathrm{Var}(\\mathbb{E}[X|I])\n$$\nWe compute each term separately.\nFor the first term, $\\mathbb{E}[\\mathrm{Var}(X|I)]$:\n-   Conditional on $I=1$: $X = G+\\varepsilon$. Since $G$ and $\\varepsilon$ are independent, $\\mathrm{Var}(X|I=1) = \\mathrm{Var}(G) + \\mathrm{Var}(\\varepsilon) = v_g + v_n$.\n-   Conditional on $I=0$: $X = \\varepsilon$. Thus, $\\mathrm{Var}(X|I=0) = \\mathrm{Var}(\\varepsilon) = v_n$.\nThe expectation is therefore:\n$$\n\\mathbb{E}[\\mathrm{Var}(X|I)] = \\mathrm{Var}(X|I=1)\\cdot p + \\mathrm{Var}(X|I=0)\\cdot(1-p) = (v_g + v_n)p + v_n(1-p) = p v_g + v_n\n$$\nFor the second term, $\\mathrm{Var}(\\mathbb{E}[X|I])$:\n-   Conditional on $I=1$: $\\mathbb{E}[X|I=1] = \\mathbb{E}[G+\\varepsilon] = \\mu+0 = \\mu$.\n-   Conditional on $I=0$: $\\mathbb{E}[X|I=0] = \\mathbb{E}[\\varepsilon] = 0$.\nThe random variable $\\mathbb{E}[X|I]$ takes the value $\\mu$ with probability $p$ and $0$ with probability $1-p$. The variance of this Bernoulli-like variable is:\n$$\n\\mathrm{Var}(\\mathbb{E}[X|I]) = \\mathbb{E}[(\\mathbb{E}[X|I])^2] - (\\mathbb{E}[\\mathbb{E}[X|I]])^2 = (\\mu^2 p + 0^2(1-p)) - (p\\mu)^2 = p\\mu^2 - p^2\\mu^2 = p(1-p)\\mu^2\n$$\nCombining the two terms, the total variance is:\n$$\ns^2_{\\mathrm{obs}} = (p v_g + v_n) + p(1-p)\\mu^2\n$$\nSolving for $v_g$, we obtain the estimator $\\widehat{v_g}$ by substituting $\\widehat{\\mu}$:\n$$\n\\widehat{v_g} = \\frac{s^2_{\\mathrm{obs}} - v_n - p(1-p)\\widehat{\\mu}^2}{p} = \\frac{1}{p}\\left(s^2_{\\mathrm{obs}} - v_n - p(1-p)\\left(\\frac{m_{\\mathrm{obs}}}{p}\\right)^2\\right) = \\frac{1}{p}\\left(s^2_{\\mathrm{obs}} - v_n - \\frac{1-p}{p}m_{\\mathrm{obs}}^2\\right)\n$$\n\n**Step 3: Derivation of the Estimator for Time Constant $\\tau$**\n\nThe observed correlation $r_{\\mathrm{obs}}$ links to $\\tau$. This correlation is computed between amplitudes of two consecutive trials, say $X_1$ and $X_2$, separated by $\\Delta t$, conditional on both trials having a release event ($I_1=1, I_2=1$).\nLet $X'_1 = X_1|_{I_1=1} = G_1+\\varepsilon_1$ and $X'_2 = X_2|_{I_2=1} = G_2+\\varepsilon_2$.\n$$\nr_{\\mathrm{obs}} = \\mathrm{Corr}(X'_1, X'_2) = \\frac{\\mathrm{Cov}(X'_1, X'_2)}{\\sqrt{\\mathrm{Var}(X'_1)\\mathrm{Var}(X'_2)}}\n$$\nThe variance in the denominator is $\\mathrm{Var}(X'_1) = \\mathrm{Var}(G_1+\\varepsilon_1) = \\mathrm{Var}(G_1)+\\mathrm{Var}(\\varepsilon_1) = v_g + v_n$.\nThe covariance in the numerator is $\\mathrm{Cov}(G_1+\\varepsilon_1, G_2+\\varepsilon_2)$. Given that $\\varepsilon_1, \\varepsilon_2$ are independent of each other and of the latent process $G$, this simplifies to:\n$$\n\\mathrm{Cov}(G_1, G_2) + \\mathrm{Cov}(G_1, \\varepsilon_2) + \\mathrm{Cov}(\\varepsilon_1, G_2) + \\mathrm{Cov}(\\varepsilon_1, \\varepsilon_2) = \\mathrm{Cov}(G_1, G_2) = v_g e^{-\\Delta t/\\tau}\n$$\nSubstituting these into the correlation formula:\n$$\nr_{\\mathrm{obs}} = \\frac{v_g e^{-\\Delta t/\\tau}}{v_g + v_n}\n$$\nWe solve for $\\tau$ by first isolating the exponential term:\n$$\ne^{-\\Delta t/\\tau} = r_{\\mathrm{obs}} \\frac{v_g + v_n}{v_g} = r_{\\mathrm{obs}}\\left(1 + \\frac{v_n}{v_g}\\right)\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{\\Delta t}{\\tau} = \\ln\\left(r_{\\mathrm{obs}}\\left(1 + \\frac{v_n}{v_g}\\right)\\right)\n$$\nThis gives the estimator for $\\tau$:\n$$\n\\widehat{\\tau} = -\\frac{\\Delta t}{\\ln\\left(r_{\\mathrm{obs}}\\left(1 + \\frac{v_n}{\\widehat{v_g}}\\right)\\right)}\n$$\n\n**Step 4: Derivation of the Estimator for Noise Amplitude $\\sigma$**\n\nFinally, we use the definitional relationship $v_g = \\frac{\\sigma^2 \\tau}{2}$ to find $\\sigma$. Rearranging the formula yields $\\sigma^2 = \\frac{2v_g}{\\tau}$. Taking the square root gives the estimator for $\\sigma$:\n$$\n\\widehat{\\sigma} = \\sqrt{\\frac{2\\widehat{v_g}}{\\widehat{\\tau}}}\n$$\nThe parameter $\\sigma$ must be non-negative, so we take the positive root. This completes the set of derivations for all three unknown parameters.\n\nThe computational procedure is to apply these four derived formulae sequentially, using the provided observable statistics for each test case.",
            "answer": "[[12.000000,4.000000,5.000000],[8.000000,3.000000,8.000000],[5.000000,2.000000,4.000000],[15.000000,10.000000,6.000000],[7.000000,3.000000,10.000000]]"
        }
    ]
}