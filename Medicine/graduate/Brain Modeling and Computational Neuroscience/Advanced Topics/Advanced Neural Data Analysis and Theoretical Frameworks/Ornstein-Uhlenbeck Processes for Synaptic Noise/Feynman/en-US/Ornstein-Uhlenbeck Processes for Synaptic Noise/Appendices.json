{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration of the Ornstein-Uhlenbeck (OU) process, we start with the most fundamental task: simulating its dynamics on a computer. This practice guides you through the derivation of the Euler-Maruyama scheme, a cornerstone method for the numerical integration of stochastic differential equations. By constructing the scheme from first principles and analyzing how its stationary variance deviates from the true continuous-time process, you will gain a crucial, quantitative understanding of the trade-off between computational speed (the time step $\\Delta t$) and simulation accuracy . This exercise builds the foundation for all numerical work with stochastic models.",
            "id": "4006881",
            "problem": "A single-compartment neuron receives synaptic input whose net fluctuation is modeled as an Ornstein–Uhlenbeck (OU) process. Let $X_{t}$ denote the synaptic variable, assumed to follow the stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} = -\\frac{1}{\\tau}\\big(X_{t}-\\mu\\big)\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\nwhere $\\tau > 0$ is the correlation time, $\\mu$ is the mean level, $\\sigma > 0$ is the noise intensity, and $W_{t}$ is standard Brownian motion (Wiener process). Starting only from the definitions of a stochastic differential equation (SDE), the Itō increment properties of Brownian motion, and the general Euler–Maruyama (EM) construction principle, derive a time-stepping scheme that advances $X_{t}$ over a fixed step $\\Delta t > 0$ using a normal random variable at each step.\n\nThen, view the resulting discrete-time scheme as a first-order autoregressive process and, without invoking any closed-form exact solution for the continuous OU process, derive the stationary variance of the discrete-time scheme as a function of $\\Delta t$, $\\tau$, and $\\sigma$. Next, derive the stationary variance of the continuous-time OU process directly from its moment equation. Define the dimensionless step size $h \\equiv \\Delta t/\\tau$ and the relative stationary-variance error\n$$\n\\varepsilon_{\\mathrm{rel}}(h) \\equiv \\frac{\\operatorname{Var}_{\\mathrm{disc}}(h)-\\operatorname{Var}_{\\mathrm{cont}}}{\\operatorname{Var}_{\\mathrm{cont}}}.\n$$\nImpose the accuracy requirement $\\varepsilon_{\\mathrm{rel}}(h) \\le \\epsilon$ for a prescribed tolerance $\\epsilon \\in (0,1)$, and determine the largest admissible dimensionless step size $h_{\\max}$ as a closed-form expression in terms of $\\epsilon$. Provide your final answer as the analytic expression for $h_{\\max}$. No numerical evaluation is required, and you should not report any units for your final expression.",
            "solution": "The problem is to first derive a discrete-time approximation for an Ornstein-Uhlenbeck (OU) process using the Euler-Maruyama method, then analyze the stationary variance of this discrete scheme and its continuous-time counterpart, and finally determine an accuracy constraint on the time step. The problem is well-posed and scientifically sound, allowing for a complete derivation.\n\nThe Ornstein-Uhlenbeck process is described by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} = -\\frac{1}{\\tau}(X_{t}-\\mu)\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nwhere $\\tau > 0$, $\\mu$, and $\\sigma > 0$ are constants, and $W_t$ is a standard Wiener process.\n\nFirst, we derive the time-stepping scheme. The Euler-Maruyama method approximates the SDE over a small, finite time step $\\Delta t > 0$. We discretize time as $t_n = n \\Delta t$ and let $X_n \\equiv X_{t_n}$. The differential equation is approximated by the difference equation:\n$$\nX_{n+1} - X_n \\approx -\\frac{1}{\\tau}(X_n - \\mu)\\Delta t + \\sigma (W_{t_{n+1}} - W_{t_n})\n$$\nThe increment of the Wiener process, $\\Delta W_n \\equiv W_{t_{n+1}} - W_{t_n}$, is a normally distributed random variable with mean $0$ and variance $\\Delta t$. We can thus write $\\Delta W_n = \\sqrt{\\Delta t} Z_n$, where $Z_n$ is a standard normal random variable, $Z_n \\sim \\mathcal{N}(0,1)$. Substituting this into the difference equation gives the time-stepping scheme:\n$$\nX_{n+1} = X_n - \\frac{\\Delta t}{\\tau}(X_n - \\mu) + \\sigma \\sqrt{\\Delta t} Z_n\n$$\nRearranging the terms, we get:\n$$\nX_{n+1} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)X_n + \\frac{\\Delta t}{\\tau}\\mu + \\sigma \\sqrt{\\Delta t} Z_n\n$$\nThis is the desired first-order autoregressive, AR($1$), process.\n\nNext, we derive the stationary variance of this discrete-time scheme, which we denote as $\\operatorname{Var}_{\\mathrm{disc}}$. A stationary process has a time-independent mean and variance. Let $\\mathbb{E}[X_n] = \\mu_{\\mathrm{disc}}$ and $\\operatorname{Var}(X_n) = \\sigma^2_{\\mathrm{disc}}$ for all $n$ in the stationary state. Taking the expectation of the scheme, and using $\\mathbb{E}[Z_n]=0$:\n$$\n\\mathbb{E}[X_{n+1}] = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)\\mathbb{E}[X_n] + \\frac{\\Delta t}{\\tau}\\mu\n$$\n$$\n\\mu_{\\mathrm{disc}} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)\\mu_{\\mathrm{disc}} + \\frac{\\Delta t}{\\tau}\\mu\n$$\nSolving for $\\mu_{\\mathrm{disc}}$ yields $\\mu_{\\mathrm{disc}} = \\mu$. The stationary mean of the discrete process matches the mean of the continuous process.\n\nTo find the stationary variance, we compute the variance of the scheme. Since $X_n$ and the subsequent noise term $Z_n$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}(X_{n+1}) = \\operatorname{Var}\\left(\\left(1 - \\frac{\\Delta t}{\\tau}\\right)X_n + \\sigma \\sqrt{\\Delta t} Z_n\\right)\n$$\n$$\n\\operatorname{Var}(X_{n+1}) = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2 \\operatorname{Var}(X_n) + (\\sigma \\sqrt{\\Delta t})^2 \\operatorname{Var}(Z_n)\n$$\nIn the stationary state, $\\operatorname{Var}(X_{n+1}) = \\operatorname{Var}(X_n) = \\sigma^2_{\\mathrm{disc}}$, and since $\\operatorname{Var}(Z_n)=1$:\n$$\n\\sigma^2_{\\mathrm{disc}} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2 \\sigma^2_{\\mathrm{disc}} + \\sigma^2 \\Delta t\n$$\nSolving for $\\sigma^2_{\\mathrm{disc}}$:\n$$\n\\sigma^2_{\\mathrm{disc}}\\left[1 - \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2\\right] = \\sigma^2 \\Delta t\n$$\n$$\n\\sigma^2_{\\mathrm{disc}}\\left[1 - \\left(1 - 2\\frac{\\Delta t}{\\tau} + \\frac{(\\Delta t)^2}{\\tau^2}\\right)\\right] = \\sigma^2 \\Delta t\n$$\n$$\n\\sigma^2_{\\mathrm{disc}}\\left[2\\frac{\\Delta t}{\\tau} - \\frac{(\\Delta t)^2}{\\tau^2}\\right] = \\sigma^2 \\Delta t\n$$\nAssuming $\\Delta t \\neq 0$ and the process is stable (which requires $2\\frac{\\Delta t}{\\tau} - \\frac{(\\Delta t)^2}{\\tau^2} > 0$, or $\\Delta t < 2\\tau$), we can divide to find:\n$$\n\\sigma^2_{\\mathrm{disc}} = \\frac{\\sigma^2 \\Delta t}{\\frac{\\Delta t}{\\tau}\\left(2 - \\frac{\\Delta t}{\\tau}\\right)} = \\frac{\\sigma^2 \\tau}{2 - \\frac{\\Delta t}{\\tau}}\n$$\nSo, the stationary variance of the discrete scheme is $\\operatorname{Var}_{\\mathrm{disc}} = \\frac{\\sigma^2 \\tau}{2 - \\Delta t/\\tau}$.\n\nNow, we derive the stationary variance of the continuous-time OU process, $\\operatorname{Var}_{\\mathrm{cont}}$. Let $Y_t = X_t - \\mu$. The SDE for $Y_t$ is $\\mathrm{d}Y_t = -\\frac{1}{\\tau}Y_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t$. The variance of $X_t$ is equal to the variance of $Y_t$. We derive the differential equation for the second moment $m_2(t) = \\mathbb{E}[Y_t^2]$. Using Itō's lemma for $f(Y) = Y^2$, we have $\\mathrm{d}f(Y_t) = f'(Y_t)\\mathrm{d}Y_t + \\frac{1}{2}f''(Y_t)(\\mathrm{d}Y_t)^2$. Here, $f'(Y) = 2Y$ and $f''(Y) = 2$. The quadratic variation is $(\\mathrm{d}Y_t)^2 = \\sigma^2 \\mathrm{d}t$.\n$$\n\\mathrm{d}(Y_t^2) = 2Y_t \\left(-\\frac{1}{\\tau}Y_t \\mathrm{d}t + \\sigma \\mathrm{d}W_t\\right) + \\frac{1}{2}(2)(\\sigma^2 \\mathrm{d}t) = \\left(-\\frac{2}{\\tau}Y_t^2 + \\sigma^2\\right)\\mathrm{d}t + 2\\sigma Y_t \\mathrm{d}W_t\n$$\nTaking the expectation, and noting that the expectation of the Itō integral term is zero ($\\mathbb{E}[2\\sigma \\int Y_s \\mathrm{d}W_s] = 0$):\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}[Y_t^2] = -\\frac{2}{\\tau}\\mathbb{E}[Y_t^2] + \\sigma^2\n$$\nIn the stationary state, the time derivative is zero, so $\\frac{\\mathrm{d}m_2}{\\mathrm{d}t} = 0$. Let $m_{2,\\mathrm{ss}}$ be the stationary second moment.\n$$\n0 = -\\frac{2}{\\tau}m_{2,\\mathrm{ss}} + \\sigma^2 \\quad \\implies \\quad m_{2,\\mathrm{ss}} = \\frac{\\sigma^2 \\tau}{2}\n$$\nSince the stationary mean of $Y_t$ is $0$, its stationary variance is $\\operatorname{Var}_{\\mathrm{cont}} = m_{2,\\mathrm{ss}} = \\frac{\\sigma^2 \\tau}{2}$.\n\nWe now define the dimensionless step size $h \\equiv \\Delta t / \\tau$ and the relative stationary-variance error $\\varepsilon_{\\mathrm{rel}}(h)$.\n$$\n\\operatorname{Var}_{\\mathrm{disc}}(h) = \\frac{\\sigma^2 \\tau}{2 - h}\n$$\n$$\n\\varepsilon_{\\mathrm{rel}}(h) = \\frac{\\operatorname{Var}_{\\mathrm{disc}}(h)-\\operatorname{Var}_{\\mathrm{cont}}}{\\operatorname{Var}_{\\mathrm{cont}}} = \\frac{\\frac{\\sigma^2 \\tau}{2-h} - \\frac{\\sigma^2 \\tau}{2}}{\\frac{\\sigma^2 \\tau}{2}}\n$$\nCanceling the term $\\frac{\\sigma^2 \\tau}{2}$:\n$$\n\\varepsilon_{\\mathrm{rel}}(h) = \\frac{\\frac{2}{2-h} - 1}{1} = \\frac{2 - (2-h)}{2-h} = \\frac{h}{2-h}\n$$\nFinally, we impose the accuracy requirement $\\varepsilon_{\\mathrm{rel}}(h) \\le \\epsilon$ for a given tolerance $\\epsilon \\in (0,1)$ and find the largest admissible step size $h_{\\max}$.\n$$\n\\frac{h}{2-h} \\le \\epsilon\n$$\nFor the discrete process to be stable, we must have $h < 2$, so the denominator $2-h$ is positive. We can multiply both sides by $2-h$ without changing the inequality's direction.\n$$\nh \\le \\epsilon(2-h) = 2\\epsilon - \\epsilon h\n$$\n$$\nh + \\epsilon h \\le 2\\epsilon\n$$\n$$\nh(1+\\epsilon) \\le 2\\epsilon\n$$\nSince $1+\\epsilon > 0$, we can divide to get:\n$$\nh \\le \\frac{2\\epsilon}{1+\\epsilon}\n$$\nThe function $\\varepsilon_{\\mathrm{rel}}(h)$ is monotonically increasing for $h \\in [0, 2)$. Therefore, the inequality holds for all $h$ up to the maximum value where equality is achieved. The largest admissible dimensionless step size is:\n$$\nh_{\\max} = \\frac{2\\epsilon}{1+\\epsilon}\n$$",
            "answer": "$$\n\\boxed{\\frac{2\\epsilon}{1+\\epsilon}}\n$$"
        },
        {
            "introduction": "While the Euler-Maruyama method is a versatile tool, it introduces discretization errors that depend on the chosen time step. For certain classes of SDEs, however, it is possible to bypass this issue entirely. This exercise explores the elegant case of the OU process, for which an exact analytical solution to the discrete-time update exists due to its linear nature . By deriving this exact simulation algorithm, you will not only discover a more accurate and efficient way to generate OU trajectories but also gain a deeper insight into the source of numerical bias in approximate schemes like Euler-Maruyama.",
            "id": "4006922",
            "problem": "In conductance-based synaptic noise modeling, a common choice for the stochastic dynamics of a fluctuating synaptic variable is the Ornstein–Uhlenbeck (OU) process. Consider the linear stochastic differential equation (SDE)\n$$\ndX_t = \\frac{\\mu - X_t}{\\tau}\\,dt + \\sigma\\, dW_t,\n$$\nwhere $X_t$ is the synaptic variable, $\\mu$ is a constant mean, $\\tau$ is a positive time constant, $\\sigma$ is a positive noise amplitude, and $W_t$ is a standard Wiener process (Brownian motion). Let $X_n \\equiv X_{t_n}$ be the value sampled on a uniform grid with step size $\\Delta t = t_{n+1} - t_n > 0$. Define $\\xi_n \\sim \\mathcal{N}(0,1)$ as independent standard normal random variables.\n\nFrom fundamental properties of linear SDEs and Itô integrals, the one-step transition $X_{n} \\to X_{n+1}$ over a time interval $\\Delta t$ is Gaussian with a mean that relaxes toward $\\mu$ and a variance determined by the filtered white-noise drive. The Euler–Maruyama (EM) method is the standard first-order explicit time-stepping scheme for SDEs.\n\nWhich option correctly gives the exact discrete-time update mapping $X_n \\mapsto X_{n+1}$ (i.e., the exact one-step transition in distribution) for the OU process above, and correctly compares its discretization bias at grid times to that of the Euler–Maruyama scheme in terms of the stationary variance?\n\nA. \n$$\nX_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\frac{\\tau}{2}\\Big(1 - e^{-2\\Delta t/\\tau}\\Big)}\\, \\xi_n.\n$$\nRelative to the exact scheme, the Euler–Maruyama stationary variance is biased upward by order $O(\\Delta t)$ (larger than the exact $\\sigma^2 \\tau/2$), whereas the exact update reproduces the stationary variance exactly for any $\\Delta t > 0$.\n\nB.\n$$\nX_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\frac{1 - e^{-\\Delta t/\\tau}}{\\tau}}\\, \\xi_n.\n$$\nEuler–Maruyama produces no bias in stationary mean or variance at any finite $\\Delta t > 0$.\n\nC.\n$$\nX_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\Delta t}\\, \\xi_n.\n$$\nEuler–Maruyama’s stationary variance is biased downward by order $O(\\Delta t^2)$ relative to the exact $\\sigma^2 \\tau/2$.\n\nD.\n$$\nX_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\tau\\Big(1 - e^{-2\\Delta t/\\tau}\\Big)}\\, \\xi_n.\n$$\nEuler–Maruyama’s stationary variance is biased downward by order $O(\\Delta t)$, while the exact update has a stationary mean biased by order $O(\\Delta t)$ but variance exact.",
            "solution": "The problem requires us to find the exact discrete-time update rule for an Ornstein-Uhlenbeck (OU) process and to analyze the discretization bias of the Euler-Maruyama (EM) scheme's stationary variance.\n\nThe Ornstein-Uhlenbeck process is described by the linear stochastic differential equation (SDE):\n$$\ndX_t = \\frac{\\mu - X_t}{\\tau}\\,dt + \\sigma\\,dW_t\n$$\nThis equation can be rewritten as:\n$$\ndX_t + \\frac{1}{\\tau} X_t \\,dt = \\frac{\\mu}{\\tau}\\,dt + \\sigma\\,dW_t\n$$\nThis is a first-order linear inhomogeneous SDE. We can solve it using an integrating factor, which for the term $\\frac{1}{\\tau}X_t$ is $I(t) = e^{\\int (1/\\tau) dt} = e^{t/\\tau}$.\nMultiplying the SDE by the integrating factor $e^{t/\\tau}$ allows us to write the left side as a total differential using the Itô product rule: $d(f(t)X_t) = f'(t)X_t dt + f(t)dX_t$.\n$$\ne^{t/\\tau} dX_t + \\frac{1}{\\tau} e^{t/\\tau} X_t \\,dt = d(X_t e^{t/\\tau})\n$$\nSo, the SDE becomes:\n$$\nd(X_t e^{t/\\tau}) = \\frac{\\mu}{\\tau} e^{t/\\tau}\\,dt + \\sigma e^{t/\\tau}\\,dW_t\n$$\nWe integrate this equation from time $t_n$ to $t_{n+1} = t_n + \\Delta t$:\n$$\n\\int_{t_n}^{t_{n+1}} d(X_s e^{s/\\tau}) = \\int_{t_n}^{t_{n+1}} \\frac{\\mu}{\\tau} e^{s/\\tau}\\,ds + \\int_{t_n}^{t_{n+1}} \\sigma e^{s/\\tau}\\,dW_s\n$$\n$$\nX_{t_{n+1}}e^{t_{n+1}/\\tau} - X_{t_n}e^{t_n/\\tau} = \\frac{\\mu}{\\tau} \\left[ \\tau e^{s/\\tau} \\right]_{t_n}^{t_{n+1}} + \\sigma \\int_{t_n}^{t_{n+1}} e^{s/\\tau}\\,dW_s\n$$\n$$\nX_{t_{n+1}}e^{t_{n+1}/\\tau} - X_{t_n}e^{t_n/\\tau} = \\mu (e^{t_{n+1}/\\tau} - e^{t_n/\\tau}) + \\sigma \\int_{t_n}^{t_{n+1}} e^{s/\\tau}\\,dW_s\n$$\nTo solve for $X_{t_{n+1}}$, we multiply the entire equation by $e^{-t_{n+1}/\\tau}$:\n$$\nX_{t_{n+1}} - X_{t_n}e^{(t_n-t_{n+1})/\\tau} = \\mu(1 - e^{(t_n-t_{n+1})/\\tau}) + \\sigma \\int_{t_n}^{t_{n+1}} e^{(s-t_{n+1})/\\tau}\\,dW_s\n$$\nSubstituting $t_{n+1} - t_n = \\Delta t$ and using the notation $X_n = X_{t_n}$:\n$$\nX_{n+1} = X_n e^{-\\Delta t/\\tau} + \\mu(1 - e^{-\\Delta t/\\tau}) + \\sigma \\int_{t_n}^{t_{n+1}} e^{(s-t_{n+1})/\\tau}\\,dW_s\n$$\nRearranging the terms:\n$$\nX_{n+1} = \\mu + (X_n - \\mu)e^{-\\Delta t/\\tau} + \\sigma \\int_{t_n}^{t_{n+1}} e^{(s-t_{n+1})/\\tau}\\,dW_s\n$$\nThe last term is an Itô integral of a deterministic function with respect to a Wiener process. This integral represents a Gaussian random variable with a mean of $0$ and a variance equal to the integral of the square of the deterministic function.\nThe variance of the stochastic term is:\n$$\nV = \\text{Var}\\left( \\sigma \\int_{t_n}^{t_{n+1}} e^{(s-t_{n+1})/\\tau}\\,dW_s \\right) = \\sigma^2 \\int_{t_n}^{t_{n+1}} \\left( e^{(s-t_{n+1})/\\tau} \\right)^2 ds = \\sigma^2 \\int_{t_n}^{t_{n+1}} e^{2(s-t_{n+1})/\\tau}\\,ds\n$$\nWe perform the integration:\n$$\nV = \\sigma^2 \\left[ \\frac{\\tau}{2} e^{2(s-t_{n+1})/\\tau} \\right]_{t_n}^{t_{n+1}} = \\frac{\\sigma^2 \\tau}{2} \\left( e^0 - e^{2(t_n-t_{n+1})/\\tau} \\right) = \\frac{\\sigma^2 \\tau}{2} \\left( 1 - e^{-2\\Delta t/\\tau} \\right)\n$$\nThe stochastic term is thus a Gaussian random variable with mean $0$ and variance $V$. We can write it as $\\sqrt{V}\\xi_n$, where $\\xi_n \\sim \\mathcal{N}(0,1)$.\nThe exact discrete-time update rule is therefore:\n$$\nX_{n+1} = \\mu + (X_n - \\mu) e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\frac{\\tau}{2}\\left(1 - e^{-2\\Delta t/\\tau}\\right)}\\, \\xi_n\n$$\nThis is an autoregressive process of order $1$ (AR(1)). Its stationary mean $E[X_\\infty]$ and variance $\\text{Var}(X_\\infty)$ can be calculated. From $E[X_{n+1}] = E[X_n] = E[X_\\infty]$, we get $E[X_\\infty] = \\mu$. From $\\text{Var}(X_{n+1}) = \\text{Var}(X_n) = \\text{Var}(X_\\infty)$, we get:\n$$\n\\text{Var}(X_\\infty) = (e^{-\\Delta t/\\tau})^2 \\text{Var}(X_\\infty) + \\frac{\\sigma^2 \\tau}{2} \\left( 1 - e^{-2\\Delta t/\\tau} \\right)\n$$\n$$\n\\text{Var}(X_\\infty) (1 - e^{-2\\Delta t/\\tau}) = \\frac{\\sigma^2 \\tau}{2} \\left( 1 - e^{-2\\Delta t/\\tau} \\right) \\implies \\text{Var}(X_\\infty) = \\frac{\\sigma^2 \\tau}{2}\n$$\nThe exact update scheme reproduces the stationary mean $\\mu$ and stationary variance $\\sigma^2 \\tau/2$ of the continuous OU process exactly for any time step $\\Delta t > 0$.\n\nNext, we analyze the Euler-Maruyama (EM) scheme. The EM discretization of the SDE is:\n$$\nX_{n+1} = X_n + \\frac{\\mu - X_n}{\\tau}\\Delta t + \\sigma \\sqrt{\\Delta t}\\,\\xi_n\n$$\nRearranging gives another AR(1) process:\n$$\nX_{n+1} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)X_n + \\frac{\\mu \\Delta t}{\\tau} + \\sigma \\sqrt{\\Delta t}\\,\\xi_n\n$$\nFor this scheme to be stable, we require $|1 - \\Delta t/\\tau| < 1$, which implies $0 < \\Delta t < 2\\tau$.\nThe stationary mean $E[X_\\infty]_{EM}$ is:\n$$\nE[X_\\infty]_{EM} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)E[X_\\infty]_{EM} + \\frac{\\mu \\Delta t}{\\tau} \\implies E[X_\\infty]_{EM}\\left(\\frac{\\Delta t}{\\tau}\\right) = \\frac{\\mu \\Delta t}{\\tau} \\implies E[X_\\infty]_{EM} = \\mu\n$$\nThe EM scheme reproduces the stationary mean exactly (if stable).\nThe stationary variance $\\text{Var}(X_\\infty)_{EM}$ is:\n$$\n\\text{Var}(X_\\infty)_{EM} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2 \\text{Var}(X_\\infty)_{EM} + \\text{Var}(\\sigma \\sqrt{\\Delta t}\\,\\xi_n)\n$$\n$$\n\\text{Var}(X_\\infty)_{EM}\\left(1 - \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2\\right) = \\sigma^2 \\Delta t\n$$\n$$\n\\text{Var}(X_\\infty)_{EM}\\left(1 - \\left(1 - \\frac{2\\Delta t}{\\tau} + \\frac{\\Delta t^2}{\\tau^2}\\right)\\right) = \\sigma^2 \\Delta t\n$$\n$$\n\\text{Var}(X_\\infty)_{EM}\\left(\\frac{2\\Delta t}{\\tau} - \\frac{\\Delta t^2}{\\tau^2}\\right) = \\sigma^2 \\Delta t\n$$\n$$\n\\text{Var}(X_\\infty)_{EM} = \\frac{\\sigma^2 \\Delta t}{\\frac{\\Delta t}{\\tau}\\left(2 - \\frac{\\Delta t}{\\tau}\\right)} = \\frac{\\sigma^2 \\tau}{2 - \\frac{\\Delta t}{\\tau}}\n$$\nComparing the EM stationary variance to the exact variance $V_{exact} = \\sigma^2\\tau/2$:\nSince $\\Delta t/\\tau > 0$, the denominator $2 - \\Delta t/\\tau < 2$, so $\\text{Var}(X_\\infty)_{EM} > \\sigma^2\\tau/2$. The EM scheme overestimates the variance, producing an upward bias.\nTo find the order of the bias for small $\\Delta t$, we perform a Taylor expansion:\n$$\n\\text{Var}(X_\\infty)_{EM} = \\frac{\\sigma^2 \\tau}{2} \\left( \\frac{1}{1 - \\frac{\\Delta t}{2\\tau}} \\right) \\approx \\frac{\\sigma^2 \\tau}{2} \\left( 1 + \\frac{\\Delta t}{2\\tau} + O(\\Delta t^2) \\right) = \\frac{\\sigma^2 \\tau}{2} + \\frac{\\sigma^2 \\Delta t}{4} + O(\\Delta t^2)\n$$\nThe bias is $\\text{Var}(X_\\infty)_{EM} - V_{exact} = \\frac{\\sigma^2 \\Delta t}{4} + O(\\Delta t^2)$, which is an upward bias of order $O(\\Delta t)$.\n\nNow we evaluate the given options:\n\n**A.** The update formula is $X_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\frac{\\tau}{2}(1 - e^{-2\\Delta t/\\tau})}\\, \\xi_n$. This matches our derived exact update rule. The statement claims the EM stationary variance is biased upward by order $O(\\Delta t)$ and is larger than the exact variance $\\sigma^2 \\tau/2$, while the exact scheme reproduces the variance exactly. This matches our analysis completely.\n**Verdict: Correct**\n\n**B.** The update formula is $X_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\frac{1 - e^{-\\Delta t/\\tau}}{\\tau}}\\, \\xi_n$. The noise term is incorrect. The variance of this noise term, $\\sigma^2 (1 - e^{-\\Delta t/\\tau})/\\tau$, is dimensionally inconsistent (units of $[X]^2/\\text{time}^2$ instead of $[X]^2$). The statement claims EM has no bias in variance, which is false.\n**Verdict: Incorrect**\n\n**C.** The update formula is $X_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\Delta t}\\, \\xi_n$. This formula incorrectly combines the drift term from the exact solution with the diffusion term from the EM scheme. It is not the exact update. The statement claims a downward bias of order $O(\\Delta t^2)$ for the EM variance, which is false; the bias is upward and $O(\\Delta t)$.\n**Verdict: Incorrect**\n\n**D.** The update formula is $X_{n+1} = \\mu + (X_n - \\mu)\\, e^{-\\Delta t/\\tau} + \\sigma \\sqrt{\\tau(1 - e^{-2\\Delta t/\\tau})}\\, \\xi_n$. This is incorrect, as the noise variance is twice the correct value (missing a factor of $1/2$ inside the square root). The statement claims EM variance is biased downward (false) and the exact update has a biased mean (false).\n**Verdict: Incorrect**\n\nBased on the detailed derivation and analysis, only option A is correct in all aspects.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Having mastered both approximate and exact simulation techniques, we now turn to a core challenge in computational neuroscience: connecting our theoretical models to experimental data. This practice places you in the role of a quantitative researcher tasked with inferring the parameters of a latent OU process from realistic, noisy observations . You will derive estimators for the process parameters ($\\mu$, $\\sigma$, and $\\tau$) by carefully dissecting a set of mock experimental measurements that include confounding factors like recording noise and probabilistic synaptic release. This exercise is a practical lesson in statistical inference and demonstrates how the abstract properties of the OU process can be used to interpret and quantify real-world biophysical phenomena.",
            "id": "4006846",
            "problem": "You are given a modeling task in the context of synaptic noise represented by an Ornstein-Uhlenbeck process in brain modeling and computational neuroscience. Miniature Excitatory Postsynaptic Currents (mEPSCs) are modeled as arising from a latent synaptic conductance process with recording noise. Use first-principles to derive an estimator for the Ornstein-Uhlenbeck parameters based on observable distributional statistics and release probability.\n\nStart from the following base and assumptions:\n\n- The latent synaptic conductance $g(t)$ obeys the Ornstein-Uhlenbeck stochastic differential equation\n$$\n\\mathrm{d}g(t) = -\\frac{1}{\\tau}\\left(g(t)-\\mu\\right)\\mathrm{d}t + \\sigma\\,\\mathrm{d}W(t),\n$$\nwhere $W(t)$ is a standard Wiener process (Brownian motion), and $\\mu$, $\\sigma$, $\\tau$ are constant parameters. The steady-state distribution of $g(t)$ is Gaussian with mean $\\mu$ and variance that depends on $\\sigma$ and $\\tau$.\n- At each trial, a release event occurs with probability $p$ independently across trials. Let $I\\in\\{0,1\\}$ be the Bernoulli indicator with $\\mathbb{P}(I=1)=p$. Conditional on $I=1$, the observed mEPSC amplitude is modeled as $X=G+\\varepsilon$, and conditional on $I=0$, as $X=\\varepsilon$, where $G$ is a draw from the steady-state of the Ornstein-Uhlenbeck process and $\\varepsilon$ is independent additive recording noise.\n- The recording noise $\\varepsilon$ is modeled as Gaussian with zero mean and known variance $v_n$, independent across trials and independent of $G$.\n- Among trials with $I=1$, the amplitudes on consecutive trials separated by inter-trial interval $\\Delta t$ exhibit correlation due to the autocorrelation of the latent Ornstein-Uhlenbeck process and additive noise.\n\nYou are provided, for each test case:\n- The release probability $p$ (unitless, expressed as a decimal).\n- The recording noise variance $v_n$ in picoampere-squared ($\\text{pA}^2$).\n- The inter-trial interval $\\Delta t$ in milliseconds (ms).\n- The observed amplitude mean $m_{\\mathrm{obs}}$ in picoamperes (pA) across all trials.\n- The observed amplitude variance $s^2_{\\mathrm{obs}}$ in picoampere-squared ($\\text{pA}^2$) across all trials.\n- The observed conditional Pearson correlation $r_{\\mathrm{obs}}$ (unitless) between amplitudes of two consecutive trials, computed only on pairs where both trials had a release event ($I=1$ in both).\n\nYour task:\n1. Derive, from the stated assumptions and the Ornstein-Uhlenbeck steady-state properties and independence structure, expressions linking the observable statistics $m_{\\mathrm{obs}}$, $s^2_{\\mathrm{obs}}$, $r_{\\mathrm{obs}}$ to the parameters $\\mu$, $\\sigma$, and $\\tau$, explicitly accounting for the mixture due to stochastic release and the additive recording noise.\n2. Use those derived relations to compute parameter estimates $(\\widehat{\\mu},\\widehat{\\sigma},\\widehat{\\tau})$ from $(p,v_n,\\Delta t,m_{\\mathrm{obs}},s^2_{\\mathrm{obs}},r_{\\mathrm{obs}})$.\n\nUnits and output:\n- Report $\\widehat{\\mu}$ in picoamperes (pA).\n- Report $\\widehat{\\sigma}$ in picoamperes per square root millisecond ($\\text{pA}/\\sqrt{\\text{ms}}$).\n- Report $\\widehat{\\tau}$ in milliseconds (ms).\n- For numerical output, do not include unit strings; values must be floats rounded to six decimal places.\n\nTest suite:\nCompute $(\\widehat{\\mu},\\widehat{\\sigma},\\widehat{\\tau})$ for each of the following parameter sets. All quantities are given in appropriate units as specified above, with numbers written in decimal form.\n\n- Case 1: $p=0.35$, $v_n=9$, $\\Delta t=2$, $m_{\\mathrm{obs}}=4.2$, $s^2_{\\mathrm{obs}}=58.56$, $r_{\\mathrm{obs}}=0.603108$.\n- Case 2: $p=0.05$, $v_n=4$, $\\Delta t=5$, $m_{\\mathrm{obs}}=0.4$, $s^2_{\\mathrm{obs}}=9.29$, $r_{\\mathrm{obs}}=0.557019$.\n- Case 3: $p=0.95$, $v_n=1$, $\\Delta t=1$, $m_{\\mathrm{obs}}=4.75$, $s^2_{\\mathrm{obs}}=7.8875$, $r_{\\mathrm{obs}}=0.614169$.\n- Case 4: $p=0.4$, $v_n=100$, $\\Delta t=4$, $m_{\\mathrm{obs}}=6.0$, $s^2_{\\mathrm{obs}}=211.6$, $r_{\\mathrm{obs}}=0.358000$.\n- Case 5: $p=0.5$, $v_n=10$, $\\Delta t=20$, $m_{\\mathrm{obs}}=3.5$, $s^2_{\\mathrm{obs}}=27.25$, $r_{\\mathrm{obs}}=0.183940$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of three floats $[\\widehat{\\mu},\\widehat{\\sigma},\\widehat{\\tau}]$ in the order specified, for the five cases above. For example, the final output format must be like\n`[[\\widehat{\\mu}_1,\\widehat{\\sigma}_1,\\widehat{\\tau}_1],[\\widehat{\\mu}_2,\\widehat{\\sigma}_2,\\dots]]`\nwith all values rounded to six decimal places and no unit strings included.",
            "solution": "The task is to derive estimators for the parameters $(\\mu, \\sigma, \\tau)$ of an Ornstein-Uhlenbeck (OU) process from a set of observable statistics. The derivation proceeds from first principles based on the provided model assumptions.\n\nThe latent synaptic conductance $g(t)$ is described by the Ornstein-Uhlenbeck stochastic differential equation:\n$$\n\\mathrm{d}g(t) = -\\frac{1}{\\tau}\\left(g(t)-\\mu\\right)\\mathrm{d}t + \\sigma\\,\\mathrm{d}W(t)\n$$\nThis process is foundational. Its key statistical properties in the steady state are:\n1.  The stationary distribution of $g(t)$, which we denote by the random variable $G$, is a Gaussian distribution, $G \\sim \\mathcal{N}(\\mu, v_g)$.\n2.  The mean of this distribution is $\\mathbb{E}[G] = \\mu$.\n3.  The variance, $v_g = \\mathrm{Var}(G)$, is a function of the process parameters: $v_g = \\frac{\\sigma^2 \\tau}{2}$.\n4.  The autocorrelation function for a time lag $\\Delta t$ is $\\mathrm{Corr}(g(t), g(t+\\Delta t)) = e^{-\\Delta t/\\tau}$, which gives the autocovariance as $\\mathrm{Cov}(g(t), g(t+\\Delta t)) = \\mathrm{Var}(G) e^{-\\Delta t/\\tau} = v_g e^{-\\Delta t/\\tau}$.\n\nThe observed mEPSC amplitude, $X$, is modeled as a mixture reflecting stochastic synaptic release. A release occurs with probability $p$, indicated by a Bernoulli variable $I \\sim \\mathrm{Bernoulli}(p)$. The observed amplitude is $X = G + \\varepsilon$ if a release occurs ($I=1$), and $X = \\varepsilon$ if it fails ($I=0$). The recording noise $\\varepsilon$ is independent of the signal $G$ and follows a Gaussian distribution $\\varepsilon \\sim \\mathcal{N}(0, v_n)$. This can be written compactly as $X = I \\cdot G + \\varepsilon$.\n\nWe derive the estimators by relating the theoretical moments of $X$ to the observed statistics ($m_{\\mathrm{obs}}$, $s^2_{\\mathrm{obs}}$, $r_{\\mathrm{obs}}$).\n\n**Step 1: Derivation of the Estimator for Mean Conductance $\\mu$**\n\nWe relate the observed mean amplitude $m_{\\mathrm{obs}}$ to $\\mu$. Using the law of total expectation on $X = I \\cdot G + \\varepsilon$:\n$$\nm_{\\mathrm{obs}} = \\mathbb{E}[X] = \\mathbb{E}[I \\cdot G + \\varepsilon]\n$$\nDue to the independence of $I$, $G$, and $\\varepsilon$, and $\\mathbb{E}[\\varepsilon]=0$:\n$$\nm_{\\mathrm{obs}} = \\mathbb{E}[I] \\mathbb{E}[G] + \\mathbb{E}[\\varepsilon] = p \\cdot \\mu + 0 = p\\mu\n$$\nThis provides a direct relationship. Solving for $\\mu$, we get the estimator:\n$$\n\\widehat{\\mu} = \\frac{m_{\\mathrm{obs}}}{p}\n$$\n\n**Step 2: Derivation for the Latent Variance $v_g$**\n\nWe relate the observed variance $s^2_{\\mathrm{obs}}$ to the latent variance $v_g$. Using the law of total variance for $X$:\n$$\ns^2_{\\mathrm{obs}} = \\mathrm{Var}(X) = \\mathbb{E}[\\mathrm{Var}(X|I)] + \\mathrm{Var}(\\mathbb{E}[X|I])\n$$\nWe compute each term separately.\nFor the first term, $\\mathbb{E}[\\mathrm{Var}(X|I)]$:\n-   Conditional on $I=1$: $X = G+\\varepsilon$. Since $G$ and $\\varepsilon$ are independent, $\\mathrm{Var}(X|I=1) = \\mathrm{Var}(G) + \\mathrm{Var}(\\varepsilon) = v_g + v_n$.\n-   Conditional on $I=0$: $X = \\varepsilon$. Thus, $\\mathrm{Var}(X|I=0) = \\mathrm{Var}(\\varepsilon) = v_n$.\nThe expectation is therefore:\n$$\n\\mathbb{E}[\\mathrm{Var}(X|I)] = \\mathrm{Var}(X|I=1)\\cdot p + \\mathrm{Var}(X|I=0)\\cdot(1-p) = (v_g + v_n)p + v_n(1-p) = p v_g + v_n\n$$\nFor the second term, $\\mathrm{Var}(\\mathbb{E}[X|I])$:\n-   Conditional on $I=1$: $\\mathbb{E}[X|I=1] = \\mathbb{E}[G+\\varepsilon] = \\mu+0 = \\mu$.\n-   Conditional on $I=0$: $\\mathbb{E}[X|I=0] = \\mathbb{E}[\\varepsilon] = 0$.\nThe random variable $\\mathbb{E}[X|I]$ takes the value $\\mu$ with probability $p$ and $0$ with probability $1-p$. The variance of this Bernoulli-like variable is:\n$$\n\\mathrm{Var}(\\mathbb{E}[X|I]) = \\mathbb{E}[(\\mathbb{E}[X|I])^2] - (\\mathbb{E}[\\mathbb{E}[X|I]])^2 = (\\mu^2 p + 0^2(1-p)) - (p\\mu)^2 = p\\mu^2 - p^2\\mu^2 = p(1-p)\\mu^2\n$$\nCombining the two terms, the total variance is:\n$$\ns^2_{\\mathrm{obs}} = (p v_g + v_n) + p(1-p)\\mu^2\n$$\nSolving for $v_g$, we obtain the estimator $\\widehat{v_g}$ by substituting $\\widehat{\\mu}$:\n$$\n\\widehat{v_g} = \\frac{s^2_{\\mathrm{obs}} - v_n - p(1-p)\\widehat{\\mu}^2}{p} = \\frac{1}{p}\\left(s^2_{\\mathrm{obs}} - v_n - p(1-p)\\left(\\frac{m_{\\mathrm{obs}}}{p}\\right)^2\\right) = \\frac{1}{p}\\left(s^2_{\\mathrm{obs}} - v_n - \\frac{1-p}{p}m_{\\mathrm{obs}}^2\\right)\n$$\n\n**Step 3: Derivation of the Estimator for Time Constant $\\tau$**\n\nThe observed correlation $r_{\\mathrm{obs}}$ links to $\\tau$. This correlation is computed between amplitudes of two consecutive trials, say $X_1$ and $X_2$, separated by $\\Delta t$, conditional on both trials having a release event ($I_1=1, I_2=1$).\nLet $X'_1 = X_1|_{I_1=1} = G_1+\\varepsilon_1$ and $X'_2 = X_2|_{I_2=1} = G_2+\\varepsilon_2$.\n$$\nr_{\\mathrm{obs}} = \\mathrm{Corr}(X'_1, X'_2) = \\frac{\\mathrm{Cov}(X'_1, X'_2)}{\\sqrt{\\mathrm{Var}(X'_1)\\mathrm{Var}(X'_2)}}\n$$\nThe variance in the denominator is $\\mathrm{Var}(X'_1) = \\mathrm{Var}(G_1+\\varepsilon_1) = \\mathrm{Var}(G_1)+\\mathrm{Var}(\\varepsilon_1) = v_g + v_n$.\nThe covariance in the numerator is $\\mathrm{Cov}(G_1+\\varepsilon_1, G_2+\\varepsilon_2)$. Given that $\\varepsilon_1, \\varepsilon_2$ are independent of each other and of the latent process $G$, this simplifies to:\n$$\n\\mathrm{Cov}(G_1, G_2) + \\mathrm{Cov}(G_1, \\varepsilon_2) + \\mathrm{Cov}(\\varepsilon_1, G_2) + \\mathrm{Cov}(\\varepsilon_1, \\varepsilon_2) = \\mathrm{Cov}(G_1, G_2) = v_g e^{-\\Delta t/\\tau}\n$$\nSubstituting these into the correlation formula:\n$$\nr_{\\mathrm{obs}} = \\frac{v_g e^{-\\Delta t/\\tau}}{v_g + v_n}\n$$\nWe solve for $\\tau$ by first isolating the exponential term:\n$$\ne^{-\\Delta t/\\tau} = r_{\\mathrm{obs}} \\frac{v_g + v_n}{v_g} = r_{\\mathrm{obs}}\\left(1 + \\frac{v_n}{v_g}\\right)\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{\\Delta t}{\\tau} = \\ln\\left(r_{\\mathrm{obs}}\\left(1 + \\frac{v_n}{v_g}\\right)\\right)\n$$\nThis gives the estimator for $\\tau$:\n$$\n\\widehat{\\tau} = -\\frac{\\Delta t}{\\ln\\left(r_{\\mathrm{obs}}\\left(1 + \\frac{v_n}{\\widehat{v_g}}\\right)\\right)}\n$$\n\n**Step 4: Derivation of the Estimator for Noise Amplitude $\\sigma$**\n\nFinally, we use the definitional relationship $v_g = \\frac{\\sigma^2 \\tau}{2}$ to find $\\sigma$. Rearranging the formula yields $\\sigma^2 = \\frac{2v_g}{\\tau}$. Taking the square root gives the estimator for $\\sigma$:\n$$\n\\widehat{\\sigma} = \\sqrt{\\frac{2\\widehat{v_g}}{\\widehat{\\tau}}}\n$$\nThe parameter $\\sigma$ must be non-negative, so we take the positive root. This completes the set of derivations for all three unknown parameters.\n\nThe computational procedure is to apply these four derived formulae sequentially, using the provided observable statistics for each test case.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes Ornstein-Uhlenbeck process parameters from observable statistics.\n    \"\"\"\n    # Test cases defined as tuples of (p, v_n, Delta_t, m_obs, s^2_obs, r_obs)\n    test_cases = [\n        # p, v_n, Delta_t, m_obs, s^2_obs, r_obs\n        (0.35, 9.0, 2.0, 4.2, 58.56, 0.603108),\n        (0.05, 4.0, 5.0, 0.4, 9.29, 0.557019),\n        (0.95, 1.0, 1.0, 4.75, 7.8875, 0.614169),\n        (0.4, 100.0, 4.0, 6.0, 211.6, 0.358000),\n        (0.5, 10.0, 20.0, 3.5, 27.25, 0.183940),\n    ]\n\n    results_as_lists = []\n    for case in test_cases:\n        p, v_n, delta_t, m_obs, s2_obs, r_obs = case\n\n        # Step 1: Estimate mu\n        # mu_hat = m_obs / p\n        if p == 0:\n            # Avoid division by zero, although not expected in these test cases\n            # Based on validation, p > 0 for all cases.\n            mu_hat = float('nan') \n        else:\n            mu_hat = m_obs / p\n\n        # Step 2: Estimate latent variance v_g\n        # v_g_hat = (s^2_obs - v_n - p*(1-p)*mu_hat^2) / p\n        # which simplifies to avoid re-using mu_hat:\n        # v_g_hat = (s^2_obs - v_n - ((1-p)/p)*m_obs^2) / p\n        if p == 0:\n            v_g_hat = float('nan')\n        else:\n            term_mu_sq = ((1 - p) / p) * (m_obs ** 2)\n            v_g_hat = (s2_obs - v_n - term_mu_sq) / p\n\n        # Step 3: Estimate time constant tau\n        # tau_hat = -delta_t / ln(r_obs * (1 + v_n / v_g_hat))\n        if v_g_hat = 0:\n            # Physical constraint: variance must be positive.\n            tau_hat = float('nan')\n        else:\n            log_arg = r_obs * (1 + v_n / v_g_hat)\n            if log_arg = 0 or log_arg >= 1:\n                # Log argument must be in (0, 1) for a positive, finite tau.\n                tau_hat = float('nan') \n            else:\n                tau_hat = -delta_t / np.log(log_arg)\n\n        # Step 4: Estimate noise amplitude sigma\n        # sigma_hat = sqrt(2 * v_g_hat / tau_hat)\n        if tau_hat = 0 or v_g_hat  0:\n            sigma_hat = float('nan')\n        else:\n            sigma_hat = math.sqrt(2 * v_g_hat / tau_hat)\n\n        # Append results in specified order (mu, sigma, tau), rounded to 6 decimal places.\n        results_as_lists.append([\n            round(mu_hat, 6),\n            round(sigma_hat, 6),\n            round(tau_hat, 6)\n        ])\n        \n    # Format the final output string as a list of lists.\n    # The map(str, ...) will call str() on each sublist, e.g., str([1.0, 2.0, 3.0]) -> '[1.0, 2.0, 3.0]'\n    # Joining these with a comma and enclosing in brackets produces the required format.\n    # To match the example format exactly (no spaces after commas), we build the strings manually.\n    result_strings = []\n    for sublist in results_as_lists:\n        # Convert each float to string, with the required precision.\n        # Although round() was used, printing ensures trailing zeros if needed.\n        str_vals = [f\"{val:.6f}\" for val in sublist]\n        result_strings.append(f\"[{','.join(str_vals)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}