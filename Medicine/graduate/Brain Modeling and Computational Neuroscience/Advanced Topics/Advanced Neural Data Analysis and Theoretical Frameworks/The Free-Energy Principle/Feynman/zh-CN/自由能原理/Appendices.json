{
    "hands_on_practices": [
        {
            "introduction": "最小化变分自由能不仅仅是一个数学优化过程，它体现了奥卡姆剃刀原理，即在精确解释数据与保持信念简单性之间寻求平衡。本练习  将让您解析地剖析自由能的构成部分——准确性（accuracy）和复杂性（complexity）。通过计算学习前后的这些项，您将直观地理解智能体如何通过付出“复杂性代价”来获得一个更准确的世界模型。",
            "id": "4028543",
            "problem": "考虑一个在脑模型和计算神经科学中用于感知的自由能原理下的单潜态生成模型。设潜态为 $s \\in \\mathbb{R}$，其高斯先验为 $p(s) = \\mathcal{N}(s; \\mu_{0}, \\sigma_{0}^{2})$，似然为 $p(o \\mid s) = \\mathcal{N}(o; s, \\sigma_{o}^{2})$，其中 $o \\in \\mathbb{R}$ 是一个观测数据点。假设一个平均场变分后验 $q(s) = \\mathcal{N}(s; \\mu_{q}, \\sigma_{q}^{2})$。变分自由能由以下基本恒等式定义\n$$\nF[q] = \\mathbb{E}_{q}[\\ln q(s)] - \\mathbb{E}_{q}[\\ln p(o,s)],\n$$\n并可通过因子分解 $p(o,s) = p(o \\mid s)p(s)$ 分解为一个准确度项和一个复杂度项。复杂度是Kullback–Leibler (KL) 散度 $D_{\\mathrm{KL}}(q(s) \\| p(s))$，准确度是期望对数似然 $\\mathbb{E}_{q}[\\ln p(o \\mid s)]$。从上述定义出发，并且只使用从这些定义中得出的高斯恒等式，推导此共轭高斯模型中准确度和复杂度分量的闭式表达式。\n\n现在考虑一个具体的、科学上现实的设定，其参数为 $\\mu_{0} = 0$，$\\sigma_{0}^{2} = 1$，$\\sigma_{o}^{2} = \\frac{1}{4}$，以及单个观测数据 $o = 1$。定义“学习前”为初始化 $q_{\\text{before}}(s) = p(s)$，“学习后”为通过在高斯族上最小化 $F[q]$ 得到的精确高斯后验 $q_{\\text{after}}(s) = p(s \\mid o)$。计算 $q_{\\text{before}}$ 和 $q_{\\text{after}}$ 的准确度和复杂度。\n\n以单个行矩阵的形式提供你的最终答案，其中包含由你的推导指定的精确解析形式的四个量，顺序如下：\n$$\n\\big[\\text{学习前准确度}, \\ \\text{学习前复杂度}, \\ \\text{学习后准确度}, \\ \\text{学习后复杂度}\\big].\n$$\n用精确解析形式表示你的答案（不要近似或四舍五入）。最后，简要解释奥卡姆权衡（Occam trade-off），说明准确度和复杂度的变化如何解释“学习前”与“学习后”之间变分自由能 $F[q]$ 的变化。",
            "solution": "该问题要求在一个简单的变分自由能框架内，针对一个高斯生成模型，推导准确度和复杂度项，然后计算这两个量在两个特定状态下的值：“学习前”（$q_{\\text{before}}(s) = p(s)$）和“学习后”（$q_{\\text{after}}(s) = p(s \\mid o)$）。\n\n生成模型的规定如下：\n- 潜态 $s$ 的高斯先验：$p(s) = \\mathcal{N}(s; \\mu_{0}, \\sigma_{0}^{2})$\n- 观测 $o$ 的高斯似然：$p(o \\mid s) = \\mathcal{N}(o; s, \\sigma_{o}^{2})$\n\n假设近似后验来自同一族分布：$q(s) = \\mathcal{N}(s; \\mu_{q}, \\sigma_{q}^{2})$。\n\n变分自由能 $F[q]$ 分解为 $F[q] = \\text{复杂度} - \\text{准确度}$，其中：\n- 复杂度 $= D_{\\mathrm{KL}}(q(s) \\| p(s))$\n- 准确度 $= \\mathbb{E}_{q}[\\ln p(o \\mid s)]$\n\n我们首先推导这两项的一般闭式表达式。\n\n**1. 复杂度项的推导**\n\n复杂度是从先验 $p(s)$ 到变分后验 $q(s)$ 的 Kullback-Leibler (KL) 散度。对于两个高斯分布，其标准形式为：\n$$\n\\text{复杂度} = D_{\\mathrm{KL}}(q(s) \\| p(s)) = \\frac{1}{2} \\left( \\ln\\left(\\frac{\\sigma_{0}^{2}}{\\sigma_{q}^{2}}\\right) - 1 + \\frac{\\sigma_{q}^{2} + (\\mu_{q} - \\mu_{0})^{2}}{\\sigma_{0}^{2}} \\right)\n$$\n\n**2. 准确度项的推导**\n\n准确度是在变分后验下观测的期望对数似然。\n对数似然为 $\\ln p(o \\mid s) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{o}^{2}) - \\frac{(o - s)^{2}}{2\\sigma_{o}^{2}}$。\n对 $q(s)$ 取期望，得到：\n$$\n\\text{准确度} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{o}^{2}) - \\frac{(o - \\mu_{q})^{2} + \\sigma_{q}^{2}}{2\\sigma_{o}^{2}}\n$$\n\n**3. 应用于特定情况**\n\n问题给出的参数为：$\\mu_{0} = 0$，$\\sigma_{0}^{2} = 1$，$\\sigma_{o}^{2} = \\frac{1}{4}$，以及观测值 $o = 1$。\n\n**“学习前”状态：**\n在这里，变分分布被设为先验分布：$q_{\\text{before}}(s) = p(s)$。这意味着其参数为 $\\mu_{q} = \\mu_{0} = 0$ 和 $\\sigma_{q}^{2} = \\sigma_{0}^{2} = 1$。\n- **学习前的复杂度：**\n  由于 $q_{\\text{before}}(s) = p(s)$，根据定义，KL散度为零。\n  $$\n  \\text{学习前复杂度} = D_{\\mathrm{KL}}(p(s) \\| p(s)) = 0\n  $$\n- **学习前的准确度：**\n  我们使用推导出的准确度公式，并代入 $\\mu_{q} = 0$，$\\sigma_{q}^{2} = 1$，$o = 1$ 和 $\\sigma_{o}^{2} = \\frac{1}{4}$。\n  $$\n  \\text{学习前准确度} = -\\frac{1}{2}\\ln\\left(2\\pi\\left(\\frac{1}{4}\\right)\\right) - \\frac{(1 - 0)^{2} + 1}{2\\left(\\frac{1}{4}\\right)} = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - \\frac{2}{1/2} = -4 - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right)\n  $$\n\n**“学习后”状态：**\n在这里，变分分布是精确后验 $q_{\\text{after}}(s) = p(s \\mid o)$。对于这个共轭模型，后验也是高斯分布。我们通过结合先验和似然来找到其参数。\n后验精度是先验精度和似然精度之和：\n$$\n\\frac{1}{\\sigma_{q, \\text{after}}^{2}} = \\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma_{o}^{2}} = \\frac{1}{1} + \\frac{1}{1/4} = 1 + 4 = 5 \\implies \\sigma_{q, \\text{after}}^{2} = \\frac{1}{5}\n$$\n后验均值是精度加权的平均值：\n$$\n\\mu_{q, \\text{after}} = \\sigma_{q, \\text{after}}^{2} \\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{o}{\\sigma_{o}^{2}}\\right) = \\frac{1}{5} \\left(\\frac{0}{1} + \\frac{1}{1/4}\\right) = \\frac{1}{5}(0 + 4) = \\frac{4}{5}\n$$\n因此，$q_{\\text{after}}(s) = \\mathcal{N}(s; \\mu_{q} = \\frac{4}{5}, \\sigma_{q}^{2} = \\frac{1}{5})$。\n- **学习后的复杂度：**\n  使用复杂度公式，并代入 $\\mu_{q} = \\frac{4}{5}$，$\\sigma_{q}^{2} = \\frac{1}{5}$，$\\mu_{0} = 0$，$\\sigma_{0}^{2} = 1$。\n  $$\n  \\text{学习后复杂度} = \\frac{1}{2} \\left( \\ln\\left(\\frac{1}{1/5}\\right) - 1 + \\frac{1/5 + (4/5 - 0)^{2}}{1} \\right)\n  $$\n  $$\n  = \\frac{1}{2} \\left( \\ln(5) - 1 + \\frac{1}{5} + \\frac{16}{25} \\right) = \\frac{1}{2} \\left( \\ln(5) - \\frac{25}{25} + \\frac{5}{25} + \\frac{16}{25} \\right) = \\frac{1}{2} \\left( \\ln(5) - \\frac{4}{25} \\right) = \\frac{1}{2}\\ln(5) - \\frac{2}{25}\n  $$\n- **学习后的准确度：**\n  使用准确度公式，并代入 $\\mu_{q} = \\frac{4}{5}$，$\\sigma_{q}^{2} = \\frac{1}{5}$，$o = 1$ 和 $\\sigma_{o}^{2} = \\frac{1}{4}$。\n  $$\n  \\text{学习后准确度} = -\\frac{1}{2}\\ln\\left(2\\pi\\left(\\frac{1}{4}\\right)\\right) - \\frac{(1 - 4/5)^{2} + 1/5}{2(1/4)}\n  $$\n  $$\n  = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - \\frac{(1/5)^{2} + 1/5}{1/2} = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - 2\\left(\\frac{1}{25} + \\frac{5}{25}\\right) = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - 2\\left(\\frac{6}{25}\\right) = -\\frac{12}{25} - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right)\n  $$\n\n计算出的四个量是：\n$\\text{学习前准确度} = -4 - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right)$\n$\\text{学习前复杂度} = 0$\n$\\text{学习后准确度} = -\\frac{12}{25} - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right)$\n$\\text{学习后复杂度} = \\frac{1}{2}\\ln(5) - \\frac{2}{25}$\n\n**奥卡姆权衡的解释**\n自由能原理假设，像大脑这样的系统旨在最小化变分自由能 $F[q] = \\text{复杂度} - \\text{准确度}$。这种最小化过程涉及一种类似奥卡姆剃刀的权衡。复杂度项 $D_{\\mathrm{KL}}(q(s) \\| p(s))$ 对偏离先验信念 $p(s)$ 的后验信念 $q(s)$ 进行惩罚，从而倾向于简单性。准确度项 $\\mathbb{E}_{q}[\\ln p(o \\mid s)]$ 是数据的期望对数似然，它奖励那些能为感官观测提供良好解释的信念。最小化 $F[q]$ 等价于在最小化复杂度的同时最大化准确度。\n\n比较“学习前”和“学习后”的状态揭示了这种权衡的实际作用：\n- 学习前，信念等于先验（$q_{\\text{before}} = p(s)$）。这是可能的最“简单”状态，因为复杂度为零。然而，这个信念对于特定观测 $o=1$ 是一个糟糕的解释，导致准确度得分很低（一个大的负数）。\n- 学习后，系统通过整合来自观测的证据，将其信念更新为后验 $q_{\\text{after}} = p(s|o)$。这次更新使复杂度从 $0$ 增加到 $\\frac{1}{2}\\ln(5) - \\frac{2}{25} \\approx 0.725$。这个复杂度的“成本”是为了换取准确度的大幅提升而付出的。准确度得分从 $-4 - \\frac{1}{2}\\ln(\\frac{\\pi}{2})$ 增加到 $-\\frac{12}{25} - \\frac{1}{2}\\ln(\\frac{\\pi}{2})$，提升了 $4 - \\frac{12}{25} = \\frac{88}{25} = 3.52$。\n惊奇（surprise）的减少（即准确度的增加）远远超过了模型复杂度增加的成本。自由能的变化 $\\Delta F = \\Delta(\\text{复杂度}) - \\Delta(\\text{准确度})$ 因此为负，表明学习成功地最小化了系统对其感觉的总体惊奇。最终的信念状态 $q_{\\text{after}}$ 比初始状态更复杂，但它代表了在给定感官证据的情况下，对世界潜态的一个更准确的模型。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -4 - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) & 0 & -\\frac{12}{25} - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) & \\frac{1}{2}\\ln(5) - \\frac{2}{25} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "自由能原理的应用超越了被动感知，为我们提供了一个关于行动的统一理论，即主动推断（active inference）。在此框架下，智能体通过评估哪项策略能在未来预期中最小化自由能来选择行动。这个动手练习  将引导您计算预期自由能，从而展示智能体的决策是如何被实现偏好结果（实用价值）和减少对世界不确定性（认知价值）这两种动机共同驱动的。",
            "id": "4028595",
            "problem": "考虑一个单步决策问题，该问题是在大脑建模和计算神经科学中的自由能原理 (FEP) 下构建的。隐状态是二元的，$s \\in \\{s_{0}, s_{1}\\}$，其当前信念（识别密度）$Q(s)$ 由 $Q(s_{0}) = 0.6$ 和 $Q(s_{1}) = 0.4$ 给出。观测是二元的，$o \\in \\{0,1\\}$，并且有两个候选的单步策略 $\\pi_{1}$ 和 $\\pi_{2}$，分别对应于两个行动 $a_{1}$ 和 $a_{2}$，它们调节似然映射 $P(o \\mid s, a)$ 如下：\n- 在行动 $a_{1}$ 下：$P(o=1 \\mid s_{0}, a_{1}) = 0.8$，$P(o=0 \\mid s_{0}, a_{1}) = 0.2$，$P(o=1 \\mid s_{1}, a_{1}) = 0.3$，$P(o=0 \\mid s_{1}, a_{1}) = 0.7$。\n- 在行动 $a_{2}$ 下：$P(o=1 \\mid s_{0}, a_{2}) = 0.6$，$P(o=0 \\mid s_{0}, a_{2}) = 0.4$，$P(o=1 \\mid s_{1}, a_{2}) = 0.1$，$P(o=0 \\mid s_{1}, a_{2}) = 0.9$。\n\n偏好的结果由先验偏好分布 $P^{\\ast}(o)$ 编码，其中 $P^{\\ast}(1) = 0.9$ 和 $P^{\\ast}(0) = 0.1$。假设只有一个时间步，并且用于评估期望自由能的隐状态先验与当前信念相匹配，即 $P(s) = Q(s)$。使用自然对数 $\\ln$。\n\n从单个观测的变分自由能的定义出发（该定义涉及识别分布 $Q(s \\mid o, \\pi)$ 和带有结果偏好的生成模型 $P^{\\ast}(o)$），并仅使用变分贝叶斯推断的核心定义（例如，Kullback–Leibler 散度 (KLD)、互信息和全期望定律），推导一个策略 $\\pi$ 的单步期望自由能 $\\mathcal{G}(\\pi)$ 的表达式，该表达式可分为 (i) 一个依赖于 $P^{\\ast}(o)$ 和预测分布 $Q(o \\mid \\pi)$ 的外在项和 (ii) 一个是在 $\\pi$ 下隐状态和结果之间的互信息的认知项。然后，数值计算 $\\mathcal{G}(\\pi_{1})$ 和 $\\mathcal{G}(\\pi_{2})$，并确定哪个策略最小化 $\\mathcal{G}(\\pi)$。\n\n作为最终答案，报告最小化策略 $\\pi_{i}$ 的索引 $i \\in \\{1,2\\}$。最终索引无需舍入说明。如果需要中间数值计算，请使用精确算术和 $\\ln$ 求值；任何近似都应在您的推导中明确指出。",
            "solution": "本题要求我们计算并比较两个策略 $\\pi_1$ 和 $\\pi_2$ 的预期自由能 $\\mathcal{G}(\\pi)$，并选择预期自由能较小的策略。预期自由能 $\\mathcal{G}(\\pi)$ 分解为外在项（或实用价值）和认知项（或信息增益）。\n$$ \\mathcal{G}(\\pi) = \\underbrace{\\mathbb{E}_{Q(o|\\pi)}[-\\ln P^\\ast(o)]}_{\\text{外在项}} + \\underbrace{\\mathbb{E}_{Q(o|\\pi)}[D_{\\mathrm{KL}}(Q(s|o,\\pi) \\| Q(s))]}_{\\text{认知项}} $$\n\n**1. 策略 $\\pi_1$ 的计算**\n\n首先，计算在当前信念 $Q(s)$ 下，执行策略 $\\pi_1$（行动 $a_1$）后，观测到结果 $o$ 的预测分布 $Q(o|\\pi_1)$：\n$Q(o=1|\\pi_1) = P(o=1|s_0, a_1)Q(s_0) + P(o=1|s_1, a_1)Q(s_1) = (0.8)(0.6) + (0.3)(0.4) = 0.6$\n$Q(o=0|\\pi_1) = 1 - 0.6 = 0.4$\n\n**外在项 $\\mathcal{G}_E(\\pi_1)$** 是预期结果与偏好 $P^\\ast(o)$ 之间的交叉熵：\n$\\mathcal{G}_E(\\pi_1) = -[Q(o=1|\\pi_1)\\ln P^\\ast(1) + Q(o=0|\\pi_1)\\ln P^\\ast(0)] = -[0.6\\ln(0.9) + 0.4\\ln(0.1)] \\approx 0.9843$\n\n**认知项 $\\mathcal{G}_I(\\pi_1)$** 是预期的信息增益（互信息）：\n为此，先计算后验信念 $Q(s|o,\\pi_1) = P(o|s,a_1)Q(s)/Q(o|\\pi_1)$:\n- 若 $o=1$: $Q(s_0|o=1,\\pi_1)=0.8, Q(s_1|o=1,\\pi_1)=0.2$\n- 若 $o=0$: $Q(s_0|o=0,\\pi_1)=0.3, Q(s_1|o=0,\\pi_1)=0.7$\n\n$\\mathcal{G}_I(\\pi_1) = Q(o=1|\\pi_1)D_{\\mathrm{KL}}(Q(s|o=1,\\pi_1)\\|Q(s)) + Q(o=0|\\pi_1)D_{\\mathrm{KL}}(Q(s|o=0,\\pi_1)\\|Q(s))$\n$\\mathcal{G}_I(\\pi_1) = 0.6 [0.8\\ln\\frac{0.8}{0.6} + 0.2\\ln\\frac{0.2}{0.4}] + 0.4 [0.3\\ln\\frac{0.3}{0.6} + 0.7\\ln\\frac{0.7}{0.4}] \\approx 0.1285$\n\n**总预期自由能 $\\mathcal{G}(\\pi_1)$**:\n$\\mathcal{G}(\\pi_1) = \\mathcal{G}_E(\\pi_1) + \\mathcal{G}_I(\\pi_1) \\approx 0.9843 + 0.1285 = 1.1128$\n\n**2. 策略 $\\pi_2$ 的计算**\n\n同样，计算预测分布 $Q(o|\\pi_2)$：\n$Q(o=1|\\pi_2) = P(o=1|s_0, a_2)Q(s_0) + P(o=1|s_1, a_2)Q(s_1) = (0.6)(0.6) + (0.1)(0.4) = 0.4$\n$Q(o=0|\\pi_2) = 1 - 0.4 = 0.6$\n\n**外在项 $\\mathcal{G}_E(\\pi_2)$**:\n$\\mathcal{G}_E(\\pi_2) = -[0.4\\ln(0.9) + 0.6\\ln(0.1)] \\approx 1.4237$\n\n**认知项 $\\mathcal{G}_I(\\pi_2)$**:\n后验信念 $Q(s|o,\\pi_2)$:\n- 若 $o=1$: $Q(s_0|o=1,\\pi_2)=0.9, Q(s_1|o=1,\\pi_2)=0.1$\n- 若 $o=0$: $Q(s_0|o=0,\\pi_2)=0.4, Q(s_1|o=0,\\pi_2)=0.6$\n\n$\\mathcal{G}_I(\\pi_2) = 0.4 [0.9\\ln\\frac{0.9}{0.6} + 0.1\\ln\\frac{0.1}{0.4}] + 0.6 [0.4\\ln\\frac{0.4}{0.6} + 0.6\\ln\\frac{0.6}{0.4}] \\approx 0.1392$\n\n**总预期自由能 $\\mathcal{G}(\\pi_2)$**:\n$\\mathcal{G}(\\pi_2) = \\mathcal{G}_E(\\pi_2) + \\mathcal{G}_I(\\pi_2) \\approx 1.4237 + 0.1392 = 1.5629$\n\n**3. 结论**\n\n比较两个策略的预期自由能：$\\mathcal{G}(\\pi_1) \\approx 1.1128  \\mathcal{G}(\\pi_2) \\approx 1.5629$。由于策略 $\\pi_1$ 的预期自由能更低，智能体将选择执行策略 $\\pi_1$。因此，最小化策略的索引是 1。",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}