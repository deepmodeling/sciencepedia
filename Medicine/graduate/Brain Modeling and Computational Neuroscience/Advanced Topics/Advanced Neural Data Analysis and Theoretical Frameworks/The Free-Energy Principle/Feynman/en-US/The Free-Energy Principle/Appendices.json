{
    "hands_on_practices": [
        {
            "introduction": "The Free-energy Principle posits that perception is a process of probabilistic inference, where the brain continuously updates its internal model of the world to best explain sensory input. This first practice provides a concrete, hands-on calculation of this core mechanism. By deriving and applying the coordinate-ascent updates for a simple mean-field approximation, you will see exactly how a system can infer the hidden causes of its sensations by iteratively minimizing variational free energy .",
            "id": "4028571",
            "problem": "Consider a binary latent-state generative model grounded in the free-energy principle for perception, with factorization $p(s_1,s_2,o)=p(s_1)\\,p(s_2)\\,p(o\\mid s_1,s_2)$, where $s_1\\in\\{0,1\\}$ and $s_2\\in\\{0,1\\}$ denote latent states and $o\\in\\{0,1\\}$ denotes an observed sensory outcome. Assume an observation $o=1$ has been recorded. The priors are $p(s_1=1)=0.3$ and $p(s_2=1)=0.6$ (hence $p(s_1=0)=0.7$ and $p(s_2=0)=0.4$). The likelihood table is specified by\n$$\np(o=1\\mid s_1,s_2)=\n\\begin{cases}\n0.2 & \\text{if } (s_1,s_2)=(0,0),\\\\\n0.5 & \\text{if } (s_1,s_2)=(0,1),\\\\\n0.6 & \\text{if } (s_1,s_2)=(1,0),\\\\\n0.9 & \\text{if } (s_1,s_2)=(1,1).\n\\end{cases}\n$$\nAdopt a mean-field variational approximation $q(s_1,s_2)=q_1(s_1)\\,q_2(s_2)$, and perform one full coordinate-ascent iteration as follows: initialize $q_2(s_2=1)=\\frac{1}{2}$ (so $q_2(s_2=0)=\\frac{1}{2}$), update $q_1(s_1)$ holding $q_2(s_2)$ fixed, and then update $q_2(s_2)$ using the newly updated $q_1(s_1)$.\n\nUsing only foundational variational principles (variational free energy minimization and mean-field factorization) to derive the update rules and then applying the given numerical specifications, compute the updated posterior means $q_1(s_1=1)$ and $q_2(s_2=1)$ after this single iteration. Round your numerical answers to four significant figures. Express your final results as a row matrix using the LaTeX `pmatrix` environment, with the first entry $q_1(s_1=1)$ and the second entry $q_2(s_2=1)$.",
            "solution": "The objective is to minimize the variational free energy with respect to the approximate posterior distribution $q(s_1,s_2)$ under the mean-field assumption $q(s_1,s_2) = q_1(s_1)q_2(s_2)$. The general coordinate-ascent variational inference (CAVI) update rule for a factor $q_j(s_j)$ is given by:\n$$\n\\ln q_j^*(s_j) = \\mathbb{E}_{q_{\\neg j}}[\\ln p(\\mathbf{s}, o)] + \\text{const}\n$$\nwhere $\\mathbf{s} = (s_1, s_2)$, $o$ is the observation, and the expectation is taken over all variables other than $s_j$. For our model, $p(\\mathbf{s}, o) = p(o \\mid s_1, s_2) p(s_1) p(s_2)$.\n\n**1. Derive Update Rule for $q_1(s_1)$**\nThe update for $q_1(s_1)$ is:\n$$\n\\ln q_1^*(s_1) = \\mathbb{E}_{q_2(s_2)}[\\ln(p(o \\mid s_1, s_2)p(s_1)p(s_2))] + \\text{const}\n$$\n$$\n= \\ln p(s_1) + \\mathbb{E}_{q_2(s_2)}[\\ln p(o \\mid s_1, s_2)] + \\mathbb{E}_{q_2(s_2)}[\\ln p(s_2)] + \\text{const}\n$$\nSince the term involving $p(s_2)$ does not depend on $s_1$, it is absorbed into the constant. This gives:\n$$\nq_1^*(s_1) \\propto \\exp\\left( \\ln p(s_1) + \\mathbb{E}_{q_2(s_2)}[\\ln p(o \\mid s_1, s_2)] \\right)\n$$\n$$\nq_1^*(s_1) \\propto p(s_1) \\exp\\left( \\sum_{s_2 \\in \\{0,1\\}} q_2(s_2) \\ln p(o \\mid s_1, s_2) \\right)\n$$\n\n**2. Perform the Iteration**\nWe are given the observation $o=1$, priors $p(s_1=1)=0.3$ and $p(s_2=1)=0.6$, and the likelihood table. Let $\\mu_1 = q_1(s_1=1)$ and $\\mu_2 = q_2(s_2=1)$.\n\n**Initialization:** We start with $\\mu_2^{(0)} = 0.5$, so $q_2(s_2=0) = 0.5$ and $q_2(s_2=1) = 0.5$.\n\n**Update $q_1(s_1)$:**\nWe compute the unnormalized probabilities for $s_1=1$ and $s_1=0$.\nFor $s_1=1$:\n$$\n\\tilde{q}_1(1) = p(s_1=1) \\exp\\left( q_2(0) \\ln p(o=1|1,0) + q_2(1) \\ln p(o=1|1,1) \\right)\n$$\n$$\n= 0.3 \\exp\\left( 0.5 \\ln(0.6) + 0.5 \\ln(0.9) \\right) = 0.3 \\exp(0.5 \\ln(0.54)) = 0.3 \\sqrt{0.54} \\approx 0.22045\n$$\nFor $s_1=0$:\n$$\n\\tilde{q}_1(0) = p(s_1=0) \\exp\\left( q_2(0) \\ln p(o=1|0,0) + q_2(1) \\ln p(o=1|0,1) \\right)\n$$\n$$\n= 0.7 \\exp\\left( 0.5 \\ln(0.2) + 0.5 \\ln(0.5) \\right) = 0.7 \\exp(0.5 \\ln(0.1)) = 0.7 \\sqrt{0.1} \\approx 0.22136\n$$\nNow, we normalize to get the updated posterior mean for $s_1$:\n$$\n\\mu_1^{(1)} = q_1(s_1=1) = \\frac{\\tilde{q}_1(1)}{\\tilde{q}_1(1) + \\tilde{q}_1(0)} = \\frac{0.22045}{0.22045 + 0.22136} \\approx \\frac{0.22045}{0.44181} \\approx 0.499026\n$$\nRounding to four significant figures, $\\mu_1^{(1)} = 0.4990$. Thus, $q_1(s_1=0) = 1 - 0.4990 = 0.5010$.\n\n**Update $q_2(s_2)$:**\nWe use the newly updated $q_1(s_1)$, with $\\mu_1^{(1)} = 0.4990$. The update rule for $q_2$ is analogous:\n$$\nq_2^*(s_2) \\propto p(s_2) \\exp\\left( \\sum_{s_1 \\in \\{0,1\\}} q_1(s_1) \\ln p(o \\mid s_1, s_2) \\right)\n$$\nFor $s_2=1$:\n$$\n\\tilde{q}_2(1) = p(s_2=1) \\exp\\left( q_1(0) \\ln p(o=1|0,1) + q_1(1) \\ln p(o=1|1,1) \\right)\n$$\n$$\n= 0.6 \\exp\\left( 0.5010 \\ln(0.5) + 0.4990 \\ln(0.9) \\right) \\approx 0.6 \\exp(-0.34724 - 0.05259) \\approx 0.6 \\exp(-0.39983) \\approx 0.40228\n$$\nFor $s_2=0$:\n$$\n\\tilde{q}_2(0) = p(s_2=0) \\exp\\left( q_1(0) \\ln p(o=1|0,0) + q_1(1) \\ln p(o=1|1,0) \\right)\n$$\n$$\n= 0.4 \\exp\\left( 0.5010 \\ln(0.2) + 0.4990 \\ln(0.6) \\right) \\approx 0.4 \\exp(-0.80631 - 0.25489) \\approx 0.4 \\exp(-1.0612) \\approx 0.13842\n$$\nNormalize to get the updated posterior mean for $s_2$:\n$$\n\\mu_2^{(1)} = q_2(s_2=1) = \\frac{\\tilde{q}_2(1)}{\\tilde{q}_2(1) + \\tilde{q}_2(0)} = \\frac{0.40228}{0.40228 + 0.13842} \\approx \\frac{0.40228}{0.54070} \\approx 0.74400\n$$\nRounding to four significant figures, $\\mu_2^{(1)} = 0.7440$.\n\nAfter one full iteration, the updated posterior means are $q_1(s_1=1) = 0.4990$ and $q_2(s_2=1) = 0.7440$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4990 & 0.7440\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "To truly understand the Free-energy Principle, one must grasp the fundamental trade-off it describes: the balance between accuracy and complexity. This exercise makes this abstract concept tangible by examining a simple learning scenario through the lens of free energy decomposition. You will calculate how an agent's beliefs, initially simple but inaccurate, become more complex but also more accurate after observing data, quantitatively demonstrating the Occam's razor-like nature of belief updating .",
            "id": "4028543",
            "problem": "Consider a single latent-state generative model under the free-energy principle for perception in brain modeling and computational neuroscience. Let the latent state be $s \\in \\mathbb{R}$ with Gaussian prior $p(s) = \\mathcal{N}(s; \\mu_{0}, \\sigma_{0}^{2})$ and the likelihood be $p(o \\mid s) = \\mathcal{N}(o; s, \\sigma_{o}^{2})$, where $o \\in \\mathbb{R}$ is one observed datum. Assume a mean-field variational posterior $q(s) = \\mathcal{N}(s; \\mu_{q}, \\sigma_{q}^{2})$. The variational free energy is defined by the fundamental identity\n$$\nF[q] = \\mathbb{E}_{q}[\\ln q(s)] - \\mathbb{E}_{q}[\\ln p(o,s)],\n$$\nand can be decomposed into an accuracy term and a complexity term via the factorization $p(o,s) = p(o \\mid s)p(s)$. The complexity is the Kullback–Leibler (KL) divergence $D_{\\mathrm{KL}}(q(s) \\| p(s))$ and the accuracy is the expected log-likelihood $\\mathbb{E}_{q}[\\ln p(o \\mid s)]$. Starting from the above definitions and using only Gaussian identities that follow from these definitions, derive closed-form expressions for the accuracy and complexity components in this conjugate Gaussian model.\n\nNow consider the concrete, scientifically realistic setting with parameters $\\mu_{0} = 0$, $\\sigma_{0}^{2} = 1$, $\\sigma_{o}^{2} = \\frac{1}{4}$, and a single observed datum $o = 1$. Define “before learning” as the initialization $q_{\\text{before}}(s) = p(s)$ and “after learning” as the exact Gaussian posterior $q_{\\text{after}}(s) = p(s \\mid o)$ obtained by minimizing $F[q]$ over the Gaussian family. Compute the accuracy and complexity for both $q_{\\text{before}}$ and $q_{\\text{after}}$.\n\nProvide your final answer as a single row matrix containing the four quantities in the exact analytic form specified by your derivations, in the following order:\n$$\n\\big[\\text{accuracy}_{\\text{before}}, \\ \\text{complexity}_{\\text{before}}, \\ \\text{accuracy}_{\\text{after}}, \\ \\text{complexity}_{\\text{after}}\\big].\n$$\nExpress your answers in exact analytic form (do not approximate or round). Finally, briefly interpret the Occam trade-off in terms of how the change in accuracy and complexity explains the change in variational free energy $F[q]$ between “before” and “after” learning.",
            "solution": "The problem asks for the derivation of accuracy and complexity terms for a simple Gaussian generative model, and then to compute these quantities for two specific states: \"before learning\" ($q_{\\text{before}}(s) = p(s)$) and \"after learning\" ($q_{\\text{after}}(s) = p(s \\mid o)$).\n\nThe generative model is specified by a Gaussian prior $p(s) = \\mathcal{N}(s; \\mu_{0}, \\sigma_{0}^{2})$ and a Gaussian likelihood $p(o \\mid s) = \\mathcal{N}(o; s, \\sigma_{o}^{2})$. The approximate posterior is $q(s) = \\mathcal{N}(s; \\mu_{q}, \\sigma_{q}^{2})$. The variational free energy is decomposed as $F[q] = \\text{Complexity} - \\text{Accuracy}$, where Complexity is $D_{\\mathrm{KL}}(q(s) \\| p(s))$ and Accuracy is $\\mathbb{E}_{q}[\\ln p(o \\mid s)]$.\n\n**1. Derivation of Closed-Form Expressions**\n\n**Complexity:** The KL divergence between two Gaussian distributions $q(s)=\\mathcal{N}(\\mu_q, \\sigma_q^2)$ and $p(s)=\\mathcal{N}(\\mu_0, \\sigma_0^2)$ is:\n$$\nD_{\\mathrm{KL}}(q(s) \\| p(s)) = \\int q(s) \\ln \\frac{q(s)}{p(s)} ds = \\mathbb{E}_{q}[\\ln q(s) - \\ln p(s)]\n$$\nThe well-known closed-form solution is:\n$$\n\\text{Complexity} = D_{\\mathrm{KL}}(q(s) \\| p(s)) = \\frac{1}{2} \\left( \\ln\\left(\\frac{\\sigma_{0}^{2}}{\\sigma_{q}^{2}}\\right) - 1 + \\frac{\\sigma_{q}^{2} + (\\mu_{q} - \\mu_{0})^{2}}{\\sigma_{0}^{2}} \\right)\n$$\n\n**Accuracy:** The expected log-likelihood under the variational posterior is:\n$$\n\\text{Accuracy} = \\mathbb{E}_{q}[\\ln p(o \\mid s)] = \\mathbb{E}_{q}\\left[-\\frac{1}{2}\\ln(2\\pi\\sigma_{o}^{2}) - \\frac{(o - s)^{2}}{2\\sigma_{o}^{2}}\\right]\n$$\n$$\n= -\\frac{1}{2}\\ln(2\\pi\\sigma_{o}^{2}) - \\frac{1}{2\\sigma_{o}^{2}}\\mathbb{E}_{q}[(o - s)^{2}]\n$$\nThe expectation $\\mathbb{E}_{q}[(o - s)^{2}]$ is $\\mathbb{E}_{q}[(o - \\mu_q + \\mu_q - s)^2] = \\mathbb{E}_{q}[(o - \\mu_q)^2 - 2(o - \\mu_q)(s - \\mu_q) + (s - \\mu_q)^2] = (o - \\mu_q)^2 + \\sigma_q^2$.\nThus, the closed-form expression is:\n$$\n\\text{Accuracy} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{o}^{2}) - \\frac{(o - \\mu_{q})^{2} + \\sigma_{q}^{2}}{2\\sigma_{o}^{2}}\n$$\n\n**2. Application to the Specific Case**\nParameters: $\\mu_{0} = 0$, $\\sigma_{0}^{2} = 1$, $\\sigma_{o}^{2} = 1/4$, and observation $o = 1$.\n\n**\"Before learning\" state:** $q_{\\text{before}}(s) = p(s)$, so $\\mu_{q} = \\mu_{0} = 0$ and $\\sigma_{q}^{2} = \\sigma_{0}^{2} = 1$.\n- **Complexity before learning:** By definition, since $q_{\\text{before}}(s) = p(s)$, the KL divergence is zero.\n  $$\n  \\text{complexity}_{\\text{before}} = D_{\\mathrm{KL}}(p(s) \\| p(s)) = 0\n  $$\n- **Accuracy before learning:** We use the formula with $\\mu_{q} = 0$, $\\sigma_{q}^{2} = 1$, $o = 1$, $\\sigma_{o}^{2} = 1/4$.\n  $$\n  \\text{accuracy}_{\\text{before}} = -\\frac{1}{2}\\ln\\left(2\\pi\\cdot\\frac{1}{4}\\right) - \\frac{(1 - 0)^{2} + 1}{2\\cdot(1/4)} = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - \\frac{2}{1/2} = -4 - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right)\n  $$\n\n**\"After learning\" state:** $q_{\\text{after}}(s) = p(s \\mid o)$. For this conjugate model, the posterior is Gaussian with parameters given by standard Bayesian updates:\n- Posterior precision: $\\frac{1}{\\sigma_{q, \\text{after}}^{2}} = \\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma_{o}^{2}} = \\frac{1}{1} + \\frac{1}{1/4} = 5 \\implies \\sigma_{q, \\text{after}}^{2} = \\frac{1}{5}$.\n- Posterior mean: $\\mu_{q, \\text{after}} = \\sigma_{q, \\text{after}}^{2} \\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{o}{\\sigma_{o}^{2}}\\right) = \\frac{1}{5} \\left(\\frac{0}{1} + \\frac{1}{1/4}\\right) = \\frac{4}{5}$.\nSo, $q_{\\text{after}}(s) = \\mathcal{N}(s; \\mu_{q} = 4/5, \\sigma_{q}^{2} = 1/5)$.\n- **Complexity after learning:**\n  $$\n  \\text{complexity}_{\\text{after}} = \\frac{1}{2} \\left( \\ln\\left(\\frac{1}{1/5}\\right) - 1 + \\frac{1/5 + (4/5 - 0)^{2}}{1} \\right) = \\frac{1}{2} \\left( \\ln(5) - 1 + \\frac{1}{5} + \\frac{16}{25} \\right)\n  $$\n  $$\n  = \\frac{1}{2} \\left( \\ln(5) - \\frac{25}{25} + \\frac{5}{25} + \\frac{16}{25} \\right) = \\frac{1}{2} \\left( \\ln(5) - \\frac{4}{25} \\right) = \\frac{1}{2}\\ln(5) - \\frac{2}{25}\n  $$\n- **Accuracy after learning:**\n  $$\n  \\text{accuracy}_{\\text{after}} = -\\frac{1}{2}\\ln\\left(2\\pi\\cdot\\frac{1}{4}\\right) - \\frac{(1 - 4/5)^{2} + 1/5}{2\\cdot(1/4)} = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - \\frac{(1/5)^{2} + 1/5}{1/2}\n  $$\n  $$\n  = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - 2\\left(\\frac{1}{25} + \\frac{5}{25}\\right) = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) - \\frac{12}{25}\n  $$\n\n**Interpretation of the Occam Trade-off**\nThe goal is to minimize variational free energy, $F[q] = \\text{Complexity} - \\text{Accuracy}$, which is equivalent to maximizing $\\text{Accuracy} - \\text{Complexity}$.\n- **Before learning:** The belief equals the prior. Complexity is 0 (the simplest possible model). Accuracy is low ($\\approx -4.22$), as the prior belief is a poor explanation for the specific observation $o=1$.\n- **After learning:** The belief is updated to the posterior. This increases complexity from $0$ to $\\frac{1}{2}\\ln(5) - \\frac{2}{25} \\approx 0.72$. This cost is incurred because the belief now diverges from the prior. However, this update also significantly improves accuracy from approx. -4.22 to approx. -0.70. The increase in accuracy ($3.52$) far outweighs the increase in complexity ($0.72$).\nThis demonstrates the Occam's razor trade-off: the system pays a small price in complexity to achieve a large gain in accuracy, thereby reducing its overall free energy (surprise). The final belief is more complex but provides a much better, more parsimonious explanation of the sensory evidence.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -4 - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) & 0 & -\\frac{12}{25} - \\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right) & \\frac{1}{2}\\ln(5) - \\frac{2}{25} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key triumph of the Free-energy Principle is its ability to unify perception and action under a single imperative. This final practice moves from passive perception to active inference, where an agent must choose what to do next. By calculating the expected free energy, $\\mathcal{G}(\\pi)$, for different policies, you will explore how an agent evaluates potential actions based on both their instrumental value (leading to preferred outcomes) and their epistemic value (resolving uncertainty about the world) .",
            "id": "4028595",
            "problem": "Consider a single-step decision problem formulated under the Free-Energy Principle (FEP) in brain modeling and computational neuroscience. Hidden states are binary, $s \\in \\{s_{0}, s_{1}\\}$, with a current belief (recognition density) $Q(s)$ given by $Q(s_{0}) = 0.6$ and $Q(s_{1}) = 0.4$. Observations are binary, $o \\in \\{0,1\\}$, and there are two candidate one-step policies $\\pi_{1}$ and $\\pi_{2}$ corresponding to two actions $a_{1}$ and $a_{2}$ that modulate the likelihood mapping $P(o \\mid s, a)$ as follows:\n- Under action $a_{1}$: $P(o=1 \\mid s_{0}, a_{1}) = 0.8$, $P(o=0 \\mid s_{0}, a_{1}) = 0.2$, $P(o=1 \\mid s_{1}, a_{1}) = 0.3$, $P(o=0 \\mid s_{1}, a_{1}) = 0.7$.\n- Under action $a_{2}$: $P(o=1 \\mid s_{0}, a_{2}) = 0.6$, $P(o=0 \\mid s_{0}, a_{2}) = 0.4$, $P(o=1 \\mid s_{1}, a_{2}) = 0.1$, $P(o=0 \\mid s_{1}, a_{2}) = 0.9$.\n\nPreferred outcomes are encoded by a prior preference distribution $P^{\\ast}(o)$ with $P^{\\ast}(1) = 0.9$ and $P^{\\ast}(0) = 0.1$. Assume a single time step, and that the prior over hidden states used in scoring expected free energy matches the current belief, i.e., $P(s) = Q(s)$. Use the natural logarithm $\\ln$.\n\nStarting from the definition of variational free energy for a single observation in terms of a recognition distribution $Q(s \\mid o, \\pi)$ and a generative model with outcome preferences $P^{\\ast}(o)$, and using only core definitions from variational Bayesian inference (e.g., Kullback–Leibler divergence (KLD), mutual information, and law of total expectation), derive an expression for the one-step expected free energy $\\mathcal{G}(\\pi)$ of a policy $\\pi$ that separates into (i) an extrinsic term that depends on $P^{\\ast}(o)$ and the predictive distribution $Q(o \\mid \\pi)$ and (ii) an epistemic term that is the mutual information between hidden states and outcomes under $\\pi$. Then compute $\\mathcal{G}(\\pi_{1})$ and $\\mathcal{G}(\\pi_{2})$ numerically, and determine which policy minimizes $\\mathcal{G}(\\pi)$.\n\nAs your final answer, report the index $i \\in \\{1,2\\}$ of the minimizing policy $\\pi_{i}$. No rounding instruction is needed for the final index. If intermediate numerical computations are required, use exact arithmetic and $\\ln$ evaluations; any approximations should be clearly indicated within your derivation.",
            "solution": "The expected free energy $\\mathcal{G}(\\pi)$ of a policy $\\pi$ is the free energy expected over the outcomes predicted by that policy.\n\n**1. Derivation of the Expected Free Energy Expression**\nThe free energy for a future outcome $o$ under policy $\\pi$ is the KL divergence between the approximate posterior belief $Q(s|o,\\pi)$ and a generative model where outcomes follow preferences $P^\\ast(o)$ and states follow a prior $P(s)$:\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln Q(s|o,\\pi) - \\ln (P(s)P^\\ast(o))] $$\n$$ F(o|\\pi) = \\underbrace{\\mathbb{E}_{Q(s|o,\\pi)}\\left[\\ln \\frac{Q(s|o,\\pi)}{P(s)}\\right]}_{D_{KL}[Q(s|o,\\pi) \\| P(s)]} - \\ln P^\\ast(o) $$\nThe expected free energy $\\mathcal{G}(\\pi)$ is the expectation of $F(o|\\pi)$ over predicted outcomes $Q(o|\\pi) = \\sum_s P(o|s,\\pi)Q(s)$:\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) \\| P(s)] - \\ln P^\\ast(o)] $$\n$$ \\mathcal{G}(\\pi) = \\underbrace{\\sum_o Q(o|\\pi) D_{KL}[Q(s|o,\\pi) \\| P(s)]}_{\\text{Epistemic Value}} + \\underbrace{\\sum_o Q(o|\\pi)(-\\ln P^\\ast(o))}_{\\text{Extrinsic Value}} $$\nThe **extrinsic term** is the cross-entropy between predicted outcomes and preferred outcomes. The **epistemic term**, using the problem's condition that $P(s)=Q(s)$, is the expected information gain, i.e., the mutual information $I(s;o|\\pi)$ between states and outcomes under the policy. Agents select the policy that minimizes $\\mathcal{G}(\\pi)$.\n\n**2. Computation for Policy $\\pi_1$ (Action $a_1$)**\n- **Predictive outcome distribution $Q(o|\\pi_1)$:**\n  $Q(o=1|\\pi_1) = P(o=1|s_0, a_1)Q(s_0) + P(o=1|s_1, a_1)Q(s_1) = (0.8)(0.6) + (0.3)(0.4) = 0.48 + 0.12 = 0.6$.\n  $Q(o=0|\\pi_1) = 1 - 0.6 = 0.4$.\n- **Extrinsic Value:**\n  $\\mathcal{G}_E(\\pi_1) = 0.4(-\\ln 0.1) + 0.6(-\\ln 0.9) \\approx 0.4(2.3026) + 0.6(0.1054) \\approx 0.9843$.\n- **Epistemic Value (Mutual Information):**\n  $I(s;o|\\pi_1) = H(s) - H(s|o,\\pi_1)$, where $H(s) = -[0.6\\ln(0.6) + 0.4\\ln(0.4)] \\approx 0.6730$.\n  Posteriors: $Q(s_0|o=1, \\pi_1)=0.8, Q(s_1|o=1, \\pi_1)=0.2$; $Q(s_0|o=0, \\pi_1)=0.3, Q(s_1|o=0, \\pi_1)=0.7$.\n  $H(s|o=1) = -[0.8\\ln(0.8) + 0.2\\ln(0.2)] \\approx 0.5004$.\n  $H(s|o=0) = -[0.3\\ln(0.3) + 0.7\\ln(0.7)] \\approx 0.6109$.\n  $H(s|o, \\pi_1) = 0.6(0.5004) + 0.4(0.6109) \\approx 0.5446$.\n  $\\mathcal{G}_I(\\pi_1) = 0.6730 - 0.5446 = 0.1284$.\n- **Total Expected Free Energy:**\n  $\\mathcal{G}(\\pi_1) = \\mathcal{G}_E(\\pi_1) - \\mathcal{G}_I(\\pi_1) \\approx 0.9843 - 0.1284 \\approx 0.8559$. Wait, the standard definition of EFE is Risk + Ambiguity. G = Extrinsic - Epistemic is not standard. The derivation gives G = Extrinsic + Epistemic. Let's re-read Friston's papers. Expected free energy is the quantity to be *minimized*. It's a cost. Epistemic value or information gain is something good, so it should reduce the cost. The standard decomposition is $\\mathcal{G}(\\pi) = \\text{Risk} + \\text{Ambiguity}$. Risk = $D_{KL}[Q(o|\\pi) || P(o)]$, where $P(o)$ are preferences. Ambiguity = $-\\mathbb{E}_{Q(o|\\pi)}[H(P(s|o,\\pi))]$. My first derivation was correct: $\\mathcal{G}(\\pi) = \\text{Epistemic} + \\text{Extrinsic}$. It is the sum of two cost terms. Epistemic term is expected posterior uncertainty, Extrinsic is divergence from goals. Wait, no. My derived Epistemic Term is Mutual Information, which is a *positive* value that is maximized in information-seeking. Minimizing EFE must correspond to maximizing epistemic value. So $\\mathcal{G}(\\pi) = \\text{Extrinsic Value} - \\text{Epistemic Value}$.\nLet's re-derive from the standard definition $G(\\pi) = - \\mathbb{E}_{Q(o,s|\\pi)}[\\ln P(o,s|\\pi)]$. The derivation is complex. Let's use the common form: $\\mathcal{G}(\\pi) = \\underbrace{D_{KL}[Q(o|\\pi)||P(o)]}_{\\text{Risk (Extrinsic)}} + \\underbrace{\\mathbb{E}_{Q(s)}[H(P(o|s,\\pi))]}_{\\text{Ambiguity (Epistemic)}}$. This is the form to be minimized.\nRisk for $\\pi_1$: $0.6\\ln\\frac{0.6}{0.9} + 0.4\\ln\\frac{0.4}{0.1} \\approx 0.6(-0.405) + 0.4(1.386) = -0.243 + 0.554 = 0.311$.\nAmbiguity for $\\pi_1$: $Q(s_0)H(P(o|s_0,\\pi_1)) + Q(s_1)H(P(o|s_1,\\pi_1))$.\n$H(P(o|s_0,\\pi_1)) = -[0.8\\ln(0.8) + 0.2\\ln(0.2)] \\approx 0.500$.\n$H(P(o|s_1,\\pi_1)) = -[0.3\\ln(0.3) + 0.7\\ln(0.7)] \\approx 0.611$.\nAmbiguity: $0.6(0.500) + 0.4(0.611) = 0.300 + 0.244 = 0.544$.\n$\\mathcal{G}(\\pi_1) = 0.311 + 0.544 = 0.855$.\n\n**My initial derivation was correct, my interpretation was wrong. Let's stick to the first derivation as requested by the prompt.**\n$\\mathcal{G}(\\pi) = \\text{Extrinsic} + \\text{Epistemic}$.\n$\\mathcal{G}(\\pi_1) = 0.9843 + 0.1284 = 1.1127$.\n\n**3. Computation for Policy $\\pi_2$ (Action $a_2$)**\n- **Predictive outcome distribution $Q(o|\\pi_2)$:**\n  $Q(o=1|\\pi_2) = (0.6)(0.6) + (0.1)(0.4) = 0.36 + 0.04 = 0.4$.\n  $Q(o=0|\\pi_2) = 0.6$.\n- **Extrinsic Value:**\n  $\\mathcal{G}_E(\\pi_2) = 0.6(-\\ln 0.1) + 0.4(-\\ln 0.9) \\approx 0.6(2.3026) + 0.4(0.1054) \\approx 1.4237$.\n- **Epistemic Value (Mutual Information):**\n  $I(s;o|\\pi_2) = H(s) - H(s|o,\\pi_2)$.\n  Posteriors: $Q(s_0|o=1, \\pi_2)=0.9, Q(s_1|o=1, \\pi_2)=0.1$; $Q(s_0|o=0, \\pi_2)=0.4, Q(s_1|o=0, \\pi_2)=0.6$.\n  $H(s|o=1) = -[0.9\\ln(0.9) + 0.1\\ln(0.1)] \\approx 0.3251$.\n  $H(s|o=0) = -[0.4\\ln(0.4) + 0.6\\ln(0.6)] = H(s) \\approx 0.6730$.\n  $H(s|o,\\pi_2) = 0.4(0.3251) + 0.6(0.6730) \\approx 0.1300 + 0.4038 = 0.5338$.\n  $\\mathcal{G}_I(\\pi_2) = 0.6730 - 0.5338 = 0.1392$.\n- **Total Expected Free Energy:**\n  $\\mathcal{G}(\\pi_2) = \\mathcal{G}_E(\\pi_2) + \\mathcal{G}_I(\\pi_2) \\approx 1.4237 + 0.1392 = 1.5629$.\n\n**4. Conclusion**\nComparing the total expected free energy for both policies to be minimized:\n$$ \\mathcal{G}(\\pi_1) \\approx 1.1127 $$\n$$ \\mathcal{G}(\\pi_2) \\approx 1.5629 $$\nSince $\\mathcal{G}(\\pi_1)  \\mathcal{G}(\\pi_2)$, the agent selects the policy that minimizes the expected free energy. Therefore, policy $\\pi_1$ is optimal. The index of the minimizing policy is $i=1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}