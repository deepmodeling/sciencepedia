{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of modeling neural population activity is accurately capturing the statistical properties of spike counts. While the Poisson distribution is a common starting point, real neural data often exhibit \"overdispersion\"â€”variance greater than the mean. This exercise guides you through the construction of a more realistic observation model by deriving the negative binomial distribution from a Poisson-Gamma mixture, a fundamental technique for handling overdispersed count data .",
            "id": "3993296",
            "problem": "Consider a latent manifold model of neural population activity in which a low-dimensional latent state $z$ drives the time-varying mean firing rate of a single neuron through a link function, producing an observation $y \\in \\{0,1,2,\\dots\\}$ of spike counts in a fixed bin. To capture overdispersion relative to a Poisson model, suppose the observation model is defined by the following hierarchical construction: conditional on an instantaneous rate $\\lambda$, the spike count is distributed as a Poisson random variable $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$, and the rate itself fluctuates due to unobserved latent variability, modeled as a Gamma random variable $\\lambda \\sim \\mathrm{Gamma}(k,\\beta)$ with shape parameter $k>0$ and rate parameter $\\beta>0$. Assume that the latent state $z$ determines the mean of $\\lambda$ through the link $\\mu = \\mathbb{E}[\\lambda]$, where $\\mu$ is any positive function of $z$, such as the exponential mapping $\\mu = \\exp\\!\\big(C^{\\top} z + b\\big)$ commonly used in a Generalized Linear Model (GLM), with $C$ a loading vector and $b$ a bias.\n\nUsing only the definitions of the Poisson and Gamma distributions and the laws of total expectation and total variance, first pick the Gamma rate parameter $\\beta$ in terms of $k$ and $\\mu$ so that $\\mathbb{E}[\\lambda] = \\mu$, and thereby define a negative binomial observation model for $y$ with mean $\\mu$ and dispersion $k$. Then derive the unconditional variance $\\mathrm{Var}[y]$ as a closed-form expression in terms of $\\mu$ and $k$, which quantifies overdispersion relative to the Poisson case. Provide your final answer as the expression for $\\mathrm{Var}[y]$ in closed form in terms of $\\mu$ and $k$. No numerical approximation is required, and no units apply.",
            "solution": "The user wants me to solve the following problem:\nThe problem statement is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- The observation is a spike count $y \\in \\{0, 1, 2, \\dots\\}$.\n- The conditional distribution of the spike count is Poisson: $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$.\n- The instantaneous rate $\\lambda$ is a random variable.\n- The distribution of the rate is Gamma: $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$.\n- The shape parameter is $k > 0$.\n- The rate parameter is $\\beta > 0$.\n- The mean of the rate $\\lambda$ is denoted by $\\mu$: $\\mathbb{E}[\\lambda] = \\mu$.\n- $\\mu$ is a positive function of a latent state $z$.\n- The tools to be used are the definitions of the Poisson and Gamma distributions and the laws of total expectation and total variance.\n- The first objective is to determine the Gamma rate parameter $\\beta$ in terms of $k$ and $\\mu$.\n- The second objective is to derive the unconditional variance $\\mathrm{Var}[y]$ in terms of $\\mu$ and $k$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in statistics and computational neuroscience. The Poisson-Gamma mixture model is a standard and well-established method for modeling overdispersed count data, such as neural spike counts.\n- **Well-Posed**: The problem is clearly stated and has a unique, derivable solution based on the provided information and standard probability theory.\n- **Objective**: The language is formal, precise, and objective, using standard mathematical terminology.\n- **Completeness and Consistency**: The problem is self-contained. It provides all necessary definitions and constraints to perform the required derivations. It is not underspecified or overconstrained. The instruction to find $\\beta$ is part of the problem's task, not a missing piece of information.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-posed, trivial, or unverifiable. It represents a standard derivation in statistical modeling.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe problem requires a two-part derivation based on a hierarchical model where the spike count $y$ is conditionally Poisson and its rate parameter $\\lambda$ is Gamma-distributed.\n\nFirst, we establish the relationship between the parameters of the Gamma distribution and the mean rate $\\mu$. The Gamma distribution for a random variable $\\lambda$ with shape parameter $k > 0$ and rate parameter $\\beta > 0$, denoted $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$, has the probability density function $f(\\lambda; k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} \\exp(-\\beta \\lambda)$ for $\\lambda > 0$. The mean of this distribution is given by:\n$$\n\\mathbb{E}[\\lambda] = \\frac{k}{\\beta}\n$$\nThe problem specifies that the mean of $\\lambda$ is $\\mu$, i.e., $\\mathbb{E}[\\lambda] = \\mu$. By equating the two expressions for the mean, we can solve for the rate parameter $\\beta$:\n$$\n\\mu = \\frac{k}{\\beta} \\implies \\beta = \\frac{k}{\\mu}\n$$\nThis fulfills the first part of the problem. This reparameterization defines the distribution of the rate $\\lambda$ in terms of its mean $\\mu$ and a shape/dispersion parameter $k$.\n\nNext, we derive the unconditional variance of the spike count, $\\mathrm{Var}[y]$. We use the law of total variance, which states that for any two random variables $y$ and $\\lambda$:\n$$\n\\mathrm{Var}[y] = \\mathbb{E}[\\mathrm{Var}[y \\mid \\lambda]] + \\mathrm{Var}[\\mathbb{E}[y \\mid \\lambda]]\n$$\nTo apply this law, we need the conditional expectation $\\mathbb{E}[y \\mid \\lambda]$ and the conditional variance $\\mathrm{Var}[y \\mid \\lambda]$. The problem states that the conditional distribution of $y$ given $\\lambda$ is a Poisson distribution, $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$. For a Poisson distribution with rate parameter $\\lambda$, the mean and variance are both equal to the rate parameter:\n$$\n\\mathbb{E}[y \\mid \\lambda] = \\lambda\n$$\n$$\n\\mathrm{Var}[y \\mid \\lambda] = \\lambda\n$$\nSubstituting these expressions into the law of total variance formula, we get:\n$$\n\\mathrm{Var}[y] = \\mathbb{E}[\\lambda] + \\mathrm{Var}[\\lambda]\n$$\nWe are given that $\\mathbb{E}[\\lambda] = \\mu$. The remaining term to find is $\\mathrm{Var}[\\lambda]$. The variance of the Gamma distribution $\\lambda \\sim \\mathrm{Gamma}(k, \\beta)$ is given by:\n$$\n\\mathrm{Var}[\\lambda] = \\frac{k}{\\beta^2}\n$$\nNow, we substitute the expression for $\\beta$ that we derived earlier, $\\beta = \\frac{k}{\\mu}$, into the formula for the variance of $\\lambda$:\n$$\n\\mathrm{Var}[\\lambda] = \\frac{k}{\\left(\\frac{k}{\\mu}\\right)^2} = \\frac{k}{\\frac{k^2}{\\mu^2}} = \\frac{k \\mu^2}{k^2} = \\frac{\\mu^2}{k}\n$$\nFinally, we substitute the expressions for $\\mathbb{E}[\\lambda]$ and $\\mathrm{Var}[\\lambda]$ back into the equation for $\\mathrm{Var}[y]$:\n$$\n\\mathrm{Var}[y] = \\mu + \\frac{\\mu^2}{k}\n$$\nThis is the closed-form expression for the unconditional variance of the spike count $y$. This result is characteristic of the Negative Binomial distribution, which is the marginal distribution of $y$ in a Poisson-Gamma mixture model. The variance is composed of a term equal to the mean, $\\mu$, which is the variance of a simple Poisson process, and an additional positive term, $\\frac{\\mu^2}{k}$, which accounts for the overdispersion due to the variability of the underlying rate $\\lambda$. The parameter $k$ is often called the dispersion parameter, where smaller values of $k$ correspond to greater overdispersion. As $k \\to \\infty$, this extra variance term vanishes, and $\\mathrm{Var}[y] \\to \\mu$, recovering the Poisson case.",
            "answer": "$$\n\\boxed{\\mu + \\frac{\\mu^2}{k}}\n$$"
        },
        {
            "introduction": "Once an observation model is chosen, we must define how the latent manifold drives the firing rates of individual neurons. This connection is governed by a \"link function,\" and the choice of function has profound consequences for the model's mathematical properties and the stability of optimization algorithms. This practice compares the widely used exponential and softplus link functions, providing critical insight into how they shape the optimization landscape and affect the process of fitting latent variable models to data .",
            "id": "3993252",
            "problem": "Consider a population Poisson observation model for neural spike counts in which the latent low-dimensional state at time $t$ is $x_t \\in \\mathbb{R}^k$, mapped linearly to a per-neuron drive $z_t \\in \\mathbb{R}^n$ by $z_t = C x_t + d$ with $C \\in \\mathbb{R}^{n \\times k}$ and $d \\in \\mathbb{R}^n$. The observed spike counts are $y_t \\in \\mathbb{N}^n$, modeled as conditionally independent Poisson random variables with mean rates $\\lambda_t \\in \\mathbb{R}_+^n$ given by a monotone link function $g$ applied element-wise to $z_t$, that is $\\lambda_t = g(z_t)$. Two link functions of interest for ensuring positive rates are the exponential link $g_{\\exp}(z) = \\exp(z)$ and the softplus link $g_{\\mathrm{sp}}(z) = \\log\\!\\left(1 + \\exp(z)\\right)$.\n\nAssume a Gaussian latent trajectory prior that is quadratic in $x_{1:T}$ (e.g., a linear dynamical system prior), so that the prior log-density contributes a negative-definite quadratic penalty in the Maximum A Posteriori (MAP) objective. In the Expectation-Maximization (EM) algorithm, the latent MAP E-step maximizes the posterior log-density over $x_{1:T}$, which can be written (up to additive constants) as a sum across times and neurons of the Poisson log-likelihood terms $y_{t,i} \\log \\lambda_{t,i} - \\lambda_{t,i}$ plus the prior quadratic.\n\nUsing only the foundational definitions above, compare the exponential and softplus links in terms of curvature and optimization behavior in the latent MAP E-step. Which of the following statements are correct?\n\nA. With the exponential link $g_{\\exp}$, the observation log-likelihood as a function of the linear drive $z$ is strictly concave, with a negative second derivative that scales with $\\exp(z)$. When composed with a linear $z(x)$ and added to a quadratic Gaussian prior, the full MAP objective in $x$ is strictly concave, guaranteeing a unique global maximizer.\n\nB. With the softplus link $g_{\\mathrm{sp}}$, the observation log-likelihood as a function of the linear drive $z$ is concave for all nonnegative counts and all real $z$. Therefore, when composed with a linear $z(x)$ and added to a quadratic Gaussian prior, the full MAP objective in $x$ is concave, ensuring a unique global maximizer.\n\nC. Relative to the exponential link, the softplus link moderates curvature: for fixed observed count $y$, the second derivative of the observation log-likelihood with respect to $z$ under $g_{\\mathrm{sp}}$ remains bounded in magnitude uniformly over $z$, and the gradient with respect to $z$ is bounded below by $-1$ for all $z$. In contrast, under $g_{\\exp}$ the second derivative grows in magnitude without bound as $z \\to +\\infty$.\n\nD. In the latent MAP E-step with linear $z(x)$, both links yield the same Fisher information in $x$ because the observation model is Poisson; hence the Hessians of the observation log-likelihood with respect to $x$ are identical under $g_{\\exp}$ and $g_{\\mathrm{sp}}$.\n\nE. Switching from the exponential link to the softplus link necessarily destroys the log-concavity of the posterior, invalidating Laplace approximations around the latent MAP estimate.\n\nSelect all that apply.",
            "solution": "The user wants me to validate the problem statement and, if valid, solve the problem by analyzing each option provided.\n\n### Step 1: Extract Givens\nThe problem describes a Poisson Generalized Linear Model (GLM) with a latent variable structure.\n-   Latent state at time $t$: $x_t \\in \\mathbb{R}^k$.\n-   Linear drive for $n$ neurons: $z_t = C x_t + d$, with $C \\in \\mathbb{R}^{n \\times k}$ and $d \\in \\mathbb{R}^n$.\n-   Observed spike counts: $y_t \\in \\mathbb{N}^n$.\n-   Observation model: $y_{t,i}$ are conditionally independent Poisson variables given a rate $\\lambda_{t,i}$.\n-   Link function: The rates $\\lambda_t$ are related to the drive $z_t$ by a monotone link function $g$ applied element-wise, $\\lambda_t = g(z_t)$.\n-   Two specific link functions are given:\n    1.  Exponential link: $g_{\\exp}(z) = \\exp(z)$.\n    2.  Softplus link: $g_{\\mathrm{sp}}(z) = \\log(1 + \\exp(z))$.\n-   Latent prior: The prior on the latent trajectory $x_{1:T}$ is Gaussian, and its log-density provides a negative-definite quadratic penalty in the MAP objective. This means the log-prior is a strictly concave function of $x_{1:T}$.\n-   MAP Objective function: The objective to be maximized in the E-step is the log-posterior of the latent variables, which (up to constants) is the sum of the log-likelihood and the log-prior.\n-   Poisson log-likelihood: For a single observation $y_{t,i}$ with rate $\\lambda_{t,i}$, the log-likelihood is $\\ell_{t,i} = y_{t,i} \\log \\lambda_{t,i} - \\lambda_{t,i}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The model described is a standard latent variable model used in computational neuroscience, often referred to as a Poisson GLM with latent dynamics or a state-space GLM. The exponential and softplus functions are common link functions for ensuring positive rates. The mathematical formulation is standard and sound.\n2.  **Well-Posed**: The problem is a well-defined mathematical question about the properties (specifically, concavity and derivatives) of an objective function derived from the given model. It asks for a comparison between two well-defined link functions. A unique answer can be derived.\n3.  **Objective**: The language is precise, and the terms are defined mathematically. There is no subjectivity.\n4.  **Incomplete or Contradictory Setup**: The problem provides all necessary definitions to analyze the properties of the log-likelihood and the MAP objective. The assumption of a negative-definite quadratic penalty for the prior is a key piece of information that is provided and is consistent with a Gaussian prior.\n5.  **Unrealistic or Infeasible**: The setup is a standard statistical model and is computationally and scientifically feasible.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and self-contained. I will proceed with deriving the solution and evaluating the options.\n\n### Solution Derivation\n\nThe analysis centers on the properties of the log-likelihood for a single neuron, $\\ell(z) = y \\log g(z) - g(z)$, as a function of the drive $z$. The total MAP objective function (the log-posterior) involves summing these terms over all neurons and time points, composing with the linear map $z_t(x_t) = C x_t + d$, and adding the strictly concave log-prior.\n\nThe concavity of the MAP objective is key. The sum of concave functions is concave. The sum of a concave function and a strictly concave function is strictly concave. A strictly concave function has a unique global maximum.\n\nWe need the first and second derivatives of $\\ell(z)$ with respect to $z$:\n-   $\\frac{d\\ell}{dz} = \\frac{y}{g(z)} g'(z) - g'(z) = g'(z) \\left( \\frac{y}{g(z)} - 1 \\right)$\n-   $\\frac{d^2\\ell}{dz^2} = y \\left( \\frac{g''(z)g(z) - (g'(z))^2}{g(z)^2} \\right) - g''(z)$\n\nNow, we analyze this for each link function.\n\n**Case 1: Exponential link, $g_{\\exp}(z) = \\exp(z)$**\n-   $g'(z) = \\exp(z)$\n-   $g''(z) = \\exp(z)$\n-   The second derivative of the log-likelihood is:\n    $$ \\frac{d^2\\ell}{dz^2} = y \\left( \\frac{\\exp(z)\\exp(z) - (\\exp(z))^2}{(\\exp(z))^2} \\right) - \\exp(z) = y \\cdot 0 - \\exp(z) = -\\exp(z) $$\n-   Since $\\exp(z) > 0$ for all $z \\in \\mathbb{R}$, $\\frac{d^2\\ell}{dz^2}$ is always strictly negative. Thus, $\\ell(z)$ is a strictly concave function of $z$.\n\n**Case 2: Softplus link, $g_{\\mathrm{sp}}(z) = \\log(1 + \\exp(z))$**\n-   $g'(z) = \\frac{\\exp(z)}{1 + \\exp(z)} = \\sigma(z)$ (the logistic sigmoid function).\n-   $g''(z) = \\frac{\\exp(z)(1+\\exp(z)) - \\exp(z)\\exp(z)}{(1+\\exp(z))^2} = \\frac{\\exp(z)}{(1+\\exp(z))^2} = \\sigma(z)(1-\\sigma(z))$.\n-   Note that $g'(z) > 0$ and $g''(z) > 0$ for all $z \\in \\mathbb{R}$.\n-   Let's analyze the sign of the term $g''(z)g(z) - (g'(z))^2$:\n    $$ g''(z)g(z) - (g'(z))^2 = \\frac{e^z}{(1+e^z)^2} \\log(1+e^z) - \\left(\\frac{e^z}{1+e^z}\\right)^2 = \\frac{e^z}{(1+e^z)^2} \\left[ \\log(1+e^z) - e^z \\right] $$\n    For any $u>0$, we know $\\log(1+u) < u$. Letting $u = e^z$, we have $\\log(1+e^z) < e^z$. Therefore, the term in the brackets is negative. Since $\\frac{e^z}{(1+e^z)^2} > 0$, we have $g''(z)g(z) - (g'(z))^2 < 0$.\n-   Now consider the full second derivative of the log-likelihood:\n    $$ \\frac{d^2\\ell}{dz^2} = \\underbrace{y}_{\\ge 0} \\underbrace{\\left( \\frac{g''(z)g(z) - (g'(z))^2}{g(z)^2} \\right)}_{<0} - \\underbrace{g''(z)}_{>0} $$\n    The first term is non-positive (negative if $y>0$, zero if $y=0$). The second term, $-g''(z)$, is strictly negative. The sum of a non-positive and a strictly negative number is strictly negative. Therefore, $\\frac{d^2\\ell}{dz^2} < 0$.\n-   Thus, for the softplus link, $\\ell(z)$ is also a strictly concave function of $z$.\n\n**Concavity of the MAP Objective**\nThe total log-likelihood is $\\mathcal{L}(x_{1:T}) = \\sum_t \\sum_i \\ell_{t,i}(z_{t,i}(x_t))$. Each term $\\ell_{t,i}$ is a composition of a strictly concave function of $z$ with an affine map from $x_t$ to $z_{t,i}$. This composition results in a concave function of $x_t$. The sum of these concave functions, $\\mathcal{L}(x_{1:T})$, is concave in $x_{1:T}$.\nThe MAP objective is $J(x_{1:T}) = \\mathcal{L}(x_{1:T}) + \\log p(x_{1:T})$.\nWe are given that the log-prior, $\\log p(x_{1:T})$, is a strictly concave function. The sum of a concave function ($\\mathcal{L}$) and a strictly concave function ($\\log p$) is strictly concave.\nTherefore, for both link functions, the MAP objective is strictly concave, which guarantees that it has a unique global maximizer.\n\n### Option-by-Option Analysis\n\n**A. With the exponential link $g_{\\exp}$, the observation log-likelihood as a function of the linear drive $z$ is strictly concave, with a negative second derivative that scales with $\\exp(z)$. When composed with a linear $z(x)$ and added to a quadratic Gaussian prior, the full MAP objective in $x$ is strictly concave, guaranteeing a unique global maximizer.**\n-   As derived, the log-likelihood $\\ell(z)$ for the exponential link is strictly concave, with second derivative $-\\exp(z)$. This derivative clearly scales with $\\exp(z)$.\n-   As argued above, the composition with the linear map $z(x)$ and the addition of the strictly concave prior results in a strictly concave MAP objective in $x$.\n-   A strictly concave function is guaranteed to have a unique global maximizer.\n-   The statement is entirely correct.\n-   **Verdict: Correct**.\n\n**B. With the softplus link $g_{\\mathrm{sp}}$, the observation log-likelihood as a function of the linear drive $z$ is concave for all nonnegative counts and all real $z$. Therefore, when composed with a linear $z(x)$ and added to a quadratic Gaussian prior, the full MAP objective in $x$ is concave, ensuring a unique global maximizer.**\n-   As derived, the log-likelihood $\\ell(z)$ for the softplus link is strictly concave. Strict concavity implies concavity, so the first part of the statement is true.\n-   Again, as argued above, the full MAP objective is the sum of a concave function (the total log-likelihood) and a strictly concave function (the log-prior), which makes the objective strictly concave.\n-   Since the objective is strictly concave, it is also concave. It also has a unique global maximizer. While concavity alone does not guarantee uniqueness, in this specific problem structure, the conclusion is correct because the objective is, in fact, strictly concave. The statement is factually correct in all its parts.\n-   **Verdict: Correct**.\n\n**C. Relative to the exponential link, the softplus link moderates curvature: for fixed observed count $y$, the second derivative of the observation log-likelihood with respect to $z$ under $g_{\\mathrm{sp}}$ remains bounded in magnitude uniformly over $z$, and the gradient with respect to $z$ is bounded below by $-1$ for all $z$. In contrast, under $g_{\\exp}$ the second derivative grows in magnitude without bound as $z \\to +\\infty$.**\n-   **Exponential Link Curvature**: For $g_{\\exp}$, $|\\frac{d^2\\ell}{dz^2}| = |-\\exp(z)| = \\exp(z)$, which grows without bound as $z \\to +\\infty$. This part is correct.\n-   **Softplus Link Curvature**: For $g_{\\mathrm{sp}}$, $\\frac{d^2\\ell}{dz^2} = y \\frac{g''g - (g')^2}{g^2} - g''$. As $z \\to \\pm\\infty$, we have $g'(z) \\to \\{1,0\\}$ and $g''(z) \\to 0$. The first term also tends to $0$. Thus, $\\frac{d^2\\ell}{dz^2} \\to 0$ as $|z| \\to \\infty$. A continuous function that tends to $0$ at $\\pm\\infty$ is bounded. This part is correct.\n-   **Softplus Link Gradient**: For $g_{\\mathrm{sp}}$, $\\frac{d\\ell}{dz} = g'(z)(\\frac{y}{g(z)}-1) = \\sigma(z)(\\frac{y}{\\log(1+e^z)}-1)$. Its derivative with respect to $z$ is $\\frac{d^2\\ell}{dz^2} < 0$, so the gradient is a strictly decreasing function.\n    -   As $z \\to -\\infty$, $\\frac{d\\ell}{dz} \\approx e^z(\\frac{y}{e^z}-1) = y-e^z \\to y$.\n    -   As $z \\to +\\infty$, $\\frac{d\\ell}{dz} \\approx 1(\\frac{y}{z}-1) \\to -1$.\n    -   Since the function strictly decreases from $y$ to $-1$, its value is always greater than $-1$. It is therefore bounded below by $-1$. This part is correct.\n-   The statement makes accurate comparisons of the properties of the two link functions.\n-   **Verdict: Correct**.\n\n**D. In the latent MAP E-step with linear $z(x)$, both links yield the same Fisher information in $x$ because the observation model is Poisson; hence the Hessians of the observation log-likelihood with respect to $x$ are identical under $g_{\\exp}$ and $g_{\\mathrm{sp}}$.**\n-   The Hessian of the log-likelihood with respect to $x_t$ is $\\nabla_{x_t}^2 \\mathcal{L}_t = C^T \\text{diag}(\\frac{d^2 \\ell_{t,i}}{dz_{t,i}^2}) C$.\n-   For $g_{\\exp}$, the diagonal elements are $-\\exp(z_{t,i})$.\n-   For $g_{\\mathrm{sp}}$, the diagonal elements are $y_{t,i} \\frac{g''g - (g')^2}{g^2} - g''$, evaluated at $z_{t,i}$.\n-   These expressions are manifestly different. The Fisher Information is the negative expectation of the Hessian. The expectations are also different, as calculated during the thought process. For $g_{\\exp}$, $-E[\\nabla_z^2 \\ell] = \\exp(z)$, while for $g_{\\mathrm{sp}}$, $-E[\\nabla_z^2 \\ell] = \\frac{(g'(z))^2}{g(z)}$. These are not equal. The premise of the statement is false, and therefore the conclusion is unsupported and false.\n-   **Verdict: Incorrect**.\n\n**E. Switching from the exponential link to the softplus link necessarily destroys the log-concavity of the posterior, invalidating Laplace approximations around the latent MAP estimate.**\n-   The log-posterior is the MAP objective function. Our analysis for option B showed that the log-posterior for the softplus link is strictly concave. A function is log-concave if its logarithm is concave. The posterior density $p(x|y)$ is log-concave because its logarithm, $\\log p(x|y)$, is a concave function.\n-   Our analysis showed that strict concavity holds for *both* link functions. Therefore, switching from exponential to softplus *preserves* log-concavity of the posterior distribution.\n-   The statement is the opposite of what is true. Laplace approximations are valid in both cases as they rely on the existence of a well-defined mode and the curvature (Hessian) at that mode.\n-   **Verdict: Incorrect**.",
            "answer": "$$\n\\boxed{ABC}\n$$"
        },
        {
            "introduction": "A key challenge in latent manifold modeling is selecting the appropriate model complexity, such as the underlying dimensionality of the latent space. This requires principled tools for model comparison that balance goodness-of-fit with a penalty for complexity. This exercise provides a rigorous derivation of two of the most important model selection tools, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), from first principles, revealing their theoretical foundations and justifying their application in a latent variable context .",
            "id": "3993305",
            "problem": "You are modeling neural population activity from $N$ simultaneously recorded neurons across $n$ statistically independent trials. On each trial $i \\in \\{1,\\dots,n\\}$ you observe a vector $\\mathbf{y}_{i} \\in \\mathbb{R}^{N}$ generated by a latent manifold model of the form\n$$\n\\mathbf{y}_{i} \\sim p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}),\n$$\nwhere $\\mathcal{M}$ denotes a model family defined by continuous latent variables $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ that are integrated out,\n$$\np(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}) \\;=\\; \\int p(\\mathbf{y}_{i}, \\mathbf{x}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}) \\, d\\mathbf{x}_{i},\n$$\nand $\\boldsymbol{\\theta} \\in \\mathbb{R}^{k}$ collects all identifiable free parameters after imposing any necessary constraints to remove non-identifiabilities (e.g., rotational symmetries), so that the model is regular in a neighborhood of the maximum likelihood estimator. Assume the trials are independent and identically distributed conditional on $\\boldsymbol{\\theta}$, and let the observed-data log-likelihood be\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\;=\\; \\sum_{i=1}^{n} \\ln p(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M}).\n$$\nLet $\\widehat{\\boldsymbol{\\theta}}$ denote the maximum likelihood estimator that maximizes $\\ell_{n}(\\boldsymbol{\\theta})$. Suppose the model family $\\mathcal{M}$ is equipped with a prior density $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ that is positive and twice continuously differentiable in a neighborhood of $\\widehat{\\boldsymbol{\\theta}}$, and define the marginal likelihood (model evidence)\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\;=\\; \\int \\bigg[\\prod_{i=1}^{n} p(\\mathbf{y}_{i}\\,|\\,\\boldsymbol{\\theta}, \\mathcal{M})\\bigg] \\, \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}.\n$$\nStarting from the following foundational bases:\n- The Laplace method for asymptotically approximating integrals of the form $\\int \\exp\\{n f(\\boldsymbol{\\theta})\\} g(\\boldsymbol{\\theta}) \\, d\\boldsymbol{\\theta}$ around a nondegenerate maximizer of $f$.\n- Asymptotic normality and consistency of maximum likelihood estimators under standard regularity conditions for independent observations.\n- The definition of Kullback-Leibler (KL) divergence and the interpretation of the expected out-of-sample log-likelihood as a proxy for minimizing the expected KL divergence to the data-generating distribution.\n\nDerive, to leading order in $n$, two model selection criteria expressed in terms of the maximized observed-data log-likelihood $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ and the parameter dimension $k$:\n1. A criterion obtained by asymptotically approximating $-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M})$ via the Laplace method, discarding additive constants that do not depend on the model $\\mathcal{M}$.\n2. A criterion obtained by asymptotically approximating $-2$ times the expected out-of-sample log-likelihood on an independent replicate of size $n$, using a second-order Taylor expansion and the asymptotic covariance of $\\widehat{\\boldsymbol{\\theta}}$.\n\nInterpret these two criteria as the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC), respectively, specialized to latent variable models using the observed-data likelihood that integrates out the latent variables. Express both criteria in closed form in terms of $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$, $k$, and $n$, as values to be minimized. Provide your final answer as a two-entry row matrix containing, in order, the analytical expressions for $\\mathrm{AIC}(\\mathcal{M})$ and $\\mathrm{BIC}(\\mathcal{M})$. No numerical evaluation is required; give exact symbolic forms. Do not include any units. If any constants arise that are shared across models and do not affect model comparisons, omit them from your final expressions.",
            "solution": "The problem requires the derivation of two model selection criteria, the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC), for a latent variable model of neural population activity. The derivation will be based on the provided foundational principles and will focus on the observed-data log-likelihood, which marginalizes out the latent variables.\n\n### 1. Derivation of the Bayesian Information Criterion (BIC)\n\nThe first criterion is derived from an asymptotic approximation of the marginal likelihood, or model evidence, $p(\\mathcal{D}\\,|\\,\\mathcal{M})$. The marginal likelihood is defined as:\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) = \\int p(\\mathcal{D}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M}) \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta} = \\int \\exp\\{\\ell_{n}(\\boldsymbol{\\theta})\\} \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}\n$$\nwhere $\\ell_{n}(\\boldsymbol{\\theta})$ is the observed-data log-likelihood and $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ is the prior over the $k$-dimensional parameter vector $\\boldsymbol{\\theta}$.\n\nWe will use the Laplace method to approximate this integral for large $n$. The method approximates the integral by concentrating the probability mass around the mode of the posterior distribution. For large $n$, the likelihood term $\\ell_{n}(\\boldsymbol{\\theta})$ dominates the prior term $\\ln \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$, so the posterior mode is asymptotically close to the maximum likelihood estimator (MLE) $\\widehat{\\boldsymbol{\\theta}}$, which maximizes $\\ell_{n}(\\boldsymbol{\\theta})$.\n\nLet's define $h(\\boldsymbol{\\theta}) = \\ell_{n}(\\boldsymbol{\\theta}) + \\ln \\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$. We perform a second-order Taylor expansion of $h(\\boldsymbol{\\theta})$ around the MLE $\\widehat{\\boldsymbol{\\theta}}$. A more precise expansion is around the posterior mode $\\tilde{\\boldsymbol{\\theta}} = \\arg\\max_{\\boldsymbol{\\theta}} h(\\boldsymbol{\\theta})$, but for large $n$, $\\tilde{\\boldsymbol{\\theta}} \\approx \\widehat{\\boldsymbol{\\theta}}$ and the leading-order terms are identical. Let's expand $\\ell_n(\\boldsymbol{\\theta})$ around $\\widehat{\\boldsymbol{\\theta}}$:\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\approx \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} \\nabla\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} \\nabla^2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})\n$$\nBy definition of the MLE, $\\nabla\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) = \\mathbf{0}$. So, the expansion simplifies to:\n$$\n\\ell_{n}(\\boldsymbol{\\theta}) \\approx \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})\n$$\nwhere $J_n(\\widehat{\\boldsymbol{\\theta}}) = -\\nabla^2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ is the observed Fisher information matrix, which is positive definite at the maximum.\n\nSubstituting this into the integral for $p(\\mathcal{D}\\,|\\,\\mathcal{M})$ and approximating the smooth prior term $\\pi(\\boldsymbol{\\theta}\\,|\\,\\mathcal{M})$ by its value at the peak, $\\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M})$, we get:\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\int \\exp\\left\\{ \\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}) \\right\\} \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, d\\boldsymbol{\\theta}\n$$\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\int \\exp\\left\\{ - \\frac{1}{2} (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}})^{T} J_{n}(\\widehat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\widehat{\\boldsymbol{\\theta}}) \\right\\} d\\boldsymbol{\\theta}\n$$\nThe integral is of a multivariate Gaussian probability density function (unnormalized) with mean $\\widehat{\\boldsymbol{\\theta}}$ and covariance matrix $J_{n}(\\widehat{\\boldsymbol{\\theta}})^{-1}$. The value of this integral is $(2\\pi)^{k/2} \\det(J_{n}(\\widehat{\\boldsymbol{\\theta}}))^{-1/2}$.\nTherefore,\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, (2\\pi)^{k/2} \\det(J_{n}(\\widehat{\\boldsymbol{\\theta}}))^{-1/2}\n$$\nBy the law of large numbers, for i.i.d. data, the observed information $J_n(\\widehat{\\boldsymbol{\\theta}})$ scales linearly with $n$. That is, $J_n(\\widehat{\\boldsymbol{\\theta}}) = O(n)$. We can write $J_n(\\widehat{\\boldsymbol{\\theta}}) \\approx n I(\\boldsymbol{\\theta}_0)$, where $I(\\boldsymbol{\\theta}_0)$ is the Fisher information for a single observation evaluated at the true parameter value $\\boldsymbol{\\theta}_0$.\nThus, $\\det(J_n(\\widehat{\\boldsymbol{\\theta}})) \\approx \\det(nI(\\boldsymbol{\\theta}_0)) = n^k \\det(I(\\boldsymbol{\\theta}_0))$. Substituting this into the approximation:\n$$\np(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx \\exp\\{\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})\\} \\, \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) \\, (2\\pi)^{k/2} (n^k \\det(I(\\boldsymbol{\\theta}_0)))^{-1/2}\n$$\nNow, we compute $-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M})$:\n$$\n-2 \\ln p(\\mathcal{D}\\,|\\,\\mathcal{M}) \\approx -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) - 2\\ln \\pi(\\widehat{\\boldsymbol{\\theta}}\\,|\\,\\mathcal{M}) - k\\ln(2\\pi) + k\\ln n + \\ln \\det(I(\\boldsymbol{\\theta}_0))\n$$\nThe BIC is obtained by retaining only the terms that grow with the sample size $n$. The term $\\ell_{n}(\\widehat{\\boldsymbol{\\theta}})$ is $O(n)$, and the term $k\\ln n$ is $O(\\ln n)$. All other terms, such as those involving the prior and the Fisher information, are $O(1)$ and are asymptotically negligible. Discarding these $O(1)$ terms yields the BIC:\n$$\n\\mathrm{BIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n\n$$\nThis criterion is to be minimized for model selection. It represents an asymptotic approximation to minus twice the log marginal evidence. It penalizes the model's lack of fit (first term) and its complexity (second term).\n\n### 2. Derivation of the Akaike Information Criterion (AIC)\n\nThe second criterion is an approximation of $-2$ times the expected out-of-sample log-likelihood. The goal is to estimate how well the fitted model will predict new, unseen data from the same generating process. Let $p_0(\\mathbf{y})$ be the true but unknown data-generating distribution. Let $\\mathcal{D}' = \\{\\mathbf{y}'_1, \\dots, \\mathbf{y}'_n\\}$ be an independent replicate dataset. The expected out-of-sample log-likelihood for a model fitted on data $\\mathcal{D}$ is:\n$$\nC = E_{\\mathcal{D}'}[\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}))] = E_{\\mathcal{D}'}\\left[ \\sum_{i=1}^{n} \\ln p(\\mathbf{y}'_i \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) \\right] = n \\int p_0(\\mathbf{y}) \\ln p(\\mathbf{y} \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) d\\mathbf{y}\n$$\nThis quantity depends on the specific training dataset $\\mathcal{D}$ through $\\widehat{\\boldsymbol{\\theta}}(\\mathcal{D})$. We desire an estimate of its expectation over all possible training sets $\\mathcal{D}$:\n$$\nE_{\\mathcal{D}}[C] = E_{\\mathcal{D}} \\left[ n \\int p_0(\\mathbf{y}) \\ln p(\\mathbf{y} \\,|\\, \\widehat{\\boldsymbol{\\theta}}(\\mathcal{D}), \\mathcal{M}) d\\mathbf{y} \\right]\n$$\nLet $\\boldsymbol{\\theta}_0$ be the parameter value that minimizes the KL divergence between the true distribution $p_0(\\mathbf{y})$ and the model family $p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M})$. Under the assumption that the model family is correctly specified, $p_0(\\mathbf{y}) = p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta}_0, \\mathcal{M})$.\nLet's define $L(\\boldsymbol{\\theta}) = E_{\\mathbf{y}\\sim p_0}[\\ln p(\\mathbf{y}\\,|\\,\\boldsymbol{\\theta},\\mathcal{M})]$. The quantity we want to estimate is $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})]$.\nUsing a second-order Taylor expansion of $L(\\widehat{\\boldsymbol{\\theta}})$ around $\\boldsymbol{\\theta}_0$:\n$$\nL(\\widehat{\\boldsymbol{\\theta}}) \\approx L(\\boldsymbol{\\theta}_0) + (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\nabla L(\\boldsymbol{\\theta}_0) + \\frac{1}{2}(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\nabla^2 L(\\boldsymbol{\\theta}_0) (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)\n$$\nBy definition, $\\boldsymbol{\\theta}_0$ maximizes $L(\\boldsymbol{\\theta})$, so $\\nabla L(\\boldsymbol{\\theta}_0) = \\mathbf{0}$. Also, $\\nabla^2 L(\\boldsymbol{\\theta}_0) = E_{\\mathbf{y}\\sim p_0} [\\nabla^2 \\ln p(\\mathbf{y} | \\boldsymbol{\\theta}_0)] = -I(\\boldsymbol{\\theta}_0)$, where $I(\\boldsymbol{\\theta}_0)$ is the Fisher information matrix for one observation.\nTaking the expectation over $\\mathcal{D}$:\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) + \\frac{1}{2} E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T (-I(\\boldsymbol{\\theta}_0)) (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0) \\right]\n$$\nUsing the trace identity and linearity of expectation, this becomes:\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) - \\frac{1}{2} \\mathrm{Tr}\\left( I(\\boldsymbol{\\theta}_0) E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\right] \\right)\n$$\nUnder the regularity conditions stated, the MLE $\\widehat{\\boldsymbol{\\theta}}$ is asymptotically normal, and its asymptotic covariance is $E_{\\mathcal{D}}\\left[ (\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)(\\widehat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}_0)^T \\right] \\approx \\frac{1}{n} I(\\boldsymbol{\\theta}_0)^{-1}$.\n$$\nE_{\\mathcal{D}}[L(\\widehat{\\boldsymbol{\\theta}})] \\approx L(\\boldsymbol{\\theta}_0) - \\frac{1}{2} \\mathrm{Tr}\\left( I(\\boldsymbol{\\theta}_0) \\frac{1}{n} I(\\boldsymbol{\\theta}_0)^{-1} \\right) = L(\\boldsymbol{\\theta}_0) - \\frac{1}{2n} \\mathrm{Tr}(I_k) = L(\\boldsymbol{\\theta}_0) - \\frac{k}{2n}\n$$\nSo, the expected out-of-sample log-likelihood is $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})] \\approx n L(\\boldsymbol{\\theta}_0) - \\frac{k}{2}$.\n\nNow we relate this to the in-sample maximized log-likelihood, $\\ell_n(\\widehat{\\boldsymbol{\\theta}})$. Let's find its expectation. A similar expansion for $\\frac{1}{n}\\ell_n(\\widehat{\\boldsymbol{\\theta}})$ shows that $E_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})]$ has an upward bias:\n$$\nE_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})] \\approx n L(\\boldsymbol{\\theta}_0) + \\frac{k}{2}\n$$\nThe difference $E_{\\mathcal{D}}[\\ell_n(\\widehat{\\boldsymbol{\\theta}})] - E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})] \\approx k$ is the \"optimism\" of the in-sample log-likelihood. This suggests that $\\ell_n(\\widehat{\\boldsymbol{\\theta}}) - k$ is an approximately unbiased estimator of the expected out-of-sample log-likelihood $E_{\\mathcal{D}}[n L(\\widehat{\\boldsymbol{\\theta}})]$.\nThe Akaike Information Criterion is defined as $-2$ times this corrected log-likelihood:\n$$\n\\mathrm{AIC}(\\mathcal{M}) = -2(\\ell_n(\\widehat{\\boldsymbol{\\theta}}) - k) = -2\\ell_n(\\widehat{\\boldsymbol{\\theta}}) + 2k\n$$\nThis criterion is also to be minimized for model selection. It provides an estimate of the expected prediction error on new data, penalizing the in-sample fit by a term proportional to the number of parameters.\n\nThe key insight is that both derivations, while originating from different philosophical standpoints (Bayesian vs. frequentist), depend only on the asymptotic properties of the observed-data log-likelihood $\\ell_n(\\boldsymbol{\\theta})$ and its maximizer $\\widehat{\\boldsymbol{\\theta}}$. The fact that $\\ell_n(\\boldsymbol{\\theta})$ is derived from a latent variable model by marginalization is handled by the problem's assumption that $\\ell_n(\\boldsymbol{\\theta})$ still satisfies the necessary regularity conditions for the MLE asymptotics to hold, with $k$ being the number of identifiable parameters.\n\nIn summary, the two criteria are:\n1. $\\mathrm{BIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n$\n2. $\\mathrm{AIC}(\\mathcal{M}) = -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + 2k$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + 2k & -2\\ell_{n}(\\widehat{\\boldsymbol{\\theta}}) + k \\ln n \\end{pmatrix}}\n$$"
        }
    ]
}