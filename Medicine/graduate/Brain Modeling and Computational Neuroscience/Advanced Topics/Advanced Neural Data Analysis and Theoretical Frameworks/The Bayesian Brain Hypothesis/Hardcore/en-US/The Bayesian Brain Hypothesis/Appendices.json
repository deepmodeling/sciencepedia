{
    "hands_on_practices": [
        {
            "introduction": "At the heart of the Bayesian brain lies the mathematical operation of combining prior knowledge with sensory evidence. This first exercise walks you through the foundational derivation for Gaussian distributions, a common modeling choice for neural variables. By completing the square, you will see precisely how the posterior belief emerges as a precision-weighted average of the prior and the likelihood, a central principle in neural computation. ",
            "id": "4063569",
            "problem": "In the Bayesian brain hypothesis, cortical computation is often modeled as probabilistic inference that combines internally generated predictions (priors) with incoming sensory evidence (likelihoods), with the relative influence modulated by precision (inverse variance). Consider a neuromorphic sensory estimation module for a single latent cause $s$ with prior $p(s)=\\mathcal{N}(0,1)$, and a scalar observation $x$ generated conditionally by $p(x\\mid s)=\\mathcal{N}(s,0.25)$. Starting only from Bayes' rule $p(s\\mid x)\\propto p(x\\mid s)\\,p(s)$ and the definition of precision as the inverse of variance, derive the posterior distribution $p(s\\mid x)$ by explicitly completing the square in the exponent of the product of Gaussians. Then, compute the posterior mean and posterior variance of $s$ given $x$, and interpret the posterior mean as a precision-weighted average of the prior mean and the observed value. \n\nExpress your final answer as a single row matrix containing the posterior mean and posterior variance, in exact symbolic form as functions of $x$. No rounding is required. No physical units are involved.",
            "solution": "The problem requires the derivation of the posterior distribution $p(s \\mid x)$ for a latent variable $s$ given an observation $x$, based on a specified prior $p(s)$ and likelihood $p(x \\mid s)$. The derivation must proceed from Bayes' rule by completing the square in the exponent of the product of Gaussian densities. Subsequently, the posterior mean and variance are to be computed and interpreted.\n\nThe provided distributions are:\n1.  Prior distribution for $s$: $p(s) = \\mathcal{N}(\\mu_{prior}, \\sigma_{prior}^2)$, where the mean is $\\mu_{prior} = 0$ and the variance is $\\sigma_{prior}^2 = 1$.\n2.  Likelihood of observation $x$ given $s$: $p(x \\mid s) = \\mathcal{N}(s, \\sigma_{like}^2)$, where the variance is $\\sigma_{like}^2 = 0.25$. Note that the mean of this distribution is the latent variable $s$.\n\nThe probability density function (PDF) for a general Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by $f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$.\nThus, the specific PDFs for the prior and likelihood are:\n$p(s) = \\frac{1}{\\sqrt{2\\pi(1)}} \\exp\\left(-\\frac{(s-0)^2}{2(1)}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{s^2}{2}\\right)$\n$p(x \\mid s) = \\frac{1}{\\sqrt{2\\pi(0.25)}} \\exp\\left(-\\frac{(x-s)^2}{2(0.25)}\\right) = \\frac{1}{\\sqrt{0.5\\pi}} \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right)$\n\nAccording to Bayes' rule, the posterior distribution $p(s \\mid x)$ is proportional to the product of the likelihood and the prior:\n$p(s \\mid x) \\propto p(x \\mid s) p(s)$\n\nSubstituting the expressions for the PDFs, we get:\n$p(s \\mid x) \\propto \\left[ \\frac{1}{\\sqrt{0.5\\pi}} \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{s^2}{2}\\right) \\right]$\n\nSince we are working with proportionality, the constant normalization factors can be ignored. The posterior is proportional to the exponential part:\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right) \\exp\\left(-\\frac{s^2}{2}\\right) = \\exp\\left(-\\frac{(x-s)^2}{0.5} - \\frac{s^2}{2}\\right)$\n\nLet's analyze the exponent, which we denote as $\\Phi(s)$:\n$\\Phi(s) = -\\left(\\frac{(s-x)^2}{0.5} + \\frac{s^2}{2}\\right) = -\\left(2(s-x)^2 + \\frac{s^2}{2}\\right)$\n\nTo identify the form of the posterior distribution, we expand the terms in the exponent and collect powers of $s$. A posterior of Gaussian form $\\mathcal{N}(\\mu_{post}, \\sigma_{post}^2)$ will have an exponent of the form $-\\frac{(s-\\mu_{post})^2}{2\\sigma_{post}^2} + C$, where $C$ is a constant with respect to $s$.\n$\\Phi(s) = -\\left(2(s^2 - 2sx + x^2) + \\frac{s^2}{2}\\right)$\n$\\Phi(s) = -\\left(2s^2 - 4sx + 2x^2 + \\frac{s^2}{2}\\right)$\n$\\Phi(s) = -\\left(\\left(2 + \\frac{1}{2}\\right)s^2 - 4sx + 2x^2\\right)$\n$\\Phi(s) = -\\left(\\frac{5}{2}s^2 - 4sx + 2x^2\\right)$\n\nNow, we complete the square for the terms involving $s$. The general form is $As^2+Bs+C$. We factor out $A$ from the terms involving $s$: $A(s^2 + \\frac{B}{A}s) + C = A\\left(s+\\frac{B}{2A}\\right)^2 + C - \\frac{B^2}{4A}$.\nIn our expression, the term in the parenthesis is $\\frac{5}{2}s^2 - 4sx + 2x^2$. Here, $A = \\frac{5}{2}$ and $B = -4x$.\n$\\frac{5}{2}s^2 - 4sx + 2x^2 = \\frac{5}{2}\\left(s^2 - \\frac{2}{5}(4x)s\\right) + 2x^2$\n$= \\frac{5}{2}\\left(s^2 - \\frac{8x}{5}s\\right) + 2x^2$\n$= \\frac{5}{2}\\left[\\left(s - \\frac{4x}{5}\\right)^2 - \\left(\\frac{4x}{5}\\right)^2\\right] + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{5}{2}\\left(\\frac{16x^2}{25}\\right) + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{8x^2}{5} + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{8x^2}{5} + \\frac{10x^2}{5}$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 + \\frac{2x^2}{5}$\n\nSubstituting this back into the expression for $\\Phi(s)$:\n$\\Phi(s) = -\\left(\\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 + \\frac{2x^2}{5}\\right) = -\\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{2x^2}{5}$\n\nTherefore, the posterior distribution is:\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{1}{2} \\cdot 5 \\left(s - \\frac{4x}{5}\\right)^2\\right) \\exp\\left(-\\frac{2x^2}{5}\\right)$\n\nThe term $\\exp(-2x^2/5)$ is constant with respect to $s$ and can be absorbed into the normalization constant. The remaining expression has the form of a Gaussian PDF for $s$:\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{(s - \\mu_{post})^2}{2\\sigma_{post}^2}\\right)$\n\nBy comparing the derived exponent with the general form, we can identify the posterior mean $\\mu_{post}$ and posterior variance $\\sigma_{post}^2$:\n$\\mu_{post} = \\frac{4x}{5}$\n$\\frac{1}{2\\sigma_{post}^2} = \\frac{5}{2} \\implies \\sigma_{post}^2 = \\frac{1}{5}$\n\nSo, the posterior distribution is a Gaussian: $p(s \\mid x) = \\mathcal{N}\\left(\\frac{4x}{5}, \\frac{1}{5}\\right)$.\nThe posterior mean is $\\mu_{post} = \\frac{4x}{5}$.\nThe posterior variance is $\\sigma_{post}^2 = \\frac{1}{5}$.\n\nTo interpret the posterior mean as a precision-weighted average, we first define precisions. Precision $\\lambda$ is the inverse of variance, $\\lambda = 1/\\sigma^2$.\nPrior precision: $\\lambda_{prior} = 1/\\sigma_{prior}^2 = 1/1 = 1$.\nLikelihood precision: $\\lambda_{like} = 1/\\sigma_{like}^2 = 1/0.25 = 4$.\n\nThe general formula for the posterior mean $\\mu_{post}$ in the case of a Gaussian prior and Gaussian likelihood is a precision-weighted average of the prior mean $\\mu_{prior}$ and the data $x$ (which is the mean of the likelihood function evaluated at $s=x$):\n$\\mu_{post} = \\frac{\\lambda_{prior}\\mu_{prior} + \\lambda_{like}x}{\\lambda_{prior} + \\lambda_{like}}$\nThe posterior precision is the sum of the prior and likelihood precisions:\n$\\lambda_{post} = \\lambda_{prior} + \\lambda_{like}$\nAnd the posterior variance is the inverse of the posterior precision:\n$\\sigma_{post}^2 = \\frac{1}{\\lambda_{post}} = \\frac{1}{\\lambda_{prior} + \\lambda_{like}}$\n\nLet's verify our results using these general formulas with the given values:\n$\\mu_{prior} = 0$, $\\lambda_{prior} = 1$, $\\lambda_{like} = 4$.\n$\\mu_{post} = \\frac{(1)(0) + (4)x}{1 + 4} = \\frac{4x}{5}$\n$\\sigma_{post}^2 = \\frac{1}{1 + 4} = \\frac{1}{5}$\n\nThe results match perfectly. The interpretation is that the posterior mean $\\mu_{post}$ is a weighted average of the prior's belief about the mean ($\\mu_{prior}=0$) and the sensory evidence ($x$). The weights are the respective precisions. The data has a precision of $4$ while the prior has a precision of $1$, so the sensory data has four times the influence on the final estimate as the prior belief. This can be seen by writing the posterior mean as:\n$\\mu_{post} = \\left(\\frac{\\lambda_{prior}}{\\lambda_{prior}+\\lambda_{like}}\\right)\\mu_{prior} + \\left(\\frac{\\lambda_{like}}{\\lambda_{prior}+\\lambda_{like}}\\right)x = \\left(\\frac{1}{5}\\right)(0) + \\left(\\frac{4}{5}\\right)x = \\frac{4x}{5}$.\n\nThe final answer requires the posterior mean and posterior variance in a single row matrix.\nPosterior mean: $\\frac{4x}{5}$\nPosterior variance: $\\frac{1}{5}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{4x}{5} & \\frac{1}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With the core mechanics of Bayesian integration in hand, we can now use this framework to make testable predictions about perception and behavior. This problem simulates a perceptual estimation task, asking you to predict how an observer's report changes when the reliability of their prior beliefs or sensory information is manipulated. This exercise demonstrates the explanatory power of Bayesian models in accounting for how context and stimulus quality jointly shape our perception. ",
            "id": "4027112",
            "problem": "In a one-dimensional perceptual estimation task under the Bayesian brain hypothesis, a latent stimulus $s \\in \\mathbb{R}$ is encoded with a Gaussian prior and observed through a Gaussian likelihood. The prior is $p(s) = \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ and the likelihood is $p(x \\mid s) = \\mathcal{N}(s, \\sigma_{\\ell}^{2})$ for an observed measurement $x$. Define the precision of the prior as $\\lambda_{0} = 1/\\sigma_{0}^{2}$ and the precision of the likelihood as $\\lambda_{\\ell} = 1/\\sigma_{\\ell}^{2}$. Assume that behavior reports the posterior mean under squared-error loss.\n\nStarting only from Bayesâ€™ theorem and the definition of the Gaussian (Normal) probability density, derive the posterior mean as a function of $\\lambda_{0}$, $\\lambda_{\\ell}$, $\\mu_{0}$, and $x$ by completing the square in the exponent of the unnormalized posterior. Then use this to predict how the behavioral estimate changes when manipulating prior precision versus likelihood precision.\n\nConsider the following scientifically plausible parameters for a heading-direction estimation task, where angles are in degrees and the units of variance are $\\mathrm{deg}^{2}$:\n- Prior mean $\\mu_{0} = 2^{\\circ}$,\n- Prior variance $\\sigma_{0}^{2} = 9\\,\\mathrm{deg}^{2}$,\n- Likelihood variance $\\sigma_{\\ell}^{2} = 4\\,\\mathrm{deg}^{2}$,\n- Observed measurement $x = 8^{\\circ}$.\n\nDefine the baseline predicted report $y_{0}$ as the posterior mean with the above parameters. Consider two manipulations:\n- Manipulation A: increase prior precision by a factor of $3$ (i.e., replace $\\lambda_{0}$ by $3\\lambda_{0}$), leaving $\\lambda_{\\ell}$ unchanged; denote the predicted report by $y_{A}$.\n- Manipulation B: increase likelihood precision by a factor of $3$ (i.e., replace $\\lambda_{\\ell}$ by $3\\lambda_{\\ell}$), leaving $\\lambda_{0}$ unchanged; denote the predicted report by $y_{B}$.\n\nCompute the behavioral difference $\\Delta = y_{B} - y_{A}$ predicted by the Bayesian observer for these manipulations. Express the final answer in degrees and round your answer to four significant figures.",
            "solution": "We begin by deriving the posterior distribution $p(s \\mid x)$ for the latent stimulus $s$ given the measurement $x$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$p(s \\mid x) \\propto p(x \\mid s) p(s)$$\nThe prior $p(s)$ is a Gaussian distribution with mean $\\mu_{0}$ and variance $\\sigma_{0}^{2}$:\n$$p(s) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\nThe likelihood $p(x \\mid s)$ is also a Gaussian distribution with mean $s$ and variance $\\sigma_{\\ell}^{2}$:\n$$p(x \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\ell}^{2}}} \\exp\\left(-\\frac{(x-s)^{2}}{2\\sigma_{\\ell}^{2}}\\right)$$\nMultiplying these two, we find the posterior is proportional to:\n$$p(s \\mid x) \\propto \\exp\\left(-\\frac{(s-\\mu_{0})^{2}}{2\\sigma_{0}^{2}} - \\frac{(x-s)^{2}}{2\\sigma_{\\ell}^{2}}\\right)$$\nTo find the parameters of the posterior distribution, we focus on the exponent. We express the variances in terms of precisions, $\\lambda_{0} = 1/\\sigma_{0}^{2}$ and $\\lambda_{\\ell} = 1/\\sigma_{\\ell}^{2}$. The expression in the exponent is:\n$$E = -\\frac{1}{2}\\left( \\lambda_{0}(s-\\mu_{0})^{2} + \\lambda_{\\ell}(x-s)^{2} \\right)$$\nWe expand the squared terms:\n$$E = -\\frac{1}{2}\\left( \\lambda_{0}(s^{2} - 2s\\mu_{0} + \\mu_{0}^{2}) + \\lambda_{\\ell}(x^{2} - 2xs + s^{2}) \\right)$$\nNow, we group terms by powers of $s$:\n$$E = -\\frac{1}{2}\\left( s^{2}(\\lambda_{0} + \\lambda_{\\ell}) - 2s(\\lambda_{0}\\mu_{0} + \\lambda_{\\ell}x) + (\\lambda_{0}\\mu_{0}^{2} + \\lambda_{\\ell}x^{2}) \\right)$$\nThe goal is to complete the square for the terms involving $s$ to match the form of a Gaussian exponent, which is $-\\frac{(s - \\mu_{\\text{post}})^{2}}{2\\sigma_{\\text{post}}^{2}} = -\\frac{1}{2\\sigma_{\\text{post}}^{2}}(s^{2} - 2s\\mu_{\\text{post}} + \\mu_{\\text{post}}^{2})$.\nLet $\\lambda_{\\text{post}} = 1/\\sigma_{\\text{post}}^{2}$. Then the exponent is $-\\frac{\\lambda_{\\text{post}}}{2}(s^{2} - 2s\\mu_{\\text{post}} + \\mu_{\\text{post}}^{2})$.\nComparing the coefficient of $s^{2}$, we identify the posterior precision $\\lambda_{\\text{post}}$:\n$$\\lambda_{\\text{post}} = \\lambda_{0} + \\lambda_{\\ell}$$\nThis implies the posterior variance is $\\sigma_{\\text{post}}^{2} = \\frac{1}{\\lambda_{0} + \\lambda_{\\ell}}$.\nNext, comparing the coefficient of the $-2s$ term, we have:\n$$\\lambda_{\\text{post}}\\mu_{\\text{post}} = \\lambda_{0}\\mu_{0} + \\lambda_{\\ell}x$$\nSolving for the posterior mean, $\\mu_{\\text{post}}$, we get:\n$$\\mu_{\\text{post}} = \\frac{\\lambda_{0}\\mu_{0} + \\lambda_{\\ell}x}{\\lambda_{0} + \\lambda_{\\ell}}$$\nThe problem states that the behavioral report is the posterior mean. Let's call the report $y$. So, $y = \\mu_{\\text{post}}$. This equation shows that the behavioral estimate is a weighted average of the prior mean $\\mu_{0}$ and the sensory measurement $x$, where the weights are the respective precisions $\\lambda_{0}$ and $\\lambda_{\\ell}$. Increasing the prior precision $\\lambda_{0}$ will pull the estimate closer to the prior mean $\\mu_{0}$, while increasing the likelihood precision $\\lambda_{\\ell}$ will pull the estimate closer to the measurement $x$.\n\nNow we compute the numerical values. The given parameters are:\n$\\mu_{0} = 2$\n$\\sigma_{0}^{2} = 9 \\implies \\lambda_{0} = 1/9$\n$\\sigma_{\\ell}^{2} = 4 \\implies \\lambda_{\\ell} = 1/4$\n$x = 8$\nAll values involving angles are in degrees.\n\nFor Manipulation A, we increase the prior precision by a factor of $3$. The new parameters are:\n$\\lambda_{0, A} = 3 \\times \\lambda_{0} = 3 \\times \\frac{1}{9} = \\frac{1}{3}$\n$\\lambda_{\\ell, A} = \\lambda_{\\ell} = \\frac{1}{4}$\nThe predicted report $y_{A}$ is:\n$$y_{A} = \\frac{\\lambda_{0, A}\\mu_{0} + \\lambda_{\\ell, A}x}{\\lambda_{0, A} + \\lambda_{\\ell, A}} = \\frac{(\\frac{1}{3})(2) + (\\frac{1}{4})(8)}{\\frac{1}{3} + \\frac{1}{4}} = \\frac{\\frac{2}{3} + 2}{\\frac{4+3}{12}} = \\frac{\\frac{8}{3}}{\\frac{7}{12}} = \\frac{8}{3} \\times \\frac{12}{7} = \\frac{32}{7}$$\n\nFor Manipulation B, we increase the likelihood precision by a factor of $3$. The new parameters are:\n$\\lambda_{0, B} = \\lambda_{0} = \\frac{1}{9}$\n$\\lambda_{\\ell, B} = 3 \\times \\lambda_{\\ell} = 3 \\times \\frac{1}{4} = \\frac{3}{4}$\nThe predicted report $y_{B}$ is:\n$$y_{B} = \\frac{\\lambda_{0, B}\\mu_{0} + \\lambda_{\\ell, B}x}{\\lambda_{0, B} + \\lambda_{\\ell, B}} = \\frac{(\\frac{1}{9})(2) + (\\frac{3}{4})(8)}{\\frac{1}{9} + \\frac{3}{4}} = \\frac{\\frac{2}{9} + 6}{\\frac{4+27}{36}} = \\frac{\\frac{56}{9}}{\\frac{31}{36}} = \\frac{56}{9} \\times \\frac{36}{31} = \\frac{224}{31}$$\n\nFinally, we compute the behavioral difference $\\Delta = y_{B} - y_{A}$:\n$$\\Delta = \\frac{224}{31} - \\frac{32}{7}$$\nTo subtract these fractions, we find a common denominator, which is $31 \\times 7 = 217$.\n$$\\Delta = \\frac{224 \\times 7 - 32 \\times 31}{217} = \\frac{1568 - 992}{217} = \\frac{576}{217}$$\nNow we compute the numerical value and round to four significant figures:\n$$\\Delta = \\frac{576}{217} \\approx 2.654377...$$\nRounding to four significant figures gives $2.654$. The units are degrees.",
            "answer": "$$\n\\boxed{2.654}\n$$"
        },
        {
            "introduction": "A key goal of computational neuroscience is not just to build models, but to use them to understand the sources of variability and suboptimality in neural processing and behavior. This final practice is a complete modeling exercise where you will fit competing Bayesian observer models to psychometric data from a task where a stimulus $s$ must be classified as 'right' ($s \\gt 0$) versus 'left' ($s \\lt 0$). By implementing a model comparison procedure, you will learn to distinguish whether an observer's suboptimal performance stems from an inaccurate internal model of the world (a faulty prior) or from excessive sensory noise, providing a powerful tool for scientific inference. ",
            "id": "4027148",
            "problem": "A Bayesian observer model in perception posits that an observer combines sensory evidence with prior expectations to make decisions. Consider a binary discrimination task in which, on each trial, a scalar stimulus $s \\in \\mathbb{R}$ must be classified as being on the \"right\" side ($s > 0$) versus the \"left\" side ($s < 0$). The Bayesian brain hypothesis models this observer as performing probabilistic inference according to Bayes' rule under Gaussian assumptions, with occasional lapses resulting in random choices independent of the stimulus. The psychometric data consist of counts of \"right\" choices across stimulus levels.\n\nFundamental base:\n- Bayes' rule: for a latent variable $s$ with prior $p(s)$ and observation $x$ with likelihood $p(x \\mid s)$, the posterior is $p(s \\mid x) \\propto p(x \\mid s)p(s)$.\n- Gaussian prior and likelihood: assume $p(s) = \\mathcal{N}(\\mu_p,\\tau_p^2)$ and $p(x \\mid s) = \\mathcal{N}(s,\\sigma^2)$.\n- Binary decision: under symmetric loss and a two-alternative forced choice (2AFC) with categories \"right\" versus \"left\", an optimal Bayesian observer classifies \"right\" when the posterior evidence favors the \"right\" hypothesis (equivalently, the posterior mean exceeds zero).\n- Lapse mechanism: with probability $\\lambda \\in [0,1)$, the observer ignores stimulus information and makes a random choice, yielding $0.5$ probability of \"right\" irrespective of $s$.\n\nGiven these bases, the program must:\n- Derive from first principles the psychometric function $p_{\\mathrm{right}}(s)$ for the Bayesian observer with lapse rate. It must formalize and implement the probabilistic choice rule implied by the Gaussian prior and likelihood, the binary thresholding decision, and the lapse component.\n- Fit psychometric data by maximum likelihood under two model classes to distinguish whether suboptimality arises primarily from incorrect priors or from increased internal noise:\n  1. Incorrect-prior model: assumes known sensory noise variance $\\sigma_0^2$, and fits prior mean $\\mu_p$, prior standard deviation $\\tau_p$, and lapse rate $\\lambda$.\n  2. Incorrect-noise model: assumes a flat prior (zero mean and effectively infinite variance so that it does not bias the decision boundary), and fits sensory noise standard deviation $\\sigma$ and lapse rate $\\lambda$.\n- Use the binomial likelihood for observed counts at each stimulus level. Apply model comparison via the Bayesian Information Criterion (BIC), where for a model with $k$ free parameters and $N$ total trials, $\\mathrm{BIC} = k \\ln N - 2 \\ln \\hat{L}$ with $\\hat{L}$ the maximum likelihood. Choose the model with the lower $\\mathrm{BIC}$ as the explanatory mechanism.\n\nData generation for the test suite:\n- For each test case, define stimulus levels $s_i$ and trial counts $n_i$. Generate synthetic counts $k_i$ deterministically by computing the expected number of \"right\" choices as $n_i p_{\\mathrm{right}}(s_i)$ under the specified generative parameters and rounding to the nearest integer.\n- The program must not use randomness; it must reproduce the counts deterministically from the generative parameters as specified.\n\nImplement and test on the following test suite that probes different regimes:\n- Test Case A (happy path, incorrect priors): baseline sensory noise standard deviation $\\sigma_0 = 1.0$, generative prior mean $\\mu_p^{\\mathrm{true}} = 0.8$, generative prior standard deviation $\\tau_p^{\\mathrm{true}} = 0.7$, generative sensory noise standard deviation $\\sigma^{\\mathrm{true}} = 1.0$, generative lapse rate $\\lambda^{\\mathrm{true}} = 0.05$. Use stimulus levels $s_i \\in \\{-3,-2,-1,0,1,2,3\\}$ with $n_i = 1000$ trials each. The ground-truth mechanism label for this case is \"prior\".\n- Test Case B (happy path, increased noise): baseline sensory noise standard deviation $\\sigma_0 = 1.0$, generative prior mean $\\mu_p^{\\mathrm{true}} = 0.0$, generative prior standard deviation $\\tau_p^{\\mathrm{true}}$ effectively infinite (flat), generative sensory noise standard deviation $\\sigma^{\\mathrm{true}} = 2.0$, generative lapse rate $\\lambda^{\\mathrm{true}} = 0.05$. Use stimulus levels $s_i \\in \\{-3,-2,-1,0,1,2,3\\}$ with $n_i = 1000$ trials each. The ground-truth mechanism label for this case is \"noise\".\n- Test Case C (boundary condition, high lapse with incorrect priors): baseline sensory noise standard deviation $\\sigma_0 = 1.5$, generative prior mean $\\mu_p^{\\mathrm{true}} = 0.6$, generative prior standard deviation $\\tau_p^{\\mathrm{true}} = 0.5$, generative sensory noise standard deviation $\\sigma^{\\mathrm{true}} = 1.5$, generative lapse rate $\\lambda^{\\mathrm{true}} = 0.3$. Use stimulus levels $s_i \\in \\{-3,-2,-1,0,1,2,3\\}$ with $n_i = 1000$ trials each. The ground-truth mechanism label for this case is \"prior\".\n\nSolution expectations:\n- Derive the decision boundary and the psychometric function $p_{\\mathrm{right}}(s)$ implied by the Gaussian prior and likelihood and the binary decision rule, then incorporate the lapse mechanism.\n- Implement maximum likelihood estimation for each model class with appropriate parameter constraints: $\\sigma > 0$, $\\tau_p > 0$, and $\\lambda \\in [0,1)$. Use robust numerical optimization.\n- Compute $\\mathrm{BIC}$ for each model and select the lower $\\mathrm{BIC}$. For each test case, output a boolean that is $True$ if the selected model matches the ground-truth label and $False$ otherwise.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\"), where each result is a boolean corresponding to Test Cases A, B, and C in order.",
            "solution": "The problem requires us to develop a computational model of a Bayesian observer for a binary perceptual decision task, fit it to synthetic psychometric data, and use model comparison to determine the likely source of suboptimality. The solution involves three main stages: first, the mathematical derivation of the observer's psychometric function; second, the formulation of the statistical fitting procedure using maximum likelihood and model comparison via the Bayesian Information Criterion (BIC); and third, the implementation and testing of this procedure on provided test cases.\n\n### Derivation of the Psychometric Function\n\nThe observer's goal is to classify a stimulus $s \\in \\mathbb{R}$ as \"right\" ($s > 0$) or \"left\" ($s < 0$). The observer receives a noisy sensory measurement $x$, which is related to the true stimulus $s$ through a likelihood distribution $p(x \\mid s)$. This measurement is then combined with a prior belief about the stimulus, $p(s)$, to form a posterior distribution $p(s \\mid x)$ via Bayes' rule:\n\n$$\np(s \\mid x) \\propto p(x \\mid s) p(s)\n$$\n\nThe problem specifies Gaussian forms for both the prior and the likelihood:\n- Prior: $p(s) = \\mathcal{N}(s; \\mu_p, \\tau_p^2)$, representing the observer's belief about the distribution of stimuli before seeing the current measurement.\n- Likelihood: $p(x \\mid s) = \\mathcal{N}(x; s, \\sigma^2)$, representing the sensory noise corrupting the true stimulus value.\n\nSince the prior and likelihood are conjugate Gaussian distributions, the posterior distribution $p(s \\mid x)$ is also a Gaussian, $p(s \\mid x) = \\mathcal{N}(s; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$. The posterior precision ($1/\\sigma_{\\text{post}}^2$) is the sum of the prior and likelihood precisions:\n\n$$\n\\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\tau_p^2} + \\frac{1}{\\sigma^2}\n$$\n\nThe posterior mean, $\\mu_{\\text{post}}$, is a precision-weighted average of the prior mean $\\mu_p$ and the sensory measurement $x$:\n\n$$\n\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left( \\frac{\\mu_p}{\\tau_p^2} + \\frac{x}{\\sigma^2} \\right) = \\frac{\\sigma^2 \\mu_p + \\tau_p^2 x}{\\sigma^2 + \\tau_p^2}\n$$\n\nThe observer makes a \"right\" decision if the posterior evidence favors the \"right\" category. With a symmetric loss function, this corresponds to deciding \"right\" if the posterior mean of the stimulus is greater than $0$:\n\n$$\n\\mu_{\\text{post}} > 0\n$$\n\nSubstituting the expression for $\\mu_{\\text{post}}$ and solving for $x$:\n$$\n\\frac{\\sigma^2 \\mu_p + \\tau_p^2 x}{\\sigma^2 + \\tau_p^2} > 0 \\implies \\tau_p^2 x > -\\sigma^2 \\mu_p \\implies x > -\\frac{\\sigma^2 \\mu_p}{\\tau_p^2}\n$$\nThis defines a decision threshold, $T = -\\frac{\\sigma^2 \\mu_p}{\\tau_p^2}$, on the sensory measurement $x$. The observer chooses \"right\" if and only if $x > T$.\n\nThe probability of choosing \"right\" for a given stimulus $s$, denoted $p_{\\text{bayes\\_right}}(s)$, is the probability that the random measurement $x \\sim \\mathcal{N}(s, \\sigma^2)$ exceeds this threshold $T$:\n\n$$\np_{\\text{bayes\\_right}}(s) = P(x > T \\mid s)\n$$\n\nThis probability can be calculated using the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = P(Z \\le z)$ where $Z \\sim \\mathcal{N}(0,1)$.\n\n$$\np_{\\text{bayes\\_right}}(s) = P\\left(\\frac{x-s}{\\sigma} > \\frac{T-s}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{T-s}{\\sigma}\\right) = \\Phi\\left(-\\frac{T-s}{\\sigma}\\right) = \\Phi\\left(\\frac{s-T}{\\sigma}\\right)\n$$\n\nSubstituting the expression for $T$:\n\n$$\np_{\\text{bayes\\_right}}(s) = \\Phi\\left(\\frac{s - (-\\frac{\\sigma^2 \\mu_p}{\\tau_p^2})}{\\sigma}\\right) = \\Phi\\left(\\frac{s + \\frac{\\sigma^2 \\mu_p}{\\tau_p^2}}{\\sigma}\\right)\n$$\n\nFinally, we incorporate the lapse rate $\\lambda$, which is the probability of making a random choice regardless of the stimulus. With probability $1-\\lambda$, the observer follows the Bayesian decision rule, and with probability $\\lambda$, the observer guesses, choosing \"right\" with probability $0.5$. The full psychometric function $p_{\\mathrm{right}}(s)$ is therefore a mixture model:\n\n$$\np_{\\mathrm{right}}(s) = (1 - \\lambda) p_{\\text{bayes\\_right}}(s) + 0.5 \\lambda\n$$\n\n### Model Definitions and Specialization\n\nWe consider two distinct models of suboptimality:\n\n1.  **Incorrect-Prior Model:** Suboptimality arises from a biased or incorrectly calibrated prior. Here, the sensory noise $\\sigma$ is assumed to be known and fixed at a baseline value $\\sigma_0$. The free parameters to be fitted are the prior mean $\\mu_p$, the prior standard deviation $\\tau_p$, and the lapse rate $\\lambda$. The psychometric function is:\n    $$\n    p_{\\mathrm{right}}(s; \\mu_p, \\tau_p, \\lambda | \\sigma_0) = (1-\\lambda) \\Phi\\left( \\frac{s + \\frac{\\sigma_0^2 \\mu_p}{\\tau_p^2}}{\\sigma_0} \\right) + 0.5 \\lambda\n    $$\n\n2.  **Incorrect-Noise Model:** Suboptimality arises from elevated sensory noise. Here, the prior is assumed to be \"flat\" or uninformative, which corresponds to setting $\\mu_p=0$ and letting $\\tau_p \\to \\infty$. In this limit, the decision threshold $T = -\\frac{\\sigma^2 \\mu_p}{\\tau_p^2}$ approaches $0$. The decision rule simplifies to $x > 0$. The free parameters are the sensory noise standard deviation $\\sigma$ and the lapse rate $\\lambda$. The psychometric function becomes:\n    $$\n    p_{\\mathrm{right}}(s; \\sigma, \\lambda) = (1-\\lambda) \\Phi\\left(\\frac{s}{\\sigma}\\right) + 0.5 \\lambda\n    $$\n\n### Model Fitting and Comparison\n\nTo fit these models to data, we use maximum likelihood estimation. The data consist of pairs $(k_i, n_i)$ for each stimulus level $s_i$, where $k_i$ is the number of \"right\" choices out of $n_i$ total trials. The likelihood of observing $k_i$ for a given $s_i$ follows a binomial distribution:\n\n$$\nL_i(\\theta) = P(k_i \\mid n_i, s_i; \\theta) = \\binom{n_i}{k_i} p_{\\mathrm{right}}(s_i; \\theta)^{k_i} (1 - p_{\\mathrm{right}}(s_i; \\theta))^{n_i - k_i}\n$$\nwhere $\\theta$ represents the set of free parameters for a given model. The total log-likelihood is the sum over all stimulus levels: $\\ln \\hat{L}(\\theta) = \\sum_i \\ln L_i(\\theta)$. We seek the parameters $\\hat{\\theta}$ that maximize this function, which is equivalent to minimizing the negative log-likelihood (NLL).\n\nFor model comparison, we use the Bayesian Information Criterion (BIC):\n$$\n\\mathrm{BIC} = k \\ln N - 2 \\ln \\hat{L}\n$$\nwhere $k$ is the number of free parameters in the model ($k=3$ for the prior model, $k=2$ for the noise model), $N = \\sum_i n_i$ is the total number of trials, and $\\hat{L}$ is the maximized likelihood value. The model with the lower BIC is preferred, as it provides a better trade-off between goodness of fit and model complexity.\n\nThe implementation will proceed by first generating the synthetic count data $k_i = \\text{round}(n_i p_{\\mathrm{right}}(s_i))$ using the true generative parameters for each test case. Then, for each dataset, we will minimize the NLL for both the incorrect-prior and incorrect-noise models using numerical optimization, subject to parameter constraints ($\\tau_p > 0$, $\\sigma > 0$, $0 \\le \\lambda < 1$). Finally, we will compute the BIC for each model, select the one with the lower BIC, and check if this matches the ground-truth generating mechanism.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the model comparison for the Bayesian observer.\n    \"\"\"\n\n    # Helper function to prevent log(0) issues\n    def _safe_log(x):\n        return np.log(np.maximum(x, 1e-9))\n\n    # --- Psychometric Function Definitions ---\n\n    def psychometric_function_prior(s, mu_p, tau_p, lam, sigma_0):\n        \"\"\"\n        Psychometric function for the 'incorrect-prior' model.\n        p_right(s) = (1-lambda) * Phi((s + (sigma_0^2 * mu_p / tau_p^2)) / sigma_0) + 0.5 * lambda\n        \"\"\"\n        # Ensure tau_p is positive\n        tau_p = np.maximum(tau_p, 1e-9)\n        \n        # Bayesian part: probability of making a 'right' choice based on evidence\n        p_bayes_right = norm.cdf((s + (sigma_0**2 * mu_p) / tau_p**2) / sigma_0)\n        \n        # Mixture with lapse rate\n        p_right = (1.0 - lam) * p_bayes_right + 0.5 * lam\n        return p_right\n\n    def psychometric_function_noise(s, sigma, lam):\n        \"\"\"\n        Psychometric function for the 'incorrect-noise' model.\n        p_right(s) = (1-lambda) * Phi(s / sigma) + 0.5 * lambda\n        \"\"\"\n        # Ensure sigma is positive\n        sigma = np.maximum(sigma, 1e-9)\n        \n        # Bayesian part (with flat prior)\n        p_bayes_right = norm.cdf(s / sigma)\n        \n        # Mixture with lapse rate\n        p_right = (1.0 - lam) * p_bayes_right + 0.5 * lam\n        return p_right\n\n    # --- Negative Log-Likelihood (NLL) Definitions ---\n\n    def nll_prior(params, s, k, n, sigma_0):\n        \"\"\"Negative log-likelihood for the incorrect-prior model.\"\"\"\n        mu_p, tau_p, lam = params\n        p_right = psychometric_function_prior(s, mu_p, tau_p, lam, sigma_0)\n        \n        log_likelihood = k * _safe_log(p_right) + (n - k) * _safe_log(1.0 - p_right)\n        return -np.sum(log_likelihood)\n\n    def nll_noise(params, s, k, n):\n        \"\"\"Negative log-likelihood for the incorrect-noise model.\"\"\"\n        sigma, lam = params\n        p_right = psychometric_function_noise(s, sigma, lam)\n        \n        log_likelihood = k * _safe_log(p_right) + (n - k) * _safe_log(1.0 - p_right)\n        return -np.sum(log_likelihood)\n\n    # --- Fitter and BIC Calculation ---\n\n    def fit_and_compare(s, k, n, sigma_0, ground_truth_label):\n        \"\"\"\n        Fits both models, computes BIC, and compares to ground truth.\n        \"\"\"\n        total_trials = np.sum(n)\n        \n        # 1. Fit Incorrect-Prior Model\n        params_prior_initial = [0.0, 1.0, 0.05]\n        bounds_prior = [(-np.inf, np.inf), (1e-6, np.inf), (0, 0.999)]\n        \n        res_prior = minimize(\n            nll_prior,\n            params_prior_initial,\n            args=(s, k, n, sigma_0),\n            method='L-BFGS-B',\n            bounds=bounds_prior\n        )\n        \n        nll_min_prior = res_prior.fun\n        num_params_prior = 3\n        bic_prior = num_params_prior * np.log(total_trials) + 2 * nll_min_prior\n\n        # 2. Fit Incorrect-Noise Model\n        params_noise_initial = [1.0, 0.05]\n        bounds_noise = [(1e-6, np.inf), (0, 0.999)]\n\n        res_noise = minimize(\n            nll_noise,\n            params_noise_initial,\n            args=(s, k, n),\n            method='L-BFGS-B',\n            bounds=bounds_noise\n        )\n        \n        nll_min_noise = res_noise.fun\n        num_params_noise = 2\n        bic_noise = num_params_noise * np.log(total_trials) + 2 * nll_min_noise\n\n        # 3. Compare models\n        if bic_prior < bic_noise:\n            selected_model = \"prior\"\n        else:\n            selected_model = \"noise\"\n            \n        return selected_model == ground_truth_label\n\n    # --- Test Cases ---\n\n    test_cases = [\n        # Test Case A (happy path, incorrect priors)\n        {\n            \"sigma_0\": 1.0,\n            \"gen_params\": {\"mu_p\": 0.8, \"tau_p\": 0.7, \"sigma\": 1.0, \"lambda\": 0.05},\n            \"gen_model\": \"prior\",\n            \"ground_truth\": \"prior\"\n        },\n        # Test Case B (happy path, increased noise)\n        {\n            \"sigma_0\": 1.0,\n            \"gen_params\": {\"mu_p\": 0.0, \"tau_p\": np.inf, \"sigma\": 2.0, \"lambda\": 0.05},\n            \"gen_model\": \"noise\",\n            \"ground_truth\": \"noise\"\n        },\n        # Test Case C (boundary condition, high lapse with incorrect priors)\n        {\n            \"sigma_0\": 1.5,\n            \"gen_params\": {\"mu_p\": 0.6, \"tau_p\": 0.5, \"sigma\": 1.5, \"lambda\": 0.3},\n            \"gen_model\": \"prior\",\n            \"ground_truth\": \"prior\"\n        }\n    ]\n    \n    stim_levels = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    num_trials_per_level = np.full_like(stim_levels, 1000)\n\n    results = []\n\n    for case in test_cases:\n        # Generate synthetic data deterministically\n        gp = case[\"gen_params\"]\n        if case[\"gen_model\"] == \"prior\":\n            p_gen = psychometric_function_prior(stim_levels, gp[\"mu_p\"], gp[\"tau_p\"], gp[\"lambda\"], gp[\"sigma\"])\n        else: # \"noise\" model\n            p_gen = psychometric_function_noise(stim_levels, gp[\"sigma\"], gp[\"lambda\"])\n        \n        k_counts = np.round(num_trials_per_level * p_gen).astype(int)\n        \n        # Run the fitting and comparison\n        is_correct = fit_and_compare(\n            stim_levels,\n            k_counts,\n            num_trials_per_level,\n            case[\"sigma_0\"],\n            case[\"ground_truth\"]\n        )\n        results.append(is_correct)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}