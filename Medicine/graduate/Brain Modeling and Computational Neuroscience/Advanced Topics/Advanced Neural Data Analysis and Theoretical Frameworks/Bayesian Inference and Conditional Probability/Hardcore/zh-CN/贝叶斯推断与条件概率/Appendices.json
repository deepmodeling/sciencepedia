{
    "hands_on_practices": [
        {
            "introduction": "贝叶斯推断的核心在于如何结合先验知识与新观测到的数据。本练习将从一个基础但至关重要的例子开始：高斯似然与高斯先验的结合。通过从第一性原理推导后验分布，您将亲身体验如何精确地量化信念的更新过程，并理解后验均值如何表示为由各自精度加权的先验均值和观测数据的平均值 。",
            "id": "3964645",
            "problem": "在一个神经编码实验中，单次试验的感觉反应 $y$ 被建模为一个未知的潜刺激表征 $\\theta$ 的带噪观测。假设在测量阶段，由于突触和读出变异性，存在加性高斯噪声，并且潜表征上存在一个高斯先验，该先验反映了先前学习到的环境统计特性。具体来说，假设似然和先验由 $p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, \\sigma^{2})$ 和 $p(\\theta) = \\mathcal{N}(\\theta; \\mu_{0}, \\tau_{0}^{2})$ 指定，其中 $\\sigma^{2} > 0$ 是观测噪声方差，$\\mu_{0} \\in \\mathbb{R}$ 是先验均值，$\\tau_{0}^{2} > 0$ 是先验方差。利用贝叶斯定理的定义和高斯分布族的性质，从第一性原理出发，推导后验分布 $p(\\theta \\mid y)$，计算其均值和方差的闭式解，并将后验均值表示为精度加权形式，其中精度定义为方差的倒数。你的最终答案应该是后验均值和后验方差的两个闭式表达式。不需要数值近似。",
            "solution": "该问题是有效的。它描述了计算神经科学中一个标准的贝叶斯推断场景，涉及高斯似然和高斯先验。所有组成部分在科学上都是合理的、定义明确的，并且该问题在数学上是适定的，要求进行有唯一解的标准推导。\n\n任务是推导给定观测值 $y$ 的情况下，潜变量 $\\theta$ 的后验分布 $p(\\theta \\mid y)$。似然和先验分布如下：\n似然：$p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)$\n先验：$p(\\theta) = \\mathcal{N}(\\theta; \\mu_0, \\tau_0^2) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)$\n\n根据贝叶斯定理，后验分布与似然和先验的乘积成正比：\n$$ p(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} \\propto p(y \\mid \\theta) p(\\theta) $$\n$p(y)$ 项是一个不依赖于 $\\theta$ 的归一化常数。我们可以通过分析似然和先验的乘积来找到后验的函数形式。\n\n代入高斯分布的表达式：\n$$ p(\\theta \\mid y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) \\right] $$\n忽略常数乘法因子，我们关注指数项：\n$$ p(\\theta \\mid y) \\propto \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2} - \\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) $$\n我们来分析指数部分，记为 $E$：\n$$ E = -\\frac{1}{2} \\left( \\frac{(y - \\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right) $$\n我们展开括号内的平方项：\n$$ E = -\\frac{1}{2} \\left( \\frac{y^2 - 2y\\theta + \\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau_0^2} \\right) $$\n为了确定一个新的关于 $\\theta$ 的高斯分布，我们按 $\\theta$ 的幂次收集项：\n$$ E = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) - 2\\theta \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right) \\right] $$\n$\\theta$ 的后验分布形式为 $p(\\theta \\mid y) = \\mathcal{N}(\\theta; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$，其指数形式为 $-\\frac{(\\theta - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2}$。展开此式得到：\n$$ -\\frac{(\\theta - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} = -\\frac{1}{2\\sigma_{\\text{post}}^2} (\\theta^2 - 2\\theta\\mu_{\\text{post}} + \\mu_{\\text{post}}^2) = -\\frac{1}{2} \\left( \\frac{1}{\\sigma_{\\text{post}}^2}\\theta^2 - \\frac{2\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2}\\theta + \\frac{\\mu_{\\text{post}}^2}{\\sigma_{\\text{post}}^2} \\right) $$\n通过将我们推导出的指数 $E$ 中 $\\theta$ 的各次幂的系数与这个一般形式进行比较，我们可以确定后验均值 $\\mu_{\\text{post}}$ 和方差 $\\sigma_{\\text{post}}^2$。\n\n首先，比较 $\\theta^2$ 项的系数：\n$$ \\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} $$\n这立即给出了我们后验方差 $\\sigma_{\\text{post}}^2$：\n$$ \\sigma_{\\text{post}}^2 = \\left( \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\right)^{-1} = \\left( \\frac{\\tau_0^2 + \\sigma^2}{\\sigma^2 \\tau_0^2} \\right)^{-1} = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} $$\n接下来，我们比较 $\\theta$ 项的系数：\n$$ \\frac{2\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2} = 2 \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\n现在，代入我们找到的 $\\sigma_{\\text{post}}^2$ 的表达式：\n$$ \\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} \\right) \\left( \\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2} \\right) \\left( \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 \\tau_0^2} \\right) $$\n$$ \\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} $$\n因此，后验分布是一个高斯分布，$p(\\theta \\mid y) = \\mathcal{N}(\\theta; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$，其均值和方差已推导得出。\n\n问题还要求以后验均值的精度加权形式表示。精度定义为方差的倒数。令 $\\pi_y = 1/\\sigma^2$ 为似然的精度，$\\pi_0 = 1/\\tau_0^2$ 为先验的精度。令 $\\pi_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ 为后验的精度。\n根据我们对后验方差的推导：\n$$ \\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} \\implies \\pi_{\\text{post}} = \\pi_y + \\pi_0 $$\n这表明后验精度是似然精度和先验精度的和。\n现在来看后验均值。我们可以通过将 $\\mu_{\\text{post}}$ 表达式的分子和分母同除以 $\\sigma^2 \\tau_0^2$ 来重写该表达式：\n$$ \\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2} = \\frac{y \\frac{1}{\\sigma^2} + \\mu_0 \\frac{1}{\\tau_0^2}}{\\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma^2}} $$\n用精度来表示，这变为：\n$$ \\mu_{\\text{post}} = \\frac{y \\pi_y + \\mu_0 \\pi_0}{\\pi_y + \\pi_0} = \\frac{\\pi_y}{\\pi_y + \\pi_0} y + \\frac{\\pi_0}{\\pi_y + \\pi_0} \\mu_0 $$\n这种形式完美地说明了后验均值是观测值（测量值）$y$ 和先验均值 $\\mu_0$ 的加权平均，其权重由它们各自的精度决定。更精确（不确定性更小）的来源对最终估计的贡献更大。\n\n最终要求的答案是后验均值和后验方差的闭式表达式。\n后验均值： $\\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}$\n后验方差： $\\sigma_{\\text{post}}^2 = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2}$",
            "answer": "$$\n\\boxed{\n\\mu_{\\text{post}} = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}, \\quad \\sigma_{\\text{post}}^2 = \\frac{\\sigma^2 \\tau_0^2}{\\sigma^2 + \\tau_0^2}\n}\n$$"
        },
        {
            "introduction": "在更现实的神经科学模型（如广义线性模型 GLM）中，我们常常无法精确地计算模型证据（即边缘似然）。本练习旨在介绍一种强大的分析近似方法——拉普拉斯近似。通过在后验概率的峰值处进行二阶泰勒展开，您将学会如何估算模型证据，并深入理解“奥卡姆因子”是如何在贝叶斯框架下自动惩罚过于复杂的模型，从而实现模型选择 。",
            "id": "3964638",
            "problem": "在 $n$ 次刺激呈现中记录单个皮层神经元的活动，产生脉冲计数 $\\{y_i\\}_{i=1}^{n}$。该过程由一个泊松广义线性模型 (GLM) 建模，其中条件强度为 $\\lambda_i(\\theta) = \\exp(\\theta^{\\top} x_i)$，其中 $x_i \\in \\mathbb{R}^{d}$ 是特征向量，$\\theta \\in \\mathbb{R}^{d}$ 是参数向量。$\\theta$ 的先验是高斯分布，$\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$。令 $\\hat{\\theta}$ 表示最大后验 (MAP) 估计，令 $H(\\hat{\\theta})$ 表示在 $\\hat{\\theta}$ 处求值的负对数后验概率的海森矩阵，即 $H(\\hat{\\theta}) = -\\nabla_{\\theta}^{2}\\left[\\ln p(y\\mid \\theta) + \\ln p(\\theta)\\right]\\big|_{\\theta=\\hat{\\theta}}$，并假设其为正定矩阵。\n\n从贝叶斯法则和证据（边缘似然）$p(y)$ 的定义出发，通过对对数后验在 $\\hat{\\theta}$ 附近进行二阶泰勒展开，并精确计算得到的高斯积分，来推导 $\\ln p(y)$ 的主阶近似。在您的推导中，请明确指出由后验曲率产生的乘法项，即“Occam 因子”，并用 $d$ 和 $H(\\hat{\\theta})$ 表示它。请简要提供一个概念性解释，说明在该神经 GLM 的背景下，这个项如何惩罚过于灵活的模型。\n\n对于一个 $d=3$ 的特定实验，假设在 MAP 处的海森矩阵是对角的，\n$$\nH(\\hat{\\theta}) = \\begin{pmatrix}\n120  0  0\\\\\n0  50  0\\\\\n0  0  10\n\\end{pmatrix}.\n$$\n计算 Occam 因子，结果为一个实数值。将您的答案四舍五入到四位有效数字。",
            "solution": "用户想要解决一个计算神经科学中的问题，该问题与泊松广义线性模型 (GLM) 的贝叶斯推断有关。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **模型：** 单个皮层神经元的脉冲计数 $\\{y_i\\}_{i=1}^{n}$ 由一个泊松广义线性模型 (GLM) 建模。\n*   **条件强度：** 第 $i$ 个刺激的率参数为 $\\lambda_i(\\theta) = \\exp(\\theta^{\\top} x_i)$。\n*   **刺激特征：** 特征向量为 $x_i \\in \\mathbb{R}^{d}$。\n*   **模型参数：** 参数向量为 $\\theta \\in \\mathbb{R}^{d}$。\n*   **先验分布：** 参数的先验是高斯分布，$\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$。\n*   **MAP 估计：** $\\hat{\\theta}$ 表示 $\\theta$ 的最大后验估计。\n*   **海森矩阵：** $H(\\hat{\\theta})$ 是在 $\\hat{\\theta}$ 处求值的负对数后验的海森矩阵。其定义为 $H(\\hat{\\theta}) = -\\nabla_{\\theta}^{2}\\left[\\ln p(y\\mid \\theta) + \\ln p(\\theta)\\right]\\big|_{\\theta=\\hat{\\theta}}$，并假设为正定。\n*   **任务 1：** 推导对数证据 $\\ln p(y)$ 的主阶近似。\n*   **任务 1 的方法：** 对对数后验在 $\\hat{\\theta}$ 附近进行二阶泰勒展开，并计算得到的高斯积分。\n*   **任务 2：** 将由后验曲率产生的乘法项识别为“Occam 因子”，并用 $d$ 和 $H(\\hat{\\theta})$ 表示。\n*   **任务 3：** 在神经 GLM 的背景下，提供此 Occam 因子的概念性解释。\n*   **任务 4：** 对于一个参数维度 $d=3$ 且海森矩阵为 $H(\\hat{\\theta}) = \\begin{pmatrix} 120  0  0\\\\ 0  50  0\\\\ 0  0  10 \\end{pmatrix}$ 的特定情况，计算 Occam 因子的数值，四舍五入到四位有效数字。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n*   **科学依据：** 该问题牢固地基于标准的贝叶斯统计方法及其在计算神经科学中的应用。泊松 GLM 是脉冲计数数据的经典模型，使用高斯先验、MAP 估计以及用于模型证据的拉普拉斯近似都是该领域中公认的基础技术。\n*   **良态问题：** 这是一个良态问题。它要求一个标准的推导（拉普拉斯近似），其形式是唯一的。它为推导和后续的数值计算提供了所有必要的信息。海森矩阵是正定的假设对于在 MAP 估计附近进行高斯近似的有效性至关重要。\n*   **客观性：** 问题陈述精确、量化，并且没有主观或模糊的语言。\n\n该问题没有违反任何指定的无效标准。这是一个应用于大脑建模的贝叶斯统计学中有效的、可形式化的问题。\n\n**步骤 3：结论与行动**\n\n该问题是**有效的**。开始进行求解。\n\n### 推导与求解\n\n主要目标是近似模型证据（或边缘似然）$p(y)$，它被定义为似然函数与先验在所有可能参数值上的积分：\n$$\np(y) = \\int p(y|\\theta) p(\\theta) \\, d\\theta\n$$\n被积函数 $p(y|\\theta) p(\\theta)$ 与后验分布 $p(\\theta|y)$ 成正比。让我们将未归一化的后验的对数定义为 $L(\\theta) = \\ln[p(y|\\theta) p(\\theta)] = \\ln p(y|\\theta) + \\ln p(\\theta)$。证据可以写成：\n$$\np(y) = \\int \\exp(L(\\theta)) \\, d\\theta\n$$\n拉普拉斯近似方法包括对 $L(\\theta)$ 在其最大值点，即最大后验 (MAP) 估计 $\\hat{\\theta}$ 处，进行二阶泰勒级数展开。展开式为：\n$$\nL(\\theta) \\approx L(\\hat{\\theta}) + (\\theta - \\hat{\\theta})^{\\top} \\nabla_{\\theta} L(\\theta)\\big|_{\\theta=\\hat{\\theta}} + \\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} \\nabla_{\\theta}^{2} L(\\theta)\\big|_{\\theta=\\hat{\\theta}} (\\theta - \\hat{\\theta})\n$$\n根据 MAP 估计 $\\hat{\\theta}$ 的定义，它是后验分布的一个众数，因此梯度项为零：$\\nabla_{\\theta} L(\\theta)\\big|_{\\theta=\\hat{\\theta}} = 0$。问题将*负*对数后验的海森矩阵定义为 $H(\\hat{\\theta}) = -\\nabla_{\\theta}^{2} L(\\theta)\\big|_{\\theta=\\hat{\\theta}}$。将这些代入展开式，得到：\n$$\nL(\\theta) \\approx L(\\hat{\\theta}) - \\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\n$$\n现在，我们将此近似代回到证据的积分中：\n$$\np(y) \\approx \\int \\exp\\left(L(\\hat{\\theta}) - \\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\\right) \\, d\\theta\n$$\n项 $\\exp(L(\\hat{\\theta}))$ 相对于 $\\theta$ 是常数，可以从积分中提出：\n$$\np(y) \\approx \\exp(L(\\hat{\\theta})) \\int \\exp\\left(-\\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\\right) \\, d\\theta\n$$\n剩下的积分是一个 $d$ 维多元高斯分布的归一化常数，其均值为 $\\hat{\\theta}$，精度矩阵（协方差的逆）为 $H(\\hat{\\theta})$。该积分的值由下式给出：\n$$\n\\int \\exp\\left(-\\frac{1}{2} (\\theta - \\hat{\\theta})^{\\top} H(\\hat{\\theta}) (\\theta - \\hat{\\theta})\\right) \\, d\\theta = (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\n因此，证据的近似值为：\n$$\np(y) \\approx \\exp(L(\\hat{\\theta})) (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\n代回 $L(\\hat{\\theta}) = \\ln p(y|\\hat{\\theta}) + \\ln p(\\hat{\\theta})$，我们得到：\n$$\np(y) \\approx p(y|\\hat{\\theta}) p(\\hat{\\theta}) (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\n取自然对数，得到对数证据的期望近似值：\n$$\n\\ln p(y) \\approx \\ln p(y|\\hat{\\theta}) + \\ln p(\\hat{\\theta}) + \\frac{d}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(H(\\hat{\\theta})))\n$$\n这个表达式表明，对数证据近似等于在其峰值处求值的对数后验，再加上一个与后验体积相关的修正项。\n\n**Occam 因子与解释**\n\n证据近似是几项的乘积：$p(y) \\approx \\underbrace{p(y|\\hat{\\theta})}_\\text{最佳拟合似然} \\times \\underbrace{p(\\hat{\\theta})}_\\text{MAP处的先验} \\times \\underbrace{(2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}}_\\text{曲率项}$。\n\n问题要求将从后验曲率中产生的乘法项识别为“Occam 因子”。根据我们的推导，这个项是：\n$$\n\\text{Occam 因子} = (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2}\n$$\n这个项作为对模型复杂度的惩罚，是 Occam 剃刀原则的体现。在神经 GLM 的背景下，其概念性解释如下：总证据在模型的拟合优度（高似然 $p(y|\\hat{\\theta})$）和其复杂性之间取得平衡。一个过于灵活的模型（例如，具有许多参数或弱先验约束的模型）可以扭曲自身以非常好地拟合观测到的脉冲数据，从而产生很高的似然值。然而，这种极端拟合通常会导致后验分布在 MAP 估计 $\\hat{\\theta}$ 周围异常尖锐和狭窄，因为模型的灵活性被“用尽”来解释特定的数据实例。一个非常尖锐的后验对应于高曲率，意味着海森矩阵 $H(\\hat{\\theta})$ 有一个大的行列式 $\\det(H(\\hat{\\theta}))$。由于 Occam 因子与 $\\det(H(\\hat{\\theta}))^{-1/2}$ 成正比，大的行列式会导致一个非常小的 Occam 因子。这个小的乘法因子会严重惩罚高似然分数，从而减少了对过于灵活的模型的总体证据。本质上，Occam 因子偏好那些能为数据提供良好、鲁棒解释（合理的似然和合理集中的后验）而不过度调整的模型，从而防止过拟合。\n\n**数值计算**\n\n给定维度 $d=3$ 和海森矩阵：\n$$\nH(\\hat{\\theta}) = \\begin{pmatrix}\n120  0  0\\\\\n0  50  0\\\\\n0  0  10\n\\end{pmatrix}\n$$\n由于该矩阵是对角矩阵，其行列式是对角线元素的乘积：\n$$\n\\det(H(\\hat{\\theta})) = 120 \\times 50 \\times 10 = 60000\n$$\n现在我们可以计算 Occam 因子：\n$$\n\\text{Occam 因子} = (2\\pi)^{d/2} \\det(H(\\hat{\\theta}))^{-1/2} = (2\\pi)^{3/2} (60000)^{-1/2}\n$$\n让我们来计算这个表达式：\n$$\n (2\\pi)^{3/2} = (2\\pi)\\sqrt{2\\pi} \\approx 15.7496099\n$$\n$$\n(60000)^{-1/2} = \\frac{1}{\\sqrt{60000}} = \\frac{1}{100\\sqrt{6}} \\approx \\frac{1}{244.948974} \\approx 0.0040824829\n$$\n将这两个值相乘：\n$$\n\\text{Occam 因子} \\approx 15.7496099 \\times 0.0040824829 \\approx 0.06429708\n$$\n四舍五入到四位有效数字，我们得到 $0.06430$。",
            "answer": "$$\n\\boxed{0.06430}\n$$"
        },
        {
            "introduction": "随着模型复杂性和数据规模的增长，传统的贝叶斯计算方法可能变得非常缓慢。本练习将带您进入现代计算神经科学的前沿，探索如何使用摊销变分推断（Amortized Variational Inference）来高效地处理复杂的潜变量模型。您将通过神经网络来参数化近似后验，并实现一个基于“重参数化技巧”的证据下界（ELBO）随机估计器，这是现代概率机器学习的核心技术之一 。",
            "id": "3964683",
            "problem": "考虑一个计算神经科学中用于多神经元尖峰计数的潜变量模型。设潜在大脑皮层状态为一个$K$维随机变量$z \\in \\mathbb{R}^K$，其先验分布$p(z)$由标准多元高斯分布给出，$p(z) = \\mathcal{N}(z; 0, I_K)$，其中$I_K$是$K \\times K$的单位矩阵。给定$z$，观测到的$D$个神经元的尖峰计数$y \\in \\mathbb{N}_0^D$被建模为条件独立的泊松随机变量，其速率$\\lambda(z) \\in \\mathbb{R}_+^D$由一个仿射映射$z$上应用的指数非线性函数定义，即$\\lambda(z) = \\exp(A z + c)$，其中$A \\in \\mathbb{R}^{D \\times K}$，$c \\in \\mathbb{R}^D$。条件似然为$p(y \\mid z) = \\prod_{d=1}^D \\mathrm{Poisson}(y_d; \\lambda_d(z))$。\n\n您将通过将近似后验$q_\\phi(z \\mid y)$参数化为一个对角多元高斯分布来实现分摊变分推断，其均值和对数方差由一个将$y$映射到这些参数的神经网络给出。具体来说，定义一个单层神经网络（一个仿射映射），其参数为$\\phi = \\{W_\\mu, b_\\mu, W_{\\log\\sigma^2}, b_{\\log\\sigma^2}\\}$，其中$W_\\mu \\in \\mathbb{R}^{K \\times D}$，$b_\\mu \\in \\mathbb{R}^K$，$W_{\\log\\sigma^2} \\in \\mathbb{R}^{K \\times D}$，以及$b_{\\log\\sigma^2} \\in \\mathbb{R}^K$。该网络的输出为\n$$\n\\mu_\\phi(y) = W_\\mu y + b_\\mu, \\quad \\log \\sigma_\\phi^2(y) = W_{\\log\\sigma^2} y + b_{\\log\\sigma^2},\n$$\n近似后验为\n$$\nq_\\phi(z \\mid y) = \\mathcal{N}\\!\\Big(z; \\mu_\\phi(y), \\operatorname{diag}(\\sigma_\\phi^2(y))\\Big),\n$$\n其中$\\sigma_\\phi^2(y) = \\exp(\\log \\sigma_\\phi^2(y))$是逐元素应用的。\n\n从条件概率、贝叶斯法则、Kullback-Leibler散度和琴生不等式的定义出发，使用重参数化$z = \\mu_\\phi(y) + \\sigma_\\phi(y) \\odot \\epsilon$（其中$\\epsilon \\sim \\mathcal{N}(0, I_K)$，$\\odot$表示逐元素乘法），为给定的观测值$y$推导证据下界（ELBO）的随机估计量。然后，实现一个程序，通过对$L$个独立的$\\epsilon$样本进行平均，为指定的参数集计算此ELBO的蒙特卡洛估计。\n\n您的程序必须正确且高效地实现以下元素：\n- 先验$p(z) = \\mathcal{N}(0, I_K)$的对数$\\log p(z)$。\n- 泊松观测的对数似然$\\log p(y \\mid z)$，其速率为$\\lambda(z) = \\exp(A z + c)$，使用涉及伽马函数计算阶乘项对数的精确表达式，以确保数值稳定性。\n- 对角高斯分布的对数密度$\\log q_\\phi(z \\mid y)$。\n- 使用固定的随机种子，通过重参数化技巧生成的$L$个样本的平均值，作为单个观测值$y$的ELBO的随机蒙特卡洛估计量。\n\n使用以下固定维度：$K=2$个潜维度和$D=3$个观测神经元。随机数生成器种子必须设置为整数$42$，以确保可复现性。不涉及物理单位。所有角度（如果出现）都将以弧度为单位；本问题中没有角度。所有量都是无量纲的。\n\n测试套件：\n为以下$4$个测试用例提供蒙特卡洛ELBO估计。在所有情况下，$A \\in \\mathbb{R}^{3 \\times 2}$和$c \\in \\mathbb{R}^3$为：\n$$\nA = \\begin{bmatrix}\n0.3  -0.2 \\\\\n0.1  0.25 \\\\\n-0.05  0.4\n\\end{bmatrix}, \\quad\nc = \\begin{bmatrix}\n1.0 \\\\\n0.5 \\\\\n0.2\n\\end{bmatrix}.\n$$\n每个测试用例指定$(y, W_\\mu, b_\\mu, W_{\\log\\sigma^2}, b_{\\log\\sigma^2}, L)$。\n\n- 情况$1$（正常情况）：\n$$\ny = \\begin{bmatrix} 5 \\\\ 0 \\\\ 3 \\end{bmatrix},\\quad\nW_\\mu = \\begin{bmatrix}\n0.02  -0.01  0.03 \\\\\n-0.04  0.05  0.02\n\\end{bmatrix},\\quad\nb_\\mu = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{\\log\\sigma^2} = \\begin{bmatrix}\n-0.02  0.01  -0.01 \\\\\n0.03  -0.02  0.01\n\\end{bmatrix},\\quad\nb_{\\log\\sigma^2} = \\begin{bmatrix} -0.5 \\\\ -0.5 \\end{bmatrix},\\quad\nL = 100.\n$$\n\n- 情况$2$（边界计数：全为零）：\n$$\ny = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\nW_\\mu = \\begin{bmatrix}\n0.02  -0.01  0.03 \\\\\n-0.04  0.05  0.02\n\\end{bmatrix},\\quad\nb_\\mu = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{\\log\\sigma^2} = \\begin{bmatrix}\n-0.02  0.01  -0.01 \\\\\n0.03  -0.02  0.01\n\\end{bmatrix},\\quad\nb_{\\log\\sigma^2} = \\begin{bmatrix} -0.5 \\\\ -0.5 \\end{bmatrix},\\quad\nL = 100.\n$$\n\n- 情况$3$（较高计数）：\n$$\ny = \\begin{bmatrix} 20 \\\\ 15 \\\\ 25 \\end{bmatrix},\\quad\nW_\\mu = \\begin{bmatrix}\n0.01  0.00  0.02 \\\\\n-0.03  0.02  0.01\n\\end{bmatrix},\\quad\nb_\\mu = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix},\n$$\n$$\nW_{\\log\\sigma^2} = \\begin{bmatrix}\n0.00  0.00  0.00 \\\\\n0.00  0.00  0.00\n\\end{bmatrix},\\quad\nb_{\\log\\sigma^2} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\nL = 500.\n$$\n\n- 情况$4$（$q_\\phi$中近简并方差）：\n$$\ny = \\begin{bmatrix} 4 \\\\ 4 \\\\ 4 \\end{bmatrix},\\quad\nW_\\mu = \\begin{bmatrix}\n0.02  0.02  0.02 \\\\\n-0.02  -0.02  -0.02\n\\end{bmatrix},\\quad\nb_\\mu = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{\\log\\sigma^2} = \\begin{bmatrix}\n0.00  0.00  0.00 \\\\\n0.00  0.00  0.00\n\\end{bmatrix},\\quad\nb_{\\log\\sigma^2} = \\begin{bmatrix} -4.5 \\\\ -4.5 \\end{bmatrix},\\quad\nL = 100.\n$$\n\n您的程序应生成单行输出，其中包含$4$个测试用例的$4$个蒙特卡洛ELBO估计值，形式为用方括号括起来的逗号分隔列表，即$\\big[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4 \\big]$。每个$\\text{result}_i$必须是按规定对$L$个样本进行平均计算得到的浮点数。不应产生其他输出。对所有蒙特卡洛采样使用固定的随机种子$42$。",
            "solution": "我们从贝叶斯推断和条件概率的基础开始。后验分布$p(z \\mid y)$由贝叶斯法则定义为\n$$\np(z \\mid y) = \\frac{p(y \\mid z) p(z)}{p(y)},\n$$\n其中证据$p(y)$由下式给出\n$$\np(y) = \\int p(y \\mid z) p(z) \\, dz.\n$$\n在潜变量模型中，直接计算$p(y)$通常是不可行的，这促使我们使用变分推断。在变分推断中，我们假设一个分布族$q_\\phi(z \\mid y)$，并寻找参数$\\phi$以最小化$q_\\phi(z \\mid y)$与真实后验$p(z \\mid y)$之间的Kullback-Leibler散度（KLD），\n$$\n\\operatorname{KL}\\!\\big(q_\\phi(z \\mid y) \\,\\|\\, p(z \\mid y)\\big) = \\mathbb{E}_{q_\\phi}\\!\\left[ \\log \\frac{q_\\phi(z \\mid y)}{p(z \\mid y)} \\right].\n$$\n在KLD内部使用贝叶斯法则并重新整理，我们得到一个可处理的目标，称为证据下界（ELBO）。具体来说，通过对对数证据应用琴生不等式和代数操作，可以推导出对于任何$q_\\phi(z \\mid y)$，\n$$\n\\log p(y) \\ge \\mathbb{E}_{q_\\phi(z \\mid y)}\\!\\left[ \\log p(y \\mid z) + \\log p(z) - \\log q_\\phi(z \\mid y) \\right] \\equiv \\mathcal{L}(\\phi; y),\n$$\n并且关于$\\phi$最大化$\\mathcal{L}(\\phi; y)$会最小化$\\operatorname{KL}\\!\\big(q_\\phi(z \\mid y) \\,\\|\\, p(z \\mid y)\\big)$。\n\n现在我们指定模型组件。先验是$p(z) = \\mathcal{N}(z; 0, I_K)$，其对数密度为\n$$\n\\log p(z) = -\\frac{1}{2} \\sum_{k=1}^K \\left( z_k^2 + \\log(2\\pi) \\right).\n$$\n似然是泊松分布，其速率为$\\lambda(z) = \\exp(A z + c)$，其中指数函数是逐元素应用的。对于$y \\in \\mathbb{N}_0^D$，对数似然是\n$$\n\\log p(y \\mid z) = \\sum_{d=1}^D \\left( y_d \\log \\lambda_d(z) - \\lambda_d(z) - \\log(y_d!) \\right).\n$$\n为了数值稳定性，项$\\log(y_d!)$使用对数伽马函数计算，即$\\log(y_d!) = \\log \\Gamma(y_d + 1)$。\n\n变分族是一个对角多元高斯分布$q_\\phi(z \\mid y) = \\mathcal{N}\\!\\big(z; \\mu_\\phi(y), \\operatorname{diag}(\\sigma_\\phi^2(y))\\big)$，由一个神经网络参数化。对于$\\mu_\\phi(y) = W_\\mu y + b_\\mu$和$\\log \\sigma_\\phi^2(y) = W_{\\log\\sigma^2} y + b_{\\log\\sigma^2}$，我们有$\\sigma_\\phi^2(y) = \\exp(\\log \\sigma_\\phi^2(y))$（逐元素）。$q_\\phi$的对数密度是\n$$\n\\log q_\\phi(z \\mid y) = -\\frac{1}{2} \\sum_{k=1}^K \\left( \\log(2\\pi) + \\log \\sigma_{\\phi,k}^2(y) + \\frac{\\big(z_k - \\mu_{\\phi,k}(y)\\big)^2}{\\sigma_{\\phi,k}^2(y)} \\right).\n$$\n\n为了构建ELBO的随机估计量，我们使用重参数化技巧。设$\\epsilon \\sim \\mathcal{N}(0, I_K)$，并写出\n$$\nz = \\mu_\\phi(y) + \\sigma_\\phi(y) \\odot \\epsilon,\n$$\n其中$\\sigma_\\phi(y)$是$\\sigma_\\phi^2(y)$的逐元素平方根，$\\odot$表示逐元素乘法。这种表示允许通过从固定分布中采样$\\epsilon$并通过网络输出进行可微变换来从$q_\\phi(z \\mid y)$中采样。然后，具有$L$个样本的ELBO的蒙特卡洛估计量为\n$$\n\\widehat{\\mathcal{L}}(\\phi; y) = \\frac{1}{L} \\sum_{l=1}^L \\left[ \\log p\\big(y \\mid z^{(l)}\\big) + \\log p\\big(z^{(l)}\\big) - \\log q_\\phi\\big(z^{(l)} \\mid y\\big) \\right],\n$$\n其中$z^{(l)} = \\mu_\\phi(y) + \\sigma_\\phi(y) \\odot \\epsilon^{(l)}$，且对于$l = 1, \\dots, L$，$\\epsilon^{(l)} \\sim \\mathcal{N}(0, I_K)$是独立同分布的。\n\n算法设计：\n- 固定维度$K = 2$和$D = 3$。\n- 使用上述公式实现$\\log p(z)$、$\\log p(y \\mid z)$和$\\log q_\\phi(z \\mid y)$的函数。为确保稳健性，对每个$d$通过$\\log \\Gamma(y_d + 1)$计算$\\log(y_d!)$，并小心计算$\\lambda(z) = \\exp(A z + c)$以保持在给定参数的浮点范围内。\n- 使用提供的仿射参数实现神经网络映射$y \\mapsto \\mu_\\phi(y)$和$y \\mapsto \\log \\sigma_\\phi^2(y)$。逐元素计算$\\sigma_\\phi(y) = \\exp\\big(\\tfrac{1}{2} \\log \\sigma_\\phi^2(y)\\big)$。\n- 使用具有固定种子的专用随机数生成器以保证可复现性。对于每个测试用例，抽取$L$个独立样本$\\epsilon^{(l)} \\sim \\mathcal{N}(0, I_K)$，通过重参数化形成$z^{(l)}$，评估三个对数项，并求平均以获得$\\widehat{\\mathcal{L}}(\\phi; y)$。\n- 汇总四个测试用例的结果，并以要求的单行格式打印它们。\n\n覆盖范围考虑：\n- 具有中等计数的正常情况用例验证了通用计算。\n- 所有计数均为零的边界情况用例测试了在$y_d \\log \\lambda_d(z)$项消失而$\\log(y_d!)$项不平凡时的似然函数。\n- 较高计数的用例测试了在较大观测计数下的数值稳定性。\n- 近简并方差情况（非常小的$\\sigma_\\phi^2$）探讨了当近似后验非常尖锐时，$q_\\phi$的对数密度和重参数化的行为。\n\n所有输出都是浮点数，表示每种情况的蒙特卡洛ELBO估计。不涉及百分比或角度，所有量都是无量纲的。最终程序精确地生成一行包含四个结果列表的输出，如指定格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef log_prior_standard_normal(z):\n    \"\"\"\n    Compute log p(z) for z ~ N(0, I).\n    z: array of shape (K,) or (L, K)\n    Returns: scalar or array of shape (L,)\n    \"\"\"\n    # For standard normal: log p(z) = -0.5 * sum( z^2 + log(2*pi) )\n    if z.ndim == 1:\n        return -0.5 * (np.sum(z**2) + z.size * np.log(2.0 * np.pi))\n    elif z.ndim == 2:\n        return -0.5 * (np.sum(z**2, axis=1) + z.shape[1] * np.log(2.0 * np.pi))\n    else:\n        raise ValueError(\"z must be 1D or 2D\")\n\ndef log_poisson_likelihood(y, z, A, c):\n    \"\"\"\n    Compute log p(y | z) where y_d ~ Poisson(lambda_d(z)), lambda(z) = exp(A z + c).\n    y: array of shape (D,)\n    z: array of shape (K,) or (L, K)\n    A: array of shape (D, K)\n    c: array of shape (D,)\n    Returns: scalar or array of shape (L,)\n    \"\"\"\n    if z.ndim == 1:\n        Azc = A @ z + c  # shape (D,)\n        lam = np.exp(Azc)  # positive\n        # y * log(lam) - lam - log(y!)\n        # Use gammaln for log-factorial: log(y!) = gammaln(y+1)\n        return np.sum(y * np.log(lam) - lam - gammaln(y + 1.0))\n    elif z.ndim == 2:\n        # Batch compute\n        Azc = z @ A.T + c  # shape (L, D)\n        lam = np.exp(Azc)\n        # Broadcast y over L, compute per sample\n        term1 = (y * np.log(lam)).sum(axis=1)\n        term2 = lam.sum(axis=1)\n        term3 = np.sum(gammaln(y + 1.0))\n        return term1 - term2 - term3\n    else:\n        raise ValueError(\"z must be 1D or 2D\")\n\ndef nn_params_mu_logvar(y, W_mu, b_mu, W_logvar, b_logvar):\n    \"\"\"\n    Compute mu(y) and logvar(y) from affine neural network.\n    y: array of shape (D,)\n    W_mu: array of shape (K, D)\n    b_mu: array of shape (K,)\n    W_logvar: array of shape (K, D)\n    b_logvar: array of shape (K,)\n    Returns: mu (K,), logvar (K,)\n    \"\"\"\n    mu = W_mu @ y + b_mu\n    logvar = W_logvar @ y + b_logvar\n    return mu, logvar\n\ndef log_q_diag_gaussian(z, mu, logvar):\n    \"\"\"\n    Compute log q(z | y) for diagonal Gaussian with mu and logvar.\n    z: array of shape (K,) or (L, K)\n    mu: array of shape (K,)\n    logvar: array of shape (K,)\n    Returns: scalar or array of shape (L,)\n    \"\"\"\n    if z.ndim == 1:\n        return -0.5 * (np.sum(np.log(2.0 * np.pi) + logvar + (z - mu)**2 / np.exp(logvar)))\n    elif z.ndim == 2:\n        # Broadcast mu and logvar\n        diff = z - mu  # (L, K)\n        return -0.5 * (z.shape[1] * np.log(2.0 * np.pi) + np.sum(logvar) + np.sum(diff**2 / np.exp(logvar), axis=1))\n    else:\n        raise ValueError(\"z must be 1D or 2D\")\n\ndef sample_z_reparameterized(mu, logvar, L, rng):\n    \"\"\"\n    Sample L latent variables via reparameterization: z = mu + sigma * eps.\n    mu: array of shape (K,)\n    logvar: array of shape (K,)\n    L: integer number of samples\n    rng: numpy Generator\n    Returns: z_samples array of shape (L, K)\n    \"\"\"\n    sigma = np.exp(0.5 * logvar)\n    eps = rng.standard_normal(size=(L, mu.size))\n    z_samples = mu + sigma * eps\n    return z_samples\n\ndef elbo_mc_estimate(y, A, c, W_mu, b_mu, W_logvar, b_logvar, L, rng):\n    \"\"\"\n    Compute Monte Carlo estimate of ELBO for one observation y and given parameters.\n    \"\"\"\n    mu, logvar = nn_params_mu_logvar(y, W_mu, b_mu, W_logvar, b_logvar)\n    z_samples = sample_z_reparameterized(mu, logvar, L, rng)\n    log_pz = log_prior_standard_normal(z_samples)  # (L,)\n    log_py_z = log_poisson_likelihood(y, z_samples, A, c)  # (L,)\n    log_q = log_q_diag_gaussian(z_samples, mu, logvar)  # (L,)\n    elbo_samples = log_py_z + log_pz - log_q  # (L,)\n    return float(np.mean(elbo_samples))\n\ndef solve():\n    # Fixed random seed for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Fixed model parameters A and c\n    A = np.array([[0.3, -0.2],\n                  [0.1,  0.25],\n                  [-0.05, 0.4]], dtype=np.float64)\n    c = np.array([1.0, 0.5, 0.2], dtype=np.float64)\n\n    test_cases = [\n        # Case 1: happy path\n        {\n            \"y\": np.array([5.0, 0.0, 3.0], dtype=np.float64),\n            \"W_mu\": np.array([[0.02, -0.01, 0.03],\n                              [-0.04, 0.05, 0.02]], dtype=np.float64),\n            \"b_mu\": np.array([0.0, 0.0], dtype=np.float64),\n            \"W_logvar\": np.array([[-0.02, 0.01, -0.01],\n                                  [0.03, -0.02, 0.01]], dtype=np.float64),\n            \"b_logvar\": np.array([-0.5, -0.5], dtype=np.float64),\n            \"L\": 100\n        },\n        # Case 2: boundary counts all zeros\n        {\n            \"y\": np.array([0.0, 0.0, 0.0], dtype=np.float64),\n            \"W_mu\": np.array([[0.02, -0.01, 0.03],\n                              [-0.04, 0.05, 0.02]], dtype=np.float64),\n            \"b_mu\": np.array([0.0, 0.0], dtype=np.float64),\n            \"W_logvar\": np.array([[-0.02, 0.01, -0.01],\n                                  [0.03, -0.02, 0.01]], dtype=np.float64),\n            \"b_logvar\": np.array([-0.5, -0.5], dtype=np.float64),\n            \"L\": 100\n        },\n        # Case 3: higher counts\n        {\n            \"y\": np.array([20.0, 15.0, 25.0], dtype=np.float64),\n            \"W_mu\": np.array([[0.01, 0.00, 0.02],\n                              [-0.03, 0.02, 0.01]], dtype=np.float64),\n            \"b_mu\": np.array([0.1, -0.1], dtype=np.float64),\n            \"W_logvar\": np.array([[0.00, 0.00, 0.00],\n                                  [0.00, 0.00, 0.00]], dtype=np.float64),\n            \"b_logvar\": np.array([0.0, 0.0], dtype=np.float64),\n            \"L\": 500\n        },\n        # Case 4: near-degenerate variance in q\n        {\n            \"y\": np.array([4.0, 4.0, 4.0], dtype=np.float64),\n            \"W_mu\": np.array([[0.02, 0.02, 0.02],\n                              [-0.02, -0.02, -0.02]], dtype=np.float64),\n            \"b_mu\": np.array([0.0, 0.0], dtype=np.float64),\n            \"W_logvar\": np.array([[0.00, 0.00, 0.00],\n                                  [0.00, 0.00, 0.00]], dtype=np.float64),\n            \"b_logvar\": np.array([-4.5, -4.5], dtype=np.float64),\n            \"L\": 100\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        y = case[\"y\"]\n        W_mu = case[\"W_mu\"]\n        b_mu = case[\"b_mu\"]\n        W_logvar = case[\"W_logvar\"]\n        b_logvar = case[\"b_logvar\"]\n        L = case[\"L\"]\n        elbo = elbo_mc_estimate(y, A, c, W_mu, b_mu, W_logvar, b_logvar, L, rng)\n        results.append(elbo)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}