{
    "hands_on_practices": [
        {
            "introduction": "Before we can infer causal relationships from time series data, we must first decide on the structure of our predictive model. For a Vector Autoregressive (VAR) model, the most critical parameter to choose is the lag order, $p$, which determines the temporal depth of the interactions we consider. This choice embodies a fundamental trade-off between bias and variance: a model that is too simple (small $p$) will be biased by failing to capture the true dynamics, while a model that is too complex (large $p$) will have high variance, fitting noise instead of signal. This exercise  guides you through the practical use of information criteria like the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to navigate this trade-off, a crucial first step for any rigorous analysis. Understanding their theoretical properties is key to building models that are both accurate and reliable for causal inference.",
            "id": "3967401",
            "problem": "You are analyzing multivariate neural time series recorded from $m$ simultaneously sampled cortical sites, with $T$ time points per site, to estimate directed interactions in the sense of Granger causality (GC). You model the $m$-dimensional process $\\{x_t\\}_{t=1}^{T}$ as a stable Gaussian vector autoregression of order $p$, abbreviated $\\mathrm{VAR}(p)$, given by\n$$\nx_t \\;=\\; \\sum_{i=1}^{p} A_i\\, x_{t-i} \\;+\\; \\varepsilon_t,\\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\Sigma),\\quad t=p+1,\\dots,T,\n$$\nwhere $A_i \\in \\mathbb{R}^{m \\times m}$ are coefficient matrices and $\\Sigma \\in \\mathbb{R}^{m \\times m}$ is a positive definite innovation covariance. You must choose the lag order $p$ before computing GC and, in parallel, you wish to keep in mind implications for transfer entropy as an embedding-dimension analogue.\n\nYour laboratory’s protocol is to select $p$ by minimizing an information criterion computed from the Gaussian log-likelihood of the fitted model, using either Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), and to verify that residuals are approximately white and the fitted model is stable.\n\nWhich of the following statements about how to choose the lag order $p$ with AIC and BIC in this neural time series context, and about the associated bias–variance trade-off for GC estimates, are correct?\n\nA. In a Gaussian $\\mathrm{VAR}(p)$ with $m$ variables and $T$ usable samples, the Akaike Information Criterion minimizes $-2\\ell$ plus a penalty of $2 n_\\theta$, where $n_\\theta = m^2 p + m$ counts the regression parameters (lag coefficients and intercept if included). Because the penalty does not grow with $T$, AIC is not consistent for the true finite order and tends to select larger $p$ asymptotically, which can reduce predictive bias at the expense of higher estimator variance when $T$ is moderate.\n\nB. The Bayesian Information Criterion uses a penalty term $n_\\theta \\ln T$ with $n_\\theta$ as in Option A, so the penalty increases with $T$. Under standard regularity conditions for stationary Gaussian $\\mathrm{VAR}(p)$, BIC is consistent for the true finite order $p_0$ as $T \\to \\infty$, which makes it preferable for structural inference such as GC when $T$ is large.\n\nC. Because the innovation covariance $\\Sigma$ has $m(m+1)/2$ free parameters, those must be included in $n_\\theta$ for AIC and BIC; otherwise, the criteria are invalid and will bias the selected $p$ even when comparing models with different $p$ but the same $m$.\n\nD. Increasing $p$ always decreases the variance of GC estimates because richer models capture more dynamics, so the bias–variance trade-off generally favors taking $p$ as large as possible regardless of $T$.\n\nE. A practical protocol is to pick a ceiling $p_{\\max}$ from neurophysiological time scales and the sampling interval, fit $\\mathrm{VAR}(p)$ for $p \\in \\{1,\\dots,p_{\\max}\\}$, ensure stability and residual whiteness, and then select $p$ by minimizing BIC for causal inference; if residuals remain autocorrelated at the selected $p$, increase $p_{\\max}$ and repeat. This addresses underfitting bias while controlling variance by regularization through the $\\ln T$ penalty.",
            "solution": "The problem statement is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- The system under analysis is a multivariate neural time series from $m$ cortical sites, with $T$ time points per site.\n- The objective is to estimate directed interactions using Granger causality (GC).\n- The model used is a stable Gaussian vector autoregression of order $p$, denoted $\\mathrm{VAR}(p)$.\n- The model equation is $x_t \\;=\\; \\sum_{i=1}^{p} A_i\\, x_{t-i} \\;+\\; \\varepsilon_t$, for $t=p+1,\\dots,T$.\n- $x_t \\in \\mathbb{R}^m$ is the state vector at time $t$.\n- $A_i \\in \\mathbb{R}^{m \\times m}$ are the coefficient matrices for lag $i$.\n- $\\varepsilon_t$ is the innovation process, with $\\varepsilon_t \\sim \\mathcal{N}(0,\\Sigma)$, where $\\Sigma \\in \\mathbb{R}^{m \\times m}$ is a positive definite covariance matrix.\n- The model order $p$ needs to be selected.\n- The proposed selection method is to minimize either the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC), calculated from the Gaussian log-likelihood.\n- A required diagnostic check is to verify that the model residuals are approximately white noise and the fitted model is stable.\n- The question asks to identify the correct statements regarding the choice of $p$ and its implications.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded (Critical):** The problem is firmly grounded in the standard, accepted methodology of time series analysis and computational neuroscience. The use of $\\mathrm{VAR}$ models for Granger causality, and the application of AIC and BIC for model order selection, are canonical techniques. The relationship between GC and transfer entropy for Gaussian processes is also a well-established fact.\n- **Well-Posed:** The problem is well-posed. It asks for an evaluation of several statements about a clearly defined statistical procedure. There exists a basis in statistical theory to judge each statement as correct or incorrect.\n- **Objective (Critical):** The problem is stated in precise, objective, and standard scientific language, free from subjectivity or ambiguity.\n\nThe problem statement is found to be scientifically sound, well-posed, and objective. It contains no fatal flaws, contradictions, or missing information that would preclude a rigorous analysis.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full analysis will be performed.\n\n### Principle-Based Derivation and Option Analysis\n\nThe selection of the lag order $p$ for a $\\mathrm{VAR}(p)$ model is a crucial step that embodies a bias-variance trade-off. An order $p$ that is too small leads to model misspecification and biased estimates (underfitting), as significant lagged relationships are omitted. This can result in spurious or missed causal links. An order $p$ that is too large leads to the estimation of many unnecessary parameters, which increases the variance of the coefficient estimates and, consequently, the variance of the GC estimates (overfitting). This can reduce the statistical power to detect true connections.\n\nInformation criteria like AIC and BIC provide a formal way to manage this trade-off. They are based on the maximized log-likelihood ($\\ell$) of the model, with a penalty for the number of estimated parameters ($k$). For a Gaussian $\\mathrm{VAR}(p)$ model fit to $T_{\\text{eff}} = T-p$ effective data points, the general form is:\n$$ \\text{IC} = -2\\ell + \\text{penalty}(k, T_{\\text{eff}}) $$\nThe log-likelihood term, up to a constant, is related to the determinant of the estimated residual covariance matrix, $\\hat{\\Sigma}_p$. Specifically, $-2\\ell$ is proportional to $T_{\\text{eff}} \\ln(\\det(\\hat{\\Sigma}_p))$.\n- **Akaike Information Criterion (AIC):** $AIC = -2\\ell + 2k$.\n- **Bayesian Information Criterion (BIC):** $BIC = -2\\ell + k \\ln(T_{\\text{eff}})$.\n\nThe number of parameters $k$ (denoted $n_\\theta$ in the problem) in a $\\mathrm{VAR}(p)$ model with an intercept $c \\in \\mathbb{R}^m$ is the sum of parameters in the $p$ coefficient matrices ($m \\times m$ each), the intercept vector ($m \\times 1$), and the symmetric covariance matrix $\\Sigma$ ($m(m+1)/2$). Thus, the total number of free parameters is $k = m^2p + m + m(m+1)/2$.\n\nNow, let's evaluate each option.\n\n**A. In a Gaussian $\\mathrm{VAR}(p)$ with $m$ variables and $T$ usable samples, the Akaike Information Criterion minimizes $-2\\ell$ plus a penalty of $2 n_\\theta$, where $n_\\theta = m^2 p + m$ counts the regression parameters (lag coefficients and intercept if included). Because the penalty does not grow with $T$, AIC is not consistent for the true finite order and tends to select larger $p$ asymptotically, which can reduce predictive bias at the expense of higher estimator variance when $T$ is moderate.**\nThe statement correctly identifies the form of the AIC penalty as $2k$. It defines the number of parameters $k = n_\\theta = m^2p + m$, which includes the mean-related parameters that vary with $p$. This is a standard and valid approach for comparing models of different orders $p$ but fixed dimension $m$ (as the parameters in $\\Sigma$ would add a constant penalty term, not affecting the choice of $p$). The penalty factor of $2$ does not depend on the sample size $T$. Consequently, as $T \\to \\infty$, the likelihood term dominates, and AIC has a non-zero probability of selecting a model with more parameters than the true model (i.e., it is not a consistent estimator of model order). This tendency to select larger models can reduce the model specification bias, which is beneficial for prediction, but it comes at the cost of increased variance in the parameter estimates, especially for finite sample sizes. All parts of this statement are theoretically sound.\n**Verdict: Correct**\n\n**B. The Bayesian Information Criterion uses a penalty term $n_\\theta \\ln T$ with $n_\\theta$ as in Option A, so the penalty increases with $T$. Under standard regularity conditions for stationary Gaussian $\\mathrm{VAR}(p)$, BIC is consistent for the true finite order $p_0$ as $T \\to \\infty$, which makes it preferable for structural inference such as GC when $T$ is large.**\nThe statement correctly identifies the BIC penalty term as $k \\ln(T)$ (here $k=n_\\theta$ and $T$ stands for the effective sample size, which is standard notation in asymptotic contexts). This penalty increases with the sample size $T$, making it much stricter than AIC's penalty for large $T$. A key theoretical result is that, under regularity conditions (including the existence of a true, finite model order $p_0$), BIC is a consistent model selection criterion. This means that as $T \\to \\infty$, the probability that BIC selects the true model order $p_0$ approaches $1$. Because structural inference, such as identifying causal links with GC, is concerned with recovering the \"true\" data-generating process, a consistent criterion like BIC is generally preferred over a non-consistent one like AIC, particularly when the sample size is large enough for asymptotic properties to be relevant.\n**Verdict: Correct**\n\n**C. Because the innovation covariance $\\Sigma$ has $m(m+1)/2$ free parameters, those must be included in $n_\\theta$ for AIC and BIC; otherwise, the criteria are invalid and will bias the selected $p$ even when comparing models with different $p$ but the same $m$.**\nThis statement is incorrect. While it is true that a full accounting of all model parameters includes the $m(m+1)/2$ free parameters in the covariance matrix $\\Sigma$, these parameters do not need to be included in the penalty term *for the specific purpose of selecting the lag order $p$*. When comparing a set of $\\mathrm{VAR}(p)$ models where only $p$ varies and the dimension $m$ is fixed, the number of parameters in $\\Sigma$ is the same for all candidate models. Let $k_p = m^2p + m$ be the number of parameters in the mean equation. The full AIC would be $AIC_{full}(p) = -2\\ell(p) + 2(k_p + m(m+1)/2)$. The simplified AIC is $AIC_{simple}(p) = -2\\ell(p) + 2k_p$. The difference $AIC_{full}(p) - AIC_{simple}(p) = 2 \\cdot m(m+1)/2 = m(m+1)$ is a constant that is independent of $p$. Adding a constant to a function does not change the location of its minimum. Therefore, $\\mathrm{arg min}_p AIC_{full}(p) = \\mathrm{arg min}_p AIC_{simple}(p)$. The same logic applies to BIC. The criteria are not \"invalid\" and the selection is not \"biased\" by omitting this constant term. The only context where this term would be essential is if one were comparing models with different dimensions $m$.\n**Verdict: Incorrect**\n\n**D. Increasing $p$ always decreases the variance of GC estimates because richer models capture more dynamics, so the bias–variance trade-off generally favors taking $p$ as large as possible regardless of $T$.**\nThis statement fundamentally misrepresents the bias-variance trade-off. Increasing the model order $p$ means estimating more parameters from the same amount of data. This *increases*, not decreases, the sampling variance of the coefficient estimates ($A_i$). Since GC measures are functions of these coefficients, their estimates also become more variable. While a richer model (larger $p$) can reduce the model's structural bias (by \"capturing more dynamics\"), it does so at the expense of higher estimator variance. The essence of the trade-off is finding an optimal $p$ that balances low bias and low variance, not to make $p$ as large as possible. Excessively large $p$ leads to overfitting and highly unreliable estimates.\n**Verdict: Incorrect**\n\n**E. A practical protocol is to pick a ceiling $p_{\\max}$ from neurophysiological time scales and the sampling interval, fit $\\mathrm{VAR}(p)$ for $p \\in \\{1,\\dots,p_{\\max}\\}$, ensure stability and residual whiteness, and then select $p$ by minimizing BIC for causal inference; if residuals remain autocorrelated at the selected $p$, increase $p_{\\max}$ and repeat. This addresses underfitting bias while controlling variance by regularization through the $\\ln T$ penalty.**\nThis statement outlines a sound and comprehensive practical workflow.\n1.  Choosing $p_{\\max}$ based on domain knowledge (neurophysiology) is a crucial first step to constrain the search space reasonably.\n2.  Searching over a range of lag orders $\\{1,\\dots,p_{\\max}\\}$ is the standard procedure.\n3.  Verifying model stability and residual whiteness are essential diagnostic checks for model adequacy. If residuals are not white, the model is misspecified, violating the assumptions of GC.\n4.  Using BIC is appropriate for causal/structural inference, as explained in B.\n5.  The iterative loop—increasing $p_{\\max}$ if diagnostics fail even for the BIC-selected model—is a critical step to ensure that the initial search space was not too small, thus guarding against underfitting.\n6.  The final sentence accurately summarizes the logic: the procedure aims to find a model complex enough to avoid underfitting bias (by testing up to $p_{\\max}$ and checking residuals) while using the strong penalty of BIC to prevent overfitting and control estimator variance. The penalty term in information criteria can be conceptually understood as a form of regularization on model complexity. This entire protocol is a staple of best practices in applied time series analysis.\n**Verdict: Correct**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "Once a VAR model of appropriate order is specified, the core question of Granger causality can be formally addressed. This involves statistically comparing a full model that includes the putative causal connection against a nested, restricted model that excludes it. The standard approach for this comparison in a linear-Gaussian framework is the $F$-test, which quantifies whether including past values from a source variable leads to a statistically significant reduction in the prediction error of a target variable. This practice problem  demystifies the $F$-test by tasking you with its derivation from the foundational concept of residual sums of squares. By working through both the theory and a concrete calculation, you will gain a robust understanding of the statistical engine that powers linear Granger causality analysis.",
            "id": "3967426",
            "problem": "Consider two simultaneously recorded neural time series from two brain regions, denoted by $x_t$ and $y_t$, sampled at $T$ equally spaced time points. Suppose these processes are jointly modeled by a bivariate Vector Autoregressive (VAR) model of order $p$, and the Granger causality question of interest is whether $x_t$ Granger-causes $y_t$. Focus on the single-output regression for $y_t$ with lagged predictors. Under the joint stationarity and full-rank design assumptions, and assuming additive independent and identically distributed Gaussian innovations with constant variance, Ordinary Least Squares (OLS) estimation is applied to both an unrestricted and a restricted linear model for $y_t$, where the restricted model excludes the lagged predictors of $x_t$.\n\nStarting from the linear regression formulation for $y_t$ (with intercept), the Gaussian noise model, and the least squares principle, derive the appropriate $F$-test for a set of $q$ linear restrictions that remove the lagged $x_t$ terms from the $y_t$ equation. Your derivation should begin from the definitions of residual sums of squares and the sampling distributions of sums of squared residuals under the null hypothesis, and proceed to an expression for the test statistic in terms of residual sums of squares and model dimensions only.\n\nThen, apply your derived result to the following concrete scenario. A bivariate VAR with order $p=2$ and an intercept is fit to $T=200$ time points. The $y_t$ equation in the unrestricted regression includes lagged predictors $\\{y_{t-1}, y_{t-2}, x_{t-1}, x_{t-2}\\}$ and an intercept, while the restricted regression includes only $\\{y_{t-1}, y_{t-2}\\}$ and an intercept. Let $N$ denote the number of usable rows after accounting for lags, and let $k_U$ denote the number of regressors (including the intercept) in the unrestricted $y_t$ regression. The OLS fits yield residual sums of squares $\\mathrm{RSS}_U = 95.3$ for the unrestricted regression and $\\mathrm{RSS}_R = 106.7$ for the restricted regression. Compute the numerical value of the $F$-statistic for testing the null hypothesis of no Granger causality from $x_t$ to $y_t$ in this setup. Round your answer to four significant figures. No units are required in the final answer.",
            "solution": "The problem asks for two parts: first, to derive the general formula for the $F$-test for linear restrictions in the context of Granger causality in a Vector Autoregressive (VAR) model; and second, to apply this formula to compute a numerical value for a specific scenario.\n\nPart 1: Derivation of the $F$-statistic\n\nWe consider a single-output linear regression for the time series $y_t$. The unrestricted model includes an intercept and $p$ lags of both $y_t$ and $x_t$ as predictors. For time points $t=p+1, \\dots, T$, the model is:\n$$ y_t = \\beta_0 + \\sum_{i=1}^{p} \\alpha_i y_{t-i} + \\sum_{i=1}^{p} \\gamma_i x_{t-i} + \\epsilon_t $$\nThis is the unrestricted model, which we can denote as $U$. The error terms $\\epsilon_t$ are assumed to be independent and identically distributed as a Gaussian distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$. The total number of usable observations after accounting for the $p$ lags is $N = T - p$. The number of parameters (regressors) in this unrestricted model is $k_U = 1 \\text{ (intercept)} + p \\text{ (lags of } y) + p \\text{ (lags of } x) = 1 + 2p$.\n\nThe null hypothesis for the Granger causality test is that $x_t$ does not Granger-cause $y_t$. This corresponds to the set of linear restrictions that the coefficients of all lagged $x_t$ terms are zero:\n$$ H_0: \\gamma_1 = \\gamma_2 = \\dots = \\gamma_p = 0 $$\nThere are $q=p$ such restrictions. The problem statement refers to these generally as a set of $q$ linear restrictions.\n\nUnder this null hypothesis, we have the restricted model, denoted as $R$:\n$$ y_t = \\beta_0 + \\sum_{i=1}^{p} \\alpha_i y_{t-i} + \\epsilon_t $$\nThe number of parameters in the restricted model is $k_R = 1 + p$. The number of restrictions is indeed $q = k_U - k_R$.\n\nFor both models, we fit the parameters using Ordinary Least Squares (OLS), which minimizes the Residual Sum of Squares (RSS). Let $\\mathrm{RSS}_U$ be the RSS for the unrestricted model and $\\mathrm{RSS}_R$ be the RSS for the restricted model.\n$$ \\mathrm{RSS}_U = \\sum_{t=p+1}^{T} (y_t - \\hat{y}_{t,U})^2 $$\n$$ \\mathrm{RSS}_R = \\sum_{t=p+1}^{T} (y_t - \\hat{y}_{t,R})^2 $$\nwhere $\\hat{y}_{t,U}$ and $\\hat{y}_{t,R}$ are the fitted values from the unrestricted and restricted models, respectively. Since the restricted model is a special case of the unrestricted model, it must be that $\\mathrm{RSS}_R \\ge \\mathrm{RSS}_U$.\n\nThe derivation of the $F$-test begins from the sampling distributions of these RSS terms, which are consequences of the assumed Gaussian noise model and the properties of OLS. According to statistical theory (Cochran's theorem):\n$1$. The RSS of the full (unrestricted) model, when scaled by the true unknown error variance $\\sigma^2$, follows a chi-squared distribution with degrees of freedom equal to the number of observations minus the number of parameters:\n$$ \\frac{\\mathrm{RSS}_U}{\\sigma^2} \\sim \\chi^2_{N-k_U} $$\n$2$. Under the null hypothesis $H_0$, the difference in RSS between the restricted and unrestricted models, also scaled by $\\sigma^2$, follows a chi-squared distribution with degrees of freedom equal to the number of restrictions $q$:\n$$ \\frac{\\mathrm{RSS}_R - \\mathrm{RSS}_U}{\\sigma^2} \\sim \\chi^2_q $$\n$3$. A crucial result is that, under $H_0$, the two quantities $\\mathrm{RSS}_U$ and $(\\mathrm{RSS}_R - \\mathrm{RSS}_U)$ are statistically independent.\n\nAn $F$-distributed random variable is defined as the ratio of two independent chi-squared variables, each divided by its degrees of freedom. We can therefore construct the test statistic as the ratio of the quantities from points $2$ and $1$ above, each divided by its respective degrees of freedom ($q$ and $N-k_U$):\n$$ F = \\frac{\\left( \\frac{\\mathrm{RSS}_R - \\mathrm{RSS}_U}{\\sigma^2} \\right) / q}{\\left( \\frac{\\mathrm{RSS}_U}{\\sigma^2} \\right) / (N-k_U)} $$\nThe unknown variance term $\\sigma^2$ conveniently cancels out from the numerator and the denominator. This yields the final expression for the $F$-statistic in terms of the RSS values and model dimensions:\n$$ F = \\frac{(\\mathrm{RSS}_R - \\mathrm{RSS}_U) / q}{\\mathrm{RSS}_U / (N-k_U)} $$\nUnder the null hypothesis $H_0$, this statistic follows an $F$-distribution with $q$ and $N-k_U$ degrees of freedom, written as $F \\sim F(q, N-k_U)$.\n\nPart 2: Numerical Calculation\n\nWe are given the following information for the specific scenario:\n- Total number of time points, $T = 200$.\n- Order of the VAR model, $p = 2$.\n- Residual sum of squares for the unrestricted model, $\\mathrm{RSS}_U = 95.3$.\n- Residual sum of squares for the restricted model, $\\mathrm{RSS}_R = 106.7$.\n\nFrom this information, we must determine the model dimensions $N$, $k_U$, and $q$.\n- The number of usable observations is $N = T - p = 200 - 2 = 198$.\n- The unrestricted model for $y_t$ includes an intercept, $p=2$ lags of $y_t$ ($y_{t-1}, y_{t-2}$), and $p=2$ lags of $x_t$ ($x_{t-1}, x_{t-2}$). The number of regressors (parameters) is $k_U = 1 (\\text{intercept}) + 2 (\\text{lags of } y) + 2 (\\text{lags of } x) = 5$.\n- The restricted model for $y_t$ excludes the $p=2$ lags of $x_t$. The number of restrictions is therefore $q=2$. This is consistent with the difference in the number of parameters between the unrestricted model ($k_U=5$) and the restricted model (which would have $k_R = 1+2=3$ parameters), so $q = k_U - k_R = 5 - 3 = 2$.\n- The degrees of freedom for the denominator of the $F$-statistic are $N-k_U = 198 - 5 = 193$.\n\nNow we substitute these values into the derived formula for the $F$-statistic:\n$$ F = \\frac{(\\mathrm{RSS}_R - \\mathrm{RSS}_U) / q}{\\mathrm{RSS}_U / (N-k_U)} = \\frac{(106.7 - 95.3) / 2}{95.3 / (198 - 5)} $$\nFirst, we compute the terms in the numerator and denominator:\n$$ \\mathrm{RSS}_R - \\mathrm{RSS}_U = 11.4 $$\n$$ \\text{Numerator term} = \\frac{11.4}{2} = 5.7 $$\n$$ \\text{Denominator term} = \\frac{95.3}{193} \\approx 0.49378238 $$\nFinally, we compute the ratio:\n$$ F = \\frac{5.7}{95.3 / 193} \\approx \\frac{5.7}{0.49378238} \\approx 11.5435528 $$\nThe problem requires the answer to be rounded to four significant figures. The fifth significant digit is $3$, which is less than $5$, so we round down.\n$$ F \\approx 11.54 $$\nThis value would then be compared to the critical value from the $F(2, 193)$ distribution to determine the statistical significance of the Granger causality from $x_t$ to $y_t$.",
            "answer": "$$\\boxed{11.54}$$"
        },
        {
            "introduction": "In complex systems like neural circuits, interactions are rarely isolated. A statistical dependency observed between two nodes, $Y$ and $X$, could reflect a direct causal influence ($Y \\to X$), an indirect one mediated by a third node ($Y \\to Z \\to X$), or a spurious correlation induced by a common driver ($Y \\leftarrow Z \\to X$). Distinguishing these scenarios requires moving beyond simple pairwise tests to a more nuanced, network-level perspective. By systematically comparing the results of pairwise, conditional, and partial Granger causality, we can begin to dissect the network and infer the underlying causal architecture. This advanced, hands-on exercise  challenges you to implement and interpret this suite of tests on simulated data, equipping you with the practical skills to investigate network hypotheses and avoid common misinterpretations of bivariate causality measures in a multivariate world.",
            "id": "3967370",
            "problem": "Construct and implement, from first principles, a test to decide whether a discrete-time neural process $Y$ Granger-causes $X$ only indirectly via $Z$, by comparing pairwise, conditional, and partial Granger causality on simulated multivariate autoregressive data. Use the following foundational base: linear Gaussian multivariate autoregression, least-squares projection, and nested linear model comparison. You must not assume or use any pre-packaged causality routines or shortcut formulas; derive and implement all steps using ordinary least squares and classical nested-model hypothesis testing.\n\nAssume three scalar neural processes $X_t$, $Y_t$, $Z_t$ that evolve as a stationary vector autoregression of order $p$ with Gaussian innovations:\n$$\n\\mathbf{s}_t = \\mathbf{A}_1 \\mathbf{s}_{t-1} + \\mathbf{A}_2 \\mathbf{s}_{t-2} + \\boldsymbol{\\varepsilon}_t,\n$$\nwhere $\\mathbf{s}_t = [X_t, Y_t, Z_t]^\\top$, $\\mathbf{A}_1 \\in \\mathbb{R}^{3 \\times 3}$ and $\\mathbf{A}_2 \\in \\mathbb{R}^{3 \\times 3}$ are coefficient matrices, and $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma})$ with $\\mathbf{\\Sigma}$ being a positive definite covariance matrix. Let $p = 2$.\n\nYou must implement the following quantities via least-squares linear prediction and nested model comparison:\n\n- Pairwise Granger causality from $Y$ to $X$, assessed by checking whether adding the past of $Y$ (up to lag $p$) to an autoregressive predictor for $X$ reduces the one-step-ahead mean-squared prediction error compared to using only the past of $X$.\n\n- Conditional Granger causality from $Y$ to $X$ given $Z$, assessed by checking whether adding the past of $Y$ (up to lag $p$) to a predictor for $X$ that already includes the past of $X$ and the past of $Z$ reduces the one-step-ahead mean-squared prediction error.\n\n- Partial Granger causality from $Y$ to $X$ “partialled by” $Z$, defined in the time domain as follows: first remove linear dependence on the past of $Z$ (up to lag $p$) from both $X_t$ and $Y_t$ via least-squares projection, thereby forming residual series $r^X_t$ and $r^Y_t$; then test pairwise Granger causality from $Y$ to $X$ on the residual series by checking whether adding the past of $r^Y$ (up to lag $p$) to an autoregressive predictor for $r^X$ reduces the one-step-ahead mean-squared prediction error.\n\nUse classical nested linear model hypothesis testing with the $\\mathsf{F}$ distribution to decide significance at level $\\alpha = 0.01$. For a restricted model with residual sum of squares $RSS_R$, an unrestricted model with residual sum of squares $RSS_U$, $d$ additional regressors between unrestricted and restricted, sample size $N$ (number of usable prediction targets after lagging), and $k_U$ unrestricted regressors including intercept, compute the test statistic using the standard nested linear regression $\\mathsf{F}$-test and its corresponding tail probability under the null hypothesis; do not use any alternative shortcuts.\n\nSimulate data using the following test suite, each with Gaussian innovations of covariance $\\mathbf{\\Sigma} = \\operatorname{diag}([1,1,1])$, burn-in of $B = 200$ samples, and lag order $p = 2$. For each case, use the specified number of samples $T$ and coefficient matrices $\\mathbf{A}_1$ and $\\mathbf{A}_2$, where rows index targets $[X,Y,Z]$ and columns index sources $[X,Y,Z]$:\n\n- Case $1$ (indirect-only $Y \\rightarrow Z \\rightarrow X$): $T = 1000$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.4  0.0  0.5 \\\\\n0.0  0.4  0.0 \\\\\n0.0  0.5  0.4\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\begin{bmatrix}\n0.2  0.0  0.0 \\\\\n0.0  0.2  0.0 \\\\\n0.0  0.0  0.2\n\\end{bmatrix}.\n$$\n\n- Case $2$ (direct $Y \\rightarrow X$ plus indirect path): $T = 1000$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.4  0.3  0.5 \\\\\n0.0  0.4  0.0 \\\\\n0.0  0.5  0.4\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\begin{bmatrix}\n0.2  0.0  0.0 \\\\\n0.0  0.2  0.0 \\\\\n0.0  0.0  0.2\n\\end{bmatrix}.\n$$\n\n- Case $3$ (no influence from $Y$): $T = 1000$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.4  0.0  0.5 \\\\\n0.0  0.4  0.0 \\\\\n0.0  0.0  0.4\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\begin{bmatrix}\n0.2  0.0  0.0 \\\\\n0.0  0.2  0.0 \\\\\n0.0  0.0  0.2\n\\end{bmatrix}.\n$$\n\n- Case $4$ (weak couplings and small sample): $T = 200$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.3  0.0  0.2 \\\\\n0.0  0.3  0.0 \\\\\n0.0  0.2  0.3\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\begin{bmatrix}\n0.1  0.0  0.0 \\\\\n0.0  0.1  0.0 \\\\\n0.0  0.0  0.1\n\\end{bmatrix}.\n$$\n\n- Case $5$ (common driver $Z$ to both $X$ and $Y$, no $Y \\rightarrow Z$): $T = 1000$,\n$$\n\\mathbf{A}_1 = \\begin{bmatrix}\n0.4  0.0  0.5 \\\\\n0.0  0.4  0.5 \\\\\n0.0  0.0  0.4\n\\end{bmatrix}, \\quad\n\\mathbf{A}_2 = \\begin{bmatrix}\n0.2  0.0  0.0 \\\\\n0.0  0.2  0.0 \\\\\n0.0  0.0  0.2\n\\end{bmatrix}.\n$$\n\nFor each case, simulate the process, then compute:\n- Pairwise Granger causality significance from $Y$ to $X$.\n- Conditional Granger causality significance from $Y$ to $X$ given $Z$.\n- Partial Granger causality significance from $Y$ to $X$ partialled by $Z$ (as defined above).\n\nDecide that \"$Y$ Granger-causes $X$ only indirectly via $Z$\" if and only if the pairwise test is significant and both the conditional and partial tests are not significant at level $\\alpha = 0.01$.\n\nYour program should produce a single line of output containing the boolean results for the five cases, as a comma-separated list enclosed in square brackets (e.g., $[{\\rm True},{\\rm False},{\\rm True},{\\rm False},{\\rm False}]$). No physical units are involved, and the final outputs are booleans.",
            "solution": "The present task is to construct and implement a statistical test to determine if a neural process $Y$ Granger-causes another process $X$ solely through an indirect pathway mediated by a third process $Z$. This analysis will be performed by comparing three distinct measures of Granger causality—pairwise, conditional, and partial—derived from first principles using a linear autoregressive framework. The foundation of our method will be ordinary least-squares (OLS) regression and the classical $\\mathsf{F}$-test for nested linear models.\n\nWe begin by defining the system. We consider three zero-mean, jointly wide-sense stationary, scalar time series, $X_t$, $Y_t$, and $Z_t$, generated by a vector autoregressive (VAR) process of order $p$. For this problem, $p=2$. The model is specified as:\n$$\n\\mathbf{s}_t = \\mathbf{A}_1 \\mathbf{s}_{t-1} + \\mathbf{A}_2 \\mathbf{s}_{t-2} + \\boldsymbol{\\varepsilon}_t\n$$\nwhere $\\mathbf{s}_t = [X_t, Y_t, Z_t]^\\top$ is the state vector at time $t$, $\\mathbf{A}_1, \\mathbf{A}_2 \\in \\mathbb{R}^{3 \\times 3}$ are the coefficient matrices describing the linear interactions at lags $1$ and $2$, respectively, and $\\boldsymbol{\\varepsilon}_t$ is a vector of i.i.d. Gaussian white noise innovations, with $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma})$.\n\nThe core principle of Granger causality is predictive power. We say that $Y$ Granger-causes $X$ if the past of $Y$ contains information that improves the prediction of $X$ beyond the information already contained in the past of $X$ itself. We will formalize and test this concept in three different contexts. For each test, we will compare a restricted linear model (null hypothesis, $H_0$) with an unrestricted one (alternative hypothesis, $H_1$).\n\nLet $X_{t-1:t-p}$ denote the set of lagged variables $\\{X_{t-1}, X_{t-2}, \\dots, X_{t-p}\\}$. A constant (intercept) term is included in all regression models.\n\n**1. Pairwise Granger Causality ($Y \\rightarrow X$)**\nThis test assesses the direct predictive link from $Y$ to $X$, ignoring all other processes.\n- **Restricted Model ($H_0$: $Y$ does not Granger-cause $X$):** We predict $X_t$ using only its own past.\n$$\nX_t = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_{t-i} + \\epsilon_{R,t}\n$$\n- **Unrestricted Model ($H_1$: $Y$ Granger-causes $X$):** We predict $X_t$ using its own past and the past of $Y$.\n$$\nX_t = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_{t-i} + \\sum_{j=1}^{p} \\gamma_j Y_{t-j} + \\epsilon_{U,t}\n$$\nThe test evaluates if the coefficients $\\gamma_j$ are jointly non-zero.\n\n**2. Conditional Granger Causality ($Y \\rightarrow X | Z$)**\nThis test assesses the predictive link from $Y$ to $X$ in the context of $Z$. It asks if $Y$ provides unique information for predicting $X$ that is not already present in the past of $X$ and $Z$.\n- **Restricted Model ($H_0$: $Y$ does not Granger-cause $X$ conditional on $Z$):** We predict $X_t$ using its own past and the past of $Z$.\n$$\nX_t = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_{t-i} + \\sum_{k=1}^{p} \\delta_k Z_{t-k} + \\epsilon_{R,t}\n$$\n- **Unrestricted Model ($H_1$: $Y$ Granger-causes $X$ conditional on $Z$):** We predict $X_t$ using the past of all three processes.\n$$\nX_t = \\beta_0 + \\sum_{i=1}^{p} \\beta_i X_{t-i} + \\sum_{j=1}^{p} \\gamma_j Y_{t-j} + \\sum_{k=1}^{p} \\delta_k Z_{t-k} + \\epsilon_{U,t}\n$$\nThe test evaluates if the coefficients $\\gamma_j$ are jointly non-zero.\n\n**3. Partial Granger Causality ($Y \\rightarrow X \\cdot Z$)**\nThis test follows a two-stage procedure as defined. It aims to find the causal relationship between $Y$ and $X$ after removing the linear influence of $Z$ from both.\n- **Stage 1: Residualization.** We perform two OLS regressions to compute residuals.\n    - First, we model $X_t$ as a function of the past of $Z$:\n    $$\n    X_t = \\alpha_0 + \\sum_{k=1}^{p} \\alpha_k Z_{t-k} + r^X_t\n    $$\n    The residuals $r^X_t$ represent the part of $X_t$ not linearly predictable from the past of $Z$.\n    - Similarly, we model $Y_t$ as a function of the past of $Z$:\n    $$\n    Y_t = \\zeta_0 + \\sum_{k=1}^{p} \\zeta_k Z_{t-k} + r^Y_t\n    $$\n    The residuals $r^Y_t$ represent the part of $Y_t$ not linearly predictable from the past of $Z$.\n- **Stage 2: Pairwise Granger Causality on Residuals ($r^Y \\rightarrow r^X$).** We now test for Granger causality from the residual series $r^Y$ to $r^X$.\n    - **Restricted Model ($H_0$):** We predict $r^X_t$ using only its own past.\n    $$\n    r^X_t = \\beta_0 + \\sum_{i=1}^{p} \\beta_i r^X_{t-i} + \\epsilon_{R,t}\n    $$\n    - **Unrestricted Model ($H_1$):** We predict $r^X_t$ using its own past and the past of $r^Y$.\n    $$\n    r^X_t = \\beta_0 + \\sum_{i=1}^{p} \\beta_i r^X_{t-i} + \\sum_{j=1}^{p} \\gamma_j r^Y_{t-j} + \\epsilon_{U,t}\n    $$\n\n**Hypothesis Testing via $\\mathsf{F}$-Test**\nFor each of the three causality tests, we fit the corresponding restricted and unrestricted models via OLS to obtain their respective Residual Sums of Squares, $RSS_R$ and $RSS_U$. The significance of adding the new regressors (the past of $Y$) is assessed using the $\\mathsf{F}$-statistic for nested models:\n$$\n\\mathsf{F} = \\frac{(RSS_R - RSS_U) / d}{RSS_U / (N - k_U)}\n$$\nwhere:\n- $N$ is the number of data points used for the regression.\n- $d$ is the number of additional regressors in the unrestricted model (here, $d=p=2$).\n- $k_U$ is the total number of regressors in the unrestricted model, including the intercept. For pairwise, $k_U = 1+2p$. For conditional, $k_U = 1+3p$. For partial (on residuals), $k_U = 1+2p$.\n- The number of samples $N$ is $T-p$ for the pairwise and conditional tests, and $T-2p$ for the partial test due to the two-stage process.\n\nUnder the null hypothesis $H_0$, this statistic follows an $\\mathsf{F}$-distribution with $d$ and $N - k_U$ degrees of freedom, $\\mathsf{F}(d, N-k_U)$. We compute the p-value, which is the probability of observing an $\\mathsf{F}$-statistic at least as large as the one calculated, assuming $H_0$ is true. If this p-value is less than the significance level $\\alpha = 0.01$, we reject $H_0$ and conclude that causality is significant.\n\n**Decision Criterion for Indirect-Only Causality**\nThe signature of an indirect-only causal pathway $Y \\rightarrow Z \\rightarrow X$ is that the influence from $Y$ to $X$ is present when $Z$ is ignored, but vanishes when $Z$ is accounted for. The problem defines this condition as follows:\n- Pairwise Granger causality ($Y \\rightarrow X$) is significant.\n- **AND** Conditional Granger causality ($Y \\rightarrow X|Z$) is **not** significant.\n- **AND** Partial Granger causality ($Y \\rightarrow X \\cdot Z$) is **not** significant.\n\nThis set of conditions forms our final test for identifying an exclusively indirect influence. We will now implement this procedure.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    np.random.seed(0)  # for reproducibility\n    p = 2\n    alpha = 0.01\n    B = 200 # burn-in\n\n    test_cases = [\n        # Case 1: Indirect-only Y - Z - X\n        {'T': 1000,\n         'A1': np.array([[0.4, 0.0, 0.5],\n                         [0.0, 0.4, 0.0],\n                         [0.0, 0.5, 0.4]]),\n         'A2': np.array([[0.2, 0.0, 0.0],\n                         [0.0, 0.2, 0.0],\n                         [0.0, 0.0, 0.2]])},\n        # Case 2: Direct Y - X plus indirect path\n        {'T': 1000,\n         'A1': np.array([[0.4, 0.3, 0.5],\n                         [0.0, 0.4, 0.0],\n                         [0.0, 0.5, 0.4]]),\n         'A2': np.array([[0.2, 0.0, 0.0],\n                         [0.0, 0.2, 0.0],\n                         [0.0, 0.0, 0.2]])},\n        # Case 3: No influence from Y\n        {'T': 1000,\n         'A1': np.array([[0.4, 0.0, 0.5],\n                         [0.0, 0.4, 0.0],\n                         [0.0, 0.0, 0.4]]),\n         'A2': np.array([[0.2, 0.0, 0.0],\n                         [0.0, 0.2, 0.0],\n                         [0.0, 0.0, 0.2]])},\n        # Case 4: Weak couplings and small sample\n        {'T': 200,\n         'A1': np.array([[0.3, 0.0, 0.2],\n                         [0.0, 0.3, 0.0],\n                         [0.0, 0.2, 0.3]]),\n         'A2': np.array([[0.1, 0.0, 0.0],\n                         [0.0, 0.1, 0.0],\n                         [0.0, 0.0, 0.1]])},\n        # Case 5: Common driver Z to both X and Y\n        {'T': 1000,\n         'A1': np.array([[0.4, 0.0, 0.5],\n                         [0.0, 0.4, 0.5],\n                         [0.0, 0.0, 0.4]]),\n         'A2': np.array([[0.2, 0.0, 0.0],\n                         [0.0, 0.2, 0.0],\n                         [0.0, 0.0, 0.2]])}\n    ]\n\n    Sigma = np.identity(3)\n    results = []\n\n    for case in test_cases:\n        T, A1, A2 = case['T'], case['A1'], case['A2']\n        \n        # 1. Simulate data\n        s = np.zeros((T + B, 3))\n        # Initial values (p=2 lags)\n        s[0, :] = np.random.multivariate_normal(np.zeros(3), Sigma)\n        s[1, :] = np.random.multivariate_normal(np.zeros(3), Sigma)\n        \n        innovations = np.random.multivariate_normal(np.zeros(3), Sigma, size=T + B)\n        \n        for t in range(p, T + B):\n            s[t, :] = A1 @ s[t-1, :] + A2 @ s[t-2, :] + innovations[t, :]\n            \n        data = s[B:] # discard burn-in\n        X, Y, Z = data[:, 0], data[:, 1], data[:, 2]\n\n        # 2. Run causality analysis\n        sig_pair, sig_cond, sig_part = run_causality_analysis(X, Y, Z, p, alpha)\n\n        # 3. Apply decision rule\n        is_indirect_only = sig_pair and not sig_cond and not sig_part\n        results.append(is_indirect_only)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef f_test(rss_r, rss_u, d, N, k_u):\n    \"\"\"\n    Computes the F-statistic and p-value for a nested model test.\n    \"\"\"\n    if rss_u = 0 or N - k_u = 0: return 1.0, 0.0 # handle degenerate cases\n    f_stat = ((rss_r - rss_u) / d) / (rss_u / (N - k_u))\n    p_value = f_dist.sf(f_stat, d, N - k_u)\n    return p_value, f_stat\n\ndef get_lagged_matrix(series_list, p, T):\n    \"\"\"\n    Creates a design matrix from lagged series.\n    \"\"\"\n    N = T - p\n    lagged_cols = []\n    for series in series_list:\n        for i in range(1, p + 1):\n            lagged_cols.append(series[p - i : T - i])\n    \n    # Add intercept\n    design_matrix = np.hstack([np.ones((N, 1))] + [col.reshape(-1, 1) for col in lagged_cols])\n    return design_matrix\n\ndef ols_rss(y, X_design):\n    \"\"\"\n    Performs OLS and returns the Residual Sum of Squares (RSS).\n    \"\"\"\n    try:\n        # np.linalg.lstsq returns RSS as the second element of the tuple if y is 1D\n        residuals_sq = np.linalg.lstsq(X_design, y, rcond=None)[1]\n        if residuals_sq.size == 0: \n            # This case occurs if the system is perfectly determined or overdetermined with perfect fit\n            return 0.0\n        return residuals_sq[0]\n    except np.linalg.LinAlgError:\n        return np.inf\n\ndef run_causality_analysis(X, Y, Z, p, alpha):\n    \"\"\"\n    Performs pairwise, conditional, and partial Granger causality tests.\n    \"\"\"\n    T = len(X)\n    N = T - p\n\n    # --- 1. Pairwise GC (Y - X) ---\n    y_target = X[p:]\n    \n    # Restricted model: X ~ past(X)\n    X_design_R = get_lagged_matrix([X], p, T)\n    rss_r_pair = ols_rss(y_target, X_design_R)\n\n    # Unrestricted model: X ~ past(X) + past(Y)\n    X_design_U = get_lagged_matrix([X, Y], p, T)\n    rss_u_pair = ols_rss(y_target, X_design_U)\n    \n    d_pair = p\n    k_u_pair = 1 + 2 * p\n    p_val_pair, _ = f_test(rss_r_pair, rss_u_pair, d_pair, N, k_u_pair)\n    sig_pair = p_val_pair  alpha\n\n    # --- 2. Conditional GC (Y - X | Z) ---\n    # Restricted model: X ~ past(X) + past(Z)\n    X_design_R = get_lagged_matrix([X, Z], p, T)\n    rss_r_cond = ols_rss(y_target, X_design_R)\n\n    # Unrestricted model: X ~ past(X) + past(Y) + past(Z)\n    X_design_U = get_lagged_matrix([X, Y, Z], p, T)\n    rss_u_cond = ols_rss(y_target, X_design_U)\n\n    d_cond = p\n    k_u_cond = 1 + 3 * p\n    p_val_cond, _ = f_test(rss_r_cond, rss_u_cond, d_cond, N, k_u_cond)\n    sig_cond = p_val_cond  alpha\n\n    # --- 3. Partial GC (Y - X . Z) ---\n    # Stage 1: Get residuals\n    y_target_X_res = X[p:]\n    y_target_Y_res = Y[p:]\n    \n    X_design_res = get_lagged_matrix([Z], p, T)\n    \n    b_x, _, _, _ = np.linalg.lstsq(X_design_res, y_target_X_res, rcond=None)\n    r_X = y_target_X_res - X_design_res @ b_x\n    \n    b_y, _, _, _ = np.linalg.lstsq(X_design_res, y_target_Y_res, rcond=None)\n    r_Y = y_target_Y_res - X_design_res @ b_y\n\n    # Stage 2: GC on residuals\n    T_res = len(r_X)\n    N_res = T_res - p\n    \n    y_target_res = r_X[p:]\n\n    # Restricted model: r_X ~ past(r_X)\n    X_design_R = get_lagged_matrix([r_X], p, T_res)\n    rss_r_part = ols_rss(y_target_res, X_design_R)\n\n    # Unrestricted model: r_X ~ past(r_X) + past(r_Y)\n    X_design_U = get_lagged_matrix([r_X, r_Y], p, T_res)\n    rss_u_part = ols_rss(y_target_res, X_design_U)\n    \n    d_part = p\n    k_u_part = 1 + 2 * p\n    p_val_part, _ = f_test(rss_r_part, rss_u_part, d_part, N_res, k_u_part)\n    sig_part = p_val_part  alpha\n    \n    return sig_pair, sig_cond, sig_part\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}