{
    "hands_on_practices": [
        {
            "introduction": "本练习是理解贝叶斯感知模型的基石。它阐明了一个核心科学原理：我们对世界的感知并非对感官输入的被动记录，而是将这些输入（即“似然”）与我们已有的信念（即“先验”）相结合的主动推断过程。通过亲手推导后验均值如何被先验所“拉动”，你将从第一性原理出发，定量地理解感知偏差和错觉的形成机制，并体会到“通过分析进行合成”这一思想的精髓。",
            "id": "3984161",
            "problem": "考虑一个在感知推断中通过合成进行分析的实例，在生成模型中，潜变量 $z$ 代表一个被感知的物理属性，观测值 $x$ 是 $z$ 的一个带噪声的测量。该生成模型由一个高斯先验 $z \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ 和一个高斯似然 $x \\mid z \\sim \\mathcal{N}(z,\\sigma_x^2)$ 指定。在这里，$\\mu_0$ 编码了一个有偏的先验期望，$\\sigma_0^2$ 编码了先验不确定性，而 $\\sigma_x^2$ 编码了感知不确定性。观察者执行贝叶斯推断以获得后验概率 $p(z \\mid x)$，并使用后验均值 $\\mathbb{E}[z \\mid x]$ 作为 $z$ 的估计值。\n\n仅从贝叶斯规则的定义和高斯概率密度函数的标准形式出发，推导后验分布 $p(z \\mid x)$ 并求出其均值 $\\mathbb{E}[z \\mid x]$。将错觉强度 $S(x)$ 定义为贝叶斯后验均值与最大似然估计之间的偏差，即 $S(x) = \\mathbb{E}[z \\mid x] - x$。请提供一个关于 $\\mu_0$、$\\sigma_0^2$、$\\sigma_x^2$ 和 $x$ 的 $S(x)$ 的单一闭式解析表达式。不需要数值近似，也无需报告单位。你的最终答案必须是单一的解析表达式。",
            "solution": "该问题是有效的，因为它在科学上基于贝叶斯统计，这是计算神经科学的核心组成部分。该问题是适定的，提供了所有必要信息且无矛盾，从而可以推导出唯一且有意义的解。该问题使用精确的数学语言进行客观陈述。\n\n我们首先应用贝叶斯规则来求后验分布 $p(z \\mid x)$。贝叶斯规则表述为：\n$$\np(z \\mid x) = \\frac{p(x \\mid z) p(z)}{p(x)}\n$$\n分母中的项 $p(x)$ 是边际似然，它作为一个归一化常数，不依赖于潜变量 $z$。因此，我们可以将后验表示为与似然和先验的乘积成正比：\n$$\np(z \\mid x) \\propto p(x \\mid z) p(z)\n$$\n问题指定了一个高斯先验，$z \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$，和一个高斯似然，$x \\mid z \\sim \\mathcal{N}(z, \\sigma_x^2)$。它们各自的概率密度函数 (PDF) 为：\n$$\np(z) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right)\n$$\n$$\np(x \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\n将这两个 PDF 相乘，并忽略常数系数（因为我们处理的是正比关系），我们得到：\n$$\np(z \\mid x) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right) \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\n$$\np(z \\mid x) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\n为了确定后验分布的形式，我们分析指数的参数。令该参数表示为 $E$。\n$$\nE = -\\frac{1}{2} \\left[ \\frac{(z - \\mu_0)^2}{\\sigma_0^2} + \\frac{(z - x)^2}{\\sigma_x^2} \\right]\n$$\n我们展开平方项：\n$$\nE = -\\frac{1}{2} \\left[ \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\sigma_0^2} + \\frac{z^2 - 2zx + x^2}{\\sigma_x^2} \\right]\n$$\n现在，我们根据 $z$ 的幂次对各项进行分组：\n$$\nE = -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2} \\right) - 2z \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2} \\right) + \\left( \\frac{\\mu_0^2}{\\sigma_0^2} + \\frac{x^2}{\\sigma_x^2} \\right) \\right]\n$$\n$p(z \\mid x)$ 的表达式是一个关于 $z$ 的二次函数的指数。这正是一个高斯分布的形式。对于一个均值为 $\\mu_{post}$、方差为 $\\sigma_{post}^2$ 的变量 $z$，其通用高斯 PDF 的形式为：\n$$\np(z) \\propto \\exp\\left( -\\frac{(z - \\mu_{post})^2}{2\\sigma_{post}^2} \\right) = \\exp\\left( -\\frac{z^2 - 2z\\mu_{post} + \\mu_{post}^2}{2\\sigma_{post}^2} \\right)\n$$\n$$\np(z) \\propto \\exp\\left( -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_{post}^2} \\right) - 2z \\left( \\frac{\\mu_{post}}{\\sigma_{post}^2} \\right) + \\frac{\\mu_{post}^2}{\\sigma_{post}^2} \\right] \\right)\n$$\n通过比较我们推导出的 $E$ 的表达式与标准形式中 $z^2$ 和 $z$ 项的系数，我们可以确定后验方差 $\\sigma_{post}^2$ 和后验均值 $\\mu_{post}$。\n令 $z^2$ 的系数相等：\n$$\n\\frac{1}{\\sigma_{post}^2} = \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2} = \\frac{\\sigma_x^2 + \\sigma_0^2}{\\sigma_0^2 \\sigma_x^2} \\implies \\sigma_{post}^2 = \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n令线性 $z$ 项的系数相等：\n$$\n\\frac{\\mu_{post}}{\\sigma_{post}^2} = \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2}\n$$\n现在我们求解后验均值 $\\mu_{post}$：\n$$\n\\mu_{post} = \\sigma_{post}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2} \\right) = \\left( \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2} \\right) \\left( \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 \\sigma_x^2} \\right)\n$$\n$$\n\\mu_{post} = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n因此，后验分布 $p(z \\mid x)$ 是一个均值为 $\\mu_{post}$、方差为 $\\sigma_{post}^2$ 的高斯分布。观察者对 $z$ 的估计是后验均值 $\\mathbb{E}[z \\mid x]$，也就是 $\\mu_{post}$。\n$$\n\\mathbb{E}[z \\mid x] = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n问题将错觉强度 $S(x)$ 定义为贝叶斯后验均值与最大似然估计 (MLE) 之间的偏差。似然函数是 $p(x|z) = \\mathcal{N}(x; z, \\sigma_x^2)$。$z$ 的最大似然估计是使该函数最大化的值，即 $z_{MLE} = x$。问题陈述通过定义 $S(x) = \\mathbb{E}[z \\mid x] - x$ 正确地指出了这一点。\n现在我们可以推导 $S(x)$ 的表达式：\n$$\nS(x) = \\mathbb{E}[z \\mid x] - x = \\left( \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2} \\right) - x\n$$\n为了简化，我们将所有项通分：\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x(\\sigma_0^2 + \\sigma_x^2)}{\\sigma_0^2 + \\sigma_x^2}\n$$\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x \\sigma_0^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n包含 $x \\sigma_0^2$ 的项相互抵消：\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n在分子中提出公因子 $\\sigma_x^2$ 得到最终的闭式表达式：\n$$\nS(x) = \\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$",
            "answer": "$$\n\\boxed{\\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}}\n$$"
        },
        {
            "introduction": "在掌握了基本的贝叶斯整合原理后，我们将探索更为复杂和接近大脑真实结构的层级模型。大脑皮层被认为是以层级方式组织起来的，其中高层级的抽象概念逐级生成低层级的具体表征，本练习通过一个三层线性高斯模型，让你亲身体验在这样的层级结构中，信息是如何流动的。 通过计算每个层级变量的后验分布，你将深入理解感官证据如何自下而上传播，以及先验信念如何自上而下地施加影响，并熟练运用精度矩阵这一强大工具来分析高斯模型链。",
            "id": "3984111",
            "problem": "考虑一个具有单一标量观测值的三层潜在层级结构的通过分析进行合成线性高斯生成模型。设潜在变量为 $z_3$、$z_2$ 和 $z_1$，其生成结构如下：\n- 顶层潜在变量具有零均值高斯先验：$z_3 \\sim \\mathcal{N}(0, \\tau_3^{-1})$，其中 $\\tau_3 > 0$ 是先验精度。\n- 中间层条件分布为高斯分布：$z_2 \\mid z_3 \\sim \\mathcal{N}(z_3, \\tau_2^{-1})$，其中 $\\tau_2 > 0$ 是条件精度。\n- 底层条件分布为高斯分布：$z_1 \\mid z_2 \\sim \\mathcal{N}(z_2, \\tau_1^{-1})$，其中 $\\tau_1 > 0$ 是条件精度。\n- 观测模型为高斯分布：$x \\mid z_1 \\sim \\mathcal{N}(z_1, \\gamma^{-1})$，其中 $\\gamma > 0$ 是观测精度， $x \\in \\mathbb{R}$ 是观测值。\n\n仅使用以下基本事实：Bayes 法则将高斯因子的乘积转化为高斯后验，以及多元高斯分布可以写成带有精度矩阵和位移向量的标准（信息）形式，推导给定 $x$ 时 $(z_1, z_2, z_3)$ 的联合后验分布，并由此计算 $\\ell \\in \\{1, 2, 3\\}$ 的边际后验均值 $\\mu_\\ell = \\mathbb{E}[z_\\ell \\mid x]$ 和边际后验精度 $\\tau_\\ell^{\\text{post}} = \\left(\\operatorname{Var}[z_\\ell \\mid x]\\right)^{-1}$。请用 $x$、$\\tau_1$、$\\tau_2$、$\\tau_3$ 和 $\\gamma$ 将你的答案表示为精确的符号表达式。\n\n你的最终答案必须是一个单行矩阵，按 $\\mu_1, \\mu_2, \\mu_3, \\tau_1^{\\text{post}}, \\tau_2^{\\text{post}}, \\tau_3^{\\text{post}}$ 的顺序排列。不需要数值近似或四舍五入，也不涉及单位。",
            "solution": "用户提供了一个有效的问题陈述。它在贝叶斯统计和层级建模方面有科学依据，问题是良定的，具有唯一且有意义的解，并且陈述客观。所有必要信息都已提供，没有矛盾或含糊之处。我现在开始解答。\n\n该问题要求解一个三层线性高斯生成模型中潜在变量 $z_1$、$z_2$ 和 $z_3$ 的边际后验均值和精度。该模型由以下分布定义：\n$p(z_3) = \\mathcal{N}(z_3 \\mid 0, \\tau_3^{-1})$\n$p(z_2 \\mid z_3) = \\mathcal{N}(z_2 \\mid z_3, \\tau_2^{-1})$\n$p(z_1 \\mid z_2) = \\mathcal{N}(z_1 \\mid z_2, \\tau_1^{-1})$\n$p(x \\mid z_1) = \\mathcal{N}(x \\mid z_1, \\gamma^{-1})$\n\n所有变量的联合分布由乘积 $p(x, z_1, z_2, z_3) = p(x \\mid z_1) p(z_1 \\mid z_2) p(z_2 \\mid z_3) p(z_3)$ 给出。\n根据 Bayes 法则，给定观测值 $x$ 时，潜在变量 $\\mathbf{z} = (z_1, z_2, z_3)^T$ 的联合后验分布与联合分布成正比：\n$p(\\mathbf{z} \\mid x) \\propto p(x, \\mathbf{z})$\n\n由于每个因子都是高斯分布，它们的乘积也是高斯分布（或者更准确地说，其密度与一个高斯函数成正比）。我们将处理对数后验，它将是变量 $z_1, z_2, z_3$ 的一个二次函数。高斯密度 $\\mathcal{N}(y \\mid \\mu, \\tau^{-1})$ 的对数，在忽略一个加性常数的情况下，为 $-\\frac{\\tau}{2}(y-\\mu)^2$。\n\n对数后验与各个概率密度的对数之和成正比：\n$$ \\ln p(z_1, z_2, z_3 \\mid x) = -\\frac{\\gamma}{2}(z_1 - x)^2 - \\frac{\\tau_1}{2}(z_1 - z_2)^2 - \\frac{\\tau_2}{2}(z_2 - z_3)^2 - \\frac{\\tau_3}{2}(z_3 - 0)^2 + C $$\n其中 $C$ 是一个不依赖于 $z_1, z_2, z_3$ 的常数。\n\n一个多元高斯后验分布 $p(\\mathbf{z} \\mid x) = \\mathcal{N}(\\mathbf{z} \\mid \\boldsymbol{\\mu}_{\\text{post}}, \\mathbf{\\Lambda}_{\\text{post}}^{-1})$ 可以用其标准或信息形式写出，其对数密度为：\n$$ \\ln p(\\mathbf{z} \\mid x) = -\\frac{1}{2} \\mathbf{z}^T \\mathbf{\\Lambda}_{\\text{post}} \\mathbf{z} + \\mathbf{h}_{\\text{post}}^T \\mathbf{z} + C' $$\n在此，$\\mathbf{\\Lambda}_{\\text{post}}$ 是后验精度矩阵，$\\mathbf{h}_{\\text{post}}$ 是势向量或位移向量。后验均值和协方差分别由 $\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1} \\mathbf{h}_{\\text{post}}$ 和 $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$ 给出。\n\n为了找到 $\\mathbf{\\Lambda}_{\\text{post}}$ 和 $\\mathbf{h}_{\\text{post}}$，我们展开对数后验的表达式，并收集关于 $\\mathbf{z}$ 的二次项和一次项：\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\left[ \\gamma(z_1^2 - 2z_1x + x^2) + \\tau_1(z_1^2 - 2z_1z_2 + z_2^2) + \\tau_2(z_2^2 - 2z_2z_3 + z_3^2) + \\tau_3z_3^2 \\right] $$\n按 $z_i z_j$ 和 $z_i$ 对各项进行分组：\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\left[ (\\gamma + \\tau_1)z_1^2 + (\\tau_1 + \\tau_2)z_2^2 + (\\tau_2 + \\tau_3)z_3^2 - 2\\tau_1z_1z_2 - 2\\tau_2z_2z_3 \\right] + \\gamma x z_1 $$\n将此与标准形式 $-\\frac{1}{2}\\mathbf{z}^T \\mathbf{\\Lambda}_{\\text{post}} \\mathbf{z} + \\mathbf{h}_{\\text{post}}^T\\mathbf{z}$ 相比较，我们可以识别出后验精度矩阵 $\\mathbf{\\Lambda}_{\\text{post}}$ 和位移向量 $\\mathbf{h}_{\\text{post}}$：\n$$ \\mathbf{\\Lambda}_{\\text{post}} = \\begin{pmatrix} \\gamma + \\tau_1 & -\\tau_1 & 0 \\\\ -\\tau_1 & \\tau_1 + \\tau_2 & -\\tau_2 \\\\ 0 & -\\tau_2 & \\tau_2 + \\tau_3 \\end{pmatrix} $$\n$$ \\mathbf{h}_{\\text{post}} = \\begin{pmatrix} \\gamma x \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n后验均值向量为 $\\boldsymbol{\\mu}_{\\text{post}} = (\\mu_1, \\mu_2, \\mu_3)^T = \\mathbf{\\Lambda}_{\\text{post}}^{-1} \\mathbf{h}_{\\text{post}}$。后验协方差矩阵为 $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$。边际后验方差是该矩阵的对角元素，$\\operatorname{Var}[z_\\ell \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{\\ell\\ell}$。边际后验精度为 $\\tau_\\ell^{\\text{post}} = \\left((\\mathbf{\\Sigma}_{\\text{post}})_{\\ell\\ell}\\right)^{-1}$。\n\n为了计算 $\\mathbf{\\Lambda}_{\\text{post}}$ 的逆矩阵，我们首先求其行列式，记为 $D$：\n$$ D = \\det(\\mathbf{\\Lambda}_{\\text{post}}) = (\\gamma + \\tau_1) \\left( (\\tau_1 + \\tau_2)(\\tau_2 + \\tau_3) - (-\\tau_2)^2 \\right) - (-\\tau_1) \\left( (-\\tau_1)(\\tau_2 + \\tau_3) - 0 \\right) $$\n$$ D = (\\gamma + \\tau_1)(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2(\\tau_2 + \\tau_3) $$\n$$ D = \\gamma(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) + \\tau_1(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2\\tau_2 - \\tau_1^2\\tau_3 $$\n$$ D = \\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3 $$\n\n逆矩阵为 $\\mathbf{\\Lambda}_{\\text{post}}^{-1} = \\frac{1}{D} \\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}})$，其中 $\\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}})$ 是伴随矩阵。由于 $\\mathbf{\\Lambda}_{\\text{post}}$ 是对称的，其伴随矩阵就是代数余子式矩阵。我们来计算必要的代数余子式 $C_{ij}$：\n$C_{11} = (\\tau_1+\\tau_2)(\\tau_2+\\tau_3) - \\tau_2^2 = \\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3$\n$C_{21} = -(-\\tau_1(\\tau_2+\\tau_3) - 0) = \\tau_1(\\tau_2+\\tau_3)$\n$C_{31} = (-\\tau_1)(-\\tau_2) - 0 = \\tau_1\\tau_2$\n$C_{22} = (\\gamma+\\tau_1)(\\tau_2+\\tau_3)$\n$C_{33} = (\\gamma+\\tau_1)(\\tau_1+\\tau_2) - (-\\tau_1)^2 = \\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2$\n\n现在我们计算后验均值 $\\boldsymbol{\\mu}_{\\text{post}} = \\frac{1}{D} \\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}}) \\mathbf{h}_{\\text{post}}$：\n$$ \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} = \\frac{1}{D} \\begin{pmatrix} C_{11} & C_{21} & C_{31} \\\\ C_{12} & C_{22} & C_{32} \\\\ C_{13} & C_{23} & C_{33} \\end{pmatrix} \\begin{pmatrix} \\gamma x \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{\\gamma x}{D} \\begin{pmatrix} C_{11} \\\\ C_{21} \\\\ C_{31} \\end{pmatrix} $$\n代入代数余子式和 $D$：\n$$ \\mu_1 = \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n$$ \\mu_2 = \\frac{\\gamma \\tau_1 (\\tau_2 + \\tau_3) x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n$$ \\mu_3 = \\frac{\\gamma \\tau_1 \\tau_2 x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n为了最终答案的一致性，我们可以重新排列 $\\mu_1$ 分子中的项，使其与分母中的顺序相匹配：$\\mu_1 = \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}$。\n\n接下来，我们求边际后验精度。边际方差是 $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$ 的对角元素：\n$$ \\operatorname{Var}[z_1 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{11} = \\frac{C_{11}}{D} = \\frac{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3}{D} $$\n$$ \\operatorname{Var}[z_2 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{22} = \\frac{C_{22}}{D} = \\frac{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)}{D} $$\n$$ \\operatorname{Var}[z_3 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{33} = \\frac{C_{33}}{D} = \\frac{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2}{D} $$\n\n边际后验精度是这些方差的倒数：\n$$ \\tau_1^{\\text{post}} = \\frac{D}{C_{11}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} = \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} $$\n后一种简化形式更优雅且更具启发性，它证实了后验精度是似然精度（$\\gamma$）与 $z_1$ 的边际先验精度之和。\n$$ \\tau_2^{\\text{post}} = \\frac{D}{C_{22}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)} $$\n$$ \\tau_3^{\\text{post}} = \\frac{D}{C_{33}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2} $$\n这些表达式不能像 $\\tau_1^{\\text{post}}$ 那样被简洁地化简，因此将以其分数形式呈现。我们代入 $D$ 的表达式，并像之前一样为保持一致性而重新排序。例如，$\\tau_2^{\\text{post}} = \\frac{\\gamma(\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)}$。\n\n最终结果按要求整理成一个单行矩阵。",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 (\\tau_2 + \\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 \\tau_2 x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{\\gamma(\\tau_1 + \\tau_2) + \\tau_1\\tau_2} \\end{pmatrix}} $$"
        },
        {
            "introduction": "前面的练习侧重于推断的静态数学描述，而本练习将我们带入动态的、算法层面的视角。预测编码理论提出，大脑通过一个迭代过程实现“通过分析进行合成”：高层级的神经回路生成预测，并与来自低层级的感官信号进行比较，两者之间的不匹配（即“预测误差”）会驱动内部模型的更新。这个编程实践任务让你能够具体实现这一理论。 通过调整感官输入的精度，你将能直接观察到它如何扮演“预测误差增益”的角色，并量化其对推断收敛速度的影响，从而搭建起从抽象数学理论到具体计算模拟的桥梁。",
            "id": "3984145",
            "problem": "您的任务是在生成模型和“通过分析进行合成”的框架下，设计并执行一个计算实验，以展示在预测编码中，增加感觉噪声的精度（表示为 $\\Pi_x$）如何调节误差单元的增益和收敛速度。考虑一个线性高斯生成模型，其中潜状态向量 $z \\in \\mathbb{R}^2$ 通过一个合成映射 $C \\in \\mathbb{R}^{3 \\times 2}$ 和加性感觉噪声，产生一个观测向量 $x \\in \\mathbb{R}^3$。该生成模型定义为\n$$\nx = C z + \\varepsilon_x,\n$$\n其中感觉噪声从一个零均值、协方差矩阵为 $\\Sigma_x \\in \\mathbb{R}^{3 \\times 3}$ 的高斯分布中抽取，其精度为 $\\Pi_x = \\Sigma_x^{-1}$。$z$ 的先验是均值为 $\\mu_z$、精度为 $\\Pi_z$ 的高斯分布。\n\n在预测编码中，通过最小化一个“合成-分析”一致性目标函数来推断潜变量 $z$，该目标函数惩罚预测误差和与先验的偏差。对于单个观测 $x$，一个合适的目标函数是\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2.\n$$\n您必须推导最小化 $J(z)$ 的 $z$ 的梯度下降更新规则，并实现由此产生的迭代式预测编码动力学。设误差单元定义为 $e_x(t) = x - C z(t)$，其中迭代指数 $t \\in \\{0,1,2,\\ldots\\}$。施加于误差单元的增益体现在精度加权误差 $\\tilde{e}_x(t) = \\Pi_x\\, e_x(t)$ 中。通过以下比率量化初始化时的瞬时增益\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2}.\n$$\n通过梯度 $\\|\\nabla J(z(t))\\|_2$ 的欧几里得范数下降到指定容差 $\\varepsilon$ 以下所需的迭代次数来量化收敛速度，并受最大迭代预算 $T_{\\max}$ 的限制。\n\n请使用以下在科学上合理且自洽的实验参数，并在所有测试案例中保持一致：\n- 合成矩阵 $C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$。\n- 先验均值 $\\mu_z = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 和先验精度 $\\Pi_z = 1$。\n- 真实潜变量 $z_{\\text{true}} = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$，确定性感官噪声向量 $\\varepsilon_x = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$，以及观测值 $x = C z_{\\text{true}} + \\varepsilon_x$。\n- 初始潜变量 $z(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$。\n- 固定学习率 $\\eta = 0.005$，容差 $\\varepsilon = 10^{-6}$，以及最大迭代次数 $T_{\\max} = 10000$。\n\n从上述定义出发，推导降低 $J(z)$ 的 $z$ 的梯度下降更新规则，实现该动力学，并对以下感觉精度 $\\Pi_x$ 的测试集进行实验：\n1. $\\Pi_x = 0.01$ (极低精度，感觉似然几乎不提供信息)。\n2. $\\Pi_x = 0.1$ (低精度)。\n3. $\\Pi_x = 1.0$ (中等精度)。\n4. $\\Pi_x = 10.0$ (高精度)。\n5. $\\Pi_x = 100.0$ (极高精度)。\n\n对于每个测试案例，计算并返回以下三个量：\n- 如上定义的增益 $G(\\Pi_x)$，一个浮点数。\n- 直到 $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$ 所需的迭代次数 $N(\\Pi_x)$，一个整数；如果在 $T_{\\max}$ 次迭代内未收敛，则设 $N(\\Pi_x) = T_{\\max}$。\n- 一个布尔值 $B(\\Pi_x)$，指示是否达到收敛，其中当终止时 $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$ 则 $B(\\Pi_x) = \\text{True}$，否则为 $\\text{False}$。\n\n您的程序应生成单行输出，包含一个方括号括起来的逗号分隔列表，其中每个测试案例的结果本身是一个形如 $[G(\\Pi_x),N(\\Pi_x),B(\\Pi_x)]$ 的逗号分隔列表。例如，对于三个案例，您的输出将如下所示\n$$\n\\big[ [G_1,N_1,B_1],[G_2,N_2,B_3],[G_3,N_3,B_3] \\big],\n$$\n但其值对应于指定的五个测试精度。不涉及任何物理单位或角度；所有量都是无量纲的。",
            "solution": "问题陈述已经过严格验证，被认为是科学上合理、适定且自洽的。它提出了一个线性高斯生成模型中推断的标准公式，这是计算神经科学和机器学习中的一个常见范式。所有参数都有明确定义，任务是一个清晰的计算实验。\n\n问题的核心是通过最小化一个目标函数 $J(z)$，从观测 $x \\in \\mathbb{R}^3$ 中推断一个潜状态向量 $z \\in \\mathbb{R}^2$。该函数表示在给定 $x$ 条件下 $z$ 的负对数后验概率，在预测编码的背景下也称为自由能泛函。目标函数由下式给出：\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2\n$$\n在这里，第一项是负对数似然，它惩罚由标量感觉精度 $\\Pi_x$ 加权的预测误差 $x - C z$。第二项是负对数先验，它惩罚由标量先验精度 $\\Pi_z$ 加权的潜状态 $z$ 与其先验均值 $\\mu_z$ 的偏差。该模型假设各向同性的高斯噪声，因此使用标量精度。\n\n为了实现预测编码动力学，我们使用梯度下降来找到最小化 $J(z)$ 的 $z$ 值。更新规则是 $z(t+1) = z(t) - \\eta \\nabla J(z(t))$，其中 $\\eta$ 是学习率。我们必须首先推导 $J(z)$ 相对于 $z$ 的梯度。\n\n我们使用向量转置表示法展开 $J(z)$ 中的各项：\n$$\nJ(z) = \\frac{1}{2}\\Pi_x (x - Cz)^T(x - Cz) + \\frac{1}{2}\\Pi_z(z - \\mu_z)^T(z - \\mu_z)\n$$\n我们分别求每一项的梯度。对于第一项，我们使用向量微积分的链式法则，注意到 $\\nabla_v(v^T v) = 2v$。\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_x (x - Cz)^T(x - Cz)\\right) = \\frac{1}{2}\\Pi_x \\left( 2 C^T(Cz - x) \\right) = \\Pi_x C^T(Cz - x)\n$$\n对于第二项：\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_z(z - \\mu_z)^T(z - \\mu_z)\\right) = \\frac{1}{2}\\Pi_z \\left( 2 (z - \\mu_z) \\right) = \\Pi_z(z - \\mu_z)\n$$\n将它们合并，总梯度为：\n$$\n\\nabla J(z) = \\Pi_x C^T(Cz - x) + \\Pi_z(z - \\mu_z)\n$$\n问题将预测误差定义为 $e_x(t) = x - C z(t)$。将其代入梯度表达式，我们得到：\n$$\n\\nabla J(z(t)) = -\\Pi_x C^T e_x(t) + \\Pi_z(z(t) - \\mu_z)\n$$\n因此，潜状态 $z$ 的梯度下降更新规则为：\n$$\nz(t+1) = z(t) - \\eta \\left( \\Pi_x C^T(C z(t) - x) + \\Pi_z(z(t) - \\mu_z) \\right)\n$$\n这个迭代规则描述了推断过程的动力学，其中潜状态被更新以减小目标函数 $J(z)$。\n\n接下来，我们计算初始化时的瞬时增益 $G(\\Pi_x)$。它被定义为在时间 $t=0$ 时，精度加权误差的范数与原始误差的范数之比：\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2} = \\frac{\\|\\Pi_x\\, e_x(0)\\|_2}{\\|e_x(0)\\|_2}\n$$\n由于 $\\Pi_x$ 是一个标量，我们可以将其从范数中提出：\n$$\nG(\\Pi_x) = \\frac{|\\Pi_x| \\|e_x(0)\\|_2}{\\|e_x(0)\\|_2} = |\\Pi_x|\n$$\n考虑到所有指定的 $\\Pi_x$ 测试值都是正数，增益就是 $G(\\Pi_x) = \\Pi_x$。这表明感觉精度直接缩放了感觉预测误差对系统动力学的贡献。\n\n计算实验按以下步骤进行。首先，我们使用提供的参数计算恒定的观测向量 $x = C z_{\\text{true}} + \\varepsilon_x$。然后，对于测试集中的每个 $\\Pi_x$ 值，我们执行以下步骤：\n1. 计算增益 $G(\\Pi_x) = \\Pi_x$。\n2. 初始化潜状态 $z(0) = \\begin{bmatrix} 0.0, 0.0 \\end{bmatrix}^T$。\n3. 从 $0$ 到 $T_{\\max}$ 迭代 $N$ 步：\n    a. 使用当前状态 $z(N)$ 计算梯度 $\\nabla J(z(N))$。\n    b. 计算梯度的欧几里得范数 $\\|\\nabla J(z(N))\\|_2$。\n    c. 如果范数小于或等于容差 $\\varepsilon=10^{-6}$，则过程收敛。我们记录迭代次数 $N(\\Pi_x) = N$ 和收敛标志 $B(\\Pi_x) = \\text{True}$，然后终止当前 $\\Pi_x$ 的循环。\n    d. 如果未满足收敛准则且 $N < T_{\\max}$，则使用梯度下降规则更新状态为 $z(N+1)$。\n4. 如果循环完成而未达到容差（即 $N=T_{\\max}$），我们设置 $N(\\Pi_x) = T_{\\max}$ 和 $B(\\Pi_x) = \\text{False}$。\n\n对所有指定的 $\\Pi_x$ 值重复此过程，并为每个案例收集结果—$[G(\\Pi_x), N(\\Pi_x), B(\\Pi_x)]$。预期结果将显示，随着 $\\Pi_x$ 的增加，收敛所需的迭代次数 $N(\\Pi_x)$ 会减少，这阐释了预测编码的一个基本原理：更高的感觉精度通过赋予预测误差更大的权重来加速推断，从而导致对内部模型更快的修正。",
            "answer": "[[0.01,10000,False],[0.1,9153,True],[1.0,1046,True],[10.0,137,True],[100.0,31,True]]"
        }
    ]
}