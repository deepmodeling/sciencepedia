{
    "hands_on_practices": [
        {
            "introduction": "本实践从分析-综合（analysis-by-synthesis）的一个基本实例入手，探讨大脑如何将先验知识与不确定的感官证据相结合。你将通过推导高斯先验和高斯似然的后验分布，来量化这种结合如何导致知觉偏差或“错觉”。这个练习不仅能让你掌握贝叶斯推断的核心数学技巧，还能让你直观地理解先验信念是如何塑造我们对世界的感知的。",
            "id": "3984161",
            "problem": "考虑一个在感知推断中通过合成进行分析的实例，其中生成模型中的一个潜在原因 $z$ 代表一个感知到的物理属性，而观测值 $x$ 是对 $z$ 的一个带噪声的测量。该生成模型由一个高斯先验 $z \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ 和一个高斯似然 $x \\mid z \\sim \\mathcal{N}(z,\\sigma_x^2)$ 指定。在此，$\\mu_0$ 表示一个有偏的先验期望，$\\sigma_0^2$ 表示先验不确定性，而 $\\sigma_x^2$ 表示感知不确定性。观察者执行贝叶斯推断以获得后验 $p(z \\mid x)$，并使用后验均值 $\\mathbb{E}[z \\mid x]$作为 $z$ 的估计值。仅从贝叶斯法则的定义和高斯概率密度函数的标准形式出发，推导后验分布 $p(z \\mid x)$ 并求出其均值 $\\mathbb{E}[z \\mid x]$。将错觉强度 $S(x)$ 定义为贝叶斯后验均值与最大似然估计之间的偏差，即 $S(x) = \\mathbb{E}[z \\mid x] - x$。请提供一个关于 $\\mu_0$、$\\sigma_0^2$、$\\sigma_x^2$ 和 $x$ 的 $S(x)$ 的单一闭式解析表达式。不需要数值近似，也不需要报告单位。你的最终答案必须是一个单一的解析表达式。",
            "solution": "该问题是有效的，因为它在科学上植根于贝叶斯统计，这是计算神经科学的一个核心组成部分。该问题是适定的，提供了所有必要信息且没有矛盾，从而可以推导出唯一的、有意义的解。该问题使用精确的数学语言进行了客观陈述。\n\n我们首先应用贝叶斯法则来求后验分布 $p(z \\mid x)$。贝叶斯法则表述如下：\n$$\np(z \\mid x) = \\frac{p(x \\mid z) p(z)}{p(x)}\n$$\n分母中的 $p(x)$ 是边缘似然，它作为一个归一化常数，不依赖于潜在原因 $z$。因此，我们可以将后验表示为似然与先验的乘积成正比：\n$$\np(z \\mid x) \\propto p(x \\mid z) p(z)\n$$\n问题指定了一个高斯先验 $z \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$ 和一个高斯似然 $x \\mid z \\sim \\mathcal{N}(z, \\sigma_x^2)$。它们各自的概率密度函数 (PDF) 如下：\n$$\np(z) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right)\n$$\n$$\np(x \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\n将这两个 PDF 相乘，并忽略常数系数（因为我们处理的是正比关系），我们得到：\n$$\np(z \\mid x) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} \\right) \\exp\\left( -\\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\n$$\np(z \\mid x) \\propto \\exp\\left( -\\frac{(z - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - z)^2}{2\\sigma_x^2} \\right)\n$$\n为了确定后验分布的形式，我们分析指数部分的参数。设该参数为 $E$。\n$$\nE = -\\frac{1}{2} \\left[ \\frac{(z - \\mu_0)^2}{\\sigma_0^2} + \\frac{(z - x)^2}{\\sigma_x^2} \\right]\n$$\n我们展开平方项：\n$$\nE = -\\frac{1}{2} \\left[ \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\sigma_0^2} + \\frac{z^2 - 2zx + x^2}{\\sigma_x^2} \\right]\n$$\n现在，我们根据 $z$ 的幂次对各项进行分组：\n$$\nE = -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2} \\right) - 2z \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2} \\right) + \\left( \\frac{\\mu_0^2}{\\sigma_0^2} + \\frac{x^2}{\\sigma_x^2} \\right) \\right]\n$$\n$p(z \\mid x)$ 的表达式是一个关于 $z$ 的二次函数的指数形式。这是一个高斯分布的形式。对于一个均值为 $\\mu_{post}$、方差为 $\\sigma_{post}^2$ 的变量 $z$，其通用高斯 PDF 具有以下形式：\n$$\np(z) \\propto \\exp\\left( -\\frac{(z - \\mu_{post})^2}{2\\sigma_{post}^2} \\right) = \\exp\\left( -\\frac{z^2 - 2z\\mu_{post} + \\mu_{post}^2}{2\\sigma_{post}^2} \\right)\n$$\n$$\np(z) \\propto \\exp\\left( -\\frac{1}{2} \\left[ z^2 \\left( \\frac{1}{\\sigma_{post}^2} \\right) - 2z \\left( \\frac{\\mu_{post}}{\\sigma_{post}^2} \\right) + \\frac{\\mu_{post}^2}{\\sigma_{post}^2} \\right] \\right)\n$$\n通过比较我们推导出的 $E$ 的表达式与标准形式中 $z^2$ 和 $z$ 项的系数，我们可以确定后验方差 $\\sigma_{post}^2$ 和后验均值 $\\mu_{post}$。\n令 $z^2$ 的系数相等：\n$$\n\\frac{1}{\\sigma_{post}^2} = \\frac{1}{\\sigma_0^2} + \\frac{1}{\\sigma_x^2} = \\frac{\\sigma_x^2 + \\sigma_0^2}{\\sigma_0^2 \\sigma_x^2} \\implies \\sigma_{post}^2 = \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n令线性项 $z$ 的系数相等：\n$$\n\\frac{\\mu_{post}}{\\sigma_{post}^2} = \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2}\n$$\n现在我们求解后验均值 $\\mu_{post}$：\n$$\n\\mu_{post} = \\sigma_{post}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{x}{\\sigma_x^2} \\right) = \\left( \\frac{\\sigma_0^2 \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2} \\right) \\left( \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 \\sigma_x^2} \\right)\n$$\n$$\n\\mu_{post} = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n因此，后验分布 $p(z \\mid x)$ 是一个均值为 $\\mu_{post}$、方差为 $\\sigma_{post}^2$ 的高斯分布。观察者对 $z$ 的估计是后验均值 $\\mathbb{E}[z \\mid x]$，即 $\\mu_{post}$。\n$$\n\\mathbb{E}[z \\mid x] = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n问题将错觉强度 $S(x)$ 定义为贝叶斯后验均值与最大似然估计 (MLE) 之间的偏差。似然函数为 $p(x|z) = \\mathcal{N}(x; z, \\sigma_x^2)$。$z$ 的最大似然估计是使该函数最大化的值，即 $z_{MLE} = x$。问题陈述通过定义 $S(x) = \\mathbb{E}[z \\mid x] - x$ 正确地指出了这一点。\n现在我们可以推导 $S(x)$ 的表达式：\n$$\nS(x) = \\mathbb{E}[z \\mid x] - x = \\left( \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2}{\\sigma_0^2 + \\sigma_x^2} \\right) - x\n$$\n为了简化，我们将所有项通分：\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x(\\sigma_0^2 + \\sigma_x^2)}{\\sigma_0^2 + \\sigma_x^2}\n$$\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 + x \\sigma_0^2 - x \\sigma_0^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n包含 $x \\sigma_0^2$ 的项相互抵消：\n$$\nS(x) = \\frac{\\mu_0 \\sigma_x^2 - x \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$\n在分子中提出公因式 $\\sigma_x^2$ 得到最终的闭式表达式：\n$$\nS(x) = \\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}\n$$",
            "answer": "$$\n\\boxed{\\frac{(\\mu_0 - x) \\sigma_x^2}{\\sigma_0^2 + \\sigma_x^2}}\n$$"
        },
        {
            "introduction": "在理解了单层贝叶斯推断后，我们将模型扩展到一个更符合大脑皮层结构的层次化生成模型中。本实践要求你处理一个三层线性高斯模型，并计算每一层级潜在变量的后验均值和精度。通过使用高斯分布的信息形式（或称典范形式），你将学会如何在一个更复杂的系统中追踪自上而下和自下而上的信息流，为理解预测编码等高级理论奠定数学基础。",
            "id": "3984111",
            "problem": "考虑一个通过分析合成的线性高斯生成模型，该模型具有三层潜在层级和单个标量观测值。设潜在变量为 $z_3$、$z_2$ 和 $z_1$，其生成结构如下：\n- 顶层潜在变量服从零均值高斯先验：$z_3 \\sim \\mathcal{N}(0, \\tau_3^{-1})$，其中 $\\tau_3 > 0$ 是先验精度。\n- 中层条件分布是高斯分布：$z_2 \\mid z_3 \\sim \\mathcal{N}(z_3, \\tau_2^{-1})$，其中 $\\tau_2 > 0$ 是条件精度。\n- 底层条件分布是高斯分布：$z_1 \\mid z_2 \\sim \\mathcal{N}(z_2, \\tau_1^{-1})$，其中 $\\tau_1 > 0$ 是条件精度。\n- 观测模型是高斯模型：$x \\mid z_1 \\sim \\mathcal{N}(z_1, \\gamma^{-1})$，其中 $\\gamma > 0$ 是观测精度， $x \\in \\mathbb{R}$ 是观测值。\n\n仅使用以下基本事实：贝叶斯法则将高斯因子的乘积转化为高斯后验，以及多元高斯分布可以用精度矩阵和位移向量写成规范（信息）形式，推导在给定 $x$ 的条件下 $(z_1, z_2, z_3)$ 的联合后验分布，并由此计算边际后验均值 $\\mu_\\ell = \\mathbb{E}[z_\\ell \\mid x]$ 和边际后验精度 $\\tau_\\ell^{\\text{post}} = \\left(\\operatorname{Var}[z_\\ell \\mid x]\\right)^{-1}$，其中 $\\ell \\in \\{1, 2, 3\\}$。请用包含 $x$、$\\tau_1$、$\\tau_2$、$\\tau_3$ 和 $\\gamma$ 的精确符号表达式表示您的答案。\n\n您的最终答案必须是一个单行矩阵，顺序为 $\\mu_1, \\mu_2, \\mu_3, \\tau_1^{\\text{post}}, \\tau_2^{\\text{post}}, \\tau_3^{\\text{post}}$。不需要数值近似或四舍五入，也不涉及单位。",
            "solution": "用户提供了一个有效的问题陈述。它在科学上基于贝叶斯统计和层级建模，问题阐述清晰，具有唯一且有意义的解，并且陈述客观。所有必要信息均已提供，没有矛盾或含糊之处。我现在开始解答。\n\n问题要求解一个三层线性高斯生成模型中潜在变量 $z_1$、$z_2$ 和 $z_3$ 的边际后验均值和精度。该模型由以下分布定义：\n$p(z_3) = \\mathcal{N}(z_3 \\mid 0, \\tau_3^{-1})$\n$p(z_2 \\mid z_3) = \\mathcal{N}(z_2 \\mid z_3, \\tau_2^{-1})$\n$p(z_1 \\mid z_2) = \\mathcal{N}(z_1 \\mid z_2, \\tau_1^{-1})$\n$p(x \\mid z_1) = \\mathcal{N}(x \\mid z_1, \\gamma^{-1})$\n\n所有变量的联合分布由乘积 $p(x, z_1, z_2, z_3) = p(x \\mid z_1) p(z_1 \\mid z_2) p(z_2 \\mid z_3) p(z_3)$ 给出。\n根据贝叶斯法则，在给定观测值 $x$ 的情况下，潜在变量 $\\mathbf{z} = (z_1, z_2, z_3)^T$ 的联合后验分布与联合分布成正比：\n$p(\\mathbf{z} \\mid x) \\propto p(x, \\mathbf{z})$\n\n由于每个因子都是高斯分布，它们的乘积也是一个高斯分布（或者更准确地说，其密度与一个高斯函数成正比）。我们将处理对数后验，它将是变量 $z_1, z_2, z_3$ 的二次函数。一个高斯密度 $\\mathcal{N}(y \\mid \\mu, \\tau^{-1})$ 的对数，在相差一个加性常数的情况下，为 $-\\frac{\\tau}{2}(y-\\mu)^2$。\n\n对数后验与各个概率密度对数的和成正比：\n$$ \\ln p(z_1, z_2, z_3 \\mid x) = -\\frac{\\gamma}{2}(z_1 - x)^2 - \\frac{\\tau_1}{2}(z_1 - z_2)^2 - \\frac{\\tau_2}{2}(z_2 - z_3)^2 - \\frac{\\tau_3}{2}(z_3 - 0)^2 + C $$\n其中 $C$ 是一个不依赖于 $z_1, z_2, z_3$ 的常数。\n\n一个多元高斯后验 $p(\\mathbf{z} \\mid x) = \\mathcal{N}(\\mathbf{z} \\mid \\boldsymbol{\\mu}_{\\text{post}}, \\mathbf{\\Lambda}_{\\text{post}}^{-1})$ 可以写成其规范形式或信息形式，其对数密度为：\n$$ \\ln p(\\mathbf{z} \\mid x) = -\\frac{1}{2} \\mathbf{z}^T \\mathbf{\\Lambda}_{\\text{post}} \\mathbf{z} + \\mathbf{h}_{\\text{post}}^T \\mathbf{z} + C' $$\n这里，$\\mathbf{\\Lambda}_{\\text{post}}$ 是后验精度矩阵，$\\mathbf{h}_{\\text{post}}$ 是势向量或位移向量。后验均值和协方差分别由 $\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1} \\mathbf{h}_{\\text{post}}$ 和 $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$ 给出。\n\n为了找到 $\\mathbf{\\Lambda}_{\\text{post}}$ 和 $\\mathbf{h}_{\\text{post}}$，我们展开对数后验的表达式，并收集 $\\mathbf{z}$ 的二次项和线性项：\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\left[ \\gamma(z_1^2 - 2z_1x + x^2) + \\tau_1(z_1^2 - 2z_1z_2 + z_2^2) + \\tau_2(z_2^2 - 2z_2z_3 + z_3^2) + \\tau_3z_3^2 \\right] $$\n按 $z_i z_j$ 和 $z_i$ 对各项进行分组：\n$$ \\ln p(\\mathbf{z} \\mid x) \\propto -\\frac{1}{2} \\left[ (\\gamma + \\tau_1)z_1^2 + (\\tau_1 + \\tau_2)z_2^2 + (\\tau_2 + \\tau_3)z_3^2 - 2\\tau_1z_1z_2 - 2\\tau_2z_2z_3 \\right] + \\gamma x z_1 $$\n将其与规范形式 $-\\frac{1}{2}\\mathbf{z}^T \\mathbf{\\Lambda}_{\\text{post}} \\mathbf{z} + \\mathbf{h}_{\\text{post}}^T\\mathbf{z}$ 进行比较，我们可以确定后验精度矩阵 $\\mathbf{\\Lambda}_{\\text{post}}$ 和位移向量 $\\mathbf{h}_{\\text{post}}$：\n$$ \\mathbf{\\Lambda}_{\\text{post}} = \\begin{pmatrix} \\gamma + \\tau_1 & -\\tau_1 & 0 \\\\ -\\tau_1 & \\tau_1 + \\tau_2 & -\\tau_2 \\\\ 0 & -\\tau_2 & \\tau_2 + \\tau_3 \\end{pmatrix} $$\n$$ \\mathbf{h}_{\\text{post}} = \\begin{pmatrix} \\gamma x \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n后验均值向量为 $\\boldsymbol{\\mu}_{\\text{post}} = (\\mu_1, \\mu_2, \\mu_3)^T = \\mathbf{\\Lambda}_{\\text{post}}^{-1} \\mathbf{h}_{\\text{post}}$。后验协方差矩阵为 $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$。边际后验方差是该矩阵的对角元素，$\\operatorname{Var}[z_\\ell \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{\\ell\\ell}$。边际后验精度为 $\\tau_\\ell^{\\text{post}} = \\left((\\mathbf{\\Sigma}_{\\text{post}})_{\\ell\\ell}\\right)^{-1}$。\n\n为了计算 $\\mathbf{\\Lambda}_{\\text{post}}$ 的逆，我们首先求其行列式，记为 $D$：\n$$ D = \\det(\\mathbf{\\Lambda}_{\\text{post}}) = (\\gamma + \\tau_1) \\left( (\\tau_1 + \\tau_2)(\\tau_2 + \\tau_3) - (-\\tau_2)^2 \\right) - (-\\tau_1) \\left( (-\\tau_1)(\\tau_2 + \\tau_3) - 0 \\right) $$\n$$ D = (\\gamma + \\tau_1)(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2(\\tau_2 + \\tau_3) $$\n$$ D = \\gamma(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) + \\tau_1(\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) - \\tau_1^2\\tau_2 - \\tau_1^2\\tau_3 $$\n$$ D = \\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3 $$\n\n逆矩阵为 $\\mathbf{\\Lambda}_{\\text{post}}^{-1} = \\frac{1}{D} \\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}})$，其中 $\\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}})$ 是伴随矩阵。由于 $\\mathbf{\\Lambda}_{\\text{post}}$ 是对称的，其伴随矩阵是代数余子式矩阵。我们来计算必要的代数余子式 $C_{ij}$：\n$C_{11} = (\\tau_1+\\tau_2)(\\tau_2+\\tau_3) - \\tau_2^2 = \\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3$\n$C_{21} = -(-\\tau_1(\\tau_2+\\tau_3) - 0) = \\tau_1(\\tau_2+\\tau_3)$\n$C_{31} = (-\\tau_1)(-\\tau_2) - 0 = \\tau_1\\tau_2$\n$C_{22} = (\\gamma+\\tau_1)(\\tau_2+\\tau_3)$\n$C_{33} = (\\gamma+\\tau_1)(\\tau_1+\\tau_2) - (-\\tau_1)^2 = \\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2$\n\n现在我们计算后验均值 $\\boldsymbol{\\mu}_{\\text{post}} = \\frac{1}{D} \\operatorname{adj}(\\mathbf{\\Lambda}_{\\text{post}}) \\mathbf{h}_{\\text{post}}$：\n$$ \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{pmatrix} = \\frac{1}{D} \\begin{pmatrix} C_{11} & C_{21} & C_{31} \\\\ C_{12} & C_{22} & C_{32} \\\\ C_{13} & C_{23} & C_{33} \\end{pmatrix} \\begin{pmatrix} \\gamma x \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{\\gamma x}{D} \\begin{pmatrix} C_{11} \\\\ C_{21} \\\\ C_{31} \\end{pmatrix} $$\n代入代数余子式和 $D$：\n$$ \\mu_1 = \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3) x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n$$ \\mu_2 = \\frac{\\gamma \\tau_1 (\\tau_2 + \\tau_3) x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n$$ \\mu_3 = \\frac{\\gamma \\tau_1 \\tau_2 x}{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3} $$\n为了最终答案的一致性，我们可以重新排列 $\\mu_1$ 分子中的项以匹配分母中的顺序：$\\mu_1 = \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}$。\n\n接下来，我们求解边际后验精度。边际方差是 $\\mathbf{\\Sigma}_{\\text{post}} = \\mathbf{\\Lambda}_{\\text{post}}^{-1}$ 的对角元素：\n$$ \\operatorname{Var}[z_1 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{11} = \\frac{C_{11}}{D} = \\frac{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3}{D} $$\n$$ \\operatorname{Var}[z_2 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{22} = \\frac{C_{22}}{D} = \\frac{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)}{D} $$\n$$ \\operatorname{Var}[z_3 \\mid x] = (\\mathbf{\\Sigma}_{\\text{post}})_{33} = \\frac{C_{33}}{D} = \\frac{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2}{D} $$\n\n边际后验精度是这些方差的倒数：\n$$ \\tau_1^{\\text{post}} = \\frac{D}{C_{11}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} = \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_1\\tau_3 + \\tau_2\\tau_3} $$\n后一种简化形式更为优雅和富有启发性，它证实了后验精度是似然精度（$\\gamma$）和 $z_1$ 的边际先验精度之和。\n$$ \\tau_2^{\\text{post}} = \\frac{D}{C_{22}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)} $$\n$$ \\tau_3^{\\text{post}} = \\frac{D}{C_{33}} = \\frac{\\gamma\\tau_1\\tau_2 + \\gamma\\tau_1\\tau_3 + \\gamma\\tau_2\\tau_3 + \\tau_1\\tau_2\\tau_3}{\\gamma\\tau_1 + \\gamma\\tau_2 + \\tau_1\\tau_2} $$\n这些表达式不能像 $\\tau_1^{\\text{post}}$ 那样简洁地简化，因此将以分数形式呈现。为了保持一致性，我们代入 $D$ 的表达式并像之前一样重新排序。例如，$\\tau_2^{\\text{post}} = \\frac{\\gamma(\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)}$。\n\n按要求将最终结果整理成一个单行矩阵。",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 (\\tau_2 + \\tau_3) x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\frac{\\gamma \\tau_1 \\tau_2 x}{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3} & \\gamma + \\frac{\\tau_1\\tau_2\\tau_3}{\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{(\\gamma + \\tau_1)(\\tau_2 + \\tau_3)} & \\frac{\\gamma (\\tau_1\\tau_2 + \\tau_2\\tau_3 + \\tau_1\\tau_3) + \\tau_1\\tau_2\\tau_3}{\\gamma(\\tau_1 + \\tau_2) + \\tau_1\\tau_2} \\end{pmatrix}} $$"
        },
        {
            "introduction": "最后，我们将从静态的解析解转向动态的算法实现，亲手模拟“分析-综合”的完整过程。本实践将指导你通过梯度下降来最小化一个自由能目标函数，从而实现预测编码的推断动力学。通过设计并执行一个计算实验，你将验证感官信息的精度（precision）如何调节预测误差的增益，并最终影响推断过程的收敛速度，从而将前两个练习的抽象概念转化为可执行的神经计算模型。",
            "id": "3984145",
            "problem": "您的任务是设计并执行一个计算实验，在生成模型和“通过合成进行分析”（analysis-by-synthesis）的背景下，以展示增加感觉噪声的精度（表示为 $\\Pi_x$）如何调节预测编码中误差单元的增益和收敛速度。考虑一个线性高斯生成模型，其中潜在状态向量 $z \\in \\mathbb{R}^2$ 通过一个合成映射 $C \\in \\mathbb{R}^{3 \\times 2}$ 和加性感觉噪声产生观测向量 $x \\in \\mathbb{R}^3$。该生成模型定义为\n$$\nx = C z + \\varepsilon_x,\n$$\n其中感觉噪声从均值为零、协方差矩阵为 $\\Sigma_x \\in \\mathbb{R}^{3 \\times 3}$ 的高斯分布中抽取，其精度为 $\\Pi_x = \\Sigma_x^{-1}$。$z$ 的先验是均值为 $\\mu_z$、精度为 $\\Pi_z$ 的高斯分布。\n\n在预测编码中，潜在变量 $z$ 是通过最小化一个“合成-分析一致性”目标函数来推断的，该目标函数惩罚预测误差和与先验的偏差。对于单个观测值 $x$，一个合适的目标函数是\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2.\n$$\n您必须推导最小化 $J(z)$ 的 $z$ 的梯度下降更新规则，并实现由此产生的迭代预测编码动力学。令误差单元定义为 $e_x(t) = x - C z(t)$，其中迭代索引为 $t \\in \\{0,1,2,\\ldots\\}$。施加于误差单元的增益体现在经精度加权的误差 $\\tilde{e}_x(t) = \\Pi_x\\, e_x(t)$ 中。通过以下比率来量化初始化时的瞬时增益\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2}.\n$$\n通过梯度 $\\|\\nabla J(z(t))\\|_2$ 的欧几里得范数降至指定容差 $\\varepsilon$ 以下所需的迭代次数来量化收敛速度，同时受最大迭代预算 $T_{\\max}$ 的限制。\n\n在所有测试用例中统一实现以下实验参数，这些参数在科学上是合理的且自洽的：\n- 合成矩阵 $C = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$。\n- 先验均值 $\\mu_z = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 和先验精度 $\\Pi_z = 1$。\n- 真实潜在值 $z_{\\text{true}} = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$，确定性感觉噪声向量 $\\varepsilon_x = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$，以及观测值 $x = C z_{\\text{true}} + \\varepsilon_x$。\n- 初始潜在值 $z(0) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$。\n- 固定学习率 $\\eta = 0.005$，容差 $\\varepsilon = 10^{-6}$，以及最大迭代次数 $T_{\\max} = 10000$。\n\n从上述定义出发，推导减小 $J(z)$ 的 $z$ 的梯度下降更新规则，实现该动力学，并对以下感觉精度 $\\Pi_x$ 的测试套件运行实验：\n1. $\\Pi_x = 0.01$（极低精度，感觉似然几乎不提供信息）。\n2. $\\Pi_x = 0.1$（低精度）。\n3. $\\Pi_x = 1.0$（中等精度）。\n4. $\\Pi_x = 10.0$（高精度）。\n5. $\\Pi_x = 100.0$（极高精度）。\n\n对于每个测试用例，计算并返回以下三个量：\n- 如上定义的增益 $G(\\Pi_x)$，一个浮点数。\n- 直到 $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$ 所需的迭代次数 $N(\\Pi_x)$，一个整数；如果在 $T_{\\max}$ 次迭代内未收敛，则设置 $N(\\Pi_x) = T_{\\max}$。\n- 一个布尔值 $B(\\Pi_x)$，指示是否达到收敛，其中如果在终止时 $\\|\\nabla J(z(t))\\|_2 \\le \\varepsilon$，则 $B(\\Pi_x) = \\text{True}$，否则为 $\\text{False}$。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表用方括号括起来，每个测试用例的结果本身也是一个形如 $[G(\\Pi_x),N(\\Pi_x),B(\\Pi_x)]$ 的逗号分隔列表。例如，对于三个用例，您的输出将如下所示\n$$\n\\big[ [G_1,N_1,B_1],[G_2,N_2,B_2],[G_3,N_3,B_3] \\big],\n$$\n但其中的值为五个指定测试精度所对应的值。不涉及任何物理单位或角度；所有量均为无量纲量。",
            "solution": "问题陈述已经过严格验证，被认为是科学可靠、定义明确且自洽的。它展示了线性高斯生成模型中推断的标准公式，这是计算神经科学和机器学习中的一个常见范式。所有参数都有明确定义，任务是一个清晰的计算实验。\n\n问题的核心是通过最小化目标函数 $J(z)$，从观测值 $x \\in \\mathbb{R}^3$ 推断潜在状态向量 $z \\in \\mathbb{R}^2$。该函数表示在给定 $x$ 的条件下 $z$ 的负对数后验概率，在预测编码的背景下也称为自由能泛函。目标函数由下式给出：\n$$\nJ(z) = \\frac{1}{2}\\,\\Pi_x\\,\\|x - C z\\|_2^2 + \\frac{1}{2}\\,\\Pi_z\\,\\|z - \\mu_z\\|_2^2\n$$\n在此，第一项是负对数似然，惩罚由标量感觉精度 $\\Pi_x$ 加权的预测误差 $x - C z$。第二项是负对数先验，惩罚潜在状态 $z$ 与其先验均值 $\\mu_z$ 的偏差，由标量先验精度 $\\Pi_z$ 加权。该模型假设噪声是各向同性的高斯噪声，因此使用标量精度。\n\n为了实现预测编码动力学，我们使用梯度下降法来寻找最小化 $J(z)$ 的 $z$ 值。更新规则为 $z(t+1) = z(t) - \\eta \\nabla J(z(t))$，其中 $\\eta$ 是学习率。我们必须首先推导 $J(z)$ 关于 $z$ 的梯度。\n\n我们使用向量转置表示法展开 $J(z)$ 中的各项：\n$$\nJ(z) = \\frac{1}{2}\\Pi_x (x - Cz)^T(x - Cz) + \\frac{1}{2}\\Pi_z(z - \\mu_z)^T(z - \\mu_z)\n$$\n我们分别求每一项的梯度。对于第一项，我们使用向量微积分的链式法则，注意到 $\\nabla_v(v^T v) = 2v$。\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_x (x - Cz)^T(x - Cz)\\right) = \\frac{1}{2}\\Pi_x \\left( 2 C^T(Cz - x) \\right) = \\Pi_x C^T(Cz - x)\n$$\n对于第二项：\n$$\n\\nabla_z \\left(\\frac{1}{2}\\Pi_z(z - \\mu_z)^T(z - \\mu_z)\\right) = \\frac{1}{2}\\Pi_z \\left( 2 (z - \\mu_z) \\right) = \\Pi_z(z - \\mu_z)\n$$\n将它们结合起来，总梯度为：\n$$\n\\nabla J(z) = \\Pi_x C^T(Cz - x) + \\Pi_z(z - \\mu_z)\n$$\n问题将预测误差定义为 $e_x(t) = x - C z(t)$。将其代入梯度表达式，我们得到：\n$$\n\\nabla J(z(t)) = -\\Pi_x C^T e_x(t) + \\Pi_z(z(t) - \\mu_z)\n$$\n因此，潜在状态 $z$ 的梯度下降更新规则是：\n$$\nz(t+1) = z(t) - \\eta \\left( \\Pi_x C^T(C z(t) - x) + \\Pi_z(z(t) - \\mu_z) \\right)\n$$\n这个迭代规则描述了推断过程的动力学，其中潜在状态被更新以减小目标函数 $J(z)$。\n\n接下来，我们计算初始化时的瞬时增益 $G(\\Pi_x)$。它被定义为在时间 $t=0$ 时，经精度加权的误差范数与原始误差范数之比：\n$$\nG(\\Pi_x) = \\frac{\\|\\tilde{e}_x(0)\\|_2}{\\|e_x(0)\\|_2} = \\frac{\\|\\Pi_x\\, e_x(0)\\|_2}{\\|e_x(0)\\|_2}\n$$\n由于 $\\Pi_x$ 是一个标量，我们可以将其从范数中提出：\n$$\nG(\\Pi_x) = \\frac{|\\Pi_x| \\|e_x(0)\\|_2}{\\|e_x(0)\\|_2} = |\\Pi_x|\n$$\n鉴于所有为 $\\Pi_x$ 指定的测试值都是正数，增益就是 $G(\\Pi_x) = \\Pi_x$。这表明感觉精度直接缩放了感觉预测误差对系统动力学的贡献。\n\n计算实验按以下步骤进行。首先，我们使用提供的参数计算恒定的观测向量 $x = C z_{\\text{true}} + \\varepsilon_x$。然后，对于测试套件中的每个 $\\Pi_x$ 值，我们执行以下步骤：\n1. 计算增益 $G(\\Pi_x) = \\Pi_x$。\n2. 初始化潜在状态 $z(0) = \\begin{bmatrix} 0.0, 0.0 \\end{bmatrix}^T$。\n3. 从 $0$ 到 $T_{\\max}$ 迭代 $N$ 步：\n    a. 使用当前状态 $z(N)$ 计算梯度 $\\nabla J(z(N))$。\n    b. 计算梯度的欧几里得范数 $\\|\\nabla J(z(N))\\|_2$。\n    c. 如果范数小于或等于容差 $\\varepsilon=10^{-6}$，则过程已收敛。我们记录迭代次数 $N(\\Pi_x) = N$ 和收敛标志 $B(\\Pi_x) = \\text{True}$，然后终止当前 $\\Pi_x$ 的循环。\n    d. 如果未满足收敛准则且 $N  T_{\\max}$，则使用梯度下降规则更新状态至 $z(N+1)$。\n4. 如果循环完成而未达到容差（即 $N=T_{\\max}$），我们设置 $N(\\Pi_x) = T_{\\max}$ 和 $B(\\Pi_x) = \\text{False}$。\n\n对所有指定的 $\\Pi_x$ 值重复此过程，并收集每个案例的结果——$[G(\\Pi_x), N(\\Pi_x), B(\\Pi_x)]$。预计结果将显示，随着 $\\Pi_x$ 的增加，收敛所需的迭代次数 $N(\\Pi_x)$ 会减少，这阐释了预测编码的一个基本原理：更高的感觉精度通过赋予预测误差更大的权重来加速推断，从而导致对内部模型更快的修正。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and executes a computational experiment on a predictive coding model.\n    \"\"\"\n    # 1. Define fixed parameters for the experiment\n    C = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n    mu_z = np.array([0.0, 0.0])\n    Pi_z = 1.0\n    z_true = np.array([2.0, -1.0])\n    eps_x_vec = np.array([0.1, -0.2, 0.05])\n    z_initial = np.array([0.0, 0.0])\n    \n    eta = 0.005  # Learning rate\n    tolerance = 1e-6\n    T_max = 10000\n\n    # Test suite for sensory precision\n    test_precisions = [0.01, 0.1, 1.0, 10.0, 100.0]\n\n    # Pre-compute constant values\n    x = C @ z_true + eps_x_vec\n    C_T = C.T\n\n    # Store results for all test cases\n    all_results = []\n\n    # 2. Iterate through each test case\n    for Pi_x in test_precisions:\n        # Calculate gain G(Pi_x)\n        # G = ||Pi_x * e_x(0)|| / ||e_x(0)|| = |Pi_x| * ||e_x(0)|| / ||e_x(0)||\n        # Since Pi_x is positive, G = Pi_x.\n        gain = float(Pi_x)\n\n        # Initialize variables for the iterative process\n        z = z_initial.copy()\n        converged = False\n        num_iterations = 0\n\n        # 3. Run predictive coding dynamics (gradient descent)\n        for i in range(T_max + 1):\n            # Calculate gradient: grad_J = Pi_x * C^T * (C*z - x) + Pi_z * (z - mu_z)\n            # With mu_z = 0, this is Pi_x * C^T * (C*z - x) + Pi_z * z\n            pred_error = C @ z - x\n            grad_likelihood = Pi_x * (C_T @ pred_error)\n            grad_prior = Pi_z * z\n            gradient = grad_likelihood + grad_prior\n            \n            grad_norm = np.linalg.norm(gradient)\n\n            # Check for convergence\n            if grad_norm = tolerance:\n                num_iterations = i\n                converged = True\n                break\n            \n            # If we are at the last possible iteration and haven't converged\n            if i == T_max:\n                num_iterations = T_max\n                converged = (grad_norm = tolerance) # Final check for rare case\n                break\n\n            # Update latent state z\n            z -= eta * gradient\n        \n        # Store results for this test case\n        all_results.append([gain, num_iterations, converged])\n\n    # 4. Format and print the final output\n    result_str_parts = []\n    for res in all_results:\n        # Ensure True/False are capitalized as per Python bool standard\n        res_str = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_str_parts.append(res_str)\n    \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}