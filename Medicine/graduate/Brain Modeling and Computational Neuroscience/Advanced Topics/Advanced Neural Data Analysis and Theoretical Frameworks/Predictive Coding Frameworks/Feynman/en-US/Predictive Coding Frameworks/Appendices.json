{
    "hands_on_practices": [
        {
            "introduction": "The predictive coding framework is fundamentally rooted in the mathematics of Bayesian inference. This first exercise provides a hands-on derivation of the cornerstone of this framework: how a system can optimally combine a prior belief with new sensory evidence. By working through this problem, you will derive the classic result that the updated belief (the posterior) is a precision-weighted average of the prior and the likelihood, revealing the elegant mathematical principle by which the brain can weigh its expectations against incoming sensory data. ",
            "id": "5052192",
            "problem": "Consider a single-latent-variable generative model commonly used to formalize Predictive Coding and Bayesian brain frameworks in neurobiology. A latent cause $x$ represents the brain’s prediction about a sensory input, and a measurement $y$ is the observed sensory signal. Assume a Gaussian prior on the latent cause, $x \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, and a Gaussian likelihood for the observation given the cause, $y \\mid x \\sim \\mathcal{N}(x, \\sigma_{y}^{2})$, where $\\sigma_{0}^{2} > 0$ and $\\sigma_{y}^{2} > 0$ are known constants. Using Bayes’ rule and the well-tested fact that products of Gaussian densities yield another Gaussian density in the variable of interest, derive the posterior density $p(x \\mid y)$ in closed form and show that it is Gaussian. Then express the posterior mean and posterior variance using precision-weighted combinations of the prior mean and the observation. Define precision as the inverse of variance, so that $\\tau_{0} = 1 / \\sigma_{0}^{2}$ and $\\tau_{y} = 1 / \\sigma_{y}^{2}$. Provide the final expressions for the posterior mean and the posterior variance in terms of $\\mu_{0}$, $y$, $\\tau_{0}$, and $\\tau_{y}$. No numerical approximation is required; present exact symbolic expressions. The final answer must be a calculation and should be expressed as a single row matrix containing the posterior mean and posterior variance.",
            "solution": "The problem statement is evaluated and found to be valid. It is a well-posed problem in Bayesian statistics, grounded in established mathematical principles and directly applicable to the specified neurobiological framework of predictive coding. All necessary information is provided, and the terms are unambiguous.\n\nThe task is to derive the posterior probability density function (PDF) $p(x \\mid y)$ for a latent cause $x$ given an observation $y$. We are given the prior distribution for $x$ and the likelihood function for $y$ given $x$.\n\nThe prior is a Gaussian distribution:\n$$p(x) = \\mathcal{N}(x \\mid \\mu_{0}, \\sigma_{0}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\nThe likelihood is also a Gaussian distribution:\n$$p(y \\mid x) = \\mathcal{N}(y \\mid x, \\sigma_{y}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y}^{2}}} \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right)$$\nAccording to Bayes' rule, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\nSubstituting the given distributions, we have:\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right) \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\nWe can combine the arguments of the exponential functions:\n$$p(x \\mid y) \\propto \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_{y}^{2}} - \\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}} \\right)$$\nTo find the form of the posterior, we analyze the exponent, focusing on the terms that depend on $x$. Let $E(x)$ be the argument of the exponential:\n$$E(x) = -\\frac{1}{2} \\left( \\frac{(y - x)^2}{\\sigma_{y}^{2}} + \\frac{(x - \\mu_{0})^2}{\\sigma_{0}^{2}} \\right)$$\nExpanding the squared terms:\n$$E(x) = -\\frac{1}{2} \\left( \\frac{y^2 - 2yx + x^2}{\\sigma_{y}^{2}} + \\frac{x^2 - 2x\\mu_{0} + \\mu_{0}^2}{\\sigma_{0}^{2}} \\right)$$\nWe now group the terms by powers of $x$:\n$$E(x) = -\\frac{1}{2} \\left[ x^2 \\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma_{y}^{2}}\\right) - 2x \\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{y}{\\sigma_{y}^{2}}\\right) + \\left(\\frac{\\mu_{0}^2}{\\sigma_{0}^{2}} + \\frac{y^2}{\\sigma_{y}^{2}}\\right) \\right]$$\nThe problem defines precision as the inverse of variance: $\\tau_{0} = 1/\\sigma_{0}^{2}$ and $\\tau_{y} = 1/\\sigma_{y}^{2}$. Substituting these into the expression:\n$$E(x) = -\\frac{1}{2} \\left[ x^2 (\\tau_{0} + \\tau_{y}) - 2x (\\tau_{0}\\mu_{0} + \\tau_{y}y) + (\\tau_{0}\\mu_{0}^2 + \\tau_{y}y^2) \\right]$$\nThis expression is a quadratic function of $x$. This implies that the posterior distribution $p(x \\mid y)$ is a Gaussian distribution, as stated in the problem. A generic Gaussian PDF for a variable $x$ with mean $\\mu_{\\text{post}}$ and variance $\\sigma_{\\text{post}}^2$ has an exponent of the form:\n$$-\\frac{(x - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} = -\\frac{1}{2\\sigma_{\\text{post}}^2} (x^2 - 2x\\mu_{\\text{post}} + \\mu_{\\text{post}}^2) = -\\frac{1}{2} (\\tau_{\\text{post}}x^2 - 2\\tau_{\\text{post}}\\mu_{\\text{post}}x + \\text{const})$$\nwhere $\\tau_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ is the posterior precision.\n\nBy comparing the coefficients of the powers of $x$ in our derived exponent $E(x)$ with the general form, we can identify the parameters of the posterior distribution.\nComparing the coefficients of the $x^2$ term:\n$$\\tau_{\\text{post}} = \\tau_{0} + \\tau_{y}$$\nThis shows that the posterior precision is the sum of the prior precision and the likelihood precision. The posterior variance, $\\sigma_{\\text{post}}^2$, is the inverse of the posterior precision:\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\tau_{\\text{post}}} = \\frac{1}{\\tau_{0} + \\tau_{y}}$$\nNow, comparing the coefficients of the $x$ term:\n$$2\\tau_{\\text{post}}\\mu_{\\text{post}} = 2(\\tau_{0}\\mu_{0} + \\tau_{y}y)$$\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{\\text{post}}}$$\nSubstituting the expression for $\\tau_{\\text{post}}$:\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{0} + \\tau_{y}}$$\nThe posterior mean is a precision-weighted average of the prior mean $\\mu_{0}$ and the observed data $y$. The terms in $E(x)$ that do not depend on $x$ are absorbed into the normalization constant of the posterior Gaussian PDF.\n\nThus, the posterior distribution $p(x \\mid y)$ is a Gaussian $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$ with the derived mean and variance.\n\nThe posterior mean is:\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{0} + \\tau_{y}}$$\nThe posterior variance is:\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\tau_{0} + \\tau_{y}}$$\nThese are the final symbolic expressions required.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\tau_{0} \\mu_{0} + \\tau_{y} y}{\\tau_{0} + \\tau_{y}} & \\frac{1}{\\tau_{0} + \\tau_{y}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the previous exercise established the ideal solution for a simple linear model, perception often involves navigating ambiguous or conflicting information. This practice explores how predictive coding handles such complexity by modeling a non-convex free energy landscape, which represents a scenario with multiple plausible interpretations of the sensory input. You will implement a gradient descent algorithm to simulate the brain \"settling\" into a stable percept, and in doing so, discover how prior beliefs can create distinct basins of attraction that shape our perceptual experience, especially when sensory data is ambiguous. ",
            "id": "4011074",
            "problem": "Consider a one-dimensional predictive coding system that performs approximate Bayesian inference by minimizing a Variational Free Energy (VFE). The latent variable is denoted by $x \\in \\mathbb{R}$ and the observed data by $y \\in \\mathbb{R}$. The generative model comprises a Gaussian likelihood with precision $\\lambda > 0$ and a non-Gaussian prior whose negative log-density induces a double-well potential. The system seeks the Maximum A Posteriori (MAP) estimate by minimizing the VFE over $x$ using gradient descent.\n\nFundamental base:\n- The posterior over $x$ given $y$ is proportional to the product of the likelihood and the prior according to Bayes' rule.\n- Under Gaussian observation noise with precision $\\lambda$, the negative log-likelihood contributes a precision-weighted squared prediction error term to the VFE.\n- Predictive coding realizes inference as gradient descent on the VFE, where the descent direction is the negative gradient of the objective with respect to the latent variable $x$.\n\nConstruct a double-well free energy function using the following components:\n- A prior energy term that creates two stable modes at $\\pm m$, with $m > 0$, given by a quartic double-well potential $U(x) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2$, with $\\alpha > 0$.\n- A data-fitting term from a Gaussian likelihood with precision $\\lambda$: $\\dfrac{\\lambda}{2} \\left( y - x \\right)^2$.\n\nThe Variational Free Energy to be minimized is therefore\n$$\nF(x; \\alpha, m, \\lambda, y) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 + \\dfrac{\\lambda}{2} \\left( y - x \\right)^2.\n$$\n\nTasks:\n1. Starting from the definitions above, derive the gradient descent update rule for $x$ that minimizes $F(x; \\alpha, m, \\lambda, y)$.\n2. Implement a deterministic algorithm that, given $(\\alpha, m, \\lambda, y)$ and an initialization $x_0$, performs gradient descent on $F$ until convergence. Convergence must be declared when the absolute gradient magnitude is less than $10^{-10}$ or when the iteration count exceeds the specified maximum. Use backtracking to ensure that each accepted update does not increase the free energy. The backtracking procedure should halve the current step size whenever a proposed update increases $F$, with at most $20$ halvings per iteration.\n3. After convergence, assign a mode index to the solution $x^\\star$ as follows. Let $d_+ = |x^\\star - m|$ and $d_- = |x^\\star + m|$. Output $+1$ if $d_+ < d_-$ (closer to the right well at $+m$), $-1$ if $d_- < d_+$ (closer to the left well at $-m$), and $0$ if $d_+ = d_-$ (equidistant, which occurs at $x^\\star = 0$).\n4. Analyze how nonconvexity affects convergence by showing, through the test suite below, how initialization determines the basin of attraction and the final mode.\n\nTest suite:\nFor each case, use the tuple $(\\alpha, m, \\lambda, y, x_0, \\eta, \\text{max\\_iter})$ where $\\eta$ is the initial step size for gradient descent and $\\text{max\\_iter}$ is the maximum number of iterations. Use these exact values:\n- Case $1$: $(4.0, 1.0, 2.0, 0.2, 0.9, 0.02, 20000)$, a general case where the observation slightly favors the right well.\n- Case $2$: $(4.0, 1.0, 2.0, -0.2, -0.9, 0.02, 20000)$, a symmetric counterpart favoring the left well.\n- Case $3$: $(4.0, 1.0, 2.0, 0.0, 0.0, 0.02, 20000)$, an unstable critical point exactly at the saddle.\n- Case $4$: $(10.0, 1.0, 0.5, 0.0, -0.1, 0.02, 20000)$, a strong prior with weak data pulling initialization into the left basin.\n- Case $5$: $(1.0, 1.0, 50.0, 0.3, -0.7, 0.02, 20000)$, data-dominated regime that pulls the solution to the right well.\n- Case $6$: $(4.0, 1.5, 2.0, -0.1, -0.4, 0.02, 20000)$, altered well positions with $m = 1.5$.\n\nFinal output format:\nYour program should produce a single line of output containing the mode indices for the six test cases as a comma-separated list enclosed in square brackets (for example, $[+1,-1,0,-1,+1,-1]$). Each entry must be an integer from $\\{-1, 0, +1\\}$ representing the final mode index determined by the convergence of gradient descent from the specified initialization.",
            "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of computational neuroscience and Bayesian inference, specifically the predictive coding framework. The problem is well-posed, providing a clear objective function, a standard numerical optimization task, and all necessary parameters. The definitions and goals are objective and mathematically precise. All givens are self-contained and consistent.\n\nThe provided givens are:\n- Latent variable: $x \\in \\mathbb{R}$\n- Observed data: $y \\in \\mathbb{R}$\n- Prior energy (quartic double-well potential): $U(x) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2$, with parameters $\\alpha > 0$ and $m > 0$.\n- Data-fitting energy (negative log-likelihood of a Gaussian): $\\dfrac{\\lambda}{2} \\left( y - x \\right)^2$, with precision $\\lambda > 0$.\n- Variational Free Energy (VFE): $F(x; \\alpha, m, \\lambda, y) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 + \\dfrac{\\lambda}{2} \\left( y - x \\right)^2$.\n- Optimization method: Gradient descent with backtracking to find $x^\\star = \\arg\\min_x F(x)$.\n- Convergence criteria: Absolute gradient magnitude $|\\nabla_x F| < 10^{-10}$ or iteration count exceeds `max_iter`.\n- Backtracking rule: If an update increases $F$, the step size is halved. This is repeated at most $20$ times per iteration.\n- Mode classification: The converged state $x^\\star$ is assigned a mode index based on its proximity to the prior's potential wells at $+m$ and $-m$.\n- Test cases: A suite of six parameter sets $(\\alpha, m, \\lambda, y, x_0, \\eta, \\text{max\\_iter})$ to test the implementation.\n\n**1. Derivation of the Gradient Descent Update Rule**\n\nThe core of the predictive coding process described here is the minimization of the Variational Free Energy, $F(x)$, via gradient descent. The update rule for the latent variable $x$ is given by moving $x$ in the direction opposite to the gradient of $F(x)$. First, we must compute the gradient, $\\dfrac{dF}{dx}$.\n\nThe VFE is the sum of two terms:\n$$\nF(x) = F_{prior}(x) + F_{data}(x) = \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 + \\dfrac{\\lambda}{2} \\left( y - x \\right)^2\n$$\n\nWe differentiate each term with respect to $x$ using the chain rule:\n\nFor the prior energy term, $F_{prior}(x)$:\n$$\n\\dfrac{dF_{prior}}{dx} = \\dfrac{d}{dx} \\left[ \\dfrac{\\alpha}{4} \\left( x^2 - m^2 \\right)^2 \\right] = \\dfrac{\\alpha}{4} \\cdot 2 \\left( x^2 - m^2 \\right) \\cdot \\dfrac{d}{dx}(x^2 - m^2) = \\dfrac{\\alpha}{2} \\left( x^2 - m^2 \\right) (2x) = \\alpha x (x^2 - m^2)\n$$\nThis simplifies to $\\alpha (x^3 - m^2 x)$.\n\nFor the data-fitting energy term, $F_{data}(x)$:\n$$\n\\dfrac{dF_{data}}{dx} = \\dfrac{d}{dx} \\left[ \\dfrac{\\lambda}{2} \\left( y - x \\right)^2 \\right] = \\dfrac{\\lambda}{2} \\cdot 2 \\left( y - x \\right) \\cdot \\dfrac{d}{dx}(y - x) = \\lambda (y - x) (-1) = \\lambda (x - y)\n$$\n\nThe total gradient of the VFE is the sum of these two components:\n$$\n\\dfrac{dF}{dx} = \\alpha (x^3 - m^2 x) + \\lambda(x - y)\n$$\n\nThe gradient descent update rule for $x$ at iteration $k$, with a step size (learning rate) $\\eta_k$, is:\n$$\nx_{k+1} = x_k - \\eta_k \\left. \\dfrac{dF}{dx} \\right|_{x=x_k}\n$$\nSubstituting the derived gradient, we get the explicit update rule:\n$$\nx_{k+1} = x_k - \\eta_k \\left( \\alpha (x_k^3 - m^2 x_k) + \\lambda(x_k - y) \\right)\n$$\n\n**2. Algorithmic Design and Implementation**\n\nThe problem requires a deterministic algorithm to perform this gradient descent until convergence. A non-convex function like $F(x)$ can have multiple local minima, and the standard gradient descent algorithm only guarantees convergence to one of these, depending on the initialization $x_0$. To ensure stable convergence, a backtracking line search is employed to determine the step size $\\eta_k$ at each iteration.\n\nThe algorithm proceeds as follows:\n1.  Initialize $x$ with the given starting value $x_0$.\n2.  Begin an iterative loop, up to a maximum of `max_iter` iterations.\n3.  In each iteration $k$, calculate the gradient $g_k = \\dfrac{dF}{dx}|_{x=x_k}$.\n4.  Check for convergence: if the absolute magnitude of the gradient, $|g_k|$, is less than the tolerance $10^{-10}$, the process has converged to a stationary point. The loop is terminated.\n5.  If not converged, perform a backtracking line search to find an appropriate step size $\\eta_k$.\n    a. Start with the initial step size $\\eta_k = \\eta$. Let the current energy be $F_k = F(x_k)$.\n    b. Propose a new state $x_{prop} = x_k - \\eta_k g_k$.\n    c. If $F(x_{prop}) < F_k$, the step size is accepted.\n    d. If $F(x_{prop}) \\geq F_k$, the step size is too large. It is halved ($\\eta_k \\leftarrow \\eta_k / 2$), and step 5b is repeated. This halving is done at most $20$ times.\n    e. If a suitable step size is not found after all allowed halvings, it implies that no further progress can be made from $x_k$. The algorithm is considered converged at $x_k$.\n6.  Update the state using the accepted step size: $x_{k+1} = x_k - \\eta_k g_k$.\n7.  The loop continues until a termination condition (gradient tolerance, max iterations, or failed backtracking) is met. The final value of $x$ is the solution $x^\\star$.\n\n**3. Mode Classification**\n\nAfter convergence to a stable point $x^\\star$, the solution must be classified according to which of the prior's two potential wells it landed in. The wells are centered at $+m$ and $-m$. The classification is based on the Euclidean distance from $x^\\star$ to these two points:\n-   Let $d_+ = |x^\\star - m|$ be the distance to the right well.\n-   Let $d_- = |x^\\star + m|$ be the distance to the left well.\n\nThe mode index is assigned as:\n-   $+1$ if $d_+ < d_-$, meaning $x^\\star$ is closer to the right well. This is true for any $x^\\star > 0$.\n-   $-1$ if $d_- < d_+$, meaning $x^\\star$ is closer to the left well. This is true for any $x^\\star < 0$.\n-   $0$ if $d_+ = d_-$, meaning $x^\\star$ is equidistant from both wells. This occurs only if $x^\\star = 0$, which is a saddle point (local maximum) of the VFE when $y=0$.\n\nThis procedure correctly partitions the real line into three regions corresponding to the basins of attraction of the two minima and the unstable fixed point between them. The test suite demonstrates how the interplay of the prior ($\\alpha, m$), the data ($\\lambda, y$), and the initialization ($x_0$) determines which of these regions the system converges to. For instance, a strong data term (large $\\lambda$) can pull the system out of the basin suggested by its initialization, whereas a strong prior (large $\\alpha$) will make the system more resilient to conflicting data.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the predictive coding problem for a suite of test cases.\n    \"\"\"\n\n    class PredictiveCodingSystem:\n        \"\"\"\n        Implements the minimization of a double-well Variational Free Energy function.\n        \"\"\"\n        def __init__(self, alpha, m, lamb, y, x0, eta, max_iter):\n            self.alpha = float(alpha)\n            self.m = float(m)\n            self.lamb = float(lamb)\n            self.y = float(y)\n            self.x = float(x0)\n            self.eta = float(eta)\n            self.max_iter = int(max_iter)\n            self.grad_tol = 1e-10\n            self.max_backtrack = 20\n\n        def free_energy(self, x):\n            \"\"\"Calculates the Variational Free Energy F(x).\"\"\"\n            prior_energy = (self.alpha / 4.0) * ((x**2 - self.m**2)**2)\n            data_term = (self.lamb / 2.0) * ((self.y - x)**2)\n            return prior_energy + data_term\n\n        def gradient(self, x):\n            \"\"\"Calculates the gradient of F(x) with respect to x.\"\"\"\n            prior_grad = self.alpha * x * (x**2 - self.m**2)\n            data_grad = self.lamb * (x - self.y)\n            return prior_grad + data_grad\n\n        def minimize(self):\n            \"\"\"\n            Performs gradient descent with backtracking to find the minimum of F(x).\n            \"\"\"\n            for _ in range(self.max_iter):\n                grad = self.gradient(self.x)\n\n                if abs(grad) < self.grad_tol:\n                    # Convergence based on small gradient\n                    break\n\n                current_energy = self.free_energy(self.x)\n                step_eta = self.eta\n                found_descent_step = False\n                \n                # Backtracking line search to find a step that decreases energy\n                for i in range(self.max_backtrack + 1):\n                    x_proposed = self.x - step_eta * grad\n                    \n                    if self.free_energy(x_proposed) < current_energy:\n                        self.x = x_proposed\n                        found_descent_step = True\n                        break # Step accepted, exit backtracking\n                    \n                    step_eta /= 2.0\n                \n                if not found_descent_step:\n                    # Could not find a descent step even after max halvings.\n                    # This implies we are at a minimum (to machine precision).\n                    break\n            \n            return self.x\n\n        def get_mode_index(self, x_star):\n            \"\"\"\n            Classifies the converged state x_star into a mode index {-1, 0, +1}.\n            \"\"\"\n            d_plus = abs(x_star - self.m)\n            d_minus = abs(x_star + self.m)\n\n            if d_plus < d_minus:\n                return 1\n            elif d_minus < d_plus:\n                return -1\n            else:\n                return 0\n\n    def solve_case(params):\n        \"\"\"\n        Initializes and runs the minimization for a single test case.\n        \"\"\"\n        alpha, m, lamb, y, x0, eta, max_iter = params\n        system = PredictiveCodingSystem(alpha, m, lamb, y, x0, eta, max_iter)\n        x_star = system.minimize()\n        return system.get_mode_index(x_star)\n\n    # Test suite as defined in the problem statement\n    test_cases = [\n        # (alpha, m, lambda, y, x0, eta, max_iter)\n        (4.0, 1.0, 2.0, 0.2, 0.9, 0.02, 20000),   # Case 1\n        (4.0, 1.0, 2.0, -0.2, -0.9, 0.02, 20000),  # Case 2\n        (4.0, 1.0, 2.0, 0.0, 0.0, 0.02, 20000),   # Case 3\n        (10.0, 1.0, 0.5, 0.0, -0.1, 0.02, 20000),  # Case 4\n        (1.0, 1.0, 50.0, 0.3, -0.7, 0.02, 20000),  # Case 5\n        (4.0, 1.5, 2.0, -0.1, -0.4, 0.02, 20000),  # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        mode_index = solve_case(case)\n        results.append(mode_index)\n\n    # Format the final output string exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Perception is not a passive process; we actively move our eyes, heads, and bodies to gather more informative data about the world. This final practice extends the predictive coding framework into the domain of Active Inference, where agents act to minimize their uncertainty. You will implement a model that decides on an action by prospectively calculating its \"epistemic value\"—the expected reduction in posterior entropy. This exercise demonstrates how the drive to make accurate predictions about the world can be formalized to provide a first-principles account of goal-directed, information-seeking behavior. ",
            "id": "4011073",
            "problem": "Consider a binary hidden-state generative model in the tradition of predictive coding and Active Inference (AI), where the agent can take actions to influence the informativeness of subsequent sensory data. The hidden state is denoted by $s \\in \\{0,1\\}$, the observation by $o \\in \\{0,1\\}$, and the action by $a \\in \\{0,1\\}$, where $a=0$ indicates a less informative (passive) sampling and $a=1$ indicates a more informative (active) sampling. The agent's prior belief over the hidden state is $p(s=1)=\\pi$ and $p(s=0)=1-\\pi$.\n\nThe likelihood model of observations given the hidden state and action is parameterized by an action-dependent accuracy parameter $\\alpha_a \\in [0.5,1]$, with the following symmetry:\n- $p(o=1 \\mid s=1,a)=\\alpha_a$, $p(o=0 \\mid s=1,a)=1-\\alpha_a$,\n- $p(o=0 \\mid s=0,a)=\\alpha_a$, $p(o=1 \\mid s=0,a)=1-\\alpha_a$.\n\nThe task is to demonstrate how Active Inference resolves ambiguous sensory input via action that disambiguates states, by computing the expected reduction in posterior entropy after taking an action. Use the following fundamental bases:\n- Bayes' rule: $p(s \\mid o,a) = \\dfrac{p(o \\mid s,a)\\,p(s)}{p(o \\mid a)}$, with $p(o \\mid a)=\\sum_{s} p(o \\mid s,a)\\,p(s)$.\n- Shannon entropy (in nats): for a binary distribution $p(s)$, $H[p(s)] = -\\sum_{s \\in \\{0,1\\}} p(s)\\,\\ln p(s)$.\n\nDefine the expected reduction in posterior entropy for action $a$ as the difference between the prior entropy and the expected posterior entropy after observing $o$ generated under action $a$, namely:\n- prior entropy: $H[p(s)]$,\n- posterior entropy given an observation: $H[p(s \\mid o,a)]$,\n- expected posterior entropy: $\\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$,\n- expected reduction: $H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$.\n\nYou must:\n1. Derive from first principles the computation of the expected reduction in posterior entropy for any given $(\\pi,\\alpha_0,\\alpha_1)$ using only Bayes' rule and Shannon entropy.\n2. Implement a program that, for each test case, computes the expected reduction in posterior entropy for $a=0$ and $a=1$, selects the action that maximizes the expected reduction (tie-breaking in favor of the smaller index), and outputs the result for each test case as a list $[R_0,R_1,A^\\ast]$ where $R_0$ and $R_1$ are the expected reductions (in nats) for $a=0$ and $a=1$ respectively (rounded to six decimal places), and $A^\\ast \\in \\{0,1\\}$ is the selected action index.\n\nUse the following test suite:\n- Case $1$: $\\pi=0.5$, $\\alpha_0=0.6$, $\\alpha_1=0.9$.\n- Case $2$: $\\pi=0.99$, $\\alpha_0=0.8$, $\\alpha_1=0.95$.\n- Case $3$: $\\pi=0.5$, $\\alpha_0=0.5$, $\\alpha_1=0.5$.\n- Case $4$: $\\pi=0.7$, $\\alpha_0=0.55$, $\\alpha_1=0.75$.\n\nAll entropy quantities must be expressed in nats and rounded to six decimal places. Your program should produce a single line of output containing the results for the above cases as a comma-separated list enclosed in square brackets, where each element is itself a list in the form `[R_0,R_1,A^\\ast]`. For example: $[[0.123456,0.234567,1],[\\dots]]$.\n\nNo external input is allowed; hard-code the test suite in the program. The solution must be applicable in purely mathematical terms and solvable using any modern programming language. Ensure numerical stability and scientific realism by not introducing invalid probabilities (e.g., probabilities outside $[0,1]$).",
            "solution": "The user-provided problem is first validated against the established criteria.\n\n### Step 1: Extract Givens\n- **Hidden state:** $s \\in \\{0,1\\}$\n- **Observation:** $o \\in \\{0,1\\}$\n- **Action:** $a \\in \\{0,1\\}$\n- **Prior belief:** $p(s=1) = \\pi$, $p(s=0) = 1-\\pi$.\n- **Likelihood model:** This is a symmetric binary channel where the probability of a correct observation is $\\alpha_a \\in [0.5, 1]$, dependent on action $a$.\n  - $p(o=1 \\mid s=1,a) = \\alpha_a$\n  - $p(o=0 \\mid s=1,a) = 1-\\alpha_a$\n  - $p(o=0 \\mid s=0,a) = \\alpha_a$\n  - $p(o=1 \\mid s=0,a) = 1-\\alpha_a$\n- **Fundamental principles:**\n  - Bayes' rule: $p(s \\mid o,a) = \\frac{p(o \\mid s,a)\\,p(s)}{p(o \\mid a)}$, where $p(o \\mid a) = \\sum_{s} p(o \\mid s,a)\\,p(s)$.\n  - Shannon entropy (in nats): $H[p(x)] = -\\sum_i p(x_i)\\,\\ln p(x_i)$.\n- **Objective function:** The expected reduction in posterior entropy for action $a$, denoted $R_a$, is defined as:\n  $R_a = H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$, where:\n  - $H[p(s)]$ is the entropy of the prior distribution over states.\n  - $H[p(s \\mid o,a)]$ is the entropy of the posterior distribution over states after taking action $a$ and observing $o$.\n  - $\\mathbb{E}_{o \\sim p(o \\mid a)}[\\cdot]$ denotes the expectation over observations $o$ drawn from the distribution $p(o \\mid a)$.\n- **Action selection:** The optimal action $A^\\ast$ is the one that maximizes the expected reduction in entropy, with ties broken in favor of the smaller action index ($a=0$). $A^\\ast = \\arg\\max_a R_a$.\n- **Test cases:**\n    1. $\\pi=0.5$, $\\alpha_0=0.6$, $\\alpha_1=0.9$\n    2. $\\pi=0.99$, $\\alpha_0=0.8$, $\\alpha_1=0.95$\n    3. $\\pi=0.5$, $\\alpha_0=0.5$, $\\alpha_1=0.5$\n    4. $\\pi=0.7$, $\\alpha_0=0.55$, $\\alpha_1=0.75$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in Bayesian probability and information theory, which are foundational to computational neuroscience and machine learning. The model, though simple, is a standard formulation used to illustrate the principles of Active Inference and predictive coding, where agents act to reduce their uncertainty about the world. It is scientifically sound.\n- **Well-Posed:** All necessary parameters ($\\pi, \\alpha_0, \\alpha_1$), definitions (entropy, Bayes' rule), and objectives are explicitly provided. The problem is self-contained and has a unique, computable solution for each test case.\n- **Objective:** The problem is stated using precise mathematical language, free from any subjective or ambiguous terminology.\n- **Other criteria:** The problem is not contradictory, incomplete, or based on unrealistic physics. It is a formalizable and non-trivial computational task.\n\n### Step 3: Verdict and Action\nThe problem is valid. A step-by-step derivation of the solution is provided below.\n\n### Derivation of the Solution\n\nThe solution requires computing the expected reduction in posterior entropy, $R_a$, for each action $a \\in \\{0,1\\}$. This quantity is also known as the expected information gain or the mutual information $I(S;O|a)$.\n\n**1. Prior Entropy**\nThe prior distribution over the binary state $s$ is a Bernoulli distribution with parameter $\\pi$. Its Shannon entropy, $H[p(s)]$, is:\n$$ H[p(s)] = -\\left[ p(s=0)\\ln(p(s=0)) + p(s=1)\\ln(p(s=1)) \\right] = -\\left[ (1-\\pi)\\ln(1-\\pi) + \\pi\\ln(\\pi) \\right] $$\nFor brevity, we can use the binary entropy function $h(x) = -x\\ln(x) - (1-x)\\ln(1-x)$. Thus, $H[p(s)] = h(\\pi)$. By convention, $0\\ln(0)=0$, so if $\\pi=0$ or $\\pi=1$, the entropy is $0$.\n\n**2. Marginal Probability of Observations**\nFor a given action $a$ with accuracy parameter $\\alpha_a$, we compute the probability of each observation $o$ by marginalizing over the hidden states $s$:\n$$ p(o \\mid a) = \\sum_{s \\in \\{0,1\\}} p(o \\mid s, a) p(s) $$\nFor $o=1$:\n$$ p(o=1 \\mid a) = p(o=1 \\mid s=1, a)p(s=1) + p(o=1 \\mid s=0, a)p(s=0) = \\alpha_a \\pi + (1-\\alpha_a)(1-\\pi) $$\nFor $o=0$:\n$$ p(o=0 \\mid a) = p(o=0 \\mid s=1, a)p(s=1) + p(o=0 \\mid s=0, a)p(s=0) = (1-\\alpha_a)\\pi + \\alpha_a(1-\\pi) $$\nIt can be verified that $p(o=1 \\mid a) + p(o=0 \\mid a) = 1$.\n\n**3. Posterior Probability of Hidden States**\nUsing Bayes' rule, we find the posterior probability of the state $s=1$ given an observation $o$ and action $a$, which we denote $\\pi'_{o,a}$:\n$$ \\pi'_{o,a} = p(s=1 \\mid o, a) = \\frac{p(o \\mid s=1, a)p(s=1)}{p(o \\mid a)} $$\nIf $o=1$:\n$$ \\pi'_{1,a} = \\frac{\\alpha_a \\pi}{p(o=1 \\mid a)} = \\frac{\\alpha_a \\pi}{\\alpha_a \\pi + (1-\\alpha_a)(1-\\pi)} $$\nIf $o=0$:\n$$ \\pi'_{0,a} = \\frac{(1-\\alpha_a) \\pi}{p(o=0 \\mid a)} = \\frac{(1-\\alpha_a) \\pi}{(1-\\alpha_a)\\pi + \\alpha_a(1-\\pi)} $$\nThe posterior probability of $s=0$ is then $1 - \\pi'_{o,a}$.\n\n**4. Expected Posterior Entropy**\nThe entropy of the posterior distribution $p(s \\mid o, a)$ is $H[p(s \\mid o, a)] = h(\\pi'_{o,a})$. The expected posterior entropy is the average of these posterior entropies, weighted by the probability of each observation:\n$$ \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] = p(o=0 \\mid a)H[p(s \\mid o=0, a)] + p(o=1 \\mid a)H[p(s \\mid o=1, a)] $$\n$$ \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] = p(o=0 \\mid a) \\cdot h(\\pi'_{0,a}) + p(o=1 \\mid a) \\cdot h(\\pi'_{1,a}) $$\n\n**5. Expected Reduction in Entropy**\nThe final value, $R_a$, is the difference between the prior entropy and the expected posterior entropy:\n$$ R_a = H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] $$\nThis calculation is performed for both $a=0$ (with $\\alpha_0$) and $a=1$ (with $\\alpha_1$) to obtain $R_0$ and $R_1$.\n\n**6. Action Selection**\nThe agent chooses the action that maximizes the expected information gain.\n$$ A^\\ast = \\begin{cases} 1 & \\text{if } R_1 > R_0 \\\\ 0 & \\text{if } R_1 \\le R_0 \\end{cases} $$\nThis adheres to the specified tie-breaking rule. The result for each test case is the triplet $[R_0, R_1, A^\\ast]$. An implementation of this procedure will be provided in the following section.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Active Inference problem by calculating expected entropy reduction for given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (pi, alpha_0, alpha_1)\n        (0.5, 0.6, 0.9),\n        (0.99, 0.8, 0.95),\n        (0.5, 0.5, 0.5),\n        (0.7, 0.55, 0.75),\n    ]\n\n    def binary_entropy(p):\n        \"\"\"\n        Calculates the Shannon entropy for a binary probability distribution p(x=1)=p.\n        H(p) = -p*ln(p) - (1-p)*ln(1-p) in nats.\n        Handles edge cases where p=0 or p=1.\n        \"\"\"\n        if p == 0 or p == 1:\n            return 0.0\n        return -p * np.log(p) - (1 - p) * np.log(1 - p)\n\n    def calculate_expected_reduction(pi, alpha_a):\n        \"\"\"\n        Computes the expected reduction in posterior entropy for a given prior pi and action accuracy alpha_a.\n        \"\"\"\n        # 1. Prior Entropy\n        prior_entropy = binary_entropy(pi)\n        \n        # If prior is certain, no uncertainty to reduce.\n        if prior_entropy == 0.0:\n            return 0.0\n\n        # Special case: if action is uninformative, posterior equals prior, so no reduction.\n        if alpha_a == 0.5:\n            return 0.0\n\n        # 2. Marginal Probability of Observations\n        # p(o=1 | a)\n        p_o1_a = alpha_a * pi + (1 - alpha_a) * (1 - pi)\n        # p(o=0 | a)\n        p_o0_a = 1.0 - p_o1_a\n\n        # 3. Posterior Probabilities of Hidden States\n        # p(s=1 | o=1, a)\n        # Handle cases where p_o1_a might be zero if pi=0 or pi=1.\n        if p_o1_a > 0:\n            post_s1_o1 = (alpha_a * pi) / p_o1_a\n        else: # This happens if pi=0 and alpha_a=1, or pi=1 and alpha_a=0 (not in domain)\n            post_s1_o1 = 0\n            \n        # p(s=1 | o=0, a)\n        if p_o0_a > 0:\n            post_s1_o0 = ((1 - alpha_a) * pi) / p_o0_a\n        else: # This happens if pi=1 and alpha_a=1, or pi=0 and alpha_a=0 (not in domain)\n            post_s1_o0 = 0\n\n        # 4. Expected Posterior Entropy\n        # H[p(s|o=1,a)]\n        post_entropy_o1 = binary_entropy(post_s1_o1)\n        # H[p(s|o=0,a)]\n        post_entropy_o0 = binary_entropy(post_s1_o0)\n        \n        expected_posterior_entropy = p_o1_a * post_entropy_o1 + p_o0_a * post_entropy_o0\n\n        # 5. Expected Reduction in Entropy\n        reduction = prior_entropy - expected_posterior_entropy\n        \n        return reduction\n\n    \n    results_str_list = []\n    for case in test_cases:\n        pi, alpha_0, alpha_1 = case\n        \n        # Compute reduction for action a=0\n        R0 = calculate_expected_reduction(pi, alpha_0)\n        \n        # Compute reduction for action a=1\n        R1 = calculate_expected_reduction(pi, alpha_1)\n        \n        # Select action that maximizes reduction, tie-break to a=0\n        A_star = 1 if R1 > R0 else 0\n        \n        # Format results as specified\n        formatted_r0 = f\"{round(R0, 6):.6f}\"\n        formatted_r1 = f\"{round(R1, 6):.6f}\"\n        result_str = f\"[{formatted_r0},{formatted_r1},{A_star}]\"\n        results_str_list.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        }
    ]
}