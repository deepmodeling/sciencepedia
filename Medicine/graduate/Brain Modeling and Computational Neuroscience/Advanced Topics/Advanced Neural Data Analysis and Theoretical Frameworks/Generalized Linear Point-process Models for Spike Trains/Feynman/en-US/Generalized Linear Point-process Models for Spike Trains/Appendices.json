{
    "hands_on_practices": [
        {
            "introduction": "The theoretical elegance of a continuous-time GLM must be translated into a discrete-time computational framework to be applied to real, sampled data. This practice guides you through the crucial first step: discretizing the continuous convolution that filters an external stimulus. By performing this conversion, you will derive the entries of the design matrix, which forms the core of the regression problem solved during model fitting .",
            "id": "3983850",
            "problem": "Consider a single-neuron point-process Generalized Linear Model (GLM) for spike trains in continuous time, where the conditional intensity at time $t$ depends on a filtered exogenous stimulus $s(t)$ via a linear functional of a causal temporal kernel $g(\\tau)$, so that the linear predictor contains the term $\\int_{0}^{T_b} g(\\tau)\\, s(t-\\tau)\\, d\\tau$, with $g(\\tau) = 0$ for $\\tau \\notin [0, T_b]$. Suppose the kernel $g(\\tau)$ is parameterized in a basis expansion $g(\\tau) = \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau)$, where $\\{b_j(\\tau)\\}_{j=1}^{p}$ are fixed causal basis functions supported on $[0, T_b]$, and $\\{\\theta_j\\}_{j=1}^{p}$ are unknown coefficients to be estimated. Thus, the stimulus-related contribution to the linear predictor can be written as $\\sum_{j=1}^{p} \\theta_j\\, (b_j * s)(t)$, where $(b_j * s)(t) \\equiv \\int_{0}^{T_b} b_j(\\tau)\\, s(t-\\tau)\\, d\\tau$.\n\nYou discretize time with uniform bin width $\\Delta t  0$ and sample all signals on the grid $t_n = n\\, \\Delta t$, $n = 1,2,\\dots,N$. Define the discrete sequences $s[n] \\equiv s(n\\, \\Delta t)$ and $b_j[\\ell] \\equiv b_j(\\ell\\, \\Delta t)$ for integer lags $\\ell \\in \\{0,1,\\dots,L-1\\}$, where $L \\equiv \\lfloor T_b / \\Delta t \\rfloor + 1$, and take $s[n] = 0$ whenever its argument falls outside the observed window. Use a Riemann-sum discretization that retains $\\Delta t$ explicitly and do not absorb $\\Delta t$ into the basis functions or parameters.\n\nYour tasks are:\n1. Starting from the definition of continuous-time convolution and the basis expansion for $g(\\tau)$, show how the discrete-time approximation yields a representation of the stimulus-related predictor as a matrix-vector product $(X \\boldsymbol{\\theta})[n]$ with a design matrix $X \\in \\mathbb{R}^{N \\times p}$, where the $(n,j)$-th entry $X_{n,j}$ depends on the discrete-time stimulus history $\\{s[n-\\ell]\\}_{\\ell=0}^{L-1}$ and the basis samples $\\{b_j[\\ell]\\}_{\\ell=0}^{L-1}$.\n2. Under the causal support and zero-padding assumptions stated above, provide the closed-form analytic expression for the design-matrix entry $X_{n,j}$ in terms of $\\Delta t$, $\\{b_j[\\ell]\\}$, and $\\{s[n-\\ell]\\}$.\n\nAnswer format requirement: Provide your final answer as a single closed-form analytic expression for $X_{n,j}$. No numerical rounding is required. No physical units are needed in the final expression.",
            "solution": "The problem statement is parsed and validated as follows.\n\n### Step 1: Extract Givens\n- **Model:** Single-neuron point-process Generalized Linear Model (GLM) in continuous time.\n- **Stimulus Filter Term:** The stimulus-related contribution to the linear predictor is given by the convolution integral $u(t) = \\int_{0}^{T_b} g(\\tau)\\, s(t-\\tau)\\, d\\tau$.\n- **Kernel Properties:** The temporal kernel $g(\\tau)$ is causal and has support on the interval $[0, T_b]$, i.e., $g(\\tau) = 0$ for $\\tau \\notin [0, T_b]$.\n- **Kernel Parameterization:** The kernel is expanded in a basis of $p$ fixed functions $\\{b_j(\\tau)\\}_{j=1}^{p}$: $g(\\tau) = \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau)$. The basis functions $b_j(\\tau)$ are also causal and supported on $[0, T_b]$. The coefficients $\\{\\theta_j\\}_{j=1}^{p}$ are the parameters to be estimated.\n- **Predictor Form:** The stimulus contribution can be rewritten as a sum over basis components: $\\sum_{j=1}^{p} \\theta_j\\, (b_j * s)(t)$, where $(b_j * s)(t) \\equiv \\int_{0}^{T_b} b_j(\\tau)\\, s(t-\\tau)\\, d\\tau$.\n- **Time Discretization:** Time is discretized with a uniform bin width $\\Delta t  0$, yielding a time grid $t_n = n\\, \\Delta t$ for $n = 1, 2, \\dots, N$.\n- **Signal Discretization:** The continuous signals are sampled on this grid: $s[n] \\equiv s(n\\, \\Delta t)$ and $b_j[\\ell] \\equiv b_j(\\ell\\, \\Delta t)$ for integer lag indices $\\ell \\in \\{0, 1, \\dots, L-1\\}$.\n- **Lag Definition:** The maximum number of lag steps is derived from the kernel support duration $T_b$ and the time step $\\Delta t$: $L \\equiv \\lfloor T_b / \\Delta t \\rfloor + 1$.\n- **Boundary Condition:** The stimulus signal is assumed to be zero-padded: $s[n] = 0$ if its time index falls outside the observed window.\n- **Approximation Method:** The continuous-time integral is to be approximated by a Riemann sum, retaining the factor $\\Delta t$ explicitly.\n- **Task:** Derive the expression for the $(n,j)$-th entry, $X_{n,j}$, of the design matrix $X$ in the discrete-time linear predictor representation $(X \\boldsymbol{\\theta})[n]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is firmly grounded in computational neuroscience and statistical signal processing. The use of a GLM with a basis function expansion to model neural firing is a standard and well-established technique (e.g., Pillow et al., 2008). The formulation is scientifically sound.\n- **Well-Posedness:** The problem is well-posed. It asks for the derivation of a discrete algebraic representation from a continuous integral formulation under a specified discretization scheme. The instructions are clear, and a unique answer is expected.\n- **Objectivity:** The problem statement is expressed in precise mathematical language, free from any subjective or ambiguous terminology.\n- **Flaw Analysis:**\n    1.  **Scientific/Factual Unsoundness:** None. The model is a cornerstone of modern neural data analysis.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is formal and directly relevant to the implementation of GLMs for spike trains.\n    3.  **Incomplete/Contradictory Setup:** None. All necessary components—the integral form, the basis expansion, the discretization rule (Riemann sum), and the definitions of discrete signals—are provided.\n    4.  **Unrealistic/Infeasible:** None. This is a standard procedure for fitting such models to data.\n    5.  **Ill-Posed/Poorly Structured:** None. The task is a straightforward mathematical derivation.\n    6.  **Pseudo-Profound/Trivial:** None. The derivation is a fundamental step in understanding and implementing these models.\n    7.  **Outside Scientific Verifiability:** None. The derivation is subject to standard mathematical verification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n***\n\n### Solution Derivation\n\nThe objective is to find the expression for the element $X_{n,j}$ of the design matrix $X$ in the discrete-time formulation of the stimulus-dependent part of a GLM's linear predictor.\n\nWe begin with the continuous-time expression for the stimulus-filtered component of the linear predictor at time $t$:\n$$u(t) = \\int_{0}^{T_b} g(\\tau)\\, s(t-\\tau)\\, d\\tau$$\nThe filter kernel $g(\\tau)$ is parameterized by a linear combination of basis functions $b_j(\\tau)$:\n$$g(\\tau) = \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau)$$\nSubstituting this expansion into the integral gives:\n$$u(t) = \\int_{0}^{T_b} \\left( \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau) \\right) s(t-\\tau)\\, d\\tau$$\nSince the summation is finite and the integral converges, we can interchange the order of summation and integration:\n$$u(t) = \\sum_{j=1}^{p} \\theta_j \\left( \\int_{0}^{T_b} b_j(\\tau)\\, s(t-\\tau)\\, d\\tau \\right)$$\nThis expression gives the continuous-time predictor. To obtain the discrete-time version, we evaluate $u(t)$ at the discrete time points $t_n = n\\, \\Delta t$:\n$$u(t_n) \\equiv u[n] = \\sum_{j=1}^{p} \\theta_j \\left( \\int_{0}^{T_b} b_j(\\tau)\\, s(t_n - \\tau)\\, d\\tau \\right)$$\nThe next step is to discretize the integral term. The problem specifies a Riemann sum approximation. Let's denote the integral for the $j$-th basis function as $I_j[n]$:\n$$I_j[n] = \\int_{0}^{T_b} b_j(\\tau)\\, s(t_n - \\tau)\\, d\\tau$$\nWe approximate this integral using a sum. We partition the integration interval $[0, T_b]$ into subintervals of width $\\Delta t$. The integration variable $\\tau$ is discretized as $\\tau_\\ell = \\ell\\, \\Delta t$, with the differential element $d\\tau$ approximated as $\\Delta t$. The summation will be over the discrete lag indices $\\ell$. The problem defines the discrete basis functions $b_j[\\ell]$ for $\\ell \\in \\{0, 1, \\dots, L-1\\}$, where $L = \\lfloor T_b / \\Delta t \\rfloor + 1$. This range of $\\ell$ covers the support of the kernel. A left-hand Riemann sum is appropriate here, evaluating the integrand at the start of each interval:\n$$I_j[n] \\approx \\sum_{\\ell=0}^{L-1} \\left[ b_j(\\ell\\, \\Delta t)\\, s(t_n - \\ell\\, \\Delta t) \\right] \\Delta t$$\nNote that the summation limit $L-1 = \\lfloor T_b / \\Delta t \\rfloor$ ensures that the argument $\\ell \\Delta t$ remains within or at the boundary of the support $[0, T_b]$.\n\nNow, we substitute the definitions of the discrete-time signals $s[k] \\equiv s(k\\, \\Delta t)$ and $b_j[\\ell] \\equiv b_j(\\ell\\, \\Delta t)$:\n- $b_j(\\ell\\, \\Delta t) = b_j[\\ell]$\n- $s(t_n - \\ell\\, \\Delta t) = s((n-\\ell)\\Delta t) = s[n-\\ell]$\n\nSubstituting these into the sum gives the discrete approximation of the convolution for the $j$-th basis component:\n$$I_j[n] \\approx \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell]$$\nThis expression represents the filtered stimulus through the $j$-th basis function at discrete time $n$.\n\nWe can now substitute this back into the expression for the discrete predictor $u[n]$:\n$$u[n] \\approx \\sum_{j=1}^{p} \\theta_j \\left( \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell] \\right)$$\nThe problem asks to represent this in the form of a matrix-vector product, $(X \\boldsymbol{\\theta})[n]$, where $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2, \\dots, \\theta_p]^T$ is the column vector of parameters. The $n$-th element of this product is given by the dot product of the $n$-th row of the design matrix $X$ with the vector $\\boldsymbol{\\theta}$:\n$$(X \\boldsymbol{\\theta})[n] = \\sum_{j=1}^{p} X_{n,j}\\, \\theta_j$$\nBy comparing this target form with our derived expression for $u[n]$, we can identify the term that multiplies each $\\theta_j$.\n$$u[n] = \\sum_{j=1}^{p} \\underbrace{\\left( \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell] \\right)}_{X_{n,j}} \\theta_j$$\nBy direct inspection, the $(n,j)$-th entry of the design matrix $X$ is therefore:\n$$X_{n,j} = \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell]$$\nThis is the closed-form expression for the design matrix entry. Each column $j$ of the matrix $X$ corresponds to the stimulus $s$ convolved with the $j$-th basis function $b_j$, and each row $n$ corresponds to a discrete time point.",
            "answer": "$$\\boxed{\\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell]}$$"
        },
        {
            "introduction": "A fitted model is only as useful as our ability to interpret its parameters correctly. This exercise addresses the critical and often subtle issue of how parameter estimates are affected by practical choices like the units of measurement for a stimulus and the size of the time bins used for analysis. By working through the explicit effects of these choices, you will learn to maintain a consistent physical interpretation of your model's firing rate predictions, a key skill for reproducible and meaningful science .",
            "id": "3983864",
            "problem": "A single neuron is modeled as a conditional point process with conditional intensity function $\\lambda(t)$, defined by the fundamental relation that for sufficiently small $dt$, the probability of exactly one spike in the interval $[t, t+dt)$ given the history $\\mathcal{H}_t$ is $P(N(t+dt)-N(t)=1 \\mid \\mathcal{H}_t) \\approx \\lambda(t)\\,dt$, and the expected spike count in an interval $[t, t+\\Delta)$ is $E[N(t+\\Delta)-N(t) \\mid \\mathcal{H}_t] \\approx \\int_t^{t+\\Delta} \\lambda(s)\\,ds \\approx \\lambda(t)\\,\\Delta$ for small $\\Delta$. Under a Generalized Linear Model (GLM) with log link, the conditional intensity is specified as $\\lambda(t) = \\exp(\\beta_0 + \\beta_x x(t))$, where $x(t)$ is a stimulus covariate measured in volts and $\\lambda(t)$ is interpreted in spikes per second.\n\nA discrete-time Poisson GLM is fit to binned spike counts $y_k$ over bins of width $\\Delta$ seconds, with mean $\\mu_k \\approx \\lambda(t_k)\\,\\Delta$. Suppose one fits this discrete-time model without including an explicit offset for $\\Delta$, using a linear predictor $\\theta_0 + \\theta_x z_k$, where $z_k$ is the regressor actually supplied to the model.\n\nYou change the stimulus unit from volts to millivolts by defining $x'(t) = s\\,x(t)$ with $s = 1000$, and you change the bin width from $\\Delta$ seconds to $\\Delta'$ seconds. Derive, from the foundational definition of conditional intensity and the Poisson approximation for small bins, how these unit changes affect the coefficients so that the interpretation of $\\lambda(t)$ remains in spikes per second. In particular, derive expressions for the new discrete-time coefficients $(\\theta'_0, \\theta'_x)$ in terms of $(\\beta_0, \\beta_x)$, $s$, and $\\Delta'$ when fitting without an explicit offset. Then, using the values $\\beta_0 = \\ln(12)$, $\\beta_x = 0.25$ (per volt), original bin width $\\Delta = 0.01$ seconds, and new bin width $\\Delta' = 0.002$ seconds, compute the final $(\\theta'_0, \\theta'_x)$ for the millivolt-scaled stimulus $x'(t)$.\n\nExpress your final answer as a row matrix containing $\\theta'_0$ and $\\theta'_x$ in exact closed form. No rounding is required, and no units should be included inside the final row matrix.",
            "solution": "The problem asks us to determine how the coefficients of a discrete-time Poisson Generalized Linear Model (GLM) for neural spike counts change when the units of the stimulus covariate and the time bin width are altered. The key is to relate the parameters of the fitted discrete model to the parameters of the underlying continuous-time conditional intensity function, ensuring the physical model remains invariant.\n\nFirst, let us establish the relationship between the continuous-time model and the discrete-time model. The continuous-time conditional intensity function is given by a GLM with a log link:\n$$\n\\lambda(t) = \\exp(\\beta_0 + \\beta_x x(t))\n$$\nHere, $\\lambda(t)$ has units of spikes per second (s$^{-1}$), $x(t)$ is the stimulus in volts (V), $\\beta_x$ has units of V$^{-1}$, and $\\beta_0$ is a dimensionless baseline log-rate.\n\nFor a discrete-time model, the spike train is binned into intervals of width $\\Delta$. The expected number of spikes, $\\mu_k$, in the $k$-th bin centered at time $t_k$ is approximated by:\n$$\n\\mu_k \\approx \\lambda(t_k) \\Delta\n$$\nThe discrete-time model is a Poisson GLM with a log link, so the mean $\\mu_k$ is related to a linear predictor $\\eta_k$ by $\\mu_k = \\exp(\\eta_k)$. Taking the natural logarithm of the expression for $\\mu_k$:\n$$\n\\eta_k = \\ln(\\mu_k) = \\ln(\\lambda(t_k) \\Delta) = \\ln(\\lambda(t_k)) + \\ln(\\Delta)\n$$\nSubstituting the expression for $\\lambda(t_k)$:\n$$\n\\eta_k = \\ln(\\exp(\\beta_0 + \\beta_x x(t_k))) + \\ln(\\Delta) = \\beta_0 + \\beta_x x(t_k) + \\ln(\\Delta)\n$$\nThis is the full linear predictor for the discrete-time model. The term $\\ln(\\Delta)$ is an offset, a predictor whose coefficient is fixed to $1$. The problem states that the model is fit *without an explicit offset*. This implies that the $\\ln(\\Delta)$ term is absorbed into the intercept of the fitted model. If the fitted model has the form $\\theta_0 + \\theta_x z_k$ where $z_k = x(t_k)$ is the regressor, then by comparing forms, we would find $\\theta_x = \\beta_x$ and $\\theta_0 = \\beta_0 + \\ln(\\Delta)$.\n\nNow, we introduce changes to the units. The stimulus unit is changed from volts to millivolts, and the bin width is changed.\nThe new stimulus is $x'(t) = s\\,x(t)$, where $s=1000$ mV/V.\nThe new bin width is $\\Delta'$.\n\nThe underlying physical reality, the conditional intensity $\\lambda(t)$, must not change. We can express $\\lambda(t)$ in terms of the new stimulus variable $x'(t)$. Since $x(t) = x'(t)/s$, we have:\n$$\n\\lambda(t) = \\exp\\left(\\beta_0 + \\beta_x \\frac{x'(t)}{s}\\right) = \\exp\\left(\\beta_0 + \\left(\\frac{\\beta_x}{s}\\right) x'(t)\\right)\n$$\nThis shows how the continuous-time stimulus coefficient changes with the unit scaling. The new coefficient is $\\beta'_x = \\beta_x/s$. The intercept $\\beta_0$ remains unchanged.\n\nWe are asked to find the coefficients $(\\theta'_0, \\theta'_x)$ of a new discrete-time model that is fitted using the new stimulus $x'(t)$ and the new bin width $\\Delta'$. The linear predictor for this new model is specified as:\n$$\n\\eta'_k = \\theta'_0 + \\theta'_x x'(t_k)\n$$\nTo find expressions for $\\theta'_0$ and $\\theta'_x$, we derive $\\eta'_k$ from first principles. The expected spike count in a new bin of width $\\Delta'$ is $\\mu'_k = \\lambda(t_k)\\Delta'$. The corresponding linear predictor is:\n$$\n\\eta'_k = \\ln(\\mu'_k) = \\ln(\\lambda(t_k)\\Delta') = \\ln(\\lambda(t_k)) + \\ln(\\Delta')\n$$\nSubstituting the original expression for $\\lambda(t_k)$ in terms of $x(t_k)$:\n$$\n\\eta'_k = (\\beta_0 + \\beta_x x(t_k)) + \\ln(\\Delta')\n$$\nTo compare this with the fitted model form, we must express it in terms of the new stimulus regressor $x'(t_k)$. Using $x(t_k) = x'(t_k)/s$:\n$$\n\\eta'_k = \\beta_0 + \\beta_x \\left(\\frac{x'(t_k)}{s}\\right) + \\ln(\\Delta')\n$$\nRearranging the terms to match the form $\\theta'_0 + \\theta'_x x'(t_k)$:\n$$\n\\eta'_k = (\\beta_0 + \\ln(\\Delta')) + \\left(\\frac{\\beta_x}{s}\\right) x'(t_k)\n$$\nBy comparing this derived linear predictor with the fitted form $\\eta'_k = \\theta'_0 + \\theta'_x x'(t_k)$, we can directly identify the coefficients:\n$$\n\\theta'_0 = \\beta_0 + \\ln(\\Delta')\n$$\n$$\n\\theta'_x = \\frac{\\beta_x}{s}\n$$\nThese are the general expressions for the new discrete-time coefficients.\n\nFinally, we compute the numerical values for these coefficients using the provided data:\n$\\beta_0 = \\ln(12)$\n$\\beta_x = 0.25$ (per volt)\n$s = 1000$ (millivolts per volt)\n$\\Delta' = 0.002$ seconds\n\nSubstituting these values into our derived expressions:\nFor the intercept $\\theta'_0$:\n$$\n\\theta'_0 = \\ln(12) + \\ln(0.002) = \\ln(12 \\times 0.002) = \\ln(0.024)\n$$\nFor the stimulus coefficient $\\theta'_x$:\n$$\n\\theta'_x = \\frac{0.25}{1000} = 0.00025\n$$\nThe new coefficients for the discrete-time model fitted with stimulus in millivolts and a bin width of $0.002$ seconds are $\\theta'_0 = \\ln(0.024)$ and $\\theta'_x = 0.00025$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\ln(0.024)  0.00025 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After fitting a GLM, we must critically assess its performance: does it truly capture the statistical structure of the observed spike train? This practice introduces the time-rescaling theorem, a powerful and elegant method for model validation rooted in the fundamental theory of point processes. You will derive the necessary transformation and apply it to a model, converting the observed spike train into a sequence of values that can be easily checked for uniformity, thereby providing a robust goodness-of-fit diagnostic .",
            "id": "3983810",
            "problem": "Consider a simple stationary point process model for a neuron's spike train indexed by spike times $\\{t_i\\}_{i=1}^{N}$ over a finite observation window, with conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$, where $\\mathcal{H}_t$ denotes the spike history up to time $t$. The conditional intensity function is defined by the fundamental limit $\\lambda(t \\mid \\mathcal{H}_t) = \\lim_{\\Delta t \\to 0^{+}} \\frac{\\mathbb{P}\\{\\text{one spike in }[t, t+\\Delta t) \\mid \\mathcal{H}_t\\}}{\\Delta t}$, and the inter-spike interval survival function is given by the standard hazard-survival relation $S(s) = \\exp\\!\\left(-\\int_{0}^{s} \\lambda(t_{i-1}+u \\mid \\mathcal{H}_{t_{i-1}+u}) \\, du\\right)$, where $s$ is the elapsed time since the last spike at $t_{i-1}$.\n\nStarting only from these definitions and the well-tested fact that a nonhomogeneous Poisson process with rate $\\lambda(t)$ can be mapped to a homogeneous Poisson process of rate $1$ by the deterministic time change $\\tau(t) = \\int_{0}^{t} \\lambda(u) \\, du$, do the following:\n\n1. State the time-rescaling theorem for simple point processes with conditional intensity $\\lambda(t \\mid \\mathcal{H}_t)$, and derive the monotone transformation of inter-spike intervals that yields independent and identically distributed random variables with the Uniform$(0,1)$ law under a correctly specified model.\n\n2. Consider a Generalized Linear Model (GLM) point-process intensity with an exponential (log link) parameterization over the interval $[0, 0.6]$ seconds. There are two observed spikes at times $t_1 = 0.2$ and $t_2 = 0.6$ seconds. There is no covariate other than a spike-history term. The conditional intensity is\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp\\!\\left(\\beta_0 + \\beta_h \\, \\kappa(t - t_1) \\mathbf{1}\\{t  t_1\\}\\right),\n$$\nwhere $\\beta_0 \\in \\mathbb{R}$ and $\\beta_h \\in \\mathbb{R}$ are parameters, and the spike-history kernel is\n$$\n\\kappa(s) = \\begin{cases}\n1,  0 \\leq s  0.5, \\\\\n0,  s \\geq 0.5.\n\\end{cases}\n$$\nAssume the spike-train history is empty before $t_1$ (i.e., no spikes before $t_1$). Use the transformation you derived in part 1 to compute the two transformed variables associated with the inter-spike intervals $(t_0, t_1)$ and $(t_1, t_2)$, where $t_0 = 0$. Express your final answer for the pair of transformed values as a single closed-form analytic expression, in terms of $\\beta_0$ and $\\beta_h$, and present it as a row vector. Time is measured in seconds; the transformed variables are dimensionless. No numerical approximation or rounding is required; give exact expressions.",
            "solution": "This problem is valid as it is scientifically grounded in the theory of point processes and computational neuroscience, well-posed with a unique and meaningful solution, and objective in its formulation. All necessary information is provided, and there are no contradictions.\n\nThe problem is divided into two parts. The first part requires the statement and derivation of the transformation that converts inter-spike intervals (ISIs) from a general point process into independent and identically distributed (i.i.d.) Uniform$(0,1)$ random variables. The second part requires the application of this transformation to a specific Generalized Linear Model (GLM) for a spike train.\n\n**Part 1: The Time-Rescaling Transformation**\n\nThe time-rescaling theorem for a simple point process with a given conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$ provides a method for model assessment. If the model for the conditional intensity is correct, a specific transformation of the observed spike times will result in a set of i.i.d. random variables following a known distribution.\n\nLet $\\{t_i\\}$ be the sequence of spike times, with $t_0$ as the start of the observation period. The $i$-th inter-spike interval is $\\Delta t_i = t_i - t_{i-1}$. Let $\\Delta T_i$ be the random variable corresponding to this interval. The conditional intensity $\\lambda(t \\mid \\mathcal{H}_t)$ acts as the hazard rate for the process at time $t$. Given the history up to the previous spike at $t_{i-1}$, the probability of a spike in the next small interval of time depends on this function.\n\nThe survival function $S(s \\mid \\mathcal{H}_{t_{i-1}})$, which is the probability that the $i$-th ISI is greater than a duration $s$, is given by the product integral of the hazard function. The problem provides this relation:\n$$\nS(s \\mid \\mathcal{H}_{t_{i-1}}) = \\mathbb{P}(\\Delta T_i  s \\mid \\mathcal{H}_{t_{i-1}}) = \\exp\\left(-\\int_{0}^{s} \\lambda(t_{i-1}+u \\mid \\mathcal{H}_{t_{i-1}+u}) \\, du\\right)\n$$\nBy performing a change of variables $v = t_{i-1}+u$, the integral can be rewritten over the absolute time axis:\n$$\nS(s \\mid \\mathcal{H}_{t_{i-1}}) = \\exp\\left(-\\int_{t_{i-1}}^{t_{i-1}+s} \\lambda(v \\mid \\mathcal{H}_v) \\, dv\\right)\n$$\nThe cumulative distribution function (CDF) of the ISI, $F(s \\mid \\mathcal{H}_{t_{i-1}}) = \\mathbb{P}(\\Delta T_i \\leq s \\mid \\mathcal{H}_{t_{i-1}})$, is simply $1 - S(s \\mid \\mathcal{H}_{t_{i-1}})$. Therefore,\n$$\nF(s \\mid \\mathcal{H}_{t_{i-1}}) = 1 - \\exp\\left(-\\int_{t_{i-1}}^{t_{i-1}+s} \\lambda(v \\mid \\mathcal{H}_v) \\, dv\\right)\n$$\nAccording to the probability integral transform (PIT) theorem, if $X$ is a continuous random variable with CDF $F_X(x)$, then the random variable $Y = F_X(X)$ is uniformly distributed on the interval $(0,1)$.\n\nApplying the PIT to our case, we can define a set of transformed variables $z_i$ by evaluating the conditional CDF of each ISI at the observed ISI duration $\\Delta t_i = t_i - t_{i-1}$.\n$$\nz_i = F(\\Delta t_i \\mid \\mathcal{H}_{t_{i-1}}) = 1 - \\exp\\left(-\\int_{t_{i-1}}^{t_i} \\lambda(v \\mid \\mathcal{H}_v) \\, dv\\right)\n$$\nIf the model $\\lambda(t \\mid \\mathcal{H}_t)$ is a correct specification of the true data-generating process, the resulting sequence of variables $\\{z_i\\}$ will be independent and identically distributed according to the Uniform$(0,1)$ distribution. This transformation is the one required by the problem.\n\nThis result connects to the time-rescaling for a nonhomogeneous Poisson process. The integral $\\tau_i = \\int_{t_{i-1}}^{t_i} \\lambda(v \\mid \\mathcal{H}_v) \\, dv$ can be interpreted as the \"rescaled time\". The theorem states that these rescaled times $\\{\\tau_i\\}$ are i.i.d. random variables from a standard exponential distribution with rate $1$. The CDF of an Exponential$(1)$ distribution is $F(x) = 1 - \\exp(-x)$. Applying the PIT to the variables $\\tau_i$ gives $z_i = F(\\tau_i) = 1-\\exp(-\\tau_i)$, yielding the same Uniform$(0,1)$ variables.\n\n**Part 2: Application to the GLM Spike Train**\n\nWe are given a specific GLM for the conditional intensity over the interval $[0, 0.6]$ seconds:\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp\\left(\\beta_0 + \\beta_h \\, \\kappa(t - t_1) \\mathbf{1}\\{t  t_1\\}\\right)\n$$\nwith a spike-history kernel\n$$\n\\kappa(s) = \\begin{cases} 1,  0 \\leq s  0.5 \\\\ 0,  s \\geq 0.5 \\end{cases}\n$$\nThe observed spikes are at $t_1 = 0.2$ and $t_2 = 0.6$. The observation starts at $t_0 = 0$. We need to compute the transformed variables $z_1$ and $z_2$ for the two inter-spike intervals $(t_0, t_1)$ and $(t_1, t_2)$.\n\n**First Interval: $(t_0, t_1) = (0, 0.2)$**\n\nThe first transformed variable is $z_1 = 1 - \\exp\\left(-\\int_{0}^{0.2} \\lambda(t \\mid \\mathcal{H}_t) \\, dt\\right)$.\nFor $t \\in [0, 0.2]$, the history $\\mathcal{H}_t$ is empty, and the condition $t  t_1$ (i.e., $t  0.2$) is false. The indicator function $\\mathbf{1}\\{t  0.2\\}$ is $0$.\nTherefore, the conditional intensity simplifies to the baseline rate:\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp(\\beta_0 + \\beta_h \\cdot 0) = \\exp(\\beta_0) \\quad \\text{for } t \\in [0, 0.2]\n$$\nThe integral of the intensity over this interval is:\n$$\n\\int_{0}^{0.2} \\exp(\\beta_0) \\, dt = \\exp(\\beta_0) \\int_{0}^{0.2} dt = 0.2 \\exp(\\beta_0)\n$$\nThe first transformed variable is thus:\n$$\nz_1 = 1 - \\exp(-0.2 \\exp(\\beta_0))\n$$\n\n**Second Interval: $(t_1, t_2) = (0.2, 0.6)$**\n\nThe second transformed variable is $z_2 = 1 - \\exp\\left(-\\int_{0.2}^{0.6} \\lambda(t \\mid \\mathcal{H}_t) \\, dt\\right)$.\nFor $t \\in (0.2, 0.6]$, the history $\\mathcal{H}_t$ contains the spike at $t_1 = 0.2$. The condition $t  t_1$ is true, so the indicator function $\\mathbf{1}\\{t  0.2\\}$ is $1$.\nThe argument to the kernel function is $s = t - t_1 = t - 0.2$. As $t$ ranges from $0.2$ to $0.6$, $s$ ranges from $0$ to $0.4$.\nAccording to the definition of $\\kappa(s)$, for $s \\in [0, 0.4]$, which is a subset of $[0, 0.5)$, we have $\\kappa(s) = 1$.\nTherefore, the conditional intensity for $t \\in (0.2, 0.6]$ is constant:\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp(\\beta_0 + \\beta_h \\cdot 1) = \\exp(\\beta_0 + \\beta_h) \\quad \\text{for } t \\in (0.2, 0.6]\n$$\nThe integral of the intensity over this interval is:\n$$\n\\int_{0.2}^{0.6} \\exp(\\beta_0 + \\beta_h) \\, dt = \\exp(\\beta_0 + \\beta_h) \\int_{0.2}^{0.6} dt = (0.6 - 0.2) \\exp(\\beta_0 + \\beta_h) = 0.4 \\exp(\\beta_0 + \\beta_h)\n$$\nThe second transformed variable is thus:\n$$\nz_2 = 1 - \\exp(-0.4 \\exp(\\beta_0 + \\beta_h))\n$$\nThe pair of transformed values $(z_1, z_2)$ is to be presented as a row vector.\n$$\n\\begin{pmatrix} z_1  z_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\exp(-0.2 \\exp(\\beta_0))  1 - \\exp(-0.4 \\exp(\\beta_0 + \\beta_h)) \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 - \\exp(-0.2 \\exp(\\beta_0))  1 - \\exp(-0.4 \\exp(\\beta_0 + \\beta_h)) \\end{pmatrix}}\n$$"
        }
    ]
}