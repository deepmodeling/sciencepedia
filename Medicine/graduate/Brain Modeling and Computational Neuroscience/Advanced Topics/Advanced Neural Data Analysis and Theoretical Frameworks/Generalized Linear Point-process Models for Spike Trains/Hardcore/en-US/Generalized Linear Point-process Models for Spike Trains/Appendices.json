{
    "hands_on_practices": [
        {
            "introduction": "The theoretical formulation of a Generalized Linear Model (GLM) for spike trains is elegant, but its practical application requires translating continuous-time integrals into a discrete-time format suitable for computation. This exercise walks you through this crucial process, showing how a filtered stimulus term is transformed into a design matrix for a regression analysis. Mastering this step  is essential for implementing and fitting GLMs to real, discretely sampled neural data.",
            "id": "3983850",
            "problem": "Consider a single-neuron point-process Generalized Linear Model (GLM) for spike trains in continuous time, where the conditional intensity at time $t$ depends on a filtered exogenous stimulus $s(t)$ via a linear functional of a causal temporal kernel $g(\\tau)$, so that the linear predictor contains the term $\\int_{0}^{T_b} g(\\tau)\\, s(t-\\tau)\\, d\\tau$, with $g(\\tau) = 0$ for $\\tau \\notin [0, T_b]$. Suppose the kernel $g(\\tau)$ is parameterized in a basis expansion $g(\\tau) = \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau)$, where $\\{b_j(\\tau)\\}_{j=1}^{p}$ are fixed causal basis functions supported on $[0, T_b]$, and $\\{\\theta_j\\}_{j=1}^{p}$ are unknown coefficients to be estimated. Thus, the stimulus-related contribution to the linear predictor can be written as $\\sum_{j=1}^{p} \\theta_j\\, (b_j * s)(t)$, where $(b_j * s)(t) \\equiv \\int_{0}^{T_b} b_j(\\tau)\\, s(t-\\tau)\\, d\\tau$.\n\nYou discretize time with uniform bin width $\\Delta t > 0$ and sample all signals on the grid $t_n = n\\, \\Delta t$, $n = 1,2,\\dots,N$. Define the discrete sequences $s[n] \\equiv s(n\\, \\Delta t)$ and $b_j[\\ell] \\equiv b_j(\\ell\\, \\Delta t)$ for integer lags $\\ell \\in \\{0,1,\\dots,L-1\\}$, where $L \\equiv \\lfloor T_b / \\Delta t \\rfloor + 1$, and take $s[n] = 0$ whenever its argument falls outside the observed window. Use a Riemann-sum discretization that retains $\\Delta t$ explicitly and do not absorb $\\Delta t$ into the basis functions or parameters.\n\nYour tasks are:\n1. Starting from the definition of continuous-time convolution and the basis expansion for $g(\\tau)$, show how the discrete-time approximation yields a representation of the stimulus-related predictor as a matrix-vector product $(X \\boldsymbol{\\theta})[n]$ with a design matrix $X \\in \\mathbb{R}^{N \\times p}$, where the $(n,j)$-th entry $X_{n,j}$ depends on the discrete-time stimulus history $\\{s[n-\\ell]\\}_{\\ell=0}^{L-1}$ and the basis samples $\\{b_j[\\ell]\\}_{\\ell=0}^{L-1}$.\n2. Under the causal support and zero-padding assumptions stated above, provide the closed-form analytic expression for the design-matrix entry $X_{n,j}$ in terms of $\\Delta t$, $\\{b_j[\\ell]\\}$, and $\\{s[n-\\ell]\\}$.\n\nAnswer format requirement: Provide your final answer as a single closed-form analytic expression for $X_{n,j}$. No numerical rounding is required. No physical units are needed in the final expression.",
            "solution": "The problem statement is parsed and validated as follows.\n\n### Step 1: Extract Givens\n- **Model:** Single-neuron point-process Generalized Linear Model (GLM) in continuous time.\n- **Stimulus Filter Term:** The stimulus-related contribution to the linear predictor is given by the convolution integral $u(t) = \\int_{0}^{T_b} g(\\tau)\\, s(t-\\tau)\\, d\\tau$.\n- **Kernel Properties:** The temporal kernel $g(\\tau)$ is causal and has support on the interval $[0, T_b]$, i.e., $g(\\tau) = 0$ for $\\tau \\notin [0, T_b]$.\n- **Kernel Parameterization:** The kernel is expanded in a basis of $p$ fixed functions $\\{b_j(\\tau)\\}_{j=1}^{p}$: $g(\\tau) = \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau)$. The basis functions $b_j(\\tau)$ are also causal and supported on $[0, T_b]$. The coefficients $\\{\\theta_j\\}_{j=1}^{p}$ are the parameters to be estimated.\n- **Predictor Form:** The stimulus contribution can be rewritten as a sum over basis components: $\\sum_{j=1}^{p} \\theta_j\\, (b_j * s)(t)$, where $(b_j * s)(t) \\equiv \\int_{0}^{T_b} b_j(\\tau)\\, s(t-\\tau)\\, d\\tau$.\n- **Time Discretization:** Time is discretized with a uniform bin width $\\Delta t > 0$, yielding a time grid $t_n = n\\, \\Delta t$ for $n = 1, 2, \\dots, N$.\n- **Signal Discretization:** The continuous signals are sampled on this grid: $s[n] \\equiv s(n\\, \\Delta t)$ and $b_j[\\ell] \\equiv b_j(\\ell\\, \\Delta t)$ for integer lag indices $\\ell \\in \\{0, 1, \\dots, L-1\\}$.\n- **Lag Definition:** The maximum number of lag steps is derived from the kernel support duration $T_b$ and the time step $\\Delta t$: $L \\equiv \\lfloor T_b / \\Delta t \\rfloor + 1$.\n- **Boundary Condition:** The stimulus signal is assumed to be zero-padded: $s[n] = 0$ if its time index falls outside the observed window.\n- **Approximation Method:** The continuous-time integral is to be approximated by a Riemann sum, retaining the factor $\\Delta t$ explicitly.\n- **Task:** Derive the expression for the $(n,j)$-th entry, $X_{n,j}$, of the design matrix $X$ in the discrete-time linear predictor representation $(X \\boldsymbol{\\theta})[n]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is firmly grounded in computational neuroscience and statistical signal processing. The use of a GLM with a basis function expansion to model neural firing is a standard and well-established technique (e.g., Pillow et al., 2008). The formulation is scientifically sound.\n- **Well-Posedness:** The problem is well-posed. It asks for the derivation of a discrete algebraic representation from a continuous integral formulation under a specified discretization scheme. The instructions are clear, and a unique answer is expected.\n- **Objectivity:** The problem statement is expressed in precise mathematical language, free from any subjective or ambiguous terminology.\n- **Flaw Analysis:**\n    1.  **Scientific/Factual Unsoundness:** None. The model is a cornerstone of modern neural data analysis.\n    2.  **Non-Formalizable/Irrelevant:** None. The problem is formal and directly relevant to the implementation of GLMs for spike trains.\n    3.  **Incomplete/Contradictory Setup:** None. All necessary components—the integral form, the basis expansion, the discretization rule (Riemann sum), and the definitions of discrete signals—are provided.\n    4.  **Unrealistic/Infeasible:** None. This is a standard procedure for fitting such models to data.\n    5.  **Ill-Posed/Poorly Structured:** None. The task is a straightforward mathematical derivation.\n    6.  **Pseudo-Profound/Trivial:** None. The derivation is a fundamental step in understanding and implementing these models.\n    7.  **Outside Scientific Verifiability:** None. The derivation is subject to standard mathematical verification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n***\n\n### Solution Derivation\n\nThe objective is to find the expression for the element $X_{n,j}$ of the design matrix $X$ in the discrete-time formulation of the stimulus-dependent part of a GLM's linear predictor.\n\nWe begin with the continuous-time expression for the stimulus-filtered component of the linear predictor at time $t$:\n$$u(t) = \\int_{0}^{T_b} g(\\tau)\\, s(t-\\tau)\\, d\\tau$$\nThe filter kernel $g(\\tau)$ is parameterized by a linear combination of basis functions $b_j(\\tau)$:\n$$g(\\tau) = \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau)$$\nSubstituting this expansion into the integral gives:\n$$u(t) = \\int_{0}^{T_b} \\left( \\sum_{j=1}^{p} \\theta_j\\, b_j(\\tau) \\right) s(t-\\tau)\\, d\\tau$$\nSince the summation is finite and the integral converges, we can interchange the order of summation and integration:\n$$u(t) = \\sum_{j=1}^{p} \\theta_j \\left( \\int_{0}^{T_b} b_j(\\tau)\\, s(t-\\tau)\\, d\\tau \\right)$$\nThis expression gives the continuous-time predictor. To obtain the discrete-time version, we evaluate $u(t)$ at the discrete time points $t_n = n\\, \\Delta t$:\n$$u(t_n) \\equiv u[n] = \\sum_{j=1}^{p} \\theta_j \\left( \\int_{0}^{T_b} b_j(\\tau)\\, s(t_n - \\tau)\\, d\\tau \\right)$$\nThe next step is to discretize the integral term. The problem specifies a Riemann sum approximation. Let's denote the integral for the $j$-th basis function as $I_j[n]$:\n$$I_j[n] = \\int_{0}^{T_b} b_j(\\tau)\\, s(t_n - \\tau)\\, d\\tau$$\nWe approximate this integral using a sum. We partition the integration interval $[0, T_b]$ into subintervals of width $\\Delta t$. The integration variable $\\tau$ is discretized as $\\tau_\\ell = \\ell\\, \\Delta t$, with the differential element $d\\tau$ approximated as $\\Delta t$. The summation will be over the discrete lag indices $\\ell$. The problem defines the discrete basis functions $b_j[\\ell]$ for $\\ell \\in \\{0, 1, \\dots, L-1\\}$, where $L = \\lfloor T_b / \\Delta t \\rfloor + 1$. This range of $\\ell$ covers the support of the kernel. A left-hand Riemann sum is appropriate here, evaluating the integrand at the start of each interval:\n$$I_j[n] \\approx \\sum_{\\ell=0}^{L-1} \\left[ b_j(\\ell\\, \\Delta t)\\, s(t_n - \\ell\\, \\Delta t) \\right] \\Delta t$$\nNote that the summation limit $L-1 = \\lfloor T_b / \\Delta t \\rfloor$ ensures that the argument $\\ell \\Delta t$ remains within or at the boundary of the support $[0, T_b]$.\n\nNow, we substitute the definitions of the discrete-time signals $s[k] \\equiv s(k\\, \\Delta t)$ and $b_j[\\ell] \\equiv b_j(\\ell\\, \\Delta t)$:\n- $b_j(\\ell\\, \\Delta t) = b_j[\\ell]$\n- $s(t_n - \\ell\\, \\Delta t) = s((n-\\ell)\\Delta t) = s[n-\\ell]$\n\nSubstituting these into the sum gives the discrete approximation of the convolution for the $j$-th basis component:\n$$I_j[n] \\approx \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell]$$\nThis expression represents the filtered stimulus through the $j$-th basis function at discrete time $n$.\n\nWe can now substitute this back into the expression for the discrete predictor $u[n]$:\n$$u[n] \\approx \\sum_{j=1}^{p} \\theta_j \\left( \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell] \\right)$$\nThe problem asks to represent this in the form of a matrix-vector product, $(X \\boldsymbol{\\theta})[n]$, where $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2, \\dots, \\theta_p]^T$ is the column vector of parameters. The $n$-th element of this product is given by the dot product of the $n$-th row of the design matrix $X$ with the vector $\\boldsymbol{\\theta}$:\n$$(X \\boldsymbol{\\theta})[n] = \\sum_{j=1}^{p} X_{n,j}\\, \\theta_j$$\nBy comparing this target form with our derived expression for $u[n]$, we can identify the term that multiplies each $\\theta_j$.\n$$u[n] = \\sum_{j=1}^{p} \\underbrace{\\left( \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell] \\right)}_{X_{n,j}} \\theta_j$$\nBy direct inspection, the $(n,j)$-th entry of the design matrix $X$ is therefore:\n$$X_{n,j} = \\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell]$$\nThis is the closed-form expression for the design matrix entry. Each column $j$ of the matrix $X$ corresponds to the stimulus $s$ convolved with the $j$-th basis function $b_j$, and each row $n$ corresponds to a discrete time point.",
            "answer": "$$\\boxed{\\Delta t \\sum_{\\ell=0}^{L-1} b_j[\\ell]\\, s[n-\\ell]}$$"
        },
        {
            "introduction": "A key challenge in statistical modeling is choosing the right level of complexity; for neural data, this often means deciding whether to include a spike-history filter. This practice introduces the likelihood ratio test, a powerful statistical tool for comparing nested models and formally testing such hypotheses. By deriving the test statistic from the point-process likelihood , you will learn how to make principled decisions about model structure.",
            "id": "3983854",
            "problem": "Consider a single neuron observed over the interval $\\left[0, T\\right]$, producing a spike train represented by the counting process $\\{N(t): t \\in \\left[0, T\\right]\\}$ with jump times $\\{t_i\\}_{i=1}^{n}$, where $n = N(T)$. Let $\\mathcal{H}_t$ denote the history of the process up to time $t$. Assume a generalized linear model (GLM) for point processes with a log link for the conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$ of the form\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp\\!\\Big( \\alpha + x(t)^{\\top} \\beta + \\int_{0}^{\\infty} h(\\tau)\\, \\mathrm{d}N(t-\\tau) \\Big),\n$$\nwhere $x(t) \\in \\mathbb{R}^{p}$ is a known covariate process, $\\beta \\in \\mathbb{R}^{p}$ and $\\alpha \\in \\mathbb{R}$ are unknown nuisance parameters, and $h(\\tau)$ is an unknown history filter modeling spike-history effects. Suppose the history filter is parameterized via a known basis $\\{b_k(\\tau)\\}_{k=1}^{K}$ as $h(\\tau) = \\sum_{k=1}^{K} w_k\\, b_k(\\tau)$ with $w \\in \\mathbb{R}^{K}$. Define the corresponding filtered history covariates $s_k(t) \\equiv \\int_{0}^{\\infty} b_k(\\tau)\\, \\mathrm{d}N(t-\\tau)$, and let the natural parameter be $\\eta(t) \\equiv \\alpha + x(t)^{\\top} \\beta + \\sum_{k=1}^{K} w_k\\, s_k(t)$ so that $\\lambda(t \\mid \\mathcal{H}_t) = \\exp(\\eta(t))$.\n\nYou are to formulate a likelihood ratio test for the presence of the history filter. Work from first principles beginning with the definition of the conditional intensity and the general likelihood for simple point processes. Do not assume any closed-form estimators for the parameters. Clearly define the null hypothesis $H_0$ and the alternative $H_1$, and express the likelihoods in terms of the spike times $\\{t_i\\}_{i=1}^{n}$ and the integral of the conditional intensity. Then, derive the likelihood ratio test statistic under standard regularity conditions suitable for applying Wilks’ theorem, and write it explicitly in terms of the fitted natural parameters under the full and reduced models.\n\nYour final answer must be a single closed-form analytic expression for the likelihood ratio test statistic in terms of $\\{\\eta_{\\text{full}}(t), \\eta_{\\text{red}}(t)\\}$ evaluated at their respective maximum likelihood estimates; do not include its sampling distribution in the final answer. No numerical evaluation is required. If you introduce any additional notation, define it clearly. The answer has no units and does not require rounding.",
            "solution": "The task is to derive the likelihood ratio test statistic for the presence of a history filter in a generalized linear model (GLM) for a point process. We begin from the fundamental definition of the likelihood for a simple point process.\n\nLet the observed spike train on the interval $\\left[0, T\\right]$ be represented by the counting process $N(t)$ with $n = N(T)$ spike times $\\{t_i\\}_{i=1}^{n}$. For a point process governed by a conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$, where $\\mathcal{H}_t$ is the history up to time $t$, the likelihood of observing the spike train $\\{t_i\\}_{i=1}^{n}$ is given by:\n$$\nL = \\left( \\prod_{i=1}^{n} \\lambda(t_i \\mid \\mathcal{H}_{t_i}) \\right) \\exp\\left( -\\int_{0}^{T} \\lambda(t \\mid \\mathcal{H}_t) \\, \\mathrm{d}t \\right)\n$$\nThe log-likelihood, denoted $\\mathcal{L}L$, is therefore:\n$$\n\\mathcal{L}L = \\sum_{i=1}^{n} \\ln \\lambda(t_i \\mid \\mathcal{H}_{t_i}) - \\int_{0}^{T} \\lambda(t \\mid \\mathcal{H}_t) \\, \\mathrm{d}t\n$$\nIn the context of the specified GLM, the conditional intensity is related to a natural parameter $\\eta(t)$ via a log link function: $\\lambda(t \\mid \\mathcal{H}_t) = \\exp(\\eta(t))$. Substituting this into the log-likelihood expression yields:\n$$\n\\mathcal{L}L = \\sum_{i=1}^{n} \\eta(t_i) - \\int_{0}^{T} \\exp(\\eta(t)) \\, \\mathrm{d}t\n$$\nThe problem asks for a likelihood ratio test for the presence of the history filter $h(\\tau)$. The filter is parameterized as $h(\\tau) = \\sum_{k=1}^{K} w_k\\, b_k(\\tau)$. The presence of the filter is determined by the vector of weights $w = (w_1, \\dots, w_K)^{\\top}$. We can formulate the null and alternative hypotheses in terms of these weights.\n\nThe null hypothesis, $H_0$, corresponds to the absence of the history filter. This is equivalent to all the weights being zero.\n$$\nH_0: w = 0\n$$\nThe alternative hypothesis, $H_1$, corresponds to the presence of a history filter, meaning at least one weight is non-zero.\n$$\nH_1: w \\neq 0\n$$\nThese hypotheses define two nested models: a reduced model under $H_0$ and a full model under $H_1$.\n\nThe full model, corresponding to $H_1$, includes all parameters: an intercept $\\alpha$, a covariate parameter vector $\\beta \\in \\mathbb{R}^{p}$, and the history filter weights $w \\in \\mathbb{R}^{K}$. The total set of parameters is $\\theta_{\\text{full}} = (\\alpha, \\beta^{\\top}, w^{\\top})^{\\top}$. The natural parameter for the full model is:\n$$\n\\eta_{\\text{full}}(t; \\theta_{\\text{full}}) = \\alpha + x(t)^{\\top} \\beta + \\sum_{k=1}^{K} w_k s_k(t)\n$$\nwhere $s_k(t) = \\int_{0}^{\\infty} b_k(\\tau)\\, \\mathrm{d}N(t-\\tau)$. The log-likelihood for the full model is:\n$$\n\\mathcal{L}L_{\\text{full}}(\\theta_{\\text{full}}) = \\sum_{i=1}^{n} \\eta_{\\text{full}}(t_i; \\theta_{\\text{full}}) - \\int_0^T \\exp(\\eta_{\\text{full}}(t; \\theta_{\\text{full}})) \\, \\mathrm{d}t\n$$\n\nThe reduced model, corresponding to $H_0$, is a special case of the full model with the constraint $w=0$. The set of parameters is $\\theta_{\\text{red}} = (\\alpha, \\beta^{\\top})^{\\top}$. The natural parameter for the reduced model is:\n$$\n\\eta_{\\text{red}}(t; \\theta_{\\text{red}}) = \\alpha + x(t)^{\\top} \\beta\n$$\nThe log-likelihood for the reduced model is:\n$$\n\\mathcal{L}L_{\\text{red}}(\\theta_{\\text{red}}) = \\sum_{i=1}^{n} \\eta_{\\text{red}}(t_i; \\theta_{\\text{red}}) - \\int_0^T \\exp(\\eta_{\\text{red}}(t; \\theta_{\\text{red}})) \\, \\mathrm{d}t\n$$\nThe likelihood ratio test statistic, $D$, is defined as $-2$ times the logarithm of the ratio of the maximized likelihoods under $H_0$ and $H_1$.\n$$\nD = -2 \\ln \\frac{\\sup_{\\theta \\in H_0} L(\\theta)}{\\sup_{\\theta \\in H_1} L(\\theta)} = -2 \\ln \\frac{L(\\hat{\\theta}_{\\text{red}})}{L(\\hat{\\theta}_{\\text{full}})}\n$$\nwhere $\\hat{\\theta}_{\\text{red}}$ and $\\hat{\\theta}_{\\text{full}}$ are the maximum likelihood estimates (MLEs) of the parameters under the reduced and full models, respectively. In terms of log-likelihoods, this is:\n$$\nD = 2 \\left( \\mathcal{L}L(\\hat{\\theta}_{\\text{full}}) - \\mathcal{L}L(\\hat{\\theta}_{\\text{red}}) \\right)\n$$\nLet us define the fitted natural parameters. Let $\\hat{\\eta}_{\\text{full}}(t) = \\eta_{\\text{full}}(t; \\hat{\\theta}_{\\text{full}})$ be the natural parameter for the full model evaluated at its MLE. Similarly, let $\\hat{\\eta}_{\\text{red}}(t) = \\eta_{\\text{red}}(t; \\hat{\\theta}_{\\text{red}})$ be the natural parameter for the reduced model evaluated at its MLE. The maximized log-likelihoods can be written as:\n$$\n\\mathcal{L}L_{\\text{full}}(\\hat{\\theta}_{\\text{full}}) = \\sum_{i=1}^{n} \\hat{\\eta}_{\\text{full}}(t_i) - \\int_0^T \\exp(\\hat{\\eta}_{\\text{full}}(t)) \\, \\mathrm{d}t\n$$\n$$\n\\mathcal{L}L_{\\text{red}}(\\hat{\\theta}_{\\text{red}}) = \\sum_{i=1}^{n} \\hat{\\eta}_{\\text{red}}(t_i) - \\int_0^T \\exp(\\hat{\\eta}_{\\text{red}}(t)) \\, \\mathrm{d}t\n$$\nSubstituting these expressions into the formula for $D$, we get:\n$$\nD = 2 \\left( \\left[ \\sum_{i=1}^{n} \\hat{\\eta}_{\\text{full}}(t_i) - \\int_0^T \\exp(\\hat{\\eta}_{\\text{full}}(t)) \\, \\mathrm{d}t \\right] - \\left[ \\sum_{i=1}^{n} \\hat{\\eta}_{\\text{red}}(t_i) - \\int_0^T \\exp(\\hat{\\eta}_{\\text{red}}(t)) \\, \\mathrm{d}t \\right] \\right)\n$$\nRearranging the terms by grouping the sums and integrals gives the final expression for the likelihood ratio test statistic:\n$$\nD = 2 \\left( \\sum_{i=1}^{n} \\left( \\hat{\\eta}_{\\text{full}}(t_i) - \\hat{\\eta}_{\\text{red}}(t_i) \\right) - \\int_0^T \\left( \\exp(\\hat{\\eta}_{\\text{full}}(t)) - \\exp(\\hat{\\eta}_{\\text{red}}(t)) \\right) \\, \\mathrm{d}t \\right)\n$$\nUnder the regularity conditions for Wilks' theorem, this statistic $D$ is asymptotically distributed as a chi-squared random variable with $K$ degrees of freedom under $H_0$, where $K$ is the number of constrained parameters (the dimension of $w$).",
            "answer": "$$\n\\boxed{2 \\left( \\sum_{i=1}^{n} \\left( \\hat{\\eta}_{\\text{full}}(t_i) - \\hat{\\eta}_{\\text{red}}(t_i) \\right) - \\int_0^T \\left( \\exp(\\hat{\\eta}_{\\text{full}}(t)) - \\exp(\\hat{\\eta}_{\\text{red}}(t)) \\right) \\, \\mathrm{d}t \\right)}\n$$"
        },
        {
            "introduction": "After fitting a model, how can we be sure it accurately captures the dynamics of the spike train? The time-rescaling theorem provides a powerful and elegant answer to this question of goodness-of-fit. This exercise guides you through the derivation and application of this fundamental theorem , transforming spike intervals into a sequence that should be uniformly distributed if the model is correct.",
            "id": "3983810",
            "problem": "Consider a simple stationary point process model for a neuron's spike train indexed by spike times $\\{t_i\\}_{i=1}^{N}$ over a finite observation window, with conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$, where $\\mathcal{H}_t$ denotes the spike history up to time $t$. The conditional intensity function is defined by the fundamental limit $\\lambda(t \\mid \\mathcal{H}_t) = \\lim_{\\Delta t \\to 0^{+}} \\frac{\\mathbb{P}\\{\\text{one spike in }[t, t+\\Delta t) \\mid \\mathcal{H}_t\\}}{\\Delta t}$, and the inter-spike interval survival function is given by the standard hazard-survival relation $S(s) = \\exp\\!\\left(-\\int_{0}^{s} \\lambda(t_{i-1}+u \\mid \\mathcal{H}_{t_{i-1}+u}) \\, du\\right)$, where $s$ is the elapsed time since the last spike at $t_{i-1}$.\n\nStarting only from these definitions and the well-tested fact that a nonhomogeneous Poisson process with rate $\\lambda(t)$ can be mapped to a homogeneous Poisson process of rate $1$ by the deterministic time change $\\tau(t) = \\int_{0}^{t} \\lambda(u) \\, du$, do the following:\n\n1. State the time-rescaling theorem for simple point processes with conditional intensity $\\lambda(t \\mid \\mathcal{H}_t)$, and derive the monotone transformation of inter-spike intervals that yields independent and identically distributed random variables with the Uniform$(0,1)$ law under a correctly specified model.\n\n2. Consider a Generalized Linear Model (GLM) point-process intensity with an exponential (log link) parameterization over the interval $[0, 0.6]$ seconds. There are two observed spikes at times $t_1 = 0.2$ and $t_2 = 0.6$ seconds. There is no covariate other than a spike-history term. The conditional intensity is\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp\\!\\left(\\beta_0 + \\beta_h \\, \\kappa(t - t_1) \\mathbf{1}\\{t > t_1\\}\\right),\n$$\nwhere $\\beta_0 \\in \\mathbb{R}$ and $\\beta_h \\in \\mathbb{R}$ are parameters, and the spike-history kernel is\n$$\n\\kappa(s) = \\begin{cases}\n1,  0 \\leq s  0.5, \\\\\n0,  s \\geq 0.5.\n\\end{cases}\n$$\nAssume the spike-train history is empty before $t_1$ (i.e., no spikes before $t_1$). Use the transformation you derived in part 1 to compute the two transformed variables associated with the inter-spike intervals $(t_0, t_1)$ and $(t_1, t_2)$, where $t_0 = 0$. Express your final answer for the pair of transformed values as a single closed-form analytic expression, in terms of $\\beta_0$ and $\\beta_h$, and present it as a row vector. Time is measured in seconds; the transformed variables are dimensionless. No numerical approximation or rounding is required; give exact expressions.",
            "solution": "This problem is valid as it is scientifically grounded in the theory of point processes and computational neuroscience, well-posed with a unique and meaningful solution, and objective in its formulation. All necessary information is provided, and there are no contradictions.\n\nThe problem is divided into two parts. The first part requires the statement and derivation of the transformation that converts inter-spike intervals (ISIs) from a general point process into independent and identically distributed (i.i.d.) Uniform$(0,1)$ random variables. The second part requires the application of this transformation to a specific Generalized Linear Model (GLM) for a spike train.\n\n**Part 1: The Time-Rescaling Transformation**\n\nThe time-rescaling theorem for a simple point process with a given conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$ provides a method for model assessment. If the model for the conditional intensity is correct, a specific transformation of the observed spike times will result in a set of i.i.d. random variables following a known distribution.\n\nLet $\\{t_i\\}$ be the sequence of spike times, with $t_0$ as the start of the observation period. The $i$-th inter-spike interval is $\\Delta t_i = t_i - t_{i-1}$. Let $\\Delta T_i$ be the random variable corresponding to this interval. The conditional intensity $\\lambda(t \\mid \\mathcal{H}_t)$ acts as the hazard rate for the process at time $t$. Given the history up to the previous spike at $t_{i-1}$, the probability of a spike in the next small interval of time depends on this function.\n\nThe survival function $S(s \\mid \\mathcal{H}_{t_{i-1}})$, which is the probability that the $i$-th ISI is greater than a duration $s$, is given by the product integral of the hazard function. The problem provides this relation:\n$$\nS(s \\mid \\mathcal{H}_{t_{i-1}}) = \\mathbb{P}(\\Delta T_i > s \\mid \\mathcal{H}_{t_{i-1}}) = \\exp\\left(-\\int_{0}^{s} \\lambda(t_{i-1}+u \\mid \\mathcal{H}_{t_{i-1}+u}) \\, du\\right)\n$$\nBy performing a change of variables $v = t_{i-1}+u$, the integral can be rewritten over the absolute time axis:\n$$\nS(s \\mid \\mathcal{H}_{t_{i-1}}) = \\exp\\left(-\\int_{t_{i-1}}^{t_{i-1}+s} \\lambda(v \\mid \\mathcal{H}_v) \\, dv\\right)\n$$\nThe cumulative distribution function (CDF) of the ISI, $F(s \\mid \\mathcal{H}_{t_{i-1}}) = \\mathbb{P}(\\Delta T_i \\leq s \\mid \\mathcal{H}_{t_{i-1}})$, is simply $1 - S(s \\mid \\mathcal{H}_{t_{i-1}})$. Therefore,\n$$\nF(s \\mid \\mathcal{H}_{t_{i-1}}) = 1 - \\exp\\left(-\\int_{t_{i-1}}^{t_{i-1}+s} \\lambda(v \\mid \\mathcal{H}_v) \\, dv\\right)\n$$\nAccording to the probability integral transform (PIT) theorem, if $X$ is a continuous random variable with CDF $F_X(x)$, then the random variable $Y = F_X(X)$ is uniformly distributed on the interval $(0,1)$.\n\nApplying the PIT to our case, we can define a set of transformed variables $z_i$ by evaluating the conditional CDF of each ISI at the observed ISI duration $\\Delta t_i = t_i - t_{i-1}$.\n$$\nz_i = F(\\Delta t_i \\mid \\mathcal{H}_{t_{i-1}}) = 1 - \\exp\\left(-\\int_{t_{i-1}}^{t_i} \\lambda(v \\mid \\mathcal{H}_v) \\, dv\\right)\n$$\nIf the model $\\lambda(t \\mid \\mathcal{H}_t)$ is a correct specification of the true data-generating process, the resulting sequence of variables $\\{z_i\\}$ will be independent and identically distributed according to the Uniform$(0,1)$ distribution. This transformation is the one required by the problem.\n\nThis result connects to the time-rescaling for a nonhomogeneous Poisson process. The integral $\\tau_i = \\int_{t_{i-1}}^{t_i} \\lambda(v \\mid \\mathcal{H}_v) \\, dv$ can be interpreted as the \"rescaled time\". The theorem states that these rescaled times $\\{\\tau_i\\}$ are i.i.d. random variables from a standard exponential distribution with rate $1$. The CDF of an Exponential$(1)$ distribution is $F(x) = 1 - \\exp(-x)$. Applying the PIT to the variables $\\tau_i$ gives $z_i = F(\\tau_i) = 1-\\exp(-\\tau_i)$, yielding the same Uniform$(0,1)$ variables.\n\n**Part 2: Application to the GLM Spike Train**\n\nWe are given a specific GLM for the conditional intensity over the interval $[0, 0.6]$ seconds:\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp\\left(\\beta_0 + \\beta_h \\, \\kappa(t - t_1) \\mathbf{1}\\{t > t_1\\}\\right)\n$$\nwith a spike-history kernel\n$$\n\\kappa(s) = \\begin{cases} 1,  0 \\leq s  0.5 \\\\ 0,  s \\geq 0.5 \\end{cases}\n$$\nThe observed spikes are at $t_1 = 0.2$ and $t_2 = 0.6$. The observation starts at $t_0 = 0$. We need to compute the transformed variables $z_1$ and $z_2$ for the two inter-spike intervals $(t_0, t_1)$ and $(t_1, t_2)$.\n\n**First Interval: $(t_0, t_1) = (0, 0.2)$**\n\nThe first transformed variable is $z_1 = 1 - \\exp\\left(-\\int_{0}^{0.2} \\lambda(t \\mid \\mathcal{H}_t) \\, dt\\right)$.\nFor $t \\in [0, 0.2]$, the history $\\mathcal{H}_t$ is empty, and the condition $t > t_1$ (i.e., $t > 0.2$) is false. The indicator function $\\mathbf{1}\\{t > 0.2\\}$ is $0$.\nTherefore, the conditional intensity simplifies to the baseline rate:\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp(\\beta_0 + \\beta_h \\cdot 0) = \\exp(\\beta_0) \\quad \\text{for } t \\in [0, 0.2]\n$$\nThe integral of the intensity over this interval is:\n$$\n\\int_{0}^{0.2} \\exp(\\beta_0) \\, dt = \\exp(\\beta_0) \\int_{0}^{0.2} dt = 0.2 \\exp(\\beta_0)\n$$\nThe first transformed variable is thus:\n$$\nz_1 = 1 - \\exp(-0.2 \\exp(\\beta_0))\n$$\n\n**Second Interval: $(t_1, t_2) = (0.2, 0.6)$**\n\nThe second transformed variable is $z_2 = 1 - \\exp\\left(-\\int_{0.2}^{0.6} \\lambda(t \\mid \\mathcal{H}_t) \\, dt\\right)$.\nFor $t \\in (0.2, 0.6]$, the history $\\mathcal{H}_t$ contains the spike at $t_1 = 0.2$. The condition $t > t_1$ is true, so the indicator function $\\mathbf{1}\\{t > 0.2\\}$ is $1$.\nThe argument to the kernel function is $s = t - t_1 = t - 0.2$. As $t$ ranges from $0.2$ to $0.6$, $s$ ranges from $0$ to $0.4$.\nAccording to the definition of $\\kappa(s)$, for $s \\in [0, 0.4]$, which is a subset of $[0, 0.5)$, we have $\\kappa(s) = 1$.\nTherefore, the conditional intensity for $t \\in (0.2, 0.6]$ is constant:\n$$\n\\lambda(t \\mid \\mathcal{H}_t) = \\exp(\\beta_0 + \\beta_h \\cdot 1) = \\exp(\\beta_0 + \\beta_h) \\quad \\text{for } t \\in (0.2, 0.6]\n$$\nThe integral of the intensity over this interval is:\n$$\n\\int_{0.2}^{0.6} \\exp(\\beta_0 + \\beta_h) \\, dt = \\exp(\\beta_0 + \\beta_h) \\int_{0.2}^{0.6} dt = (0.6 - 0.2) \\exp(\\beta_0 + \\beta_h) = 0.4 \\exp(\\beta_0 + \\beta_h)\n$$\nThe second transformed variable is thus:\n$$\nz_2 = 1 - \\exp(-0.4 \\exp(\\beta_0 + \\beta_h))\n$$\nThe pair of transformed values $(z_1, z_2)$ is to be presented as a row vector.\n$$\n\\begin{pmatrix} z_1  z_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\exp(-0.2 \\exp(\\beta_0))  1 - \\exp(-0.4 \\exp(\\beta_0 + \\beta_h)) \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 - \\exp(-0.2 \\exp(\\beta_0))  1 - \\exp(-0.4 \\exp(\\beta_0 + \\beta_h)) \\end{pmatrix}}\n$$"
        }
    ]
}