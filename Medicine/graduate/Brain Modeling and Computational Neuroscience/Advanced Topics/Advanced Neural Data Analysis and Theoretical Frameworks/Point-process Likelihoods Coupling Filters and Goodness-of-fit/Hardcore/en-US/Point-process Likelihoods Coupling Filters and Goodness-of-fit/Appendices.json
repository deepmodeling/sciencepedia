{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration, we start with the simplest possible model of a spike train: the homogeneous Poisson process. This model assumes a neuron fires with a constant average rate, independent of its history or external stimuli. This exercise  is fundamental, as it guides you through the derivation of the point-process likelihood and the analytical calculation of the maximum likelihood estimate (MLE) for the neuron's firing rate. Mastering this foundational case provides the essential intuition for how statistical estimation connects theoretical models to observed neural data.",
            "id": "4010017",
            "problem": "You observe a single-neuron spike train over the interval $\\left[0, T\\right]$, with $T0$ measured in seconds, and recorded spike times $\\{t_i\\}_{i=1}^{N}$ satisfying $0  t_1  t_2  \\cdots  t_N  T$, where $N \\in \\mathbb{N}$ is the total number of spikes observed. Assume the spike train is generated by a simple point process with a well-defined conditional intensity function $\\lambda(t \\mid \\mathcal{H}_t)$, where $\\mathcal{H}_t$ denotes the history of the process up to time $t$. Consider the model class in which the conditional intensity is constant in time and independent of history, i.e., $\\lambda(t \\mid \\mathcal{H}_t) = \\lambda_0$ for all $t \\in [0, T]$, with $\\lambda_0  0$ having units of $\\text{s}^{-1}$. \n\nStarting from the core definition of the conditional intensity function for a simple point process—namely, that for sufficiently small $\\Delta t0$, the probability of an event in $\\left[t, t + \\Delta t\\right)$ given $\\mathcal{H}_t$ satisfies $\\mathbb{P}\\{\\text{event in } [t, t+\\Delta t) \\mid \\mathcal{H}_t\\} = \\lambda(t \\mid \\mathcal{H}_t)\\,\\Delta t + o(\\Delta t)$, and the probability of no event in that interval is $1 - \\lambda(t \\mid \\mathcal{H}_t)\\,\\Delta t + o(\\Delta t)$, derive the exact log-likelihood of the observed spike train under the constant-intensity model. Then, using this log-likelihood, analytically derive the maximum likelihood estimator (MLE) for $\\lambda_0$.\n\nExpress your final answer as a single closed-form analytic expression for the MLE. If any units are required, express them in $\\text{s}^{-1}$. No rounding is required.",
            "solution": "The problem requires the derivation of the log-likelihood for an observed spike train under a constant-intensity point process model, and subsequently, the derivation of the maximum likelihood estimator (MLE) for the model's single parameter, the constant intensity $\\lambda_0$.\n\nThe problem is valid as it is scientifically grounded in the theory of point processes and statistical estimation, which are fundamental to computational neuroscience. It is well-posed, objective, and contains all necessary information for a unique and meaningful solution.\n\nLet the observed spike train be the set of spike times $\\{t_i\\}_{i=1}^{N}$ over the interval $[0, T]$, where $N$ is the total count of observed spikes. The model assumes the spike train is generated by a process with a conditional intensity function that is constant and independent of the history $\\mathcal{H}_t$:\n$$ \\lambda(t \\mid \\mathcal{H}_t) = \\lambda_0 $$\nwhere $\\lambda_0$ is a positive constant with units of $\\text{s}^{-1}$. This model is also known as a homogeneous Poisson process.\n\nThe likelihood of observing a particular realization of a point process, specified by the set of event times $\\{t_i\\}_{i=1}^{N}$ in the interval $[0, T]$, is given by the general formula for a point process likelihood density:\n$$ L(\\theta; \\{t_i\\}_{i=1}^N) = \\left( \\prod_{i=1}^{N} \\lambda(t_i \\mid \\mathcal{H}_{t_i}; \\theta) \\right) \\exp\\left( -\\int_{0}^{T} \\lambda(s \\mid \\mathcal{H}_s; \\theta) \\, ds \\right) $$\nwhere $\\theta$ represents the set of model parameters.\n\nIn our specific case, the parameter set consists of only $\\lambda_0$, so $\\theta = \\{\\lambda_0\\}$. The conditional intensity function is simply $\\lambda(t \\mid \\mathcal{H}_t; \\lambda_0) = \\lambda_0$. We substitute this into the general likelihood formula.\n\nThe product term becomes:\n$$ \\prod_{i=1}^{N} \\lambda(t_i \\mid \\mathcal{H}_{t_i}; \\lambda_0) = \\prod_{i=1}^{N} \\lambda_0 = \\lambda_0^N $$\n\nThe integral in the exponent becomes:\n$$ \\int_{0}^{T} \\lambda(s \\mid \\mathcal{H}_s; \\lambda_0) \\, ds = \\int_{0}^{T} \\lambda_0 \\, ds = \\lambda_0 [s]_0^T = \\lambda_0 T $$\n\nSubstituting these two components back into the likelihood formula gives the likelihood function for the observed data under the constant-intensity model:\n$$ L(\\lambda_0; \\{t_i\\}_{i=1}^N, T) = \\lambda_0^N \\exp(-\\lambda_0 T) $$\nThis expression represents the joint probability density of observing $N$ spikes at the specific times $\\{t_i\\}$ and no other spikes in the interval $[0, T]$. Note that the likelihood depends on the data only through the sufficient statistics $N$ and $T$.\n\nThe first part of the task is to derive the log-likelihood. This is the natural logarithm of the likelihood function, denoted by $\\ell(\\lambda_0)$:\n$$ \\ell(\\lambda_0) = \\ln\\left( L(\\lambda_0) \\right) = \\ln\\left( \\lambda_0^N \\exp(-\\lambda_0 T) \\right) $$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(x^y) = y\\ln(x)$, we get:\n$$ \\ell(\\lambda_0) = \\ln(\\lambda_0^N) + \\ln(\\exp(-\\lambda_0 T)) $$\n$$ \\ell(\\lambda_0) = N \\ln(\\lambda_0) - \\lambda_0 T $$\nThis is the exact log-likelihood of the observed spike train under the constant-intensity model.\n\nThe second part of the task is to find the maximum likelihood estimator (MLE) for $\\lambda_0$. The MLE, which we will denote as $\\hat{\\lambda}_0$, is the value of $\\lambda_0$ that maximizes the log-likelihood function $\\ell(\\lambda_0)$. To find this value, we take the derivative of $\\ell(\\lambda_0)$ with respect to $\\lambda_0$ and set it to zero.\n$$ \\frac{d\\ell(\\lambda_0)}{d\\lambda_0} = \\frac{d}{d\\lambda_0} \\left( N \\ln(\\lambda_0) - \\lambda_0 T \\right) $$\nApplying the rules of differentiation:\n$$ \\frac{d\\ell(\\lambda_0)}{d\\lambda_0} = N \\left(\\frac{1}{\\lambda_0}\\right) - T = \\frac{N}{\\lambda_0} - T $$\nSetting this derivative to zero to find the critical point:\n$$ \\frac{N}{\\hat{\\lambda}_0} - T = 0 $$\n$$ \\frac{N}{\\hat{\\lambda}_0} = T $$\nSolving for $\\hat{\\lambda}_0$ gives the MLE:\n$$ \\hat{\\lambda}_0 = \\frac{N}{T} $$\n\nTo confirm that this value corresponds to a maximum, we must check the second derivative of the log-likelihood function:\n$$ \\frac{d^2\\ell(\\lambda_0)}{d\\lambda_0^2} = \\frac{d}{d\\lambda_0} \\left( \\frac{N}{\\lambda_0} - T \\right) = - \\frac{N}{\\lambda_0^2} $$\nThe problem states $N \\in \\mathbb{N}$, and typically spikes are observed, so we can assume $N > 0$. Since $\\lambda_0 > 0$ by definition, $\\lambda_0^2$ is positive. Therefore, the second derivative is negative for all valid values of $\\lambda_0$:\n$$ \\frac{d^2\\ell(\\lambda_0)}{d\\lambda_0^2} = - \\frac{N}{\\lambda_0^2}  0 $$\nA negative second derivative confirms that the log-likelihood function is concave and the critical point we found is indeed a maximum. In the case where $N=0$, the log-likelihood is $\\ell(\\lambda_0) = -\\lambda_0 T$, which is maximized as $\\lambda_0 \\rightarrow 0$. The formula $\\hat{\\lambda}_0 = N/T = 0/T = 0$ is consistent with this limiting case.\n\nThus, the maximum likelihood estimator for the constant intensity $\\lambda_0$ is the total number of spikes $N$ divided by the duration of the observation interval $T$. This result is intuitively pleasing: the best estimate for the rate of events is the observed average rate of events. The units of $\\hat{\\lambda}_0$ are spikes per unit time, or $\\text{s}^{-1}$, as required.",
            "answer": "$$\\boxed{\\frac{N}{T}}$$"
        },
        {
            "introduction": "Real neurons rarely fire with a constant rate; their activity is dynamically modulated by stimuli and their own recent spiking history. The Generalized Linear Model (GLM) provides a powerful and flexible framework for capturing these dependencies. This next practice  challenges you to derive the log-likelihood function and its gradient for a point-process GLM. These two quantities are the cornerstone of fitting GLMs to data, as they are the essential inputs for the numerical optimization algorithms used to find the model parameters that best explain a neuron's responses.",
            "id": "4009974",
            "problem": "Consider a single neuron observed over a fixed window $[0,T]$ with spike times $0  t_{1}  t_{2}  \\cdots  t_{N} \\leq T$. Its conditional intensity function $\\lambda(t \\mid \\mathcal{H}_{t})$ is modeled by a Generalized Linear Model (GLM). Specifically, assume\n$$\n\\lambda(t \\mid \\mathcal{H}_{t}) \\;=\\; \\exp\\!\\big\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t)\\big\\},\n$$\nwhere $\\beta_{0} \\in \\mathbb{R}$ is a baseline parameter, $\\mathbf{w} \\in \\mathbb{R}^{p}$ are coupling weights, and $\\mathbf{x}(t) \\in \\mathbb{R}^{p}$ are continuous-time covariates that may include stimulus-driven terms, spike-history terms constructed by convolving the neuron’s own spike train with a self-history filter, and coupling terms constructed by convolving other neurons’ spike trains with coupling filters. Assume $\\mathbf{x}(t)$ is piecewise continuous and integrable on $[0,T]$, and that $\\lambda(t \\mid \\mathcal{H}_{t})$ is well-defined and finite for all $t \\in [0,T]$.\n\nStarting from first principles for point processes, derive an explicit expression for the log-likelihood $\\ell(\\beta_{0},\\mathbf{w})$ of the observed spike train under this model, and then derive the gradient of the log-likelihood with respect to $\\mathbf{w}$. Your derivation must begin from fundamental definitions of conditional intensity and survival for a point process, not from any pre-stated closed-form likelihood or gradient expression. Express your final results as closed-form analytic expressions in terms of $\\beta_{0}$, $\\mathbf{w}$, $\\mathbf{x}(t)$, the spike times $\\{t_{i}\\}_{i=1}^{N}$, and the observation window $[0,T]$. The final answer must be a calculation in the form of exact analytic expressions. Do not introduce any regularization terms. If you find intermediate vector or matrix derivatives, you must carry them through analytically to yield the final expressions requested.",
            "solution": "The problem requires the derivation of the log-likelihood function and its gradient for a neural spike train modeled by a Generalized Linear Model (GLM) with an exponential link function. The derivation must begin from the first principles of point process theory.\n\n### Part 1: Derivation of the Log-Likelihood Function $\\ell(\\beta_0, \\mathbf{w})$\n\nA temporal point process on an interval $[0, T]$ is characterized by its conditional intensity function, $\\lambda(t \\mid \\mathcal{H}_{t})$, which represents the instantaneous probability of an event (a spike) at time $t$, given the history of previous events $\\mathcal{H}_{t}$.\n\nFrom first principles, the likelihood of observing a particular sequence of spike times $0  t_1  t_2  \\dots  t_N \\leq T$ can be constructed by considering the joint probability density of these events. This can be expressed as the product of the conditional densities for each spike, multiplied by the probability of no spikes occurring in the intervals between spikes and after the last spike.\n\nLet $t_0 = 0$. The probability density for observing the first spike at $t_1$ is given by the product of the instantaneous rate at $t_1$ and the probability of *no* spikes in the interval $(0, t_1]$. This \"survival\" probability is given by $S(t_1 \\mid 0)$. Thus, the density is $p(t_1) = \\lambda(t_1 \\mid \\mathcal{H}_{t_1}) S(t_1 \\mid 0)$.\n\nMore generally, the conditional probability density for the $i$-th spike to occur at $t_i$, given the $(i-1)$-th spike occurred at $t_{i-1}$, is:\n$$\np(t_i \\mid t_{i-1}, \\mathcal{H}_{t_{i-1}}) = \\lambda(t_i \\mid \\mathcal{H}_{t_i}) S(t_i \\mid t_{i-1})\n$$\nwhere $S(t_i \\mid t_{i-1})$ is the probability of surviving (i.e., not spiking) in the interval $(t_{i-1}, t_i]$. The survival function is related to the integrated conditional intensity:\n$$\nS(t_b \\mid t_a) = \\exp \\left( - \\int_{t_a}^{t_b} \\lambda(u \\mid \\mathcal{H}_u) du \\right)\n$$\n\nThe full likelihood $L$ for observing the spike train $\\{t_1, \\dots, t_N\\}$ in the window $[0, T]$ is the product of the conditional densities for each spike, multiplied by the survival probability from the last spike $t_N$ to the end of the observation window $T$:\n$$\nL = \\left( \\prod_{i=1}^{N} p(t_i \\mid t_{i-1}, \\mathcal{H}_{t_{i-1}}) \\right) S(T \\mid t_N)\n$$\nSubstituting the expressions for the conditional densities:\n$$\nL = \\left( \\prod_{i=1}^{N} \\lambda(t_i \\mid \\mathcal{H}_{t_i}) S(t_i \\mid t_{i-1}) \\right) S(T \\mid t_N)\n$$\nWe can group the terms:\n$$\nL = \\left( \\prod_{i=1}^{N} \\lambda(t_i \\mid \\mathcal{H}_{t_i}) \\right) \\left( \\prod_{i=1}^{N} S(t_i \\mid t_{i-1}) \\right) S(T \\mid t_N)\n$$\nThe product of the survival terms is:\n$$\n\\left( \\prod_{i=1}^{N} \\exp \\left( - \\int_{t_{i-1}}^{t_i} \\lambda(u \\mid \\mathcal{H}_u) du \\right) \\right) \\exp \\left( - \\int_{t_N}^{T} \\lambda(u \\mid \\mathcal{H}_u) du \\right)\n$$\n$$\n= \\exp \\left( - \\sum_{i=1}^{N} \\int_{t_{i-1}}^{t_i} \\lambda(u \\mid \\mathcal{H}_u) du - \\int_{t_N}^{T} \\lambda(u \\mid \\mathcal{H}_u) du \\right)\n$$\nBy the additivity of integrals, the sum in the exponent is simply the integral over the entire observation window $[0, T]$:\n$$\n= \\exp \\left( - \\int_{0}^{T} \\lambda(u \\mid \\mathcal{H}_u) du \\right)\n$$\nThus, the likelihood function is:\n$$\nL(\\beta_0, \\mathbf{w}) = \\left( \\prod_{i=1}^{N} \\lambda(t_i \\mid \\mathcal{H}_{t_i}) \\right) \\exp \\left( - \\int_{0}^{T} \\lambda(u \\mid \\mathcal{H}_u) du \\right)\n$$\nThe log-likelihood, $\\ell = \\ln(L)$, is then:\n$$\n\\ell(\\beta_0, \\mathbf{w}) = \\sum_{i=1}^{N} \\ln(\\lambda(t_i \\mid \\mathcal{H}_{t_i})) - \\int_{0}^{T} \\lambda(u \\mid \\mathcal{H}_u) du\n$$\nNow, we substitute the given GLM form for the conditional intensity: $\\lambda(t \\mid \\mathcal{H}_t) = \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t)\\}$.\nThe logarithm of the intensity at each spike time $t_i$ is:\n$$\n\\ln(\\lambda(t_i \\mid \\mathcal{H}_{t_i})) = \\ln \\left( \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t_i)\\} \\right) = \\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t_i)\n$$\nSubstituting this and the expression for $\\lambda(u)$ into the log-likelihood equation, we obtain the final expression for the log-likelihood:\n$$\n\\ell(\\beta_0, \\mathbf{w}) = \\sum_{i=1}^{N} \\left( \\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t_i) \\right) - \\int_{0}^{T} \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} du\n$$\n\n### Part 2: Derivation of the Gradient $\\nabla_{\\mathbf{w}} \\ell(\\beta_0, \\mathbf{w})$\n\nTo find the gradient of the log-likelihood with respect to the weight vector $\\mathbf{w}$, we differentiate $\\ell(\\beta_0, \\mathbf{w})$ with respect to $\\mathbf{w}$.\n$$\n\\nabla_{\\mathbf{w}} \\ell(\\beta_0, \\mathbf{w}) = \\nabla_{\\mathbf{w}} \\left( \\sum_{i=1}^{N} (\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t_i)) - \\int_{0}^{T} \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} du \\right)\n$$\nWe differentiate term by term. First, the summation term:\n$$\n\\nabla_{\\mathbf{w}} \\left( \\sum_{i=1}^{N} (\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t_i)) \\right) = \\sum_{i=1}^{N} \\nabla_{\\mathbf{w}} (\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t_i))\n$$\nSince $\\beta_0$ does not depend on $\\mathbf{w}$, and using the vector calculus identity $\\nabla_{\\mathbf{z}} (\\mathbf{z}^{\\top} \\mathbf{a}) = \\mathbf{a}$, we have:\n$$\n= \\sum_{i=1}^{N} \\mathbf{x}(t_i)\n$$\nNext, we differentiate the integral term. Since the limits of integration $0$ and $T$ are not functions of $\\mathbf{w}$, we can use the Leibniz integral rule (differentiation under the integral sign):\n$$\n\\nabla_{\\mathbf{w}} \\left( \\int_{0}^{T} \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} du \\right) = \\int_{0}^{T} \\nabla_{\\mathbf{w}} \\left( \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} \\right) du\n$$\nWe apply the chain rule to the integrand. Let $f(\\mathbf{w}) = \\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)$. The gradient of the outer exponential function is $\\nabla_{\\mathbf{w}} \\exp(f(\\mathbf{w})) = \\exp(f(\\mathbf{w})) \\nabla_{\\mathbf{w}} f(\\mathbf{w})$.\nThe gradient of the exponent is:\n$$\n\\nabla_{\\mathbf{w}} f(\\mathbf{w}) = \\nabla_{\\mathbf{w}} (\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)) = \\mathbf{x}(u)\n$$\nTherefore, the gradient of the integrand is:\n$$\n\\nabla_{\\mathbf{w}} \\left( \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} \\right) = \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} \\mathbf{x}(u) = \\lambda(u \\mid \\mathcal{H}_u) \\mathbf{x}(u)\n$$\nSubstituting this back into the integral, the gradient of the integral term is:\n$$\n\\int_{0}^{T} \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} \\mathbf{x}(u) du\n$$\nCombining the gradients of both parts, we obtain the gradient of the log-likelihood with respect to $\\mathbf{w}$:\n$$\n\\nabla_{\\mathbf{w}} \\ell(\\beta_0, \\mathbf{w}) = \\sum_{i=1}^{N} \\mathbf{x}(t_i) - \\int_{0}^{T} \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} \\mathbf{x}(u) du\n$$\nThis expression represents the difference between the sum of the covariate vectors at the observed spike times and their expectation integrated over the entire observation window, weighted by the model's predicted firing rate.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\displaystyle \\sum_{i=1}^{N} \\left( \\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(t_i) \\right) - \\int_{0}^{T} \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} du\n\n\\displaystyle \\sum_{i=1}^{N} \\mathbf{x}(t_i) - \\int_{0}^{T} \\exp\\{\\beta_{0} + \\mathbf{w}^{\\top} \\mathbf{x}(u)\\} \\mathbf{x}(u) du\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building and fitting a model is only half the battle; the crucial final step is to rigorously assess its performance. A good model should capture the statistical structure of the data, leaving behind only random, unpredictable noise. This exercise  provides hands-on practice with a standard technique for model validation: residual analysis. You will learn to compute Pearson residuals to quantify the moment-by-moment deviation between the observed spike counts and your model's predictions, and then use autocorrelation tests to check if these residuals contain any non-random structure that the model failed to explain.",
            "id": "4010013",
            "problem": "Consider a discrete-time approximation to a neuronal point process where time is partitioned into uniform bins of width $\\Delta$ seconds. Let $n_t$ denote the spike count in bin $t$, and let $\\lambda_t$ denote the fitted conditional intensity (in spikes per second) from a previously estimated model that may include stimulus drive and history coupling. Assume the following foundational facts: (i) under a correctly specified generalized linear model for the conditional intensity, the binned counts have independent increments conditioned on past history, (ii) for sufficiently small $\\Delta$, the expected count in bin $t$ is $\\mu_t \\approx \\lambda_t \\Delta$, and (iii) under the Poisson assumption for bin counts, deviations of $n_t$ from $\\mu_t$ can be standardized by the appropriate variance scaling. Starting from these bases, derive the time-binned Pearson residual from first principles, derive the sample autocorrelation function at a fixed lag for a sequence, and derive a portmanteau statistic that aggregates autocorrelations across lags to test the null hypothesis of no residual autocorrelation.\n\nYour program must:\n- Implement the derived Pearson residual for a sequence of binned counts $\\{n_t\\}_{t=1}^T$ and fitted intensities $\\{\\lambda_t\\}_{t=1}^T$ given a fixed bin width $\\Delta$. If any bin has $\\mu_t = \\lambda_t \\Delta = 0$, you must handle it numerically in a scientifically sound manner by introducing a vanishingly small positive constant $\\varepsilon$ to avoid division by zero.\n- Compute the sample autocorrelation of the Pearson residual sequence at lags $1$ through $L$, and then compute a portmanteau statistic aggregating these autocorrelations to test for residual autocorrelation under the null. Use a fixed significance level $\\alpha$ for the test and return a boolean indicating whether to reject the null (i.e., whether there is statistically significant autocorrelation remaining in the residuals).\n\nYou must use the following test suite with scientifically plausible parameters. All randomness must be made reproducible by using the specified pseudo-random seeds. Angles are not involved; time is measured in seconds, intensities in spikes per second.\n\nTest case 1 (well-specified model, independent increments):\n- Bin width $\\Delta = 0.01$.\n- Number of bins $T = 10000$.\n- Fitted intensity constant $\\lambda_t = 20$ for all $t$.\n- Generate counts $n_t$ as independent Poisson draws with mean $\\mu_t = \\lambda_t \\Delta$ using pseudo-random seed $12345$.\n- Maximum lag $L = 1$, significance level $\\alpha = 0.05$, and numerical stabilizer $\\varepsilon = 10^{-12}$.\n\nTest case 2 (model misfit with positive first-order dependence):\n- Bin width $\\Delta = 0.01$.\n- Number of bins $T = 4000$.\n- Fitted intensity constant $\\lambda_t = 20$ for all $t$.\n- Generate counts $n_t \\in \\{0,1\\}$ using a binary first-order Markov process with transition probabilities $P(n_t=1 \\mid n_{t-1}=0) = p_0$ and $P(n_t=1 \\mid n_{t-1}=1) = p_1$, where $p_0 = 0.1$ and $p_1 = 0.5$. Initialize $n_1 = 0$ and use pseudo-random seed $54321$ for subsequent draws.\n- Maximum lag $L = 1$, significance level $\\alpha = 0.05$, and numerical stabilizer $\\varepsilon = 10^{-12}$.\n\nTest case 3 (edge case with structural bursts and zero predicted intensity segments):\n- Bin width $\\Delta = 0.01$.\n- Number of bins $T = 800$.\n- Fitted intensity $\\lambda_t = 0$ for $t \\in \\{1,\\dots,100\\}$ and $\\lambda_t = 5$ for $t \\in \\{101,\\dots,800\\}$.\n- Deterministic counts $n_t = 0$ for $t \\in \\{1,\\dots,100\\}$; for $t \\in \\{101,\\dots,800\\}$, set $n_t$ to follow a repeating pattern of four ones followed by four zeros (i.e., $1,1,1,1,0,0,0,0$ repeated without randomness), to represent bursty structure not captured by the fitted intensity.\n- Maximum lag $L = 1$, significance level $\\alpha = 0.05$, and numerical stabilizer $\\varepsilon = 10^{-12}$.\n\nAlgorithmic requirements:\n- Derive the Pearson residual from the foundational assumptions and implement it to obtain a residual sequence $\\{r_t\\}_{t=1}^T$ for each test case.\n- Derive the sample autocorrelation estimator and implement the computation of autocorrelation at lags $1$ through $L$.\n- Derive and implement a portmanteau statistic that, under the null hypothesis of no autocorrelation, follows an appropriate reference distribution, and use it to compute a decision rule at significance level $\\alpha$ to output a boolean indicating whether to reject the null.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each entry is a boolean indicating, for the corresponding test case, whether statistically significant autocorrelation was detected in the Pearson residuals at the specified lags using the portmanteau test.\n\nYour implementation must be self-contained and must not require any user input. The final output format must strictly match the specification above.",
            "solution": "We begin with a point process model observed in discrete time. Let $\\Delta$ denote the bin width in seconds, let $n_t$ denote the spike count in bin $t$, and let $\\lambda(t \\mid \\mathcal{H}_t)$ denote the conditional intensity at time $t$ given the history $\\mathcal{H}_t$. Over a bin where the intensity is approximately constant, we denote $\\lambda_t \\approx \\lambda(t \\mid \\mathcal{H}_t)$ and use the well-tested approximation that the expected count in bin $t$ is $\\mu_t = \\mathbb{E}[n_t \\mid \\mathcal{H}_t] \\approx \\lambda_t \\Delta$. Under the Poisson model for counts in small bins, we have $\\operatorname{Var}(n_t \\mid \\mathcal{H}_t) \\approx \\mu_t$ and, conditional on history, increments are independent when the model is correctly specified.\n\nFrom these bases, a standardized residual that compares observed counts $n_t$ to their expected counts $\\mu_t$ is the Pearson residual. To derive it, define the deviation $d_t = n_t - \\mu_t$ and note that the appropriate variance scale under the Poisson assumption per bin is $\\sigma_t^2 \\approx \\mu_t$. The standardized deviation is therefore\n$$\nr_t = \\frac{d_t}{\\sqrt{\\sigma_t^2}} \\approx \\frac{n_t - \\mu_t}{\\sqrt{\\mu_t}}.\n$$\nThis sequence $\\{r_t\\}$ should be approximately uncorrelated and mean-zero under a well-specified model. Numerically, to avoid division by zero in bins where $\\mu_t = 0$, we introduce a vanishingly small stabilizer $\\varepsilon  0$ and use\n$$\nr_t = \\frac{n_t - \\mu_t}{\\sqrt{\\mu_t + \\varepsilon}},\n$$\nwhich recovers the standard definition when $\\mu_t  0$ and yields $r_t \\approx 0$ when $n_t = \\mu_t = 0$.\n\nTo assess remaining structure, we measure autocorrelation in the residuals. For a finite sequence $\\{r_t\\}_{t=1}^T$ with sample mean $\\bar{r} = \\frac{1}{T} \\sum_{t=1}^T r_t$, the sample autocovariance at lag $k$ is\n$$\n\\hat{\\gamma}_k = \\frac{1}{T} \\sum_{t=k+1}^T (r_t - \\bar{r})(r_{t-k} - \\bar{r}),\n$$\nand the sample variance is\n$$\n\\hat{\\gamma}_0 = \\frac{1}{T} \\sum_{t=1}^T (r_t - \\bar{r})^2.\n$$\nThe sample autocorrelation at lag $k$ is then\n$$\n\\hat{\\rho}_k = \\frac{\\hat{\\gamma}_k}{\\hat{\\gamma}_0}.\n$$\nUnder the null hypothesis that the residuals are white noise, $\\hat{\\rho}_k$ should be close to $0$ for all small lags.\n\nFor a joint test across multiple lags, we use a portmanteau statistic derived from aggregating autocorrelations. A widely used choice is the Box–Ljung statistic, defined for a maximum lag $L$ by\n$$\nQ = T(T+2)\\sum_{k=1}^L \\frac{\\hat{\\rho}_k^2}{T-k}.\n$$\nUnder the null of no autocorrelation, $Q$ is approximately distributed as a chi-square random variable with $L$ degrees of freedom for large $T$. Thus, a principled decision rule at significance level $\\alpha$ is to reject the null when\n$$\nQ  \\chi^2_{1-\\alpha}(L),\n$$\nwhere $\\chi^2_{1-\\alpha}(L)$ is the $(1-\\alpha)$ quantile of the chi-square distribution with $L$ degrees of freedom. Equivalently, we compute the $p$-value via the survival function and reject the null when $p  \\alpha$.\n\nAlgorithmic design:\n- For each test case, compute $\\mu_t = \\lambda_t \\Delta$ for all $t$.\n- Compute Pearson residuals via $r_t = \\frac{n_t - \\mu_t}{\\sqrt{\\mu_t + \\varepsilon}}$.\n- Compute the sample mean $\\bar{r}$ and then $\\hat{\\rho}_k$ for $k \\in \\{1,\\dots,L\\}$ as defined above.\n- Compute the Box–Ljung $Q$ statistic by summing the scaled squared autocorrelations.\n- Using the chi-square reference distribution with $L$ degrees of freedom, compute the $p$-value and return a boolean indicating whether $p  \\alpha$.\n\nTest suite interpretation and scientific realism:\n- In test case $1$, counts are generated from a Poisson model with the same expected counts as the fitted intensity, so the residuals should be approximately white and the test should typically not reject the null.\n- In test case $2$, counts are generated from a binary Markov process with positive dependence, while the fitted intensity is constant and does not incorporate coupling, so the residuals should show positive lag-$1$ autocorrelation, leading to rejection.\n- In test case $3$, counts have deterministic burst structure with stretches of ones and zeros, and the fitted intensity fails to capture the structure (including a segment with $\\lambda_t = 0$). The residuals will exhibit positive autocorrelation within bursts, and the portmanteau test should reject, while $\\varepsilon$ ensures numerically stable residuals in zero-intensity bins.\n\nThe program implements these steps for the specified parameters and prints a single line in the format $[result1,result2,result3]$, where each entry is a boolean indicating whether residual autocorrelation was detected at level $\\alpha$ using the Box–Ljung statistic at maximum lag $L$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef pearson_residuals(counts, lambdas, delta, epsilon=1e-12):\n    \"\"\"\n    Compute Pearson residuals r_t = (n_t - mu_t) / sqrt(mu_t + epsilon),\n    where mu_t = lambda_t * delta.\n    \"\"\"\n    mu = lambdas * delta\n    # Residuals with numerical stabilizer for zero expected counts\n    resid = (counts - mu) / np.sqrt(mu + epsilon)\n    return resid\n\ndef sample_autocorrelations(x, max_lag):\n    \"\"\"\n    Compute sample autocorrelations rho_k for k=1..max_lag.\n    Uses mean-centered sequence and variance normalization.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    T = x.shape[0]\n    m = x.mean()\n    x_centered = x - m\n    var = np.sum(x_centered ** 2) / T\n    # If variance is numerically zero, return zeros to avoid division by zero\n    if var == 0.0:\n        return np.zeros(max_lag, dtype=float)\n    rhos = []\n    for k in range(1, max_lag + 1):\n        cov = np.sum(x_centered[k:] * x_centered[:-k]) / T\n        rhos.append(cov / var)\n    return np.array(rhos, dtype=float)\n\ndef box_ljung_q(rhos, T):\n    \"\"\"\n    Compute the Box-Ljung Q statistic for given autocorrelations rhos (lags 1..L)\n    and series length T.\n    Q = T(T+2) sum_{k=1}^L rhos_k^2 / (T - k)\n    \"\"\"\n    L = len(rhos)\n    denom = np.array([T - k for k in range(1, L + 1)], dtype=float)\n    Q = T * (T + 2) * np.sum((rhos ** 2) / denom)\n    return Q\n\ndef test_case_1():\n    # Well-specified model: Poisson counts matching fitted intensity\n    delta = 0.01\n    T = 10000\n    lam = 20.0\n    lambdas = np.full(T, lam, dtype=float)\n    rng = np.random.default_rng(12345)\n    mu = lam * delta\n    counts = rng.poisson(mu, size=T)\n    L = 1\n    alpha = 0.05\n    eps = 1e-12\n    resid = pearson_residuals(counts, lambdas, delta, eps)\n    rhos = sample_autocorrelations(resid, L)\n    Q = box_ljung_q(rhos, T)\n    pval = chi2.sf(Q, df=L)\n    return pval  alpha\n\ndef test_case_2():\n    # Model misfit: binary Markov counts with positive dependence; fitted intensity constant\n    delta = 0.01\n    T = 4000\n    lam = 20.0\n    lambdas = np.full(T, lam, dtype=float)\n    p0 = 0.1\n    p1 = 0.5\n    rng = np.random.default_rng(54321)\n    counts = np.zeros(T, dtype=int)\n    # Initialize state\n    state = 0\n    counts[0] = state\n    for t in range(1, T):\n        if state == 0:\n            state = 1 if rng.random()  p0 else 0\n        else:\n            state = 1 if rng.random()  p1 else 0\n        counts[t] = state\n    L = 1\n    alpha = 0.05\n    eps = 1e-12\n    resid = pearson_residuals(counts, lambdas, delta, eps)\n    rhos = sample_autocorrelations(resid, L)\n    Q = box_ljung_q(rhos, T)\n    pval = chi2.sf(Q, df=L)\n    return pval  alpha\n\ndef test_case_3():\n    # Edge case: zero predicted intensity segment and deterministic bursts\n    delta = 0.01\n    T = 800\n    lambdas = np.zeros(T, dtype=float)\n    lambdas[100:] = 5.0\n    counts = np.zeros(T, dtype=int)\n    # Bursty pattern: for t >= 101, repeat [1,1,1,1,0,0,0,0]\n    pattern = [1, 1, 1, 1, 0, 0, 0, 0]\n    idx = 100\n    while idx  T:\n        for v in pattern:\n            if idx >= T:\n                break\n            counts[idx] = v\n            idx += 1\n    L = 1\n    alpha = 0.05\n    eps = 1e-12\n    resid = pearson_residuals(counts, lambdas, delta, eps)\n    rhos = sample_autocorrelations(resid, L)\n    Q = box_ljung_q(rhos, T)\n    pval = chi2.sf(Q, df=L)\n    return pval  alpha\n\ndef solve():\n    results = []\n    results.append(test_case_1())\n    results.append(test_case_2())\n    results.append(test_case_3())\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}