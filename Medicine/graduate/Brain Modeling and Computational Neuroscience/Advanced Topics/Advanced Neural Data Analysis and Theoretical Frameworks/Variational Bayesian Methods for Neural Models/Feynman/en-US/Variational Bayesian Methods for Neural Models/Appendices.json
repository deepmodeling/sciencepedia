{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will first tackle a scenario where variational inference yields elegant, analytical solutions. In this exercise (), you will derive the coordinate ascent updates for a hierarchical model of neural spiking data by applying the general rule for mean-field updates, $q^*(x) \\propto \\exp(\\mathbb{E}_{-x}[\\ln p(\\text{variables}, \\text{data})])$. This practice is crucial for understanding the core mechanism of variational inference in conjugate-exponential models, where the approximate posterior distributions retain the same functional form as their priors.",
            "id": "4032028",
            "problem": "Consider a population of $N$ neurons observed over a fixed window. Let $y_{i} \\in \\{0,1,2,\\dots\\}$ denote the spike count from neuron $i$ within a window of duration $s_{i} > 0$, for $i = 1,\\dots,N$. Assume a conditionally independent generative model in which each count $y_{i}$ is drawn from a Poisson distribution with neuron-specific rate $\\lambda_{i} > 0$ scaled by the window duration, and the rates are drawn from a hierarchical Gamma prior with a global rate parameter $b > 0$ shared across neurons. The model is\n$$\np(y,\\lambda,b) \\;=\\; \\prod_{i=1}^{N} p(y_{i} \\mid \\lambda_{i}) \\, p(\\lambda_{i} \\mid a,b) \\; p(b \\mid c,d),\n$$\nwhere\n$$\np(y_{i} \\mid \\lambda_{i}) \\;=\\; \\text{Poisson}(y_{i} \\mid s_{i} \\lambda_{i}), \\quad p(\\lambda_{i} \\mid a,b) \\;=\\; \\text{Gamma}(\\lambda_{i} \\mid a,b), \\quad p(b \\mid c,d) \\;=\\; \\text{Gamma}(b \\mid c,d).\n$$\nHere, $\\text{Poisson}(y \\mid \\mu)$ denotes the Poisson mass function $p(y \\mid \\mu) = \\exp(-\\mu)\\,\\mu^{y}/y!$, and $\\text{Gamma}(x \\mid \\alpha,\\beta)$ denotes the Gamma density with shape $\\alpha$ and rate $\\beta$, $p(x \\mid \\alpha,\\beta) = \\beta^{\\alpha}/\\Gamma(\\alpha)\\,x^{\\alpha - 1}\\exp(-\\beta x)$, with $\\Gamma(\\cdot)$ the Gamma function. All hyperparameters $a,c,d$ are known positive constants, and $s_{i}$ are known positive durations.\n\nRepresent this model on a factor graph with variable nodes $\\{\\lambda_{i}\\}_{i=1}^{N}$ and $b$, and factor nodes given by the likelihoods $\\{p(y_{i} \\mid \\lambda_{i})\\}_{i=1}^{N}$, the priors $\\{p(\\lambda_{i} \\mid a,b)\\}_{i=1}^{N}$, and the hyperprior $p(b \\mid c,d)$. Using the framework of Variational Message Passing (VMP), under a mean-field family $q(\\lambda,b) = \\prod_{i=1}^{N} q(\\lambda_{i})\\, q(b)$ restricted to conjugate-exponential factors consistent with the true model, derive the closed-form coordinate updates for $q(\\lambda_{i})$ and $q(b)$ that arise from the factor graph.\n\nThe derivation must start from foundational principles appropriate for variational Bayesian methods in computational neuroscience:\n- The Evidence Lower Bound (ELBO) definition for variational inference and its coordinate ascent characterization.\n- The mean-field factorization and the generic update $q(x) \\propto \\exp\\big(\\mathbb{E}_{-x}[\\ln p(x,\\text{others},\\text{data})]\\big)$.\n- The conjugate-exponential family structure of the Poisson-Gamma hierarchy and its implications for natural-parameter addition.\n\nYour final answer must be the pair of closed-form analytic expressions for the VMP updates at the variable nodes in terms of expectations of neighboring variables, expressed using only the model quantities $(y_{i}, s_{i}, a, c, d)$ and expectations under the current variational factors. Do not provide an inequality or an equation to be solved numerically. No numerical rounding or units are required for this problem. Express the final answer as a single analytical expression containing both updates, using the canonical Gamma-family parameterization specified above.",
            "solution": "The problem requires the derivation of the Variational Message Passing (VMP) coordinate ascent updates for the variational posterior distributions $q(\\lambda_i)$ and $q(b)$ for a hierarchical Poisson-Gamma model of neural spike counts. The derivation will be grounded in the principles of variational inference for conjugate-exponential models.\n\nFirst, we establish the structure of the model. The joint probability distribution over the observed data $y = \\{y_i\\}_{i=1}^{N}$, latent rates $\\lambda = \\{\\lambda_i\\}_{i=1}^{N}$, and the global rate parameter $b$ is given by:\n$$\np(y, \\lambda, b) = p(b \\mid c, d) \\prod_{i=1}^{N} p(y_i \\mid \\lambda_i) p(\\lambda_i \\mid a, b)\n$$\nThe component distributions are:\n-   $p(y_i \\mid \\lambda_i) = \\text{Poisson}(y_i \\mid s_i \\lambda_i) = \\frac{(s_i \\lambda_i)^{y_i} \\exp(-s_i \\lambda_i)}{y_i!}$\n-   $p(\\lambda_i \\mid a, b) = \\text{Gamma}(\\lambda_i \\mid a, b) = \\frac{b^a}{\\Gamma(a)} \\lambda_i^{a-1} \\exp(-b \\lambda_i)$\n-   $p(b \\mid c, d) = \\text{Gamma}(b \\mid c, d) = \\frac{d^c}{\\Gamma(c)} b^{c-1} \\exp(-d b)$\n\nThe model can be represented as a factor graph. The variables $\\lambda_1, \\dots, \\lambda_N$ and $b$ are variable nodes. The factors are $f_i(\\lambda_i) = p(y_i \\mid \\lambda_i)$ for $i=1, \\dots, N$, which each connect to a single variable node $\\lambda_i$; $g_i(\\lambda_i, b) = p(\\lambda_i \\mid a, b)$ for $i=1, \\dots, N$, which each connect to a pair of nodes $(\\lambda_i, b)$; and $h(b) = p(b \\mid c, d)$, which connects to the node $b$. This graph structure makes the conditional dependencies explicit: the update for $\\lambda_i$ will depend on its neighbors, which are the data $y_i$ and the global parameter $b$. The update for $b$ will depend on all the rates $\\lambda_i$ and its own prior parameters.\n\nVariational inference aims to approximate the true posterior $p(\\lambda, b \\mid y)$ with a factorized distribution $q(\\lambda, b)$ by maximizing the Evidence Lower Bound (ELBO). For a mean-field approximation $q(\\lambda, b) = q(b) \\prod_{i=1}^{N} q(\\lambda_i)$, the optimal distribution for a single factor $q_j$ (here, $q(\\lambda_i)$ or $q(b)$) while holding the others fixed is given by the coordinate ascent update rule:\n$$\n\\ln q^*(x_j) = \\mathbb{E}_{q_{-j}}[\\ln p(y, \\lambda, b)] + \\text{constant}\n$$\nwhere $\\mathbb{E}_{q_{-j}}$ denotes the expectation over all variables except $x_j$ under their current variational distributions. Because the model is constructed from conjugate-exponential families, the updated variational distributions $q^*(\\lambda_i)$ and $q^*(b)$ will remain in the same family as their priors (Gamma distributions).\n\nTo apply this rule, we first write the full log-joint probability:\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\ln p(y_i \\mid \\lambda_i) + \\sum_{i=1}^N \\ln p(\\lambda_i \\mid a, b) + \\ln p(b \\mid c, d)\n$$\nExpanding this, we get:\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\left( y_i \\ln(s_i \\lambda_i) - s_i \\lambda_i - \\ln(y_i!) \\right) + \\sum_{i=1}^N \\left( a \\ln b - \\ln\\Gamma(a) + (a-1)\\ln\\lambda_i - b\\lambda_i \\right) + \\left( c \\ln d - \\ln\\Gamma(c) + (c-1)\\ln b - db \\right)\n$$\nWe can drop terms that are constant with respect to all latent variables $\\lambda$ and $b$:\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\left( y_i \\ln\\lambda_i - s_i \\lambda_i + (a-1)\\ln\\lambda_i - b\\lambda_i \\right) + Na \\ln b + (c-1)\\ln b - db + \\text{constant}\n$$\nCollecting terms for each variable:\n$$\n\\ln p(y, \\lambda, b) = \\sum_{i=1}^N \\left[ (y_i + a - 1)\\ln\\lambda_i - (s_i + b)\\lambda_i \\right] + (Na + c - 1)\\ln b - db + \\text{constant}\n$$\n\n**Derivation of the update for $q(\\lambda_i)$**\nThe optimal variational distribution $q^*(\\lambda_i)$ is found by taking the expectation of the log-joint probability with respect to all other variables, which in this case is just $b$ under its current variational distribution $q(b)$.\n$$\n\\ln q^*(\\lambda_i) = \\mathbb{E}_{q(b)}[\\ln p(y, \\lambda, b)] + \\text{constant}_{\\lambda_i}\n$$\nWe only need terms from $\\ln p(y, \\lambda, b)$ that depend on $\\lambda_i$:\n$$\n\\ln q^*(\\lambda_i) \\propto \\mathbb{E}_{q(b)}\\left[ (y_i + a - 1)\\ln\\lambda_i - (s_i + b)\\lambda_i \\right]\n$$\n$$\n\\ln q^*(\\lambda_i) \\propto (y_i + a - 1)\\ln\\lambda_i - (s_i + \\mathbb{E}_{q}[b])\\lambda_i\n$$\nThis expression is the logarithm of the kernel of a Gamma distribution. The log-density of a $\\text{Gamma}(\\lambda_i \\mid \\tilde{a}_i, \\tilde{b}_i)$ distribution is $(\\tilde{a}_i - 1)\\ln\\lambda_i - \\tilde{b}_i\\lambda_i + \\text{constant}$. By comparison, we identify the updated parameters of the variational posterior for $\\lambda_i$:\nShape: $\\tilde{a}_i = y_i + a$\nRate: $\\tilde{b}_i = s_i + \\mathbb{E}_{q}[b]$\nThus, the updated variational distribution for $\\lambda_i$ is $q^*(\\lambda_i) = \\text{Gamma}(\\lambda_i \\mid y_i + a, s_i + \\mathbb{E}_{q}[b])$.\n\n**Derivation of the update for $q(b)$**\nSimilarly, the optimal variational distribution $q^*(b)$ is found by taking the expectation with respect to all $\\lambda_i$ under their current variational distributions $q(\\lambda_i)$.\n$$\n\\ln q^*(b) = \\mathbb{E}_{\\prod_j q(\\lambda_j)}[\\ln p(y, \\lambda, b)] + \\text{constant}_{b}\n$$\nWe isolate the terms from $\\ln p(y, \\lambda, b)$ that depend on $b$:\n$$\n\\ln q^*(b) \\propto \\mathbb{E}_{\\prod_j q(\\lambda_j)}\\left[ -\\sum_{i=1}^N b\\lambda_i + (Na + c - 1)\\ln b - db \\right]\n$$\n$$\n\\ln q^*(b) \\propto \\mathbb{E}_{\\prod_j q(\\lambda_j)}\\left[ (Na + c - 1)\\ln b - b \\left( d + \\sum_{i=1}^N \\lambda_i \\right) \\right]\n$$\nTaking the expectation with respect to the $q(\\lambda_i)$ distributions yields:\n$$\n\\ln q^*(b) \\propto (Na + c - 1)\\ln b - b \\left( d + \\sum_{i=1}^N \\mathbb{E}_{q}[\\lambda_i] \\right)\n$$\nThis is the log-kernel of a Gamma distribution for $b$. By comparing with the general form $(\\tilde{c}-1)\\ln b - \\tilde{d}b + \\text{constant}$, we identify the updated parameters:\nShape: $\\tilde{c} = Na + c$\nRate: $\\tilde{d} = d + \\sum_{i=1}^N \\mathbb{E}_{q}[\\lambda_i]$\nThus, the updated variational distribution for $b$ is $q^*(b) = \\text{Gamma}(b \\mid Na + c, d + \\sum_{i=1}^N \\mathbb{E}_{q}[\\lambda_i])$.\n\nThe expectations $\\mathbb{E}_{q}[b]$ and $\\mathbb{E}_{q}[\\lambda_i]$ are computed using the current parameters of the variational distributions. For a distribution $q(x) = \\text{Gamma}(x \\mid \\alpha, \\beta)$, the expectation is $\\mathbb{E}_{q}[x] = \\alpha/\\beta$. The VMP algorithm proceeds by iteratively applying these updates for each variable until the ELBO converges.\n\nThe resulting pair of VMP updates for the variational distributions at the variable nodes are the new functional forms for $q(\\lambda_i)$ and $q(b)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nq^*(\\lambda_i) = \\text{Gamma}(\\lambda_i \\mid y_i + a, s_i + \\mathbb{E}_{q}[b])\n&\nq^*(b) = \\text{Gamma}(b \\mid Na + c, d + \\sum_{i=1}^{N} \\mathbb{E}_{q}[\\lambda_i])\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While conjugate models provide an elegant starting point, many modern neural models are non-conjugate, requiring a more general approach to optimization. This next practice () challenges you to derive the stochastic gradient estimators for a latent variable model of neural activity, a cornerstone of techniques like the Variational Autoencoder (VAE). Mastering the reparameterization trick and the correct scaling for mini-batch updates is essential for applying variational methods to large-scale, complex datasets.",
            "id": "4032003",
            "problem": "Consider a latent-variable neural spike model with $N$ independent observations $\\{x_{i}\\}_{i=1}^{N}$, where each $x_{i} \\in \\mathbb{N}^{K}$ is a vector of spike counts for $K$ neurons collected over a fixed time window. The generative model introduces latent variables $z_{i} \\in \\mathbb{R}^{D}$ with a standard normal prior $p(z_{i}) = \\mathcal{N}(0, I)$, and a Poisson likelihood with rates parameterized by a linear-nonlinear mapping: for each neuron $k \\in \\{1,\\dots,K\\}$, the conditional distribution is\n$$\np_{\\theta}(x_{ik} \\mid z_{i}) = \\mathrm{Poisson}(\\lambda_{ik}), \\quad \\text{with} \\quad \\ln \\lambda_{ik} = (W z_{i} + b)_{k},\n$$\nwhere $\\theta = (W, b)$, $W \\in \\mathbb{R}^{K \\times D}$, and $b \\in \\mathbb{R}^{K}$. Let the variational family be amortized diagonal Gaussian, $q_{\\phi}(z_{i} \\mid x_{i}) = \\mathcal{N}\\!\\big(\\mu_{\\phi}(x_{i}), \\mathrm{diag}(\\sigma_{\\phi}(x_{i})^{2})\\big)$, with $\\mu_{\\phi} : \\mathbb{R}^{K} \\to \\mathbb{R}^{D}$ and $\\sigma_{\\phi} : \\mathbb{R}^{K} \\to \\mathbb{R}^{D}$ differentiable maps that depend on parameters $\\phi$. Use the standard reparameterization $z_{i} = \\mu_{\\phi}(x_{i}) + \\sigma_{\\phi}(x_{i}) \\odot \\epsilon_{i}$ with $\\epsilon_{i} \\sim \\mathcal{N}(0, I)$ and $\\odot$ denoting elementwise multiplication.\n\nStarting from the definition of the Evidence Lower Bound (ELBO),\n$$\n\\mathcal{L}(\\theta, \\phi) = \\sum_{i=1}^{N} \\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}\\!\\big[ \\ln p_{\\theta}(x_{i} \\mid z_{i}) + \\ln p(z_{i}) - \\ln q_{\\phi}(z_{i} \\mid x_{i}) \\big],\n$$\nderive stochastic gradient estimators of $\\nabla_{W} \\mathcal{L}$, $\\nabla_{b} \\mathcal{L}$, and $\\nabla_{\\phi} \\mathcal{L}$ under mini-batch subsampling. Assume a mini-batch $\\mathcal{B} \\subset \\{1,\\dots,N\\}$ of size $m$ is drawn uniformly at random without replacement, and for each $i \\in \\mathcal{B}$, $L$ independent standard normal samples $\\{\\epsilon_{i}^{(\\ell)}\\}_{\\ell=1}^{L}$ are used to form Monte Carlo samples $z_{i}^{(\\ell)} = \\mu_{\\phi}(x_{i}) + \\sigma_{\\phi}(x_{i}) \\odot \\epsilon_{i}^{(\\ell)}$. Express the stochastic gradient estimators explicitly in terms of $N$, $m$, $L$, $W$, $b$, $\\mu_{\\phi}(x_{i})$, $\\sigma_{\\phi}(x_{i})$, $x_{i}$, and $\\epsilon_{i}^{(\\ell)}$, and present the scaling rule(s) necessary to ensure unbiasedness with respect to the full-data ELBO.\n\nYou must derive your expressions from first principles, beginning with the ELBO definition and the independence structure of the model, without invoking pre-packaged gradient identities. Clarify how the reparameterization enters the gradient with respect to $\\phi$, and use the analytical form of the Kullback–Leibler divergence for diagonal Gaussian $q_{\\phi}(z_{i} \\mid x_{i})$ versus standard normal $p(z_{i})$ to obtain the part of the gradient that does not require Monte Carlo sampling. Conclude by giving a single closed-form analytic expression for the unbiased stochastic gradients of $\\nabla_{W} \\mathcal{L}$, $\\nabla_{b} \\mathcal{L}$, and $\\nabla_{\\phi} \\mathcal{L}$ in a compact form. No numerical approximation or rounding is required, and no units are involved. Use $\\,\\ln\\,$ for the natural logarithm, and define any Jacobians you use explicitly in your derivation.",
            "solution": "The Evidence Lower Bound (ELBO) for the entire dataset $\\{x_i\\}_{i=1}^N$ is given by:\n$$\n\\mathcal{L}(\\theta, \\phi) = \\sum_{i=1}^{N} \\mathcal{L}_i(\\theta, \\phi) = \\sum_{i=1}^{N} \\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}\\!\\big[ \\ln p_{\\theta}(x_{i} \\mid z_{i}) + \\ln p(z_{i}) - \\ln q_{\\phi}(z_{i} \\mid x_{i}) \\big]\n$$\nwhere $\\theta = (W, b)$. The term within the expectation can be split. The ELBO for a single data point $x_i$ can be written as the sum of an expected log-likelihood term and a Kullback-Leibler (KL) divergence term:\n$$\n\\mathcal{L}_i(\\theta, \\phi) = \\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}\\!\\big[ \\ln p_{\\theta}(x_{i} \\mid z_{i}) \\big] - D_{KL}\\big( q_{\\phi}(z_{i} \\mid x_{i}) \\,\\|\\, p(z_{i}) \\big)\n$$\nThe gradient of the total ELBO is the sum of the gradients for each data point: $\\nabla \\mathcal{L} = \\sum_{i=1}^{N} \\nabla \\mathcal{L}_i$. To form an unbiased stochastic estimator of this gradient using a mini-batch $\\mathcal{B} \\subset \\{1,\\dots,N\\}$ of size $m$, we average the gradients for the data points in the mini-batch and scale the result by $\\frac{N}{m}$. That is, $\\widehat{\\nabla \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\nabla \\mathcal{L}_i$. Our task is to derive the expressions for $\\nabla_{W} \\mathcal{L}_i$, $\\nabla_{b} \\mathcal{L}_i$, and $\\nabla_{\\phi} \\mathcal{L}_i$.\n\nFirst, we find the analytical form of the KL divergence term. The prior is $p(z_i) = \\mathcal{N}(0, I)$ and the variational posterior is $q_{\\phi}(z_i \\mid x_i) = \\mathcal{N}(z_i \\mid \\mu_i, \\mathrm{diag}(\\sigma_i^2))$, where for brevity we denote $\\mu_i = \\mu_{\\phi}(x_i)$ and $\\sigma_i = \\sigma_{\\phi}(x_i)$. The KL divergence between two multivariate Gaussian distributions $\\mathcal{N}(\\mu_1, \\Sigma_1)$ and $\\mathcal{N}(\\mu_2, \\Sigma_2)$ is $D_{KL}(\\mathcal{N}_1 \\| \\mathcal{N}_2) = \\frac{1}{2} \\left[ \\mathrm{tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2-\\mu_1)^T\\Sigma_2^{-1}(\\mu_2-\\mu_1) - D + \\ln\\left(\\frac{\\det\\Sigma_2}{\\det\\Sigma_1}\\right) \\right]$.\nFor our specific case, this simplifies to:\n$$\nD_{KL}\\big( q_{\\phi}(z_{i} \\mid x_{i}) \\,\\|\\, p(z_{i}) \\big) = \\frac{1}{2} \\sum_{j=1}^{D} \\left( \\mu_{ij}^{2} + \\sigma_{ij}^{2} - 2\\ln(\\sigma_{ij}) - 1 \\right)\n$$\nwhere $\\mu_{ij}$ and $\\sigma_{ij}$ are the $j$-th components of $\\mu_i$ and $\\sigma_i$, respectively. Note that this term depends on $\\phi$ (through $\\mu_i$ and $\\sigma_i$) but not on $\\theta=(W, b)$.\n\nNext, we analyze the expected log-likelihood term. The log-likelihood for a single spike count $x_{ik}$ is $\\ln p_{\\theta}(x_{ik} \\mid z_i) = x_{ik} \\ln(\\lambda_{ik}) - \\lambda_{ik} - \\ln(x_{ik}!)$. With $\\ln \\lambda_{ik} = (W z_i + b)_k$, so $\\lambda_{ik} = \\exp((W z_i + b)_k)$. The full log-likelihood for observation $x_i$ is:\n$$\n\\ln p_{\\theta}(x_i \\mid z_i) = \\sum_{k=1}^{K} \\left( x_{ik} (W z_i + b)_k - \\exp((W z_i + b)_k) \\right) - \\sum_{k=1}^{K} \\ln(x_{ik}!)\n$$\nThe expectation $\\mathbb{E}_{q_{\\phi}(z_{i} \\mid x_{i})}[\\cdot]$ is intractable. We approximate it using a Monte Carlo estimate with $L$ samples, employing the reparameterization trick: $z_i^{(\\ell)} = \\mu_i + \\sigma_i \\odot \\epsilon_i^{(\\ell)}$, where $\\epsilon_i^{(\\ell)} \\sim \\mathcal{N}(0, I)$ for $\\ell=1, \\dots, L$.\n$$\n\\mathbb{E}_{q_{\\phi}}[\\ln p_{\\theta}(x_i \\mid z_i)] \\approx \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)})\n$$\nThe Monte Carlo estimator for $\\mathcal{L}_i$ is thus:\n$$\n\\hat{\\mathcal{L}}_i(\\theta, \\phi) = \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) - \\frac{1}{2} \\sum_{j=1}^{D} \\left( \\mu_{ij}^{2} + \\sigma_{ij}^{2} - 2\\ln(\\sigma_{ij}) - 1 \\right)\n$$\n\nWe now derive the gradients.\n\n**Gradients with respect to model parameters $\\theta = (W, b)$:**\nThe parameters $W$ and $b$ only appear in the log-likelihood term. The gradient operator can be passed through the summation and the Monte Carlo average.\nLet $\\eta_i^{(\\ell)} = W z_i^{(\\ell)} + b$, so $\\lambda_i^{(\\ell)} = \\exp(\\eta_i^{(\\ell)})$. The residual for neuron $k$ is $x_{ik} - \\lambda_{ik}^{(\\ell)}$.\nFor the bias vector $b$:\n$$\n\\nabla_b \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) = x_i - \\lambda_i^{(\\ell)}\n$$\nFor the weight matrix $W$: The derivative with respect to the matrix element $W_{kj}$ is $(x_{ik} - \\lambda_{ik}^{(\\ell)})z_{ij}^{(\\ell)}$. In matrix form, this is an outer product:\n$$\n\\nabla_W \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) = (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T\n$$\nAveraging over the $L$ Monte Carlo samples gives the gradient estimators for $\\mathcal{L}_i$. The full unbiased mini-batch stochastic gradient estimators are then:\n$$\n\\widehat{\\nabla_W \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left[ \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T \\right]\n$$\n$$\n\\widehat{\\nabla_b \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left[ \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) \\right]\n$$\nwhere $\\lambda_i^{(\\ell)} = \\exp(W(\\mu_i + \\sigma_i \\odot \\epsilon_i^{(\\ell)}) + b)$.\n\n**Gradient with respect to variational parameters $\\phi$:**\nThe parameter vector $\\phi$ influences $\\mathcal{L}_i$ through both the log-likelihood term (via the distribution of $z_i$) and the KL-divergence term (via $\\mu_i$ and $\\sigma_i$). We use the chain rule, taking gradients with respect to the outputs of the inference network, $\\mu_i$ and $\\sigma_i$.\n\nThe key step for the log-likelihood term is the reparameterization trick, which re-expresses the expectation with respect to $q_\\phi$ as an expectation with respect to a fixed distribution $p(\\epsilon)$ that does not depend on $\\phi$. This allows interchanging the gradient and expectation operators:\n$$\n\\nabla_{\\phi} \\mathbb{E}_{q_{\\phi}(z_i \\mid x_i)}[\\ln p_{\\theta}(x_i \\mid z_i)] = \\nabla_{\\phi} \\mathbb{E}_{p(\\epsilon_i)}[\\ln p_{\\theta}(x_i \\mid \\mu_i + \\sigma_i \\odot \\epsilon_i)] = \\mathbb{E}_{p(\\epsilon_i)}[\\nabla_{\\phi} \\ln p_{\\theta}(x_i \\mid \\mu_i + \\sigma_i \\odot \\epsilon_i)]\n$$\nThe gradient is then estimated with Monte Carlo sampling. We compute gradients of $\\hat{\\mathcal{L}}_i$ with respect to $\\mu_i$ and $\\sigma_i$ and then chain to $\\phi$.\n$$\n\\nabla_{\\phi} \\hat{\\mathcal{L}}_i = (\\nabla_{\\mu_i}\\hat{\\mathcal{L}}_i)^T \\frac{\\partial \\mu_i}{\\partial \\phi} + (\\nabla_{\\sigma_i}\\hat{\\mathcal{L}}_i)^T \\frac{\\partial \\sigma_i}{\\partial \\phi}\n$$\nThe terms $\\frac{\\partial \\mu_i}{\\partial \\phi}$ and $\\frac{\\partial \\sigma_i}{\\partial \\phi}$ are the Jacobians of the inference networks. Let's define them as $J_{\\mu_i} \\triangleq \\nabla_{\\phi} \\mu_{\\phi}(x_i)$ and $J_{\\sigma_i} \\triangleq \\nabla_{\\phi} \\sigma_{\\phi}(x_i)$.\n\nThe gradient of the log-likelihood term w.r.t. $\\mu_i$ is:\n$$\n\\nabla_{\\mu_i} \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) \\right) = \\frac{1}{L} \\sum_{\\ell=1}^{L} (\\nabla_{z_i^{(\\ell)}} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}))^T \\frac{\\partial z_i^{(\\ell)}}{\\partial \\mu_i}\n$$\nWe have $\\nabla_z \\ln p_\\theta(x_i|z) = W^T (x_i - \\lambda_i)$ and $\\frac{\\partial z_i^{(\\ell)}}{\\partial \\mu_i} = I$. So the gradient is $\\frac{1}{L} \\sum_{\\ell=1}^{L} W^T(x_i - \\lambda_i^{(\\ell)})$.\nThe gradient of the log-likelihood term w.r.t. $\\sigma_i$ is:\n$$\n\\nabla_{\\sigma_i} \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}) \\right) = \\frac{1}{L} \\sum_{\\ell=1}^{L} (\\nabla_{z_i^{(\\ell)}} \\ln p_{\\theta}(x_i \\mid z_i^{(\\ell)}))^T \\frac{\\partial z_i^{(\\ell)}}{\\partial \\sigma_i}\n$$\nHere, $\\frac{\\partial z_i^{(\\ell)}}{\\partial \\sigma_i} = \\mathrm{diag}(\\epsilon_i^{(\\ell)})$, leading to element-wise multiplication. The gradient is $\\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T(x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)}$.\n\nThe gradients of the negative KL-divergence term, $-\\frac{1}{2} \\sum_{j=1}^{D} (\\mu_{ij}^2 + \\sigma_{ij}^2 - 2\\ln \\sigma_{ij} - 1)$, are:\n$$\n\\nabla_{\\mu_i} (-D_{KL}) = -\\mu_i\n$$\n$$\n\\nabla_{\\sigma_i} (-D_{KL}) = -(\\sigma_i - \\sigma_i^{\\odot-1}) = \\sigma_i^{\\odot-1} - \\sigma_i\n$$\nwhere $\\sigma_i^{\\odot-1}$ is the vector of element-wise reciprocals $(1/\\sigma_{i1}, \\dots, 1/\\sigma_{iD})^T$.\n\nCombining these parts, the gradient of $\\hat{\\mathcal{L}}_i$ with respect to $\\phi$ is:\n$$\n\\nabla_{\\phi} \\hat{\\mathcal{L}}_i = J_{\\mu_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} W^T (x_i - \\lambda_i^{(\\ell)}) \\right) - \\mu_i \\right] + J_{\\sigma_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T (x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)} \\right) + (\\sigma_i^{\\odot-1} - \\sigma_i) \\right]\n$$\nThe full unbiased mini-batch stochastic gradient estimator is:\n$$\n\\widehat{\\nabla_{\\phi} \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\nabla_{\\phi} \\hat{\\mathcal{L}}_i\n$$\nThe necessary scaling rule for unbiasedness is the factor $\\frac{N}{m}$ applied to the sum over the mini-batch $\\mathcal{B}$.\n\nTo provide the single compact expression for the gradients, we define the following quantities for each $i \\in \\mathcal{B}$:\nLet $\\mu_i = \\mu_{\\phi}(x_i)$, $\\sigma_i=\\sigma_{\\phi}(x_i)$. For $\\ell=1, \\dots, L$, let $\\epsilon_i^{(\\ell)} \\sim \\mathcal{N}(0, I)$, $z_i^{(\\ell)} = \\mu_i+\\sigma_i \\odot \\epsilon_i^{(\\ell)}$, and $\\lambda_i^{(\\ell)} = \\exp(W z_i^{(\\ell)} + b)$.\nAlso let $J_{\\mu_i} \\triangleq \\nabla_{\\phi} \\mu_{\\phi}(x_i)$ and $J_{\\sigma_i} \\triangleq \\nabla_{\\phi} \\sigma_{\\phi}(x_i)$.\n\nThe unbiased stochastic gradient estimators are:\n$$\n\\widehat{\\nabla_W \\mathcal{L}} = \\frac{N}{mL} \\sum_{i \\in \\mathcal{B}} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T\n$$\n$$\n\\widehat{\\nabla_b \\mathcal{L}} = \\frac{N}{mL} \\sum_{i \\in \\mathcal{B}} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)})\n$$\n$$\n\\widehat{\\nabla_{\\phi} \\mathcal{L}} = \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left( J_{\\mu_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} W^T (x_i - \\lambda_i^{(\\ell)}) \\right) - \\mu_i \\right] + J_{\\sigma_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T (x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)} \\right) + \\frac{1}{\\sigma_i} - \\sigma_i \\right] \\right)\n$$\nwhere division and exponentiation of vectors are element-wise. The sum and average can be moved inside for $W$ and $b$ for a more compact notation. The final expressions are reported below.",
            "answer": "$$\n\\boxed{\\begin{aligned}\n\\widehat{\\nabla_W \\mathcal{L}} &= \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) (z_i^{(\\ell)})^T \\\\\n\\widehat{\\nabla_b \\mathcal{L}} &= \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\frac{1}{L} \\sum_{\\ell=1}^{L} (x_i - \\lambda_i^{(\\ell)}) \\\\\n\\widehat{\\nabla_{\\phi} \\mathcal{L}} &= \\frac{N}{m} \\sum_{i \\in \\mathcal{B}} \\left( J_{\\mu_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} W^T (x_i - \\lambda_i^{(\\ell)}) \\right) - \\mu_i \\right] + J_{\\sigma_i}^T \\left[ \\left( \\frac{1}{L} \\sum_{\\ell=1}^{L} (W^T (x_i - \\lambda_i^{(\\ell)})) \\odot \\epsilon_i^{(\\ell)} \\right) + \\sigma_i^{\\odot-1} - \\sigma_i \\right] \\right)\n\\end{aligned}}\n$$"
        },
        {
            "introduction": "After mastering the derivation of variational updates, it is vital to build an intuition for how the different components of the Evidence Lower Bound (ELBO) interact. This exercise () guides you through a computational experiment to quantify the impact of prior model misspecification on the learned posterior. By analyzing a linear-Gaussian system where exact posteriors can be calculated, you will gain a clear, quantitative understanding of how the KL divergence acts as a regularizer and how an inaccurate prior can bias your inference.",
            "id": "4032007",
            "problem": "Consider a linear-Gaussian neural encoding model with latent state $\\mathbf{z} \\in \\mathbb{R}^{M}$, stimulus $\\mathbf{s} \\in \\mathbb{R}^{D}$, and neural response $\\mathbf{y} \\in \\mathbb{R}^{K}$. The generative model is specified by\n$$\n\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s} \\sim \\mathcal{N}(\\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{s}, \\sigma^{2}\\mathbf{I}_{K}),\n$$\nwhere $\\mathbf{W} \\in \\mathbb{R}^{K \\times M}$, $\\mathbf{U} \\in \\mathbb{R}^{K \\times D}$, and $\\sigma^{2} \\in \\mathbb{R}_{+}$. The true latent prior used to generate data is\n$$\np_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M}).\n$$\nHowever, during inference you will assume a (possibly inaccurate) prior\n$$\np(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_{p}, \\boldsymbol{\\Sigma}_{p}),\n$$\nand use a Gaussian variational family $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$. For the linear-Gaussian model, the maximizer of the Evidence Lower Bound (ELBO) over Gaussian $q$ under the assumed model equals the exact posterior of the assumed model,\n$$\nq^{\\star}(\\mathbf{z}) = p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s}).\n$$\n\nYour task is to evaluate, for several assumed priors $p(\\mathbf{z})$, how inaccurate prior specifications influence the ELBO optimization through the Kullback–Leibler divergence (KL) regularization term $\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))$, and to quantify its effect on the learned posterior mean relative to the posterior under the correct prior $p_{\\text{true}}(\\mathbf{z})$.\n\nUse the following fixed model parameters and dataset:\n- Dimensions: $K = 3$, $M = 2$, $D = 2$.\n- Observation noise: $\\sigma^{2} = 0.25$.\n- Encoding matrices\n$$\n\\mathbf{W} = \\begin{bmatrix}\n0.9 & -0.3 \\\\\n0.2 & 0.8 \\\\\n-0.5 & 0.4\n\\end{bmatrix},\n\\quad\n\\mathbf{U} = \\begin{bmatrix}\n0.5 & -0.2 \\\\\n-0.1 & 0.3 \\\\\n0.0 & 0.4\n\\end{bmatrix}.\n$$\n- Dataset of $N = 4$ stimulus–response pairs generated by $\\mathbf{y}_{i} = \\mathbf{W}\\mathbf{z}_{i} + \\mathbf{U}\\mathbf{s}_{i} + \\boldsymbol{\\epsilon}_{i}$ with $\\boldsymbol{\\epsilon}_{i}$ consistent with $\\sigma^{2}$ (but fixed for determinism):\n  - Stimuli $\\{\\mathbf{s}_{i}\\}_{i=1}^{4}$:\n    - $\\mathbf{s}_{1} = [1.0, 0.5]^{\\top}$,\n    - $\\mathbf{s}_{2} = [-0.5, 1.2]^{\\top}$,\n    - $\\mathbf{s}_{3} = [0.0, -1.0]^{\\top}$,\n    - $\\mathbf{s}_{4} = [2.0, 0.0]^{\\top}$.\n  - Latents $\\{\\mathbf{z}_{i}\\}_{i=1}^{4}$:\n    - $\\mathbf{z}_{1} = [0.3, -0.7]^{\\top}$,\n    - $\\mathbf{z}_{2} = [-1.0, 0.5]^{\\top}$,\n    - $\\mathbf{z}_{3} = [0.2, 0.2]^{\\top}$,\n    - $\\mathbf{z}_{4} = [1.5, -0.3]^{\\top}$.\n  - Fixed noise vectors $\\{\\boldsymbol{\\epsilon}_{i}\\}_{i=1}^{4}$:\n    - $\\boldsymbol{\\epsilon}_{1} = [0.05, -0.02, 0.10]^{\\top}$,\n    - $\\boldsymbol{\\epsilon}_{2} = [-0.10, 0.20, -0.05]^{\\top}$,\n    - $\\boldsymbol{\\epsilon}_{3} = [0.00, -0.08, 0.03]^{\\top}$,\n    - $\\boldsymbol{\\epsilon}_{4} = [0.12, -0.15, 0.00]^{\\top}$.\n  - The corresponding responses $\\{\\mathbf{y}_{i}\\}_{i=1}^{4}$ implied by the above are:\n    - $\\mathbf{y}_{1} = [0.93, -0.47, -0.13]^{\\top}$,\n    - $\\mathbf{y}_{2} = [-1.64, 0.81, 1.13]^{\\top}$,\n    - $\\mathbf{y}_{3} = [0.32, -0.18, -0.39]^{\\top}$,\n    - $\\mathbf{y}_{4} = [2.56, -0.29, -0.87]^{\\top}$.\n\nTest suite of assumed priors $p(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_{p}, \\operatorname{diag}(\\mathbf{v}_{p}))$:\n- Case A (correct prior): $\\boldsymbol{\\mu}_{p} = [0.0, 0.0]^{\\top}$, $\\mathbf{v}_{p} = [1.0, 1.0]^{\\top}$.\n- Case B (uninformative broad prior): $\\boldsymbol{\\mu}_{p} = [0.0, 0.0]^{\\top}$, $\\mathbf{v}_{p} = [10.0, 10.0]^{\\top}$.\n- Case C (mildly inaccurate prior): $\\boldsymbol{\\mu}_{p} = [0.5, -0.2]^{\\top}$, $\\mathbf{v}_{p} = [1.5, 0.7]^{\\top}$.\n- Case D (overconfident wrong prior): $\\boldsymbol{\\mu}_{p} = [1.0, 1.0]^{\\top}$, $\\mathbf{v}_{p} = [0.3, 0.3]^{\\top}$.\n\nFor each prior case, do the following for each data point $(\\mathbf{s}_{i}, \\mathbf{y}_{i})$:\n1) Compute the optimal variational posterior under the assumed model, $q^{\\star}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$, where\n$$\n\\mathbf{S} = \\left(\\boldsymbol{\\Sigma}_{p}^{-1} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W} \\right)^{-1}, \\quad\n\\mathbf{m} = \\mathbf{S}\\left(\\boldsymbol{\\Sigma}_{p}^{-1}\\boldsymbol{\\mu}_{p} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\left(\\mathbf{y}_{i} - \\mathbf{U}\\mathbf{s}_{i}\\right)\\right).\n$$\n2) Compute the expected log-likelihood term\n$$\n\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})] = -\\frac{K}{2}\\log(2\\pi \\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\left(\\left\\|\\mathbf{y}_{i} - \\mathbf{U}\\mathbf{s}_{i} - \\mathbf{W}\\mathbf{m}\\right\\|_{2}^{2} + \\operatorname{tr}(\\mathbf{W}\\mathbf{S}\\mathbf{W}^{\\top})\\right).\n$$\n3) Compute the Kullback–Leibler divergence\n$$\n\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z})) = \\frac{1}{2}\\left(\\log\\frac{\\det \\boldsymbol{\\Sigma}_{p}}{\\det \\mathbf{S}} - M + \\operatorname{tr}(\\boldsymbol{\\Sigma}_{p}^{-1}\\mathbf{S}) + (\\mathbf{m}-\\boldsymbol{\\mu}_{p})^{\\top}\\boldsymbol{\\Sigma}_{p}^{-1}(\\mathbf{m}-\\boldsymbol{\\mu}_{p})\\right).\n$$\n4) Compute the ELBO for the data point as\n$$\n\\text{ELBO}_{i} = \\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})] - \\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z})).\n$$\n5) Compute the posterior mean under the correct prior $p_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M})$ for the same $(\\mathbf{s}_{i}, \\mathbf{y}_{i})$:\n$$\n\\mathbf{S}_{\\text{true}} = \\left(\\mathbf{I}_{M} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W}\\right)^{-1}, \\quad\n\\mathbf{m}_{\\text{true}} = \\mathbf{S}_{\\text{true}}\\left(\\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\left(\\mathbf{y}_{i} - \\mathbf{U}\\mathbf{s}_{i}\\right)\\right).\n$$\nQuantify the deviation of the learned posterior mean via\n$$\n\\Delta_{i} = \\left\\|\\mathbf{m} - \\mathbf{m}_{\\text{true}}\\right\\|_{2}.\n$$\n6) Quantify the relative influence of the regularization term in the negative ELBO objective $-\\text{ELBO}_{i} = -\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})] + \\mathrm{KL}$ using\n$$\n\\rho_{i} = \\frac{\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))}{\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z})) + \\left(-\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})]\\right)}.\n$$\n\nFor each prior case, report the averages across the $N = 4$ data points:\n- $\\overline{\\text{ELBO}} = \\frac{1}{N}\\sum_{i=1}^{N} \\text{ELBO}_{i}$,\n- $\\overline{\\mathrm{KL}} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathrm{KL}_{i}$,\n- $\\overline{\\Delta} = \\frac{1}{N}\\sum_{i=1}^{N} \\Delta_{i}$,\n- $\\overline{\\rho} = \\frac{1}{N}\\sum_{i=1}^{N} \\rho_{i}$.\n\nProgram requirements:\n- Implement the above computations exactly for the four prior cases A, B, C, and D.\n- The final program output must be a single line containing a comma-separated list of floating-point numbers enclosed in square brackets. The sequence must be\n$$\n[\\overline{\\text{ELBO}}_{A}, \\overline{\\mathrm{KL}}_{A}, \\overline{\\Delta}_{A}, \\overline{\\rho}_{A}, \\overline{\\text{ELBO}}_{B}, \\overline{\\mathrm{KL}}_{B}, \\overline{\\Delta}_{B}, \\overline{\\rho}_{B}, \\overline{\\text{ELBO}}_{C}, \\overline{\\mathrm{KL}}_{C}, \\overline{\\Delta}_{C}, \\overline{\\rho}_{C}, \\overline{\\text{ELBO}}_{D}, \\overline{\\mathrm{KL}}_{D}, \\overline{\\Delta}_{D}, \\overline{\\rho}_{D}],\n$$\nwith each value rounded to $6$ decimal places.\n- No other text should be printed.\n\nNo physical units or angle units are involved in this problem; all quantities are dimensionless.",
            "solution": "The problem requires us to analyze the effects of prior misspecification in a variational Bayesian framework for a linear-Gaussian neural encoding model. We will compute several metrics for four different assumed priors—one correct, one uninformative, one mildly inaccurate, and one overconfidently wrong—to quantify their impact on the learned posterior distribution and the Evidence Lower Bound (ELBO).\n\nThe generative model is defined as:\n$$\np(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s}) = \\mathcal{N}(\\mathbf{y} \\mid \\mathbf{W}\\mathbf{z} + \\mathbf{U}\\mathbf{s}, \\sigma^{2}\\mathbf{I}_{K})\n$$\nThe full generative process, including the prior on the latent variable $\\mathbf{z}$, is $p(\\mathbf{y}, \\mathbf{z} \\mid \\mathbf{s}) = p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s}) p(\\mathbf{z})$.\n\nVariational inference aims to approximate the true posterior $p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s})$ with a distribution $q(\\mathbf{z})$ from a tractable family by maximizing the ELBO, which is a lower bound on the log model evidence $\\log p(\\mathbf{y} \\mid \\mathbf{s})$. The ELBO is given by:\n$$\n\\text{ELBO}(q) = \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s})] - \\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))\n$$\nThis expression decomposes the objective into two terms:\n1.  The expected log-likelihood, $\\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s})]$, which promotes posteriors $q(\\mathbf{z})$ that accurately reconstruct the observed data $\\mathbf{y}$. This is a data-fit or reconstruction term.\n2.  The Kullback-Leibler (KL) divergence, $\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))$, which penalizes the deviation of the approximate posterior $q(\\mathbf{z})$ from the assumed prior $p(\\mathbf{z})$. This acts as a regularization term.\n\nFor a linear-Gaussian model with a Gaussian prior $p(\\mathbf{z}) = \\mathcal{N}(\\boldsymbol{\\mu}_{p}, \\boldsymbol{\\Sigma}_{p})$ and a Gaussian likelihood, the true posterior $p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s})$ is also Gaussian. Consequently, when we use a Gaussian variational family $q(\\mathbf{z}) = \\mathcal{N}(\\mathbf{m}, \\mathbf{S})$, the optimal variational posterior $q^{\\star}(\\mathbf{z})$ that maximizes the ELBO is precisely the true posterior of the *assumed* model, $p(\\mathbf{z} \\mid \\mathbf{y}, \\mathbf{s})$. The parameters of this posterior are derived by completing the square in the exponent of the log-posterior $\\log p(\\mathbf{y} \\mid \\mathbf{z}, \\mathbf{s}) + \\log p(\\mathbf{z})$. This yields the analytical expressions for the posterior covariance $\\mathbf{S}$ and mean $\\mathbf{m}$:\n$$\n\\mathbf{S}^{-1} = \\boldsymbol{\\Sigma}_{p}^{-1} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W} \\implies \\mathbf{S} = \\left(\\boldsymbol{\\Sigma}_{p}^{-1} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}\\mathbf{W} \\right)^{-1}\n$$\n$$\n\\mathbf{S}^{-1}\\mathbf{m} = \\boldsymbol{\\Sigma}_{p}^{-1}\\boldsymbol{\\mu}_{p} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}(\\mathbf{y} - \\mathbf{U}\\mathbf{s}) \\implies \\mathbf{m} = \\mathbf{S}\\left(\\boldsymbol{\\Sigma}_{p}^{-1}\\boldsymbol{\\mu}_{p} + \\frac{1}{\\sigma^{2}}\\mathbf{W}^{\\top}(\\mathbf{y} - \\mathbf{U}\\mathbf{s})\\right)\n$$\nThese formulas describe how the prior information (mean $\\boldsymbol{\\mu}_p$, precision $\\boldsymbol{\\Sigma}_p^{-1}$) is combined with the information from the data (summarized by the likelihood-derived terms) to form the posterior.\n\nThe analysis proceeds by computing the following metrics for each assumed prior and for each data point $(\\mathbf{s}_i, \\mathbf{y}_i)$:\n-   $\\mathbf{m}_{\\text{true}}$: The posterior mean if we had used the correct generative prior $p_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M})$. This serves as our ground-truth reference for the latent state estimate for each data point.\n-   $\\Delta_{i} = \\left\\|\\mathbf{m} - \\mathbf{m}_{\\text{true}}\\right\\|_{2}$: The Euclidean distance between the posterior mean computed with the (possibly incorrect) assumed prior and the reference mean. This directly quantifies the error in the latent state estimate due to prior misspecification.\n-   $\\text{ELBO}_{i}$: The value of the evidence lower bound. For $q=q^\\star$, this equals the log marginal likelihood of the data under the assumed model, $\\log p(\\mathbf{y}_i | \\mathbf{s}_i)$. It measures how well the assumed model (prior + likelihood) explains the data.\n-   $\\mathrm{KL}_{i}$: The KL divergence from the prior to the posterior. It measures the information gain from observing the data, or equivalently, the \"cost\" of deviating from the prior.\n-   $\\rho_{i}$: The ratio of the KL divergence to the negative ELBO. The negative ELBO can be interpreted as a total cost or \"free energy\". $\\rho_i$ thus represents the fraction of this total cost attributable to the regularization term, showing the relative influence of the prior in shaping the final posterior.\n\nThe computational procedure is as follows:\n1.  Initialize all model parameters ($\\mathbf{W}$, $\\mathbf{U}$, $\\sigma^2$) and the dataset ($\\{\\mathbf{s}_i, \\mathbf{y}_i\\}$) as specified.\n2.  For each of the $N=4$ data points, pre-compute the reference posterior mean $\\mathbf{m}_{\\text{true}, i}$ using the true prior $p_{\\text{true}}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{M})$.\n3.  Iterate through the four specified prior cases (A, B, C, D).\n4.  For each prior case, iterate through the four data points.\n5.  Inside the inner loop, for a given prior specification $(\\boldsymbol{\\mu}_{p}, \\boldsymbol{\\Sigma}_{p})$ and data point $(\\mathbf{s}_i, \\mathbf{y}_i)$:\n    a. Calculate the posterior parameters $\\mathbf{S}$ and $\\mathbf{m}$ using the provided analytical formulas.\n    b. Compute the error in the posterior mean, $\\Delta_{i} = \\left\\|\\mathbf{m} - \\mathbf{m}_{\\text{true}, i}\\right\\|_{2}$.\n    c. Calculate the expected log-likelihood, $\\mathbb{E}_{q}[\\log p(\\mathbf{y}_{i} \\mid \\mathbf{z}, \\mathbf{s}_{i})]$.\n    d. Calculate the KL divergence, $\\mathrm{KL}(q(\\mathbf{z}) \\parallel p(\\mathbf{z}))$.\n    e. Compute the ELBO, $\\text{ELBO}_{i} = \\mathbb{E}_{q}[\\dots] - \\mathrm{KL}$.\n    f. Compute the relative KL influence, $\\rho_i$.\n6.  For each prior case, average the computed metrics ($\\text{ELBO}_{i}$, $\\mathrm{KL}_{i}$, $\\Delta_{i}$, $\\rho_{i}$) over the $N=4$ data points.\n7.  Finally, collect and format the 16 averaged values as requested for the final output. This involves a direct implementation of the matrix and vector operations defined by the equations in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the variational Bayesian inference problem for a linear-Gaussian model\n    under different prior specifications.\n    \"\"\"\n    # Fixed model parameters and dataset\n    K, M, D = 3, 2, 2\n    sigma2 = 0.25\n    inv_sigma2 = 1.0 / sigma2\n\n    W = np.array([\n        [0.9, -0.3],\n        [0.2, 0.8],\n        [-0.5, 0.4]\n    ])\n\n    U = np.array([\n        [0.5, -0.2],\n        [-0.1, 0.3],\n        [0.0, 0.4]\n    ])\n\n    stimuli_data = [\n        np.array([1.0, 0.5]),\n        np.array([-0.5, 1.2]),\n        np.array([0.0, -1.0]),\n        np.array([2.0, 0.0])\n    ]\n\n    responses_data = [\n        np.array([0.93, -0.47, -0.13]),\n        np.array([-1.64, 0.81, 1.13]),\n        np.array([0.32, -0.18, -0.39]),\n        np.array([2.56, -0.29, -0.87])\n    ]\n    \n    N = len(stimuli_data)\n\n    # Test suite of assumed priors\n    test_cases = [\n        # Case A: Correct prior\n        {'mu_p': np.array([0.0, 0.0]), 'v_p': np.array([1.0, 1.0])},\n        # Case B: Uninformative broad prior\n        {'mu_p': np.array([0.0, 0.0]), 'v_p': np.array([10.0, 10.0])},\n        # Case C: Mildly inaccurate prior\n        {'mu_p': np.array([0.5, -0.2]), 'v_p': np.array([1.5, 0.7])},\n        # Case D: Overconfident wrong prior\n        {'mu_p': np.array([1.0, 1.0]), 'v_p': np.array([0.3, 0.3])}\n    ]\n\n    # Pre-compute constant matrices\n    WTW = W.T @ W\n    WT = W.T\n    I_M = np.eye(M)\n\n    # --- Compute posterior means under the correct prior (m_true) for all data points ---\n    Sigma_p_true_inv = I_M  # Since Sigma_p_true is Identity\n    S_true_inv = Sigma_p_true_inv + inv_sigma2 * WTW\n    S_true = np.linalg.inv(S_true_inv)\n\n    m_true_list = []\n    for i in range(N):\n        s_i = stimuli_data[i]\n        y_i = responses_data[i]\n        \n        # This part of the calculation for m_true uses mu_p_true = 0\n        m_true = S_true @ (inv_sigma2 * WT @ (y_i - U @ s_i))\n        m_true_list.append(m_true)\n\n    final_results = []\n    # --- Loop through each test case ---\n    for case_params in test_cases:\n        mu_p = case_params['mu_p']\n        v_p = case_params['v_p']\n        \n        Sigma_p = np.diag(v_p)\n        Sigma_p_inv = np.diag(1.0 / v_p)\n        log_det_Sigma_p = np.sum(np.log(v_p))\n\n        # Store metrics for this case to be averaged later\n        elbo_list, kl_list, delta_list, rho_list = [], [], [], []\n\n        # --- Loop through each data point ---\n        for i in range(N):\n            s_i = stimuli_data[i]\n            y_i = responses_data[i]\n            m_true = m_true_list[i]\n\n            # 1) Compute optimal variational posterior q*(z) = N(m, S)\n            S_inv = Sigma_p_inv + inv_sigma2 * WTW\n            S = np.linalg.inv(S_inv)\n            log_det_S = np.linalg.slogdet(S)[1]\n            \n            m = S @ (Sigma_p_inv @ mu_p + inv_sigma2 * WT @ (y_i - U @ s_i))\n\n            # 2) Compute expected log-likelihood\n            term_const = -0.5 * K * np.log(2 * np.pi * sigma2)\n            y_hat = y_i - U @ s_i\n            reconstruction_term = np.sum((y_hat - W @ m)**2)\n            trace_term = np.trace(WTW @ S)\n            \n            E_log_lik = term_const - 0.5 * inv_sigma2 * (reconstruction_term + trace_term)\n\n            # 3) Compute KL divergence KL(q || p)\n            trace_Sigma_inv_S = np.trace(Sigma_p_inv @ S)\n            m_minus_mu = m - mu_p\n            quad_term = m_minus_mu.T @ Sigma_p_inv @ m_minus_mu\n            \n            kl_div = 0.5 * (log_det_Sigma_p - log_det_S - M + trace_Sigma_inv_S + quad_term)\n            \n            # 4) Compute ELBO\n            elbo = E_log_lik - kl_div\n            \n            # 5) Compute deviation Delta\n            delta = np.linalg.norm(m - m_true)\n\n            # 6) Compute relative influence rho\n            neg_E_log_lik = -E_log_lik\n            # The denominator is -ELBO, which is guaranteed to be positive\n            rho = kl_div / (kl_div + neg_E_log_lik)\n\n            # Append to lists\n            elbo_list.append(elbo)\n            kl_list.append(kl_div)\n            delta_list.append(delta)\n            rho_list.append(rho)\n\n        # Average the metrics for the current case\n        avg_elbo = np.mean(elbo_list)\n        avg_kl = np.mean(kl_list)\n        avg_delta = np.mean(delta_list)\n        avg_rho = np.mean(rho_list)\n        \n        final_results.extend([avg_elbo, avg_kl, avg_delta, avg_rho])\n\n    # Final print statement in the exact required format\n    output_str = ','.join(f'{x:.6f}' for x in final_results)\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}