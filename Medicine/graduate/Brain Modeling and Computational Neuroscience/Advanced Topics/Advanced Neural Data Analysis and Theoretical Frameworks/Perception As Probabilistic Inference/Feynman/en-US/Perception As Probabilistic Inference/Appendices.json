{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of perceiving the world probabilistically lies in defining a generative model, which mathematically describes how hidden states of the world generate observable sensory signals. This first exercise tasks you with building such a model from the ground up for a classic visual detection scenario . Formalizing the prior beliefs and the likelihood of evidence is the essential first step to performing any Bayesian inference.",
            "id": "4008976",
            "problem": "Consider a single-trial visual detection task in which a latent binary variable $z \\in \\{0,1\\}$ encodes whether a target is absent ($z=0$) or present ($z=1$). The measurement $x \\in \\mathbb{R}$ is a scalar sensory observation. Assume that the measurement noise is Gaussian and additive, and that the baseline measurement when the target is absent has mean $0$, whereas the measurement when the target is present has mean $\\mu_1 \\in \\mathbb{R}$. Let the prior probability of target presence be $\\pi \\in (0,1)$, and let the noise variance be $\\sigma^2 \\in (0,\\infty)$. Define the parameter vector as $\\theta = (\\pi, \\mu_1, \\sigma^2)$. Which option correctly specifies a scientifically sound joint generative model $p(x,z \\mid \\theta)$ such that the latent cause $z$ influences the observation $x$ through Gaussian noise, and give a clear interpretation of each term in the specification?\n\nA. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $z\\,\\mu_1$ and variance $\\sigma^2$, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.\n\nB. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{x^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $0$ and variance $\\sigma^2$ for both $z \\in \\{0,1\\}$, and $\\theta=(\\pi,\\sigma^2)$ parameterizes the prior and the likelihood.\n\nC. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - z\\,\\mu_1|}{b}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Laplace with mean $z\\,\\mu_1$ and scale $b>0$, and $\\theta=(\\pi,\\mu_1,b)$ parameterizes the prior and the likelihood.\n\nD. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian up to proportionality, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.\n\nE. $p(x,z \\mid \\theta) = p(x \\mid \\theta)\\,p(z \\mid x,\\theta)$ with $p(x \\mid \\theta) = \\pi\\,\\mathcal{N}(x; \\mu_1, \\sigma^2) + (1-\\pi)\\,\\mathcal{N}(x; 0, \\sigma^2)$ and $p(z=1 \\mid x,\\theta) = \\frac{1}{1+\\exp(-\\alpha x)}$, where $\\alpha \\in \\mathbb{R}$ controls the slope, and $\\theta=(\\pi,\\mu_1,\\sigma^2,\\alpha)$ parameterizes the mixture and the posterior classifier.\n\nSelect the correct option(s).",
            "solution": "The user requires a critical validation of the problem statement, followed by a full derivation and evaluation of the options provided for a question on generative models in computational neuroscience.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\nThe problem statement provides the following information:\n- A latent binary variable $z \\in \\{0,1\\}$.\n- $z=0$ represents target absence.\n- $z=1$ represents target presence.\n- A scalar sensory observation $x \\in \\mathbb{R}$.\n- The measurement noise is Gaussian and additive.\n- The mean of the measurement is $0$ when the target is absent ($z=0$).\n- The mean of the measurement is $\\mu_1 \\in \\mathbb{R}$ when the target is present ($z=1$).\n- The prior probability of target presence is $P(z=1) = \\pi$, with $\\pi \\in (0,1)$.\n- The noise variance is $\\sigma^2$, with $\\sigma^2 \\in (0,\\infty)$.\n- The parameter vector is defined as $\\theta = (\\pi, \\mu_1, \\sigma^2)$.\n- The task is to identify the option that correctly specifies the joint generative model $p(x,z \\mid \\theta)$ and its interpretation, where the latent cause $z$ influences the observation $x$ through Gaussian noise.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a canonical model from signal detection theory (SDT), which is a fundamental framework in psychophysics, perception, and computational neuroscience for modeling decisions under uncertainty. It is a classic example of perception as probabilistic inference. The model is scientifically sound and widely used.\n- **Well-Posed:** The problem is well-posed. It provides clear definitions for the variables ($z, x$), parameters ($\\pi, \\mu_1, \\sigma^2$), and the structure of the probabilistic relationship (prior on a binary variable, Gaussian conditional distributions). The goal—to construct the joint probability distribution $p(x,z \\mid \\theta)$—is a standard and solvable task in probability theory.\n- **Objective:** The language is formal, mathematical, and unambiguous. There are no subjective or opinion-based claims.\n\nThe problem statement does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, complete, and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Derivation of the Joint Generative Model\n\nThe goal is to construct the joint probability distribution $p(x,z \\mid \\theta)$. A generative model specifies how data are generated from latent causes. This is naturally expressed using the chain rule of probability as:\n$$p(x,z \\mid \\theta) = p(x \\mid z, \\theta) \\, p(z \\mid \\theta)$$\nThis factorization represents the generative sequence: first, a latent cause $z$ is drawn from its prior distribution $p(z \\mid \\theta)$, and then an observation $x$ is generated from the likelihood (or conditional distribution) $p(x \\mid z, \\theta)$.\n\n1.  **The Prior Distribution $p(z \\mid \\theta)$:**\n    The latent variable $z$ is binary, $z \\in \\{0, 1\\}$. We are given the prior probability of target presence, $P(z=1) = \\pi$. Since there are only two outcomes, the probability of target absence is $P(z=0) = 1-\\pi$. This describes a Bernoulli distribution. A compact mathematical expression for the probability mass function of a Bernoulli variable is:\n    $$p(z \\mid \\theta) = \\pi^z (1-\\pi)^{1-z}$$\n    This expression correctly yields $\\pi$ for $z=1$ and $1-\\pi$ for $z=0$. The parameter $\\theta$ relevant for the prior is $\\pi$.\n\n2.  **The Likelihood Function $p(x \\mid z, \\theta)$:**\n    The problem states that the observation $x$ is generated from a process involving additive Gaussian noise. The mean of this process depends on the state of the latent variable $z$.\n    -   When $z=0$ (target absent), the mean of $x$ is $0$. The distribution of $x$ is Gaussian with mean $0$ and variance $\\sigma^2$. The probability density function (PDF) is:\n        $$p(x \\mid z=0, \\theta) = \\mathcal{N}(x; 0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - 0)^2}{2\\sigma^2}\\right)$$\n    -   When $z=1$ (target present), the mean of $x$ is $\\mu_1$. The distribution of $x$ is Gaussian with mean $\\mu_1$ and variance $\\sigma^2$. The PDF is:\n        $$p(x \\mid z=1, \\theta) = \\mathcal{N}(x; \\mu_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu_1)^2}{2\\sigma^2}\\right)$$\n    We can write a single expression for the likelihood that covers both cases. The mean of the Gaussian distribution is $0$ if $z=0$ and $\\mu_1$ if $z=1$. This can be written compactly as $z\\,\\mu_1$. Therefore, the general form of the likelihood is:\n    $$p(x \\mid z, \\theta) = \\mathcal{N}(x; z\\,\\mu_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$$\n    The parameters from $\\theta$ relevant for the likelihood are $\\mu_1$ and $\\sigma^2$.\n\n3.  **The Joint Distribution $p(x,z \\mid \\theta)$:**\n    Combining the prior and the likelihood, we get the joint distribution:\n    $$p(x,z \\mid \\theta) = p(z \\mid \\theta) \\, p(x \\mid z, \\theta) = \\left[ \\pi^z (1-\\pi)^{1-z} \\right] \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right) \\right]$$\n\n### Evaluation of Options\n\n**A. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $z\\,\\mu_1$ and variance $\\sigma^2$, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** This expression exactly matches the one derived from first principles. The term $\\pi^z(1-\\pi)^{1-z}$ correctly represents the Bernoulli prior $p(z \\mid \\theta)$. The term $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$ correctly represents the Gaussian likelihood $p(x \\mid z, \\theta)$ with a mean of $z\\,\\mu_1$ (which is $0$ for $z=0$ and $\\mu_1$ for $z=1$) and variance $\\sigma^2$.\n-   **Interpretation:** The interpretation provided is fully consistent with the problem statement and the mathematical derivation. $\\pi$ is the prior for $z=1$, the conditional distribution of $x$ given $z$ is indeed Gaussian with mean $z\\mu_1$ and variance $\\sigma^2$, and the parameters $\\theta$ are correctly partitioned between the prior and the likelihood.\n-   **Verdict:** **Correct**.\n\n**B. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{x^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian with mean $0$ and variance $\\sigma^2$ for both $z \\in \\{0,1\\}$, and $\\theta=(\\pi,\\sigma^2)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** The likelihood part of this expression, $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{x^2}{2\\sigma^2}\\right)$, corresponds to a Gaussian distribution with a mean of $0$ for all values of $z$. This means that the observation $x$ is independent of the latent cause $z$, i.e., $p(x \\mid z, \\theta) = p(x \\mid \\theta)$.\n-   **Interpretation:** This contradicts the problem statement that \"the measurement when the target is present has mean $\\mu_1$.\" The model specified in this option implies the latent cause $z$ has no influence on the mean of the observation $x$.\n-   **Verdict:** **Incorrect**.\n\n**C. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - z\\,\\mu_1|}{b}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Laplace with mean $z\\,\\mu_1$ and scale $b>0$, and $\\theta=(\\pi,\\mu_1,b)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** The likelihood part of this expression, $\\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - z\\,\\mu_1|}{b}\\right)$, is the PDF of a Laplace distribution, not a Gaussian distribution.\n-   **Interpretation:** This contradicts the explicit problem requirement that \"the measurement noise is Gaussian and additive.\"\n-   **Verdict:** **Incorrect**.\n\n**D. $p(x,z \\mid \\theta) = \\big[\\pi^{z}(1-\\pi)^{1-z}\\big] \\,\\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, where $\\pi$ is the prior probability of $z=1$, the conditional distribution $x \\mid z$ is Gaussian up to proportionality, and $\\theta=(\\pi,\\mu_1,\\sigma^2)$ parameterizes the prior and the likelihood.**\n-   **Mathematical Expression:** The expression for the likelihood, $\\exp\\!\\left(-\\frac{(x - z\\,\\mu_1)^2}{2\\sigma^2}\\right)$, is missing the normalization constant $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$. A joint probability density function for variables including a continuous variable (like $x$) must be normalized such that its integral over the continuous variables and sum over the discrete variables equals $1$. Without this constant, the function is not a valid joint PDF. The question asks for a correct specification of the generative model, which implies a properly defined probability distribution.\n-   **Interpretation:** While the statement \"is Gaussian up to proportionality\" acknowledges the missing term, it describes an incomplete and thus incorrect specification for a probability density function.\n-   **Verdict:** **Incorrect**.\n\n**E. $p(x,z \\mid \\theta) = p(x \\mid \\theta)\\,p(z \\mid x,\\theta)$ with $p(x \\mid \\theta) = \\pi\\,\\mathcal{N}(x; \\mu_1, \\sigma^2) + (1-\\pi)\\,\\mathcal{N}(x; 0, \\sigma^2)$ and $p(z=1 \\mid x,\\theta) = \\frac{1}{1+\\exp(-\\alpha x)}$, where $\\alpha \\in \\mathbb{R}$ controls the slope, and $\\theta=(\\pi,\\mu_1,\\sigma^2,\\alpha)$ parameterizes the mixture and the posterior classifier.**\n-   **Mathematical Expression:** This option uses the factorization $p(x,z) = p(x)p(z|x)$. This is mathematically valid, but it represents the \"diagnostic\" or \"discriminative\" direction ($x \\to z$), not the \"generative\" direction ($z \\to x$) where the latent cause influences the observation. The problem explicitly asks for the generative model. Furthermore, the posterior probability $p(z=1 \\mid x,\\theta)$ is specified as a simple logistic function of $x$, which is an approximation. The true posterior derived from the generative model in option A is a logistic function of a linear function of $x$, specifically involving an offset term. The introduction of an independent parameter $\\alpha$ is also inconsistent with the original parameter set $\\theta$.\n-   **Interpretation:** This formulation describes a model structure that is conceptually different from the generative process requested. It mixes a marginal distribution (evidence) with a separately parameterized posterior (classifier). This is not the standard specification of the joint generative model itself.\n-   **Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Probabilistic inference yields a full posterior distribution of beliefs, but behavior often requires committing to a single estimate. This practice delves into Bayesian decision theory to explore how an optimal estimate is chosen . By examining different loss functions, you will discover why the 'best' perceptual judgment depends on the costs of being wrong, providing a rational basis for perceptual biases.",
            "id": "4008950",
            "problem": "Consider a latent stimulus variable $z \\in \\mathbb{R}$ (for example, a visual orientation) and a sensory observation $x \\in \\mathbb{R}$. The perceptual system combines a likelihood $p(x \\mid z)$ with a prior $p(z)$ to form a posterior $p(z \\mid x)$ via Bayes' rule. In Bayesian decision theory, the optimal estimator $\\hat{z}(x)$ minimizes posterior expected loss, defined by the risk functional $\\mathcal{R}(\\hat{z};x) = \\int L(\\hat{z},z) \\, p(z \\mid x) \\, dz$, where $L(\\hat{z},z)$ is a task-dependent loss. Two canonical estimators are the Maximum A Posteriori (MAP) estimator $\\hat{z}_{\\mathrm{MAP}}(x) = \\arg\\max_{z} p(z \\mid x)$ and the Minimum Mean Squared Error (MMSE) estimator $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$, which arise under different loss specifications. In advanced perceptual settings, loss may be asymmetric: for instance, underestimation of $z$ may be more costly than overestimation due to motor or safety constraints. Consider the asymmetric linear loss\n$$\nL(\\hat{z},z) = c_{+} \\, (z - \\hat{z})_{+} + c_{-} \\, (\\hat{z} - z)_{+},\n$$\nwhere $(a)_{+} = \\max\\{a,0\\}$, $c_{+} > 0$ is the cost weight when the true $z$ exceeds the estimate $\\hat{z}$ (underestimation), and $c_{-} > 0$ is the cost weight when $\\hat{z}$ exceeds $z$ (overestimation). Assume $p(z \\mid x)$ is continuous with cumulative distribution function $F_{Z \\mid X}(z \\mid x)$. Select all statements that are correct about how $\\hat{z}_{\\mathrm{MAP}}$ and $\\hat{z}_{\\mathrm{MMSE}}$ contrast, and when they differ under asymmetric loss.\n\nA. The Bayes estimator that minimizes posterior expected loss is $\\hat{z}(x) = \\arg\\min_{\\hat{z}} \\int L(\\hat{z},z) \\, p(z \\mid x) \\, dz$; under squared-error loss $L(\\hat{z},z) = (z - \\hat{z})^{2}$ it is $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$; under a limiting neighborhood version of $0$-$1$ loss in the continuous case it is the posterior mode $\\hat{z}_{\\mathrm{MAP}}(x)$; and under the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hat{z})_{+} + c_{-} (\\hat{z} - z)_{+}$ it is the posterior quantile solving $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$, which coincides with both $\\hat{z}_{\\mathrm{MMSE}}$ and $\\hat{z}_{\\mathrm{MAP}}$ only when $p(z \\mid x)$ is symmetric unimodal and $c_{+} = c_{-}$.\n\nB. The MAP estimator $\\hat{z}_{\\mathrm{MAP}}(x)$ minimizes expected loss for any loss function $L(\\hat{z},z)$ because it maximizes $p(z \\mid x)$.\n\nC. For any convex loss function $L(\\hat{z},z)$, the Bayes-optimal estimator is always $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$, so asymmetry in loss does not alter the optimal estimator.\n\nD. For the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hat{z})_{+} + c_{-} (\\hat{z} - z)_{+}$ with $c_{+} > c_{-}$, the Bayes-optimal estimator satisfies $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{-}}{c_{+} + c_{-}}$, implying a downward shift of the estimator relative to $\\mathbb{E}[z \\mid x]$.\n\nE. If $p(z \\mid x)$ is Gaussian and the loss is squared-error, then $\\hat{z}_{\\mathrm{MAP}}(x) = \\hat{z}_{\\mathrm{MMSE}}(x)$; under the asymmetric linear loss with $c_{+} \\neq c_{-}$, the Bayes-optimal estimator is a posterior quantile rather than the posterior mean, so $\\hat{z}_{\\mathrm{MMSE}}(x)$ is not optimal for that loss.",
            "solution": "The problem statement is a valid and well-posed question in Bayesian decision theory, a standard framework in computational neuroscience and statistics. The definitions and concepts provided are scientifically grounded, internally consistent, and sufficient for a rigorous analysis. We will proceed to derive the optimal estimators for the specified loss functions and then evaluate each statement.\n\n### Derivation of Bayes Estimators\n\nThe Bayes-optimal estimator $\\hat{z}(x)$ is the value that minimizes the posterior expected loss, or risk, $\\mathcal{R}(\\hat{z};x)$:\n$$\n\\hat{z}(x) = \\arg\\min_{\\hat{z}} \\mathcal{R}(\\hat{z};x) = \\arg\\min_{\\hat{z}} \\int_{-\\infty}^{\\infty} L(\\hat{z},z) \\, p(z \\mid x) \\, dz\n$$\nWe analyze the specific estimators resulting from different choices of the loss function $L(\\hat{z},z)$.\n\n**1. Squared-Error Loss:** $L(\\hat{z},z) = (z - \\hat{z})^{2}$\nThe risk is $\\mathcal{R}(\\hat{z};x) = \\int (z - \\hat{z})^{2} p(z \\mid x) \\, dz$. To find the minimum, we set the derivative with respect to $\\hat{z}$ to zero:\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = \\frac{\\partial}{\\partial \\hat{z}} \\int (z^2 - 2z\\hat{z} + \\hat{z}^2) p(z \\mid x) \\, dz = \\int (-2z + 2\\hat{z}) p(z \\mid x) \\, dz\n$$\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = 2\\hat{z} \\int p(z \\mid x) \\, dz - 2 \\int z p(z \\mid x) \\, dz = 2\\hat{z} - 2\\mathbb{E}[z \\mid x]\n$$\nSetting the derivative to zero gives $\\hat{z} = \\mathbb{E}[z \\mid x]$. The second derivative is $\\frac{\\partial^2 \\mathcal{R}}{\\partial \\hat{z}^2} = 2 > 0$, confirming a minimum. Thus, the optimal estimator for squared-error loss is the posterior mean, $\\hat{z}_{\\mathrm{MMSE}}(x)$.\n\n**2. 0-1 Loss (Limiting Case):** $L(\\hat{z},z) = \\begin{cases} 0 & \\text{if } |z - \\hat{z}| \\le \\epsilon \\\\ 1 & \\text{if } |z - \\hat{z}| > \\epsilon \\end{cases}$ for a small $\\epsilon > 0$.\nThe risk is $\\mathcal{R}(\\hat{z};x) = \\int_{|z - \\hat{z}| > \\epsilon} 1 \\cdot p(z \\mid x) \\, dz = 1 - \\int_{\\hat{z}-\\epsilon}^{\\hat{z}+\\epsilon} p(z \\mid x) \\, dz$.\nMinimizing the risk is equivalent to maximizing the term $\\int_{\\hat{z}-\\epsilon}^{\\hat{z}+\\epsilon} p(z \\mid x) \\, dz$. As $\\epsilon \\to 0$, this integral is maximized by choosing $\\hat{z}$ to be the value where the density $p(z \\mid x)$ is highest. This is, by definition, the posterior mode. Thus, the optimal estimator for this loss is the Maximum A Posteriori (MAP) estimator, $\\hat{z}_{\\mathrm{MAP}}(x) = \\arg\\max_z p(z \\mid x)$.\n\n**3. Asymmetric Linear Loss:** $L(\\hat{z},z) = c_{+} \\, (z - \\hat{z})_{+} + c_{-} \\, (\\hat{z} - z)_{+}$\nThe risk can be written as:\n$$\n\\mathcal{R}(\\hat{z};x) = \\int_{-\\infty}^{\\hat{z}} c_{-} (\\hat{z} - z) p(z \\mid x) \\, dz + \\int_{\\hat{z}}^{\\infty} c_{+} (z - \\hat{z}) p(z \\mid x) \\, dz\n$$\nWe differentiate with respect to $\\hat{z}$ using the Leibniz integral rule:\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = \\int_{-\\infty}^{\\hat{z}} c_{-} p(z \\mid x) \\, dz - \\int_{\\hat{z}}^{\\infty} c_{+} p(z \\mid x) \\, dz\n$$\nLet $F_{Z \\mid X}(z \\mid x)$ be the cumulative distribution function (CDF) of the posterior $p(z \\mid x)$. The derivative is:\n$$\n\\frac{\\partial \\mathcal{R}}{\\partial \\hat{z}} = c_{-} F_{Z \\mid X}(\\hat{z} \\mid x) - c_{+} [1 - F_{Z \\mid X}(\\hat{z} \\mid x)]\n$$\nSetting this to zero to find the optimal $\\hat{z}$:\n$$\nc_{-} F_{Z \\mid X}(\\hat{z} \\mid x) = c_{+} - c_{+} F_{Z \\mid X}(\\hat{z} \\mid x)\n$$\n$$\n(c_{+} + c_{-}) F_{Z \\mid X}(\\hat{z} \\mid x) = c_{+}\n$$\n$$\nF_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}\n$$\nThe second derivative is $(c_{+} + c_{-})p(\\hat{z} \\mid x) > 0$, confirming a minimum. The optimal estimator is the quantile of the posterior distribution corresponding to the cumulative probability $\\frac{c_{+}}{c_{+} + c_{-}}$.\n\n### Evaluation of Options\n\n**A. The Bayes estimator that minimizes posterior expected loss is $\\hat{z}(x) = \\arg\\min_{\\hat{z}} \\int L(\\hat{z},z) \\, p(z \\mid x) \\, dz$; under squared-error loss $L(\\hat{z},z) = (z - \\hat{z})^{2}$ it is $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$; under a limiting neighborhood version of $0$-$1$ loss in the continuous case it is the posterior mode $\\hat{z}_{\\mathrm{MAP}}(x)$; and under the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hat{z})_{+} + c_{-} (\\hat{z} - z)_{+}$ it is the posterior quantile solving $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$, which coincides with both $\\hat{z}_{\\mathrm{MMSE}}$ and $\\hat{z}_{\\mathrm{MAP}}$ only when $p(z \\mid x)$ is symmetric unimodal and $c_{+} = c_{-}$.**\nThis statement is a comprehensive summary.\n- The initial definition of the Bayes estimator is correct.\n- The identification of the MMSE estimator for squared-error loss is correct, as shown in our derivation.\n- The identification of the MAP estimator for 0-1 loss is correct, as shown in our derivation.\n- The identification of the quantile estimator for asymmetric linear loss, with the correct formula $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$, is correct, as shown in our derivation.\n- The final condition for coincidence: If $p(z \\mid x)$ is symmetric and unimodal, its mean ($\\hat{z}_{\\mathrm{MMSE}}$), median, and mode ($\\hat{z}_{\\mathrm{MAP}}$) are all equal. If $c_{+} = c_{-}$, the optimal quantile is at probability $\\frac{c_{+}}{c_{+} + c_{-}} = \\frac{1}{2}$, which is the median. Therefore, under these conditions, all three estimators coincide. The \"only when\" part holds for a broad class of well-behaved unimodal distributions typically used in such models. For the estimators to coincide, we need mean = mode = $q$-quantile. For a unimodal distribution, mean=mode implies symmetry. Symmetry implies mean=median. So the $q$-quantile must be the median, which requires $q=1/2$, and thus $c_+=c_-$. The statement is accurate in the standard context.\nVerdict: **Correct**.\n\n**B. The MAP estimator $\\hat{z}_{\\mathrm{MAP}}(x)$ minimizes expected loss for any loss function $L(\\hat{z},z)$ because it maximizes $p(z \\mid x)$.**\nThis statement is false. The optimal Bayes estimator is dependent on the specific loss function. As derived above, squared-error loss leads to the posterior mean ($\\hat{z}_{\\mathrm{MMSE}}$), not the posterior mode ($\\hat{z}_{\\mathrm{MAP}}$), unless the posterior is symmetric and unimodal. Maximizing the posterior density is a distinct optimization criterion from minimizing the posterior expected loss for a general loss function.\nVerdict: **Incorrect**.\n\n**C. For any convex loss function $L(\\hat{z},z)$, the Bayes-optimal estimator is always $\\hat{z}_{\\mathrm{MMSE}}(x) = \\mathbb{E}[z \\mid x]$, so asymmetry in loss does not alter the optimal estimator.**\nThis statement is false. The asymmetric linear loss function is convex in $\\hat{z}$, but as we derived, its optimal estimator is a posterior quantile, not the posterior mean (unless the quantile happens to equal the mean). Another counterexample is the absolute error loss $L(\\hat{z},z) = |z-\\hat{z}|$, which is also convex and yields the posterior median as the optimal estimator. The assertion that asymmetry does not alter the estimator is also false; if $c_{+} \\neq c_{-}$, the quantile is not the median, demonstrating the effect of asymmetry.\nVerdict: **Incorrect**.\n\n**D. For the asymmetric linear loss $L(\\hat{z},z) = c_{+} (z - \\hatz)_{+} + c_{-} (\\hat{z} - z)_{+}$ with $c_{+} > c_{-}$, the Bayes-optimal estimator satisfies $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{-}}{c_{+} + c_{-}}$, implying a downward shift of the estimator relative to $\\mathbb{E}[z \\mid x]$.**\nThis statement contains two errors. First, our derivation shows the correct formula is $F_{Z \\mid X}(\\hat{z} \\mid x) = \\frac{c_{+}}{c_{+} + c_{-}}$. The statement incorrectly has $c_{-}$ in the numerator. Second, a high cost for underestimation ($c_{+} > c_{-}$) should intuitively lead to an upward bias in the estimate to avoid costly underestimates. The correct formula confirms this: if $c_{+} > c_{-}$, then $\\frac{c_{+}}{c_{+} + c_{-}} > \\frac{1}{2}$, so the optimal estimate is a quantile above the median, representing an upward shift for symmetric posteriors. The incorrect formula in the statement would imply a quantile below the median, leading to the incorrect conclusion of a downward shift.\nVerdict: **Incorrect**.\n\n**E. If $p(z \\mid x)$ is Gaussian and the loss is squared-error, then $\\hat{z}_{\\mathrm{MAP}}(x) = \\hat{z}_{\\mathrm{MMSE}}(x)$; under the asymmetric linear loss with $c_{+} \\neq c_{-}$, the Bayes-optimal estimator is a posterior quantile rather than the posterior mean, so $\\hat{z}_{\\mathrm{MMSE}}(x)$ is not optimal for that loss.**\n- First part: A Gaussian posterior is a symmetric and unimodal distribution. Its mean, median, and mode are all located at the center of the distribution. Therefore, for a Gaussian posterior, $\\hat{z}_{\\mathrm{MAP}}(x)$ (the mode) is equal to $\\hat{z}_{\\mathrm{MMSE}}(x)$ (the mean). This is correct.\n- Second part: For the asymmetric linear loss with $c_{+} \\neq c_{-}$, the optimal estimator is the quantile at $q = \\frac{c_{+}}{c_{+} + c_{-}} \\neq \\frac{1}{2}$. For a Gaussian distribution (or any symmetric one), the mean is the median (the $1/2$-quantile). Since the optimal estimator is a quantile different from the median, it is not the posterior mean. This is correct.\n- Conclusion: Consequently, the MMSE estimator is not optimal for this asymmetric loss. This is the correct logical conclusion.\nThe entire statement is factually correct.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "A Bayesian agent's inferences are only as good as its internal model of the world. This exercise explores the consequences of a mismatched prior, where the brain's assumptions do not align with the true environmental statistics . By analytically deriving the mean squared error, you will quantify how inaccuracies in the assumed prior contribute to both systematic bias and variance in perceptual estimates.",
            "id": "4008934",
            "problem": "Consider a single latent scalar stimulus $z$ and a noisy measurement $x$ generated by the true environment according to the following linear Gaussian model: the latent variable $z$ is drawn from a Gaussian prior $p_{\\text{true}}(z) = \\mathcal{N}(z; \\mu, \\sigma_{z}^{2})$, and the measurement is given by $x = z + \\varepsilon$, where the measurement noise $\\varepsilon$ is independent of $z$ and distributed as $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$. A perceptual estimator (e.g., a brain performing probabilistic inference) forms an internal prior $p_{\\text{int}}(z) = \\mathcal{N}(z; \\mu_{0}, \\tau^{2})$ that is potentially mismatched to the true environment, and uses Bayesian least squares (posterior mean) under this internal prior to produce an estimate $\\hat{z}(x)$ from the observation $x$.\n\nStarting from Bayes’ rule and the definition of Gaussian likelihoods and priors, derive the estimator $\\hat{z}(x)$ as the posterior mean under the internal prior $p_{\\text{int}}(z)$ and the likelihood implied by the measurement model. Then, under the true generative process specified above, compute the mean squared error (MSE), defined as $\\mathbb{E}\\!\\left[\\left(\\hat{z}(x) - z\\right)^{2}\\right]$, where the expectation is with respect to the true joint distribution of $(z, x)$ induced by $p_{\\text{true}}(z)$ and $p(x \\mid z)$. Express your final answer as a single, fully simplified analytic expression in terms of the parameters $\\mu$, $\\mu_0$, $\\sigma_z^2$, $\\sigma^2$, and $\\tau^2$. No numerical approximation is required, and you should not leave any integrals unevaluated. The final answer must be an analytic expression; do not report any inequalities or equations other than the requested expression.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It formalizes a standard scenario in computational neuroscience and Bayesian statistics concerning mismatched priors. All parameters and distributions are clearly defined, and the task is to derive a specific analytical quantity, the Mean Squared Error (MSE), under these conditions. The problem is valid and admits a unique solution.\n\nThe solution process involves two main parts: first, deriving the Bayesian estimator $\\hat{z}(x)$ under the agent's internal model, and second, calculating the MSE of this estimator under the true generative model of the environment.\n\nPart 1: Derivation of the Estimator $\\hat{z}(x)$\n\nThe estimator $\\hat{z}(x)$ is the posterior mean under the internal model. The internal model consists of the internal prior $p_{\\text{int}}(z)$ and the likelihood $p(x|z)$.\n\nThe internal prior is given as a Gaussian distribution:\n$$p_{\\text{int}}(z) = \\mathcal{N}(z; \\mu_{0}, \\tau^{2}) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{(z-\\mu_0)^2}{2\\tau^2}\\right)$$\n\nThe measurement model is $x = z + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$. This implies that the conditional distribution of the measurement $x$ given the latent variable $z$, i.e., the likelihood function, is:\n$$p(x|z) = \\mathcal{N}(x; z, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-z)^2}{2\\sigma^2}\\right)$$\n\nAccording to Bayes' rule, the posterior distribution of $z$ given $x$ under the internal model is proportional to the product of the likelihood and the internal prior:\n$$p_{\\text{int}}(z|x) \\propto p(x|z) p_{\\text{int}}(z)$$\n$$p_{\\text{int}}(z|x) \\propto \\exp\\left(-\\frac{(x-z)^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(z-\\mu_0)^2}{2\\tau^2}\\right)$$\n$$p_{\\text{int}}(z|x) \\propto \\exp\\left(-\\frac{1}{2} \\left[ \\frac{(z-x)^2}{\\sigma^2} + \\frac{(z-\\mu_0)^2}{\\tau^2} \\right] \\right)$$\n\nTo find the parameters of the posterior distribution, we complete the square for $z$ in the exponent. The term inside the square brackets is:\n$$ \\frac{z^2 - 2zx + x^2}{\\sigma^2} + \\frac{z^2 - 2z\\mu_0 + \\mu_0^2}{\\tau^2} = z^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2z \\left(\\frac{x}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}\\right) + \\text{terms not dependent on } z $$\nThe posterior distribution $p_{\\text{int}}(z|x)$ is a Gaussian, let's say $\\mathcal{N}(z; \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$, whose exponent is of the form $-\\frac{(z-\\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2}$. Expanding this gives $-\\frac{1}{2\\sigma_{\\text{post}}^2} (z^2 - 2z\\mu_{\\text{post}} + \\mu_{\\text{post}}^2)$.\n\nBy comparing the coefficients of $z^2$ and $z$, we can identify the posterior variance $\\sigma_{\\text{post}}^2$ and mean $\\mu_{\\text{post}}$.\nThe precision (inverse variance) of the posterior is the sum of the precisions of the likelihood and the prior:\n$$\\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2} = \\frac{\\tau^2 + \\sigma^2}{\\sigma^2 \\tau^2} \\implies \\sigma_{\\text{post}}^2 = \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2}$$\nThe mean of the posterior is given by:\n$$\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2} = \\frac{x}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}$$\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left(\\frac{x}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}\\right) = \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2} \\left(\\frac{x\\tau^2 + \\mu_0\\sigma^2}{\\sigma^2\\tau^2}\\right) = \\frac{\\tau^2 x + \\sigma^2 \\mu_0}{\\sigma^2 + \\tau^2}$$\nThe estimator $\\hat{z}(x)$ is the posterior mean $\\mu_{\\text{post}}$.\n$$\\hat{z}(x) = \\frac{\\tau^2}{\\sigma^2 + \\tau^2}x + \\frac{\\sigma^2}{\\sigma^2 + \\tau^2}\\mu_0$$\n\nPart 2: Computation of the Mean Squared Error (MSE)\n\nThe MSE is defined as the expectation of the squared error, $\\mathbb{E}[(\\hat{z}(x) - z)^2]$, where the expectation is taken with respect to the true generative process. In the true process, $z \\sim p_{\\text{true}}(z) = \\mathcal{N}(z; \\mu, \\sigma_z^2)$ and $x = z + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The variables $z$ and $\\varepsilon$ are independent.\n\nFirst, we express the error term $\\hat{z}(x) - z$ in terms of $z$ and $\\varepsilon$:\n$$\\hat{z}(x) - z = \\left(\\frac{\\tau^2 x + \\sigma^2 \\mu_0}{\\sigma^2 + \\tau^2}\\right) - z$$\nSubstitute $x = z + \\varepsilon$:\n$$\\hat{z}(x) - z = \\frac{\\tau^2(z+\\varepsilon) + \\sigma^2 \\mu_0}{\\sigma^2 + \\tau^2} - z = \\frac{\\tau^2 z + \\tau^2 \\varepsilon + \\sigma^2 \\mu_0 - z(\\sigma^2 + \\tau^2)}{\\sigma^2 + \\tau^2}$$\n$$= \\frac{(\\tau^2 - \\sigma^2 - \\tau^2)z + \\tau^2 \\varepsilon + \\sigma^2 \\mu_0}{\\sigma^2 + \\tau^2} = \\frac{-\\sigma^2 z + \\tau^2 \\varepsilon + \\sigma^2 \\mu_0}{\\sigma^2 + \\tau^2}$$\n$$= \\frac{\\tau^2 \\varepsilon - \\sigma^2 (z - \\mu_0)}{\\sigma^2 + \\tau^2}$$\nNow, we compute the expectation of the square of this expression:\n$$\\text{MSE} = \\mathbb{E}\\left[\\left(\\frac{\\tau^2 \\varepsilon - \\sigma^2 (z - \\mu_0)}{\\sigma^2 + \\tau^2}\\right)^2\\right] = \\frac{1}{(\\sigma^2 + \\tau^2)^2} \\mathbb{E}\\left[(\\tau^2 \\varepsilon - \\sigma^2 (z - \\mu_0))^2\\right]$$\nExpanding the squared term inside the expectation:\n$$(\\tau^2 \\varepsilon - \\sigma^2 (z - \\mu_0))^2 = (\\tau^2)^2 \\varepsilon^2 - 2\\tau^2\\sigma^2\\varepsilon(z-\\mu_0) + (\\sigma^2)^2 (z-\\mu_0)^2$$\nWe take the expectation of this sum term by term. The expectation is over the true distributions of $z$ and $\\varepsilon$.\n1.  $\\mathbb{E}[(\\tau^2)^2 \\varepsilon^2] = \\tau^4 \\mathbb{E}[\\varepsilon^2]$. Since $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, its mean is $\\mathbb{E}[\\varepsilon]=0$ and its variance is $\\text{Var}(\\varepsilon)=\\sigma^2$. We know $\\text{Var}(\\varepsilon) = \\mathbb{E}[\\varepsilon^2] - (\\mathbb{E}[\\varepsilon])^2$, so $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$. Thus, this term is $\\tau^4 \\sigma^2$.\n2.  $\\mathbb{E}[-2\\tau^2\\sigma^2\\varepsilon(z-\\mu_0)] = -2\\tau^2\\sigma^2 \\mathbb{E}[\\varepsilon(z-\\mu_0)]$. Since $z$ and $\\varepsilon$ are independent, $\\mathbb{E}[\\varepsilon(z-\\mu_0)] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[z-\\mu_0]$. As $\\mathbb{E}[\\varepsilon]=0$, this term is $0$.\n3.  $\\mathbb{E}[(\\sigma^2)^2 (z-\\mu_0)^2] = \\sigma^4 \\mathbb{E}[(z-\\mu_0)^2]$. For a random variable $Y$ and constant $c$, $\\mathbb{E}[(Y-c)^2] = \\text{Var}(Y) + (\\mathbb{E}[Y]-c)^2$. Here, $Y=z$ and $c=\\mu_0$. Under the true distribution, $\\mathbb{E}[z]=\\mu$ and $\\text{Var}(z)=\\sigma_z^2$. Therefore, $\\mathbb{E}[(z-\\mu_0)^2] = \\text{Var}(z) + (\\mathbb{E}[z]-\\mu_0)^2 = \\sigma_z^2 + (\\mu - \\mu_0)^2$. This term becomes $\\sigma^4 (\\sigma_z^2 + (\\mu - \\mu_0)^2)$.\n\nCombining these results, the expectation is:\n$$\\mathbb{E}\\left[(\\tau^2 \\varepsilon - \\sigma^2 (z - \\mu_0))^2\\right] = \\tau^4 \\sigma^2 + 0 + \\sigma^4 (\\sigma_z^2 + (\\mu - \\mu_0)^2)$$\nFinally, substituting this back into the MSE expression:\n$$\\text{MSE} = \\frac{\\tau^4 \\sigma^2 + \\sigma^4 (\\sigma_z^2 + (\\mu - \\mu_0)^2)}{(\\sigma^2 + \\tau^2)^2}$$\nExpanding the second term in the numerator for the final expression gives:\n$$\\text{MSE} = \\frac{\\tau^4 \\sigma^2 + \\sigma^4 \\sigma_z^2 + \\sigma^4 (\\mu - \\mu_0)^2}{(\\sigma^2 + \\tau^2)^2}$$\nThis expression represents the total mean squared error, which is composed of three parts: a term related to the measurement noise $(\\tau^4 \\sigma^2)$, a term related to the variance mismatch $(\\sigma^4 \\sigma_z^2)$, and a term related to the bias from the prior mean mismatch $(\\sigma^4 (\\mu - \\mu_0)^2)$, all scaled by the squared sum of the internal model's precisions' reciprocals.",
            "answer": "$$\\boxed{\\frac{\\tau^4 \\sigma^2 + \\sigma^4 \\sigma_z^2 + \\sigma^4 (\\mu - \\mu_0)^2}{(\\sigma^2 + \\tau^2)^2}}$$"
        }
    ]
}