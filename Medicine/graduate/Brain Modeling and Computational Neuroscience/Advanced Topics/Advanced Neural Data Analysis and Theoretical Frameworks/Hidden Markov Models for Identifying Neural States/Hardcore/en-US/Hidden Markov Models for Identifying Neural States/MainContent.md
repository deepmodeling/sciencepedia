## Introduction
Neural activity is inherently dynamic, with brain circuits transitioning between distinct computational states to support cognition and behavior. However, these underlying states are not directly observable; we can only access them through noisy, high-dimensional measurements like spike trains or local field potentials. This presents a fundamental challenge in computational neuroscience: how can we infer the sequence of hidden neural states from the complex data we record? The Hidden Markov Model (HMM) offers a powerful and principled probabilistic framework for solving this problem, providing a statistical lens to decode the brain's latent dynamics.

This article offers a graduate-level exploration of HMMs for neural state analysis. We will build a complete understanding of this essential tool, from its mathematical underpinnings to its practical implementation. The first chapter, **Principles and Mechanisms**, delves into the formal definition of the HMM, the elegant algorithms for inference and learning, and critical practical considerations for robust application. The second chapter, **Applications and Interdisciplinary Connections**, demonstrates how HMMs are tailored to analyze diverse neural datasets and showcases their wide-ranging impact across other scientific disciplines. Finally, **Hands-On Practices** provides a set of targeted exercises to solidify your understanding by implementing key components of the HMM framework. By progressing through these chapters, you will gain the theoretical knowledge and practical intuition needed to apply HMMs to uncover the hidden structure in complex time-series data.

## Principles and Mechanisms

This chapter delineates the fundamental principles and mathematical mechanisms that define Hidden Markov Models (HMMs). We will formally construct the model, explore the core computational problems of inference and learning, and discuss several advanced topics critical for the robust application of HMMs to neural data. Our objective is to build a rigorous conceptual and practical foundation for identifying latent neural states from complex, time-varying neurophysiological recordings.

### Formal Definition of the Hidden Markov Model

A Hidden Markov Model is a probabilistic generative model for time-series data, characterized by an unobserved, or **hidden**, sequence of states that evolve according to a Markov chain, and a set of observed emissions that are conditionally dependent on these hidden states. To formally define an HMM for neural state analysis, we must specify its constituent components .

Let us consider a time series of neural observations $\{x_t\}_{t=1}^{T}$, where $x_t$ is a measurement recorded in time bin $t$. This measurement could be a vector of spike counts from a population of neurons, a [feature vector](@entry_id:920515) derived from the local field potential, or another summary of neural activity. The core hypothesis of the HMM is that this observed sequence is generated by an underlying, unobserved sequence of discrete neural states $\{z_t\}_{t=1}^{T}$. Each latent state variable $z_t$ takes a value from a [finite set](@entry_id:152247) $\{1, \dots, K\}$, where $K$ is the total number of putative neural states.

The complete parameter set of an HMM, denoted $\theta$, consists of three parts: $\theta = (\boldsymbol{\pi}, A, \Phi)$.

#### The Latent State Dynamics

The evolution of the hidden state sequence is governed by a first-order, time-homogeneous **Markov chain**. This is the structural backbone of the model and is defined by two key properties.

First, the state at time $t$ is conditionally independent of all past states given the state at time $t-1$. This is the **Markov property**:
$$
p(z_t \mid z_{t-1}, z_{t-2}, \dots, z_1) = p(z_t \mid z_{t-1})
$$
This assumption implies that the immediate past state, $z_{t-1}$, contains all the information necessary to predict the next state, $z_t$. In the language of graphical models, the state at time $t$ has only a single parent: the state at time $t-1$ .

The dynamics of this chain are specified by:
1.  **Initial State Distribution ($\boldsymbol{\pi}$)**: A categorical distribution over the first state, $z_1$. It is a vector of length $K$ where each element $\pi_k$ represents the probability that the sequence begins in state $k$:
    $$
    \pi_k = p(z_1 = k), \quad \text{with } \sum_{k=1}^K \pi_k = 1.
    $$

2.  **State Transition Matrix ($A$)**: A $K \times K$ matrix where each entry $A_{ij}$ specifies the probability of transitioning from state $i$ to state $j$ in one time step. Because the model is time-homogeneous, this matrix is constant for all time steps.
    $$
    A_{ij} = p(z_t = j \mid z_{t-1} = i)
    $$
    Since for any given starting state $i$, the system must transition to one of the $K$ possible states, each row of the transition matrix must sum to one: $\sum_{j=1}^K A_{ij} = 1$ for all $i \in \{1, \dots, K\}$ .

#### The Emission Process

The second defining characteristic of an HMM is the relationship between the hidden states and the observations. The observation at time $t$, $x_t$, is assumed to be conditionally independent of all other states and all other observations, given the current hidden state $z_t$.
$$
p(x_t \mid z_{1:T}, x_{1:t-1}) = p(x_t \mid z_t)
$$
This means that the state $z_t$ is the sole determinant of the statistical properties of the observation $x_t$. The full set of these [conditional probability](@entry_id:151013) distributions, one for each state, is known as the **emission model**, denoted $\Phi = \{\phi_k\}_{k=1}^K$, where $\phi_k$ are the parameters of the emission distribution for state $k$.

The choice of emission distribution family is critical and must be matched to the type of neural data being modeled.
-   For binned spike counts from a population of $D$ neurons, where $x_t \in \mathbb{N}^D$, a common choice is the **independent Poisson distribution**. For each state $k$, we define a vector of firing rates $\boldsymbol{\lambda}_k \in \mathbb{R}_{>0}^D$. The emission probability is the product of $D$ independent Poisson distributions:
    $$
    p(x_t \mid z_t = k) = \prod_{d=1}^D \text{Poisson}(x_{t,d}; \lambda_{k,d})
    $$
    where $x_{t,d}$ is the spike count for neuron $d$ in bin $t$, and $\lambda_{k,d}$ is its firing rate in state $k$ .

-   For continuous, vector-valued data, such as vectors of firing rates estimated over a sliding window or features of the [local field potential](@entry_id:1127395) where $x_t \in \mathbb{R}^D$, a **multivariate Gaussian distribution** is often employed. Each state $k$ is characterized by a [mean vector](@entry_id:266544) $\boldsymbol{\mu}_k \in \mathbb{R}^D$ and a covariance matrix $\Sigma_k \in \mathbb{R}^{D \times D}$:
    $$
    p(x_t \mid z_t = k) = \mathcal{N}(x_t; \boldsymbol{\mu}_k, \Sigma_k)
    $$
    This model allows each state to represent a distinct pattern of mean population activity and a unique structure of correlations between neural units .

#### The Generative Process and Joint Probability

The HMM provides a complete generative story for the data. A sequence of observations is generated as follows:
1.  Sample the initial state $z_1$ from the initial distribution $\boldsymbol{\pi}$.
2.  For $t=2, \dots, T$, sample the next state $z_t$ from the transition distribution given the previous state, $p(z_t \mid z_{t-1})$, i.e., from the $z_{t-1}$-th row of matrix $A$.
3.  For each $t=1, \dots, T$, sample the observation $x_t$ from the emission distribution corresponding to the current state, $p(x_t \mid z_t)$.

By combining the Markov property of the latent chain and the conditional independence of the emissions, the joint probability of a specific sequence of states $\mathbf{z}_{1:T}$ and observations $\mathbf{x}_{1:T}$ factorizes into a product of initial, transition, and emission probabilities:
$$
p(\mathbf{x}_{1:T}, \mathbf{z}_{1:T}) = p(z_1) \left( \prod_{t=2}^T p(z_t \mid z_{t-1}) \right) \left( \prod_{t=1}^T p(x_t \mid z_t) \right)
$$
In terms of the model parameters, this is:
$$
p(\mathbf{x}_{1:T}, \mathbf{z}_{1:T} \mid \theta) = \pi_{z_1} \left( \prod_{t=2}^T A_{z_{t-1}, z_t} \right) \left( \prod_{t=1}^T p(x_t \mid \phi_{z_t}) \right)
$$
This factorization is the mathematical foundation for all inference and learning algorithms in HMMs .

### The Core Computational Problems: Inference

The "hidden" nature of the states presents the primary analytical challenge. Unlike a fully observed Markov chain where the state sequence is known and the likelihood is simply $p(x_1) \prod p(x_t|x_{t-1})$, in an HMM we must infer the latent states from the observations . The main goal of inference is to compute posterior distributions over the latent states given the observed data. The most common inference tasks are [filtering and smoothing](@entry_id:188825).

-   **Filtering**: The task of estimating the distribution over the current state $z_t$ given all observations up to the current time, $p(z_t \mid x_{1:t})$. This is an *online* task, suitable for real-time analysis where data arrives sequentially.

-   **Smoothing**: The task of estimating the distribution over the state $z_t$ given the *entire* sequence of observations, $p(z_t \mid x_{1:T})$. This is an *offline* task performed after all data has been collected. Because smoothing incorporates evidence from both past ($x_{1:t-1}$) and future ($x_{t+1:T}$) observations relative to time $t$, it generally provides more accurate and robust state estimates than filtering. For [post-hoc analysis](@entry_id:165661) of neural recordings, smoothing is the standard approach.

#### The Forward-Backward Algorithm for Smoothing

Exact inference in HMMs is made tractable by the model's chain structure, which permits the use of [dynamic programming](@entry_id:141107). The cornerstone of HMM inference is the **Forward-Backward algorithm**. This algorithm efficiently computes the smoothed posterior probabilities, often denoted $\gamma_t(k) = p(z_t = k \mid x_{1:T})$, by defining two intermediate quantities .

1.  **The Forward Messages ($\alpha_t(k)$)**: The forward variable $\alpha_t(k)$ is the joint probability of observing the first $t$ observations and being in state $k$ at time $t$.
    $$
    \alpha_t(k) = p(x_{1:t}, z_t = k)
    $$
    These can be computed recursively. The initialization at $t=1$ is $\alpha_1(k) = \pi_k p(x_1 \mid z_1=k)$. The recursion for $t > 1$ is:
    $$
    \alpha_t(k) = \left( \sum_{j=1}^K \alpha_{t-1}(j) A_{jk} \right) p(x_t \mid z_t=k)
    $$
    The [forward pass](@entry_id:193086) propagates information from the past ($t=1$) to the future ($t=T$).

2.  **The Backward Messages ($\beta_t(k)$)**: The backward variable $\beta_t(k)$ is the conditional probability of observing the future data from time $t+1$ to $T$, given that the system is in state $k$ at time $t$.
    $$
    \beta_t(k) = p(x_{t+1:T} \mid z_t = k)
    $$
    These are also computed recursively, but starting from the end of the sequence. The initialization at $t=T$ is $\beta_T(k) = 1$ for all $k$. The [recursion](@entry_id:264696) proceeds backwards from $t=T-1$ to $1$:
    $$
    \beta_t(k) = \sum_{j=1}^K A_{kj} p(x_{t+1} \mid z_{t+1}=j) \beta_{t+1}(j)
    $$
    The backward pass propagates evidence from the future back to the past.

By combining the information from the past (summarized by $\alpha_t(k)$) and the future (summarized by $\beta_t(k)$), we can compute the smoothed posterior for state $k$ at time $t$. Using the definition of conditional probability and the HMM's independence properties, we arrive at the elegant result :
$$
\gamma_t(k) = p(z_t=k \mid x_{1:T}) = \frac{p(x_{1:T}, z_t=k)}{p(x_{1:T})} = \frac{\alpha_t(k) \beta_t(k)}{\sum_{j=1}^K \alpha_t(j) \beta_t(j)}
$$
The complexity of this algorithm is $\mathcal{O}(K^2 T)$, making exact inference feasible even for long time series. This contrasts with a naive brute-force approach of summing over all $K^T$ latent paths, which is computationally intractable.

### Learning Model Parameters: The Expectation-Maximization Algorithm

The second fundamental problem is **learning**: given an observed sequence $x_{1:T}$, how do we estimate the model parameters $\theta = (\boldsymbol{\pi}, A, \Phi)$? The standard approach is **maximum likelihood estimation**, which aims to find the parameters that maximize the marginal log-likelihood of the data, $\ln p(x_{1:T} \mid \theta)$.

Directly maximizing this function is difficult because it involves a sum over all $K^T$ latent paths. The **Expectation-Maximization (EM) algorithm** provides an [iterative method](@entry_id:147741) to find a [local maximum](@entry_id:137813) of the [likelihood function](@entry_id:141927) in models with [latent variables](@entry_id:143771). EM alternates between two steps:

1.  **E-Step (Expectation)**: Given the current parameter estimates $\theta^{\text{old}}$, compute the posterior distribution over the [latent variables](@entry_id:143771), $p(z_{1:T} \mid x_{1:T}, \theta^{\text{old}})$. Then, compute the expected value of the complete-data [log-likelihood](@entry_id:273783), $Q(\theta \mid \theta^{\text{old}}) = E[\ln p(x_{1:T}, z_{1:T} \mid \theta)]$. In practice, this step involves using the Forward-Backward algorithm to compute the required smoothed posteriors: the state occupancies $\gamma_t(k) = p(z_t=k \mid x_{1:T}, \theta^{\text{old}})$ and the [transition probabilities](@entry_id:158294) $\xi_t(i,j) = p(z_{t-1}=i, z_t=j \mid x_{1:T}, \theta^{\text{old}})$.

2.  **M-Step (Maximization)**: Find the new parameter estimates $\theta^{\text{new}}$ that maximize the $Q$ function. This step is equivalent to a weighted maximum likelihood estimation, where the weights are the posterior probabilities computed in the E-step.
    $$
    \theta^{\text{new}} = \arg\max_{\theta} Q(\theta \mid \theta^{\text{old}})
    $$

The updates for the transition and emission parameters are decoupled and can be solved independently. The updates for $\boldsymbol{\pi}$ and $A$ are intuitive:
-   $\pi_k^{\text{new}} \propto \gamma_1(k)$ (The initial probability of state $k$ is its expected occupancy at time $t=1$).
-   $A_{ij}^{\text{new}} = \frac{\sum_{t=2}^T \xi_t(i,j)}{\sum_{t=2}^T \gamma_{t-1}(i)}$ (The [transition probability](@entry_id:271680) from $i$ to $j$ is the expected number of transitions from $i$ to $j$ divided by the expected number of times the system was in state $i$).

The M-step updates for emission parameters depend on the chosen family. Let's examine two important cases.

#### M-Step for Poisson Emissions

Consider the case of modeling spike counts $x_t$, where the emission model is $p(x_t \mid z_t=k) = \text{Poisson}(x_t; \lambda_k \Delta t)$. Here, $\lambda_k$ is the firing rate (spikes/time) and $\Delta t$ is the duration of the time bin. The parameter to be estimated is the rate $\lambda_k$. The part of the $Q$ function relevant to $\lambda_k$ is:
$$
Q_k(\lambda_k) = \sum_{t=1}^T \gamma_t(k) \ln p(x_t \mid z_t=k, \lambda_k) = \sum_{t=1}^T \gamma_t(k) \left( -\lambda_k \Delta t + x_t \ln(\lambda_k \Delta t) - \ln(x_t!) \right)
$$
To maximize this with respect to $\lambda_k$, we set its derivative to zero and solve . This yields the update:
$$
\lambda_k^{\text{new}} = \frac{\sum_{t=1}^{T} \gamma_{t}(k) x_{t}}{ \left( \sum_{t=1}^{T} \gamma_{t}(k) \right) \Delta t}
$$
This result is highly interpretable: the new estimate for the rate in state $k$ is the total expected number of spikes emitted in that state (numerator) divided by the total expected time spent in that state (denominator). The inclusion of $\Delta t$ is crucial for ensuring the estimated parameter $\lambda_k$ is a true physical rate, independent of the chosen bin size.

#### M-Step for Gaussian Emissions

Now consider vector-valued observations $x_t \in \mathbb{R}^D$ with Gaussian emissions $p(x_t \mid z_t=k) = \mathcal{N}(x_t; \boldsymbol{\mu}_k, \Sigma_k)$. Maximizing the corresponding part of the $Q$ function with respect to $\boldsymbol{\mu}_k$ and $\Sigma_k$ yields the following updates :
$$
\boldsymbol{\mu}_k^{\text{new}} = \frac{\sum_{t=1}^{T} \gamma_t(k) x_t}{\sum_{t=1}^{T} \gamma_t(k)}
$$
$$
\Sigma_k^{\text{new}} = \frac{\sum_{t=1}^{T} \gamma_t(k) (x_t - \boldsymbol{\mu}_k^{\text{new}})(x_t - \boldsymbol{\mu}_k^{\text{new}})^T}{\sum_{t=1}^{T} \gamma_t(k)}
$$
These are the familiar expressions for the mean and covariance of a Gaussian distribution, but here they are *weighted* by the posterior responsibilities $\gamma_t(k)$. Each data point $x_t$ contributes to the estimate of state $k$'s parameters in proportion to the posterior probability that the system was in state $k$ when that data point was generated.

### Advanced Topics and Practical Considerations

Applying HMMs effectively requires an understanding of their implicit assumptions and potential pitfalls. We now turn to several advanced topics that are crucial for practitioners.

#### Model Selection and Diagnosis

A key question is whether an HMM is an appropriate model for a given dataset. One way to address this is to compare it with simpler alternatives, such as a non-switching **[linear dynamical system](@entry_id:1127277) (LDS)**. An LDS assumes the data is generated by a single continuous latent state evolving with linear-Gaussian dynamics. A fundamental property of an LDS is that its one-step-ahead predictive distribution, $p(y_t \mid y_{1:t-1})$, is always a single, unimodal Gaussian distribution.

In contrast, an HMM (or more generally, a switching [linear dynamical system](@entry_id:1127277)) generates observations from a mixture of distributions, one for each discrete state. Its one-step-ahead predictive distribution is a **mixture of Gaussians**, which can be multimodal. This provides a powerful diagnostic tool. If, after fitting a simple LDS model, the predictive residuals (the difference between observations and predictions) exhibit a multimodal distribution (e.g., a bimodal histogram), this is strong evidence of model misfit. Such multimodality indicates the presence of discrete underlying regimes that the single-mode LDS cannot capture, suggesting that a switching model like an HMM is necessary .

#### Identifiability and the Label Switching Problem

A critical theoretical property of any statistical model is **identifiability**: can the true parameters that generated the data be uniquely recovered, given an infinite amount of data? For HMMs, the answer is yes, but only up to a permutation of the state labels. This is because the likelihood of the observed data is invariant to a consistent relabeling of the hidden states .

If we have a parameter set $\theta = (\boldsymbol{\pi}, A, \Phi)$ and we permute the state labels according to a permutation $\sigma$, we can define a new parameter set $\theta'$. For a two-state model with the swap permutation $\sigma(1)=2, \sigma(2)=1$, the transformed parameters are: $\pi'_1 = \pi_2, \pi'_2=\pi_1$; $A'_{11}=A_{22}, A'_{12}=A_{21}, A'_{21}=A_{12}, A'_{22}=A_{11}$; and the emission parameters are swapped, $\phi'_1=\phi_2, \phi'_2=\phi_1$. For any such permutation, the marginal likelihood is unchanged:
$$
p(x_{1:T} \mid \theta') = p(x_{1:T} \mid \theta)
$$
This is known as the **[label switching](@entry_id:751100)** problem. It has profound practical consequences.
-   In maximum likelihood estimation with EM, the algorithm may converge to any of the $K!$ equivalent parameter settings.
-   In Bayesian inference using MCMC, the posterior distribution $p(\theta \mid x_{1:T})$ will possess $K!$ symmetric modes. A sampler may get trapped in one mode, failing to explore the full posterior, leading to poor mixing and incorrect parameter summaries.

The solution is to impose **[identifiability](@entry_id:194150) constraints** on the parameters to select a single, canonical representative from each set of equivalent solutions. For example, if the states have distinct emission means, we can enforce an ordering, such as $\mu_1  \mu_2  \dots  \mu_K$. This breaks the [permutation symmetry](@entry_id:185825) without altering the statistical model itself .

Deeper identifiability requires that the emission distributions for each state be distinct and [linearly independent](@entry_id:148207), and that the transition matrix be non-degenerate. When these conditions are violated (e.g., two states have identical emission distributions), states become fundamentally indistinguishable. Even when these conditions hold, if emission distributions overlap strongly, the model is theoretically identifiable but may be practically non-identifiable. This situation manifests as a nearly singular **Fisher Information Matrix (FIM)**, leading to extremely high variance in parameter estimates from finite data .

#### Implicit Assumptions: The Dwell-Time Distribution

A standard HMM makes a strong, implicit assumption about the time spent in each state. The **dwell time**, defined as the number of consecutive time steps spent in a state, follows a **[geometric distribution](@entry_id:154371)**. This can be seen directly from the Markov property . Once in state $k$, the probability of remaining in state $k$ at the next step is $A_{kk}$, and the probability of leaving is $1-A_{kk}$. The probability of a dwell time of exactly $d$ steps is the probability of $d-1$ self-transitions followed by one transition out:
$$
p(D=d \mid \text{enter } k) = (A_{kk})^{d-1} (1-A_{kk})
$$
The expected dwell time is $\mathbb{E}[D] = \frac{1}{1-A_{kk}}$. A key feature of the [geometric distribution](@entry_id:154371) is its "memoryless" property: the probability of leaving a state at the next step (the hazard rate) is constant, $1-A_{kk}$, regardless of how long the system has already been in that state.

This memoryless assumption is often a poor fit for biological processes, including neural dynamics. Neural states can exhibit persistence, adaptation, or metastability, where the probability of transitioning may change as a function of time spent in the state. Empirical dwell-time distributions in neural data are often found to have "heavier tails" than the [geometric distribution](@entry_id:154371), meaning that very long durations occur more frequently than predicted by the HMM. To address this limitation, extensions such as **Hidden Semi-Markov Models (HSMMs)** have been developed. HSMMs replace the implicit geometric dwell time with an explicit, arbitrary duration distribution for each state, allowing for more realistic modeling of state persistence .