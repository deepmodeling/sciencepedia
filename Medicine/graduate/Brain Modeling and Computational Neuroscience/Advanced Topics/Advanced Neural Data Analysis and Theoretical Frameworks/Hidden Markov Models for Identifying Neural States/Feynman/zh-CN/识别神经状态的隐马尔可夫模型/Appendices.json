{
    "hands_on_practices": [
        {
            "introduction": "要真正理解隐马尔可夫模型（HMM）的内在机制，最有效的方法之一是从其核心定义出发进行计算。这个练习要求您通过暴力枚举所有可能的隐藏状态序列，来计算一个简短观测序列的边缘对数似然。通过这种第一性原理的计算，您将对模型的生成过程和似然函数的构成建立一个坚实而直观的理解，并体会到开发更高效算法的必要性 。",
            "id": "3987961",
            "problem": "考虑一个双状态隐马尔可夫模型 (HMM)，它用于从一个标量神经特征时间序列中识别潜在的神经状态。设时间 $t$ 的潜在状态为 $z_t \\in \\{1,2\\}$，其初始分布为 $\\pi = [0.5, 0.5]$，转移矩阵为\n$$\nA=\\begin{pmatrix}\n0.9  0.1 \\\\\n0.2  0.8\n\\end{pmatrix}.\n$$\n观测模型是高斯模型：对于每个时间 $t$，观测到的标量 $x_t$ 在给定潜在状态的条件下是条件独立的，并且满足\n$$\nx_t \\mid z_t=k \\sim \\mathcal{N}(\\mu_k, \\sigma^2),\n$$\n其中 $\\mu_1 = 0$，$\\mu_2 = 3$，且 $\\sigma = 1$。仅使用隐马尔可夫模型 (HMM) 的核心定义——即潜在链的马尔可夫性质、观测值在给定潜在状态下的条件独立性以及高斯发射密度——从第一性原理出发，为长度为3的观测序列\n$$\nx_{1:3} = [0,\\, 3,\\, 3],\n$$\n推导出边际对数似然 $\\log p(x_{1:3})$，方法是通过在以下定义中对所有潜在序列 $z_{1:3} \\in \\{1,2\\}^3$ 进行显式求和\n$$\np(x_{1:3}) \\;=\\; \\sum_{z_{1:3} \\in \\{1,2\\}^3} p(z_1)\\, p(x_1 \\mid z_1)\\, p(z_2 \\mid z_1)\\, p(x_2 \\mid z_2)\\, p(z_3 \\mid z_2)\\, p(x_3 \\mid z_3).\n$$\n将你的最终答案表示为一个用 $\\ln(\\cdot)$ 和 $\\exp(\\cdot)$ 表示的、明确所有常数的单一闭合形式解析表达式。不要近似或四舍五入；提供精确的解析表达式。最终答案应该是一个以奈特 (nats) 为单位的纯数。",
            "solution": "### 步骤 1：提取已知条件\n- **潜在状态**：$z_t \\in \\{1, 2\\}$。\n- **初始分布**：$\\pi = [\\pi_1, \\pi_2] = [0.5, 0.5]$。这意味着 $p(z_1=1) = 0.5$ 且 $p(z_1=2) = 0.5$。\n- **转移矩阵**：从状态 $i$ 转移到状态 $j$ 的概率由 $A_{ij} = p(z_t=j \\mid z_{t-1}=i)$ 给出。\n$$\nA=\\begin{pmatrix}\nA_{11}  A_{12} \\\\\nA_{21}  A_{22}\n\\end{pmatrix} = \\begin{pmatrix}\n0.9  0.1 \\\\\n0.2  0.8\n\\end{pmatrix}\n$$\n- **观测模型**：给定潜在状态 $z_t=k$ 时观测到 $x_t$ 的概率是一个高斯分布，$p(x_t \\mid z_t=k) = \\mathcal{N}(x_t; \\mu_k, \\sigma^2)$。\n- **发射参数**：状态 1 的均值为 $\\mu_1 = 0$。状态 2 的均值为 $\\mu_2 = 3$。两个状态的标准差均为 $\\sigma = 1$。\n- **观测序列**：一个长度为 3 的序列：$x_{1:3} = [x_1, x_2, x_3] = [0, 3, 3]$。\n- **任务**：通过对所有 $2^3=8$ 种可能的潜在序列 $z_{1:3} \\in \\{1,2\\}^3$ 进行显式求和，计算边际对数似然 $\\ln p(x_{1:3})$。使用的公式为：\n$$\np(x_{1:3}) = \\sum_{z_{1:3} \\in \\{1,2\\}^3} p(z_1) p(x_1 \\mid z_1) p(z_2 \\mid z_1) p(x_2 \\mid z_2) p(z_3 \\mid z_2) p(x_3 \\mid z_3)\n$$\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，是隐马尔可夫模型在计算科学中的一个标准应用。它是一个适定问题，提供了所有必要的参数和明确的目标。语言客观而精确。所有数据和约束都是自洽且一致的。未发现任何缺陷。\n\n### 步骤 3：结论与行动\n该问题有效。将提供完整的解答。\n\n### 边际对数似然的推导\n\n边际似然 $p(x_{1:3})$ 是联合概率 $p(x_{1:3}, z_{1:3})$ 对所有可能的潜在状态序列 $z_{1:3}$ 的和。对于一个特定的序列 $z_{1:3} = (z_1, z_2, z_3)$，其联合概率由初始概率、转移概率和发射概率的乘积给出：\n$$\np(x_{1:3}, z_{1:3}) = p(z_1) \\cdot A_{z_1 z_2} \\cdot A_{z_2 z_3} \\cdot p(x_1 \\mid z_1) \\cdot p(x_2 \\mid z_2) \\cdot p(x_3 \\mid z_3)\n$$\n\n首先，我们定义每个状态的高斯发射概率密度函数：\n对于状态 $z_t=1$：\n$$\np(x_t \\mid z_t=1) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_t - \\mu_1)^2}{2\\sigma^2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_t^2}{2}\\right)\n$$\n对于状态 $z_t=2$：\n$$\np(x_t \\mid z_t=2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_t - \\mu_2)^2}{2\\sigma^2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_t - 3)^2}{2}\\right)\n$$\n\n接下来，我们为给定的观测序列 $x_{1:3} = [0, 3, 3]$ 计算这些发射概率：\n- 对于 $x_1=0$：\n  - $p(x_1=0 \\mid z_1=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{0^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}}$\n  - $p(x_1=0 \\mid z_1=2) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(0-3)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{9}{2}\\right)$\n- 对于 $x_2=3$：\n  - $p(x_2=3 \\mid z_2=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{3^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{9}{2}\\right)$\n  - $p(x_2=3 \\mid z_2=2) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(3-3)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}}$\n- 对于 $x_3=3$：\n  - $p(x_3=3 \\mid z_3=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{3^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{9}{2}\\right)$\n  - $p(x_3=3 \\mid z_3=2) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(3-3)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}}$\n\n在求和的每一项中都会出现一个公因子 $(\\frac{1}{\\sqrt{2\\pi}})^3 = (2\\pi)^{-3/2}$。\n\n我们现在为 $8$ 种可能的潜在序列 $z_{1:3}$ 中的每一种计算联合概率 $p(x_{1:3}, z_{1:3})$：\n\n1.  $z_{1:3} = (1, 1, 1)$：\n    $p_{111} = \\pi_1 A_{11} A_{11} \\cdot p(x_1|1) p(x_2|1) p(x_3|1) = (0.5)(0.9)(0.9) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.405) \\exp(-9)$\n2.  $z_{1:3} = (1, 1, 2)$：\n    $p_{112} = \\pi_1 A_{11} A_{12} \\cdot p(x_1|1) p(x_2|1) p(x_3|2) = (0.5)(0.9)(0.1) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.045) \\exp(-\\frac{9}{2})$\n3.  $z_{1:3} = (1, 2, 1)$：\n    $p_{121} = \\pi_1 A_{12} A_{21} \\cdot p(x_1|1) p(x_2|2) p(x_3|1) = (0.5)(0.1)(0.2) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.01) \\exp(-\\frac{9}{2})$\n4.  $z_{1:3} = (1, 2, 2)$：\n    $p_{122} = \\pi_1 A_{12} A_{22} \\cdot p(x_1|1) p(x_2|2) p(x_3|2) = (0.5)(0.1)(0.8) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.04)$\n5.  $z_{1:3} = (2, 1, 1)$：\n    $p_{211} = \\pi_2 A_{21} A_{11} \\cdot p(x_1|2) p(x_2|1) p(x_3|1) = (0.5)(0.2)(0.9) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.09) \\exp(-\\frac{27}{2})$\n6.  $z_{1:3} = (2, 1, 2)$：\n    $p_{212} = \\pi_2 A_{21} A_{12} \\cdot p(x_1|2) p(x_2|1) p(x_3|2) = (0.5)(0.2)(0.1) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.01) \\exp(-9)$\n7.  $z_{1:3} = (2, 2, 1)$：\n    $p_{221} = \\pi_2 A_{22} A_{21} \\cdot p(x_1|2) p(x_2|2) p(x_3|1) = (0.5)(0.8)(0.2) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) = (2\\pi)^{-3/2} (0.08) \\exp(-9)$\n8.  $z_{1:3} = (2, 2, 2)$：\n    $p_{222} = \\pi_2 A_{22} A_{22} \\cdot p(x_1|2) p(x_2|2) p(x_3|2) = (0.5)(0.8)(0.8) \\cdot \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{9}{2}) \\cdot \\frac{1}{\\sqrt{2\\pi}} \\cdot \\frac{1}{\\sqrt{2\\pi}} = (2\\pi)^{-3/2} (0.32) \\exp(-\\frac{9}{2})$\n\n边际似然 $p(x_{1:3})$ 是这 $8$ 个概率的和：\n$$\np(x_{1:3}) = \\sum_{z_1,z_2,z_3} p_{z_1 z_2 z_3}\n$$\n提出公因子 $(2\\pi)^{-3/2}$：\n$$\np(x_{1:3}) = (2\\pi)^{-3/2} \\left[ (0.405)\\exp(-9) + (0.045)\\exp(-\\frac{9}{2}) + (0.01)\\exp(-\\frac{9}{2}) + 0.04 + (0.09)\\exp(-\\frac{27}{2}) + (0.01)\\exp(-9) + (0.08)\\exp(-9) + (0.32)\\exp(-\\frac{9}{2}) \\right]\n$$\n按指数因子对各项进行分组：\n$$\np(x_{1:3}) = (2\\pi)^{-3/2} \\left[ 0.04 + (0.045+0.01+0.32)\\exp(-\\frac{9}{2}) + (0.405+0.01+0.08)\\exp(-9) + (0.09)\\exp(-\\frac{27}{2}) \\right]\n$$\n$$\np(x_{1:3}) = (2\\pi)^{-3/2} \\left[ 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right]\n$$\n最后，我们通过取 $p(x_{1:3})$ 的自然对数来计算边际对数似然：\n$$\n\\ln p(x_{1:3}) = \\ln\\left( (2\\pi)^{-3/2} \\left[ 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right] \\right)\n$$\n使用属性 $\\ln(ab) = \\ln(a) + \\ln(b)$：\n$$\n\\ln p(x_{1:3}) = \\ln((2\\pi)^{-3/2}) + \\ln\\left( 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right)\n$$\n$$\n\\ln p(x_{1:3}) = -\\frac{3}{2}\\ln(2\\pi) + \\ln\\left( 0.04 + 0.375\\exp(-\\frac{9}{2}) + 0.495\\exp(-9) + 0.09\\exp(-\\frac{27}{2}) \\right)\n$$\n这就是边际对数似然的最终闭合形式解析表达式。",
            "answer": "$$\n\\boxed{-\\frac{3}{2}\\ln(2\\pi) + \\ln\\left(0.04 + 0.375\\exp\\left(-\\frac{9}{2}\\right) + 0.495\\exp(-9) + 0.09\\exp\\left(-\\frac{27}{2}\\right)\\right)}\n$$"
        },
        {
            "introduction": "在了解了暴力计算的局限性后，我们转向解决真实世界问题所需的高效推理算法。这个练习将指导您实现前向-后向算法，这是HMM工具箱中的基石，用于高效计算给定整个观测序列下任意时刻的隐藏状态后验分布。掌握这种“平滑”推断能力对于从嘈杂的神经数据中准确识别潜在状态至关重要，而本练习中对数值稳定性的强调也为您处理真实长序列数据做好了准备 。",
            "id": "3987985",
            "problem": "给定几个为离散时间潜在神经状态识别而定制的小型隐马尔可夫模型 (HMM)。每个 HMM 都有一个有限的潜在状态集 $\\{1,\\dots,K\\}$，一个初始状态分布 $\\pi$，一个状态转移矩阵 $A$，以及一个分类发射模型，其概率排列成一个矩阵 $B$，其中 $B_{k,m} = p(x_t = m \\mid z_t = k)$。您将实现前向-后向算法，以在给定整个观测序列 $x_{1:4}$ 的情况下，计算在时间索引 $t=2$（即 $z_2$）的潜在状态上的后验分布。您的程序应为每个测试用例输出后验分布 $p(z_2 = k \\mid x_{1:4})$，其中状态索引 $k$ 的顺序为 $k=1,\\dots,K$。\n\n仅使用以下基本原理：\n- HMM 的联合分布可分解为 $p(z_{1:T}, x_{1:T}) = p(z_1)\\prod_{t=2}^{T} p(z_t\\mid z_{t-1}) \\prod_{t=1}^{T} p(x_t\\mid z_t)$。\n- 马尔可夫性质：$p(z_t \\mid z_{1:t-1}) = p(z_t \\mid z_{t-1})$ 和观测的条件独立性：$p(x_t \\mid z_{1:T}, x_{1:t-1}) = p(x_t \\mid z_t)$。\n- 用于在链上进行精确推断的贝叶斯法则和和-积原理。\n\n您必须通过采用显式消息缩放或对数域等效方法，来设计您的计算，使其对于长序列和小概率是数值稳定的。对于此问题，您必须实现带按时缩放常数的缩放前向-后向算法，以防止数值下溢。不要依赖外部黑盒 HMM 工具包。\n\n定义和约定：\n- 时间索引为 $t=1,2,3,4$。\n- 每个测试用例的状态空间大小为 $K$，观测字母表大小为 $M$（二者均在下面给出）。\n- 初始分布 $\\pi$ 是一个长度为 $K$ 的向量，其条目为 $\\pi_k = p(z_1 = k)$，顺序为 $k=1,\\dots,K$。\n- 转移矩阵 $A$ 是一个 $K \\times K$ 矩阵，其条目为 $A_{i,j} = p(z_t=j \\mid z_{t-1}=i)$。\n- 发射矩阵 $B$ 是一个 $K \\times M$ 矩阵，其条目为 $B_{k,m} = p(x_t = m \\mid z_t = k)$，其中观测符号是 $\\{0,1,\\dots,M-1\\}$ 中的整数。\n- 观测序列为 $x_{1:4} = (x_1,x_2,x_3,x_4)$，每个测试用例都明确给出。\n\n您对每个测试用例的任务：\n- 使用从 HMM 分解和和-积原理派生的缩放前向-后向消息，计算后验向量 $\\gamma_2$，其分量为 $\\gamma_2(k) = p(z_2 = k \\mid x_{1:4})$，其中 $k=1,\\dots,K$。确保最终向量被归一化，使得在数值精度范围内 $\\sum_{k=1}^{K} \\gamma_2(k) = 1$。\n\n测试套件：\n- 以下所有数字都是概率，必须完全按照给定的数值使用。每个测试都是独立的，必须按规定运行。\n\n测试用例 $1$：\n- $K=2$, $M=3$。\n- $\\pi = [0.6, 0.4]$。\n- $A = \\begin{pmatrix} 0.7  0.3 \\\\ 0.2  0.8 \\end{pmatrix}$。\n- $B = \\begin{pmatrix} 0.5  0.4  0.1 \\\\ 0.1  0.3  0.6 \\end{pmatrix}$。\n- $x_{1:4} = (2,\\,1,\\,0,\\,1)$。\n\n测试用例 $2$：\n- $K=3$, $M=3$。\n- $\\pi = [0.2, 0.5, 0.3]$。\n- $A = \\begin{pmatrix} 0.90  0.09  0.01 \\\\ 0.05  0.90  0.05 \\\\ 0.02  0.08  0.90 \\end{pmatrix}$。\n- $B = \\begin{pmatrix} 0.6  0.3  0.1 \\\\ 0.2  0.5  0.3 \\\\ 0.1  0.2  0.7 \\end{pmatrix}$。\n- $x_{1:4} = (0,\\,2,\\,1,\\,2)$。\n\n测试用例 $3$（通过使用非常小的概率来测试下溢处理）：\n- $K=2$, $M=2$。\n- $\\pi = [0.001, 0.999]$。\n- $A = \\begin{pmatrix} 0.0001  0.9999 \\\\ 0.00001  0.99999 \\end{pmatrix}$。\n- $B = \\begin{pmatrix} 0.001  0.999 \\\\ 0.999999  0.000001 \\end{pmatrix}$。\n- $x_{1:4} = (1,\\,1,\\,0,\\,1)$。\n\n测试用例 $4$：\n- $K=3$, $M=4$。\n- $\\pi = [\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}]$。\n- $A = \\begin{pmatrix} 0.6  0.3  0.1 \\\\ 0.2  0.5  0.3 \\\\ 0.25  0.25  0.5 \\end{pmatrix}$。\n- $B = \\begin{pmatrix} 0.25  0.25  0.25  0.25 \\\\ 0.1  0.2  0.3  0.4 \\\\ 0.4  0.3  0.2  0.1 \\end{pmatrix}$。\n- $x_{1:4} = (3,\\,2,\\,1,\\,0)$。\n\n输出规格：\n- 对于每个测试用例，按状态顺序 $k=1,\\dots,K$ 输出后验向量 $\\left[\\,\\gamma_2(1),\\,\\gamma_2(2),\\,\\dots,\\,\\gamma_2(K)\\,\\right]$。\n- 将每个分量四舍五入到恰好 $6$ 位小数。\n- 将所有测试用例的输出聚合到单行中，作为一个列表的列表，不含多余的空格，例如：$\\texttt{[[a\\_1,\\dots,a\\_K],[b\\_1,\\dots,b\\_K],\\dots]}$。\n- 您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表，与上述格式完全一致。\n\n不涉及物理单位或角度。所有答案必须是按规定四舍五入的纯数字。通过遵守概率约束、正确归一化和通过缩放实现数值稳定性来确保科学真实性。最终程序必须完全自包含，不需要任何输入。",
            "solution": "用户希望在给定观测序列的情况下，计算隐马尔可夫模型 (HMM) 中特定时间步的潜在状态的后验分布。该问题定义明确，要求实现前向-后向算法，特别是需要进行缩放以确保数值稳定性。\n\n### 第一步：提取已知条件\n该问题为几个测试用例提供了以下数据：\n- 一个潜在状态集 $\\{1, \\dots, K\\}$。\n- 一个长度为 $K$ 的初始状态分布向量 $\\pi$，其中 $\\pi_k = p(z_1 = k)$。\n- 一个 $K \\times K$ 的状态转移矩阵 $A$，其中 $A_{i,j} = p(z_t=j \\mid z_{t-1}=i)$。\n- 一个 $K \\times M$ 的发射矩阵 $B$，其中 $B_{k,m} = p(x_t = m \\mid z_t = k)$。\n- 一个长度为 $T=4$ 的观测序列 $x_{1:4} = (x_1, x_2, x_3, x_4)$。\n- 具体任务是为每个状态 $k=1,\\dots,K$ 计算后验分布 $\\gamma_2(k) = p(z_2=k \\mid x_{1:4})$。\n- 时间索引指定为 $t=1,2,3,4$。观测符号是整数 $\\{0, \\dots, M-1\\}$。\n- 提供了四个测试用例，包含 $K, M, \\pi, A, B,$ 和 $x_{1:4}$ 的具体参数。\n\n### 第二步：使用提取的已知条件进行验证\n- **科学依据**：该问题基于隐马尔可夫模型理论，这是各 STEM 领域中统计建模的基石。分解式 $p(z_{1:T}, x_{1:T}) = p(z_1)\\prod_{t=2}^{T} p(z_t\\mid z_{t-1}) \\prod_{t=1}^{T} p(x_t\\mid z_t)$ 是 HMM 联合分布的标准定义。计算平滑后验 $p(z_t \\mid x_{1:T})$ 的任务是一个基本的推断问题，可以通过前向-后向算法解决。所有测试用例中提供的 $\\pi, A,$ 和 $B$ 中的概率参数都是有效的（非负且行和为 $1$）。该问题具有科学合理性。\n- **适定性**：该问题是适定的。给定一个完全指定的 HMM 和一个观测序列，前向-后向算法提供了一种唯一且稳定的方法来计算所需的后验概率。\n- **客观性**：该问题使用标准符号进行数学上的精确陈述，并提供了确切的数值。没有主观或模糊的元素。\n- **完整性**：为每个测试用例提供了所有必要的信息（$\\pi, A, B, x_{1:4}$）。\n- **数值稳定性**：该问题正确地指出了长序列或小概率可能导致的数值下溢问题，并明确要求实现缩放的前向-后向算法，这是应对这一挑战的标准且正确的方法。\n\n### 第三步：结论与行动\n该问题是有效的。它是一个定义明确、具有科学依据的计算统计学任务。我现在将着手推导和实现解决方案。\n\n### 算法解决方案：缩放的前向-后向算法\n\n目标是计算后验状态分布（平滑分布）$\\gamma_t(k) = p(z_t=k \\mid x_{1:T})$。这可以用前向和后向消息表示：\n$$\n\\gamma_t(k) = \\frac{p(z_t=k, x_{1:T})}{p(x_{1:T})} = \\frac{\\alpha_t(k) \\beta_t(k)}{\\sum_{j=1}^K \\alpha_t(j) \\beta_t(j)}\n$$\n其中 $\\alpha_t(k) = p(z_t=k, x_{1:t})$ 是前向消息，$\\beta_t(k) = p(x_{t+1:T} \\mid z_t=k)$ 是后向消息。直接计算这些消息可能导致数值下溢。我们将使用该算法的缩放版本。\n\n**1. 缩放前向传递**\n\n我们定义缩放的前向消息 $\\hat{\\alpha}_t(k) = p(z_t=k \\mid x_{1:t})$ 和缩放常数 $c_t = p(x_t \\mid x_{1:t-1})$。\n\n- **初始化 ($t=1$):**\n  未缩放的消息为 $\\alpha_1(k) = p(z_1=k, x_1) = \\pi_k B_{k, x_1}$。\n  第一个缩放常数为 $c_1 = p(x_1) = \\sum_{j=1}^K \\alpha_1(j)$。\n  缩放后的消息为 $\\hat{\\alpha}_1(k) = \\frac{\\alpha_1(k)}{c_1}$。\n\n- **递归 ($t=2, \\dots, T$):**\n  第 $t$ 步的未缩放消息由第 $t-1$ 步的缩放消息计算得出：\n  $$\n  \\alpha'_t(j) = \\left( \\sum_{i=1}^K \\hat{\\alpha}_{t-1}(i) A_{i,j} \\right) B_{j, x_t}\n  $$\n  第 $t$ 步的缩放常数为 $c_t = \\sum_{j=1}^K \\alpha'_t(j)$。\n  缩放后的消息为 $\\hat{\\alpha}_t(j) = \\frac{\\alpha'_t(j)}{c_t}$。\n\n我们将为所有 $t=1, \\dots, 4$ 计算并存储向量 $\\hat{\\alpha}_t$ 和标量 $c_t$。对于这个问题，我们只需要 $\\hat{\\alpha}_2$。\n\n**2. 缩放后向传递**\n\n我们定义缩放的后向消息 $\\hat{\\beta}_t(k)$，它们使用与前向传递相同的常数 $c_t$ 进行缩放。\n\n- **初始化 ($t=T=4$):**\n  未缩放的消息 $\\beta_T(k) = 1$ 对所有 $k$ 成立。我们定义缩放的消息 $\\hat{\\beta}_T(k) = 1$。\n\n- **递归 ($t=T-1, \\dots, 1$):**\n  缩放后向消息的递归推导如下：\n  $$\n  \\hat{\\beta}_t(i) = \\frac{1}{c_{t+1}} \\sum_{j=1}^K A_{i,j} B_{j, x_{t+1}} \\hat{\\beta}_{t+1}(j)\n  $$\n  我们需要为 $t=3, 2, 1$ 计算这个值，以获得 $\\hat{\\beta}_2$。\n\n**3. 后验计算**\n\n未归一化的后验由相应缩放的前向和后向消息的乘积给出：\n$$\n\\gamma_t(k) \\propto \\hat{\\alpha}_t(k) \\hat{\\beta}_t(k)\n$$\n然后将该乘积归一化以确保其和为 $1$。对于这个问题，我们需要为 $k=1, \\dots, K$ 计算 $\\gamma_2(k)$：\n$$\n\\gamma_2(k) = \\frac{\\hat{\\alpha}_2(k) \\hat{\\beta}_2(k)}{\\sum_{j=1}^K \\hat{\\alpha}_2(j) \\hat{\\beta}_2(j)}\n$$\n\n对每个测试用例都遵循此过程，以获得所需的 $z_2$ 的后验分布。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the HMM problem for all test cases.\n    It computes the posterior distribution p(z_2 | x_{1:4}) using the\n    scaled forward-backward algorithm and formats the output as specified.\n    \"\"\"\n\n    def calculate_posterior_z2(pi, A, B, x):\n        \"\"\"\n        Implements the scaled forward-backward algorithm to compute gamma_2.\n        \n        Args:\n            pi (np.array): Initial state distribution (K,).\n            A (np.array): State transition matrix (K, K).\n            B (np.array): Emission matrix (K, M).\n            x (list): Observation sequence of length T.\n\n        Returns:\n            np.array: The posterior distribution p(z_2 | x_{1:T}) of shape (K,).\n        \"\"\"\n        T = len(x)\n        K = A.shape[0]\n\n        # --- Scaled Forward Pass ---\n        # We only need alpha_hat_2 for the final result, so we only need to go up to t=2.\n        # But the backward pass needs all scaling constants c, so we do the full forward pass.\n        alpha_hats = np.zeros((T, K))\n        c = np.zeros(T)\n\n        # Time t=1 (index 0)\n        # pi has shape (K,), B[:, x[0]] has shape (K,)\n        alpha_1_prime = pi * B[:, x[0]]\n        c[0] = np.sum(alpha_1_prime)\n        alpha_hats[0, :] = alpha_1_prime / c[0]\n\n        # Time t=2...T\n        for t in range(1, T):\n            # alpha_hats[t-1,:] is (K,), A is (K, K). Result of @ is (K,)\n            # B[:, x[t]] is (K,).\n            alpha_t_prime = (alpha_hats[t-1, :] @ A) * B[:, x[t]]\n            c[t] = np.sum(alpha_t_prime)\n            alpha_hats[t, :] = alpha_t_prime / c[t]\n\n        # --- Scaled Backward Pass ---\n        # We need to compute up to beta_hat_2.\n        beta_hats = np.zeros((T, K))\n\n        # Time t=T (index T-1)\n        beta_hats[T - 1, :] = 1.0\n\n        # Time t=T-1...1\n        # Loop from t=T-2 down to 0, which corresponds to time T-1 down to 1\n        for t in range(T - 2, -1, -1):\n            # A is (K,K), B[:, x[t+1]] is (K,), beta_hats[t+1,:] is (K,)\n            # The term to be left-multiplied by A is a vector of shape (K,)\n            term_to_sum = B[:, x[t+1]] * beta_hats[t+1, :]\n            # The result of A @ term_to_sum is a vector of shape (K,)\n            beta_hats[t, :] = (A @ term_to_sum) / c[t + 1]\n\n        # --- Posterior for t=2 (index 1) ---\n        # gamma_2 is proportional to the element-wise product of alpha_hat_2 and beta_hat_2.\n        gamma_2_unnormalized = alpha_hats[1, :] * beta_hats[1, :]\n        \n        # Normalize to get the final posterior distribution.\n        gamma_2 = gamma_2_unnormalized / np.sum(gamma_2_unnormalized)\n\n        return gamma_2\n\n    test_cases = [\n        (\n            np.array([0.6, 0.4]),\n            np.array([[0.7, 0.3], [0.2, 0.8]]),\n            np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]),\n            [2, 1, 0, 1]\n        ),\n        (\n            np.array([0.2, 0.5, 0.3]),\n            np.array([[0.90, 0.09, 0.01], [0.05, 0.90, 0.05], [0.02, 0.08, 0.90]]),\n            np.array([[0.6, 0.3, 0.1], [0.2, 0.5, 0.3], [0.1, 0.2, 0.7]]),\n            [0, 2, 1, 2]\n        ),\n        (\n            np.array([0.001, 0.999]),\n            np.array([[0.0001, 0.9999], [0.00001, 0.99999]]),\n            np.array([[0.001, 0.999], [0.999999, 0.000001]]),\n            [1, 1, 0, 1]\n        ),\n        (\n            np.array([1/3, 1/3, 1/3]),\n            np.array([[0.6, 0.3, 0.1], [0.2, 0.5, 0.3], [0.25, 0.25, 0.5]]),\n            np.array([[0.25, 0.25, 0.25, 0.25], [0.1, 0.2, 0.3, 0.4], [0.4, 0.3, 0.2, 0.1]]),\n            [3, 2, 1, 0]\n        )\n    ]\n\n    all_results_str = []\n    for pi, A, B, x in test_cases:\n        gamma_2 = calculate_posterior_z2(pi, A, B, x)\n        # Format each list of results to 6 decimal places without extra spaces\n        inner_list_str = ','.join(f'{v:.6f}' for v in gamma_2)\n        all_results_str.append(f'[{inner_list_str}]')\n    \n    # Combine all formatted results into a single string representing a list of lists\n    final_output = f\"[[{','.join(all_results_str)}]]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "前面的练习假定模型参数是已知的，但在实际的神经科学应用中，我们的目标往往是从数据中学习这些参数。这个综合性练习将引导您实现期望最大化（EM）算法（也称为Baum-Welch算法）的一次完整迭代，这是训练HMM的标准方法。通过实现E-步（利用前向-后向算法）和M-步（更新参数），您将掌握如何从原始神经活动记录中自动发现隐藏的神经状态及其动态特性 。",
            "id": "3987936",
            "problem": "您的任务是为一个具有泊松发射的双状态隐马尔可夫模型（HMM）实现单次期望最大化（EM）迭代，以从尖峰计数数据中识别潜在的神经状态。核心目标是通过前向-后向算法计算后验状态边缘概率 $\\gamma_t(k)$ 和成对后验概率 $\\xi_t(i,j)$，更新转移矩阵 $A$ 和泊松率 $\\lambda_k$，并验证观测数据对数似然 $\\log p(x_{1:T} \\mid \\theta)$ 在 EM 更新后严格增加。\n\n从基本概率定义开始。一个隐马尔可夫模型（HMM）包括：\n- 一个关于 $K$ 个离散状态的初始状态分布 $\\pi$。\n- 一个状态转移矩阵 $A$，其条目为 $A_{ij} = p(z_{t+1}=j \\mid z_t=i)$，其中行和为 $1$。\n- 一个发射模型 $p(x_t \\mid z_t)$，用于描述在给定潜在状态 $z_t$ 的条件下观测数据 $x_t$ 的概率。\n\n对于本问题，发射模型是泊松分布：对于尖峰计数 $x_t \\in \\{0,1,2,\\dots\\}$ 和状态 $k \\in \\{1,2\\}$，似然函数为\n$$\np(x_t \\mid z_t = k, \\lambda_k) = \\frac{\\lambda_k^{x_t} e^{-\\lambda_k}}{x_t!}.\n$$\n令 $x_{1:T} \\equiv (x_1, x_2, \\dots, x_T)$ 和 $z_{1:T} \\equiv (z_1, z_2, \\dots, z_T)$ 分别表示观测到的计数和潜在状态。在参数 $\\theta \\equiv (\\pi, A, \\lambda)$ 下的联合分布为\n$$\np(x_{1:T}, z_{1:T} \\mid \\theta) = \\pi_{z_1} \\left( \\prod_{t=1}^{T-1} A_{z_t, z_{t+1}} \\right) \\left( \\prod_{t=1}^{T} p(x_t \\mid z_t, \\lambda_{z_t}) \\right).\n$$\n\n需要实现和验证的任务：\n1. 对于每个测试用例，使用给定的真实参数 $(\\pi^{\\text{true}}, A^{\\text{true}}, \\lambda^{\\text{true}})$ 从一个双状态 HMM 中生成一个尖峰计数的合成序列 $x_{1:T}$，并使用指定的随机种子以确保可复现性。使用上述联合分布所隐含的生成过程（采样 $z_1 \\sim \\pi^{\\text{true}}$，然后递归地采样 $z_{t+1} \\sim A^{\\text{true}}_{z_t, \\cdot}$，并为每个 $t$ 采样 $x_t \\sim \\text{Poisson}(\\lambda^{\\text{true}}_{z_t})$）。\n2. 给定初始参数 $\\theta^{(0)} \\equiv (\\pi^{(0)}, A^{(0)}, \\lambda^{(0)})$，从基本原理推导前向-后向算法，以计算后验状态边缘概率 $\\gamma_t(k) \\equiv p(z_t=k \\mid x_{1:T}, \\theta^{(0)})$ 和成对后验概率 $\\xi_t(i,j) \\equiv p(z_t=i, z_{t+1}=j \\mid x_{1:T}, \\theta^{(0)})$，其中 $t \\in \\{1,\\dots,T-1\\}$。\n3. 从 $\\theta^{(0)}$ 下的期望完全数据对数似然出发，使用从 $\\gamma_t(k)$ 和 $\\xi_t(i,j)$ 获得的充分统计量，推导转移矩阵 $A$ 和泊松率 $\\lambda_k$ 的最大化（M步）更新规则，同时保持初始分布 $\\pi$ 固定为 $\\pi^{(0)}$。\n4. 使用数值稳定的缩放前向算法，计算 EM 更新前后的观测数据对数似然 $\\log p(x_{1:T} \\mid \\theta)$。使用自然对数（以 $e$ 为底）。\n5. 对于每个测试用例，验证对数似然在 EM 更新后是否严格增加，即检查是否有 $\\log p(x_{1:T} \\mid \\theta^{(1)})  \\log p(x_{1:T} \\mid \\theta^{(0)})$，其中 $\\theta^{(1)}$ 是更新后的参数。\n\n测试套件规范：\n- 所有测试用例均使用 $K = 2$ 个状态。以下四组参数涵盖了不同的情况：\n  - 情况1（一般混合，中等速率，长度 $T=50$）：\n    - 真实参数：$\\pi^{\\text{true}} = [0.6, 0.4]$, \n      $A^{\\text{true}} = \\begin{bmatrix} 0.95  0.05 \\\\ 0.05  0.95 \\end{bmatrix}$, \n      $\\lambda^{\\text{true}} = [3.0, 10.0]$, \n      随机种子 $1234$。\n    - 初始参数：$\\pi^{(0)} = [0.5, 0.5]$, \n      $A^{(0)} = \\begin{bmatrix} 0.8  0.2 \\\\ 0.2  0.8 \\end{bmatrix}$, \n      $\\lambda^{(0)} = [5.0, 5.0]$。\n  - 情况2（强速率对比，长度 $T=60$）：\n    - 真实参数：$\\pi^{\\text{true}} = [0.5, 0.5]$, \n      $A^{\\text{true}} = \\begin{bmatrix} 0.9  0.1 \\\\ 0.1  0.9 \\end{bmatrix}$, \n      $\\lambda^{\\text{true}} = [0.5, 20.0]$, \n      随机种子 $5678$。\n    - 初始参数：$\\pi^{(0)} = [0.5, 0.5]$, \n      $A^{(0)} = \\begin{bmatrix} 0.7  0.3 \\\\ 0.3  0.7 \\end{bmatrix}$, \n      $\\lambda^{(0)} = [1.0, 15.0]$。\n  - 情况3（近确定性转移，长度 $T=40$）：\n    - 真实参数：$\\pi^{\\text{true}} = [0.7, 0.3]$, \n      $A^{\\text{true}} = \\begin{bmatrix} 0.99  0.01 \\\\ 0.01  0.99 \\end{bmatrix}$, \n      $\\lambda^{\\text{true}} = [2.0, 12.0]$, \n      随机种子 $1357$。\n    - 初始参数：$\\pi^{(0)} = [0.5, 0.5]$, \n      $A^{(0)} = \\begin{bmatrix} 0.85  0.15 \\\\ 0.15  0.85 \\end{bmatrix}$, \n      $\\lambda^{(0)} = [4.0, 9.0]$。\n  - 情况4（短序列边缘情况，长度 $T=5$）：\n    - 真实参数：$\\pi^{\\text{true}} = [0.5, 0.5]$, \n      $A^{\\text{true}} = \\begin{bmatrix} 0.9  0.1 \\\\ 0.1  0.9 \\end{bmatrix}$, \n      $\\lambda^{\\text{true}} = [1.0, 8.0]$, \n      随机种子 $2468$。\n    - 初始参数：$\\pi^{(0)} = [0.5, 0.5]$, \n      $A^{(0)} = \\begin{bmatrix} 0.6  0.4 \\\\ 0.4  0.6 \\end{bmatrix}$, \n      $\\lambda^{(0)} = [2.0, 6.0]$。\n\n答案规范：\n- 对于每个测试用例，计算一个布尔值，指示对数似然在 EM 更新后是否严格增加。\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$\\texttt{[result1,result2,result3,result4]}$）。每个条目必须是一个布尔值 $\\texttt{True}$ 或 $\\texttt{False}$，并按测试用例的顺序（1到4）排列。\n- 本问题不涉及物理单位或角度单位。如果在实现中需要任何分数值，请使用浮点数。所有对数必须是自然对数。",
            "solution": "该问题要求为一个具有泊松发射的双状态隐马尔可夫模型（HMM）实现并验证单次期望最大化（EM）迭代。该过程涉及生成合成数据，通过前向-后向算法执行 E 步，执行 M 步以更新参数，并验证观测数据对数似然的单调增加。\n\n设 HMM 由参数 $\\theta = (\\pi, A, \\lambda)$ 定义，其中 $\\pi$ 是初始状态分布， $A$ 是状态转移矩阵，$\\lambda$ 是泊松发射率的向量。该模型有 $K=2$ 个潜在状态 $z_t \\in \\{1, 2\\}$，并产生观测到的尖峰计数 $x_t \\in \\{0, 1, 2, \\dots\\}$。\n\n首先，对于每个测试用例，根据真实参数 $(\\pi^{\\text{true}}, A^{\\text{true}}, \\lambda^{\\text{true}})$ 和指定的随机种子，生成一个长度为 $T$ 的观测序列 $x_{1:T} = (x_1, \\dots, x_T)$。这涉及到采样一个初始状态 $z_1 \\sim \\text{Categorical}(\\pi^{\\text{true}})$，然后递归地采样状态 $z_{t+1} \\sim \\text{Categorical}(A_{z_t, \\cdot}^{\\text{true}})$ 和观测值 $x_t \\sim \\text{Poisson}(\\lambda_{z_t}^{\\text{true}})$。\n\n任务的核心是使用一次 EM 迭代将一组初始参数 $\\theta^{(0)} = (\\pi^{(0)}, A^{(0)}, \\lambda^{(0)})$ 更新为新参数 $\\theta^{(1)}$。EM 算法通过迭代地最大化期望完全数据对数似然来最大化观测数据的对数似然 $\\log p(x_{1:T} \\mid \\theta)$。完全数据对数似然由下式给出：\n$$\n\\log p(x_{1:T}, z_{1:T} \\mid \\theta) = \\log \\pi_{z_1} + \\sum_{t=1}^{T-1} \\log A_{z_t, z_{t+1}} + \\sum_{t=1}^{T} \\log p(x_t \\mid z_t, \\lambda_{z_t})\n$$\nEM 算法在两个步骤之间交替进行：\n\n**1. 期望（E）步**\n在 E 步中，我们计算关于在给定观测数据和当前参数 $\\theta^{(0)}$ 下的潜在状态后验分布的期望完全数据对数似然。这就是 $Q$ 函数：\n$$\nQ(\\theta \\mid \\theta^{(0)}) = E_{z_{1:T} \\mid x_{1:T}, \\theta^{(0)}}[\\log p(x_{1:T}, z_{1:T} \\mid \\theta)]\n$$\n评估这个期望需要计算后验状态边缘概率 $\\gamma_t(k) = p(z_t=k \\mid x_{1:T}, \\theta^{(0)})$ 和后验成对边缘概率 $\\xi_t(i,j) = p(z_t=i, z_{t+1}=j \\mid x_{1:T}, \\theta^{(0)})$。这些是通过前向-后向算法找到的。为了防止数值下溢，使用了该算法的缩放版本。\n\n**前向算法（缩放版）**\n前向消息 $\\alpha_t(k) = p(x_{1:t}, z_t=k \\mid \\theta^{(0)})$ 是递归计算的。为了保持数值稳定性，我们定义缩放后的消息 $\\hat{\\alpha}_t(k) = p(z_t=k \\mid x_{1:t}, \\theta^{(0)})$。\n在 $t=1$ 时的初始化是：\n$$\n\\alpha_1(k) = \\pi^{(0)}_k p(x_1 \\mid z_1=k, \\lambda^{(0)}_k)\n$$\n缩放因子是 $c_1 = \\sum_{k=1}^K \\alpha_1(k)$，缩放后的消息是 $\\hat{\\alpha}_1(k) = \\alpha_1(k) / c_1$。\n对于 $t=2, \\dots, T$，递归是：\n$$\n\\alpha_t(k) = p(x_t \\mid z_t=k, \\lambda^{(0)}_k) \\sum_{j=1}^K \\hat{\\alpha}_{t-1}(j) A^{(0)}_{jk}\n$$\n缩放因子是 $c_t = \\sum_{k=1}^K \\alpha_t(k)$，缩放后的消息是 $\\hat{\\alpha}_t(k) = \\alpha_t(k) / c_t$。\n观测数据对数似然是这种缩放的直接产物：\n$$\n\\log p(x_{1:T} \\mid \\theta^{(0)}) = \\sum_{t=1}^T \\log c_t\n$$\n\n**后向算法（缩放版）**\n后向消息 $\\beta_t(k) = p(x_{t+1:T} \\mid z_t=k, \\theta^{(0)})$ 也是递归计算的。我们定义与前向缩放兼容的缩放后消息 $\\hat{\\beta}_t(k)$。\n在 $t=T$ 时的初始化是 $\\hat{\\beta}_T(k) = 1$，对于所有 $k \\in \\{1, \\dots, K\\}$。\n对于 $t=T-1, \\dots, 1$ 的递归是：\n$$\n\\hat{\\beta}_t(i) = \\frac{1}{c_{t+1}} \\sum_{j=1}^K A^{(0)}_{ij} p(x_{t+1} \\mid z_{t+1}=j, \\lambda^{(0)}_j) \\hat{\\beta}_{t+1}(j)\n$$\n\n**后验概率**\n使用缩放后的前向和后向消息，所需的后验概率计算如下：\n状态边缘后验概率 $\\gamma_t(k)$：\n$$\n\\gamma_t(k) = p(z_t=k \\mid x_{1:T}, \\theta^{(0)}) = \\frac{\\alpha_t(k)\\beta_t(k)}{p(x_{1:T}|\\theta^{(0)})} = \\hat{\\alpha}_t(k) \\hat{\\beta}_t(k)\n$$\n成对边缘后验概率 $\\xi_t(i,j)$：\n$$\n\\xi_t(i,j) = p(z_t=i, z_{t+1}=j \\mid x_{1:T}, \\theta^{(0)}) = \\frac{\\alpha_t(i) A^{(0)}_{ij} p(x_{t+1} \\mid z_{t+1}=j, \\lambda_j^{(0)}) \\beta_{t+1}(j)}{p(x_{1:T}|\\theta^{(0)})}\n$$\n$$\n= \\frac{\\hat{\\alpha}_t(i) A^{(0)}_{ij} p(x_{t+1} \\mid z_{t+1}=j, \\lambda_j^{(0)}) \\hat{\\beta}_{t+1}(j)}{c_{t+1}}\n$$\n\n**2. 最大化（M）步**\n在 M 步中，我们找到最大化 $Q$ 函数 $Q(\\theta \\mid \\theta^{(0)})$ 的参数 $\\theta^{(1)}$。初始分布 $\\pi$ 保持固定为 $\\pi^{(0)}$。\n\n**转移矩阵更新**\n$Q(\\theta \\mid \\theta^{(0)})$ 中依赖于 $A$ 的部分是 $\\sum_{t=1}^{T-1} \\sum_{i=1}^K \\sum_{j=1}^K \\xi_t(i,j) \\log A_{ij}$。在约束条件 $\\sum_j A_{ij} = 1$（对于每一行 $i$）下最大化此式，得到更新规则：\n$$\nA^{(1)}_{ij} = \\frac{\\text{从 } i \\text{ 到 } j \\text{ 的期望转移次数}}{\\text{从 } i \\text{ 出发的期望转移次数}} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}\n$$\n\n**发射率更新**\n$Q(\\theta \\mid \\theta^{(0)})$ 中依赖于 $\\lambda$ 的部分是 $\\sum_{t=1}^T \\sum_{k=1}^K \\gamma_t(k) \\log p(x_t \\mid z_t=k, \\lambda_k)$。对于泊松发射模型，$\\log p(x_t \\mid z_t=k, \\lambda_k) = x_t \\log \\lambda_k - \\lambda_k - \\log(x_t!)$。关于 $\\lambda_k$ 进行最大化，得到：\n$$\n\\lambda^{(1)}_k = \\frac{\\text{状态 } k \\text{ 中的期望总计数}}{\\text{状态 } k \\text{ 中的期望持续时间}} = \\frac{\\sum_{t=1}^T \\gamma_t(k) x_t}{\\sum_{t=1}^T \\gamma_t(k)}\n$$\n初始分布是固定的，因此 $\\pi^{(1)} = \\pi^{(0)}$。这就定义了新的参数集 $\\theta^{(1)} = (\\pi^{(0)}, A^{(1)}, \\lambda^{(1)})$。\n\n**3. 验证**\n最后，我们通过使用 $\\theta^{(1)}$ 运行缩放前向算法，来计算使用更新后参数的对数似然 $\\log p(x_{1:T} \\mid \\theta^{(1)})$。EM 算法的基本性质保证了 $\\log p(x_{1:T} \\mid \\theta^{(1)}) \\ge \\log p(x_{1:T} \\mid \\theta^{(0)})$。本问题要求验证严格增加，这是预期的，因为初始参数 $\\theta^{(0)}$ 不在似然函数的局部最大值点上。\n\n实现将对每个测试用例执行这些步骤，并报告严格不等式 $\\log p(x_{1:T} \\mid \\theta^{(1)})  \\log p(x_{1:T} \\mid \\theta^{(0)})$ 是否成立。所有对数都是自然对数（以 $e$ 为底）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp, gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run the EM HMM validation for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"T\": 50,\n            \"pi_true\": np.array([0.6, 0.4]),\n            \"A_true\": np.array([[0.95, 0.05], [0.05, 0.95]]),\n            \"lambda_true\": np.array([3.0, 10.0]),\n            \"seed\": 1234,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.8, 0.2], [0.2, 0.8]]),\n            \"lambda0\": np.array([5.0, 5.0])\n        },\n        {\n            \"T\": 60,\n            \"pi_true\": np.array([0.5, 0.5]),\n            \"A_true\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"lambda_true\": np.array([0.5, 20.0]),\n            \"seed\": 5678,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.7, 0.3], [0.3, 0.7]]),\n            \"lambda0\": np.array([1.0, 15.0])\n        },\n        {\n            \"T\": 40,\n            \"pi_true\": np.array([0.7, 0.3]),\n            \"A_true\": np.array([[0.99, 0.01], [0.01, 0.99]]),\n            \"lambda_true\": np.array([2.0, 12.0]),\n            \"seed\": 1357,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.85, 0.15], [0.15, 0.85]]),\n            \"lambda0\": np.array([4.0, 9.0])\n        },\n        {\n            \"T\": 5,\n            \"pi_true\": np.array([0.5, 0.5]),\n            \"A_true\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"lambda_true\": np.array([1.0, 8.0]),\n            \"seed\": 2468,\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.6, 0.4], [0.4, 0.6]]),\n            \"lambda0\": np.array([2.0, 6.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x_obs = generate_data(\n            case[\"T\"], case[\"pi_true\"], case[\"A_true\"], case[\"lambda_true\"], case[\"seed\"]\n        )\n\n        pi0, A0, lambda0 = case[\"pi0\"], case[\"A0\"], case[\"lambda0\"]\n\n        # E-step with initial parameters theta_0\n        gamma, xi, log_L0 = forward_backward(x_obs, pi0, A0, lambda0)\n        \n        # M-step to get updated parameters theta_1\n        pi1, A1, lambda1 = m_step(x_obs, gamma, xi, pi0)\n\n        # Calculate log-likelihood with updated parameters theta_1\n        _, _, log_L1 = forward_backward(x_obs, pi1, A1, lambda1)\n\n        # Verify strict increase in log-likelihood\n        results.append(log_L1 > log_L0)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef generate_data(T, pi_true, A_true, lambda_true, seed):\n    \"\"\"\n    Generates a sequence of spike counts from an HMM with Poisson emissions.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    K = len(pi_true)\n    states = np.arange(K)\n    \n    z_obs = np.zeros(T, dtype=int)\n    x_obs = np.zeros(T, dtype=int)\n\n    # Sample initial state z_1\n    z_obs[0] = rng.choice(states, p=pi_true)\n    x_obs[0] = rng.poisson(lambda_true[z_obs[0]])\n\n    # Sample remaining states and observations\n    for t in range(T - 1):\n        z_obs[t+1] = rng.choice(states, p=A_true[z_obs[t], :])\n        x_obs[t+1] = rng.poisson(lambda_true[z_obs[t+1]])\n    \n    return x_obs\n\n\ndef poisson_log_pmf(x, rate):\n    \"\"\"\n    Computes the log of the Poisson probability mass function.\n    Handles rate=0 case.\n    \"\"\"\n    # Use np.errstate to handle log(0) without warnings\n    with np.errstate(divide='ignore'):\n        log_rate = np.log(rate)\n    log_rate[rate == 0] = -np.inf # log(0) is -inf\n    # If rate is 0, pmf is 1 for x=0 and 0 for x>0\n    log_p = x * log_rate - rate - gammaln(x + 1)\n    if isinstance(x, np.ndarray):\n      log_p[np.logical_and(rate == 0, x != 0)] = -np.inf\n      log_p[np.logical_and(rate == 0, x == 0)] = 0\n    elif rate == 0:\n      log_p = 0. if x == 0 else -np.inf\n    return log_p\n\n\ndef forward_backward(x_obs, pi, A, lmbda):\n    \"\"\"\n    Performs the forward-backward algorithm to compute posteriors and log-likelihood.\n    Uses log-space calculations for numerical stability.\n    \"\"\"\n    T = len(x_obs)\n    K = len(pi)\n\n    # Compute emission log-probabilities\n    log_em_probs = np.zeros((T, K))\n    for k in range(K):\n        # Handle potential lmbda[k] == 0 from M-step\n        if lmbda[k]  0: lmbda[k] = 1e-9 # Should not happen with correct M-step\n        if lmbda[k] == 0:\n             # Manually handle log(0) issues for Poisson\n             log_em_probs[:, k] = np.where(x_obs == 0, 0, -np.inf)\n        else:\n            log_em_probs[:, k] = poisson_log_pmf(x_obs, lmbda[k])\n    \n    # --- Forward pass (scaled alphas in log-space) ---\n    log_alpha = np.zeros((T, K))\n    \n    # Initialization t=0\n    log_alpha[0, :] = np.log(pi) + log_em_probs[0, :]\n\n    # Recursion t=1 to T-1\n    for t in range(1, T):\n        for k in range(K):\n            log_alpha[t, k] = logsumexp(log_alpha[t-1, :] + np.log(A[:, k])) + log_em_probs[t, k]\n\n    log_likelihood = logsumexp(log_alpha[T-1, :])\n\n    # --- Backward pass (in log-space) ---\n    log_beta = np.zeros((T, K))\n    # Initialization t=T-1 is log(1) = 0\n    \n    # Recursion t=T-2 down to 0\n    for t in range(T - 2, -1, -1):\n        for i in range(K):\n            terms = np.log(A[i, :]) + log_em_probs[t+1, :] + log_beta[t+1, :]\n            log_beta[t, i] = logsumexp(terms)\n\n    # --- Compute posteriors ---\n    # Gamma: p(z_t=k | x_{1:T})\n    log_gamma = log_alpha + log_beta - log_likelihood\n    gamma = np.exp(log_gamma)\n    \n    # Xi: p(z_t=i, z_{t+1}=j | x_{1:T})\n    log_xi = np.zeros((T - 1, K, K))\n    for t in range(T - 1):\n        for i in range(K):\n            for j in range(K):\n                log_xi[t, i, j] = log_alpha[t, i] + np.log(A[i, j]) + \\\n                                  log_em_probs[t+1, j] + log_beta[t+1, j] - \\\n                                  log_likelihood\n    xi = np.exp(log_xi)\n\n    return gamma, xi, log_likelihood\n\n\ndef m_step(x_obs, gamma, xi, pi0):\n    \"\"\"\n    Performs the M-step to update the HMM parameters.\n    \"\"\"\n    K = gamma.shape[1]\n\n    # Update transition matrix A\n    sum_gamma_T_minus_1 = np.sum(gamma[:-1, :], axis=0)\n    sum_xi = np.sum(xi, axis=0)\n    A1 = sum_xi / sum_gamma_T_minus_1[:, np.newaxis]\n    \n    # Update Poisson rates lambda\n    sum_gamma_T = np.sum(gamma, axis=0)\n    weighted_x = np.sum(gamma * x_obs[:, np.newaxis], axis=0)\n    \n    # Handle case where a state is never visited (sum_gamma_T[k] is 0)\n    # To avoid division by zero. A small value is assigned.\n    lambda1 = np.divide(weighted_x, sum_gamma_T, out=np.zeros_like(weighted_x), where=sum_gamma_T!=0)\n\n    # Initial distribution pi is fixed\n    pi1 = pi0\n\n    return pi1, A1, lambda1\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}