{
    "hands_on_practices": [
        {
            "introduction": "A fundamental challenge in connectomics is distinguishing direct neural interactions from spurious correlations induced by common inputs. Two brain regions may exhibit highly correlated activity simply because they are both driven by a third region, not because they communicate directly. This hands-on simulation practice  provides an intuitive yet rigorous understanding of this phenomenon by comparing standard Pearson correlation with partial correlation. By simulating data from a known causal structure, you will see firsthand how conditioning on a common driver can correctly remove the spurious association, a critical skill for interpreting functional connectivity graphs.",
            "id": "3972305",
            "problem": "You will construct and analyze simulated data from linear Gaussian structural equation models to study how a latent common driver induces spurious pairwise associations and how conditioning on the driver removes these associations. The setting is directly applicable to functional connectivity analysis, where observed node activities may be jointly influenced by unobserved or observed common inputs.\n\nStart from the following fundamental base:\n- For a zero-mean random vector $X \\in \\mathbb{R}^p$ with covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$, the covariance between components $X_i$ and $X_j$ is $\\operatorname{Cov}(X_i,X_j) = \\Sigma_{ij}$ and the variance is $\\operatorname{Var}(X_i) = \\Sigma_{ii}$.\n- The Pearson correlation between $X_i$ and $X_j$ is defined as $\\rho_{ij} = \\dfrac{\\Sigma_{ij}}{\\sqrt{\\Sigma_{ii}\\Sigma_{jj}}}$.\n- For a jointly Gaussian vector, the partial correlation between $X_i$ and $X_j$ given a conditioning set $S$ equals the correlation of the residuals after linearly regressing $X_i$ and $X_j$ on $X_S$. Equivalently, for a jointly Gaussian vector with precision matrix $K = \\Sigma^{-1}$, the partial correlation between $X_i$ and $X_j$ given all other variables is $\\rho_{ij \\cdot \\text{others}} = -\\dfrac{K_{ij}}{\\sqrt{K_{ii} K_{jj}}}$.\n\nYour task is to implement a program that simulates four test cases, estimates sample covariances from the simulated data, and computes both the sample correlation and the sample partial correlation dictated by the observation and conditioning sets. You must use the precision-matrix characterization of partial correlation when you condition on all variables in the observed set except the two of interest. The data-generating models follow linear structural equations with independent Gaussian noise and zero means. All variables are dimensionless. Angles are not involved. No physical units are present.\n\nLet $\\varepsilon_x, \\varepsilon_y, \\varepsilon_w$ denote mutually independent Gaussian noise terms with variances $\\sigma_x^2, \\sigma_y^2, \\sigma_w^2$, respectively. Let $Z$ denote a Gaussian common driver with variance $\\sigma_z^2$. Let $W$ be an independent Gaussian control variable when present. Let $a, b, c$ denote scalar coefficients. The simulation parameters for the test suite are:\n\n- Test A (observed common driver, no direct edge):\n  - Structural equations: $Z \\sim \\mathcal{N}(0,\\sigma_z^2)$, $X = a Z + \\varepsilon_x$, $Y = b Z + \\varepsilon_y$.\n  - Parameters: $a = 0.9$, $b = 0.7$, $\\sigma_z^2 = 1.0$, $\\sigma_x^2 = 1.0$, $\\sigma_y^2 = 1.0$.\n  - Sample size: $N = 100000$.\n  - Observed variables: $[X,Y,Z]$; compute correlation between $X$ and $Y$, and partial correlation between $X$ and $Y$ given $Z$.\n\n- Test B (latent common driver unobserved, conditioning on an irrelevant control):\n  - Structural equations: $Z \\sim \\mathcal{N}(0,\\sigma_z^2)$, $W \\sim \\mathcal{N}(0,\\sigma_w^2)$ independent of $Z$, $X = a Z + \\varepsilon_x$, $Y = b Z + \\varepsilon_y$.\n  - Parameters: $a = 0.9$, $b = 0.7$, $\\sigma_z^2 = 1.0$, $\\sigma_x^2 = 1.0$, $\\sigma_y^2 = 1.0$, $\\sigma_w^2 = 1.0$.\n  - Sample size: $N = 100000$.\n  - Observed variables: $[X,Y,W]$ (the common driver $Z$ is unobserved); compute correlation between $X$ and $Y$, and partial correlation between $X$ and $Y$ given $W$.\n\n- Test C (observed common driver with an additional direct edge):\n  - Structural equations: $Z \\sim \\mathcal{N}(0,\\sigma_z^2)$, $X = a Z + \\varepsilon_x$, $Y = b Z + c X + \\varepsilon_y$.\n  - Parameters: $a = 0.9$, $b = 0.7$, $c = 0.5$, $\\sigma_z^2 = 1.0$, $\\sigma_x^2 = 1.0$, $\\sigma_y^2 = 1.0$.\n  - Sample size: $N = 100000$.\n  - Observed variables: $[X,Y,Z]$; compute correlation between $X$ and $Y$, and partial correlation between $X$ and $Y$ given $Z$.\n\n- Test D (no common driver, no direct edge):\n  - Structural equations: $X = \\varepsilon_x$, $Y = \\varepsilon_y$, $W = \\varepsilon_w$.\n  - Parameters: $\\sigma_x^2 = 1.0$, $\\sigma_y^2 = 1.0$, $\\sigma_w^2 = 1.0$.\n  - Sample size: $N = 100000$.\n  - Observed variables: $[X,Y,W]$; compute correlation between $X$ and $Y$, and partial correlation between $X$ and $Y$ given $W$.\n\nFor each test, compute:\n- The sample Pearson correlation between $X$ and $Y$ estimated from the observed variables in that test.\n- The sample partial correlation between $X$ and $Y$ given the indicated conditioning set for that test, computed via the inverse of the sample covariance matrix of the observed variables and the corresponding precision submatrix entry.\n\nReport, for each test, three outputs:\n- The sample correlation between $X$ and $Y$ rounded to three decimal places.\n- The sample partial correlation between $X$ and $Y$ given the specified conditioning set, rounded to three decimal places.\n- A boolean flag defined as $\\lvert \\text{partial correlation} \\rvert  \\tau$, with threshold $\\tau = 0.05$, indicating whether conditioning removed the spurious edge.\n\nYour program must:\n- Use a fixed random seed for reproducibility.\n- Simulate Gaussian data according to the structural equations and parameters above.\n- Estimate the required quantities from the simulated data.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n  $[$corr$\\_A$, pcor$\\_A$, removed$\\_A$, corr$\\_B$, pcor$\\_B$, removed$\\_B$, corr$\\_C$, pcor$\\_C$, removed$\\_C$, corr$\\_D$, pcor$\\_D$, removed$\\_D]$.\nAll floating-point values must be rounded to three decimal places. The booleans must be unrounded logical values.\n\nThe final output must be a single line with this list and no additional text. There are no physical units involved, and no angles or percentages are required.",
            "solution": "The task is to simulate data from four distinct linear Gaussian structural equation models (SEMs) to explore the concepts of marginal and partial correlation in the context of functional connectivity. Specifically, we will investigate how a common driver can induce a spurious correlation between two variables and how conditioning on this driver can correctly identify the lack of a direct link. Conversely, we will see how a true direct link persists as a non-zero partial correlation. The analysis relies on fundamental principles of multivariate Gaussian distributions.\n\nFor a set of $p$ jointly Gaussian random variables with zero mean, let their vector be $V \\in \\mathbb{R}^p$ and their covariance matrix be $\\Sigma \\in \\mathbb{R}^{p \\times p}$. The Pearson correlation between two variables $V_i$ and $V_j$ is defined as:\n$$\n\\rho_{ij} = \\frac{\\operatorname{Cov}(V_i, V_j)}{\\sqrt{\\operatorname{Var}(V_i)\\operatorname{Var}(V_j)}} = \\frac{\\Sigma_{ij}}{\\sqrt{\\Sigma_{ii}\\Sigma_{jj}}}\n$$\nThe partial correlation between $V_i$ and $V_j$ given all other variables in the set, denoted $V_{\\setminus \\{i, j\\}}$, is a measure of their linear association after removing the linear effects of all other variables. For a multivariate Gaussian distribution, this can be conveniently calculated from the precision matrix, $K = \\Sigma^{-1}$, as:\n$$\n\\rho_{ij \\cdot \\text{rest}} = -\\frac{K_{ij}}{\\sqrt{K_{ii}K_{jj}}}\n$$\nWe will simulate data for each of the four test cases, estimate the sample covariance matrix $\\hat{\\Sigma}$ from the simulated data vectors, and then compute the sample Pearson correlation and sample partial correlation using the formulas above with $\\hat{\\Sigma}$ and its inverse, the sample precision matrix $\\hat{K} = \\hat{\\Sigma}^{-1}$. A fixed random seed and a large sample size of $N=100000$ are used to ensure reproducible and statistically stable results.\n\nThe procedure for each test case is as follows:\n1.  Generate $N=100000$ samples for the independent Gaussian variables ($Z$, $W$, $\\varepsilon_x$, $\\varepsilon_y$, $\\varepsilon_w$) with their specified variances.\n2.  Construct the observed variables ($X$, $Y$, and either $Z$ or $W$) according to the given structural equations for that test case.\n3.  Arrange the observed variables into a data matrix of shape $(p, N)$, where $p=3$.\n4.  Compute the $3 \\times 3$ sample covariance matrix $\\hat{\\Sigma}$ and the sample correlation matrix from this data. The Pearson correlation $\\rho_{XY}$ is the $(0,1)$ entry of the correlation matrix (assuming $X$ is the first variable and $Y$ is the second).\n5.  Invert the sample covariance matrix to obtain the sample precision matrix, $\\hat{K} = \\hat{\\Sigma}^{-1}$.\n6.  Calculate the sample partial correlation $\\rho_{XY \\cdot \\text{given}}$ using the corresponding entries of $\\hat{K}$.\n7.  Determine a boolean flag, `removed`, defined as $|\\rho_{XY \\cdot \\text{given}}|  \\tau$ with a threshold of $\\tau = 0.05$.\n\nBelow is an analysis of each specific test case.\n\nTest A: Observed common driver ($X \\leftarrow Z \\rightarrow Y$)\nThe model is $X = aZ + \\varepsilon_x$ and $Y = bZ + \\varepsilon_y$, with $a=0.9$ and $b=0.7$. The common driver $Z$ is observed. There is no direct link between $X$ and $Y$. The correlation between $X$ and $Y$ arises solely because they share a common cause, $Z$. The theoretical correlation is $\\operatorname{Corr}(X,Y) = \\frac{ab\\sigma_z^2}{\\sqrt{(a^2\\sigma_z^2+\\sigma_x^2)(b^2\\sigma_z^2+\\sigma_y^2)}} \\approx 0.384$. When we condition on $Z$, we block the only path connecting $X$ and $Y$. Their conditional relationship is governed by the residuals $\\varepsilon_x$ and $\\varepsilon_y$, which are independent. Therefore, the partial correlation $\\rho_{XY|Z}$ is expected to be $0$. The sample estimate should be very close to $0$, and the `removed` flag should be `True`.\n\nTest B: Latent common driver ($X \\leftarrow Z \\rightarrow Y$, condition on irrelevant $W$)\nThe model for $X$ and $Y$ is the same as in Test A, but the common driver $Z$ is unobserved (latent). Instead, we observe an independent variable $W$. The correlation between $X$ and $Y$ is the same as in Test A ($\\approx 0.384$) due to the latent common cause. Conditioning on $W$, which is statistically independent of $X$, $Y$, and $Z$, provides no information about their relationship. Therefore, the partial correlation $\\rho_{XY|W}$ is expected to be identical to the marginal correlation $\\rho_{XY}$. The sample estimate of partial correlation should be close to the sample marginal correlation, and the `removed` flag should be `False`.\n\nTest C: Common driver and direct edge ($X \\leftarrow Z \\rightarrow Y \\leftarrow X$)\nThe model is $X = aZ + \\varepsilon_x$ and $Y = bZ + cX + \\varepsilon_y$, with $a=0.9$, $b=0.7$, and $c=0.5$. Here, $Y$ is influenced by $Z$ both directly and indirectly through $X$. There are two paths creating a correlation between $X$ and $Y$: the confounding path $X \\leftarrow Z \\rightarrow Y$ and the direct path $X \\rightarrow Y$. The marginal correlation $\\rho_{XY}$ will be strong, reflecting both contributions. When we condition on the common driver $Z$, we block the confounding path. However, the direct influence of $X$ on $Y$ (the term $cX$) remains. The partial correlation $\\rho_{XY|Z}$ will therefore be non-zero, capturing the strength of this direct link. The expected partial correlation is $\\approx 0.447$. The `removed` flag should be `False`.\n\nTest D: Independent variables\nThe model is $X = \\varepsilon_x$, $Y = \\varepsilon_y$, $W = \\varepsilon_w$. All three observed variables are mutually independent by definition. There is no structural path connecting $X$ and $Y$. Therefore, both the marginal correlation $\\rho_{XY}$ and the partial correlation $\\rho_{XY|W}$ are theoretically $0$. Due to sampling variability, the sample estimates will be small values fluctuating around $0$. Both should fall well below the threshold $\\tau=0.05$, so the `removed` flag is expected to be `True`.\n\nThe implementation will now proceed to compute these values from simulated data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Simulates data from four structural equation models, computes sample\n    correlation and partial correlation, and determines if conditioning removes\n    the association.\n    \"\"\"\n    \n    # Set up global parameters\n    N = 100000\n    rng = np.random.default_rng(seed=0)\n    tau = 0.05\n    all_results = []\n\n    # --- Test Case A: Observed Common Driver ---\n    a = 0.9\n    b = 0.7\n    sigma_z_sq, sigma_x_sq, sigma_y_sq = 1.0, 1.0, 1.0\n\n    Z = rng.normal(loc=0, scale=np.sqrt(sigma_z_sq), size=N)\n    eps_x = rng.normal(loc=0, scale=np.sqrt(sigma_x_sq), size=N)\n    eps_y = rng.normal(loc=0, scale=np.sqrt(sigma_y_sq), size=N)\n\n    X = a * Z + eps_x\n    Y = b * Z + eps_y\n    \n    # Observed variables: [X, Y, Z]\n    data_A = np.vstack([X, Y, Z])\n    \n    # Pearson correlation rho(X, Y)\n    corr_mat_A = np.corrcoef(data_A)\n    corr_A = corr_mat_A[0, 1]\n\n    # Partial correlation rho(X, Y | Z) via precision matrix\n    cov_mat_A = np.cov(data_A)\n    prec_mat_A = np.linalg.inv(cov_mat_A)\n    pcorr_A = -prec_mat_A[0, 1] / np.sqrt(prec_mat_A[0, 0] * prec_mat_A[1, 1])\n\n    removed_A = abs(pcorr_A)  tau\n    all_results.extend([round(corr_A, 3), round(pcorr_A, 3), removed_A])\n\n    # --- Test Case B: Latent Common Driver ---\n    a = 0.9\n    b = 0.7\n    sigma_z_sq, sigma_x_sq, sigma_y_sq, sigma_w_sq = 1.0, 1.0, 1.0, 1.0\n\n    Z_latent = rng.normal(loc=0, scale=np.sqrt(sigma_z_sq), size=N)\n    W = rng.normal(loc=0, scale=np.sqrt(sigma_w_sq), size=N)\n    eps_x = rng.normal(loc=0, scale=np.sqrt(sigma_x_sq), size=N)\n    eps_y = rng.normal(loc=0, scale=np.sqrt(sigma_y_sq), size=N)\n\n    X = a * Z_latent + eps_x\n    Y = b * Z_latent + eps_y\n\n    # Observed variables: [X, Y, W]\n    data_B = np.vstack([X, Y, W])\n\n    # Pearson correlation rho(X, Y)\n    corr_mat_B = np.corrcoef(data_B)\n    corr_B = corr_mat_B[0, 1]\n\n    # Partial correlation rho(X, Y | W)\n    cov_mat_B = np.cov(data_B)\n    prec_mat_B = np.linalg.inv(cov_mat_B)\n    pcorr_B = -prec_mat_B[0, 1] / np.sqrt(prec_mat_B[0, 0] * prec_mat_B[1, 1])\n\n    removed_B = abs(pcorr_B)  tau\n    all_results.extend([round(corr_B, 3), round(pcorr_B, 3), removed_B])\n\n    # --- Test Case C: Common Driver and Direct Edge ---\n    a = 0.9\n    b = 0.7\n    c = 0.5\n    sigma_z_sq, sigma_x_sq, sigma_y_sq = 1.0, 1.0, 1.0\n    \n    Z = rng.normal(loc=0, scale=np.sqrt(sigma_z_sq), size=N)\n    eps_x = rng.normal(loc=0, scale=np.sqrt(sigma_x_sq), size=N)\n    eps_y = rng.normal(loc=0, scale=np.sqrt(sigma_y_sq), size=N)\n\n    X = a * Z + eps_x\n    Y = b * Z + c * X + eps_y\n    \n    # Observed variables: [X, Y, Z]\n    data_C = np.vstack([X, Y, Z])\n\n    # Pearson correlation rho(X, Y)\n    corr_mat_C = np.corrcoef(data_C)\n    corr_C = corr_mat_C[0, 1]\n\n    # Partial correlation rho(X, Y | Z)\n    cov_mat_C = np.cov(data_C)\n    prec_mat_C = np.linalg.inv(cov_mat_C)\n    pcorr_C = -prec_mat_C[0, 1] / np.sqrt(prec_mat_C[0, 0] * prec_mat_C[1, 1])\n\n    removed_C = abs(pcorr_C)  tau\n    all_results.extend([round(corr_C, 3), round(pcorr_C, 3), removed_C])\n\n    # --- Test Case D: No Common Driver, No Direct Edge ---\n    sigma_x_sq, sigma_y_sq, sigma_w_sq = 1.0, 1.0, 1.0\n\n    X = rng.normal(loc=0, scale=np.sqrt(sigma_x_sq), size=N)\n    Y = rng.normal(loc=0, scale=np.sqrt(sigma_y_sq), size=N)\n    W = rng.normal(loc=0, scale=np.sqrt(sigma_w_sq), size=N)\n\n    # Observed variables: [X, Y, W]\n    data_D = np.vstack([X, Y, W])\n    \n    # Pearson correlation rho(X, Y)\n    corr_mat_D = np.corrcoef(data_D)\n    corr_D = corr_mat_D[0, 1]\n\n    # Partial correlation rho(X, Y | W)\n    cov_mat_D = np.cov(data_D)\n    prec_mat_D = np.linalg.inv(cov_mat_D)\n    pcorr_D = -prec_mat_D[0, 1] / np.sqrt(prec_mat_D[0, 0] * prec_mat_D[1, 1])\n\n    removed_D = abs(pcorr_D)  tau\n    all_results.extend([round(corr_D, 3), round(pcorr_D, 3), removed_D])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond pairwise analysis, we often wish to infer the entire conditional independence graph of a brain network simultaneously. The graphical lasso is a powerful technique for this, estimating a sparse precision matrix that reveals the network of direct connections. A crucial step in this process is selecting the regularization parameter $\\lambda$, which controls the sparsity of the resulting graph. This exercise  guides you through selecting an optimal $\\lambda$ by integrating two key methodologies: the Bayesian Information Criterion (BIC) for model fit and subsampling-based stability selection for reproducibility, ensuring the inferred network is both parsimonious and robust.",
            "id": "3972302",
            "problem": "Consider a zero-mean multivariate Gaussian model for neural time series from $p=4$ regions of interest, used to infer functional connectivity via a sparse precision (inverse covariance) matrix. Let $X \\in \\mathbb{R}^{n \\times p}$ denote $n=200$ independent samples and $S = \\frac{1}{n} X^{\\top} X$ the sample covariance matrix. The graphical least absolute shrinkage and selection operator (graphical lasso) estimates a precision matrix $\\hat{\\Theta}_{\\lambda}$ by maximizing a penalized Gaussian log-likelihood with an $\\ell_{1}$ penalty of strength $\\lambda  0$ applied to the off-diagonal entries.\n\nYou are provided three candidate regularization parameters $\\lambda \\in \\{0.10, 0.25, 0.50\\}$. For each $\\lambda$, the corresponding solution $\\hat{\\Theta}_{\\lambda}$ has the following summary statistics (computed from the data and $\\hat{\\Theta}_{\\lambda}$):\n\n- For $\\lambda = 0.10$:\n  - $\\log \\det(\\hat{\\Theta}_{0.10}) = 1.20$,\n  - $\\operatorname{tr}\\!\\left(S \\hat{\\Theta}_{0.10}\\right) = 4.60$,\n  - estimated edge set $E_{0.10} = \\{(1,2), (2,3), (3,4)\\}$ with $|E_{0.10}| = 3$.\n\n- For $\\lambda = 0.25$:\n  - $\\log \\det(\\hat{\\Theta}_{0.25}) = 1.00$,\n  - $\\operatorname{tr}\\!\\left(S \\hat{\\Theta}_{0.25}\\right) = 4.55$,\n  - estimated edge set $E_{0.25} = \\{(1,2), (3,4)\\}$ with $|E_{0.25}| = 2$.\n\n- For $\\lambda = 0.50$:\n  - $\\log \\det(\\hat{\\Theta}_{0.50}) = 0.85$,\n  - $\\operatorname{tr}\\!\\left(S \\hat{\\Theta}_{0.50}\\right) = 4.60$,\n  - estimated edge set $E_{0.50} = \\{(3,4)\\}$ with $|E_{0.50}| = 1$.\n\nTo assess reproducibility via subsampling-based stability selection, suppose you generated $B=100$ half-sample subsamples. For each potential undirected edge $(i,j)$ and each $\\lambda$, the fraction of subsamples in which $(i,j)$ appears in the estimated edge set defines the selection probability $\\pi_{ij}(\\lambda) \\in [0,1]$. The observed selection probabilities are:\n\n- For $\\lambda = 0.10$: $\\pi_{12}=0.78$, $\\pi_{23}=0.81$, $\\pi_{34}=0.83$, $\\pi_{13}=0.30$, $\\pi_{14}=0.10$, $\\pi_{24}=0.25$.\n- For $\\lambda = 0.25$: $\\pi_{12}=0.86$, $\\pi_{34}=0.88$, $\\pi_{23}=0.40$, $\\pi_{13}=0.18$, $\\pi_{14}=0.08$, $\\pi_{24}=0.20$.\n- For $\\lambda = 0.50$: $\\pi_{34}=0.90$, $\\pi_{12}=0.52$, $\\pi_{23}=0.22$, $\\pi_{13}=0.12$, $\\pi_{14}=0.05$, $\\pi_{24}=0.10$.\n\nTasks:\n\n1. Starting from the Gaussian graphical model and the effect of an $\\ell_{1}$ penalty on the off-diagonal entries of the precision matrix, explain how the sparsity pattern of $\\hat{\\Theta}_{\\lambda}$ evolves as $\\lambda$ varies, and why edges are pruned monotonically as $\\lambda$ increases.\n\n2. Starting from the definition of the Gaussian log-likelihood and the information criteria, derive the Bayesian Information Criterion (BIC) for a Gaussian graphical model with precision matrix $\\Theta$ and show how the model dimension (degrees of freedom) relates to the number of nonzero unique parameters in $\\Theta$. State your counting convention explicitly.\n\n3. Define a stability selection rule based on a threshold $\\pi_{\\mathrm{thr}} = 0.80$ that declares a model at a given $\\lambda$ to be stable if and only if every edge in $E_{\\lambda}$ has selection probability at least $\\pi_{\\mathrm{thr}}$, and every edge not in $E_{\\lambda}$ has selection probability less than $\\pi_{\\mathrm{thr}}$.\n\n4. Using your derivations, among the candidate $\\lambda \\in \\{0.10, 0.25, 0.50\\}$, select the single $\\lambda$ that minimizes the BIC subject to the stability rule in Task 3. Report the selected $\\lambda$ as a real number. Round your answer to three significant figures.",
            "solution": "The problem requires a multi-step analysis involving the interpretation of the graphical lasso, the derivation of the Bayesian Information Criterion (BIC) for Gaussian graphical models, and the application of a stability selection criterion to choose an optimal regularization parameter $\\lambda$. We will address each of the four tasks in sequence.\n\nFirst, we address the explanation of the sparsity pattern as a function of the regularization parameter $\\lambda$. The graphical lasso estimates the precision matrix $\\hat{\\Theta}_{\\lambda}$ by solving the following optimization problem:\n$$\n\\hat{\\Theta}_{\\lambda} = \\arg \\max_{\\Theta \\succ 0} \\left\\{ \\log \\det(\\Theta) - \\operatorname{tr}(S \\Theta) - \\lambda \\sum_{i \\neq j} |\\Theta_{ij}| \\right\\}\n$$\nwhere $\\Theta$ is a positive definite matrix, $S$ is the sample covariance matrix, and $\\lambda  0$ is the regularization parameter. The term $\\log \\det(\\Theta) - \\operatorname{tr}(S \\Theta)$ is proportional to the Gaussian log-likelihood. The term $\\lambda \\sum_{i \\neq j} |\\Theta_{ij}|$, also written as $\\lambda \\|\\Theta\\|_{\\text{off},1}$, is an $\\ell_1$-norm penalty applied to the off-diagonal elements of the precision matrix. This penalty encourages sparsity, meaning it drives some of the off-diagonal elements $\\Theta_{ij}$ to exactly zero. An element $\\Theta_{ij}$ being zero corresponds to conditional independence between variables $i$ and $j$ given all other variables, which translates to the absence of an edge between nodes $i$ and $j$ in the associated dependency graph. The parameter $\\lambda$ controls the strength of this penalty. As $\\lambda$ increases, the cost for having a non-zero off-diagonal element increases. Consequently, to maximize the objective function, the algorithm will set more off-diagonal elements to zero. This results in a sparser graph. The set of edges is pruned monotonically because for any two regularization parameters $\\lambda_1  \\lambda_2$, the set of estimated edges $E_{\\lambda_2}$ is generally a subset of $E_{\\lambda_1}$ (i.e., $E_{\\lambda_2} \\subseteq E_{\\lambda_1}$). This is a fundamental property of the lasso solution path: once an element is set to zero for a given $\\lambda$, it remains zero for all stronger penalties. The provided data demonstrates the monotonic decrease in the number of edges: $|E_{0.10}|=3$, $|E_{0.25}|=2$, and $|E_{0.50}|=1$. Note that in this specific example, $E_{0.25} = \\{(1,2), (3,4)\\}$ is not a subset of $E_{0.10} = \\{(1,2), (2,3), (3,4)\\}$, which indicates a slight deviation from the theoretical path, possibly due to numerical optimization or specific data characteristics. However, the number of edges is monotonically decreasing, which is the key consequence of increasing $\\lambda$.\n\nSecond, we derive the Bayesian Information Criterion (BIC) for this model. The general form of BIC is:\n$$\n\\text{BIC} = k \\ln(n) - 2 \\mathcal{L}_{\\text{max}}\n$$\nwhere $k$ is the number of free parameters in the model, $n$ is the number of samples, and $\\mathcal{L}_{\\text{max}}$ is the maximized value of the log-likelihood function. For $n$ independent and identically distributed samples $\\mathbf{x}_1, \\dots, \\mathbf{x}_n$ from a zero-mean $p$-variate Gaussian distribution with precision matrix $\\Theta$, the log-likelihood is:\n$$\n\\mathcal{L}(\\Theta) = \\sum_{i=1}^n \\log p(\\mathbf{x}_i | \\Theta) = \\sum_{i=1}^n \\left( \\frac{1}{2} \\log \\det(\\Theta) - \\frac{p}{2} \\log(2\\pi) - \\frac{1}{2} \\mathbf{x}_i^\\top \\Theta \\mathbf{x}_i \\right)\n$$\nSumming over the $n$ samples, we get:\n$$\n\\mathcal{L}(\\Theta) = \\frac{n}{2} \\log \\det(\\Theta) - \\frac{np}{2} \\log(2\\pi) - \\frac{1}{2} \\operatorname{tr}\\left( \\Theta \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\top \\right)\n$$\nUsing the definition of the sample covariance matrix $S = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\top$, this becomes:\n$$\n\\mathcal{L}(\\Theta) = \\frac{n}{2} \\left( \\log \\det(\\Theta) - \\operatorname{tr}(S\\Theta) \\right) - \\frac{np}{2} \\log(2\\pi)\n$$\nFor a model estimated with a given $\\lambda$, we use the corresponding estimate $\\hat{\\Theta}_{\\lambda}$. The maximized log-likelihood is $\\mathcal{L}_{\\text{max}} = \\mathcal{L}(\\hat{\\Theta}_{\\lambda})$. We then substitute this into the BIC formula. Note that some definitions of BIC use $-2\\mathcal{L}_{\\text{max}} + k \\ln(n)$ and seek to minimize it, which is equivalent. We will adopt the minimization form.\n$$\n\\text{BIC} = -2 \\mathcal{L}(\\hat{\\Theta}_{\\lambda}) + k \\ln(n) = -n \\left( \\log \\det(\\hat{\\Theta}_{\\lambda}) - \\operatorname{tr}(S\\hat{\\Theta}_{\\lambda}) \\right) + np \\log(2\\pi) + k \\ln(n)\n$$\nSince the term $np \\log(2\\pi)$ is constant across all models for a given dataset, it can be dropped when comparing BIC values. Thus, the comparative BIC is:\n$$\n\\text{BIC}(\\lambda) = n \\left( \\operatorname{tr}(S\\hat{\\Theta}_{\\lambda}) - \\log \\det(\\hat{\\Theta}_{\\lambda}) \\right) + k \\ln(n)\n$$\nThe number of free parameters, $k$, must be determined. Our counting convention is as follows: The precision matrix $\\Theta$ is symmetric. A model is defined by its non-zero entries. We always estimate the $p$ diagonal entries. The off-diagonal structure is defined by the edge set $E_{\\lambda}$. The number of unique non-zero off-diagonal parameters is $|E_{\\lambda}|$, since $\\Theta_{ij} = \\Theta_{ji}$. Therefore, the total number of free parameters is $k = p + |E_{\\lambda}|$.\n\nThird, we state the stability selection rule. A model corresponding to $\\hat{\\Theta}_{\\lambda}$ and its edge set $E_{\\lambda}$ is declared stable if and only if both of the following conditions are met with respect to the threshold $\\pi_{\\mathrm{thr}} = 0.80$:\n1. For every edge $(i, j) \\in E_{\\lambda}$, the corresponding selection probability must satisfy $\\pi_{ij}(\\lambda) \\ge \\pi_{\\mathrm{thr}}$.\n2. For every potential edge $(i, j) \\notin E_{\\lambda}$, the corresponding selection probability must satisfy $\\pi_{ij}(\\lambda)  \\pi_{\\mathrm{thr}}$.\n\nFourth, we select the optimal $\\lambda$ from the set $\\{0.10, 0.25, 0.50\\}$ by first filtering for stability and then minimizing the BIC. We use the given data: $n=200$, $p=4$, $\\pi_{\\mathrm{thr}} = 0.80$.\n\nWe test each $\\lambda$ for stability:\n- **For $\\lambda = 0.10$**:\n  - $E_{0.10} = \\{(1,2), (2,3), (3,4)\\}$.\n  - Condition 1 check: We require $\\pi_{ij}(0.10) \\ge 0.80$ for all edges in $E_{0.10}$.\n  - We have $\\pi_{12}(0.10) = 0.78$, which is less than $0.80$.\n  - Therefore, the model for $\\lambda = 0.10$ is **not stable**.\n\n- **For $\\lambda = 0.25$**:\n  - $E_{0.25} = \\{(1,2), (3,4)\\}$.\n  - Condition 1 check:\n    - $\\pi_{12}(0.25) = 0.86 \\ge 0.80$. (Pass)\n    - $\\pi_{34}(0.25) = 0.88 \\ge 0.80$. (Pass)\n  - Condition 2 check: We require $\\pi_{ij}(0.25)  0.80$ for all edges not in $E_{0.25}$. These edges are $(2,3), (1,3), (1,4), (2,4)$.\n    - $\\pi_{23}(0.25) = 0.40  0.80$. (Pass)\n    - $\\pi_{13}(0.25) = 0.18  0.80$. (Pass)\n    - $\\pi_{14}(0.25) = 0.08  0.80$. (Pass)\n    - $\\pi_{24}(0.25) = 0.20  0.80$. (Pass)\n  - All conditions are met. The model for $\\lambda = 0.25$ is **stable**.\n\n- **For $\\lambda = 0.50$**:\n  - $E_{0.50} = \\{(3,4)\\}$.\n  - Condition 1 check:\n    - $\\pi_{34}(0.50) = 0.90 \\ge 0.80$. (Pass)\n  - Condition 2 check: We require $\\pi_{ij}(0.50)  0.80$ for all other edges.\n    - $\\pi_{12}(0.50) = 0.52  0.80$. (Pass)\n    - $\\pi_{23}(0.50) = 0.22  0.80$. (Pass)\n    - $\\pi_{13}(0.50) = 0.12  0.80$. (Pass)\n    - $\\pi_{14}(0.50) = 0.05  0.80$. (Pass)\n    - $\\pi_{24}(0.50) = 0.10  0.80$. (Pass)\n  - All conditions are met. The model for $\\lambda = 0.50$ is **stable**.\n\nNow we compare the BIC for the two stable models, $\\lambda = 0.25$ and $\\lambda = 0.50$.\nThe formula is $\\text{BIC}(\\lambda) = n \\left( \\operatorname{tr}(S\\hat{\\Theta}_{\\lambda}) - \\log \\det(\\hat{\\Theta}_{\\lambda}) \\right) + (p + |E_{\\lambda}|) \\ln(n)$.\nWe have $n=200$, $p=4$, and $\\ln(200) \\approx 5.2983$.\n\n- **BIC for $\\lambda = 0.25$**:\n  - $\\operatorname{tr}(S\\hat{\\Theta}_{0.25}) - \\log \\det(\\hat{\\Theta}_{0.25}) = 4.55 - 1.00 = 3.55$.\n  - $|E_{0.25}|=2$, so $k = p + |E_{0.25}| = 4 + 2 = 6$.\n  - $\\text{BIC}(0.25) = 200 \\times (3.55) + 6 \\times \\ln(200) = 710 + 6 \\ln(200)$.\n  - $\\text{BIC}(0.25) \\approx 710 + 6 \\times 5.2983 = 710 + 31.7898 = 741.7898$.\n\n- **BIC for $\\lambda = 0.50$**:\n  - $\\operatorname{tr}(S\\hat{\\Theta}_{0.50}) - \\log \\det(\\hat{\\Theta}_{0.50}) = 4.60 - 0.85 = 3.75$.\n  - $|E_{0.50}|=1$, so $k = p + |E_{0.50}| = 4 + 1 = 5$.\n  - $\\text{BIC}(0.50) = 200 \\times (3.75) + 5 \\times \\ln(200) = 750 + 5 \\ln(200)$.\n  - $\\text{BIC}(0.50) \\approx 750 + 5 \\times 5.2983 = 750 + 26.4915 = 776.4915$.\n\nComparing the two values, $\\text{BIC}(0.25) \\approx 741.79$ and $\\text{BIC}(0.50) \\approx 776.49$.\nSince $741.79  776.49$, the model for $\\lambda = 0.25$ has the lower BIC and is therefore preferred.\n\nThe selected value of $\\lambda$ is $0.25$. As a real number rounded to three significant figures, this is $0.250$.",
            "answer": "$$\n\\boxed{0.250}\n$$"
        },
        {
            "introduction": "Neural communication is often mediated by synchronized oscillations, making frequency-domain analysis essential for understanding functional connectivity. Magnitude-squared coherence is a cornerstone measure for quantifying the consistency of phase relationships between signals at specific frequencies. This practice  delves into the theoretical and practical foundations of its estimation using Welch's method. You will derive the estimator from first principles and then tackle the critical trade-off between spectral resolution and estimator variance by selecting an appropriate window length and overlap for analyzing narrowband oscillations, a common task in electrophysiology.",
            "id": "3972316",
            "problem": "You are given two simultaneously recorded zero-mean discrete-time local field potentials sampled at sampling frequency $f_{s}$ from two cortical regions. The aim is to estimate their Cross-Spectral Density (CSD) and Magnitude-Squared Coherence (MSC) as measures of functional connectivity. Let $x[n]$ and $y[n]$ be wide-sense stationary realizations with a narrowband oscillatory interaction centered at frequency $f_{0}$ and additive noise. You will derive an estimator for the CSD using the classical segment-averaging procedure known as Welchâ€™s method and analyze window design choices required for accurate coherence in the case of narrowband oscillations.\n\nStart from the foundational definitions:\n- The cross-correlation function $R_{xy}[\\tau]$ of two wide-sense stationary discrete-time processes $x[n]$ and $y[n]$ is $R_{xy}[\\tau] = \\mathbb{E}\\{x[n] y[n+\\tau]\\}$.\n- The Cross-Spectral Density (CSD) $S_{xy}(f)$ is defined as the discrete-time Fourier transform of $R_{xy}[\\tau]$.\n- The Magnitude-Squared Coherence (MSC) $\\gamma^{2}(f)$ is defined as $\\gamma^{2}(f) = \\frac{|S_{xy}(f)|^{2}}{S_{xx}(f) S_{yy}(f)}$, where $S_{xx}(f)$ and $S_{yy}(f)$ are the Power Spectral Densities (PSD) of $x[n]$ and $y[n]$, respectively.\n\nUsing only these base definitions and the well-tested idea of average periodograms, derive a discrete-time Welch estimator for the CSD that uses $K$ overlapping, windowed segments of length $L$ samples and a deterministic window $w[n]$ of finite support on $\\{0,\\dots,L-1\\}$. Use the following conventions for your derivation:\n- For segment index $k \\in \\{1,\\dots,K\\}$, form windowed segments $x_{k}[n] = w[n] \\, x[n+n_{k}]$ and $y_{k}[n] = w[n] \\, y[n+n_{k}]$, where $n_{k}$ is the starting sample of segment $k$.\n- Let $X_{k}(f_{m})$ and $Y_{k}(f_{m})$ denote the Discrete Fourier Transform (DFT) of $x_{k}[n]$ and $y_{k}[n]$ at the $m$-th frequency bin $f_{m} = m \\frac{f_{s}}{L}$, $m \\in \\{0,\\dots,L-1\\}$, using the standard complex-exponential DFT convention.\n- Normalize by the window energy $U = \\frac{1}{L} \\sum_{n=0}^{L-1} w^{2}[n]$ so that the estimator has correct scaling for white-noise inputs.\n\nThen analyze the window-length and overlap choices needed for accurate coherence in narrowband oscillations. Consider the following scientifically realistic scenario for narrowband gamma oscillations:\n- Sampling frequency $f_{s} = 1000$ Hz.\n- Total number of samples $N = 64{,}000$ (i.e., $64$ seconds of data).\n- Target oscillation center $f_{0} = 40$ Hz with an effective half-power bandwidth of approximately $2$ Hz.\n- You will use a Hann window for leakage control.\n- To resolve the narrowband interaction without excessive spectral leakage, constrain the frequency-bin spacing to satisfy $\\frac{f_{s}}{L} \\leq 0.5$ Hz.\n- To reduce estimator variance sufficiently for stable coherence estimates, require at least $K_{\\min} = 60$ segments. Segmentation is formed by sliding the window by $L(1-q)$ samples, where $q \\in [0,1)$ is the overlap fraction.\n\nTasks:\n1. Derive the Welch estimator for the CSD as a closed-form analytic expression in terms of $K$, $L$, $f_{s}$, $U$, and the segment DFTs $X_{k}(f_{m})$ and $Y_{k}(f_{m})$.\n2. Determine the minimal window length $L$ (in samples) that meets the frequency-bin spacing constraint.\n3. Derive the formula for the number of segments $K$ as a function of $N$, $L$, and $q$ under contiguous, fixed-step sliding, and compute the minimal overlap fraction $q$ that achieves $K \\geq K_{\\min}$ for the given $N$ and your computed $L$.\n\nExpress the final window length in samples and report the required minimum overlap as a unitless fraction. Round the overlap fraction to four significant figures. Your final answer must be given as a single row matrix containing:\n- The derived Welch CSD estimator expression,\n- The minimal window length,\n- The minimal overlap fraction.",
            "solution": "The objective is to derive the Welch estimator for the Cross-Spectral Density (CSD) and then determine the necessary windowing parameters (length and overlap) for a specific signal analysis scenario.\n\n### Task 1: Derivation of the Welch CSD Estimator\n\nThe Cross-Spectral Density (CSD) $S_{xy}(f)$ is the Discrete-Time Fourier Transform (DTFT) of the cross-correlation function $R_{xy}[\\tau]$. An estimator for the CSD can be constructed by averaging periodograms, a technique known as Welch's method.\n\nLet $x[n]$ and $y[n]$ be two zero-mean, wide-sense stationary discrete-time processes. We divide the signals into $K$ potentially overlapping segments of length $L$. For the $k$-th segment, starting at sample $n_k$, we apply a window function $w[n]$:\n$$x_k[n] = w[n] x[n+n_k], \\quad n \\in \\{0, 1, \\dots, L-1\\}$$\n$$y_k[n] = w[n] y[n+n_k], \\quad n \\in \\{0, 1, \\dots, L-1\\}$$\n\nThe Discrete Fourier Transforms (DFTs) of these windowed segments at frequency bin $m$ are given by:\n$$X_k(f_m) = \\sum_{n=0}^{L-1} x_k[n] \\exp\\left(-j \\frac{2\\pi mn}{L}\\right)$$\n$$Y_k(f_m) = \\sum_{n=0}^{L-1} y_k[n] \\exp\\left(-j \\frac{2\\pi mn}{L}\\right)$$\nwhere $f_m = m \\frac{f_s}{L}$ is the physical frequency in Hz.\n\nThe cross-periodogram for the $k$-th segment is formed by the product $X_k(f_m) Y_k^*(f_m)$, where $Y_k^*(f_m)$ is the complex conjugate of $Y_k(f_m)$. To obtain an estimator for the CSD, we must apply a normalization factor. The estimator should be scaled such that its expected value is the true CSD.\n\nThe windowing process reduces the total energy of the signal segment. This loss is corrected by normalizing by the window's energy. A common convention, specified in the problem, defines a normalization constant $U$ related to the mean-squared value of the window function:\n$$U = \\frac{1}{L} \\sum_{n=0}^{L-1} w^2[n]$$\nThe total energy of the window is $\\sum_{n=0}^{L-1} w^2[n] = L U$. The normalization factor for the power in a single segment's periodogram is thus $\\frac{1}{L U}$. The estimator for the discrete-time CSD from a single segment is $\\frac{1}{L U} X_k(f_m) Y_k^*(f_m)$.\n\nWelch's method improves the estimator by averaging the periodograms from all $K$ segments, which reduces the variance of the estimate:\n$$\\hat{S}_{xy, \\text{discrete}}(f_m) = \\frac{1}{K} \\sum_{k=1}^K \\frac{1}{L U} X_k(f_m) Y_k^*(f_m) = \\frac{1}{K L U} \\sum_{k=1}^K X_k(f_m) Y_k^*(f_m)$$\nThis expression provides an estimate of the discrete-time CSD, which has units of power (e.g., $\\text{V}^2$). For applications involving physical signals sampled in time, it is conventional to express the CSD as a power spectral density, with units of power per unit frequency (e.g., $\\text{V}^2/\\text{Hz}$). To convert from power to power density, we must divide by the sampling frequency, $f_s$.\n\nTherefore, the Welch estimator for the CSD, denoted $\\hat{S}_{xy}(f_m)$, in units of power per Hz is:\n$$\\hat{S}_{xy}(f_m) = \\frac{1}{f_s} \\hat{S}_{xy, \\text{discrete}}(f_m) = \\frac{1}{f_s K L U} \\sum_{k=1}^K X_k(f_m) Y_k^*(f_m)$$\nThis is the final expression for the Welch CSD estimator as requested.\n\n### Task 2: Minimal Window Length\n\nThe problem imposes a constraint on the frequency resolution of the analysis. The frequency-bin spacing, $\\Delta f$, must be no more than $0.5$ Hz. The bin spacing is determined by the sampling frequency $f_s$ and the window length $L$ (which is equal to the DFT length):\n$$\\Delta f = \\frac{f_s}{L}$$\nThe constraint is $\\Delta f \\leq 0.5$ Hz. We are given $f_s = 1000$ Hz. Substituting these values into the inequality:\n$$\\frac{1000}{L} \\leq 0.5$$\nTo find the minimal window length $L$, we solve for $L$:\n$$L \\geq \\frac{1000}{0.5}$$\n$$L \\geq 2000$$\nThus, the minimal window length that satisfies the frequency resolution constraint is $L = 2000$ samples. This corresponds to a time window of $L/f_s = 2000 / 1000 = 2$ seconds.\n\n### Task 3: Minimal Overlap Fraction\n\nThe number of segments, $K$, that can be extracted from a total of $N$ samples depends on the window length $L$ and the overlap between consecutive segments. The overlap is given as a fraction $q \\in [0, 1)$. The window is advanced by a step of $S$ samples for each new segment, where the step size is related to $L$ and $q$ by $S = L(1-q)$.\n\nThe first segment occupies samples $\\{0, \\dots, L-1\\}$. The last sample that can be the start of a full-length segment is at index $N-L$. The number of segments $K$ can be determined by considering the number of steps that fit within the available span of the signal. The first segment starts at index $0$. The subsequent $(K-1)$ segments start at indices $S, 2S, \\dots, (K-1)S$. The start of the last segment must satisfy:\n$$(K-1)S \\leq N-L$$\nSubstituting $S = L(1-q)$:\n$$(K-1)L(1-q) \\leq N-L$$\nSolving for $K$:\n$$K-1 \\leq \\frac{N-L}{L(1-q)}$$\n$$K \\leq \\frac{N-L}{L(1-q)} + 1$$\nThis formula gives the maximum number of segments for given parameters. To find the minimum overlap fraction $q$ required to achieve at least $K_{\\min} = 60$ segments, we set $K = K_{\\min}$:\n$$K_{\\min} \\leq \\frac{N-L}{L(1-q)} + 1$$\nWe use the given values: $N = 64000$, $L = 2000$, and $K_{\\min} = 60$.\n$$60 \\leq \\frac{64000 - 2000}{2000(1-q)} + 1$$\n$$59 \\leq \\frac{62000}{2000(1-q)}$$\n$$59 \\leq \\frac{31}{1-q}$$\nNow, we solve for the minimal $q$. Since $q  1$, the term $(1-q)$ is positive, so we can multiply without changing the inequality direction:\n$$59(1-q) \\leq 31$$\n$$1-q \\leq \\frac{31}{59}$$\n$$q \\geq 1 - \\frac{31}{59}$$\n$$q \\geq \\frac{59 - 31}{59}$$\n$$q \\geq \\frac{28}{59}$$\nThe minimal required overlap fraction is $\\frac{28}{59}$. To provide a numerical answer, we compute the value and round to four significant figures as requested:\n$$q_{\\min} = \\frac{28}{59} \\approx 0.47457627...$$\nRounding to four significant figures gives:\n$$q_{\\min} = 0.4746$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{f_s K L U} \\sum_{k=1}^{K} X_{k}(f_{m}) Y_{k}^{*}(f_{m})  2000  0.4746\n\\end{pmatrix}\n}\n$$"
        }
    ]
}