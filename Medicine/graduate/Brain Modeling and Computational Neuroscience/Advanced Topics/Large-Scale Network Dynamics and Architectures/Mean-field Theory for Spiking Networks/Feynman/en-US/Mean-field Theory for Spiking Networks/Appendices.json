{
    "hands_on_practices": [
        {
            "introduction": "The first step in building a mean-field model is to characterize the synaptic bombardment a neuron receives from the network. This practice  grounds this abstraction in first principles, treating the synaptic input as a \"shot noise\" process driven by Poisson spike trains. You will derive the stationary mean and variance of the resulting synaptic current, the two key statistical moments that serve as the foundation for the widely used diffusion approximation.",
            "id": "4051240",
            "problem": "Consider a stationary presynaptic spike train modeled as a homogeneous Poisson point process with rate $\\nu$ (events per unit time). Let the postsynaptic response to a single spike be governed by the causal single-exponential synaptic kernel $\\alpha(t) = \\exp(-t/\\tau_s) H(t)$, where $H(t)$ is the Heaviside step function and $\\tau_s  0$ is the synaptic time constant. Two synaptic models are considered:\n\n1. A current-based synapse, in which the postsynaptic current is given by $I(t) = J \\sum_{k} \\alpha(t - t_k)$, where $J$ is a constant current increment per spike and $\\{t_k\\}$ are the presynaptic spike times.\n\n2. A conductance-based synapse, in which the postsynaptic conductance is given by $g(t) = g_0 \\sum_{k} \\alpha(t - t_k)$, where $g_0$ is a constant conductance increment per spike.\n\nStarting only from:\n- The definition of a homogeneous Poisson point process and its stationarity.\n- Well-tested properties of sums over Poisson point processes (e.g., the expectation of sums and second moments over independent points), and the fact that $\\alpha(t)$ is integrable and square-integrable on $[0,\\infty)$.\n\nDerive the stationary mean and variance of the synaptic current $I(t)$ for the current-based synapse, and the stationary mean and variance of the synaptic conductance $g(t)$ for the conductance-based synapse. Express your final answer as closed-form analytic expressions in terms of $\\nu$, $\\tau_s$, $J$, and $g_0$. Do not substitute any numerical values. Provide the final answer as a row matrix with entries in the order $\\langle I \\rangle$, $\\mathrm{Var}(I)$, $\\langle g \\rangle$, $\\mathrm{Var}(g)$. No rounding is required. Express the answer symbolically only.",
            "solution": "The problem asks for the stationary mean and variance of the synaptic current $I(t)$ and synaptic conductance $g(t)$, which are driven by a homogeneous Poisson spike train with rate $\\nu$.\n\nBoth the current $I(t)$ and the conductance $g(t)$ can be represented as linear filtered Poisson processes, also known as shot noise. A general shot noise process $X(t)$ is defined as a sum of responses to events occurring at random times $\\{t_k\\}$:\n$$X(t) = \\sum_{k} h(t - t_k)$$\nwhere $\\{t_k\\}$ are the event times of a Poisson process with rate $\\nu$, and $h(t)$ is the response function or kernel.\n\nFor a stationary, homogeneous Poisson process, the stationary mean $\\langle X \\rangle$ and variance $\\mathrm{Var}(X)$ of the process $X(t)$ are given by Campbell's theorems. The formulas are:\n$$ \\langle X \\rangle = \\nu \\int_{-\\infty}^{\\infty} h(s) ds $$\n$$ \\mathrm{Var}(X) = \\nu \\int_{-\\infty}^{\\infty} h(s)^2 ds $$\nThese formulas are valid under the condition that the integrals converge, which is satisfied here as the kernel $\\alpha(t)$ is stated to be integrable and square-integrable.\n\nThe synaptic kernel is given as $\\alpha(t) = \\exp(-t/\\tau_s) H(t)$, where $H(t)$ is the Heaviside step function. This kernel is causal, meaning $\\alpha(t) = 0$ for $t  0$. We will apply these formulas to both the current-based and conductance-based synaptic models.\n\nFirst, we calculate the necessary integrals of the kernel $\\alpha(t)$.\nThe integral of $\\alpha(t)$ is:\n$$ \\int_{-\\infty}^{\\infty} \\alpha(s) ds = \\int_0^{\\infty} \\exp(-s/\\tau_s) ds = \\left[ -\\tau_s \\exp(-s/\\tau_s) \\right]_0^{\\infty} = 0 - (-\\tau_s \\exp(0)) = \\tau_s $$\n\nThe integral of $\\alpha(t)^2$ is:\n$$ \\int_{-\\infty}^{\\infty} \\alpha(s)^2 ds = \\int_0^{\\infty} \\left( \\exp(-s/\\tau_s) \\right)^2 ds = \\int_0^{\\infty} \\exp(-2s/\\tau_s) ds = \\left[ -\\frac{\\tau_s}{2} \\exp(-2s/\\tau_s) \\right]_0^{\\infty} = 0 - \\left(-\\frac{\\tau_s}{2} \\exp(0)\\right) = \\frac{\\tau_s}{2} $$\n\nNow we can proceed to calculate the moments for each model.\n\n1.  Current-Based Synapse\nThe postsynaptic current is given by $I(t) = J \\sum_{k} \\alpha(t - t_k)$.\nThis is a shot noise process where the response kernel is $h_I(t) = J \\alpha(t)$.\n\nThe mean current, $\\langle I \\rangle$, is:\n$$ \\langle I \\rangle = \\nu \\int_{-\\infty}^{\\infty} h_I(s) ds = \\nu \\int_{-\\infty}^{\\infty} J \\alpha(s) ds = \\nu J \\tau_s $$\n\nThe variance of the current, $\\mathrm{Var}(I)$, is:\n$$ \\mathrm{Var}(I) = \\nu \\int_{-\\infty}^{\\infty} h_I(s)^2 ds = \\nu \\int_{-\\infty}^{\\infty} (J \\alpha(s))^2 ds = \\frac{\\nu J^2 \\tau_s}{2} $$\n\n2.  Conductance-Based Synapse\nThe postsynaptic conductance is given by $g(t) = g_0 \\sum_{k} \\alpha(t - t_k)$.\nThis is also a shot noise process, but with the response kernel $h_g(t) = g_0 \\alpha(t)$. The calculation is identical to the current-based case, with the constant $J$ being replaced by $g_0$.\n\nThe mean conductance, $\\langle g \\rangle$, is:\n$$ \\langle g \\rangle = \\nu \\int_{-\\infty}^{\\infty} h_g(s) ds = \\nu g_0 \\int_{-\\infty}^{\\infty} \\alpha(s) ds = \\nu g_0 \\tau_s $$\n\nThe variance of the conductance, $\\mathrm{Var}(g)$, is:\n$$ \\mathrm{Var}(g) = \\nu \\int_{-\\infty}^{\\infty} h_g(s)^2 ds = \\nu g_0^2 \\int_{-\\infty}^{\\infty} \\alpha(s)^2 ds = \\frac{\\nu g_0^2 \\tau_s}{2} $$\n\nThe four requested quantities are:\n- Mean current: $\\langle I \\rangle = \\nu J \\tau_s$\n- Variance of current: $\\mathrm{Var}(I) = \\frac{\\nu J^2 \\tau_s}{2}$\n- Mean conductance: $\\langle g \\rangle = \\nu g_0 \\tau_s$\n- Variance of conductance: $\\mathrm{Var}(g) = \\frac{\\nu g_0^2 \\tau_s}{2}$\n\nThe final answer is to be presented as a row matrix with these four values in order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\nu J \\tau_s  \\frac{\\nu J^2 \\tau_s}{2}  \\nu g_0 \\tau_s  \\frac{\\nu g_0^2 \\tau_s}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the diffusion approximation, which models synaptic input as a Gaussian process defined by its mean and variance, is powerful, it is not universally valid. This exercise  delves into the mathematical justification for this approximation and equips you with the tools to assess its limits. By calculating the third Kramers-Moyal coefficient and the corresponding third cumulant of the synaptic current, you will quantify the input's non-Gaussian nature and understand how its validity depends on network and synaptic parameters.",
            "id": "4051248",
            "problem": "Consider a single-compartment neuron receiving a large number of statistically independent synaptic inputs that can be aggregated into a single homogeneous Poisson point process (PPP) of rate $\\nu$. Each presynaptic spike causes a postsynaptic current jump of amplitude $J$ followed by an exponential decay with synaptic time constant $\\tau_s$. The total synaptic current $I(t)$ obeys the linear shot-noise dynamics\n$$\n\\frac{d I(t)}{dt} \\;=\\; -\\frac{I(t)}{\\tau_s} \\;+\\; J \\sum_k \\delta\\!\\left(t-t_k\\right),\n$$\nwhere $\\{t_k\\}$ are the Poisson event times. Equivalently, in differential form with respect to the counting process $N_t$ of the PPP,\n$$\ndI(t) \\;=\\; -\\frac{I(t)}{\\tau_s}\\,dt \\;+\\; J\\,dN_t,\n$$\nwith $\\mathbb{E}[dN_t] = \\nu\\,dt$.\n\nStarting from the definition of the Kramers–Moyal expansion (KME), the $n$-th Kramers–Moyal coefficient $a_n(I)$ is\n$$\na_n(I) \\;=\\; \\lim_{dt\\to 0} \\frac{1}{dt}\\,\\mathbb{E}\\!\\left[ \\big(\\Delta I\\big)^{n} \\,\\big|\\, I(t)=I \\right],\n$$\nwhere $\\Delta I \\equiv I(t+dt)-I(t)$.\n\n1) Using only the dynamics above and the properties of a homogeneous Poisson process in an infinitesimal interval, derive the first three Kramers–Moyal coefficients $a_1(I)$, $a_2(I)$, and $a_3(I)$.\n\n2) In the stationary regime, the synaptic current can be represented as the linear superposition\n$$\nI(t) \\;=\\; \\sum_{t_k  t} J \\exp\\!\\left(-\\frac{t-t_k}{\\tau_s}\\right).\n$$\nUsing well-tested results for Poisson shot noise, compute the first three equal-time cumulants $\\kappa_1$, $\\kappa_2$, and $\\kappa_3$ of $I(t)$.\n\n3) The diffusion approximation neglects third and higher Kramers–Moyal terms, hence an error indicator is the standardized third cumulant (skewness magnitude)\n$$\n\\gamma \\;\\equiv\\; \\frac{|\\kappa_3|}{\\kappa_2^{3/2}}.\n$$\nExpress $\\gamma$ as a closed-form analytic expression in terms of $J$, $\\nu$, and $\\tau_s$. Provide your final answer as a single symbolic expression. No numerical evaluation or rounding is required, and no units should be included in the final expression.",
            "solution": "We begin from the stochastic differential form\n$$\ndI(t) \\;=\\; -\\frac{I(t)}{\\tau_s}\\,dt \\;+\\; J\\,dN_t,\n$$\nwhere $N_t$ is the counting process of a homogeneous Poisson point process (PPP) with rate $\\nu$. Over an infinitesimal interval $[t,t+dt)$, the increment is\n$$\n\\Delta I \\;=\\; -\\frac{I}{\\tau_s}\\,dt \\;+\\; J\\,\\Delta N,\n$$\nwith $\\Delta N \\equiv N_{t+dt}-N_t$. For a homogeneous PPP, $\\mathbb{P}(\\Delta N=1)=\\nu\\,dt + o(dt)$, $\\mathbb{P}(\\Delta N=0)=1-\\nu\\,dt+o(dt)$, and $\\mathbb{P}(\\Delta N \\ge 2)=o(dt)$.\n\n1) Computation of the first three Kramers–Moyal coefficients.\n\nBy definition,\n$$\na_n(I) \\;=\\; \\lim_{dt\\to 0}\\frac{1}{dt}\\,\\mathbb{E}\\!\\left[ \\big(\\Delta I\\big)^n \\,\\big|\\, I(t)=I \\right].\n$$\nWe evaluate these up to terms $O(dt)$.\n\nFor $n=1$,\n$$\n\\mathbb{E}[\\Delta I \\mid I] \\;=\\; -\\frac{I}{\\tau_s}\\,dt \\;+\\; J\\,\\mathbb{E}[\\Delta N] \\;=\\; -\\frac{I}{\\tau_s}\\,dt \\;+\\; J\\,\\nu\\,dt,\n$$\nthus\n$$\na_1(I) \\;=\\; -\\frac{I}{\\tau_s} \\;+\\; J\\,\\nu.\n$$\n\nFor $n=2$, expand $\\Delta I = -\\frac{I}{\\tau_s}dt + J \\Delta N$. Products involving $dt$ and $\\Delta N$ are $O(dt^2)$ in expectation and can be neglected at leading order. The $O(dt)$ contribution arises from $(J\\Delta N)^2$. Since $\\Delta N^2 = \\Delta N$ for $\\Delta N \\in \\{0,1\\}$ to $O(dt)$,\n$$\n\\mathbb{E}\\!\\left[(\\Delta I)^2 \\mid I\\right] \\;=\\; J^2\\,\\mathbb{E}[\\Delta N^2] \\;+\\; o(dt) \\;=\\; J^2 \\nu\\,dt \\;+\\; o(dt),\n$$\nhence\n$$\na_2(I) \\;=\\; J^2 \\nu.\n$$\n\nFor $n=3$, by the same logic the leading $O(dt)$ contribution is $(J\\Delta N)^3$, and since $\\Delta N^3=\\Delta N$ for $\\Delta N\\in\\{0,1\\}$ to $O(dt)$,\n$$\n\\mathbb{E}\\!\\left[(\\Delta I)^3 \\mid I\\right] \\;=\\; J^3\\,\\mathbb{E}[\\Delta N^3] \\;+\\; o(dt) \\;=\\; J^3 \\nu\\,dt \\;+\\; o(dt),\n$$\nso\n$$\na_3(I) \\;=\\; J^3 \\nu.\n$$\n\nThus, up to third order,\n$$\na_1(I) \\;=\\; -\\frac{I}{\\tau_s} + J\\nu,\\qquad a_2(I) \\;=\\; J^2 \\nu,\\qquad a_3(I) \\;=\\; J^3 \\nu.\n$$\n\n2) Stationary equal-time cumulants of $I(t)$.\n\nIn steady state, $I(t)$ can be written as a Poisson shot noise\n$$\nI(t) \\;=\\; \\sum_{t_k  t} h(t-t_k),\\qquad h(s) \\;=\\; J \\exp\\!\\left(-\\frac{s}{\\tau_s}\\right)\\,\\mathbf{1}_{\\{s\\ge 0\\}}.\n$$\nFor a homogeneous PPP with rate $\\nu$ and deterministic kernel $h$, the $n$-th equal-time cumulant $\\kappa_n$ of $I(t)$ is given by the well-tested Campbell theorem for cumulants of Poisson shot noise:\n$$\n\\kappa_n \\;=\\; \\nu \\int_{0}^{\\infty} \\big(h(s)\\big)^n\\,ds.\n$$\nFor the exponential kernel,\n$$\n\\int_{0}^{\\infty} \\big(h(s)\\big)^n\\,ds \\;=\\; \\int_{0}^{\\infty} J^n \\exp\\!\\left(-\\frac{n s}{\\tau_s}\\right)\\,ds \\;=\\; J^n \\frac{\\tau_s}{n}.\n$$\nTherefore,\n$$\n\\kappa_n \\;=\\; \\nu\\,J^n\\,\\frac{\\tau_s}{n}.\n$$\nIn particular,\n$$\n\\kappa_1 \\;=\\; \\nu J \\tau_s,\\qquad \\kappa_2 \\;=\\; \\nu J^2 \\frac{\\tau_s}{2},\\qquad \\kappa_3 \\;=\\; \\nu J^3 \\frac{\\tau_s}{3}.\n$$\n\n3) Standardized third cumulant (skewness magnitude) as an error indicator.\n\nDefine\n$$\n\\gamma \\;\\equiv\\; \\frac{|\\kappa_3|}{\\kappa_2^{3/2}}.\n$$\nSubstituting the expressions above,\n$$\n\\gamma \\;=\\; \\frac{\\left|\\nu J^3 \\frac{\\tau_s}{3}\\right|}{\\left(\\nu J^2 \\frac{\\tau_s}{2}\\right)^{3/2}}\n\\;=\\; \\frac{|J|^3 \\nu \\tau_s / 3}{|J|^3 (\\nu \\tau_s)^{3/2} / 2^{3/2}}\n\\;=\\; \\frac{2^{3/2}}{3}\\,\\frac{1}{\\sqrt{\\nu \\tau_s}}.\n$$\nThus the diffusion approximation error indicator based on the smallness of the third cumulant is\n$$\n\\gamma \\;=\\; \\frac{2^{3/2}}{3}\\,(\\nu \\tau_s)^{-1/2}.\n$$\nNotably, the dependence on $J$ cancels in the standardized measure; only the product $\\nu \\tau_s$, which represents the number of synaptic events within a synaptic time constant, controls the magnitude. A larger $\\nu \\tau_s$ yields a smaller skewness and hence a better diffusion approximation.",
            "answer": "$$\\boxed{\\frac{2^{3/2}}{3}\\,(\\nu\\,\\tau_s)^{-1/2}}$$"
        },
        {
            "introduction": "With a grasp of the mean-field framework, we can now apply it to predict collective network behavior, such as the stability of the asynchronous state. This exercise  demonstrates how to perform a linear stability analysis on a classic excitatory-inhibitory network model. By calculating the Jacobian matrix of the linearized rate dynamics and examining its dominant eigenvalue, you will uncover how inhibitory self-coupling plays a crucial role in stabilizing network activity, a cornerstone principle in neuroscience.",
            "id": "3996543",
            "problem": "Consider a two-population excitatory-inhibitory network of leaky integrate-and-fire neurons in the asynchronous state. In the diffusion mean-field approximation, small perturbations $\\delta r_{E}(t)$ and $\\delta r_{I}(t)$ of the population-averaged firing rates around a fixed point $(r_{E}^{\\ast}, r_{I}^{\\ast})$ evolve according to a linear system that results from first-order Taylor expansion of the transfer functions around the fixed point. Let $g_{E}  0$ and $g_{I}  0$ denote the slope (gain) of the excitatory and inhibitory population transfer functions at $(r_{E}^{\\ast}, r_{I}^{\\ast})$, respectively, and let $\\tau_{E}  0$ and $\\tau_{I}  0$ be their effective rate time constants. Synaptic strengths are parameterized by positive magnitudes $J_{EE}  0$, $J_{EI}  0$, $J_{IE}  0$, and $J_{II}  0$, where the negative sign of inhibition is carried explicitly in the linearized dynamics. Specifically, the linearized mean-field dynamics are\n$$\n\\begin{aligned}\n\\delta \\dot{r}_{E}(t) = -\\frac{1}{\\tau_{E}} \\, \\delta r_{E}(t) + g_{E} \\left( J_{EE} \\, \\delta r_{E}(t) - J_{EI} \\, \\delta r_{I}(t) \\right), \\\\\n\\delta \\dot{r}_{I}(t) = -\\frac{1}{\\tau_{I}} \\, \\delta r_{I}(t) + g_{I} \\left( J_{IE} \\, \\delta r_{E}(t) - J_{II} \\, \\delta r_{I}(t) \\right).\n\\end{aligned}\n$$\nThese equations define the Jacobian matrix $\\mathbf{A}$ of the linearized system at the fixed point via $\\delta \\dot{\\mathbf{r}}(t) = \\mathbf{A} \\, \\delta \\mathbf{r}(t)$ with $\\delta \\mathbf{r} = (\\delta r_{E}, \\delta r_{I})^{\\top}$. Assume the parameter regime is such that the eigenvalues of $\\mathbf{A}$ are real (the discriminant of the characteristic polynomial is nonnegative). Using only the fundamental definition of linearization and eigenvalues for a $2 \\times 2$ matrix, calculate the effect of increasing the inhibitory self-coupling magnitude $J_{II}$ on the dominant eigenvalue (the larger real eigenvalue) of $\\mathbf{A}$ by deriving a closed-form analytic expression for $\\frac{\\partial \\lambda_{\\max}}{\\partial J_{II}}$ in terms of $(\\tau_{E}, \\tau_{I}, g_{E}, g_{I}, J_{EE}, J_{EI}, J_{IE}, J_{II})$. Your final answer must be a single analytical expression. Then, interpret the sign of your expression in terms of stabilization of the asynchronous fixed point. No numerical evaluation is required and no rounding is needed. Express the final derivative as a dimensionless symbolic expression.",
            "solution": "The linear system is given by $\\delta \\dot{\\mathbf{r}}(t) = \\mathbf{A} \\, \\delta \\mathbf{r}(t)$, where $\\delta \\mathbf{r} = (\\delta r_{E}, \\delta r_{I})^{\\top}$ and the Jacobian matrix $\\mathbf{A}$ is:\n$$\n\\mathbf{A} = \\begin{pmatrix} g_{E} J_{EE} - \\frac{1}{\\tau_{E}}  -g_{E} J_{EI} \\\\ g_{I} J_{IE}  -g_{I} J_{II} - \\frac{1}{\\tau_{I}} \\end{pmatrix}\n$$\nLet the elements be $A_{11}, A_{12}, A_{21}, A_{22}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\lambda^2 - \\text{Tr}(\\mathbf{A})\\lambda + \\det(\\mathbf{A}) = 0$. The solutions are:\n$$\n\\lambda = \\frac{\\text{Tr}(\\mathbf{A}) \\pm \\sqrt{\\text{Tr}(\\mathbf{A})^2 - 4\\det(\\mathbf{A})}}{2}\n$$\nThe dominant eigenvalue, $\\lambda_{\\max}$, corresponds to the '+' sign, as the eigenvalues are assumed to be real ($\\Delta = \\text{Tr}(\\mathbf{A})^2 - 4\\det(\\mathbf{A}) \\ge 0$):\n$$\n\\lambda_{\\max} = \\frac{1}{2} \\left( \\text{Tr}(\\mathbf{A}) + \\sqrt{\\Delta} \\right)\n$$\nWe compute the partial derivative $\\frac{\\partial \\lambda_{\\max}}{\\partial J_{II}}$. First, we find the derivatives of the trace and determinant with respect to $J_{II}$.\nThe trace is $\\text{Tr}(\\mathbf{A}) = A_{11} + A_{22} = g_{E} J_{EE} - \\frac{1}{\\tau_{E}} - g_{I} J_{II} - \\frac{1}{\\tau_{I}}$.\n$$\n\\frac{\\partial \\text{Tr}(\\mathbf{A})}{\\partial J_{II}} = -g_{I}\n$$\nThe determinant is $\\det(\\mathbf{A}) = A_{11}A_{22} - A_{12}A_{21}$. Only $A_{22}$ depends on $J_{II}$.\n$$\n\\frac{\\partial \\det(\\mathbf{A})}{\\partial J_{II}} = A_{11} \\frac{\\partial A_{22}}{\\partial J_{II}} = A_{11} (-g_I) = -g_I \\left( g_{E} J_{EE} - \\frac{1}{\\tau_{E}} \\right)\n$$\nThe derivative of the discriminant $\\Delta$ is:\n$$\n\\frac{\\partial \\Delta}{\\partial J_{II}} = 2\\text{Tr}(\\mathbf{A}) \\frac{\\partial \\text{Tr}(\\mathbf{A})}{\\partial J_{II}} - 4 \\frac{\\partial \\det(\\mathbf{A})}{\\partial J_{II}} = 2\\text{Tr}(\\mathbf{A})(-g_{I}) - 4(-g_{I} A_{11}) = -2g_{I}(\\text{Tr}(\\mathbf{A}) - 2A_{11})\n$$\nSince $\\text{Tr}(\\mathbf{A}) = A_{11} + A_{22}$, this simplifies to $\\frac{\\partial \\Delta}{\\partial J_{II}} = -2g_{I}(A_{22} - A_{11})$.\nAssembling the derivative of $\\lambda_{\\max}$:\n$$\n\\frac{\\partial \\lambda_{\\max}}{\\partial J_{II}} = \\frac{1}{2} \\left( \\frac{\\partial \\text{Tr}(\\mathbf{A})}{\\partial J_{II}} + \\frac{1}{2\\sqrt{\\Delta}} \\frac{\\partial \\Delta}{\\partial J_{II}} \\right) = \\frac{1}{2} \\left( -g_{I} + \\frac{-2g_{I}(A_{22} - A_{11})}{2\\sqrt{\\Delta}} \\right)\n$$\n$$\n\\frac{\\partial \\lambda_{\\max}}{\\partial J_{II}} = -\\frac{g_{I}}{2} \\left( 1 + \\frac{A_{22} - A_{11}}{\\sqrt{\\Delta}} \\right) = -\\frac{g_{I}}{2} \\left( 1 - \\frac{A_{11} - A_{22}}{\\sqrt{(A_{11} - A_{22})^2 + 4A_{12}A_{21}}} \\right)\n$$\nSubstituting the expressions for the matrix elements yields the final formula in the answer box.\n\n**Interpretation:**\nThe stability of the fixed point is determined by the sign of $\\lambda_{\\max}$. A negative derivative $\\frac{\\partial \\lambda_{\\max}}{\\partial J_{II}}$ means that increasing inhibitory self-coupling $J_{II}$ pushes the eigenvalue to more negative values, thus having a stabilizing effect. Conversely, a positive derivative implies a destabilizing effect.\n\nLet's analyze the sign of the derivative. All parameters ($g$, $J$, $\\tau$) are positive.\nThe term $4A_{12}A_{21} = 4(-g_E J_{EI})(g_I J_{IE}) = -4g_E g_I J_{EI} J_{IE}$ is strictly negative.\nLet $X = A_{11} - A_{22}$ and $P = -4A_{12}A_{21} > 0$. The derivative expression contains the term $S = 1 - \\frac{X}{\\sqrt{X^2 - P}}$. For real eigenvalues, we must have $X^2 - P \\ge 0$.\nThe term $Z = \\frac{X}{\\sqrt{X^2 - P}}$ satisfies $|Z| \\ge 1$. The sign of $S$ depends on the sign of $X$:\n- If $X = A_{11} - A_{22} > 0$, then $Z \\ge 1$, which implies $S \\le 0$. The derivative $\\frac{\\partial \\lambda_{\\max}}{\\partial J_{II}} = -\\frac{g_I}{2}S \\ge 0$. Increasing $J_{II}$ is **destabilizing**. This regime, where the excitatory sub-population is more unstable on its own than the inhibitory sub-population is stable, is common in so-called \"inhibition-stabilized networks\" (ISNs).\n- If $X = A_{11} - A_{22}  0$, then $Z \\le -1$, which implies $S \\ge 2 > 0$. The derivative $\\frac{\\partial \\lambda_{\\max}}{\\partial J_{II}} = -\\frac{g_I}{2}S  0$. Increasing $J_{II}$ is **stabilizing**.\n\nTherefore, the effect of inhibitory self-coupling is not universally stabilizing but depends on the other network parameters, specifically on whether the excitatory population's effective self-excitation ($A_{11}$) is stronger or weaker than the inhibitory population's effective self-damping ($A_{22}$).",
            "answer": "$$\n\\boxed{-\\frac{g_{I}}{2} \\left( 1 - \\frac{g_{E} J_{EE} + g_{I} J_{II} - \\frac{1}{\\tau_{E}} + \\frac{1}{\\tau_{I}}}{\\sqrt{\\left(g_{E} J_{EE} + g_{I} J_{II} - \\frac{1}{\\tau_{E}} + \\frac{1}{\\tau_{I}}\\right)^{2} - 4 g_{E} g_{I} J_{EI} J_{IE}}} \\right)}\n$$"
        }
    ]
}