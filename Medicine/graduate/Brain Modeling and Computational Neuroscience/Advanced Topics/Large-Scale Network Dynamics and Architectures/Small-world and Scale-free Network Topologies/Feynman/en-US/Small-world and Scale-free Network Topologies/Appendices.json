{
    "hands_on_practices": [
        {
            "introduction": "The small-world architecture, balancing high local clustering with short global path lengths, is a key organizing principle of brain networks. To quantitatively assess if a given network exhibits this property, its clustering coefficient, $C$, and characteristic path length, $L$, must be compared against those of benchmark null models. This exercise  provides direct practice in calculating the two most common small-world indices, $\\sigma$ and $\\omega$, which formalize the comparison to random and regular lattice networks, respectively, and are foundational tools in computational connectomics.",
            "id": "4018976",
            "problem": "A cortical structural connectome is modeled as an undirected, simple, connected network with $n$ nodes, where $n = 500$. The network is obtained from diffusion Magnetic Resonance Imaging (MRI), thresholded to retain a single connected component. Its global clustering coefficient $C$ and characteristic path length $L$ are measured using standard graph-theoretic definitions based on triadic closure and mean geodesic distance, respectively. To assess small-world organization, two null models are constructed:\n\n- A random null preserving the empirical degree sequence via the Configuration Model (CM), yielding baseline values $C_{\\mathrm{rand}}$ and $L_{\\mathrm{rand}}$.\n- A lattice null formed as a degree-matched ring-lattice embedding on a one-dimensional circle, yielding baseline values $C_{\\mathrm{latt}}$ and $L_{\\mathrm{latt}}$.\n\nFor the empirical network and its nulls, the following quantities are obtained:\n- $C = 0.42$\n- $L = 3.4$\n- $C_{\\mathrm{rand}} = 0.024$\n- $L_{\\mathrm{rand}} = 3.0$\n- $C_{\\mathrm{latt}} = 0.48$\n- $L_{\\mathrm{latt}} = 31.25$\n\nUsing fundamental definitions of clustering and path length and their interpretation relative to random and lattice baselines, compute the small-world indices $\\sigma$ and $\\omega$ for this network. Report both $\\sigma$ and $\\omega$ as numerical values rounded to four significant figures. Then, based on canonical criteria relating these indices to small-world organization, briefly interpret whether the network exhibits small-world topology. No physical units are required for the final numerical values.",
            "solution": "The problem is well-posed and scientifically grounded, providing all necessary data to compute and interpret two standard measures of small-world network topology.\n\nThe analysis begins by computing the small-world index $\\sigma$, as originally proposed by Watts and Strogatz. This index quantifies the small-world property by comparing the clustering and path length of an empirical network to those of a random network with a preserved degree sequence. The index is defined as the ratio of the normalized clustering coefficient, $\\gamma$, to the normalized characteristic path length, $\\lambda$.\n\nFirst, we define the normalized clustering coefficient, $\\gamma$, which is the ratio of the empirical network's clustering coefficient, $C$, to that of the random null model, $C_{\\mathrm{rand}}$.\n$$\n\\gamma = \\frac{C}{C_{\\mathrm{rand}}}\n$$\nUsing the provided values:\n$$\n\\gamma = \\frac{0.42}{0.024} = 17.5\n$$\nThis result indicates that the empirical network is $17.5$ times more clustered than a corresponding random network.\n\nNext, we define the normalized characteristic path length, $\\lambda$, which is the ratio of the empirical network's path length, $L$, to that of the random null model, $L_{\\mathrm{rand}}$.\n$$\n\\lambda = \\frac{L}{L_{\\mathrm{rand}}}\n$$\nUsing the provided values:\n$$\n\\lambda = \\frac{3.4}{3.0} \\approx 1.1333\n$$\nThis result shows that the empirical network's path length is only about $13.3\\%$ longer than that of a random graph.\n\nThe small-world index $\\sigma$ is then computed as:\n$$\n\\sigma = \\frac{\\gamma}{\\lambda} = \\frac{C/C_{\\mathrm{rand}}}{L/L_{\\mathrm{rand}}}\n$$\nSubstituting the calculated values of $\\gamma$ and $\\lambda$:\n$$\n\\sigma = \\frac{17.5}{3.4/3.0} = \\frac{17.5 \\times 3.0}{3.4} = \\frac{52.5}{3.4} \\approx 15.441176...\n$$\nRounding to four significant figures, we get $\\sigma = 15.44$.\n\nA second, more recent small-world index, $\\omega$, was developed to provide a measure bounded between $-1$ and $1$, comparing the empirical network to both random and regular (lattice) nulls simultaneously. The index $\\omega$ is defined as:\n$$\n\\omega = \\frac{L_{\\mathrm{rand}}}{L} - \\frac{C}{C_{\\mathrm{latt}}}\n$$\nThis formulation balances the deviation from a random network (in terms of path length) against the deviation from a lattice network (in terms of clustering).\n\nUsing the provided quantities:\n$$\n\\omega = \\frac{3.0}{3.4} - \\frac{0.42}{0.48}\n$$\nWe compute the two terms separately:\nThe first term is $L_{\\mathrm{rand}}/L = \\frac{3.0}{3.4} \\approx 0.88235...$\nThe second term is $C/C_{\\mathrm{latt}} = \\frac{0.42}{0.48} = \\frac{42}{48} = \\frac{7}{8} = 0.875$.\nSubtracting the second term from the first:\n$$\n\\omega \\approx 0.88235... - 0.875 = 0.00735...\n$$\nRounding to four significant figures, we get $\\omega = 0.007353$.\n\nInterpretation:\nA network is canonically considered to possess a small-world topology if it exhibits high clustering, similar to a regular lattice, and a short characteristic path length, similar to a random network.\nThe condition for the $\\sigma$ index is $\\sigma > 1$. In our case, $\\sigma \\approx 15.44$, which is substantially greater than $1$. This indicates that the network has a much higher degree of clustering relative to its path length when compared to a random network, a key feature of small-world organization.\nThe $\\omega$ index maps network topology onto a spectrum where values close to $1$ indicate a random-like network ($L \\approx L_{\\mathrm{rand}}$, $C \\ll C_{\\mathrm{latt}}$), values close to $-1$ indicate a lattice-like network ($L \\gg L_{\\mathrm{rand}}$, $C \\approx C_{\\mathrm{latt}}$), and values close to $0$ indicate a small-world network ($L \\approx L_{\\mathrm{rand}}$, $C \\approx C_{\\mathrm{latt}}$). Our calculated value of $\\omega \\approx 0.007353$ is very close to $0$, which strongly corroborates the finding from the $\\sigma$ index. The network is thus characterized as having a path length close to that of a random network while retaining clustering close to that of a regular lattice.\n\nBoth indices consistently demonstrate that the cortical structural connectome exhibits a prominent small-world topology.",
            "answer": "$$\\boxed{\\begin{pmatrix} 15.44  0.007353 \\end{pmatrix}}$$"
        },
        {
            "introduction": "The choice of a null model is a critical decision that defines the baseline for statistical inference in network science. While a simple random graph is a common starting point, a degree-preserving null model allows us to distinguish topological features that arise merely from the network's degree distribution—such as the presence of hubs—from true higher-order organization. This practice  challenges you to think critically about how to interpret metrics like the small-world index when using a degree-preserving null, especially in the context of brain networks which often exhibit heavy-tailed degree distributions.",
            "id": "4018995",
            "problem": "In a structural brain network modeled as an undirected simple graph $G = (V,E)$ with $|V| = n$ and $|E| = m$, let the degree sequence be $\\{k_i\\}_{i=1}^n$ where $k_i$ is the degree of node $i$. Define the local clustering coefficient of node $i$ by $C_i = \\frac{2 T_i}{k_i (k_i - 1)}$, where $T_i$ is the number of triangles that include node $i$, and define the global clustering coefficient by $C = \\frac{1}{n} \\sum_{i=1}^n C_i$. Define the characteristic path length $L$ as the average of the shortest-path distances over all unordered pairs of distinct nodes. Consider the degree-preserving randomization known as the Configuration Model (CM), which constructs a null ensemble by uniformly random stub-matching while preserving the degree sequence $\\{k_i\\}_{i=1}^n$. Denote by $C_{\\mathrm{deg}}$ and $L_{\\mathrm{deg}}$ the ensemble means of the global clustering coefficient and characteristic path length under this degree-preserving null, respectively. Define the Small-World Index (SWI) by\n$$\n\\sigma = \\frac{C/C_{\\mathrm{deg}}}{L/L_{\\mathrm{deg}}}.\n$$\nYou are told that the empirical degree sequence $\\{k_i\\}$ is heavy-tailed (often described as scale-free) such that hubs are present, and the network is sparse, meaning $m = O(n)$. Using first principles from random graph theory with given degree distributions and the definitions above, reason about how degree-preserving randomization changes higher-order structures and how to interpret the resulting small-world index. Which of the following statements are correct?\n\nA. Degree-preserving randomization generally reduces $C$ and drives motif counts (for example, $3$-node triangles and $4$-node feed-forward motifs) toward values consistent with edges being independently placed conditional on the degree sequence; thus $\\sigma$ computed against a degree-preserving null quantifies excess clustering beyond degree heterogeneity.\n\nB. Because the degree sequence $\\{k_i\\}$ is preserved, degree-preserving randomization exactly preserves both triangle counts and $C$, and therefore $\\sigma$ necessarily equals $1$.\n\nC. In heavy-tailed degree sequences, shortest paths are often dominated by hubs, so $L$ can remain close to $L_{\\mathrm{deg}}$ after degree-preserving randomization; therefore $\\sigma$ may be driven primarily by $C/C_{\\mathrm{deg}}$, and $\\sigma > 1$ indicates clustering arising from constraints beyond the degree sequence, such as spatial embedding or Hebbian learning.\n\nD. If the network is scale-free, computing $\\sigma$ relative to an Erdős–Rényi (ER) null $G(n,p)$ with the same edge density is effectively equivalent to using a degree-preserving null, so the choice of null does not affect interpretation.\n\nE. If $\\sigma  1$ under a degree-preserving null, it necessarily implies the network is not small-world in any scientifically meaningful sense, even if $L \\ll L_{\\mathrm{deg}}$.",
            "solution": "We begin from the definitions of the clustering coefficient, characteristic path length, and the properties of the Configuration Model (CM) null ensemble, which preserves the degree sequence $\\{k_i\\}_{i=1}^n$ while otherwise randomizing connectivity. We also use well-tested facts from random graph theory for graphs with given degree distributions.\n\nFirst, we examine how degree-preserving randomization affects clustering and motifs. The local clustering coefficient of node $i$ is\n$$\nC_i = \\frac{2 T_i}{k_i (k_i - 1)},\n$$\nwhere $T_i$ counts the number of triangles that include node $i$. In CM, edges are matched uniformly at random subject to the node degrees. Conditioned on the degree sequence, the approximate probability that two nodes $j$ and $k$ are connected can be written in the spirit of the Chung–Lu model as\n$$\n\\mathbb{P}\\big((j,k)\\in E\\big) \\approx \\frac{k_j k_k}{2m},\n$$\nwhere $m = |E|$. While CM and Chung–Lu differ in details, this approximation captures the correct scaling behavior for sparse, degree-heterogeneous networks. For the neighbors $N(i)$ of node $i$, the expected number of edges among them is\n$$\n\\mathbb{E}\\left[\\text{edges in } N(i)\\right] \\approx \\sum_{\\{j,k\\}\\subset N(i)} \\frac{k_j k_k}{2m}.\n$$\nTherefore, the expected local clustering is\n$$\n\\mathbb{E}[C_i] \\approx \\frac{2}{k_i (k_i - 1)} \\sum_{\\{j,k\\}\\subset N(i)} \\frac{k_j k_k}{2m} = \\frac{1}{m} \\cdot \\frac{\\sum_{\\{j,k\\}\\subset N(i)} k_j k_k}{\\binom{k_i}{2}}.\n$$\nIn sparse networks with $m = O(n)$, and in the absence of explicit triadic closure mechanisms, this expectation tends to be small and often scales as $O(1/n)$ when averaged over nodes (the exact prefactor depends on degree correlations, but the asymptotic decay remains). Consequently, CM randomization reduces $C$ relative to empirical networks that possess local dependencies, spatial embedding, or Hebbian mechanisms promoting triangles.\n\nSimilarly, for motif counts (such as $3$-node triangles or $4$-node feed-forward motifs), CM induces approximate independence of edges conditional on degrees. As a result, overrepresented motifs in empirical networks—arising from local wiring rules, spatial proximity, or learning—are driven toward the baseline expected from the degree sequence alone. Formally, the expected count of a motif that involves $r$ edges on a fixed $q$-node set in CM is approximately the sum over all $q$-tuples of the product of the $r$ edge probabilities, each of which is $O(1/n)$ in sparse regimes, leading to a strong reduction relative to an empirical network with structured dependencies. This establishes that degree-preserving randomization alters higher-order structures such as clustering and motifs even though it preserves $\\{k_i\\}$.\n\nNext, we consider the characteristic path length. For random graphs with a given degree distribution, a well-tested result is that the typical distance scales as\n$$\nL \\sim \\frac{\\ln n}{\\ln \\kappa},\n$$\nwhere\n$$\n\\kappa = \\frac{\\langle k^2 \\rangle - \\langle k \\rangle}{\\langle k \\rangle},\n$$\nand $\\langle k \\rangle$ and $\\langle k^2 \\rangle$ denote the first and second moments of the degree distribution. In heavy-tailed (often called scale-free) degree sequences, $\\langle k^2 \\rangle$ can be large, producing large $\\kappa$ and hence small $L$. Under CM randomization, the degree sequence is preserved, so $\\langle k \\rangle$, $\\langle k^2 \\rangle$, and thus $\\kappa$ remain essentially unchanged. Consequently, $L_{\\mathrm{deg}}$ computed on the CM ensemble is close to the empirical $L$ when shortest paths are dominated by hubs. In many brain networks, the empirical $L$ may be similar to, or even slightly larger or smaller than, $L_{\\mathrm{deg}}$ depending on spatial constraints; the key point is that preserving degrees largely preserves shortest-path scaling.\n\nFinally, we interpret the Small-World Index (SWI)\n$$\n\\sigma = \\frac{C/C_{\\mathrm{deg}}}{L/L_{\\mathrm{deg}}}.\n$$\nIf $L \\approx L_{\\mathrm{deg}}$, then\n$$\n\\sigma \\approx \\frac{C}{C_{\\mathrm{deg}}},\n$$\nso $\\sigma > 1$ indicates that clustering in the empirical network exceeds what is induced by the degree sequence alone, consistent with explicit local rules or spatial embedding. Conversely, $\\sigma  1$ indicates lower clustering than the degree-preserving baseline; however, small-worldness is not an absolute property independent of the choice of null: different nulls (for example, spatially embedded nulls or lattice baselines) can yield different interpretations. Therefore, $\\sigma  1$ under a degree-preserving null does not “necessarily” rule out small-world characteristics under other scientifically appropriate baselines; it simply indicates that, relative to the degree-preserving null, clustering is not enhanced.\n\nWe now evaluate each option:\n\nA. Degree-preserving randomization generally reduces $C$ and drives motif counts toward degree-conditioned independence, and $\\sigma$ quantifies excess clustering beyond the degree sequence. This follows from the CM expectation for clustering and motif counts and the definition of $\\sigma$. Verdict: Correct.\n\nB. Preserving $\\{k_i\\}$ does not preserve triangle counts or $C$; CM randomization removes local dependencies and typically reduces $C$, so $\\sigma$ need not equal $1$. Verdict: Incorrect.\n\nC. Heavy-tailed degrees lead to hub-dominated shortest paths; CM preserves degrees, so $L$ can remain close to $L_{\\mathrm{deg}}$, making $\\sigma$ largely reflect $C/C_{\\mathrm{deg}}$. Interpreting $\\sigma > 1$ as clustering beyond degree heterogeneity is consistent with spatial or learning constraints. Verdict: Correct.\n\nD. An Erdős–Rényi null $G(n,p)$ matches edge density but not the degree sequence; in heavy-tailed networks the choice of null strongly affects $C_{\\mathrm{null}}$ and $L_{\\mathrm{null}}$. ER and CM are not equivalent, and the interpretation of $\\sigma$ depends on the null. Verdict: Incorrect.\n\nE. $\\sigma  1$ under a degree-preserving null does not “necessarily” imply no small-world characteristics under any scientifically meaningful sense, because small-worldness depends on the baseline. Different null ensembles (for example, spatial or lattice baselines) can produce different conclusions. Verdict: Incorrect.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "A network's static topology places strong constraints on its dynamic processes, including communication and information integration. The small-world architecture is believed to be ubiquitous in the brain precisely because its blend of local and long-range connections facilitates efficient signaling. In this exercise , you will model information flow as a random walk and compute the Mean First Passage Time (MFPT) between network modules, demonstrating quantitatively how the introduction of a few long-range \"shortcuts\" can dramatically reduce communication time, thereby providing a dynamic basis for the functional advantages of small-world structure.",
            "id": "4019006",
            "problem": "Consider a brain-inspired modular network constructed using the Watts–Strogatz (WS) small-world model. Let there be $n$ nodes arranged on a ring, and let each node connect to $k/2$ nearest neighbors on either side, resulting in degree $k$ per node. Each undirected edge originating from node $i$ to a forward neighbor is rewired independently with probability $p$ to connect node $i$ to a uniformly chosen node $m \\neq i$ that is not already a neighbor of $i$, preserving the degree $k$ of node $i$. Assume no self-loops and no multi-edges are created.\n\nDefine two disjoint modules, a source module $\\mathcal{S}$ and a target module $\\mathcal{T}$, by index sets on the ring:\n- $\\mathcal{S} = \\{0, 1, \\dots, m - 1\\}$,\n- $\\mathcal{T} = \\{\\mathrm{offset}, \\mathrm{offset} + 1, \\dots, \\mathrm{offset} + m - 1\\}$,\nwith the constraint that $\\mathrm{offset} + m \\le n$ so that $\\mathcal{S} \\cap \\mathcal{T} = \\emptyset$.\n\nConsider a simple random walk (RW) on this undirected graph: at each time step, a walker at node $i$ moves to one of its neighbors chosen uniformly at random. Define the Mean First Passage Time (MFPT) as the expected number of RW steps required to reach any node in the target module $\\mathcal{T}$ starting from a node in the source module $\\mathcal{S}$, where the initial node is drawn uniformly from $\\mathcal{S}$. To properly encode the absorbing nature of the target module, treat all nodes in $\\mathcal{T}$ as absorbing states of the RW (once the RW enters $\\mathcal{T}$, it remains there).\n\nYour task is to compute the MFPT between the modules $\\mathcal{S}$ and $\\mathcal{T}$ for the specified WS networks. Throughout, model the RW as a time-homogeneous discrete-time Markov chain with state space given by the nodes of the graph and transition probabilities determined by the graph's adjacency and degrees. Use scientifically sound methods for computing absorption times in absorbing Markov chains, starting from the foundational definitions of Markov chains and random walks. Do not assume any special formulas beyond these foundations without proper derivation.\n\nInterpretation requirement: Although the final program output is numerical, the solution must explain how the computed MFPT relates to cross-module information integration in small-world networks. Specifically, connect how the presence of long-range shortcuts (as controlled by $p$) and local degree $k$ impacts the MFPT and thus the efficiency of inter-module communication.\n\nUse the following test suite of parameter sets $(n, k, p, m, \\mathrm{offset}, \\mathrm{seed})$ to generate deterministic networks and compute MFPT values:\n1. Baseline lattice: $(n = 100, k = 4, p = 0.0, m = 10, \\mathrm{offset} = 50, \\mathrm{seed} = 42)$.\n2. Small-world regime: $(n = 100, k = 4, p = 0.1, m = 10, \\mathrm{offset} = 50, \\mathrm{seed} = 43)$.\n3. Increased randomness: $(n = 100, k = 4, p = 0.5, m = 10, \\mathrm{offset} = 50, \\mathrm{seed} = 44)$.\n4. Random graph limit: $(n = 100, k = 4, p = 1.0, m = 10, \\mathrm{offset} = 50, \\mathrm{seed} = 45)$.\n5. Near modules (short ring distance): $(n = 100, k = 4, p = 0.1, m = 10, \\mathrm{offset} = 12, \\mathrm{seed} = 46)$.\n6. Point-to-module: $(n = 100, k = 4, p = 0.1, m = 1, \\mathrm{offset} = 50, \\mathrm{seed} = 47)$.\n7. Higher local degree: $(n = 100, k = 8, p = 0.1, m = 10, \\mathrm{offset} = 50, \\mathrm{seed} = 48)$.\n\nFor each case, build the WS graph using the specified parameters and random seed, construct the RW transition matrix, and compute the MFPT from $\\mathcal{S}$ to $\\mathcal{T}$ by treating $\\mathcal{T}$ as absorbing. Average the first passage time over all starting nodes in $\\mathcal{S}$ with a uniform initial distribution over $\\mathcal{S}$. Express each MFPT as a float rounded to six decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[result1,result2,result3]`). The list must be in the same order as the test suite above. There are no physical units in this problem. All probabilities must be treated as decimals (not percentages), and angles are not involved in this task.",
            "solution": "The problem requires the computation of the Mean First Passage Time (MFPT) for a random walk between two specified modules, a source module $\\mathcal{S}$ and a target module $\\mathcal{T}$, within a network constructed using the Watts-Strogatz (WS) model. The solution involves three main stages: generating the WS network, modeling the random walk as an absorbing Markov chain, and deriving the MFPT from the properties of this chain.\n\n**1. Watts-Strogatz Network Generation**\n\nThe network is constructed on a set of $n$ nodes. The process begins with a regular ring lattice and then introduces randomness.\n\nFirst, a regular graph is created where each of the $n$ nodes is connected to its $k$ nearest neighbors, $k/2$ on each side along the ring. This initial structure is a lattice characterized by high clustering and long path lengths. The adjacency list $\\text{adj}$ represents this graph, where $\\text{adj}[i]$ contains the neighbors of node $i$.\n\nSecond, randomness is introduced via edge rewiring. For each node $i \\in \\{0, 1, \\dots, n-1\\}$, we consider its edges to its $k/2$ clockwise neighbors, as they were in the original lattice. Each of these edges is rewired with a given probability $p$. The rewiring of an edge $(i, j)$ consists of removing this edge and adding a new edge $(i, m)$, where $m$ is a node chosen uniformly at random from all possible nodes that are not $i$ and are not already connected to $i$. This process ensures no self-loops or multiple edges are formed. The parameter $p$ controls the network topology:\n-   If $p=0$, the network remains a regular lattice.\n-   If $p=1$, the network becomes a random graph (with a nearly preserved degree distribution from the initial lattice).\n-   For small, non-zero values of $p$ (e.g., $p \\approx 0.1$), the network enters the \"small-world\" regime, possessing both high clustering like a lattice and short average path lengths like a random graph. These long-range \"shortcuts\" are crucial for efficient information transfer.\n\nA specific random seed, $\\mathrm{seed}$, is used to ensure the deterministic and reproducible generation of the random aspects of the network.\n\n**2. Random Walk as an Absorbing Markov Chain**\n\nA simple random walk on the generated undirected graph is a discrete-time stochastic process. The state of the process at any time is the current node of the walker. At each time step, the walker moves from its current node $i$ to one of its neighbors $j$ with a transition probability $P_{ij} = 1 / d_i$, where $d_i$ is the degree (number of neighbors) of node $i$. For non-neighbor nodes $j'$, $P_{ij'} = 0$.\n\nThe problem specifies that the target module $\\mathcal{T}$ is absorbing. This means once the walker enters any node in $\\mathcal{T}$, it remains there. This transforms the random walk into an absorbing Markov chain. The state space $V = \\{0, 1, \\dots, n-1\\}$ is partitioned into two sets:\n-   The set of transient states $V' = V \\setminus \\mathcal{T}$, which can be left.\n-   The set of absorbing states $\\mathcal{T}$, which cannot be left.\n\nThe transition probability matrix $P$ of this Markov chain can be arranged in a canonical block form:\n$$\nP = \\begin{pmatrix} Q  R \\\\ 0  I \\end{pmatrix}\n$$\nHere, $Q$ is an $N_t \\times N_t$ submatrix describing transitions between transient states, where $N_t = |V'|$. $R$ is an $N_t \\times N_a$ submatrix for transitions from transient to absorbing states, where $N_a = |\\mathcal{T}|$. $I$ is the $N_a \\times N_a$ identity matrix, representing the fact that the walker stays in an absorbing state, and $0$ is a matrix of zeros.\n\n**3. Mean First Passage Time Calculation**\n\nThe MFPT from a starting node $i \\in \\mathcal{S}$ to the target module $\\mathcal{T}$ is the expected number of steps to first reach any node in $\\mathcal{T}$. Let $t_i$ be this expected time for a starting transient state $i \\in V'$. By conditioning on the first step of the walk, we can establish a system of linear equations for all $t_i$. The expected time from $i$ is one step plus the expected remaining time from the next state.\n$$\nt_i = 1 + \\sum_{j \\in V} P_{ij} t_j\n$$\nSince for any absorbing state $k \\in \\mathcal{T}$, the time to absorption is $t_k=0$, the sum can be restricted to transient states:\n$$\nt_i = 1 + \\sum_{j \\in V'} P_{ij} t_j\n$$\nThis forms a system of $N_t$ linear equations, one for each transient state $i \\in V'$. In matrix notation, this system is written as:\n$$\n\\mathbf{t} = \\mathbf{1} + Q \\mathbf{t}\n$$\nwhere $\\mathbf{t}$ is the column vector of the $t_i$ values and $\\mathbf{1}$ is a column vector of ones. Rearranging the equation gives:\n$$\n(I - Q) \\mathbf{t} = \\mathbf{1}\n$$\nThe solution for the vector of expected absorption times is found by inverting the matrix $(I - Q)$:\n$$\n\\mathbf{t} = (I - Q)^{-1} \\mathbf{1}\n$$\nThe matrix $N = (I - Q)^{-1}$ is known as the fundamental matrix of the absorbing Markov chain. The entry $N_{ij}$ represents the expected number of times the process is in transient state $j$ given it started in transient state $i$. The total expected number of steps before absorption, $t_i$, is the sum of expected visits to all transient states, which is $\\sum_j N_{ij}$. This is precisely the $i$-th element of the vector $N\\mathbf{1}$.\n\nThe final step is to average these expected times over all possible starting nodes in the source module $\\mathcal{S}$, assuming a uniform initial distribution:\n$$\n\\text{MFPT} = \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} t_i\n$$\n\n**Interpretation in the Context of Brain Networks**\n\nThis calculation provides insight into information integration efficiency in modular networks, a concept central to computational neuroscience. The random walker simulates a signal or piece of information propagating through the network. The MFPT measures how quickly, on average, a signal originating in the source module $\\mathcal{S}$ can reach the target module $\\mathcal{T}$.\n\n-   For $p=0$ (lattice), the walker moves diffusively. The MFPT is high, especially for distant modules, reflecting poor integration.\n-   For small $p > 0$ (small-world), the introduction of even a few long-range shortcuts drastically reduces the MFPT. These shortcuts allow the walker to bypass long lattice paths, enabling efficient communication between otherwise segregated modules. This balance of local connectivity (high clustering) and global reach (short path length) is a hallmark of many real-world networks, including the brain's structural and functional connectomes.\n-   Increasing the rewiring probability $p$ further continues to decrease path lengths, but diminishes the local, module-like structure.\n-   Increasing the local degree $k$ provides more pathways for the walker, generally decreasing the MFPT and enhancing communication.\n-   The effect of the ring distance `offset` is most pronounced for small $p$. As $p$ increases, the network's geometry is increasingly dominated by random shortcuts, making the physical distance on the initial ring less relevant to the traversal time.",
            "answer": "```python\nimport numpy as np\n\ndef generate_ws_graph(n, k, p, seed):\n    \"\"\"\n    Generates a Watts-Strogatz small-world graph.\n\n    Args:\n        n (int): Number of nodes.\n        k (int): Each node is connected to k nearest neighbors.\n        p (float): The probability of rewiring each edge.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        list[list[int]]: An adjacency list representation of the graph.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Store adjacency list as sets for efficient addition/removal\n    adj = [set() for _ in range(n)]\n    \n    # 1. Create a regular ring lattice\n    for i in range(n):\n        for j in range(1, k // 2 + 1):\n            neighbor = (i + j) % n\n            adj[i].add(neighbor)\n            adj[neighbor].add(i)\n\n    # 2. Rewire edges\n    # Iterate over each node and its original clockwise edges to consider for rewiring\n    for i in range(n):\n        original_cw_neighbors = [(i + j) % n for j in range(1, k // 2 + 1)]\n\n        for j in original_cw_neighbors:\n            if rng.random()  p:\n                # The set of forbidden new targets for node i: itself and its current neighbors.\n                # This must be re-evaluated for each rewiring as adj changes.\n                current_neighbors = adj[i]\n                \n                # The set of valid new targets for the rewired edge.\n                possible_targets = [node for node in range(n) if node != i and node not in current_neighbors]\n                \n                if not possible_targets:\n                    # Very unlikely for the given parameters, but a safe check.\n                    continue\n\n                # Choose a new target node 'm' uniformly at random.\n                m = rng.choice(possible_targets)\n                \n                # Check if edge (i,j) still exists before removing\n                # (it could have been removed by a previous rewiring from node j's perspective)\n                if j in adj[i]:\n                    # Remove old undirected edge (i, j)\n                    adj[i].remove(j)\n                    adj[j].remove(i)\n                \n                    # Add new undirected edge (i, m)\n                    adj[i].add(m)\n                    adj[m].add(i)\n                \n    # Convert sets to lists for the final adjacency list.\n    adj_list = [sorted(list(s)) for s in adj]\n    return adj_list\n\ndef compute_mfpt(adj, n, m_size, offset):\n    \"\"\"\n    Computes the Mean First Passage Time (MFPT) from a source to a target module.\n\n    Args:\n        adj (list[list[int]]): Adjacency list of the graph.\n        n (int): Number of nodes in the graph.\n        m_size (int): Size of the source and target modules.\n        offset (int): Starting index of the target module.\n\n    Returns:\n        float: The computed MFPT.\n    \"\"\"\n    # Define source and target modules as per problem description\n    source_module = set(range(m_size))\n    target_module = set(range(offset, offset + m_size))\n    \n    # Identify transient states (all nodes not in the target module)\n    all_nodes = set(range(n))\n    transient_states = sorted(list(all_nodes - target_module))\n    N_t = len(transient_states)\n    \n    # Map original transient node indices to matrix indices (0 to N_t-1)\n    node_to_idx = {node: i for i, node in enumerate(transient_states)}\n    \n    # Build the Q submatrix of the transition matrix P\n    # Q contains probabilities of transitioning between transient states\n    Q = np.zeros((N_t, N_t), dtype=np.float64)\n    \n    for i_orig in transient_states:\n        i_new = node_to_idx[i_orig]\n        neighbors = adj[i_orig]\n        degree = len(neighbors)\n        if degree == 0:\n            continue\n            \n        for j_orig in neighbors:\n            if j_orig in transient_states:\n                j_new = node_to_idx[j_orig]\n                Q[i_new, j_new] = 1.0 / degree\n    \n    # Compute the fundamental matrix N = (I - Q)^-1\n    try:\n        I = np.identity(N_t, dtype=np.float64)\n        N = np.linalg.inv(I - Q)\n    except np.linalg.LinAlgError:\n        # This case suggests a disconnected component of transient states\n        # not leading to absorption, resulting in infinite FPT.\n        return float('inf')\n\n    # The vector of expected absorption times for each transient start state\n    # is the sum of each row of the fundamental matrix.\n    t = N.sum(axis=1)\n    \n    # Average the first passage times over all starting nodes in the source module\n    fpt_sum = 0.0\n    num_source_nodes = 0\n    for s_node in source_module:\n        # All nodes in S are transient, as S and T are disjoint\n        if s_node in node_to_idx:\n            s_idx = node_to_idx[s_node]\n            fpt_sum += t[s_idx]\n            num_source_nodes += 1\n            \n    if num_source_nodes == 0:\n        return 0.0\n        \n    mfpt = fpt_sum / num_source_nodes\n    return mfpt\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (n, k, p, m, offset, seed)\n        (100, 4, 0.0, 10, 50, 42),\n        (100, 4, 0.1, 10, 50, 43),\n        (100, 4, 0.5, 10, 50, 44),\n        (100, 4, 1.0, 10, 50, 45),\n        (100, 4, 0.1, 10, 12, 46),\n        (100, 4, 0.1, 1, 50, 47),\n        (100, 8, 0.1, 10, 50, 48),\n    ]\n\n    results = []\n    for params in test_cases:\n        n, k, p, m, offset, seed = params\n        adj = generate_ws_graph(n, k, p, seed)\n        mfpt = compute_mfpt(adj, n, m, offset)\n        results.append(f\"{mfpt:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}