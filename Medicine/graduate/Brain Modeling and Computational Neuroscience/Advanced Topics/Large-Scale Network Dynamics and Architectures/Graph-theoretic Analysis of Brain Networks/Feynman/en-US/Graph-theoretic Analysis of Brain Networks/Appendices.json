{
    "hands_on_practices": [
        {
            "introduction": "When analyzing weighted brain networks, a common first step is to create a binary graph by applying a threshold. This practice demonstrates why the choice of thresholding method is not trivial; it can profoundly influence topological metrics and potentially lead to spurious conclusions when comparing groups. By implementing and contrasting global versus proportional thresholding, you will gain hands-on experience with the critical need to control for network density to ensure that observed group differences in metrics like clustering and efficiency reflect genuine topological changes, not simple variations in connection strength .",
            "id": "3985524",
            "problem": "You are given undirected, weighted brain networks represented by adjacency matrices derived from pairwise functional or structural connectivity among regions. Consider a network $\\mathcal{G}=(V,E,w)$ with $|V|=N$ nodes and nonnegative, symmetric weights $w_{ij}=w_{ji}\\ge 0$, $w_{ii}=0$. Two thresholding strategies are commonly used to binarize such weighted graphs for topological analysis: (i) global thresholding, which retains edges whose weight exceeds a fixed absolute threshold, and (ii) proportional thresholding, which retains a fixed proportion of the strongest edges, thereby matching density across graphs.\n\nStarting from the fundamental definitions of graph density and standard topological measures, implement both thresholding strategies and quantify how differences across groups can be confounded by density. Your program must compute the following for each group and thresholding method, and then compare between groups:\n\n1. Density $\\rho$ of the resulting binary graph, defined for undirected simple graphs as\n$$\n\\rho = \\frac{m}{\\frac{N(N-1)}{2}},\n$$\nwhere $m$ is the number of edges present after thresholding.\n\n2. Giant component fraction $g$, defined as\n$$\ng = \\frac{|\\text{largest connected component}|}{N}.\n$$\n\n3. Average local clustering coefficient $\\bar{C}$ for the binary graph, where the local clustering coefficient at node $i$ with degree $k_i$ is\n$$\nC_i = \n\\begin{cases}\n\\frac{2 T_i}{k_i (k_i - 1)}, & \\text{if } k_i \\ge 2,\\\\\n0, & \\text{if } k_i < 2,\n\\end{cases}\n$$\nand $T_i$ is the number of edges among the neighbors of $i$. The average is\n$$\n\\bar{C} = \\frac{1}{N} \\sum_{i=1}^N C_i.\n$$\n\n4. Global efficiency (GE), denoted $E_{\\mathrm{glob}}$, defined for a binary graph using unweighted shortest path lengths $d(i,j)$ as\n$$\nE_{\\mathrm{glob}} = \\frac{2}{N(N-1)} \\sum_{1 \\le i < j \\le N} \\frac{1}{d(i,j)},\n$$\nwith the convention that $\\frac{1}{d(i,j)}=0$ for disconnected pairs.\n\nThresholding strategies to implement:\n\n- Global thresholding at a specified absolute threshold $\\theta$: produce a binary adjacency matrix $A^{(\\mathrm{global})}$ with entries\n$$\nA^{(\\mathrm{global})}_{ij} =\n\\begin{cases}\n1, & \\text{if } w_{ij} \\ge \\theta \\text{ and } i \\ne j,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\n- Proportional thresholding to achieve a specified target density $d$: from all off-diagonal pairs $(i,j)$ with $i<j$, select the top $K$ edges by weight, where\n$$\nK = \\left\\lfloor \\frac{N(N-1)}{2} \\cdot d + \\frac{1}{2} \\right\\rfloor,\n$$\nwhich is rounding to the nearest integer. When ties occur in weights, break ties deterministically by lexicographic order on $(i,j)$, prioritizing smaller $i$ then smaller $j$. The resulting binary adjacency $A^{(\\mathrm{prop})}$ includes those $K$ edges and is symmetric.\n\nFor each test case, compute the metrics for Group A and Group B under both thresholding strategies. Then compute absolute differences between Group A and Group B for each measure under each strategy:\n- $\\Delta \\rho_{\\mathrm{global}}$, $\\Delta \\rho_{\\mathrm{prop}}$;\n- $\\Delta E_{\\mathrm{global}}$, $\\Delta E_{\\mathrm{prop}}$;\n- $\\Delta \\bar{C}_{\\mathrm{global}}$, $\\Delta \\bar{C}_{\\mathrm{prop}}$;\n- $\\Delta g_{\\mathrm{global}}$, $\\Delta g_{\\mathrm{prop}}$.\n\nFinally, for each test case, compute a boolean flag $\\mathrm{prop\\_better}$ which is $\\mathrm{True}$ if\n$$\n\\left(|\\Delta E_{\\mathrm{prop}}| + |\\Delta \\bar{C}_{\\mathrm{prop}}| + |\\Delta g_{\\mathrm{prop}}|\\right) < \\left(|\\Delta E_{\\mathrm{global}}| + |\\Delta \\bar{C}_{\\mathrm{global}}| + |\\Delta g_{\\mathrm{global}}|\\right),\n$$\nand $\\mathrm{False}$ otherwise. This formalizes the argument for density-matched comparisons: if proportional thresholding reduces discrepancies in topological measures relative to global thresholding, then it mitigates density-driven confounds across groups.\n\nAngle units do not apply. No physical units are involved. Densities and proportions must be expressed as decimals.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this top-level list must be the string representation of a list for the corresponding test case in the following order:\n$$\n[\\Delta \\rho_{\\mathrm{global}}, \\Delta \\rho_{\\mathrm{prop}}, \\Delta E_{\\mathrm{global}}, \\Delta E_{\\mathrm{prop}}, \\Delta \\bar{C}_{\\mathrm{global}}, \\Delta \\bar{C}_{\\mathrm{prop}}, \\Delta g_{\\mathrm{global}}, \\Delta g_{\\mathrm{prop}}, \\mathrm{prop\\_better}].\n$$\n\nTest suite and parameters:\n\nAll test cases use $N=6$ nodes labeled $0,1,2,3,4,5$. Adjacency is given via upper-triangular edge lists $(i,j,w_{ij})$ for $i<j$, with symmetric completion. Group B is either obtained by scaling Group A or explicitly specified.\n\n- Test Case $1$ (happy path, cross-group weight scaling creates density mismatch under global thresholding):\n    - Group A edges:\n        - $(0,1,$ $0.90)$, $(0,2,$ $0.75)$, $(1,2,$ $0.85)$,\n        - $(3,4,$ $0.88)$, $(3,5,$ $0.80)$, $(4,5,$ $0.77)$,\n        - $(0,3,$ $0.40)$, $(0,4,$ $0.45)$, $(0,5,$ $0.42)$,\n        - $(1,3,$ $0.50)$, $(1,4,$ $0.48)$, $(1,5,$ $0.44)$,\n        - $(2,3,$ $0.38)$, $(2,4,$ $0.41)$, $(2,5,$ $0.43)$.\n    - Group B is Group A scaled by $\\alpha=$ $1.50$ (i.e., $w^{(B)}_{ij} = \\alpha \\cdot w^{(A)}_{ij}$).\n    - Global threshold $\\theta=$ $0.55$.\n    - Proportional target density $d=$ $0.50$.\n\n- Test Case $2$ (boundary condition, high global threshold yields fragmentation in Group A):\n    - Group A edges:\n        - $(0,1,$ $0.82)$, $(0,2,$ $0.79)$, $(1,2,$ $0.81)$,\n        - $(3,4,$ $0.83)$, $(3,5,$ $0.80)$, $(4,5,$ $0.78)$,\n        - $(0,3,$ $0.25)$, $(0,4,$ $0.30)$, $(0,5,$ $0.28)$,\n        - $(1,3,$ $0.26)$, $(1,4,$ $0.27)$, $(1,5,$ $0.29)$,\n        - $(2,3,$ $0.24)$, $(2,4,$ $0.31)$, $(2,5,$ $0.23)$.\n    - Group B is Group A scaled by $\\alpha=$ $1.20$.\n    - Global threshold $\\theta=$ $0.80$.\n    - Proportional target density $d=$ $0.20$.\n\n- Test Case $3$ (edge case with many tie weights; deterministic tie-breaking required):\n    - Group A edges (many equal to $0.60$):\n        - $(0,1,$ $0.65)$, $(0,2,$ $0.60)$, $(1,2,$ $0.65)$,\n        - $(3,4,$ $0.60)$, $(3,5,$ $0.65)$, $(4,5,$ $0.60)$,\n        - $(0,3,$ $0.60)$, $(0,4,$ $0.60)$, $(0,5,$ $0.60)$,\n        - $(1,3,$ $0.60)$, $(1,4,$ $0.60)$, $(1,5,$ $0.60)$,\n        - $(2,3,$ $0.60)$, $(2,4,$ $0.60)$, $(2,5,$ $0.60)$.\n    - Group B edges, same as Group A except add $\\epsilon=$ $0.01$ to $(1,5)$, $(2,4)$, $(2,5)$:\n        - $(0,1,$ $0.65)$, $(0,2,$ $0.60)$, $(1,2,$ $0.65)$,\n        - $(3,4,$ $0.60)$, $(3,5,$ $0.65)$, $(4,5,$ $0.60)$,\n        - $(0,3,$ $0.60)$, $(0,4,$ $0.60)$, $(0,5,$ $0.60)$,\n        - $(1,3,$ $0.60)$, $(1,4,$ $0.60)$, $(1,5,$ $0.61)$,\n        - $(2,3,$ $0.60)$, $(2,4,$ $0.61)$, $(2,5,$ $0.61)$.\n    - Global threshold $\\theta=$ $0.60$.\n    - Proportional target density $d=$ $0.40$.\n\n- Test Case $4$ (control, identical groups; global threshold by coincidence yields matched density):\n    - Group A edges identical to Test Case $1$ Group A.\n    - Group B edges identical to Group A.\n    - Global threshold $\\theta=$ $0.45$.\n    - Proportional target density $d=$ $0.60$.\n\nYour task is to produce a complete, runnable program that constructs these graphs, applies both thresholding strategies per test case, computes the metrics for Group A and Group B, computes the absolute differences for each measure, and returns for each test case the list\n$$\n[\\Delta \\rho_{\\mathrm{global}}, \\Delta \\rho_{\\mathrm{prop}}, \\Delta E_{\\mathrm{global}}, \\Delta E_{\\mathrm{prop}}, \\Delta \\bar{C}_{\\mathrm{global}}, \\Delta \\bar{C}_{\\mathrm{prop}}, \\Delta g_{\\mathrm{global}}, \\Delta g_{\\mathrm{prop}}, \\mathrm{prop\\_better}],\n$$\naggregated into a single line as a comma-separated list enclosed in square brackets, for example:\n$$\n[\\text{case1\\_result},\\text{case2\\_result},\\text{case3\\_result},\\text{case4\\_result}],\n$$\nwhere each $\\text{caseX\\_result}$ is itself a list in the order specified above.",
            "solution": "The user wants to implement a program to analyze the effect of graph thresholding strategies on network metrics. This problem is well-defined, scientifically grounded in computational neuroscience, and provides all necessary information for a complete solution. It requires the implementation of standard graph-theoretic measures and two common thresholding techniques.\n\nThe validation steps confirm the problem is valid:\n1.  **Givens Extraction**: All data, variables, definitions, and conditions have been extracted. This includes network parameters ($N=6$), edge weights for four test cases, thresholding parameters ($\\theta, d$), and explicit mathematical formulas for density ($\\rho$), giant component fraction ($g$), average local clustering coefficient ($\\bar{C}$), and global efficiency ($E_{\\mathrm{glob}}$). The procedure for comparing groups and determining the `prop_better` flag is also clearly defined.\n2.  **Validation Check**: The problem is scientifically sound, addressing a genuine methodological issue in network analysis. It is well-posed, with deterministic rules for graph construction, thresholding (including tie-breaking), and metric calculation, ensuring a unique solution. The language is objective and precise. The problem is complete, consistent, and computationally feasible. It is a substantive computational task, not trivial or tautological.\n3.  **Verdict**: The problem is valid.\n\nThe solution will be structured as follows:\n\nFirst, a function `build_adjacency_matrix` will construct a weighted, symmetric adjacency matrix $W$ from the provided list of upper-triangular edges for $N=6$ nodes.\n\nSecond, two functions will implement the thresholding strategies:\n-   `global_threshold(W, theta)` will apply an absolute weight threshold $\\theta$ to $W$ to produce a binary adjacency matrix $A$.\n-   `proportional_threshold(W, d)` will retain a fraction $d$ of the strongest edges. It will calculate the number of edges $K = \\lfloor \\frac{N(N-1)}{2} \\cdot d + 0.5 \\rfloor$. Edges will be sorted in descending order of weight. Ties in weight will be broken deterministically by lexicographical order of node indices $(i, j)$ with $i<j$. The top $K$ edges will form the binary adjacency matrix $A$.\n\nThird, a function `compute_metrics(A)` will calculate the four required topological measures from a binary adjacency matrix $A$:\n-   **Density ($\\rho$)**: Calculated as $\\rho = \\frac{m}{N(N-1)/2}$, where $m$ is the number of edges, which is half the sum of all elements in $A$.\n-   **Giant Component Fraction ($g$)**: This requires finding connected components. We will use a Breadth-First Search (BFS) or Depth-First Search (DFS) based algorithm, available in `scipy.sparse.csgraph.connected_components`. The fraction is the size of the largest component divided by $N$.\n-   **Average Local Clustering Coefficient ($\\bar{C}$)**: For each node $i$, the local clustering coefficient $C_i$ is calculated. This involves finding the degree $k_i$ of node $i$ and the number of triangles $T_i$ it participates in. $C_i = 2T_i / (k_i(k_i-1))$ for $k_i \\ge 2$. The average $\\bar{C}$ is the mean of all $C_i$. $T_i$ can be found by counting edges in the subgraph induced by the neighbors of $i$.\n-   **Global Efficiency ($E_{\\mathrm{glob}}$)**: This metric requires all-pairs shortest path lengths, $d(i,j)$. An algorithm like Floyd-Warshall or running BFS from every node is needed. `scipy.sparse.csgraph.shortest_path` provides this functionality. The efficiency is the normalized sum of the inverse path lengths, $E_{\\mathrm{glob}} = \\frac{2}{N(N-1)} \\sum_{i < j} \\frac{1}{d(i,j)}$, where $1/d(i,j)=0$ if $i$ and $j$ are disconnected.\n\nFinally, a main loop will iterate through the test cases. For each case, it will:\n1.  Construct the weighted matrices for Group A ($W_A$) and Group B ($W_B$).\n2.  Apply global thresholding to get $A^{(\\mathrm{global})}_A$ and $A^{(\\mathrm{global})}_B$.\n3.  Apply proportional thresholding to get $A^{(\\mathrm{prop})}_A$ and $A^{(\\mathrm{prop})}_B$.\n4.  Compute all four metrics for these four binary graphs.\n5.  Calculate the absolute differences: $\\Delta M = |M_A - M_B|$ for each metric and thresholding strategy.\n6.  Evaluate the `prop_better` boolean flag based on the comparison of the sum of absolute differences for $E_{\\mathrm{glob}}$, $\\bar{C}$, and $g$.\n7.  Store the results for each test case in a list, and aggregate these lists into a final list for output. The output will be formatted as a single-line string representation of this final list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import csgraph\n\ndef build_adjacency_matrix(N, edges):\n    \"\"\"Constructs a weighted adjacency matrix from an edge list.\"\"\"\n    W = np.zeros((N, N), dtype=float)\n    for i, j, w in edges:\n        W[i, j] = w\n        W[j, i] = w\n    return W\n\ndef global_threshold(W, theta):\n    \"\"\"Applies a global absolute threshold to a weighted matrix.\"\"\"\n    A = (W >= theta).astype(int)\n    np.fill_diagonal(A, 0)\n    return A\n\ndef proportional_threshold(W, d):\n    \"\"\"Applies proportional thresholding to a weighted matrix.\"\"\"\n    N = W.shape[0]\n    num_possible_edges = N * (N - 1) / 2\n    K = int(num_possible_edges * d + 0.5)\n\n    # Extract upper triangular edges with indices and weights\n    edge_list = []\n    for i in range(N):\n        for j in range(i + 1, N):\n            edge_list.append((W[i, j], i, j))\n\n    # Sort: 1) weight descending, 2) i ascending, 3) j ascending\n    edge_list.sort(key=lambda x: (-x[0], x[1], x[2]))\n\n    A = np.zeros((N, N), dtype=int)\n    for k in range(min(K, len(edge_list))):\n        _, i, j = edge_list[k]\n        A[i, j] = 1\n        A[j, i] = 1\n    return A\n\ndef compute_metrics(A):\n    \"\"\"Computes density, giant component fraction, avg clustering, and global efficiency.\"\"\"\n    N = A.shape[0]\n    if N <= 1:\n        return 0.0, 1.0 if N==1 else 0.0, 0.0, 0.0\n\n    # 1. Density\n    num_edges = np.sum(A) / 2.0\n    num_possible_edges = N * (N - 1) / 2.0\n    density = num_edges / num_possible_edges if num_possible_edges > 0 else 0.0\n\n    # 2. Giant component fraction\n    n_components, labels = csgraph.connected_components(csgraph=A, directed=False, return_labels=True)\n    if n_components > 0:\n        component_sizes = np.bincount(labels)\n        giant_fraction = np.max(component_sizes) / N\n    else: # Should not happen for N > 0\n        giant_fraction = 0.0\n\n\n    # 3. Average local clustering coefficient\n    degrees = np.sum(A, axis=1)\n    C_local = np.zeros(N)\n    for i in range(N):\n        if degrees[i] >= 2:\n            neighbors = np.where(A[i] == 1)[0]\n            subgraph = A[np.ix_(neighbors, neighbors)]\n            num_neighbor_edges = np.sum(subgraph) / 2.0\n            C_local[i] = (2 * num_neighbor_edges) / (degrees[i] * (degrees[i] - 1))\n    C_avg = np.mean(C_local)\n\n    # 4. Global efficiency\n    dist_matrix = csgraph.shortest_path(csgraph=A, directed=False, unweighted=True)\n    with np.errstate(divide='ignore'):\n        inv_dist_matrix = 1.0 / dist_matrix\n    inv_dist_matrix[np.isinf(inv_dist_matrix)] = 0 # Handle disconnected pairs\n    np.fill_diagonal(inv_dist_matrix, 0)\n    \n    E_glob = np.sum(inv_dist_matrix) / (N * (N - 1))\n\n    return density, giant_fraction, C_avg, E_glob\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"N\": 6,\n            \"group_a_edges\": [\n                (0,1,0.90), (0,2,0.75), (1,2,0.85), (3,4,0.88), (3,5,0.80), (4,5,0.77),\n                (0,3,0.40), (0,4,0.45), (0,5,0.42), (1,3,0.50), (1,4,0.48), (1,5,0.44),\n                (2,3,0.38), (2,4,0.41), (2,5,0.43)\n            ],\n            \"group_b_transform\": lambda w: w * 1.50,\n            \"theta\": 0.55,\n            \"d\": 0.50\n        },\n        # Test Case 2\n        {\n            \"N\": 6,\n            \"group_a_edges\": [\n                (0,1,0.82), (0,2,0.79), (1,2,0.81), (3,4,0.83), (3,5,0.80), (4,5,0.78),\n                (0,3,0.25), (0,4,0.30), (0,5,0.28), (1,3,0.26), (1,4,0.27), (1,5,0.29),\n                (2,3,0.24), (2,4,0.31), (2,5,0.23)\n            ],\n            \"group_b_transform\": lambda w: w * 1.20,\n            \"theta\": 0.80,\n            \"d\": 0.20\n        },\n        # Test Case 3\n        {\n            \"N\": 6,\n            \"group_a_edges\": [\n                (0,1,0.65), (0,2,0.60), (1,2,0.65), (3,4,0.60), (3,5,0.65), (4,5,0.60),\n                (0,3,0.60), (0,4,0.60), (0,5,0.60), (1,3,0.60), (1,4,0.60), (1,5,0.60),\n                (2,3,0.60), (2,4,0.60), (2,5,0.60)\n            ],\n            \"group_b_edges\": [\n                (0,1,0.65), (0,2,0.60), (1,2,0.65), (3,4,0.60), (3,5,0.65), (4,5,0.60),\n                (0,3,0.60), (0,4,0.60), (0,5,0.60), (1,3,0.60), (1,4,0.60), (1,5,0.61),\n                (2,3,0.60), (2,4,0.61), (2,5,0.61)\n            ],\n            \"theta\": 0.60,\n            \"d\": 0.40\n        },\n        # Test Case 4\n        {\n            \"N\": 6,\n            \"group_a_edges\": [\n                (0,1,0.90), (0,2,0.75), (1,2,0.85), (3,4,0.88), (3,5,0.80), (4,5,0.77),\n                (0,3,0.40), (0,4,0.45), (0,5,0.42), (1,3,0.50), (1,4,0.48), (1,5,0.44),\n                (2,3,0.38), (2,4,0.41), (2,5,0.43)\n            ],\n            \"group_b_edges\": [\n                (0,1,0.90), (0,2,0.75), (1,2,0.85), (3,4,0.88), (3,5,0.80), (4,5,0.77),\n                (0,3,0.40), (0,4,0.45), (0,5,0.42), (1,3,0.50), (1,4,0.48), (1,5,0.44),\n                (2,3,0.38), (2,4,0.41), (2,5,0.43)\n            ],\n            \"theta\": 0.45,\n            \"d\": 0.60\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        W_a = build_adjacency_matrix(N, case[\"group_a_edges\"])\n        \n        if \"group_b_transform\" in case:\n            group_b_edges = [(i,j, case[\"group_b_transform\"](w)) for i,j,w in case[\"group_a_edges\"]]\n            W_b = build_adjacency_matrix(N, group_b_edges)\n        else:\n            W_b = build_adjacency_matrix(N, case[\"group_b_edges\"])\n\n        theta, d = case[\"theta\"], case[\"d\"]\n\n        # Global Thresholding\n        A_a_glob = global_threshold(W_a, theta)\n        A_b_glob = global_threshold(W_b, theta)\n        metrics_a_glob = compute_metrics(A_a_glob)\n        metrics_b_glob = compute_metrics(A_b_glob)\n\n        # Proportional Thresholding\n        A_a_prop = proportional_threshold(W_a, d)\n        A_b_prop = proportional_threshold(W_b, d)\n        metrics_a_prop = compute_metrics(A_a_prop)\n        metrics_b_prop = compute_metrics(A_b_prop)\n        \n        # Calculate differences (A-B)\n        delta_rho_glob = abs(metrics_a_glob[0] - metrics_b_glob[0])\n        delta_g_glob = abs(metrics_a_glob[1] - metrics_b_glob[1])\n        delta_C_glob = abs(metrics_a_glob[2] - metrics_b_glob[2])\n        delta_E_glob = abs(metrics_a_glob[3] - metrics_b_glob[3])\n\n        delta_rho_prop = abs(metrics_a_prop[0] - metrics_b_prop[0])\n        delta_g_prop = abs(metrics_a_prop[1] - metrics_b_prop[1])\n        delta_C_prop = abs(metrics_a_prop[2] - metrics_b_prop[2])\n        delta_E_prop = abs(metrics_a_prop[3] - metrics_b_prop[3])\n        \n        # prop_better flag\n        sum_delta_glob = delta_E_glob + delta_C_glob + delta_g_glob\n        sum_delta_prop = delta_E_prop + delta_C_prop + delta_g_prop\n        prop_better = sum_delta_prop < sum_delta_glob\n\n        case_result = [\n            round(delta_rho_glob, 10), round(delta_rho_prop, 10),\n            round(delta_E_glob, 10), round(delta_E_prop, 10),\n            round(delta_C_glob, 10), round(delta_C_prop, 10),\n            round(delta_g_glob, 10), round(delta_g_prop, 10),\n            prop_better\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Betweenness centrality is a key measure for identifying influential \"hub\" nodes that act as bridges in a network's communication pathways. The very definition of a \"shortest path\" depends on how we convert biological measures of connection strength, like fiber count or functional correlation, into a mathematical edge length. This exercise challenges you to compute betweenness centrality from first principles and investigate how different, plausible transformations of edge weights can alter which nodes are identified as the most central, providing a crucial lesson in methodological sensitivity .",
            "id": "3985632",
            "problem": "You are given undirected weighted graphs representing structural brain connectivity, where edge weights encode conductance. Starting from core definitions in graph theory, derive betweenness centrality from shortest path enumerations and implement an algorithm to compute node betweenness centrality for weighted graphs using two different edge weight transformations. Then analyze how these transformations impact the ranking of nodes by betweenness centrality.\n\nFoundational base and definitions to start from:\n- A graph is defined as $G = (V, E)$ with $|V| = n$ nodes and edges $E \\subseteq V \\times V$. Each undirected edge $(i,j) \\in E$ has a conductance $c_{ij} \\in (0,1]$.\n- A path from node $s$ to node $t$ is a sequence of nodes $(v_0, v_1, \\dots, v_k)$ with $v_0 = s$, $v_k = t$, and $(v_{r-1}, v_r) \\in E$ for all $r$. The path length is the sum of edge lengths along the path.\n- For a given transformation $f$ of conductance to edge length, the shortest path distance from $s$ to $t$ is the minimum path length over all paths from $s$ to $t$ under edge lengths $w_{ij} = f(c_{ij})$.\n- Betweenness centrality of a node $v$ is defined by $$C_B(v) = \\sum_{\\substack{s \\neq v \\neq t \\\\ s \\neq t}} \\frac{\\sigma_{st}(v)}{\\sigma_{st}},$$ where $\\sigma_{st}$ is the number of shortest paths from $s$ to $t$ and $\\sigma_{st}(v)$ is the number of those shortest paths that pass through $v$. For undirected graphs, the normalized betweenness centrality is $$\\tilde{C}_B(v) = \\frac{C_B(v)}{\\frac{(n-1)(n-2)}{2}}.$$\n\nEdge weight transformations to analyze:\n- Inverse-resistance mapping: $$f_{\\mathrm{inv}}(c) = \\frac{1}{c}.$$\n- Additive negative-log mapping: $$f_{\\mathrm{neglog}}(c) = -\\log(c),$$ where $\\log$ denotes the natural logarithm. Since $c \\in (0,1]$, $-\\log(c) \\ge 0$ ensures nonnegative edge lengths for shortest path computations.\n\nTask:\n- Implement, for each graph, the computation of $\\tilde{C}_B(v)$ for all nodes under both $f_{\\mathrm{inv}}$ and $f_{\\mathrm{neglog}}$ by explicitly enumerating shortest path dependencies as required by the definition (do not use pre-packaged centrality functions).\n- For each graph, produce three quantities:\n  1. A boolean indicating whether the ranking of nodes by $\\tilde{C}_B(v)$ (sorted in descending order, breaking ties by smaller node index) differs between $f_{\\mathrm{inv}}$ and $f_{\\mathrm{neglog}}$.\n  2. The node index that achieves the maximum signed difference $\\Delta(v) = \\tilde{C}_B^{\\mathrm{neglog}}(v) - \\tilde{C}_B^{\\mathrm{inv}}(v)$, with ties broken by the smallest node index.\n  3. The absolute difference at that node, $|\\Delta(v)|$, rounded to $6$ decimal places.\n\nGraphs (test suite):\n- Test case $1$ (happy path, modular network with bridges): $n = 6$ and edges with conductance values\n  $$\\{(0,1,0.9),(1,2,0.85),(0,2,0.8),(3,4,0.88),(4,5,0.9),(3,5,0.82),(2,3,0.5),(1,4,0.45)\\}.$$\n- Test case $2$ (boundary, complete graph with equal weights): $n = 4$ and all pairs $(i,j)$ with $i < j$ have conductance\n  $$c_{ij} = 0.6.$$\n- Test case $3$ (significant edge-case, competing path structures): $n = 4$ and edges\n  $$\\{(0,1,0.8),(1,2,0.9),(2,3,0.9),(1,3,0.7)\\}.$$\n\nAlgorithmic constraints:\n- Work with undirected graphs.\n- Use nonnegative edge lengths for shortest path computations.\n- Compute $\\sigma_{st}$ and $\\sigma_{st}(v)$ via shortest path enumeration consistent with the definitions above.\n- Normalize betweenness centrality as $\\tilde{C}_B(v)$ using $\\frac{(n-1)(n-2)}{2}$ for undirected graphs.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n  $$[\\mathrm{changed}_1, \\mathrm{argmax\\_node}_1, \\mathrm{diff}_1, \\mathrm{changed}_2, \\mathrm{argmax\\_node}_2, \\mathrm{diff}_2, \\mathrm{changed}_3, \\mathrm{argmax\\_node}_3, \\mathrm{diff}_3],$$\n  where each $\\mathrm{changed}_k$ is a boolean, each $\\mathrm{argmax\\_node}_k$ is an integer node index, and each $\\mathrm{diff}_k$ is a float rounded to $6$ decimals.\n\nAngle units are not applicable. No physical units are required. Percentages must not be used; differences must be expressed as decimal floats.\n\nYour submission must be a complete, runnable program.",
            "solution": "The problem requires the computation and analysis of node betweenness centrality in undirected, weighted graphs representing brain connectivity. The edge weights are given as conductances, which must be transformed into edge lengths for shortest path computations. The analysis hinges on comparing the centrality rankings produced by two distinct transformations: the inverse-resistance mapping and the additive negative-log mapping.\n\nTo solve this, we must first establish a rigorous method for calculating betweenness centrality from its fundamental definition. The betweenness centrality of a node $v$, $C_B(v)$, is defined as the sum of fractional counts over all pairs of distinct source and target nodes, $(s, t)$, in the graph:\n$$C_B(v) = \\sum_{\\substack{s \\neq v \\neq t \\\\ s \\neq t}} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$$\nHere, $\\sigma_{st}$ is the total number of shortest paths between $s$ and $t$, and $\\sigma_{st}(v)$ is the number of those shortest paths that pass through node $v$. For an undirected graph with $n = |V|$ nodes, this value is typically normalized by the number of pairs of nodes other than $v$, which is $\\frac{(n-1)(n-2)}{2}$.\n\nA direct enumeration of all paths for all pairs of nodes is computationally prohibitive. A far more efficient method is Brandes' algorithm, which calculates centrality by iterating through each node as a source and accumulating dependencies. This algorithm consists of two main phases for each source node $s \\in V$:\n\nPhase 1: Shortest Path Tree and Path Counting\nFor a given source node $s$, we compute the shortest path distance, $d(s,u)$, from $s$ to every other node $u \\in V$. Since the edge lengths, derived from transformations $f_{\\mathrm{inv}}(c) = 1/c$ and $f_{\\mathrm{neglog}}(c) = -\\log(c)$, are guaranteed to be non-negative, Dijkstra's algorithm is a suitable choice. As we execute Dijkstra's, we augment it to not only find the lengths of shortest paths but also to count them.\nLet $\\sigma_{su}$ be the number of shortest paths from $s$ to $u$.\n1.  Initialize distances $d(s,u) = \\infty$ for all $u \\neq s$, and $d(s,s) = 0$.\n2.  Initialize path counts $\\sigma_{su} = 0$ for all $u \\neq s$, and $\\sigma_{ss} = 1$.\n3.  Use a priority queue to explore nodes, starting with $s$.\n4.  When relaxing an edge $(u, v)$ with weight $w_{uv}$:\n    - If a shorter path to $v$ is found via $u$, i.e., $d(s,u) + w_{uv} < d(s,v)$, we update the distance: $d(s,v) = d(s,u) + w_{uv}$. All previously found shortest paths to $v$ are now obsolete. The number of shortest paths to $v$ is now equal to the number of shortest paths to $u$, so we set $\\sigma_{sv} = \\sigma_{su}$. We also record $u$ as the sole predecessor of $v$ on shortest paths from $s$.\n    - If a path of the same length is found, i.e., $d(s,u) + w_{uv}$ is approximately equal to $d(s,v)$ (using a small tolerance for floating-point arithmetic), we have found additional shortest paths. We update the count $\\sigma_{sv} = \\sigma_{sv} + \\sigma_{su}$ and add $u$ to the list of $v$'s predecessors.\n\nPhase 2: Dependency Accumulation\nAfter the SSSP from source $s$ is complete, we have a Directed Acyclic Graph (DAG) of all shortest paths originating from $s$. We can now calculate the \"dependency\" of the source $s$ on each node $v$, which is defined as $\\delta_s(v) = \\sum_{t \\in V} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$. This can be computed efficiently by processing the nodes in decreasing order of their distance from $s$. This order is naturally obtained by taking the nodes from the SSSP phase in the reverse order they were finalized.\n1.  For each node $v$, initialize its dependency score $\\delta_s(v) = 0$.\n2.  Iterate through all nodes $w$ in decreasing order of $d(s,w)$.\n3.  For each predecessor $v$ of $w$ on a shortest path from $s$:\n    The fraction of shortest paths from $s$ to $w$ that pass through $v$ is $\\frac{\\sigma_{sv}}{\\sigma_{sw}}$. Any shortest path from $s$ to any node beyond $w$ (say, $t$) that passes through $(v,w)$ will contribute to the dependency of $s$ on $v$. The total dependency of $s$ on $v$ is incremented by the dependencies that flow through $w$. The accumulation rule is:\n    $$ \\delta_s(v) \\leftarrow \\delta_s(v) + \\frac{\\sigma_{sv}}{\\sigma_{sw}} (1 + \\delta_s(w)) $$\n    The '1' in the term $(1 + \\delta_s(w))$ accounts for paths from $s$ that terminate at $w$, for which $w$ itself is the target.\n4.  The contribution of source $s$ to the total betweenness centrality of a node $v$ is simply $\\delta_s(v)$.\n\nBy repeating this two-phase process for every node $s \\in V$ as a source, we can compute the total betweenness centrality. For an undirected graph, the sum $\\sum_{s \\in V} \\delta_s(v)$ counts each path twice (once for $(s,t)$ and once for $(t,s)$). Therefore, the final betweenness score is $C_B(v) = \\frac{1}{2}\\sum_{s \\in V} \\delta_s(v)$. This is then normalized by dividing by $\\frac{(n-1)(n-2)}{2}$.\n\nThe core of the analysis lies in applying this algorithm twice for each graph, once with edge lengths $w_{ij}=f_{\\mathrm{inv}}(c_{ij})$ and once with $w'_{ij}=f_{\\mathrm{neglog}}(c_{ij})$. Because these transformations are non-linearly related ($1/c$ vs. $-\\log(c)$), they can alter the relative lengths of different paths. A path that is shortest under one transformation may not be the shortest under the other. This change in the underlying shortest path structure directly affects the $\\sigma$ and $\\delta$ values, leading to different betweenness centrality scores and potentially different node rankings.\n\nThe final step involves comparing the two sets of normalized centrality scores for each graph to determine:\n1.  If the node ranking (descending centrality, with ties broken by smaller node index) is altered.\n2.  The node with the largest signed difference in centrality, $\\Delta(v) = \\tilde{C}_B^{\\mathrm{neglog}}(v) - \\tilde{C}_B^{\\mathrm{inv}}(v)$.\n3.  The magnitude of this maximum difference.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport heapq\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path, modular network with bridges)\n        (6, [(0, 1, 0.9), (1, 2, 0.85), (0, 2, 0.8), (3, 4, 0.88), \n             (4, 5, 0.9), (3, 5, 0.82), (2, 3, 0.5), (1, 4, 0.45)]),\n        # Test case 2 (boundary, complete graph with equal weights)\n        (4, [(0, 1, 0.6), (0, 2, 0.6), (0, 3, 0.6), \n             (1, 2, 0.6), (1, 3, 0.6), (2, 3, 0.6)]),\n        # Test case 3 (significant edge-case, competing path structures)\n        (4, [(0, 1, 0.8), (1, 2, 0.9), (2, 3, 0.9), (1, 3, 0.7)])\n    ]\n\n    results = []\n    for n, edges in test_cases:\n        changed, argmax_node, diff = process_graph(n, edges)\n        results.extend([changed, argmax_node, diff])\n    \n    # Format a boolean as a lowercase 'true' or 'false'\n    formatted_results = [str(r).lower() if isinstance(r, bool) else r for r in results]\n    print(f\"[{','.join(map(str, formatted_results))}]\")\n\ndef process_graph(n, edges):\n    \"\"\"\n    Computes and compares betweenness centrality for a single graph under two weight transformations.\n    Args:\n        n (int): Number of nodes in the graph.\n        edges (list of tuples): Edges of the graph, as (u, v, conductance).\n    Returns:\n        tuple: (ranking_changed, max_diff_node, max_diff_value)\n    \"\"\"\n    # Define weight transformations\n    f_inv = lambda c: 1.0 / c\n    f_neglog = lambda c: -np.log(c)\n\n    # Compute betweenness centrality for both transformations\n    bc_inv = compute_betweenness_centrality(n, edges, f_inv)\n    bc_neglog = compute_betweenness_centrality(n, edges, f_neglog)\n\n    # 1. Compare node rankings\n    # Key for sorting: primary descending by centrality, secondary ascending by node index\n    ranking_inv = sorted(range(n), key=lambda i: (-bc_inv[i], i))\n    ranking_neglog = sorted(range(n), key=lambda i: (-bc_neglog[i], i))\n    rankings_differ = (ranking_inv != ranking_neglog)\n\n    # 2. Find node with maximum signed difference\n    delta = bc_neglog - bc_inv\n    # Break ties in delta by choosing the smaller node index\n    max_val = np.max(delta)\n    candidates = np.where(np.isclose(delta, max_val))[0]\n    argmax_node = np.min(candidates)\n\n\n    # 3. Calculate the absolute difference at that node\n    max_diff_abs = np.abs(delta[argmax_node])\n\n    return rankings_differ, int(argmax_node), round(max_diff_abs, 6)\n\ndef compute_betweenness_centrality(n, edges, transform_func):\n    \"\"\"\n    Computes normalized betweenness centrality for a graph using Brandes' algorithm.\n    Args:\n        n (int): Number of nodes.\n        edges (list of tuples): Edges with conductance values.\n        transform_func (function): Function to convert conductance to edge length.\n    Returns:\n        np.array: A numpy array of normalized betweenness centrality for each node.\n    \"\"\"\n    adj = [[] for _ in range(n)]\n    for u, v, conductance in edges:\n        weight = transform_func(conductance)\n        adj[u].append((v, weight))\n        adj[v].append((u, weight))\n        \n    betweenness = np.zeros(n, dtype=float)\n    \n    for s in range(n):\n        # Phase 1: SSSP (Dijkstra) and path counting\n        dist = np.full(n, np.inf)\n        sigma = np.zeros(n, dtype=float)\n        predecessors = [[] for _ in range(n)]\n        \n        dist[s] = 0.0\n        sigma[s] = 1.0\n        pq = [(0.0, s)]\n        stack = []\n\n        while pq:\n            d, u = heapq.heappop(pq)\n            \n            if d > dist[u] + 1e-12: # Add tolerance\n                continue\n                \n            stack.append(u)\n            \n            for v, weight in adj[u]:\n                # Using a small tolerance for floating point comparison\n                if dist[v] > dist[u] + weight + 1e-12:\n                    dist[v] = dist[u] + weight\n                    sigma[v] = sigma[u]\n                    predecessors[v] = [u]\n                    heapq.heappush(pq, (dist[v], v))\n                elif np.isclose(dist[v], dist[u] + weight):\n                    sigma[v] += sigma[u]\n                    predecessors[v].append(u)\n\n        # Phase 2: Dependency accumulation\n        delta = np.zeros(n, dtype=float)\n        while stack:\n            w = stack.pop()\n            for v in predecessors[w]:\n                if sigma[w] != 0:\n                    delta[v] += (sigma[v] / sigma[w]) * (1.0 + delta[w])\n            if w != s:\n                betweenness[w] += delta[w]\n\n    # For undirected graphs, Brandes' algorithm sums contributions for s->t and t->s\n    # so final sum is already C_B(v). No division by 2 needed.\n    # However, problem description implies summation over ordered pairs (s,t), \n    # and normalization by (n-1)(n-2)/2 suggests we are counting each pair once.\n    # The standard Brandes implementation for undirected graphs does sum over all s,\n    # and then divides by 2.\n    betweenness /= 2.0\n    \n    # Normalize\n    if n > 2:\n        norm_factor = ((n - 1) * (n - 2)) / 2.0\n        if norm_factor > 0:\n            betweenness /= norm_factor\n        \n    return betweenness\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Comparing brain networks across groups, such as patients and controls, involves a massive number of statistical tests—one for every possible connection—which inflates the risk of false positives. The Network-Based Statistic (NBS) is an elegant solution that shifts the statistical question from individual edges to entire connected components of edges that show a group effect, providing powerful control over the family-wise error rate. In this advanced practice, you will implement the NBS pipeline, including permutation testing, to understand how this widely used method enables robust statistical inference to identify subnetworks associated with clinical or cognitive variables .",
            "id": "3985525",
            "problem": "You are given two groups of weighted, undirected graphs representing brain connectivity networks. Each graph is represented by a symmetric adjacency matrix with $0$ diagonal and real-valued edge weights. The goal is to implement the Network-Based Statistic (NBS) and to justify mathematically why it controls the Family-Wise Error Rate (FWER) under a valid null model. Your program must compute connected components of edge differences between two groups and use permutation-based maximal component size to obtain FWER-corrected $p$-values.\n\nFundamental base for the derivation and algorithm:\n- Randomization inference under the null hypothesis relies on exchangeability: if the null hypothesis of no group effect holds, then the joint distribution of the data is invariant under permutations of group labels. This implies that the distribution of any test statistic under the null can be approximated by permuting labels.\n- For a two-sample comparison at each edge, a well-tested choice of test statistic is the Welch two-sample $t$ statistic. For edge $(u,v)$, let $x_{uv}^{(i)}$ be the weight for subject $i$ and group label $\\ell^{(i)}\\in\\{0,1\\}$. For group $g\\in\\{0,1\\}$, let $\\bar{x}_{uv,g}$ and $s^{2}_{uv,g}$ be the sample mean and sample variance across subjects in group $g$, and $n_g$ the number of subjects in group $g$. The Welch statistic is $t_{uv} = \\dfrac{\\bar{x}_{uv,1} - \\bar{x}_{uv,0}}{\\sqrt{ s^{2}_{uv,1}/n_1 + s^{2}_{uv,0}/n_0 }}$, which is a well-tested device for comparing means under heteroscedasticity and is used here solely for thresholding to define connected components; the inferential control is achieved at the component level via permutation of the maximal component size.\n- A connected component for NBS is defined on the supra-threshold edge set: given a threshold $\\tau>0$, define the binary mask $M_{uv} = \\mathbf{1}\\{ |t_{uv}| \\ge \\tau \\}$ for $u<v$. Form the undirected graph $G_\\tau$ on the node set with edges where $M_{uv}=1$. The connected components of $G_\\tau$ induce disjoint edge sets; the size of a component is the number of edges it contains.\n\nYour program must implement the following algorithm for each test case:\n1. Construct the subject-level adjacency matrices for two groups, ensuring symmetry and $0$ diagonal. The graphs have $N$ nodes, $n_0$ subjects in group $0$, and $n_1$ subjects in group $1$.\n2. Compute the Welch two-sample $t$ statistic $t_{uv}$ independently at each undirected edge $(u,v)$ with $u<v$.\n3. Threshold the absolute statistics at level $\\tau$ to build the supra-threshold graph $G_\\tau$, and find all connected components. For each component $C$, compute its size $S(C)$ as the number of edges in $C$.\n4. Generate $B$ permutations of the group labels under the null (shuffle all labels among subjects), repeat steps $2$ and $3$ for each permutation, and record the maximal component size $S_{\\max}^{(b)}$ over components in permutation $b$ (set $S_{\\max}^{(b)}=0$ if no supra-threshold edges are present).\n5. For each observed component $C$, compute the FWER-corrected $p$-value $p(C)$ as the permutation tail probability based on the maximal component size: $p(C) = \\dfrac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{ S_{\\max}^{(b)} \\ge S(C) \\}}{B+1}$. This choice is a conservative, finite-sample estimator of the tail probability under the null.\n6. Given a significance level $\\alpha$, count the number of components with $p(C) < \\alpha$.\n\nScientific realism and generation of synthetic data:\n- Subject-level adjacency matrices are generated as independent Gaussian weights on the upper triangle, symmetrized to enforce undirected graphs and with diagonal set to $0$. All subjects share the same baseline variance $\\sigma^2$ at each edge.\n- Group differences, when present, are introduced by adding a fixed mean shift $\\mu$ to specified edges for all subjects in group $1$. This yields an interpretable, controlled effect size for assessing NBS performance.\n\nTest suite specification:\nProvide three test cases. Each case defines $(N, n_0, n_1, \\sigma, \\text{effect edges}, \\text{effect magnitudes}, \\tau, B, \\alpha)$ precisely.\n\n- Case $1$ (connected cluster present; happy path):\n  - $N=10$, $n_0=14$, $n_1=14$, $\\sigma=1.0$.\n  - Effect edges in group $1$: $(0,1)$, $(1,2)$, $(2,3)$, $(3,0)$, $(1,3)$ with a mean shift $\\mu=1.1$ applied to each listed edge.\n  - Threshold $\\tau=2.5$, permutations $B=500$, significance level $\\alpha=0.05$.\n  - Expected behavior: a single connected component corresponding to these edges, likely yielding at least one significant component after FWER correction.\n\n- Case $2$ (no true differences; boundary case):\n  - $N=10$, $n_0=12$, $n_1=12$, $\\sigma=1.0$.\n  - Effect edges: none; $\\mu=0.0$.\n  - Threshold $\\tau=2.5$, permutations $B=400$, significance level $\\alpha=0.05$.\n  - Expected behavior: zero significant components.\n\n- Case $3$ (multiple separated components; edge-case coverage):\n  - $N=10$, $n_0=10$, $n_1=10$, $\\sigma=1.0$.\n  - Two disjoint sets of effect edges in group $1$:\n    - Triangle component with edges $(4,5)$, $(5,6)$, $(6,4)$, mean shift $\\mu_{\\triangle}=1.0$.\n    - Chain component with edges $(7,8)$, $(8,9)$, mean shift $\\mu_{\\text{chain}}=0.95$.\n  - Threshold $\\tau=2.0$, permutations $B=400$, significance level $\\alpha=0.05$.\n  - Expected behavior: two separate connected components; both are likely to be detected as significant after FWER correction.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the integer count of significant connected components for the corresponding test case, in the order $[ \\text{Case }1, \\text{Case }2, \\text{Case }3 ]$. For example, output must have the form $[k_1,k_2,k_3]$ with each $k_i$ an integer.",
            "solution": "The task is to implement the Network-Based Statistic (NBS) and to justify its control of the Family-Wise Error Rate (FWER). The scientific and algorithmic basis proceeds from first principles of randomization inference and graph connectivity.\n\nFirst principles and definitions:\n- Under the null hypothesis $\\mathcal{H}_0$ of no group differences, the data are exchangeable with respect to group labels: permutations of labels do not change the joint distribution of the observed data. This assumption is widely used in randomization tests and is satisfied in the absence of group-wise systematic differences.\n- For each undirected edge $(u,v)$, the Welch two-sample $t$ statistic is defined by $t_{uv} = \\dfrac{\\bar{x}_{uv,1} - \\bar{x}_{uv,0}}{\\sqrt{ s^{2}_{uv,1}/n_1 + s^{2}_{uv,0}/n_0 }}$, where $\\bar{x}_{uv,g}$ is the sample mean, $s^{2}_{uv,g}$ is the sample variance in group $g$, and $n_g$ is the group sample size. This statistic is a well-tested measure of standardized difference that accommodates unequal variances; we use it only to decide which edges are supra-threshold.\n- Given a threshold $\\tau>0$, define a supra-threshold graph $G_\\tau$ whose edges are those $(u,v)$ such that $|t_{uv}|\\ge\\tau$. Compute the connected components of $G_\\tau$; each component $C$ is associated with the edge set that lies within the nodes of the component, and its size $S(C)$ is the number of edges in $C$.\n\nAlgorithmic steps:\n1. Data generation for each test case follows a Gaussian model: for each subject, draw upper-triangular entries of the adjacency matrix independently from $\\mathcal{N}(0,\\sigma^2)$, symmetrize, and set the diagonal to $0$. For specified effect edges and mean shifts, add the shift to the corresponding edges for all subjects in group $1$. This yields $\\mathbb{E}[x_{uv}^{(i)} \\mid \\ell^{(i)}=1] - \\mathbb{E}[x_{uv}^{(i)} \\mid \\ell^{(i)}=0] = \\mu$ at the chosen edges, with no shift elsewhere.\n2. Compute $t_{uv}$ across all edges using the Welch formula. In practice, for numerical stability, a small positive constant can be added to the denominator to prevent division by $0$ in degenerate cases, though Gaussian variation and multiple subjects render this event highly improbable.\n3. Threshold at level $\\tau$ to obtain the supra-threshold edge mask, construct $G_\\tau$, and find connected components using standard breadth-first search (BFS) on the node-level graph derived from the supra-threshold edge set. For each component, compute the number of edges in the component as $S(C)$ by counting edges with both endpoints in the node set of $C$.\n4. Permutation test for FWER control: shuffle group labels across all subjects to generate a null sample because exchangeability holds under $\\mathcal{H}_0$. For each permutation $b\\in\\{1,\\dots,B\\}$, recompute $t_{uv}$, threshold, compute connected components, and record the maximum component size $S_{\\max}^{(b)}$. The sequence $\\{S_{\\max}^{(b)}\\}_{b=1}^{B}$ approximates the null distribution of the maximal component size.\n5. For each observed component $C$, compute the FWER-corrected $p$-value $p(C)$ using the tail probability of the maximal null distribution: $p(C) = \\dfrac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{ S_{\\max}^{(b)} \\ge S(C) \\}}{B+1}$. This estimator is conservative and ensures that the probability of obtaining any component with size at least $S(C)$ under $\\mathcal{H}_0$ does not exceed $p(C)$.\n6. Given $\\alpha$, count components with $p(C) < \\alpha$.\n\nDerivation of FWER control:\n- Let $\\mathcal{C}_{\\text{obs}}$ be the set of observed components from the original labeling, and define their sizes $\\{ S(C) : C\\in\\mathcal{C}_{\\text{obs}} \\}$. Consider the event $\\mathcal{E}$ that at least one observed component is a false positive under $\\mathcal{H}_0$ when using the decision rule $p(C) < \\alpha$.\n- The NBS decision with maximal-statistic correction compares each observed $S(C)$ to the null distribution of the maximum component size $S_{\\max}$ obtained by permutations. Define the $(1-\\alpha)$ upper quantile $q_{1-\\alpha}$ of $S_{\\max}$ (in finite samples, the permutation tail cut is determined by the empirical distribution of $\\{S_{\\max}^{(b)}\\}$).\n- Because the permutation distribution approximates the null law of $S_{\\max}$ under exchangeability, the decision $S(C) > q_{1-\\alpha}$ is equivalent to $p(C) < \\alpha$ under the tail rule $p(C) = \\mathbb{P}(S_{\\max} \\ge S(C) \\mid \\mathcal{H}_0)$.\n- The FWER is $\\mathbb{P}(\\exists C \\text{ falsely declared significant}) \\le \\mathbb{P}(S_{\\max} \\ge q_{1-\\alpha} \\mid \\mathcal{H}_0) = \\alpha$ by the definition of the upper quantile of $S_{\\max}$. The inequality follows because the event of at least one false positive component implies that the maximal size among null components exceeds the threshold; in other words, if any null component yields a rejection, the maximal null component must exceed the rejection size threshold, so controlling the maximal null single statistic controls the union over all possible components.\n- This is a strong control argument: by calibrating the decision threshold using the distribution of the sample maximum of component sizes under the null, we implicitly control the probability of at least one false rejection across the family of components. The logic is equivalent to the classical maxima-based family-wise error control seen in cluster-based random field methods, but here applied to graph-connected components.\n- In finite samples, the estimator $p(C) = \\dfrac{1 + \\sum_{b=1}^{B} \\mathbf{1}\\{ S_{\\max}^{(b)} \\ge S(C) \\}}{B+1}$ ensures that $\\hat{p}(C)$ is never $0$, maintaining conservative control, and converges to the true tail probability as $B\\to\\infty$ under exchangeability.\n\nTest suite behavior:\n- Case $1$: The supra-threshold edges with $\\mu=1.1$ and $\\tau=2.5$ are likely to cluster into a single connected component of size $5$, yielding at least one significant component after correction at $\\alpha=0.05$.\n- Case $2$: With no effect edges, any observed components are attributable to noise; permutation-based maxima calibration should yield zero significant components at $\\alpha=0.05$.\n- Case $3$: Two disjoint connected components (triangle and chain) are formed by supra-threshold edges with $\\mu_{\\triangle}=1.0$ and $\\mu_{\\text{chain}}=0.95$ at $\\tau=2.0$. Both are likely to be declared significant at $\\alpha=0.05$ after FWER correction.\n\nThe program returns a single line $[k_1,k_2,k_3]$, where each $k_i$ is the integer count of significant components for the corresponding case. The design ensures universal applicability: definitions use basic graph theory and randomization testing, all computations are unitless real-valued matrices, angle units are not involved, and significance levels are provided as decimals. The algorithm avoids shortcut formulas for the inferential control, relying instead on principled permutation of the maximal component size distribution.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef welch_t_stat(edge_values_group0: np.ndarray, edge_values_group1: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute Welch's t-statistic for each edge between two groups.\n    edge_values_group0: shape (num_edges, n0)\n    edge_values_group1: shape (num_edges, n1)\n    Returns: t-statistics per edge, shape (num_edges,)\n    \"\"\"\n    m0 = edge_values_group0.mean(axis=1)\n    m1 = edge_values_group1.mean(axis=1)\n    v0 = edge_values_group0.var(axis=1, ddof=1)\n    v1 = edge_values_group1.var(axis=1, ddof=1)\n    n0 = edge_values_group0.shape[1]\n    n1 = edge_values_group1.shape[1]\n    denom = np.sqrt(v0 / n0 + v1 / n1)\n    # Avoid division by zero: add tiny epsilon\n    eps = 1e-12\n    denom = np.maximum(denom, eps)\n    tvals = (m1 - m0) / denom\n    return tvals\n\ndef supra_threshold_components(num_nodes: int, tvals: np.ndarray, edges: np.ndarray, tau: float):\n    \"\"\"\n    Given t-statistics per edge and a threshold tau, compute connected components\n    on the node graph induced by supra-threshold edges, and return component sizes (edge counts).\n    \"\"\"\n    # Build supra-threshold adjacency\n    supra_mask = np.abs(tvals) >= tau\n    if not np.any(supra_mask):\n        return []  # no components\n    adj = np.zeros((num_nodes, num_nodes), dtype=bool)\n    supra_edges = edges[supra_mask]\n    for (u, v) in supra_edges:\n        adj[u, v] = True\n        adj[v, u] = True\n\n    # BFS to find connected components of nodes\n    visited = np.zeros(num_nodes, dtype=bool)\n    component_sizes = []\n    for start in range(num_nodes):\n        if visited[start]:\n            continue\n        # Only start BFS if node participates in any supra-threshold edge\n        if not adj[start].any():\n            visited[start] = True\n            continue\n        queue = [start]\n        visited[start] = True\n        comp_nodes = [start]\n        while queue:\n            cur = queue.pop(0)\n            neighbors = np.where(adj[cur])[0]\n            for nb in neighbors:\n                if not visited[nb]:\n                    visited[nb] = True\n                    queue.append(nb)\n                    comp_nodes.append(nb)\n        # Count edges within this component\n        comp_nodes_arr = np.array(comp_nodes, dtype=int)\n        sub_adj = adj[np.ix_(comp_nodes_arr, comp_nodes_arr)]\n        # Each undirected edge counted twice in adjacency (i,j) and (j,i); divide by 2\n        edge_count = int(sub_adj.sum() // 2)\n        if edge_count > 0:\n            component_sizes.append(edge_count)\n    return component_sizes\n\ndef nbs_count_significant(all_subject_adjs: np.ndarray, labels: np.ndarray, num_nodes: int, tau: float, B: int, alpha: float) -> int:\n    \"\"\"\n    Compute NBS components and FWER-corrected p-values; return count of significant components.\n    all_subject_adjs: shape (n_subjects, num_nodes, num_nodes)\n    labels: shape (n_subjects,), values in {0,1}\n    tau: threshold for t-statistics\n    B: number of permutations\n    alpha: significance level\n    \"\"\"\n    n_subjects = all_subject_adjs.shape[0]\n    # Enumerate undirected edges (u<v)\n    edges = []\n    for u in range(num_nodes):\n        for v in range(u + 1, num_nodes):\n            edges.append((u, v))\n    edges = np.array(edges, dtype=int)\n    num_edges = edges.shape[0]\n\n    # Build edge-by-subject matrix\n    e_by_sub = np.zeros((num_edges, n_subjects), dtype=float)\n    for si in range(n_subjects):\n        A = all_subject_adjs[si]\n        e_vals = A[edges[:, 0], edges[:, 1]]\n        e_by_sub[:, si] = e_vals\n\n    # Observed t-statistics and components\n    mask0 = labels == 0\n    mask1 = labels == 1\n    t_obs = welch_t_stat(e_by_sub[:, mask0], e_by_sub[:, mask1])\n    obs_component_sizes = supra_threshold_components(num_nodes, t_obs, edges, tau)\n\n    # If no observed components, no significant findings\n    if len(obs_component_sizes) == 0:\n        return 0\n\n    # Permutation null of maximal component size\n    rng = np.random.default_rng(123456789)\n    max_sizes = np.zeros(B, dtype=int)\n    for b in range(B):\n        perm = rng.permutation(n_subjects)\n        perm_labels = labels[perm]\n        p_mask0 = perm_labels == 0\n        p_mask1 = perm_labels == 1\n        # If a permutation produces empty group, skip; but with balanced labels this won't happen\n        t_perm = welch_t_stat(e_by_sub[:, p_mask0], e_by_sub[:, p_mask1])\n        comp_sizes_perm = supra_threshold_components(num_nodes, t_perm, edges, tau)\n        max_sizes[b] = max(comp_sizes_perm) if len(comp_sizes_perm) > 0 else 0\n\n    # Compute FWER-corrected p-values for observed components\n    B_float = float(B + 1)  # for +1 correction\n    max_sizes_sorted = np.sort(max_sizes)\n    # tail counts by direct comparison\n    significant_count = 0\n    for s in obs_component_sizes:\n        tail_ge = np.count_nonzero(max_sizes >= s)\n        pval = (1 + tail_ge) / B_float\n        if pval < alpha:\n            significant_count += 1\n    return significant_count\n\ndef generate_synthetic_data_case_1():\n    # Case 1: N=10, n0=14, n1=14, sigma=1.0, effect edges with mu=1.1\n    N = 10\n    n0 = 14\n    n1 = 14\n    sigma = 1.0\n    mu = 1.1\n    # Define effect edges\n    effect_edges = [(0, 1), (1, 2), (2, 3), (3, 0), (1, 3)]\n    rng = np.random.default_rng(42)\n    def sample_adj():\n        A = rng.normal(loc=0.0, scale=sigma, size=(N, N))\n        # Symmetrize and zero diagonal\n        A = (A + A.T) / 2.0\n        np.fill_diagonal(A, 0.0)\n        return A\n    adjs0 = np.array([sample_adj() for _ in range(n0)])\n    adjs1 = np.array([sample_adj() for _ in range(n1)])\n    # Apply effect to group 1\n    for s in range(n1):\n        for (u, v) in effect_edges:\n            adjs1[s, u, v] += mu\n            adjs1[s, v, u] += mu\n    all_adjs = np.vstack([adjs0, adjs1])\n    labels = np.array([0] * n0 + [1] * n1, dtype=int)\n    tau = 2.5\n    B = 500\n    alpha = 0.05\n    return all_adjs, labels, N, tau, B, alpha\n\ndef generate_synthetic_data_case_2():\n    # Case 2: N=10, n0=12, n1=12, sigma=1.0, no effect\n    N = 10\n    n0 = 12\n    n1 = 12\n    sigma = 1.0\n    rng = np.random.default_rng(123)\n    def sample_adj():\n        A = rng.normal(loc=0.0, scale=sigma, size=(N, N))\n        A = (A + A.T) / 2.0\n        np.fill_diagonal(A, 0.0)\n        return A\n    adjs0 = np.array([sample_adj() for _ in range(n0)])\n    adjs1 = np.array([sample_adj() for _ in range(n1)])\n    all_adjs = np.vstack([adjs0, adjs1])\n    labels = np.array([0] * n0 + [1] * n1, dtype=int)\n    tau = 2.5\n    B = 400\n    alpha = 0.05\n    return all_adjs, labels, N, tau, B, alpha\n\ndef generate_synthetic_data_case_3():\n    # Case 3: N=10, n0=10, n1=10, sigma=1.0, two components\n    N = 10\n    n0 = 10\n    n1 = 10\n    sigma = 1.0\n    mu_tri = 1.0\n    mu_chain = 0.95\n    tri_edges = [(4, 5), (5, 6), (6, 4)]\n    chain_edges = [(7, 8), (8, 9)]\n    rng = np.random.default_rng(2024)\n    def sample_adj():\n        A = rng.normal(loc=0.0, scale=sigma, size=(N, N))\n        A = (A + A.T) / 2.0\n        np.fill_diagonal(A, 0.0)\n        return A\n    adjs0 = np.array([sample_adj() for _ in range(n0)])\n    adjs1 = np.array([sample_adj() for _ in range(n1)])\n    for s in range(n1):\n        for (u, v) in tri_edges:\n            adjs1[s, u, v] += mu_tri\n            adjs1[s, v, u] += mu_tri\n        for (u, v) in chain_edges:\n            adjs1[s, u, v] += mu_chain\n            adjs1[s, v, u] += mu_chain\n    all_adjs = np.vstack([adjs0, adjs1])\n    labels = np.array([0] * n0 + [1] * n1, dtype=int)\n    tau = 2.0\n    B = 400\n    alpha = 0.05\n    return all_adjs, labels, N, tau, B, alpha\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        generate_synthetic_data_case_1(),\n        generate_synthetic_data_case_2(),\n        generate_synthetic_data_case_3(),\n    ]\n\n    results = []\n    for (all_adjs, labels, N, tau, B, alpha) in test_cases:\n        count_sig = nbs_count_significant(all_adjs, labels, N, tau, B, alpha)\n        results.append(count_sig)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}