{
    "hands_on_practices": [
        {
            "introduction": "深度卷积网络（DCNs）与灵长类动物腹侧视觉通路的一个关键相似之处在于其层次化结构，即神经元的感受野在连续的处理阶段中逐渐增大。这个练习提供了一种动手实践的方式，通过推导和计算感受野大小如何跨层级扩展，来理解这一基本的架构属性 。掌握这项技能对于设计网络以及解释其与生物视觉通路的对应关系至关重要。",
            "id": "3988311",
            "problem": "考虑一个灵长类动物腹侧视觉通路的分层前馈模型，该模型实现为卷积神经网络（CNN）中的卷积层堆栈。在视觉神经科学中，一个神经元的感受野是感觉输入空间的一个子集，该空间中的刺激会调节该神经元的活动。在具有离散输入的计算模型中，某层一个单元的感受野是输入样本的集合，这些样本的值可以通过其下方的操作序列影响该单元的激活。假设沿着图像行进行一维空间索引；对于二维各向同性的卷积核和步幅，一维的结果同样适用于两个轴。\n\n从以下基本出发点开始：(i) 上述感受野的定义，即影响一个单元激活的输入子集，以及 (ii) 一个经过充分验证的事实，即一个卷积核大小为 $k_i$、步幅为 $s_i$ 的卷积层，其当前层的每个单元会从前一层采样 $k_i$ 个相邻单元，并且当前层的单元相对于前一层以 $s_i$ 个索引的间距进行定位。令 $R_i$ 表示第 $i$ 层一个单元相对于原始输入样本的感受野大小，令 $S_i$ 表示第 $i$ 层单元相对于输入的有效采样步幅。\n\n根据第一性原理，推导出 $R_L$ 关于 $\\{k_i\\}_{i=1}^{L}$ 和 $\\{s_i\\}_{i=1}^{L}$ 的闭式表达式。然后，将您的推导应用于以下受腹侧视觉通路启发的具有 $L=4$ 层的架构：\n- 第 $1$ 层：卷积核大小 $k_1=7$，步幅 $s_1=2$（类似V1的简单-复杂细胞整合与子采样），\n- 第 $2$ 层：卷积核大小 $k_2=5$，步幅 $s_2=2$（类似V2的整合与子采样），\n- 第 $3$ 层：卷积核大小 $k_3=9$，步幅 $s_3=2$（类似V4的整合与子采样），\n- 第 $4$ 层：卷积核大小 $k_4=9$，步幅 $s_4=1$（类似颞下皮层的整合，无子采样）。\n\n请给出您的最终答案，即等于 $R_4$ 的单个整数，$R_4$ 是顶层感受野的大小，以输入样本数量衡量。无需四舍五入。最终答案中不要包含任何单位。",
            "solution": "感受野被定义为能够影响特定单元激活值的输入集合。在一个分层前馈堆栈中，这个集合随着深度的增加而增长，因为每个单元都会聚合其下方多个单元的信息，而这些单元中的每一个本身都受到原始输入的不断扩大的子集的影响。\n\n我们在一维离散模型中将其形式化。设 $R_i$ 为第 $i$ 层相对于原始输入（第 $0$ 层）的感受野大小。基准情况是 $R_0 = 1$，因为一个输入样本影响其自身。定义 $S_i$ 为第 $i$ 层相对于输入的有效采样步幅。由于步幅是乘法复合的，我们有\n$$\nS_i \\;=\\; \\prod_{j=1}^{i} s_j, \\quad \\text{with} \\quad S_0 = 1.\n$$\n考虑从第 $i-1$ 层到第 $i$ 层的过渡。第 $i$ 层的一个单元会从第 $i-1$ 层采样 $k_i$ 个相邻单元。最左边和最右边被采样的单元在第 $i-1$ 层相隔 $k_i - 1$ 步，而第 $i-1$ 层的每一步对应于输入中的 $S_{i-1}$ 步。因此，由第 $i$ 层贡献的感受野增量扩展为 $(k_i - 1) \\, S_{i-1}$。第 $i$ 层的总感受野是第 $i-1$ 层的感受野加上这个增量：\n$$\nR_i \\;=\\; R_{i-1} \\;+\\; (k_i - 1) \\, S_{i-1}.\n$$\n从 $R_0 = 1$ 展开这个递推关系，得到一个闭式解：\n$$\nR_L \\;=\\; 1 \\;+\\; \\sum_{i=1}^{L} (k_i - 1) \\, \\prod_{j=1}^{i-1} s_j.\n$$\n我们现在为指定的架构计算 $R_4$：\n- 给定 $k_1 = 7, s_1 = 2, k_2 = 5, s_2 = 2, k_3 = 9, s_3 = 2, k_4 = 9, s_4 = 1$。\n- 计算 $i = 1,2,3,4$ 时的乘积 $\\prod_{j=1}^{i-1} s_j$：\n  - 对于 $i=1$，空积为 $1$，所以 $\\prod_{j=1}^{0} s_j = 1$。\n  - 对于 $i=2$，$\\prod_{j=1}^{1} s_j = s_1 = 2$。\n  - 对于 $i=3$，$\\prod_{j=1}^{2} s_j = s_1 s_2 = 2 \\cdot 2 = 4$。\n  - 对于 $i=4$，$\\prod_{j=1}^{3} s_j = s_1 s_2 s_3 = 2 \\cdot 2 \\cdot 2 = 8$。\n- 计算每一项 $(k_i - 1) \\prod_{j=1}^{i-1} s_j$：\n  - 对于 $i=1$：$(k_1 - 1) \\cdot 1 = (7 - 1) \\cdot 1 = 6$。\n  - 对于 $i=2$：$(k_2 - 1) \\cdot 2 = (5 - 1) \\cdot 2 = 4 \\cdot 2 = 8$。\n  - 对于 $i=3$：$(k_3 - 1) \\cdot 4 = (9 - 1) \\cdot 4 = 8 \\cdot 4 = 32$。\n  - 对于 $i=4$：$(k_4 - 1) \\cdot 8 = (9 - 1) \\cdot 8 = 8 \\cdot 8 = 64$。\n- 求和并加上基数 $1$：\n$$\nR_4 \\;=\\; 1 + (6 + 8 + 32 + 64) \\;=\\; 1 + 110 \\;=\\; 111.\n$$\n\n因此，顶层感受野在一个维度上跨越 $111$ 个输入样本。对于各向同性的二维卷积核和步幅，感受野在每个轴上的范围是相同的，其面积将是 $111 \\times 111$，但题目要求的是一维跨度 $R_4$。",
            "answer": "$$\\boxed{111}$$"
        },
        {
            "introduction": "尽管DCNs在物体识别任务中能达到类似人类的性能，但它们有时会依赖于与人类不同的、甚至更肤浅的视觉线索，例如更多地依赖纹理而非形状。本练习通过一个简化的理论模型，深入探讨了这种“纹理偏差” 。您将分析性地推导出数据集的构成如何导致这种偏差，并通过求解一个重加权方案来抵消它，从而更深刻地理解训练数据如何塑造模型的行为。",
            "id": "3974084",
            "problem": "考虑一个简化的理论分析，探讨一个通过经验风险最小化训练的深度卷积神经网络（DCNN）在作为灵长类腹侧视觉流模型时，如何获得纹理偏差。假设一个二元分类任务，标签为 $y \\in \\{-1, +1\\}$，每张图像有两个潜在线索：形状线索 $s$ 和纹理线索 $t$。假设从一个固定的特征表示 $(s,t)$ 出发的线性读出器通过最小二乘法进行训练以预测 $y$，这在大样本极限下是基于梯度的训练动态的一个标准分析代理。每个训练样本由三种增强机制之一生成，其混合比例为 $f_A$、$f_B$ 和 $f_C$，且总和为 $1$：\n- 机制 A（比例 $f_A$）：形状和纹理都与标签对齐，模型为 $s = y + \\eta_{s,A}$ 和 $t = y + \\eta_{t,A}$。\n- 机制 B（比例 $f_B$）：形状对齐但纹理被随机化（破坏），模型为 $s = y + \\eta_{s,B}$ 和 $t = \\eta_{t,B}$，与 $y$ 无关。\n- 机制 C（比例 $f_C$）：一种线索冲突操作，其中纹理与标签反对齐，模型为 $s = y + \\eta_{s,C}$ 和 $t = -y + \\eta_{t,C}$。\n\n假设在所有机制中：$y$ 取 $+1$ 或 $-1$ 的概率相等，所有噪声 $\\eta$ 均为零均值，与 $y$ 以及在不同线索和机制之间相互独立，并具有以下方差：$\\sigma_{s,A}^{2} = \\sigma_{s,B}^{2} = \\sigma_{s,C}^{2} = 2.25$，$\\sigma_{t,A}^{2} = \\sigma_{t,C}^{2} = 0.0025$，以及 $\\sigma_{t,B}^{2} = 1.0$。假设数据集构成为 $f_A = 0.9$，$f_B = 0.05$，$f_C = 0.05$。\n\n定义最小二乘线性读出器 $w = (w_s, w_t)^\\top$，它最小化期望平方误差 $E[(y - w_s s - w_t t)^2]$。在大样本极限下，$w$ 满足正规方程 $w = \\Sigma_{xx}^{-1} \\Sigma_{xy}$，其中 $\\Sigma_{xx} = \\mathrm{Cov}([s,t]^\\top)$ 和 $\\Sigma_{xy} = \\mathrm{Cov}([s,t]^\\top, y)$ 是在混合分布下的协方差。\n\n任务：\n1. 从上述生成性假设和正规方程出发，推导 $\\Sigma_{xx}$ 和 $\\Sigma_{xy}$ 作为 $f_A$、$f_B$、$f_C$ 及指定噪声方差的函数的解析表达式。然后计算 $w$，并对于给定的数值，证明 $|w_t| > |w_s|$（纹理偏差）。\n2. 提出一种有原则的重要性重加权方法，用非负因子 $\\alpha_A$、$\\alpha_B$、$\\alpha_C$（不必总和为1）对不同机制的训练样本进行重加权，旨在消除加权总体中纹理和标签之间的净相关性，即在大样本极限下强制实现 $\\mathrm{Cov}_{\\alpha}(t,y) = 0$。采用约定 $\\alpha_A = 1$ 和 $\\alpha_B = 1$，求解能实现 $\\mathrm{Cov}_{\\alpha}(t,y) = 0$ 的单个自由参数 $\\alpha_C$。\n\n给出在指定的 $f_A$、$f_B$ 和 $f_C$ 下实现此目标的 $\\alpha_C$ 的值作为最终答案。最终答案应表示为一个精确的实数（不要四舍五入）。",
            "solution": "根据问题陈述，解答分为两部分。首先，我们推导线性读出器的权重 $w_s$ 和 $w_t$，并证明纹理偏差的存在。其次，我们确定消除纹理-标签相关性所需的重要性权重 $\\alpha_C$。\n\n首先，我们计算所需的协方差矩阵 $\\Sigma_{xx}$ 和 $\\Sigma_{xy}$。总体是三种机制的混合，因此我们使用全期望定律。对于任何随机变量 $Z$，其期望为 $E[Z] = f_A E[Z|A] + f_B E[Z|B] + f_C E[Z|C]$，其中 $A$、$B$ 和 $C$ 表示三种数据生成机制。\n\n首先，我们确定变量的均值。标签 $y$ 是平衡的，所以 $E[y] = \\frac{1}{2}(+1) + \\frac{1}{2}(-1) = 0$。因此，$E[y^2] = \\frac{1}{2}(+1)^2 + \\frac{1}{2}(-1)^2 = 1$。噪声 $\\eta$ 是零均值的，$E[\\eta]=0$，并且与 $y$ 以及相互之间独立。\n\n线索 $s$ 和 $t$ 的均值为：\n$E[s] = E[f_A(y+\\eta_{s,A}) + f_B(y+\\eta_{s,B}) + f_C(y+\\eta_{s,C})] = (f_A+f_B+f_C)E[y] = E[y] = 0$。\n$E[t] = E[f_A(y+\\eta_{t,A}) + f_B(\\eta_{t,B}) + f_C(-y+\\eta_{t,C})] = f_A E[y] + f_B E[\\eta_{t,B}] - f_C E[y] + f_C E[\\eta_{t,C}] = 0$。\n由于所有变量 $y, s, t$ 都是零均值，协方差简化为乘积的期望，例如 $\\mathrm{Cov}(s,y) = E[sy]$。\n\n我们计算协方差矩阵所需的乘积的条件期望。\n对于机制 A（$s = y + \\eta_{s,A}$，$t = y + \\eta_{t,A}$）：\n$E[sy|A] = E[(y+\\eta_{s,A})y] = E[y^2] = 1$。\n$E[ty|A] = E[(y+\\eta_{t,A})y] = E[y^2] = 1$。\n$E[s^2|A] = E[(y+\\eta_{s,A})^2] = E[y^2] + E[\\eta_{s,A}^2] = 1 + \\sigma_{s,A}^2$。\n$E[t^2|A] = E[(y+\\eta_{t,A})^2] = E[y^2] + E[\\eta_{t,A}^2] = 1 + \\sigma_{t,A}^2$。\n$E[st|A] = E[(y+\\eta_{s,A})(y+\\eta_{t,A})] = E[y^2] = 1$。\n\n对于机制 B（$s = y + \\eta_{s,B}$，$t = \\eta_{t,B}$）：\n$E[sy|B] = E[(y+\\eta_{s,B})y] = E[y^2] = 1$。\n$E[ty|B] = E[\\eta_{t,B}y] = E[\\eta_{t,B}]E[y] = 0$。\n$E[s^2|B] = E[(y+\\eta_{s,B})^2] = 1 + \\sigma_{s,B}^2$。\n$E[t^2|B] = E[\\eta_{t,B}^2] = \\sigma_{t,B}^2$。\n$E[st|B] = E[(y+\\eta_{s,B})\\eta_{t,B}] = 0$。\n\n对于机制 C（$s = y + \\eta_{s,C}$，$t = -y + \\eta_{t,C}$）：\n$E[sy|C] = E[(y+\\eta_{s,C})y] = E[y^2] = 1$。\n$E[ty|C] = E[(-y+\\eta_{t,C})y] = -E[y^2] = -1$。\n$E[s^2|C] = E[(y+\\eta_{s,C})^2] = 1 + \\sigma_{s,C}^2$。\n$E[t^2|C] = E[(-y+\\eta_{t,C})^2] = E[y^2] + E[\\eta_{t,C}^2] = 1 + \\sigma_{t,C}^2$。\n$E[st|C] = E[(y+\\eta_{s,C})(-y+\\eta_{t,C})] = -E[y^2] = -1$。\n\n现在我们使用混合比例 $f_A, f_B, f_C$ 将这些组合起来。\n向量 $\\Sigma_{xy}$：\n$\\mathrm{Cov}(s,y) = E[sy] = f_A(1) + f_B(1) + f_C(1) = 1$。\n$\\mathrm{Cov}(t,y) = E[ty] = f_A(1) + f_B(0) + f_C(-1) = f_A - f_C$。\n所以，$\\Sigma_{xy} = \\begin{pmatrix} 1 \\\\ f_A - f_C \\end{pmatrix}$。\n\n矩阵 $\\Sigma_{xx}$：\n$\\mathrm{Var}(s) = E[s^2] = f_A(1+\\sigma_{s,A}^2) + f_B(1+\\sigma_{s,B}^2) + f_C(1+\\sigma_{s,C}^2)$。\n给定 $\\sigma_{s,A}^2 = \\sigma_{s,B}^2 = \\sigma_{s,C}^2 = \\sigma_s^2$，这等于 $(f_A+f_B+f_C)(1+\\sigma_s^2) = 1 + \\sigma_s^2$。\n$\\mathrm{Var}(t) = E[t^2] = f_A(1+\\sigma_{t,A}^2) + f_B\\sigma_{t,B}^2 + f_C(1+\\sigma_{t,C}^2)$。\n$\\mathrm{Cov}(s,t) = E[st] = f_A(1) + f_B(0) + f_C(-1) = f_A-f_C$。\n所以，$\\Sigma_{xx} = \\begin{pmatrix} 1+\\sigma_s^2  f_A-f_C \\\\ f_A-f_C  f_A(1+\\sigma_{t,A}^2) + f_B\\sigma_{t,B}^2 + f_C(1+\\sigma_{t,C}^2) \\end{pmatrix}$。\n\n第1部分：计算权重并证明纹理偏差。\n我们代入给定的数值：$f_A = 0.9$，$f_B = 0.05$，$f_C = 0.05$。\n$\\sigma_s^2 = 2.25$，$\\sigma_{t,A}^2 = 0.0025$，$\\sigma_{t,B}^2 = 1.0$，$\\sigma_{t,C}^2 = 0.0025$。\n\n$f_A - f_C = 0.9 - 0.05 = 0.85$。\n$\\Sigma_{xy} = \\begin{pmatrix} 1 \\\\ 0.85 \\end{pmatrix}$。\n\n$\\mathrm{Var}(s) = 1 + 2.25 = 3.25$。\n$\\mathrm{Var}(t) = 0.9(1+0.0025) + 0.05(1.0) + 0.05(1+0.0025) = 0.9(1.0025) + 0.05 + 0.05(1.0025) = 0.95(1.0025) + 0.05 = 0.952375 + 0.05 = 1.002375$。\n$\\mathrm{Cov}(s,t) = 0.85$。\n$\\Sigma_{xx} = \\begin{pmatrix} 3.25  0.85 \\\\ 0.85  1.002375 \\end{pmatrix}$。\n\n权重为 $w = \\Sigma_{xx}^{-1} \\Sigma_{xy}$。\n$\\Sigma_{xx}$ 的逆矩阵是 $\\frac{1}{\\det(\\Sigma_{xx})} \\begin{pmatrix} \\mathrm{Var}(t)  -\\mathrm{Cov}(s,t) \\\\ -\\mathrm{Cov}(s,t)  \\mathrm{Var}(s) \\end{pmatrix}$。\n$\\det(\\Sigma_{xx}) = \\mathrm{Var}(s)\\mathrm{Var}(t) - (\\mathrm{Cov}(s,t))^2 = (3.25)(1.002375) - (0.85)^2 = 3.25771875 - 0.7225 = 2.53521875$。\n$w = \\begin{pmatrix} w_s \\\\ w_t \\end{pmatrix} = \\frac{1}{2.53521875} \\begin{pmatrix} 1.002375  -0.85 \\\\ -0.85  3.25 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.85 \\end{pmatrix}$。\n\n$w_s = \\frac{1 \\times 1.002375 - 0.85 \\times 0.85}{2.53521875} = \\frac{1.002375 - 0.7225}{2.53521875} = \\frac{0.279875}{2.53521875}$。\n$w_t = \\frac{-0.85 \\times 1 + 3.25 \\times 0.85}{2.53521875} = \\frac{0.85(3.25 - 1)}{2.53521875} = \\frac{0.85 \\times 2.25}{2.53521875} = \\frac{1.9125}{2.53521875}$。\n\n为了证明纹理偏差（$|w_t| > |w_s|$），我们比较分子，因为分母是正的并且对两者是共同的。\n$w_s$ 的分子：$0.279875$。\n$w_t$ 的分子：$1.9125$。\n由于 $1.9125 > 0.279875$，且两者均为正，我们有 $|w_t| > |w_s|$。这证明了存在纹理偏差，意味着模型更多地依赖纹理线索而非形状线索。\n\n第2部分：重要性重加权以消除纹理相关性。\n我们为每个机制引入非负的重要性权重 $\\alpha_A, \\alpha_B, \\alpha_C$。现在，线性读出器通过最小化加权期望平方误差 $E[\\alpha(y - w \\cdot x)^2]$ 来训练。大样本解满足加权正规方程 $w_\\alpha = (E[\\alpha x x^\\top])^{-1} E[\\alpha x y]$。\n问题要求消除纹理和标签之间的净相关性，我们将其解释为将交叉协方差项 $E[\\alpha x y]$ 的纹理-标签分量设置为零。该分量为 $E[\\alpha t y]$。\n$E[\\alpha t y] = \\sum_{R \\in \\{A,B,C\\}} f_R \\alpha_R E[ty|R]$。\n使用我们之前计算的条件期望：\n$E[\\alpha t y] = f_A \\alpha_A E[ty|A] + f_B \\alpha_B E[ty|B] + f_C \\alpha_C E[ty|C]$\n$E[\\alpha t y] = f_A \\alpha_A (1) + f_B \\alpha_B (0) + f_C \\alpha_C (-1) = f_A \\alpha_A - f_C \\alpha_C$。\n\n我们将其设为零以满足条件：\n$f_A \\alpha_A - f_C \\alpha_C = 0$。\n这得到 $\\alpha_C = \\frac{f_A \\alpha_A}{f_C}$。\n\n使用指定的约定 $\\alpha_A = 1$ 和 $\\alpha_B = 1$，以及数据集构成 $f_A = 0.9$ 和 $f_C = 0.05$，我们可以解出 $\\alpha_C$：\n$\\alpha_C = \\frac{0.9 \\times 1}{0.05} = \\frac{90}{5} = 18$。\n因子 $\\alpha_C = 18$ 是非负的，符合要求。这意味着我们必须将来自线索冲突机制 $C$ 的每个样本的权重相对于来自机制 $A$ 和 $B$ 的样本（权重为1）上调18倍，以消除纹理和标签之间学习到的关联的偏差。",
            "answer": "$$\\boxed{18}$$"
        },
        {
            "introduction": "为了理解像DCN或大脑这样的复杂系统中不同组件的功能，研究人员经常采用“损伤”研究方法。这个计算练习将指导您模拟一个“虚拟损伤”实验，通过编程方式从DCN模型中“移除”被识别为关键的特征图，并测量其对分类能力的影响 。该练习让您亲身体验一种强大的计算神经科学技术，用于探究神经网络表征中不同单元的功能作用。",
            "id": "3973984",
            "problem": "给定一个模拟大脑腹侧视觉流（VVS）的深度卷积网络（DCN）特征图的合成表示。考虑一个包含三个类别的多分类场景。每个样本由一个特征向量表示，该向量对应于DCN单层中 $D$ 个特征图（通道）的响应。目标是量化当移除一组由显著性度量识别为关键的特征图时，类别的线性可解码性如何变化，并将此变化与特定类别的缺陷联系起来。\n\n从以下定义和原则开始：\n\n- 设有 $C$ 个类别，索引为 $c \\in \\{0,1,2\\}$。每个样本都有一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^D$。\n- 线性解码器以一对多（one-versus-rest）的方式进行训练。对于类别 $c$，定义目标标签 $y_{i}^{(c)} \\in \\{+1,-1\\}$，如果样本 $i$ 属于类别 $c$，则 $y_{i}^{(c)} = +1$，否则 $y_{i}^{(c)} = -1$。\n- 为了包含截距（偏置）项而不对其进行惩罚，将特征向量增广为 $\\tilde{\\mathbf{x}} = [\\mathbf{x}^\\top, 1]^\\top \\in \\mathbb{R}^{D+1}$，并相应地定义一个对角惩罚矩阵 $\\mathbf{P} \\in \\mathbb{R}^{(D+1)\\times(D+1)}$，其中偏置项的 $\\mathbf{P}_{00} = 0$，所有特征维度 $k \\in \\{1,\\dots,D\\}$ 的 $\\mathbf{P}_{kk} = 1$。\n- 对于类别 $c$，通过求解以下问题来训练岭回归解码器\n$$\n\\mathbf{w}^{(c)} = \\arg\\min_{\\mathbf{w} \\in \\mathbb{R}^{D+1}} \\sum_{i} \\left( y_{i}^{(c)} - \\tilde{\\mathbf{x}}_{i}^\\top \\mathbf{w} \\right)^2 + \\lambda \\, \\mathbf{w}^\\top \\mathbf{P} \\mathbf{w},\n$$\n其闭式解为\n$$\n\\mathbf{w}^{(c)} = \\left( \\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}} + \\lambda \\mathbf{P} \\right)^{-1} \\tilde{\\mathbf{X}}^\\top \\mathbf{y}^{(c)},\n$$\n其中 $\\tilde{\\mathbf{X}}$ 是增广训练特征矩阵，$\\mathbf{y}^{(c)}$ 是类别 $c$ 的一对多标签向量。\n- 对于一个具有增广特征 $\\tilde{\\mathbf{x}}$ 的测试样本，类别 $c$ 的线性分数为 $s^{(c)}(\\tilde{\\mathbf{x}}) = \\tilde{\\mathbf{x}}^\\top \\mathbf{w}^{(c)}$。多分类预测通过赢家通吃规则执行，选择 $\\hat{c} = \\arg\\max_{c} s^{(c)}(\\tilde{\\mathbf{x}})$。\n- 类别 $c$ 的可解码性被度量为来自类别 $c$ 的测试样本中被多分类解码器正确预测的比例。将此准确率记为 $A_c \\in [0,1]$。\n\n为了识别类别 $c$ 的关键特征图，使用一种适用于线性解码器的基于梯度的显著性度量。$s^{(c)}(\\tilde{\\mathbf{x}})$ 相对于非偏置特征维度的梯度是权重向量 $\\mathbf{w}^{(c)}_{\\text{feat}} \\in \\mathbb{R}^D$（不包括偏置项）。将特征 $k$ 的特定类别显著性定义为\n$$\nS_c(k) = \\left| w^{(c)}_{\\text{feat},k} \\right| \\cdot \\mathbb{E}_{\\mathbf{x} \\sim \\text{test}, \\, y=c}\\left[ \\left| x_k \\right| \\right],\n$$\n该公式通过在留出的测试数据上，类别 $c$ 内该特征的平均绝对激活值来缩放其绝对权重。定义一个全局显著性为\n$$\nS_{\\text{global}}(k) = \\sum_{c=0}^{C-1} S_c(k).\n$$\n\n特征移除定义为：通过根据指定类别 $c$ 的 $S_c(k)$（按类别选择）或根据 $S_{\\text{global}}(k)$（全局选择）选择大小为 $K$ 的最显著特征索引集合 $\\mathcal{M} \\subset \\{1,\\dots,D\\}$，然后在训练和测试表示中将这些特征维度置零：\n$$\nx_k \\leftarrow 0 \\quad \\text{for all } k \\in \\mathcal{M}.\n$$\n\n对于给定的正则化参数 $\\lambda$ 和选择模式，计算：\n1. 基线可解码性 $A_c^{\\text{base}}$，通过在标准化的原始特征上训练解码器并在标准化的测试特征上进行评估得出。\n2. 修改后的可解码性 $A_c^{\\text{mod}}$，通过在将所选特征维度置零后的标准化特征上训练解码器，并在经过同样置零操作后的标准化测试特征上进行评估得出。\n3. 特定类别的缺陷\n$$\n\\Delta_c = A_c^{\\text{base}} - A_c^{\\text{mod}},\n$$\n表示为 $[ -1, 1 ]$ 区间内的一个小数。\n\n数据集生成协议（固定的且可复现的），适用于 $C=3$ 个类别和 $D=30$ 个特征图：\n- 将特征索引划分为三个不相交的组 $G_0 = \\{1,\\dots,10\\}$，$G_1 = \\{11,\\dots,20\\}$ 和 $G_2 = \\{21,\\dots,30\\}$。\n- 对每个类别 $c$，抽取训练样本：对于 $G_c$ 中的特征，从均值为 $\\mu_{\\text{high}} = 2.0$、标准差为 $\\sigma = 0.5$ 的高斯分布中采样；对于所有其他特征，从均值为 $0$、标准差为 $\\sigma = 0.5$ 的高斯分布中采样。对测试样本执行相同操作。\n- 使用 $N_{\\text{train}} = 300$ 个训练样本（在类别间均匀分布）和 $N_{\\text{test}} = 180$ 个测试样本（均匀分布），并使用固定的随机种子以确保可复现性。\n- 通过 z-score 对每个特征维度进行标准化，使用仅在训练数据上计算的统计量：对每个特征 $k$，计算训练均值 $m_k$ 和标准差 $s_k$；然后通过 $x_k \\leftarrow (x_k - m_k) / s_k$ 转换所有输入，并遵循约定，如果 $s_k = 0$，则设 $s_k = 1$。\n\n测试套件：\n- 每个测试用例是一个元组 $(K, \\lambda, \\text{mode}, c^\\star)$，描述如下：\n    - $K$：要移除的显著特征数量（一个整数）。\n    - $\\lambda$：岭正则化系数（一个正浮点数）。\n    - `mode`：可以是 \"per\\_class\" 或 \"global\"，指示是为特定类别 $c^\\star$ 计算显著性还是跨所有类别计算。\n    - $c^\\star$：用于按类别选择的类别索引（$\\{0,1,2\\}$ 中的一个整数）；如果 `mode` = `global`，则设置 $c^\\star = -1$ 并忽略它。\n- 使用以下测试用例：\n    1. $(5, 0.01, \\text{\"per\\_class\"}, 0)$：移除少量类别 0 关键特征的常规路径。\n    2. $(0, 0.01, \\text{\"per\\_class\"}, 0)$：不进行移除的边界条件。\n    3. $(10, 0.50, \\text{\"per\\_class\"}, 1)$：在较强正则化下移除一整块类别 1 诊断性特征的边缘情况。\n    4. $(12, 0.01, \\text{\"global\"}, -1)$：全局移除跨类别显著特征。\n\n你的程序应该实现上述协议，为每个测试用例计算 $c \\in \\{0,1,2\\}$ 的缺陷值 $\\Delta_c$，并生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。最终输出应为扁平化的缺陷值列表，按测试用例排序，然后在每个测试用例内按类别索引排序，即 $[\\Delta^{(1)}_0, \\Delta^{(1)}_1, \\Delta^{(1)}_2, \\Delta^{(2)}_0, \\dots, \\Delta^{(4)}_2]$，其中上标表示上面列表中的测试用例索引。",
            "solution": "该问题的解决方案遵循一个系统的计算流程，该流程通过模拟“虚拟损伤”实验来量化特征移除对分类性能的影响。为确保结果的可复现性，所有随机过程均使用固定的种子（例如，42）。\n\n1.  **数据生成与标准化**：首先，根据问题中描述的协议生成训练和测试数据集。该协议为三个类别中的每一个类别定义了特定的诊断性特征。然后，使用训练集的统计数据（均值和标准差）对训练集和测试集的所有特征维度进行 z-score 标准化。最后，通过在每个特征向量前添加一个值为1的偏置项来增广数据。\n\n2.  **基线模型训练与评估**：对于每个测试用例中指定的正则化参数 $\\lambda$，我们在完整的、标准化的训练数据上为每个类别训练一个一对多（one-versus-rest）岭回归解码器。使用这些解码器在测试集上进行预测，并计算每个类别的基线准确率 $A_c^{\\text{base}}$。\n\n3.  **显著性计算与特征选择**：利用基线模型训练出的权重，我们根据测试用例指定的模式（`per_class` 或 `global`）计算每个特征的显著性分数。如果模式是 `per_class`，我们使用为特定类别 $c^\\star$ 计算的显著性；如果是 `global`，我们使用所有类别的显著性总和。然后，我们识别出具有最高显著性分数的 $K$ 个特征。\n\n4.  **“损伤”模型训练与评估**：我们通过在标准化数据集中将被选为“最显著”的 $K$ 个特征的对应列置零，来创建一个“已损伤”或修改后的数据集。然后，我们在这个修改后的训练数据上训练一组新的解码器，并在修改后的测试数据上评估它们的性能，得到修改后的准确率 $A_c^{\\text{mod}}$。\n\n5.  **缺陷计算**：最后，对于每个类别 $c$，性能缺陷被计算为基线准确率与修改后准确率之差：$\\Delta_c = A_c^{\\text{base}} - A_c^{\\text{mod}}$。对于 $K=0$ 的情况，由于没有特征被移除，缺陷自然为零。\n\n对所有四个测试用例重复此过程，并将所有计算出的缺陷值按顺序组合，得到最终结果。",
            "answer": "[0.31666667,0.00000000,-0.01666667,0.00000000,0.00000000,0.00000000,0.00000000,1.00000000,0.00000000,0.98333333,0.95000000,1.00000000]"
        }
    ]
}