{
    "hands_on_practices": [
        {
            "introduction": "阿贝尔沙堆模型是自组织临界性的基础范例。通过在一个小网格上手动追踪一次雪崩，你可以建立一个具体的直觉，理解简单的局部相互作用规则如何产生可以遍及整个系统的复杂非线性动力学。这个练习  对于理解不稳定性的传播和稳定化核心机制至关重要。",
            "id": "4142578",
            "problem": "考虑一个具有开放边界的二维方格晶格上的阿贝尔沙堆模型（ASM）。设晶格为 $3 \\times 3$ 的网格，其格点由坐标 $(i,j) \\in \\{-1,0,1\\} \\times \\{-1,0,1\\}$ 索引，原点为 $(0,0)$。每个格点 $(i,j)$ 具有一个整数高度 $h(i,j) \\in \\mathbb{Z}_{\\ge 0}$。如果一个格点的高度至少为 $4$，则该格点是不稳定的。在格点 $(i,j)$ 处的一次坍塌会使 $h(i,j)$ 减少 $4$，并使其每个最近邻（内部最多四个，边界上较少）的高度增加 $1$。在开放边界条件下，任何将被送到 $3 \\times 3$ 网格之外的邻居的沙粒都会被耗散并永久离开系统。\n\n从所有九个格点高度均为 $h(i,j) = 3$ 的最大稳定初始构型开始。在原点 $(0,0)$ 处添加一个沙粒，并通过迭代地坍塌不稳定格点来稳定该构型，直到所有格点都变得稳定。通过明确列出您应用的坍塌序列并报告所有格点的最终稳定高度来演示稳定过程。\n\n在此稳定过程中，所有格点上发生的坍塌总数是多少？请以单个整数形式提供您的答案。无需四舍五入，也不涉及单位。",
            "solution": "该问题陈述已被验证，并被认为是复杂自适应系统建模领域内，特别是阿贝尔沙堆模型（ASM）领域中，一个适定的、有科学依据的问题。\n\n该系统是一个 $3 \\times 3$ 的格点网格，由坐标 $(i,j)$ 索引，其中 $i, j \\in \\{-1, 0, 1\\}$。每个格点 $(i,j)$ 都有一个整数高度 $h(i,j)$。如果一个格点的高度 $h(i,j) \\ge 4$，则该格点是不稳定的。在格点 $(i,j)$ 处的一次坍塌会导致高度发生如下变化：\n$h(i,j) \\to h(i,j) - 4$\n$h(\\text{neighbors}) \\to h(\\text{neighbors}) + 1$\n系统具有开放边界，因此任何会流向网格外部格点的沙粒都会丢失。\n\n初始状态是最大稳定构型，其中所有格点的高度均为 $3$。我们可以将高度构型表示为一个矩阵 $H$，其中矩阵索引 $(r,c)$（$r,c \\in \\{1,2,3\\}$）对应于网格坐标 $(j-2, 2-r)$。\n初始构型 $H_{init}$ 为：\n$$ H_{init} = \\begin{pmatrix} 3  3  3 \\\\ 3  3  3 \\\\ 3  3  3 \\end{pmatrix} $$\n\n该过程从向原点，即格点 $(0,0)$ 添加一个沙粒开始。这对应于网格的中心。格点 $(0,0)$ 的高度变为 $3+1=4$。现在的构型是：\n$$ H_0 = \\begin{pmatrix} 3  3  3 \\\\ 3  4  3 \\\\ 3  3  3 \\end{pmatrix} $$\n格点 $(0,0)$ 现在是不稳定的。稳定过程以一系列坍塌的形式进行。ASM 的一个关键性质是，最终构型和总坍塌次数与不稳定格点被坍塌的顺序无关。\n\n**第一步：第一次坍塌**\n唯一不稳定的格点是 $(0,0)$，其高度 $h(0,0)=4$。我们使该格点坍塌。\n- 格点 $(0,0)$ 的高度减少 $4$：$h(0,0) \\to 4-4=0$。\n- 它的四个最近邻 $(0,-1)$、$(0,1)$、$(-1,0)$ 和 $(1,0)$ 的高度各增加 $1$。它们的高度变为 $3+1=4$。\n得到的构型 $H_1$ 是：\n$$ H_1 = \\begin{pmatrix} 3  4  3 \\\\ 4  0  4 \\\\ 3  4  3 \\end{pmatrix} $$\n到目前为止的坍塌次数为 $1$。\n\n**第二步：第二轮坍塌**\n构型 $H_1$ 有四个不稳定格点：四个边缘格点 $(0,-1)$、$(0,1)$、$(-1,0)$ 和 $(1,0)$，它们的高度都为 $4$。我们使这四个格点坍塌。顺序无关紧要。让我们分析这四次坍塌的集体效应。\n- 四个格点中的每一个都坍塌一次。它们的高度减少 $4$：$h \\to 4-4=0$。\n- 格点 $(0,0)$：它是所有四个坍塌格点的邻居。其高度增加 $4$：$h(0,0) \\to 0+4=4$。\n- 角格点 $(\\pm 1, \\pm 1)$：每个角格点是两个坍塌边缘格点的邻居。例如，格点 $(-1,-1)$ 是 $(-1,0)$ 和 $(0,-1)$ 的邻居。因此，每个角格点的高度增加 $2$：$h \\to 3+2=5$。\n得到的构型 $H_2$ 是：\n$$ H_2 = \\begin{pmatrix} 5  0  5 \\\\ 0  4  0 \\\\ 5  0  5 \\end{pmatrix} $$\n这一轮的坍塌次数为 $4$。总坍塌次数现在是 $1+4=5$。\n\n**第三步：最后一轮坍塌**\n构型 $H_2$ 有五个不稳定格点：四个高度为 $5$ 的角格点和高度为 $4$ 的中心格点。我们继续使这些格点坍塌。\n1.  首先，让我们使中心格点 $(0,0)$ 坍塌，其高度 $h(0,0)=4$。\n    - 其高度变为 $h(0,0) \\to 4-4=0$。\n    - 它的四个邻居，即边缘格点，当前高度为 $0$。它们的高度增加 $1$：$h \\to 0+1=1$。\n    这次坍塌是总共第 $6$ 次坍塌。构型变为：\n    $$ H_{2a} = \\begin{pmatrix} 5  1  5 \\\\ 1  0  1 \\\\ 5  1  5 \\end{pmatrix} $$\n2.  现在，四个角格点 $(\\pm 1, \\pm 1)$ 是不稳定的，高度为 $5$。我们使这四个格点坍塌。\n    - 它们的高度减少 $4$：$h \\to 5-4=1$。\n    - 每个角格点在网格上有两个邻居。例如，坍塌 $(-1,-1)$ 会向 $(-1,0)$ 和 $(0,-1)$ 各添加一个沙粒。\n    - 让我们看一下对一个边缘格点的影响，比如 $(-1,0)$。它当前的高度是 $1$。它从 $(-1,-1)$ 的坍塌中接收一个沙粒，从 $(-1,1)$ 的坍塌中接收一个沙粒。其高度变为 $h(-1,0) \\to 1+1+1=3$。\n    - 根据对称性，所有四个边缘格点的高度将从 $1$ 变为 $3$。\n这一轮涉及另外 $4$ 次坍塌，每个角格点一次。最终的稳定构型 $H_f$ 是：\n$$ H_f = \\begin{pmatrix} 1  3  1 \\\\ 3  0  3 \\\\ 1  3  1 \\end{pmatrix} $$\n所有高度现在都小于 $4$，因此构型是稳定的。\n\n**坍塌总结**\n为了找到总坍塌次数，我们将每个阶段的坍塌次数相加：\n- 第一步：格点 $(0,0)$ 处 $1$ 次坍塌。\n- 第二步：$4$ 次坍塌，四个边缘格点各一次。\n- 第三步：格点 $(0,0)$ 处 $1$ 次坍塌和四个角格点各一次的 $4$ 次坍塌。总共是 $5$ 次坍塌。\n\n总坍塌次数 = $1 + 4 + 5 = 10$。\n\n所有格点 $(i,j)$ 的最终稳定高度为：\n- $h(0,0) = 0$\n- $h(\\pm 1, 0) = h(0, \\pm 1) = 3$\n- $h(\\pm 1, \\pm 1) = 1$\n\n在稳定过程中发生的总坍塌次数是 $10$。",
            "answer": "$$\\boxed{10}$$"
        },
        {
            "introduction": "虽然理想化的模型很有用，但像神经网络这样的真实世界系统本质上是耗散的。这个问题  从离散规则转向更抽象的连续描述，将雪崩建模为一个带有“泄漏”项的扩散过程，该项代表了这种耗散。在这里应用标度分析将帮助你推导出这种微观耗散力如何引入一个特征尺度，从而有效地限制了雪崩的最大规模。",
            "id": "1931689",
            "problem": "考虑一个“漏沙堆”的简化模型，这是一个表现出自组织临界性的系统。该系统建立在一个大的 $d$ 维晶格上。当晶格上的一个位置变得不稳定时，它会坍塌，将其不稳定性传递给其邻居。这个过程可以引发连锁反应，即“雪崩”。这个不稳定前沿在晶格中的传播可以被建模为一个有效扩散常数为 $D$ 的扩散过程。\n\n与标准沙堆模型不同，在这个“漏沙”版本中，每个“不稳定单元”（可以被认为是一个不稳定的沙粒）在单位时间内都有一个小的、恒定的概率 $\\epsilon$ 自发地从系统中消失。这种“泄漏”意味着任何正在进行的雪崩最终都会因为维持其存在的不稳定性消失而熄灭。因此，雪崩的特征寿命由这个泄漏率决定。\n\n假设雪崩的最大尺寸 $S_{max}$（定义为雪崩中总的坍塌事件数）与雪崩因泄漏而终止前所达到的特征空间范围内包含的总位置数成正比。比例常数是一个无量纲因子 $C$。\n\n推导最大雪崩尺寸 $S_{max}$ 的表达式，用有效扩散常数 $D$、泄漏率 $\\epsilon$、空间维度 $d$ 和比例常数 $C$ 来表示。",
            "solution": "通过一个带有线性泄漏项的扩散方程来模拟不稳定密度 $\\rho(\\mathbf{r},t)$ 的传播，\n$$\n\\frac{\\partial \\rho}{\\partial t}=D\\nabla^{2}\\rho-\\epsilon\\,\\rho.\n$$\n泄漏引入了一个由速率 $\\epsilon$ 设定的特征衰减时间尺度，即\n$$\n\\tau\\sim \\frac{1}{\\epsilon}.\n$$\n在此期间，具有常数 $D$ 的扩散探索了一个特征长度\n$$\n\\ell\\sim \\sqrt{D\\,\\tau}=\\sqrt{\\frac{D}{\\epsilon}}.\n$$\n等效地，对于在长度 $\\ell$ 上变化的模式，平衡扩散项和泄漏项的量级可得 $D/\\ell^{2}\\sim \\epsilon$，这也得出了相同的 $\\ell$。\n\n雪崩探索的区域在 $d$ 维空间中的特征体积与 $\\ell^{d}$ 成比例，因此该区域内的总位置数与 $\\ell^{d}$ 成正比，其中与晶格相关的常数被吸收到一个无量纲因子中。根据假设，最大雪崩尺寸 $S_{max}$ 与该位置数成正比，比例常数为 $C$，可得\n$$\nS_{max}=C\\,\\ell^{d}=C\\left(\\frac{D}{\\epsilon}\\right)^{\\frac{d}{2}}.\n$$\n这表达了由泄漏控制的寿命和扩散限制的空间扩展所设定的截止尺寸。",
            "answer": "$$\\boxed{C\\left(\\frac{D}{\\epsilon}\\right)^{\\frac{d}{2}}}$$"
        },
        {
            "introduction": "在经验数据中识别自组织临界性是一项重大挑战，它需要的不仅仅是对图表的视觉检查。这个高级计算练习  介绍了现代研究中使用的严谨统计方法，用于检验幂律分布并与合理的替代方案进行比较。完成这项实践将使你掌握批判性评估自组织临界性主张的工具，并避免常见的统计陷阱。",
            "id": "4301973",
            "problem": "设计并实现一个完整的程序，该程序针对一组代表复杂系统中事件大小的合成数据集，评估数据是否支持将纯幂律作为其尾部分布的最简约模型，同时避免将噪声过度解释为自组织临界（SOC）。您的解决方案必须基于概率论和统计推断的基本原理，并使用似然比来比较纯幂律模型与其他替代模型。最终输出必须为单行，包含一个布尔值列表，总结对每个数据集的决策。\n\n基本定义与假设：\n- 自组织临界（SOC）在经验上与无标度、重尾的事件规模分布相关联。一个典型模型是事件大小 $x \\ge x_{\\min}$ 的连续下截断幂律，其概率密度函数（pdf）为\n$$\nf_{\\mathrm{PL}}(x \\mid \\alpha, x_{\\min}) = (\\alpha - 1) x_{\\min}^{\\alpha - 1} x^{-\\alpha}, \\quad \\alpha > 1, \\; x \\ge x_{\\min}.\n$$\n- 竞争模型包括：\n  1. 带速率参数 $\\lambda$ 的下截断指数分布，\n  $$\n  f_{\\mathrm{EXP}}(x \\mid \\lambda, x_{\\min}) = \\lambda \\exp\\big(-\\lambda (x - x_{\\min})\\big), \\quad \\lambda > 0, \\; x \\ge x_{\\min}.\n  $$\n  2. 带参数 $\\mu$ 和 $\\sigma$ 的下截断对数正态分布，\n  $$\n  f_{\\mathrm{LN}}(x \\mid \\mu, \\sigma, x_{\\min}) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} \\exp\\!\\left( -\\frac{(\\ln x - \\mu)^2}{2\\sigma^2} \\right) \\bigg/ \\Big(1 - \\Phi\\!\\Big( \\frac{\\ln x_{\\min} - \\mu}{\\sigma} \\Big) \\Big), \\quad \\sigma > 0, \\; x \\ge x_{\\min},\n  $$\n  其中 $\\Phi(\\cdot)$ 是标准正态累积分布函数。\n  3. 在 $x \\in [x_{\\min}, x_{\\max}]$ 上带指数 $\\alpha$ 的双截断幂律，\n  $$\n  f_{\\mathrm{TPL}}(x \\mid \\alpha, x_{\\min}, x_{\\max}) = \\frac{x^{-\\alpha}}{Z(\\alpha;x_{\\min},x_{\\max})}, \\quad Z(\\alpha;x_{\\min},x_{\\max}) = \n  \\begin{cases}\n    \\dfrac{x_{\\min}^{1-\\alpha} - x_{\\max}^{1-\\alpha}}{1 - \\alpha},  \\alpha \\ne 1, \\\\\n    \\ln\\!\\left(\\dfrac{x_{\\max}}{x_{\\min}}\\right),  \\alpha = 1,\n  \\end{cases}\n  $$\n  其中 $x_{\\max}$ 取为样本最大值，以避免自由边界的病态问题。\n\n- 最大似然估计（MLE）用于对每个尾部模型中 $x_i \\ge x_{\\min}$ 的观测值进行参数拟合。对于下截断幂律，其指数的 MLE 为\n$$\n\\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)},\n$$\n其中 $n$ 是尾部观测值的数量。对于下截断指数分布，\n$$\n\\hat{\\lambda} = \\frac{1}{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - x_{\\min})}.\n$$\n下截断对数正态分布 $(\\mu, \\sigma)$ 需要通过数值方法最大化其对数似然。双截断幂律的指数 $\\alpha$ 是在将 $x_{\\max}$ 固定为样本最大值的情况下，通过数值最大化对数似然来估计的。\n\n- Kolmogorov–Smirnov（KS）方法用于为纯幂律尾部选择下截断值 $x_{\\min}$：在一组候选截断值 $x_{\\min}$（这些值至少保留 $n_{\\text{tail}}$ 个观测值）上，计算 $\\hat{\\alpha}(x_{\\min})$ 以及尾部经验累积分布函数与 $F_{\\mathrm{PL}}(x \\mid \\hat{\\alpha}(x_{\\min}), x_{\\min}) = 1 - (x/x_{\\min})^{1-\\hat{\\alpha}}$ 之间的单样本 KS 距离。选择使 KS 距离最小的 $x_{\\min}$，并保留相应的尾部数据。\n\n- 模型比较使用对数似然比和针对非嵌套模型的 Vuong 检验，并采用贝叶斯信息准则（BIC）进行校正。给定两个已拟合的模型 $M_1$ 和 $M_2$，定义每个观测值的对数似然贡献为 $\\ell_{1,i}$ 和 $\\ell_{2,i}$。对于 $n$ 个尾部观测值，定义\n$$\n\\delta_i = \\ell_{1,i} - \\ell_{2,i}, \\quad \\bar{\\delta} = \\frac{1}{n} \\sum_{i=1}^{n} \\delta_i, \\quad s = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^{n} (\\delta_i - \\bar{\\delta})^2 }.\n$$\n设 $k_1$ 和 $k_2$ 分别是模型 $M_1$ 和 $M_2$ 的参数数量。经 BIC 校正的均值为\n$$\n\\bar{\\delta}_{\\mathrm{BIC}} = \\bar{\\delta} - \\frac{(k_1 - k_2) \\ln n}{2n}.\n$$\nVuong $z$-统计量为\n$$\nz = \\frac{\\sqrt{n} \\, \\bar{\\delta}_{\\mathrm{BIC}}}{s},\n$$\n其双边 $p$-值在标准正态分布下计算。总对数似然比为 $L = \\sum_{i=1}^{n} \\delta_i$。\n\n- 为避免将噪声过度解释为 SOC 的决策准则：\n  1. 尾部样本量必须满足 $n_{\\text{tail}} \\ge 100$。\n  2. 估计出的纯幂律指数必须落在物理上合理的重尾范围 $1.5 \\le \\hat{\\alpha} \\le 3.5$ 内。\n  3. 针对三次两两比较使用 Bonferroni 校正，要求在与下截断指数分布和下截断对数正态分布比较时，$p  0.05/3$ 且对数似然比为正（有利于纯幂律模型）。\n  4. 纯幂律模型不应显著差于双截断幂律模型；具体而言，要么对数似然比为非负，要么在经 Bonferroni 调整的 $0.05/3$ 水平上 $p$-值不显著。\n  5. 所有推断都在由 KS 方法选定的 $x_{\\min}$ 所定义的尾部上进行。\n\n您的程序必须：\n- 生成以下五个合成数据集，每个数据集使用固定的随机种子，然后应用上述尾部选择、拟合和模型比较程序，为每个数据集生成一个布尔值，以指示数据是否满足支持 SOC 的准则。\n  1. 数据集 A（理想路径）：连续纯幂律分布，其中 $n = 5000$，$\\alpha = 2.5$，$x_{\\min} = 1$，种子 $= 12345$。\n  2. 数据集 B（替代重尾）：连续对数正态分布，其中 $n = 5000$，$\\mu = 0$，$\\sigma = 1$，种子 $= 23456$。\n  3. 数据集 C（瘦尾）：下截断指数分布，其中 $n = 5000$，$\\lambda = 1$，$x_{\\min} = 1$，种子 $= 34567$。\n  4. 数据集 D（有限支撑重尾）：双截断幂律分布，其中 $n = 5000$，$\\alpha = 2.2$，$x_{\\min} = 1$，$x_{\\max} = 100$，种子 $= 45678$。\n  5. 数据集 E（边缘案例）：连续纯幂律分布，小样本 $n = 50$，$\\alpha = 2.5$，$x_{\\min} = 1$，种子 $= 56789$。\n- 对每个数据集，通过在一组至少保留 50 个尾部观测值的候选集上进行 KS 最小化来估计 $x_{\\min}$，在选定的尾部上拟合所有模型，为配对 $(\\mathrm{PL}, \\mathrm{EXP})$、$(\\mathrm{PL}, \\mathrm{LN})$ 和 $(\\mathrm{PL}, \\mathrm{TPL})$ 计算经 BIC 校正的 Vuong 统计量，并应用上述决策准则以产生单个布尔结果。\n\n要求的最终输出格式：\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是一个布尔值，按顺序对应于数据集 A 到 E。\n\n本问题不涉及物理单位。角度不适用。所有数值结果必须以布尔值的形式报告，在任何情况下均不得使用百分号。程序必须是完全确定性的，使用指定的种子，并且不得要求任何用户输入或外部文件。",
            "solution": "该问题要求设计并实现一个严谨的统计流程，用以评估合成数据集是否展现出与自组织临界（SOC）相符的行为，而 SOC 的经验特征是事件规模呈纯幂律分布。该解决方案通过一个基于最大似然估计和似然比检验的原则性框架，将幂律模型与合理的替代模型——指数分布、对数正态分布和截断幂律分布——进行系统性比较，从而避免对 SOC 的虚假识别。\n\n### 1. 整体方法框架\n\n解决方案的核心是应用于每个数据集的一个多阶段验证过程。该过程设计得较为保守，仅在有强有力的统计证据时才接受幂律为最佳模型。这些阶段包括：\n1.  **数据生成**：根据指定的分布创建合成数据集，作为测试案例。\n2.  **尾部识别**：从数据中客观地确定重尾的起始点 $x_{\\min}$。\n3.  **模型拟合**：使用最大似然估计（MLE）将纯幂律模型和三个替代模型拟合到已识别的尾部数据。\n4.  **模型比较**：使用经贝叶斯信息准则（BIC）校正的、针对非嵌套模型的 Vuong 检验，将幂律模型与每个替代模型进行两两比较。\n5.  **决策综合**：使用一套预定义的准则（包含统计显著性、参数的物理合理性和样本量）来做出最终的布尔决策。\n\n### 2. 数据生成\n\n为了测试该方法，我们生成了五个不同的人工数据集。对于一个具有连续累积分布函数（CDF）$F(x)$ 的随机变量 $X$，其生成过程依赖于逆变换采样原理。即抽取一个在 $[0, 1)$ 区间内均匀分布的随机数 $U$，样本值则通过 $x = F^{-1}(U)$ 计算得出。\n\n-   **连续幂律（PL）**：其累积分布函数为 $F(x) = 1 - (x/x_{\\min})^{1-\\alpha}$。对其求逆可得生成函数 $x = x_{\\min}(1-U)^{-1/(\\alpha-1)}$，其中 $U$ 是一个均匀随机变量。\n-   **下截断指数（EXP）**：其累积分布函数为 $F(x) = 1 - \\exp(-\\lambda(x-x_{\\min}))$。其生成函数为 $x = x_{\\min} - \\frac{1}{\\lambda}\\ln(1-U)$。\n-   **连续对数正态（LN）**：通过从正态分布 $N(\\mu, \\sigma)$ 中抽取一个样本 $y$，然后计算 $x = \\exp(y)$ 来生成。\n-   **双截断幂律（TPL）**：对于 $\\alpha \\ne 1$，其累积分布函数为 $F(x) = \\frac{x^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}$。其生成函数为 $x = \\left[x_{\\min}^{1-\\alpha} + U(x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha})\\right]^{1/(1-\\alpha)}$。\n\n固定的随机种子确保整个分析过程是确定性和可复现的。\n\n### 3. 尾部识别：Kolmogorov-Smirnov 方法\n\n幂律分析中的一个关键步骤是识别尾部区域的下界 $x_{\\min}$，在该值之上，我们假设幂律成立。我们采用 Clauset、Shalizi 和 Newman 提出的方法，该方法选择能使数据最佳拟合幂律分布的 $x_{\\min}$。\n\n算法流程如下：\n1.  从数据集的唯一值中选取一组 $x_{\\min}$ 的候选值。为确保有足够的数据进行可靠拟合，只考虑那些能产生至少包含 $n_{\\text{tail-search}} = 50$ 个数据点的尾部的候选值。\n2.  对于每个候选 $x_{\\min}$，数据点 $x_i \\ge x_{\\min}$ 构成其尾部。\n3.  使用其解析 MLE 公式为该尾部估计幂律指数 $\\hat{\\alpha}$。\n4.  计算 Kolmogorov-Smirnov（KS）统计量 $D$。这是尾部数据的经验累积分布函数与拟合的幂律分布的理论累积分布函数 $F_{\\mathrm{PL}}(x | \\hat{\\alpha}, x_{\\min})$ 之间的最大绝对差。\n5.  最佳的 $x_{\\min}$ 是使 KS 统计量 $D$ 最小的候选值。此 $x_{\\min}$ 及其对应的尾部数据将用于所有后续分析。\n\n### 4. 模型拟合：最大似然估计（MLE）\n\n对于一个给定参数为 $\\theta$ 的模型和一组包含 $n$ 个尾部数据点 $\\{x_i\\}$ 的集合，MLE 旨在找到参数值 $\\hat{\\theta}$，以最大化似然函数 $L(\\theta | \\{x_i\\}) = \\prod_{i=1}^n f(x_i | \\theta)$，或等效地最大化对数似然函数 $\\mathcal{L}(\\theta | \\{x_i\\}) = \\sum_{i=1}^n \\ln f(x_i | \\theta)$。选择 MLE 是因为它具有渐近无偏性、有效性和一致性等理想性质。\n\n-   **幂律（PL, $k=1$）**：指数 $\\hat{\\alpha}$ 的 MLE 由解析公式给出：$\\hat{\\alpha} = 1 + n / \\sum_{i=1}^{n} \\ln(x_i/x_{\\min})$。\n-   **指数（EXP, $k=1$）**：速率 $\\hat{\\lambda}$ 的 MLE 也是解析的：$\\hat{\\lambda} = \\left(\\frac{1}{n} \\sum_{i=1}^{n} x_i - x_{\\min}\\right)^{-1}$。\n-   **对数正态（LN, $k=2$）**：涉及参数 $\\mu$ 和 $\\sigma$ 的截断对数正态分布的对数似然函数没有闭式解来求其最大值。因此，$\\hat{\\mu}$ 和 $\\hat{\\sigma}$ 是通过使用拟牛顿优化算法（`scipy.optimize.minimize`）对负对数似然函数进行数值最小化来找到的。\n-   **双截断幂律（TPL, $k=1$）**：与对数正态情况类似，指数 $\\hat{\\alpha}$ 的 MLE（在 $x_{\\min}$ 和 $x_{\\max}$ 固定的情况下）必须通过数值最大化其对数似然函数来找到。这是一个一维优化问题，由 `scipy.optimize.minimize_scalar` 处理。为确保数值稳定性，在 $\\alpha=1$ 附近对归一化常数 $Z(\\alpha)$ 进行了特殊处理。\n\n### 5. 模型比较：带 BIC 校正的 Vuong 检验\n\n为了决定哪个模型最能描述数据，我们采用两两比较的策略。Vuong 检验是一种用于模型选择的似然比检验，它可以比较非嵌套模型，这正是我们的情况（例如，幂律与对数正态）。\n\n给定两个模型 $M_1$ 和 $M_2$，它们有已拟合的参数和每个观测值的对数似然 $\\ell_{1,i}$ 和 $\\ell_{2,i}$：\n1.  计算逐点的对数似然比：$\\delta_i = \\ell_{1,i} - \\ell_{2,i}$。\n2.  总对数似然比为 $L = \\sum_{i} \\delta_i$。一个正的 $L$ 值表明模型 $M_1$ 更优。\n3.  为了考虑模型的简约性，应用了贝叶斯信息准则（BIC）校正。这会对参数更多的模型进行惩罚。经 BIC 校正的对数似然比为 $L_{\\mathrm{BIC}} = L - \\frac{(k_1 - k_2) \\ln n}{2}$，其中 $k_1$ 和 $k_2$ 是参数的数量。\n4.  Vuong $z$-统计量计算为 $z = \\frac{L_{\\mathrm{BIC}}}{\\sqrt{n} s}$，其中 $s$ 是 $\\delta_i$ 值的样本标准差。\n5.  在模型无法区分的原假设下，$z$ 服从标准正态分布。根据 $z$ 计算双边 $p$-值。一个小的 $p$-值表示一个模型显著优于另一个，而 $z$ 的符号决定了哪一个更优。\n\n### 6. SOC 的决策准则\n\n最终的布尔决策是通过应用一套严格、顺序的准则来做出的，这些准则旨在防止将统计噪声或其他过程误识别为 SOC：\n1.  **充足的尾部数据**：通过 KS 方法选择的尾部数据点数量 $n_{\\text{tail}}$ 必须至少为 100。这确保所有后续的统计检验都有足够的统计功效。\n2.  **合理的指数**：估计的幂律指数 $\\hat{\\alpha}$ 必须在 $[1.5, 3.5]$ 范围内。该范围与许多 SOC 的理论模型和经验观察一致。\n3.  **优于瘦尾和对数正态替代模型**：幂律模型必须在统计上优于指数模型和对数正态模型。这通过 Vuong 检验进行评估。对于两个比较（PL vs. EXP 和 PL vs. LN），总对数似然比 $L$ 必须为正（支持 PL），并且其优越性必须在 Bonferroni 校正水平 $p  0.05/3$ 上是显著的。\n4.  **不劣于截断幂律**：纯幂律模型不能显著差于双截断幂律模型。这用于检查强烈的有限规模效应。如果似然比支持纯幂律（$L \\ge 0$），或者差异在统计上不显著（$p \\ge 0.05/3$），则满足此条件。\n\n只有当一个数据集通过所有这些准则时，才认为它支持 SOC 假说。这种结构化的方法为该问题提供了一个稳健且客观的框架。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize, minimize_scalar\nfrom scipy.special import erf, log_ndtr\n\n# ==============================================================================\n# 1. Data Generation\n# ==============================================================================\n\ndef generate_power_law(n, alpha, x_min, seed):\n    \"\"\"Generates n samples from a continuous power-law distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    u = rng.uniform(0, 1, n)\n    return x_min * (1 - u)**(-1.0 / (alpha - 1.0))\n\ndef generate_lognormal(n, mu, sigma, seed):\n    \"\"\"Generates n samples from a lognormal distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    return rng.lognormal(mean=mu, sigma=sigma, size=n)\n\ndef generate_truncated_exponential(n, lam, x_min, seed):\n    \"\"\"Generates n samples from a lower-truncated exponential distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    u = rng.uniform(0, 1, n)\n    return x_min - (1.0 / lam) * np.log(1 - u)\n\ndef generate_doubly_truncated_power_law(n, alpha, x_min, x_max, seed):\n    \"\"\"Generates n samples from a doubly-truncated power-law distribution.\"\"\"\n    rng = np.random.default_rng(seed)\n    u = rng.uniform(0, 1, n)\n    if alpha == 1.0:\n        return x_min * np.exp(u * np.log(x_max / x_min))\n    else:\n        v = 1.0 - alpha\n        return (x_min**v + u * (x_max**v - x_min**v))**(1.0 / v)\n\n# ==============================================================================\n# 2. Model Fitting and Likelihoods\n# ==============================================================================\n\nclass Model:\n    def log_likelihood_per_point(self, params, data, x_min):\n        raise NotImplementedError\n\n    def fit(self, data, x_min):\n        raise NotImplementedError\n        \nclass PowerLaw(Model):\n    k = 1\n    \n    def fit(self, data, x_min):\n        n = len(data)\n        alpha = 1.0 + n / np.sum(np.log(data / x_min))\n        return (alpha,)\n\n    def log_likelihood_per_point(self, params, data, x_min):\n        alpha = params[0]\n        if alpha = 1: return np.full_like(data, -np.inf)\n        return np.log(alpha - 1) + (alpha - 1) * np.log(x_min) - alpha * np.log(data)\n\nclass Exponential(Model):\n    k = 1\n\n    def fit(self, data, x_min):\n        lam = 1.0 / (np.mean(data) - x_min)\n        return (lam,)\n\n    def log_likelihood_per_point(self, params, data, x_min):\n        lam = params[0]\n        if lam = 0: return np.full_like(data, -np.inf)\n        return np.log(lam) - lam * (data - x_min)\n\nclass Lognormal(Model):\n    k = 2\n\n    def fit(self, data, x_min):\n        def neg_log_likelihood(params, data, x_min):\n            return -np.sum(self.log_likelihood_per_point(params, data, x_min))\n        \n        # Initial guess from untruncated data properties\n        log_data = np.log(data)\n        mu_guess = np.mean(log_data)\n        sigma_guess = np.std(log_data)\n        \n        result = minimize(\n            neg_log_likelihood, \n            x0=[mu_guess, sigma_guess], \n            args=(data, x_min), \n            method='L-BFGS-B', \n            bounds=[(None, None), (1e-6, None)]\n        )\n        return tuple(result.x)\n\n    def log_likelihood_per_point(self, params, data, x_min):\n        mu, sigma = params\n        if sigma = 0: return np.full_like(data, -np.inf)\n        \n        log_data = np.log(data)\n        norm_const = -log_ndtr((np.log(x_min) - mu) / sigma)\n\n        log_pdf_untruncated = -log_data - np.log(sigma) - 0.5 * np.log(2 * np.pi) - ((log_data - mu)**2) / (2 * sigma**2)\n        return log_pdf_untruncated - norm_const\n\nclass TruncatedPowerLaw(Model):\n    k = 1\n\n    def fit(self, data, x_min):\n        x_max = np.max(data)\n        \n        def neg_log_likelihood(alpha, data, x_min, x_max_val):\n            return -np.sum(self.log_likelihood_per_point((alpha,), data, x_min, x_max_val))\n\n        result = minimize_scalar(\n            neg_log_likelihood, \n            args=(data, x_min, x_max),\n            bounds=(0.1, 10), # Reasonable search range for alpha\n            method='bounded'\n        )\n        return (result.x,)\n\n    def _z_alpha(self, alpha, x_min, x_max):\n        # Numerically stable calculation of the normalization constant Z\n        if np.abs(1.0 - alpha)  1e-8:\n            return np.log(x_max / x_min)\n        else:\n            v = 1.0 - alpha\n            # Use log-power to avoid overflow with large exponents\n            log_x_min_v = v * np.log(x_min)\n            log_x_max_v = v * np.log(x_max)\n            return (np.exp(log_x_max_v) - np.exp(log_x_min_v)) / v\n\n    def log_likelihood_per_point(self, params, data, x_min, x_max_val=None):\n        alpha = params[0]\n        if x_max_val is None: x_max_val = np.max(data)\n        \n        z = self._z_alpha(alpha, x_min, x_max_val)\n        if z = 0: return np.full_like(data, -np.inf)\n        \n        return -alpha * np.log(data) - np.log(z)\n\n# ==============================================================================\n# 3. Tail and Model Comparison\n# ==============================================================================\ndef find_xmin(data, min_tail_size):\n    unique_data = np.unique(data)\n    possible_xmins = unique_data[unique_data > 0]\n    \n    best_d = np.inf\n    best_xmin = -1\n    \n    for x_min_candidate in possible_xmins:\n        tail = data[data >= x_min_candidate]\n        n_tail = len(tail)\n        \n        if n_tail  min_tail_size:\n            break # Since data is sorted, subsequent tails will be smaller\n            \n        # Fit power law\n        try:\n            alpha = 1.0 + n_tail / np.sum(np.log(tail / x_min_candidate))\n        except (ValueError, ZeroDivisionError):\n            continue\n\n        if alpha = 1.0: # Not a valid power law\n            continue\n        \n        # Calculate theoretical CDF\n        empirical_cdf = np.arange(1, n_tail + 1) / n_tail\n        theoretical_cdf = 1.0 - (tail / x_min_candidate)**(1.0 - alpha)\n        \n        # KS statistic\n        d = np.max(np.abs(empirical_cdf - theoretical_cdf))\n        \n        if d  best_d:\n            best_d = d\n            best_xmin = x_min_candidate\n            \n    if best_xmin == -1:\n        return np.min(data), data[data >= np.min(data)]\n    \n    final_tail = data[data >= best_xmin]\n    return best_xmin, final_tail\n\ndef vuong_test(data, x_min, model1, params1, model2, params2):\n    n = len(data)\n    \n    is_tpl = isinstance(model2, TruncatedPowerLaw)\n    x_max = np.max(data) if is_tpl else None\n    \n    l1 = model1.log_likelihood_per_point(params1, data, x_min)\n    l2 = model2.log_likelihood_per_point(params2, data, x_min, x_max_val=x_max)\n\n    delta = l1 - l2\n    \n    # Handle -inf from bad fits\n    if np.any(np.isinf(l1)) or np.any(np.isinf(l2)):\n        return -np.inf, 1.0\n\n    L = np.sum(delta)\n    s = np.std(delta, ddof=1)\n    if s == 0: # Models are identical or one is a scaled version of other\n        return L, 1.0\n\n    k1, k2 = model1.k, model2.k\n    bic_correction = (k1 - k2) * np.log(n) / 2.0\n    L_bic = L - bic_correction\n    \n    z = L_bic / (s * np.sqrt(n))\n    p_value = 2 * norm.sf(np.abs(z))\n    \n    return L, p_value\n\n# ==============================================================================\n# 4. Main Solver\n# ==============================================================================\ndef analyze_dataset(data):\n    \"\"\"Applies the full statistical pipeline to one dataset.\"\"\"\n    p_alpha = 0.05 / 3.0\n    \n    # 1. Tail identification\n    best_xmin, tail_data = find_xmin(data, min_tail_size=50)\n    tail_data.sort()\n    n_tail = len(tail_data)\n\n    # 2. Criterion 1: Sufficient tail size\n    if n_tail  100:\n        return False\n        \n    pl_model = PowerLaw()\n\n    # 3. Fit PL and check Criterion 2\n    try:\n        alpha_hat = pl_model.fit(tail_data, best_xmin)[0]\n    except (ValueError, ZeroDivisionError):\n        return False\n\n    if not (1.5 = alpha_hat = 3.5):\n        return False\n\n    params_pl = (alpha_hat,)\n    \n    # 4. Fit alternative models\n    exp_model, ln_model, tpl_model = Exponential(), Lognormal(), TruncatedPowerLaw()\n    try:\n        params_exp = exp_model.fit(tail_data, best_xmin)\n        params_ln = ln_model.fit(tail_data, best_xmin)\n        params_tpl = tpl_model.fit(tail_data, best_xmin)\n    except (RuntimeError, ValueError, ZeroDivisionError):\n        # Catch potential optimization or numerical errors\n        return False\n        \n    # 5. Vuong tests and Criteria 3  4\n    # PL vs EXP\n    L_pl_exp, p_pl_exp = vuong_test(tail_data, best_xmin, pl_model, params_pl, exp_model, params_exp)\n    if not (L_pl_exp > 0 and p_pl_exp  p_alpha):\n        return False\n        \n    # PL vs LN\n    L_pl_ln, p_pl_ln = vuong_test(tail_data, best_xmin, pl_model, params_pl, ln_model, params_ln)\n    if not (L_pl_ln > 0 and p_pl_ln  p_alpha):\n        return False\n        \n    # PL vs TPL\n    L_pl_tpl, p_pl_tpl = vuong_test(tail_data, best_xmin, pl_model, params_pl, tpl_model, params_tpl)\n    if not (L_pl_tpl >= 0 or p_pl_tpl >= p_alpha):\n        return False\n\n    return True\n\ndef solve():\n    test_cases = [\n        ('A', 'pl', {'n': 5000, 'alpha': 2.5, 'x_min': 1, 'seed': 12345}),\n        ('B', 'ln', {'n': 5000, 'mu': 0, 'sigma': 1, 'seed': 23456}),\n        ('C', 'exp', {'n': 5000, 'lam': 1, 'x_min': 1, 'seed': 34567}),\n        ('D', 'tpl', {'n': 5000, 'alpha': 2.2, 'x_min': 1, 'x_max': 100, 'seed': 45678}),\n        ('E', 'pl', {'n': 50, 'alpha': 2.5, 'x_min': 1, 'seed': 56789}),\n    ]\n    \n    results = []\n    \n    for _, type, params in test_cases:\n        if type == 'pl':\n            data = generate_power_law(**params)\n        elif type == 'ln':\n            data = generate_lognormal(**params)\n        elif type == 'exp':\n            data = generate_truncated_exponential(**params)\n        elif type == 'tpl':\n            data = generate_doubly_truncated_power_law(**params)\n        \n        result = analyze_dataset(data)\n        results.append(result)\n\n    # Format output as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    # To conform to platform requirements, run solve() directly.\n    solve()\n\n```"
        }
    ]
}