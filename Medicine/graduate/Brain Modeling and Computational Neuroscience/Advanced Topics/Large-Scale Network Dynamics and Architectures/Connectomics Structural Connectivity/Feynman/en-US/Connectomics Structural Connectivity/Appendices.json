{
    "hands_on_practices": [
        {
            "introduction": "Structural connectivity matrices are foundational to network neuroscience, but their construction is not trivial. Simply counting the number of algorithmic streamlines connecting two regions yields a biased measure of connection strength. This exercise  guides you through principled methods to derive more physically meaningful edge weights, such as the total axonal cross-sectional area, by constraining the model with local microstructural information. By implementing simplified versions of the Spherical-deconvolution Informed Filtering of Tractograms (SIFT) and Convex Optimization Modeling for Microstructure Informed Tractography (COMMIT) frameworks, you will learn how to move from biased streamline counts to robust connectivity estimates.",
            "id": "3972555",
            "problem": "You are given a tractography-derived structural connectivity dataset consisting of a finite set of regions of interest and a finite set of streamlines. In such datasets, each streamline is an algorithmic surrogate for a bundle of axons, but the raw streamline count is known to be biased by local tracking parameters, curvature, and length. A physically interpretable edge weight for a pair of regions $i,j$ is the total cross-sectional area of axons connecting $i$ and $j$, denoted $A_{ij}$, with units in square millimeters ($\\text{mm}^2$). Each voxel contains measured fiber volume (from diffusion modeling) defined as the fiber volume fraction $f_v$ times the voxel volume $V_v$, denoted $b_v = f_v V_v$ in cubic millimeters ($\\text{mm}^3$). Each streamline $k$ traverses a path that contributes a segment length $l_{vk}$ inside voxel $v$ in millimeters ($\\text{mm}$). The total fiber volume in voxel $v$ can be approximated from streamlines by the principle that volume equals cross-sectional area times path length, yielding the conservation law\n$$\n\\sum_{k=1}^{K} w_k \\, l_{vk} \\approx b_v,\n$$\nwhere $w_k$ is the effective cross-sectional area assigned to streamline $k$ in $\\text{mm}^2$, $K$ is the total number of streamlines, and $v$ indexes voxels. For an edge $(i,j)$, the physically motivated weight is\n$$\nA_{ij} \\equiv \\sum_{k \\in \\mathcal{E}_{ij}} w_k,\n$$\nwhere $\\mathcal{E}_{ij}$ is the set of streamlines that connect regions $i$ and $j$.\n\nStarting from the following fundamental bases:\n- The definition of fiber volume as cross-sectional area times length, i.e., for each segment, volume equals area $\\times$ length, and voxel fiber volume $b_v$ is the sum of the contributions inside that voxel.\n- The algorithmic streamline count $S_{ij}$ is a surrogate variable that requires calibration to relate to physical area.\n- The principle of least squares fitting, where model parameters are chosen to minimize the sum of squared differences between modeled and measured quantities, and non-negativity constraints enforce physical plausibility of areas.\n\nYour tasks are:\n1. Define the naive edge weight using a constant per-streamline cross-sectional area $a_0$, yielding $A_{ij}^{\\text{naive}} = a_0 S_{ij}$. This uses only the streamline count $S_{ij}$ and a calibration constant $a_0$ in $\\text{mm}^2$ per streamline.\n2. Implement a simplified Spherical-deconvolution Informed Filtering of Tractograms (SIFT) approach (first formally define as \"Spherical-deconvolution Informed Filtering of Tractograms (SIFT)\") by finding a single global scaling factor $\\alpha \\ge 0$ applied uniformly to all streamlines, such that the modeled voxel fiber volumes\n$$\n\\hat{b}_v(\\alpha) = \\sum_{k=1}^{K} (\\alpha a_0) l_{vk}\n$$\nminimize the total squared error $\\sum_v \\left(\\hat{b}_v(\\alpha) - b_v\\right)^2$. Use the least squares optimality condition to derive the formula for $\\alpha$ and enforce $\\alpha \\ge 0$. The SIFT-corrected edge weight is then $A_{ij}^{\\text{SIFT}} = \\alpha a_0 S_{ij}$. Units must be in $\\text{mm}^2$.\n3. Implement a simplified Convex Optimization Modeling for Microstructure Informed Tractography (COMMIT) approach (first formally define as \"Convex Optimization Modeling for Microstructure Informed Tractography (COMMIT)\") by solving the non-negative least squares problem\n$$\n\\min_{\\mathbf{w} \\ge 0} \\left\\| \\mathbf{A}\\mathbf{w} - \\mathbf{b} \\right\\|_2^2,\n$$\nwhere $\\mathbf{A} \\in \\mathbb{R}^{V \\times K}$ has entries $A_{vk} = l_{vk}$, $\\mathbf{w} \\in \\mathbb{R}^K$ contains streamline areas $w_k$ in $\\text{mm}^2$, and $\\mathbf{b} \\in \\mathbb{R}^{V}$ contains measured voxel fiber volumes $b_v$ in $\\text{mm}^3$. The COMMIT-corrected edge weight for $(i,j)$ is $A_{ij}^{\\text{COMMIT}} = \\sum_{k \\in \\mathcal{E}_{ij}} w_k$. Units must be in $\\text{mm}^2$.\n\nAngle units do not appear in this problem. All lengths must be treated in millimeters, volumes in cubic millimeters ($\\text{mm}^3$), and areas in square millimeters ($\\text{mm}^2$). Your program must compute, for each test case, the triple $\\left[A_{ij}^{\\text{naive}}, A_{ij}^{\\text{SIFT}}, A_{ij}^{\\text{COMMIT}}\\right]$ and print them as floating-point numbers in $\\text{mm}^2$, each rounded to exactly six decimal places.\n\nTest Suite:\nFor each test case, you are provided the design matrix $\\mathbf{A}$ (voxel-by-streamline length segments in $\\text{mm}$), the measured voxel fiber volumes $\\mathbf{b}$ (in $\\text{mm}^3$), an assignment of streamlines to edges, the target edge $(i,j)$ to report, and the constant $a_0$ (in $\\text{mm}^2$ per streamline). The streamline count $S_{ij}$ is the number of streamlines assigned to the target edge $(i,j)$.\n\n- Test Case 1 (single edge, two voxels):\n  - $\\mathbf{A} = \\begin{bmatrix} 1.2 & 0.5 & 0.3 \\\\ 0.8 & 1.0 & 0.0 \\end{bmatrix}$ ($\\text{mm}$),\n  - $\\mathbf{b} = \\begin{bmatrix} 0.24 \\\\ 0.16 \\end{bmatrix}$ ($\\text{mm}^3$),\n  - Streamline-to-edge mapping: all $3$ streamlines belong to edge $(1,2)$,\n  - Report edge $(1,2)$,\n  - $a_0 = 0.05$ ($\\text{mm}^2$ per streamline).\n- Test Case 2a (shared tractogram, edge $(1,2)$):\n  - $\\mathbf{A} = \\begin{bmatrix} 0.9 & 0.7 & 0.0 & 0.1 \\\\ 0.4 & 0.2 & 0.6 & 0.5 \\\\ 0.0 & 0.1 & 0.8 & 1.0 \\end{bmatrix}$ ($\\text{mm}$),\n  - $\\mathbf{b} = \\begin{bmatrix} 0.25 \\\\ 0.225 \\\\ 0.30 \\end{bmatrix}$ ($\\text{mm}^3$),\n  - Streamline-to-edge mapping: first $2$ streamlines belong to edge $(1,2)$, last $2$ belong to edge $(2,3)$,\n  - Report edge $(1,2)$,\n  - $a_0 = 0.04$ ($\\text{mm}^2$ per streamline).\n- Test Case 2b (shared tractogram, edge $(2,3)$):\n  - Same $\\mathbf{A}$ and $\\mathbf{b}$ as Test Case $2a$,\n  - Streamline-to-edge mapping: first $2$ streamlines belong to edge $(1,2)$, last $2$ belong to edge $(2,3)$,\n  - Report edge $(2,3)$,\n  - $a_0 = 0.04$ ($\\text{mm}^2$ per streamline).\n- Test Case 3 (zero fiber volume boundary):\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix}$ ($\\text{mm}$),\n  - $\\mathbf{b} = \\begin{bmatrix} 0.0 \\end{bmatrix}$ ($\\text{mm}^3$),\n  - Streamline-to-edge mapping: both $2$ streamlines belong to edge $(3,4)$,\n  - Report edge $(3,4)$,\n  - $a_0 = 0.03$ ($\\text{mm}^2$ per streamline).\n- Test Case 4 (overdetermined single-streamline fit):\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$ ($\\text{mm}$),\n  - $\\mathbf{b} = \\begin{bmatrix} 0.3 \\\\ 0.1 \\end{bmatrix}$ ($\\text{mm}^3$),\n  - Streamline-to-edge mapping: the single streamline belongs to edge $(4,5)$,\n  - Report edge $(4,5)$,\n  - $a_0 = 0.02$ ($\\text{mm}^2$ per streamline).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes its triple $\\left[A_{ij}^{\\text{naive}}, A_{ij}^{\\text{SIFT}}, A_{ij}^{\\text{COMMIT}}\\right]$ in $\\text{mm}^2$, rounded to six decimal places. For example, the program must print the string representation of a list of five inner lists, each formatted as $[x_1,x_2,x_3]$, resulting in an output like $[[t_{1,1},t_{1,2},t_{1,3}],[t_{2,1},t_{2,2},t_{2,3}],\\ldots]$.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It presents a valid computational problem in the field of connectomics, specifically addressing the calibration of structural connectivity weights derived from diffusion MRI tractography. The provided data and mathematical formulations are self-contained and consistent.\n\nThe task is to compute edge weights for a structural brain network using three different methods: a naive streamline count-based approach, a simplified global filtering approach (SIFT), and a more sophisticated streamline-specific optimization approach (COMMIT). For each method, we will derive the formula and then apply it to the provided test cases. All mathematical entities, including variables, numbers, and operators, are formatted in LaTeX as required.\n\n### 1. Naive Edge Weight ($A_{ij}^{\\text{naive}}$)\n\nThis model is the most direct but biophysically simplistic way to estimate connectivity strength. It assumes that each algorithmic streamline represents an identical, constant contribution to the total axonal cross-sectional area connecting two regions.\n\nThe naive edge weight, $A_{ij}^{\\text{naive}}$, for a connection between region $i$ and region $j$ is defined as the product of the number of streamlines connecting these regions, $S_{ij}$, and a constant, pre-defined cross-sectional area per streamline, $a_0$. The units of $a_0$ are $\\text{mm}^2$ per streamline, and $S_{ij}$ is a dimensionless count, so the resulting weight $A_{ij}^{\\text{naive}}$ is correctly in units of $\\text{mm}^2$.\n\nThe formula is given as:\n$$\nA_{ij}^{\\text{naive}} = a_0 S_{ij}\n$$\nwhere $S_{ij} = |\\mathcal{E}_{ij}|$ is the cardinality of the set of streamlines connecting regions $i$ and $j$.\n\n### 2. SIFT-Corrected Edge Weight ($A_{ij}^{\\text{SIFT}}$)\n\nThe Spherical-deconvolution Informed Filtering of Tractograms (SIFT) approach seeks to improve upon the naive model by relating the streamlines to an underlying biophysical measurement. The core idea is to enforce a consistency between the total volume of streamlines passing through each voxel and the measured fiber volume within that voxel.\n\nIn this simplified version, we find a single global scaling factor, $\\alpha \\ge 0$, which is applied to the cross-sectional area of all streamlines uniformly. The modeled fiber volume in voxel $v$, $\\hat{b}_v(\\alpha)$, is the sum of the volumes of all streamline segments within that voxel:\n$$\n\\hat{b}_v(\\alpha) = \\sum_{k=1}^{K} w_k l_{vk} = \\sum_{k=1}^{K} (\\alpha a_0) l_{vk} = \\alpha a_0 \\sum_{k=1}^{K} l_{vk}\n$$\nHere, $w_k = \\alpha a_0$ is the uniform scaled area for each streamline $k$, and $l_{vk}$ is the length of streamline $k$ in voxel $v$.\n\nWe determine the optimal $\\alpha$ by minimizing the total squared error, $E(\\alpha)$, between the modeled voxel volumes, $\\hat{b}_v(\\alpha)$, and the measured voxel fiber volumes, $b_v$, across all $V$ voxels:\n$$\nE(\\alpha) = \\sum_{v=1}^{V} \\left(\\hat{b}_v(\\alpha) - b_v\\right)^2 = \\sum_{v=1}^{V} \\left( \\alpha a_0 \\sum_{k=1}^{K} l_{vk} - b_v \\right)^2\n$$\nTo find the minimum, we compute the derivative of $E(\\alpha)$ with respect to $\\alpha$ and set it to zero:\n$$\n\\frac{dE}{d\\alpha} = \\sum_{v=1}^{V} 2 \\left( \\alpha a_0 \\sum_{k=1}^{K} l_{vk} - b_v \\right) \\left( a_0 \\sum_{k=1}^{K} l_{vk} \\right) = 0\n$$\nAssuming $a_0 \\neq 0$, we can simplify:\n$$\n\\sum_{v=1}^{V} \\left( \\alpha a_0 \\left(\\sum_{k=1}^{K} l_{vk}\\right)^2 - b_v \\sum_{k=1}^{K} l_{vk} \\right) = 0\n$$\nSolving for $\\alpha$:\n$$\n\\alpha a_0 \\sum_{v=1}^{V} \\left(\\sum_{k=1}^{K} l_{vk}\\right)^2 = \\sum_{v=1}^{V} b_v \\left(\\sum_{k=1}^{K} l_{vk}\\right)\n$$\n$$\n\\alpha = \\frac{\\sum_{v=1}^{V} b_v \\left(\\sum_{k=1}^{K} l_{vk}\\right)}{a_0 \\sum_{v=1}^{V} \\left(\\sum_{k=1}^{K} l_{vk}\\right)^2}\n$$\nIn matrix notation, where $\\mathbf{A}$ is the $V \\times K$ matrix with entries $l_{vk}$, $\\mathbf{b}$ is the $V \\times 1$ vector of measured volumes, and $\\mathbf{1}$ is a $K \\times 1$ vector of ones, let $\\mathbf{c} = \\mathbf{A}\\mathbf{1}$. The vector $\\mathbf{c}$ has entries $c_v = \\sum_{k=1}^{K} l_{vk}$. The formula for $\\alpha$ becomes:\n$$\n\\alpha = \\frac{\\mathbf{b}^T \\mathbf{c}}{a_0 (\\mathbf{c}^T \\mathbf{c})} = \\frac{\\mathbf{b}^T (\\mathbf{A}\\mathbf{1})}{a_0 \\|\\mathbf{A}\\mathbf{1}\\|_2^2}\n$$\nTo enforce the physical constraint $\\alpha \\ge 0$, we take the maximum of this value and zero.\n$$\n\\alpha = \\max\\left(0, \\frac{\\mathbf{b}^T (\\mathbf{A}\\mathbf{1})}{a_0 \\|\\mathbf{A}\\mathbf{1}\\|_2^2}\\right)\n$$\nThe SIFT-corrected edge weight is then computed using this global factor $\\alpha$:\n$$\nA_{ij}^{\\text{SIFT}} = \\alpha a_0 S_{ij}\n$$\n\n### 3. COMMIT-Corrected Edge Weight ($A_{ij}^{\\text{COMMIT}}$)\n\nThe Convex Optimization Modeling for Microstructure Informed Tractography (COMMIT) framework provides a more powerful and flexible model. Instead of applying a single global correction factor, it assigns an individual cross-sectional area, $w_k \\ge 0$, to each streamline. This allows the model to account for the fact that different fiber bundles may have different densities and properties.\n\nThe model assumes that the measured fiber volume $b_v$ in a voxel is the linear sum of the contributions from each streamline passing through it:\n$$\n\\sum_{k=1}^{K} w_k l_{vk} \\approx b_v\n$$\nThis can be written for all voxels simultaneously as the matrix equation $\\mathbf{A}\\mathbf{w} \\approx \\mathbf{b}$, where $\\mathbf{A}$ is the matrix of segment lengths $l_{vk}$, $\\mathbf{w}$ is the vector of unknown streamline areas $w_k$, and $\\mathbf{b}$ is the vector of measured voxel volumes $b_v$.\n\nThe unknown weights $\\mathbf{w}$ are found by solving the non-negative least squares (NNLS) problem, which minimizes the squared error subject to the physical constraint that cross-sectional areas cannot be negative:\n$$\n\\min_{\\mathbf{w} \\ge 0} \\|\\mathbf{A}\\mathbf{w} - \\mathbf{b}\\|_2^2\n$$\nThis is a standard convex optimization problem that can be solved numerically. Once the optimal vector $\\mathbf{w}^* = [w_1^*, w_2^*, \\dots, w_K^*]^T$ is found, the COMMIT-corrected edge weight for the connection $(i,j)$ is the sum of the areas of all streamlines that constitute that edge:\n$$\nA_{ij}^{\\text{COMMIT}} = \\sum_{k \\in \\mathcal{E}_{ij}} w_k^*\n$$\nThis approach is superior to SIFT as it can \"prune\" spurious streamlines by assigning them a weight of $w_k^*=0$ and can differentially weight streamlines even if they belong to the same edge, reflecting a more complex underlying biology.\n\nThe procedure for each test case is to:\n1.  Identify the number of streamlines $S_{ij}$ and the indices corresponding to the target edge $(i, j)$.\n2.  Calculate $A_{ij}^{\\text{naive}}$ using the given $a_0$ and $S_{ij}$.\n3.  Calculate the global scalar $\\alpha$ for the SIFT model and then $A_{ij}^{\\text{SIFT}}$.\n4.  Solve the NNLS problem for the vector $\\mathbf{w}$ and sum the appropriate elements to find $A_{ij}^{\\text{COMMIT}}$.\n5.  Format the resulting triplet $[A_{ij}^{\\text{naive}}, A_{ij}^{\\text{SIFT}}, A_{ij}^{\\text{COMMIT}}]$ as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Computes and prints structural connectivity edge weights using Naive, SIFT, and COMMIT models.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: single edge, two voxels\n        {\n            \"A\": np.array([[1.2, 0.5, 0.3], [0.8, 1.0, 0.0]]),\n            \"b\": np.array([0.24, 0.16]),\n            \"edge_indices\": [0, 1, 2],  # All 3 streamlines for edge (1,2)\n            \"a0\": 0.05\n        },\n        # Test Case 2a: shared tractogram, edge (1,2)\n        {\n            \"A\": np.array([[0.9, 0.7, 0.0, 0.1], \n                           [0.4, 0.2, 0.6, 0.5], \n                           [0.0, 0.1, 0.8, 1.0]]),\n            \"b\": np.array([0.25, 0.225, 0.30]),\n            \"edge_indices\": [0, 1],  # First 2 streamlines for edge (1,2)\n            \"a0\": 0.04\n        },\n        # Test Case 2b: shared tractogram, edge (2,3)\n        {\n            \"A\": np.array([[0.9, 0.7, 0.0, 0.1],\n                           [0.4, 0.2, 0.6, 0.5],\n                           [0.0, 0.1, 0.8, 1.0]]),\n            \"b\": np.array([0.25, 0.225, 0.30]),\n            \"edge_indices\": [2, 3],  # Last 2 streamlines for edge (2,3)\n            \"a0\": 0.04\n        },\n        # Test Case 3: zero fiber volume boundary\n        {\n            \"A\": np.array([[1.0, 1.0]]),\n            \"b\": np.array([0.0]),\n            \"edge_indices\": [0, 1],  # Both 2 streamlines for edge (3,4)\n            \"a0\": 0.03\n        },\n        # Test Case 4: overdetermined single-streamline fit\n        {\n            \"A\": np.array([[1.0], [1.0]]),\n            \"b\": np.array([0.3, 0.1]),\n            \"edge_indices\": [0],  # The single streamline for edge (4,5)\n            \"a0\": 0.02\n        }\n    ]\n\n    all_results_formatted = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        edge_indices = case[\"edge_indices\"]\n        a0 = case[\"a0\"]\n\n        # Number of streamlines for the target edge\n        S_ij = len(edge_indices)\n\n        # 1. Naive model\n        A_naive = a0 * S_ij\n\n        # 2. SIFT model\n        # The vector c has elements c_v = sum_k l_vk\n        # This is equivalent to summing each row of A, or A @ 1_K\n        c = A.sum(axis=1)\n        \n        numerator = b.dot(c)\n        denominator_part = c.dot(c)\n        \n        alpha = 0.0\n        # Avoid division by zero if all streamline lengths are zero\n        if denominator_part > 1e-12:\n            alpha = numerator / (a0 * denominator_part)\n        \n        # Enforce non-negativity constraint\n        alpha = max(0.0, alpha)\n        \n        A_sift = alpha * a0 * S_ij\n\n        # 3. COMMIT model\n        # Solve the non-negative least squares problem: min ||Aw - b||^2 s.t. w >= 0\n        w, _ = nnls(A, b)\n        \n        # Sum the weights of the streamlines belonging to the target edge\n        A_commit = w[edge_indices].sum()\n\n        results_tuple = (A_naive, A_sift, A_commit)\n        all_results_formatted.append(f\"[{','.join(f'{x:.6f}' for x in results_tuple)}]\")\n    \n    # Print final output in the exact specified format\n    print(f\"[{','.join(all_results_formatted)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a structural connectivity matrix is constructed, it is tempting to immediately compute network metrics. However, these metrics can be heavily influenced by non-topological factors, such as the sheer size of the brain regions being connected. This practice  explores a common and critical pre-processing step: normalizing the connectivity matrix by regional size to mitigate such biases. You will investigate how this normalization quantitatively impacts fundamental network measures like node strength and modularity, sharpening your understanding of how to perform robust and meaningful connectomic analysis.",
            "id": "3972545",
            "problem": "Consider a weighted, undirected structural connectivity (SC) network represented by an adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$, where $A$ is symmetric, $A_{ij} \\ge 0$, and $A_{ii} = 0$ for all $i$. Let $w \\in \\mathbb{R}^N$ be a vector of strictly positive regional sizes (for example, volumes or surface areas). Define a normalized adjacency matrix $\\tilde{A} \\in \\mathbb{R}^{N \\times N}$ by scaling edge weights with the product of regional sizes:\n$$\n\\tilde{A}_{ij} = \\frac{A_{ij}}{w_i w_j}.\n$$\nFor any $A$, define the node strength vector $s \\in \\mathbb{R}^N$ by\n$$\ns_i = \\sum_{j=1}^N A_{ij},\n$$\nand similarly $\\tilde{s}_i = \\sum_{j=1}^N \\tilde{A}_{ij}$. Define the total edge weight $m$ and normalized total edge weight $\\tilde{m}$ by\n$$\nm = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N A_{ij}, \\quad \\tilde{m} = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\tilde{A}_{ij}.\n$$\nGiven a community assignment $c \\in \\{0,1,2,\\dots\\}^N$, and a resolution parameter $\\gamma \\in \\mathbb{R}_{>0}$, define the weighted Newman–Girvan modularity (NG modularity) for $A$ as\n$$\nQ(A,c,\\gamma) = \\frac{1}{2m} \\sum_{i=1}^N \\sum_{j=1}^N \\left( A_{ij} - \\gamma \\frac{s_i s_j}{2m} \\right) \\delta(c_i,c_j),\n$$\nwhere $\\delta(\\cdot,\\cdot)$ is the Kronecker delta. Define $Q(\\tilde{A},c,\\gamma)$ analogously by replacing $A$ with $\\tilde{A}$ and $s$ with $\\tilde{s}$ and $m$ with $\\tilde{m}$. All quantities are dimensionless.\n\nYour task is to analyze the impact of the normalization $\\tilde{A}$ on node strength and modularity. For each test case below, compute:\n- The mean of the strength ratios $r_i = \\tilde{s}_i / s_i$ across nodes, denoted $\\mu_r$.\n- The standard deviation of the strength ratios across nodes, denoted $\\sigma_r$.\n- The change in modularity $\\Delta Q = Q(\\tilde{A},c,\\gamma) - Q(A,c,\\gamma)$.\n\nReturn the results for all test cases as a single line: a comma-separated list of lists, each inner list containing $[\\mu_r,\\sigma_r,\\Delta Q]$ for that test case, with each floating-point number rounded to six decimal places, enclosed in square brackets. For example, an output of the form $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3]]$. No additional text should be printed.\n\nUse the following test suite:\n- Test case $1$ ($N=4$):\n  $$\n  A^{(1)} = \\begin{bmatrix}\n  0 & 10 & 4 & 0 \\\\\n  10 & 0 & 2 & 1 \\\\\n  4 & 2 & 0 & 3 \\\\\n  0 & 1 & 3 & 0\n  \\end{bmatrix}, \\quad\n  w^{(1)} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\\\ 2 \\end{bmatrix}, \\quad\n  \\gamma^{(1)} = 1, \\quad\n  c^{(1)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}.\n  $$\n- Test case $2$ ($N=4$, same $A$ as test case $1$ but heterogeneous sizes):\n  $$\n  A^{(2)} = A^{(1)}, \\quad\n  w^{(2)} = \\begin{bmatrix} 1.0 \\\\ 2.5 \\\\ 0.8 \\\\ 3.0 \\end{bmatrix}, \\quad\n  \\gamma^{(2)} = 1, \\quad\n  c^{(2)} = c^{(1)}.\n  $$\n- Test case $3$ ($N=5$):\n  $$\n  A^{(3)} = \\begin{bmatrix}\n  0 & 8 & 7 & 1 & 1 \\\\\n  8 & 0 & 6 & 2 & 1 \\\\\n  7 & 6 & 0 & 1 & 1 \\\\\n  1 & 2 & 1 & 0 & 9 \\\\\n  1 & 1 & 1 & 9 & 0\n  \\end{bmatrix}, \\quad\n  w^{(3)} = \\begin{bmatrix} 1.5 \\\\ 2.0 \\\\ 2.5 \\\\ 1.0 \\\\ 1.2 \\end{bmatrix}, \\quad\n  \\gamma^{(3)} = 1, \\quad\n  c^{(3)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}.\n  $$\n\nScientific realism and constraints:\n- The matrix $A$ is symmetric with nonnegative entries and zero diagonal, representing dimensionless connectivity weights such as streamline counts.\n- The vector $w$ has strictly positive entries.\n- The analysis must be based on the provided definitions without using empirical shortcuts.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes an inner list $[\\mu_r,\\sigma_r,\\Delta Q]$ with values rounded to six decimal places, for example:\n$$\n[[\\mu_r^{(1)},\\sigma_r^{(1)},\\Delta Q^{(1)}],[\\mu_r^{(2)},\\sigma_r^{(2)},\\Delta Q^{(2)}],[\\mu_r^{(3)},\\sigma_r^{(3)},\\Delta Q^{(3)}]].\n$$",
            "solution": "The user-provided problem has been analyzed and is deemed valid. It is scientifically grounded in network theory, mathematically well-posed, and all necessary data and definitions are provided. The task is to compute the effect of a regional-size-based normalization on network properties for three specific test cases.\n\nThe methodological approach involves a sequence of calculations for both the original structural connectivity matrix $A$ and its normalized counterpart $\\tilde{A}$.\n\nFirst, we establish the quantities for the original, unnormalized network. Given an adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$, the node strength vector $s \\in \\mathbb{R}^N$ is computed. The strength $s_i$ of a node $i$ is the sum of weights of all its connections:\n$$\ns_i = \\sum_{j=1}^{N} A_{ij}\n$$\nThis can be efficiently calculated using matrix-vector multiplication, $s = A \\mathbf{1}$, where $\\mathbf{1}$ is an $N$-dimensional vector of ones, or more simply by summing the rows of $A$. The total edge weight of the network, $m$, is half the sum of all entries in $A$, or equivalently, half the sum of all node strengths, since each edge is counted twice in the sum of strengths:\n$$\nm = \\frac{1}{2} \\sum_{i=1}^N s_i\n$$\n\nNext, we apply the specified normalization. Given a vector of strictly positive regional sizes $w \\in \\mathbb{R}^N$, the normalized adjacency matrix $\\tilde{A}$ is defined as:\n$$\n\\tilde{A}_{ij} = \\frac{A_{ij}}{w_i w_j}\n$$\nThis operation can be implemented by first constructing an outer product matrix $W_{ij} = w_i w_j$ and then performing element-wise division: $\\tilde{A} = A \\oslash W$. Since $w_i > 0$ for all $i$, no division by zero will occur.\n\nFollowing the normalization, we compute the corresponding quantities for the normalized network. The normalized node strength vector $\\tilde{s}$ and total normalized edge weight $\\tilde{m}$ are calculated analogously to their unnormalized counterparts:\n$$\n\\tilde{s}_i = \\sum_{j=1}^{N} \\tilde{A}_{ij}, \\quad \\quad \\tilde{m} = \\frac{1}{2} \\sum_{i=1}^{N} \\tilde{s}_i\n$$\n\nWith both sets of strengths, we can compute the vector of strength ratios $r \\in \\mathbb{R}^N$, where $r_i = \\tilde{s}_i / s_i$. The problem requires the mean $\\mu_r$ and the population standard deviation $\\sigma_r$ of these ratios. Since all given matrices $A$ describe connected nodes, all $s_i>0$, ensuring the ratios are well-defined.\n\nThe final part of the analysis involves computing the change in modularity, $\\Delta Q$. The Newman-Girvan modularity for a given community partition $c$ and resolution parameter $\\gamma$ is:\n$$\nQ(A,c,\\gamma) = \\frac{1}{2m} \\sum_{i=1}^N \\sum_{j=1}^N \\left( A_{ij} - \\gamma \\frac{s_i s_j}{2m} \\right) \\delta(c_i,c_j)\n$$\nwhere $\\delta(c_i, c_j)$ is the Kronecker delta, which is $1$ if nodes $i$ and $j$ are in the same community ($c_i=c_j$) and $0$ otherwise. The term $\\gamma \\frac{s_i s_j}{2m}$ represents the expected weight of an edge between nodes $i$ and $j$ in a random null model.\n\nFor computation, this can be expressed in matrix form. Let $B$ be the modularity matrix, with elements $B_{ij} = A_{ij} - \\gamma \\frac{s_i s_j}{2m}$. Let $M$ be a binary matrix where $M_{ij} = 1$ if $c_i=c_j$ and $0$ otherwise. The modularity is then:\n$$\nQ = \\frac{1}{2m} \\sum_{i,j} B_{ij} M_{ij} = \\frac{\\text{sum}(B \\odot M)}{2m}\n$$\nwhere $\\odot$ denotes the element-wise product. We compute this quantity for both the original network, $Q(A,c,\\gamma)$, and the normalized network, $Q(\\tilde{A},c,\\gamma)$, using the respective matrices and derived quantities ($\\tilde{A}, \\tilde{s}, \\tilde{m}$). The change in modularity is their difference: $\\Delta Q = Q(\\tilde{A},c,\\gamma) - Q(A,c,\\gamma)$.\n\nAn important special case occurs when all regional sizes are uniform, i.e., $w_i = w_0$ for some constant $w_0 > 0$. In this scenario, $\\tilde{A}_{ij} = A_{ij}/w_0^2$, which implies $\\tilde{s}_i = s_i/w_0^2$ and $\\tilde{m} = m/w_0^2$. Substituting these into the modularity formula for $\\tilde{A}$ reveals that the factors of $w_0^2$ cancel out, yielding $Q(\\tilde{A},c,\\gamma) = Q(A,c,\\gamma)$. Therefore, for test case 1 where $w$ is uniform, we expect $\\Delta Q = 0$, providing a robust check for the implementation. For non-uniform $w$, this cancellation does not occur, and $\\Delta Q$ will generally be non-zero.\n\nThe outlined procedure is applied to each of the three test cases provided. The final results $[\\mu_r, \\sigma_r, \\Delta Q]$ for each case are calculated and formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the connectomics problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            np.array([\n                [0, 10, 4, 0],\n                [10, 0, 2, 1],\n                [4, 2, 0, 3],\n                [0, 1, 3, 0]\n            ]),\n            np.array([2.0, 2.0, 2.0, 2.0]),\n            1.0,\n            np.array([0, 0, 1, 1])\n        ),\n        # Test case 2\n        (\n            np.array([\n                [0, 10, 4, 0],\n                [10, 0, 2, 1],\n                [4, 2, 0, 3],\n                [0, 1, 3, 0]\n            ]),\n            np.array([1.0, 2.5, 0.8, 3.0]),\n            1.0,\n            np.array([0, 0, 1, 1])\n        ),\n        # Test case 3\n        (\n            np.array([\n                [0, 8, 7, 1, 1],\n                [8, 0, 6, 2, 1],\n                [7, 6, 0, 1, 1],\n                [1, 2, 1, 0, 9],\n                [1, 1, 1, 9, 0]\n            ]),\n            np.array([1.5, 2.0, 2.5, 1.0, 1.2]),\n            1.0,\n            np.array([0, 0, 0, 1, 1])\n        )\n    ]\n\n    results = []\n    \n    def calculate_modularity(A_mat, s_vec, m_val, c_vec, gamma_val):\n        \"\"\"\n        Calculates the Newman-Girvan modularity.\n        \"\"\"\n        if m_val == 0:\n            return 0.0\n        \n        # Create a mask where M[i,j] is 1 if nodes i and j are in the same community\n        community_mask = c_vec[:, np.newaxis] == c_vec\n        \n        # Calculate the null model term\n        null_model = gamma_val * np.outer(s_vec, s_vec) / (2 * m_val)\n        \n        # Modularity matrix\n        B = A_mat - null_model\n        \n        # Calculate Q by summing elements of B within communities\n        modularity_sum = np.sum(B * community_mask)\n        \n        Q = modularity_sum / (2 * m_val)\n        return Q\n\n    for case in test_cases:\n        A, w, gamma, c = case\n\n        # --- Calculations for the original matrix A ---\n        s = A.sum(axis=1)\n        m = s.sum() / 2.0\n\n        # --- Normalization and calculations for A_tilde ---\n        # Using outer product for element-wise division: A_ij / (w_i * w_j)\n        W_outer = np.outer(w, w)\n        A_tilde = A / W_outer\n        \n        s_tilde = A_tilde.sum(axis=1)\n        m_tilde = s_tilde.sum() / 2.0\n        \n        # --- Compute strength ratios and their statistics ---\n        # s is guaranteed to be non-zero for the given test cases\n        ratios = s_tilde / s\n        mu_r = np.mean(ratios)\n        # Population standard deviation (ddof=0 is the default)\n        sigma_r = np.std(ratios)\n\n        # --- Compute modularity change ---\n        Q_A = calculate_modularity(A, s, m, c, gamma)\n        Q_Atilde = calculate_modularity(A_tilde, s_tilde, m_tilde, c, gamma)\n        delta_Q = Q_Atilde - Q_A\n        \n        # Round results to six decimal places\n        case_result = [\n            np.round(mu_r, 6),\n            np.round(sigma_r, 6),\n            np.round(delta_Q, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The template uses map(str, ...) which calls str() on each inner list.\n    # This correctly formats the output as a string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central goal of connectomics is to understand how the brain's static structural wiring shapes its dynamic activity. A powerful way to explore this link is by modeling signal propagation, such as diffusion, on the structural network. In this exercise , you will use the graph Laplacian, derived from the structural connectivity matrix, to analyze the fundamental diffusion modes of a network. By calculating the Laplacian's eigenvalues and eigenvectors, you will uncover the intrinsic timescales of the network and see how its topology dictates the potential patterns of communication.",
            "id": "3972576",
            "problem": "You are given structural connectivity networks represented as symmetric, nonnegative adjacency matrices $A \\in \\mathbb{R}^{n \\times n}$ for $n$ brain regions. The degree matrix is defined as $D \\in \\mathbb{R}^{n \\times n}$ with diagonal entries $D_{ii} = \\sum_{j=1}^{n} A_{ij}$, and the combinatorial graph Laplacian is $L = D - A$. A linear diffusion process on the network is modeled by the ordinary differential equation\n$$\n\\frac{d x(t)}{d t} = -\\kappa L x(t),\n$$\nwhere $x(t) \\in \\mathbb{R}^{n}$ is the activity state over regions at time $t$, and $\\kappa > 0$ is a diffusion coefficient with units of $\\mathrm{s}^{-1}$. For a symmetric $L$, there exists an orthonormal eigenbasis $\\{v_i\\}_{i=1}^{n}$ with corresponding nonnegative eigenvalues $\\{\\lambda_i\\}_{i=1}^{n}$ satisfying $L v_i = \\lambda_i v_i$. Modal solutions along eigenvectors decay as $e^{-\\kappa \\lambda_i t}$, so the magnitude of $\\lambda_i$ sets a diffusion timescale.\n\nStarting from these definitions, for each provided test case:\n- Compute $L = D - A$.\n- Compute all eigenvalues $\\{\\lambda_i\\}_{i=1}^{n}$ and associated orthonormal eigenvectors $\\{v_i\\}_{i=1}^{n}$.\n- Let $\\epsilon = 10^{-9}$ be a numerical tolerance. Define the number of connected components $C$ as the count of eigenvalues $\\lambda_i$ with $|\\lambda_i| \\le \\epsilon$.\n- Define $\\lambda_{+}$ as the smallest strictly positive eigenvalue, i.e., the minimum $\\lambda_i$ such that $\\lambda_i > \\epsilon$. If no such eigenvalue exists, treat $\\lambda_{+}$ as $0$.\n- Define the slowest nontrivial diffusion time constant\n$$\n\\tau_{+} = \\frac{1}{\\kappa \\lambda_{+}}\n$$\nand express it in seconds.\n- Given an initial condition $x_0 \\in \\mathbb{R}^{n}$, compute the energy fraction in the slowest nontrivial mode\n$$\nf_{+} = \\frac{|\\langle v_{+}, x_0 \\rangle|^2}{\\|x_0\\|_2^2},\n$$\nwhere $v_{+}$ is the eigenvector associated with $\\lambda_{+}$. If $\\lambda_{+} = 0$, set $f_{+} = 0$.\n\nYour program must apply this procedure to the following test suite and produce the specified outputs.\n\nTest Suite:\n- Case $1$ (connected chain):\n  - $n = 4$,\n  - $$A_1 = \\begin{bmatrix}\n  0 & 0.8 & 0 & 0 \\\\\n  0.8 & 0 & 0.5 & 0 \\\\\n  0 & 0.5 & 0 & 0.6 \\\\\n  0 & 0 & 0.6 & 0\n  \\end{bmatrix},$$\n  - $\\kappa_1 = 0.2 \\, \\mathrm{s}^{-1}$,\n  - $$x_{0,1} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.$$\n- Case $2$ (disconnected: pair plus isolated node):\n  - $n = 3$,\n  - $$A_2 = \\begin{bmatrix}\n  0 & 1.0 & 0 \\\\\n  1.0 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{bmatrix},$$\n  - $\\kappa_2 = 0.3 \\, \\mathrm{s}^{-1}$,\n  - $$x_{0,2} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.$$\n- Case $3$ (two communities weakly coupled):\n  - $n = 6$,\n  - $$A_3 = \\begin{bmatrix}\n  0 & 1 & 1 & 0 & 0 & 0 \\\\\n  1 & 0 & 1 & 0 & 0 & 0 \\\\\n  1 & 1 & 0 & 0.1 & 0 & 0 \\\\\n  0 & 0 & 0.1 & 0 & 1 & 1 \\\\\n  0 & 0 & 0 & 1 & 0 & 1 \\\\\n  0 & 0 & 0 & 1 & 1 & 0\n  \\end{bmatrix},$$\n  - $\\kappa_3 = 0.1 \\, \\mathrm{s}^{-1}$,\n  - $$x_{0,3} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.$$\n\nRequired Output:\n- For each case, compute and return the list $[C, \\lambda_{+}, \\tau_{+}, f_{+}]$.\n- Express $\\tau_{+}$ in seconds.\n- Express all floats as decimal values rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, with each case’s result itself being a list. For example, the output format must be of the form $[[r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}],[r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}],[r_{3,1}, r_{3,2}, r_{3,3}, r_{3,4}]]$.",
            "solution": "The user has provided a problem that requires the analysis of structural brain networks using concepts from spectral graph theory and modeling diffusion processes. I will first validate the problem statement and then provide a comprehensive solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following definitions and data:\n-   An adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, symmetric and nonnegative, represents structural connectivity for $n$ regions.\n-   The degree matrix $D \\in \\mathbb{R}^{n \\times n}$ is diagonal with $D_{ii} = \\sum_{j=1}^{n} A_{ij}$.\n-   The combinatorial graph Laplacian is $L = D - A$.\n-   A linear diffusion process is modeled by $\\frac{d x(t)}{d t} = -\\kappa L x(t)$, where $\\kappa > 0$ is a diffusion coefficient in $\\mathrm{s}^{-1}$.\n-   $L$ is symmetric, so it has an orthonormal eigenbasis $\\{v_i\\}_{i=1}^{n}$ with real, nonnegative eigenvalues $\\{\\lambda_i\\}_{i=1}^{n}$ such that $L v_i = \\lambda_i v_i$.\n-   Numerical tolerance for eigenvalue comparison: $\\epsilon = 10^{-9}$.\n-   Number of connected components, $C$: count of eigenvalues $\\lambda_i$ where $|\\lambda_i| \\le \\epsilon$.\n-   Smallest strictly positive eigenvalue, $\\lambda_{+}$: the minimum $\\lambda_i$ such that $\\lambda_i > \\epsilon$. If none exists, $\\lambda_{+} = 0$.\n-   Slowest nontrivial diffusion time constant, $\\tau_{+} = \\frac{1}{\\kappa \\lambda_{+}}$.\n-   Energy fraction in the slowest nontrivial mode, $f_{+} = \\frac{|\\langle v_{+}, x_0 \\rangle|^2}{\\|x_0\\|_2^2}$, where $v_{+}$ is the eigenvector for $\\lambda_{+}$. If $\\lambda_{+} = 0$, then $f_{+} = 0$.\n\nTest Suite:\n1.  **Case 1 (connected chain):**\n    -   $n = 4$\n    -   $A_1 = \\begin{bmatrix} 0 & 0.8 & 0 & 0 \\\\ 0.8 & 0 & 0.5 & 0 \\\\ 0 & 0.5 & 0 & 0.6 \\\\ 0 & 0 & 0.6 & 0 \\end{bmatrix}$\n    -   $\\kappa_1 = 0.2 \\, \\mathrm{s}^{-1}$\n    -   $x_{0,1} = \\begin{bmatrix} 1, 0, 0, 0 \\end{bmatrix}^T$\n2.  **Case 2 (disconnected):**\n    -   $n = 3$\n    -   $A_2 = \\begin{bmatrix} 0 & 1.0 & 0 \\\\ 1.0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n    -   $\\kappa_2 = 0.3 \\, \\mathrm{s}^{-1}$\n    -   $x_{0,2} = \\begin{bmatrix} 0, 0, 1 \\end{bmatrix}^T$\n3.  **Case 3 (weakly coupled communities):**\n    -   $n = 6$\n    -   $A_3 = \\begin{bmatrix} 0 & 1 & 1 & 0 & 0 & 0 \\\\ 1 & 0 & 1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0.1 & 0 & 0 \\\\ 0 & 0 & 0.1 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 & 1 & 0 \\end{bmatrix}$\n    -   $\\kappa_3 = 0.1 \\, \\mathrm{s}^{-1}$\n    -   $x_{0,3} = \\begin{bmatrix} 0, 0, 1, 0, 0, 0 \\end{bmatrix}^T$\n\nRequired Output: For each case, a list $[C, \\lambda_{+}, \\tau_{+}, f_{+}]$ with floats rounded to $6$ decimal places, formatted into a single line.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is subjected to rigorous validation.\n-   **Scientifically Grounded**: The problem is based on fundamental principles of spectral graph theory and its application to network diffusion, a standard and well-established topic in computational neuroscience and physics. All definitions (degree matrix, graph Laplacian, eigenvalues/eigenvectors) are standard. The diffusion equation $\\frac{d x}{d t} = -\\kappa L x$ is a canonical model for heat/mass transfer on graphs.\n-   **Well-Posed**: The problem is mathematically well-posed. The adjacency matrices are given as symmetric, which guarantees that the resulting Laplacian matrix $L$ is also symmetric and positive semi-definite. This ensures that all eigenvalues are real and non-negative, and that a complete orthonormal basis of eigenvectors exists. The tasks—computing $L$, its eigensystem, and derived quantities—are all well-defined mathematical operations. Edge cases, such as the non-existence of a strictly positive eigenvalue, are explicitly handled by the problem's definitions.\n-   **Objective**: The problem is stated in precise, objective mathematical language. There are no subjective or ambiguous terms.\n-   **Complete and Consistent**: All necessary data ($A$, $\\kappa$, $x_0$) and definitions ($\\epsilon$, $C$, $\\lambda_{+}$, etc.) are provided for each case. The dimensions of matrices and vectors are consistent. The units for $\\kappa$ are consistent with the calculation of a time constant $\\tau_{+}$.\n-   **Realistic and Feasible**: The provided matrices are plausible, simplified representations of real-world connectivity. The computations required are standard and can be performed with common numerical libraries.\n-   **No Other Flaws**: The problem is not trivial, metaphorical, or unfalsifiable. It is a direct computational task relevant to connectomics.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with a full solution.\n\n### Solution\n\nThe problem requires a series of computations based on the spectral properties of the graph Laplacian matrix derived from a given structural connectivity matrix. The procedure outlined is a standard approach in network neuroscience to understand the dynamic implications of a static network structure.\n\n**Principle-Based Design**\n\nFor each test case, we will execute the following sequence of steps, which are grounded in linear algebra and spectral graph theory.\n\n1.  **Construct the Graph Laplacian ($L$)**:\n    Given a symmetric adjacency matrix $A$, the structural network is an undirected, weighted graph. The strength of a node (region) is its total connection weight. This is captured by the degree matrix $D$, a diagonal matrix where each diagonal entry $D_{ii}$ is the sum of weights of all edges connected to node $i$, i.e., $D_{ii} = \\sum_{j=1}^{n} A_{ij}$. The Graph Laplacian, $L = D - A$, is a fundamental matrix in graph theory. Its structure encodes both node degrees (on the diagonal) and adjacency (off-diagonal). For any vector $x$, the quadratic form $x^T L x = \\sum_{i,j} A_{ij}(x_i - x_j)^2$ measures the variation of the signal $x$ across the network's edges.\n\n2.  **Eigen-decomposition of the Laplacian**:\n    Since $L$ is real and symmetric, it can be diagonalized by an orthogonal matrix of its eigenvectors, and its eigenvalues $\\{\\lambda_i\\}$ are real. Furthermore, $L$ is positive semi-definite, meaning $\\lambda_i \\ge 0$ for all $i$. The set of eigenvalues (the spectrum of the graph) reveals crucial information about the graph's structure. The eigenvectors $\\{v_i\\}$ form an orthonormal basis for $\\mathbb{R}^n$. We will compute these eigenvalues and eigenvectors using a standard numerical algorithm designed for symmetric matrices. The eigenvalues will be sorted in non-decreasing order: $0 \\le \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$.\n\n3.  **Determine Number of Connected Components ($C$)**:\n    A key theorem of spectral graph theory states that the multiplicity of the eigenvalue $\\lambda=0$ is equal to the number of connected components in the graph. A diffusion process on a disconnected graph can reach a separate equilibrium state within each component. Numerically, we count the number of eigenvalues $\\lambda_i$ that are smaller than or equal to a small tolerance $\\epsilon$, i.e., $C = |\\{i : |\\lambda_i| \\le \\epsilon\\}|$.\n\n4.  **Identify the Slowest Nontrivial Mode ($\\lambda_{+}$, $v_{+}$) and Timescale ($\\tau_{+}$)**:\n    The solution to the diffusion equation $\\frac{d x(t)}{d t} = -\\kappa L x(t)$ with initial condition $x(0)=x_0$ can be expressed as a sum over the eigenmodes: $x(t) = \\sum_{i=1}^n \\langle v_i, x_0 \\rangle e^{-\\kappa \\lambda_i t} v_i$.\n    -   The modes corresponding to $\\lambda_i = 0$ are stationary ($e^0=1$) and represent the steady-state solutions where activity is constant within each connected component.\n    -   The modes for $\\lambda_i > 0$ are transient, decaying at a rate determined by $\\kappa \\lambda_i$. The slowest decaying *nontrivial* mode is the one associated with the smallest strictly positive eigenvalue, which we denote $\\lambda_{+}$. This eigenvalue is also known as the Fiedler value or algebraic connectivity of the graph. A smaller $\\lambda_{+}$implies a slower convergence to the network-wide equilibrium, often indicating the presence of bottlenecks or weakly connected communities.\n    -   We find $\\lambda_{+}$ by taking the minimum of the set $\\{\\lambda_i | \\lambda_i > \\epsilon\\}$. The corresponding eigenvector is $v_{+}$.\n    -   The characteristic time constant for this slowest mode is $\\tau_{+} = \\frac{1}{\\kappa \\lambda_{+}}$. This value, expressed in seconds, gives a quantitative measure of the timescale for global information integration in the network. If no such $\\lambda_i > \\epsilon$ exists (i.e., the graph has no edges), we take $\\lambda_{+} = 0$, and consequently, we define $\\tau_{+} = 0$ to handle the singularity.\n\n5.  **Calculate the Energy Fraction ($f_{+}$) of the Initial State**:\n    The initial condition $x_0$ can be projected onto the eigenbasis. The component of $x_0$ along a given eigenvector $v_i$ is given by the inner product $\\langle v_i, x_0 \\rangle$. The squared magnitude of this projection, $|\\langle v_i, x_0 \\rangle|^2$, represents the \"energy\" of the initial state in that mode. The total energy is $\\|x_0\\|_2^2$.\n    -   The fraction of energy in the slowest nontrivial mode is therefore $f_{+} = \\frac{|\\langle v_{+}, x_0 \\rangle|^2}{\\|x_0\\|_2^2}$.\n    -   This value tells us how much the specific initial pattern of activity $x_0$ excites the slowest dynamic mode of the network. If $x_0$ is orthogonal to $v_{+}$, then $f_{+}=0$, and this mode will not be present in the system's temporal evolution. If $\\lambda_{+} = 0$ (meaning no nontrivial modes exist), the problem specifies to set $f_{+} = 0$.\n\nThis procedure will be implemented for each of the three test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final results.\n    \"\"\"\n\n    # Numerical tolerance for eigenvalue zero-check\n    epsilon = 1e-9\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0.0, 0.8, 0.0, 0.0],\n                [0.8, 0.0, 0.5, 0.0],\n                [0.0, 0.5, 0.0, 0.6],\n                [0.0, 0.0, 0.6, 0.0]\n            ]),\n            \"kappa\": 0.2,\n            \"x0\": np.array([1.0, 0.0, 0.0, 0.0])\n        },\n        {\n            \"A\": np.array([\n                [0.0, 1.0, 0.0],\n                [1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ]),\n            \"kappa\": 0.3,\n            \"x0\": np.array([0.0, 0.0, 1.0])\n        },\n        {\n            \"A\": np.array([\n                [0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n                [1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n                [1.0, 1.0, 0.0, 0.1, 0.0, 0.0],\n                [0.0, 0.0, 0.1, 0.0, 1.0, 1.0],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 1.0],\n                [0.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n            ]),\n            \"kappa\": 0.1,\n            \"x0\": np.array([0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_properties(case[\"A\"], case[\"kappa\"], case[\"x0\"], epsilon)\n        all_results.append(result)\n\n    # Format the final output string as per requirements.\n    output_parts = []\n    for res in all_results:\n        C, lambda_plus, tau_plus, f_plus = res\n        part = f\"[{C},{lambda_plus:.6f},{tau_plus:.6f},{f_plus:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\ndef compute_properties(A, kappa, x0, epsilon):\n    \"\"\"\n    Computes the required network properties for a single case.\n    \n    Args:\n        A (np.ndarray): The adjacency matrix.\n        kappa (float): The diffusion coefficient.\n        x0 (np.ndarray): The initial condition vector.\n        epsilon (float): The numerical tolerance for zero.\n\n    Returns:\n        list: A list containing [C, lambda_plus, tau_plus, f_plus].\n    \"\"\"\n    # Step 1: Compute the graph Laplacian L = D - A.\n    D = np.diag(np.sum(A, axis=1))\n    L = D - A\n\n    # Step 2: Compute eigenvalues and eigenvectors.\n    # np.linalg.eigh is used for symmetric matrices. It returns sorted eigenvalues.\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n\n    # Step 3: Compute the number of connected components, C.\n    C = np.sum(np.abs(eigenvalues) <= epsilon)\n\n    # Step 4: Find lambda_plus (smallest strictly positive eigenvalue) and v_plus.\n    lambda_plus = 0.0\n    v_plus = None\n    \n    # Find indices of all strictly positive eigenvalues\n    positive_eval_indices = np.where(eigenvalues > epsilon)[0]\n\n    if len(positive_eval_indices) > 0:\n        # The first one in the sorted list is the smallest positive one.\n        idx_plus = positive_eval_indices[0]\n        lambda_plus = eigenvalues[idx_plus]\n        v_plus = eigenvectors[:, idx_plus]\n\n    # Step 5: Compute the slowest nontrivial diffusion time constant tau_plus.\n    tau_plus = 0.0\n    if lambda_plus > 0:\n        tau_plus = 1.0 / (kappa * lambda_plus)\n\n    # Step 6: Compute the energy fraction in the slowest nontrivial mode, f_plus.\n    f_plus = 0.0\n    # The condition \"If lambda_plus = 0, set f_plus = 0\" is met if v_plus is None.\n    if v_plus is not None:\n        # Note: np.linalg.norm(x0)**2 is equivalent to np.dot(x0, x0)\n        norm_x0_sq = np.dot(x0, x0)\n        # Handle the case of a zero vector for x0, though not in test cases.\n        if norm_x0_sq > 0:\n            # Inner product <v_plus, x0>\n            dot_product = np.dot(v_plus, x0)\n            f_plus = (dot_product ** 2) / norm_x0_sq\n\n    return [C, lambda_plus, tau_plus, f_plus]\n\n# Execute the main function.\nsolve()\n```"
        }
    ]
}