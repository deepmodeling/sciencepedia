{
    "hands_on_practices": [
        {
            "introduction": "我们的第一个练习将从时间差（TD）学习的基本机制入手，以巩固我们的理解。通过在一个简单的状态链中手动追踪奖励预测误差的传播过程，您将直观地掌握价值是如何被学习并赋予给奖励之前的状态的。这个过程据信是由多巴胺的相位性信号所调控的。",
            "id": "3975958",
            "problem": "考虑一个马尔可夫决策过程 (MDP)，它有三个非终止状态 $s_{0}$、$s_{1}$ 和 $s_{2}$，这些状态以链式结构 $s_{0} \\rightarrow s_{1} \\rightarrow s_{2} \\rightarrow \\text{终止状态}$ 进行确定性转移。除了进入终止状态的最后一次转移会产生 $r=1$ 的奖励外，所有其他转移的奖励均为零。按照惯例，终止状态的价值为零。设折扣因子为 $\\gamma = 0.9$，步长（学习率）为 $\\alpha = 0.1$。将所有非终止状态的价值函数初始化为 $V(s) = 0$。\n\n在零步自举的时序差分学习 (TD(0)) 中，奖励预测误差 (RPE)——在计算神经科学中被认为与阶段性多巴胺活动相对应——在时间 $t$ 定义为 $\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$，价值更新公式为 $V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}$。\n\n模拟两个从 $s_{0}$ 开始并进行到终止状态的完整片段。计算每个片段中每个时间步的 $\\delta_{t}$ 值序列以及相应的 $V$ 值更新。最后，报告第二个片段之后的状态价值估计值 $V(s_{2})$。请以一个不带单位的实数形式提供您的最终答案。无需四舍五入。",
            "solution": "用户想要找出在经过两轮时序差分 (TD) 学习之后，状态 $s_2$ 的价值。\n\n### 问题验证\n问题陈述提供了一个定义明确的马尔可夫决策过程 (MDP)，具有确定的状态转移链 ($s_{0} \\rightarrow s_{1} \\rightarrow s_{2} \\rightarrow \\text{终止状态}$)、指定的奖励、折扣因子 $\\gamma = 0.9$ 以及学习率 $\\alpha = 0.1$。初始价值函数对于所有非终止状态给定为 $V(s) = 0$，并且 $V(\\text{终止状态}) = 0$。价值函数 $V(s)$ 和奖励预测误差 $\\delta_t$ 的更新规则根据 TD(0) 算法被明确定义。该问题是自洽的，在强化学习的框架内是科学合理的，并且在数学上是适定的。没有矛盾、歧义或缺失的信息。该问题是有效的。\n\n### 解题推导\n我们将模拟学习过程，进行两个完整的片段，每个片段都从状态 $s_0$ 开始。一个片段是从 $s_0$ 到终止状态的一系列转移。价值函数在每个时间步更新。\n\nTD(0) 的更新规则如下：\n奖励预测误差 (RPE)：$\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$\n价值更新：$V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}$\n\n给定的参数是：\n折扣因子：$\\gamma = 0.9$\n学习率：$\\alpha = 0.1$\n\n价值函数的初始状态：\n$V_0(s_0) = 0$\n$V_0(s_1) = 0$\n$V_0(s_2) = 0$\n$V_0(\\text{终止状态}) = 0$\n\n**片段 1**\n片段的轨迹是 $s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow \\text{终止状态}$。\n\n**第1步：转移 $s_0 \\rightarrow s_1$**\n- 时间步 $t=0$：状态 $s_t = s_0$，下一状态 $s_{t+1} = s_1$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_0$ 使用初始价值函数 $V_0$ 计算：\n$$\n\\delta_0 = r_{1} + \\gamma V_0(s_1) - V_0(s_0) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- 状态 $s_0$ 的价值更新如下：\n$$\nV(s_0) \\leftarrow V_0(s_0) + \\alpha \\delta_0 = 0 + 0.1 \\times 0 = 0\n$$\n- 现在的价值函数为：$V(s_0)=0, V(s_1)=0, V(s_2)=0$。\n\n**第2步：转移 $s_1 \\rightarrow s_2$**\n- 时间步 $t=1$：状态 $s_t = s_1$，下一状态 $s_{t+1} = s_2$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_1$ 使用当前的价值函数计算：\n$$\n\\delta_1 = r_{2} + \\gamma V(s_2) - V(s_1) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- 状态 $s_1$ 的价值更新如下：\n$$\nV(s_1) \\leftarrow V(s_1) + \\alpha \\delta_1 = 0 + 0.1 \\times 0 = 0\n$$\n- 现在的价值函数为：$V(s_0)=0, V(s_1)=0, V(s_2)=0$。\n\n**第3步：转移 $s_2 \\rightarrow \\text{终止状态}$**\n- 时间步 $t=2$：状态 $s_t = s_2$，下一状态 $s_{t+1} = \\text{终止状态}$。\n- 奖励 $r_{t+1} = 1$。\n- RPE $\\delta_2$ 使用 $V(s_2)$ 的当前值和给定的 $V(\\text{终止状态})$ 值计算：\n$$\n\\delta_2 = r_{3} + \\gamma V(\\text{terminal}) - V(s_2) = 1 + 0.9 \\times 0 - 0 = 1\n$$\n- 状态 $s_2$ 的价值更新如下：\n$$\nV(s_2) \\leftarrow V(s_2) + \\alpha \\delta_2 = 0 + 0.1 \\times 1 = 0.1\n$$\n\n在片段 1 结束时，价值函数（记为 $V_1$）为：\n$V_1(s_0) = 0$\n$V_1(s_1) = 0$\n$V_1(s_2) = 0.1$\n\n**片段 2**\n第二个片段从价值函数 $V_1$ 开始。轨迹同样是 $s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow \\text{终止状态}$。\n\n**第1步：转移 $s_0 \\rightarrow s_1$**\n- 时间步 $t=0$：状态 $s_t = s_0$，下一状态 $s_{t+1} = s_1$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_0$ 使用价值函数 $V_1$ 计算：\n$$\n\\delta_0 = r_{1} + \\gamma V_1(s_1) - V_1(s_0) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- 状态 $s_0$ 的价值更新如下：\n$$\nV(s_0) \\leftarrow V_1(s_0) + \\alpha \\delta_0 = 0 + 0.1 \\times 0 = 0\n$$\n- 现在的价值函数为：$V(s_0)=0, V(s_1)=0, V(s_2)=0.1$。\n\n**第2步：转移 $s_1 \\rightarrow s_2$**\n- 时间步 $t=1$：状态 $s_t = s_1$，下一状态 $s_{t+1} = s_2$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_1$ 使用当前的价值函数计算（在上一个片段结束时，该函数仍有 $V(s_1)=0$ 和 $V(s_2)=0.1$）：\n$$\n\\delta_1 = r_{2} + \\gamma V(s_2) - V(s_1) = 0 + 0.9 \\times 0.1 - 0 = 0.09\n$$\n- 状态 $s_1$ 的价值更新如下：\n$$\nV(s_1) \\leftarrow V(s_1) + \\alpha \\delta_1 = 0 + 0.1 \\times 0.09 = 0.009\n$$\n- 现在的价值函数为：$V(s_0)=0, V(s_1)=0.009, V(s_2)=0.1$。\n\n**第3步：转移 $s_2 \\rightarrow \\text{终止状态}$**\n- 时间步 $t=2$：状态 $s_t = s_2$，下一状态 $s_{t+1} = \\text{终止状态}$。\n- 奖励 $r_{t+1} = 1$。\n- RPE $\\delta_2$ 使用 $V(s_2)$ 的当前值（该值仍为 $0.1$）计算：\n$$\n\\delta_2 = r_{3} + \\gamma V(\\text{terminal}) - V(s_2) = 1 + 0.9 \\times 0 - 0.1 = 0.9\n$$\n- 状态 $s_2$ 的价值更新如下：\n$$\nV(s_2) \\leftarrow V(s_2) + \\alpha \\delta_2 = 0.1 + 0.1 \\times 0.9 = 0.1 + 0.09 = 0.19\n$$\n\n在第二个完整片段之后，$V(s_2)$ 的最终值为 $0.19$。\n最终的价值函数为：\n$V_2(s_0) = 0$\n$V_2(s_1) = 0.009$\n$V_2(s_2) = 0.19$\n\n问题要求的是第二个片段后 $V(s_2)$ 的价值估计值。",
            "answer": "$$\\boxed{0.19}$$"
        },
        {
            "introduction": "从算法层面转向其潜在的生物物理实现，这个练习将探讨单个突触上的“三因子”学习法则。您将为一个由突触前脉冲产生的效价轨迹（eligibility trace）建模，并计算一个后续延迟的多巴胺脉冲如何门控该轨迹，从而产生持久的突触强度变化。这个练习在抽象的强化学习概念和突触可塑性机制之间建立了一座具体的数学桥梁。",
            "id": "3976000",
            "problem": "在计算强化学习中，三因子突触学习规则的一个核心要素是资格迹，它在突触前脉冲发放时累积，并在脉冲之间衰减，然后由多巴胺等神经调质信号进行门控。考虑一个单一突触，其接收的突触前脉冲序列由狄拉克δ函数 $\\delta(t)$ 定义为 $s(t) = \\delta(t - t_{1}) + \\delta(t - t_{2})$，脉冲时间分别为 $t_{1} = 0.012$ s 和 $t_{2} = 0.058$ s。资格迹 $e(t)$ 被建模为一个线性衰减记忆变量，服从常微分方程 $\\frac{d e(t)}{d t} = -\\frac{1}{\\tau_{e}} e(t) + \\alpha\\, s(t)$，初始条件为 $e(0) = 0$，其中衰减常数为 $\\tau_{e} = 0.040$ s，增量幅度为 $\\alpha = 1$。一个多巴胺浓度脉冲 $R(t)$ 在时间 $t_{d} = 0.030$ s 时触发，并被建模为一个因果指数函数 $R(t) = R_{0}\\, \\exp\\!\\big( -\\frac{t - t_{d}}{\\tau_{R}} \\big)\\, u(t - t_{d})$，其中 $u(t)$ 是亥维赛单位阶跃函数，幅度为 $R_{0} = 0.8$，衰减常数为 $\\tau_{R} = 0.050$ s。突触权重更新遵循三因子规则 $\\Delta w = \\eta \\int_{0}^{\\infty} R(t)\\, e(t)\\, dt$，其中学习率为 $\\eta = 0.25$。\n\n从资格迹的动力学定义和给定的多巴胺脉冲形式出发，首先显式推导出所有 $t \\ge 0$ 的 $e(t)$，然后以闭式形式计算突触更新 $\\Delta w$。请用自然指数函数将您的最终答案 $\\Delta w$ 表示为单个精确的解析表达式，不要进行任何数值近似。最终答案无需单位。",
            "solution": "该问题陈述在科学上是合理的、适定的，并提供了一套完整且一致的定义和参数。它代表了计算神经科学中的一个标准建模问题，需要求解一个线性常微分方程并计算一个定积分。因此，该问题是有效的，并将提供解答。\n\n解答过程包括两个主要部分：首先，推导资格迹 $e(t)$ 的显式时间依赖表达式；其次，通过对资格迹和多巴胺信号的乘积进行积分，计算总的突触权重更新 $\\Delta w$。\n\n**第一部分：资格迹 $e(t)$ 的推导**\n\n资格迹 $e(t)$ 由以下一阶线性常微分方程决定：\n$$ \\frac{d e(t)}{d t} + \\frac{1}{\\tau_{e}} e(t) = \\alpha\\, s(t) $$\n初始条件为 $e(0) = 0$。输入强迫项是突触前脉冲序列 $s(t) = \\delta(t - t_{1}) + \\delta(t - t_{2})$，其中 $\\delta(t)$ 是狄拉克δ函数。\n\n此类微分方程的解可以通过参数变易法或将输入与系统的脉冲响应进行卷积来求得。对于一个通用的强迫项 $f(t)$ 和初始条件 $e(0)=0$，解为：\n$$ e(t) = \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_e}\\right) f(t') dt' $$\n在我们的情况中，$f(t) = \\alpha s(t)$。代入 $s(t)$ 得到：\n$$ e(t) = \\alpha \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_e}\\right) \\left[ \\delta(t' - t_{1}) + \\delta(t' - t_{2}) \\right] dt' $$\n根据狄拉克δ函数的筛选性质，即 $\\int g(x) \\delta(x-c) dx = g(c)$，我们可以对每个δ函数项计算积分。结果是指数衰减函数的叠加，每个函数都在一个脉冲时间点启动。使用亥维赛阶跃函数 $u(t)$ 表示因果性，$e(t)$ 的表达式为：\n$$ e(t) = \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) u(t - t_1) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) u(t - t_2) $$\n给定脉冲时间 $t_{1} = 0.012$ 和 $t_{2} = 0.058$，我们可以将 $e(t)$ 写成 $t \\ge 0$ 时的分段形式：\n$$ e(t) = \\begin{cases} 0  \\text{for } 0 \\le t   t_1 \\\\ \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right)  \\text{for } t_1 \\le t   t_2 \\\\ \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right)  \\text{for } t \\ge t_2 \\end{cases} $$\n\n**第二部分：突触权重更新 $\\Delta w$ 的计算**\n\n突触权重更新 $\\Delta w$ 由三因子学习规则定义：\n$$ \\Delta w = \\eta \\int_{0}^{\\infty} R(t)\\, e(t)\\, dt $$\n多巴胺信号为 $R(t) = R_{0}\\, \\exp\\left( -\\frac{t - t_{d}}{\\tau_{R}} \\right)\\, u(t - t_{d})$。由于当 $t   t_d$ 时 $R(t)=0$，积分的有效下限是 $t_d$。给定的时间参数为 $t_1 = 0.012$、$t_d = 0.030$ 和 $t_2 = 0.058$，它们的顺序是 $t_1   t_d   t_2$。\n\n我们必须在 $t_2$ 处分割积分，因为 $e(t)$ 的函数形式在这一点发生了变化：\n$$ \\Delta w = \\eta \\int_{t_d}^{\\infty} R(t) e(t) dt = \\eta \\left[ \\int_{t_d}^{t_2} R(t) e(t) dt + \\int_{t_2}^{\\infty} R(t) e(t) dt \\right] $$\n代入 $e(t)$ 各自的分段表达式：\n$$ \\Delta w = \\eta \\left[ \\int_{t_d}^{t_2} R(t) \\left(\\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right)\\right) dt + \\int_{t_2}^{\\infty} R(t) \\left(\\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right)\\right) dt \\right] $$\n可以通过重组对应于每个脉冲对资格迹贡献的项来简化此表达式：\n$$ \\Delta w = \\eta \\alpha \\left[ \\int_{t_d}^{\\infty} R(t) \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) dt + \\int_{t_2}^{\\infty} R(t) \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) dt \\right] $$\n我们将这两个积分定义为 $I_1$ 和 $I_2$：\n$I_1 = \\int_{t_d}^{\\infty} R_0 \\exp\\left(-\\frac{t-t_d}{\\tau_R}\\right) \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) dt$\n$I_2 = \\int_{t_2}^{\\infty} R_0 \\exp\\left(-\\frac{t-t_d}{\\tau_R}\\right) \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) dt$\n\n对于 $I_1$：\n$$ I_1 = R_0 \\exp\\left(\\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e}\\right) \\int_{t_d}^{\\infty} \\exp\\left(-t\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right)\\right) dt $$\n令有效衰减时间常数为 $\\tau_{eff} = \\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right)^{-1} = \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e}$。积分为：\n$$ \\int_{t_d}^{\\infty} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right) dt = \\left[-\\tau_{eff} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right)\\right]_{t_d}^{\\infty} = \\tau_{eff} \\exp\\left(-\\frac{t_d}{\\tau_{eff}}\\right) $$\n将此结果代回，$I_1$ 的最终表达式的指数变为 $\\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e} - \\frac{t_d}{\\tau_{eff}} = \\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e} - t_d\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right) = \\frac{t_1 - t_d}{\\tau_e}$。\n所以，$I_1 = R_0 \\tau_{eff} \\exp\\left(\\frac{t_1 - t_d}{\\tau_e}\\right)$。\n\n对于 $I_2$：\n$$ I_2 = R_0 \\exp\\left(\\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e}\\right) \\int_{t_2}^{\\infty} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right) dt $$\n该积分的计算结果为 $\\tau_{eff} \\exp\\left(-\\frac{t_2}{\\tau_{eff}}\\right)$。$I_2$ 的指数变为 $\\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e} - \\frac{t_2}{\\tau_{eff}} = \\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e} - t_2\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right) = \\frac{t_d - t_2}{\\tau_R}$。\n所以，$I_2 = R_0 \\tau_{eff} \\exp\\left(\\frac{t_d - t_2}{\\tau_R}\\right)$。\n\n总的权重更新为：\n$$ \\Delta w = \\eta \\alpha (I_1 + I_2) = \\eta \\alpha R_0 \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e} \\left[ \\exp\\left(\\frac{t_1 - t_d}{\\tau_e}\\right) + \\exp\\left(\\frac{t_d - t_2}{\\tau_R}\\right) \\right] $$\n这是闭式解析结果。现在，我们代入给定的数值：\n$\\eta = 0.25$，$\\alpha = 1$，$R_0 = 0.8$，$\\tau_e = 0.040$，$\\tau_R = 0.050$，$t_1 = 0.012$，$t_d = 0.030$，$t_2 = 0.058$。\n\n前置因子是：\n$$ \\eta \\alpha R_0 \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e} = (0.25)(1)(0.8) \\frac{(0.050)(0.040)}{0.050 + 0.040} = (0.2) \\frac{0.002}{0.09} = \\frac{1}{5} \\cdot \\frac{2}{90} = \\frac{1}{5} \\cdot \\frac{1}{45} = \\frac{1}{225} $$\n指数函数的参数是：\n$$ \\frac{t_1 - t_d}{\\tau_e} = \\frac{0.012 - 0.030}{0.040} = \\frac{-0.018}{0.040} = -\\frac{18}{40} = -\\frac{9}{20} $$\n$$ \\frac{t_d - t_2}{\\tau_R} = \\frac{0.030 - 0.058}{0.050} = \\frac{-0.028}{0.050} = -\\frac{28}{50} = -\\frac{14}{25} $$\n代入这些值，得到 $\\Delta w$ 的最终精确表达式。\n$$ \\Delta w = \\frac{1}{225} \\left( \\exp\\left(-\\frac{9}{20}\\right) + \\exp\\left(-\\frac{14}{25}\\right) \\right) $$",
            "answer": "$$\\boxed{\\frac{1}{225} \\left( \\exp\\left(-\\frac{9}{20}\\right) + \\exp\\left(-\\frac{14}{25}\\right) \\right)}$$"
        },
        {
            "introduction": "这最后一个练习将价值学习和行动学习的概念整合到一个完整的“行动者-评论家”（actor-critic）框架中，这是大脑决策的一个强大模型。您将计算由单个时间差（TD）误差信号驱动的评论家（估计状态价值）和行动者（形成策略）的同步更新。这个问题阐释了像多巴胺这样的全局性神经调质信号，如何能够协调不同神经系统的学习，以改善目标导向行为。",
            "id": "3975960",
            "problem": "考虑一个在强化学习 (RL) 框架内建模的智能体，该智能体使用 actor-critic 架构。Actor 实现了一个基于动作的 softmax 策略，该策略具有逆温度 $\\,\\beta\\,$ 和线性动作偏好；Critic 实现了一个基于状态特征的线性价值函数。时相性多巴胺被建模为一个标量爆发，它乘性地门控可塑性。仅使用以下基本定义，推导在单个转换（transition）上 actor 和 critic 参数的并发单步更新：\n- 带有线性偏好的 softmax 策略：$\\pi(a \\mid s) = \\dfrac{\\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_a(s)\\big)}{\\sum_{a'} \\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_{a'}(s)\\big)}$，其中 $\\boldsymbol{\\theta}$ 是策略参数，$\\boldsymbol{\\phi}_a(s)$ 是状态 $s$ 下特定于动作的特征。\n- 价值函数近似：$V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$，其中 $\\boldsymbol{w}$ 是价值参数，$\\boldsymbol{x}(s)$ 是状态特征。\n- 单步时间差分 (TD) 奖励预测误差：$\\delta_t = r_t + \\gamma\\, V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$。\n- 多巴胺门控函数：$g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}$，它在 actor 和 critic 的更新中乘性地门控可塑性。\n\n观测到单个转换 $(s_t, a_t, r_t, s_{t+1})$，并指定了以下量：\n- 逆温度 $\\,\\beta = 1\\,$。\n- 策略参数 $\\,\\boldsymbol{\\theta} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{pmatrix}\\,$。\n- 在 $\\,s_t\\,$ 处的动作特征向量：$\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n- 价值参数 $\\,\\boldsymbol{w} = \\begin{pmatrix} \\tfrac{1}{10} \\\\ \\tfrac{2}{10} \\end{pmatrix}\\,$。\n- 状态特征向量：$\\boldsymbol{x}(s_t) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $\\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$。\n- 奖励 $\\,r_t = \\tfrac{1}{2}\\,$ 和折扣因子 $\\,\\gamma = \\tfrac{9}{10}\\,$。\n- 在 $\\,s_t\\,$ 处采样的动作为 $\\,a_t = 1\\,$。\n- 多巴胺爆发幅度 $\\,D = \\tfrac{6}{5}\\,$，门控参数为 $\\,k = 5\\,$ 和 $\\,D_0 = \\tfrac{4}{5}\\,$，因此 $\\,g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}\\,$。\n- Actor 学习率 $\\,\\alpha_a = \\tfrac{1}{10}\\,$ 和 critic 学习率 $\\,\\alpha_c = \\tfrac{1}{5}\\,$。\n\n仅从这些定义出发，推导该单次转换的并发单步参数更新 $\\,\\Delta \\boldsymbol{\\theta}\\,$ 和 $\\,\\Delta \\boldsymbol{w}\\,$。假设时相性多巴胺通过 $\\,g(D)\\,$ 乘性地门控两个更新，并且两个更新都由 TD 误差 $\\,\\delta_t\\,$ 进行缩放。请将最终答案表示为一个单行矩阵，其中列出四个分量 $\\,\\Delta \\theta_1, \\Delta \\theta_2, \\Delta w_1, \\Delta w_2\\,$，使用 $\\,\\exp(\\cdot)\\,$ 以精确的符号形式表示。不需要四舍五入。",
            "solution": "将通过首先提取给定信息，然后评估其科学基础、一致性和形式结构来验证问题。\n\n### 步骤 1：提取给定信息\n-   **带有线性偏好的 softmax 策略**：$\\pi(a \\mid s) = \\dfrac{\\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_a(s)\\big)}{\\sum_{a'} \\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_{a'}(s)\\big)}$\n-   **价值函数近似**：$V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$\n-   **单步时间差分 (TD) 误差**：$\\delta_t = r_t + \\gamma\\, V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$\n-   **多巴胺门控函数**：$g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}$\n-   **假设**：时相性多巴胺通过 $g(D)$ 乘性地门控 actor 和 critic 的更新，并且两个更新都由 TD 误差 $\\delta_t$ 缩放。\n-   **逆温度**：$\\beta = 1$\n-   **策略参数**：$\\boldsymbol{\\theta} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{pmatrix}$\n-   **在 $s_t$ 处的动作特征向量**：$\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n-   **价值参数**：$\\boldsymbol{w} = \\begin{pmatrix} \\tfrac{1}{10} \\\\ \\tfrac{2}{10} \\end{pmatrix}$\n-   **状态特征向量**：$\\boldsymbol{x}(s_t) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $\\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$\n-   **奖励**：$r_t = \\tfrac{1}{2}$\n-   **折扣因子**：$\\gamma = \\tfrac{9}{10}$\n-   **采取的动作**：$a_t = 1$\n-   **多巴胺参数**：$D = \\tfrac{6}{5}$，$k = 5$，$D_0 = \\tfrac{4}{5}$\n-   **学习率**：$\\alpha_a = \\tfrac{1}{10}$ (actor)，$\\alpha_c = \\tfrac{1}{5}$ (critic)\n\n### 步骤 2：使用提取的给定信息进行验证\n该问题在计算神经科学和机器学习领域，特别是强化学习（RL）领域，具有科学依据。所提供的所有定义（actor-critic 架构、softmax 策略、线性函数近似、TD 误差）都是 RL 中的标准概念。将多巴胺建模为突触可塑性的乘性门控是计算神经科学中一个公认的假说。该问题是适定的，提供了计算唯一解所需的所有必要参数和函数形式。语言客观且精确。数据在数值上和维度上都是一致的。该问题并非无足轻重，因为它需要系统地应用多个理论概念。它没有过多的隐喻，并且与指定的主题直接相关。所有条件均已满足。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解答。\n\n### 解的推导\n\n目标是找到单步参数更新 $\\Delta \\boldsymbol{\\theta}$ 和 $\\Delta \\boldsymbol{w}$。问题指明，两个更新都由 TD 误差 $\\delta_t$ 和多巴胺门控函数 $g(D)$ 缩放。更新遵循随机梯度上升/下降的原则。\n\n**1. Critic 更新规则**\nCritic 的参数 $\\boldsymbol{w}$ 被更新以最小化 TD 误差。这通常通过对平方 TD 误差进行梯度下降来完成，对于单个样本，这会导致一个与 TD 误差和价值函数梯度成比例的更新。对于线性逼近器 $V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$，关于 $\\boldsymbol{w}$ 的梯度是 $\\nabla_{\\boldsymbol{w}} V_{\\boldsymbol{w}}(s_t) = \\boldsymbol{x}(s_t)$。更新规则是：\n$$ \\Delta \\boldsymbol{w} = \\alpha_c \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{w}} V_{\\boldsymbol{w}}(s_t) = \\alpha_c \\, g(D) \\, \\delta_t \\, \\boldsymbol{x}(s_t) $$\n\n**2. Actor 更新规则**\nActor 的参数 $\\boldsymbol{\\theta}$ 使用策略梯度方法进行更新，以增加导致正 TD 误差的动作的概率。更新与对数策略的梯度成正比，并由 TD 误差缩放。这被称为 actor-critic 算法。更新规则是：\n$$ \\Delta \\boldsymbol{\\theta} = \\alpha_a \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t) $$\nsoftmax 函数的对数策略的梯度是：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a \\mid s) = \\beta \\left( \\boldsymbol{\\phi}_a(s) - \\sum_{a'} \\pi(a' \\mid s) \\boldsymbol{\\phi}_{a'}(s) \\right) = \\beta \\left( \\boldsymbol{\\phi}_a(s) - \\mathbb{E}_{a' \\sim \\pi(\\cdot \\mid s)}[\\boldsymbol{\\phi}_{a'}(s)] \\right) $$\n\n现在，我们逐步计算所需的量。\n\n**A. 计算 TD 误差 $\\delta_t$**\n首先，我们计算状态 $s_t$ 和 $s_{t+1}$ 的价值估计。\n$$ V_{\\boldsymbol{w}}(s_t) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s_t) = \\begin{pmatrix} \\frac{1}{10}  \\frac{2}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{10} \\cdot 1 + \\frac{2}{10} \\cdot 1 = \\frac{3}{10} $$\n$$ V_{\\boldsymbol{w}}(s_{t+1}) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} \\frac{1}{10}  \\frac{2}{10} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\frac{1}{10} \\cdot 0 + \\frac{2}{10} \\cdot 2 = \\frac{4}{10} $$\n现在，我们计算 TD 误差 $\\delta_t = r_t + \\gamma V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$。\n$$ \\delta_t = \\frac{1}{2} + \\frac{9}{10} \\cdot \\frac{4}{10} - \\frac{3}{10} = \\frac{5}{10} + \\frac{36}{100} - \\frac{30}{100} = \\frac{50+36-30}{100} = \\frac{56}{100} = \\frac{14}{25} $$\n\n**B. 计算多巴胺门控项 $g(D)$**\n门控项是 $g(D) = \\frac{1}{1 + \\exp(-k(D - D_0))}$。\n指数的参数是：\n$$ k(D - D_0) = 5 \\left( \\frac{6}{5} - \\frac{4}{5} \\right) = 5 \\left( \\frac{2}{5} \\right) = 2 $$\n因此，门控项是：\n$$ g(D) = \\frac{1}{1 + \\exp(-2)} $$\n为了代数上的简洁，我们可以将其写为 $g(D) = \\frac{\\exp(2)}{\\exp(2)+1}$。\n\n**C. 计算 Critic 参数更新 $\\Delta \\boldsymbol{w}$**\n使用推导出的公式 $\\Delta \\boldsymbol{w} = \\alpha_c \\, g(D) \\, \\delta_t \\, \\boldsymbol{x}(s_t)$：\n$$ \\Delta \\boldsymbol{w} = \\frac{1}{5} \\cdot \\frac{1}{1 + \\exp(-2)} \\cdot \\frac{14}{25} \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ \\Delta \\boldsymbol{w} = \\frac{14}{125(1 + \\exp(-2))} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{14 \\exp(2)}{125(\\exp(2)+1)} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n分量是：\n$$ \\Delta w_1 = \\Delta w_2 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$\n\n**D. 计算 Actor 参数更新 $\\Delta \\boldsymbol{\\theta}$**\n首先，我们计算策略梯度项 $\\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t)$。采取的动作是 $a_t=1$。\n我们需要动作概率 $\\pi(a' \\mid s_t)$。这些概率取决于动作偏好 $\\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_{a'}(s_t)$。\n给定 $\\beta=1$：\n$$ a=1 \\text{ 的偏好}: \\quad p_1 = \\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{2} $$\n$$ a=2 \\text{ 的偏好}: \\quad p_2 = \\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = -\\frac{1}{2} $$\n概率是：\n$$ \\pi(1 \\mid s_t) = \\frac{\\exp(p_1)}{\\exp(p_1) + \\exp(p_2)} = \\frac{\\exp(1/2)}{\\exp(1/2) + \\exp(-1/2)} $$\n$$ \\pi(2 \\mid s_t) = \\frac{\\exp(p_2)}{\\exp(p_1) + \\exp(p_2)} = \\frac{\\exp(-1/2)}{\\exp(1/2) + \\exp(-1/2)} $$\n让我们通过分子和分母同乘以 $\\exp(1/2)$ 来简化 $\\pi(2 \\mid s_t)$：\n$$ \\pi(2 \\mid s_t) = \\frac{\\exp(-1/2)\\exp(1/2)}{(\\exp(1/2) + \\exp(-1/2))\\exp(1/2)} = \\frac{\\exp(0)}{\\exp(1) + \\exp(0)} = \\frac{1}{\\exp(1) + 1} $$\n$a_t=1$ 的策略梯度是 $\\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\beta (\\boldsymbol{\\phi}_1(s_t) - [\\pi(1|s_t)\\boldsymbol{\\phi}_1(s_t) + \\pi(2|s_t)\\boldsymbol{\\phi}_2(s_t)])$。当 $\\beta=1$ 时：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = (1 - \\pi(1 \\mid s_t))\\boldsymbol{\\phi}_1(s_t) - \\pi(2 \\mid s_t)\\boldsymbol{\\phi}_2(s_t) $$\n由于 $1 - \\pi(1 \\mid s_t) = \\pi(2 \\mid s_t)$，这可以简化为：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\pi(2 \\mid s_t) (\\boldsymbol{\\phi}_1(s_t) - \\boldsymbol{\\phi}_2(s_t)) $$\n$$ \\boldsymbol{\\phi}_1(s_t) - \\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n所以策略梯度项是：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\frac{1}{\\exp(1) + 1} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n现在我们组合 actor 的更新 $\\Delta \\boldsymbol{\\theta} = \\alpha_a \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t)$：\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{1}{10} \\cdot \\frac{1}{1 + \\exp(-2)} \\cdot \\frac{14}{25} \\cdot \\frac{1}{\\exp(1) + 1} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{14}{250(1 + \\exp(-2))(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{7}{125(1 + \\exp(-2))(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n使用恒等式 $1/(1+\\exp(-2)) = \\exp(2)/(\\exp(2)+1)$：\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{7 \\exp(2)}{125(\\exp(2)+1)(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n分量是：\n$$ \\Delta \\theta_1 = \\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta \\theta_2 = -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n\n**最终答案组合**\n最终答案是一个包含四个分量的行矩阵：$\\Delta\\theta_1$, $\\Delta\\theta_2$, $\\Delta w_1$, $\\Delta w_2$。\n$$ \\Delta \\theta_1 = \\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta \\theta_2 = -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta w_1 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$\n$$ \\Delta w_2 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)}  -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)}  \\frac{14 \\exp(2)}{125(\\exp(2) + 1)}  \\frac{14 \\exp(2)}{125(\\exp(2) + 1)}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}