{
    "hands_on_practices": [
        {
            "introduction": "时间差分（TD）学习是理解大脑如何通过经验进行学习的计算模型基石。本练习将引导您手动模拟 TD(0) 算法的核心过程，在一个简单的确定性环境中逐步计算奖励预测误差（$ \\delta_t $）并更新状态价值函数。通过这个练习，您将直观地感受到价值信息是如何从奖励点随学习过程“反向传播”的，这与多巴胺信号在学习试验中将奖励信息传递给先前状态的理论模型相呼应 。",
            "id": "3975958",
            "problem": "考虑一个马尔可夫决策过程 (MDP)，它有三个非终止状态 $s_{0}$、$s_{1}$ 和 $s_{2}$，这些状态按链式结构确定性地转移 $s_{0} \\rightarrow s_{1} \\rightarrow s_{2} \\rightarrow \\text{terminal}$。除了进入终止状态的最后一次转移会产生奖励 $r=1$ 外，所有其他转移的奖励均为零。按照惯例，终止状态的价值为零。设折扣因子为 $\\gamma = 0.9$，步长（学习率）为 $\\alpha = 0.1$。将所有非终止状态的价值函数初始化为 $V(s) = 0$。\n\n在采用零步自举的时间差分学习 (TD(0)) 中，奖励预测误差 (RPE) 在计算神经科学中被认为是与阶段性多巴胺活动相对应的，它在时间 $t$ 被定义为 $\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$，价值更新规则为 $V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}$。\n\n模拟两个从 $s_{0}$ 开始并进行到终止状态的完整回合。计算每个回合中每个时间步的 $\\delta_{t}$ 值序列以及相应的 $V$ 值更新。最后，报告第二个回合结束后状态 $s_{2}$ 的价值估计值 $V(s_{2})$。以一个不带单位的实数形式给出你的最终答案。无需四舍五入。",
            "solution": "用户想要找出经过两个回合的时间差分 (TD) 学习后，状态 $s_2$ 的价值。\n\n### 问题验证\n问题陈述提供了一个定义明确的马尔可夫决策过程 (MDP)，其中包含一个确定性的状态转移链 ($s_{0} \\rightarrow s_{1} \\rightarrow s_{2} \\rightarrow \\text{terminal}$)、指定的奖励、折扣因子 $\\gamma = 0.9$ 和学习率 $\\alpha = 0.1$。初始价值函数被设定为所有非终止状态的 $V(s) = 0$，以及 $V(\\text{terminal}) = 0$。价值函数 $V(s)$ 和奖励预测误差 $\\delta_t$ 的更新规则根据 TD(0) 算法被明确定义。该问题是自洽的，在强化学习的框架内是科学合理的，并且在数学上是适定的。没有矛盾、歧义或缺失的信息。该问题有效。\n\n### 解题推导\n我们将模拟两个完整回合的学习过程，每个回合都从状态 $s_0$ 开始。一个回合是从 $s_0$ 到终止状态的一系列转移。价值函数在每个时间步都会更新。\n\nTD(0) 的更新规则是：\n奖励预测误差 (RPE): $\\delta_{t} = r_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$\n价值更新: $V(s_{t}) \\leftarrow V(s_{t}) + \\alpha \\delta_{t}$\n\n给定的参数是：\n折扣因子: $\\gamma = 0.9$\n学习率: $\\alpha = 0.1$\n\n价值函数的初始状态：\n$V_0(s_0) = 0$\n$V_0(s_1) = 0$\n$V_0(s_2) = 0$\n$V_0(\\text{terminal}) = 0$\n\n**回合 1**\n该回合的轨迹是 $s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow \\text{terminal}$。\n\n**第 1 步：转移 $s_0 \\rightarrow s_1$**\n- 时间步 $t=0$：状态 $s_t = s_0$，下一状态 $s_{t+1} = s_1$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_0$ 使用初始价值函数 $V_0$ 计算：\n$$\n\\delta_0 = r_{1} + \\gamma V_0(s_1) - V_0(s_0) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- 状态 $s_0$ 的价值被更新：\n$$\nV(s_0) \\leftarrow V_0(s_0) + \\alpha \\delta_0 = 0 + 0.1 \\times 0 = 0\n$$\n- 现在的价值函数是：$V(s_0)=0, V(s_1)=0, V(s_2)=0$。\n\n**第 2 步：转移 $s_1 \\rightarrow s_2$**\n- 时间步 $t=1$：状态 $s_t = s_1$，下一状态 $s_{t+1} = s_2$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_1$ 使用当前的价值函数计算：\n$$\n\\delta_1 = r_{2} + \\gamma V(s_2) - V(s_1) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- 状态 $s_1$ 的价值被更新：\n$$\nV(s_1) \\leftarrow V(s_1) + \\alpha \\delta_1 = 0 + 0.1 \\times 0 = 0\n$$\n- 现在的价值函数是：$V(s_0)=0, V(s_1)=0, V(s_2)=0$。\n\n**第 3 步：转移 $s_2 \\rightarrow \\text{terminal}$**\n- 时间步 $t=2$：状态 $s_t = s_2$，下一状态 $s_{t+1} = \\text{terminal}$。\n- 奖励 $r_{t+1} = 1$。\n- RPE $\\delta_2$ 使用 $V(s_2)$ 的当前值和给定的 $V(\\text{terminal})$ 值计算：\n$$\n\\delta_2 = r_{3} + \\gamma V(\\text{terminal}) - V(s_2) = 1 + 0.9 \\times 0 - 0 = 1\n$$\n- 状态 $s_2$ 的价值被更新：\n$$\nV(s_2) \\leftarrow V(s_2) + \\alpha \\delta_2 = 0 + 0.1 \\times 1 = 0.1\n$$\n\n在回合 1 结束时，价值函数（记为 $V_1$）是：\n$V_1(s_0) = 0$\n$V_1(s_1) = 0$\n$V_1(s_2) = 0.1$\n\n**回合 2**\n第二个回合从价值函数 $V_1$ 开始。轨迹同样是 $s_0 \\rightarrow s_1 \\rightarrow s_2 \\rightarrow \\text{terminal}$。\n\n**第 1 步：转移 $s_0 \\rightarrow s_1$**\n- 时间步 $t=0$：状态 $s_t = s_0$，下一状态 $s_{t+1} = s_1$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_0$ 使用价值函数 $V_1$ 计算：\n$$\n\\delta_0 = r_{1} + \\gamma V_1(s_1) - V_1(s_0) = 0 + 0.9 \\times 0 - 0 = 0\n$$\n- 状态 $s_0$ 的价值被更新：\n$$\nV(s_0) \\leftarrow V_1(s_0) + \\alpha \\delta_0 = 0 + 0.1 \\times 0 = 0\n$$\n- 现在的价值函数是：$V(s_0)=0, V(s_1)=0, V(s_2)=0.1$。\n\n**第 2 步：转移 $s_1 \\rightarrow s_2$**\n- 时间步 $t=1$：状态 $s_t = s_1$，下一状态 $s_{t+1} = s_2$。\n- 奖励 $r_{t+1} = 0$。\n- RPE $\\delta_1$ 使用当前的价值函数计算（其中 $V(s_1)=0$ 且 $V(s_2)=0.1$ 仍是上一个回合结束时的值）：\n$$\n\\delta_1 = r_{2} + \\gamma V(s_2) - V(s_1) = 0 + 0.9 \\times 0.1 - 0 = 0.09\n$$\n- 状态 $s_1$ 的价值被更新：\n$$\nV(s_1) \\leftarrow V(s_1) + \\alpha \\delta_1 = 0 + 0.1 \\times 0.09 = 0.009\n$$\n- 现在的价值函数是：$V(s_0)=0, V(s_1)=0.009, V(s_2)=0.1$。\n\n**第 3 步：转移 $s_2 \\rightarrow \\text{terminal}$**\n- 时间步 $t=2$：状态 $s_t = s_2$，下一状态 $s_{t+1} = \\text{terminal}$。\n- 奖励 $r_{t+1} = 1$。\n- RPE $\\delta_2$ 使用 $V(s_2)$ 的当前值（仍为 $0.1$）计算：\n$$\n\\delta_2 = r_{3} + \\gamma V(\\text{terminal}) - V(s_2) = 1 + 0.9 \\times 0 - 0.1 = 0.9\n$$\n- 状态 $s_2$ 的价值被更新：\n$$\nV(s_2) \\leftarrow V(s_2) + \\alpha \\delta_2 = 0.1 + 0.1 \\times 0.9 = 0.1 + 0.09 = 0.19\n$$\n\n在第二个完整回合结束后，$V(s_2)$ 的最终值为 $0.19$。\n最终的价值函数是：\n$V_2(s_0) = 0$\n$V_2(s_1) = 0.009$\n$V_2(s_2) = 0.19$\n\n问题要求的是第二个回合后状态 $s_2$ 的价值估计值。",
            "answer": "$$\\boxed{0.19}$$"
        },
        {
            "introduction": "在理解了抽象的 TD 算法后，我们来探讨一个更符合生物学现实的神经实现机制。本练习旨在解决“时间信用分配”难题：一个延迟的多巴胺信号如何能够精确地加强一个在较早时间点活跃的突触？为此，我们将引入“效用迹”（eligibility trace）这一概念，它可被视为近期突触活动的一种衰减记忆 。您将计算该效用迹如何与一个模拟多巴胺脉冲的延迟神经调质信号相互作用，从而实现一个三因子学习规则，为连接强化学习算法与突触可塑性提供了具体的数学桥梁。",
            "id": "3976000",
            "problem": "在计算强化学习中，三因子突触学习规则的一个核心要素是资格迹，它在突触前脉冲发放时刻累积，并在脉冲间隔期间衰减，然后由多巴胺等神经调节信号进行门控。考虑一个接收突触前脉冲序列的单一突触，该脉冲序列由狄拉克δ函数 $\\delta(t)$ 定义为 $s(t) = \\delta(t - t_{1}) + \\delta(t - t_{2})$，脉冲发放时间为 $t_{1} = 0.012$ 秒和 $t_{2} = 0.058$ 秒。资格迹 $e(t)$ 被建模为一个线性衰减的记忆变量，遵循常微分方程 $\\frac{d e(t)}{d t} = -\\frac{1}{\\tau_{e}} e(t) + \\alpha\\, s(t)$，初始条件为 $e(0) = 0$，其中衰减常数为 $\\tau_{e} = 0.040$ 秒，增量幅度为 $\\alpha = 1$。在时间 $t_{d} = 0.030$ 秒触发一个多巴胺浓度脉冲 $R(t)$，并将其建模为一个因果指数函数 $R(t) = R_{0}\\, \\exp\\!\\big( -\\frac{t - t_{d}}{\\tau_{R}} \\big)\\, u(t - t_{d})$，其中 $u(t)$ 是亥维赛德单位阶跃函数，幅度为 $R_{0} = 0.8$，衰减常数为 $\\tau_{R} = 0.050$ 秒。突触权重更新遵循三因子规则 $\\Delta w = \\eta \\int_{0}^{\\infty} R(t)\\, e(t)\\, dt$，其中学习率为 $\\eta = 0.25$。\n\n从资格迹的动态定义和给定的多巴胺脉冲形式出发，首先明确推导出对于所有 $t \\ge 0$ 的 $e(t)$，然后以封闭形式计算突触更新 $\\Delta w$。请使用自然指数函数将您对 $\\Delta w$ 的最终答案表示为单个精确的解析表达式，无需任何数值近似。最终答案中不需要单位。",
            "solution": "问题陈述在科学上是合理的、适定的，并提供了一套完整且一致的定义和参数。它代表了计算神经科学中的一个标准建模问题，需要求解一个线性常微分方程并计算一个定积分。因此，该问题是有效的，并将提供解答。\n\n解题过程包括两个主要部分：首先，推导资格迹 $e(t)$ 的显式时间依赖表达式；其次，通过对资格迹和多巴胺信号的乘积进行积分，计算总的突触权重更新 $\\Delta w$。\n\n**第一部分：资格迹 $e(t)$ 的推导**\n\n资格迹 $e(t)$ 由以下一阶线性常微分方程控制：\n$$ \\frac{d e(t)}{d t} + \\frac{1}{\\tau_{e}} e(t) = \\alpha\\, s(t) $$\n初始条件为 $e(0) = 0$。输入驱动项是突触前脉冲序列 $s(t) = \\delta(t - t_{1}) + \\delta(t - t_{2})$，其中 $\\delta(t)$ 是狄拉克δ函数。\n\n此类微分方程的解可以使用参数变易法或通过将输入与系统的脉冲响应进行卷积来求得。对于一般驱动项 $f(t)$ 和初始条件 $e(0)=0$，解为：\n$$ e(t) = \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_e}\\right) f(t') dt' $$\n在我们的情况中，$f(t) = \\alpha s(t)$。代入 $s(t)$ 可得：\n$$ e(t) = \\alpha \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_e}\\right) \\left[ \\delta(t' - t_{1}) + \\delta(t' - t_{2}) \\right] dt' $$\n根据狄拉克δ函数的筛选性质，即 $\\int g(x) \\delta(x-c) dx = g(c)$，我们可以对每个δ函数项的积分进行求值。结果是指数衰减函数的叠加，每个函数都在一个脉冲发放时间点启动。使用亥维赛德阶跃函数 $u(t)$ 来表示因果性，$e(t)$ 的表达式为：\n$$ e(t) = \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) u(t - t_1) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) u(t - t_2) $$\n给定脉冲发放时间 $t_{1} = 0.012$ 和 $t_{2} = 0.058$，我们可以将 $e(t)$ 写成关于 $t \\ge 0$ 的分段形式：\n$$ e(t) = \\begin{cases} 0  \\text{for } 0 \\le t  t_1 \\\\ \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right)  \\text{for } t_1 \\le t  t_2 \\\\ \\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right)  \\text{for } t \\ge t_2 \\end{cases} $$\n\n**第二部分：突触权重更新 $\\Delta w$ 的计算**\n\n突触权重更新 $\\Delta w$ 由三因子学习规则定义：\n$$ \\Delta w = \\eta \\int_{0}^{\\infty} R(t)\\, e(t)\\, dt $$\n多巴胺信号为 $R(t) = R_{0}\\, \\exp\\left( -\\frac{t - t_{d}}{\\tau_{R}} \\right)\\, u(t - t_{d})$。由于当 $t  t_d$ 时 $R(t)=0$，积分的有效下限是 $t_d$。给定的时间参数为 $t_1 = 0.012$, $t_d = 0.030$, 和 $t_2 = 0.058$，它们的顺序是 $t_1  t_d  t_2$。\n\n我们必须在 $t_2$ 处分割积分，因为 $e(t)$ 的函数形式在这一点发生变化：\n$$ \\Delta w = \\eta \\int_{t_d}^{\\infty} R(t) e(t) dt = \\eta \\left[ \\int_{t_d}^{t_2} R(t) e(t) dt + \\int_{t_2}^{\\infty} R(t) e(t) dt \\right] $$\n代入 $e(t)$ 各自的分段表达式：\n$$ \\Delta w = \\eta \\left[ \\int_{t_d}^{t_2} R(t) \\left(\\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right)\\right) dt + \\int_{t_2}^{\\infty} R(t) \\left(\\alpha \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) + \\alpha \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right)\\right) dt \\right] $$\n这个表达式可以通过重新组合与每个脉冲对资格迹的贡献相对应的项来简化：\n$$ \\Delta w = \\eta \\alpha \\left[ \\int_{t_d}^{\\infty} R(t) \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) dt + \\int_{t_2}^{\\infty} R(t) \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) dt \\right] $$\n让我们将这两个积分定义为 $I_1$ 和 $I_2$：\n$I_1 = \\int_{t_d}^{\\infty} R_0 \\exp\\left(-\\frac{t-t_d}{\\tau_R}\\right) \\exp\\left(-\\frac{t-t_1}{\\tau_e}\\right) dt$\n$I_2 = \\int_{t_2}^{\\infty} R_0 \\exp\\left(-\\frac{t-t_d}{\\tau_R}\\right) \\exp\\left(-\\frac{t-t_2}{\\tau_e}\\right) dt$\n\n对于 $I_1$：\n$$ I_1 = R_0 \\exp\\left(\\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e}\\right) \\int_{t_d}^{\\infty} \\exp\\left(-t\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right)\\right) dt $$\n设有效衰减时间常数为 $\\tau_{eff} = \\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right)^{-1} = \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e}$。积分为：\n$$ \\int_{t_d}^{\\infty} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right) dt = \\left[-\\tau_{eff} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right)\\right]_{t_d}^{\\infty} = \\tau_{eff} \\exp\\left(-\\frac{t_d}{\\tau_{eff}}\\right) $$\n将此代回，$I_1$ 最终表达式的指数变为 $\\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e} - \\frac{t_d}{\\tau_{eff}} = \\frac{t_d}{\\tau_R} + \\frac{t_1}{\\tau_e} - t_d\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right) = \\frac{t_1 - t_d}{\\tau_e}$。\n所以，$I_1 = R_0 \\tau_{eff} \\exp\\left(\\frac{t_1 - t_d}{\\tau_e}\\right)$。\n\n对于 $I_2$：\n$$ I_2 = R_0 \\exp\\left(\\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e}\\right) \\int_{t_2}^{\\infty} \\exp\\left(-\\frac{t}{\\tau_{eff}}\\right) dt $$\n该积分计算结果为 $\\tau_{eff} \\exp\\left(-\\frac{t_2}{\\tau_{eff}}\\right)$。$I_2$ 的指数变为 $\\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e} - \\frac{t_2}{\\tau_{eff}} = \\frac{t_d}{\\tau_R} + \\frac{t_2}{\\tau_e} - t_2\\left(\\frac{1}{\\tau_R} + \\frac{1}{\\tau_e}\\right) = \\frac{t_d - t_2}{\\tau_R}$。\n所以，$I_2 = R_0 \\tau_{eff} \\exp\\left(\\frac{t_d - t_2}{\\tau_R}\\right)$。\n\n总权重更新为：\n$$ \\Delta w = \\eta \\alpha (I_1 + I_2) = \\eta \\alpha R_0 \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e} \\left[ \\exp\\left(\\frac{t_1 - t_d}{\\tau_e}\\right) + \\exp\\left(\\frac{t_d - t_2}{\\tau_R}\\right) \\right] $$\n这是封闭形式的解析结果。现在，我们代入给定的数值：\n$\\eta = 0.25$, $\\alpha = 1$, $R_0 = 0.8$, $\\tau_e = 0.040$, $\\tau_R = 0.050$, $t_1 = 0.012$, $t_d = 0.030$, $t_2 = 0.058$。\n\n前置因子为：\n$$ \\eta \\alpha R_0 \\frac{\\tau_R \\tau_e}{\\tau_R + \\tau_e} = (0.25)(1)(0.8) \\frac{(0.050)(0.040)}{0.050 + 0.040} = (0.2) \\frac{0.002}{0.09} = \\frac{1}{5} \\cdot \\frac{2}{90} = \\frac{1}{5} \\cdot \\frac{1}{45} = \\frac{1}{225} $$\n指数函数的参数为：\n$$ \\frac{t_1 - t_d}{\\tau_e} = \\frac{0.012 - 0.030}{0.040} = \\frac{-0.018}{0.040} = -\\frac{18}{40} = -\\frac{9}{20} $$\n$$ \\frac{t_d - t_2}{\\tau_R} = \\frac{0.030 - 0.058}{0.050} = \\frac{-0.028}{0.050} = -\\frac{28}{50} = -\\frac{14}{25} $$\n代入这些值，得到 $\\Delta w$ 的最终精确表达式。\n$$ \\Delta w = \\frac{1}{225} \\left( \\exp\\left(-\\frac{9}{20}\\right) + \\exp\\left(-\\frac{14}{25}\\right) \\right) $$",
            "answer": "$$\\boxed{\\frac{1}{225} \\left( \\exp\\left(-\\frac{9}{20}\\right) + \\exp\\left(-\\frac{14}{25}\\right) \\right)}$$"
        },
        {
            "introduction": "学习的最终目的是为了优化行动，而不仅仅是预测价值。最后的这个练习将带您进入极具影响力的“行动者-评论家”（Actor-Critic）框架，该框架将负责选择行动的“行动者”与负责评估价值的“评论家”分离开来。本练习将演示由评论家计算出的单一 TD 预测误差信号 $ \\delta_t $（在模型中类比为多巴胺爆发）如何同时驱动两个部分的学习过程 。这个信号既能改进评论家的价值预测，又能更新行动者的策略，以促成未来更好的决策，从而展示了一个全局性的神经调质信号如何协调负责评估和行动的不同神经模块进行复杂学习。",
            "id": "3975960",
            "problem": "考虑一个在强化学习 (RL) 框架内建模的智能体，该智能体使用 actor-critic (行动者-评论家) 架构。Actor (行动者) 实现了一个在动作上带有逆温度 $\\,\\beta\\,$ 和线性动作偏好的 softmax 策略，而 critic (评论家) 实现了一个基于状态特征的线性价值函数。时相性多巴胺被建模为一个乘性门控可塑性的标量脉冲。请仅使用以下基本定义，推导在单次转移中对 actor 和 critic 参数的同步单步更新：\n- 带有线性偏好的 softmax 策略：$\\pi(a \\mid s) = \\dfrac{\\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_a(s)\\big)}{\\sum_{a'} \\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_{a'}(s)\\big)}$，其中 $\\boldsymbol{\\theta}$ 是策略参数，$\\boldsymbol{\\phi}_a(s)$ 是状态 $s$ 下特定于动作的特征。\n- 价值函数近似：$V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$，其中 $\\boldsymbol{w}$ 是价值参数，$\\boldsymbol{x}(s)$ 是状态特征。\n- 单步时间差分 (TD) 奖励预测误差：$\\delta_t = r_t + \\gamma\\, V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$。\n- 多巴胺门控函数：$g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}$，该函数乘性地门控 actor 和 critic 更新中的可塑性。\n\n观测到一次转移 $(s_t, a_t, r_t, s_{t+1})$，并指定了以下量值：\n- 逆温度 $\\,\\beta = 1\\,$。\n- 策略参数 $\\,\\boldsymbol{\\theta} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}\\,$.\n- 在 $\\,s_t\\,$ 处的动作特征向量：$\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n- 价值参数 $\\,\\boldsymbol{w} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{2}{10} \\end{pmatrix}\\,$.\n- 状态特征向量：$\\boldsymbol{x}(s_t) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $\\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$。\n- 奖励 $\\,r_t = \\tfrac{1}{2}\\,$ 和折扣因子 $\\,\\gamma = \\tfrac{9}{10}\\,$.\n- 在 $\\,s_t\\,$ 采样的动作为 $\\,a_t = 1\\,$.\n- 多巴胺脉冲幅度 $\\,D = \\tfrac{6}{5}\\,$，门控参数为 $\\,k = 5\\,$ 和 $\\,D_0 = \\tfrac{4}{5}\\,$，因此 $\\,g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}\\,$.\n- Actor 学习率 $\\,\\alpha_a = \\tfrac{1}{10}\\,$ 和 critic 学习率 $\\,\\alpha_c = \\tfrac{1}{5}\\,$.\n\n仅从这些定义出发，推导此次单次转移的同步单步参数更新 $\\,\\Delta \\boldsymbol{\\theta}\\,$ 和 $\\,\\Delta \\boldsymbol{w}\\,$，假设时相性多巴胺通过 $\\,g(D)\\,$ 乘性地门控两个更新，并且两个更新都由 TD 误差 $\\,\\delta_t\\,$ 缩放。请将您的最终答案表示为一个单行矩阵，其中列出四个分量 $\\,\\Delta \\theta_1, \\Delta \\theta_2, \\Delta w_1, \\Delta w_2\\,$，使用 $\\,\\exp(\\cdot)\\,$ 以精确符号形式表示。不需要四舍五入。",
            "solution": "将首先通过提取给定信息，然后评估其科学基础、一致性和形式结构来验证该问题。\n\n### 第 1 步：提取给定条件\n-   **带有线性偏好的 softmax 策略**：$\\pi(a \\mid s) = \\dfrac{\\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_a(s)\\big)}{\\sum_{a'} \\exp\\!\\big(\\beta\\, \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}_{a'}(s)\\big)}$\n-   **价值函数近似**：$V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$\n-   **单步时间差分 (TD) 误差**：$\\delta_t = r_t + \\gamma\\, V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$\n-   **多巴胺门控函数**：$g(D) = \\dfrac{1}{1 + \\exp\\!\\big(-k(D - D_0)\\big)}$\n-   **假设**：时相性多巴胺通过 $g(D)$ 乘性地门控 actor 和 critic 的更新，并且两个更新都由 TD 误差 $\\delta_t$ 缩放。\n-   **逆温度**：$\\beta = 1$\n-   **策略参数**：$\\boldsymbol{\\theta} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}$\n-   **在 $s_t$ 处的动作特征向量**：$\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n-   **价值参数**：$\\boldsymbol{w} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{2}{10} \\end{pmatrix}$\n-   **状态特征向量**：$\\boldsymbol{x}(s_t) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $\\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$\n-   **奖励**：$r_t = \\frac{1}{2}$\n-   **折扣因子**：$\\gamma = \\frac{9}{10}$\n-   **执行的动作**：$a_t = 1$\n-   **多巴胺参数**：$D = \\frac{6}{5}$，$k = 5$，$D_0 = \\frac{4}{5}$\n-   **学习率**：$\\alpha_a = \\frac{1}{10}$ (actor)，$\\alpha_c = \\frac{1}{5}$ (critic)\n\n### 第 2 步：使用提取的给定条件进行验证\n该问题在计算神经科学和机器学习领域，特别是在强化学习 (RL) 领域，具有科学依据。所有提供的定义（actor-critic 架构、softmax 策略、线性函数近似、TD 误差）都是 RL 中的标准概念。将多巴胺建模为突触可塑性的乘性门控是计算神经科学中一个公认的假设。该问题是适定的，提供了计算唯一解所需的所有必要参数和函数形式。语言客观而精确。数据在数值和维度上是一致的。该问题并非无足轻重，因为它需要系统地应用若干理论概念。它没有过多的隐喻，并且与指定的主题直接相关。所有条件均已满足。\n\n### 第 3 步：结论与行动\n问题有效。将提供完整的解答。\n\n### 解的推导\n\n目标是找到单步参数更新 $\\Delta \\boldsymbol{\\theta}$ 和 $\\Delta \\boldsymbol{w}$。问题指明，两个更新都由 TD 误差 $\\delta_t$ 和多巴胺门控函数 $g(D)$ 缩放。这些更新遵循随机梯度上升/下降的原理。\n\n**1. Critic (评论家) 更新规则**\nCritic 的参数 $\\boldsymbol{w}$ 被更新以最小化 TD 误差。这通常通过对 TD 误差平方进行梯度下降来完成，对于单个样本，这导致更新与 TD 误差和价值函数的梯度成正比。对于线性近似器 $V_{\\boldsymbol{w}}(s) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s)$，关于 $\\boldsymbol{w}$ 的梯度是 $\\nabla_{\\boldsymbol{w}} V_{\\boldsymbol{w}}(s_t) = \\boldsymbol{x}(s_t)$。更新规则是：\n$$ \\Delta \\boldsymbol{w} = \\alpha_c \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{w}} V_{\\boldsymbol{w}}(s_t) = \\alpha_c \\, g(D) \\, \\delta_t \\, \\boldsymbol{x}(s_t) $$\n\n**2. Actor (行动者) 更新规则**\nActor 的参数 $\\boldsymbol{\\theta}$ 使用策略梯度方法进行更新，以增加导致正 TD 误差的动作的概率。该更新与对数策略的梯度成正比，并由 TD 误差缩放。这被称为 actor-critic 算法。更新规则是：\n$$ \\Delta \\boldsymbol{\\theta} = \\alpha_a \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t) $$\nsoftmax 函数的对数策略梯度是：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a \\mid s) = \\beta \\left( \\boldsymbol{\\phi}_a(s) - \\sum_{a'} \\pi(a' \\mid s) \\boldsymbol{\\phi}_{a'}(s) \\right) = \\beta \\left( \\boldsymbol{\\phi}_a(s) - \\mathbb{E}_{a' \\sim \\pi(\\cdot \\mid s)}[\\boldsymbol{\\phi}_{a'}(s)] \\right) $$\n\n现在，我们逐步计算所需的量。\n\n**A. 计算 TD 误差 $\\delta_t$**\n首先，我们计算状态 $s_t$ 和 $s_{t+1}$ 的价值估计。\n$$ V_{\\boldsymbol{w}}(s_t) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s_t) = \\begin{pmatrix} \\frac{1}{10}  \\frac{2}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{10} \\cdot 1 + \\frac{2}{10} \\cdot 1 = \\frac{3}{10} $$\n$$ V_{\\boldsymbol{w}}(s_{t+1}) = \\boldsymbol{w}^{\\top} \\boldsymbol{x}(s_{t+1}) = \\begin{pmatrix} \\frac{1}{10}  \\frac{2}{10} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\frac{1}{10} \\cdot 0 + \\frac{2}{10} \\cdot 2 = \\frac{4}{10} $$\n现在，我们计算 TD 误差 $\\delta_t = r_t + \\gamma V_{\\boldsymbol{w}}(s_{t+1}) - V_{\\boldsymbol{w}}(s_t)$。\n$$ \\delta_t = \\frac{1}{2} + \\frac{9}{10} \\cdot \\frac{4}{10} - \\frac{3}{10} = \\frac{5}{10} + \\frac{36}{100} - \\frac{30}{100} = \\frac{50+36-30}{100} = \\frac{56}{100} = \\frac{14}{25} $$\n\n**B. 计算多巴胺门控项 $g(D)$**\n门控项为 $g(D) = \\frac{1}{1 + \\exp(-k(D - D_0))}$。\n指数的参数是：\n$$ k(D - D_0) = 5 \\left( \\frac{6}{5} - \\frac{4}{5} \\right) = 5 \\left( \\frac{2}{5} \\right) = 2 $$\n因此，门控项为：\n$$ g(D) = \\frac{1}{1 + \\exp(-2)} $$\n为了代数上的简洁，我们可以将其写为 $g(D) = \\frac{\\exp(2)}{\\exp(2)+1}$。\n\n**C. 计算 Critic 参数更新 $\\Delta \\boldsymbol{w}$**\n使用推导出的公式 $\\Delta \\boldsymbol{w} = \\alpha_c \\, g(D) \\, \\delta_t \\, \\boldsymbol{x}(s_t)$:\n$$ \\Delta \\boldsymbol{w} = \\frac{1}{5} \\cdot \\frac{1}{1 + \\exp(-2)} \\cdot \\frac{14}{25} \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ \\Delta \\boldsymbol{w} = \\frac{14}{125(1 + \\exp(-2))} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{14 \\exp(2)}{125(\\exp(2)+1)} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n各分量为：\n$$ \\Delta w_1 = \\Delta w_2 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$\n\n**D. 计算 Actor 参数更新 $\\Delta \\boldsymbol{\\theta}$**\n首先，我们计算策略梯度项 $\\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t)$。执行的动作为 $a_t=1$。\n我们需要动作概率 $\\pi(a' \\mid s_t)$。这些概率取决于动作偏好 $\\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_{a'}(s_t)$。\n给定 $\\beta=1$:\n$$ a=1 \\text{的偏好}: \\quad p_1 = \\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_1(s_t) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{2} $$\n$$ a=2 \\text{的偏好}: \\quad p_2 = \\boldsymbol{\\theta}^{\\top}\\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = -\\frac{1}{2} $$\n概率为：\n$$ \\pi(1 \\mid s_t) = \\frac{\\exp(p_1)}{\\exp(p_1) + \\exp(p_2)} = \\frac{\\exp(1/2)}{\\exp(1/2) + \\exp(-1/2)} $$\n$$ \\pi(2 \\mid s_t) = \\frac{\\exp(p_2)}{\\exp(p_1) + \\exp(p_2)} = \\frac{\\exp(-1/2)}{\\exp(1/2) + \\exp(-1/2)} $$\n我们通过将分子和分母乘以 $\\exp(1/2)$ 来简化 $\\pi(2 \\mid s_t)$：\n$$ \\pi(2 \\mid s_t) = \\frac{\\exp(-1/2)\\exp(1/2)}{(\\exp(1/2) + \\exp(-1/2))\\exp(1/2)} = \\frac{\\exp(0)}{\\exp(1) + \\exp(0)} = \\frac{1}{\\exp(1) + 1} $$\n$a_t=1$ 的策略梯度是 $\\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\beta (\\boldsymbol{\\phi}_1(s_t) - [\\pi(1|s_t)\\boldsymbol{\\phi}_1(s_t) + \\pi(2|s_t)\\boldsymbol{\\phi}_2(s_t)])$。当 $\\beta=1$ 时：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = (1 - \\pi(1 \\mid s_t))\\boldsymbol{\\phi}_1(s_t) - \\pi(2 \\mid s_t)\\boldsymbol{\\phi}_2(s_t) $$\n因为 $1 - \\pi(1 \\mid s_t) = \\pi(2 \\mid s_t)$，这可以简化为：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\pi(2 \\mid s_t) (\\boldsymbol{\\phi}_1(s_t) - \\boldsymbol{\\phi}_2(s_t)) $$\n$$ \\boldsymbol{\\phi}_1(s_t) - \\boldsymbol{\\phi}_2(s_t) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n所以策略梯度项是：\n$$ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(1 \\mid s_t) = \\frac{1}{\\exp(1) + 1} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n现在我们组合 actor 的更新 $\\Delta \\boldsymbol{\\theta} = \\alpha_a \\, g(D) \\, \\delta_t \\, \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi(a_t \\mid s_t)$:\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{1}{10} \\cdot \\frac{1}{1 + \\exp(-2)} \\cdot \\frac{14}{25} \\cdot \\frac{1}{\\exp(1) + 1} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{14}{250(1 + \\exp(-2))(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{7}{125(1 + \\exp(-2))(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n使用恒等式 $1/(1+\\exp(-2)) = \\exp(2)/(\\exp(2)+1)$:\n$$ \\Delta \\boldsymbol{\\theta} = \\frac{7 \\exp(2)}{125(\\exp(2)+1)(\\exp(1) + 1)} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n各分量为：\n$$ \\Delta \\theta_1 = \\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta \\theta_2 = -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n\n**最终答案组合**\n最终答案是一个包含四个分量的行矩阵：$\\Delta\\theta_1$、$\\Delta\\theta_2$、$\\Delta w_1$、$\\Delta w_2$。\n$$ \\Delta \\theta_1 = \\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta \\theta_2 = -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)} $$\n$$ \\Delta w_1 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$\n$$ \\Delta w_2 = \\frac{14 \\exp(2)}{125(\\exp(2) + 1)} $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)}  -\\frac{7 \\exp(2)}{125(\\exp(2) + 1)(\\exp(1) + 1)}  \\frac{14 \\exp(2)}{125(\\exp(2) + 1)}  \\frac{14 \\exp(2)}{125(\\exp(2) + 1)}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}