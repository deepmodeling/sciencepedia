{
    "hands_on_practices": [
        {
            "introduction": "赫布学习的“神经元同步发放，则连接增强”原则虽然直观，但其基本形式存在固有的不稳定性，可能导致突触权重的无限制增长。本练习通过对比几种赫布型可塑性规则，包括相关性赫布规则、协方差赫布规则以及Oja规则，来深入探讨这一问题。通过分析这些规则在非零均值输入下的动态行为，你将揭示为何需要突触归一化，并理解Oja规则是如何通过一个局部可计算的项来有效稳定学习过程的。",
            "id": "4025537",
            "problem": "一个线性神经元接收一个输入向量 $\\mathbf{x} \\in \\mathbb{R}^n$，该向量随时间从一个平稳分布中独立抽取，其均值为 $\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{x}] \\neq \\mathbf{0}$，协方差矩阵为 $\\mathbf{C} = \\mathbb{E}\\left[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^\\top\\right]$，其中 $\\mathbf{C}$ 是对称半正定的。该神经元产生输出 $y = \\mathbf{w}^\\top \\mathbf{x}$，其中 $\\mathbf{w} \\in \\mathbb{R}^n$ 是突触权重。考虑在连续时间极限下，以下具有小学习率 $\\eta > 0$ 的突触可塑性规则，其中 $\\Delta \\mathbf{w}$ 表示在一个小时间步长内的增量：\n\n- 相关性赫布规则 (Correlation Hebbian rule): $\\Delta \\mathbf{w} = \\eta\\, y\\, \\mathbf{x}$。\n- 协方差赫布规则 (Covariance Hebbian rule): $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - \\boldsymbol{\\mu})$。\n- Oja 规则 (Oja's rule)（一种局部突触归一化规则）: $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - y\\, \\mathbf{w})$。\n\n从均值和协方差的定义以及线性神经元模型出发，分析当 $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ 时，相关性赫布规则和协方差赫布规则的期望权重动态和范数行为，并解释在有无输入均值中心化的情况下，Oja 规则如何改变稳定性和渐近行为。假设标准的正则性条件确保在小 $\\eta$ 极限下期望和微分可以互换，并且协方差矩阵 $\\mathbf{C}$ 至少有一个严格为正的特征值。\n\n下列哪个陈述是正确的？\n\nA. 在相关性赫布规则下，期望权重动态满足 $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\left(\\mathbf{C}\\,\\mathbf{w} + (\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}\\right)$，这意味着当 $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ 时，权重会沿着 $\\boldsymbol{\\mu}$ 方向发生无界漂移；如果没有额外的归一化，不存在有限范数的稳定不动点。\n\nB. 在协方差赫布规则下，期望权重动态满足 $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\, \\mathbf{C}\\,\\mathbf{w}$，因此 $\\mathbf{w}$ 的方向会与 $\\mathbf{C}$ 的最大特征值所对应的特征向量对齐，但在没有归一化的情况下，范数 $\\lVert \\mathbf{w} \\rVert$ 会无界增长。\n\nC. 仅协方差赫布规则就能保证对于任何半正定矩阵 $\\mathbf{C}$，无论 $\\boldsymbol{\\mu}$ 的值是多少，$\\mathbf{w}$ 都能全局稳定地收敛到一个有界不动点。\n\nD. 用 Oja 规则替换任一赫布规则后，当输入是均值中心化的（$\\boldsymbol{\\mu} = \\mathbf{0}$）时，期望动态变为 $\\dfrac{d\\mathbf{w}}{dt} = \\mathbf{C}\\,\\mathbf{w} - (\\mathbf{w}^\\top \\mathbf{C}\\,\\mathbf{w})\\, \\mathbf{w}$，从而使 $\\mathbf{w}$ 稳定收敛到与 $\\mathbf{C}$ 最大特征值对应的单位范数特征向量；当 $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ 且使用原始输入时，漂移中会出现一个额外的项 $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$，该项会使解偏向 $\\boldsymbol{\\mu}$，除非输入被中心化。\n\nE. 当 $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ 时，相关性赫布规则是稳定的，因为项 $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ 起到了权重衰减的作用，从而减小了 $\\lVert \\mathbf{w} \\rVert$。",
            "solution": "该问题陈述提出了对计算神经科学中几种赫布型突触可塑性规则的标准分析。其定义、假设和物理背景与该领域的既有文献一致。该设置在数学上是明确且自洽的。所有术语都是标准的，它们之间的关系也得到了正确规定。该问题具有科学依据，提法得当，且是客观的。\n\n分析过程是通过推导每种学习规则在连续时间极限下的期望动态来进行的。对于一个小的学习率 $\\eta$，权重向量 $\\mathbf{w}$ 的演化可以用常微分方程（ODE）$\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}\\left[\\frac{\\Delta \\mathbf{w}}{\\eta}\\right]$ 来近似，其中期望是对输入向量 $\\mathbf{x}$ 的分布求得的。\n\n首先，我们建立一个关键的恒等式，将二阶矩矩阵 $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top]$ 与协方差矩阵 $\\mathbf{C}$ 和均值向量 $\\boldsymbol{\\mu}$ 联系起来。根据定义，$\\mathbf{C} = \\mathbb{E}[(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^\\top]$。展开此表达式可得：\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top - \\mathbf{x}\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\mathbf{x}^\\top + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top]\n$$\n利用期望的线性性质以及 $\\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}$：\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\mathbb{E}[\\mathbf{x}]\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\mathbb{E}[\\mathbf{x}^\\top] + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\n$$\n\\mathbf{C} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\n因此，二阶矩矩阵由下式给出：\n$$\n\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top\n$$\n\n有了这个恒等式，我们来分析每一种规则。\n\n**相关性赫布规则 (Correlation Hebbian Rule)**\n更新规则为 $\\Delta \\mathbf{w} = \\eta\\, y\\, \\mathbf{x}$。代入 $y = \\mathbf{w}^\\top \\mathbf{x}$，我们得到 $\\Delta \\mathbf{w} = \\eta\\, (\\mathbf{w}^\\top \\mathbf{x})\\, \\mathbf{x}$。期望变化为：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[(\\mathbf{w}^\\top \\mathbf{x})\\, \\mathbf{x}] = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top \\mathbf{w}]\n$$\n由于 $\\mathbf{w}$ 相对于对 $\\mathbf{x}$ 的期望是常数：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] \\mathbf{w}\n$$\n代入 $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top$：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top) \\mathbf{w} = \\eta (\\mathbf{C}\\mathbf{w} + \\boldsymbol{\\mu}(\\boldsymbol{\\mu}^\\top\\mathbf{w})) = \\eta (\\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu})\n$$\n动态有两个驱动项。第一项 $\\eta\\mathbf{C}\\mathbf{w}$ 驱使 $\\mathbf{w}$ 朝向 $\\mathbf{C}$ 的主特征向量。第二项 $\\eta(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$ 为 $\\mathbf{w}$ 沿着均值向量 $\\boldsymbol{\\mu}$ 的分量提供正反馈。例如，如果 $\\mathbf{w}$ 沿 $\\boldsymbol{\\mu}$ 有一个初始分量，使得 $\\mathbf{w}^\\top\\boldsymbol{\\mu} > 0$，那么这个分量将呈指数增长。这表明存在不稳定性，并导致权重向量范数 $\\lVert \\mathbf{w} \\rVert$ 的无界增长。不存在有限范数的稳定不动点。\n\n**协方差赫布规则 (Covariance Hebbian Rule)**\n更新规则为 $\\Delta \\mathbf{w} = \\eta\\, y\\, (\\mathbf{x} - \\boldsymbol{\\mu})$。代入 $y = \\mathbf{w}^\\top \\mathbf{x}$：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[(\\mathbf{w}^\\top \\mathbf{x})(\\mathbf{x} - \\boldsymbol{\\mu})] = \\eta \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top \\mathbf{w} - \\mathbf{x}\\boldsymbol{\\mu}^\\top \\mathbf{w}]\n$$\n利用期望的线性性质并将 $\\mathbf{w}$ 视为常数：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top]\\mathbf{w} - \\mathbb{E}[\\mathbf{x}]\\boldsymbol{\\mu}^\\top \\mathbf{w})\n$$\n代入 $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top$ 和 $\\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}$：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta ((\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w}) = \\eta (\\mathbf{C}\\mathbf{w} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w} - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top \\mathbf{w}) = \\eta \\mathbf{C}\\mathbf{w}\n$$\n这是一个线性常微分方程。其解代表了用于寻找与 $\\mathbf{C}$ 的最大特征值相对应的特征向量的幂迭代法。如果 $\\mathbf{w}(t) = \\sum_i c_i(t) \\mathbf{v}_i$，其中 $\\mathbf{v}_i$ 是 $\\mathbf{C}$ 的特征向量，特征值为 $\\lambda_i$，则系数的动态为 $\\frac{dc_i}{dt} = \\eta \\lambda_i c_i$，从而得到 $c_i(t) = c_i(0)e^{\\eta\\lambda_i t}$。当 $t\\to\\infty$ 时，对应于最大特征值 $\\lambda_{\\text{max}}$ 的项将占主导地位。$\\mathbf{w}$ 的方向将与相应的特征向量 $\\mathbf{v}_{\\text{max}}$ 对齐。由于问题陈述指出 $\\mathbf{C}$ 至少有一个严格为正的特征值，所以 $\\lambda_{\\text{max}} > 0$。因此，范数 $\\lVert \\mathbf{w} \\rVert$ 将无界地指数增长。\n\n**Oja 规则 (Oja's Rule)**\n更新规则为 $\\Delta \\mathbf{w} = \\eta\\, y (\\mathbf{x} - y \\mathbf{w})$。期望动态为：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbb{E}[y\\mathbf{x} - y^2\\mathbf{w}] = \\eta (\\mathbb{E}[y\\mathbf{x}] - \\mathbb{E}[y^2]\\mathbf{w})\n$$\n第一项与相关性规则中的相同：$\\mathbb{E}[y\\mathbf{x}] = (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$。\n第二项是期望输出的平方：$\\mathbb{E}[y^2] = \\mathbb{E}[(\\mathbf{w}^\\top\\mathbf{x})^2] = \\mathbb{E}[\\mathbf{w}^\\top\\mathbf{x}\\mathbf{x}^\\top\\mathbf{w}] = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] \\mathbf{w} = \\mathbf{w}^\\top(\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$。\n完整的动态为：\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta \\left( (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} - (\\mathbf{w}^\\top(\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w})\\mathbf{w} \\right)\n$$\n情况1：均值中心化输入，$\\boldsymbol{\\mu} = \\mathbf{0}$。\n动态简化为 $\\frac{d\\mathbf{w}}{dt} = \\eta (\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\mathbf{w})$。\n为分析范数，我们计算 $\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\mathbf{w}^\\top \\frac{d\\mathbf{w}}{dt}$：\n$$\n\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\mathbf{w}^\\top \\eta (\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\mathbf{w}) = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} - (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})\\lVert\\mathbf{w}\\rVert^2) = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w})(1 - \\lVert\\mathbf{w}\\rVert^2)\n$$\n由于 $\\mathbf{C}$ 是半正定的且有一个严格为正的特征值，对于任何不在零空间中的 $\\mathbf{w}$，都有 $\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} > 0$。动态驱使 $\\lVert\\mathbf{w}\\rVert^2 \\to 1$。该规则将权重向量的范数稳定到单位长度。更详细的稳定性分析表明，$\\mathbf{w}$ 收敛到与 $\\mathbf{C}$ 的最大特征值相对应的单位范数特征向量 $\\mathbf{v}_{\\text{max}}$。\n情况2：非零均值，$\\boldsymbol{\\mu} \\neq \\mathbf{0}$。\n动态由相关项 $\\mathbb{E}[y\\mathbf{x}] = (\\mathbf{C} + \\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w} = \\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$ 驱动。与均值中心化的情况相比，驱动项或“漂移”项包含一个附加分量 $(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$，这会使权重向量的演化偏向输入均值 $\\boldsymbol{\\mu}$ 的方向，与向 $\\mathbf{C}$ 的主特征向量对齐的趋势相竞争。\n\n现在我们评估给出的选项。\n\n**A. 在相关性赫布规则下，期望权重动态满足 $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\left(\\mathbf{C}\\,\\mathbf{w} + (\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}\\right)$，这意味着当 $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ 时，权重会沿着 $\\boldsymbol{\\mu}$ 方向发生无界漂移；如果没有额外的归一化，不存在有限范数的稳定不动点。**\n我们的推导与所给的动态方程相符。项 $\\eta(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$ 沿着 $\\boldsymbol{\\mu}$ 的方向产生正反馈，导致无界增长。这一项和 $\\eta\\mathbf{C}\\mathbf{w}$ 项（由于 $\\lambda_{\\text{max}} > 0$）都会导致范数增长，因此系统是不稳定的，没有有限范数的稳定不动点。\n**结论：正确。**\n\n**B. 在协方差赫布规则下，期望权重动态满足 $\\dfrac{d\\mathbf{w}}{dt} = \\eta\\, \\mathbf{C}\\,\\mathbf{w}$，因此 $\\mathbf{w}$ 的方向会与 $\\mathbf{C}$ 的最大特征值所对应的特征向量对齐，但在没有归一化的情况下，范数 $\\lVert \\mathbf{w} \\rVert$ 会无界增长。**\n我们的推导证实了动态为 $\\frac{d\\mathbf{w}}{dt} = \\eta \\mathbf{C}\\mathbf{w}$。这是幂迭代法，它导致 $\\mathbf{w}$ 的方向收敛到 $\\mathbf{C}$ 的主特征向量。由于 $\\lambda_{\\text{max}} > 0$，$\\mathbf{w}$ 的范数会以 $e^{\\eta\\lambda_{\\text{max}}t}$ 的形式指数增长。\n**结论：正确。**\n\n**C. 仅协方差赫布规则就能保证对于任何半正定矩阵 $\\mathbf{C}$，无论 $\\boldsymbol{\\mu}$ 的值是多少，$\\mathbf{w}$ 都能全局稳定地收敛到一个有界不动点。**\n这是错误的。正如对B的分析所确立的，只要 $\\mathbf{C}$ 有一个正特征值，协方差规则就会导致权重范数的无界增长。它不会收敛到一个有界不动点（除了在 $\\mathbf{w}=\\mathbf{0}$ 处的不稳定不动点）。\n**结论：不正确。**\n\n**D. 用 Oja 规则替换任一赫布规则后，当输入是均值中心化的（$\\boldsymbol{\\mu} = \\mathbf{0}$）时，期望动态变为 $\\dfrac{d\\mathbf{w}}{dt} = \\mathbf{C}\\,\\mathbf{w} - (\\mathbf{w}^\\top \\mathbf{C}\\,\\mathbf{w})\\, \\mathbf{w}$，从而使 $\\mathbf{w}$ 稳定收敛到与 $\\mathbf{C}$ 最大特征值对应的单位范数特征向量；当 $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ 且使用原始输入时，漂移中会出现一个额外的项 $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$，该项会使解偏向 $\\boldsymbol{\\mu}$，除非输入被中心化。**\n对 Oja 规则的分析证实了这个陈述的两个部分。对于 $\\boldsymbol{\\mu}=\\mathbf{0}$，动态（相差一个比例因子 $\\eta$）和稳定性属性的描述是正确的。对于 $\\boldsymbol{\\mu}\\neq\\mathbf{0}$，动态中的有效“漂移”项从 $\\mathbf{C}\\mathbf{w}$ 变为 $(\\mathbf{C}+\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top)\\mathbf{w}$，其中包括了额外的项 $(\\mathbf{w}^\\top\\boldsymbol{\\mu})\\boldsymbol{\\mu}$。这一项正确地描述了朝向输入均值的偏置。\n**结论：正确。**\n\n**E. 当 $\\boldsymbol{\\mu} \\neq \\mathbf{0}$ 时，相关性赫布规则是稳定的，因为项 $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ 起到了权重衰减的作用，从而减小了 $\\lVert \\mathbf{w} \\rVert$。**\n这个陈述提出了一个根本错误的断言。项 $(\\mathbf{w}^\\top \\boldsymbol{\\mu})\\, \\boldsymbol{\\mu}$ 是一个正反馈项，而不是衰减项。一个衰减项应该有一个负号（例如，$-\\gamma\\mathbf{w}$）。我们对范数导数的分析 $\\frac{d}{dt}\\lVert\\mathbf{w}\\rVert^2 = 2\\eta (\\mathbf{w}^\\top\\mathbf{C}\\mathbf{w} + (\\mathbf{w}^\\top\\boldsymbol{\\mu})^2)$ 表明，两个项都促进了范数的增长，保证了不稳定性，而不是稳定性。\n**结论：不正确。**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "Oja规则通过一个惩罚项巧妙地实现了对突触权重向量的$L_2$范数归一化，从而将学习动态引向输入数据的主成分。然而，$L_2$归一化只是众多可能性中的一种。本练习旨在探索不同归一化约束对学习结果的根本性影响，要求你对比在$L_2$和$L_1$范数约束下最大化输出方差的动态。通过这个对比，你将深入理解为何不同的“突触总强度”约束会导致神经元发展出截然不同的表征策略——分别是密集的主成分和稀疏的特征选择。",
            "id": "4025498",
            "problem": "一个线性神经元接收零均值输入 $x \\in \\mathbb{R}^d$，其协方差矩阵 $C \\in \\mathbb{R}^{d \\times d}$ 是对称半正定的。其输出为 $y = w^\\top x$，其中 $w \\in \\mathbb{R}^d$ 是突触权重向量。考虑在固定范数突触归一化约束下使期望输出功率上升的突触可塑性规则，因此在连续时间极限下，该动力学实现了对目标函数 $J(w) = \\mathbb{E}[y^2] = w^\\top C w$ 的约束上升，约束条件为 $L_2$ 约束 $\\|w\\|_2 = 1$ 或 $L_1$ 约束 $\\|w\\|_1 = 1$。这种设置是带有突触归一化的 Hebbian 学习的标准抽象，其中 Oja 法则是一个经典的 $L_2$ 归一化实例。假设一个数据体系，其中有一小组 $k$ 个主导特征：存在一个索引集 $S \\subset \\{1,\\dots,d\\}$，其大小为 $|S| = k$ 且 $k \\ll d$，使得主子矩阵 $C_{SS}$ 的特征值远大于 $C$ 限制在 $S^c$ 上的特征值，并且在 $S$ 内，$C$ 的非对角线元素不全为零，因此 $C$ 的最大特征向量在 $S$ 中的所有坐标上都具有非零载荷（符号相同）。在此体系下，通过选择所有关于不动点、其稳定性以及在两种不同范数约束下收敛后的 $w$ 的稀疏性的正确陈述来回答以下问题。\n\nA. 在 $L_2$ 约束下，渐近稳定不动点与 $C$ 的最大特征值相关的单位范数特征向量重合（在全局符号上有所不同）。在所述的具有相互关联的主导特征的体系中，收敛后的 $w$ 通常在 $S$ 的所有坐标上都有非零分量（并且在环境基中不一定是稀疏的）。\n\nB. 在 $L_1$ 约束下，如果 $C$ 是对角的，且具有唯一的严格最大对角元素 $C_{ii}$，那么约束上升的每个渐近稳定不动点在坐标 $i$ 处都恰好有一个非零项（符号可正可负），即 $w = \\pm e_i$，而所有其他坐标都恰好为零。\n\nC. 在 $L_2$ 约束下，如果 $C$ 的前两个最大特征值之间存在简单的谱隙，则存在一个由前两个最大特征向量的生成空间中所有单位向量组成的连续单参数族（一个圆）的渐近稳定不动点。\n\nD. 在 $L_1$ 约束下，当有两个相等的主导方差且其他方差可忽略不计时，存在一个渐近稳定不动点的连续体，在 $L_1$ 球面上形成一个非平凡的线段，其在两个主导坐标上都具有非零权重。\n\nE. 在 $L_2$ 约束下，非主特征向量是不稳定的，稳定性由谱隙决定；在 $L_1$ 约束下，当多个主导特征完全相等时（例如，在对角矩阵 $C$ 中有相等的对角元素），存在多个与相等特征相对应的离散稀疏吸引子（符号对称），但不存在稳定不动点的连续体。",
            "solution": "用户需要对一个线性神经元的不动点及其稳定性进行分析，该神经元的权重会自适应地在 $L_2$ 或 $L_1$ 范数约束下最大化输出方差。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n- 神经元输入：$x \\in \\mathbb{R}^d$，且 $\\mathbb{E}[x]=0$。\n- 输入协方差：$C = \\mathbb{E}[xx^\\top] \\in \\mathbb{R}^{d \\times d}$，对称半正定。\n- 神经元输出：$y = w^\\top x$，权重向量为 $w \\in \\mathbb{R}^d$。\n- 目标函数：最大化 $J(w) = \\mathbb{E}[y^2] = \\mathbb{E}[(w^\\top x)(x^\\top w)] = w^\\top \\mathbb{E}[xx^\\top] w = w^\\top C w$。\n- 约束条件：动力学被约束在 $\\|w\\|_2 = 1$ 或 $\\|w\\|_1 = 1$ 上。\n- 特殊数据体系：存在一个索引集 $S \\subset \\{1,\\dots,d\\}$，其大小为 $|S| = k \\ll d$。与 $S$ 中坐标所张成的子空间相关的特征值远大于其他特征值。子矩阵 $C_{SS}$ 具有非零的非对角线元素，并且 $C$ 的最大特征向量在 $S$ 中的所有索引上都具有同号的非零分量。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学依据：** 该问题是计算神经科学和机器学习中的一个标准表述。在 $\\|w\\|_2=1$ 约束下最大化 $w^\\top C w$ 是主成分分析（PCA）的定义。所描述的动力学对应于 PCA 的神经实现，如 Oja 法则。使用 $L_1$ 约束与稀疏 PCA 相关。这些都是公认的基本概念。\n- **良构性：** 该问题是良构的。它要求在一个紧集上求解一个约束优化问题的不动点的性质。目标函数是连续的，约束集（$L_1$ 和 $L_2$ 单位球面）是紧的，这保证了最大值的存在。在梯度上升动力学下分析这些最大值的稳定性是一个标准程序。\n- **客观性：** 语言是技术性的、精确的，并且没有主观因素。\n\n**第 3 步：结论与行动**\n问题陈述在科学上是合理的、良构的且客观的。它是有效的。我将继续对每个选项进行详细分析。\n\n### 求解推导\n\n该问题涉及在范数约束下，寻找目标函数 $J(w) = w^\\top C w$ 上连续时间梯度上升动力学的稳定不动点。目标函数的梯度是 $\\nabla_w J(w) = 2Cw$。\n\n**$L_2$ 约束分析：$\\|w\\|_2 = 1$**\n这是经典的主成分分析（PCA）问题。约束优化的不动点是 $Cw = \\lambda w$ 的解，其中 $\\lambda$ 是拉格朗日乘子。因此，不动点是协方差矩阵 $C$ 的单位范数特征向量。在不动点 $w=v_i$（其中 $v_i$ 是特征值为 $\\lambda_i$ 的特征向量）处的目标函数值为 $J(v_i) = v_i^\\top C v_i = v_i^\\top (\\lambda_i v_i) = \\lambda_i \\|v_i\\|_2^2 = \\lambda_i$。为了最大化 $J(w)$，必须选择与最大特征值 $\\lambda_{\\max}$ 对应的特征向量。\n\n为了分析稳定性，我们考虑投影到球面上的梯度流，其一般形式为 $\\dot{w} \\propto (I - ww^\\top)Cw$。Oja 法则导出连续时间动力学 $\\dot{w} = Cw - (w^\\top Cw)w$。对这些动力学的稳定性分析表明，一个特征向量 $v_i$ 是渐近稳定不动点，当且仅当其对应的特征值 $\\lambda_i$ 是 $C$ 的唯一最大特征值。如果 $\\lambda_i  \\lambda_{\\max}$，任何在最大特征向量 $v_{\\max}$ 方向上的小扰动都会增长，使得 $v_i$ 成为一个不稳定的不动点（具体来说，是一个鞍点）。如果最大特征值是简并的（例如 $\\lambda_1 = \\lambda_2 > \\lambda_3$），那么由相应特征向量生成的整个子空间是一个稳定不动点流形。\n\n**$L_1$ 约束分析：$\\|w\\|_1 = 1$**\n这种表述被称为稀疏 PCA。约束流形是一个交叉多胞体，它在 $\\pm e_i$（标准基向量）处有“角”，并有平坦的“面”。这种几何结构与二次目标函数相结合，倾向于产生稀疏解，意味着最优 $w$ 的许多分量都恰好为零。最大值的 Karush-Kuhn-Tucker (KKT) 条件涉及 $L_1$ 范数的次梯度。这通常导致解的支撑集（其非零元素的集合）很小。不动点的稳定性取决于目标函数在约束曲面上的局部几何形状。梯度上升动力学将驱动权重向量 $w$ 朝着 $L_1$ 球面上 $J(w)$ 的局部极大值移动。\n\n### 逐项分析\n\n**A. 在 $L_2$ 约束下，渐近稳定不动点与 $C$ 的最大特征值相关的单位范数特征向量重合（在全局符号上有所不同）。在所述的具有相互关联的主导特征的体系中，收敛后的 $w$ 通常在 $S$ 的所有坐标上都有非零分量（并且在环境基中不一定是稀疏的）。**\n- 陈述的第一部分是 PCA 和 Hebbian 学习理论的标准结果，如上所述。假设最大特征值是简单的（非简并的），那么唯一的渐近稳定不动点是 $\\pm v_{\\max}$，即与 $\\lambda_{\\max}$ 对应的特征向量。\n- 第二部分涉及问题中描述的特定数据体系。问题明确假设“$C$ 的最大特征向量……在 $S$ 的所有坐标上都具有非零载荷”。由于动力学收敛到这个最大特征向量，收敛后的权重向量 $w$ 确实会在 $S$ 的所有坐标上都有非零分量。主成分通常不是稀疏的，因此收敛后的 $w$ 在 $S$ 之外也可能有非零（尽管可能很小）的分量。\n- **结论：正确。**\n\n**B. 在 $L_1$ 约束下，如果 $C$ 是对角的，且具有唯一的严格最大对角元素 $C_{ii}$，那么约束上升的每个渐近稳定不动点在坐标 $i$ 处都恰好有一个非零项（符号可正可负），即 $w = \\pm e_i$，而所有其他坐标都恰好为零。**\n- 在这种情况下，$C$ 是对角的，所以对于 $j \\neq k$，$C_{jk} = 0$。目标函数变为 $J(w) = w^\\top C w = \\sum_{j=1}^d C_{jj}w_j^2$。我们希望在 $\\sum_{j=1}^d |w_j| = 1$ 的约束下最大化它。\n- 设 $C_{ii}$ 是唯一的最大对角元素。对于任何向量 $w$，如果其某个权重在另一个分量上 $w_j \\neq 0$ ($j \\neq i$)，我们可以通过将该权重移到第 $i$ 个分量来增加目标函数值。具体来说，如果我们将 $|w_j|$ 减少一个很小的量 $\\delta > 0$，并将 $|w_i|$ 增加 $\\delta$（保持 $L_1$ 范数不变），$J(w)$ 的变化量大约与 $2|w_i|\\delta C_{ii} - 2|w_j|\\delta C_{jj}$ 成正比，再加上 $\\delta^2$ 的高阶项。由于 $C_{ii} > C_{jj}$，我们总是可以构造一个增加 $J(w)$ 的变化，除非所有权重都在第 $i$ 个分量上。\n- 因此，全局极大值出现在 $w = \\pm e_i$ 处，此时 $J(w)=C_{ii}$。这些是 $L_1$ 球面的“角”。梯度上升动力学将收敛到这两个点之一，使它们成为唯一的渐近稳定不动点。\n- **结论：正确。**\n\n**C. 在 $L_2$ 约束下，如果 $C$ 的前两个最大特征值之间存在简单的谱隙，则存在一个由前两个最大特征向量的生成空间中所有单位向量组成的连续单参数族（一个圆）的渐近稳定不动点。**\n- “前两个最大特征值之间存在简单的谱隙”意味着 $\\lambda_1 > \\lambda_2 \\ge \\dots$。这表示最大特征值 $\\lambda_1$ 是唯一的（非简并的）。\n- 如我们在对 $L_2$ 情况的一般分析中所确立的，如果最大特征值是唯一的，那么唯一的渐近稳定不动点是相应的特征向量 $\\pm v_1$。只有两个这样的点。\n- 只有当最大特征值是简并的，即 $\\lambda_1 = \\lambda_2$ 时，才会出现一个连续的稳定不动点族（一个圆）。但前提明确说明存在谱隙。因此，在 $\\{v_1, v_2\\}$ 的生成空间中，任何不是 $\\pm v_1$ 的向量都不是稳定不动点。实际上，这样的向量甚至根本不是不动点（除非它们是 $\\pm v_2$，而 $\\pm v_2$ 是不稳定的）。\n- **结论：不正确。**\n\n**D. 在 $L_1$ 约束下，当有两个相等的主导方差且其他方差可忽略不计时，存在一个渐近稳定不动点的连续体，在 $L_1$ 球面上形成一个非平凡的线段，其在两个主导坐标上都具有非零权重。**\n- 此场景假设一个对角矩阵 $C$，其中 $C_{11} = C_{22} > C_{jj}$ 对于 $j > 2$。目标函数为 $J(w) = C_{11} w_1^2 + C_{11} w_2^2 + \\sum_{j>2} C_{jj} w_j^2$。\n- 当对所有 $j>2$ 都有 $w_j=0$ 且 $|w_1|+|w_2|=1$ 时，$J(w)$ 达到最大值。这组点在 $(w_1, w_2)$ 平面上形成一个正方形，这是一个全局极大值的连续体。\n- 然而，该陈述是关于*渐近稳定*不动点的。我们来分析这个集合内部的稳定性。设 $|w_1|=\\alpha$ 和 $|w_2|=1-\\alpha$，其中 $\\alpha \\in [0,1]$。目标函数值为 $J(\\alpha) = C_{11}(\\alpha^2 + (1-\\alpha)^2)$。这个关于 $\\alpha$ 的函数是一个开口向上的抛物线（$J''(\\alpha) = 4C_{11} > 0$），其最小值在 $\\alpha=1/2$ 处。最大值出现在区间的边界，即 $\\alpha=0$ 和 $\\alpha=1$。\n- 这意味着线段内部的任何点（其中 $|w_1|,|w_2| > 0$）在梯度上升动力学下都是不稳定的。动力学将把解推向其中一个权重为零的角点（即推向 $w=\\pm e_1$ 或 $w=\\pm e_2$）。因此，不存在*稳定*不动点的连续体。\n- **结论：不正确。**\n\n**E. 在 $L_2$ 约束下，非主特征向量是不稳定的，稳定性由谱隙决定；在 $L_1$ 约束下，当多个主导特征完全相等时（例如，在对角矩阵 $C$ 中有相等的对角元素），存在多个与相等特征相对应的离散稀疏吸引子（符号对称），但不存在稳定不动点的连续体。**\n- 第一部分，关于 $L_2$ 约束，是对稳定性分析的正确总结。对于特征值 $\\lambda_i  \\lambda_{\\max}$ 的特征向量 $v_i$ 是不稳定的，因为沿具有更大特征值的特征向量的扰动将会增长。这种变化的速率 $(\\lambda_j - \\lambda_i)$ 取决于谱隙。\n- 第二部分，关于 $L_1$ 约束，描述了为选项 D 分析的情况。当主导特征相等时（$C_{11} = C_{22}$），分析表明稳定不动点是离散的稀疏解 $w=\\pm e_1$ 和 $w=\\pm e_2$。这些是“与相等特征相对应的多个离散稀疏吸引子”。分析还表明“不存在稳定不动点的连续体”。这是一个正确的描述。\n- **结论：正确。**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "现实世界中的感觉输入统计特性很少是静态的。一个有效的学习规则必须能够追踪这些变化。本练习将Oja规则置于一个动态非平稳的环境中，其中输入数据的主成分方向在持续旋转。你的任务是推导神经元的突触权重在追踪这个旋转特征时产生的稳态“相位滞后”。通过这个练习，你将具体地理解学习率 $\\eta$ 在适应性系统中的关键作用，并揭示学习系统在追踪动态目标时速度与稳定性之间的内在权衡。",
            "id": "4025497",
            "problem": "考虑一个单一线性神经元，其接收一个二维零均值输入 $x(t) \\in \\mathbb{R}^{2}$，该输入的协方差矩阵 $C(t)$ 是时变的。突触权重向量为 $w(t) \\in \\mathbb{R}^{2}$。该突触遵循 Oja 法则，这是一种归一化的赫布可塑性定律，其确定性平均场形式由下式给出\n$$\n\\dot{w}(t) \\;=\\; \\eta \\left( C(t)\\, w(t) \\;-\\; \\big(w(t)^{\\top} C(t)\\, w(t)\\big)\\, w(t) \\right),\n$$\n其中 $\\eta>0$ 是学习率。假设输入协方差是秩为一的，且主轴在旋转，\n$$\nC(t) \\;=\\; e_{1}(t)\\, e_{1}(t)^{\\top}, \\qquad e_{1}(t) \\;=\\; \\begin{pmatrix}\\cos\\big(\\Omega t\\big) \\\\ \\sin\\big(\\Omega t\\big)\\end{pmatrix},\n$$\n其中 $\\Omega>0$ 是一个恒定的漂移角频率，角度以弧度为单位。定义有符号角滞后 $\\delta$ 为权重方向与瞬时最大特征向量方向之间的稳态恒定差异，即，\n$$\n\\delta \\;=\\; \\psi(t) \\;-\\; \\theta(t),\n$$\n在锁相状态下，$\\delta$ 是不随时间变化的，其中 $w(t) = \\big(\\cos\\psi(t),\\, \\sin\\psi(t)\\big)^{\\top}$，$e_{1}(t) = \\big(\\cos\\theta(t),\\, \\sin\\theta(t)\\big)^{\\top}$，且 $\\theta(t) = \\Omega t$。从这些定义和给定的动力学出发，在系统达到锁相的假设下，推导稳态有符号滞后 $\\delta$ 作为 $\\eta$ 和 $\\Omega$ 的函数的闭式解析表达式。将你的最终答案表示为以弧度为单位的单个解析表达式。无需进行数值计算。",
            "solution": "用户想要解决一个计算神经科学中的问题。\n\n### 步骤 1：问题验证\n\n**1. 提取已知条件：**\n*   **神经元模型：** 单一线性神经元，权重向量为 $w(t) \\in \\mathbb{R}^{2}$。\n*   **输入：** 零均值输入 $x(t) \\in \\mathbb{R}^{2}$。\n*   **学习法则（Oja 法则）：** 权重向量 $w(t)$ 的确定性平均场动力学由下式给出：\n    $$ \\dot{w}(t) = \\eta \\left( C(t)\\, w(t) - \\big(w(t)^{\\top} C(t)\\, w(t)\\big)\\, w(t) \\right) $$\n    其中 $\\eta > 0$ 是学习率。\n*   **输入协方差：** 输入协方差矩阵 $C(t)$ 是时变的、秩为一的，并定义为：\n    $$ C(t) = e_{1}(t)\\, e_{1}(t)^{\\top} $$\n    其主特征向量 $e_{1}(t)$ 由下式给出：\n    $$ e_{1}(t) = \\begin{pmatrix}\\cos\\big(\\Omega t\\big) \\\\ \\sin\\big(\\Omega t\\big)\\end{pmatrix} $$\n    其中 $\\Omega > 0$ 是一个恒定的角频率。\n*   **参数化：** 权重向量由其角度 $\\psi(t)$ 参数化为 $w(t) = \\big(\\cos\\psi(t),\\, \\sin\\psi(t)\\big)^{\\top}$，这意味着其范数固定为 $\\|w(t)\\| = 1$。主特征向量由其角度 $\\theta(t) = \\Omega t$ 参数化为 $e_{1}(t) = \\big(\\cos\\theta(t),\\, \\sin\\theta(t)\\big)^{\\top}$。\n*   **稳态条件（锁相）：** 假设系统处于锁相状态，其中有符号角滞后 $\\delta = \\psi(t) - \\theta(t)$ 是一个不随时间变化的常数。\n*   **目标：** 推导稳态滞后 $\\delta$ 作为 $\\eta$ 和 $\\Omega$ 的函数的闭式解析表达式。\n\n**2. 使用提取的已知条件进行验证：**\n*   **科学依据：** 该问题在计算神经科学中有坚实的基础。Oja 法则是突触可塑性的一个典型模型，分析其在时变输入下的行为是一个标准且有意义的理论问题。\n*   **适定性：** 该问题提供了一个明确定义的微分方程系统，并要求在一个清晰的假设（锁相）下求一个特定的稳态属性。给出的条件是充分且一致的。\n*   **客观性：** 问题陈述由精确的数学定义构成，没有任何主观或模棱两可的语言。\n*   **缺陷检查表：** 该问题没有违反任何无效性标准。它在科学上是合理的、可形式化的、完整的和适定的。\n\n**3. 结论与行动：**\n*   **结论：** 问题是有效的。\n*   **行动：** 继续推导解答。\n\n### 步骤 2：解答推导\n\n解答过程是通过找到权重向量时间导数 $\\dot{w}(t)$ 的两种不同表达式并令它们相等。一个表达式来自给定的 Oja 法则，另一个来自旋转权重向量的运动学定义。\n\n首先，我们使用提供的参数化来简化 Oja 法则中的各项。权重向量 $w(t)$ 和主特征向量 $e_{1}(t)$ 之间的标量积是：\n$$ w(t)^{\\top} e_{1}(t) = \\cos\\psi(t)\\cos\\theta(t) + \\sin\\psi(t)\\sin\\theta(t) = \\cos(\\psi(t)-\\theta(t)) = \\cos(\\delta) $$\n由于在锁相状态下 $\\delta$ 是常数，因此该乘积也是常数。\n\n现在，我们将此代入 Oja 法则的各组成部分。项 $C(t)w(t)$ 变为：\n$$ C(t)w(t) = \\left(e_{1}(t)e_{1}(t)^{\\top}\\right)w(t) = e_{1}(t)\\left(e_{1}(t)^{\\top}w(t)\\right) = e_{1}(t)\\cos(\\delta) $$\n二次项 $w(t)^{\\top}C(t)w(t)$ 变为：\n$$ w(t)^{\\top}C(t)w(t) = w(t)^{\\top}\\left(e_{1}(t)\\cos(\\delta)\\right) = \\left(w(t)^{\\top}e_{1}(t)\\right)\\cos(\\delta) = \\cos(\\delta)\\cos(\\delta) = \\cos^{2}(\\delta) $$\n将这些表达式代回 Oja 法则方程，得到我们关于 $\\dot{w}(t)$ 的第一个表达式：\n$$ \\dot{w}(t) = \\eta \\left( e_{1}(t)\\cos(\\delta) - \\cos^{2}(\\delta)w(t) \\right) $$\n\n接下来，我们从其运动学定义推导 $\\dot{w}(t)$ 的第二个表达式。权重向量为 $w(t) = \\begin{pmatrix}\\cos\\psi(t) \\\\ \\sin\\psi(t)\\end{pmatrix}$。其时间导数为：\n$$ \\dot{w}(t) = \\begin{pmatrix}-\\sin\\psi(t)\\dot{\\psi}(t) \\\\ \\cos\\psi(t)\\dot{\\psi}(t)\\end{pmatrix} = \\dot{\\psi}(t) \\begin{pmatrix}-\\sin\\psi(t) \\\\ \\cos\\psi(t)\\end{pmatrix} $$\n向量 $\\begin{pmatrix}-\\sin\\psi(t) \\\\ \\cos\\psi(t)\\end{pmatrix}$ 是与 $w(t)$ 正交的单位向量，我们将其表示为 $w_{\\perp}(t)$。\n锁相条件 $\\delta = \\psi(t) - \\theta(t) = \\text{常数}$ 意味着它们的时间导数相等：$\\dot{\\psi}(t) = \\dot{\\theta}(t)$。因为 $\\theta(t)=\\Omega t$，我们有 $\\dot{\\theta}(t) = \\Omega$。因此，$\\dot{\\psi}(t) = \\Omega$。\n将此代入 $\\dot{w}(t)$ 的表达式中，得到我们的第二个表达式：\n$$ \\dot{w}(t) = \\Omega w_{\\perp}(t) $$\n\n现在，我们令两个推导出的 $\\dot{w}(t)$ 表达式相等：\n$$ \\Omega w_{\\perp}(t) = \\eta \\left( e_{1}(t)\\cos(\\delta) - \\cos^{2}(\\delta)w(t) \\right) $$\n为了求解 $\\delta$，我们将这个向量方程投影到基向量 $w_{\\perp}(t)$ 上。我们用 $w_{\\perp}(t)^{\\top}$ 对等式两边取点积：\n$$ w_{\\perp}(t)^{\\top} \\left( \\Omega w_{\\perp}(t) \\right) = w_{\\perp}(t)^{\\top} \\left( \\eta e_{1}(t)\\cos(\\delta) - \\eta \\cos^{2}(\\delta)w(t) \\right) $$\n让我们计算两边的点积。\n左边：\n$$ \\Omega \\left( w_{\\perp}(t)^{\\top}w_{\\perp}(t) \\right) = \\Omega \\|w_{\\perp}(t)\\|^{2} = \\Omega $$\n右边：\n$$ \\eta\\cos(\\delta) \\left(w_{\\perp}(t)^{\\top}e_{1}(t)\\right) - \\eta\\cos^{2}(\\delta) \\left(w_{\\perp}(t)^{\\top}w(t)\\right) $$\n第二项为零，因为 $w_{\\perp}(t)$ 和 $w(t)$ 是正交的。我们需要计算标量积 $w_{\\perp}(t)^{\\top}e_{1}(t)$：\n$$ w_{\\perp}(t)^{\\top}e_{1}(t) = \\begin{pmatrix}-\\sin\\psi(t)  \\cos\\psi(t)\\end{pmatrix} \\begin{pmatrix}\\cos\\theta(t) \\\\ \\sin\\theta(t)\\end{pmatrix} $$\n$$ = \\cos\\psi(t)\\sin\\theta(t) - \\sin\\psi(t)\\cos\\theta(t) = \\sin(\\theta(t)-\\psi(t)) $$\n因为 $\\delta = \\psi(t)-\\theta(t)$，我们有 $\\theta(t)-\\psi(t) = -\\delta$。因此：\n$$ w_{\\perp}(t)^{\\top}e_{1}(t) = \\sin(-\\delta) = -\\sin(\\delta) $$\n将这些结果代回投影方程：\n$$ \\Omega = \\eta\\cos(\\delta) \\left(-\\sin(\\delta)\\right) $$\n$$ \\Omega = -\\eta\\sin(\\delta)\\cos(\\delta) $$\n使用三角恒等式 $\\sin(2x)=2\\sin(x)\\cos(x)$，我们可以将方程重写为：\n$$ \\Omega = -\\frac{\\eta}{2}\\sin(2\\delta) $$\n求解 $\\sin(2\\delta)$：\n$$ \\sin(2\\delta) = -\\frac{2\\Omega}{\\eta} $$\n为了求得 $\\delta$，我们对两边取反正弦。为了存在一个稳定的锁相解，权重向量必须滞后于特征向量，这意味着 $\\delta$ 是一个小的负角度。反正弦函数的主值提供了稳定解。\n$$ 2\\delta = \\arcsin\\left(-\\frac{2\\Omega}{\\eta}\\right) $$\n使用性质 $\\arcsin(-x) = -\\arcsin(x)$，我们得到：\n$$ 2\\delta = -\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right) $$\n最后，求解 $\\delta$：\n$$ \\delta = -\\frac{1}{2}\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right) $$\n该表达式要求 $|\\frac{2\\Omega}{\\eta}| \\le 1$，或 $\\eta \\ge 2\\Omega$，以使实数解存在。这个条件表明，为了让神经元能够跟踪输入的主成分，学习率必须相对于旋转频率足够大。问题假设达到了这样的锁相状态。负号表示权重向量滞后于旋转的特征向量，这在物理上是符合预期的。",
            "answer": "$$\n\\boxed{-\\frac{1}{2}\\arcsin\\left(\\frac{2\\Omega}{\\eta}\\right)}\n$$"
        }
    ]
}