{
    "hands_on_practices": [
        {
            "introduction": "Before delving into the complexities of learning, we must first understand the fundamental architecture of the cerebellar microcircuit. This exercise provides a concrete starting point by translating the canonical pathway—from mossy fiber input to deep nuclear output—into a precise mathematical representation. By constructing an adjacency matrix, you will practice modeling signal flow in a neural circuit and calculate the net effect of this key polysynaptic path, revealing its intrinsically inhibitory nature. ",
            "id": "4027513",
            "problem": "Consider a canonical cerebellar microcircuit comprised of the following four node types: mossy fibers ($M$), granule cells ($G$), Purkinje cells ($P$), and deep cerebellar nuclei ($D$). In biological terms, the mossy fiber to granule cell synapse is glutamatergic and therefore excitatory, the parallel fiber (granule cell axon) to Purkinje cell synapse is excitatory, and the Purkinje cell to deep cerebellar nuclei synapse is gamma-aminobutyric acid (GABA)-mediated and therefore inhibitory. In a linear rate-based model at the population level, let the state vector be $x \\in \\mathbb{R}^{4}$ ordered as $(M, G, P, D)$, and let the synaptic graph be represented by an adjacency matrix $A \\in \\mathbb{R}^{4 \\times 4}$, where $A_{j i}$ is the signed gain from node $i$ to node $j$ per unit time.\n\nThe nonzero synaptic gains are given as follows: from mossy fibers to granule cells $w_{G M} = 0.8$, from granule cells to Purkinje cells $w_{P G} = 0.9$, and from Purkinje cells to deep cerebellar nuclei $w_{D P} = 1.2$. Because the Purkinje cell to deep cerebellar nuclei synapse is inhibitory, it contributes a negative sign. All other direct connections among these four nodes are set to zero for this construction.\n\n1. Using these rules and the node order $(M, G, P, D)$, construct the directed adjacency matrix $A$.\n2. Starting from the linear time-invariant population dynamics $x_{t+1} = A x_{t}$ and first principles of linear composition of synaptic actions, derive the net signed gain transmitted exclusively along the Purkinje cell branch $M \\to G \\to P \\to D$. Express the final result as a single dimensionless real number.\n\nYour final answer must be a single real number. No rounding is required. Express the final answer as a dimensionless quantity.",
            "solution": "First, we address the construction of the directed adjacency matrix $A$. The state vector $x$ is ordered as $(M, G, P, D)$, which corresponds to indices $1, 2, 3, 4$ respectively. The element $A_{ji}$ of the adjacency matrix represents the signed gain from node $i$ (the source) to node $j$ (the target).\n\nThe given connections and their corresponding non-zero matrix elements are:\n1.  The connection from mossy fibers ($M$, index $1$) to granule cells ($G$, index $2$) is excitatory with a gain of $w_{GM} = 0.8$. Therefore, the matrix element $A_{21}$ is positive:\n    $$A_{21} = +w_{GM} = 0.8$$\n2.  The connection from granule cells ($G$, index $2$) to Purkinje cells ($P$, index $3$) is excitatory with a gain of $w_{PG} = 0.9$. Therefore, the matrix element $A_{32}$ is positive:\n    $$A_{32} = +w_{PG} = 0.9$$\n3.  The connection from Purkinje cells ($P$, index $3$) to deep cerebellar nuclei ($D$, index $4$) is inhibitory with a gain of $w_{DP} = 1.2$. Therefore, the matrix element $A_{43}$ is negative:\n    $$A_{43} = -w_{DP} = -1.2$$\n\nThe problem states that all other direct connections among these four nodes are set to zero. This implies that all other elements of the matrix $A$ are $0$. With the ordering $(M, G, P, D)$, the resulting $4 \\times 4$ adjacency matrix $A$ is:\n$$\nA = \n\\begin{pmatrix}\nA_{11}  A_{12}  A_{13}  A_{14} \\\\\nA_{21}  A_{22}  A_{23}  A_{24} \\\\\nA_{31}  A_{32}  A_{33}  A_{34} \\\\\nA_{41}  A_{42}  A_{43}  A_{44}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  0  0  0 \\\\\n0.8  0  0  0 \\\\\n0  0.9  0  0 \\\\\n0  0  -1.2  0\n\\end{pmatrix}\n$$\n\nNext, we derive the net signed gain transmitted exclusively along the Purkinje cell branch $M \\to G \\to P \\to D$. The system dynamics are given by the linear time-invariant equation $x_{t+1} = A x_{t}$. The effect of a signal propagating along a multi-step path is found by composing the actions of the adjacency matrix.\n\nA signal starting at the mossy fibers ($M$) at time $t=0$ propagates through the network.\n- At time $t=1$, the activity from $M$ influences $G$. The gain is given by $A_{21}$.\n- At time $t=2$, the activity from $G$ (which originated from $M$) influences $P$. The gain of this step is $A_{32}$. The cumulative gain from $M$ to $P$ over these two steps is the product $A_{32} \\times A_{21}$.\n- At time $t=3$, the activity from $P$ (which originated from $G$, which originated from $M$) influences $D$. The gain of this final step is $A_{43}$.\n\nThe net signed gain along the entire trisynaptic path $M \\to G \\to P \\to D$ is the product of the individual signed gains for each synaptic step in the sequence. This corresponds to the influence of an initial impulse at node $M$ on node $D$ after three time steps, along this specific path. Mathematically, this is the product of the corresponding matrix elements:\n$$ \\text{Net Gain} = A_{43} \\times A_{32} \\times A_{21} $$\n\nWe calculate the product of the gains:\n$$ \\text{Net Gain} = (-w_{DP}) \\times (+w_{PG}) \\times (+w_{GM}) $$\nSubstituting the given numerical values:\n$$ \\text{Net Gain} = (-1.2) \\times (0.9) \\times (0.8) $$\nFirst, we compute the product of the first two terms:\n$$ (-1.2) \\times (0.9) = -1.08 $$\nThen, we multiply this result by the third term:\n$$ -1.08 \\times 0.8 = -0.864 $$\n\nThe net signed gain along the path $M \\to G \\to P \\to D$ is a dimensionless quantity representing the overall amplification or attenuation and inversion of the signal. The negative sign indicates that the overall effect of this pathway is inhibitory, which is expected due to the final inhibitory synapse from the Purkinje cell.",
            "answer": "$$\n\\boxed{-0.864}\n$$"
        },
        {
            "introduction": "The cerebellum learns by modifying the strength of its synapses, a process governed by precise rules of synaptic plasticity. This practice moves beyond static circuit diagrams to the dynamics of learning, focusing on the well-established phenomenon of Spike-Timing-Dependent Plasticity (STDP) at the parallel fiber-Purkinje cell synapse. You will implement an experimentally-informed plasticity model to compute the total synaptic weight change resulting from a specific, realistic pattern of neural activity, providing direct insight into how long-term depression (LTD) is induced. ",
            "id": "4027448",
            "problem": "Consider a cerebellar Purkinje cell receiving excitatory inputs from Parallel Fibers (PF) and an instructive climbing fiber (CF) complex spike at time $t_{\\mathrm{CF}} = 0$. In an experimentally supported correlation-based learning framework, synaptic weight change at a PF-to-Purkinje synapse is driven by the relative timing between PF spikes and the CF event, following a Spike-Timing-Dependent Plasticity (STDP) kernel $K(\\Delta t)$, where $\\Delta t$ denotes the post-pre interval. Empirically, PF spikes that precede the CF by up to a few hundred milliseconds tend to produce long-term depression (LTD), whereas PF spikes following the CF have weaker and shorter-lived long-term potentiation (LTP). Assume the following experimentally informed STDP kernel:\n$$\nK(\\Delta t) =\n\\begin{cases}\n- A_{-} \\exp\\!\\left(-\\dfrac{\\Delta t}{\\tau_{-}}\\right),  \\Delta t  0 \\\\\nA_{+} \\exp\\!\\left(\\dfrac{\\Delta t}{\\tau_{+}}\\right),  \\Delta t \\le 0\n\\end{cases}\n$$\nwhere $A_{-}$, $\\tau_{-}$, $A_{+}$, and $\\tau_{+}$ are positive constants.\n\nLet the PF input be a burst of $N = 5$ spikes that precede the CF. The burst starts at time $t_{1} = -\\Delta t$ and continues at fixed interspike interval $\\delta$, so that $t_{i} = -\\Delta t + (i-1)\\delta$ for $i \\in \\{1,2,3,4,5\\}$. Short-term facilitation is modeled by an amplitude factor $f_{i} = 1 + \\alpha (i-1)$ multiplying the contribution of the $i$-th PF spike. Assume linear superposition of PF spike contributions to the synaptic update. The total synaptic change is given by\n$$\n\\Delta w = \\eta \\sum_{i=1}^{5} f_{i} \\, K\\!\\left(t_{\\mathrm{CF}} - t_{i}\\right),\n$$\nwith learning rate $\\eta  0$.\n\nUse the following parameter values, consistent with cerebellar plasticity observations:\n- $A_{-} = 0.8$,\n- $\\tau_{-} = 50\\,\\mathrm{ms}$,\n- $A_{+} = 0.2$,\n- $\\tau_{+} = 20\\,\\mathrm{ms}$,\n- $\\Delta t = 50\\,\\mathrm{ms}$,\n- $\\delta = 10\\,\\mathrm{ms}$,\n- $\\alpha = 0.2$,\n- $\\eta = 0.05$.\n\nCompute the sign and magnitude of the total synaptic weight change $\\Delta w$ for this PF firing pattern. Express the final change in synaptic weight as a dimensionless number. Round your answer to four significant figures.",
            "solution": "The objective is to compute the total synaptic weight change, $\\Delta w$, given by the formula:\n$$\n\\Delta w = \\eta \\sum_{i=1}^{5} f_{i} \\, K(t_{\\mathrm{CF}} - t_{i})\n$$\nThe calculation proceeds in several steps:\n1.  Determine the timing difference for each PF spike relative to the CF spike.\n2.  Evaluate the STDP kernel $K$ for each timing difference.\n3.  Calculate the short-term facilitation factor $f_i$ for each spike.\n4.  Compute the sum of the contributions from all spikes and multiply by the learning rate.\n\nFirst, we calculate the argument for the kernel function, which is the post-pre time interval. The postsynaptic event is the CF spike at $t_{\\mathrm{CF}} = 0$. The presynaptic events are the PF spikes at times $t_i$. Let's denote the timing argument for the $i$-th spike as $\\Delta t_i = t_{\\mathrm{CF}} - t_{i}$.\nGiven $t_{i} = -\\Delta t + (i-1)\\delta$, with parameter values $\\Delta t = 50\\,\\mathrm{ms}$ and $\\delta = 10\\,\\mathrm{ms}$:\n$$\n\\Delta t_i = 0 - (-\\Delta t + (i-1)\\delta) = \\Delta t - (i-1)\\delta = 50 - (i-1)10\n$$\nThe units of time (milliseconds, $\\mathrm{ms}$) are consistent throughout, so we can treat the values as dimensionless numbers during calculation.\n\nLet's compute $\\Delta t_i$ for each spike $i \\in \\{1, 2, 3, 4, 5\\}$:\n- For $i=1$: $\\Delta t_1 = 50 - (1-1)10 = 50\\,\\mathrm{ms}$\n- For $i=2$: $\\Delta t_2 = 50 - (2-1)10 = 40\\,\\mathrm{ms}$\n- For $i=3$: $\\Delta t_3 = 50 - (3-1)10 = 30\\,\\mathrm{ms}$\n- For $i=4$: $\\Delta t_4 = 50 - (4-1)10 = 20\\,\\mathrm{ms}$\n- For $i=5$: $\\Delta t_5 = 50 - (5-1)10 = 10\\,\\mathrm{ms}$\n\nAll calculated timing differences $\\Delta t_i$ are positive. This confirms that all PF spikes precede the CF spike. Therefore, we must use the branch of the STDP kernel for $\\Delta t  0$:\n$$\nK(\\Delta t_i) = -A_{-} \\exp\\left(-\\frac{\\Delta t_i}{\\tau_{-}}\\right)\n$$\nwith $A_{-} = 0.8$ and $\\tau_{-} = 50\\,\\mathrm{ms}$.\n\nNext, we calculate the short-term facilitation factor $f_i = 1 + \\alpha (i-1)$ for each spike, with $\\alpha = 0.2$:\n- For $i=1$: $f_1 = 1 + 0.2(1-1) = 1.0$\n- For $i=2$: $f_2 = 1 + 0.2(2-1) = 1.2$\n- For $i=3$: $f_3 = 1 + 0.2(3-1) = 1.4$\n- For $i=4$: $f_4 = 1 + 0.2(4-1) = 1.6$\n- For $i=5$: $f_5 = 1 + 0.2(5-1) = 1.8$\n\nNow we can write the full expression for $\\Delta w$:\n$$\n\\Delta w = \\eta \\sum_{i=1}^{5} f_{i} \\left( -A_{-} \\exp\\left(-\\frac{\\Delta t_i}{\\tau_{-}}\\right) \\right) = -\\eta A_{-} \\sum_{i=1}^{5} f_{i} \\exp\\left(-\\frac{\\Delta t_i}{\\tau_{-}}\\right)\n$$\nSubstituting the known values $\\eta=0.05$ and $A_{-}=0.8$:\n$$\n\\Delta w = - (0.05)(0.8) \\sum_{i=1}^{5} f_{i} \\exp\\left(-\\frac{\\Delta t_i}{50}\\right) = -0.04 \\sum_{i=1}^{5} f_{i} \\exp\\left(-\\frac{\\Delta t_i}{50}\\right)\n$$\nLet's compute each term in the summation, which we denote as $S_{i} = f_{i} \\exp(-\\frac{\\Delta t_i}{50})$:\n- $S_1 = 1.0 \\times \\exp\\left(-\\frac{50}{50}\\right) = 1.0 \\times \\exp(-1.0) \\approx 0.367879$\n- $S_2 = 1.2 \\times \\exp\\left(-\\frac{40}{50}\\right) = 1.2 \\times \\exp(-0.8) \\approx 1.2 \\times 0.449329 = 0.539195$\n- $S_3 = 1.4 \\times \\exp\\left(-\\frac{30}{50}\\right) = 1.4 \\times \\exp(-0.6) \\approx 1.4 \\times 0.548812 = 0.768337$\n- $S_4 = 1.6 \\times \\exp\\left(-\\frac{20}{50}\\right) = 1.6 \\times \\exp(-0.4) \\approx 1.6 \\times 0.670320 = 1.072512$\n- $S_5 = 1.8 \\times \\exp\\left(-\\frac{10}{50}\\right) = 1.8 \\times \\exp(-0.2) \\approx 1.8 \\times 0.818731 = 1.473716$\n\nThe total sum is:\n$$\n\\sum_{i=1}^{5} S_i \\approx 0.367879 + 0.539195 + 0.768337 + 1.072512 + 1.473716 = 4.221639\n$$\nFinally, we compute $\\Delta w$:\n$$\n\\Delta w = -0.04 \\times 4.221639 = -0.16886556\n$$\nThe problem requires the answer to be rounded to four significant figures. The sign of $\\Delta w$ is negative, indicating a net depression of the synaptic weight (LTD), which is consistent with the biology of pre-CF PF activity.\n$$\n\\Delta w \\approx -0.1689\n$$",
            "answer": "$$\n\\boxed{-0.1689}\n$$"
        },
        {
            "introduction": "Having examined the circuit's structure and the rules of its plasticity, we now address the ultimate function: how does the cerebellum learn to control movement? This practice frames motor learning as a supervised function approximation problem, a powerful and influential model of cerebellar computation. You will determine the optimal synaptic weights that allow a population of basis functions—representing granule cell activity—to generate a desired torque profile, exploring the critical difference between a purely mathematical solution and one that respects biological constraints. ",
            "id": "4027515",
            "problem": "You are modeling the transformation performed by the cerebellum during motor learning as a supervised function approximation from kinematic basis functions to a desired torque trajectory. Let there be $m$ basis functions $\\{ \\phi_i(t) \\}_{i=1}^m$ sampled at $n$ time points $\\{ t_k \\}_{k=1}^n$ over an interval of duration $1$ second. Define the design matrix $X \\in \\mathbb{R}^{n \\times m}$ by $X_{k,i} = \\phi_i(t_k)$ and the target torque vector $y \\in \\mathbb{R}^n$ by $y_k = y(t_k)$, where $y(t)$ is the desired torque profile. The torque has physical units Newton meter (N·m), and all angles used inside trigonometric functions are in radians. The modeled cerebellar output is $\\hat{y}(t) = \\sum_{i=1}^m w_i \\phi_i(t)$, where $w \\in \\mathbb{R}^m$ represents synaptic weights.\n\nStarting from first principles of least squares estimation, the unconstrained optimal weights $w^{*}$ minimize the empirical mean squared error\n$$\nJ(w) = \\frac{1}{n} \\sum_{k=1}^n \\left( \\sum_{i=1}^m w_i \\phi_i(t_k) - y(t_k) \\right)^2,\n$$\nsubject to no constraints on $w$. Biological synaptic constraints such as non-negativity require $w_i \\ge 0$ for all $i \\in \\{1,\\dots,m\\}$, which alters the optimization problem to a Non-Negative Least Squares problem.\n\nYour task is to:\n- Compute the unconstrained optimal weights $w^{*}$ by minimizing $J(w)$.\n- Compute the constrained optimal weights $\\tilde{w}$ under $w_i \\ge 0$ for all $i$.\n- Quantify the effect of the non-negativity constraint on the fit quality and the weights.\n\nUse the following test suite. In all tests, use $n = 100$ samples at times $t_k = \\frac{k-1}{n-1}$ seconds for $k \\in \\{1,\\dots,n\\}$, and Gaussian basis functions\n$$\n\\phi_i(t) = \\exp\\left( - \\frac{(t - c_i)^2}{2\\sigma^2} \\right),\n$$\nwhich are dimensionless. For each case, $X_{k,i} = \\phi_i(t_k)$ and $y_k = y(t_k)$ with $y(t)$ specified per case.\n\n- Test Case $1$ (happy path, well-conditioned, positive target):\n  - $m = 3$\n  - Centers $c = [0.2, 0.5, 0.8]$ (seconds)\n  - $\\sigma = 0.08$ (seconds)\n  - Desired torque $y(t) = 1.5 \\sin^2(2\\pi t)$ in N·m.\n- Test Case $2$ (boundary case, nearly collinear basis, narrow target):\n  - $m = 3$\n  - Centers $c = [0.45, 0.5, 0.55]$ (seconds)\n  - $\\sigma = 0.03$ (seconds)\n  - Desired torque $y(t) = \\exp\\left( - \\frac{(t - 0.5)^2}{2 \\cdot 0.02^2} \\right)$ in N·m.\n- Test Case $3$ (edge case, sign-changing target with positive basis):\n  - $m = 3$\n  - Centers $c = [0.25, 0.5, 0.75]$ (seconds)\n  - $\\sigma = 0.06$ (seconds)\n  - Desired torque $y(t) = 0.8 \\sin(2\\pi t)$ in N·m.\n\nFor each test case, compute and return:\n- The root mean square error of the unconstrained solution $RMSE_{\\text{uncon}} = \\sqrt{ \\frac{1}{n} \\sum_{k=1}^n ( (Xw^{*})_k - y_k )^2 }$ in N·m.\n- The root mean square error of the non-negative solution $RMSE_{\\text{nn}} = \\sqrt{ \\frac{1}{n} \\sum_{k=1}^n ( (X\\tilde{w})_k - y_k )^2 }$ in N·m.\n- A boolean indicating whether the unconstrained solution has any negative weights, i.e., whether $\\exists i$ such that $w^{*}_i  0$.\n- The Euclidean norm difference $\\| w^{*} - \\tilde{w} \\|_2$ (dimensionless).\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case results enclosed in square brackets, with each per-case result itself being a list in the order $[RMSE_{\\text{uncon}}, RMSE_{\\text{nn}}, \\text{has\\_neg}, \\| w^{*} - \\tilde{w} \\|_2]$. For example, the output format is $[[r_1,s_1,b_1,d_1],[r_2,s_2,b_2,d_2],[r_3,s_3,b_3,d_3]]$, where $r_i$, $s_i$, and $d_i$ are floats in N·m (for $r_i$ and $s_i$) and dimensionless (for $d_i$), and $b_i$ is a boolean. No rounding is required; report the raw numeric values produced by standard floating-point computation.",
            "solution": "The problem requires us to model a cerebellar motor learning task as a function approximation problem. We will find optimal weights $w$ for a linear combination of basis functions $\\hat{y}(t) = \\sum_{i=1}^m w_i \\phi_i(t)$ that approximates a target torque trajectory $y(t)$. We will solve this problem under two conditions: first, with no constraints on the weights $w_i$, and second, with the biological constraint of non-negativity, $w_i \\ge 0$.\n\nThe relationship between the predicted torque $\\hat{y}$, weights $w$, and basis functions $\\phi_i$ is expressed in matrix form as $\\hat{y} = Xw$. Here, $\\hat{y} \\in \\mathbb{R}^n$ is the vector of predicted torques, $w \\in \\mathbb{R}^m$ is the vector of synaptic weights, and $X \\in \\mathbb{R}^{n \\times m}$ is the design matrix where $X_{k,i} = \\phi_i(t_k)$. The objective is to find weights $w$ that minimize the Mean Squared Error (MSE), $J(w) = \\frac{1}{n} \\|Xw - y\\|_2^2$.\n\n**1. Unconstrained Optimal Weights ($w^{*}$)**\nThis is a standard Ordinary Least Squares (OLS) problem. To find the minimum, we set the gradient of the cost function with respect to $w$ to zero:\n$$\n\\nabla_w J(w) = \\frac{2}{n} X^T(Xw - y) = 0\n$$\nThis leads to the normal equations, $X^T X w = X^T y$. Assuming that $X^T X$ is invertible, the unique solution $w^*$ is:\n$$\nw^* = (X^T X)^{-1} X^T y\n$$\nThis solution is typically found using numerically stable methods like QR decomposition or SVD.\n\n**2. Constrained Optimal Weights ($\\tilde{w}$)**\nWith the constraint $w_i \\ge 0$, the problem becomes a Non-Negative Least Squares (NNLS) optimization:\n$$\n\\text{minimize} \\quad \\|Xw - y\\|_2^2 \\quad \\text{subject to} \\quad w \\ge 0\n$$\nThis is a convex optimization problem solved using iterative algorithms, such as active-set methods.\n\n**3. Computational Strategy and Metrics**\nFor each test case, we perform the following steps:\n1.  Construct the design matrix $X$ and target vector $y$ based on the given parameters.\n2.  Solve for the unconstrained weights $w^*$ using an OLS algorithm.\n3.  Solve for the non-negative weights $\\tilde{w}$ using an NNLS algorithm.\n4.  Calculate the four required metrics:\n    - $RMSE_{\\text{uncon}} = \\sqrt{\\frac{1}{n} \\|Xw^* - y\\|_2^2}$\n    - $RMSE_{\\text{nn}} = \\sqrt{\\frac{1}{n} \\|X\\tilde{w} - y\\|_2^2}$\n    - A boolean indicating if any $w_i^*  0$.\n    - The Euclidean norm difference $\\| w^{*} - \\tilde{w} \\|_2$.\nThe provided code implements this strategy to generate the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Main function to solve the cerebellar learning problem for all test cases.\n    \"\"\"\n\n    def compute_for_case(m, c_centers, sigma, y_func_str):\n        \"\"\"\n        Computes the required metrics for a single test case.\n\n        Args:\n            m (int): Number of basis functions.\n            c_centers (list): List of centers for the Gaussian basis functions.\n            sigma (float): Width of the Gaussian basis functions.\n            y_func_str (str): A string lambda expression for the target torque y(t).\n\n        Returns:\n            list: A list containing [RMSE_uncon, RMSE_nn, has_neg, norm_diff].\n        \"\"\"\n        n = 100\n        t = np.linspace(0, 1, n)\n\n        # Create design matrix X\n        # X_{k,i} = phi_i(t_k)\n        # Use broadcasting to efficiently compute the matrix\n        # t is (n, 1), c_centers is (1, m)\n        # The result of (t[:, np.newaxis] - c_centers) is an (n, m) matrix\n        c_centers_np = np.array(c_centers)\n        X = np.exp(-(t[:, np.newaxis] - c_centers_np[np.newaxis, :])**2 / (2 * sigma**2))\n\n        # Create target vector y\n        # The lambda function is created using eval for dynamic function definition.\n        # This is safe in this context as the strings are defined internally.\n        y_func = eval(y_func_str)\n        y = y_func(t)\n\n        # 1. Unconstrained solution (Ordinary Least Squares)\n        w_star, residuals_star, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        \n        # 2. Constrained solution (Non-Negative Least Squares)\n        w_tilde, _ = nnls(X, y)\n\n        # 3. Compute metrics\n        # RMSE for unconstrained solution\n        y_hat_star = X @ w_star\n        rmse_uncon = np.sqrt(np.mean((y_hat_star - y)**2))\n\n        # RMSE for non-negative solution\n        y_hat_tilde = X @ w_tilde\n        rmse_nn = np.sqrt(np.mean((y_hat_tilde - y)**2))\n\n        # Check if unconstrained solution has negative weights\n        has_neg = np.any(w_star  0)\n\n        # Euclidean norm difference between weight vectors\n        norm_diff = np.linalg.norm(w_star - w_tilde)\n\n        return [rmse_uncon, rmse_nn, bool(has_neg), norm_diff]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: m, centers, sigma, y(t)\n        (3, [0.2, 0.5, 0.8], 0.08, 'lambda t: 1.5 * np.sin(2 * np.pi * t)**2'),\n        # Case 2: m, centers, sigma, y(t)\n        (3, [0.45, 0.5, 0.55], 0.03, 'lambda t: np.exp(-(t - 0.5)**2 / (2 * 0.02**2))'),\n        # Case 3: m, centers, sigma, y(t)\n        (3, [0.25, 0.5, 0.75], 0.06, 'lambda t: 0.8 * np.sin(2 * np.pi * t)')\n    ]\n\n    results = []\n    for case in test_cases:\n        m_val, c_val, sigma_val, y_str = case\n        result = compute_for_case(m_val, c_val, sigma_val, y_str)\n        results.append(result)\n\n    # Format the final output as a string representation of a list of lists.\n    # The str() on each inner list will create a bracketed representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}