{
    "hands_on_practices": [
        {
            "introduction": "A key feature of voltage-based STDP is its departure from purely spike-centric learning, incorporating the influence of subthreshold membrane potential dynamics. This exercise provides a foundational look at this mechanism by isolating the contribution of a single excitatory postsynaptic potential ($EPSP$) to synaptic potentiation. By deriving the potentiation signal from the ground up, you will gain a concrete understanding of how the interplay between presynaptic activity and postsynaptic depolarization, even without a spike, drives synaptic change .",
            "id": "4032662",
            "problem": "A single-compartment leaky integrate-and-fire (LIF) neuron receives one excitatory presynaptic spike at time $t=0$ that evokes a current-based excitatory postsynaptic current (EPSC) with an exponential decay. Assume the subthreshold membrane voltage $V(t)$ evolves according to the linear membrane equation\n$$C\\,\\frac{dV}{dt}=-\\frac{C}{\\tau_{m}}\\left(V(t)-E_{L}\\right)+I_{0}\\,\\exp\\!\\left(-\\frac{t}{\\tau_{s}}\\right)\\,H(t),$$\nwith $V(0^{-})=E_{L}$, where $C$ is the membrane capacitance, $\\tau_{m}$ is the membrane time constant, $E_{L}$ is the leak reversal (resting) potential, $I_{0}>0$ is the EPSC amplitude, $\\tau_{s}>0$ is the synaptic decay constant, and $H(t)$ is the Heaviside step function. There is no postsynaptic spiking. The presynaptic trace $x_{\\mathrm{pre}}(t)$ obeys the standard first-order linear kinetics used in voltage-based Spike-Timing-Dependent Plasticity (STDP),\n$$\\frac{dx_{\\mathrm{pre}}}{dt}=-\\frac{x_{\\mathrm{pre}}}{\\tau_{x}}+\\delta(t),$$\nwith $x_{\\mathrm{pre}}(0^{-})=0$ and $\\tau_{x}>0$. Consider the voltage-gated potentiation term used in voltage-based STDP,\n$$\\mathcal{P}=\\int_{-\\infty}^{\\infty} x_{\\mathrm{pre}}(t)\\,\\big(V(t)-\\theta_{+}\\big)_{+}\\,dt,$$\nwhere $(\\cdot)_{+}$ denotes the positive-part operator, and take the long-term potentiation threshold to equal rest, $\\theta_{+}=E_{L}$, to isolate the purely subthreshold contribution of the EPSP (excitatory postsynaptic potential). Work entirely in the subthreshold regime and assume $\\tau_{m}\\neq \\tau_{s}$, $\\tau_{m}>0$, $\\tau_{s}>0$, and $\\tau_{x}>0$.\n\nStarting from these definitions and equations only (no additional shortcut formulas), derive a closed-form analytic expression for $\\mathcal{P}$ in terms of $I_{0}$, $C$, $\\tau_{m}$, $\\tau_{s}$, and $\\tau_{x}$. Express the final answer symbolically. Do not include units in the final boxed answer.",
            "solution": "The problem requires the derivation of a closed-form analytic expression for the voltage-based STDP potentiation term $\\mathcal{P}$. The process involves solving two first-order linear ordinary differential equations (ODEs), one for the presynaptic trace $x_{\\mathrm{pre}}(t)$ and one for the subthreshold membrane voltage $V(t)$, and then evaluating the integral defining $\\mathcal{P}$.\n\nFirst, we solve for the presynaptic trace $x_{\\mathrm{pre}}(t)$. The governing equation is:\n$$ \\frac{dx_{\\mathrm{pre}}}{dt}=-\\frac{x_{\\mathrm{pre}}}{\\tau_{x}}+\\delta(t) $$\nwith the initial condition $x_{\\mathrm{pre}}(0^{-})=0$. This is a first-order linear ODE with a non-homogeneous term involving the Dirac delta function $\\delta(t)$. To find the effect of the delta function at $t=0$, we integrate the equation over an infinitesimal interval $[0^{-}, 0^{+}]$:\n$$ \\int_{0^{-}}^{0^{+}} \\frac{dx_{\\mathrm{pre}}}{dt} dt + \\frac{1}{\\tau_{x}} \\int_{0^{-}}^{0^{+}} x_{\\mathrm{pre}}(t) dt = \\int_{0^{-}}^{0^{+}} \\delta(t) dt $$\nThe first term evaluates to $x_{\\mathrm{pre}}(0^{+}) - x_{\\mathrm{pre}}(0^{-})$. As $x_{\\mathrm{pre}}(t)$ must be finite, the integral of $x_{\\mathrm{pre}}(t)$ over an infinitesimal interval is zero. The integral of the delta function is $1$ by definition. Substituting the initial condition $x_{\\mathrm{pre}}(0^{-})=0$, we obtain:\n$$ x_{\\mathrm{pre}}(0^{+}) - 0 + 0 = 1 \\implies x_{\\mathrm{pre}}(0^{+}) = 1 $$\nFor $t>0$, the ODE becomes homogeneous:\n$$ \\frac{dx_{\\mathrm{pre}}}{dt}=-\\frac{x_{\\mathrm{pre}}}{\\tau_{x}} $$\nThe general solution is $x_{\\mathrm{pre}}(t) = A\\exp(-t/\\tau_{x})$ for some constant $A$. Applying the condition $x_{\\mathrm{pre}}(0^{+})=1$, we find $A=1$. Combining this with the fact that $x_{\\mathrm{pre}}(t)=0$ for $t<0$, the solution is:\n$$ x_{\\mathrm{pre}}(t) = \\exp\\left(-\\frac{t}{\\tau_{x}}\\right) H(t) $$\nwhere $H(t)$ is the Heaviside step function.\n\nNext, we solve for the membrane voltage $V(t)$. It is convenient to define the voltage relative to the resting potential, $v(t) = V(t) - E_{L}$. Since $E_L$ is a constant, $\\frac{dV}{dt} = \\frac{dv}{dt}$. The initial condition $V(0^{-})=E_{L}$ becomes $v(0^{-})=0$. Substituting $V(t) = v(t) + E_{L}$ into the membrane equation:\n$$ C\\,\\frac{dv}{dt}=-\\frac{C}{\\tau_{m}}v(t)+I_{0}\\,\\exp\\left(-\\frac{t}{\\tau_{s}}\\right)\\,H(t) $$\nDividing by $C$ and rearranging gives a standard first-order linear ODE for $v(t)$:\n$$ \\frac{dv}{dt} + \\frac{1}{\\tau_{m}}v(t) = \\frac{I_{0}}{C}\\exp\\left(-\\frac{t}{\\tau_{s}}\\right)H(t) $$\nFor $t>0$, we can solve this using an integrating factor $\\mu(t) = \\exp\\left(\\int \\frac{1}{\\tau_{m}} dt\\right) = \\exp(t/\\tau_{m})$. Multiplying the ODE by $\\mu(t)$:\n$$ \\frac{d}{dt}\\left(v(t)\\exp\\left(\\frac{t}{\\tau_{m}}\\right)\\right) = \\frac{I_{0}}{C}\\exp\\left(-\\frac{t}{\\tau_{s}}\\right)\\exp\\left(\\frac{t}{\\tau_{m}}\\right) = \\frac{I_{0}}{C}\\exp\\left(t\\left(\\frac{1}{\\tau_{m}}-\\frac{1}{\\tau_{s}}\\right)\\right) $$\nIntegrating from $0$ to an arbitrary time $t>0$:\n$$ v(t)\\exp\\left(\\frac{t}{\\tau_{m}}\\right) - v(0)\\exp(0) = \\int_{0}^{t} \\frac{I_{0}}{C}\\exp\\left(u\\left(\\frac{1}{\\tau_{m}}-\\frac{1}{\\tau_{s}}\\right)\\right) du $$\nWith $v(0)=0$ and since $\\tau_{m} \\neq \\tau_{s}$, we can evaluate the integral:\n$$ v(t)\\exp\\left(\\frac{t}{\\tau_{m}}\\right) = \\frac{I_{0}}{C} \\left[ \\frac{\\exp\\left(u\\left(\\frac{1}{\\tau_{m}}-\\frac{1}{\\tau_{s}}\\right)\\right)}{\\frac{1}{\\tau_{m}}-\\frac{1}{\\tau_{s}}} \\right]_{0}^{t} = \\frac{I_{0}}{C} \\frac{\\tau_{m}\\tau_{s}}{\\tau_{s}-\\tau_{m}} \\left( \\exp\\left(t\\left(\\frac{1}{\\tau_{m}}-\\frac{1}{\\tau_{s}}\\right)\\right) - 1 \\right) $$\nMultiplying by $\\exp(-t/\\tau_{m})$ to isolate $v(t)$:\n$$ v(t) = \\frac{I_{0}\\tau_{m}\\tau_{s}}{C(\\tau_{s}-\\tau_{m})} \\left( \\exp\\left(-\\frac{t}{\\tau_{s}}\\right) - \\exp\\left(-\\frac{t}{\\tau_{m}}\\right) \\right) $$\nThis expression for $v(t)=V(t)-E_L$ holds for $t>0$. For $t>0$, since $I_0, C, \\tau_m, \\tau_s$ are all positive, the sign of $v(t)$ is determined by the term $\\frac{\\exp(-t/\\tau_{s}) - \\exp(-t/\\tau_{m})}{\\tau_s - \\tau_m}$. This term is always positive for $t>0$ (it is the divided difference of the exponential function), so $V(t) > E_L$ for all $t>0$.\n\nNow we can compute the potentiation term $\\mathcal{P}$:\n$$ \\mathcal{P}=\\int_{-\\infty}^{\\infty} x_{\\mathrm{pre}}(t)\\,\\big(V(t)-\\theta_{+}\\big)_{+}\\,dt $$\nWith the given condition $\\theta_{+} = E_{L}$, this becomes:\n$$ \\mathcal{P} = \\int_{-\\infty}^{\\infty} x_{\\mathrm{pre}}(t)\\,\\big(V(t)-E_{L}\\big)_{+}\\,dt = \\int_{-\\infty}^{\\infty} x_{\\mathrm{pre}}(t)\\,\\big(v(t)\\big)_{+}\\,dt $$\nSince $x_{\\mathrm{pre}}(t)=0$ for $t \\le 0$ and we have shown $v(t)>0$ for $t>0$, we have $(v(t))_{+} = v(t)$ for $t>0$. The integral simplifies to:\n$$ \\mathcal{P} = \\int_{0}^{\\infty} x_{\\mathrm{pre}}(t)\\,v(t)\\,dt $$\nSubstituting the derived expressions for $x_{\\mathrm{pre}}(t)$ and $v(t)$:\n$$ \\mathcal{P} = \\int_{0}^{\\infty} \\exp\\left(-\\frac{t}{\\tau_{x}}\\right) \\left[ \\frac{I_{0}\\tau_{m}\\tau_{s}}{C(\\tau_{s}-\\tau_{m})} \\left( \\exp\\left(-\\frac{t}{\\tau_{s}}\\right) - \\exp\\left(-\\frac{t}{\\tau_{m}}\\right) \\right) \\right] dt $$\nWe can pull the constant factors out of the integral:\n$$ \\mathcal{P} = \\frac{I_{0}\\tau_{m}\\tau_{s}}{C(\\tau_{s}-\\tau_{m})} \\int_{0}^{\\infty} \\left( \\exp\\left(-t\\left(\\frac{1}{\\tau_{x}}+\\frac{1}{\\tau_{s}}\\right)\\right) - \\exp\\left(-t\\left(\\frac{1}{\\tau_{x}}+\\frac{1}{\\tau_{m}}\\right)\\right) \\right) dt $$\nThe integral of an exponential $\\int_{0}^{\\infty} \\exp(-at) dt = 1/a$ for $a>0$. Both decay rates are positive since all time constants are positive.\n$$ \\int_{0}^{\\infty} (\\dots) dt = \\left[ \\frac{1}{\\frac{1}{\\tau_{x}}+\\frac{1}{\\tau_{s}}} - \\frac{1}{\\frac{1}{\\tau_{x}}+\\frac{1}{\\tau_{m}}} \\right] $$\n$$ = \\left[ \\frac{\\tau_{x}\\tau_{s}}{\\tau_{x}+\\tau_{s}} - \\frac{\\tau_{x}\\tau_{m}}{\\tau_{x}+\\tau_{m}} \\right] $$\nWe find a common denominator for the term in the brackets:\n$$ = \\frac{\\tau_{x}\\tau_{s}(\\tau_{x}+\\tau_{m}) - \\tau_{x}\\tau_{m}(\\tau_{x}+\\tau_{s})}{(\\tau_{x}+\\tau_{s})(\\tau_{x}+\\tau_{m})} $$\n$$ = \\frac{\\tau_{x}^{2}\\tau_{s} + \\tau_{x}\\tau_{s}\\tau_{m} - \\tau_{x}^{2}\\tau_{m} - \\tau_{x}\\tau_{m}\\tau_{s}}{(\\tau_{x}+\\tau_{s})(\\tau_{x}+\\tau_{m})} = \\frac{\\tau_{x}^{2}(\\tau_{s}-\\tau_{m})}{(\\tau_{x}+\\tau_{s})(\\tau_{x}+\\tau_{m})} $$\nFinally, we multiply this result by the prefactor:\n$$ \\mathcal{P} = \\frac{I_{0}\\tau_{m}\\tau_{s}}{C(\\tau_{s}-\\tau_{m})} \\cdot \\frac{\\tau_{x}^{2}(\\tau_{s}-\\tau_{m})}{(\\tau_{x}+\\tau_{s})(\\tau_{x}+\\tau_{m})} $$\nThe term $(\\tau_{s}-\\tau_{m})$ cancels out, yielding the final expression for $\\mathcal{P}$:\n$$ \\mathcal{P} = \\frac{I_{0}\\tau_{m}\\tau_{s}\\tau_{x}^{2}}{C(\\tau_{x}+\\tau_{m})(\\tau_{x}+\\tau_{s})} $$\nThis is the closed-form analytic expression for the potentiation term.",
            "answer": "$$ \\boxed{\\frac{I_{0}\\tau_{m}\\tau_{s}\\tau_{x}^{2}}{C(\\tau_{m}+\\tau_{x})(\\tau_{s}+\\tau_{x})}} $$"
        },
        {
            "introduction": "Moving from the impact of single events to long-term synaptic evolution requires a shift to a statistical framework. This practice explores how synaptic weights behave over time when subjected to both Hebbian-like potentiation and homeostatic stabilization, which can be an effective outcome of inhibitory STDP. You will use the powerful tools of stochastic calculus to show how these competing forces, combined with inherent biological noise, give rise to a stable, log-normal distribution of synaptic weights, a hallmark feature observed in many cortical areas .",
            "id": "4032622",
            "problem": "Consider a single excitatory synapse with weight $w(t) \\in \\mathbb{R}_{+}$ projecting onto a neuron whose plasticity follows a voltage-based Spike-Timing Dependent Plasticity (STDP) rule, and suppose that inhibitory STDP provides homeostatic stabilization of the postsynaptic membrane voltage. Assume synaptic updates occur on a slow timescale relative to membrane fluctuations and spike statistics, so that a mesoscopic coarse-graining yields an effective multiplicative learning-rate modulation $\\eta(t)$ that can be decomposed as $\\eta(t) = \\eta_{0} + \\delta \\eta(t)$, where $\\delta \\eta(t)$ is a zero-mean fluctuation. Let the inhibitory STDP induce a soft normalization that opposes runaway growth in the logarithm of the weight. Under these conditions, model the coarse-grained weight dynamics in the Itô interpretation by the stochastic differential equation\n$$\n\\mathrm{d} w = w \\bigl[\\alpha - \\beta \\ln w \\bigr] \\,\\mathrm{d} t + w \\sigma \\,\\mathrm{d} W_{t},\n$$\nwhere $W_{t}$ is a standard Wiener process, $\\alpha \\in \\mathbb{R}$ captures the net mean potentiation drive generated by voltage-based STDP, $\\beta > 0$ encodes the inhibitory STDP-induced homeostatic stabilization in logarithmic weight, and $\\sigma > 0$ quantifies the multiplicative fluctuations originating from $\\delta \\eta(t)$.\n\nStarting only from this effective description and fundamental tools of stochastic calculus, derive the stationary distribution of $w(t)$ and show that it is log-normal. Then, derive closed-form expressions for the stationary mean and the stationary variance of $w$ in terms of $\\alpha$, $\\beta$, and $\\sigma$. Express your final answer as a row matrix containing the stationary mean followed by the stationary variance. No numerical evaluation is required. Provide the exact analytical expressions, and do not include units. If any conditions on parameters are required for stationarity, state them in your derivation but do not include them in the final answer.",
            "solution": "The problem provides a stochastic differential equation (SDE) for the dynamics of a synaptic weight $w(t)$ and asks for the derivation of its stationary distribution, mean, and variance. The analysis begins by validating the problem statement. The SDE is a form of geometric Brownian motion with logarithmic mean-reversion, a standard and well-posed model in computational neuroscience and quantitative finance. All parameters ($\\alpha$, $\\beta$, $\\sigma$) are defined, and their constraints ($\\beta > 0$, $\\sigma > 0$) are physically meaningful. The problem is scientifically grounded, formally specified, and objective. Therefore, the problem is valid and we may proceed with the solution.\n\nThe given SDE in the Itô interpretation is:\n$$\n\\mathrm{d} w = w \\bigl[\\alpha - \\beta \\ln w \\bigr] \\,\\mathrm{d} t + w \\sigma \\,\\mathrm{d} W_{t}\n$$\nThis equation is non-linear and has multiplicative noise. A standard technique to solve such equations is to find a transformation of the variable $w$ that leads to a simpler SDE. The multiplicative nature of both the drift and diffusion terms suggests a logarithmic transformation. Let us define a new variable $x(t)$ as:\n$$\nx(t) = \\ln w(t)\n$$\nTo find the SDE for $x(t)$, we apply Itô's lemma. For a function $f(w)$, Itô's lemma states:\n$$\n\\mathrm{d}f(w) = \\frac{\\partial f}{\\partial w} \\mathrm{d}w + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial w^2} (\\mathrm{d}w)^2\n$$\nIn our case, $f(w) = \\ln w$, so the derivatives are:\n$$\n\\frac{\\partial f}{\\partial w} = \\frac{1}{w} \\quad \\text{and} \\quad \\frac{\\partial^2 f}{\\partial w^2} = -\\frac{1}{w^2}\n$$\nThe quadratic variation term $(\\mathrm{d}w)^2$ is calculated from the diffusion term of the SDE for $w(t)$. In the Itô calculus, $(\\mathrm{d}t)^2 = 0$, $\\mathrm{d}t \\mathrm{d}W_t = 0$, and $(\\mathrm{d}W_t)^2 = \\mathrm{d}t$.\n$$\n(\\mathrm{d}w)^2 = \\left( w \\bigl[\\alpha - \\beta \\ln w \\bigr] \\,\\mathrm{d} t + w \\sigma \\,\\mathrm{d} W_{t} \\right)^2 = (w \\sigma)^2 (\\mathrm{d}W_{t})^2 + \\dots = w^2 \\sigma^2 \\mathrm{d}t\n$$\nSubstituting these into Itô's lemma for $x(t) = f(w(t))$:\n$$\n\\mathrm{d}x = \\frac{1}{w} \\mathrm{d}w - \\frac{1}{2w^2} (w^2 \\sigma^2 \\mathrm{d}t)\n$$\nNow, substitute the expression for $\\mathrm{d}w$:\n$$\n\\mathrm{d}x = \\frac{1}{w} \\left( w \\bigl[\\alpha - \\beta \\ln w \\bigr] \\,\\mathrm{d} t + w \\sigma \\,\\mathrm{d} W_{t} \\right) - \\frac{1}{2}\\sigma^2 \\mathrm{d}t\n$$\nSimplifying the expression yields:\n$$\n\\mathrm{d}x = \\bigl(\\alpha - \\beta \\ln w \\bigr) \\mathrm{d}t + \\sigma \\mathrm{d}W_t - \\frac{1}{2}\\sigma^2 \\mathrm{d}t\n$$\nReplacing $\\ln w$ with $x$, we obtain the SDE for $x(t)$:\n$$\n\\mathrm{d}x = \\left(\\alpha - \\frac{\\sigma^2}{2} - \\beta x \\right) \\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\nThis is the equation for an Ornstein-Uhlenbeck (OU) process. The general form of an OU process is $\\mathrm{d}x = \\theta(\\mu - x)\\mathrm{d}t + \\sigma_{\\text{OU}}\\mathrm{d}W_t$. By rearranging our equation for $x(t)$, we can identify the parameters:\n$$\n\\mathrm{d}x = \\beta \\left( \\frac{\\alpha - \\sigma^2/2}{\\beta} - x \\right) \\mathrm{d}t + \\sigma \\mathrm{d}W_t\n$$\nThe parameters are the mean-reversion rate $\\theta = \\beta$, the long-term mean $\\mu_x = \\frac{\\alpha - \\sigma^2/2}{\\beta}$, and the volatility $\\sigma_{\\text{OU}} = \\sigma$.\n\nFor an OU process to have a stationary distribution, the mean-reversion rate must be positive. In this problem, it is given that $\\beta > 0$, so a stationary state exists. The stationary distribution of an OU process is a normal (Gaussian) distribution. The mean of this distribution is the long-term mean $\\mu_x$, and its variance, which we denote by $\\sigma_x^2$, is given by $\\sigma_x^2 = \\frac{\\sigma_{\\text{OU}}^2}{2\\theta}$.\nSubstituting our parameters:\n- Stationary mean of $x$: $\\quad E[x]_{\\text{stat}} = \\mu_x = \\frac{\\alpha - \\frac{\\sigma^2}{2}}{\\beta}$\n- Stationary variance of $x$: $\\quad \\text{Var}(x)_{\\text{stat}} = \\sigma_x^2 = \\frac{\\sigma^2}{2\\beta}$\n\nSo, in the stationary state, $x(t)$ is a random variable drawn from a normal distribution, $x \\sim N(\\mu_x, \\sigma_x^2)$.\nSince $w = \\exp(x)$, and $x$ is normally distributed, the synaptic weight $w$ follows a log-normal distribution by definition. This proves the first part of the problem.\n\nNext, we derive the stationary mean and variance of $w$. If $w = \\exp(x)$ where $x \\sim N(\\mu_x, \\sigma_x^2)$, its moments can be found using the moment-generating function of the normal distribution, $M_x(k) = E[\\exp(kx)] = \\exp(k\\mu_x + \\frac{1}{2}k^2\\sigma_x^2)$.\n\nThe stationary mean of $w$, $E[w]_{\\text{stat}}$, is the first moment ($k=1$):\n$$\nE[w]_{\\text{stat}} = E[\\exp(x)] = \\exp\\left(\\mu_x + \\frac{1}{2}\\sigma_x^2\\right)\n$$\nSubstituting the expressions for $\\mu_x$ and $\\sigma_x^2$:\n$$\nE[w]_{\\text{stat}} = \\exp\\left( \\frac{\\alpha - \\sigma^2/2}{\\beta} + \\frac{1}{2}\\frac{\\sigma^2}{2\\beta} \\right) = \\exp\\left( \\frac{\\alpha}{\\beta} - \\frac{\\sigma^2}{2\\beta} + \\frac{\\sigma^2}{4\\beta} \\right)\n$$\n$$\nE[w]_{\\text{stat}} = \\exp\\left( \\frac{\\alpha}{\\beta} - \\frac{\\sigma^2}{4\\beta} \\right) = \\exp\\left( \\frac{4\\alpha - \\sigma^2}{4\\beta} \\right)\n$$\n\nThe stationary variance of $w$ is given by $\\text{Var}(w)_{\\text{stat}} = E[w^2]_{\\text{stat}} - (E[w]_{\\text{stat}})^2$. We first need the second moment of $w$, $E[w^2]_{\\text{stat}}$, which corresponds to $k=2$:\n$$\nE[w^2]_{\\text{stat}} = E[\\exp(2x)] = \\exp\\left(2\\mu_x + \\frac{1}{2}(2^2)\\sigma_x^2\\right) = \\exp\\left(2\\mu_x + 2\\sigma_x^2\\right)\n$$\nSubstituting the expressions for $\\mu_x$ and $\\sigma_x^2$:\n$$\nE[w^2]_{\\text{stat}} = \\exp\\left( 2\\left(\\frac{\\alpha - \\sigma^2/2}{\\beta}\\right) + 2\\left(\\frac{\\sigma^2}{2\\beta}\\right) \\right) = \\exp\\left( \\frac{2\\alpha - \\sigma^2}{\\beta} + \\frac{\\sigma^2}{\\beta} \\right)\n$$\n$$\nE[w^2]_{\\text{stat}} = \\exp\\left(\\frac{2\\alpha}{\\beta}\\right)\n$$\nNow, we can compute the variance:\n$$\n\\text{Var}(w)_{\\text{stat}} = E[w^2]_{\\text{stat}} - (E[w]_{\\text{stat}})^2 = \\exp\\left(\\frac{2\\alpha}{\\beta}\\right) - \\left[ \\exp\\left( \\frac{4\\alpha - \\sigma^2}{4\\beta} \\right) \\right]^2\n$$\n$$\n\\text{Var}(w)_{\\text{stat}} = \\exp\\left(\\frac{2\\alpha}{\\beta}\\right) - \\exp\\left( 2 \\cdot \\frac{4\\alpha - \\sigma^2}{4\\beta} \\right) = \\exp\\left(\\frac{2\\alpha}{\\beta}\\right) - \\exp\\left( \\frac{4\\alpha - \\sigma^2}{2\\beta} \\right)\n$$\n$$\n\\text{Var}(w)_{\\text{stat}} = \\exp\\left(\\frac{2\\alpha}{\\beta}\\right) - \\exp\\left( \\frac{2\\alpha}{\\beta} - \\frac{\\sigma^2}{2\\beta} \\right)\n$$\nBoth forms for the variance are equivalent. We will provide the first derived form in the final answer.\n\nThe closed-form expressions for the stationary mean and variance are:\n- Stationary Mean: $\\exp\\left( \\frac{4\\alpha - \\sigma^2}{4\\beta} \\right)$\n- Stationary Variance: $\\exp\\left(\\frac{2\\alpha}{\\beta}\\right) - \\exp\\left( \\frac{2\\alpha}{\\beta} - \\frac{\\sigma^2}{2\\beta} \\right)$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\exp\\left(\\frac{4\\alpha - \\sigma^2}{4\\beta}\\right) & \\exp\\left(\\frac{2\\alpha}{\\beta}\\right) - \\exp\\left(\\frac{4\\alpha - \\sigma^2}{2\\beta}\\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A central task in computational neuroscience is to connect theoretical models with experimental data, but this process can reveal subtle ambiguities in a model's formulation. This exercise delves into the critical issue of parameter identifiability, a common challenge when fitting complex models to limited data. By analyzing a hybrid STDP model using the Fisher Information Matrix, you will discover how certain parameters can become entangled, making them impossible to determine independently from standard experiments, a crucial lesson for any aspiring modeler .",
            "id": "4032628",
            "problem": "Consider a minimal hybrid plasticity model that combines timing dependence, rate dependence, and voltage dependence, and is modulated by inhibition, intended to capture experimentally observed features of Spike-Timing-Dependent Plasticity (STDP) for both excitatory and inhibitory mechanisms. Spike-Timing-Dependent Plasticity (STDP) refers to synaptic weight changes that depend on the precise timing between presynaptic and postsynaptic spikes. Let a single synapse with weight $w$ obey an instantaneous learning rule of the form\n\n$$\n\\dot{w}(t) \\;=\\; \\eta\\, g\\!\\left(V(t)\\right)\\, h\\!\\left(I_{\\mathrm{inh}}(t)\\right)\\,\\Big[A_{+}\\,x_{\\mathrm{pre}}(t)\\,y_{\\mathrm{post}}^{+}(t)\\;-\\;A_{-}\\,x_{\\mathrm{post}}(t)\\,y_{\\mathrm{pre}}^{-}(t)\\Big],\n$$\n\nwhere $x_{\\mathrm{pre}}(t)$ and $x_{\\mathrm{post}}(t)$ are causal spike traces generated by presynaptic and postsynaptic spikes, respectively, $y_{\\mathrm{post}}^{+}(t)$ and $y_{\\mathrm{pre}}^{-}(t)$ are causal windows that pair the appropriate pre/post orderings, $V(t)$ is the postsynaptic membrane potential, $I_{\\mathrm{inh}}(t)$ is a measure of inhibitory drive, $\\eta>0$ is a small learning rate, and $A_{+}>0$, $A_{-}>0$ are unknown amplitudes. Assume the spike traces and windows are generated by single-exponential kernels with time constants $\\tau_{x}>0$ and $\\tau_{y}>0$, respectively, so that for a spike at time $t_{s}$, the corresponding trace is proportional to $\\exp\\!\\left(-(t-t_{s})/\\tau\\right)$ for $t\\ge t_{s}$ and zero otherwise. Assume that during standard paired-pulse experiments, the voltage is clamped sufficiently strongly that $V(t)$ is approximately constant at a baseline $\\bar{V}$, and inhibitory drive is stationary with mean $\\bar{I}_{\\mathrm{inh}}$, so that $g(V(t))\\approx g(\\bar{V})$ and $h(I_{\\mathrm{inh}}(t))\\approx h(\\bar{I}_{\\mathrm{inh}})$. Finally, assume low-rate protocols in which presynaptic and postsynaptic spikes occur as independent Poisson processes of rates $r_{\\mathrm{pre}}$ and $r_{\\mathrm{post}}$, respectively, and that trials are repeated over a duration $T$ with non-overlapping pairs, so that the expected total change equals the sum over isolated pre–post interactions.\n\nUnder these assumptions, show that the expected weight change induced by a single pre–post spike pair with postsynaptic spike time offset $\\Delta t$ relative to the presynaptic spike can be written as\n\n$$\n\\Delta w(\\Delta t)\\;=\\;\\kappa\\Big[A_{+}\\,\\mathrm{e}^{-\\Delta t/\\tau_{+}}\\,H(\\Delta t)\\;-\\;A_{-}\\,\\mathrm{e}^{\\Delta t/\\tau_{-}}\\,H(-\\Delta t)\\Big],\n$$\n\nwhere $H(\\cdot)$ is the Heaviside step function, $\\tau_{+}>0$ and $\\tau_{-}>0$ are effective timing constants determined by $\\tau_{x}$ and $\\tau_{y}$, and\n\n$$\n\\kappa \\;=\\; \\eta\\, r_{\\mathrm{pre}}\\, r_{\\mathrm{post}}\\, T\\, g(\\bar{V})\\, h(\\bar{I}_{\\mathrm{inh}}).\n$$\n\nYou may assume linear superposition of isolated spike-pair contributions in the low-rate limit and stationarity of $g(\\bar{V})$ and $h(\\bar{I}_{\\mathrm{inh}})$ across the protocol.\n\nSuppose now that an experimenter measures the mean plasticity outcome at a set of $K$ time offsets $\\{\\Delta t_{i}\\}_{i=1}^{K}$ comprising both $K_{+}\\ge 1$ positive and $K_{-}\\ge 1$ negative lags ($K_{+}+K_{-}=K$), and that each measurement obeys a homoscedastic Gaussian observation model with variance $\\sigma^{2}>0$:\n\n$$\ny_{i}\\;=\\;\\kappa\\Big[A_{+}\\,\\mathrm{e}^{-\\Delta t_{i}/\\tau_{+}}\\,H(\\Delta t_{i})\\;-\\;A_{-}\\,\\mathrm{e}^{\\Delta t_{i}/\\tau_{-}}\\,H(-\\Delta t_{i})\\Big]\\;+\\;\\varepsilon_{i},\\qquad \\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2}),\n$$\n\nwith $\\tau_{+}$ and $\\tau_{-}$ known a priori from independent calibration. Consider the parameter vector $\\boldsymbol{\\theta}=(A_{+},A_{-},\\kappa)^{\\top}$. Starting from the definition of the Fisher information for a Gaussian model with known variance, derive the Fisher information matrix $F(\\boldsymbol{\\theta})$ up to a common factor, and determine its rank for such a protocol that includes both positive and negative time lags. Provide the rank of $F(\\boldsymbol{\\theta})$ as your final answer. The final answer must be a single integer with no units.",
            "solution": "The problem asks for the rank of the Fisher Information Matrix (FIM) for a set of parameters $\\boldsymbol{\\theta}=(A_{+},A_{-},\\kappa)^{\\top}$ in a statistical model of synaptic plasticity.\n\nFirst, we establish the statistical model and the quantities required to compute the FIM. The model describes measurements $y_i$ at different spike-time offsets $\\Delta t_i$ as:\n$$\ny_{i} \\;=\\; \\mu(\\Delta t_i; \\boldsymbol{\\theta}) \\;+\\; \\varepsilon_{i},\\qquad \\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2})\n$$\nThe mean of the observation $y_i$, denoted $\\mu_i$, is given by:\n$$\n\\mu_i \\equiv \\mu(\\Delta t_i; \\boldsymbol{\\theta}) \\;=\\; \\kappa\\Big[A_{+}\\,\\mathrm{e}^{-\\Delta t_{i}/\\tau_{+}}\\,H(\\Delta t_{i})\\;-\\;A_{-}\\,\\mathrm{e}^{\\Delta t_{i}/\\tau_{-}}\\,H(-\\Delta t_{i})\\Big]\n$$\nThe parameters of interest are $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\theta_3)^{\\top} = (A_{+}, A_{-}, \\kappa)^{\\top}$. The time constants $\\tau_{+}, \\tau_{-}$ and the noise variance $\\sigma^2$ are assumed to be known. The experiment includes $K_+\\ge 1$ measurements with positive lags ($\\Delta t_i > 0$) and $K_-\\ge 1$ measurements with negative lags ($\\Delta t_i < 0$).\n\nFor a model with additive Gaussian noise with known variance $\\sigma^2$, the elements of the Fisher Information Matrix $F(\\boldsymbol{\\theta})$ are given by:\n$$\nF_{jk}(\\boldsymbol{\\theta}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{K} \\frac{\\partial \\mu_i}{\\partial \\theta_j} \\frac{\\partial \\mu_i}{\\partial \\theta_k}\n$$\nwhere $K$ is the total number of measurements. We need to compute the partial derivatives of $\\mu_i$ with respect to each parameter in $\\boldsymbol{\\theta}$. Let us define two basis functions based on the experimental lags $\\Delta t_i$:\n$$\nf_{i,+} = \\mathrm{e}^{-\\Delta t_{i}/\\tau_{+}}\\,H(\\Delta t_{i})\n$$\n$$\nf_{i,-} = \\mathrm{e}^{\\Delta t_{i}/\\tau_{-}}\\,H(-\\Delta t_{i})\n$$\nWith these definitions, the mean $\\mu_i$ can be written more compactly as:\n$$\n\\mu_i = \\kappa \\left( A_{+} f_{i,+} - A_{-} f_{i,-} \\right)\n$$\nNote that for any given $\\Delta t_i \\neq 0$, one of $f_{i,+}$ or $f_{i,-}$ is zero. Specifically, if $\\Delta t_i > 0$, $f_{i,-}=0$, and if $\\Delta t_i < 0$, $f_{i,+}=0$.\n\nThe partial derivatives of $\\mu_i$ are:\n1. With respect to $\\theta_1 = A_{+}$:\n$$\n\\frac{\\partial \\mu_i}{\\partial A_{+}} = \\kappa f_{i,+}\n$$\n2. With respect to $\\theta_2 = A_{-}$:\n$$\n\\frac{\\partial \\mu_i}{\\partial A_{-}} = -\\kappa f_{i,-}\n$$\n3. With respect to $\\theta_3 = \\kappa$:\n$$\n\\frac{\\partial \\mu_i}{\\partial \\kappa} = A_{+} f_{i,+} - A_{-} f_{i,-}\n$$\n\nNow we can construct the $3 \\times 3$ FIM. Let us compute its elements, ignoring the constant factor $1/\\sigma^2$ as it does not affect the rank.\n$$\nF_{11} = \\sum_{i=1}^{K} \\left(\\frac{\\partial \\mu_i}{\\partial A_{+}}\\right)^2 = \\sum_{i=1}^{K} (\\kappa f_{i,+})^2 = \\kappa^2 \\sum_{i: \\Delta t_i > 0} (\\mathrm{e}^{-\\Delta t_i/\\tau_+})^2 = \\kappa^2 S_{+}\n$$\nwhere $S_{+} = \\sum_{i: \\Delta t_i > 0} \\mathrm{e}^{-2\\Delta t_i/\\tau_+}$. Since $K_+ \\ge 1$, we have $S_{+} > 0$.\n$$\nF_{22} = \\sum_{i=1}^{K} \\left(\\frac{\\partial \\mu_i}{\\partial A_{-}}\\right)^2 = \\sum_{i=1}^{K} (-\\kappa f_{i,-})^2 = \\kappa^2 \\sum_{i: \\Delta t_i < 0} (\\mathrm{e}^{\\Delta t_i/\\tau_-})^2 = \\kappa^2 S_{-}\n$$\nwhere $S_{-} = \\sum_{i: \\Delta t_i < 0} \\mathrm{e}^{2\\Delta t_i/\\tau_-}$. Since $K_- \\ge 1$, we have $S_{-} > 0$.\n$$\nF_{33} = \\sum_{i=1}^{K} \\left(\\frac{\\partial \\mu_i}{\\partial \\kappa}\\right)^2 = \\sum_{i=1}^{K} (A_{+} f_{i,+} - A_{-} f_{i,-})^2 = \\sum_{i: \\Delta t_i > 0} (A_{+} f_{i,+})^2 + \\sum_{i: \\Delta t_i < 0} (-A_{-} f_{i,-})^2 = A_{+}^2 S_{+} + A_{-}^2 S_{-}\n$$\nFor the off-diagonal elements, we use the fact that $f_{i,+} f_{i,-} = 0$ for all $i$.\n$$\nF_{12} = F_{21} = \\sum_{i=1}^{K} \\frac{\\partial \\mu_i}{\\partial A_{+}} \\frac{\\partial \\mu_i}{\\partial A_{-}} = \\sum_{i=1}^{K} (\\kappa f_{i,+})(-\\kappa f_{i,-}) = -\\kappa^2 \\sum_{i=1}^{K} f_{i,+} f_{i,-} = 0\n$$\n$$\nF_{13} = F_{31} = \\sum_{i=1}^{K} \\frac{\\partial \\mu_i}{\\partial A_{+}} \\frac{\\partial \\mu_i}{\\partial \\kappa} = \\sum_{i=1}^{K} (\\kappa f_{i,+})(A_{+} f_{i,+} - A_{-} f_{i,-}) = \\kappa A_{+} \\sum_{i=1}^{K} f_{i,+}^2 = \\kappa A_{+} S_{+}\n$$\n$$\nF_{23} = F_{32} = \\sum_{i=1}^{K} \\frac{\\partial \\mu_i}{\\partial A_{-}} \\frac{\\partial \\mu_i}{\\partial \\kappa} = \\sum_{i=1}^{K} (-\\kappa f_{i,-})(A_{+} f_{i,+} - A_{-} f_{i,-}) = \\kappa A_{-} \\sum_{i=1}^{K} f_{i,-}^2 = \\kappa A_{-} S_{-}\n$$\nAssembling the matrix (up to the $1/\\sigma^2$ factor):\n$$\nF(\\boldsymbol{\\theta}) \\propto \\begin{pmatrix}\n\\kappa^2 S_{+} & 0 & \\kappa A_{+} S_{+} \\\\\n0 & \\kappa^2 S_{-} & \\kappa A_{-} S_{-} \\\\\n\\kappa A_{+} S_{+} & \\kappa A_{-} S_{-} & A_{+}^2 S_{+} + A_{-}^2 S_{-}\n\\end{pmatrix}\n$$\nTo determine the rank of $F$, we check for linear dependencies among its columns (or rows). Let the columns be $\\mathbf{c}_1, \\mathbf{c}_2, \\mathbf{c}_3$.\n$$\n\\mathbf{c}_1 = \\begin{pmatrix} \\kappa^2 S_{+} \\\\ 0 \\\\ \\kappa A_{+} S_{+} \\end{pmatrix}, \\quad\n\\mathbf{c}_2 = \\begin{pmatrix} 0 \\\\ \\kappa^2 S_{-} \\\\ \\kappa A_{-} S_{-} \\end{pmatrix}, \\quad\n\\mathbf{c}_3 = \\begin{pmatrix} \\kappa A_{+} S_{+} \\\\ \\kappa A_{-} S_{-} \\\\ A_{+}^2 S_{+} + A_{-}^2 S_{-} \\end{pmatrix}\n$$\nLet us test if $\\mathbf{c}_3$ is a linear combination of $\\mathbf{c}_1$ and $\\mathbf{c}_2$. We are looking for constants $\\alpha$ and $\\beta$ such that $\\mathbf{c}_3 = \\alpha \\mathbf{c}_1 + \\beta \\mathbf{c}_2$.\nComparing the first components: $\\kappa A_{+} S_{+} = \\alpha (\\kappa^2 S_{+})$. Since $\\kappa > 0$ and $S_+ > 0$, this implies $\\alpha = A_{+}/\\kappa$.\nComparing the second components: $\\kappa A_{-} S_{-} = \\beta (\\kappa^2 S_{-})$. Since $\\kappa > 0$ and $S_- > 0$, this implies $\\beta = A_{-}/\\kappa$.\nNow we check if this holds for the third component:\n$$\n\\alpha (\\kappa A_{+} S_{+}) + \\beta (\\kappa A_{-} S_{-}) = \\left(\\frac{A_{+}}{\\kappa}\\right) (\\kappa A_{+} S_{+}) + \\left(\\frac{A_{-}}{\\kappa}\\right) (\\kappa A_{-} S_{-}) = A_{+}^2 S_{+} + A_{-}^2 S_{-}\n$$\nThis is precisely the third component of $\\mathbf{c}_3$. Thus, we have found a linear dependence:\n$$\n\\mathbf{c}_3 = \\frac{A_{+}}{\\kappa} \\mathbf{c}_1 + \\frac{A_{-}}{\\kappa} \\mathbf{c}_2\n$$\nSince one column is a linear combination of the other two, the rank of the matrix is less than $3$.\nNext, we check if $\\mathbf{c}_1$ and $\\mathbf{c}_2$ are linearly independent. We seek a non-trivial solution to $\\alpha \\mathbf{c}_1 + \\beta \\mathbf{c}_2 = \\mathbf{0}$:\n$$\n\\alpha \\begin{pmatrix} \\kappa^2 S_{+} \\\\ 0 \\\\ \\kappa A_{+} S_{+} \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ \\kappa^2 S_{-} \\\\ \\kappa A_{-} S_{-} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe first row gives $\\alpha \\kappa^2 S_{+} = 0$, which implies $\\alpha=0$ as $\\kappa \\ne 0$ and $S_+ \\ne 0$. The second row gives $\\beta \\kappa^2 S_{-} = 0$, which implies $\\beta=0$. Since the only solution is the trivial one ($\\alpha=0, \\beta=0$), the columns $\\mathbf{c}_1$ and $\\mathbf{c}_2$ are linearly independent.\nThe matrix $F$ has two linearly independent columns, and the third is a linear combination of these two. Therefore, the dimension of the column space is $2$.\nThe rank of the Fisher Information Matrix $F(\\boldsymbol{\\theta})$ is $2$.\n\nThis result reflects a structural non-identifiability in the model. The parameters $A_{+}$, $A_{-}$, and $\\kappa$ only appear in the model through the products $B_{+} = \\kappa A_{+}$ and $B_{-} = \\kappa A_{-}$. From the data, one can estimate the two parameters $B_{+}$ and $B_{-}$, but it is impossible to uniquely determine the three original parameters. For any scalar $\\alpha \\neq 0$, the parameter set $(A_{+}/\\alpha, A_{-}/\\alpha, \\kappa\\alpha)$ would yield the same model predictions. This single degree of unconstrained freedom reduces the rank of the $3 \\times 3$ FIM from $3$ to $2$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}