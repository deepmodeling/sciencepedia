{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes you back to the fundamentals of Optimal Feedback Control. By solving a simple, finite-horizon Linear Quadratic Regulator (LQR) problem from scratch using dynamic programming, you will gain a concrete understanding of how the Bellman optimality principle is applied recursively. Working through this scalar case helps build intuition for the core logic of the Riccati recursion without the immediate complexity of matrix algebra .",
            "id": "4006058",
            "problem": "Consider a simplified neural control setting where a single scalar cortical activity variable $x_t$ encodes a motor-relevant state to be regulated over a short planning horizon by a motor command $u_t$. The neural plant evolves according to the discrete-time linear dynamics $x_{t+1} = a x_t + b u_t$, where $a \\in \\mathbb{R}$ models intrinsic state propagation and $b \\in \\mathbb{R}$ models control effectiveness. The objective over a two-step horizon $t \\in \\{0,1\\}$ is to select controls $u_0$ and $u_1$ to minimize the cumulative quadratic cost \n$$J = q x_0^{2} + r u_0^{2} + q x_1^{2} + r u_1^{2} + q x_2^{2},$$\nwhere $q \\ge 0$ penalizes state deviation (interpretable as task error or deviation from a neural target) and $r > 0$ penalizes control effort (interpretable as motor command energetic cost). The final penalty $q x_2^{2}$ at time $t=2$ reflects terminal deviation cost. Assume an initial state $x_0 \\in \\mathbb{R}$ is given and $b \\ne 0$. Using the Bellman optimality principle (dynamic programming) applied to this finite-horizon problem, derive the optimal initial control $u_0^{\\star}$ explicitly as a function of $a$, $b$, $q$, $r$, and $x_0$. Express your final answer as a single closed-form analytical expression. No rounding is required, and no physical units are to be attached to the answer.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- **System Dynamics:** The state $x_t \\in \\mathbb{R}$ evolves according to the discrete-time linear equation $x_{t+1} = a x_t + b u_t$.\n- **Control Input:** $u_t \\in \\mathbb{R}$.\n- **System Parameters:** $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$, with the constraint $b \\ne 0$.\n- **Time Horizon:** The problem is defined over a two-step horizon, for $t \\in \\{0,1\\}$, with states up to $x_2$.\n- **Cost Function:** The objective is to minimize the cumulative cost $J = q x_0^{2} + r u_0^{2} + q x_1^{2} + r u_1^{2} + q x_2^{2}$.\n- **Cost Parameters:** $q \\ge 0$ and $r > 0$.\n- **Initial Condition:** The initial state $x_0 \\in \\mathbb{R}$ is given.\n- **Objective:** Derive the optimal initial control $u_0^{\\star}$ as a function of $a$, $b$, $q$, $r$, and $x_0$.\n- **Methodology:** Use the Bellman optimality principle (dynamic programming).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a classic finite-horizon, discrete-time Linear Quadratic Regulator (LQR) problem. This is a fundamental topic in optimal control theory and is a well-established, scientifically valid framework for modeling control systems, including simplified models in computational neuroscience.\n- **Well-Posed:** The problem is well-posed. The cost function is quadratic and convex since the state penalty coefficient $q \\ge 0$ and the control penalty coefficient $r > 0$. The system dynamics are linear. The condition $r > 0$ ensures a unique minimum exists for the control inputs. The condition $b \\ne 0$ ensures the system is controllable. Thus, a unique, stable, and meaningful solution for the optimal control sequence exists.\n- **Objective:** The problem is stated using precise mathematical language, free from ambiguity or subjective claims.\n- **Flaw Checklist:** The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, feasible, and well-posed. It is a standard problem that requires substantive reasoning.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of the Optimal Control\nThe problem is to find the control sequence $u_0, u_1$ that minimizes the cost function $J$. We will use dynamic programming, working backwards from the final time step $t=2$.\n\nThe cost function is $J = q x_0^{2} + r u_0^{2} + q x_1^{2} + r u_1^{2} + q x_2^{2}$. This can be structured for dynamic programming by defining a stage cost $L(x_t, u_t) = q x_t^2 + r u_t^2$ for $t \\in \\{0, 1\\}$ and a terminal cost $M(x_2) = q x_2^2$. The total cost is then $J = L(x_0, u_0) + L(x_1, u_1) + M(x_2)$.\n\nWe define the optimal cost-to-go, or value function, $V_t(x_t)$ as the minimum cost achievable from state $x_t$ at time $t$ to the end of the horizon.\n\n**Step T=2 (Final Step):**\nThe value function at the terminal time $t=2$ is simply the terminal cost:\n$$V_2(x_2) = M(x_2) = q x_2^2$$\nThis is a quadratic function of the state. We can write this as $V_2(x_2) = P_2 x_2^2$, where $P_2 = q$.\n\n**Step T=1:**\nAccording to the Bellman principle, the value function at time $t=1$ is given by:\n$$V_1(x_1) = \\min_{u_1} \\left[ L(x_1, u_1) + V_2(x_2) \\right]$$\nSubstituting the expressions for the stage cost, value function $V_2$, and the system dynamics $x_2 = a x_1 + b u_1$:\n$$V_1(x_1) = \\min_{u_1} \\left[ q x_1^2 + r u_1^2 + P_2(a x_1 + b u_1)^2 \\right]$$\nTo find the optimal control $u_1^{\\star}$ that minimizes this expression, we take the partial derivative with respect to $u_1$ and set it to zero. Since $r>0$, the quadratic function in $u_1$ is strictly convex, and this will yield the unique minimum.\n$$\\frac{\\partial}{\\partial u_1} \\left[ q x_1^2 + r u_1^2 + P_2(a x_1 + b u_1)^2 \\right] = 2 r u_1 + 2 P_2 (a x_1 + b u_1) b = 0$$\n$$r u_1 + P_2 a b x_1 + P_2 b^2 u_1 = 0$$\n$$(r + P_2 b^2) u_1 = -P_2 a b x_1$$\n$$u_1^{\\star}(x_1) = -\\frac{P_2 a b}{r + P_2 b^2} x_1$$\nThis is the optimal feedback control law at $t=1$. Now we find the value function $V_1(x_1)$ by substituting $u_1^{\\star}$ back into its definition. The value function will also be a quadratic function of the state, $V_1(x_1) = P_1 x_1^2$.\n$$V_1(x_1) = q x_1^2 + r (u_1^{\\star})^2 + P_2(a x_1 + b u_1^{\\star})^2$$\nSubstituting $u_1^{\\star} = -K_1 x_1$ where $K_1 = \\frac{P_2 a b}{r + P_2 b^2}$:\n$$V_1(x_1) = q x_1^2 + r K_1^2 x_1^2 + P_2(a - b K_1)^2 x_1^2 = \\left( q + r K_1^2 + P_2(a - b K_1)^2 \\right) x_1^2$$\nThus, $P_1 = q + r K_1^2 + P_2(a - b K_1)^2$. A more direct form of this update, known as the discrete-time Riccati equation, is $P_1 = q + a^2 P_2 - \\frac{(a b P_2)^2}{r + b^2 P_2} = q + \\frac{a^2 P_2 r}{r + b^2 P_2}$.\nSubstituting $P_2 = q$:\n$$P_1 = q + \\frac{a^2 q r}{r + q b^2}$$\n\n**Step T=0:**\nWe repeat the process for $t=0$ to find the desired optimal control $u_0^{\\star}$. The value function at $t=0$ is:\n$$V_0(x_0) = \\min_{u_0} \\left[ L(x_0, u_0) + V_1(x_1) \\right]$$\nSubstituting stage cost, $V_1(x_1) = P_1 x_1^2$, and dynamics $x_1 = a x_0 + b u_0$:\n$$V_0(x_0) = \\min_{u_0} \\left[ q x_0^2 + r u_0^2 + P_1(a x_0 + b u_0)^2 \\right]$$\nThe term $q x_0^2$ does not depend on $u_0$, so it does not affect the optimization. To find $u_0^{\\star}$, we differentiate the expression inside the minimum with respect to $u_0$ and set it to zero:\n$$\\frac{\\partial}{\\partial u_0} \\left[ q x_0^2 + r u_0^2 + P_1(a x_0 + b u_0)^2 \\right] = 2 r u_0 + 2 P_1(a x_0 + b u_0) b = 0$$\n$$r u_0 + P_1 a b x_0 + P_1 b^2 u_0 = 0$$\n$$(r + P_1 b^2) u_0 = -P_1 a b x_0$$\n$$u_0^{\\star}(x_0) = -\\frac{P_1 a b}{r + P_1 b^2} x_0$$\nThe final step is to substitute the expression for $P_1$ into this equation to get $u_0^{\\star}$ as a function of the given parameters.\nWe have $P_1 = q + \\frac{a^2 q r}{r + q b^2} = \\frac{q(r + q b^2) + a^2 q r}{r + q b^2} = \\frac{q r + q^2 b^2 + a^2 q r}{r + q b^2} = \\frac{q(r(1+a^2) + q b^2)}{r + q b^2}$.\n\nNow, let's compute the components of the expression for $u_0^{\\star}$:\nThe numerator term is $P_1 a b x_0$:\n$$P_1 a b x_0 = a b x_0 \\frac{q(r(1+a^2) + q b^2)}{r + q b^2}$$\nThe denominator term is $r + P_1 b^2$:\n$$r + P_1 b^2 = r + b^2 \\left( q + \\frac{a^2 q r}{r + q b^2} \\right) = r + q b^2 + \\frac{a^2 q r b^2}{r + q b^2}$$\n$$= \\frac{(r+q b^2)^2 + a^2 q r b^2}{r + q b^2}$$\nNow, form the fraction for $u_0^{\\star}$:\n$$u_0^{\\star} = - \\frac{P_1 a b}{r + P_1 b^2} x_0 = -abx_0 \\frac{P_1}{r+P_1b^2} = -abx_0 \\frac{\\frac{q(r(1+a^2) + q b^2)}{r + q b^2}}{\\frac{(r+q b^2)^2 + a^2 q r b^2}{r + q b^2}}$$\nSimplifying by canceling the common term $(r + q b^2)$ in the denominator of the numerator and denominator fractions:\n$$u_0^{\\star} = -abx_0 \\frac{q(r(1+a^2) + q b^2)}{(r+q b^2)^2 + a^2 q r b^2}$$\nThis can be written as a single fraction:\n$$u_0^{\\star} = - \\frac{a b q (r(1+a^2) + q b^2) x_0}{(r+q b^2)^2 + a^2 r q b^2}$$\nThis final expression gives the optimal initial control $u_0^{\\star}$ as a function of the system parameters $a, b$, cost parameters $q, r$, and the initial state $x_0$, as required.",
            "answer": "$$\\boxed{-\\frac{a b q (r(1+a^{2}) + q b^{2}) x_0}{(r+q b^{2})^{2} + a^{2} r q b^{2}}}$$"
        },
        {
            "introduction": "Beyond simply deriving a controller, it is crucial to analyze the inherent properties of the system being controlled. This practice introduces the controllability Gramian, a fundamental tool for quantifying the \"energy\" required to move the system's state in different directions. By computing the Gramian and interpreting its structure, you will connect an abstract mathematical object to the practical concept of minimal control effort, revealing why some state transitions are inherently easier to achieve than others .",
            "id": "4006062",
            "problem": "Consider a continuous-time, linear time-invariant neural population model with state $\\mathbf{x}(t) \\in \\mathbb{R}^{2}$ obeying the dynamics $\\frac{d\\mathbf{x}}{dt} = A \\mathbf{x} + B u(t)$, where $A \\in \\mathbb{R}^{2 \\times 2}$ models synaptic leak and inter-areal coupling, and $B \\in \\mathbb{R}^{2 \\times 1}$ specifies a single channel of stimulation. Let\n$$\nA = \\begin{pmatrix}\n-1 & 2 \\\\\n-3 & -2\n\\end{pmatrix}, \n\\quad\nB = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nAssume $A$ is stable (all eigenvalues have strictly negative real parts), and define the infinite-horizon controllability Gramian by\n$$\nW_c = \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)\\, dt.\n$$\nStarting from the given integral definition and fundamental properties of matrix exponentials and linear time-invariant systems, compute $W_c$ exactly. Then, using the foundations of optimal feedback control theory in continuous time, explain why the eigenvectors of $W_c$ identify directions in state space that are easier or harder to control, and how this relates to minimal control effort.\n\nFinally, let the control objective be to drive the system from the resting state $\\mathbf{x}(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ to the target state $\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ at some large enough terminal time so that the infinite-horizon controllability Gramian is applicable. Over all admissible inputs $u(t)$ achieving $\\mathbf{x}(T) = \\mathbf{x}_f$ for sufficiently large $T$, the minimal squared $L^{2}$ control effort is the quantity you must compute. Express your final answer as a single real number (dimensionless). No rounding is required.",
            "solution": "The user wants to solve a problem based on optimal feedback control theory.\n\n### Step 1: Extract Givens\n-   System dynamics: $\\frac{d\\mathbf{x}}{dt} = A \\mathbf{x} + B u(t)$, with $\\mathbf{x}(t) \\in \\mathbb{R}^{2}$.\n-   System matrix: $A = \\begin{pmatrix} -1 & 2 \\\\ -3 & -2 \\end{pmatrix}$.\n-   Input matrix: $B = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   Assumption: The matrix $A$ is stable.\n-   Definition of the infinite-horizon controllability Gramian: $W_c = \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)\\, dt$.\n-   Initial state: $\\mathbf{x}(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n-   Target state: $\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n-   Objective:\n    1.  Compute $W_c$ exactly.\n    2.  Explain why the eigenvectors of $W_c$ identify directions of easier/harder control.\n    3.  Compute the minimal squared $L^2$ control effort to drive the system from $\\mathbf{x}(0)$ to $\\mathbf{x}_f$ in the infinite-horizon limit.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is rooted in standard linear time-invariant (LTI) control theory.\n\n1.  **Scientific or Factual Unsoundness**: The model is a standard LTI state-space representation, a cornerstone of control theory and a common simplification in computational neuroscience. The concepts of the controllability Gramian and minimal control effort are fundamental and mathematically rigorous.\n2.  **Well-Posedness**: The problem requires checking the stability of $A$ and the controllability of the pair $(A, B)$.\n    -   **Stability of $A$**: The characteristic equation is $\\det(A - \\lambda I) = 0$.\n        $$ (\\!-\\!1-\\lambda)(-2-\\lambda) - (2)(-3) = \\lambda^2+3\\lambda+2+6 = \\lambda^2+3\\lambda+8 = 0 $$\n        The eigenvalues are $\\lambda = \\frac{-3 \\pm \\sqrt{3^2 - 4(1)(8)}}{2} = \\frac{-3 \\pm i\\sqrt{23}}{2}$. The real part of both eigenvalues is $-\\frac{3}{2}$, which is strictly negative. Thus, the system matrix $A$ is stable (Hurwitz), and the assumption holds. The integral for $W_c$ converges.\n    -   **Controllability**: The controllability matrix is $\\mathcal{C} = \\begin{pmatrix} B & AB \\end{pmatrix}$.\n        $$ AB = \\begin{pmatrix} -1 & 2 \\\\ -3 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -3 \\end{pmatrix} $$\n        $$ \\mathcal{C} = \\begin{pmatrix} 1 & -1 \\\\ 0 & -3 \\end{pmatrix} $$\n        The determinant is $\\det(\\mathcal{C}) = (1)(-3) - (-1)(0) = -3 \\neq 0$. The system is controllable, so any finite state $\\mathbf{x}_f$ can be reached from the origin.\n3.  **Completeness**: All necessary matrices, definitions, and boundary conditions are provided.\n\nThe problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid and can be solved.\n\nThe solution proceeds in three parts as requested.\n\n**Part 1: Computation of the Controllability Gramian $W_c$**\n\nFor a stable LTI system, the infinite-horizon controllability Gramian $W_c = \\int_{0}^{\\infty} \\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)\\, dt$ is the unique, symmetric, positive definite solution to the continuous-time Lyapunov equation:\n$$ A W_c + W_c A^{\\top} = -BB^{\\top} $$\nThis equation is a fundamental property derived by integrating the derivative of the integrand $\\exp(A t)\\, B B^{\\top} \\exp(A^{\\top} t)$ from $t=0$ to $t=\\infty$ and using the stability of $A$.\n\nLet $W_c = \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{pmatrix}$. Due to symmetry, $w_{12} = w_{21}$.\nThe matrices are:\n$$ A = \\begin{pmatrix} -1 & 2 \\\\ -3 & -2 \\end{pmatrix}, \\quad A^{\\top} = \\begin{pmatrix} -1 & -3 \\\\ 2 & -2 \\end{pmatrix} $$\n$$ -BB^{\\top} = -\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = -\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nSubstituting these into the Lyapunov equation:\n$$ \\begin{pmatrix} -1 & 2 \\\\ -3 & -2 \\end{pmatrix} \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{12} & w_{22} \\end{pmatrix} + \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{12} & w_{22} \\end{pmatrix} \\begin{pmatrix} -1 & -3 \\\\ 2 & -2 \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nMultiplying the matrices on the left-hand side yields:\n$$ \\begin{pmatrix} -w_{11} + 2w_{12} & -w_{12} + 2w_{22} \\\\ -3w_{11} - 2w_{12} & -3w_{12} - 2w_{22} \\end{pmatrix} + \\begin{pmatrix} -w_{11} + 2w_{12} & -3w_{11} - 2w_{12} \\\\ -w_{12} + 2w_{22} & -3w_{12} - 2w_{22} \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nSumming these matrices gives:\n$$ \\begin{pmatrix} -2w_{11} + 4w_{12} & -3w_{11} - 3w_{12} + 2w_{22} \\\\ -3w_{11} - 3w_{12} + 2w_{22} & -6w_{12} - 4w_{22} \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nThis yields a system of three linear equations for the three unknown elements of $W_c$:\n1.  $-2w_{11} + 4w_{12} = -1$\n2.  $-3w_{11} - 3w_{12} + 2w_{22} = 0$\n3.  $-6w_{12} - 4w_{22} = 0$\n\nFrom equation (3), $w_{22} = -\\frac{6}{4}w_{12} = -\\frac{3}{2}w_{12}$.\nSubstitute this into equation (2):\n$$ -3w_{11} - 3w_{12} + 2\\left(-\\frac{3}{2}w_{12}\\right) = 0 \\implies -3w_{11} - 3w_{12} - 3w_{12} = 0 \\implies -3w_{11} = 6w_{12} \\implies w_{11} = -2w_{12} $$\nSubstitute this result for $w_{11}$ into equation (1):\n$$ -2(-2w_{12}) + 4w_{12} = -1 \\implies 4w_{12} + 4w_{12} = -1 \\implies 8w_{12} = -1 \\implies w_{12} = -\\frac{1}{8} $$\nNow, we find the other components:\n$$ w_{11} = -2\\left(-\\frac{1}{8}\\right) = \\frac{1}{4} $$\n$$ w_{22} = -\\frac{3}{2}\\left(-\\frac{1}{8}\\right) = \\frac{3}{16} $$\nThus, the controllability Gramian is:\n$$ W_c = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{8} \\\\ -\\frac{1}{8} & \\frac{3}{16} \\end{pmatrix} $$\n\n**Part 2: Interpretation of the Eigenvectors of $W_c$**\n\nThe controllability Gramian $W_c$ characterizes the reachable states from the origin. The set of all states $\\mathbf{x}$ that can be reached from $\\mathbf{x}(0) = \\mathbf{0}$ using control inputs $u(t)$ with a total squared $L^2$ norm (energy) of unity, i.e., $\\int_0^\\infty u(t)^2 dt \\leq 1$, forms an ellipsoid in the state space defined by the quadratic form $\\mathbf{x}^{\\top} W_c^{-1} \\mathbf{x} \\leq 1$.\n\nThe eigenvectors of $W_c$ define the principal axes of this reachable-set ellipsoid. Let the eigendecomposition of the symmetric matrix $W_c$ be $W_c = V \\Lambda V^{\\top}$, where $V$ is an orthogonal matrix whose columns are the eigenvectors $\\mathbf{v}_i$, and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_i$. The inverse is $W_c^{-1} = V \\Lambda^{-1} V^{\\top}$.\n\nThe ellipsoid equation becomes $\\mathbf{x}^{\\top} (V \\Lambda^{-1} V^{\\top}) \\mathbf{x} \\leq 1$. By changing coordinates to the basis of eigenvectors, $\\mathbf{y} = V^{\\top}\\mathbf{x}$, the equation simplifies to $\\mathbf{y}^{\\top} \\Lambda^{-1} \\mathbf{y} \\leq 1$, or $\\sum_i \\frac{y_i^2}{\\lambda_i} \\leq 1$. The semi-axis length along the $i$-th eigenvector direction is $\\sqrt{\\lambda_i}$.\n\nA large eigenvalue $\\lambda_i$ corresponds to a long semi-axis of the ellipsoid. This means that a unit-energy input can drive the state far in the direction of the corresponding eigenvector $\\mathbf{v}_i$. Therefore, directions in state space aligned with eigenvectors having large eigenvalues are \"easy\" to control; they require little control effort to achieve a given displacement.\n\nConversely, a small eigenvalue $\\lambda_j$ corresponds to a short semi-axis. This means that even a unit-energy input can only move the state a small distance in the direction of $\\mathbf{v}_j$. These directions are \"hard\" to control, requiring large control effort for significant displacement.\n\nThis relationship is explicitly captured by the formula for the minimal control effort required to reach a target state $\\mathbf{x}_f$, which is $E_{\\min} = \\mathbf{x}_f^{\\top} W_c^{-1} \\mathbf{x}_f$. If $\\mathbf{x}_f$ is an eigenvector $\\mathbf{v}_i$, then $E_{\\min} = \\mathbf{v}_i^{\\top} (V \\Lambda^{-1} V^{\\top}) \\mathbf{v}_i = \\mathbf{e}_i^{\\top} \\Lambda^{-1} \\mathbf{e}_i = 1/\\lambda_i$, where $\\mathbf{e}_i$ is the standard basis vector. The minimal effort is inversely proportional to the eigenvalue, formally linking larger eigenvalues to easier control.\n\n**Part 3: Computation of Minimal Control Effort**\n\nThe problem asks for the minimal squared $L^2$ control effort, $E_{\\min} = \\int_0^T u(t)^2 dt$, to steer the system from $\\mathbf{x}(0) = \\mathbf{0}$ to $\\mathbf{x}(T) = \\mathbf{x}_f$ for a sufficiently large time $T$. For $T \\to \\infty$, this minimal effort is given by the quadratic form:\n$$ E_{\\min} = \\mathbf{x}_f^{\\top} W_c^{-1} \\mathbf{x}_f $$\nwhere $W_c$ is the infinite-horizon controllability Gramian computed in Part 1. First, we must calculate the inverse of $W_c$:\n$$ W_c = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{8} \\\\ -\\frac{1}{8} & \\frac{3}{16} \\end{pmatrix} = \\frac{1}{16} \\begin{pmatrix} 4 & -2 \\\\ -2 & 3 \\end{pmatrix} $$\nThe determinant is:\n$$ \\det(W_c) = \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{16}\\right) - \\left(-\\frac{1}{8}\\right)^2 = \\frac{3}{64} - \\frac{1}{64} = \\frac{2}{64} = \\frac{1}{32} $$\nThe inverse $W_c^{-1}$ is:\n$$ W_c^{-1} = \\frac{1}{\\det(W_c)} \\begin{pmatrix} \\frac{3}{16} & \\frac{1}{8} \\\\ \\frac{1}{8} & \\frac{1}{4} \\end{pmatrix} = 32 \\begin{pmatrix} \\frac{3}{16} & \\frac{1}{8} \\\\ \\frac{1}{8} & \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 32 \\cdot \\frac{3}{16} & 32 \\cdot \\frac{1}{8} \\\\ 32 \\cdot \\frac{1}{8} & 32 \\cdot \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 6 & 4 \\\\ 4 & 8 \\end{pmatrix} $$\nNow, we compute the minimal effort $E_{\\min}$ with the target state $\\mathbf{x}_f = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$:\n$$ E_{\\min} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 6 & 4 \\\\ 4 & 8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nFirst, perform the matrix-vector multiplication:\n$$ \\begin{pmatrix} 6 & 4 \\\\ 4 & 8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 6(1) + 4(-1) \\\\ 4(1) + 8(-1) \\end{pmatrix} = \\begin{pmatrix} 6 - 4 \\\\ 4 - 8 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -4 \\end{pmatrix} $$\nThen, perform the final vector-vector multiplication:\n$$ E_{\\min} = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -4 \\end{pmatrix} = 1(2) + (-1)(-4) = 2 + 4 = 6 $$\nThe minimal squared $L^2$ control effort is $6$.",
            "answer": "$$ \\boxed{6} $$"
        },
        {
            "introduction": "This final practice applies optimal control principles to a classic problem in robotics: managing redundancy. You will formulate and solve a quadratic program to control a multi-joint arm, where the goal is to achieve a desired end-effector velocity while simultaneously minimizing joint movement effort and maintaining a preferred posture. This exercise demonstrates how optimal feedback control provides a powerful framework for resolving ambiguity in over-actuated systems, a common challenge in both engineering and biological motor control .",
            "id": "4006060",
            "problem": "Consider a planar redundant robotic arm with three revolute joints modeled as a kinematic chain with link lengths $L_1$, $L_2$, and $L_3$ in meters. Let the joint angle vector be $q \\in \\mathbb{R}^3$ in radians and the joint velocity command be $\\dot{q} \\in \\mathbb{R}^3$ in radians per second. The end-effector Cartesian velocity in the plane is $v \\in \\mathbb{R}^2$ in meters per second and is related to the joint velocities by the kinematic linearization\n$$\nv = J(q) \\, \\dot{q},\n$$\nwhere $J(q) \\in \\mathbb{R}^{2 \\times 3}$ is the planar manipulator Jacobian at configuration $q$.\n\nIn Optimal Feedback Control (OFC), one designs a control law to achieve a task while minimizing effort. Here, at a single time instant, you will compute the null-space control that minimizes a quadratic effort while maintaining an exact end-effector velocity tracking constraint. This is expressed as the following convex Quadratic Program (QP):\n$$\n\\begin{aligned}\n\\text{minimize}_{\\dot{q} \\in \\mathbb{R}^3} \\quad & \\frac{1}{2} \\, \\dot{q}^\\top R \\, \\dot{q} + \\frac{1}{2} \\, \\lVert \\dot{q} - u_0 \\rVert_Q^2 \\\\\n\\text{subject to} \\quad & J(q) \\, \\dot{q} = v_d,\n\\end{aligned}\n$$\nwhere $R \\in \\mathbb{R}^{3 \\times 3}$ is a symmetric positive definite effort-weight matrix, $Q \\in \\mathbb{R}^{3 \\times 3}$ is a symmetric positive definite weight matrix, and $u_0 \\in \\mathbb{R}^3$ is a posture-regulating feedback term given by\n$$\nu_0 = k_p \\, (q_{\\text{ref}} - q).\n$$\nHere $k_p > 0$ is a scalar posture feedback gain with unit $\\text{s}^{-1}$, and $q_{\\text{ref}} \\in \\mathbb{R}^3$ is a reference posture in radians. The matrix $Q$ is taken to be $Q = \\alpha I_3$ with $\\alpha > 0$, where $I_3$ is the $3 \\times 3$ identity matrix. The equality constraint enforces exact end-effector velocity tracking to the desired Cartesian velocity $v_d \\in \\mathbb{R}^2$ in meters per second.\n\nYou must:\n- Use the standard planar $3$-link forward kinematics definitions for a serial chain in the plane and derive $J(q)$ via partial derivatives of the end-effector position with respect to $q$.\n- Formulate the QP in the canonical form with a quadratic term and a linear term and solve it exactly using the Karush–Kuhn–Tucker (KKT) conditions at this single time instant.\n- Compute the optimal joint velocity $\\dot{q}^\\star$ that minimizes the quadratic cost while satisfying the equality constraint for each test case.\n\nFundamental base for derivation and implementation:\n- The kinematic relation $v = J(q)\\,\\dot{q}$ for a robotic manipulator.\n- The definition of the planar Jacobian via partial derivatives of forward kinematics.\n- The structure of convex quadratic programming with equality constraints and the KKT system.\n\nUse the following fixed link lengths (in meters): $L_1 = 0.3$, $L_2 = 0.3$, $L_3 = 0.2$.\n\nAll joint angles must be in radians, joint velocities in radians per second, Cartesian velocities in meters per second, and link lengths in meters. The numeric output must be joint velocities in radians per second, rounded to six decimal places.\n\nTest Suite:\n- Case $1$ (happy path): $q = [0.2, -0.5, 0.3]$, $q_{\\text{ref}} = [0.0, 0.0, 0.0]$, $v_d = [0.05, -0.02]$, $R = \\mathrm{diag}(2.0, 1.0, 1.0)$, $\\alpha = 0.1$, $k_p = 2.0$.\n- Case $2$ (null-space motion with zero task velocity): $q = [1.0, -1.2, 0.5]$, $q_{\\text{ref}} = [0.2, -0.4, 0.1]$, $v_d = [0.0, 0.0]$, $R = \\mathrm{diag}(1.0, 1.0, 1.0)$, $\\alpha = 0.5$, $k_p = 1.5$.\n- Case $3$ (high effort penalty, small posture weight): $q = [0.1, 0.2, -0.1]$, $q_{\\text{ref}} = [0.2, 0.2, -0.4]$, $v_d = [0.02, 0.02]$, $R = \\mathrm{diag}(10.0, 10.0, 10.0)$, $\\alpha = 0.01$, $k_p = 1.0$.\n- Case $4$ (anisotropic effort weights): $q = [0.6, 0.4, -0.3]$, $q_{\\text{ref}} = [0.0, 0.5, -0.2]$, $v_d = [-0.03, 0.04]$, $R = \\mathrm{diag}(0.5, 2.0, 5.0)$, $\\alpha = 0.5$, $k_p = 2.0$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of three floats corresponding to $\\dot{q}^\\star$ for a test case, rounded to six decimals. For example: $[[a_1,a_2,a_3],[b_1,b_2,b_3],\\dots]$.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of robotic kinematics and optimal control theory, is well-posed as a convex quadratic program with linear constraints, and all necessary parameters and conditions are provided for a unique solution.\n\nThe task is to compute the optimal joint velocity command $\\dot{q}^\\star$ for a $3$-link planar redundant manipulator. This is achieved by solving a quadratic programming (QP) problem at a single time instant. The solution minimizes a cost function comprising a control effort term and a posture regulation term, subject to an exact end-effector velocity tracking constraint. The solution will be derived using the method of Lagrange multipliers, which for this problem corresponds to the Karush-Kuhn-Tucker (KKT) conditions.\n\nFirst, we establish the forward kinematics and the Jacobian of the manipulator. Let the joint angles be $q = [q_1, q_2, q_3]^\\top$. The end-effector position $p_e(q) = [x_e(q), y_e(q)]^\\top$ for a serial chain starting at the origin is given by:\n$$ x_e(q) = L_1 \\cos(q_1) + L_2 \\cos(q_1+q_2) + L_3 \\cos(q_1+q_2+q_3) $$\n$$ y_e(q) = L_1 \\sin(q_1) + L_2 \\sin(q_1+q_2) + L_3 \\sin(q_1+q_2+q_3) $$\nwhere $L_1$, $L_2$, and $L_3$ are the link lengths.\n\nThe end-effector velocity $v = \\dot{p}_e$ is related to the joint velocities $\\dot{q}$ by the manipulator Jacobian $J(q) = \\frac{\\partial p_e}{\\partial q} \\in \\mathbb{R}^{2 \\times 3}$. The elements of the Jacobian are the partial derivatives of the end-effector position components with respect to each joint angle. Let $c_1 = \\cos(q_1)$, $s_1 = \\sin(q_1)$, $c_{12} = \\cos(q_1+q_2)$, $s_{12} = \\sin(q_1+q_2)$, $c_{123} = \\cos(q_1+q_2+q_3)$, and $s_{123} = \\sin(q_1+q_2+q_3)$. The Jacobian is:\n$$ J(q) = \\begin{bmatrix} \\frac{\\partial x_e}{\\partial q_1} & \\frac{\\partial x_e}{\\partial q_2} & \\frac{\\partial x_e}{\\partial q_3} \\\\ \\frac{\\partial y_e}{\\partial q_1} & \\frac{\\partial y_e}{\\partial q_2} & \\frac{\\partial y_e}{\\partial q_3} \\end{bmatrix} $$\nThe partial derivatives are:\n$$ \\frac{\\partial x_e}{\\partial q_1} = -L_1 s_1 - L_2 s_{12} - L_3 s_{123} $$\n$$ \\frac{\\partial x_e}{\\partial q_2} = -L_2 s_{12} - L_3 s_{123} $$\n$$ \\frac{\\partial x_e}{\\partial q_3} = -L_3 s_{123} $$\n$$ \\frac{\\partial y_e}{\\partial q_1} = L_1 c_1 + L_2 c_{12} + L_3 c_{123} $$\n$$ \\frac{\\partial y_e}{\\partial q_2} = L_2 c_{12} + L_3 c_{123} $$\n$$ \\frac{\\partial y_e}{\\partial q_3} = L_3 c_{123} $$\n\nNext, we formulate the QP. The objective is to minimize the cost function:\n$$ C(\\dot{q}) = \\frac{1}{2} \\dot{q}^\\top R \\dot{q} + \\frac{1}{2} \\lVert \\dot{q} - u_0 \\rVert_Q^2 $$\nsubject to the constraint $J(q) \\dot{q} = v_d$.\nThe cost function can be expanded and rewritten in a canonical quadratic form.\n$$ C(\\dot{q}) = \\frac{1}{2} \\dot{q}^\\top R \\dot{q} + \\frac{1}{2} (\\dot{q} - u_0)^\\top Q (\\dot{q} - u_0) $$\n$$ C(\\dot{q}) = \\frac{1}{2} \\dot{q}^\\top R \\dot{q} + \\frac{1}{2} (\\dot{q}^\\top Q \\dot{q} - \\dot{q}^\\top Q u_0 - u_0^\\top Q \\dot{q} + u_0^\\top Q u_0) $$\nSince $Q$ is symmetric, $u_0^\\top Q \\dot{q} = (\\dot{q}^\\top Q u_0)^\\top = \\dot{q}^\\top Q u_0$.\n$$ C(\\dot{q}) = \\frac{1}{2} \\dot{q}^\\top (R+Q) \\dot{q} - (Q u_0)^\\top \\dot{q} + \\frac{1}{2} u_0^\\top Q u_0 $$\nThe constant term $\\frac{1}{2} u_0^\\top Q u_0$ does not affect the location of the minimum, so we can omit it from the optimization. The QP is:\n$$\n\\begin{aligned}\n\\text{minimize}_{\\dot{q} \\in \\mathbb{R}^3} \\quad & \\frac{1}{2} \\dot{q}^\\top H \\dot{q} + c^\\top \\dot{q} \\\\\n\\text{subject to} \\quad & J(q) \\dot{q} = v_d,\n\\end{aligned}\n$$\nwhere $H = R+Q$ is the Hessian matrix and $c = -Q u_0$ is the linear cost vector. Since $R$ and $Q$ are symmetric positive definite, $H$ is also symmetric positive definite, which makes the problem strictly convex and guarantees a unique minimum.\n\nTo solve this equality-constrained QP, we form the Lagrangian $\\mathcal{L}(\\dot{q}, \\lambda)$:\n$$ \\mathcal{L}(\\dot{q}, \\lambda) = \\frac{1}{2} \\dot{q}^\\top H \\dot{q} - (Q u_0)^\\top \\dot{q} + \\lambda^\\top (J(q)\\dot{q} - v_d) $$\nwhere $\\lambda \\in \\mathbb{R}^2$ is the vector of Lagrange multipliers. The KKT conditions for optimality require the gradient of the Lagrangian to be zero.\n\n1. Stationarity: $\\nabla_{\\dot{q}} \\mathcal{L} = 0$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}} = H \\dot{q} - Q u_0 + J(q)^\\top \\lambda = 0 $$\n$$ H \\dot{q} = Q u_0 - J(q)^\\top \\lambda $$\nSince $H$ is invertible, we can express $\\dot{q}$ in terms of $\\lambda$:\n$$ \\dot{q} = H^{-1} (Q u_0 - J(q)^\\top \\lambda) $$\n\n2. Primal Feasibility: The constraint must hold.\n$$ J(q) \\dot{q} = v_d $$\n\nWe solve for $\\lambda$ by substituting the expression for $\\dot{q}$ into the feasibility constraint:\n$$ J(q) [H^{-1} (Q u_0 - J(q)^\\top \\lambda)] = v_d $$\n$$ J(q) H^{-1} Q u_0 - J(q) H^{-1} J(q)^\\top \\lambda = v_d $$\n$$ (J(q) H^{-1} J(q)^\\top) \\lambda = J(q) H^{-1} Q u_0 - v_d $$\nFor non-singular configurations, the Jacobian $J(q)$ has full row rank, and the matrix $S = J(q) H^{-1} J(q)^\\top$ is a $2 \\times 2$ invertible matrix. We can solve for $\\lambda$:\n$$ \\lambda = (J(q) H^{-1} J(q)^\\top)^{-1} (J(q) H^{-1} Q u_0 - v_d) $$\n\nFinally, we substitute $\\lambda$ back into the expression for $\\dot{q}$ to obtain the optimal joint velocity $\\dot{q}^\\star$:\n$$ \\dot{q}^\\star = H^{-1} (Q u_0 - J(q)^\\top \\lambda) $$\nThe posture-regulating feedback term is $u_0 = k_p (q_{\\text{ref}} - q)$ and the weight matrix $Q = \\alpha I_3$, where $I_3$ is the $3 \\times 3$ identity matrix.\n\nThe algorithm for computation is as follows:\n1. For each test case, retrieve the parameters: $q$, $q_{\\text{ref}}$, $v_d$, $R$, $\\alpha$, $k_p$.\n2. Compute the posture feedback term: $u_0 = k_p (q_{\\text{ref}} - q)$.\n3. Construct the matrices $Q = \\alpha I_3$ and $H = R+Q$.\n4. Compute the Jacobian matrix $J(q)$ for the current joint configuration $q$.\n5. Compute the matrix $S = J(q) H^{-1} J(q)^\\top$. This involves inverting $H$.\n6. Compute the vector $b = J(q) H^{-1} Q u_0 - v_d$.\n7. Solve the $2 \\times 2$ linear system $S \\lambda = b$ for the Lagrange multipliers $\\lambda$.\n8. Compute the optimal joint velocity vector $\\dot{q}^\\star = H^{-1} (Q u_0 - J(q)^\\top \\lambda)$.\n9. Package the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal joint velocities for a 3-link planar manipulator\n    based on a quadratic programming formulation for optimal feedback control.\n    \"\"\"\n\n    # Fixed link lengths in meters\n    L1, L2, L3 = 0.3, 0.3, 0.2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"q\": np.array([0.2, -0.5, 0.3]),\n            \"q_ref\": np.array([0.0, 0.0, 0.0]),\n            \"v_d\": np.array([0.05, -0.02]),\n            \"R\": np.diag([2.0, 1.0, 1.0]),\n            \"alpha\": 0.1,\n            \"k_p\": 2.0,\n        },\n        {\n            \"q\": np.array([1.0, -1.2, 0.5]),\n            \"q_ref\": np.array([0.2, -0.4, 0.1]),\n            \"v_d\": np.array([0.0, 0.0]),\n            \"R\": np.diag([1.0, 1.0, 1.0]),\n            \"alpha\": 0.5,\n            \"k_p\": 1.5,\n        },\n        {\n            \"q\": np.array([0.1, 0.2, -0.1]),\n            \"q_ref\": np.array([0.2, 0.2, -0.4]),\n            \"v_d\": np.array([0.02, 0.02]),\n            \"R\": np.diag([10.0, 10.0, 10.0]),\n            \"alpha\": 0.01,\n            \"k_p\": 1.0,\n        },\n        {\n            \"q\": np.array([0.6, 0.4, -0.3]),\n            \"q_ref\": np.array([0.0, 0.5, -0.2]),\n            \"v_d\": np.array([-0.03, 0.04]),\n            \"R\": np.diag([0.5, 2.0, 5.0]),\n            \"alpha\": 0.5,\n            \"k_p\": 2.0,\n        },\n    ]\n\n    results = []\n    \n    def compute_jacobian(q):\n        \"\"\"Computes the 2x3 planar manipulator Jacobian.\"\"\"\n        q1, q2, q3 = q[0], q[1], q[2]\n        \n        s1 = np.sin(q1)\n        c1 = np.cos(q1)\n        s12 = np.sin(q1 + q2)\n        c12 = np.cos(q1 + q2)\n        s123 = np.sin(q1 + q2 + q3)\n        c123 = np.cos(q1 + q2 + q3)\n\n        J = np.zeros((2, 3))\n        J[0, 0] = -L1 * s1 - L2 * s12 - L3 * s123\n        J[0, 1] = -L2 * s12 - L3 * s123\n        J[0, 2] = -L3 * s123\n        J[1, 0] = L1 * c1 + L2 * c12 + L3 * c123\n        J[1, 1] = L2 * c12 + L3 * c123\n        J[1, 2] = L3 * c123\n        \n        return J\n\n    for case in test_cases:\n        q = case[\"q\"]\n        q_ref = case[\"q_ref\"]\n        v_d = case[\"v_d\"]\n        R = case[\"R\"]\n        alpha = case[\"alpha\"]\n        k_p = case[\"k_p\"]\n\n        # Step 1: Compute posture feedback term u0\n        u0 = k_p * (q_ref - q)\n\n        # Step 2: Construct Q and H matrices\n        Q = alpha * np.identity(3)\n        H = R + Q\n        H_inv = np.linalg.inv(H)\n\n        # Step 3: Compute Jacobian\n        J = compute_jacobian(q)\n\n        # Step 4: Solve for Lagrange multipliers lambda\n        # lambda = (J H_inv J^T)^-1 * (J H_inv Q u0 - v_d)\n        JH_inv = J @ H_inv\n        JH_inv_JT = JH_inv @ J.T\n        \n        # Check if JH_inv_JT is invertible\n        if np.linalg.det(JH_inv_JT) < 1e-12:\n            # Handle singularity, though not expected for these test cases\n            # Use pseudo-inverse for robustness, although direct inverse is fine here\n            JH_inv_JT_inv = np.linalg.pinv(JH_inv_JT)\n        else:\n            JH_inv_JT_inv = np.linalg.inv(JH_inv_JT)\n\n        rhs = JH_inv @ Q @ u0 - v_d\n        lambda_val = JH_inv_JT_inv @ rhs\n        \n        # Step 5: Compute optimal joint velocity q_dot_star\n        # q_dot_star = H_inv * (Q * u0 - J^T * lambda)\n        q_dot_star = H_inv @ (Q @ u0 - J.T @ lambda_val)\n\n        # Round to six decimal places and convert to list\n        result = [round(x, 6) for x in q_dot_star]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}