{
    "hands_on_practices": [
        {
            "introduction": "要真正理解计算记忆模型，没有什么比亲手构建一个更有效了。这项练习将指导您实现一个稀疏分布式存储器 (Sparse Distributed Memory, SDM)，这是一种经典的内容可寻址联想记忆模型。通过这个实践，您将探索其核心原理：一个巨大的地址空间、基于汉明距离的邻近激活，以及通过叠加方式进行存储，从而深入了解分布式表征和噪声鲁棒性的概念。",
            "id": "3971099",
            "problem": "考虑实现一个简易的稀疏分布式存储器（SDM），这是一种在脑模型和计算神经科学中广为人知的联想记忆模型。SDM由一组固定的二进制地址和相应的每比特整数计数器组成。存储和检索由二进制汉明空间中的邻近性定义。假设以下基本原理：\n\n- 二进制模式表示为固定长度为 $N$ 的向量 $x \\in \\{0,1\\}^N$。\n- 两个二进制向量 $x$ 和 $y$ 之间的汉明距离为 $d_H(x,y) = \\sum_{k=1}^{N} \\mathbf{1}[x_k \\neq y_k]$。\n- 分布式存储通过整数计数器的加性叠加来建模；当写入一个模式时，在与该模式足够接近的存储位置，比特为 $1$ 的计数器加 $1$，比特为 $0$ 的计数器减 $1$。\n- 检索过程对足够近的位置的计数器求和，并应用阈值决策规则来重建一个二进制向量。\n\n你必须实现一个完整的程序来构建一个 $N=256$ 比特的SDM。设有 $M$ 个硬位置（地址），每个都是从 $\\{0,1\\}^N$ 中均匀采样的独立随机向量。每个硬位置存储一个由 $N$ 个初始化为零的整数计数器组成的向量。程序必须：\n\n1. 生成 $P$ 个随机二进制模式 $x^{(1)},\\dots,x^{(P)}$，每个模式都在 $\\{0,1\\}^{N}$ 中，且独立均匀分布。\n2. 对于给定的写入半径 $r_w$，将每个模式 $x^{(p)}$ 写入所有汉明距离满足 $d_H(a_i, x^{(p)}) \\le r_w$ 的硬位置 $a_i$ 中，并根据以下规则更新计数器：\n   - 对于每个比特索引 $k \\in \\{1,\\dots,N\\}$：如果 $x^{(p)}_k = 1$，则计数器 $c_{i,k}$ 加 $1$；如果 $x^{(p)}_k = 0$，则计数器 $c_{i,k}$ 减 $1$。\n3. 对于给定的读取半径 $r_r$ 和阈值 $\\theta$，通过以下方式回忆每个存储的模式 $x^{(p)}$：\n   - 选择所有满足 $d_H(a_i, x^{(p)}) \\le r_r$ 的硬位置 $a_i$。\n   - 计算每比特的总和 $s_k = \\sum_{i: d_H(a_i, x^{(p)}) \\le r_r} c_{i,k}$，其中 $k \\in \\{1,\\dots,N\\}$。\n   - 通过阈值规则生成回忆出的二进制向量 $\\hat{x}^{(p)}$：如果 $s_k \\ge \\theta$，则 $\\hat{x}^{(p)}_k = 1$，否则 $\\hat{x}^{(p)}_k = 0$。\n4. 评估回忆准确率，其定义为在所有存储模式中被正确回忆的比特所占的比例：\n   $$\\text{accuracy} = \\frac{1}{P N} \\sum_{p=1}^{P} \\sum_{k=1}^{N} \\mathbf{1}[\\hat{x}^{(p)}_k = x^{(p)}_k].$$\n   准确率必须以 $[0,1]$ 范围内的小数形式报告。\n\n在所有测试用例中，使用同一组硬位置和同一组模式，以确保可比性。使用固定的随机种子以确保结果可复现。\n\n为保证科学真实性和覆盖范围，采用以下固定参数：\n- 比特数：$N=256$。\n- 硬位置数：$M=2048$。\n- 模式数：$P=200$。\n- 地址和模式的随机种子：$s=12345$。\n\n通过改变写入半径 $r_w$（这会影响每次写入时更新的硬位置的预期比例）来改变写入密度，并改变读取阈值 $\\theta$。在所有测试用例中，使用读取半径 $r_r = r_w$。实现该程序，以在以下测试套件上评估准确率：\n\n- 测试用例 1：$(r_w, \\theta) = (104, 0)$，低写入密度（大约低于平均汉明距离三个标准差）。\n- 测试用例 2：$(r_w, \\theta) = (120, 0)$，中等写入密度（大约低于平均值一个标准差）。\n- 测试用例 3：$(r_w, \\theta) = (120, 8)$，与测试用例 2 密度相同，但使用正阈值以减少虚假回忆。\n- 测试用例 4：$(r_w, \\theta) = (136, 0)$，高写入密度（大约高于平均值一个标准差）。\n- 测试用例 5：$(r_w, \\theta) = (256, 0)$，最大写入密度（所有位置都被更新和读取）。\n- 测试用例 6：$(r_w, \\theta) = (0, 0)$，边界情况，只有地址完全匹配时才会被更新或读取。\n\n所有距离均为汉明距离；所有阈值均为整数；所有准确率必须为小数。最终程序必须为所有测试用例生成准确率，并将其作为单行输出，格式为方括号内包含的逗号分隔列表，例如 $[a_1,a_2,\\dots,a_6]$，其中每个 $a_i$ 是根据上述定义计算的测试用例 $i$ 的准确率。不涉及物理单位；不存在角度；百分比必须表示为不带百分号的小数。",
            "solution": "用户提供了一个定义明确的计算问题，要求实现一个稀疏分布式存储器（SDM）模型。该问题具有科学依据，形式化规定清晰，并且计算上是可行的。所有必要的参数和评估指标都已明确说明，使得该问题适合于一个完整的解决方案。任务的核心是模拟 SDM 中二进制模式的存储和检索，并在各种参数设置下评估其回忆准确率。\n\n### 数学和算法框架\n\n**1. 数据表示和生成**\n\n- 所有数据，包括硬位置地址和内存模式，都表示为固定长度 $N = 256$ 的二进制向量。向量 $x$ 是空间 $\\{0,1\\}^{N}$ 中的一个元素。\n- SDM 由 $M = 2048$ 个硬位置 $\\{a_i\\}_{i=1}^{M}$ 定义，其中每个 $a_i \\in \\{0,1\\}^{N}$ 是一个其分量从 $\\{0,1\\}$ 中独立均匀采样的向量。\n- 类似地，生成一组 $P = 200$ 个模式 $\\{x^{(p)}\\}_{p=1}^{P}$，其中每个 $x^{(p)} \\in \\{0,1\\}^{N}$ 是一个随机二进制向量。\n- 为确保可复现性，所有随机元素都使用固定的随机种子 $s = 12345$ 生成。\n- 存储器本身由一个大小为 $M \\times N$ 的计数器矩阵 $C$ 组成。每个元素 $c_{i,k}$ 是一个整数计数器，对应于第 $i$ 个硬位置的第 $k$ 个比特。所有计数器都初始化为 $0$。\n\n**2. 距离度量**\n\n模式和硬位置之间的相互作用由汉明距离 $d_H$ 决定。对于两个向量 $x, y \\in \\{0,1\\}^{N}$，距离是它们对应比特不同的位置数量：\n$$d_H(x,y) = \\sum_{k=1}^{N} \\mathbf{1}[x_k \\neq y_k]$$\n其中 $\\mathbf{1}[\\cdot]$ 是指示函数，当其参数为真时为 $1$，否则为 $0$。\n\n**3. 写入操作（存储）**\n\n这 $P$ 个模式的存储是迭代执行的。对于每个模式 $x^{(p)}$ ($p \\in \\{1, \\dots, P\\}$)，执行以下步骤：\n- 从模式 $x^{(p)}$ 派生出一个“更新向量”$u^{(p)} \\in \\{-1, 1\\}^{N}$：$u^{(p)}_k = 2x^{(p)}_k - 1$。这将比特值 $1$ 映射到 $+1$，比特值 $0$ 映射到 $-1$。\n- 对于每个硬位置 $a_i$ ($i \\in \\{1, \\dots, M\\}$)，计算汉明距离 $d_H(a_i, x^{(p)})$。\n- 如果 $d_H(a_i, x^{(p)}) \\le r_w$（其中 $r_w$ 是指定的写入半径），则通过加上更新向量来更新相应的计数器行 $c_{i, \\cdot}$：\n  $$c_{i,k} \\leftarrow c_{i,k} + u^{(p)}_k \\quad \\text{for all } k \\in \\{1, \\dots, N\\}$$\n对所有 $P$ 个模式重复此过程，更新是累积的（叠加）。\n\n**4. 读取操作（检索）**\n\n为了评估存储器的保真度，我们尝试回忆每个原始模式。对于每个用作检索提示的模式 $x^{(p)}$：\n- 确定用于读取的活动硬位置集合。这些是所有满足 $d_H(a_i, x^{(p)}) \\le r_r$ 的位置 $a_i$，其中 $r_r$ 是读取半径。在这个问题中，给定 $r_r = r_w$。\n- 通过对所有活动位置的计数器向量求和，计算出一个和向量 $s \\in \\mathbb{Z}^{N}$：\n  $$s_k = \\sum_{i : d_H(a_i, x^{(p)}) \\le r_r} c_{i,k} \\quad \\text{for all } k \\in \\{1, \\dots, N\\}$$\n  如果没有位置在读取半径内，则和向量为零向量。\n- 通过对和向量应用阈值规则，构建检索到的模式 $\\hat{x}^{(p)} \\in \\{0,1\\}^{N}$：\n  $$\\hat{x}^{(p)}_k = \\begin{cases} 1  \\text{if } s_k \\ge \\theta \\\\ 0  \\text{if } s_k  \\theta \\end{cases}$$\n  其中 $\\theta$ 是给定的整数阈值。\n\n**5. 准确率评估**\n\n性能通过所有模式的总体回忆准确率来衡量。准确率定义为正确检索的比特所占的比例：\n$$\\text{accuracy} = \\frac{1}{P \\cdot N} \\sum_{p=1}^{P} \\sum_{k=1}^{N} \\mathbf{1}[\\hat{x}^{(p)}_k = x^{(p)}_k]$$\n总比特数为 $P \\cdot N = 200 \\cdot 256 = 51200$。\n\n### 实现策略\n\n该算法使用 Python 实现，并利用 NumPy 库进行高效的数值计算。\n- **初始化**：使用种子 $s = 12345$ 创建一个 `numpy.random.default_rng` 实例。该生成器用于创建 `addresses` 数组（形状为 `(2048, 256)`）和 `patterns` 数组（形状为 `(200, 256)`），两者类型均为 `numpy.int8` 以节省内存。\n- **主循环**：程序遍历测试用例列表 `(r_w, \\theta)`。对于每个用例，它都会重新初始化一个形状为 `(2048, 256)` 的 `counters` 矩阵，数据类型为 `numpy.int32`，以防止求和过程中发生溢出。\n- **向量化**：核心操作被向量化以提高性能。\n    - **汉明距离**：从单个模式 `p`（形状为 `(256,)`）到所有 `M` 个地址（形状为 `(2048, 256)`）的距离可以通过广播高效计算：`numpy.sum(addresses != p, axis=1)`。这将产生一个包含 $M$ 个距离的数组。\n    - **计数器更新**：在写入阶段，对于给定的模式，会创建一个形状为 `(2048,)` 的布尔掩码来识别活动地址。然后，在一个单一操作中将更新向量添加到 `counters` 矩阵的所有活动行：`counters[mask, :] += update_vector`。\n    - **求和**：在读取阶段，一个类似的布尔掩码会识别活动的读取位置。和向量通过 `numpy.sum(counters[mask, :], axis=0)` 计算得出。\n- **准确率计算**：在检索到一个模式后，正确比特的数量通过 `numpy.sum(recalled_pattern == original_pattern)` 找到。这些计数被累加起来，最终的准确率通过除以总比特数 $P \\cdot N$ 计算得出。\n每个测试用例的结果被收集并格式化为所需的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Sparse Distributed Memory (SDM) simulation\n    and evaluates its accuracy on a suite of test cases.\n    \"\"\"\n    # Fixed parameters for the SDM model\n    N = 256  # Number of bits in vectors\n    M = 2048 # Number of hard locations\n    P = 200  # Number of patterns to store\n    SEED = 12345 # Random seed for reproducibility\n\n    # Test cases: (write_radius, threshold)\n    # The read radius r_r is always equal to the write radius r_w.\n    test_cases = [\n        (104, 0),\n        (120, 0),\n        (120, 8),\n        (136, 0),\n        (256, 0),\n        (0, 0),\n    ]\n\n    # --- Step 1: Initialization ---\n    # Create a random number generator with a fixed seed\n    rng = np.random.default_rng(SEED)\n\n    # Generate M hard location addresses, each a random N-bit vector\n    addresses = rng.integers(0, 2, size=(M, N), dtype=np.int8)\n\n    # Generate P patterns to be stored and recalled, each a random N-bit vector\n    patterns = rng.integers(0, 2, size=(P, N), dtype=np.int8)\n\n    # --- Loop over test cases ---\n    results = []\n    for r_w, theta in test_cases:\n        r_r = r_w  # Read radius is same as write radius\n\n        # Initialize counter matrix for this test case\n        # Use int32 to avoid overflow, as max counter value can be P=200.\n        counters = np.zeros((M, N), dtype=np.int32)\n\n        # --- Step 2: Write Phase (Store all patterns) ---\n        # Pre-compute update vectors for all patterns (+1 for bit 1, -1 for bit 0)\n        update_vectors = 2 * patterns - 1\n\n        for p in range(P):\n            pattern = patterns[p]\n            update_vector = update_vectors[p]\n\n            # Calculate Hamming distances from the current pattern to all hard locations\n            # Broadcasting (addresses != pattern) handles the comparison efficiently.\n            distances = np.sum(addresses != pattern, axis=1)\n\n            # Create a boolean mask for hard locations within the write radius\n            write_mask = distances = r_w\n\n            # Update counters for all active locations in a single vectorized operation\n            if np.any(write_mask):\n                counters[write_mask, :] += update_vector\n        \n        # --- Step 3  4: Read Phase and Accuracy Evaluation ---\n        total_correct_bits = 0\n        for p in range(P):\n            pattern_to_recall = patterns[p]\n\n            # Calculate Hamming distances from the cue pattern to all hard locations\n            distances = np.sum(addresses != pattern_to_recall, axis=1)\n\n            # Create a boolean mask for hard locations within the read radius\n            read_mask = distances = r_r\n\n            # Compute the sum vector by summing counters from active locations\n            if np.any(read_mask):\n                sum_vector = np.sum(counters[read_mask, :], axis=0)\n            else:\n                # If no locations are active, sum vector is all zeros\n                sum_vector = np.zeros(N, dtype=np.int32)\n            \n            # Apply threshold to get the recalled binary pattern\n            recalled_pattern = (sum_vector >= theta).astype(np.int8)\n\n            # Count the number of correctly recalled bits for this pattern\n            total_correct_bits += np.sum(recalled_pattern == pattern_to_recall)\n\n        # Calculate the final accuracy for this test case\n        accuracy = total_correct_bits / (P * N)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了抽象的联想记忆模型之后，我们转向一个更具生物学真实性的场景。这项练习要求您模拟一个脉冲神经网络，以研究序列学习的机制，这是情节记忆等功能的关键。您将探索尖峰时间依赖可塑性 (Spike-Timing Dependent Plasticity, STDP) 如何在θ和γ振荡的背景下塑造神经连接，从而揭示大脑中潜在的时间编码策略。",
            "id": "3971152",
            "problem": "您需要设计并实现一个仿真，该仿真模拟一个离散时间的漏电整合发放脉冲网络。此网络具有由抑制性中间神经元介导的 theta 节律，以及驱动兴奋性神经元产生序列脉冲模式的嵌套 gamma 爆发兴奋。序列学习必须由脉冲时间依赖可塑性 (STDP) 控制。核心目标是量化 gamma 爆发相对于持续 theta 抑制的相位时序如何调节已学习的前向序列连接性。\n\n基本原理与定义：\n- 漏电整合发放神经元的膜电位 $v(t)$ 根据以下常微分方程演化\n$$\n\\frac{dv}{dt} = \\frac{1}{\\tau_m}\\left(v_{\\mathrm{rest}} - v(t) + I_{\\mathrm{syn}}(t) + I_{\\mathrm{ext}}(t) + I_{\\mathrm{inh}}(t)\\right),\n$$\n其中 $\\tau_m$ 是膜时间常数，$v_{\\mathrm{rest}}$ 是静息电位，$I_{\\mathrm{syn}}(t)$ 是来自其他兴奋性神经元的突触输入电流，$I_{\\mathrm{ext}}(t)$ 是驱动 gamma 爆发脉冲的确定性外部电流，而 $I_{\\mathrm{inh}}(t)$ 是由 theta 节律形成的抑制性电流。当 $v(t)$ 超过阈值 $v_{\\mathrm{th}}$ 时，会产生一个脉冲，之后 $v$ 被重置为 $v_{\\mathrm{reset}}$，并在一段固定的不应期内阻止进一步的脉冲发放。\n- theta 节律是一种调节网络兴奋性的正弦抑制性电流：\n$$\nI_{\\mathrm{inh}}(t) = -A_{\\mathrm{inh}}\\left(1 + \\sin(2\\pi f_\\theta t)\\right),\n$$\n其中 $A_{\\mathrm{inh}}$ 为抑制幅度，$f_\\theta$ 为 theta 频率。\n- Gamma 爆发被建模为短暂的外部脉冲 $I_{\\mathrm{ext}}(t)$，它们以固定的脉冲间延迟顺序施加于兴奋性神经元，以诱导产生一个脉冲序列。每个 gamma 爆发在一个 theta 周期内的开始时间由一个相对于该 theta 周期开始的相位偏移 $\\phi$（以弧度为单位）控制。\n- 兴奋性突触输入源于脉冲触发且呈指数衰减的突触轨迹。令 $S_{ij}(t)$ 表示从突触前神经元 $i$ 到突触后神经元 $j$ 的突触激活轨迹。每当突触前神经元 $i$ 发放一个脉冲，轨迹 $S_{ij}$ 就会增加，否则它会以突触时间常数 $\\tau_{\\mathrm{syn}}$ 指数衰减。流向神经元 $j$ 的突触后电流为：\n$$\nI_{\\mathrm{syn},j}(t) = g_{\\mathrm{exc}} \\sum_{i \\ne j} W_{ij} S_{ij}(t),\n$$\n其中 $g_{\\mathrm{exc}}$ 是突触增益，$W_{ij}$ 是兴奋性突触权重。\n- 脉冲时间依赖可塑性 (STDP) 根据突触前神经元 $i$ 的脉冲与突触后神经元 $j$ 的脉冲之间的时间关系来更新 $W_{ij}$。使用一个基于配对的规则，该规则通过用于突触前活动的指数衰减轨迹 $x_i(t)$ 和用于突触后活动的指数衰减轨迹 $y_j(t)$ 来实现：\n    - 当突触前神经元 $i$ 发放脉冲时：$x_i \\leftarrow x_i + 1$，并且对于所有 $j \\ne i$，应用长时程抑制 (LTD)：$W_{ij} \\leftarrow W_{ij} - A_- y_j$。\n    - 当突触后神经元 $j$ 发放脉冲时：$y_j \\leftarrow y_j + 1$，并且对于所有 $i \\ne j$，应用长时程增强 (LTP)：$W_{ij} \\leftarrow W_{ij} + A_+ x_i$。\n    - 轨迹呈指数衰减：$\\dot{x}_i = -x_i/\\tau_+$ 且 $\\dot{y}_j = -y_j/\\tau_-$，其中 $\\tau_+$ 和 $\\tau_-$ 是时间常数。权重被限制在 $[0,W_{\\max}]$ 范围内，并且 $W_{ii}=0$。\n- 序列学习度量（前向偏置分数）：对于一个由 $N$ 个兴奋性神经元组成的有序链（索引从 $0$ 到 $N-1$），将训练后的平均前向偏置定义为\n$$\nS = \\frac{1}{N-1} \\sum_{i=0}^{N-2} \\left( W_{i,i+1} - W_{i+1,i} \\right).\n$$\n一个更大的正值 $S$ 表明学习到的前向连接性更强，与诱导的脉冲序列一致。\n\n实现约束与离散化：\n- 对膜电位的更新使用时间步长为 $\\Delta t$ 的显式欧拉离散化：\n$$\nv_{j}(t+\\Delta t) = v_{j}(t) + \\frac{\\Delta t}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_{j}(t) + I_{\\mathrm{syn},j}(t) + I_{\\mathrm{ext},j}(t) + I_{\\mathrm{inh}}(t)\\right),\n$$\n在脉冲发放时，需遵循阈值穿越和不应期重置 $v_{j}\\leftarrow v_{\\mathrm{reset}}$ 的规则。突触轨迹的更新方式为 $S_{ij}(t+\\Delta t) = S_{ij}(t)\\,e^{-\\Delta t/\\tau_{\\mathrm{syn}}}$ 再加上脉冲触发的增量。突触前和突触后的 STDP 轨迹在每个时间步长也同样进行衰减。\n- 仿真必须是确定性的，具有固定的参数且无随机噪声。兴奋性网络的大小为 $N=5$。不允许自连接，即对于所有 $i$，$W_{ii}=0$。\n\ntheta 周期内的 Gamma 爆发生成：\n- 令 theta 周期为 $T_\\theta = 1/f_\\theta$，theta 周期的开始时间为 $t_k = k T_\\theta$，其中 $k$ 为非负整数。\n- 对于每个 theta 周期，安排一个 gamma 爆发，其开始时间为 $t_k + \\phi \\cdot T_\\theta / (2\\pi)$。在该爆发内部，向神经元 $0,1,\\dots,N-1$ 顺序地传递持续时间为 $d_{\\mathrm{pulse}}$、幅度为 $I_{\\mathrm{pulse}}$ 的相同矩形脉冲，脉冲间的延迟为 $\\Delta_{\\gamma}$。因此，神经元 $n$ 在时间区间 $[t_k + \\phi T_\\theta/(2\\pi) + n \\Delta_{\\gamma},\\ t_k + \\phi T_\\theta/(2\\pi) + n \\Delta_{\\gamma} + d_{\\mathrm{pulse}}]$ 内接收其脉冲。\n\n仿真参数和单位：\n- 时间参数：$\\Delta t = 0.0005$ 秒，总仿真时间 $T_{\\mathrm{total}} = 1.0$ 秒，不应期时间 $t_{\\mathrm{ref}} = 0.005$ 秒，膜时间常数 $\\tau_m = 0.02$ 秒，突触时间常数 $\\tau_{\\mathrm{syn}} = 0.005$ 秒，STDP 时间常数 $\\tau_{+} = 0.02$ 秒和 $\\tau_{-} = 0.02$ 秒。\n- Theta 频率：$f_\\theta = 6.0$ 赫兹。\n- 在此模型中，膜电位是无量纲的：$v_{\\mathrm{rest}} = 0.0$，$v_{\\mathrm{th}} = 1.0$，$v_{\\mathrm{reset}} = 0.0$。\n- 突触参数：$g_{\\mathrm{exc}} = 0.05$，初始权重 $W_{ij} = 0.01$（当 $i \\ne j$ 时），$W_{\\max} = 0.2$，STDP 系数 $A_+ = 0.005$，$A_- = 0.006$。\n- 外部脉冲参数：脉冲幅度 $I_{\\mathrm{pulse}} = 1.2$（无量纲电流单位），脉冲持续时间 $d_{\\mathrm{pulse}} = 0.003$ 秒。\n- 抑制幅度 $A_{\\mathrm{inh}}$ 根据不同测试用例而变化，并作为负电流作用；$\\phi$ 的单位是弧度。\n\n所需输出与测试套件：\n- 实现仿真，为每个指定的参数集计算前向偏置分数 $S$。所有输出均为无量纲实数。\n- 用于评估不同 gamma 爆发相位时序和边界条件的测试套件参数集：\n    1. $\\phi = 0.0$ 弧度, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ 秒。\n    2. $\\phi = \\pi/2$ 弧度, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ 秒。\n    3. $\\phi = \\pi$ 弧度, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ 秒。\n    4. $\\phi = 3\\pi/2$ 弧度, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ 秒。\n    5. $\\phi = 3\\pi/2$ 弧度, $A_{\\mathrm{inh}} = 1.6$, $\\Delta_{\\gamma} = 0.005$ 秒。\n    6. $\\phi = 3\\pi/2$ 弧度, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.02$ 秒。\n- 最终输出格式：您的程序应产生单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，“[result1,result2,...]”）。每个结果是对应测试用例的标量前向偏置分数 $S$。\n\n您的实现必须是一个完整、可运行的程序，该程序遵守指定的运行时环境和库约束，不接受任何输入，并精确地打印所述格式的单行内容。角度必须解释为弧度。时间参数必须解释为秒。",
            "solution": "用户提供的问题是计算神经科学领域一个有效、适定且有科学依据的练习。它要求设计并实现一个仿真，以探究 theta 和 gamma 振荡之间的相位关系如何通过脉冲时间依赖可塑性 (STDP) 影响序列学习。该模型基于已确立的生物物理和计算原理。\n\n解决方案涉及开发一个漏电整合发放 (LIF) 神经元网络的离散时间仿真。模型的...核心组件和仿真算法详述如下。\n\n**1. 漏电整合发放 (LIF) 神经元模型**\n网络的基本单元是 LIF 神经元，这是一种标准的神经元动力学简化模型。每个神经元 $j$ 的膜电位 $v_j(t)$ 随时间根据流入细胞的净电流而演化。其动力学由以下微分方程控制：\n$$\n\\frac{dv_j}{dt} = \\frac{1}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_j(t) + I_{\\mathrm{total},j}(t)\\right)\n$$\n其中 $\\tau_m = 0.02$ s 是膜时间常数，$v_{\\mathrm{rest}} = 0.0$ 是静息电位，$I_{\\mathrm{total},j}(t)$ 是所有输入电流的总和。当 $v_j(t)$ 达到阈值 $v_{\\mathrm{th}} = 1.0$ 时，神经元发放一个脉冲，其电位被重置为 $v_{\\mathrm{reset}} = 0.0$，并进入一个 $t_{\\mathrm{ref}} = 0.005$ s 的不应期，在此期间它不能再次发放脉冲。\n\n为了便于计算实现，我们使用时间步长为 $\\Delta t = 0.0005$ s 的显式欧拉方法将该方程离散化：\n$$\nv_{j}(t+\\Delta t) = v_{j}(t) + \\frac{\\Delta t}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_{j}(t) + I_{\\mathrm{total},j}(t)\\right)\n$$\n总电流 $I_{\\mathrm{total},j}(t)$ 是三个分量的总和：$I_{\\mathrm{total},j}(t) = I_{\\mathrm{syn},j}(t) + I_{\\mathrm{ext},j}(t) + I_{\\mathrm{inh}}(t)$。\n\n**2. 节律性与外部输入**\n网络的活动由两个关键的外部电流协调：\n\n- **Theta 节律性抑制 ($I_{\\mathrm{inh}}$)：**一个全局性的抑制电流调节网络中所有神经元的兴奋性，模仿抑制性中间神经元群体的效果。该电流遵循一个正弦模式：\n  $$\n  I_{\\mathrm{inh}}(t) = -A_{\\mathrm{inh}}\\left(1 + \\sin(2\\pi f_\\theta t)\\right)\n  $$\n  此处，$f_\\theta = 6.0$ Hz 是 theta 频率，$A_{\\mathrm{inh}}$ 是抑制幅度，它在不同测试用例中有所不同。该电流创造了高兴奋性和低兴奋性的周期性窗口。正弦函数的相位决定了抑制水平：在相位为 $3\\pi/2, 7\\pi/2, \\dots$ 时出现最小抑制 ($I_{\\mathrm{inh}} = 0$)，在相位为 $\\pi/2, 5\\pi/2, \\dots$ 时出现最大抑制 ($I_{\\mathrm{inh}} = -2A_{\\mathrm{inh}}$)。\n\n- **Gamma 爆发兴奋 ($I_{\\mathrm{ext}}$)：**为了诱导一个序列发放模式，向神经元施加一个外部兴奋性电流的“gamma 爆发”。一次爆发包含 $N=5$ 个短暂的矩形脉冲，其幅度为 $I_{\\mathrm{pulse}} = 1.2$，持续时间为 $d_{\\mathrm{pulse}} = 0.003$ s。这些脉冲以 $\\Delta_{\\gamma}$ 的脉冲间延迟顺序传递给神经元 $0, 1, \\dots, N-1$。关键的实验参数是相位偏移 $\\phi$，它决定了整个爆发在每个 theta 周期内的开始时间。对于 theta 周期 $k$（从 $t_k = k/f_\\theta$ 开始），爆发在 $t_k + \\phi/(2\\pi f_\\theta)$ 开始，从而将诱导序列的驱动精确地锁相到底层的 theta 节律上。\n\n**3. 突触传递与可塑性 (STDP)**\n$N=5$ 个兴奋性神经元之间的连接是可塑的，这意味着它们的强度（权重 $W_{ij}$）会根据神经活动而演化。\n\n- **突触电流 ($I_{\\mathrm{syn}}$)：**当一个突触前神经元 $i$ 发放脉冲时，它会对突触后神经元 $j$ 的输入电流产生贡献。这通过一个突触轨迹 $S_{ij}(t)$ 来建模，当神经元 $i$ 发放脉冲时，该轨迹增加 1.0，否则以时间常数 $\\tau_{\\mathrm{syn}} = 0.005$ s 指数衰减。流向神经元 $j$ 的总突触电流是所有突触前输入的加权和：\n  $$\n  I_{\\mathrm{syn},j}(t) = g_{\\mathrm{exc}} \\sum_{i \\ne j} W_{ij} S_{ij}(t)\n  $$\n  其中 $g_{\\mathrm{exc}} = 0.05$ 是一个恒定的突触增益。\n\n- **脉冲时间依赖可塑性 (STDP)：**权重 $W_{ij}$ 根据一个基于配对的 STDP 规则进行更新，该规则根据突触前和突触后脉冲的相对时间来增强或减弱突触。这是通过为每个神经元使用两个额外的轨迹来实现的：一个突触前轨迹 $x_i(t)$ 和一个突触后轨迹 $y_j(t)$。两者都以时间常数 $\\tau_{+} = \\tau_{-} = 0.02$ s 指数衰减。\n    - 当突触前神经元 $i$ 发放脉冲时，其轨迹 $x_i$ 增加。此事件会为其输出突触触发长时程抑制 (LTD)：$W_{ij} \\leftarrow W_{ij} - A_- y_j$，其中 $A_- = 0.006$，$y_j$ 是神经元 $j$ 的突触后轨迹的当前值。\n    - 当突触后神经元 $j$ 发放脉冲时，其轨迹 $y_j$ 增加。此事件会为其输入突触触发长时程增强 (LTP)：$W_{ij} \\leftarrow W_{ij} + A_+ x_i$，其中 $A_+ = 0.005$，$x_i$ 是神经元 $i$ 的突触前轨迹的当前值。\n  在我们的离散仿真中，如果一个神经元发放脉冲，其输入连接的 LTP 和输出连接的 LTD 都会根据脉冲前一刻的轨迹值进行计算。然后，该神经元自身的 $x$ 和 $y$ 轨迹会增加。权重被裁剪到 $[0, W_{\\max}]$ 范围内，其中 $W_{\\max} = 0.2$。\n\n**4. 仿真算法与度量**\n仿真通过从 $t=0$ 到 $T_{\\mathrm{total}}=1.0$ s 的时间迭代进行。在每个时间步长 $\\Delta t$：\n1. 通过将 theta 抑制、外部 gamma 脉冲（如果有）以及来自其他神经元的突触电流相加，计算每个神经元的总电流。\n2. 所有非不应期神经元的膜电位通过欧拉方法进行更新。\n3. 电位超过 $v_{\\mathrm{th}}$ 的神经元被识别为正在发放脉冲。\n4. 对于每个发放脉冲的神经元，应用 STDP 权重更新（LTP 和 LTD），重置其电位，并启动其不应期计数器。相应的突触和 STDP 轨迹会增加。\n5. 所有轨迹变量（$S_{ij}$, $x_i$, $y_j$）都进行指数衰减。不应期计数器递减。\n在整个仿真持续时间内重复此过程。\n\n仿真结束后，使用最终的连接矩阵 $W$ 来计算**前向偏置分数** $S$：\n$$\nS = \\frac{1}{N-1} \\sum_{i=0}^{N-2} \\left( W_{i,i+1} - W_{i+1,i} \\right)\n$$\n该度量通过比较期望方向（$W_{i,i+1}$）与反向（$W_{i+1,i}$）的连接强度，量化了网络学习前向序列（$0 \\to 1 \\to \\dots \\to 4$）的成功程度。一个更大的正分数值表示更强、更准确的序列学习。最终答案中提供的 Python 实现会为问题中指定的每个测试用例执行这个完整的仿真。",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(phi, A_inh, delta_gamma):\n    \"\"\"\n    Runs a single simulation of the spiking network with a given parameter set.\n    \"\"\"\n    # 1. PARAMETERS\n    # Time parameters\n    DT = 0.0005  # s\n    T_TOTAL = 1.0  # s\n    T_REF = 0.005  # s\n    TAU_M = 0.02  # s\n    TAU_SYN = 0.005  # s\n    TAU_PLUS = 0.02  # s\n    TAU_MINUS = 0.02  # s\n\n    # Network and neuron parameters\n    N = 5\n    V_REST = 0.0\n    V_TH = 1.0\n    V_RESET = 0.0\n    G_EXC = 0.05\n    W_MAX = 0.2\n    A_PLUS = 0.005\n    A_MINUS = 0.006\n\n    # Input parameters\n    F_THETA = 6.0\n    I_PULSE = 1.2\n    D_PULSE = 0.003\n    T_THETA = 1.0 / F_THETA\n\n    # Discretized and pre-calculated values\n    NUM_STEPS = int(T_TOTAL / DT)\n    REFRACTORY_STEPS = int(T_REF / DT)\n    EXP_DECAY_SYN = np.exp(-DT / TAU_SYN)\n    EXP_DECAY_PLUS = np.exp(-DT / TAU_PLUS)\n    EXP_DECAY_MINUS = np.exp(-DT / TAU_MINUS)\n\n    # 2. STATE VARIABLES INITIALIZATION\n    v = np.full(N, V_REST, dtype=np.float64)\n    W = np.full((N, N), 0.01, dtype=np.float64)\n    np.fill_diagonal(W, 0)\n    \n    S_trace = np.zeros((N, N), dtype=np.float64)\n    x_trace = np.zeros(N, dtype=np.float64)\n    y_trace = np.zeros(N, dtype=np.float64)\n    refractory_counters = np.zeros(N, dtype=np.int32)\n    \n    # 3. SIMULATION LOOP\n    for step in range(NUM_STEPS):\n        t = step * DT\n\n        # Calculate currents\n        # Inhibitory current\n        I_inh = -A_inh * (1.0 + np.sin(2.0 * np.pi * F_THETA * t))\n\n        # External pulse current\n        I_ext = np.zeros(N, dtype=np.float64)\n        k_cycle = np.floor(t / T_THETA)\n        t_burst_start = k_cycle * T_THETA + phi * T_THETA / (2.0 * np.pi)\n        \n        for n in range(N):\n            t_pulse_start = t_burst_start + n * delta_gamma\n            t_pulse_end = t_pulse_start + D_PULSE\n            if t_pulse_start = t  t_pulse_end:\n                I_ext[n] = I_PULSE\n\n        # Synaptic current\n        I_syn = G_EXC * np.sum(W * S_trace, axis=0) # Sum over presynaptic neurons (axis=0)\n\n        # Total current\n        I_total = I_syn + I_ext + I_inh\n\n        # Update membrane potential for non-refractory neurons\n        non_refractory_mask = (refractory_counters == 0)\n        v[non_refractory_mask] += (DT / TAU_M) * (V_REST - v[non_refractory_mask] + I_total[non_refractory_mask])\n\n        # Check for spikes\n        spiked_neurons = np.where((v >= V_TH)  non_refractory_mask)[0]\n\n        if spiked_neurons.size > 0:\n            x_trace_old = x_trace.copy()\n            y_trace_old = y_trace.copy()\n\n            for s_idx in spiked_neurons:\n                # LTP for incoming synapses\n                W[:, s_idx] += A_PLUS * x_trace_old\n                \n                # LTD for outgoing synapses\n                W[s_idx, :] -= A_MINUS * y_trace_old\n            \n            # Clip weights and zero the diagonal\n            W = np.clip(W, 0, W_MAX)\n            np.fill_diagonal(W, 0)\n\n            for s_idx in spiked_neurons:\n                # Update traces and reset state for spiking neurons\n                x_trace[s_idx] += 1.0\n                y_trace[s_idx] += 1.0\n                S_trace[s_idx, :] += 1.0 # Spike from s_idx affects traces to all other neurons\n                \n                v[s_idx] = V_RESET\n                refractory_counters[s_idx] = REFRACTORY_STEPS\n\n        # Decay traces\n        S_trace *= EXP_DECAY_SYN\n        x_trace *= EXP_DECAY_PLUS\n        y_trace *= EXP_DECAY_MINUS\n        \n        # Update refractory counters\n        refractory_counters[refractory_counters > 0] -= 1\n\n    # 4. CALCULATE FORWARD-BIAS SCORE\n    forward_sum = np.sum(np.diag(W, k=1))  # W[i, i+1]\n    backward_sum = np.sum(np.diag(W, k=-1)) # W[i+1, i]\n    \n    score = (forward_sum - backward_sum) / (N - 1)\n    \n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (phi, A_inh, delta_gamma)\n        (0.0, 0.8, 0.005),\n        (np.pi / 2, 0.8, 0.005),\n        (np.pi, 0.8, 0.005),\n        (3 * np.pi / 2, 0.8, 0.005),\n        (3 * np.pi / 2, 1.6, 0.005),\n        (3 * np.pi / 2, 0.8, 0.020),\n    ]\n\n    results = []\n    for case in test_cases:\n        phi, A_inh, delta_gamma = case\n        result = run_simulation(phi, A_inh, delta_gamma)\n        results.append(result)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "计算建模的强大之处不仅在于详细的模拟，还在于能够抽象出核心原理。这项练习将重点从模拟转向分析，探讨工作记忆的保真度问题。您将学习如何将一个具有随机突触输入的循环神经网络简化为一个Ornstein-Uhlenbeck (OU) 过程，并利用信息论工具来量化记忆随时间衰退的速率，从而连接起底层神经动力学和高层认知功能。",
            "id": "3971126",
            "problem": "考虑一个标量工作记忆变量 $X_t$，它代表一个循环神经网络中维持的值。在其稳定工作点 $X^{\\star}$ 附近，该电路可以被线性化，因此编码 $X_t$ 的群体平均活动遵循一个带有循环稳定和突触散粒噪声的漏积分器模型，\n$$\nC \\,\\frac{dX_t}{dt} \\;=\\; -g_L\\,X_t \\;+\\; g_R\\,(X^{\\star}-X_t) \\;+\\; J(t),\n$$\n其中 $C$ 是群体平均状态的有效电容（或惯性），$g_L0$ 是有效泄漏，$g_R0$ 是有效循环稳定增益，$J(t)$ 是总突触输入电流。假设 $J(t)$ 是 $N$ 个独立突触过程的总和，每个过程由速率为 $r$ 的齐次泊松脉冲序列驱动，并产生幅度为 $w$、指数核为 $h(t) = \\tau_s^{-1} \\exp(-t/\\tau_s)\\,u(t)$ 的突触后电流，其中 $u(t)$ 是单位阶跃函数且 $\\int_{0}^{\\infty} h(t)\\,dt = 1$。$X_t$ 所维持的值从群体活动中线性读出，并且存储的目标 $X^{\\star}$ 在我们关心的时间段内是恒定的。假设时间尺度分离的情况，即 $\\tau_s \\ll C/(g_L+g_R)$，因此在 $X_t$ 的层面上，有色突触涨落可以被具有相同零频功率的高斯白噪声所近似。\n\n1) 从这些假设出发，并且仅使用线性系统、泊松散粒噪声和滤波泊松过程的高斯近似的基本定义，推导 $X_t$ 的有效 Ornstein–Uhlenbeck (OU) 随机微分方程，其形式为\n$$\ndX_t \\;=\\; -\\lambda\\,(X_t-\\mu)\\,dt \\;+\\; \\sigma\\, dW_t,\n$$\n并用 $C$、$g_L$、$g_R$、$N$、$r$ 和 $w$ 来表示漂移率 $\\lambda$、不动点均值 $\\mu$ 和扩散幅度 $\\sigma$。\n\n2) 将延迟 $t$ 时的记忆保真度定义为单次观测 $X_t$ 所包含的关于初始状态 $X_0$ 的费雪信息 (FI)，并将 OU 参数视为已知。仅使用高斯信道和 OU 过程的基本性质，推导该 FI 的闭合形式解析表达式，记为 $F(t)$，它是 $t$ 和模型参数的函数。\n\n给出 $F(t)$ 的闭合形式表达式作为你的最终答案。最终答案中不要提供中间步骤。最终答案中无需包含单位。如果选择简化，请提供精确的符号简化而非十进制近似值。",
            "solution": "该问题分为两部分。第一部分是推导近似给定神经环路动力学的 Ornstein–Uhlenbeck (OU) 过程的参数。第二部分是基于稍后时间的观测来计算关于初始状态的费雪信息 (FI)。\n\n### 第一部分：Ornstein-Uhlenbeck 方程的推导\n\n我们已知工作记忆变量 $X_t$ 的动力学方程：\n$$C \\,\\frac{dX_t}{dt} \\;=\\; -g_L\\,X_t \\;+\\; g_R\\,(X^{\\star}-X_t) \\;+\\; J(t)$$\n其中 $C$ 是电容，$g_L$ 和 $g_R$ 是电导，$X^{\\star}$ 是一个恒定的目标值，$J(t)$ 是一个随机的突触输入电流。\n\n首先，我们重排方程，将包含 $X_t$ 的项组合在一起：\n$$C \\,\\frac{dX_t}{dt} \\;=\\; -(g_L+g_R)\\,X_t \\;+\\; g_R\\,X^{\\star} \\;+\\; J(t)$$\n两边同除以 $C$，我们得到：\n$$\\frac{dX_t}{dt} \\;=\\; -\\frac{g_L+g_R}{C}\\,X_t \\;+\\; \\frac{g_R\\,X^{\\star}}{C} \\;+\\; \\frac{1}{C}\\,J(t)$$\n随机输入 $J(t)$ 是 $N$ 个独立同分布的突触过程之和，$J(t) = \\sum_{i=1}^{N} J_i(t)$。每个 $J_i(t)$ 是一个滤波后的泊松过程（散粒噪声），其中速率为 $r$ 的脉冲序列与核函数 $h(t) = \\tau_s^{-1} \\exp(-t/\\tau_s)\\,u(t)$ 进行卷积，并按幅度 $w$ 进行缩放。\n\n我们可以将 $J(t)$ 分解为其均值和一个零均值的涨落部分：$J(t) = \\mathbb{E}[J(t)] + \\xi_J(t)$。\n根据散粒噪声的 Campbell 定理，单个过程 $J_i(t)$ 的均值为：\n$$\\mathbb{E}[J_i(t)] \\;=\\; r \\int_{-\\infty}^{\\infty} w\\,h(t')\\,dt' \\;=\\; r\\,w \\int_{0}^{\\infty} \\tau_s^{-1} \\exp(-t'/\\tau_s)\\,dt' \\;=\\; r\\,w$$\n由于这 $N$ 个过程是独立的，总电流的均值为：\n$$\\mathbb{E}[J(t)] \\;=\\; \\sum_{i=1}^{N} \\mathbb{E}[J_i(t)] \\;=\\; N\\,r\\,w$$\n将此代入我们的动力学方程：\n$$\\frac{dX_t}{dt} \\;=\\; -\\frac{g_L+g_R}{C}\\,X_t \\;+\\; \\frac{g_R\\,X^{\\star}}{C} \\;+\\; \\frac{N\\,r\\,w}{C} \\;+\\; \\frac{1}{C}\\,\\xi_J(t)$$\n我们可以将常数项组合起来，并将方程改写为突出不动点的形式：\n$$\\frac{dX_t}{dt} \\;=\\; -\\frac{g_L+g_R}{C} \\left( X_t - \\frac{g_R\\,X^{\\star} + N\\,r\\,w}{g_L+g_R} \\right) \\;+\\; \\frac{1}{C}\\,\\xi_J(t)$$\n该方程的形式为 $\\frac{dX_t}{dt} = -\\lambda(X_t - \\mu) + \\eta(t)$，其中 $\\eta(t) = \\frac{1}{C}\\xi_J(t)$ 是有效噪声。\n由此，我们可以确定漂移率 $\\lambda$ 和均值不动点 $\\mu$：\n$$\\lambda \\;=\\; \\frac{g_L+g_R}{C}$$\n$$\\mu \\;=\\; \\frac{g_R\\,X^{\\star} + N\\,r\\,w}{g_L+g_R}$$\n问题要求将有色噪声项 $\\eta(t)$ 近似为高斯白噪声，这相当于将随机微分方程写成 Ito 形式：\n$$dX_t \\;=\\; -\\lambda\\,(X_t-\\mu)\\,dt \\;+\\; \\sigma\\, dW_t$$\n为了求出 $\\sigma$，我们必须将白噪声过程的功率与有色噪声 $\\eta(t)$ 的低频功率谱密度 (PSD) 相匹配。白噪声过程 $\\sigma\\,dW_t/dt$ 的 PSD 就是 $\\sigma^2$。$\\eta(t)$ 的 PSD 是 $S_\\eta(\\omega) = \\frac{1}{C^2} S_{\\xi_J}(\\omega)$。\n\n一个滤波泊松过程的涨落部分 $\\xi_{J,i}(t)$ 的 PSD 由 $S_{\\xi_{J,i}}(\\omega) = r\\,w^2\\,|\\tilde{h}(\\omega)|^2$ 给出，其中 $\\tilde{h}(\\omega)$ 是核函数 $h(t)$ 的傅里叶变换。\n$$\\tilde{h}(\\omega) \\;=\\; \\int_0^\\infty \\frac{1}{\\tau_s} \\exp(-t/\\tau_s) \\exp(-i\\omega t) \\,dt \\;=\\; \\frac{1}{\\tau_s} \\frac{1}{1/\\tau_s + i\\omega} \\;=\\; \\frac{1}{1+i\\omega\\tau_s}$$\n其幅度的平方为 $|\\tilde{h}(\\omega)|^2 = \\frac{1}{1+(\\omega\\tau_s)^2}$。\n由于这 $N$ 个突触输入是独立的，它们的 PSD 相加：\n$$S_{\\xi_J}(\\omega) \\;=\\; \\sum_{i=1}^N S_{\\xi_{J,i}}(\\omega) \\;=\\; \\frac{N\\,r\\,w^2}{1+(\\omega\\tau_s)^2}$$\n白噪声近似在条件 $\\tau_s \\ll C/(g_L+g_R)$（即 $\\tau_s \\ll 1/\\lambda$）下有效。这意味着噪声相关时间 $\\tau_s$ 远小于系统的时间常数 $1/\\lambda$。在这种情况下，我们用噪声 PSD 在零频率处的值 $S_{\\xi_J}(0)$ 来近似它。\n$$S_{\\xi_J}(0) \\;=\\; N\\,r\\,w^2$$\n有效噪声 $\\eta(t) = \\frac{1}{C}\\xi_J(t)$ 在零频率处的 PSD 是：\n$$S_\\eta(0) \\;=\\; \\frac{1}{C^2} S_{\\xi_J}(0) \\;=\\; \\frac{N\\,r\\,w^2}{C^2}$$\n将其与白噪声的 PSD $\\sigma^2$ 相等，我们求出扩散幅度 $\\sigma$：\n$$\\sigma^2 \\;=\\; \\frac{N\\,r\\,w^2}{C^2} \\quad\\implies\\quad \\sigma \\;=\\; \\frac{w}{C}\\sqrt{N\\,r}$$\n\n### 第二部分：费雪信息的推导\n\n我们有 $X_t$ 的 OU 随机微分方程：\n$$dX_t \\;=\\; -\\lambda\\,(X_t-\\mu)\\,dt \\;+\\; \\sigma\\, dW_t$$\n给定 $t=0$ 时的初始状态 $X_0$，该随机微分方程的解为：\n$$X_t \\;=\\; \\mu + (X_0 - \\mu)e^{-\\lambda t} + \\sigma \\int_0^t e^{-\\lambda(t-s)}dW_s$$\n这个解表明，对于给定的 $X_0$，$X_t$ 是一个高斯随机变量。我们需要求出其条件均值和方差。Ito 积分的期望为零。\n$$\\mathbb{E}[X_t | X_0] \\;=\\; \\mu + (X_0 - \\mu)e^{-\\lambda t}$$\n方差由 Ito 等距性质给出：\n$$\\text{Var}(X_t | X_0) \\;=\\; \\mathbb{E}\\left[\\left(\\sigma \\int_0^t e^{-\\lambda(t-s)}dW_s\\right)^2\\right] \\;=\\; \\sigma^2 \\int_0^t e^{-2\\lambda(t-s)}ds$$\n$$\\text{Var}(X_t | X_0) \\;=\\; \\sigma^2 \\left[\\frac{e^{-2\\lambda(t-s)}}{2\\lambda}\\right]_0^t \\;=\\; \\frac{\\sigma^2}{2\\lambda}(1-e^{-2\\lambda t})$$\n我们记条件均值为 $\\mu_t(X_0) = \\mathbb{E}[X_t | X_0]$，条件方差为 $V_t = \\text{Var}(X_t | X_0)$。\n从观测值 $X_t$ 中得到的关于参数 $X_0$ 的费雪信息 (FI) $F(t)$ 由以下公式给出：\n$$F(t) \\;=\\; \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial X_0} \\ln p(X_t|X_0)\\right)^2\\right]$$\n对于高斯分布 $p(x|\\theta) = \\mathcal{N}(\\mu(\\theta), V)$，其中方差 $V$ 与参数 $\\theta$ 无关，FI 简化为：\n$$F(\\theta) \\;=\\; \\frac{1}{V}\\left(\\frac{\\partial \\mu(\\theta)}{\\partial \\theta}\\right)^2$$\n在我们的例子中，$\\theta=X_0$，$\\mu(\\theta)=\\mu_t(X_0)$，且 $V=V_t$。首先，我们计算均值对 $X_0$ 的导数：\n$$\\frac{\\partial \\mu_t(X_0)}{\\partial X_0} \\;=\\; \\frac{\\partial}{\\partial X_0} \\left( \\mu + (X_0 - \\mu)e^{-\\lambda t} \\right) \\;=\\; e^{-\\lambda t}$$\n现在，我们将它和方差 $V_t$ 代入 FI 公式：\n$$F(t) \\;=\\; \\frac{(e^{-\\lambda t})^2}{V_t} \\;=\\; \\frac{e^{-2\\lambda t}}{\\frac{\\sigma^2}{2\\lambda}(1-e^{-2\\lambda t})}$$\n$$F(t) \\;=\\; \\frac{2\\lambda e^{-2\\lambda t}}{\\sigma^2 (1-e^{-2\\lambda t})}$$\n这可以重写为：\n$$F(t) \\;=\\; \\frac{2\\lambda}{\\sigma^2} \\frac{1}{e^{2\\lambda t} - 1}$$\n最后，我们将第一部分中推导出的 $\\lambda$ 和 $\\sigma^2$ 的表达式（用原始模型参数表示）代入：\n$$\\lambda = \\frac{g_L+g_R}{C} \\quad \\text{和} \\quad \\sigma^2 = \\frac{N\\,r\\,w^2}{C^2}$$\n前置因子变为：\n$$\\frac{2\\lambda}{\\sigma^2} \\;=\\; \\frac{2(g_L+g_R)/C}{N\\,r\\,w^2/C^2} \\;=\\; \\frac{2(g_L+g_R)}{C} \\frac{C^2}{N\\,r\\,w^2} \\;=\\; \\frac{2C(g_L+g_R)}{N\\,r\\,w^2}$$\n指数项是 $2\\lambda t = 2\\frac{g_L+g_R}{C}t$。\n将这些代入 $F(t)$ 的表达式，即可得到最终答案。\n$$F(t) \\;=\\; \\frac{2C(g_L+g_R)}{N\\,r\\,w^2} \\frac{1}{\\exp\\left(2\\frac{g_L+g_R}{C}t\\right) - 1}$$",
            "answer": "$$\\boxed{\\frac{2C(g_L+g_R)}{Nrw^2} \\frac{1}{\\exp\\left(2t\\frac{g_L+g_R}{C}\\right) - 1}}$$"
        }
    ]
}