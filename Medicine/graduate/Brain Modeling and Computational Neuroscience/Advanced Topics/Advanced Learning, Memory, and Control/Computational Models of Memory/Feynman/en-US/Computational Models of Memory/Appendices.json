{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration, we will implement a Sparse Distributed Memory (SDM), a canonical model of content-addressable associative memory. The core principle of SDM is that information is stored in a distributed manner across a large memory space, where retrieval is based on similarity (measured by Hamming distance) rather than an exact address. By building this model from the ground up, you will gain hands-on experience with the concepts of distributed representation and the critical trade-offs between storage density, noise, and recall fidelity that govern memory systems. ",
            "id": "3971099",
            "problem": "Consider implementing a toy Sparse Distributed Memory (SDM), a well-known associative memory model used in brain modeling and computational neuroscience. The SDM consists of a set of fixed binary addresses and corresponding per-bit integer counters. Storage and retrieval are defined by proximity in binary Hamming space. The following fundamental bases are assumed:\n\n- Binary patterns are represented as vectors $x \\in \\{0,1\\}^N$ of fixed length $N$.\n- The Hamming distance between two binary vectors $x$ and $y$ is $d_H(x,y) = \\sum_{k=1}^{N} \\mathbf{1}[x_k \\ne y_k]$.\n- Distributed storage is modeled by additive superposition of integer counters; when writing a pattern, counters are incremented by $+1$ where bits are $1$ and decremented by $-1$ where bits are $0$ at memory locations sufficiently close to the pattern.\n- Retrieval sums counters from sufficiently close locations and applies a threshold decision rule to reconstruct a binary vector.\n\nYou must implement a complete program that constructs an SDM with $N=256$ bits. Let there be $M$ hard locations (addresses), each an independent random vector sampled uniformly from $\\{0,1\\}^N$. Each hard location stores a vector of $N$ integer counters initialized to zero. The program must:\n\n1. Generate $P$ random binary patterns $x^{(1)},\\dots,x^{(P)}$, each in $\\{0,1\\}^{N}$, independently and uniformly.\n2. For a given write radius $r_w$, write each pattern $x^{(p)}$ into all hard locations $a_i$ whose Hamming distance satisfies $d_H(a_i, x^{(p)}) \\le r_w$ by updating counters according to the rule:\n   - For each bit index $k \\in \\{1,\\dots,N\\}$: if $x^{(p)}_k = 1$, add $+1$ to counter $c_{i,k}$; if $x^{(p)}_k = 0$, add $-1$ to counter $c_{i,k}$.\n3. For a given read radius $r_r$ and threshold $\\theta$, recall each stored pattern $x^{(p)}$ by:\n   - Selecting all hard locations $a_i$ with $d_H(a_i, x^{(p)}) \\le r_r$.\n   - Computing the per-bit sum $s_k = \\sum_{i: d_H(a_i, x^{(p)}) \\le r_r} c_{i,k}$ for $k \\in \\{1,\\dots,N\\}$.\n   - Producing a recalled binary vector $\\hat{x}^{(p)}$ via the threshold rule: $\\hat{x}^{(p)}_k = 1$ if $s_k \\ge \\theta$, and $\\hat{x}^{(p)}_k = 0$ otherwise.\n4. Evaluate recall accuracy as the fraction of correctly recalled bits over all stored patterns:\n   $$\\text{accuracy} = \\frac{1}{P N} \\sum_{p=1}^{P} \\sum_{k=1}^{N} \\mathbf{1}[\\hat{x}^{(p)}_k = x^{(p)}_k].$$\n   The accuracy must be reported as a decimal in $[0,1]$.\n\nUse the same set of hard locations and the same set of patterns across all test cases to ensure comparability. Use a fixed random seed to make the results reproducible.\n\nFor scientific realism and coverage, adopt the following fixed parameters:\n- Number of bits: $N=256$.\n- Number of hard locations: $M=2048$.\n- Number of patterns: $P=200$.\n- Random seed for both addresses and patterns: $s=12345$.\n\nVary the write density by changing the write radius $r_w$ (which affects the expected fraction of hard locations updated per write), and vary the read threshold $\\theta$. Use the read radius $r_r = r_w$ in all test cases. Implement the program to evaluate the accuracy on the following test suite:\n\n- Test case $1$: $(r_w, \\theta) = (104, 0)$, low write density (approximately three standard deviations below the mean Hamming distance).\n- Test case $2$: $(r_w, \\theta) = (120, 0)$, moderate write density (approximately one standard deviation below the mean).\n- Test case $3$: $(r_w, \\theta) = (120, 8)$, same density as Test case $2$ but with a positive threshold to reduce spurious recall.\n- Test case $4$: $(r_w, \\theta) = (136, 0)$, high write density (approximately one standard deviation above the mean).\n- Test case $5$: $(r_w, \\theta) = (256, 0)$, maximal write density (all locations updated and read).\n- Test case $6$: $(r_w, \\theta) = (0, 0)$, boundary case where only exact-address matches would be updated or read.\n\nAll distances are Hamming distances; all thresholds are integers; all accuracies must be decimals. The final program must generate the accuracies for all test cases and output them as a single line containing a comma-separated list enclosed in square brackets, for example $[a_1,a_2,\\dots,a_6]$, where each $a_i$ is the accuracy for Test case $i$ computed as defined above. There are no physical units involved; angles are not present; percentages must be expressed as decimals without a percentage sign.",
            "solution": "The user has provided a well-defined computational problem to implement a Sparse Distributed Memory (SDM) model. The problem is scientifically grounded, formally specified, and computationally tractable. All necessary parameters and evaluation metrics are explicitly stated, rendering the problem valid for a full solution. The core of the task is to simulate the storage and retrieval of binary patterns in an SDM and to evaluate its recall accuracy under various parameter settings.\n\nThe solution proceeds by first generating the static components of the memory: a set of $M$ fixed, random binary addresses and a set of $P$ random binary patterns to be stored. Then, for each test case defined by a write radius $r_w$ and a threshold $\\theta$, the simulation performs a write phase and a read phase.\n\n### Mathematical and Algorithmic Framework\n\n**1. Data Representation and Generation**\n\n- All data, including hard location addresses and memory patterns, are represented as binary vectors of a fixed length $N = 256$. A vector $x$ is an element of the space $\\{0,1\\}^{N}$.\n- The SDM is defined by $M = 2048$ hard locations, $\\{a_i\\}_{i=1}^{M}$, where each $a_i \\in \\{0,1\\}^{N}$ is a vector whose components are sampled independently and uniformly from $\\{0,1\\}$.\n- A set of $P = 200$ patterns, $\\{x^{(p)}\\}_{p=1}^{P}$, are generated similarly, with each $x^{(p)} \\in \\{0,1\\}^{N}$ being a random binary vector.\n- To ensure reproducibility, all random elements are generated using a fixed random seed, $s = 12345$.\n- The memory itself consists of a matrix of counters, $C$, of size $M \\times N$. Each element $c_{i,k}$ is an integer counter corresponding to the $k$-th bit of the $i$-th hard location. All counters are initialized to $0$.\n\n**2. Distance Metric**\n\nThe interaction between patterns and hard locations is governed by the Hamming distance, $d_H$. For two vectors $x, y \\in \\{0,1\\}^{N}$, the distance is the number of positions at which their corresponding bits differ:\n$$d_H(x,y) = \\sum_{k=1}^{N} \\mathbf{1}[x_k \\ne y_k]$$\nwhere $\\mathbf{1}[\\cdot]$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise.\n\n**3. Write Operation (Storage)**\n\nThe storage of the $P$ patterns is performed iteratively. For each pattern $x^{(p)}$ ($p \\in \\{1, \\dots, P\\}$), the following steps are executed:\n- An \"update vector\" $u^{(p)} \\in \\{-1, 1\\}^{N}$ is derived from the pattern $x^{(p)}$: $u^{(p)}_k = 2x^{(p)}_k - 1$. This maps a bit value of $1$ to $+1$ and a bit value of $0$ to $-1$.\n- For each hard location $a_i$ ($i \\in \\{1, \\dots, M\\}$), the Hamming distance $d_H(a_i, x^{(p)})$ is computed.\n- If $d_H(a_i, x^{(p)}) \\le r_w$, where $r_w$ is the specified write radius, the corresponding row of counters $c_{i, \\cdot}$ is updated by adding the update vector:\n  $$c_{i,k} \\leftarrow c_{i,k} + u^{(p)}_k \\quad \\text{for all } k \\in \\{1, \\dots, N\\}$$\nThis process is repeated for all $P$ patterns, with the updates being cumulative (superposition).\n\n**4. Read Operation (Retrieval)**\n\nTo evaluate the memory's fidelity, we attempt to recall each of the original patterns. For each pattern $x^{(p)}$ serving as a retrieval cue:\n- The set of active hard locations for reading is identified. These are all locations $a_i$ such that $d_H(a_i, x^{(p)}) \\le r_r$, where $r_r$ is the read radius. In this problem, we are given $r_r = r_w$.\n- A sum vector $s \\in \\mathbb{Z}^{N}$ is computed by summing the counter vectors from all active locations:\n  $$s_k = \\sum_{i : d_H(a_i, x^{(p)}) \\le r_r} c_{i,k} \\quad \\text{for all } k \\in \\{1, \\dots, N\\}$$\n  If no locations are within the read radius, the sum vector is a zero vector.\n- The retrieved pattern, $\\hat{x}^{(p)} \\in \\{0,1\\}^{N}$, is constructed by applying a threshold rule to the sum vector:\n  $$\\hat{x}^{(p)}_k = \\begin{cases} 1 & \\text{if } s_k \\ge \\theta \\\\ 0 & \\text{if } s_k < \\theta \\end{cases}$$\n  where $\\theta$ is the given integer threshold.\n\n**5. Accuracy Evaluation**\n\nThe performance is measured by the overall accuracy of recall across all patterns. Accuracy is defined as the fraction of correctly retrieved bits:\n$$\\text{accuracy} = \\frac{1}{P \\cdot N} \\sum_{p=1}^{P} \\sum_{k=1}^{N} \\mathbf{1}[\\hat{x}^{(p)}_k = x^{(p)}_k]$$\nThe total number of bits is $P \\cdot N = 200 \\cdot 256 = 51200$.\n\n### Implementation Strategy\n\nThe algorithm is implemented in Python using the NumPy library for efficient numerical computation.\n- **Initialization**: A single `numpy.random.default_rng` instance is created with the seed $s = 12345$. This generator is used to create the `addresses` array (shape `(2048, 256)`) and the `patterns` array (shape `(200, 256)`), both of type `numpy.int8` to conserve memory.\n- **Main Loop**: The program iterates through the list of test cases `(r_w, \\theta)`. For each case, it re-initializes a `counters` matrix of shape `(2048, 256)` with `numpy.int32` datatype to prevent overflow during summation.\n- **Vectorization**: Core operations are vectorized to enhance performance.\n    - **Hamming Distance**: The distance from a single pattern `p` (shape `(256,)`) to all `M` addresses (shape `(2048, 256)`) is computed efficiently using broadcasting: `numpy.sum(addresses != p, axis=1)`. This yields an array of $M$ distances.\n    - **Counter Updates**: During the write phase, for a given pattern, a boolean mask of shape `(2048,)` is created to identify active addresses. The update vector is then added to all active rows of the `counters` matrix in a single operation: `counters[mask, :] += update_vector`.\n    - **Summation**: During the read phase, a similar boolean mask identifies active read locations. The sum vector is computed by `numpy.sum(counters[mask, :], axis=0)`.\n- **Accuracy Calculation**: After retrieving a pattern, the number of correct bits is found with `numpy.sum(recalled_pattern == original_pattern)`. These counts are accumulated, and the final accuracy is calculated by dividing by the total number of bits, $P \\cdot N$.\nThe results for each test case are collected and formatted into the required output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Sparse Distributed Memory (SDM) simulation\n    and evaluates its accuracy on a suite of test cases.\n    \"\"\"\n    # Fixed parameters for the SDM model\n    N = 256  # Number of bits in vectors\n    M = 2048 # Number of hard locations\n    P = 200  # Number of patterns to store\n    SEED = 12345 # Random seed for reproducibility\n\n    # Test cases: (write_radius, threshold)\n    # The read radius r_r is always equal to the write radius r_w.\n    test_cases = [\n        (104, 0),\n        (120, 0),\n        (120, 8),\n        (136, 0),\n        (256, 0),\n        (0, 0),\n    ]\n\n    # --- Step 1: Initialization ---\n    # Create a random number generator with a fixed seed\n    rng = np.random.default_rng(SEED)\n\n    # Generate M hard location addresses, each a random N-bit vector\n    addresses = rng.integers(0, 2, size=(M, N), dtype=np.int8)\n\n    # Generate P patterns to be stored and recalled, each a random N-bit vector\n    patterns = rng.integers(0, 2, size=(P, N), dtype=np.int8)\n\n    # --- Loop over test cases ---\n    results = []\n    for r_w, theta in test_cases:\n        r_r = r_w  # Read radius is same as write radius\n\n        # Initialize counter matrix for this test case\n        # Use int32 to avoid overflow, as max counter value can be P=200.\n        counters = np.zeros((M, N), dtype=np.int32)\n\n        # --- Step 2: Write Phase (Store all patterns) ---\n        # Pre-compute update vectors for all patterns (+1 for bit 1, -1 for bit 0)\n        update_vectors = 2 * patterns - 1\n\n        for p in range(P):\n            pattern = patterns[p]\n            update_vector = update_vectors[p]\n\n            # Calculate Hamming distances from the current pattern to all hard locations\n            # Broadcasting (addresses != pattern) handles the comparison efficiently.\n            distances = np.sum(addresses != pattern, axis=1)\n\n            # Create a boolean mask for hard locations within the write radius\n            write_mask = distances <= r_w\n\n            # Update counters for all active locations in a single vectorized operation\n            if np.any(write_mask):\n                counters[write_mask, :] += update_vector\n        \n        # --- Step 3 & 4: Read Phase and Accuracy Evaluation ---\n        total_correct_bits = 0\n        for p in range(P):\n            pattern_to_recall = patterns[p]\n\n            # Calculate Hamming distances from the cue pattern to all hard locations\n            distances = np.sum(addresses != pattern_to_recall, axis=1)\n\n            # Create a boolean mask for hard locations within the read radius\n            read_mask = distances <= r_r\n\n            # Compute the sum vector by summing counters from active locations\n            if np.any(read_mask):\n                sum_vector = np.sum(counters[read_mask, :], axis=0)\n            else:\n                # If no locations are active, sum vector is all zeros\n                sum_vector = np.zeros(N, dtype=np.int32)\n            \n            # Apply threshold to get the recalled binary pattern\n            recalled_pattern = (sum_vector >= theta).astype(np.int8)\n\n            # Count the number of correctly recalled bits for this pattern\n            total_correct_bits += np.sum(recalled_pattern == pattern_to_recall)\n\n        # Calculate the final accuracy for this test case\n        accuracy = total_correct_bits / (P * N)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having explored a model of memory storage and retrieval, we now turn to the fundamental question of how memories are formed at the synaptic level. This practice involves a theoretical analysis of the Bienenstock–Cooper–Munro (BCM) learning rule, a seminal model of activity-dependent synaptic plasticity. You will derive the stability conditions of a neuron governed by this rule, discovering how its unique 'sliding threshold' $\\theta_M$ automatically adjusts to input statistics to promote selectivity and prevent runaway feedback. This analytical exercise is invaluable for understanding the mathematical principles of homeostatic plasticity, which ensure that learning is both stable and meaningful. ",
            "id": "3971073",
            "problem": "Consider a single postsynaptic unit obeying the Bienenstock–Cooper–Munro (BCM) learning rule. Let the instantaneous postsynaptic activity be defined by $y(t) = \\mathbf{w}(t)^{\\top} \\mathbf{x}(t)$ for an input vector $\\mathbf{x}(t) \\in \\mathbb{R}^{n}$ and a synaptic weight vector $\\mathbf{w}(t) \\in \\mathbb{R}^{n}$. The synaptic modification rule is\n$$\n\\frac{d\\mathbf{w}}{dt} = \\eta\\, y\\, \\mathbf{x}\\, \\big(y - \\theta_{M}\\big),\n$$\nand the sliding modification threshold $\\theta_{M}(t)$ evolves according to\n$$\n\\frac{d\\theta_{M}}{dt} = \\alpha\\, \\big(y^{2} - \\theta_{M}\\big),\n$$\nwhere $\\eta > 0$ and $\\alpha > 0$ are constants, and $\\theta_{M}(t)$ is a slow variable relative to $\\mathbf{w}(t)$ (separation of timescales). Assume the input statistics are stationary and consist of two deterministic patterns: with probability $p \\in (0,1)$, $\\mathbf{x} = \\mathbf{a}$, and with probability $1-p$, $\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^{n}$ are fixed and satisfy $\\mathbf{a}^{\\top}\\mathbf{b} = 0$.\n\nStarting from the definitions above and the separation of timescales assumption, derive the stability conditions for a selective fixed point in which the weight vector aligns with $\\mathbf{a}$, i.e., $\\mathbf{w} = c\\, \\mathbf{a}$ for some scalar $c \\neq 0$. Show how the sliding threshold $\\theta_{M}$ adjusts to maintain selectivity and prevent runaway activity under the given input statistics, and determine the closed-form expression for the equilibrium modification threshold $\\theta_{M}^{\\ast}$ at this selective fixed point in terms of $p$. Express your final answer as a single analytic expression. No rounding is required, and no physical units are involved.",
            "solution": "The user wants me to analyze the Bienenstock–Cooper–Munro (BCM) learning rule for a single postsynaptic unit under specific input statistics and determine the equilibrium value of the modification threshold at a selective fixed point.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens**\n- **Postsynaptic activity:** $y(t) = \\mathbf{w}(t)^{\\top} \\mathbf{x}(t)$, where $\\mathbf{x}(t) \\in \\mathbb{R}^{n}$ is the input vector and $\\mathbf{w}(t) \\in \\mathbb{R}^{n}$ is the synaptic weight vector.\n- **Weight dynamics:** $\\frac{d\\mathbf{w}}{dt} = \\eta\\, y\\, \\mathbf{x}\\, \\big(y - \\theta_{M}\\big)$, with $\\eta > 0$.\n- **Modification threshold dynamics:** $\\frac{d\\theta_{M}}{dt} = \\alpha\\, \\big(y^{2} - \\theta_{M}\\big)$, with $\\alpha > 0$.\n- **Input statistics:** The input $\\mathbf{x}$ is drawn from a stationary distribution:\n  - $\\mathbf{x} = \\mathbf{a}$ with probability $p \\in (0,1)$.\n  - $\\mathbf{x} = \\mathbf{b}$ with probability $1-p$.\n- **Input properties:** The vectors $\\mathbf{a}$ and $\\mathbf{b}$ are fixed and orthogonal, i.e., $\\mathbf{a}^{\\top}\\mathbf{b} = 0$.\n- **Assumption:** There is a separation of timescales, with $\\theta_{M}(t)$ evolving much more slowly than $\\mathbf{w}(t)$.\n- **Objective:** Find the stability conditions for a selective fixed point $\\mathbf{w} = c\\,\\mathbf{a}$ ($c \\neq 0$) and determine the corresponding equilibrium modification threshold $\\theta_{M}^{\\ast}$.\n\n**1.2. Validation against Criteria**\n- **Scientifically Grounded:** The problem describes the BCM rule, a foundational and widely studied model for activity-dependent synaptic plasticity in computational neuroscience. The equations and setup are standard in the field.\n- **Well-Posed:** The problem provides a complete set of equations, parameters, and constraints. The objective is clearly defined and allows for a unique analytical solution.\n- **Objective:** The problem is stated in precise, mathematical language, free from any subjectivity or ambiguity.\n\n**1.3. Verdict**\nThe problem is valid as it is scientifically sound, self-contained, and well-posed. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe dynamics of the system are stochastic due to the probabilistic nature of the input. Given the stationary input statistics, we can analyze the average behavior of the system by averaging the differential equations over the input distribution. The expectation operator $\\left< \\cdot \\right>_{\\mathbf{x}}$ over the input statistics is given by $\\left< f(\\mathbf{x}) \\right>_{\\mathbf{x}} = p f(\\mathbf{a}) + (1-p) f(\\mathbf{b})$.\n\nThe averaged differential equations are:\n$$\n\\left< \\frac{d\\mathbf{w}}{dt} \\right> = \\eta \\left< y \\mathbf{x} (y - \\theta_M) \\right>_{\\mathbf{x}}\n$$\n$$\n\\left< \\frac{d\\theta_{M}}{dt} \\right> = \\alpha \\left< y^2 - \\theta_M \\right>_{\\mathbf{x}} = \\alpha (\\left< y^2 \\right>_{\\mathbf{x}} - \\theta_M)\n$$\nThe assumption of timescale separation implies that when we analyze the fast dynamics of $\\mathbf{w}$, we can treat $\\theta_M$ as a quasi-static parameter equal to its own equilibrium value. The equilibrium for $\\theta_M$ is found by setting its time derivative to zero:\n$$\n\\frac{d\\theta_{M}}{dt} = 0 \\implies \\theta_{M}^{\\ast} = \\left< y^2 \\right>_{\\mathbf{x}}\n$$\nThe term $\\left< y^2 \\right>_{\\mathbf{x}}$ is the time-averaged squared postsynaptic activity. Let $y_a = \\mathbf{w}^{\\top}\\mathbf{a}$ and $y_b = \\mathbf{w}^{\\top}\\mathbf{b}$ be the neuron's responses to patterns $\\mathbf{a}$ and $\\mathbf{b}$, respectively. Then,\n$$\n\\theta_{M}^{\\ast} = p y_a^2 + (1-p) y_b^2\n$$\nNow, we analyze the dynamics of the weight vector $\\mathbf{w}$. We substitute the equilibrium value $\\theta_M^{\\ast}$ into the averaged weight dynamics equation:\n$$\n\\left< \\frac{d\\mathbf{w}}{dt} \\right> = \\eta \\left[ p \\cdot y_a \\mathbf{a} (y_a - \\theta_M^{\\ast}) + (1-p) \\cdot y_b \\mathbf{b} (y_b - \\theta_M^{\\ast}) \\right]\n$$\nA fixed point $\\mathbf{w}^{\\ast}$ is a weight vector for which the average rate of change is zero, i.e., $\\left< \\frac{d\\mathbf{w}}{dt} \\right> = \\mathbf{0}$. We are looking for a selective fixed point where the weight vector aligns with pattern $\\mathbf{a}$, so we set $\\mathbf{w}^{\\ast} = c\\,\\mathbf{a}$ for some non-zero scalar $c$.\n\nLet's evaluate the activities and the threshold at this presumed fixed point:\n- $y_a^{\\ast} = (\\mathbf{w}^{\\ast})^{\\top}\\mathbf{a} = (c\\,\\mathbf{a})^{\\top}\\mathbf{a} = c \\|\\mathbf{a}\\|^2$.\n- $y_b^{\\ast} = (\\mathbf{w}^{\\ast})^{\\top}\\mathbf{b} = (c\\,\\mathbf{a})^{\\top}\\mathbf{b} = c (\\mathbf{a}^{\\top}\\mathbf{b}) = 0$, due to orthogonality.\n\nWith these activities, the equilibrium threshold becomes:\n$$\n\\theta_M^{\\ast} = p (y_a^{\\ast})^2 + (1-p) (y_b^{\\ast})^2 = p (c \\|\\mathbf{a}\\|^2)^2 + (1-p) (0)^2 = p c^2 \\|\\mathbf{a}\\|^4\n$$\nNow substitute these into the fixed-point condition for the weights:\n$$\n\\mathbf{0} = \\eta \\left[ p \\cdot y_a^{\\ast} \\mathbf{a} (y_a^{\\ast} - \\theta_M^{\\ast}) + (1-p) \\cdot y_b^{\\ast} \\mathbf{b} (y_b^{\\ast} - \\theta_M^{\\ast}) \\right]\n$$\nThe second term is zero because $y_b^{\\ast}=0$.\n$$\n\\mathbf{0} = \\eta \\, p \\, (c \\|\\mathbf{a}\\|^2) \\, \\mathbf{a} \\, (c \\|\\mathbf{a}\\|^2 - p c^2 \\|\\mathbf{a}\\|^4)\n$$\n$$\n\\mathbf{0} = \\eta \\, p \\, c^2 \\|\\mathbf{a}\\|^4 \\, \\mathbf{a} \\, (1 - p c \\|\\mathbf{a}\\|^2)\n$$\nSince $\\eta > 0$, $p \\in (0,1)$, $c \\neq 0$, and $\\mathbf{a}$ is a non-zero vector, for this equation to hold, the scalar factor in the parenthesis must be zero:\n$$\n1 - p c \\|\\mathbf{a}\\|^2 = 0 \\implies c = \\frac{1}{p \\|\\mathbf{a}\\|^2}\n$$\nThis determines the magnitude of the weight vector at the selective fixed point:  $\\mathbf{w}^{\\ast} = \\frac{1}{p \\|\\mathbf{a}\\|^2} \\mathbf{a}$.\n\nWith this value of $c$, we can find the definitive equilibrium values of the activities and the threshold:\n- $y_a^{\\ast} = c \\|\\mathbf{a}\\|^2 = \\frac{1}{p \\|\\mathbf{a}\\|^2} \\|\\mathbf{a}\\|^2 = \\frac{1}{p}$.\n- $y_b^{\\ast} = 0$.\n- $\\theta_M^{\\ast} = p (y_a^{\\ast})^2 = p \\left(\\frac{1}{p}\\right)^2 = \\frac{1}{p}$.\n\nThe fixed point is characterized by the postsynaptic response to the selected pattern $\\mathbf{a}$ being equal to the modification threshold, $y_a^{\\ast} = \\theta_M^{\\ast} = 1/p$. This equality ensures that further presentations of pattern $\\mathbf{a}$ do not produce any net change in the weights, as the plasticity term $(y_a^{\\ast} - \\theta_M^{\\ast})$ becomes zero.\n\nTo show that $\\theta_M$ maintains selectivity, we must verify that this fixed point is stable. We analyze the dynamics of small perturbations around the fixed point. Let $\\mathbf{w}(t) = \\mathbf{w}^{\\ast} + \\delta\\mathbf{w}(t)$. We can express any perturbation in the space spanned by the inputs as $\\delta\\mathbf{w}(t) = \\delta c_a(t) \\mathbf{a} + \\delta c_b(t) \\mathbf{b}$. The stability of the fixed point $(\\mathbf{w}^{\\ast}, \\theta_M^{\\ast})$ depends on whether these perturbations decay over time.\n\nThe dynamics for the coefficients $c_a(t)$ and $c_b(t)$ are derived by projecting the averaged weight dynamics onto $\\mathbf{a}$ and $\\mathbf{b}$:\n$$\n\\frac{dc_a}{dt} = \\eta \\, p \\, y_a (y_a - \\theta_M)\n$$\n$$\n\\frac{dc_b}{dt} = \\eta \\, (1-p) \\, y_b (y_b - \\theta_M)\n$$\nwhere $y_a = c_a \\|\\mathbf{a}\\|^2$, $y_b = c_b \\|\\mathbf{b}\\|^2$, and $\\theta_M = p y_a^2 + (1-p) y_b^2$.\n\nLinearizing these equations around the fixed point $(c_a^{\\ast}, c_b^{\\ast}) = (\\frac{1}{p\\|\\mathbf{a}\\|^2}, 0)$ yields a Jacobian matrix $J$. The eigenvalues of $J$ determine the stability.\nThe diagonal elements of the Jacobian, which are the eigenvalues due to the decoupled nature of the perturbations near the fixed point, are:\n$$\n\\lambda_a = \\left. \\frac{\\partial}{\\partial c_a} \\left(\\frac{dc_a}{dt}\\right) \\right|_{\\ast} = -\\eta \\|\\mathbf{a}\\|^2\n$$\n$$\n\\lambda_b = \\left. \\frac{\\partial}{\\partial c_b} \\left(\\frac{dc_b}{dt}\\right) \\right|_{\\ast} = -\\eta \\, (1-p) \\, \\|\\mathbf{b}\\|^2 \\, (y_a^{\\ast})^2 = -\\frac{\\eta(1-p)\\|\\mathbf{b}\\|^2}{p}\n$$\nFor the fixed point to be stable, both eigenvalues must be negative.\n- $\\lambda_a < 0$: Since $\\eta > 0$ and $\\|\\mathbf{a}\\|^2 > 0$ (for a non-trivial pattern), this condition is always met. This indicates that the magnitude of the weight vector is stable.\n- $\\lambda_b < 0$: Since $\\eta > 0$, $\\|\\mathbf{b}\\|^2 > 0$, and the problem states $p \\in (0,1)$ (which means $p>0$ and $1-p>0$), the factor $\\frac{\\eta(1-p)\\|\\mathbf{b}\\|^2}{p}$ is always positive. Thus, $\\lambda_b$ is always negative. This means that any component of the weight vector along the non-selected pattern $\\mathbf{b}$ will decay to zero, ensuring selectivity.\n\nThe stability analysis confirms that the system will converge to and remain at the selective state where $\\mathbf{w}$ is aligned with $\\mathbf{a}$. The sliding threshold $\\theta_M$ plays a crucial role by adjusting its value to $\\theta_M^{\\ast} = \\left< y^2 \\right>^{\\ast} = 1/p$. This value is precisely what is needed to balance potentiation and depression, stabilizing the learned selectivity. Specifically, for the selected input $\\mathbf{a}$, the activity is poised at the threshold ($y_a^* = \\theta_M^*$), halting further weight changes. For the unselected input $\\mathbf{b}$, the activity is zero ($y_b^* = 0$), which is below the threshold, leading to long-term depression (LTD) for any nascent synaptic component responsive to $\\mathbf{b}$ (as shown by $\\lambda_b < 0$), thus reinforcing selectivity.\n\nThe final closed-form expression for the equilibrium modification threshold $\\theta_M^{\\ast}$ at this stable, selective fixed point is determined solely by the probability $p$ of the selected pattern.",
            "answer": "$$\n\\boxed{\\frac{1}{p}}\n$$"
        },
        {
            "introduction": "Our final practice integrates the concepts of representation and learning into a sophisticated, biologically-inspired simulation. You will build a spiking neural network that learns a temporal sequence through Spike-Timing Dependent Plasticity (STDP), a learning rule that depends on the precise timing of pre- and post-synaptic spikes. The key investigation is how the learning of this sequence is modulated by the interaction between brain oscillations, specifically by timing the sequence-driving 'gamma bursts' at different phases of an underlying 'theta rhythm'. This advanced simulation bridges cellular mechanisms (STDP) with systems-level phenomena (neural oscillations), providing insight into a leading hypothesis for how the brain encodes episodic memories. ",
            "id": "3971152",
            "problem": "You are to design and implement a simulation of a discrete-time leaky integrate-and-fire spiking network with inhibitory interneuron-mediated theta rhythm and nested gamma-burst excitation that drives a sequential spike pattern among excitatory neurons. Sequence learning must be governed by Spike-Timing Dependent Plasticity (STDP). The core objective is to quantify how the phase timing of gamma bursts relative to the ongoing theta inhibition modulates the learned forward sequence connectivity.\n\nFundamental base and definitions:\n- The leaky integrate-and-fire neuron has membrane potential $v(t)$ that evolves according to the ordinary differential equation\n$$\n\\frac{dv}{dt} = \\frac{1}{\\tau_m}\\left(v_{\\mathrm{rest}} - v(t) + I_{\\mathrm{syn}}(t) + I_{\\mathrm{ext}}(t) + I_{\\mathrm{inh}}(t)\\right),\n$$\nwhere $\\tau_m$ is the membrane time constant, $v_{\\mathrm{rest}}$ is the resting potential, $I_{\\mathrm{syn}}(t)$ is the synaptic input current from other excitatory neurons, $I_{\\mathrm{ext}}(t)$ is a deterministic external current driving gamma-burst pulses, and $I_{\\mathrm{inh}}(t)$ is an inhibitory current shaped by a theta rhythm. A spike occurs when $v(t)$ crosses the threshold $v_{\\mathrm{th}}$, after which $v$ is reset to $v_{\\mathrm{reset}}$ and a refractory period prevents further spiking for a fixed time.\n- The theta rhythm is a sinusoidal inhibitory current modulating network excitability:\n$$\nI_{\\mathrm{inh}}(t) = -A_{\\mathrm{inh}}\\left(1 + \\sin(2\\pi f_\\theta t)\\right),\n$$\nwith inhibitory amplitude $A_{\\mathrm{inh}}$ and theta frequency $f_\\theta$.\n- Gamma bursts are modeled as brief external pulses $I_{\\mathrm{ext}}(t)$ applied sequentially to the excitatory neurons, spaced by a fixed inter-pulse delay to induce a spike sequence. The start time of each gamma burst within a theta cycle is controlled by a phase offset $\\phi$ (in radians) relative to the start of each theta cycle.\n- Excitatory synaptic input arises from spike-triggered synaptic traces with exponential decay. Let $S_{ij}(t)$ denote the synaptic activation trace from presynaptic neuron $i$ to postsynaptic neuron $j$. On each presynaptic spike of neuron $i$, the trace $S_{ij}$ is incremented and otherwise decays exponentially with synaptic time constant $\\tau_{\\mathrm{syn}}$. The postsynaptic current to neuron $j$ is:\n$$\nI_{\\mathrm{syn},j}(t) = g_{\\mathrm{exc}} \\sum_{i \\ne j} W_{ij} S_{ij}(t),\n$$\nwhere $g_{\\mathrm{exc}}$ is a synaptic gain and $W_{ij}$ are excitatory synaptic weights.\n- Spike-Timing Dependent Plasticity (STDP) updates $W_{ij}$ based on timing between presynaptic spikes of neuron $i$ and postsynaptic spikes of neuron $j$. Use a pair-based rule implemented with exponentially decaying traces $x_i(t)$ for presynaptic activity and $y_j(t)$ for postsynaptic activity:\n    - On a presynaptic spike of neuron $i$: $x_i \\leftarrow x_i + 1$, and for all $j \\ne i$, apply long-term depression (LTD): $W_{ij} \\leftarrow W_{ij} - A_- y_j$.\n    - On a postsynaptic spike of neuron $j$: $y_j \\leftarrow y_j + 1$, and for all $i \\ne j$, apply long-term potentiation (LTP): $W_{ij} \\leftarrow W_{ij} + A_+ x_i$.\n    - Traces decay exponentially: $\\dot{x}_i = -x_i/\\tau_+$ and $\\dot{y}_j = -y_j/\\tau_-$, where $\\tau_+$ and $\\tau_-$ are time constants. Weights are bounded to $[0,W_{\\max}]$ and $W_{ii}=0$.\n- Sequence learning metric (forward-bias score): For an ordered chain of $N$ excitatory neurons indexed $0$ to $N-1$, define the average forward bias after training as\n$$\nS = \\frac{1}{N-1} \\sum_{i=0}^{N-2} \\left( W_{i,i+1} - W_{i+1,i} \\right).\n$$\nA larger positive $S$ indicates stronger learned forward connectivity consistent with the induced spike sequence.\n\nImplementation constraints and discretization:\n- Use an explicit Euler discretization with time step $\\Delta t$ for the membrane updates:\n$$\nv_{j}(t+\\Delta t) = v_{j}(t) + \\frac{\\Delta t}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_{j}(t) + I_{\\mathrm{syn},j}(t) + I_{\\mathrm{ext},j}(t) + I_{\\mathrm{inh}}(t)\\right),\n$$\nsubject to threshold crossing and refractory reset $v_{j}\\leftarrow v_{\\mathrm{reset}}$ on spikes. Synaptic traces update as $S_{ij}(t+\\Delta t) = S_{ij}(t)\\,e^{-\\Delta t/\\tau_{\\mathrm{syn}}}$ plus spike-triggered increments. The presynaptic and postsynaptic STDP traces are similarly decayed each time step.\n- The simulation must be deterministic with fixed parameters and no random noise. The excitatory network size is $N=5$. Self-connections are disallowed, i.e., $W_{ii}=0$ for all $i$.\n\nGamma-burst generation within theta cycles:\n- Let the theta period be $T_\\theta = 1/f_\\theta$ and theta cycle start times be $t_k = k T_\\theta$ for integer $k \\ge 0$.\n- For each theta cycle, schedule one gamma burst starting at $t_k + \\phi \\cdot T_\\theta / (2\\pi)$. Within that burst, deliver identical rectangular pulses of duration $d_{\\mathrm{pulse}}$ and amplitude $I_{\\mathrm{pulse}}$ to neurons $0,1,\\dots,N-1$ sequentially, with inter-pulse delay $\\Delta_{\\gamma}$. Thus, neuron $n$ receives its pulse on the interval $[t_k + \\phi T_\\theta/(2\\pi) + n \\Delta_{\\gamma},\\ t_k + \\phi T_\\theta/(2\\pi) + n \\Delta_{\\gamma} + d_{\\mathrm{pulse}}]$.\n\nSimulation parameters and units:\n- Time parameters: $\\Delta t = 0.0005$ seconds, total simulation time $T_{\\mathrm{total}} = 1.0$ seconds, refractory time $t_{\\mathrm{ref}} = 0.005$ seconds, membrane time constant $\\tau_m = 0.02$ seconds, synaptic time constant $\\tau_{\\mathrm{syn}} = 0.005$ seconds, STDP time constants $\\tau_{+} = 0.02$ seconds and $\\tau_{-} = 0.02$ seconds.\n- Theta frequency: $f_\\theta = 6.0$ Hertz.\n- Membrane potentials are dimensionless in this model: $v_{\\mathrm{rest}} = 0.0$, $v_{\\mathrm{th}} = 1.0$, $v_{\\mathrm{reset}} = 0.0$.\n- Synaptic parameters: $g_{\\mathrm{exc}} = 0.05$, initial weights $W_{ij} = 0.01$ for $i \\ne j$, $W_{\\max} = 0.2$, STDP coefficients $A_+ = 0.005$, $A_- = 0.006$.\n- External pulse parameters: pulse amplitude $I_{\\mathrm{pulse}} = 1.2$ (dimensionless current units), pulse duration $d_{\\mathrm{pulse}} = 0.003$ seconds.\n- Inhibition amplitude $A_{\\mathrm{inh}}$ varies per test case and acts as a negative current; $\\phi$ is in radians.\n\nRequired outputs and test suite:\n- Implement the simulation to compute the forward-bias score $S$ for each specified parameter set. All outputs are unitless real numbers.\n- Test suite parameter sets to evaluate different gamma-burst phase timings and edge conditions:\n    1. $\\phi = 0.0$ radians, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ seconds.\n    2. $\\phi = \\pi/2$ radians, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ seconds.\n    3. $\\phi = \\pi$ radians, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ seconds.\n    4. $\\phi = 3\\pi/2$ radians, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.005$ seconds.\n    5. $\\phi = 3\\pi/2$ radians, $A_{\\mathrm{inh}} = 1.6$, $\\Delta_{\\gamma} = 0.005$ seconds.\n    6. $\\phi = 3\\pi/2$ radians, $A_{\\mathrm{inh}} = 0.8$, $\\Delta_{\\gamma} = 0.02$ seconds.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,...]\"). Each result is the scalar forward-bias score $S$ for the corresponding test case.\n\nYour implementation must be a complete, runnable program that adheres to the specified runtime environment and library constraints, takes no input, and prints exactly the single line in the described format. Angles must be interpreted in radians. Time parameters must be interpreted in seconds.",
            "solution": "The user-provided problem is a valid, well-posed, and scientifically grounded exercise in computational neuroscience. It requires the design and implementation of a simulation to investigate how the phase relationship between theta and gamma oscillations influences sequence learning via Spike-Timing Dependent Plasticity (STDP). The model is based on established biophysical and computational principles.\n\nThe solution involves developing a discrete-time simulation of a network of Leaky Integrate-and-Fire (LIF) neurons. The core components of the model and the simulation algorithm are detailed below.\n\n**1. Leaky Integrate-and-Fire (LIF) Neuron Model**\nThe fundamental unit of the network is the LIF neuron, a standard simplified model of neuronal dynamics. The membrane potential $v_j(t)$ of each neuron $j$ evolves over time based on the net current flowing into the cell. The dynamics are governed by the differential equation:\n$$\n\\frac{dv_j}{dt} = \\frac{1}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_j(t) + I_{\\mathrm{total},j}(t)\\right)\n$$\nwhere $\\tau_m = 0.02$ s is the membrane time constant, $v_{\\mathrm{rest}} = 0.0$ is the resting potential, and $I_{\\mathrm{total},j}(t)$ is the sum of all input currents. When $v_j(t)$ reaches a threshold $v_{\\mathrm{th}} = 1.0$, the neuron fires a spike, its potential is reset to $v_{\\mathrm{reset}} = 0.0$, and it enters a refractory period of $t_{\\mathrm{ref}} = 0.005$ s during which it cannot spike again.\n\nFor computational implementation, we discretize this equation using the explicit Euler method with a time step of $\\Delta t = 0.0005$ s:\n$$\nv_{j}(t+\\Delta t) = v_{j}(t) + \\frac{\\Delta t}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_{j}(t) + I_{\\mathrm{total},j}(t)\\right)\n$$\nThe total current $I_{\\mathrm{total},j}(t)$ is the sum of three components: $I_{\\mathrm{total},j}(t) = I_{\\mathrm{syn},j}(t) + I_{\\mathrm{ext},j}(t) + I_{\\mathrm{inh}}(t)$.\n\n**2. Rhythmic and External Inputs**\nThe network's activity is orchestrated by two key external currents:\n\n- **Theta Rhythmic Inhibition ($I_{\\mathrm{inh}}$):** A global inhibitory current modulates the excitability of all neurons in the network, mimicking the effects of inhibitory interneuron populations. This current follows a sinusoidal pattern:\n  $$\n  I_{\\mathrm{inh}}(t) = -A_{\\mathrm{inh}}\\left(1 + \\sin(2\\pi f_\\theta t)\\right)\n  $$\n  Here, $f_\\theta = 6.0$ Hz is the theta frequency, and $A_{\\mathrm{inh}}$ is the inhibitory amplitude, which varies across test cases. This current creates cyclical windows of high and low excitability. The phase of the sine function determines the level of inhibition: minimum inhibition ($I_{\\mathrm{inh}} = 0$) occurs at phases $3\\pi/2, 7\\pi/2, \\dots$, and maximum inhibition ($I_{\\mathrm{inh}} = -2A_{\\mathrm{inh}}$) occurs at phases $\\pi/2, 5\\pi/2, \\dots$.\n\n- **Gamma-Burst Excitation ($I_{\\mathrm{ext}}$):** To induce a sequential firing pattern, a \"gamma burst\" of external excitatory current is applied to the neurons. A burst consists of $N=5$ brief, rectangular pulses of amplitude $I_{\\mathrm{pulse}} = 1.2$ and duration $d_{\\mathrm{pulse}} = 0.003$ s. The pulses are delivered sequentially to neurons $0, 1, \\dots, N-1$ with an inter-pulse delay of $\\Delta_{\\gamma}$. The key experimental parameter is the phase offset $\\phi$, which dictates the start time of the entire burst within each theta cycle. The burst for theta cycle $k$ (starting at $t_k = k/f_\\theta$) begins at $t_k + \\phi/(2\\pi f_\\theta)$, precisely phase-locking the sequence-inducing drive to the underlying theta rhythm.\n\n**3. Synaptic Transmission and Plasticity (STDP)**\nConnections between the $N=5$ excitatory neurons are plastic, meaning their strengths (weights $W_{ij}$) evolve based on neural activity.\n\n- **Synaptic Current ($I_{\\mathrm{syn}}$):** When a presynaptic neuron $i$ spikes, it contributes to the input current of a postsynaptic neuron $j$. This is modeled using a synaptic trace $S_{ij}(t)$, which is incremented by $1.0$ upon a spike from $i$ and otherwise decays exponentially with time constant $\\tau_{\\mathrm{syn}} = 0.005$ s. The total synaptic current to neuron $j$ is the weighted sum over all presynaptic inputs:\n  $$\n  I_{\\mathrm{syn},j}(t) = g_{\\mathrm{exc}} \\sum_{i \\ne j} W_{ij} S_{ij}(t)\n  $$\n  where $g_{\\mathrm{exc}} = 0.05$ is a constant synaptic gain.\n\n- **Spike-Timing Dependent Plasticity (STDP):** The weights $W_{ij}$ are updated according to a pair-based STDP rule, which strengthens or weakens synapses based on the relative timing of pre- and post-synaptic spikes. This is implemented using two additional traces for each neuron: a presynaptic trace $x_i(t)$ and a postsynaptic trace $y_j(t)$. Both decay exponentially with time constant $\\tau_{+} = \\tau_{-} = 0.02$ s.\n    - Upon a spike from presynaptic neuron $i$, its trace $x_i$ increments. This event triggers Long-Term Depression (LTD) for its outgoing synapses: $W_{ij} \\leftarrow W_{ij} - A_- y_j$, where $A_- = 0.006$ and $y_j$ is the current value of the postsynaptic trace of neuron $j$.\n    - Upon a spike from postsynaptic neuron $j$, its trace $y_j$ increments. This event triggers Long-Term Potentiation (LTP) for its incoming synapses: $W_{ij} \\leftarrow W_{ij} + A_+ x_i$, where $A_+ = 0.005$ and $x_i$ is the current value of the presynaptic trace of neuron $i$.\n  In our discrete simulation, if a neuron spikes, both LTP for its incoming connections and LTD for its outgoing connections are calculated based on the trace values just before the spike. Then, its own $x$ and $y$ traces are incremented. Weights are clipped to the range $[0, W_{\\max}]$ with $W_{\\max} = 0.2$.\n\n**4. Simulation Algorithm and Metric**\nThe simulation proceeds by iterating through time from $t=0$ to $T_{\\mathrm{total}}=1.0$ s. At each time step $\\Delta t$:\n1. The total current for each neuron is computed by summing the theta inhibition, external gamma pulse (if any), and the synaptic currents from other neurons.\n2. The membrane potentials of all non-refractory neurons are updated via the Euler method.\n3. Neurons whose potential crosses $v_{\\mathrm{th}}$ are identified as spiking.\n4. For each spiking neuron, the STDP weight updates (both LTP and LTD) are applied, the potential is reset, and its refractory counter is initiated. The corresponding synaptic and STDP traces are incremented.\n5. All trace variables ($S_{ij}$, $x_i$, $y_j$) are decayed exponentially. Refractory counters are decremented.\nThis process is repeated for the entire simulation duration.\n\nAfter the simulation, the final connectivity matrix $W$ is used to calculate the **forward-bias score** $S$:\n$$\nS = \\frac{1}{N-1} \\sum_{i=0}^{N-2} \\left( W_{i,i+1} - W_{i+1,i} \\right)\n$$\nThis metric quantifies how successfully the network has learned the forward sequence ($0 \\to 1 \\to \\dots \\to 4$) by comparing the strength of connections in the desired direction ($W_{i,i+1}$) against connections in the reverse direction ($W_{i+1,i}$). A larger positive score indicates stronger, more accurate sequence learning. The Python implementation provided in the final answer executes this complete simulation for each test case specified in the problem.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(phi, A_inh, delta_gamma):\n    \"\"\"\n    Runs a single simulation of the spiking network with a given parameter set.\n    \"\"\"\n    # 1. PARAMETERS\n    # Time parameters\n    DT = 0.0005  # s\n    T_TOTAL = 1.0  # s\n    T_REF = 0.005  # s\n    TAU_M = 0.02  # s\n    TAU_SYN = 0.005  # s\n    TAU_PLUS = 0.02  # s\n    TAU_MINUS = 0.02  # s\n\n    # Network and neuron parameters\n    N = 5\n    V_REST = 0.0\n    V_TH = 1.0\n    V_RESET = 0.0\n    G_EXC = 0.05\n    W_MAX = 0.2\n    A_PLUS = 0.005\n    A_MINUS = 0.006\n\n    # Input parameters\n    F_THETA = 6.0\n    I_PULSE = 1.2\n    D_PULSE = 0.003\n    T_THETA = 1.0 / F_THETA\n\n    # Discretized and pre-calculated values\n    NUM_STEPS = int(T_TOTAL / DT)\n    REFRACTORY_STEPS = int(T_REF / DT)\n    EXP_DECAY_SYN = np.exp(-DT / TAU_SYN)\n    EXP_DECAY_PLUS = np.exp(-DT / TAU_PLUS)\n    EXP_DECAY_MINUS = np.exp(-DT / TAU_MINUS)\n\n    # 2. STATE VARIABLES INITIALIZATION\n    v = np.full(N, V_REST, dtype=np.float64)\n    W = np.full((N, N), 0.01, dtype=np.float64)\n    np.fill_diagonal(W, 0)\n    \n    S_trace = np.zeros((N, N), dtype=np.float64)\n    x_trace = np.zeros(N, dtype=np.float64)\n    y_trace = np.zeros(N, dtype=np.float64)\n    refractory_counters = np.zeros(N, dtype=np.int32)\n    \n    # 3. SIMULATION LOOP\n    for step in range(NUM_STEPS):\n        t = step * DT\n\n        # Calculate currents\n        # Inhibitory current\n        I_inh = -A_inh * (1.0 + np.sin(2.0 * np.pi * F_THETA * t))\n\n        # External pulse current\n        I_ext = np.zeros(N, dtype=np.float64)\n        k_cycle = np.floor(t / T_THETA)\n        t_burst_start = k_cycle * T_THETA + phi * T_THETA / (2.0 * np.pi)\n        \n        for n in range(N):\n            t_pulse_start = t_burst_start + n * delta_gamma\n            t_pulse_end = t_pulse_start + D_PULSE\n            if t_pulse_start <= t < t_pulse_end:\n                I_ext[n] = I_PULSE\n\n        # Synaptic current\n        I_syn = G_EXC * np.sum(W * S_trace, axis=0) # Sum over presynaptic neurons (axis=0)\n\n        # Total current\n        I_total = I_syn + I_ext + I_inh\n\n        # Update membrane potential for non-refractory neurons\n        non_refractory_mask = (refractory_counters == 0)\n        v[non_refractory_mask] += (DT / TAU_M) * (V_REST - v[non_refractory_mask] + I_total[non_refractory_mask])\n\n        # Check for spikes\n        spiked_neurons = np.where((v >= V_TH) & non_refractory_mask)[0]\n\n        if spiked_neurons.size > 0:\n            x_trace_old = x_trace.copy()\n            y_trace_old = y_trace.copy()\n\n            for s_idx in spiked_neurons:\n                # LTP for incoming synapses\n                W[:, s_idx] += A_PLUS * x_trace_old\n                \n                # LTD for outgoing synapses\n                W[s_idx, :] -= A_MINUS * y_trace_old\n            \n            # Clip weights and zero the diagonal\n            W = np.clip(W, 0, W_MAX)\n            np.fill_diagonal(W, 0)\n\n            for s_idx in spiked_neurons:\n                # Update traces and reset state for spiking neurons\n                x_trace[s_idx] += 1.0\n                y_trace[s_idx] += 1.0\n                S_trace[s_idx, :] += 1.0 # Spike from s_idx affects traces to all other neurons\n                \n                v[s_idx] = V_RESET\n                refractory_counters[s_idx] = REFRACTORY_STEPS\n\n        # Decay traces\n        S_trace *= EXP_DECAY_SYN\n        x_trace *= EXP_DECAY_PLUS\n        y_trace *= EXP_DECAY_MINUS\n        \n        # Update refractory counters\n        refractory_counters[refractory_counters > 0] -= 1\n\n    # 4. CALCULATE FORWARD-BIAS SCORE\n    forward_sum = np.sum(np.diag(W, k=1))  # W[i, i+1]\n    backward_sum = np.sum(np.diag(W, k=-1)) # W[i+1, i]\n    \n    score = (forward_sum - backward_sum) / (N - 1)\n    \n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (phi, A_inh, delta_gamma)\n        (0.0, 0.8, 0.005),\n        (np.pi / 2, 0.8, 0.005),\n        (np.pi, 0.8, 0.005),\n        (3 * np.pi / 2, 0.8, 0.005),\n        (3 * np.pi / 2, 1.6, 0.005),\n        (3 * np.pi / 2, 0.8, 0.020),\n    ]\n\n    results = []\n    for case in test_cases:\n        phi, A_inh, delta_gamma = case\n        result = run_simulation(phi, A_inh, delta_gamma)\n        results.append(result)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}