{
    "hands_on_practices": [
        {
            "introduction": "计算模型通常首先从经典且富有影响力的概念入手。稀疏分布式存储器（SDM）就是一个典型的联想存储模型，它展示了如何通过在高维空间中进行分布式存储来实现基于内容的寻址和对噪声的鲁棒性。通过亲手实现一个SDM ，您将获得构建分布式存储系统核心架构的实践经验，并探索写入密度等关键参数如何影响记忆提取的准确性。",
            "id": "3971099",
            "problem": "请考虑实现一个玩具版的稀疏分布式存储器（SDM），这是一种著名的大脑建模和计算神经科学中使用的联想记忆模型。SDM 由一组固定的二进制地址和相应的逐位整型计数器组成。存储和检索由二进制汉明空间中的邻近性定义。假定以下基本前提：\n\n- 二进制模式表示为固定长度为 $N$ 的向量 $x \\in \\{0,1\\}^N$。\n- 两个二进制向量 $x$ 和 $y$ 之间的汉明距离是 $d_H(x,y) = \\sum_{k=1}^{N} \\mathbf{1}[x_k \\ne y_k]$。\n- 分布式存储通过整型计数器的加性叠加来建模；当写入一个模式时，在与该模式足够近的存储位置上，比特为 $1$ 的计数器加 $+1$，比特为 $0$ 的计数器减 $1$。\n- 检索操作对足够近的位置的计数器求和，并应用阈值决策规则来重建一个二进制向量。\n\n您必须实现一个完整的程序来构建一个 $N=256$ 位的SDM。设有 $M$ 个硬位置（地址），每个都是从 $\\{0,1\\}^N$ 中独立均匀采样的随机向量。每个硬位置存储一个由 $N$ 个初始化为零的整型计数器组成的向量。该程序必须：\n\n1. 生成 $P$ 个随机二进制模式 $x^{(1)},\\dots,x^{(P)}$，每个模式都在 $\\{0,1\\}^{N}$ 中，且是独立均匀生成的。\n2. 对于给定的写入半径 $r_w$，将每个模式 $x^{(p)}$ 写入所有汉明距离满足 $d_H(a_i, x^{(p)}) \\le r_w$ 的硬位置 $a_i$，更新规则如下：\n   - 对于每个比特索引 $k \\in \\{1,\\dots,N\\}$：如果 $x^{(p)}_k = 1$，则计数器 $c_{i,k}$ 加 $+1$；如果 $x^{(p)}_k = 0$，则计数器 $c_{i,k}$ 减 $1$。\n3. 对于给定的读取半径 $r_r$ 和阈值 $\\theta$，通过以下方式回忆每个存储的模式 $x^{(p)}$：\n   - 选择所有满足 $d_H(a_i, x^{(p)}) \\le r_r$ 的硬位置 $a_i$。\n   - 计算逐位总和 $s_k = \\sum_{i: d_H(a_i, x^{(p)}) \\le r_r} c_{i,k}$，其中 $k \\in \\{1,\\dots,N\\}$。\n   - 通过阈值规则生成回忆出的二进制向量 $\\hat{x}^{(p)}$：如果 $s_k \\ge \\theta$，则 $\\hat{x}^{(p)}_k = 1$，否则 $\\hat{x}^{(p)}_k = 0$。\n4. 将回忆准确率评估为所有存储模式中正确回忆的比特所占的比例：\n   $$\\text{accuracy} = \\frac{1}{P N} \\sum_{p=1}^{P} \\sum_{k=1}^{N} \\mathbf{1}[\\hat{x}^{(p)}_k = x^{(p)}_k].$$\n   准确率必须以 $[0,1]$ 范围内的小数形式报告。\n\n在所有测试用例中应使用同一组硬位置和同一组模式，以确保可比性。使用固定的随机种子以使结果可复现。\n\n为保证科学真实性和覆盖范围，请采用以下固定参数：\n- 比特数：$N=256$。\n- 硬位置数量：$M=2048$。\n- 模式数量：$P=200$。\n- 用于地址和模式的随机种子：$s=12345$。\n\n通过改变写入半径 $r_w$（这会影响每次写入时更新的硬位置的期望比例）来改变写入密度，并改变读取阈值 $\\theta$。在所有测试用例中，使用读取半径 $r_r = r_w$。实现程序以在以下测试套件上评估准确率：\n\n- 测试用例1：$(r_w, \\theta) = (104, 0)$，低写入密度（大约低于平均汉明距离三个标准差）。\n- 测试用例2：$(r_w, \\theta) = (120, 0)$，中等写入密度（大约低于平均值一个标准差）。\n- 测试用例3：$(r_w, \\theta) = (120, 8)$，与测试用例2密度相同，但使用正阈值以减少虚假回忆。\n- 测试用例4：$(r_w, \\theta) = (136, 0)$，高写入密度（大约高于平均值一个标准差）。\n- 测试用例5：$(r_w, \\theta) = (256, 0)$，最大写入密度（所有位置都被更新和读取）。\n- 测试用例6：$(r_w, \\theta) = (0, 0)$，边界情况，只有地址完全匹配时才会被更新或读取。\n\n所有距离均为汉明距离；所有阈值均为整数；所有准确率必须是小数。最终程序必须为所有测试用例生成准确率，并将其作为单行输出，形式为方括号括起来的逗号分隔列表，例如 $[a_1,a_2,\\dots,a_6]$，其中每个 $a_i$ 是按上述定义计算的测试用例 $i$ 的准确率。不涉及物理单位；不存在角度；百分比必须表示为不带百分号的小数。",
            "solution": "用户提供了一个定义明确的计算问题，要求实现一个稀疏分布式存储器（SDM）模型。该问题具有科学依据，形式上明确，并且计算上是可行的。所有必要的参数和评估指标都已明确说明，这使得该问题可以得到完整的解决方案。任务的核心是模拟 SDM 中二进制模式的存储和检索，并在各种参数设置下评估其回忆准确率。\n\n解决方案首先生成存储器的静态组件：一组 $M$ 个固定的随机二进制地址和一组要存储的 $P$ 个随机二进制模式。然后，对于由写入半径 $r_w$ 和阈值 $\\theta$ 定义的每个测试用例，模拟执行一个写入阶段和一个读取阶段。\n\n### 数学和算法框架\n\n**1. 数据表示与生成**\n\n- 所有数据，包括硬位置地址和内存模式，都表示为固定长度 $N=256$ 的二进制向量。向量 $x$ 是空间 $\\{0,1\\}^N$ 中的一个元素。\n- SDM 由 $M = 2048$ 个硬位置 $\\{a_i\\}_{i=1}^{M}$ 定义，其中每个 $a_i \\in \\{0,1\\}^{N}$ 是一个向量，其分量从 $\\{0,1\\}$ 中独立均匀采样。\n- 一组 $P = 200$ 个模式 $\\{x^{(p)}\\}_{p=1}^{P}$ 也以类似方式生成，其中每个 $x^{(p)} \\in \\{0,1\\}^{N}$ 都是一个随机二进制向量。\n- 为确保可复现性，所有随机元素均使用固定的随机种子 $s = 12345$ 生成。\n- 存储器本身由一个大小为 $M \\times N$ 的计数器矩阵 $C$ 组成。每个元素 $c_{i,k}$ 是一个整型计数器，对应第 $i$ 个硬位置的第 $k$ 个比特。所有计数器都初始化为 $0$。\n\n**2. 距离度量**\n\n模式和硬位置之间的相互作用由汉明距离 $d_H$ 决定。对于两个向量 $x, y \\in \\{0,1\\}^{N}$，距离是它们对应比特不同的位置数量：\n$$d_H(x,y) = \\sum_{k=1}^{N} \\mathbf{1}[x_k \\ne y_k]$$\n其中 $\\mathbf{1}[\\cdot]$ 是指示函数，当其参数为真时值为 $1$，否则为 $0$。\n\n**3. 写入操作（存储）**\n\n$P$ 个模式的存储是迭代执行的。对于每个模式 $x^{(p)}$（$p \\in \\{1, \\dots, P\\}$），执行以下步骤：\n- 从模式 $x^{(p)}$ 派生出一个“更新向量” $u^{(p)} \\in \\{-1, 1\\}^{N}$：$u^{(p)}_k = 2x^{(p)}_k - 1$。这将比特值 $1$ 映射为 $+1$，比特值 $0$ 映射为 $-1$。\n- 对于每个硬位置 $a_i$（$i \\in \\{1, \\dots, M\\}$），计算汉明距离 $d_H(a_i, x^{(p)})$。\n- 如果 $d_H(a_i, x^{(p)}) \\le r_w$（其中 $r_w$ 是指定的写入半径），则通过加上更新向量来更新相应的计数器行 $c_{i, \\cdot}$：\n  $$c_{i,k} \\leftarrow c_{i,k} + u^{(p)}_k \\quad \\text{for all } k \\in \\{1, \\dots, N\\}$$\n对所有 $P$ 个模式重复此过程，更新是累积的（叠加）。\n\n**4. 读取操作（检索）**\n\n为了评估存储器的保真度，我们尝试回忆每个原始模式。对于用作检索提示的每个模式 $x^{(p)}$：\n- 确定用于读取的活跃硬位置集合。这些是所有满足 $d_H(a_i, x^{(p)}) \\le r_r$ 的位置 $a_i$，其中 $r_r$ 是读取半径。在本问题中，给定 $r_r = r_w$。\n- 通过对所有活跃位置的计数器向量求和，计算出一个和向量 $s \\in \\mathbb{Z}^{N}$：\n  $$s_k = \\sum_{i : d_H(a_i, x^{(p)}) \\le r_r} c_{i,k} \\quad \\text{for all } k \\in \\{1, \\dots, N\\}$$\n  如果没有位置在读取半径内，则和向量为零向量。\n- 通过对和向量应用阈值规则来构建检索到的模式 $\\hat{x}^{(p)} \\in \\{0,1\\}^{N}$：\n  $$\\hat{x}^{(p)}_k = \\begin{cases} 1  & \\text{if } s_k \\ge \\theta \\\\ 0  & \\text{if } s_k  \\theta \\end{cases}$$\n  其中 $\\theta$ 是给定的整数阈值。\n\n**5. 准确率评估**\n\n性能由所有模式的总体回忆准确率来衡量。准确率定义为正确检索的比特所占的比例：\n$$\\text{accuracy} = \\frac{1}{P \\cdot N} \\sum_{p=1}^{P} \\sum_{k=1}^{N} \\mathbf{1}[\\hat{x}^{(p)}_k = x^{(p)}_k]$$\n总比特数为 $P \\cdot N = 200 \\cdot 256 = 51200$。\n\n### 实现策略\n\n该算法使用 Python 中的 NumPy 库实现，以进行高效的数值计算。\n- **初始化**：使用种子 $s = 12345$ 创建一个 `numpy.random.default_rng` 实例。此生成器用于创建 `addresses` 数组（形状为 `(2048, 256)`）和 `patterns` 数组（形状为 `(200, 256)`），二者均为 `numpy.int8` 类型以节省内存。\n- **主循环**：程序遍历测试用例列表 `(r_w, \\theta)`。对于每个用例，它都会重新初始化一个形状为 `(2048, 256)` 的 `counters` 矩阵，其数据类型为 `numpy.int32`，以防止求和期间发生溢出。\n- **向量化**：核心操作被向量化以提升性能。\n    - **汉明距离**：从单个模式 `p`（形状为 `(256,)`）到所有 `M` 个地址（形状为 `(2048, 256)`）的距离可以通过广播高效计算：`numpy.sum(addresses != p, axis=1)`。这将产生一个包含 $M$ 个距离的数组。\n    - **计数器更新**：在写入阶段，对于一个给定的模式，会创建一个形状为 `(2048,)` 的布尔掩码来识别活跃地址。然后，更新向量通过单个操作加到 `counters` 矩阵的所有活跃行上：`counters[mask, :] += update_vector`。\n    - **求和**：在读取阶段，一个类似的布尔掩码会识别活跃的读取位置。和向量通过 `numpy.sum(counters[mask, :], axis=0)` 计算。\n- **准确率计算**：在检索一个模式后，通过 `numpy.sum(recalled_pattern == original_pattern)` 找到正确比特的数量。这些计数被累加起来，最终的准确率通过除以总比特数 $P \\cdot N$ 计算得出。\n每个测试用例的结果被收集并格式化为所要求的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Sparse Distributed Memory (SDM) simulation\n    and evaluates its accuracy on a suite of test cases.\n    \"\"\"\n    # Fixed parameters for the SDM model\n    N = 256  # Number of bits in vectors\n    M = 2048 # Number of hard locations\n    P = 200  # Number of patterns to store\n    SEED = 12345 # Random seed for reproducibility\n\n    # Test cases: (write_radius, threshold)\n    # The read radius r_r is always equal to the write radius r_w.\n    test_cases = [\n        (104, 0),\n        (120, 0),\n        (120, 8),\n        (136, 0),\n        (256, 0),\n        (0, 0),\n    ]\n\n    # --- Step 1: Initialization ---\n    # Create a random number generator with a fixed seed\n    rng = np.random.default_rng(SEED)\n\n    # Generate M hard location addresses, each a random N-bit vector\n    addresses = rng.integers(0, 2, size=(M, N), dtype=np.int8)\n\n    # Generate P patterns to be stored and recalled, each a random N-bit vector\n    patterns = rng.integers(0, 2, size=(P, N), dtype=np.int8)\n\n    # --- Loop over test cases ---\n    results = []\n    for r_w, theta in test_cases:\n        r_r = r_w  # Read radius is same as write radius\n\n        # Initialize counter matrix for this test case\n        # Use int32 to avoid overflow, as max counter value can be P=200.\n        counters = np.zeros((M, N), dtype=np.int32)\n\n        # --- Step 2: Write Phase (Store all patterns) ---\n        # Pre-compute update vectors for all patterns (+1 for bit 1, -1 for bit 0)\n        update_vectors = 2 * patterns - 1\n\n        for p in range(P):\n            pattern = patterns[p]\n            update_vector = update_vectors[p]\n\n            # Calculate Hamming distances from the current pattern to all hard locations\n            # Broadcasting (addresses != pattern) handles the comparison efficiently.\n            distances = np.sum(addresses != pattern, axis=1)\n\n            # Create a boolean mask for hard locations within the write radius\n            write_mask = distances = r_w\n\n            # Update counters for all active locations in a single vectorized operation\n            if np.any(write_mask):\n                counters[write_mask, :] += update_vector\n        \n        # --- Step 3  4: Read Phase and Accuracy Evaluation ---\n        total_correct_bits = 0\n        for p in range(P):\n            pattern_to_recall = patterns[p]\n\n            # Calculate Hamming distances from the cue pattern to all hard locations\n            distances = np.sum(addresses != pattern_to_recall, axis=1)\n\n            # Create a boolean mask for hard locations within the read radius\n            read_mask = distances = r_r\n\n            # Compute the sum vector by summing counters from active locations\n            if np.any(read_mask):\n                sum_vector = np.sum(counters[read_mask, :], axis=0)\n            else:\n                # If no locations are active, sum vector is all zeros\n                sum_vector = np.zeros(N, dtype=np.int32)\n            \n            # Apply threshold to get the recalled binary pattern\n            recalled_pattern = (sum_vector >= theta).astype(np.int8)\n\n            # Count the number of correctly recalled bits for this pattern\n            total_correct_bits += np.sum(recalled_pattern == pattern_to_recall)\n\n        # Calculate the final accuracy for this test case\n        accuracy = total_correct_bits / (P * N)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了抽象的联想存储模型之后，一个自然而然的进阶是探索更具生物真实感的序列学习模型，这对于理解语言和运动控制等认知功能至关重要。本练习  深入研究了三个关键生物机制的相互作用：脉冲神经元、脉冲时间依赖可塑性（STDP）以及嵌套的θ-γ振荡。通过构建这个复杂的仿真，您将揭示精确的脉冲时间与网络节律如何协同作用，从而为大脑学习和回忆序列提供一个可能的计算解释。",
            "id": "3971152",
            "problem": "你需要设计并实现一个离散时间的漏放电（leaky integrate-and-fire）尖峰网络模拟。该网络具有由抑制性中间神经元介导的 theta 节律，以及驱动兴奋性神经元产生序列尖峰模式的嵌套 gamma 脉冲簇兴奋。序列学习必须由尖峰时间依赖可塑性（Spike-Timing Dependent Plasticity, STDP）控制。核心目标是量化 gamma 脉冲簇相对于持续的 theta 抑制的相位时机如何调制学习到的前向序列连接性。\n\n基本原理和定义：\n- 漏放电神经元的膜电位 $v(t)$ 遵循以下常微分方程演化：\n$$\n\\frac{dv}{dt} = \\frac{1}{\\tau_m}\\left(v_{\\mathrm{rest}} - v(t) + I_{\\mathrm{syn}}(t) + I_{\\mathrm{ext}}(t) + I_{\\mathrm{inh}}(t)\\right),\n$$\n其中 $\\tau_m$ 是膜时间常数，$v_{\\mathrm{rest}}$ 是静息电位，$I_{\\mathrm{syn}}(t)$ 是来自其他兴奋性神经元的突触输入电流，$I_{\\mathrm{ext}}(t)$ 是驱动 gamma 脉冲簇的确定性外部电流，$I_{\\mathrm{inh}}(t)$ 是由 theta 节律形成的抑制性电流。当 $v(t)$ 超过阈值 $v_{\\mathrm{th}}$ 时，产生一个尖峰，之后 $v$ 被重置为 $v_{\\mathrm{reset}}$，并在一段固定的不应期内阻止再次发放尖峰。\n- Theta 节律是一个正弦抑制性电流，用于调制网络兴奋性：\n$$\nI_{\\mathrm{inh}}(t) = -A_{\\mathrm{inh}}\\left(1 + \\sin(2\\pi f_\\theta t)\\right),\n$$\n其中抑制性振幅为 $A_{\\mathrm{inh}}$，theta 频率为 $f_\\theta$。\n- Gamma 脉冲簇被建模为短暂的外部脉冲 $I_{\\mathrm{ext}}(t)$，以固定的脉冲间延迟顺序施加到兴奋性神经元上，以诱发一个尖峰序列。每个 gamma 脉冲簇在 theta 周期内的起始时间由一个相对于每个 theta 周期开始的相位偏移 $\\phi$（以弧度为单位）控制。\n- 兴奋性突触输入源于由尖峰触发且呈指数衰减的突触轨迹。设 $S_{ij}(t)$ 表示从突触前神经元 $i$ 到突触后神经元 $j$ 的突触激活轨迹。每次神经元 $i$ 发放突触前尖峰时，轨迹 $S_{ij}$ 会增加，否则以突触时间常数 $\\tau_{\\mathrm{syn}}$ 指数衰减。到神经元 $j$ 的突触后电流为：\n$$\nI_{\\mathrm{syn},j}(t) = g_{\\mathrm{exc}} \\sum_{i \\ne j} W_{ij} S_{ij}(t),\n$$\n其中 $g_{\\mathrm{exc}}$ 是突触增益，$W_{ij}$ 是兴奋性突触权重。\n- 尖峰时间依赖可塑性（STDP）根据突触前神经元 $i$ 的尖峰与突触后神经元 $j$ 的尖峰之间的时间差来更新 $W_{ij}$。使用一个基于配对的规则，通过指数衰减的轨迹 $x_i(t)$（用于突触前活动）和 $y_j(t)$（用于突触后活动）实现：\n    - 当神经元 $i$ 发放突触前尖峰时：$x_i \\leftarrow x_i + 1$，并且对于所有 $j \\ne i$，应用长时程抑制（LTD）：$W_{ij} \\leftarrow W_{ij} - A_- y_j$。\n    - 当神经元 $j$ 发放突触后尖峰时：$y_j \\leftarrow y_j + 1$，并且对于所有 $i \\ne j$，应用长时程增强（LTP）：$W_{ij} \\leftarrow W_{ij} + A_+ x_i$。\n    - 轨迹呈指数衰减：$\\dot{x}_i = -x_i/\\tau_+$ 和 $\\dot{y}_j = -y_j/\\tau_-$，其中 $\\tau_+$ 和 $\\tau_-$ 是时间常数。权重被限制在 $[0,W_{\\max}]$ 范围内，且 $W_{ii}=0$。\n- 序列学习度量（前向偏置分数）：对于一个由 $N$ 个兴奋性神经元（索引从 $0$ 到 $N-1$）组成的有序链，将训练后的平均前向偏置定义为\n$$\nS = \\frac{1}{N-1} \\sum_{i=0}^{N-2} \\left( W_{i,i+1} - W_{i+1,i} \\right).\n$$\n一个较大的正数 $S$ 表示学习到的、与诱发尖峰序列一致的更强的前向连接性。\n\n实现约束与离散化：\n- 使用时间步长为 $\\Delta t$ 的显式欧拉离散化方法来更新膜电位：\n$$\nv_{j}(t+\\Delta t) = v_{j}(t) + \\frac{\\Delta t}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_{j}(t) + I_{\\mathrm{syn},j}(t) + I_{\\mathrm{ext},j}(t) + I_{\\mathrm{inh}}(t)\\right),\n$$\n在尖峰发生时需满足阈值穿越和不应期重置 $v_{j}\\leftarrow v_{\\mathrm{reset}}$。突触轨迹更新为 $S_{ij}(t+\\Delta t) = S_{ij}(t)\\,e^{-\\Delta t/\\tau_{\\mathrm{syn}}}$，外加尖峰触发的增量。突触前和突触后的 STDP 轨迹在每个时间步同样进行衰减。\n- 模拟必须是确定性的，具有固定的参数且没有随机噪声。兴奋性网络的大小为 $N=5$。不允许自连接，即对所有 $i$，$W_{ii}=0$。\n\nTheta 周期内的 Gamma 脉冲簇生成：\n- 设 theta 周期为 $T_\\theta = 1/f_\\theta$，theta 周期开始时间为 $t_k = k T_\\theta$，其中整数 $k \\ge 0$。\n- 对于每个 theta 周期，安排一个 gamma 脉冲簇在 $t_k + \\phi \\cdot T_\\theta / (2\\pi)$ 开始。在该脉冲簇内，以脉冲间延迟 $\\Delta_{\\gamma}$ 顺序地向神经元 $0,1,\\dots,N-1$ 施加持续时间为 $d_{\\mathrm{pulse}}$、幅度为 $I_{\\mathrm{pulse}}$ 的相同矩形脉冲。因此，神经元 $n$ 在时间区间 $[t_k + \\phi T_\\theta/(2\\pi) + n \\Delta_{\\gamma},\\ t_k + \\phi T_\\theta/(2\\pi) + n \\Delta_{\\gamma} + d_{\\mathrm{pulse}}]$ 上接收其脉冲。\n\n模拟参数和单位：\n- 时间参数：$\\Delta t = 0.0005$ 秒，总模拟时间 $T_{\\mathrm{total}} = 1.0$ 秒，不应期 $t_{\\mathrm{ref}} = 0.005$ 秒，膜时间常数 $\\tau_m = 0.02$ 秒，突触时间常数 $\\tau_{\\mathrm{syn}} = 0.005$ 秒，STDP 时间常数 $\\tau_{+} = 0.02$ 秒和 $\\tau_{-} = 0.02$ 秒。\n- Theta 频率：$f_\\theta = 6.0$ 赫兹。\n- 在此模型中膜电位是无量纲的：$v_{\\mathrm{rest}} = 0.0$，$v_{\\mathrm{th}} = 1.0$，$v_{\\mathrm{reset}} = 0.0$。\n- 突触参数：$g_{\\mathrm{exc}} = 0.05$，初始权重 $W_{ij} = 0.01$（对于 $i \\ne j$），$W_{\\max} = 0.2$，STDP 系数 $A_+ = 0.005$，$A_- = 0.006$。\n- 外部脉冲参数：脉冲幅度 $I_{\\mathrm{pulse}} = 1.2$（无量纲电流单位），脉冲持续时间 $d_{\\mathrm{pulse}} = 0.003$ 秒。\n- 抑制幅度 $A_{\\mathrm{inh}}$ 在每个测试案例中有所不同，并作为负电流起作用；$\\phi$ 的单位是弧度。\n\n要求的输出和测试套件：\n- 实现模拟，为每个指定的参数集计算前向偏置分数 $S$。所有输出均为无单位实数。\n- 用于评估不同 gamma 脉冲簇相位时机和边缘条件的测试套件参数集：\n    1. $\\phi = 0.0$ 弧度，$A_{\\mathrm{inh}} = 0.8$，$\\Delta_{\\gamma} = 0.005$ 秒。\n    2. $\\phi = \\pi/2$ 弧度，$A_{\\mathrm{inh}} = 0.8$，$\\Delta_{\\gamma} = 0.005$ 秒。\n    3. $\\phi = \\pi$ 弧度，$A_{\\mathrm{inh}} = 0.8$，$\\Delta_{\\gamma} = 0.005$ 秒。\n    4. $\\phi = 3\\pi/2$ 弧度，$A_{\\mathrm{inh}} = 0.8$，$\\Delta_{\\gamma} = 0.005$ 秒。\n    5. $\\phi = 3\\pi/2$ 弧度，$A_{\\mathrm{inh}} = 1.6$，$\\Delta_{\\gamma} = 0.005$ 秒。\n    6. $\\phi = 3\\pi/2$ 弧度，$A_{\\mathrm{inh}} = 0.8$，$\\Delta_{\\gamma} = 0.02$ 秒。\n- 最终输出格式：你的程序应生成单行输出，包含一个逗号分隔的结果列表，并用方括号括起来（例如，\"[result1,result2,...]\"）。每个结果是对应测试案例的标量前向偏置分数 $S$。\n\n你的实现必须是一个完整的、可运行的程序，遵守指定的运行时环境和库约束，不接受任何输入，并严格按照描述的格式打印单行输出。角度必须以弧度解释。时间参数必须以秒解释。",
            "solution": "用户提供的问题是计算神经科学领域一个有效、适定且具有科学依据的练习。它要求设计并实现一个模拟，以研究 theta 和 gamma 振荡之间的相位关系如何通过尖峰时间依赖可塑性（STDP）影响序列学习。该模型基于已有的生物物理和计算原理。\n\n该解决方案涉及开发一个漏放电（LIF）神经元网络的离散时间模拟。模型的核心组成部分和模拟算法详述如下。\n\n**1. 漏放电（LIF）神经元模型**\n网络的基本单元是 LIF 神经元，这是一种标准的简化神经元动力学模型。每个神经元 $j$ 的膜电位 $v_j(t)$ 随时间演化，其变化基于流入细胞的净电流。其动力学由以下微分方程控制：\n$$\n\\frac{dv_j}{dt} = \\frac{1}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_j(t) + I_{\\mathrm{total},j}(t)\\right)\n$$\n其中 $\\tau_m = 0.02$ s 是膜时间常数，$v_{\\mathrm{rest}} = 0.0$ 是静息电位，$I_{\\mathrm{total},j}(t)$ 是所有输入电流的总和。当 $v_j(t)$ 达到阈值 $v_{\\mathrm{th}} = 1.0$ 时，神经元发放一个尖峰，其电位被重置为 $v_{\\mathrm{reset}} = 0.0$，并进入一个为期 $t_{\\mathrm{ref}} = 0.005$ s 的不应期，在此期间它不能再次发放尖峰。\n\n为了进行计算实现，我们使用时间步长为 $\\Delta t = 0.0005$ s 的显式欧拉方法对该方程进行离散化：\n$$\nv_{j}(t+\\Delta t) = v_{j}(t) + \\frac{\\Delta t}{\\tau_m}\\left(v_{\\mathrm{rest}} - v_{j}(t) + I_{\\mathrm{total},j}(t)\\right)\n$$\n总电流 $I_{\\mathrm{total},j}(t)$ 是三个分量的总和：$I_{\\mathrm{total},j}(t) = I_{\\mathrm{syn},j}(t) + I_{\\mathrm{ext},j}(t) + I_{\\mathrm{inh}}(t)$。\n\n**2. 节律性和外部输入**\n网络的活动由两个关键的外部电流协调：\n\n- **Theta 节律性抑制 ($I_{\\mathrm{inh}}$):** 一个全局抑制性电流调制网络中所有神经元的兴奋性，模拟抑制性中间神经元群体的效应。该电流遵循一个正弦模式：\n  $$\n  I_{\\mathrm{inh}}(t) = -A_{\\mathrm{inh}}\\left(1 + \\sin(2\\pi f_\\theta t)\\right)\n  $$\n  这里，$f_\\theta = 6.0$ Hz 是 theta 频率，$A_{\\mathrm{inh}}$ 是抑制性振幅，其值在不同测试案例中有所变化。该电流创造了高兴奋性和低兴奋性的周期性窗口。正弦函数的相位决定了抑制水平：在相位 $3\\pi/2, 7\\pi/2, \\dots$ 时抑制最小（$I_{\\mathrm{inh}} = 0$），在相位 $\\pi/2, 5\\pi/2, \\dots$ 时抑制最大（$I_{\\mathrm{inh}} = -2A_{\\mathrm{inh}}$）。\n\n- **Gamma 脉冲簇兴奋 ($I_{\\mathrm{ext}}$):** 为了诱导一个序列发放模式，一个“gamma 脉冲簇”的外部兴奋性电流被施加到神经元上。一个脉冲簇由 $N=5$ 个短暂的矩形脉冲组成，脉冲幅度为 $I_{\\mathrm{pulse}} = 1.2$，持续时间为 $d_{\\mathrm{pulse}} = 0.003$ s。这些脉冲以脉冲间延迟 $\\Delta_{\\gamma}$ 顺序地传递给神经元 $0, 1, \\dots, N-1$。关键的实验参数是相位偏移 $\\phi$，它决定了每个 theta 周期内整个脉冲簇的开始时间。theta 周期 $k$（从 $t_k = k/f_\\theta$ 开始）的脉冲簇在 $t_k + \\phi/(2\\pi f_\\theta)$ 开始，从而将诱导序列的驱动精确地锁相到潜在的 theta 节律上。\n\n**3. 突触传递和可塑性（STDP）**\n$N=5$ 个兴奋性神经元之间的连接是可塑的，这意味着它们的强度（权重 $W_{ij}$）会根据神经活动而演化。\n\n- **突触电流 ($I_{\\mathrm{syn}}$):** 当一个突触前神经元 $i$ 发放尖峰时，它对突触后神经元 $j$ 的输入电流做出贡献。这通过一个突触轨迹 $S_{ij}(t)$ 来建模，当 $i$ 发放尖峰时，该轨迹增加 $1.0$，否则以时间常数 $\\tau_{\\mathrm{syn}} = 0.005$ s 指数衰减。到神经元 $j$ 的总突触电流是所有突触前输入的加权和：\n  $$\n  I_{\\mathrm{syn},j}(t) = g_{\\mathrm{exc}} \\sum_{i \\ne j} W_{ij} S_{ij}(t)\n  $$\n  其中 $g_{\\mathrm{exc}} = 0.05$ 是一个恒定的突触增益。\n\n- **尖峰时间依赖可塑性 (STDP):** 权重 $W_{ij}$ 根据基于配对的 STDP 规则进行更新，该规则根据突触前和突触后尖峰的相对时间来增强或减弱突触。这是通过为每个神经元使用另外两个轨迹来实现的：一个突触前轨迹 $x_i(t)$ 和一个突触后轨迹 $y_j(t)$。两者都以时间常数 $\\tau_{+} = \\tau_{-} = 0.02$ s 指数衰减。\n    - 当突触前神经元 $i$ 发放尖峰时，其轨迹 $x_i$ 增加。此事件会对其传出突触触发长时程抑制（LTD）：$W_{ij} \\leftarrow W_{ij} - A_- y_j$，其中 $A_- = 0.006$，$y_j$ 是神经元 $j$ 的突触后轨迹的当前值。\n    - 当突触后神经元 $j$ 发放尖峰时，其轨迹 $y_j$ 增加。此事件会对其传入突触触发长时程增强（LTP）：$W_{ij} \\leftarrow W_{ij} + A_+ x_i$，其中 $A_+ = 0.005$，$x_i$ 是神经元 $i$ 的突触前轨迹的当前值。\n  在我们的离散模拟中，如果一个神经元发放尖峰，其传入连接的 LTP 和传出连接的 LTD 都是根据尖峰前的轨迹值计算的。然后，它自身的 $x$ 和 $y$ 轨迹增加。权重被裁剪到 $[0, W_{\\max}]$ 范围内，其中 $W_{\\max} = 0.2$。\n\n**4. 模拟算法和度量**\n模拟通过从 $t=0$ 到 $T_{\\mathrm{total}}=1.0$ s 的时间迭代进行。在每个时间步 $\\Delta t$：\n1. 通过对 theta 抑制、外部 gamma 脉冲（如果有的话）以及来自其他神经元的突触电流求和，计算每个神经元的总电流。\n2. 通过欧拉方法更新所有非不应期神经元的膜电位。\n3. 将电位超过 $v_{\\mathrm{th}}$ 的神经元识别为发放尖峰。\n4. 对于每个发放尖峰的神经元，应用 STDP 权重更新（LTP 和 LTD），重置电位，并启动其不应期计数器。相应的突触和 STDP 轨迹增加。\n5. 所有轨迹变量（$S_{ij}$、$x_i$、$y_j$）指数衰减。不应期计数器递减。\n这个过程在整个模拟期间重复进行。\n\n模拟结束后，使用最终的连接矩阵 $W$ 来计算**前向偏置分数** $S$：\n$$\nS = \\frac{1}{N-1} \\sum_{i=0}^{N-2} \\left( W_{i,i+1} - W_{i+1,i} \\right)\n$$\n该度量通过比较期望方向（$W_{i,i+1}$）与反向（$W_{i+1,i}$）的连接强度，来量化网络成功学习前向序列（$0 \\to 1 \\to \\dots \\to 4$）的程度。一个较大的正分表示更强、更准确的序列学习。最终答案中提供的 Python 实现为问题中指定的每个测试案例执行了这个完整的模拟。",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(phi, A_inh, delta_gamma):\n    \"\"\"\n    Runs a single simulation of the spiking network with a given parameter set.\n    \"\"\"\n    # 1. PARAMETERS\n    # Time parameters\n    DT = 0.0005  # s\n    T_TOTAL = 1.0  # s\n    T_REF = 0.005  # s\n    TAU_M = 0.02  # s\n    TAU_SYN = 0.005  # s\n    TAU_PLUS = 0.02  # s\n    TAU_MINUS = 0.02  # s\n\n    # Network and neuron parameters\n    N = 5\n    V_REST = 0.0\n    V_TH = 1.0\n    V_RESET = 0.0\n    G_EXC = 0.05\n    W_MAX = 0.2\n    A_PLUS = 0.005\n    A_MINUS = 0.006\n\n    # Input parameters\n    F_THETA = 6.0\n    I_PULSE = 1.2\n    D_PULSE = 0.003\n    T_THETA = 1.0 / F_THETA\n\n    # Discretized and pre-calculated values\n    NUM_STEPS = int(T_TOTAL / DT)\n    REFRACTORY_STEPS = int(T_REF / DT)\n    EXP_DECAY_SYN = np.exp(-DT / TAU_SYN)\n    EXP_DECAY_PLUS = np.exp(-DT / TAU_PLUS)\n    EXP_DECAY_MINUS = np.exp(-DT / TAU_MINUS)\n\n    # 2. STATE VARIABLES INITIALIZATION\n    v = np.full(N, V_REST, dtype=np.float64)\n    W = np.full((N, N), 0.01, dtype=np.float64)\n    np.fill_diagonal(W, 0)\n    \n    S_trace = np.zeros((N, N), dtype=np.float64)\n    x_trace = np.zeros(N, dtype=np.float64)\n    y_trace = np.zeros(N, dtype=np.float64)\n    refractory_counters = np.zeros(N, dtype=np.int32)\n    \n    # 3. SIMULATION LOOP\n    for step in range(NUM_STEPS):\n        t = step * DT\n\n        # Calculate currents\n        # Inhibitory current\n        I_inh = -A_inh * (1.0 + np.sin(2.0 * np.pi * F_THETA * t))\n\n        # External pulse current\n        I_ext = np.zeros(N, dtype=np.float64)\n        k_cycle = np.floor(t / T_THETA)\n        t_burst_start = k_cycle * T_THETA + phi * T_THETA / (2.0 * np.pi)\n        \n        for n in range(N):\n            t_pulse_start = t_burst_start + n * delta_gamma\n            t_pulse_end = t_pulse_start + D_PULSE\n            if t_pulse_start = t  t_pulse_end:\n                I_ext[n] = I_PULSE\n\n        # Synaptic current\n        I_syn = G_EXC * np.sum(W * S_trace, axis=0) # Sum over presynaptic neurons (axis=0)\n\n        # Total current\n        I_total = I_syn + I_ext + I_inh\n\n        # Update membrane potential for non-refractory neurons\n        non_refractory_mask = (refractory_counters == 0)\n        v[non_refractory_mask] += (DT / TAU_M) * (V_REST - v[non_refractory_mask] + I_total[non_refractory_mask])\n\n        # Check for spikes\n        spiked_neurons = np.where((v >= V_TH)  non_refractory_mask)[0]\n\n        if spiked_neurons.size > 0:\n            x_trace_old = x_trace.copy()\n            y_trace_old = y_trace.copy()\n\n            for s_idx in spiked_neurons:\n                # LTP for incoming synapses\n                W[:, s_idx] += A_PLUS * x_trace_old\n                \n                # LTD for outgoing synapses\n                W[s_idx, :] -= A_MINUS * y_trace_old\n            \n            # Clip weights and zero the diagonal\n            W = np.clip(W, 0, W_MAX)\n            np.fill_diagonal(W, 0)\n\n            for s_idx in spiked_neurons:\n                # Update traces and reset state for spiking neurons\n                x_trace[s_idx] += 1.0\n                y_trace[s_idx] += 1.0\n                S_trace[s_idx, :] += 1.0 # Spike from s_idx affects traces to all other neurons\n                \n                v[s_idx] = V_RESET\n                refractory_counters[s_idx] = REFRACTORY_STEPS\n\n        # Decay traces\n        S_trace *= EXP_DECAY_SYN\n        x_trace *= EXP_DECAY_PLUS\n        y_trace *= EXP_DECAY_MINUS\n        \n        # Update refractory counters\n        refractory_counters[refractory_counters > 0] -= 1\n\n    # 4. CALCULATE FORWARD-BIAS SCORE\n    forward_sum = np.sum(np.diag(W, k=1))  # W[i, i+1]\n    backward_sum = np.sum(np.diag(W, k=-1)) # W[i+1, i]\n    \n    score = (forward_sum - backward_sum) / (N - 1)\n    \n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (phi, A_inh, delta_gamma)\n        (0.0, 0.8, 0.005),\n        (np.pi / 2, 0.8, 0.005),\n        (np.pi, 0.8, 0.005),\n        (3 * np.pi / 2, 0.8, 0.005),\n        (3 * np.pi / 2, 1.6, 0.005),\n        (3 * np.pi / 2, 0.8, 0.020),\n    ]\n\n    results = []\n    for case in test_cases:\n        phi, A_inh, delta_gamma = case\n        result = run_simulation(phi, A_inh, delta_gamma)\n        results.append(result)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了存储和检索机制，另一个核心问题是记忆如何在充满噪声的神经环境中保持其保真度。本练习  将工作记忆建模为一个受突触噪声影响的持续活动状态，并引导您将其形式化为一个Ornstein–Uhlenbeck随机过程。这项分析性挑战旨在让您运用信息论工具（如费雪信息）来量化记忆随时间的衰退，从而将电路的生物物理参数与记忆系统的信息处理能力直接联系起来。",
            "id": "3971126",
            "problem": "考虑一个标量工作记忆变量 $X_t$，它表示一个循环神经网络中维持的值。在其稳定工作点 $X^{\\star}$ 附近，该电路可以被线性化，因此编码 $X_t$ 的群体平均活动遵循一个具有循环稳定和突触散粒噪声的漏积分器，\n$$\nC \\,\\frac{dX_t}{dt} \\;=\\; -g_L\\,X_t \\;+\\; g_R\\,(X^{\\star}-X_t) \\;+\\; J(t),\n$$\n其中 $C$ 是群体平均状态的有效电容（或惯性），$g_L0$ 是有效漏电导，$g_R0$ 是有效循环稳定增益，$J(t)$ 是总突触输入电流。假设 $J(t)$ 是 $N$ 个独立突触过程的总和，每个过程由速率为 $r$ 的齐次泊松尖峰序列驱动，并产生幅度为 $w$、指数核为 $h(t) = \\tau_s^{-1} \\exp(-t/\\tau_s)\\,u(t)$ 的突触后电流，其中 $u(t)$ 是单位阶跃函数且 $\\int_{0}^{\\infty} h(t)\\,dt = 1$。维持值 $X_t$ 从群体活动中线性读出，并且存储的目标 $X^{\\star}$ 在所关心的时间段内是恒定的。假定时间尺度分离区域 $\\tau_s \\ll C/(g_L+g_R)$，因此有色的突触涨落可以在 $X_t$ 的层面上被具有相同零频功率的高斯白噪声近似。\n\n1) 从这些假设出发，仅使用线性系统、泊松散粒噪声和滤波泊松过程的高斯近似的基本定义，推导 $X_t$ 的有效 Ornstein–Uhlenbeck (OU) 随机微分方程，其形式为\n$$\ndX_t \\;=\\; -\\lambda\\,(X_t-\\mu)\\,dt \\;+\\; \\sigma\\, dW_t,\n$$\n确定漂移率 $\\lambda$、不动点均值 $\\mu$ 和扩散幅度 $\\sigma$（用 $C$, $g_L$, $g_R$, $N$, $r$ 和 $w$ 表示）。\n\n2) 将延迟 $t$ 时的记忆保真度定义为在 $X_t$ 的单次观测中包含的关于初始状态 $X_0$ 的费雪信息（FI），并将 OU 参数视为已知。仅使用高斯信道和 OU 过程的基本性质，推导此 FI 的闭式解析表达式，记为 $F(t)$，作为 $t$ 和模型参数的函数。\n\n给出 $F(t)$ 的闭式表达式作为最终答案。最终答案中不要提供中间步骤。最终答案中无需包含单位。如果选择简化，请提供精确的符号简化，而非十进制近似值。",
            "solution": "问题分为两部分。第一部分是推导一个 Ornstein–Uhlenbeck (OU) 过程的参数，该过程近似于给定的神经电路动力学。第二部分是基于稍后时间的观测来计算关于初始状态的费雪信息 (FI)。\n\n### 第1部分：Ornstein-Uhlenbeck 方程的推导\n\n给定工作记忆变量 $X_t$ 的动力学方程：\n$$C \\,\\frac{dX_t}{dt} \\;=\\; -g_L\\,X_t \\;+\\; g_R\\,(X^{\\star}-X_t) \\;+\\; J(t)$$\n其中 $C$ 是电容，$g_L$ 和 $g_R$ 是电导，$X^{\\star}$ 是一个恒定的目标值，$J(t)$ 是一个随机的突触输入电流。\n\n首先，我们重新整理方程，将包含 $X_t$ 的项组合在一起：\n$$C \\,\\frac{dX_t}{dt} \\;=\\; -(g_L+g_R)\\,X_t \\;+\\; g_R\\,X^{\\star} \\;+\\; J(t)$$\n两边除以 $C$，我们得到：\n$$\\frac{dX_t}{dt} \\;=\\; -\\frac{g_L+g_R}{C}\\,X_t \\;+\\; \\frac{g_R\\,X^{\\star}}{C} \\;+\\; \\frac{1}{C}\\,J(t)$$\n随机输入 $J(t)$ 是 $N$ 个独立同分布的突触过程之和，$J(t) = \\sum_{i=1}^{N} J_i(t)$。每个 $J_i(t)$ 是一个滤波后的泊松过程（散粒噪声），其中速率为 $r$ 的尖峰序列与核 $h(t) = \\tau_s^{-1} \\exp(-t/\\tau_s)\\,u(t)$ 进行卷积，并按幅度 $w$ 进行缩放。\n\n我们可以将 $J(t)$ 分解为其均值和一个零均值涨落部分：$J(t) = \\mathbb{E}[J(t)] + \\xi_J(t)$。\n根据散粒噪声的 Campbell 定理，单个过程 $J_i(t)$ 的均值为：\n$$\\mathbb{E}[J_i(t)] \\;=\\; r \\int_{-\\infty}^{\\infty} w\\,h(t')\\,dt' \\;=\\; r\\,w \\int_{0}^{\\infty} \\tau_s^{-1} \\exp(-t'/\\tau_s)\\,dt' \\;=\\; r\\,w$$\n由于这 $N$ 个过程是独立的，总电流的均值为：\n$$\\mathbb{E}[J(t)] \\;=\\; \\sum_{i=1}^{N} \\mathbb{E}[J_i(t)] \\;=\\; N\\,r\\,w$$\n将此代入我们的动力学方程：\n$$\\frac{dX_t}{dt} \\;=\\; -\\frac{g_L+g_R}{C}\\,X_t \\;+\\; \\frac{g_R\\,X^{\\star}}{C} \\;+\\; \\frac{N\\,r\\,w}{C} \\;+\\; \\frac{1}{C}\\,\\xi_J(t)$$\n我们可以将常数项组合在一起，并将方程改写为突出不动点的形式：\n$$\\frac{dX_t}{dt} \\;=\\; -\\frac{g_L+g_R}{C} \\left( X_t - \\frac{g_R\\,X^{\\star} + N\\,r\\,w}{g_L+g_R} \\right) \\;+\\; \\frac{1}{C}\\,\\xi_J(t)$$\n这个方程的形式为 $\\frac{dX_t}{dt} = -\\lambda(X_t - \\mu) + \\eta(t)$，其中 $\\eta(t) = \\frac{1}{C}\\xi_J(t)$ 是有效噪声。\n由此，我们可以确定漂移率 $\\lambda$ 和均值不动点 $\\mu$：\n$$\\lambda \\;=\\; \\frac{g_L+g_R}{C}$$\n$$\\mu \\;=\\; \\frac{g_R\\,X^{\\star} + N\\,r\\,w}{g_L+g_R}$$\n问题要求将有色噪声项 $\\eta(t)$ 近似为高斯白噪声，这相当于将随机微分方程（SDE）写成 Ito 形式：\n$$dX_t \\;=\\; -\\lambda\\,(X_t-\\mu)\\,dt \\;+\\; \\sigma\\, dW_t$$\n为了找到 $\\sigma$，我们必须将白噪声过程的功率与有色噪声 $\\eta(t)$ 的低频功率谱密度 (PSD) 相匹配。白噪声过程 $\\sigma\\,dW_t/dt$ 的 PSD 就是 $\\sigma^2$。$\\eta(t)$ 的 PSD 是 $S_\\eta(\\omega) = \\frac{1}{C^2} S_{\\xi_J}(\\omega)$。\n\n滤波泊松过程的涨落部分 $\\xi_{J,i}(t)$ 的 PSD 由 $S_{\\xi_{J,i}}(\\omega) = r\\,w^2\\,|\\tilde{h}(\\omega)|^2$ 给出，其中 $\\tilde{h}(\\omega)$ 是核 $h(t)$ 的傅里叶变换。\n$$\\tilde{h}(\\omega) \\;=\\; \\int_0^\\infty \\frac{1}{\\tau_s} \\exp(-t/\\tau_s) \\exp(-i\\omega t) \\,dt \\;=\\; \\frac{1}{\\tau_s} \\frac{1}{1/\\tau_s + i\\omega} \\;=\\; \\frac{1}{1+i\\omega\\tau_s}$$\n幅度的平方是 $|\\tilde{h}(\\omega)|^2 = \\frac{1}{1+(\\omega\\tau_s)^2}$。\n由于 $N$ 个突触输入是独立的，它们的 PSD 相加：\n$$S_{\\xi_J}(\\omega) \\;=\\; \\sum_{i=1}^N S_{\\xi_{J,i}}(\\omega) \\;=\\; \\frac{N\\,r\\,w^2}{1+(\\omega\\tau_s)^2}$$\n白噪声近似在条件 $\\tau_s \\ll C/(g_L+g_R)$ 下有效，即 $\\tau_s \\ll 1/\\lambda$。这意味着噪声相关时间 $\\tau_s$ 远小于系统的时间常数 $1/\\lambda$。在这种情况下，我们用其在零频率处的值 $S_{\\xi_J}(0)$ 来近似噪声的 PSD。\n$$S_{\\xi_J}(0) \\;=\\; N\\,r\\,w^2$$\n有效噪声 $\\eta(t) = \\frac{1}{C}\\xi_J(t)$ 在零频率处的 PSD 为：\n$$S_\\eta(0) \\;=\\; \\frac{1}{C^2} S_{\\xi_J}(0) \\;=\\; \\frac{N\\,r\\,w^2}{C^2}$$\n将其与白噪声的 PSD $\\sigma^2$ 相等，我们得到扩散幅度 $\\sigma$：\n$$\\sigma^2 \\;=\\; \\frac{N\\,r\\,w^2}{C^2} \\quad\\implies\\quad \\sigma \\;=\\; \\frac{w}{C}\\sqrt{N\\,r}$$\n\n### 第2部分：费雪信息的推导\n\n我们有 $X_t$ 的 OU 随机微分方程：\n$$dX_t \\;=\\; -\\lambda\\,(X_t-\\mu)\\,dt \\;+\\; \\sigma\\, dW_t$$\n在给定初始状态 $X_0$ 于 $t=0$ 时，此 SDE 的解为：\n$$X_t \\;=\\; \\mu + (X_0 - \\mu)e^{-\\lambda t} + \\sigma \\int_0^t e^{-\\lambda(t-s)}dW_s$$\n这个解表明，对于给定的 $X_0$，变量 $X_t$ 是一个高斯随机变量。我们需要找到它的条件均值和方差。Ito 积分的期望为零。\n$$\\mathbb{E}[X_t | X_0] \\;=\\; \\mu + (X_0 - \\mu)e^{-\\lambda t}$$\n方差由 Ito 等距性质给出：\n$$\\text{Var}(X_t | X_0) \\;=\\; \\mathbb{E}\\left[\\left(\\sigma \\int_0^t e^{-\\lambda(t-s)}dW_s\\right)^2\\right] \\;=\\; \\sigma^2 \\int_0^t e^{-2\\lambda(t-s)}ds$$\n$$\\text{Var}(X_t | X_0) \\;=\\; \\sigma^2 \\left[\\frac{e^{-2\\lambda(t-s)}}{2\\lambda}\\right]_0^t \\;=\\; \\frac{\\sigma^2}{2\\lambda}(1-e^{-2\\lambda t})$$\n让我们将条件均值记为 $\\mu_t(X_0) = \\mathbb{E}[X_t | X_0]$，条件方差记为 $V_t = \\text{Var}(X_t | X_0)$。\n从观测值 $X_t$ 中得到的关于参数 $X_0$ 的费雪信息 (FI) $F(t)$ 由以下公式给出：\n$$F(t) \\;=\\; \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial X_0} \\ln p(X_t|X_0)\\right)^2\\right]$$\n对于高斯分布 $p(x|\\theta) = \\mathcal{N}(\\mu(\\theta), V)$，其中方差 $V$ 与参数 $\\theta$ 无关，FI 简化为：\n$$F(\\theta) \\;=\\; \\frac{1}{V}\\left(\\frac{\\partial \\mu(\\theta)}{\\partial \\theta}\\right)^2$$\n在我们的情况下，$\\theta=X_0$，$\\mu(\\theta)=\\mu_t(X_0)$，且 $V=V_t$。首先，我们计算均值对 $X_0$ 的导数：\n$$\\frac{\\partial \\mu_t(X_0)}{\\partial X_0} \\;=\\; \\frac{\\partial}{\\partial X_0} \\left( \\mu + (X_0 - \\mu)e^{-\\lambda t} \\right) \\;=\\; e^{-\\lambda t}$$\n现在，我们将此结果和方差 $V_t$ 代入 FI 公式：\n$$F(t) \\;=\\; \\frac{(e^{-\\lambda t})^2}{V_t} \\;=\\; \\frac{e^{-2\\lambda t}}{\\frac{\\sigma^2}{2\\lambda}(1-e^{-2\\lambda t})}$$\n$$F(t) \\;=\\; \\frac{2\\lambda e^{-2\\lambda t}}{\\sigma^2 (1-e^{-2\\lambda t})}$$\n这可以改写为：\n$$F(t) \\;=\\; \\frac{2\\lambda}{\\sigma^2} \\frac{1}{e^{2\\lambda t} - 1}$$\n最后，我们将第1部分中推导出的 $\\lambda$ 和 $\\sigma^2$ 的表达式（用原始模型参数表示）代入：\n$$\\lambda = \\frac{g_L+g_R}{C} \\quad \\text{and} \\quad \\sigma^2 = \\frac{N\\,r\\,w^2}{C^2}$$\n前置因子变为：\n$$\\frac{2\\lambda}{\\sigma^2} \\;=\\; \\frac{2(g_L+g_R)/C}{N\\,r\\,w^2/C^2} \\;=\\; \\frac{2(g_L+g_R)}{C} \\frac{C^2}{N\\,r\\,w^2} \\;=\\; \\frac{2C(g_L+g_R)}{N\\,r\\,w^2}$$\n指数为 $2\\lambda t = 2\\frac{g_L+g_R}{C}t$。\n将这些代入 $F(t)$ 的表达式中，得到最终答案。\n$$F(t) \\;=\\; \\frac{2C(g_L+g_R)}{N\\,r\\,w^2} \\frac{1}{\\exp\\left(2\\frac{g_L+g_R}{C}t\\right) - 1}$$",
            "answer": "$$\\boxed{\\frac{2C(g_L+g_R)}{Nrw^2} \\frac{1}{\\exp\\left(2t\\frac{g_L+g_R}{C}\\right) - 1}}$$"
        }
    ]
}