## 引言
Actor-Critic（[演员-评论家](@entry_id:634214)）架构是强化学习领域中一类功能强大且应用广泛的算法，专门用于解决复杂的[序贯决策](@entry_id:145234)与控制问题。通过巧妙地结合纯策略方法（直接学习行为策略）和纯价值方法（学习评估状态或动作的价值）的优点，它在处理高维[状态空间](@entry_id:160914)和连续动作空间方面表现出色，成为理论研究和实际应用中的主流范式。然而，要完全发挥其潜力，必须深刻理解其内在的数学原理、不同组件间的相互作用，以及如何克服高方差和学习不稳定性等固有挑战。

本文旨在系统性地剖析[Actor-Critic架构](@entry_id:1120755)。在**“原理与机制”**章节中，我们将深入其数学核心，从价值函数和[策略梯度定理](@entry_id:635009)出发，揭示评论家（Critic）如何通过[时序差分学习](@entry_id:177975)提供评估信号，以及演员（Actor）如何利用该信号改进策略。接着，在**“应用与交叉学科联系”**章节中，我们将展示该架构如何作为[计算模型](@entry_id:637456)来解释大脑的学习机制，并探讨A2C、DDPG、SAC等前沿算法如何解决现代控制任务中的复杂挑战。最后，**“动手实践”**部分将提供一系列练习，旨在将理论知识转化为解决实际问题的能力。通过本篇学习，您将对Actor-Critic方法论建立一个坚实而全面的理解。

## 原理与机制

在介绍章节之后，我们现在深入探讨Actor-Critic（[演员-评论家](@entry_id:634214)）架构的数学原理和核心机制。本章将从控制问题的基本目标出发，系统地构建Actor-Critic方法论，并阐明其在[价值函数](@entry_id:144750)估计、[策略改进](@entry_id:139587)以及两者之间复杂相互作用中的关键思想。我们将探讨这些方法为何在[计算神经科学](@entry_id:274500)和工程中如此强大，并分析确保其稳定性和效率的理论条件。

### [价值函数](@entry_id:144750)与控制目标

在[强化学习](@entry_id:141144)中，智能体的目标是学习一个策略 $\pi$，使其在与环境交互时最大化预期的累积回报。在一个由[马尔可夫决策过程](@entry_id:140981)（MDP）描述的环境中，这个目标可以被形式化。对于一个给定的策略 $\pi$，其性能由一个目标函数 $J(\pi)$ 来衡量，该函数表示从一个初始状态分布 $d_0(s)$ 开始，遵循策略 $\pi$ 所能获得的期望[折扣](@entry_id:139170)回报总和。

具体来说，在时间步 $t$ 的总[折扣](@entry_id:139170)回报定义为 $G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}$，其中 $\gamma \in (0, 1)$ 是[折扣](@entry_id:139170)因子，它决定了未来奖励相对于即时奖励的重要性。[目标函数](@entry_id:267263) $J(\pi)$ 则是该回报在初始状态下的[期望值](@entry_id:150961)：

$J(\pi) = \mathbb{E}_{\pi, s_0 \sim d_0} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right]$

为了评估一个策略，我们引入两个核心概念：**状态价值函数（state-value function）** $V^{\pi}(s)$ 和 **动作[价值函数](@entry_id:144750)（action-value function）** $Q^{\pi}(s,a)$。

- **状态价值函数** $V^{\pi}(s)$ 是从状态 $s$ 开始，并始终遵循策略 $\pi$ 的期望回报：
  $V^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) | s_0=s \right]$

- **动作[价值函数](@entry_id:144750)** $Q^{\pi}(s,a)$ 是从状态 $s$ 开始，执行动作 $a$，然后继续遵循策略 $\pi$ 的期望回报：
  $Q^{\pi}(s,a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) | s_0=s, a_0=a \right]$

这两个价值函数是相互关联的，并且它们都遵循一个重要的[自洽性](@entry_id:160889)条件，即**贝尔曼期望方程（Bellman expectation equation）**。通过将[价值函数](@entry_id:144750)的定义展开一步，并利用[马尔可夫性质](@entry_id:139474)，我们可以推导出这些方程 。

对于状态价值函数 $V^{\pi}(s)$，我们可以将其表示为所有可能动作的期望回报的加权平均，其中权重由策略 $\pi(a|s)$ 给出。每个动作的回报由即时奖励 $r(s,a)$ 和下一个状态 $s'$ 的[折扣](@entry_id:139170)价值 $\gamma V^{\pi}(s')$ 组成：

$V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left( r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V^{\pi}(s') \right)$

类似地，对于动作[价值函数](@entry_id:144750) $Q^{\pi}(s,a)$，它等于执行动作 $a$ 后的即时奖励，加上所有可能的下一个状态 $s'$ 的期望未来回报：

$Q^{\pi}(s,a) = r(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) V^{\pi}(s')$

并且，$V^{\pi}(s)$ 可以通过 $Q^{\pi}(s,a)$ 表示：$V^{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^{\pi}(s,a)$。

对于一个具有有限状态和动作空间的MDP，以及一个固定的策略 $\pi$，贝尔曼期望方程构成了一个关于所有状态 $s$ 的 $V^{\pi}(s)$ 的[线性方程组](@entry_id:148943)。我们可以将其写成矩阵形式。设 $V^{\pi}$ 是[价值函数](@entry_id:144750)的列向量，$r_{\pi}$ 是策略下的期望即时奖励向量（其元素为 $r_{\pi}(s) = \sum_a \pi(a|s)r(s,a)$），$P_{\pi}$ 是策略下的[状态转移矩阵](@entry_id:269075)（其元素为 $P_{\pi}(s'|s) = \sum_a \pi(a|s)P(s'|s,a)$）。那么，[贝尔曼方程](@entry_id:1121499)变为：

$V^{\pi} = r_{\pi} + \gamma P_{\pi} V^{\pi}$

通过求解这个方程，我们可以得到策略 $\pi$ 的[价值函数](@entry_id:144750)的[闭式](@entry_id:271343)解：

$V^{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi}$

其中 $I$ 是单位矩阵。有了[价值函数](@entry_id:144750)，$J(\pi)$ 就可以表示为初始状态分布 $d_0$ 与价值向量 $V^{\pi}$ 的点积：$J(\pi) = d_0 V^{\pi}$。这个表达式为我们提供了一个直接评估任何给定策略性能的数学工具，尽管在实践中，模型（$P_\pi, r_\pi$）通常是未知的，且[状态空间](@entry_id:160914)可能非常大甚至连续，这使得直接求解变得不可行 。

### Actor-Critic范式：广义策略迭代的实现

[Actor-Critic架构](@entry_id:1120755)优雅地将控制问题分解为两个独立的、相互作用的模块：**演员（Actor）**和**评论家（Critic）**。这种结构是**广义策略迭代（Generalized Policy Iteration, GPI）** 的一种具体实现 。GPI是一个通用框架，它交替进行两个过程：

1.  **[策略评估](@entry_id:136637)（Policy Evaluation）**：给定当前策略，估计其价值函数。
2.  **[策略改进](@entry_id:139587)（Policy Improvement）**：根据当前的价值函数估计，改进策略以获得更高的回报。

在[Actor-Critic架构](@entry_id:1120755)中：
- **评论家（Critic）** 扮演着[策略评估](@entry_id:136637)的角色。它学习一个[参数化](@entry_id:265163)的[价值函数](@entry_id:144750)，$V_w(s) \approx V^{\pi_\theta}(s)$ 或 $Q_w(s,a) \approx Q^{\pi_\theta}(s,a)$，以评估当前演员策略 $\pi_\theta$ 的好坏。
- **演员（Actor）** 扮演着[策略改进](@entry_id:139587)的角色。它是一个[参数化](@entry_id:265163)的策略 $\pi_\theta(a|s)$，根据评论家提供的评估信号来更新其参数 $\theta$，目的是使策略变得更好。

这种分离结合了纯策略方法和纯价值方法的优点。纯策略方法（如REINFORCE）直接优化策略参数，非常适合处理连续动作空间，但由于其[梯度估计](@entry_id:164549)通常基于完整的[蒙特卡洛](@entry_id:144354)回报，因此方差很高。纯价值方法（如Q-learning）通过学习价值函数来间接得到策略，它们利用**自举（bootstrapping）**来更新价值估计，从而降低了方差，提高了样本效率，但在连续动作空间中，从[Q值](@entry_id:265045)中贪婪地选择动作（$\arg\max_a Q(s,a)$）可能非常困难。

Actor-Critic方法通过让评论家使用自举的TD学习来降低方差，同时保留演员对策略的直接、可微的控制，从而在两者之间取得了平衡 。

### 评论家的作用：自举、[偏差与方差](@entry_id:894392)

评论家的核心任务是提供一个准确且低方差的评估信号。这通常通过**时序差分（Temporal Difference, TD）学习**来实现。评论家试图最小化[TD误差](@entry_id:634080) $\delta_t$，从而使其价值估计满足[贝尔曼方程](@entry_id:1121499)。对于一个V-Critic，[TD误差](@entry_id:634080)定义为：

$\delta_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t)$

这个TD误差信号在[计算神经科学](@entry_id:274500)中具有重要意义，它被认为是中脑[多巴胺神经元](@entry_id:924924)发放[奖励预测误差](@entry_id:164919)信号的[计算模型](@entry_id:637456) 。

使用TD学习引入了一个核心的权衡：**偏差-方差权衡（bias-variance trade-off）**。为了清晰地理解这一点，我们可以考虑一个简单的单状态MDP，其奖励是[独立同分布](@entry_id:169067)的，均值为 $\mu$，方差为 $\sigma^2$ 。

- **[蒙特卡洛](@entry_id:144354)（MC）方法**：使用完整的[折扣](@entry_id:139170)回报 $G = \sum_{t=0}^{\infty} \gamma^t r_t$ 作为价值的目标。$G$ 是真实价值 $V^* = \mu/(1-\gamma)$ 的一个无偏估计，但它的方差很大，为 $\mathrm{Var}(G) = \sigma^2 / (1-\gamma^2)$，因为它包含了整个轨迹的随机性。

- **时序差分（TD）方法**：使用一步TD目标 $Y = r_0 + \gamma \hat{V}$，其中 $\hat{V}$ 是评论家对真实价值的当前估计。
  - **偏差**：$Y$的[期望值](@entry_id:150961)是 $\mathbb{E}[Y] = \mu + \gamma \mathbb{E}[\hat{V}]$。其偏差为 $\mathrm{Bias}(Y) = \mathbb{E}[Y] - V^* = \gamma(\mathbb{E}[\hat{V}] - V^*)$。这意味着，如果评论家的估计本身是带偏的，TD目标也会引入偏差。
  - **方差**：$Y$的方差是 $\mathrm{Var}(Y) = \sigma^2 + \gamma^2 \mathrm{Var}(\hat{V})$。只要评论家的估计方差 $\mathrm{Var}(\hat{V})$ 远小于MC回报的方差 $\mathrm{Var}(G)$（在实践中通常如此），TD目标的方差就会显著低于MC目标。

这个权衡是设计评论家的核心。高自举（如一步TD）会带来低方差和潜在的高偏差，而低自举（如多步TD或$\lambda$接近1的TD($\lambda$)）则会增加方差，但减少偏差。在控制任务中，必须在由低方差带来的样本效率和由低偏差带来的渐近性能与稳定性之间找到平衡。

### 演员的改进：[优势函数](@entry_id:635295)的力量

为了改进策略，演员需要一个信号来告诉它在特定状态下，哪些动作比平均水平更好或更差。这个信号就是**[优势函数](@entry_id:635295)（Advantage Function）**：

$A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$

[优势函数](@entry_id:635295)衡量了在状态 $s$ 下采取动作 $a$ 相对于遵循策略 $\pi$ 的平均表现所带来的额外价值。[策略梯度定理](@entry_id:635009)表明，策略的梯度可以表示为：

$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \ln \pi_\theta(a|s) Q^{\pi_\theta}(s,a) \right]$

一个关键的洞见是，我们可以从 $Q^{\pi_\theta}(s,a)$ 中减去一个不依赖于动作的**基线（baseline）** $b(s)$，而不会改变梯度的[期望值](@entry_id:150961)，因为 $\mathbb{E}_{a \sim \pi_\theta}[\nabla_\theta \ln \pi_\theta(a|s) b(s)] = 0$ 。一个理想的基线是状态价值函数 $V^{\pi_\theta}(s)$。使用这个基线后，[策略梯度](@entry_id:635542)变为：

$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \ln \pi_\theta(a|s) A^{\pi_\theta}(s,a) \right]$

使用[优势函数](@entry_id:635295)而非[Q值](@entry_id:265045)作为更新信号，可以显著降低[策略梯度](@entry_id:635542)估计的方差，因为基线减去了状态价值的共同部分，使得更新信号更关注于动作的相对好坏。

奇妙的是，用于训练评论家的[TD误差](@entry_id:634080) $\delta_t$ 正是[优势函数](@entry_id:635295)的一个绝佳估计。我们可以证明，在[价值函数](@entry_id:144750)估计准确的情况下，TD误差的[期望值](@entry_id:150961)恰好是[优势函数](@entry_id:635295) ：

$\mathbb{E}[r_t + \gamma V^{\pi}(s_{t+1}) - V^{\pi}(s_t) | s_t, a_t] = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t) = A^{\pi}(s_t, a_t)$

因此，[Actor-Critic架构](@entry_id:1120755)形成了一个优雅的闭环：评论家通过最小化[TD误差](@entry_id:634080)来学习[价值函数](@entry_id:144750)，而这个[TD误差](@entry_id:634080)本身就可以作为[优势函数](@entry_id:635295)的低方差、单步估计，直接用于指导演员的[策略改进](@entry_id:139587)。

### 核心Actor-Critic算法

基于上述原理，我们可以构建几种不同类型的Actor-Critic算法 。

#### 优势[演员-评论家](@entry_id:634214) (A2C-style)

这是最常见的随机策略AC架构。
- **演员**: 随机策略 $\pi_\theta(a|s)$。
- **评论家**: 状态[价值函数](@entry_id:144750) $V_w(s)$。
- **评论家更新**: 评论家通过[随机梯度下降](@entry_id:139134)最小化[TD误差](@entry_id:634080)的平方，更新其参数 $w$：
  $w \leftarrow w + \beta \delta_t \nabla_w V_w(s_t)$，其中 $\delta_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t)$。
- **演员更新**: 演员使用TD误差作为[优势函数](@entry_id:635295)的估计，沿着[策略梯度](@entry_id:635542)的方向更新其参数 $\theta$：
  $\theta \leftarrow \theta + \alpha (\nabla_\theta \ln \pi_\theta(a_t|s_t)) \delta_t$。

#### Q-[演员-评论家](@entry_id:634214)

这种变体使用动作价值函数作为评论家。
- **演员**: 随机策略 $\pi_\theta(a|s)$。
- **评论家**: 动作价值函数 $Q_w(s,a)$。
- **评论家更新**: 评论家最小化[Q值](@entry_id:265045)的[TD误差](@entry_id:634080)。例如，使用SARSA式的目标：
  $\delta_t = r_t + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t)$。
- **演员更新**: 演员使用评论家的[Q值](@entry_id:265045)作为更新信号：
  $\theta \leftarrow \theta + \alpha (\nabla_\theta \ln \pi_\theta(a_t|s_t)) Q_w(s_t, a_t)$。通常也会减去一个基线（如 $V_w(s_t)$）来降低方差。

#### 确定性[演员-评论家](@entry_id:634214) (DDPG-style)

这种架构适用于连续动作空间，它学习一个确定性策略。
- **演员**: 确定性策略 $a = \mu_\theta(s)$。
- **评论家**: 动作[价值函数](@entry_id:144750) $Q_w(s,a)$。
- **评论家更新**: 评论家的更新类似于Q-learning，使用来自演员的目标动作：
  $y_t = r_t + \gamma Q_{w'}(s_{t+1}, \mu_{\theta'}(s_{t+1}))$，其中 $w'$ 和 $\theta'$ 来自缓慢更新的**[目标网络](@entry_id:635025)（target networks）**，以稳定学习。评论家通过最小化 $(y_t - Q_w(s_t, a_t))^2$ 来更新。
- **演员更新**: 由于策略是确定性的，[策略梯度](@entry_id:635542)不能再使用[似然比](@entry_id:170863)技巧。取而代之的是，演员的目标是选择能最大化评论家输出的[Q值](@entry_id:265045)的动作。因此，演员沿着 $Q_w(s, \mu_\theta(s))$ 对 $\theta$ 的梯度方向更新，这需要使用链式法则：
  $\nabla_\theta J(\theta) \approx \mathbb{E}_s[\nabla_\theta \mu_\theta(s) \nabla_a Q_w(s,a)|_{a=\mu_\theta(s)}]$。

### 高级主题：稳定性和[离策略学习](@entry_id:634676)

#### 收敛性与双时间尺度框架

在Actor-Critic算法中，演员和评论家是相互依赖的。评论家的目标是学习当前演员策略的价值函数，而演员又依赖于评论家的输出来更新自己。如果两者以相似的速度更新，就会出现“移动目标”问题：评论家永远在追逐一个不断变化的策略，导致其价值估计不准确，从而给演员一个噪声大且有偏的梯度信号，可能导致振荡或发散。

为了保证收敛性，需要采用**双时间尺度（two-timescale）**[随机近似](@entry_id:270652)框架 。这意味着评论家的学习率 $\beta_t$ 必须比演员的[学习率](@entry_id:140210) $\alpha_t$ “快”得多，形式上要求 $\alpha_t / \beta_t \to 0$。

- **直观解释**: 评论家需要在演员的策略发生显著变化之前，有足够的时间（或更大的更新步长）来接近当前策略的真实价值函数。这样，当演员更新时，它看到的是一个相对稳定和准确的评估信号。
- **形式化分析**: 这种时间尺度分离可以通过[奇异摄动理论](@entry_id:164182)进行分析。它将耦合的动力学系统[解耦](@entry_id:160890)：快速子系统（评论家）被认为在其准静态的慢变量（演员）的每个值上都达到了平衡。然后，慢速子系统（演员）的演化由一个平均化的常微分方程（ODE）描述，其中评论家的值被其平衡值所取代。这种[解耦](@entry_id:160890)减轻了同时间尺度更新中可能破坏稳定性的自举反馈。

#### 离策略（Off-Policy）学习

为了提高样本效率，现代Actor-Critic方法经常使用[经验回放](@entry_id:634839)池，这要求算法能够**离策略（off-policy）**学习——即从行为策略 $\mu$ 产生的经验中学习目标策略 $\pi_\theta$。

当[策略梯度](@entry_id:635542)是根据从不同策略 $\mu$ 中采样的动作计算时，必须进行校正以避免偏差。这通过**重要性采样（Importance Sampling, IS）**实现。对于单步Actor-Critic更新，我们只需要校正当前决策步的[动作选择](@entry_id:151649)概率。修正后的[策略梯度](@entry_id:635542)项为 ：

$\hat{g}_t = \rho_t (\nabla_\theta \ln \pi_\theta(a_t|s_t)) A^{\pi_\theta}(s_t, a_t)$

其中 $\rho_t = \frac{\pi_\theta(a_t|s_t)}{\mu(a_t|s_t)}$ 是**逐决策重要性采样比（per-decision importance sampling ratio）**。这个比率校正了在 $s_t$ 处选择动作 $a_t$ 的概率差异。我们之所以只需要校正这一步，是因为[优势函数](@entry_id:635295) $A^{\pi_\theta}$ 的定义已经隐含了从该步之后遵循目标策略 $\pi_\theta$ 的所有未来回报。

例如，在一个单状态双动作问题中，如果行为策略 $\mu(a=1)=0.8$，目标策略 $\pi_\alpha(a=1)=0.6$，那么当观察到动作 $a=1$ 时，其[重要性采样](@entry_id:145704)比为 $\rho = 0.6/0.8 = 0.75$。这个比率会乘以该步的[策略梯度](@entry_id:635542)贡献，以得到一个对真实梯度的无偏估计。完整的计算实例可以在  中找到。

#### 致命三元组与可证明收敛的方法

[离策略学习](@entry_id:634676)与自举（TD学习）和[函数逼近](@entry_id:141329)（尤其是[非线性](@entry_id:637147)的）的结合，构成了所谓的“**致命三元组（deadly triad）**”，这可能导致[价值函数](@entry_id:144750)估计的无限发散。

虽然像DDPG这样的算法在实践中通过[目标网络](@entry_id:635025)和[经验回放](@entry_id:634839)等技巧取得了巨大成功，但它们缺乏严格的收敛保证。为了解决这个问题，研究界开发了具有可证明收敛性的离策略算法 。

- **在策略（On-policy）学习是稳定的**: 值得注意的是，如果学习是在策略下进行的，即行为策略与目标策略相同，那么即使结合了自举和线性[函数逼近](@entry_id:141329)，TD学习也是稳定的。
- **梯度TD方法（Gradient TD）**: 像GTD2和TDC这样的算法通过将评论家的更新重新构造为在一个明确定义的**均方投影[贝尔曼误差](@entry_id:636460)（MSPBE）**[目标函数](@entry_id:267263)上的[随机梯度下降](@entry_id:139134)，来保证离策略设置下的收敛性。
- **强调TD方法（Emphatic TD）**: 这类方法通过引入一个额外的“强调”迹来重新加权TD更新。这种加权被精心设计，以确保底层的有效贝尔曼算子在一个加权范数下是收缩的，从而保证了收敛性。

这些高级方法为在复杂场景下设计稳定可靠的Actor-Critic智能体提供了坚实的理论基础，尽管它们的实现可能更为复杂。