{
    "hands_on_practices": [
        {
            "introduction": "演员-评论家（Actor-Critic）方法的核心在于评论家如何为演员提供有效的学习信号。此练习将引导您探索确保策略梯度无偏的一个关键理论条件——“兼容函数近似”（compatible function approximation） 。通过从第一性原理构建一个兼容的评论家特征集并计算相应的演员更新，您将深入理解策略梯度定理在实践中的应用。",
            "id": "3962001",
            "problem": "一个智能体在固定状态下执行单步控制。演员（actor）是一个关于连续动作的随机策略，而评论家（critic）使用线性函数逼近器。该策略是高斯分布，其方差固定，均值由状态依赖特征的线性形式给出。控制成本是动作与状态的目标仿射函数偏差的二次函数。施加了演员和评论家之间的兼容性条件，以确保使用评论家计算出的演员更新与真实策略梯度一致。\n\n考虑单个状态 $s \\in \\mathbb{R}$，其状态特征向量 $x(s) \\in \\mathbb{R}^{2}$ 定义为 $x(s) = \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}$。演员的策略是一个高斯分布 $\\pi_{\\theta}(a \\mid s) = \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^{2})$，其方差固定为 $\\sigma^{2} = 1$，均值为 $\\mu_{\\theta}(s) = \\theta^{\\top} x(s)$，其中 $\\theta \\in \\mathbb{R}^{2}$ 是参数向量。在状态 $s$ 和动作 $a$ 下的即时控制成本（负奖励）为\n$$\n\\ell(s,a) = \\frac{1}{2}\\,q\\,(a - \\kappa s)^{2} + \\frac{1}{2}\\,c\\,s^{2},\n$$\n其中 $q > 0$，$\\kappa \\in \\mathbb{R}$，$c > 0$。这个单步问题的动作价值函数是 $Q(s,a) = -\\ell(s,a)$。评论家使用线性函数类 $Q_{w}(s,a) = \\phi(s,a)^{\\top} w$，$w \\in \\mathbb{R}^{2}$，其中特征映射 $\\phi(s,a)$ 必须满足演员和评论家之间的兼容性要求。\n\n任务：\n1. 使用演员-评论家架构中函数逼近的兼容性定义，构建与上述高斯演员兼容的评论家特征向量 $\\phi(s,a)$。\n2. 对于固定状态 $s = 1.5$，参数 $\\theta = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$，以及控制成本系数 $q = 2$，$\\kappa = 0.8$，$c = 0.4$，显式计算演员更新向量\n$$\ng(\\theta) \\equiv \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)}\\!\\left[\\,\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\\,Q_{w}(s,a)\\,\\right]\n$$\n其中评论家参数 $w$ 的选择是为了在给定状态 $s$ 下，根据策略诱导的动作分布，将 $Q(s,a)$ 投影到由 $\\phi(s,a)$ 张成的空间上，从而求解最小二乘问题。将您对 $g(\\theta)$ 的最终数值答案表示为使用 LaTeX `\\begin{pmatrix}` 环境的单个行向量。不要四舍五入；以有限小数或分数形式提供精确值。",
            "solution": "用户提供了来自强化学习和控制理论领域的有效问题陈述。该问题提法恰当，具有科学依据，并包含推导唯一解所需的所有必要信息。\n\n问题分为两个任务。首先，为评论家构建一个兼容的特征向量。其次，为一组特定参数计算演员更新向量。\n\n**任务 1：构建评论家的特征向量 $\\phi(s,a)$**\n\n带函数逼近的演员-评论家方法理论建立了一个演员和评论家之间的兼容性条件，以确保策略梯度是无偏的。对于形式为 $Q_{w}(s,a) = \\phi(s,a)^{\\top} w$ 的线性评论家，如果评论家的特征向量 $\\phi(s,a)$ 被选择为演员策略的得分函数（score function），即对数策略关于其参数 $\\theta$ 的梯度，则可实现兼容性。\n$$\n\\phi(s,a) = \\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\n$$\n演员的策略是一个高斯分布 $\\pi_{\\theta}(a \\mid s) = \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^{2})$，其概率密度函数为：\n$$\n\\pi_{\\theta}(a \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2}\\right)\n$$\n策略的自然对数为：\n$$\n\\ln \\pi_{\\theta}(a \\mid s) = -\\frac{1}{2} \\ln(2\\pi\\sigma^2) - \\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2}\n$$\n为了求得关于 $\\theta$ 的梯度，我们对此表达式进行微分。第一项相对于 $\\theta$ 是一个常数。\n$$\n\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = \\nabla_{\\theta} \\left( -\\frac{(a - \\mu_{\\theta}(s))^2}{2\\sigma^2} \\right)\n$$\n使用链式法则，我们得到：\n$$\n\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = -\\frac{1}{2\\sigma^2} \\cdot 2(a - \\mu_{\\theta}(s)) \\cdot (-\\nabla_{\\theta} \\mu_{\\theta}(s)) = \\frac{a - \\mu_{\\theta}(s)}{\\sigma^2} \\nabla_{\\theta} \\mu_{\\theta}(s)\n$$\n策略均值定义为 $\\mu_{\\theta}(s) = \\theta^{\\top} x(s)$。其关于参数向量 $\\theta$ 的梯度是：\n$$\n\\nabla_{\\theta} \\mu_{\\theta}(s) = \\nabla_{\\theta} (\\theta^{\\top} x(s)) = x(s)\n$$\n将此结果代入得分函数的表达式，我们得到兼容的特征向量：\n$$\n\\phi(s,a) = \\frac{a - \\theta^{\\top} x(s)}{\\sigma^2} x(s)\n$$\n给定固定方差 $\\sigma^2 = 1$ 和状态特征向量 $x(s) = \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}$，评论家的特征向量为：\n$$\n\\phi(s,a) = (a - \\theta^{\\top} x(s)) x(s) = (a - (\\theta_1 s + \\theta_2 s^3)) \\begin{pmatrix} s \\\\ s^{3} \\end{pmatrix}\n$$\n\n**任务 2：计算演员更新向量 $g(\\theta)$**\n\n演员更新向量定义为：\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)}\\!\\left[\\,\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s)\\,Q_{w}(s,a)\\,\\right]\n$$\n问题陈述指出，评论家的权重向量 $w$ 是通过在策略诱导的动作分布下，将真实动作价值函数 $Q(s,a)$ 最小二乘投影到由 $\\phi(s,a)$ 张成的特征空间上来确定的。这意味着 $w$ 最小化 $\\mathbb{E}_{a \\sim \\pi_{\\theta}}[(Q_w(s,a) - Q(s,a))^2]$。解由正规方程组给出：\n$$\n\\left( \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top}] \\right) w = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) Q(s,a)]\n$$\n将 $Q_w(s,a) = \\phi(s,a)^{\\top} w$ 和 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) = \\phi(s,a)$ 代入 $g(\\theta)$ 的定义：\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top} w] = \\left( \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) \\phi(s,a)^{\\top}] \\right) w\n$$\n从正规方程组可知，这意味着演员更新 $g(\\theta)$ 正是正规方程组的右侧：\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\phi(s,a) Q(s,a)] = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\ln \\pi_{\\theta}(a \\mid s) Q(s,a)]\n$$\n这就是这个单步问题的真实策略梯度。现在我们计算这个期望。真实的动作价值函数是 $Q(s,a) = -\\ell(s,a) = -\\frac{1}{2}q(a-\\kappa s)^2 - \\frac{1}{2}c s^2$。\n$$\ng(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}} \\left[ \\left(\\frac{a - \\mu_{\\theta}(s)}{\\sigma^2} x(s)\\right) \\left(-\\frac{1}{2}q(a-\\kappa s)^2 - \\frac{1}{2}c s^2\\right) \\right]\n$$\n我们可以将不依赖于随机变量 $a$ 的项提取出来：\n$$\ng(\\theta) = \\frac{x(s)}{\\sigma^2} \\mathbb{E}_{a \\sim \\pi_{\\theta}} \\left[ (a - \\mu_{\\theta}(s)) \\left(-\\frac{q}{2}(a-\\kappa s)^2 - \\frac{c}{2}s^2\\right) \\right]\n$$\n让我们定义一个零均值随机变量 $\\epsilon = a - \\mu_{\\theta}(s)$。因为 $a \\sim \\mathcal{N}(\\mu_{\\theta}(s), \\sigma^2)$，所以 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$。动作为 $a = \\mu_{\\theta}(s) + \\epsilon$。期望变为：\n$$\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)} \\left[ \\epsilon \\left( -\\frac{q}{2}(\\mu_{\\theta}(s) + \\epsilon - \\kappa s)^2 - \\frac{c}{2}s^2 \\right) \\right]\n$$\n让我们展开期望内的项：\n$$\n\\epsilon \\left( -\\frac{q}{2} [(\\mu_{\\theta}(s) - \\kappa s) + \\epsilon]^2 - \\frac{c}{2}s^2 \\right) = \\epsilon \\left( -\\frac{q}{2} [(\\mu_{\\theta}(s) - \\kappa s)^2 + 2\\epsilon(\\mu_{\\theta}(s) - \\kappa s) + \\epsilon^2] - \\frac{c}{2}s^2 \\right)\n$$\n分配 $\\epsilon$：\n$$\n-\\frac{q}{2}(\\mu_{\\theta}(s) - \\kappa s)^2 \\epsilon - q(\\mu_{\\theta}(s) - \\kappa s) \\epsilon^2 - \\frac{q}{2}\\epsilon^3 - \\frac{c}{2}s^2 \\epsilon\n$$\n现在，我们对这个表达式取期望。我们使用零均值正态分布的矩：$\\mathbb{E}[\\epsilon] = 0$，$\\mathbb{E}[\\epsilon^2] = \\sigma^2$，以及 $\\mathbb{E}[\\epsilon^3] = 0$。\n$$\n\\mathbb{E}[\\dots] = -\\frac{q}{2}(\\mu_{\\theta}(s) - \\kappa s)^2 \\mathbb{E}[\\epsilon] - q(\\mu_{\\theta}(s) - \\kappa s) \\mathbb{E}[\\epsilon^2] - \\frac{q}{2}\\mathbb{E}[\\epsilon^3] - \\frac{c s^2}{2} \\mathbb{E}[\\epsilon]\n$$\n$$\n\\mathbb{E}[\\dots] = 0 - q(\\mu_{\\theta}(s) - \\kappa s) \\sigma^2 - 0 - 0 = -q \\sigma^2 (\\mu_{\\theta}(s) - \\kappa s)\n$$\n将此结果代回 $g(\\theta)$ 的表达式：\n$$\ng(\\theta) = \\frac{x(s)}{\\sigma^2} \\left( -q \\sigma^2 (\\mu_{\\theta}(s) - \\kappa s) \\right) = -q x(s) (\\mu_{\\theta}(s) - \\kappa s)\n$$\n我们现在代入数值：$s = 1.5$，$\\theta = \\begin{pmatrix} 0.4 \\\\ -0.1 \\end{pmatrix}$，$q=2$，$\\kappa = 0.8$。\n首先，计算所需的分量：\n状态特征向量是 $x(s) = x(1.5) = \\begin{pmatrix} 1.5 \\\\ (1.5)^3 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix}$。\n策略均值为 $\\mu_{\\theta}(s) = \\theta^{\\top} x(s) = \\begin{pmatrix} 0.4 & -0.1 \\end{pmatrix} \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} = 0.4 \\times 1.5 - 0.1 \\times 3.375 = 0.6 - 0.3375 = 0.2625$。\n目标动作函数值为 $\\kappa s = 0.8 \\times 1.5 = 1.2$。\n差值为 $\\mu_{\\theta}(s) - \\kappa s = 0.2625 - 1.2 = -0.9375$。\n现在，组装最终的梯度向量 $g(\\theta)$：\n$$\ng(\\theta) = -2 \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} (-0.9375)\n$$\n$$\ng(\\theta) = (2 \\times 0.9375) \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix} = 1.875 \\begin{pmatrix} 1.5 \\\\ 3.375 \\end{pmatrix}\n$$\n对每个分量执行最后的乘法：\n$$\ng_1(\\theta) = 1.875 \\times 1.5 = 2.8125\n$$\n$$\ng_2(\\theta) = 1.875 \\times 3.375 = 6.328125\n$$\n所以，演员更新向量是 $g(\\theta) = \\begin{pmatrix} 2.8125 \\\\ 6.328125 \\end{pmatrix}$。问题要求答案为单个行向量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.8125 & 6.328125\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在验证了单步更新的有效性后，我们需要确保整个算法能够收敛到最优策略。此练习  将强化学习与经典的随机近似理论和最优控制理论联系起来。您将首先为演员和评论家设计满足收敛条件的步长，然后推导出线性二次调节器（Linear-Quadratic Regulator, LQR）问题的精确最优解，从而为您的强化学习代理提供一个可供验证的“黄金标准”。",
            "id": "3962047",
            "problem": "考虑一个标量线性二次控制系统，其由离散时间动态 $x_{t+1} = a x_{t} + b u_{t}$ 和一个静态线性状态反馈演员 (actor) $u_{t} = -k_{t} x_{t}$ 定义。性能准则为无限时域折扣成本 $J(k) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\left(q x_{t}^{2} + r u_{t}^{2}\\right)\\right]$，其中 $0  \\gamma  1$，$q > 0$，$r > 0$。演员参数 $k_{t}$ 使用双时间尺度演员-评论家 (actor-critic) 算法进行更新，其中评论家 (critic) 参数 $v_{t}$ 通过时间差分 (TD) 学习近似价值函数 $V(x) \\approx v x^{2}$，而演员更新则对期望折扣成本执行随机梯度下降。假设对于每个固定的 $k$，演员的梯度估计是 $\\nabla J(k)$ 的无偏估计量，并且在固定 $k$ 的情况下，评论家 TD 误差在线性二次结构下定义了一个均值上的收缩。\n\n任务：\n1. 提出满足 Robbins–Monro 随机近似条件并实现适当双时间尺度分离（评论家快于演员）的评论家步长序列 $\\{\\alpha_{t}\\}$ 和演员步长序列 $\\{\\beta_{t}\\}$。明确验证所需的可和性。\n2. 从折扣线性二次控制问题的第一性原理和策略 $u_{t} = -k x_{t}$ 的定义出发，推导最小化 $J(k)$ 的最优反馈增益 $k^{\\star}$ 的精确表达式，用 $a$、$b$、$q$、$r$ 和 $\\gamma$ 表示您的结果。您的推导必须是自洽的，且仅使用所提供的系统和成本定义。\n3. 使用任务 2 的结果，对参数值 $a = 0.9$、$b = 1$、$q = 1$、$r = 0.25$ 和 $\\gamma = 0.95$ 数值计算 $k^{\\star}$。将您的最终数值答案四舍五入到四位有效数字。仅提供四舍五入后的 $k^{\\star}$ 值作为您的最终答案。\n\n您可以假设初始状态 $x_{0}$ 具有有限二阶矩，演员参数序列被约束在一个紧集内，以确保在整个迭代过程中 $\\gamma (a - b k)^{2}  1$，并且 TD 和梯度估计器中的噪声过程满足与随机近似理论一致的标准鞅差和有界方差条件。",
            "solution": "用户提供了一个分为三部分的问题，内容涉及一个用于标量离散时间线性二次控制系统的双时间尺度演员-评论家算法。问题要求提出有效的步长序列，解析推导最优控制增益，并对该增益进行数值计算。\n\n### 问题验证\n\n首先，我将验证问题陈述。\n\n**步骤 1：提取已知条件**\n\n*   系统动态：$x_{t+1} = a x_{t} + b u_{t}$\n*   演员策略：$u_{t} = -k_{t} x_{t}$\n*   成本函数：$J(k) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\left(q x_{t}^{2} + r u_{t}^{2}\\right)\\right]$\n*   折扣因子：$0  \\gamma  1$\n*   成本权重：$q > 0$，$r > 0$\n*   评论家近似：$V(x) \\approx v x^{2}$\n*   评论家参数：$v_t$\n*   演员参数：$k_t$\n*   学习率：$\\{\\alpha_{t}\\}$（评论家），$\\{\\beta_{t}\\}$（演员）\n*   算法特性：双时间尺度演员-评论家，演员使用随机梯度下降，评论家使用时间差分 (TD) 学习。\n*   假设：\n    *   演员的梯度估计是 $\\nabla J(k)$ 的无偏估计量。\n    *   在固定 $k$ 的情况下，评论家的 TD 误差在均值上是收缩的。\n    *   步长必须满足 Robbins–Monro 条件。\n    *   评论家的时间尺度比演员快。\n    *   初始状态 $x_{0}$ 具有有限二阶矩。\n    *   演员参数序列 $\\{k_t\\}$ 被约束在一个紧集内，其中 $\\gamma (a - b k)^{2}  1$。\n    *   噪声过程满足标准的随机近似条件。\n*   任务 3 的数值：$a = 0.9$，$b = 1$，$q = 1$，$r = 0.25$，$\\gamma = 0.95$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n*   **科学依据：** 该问题牢固地植根于最优控制理论（线性二次调节器）和强化学习（演员-评论家方法、随机近似）的既定原则。该模型是分析此类算法的标准基准。所提供的假设是双时间尺度随机近似算法收敛性理论分析中的标准假设。该问题在科学上是合理的。\n*   **适定性：** 问题的所有部分都是适定的。任务 1 要求提供一个有效的步长序列示例，这类示例有很多。任务 2 要求推导一个唯一的最优增益 $k^{\\star}$，在给定条件（$q > 0$，$r > 0$）下，折扣 LQR 问题的最优增益是已知的。任务 3 是一个直接的数值计算。问题是自洽的，并提供了所有必要的信息。\n*   **客观性：** 问题以精确、形式化的数学语言陈述，没有任何主观性或歧义。\n\n**步骤 3：结论与行动**\n\n该问题是有效的。这是一个来自控制理论和机器学习交叉领域的适定、有科学依据的问题。我将继续提供完整的解答。\n\n### 解答\n\n**任务 1：步长序列**\n\n双时间尺度随机近似算法的收敛要求两个步长序列，即快过程（评论家）的 $\\{\\alpha_{t}\\}$ 和慢过程（演员）的 $\\{\\beta_{t}\\}$，都满足标准的 Robbins–Monro (RM) 条件。此外，还必须满足一个时间尺度分离条件。\n\n对于一个通用的步长序列 $\\{\\eta_t\\}$，RM 条件为：\n$$\n\\sum_{t=0}^{\\infty} \\eta_t = \\infty\n\\quad \\text{和} \\quad\n\\sum_{t=0}^{\\infty} \\eta_t^2  \\infty\n$$\n第一个条件确保算法能够克服任何初始误差，而第二个条件确保每一步引入的噪声最终会消失。\n\n时间尺度分离条件，使得评论家的更新比演员“更快”，即：\n$$\n\\lim_{t \\to \\infty} \\frac{\\beta_{t}}{\\alpha_{t}} = 0\n$$\n\n我们提出形如 $\\alpha_t = (t+1)^{-p}$ 和 $\\beta_t = (t+1)^{-q}$ 的序列。\n为使这些序列满足 RM 条件，指数 $p$ 和 $q$ 必须在区间 $(0.5, 1]$ 内。这是因为级数 $\\sum_{t=0}^{\\infty} (t+1)^{-c}$ 在 $c \\le 1$ 时发散（根据积分判别法），在 $c > 1$ 时收敛。\n*   对于 $\\sum \\alpha_t = \\sum (t+1)^{-p} = \\infty$，我们需要 $p \\le 1$。\n*   对于 $\\sum \\alpha_t^2 = \\sum (t+1)^{-2p}  \\infty$，我们需要 $2p > 1$，即 $p > 0.5$。\n同样的逻辑适用于 $q$：$0.5  q \\le 1$。\n\n对于时间尺度分离条件 $\\lim_{t \\to \\infty} \\beta_t / \\alpha_t = 0$，我们需要：\n$$\n\\lim_{t \\to \\infty} \\frac{(t+1)^{-q}}{(t+1)^{-p}} = \\lim_{t \\to \\infty} (t+1)^{p-q} = 0\n$$\n当且仅当指数 $p-q$ 为负，即 $p  q$ 时，此极限为零。\n\n综合所有条件，我们必须选择 $p$ 和 $q$ 使得 $0.5  p  q \\le 1$。\n一个有效的选择是设置 $p = \\frac{2}{3}$ 和 $q = 1$。我们来定义序列并验证它们。\n\n*   评论家（快）步长：$\\alpha_{t} = \\frac{1}{(t+1)^{2/3}}$\n*   演员（慢）步长：$\\beta_{t} = \\frac{1}{t+1}$\n\n验证：\n1.  对于 $\\{\\alpha_t\\}$：$\\sum_{t=0}^{\\infty} \\alpha_t = \\sum_{t=0}^{\\infty} (t+1)^{-2/3}$ 是一个发散的 p-级数（$p=2/3 \\le 1$）。$\\sum_{t=0}^{\\infty} \\alpha_t^2 = \\sum_{t=0}^{\\infty} (t+1)^{-4/3}$ 是一个收敛的 p-级数（$p=4/3 > 1$）。RM 条件成立。\n2.  对于 $\\{\\beta_t\\}$：$\\sum_{t=0}^{\\infty} \\beta_t = \\sum_{t=0}^{\\infty} (t+1)^{-1}$ 是调和级数，它是一个发散的 p-级数（$p=1 \\le 1$）。$\\sum_{t=0}^{\\infty} \\beta_t^2 = \\sum_{t=0}^{\\infty} (t+1)^{-2}$ 是一个收敛的 p-级数（$p=2 > 1$）。RM 条件成立。\n3.  时间尺度分离：$\\lim_{t \\to \\infty} \\frac{\\beta_t}{\\alpha_t} = \\lim_{t \\to \\infty} \\frac{(t+1)^{-1}}{(t+1)^{-2/3}} = \\lim_{t \\to \\infty} (t+1)^{-1/3} = 0$。该条件得到满足。\n\n**任务 2：最优增益 $k^{\\star}$ 的推导**\n\n最优控制增益 $k^{\\star}$ 可以从这个折扣线性二次问题的贝尔曼最优性方程推导出来。已知最优价值函数是二次的，即 $V^{\\star}(x) = P x^{2}$，其中 $P > 0$ 是一个常数。\n\n贝尔曼最优性方程是：\n$$\nV^{\\star}(x) = \\min_{u} \\left\\{ q x^{2} + r u^{2} + \\gamma V^{\\star}(x_{t+1}) \\right\\}\n$$\n代入 $V^{\\star}(x) = P x^{2}$ 和 $x_{t+1} = ax + bu$：\n$$\nP x^{2} = \\min_{u} \\left\\{ q x^{2} + r u^{2} + \\gamma P (ax+bu)^{2} \\right\\}\n$$\n为了找到最小化控制输入 $u^{\\star}$，我们将最小值内的表达式对 $u$ 求导，并令结果为零：\n$$\n\\frac{\\partial}{\\partial u} \\left[ q x^{2} + r u^{2} + \\gamma P (ax+bu)^{2} \\right] = 2ru + \\gamma P \\cdot 2(ax+bu) \\cdot b = 0\n$$\n$$\nru + \\gamma P b (ax+bu) = 0\n$$\n$$\nu(r + \\gamma P b^{2}) = - \\gamma P a b x\n$$\n因此，最优控制律是线性状态反馈：\n$$\nu^{\\star} = - \\left( \\frac{\\gamma a b P}{r + \\gamma b^{2} P} \\right) x\n$$\n这具有 $u^{\\star} = -k^{\\star} x$ 的形式，其中最优增益 $k^{\\star}$ 是：\n$$\nk^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P}\n$$\n为了找到 $P$ 的值，我们将 $u^{\\star}$ 代回贝尔曼方程。该方程必须对最小化的 $u^{\\star}$ 成立。\n$$\nP x^{2} = q x^{2} + r (u^{\\star})^{2} + \\gamma P (ax+bu^{\\star})^{2}\n$$\n两边除以 $x^2$（因为方程必须对所有 $x$ 成立）并代入 $u^{\\star} = -k^{\\star}x$：\n$$\nP = q + r (k^{\\star})^{2} + \\gamma P (a-bk^{\\star})^{2}\n$$\n我们将 $k^{\\star}$ 的表达式代入这个方程。先表达 $(a-bk^{\\star})$ 会更容易：\n$$\na - bk^{\\star} = a - b \\left(\\frac{\\gamma a b P}{r + \\gamma b^{2} P}\\right) = \\frac{a(r + \\gamma b^{2} P) - \\gamma a b^{2} P}{r + \\gamma b^{2} P} = \\frac{ar}{r + \\gamma b^{2} P}\n$$\n现在将 $k^{\\star}$ 和 $(a-bk^{\\star})$ 代入 $P$ 的方程中：\n$$\nP = q + r \\left( \\frac{\\gamma a b P}{r + \\gamma b^{2} P} \\right)^{2} + \\gamma P \\left( \\frac{ar}{r + \\gamma b^{2} P} \\right)^{2}\n$$\n$$\nP = q + \\frac{r \\gamma^{2} a^{2} b^{2} P^{2} + \\gamma P a^{2} r^{2}}{(r + \\gamma b^{2} P)^{2}}\n$$\n$$\nP = q + \\frac{\\gamma a^{2} r P (r \\gamma b^2 P / (\\gamma a^2 r P) + r)}{ (r + \\gamma b^{2} P)^{2}} = q + \\frac{\\gamma a^{2} r P (r + \\gamma b^{2} P)}{(r + \\gamma b^{2} P)^{2}}\n$$\n$$\nP = q + \\frac{\\gamma a^{2} r P}{r + \\gamma b^{2} P}\n$$\n这个关于 $P$ 的方程是该折扣问题的标量离散时间代数黎卡提方程 (DARE)。我们将其整理成一个关于 $P$ 的二次方程：\n$$\nP(r + \\gamma b^{2} P) = q(r + \\gamma b^{2} P) + \\gamma a^{2} r P\n$$\n$$\nPr + \\gamma b^{2} P^{2} = qr + q\\gamma b^{2} P + \\gamma a^{2} r P\n$$\n$$\n(\\gamma b^{2}) P^{2} + (r - \\gamma q b^{2} - \\gamma a^{2} r) P - qr = 0\n$$\n这是一个形如 $A P^2 + B P + C = 0$ 的二次方程。由于 $q>0$ 和 $r>0$，与成本相关的 $P$ 值必须为正。系数 $A = \\gamma b^2 > 0$ 且系数 $C = -qr  0$。根的乘积 $C/A$ 为负，因此存在一个正实根和一个负实根。我们必须取正根：\n$$\nP = \\frac{-B + \\sqrt{B^{2} - 4AC}}{2A}\n$$\n其中 $A = \\gamma b^2$，$B = r - \\gamma(q b^2 + a^2 r)$，$C = -qr$。\n然后，通过将 $P$ 的值代入其表达式来找到最优增益 $k^{\\star}$：$k^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P}$。\n\n**任务 3：$k^{\\star}$ 的数值计算**\n\n给定参数：$a = 0.9$，$b = 1$，$q = 1$，$r = 0.25$，$\\gamma = 0.95$。\n\n首先，我们计算黎卡提方程中 $P$ 的系数：\n$A = \\gamma b^{2} = 0.95 \\times 1^{2} = 0.95$\n$B = r - \\gamma(q b^{2} + a^{2} r) = 0.25 - 0.95(1 \\times 1^{2} + 0.9^{2} \\times 0.25)$\n$B = 0.25 - 0.95(1 + 0.81 \\times 0.25) = 0.25 - 0.95(1 + 0.2025)$\n$B = 0.25 - 0.95(1.2025) = 0.25 - 1.142375 = -0.892375$\n$C = -qr = -1 \\times 0.25 = -0.25$\n\n$P$ 的方程是 $0.95 P^{2} - 0.892375 P - 0.25 = 0$。\n我们求解正根 $P$：\n$$\nP = \\frac{-(-0.892375) + \\sqrt{(-0.892375)^{2} - 4(0.95)(-0.25)}}{2(0.95)}\n$$\n$$\nP = \\frac{0.892375 + \\sqrt{0.7963320625 + 0.95}}{1.9}\n$$\n$$\nP = \\frac{0.892375 + \\sqrt{1.7463320625}}{1.9} \\approx \\frac{0.892375 + 1.32148858}{1.9}\n$$\n$$\nP \\approx \\frac{2.21386364}{1.9} \\approx 1.16519139\n$$\n现在，我们用这个 $P$ 值来计算 $k^{\\star}$：\n$$\nk^{\\star} = \\frac{\\gamma a b P}{r + \\gamma b^{2} P} = \\frac{0.95 \\times 0.9 \\times 1 \\times P}{0.25 + 0.95 \\times 1^{2} \\times P} = \\frac{0.855 P}{0.25 + 0.95 P}\n$$\n代入 $P$ 的数值：\n$$\nk^{\\star} \\approx \\frac{0.855 \\times 1.16519139}{0.25 + 0.95 \\times 1.16519139}\n$$\n$$\nk^{\\star} \\approx \\frac{0.99623864}{0.25 + 1.10693182} = \\frac{0.99623864}{1.35693182} \\approx 0.7341829\n$$\n四舍五入到四位有效数字，我们得到 $k^{\\star} \\approx 0.7342$。",
            "answer": "$$\n\\boxed{0.7342}\n$$"
        },
        {
            "introduction": "现代强化学习系统为了提高样本效率，通常采用离策略（off-policy）学习，但这会带来稳定性挑战。本练习  将带您实践一种先进的离策略校正技术——V-trace算法。通过处理截断重要性采样和多步时序差分目标，您将学习如何为一个演员-评论家代理计算稳定的价值目标和校正后的优势函数，这是大规模分布式强化学习中的一项关键技能。",
            "id": "3962015",
            "problem": "考虑一个用于马尔可夫决策过程 (MDP) 控制的离策略演员-评论家智能体，该过程具有状态 $x_t$、动作 $a_t$、奖励 $r_t$ 和折扣因子 $\\gamma \\in (0,1)$。目标策略是由 $\\theta$ 参数化的 $\\pi_{\\theta}(a \\mid x)$，数据是在一个不同的行为策略 $\\mu(a \\mid x)$ 下收集的。为了纠正离策略采样，重要性采样 (IS) 使用了每个时间步的比率 $r_t = \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$。为了减少方差并稳定学习，考虑使用截断水平为 $\\bar{\\rho} > 0$ 和 $\\bar{c} > 0$ 的截断重要性采样，定义为\n$$\n\\rho_t = \\min\\left(\\bar{\\rho}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right), \\quad c_t = \\min\\left(\\bar{c}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right).\n$$\n评论家寻求一个多步目标，该目标使用截断重要性采样来纠正时序差分误差；而演员寻求一个修正优势，该优势以一种与离策略学习保持一致的方式缩放策略梯度 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_t \\mid x_t)$。\n\n仅从上述定义和基本的强化学习 (RL) 原理（状态价值 $V^{\\pi}(x)$、动作价值 $Q^{\\pi}(x,a)$，以及 $Q^{\\pi}(x,a) = \\mathbb{E}\\left[r + \\gamma V^{\\pi}(x') \\mid x,a\\right]$）出发，推导评论家在时间 $s$ 的截断重要性采样多步价值目标 $v_s$ 以及演员使用的相应修正优势，并用 $\\rho_t$、 $c_t$、即时奖励、自举值和 $\\gamma$ 表示。然后，对于在 $t=3$ 处进行终端自举的特定 3 步轨迹段 $t \\in \\{0,1,2\\}$，使用以下数据计算在时间 $t=0$ 时的修正优势：\n- $\\gamma = 0.9$，\n- $\\bar{\\rho} = 1.3$, $\\bar{c} = 1.0$，\n- $\\mu(a_0 \\mid x_0) = 0.4$, $\\pi_{\\theta}(a_0 \\mid x_0) = 0.7$，\n- $\\mu(a_1 \\mid x_1) = 0.6$, $\\pi_{\\theta}(a_1 \\mid x_1) = 0.5$，\n- $\\mu(a_2 \\mid x_2) = 0.2$, $\\pi_{\\theta}(a_2 \\mid x_2) = 0.4$，\n- $r_0 = 1.2$, $r_1 = -0.3$, $r_2 = 0.5$，\n- $V(x_0) = 2.0$, $V(x_1) = 1.5$, $V(x_2) = 1.0$, $V(x_3) = 0$。\n\n将时间 $t=0$ 的最终标量修正优势表示为一个实数，四舍五入到四位有效数字。不需要单位。",
            "solution": "在进行求解之前，对问题陈述的有效性进行评估。\n\n### 第 1 步：提取已知条件\n- **框架**：马尔可夫决策过程 (MDP)，具有状态 $x_t$、动作 $a_t$、奖励 $r_t$。\n- **策略**：目标策略 $\\pi_{\\theta}(a \\mid x)$，行为策略 $\\mu(a \\mid x)$。\n- **折扣因子**：$\\gamma \\in (0,1)$。\n- **重要性采样 (IS) 比率**：\n  - 问题首先定义了每个时间步的比率 $r_t = \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$，但这个比率的表示法立即被 $\\rho_t$ 和 $c_t$ 的定义所取代，并且在数值计算部分，$r_t$ 用于表示奖励。按照惯例，推导和计算将专门使用 $r_t$ 表示奖励。\n  - 用于价值估计的截断比率：$\\rho_t = \\min\\left(\\bar{\\rho}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right)$。\n  - 用于递归更新的截断比率：$c_t = \\min\\left(\\bar{c}, \\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}\\right)$。\n  - 截断水平：$\\bar{\\rho} > 0$, $\\bar{c} > 0$。\n- **目标**：\n  1. 为评论家推导截断 IS 多步价值目标 $v_s$。\n  2. 推导在时间 $s$ 演员使用的相应修正优势。\n  3. 对于给定的在 $t=3$ 处进行终端自举的 3 步轨迹段 $t \\in \\{0,1,2\\}$，计算在时间 $t=0$ 时的修正优势。\n- **基本原理**：状态价值 $V^{\\pi}(x)$、动作价值 $Q^{\\pi}(x,a)$，以及贝尔曼期望方程 $Q^{\\pi}(x,a) = \\mathbb{E}\\left[r + \\gamma V^{\\pi}(x') \\mid x,a\\right]$。\n- **数值数据**：\n  - $\\gamma = 0.9$。\n  - $\\bar{\\rho} = 1.3$, $\\bar{c} = 1.0$。\n  - 策略概率：$\\mu(a_0 \\mid x_0) = 0.4$, $\\pi_{\\theta}(a_0 \\mid x_0) = 0.7$；$\\mu(a_1 \\mid x_1) = 0.6$, $\\pi_{\\theta}(a_1 \\mid x_1) = 0.5$；$\\mu(a_2 \\mid x_2) = 0.2$, $\\pi_{\\theta}(a_2 \\mid x_2) = 0.4$。\n  - 奖励：$r_0 = 1.2$, $r_1 = -0.3$, $r_2 = 0.5$。\n  - 价值函数估计：$V(x_0) = 2.0$, $V(x_1) = 1.5$, $V(x_2) = 1.0$, $V(x_3) = 0$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题具有科学依据。截断重要性采样比率 $\\rho_t$ 和 $c_t$ 的公式，以及推导多步离策略价值目标和修正优势的任务，是 V-trace 算法（Espeholt 等人，2018）的特征。V-trace 算法是现代强化学习中一种成熟且广泛使用的方法，用于大规模的离策略演员-评论家架构。该问题是适定 (well-posed) 的，为获得唯一解提供了所有必要的定义和数据。语言客观且正式。虽然存在一个小的符号模糊性，即 IS 比率最初被标记为 $r_t$，而这也是奖励的符号，但随后的更具体的 $\\rho_t$ 和 $c_t$ 定义消除了任何混淆。该问题没有违反任何无效性标准。\n\n### 第 3 步：结论和行动\n问题是有效的。将提供一个解决方案。\n\n### 价值目标和修正优势的推导\n\n问题描述了一个基于 V-trace 的学习过程。我们将根据其原理推导多步价值目标和修正优势的表达式。\n\n设 $V(x_t)$ 是价值函数的当前估计。时间 $t$ 的单步时序差分 (TD) 残差定义为：\n$$\n\\delta_t = r_t + \\gamma V(x_{t+1}) - V(x_t)\n$$\n这个残差衡量了基于观测到的转移 $(x_t, a_t, r_t, x_{t+1})$ 的价值估计 $V(x_t)$ 中的误差。\n\n对于离策略学习，TD 残差通过重要性采样进行校正。V-trace 算法基于从时间 $s$ 到 $s+n-1$ 的轨迹段，为状态 $x_s$ 定义了一个多步价值目标 $v_s$。这个目标是通过累积折扣和重要性加权的 TD 残差来构建的。在时间 $t \\ge s$ 的价值目标 $v_t$ 的递归定义是：\n$$\nv_t = V(x_t) + \\rho_t (r_t + \\gamma V(x_{t+1}) - V(x_t)) + \\gamma c_t (v_{t+1} - V(x_{t+1}))\n$$\n带有 $n$ 步的终止条件，例如 $v_{s+n} = V(x_{s+n})$。\n\n将此递归从 $t=s$ 展开到 $s+n-1$ 可以得到评论家的显式多步价值目标 $v_s$：\n$$\nv_s = V(x_s) + \\sum_{t=s}^{s+n-1} \\gamma^{t-s} \\left( \\prod_{i=s}^{t-1} c_i \\right) \\rho_t \\delta_t\n$$\n其中乘积项 $\\prod_{i=s}^{t-1} c_i$ 在 $t=s$ 时定义为 1。\n\n演员使用策略梯度方法更新策略 $\\pi_{\\theta}$。V-trace 算法使用在时间 $s$ 的修正优势估计来缩放得分函数 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a_s \\mid x_s)$。这个优势我们表示为 $\\hat{A}_s$，由下式给出：\n$$\n\\hat{A}_s = \\rho_s (r_s + \\gamma v_{s+1} - V(x_s))\n$$\n这里，$v_{s+1}$ 是后续状态 $x_{s+1}$ 的 V-trace 目标。这种形式与标准优势函数 $A(x,a) = Q(x,a) - V(x)$ 保持了密切的联系，但是 $Q(x_s, a_s)$ 是通过 $r_s + \\gamma v_{s+1}$ 来估计的，其中 $v_{s+1}$ 作为下一状态价值 $V(x_{s+1})$ 的离策略修正估计。\n\n### 在 $t=0$ 时修正优势的数值计算\n\n我们需要计算在时间 $t=0$ 时的修正优势，即 $\\hat{A}_0 = \\rho_0 (r_0 + \\gamma v_1 - V(x_0))$。这需要我们首先计算中间量 $\\rho_t$、 $c_t$、 $\\delta_t$，然后计算价值目标 $v_1$。轨迹段是 $t \\in \\{0,1,2\\}$ ($n=3$)，自举值为 $V(x_3)=0$。\n\n**1. 计算重要性采样比率和截断版本**\n未截断的比率是 $\\frac{\\pi_{\\theta}(a_t \\mid x_t)}{\\mu(a_t \\mid x_t)}$。给定 $\\bar{\\rho} = 1.3$ 和 $\\bar{c} = 1.0$：\n- 对于 $t=0$：比率 $= \\frac{0.7}{0.4} = 1.75$。\n  - $\\rho_0 = \\min(1.3, 1.75) = 1.3$。\n  - $c_0 = \\min(1.0, 1.75) = 1.0$。\n- 对于 $t=1$：比率 $= \\frac{0.5}{0.6} = \\frac{5}{6} \\approx 0.8333$。\n  - $\\rho_1 = \\min(1.3, 5/6) = 5/6$。\n  - $c_1 = \\min(1.0, 5/6) = 5/6$。\n- 对于 $t=2$：比率 $= \\frac{0.4}{0.2} = 2.0$。\n  - $\\rho_2 = \\min(1.3, 2.0) = 1.3$。\n  - $c_2 = \\min(1.0, 2.0) = 1.0$。\n\n**2. 计算 TD 残差 $\\delta_t$**\n给定 $\\gamma = 0.9$：\n- $\\delta_0 = r_0 + \\gamma V(x_1) - V(x_0) = 1.2 + 0.9(1.5) - 2.0 = 1.2 + 1.35 - 2.0 = 0.55$。\n- $\\delta_1 = r_1 + \\gamma V(x_2) - V(x_1) = -0.3 + 0.9(1.0) - 1.5 = -0.3 + 0.9 - 1.5 = -0.9$。\n- $\\delta_2 = r_2 + \\gamma V(x_3) - V(x_2) = 0.5 + 0.9(0) - 1.0 = 0.5 - 1.0 = -0.5$。\n\n**3. 计算价值目标 $v_1$**\n价值目标 $v_1$ 是在从 $t=1$ 到 $t=2$ 的剩余轨迹上计算的，并使用 $V(x_3)$ 进行自举。\n$$\nv_1 = V(x_1) + \\sum_{t=1}^{2} \\gamma^{t-1} \\left( \\prod_{i=1}^{t-1} c_i \\right) \\rho_t \\delta_t\n$$\n展开求和：\n$$\nv_1 = V(x_1) + (\\gamma^0 \\rho_1 \\delta_1) + (\\gamma^1 c_1 \\rho_2 \\delta_2)\n$$\n代入计算出的值：\n$$\nv_1 = 1.5 + \\left(1 \\cdot \\frac{5}{6} \\cdot (-0.9)\\right) + \\left(0.9 \\cdot \\frac{5}{6} \\cdot 1.3 \\cdot (-0.5)\\right)\n$$\n$$\nv_1 = 1.5 - \\frac{4.5}{6} - \\left(0.9 \\cdot \\frac{5}{6} \\cdot 0.65\\right)\n$$\n$$\nv_1 = 1.5 - 0.75 - 0.4875\n$$\n$$\nv_1 = 0.75 - 0.4875 = 0.2625\n$$\n\n**4. 计算最终修正优势 $\\hat{A}_0$**\n现在我们计算在 $t=0$ 时的修正优势：\n$$\n\\hat{A}_0 = \\rho_0 (r_0 + \\gamma v_1 - V(x_0))\n$$\n代入所有值：\n$$\n\\hat{A}_0 = 1.3 \\left( 1.2 + 0.9 \\cdot (0.2625) - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( 1.2 + 0.23625 - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( 1.43625 - 2.0 \\right)\n$$\n$$\n\\hat{A}_0 = 1.3 \\left( -0.56375 \\right)\n$$\n$$\n\\hat{A}_0 = -0.732875\n$$\n四舍五入到四位有效数字得到 $-0.7329$。",
            "answer": "$$\\boxed{-0.7329}$$"
        }
    ]
}