{
    "hands_on_practices": [
        {
            "introduction": "The Random Utility Model (RUM) provides a foundational framework for understanding choice behavior. A key insight from this framework is that choice probabilities are not just determined by the average values of the options, but also by the structure of the noise or uncertainty around those values. This exercise  provides a direct, hands-on opportunity to explore this principle by deriving how noise correlations between alternatives systematically alter choice probabilities, a crucial concept for explaining violations of simple independence axioms.",
            "id": "3999876",
            "problem": "Consider a three-alternative Random Utility Model (RUM), where the utility of alternative $i \\in \\{1,2,3\\}$ is $U_{i} = \\mu_{i} + \\varepsilon_{i}$. The deterministic components $\\mu_{1}$, $\\mu_{2}$, and $\\mu_{3}$ are fixed real numbers. The noise vector $(\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3})$ is jointly Gaussian with zero mean and covariance matrix $\\Sigma$ having unit variances on the diagonal and correlation $\\rho$ only between $\\varepsilon_{1}$ and $\\varepsilon_{2}$, that is,\n$$\n\\Sigma = \\begin{pmatrix}\n1 & \\rho & 0 \\\\\n\\rho & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nAssume the standard bivariate probit approximation for pairwise choice within the triad, whereby the probability of choosing one alternative over another is computed from the joint distribution of the corresponding two utility noises alone. Let $P_{1 \\succ 2}(\\rho)$ denote the probability that alternative $1$ is chosen over alternative $2$ in a pairwise comparison, that is $P_{1 \\succ 2}(\\rho) = \\mathbb{P}(U_{1} > U_{2})$, under this approximation.\n\nStarting from the basic definitions of the Random Utility Model and properties of jointly Gaussian variables, derive a closed-form expression for $P_{1 \\succ 2}(\\rho)$ as a function of $\\rho$, $\\mu_{1}$, and $\\mu_{2}$. Express your final answer using the standard normal cumulative distribution function (CDF) $\\Phi(\\cdot)$. No numerical evaluation is required.",
            "solution": "The problem as stated is formally sound, scientifically grounded in the principles of random utility theory and probability theory, and well-posed. All necessary information is provided to derive a unique, meaningful solution. We may therefore proceed with the derivation.\n\nThe problem asks for the probability that alternative $1$ is chosen over alternative $2$, denoted $P_{1 \\succ 2}(\\rho)$. Within the framework of the Random Utility Model (RUM), this corresponds to the event that the utility of alternative $1$, $U_{1}$, is greater than the utility of alternative $2$, $U_{2}$.\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(U_{1} > U_{2})\n$$\nThe utility for each alternative $i$ is given by the expression $U_{i} = \\mu_{i} + \\varepsilon_{i}$, where $\\mu_{i}$ is the deterministic component of utility and $\\varepsilon_{i}$ is a random noise term. Substituting this definition into the probability expression, we get:\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(\\mu_{1} + \\varepsilon_{1} > \\mu_{2} + \\varepsilon_{2})\n$$\nThis inequality can be rearranged to isolate the random terms on one side and the deterministic terms on the other:\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(\\varepsilon_{2} - \\varepsilon_{1} < \\mu_{1} - \\mu_{2})\n$$\nTo evaluate this probability, we must determine the probability distribution of the difference of the two random variables, $\\varepsilon_{2} - \\varepsilon_{1}$. Let us define a new random variable $Y = \\varepsilon_{2} - \\varepsilon_{1}$. The problem states that the noise vector $(\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3})$ is jointly Gaussian. A fundamental property of multivariate Gaussian distributions is that any linear combination of its components is also a Gaussian random variable. Therefore, $Y$ follows a normal distribution.\n\nTo fully characterize the distribution of $Y$, we need to find its mean $E[Y]$ and its variance $\\text{Var}(Y)$.\n\nThe mean of $Y$ is given by the linearity of the expectation operator:\n$$\nE[Y] = E[\\varepsilon_{2} - \\varepsilon_{1}] = E[\\varepsilon_{2}] - E[\\varepsilon_{1}]\n$$\nThe problem specifies that the noise vector has a zero mean, so $E[\\varepsilon_{1}] = 0$ and $E[\\varepsilon_{2}] = 0$. Thus, the mean of $Y$ is:\n$$\nE[Y] = 0 - 0 = 0\n$$\nThe variance of $Y$ is calculated using the formula for the variance of a linear combination of two correlated random variables:\n$$\n\\text{Var}(Y) = \\text{Var}(\\varepsilon_{2} - \\varepsilon_{1}) = \\text{Var}(\\varepsilon_{2}) + \\text{Var}((-1)\\varepsilon_{1}) + 2 \\cdot \\text{Cov}(\\varepsilon_{2}, (-1)\\varepsilon_{1})\n$$\n$$\n\\text{Var}(Y) = \\text{Var}(\\varepsilon_{2}) + (-1)^{2}\\text{Var}(\\varepsilon_{1}) - 2 \\cdot \\text{Cov}(\\varepsilon_{1}, \\varepsilon_{2})\n$$\nThe problem provides the covariance matrix $\\Sigma$:\n$$\n\\Sigma = \\begin{pmatrix}\n1 & \\rho & 0 \\\\\n\\rho & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nFrom this matrix, we extract the necessary variances and covariance:\n- The variance of $\\varepsilon_{1}$ is $\\text{Var}(\\varepsilon_{1}) = \\Sigma_{11} = 1$.\n- The variance of $\\varepsilon_{2}$ is $\\text{Var}(\\varepsilon_{2}) = \\Sigma_{22} = 1$.\n- The covariance of $\\varepsilon_{1}$ and $\\varepsilon_{2}$ is $\\text{Cov}(\\varepsilon_{1}, \\varepsilon_{2}) = \\Sigma_{12} = \\rho$.\n\nSubstituting these values into the variance formula for $Y$:\n$$\n\\text{Var}(Y) = 1 + 1 - 2\\rho = 2 - 2\\rho = 2(1-\\rho)\n$$\nSo, the random variable $Y = \\varepsilon_{2} - \\varepsilon_{1}$ is normally distributed with mean $0$ and variance $2(1-\\rho)$. We can write this as $Y \\sim \\mathcal{N}(0, 2(1-\\rho))$. The standard deviation of $Y$ is $\\sigma_{Y} = \\sqrt{2(1-\\rho)}$.\n\nWe can now express the probability $P_{1 \\succ 2}(\\rho)$ in terms of $Y$:\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}(Y < \\mu_{1} - \\mu_{2})\n$$\nTo use the standard normal cumulative distribution function (CDF), $\\Phi(\\cdot)$, we must standardize the random variable $Y$. A standard normal variable $Z \\sim \\mathcal{N}(0, 1)$ is defined as $Z = \\frac{Y - E[Y]}{\\sqrt{\\text{Var}(Y)}}$. Substituting the mean and variance of $Y$:\n$$\nZ = \\frac{Y - 0}{\\sqrt{2(1-\\rho)}} = \\frac{Y}{\\sqrt{2(1-\\rho)}}\n$$\nWe can transform the inequality inside the probability expression by dividing both sides by the standard deviation $\\sigma_{Y} = \\sqrt{2(1-\\rho)}$. Since the standard deviation is a positive quantity (we assume $|\\rho| < 1$ for a non-degenerate positive-definite covariance matrix), the direction of the inequality is preserved:\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}\\left(\\frac{Y}{\\sqrt{2(1-\\rho)}} < \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\nRecognizing the standardized variable $Z$ on the left side, we have:\n$$\nP_{1 \\succ 2}(\\rho) = \\mathbb{P}\\left(Z < \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\nBy definition, the CDF of the standard normal distribution is $\\Phi(x) = \\mathbb{P}(Z < x)$. Therefore, we can write the final expression for the probability as:\n$$\nP_{1 \\succ 2}(\\rho) = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)\n$$\nThis is the closed-form expression for the probability that alternative $1$ is chosen over alternative $2$, as a function of their deterministic utilities $\\mu_{1}$ and $\\mu_{2}$, and the correlation $\\rho$ between their noise terms. The instruction to use a \"bivariate probit approximation\" justifies ignoring the third alternative entirely for this calculation.",
            "answer": "$$\n\\boxed{\\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{2(1-\\rho)}}\\right)}\n$$"
        },
        {
            "introduction": "Computational models of choice range from abstract, descriptive formalisms to those grounded in putative neural mechanisms. This practice  pits two of the most influential models against each other: the neurally-inspired Divisive Normalization (DN) model and the statistically-principled softmax function. By working through the calculations, you will discover the conditions under which these structurally different models can yield identical predictions, offering deeper insight into the functional consequences of competitive inhibition in decision circuits.",
            "id": "3999866",
            "problem": "Consider a neural circuit model of three-alternative decision making with competitive inhibition implemented through Divisive Normalization (DN). Let each alternative $i \\in \\{1,2,3\\}$ present an input drive (gain) $g_i$, and the circuit instantiate lateral inhibitory pooling of the other alternatives via a normalization term. A widely accepted formulation of DN responses in sensory and decision circuits is that the steady-state response of alternative $i$ is given by $r_i = \\frac{g_i}{\\sigma + \\sum_{j \\neq i} g_j}$, where $\\sigma > 0$ is an additive semisaturation constant capturing background drive and inhibitory pool offset. To obtain choice probabilities, assume that the probability of selecting alternative $i$ is proportional to $r_i$ and normalized over alternatives: $p_i^{\\mathrm{DN}} = \\frac{r_i}{\\sum_{k=1}^{3} r_k}$. In parallel, consider a probabilistic decision model grounded in the Multinomial Logit (MNL, also known as the softmax) transformation of utilities, where the choice probabilities are $p_i^{\\mathrm{SM}} = \\frac{\\exp(\\beta g_i)}{\\sum_{k=1}^{3} \\exp(\\beta g_k)}$ with inverse-temperature parameter $\\beta > 0$.\n\nGiven three alternatives with gains $g = (2,1,1)$ and a DN semisaturation constant $\\sigma = 1$, proceed from these definitions to compare the DN and softmax choice probability vectors. Fit the softmax parameter $\\beta$ by equating the softmax top-choice probability $p_1^{\\mathrm{SM}}$ to the DN top-choice probability $p_1^{\\mathrm{DN}}$ obtained from the circuit. Then, analyze any residual differences between the two full distributions $\\{p_i^{\\mathrm{DN}}\\}$ and $\\{p_i^{\\mathrm{SM}}\\}$ after the fit, explaining their presence or absence based on the underlying model structures and the symmetry of the gains.\n\nProvide the fitted inverse-temperature parameter $\\beta$ as a single closed-form analytical expression. Do not round your final answer. No physical units are required for $\\beta$.",
            "solution": "The starting point is the Divisive Normalization (DN) response definition $r_i = \\frac{g_i}{\\sigma + \\sum_{j \\neq i} g_j}$ and the construction of probabilities by normalization $p_i^{\\mathrm{DN}} = \\frac{r_i}{\\sum_{k=1}^{3} r_k}$. We are given $g = (2,1,1)$ and $\\sigma = 1$.\n\nCompute the DN responses for each alternative:\n- For $i=1$, the inhibitory pool excludes $i=1$ and sums the other gains, so $\\sum_{j \\neq 1} g_j = g_2 + g_3 = 1 + 1 = 2$. Therefore,\n$$\nr_1 = \\frac{g_1}{\\sigma + \\sum_{j \\neq 1} g_j} = \\frac{2}{1 + 2} = \\frac{2}{3}.\n$$\n- For $i=2$, the pool excludes $i=2$ and sums $g_1 + g_3 = 2 + 1 = 3$, so\n$$\nr_2 = \\frac{g_2}{\\sigma + \\sum_{j \\neq 2} g_j} = \\frac{1}{1 + 3} = \\frac{1}{4}.\n$$\n- For $i=3$, the pool excludes $i=3$ and sums $g_1 + g_2 = 2 + 1 = 3$, so\n$$\nr_3 = \\frac{g_3}{\\sigma + \\sum_{j \\neq 3} g_j} = \\frac{1}{1 + 3} = \\frac{1}{4}.\n$$\n\nNormalize the responses to obtain DN choice probabilities:\n$$\n\\sum_{k=1}^{3} r_k = \\frac{2}{3} + \\frac{1}{4} + \\frac{1}{4} = \\frac{2}{3} + \\frac{1}{2} = \\frac{4}{6} + \\frac{3}{6} = \\frac{7}{6}.\n$$\nThus,\n$$\np_1^{\\mathrm{DN}} = \\frac{r_1}{\\sum_k r_k} = \\frac{\\frac{2}{3}}{\\frac{7}{6}} = \\frac{2}{3} \\cdot \\frac{6}{7} = \\frac{4}{7},\n$$\n$$\np_2^{\\mathrm{DN}} = \\frac{r_2}{\\sum_k r_k} = \\frac{\\frac{1}{4}}{\\frac{7}{6}} = \\frac{1}{4} \\cdot \\frac{6}{7} = \\frac{3}{14},\n$$\n$$\np_3^{\\mathrm{DN}} = \\frac{r_3}{\\sum_k r_k} = \\frac{\\frac{1}{4}}{\\frac{7}{6}} = \\frac{3}{14}.\n$$\nBy symmetry of the gains $(1,1)$ for alternatives $2$ and $3$, we have $p_2^{\\mathrm{DN}} = p_3^{\\mathrm{DN}}$.\n\nNext, consider the softmax (Multinomial Logit, MNL) probabilities $p_i^{\\mathrm{SM}} = \\frac{\\exp(\\beta g_i)}{\\sum_{k=1}^{3} \\exp(\\beta g_k)}$ and fit $\\beta$ so that the top-choice probability matches: $p_1^{\\mathrm{SM}} = p_1^{\\mathrm{DN}} = \\frac{4}{7}$.\n\nLet $x = \\exp(\\beta)$, so $\\exp(\\beta g_1) = \\exp(2\\beta) = x^{2}$ and $\\exp(\\beta g_2) = \\exp(\\beta g_3) = \\exp(\\beta) = x$. Then the softmax top-choice probability is\n$$\np_1^{\\mathrm{SM}} = \\frac{x^{2}}{x^{2} + x + x} = \\frac{x^{2}}{x^{2} + 2x}.\n$$\nWe equate this to the DN top choice:\n$$\n\\frac{x^{2}}{x^{2} + 2x} = \\frac{4}{7}.\n$$\nMultiply both sides by $x^{2} + 2x$ to avoid division:\n$$\nx^{2} = \\frac{4}{7} \\left(x^{2} + 2x\\right).\n$$\nExpand the right-hand side:\n$$\nx^{2} = \\frac{4}{7} x^{2} + \\frac{8}{7} x.\n$$\nSubtract $\\frac{4}{7}x^{2}$ from both sides:\n$$\nx^{2} - \\frac{4}{7} x^{2} = \\frac{8}{7} x,\n$$\n$$\n\\left(1 - \\frac{4}{7}\\right) x^{2} = \\frac{8}{7} x,\n$$\n$$\n\\frac{3}{7} x^{2} = \\frac{8}{7} x.\n$$\nMultiply both sides by $\\frac{7}{x}$ (assuming $x > 0$, which holds since $x = \\exp(\\beta)$):\n$$\n3 x = 8,\n$$\n$$\nx = \\frac{8}{3}.\n$$\nTherefore,\n$$\n\\beta = \\ln(x) = \\ln\\!\\left(\\frac{8}{3}\\right).\n$$\n\nWith this fitted $\\beta$, the full softmax distribution is\n$$\np_1^{\\mathrm{SM}} = \\frac{x^{2}}{x^{2} + 2x} = \\frac{\\left(\\frac{8}{3}\\right)^{2}}{\\left(\\frac{8}{3}\\right)^{2} + 2 \\cdot \\frac{8}{3}} = \\frac{\\frac{64}{9}}{\\frac{64}{9} + \\frac{16}{3}} = \\frac{\\frac{64}{9}}{\\frac{64}{9} + \\frac{48}{9}} = \\frac{\\frac{64}{9}}{\\frac{112}{9}} = \\frac{64}{112} = \\frac{4}{7},\n$$\nand, by symmetry for $g_2 = g_3$,\n$$\np_2^{\\mathrm{SM}} = p_3^{\\mathrm{SM}} = \\frac{x}{x^{2} + 2x} = \\frac{1}{x + 2} = \\frac{1}{\\frac{8}{3} + 2} = \\frac{1}{\\frac{8}{3} + \\frac{6}{3}} = \\frac{1}{\\frac{14}{3}} = \\frac{3}{14}.\n$$\nHence, the fitted softmax reproduces the entire DN probability vector exactly in this symmetric configuration: $\\left(p_1^{\\mathrm{SM}}, p_2^{\\mathrm{SM}}, p_3^{\\mathrm{SM}}\\right) = \\left(\\frac{4}{7}, \\frac{3}{14}, \\frac{3}{14}\\right) = \\left(p_1^{\\mathrm{DN}}, p_2^{\\mathrm{DN}}, p_3^{\\mathrm{DN}}\\right)$. A natural residual discrepancy measure, such as the Kullback-Leibler (KL) divergence, would be\n$$\nD_{\\mathrm{KL}}\\!\\left(p^{\\mathrm{DN}} \\,\\|\\, p^{\\mathrm{SM}}\\right) = \\sum_{i=1}^{3} p_i^{\\mathrm{DN}} \\ln\\!\\left(\\frac{p_i^{\\mathrm{DN}}}{p_i^{\\mathrm{SM}}}\\right) = 0,\n$$\nbecause each ratio $\\frac{p_i^{\\mathrm{DN}}}{p_i^{\\mathrm{SM}}} = 1$. The absence of residual differences here follows from two structural features: (i) the competitive DN formula used yields higher inhibition on the lower-gain alternatives when a high-gain competitor is present, and (ii) the softmax with a single parameter $\\beta$ must assign equal probabilities to alternatives with equal gains. When the gains are symmetric within the lower alternatives, matching the top probability determines $\\beta$ in a way that ensures all probabilities coincide. In more general, asymmetric scenarios (e.g., $g = (2,1,0.8)$) or with nonuniform inhibitory weights $\\sum_{j \\neq i} w_{ij} g_j$, DN and softmax will generally produce distinct distributions even after fitting $\\beta$ to match $p_1$, yielding nonzero residual discrepancies that reflect the different normalization architectures and nonlinearities.",
            "answer": "$$\\boxed{\\ln\\!\\left(\\frac{8}{3}\\right)}$$"
        },
        {
            "introduction": "Many real-world decisions are not one-shot choices but unfold over time, forcing a trade-off between gathering more information (exploration) and capitalizing on what is already known (exploitation). This computational challenge  moves beyond static choice probabilities to tackle this dynamic dilemma using the powerful framework of Markov Decision Processes. You will implement a dynamic programming algorithm to find the truly optimal policy in a classic multi-armed bandit scenario and compare it to a common heuristic, providing a concrete understanding of the principles of optimal sequential decision-making.",
            "id": "3999877",
            "problem": "Consider a triad of independent binary-outcome options (\"arms\") indexed by $i \\in \\{1,2,3\\}$. Each arm $i$ yields, upon exploitation at any discrete decision epoch, a Bernoulli reward with unknown success probability $\\theta_i \\in [0,1]$. The Bayesian decision maker has a conjugate Beta prior for each arm $i$, with parameters $(\\alpha_{i0},\\beta_{i0})$, so that the prior density for $\\theta_i$ is proportional to $\\theta_i^{\\alpha_{i0}-1}(1-\\theta_i)^{\\beta_{i0}-1}$. Time is divided into $T$ discrete decision epochs indexed by $t \\in \\{0,1,\\dots,T-1\\}$. At each epoch $t$ the decision maker may either sample one arm to observe a single Bernoulli outcome and update the corresponding posterior (with no direct reward from this observation), incurring a deterministic time cost $c > 0$, or stop sampling and exploit by committing to one arm for all remaining epochs, thereby receiving Bernoulli rewards at each remaining epoch. The objective is to maximize expected cumulative reward from exploitation minus the sum of sampling costs.\n\nFormalize the problem as a finite-horizon decision process over the Bayesian posterior sufficient statistics and time. Let the state be $s_t = \\big(t,(\\alpha_{i,t},\\beta_{i,t})_{i=1}^3\\big)$ where $(\\alpha_{i,t},\\beta_{i,t})$ are the posterior Beta parameters after the sampling history up to time $t$. The posterior predictive success probability for arm $i$ at time $t$ is $\\mu_{i,t} = \\frac{\\alpha_{i,t}}{\\alpha_{i,t}+\\beta_{i,t}}$ and the posterior variance of $\\theta_i$ is $v_{i,t} = \\frac{\\alpha_{i,t}\\beta_{i,t}}{(\\alpha_{i,t}+\\beta_{i,t})^2(\\alpha_{i,t}+\\beta_{i,t}+1)}$. If the decision maker stops at time $t$ and commits to arm $i^\\star \\in \\arg\\max_i \\mu_{i,t}$, the expected exploitation value is $(T-t)\\max_i \\mu_{i,t}$. If the decision maker samples arm $i$ at time $t$, the next state is either $s_{t+1}^+ = \\big(t+1,(\\alpha_{i,t}+1,\\beta_{i,t}),(\\alpha_{j,t},\\beta_{j,t})_{j\\neq i}\\big)$ with probability $\\mu_{i,t}$ (observed success) or $s_{t+1}^- = \\big(t+1,(\\alpha_{i,t},\\beta_{i,t}+1),(\\alpha_{j,t},\\beta_{j,t})_{j\\neq i}\\big)$ with probability $1-\\mu_{i,t}$ (observed failure), and the sampling incurs cost $c$ at time $t$.\n\nStarting from the Bayesian optimality principle for finite-horizon sequential decision making and the Bellman recursion, derive and implement a dynamic programming algorithm that computes the Bayesian optimal stopping rule and expected value function $V^\\star(s_t)$ at every reachable state. The derivation must start from the definitions of expected utility maximization under uncertainty, the Beta-Bernoulli conjugate update, and the Bellman optimality principle for finite-horizon decision processes.\n\nDefine also an urgency-gated softmax heuristic policy: at state $s_t$, assign a utility to stopping $U_{\\text{stop}}(s_t) = (T-t)\\max_i \\mu_{i,t}$ and a utility to sampling arm $i$, $U_{\\text{sample},i}(s_t) = \\alpha_{\\text{vi}}\\sqrt{v_{i,t}}\\cdot \\max(T-t-1,0) - c$, where $\\alpha_{\\text{vi}} > 0$ is a scalar \"value-of-information\" weight. Let the softmax temperature be $\\tau(t) = \\frac{\\tau_0}{1+\\gamma t}$ with $\\tau_0 > 0$ and $\\gamma \\ge 0$. The heuristic action probabilities are\n$$\n\\pi(\\text{stop}\\mid s_t) = \\frac{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right)}{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right) + \\sum_{i=1}^3 \\exp\\left(\\frac{U_{\\text{sample},i}(s_t)}{\\tau(t)}\\right)},\n\\quad\n\\pi(\\text{sample},i\\mid s_t) = \\frac{\\exp\\left(\\frac{U_{\\text{sample},i}(s_t)}{\\tau(t)}\\right)}{\\exp\\left(\\frac{U_{\\text{stop}}(s_t)}{\\tau(t)}\\right) + \\sum_{j=1}^3 \\exp\\left(\\frac{U_{\\text{sample},j}(s_t)}{\\tau(t)}\\right)}.\n$$\nCompute the expected value function $V_\\pi(s_t)$ induced by this heuristic policy via the finite-horizon Bellman expectation recursion with exact integration over posterior predictive outcomes.\n\nYour program must, for each test case described below, compute: $(1)$ a boolean indicating whether the Bayesian optimal policy stops immediately at $t=0$ (true if stop is chosen at $t=0$, false otherwise), $(2)$ the expected value $V^\\star(s_0)$ under the Bayesian optimal policy, $(3)$ the expected value $V_\\pi(s_0)$ under the urgency-gated softmax heuristic with the specified parameters, and $(4)$ the difference $V^\\star(s_0) - V_\\pi(s_0)$. All expectations must be exact with respect to the posterior predictive distributions, not Monte Carlo approximations. There are no physical units involved, and answers are real-valued floats or booleans.\n\nUse the following test suite of parameter sets (each case specifies $T$, $c$, and priors $(\\alpha_{i0},\\beta_{i0})$ for $i\\in\\{1,2,3\\}$, together with heuristic parameters $\\tau_0$, $\\gamma$, $\\alpha_{\\text{vi}}$):\n\n- Case $1$ (\"happy path\"): $T=4$, $c=0.1$, priors $(\\alpha_{10},\\beta_{10})=(1,1)$, $(\\alpha_{20},\\beta_{20})=(1,1)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, heuristic parameters $\\tau_0=0.5$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$.\n- Case $2$ (boundary, minimal horizon): $T=1$, $c=0.5$, priors $(\\alpha_{10},\\beta_{10})=(1,1)$, $(\\alpha_{20},\\beta_{20})=(1,1)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, heuristic parameters $\\tau_0=0.5$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$.\n- Case $3$ (informative prior on one arm, low sampling cost): $T=5$, $c=0.05$, priors $(\\alpha_{10},\\beta_{10})=(8,2)$, $(\\alpha_{20},\\beta_{20})=(2,8)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, heuristic parameters $\\tau_0=0.4$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$.\n- Case $4$ (high sampling cost): $T=5$, $c=0.6$, priors $(\\alpha_{10},\\beta_{10})=(1,1)$, $(\\alpha_{20},\\beta_{20})=(1,1)$, $(\\alpha_{30},\\beta_{30})=(1,1)$, heuristic parameters $\\tau_0=0.4$, $\\gamma=1.0$, $\\alpha_{\\text{vi}}=1.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the four quantities in the order described above. For example, the output format must be like $[[$boolean$, $float$, $float$, $float$],[$boolean$, $float$, $float$, $float$],\\dots]$.",
            "solution": "The problem describes a finite-horizon Bayesian optimal stopping problem, which is a specific type of Markov Decision Process (MDP). The objective is to devise a policy that maximizes the expected net reward, defined as the cumulative reward from exploitation minus the cumulative costs of sampling. We will solve this using dynamic programming via backward induction, as prescribed by the Bellman optimality principle.\n\n### 1. Formal Problem Specification\n\nLet us first formalize the components of the decision process.\n- **State Space**: The state at decision epoch $t \\in \\{0, 1, \\dots, T-1\\}$ is given by $s_t = \\big(t, (\\alpha_{1,t}, \\beta_{1,t}), (\\alpha_{2,t}, \\beta_{2,t}), (\\alpha_{3,t}, \\beta_{3,t})\\big)$. Here, $t$ is the current time, and $(\\alpha_{i,t}, \\beta_{i,t})$ are the parameters of the Beta posterior distribution for the unknown success probability $\\theta_i$ of arm $i$. The initial state is $s_0 = \\big(0, (\\alpha_{10}, \\beta_{10}), (\\alpha_{20}, \\beta_{20}), (\\alpha_{30}, \\beta_{30})\\big)$.\n\n- **Action Space**: At any state $s_t$ with $t < T$, the agent can choose an action $a_t$ from the set $A = \\{\\text{stop}\\} \\cup \\{\\text{sample } i\\}_{i=1}^3$.\n\n- **Transitions and Rewards**:\n    - If $a_t = \\text{stop}$, the process terminates. The agent receives a terminal reward equal to the expected sum of rewards from exploiting the best-looking arm for the remaining $T-t$ epochs. The posterior predictive mean success probability for arm $i$ is $\\mu_{i,t} = E[\\theta_i | s_t] = \\frac{\\alpha_{i,t}}{\\alpha_{i,t}+\\beta_{i,t}}$. The optimal arm to exploit is $i^\\star = \\arg\\max_i \\mu_{i,t}$. The terminal reward is thus $(T-t) \\max_i \\mu_{i,t}$.\n    - If $a_t = \\text{sample } i$, the agent pays a cost $c > 0$. A Bernoulli trial is performed on arm $i$. With probability $\\mu_{i,t}$ (the posterior predictive probability of success), the outcome is a success, and the state transitions to $s_{t+1}^+ = \\big(t+1, \\dots, (\\alpha_{i,t}+1, \\beta_{i,t}), \\dots\\big)$. With probability $1-\\mu_{i,t}$, the outcome is a failure, and the state transitions to $s_{t+1}^- = \\big(t+1, \\dots, (\\alpha_{i,t}, \\beta_{i,t}+1), \\dots\\big)$. The parameters for arms $j \\neq i$ remain unchanged.\n\n### 2. Bayesian Optimal Policy and Value Function $V^\\star(s_t)$\n\nThe goal is to find an optimal policy $\\pi^\\star$ that maximizes the expected total net reward. The value of this optimal policy, $V^\\star(s_t) = \\max_{\\pi} E_\\pi \\left[ \\sum_{k=t}^{T-1} R(s_k, a_k) | s_t \\right]$, can be found using the Bellman principle of optimality. For a finite-horizon problem, this implies a backward induction procedure.\n\nThe value function at the terminal time $t=T$ is $V^\\star(s_T) = 0$, as no further actions or rewards are possible.\n\nFor any time $t \\in \\{T-1, T-2, \\dots, 0\\}$, the optimal value function $V^\\star(s_t)$ is the maximum of the values of all possible actions at state $s_t$. Let's define the action-value function $Q^\\star(s_t, a_t)$ as the expected return of taking action $a_t$ in state $s_t$ and following the optimal policy thereafter.\n\n- **Value of Stopping**: If the action is to stop, the process ends, and the value is the immediate terminal reward.\n$$Q^\\star(s_t, \\text{stop}) = (T-t) \\max_{i \\in \\{1,2,3\\}} \\mu_{i,t}$$\n\n- **Value of Sampling**: If the action is to sample arm $i$, the agent incurs a cost $c$ and then transitions to a new state $s_{t+1}$, from which it will obtain the optimal value $V^\\star(s_{t+1})$. The value is the cost plus the expected optimal value of the next state.\n$$Q^\\star(s_t, \\text{sample } i) = -c + E[V^\\star(s_{t+1}) | s_t, a_t=\\text{sample } i]$$\nThe expectation is taken over the two possible outcomes of the sample (success or failure):\n$$Q^\\star(s_t, \\text{sample } i) = -c + \\mu_{i,t} V^\\star(s_{t+1}^+) + (1-\\mu_{i,t}) V^\\star(s_{t+1}^-)$$\nwhere $s_{t+1}^+$ and $s_{t+1}^-$ are the successor states after observing a success or failure on arm $i$, respectively.\n\nThe Bellman optimality equation is then:\n$$V^\\star(s_t) = \\max \\left( Q^\\star(s_t, \\text{stop}), \\max_{i \\in \\{1,2,3\\}} Q^\\star(s_t, \\text{sample } i) \\right)$$\nThe optimal action $a^\\star(s_t)$ is the action that achieves this maximum. This recursion can be solved by starting at $t=T-1$ and working backwards to $t=0$.\n\n### 3. Heuristic Policy and Value Function $V_\\pi(s_t)$\n\nThe problem also defines a heuristic policy $\\pi$ based on a softmax function over action utilities. The value function $V_\\pi(s_t)$ for this policy can be computed using the Bellman expectation equation, which is similar to the optimality equation but replaces the $\\max$ operator with an expectation over the policy's action probabilities.\n\nThe utilities are defined as:\n- $U_{\\text{stop}}(s_t) = (T-t)\\max_i \\mu_{i,t}$\n- $U_{\\text{sample},i}(s_t) = \\alpha_{\\text{vi}}\\sqrt{v_{i,t}}\\cdot \\max(T-t-1,0) - c$\n\nwhere $v_{i,t} = \\frac{\\alpha_{i,t}\\beta_{i,t}}{(\\alpha_{i,t}+\\beta_{i,t})^2(\\alpha_{i,t}+\\beta_{i,t}+1)}$ is the posterior variance of $\\theta_i$. The temperature is $\\tau(t) = \\frac{\\tau_0}{1+\\gamma t}$. The action probabilities are:\n$$\n\\pi(\\text{action}|s_t) = \\frac{\\exp\\left( \\frac{U_{\\text{action}}(s_t)}{\\tau(t)} \\right)}{\\sum_{\\text{action}'} \\exp\\left( \\frac{U_{\\text{action}'}(s_t)}{\\tau(t)} \\right)}\n$$\nThe Bellman expectation equation for $V_\\pi$ is:\n$$V_\\pi(s_t) = \\sum_{a_t \\in A} \\pi(a_t|s_t) Q_\\pi(s_t, a_t)$$\nwhere $Q_\\pi(s_t, a_t)$ is the value of taking action $a_t$ and then following policy $\\pi$.\n\n- If $a_t = \\text{stop}$: $Q_\\pi(s_t, \\text{stop}) = (T-t) \\max_i \\mu_{i,t}$.\n- If $a_t = \\text{sample } i$: $Q_\\pi(s_t, \\text{sample } i) = -c + E[V_\\pi(s_{t+1}) | s_t, a_t=\\text{sample } i]$. This expectation is expanded as before:\n$$Q_\\pi(s_t, \\text{sample } i) = -c + \\mu_{i,t}V_\\pi(s_{t+1}^+) + (1-\\mu_{i,t})V_\\pi(s_{t+1}^-)$$\n\nSubstituting these into the Bellman equation gives the recursion for $V_\\pi$:\n$$\nV_\\pi(s_t) = \\pi(\\text{stop}|s_t) \\left( (T-t)\\max_i \\mu_{i,t} \\right) + \\sum_{i=1}^3 \\pi(\\text{sample},i|s_t) \\left( -c + \\mu_{i,t}V_\\pi(s_{t+1}^+) + (1-\\mu_{i,t})V_\\pi(s_{t+1}^-) \\right)\n$$\nAs with $V^\\star$, this is solved by backward induction from the boundary condition $V_\\pi(s_T)=0$.\n\n### 4. Algorithmic Implementation\n\nA dynamic programming algorithm is implemented to solve these recursions. We use a top-down recursive approach with memoization (caching) to avoid recomputing values for the same state. A dictionary is used as a cache, where the key is the state tuple $(t, \\text{priors})$.\n\nTwo recursive functions, `compute_V_star` and `compute_V_pi`, are implemented. Each function takes a state $(t, \\text{priors})$ as input.\n1. It first checks the memoization cache for a pre-computed value for this state.\n2. If $t=T$, it returns $0$.\n3. Otherwise, it computes the values of stopping and sampling by recursively calling itself for states at time $t+1$.\n4. It then combines these values according to a) the Bellman optimality equation for $V^\\star$ or b) the Bellman expectation equation for $V_\\pi$.\n5. The result is stored in the cache and returned.\n\nTo determine the optimal action at the initial state $s_0$, we explicitly compute $Q^\\star(s_0, \\text{stop})$ and $Q^\\star(s_0, \\text{sample } i)$ for all $i$. This requires the values of $V^\\star(s_1)$ for all possible successor states, which are obtained from the recursive solver. The optimal action is then found by comparing these Q-values. If the value of stopping is greater than or equal to the maximum value of sampling, the optimal policy stops. The final program iterates through the provided test cases, setting up the parameters for each and invoking the solvers to compute the required quantities.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    class Solver:\n        \"\"\"\n        Solves the Bayesian optimal stopping problem for a given set of parameters.\n        \"\"\"\n\n        def __init__(self, T, c, priors_0, tau_0, gamma, alpha_vi):\n            self.T = T\n            self.c = c\n            self.priors_0 = tuple(tuple(p) for p in priors_0)\n            self.tau_0 = tau_0\n            self.gamma = gamma\n            self.alpha_vi = alpha_vi\n            \n            # Memoization caches for the value functions\n            self.memo_v_star = {}\n            self.memo_v_pi = {}\n\n        def run(self):\n            \"\"\"\n            Computes the four required quantities for the initial state s_0.\n            \"\"\"\n            # 1. Optimal Policy: V_star and immediate action\n            alphas_0 = np.array([p[0] for p in self.priors_0], dtype=float)\n            betas_0 = np.array([p[1] for p in self.priors_0], dtype=float)\n            mus_0 = alphas_0 / (alphas_0 + betas_0)\n\n            # Value of stopping at t=0\n            v_stop_s0 = self.T * np.max(mus_0)\n            \n            # Value of sampling each arm at t=0\n            v_samples_s0 = []\n            for i in range(3):\n                # Success state\n                priors_succ_list = list(self.priors_0)\n                priors_succ_list[i] = (self.priors_0[i][0] + 1, self.priors_0[i][1])\n                v_succ = self._compute_v_star(1, tuple(priors_succ_list))\n\n                # Failure state\n                priors_fail_list = list(self.priors_0)\n                priors_fail_list[i] = (self.priors_0[i][0], self.priors_0[i][1] + 1)\n                v_fail = self._compute_v_star(1, tuple(priors_fail_list))\n                \n                v_sample_i = -self.c + mus_0[i] * v_succ + (1 - mus_0[i]) * v_fail\n                v_samples_s0.append(v_sample_i)\n\n            v_star_s0 = max(v_stop_s0, max(v_samples_s0))\n            is_stop_optimal = (v_stop_s0 >= max(v_samples_s0))\n\n            # 2. Heuristic Policy: V_pi\n            v_pi_s0 = self._compute_v_pi(0, self.priors_0)\n\n            # 3. Difference\n            diff = v_star_s0 - v_pi_s0\n            \n            return [is_stop_optimal, v_star_s0, v_pi_s0, diff]\n\n        def _compute_v_star(self, t, priors):\n            \"\"\"\n            Computes the optimal value function V* using backward recursion.\n            \"\"\"\n            state = (t, priors)\n            if state in self.memo_v_star:\n                return self.memo_v_star[state]\n            \n            if t == self.T:\n                return 0.0\n\n            alphas = np.array([p[0] for p in priors], dtype=float)\n            betas = np.array([p[1] for p in priors], dtype=float)\n            mus = alphas / (alphas + betas)\n            \n            # Value of stopping\n            v_stop = (self.T - t) * np.max(mus)\n            \n            # Value of sampling\n            v_samples = []\n            for i in range(3):\n                priors_succ_list = list(priors)\n                priors_succ_list[i] = (alphas[i] + 1, betas[i])\n                v_succ = self._compute_v_star(t + 1, tuple(priors_succ_list))\n\n                priors_fail_list = list(priors)\n                priors_fail_list[i] = (alphas[i], betas[i] + 1)\n                v_fail = self._compute_v_star(t + 1, tuple(priors_fail_list))\n                \n                v_sample_i = -self.c + mus[i] * v_succ + (1 - mus[i]) * v_fail\n                v_samples.append(v_sample_i)\n\n            v_star = max([v_stop] + v_samples)\n            self.memo_v_star[state] = v_star\n            return v_star\n\n        def _compute_v_pi(self, t, priors):\n            \"\"\"\n            Computes the heuristic policy's value function V_pi using backward recursion.\n            \"\"\"\n            state = (t, priors)\n            if state in self.memo_v_pi:\n                return self.memo_v_pi[state]\n            \n            if t == self.T:\n                return 0.0\n\n            alphas = np.array([p[0] for p in priors], dtype=float)\n            betas = np.array([p[1] for p in priors], dtype=float)\n            mus = alphas / (alphas + betas)\n            denominators = alphas + betas\n            variances = (alphas * betas) / (denominators**2 * (denominators + 1))\n            \n            # Heuristic utilities\n            u_stop = (self.T - t) * np.max(mus)\n            rem_exploitation_steps = max(self.T - t - 1, 0)\n            u_samples = self.alpha_vi * np.sqrt(variances) * rem_exploitation_steps - self.c\n\n            # Softmax probabilities\n            temp = self.tau_0 / (1 + self.gamma * t)\n            \n            utils = np.concatenate(([u_stop], u_samples))\n            \n            if temp == 0: # Zero-temperature limit (greedy)\n                pi_actions = np.zeros_like(utils)\n                winner_indices = np.where(utils == np.max(utils))[0]\n                pi_actions[winner_indices] = 1.0 / len(winner_indices)\n            else: # Standard softmax with stability trick\n                utils_shifted = utils - np.max(utils)\n                exp_utils = np.exp(utils_shifted / temp)\n                pi_actions = exp_utils / np.sum(exp_utils)\n            \n            pi_stop = pi_actions[0]\n            pi_samples = pi_actions[1:]\n            \n            # Bellman Expectation\n            # Contribution from stopping\n            v_pi = pi_stop * u_stop\n            \n            # Contribution from sampling\n            for i in range(3):\n                if pi_samples[i] > 0:\n                    priors_succ_list = list(priors)\n                    priors_succ_list[i] = (alphas[i] + 1, betas[i])\n                    v_succ = self._compute_v_pi(t + 1, tuple(priors_succ_list))\n\n                    priors_fail_list = list(priors)\n                    priors_fail_list[i] = (alphas[i], betas[i] + 1)\n                    v_fail = self._compute_v_pi(t + 1, tuple(priors_fail_list))\n                    \n                    v_sample_i_future = -self.c + mus[i] * v_succ + (1 - mus[i]) * v_fail\n                    v_pi += pi_samples[i] * v_sample_i_future\n\n            self.memo_v_pi[state] = v_pi\n            return v_pi\n\n    test_cases = [\n        {'T': 4, 'c': 0.1, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.5, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 1, 'c': 0.5, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.5, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 5, 'c': 0.05, 'priors_0': [[8,2],[2,8],[1,1]], 'tau_0': 0.4, 'gamma': 1.0, 'alpha_vi': 1.0},\n        {'T': 5, 'c': 0.6, 'priors_0': [[1,1],[1,1],[1,1]], 'tau_0': 0.4, 'gamma': 1.0, 'alpha_vi': 1.0},\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        solver_instance = Solver(**case_params)\n        result = solver_instance.run()\n        all_results.append(result)\n    \n    # Format the final output string\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}