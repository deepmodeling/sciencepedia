{
    "hands_on_practices": [
        {
            "introduction": "Hippocampal indexing theory posits that unique identifiers are crucial for segregating episodic memories. This exercise models how these indices might arise from the spatially-tuned activity of grid cells in the medial entorhinal cortex. You will explore how global remapping, a systematic shift in grid cell firing patterns, can generate a novel and distinct neural code for a new environment, providing a concrete mechanism for creating orthogonal indices that minimize interference between memories .",
            "id": "3988822",
            "problem": "Consider a formalization of hippocampal indexing theory in which the hippocampus stores an index code for episodic bindings by linearly reading out a spatial basis constructed from medial entorhinal cortex (MEC) grid cells. Let the one-dimensional environment coordinate be $s \\in [0,L]$ and assume $M$ independent grid modules indexed by $j \\in \\{1,\\dots,M\\}$. The grid response of module $j$ at position $s$ is given by $g_{j}(s;\\phi_{j}) = \\cos(k_{j} s + \\phi_{j})$, where $k_{j} = \\frac{2\\pi}{\\lambda_{j}}$ and $\\lambda_{j} > 0$ is the grid period of module $j$, and $\\phi_{j} \\in [0,2\\pi)$ is its phase. Stack the module responses into the $M$-dimensional grid code $g(s;\\phi) = \\big(g_{1}(s;\\phi_{1}),\\dots,g_{M}(s;\\phi_{M})\\big)^{\\top}$. The hippocampal index code for context $c$ is $h(s;c) = W g(s;\\phi^{(c)})$, where $W \\in \\mathbb{R}^{N \\times M}$ is a fixed readout with full column rank and $\\phi^{(c)}$ is the phase vector for context $c$.\n\nDefine global remapping between two contexts $c$ and $c'$ as a uniform phase offset applied to all modules: there exists a single $\\delta \\in [0,2\\pi)$ such that $\\phi^{(c')}_{j} = \\phi^{(c)}_{j} + \\delta$ for all $j$. To quantify cross-context interference in the index code while abstracting away the readout $W$, consider the interference functional\n$$\nJ(\\delta) = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\frac{1}{L} \\int_{0}^{L} g_{j}\\big(s;\\phi^{(c)}_{j}\\big)\\, g_{j}\\big(s;\\phi^{(c)}_{j}+\\delta\\big)\\, \\mathrm{d}s \\right)^{2}.\n$$\nAssume the following foundational conditions:\n- $L$ is an integer multiple of every $\\lambda_{j}$, or $s$ is uniformly sampled over $[0,L]$ with $L$ sufficiently large to average over many fundamental periods.\n- The grid periods $\\{\\lambda_{j}\\}_{j=1}^{M}$ are pairwise incommensurate.\n- The readout $W$ has full column rank and does not depend on context.\n\nStarting from these assumptions and the definition of $J(\\delta)$, derive a closed-form expression for $J(\\delta)$ and determine the value of the global phase offset $\\delta^{\\star}$ that minimizes $J(\\delta)$ under the constraint that distinct episodic bindings within each context are preserved in the sense that the map $s \\mapsto h(s;c)$ remains injective almost everywhere. Provide the final answer as the value of $\\delta^{\\star}$ in radians. No rounding is required. Express the angle in radians.",
            "solution": "The problem asks for a closed-form expression for the interference functional $J(\\delta)$ and the value of the global phase offset $\\delta^{\\star}$ that minimizes this functional, subject to a constraint on injectivity.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Environment coordinate: $s \\in [0,L]$.\n- Number of grid modules: $M$, indexed by $j \\in \\{1,\\dots,M\\}$.\n- Grid response of module $j$: $g_{j}(s;\\phi_{j}) = \\cos(k_{j} s + \\phi_{j})$.\n- Wavenumber: $k_{j} = \\frac{2\\pi}{\\lambda_{j}}$, where $\\lambda_{j} > 0$ is the grid period.\n- Phase: $\\phi_{j} \\in [0,2\\pi)$.\n- Grid code vector: $g(s;\\phi) = \\big(g_{1}(s;\\phi_{1}),\\dots,g_{M}(s;\\phi_{M})\\big)^{\\top}$.\n- Hippocampal index code: $h(s;c) = W g(s;\\phi^{(c)})$, where $W \\in \\mathbb{R}^{N \\times M}$ is a fixed readout matrix.\n- Global remapping: $\\phi^{(c')}_{j} = \\phi^{(c)}_{j} + \\delta$ for all $j$, with $\\delta \\in [0,2\\pi)$.\n- Interference functional: $J(\\delta) = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\frac{1}{L} \\int_{0}^{L} g_{j}\\big(s;\\phi^{(c)}_{j}\\big)\\, g_{j}\\big(s;\\phi^{(c)}_{j}+\\delta\\big)\\, \\mathrm{d}s \\right)^{2}$.\n- Foundational conditions:\n    1. $L$ is an integer multiple of every $\\lambda_{j}$, or $L$ is sufficiently large for spatial averaging.\n    2. Grid periods $\\{\\lambda_{j}\\}_{j=1}^{M}$ are pairwise incommensurate.\n    3. Readout matrix $W$ has full column rank and is context-independent.\n- Constraint for minimization: The map $s \\mapsto h(s;c)$ must remain injective almost everywhere.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the domain of computational neuroscience, specifically using a standard formal model of grid cells and hippocampal indexing. The terms are precisely defined mathematically, and the problem is self-contained. The assumptions provided are standard in this type of analysis and are necessary for a tractable solution. The constraint regarding injectivity is a key feature of grid coding theory, where incommensurate periods ensure that the state-space trajectory of the grid code vector does not self-intersect, allowing for unique encoding of positions. This constraint's relevance to the minimization must be assessed, but its inclusion does not invalidate the problem structure. The problem is well-posed, objective, and does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nFirst, we derive the closed-form expression for $J(\\delta)$. The functional is defined as:\n$$\nJ(\\delta) = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\frac{1}{L} \\int_{0}^{L} g_{j}\\big(s;\\phi^{(c)}_{j}\\big)\\, g_{j}\\big(s;\\phi^{(c)}_{j}+\\delta\\big)\\, \\mathrm{d}s \\right)^{2}\n$$\nLet's analyze the integral term for a single module $j$, which we denote by $I_j(\\delta)$:\n$$\nI_j(\\delta) = \\frac{1}{L} \\int_{0}^{L} g_{j}\\big(s;\\phi^{(c)}_{j}\\big)\\, g_{j}\\big(s;\\phi^{(c)}_{j}+\\delta\\big)\\, \\mathrm{d}s\n$$\nSubstituting the definition of $g_j(s;\\phi)$, we have:\n$$\nI_j(\\delta) = \\frac{1}{L} \\int_{0}^{L} \\cos(k_{j} s + \\phi^{(c)}_{j}) \\cos(k_{j} s + \\phi^{(c)}_{j} + \\delta) \\, \\mathrm{d}s\n$$\nWe use the product-to-sum trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}(\\cos(A-B) + \\cos(A+B))$.\nLet $A = k_{j} s + \\phi^{(c)}_{j} + \\delta$ and $B = k_{j} s + \\phi^{(c)}_{j}$. Then $A-B = \\delta$ and $A+B = 2k_{j} s + 2\\phi^{(c)}_{j} + \\delta$.\nThe integral becomes:\n$$\nI_j(\\delta) = \\frac{1}{L} \\int_{0}^{L} \\frac{1}{2} \\left[ \\cos(\\delta) + \\cos(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta) \\right] \\mathrm{d}s\n$$\nWe can split the integral into two parts:\n$$\nI_j(\\delta) = \\frac{1}{2L} \\left[ \\int_{0}^{L} \\cos(\\delta) \\, \\mathrm{d}s + \\int_{0}^{L} \\cos(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta) \\, \\mathrm{d}s \\right]\n$$\nFor the first integral, $\\cos(\\delta)$ is a constant with respect to $s$, so $\\int_{0}^{L} \\cos(\\delta) \\, \\mathrm{d}s = L \\cos(\\delta)$.\nFor the second integral, we have:\n$$\n\\int_{0}^{L} \\cos(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta) \\, \\mathrm{d}s = \\left[ \\frac{\\sin(2k_{j} s + 2\\phi^{(c)}_{j} + \\delta)}{2k_{j}} \\right]_{0}^{L}\n$$\nThis evaluates to:\n$$\n\\frac{1}{2k_{j}} \\left[ \\sin(2k_{j}L + 2\\phi^{(c)}_{j} + \\delta) - \\sin(2\\phi^{(c)}_{j} + \\delta) \\right]\n$$\nHere we must use the foundational condition that $L$ is an integer multiple of every period $\\lambda_j$. Let $L = n_j \\lambda_j$ for some integer $n_j$. Since $k_j = \\frac{2\\pi}{\\lambda_j}$, it follows that $k_j L = \\frac{2\\pi}{\\lambda_j} (n_j \\lambda_j) = 2\\pi n_j$.\nTherefore, $2k_j L = 4\\pi n_j$, which is an integer multiple of $2\\pi$. As the sine function is periodic with period $2\\pi$, we have $\\sin(4\\pi n_j + \\theta) = \\sin(\\theta)$. The expression for the second integral simplifies to:\n$$\n\\frac{1}{2k_{j}} \\left[ \\sin(2\\phi^{(c)}_{j} + \\delta) - \\sin(2\\phi^{(c)}_{j} + \\delta) \\right] = 0\n$$\nThe same result holds under the alternative assumption of a sufficiently large $L$, as the integral of an oscillating function over many periods averages to zero.\nThus, the second integral vanishes, and we are left with:\n$$\nI_j(\\delta) = \\frac{1}{2L} [L \\cos(\\delta) + 0] = \\frac{1}{2} \\cos(\\delta)\n$$\nThis result is independent of the module index $j$ and the initial phase $\\phi^{(c)}_j$. Now we substitute this back into the expression for $J(\\delta)$:\n$$\nJ(\\delta) = \\frac{1}{M} \\sum_{j=1}^{M} \\left( \\frac{1}{2} \\cos(\\delta) \\right)^{2} = \\frac{1}{M} \\sum_{j=1}^{M} \\frac{1}{4} \\cos^2(\\delta)\n$$\nSince the term in the sum is constant, the sum is $M$ times the term:\n$$\nJ(\\delta) = \\frac{1}{M} \\cdot M \\cdot \\frac{1}{4} \\cos^2(\\delta) = \\frac{1}{4} \\cos^2(\\delta)\n$$\nThis is the closed-form expression for the interference functional $J(\\delta)$.\n\nNext, we must find the value $\\delta^{\\star}$ that minimizes $J(\\delta)$. This is equivalent to minimizing $\\cos^2(\\delta)$ for $\\delta \\in [0, 2\\pi)$. The function $\\cos^2(\\delta)$ is non-negative, and its minimum value is $0$. This minimum occurs when $\\cos(\\delta) = 0$. The values of $\\delta$ in the interval $[0, 2\\pi)$ that satisfy this condition are $\\delta = \\frac{\\pi}{2}$ and $\\delta = \\frac{3\\pi}{2}$.\n\nFinally, we consider the constraint that the map $s \\mapsto h(s;c)$ must remain injective almost everywhere. The code for any context $c$ is $h(s;c) = W g(s;\\phi^{(c)})$. Since $W$ has full column rank, injectivity of $h$ with respect to $s$ is equivalent to injectivity of the grid code $g(s;\\phi^{(c)})$. The injectivity of the grid code mapping from position $s$ to the vector space $\\mathbb{R}^M$ is guaranteed (almost everywhere) by the foundational condition that the grid periods $\\{\\lambda_j\\}$ are pairwise incommensurate. This ensures the trajectory traced by $g(s;\\phi^{(c)})$ as $s$ varies does not self-intersect.\nApplying a global phase offset $\\delta$ only shifts the starting phase of this trajectory for all modules simultaneously. It does not alter the frequencies $\\{k_j\\}$ or their incommensurability. Thus, the geometric properties of the trajectory that guarantee injectivity are preserved for any value of $\\delta$. The constraint is therefore satisfied by all possible values of $\\delta$, including the two values that minimize $J(\\delta)$.\n\nThe problem asks for \"the value\" $\\delta^{\\star}$, which suggests a single answer. By convention, in the absence of other criteria, the smallest positive angle is chosen. Both $\\frac{\\pi}{2}$ and $\\frac{3\\pi}{2}$ result in maximal decorrelation (orthogonality on average) between the representations of the two contexts, minimizing interference. We select the principal value.\n\nTherefore, the value of the global phase offset that minimizes the interference is $\\delta^{\\star} = \\frac{\\pi}{2}$.",
            "answer": "$$\n\\boxed{\\frac{\\pi}{2}}\n$$"
        },
        {
            "introduction": "Once a unique index is generated, the hippocampus must link it to the specific cortical features that constitute an episodic memory. This practice investigates the fundamental synaptic mechanism proposed for this binding process: Hebbian plasticity. By mathematically analyzing a simplified learning model, you will quantify how the connection strengths between the hippocampus and cortex evolve as more memories are stored, revealing the physical basis of the index and its relationship to memory load .",
            "id": "3988866",
            "problem": "Consider a simplified hippocampal indexing theory model in which a population of $N_{H}$ hippocampal indexing units binds to a distributed cortical population of $N_{C}$ feature units. Each episode $p \\in \\{1,\\dots,P\\}$ is represented by a hippocampal index vector $h^{(p)} \\in \\{0,1\\}^{N_{H}}$ and a cortical pattern vector $x^{(p)} \\in \\{0,1\\}^{N_{C}}$. Assume the following foundational modeling assumptions, based on the Hebbian postulate and sparse coding observations in cortex and hippocampus:\n\n- For each episode $p$, each component $h_{i}^{(p)}$ and $x_{j}^{(p)}$ is an independent Bernoulli random variable equal to $1$ with probability $a \\in (0,1)$ and $0$ otherwise, independently across indices $i$, $j$, and episodes $p$.\n- Synaptic weights are initialized at $0$ and updated once per episode by a Hebbian outer-product rule that binds the active hippocampal index to the concurrently active cortical features:\n$$\n\\Delta W_{CH} \\;=\\; \\eta \\, h^{(p)} \\big(x^{(p)}\\big)^{\\top} ,\n$$\nwhere $W_{CH} \\in \\mathbb{R}^{N_{H} \\times N_{C}}$ and $\\eta > 0$ is a learning rate. Hippocampal-to-cortical reinstatement at recall may be produced by $x_{\\mathrm{reinst}} \\,=\\, W_{CH}^{\\top} h$, which is consistent with $W_{CH}$ storing hippocampal-to-cortical bindings via its transpose.\n\nStarting from these assumptions alone, derive an analytic expression for the expected squared Frobenius norm of the weight matrix after storing $P$ statistically independent episodes:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] ,\n$$\nas a function of $N_{H}$, $N_{C}$, $\\eta$, $P$, and $a$. Express your final answer in closed form. No numerical approximation or rounding is required.",
            "solution": "The problem statement is a valid, well-posed problem in computational neuroscience. All variables, parameters, and statistical assumptions are clearly defined, allowing for a rigorous mathematical derivation. I will now proceed with the solution.\n\nThe objective is to compute the expected squared Frobenius norm of the weight matrix $W_{CH}^{(P)}$ after $P$ episodes. The weight matrix is given by the accumulation of updates from an initial state of $W_{CH}^{(0)} = 0$:\n$$\nW_{CH}^{(P)} = \\sum_{p=1}^{P} \\eta \\, h^{(p)} (x^{(p)})^{\\top}\n$$\nwhere $\\eta$ is the learning rate, $h^{(p)} \\in \\{0,1\\}^{N_H}$ is the hippocampal index vector, and $x^{(p)} \\in \\{0,1\\}^{N_C}$ is the cortical pattern vector for episode $p$. The components of the weight matrix are:\n$$\nW_{CH, ij}^{(P)} = \\eta \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)}\n$$\nfor $i \\in \\{1,\\dots,N_H\\}$ and $j \\in \\{1,\\dots,N_C\\}$.\n\nThe squared Frobenius norm of $W_{CH}^{(P)}$ is the sum of the squares of its elements:\n$$\n\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2} = \\sum_{i=1}^{N_H} \\sum_{j=1}^{N_C} \\left( W_{CH, ij}^{(P)} \\right)^2\n$$\nWe are asked to find the expectation $\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right]$. Using the linearity of the expectation operator, we can write:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] = \\mathbb{E}\\!\\left[ \\sum_{i=1}^{N_H} \\sum_{j=1}^{N_C} \\left( W_{CH, ij}^{(P)} \\right)^2 \\right] = \\sum_{i=1}^{N_H} \\sum_{j=1}^{N_C} \\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]\n$$\nThe problem states that the vector components $h_i^{(p)}$ and $x_j^{(p)}$ are independent and identically distributed (i.i.d.) Bernoulli random variables for all $i, j, p$. Consequently, the statistical properties of a weight $W_{CH, ij}^{(P)}$ are identical for all index pairs $(i,j)$. This means the expectation $\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]$ is constant for all $i, j$. We can therefore simplify the sum:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] = N_H N_C \\, \\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]\n$$\nNow, let's compute the expectation of a single squared weight element:\n$$\n\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right] = \\mathbb{E}\\!\\left[ \\left( \\eta \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right] = \\eta^2 \\, \\mathbb{E}\\!\\left[ \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right]\n$$\nExpanding the square of the sum:\n$$\n\\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 = \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right) \\left( \\sum_{q=1}^{P} h_i^{(q)} x_j^{(q)} \\right) = \\sum_{p=1}^{P} \\sum_{q=1}^{P} h_i^{(p)} x_j^{(p)} h_i^{(q)} x_j^{(q)}\n$$\nTaking the expectation gives:\n$$\n\\mathbb{E}\\!\\left[ \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right] = \\sum_{p=1}^{P} \\sum_{q=1}^{P} \\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(q)} x_j^{(p)} x_j^{(q)} \\right]\n$$\nWe can split the double summation into two cases: diagonal terms where $p=q$, and off-diagonal terms where $p \\neq q$.\n\nCase 1: $p=q$.\nThe terms are of the form $\\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(p)} x_j^{(p)} x_j^{(p)} \\right] = \\mathbb{E}\\!\\left[ (h_i^{(p)})^2 (x_j^{(p)})^2 \\right]$.\nThe variables $h_i^{(p)}$ and $x_j^{(p)}$ are Bernoulli random variables, taking values in $\\{0,1\\}$. Therefore, $(h_i^{(p)})^2 = h_i^{(p)}$ and $(x_j^{(p)})^2 = x_j^{(p)}$. The expectation becomes $\\mathbb{E}\\!\\left[ h_i^{(p)} x_j^{(p)} \\right]$.\nSince $h_i^{(p)}$ and $x_j^{(p)}$ are independent, $\\mathbb{E}\\!\\left[ h_i^{(p)} x_j^{(p)} \\right] = \\mathbb{E}\\!\\left[ h_i^{(p)} \\right] \\mathbb{E}\\!\\left[ x_j^{(p)} \\right]$.\nGiven that $P(h_i^{(p)}=1) = a$ and $P(x_j^{(p)}=1) = a$, their expectations are both $a$.\nSo, for $p=q$, the expectation is $a \\cdot a = a^2$. There are $P$ such diagonal terms.\n\nCase 2: $p \\neq q$.\nThe terms are of the form $\\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(q)} x_j^{(p)} x_j^{(q)} \\right]$.\nThe problem states that all random variables are independent across indices $i, j,$ and episodes $p$. Therefore, all four random variables in the expectation are mutually independent.\n$$\n\\mathbb{E}\\!\\left[ h_i^{(p)} h_i^{(q)} x_j^{(p)} x_j^{(q)} \\right] = \\mathbb{E}\\!\\left[ h_i^{(p)} \\right] \\mathbb{E}\\!\\left[ h_i^{(q)} \\right] \\mathbb{E}\\!\\left[ x_j^{(p)} \\right] \\mathbb{E}\\!\\left[ x_j^{(q)} \\right]\n$$\nEach expectation is equal to $a$. So, the total expectation is $a \\cdot a \\cdot a \\cdot a = a^4$.\nThere are $P^2 - P = P(P-1)$ off-diagonal terms where $p \\neq q$.\n\nCombining these two cases, we get:\n$$\n\\mathbb{E}\\!\\left[ \\left( \\sum_{p=1}^{P} h_i^{(p)} x_j^{(p)} \\right)^2 \\right] = \\sum_{p=q} a^2 + \\sum_{p \\neq q} a^4 = P \\cdot a^2 + P(P-1) \\cdot a^4\n$$\nSubstituting this back into the expression for $\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right]$:\n$$\n\\mathbb{E}\\!\\left[ \\left( W_{CH, ij}^{(P)} \\right)^2 \\right] = \\eta^2 \\left[ P a^2 + P(P-1) a^4 \\right]\n$$\nFinally, we multiply by $N_H N_C$ to obtain the expected squared Frobenius norm:\n$$\n\\mathbb{E}\\!\\left[\\left\\| W_{CH}^{(P)} \\right\\|_{F}^{2}\\right] = N_H N_C \\eta^2 \\left[ P a^2 + P(P-1) a^4 \\right]\n$$\nThis is the final closed-form expression as a function of the given parameters.",
            "answer": "$$\n\\boxed{N_{H} N_{C} \\eta^{2} \\left[ P a^{2} + P(P-1) a^{4} \\right]}\n$$"
        },
        {
            "introduction": "Memories are dynamic entities that are actively maintained and consolidated over time, rather than being static snapshots. This exercise models how hippocampal replay, a core mechanism implicated in memory consolidation, influences the long-term persistence of a memory. By solving the differential equations that govern the strength of hippocampal and cortical traces, you will compute learning curves and discover how replay frequency determines the ultimate stability and recallability of an episodic memory, contrasting active maintenance with passive forgetting .",
            "id": "3988859",
            "problem": "Consider a simplified model of Hippocampal Indexing Theory (HIT), in which a hippocampal index binds distributed cortical features into an episodic memory. Let the cortical trace strength be denoted by $s(t)$ and the hippocampal index strength be denoted by $h(t)$ for time $t \\ge 0$. Assume an initial encoding of an episode at $t=0$ that sets $s(0) = s_0$ and $h(0) = h_0$. Assume hippocampal replay occurs as reactivation events with mean rate $r \\ge 0$, modeled deterministically by a constant rate approximation.\n\nThe dynamics of $s(t)$ under replay are given by the first-order linear differential equation\n$$\n\\frac{ds}{dt} = \\alpha r \\left(1 - \\frac{s}{K}\\right) - \\lambda s,\n$$\nwith cortical capacity $K>0$, learning gain $\\alpha>0$, and forgetting rate $\\lambda>0$. The dynamics of $h(t)$ under replay are\n$$\n\\frac{dh}{dt} = \\eta r (1 - h) - \\mu h,\n$$\nwith hippocampal gain $\\eta>0$ and forgetting rate $\\mu>0$. In the absence of replay (baseline), set $r=0$ so that\n$$\n\\frac{ds}{dt} = -\\lambda s, \\quad \\frac{dh}{dt} = -\\mu h.\n$$\n\nDefine the episodic recall probability as\n$$\np(t) = \\left(1 - e^{-\\beta s(t)}\\right)\\left(1 - e^{-\\gamma h(t)}\\right),\n$$\nwith positive sensitivity parameters $\\beta>0$ and $\\gamma>0$. Let the steady-state values $s^\\ast$ and $h^\\ast$ under replay rate $r$ be the limits of $s(t)$ and $h(t)$ as $t \\to \\infty$, and let $p^\\ast$ be the corresponding steady-state recall probability.\n\nStarting from the core definitions above and well-tested modeling facts (linear first-order differential equations and exponential solutions), derive closed-form expressions for $s(t)$, $h(t)$, and $p^\\ast$ in terms of the parameters $(\\alpha,\\beta,\\eta,\\gamma,K,\\lambda,\\mu,s_0,h_0,r)$. Then, design an algorithm that:\n- Computes the learning curve $p(t)$ on an interval $t \\in [0,T]$ using the analytical solutions for $s(t)$ and $h(t)$.\n- Computes a convergence time for each parameter set defined below, where convergence time is the smallest $t \\ge 0$ such that $|p(t) - p^\\ast| \\le \\delta$. In the special case $r=0$ (baseline without replay), set $p^\\ast = 0$ and define the convergence time as the smallest $t \\ge 0$ such that $p(t) \\le \\delta$. Express time in dimensionless units consistent with the model.\n\nYour program must implement the analytical expressions and compute the convergence time via a robust numerical method that does not rely on external data. The final output must be a single line containing a comma-separated list enclosed in square brackets with the convergence times for all test cases in the test suite, in the same order as listed below.\n\nUse the following fixed parameters (all dimensionless): $\\alpha=0.4$, $\\beta=2.0$, $\\eta=0.7$, $\\gamma=3.0$, $s_0=0.1$, $h_0=0.2$, $\\delta=0.01$, and $T=200$. The forgetting rates $\\lambda$ and $\\mu$ and capacity $K$ are specified per test case. The program must compute and output convergence times for the following seven test cases:\n1. Baseline without replay: $r=0$, $K=1.0$, $\\lambda=0.05$, $\\mu=0.1$.\n2. Low replay: $r=0.05$, $K=1.0$, $\\lambda=0.05$, $\\mu=0.1$.\n3. Moderate replay: $r=0.2$, $K=1.0$, $\\lambda=0.05$, $\\mu=0.1$.\n4. High replay: $r=1.0$, $K=1.0$, $\\lambda=0.05$, $\\mu=0.1$.\n5. Extremely low replay (edge case): $r=10^{-6}$, $K=1.0$, $\\lambda=0.05$, $\\mu=0.1$.\n6. Increased cortical forgetting (edge case): $r=0.2$, $K=1.0$, $\\lambda=0.2$, $\\mu=0.1$.\n7. Reduced cortical capacity (edge case): $r=0.2$, $K=0.3$, $\\lambda=0.05$, $\\mu=0.1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5,result6,result7]\"). Each result should be a float representing the convergence time in the dimensionless time unit of the model. No angles or physical units are involved. The answers must be deterministic and reproducible.",
            "solution": "We begin by specifying the fundamental base consistent with brain modeling and computational neuroscience. Hippocampal Indexing Theory (HIT) posits that the hippocampus stores an index that binds neocortical representations for episodic memory. We adopt a standard linear-saturating form for the dynamics under replay, consistent with first-order kinetics and capacity limitations. These are well-tested modeling assumptions using linear differential equations with exponential solutions.\n\nThe cortical dynamics under replay are given by\n$$\n\\frac{ds}{dt} = \\alpha r \\left(1 - \\frac{s}{K}\\right) - \\lambda s = \\alpha r - \\left(\\frac{\\alpha r}{K} + \\lambda \\right) s,\n$$\nwhere $r$ is the mean replay rate, $\\alpha$ is a gain and $K$ is a capacity that limits growth, and $\\lambda$ is a forgetting rate. This is a linear differential equation with constant coefficients and constant input. Its solution is\n$$\ns(t) = s^\\ast + \\left(s_0 - s^\\ast\\right) e^{-c_s t},\n$$\nwith rate $c_s = \\frac{\\alpha r}{K} + \\lambda$ and steady-state\n$$\ns^\\ast = \\frac{\\alpha r}{c_s} = \\frac{\\alpha r}{\\frac{\\alpha r}{K} + \\lambda}.\n$$\nIn the baseline case $r=0$, the equation simplifies to $\\frac{ds}{dt} = -\\lambda s$ with solution $s(t) = s_0 e^{-\\lambda t}$, which is consistent with the general solution by setting $r=0$ and therefore $s^\\ast=0$.\n\nSimilarly, the hippocampal index dynamics under replay are\n$$\n\\frac{dh}{dt} = \\eta r (1 - h) - \\mu h = \\eta r - (\\eta r + \\mu) h,\n$$\nwith solution\n$$\nh(t) = h^\\ast + \\left(h_0 - h^\\ast\\right) e^{-c_h t},\n$$\nwhere $c_h = \\eta r + \\mu$ and\n$$\nh^\\ast = \\frac{\\eta r}{c_h} = \\frac{\\eta r}{\\eta r + \\mu}.\n$$\nUnder baseline $r=0$, we have $\\frac{dh}{dt} = -\\mu h$ and $h(t) = h_0 e^{-\\mu t}$, again recovered from the general solution with $h^\\ast=0$.\n\nEpisodic recall probability is modeled as a product of saturating functions of the trace strengths:\n$$\np(t) = \\left(1 - e^{-\\beta s(t)}\\right)\\left(1 - e^{-\\gamma h(t)}\\right),\n$$\nwhere $\\beta$ and $\\gamma$ scale the sensitivity of recall to cortical and hippocampal strengths, respectively. Because both $1 - e^{-\\beta s}$ and $1 - e^{-\\gamma h}$ are increasing functions of $s$ and $h$, respectively, $p(t)$ moves monotonically toward its limit as long as $s(t)$ and $h(t)$ are monotonic toward their steady-states. Given the linear stable dynamics above, $s(t)$ and $h(t)$ are monotonic (either increasing or decreasing) toward $s^\\ast$ and $h^\\ast$, respectively. The steady-state recall probability under replay is\n$$\np^\\ast = \\left(1 - e^{-\\beta s^\\ast}\\right)\\left(1 - e^{-\\gamma h^\\ast}\\right).\n$$\nUnder baseline $r=0$, we have $s^\\ast=0$ and $h^\\ast=0$, so $p^\\ast = 0$.\n\nWe are required to compute learning curves $p(t)$ and a convergence time for each parameter set. The convergence time is defined as the smallest $t \\ge 0$ such that $|p(t) - p^\\ast| \\le \\delta$, with $\\delta>0$ a small tolerance. Under baseline $r=0$, we specialize the criterion to the smallest $t \\ge 0$ such that $p(t) \\le \\delta$, because $p^\\ast = 0$. To compute this time, we use the analytical expressions for $s(t)$ and $h(t)$ and evaluate $p(t)$ efficiently. Because $p(t)$ approaches $p^\\ast$ as $t \\to \\infty$, and because $|p(t) - p^\\ast|$ decreases to $0$, we can use a robust bracketing method (bisection) on an interval $[0,T]$ with sufficiently large $T$. Define\n$$\ng(t) =\n\\begin{cases}\n|p(t) - p^\\ast| - \\delta, & r > 0, \\\\\np(t) - \\delta, & r = 0,\n\\end{cases}\n$$\nand find the smallest $t$ where $g(t) \\le 0$. Since $g(0)$ is typically positive unless the initial state is already within tolerance and $g(T) < 0$ for sufficiently large $T$, bisection can be applied. If $g(0) \\le 0$, the convergence time is $0$.\n\nAlgorithm design:\n1. For each test case, compute $c_s$, $s^\\ast$, $c_h$, and $h^\\ast$ using the formulas above.\n2. Define $s(t)$ and $h(t)$ by their closed forms and compute $p(t)$ via $p(t) = \\left(1 - e^{-\\beta s(t)}\\right)\\left(1 - e^{-\\gamma h(t)}\\right)$.\n3. Compute $p^\\ast$ using $s^\\ast$ and $h^\\ast$.\n4. Define the function $g(t)$ per the rule above and check if $g(0) \\le 0$. If so, record $0$ as the convergence time.\n5. Otherwise, apply bisection on $[0,T]$ to find the smallest $t$ such that $g(t) \\le 0$. To ensure the smallest such $t$, after finding the bracketed time, perform a short refinement: search the lower half recursively and track the last time where $g(t) > 0$ to approximate the smallest $t$ within a fixed tolerance.\n6. Return the convergence times for all test cases in the specified output format.\n\nThe test suite covers: the baseline without replay ($r=0$), low, moderate, and high replay rates; extremely small replay rate as an edge case; increased cortical forgetting at moderate replay; and reduced cortical capacity at moderate replay. These probe different asymptotic regimes and verify monotonic convergence toward steady-states or decay toward zero.\n\nThe implementation uses only the analytical closed forms for $s(t)$ and $h(t)$ and a deterministic bisection routine to find convergence times. All computations are dimensionless and require no external input. The final output is a single line containing the convergence times for the seven test cases as a comma-separated list enclosed in square brackets, as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef s_star(alpha, r, K, lam):\n    # s* = alpha r / (alpha r / K + lambda)\n    denom = (alpha * r) / K + lam\n    return (alpha * r) / denom if denom > 0 else 0.0\n\ndef h_star(eta, r, mu):\n    # h* = eta r / (eta r + mu)\n    denom = eta * r + mu\n    return (eta * r) / denom if denom > 0 else 0.0\n\ndef s_t(t, alpha, r, K, lam, s0):\n    cs = (alpha * r) / K + lam\n    sstar = s_star(alpha, r, K, lam)\n    return sstar + (s0 - sstar) * np.exp(-cs * t)\n\ndef h_t(t, eta, r, mu, h0):\n    ch = eta * r + mu\n    hstar = h_star(eta, r, mu)\n    return hstar + (h0 - hstar) * np.exp(-ch * t)\n\ndef p_t(t, alpha, beta, eta, gamma, K, lam, mu, s0, h0, r):\n    s = s_t(t, alpha, r, K, lam, s0)\n    h = h_t(t, eta, r, mu, h0)\n    return (1.0 - np.exp(-beta * s)) * (1.0 - np.exp(-gamma * h))\n\ndef p_ss(alpha, beta, eta, gamma, K, lam, mu, r):\n    sstar = s_star(alpha, r, K, lam)\n    hstar = h_star(eta, r, mu)\n    return (1.0 - np.exp(-beta * sstar)) * (1.0 - np.exp(-gamma * hstar))\n\ndef convergence_time(alpha, beta, eta, gamma, K, lam, mu, s0, h0, r, delta, T, max_iter=100, tol=1e-8):\n    # Compute steady-state recall probability\n    pstar = p_ss(alpha, beta, eta, gamma, K, lam, mu, r)\n    # Define g(t) depending on whether r==0 (baseline) or not\n    def g_baseline(t):\n        return p_t(t, alpha, beta, eta, gamma, K, lam, mu, s0, h0, r) - delta\n    def g_general(t):\n        return abs(p_t(t, alpha, beta, eta, gamma, K, lam, mu, s0, h0, r) - pstar) - delta\n\n    if r == 0.0:\n        g = g_baseline\n    else:\n        g = g_general\n\n    # If already within tolerance at t=0\n    if g(0.0) <= 0.0:\n        return 0.0\n\n    # Ensure the function becomes negative by T\n    gT = g(T)\n    if gT > 0.0:\n        # If not within tolerance by T, we still return T (conservative bound)\n        # However, given the dynamics, this should be rare with T sufficiently large.\n        return float(T)\n\n    # Bisection to find smallest t with g(t) <= 0\n    lo, hi = 0.0, T\n    # Standard bisection to find a root\n    for _ in range(max_iter):\n        mid = 0.5 * (lo + hi)\n        gm = g(mid)\n        # We want the smallest t where g(t) <= 0, so move hi when gm <= 0\n        if gm <= 0.0:\n            hi = mid\n        else:\n            lo = mid\n        if hi - lo < tol:\n            break\n    return hi\n\ndef solve():\n    # Fixed parameters (dimensionless)\n    alpha = 0.4\n    beta = 2.0\n    eta = 0.7\n    gamma = 3.0\n    s0 = 0.1\n    h0 = 0.2\n    delta = 0.01\n    T = 200.0\n\n    # Test cases: (r, K, lambda, mu)\n    test_cases = [\n        (0.0, 1.0, 0.05, 0.1),         # 1. Baseline without replay\n        (0.05, 1.0, 0.05, 0.1),        # 2. Low replay\n        (0.2, 1.0, 0.05, 0.1),         # 3. Moderate replay\n        (1.0, 1.0, 0.05, 0.1),         # 4. High replay\n        (1e-6, 1.0, 0.05, 0.1),        # 5. Extremely low replay\n        (0.2, 1.0, 0.2, 0.1),          # 6. Increased cortical forgetting\n        (0.2, 0.3, 0.05, 0.1),         # 7. Reduced cortical capacity\n    ]\n\n    results = []\n    for r, K, lam, mu in test_cases:\n        ct = convergence_time(alpha, beta, eta, gamma, K, lam, mu, s0, h0, r, delta, T)\n        # Format to a reasonable precision\n        results.append(f\"{ct:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}