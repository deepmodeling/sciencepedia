{
    "hands_on_practices": [
        {
            "introduction": "At the core of studying hippocampal place cells is the ability to \"read\" the neural code to determine an animal's location. This practice provides a foundational, hands-on application of Bayesian inference, a cornerstone of neural decoding models. By working through a concrete example with a small number of neurons and discrete locations, you will build tangible intuition for how patterns of neural spikes can be translated into an estimate of position .",
            "id": "3989039",
            "problem": "Consider a one-dimensional linear track with discrete candidate positions $x \\in \\{x_1, x_2, x_3\\}$ where $x_1 = 0\\,\\text{m}$, $x_2 = 0.5\\,\\text{m}$, and $x_3 = 1.0\\,\\text{m}$. A rat runs on the track while the spike counts of three hippocampal place cells are recorded over a fixed time window of duration $T = 0.1\\,\\text{s}$. Assume that, conditioned on position $x$, the spike counts $\\mathbf{n} = (n_1, n_2, n_3)$ across cells are independent and each follows a Poisson distribution with mean count $r_i(x)$ equal to the product of the instantaneous firing rate $f_i(x)$ (in Hertz) and the window duration $T$, that is, $r_i(x) = f_i(x) T$. The instantaneous firing rates for the three cells at the three candidate positions are:\n- At $x_1 = 0\\,\\text{m}$: $f_1(x_1) = 35$, $f_2(x_1) = 2$, $f_3(x_1) = 1$.\n- At $x_2 = 0.5\\,\\text{m}$: $f_1(x_2) = 8$, $f_2(x_2) = 28$, $f_3(x_2) = 6$.\n- At $x_3 = 1.0\\,\\text{m}$: $f_1(x_3) = 5$, $f_2(x_3) = 10$, $f_3(x_3) = 40$.\nSuppose the observed spike counts in the window are $\\mathbf{n} = (3, 2, 0)$. Let the prior over positions be uniform: $p(x_1) = p(x_2) = p(x_3)$. Starting from Bayes theorem and the Poisson generative assumption described above, derive the Bayesian decoder $p(x \\mid \\mathbf{n})$ up to a proportionality constant and compute the Maximum A Posteriori (MAP) estimate of position. Express the MAP estimate in meters. No rounding is required.",
            "solution": "The problem requires the derivation of the Bayesian decoder for a rat's position and the computation of the Maximum A Posteriori (MAP) estimate of the position. The position $x$ is a discrete variable from the set $\\{x_1, x_2, x_3\\}$, where $x_1 = 0\\,\\text{m}$, $x_2 = 0.5\\,\\text{m}$, and $x_3 = 1.0\\,\\text{m}$. The observed data is a vector of spike counts $\\mathbf{n} = (n_1, n_2, n_3) = (3, 2, 0)$ from three place cells over a time window $T = 0.1\\,\\text{s}$.\n\nThe MAP estimate of position, $\\hat{x}_{\\text{MAP}}$, is the position $x$ that maximizes the posterior probability $p(x \\mid \\mathbf{n})$. Using Bayes' theorem, the posterior probability is given by:\n$$p(x \\mid \\mathbf{n}) = \\frac{p(\\mathbf{n} \\mid x) p(x)}{p(\\mathbf{n})}$$\nwhere $p(\\mathbf{n} \\mid x)$ is the likelihood, $p(x)$ is the prior probability of the position, and $p(\\mathbf{n})$ is the evidence, which acts as a normalization constant. To find the MAP estimate, we seek to maximize the posterior:\n$$\\hat{x}_{\\text{MAP}} = \\arg\\max_{x \\in \\{x_1, x_2, x_3\\}} p(x \\mid \\mathbf{n})$$\nSince $p(\\mathbf{n})$ is constant with respect to $x$, this is equivalent to maximizing the numerator:\n$$\\hat{x}_{\\text{MAP}} = \\arg\\max_{x \\in \\{x_1, x_2, x_3\\}} p(\\mathbf{n} \\mid x) p(x)$$\nThe problem states that the prior over positions is uniform, i.e., $p(x_1) = p(x_2) = p(x_3) = c$, where $c$ is a constant (specifically, $c=1/3$). Since the prior $p(x)$ is a constant, it does not affect the location of the maximum. Therefore, maximizing the posterior is equivalent to maximizing the likelihood:\n$$\\hat{x}_{\\text{MAP}} = \\arg\\max_{x \\in \\{x_1, x_2, x_3\\}} p(\\mathbf{n} \\mid x)$$\nThe problem states that, conditioned on the position $x$, the spike counts of the three cells are independent. Thus, the likelihood can be expressed as the product of the probabilities for each cell:\n$$p(\\mathbf{n} \\mid x) = p(n_1 \\mid x) p(n_2 \\mid x) p(n_3 \\mid x) = \\prod_{i=1}^{3} p(n_i \\mid x)$$\nEach individual spike count $n_i$ is assumed to follow a Poisson distribution with mean $r_i(x) = f_i(x)T$, where $f_i(x)$ is the firing rate and $T$ is the time window. The probability mass function for the Poisson distribution is:\n$$p(n_i \\mid x) = \\frac{(r_i(x))^{n_i} \\exp(-r_i(x))}{n_i!}$$\nSubstituting this into the likelihood function gives:\n$$p(\\mathbf{n} \\mid x) = \\prod_{i=1}^{3} \\frac{(f_i(x)T)^{n_i} \\exp(-f_i(x)T)}{n_i!}$$\nThe Bayesian decoder $p(x \\mid \\mathbf{n})$ is proportional to this likelihood function, as the prior is uniform. The term $\\prod_{i=1}^{3} (1/n_i!)$ is a constant with respect to $x$. Thus, the decoder up to a proportionality constant is:\n$$p(x \\mid \\mathbf{n}) \\propto \\prod_{i=1}^{3} (f_i(x)T)^{n_i} \\exp(-f_i(x)T)$$\nTo find the MAP estimate, we can maximize the logarithm of the likelihood (the log-likelihood), which simplifies the calculation by converting products into sums. Maximizing $\\ln(p(\\mathbf{n} \\mid x))$ is equivalent to maximizing $p(\\mathbf{n} \\mid x)$.\n$$L(x) = \\ln(p(\\mathbf{n} \\mid x)) = \\ln\\left(\\prod_{i=1}^{3} \\frac{(r_i(x))^{n_i} \\exp(-r_i(x))}{n_i!}\\right) = \\sum_{i=1}^{3} [n_i \\ln(r_i(x)) - r_i(x) - \\ln(n_i!)]$$\nThe term $\\sum \\ln(n_i!)$ is independent of $x$ and can be dropped for the purpose of maximization. We seek to maximize the function $L'(x)$:\n$$L'(x) = \\sum_{i=1}^{3} [n_i \\ln(r_i(x)) - r_i(x)]$$\nWe first calculate the mean spike counts $r_i(x_j) = f_i(x_j)T$ for each cell $i$ at each position $x_j$, with $T=0.1\\,\\text{s}$. The given firing rates are:\nAt $x_1 = 0\\,\\text{m}$: $f_1(x_1)=35$, $f_2(x_1)=2$, $f_3(x_1)=1$.\n$r_1(x_1) = 35 \\times 0.1 = 3.5$, $r_2(x_1) = 2 \\times 0.1 = 0.2$, $r_3(x_1) = 1 \\times 0.1 = 0.1$.\n\nAt $x_2 = 0.5\\,\\text{m}$: $f_1(x_2)=8$, $f_2(x_2)=28$, $f_3(x_2)=6$.\n$r_1(x_2) = 8 \\times 0.1 = 0.8$, $r_2(x_2) = 28 \\times 0.1 = 2.8$, $r_3(x_2) = 6 \\times 0.1 = 0.6$.\n\nAt $x_3 = 1.0\\,\\text{m}$: $f_1(x_3)=5$, $f_2(x_3)=10$, $f_3(x_3)=40$.\n$r_1(x_3) = 5 \\times 0.1 = 0.5$, $r_2(x_3) = 10 \\times 0.1 = 1.0$, $r_3(x_3) = 40 \\times 0.1 = 4.0$.\n\nNow we compute $L'(x)$ for each position using the observed spike counts $\\mathbf{n} = (3, 2, 0)$:\nFor $x_1 = 0\\,\\text{m}$:\n$$L'(x_1) = [3 \\ln(3.5) - 3.5] + [2 \\ln(0.2) - 0.2] + [0 \\ln(0.1) - 0.1]$$\n$$L'(x_1) = 3 \\ln(3.5) + 2 \\ln(0.2) - 3.8 \\approx 3(1.2528) + 2(-1.6094) - 3.8 = 3.7584 - 3.2188 - 3.8 = -3.2604$$\n\nFor $x_2 = 0.5\\,\\text{m}$:\n$$L'(x_2) = [3 \\ln(0.8) - 0.8] + [2 \\ln(2.8) - 2.8] + [0 \\ln(0.6) - 0.6]$$\n$$L'(x_2) = 3 \\ln(0.8) + 2 \\ln(2.8) - 4.2 \\approx 3(-0.2231) + 2(1.0296) - 4.2 = -0.6693 + 2.0592 - 4.2 = -2.8101$$\n\nFor $x_3 = 1.0\\,\\text{m}$:\n$$L'(x_3) = [3 \\ln(0.5) - 0.5] + [2 \\ln(1.0) - 1.0] + [0 \\ln(4.0) - 4.0]$$\nSince $\\ln(1.0) = 0$:\n$$L'(x_3) = 3 \\ln(0.5) - 0.5 - 1.0 - 4.0 = 3 \\ln(0.5) - 5.5 \\approx 3(-0.6931) - 5.5 = -2.0793 - 5.5 = -7.5793$$\n\nComparing the values of the log-likelihood function for the three candidate positions:\n$L'(x_1) \\approx -3.2604$\n$L'(x_2) \\approx -2.8101$\n$L'(x_3) \\approx -7.5793$\nThe maximum value is $L'(x_2)$. Therefore, the posterior probability $p(x \\mid \\mathbf{n})$ is maximized at $x=x_2$.\n\nThe MAP estimate of the position is $\\hat{x}_{\\text{MAP}} = x_2 = 0.5\\,\\text{m}$.",
            "answer": "$$\\boxed{0.5}$$"
        },
        {
            "introduction": "After seeing how to decode position, a natural next question is: what are the theoretical limits on the accuracy of this neural map? Answering this requires quantifying the amount of information a population of neurons carries about a variable like position. This exercise introduces Fisher information and the Cramér-Rao Lower Bound, powerful theoretical tools for analyzing the performance limits of population codes, allowing you to derive how decoding accuracy scales with factors like the number of neurons and their firing rates .",
            "id": "3989067",
            "problem": "Consider a one-dimensional circular environment of circumference $L$ (in meters) in which an ensemble of $N$ hippocampal place cells encodes position $x \\in [0, L)$. Assume homogeneous tuning: the place field centers $\\{x_i\\}_{i=1}^{N}$ are evenly spaced on the circle, and all cells share identical Gaussian tuning curves with peak firing rate $r$ (in hertz) and spatial width $s$ (in meters), so that the mean firing rate of cell $i$ at position $x$ is $f_i(x) = r \\exp\\!\\big(-\\frac{(x - x_i)^2}{2 s^2}\\big)$. Over an observation window of duration $T$ (in seconds), spike counts $\\{k_i\\}_{i=1}^{N}$ are conditionally independent and distributed according to the Poisson distribution with means $\\lambda_i(x) = T f_i(x)$.\n\nStarting from first principles and standard definitions of the Poisson distribution and Fisher information, derive an analytic expression for the asymptotic lower bound on the expected root mean squared decoding error for any unbiased position estimator, averaged over $x$ under the homogeneous arrangement. Express your final answer as a single closed-form analytic function of $N$, $r$, $s$, $L$, and $T$. Then, based on this expression, determine the scaling laws that describe how decoding accuracy changes with the number of cells $N$ and peak firing rate $r$ under this homogeneous tuning regimen.\n\nYour final answer must be the single closed-form analytical expression for the asymptotic lower bound on the expected root mean squared decoding error as a function of $N$, $r$, $s$, $L$, and $T$. Express the error in meters. No numerical evaluation is required.",
            "solution": "The problem asks for the asymptotic lower bound on the expected root mean squared decoding error for the position $x$ of an animal on a one-dimensional circular track, given the spike counts from an ensemble of $N$ hippocampal place cells. This lower bound is given by the Cramér-Rao Lower Bound (CRLB).\n\nFor any unbiased estimator $\\hat{x}$ of the parameter $x$, its variance is bounded by the inverse of the Fisher information, $J(x)$:\n$$\n\\mathrm{Var}(\\hat{x}) \\ge \\frac{1}{J(x)}\n$$\nThe root mean squared error (RMSE) is $\\sqrt{\\mathrm{E}[(\\hat{x}-x)^2]}$. For an unbiased estimator, this is equal to the square root of the variance, $\\sqrt{\\mathrm{Var}(\\hat{x})}$. Therefore, the lower bound on the RMSE at a given position $x$ is $1/\\sqrt{J(x)}$.\n\nThe problem asks for this lower bound averaged over all positions $x \\in [0, L)$. The expected, or average, RMSE is $\\langle \\mathrm{RMSE} \\rangle_x = \\frac{1}{L} \\int_0^L \\mathrm{RMSE}(x) dx$. The lower bound on this quantity is:\n$$\n\\langle \\mathrm{RMSE} \\rangle_x \\ge \\frac{1}{L} \\int_0^L \\frac{1}{\\sqrt{J(x)}} dx\n$$\n\nFirst, we must calculate the Fisher information $J(x)$. The spike counts $\\{k_i\\}_{i=1}^{N}$ are conditionally independent and follow Poisson distributions. The total Fisher information is the sum of the information from each neuron:\n$$\nJ(x) = \\sum_{i=1}^{N} J_i(x)\n$$\nFor a Poisson process with mean rate $\\lambda(x)$, the Fisher information about the parameter $x$ is given by the formula:\n$$\nJ_i(x) = \\frac{(\\lambda_i'(x))^2}{\\lambda_i(x)}\n$$\nwhere $\\lambda_i'(x)$ is the derivative of $\\lambda_i(x)$ with respect to $x$.\n\nThe mean spike count for neuron $i$ is given as $\\lambda_i(x) = T f_i(x)$, where $T$ is the observation time and $f_i(x)$ is the tuning curve:\n$$\n\\lambda_i(x) = T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right)\n$$\nHere, $r$ is the peak firing rate, $x_i$ is the place field center, and $s$ is the spatial width of the tuning curve.\n\nWe calculate the derivative $\\lambda_i'(x)$:\n$$\n\\lambda_i'(x) = \\frac{d}{dx} \\left[ T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right) \\right]\n= T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right) \\cdot \\left(-\\frac{2(x - x_i)}{2 s^2}\\right)\n$$\n$$\n\\lambda_i'(x) = -\\frac{x - x_i}{s^2} \\lambda_i(x)\n$$\n\nNow we substitute this into the formula for $J_i(x)$:\n$$\nJ_i(x) = \\frac{\\left(-\\frac{x - x_i}{s^2} \\lambda_i(x)\\right)^2}{\\lambda_i(x)} = \\frac{(x - x_i)^2}{s^4} \\lambda_i(x)\n$$\nSubstituting the expression for $\\lambda_i(x)$:\n$$\nJ_i(x) = \\frac{(x - x_i)^2}{s^4} T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right)\n$$\n\nThe total Fisher information $J(x)$ is the sum over all $N$ neurons:\n$$\nJ(x) = \\sum_{i=1}^{N} J_i(x) = \\frac{T r}{s^4} \\sum_{i=1}^{N} (x - x_i)^2 \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right)\n$$\nFor a finite number of neurons $N$, this function $J(x)$ is not constant and depends on the position $x$. The integral for the average RMSE lower bound would be analytically intractable.\n\nThe problem, however, asks for the \"asymptotic\" lower bound. This is typically interpreted as the limit where the number of cells $N$ is large, and their fields provide dense coverage of the environment. In this limit, the summation can be approximated by an integral. The place field centers $\\{x_i\\}$ are evenly spaced on the circle of circumference $L$, so the linear density of neurons is $\\rho = N/L$. The sum can be converted to an integral over the position of the place field center, which we denote as $y$:\n$$\n\\sum_{i=1}^{N} g(x_i) \\approx \\int \\rho \\, g(y) \\, dy = \\frac{N}{L} \\int g(y) \\, dy\n$$\nApplying this approximation to $J(x)$, we replace $x_i$ with a continuous variable $y$ and integrate over the entire environment. Assuming the place field width $s$ is much smaller than the environment size $L$ ($s \\ll L$), we can extend the integration limits to $\\pm\\infty$ with negligible error.\n$$\nJ(x) \\approx \\frac{N}{L} \\int_{-\\infty}^{\\infty} \\frac{T r}{s^4} (x - y)^2 \\exp\\left(-\\frac{(x - y)^2}{2 s^2}\\right) dy\n$$\nLet $u = y - x$, so $du = dy$. The expression becomes:\n$$\nJ \\approx \\frac{N T r}{L s^4} \\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2 s^2}\\right) du\n$$\nThe result of this integral is independent of $x$, which is expected for a homogeneous code in the high-density limit. We will denote this constant asymptotic Fisher information as $J$.\n\nThe integral is a standard form of a Gaussian integral. Let $a = 1/(2s^2)$. The integral is $\\int_{-\\infty}^{\\infty} u^2 \\exp(-au^2) du$. One way to solve this is to consider the basic Gaussian integral $I(a) = \\int_{-\\infty}^{\\infty} \\exp(-au^2) du = \\sqrt{\\pi/a}$. Differentiating with respect to $a$ gives:\n$$\n\\frac{dI(a)}{da} = \\int_{-\\infty}^{\\infty} -u^2 \\exp(-au^2) du = -\\frac{1}{2} \\pi^{1/2} a^{-3/2} = -\\frac{1}{2a}\\sqrt{\\frac{\\pi}{a}}\n$$\nThus, $\\int_{-\\infty}^{\\infty} u^2 \\exp(-au^2) du = \\frac{1}{2a}\\sqrt{\\frac{\\pi}{a}}$.\nSubstituting $a = 1/(2s^2)$:\n$$\n\\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2 s^2}\\right) du = \\frac{1}{2(1/(2s^2))} \\sqrt{\\frac{\\pi}{1/(2s^2)}} = s^2 \\sqrt{2\\pi s^2} = s^3 \\sqrt{2\\pi}\n$$\nNow, we substitute this result back into the expression for $J$:\n$$\nJ \\approx \\frac{N T r}{L s^4} \\left(s^3 \\sqrt{2\\pi}\\right) = \\frac{N T r \\sqrt{2\\pi}}{L s}\n$$\nSince $J$ is constant in this asymptotic limit, the position-averaged RMSE lower bound simplifies:\n$$\n\\langle \\mathrm{RMSE} \\rangle_x \\ge \\frac{1}{L} \\int_0^L \\frac{1}{\\sqrt{J}} dx = \\frac{1}{\\sqrt{J}}\n$$\nThe final expression for the lower bound on the error is therefore $1/\\sqrt{J}$:\n$$\n\\text{Error}_{\\text{min}} = \\sqrt{\\frac{L s}{N T r \\sqrt{2\\pi}}}\n$$\nThis expression shows the scaling laws for decoding accuracy. The error decreases (accuracy increases) with the square root of the number of cells ($N$) and the square root of the peak firing rate ($r$), i.e., $\\text{Error}_{\\text{min}} \\propto N^{-1/2}$ and $\\text{Error}_{\\text{min}} \\propto r^{-1/2}$. This is a characteristic result for population codes where information combines across neurons.\nThe unit of the final expression is meters, as $[L s / (N T r)]^{1/2}$ has units $[m \\cdot m / (1 \\cdot s \\cdot s^{-1})]^{1/2} = [m^2]^{1/2} = m$.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{L s}{N T r \\sqrt{2\\pi}}}}\n$$"
        },
        {
            "introduction": "Hippocampal place fields are not static; they are highly plastic, adapting to environmental changes and behavioral goals. A key question is how the brain learns which environmental cues are reliable predictors of location, especially when cues conflict. This problem models how a place cell can resolve such ambiguity using a classic error-driven associative learning rule, providing insight into the mechanisms of remapping and the adaptive nature of the hippocampal cognitive map .",
            "id": "3989045",
            "problem": "Consider the following computational experiment motivated by hippocampal place cells and cue conflict. A one-dimensional circular track contains a stable visual landmark at position $x_{\\mathrm{v}}$ and a stable odor source at position $x_{\\mathrm{o}}$, with a fixed spatial mismatch $|\\Delta| = |x_{\\mathrm{v}} - x_{\\mathrm{o}}|$ that remains constant over trials. A single place cell is modeled as having two candidate subfields anchored to these cues. Its predicted firing-rate field at time (trial) $t$ over position $x$ is given by\n$$\nr(x;t) = w_{\\mathrm{v}}(t)\\,\\exp\\!\\left(-\\frac{d(x,x_{\\mathrm{v}})^2}{2\\sigma^2}\\right) \\;+\\; w_{\\mathrm{o}}(t)\\,\\exp\\!\\left(-\\frac{d(x,x_{\\mathrm{o}})^2}{2\\sigma^2}\\right),\n$$\nwhere $d(\\cdot,\\cdot)$ is geodesic distance on the circle, $\\sigma > 0$ is a fixed spatial scale, and $w_{\\mathrm{v}}(t), w_{\\mathrm{o}}(t) \\ge 0$ are cue-specific associative strengths that act as subfield amplitudes anchored at $x_{\\mathrm{v}}$ and $x_{\\mathrm{o}}$, respectively. The associative strengths are updated per trial by an error-driven rule derived from classical associative learning: for each cue $c \\in \\{\\mathrm{v},\\mathrm{o}\\}$,\n$$\nw_{c}(t{+}1) \\;\\leftarrow\\; \\mathrm{clip}_{[0,1]}\\!\\Big(w_{c}(t) \\;+\\; \\eta\\,S_{c}\\,\\big(\\lambda_{c} \\;-\\; \\hat{r}_{c}(t)\\big)\\Big),\n$$\nwhere $\\eta \\in [0,1]$ is the learning rate, $S_{c} \\in [0,1]$ is the cue-specific salience, $\\lambda_{c} \\in [0,1]$ is the teacher signal at the cue’s anchor (with $\\lambda_{c} = 1$ indicating that a single-field solution aligned to cue $c$ is behaviorally reinforced, and $\\lambda_{c} = 0$ indicating no reinforcement at that anchor), $\\hat{r}_{c}(t)$ is the model’s current predicted amplitude at the cue’s anchor (specifically $\\hat{r}_{\\mathrm{v}}(t) = w_{\\mathrm{v}}(t)$ and $\\hat{r}_{\\mathrm{o}}(t) = w_{\\mathrm{o}}(t)$), and $\\mathrm{clip}_{[0,1]}(\\cdot)$ projects to the interval $[0,1]$. A place field is called “active” at a cue $c$ on trial $t$ if $w_{c}(t) \\ge \\theta$, where $\\theta \\in (0,1)$ is a fixed activity threshold expressed as a fraction of the maximum possible amplitude $1$. The state is called a “double field” if both cues are active and is called a “single field” if exactly one cue is active. Ambiguity is said to be resolved on the first trial index $t$ where a single field occurs.\n\nStarting from the core definitions above and without using any shortcut formulas beyond them, implement a program that, for each parameter set in the test suite below, simulates the learning dynamics for at most $T$ trials and returns:\n- the first trial index $t_{\\star} \\in \\{1,2,\\dots,T\\}$ at which the model resolves to a single field under the activity criterion $w_{c}(t) \\ge \\theta$; if no such trial occurs, return $-1$, and\n- a winner code $u \\in \\{1,0,-1\\}$, where $u = 1$ if the visual cue is the sole active field at $t_{\\star}$, $u = 0$ if the odor cue is the sole active field at $t_{\\star}$, and $u = -1$ if no single field occurs within $T$ trials.\n\nAssume that the check for resolution is performed after each weight update per trial. Initialize $w_{\\mathrm{v}}(0)$ and $w_{\\mathrm{o}}(0)$ as specified. The spatial scale $\\sigma$ and positions $x_{\\mathrm{v}}, x_{\\mathrm{o}}$ do not affect the update at the anchors, but they motivate the interpretation of $w_{\\mathrm{v}}$ and $w_{\\mathrm{o}}$ as subfield amplitudes.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[t_{\\star},u]$ for one test case, in the same order as the test suite. For example, the required string format is like $[[t_{\\star}^{(1)},u^{(1)}],[t_{\\star}^{(2)},u^{(2)}],\\dots]$.\n\nTest suite:\n- Case $1$ (happy path; reinforcement favors the visual cue):\n  - $w_{\\mathrm{v}}(0) = 0.5$, $w_{\\mathrm{o}}(0) = 0.5$, $\\eta = 0.2$, $S_{\\mathrm{v}} = 1.0$, $S_{\\mathrm{o}} = 1.0$, $\\lambda_{\\mathrm{v}} = 1.0$, $\\lambda_{\\mathrm{o}} = 0.0$, $\\theta = 0.2$, $T = 30$.\n- Case $2$ (boundary; no learning):\n  - $w_{\\mathrm{v}}(0) = 0.5$, $w_{\\mathrm{o}}(0) = 0.5$, $\\eta = 0.0$, $S_{\\mathrm{v}} = 1.0$, $S_{\\mathrm{o}} = 1.0$, $\\lambda_{\\mathrm{v}} = 1.0$, $\\lambda_{\\mathrm{o}} = 0.0$, $\\theta = 0.2$, $T = 30$.\n- Case $3$ (winner flips; reinforcement favors the odor cue):\n  - $w_{\\mathrm{v}}(0) = 0.2$, $w_{\\mathrm{o}}(0) = 0.8$, $\\eta = 0.1$, $S_{\\mathrm{v}} = 1.0$, $S_{\\mathrm{o}} = 1.0$, $\\lambda_{\\mathrm{v}} = 0.0$, $\\lambda_{\\mathrm{o}} = 1.0$, $\\theta = 0.2$, $T = 40$.\n- Case $4$ (edge; extremely low odor salience slows extinction, no resolution within allotted trials):\n  - $w_{\\mathrm{v}}(0) = 0.5$, $w_{\\mathrm{o}}(0) = 0.5$, $\\eta = 0.1$, $S_{\\mathrm{v}} = 1.0$, $S_{\\mathrm{o}} = 0.01$, $\\lambda_{\\mathrm{v}} = 1.0$, $\\lambda_{\\mathrm{o}} = 0.0$, $\\theta = 0.2$, $T = 20$.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extraction of Givens\n\nThe provided information is as follows:\n- **Firing Rate Model:** The firing rate $r(x;t)$ of a place cell at position $x$ and trial $t$ is given by a sum of two Gaussian-like subfields:\n$$\nr(x;t) = w_{\\mathrm{v}}(t)\\,\\exp\\!\\left(-\\frac{d(x,x_{\\mathrm{v}})^2}{2\\sigma^2}\\right) \\;+\\; w_{\\mathrm{o}}(t)\\,\\exp\\!\\left(-\\frac{d(x,x_{\\mathrm{o}})^2}{2\\sigma^2}\\right)\n$$\n- **Variables:** $w_{\\mathrm{v}}(t)$ and $w_{\\mathrm{o}}(t)$ are the associative strengths for the visual (v) and odor (o) cues, respectively. $x_{\\mathrm{v}}$ and $x_{\\mathrm{o}}$ are the cue anchor positions. $\\sigma > 0$ is a spatial scale parameter. $d(\\cdot, \\cdot)$ is the geodesic distance on a circular track.\n- **Weight Update Rule:** For each cue $c \\in \\{\\mathrm{v},\\mathrm{o}\\}$, the weights are updated according to the rule:\n$$\nw_{c}(t{+}1) \\;\\leftarrow\\; \\mathrm{clip}_{[0,1]}\\!\\Big(w_{c}(t) \\;+\\; \\eta\\,S_{c}\\,\\big(\\lambda_{c} \\;-\\; \\hat{r}_{c}(t)\\big)\\Big)\n$$\n- **Rule Parameters:**\n    - $\\eta \\in [0,1]$: learning rate.\n    - $S_{c} \\in [0,1]$: cue-specific salience.\n    - $\\lambda_{c} \\in [0,1]$: teacher signal for cue $c$.\n    - $\\hat{r}_{c}(t)$: the model's predicted amplitude at the cue's anchor.\n- **Crucial Specification:** The predicted amplitudes are explicitly defined as:\n$$\n\\hat{r}_{\\mathrm{v}}(t) = w_{\\mathrm{v}}(t) \\quad \\text{and} \\quad \\hat{r}_{\\mathrm{o}}(t) = w_{\\mathrm{o}}(t)\n$$\n- **State Definitions:**\n    - **Activity Threshold:** $\\theta \\in (0,1)$. A field is \"active\" if $w_c(t) \\ge \\theta$.\n    - **Single Field:** Exactly one field is active.\n    - **Resolution:** The first trial $t$ at which a single field state occurs.\n- **Task:** For a given set of parameters $\\{w_{\\mathrm{v}}(0), w_{\\mathrm{o}}(0), \\eta, S_{\\mathrm{v}}, S_{\\mathrm{o}}, \\lambda_{\\mathrm{v}}, \\lambda_{\\mathrm{o}}, \\theta, T\\}$, determine the resolution trial $t_{\\star} \\in \\{1, 2, \\dots, T\\}$ and a winner code $u$.\n    - $u=1$ if the visual cue field is the sole active one at $t_{\\star}$.\n    - $u=0$ if the odor cue field is the sole active one at $t_{\\star}$.\n    - If no resolution occurs by trial $T$, $t_{\\star} = -1$ and $u = -1$.\n- **Test Suite:** Four distinct parameter sets are provided for simulation.\n\n### Step 2: Validation\n\nThe problem is evaluated against the established criteria:\n1.  **Scientifically Grounded:** The model is a discrete-time formulation of an error-driven learning rule, conceptually identical to the Rescorla-Wagner model, a cornerstone of associative learning theory. Its application to hippocampal place cells in a cue-conflict scenario is a standard paradigm in computational neuroscience. The problem is scientifically sound.\n2.  **Well-Posed:** The problem provides explicit initial conditions, a deterministic update rule, and a clear criterion for termination. The system's evolution is uniquely determined by the parameters. The definition of the predicted amplitudes, $\\hat{r}_c(t) = w_c(t)$, is a critical disambiguation that ensures the problem is self-contained and avoids potential confusion regarding cue interaction in the learning rule itself. An unambiguous, unique solution exists for each test case.\n3.  **Objective:** The problem is stated using precise mathematical definitions and objective language, free of subjective or ambiguous terminology.\n\nAll other invalidity criteria (Incomplete Setup, Unrealistic Conditions, etc.) are not met. The problem is rigorous and formalizable. The descriptive context regarding the spatial firing field $r(x;t)$ serves as appropriate scientific motivation, while the problem correctly specifies that the core dynamics are governed by the weights $w_c(t)$ alone.\n\n### Step 3: Verdict\n\nThe problem is **valid**.\n\n### Solution Derivation\n\nThe analysis begins by substituting the specified definitions $\\hat{r}_{\\mathrm{v}}(t) = w_{\\mathrm{v}}(t)$ and $\\hat{r}_{\\mathrm{o}}(t) = w_{\\mathrm{o}}(t)$ into the general update rule. This decouples the dynamics of the two weights, leading to two independent first-order difference equations.\n\nFor the visual cue weight $w_{\\mathrm{v}}(t)$:\n$$\nw_{\\mathrm{v}}(t{+}1) \\;\\leftarrow\\; \\mathrm{clip}_{[0,1]}\\!\\Big(w_{\\mathrm{v}}(t) \\;+\\; \\eta\\,S_{\\mathrm{v}}\\,\\big(\\lambda_{\\mathrm{v}} \\;-\\; w_{\\mathrm{v}}(t)\\big)\\Big)\n$$\nRearranging the terms inside the clipping function gives:\n$$\nw_{\\mathrm{v}}(t{+}1) \\;\\leftarrow\\; \\mathrm{clip}_{[0,1]}\\!\\Big( (1 - \\eta S_{\\mathrm{v}})w_{\\mathrm{v}}(t) + \\eta S_{\\mathrm{v}} \\lambda_{\\mathrm{v}} \\Big)\n$$\nSimilarly, for the odor cue weight $w_{\\mathrm{o}}(t)$:\n$$\nw_{\\mathrm{o}}(t{+}1) \\;\\leftarrow\\; \\mathrm{clip}_{[0,1]}\\!\\Big( (1 - \\eta S_{\\mathrm{o}})w_{\\mathrm{o}}(t) + \\eta S_{\\mathrm{o}} \\lambda_{\\mathrm{o}} \\Big)\n$$\nThese equations reveal that the system consists of two uncoupled linear transformations followed by a clipping operation. The spatial variables $x_{\\mathrm{v}}$, $x_{\\mathrm{o}}$, and $\\sigma$ are extraneous to the calculation, as correctly noted in the problem statement.\n\nThe computational task is to simulate these two equations iteratively for each test case. The algorithm is as follows:\n\n1.  Initialize the trial counter $t$ to $0$ and the weights $w_{\\mathrm{v}}$ and $w_{\\mathrm{o}}$ to their starting values, $w_{\\mathrm{v}}(0)$ and $w_{\\mathrm{o}}(0)$.\n2.  Begin a loop that iterates from trial $t=1$ to the maximum number of trials, $T$.\n3.  In each iteration, update $w_{\\mathrm{v}}$ and $w_{\\mathrm{o}}$ from their values at the previous trial ($t-1$) to their new values for trial $t$, using the derived update equations. The $\\mathrm{clip}_{[0,1]}(\\cdot)$ function ensures the weights remain in the valid range $[0,1]$.\n4.  After the update, evaluate the activity states for both cues at trial $t$. Let $A_{\\mathrm{v}} = (w_{\\mathrm{v}}(t) \\ge \\theta)$ and $A_{\\mathrm{o}} = (w_{\\mathrm{o}}(t) \\ge \\theta)$ be boolean indicators of activity.\n5.  Check for resolution to a \"single field\". This occurs if exactly one of $A_{\\mathrm{v}}$ and $A_{\\mathrm{o}}$ is true.\n    - If $A_{\\mathrm{v}}$ is true and $A_{\\mathrm{o}}$ is false, resolution has occurred. The result is $[t, 1]$. The simulation for this case terminates.\n    - If $A_{\\mathrm{v}}$ is false and $A_{\\mathrm{o}}$ is true, resolution has occurred. The result is $[t, 0]$. The simulation for this case terminates.\n6.  If the loop completes up to trial $T$ without the single-field condition being met, the ambiguity is not resolved within the allotted time. The result is $[-1, -1]$.\n\nThis procedure is systematically applied to each parameter set provided in the test suite to generate the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cue conflict resolution problem by simulating the learning dynamics\n    for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: happy path; reinforcement favors the visual cue\n        {'w_v_0': 0.5, 'w_o_0': 0.5, 'eta': 0.2, 'S_v': 1.0, 'S_o': 1.0, 'lambda_v': 1.0, 'lambda_o': 0.0, 'theta': 0.2, 'T': 30},\n        # Case 2: boundary; no learning\n        {'w_v_0': 0.5, 'w_o_0': 0.5, 'eta': 0.0, 'S_v': 1.0, 'S_o': 1.0, 'lambda_v': 1.0, 'lambda_o': 0.0, 'theta': 0.2, 'T': 30},\n        # Case 3: winner flips; reinforcement favors the odor cue\n        {'w_v_0': 0.2, 'w_o_0': 0.8, 'eta': 0.1, 'S_v': 1.0, 'S_o': 1.0, 'lambda_v': 0.0, 'lambda_o': 1.0, 'theta': 0.2, 'T': 40},\n        # Case 4: edge; extremely low odor salience slows extinction\n        {'w_v_0': 0.5, 'w_o_0': 0.5, 'eta': 0.1, 'S_v': 1.0, 'S_o': 0.01, 'lambda_v': 1.0, 'lambda_o': 0.0, 'theta': 0.2, 'T': 20},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        w_v = case['w_v_0']\n        w_o = case['w_o_0']\n        eta = case['eta']\n        S_v = case['S_v']\n        S_o = case['S_o']\n        lambda_v = case['lambda_v']\n        lambda_o = case['lambda_o']\n        theta = case['theta']\n        T = case['T']\n\n        # Initialize result for this case as unresolved\n        case_result = [-1, -1]\n\n        # Loop through trials from t=1 to T\n        for t in range(1, T + 1):\n            # The updates are based on the weights from the previous trial (t-1).\n            # These are simultaneous updates, so we calculate the new values before assigning them.\n            w_v_new = w_v + eta * S_v * (lambda_v - w_v)\n            w_o_new = w_o + eta * S_o * (lambda_o - w_o)\n\n            # Apply the clip function to project to the interval [0, 1]\n            w_v = np.clip(w_v_new, 0.0, 1.0)\n            w_o = np.clip(w_o_new, 0.0, 1.0)\n            \n            # Check for activity based on the threshold theta\n            is_v_active = (w_v >= theta)\n            is_o_active = (w_o >= theta)\n\n            # Check for single field resolution\n            if is_v_active and not is_o_active:\n                # Visual cue wins\n                case_result = [t, 1]\n                break\n            elif not is_v_active and is_o_active:\n                # Odor cue wins\n                case_result = [t, 0]\n                break\n        \n        results.append(case_result)\n\n    # Format the output string exactly as required, without extra spaces.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}