{
    "hands_on_practices": [
        {
            "introduction": "A defining feature of a hippocampal place cell is its spatially selective firing, but how does this property emerge from underlying cellular mechanisms? This practice guides you through a foundational model that connects synaptic inputs to systems-level function. By analyzing a conductance-based leaky integrate-and-fire neuron, you will see how a combination of spatially tuned excitation and broad inhibition can be transformed by the neuron's intrinsic voltage threshold to create a localized place field . This exercise provides a concrete, bottom-up understanding of how a neuron's receptive field is shaped.",
            "id": "3989050",
            "problem": "Consider a single-compartment conductance-based leaky integrate-and-fire neuron intended to model a hippocampal place cell. The neuron receives a set of synaptic conductances that depend on the spatial position $\\mathbf{x}$ of the animal. Focus on a one-dimensional track with position $x \\in [0,L]$ and assume the animal moves sufficiently slowly that the membrane potential can be treated in a quasi-steady-state regime with respect to position. The neuron has a passive leak characterized by leak conductance $g_{\\mathrm{L}}$ and leak reversal potential $E_{\\mathrm{L}}$. It receives an excitatory conductance $g_{\\mathrm{E}}(x)$ with excitatory reversal potential $E_{\\mathrm{E}}$ and an inhibitory conductance $g_{\\mathrm{I}}(x)$ with inhibitory reversal potential $E_{\\mathrm{I}}$. Assume the inhibitory conductance is spatially uniform, $g_{\\mathrm{I}}(x) \\equiv g_{\\mathrm{I0}}$, and the excitatory conductance is spatially tuned as a Gaussian\n$$\ng_{\\mathrm{E}}(x) = g_{\\mathrm{E0}} \\exp\\!\\left(-\\frac{(x - x_0)^2}{2 \\sigma^2}\\right),\n$$\nwhere $g_{\\mathrm{E0}}$ is the peak conductance, $x_0$ is the center of the field, and $\\sigma$ is the spatial scale (standard deviation) of the tuning. The neuron emits spikes when the membrane potential $V(t)$ crosses a fixed threshold $V_{\\mathrm{th}}$.\n\nStarting from the conductance-based leaky integrate-and-fire ordinary differential equation\n$$\nC_{\\mathrm{m}} \\frac{dV}{dt} = -g_{\\mathrm{L}} \\left(V - E_{\\mathrm{L}}\\right) - g_{\\mathrm{E}}\\!\\left(x(t)\\right)\\left(V - E_{\\mathrm{E}}\\right) - g_{\\mathrm{I0}} \\left(V - E_{\\mathrm{I}}\\right),\n$$\nwhere $C_{\\mathrm{m}}$ is the membrane capacitance, derive from first principles an expression for the steady-state membrane potential $V_{\\mathrm{ss}}(x)$ under the quasi-steady-state assumption for $x(t)$, and obtain a condition on $x$ such that $V_{\\mathrm{ss}}(x) \\ge V_{\\mathrm{th}}$. Show how this thresholding defines a contiguous spatial interval around $x_0$, and from this derive a formula for the place field width $w$ in meters. Your derivation must handle all boundary cases, including the absence of spiking ($w=0$) and the case where the threshold is so low that spiking occurs across the entire track ($w=L$). Use only the stated physical assumptions and the given definitions.\n\nYour program must implement the derived expression, compute the place field width $w$ for each test case listed below, and print the widths in meters. If the derived width exceeds the track length $L$, clip it to $L$. If the derived conditions imply no threshold crossing, set the width to $0$. All quantities must be treated in the International System of Units (SI): conductances in siemens, voltages in volts, positions and widths in meters. The output must be a single line containing the numerical results as a comma-separated list enclosed in square brackets, with each width formatted as a decimal number.\n\nUse the following test suite of parameter sets to evaluate your implementation. Each case provides $(L, g_{\\mathrm{L}}, g_{\\mathrm{I0}}, g_{\\mathrm{E0}}, E_{\\mathrm{L}}, E_{\\mathrm{I}}, E_{\\mathrm{E}}, V_{\\mathrm{th}}, \\sigma, x_0)$:\n\n- Case 1 (general case with a moderate place field): $(2.0,\\ 2\\times 10^{-8},\\ 1\\times 10^{-8},\\ 5\\times 10^{-8},\\ -0.065,\\ -0.070,\\ 0.0,\\ -0.050,\\ 0.1,\\ 1.0)$\n- Case 2 (stronger inhibition, narrower field): $(2.0,\\ 2\\times 10^{-8},\\ 3\\times 10^{-8},\\ 5\\times 10^{-8},\\ -0.065,\\ -0.070,\\ 0.0,\\ -0.050,\\ 0.1,\\ 1.0)$\n- Case 3 (high threshold causing no spiking): $(2.0,\\ 2\\times 10^{-8},\\ 1\\times 10^{-8},\\ 5\\times 10^{-8},\\ -0.065,\\ -0.070,\\ 0.0,\\ -0.010,\\ 0.1,\\ 1.0)$\n- Case 4 (low threshold, entire track active): $(2.0,\\ 2\\times 10^{-8},\\ 1\\times 10^{-8},\\ 5\\times 10^{-8},\\ -0.065,\\ -0.070,\\ 0.0,\\ -0.070,\\ 0.1,\\ 1.0)$\n- Case 5 (wider excitatory tuning, wider field): $(2.0,\\ 2\\times 10^{-8},\\ 1\\times 10^{-8},\\ 5\\times 10^{-8},\\ -0.065,\\ -0.070,\\ 0.0,\\ -0.050,\\ 0.2,\\ 1.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[w1,w2,w3,w4,w5]\", where each $w$ is the computed place field width in meters for the corresponding case.",
            "solution": "Begin with the conductance-based leaky integrate-and-fire dynamics\n$$\nC_{\\mathrm{m}} \\frac{dV}{dt} = -g_{\\mathrm{L}} \\left(V - E_{\\mathrm{L}}\\right) - g_{\\mathrm{E}}(x(t))\\left(V - E_{\\mathrm{E}}\\right) - g_{\\mathrm{I0}} \\left(V - E_{\\mathrm{I}}\\right).\n$$\nThis equation combines a leak current and synaptic currents, each expressed as conductance times driving force. The neuron spikes when the membrane potential $V(t)$ reaches a threshold $V_{\\mathrm{th}}$, after which the voltage is reset (the reset dynamics is not needed for the steady-state analysis).\n\nUnder the quasi-steady-state assumption, we treat the position $x$ as a parameter and set $dV/dt \\approx 0$ for the voltage dynamics at each position $x$. Solving for the steady-state membrane potential $V_{\\mathrm{ss}}(x)$ yields\n$$\n0 = -g_{\\mathrm{L}} \\left(V_{\\mathrm{ss}} - E_{\\mathrm{L}}\\right) - g_{\\mathrm{E}}(x)\\left(V_{\\mathrm{ss}} - E_{\\mathrm{E}}\\right) - g_{\\mathrm{I0}} \\left(V_{\\mathrm{ss}} - E_{\\mathrm{I}}\\right),\n$$\nwhich can be rearranged as\n$$\n\\left(g_{\\mathrm{L}} + g_{\\mathrm{E}}(x) + g_{\\mathrm{I0}}\\right) V_{\\mathrm{ss}}(x) = g_{\\mathrm{L}} E_{\\mathrm{L}} + g_{\\mathrm{E}}(x) E_{\\mathrm{E}} + g_{\\mathrm{I0}} E_{\\mathrm{I}}.\n$$\nTherefore,\n$$\nV_{\\mathrm{ss}}(x) = \\frac{g_{\\mathrm{L}} E_{\\mathrm{L}} + g_{\\mathrm{E}}(x) E_{\\mathrm{E}} + g_{\\mathrm{I0}} E_{\\mathrm{I}}}{g_{\\mathrm{L}} + g_{\\mathrm{E}}(x) + g_{\\mathrm{I0}}}.\n$$\n\nSpiking occurs when $V_{\\mathrm{ss}}(x) \\ge V_{\\mathrm{th}}$. Define constants\n$$\nA = g_{\\mathrm{L}} E_{\\mathrm{L}} + g_{\\mathrm{I0}} E_{\\mathrm{I}}, \\quad B = E_{\\mathrm{E}}, \\quad C = g_{\\mathrm{L}} + g_{\\mathrm{I0}},\n$$\nso that\n$$\nV_{\\mathrm{ss}}(x) = \\frac{A + B\\, g_{\\mathrm{E}}(x)}{C + g_{\\mathrm{E}}(x)}.\n$$\nThe threshold condition $V_{\\mathrm{ss}}(x) \\ge V_{\\mathrm{th}}$ becomes\n$$\n\\frac{A + B\\, g_{\\mathrm{E}}(x)}{C + g_{\\mathrm{E}}(x)} \\ge V_{\\mathrm{th}}.\n$$\nMultiply both sides by $C + g_{\\mathrm{E}}(x)$ (which is positive for nonnegative conductances) to obtain\n$$\nA + B\\, g_{\\mathrm{E}}(x) \\ge V_{\\mathrm{th}} C + V_{\\mathrm{th}} g_{\\mathrm{E}}(x).\n$$\nRearrange terms in $g_{\\mathrm{E}}(x)$:\n$$\n\\left(B - V_{\\mathrm{th}}\\right) g_{\\mathrm{E}}(x) \\ge V_{\\mathrm{th}} C - A.\n$$\nWhen $\\left(B - V_{\\mathrm{th}}\\right) > 0$ (which is physically reasonable because the excitatory reversal potential is typically above threshold), the inequality reduces to\n$$\ng_{\\mathrm{E}}(x) \\ge g_{\\mathrm{req}} \\equiv \\frac{V_{\\mathrm{th}} C - A}{B - V_{\\mathrm{th}}}.\n$$\nOtherwise, if $\\left(B - V_{\\mathrm{th}}\\right) \\le 0$, excitatory input cannot raise the membrane above threshold since the driving force for excitation is insufficient. In that regime, the condition reduces to checking the baseline steady-state (with $g_{\\mathrm{E}}(x) \\approx 0$), namely, whether\n$$\nV_{\\mathrm{base}} \\equiv \\frac{A}{C} \\ge V_{\\mathrm{th}}.\n$$\nIf $V_{\\mathrm{base}} \\ge V_{\\mathrm{th}}$, the neuron would be above threshold for all positions, and if $V_{\\mathrm{base}} < V_{\\mathrm{th}}$, the neuron would never reach threshold.\n\nFor the Gaussian excitatory conductance,\n$$\ng_{\\mathrm{E}}(x) = g_{\\mathrm{E0}} \\exp\\!\\left(-\\frac{(x - x_0)^2}{2 \\sigma^2}\\right),\n$$\nthe inequality $g_{\\mathrm{E}}(x) \\ge g_{\\mathrm{req}}$ becomes\n$$\n\\exp\\!\\left(-\\frac{(x - x_0)^2}{2 \\sigma^2}\\right) \\ge \\frac{g_{\\mathrm{req}}}{g_{\\mathrm{E0}}}.\n$$\nTaking natural logarithms on both sides (noting that the exponential is positive and the right-hand side must be positive to admit a solution), we have\n$$\n-\\frac{(x - x_0)^2}{2 \\sigma^2} \\ge \\ln\\!\\left(\\frac{g_{\\mathrm{req}}}{g_{\\mathrm{E0}}}\\right),\n$$\nwhich yields\n$$\n(x - x_0)^2 \\le 2 \\sigma^2 \\ln\\!\\left(\\frac{g_{\\mathrm{E0}}}{g_{\\mathrm{req}}}\\right).\n$$\nThus, provided $0 < g_{\\mathrm{req}} < g_{\\mathrm{E0}}$, the set of positions where $V_{\\mathrm{ss}}(x) \\ge V_{\\mathrm{th}}$ is the interval\n$$\nx \\in \\left[x_0 - \\sigma \\sqrt{2 \\ln\\!\\left(\\frac{g_{\\mathrm{E0}}}{g_{\\mathrm{req}}}\\right)},\\ x_0 + \\sigma \\sqrt{2 \\ln\\!\\left(\\frac{g_{\\mathrm{E0}}}{g_{\\mathrm{req}}}\\right)}\\right].\n$$\nThe place field width $w$ is then\n$$\nw = 2 \\sigma \\sqrt{2 \\ln\\!\\left(\\frac{g_{\\mathrm{E0}}}{g_{\\mathrm{req}}}\\right)}.\n$$\n\nBoundary and edge cases:\n- If $g_{\\mathrm{req}} \\ge g_{\\mathrm{E0}}$, then no position satisfies the inequality and $w = 0$.\n- If $g_{\\mathrm{req}} \\le 0$, then any nonnegative excitatory conductance suffices to reach threshold. In this situation, the neuron is at or above threshold even far from the center, and the place field spans the entire track, yielding $w = L$.\n- If $\\left(B - V_{\\mathrm{th}}\\right) \\le 0$, then evaluate $V_{\\mathrm{base}} = A/C$. If $V_{\\mathrm{base}} \\ge V_{\\mathrm{th}}$, set $w = L$; otherwise, set $w = 0$.\n- If the computed width $w$ exceeds the physical track length $L$, clip to $w = L$.\n\nAlgorithmic steps for each test case:\n1. Read $(L, g_{\\mathrm{L}}, g_{\\mathrm{I0}}, g_{\\mathrm{E0}}, E_{\\mathrm{L}}, E_{\\mathrm{I}}, E_{\\mathrm{E}}, V_{\\mathrm{th}}, \\sigma, x_0)$.\n2. Compute $A = g_{\\mathrm{L}} E_{\\mathrm{L}} + g_{\\mathrm{I0}} E_{\\mathrm{I}}$, $B = E_{\\mathrm{E}}$, $C = g_{\\mathrm{L}} + g_{\\mathrm{I0}}$.\n3. If $(B - V_{\\mathrm{th}}) \\le 0$, set $w = L$ if $A/C \\ge V_{\\mathrm{th}}$, else $w = 0$.\n4. Otherwise compute\n   $$\n   g_{\\mathrm{req}} = \\frac{V_{\\mathrm{th}} C - A}{B - V_{\\mathrm{th}}}.\n   $$\n   If $g_{\\mathrm{req}} \\le 0$, set $w = L$; if $g_{\\mathrm{req}} \\ge g_{\\mathrm{E0}}$, set $w = 0$; else set\n   $$\n   w = 2 \\sigma \\sqrt{2 \\ln\\!\\left(\\frac{g_{\\mathrm{E0}}}{g_{\\mathrm{req}}}\\right)}.\n   $$\n5. Clip $w$ to $L$ if $w > L$.\n6. Output all $w$ values as a single comma-separated list within square brackets.\n\nNumerical evaluation for the provided cases confirms the qualitative expectations:\n- Case 1: a moderate width due to balanced excitation and inhibition.\n- Case 2: stronger inhibition increases the required excitatory conductance and narrows the width.\n- Case 3: a high threshold makes $g_{\\mathrm{req}}$ exceed $g_{\\mathrm{E0}}$, yielding $w = 0$.\n- Case 4: a low threshold yields $g_{\\mathrm{req}} \\le 0$, activating the entire track, so $w = L$.\n- Case 5: increasing $\\sigma$ widens the field proportionally via the square-root dependence on $\\sigma$.\n\nThe program implements this procedure and prints the widths in meters for the five cases in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_width(L, gL, gI0, gE0, EL, EI, EE, Vth, sigma, x0):\n    # Constants derived from the conductance-based steady-state model\n    A = gL * EL + gI0 * EI\n    B = EE\n    C = gL + gI0\n\n    # Handle case where excitatory driving force is insufficient\n    if (B - Vth) <= 0:\n        V_base = A / C\n        w = L if V_base >= Vth else 0.0\n        return min(max(w, 0.0), L)\n\n    # Required excitatory conductance to reach threshold\n    g_req = (Vth * C - A) / (B - Vth)\n\n    # Edge cases based on g_req\n    if g_req <= 0:\n        # Threshold is low enough that baseline suffices; whole track is active\n        w = L\n    elif g_req >= gE0:\n        # Even peak excitatory conductance is insufficient; no place field\n        w = 0.0\n    else:\n        # Compute width from Gaussian inequality\n        # w = 2 * sigma * sqrt(2 * ln(gE0 / g_req))\n        ratio = gE0 / g_req\n        # Numerical safety: ensure ratio > 1\n        if ratio <= 1.0:\n            w = 0.0\n        else:\n            w = 2.0 * sigma * np.sqrt(2.0 * np.log(ratio))\n    # Clip to track length\n    if w > L:\n        w = L\n    if w < 0.0:\n        w = 0.0\n    return float(w)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (L, gL, gI0, gE0, EL, EI, EE, Vth, sigma, x0)\n    test_cases = [\n        (2.0, 2e-8, 1e-8, 5e-8, -0.065, -0.070, 0.0, -0.050, 0.1, 1.0),\n        (2.0, 2e-8, 3e-8, 5e-8, -0.065, -0.070, 0.0, -0.050, 0.1, 1.0),\n        (2.0, 2e-8, 1e-8, 5e-8, -0.065, -0.070, 0.0, -0.010, 0.1, 1.0),\n        (2.0, 2e-8, 1e-8, 5e-8, -0.065, -0.070, 0.0, -0.070, 0.1, 1.0),\n        (2.0, 2e-8, 1e-8, 5e-8, -0.065, -0.070, 0.0, -0.050, 0.2, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        L, gL, gI0, gE0, EL, EI, EE, Vth, sigma, x0 = case\n        w = compute_width(L, gL, gI0, gE0, EL, EI, EE, Vth, sigma, x0)\n        results.append(f\"{w:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The brain represents space not with a single neuron, but with the collective activity of a large population of place cells. This raises a fundamental question: how does the fidelity of this spatial code depend on the properties of the neural ensemble? This practice introduces Fisher information, a powerful concept used to calculate the theoretical limit on decoding accuracy. By deriving the Cramér-Rao Lower Bound for position estimation, you will uncover the fundamental \"scaling laws\" of population coding, revealing how accuracy improves with the number of neurons and their firing rates .",
            "id": "3989067",
            "problem": "Consider a one-dimensional circular environment of circumference $L$ (in meters) in which an ensemble of $N$ hippocampal place cells encodes position $x \\in [0, L)$. Assume homogeneous tuning: the place field centers $\\{x_i\\}_{i=1}^{N}$ are evenly spaced on the circle, and all cells share identical Gaussian tuning curves with peak firing rate $r$ (in hertz) and spatial width $s$ (in meters), so that the mean firing rate of cell $i$ at position $x$ is $f_i(x) = r \\exp\\!\\big(-\\frac{(x - x_i)^2}{2 s^2}\\big)$. Over an observation window of duration $T$ (in seconds), spike counts $\\{k_i\\}_{i=1}^{N}$ are conditionally independent and distributed according to the Poisson distribution with means $\\lambda_i(x) = T f_i(x)$.\n\nStarting from first principles and standard definitions of the Poisson distribution and Fisher information, derive an analytic expression for the asymptotic lower bound on the expected root mean squared decoding error for any unbiased position estimator, averaged over $x$ under the homogeneous arrangement. Express your final answer as a single closed-form analytic function of $N$, $r$, $s$, $L$, and $T$. Then, based on this expression, determine the scaling laws that describe how decoding accuracy changes with the number of cells $N$ and peak firing rate $r$ under this homogeneous tuning regimen.\n\nYour final answer must be the single closed-form analytical expression for the asymptotic lower bound on the expected root mean squared decoding error as a function of $N$, $r$, $s$, $L$, and $T$. Express the error in meters. No numerical evaluation is required.",
            "solution": "The problem asks for the asymptotic lower bound on the expected root mean squared decoding error for the position $x$ of an animal on a one-dimensional circular track, given the spike counts from an ensemble of $N$ hippocampal place cells. This lower bound is given by the Cramér-Rao Lower Bound (CRLB).\n\nFor any unbiased estimator $\\hat{x}$ of the parameter $x$, its variance is bounded by the inverse of the Fisher information, $J(x)$:\n$$\n\\mathrm{Var}(\\hat{x}) \\ge \\frac{1}{J(x)}\n$$\nThe root mean squared error (RMSE) is $\\sqrt{\\mathrm{E}[(\\hat{x}-x)^2]}$. For an unbiased estimator, this is equal to the square root of the variance, $\\sqrt{\\mathrm{Var}(\\hat{x})}$. Therefore, the lower bound on the RMSE at a given position $x$ is $1/\\sqrt{J(x)}$.\n\nThe problem asks for this lower bound averaged over all positions $x \\in [0, L)$. The expected, or average, RMSE is $\\langle \\mathrm{RMSE} \\rangle_x = \\frac{1}{L} \\int_0^L \\mathrm{RMSE}(x) dx$. The lower bound on this quantity is:\n$$\n\\langle \\mathrm{RMSE} \\rangle_x \\ge \\frac{1}{L} \\int_0^L \\frac{1}{\\sqrt{J(x)}} dx\n$$\n\nFirst, we must calculate the Fisher information $J(x)$. The spike counts $\\{k_i\\}_{i=1}^{N}$ are conditionally independent and follow Poisson distributions. The total Fisher information is the sum of the information from each neuron:\n$$\nJ(x) = \\sum_{i=1}^{N} J_i(x)\n$$\nFor a Poisson process with mean rate $\\lambda(x)$, the Fisher information about the parameter $x$ is given by the formula:\n$$\nJ_i(x) = \\frac{(\\lambda_i'(x))^2}{\\lambda_i(x)}\n$$\nwhere $\\lambda_i'(x)$ is the derivative of $\\lambda_i(x)$ with respect to $x$.\n\nThe mean spike count for neuron $i$ is given as $\\lambda_i(x) = T f_i(x)$, where $T$ is the observation time and $f_i(x)$ is the tuning curve:\n$$\n\\lambda_i(x) = T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right)\n$$\nHere, $r$ is the peak firing rate, $x_i$ is the place field center, and $s$ is the spatial width of the tuning curve.\n\nWe calculate the derivative $\\lambda_i'(x)$:\n$$\n\\lambda_i'(x) = \\frac{d}{dx} \\left[ T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right) \\right]\n= T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right) \\cdot \\left(-\\frac{2(x - x_i)}{2 s^2}\\right)\n$$\n$$\n\\lambda_i'(x) = -\\frac{x - x_i}{s^2} \\lambda_i(x)\n$$\n\nNow we substitute this into the formula for $J_i(x)$:\n$$\nJ_i(x) = \\frac{\\left(-\\frac{x - x_i}{s^2} \\lambda_i(x)\\right)^2}{\\lambda_i(x)} = \\frac{(x - x_i)^2}{s^4} \\lambda_i(x)\n$$\nSubstituting the expression for $\\lambda_i(x)$:\n$$\nJ_i(x) = \\frac{(x - x_i)^2}{s^4} T r \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right)\n$$\n\nThe total Fisher information $J(x)$ is the sum over all $N$ neurons:\n$$\nJ(x) = \\sum_{i=1}^{N} J_i(x) = \\frac{T r}{s^4} \\sum_{i=1}^{N} (x - x_i)^2 \\exp\\left(-\\frac{(x - x_i)^2}{2 s^2}\\right)\n$$\nFor a finite number of neurons $N$, this function $J(x)$ is not constant and depends on the position $x$. The integral for the average RMSE lower bound would be analytically intractable.\n\nThe problem, however, asks for the \"asymptotic\" lower bound. This is typically interpreted as the limit where the number of cells $N$ is large, and their fields provide dense coverage of the environment. In this limit, the summation can be approximated by an integral. The place field centers $\\{x_i\\}$ are evenly spaced on the circle of circumference $L$, so the linear density of neurons is $\\rho = N/L$. The sum can be converted to an integral over the position of the place field center, which we denote as $y$:\n$$\n\\sum_{i=1}^{N} g(x_i) \\approx \\int \\rho \\, g(y) \\, dy = \\frac{N}{L} \\int g(y) \\, dy\n$$\nApplying this approximation to $J(x)$, we replace $x_i$ with a continuous variable $y$ and integrate over the entire environment. Assuming the place field width $s$ is much smaller than the environment size $L$ ($s \\ll L$), we can extend the integration limits to $\\pm\\infty$ with negligible error.\n$$\nJ(x) \\approx \\frac{N}{L} \\int_{-\\infty}^{\\infty} \\frac{T r}{s^4} (x - y)^2 \\exp\\left(-\\frac{(x - y)^2}{2 s^2}\\right) dy\n$$\nLet $u = y - x$, so $du = dy$. The expression becomes:\n$$\nJ \\approx \\frac{N T r}{L s^4} \\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2 s^2}\\right) du\n$$\nThe result of this integral is independent of $x$, which is expected for a homogeneous code in the high-density limit. We will denote this constant asymptotic Fisher information as $J$.\n\nThe integral is a standard form of a Gaussian integral. Let $a = 1/(2s^2)$. The integral is $\\int_{-\\infty}^{\\infty} u^2 \\exp(-au^2) du$. One way to solve this is to consider the basic Gaussian integral $I(a) = \\int_{-\\infty}^{\\infty} \\exp(-au^2) du = \\sqrt{\\pi/a}$. Differentiating with respect to $a$ gives:\n$$\n\\frac{dI(a)}{da} = \\int_{-\\infty}^{\\infty} -u^2 \\exp(-au^2) du = -\\frac{1}{2} \\pi^{1/2} a^{-3/2} = -\\frac{1}{2a}\\sqrt{\\frac{\\pi}{a}}\n$$\nThus, $\\int_{-\\infty}^{\\infty} u^2 \\exp(-au^2) du = \\frac{1}{2a}\\sqrt{\\frac{\\pi}{a}}$.\nSubstituting $a = 1/(2s^2)$:\n$$\n\\int_{-\\infty}^{\\infty} u^2 \\exp\\left(-\\frac{u^2}{2 s^2}\\right) du = \\frac{1}{2(1/(2s^2))} \\sqrt{\\frac{\\pi}{1/(2s^2)}} = s^2 \\sqrt{2\\pi s^2} = s^3 \\sqrt{2\\pi}\n$$\nNow, we substitute this result back into the expression for $J$:\n$$\nJ \\approx \\frac{N T r}{L s^4} \\left(s^3 \\sqrt{2\\pi}\\right) = \\frac{N T r \\sqrt{2\\pi}}{L s}\n$$\nSince $J$ is constant in this asymptotic limit, the position-averaged RMSE lower bound simplifies:\n$$\n\\langle \\mathrm{RMSE} \\rangle_x \\ge \\frac{1}{L} \\int_0^L \\frac{1}{\\sqrt{J}} dx = \\frac{1}{\\sqrt{J}}\n$$\nThe final expression for the lower bound on the error is therefore $1/\\sqrt{J}$:\n$$\n\\text{Error}_{\\text{min}} = \\sqrt{\\frac{L s}{N T r \\sqrt{2\\pi}}}\n$$\nThis expression shows the scaling laws for decoding accuracy. The error decreases (accuracy increases) with the square root of the number of cells ($N$) and the square root of the peak firing rate ($r$), i.e., $\\text{Error}_{\\text{min}} \\propto N^{-1/2}$ and $\\text{Error}_{\\text{min}} \\propto r^{-1/2}$. This is a characteristic result for population codes where information combines across neurons.\nThe unit of the final expression is meters, as $[L s / (N T r)]^{1/2}$ has units $[m \\cdot m / (1 \\cdot s \\cdot s^{-1})]^{1/2} = [m^2]^{1/2} = m$.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{L s}{N T r \\sqrt{2\\pi}}}}\n$$"
        },
        {
            "introduction": "Beyond simply encoding location, the hippocampus is crucial for memory, and its internal circuitry is thought to support this function. The CA3 subregion, with its dense web of recurrent connections, is widely hypothesized to act as an autoassociative memory network. This exercise models the CA3 network as a dynamical system to explore its capacity for pattern completion (retrieving a full memory from a partial cue) and pattern separation (distinguishing between similar memories). By analyzing the system's fixed points and their stability, you will gain hands-on experience with attractor network models, a classic and influential framework for understanding memory recall in neural circuits .",
            "id": "3989022",
            "problem": "Consider a simplified model of the Cornu Ammonis area 3 (CA3) subregion of the hippocampus as an autoassociative recurrent network that supports pattern completion and pattern separation in place cell representations. Let there be a single scalar overlap order parameter $m \\in [-1,1]$ that quantifies the similarity between the current network state and a stored place-cell pattern (map). The goal is to derive, from first principles, when recurrent dynamics lead to pattern completion versus pattern separation, and to design a computational model that predicts a remapping threshold as a function of cue similarity.\n\nFundamental base for the derivation:\n- Wilson–Cowan rate equation for a homogeneous population with recurrent coupling: $$\\tau \\frac{dm}{dt} = -m + \\Phi\\big(h\\big),$$ where $\\tau$ is a time constant, $\\Phi(\\cdot)$ is a sigmoidal gain function, and $h$ is the total synaptic input to the population.\n- Recurrent excitation and global inhibition: $$h = \\beta \\left( J m - K m + I s \\right),$$ where $\\beta>0$ is the gain (inverse noise), $J \\ge 0$ is the effective recurrent excitation strength arising from Hebbian storage of the target map, $K \\ge 0$ aggregates global inhibitory feedback and interference from other maps, $I \\ge 0$ scales the external cue drive, and $s \\in [0,1]$ is the cue similarity to the target map.\n- A standard sigmoidal nonlinearity approximated by the hyperbolic tangent: $$\\Phi(h) = \\tanh(h).$$\n\nDefinitions and objectives:\n- Pattern completion is defined as convergence to a stable fixed point with overlap $m$ exceeding a prescribed criterion $m_c \\in (0,1)$.\n- Pattern separation is defined as failure to reach the criterion, typically stabilizing near $m \\approx 0$ or converging to a different attractor.\n- The remapping threshold $s^\\star$ is the minimal cue similarity $s$ for which the network achieves a stable fixed point with $m \\ge m_c$.\n\nTasks:\n1. Starting from the Wilson–Cowan rate equation and the specified $h$, derive the fixed-point equation for $m$ and a stability condition for the fixed point.\n2. Using the fixed-point and stability conditions, derive an explicit expression for the minimal cue similarity $s^\\star$ required to achieve $m \\ge m_c$ as a stable solution. Clearly state any assumptions and approximations used.\n3. Implement a program that, for each parameter set in the test suite below, computes the remapping threshold $s^\\star$, applies the stability condition, and enforces the physical bounds:\n   - If the fixed point at the criterion is unstable for any $s$, return $-1$.\n   - If the computed $s^\\star < 0$, return $0$.\n   - If the computed $s^\\star > 1$, return $-1$.\n   - Otherwise, return $s^\\star$.\n4. The final numerical outputs must be dimensionless and rounded to six decimal places.\n\nTest suite:\n- Case $1$: $(J,K,\\beta,I,m_c) = (0.8, 0.2, 1.5, 1.0, 0.5)$.\n- Case $2$: $(J,K,\\beta,I,m_c) = (0.3, 0.1, 2.0, 0.5, 0.6)$.\n- Case $3$: $(J,K,\\beta,I,m_c) = (1.6, 0.4, 1.0, 1.0, 0.5)$.\n- Case $4$: $(J,K,\\beta,I,m_c) = (1.5, 0.0, 2.0, 1.0, 0.7)$.\n- Case $5$: $(J,K,\\beta,I,m_c) = (0.2, 0.15, 1.0, 0.2, 0.6)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the five cases as a comma-separated list enclosed in square brackets, for example, [x_1,x_2,x_3,x_4,x_5], where each $x_i$ follows the rules specified above and is rounded to six decimal places. No additional text should be printed.",
            "solution": "The analysis of the hippocampal CA3 model proceeds from the fundamental principles of dynamical systems theory applied to the provided Wilson-Cowan rate equation.\n\n### Step 1: Derivation of the Fixed-Point Equation and Stability Condition\n\nThe dynamics of the overlap order parameter $m$ are governed by the differential equation:\n$$\n\\tau \\frac{dm}{dt} = -m + \\Phi(h)\n$$\nThe total synaptic input $h$ is given by:\n$$\nh = \\beta \\left( (J - K) m + I s \\right)\n$$\nSubstituting the specific gain function $\\Phi(h) = \\tanh(h)$ and the expression for $h$, we obtain the full dynamical equation:\n$$\n\\tau \\frac{dm}{dt} = -m + \\tanh\\left(\\beta \\left( (J - K) m + I s \\right)\\right)\n$$\nThis is an autonomous one-dimensional dynamical system of the form $\\tau \\frac{dm}{dt} = F(m)$, where:\n$$\nF(m) = -m + \\tanh\\left(\\beta \\left( (J - K) m + I s \\right)\\right)\n$$\n\n**Fixed-Point Equation:**\nA fixed point, denoted as $m^*$, is a state where the system ceases to evolve, i.e., $\\frac{dm}{dt} = 0$. This implies that $F(m^*) = 0$. Setting the right-hand side of the dynamical equation to zero yields the fixed-point equation:\n$$\nm^* = \\tanh\\left(\\beta \\left( (J - K) m^* + I s \\right)\\right)\n$$\nThis transcendental equation defines the equilibrium values of the overlap $m^*$ for a given set of parameters $(J, K, \\beta, I, s)$.\n\n**Stability Condition:**\nThe stability of a fixed point $m^*$ is determined by the sign of the derivative of $F(m)$ evaluated at $m=m^*$. A fixed point is stable if $\\frac{dF}{dm}|_{m=m^*} < 0$ and unstable if $\\frac{dF}{dm}|_{m=m^*} > 0$.\n\nWe compute the derivative of $F(m)$:\n$$\n\\frac{dF}{dm} = \\frac{d}{dm} \\left[ -m + \\tanh\\left(\\beta \\left( (J - K) m + I s \\right)\\right) \\right]\n$$\nUsing the chain rule, where $\\frac{d}{dx}\\tanh(u) = \\text{sech}^2(u)\\frac{du}{dx}$, we get:\n$$\n\\frac{dF}{dm} = -1 + \\text{sech}^2\\left(\\beta \\left( (J - K) m + I s \\right)\\right) \\cdot \\frac{d}{dm}\\left[\\beta \\left( (J - K) m + I s \\right)\\right]\n$$\n$$\n\\frac{dF}{dm} = -1 + \\beta(J - K) \\text{sech}^2\\left(\\beta \\left( (J - K) m + I s \\right)\\right)\n$$\nThe stability condition is $\\frac{dF}{dm}|_{m=m^*} < 0$:\n$$\n-1 + \\beta(J - K) \\text{sech}^2\\left(\\beta \\left( (J - K) m^* + I s \\right)\\right) < 0\n$$\n$$\n\\beta(J - K) \\text{sech}^2\\left(\\beta \\left( (J - K) m^* + I s \\right)\\right) < 1\n$$\nWe can simplify this condition by using the identity $\\text{sech}^2(x) = 1 - \\tanh^2(x)$ and the fixed-point equation $m^* = \\tanh\\left(\\beta \\left( (J - K) m^* + I s \\right)\\right)$. Substituting these into the inequality gives:\n$$\n\\beta(J - K) \\left(1 - \\left[\\tanh\\left(\\beta \\left( (J - K) m^* + I s \\right)\\right)\\right]^2\\right) < 1\n$$\n$$\n\\beta(J - K) (1 - (m^*)^2) < 1\n$$\nThis is the general stability condition for any fixed point $m^*$.\n\n### Step 2: Derivation of the Remapping Threshold $s^\\star$\n\nThe remapping threshold $s^\\star$ is defined as the minimum cue similarity $s$ required to achieve a stable fixed point with $m \\ge m_c$. The threshold case occurs precisely when the system can support a stable fixed point at $m = m_c$. We therefore substitute $m^* = m_c$ into the fixed-point equation and solve for the corresponding similarity, which we label $s^\\star$.\n\nThe fixed-point equation at the criterion $m_c$ is:\n$$\nm_c = \\tanh\\left(\\beta \\left( (J - K) m_c + I s^\\star \\right)\\right)\n$$\nTo solve for $s^\\star$, we first apply the inverse hyperbolic tangent function, $\\text{arctanh}(\\cdot)$, to both sides. The function $\\text{arctanh}(y)$ is defined for $y \\in (-1, 1)$. Since the problem specifies $m_c \\in (0,1)$, its application is valid.\n$$\n\\text{arctanh}(m_c) = \\beta \\left( (J - K) m_c + I s^\\star \\right)\n$$\nRearranging to isolate $s^\\star$:\n$$\n\\frac{\\text{arctanh}(m_c)}{\\beta} = (J - K) m_c + I s^\\star\n$$\n$$\nI s^\\star = \\frac{\\text{arctanh}(m_c)}{\\beta} - (J - K) m_c\n$$\nAssuming $I > 0$ (as is true for all test cases, since $I \\ge 0$ and a value of $I=0$ would imply the cue has no influence), we can divide by $I$:\n$$\ns^\\star = \\frac{1}{I} \\left[ \\frac{\\text{arctanh}(m_c)}{\\beta} - (J - K) m_c \\right]\n$$\nThis is the explicit expression for the remapping threshold $s^\\star$.\n\n### Step 3: Application of Stability and Physical Constraints\n\nThe computational task requires evaluating $s^\\star$ and then applying a series of conditions.\n\n1.  **Stability Check:** The fixed point at $m = m_c$ must be stable. We apply the stability condition derived earlier, evaluated at $m^* = m_c$:\n    $$\n    \\beta(J - K) (1 - m_c^2) < 1\n    $$\n    This condition is independent of $s$. If it is not met, i.e., if $\\beta(J - K) (1 - m_c^2) \\ge 1$, the fixed point at $m=m_c$ can never be stable, regardless of the cue similarity $s$. In this scenario, the problem states the output should be $-1$.\n\n2.  **Physical Bounds on $s^\\star$**: The cue similarity $s$ is physically constrained to the interval $[0,1]$.\n    - If the calculated $s^\\star > 1$, it means that even the strongest possible cue ($s=1$) is insufficient to achieve the pattern completion criterion. The threshold is physically unreachable, so the output must be $-1$.\n    - If the calculated $s^\\star < 0$, it implies that the network's recurrent connectivity $(J-K)$ is strong enough to spontaneously achieve the state $m_c$ (or greater) even with no positive cue ($s=0$), or even against an opposing cue ($s<0$). The minimum *physical* cue similarity required is $s=0$. Therefore, the threshold is effectively $0$.\n    - If $0 \\le s^\\star \\le 1$, the calculated value is the physically meaningful remapping threshold.\n\nThe final returned value is rounded to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the remapping threshold for a simplified hippocampal CA3 model.\n    \"\"\"\n    # Test suite: (J, K, beta, I, m_c)\n    test_cases = [\n        (0.8, 0.2, 1.5, 1.0, 0.5),\n        (0.3, 0.1, 2.0, 0.5, 0.6),\n        (1.6, 0.4, 1.0, 1.0, 0.5),\n        (1.5, 0.0, 2.0, 1.0, 0.7),\n        (0.2, 0.15, 1.0, 0.2, 0.6),\n    ]\n\n    results = []\n    for case in test_cases:\n        J, K, beta, I, m_c = case\n\n        # 1. Stability Condition Check\n        # The fixed point at m = m_c must be stable.\n        # Condition: beta * (J - K) * (1 - m_c^2) < 1\n        stability_term = beta * (J - K) * (1 - m_c**2)\n        if stability_term >= 1:\n            results.append(-1.0)\n            continue\n\n        # 2. Calculate the remapping threshold s_star\n        # s_star = (1/I) * [ (arctanh(m_c) / beta) - (J - K) * m_c ]\n        \n        # Check for I=0 to prevent division by zero, although not in test cases.\n        # If I=0, s_star is only defined if the RHS is also zero.\n        # The problem is structured such that I > 0 in test cases.\n        if I == 0:\n            # If I=0, the cue has no effect. The equation for s_star is undefined.\n            # We can't reach m_c by varying s. This a form of failure.\n            results.append(-1.0) \n            continue\n            \n        # m_c is guaranteed to be in (0,1), so arctanh is defined.\n        term1 = np.arctanh(m_c) / beta\n        term2 = (J - K) * m_c\n        s_star = (term1 - term2) / I\n\n        # 3. Apply physical bounds and problem rules\n        if s_star > 1:\n            # Threshold is physically unreachable\n            results.append(-1.0)\n        elif s_star < 0:\n            # Spontaneous completion, minimal physical cue is 0\n            results.append(0.0)\n        else:\n            # Valid threshold\n            results.append(s_star)\n\n    # Format the final output as specified.\n    # Each result is formatted to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}